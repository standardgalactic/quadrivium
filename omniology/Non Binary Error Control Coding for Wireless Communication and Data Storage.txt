
NON-BINARY ERROR
CONTROL CODING
FOR WIRELESS
COMMUNICATION
AND DATA STORAGE
Rolando Antonio Carrasco
Newcastle University, UK
Martin Johnston
Newcastle University, UK

NON-BINARY ERROR
CONTROL CODING
FOR WIRELESS
COMMUNICATION
AND DATA STORAGE


NON-BINARY ERROR
CONTROL CODING
FOR WIRELESS
COMMUNICATION
AND DATA STORAGE
Rolando Antonio Carrasco
Newcastle University, UK
Martin Johnston
Newcastle University, UK

This edition Ô¨Årst published 2008.
C 2008 John Wiley & Sons, Ltd.
Registered ofÔ¨Åce
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, United Kingdom
For details of our global editorial ofÔ¨Åces, for customer services and for information about how to apply for
permission to reuse the copyright material in this book please see our web site at www.wiley.com.
The right of the author to be identiÔ¨Åed as the author of this work has been asserted in accordance with the
Copyright, Designs and Patents Act 1988.
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in
any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the
UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be
available in electronic books.
Designations used by companies to distinguish their products are often claimed as trademarks. All brand names and
product names used in this book are trade names, service marks, trademarks or registered trademarks of their
respective owners. The publisher is not associated with any product or vendor mentioned in this book. This
publication is designed to provide accurate and authoritative information in regard to the subject matter covered. It
is sold on the understanding that the publisher is not engaged in rendering professional services. If professional
advice or other expert assistance is required, the services of a competent professional should be sought.
Library of Congress Cataloging-in-Publication Data
Carrasco, Rolando Antonio.
Non-binary error control coding for wireless communication and data storage / Rolando Antonio Carrasco,
Martin Johnston.
p. cm.
Includes bibliographical references and index.
ISBN 978-0-470-51819-9 (cloth)
1. Error-correcting codes (Information theory)
2. Wireless communicaton systems.
3. Data transmission
systems.
4. Computer storage devices.
5. Analog electronic systems.
I. Johnston, Martin, 1977‚Äì
II. Title.
TK5102.96.C37 2008
005.7 2‚Äìdc22
2008029104
A catalogue record for this book is available from the British Library.
ISBN 978-0-470-51819-9 (HB)
Typeset in 11/13pt Times by Aptara Inc., New Delhi, India.
Printed in Singapore by Markono Print Media Pte Ltd.

My wife Gladys, my daughters Gladys and Carolina, my grand children
Charles, Serena and Sophie, family and friends
Rolando Carrasco
My parents, grand parents, family and friends
Martin Johnston


Contents
Biographies
xiii
Preface
xv
Acknowledgements
xvii
1
Information, Channel Capacity and Channel Modelling
1
1.1 Introduction
1
1.1.1 Information Theory [1]
2
1.1.2 DeÔ¨Ånition of Information [1, 2]
2
1.2 Measure of Information [1‚Äì3]
2
1.2.1 Average Information
5
1.2.2 The Entropy of a Binary Source
6
1.2.3 Mutual Information
9
1.3 Channel Capacity
11
1.3.1 Binary Symmetric Input Channel
12
1.3.2 Binary Erasure Channel (BEC)
13
1.3.3 The 4-ary Symmetric Channel [1, 5]
15
1.3.4 Binary Input Capacity of a Continuous Channel (Channel Capacity)
17
1.3.5 Channel Capacity in Fading Environments
20
1.4 Channel Modelling
22
1.4.1 ClassiÔ¨Åcation of Models
22
1.5 DeÔ¨Ånition of a Communications Channel and its Parameters
24
1.5.1 Doppler Shift
26
1.5.2 Scattering
28
1.5.3 Angle of Arrival and Spectra
30
1.5.4 Multipath Channel and Tapped Delay Line Model
31
1.5.5 Delay Spread
33
1.5.6 The Fixed Broadband Wireless Access Channel
34
1.5.7 UMTS Mobile Wireless Channel Model
35
1.5.8 Simulating the Fixed Broadband Wireless Access
Channel Model
37
1.5.9 Simulating the UMTS Mobile Radio Channel
38
1.6 (Multiple-Input‚ÄìMultiple-Output) (MIMO) Channel
40

viii
CONTENTS
1.7 Magnetic Storage Channel Modelling
43
1.7.1 Longitudinal Recording
44
1.7.2 (Partial Response Maximum Likelihood) (PRML) Detection
45
1.8 Conclusions
48
References
48
2
Basic Principles of Non-Binary Codes
51
2.1 Introduction to Algebraic Concepts
51
2.1.1 Groups
51
2.1.2 Rings
52
2.1.3 Ring of Polynomials
53
2.1.4 Fields
54
2.1.5 Primitive Polynomials
56
2.1.6 Minimal Polynomials and Cyclotomic Cosets
58
2.1.7 SubÔ¨Åelds
59
2.2 Algebraic Geometry Principles
60
2.2.1 Projective and AfÔ¨Åne Space
60
2.2.2 Projective and AfÔ¨Åne Curves
60
2.2.3 Finding Points on an AfÔ¨Åne Curve
61
2.2.4 Rational Functions on Curves
64
2.2.5 Riemann‚ÄìRoch Theorem
67
2.2.6 The Zero Order of a Monomial
68
2.2.7 AG Code Parameters
70
2.3 Conclusions
71
References
71
3
Non-Binary Block Codes
73
3.1 Introduction
73
3.2 Fundamentals of Block Codes
74
3.2.1 Generator and Parity Check Matrices
77
3.2.2 Decoding Block Codes
79
3.3 Cyclic Codes
80
3.3.1 Polynomials
80
3.3.2 Systematic Cyclic Codes
82
3.4 Bose‚ÄìChaudhuri‚ÄìHocquenghem (BCH) Codes
84
3.5 Reed‚ÄìSolomon Codes
86
3.6 Decoding Reed‚ÄìSolomon Codes
88
3.6.1 Euclid‚Äôs Algorithm
89
3.6.2 Berlekamp‚ÄìMassey‚Äôs Algorithm
93
3.6.3 Determining the Error Magnitudes Using Forney‚Äôs Formula
95
3.7 Coded Modulation
96
3.7.1 Block Coded Modulation (BCM) Codes
96
3.7.2 Multi-Level Block Coding
97

CONTENTS
ix
3.7.3 Set Partitioning
99
3.7.4 Construction of a Block Code Trellis
100
3.7.5 Non-Binary BCM Codes
103
3.8 Conclusions
106
References
106
4
Algebraic‚ÄìGeometric Codes
109
4.1 Introduction
109
4.2 Construction of Algebraic‚ÄìGeometric Codes
110
4.2.1 SimpliÔ¨Åed Construction of AG Codes
113
4.2.2 Comparison of AG Codes with Reed‚ÄìSolomon Codes
115
4.2.3 Systematic Algebraic‚ÄìGeometric Codes
115
4.3 Decoding Algebraic‚ÄìGeometric Codes
119
4.3.1 ModiÔ¨Åcation of the Sets F and G
123
4.4 Majority Voting
125
4.5 Calculating the Error Magnitudes
126
4.6 Complete Hard-Decision Decoding Algorithm for
Hermitian Codes
128
4.7 Simulation Results
140
4.8 Conclusions
144
References
144
5
List Decoding
147
5.1 Introduction
147
5.2 List Decoding of Reed‚ÄìSolomon Codes Using the
Guruswami‚ÄìSudan Algorithm
148
5.2.1 Weighted Degrees and Lexicographic Orders
148
5.2.2 Interpolation Theorem
150
5.2.3 Iterative Polynomial Construction
152
5.2.4 Complexity Reduced ModiÔ¨Åcation
154
5.2.5 Factorization
159
5.2.6 Recursive CoefÔ¨Åcient Search
160
5.3 Soft-Decision List Decoding of Reed‚ÄìSolomon Codes Using
the K¬®otter‚ÄìVardy Algorithm
164
5.3.1 Mapping Reliability Values into Multiplicity Values
164
5.3.2 Solution Analysis for Soft-Decision
List Decoding
168
5.3.3 Simulation Results
171
5.4 List Decoding of Algebraic‚ÄìGeometric Codes
171
5.5 Determining the Corresponding CoefÔ¨Åcients
178
5.6 Complexity Reduction Interpolation
181
5.7 General Factorization
187

x
CONTENTS
5.8 Soft-Decision List Decoding of Hermitian Codes
192
5.8.1 System Solution
193
5.8.2 Simulation Results
199
5.9 Conclusions
199
References
200
6
Non-Binary Low-Density Parity Check Codes
201
6.1 Introduction
201
6.2 Construction of Binary LDPC Codes ‚Äì Random and Structured
Methods
201
6.2.1 Tanner Graph Representation
202
6.2.2 Structured Construction Methods
203
6.2.3 LDPC Codes from Class 1 Bose BIBD
204
6.2.4 Constructing the Generator Matrix of a Binary LDPC Code
206
6.3 Decoding of Binary LDPC Codes Using the Belief Propagation
Algorithm
208
6.3.1 Initialization of the Belief Propagation Algorithm
208
6.3.2 The Horizontal Step
210
6.3.3 The Vertical Step
211
6.3.4 Reducing the Decoding Complexity Using Fast Fourier
Transforms
216
6.4 Construction of Non-Binary LDPC Codes DeÔ¨Åned Over
Finite Fields
218
6.4.1 Construction of Non-Binary LDPC Codes from
Reed‚ÄìSolomon Codes
219
6.5 Decoding Non-Binary LDPC Codes with the Sum‚ÄìProduct
Algorithm
221
6.5.1 Received Symbol Likelihoods
222
6.5.2 Permutation of Likelihoods
223
6.5.3 Factor Graph of a Non-Binary LDPC Code
224
6.5.4 The Fast Fourier Transform for the Decoding of Non-Binary
LDPC Codes
224
6.6 Conclusions
234
References
235
7
Non-Binary Convolutional Codes
237
7.1 Introduction
237
7.1.1 The Viterbi Algorithm
243
7.1.2 Trellis Coded Modulation
243
7.1.3 TCM Based on Rings of Integers
246
7.1.4 Ring-TCM Codes for M-PSK
247
7.1.5 Ring-TCM Codes Using Quadrature Amplitude Modulation
250
7.1.6 Searching for Good Ring-TCM Codes
251

CONTENTS
xi
7.1.7
Genetic Algorithm
253
7.1.8
Performance of Ring-TCM Codes on Urban Fading Channels
261
7.2 Space-Time Coding Modulation
262
7.2.1
Introduction
262
7.2.2
Space-Time Codes Model
264
7.2.3
Performance of ST-RTCM Codes Using QPSK for MIMO
Urban Environments
266
7.2.4
Ideal CSI and Slow Fading
267
7.2.5
Log-Likelihood Function
268
7.2.6
Maximum-Likelihood Sequence Detection
269
7.2.7
ST Codes Pairwise Error Probability
269
7.2.8
ST Code Design Criteria for Slow Fading Channels
273
7.2.9
ST Code Design Criteria for Fast Fading Channels
274
7.2.10 Space-Time Ring-TCM Codes for QAM in Fading Channels
274
7.3 Conclusions
278
References
278
8
Non-Binary Turbo Codes
281
8.1 Introduction
281
8.2 The Turbo Encoder
281
8.2.1
Notation
282
8.3 The Turbo Decoder
284
8.3.1
Log-Likelihood Ratios (LLRs)
285
8.3.2
Maximum A Posteriori (MAP) Decoding
287
8.3.3
Max-Log MAP
290
8.3.4
Log MAP Algorithm
292
8.3.5
Iterative Log MAP Decoding
292
8.4 Non-Binary Turbo Codes
294
8.4.1
Multi-Dimensional Log-Likelihood Ratios
296
8.4.2
Non-Binary Iterative Turbo Decoding
297
8.5 Conclusions
298
References
299
Index
301


Biographies
Rolando A. Carrasco was born in 1945 in Santiago, Chile. He obtained his Bachelors
BEng(Hons) degree in Electrical Engineering from Santiago University, Chile in
1969 and his PhD in Signal Processing from the University of Newcastle Upon
Tyne in 1980. He was awarded the IEE Heaviside Premium in 1982 for his work
in multiprocessor systems. Between 1982 and 1984 he was employed by Alfred
Peters Limited, ShefÔ¨Åeld (now Meditech) and carried out research and development
in signal processing associated with cochlear stimulation and response. He was with
Staffordshire University from 1984, and joined Newcastle University in 2004 as
Professor of Mobile Communications. His principle research interests are the digital
signal processing algorithm for data communication systems, mobile and network
communication systems, speech recognition and processing. Professor Carrasco has
over 200 scientiÔ¨Åc publications, Ô¨Åve chapters in telecommunications reference texts
and a patent to his name. He has supervised 40 successful PhD students. He has
acted as principle investigator of several EPSRC projects, a BT research project and
Teaching Company schemes. He has been the local chairman on several international
conference organizing committees. He is a member of several organizing committees,
a member of the EPSRC College and a member of the EPSRC assessment panel. He
also has international collaborations with Chilean, Spanish and Chinese universities.
Professor Carrasco is a fellow of the Institution of Engineers and Technology (IET).
Martin Johnston was born in Wordsley, England in 1977. He received his BSc(Hons)
degree in Physics with Electronics from the University of Birmingham in 1999, his
MSc degree in Electronic Engineering from Staffordshire University in 2001 and was
awarded his PhD in the Design and Construction of Algebraic‚ÄìGeometric Codes from
Newcastle University in 2006. His research interests include the design of non-binary
error-correcting codes for wireless and data storage applications and low-complexity
decoding algorithms. He is currently a Research Associate at Newcastle University
investigating the design of new error-correction coding schemes for high-density
magnetic storage channels. Dr Johnston is a member of the Institute of Engineering
and Technology (IET).


Preface
With the increasing importance of digital communications and data storage, there is a
need for research in the area of coding theory and channel modelling to design codes
for channels that are power limited and/or bandwidth limited. Typical examples of
such channels are found in cellular communications (GSM, UMTS), Ô¨Åxed and mobile
broadband wireless access (WiMax) and magnetic storage media.
Our motivation to write this book is twofold. Firstly, it is intended to provide PhD
students and researchers in engineering with a sound background in binary and non-
binary error-correcting codes, covering different classes of block and convolutional
codes, band-width efÔ¨Åcient coded modulation techniques and spatial-temporal diver-
sity. Secondly, it is also intended to be suitable as a reference text for postgraduate
students enrolled on Master‚Äôs-level degree courses and projects in channel coding.
Chapter 1 of this book introduces the fundamentals of information theory and con-
cepts, followed by an explanation of the properties of fading channels and descriptions
of different channel models for Ô¨Åxed wireless access, mobile communications systems
and magnetic storage.
In Chapter 2, basic mathematical concepts are presented in order to explain non-
binary error-correction coding techniques, such as Groups, Rings and Fields and their
properties, in order to understand the construction of ring trellis coded modulation
(ring-TCM), ring block coded modulation (ring-BCM), Reed‚ÄìSolomon codes and
algebraic‚Äìgeometric codes.
Binary and non-binary block codes and their application to wireless communica-
tions and data storage are discussed in Chapter 3, covering the construction of binary
and non-binary Bose‚ÄìChaudhuri‚ÄìHocquengem (BCH) codes, Reed‚ÄìSolomon codes
and binary and non-binary BCM codes.
Chapter 4 introduces the construction methods of algebraic‚Äìgeometric (AG) codes,
which require an understanding of algebraic geometry. The coding parameters of AG
codes are compared with Reed‚ÄìSolomon codes and their performance and complexity
are evaluated. Simulation results showing the performance of AG and Reed‚ÄìSolomon
codes are presented on fading channels and on magnetic storage channels.
Chapter 5 presents an alternative decoding algorithm known as list decoding, which
is applied to Reed‚ÄìSolomon codes and AG codes. Hard-decision list decoding for
these codes is introduced Ô¨Årst, using the Guruswami‚ÄìSudan algorithm. This is then
followed by soft-decision list decoding, explaining the K¬®otter-Vardy algorithm for

xvi
PREFACE
Reed‚ÄìSolomon codes and modifying it for AG codes. The performance and complex-
ity of the list decoding algorithms for both Reed‚ÄìSolomon codes and AG codes are
evaluated. Simulation results for hard- and soft-decision list decoding of these codes
on AWGN and fading channels are presented and it is shown that coding gains over
conventional hard-decision decoding can be achieved.
A more recent coding scheme known as the low-density parity check (LDPC)
code is introduced in Chapter 6. This is an important class of block code capable
of near-Shannon limit performance, constructed from a sparse parity check matrix.
We begin by explaining the construction and decoding of binary LDPC codes and
extend these principles to non-binary LDPC codes. Finally, the reduction of the
decoding complexity of non-binary LDPC codes using fast Fourier transforms (FFTs)
is explained in detail, with examples.
Chapter 7 begins with an explanation of convolutional codes and shows how they
can be combined with digital modulation to create a class of bandwidth-efÔ¨Åcient codes
called TCM codes. The construction of TCM codes deÔ¨Åned over rings of integers,
known as ring-TCM codes, is explained, and the design of good ring-TCM codes
using a Genetic algorithm is presented. It is then shown how ring-TCM codes can
be combined with spatial-temporal diversity, resulting in space-time ring-TCM (ST-
RTCM) codes, and design criteria are given to construct good ST-RTCM codes for
multiple-input‚Äìmultiple-output (MIMO) fading channels. Simulation results of ST-
RTCM codes are presented for MIMO fading channels, including urban environments
such as indoor, pedestrian and vehicular scenarios.
Finally, Chapter 8 presents another recent coding scheme known as the turbo code.
This important class of code was the Ô¨Årst to achieve near-Shannon limit performance
by using a novel iterative decoding algorithm. Binary turbo encoding and decoding
are explained in this chapter, with a detailed description of the maximum a posteriori
(MAP) algorithm and simpler decoding algorithms derived from the MAP algorithm,
such as the max-log MAP and log MAP algorithms. The chapter Ô¨Ånishes by introduc-
ing non-binary turbo codes, which are a new area of research that have received very
little attention in the literature. Non-binary turbo encoding is explained and the exten-
sion of the binary turbo decoder structure in order to decode non-binary turbo codes
is shown. For further information visit http://www.wiley.com/go/carrasco non-binary

Acknowledgements
We are deeply indebted to the many reviewers who have given their time to read
through this book, in part or in full. We are also very grateful for all the discussions,
support and encouragement we have had from Prof. Paddy Farrell during the writing
of this book. Our thanks go to our colleagues, to the many research assistants and
students over the years, to our families and friends, to Bahram Honary, Mario Blaum,
Mike Darnell, Garik Markarian, Ian Wassell and Fary Ghassemlooy for the useful
discussions at the conferences we participated in. We are grateful for the contributions
of Javier Lopez, Ismael Soto, Marcella Petronio, Alvero Pereira, Li Chen and Vajira
Ganepola to some of the chapters in the book. Finally, we would like to thank Sarah
Hinton and Sarah Tilley at John Wiley and Sons for their guidance in the initial
preparation and the submission of the Ô¨Ånal version of the book.


1
Information, Channel Capacity
and Channel Modelling
1.1 Introduction
In this chapter, an introduction to the fundamental aspects of information theory is
given, with particular attention given to the derivation of the capacity of different
channel models. This is followed by an explanation of the physical properties of
fading channels and descriptions of different channel models for Ô¨Åxed wireless access,
universal mobile telecommunication systems (UMTS) for single-input‚Äìsingle-output
(SISO) and multiple-input‚Äìmultiple-output (MIMO), and Ô¨Ånally magnetic recording
channels. Therefore, this chapter presents the prerequisites for evaluating many binary
and non-binary coding schemes on various channel models.
The purpose of a communication system is, in the broadest sense, the transmission
of information from one point in space and time to another. We shall brieÔ¨Çy explore
the basic ideas of what information is and how it can be measured, and how these
ideas relate to bandwidth, capacity, signal-to-noise ratio, bit error rate and so on.
First, we address three basic questions that arise from the analysis and design of
communication systems:
 Given an information source, how do we determine the ‚Äòrate‚Äô at which the source is
transmitting information?
 For a noisy communication channel, how do we determine the maximum ‚Äòrate‚Äô at
which ‚Äòreliable‚Äô information transmission can take place over the channel?
 How will we develop statistical models that adequately represent the basic properties
of communication channels?
For modelling purposes we will divide communication channels into two cate-
gories: analogue channels and discrete channels. We wish to construct a function that
Non-Binary Error Control Coding for Wireless Communication and Data Storage
Rolando Antonio Carrasco and Martin Johnston
C 2008 John Wiley & Sons, Ltd

2
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
measures the amount of information present in an event that occurs with probability p.
A lower probability means that we obtain more information about the event, whereas
if the event is entirely predictable ( p = 1) then we obtain no information about the
event since we already have knowledge of the event before we receive it.
1.1.1 Information Theory [1]
 How can the symbols of communication be transmitted? Technical or Syntactic
level?
 How precisely do the transmitted symbols carry the desired meaning? Semantic
problem.
 How effectively does the received meaning affect conduct in the desired manner?
1.1.2 DeÔ¨Ånition of Information [1, 2]
Information: Knowledge not precisely known by the recipient, or a measure of unex-
pectedness.
Given an information source, we evaluate the rate at which the source is emitting
information as:
Rate = symbols
second
OR
Rate =
bits
second.
However, given a noisy communication channel, how do we determine the maxi-
mum rate at which reliable information transmission can take place over the channel?
1.2 Measure of Information [1‚Äì3]
We should brieÔ¨Çy explore basic ideas about what information is, how it can be mea-
sured, and how these ideas relate to bandwidth and signal-to-noise ratio. The amount
of information about an event is closely related to its probability of occurrence. Mes-
sages containing knowledge of a high probability of occurrence (i.e. those indicating
very little uncertainty in the outcome) convey relatively little information. In contrast,
those messages containing knowledge of a low probability of occurrence convey rel-
atively large amounts of information. Thus, a measure of the information received
from the knowledge of occurrence of an event is inversely related to the probability
of its occurrence.
Assume an information source transmits one of nine possible messages M1, M2,
. . ., M9 with probability of occurrence P1, P2, . . ., P9, where P1 + P2 + ¬∑ ¬∑ ¬∑ + P9 = 1,
as shown in Figure 1.1.
According to our intuition, the information content or the amount of information
in the ith message, denoted by I(Mi), must be inversely related to Pi. Also, to satisfy

MEASURE OF INFORMATION [1‚Äì3]
3
M1
P1
M2
P2
M3
P3
M4
P4
Mq
Pq
Message
Probability
Figure 1.1
Messages and their associated probabilities.
our intuitive concept of information, I(Mi) must satisfy:
I(Mi) > I(M j) if Pi < Pj
I(Mi) ‚Üí0, Pi ‚Üí1
I(Mi) ‚â•0 when 0 ‚â§Pi ‚â§1.
We can explain the concept of independent messages transmitting from the same
source. For example, the received message ‚ÄòIt will be cold today and hot tomorrow‚Äô
is the same as the sum of information received in the two messages ‚ÄòIt will be cold
today‚Äô and ‚ÄòIt will be hot tomorrow‚Äô (assuming that the weather today does not affect
the weather tomorrow).
Mathematically, we can write this as:
I(Mi and M j)
= I(Mi, M j) = I(Mi) + I(M j),
where Mi and Mj are the two independent messages. We can deÔ¨Åne a measure of
information as the logarithmic function:
I(Mi) = logx
 1
Pi

,
where x is the base 2, e, 10, . . .
(1.1)
The base x for the logarithm in (1.1) determines the unit assigned to the information
content:
x = e
nats
x = 2
bits
x = 10
Hartley,
P1 + P2 + P3 + ¬∑ ¬∑ ¬∑ + Pq = 1 where Pq is the probability of the message occurring
and q is the index value, that is 1, 2 . . .
The information content or the amount of information I(Mk) in the kth message
with the set kth probability (Pk) boundary values is:
1. I(Mk) ‚Üí0
as
Pk ‚Üí1.
Obviously, if we are absolutely certain of the outcome of an event, even before it
occurred.

4
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
2. I(Mk) ‚â•0
when
0 ‚â§Pk ‚â§1.
That is to say, the occurrence of an event M = Mk either provides some or no
information content.
3. I(Mk)>I(Mi)
if
Pk<Pi.
The less probable an event is, the more information we gain when it
occurs.
4. I(Mk Mi) = I(Mk) + I(Mi).
If I(Mk) and I(Mi) are statistically independent messages.
5. Mathematically we can prove that:
6. I(Mk and M j) = I(Mk) + I(M j),where Mk and Mj are two statistically indepen-
dent messages. The logarithmic measure is convenient.
Example 1.1: A source puts out Ô¨Åve possible messages. The probabilities of these
messages are:
P1 = 1
2
P2 = 1
4
P3 = 1
8
P4 = 1
16
P5 = 1
16.
Determine the information contained in each of these messages.
Solution:
I(M1) = log2
Ô£´
Ô£≠
1
1
2

Ô£∂
Ô£∏= 1 bit
I(M2) = log2
Ô£´
Ô£≠
1
1
4

Ô£∂
Ô£∏= 2 bits
I(M3) = log2
Ô£´
Ô£≠
1
1
8

Ô£∂
Ô£∏= 3 bits.
I(M4) = log2
Ô£´
Ô£≠
1
1
16

Ô£∂
Ô£∏= 4 bits
I(M5) = log2
Ô£´
Ô£≠
1
1
16

Ô£∂
Ô£∏= 4 bits
Information total = 14 bits.

MEASURE OF INFORMATION [1‚Äì3]
5
1.2.1 Average Information
Text messages produced by an information source consist of sequences of symbols
but the receiver of a message may interpret the entire message as a single unit. When
we attempt to deÔ¨Åne the information content of symbols, we are required to deÔ¨Åne
the average information content of symbols in a long message, and the statistical
dependence of symbols in a message sequence will change the average information
content of symbols.
Let a transmitter unit consist of U possible symbols, s1, s2, . . . , su, in a statistically
independent sequence. The possibility of occurrence of a particular symbol during a
symbol time does not depend on the symbol transmitted by the source previous in
time. Let P1, P2, . . . , Pu be the probability of occurrence of the U symbols, in a long
message having N symbols. The symbol s1 will occur on average P1N times. The
symbol s2 will occur P2N times, and the symbol si will occur PiN times. Assuming
an individual symbol s to be a message of length 1, we can deÔ¨Åne the information
content of the ith symbol as
log2( 1
Pi ) bits. The PiN occurrence of si contributes
an information content of Pi N log2( 1
Pi ) bits.
The total information content of message is then the sum of each of the U symbols
of the source:
Itotal =
U
 
i=1
NPi log2
 1
Pi

bits.
(1.2)
The average information per symbol is measured by dividing the total information
by the number of symbols N in the message:
H = Itotal
N
=
U
 
i=1
Pi log2
 1
Pi

bits/symbol.
(1.3)
The average information H is called the source entropy (bits/symbol).
Example 1.2: Determine the entropy of a source that emits one of three symbols,
A, B, C, in a statistically independent sequence, with a probability of 1
2, 1
4 and 1
4,
respectively.
Solution: The information contents of the symbols are:
one bit for A
two bit for B
two bit for C
H = 1
2 log2
Ô£´
Ô£¨Ô£¨Ô£≠
1
1
2

Ô£∂
Ô£∑Ô£∑Ô£∏+ 1
4 log2
Ô£´
Ô£¨Ô£¨Ô£≠
1
1
4

Ô£∂
Ô£∑Ô£∑Ô£∏+ 1
4 log2
Ô£´
Ô£¨Ô£¨Ô£≠
1
1
4

Ô£∂
Ô£∑Ô£∑Ô£∏.

6
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
So the average information content per symbol on the source entropy is:
H = 1
2 log2(2) + 1
4 log2(4) + 1
4 log2(4).
H = 1.5 bits/symbols
If we have a Ô¨Åxed time, say rs symbols/s, then by deÔ¨Ånition the average source
of information rate R in bits per second is the product of the average information
content per symbol and the symbol rate rs, as shown below:
R = rs ¬∑ H bits/s.
1.2.2 The Entropy of a Binary Source
For ‚Äòunbiased‚Äô results, the probability of logic 0 or logic 1 is:
P(1) = P(0) = 0.5
Entropy = H = ‚àí
M
 
i=1
Pi log2 Pi
H = ‚àíP1 log2 (P1) ‚àíP2 log2 (P2)
H = ‚àíP1 log2
1
2

‚àíP2 log2
1
2

H = 0.5 + 0.5 = 1.
This agrees with expectation. Now consider a system P1 = P2. We note that, for a
two-state system, since P1 + P2 = 1 we have P2 = 1 ‚àíP1. We can then write:
H = P1 log2
 1
P1

+ (1 ‚àíP1) log2

1
1 ‚àíP1

bits.
(1.4)
Table 1.1 shows how the entropy H varies for different values of P1 and P2.
Table 1.1
The variation of entropy for different values of P1 and P2.
P1
P2 = 1 ‚àíP1
P1 log2

1
P1

(1 ‚àíP1) log2

1
1‚àíP1

H
0
1
0
0
0
0.2
0.8
0.46
0.25
0.72
0.4
0.6
0.528
0.44
0.97
0.5
0.5
0.5
0.5
1
0.6
0.4
0.44
0.528
0.97
0.8
0.2
0.25
0.46
0.72
1
0
0
0
0

MEASURE OF INFORMATION [1‚Äì3]
7
H(x), bits
1
0
1
0.5
Prob p
0.5
Figure 1.2
Entropy function of a binary source.
From Table 1.1 the following observations can be made:
The entropy H(x) attains its maximum value, Hmax = 1 bit, when P1 = P2 = 1/2;
that is, symbols 1 and 0 are equally probable.
When P1 = 0 or 1, the entropy value of H(x) = 0, resulting in no information.
The entropy is plotted in Figure 1.2.
The next two examples show how to determine the entropy functions for an event
with a uniform and Gaussian probability distribution.
Example 1.3:
Determine the entropy of an event x with a uniform probability
distribution deÔ¨Åned by:
P(x) =
1
2.X0
, ‚àíX0 < x < X0
P(x) = 0,
elsewhere
The uniform distribution is illustrated in Figure 1.3.
P(x)
1/2X0
0
‚ÄìX0
X0
x
Figure 1.3
A uniform distribution.
Determine the entropy sources [1, 3]:
H(x) = ‚àí
 
x
P(x) log P(x) dx

8
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
H(x) =
X0
 
‚àíX0
1
2X0
log (2X0) dx
H(x) = log (2X0)
2X0
X0
 
‚àíX0
dx
.
H(x) = log (2X0)
2X0
(X0 ‚àí(‚àíX0))
H(x) = log (2X0)
2X0
(2X0) = log (2X0)
Example 1.4:
Determine the entropy of an event x with a Gaussian probability
distribution deÔ¨Åned by:
P(x) =
1
‚àö
2œÄœÉ
e‚àíx2
2œÉ2
where œÉ is the standard deviation.
The distribution is normalized so that the area under the pdf is unity
‚àû
 
‚àí‚àû
P(x) dx = 1.
The variance œÉ 2 of the distribution is given as:
‚àû
 
‚àí‚àû
x2P(x) dx = œÉ 2.
Therefore, the entropy function is:
H(x) =
‚àû
 
‚àí‚àû
1
œÉ
‚àö
2œÄ
e‚àíx2
2œÉ2
 x2
2œÉ 2 + ln œÉ
‚àö
2œÄ

dx
H(x) = 1
2 + ln

œÉ
‚àö
2œÄ

.
H(x) = ln e
1
2 + ln

œÉ
‚àö
2œÄ

H(x) = ln

œÉ
‚àö
2œÄe

nats or H(x) = log2

œÉ
‚àö
2œÄe

bits

MEASURE OF INFORMATION [1‚Äì3]
9
1.2.3 Mutual Information
Given that we think of the channel output y as a noisy version of the channel input x
value, and the entropy H is a measure of the prior uncertainty about x, how can we
measure the uncertainty about x after observing the y value?
The conditional entropy of x is deÔ¨Åned as [4]:
H(X|Y = yk) =
 
i
P (xi|yk) log2

1
P (xi|yk)

,
k = 0, 1, 2, . . . , i = 0, 1, 2, . . .
(1.5)
This quantity is itself a random variable that takes on values:
H(X|y = y0), . . . , H(Y|y = yk‚àí1)
with probabilities of:
P(y0), . . . , P(yk‚àí1) respectively.
So the mean entropy is:
H =
 
k H (X|y = yk) P (yk)
H =
 
k
 
i P (xi|yk) P (yk) log2

1
P (xi|yk)

.
Or
H =
 
k
 
i P (xi, yk) log2

1
P (xi|yk)

(1.6)
This quantity H or H (X|Y) is called conditional entropy. It represents the amount
of uncertainty remaining about the input after the channel output has been observed.
Since the entropy H(X) represents our uncertainty about the channel input before
observing the channel output, it follows that the difference is:
H(X) ‚àíH(X|Y).
This must represent our uncertainty about the channel input, which is resolved by
observing the channel output. This important quantity is called the mutual information
of the channel. Denoting the mutual information by I (X, Y), we may thus write:
I(X, Y) = H(X) ‚àíH(X|Y).
(1.7)
Similarly:
I(Y, X) = H(Y) ‚àíH(Y|X),
(1.8)
where H(Y) is the entropy of the channel output and H(Y|X) is the conditional entropy
of the channel output given the channel input.

10
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
)
(x, y
I
)
(x
H
)
(y
H
)
(x, y
H
Figure 1.4
A Venn diagram illustrating the relationship between mutual information and entropy.
Property 1
The mutual information of a channel is symmetric, that is:
I(x, y) = I(y, x).
Property 2
The mutual information is always nonnegative, that is:
I(x, y) ‚â•0.
Property 3
The mutual information of a channel is related to the joint entropy of
the channel input and output by:
I(x, y) = H(x) + H(y) ‚àíH(x, y)
and is shown in Figure 1.4.
Example 1.5: Determine the mutual information of the binary symmetric channel
in Figure 1.5 where the associated probabilities are P(x = 1) = 0.7, P(x = 0) =
0.3, P(y = 1|x = 1) = 0.8, P(y = 1|x = 0) = 0.2, P(y = 0|x = 1) = 0.2 and
P(y = 0|x = 0) = 0.8.
Solution:
H(x) = P (x = 0) log2
1
P(x = 0) + P(x = 1) log2
1
P(x = 1)
H(x) = 0.3 √ó log2
1
0.3 + 0.7 √ó log2
1
0.7
.
H(x) = log2 10 ‚àí

0.3 √ó log2 3 + 0.7 √ó log2 7

H(x) = 0.881 bits
So, the conditional entropy of x:
H(x|y = 0) = 24
38 log2
38
24 + 14
38 log2
38
24

CHANNEL CAPACITY
11
H(x|y = 1) = 3
31 log2
31
2 + 28
31 log2
31
28
H(x|y) = 0.38 H(x|y = 0) + 0.62 H(x|y = 1)
= P(y = 0)H (x|y = 0) + P(y = 1)H(x|y = 1)
= 0.645.
Therefore the mutual information of the binary symmetric channel shown in
Figure 1.5 is:
I(x, y) = H(x) ‚àíH(X|y) = 0.881 ‚àí0.645 = 0.236 bits/symbol.
1.3 Channel Capacity
Successful electrical/optical communication systems depend on how accurately the
receiver can determine the transmitted signal. Perfect identiÔ¨Åcation could be possi-
ble in the absence of noise, but noise is always present in communication systems.
The presence of noise superimposed on signal limits the receiver ability to correctly
identify the intended signal and thereby limits the rate of information transmission.
The term ‚Äònoise‚Äô is used in electrical communication systems to refer to unwanted
electrical signals that accompany the message signals. These unwanted signals arise
from a variety of sources and can be classiÔ¨Åed as man-made or naturally occurring.
Man-made noise includes such things as electromagnetic pickup of other radiating
signals. Natural noise-producing phenomena include atmospheric disturbances, ex-
traterrestrial radiation and internal circuit noise.
DeÔ¨Ånition: The capacity C of the channel is the maximum mutual information, taken
over all input distribution of x. In symbols [3, 4]:
C = max
P(xi) I(x, y)
where
P(0) = P(1) = 1
2.
(1.9)
The units of the capacity C are bits per channel input symbols (bits/s).
1 
0.7 
0.8 
1
0
0
0.2 
0.2 
0.8 
Transmitter x
Receiver y
Figure 1.5
Binary symmetric channel of Example 1.5.

12
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
0
1 ‚Äì p
1 ‚Äì p
0
1
1
X
p
p
Y
Figure 1.6
A binary symmetric channel.
1.3.1 Binary Symmetric Input Channel
Figure 1.6 shows the binary symmetric channel (BSC).
If we assume a uniform input distribution P(x = 0) = P(x = 1) = 1/2 then H(x) = 1.
Furthermore, we have the set of transition probabilities [5]:
P(y = 0 | x = 0) = P(y = 1 | x = 1) = 1 ‚àíp
P(y = 0 | x = 1) = P(y = 1 | x = 0) = p.
The conditional entropy is given by:
H(x | y) = ‚àí
 
k
 
i
P(yi | xk)P(xk) log2 P(xk | yi)

H(x | y) = ‚àí
1
2(1 ‚àíp) log2 P (x = 0 | y = 0) + 1
2 p log2 P (x = 0 | y = 1)
+1
2 p log2 P (x = 1 | y = 0) + 1
2 (1 ‚àíp) log2 P (x = 1 | y = 1)

.
The distribution of y is determined as follows:
P(y = 0) = P(y = 0 | x = 0)P(x = 0) + P(y = 1 | x = 1)P(x = 1)
‚à¥P(y = 0) = 1
2(1 ‚àíp) + 1
2 p = 1
2
,
and P(y = 1) is:
P(y = 1) = P(y = 1 | x = 0)P(x = 0) + P(y = 1 | x = 1)P(x = 1)
= 1
2
.
The result is not surprising since the channel is symmetric. The joint distribution is
determined as follows:
P(x = 0 | y = 0) = P(x = 0, y = 0)
P(y = 0)
= P(y = 0 | x = 0)p(x = 0)
P(y = 0)
P(x = 0 | y = 0) = P(y = 0 | x = 0) = 1 ‚àíp
.
P(x = 0 | y = 1) = P(x = 1 | y = 0) = p

CHANNEL CAPACITY
13
0
0.2
0.4
0.6
0.8
1
1.2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
p
I(x,y)
Figure 1.7
The mutual information of the BSC as a function of symbol error probability.
Thus:
H(x | y) = ‚àí
 1
2(1 ‚àíp) log2(1 ‚àíp) + 1
2 p log2 p + 1
2(1 ‚àíp) log2(1 ‚àíp)

= ‚àíp log2 p ‚àí(1 ‚àíp) log2(1 ‚àíp)
.
I(x, y) = 1 + p log2 p + (1 ‚àíp) log2(1 ‚àíp)
The mutual information as a function of the symbol error probability p is shown in
Figure 1.7.
1.3.2 Binary Erasure Channel (BEC)
The BEC output alphabets are 0 or 1, plus an additional element, denoted as e, called
the erasure. This channel corresponds to data loss. Each input bit is either transmitted
correctly with probability 1 ‚àíp or is erased with probability p. The BEC is shown in
Figure 1.8.
The channel probabilities are given by [1, 5]:
P(y = 0 | x = 0) = P(y = 1 | x = 1) = 1 ‚àíp
P(y = e | x = 0) = P(y = e | x = 1) = p.

14
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
0
1 ‚Äì p
1 ‚Äì p
0
1
1
p
p
e
Figure 1.8
The binary erasure channel.
The conditional entropy is given by:
H(x | y) = ‚àí
 
k
 
i
P(xk | yi)P(xk) log2 P(xk | yi)

H(x | y) = ‚àí
1
2(1 ‚àíp) log2 P (x = 0 | y = 0) + 1
2 p log2 P (x = 0 | y = e)
+1
2 p log2 P (x = 1 | y = 1) + 1
2 (1 ‚àíp) log2 P (x = 1 | y = e)

,
H(x | y) = ‚àí1
2(1 ‚àíp) log2 P (x = 0 | y = 0) ‚àí1
2 p log2 P (x = 0 | y = e)
where we used the fact that X is equiprobably distributed. The distribution of y is
determined as follows:
P(y = 0) = P(y = 0/x = 0)P(x = 0) = 1
2 (1 ‚àíp)
and similarly for P(y = e), we have
P(y = 0) = 1
2, P(y = e) = p
and:
P(x = 0 | y = 0) = P(x = 0, y = 0)
P(y = 0)
= P(y = 0 | x = 0)p(x = 0)
P(y = 0)
=
1
2(1 ‚àíp)
1
2(1 ‚àíp)
and similarly:
P(x = 0 | y = 0) = 1
P(x = 0 | y = e) = 1
2.
Thus,
H(x | y) = ‚àí(1 ‚àíp) log2 (1) ‚àíp log2
1
2

= p and
I(x, y) = H(x) ‚àí
H(x | y) = 1 ‚àíp.

CHANNEL CAPACITY
15
0
0.2
0.4
0.6
0.8
1
1.2
1.1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
p
I(x,y)
Figure 1.9
Mutual information of the binary erasure channel.
The mutual information of the binary erasure channel as a function of symbol error
probability is plotted in Figure 1.9.
1.3.3 The 4-ary Symmetric Channel [1, 5]
The 4-ary Symmetric Channel (SC) is shown in Figure 1.10.
Let us consider the case of the 4-SC, where both the input X and the output Y have
four possible values from the alphabet A = [Œ±0 . . . Œ±3]. Since we are sending 4-ary
symbols over the channel, we take the logarithm in equations [H(x), I(x, y)] to the
3p
1 ‚Äì
p
p
p
p
p
p
p
p
p
p
p p
p
p
p
p
p
p
p
p
p
p
p p
3p
1 ‚Äì
3p
1 ‚Äì
3p
1 ‚Äì
3p
1 ‚Äì
3p
1 ‚Äì
3p
1 ‚Äì
3p
1 ‚Äì
0
Œ±
0
Œ±
1
Œ±
2
Œ±
3
Œ±
1
Œ±
2
Œ±
3
Œ±
X
Y
Figure 1.10
The 4-ary symmetric channel.

16
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
base 4. The same holds for the capacity deÔ¨Åned previously. Assume a uniform input
distribution where P(x = a) = 1
4 for all a ‚ààA. Hence, H(x) = 1, and we can set the
transition probabilities:
P(y = a | x = a) = 1 ‚àíp, P(y = a | x = b) = p
3 for a = b, where a, b ‚ààA.
The conditional entropy is given by:
H(x | y) = ‚àí
 
k
 
i
P(yi | xk) P(xk) log4 P(xk | yi)

= ‚àí4
1
4 p log4 P (x = a | y = b) + 1
4 (1 ‚àíp) log4 P (x = a | y = a)

.
For a = b, where a, b ‚ààA, the distribution of y is determined as follows:
P(y = a) = 3P(y = a | x = b)P(x = b) + P(y = a | x = a)P(x = a)
= 3 √ó 1
4
p
3 + 1
4(1 ‚àíp) = 1
4.
The joint distribution is determined as follows:
P(x = a | y = a) = P(x = a, y = a)
P(y = a)
= P(y = a | x = a)P(x = a)
P(y = a)
= 1 ‚àíp.
Similarly:
P(x = a | y = b) = P(x = a, y = b)
P(y = b)
= P(y = b | x = a)P(x = a)
P(y = b)
= p
3 .
For all a, b ‚ààA, where a = b, we have:
H(x | y) = ‚àí4
1
4 p log4
p
3 + 1
4 (1 ‚àíp) log4(1 ‚àíp)

= ‚àíp log4
p
3 ‚àí(1 ‚àíp) log4(1 ‚àíp),
and:
I(x, y) = 1 + p log4
p
3 + (1 ‚àíp) log4(1 ‚àíp).
The mutual information as a function of the symbol error probability p is shown in
Figure 1.11.
Notice that when the capacity is 1, the 4-ary symbol per channel use for p = 0. The
mutual information is zero for p = 3
4.

CHANNEL CAPACITY
17
0
0.2
0.4
0.6
0.8
1
1.2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
p
I(x,y)
Figure 1.11
The mutual information of the 4-ary SC as function of symbol error probability.
1.3.4 Binary Input Capacity of a Continuous Channel (Channel Capacity)
DeÔ¨Ånition: The capacity C of a discrete memoryless channel is deÔ¨Åned as the maxi-
mum mutual information I (x, y) that can be transmitted through the channel:
I(X, Y) = H(Y) ‚àíH(Y | x) = H(x) ‚àíH(x | y)
= H(x) + H(y) ‚àíH(x, y).
The capacity is:
C = max I(x, y)
C = max
  
P(x, y) log2
P(x, y)
P(x)P(y)

.
If P(x) =
1
‚àö
2œÄ Se‚àíx2
2S is a Gaussian distribution for the signal S then P(y) =
1
‚àö2œÄ(S+N)e
y2
2(S+N) is a Gaussian distribution for the signal S and the noise N.
We will make use of the expression for information I(x, y) = H(y) ‚àíH(y | x)
and hence we require to Ô¨Ånd H(y | x). P(y | x) will be Gaussian distributed about a
particular value of x, as seen in Figure 1.12.
P(y | x) =
1
‚àö
2œÄ N
e
(y‚àíx)2
2N .

18
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
Ô£∏
Ô£∂
Ô£≠
Ô£´x
y
p
0
x
Figure 1.12
The Gaussian distribution of the conditional probability P(y | x).
It has been shown that the entropy is independent of the DC value and thus:
H(y) = log2
‚àö
2œÄeN.
Since this is independent of x:
H(y) = log2

2œÄe(S + N)
I(x, y) = log2

2œÄe (S + N) ‚àílog2
‚àö
2œÄeN
= log2

2œÄe (S + N)
2œÄeN
.
= log2

S + N
N
= log2

1 + S
N
This is the information per sample. Thus in time T information is:
Total information = 2BTI
= 2BT log2

1 + S
N = BT log2

1 + S
N

.
Maximum information rate is:
C = B log2

1 + S
N

bits/s,
where B is the bandwidth of the channel. The practical and non-practical channel
capacities of the AWGN channel are shown in Figure 1.13.
1. When the channel is noise free, permitting us to set p = 0, the channel capacity C
attains its maximum value of one bit per channel use.

CHANNEL CAPACITY
19
1.589
Figure 1.13
Channel capacity of the AWGN channel.
2. When the conditional probability of error p =1/2 due to noise, the channel capacity
C attains its minimum value of zero.
S = Average Power (Symbols)
N = Average Noise Power.
Example 1.6: Determine the capacity of a low-pass channel with usual bandwidth
of 3000 Hz and S/N = 10 dB (signal/noise) at the channel output. Assume the
channel noise to be Gaussian and white.
Solution:
C = B log2

1 + S
N

= 3000 log2 (1 + 10) ‚àº= 10 378 bits/s.
Signalling at rates close to capacity is achieved in practice by error correction
coding. An error correction code maps data sequences of k bits to code words of n

20
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
symbols. Because n > k, the code word contains structured redundancy. The code
rate, r = k/n, is a measure of the spectral efÔ¨Åciency of the code. In order to achieve
reliable communications, the code rate cannot exceed the channel capacity (r ‚â§c).
The minimum theoretical signal-to-noise ratio (S/N) required to achieve arbitrarily
reliable communications can be found by rearranging the equation for the capacity of
the AWGN channel [2].
S/N ‚â•1
2r

22r ‚àí1

.
This is the minimum S/N required for any arbitrary distinction for the input signal.
Shannon‚Äôs proof of the channel coding theorem [1, 2] used a random coding argu-
ment. Shannon showed that if one selects a rate r < c codes at random, the bit error
probability approaches zero as the block length n of the code approaches inÔ¨Ånity.
However, random codes are not practically feasible. In order to be able to encode and
decode with reasonable complexity, codes must possess some sort of a structure.
1.3.5 Channel Capacity in Fading Environments
In the case of a single input and single output (SISO) fading channel, the received
signal at the kth symbol instant is y(k) = h(k)x(k) + n(k), where h(k) is the impulse
response of the channel, x(k) is the input signal to the channel and n(k) is additive
white Gaussian noise. To ensure a compatible measure of power, set E(|h(k)|2) = 1
and E{|x(k)|2} ‚â§Es. The capacity in the case of the Ô¨Åxed fading channel with random
but unchanging channel gain is given below [6]:
C = B log2

1 + |h|2 œÅ

,
where œÅ = Es
œÉ 2n
,
(1.10)
where Es is the symbol energy, œÉ 2
n is the n-dimensional variance and h is the gain
provided by the channel. An interesting point to note is that in a random but Ô¨Åxed
fading channel the theoretical capacity can be zero, when the channel gain is close
enough to zero to make data rate impossible. In this case, the possible scenario is
determining what the chances are that a required capacity is available. This is deÔ¨Åned
by Outage probability Pout as the probability that the channel is above a threshold
capacity Cthres given by following equation [6]:
Pout = P(C > Cthres) = P

|h|2 > 2Cthres ‚àí1
œÅ

(1.11)
Pout = 1 ‚àíe‚àí2Cthres‚àí1
œÅ
,
(1.12)
where (1.13) is valid for Rayleigh fading.

CHANNEL CAPACITY
21
In the case of a time-varying channel, the channel is independent from one symbol
to the next and the average capacity of K data symbols is:
CK = 1
K
K
 
k=1

log2

1 + |h|2 œÅ

.
(1.13)
Based on the law of large numbers, as K ‚Üí‚àûthe term on the right converges to
the average or expected value, therefore:
C = Eh

log2

1 + |h|2 œÅ

,
(1.14)
where the expectation is taken over the channel values h. (1.14) is nonzero; therefore
with a Ô¨Çuctuating channel it is possible to guarantee the existence of an error-free data
rate.
Increasing the transmitted and received antenna, the capacity of the channel in-
creases by NT NR-fold, where NT is number of transmit antenna and NR is num-
ber of receive antenna. The capacity of the AWGN channel in case of multiple-
input‚Äìmultiple-output is approximately given by (1.15):
C ‚âàB log2 (1 + NT ¬∑ NR ¬∑ œÅ)
where œÅ = Es/œÉ 2
n .
(1.15)
Figure 1.14 shows the channel capacity with the increase in the number of trans-
mitted and received antennas plotted using (1.15).
0
2
4
6
8
10
12
14
0
5
10
15
20
25
30
Es/N0, dB
Capacity, bits/sec/Hertz
1 Tx, 1Rx
2 Tx, 2 Rx
3 Tx, 3 Rx
4 Tx, 4 Rx
5 Tx, 5 Rx
6 Tx, 6 Rx
7 Tx, 7 Rx
8 Tx, 8 Rx
9 Tx, 9 Rx
10 Tx, 10 Rx
Figure 1.14
Channel capacity in bits/s/Hz with increasing number of transmit and receive antennas.

22
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
1.4 Channel Modelling
This section begins by describing the fundamentals and characteristics of the channel
propagation models used in WiMAX or IEEE 802.16d (Fixed Broadband) systems
and UMTS cellular communications. Propagation models are the fundamental tools
for designing any wireless communication system. A propagation model basically
predicts what will happen to the transmitted signal while in transit to the receiver.
In general, the signal is weakened and distorted in particular ways and the receiver
must be able to accommodate for these changes if the transmitted information is to
be successfully received. The design of the transmitting and receiving equipment and
the type of communication service that is being provided will be affected by these
signal impairments and distortions. The role of propagation modelling is to predict
the performance of the system with these distortions and to determine whether it will
successfully meet its performance goals and service objectives.
The section ends with a description of magnetic storage channel modelling for lon-
gitudinal and perpendicular storage devices. In this situation, information is retrieved
not in a different location from where it was originally sent, as in radio communica-
tions, but at a different time from when it was originally stored.
1.4.1 ClassiÔ¨Åcation of Models
In wireless channels, a narrowband implies that the channel under consideration is
sufÔ¨Åciently narrow that the fading across it is Ô¨Çat (i.e. constant). It is usually used as an
idealizing assumption; no channel has perfectly Ô¨Çat fading, but the analysis of many
aspects of wireless systems is greatly simpliÔ¨Åed if Ô¨Çat fading can be assumed. Early
communication systems were narrowband systems in which median signal level pre-
diction and some description of the signal level variability (fading) statistics were the
only model parameters needed to adequately predict the system performance. How-
ever, modern communications systems use wideband, hence achieving higher data
rates.
In communications, wideband is a relative term used to describe a wide range
of frequencies in a spectrum. A system is typically described as wideband if its
bandwidth is much greater than its centre frequency or carrier frequency. For such
systems, narrowband prediction of signal levels and fading alone does not provide
enough information to predict system performance. In fact the concept of propagation
models is now enlarged to include the entire transfer function of the channel. These
new propagation models, known as channel models, now include parameters such as
signal time dispersion information and Doppler effect distortion arising from motion
of the mobile device. Time dispersion causes the signal fading to vary as a function
of frequency, so wideband channels are often known as frequency-selective fading
channels.
These propagation channel models are broadly classiÔ¨Åed into three main categories:
Theoretical, Empirical and Physical.

CHANNEL MODELLING
23
1.4.1.1 Theoretical Models
The channel models in this group are based on theoretical assumptions about the
propagation environment. The channel parameters based on these model assumptions
do not deÔ¨Åne any particular environment or model any speciÔ¨Åc channel conditions. The
theoretical models cannot be used for planning and developing any communication
systems as they do not reÔ¨Çect the exact propagation medium the signal would be
experiencing. The theoretical models can be nontime dispersive or time dispersive.
Nontime-dispersive channel models are those in which the duration of the transmitted
signal is the same on arriving at the receiving end. However, in time dispersion the
signal extends in time so that the duration of the received signal is greater than that of
the transmitted signal. The theoretical modelling of the time-dispersive channel has
been presented in [6‚Äì8]. The theoretical time-dispersive channel can also be modelled
by the tapped delay line structure, in which densely-spaced taps, multiplying constants
and tap-to-tap correlation coefÔ¨Åcients are determined on the basis of measurements
or some theoretical interpretation of how the propagation environment affects the
signal [7, 8].
1.4.1.2 Empirical Models
Empirical models are those based on observations and measurements alone. These
models are mainly used to predict the path loss, but models that predict rain fade
and multipath have also been proposed [7]. The problem can occur when trying to
use empirical models in locations that are broadly different from the environment in
which the data is measured. For example, the Hata Model [9] is based on the work of
Okumura, in which the propagation path loss is deÔ¨Åned for the urban, suburban and
open environments. But models like Hata and ITU-R are widely used because they
are simple and allow rapid computer calculations.
Empirical models can be subclassiÔ¨Åed in two categories, namely nontime dispersive
and time dispersive, as described. The time dispersive provides information relating
to the time-dispersive characteristics of the channel, that is the multipath delay spread
of the channel. Examples of this type are channel models developed by Stanford
University Interim (SUI) for use in setting up the Ô¨Åxed broadband systems. These
types of channel model are extensively used for WiMAX or IEEE 802.16 system
development [10]. This chapter is mainly concerned with the time-dispersive empirical
channel model (WiMAX speciÔ¨Åcation), which will be explained in detail later on.
1.4.1.3 Physical Models
A channel can be physically modelled by attempting to calculate the physical processes
which modify the transmitted signal. These models rely on basic principles of physics
rather than statistical outcomes from the experiments. Physical channel models are
also known as deterministic models, which often require a complete 3D map of the

24
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
propagation environment. These models are not only divided into nontime dispersive
and time dispersive, but are also modelled with respect to the site speciÔ¨Åcation.
The works published in [6, 8] are examples of site-speciÔ¨Åc, time-dispersive chan-
nel models. These models are principally identiÔ¨Åed as ray-tracing, a high-frequency
approximation approach that traces the route of electromagnetic waves leaving the
transmitter as they interrelate with the objects in the propagation environment. A
deterministic ray-tracing propagation model was used to predict the time delay and
fading characteristics for the channel in a hypothetical urban area. Using this propa-
gation model, the channel response throughout the urban area was described in terms
of the signal level, root-mean-square (RMS) delay spread, and the fading statistics at
each point in the service area. These time-dispersive models provide not only multi-
path delay spread of the channel but also information related to the angle of arrival
(AoA) of the signal.
However, those physical models which do not have signal time delay information are
known as nontime-dispersive channel models. These propagation models are specif-
ically applicable to propagation prediction in the Ô¨Åxed broadband wireless systems.
1.5 DeÔ¨Ånition of a Communications Channel and its Parameters
The main factor affecting the design of a Ô¨Åxed wireless access system is the nature
of the channel available, which affects the behaviour of electromagnetic waves prop-
agating through it. The previous subsection presented the different types of channel
modelling procedures and their subcategories. Before dealing with the speciÔ¨Åcation
and technical parameters of this model, a general background of the channel is pre-
sented along with the deÔ¨Ånitions of parameters like delay spread, path loss, fading,
Doppler effect and so on.
The main processes of a communication system consist of a source, a transmitter,
a channel, a receiver and a destination, as shown in Figure 1.15, where bk are the
message bits, xn are the modulated symbols, rn are received symbols, ÀÜxn are the
demodulated symbols and ÀÜbk are the detected message bits. The transmitter takes
information from the source and converts it into a form suitable for transmission.
The wireless communication channel consists of the medium through which the RF
signal passes when travelling from the transmitting antenna to the receiving antenna.
The medium causes the transmitted signal to be distorted in several ways, as previously
mentioned.
In the absence of a line-of-sight between the transmitting antenna and the receiving
antenna, some of the transmitted signal Ô¨Ånds a path to the receiving antenna by
Figure 1.15
Simple communication system.

DEFINITION OF A COMMUNICATIONS CHANNEL AND ITS PARAMETERS
25
reÔ¨Çecting or refracting from whatever is blocking the direct line-of-sight between
the two antennas. This action is known as a multipath signal scenario as the many
transmitted signals undergo different degrees of dispersion as they traverse multiple
paths to reach the receiving antenna.
Eventually some of the signal paths recombine vectorially at the receiving antenna,
producing a signal the amplitude of which is dependent upon the phases of the
individual component waveforms. This is the concept of signal fading, which is a
purely spatial phenomenon. If a mobile receiving antenna is moving relative to the
environment and/or the transmitting antenna, the incoming phases of the signals
will vary, producing a signal whose amplitude varies with spatial movement of the
mobile relative to the environment. Although fading is a spatial phenomenon, it is
often perceived as a temporal phenomenon as the mobile device moves through the
multipath signal Ô¨Åeld over time.
To determine a channel model, mathematical descriptions of the transmitter, the
receiver and the effect of the environment (walls) on the signal must be known.
A linear channel can be totally described by its impulse response, that is by what
the received signal would look like if the transmitted signal were an impulse. The
impulse response is the response of the channel at all frequencies, that is, once the
impulse response of the channel is known, one can predict the channel response at all
frequencies.
Let x(t) be the signal transmitted from an antenna through the channel h(t), and y(t)
be the signal received at the receiving side. Assuming no delay, multipath signals and
no other noise present in the system, the channel can be considered as a linear system
with x(t) as an input and y(t) as an output. This relationship between the input and
output in the time domain is represented in (1.16) and shown in Figure 1.16.
y (t) = h (t) ‚äóx (t) .
(1.16)
The channel impulse response h(t) is obtained by applying the impulse function to
the channel and can be represented as:
h (t) =
‚àû
 
i=0
Aie jœÜiŒ¥ (t ‚àíœÑi),
(1.17)
where Ai is the magnitude of the impulse response at delay œÑ i with phase œÜi, and
Œ¥(t) is the Dirac delta function [5]. The system in Figure 1.16 is modelled as a linear
Figure 1.16
Channel input-output in time domain.

26
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
time variant Ô¨Ålter with impulse response h (t, œÑ) and frequency response H( f, t). The
frequency response can be obtained by taking the Fourier transform of h(t):
H( f, t) =
 ‚àû
‚àí‚àû
h(t)e‚àíj2œÄ f tdt.
(1.18)
The channel output y(t) at a particular time t = œÑ is the convolution of the impulse
response h(t) with the input signal x(t), that is:
y(t) = x (t) ‚äóh (t) =
‚àû
 
œÑ=0
h(œÑ)x(t ‚àíœÑ),
(1.19)
where ‚äórepresents the convolution operation. The Ô¨Ånal output y(t) can be obtained
by adding independent noise n(t) to (1.19):
y(t) = x (t) ‚äóh (t) + n (t) =
‚àû
 
œÑ=0
h(œÑ)x(t ‚àíœÑ) + n(t).
(1.20)
However, in a wireless environment the transmitted signals are affected in different
ways, for example by the reÔ¨Çection that occurs when the signal hits a surface. At the
receiving side the signal arrives from different paths, which add delay at the receiver,
and it can be affected by moving objects in terms of a Doppler shift.
1.5.1 Doppler Shift
Doppler frequency effects occur whenever there is relative movement between the
transmitting and receiving antennas. It manifests itself as a change in the frequency of
the received signal. For a receiver tuned to a particular frequency, this phenomenon
has the effect of reducing the received signal energy as the receiver‚Äôs front end is
no longer tuned to the centre frequency of the signal and is therefore not operating
efÔ¨Åciently.
The Doppler frequency shift in each propagation path is caused by the rate of
change of signal phase (due to motion). Referring to Figure 1.17 below, if a mobile
device, moving from point A to point B at a velocity v, is receiving a signal from the
signal source, the distance travelled, d, can be found by vt, where t is the change
in time from point A to point B.
From this it can be shown geometrically that the extra distance that the wave has
to travel to get from the signal source to point B(l) with respect to point A is:
d cos (Œ±).
The change in phase of the received signal at point B relative to point A is given
by [8]:
œÜ = ‚àí2œÄ
Œª l = ‚àí2œÄvt
Œª
cos Œ±.
(1.21)

DEFINITION OF A COMMUNICATIONS CHANNEL AND ITS PARAMETERS
27
Signal Source
B
A
‚àÜl
d
Œ±
Direction of
Motion
Figure 1.17
Example of the effects of Doppler shift [8].
With the change in frequency of the received signal at point B relative to point A
given by:
f = ‚àí1
2œÄ
œÜ
t = v
Œª cos Œ±.
(1.22)
it can be seen that the change in path length is governed by the angle between the
direction of motion and the received wave. It should also be noted that when the
mobile antenna is moving closer to the signal source, a positive change in frequency
(Doppler) is caused, and conversely, if the mobile antenna is moving further away
from the signal source it causes a negative change in frequency.
If the mobile antenna is moving in the same plane as the signal source (either to or
from it) then the frequency shift is given by:
f = ¬± v/Œª.
(1.23)
This information is required for modelling the channel using the ray-tracing method,
which calculates all the angles of the received signals in order to calculate the re-
ceived signal strength over time. This method becomes impractical for large numbers
of received signals due to the computational burden. The number of signal reÔ¨Çec-
tions also limits this method, and in practice only signals with two reÔ¨Çections are
considered.
The movement of the receiving antenna relative to the transmitting antenna, coupled
with the large number of reÔ¨Çections and received signal paths, causes the resultant
RF signal envelope at the receiving antenna to appear random in nature. There-
fore, statistical methods must be employed in order to produce a mathematically
tractable model, which produces results in accordance with the observed channel
properties.

28
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
1.5.2 Scattering
At the receiving antenna, the incoming signal consists of a superposition of individual
waves. Each individual wave has the following characteristics: amplitude, frequency,
phase, polarization angle, vertical angle of arrival and horizontal angle of arrival, all
relative to the signal at the transmitter.
If we assume that both the transmitting and receiving antennas are similarly polar-
ized and operating on the same frequency, only the amplitude (An), phase (œÜn), vertical
angle of arrival (Œ≤n) and horizontal angle of arrival (Œ±n) need to be considered. Figure
1.18 shows an individual wave (n) relative to the point of reception.
The values for An, œÜn, Œ≤n and Œ±n are all random and statistically independent. The
mean square value for the amplitude (An) is given by:
E

A2
n

= E0
N ,
(1.24)
where E0 is a positive constant and N is the number of received waves at the point of
reception.
Clarke [7] makes a generalization which assumes that the height of the transmitting
antenna is approximately the same as that of the receiving antenna, so (assuming
Clarke‚Äôs model) the vertical angles of arrival (Œ≤n) can be set to zero. In practice this
is found to be a good approximation.
Y
Z
X
Œ±
Œ≤
Direction of
Received Wave
Figure 1.18
An individual wave relative to the point of reception.

DEFINITION OF A COMMUNICATIONS CHANNEL AND ITS PARAMETERS
29
The phase angles (œÜn) are uniformly distributed within the range of 0 to 2œÄ, but
the probability density functions for the angles of arrival Œ±n and Œ≤n are generally not
speciÔ¨Åed.
At the point of reception, the Ô¨Åeld resulting from the superposition of the incoming
waves (n) is given by:
E(t) =
N
 
n=0
En (t).
(1.25)
If an unmodulated carrier is transmitted, the received signal (En(t)) can be expressed
at the point of reception (x0, y0, z0) as follows [8]:
E(t) = An cos

œâ0t ‚àí2œÄ
Œª (x0 cos(Œ±n) cos(Œ≤n) + y0 sin(Œ±n) cos(Œ≤n)
+ z0 sin(Œ≤n)) + œÜn

.
(1.26)
If the receiving antenna moves in the xy plane at an angle Œ≥ (relative to the x-axis)
with a velocity v, then after a unit time, the coordinates of the received signal can be
expressed as:
v cos Œ≥, v sin Œ≥, z0.
(1.27)
Which means that the received signal E(t) can be expressed as:
E(t) = I (t) cos (œâct) ‚àíQ (t) sin (œâct) ,
(1.28)
where I(t) and Q(t) represent the in-phase and quadrature components of the signal
respectively, and can be expressed as:
I (t) =
N
 
n=1
An cos (œânt + Œ∏n)
(1.29)
and:
Q (t) =
N
 
n=1
An sin (œânt + Œ∏n),
(1.30)
where œân equals 2œÄfn, and fn represents the Doppler shift in frequency experienced
by the individual wave n. The terms œân and Œ∏n can be expressed as:
œân = 2œÄv
Œª
cos (Œ≥ ‚àíŒ±n) cos (Œ≤n)
(1.31)
and:
Œ∏n = 2œÄz0
Œª
sin (Œ≤n) + œÜn.
(1.32)

30
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
The previous equations reduce to Clarke‚Äôs two-dimensional model if Œ≤ is taken to
be zero.
If the number of received waves (N) is very large (in practice 6, but theoretically
much larger), then according to the central limit theorem the components I(t) and Q(t)
are independent Gaussian processes and are completely characterized by their mean
and autocorrelation functions. Because I(t) and Q(t) both have zero mean values, it
follows that E{E(t)} is also zero. Also, I(t) and Q(t) have variance values ( ) that
are the same as the mean square value (average power), thus the probability density
function of I(t) and Q(t) can be expressed as:
px (x) =
1
œÉ
‚àö
2œÄ
exp

‚àíx2
2œÉ 2

,
(1.33)
where x = I(t) or Q(t), and œÉ 2 = E{A2
n} = E0
N .
1.5.3 Angle of Arrival and Spectra
For a system in motion (that is, receiver antenna movement relative to the transmitting
antenna), the individual components of the received signal will experience a change
in frequency due to the Doppler effect, depending upon their individual angles of
arrival. This change in frequency is given by [8]:
fn = œân
2œÄ = v
Œª cos (Œ≥ ‚àíŒ±n) cos (Œ≤n) .
(1.34)
All frequency components within the received signal will experience this Doppler
shift in frequency and, as long as the signal bandwidth is relatively small (compared to
the receive signal path bandwidth), it can be assumed that all the individual component
waves will be affected in the same way.
The RF spectrum of the received signal can be obtained by using the Fourier
transform of the temporal autocorrelation function in terms of the time delay (œÑ):
E{E(t)E(t + œÑ)} = E{I(t)I(t + œÑ)} cos(œâcœÑ) ‚àíE{I(t)Q(t + œÑ)} sin(œâcœÑ)
= a(œÑ) cos(œâcœÑ) ‚àíc(œÑ) sin(œâcœÑ).
(1.35)
Aulin [11] showed that the correlation properties can therefore be expressed by
a(œÑ) and c(œÑ):
a (œÑ) = E0
2 E {cos (œâœÑ)}
c (œÑ) = E0
2 E {sin (œâœÑ)} .
(1.36)

DEFINITION OF A COMMUNICATIONS CHANNEL AND ITS PARAMETERS
31
In order to simplify further we make the assumption that all the signal waves arrive
in the horizontal plane (Œ±) with equal probability, so that:
pŒ± (Œ±) = 1
2œÄ .
(1.37)
Which means that by Fourier transforming the following, the power spectrum can
be obtained:
a (œÑ) = E0
2
 +œÄ
‚àíœÄ
J0 (2œÄ fmœÑ cos Œ≤)pŒ≤ (Œ≤) dŒ≤.
(1.38)
1.5.4 Multipath Channel and Tapped Delay Line Model
The Ô¨Åxed or mobile radio channel is characterized by the multipath propagation. The
signal offered to the receiver contains not only a direct line-of-sight radio wave, but
also a large number of reÔ¨Çected radio waves. This can be even worse in urban areas,
where obstacles often block the line-of-sight and a mobile antenna receives a collection
of various delayed waves. These reÔ¨Çected waves interfere with the direct wave, which
causes signiÔ¨Åcant degradation of the performance of the link. If the mobile user
moves, the channel varies with the location and time, because the relative phase and
amplitude of the reÔ¨Çective wave change. Multipath propagation seriously degrades
the performance of the communication system. The adverse effects produced by
the medium can be reduced by properly characterizing the medium in order to design
the transmitter and receiver to Ô¨Åt the channel. Figure 1.19 shows the signal arriving
at the receiver from different paths, which include the direct line-of-sight (LOS) path
and nonline-of-sight (NLOS) paths.
As the different variants of the same signal arrive at different times, some are delayed
relative to one another. This time dispersion is known as multipath delay spread.
This delay spread is an important parameter in channel modelling and is commonly
measured as root mean square (RMS) delay spread. For reliable communication over
these channels without any interference reduction techniques, the transmitted data rate
should be much smaller than the coherence bandwidth. This type of channel, when
Figure 1.19
Signal arriving at mobile station from different paths.

32
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
Small-scale Fading
delay spread)
(Based on multipath
Flat Fading
Bandwidth of signal < coherence bandwidth 
1.
of the channel
Delay spread < symbol period
2.
Bandwidth of signal > coherence bandwidth 
1.
of the channel
Delay spread > symbol period
2.
Frequency-selective Fading
Figure 1.20
The two classes of fading channels.
the transmitted bandwidth of the signal is much smaller than the coherent bandwidth,
is known as a Ô¨Çat channel or narrowband channel. When the transmitted bandwidth
of the signal is nearly equal to or larger than the coherent bandwidth, the channel
is known as a frequency-selective channel or broadband channel. The relationship
between Ô¨Çat fading and frequency-selective fading is shown in Figure 1.20.
The multipath delay proÔ¨Åle is characterized by œÑ rms, which is deÔ¨Åned as [6]:
œÑ 2
rms =
 
j PjœÑ 2
j ‚àí(œÑavg)2,
(1.39)
where œÑavg = 
j PjœÑ j, œÑ j is the delay of the jth delay component of the proÔ¨Åle and
Pj = (power in the jth delay component)/(total power in all components).
The channel output y(t) of (1.22) can be realized as a tapped delay line, as shown
in Figure 1.21, where Ai(t) is the fading amplitude of the ith tap and Ti are the delays
of the ith tap. Each tap represents a scattered ray multiplied by a time-varying fading
proÔ¨Åle coefÔ¨Åcient Ai(t). The relative tap delays are dependent on the type of channel
T1
T2
T3
T4
T5
T6
T2 ‚Äì T1
T3 ‚Äì T2
T4 ‚Äì T3
T5 ‚Äì T4
T6 ‚Äì T5
Œ£
A1(t)
A2(t)
A3(t)
A4(t)
A5(t)
A6(t)
Output
Input
Discarded
sample
Figure 1.21
An example of a tapped delay line.

DEFINITION OF A COMMUNICATIONS CHANNEL AND ITS PARAMETERS
33
model. These values are given later for the Ô¨Åxed broadband wireless-access channel
model and the urban UMTS mobile radio channel models.
1.5.5 Delay Spread
Because of the multipath reÔ¨Çections, the channel impulse response of a wireless
channel looks like a series of pulses. In practice the number of pulses that can be
distinguished is very large and depends on the time resolution of the communication
or measurement system.
The system evaluations process typically prefers to address a class of channels with
properties that are likely to be encountered, rather than one speciÔ¨Åc impulse response.
Hence it deÔ¨Ånes the (local-mean) average power, which is received with an excess
delay that falls within the interval (T, T + dt). Such characterization for all T gives the
‚Äòdelay proÔ¨Åle‚Äô of the channel. The delay proÔ¨Åle determines the frequency dispersion;
that is, the extent to which the channel fading at two different frequencies f 1 and f 2 is
correlated (Figure 1.22).
The maximum delay time spread is the total time interval during which reÔ¨Çections
with signiÔ¨Åcant energy arrive. The RMS delay spread TRMS is the standard deviation
(or root-mean-square) value of the delay of reÔ¨Çections, weighted proportionally to the
energy in the reÔ¨Çected waves. For a digital signal with high bit rate, this dispersion
is experienced as frequency-selective fading and intersymbol interference (ISI). No
serious ISI is likely to occur if the symbol duration is longer than, say, ten times
the RMS delay spread. The RMS delay spread model published in [14] follows
a lognormal distribution and the median of this distribution grows as a power of
distance. This model was developed for rural, urban and mountainous environments
(Figure 1.23).
œÑrms = T1dey,
(1.40)
Amplitude 
Amplitude 
Figure 1.22
Example of the impulse response and frequency transfer function of a multipath channel.

34
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
Figure 1.23
Expected power per unit of time.
where œÑ rms is the RMS delay spread, d is the distance in km, T1 is the median value
of the œÑrms at d = 1 km, e is an exponent that lies between 0.5 and 1.0, and y is the
lognormal variable.
The narrowband received signal fading can be characterized by a Ricean distri-
bution. The K factor is the ratio between the constant component powers and the
scattered power. The model that represents the K factor by extensive experimental
data taken at 1.9 GHz is given below [12]:
K = Fs Fh FbKod?,
(1.41)
where Fs is a seasonal factor: Fs = 1.0 in summer (leaves) and 2.5 in winter (no leaves);
Fh is the receive antenna height factor, Fh = (h/3)0.46 (h is the receive antenna height);
Fb is the beamwidth factor, Fb = (b/17)‚àí0.62 (b in degrees); Ko and Œ≥ are regression
coefÔ¨Åcients, Ko = 10; Œ≥ = ‚àí0.5; u is a lognormal variable with 0 dB mean and a
standard deviation of 8.0 dB.
1.5.6 The Fixed Broadband Wireless Access Channel
The Doppler spectrum of the Ô¨Åxed wireless system is different from that in the
mobile wireless access systems. In Ô¨Åxed wireless systems the Doppler spectrum of
the scattered component is mainly scattered around f = 0 Hz. The shape of the
spectrum is different from the mobile channels. The spectrum for mobile and Ô¨Åxed
wireless channels is given below [10].
The above Ô¨Åxed access channel power spectrum density (PSD) is calculated by
using the equation given below:
S( f ) =

1 ‚àí1.72 f 2
o + 0.785 f 4
o | fo| ‚â§1
0
| fo| > 1
where, fo =
f
fm
.
(1.42)
Figure 1.24 is based on (1.42) and is the rough approximation of the Doppler PSD,
which has the advantage that it is readily available in most existing RF simulators.
This Doppler spectrum is representative of Ô¨Åxed mobile (point-to-point) wireless

DEFINITION OF A COMMUNICATIONS CHANNEL AND ITS PARAMETERS
35
‚Äì25
‚Äì20
‚Äì15
‚Äì10
‚Äì5
0
1.5
1
0.5
0
‚Äì0.5
‚Äì1
‚Äì1.5
f /f m
PSD, dB
Figure 1.24
Doppler spectrum of a Ô¨Åxed wireless access channel.
channels and so does not represent the Doppler characteristics of a nonstationary
mobile wireless channel.
1.5.7 UMTS Mobile Wireless Channel Model
Three different propagation environments for the UMTS mobile wireless channel
model are considered: Indoor, Pedestrian and Vehicular. These environments were
chosen because they are the main scenarios that cellular mobile radios experience in
normal operation.
The Indoor environment model is characterized by having no Doppler frequency
shift, as the velocities concerned within the indoor environment are well below walk-
ing pace ( 4 miles per hour (MPH)), which produces either zero or a negligible shift
in frequency. Within this environment there is no direct line-of-sight path from trans-
mitter to receiver, so the signal propagates via many different paths as the signals are
reÔ¨Çected, refracted or diffracted. Only six paths are simulated as the power contained
within the strongest six rays are deemed strong enough to be included; the rest are
very low power and thus will not affect the results appreciably. If they were included
they would just serve to slow the simulation down.
The Pedestrian environment model is characterized by having a small Doppler
frequency shift, as the velocities concerned within the pedestrian environment are
around walking pace (‚âà4 MPH), which produces a small Doppler shift in frequency.
Within this environment there is limited/no direct line-of-sight path from transmitter to

36
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
receiver, so the signal propagates via many different paths as the signals are reÔ¨Çected,
refracted or diffracted. Only four paths are simulated as only the power contained
within the strongest four rays are deemed strong enough to be included; the rest are
very low power and thus will not affect the results appreciably.
The Vehicular environment model is characterized by having a larger Doppler
frequency shift as the velocities concerned within the vehicular environment are
reasonably large (‚âà70 MPH), producing a larger Doppler shift in frequency. Within
this environment there is no direct line-of-sight path from transmitter to receiver, so
the signal propagates via many different paths as the signals are reÔ¨Çected, refracted
or diffracted. Only six paths are simulated as only the power contained within the
strongest six rays are deemed strong enough to be included; the rest are very low
power and thus will not affect the results appreciably.
A Doppler spectrum is given in (1.43), which is the classic spectrum for mobile
radio channels, and is used by Jakes [8] and Clarke [7]:
S( f ) =
Ô£±
Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£≥
E0
4œÄ fm
1

1 ‚àí
 f
fm
2
| f | ‚â§fm
0
elsewhere
,
(1.43)
where E0 = energy constant. Although this channel is representative of the mobile
wireless channels, there are problems with this representation as the power spectral
density becomes inÔ¨Ånite at fC ¬± fm. In order to Ô¨Ånd a more realistic (and useable)
representation, Aulin [11] described the following:
S( f ) =
Ô£±
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥
0
‚àÄ| f | > fm
E0
4 sin Œ≤m
1
fm
,
‚àÄfm cos Œ≤m ‚â§| f | ‚â§fm
1
fm
œÄ
2 ‚àíarcsin 2 cos2 Œ≤m ‚àí1 ‚àí( f/ fm)2
1 ‚àí( f/ fm)2

,
‚àÄ| f | < fm cos Œ≤m
.
(1.44)
This claimed to be realistic for small values of Œ≤m (angle of signal arrival) and
is particularly useful in providing analytic solutions. The problem with this model
is that there are sharp discontinuities at ¬±Œ≤m, which causes an unrealistic response.
Therefore a model is required which produces the classic Doppler shape for mobile
wireless channels but has no inÔ¨Ånities or sharp discontinuities. Such a model was
proposed by Parsons [6], and the PDF of the angle of arrival pŒ≤ (Œ≤) is given by:
pŒ≤ (Œ≤) =
Ô£±
Ô£≤
Ô£≥
œÄ
4 |Œ≤m| cos
œÄ
2 ¬∑ Œ≤
Œ≤m

|Œ≤| ‚â§|Œ≤m| ‚â§œÄ
2
0
elsewhere
.
(1.45)

DEFINITION OF A COMMUNICATIONS CHANNEL AND ITS PARAMETERS
37
0
‚Äì10
‚Äì20
‚Äì30
‚Äì40
dB
‚Äìfm
fm
‚Äìfm  cos Œ≤m
fm  cos Œ≤m
Clarke‚Äôs model
Aulin‚Äôs model
Parsons‚Äô model
Figure 1.25
Doppler spectra of a mobile wireless access channel.
Using (1.45), the power spectral density can be expressed as:
S( f ) = FT
 E0
2
 +œÄ
‚àíœÄ
J0 (2œÄ fmœÑ cos Œ≤)pŒ≤ (Œ≤) dŒ≤

,
(1.46)
where FT is the Fourier transform and J0 is the zero-order Bessel function of the Ô¨Årst
kind.
More information on the origin of these equations can be found in [7]. The Doppler
spectra of (1.43), (1.44) and (1.46) are shown in Figure 1.25.
1.5.8 Simulating the Fixed Broadband Wireless Access Channel Model
There are different SUI(Stanford University Interim) models presented in [13]. The
description of the SUI-3 model is explained here, and the model description and
parameters are given in Table 1.2. A three-tap model is described here to represent
the multipath scenario. Doppler PSD of Ô¨Åxed wireless is used to model the real
environment.
Figure 1.26 shows the delay proÔ¨Åle of each tap. The magnitude of the channel
coefÔ¨Åcients of all the taps, generated and plotted versus time, are shown below in
Figure 1.27.

38
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
Table 1.2
SUI-3 channel parameters.
SUI-3 Channel
Tap 1
Tap 2
Tap 3
Units
Delay
0
0.5
1.0
¬µs
Power (Omni ant.)
0
‚àí5
‚àí10
dB
K Factor (Omni ant.)
1
0
0
Power (30‚ó¶ant.)
0
‚àí11
‚àí22
dB
K Factor (30‚ó¶ant.)
3
0
0
Doppler
0.4
0.4
0.4
Hz
Antenna Correlation:
œÅENV = 0.4
Gain Reduction Factor:
GRF = 3 dB
Normalization Factor:
Fomni = ‚àí1.5113 dB, F30 = ‚àí0.3573 dB
1.5.9 Simulating the UMTS Mobile Radio Channel
The parameters for three different UMTS mobile channel models based on the stan-
dards put forward by ETSI, which model an indoor, a pedestrian and a vehicular
environment, are given in this chapter. Tables 1.3‚Äì1.5 give the relative delays between
taps and the average tap power for each scenario [14].
Simulation results evaluating the bit-error rate (BER) performance of QPSK mod-
ulation on the indoor, pedestrian and vehicular UMTS channels and BFWA channel
are presented in Figure 1.28.
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Power Delay Profile for SUI-3 Channel
Delay [us]
Normalized Power of each Tap
Figure 1.26
Delay proÔ¨Åle of SUI-3 channel model.

DEFINITION OF A COMMUNICATIONS CHANNEL AND ITS PARAMETERS
39
0
10
20
30
40
50
60
70
80
90
100
‚àí40
‚àí35
‚àí30
‚àí25
‚àí20
‚àí15
‚àí10
‚àí5
0
5
10
Fading Profile of SUI-3 Channel 
Magnitude of Channel Coefficients [dB]
Time [s]
Channel Tap 1
Channel Tap 2
Channel Tap 3
Figure 1.27
Fading proÔ¨Åle of SUI-3 channel model.
Table 1.3
Channel parameters for the indoor UMTS channel
model.
Tap
Relative delay (ns)
Average power (dB)
1
0
0
2
50
‚àí3.0
3
110
‚àí10.0
4
170
‚àí18.0
5
290
‚àí26.0
6
310
‚àí32.0
Table 1.4
Channel parameters for the pedestrian UMTS channel
model.
Tap
Relative delay (ns)
Average power (dB)
1
0
0
2
110
‚àí9.7
3
190
‚àí19.2
4
410
‚àí22.8

40
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
Table 1.5
Channel parameters for the vehicular UMTS channel
model.
Tap
Relative delay (ns)
Average power (dB)
1
0
0
2
310
‚àí1.0
3
710
‚àí9.0
4
1090
‚àí10.0
5
1730
‚àí15.0
6
2510
‚àí20.0
1.6 (Multiple-Input‚ÄìMultiple-Output) (MIMO) Channel
The performance analysis of the WiMAX system is extended from a SISO model to
the MIMO model. Let us consider a multi-antenna system with NT transmit and NR
receive antennas characterized by input/output in (1.47); the antenna conÔ¨Åguration is
shown in Figure 1.29.
ym (t) =
NT
 
n=1
 ‚àû
‚àí‚àû
hmn (t, œÑ) xn (t ‚àíœÑ) dœÑ,
m = 1, 2, . . . , NR,
(1.47)
where xn(t) is the signal transmitted from the nth transmitted antenna, ym(t) is the
signal received at the mth receive antenna and hmn (t, œÑ) is the impulse response of the
channel between the nth transmitted and mth receive antennas. The above equation
1.E-07
1.E-06
1.E-05
1.E-04
1.E-03
1.E-02
1.E-01
1.E+00
24
22
20
18
16
14
12
10
8
6
4
2
0
SNR (dB)
BER
Indoor
Pedestrian
Vehicular
BFWA
Figure 1.28
BER performance of QPSK modulation on the indoor, pedestrian and vehicular and BFWA
channels.

(MULTIPLE-INPUT‚ÄìMULTIPLE-OUTPUT) (MIMO) CHANNEL
41
Transmitter
Receiver
DATA
IN
DATA
OUT
MIMO
Channel
x(t)
y(t)
H(t,œÑ)
Figure 1.29
General MIMO channel with matrix H.
can be written in matrix form as follows:
y (t) =
 ‚àû
‚àí‚àû
H (t, œÑ) x (t ‚àíœÑ) dœÑ,
(1.48)
where H(t, œÑ) is the NR √ó NT channel matrix and is given as:
H(t, œÑ) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
h11 (t, œÑ)
h12 (t, œÑ)
h13 (t, œÑ) ................... h1NT (t, œÑ)
h21 (t, œÑ)
h22 (t, œÑ)
h23 (t, œÑ) ................... h2NT (t, œÑ)
h31 (t, œÑ)
h32 (t, œÑ)
h33 (t, œÑ) ................... h3NT (t, œÑ)
.
.
.
...................
.
.
.
.
...................
.
hNR1 (t, œÑ) hNR2 (t, œÑ) hNR3 (t, œÑ) ................... hNR NT (t, œÑ)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
, (1.49)
y(t) =
%
y1 (t) , y2 (t) , . . . , yNR (t)
&
is the NR size row vector containing the signals
received from NR antenna and x(t) =
%
x1 (t) , x2 (t) , . . . , xNT (t)
&
is the NT size row
vector containing signals transmitted from NT antennas. The impulse response in the
case of the MIMO channel can be explained using the same criteria as in the SISO
case, but the distinguishing feature of MIMO systems is the spatial correlation among
the impulse response composed of H (t, œÑ).
The channel matrix H deÔ¨Ånes the input-output relations of the MIMO system
and is known as the channel transfer function. Let us consider the two-transmit and
two-receive antennae conÔ¨Åguration, as shown in Figure 1.30. The system model
comprises two transmitting and two receiving antennae and the medium between
them is modelled as the MIMO channel.
The antenna correlations among the different paths are calculated and shown in
Table 1.6, showing small correlations between the paths of the same channel. Also
note that, for example, path 0 of channel A (A0) has correlation factor of 0.4 with
path 0 of channels B, C and D (i.e. B0, C0 and D0). However, the same path 0 (A0)
has a much lower correlation between other paths of the channels B, C and D.
Similarly, examples of correlation tables for the indoor, pedestrian and vehicular
UMTS MIMO channels used in simulations are given in Tables 1.7‚Äì1.9.

42
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
Tx1
Tx2
Rx2
Rx1
Channel A
Channel B
Channel C
Channel D
Figure 1.30
2 √ó 2 MIMO channel.
Table 1.6
Correlations between four paths, A, B, C and D, with subchannel paths 0, 1, 2 of a 2 √ó 2
MIMO system.
A0
A1
A2
B0
B1
B2
C0
C1
C2
D0
D1
D2
A0
1
0.017
0.008
0.38
0.013
0.011
0.39
0.008
0.005
0.38
0.007
0.004
A1
0.017
1
0.021
0.003
0.4
0.02
0.003
0.39
0.01
0.003
0.403
0.015
A2
0.008
0.021
1
0.009
0.003
0.398
0.012
0.008
0.39
0.01
0.006
0.402
B0
0.38
0.003
0.009
1
0.003
0.008
0.39
0.004
0.006
0.39
0.008
0.002
B1
0.001
0.4
0.003
0.003
1
0.015
0.008
0.39
0.01
0.008
0.39
0.012
B2
0.01
0.02
0.39
0.008
0.015
1
0.016
0.01
0.39
0.01
0.008
0.39
C0
0.39
0.003
0.01
0.39
0.008
0.016
1
0.002
0.008
0.4
0.009
0.0006
C1
0.008
0.39
0.008
0.004
0.39
0.01
0.002
1
0.008
0.007
0.4
0.009
C2
0.005
0.011
0.39
0.006
0.01
0.39
0.008
0.008
1
0.009
0.007
0.401
D0
0.38
0.003
0.01
0.39
0.008
0.01
0.4
0.007
0.009
1
0.008
0.0039
D1
0.007
0.4
0.006
0.008
0.39
0.008
0.009
0.4
0.007
0.008
1
0.012
D2
0.004
0.001
0.4
0.002
0.01
0.39
0.006
0.009
0.4
0.003
0.012
1
Table 1.7
Mean correlation values between each path of the indoor MIMO channel.
A
B
C
D
A
1
0.006874
0.006467
0.006548
B
0.006874
1
0.005903
0.006568
C
0.006467
0.005903
1
0.006312
D
0.006548
0.006568
0.006312
1
Table 1.8
Mean correlation values between each path of the pedestrian MIMO channel.
A
B
C
D
A
1
0.049111
0.054736
0.050264
B
0.049111
1
0.056464
0.057746
C
0.054736
0.056464
1
0.062907
D
0.050264
0.057746
0.062907
1

MAGNETIC STORAGE CHANNEL MODELLING
43
Table 1.9
Mean correlation values between each path of the vehicular MIMO channel.
A
B
C
D
A
1
0.057106
0.04603
0.052264
B
0.057106
1
0.040664
0.044029
C
0.04603
0.040664
1
0.061777
D
0.052264
0.044029
0.061777
1
In Chapter 7, the UMTS MIMO channels are used to evaluate the performance
of space-time ring-TCM codes. The simplest space-time code uses a delay diversity
code. Its performance on the indoor, pedestrian and vehicular MIMO channels is given
in Figure 1.31.
1.7 Magnetic Storage Channel Modelling
Another application of error-correcting codes can be found in magnetic data storage
devices. An error-correcting scheme in this situation must be able to correct long bursts
of errors and have an efÔ¨Åcient decoding algorithm, minimizing latency, to ensure high
data rates. In this section we present a simple channel model for longitudinal magnetic
recording, a method of writing data to a magnetic disc that is currently in use. Data is
written to the magnetic disc by magnetizing microscopic areas on the disc in one of
1.E‚Äì05
1.E‚Äì04
1.E‚Äì03
1.E‚Äì02
1.E‚Äì01
1.E+00
24
22
20
18
16
14
12
10
8
6
4
2
0
Eb/N0 (dB)
BER
Indoor
Pedestrian
Vehicular
Figure 1.31
Simulation results evaluating the performance of the delay diversity code on the indoor,
pedestrian and vehicular 2 √ó 2 MIMO channels.

44
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
two directions, representing either a ‚Äò1‚Äô or a ‚Äò0‚Äô. The data is recovered by passing the
read head over the disc, which detects changes in the magnetic Ô¨Åeld corresponding
to transitions from ‚Äò0‚Äô to ‚Äò1‚Äô or ‚Äò1‚Äô to ‚Äò0‚Äô. Traditionally, data is written on the plane of
the magnetic disc; this is known as longitudinal recording. However, we have reached
the limits of storage capacity for this particular technique, and now data is written
perpendicular to the disc, known as perpendicular recording, which allows storage
capacity to be further increased. In this chapter, modelling a longitudinal magnetic
recording channel is explained, as this is still the most common recording method
used for most hard drives.
1.7.1 Longitudinal Recording
A simple linear channel model for a longitudinal magnetic recording system is given
in Figure 1.32 [15]. As stated previously, the read head measures the changes in
the direction of magnetization, and this can be modelled as a differentiator. The
differentiator with transfer function 1 ‚Äì D, where D is a memory element, subtracts
the previous bit value from the current bit value.
The transition response for longitudinal magnetic recording can be modelled as a
Lorentzian pulse, given by [15]:
h(t) =
1
1 +

2t
PW50
2 ,
(1.50)
where PW50 is the pulse width of the Lorentzian pulse at half its peak value. Some
Lorentzian pulses and the effect of varying PW50 are shown in Figure 1.33. It will be
shown that increasing PW50 increases the level of intersymbol interference (ISI).
A useful measure of ISI is the recording linear density, denoted by Ds and given
by [15]:
Ds = PW50
œÑ
,
(1.51)
Binary
Source
1 ‚Äì D
Differentiator
Lorentzian
Pulse
Electronics
Noise
{0, 1}
{‚Äì1, 0, 1}
ISI
To
Detector
Figure 1.32
Simple channel model for a longitudinal magnetic recording channel.

MAGNETIC STORAGE CHANNEL MODELLING
45
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
10
9
8
7
6
5
4
3
2
1
0
‚Äì1
‚Äì2
‚Äì3
‚Äì4
‚Äì5
‚Äì6
‚Äì7
‚Äì8
‚Äì9
‚Äì10
Time, t
h(t)
PW50 = 1
PW50 = 2
PW50 = 3
50% of peak amplitude
Figure 1.33
Lorentzian pulses with varying values of PW50.
where œÑ is the bit period. It gives the number of bits present within a time interval
equal to PW50. A higher value of Ds corresponds to a higher level of ISI.
The response of a positive transition from ‚Äò1‚Äô to a ‚Äò0‚Äô is called a dibit response,
expressed as:
d(t) = h(t) ‚àíh(t ‚àíœÑ).
(1.52)
An example of a dibit response where the Lorentzian pulse has PW50 = œÑ = 1
is given in Figure 1.34. A positive Lorentzian pulse is initiated at t = 0 and is followed
by a negative Lorentzian pulse at t = 1. The sum of the two pulses gives the dibit
response. The peak value of the dibit response is 80% of the peak value of the
Lorentzian pulses since PW50 is wide enough that both pulses overlap. Increasing
PW50 reduces the peak value of the dibit response further and this illustrates how ISI
occurs in a longitudinal magnetic recording channel.
1.7.2 (Partial Response Maximum Likelihood) (PRML) Detection
Partial response maximum likelihood (PRML) detection is a two-stage process con-
sisting of a transversal FIR Ô¨Ålter concatenated with a Viterbi decoder (explained in
Chapter 7), as shown in Figure 1.35.
The idea is to design a partial response (PR) equalizer with coefÔ¨Åcients that shape
the frequency response of the channel output to a predetermined target response

46
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
‚Äì1
‚Äì0.5
0
0.5
1
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0
‚Äì0.5
‚Äì1
‚Äì1.5
‚Äì2
‚Äì2.5
‚Äì3
‚Äì3.5
‚Äì4
‚Äì4.5
‚Äì5
t
h(t)
Positive pulse
Negative pulse
Dibit response
PW50 = 1 
PW50 = 1 
0.8
‚Äì0.8
Figure 1.34
A positive Lorentzian pulse at t = 0 followed by a negative Lorentzian pulse at t = 1, both
with PW50 = 1. The sum of the two pulses results in the dibit response.
[15, 16]. It is well known that frequency response of the channel output for longitudinal
recording with varying values of Ds matches closely with the frequency response of
the polynomial of the form:
G(D) = (1 ‚àíD)(1 + D)n,
(1.53)
where D is a delay element and n is a positive integer. For n = 1, 2 and 3 the polyno-
mials are known as PR4, EPR4 and E2PR4 respectively. For example, a longitudinal
magnetic recording channel with Ds = 2 has a response that matches closely with the
response of G(D) = (1 ‚Äì D)(1 + D) = 1 ‚Äì D2, known as PR4. Given a binary input
PR Equaliser
Viterbi Algorithm
PRML Detector
Channel
Output
Recovered
Data
Figure 1.35
Block diagram showing the two processes in PRML detection.

MAGNETIC STORAGE CHANNEL MODELLING
47
error
Channel
F(D)
G(D)
ak
sk
zk
dk
Figure 1.36
PR equalizer design criterion.
ak, the output zk of the PR equalizer should closely match the output dk of the target
polynomial G(D), as shown in Figure 1.36 [16].
1.7.2.1 PR Equalizer Design
The PR equalizer polynomial F(D) and target polynomial G(D) are expressed as
vectors F and G respectively [16]:
F = {‚àífK ‚àífK+1 ¬∑ ¬∑ ¬∑ f0 ¬∑ ¬∑ ¬∑ fK‚àí1 fK},
(1.54)
where K is a positive integer and ¬± fi, i = 0, 1, . . ., K, are the coefÔ¨Åcients of F(D).
G = { g0 g1 ¬∑ ¬∑ ¬∑ gL‚àí1},
(1.55)
where L is a positive integer and gi, i = 0, 1, . . ., L ‚Äì 1, are the coefÔ¨Åcients of G(D).
Two further matrices R and T are deÔ¨Åned as:
R = {Ri, j} = E{sk‚àíisk‚àíj},
‚àíK ‚â§i, j ‚â§K,
(1.56)
where E{} is the expectation operator. R therefore contains autocorrelation values of
the channel output:
T = {Ti, j} = E{sk‚àíiak‚àíj},
‚àíK ‚â§i ‚â§K, 0 ‚â§j ‚â§L ‚àí1,
(1.57)
T contains the cross correlation values of the channel output with the binary input.
The coefÔ¨Åcients of F can then be determined by [16]:
F = R‚àí1TG.
(1.58)
After equalization the output should be similar to the desired output from G(D). A
diagram of the PR4 polynomial G(D) = 1 ‚àíD2 is given in Figure 1.37.
Œ£
D
D
+
‚Äì
ak
dk
Figure 1.37
The PR4 target.

48
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
0/0
1/0
00
01
10
11
States
Bit Period, œÑ
1/1
0/-1
1/0
0/0
1/1
0/-1
0/0
1/1
0/0
1/1
0/0
1/1
Time
0/0
1/0
1/1
0/-1
1/0
0/0
1/1
0/-1
0/0
1/0
1/1
0/-1
1/0
0/0
1/1
0/-1
0/0
1/0
1/1
0/-1
1/0
0/0
1/1
0/-1
0/0
1/0
1/1
0/-1
1/0
0/0
1/1
0/-1
Figure 1.38
Trellis diagram of the PR4 target polyomial.
Since it has two memory elements, this can be expressed as a trellis, as shown in
Figure 1.38, with four states and one input and one output for each branch.
The Ô¨Ånal part of PRML detection involves Ô¨Ånding the maximum likelihood path
through the trellis corresponding to the original binary input ak. This can be achieved
by using the soft-decision Viterbi algorithm.
1.8 Conclusions
In this chapter, an introduction to information theory and channel modelling was
presented. In particular, the capacities of various channel models, such as the AWGN
channel and SISO and MIMO fading channels modelling a Ô¨Åxed wireless access
channel and UMTS cellular channels were examined. Simulation results were also
presented, showing the performance of uncoded schemes on the FWA and UMTS
channels for both MIMO and SISO situations. Finally, descriptions of different chan-
nel models were presented, allowing the performance for many coding schemes in
wireless and magnetic storage channels to be evaluated by computer simulation.
References
[1] Shannon, C.E. (1948) A mathematical theory of communication. Bell Systems Technical Journal,
27, 379‚Äì423, 623‚Äì56.
[2] Shannon, C.E. (1998) Communications in the presence of noise. Proceedings of IEEE, 86, 447‚Äì58.
[3] McEliece, R.J. (1977) The Theory of Information and Coding, Addison-Wesley, Massachusetts.
[4] Abramson, N. (1962) Information Theory and Coding, McGraw-Hill, New York.
[5] Proakis, J.G. (1989) Digital Communications, 2nd edn, McGraw-Hill, New York.
[6] Parsons, J.D. (2000) The Mobile Radio Propagation Channel, 2nd edn, John Wiley & Sons Inc.

REFERENCES
49
[7] Clarke, R.H. (1968) A statistical theory of mobile-radio reception. Bell Systems Technical Journal,
47, 957‚Äì1000.
[8] Jakes, W.C. (1994) Microwave Mobile Communications, John Wiley & Sons, Inc., New York.
[9] Hata, M. (1980) Empirical formula for propagation loss in land mobile radio services. IEEE
Transactions on Vehicular Technology, VT-29 (3), 317‚Äì25.
[10] IEEE 802.16 (2001) IEEE Standard for Local and Metropolitan Area Networks: part 1b, Air
Interface for Fixed Broadband Wireless Access Systems, April 8, 2002.
[11] Aulin, T. (1979) A modiÔ¨Åed model for the fading signal at a mobile radio channel. IEEE Transactions
on Vehicular Technology, VT-28 (3), 182‚Äì203.
[12] Greenstein, L.J., Erceg, V., Yeh, Y.S. and Clark, M.V. (1997) A new path-gain/delay-spread prop-
agation model for digital cellular channels. IEEE Transactions on Vehicular Technology, 46 (2),
477‚Äì85.
[13] IEEE 802.11 (1994) Wireless Access Method and Physical Layer SpeciÔ¨Åcation, New York.
[14] Erceg, V. (1999) An empirically based path loss model for wireless channels in suburban environ-
ments. IEEE Journal in Selected Areas in Communications, 17 (7), 1205‚Äì11.
[15] Vasic, B. and Kurtas, E.M. (eds) (2005) Coding and Signal Processing for Magnetic Recording
Systems, CRC Press.
[16] Moon, J. and Zeng, W. (1995) Equalization for maximum likelihood detectors. IEEE Trans. Mag.,
31 (2), 1083‚Äì8.


2
Basic Principles of Non-Binary
Codes
2.1 Introduction to Algebraic Concepts
In this chapter, the basic mathematical concepts necessary for working with non-binary
error-correcting codes are presented. The elements of the non-binary codes described
in this book belong to a non-binary alphabet and so this chapter begins with a discus-
sion of the deÔ¨Ånition and properties of a Group, which will lead to an introduction to
Rings and Fields. Later in the book, knowledge of the properties of rings will be nec-
essary to understanding the design of ring trellis coded modulation (ring-TCM) codes
and ring block coded modulation (ring-BCM) codes. Similarly, a good understanding
of Ô¨Ånite Ô¨Åelds is needed to be able to construct and decode Bose, Ray-Chaudhuri,
Hocquenghem (BCH) codes, Reed‚ÄìSolomon codes and Algebraic‚ÄìGeometric codes.
2.1.1 Groups
A set contains any elements or objects with no conditions imposed on it and can
be either Ô¨Ånite or inÔ¨Ånite. The number of elements or objects in a set is called the
cardinality. There are two binary operations that can operate on a set: multiplication
‚Äò¬∑‚Äô and addition ‚Äò+‚Äô. Certain conditions can be applied to the set under these binary
operations. The most common conditions are [1]:
 Commutativity: For two elements a and b in the set, a¬∑b = b¬∑a under multiplication
or a + b = b + a under addition.
 Identity: For any element a in the set there is an identity element b such that
a¬∑b = a under multiplication or a + b = a under addition.
 Inverse: For any element a in the set its inverse a‚àí1 must also be in the set. This
obeys a¬∑a‚àí1 = a‚àí1¬∑a = b (the identity element).
Non-Binary Error Control Coding for Wireless Communication and Data Storage
Rolando Antonio Carrasco and Martin Johnston
C 2008 John Wiley & Sons, Ltd

52
BASIC PRINCIPLES OF NON-BINARY CODES
 Associativity: For three elements a, b and c in the set, (a¬∑b)¬∑c = a¬∑(b¬∑c).
 Distributivity: For three elements a, b and c in the set, a¬∑(b + c) = (a¬∑b) + (a¬∑c).
A group is deÔ¨Åned as a set with the multiplication operation. Multiplying any two
elements in the group must result in a third element that is also in the group. This is
known as closure. A group also has the following conditions:
 associativity under multiplication,
 identity under multiplication,
 inverse.
The group is called a commutative or abelian group if it also has commutativity
under multiplication.
2.1.2 Rings
If the two binary operations ‚Äò+‚Äô and ‚Äò¬∑‚Äô are allowed then a ring can be deÔ¨Åned. A ring
must have the following conditions:
1. associativity,
2. distributivity,
3. commutativity under addition.
The ring is called a commutative ring if it also has commutativity under multiplica-
tion. If the ring has a multiplicative identity 1 then it is called a ring with identity. An
example of a ring is the ring of integers Zq under modulo-q addition and multiplica-
tion, where q is the cardinality of the ring. For example, Z4 is deÔ¨Åned as {0, 1, 2, 3}.
It is easy to see that the elements obey the three deÔ¨Ånitions of a ring. Also, all the
elements commute under multiplication and the multiplicative identity element 1 is
present, meaning that Z4 is a commutative ring with identity. Tables 2.1 and 2.2 show
the addition and multiplication tables respectively of the ring of integers Z 8 = {0, 1,
2, 3, 4, 5, 6, 7}.
Table 2.1
Addition table for Z8.
+
0
1
2
3
4
5
6
7
0
0
1
2
3
4
5
6
7
1
1
2
3
4
5
6
7
0
2
2
3
4
5
6
7
0
1
3
3
4
5
6
7
0
1
2
4
4
5
6
7
0
1
2
3
5
5
6
7
0
1
2
3
4
6
6
7
0
1
2
3
4
5
7
7
0
1
2
3
4
5
6

INTRODUCTION TO ALGEBRAIC CONCEPTS
53
Table 2.2
Multiplication table for Z8.
¬∑
0
1
2
3
4
5
6
7
0
0
0
0
0
0
0
0
0
1
0
1
2
3
4
5
6
7
2
0
2
4
6
0
2
4
6
3
0
3
6
1
4
7
2
5
4
0
4
0
4
0
4
0
4
5
0
5
2
7
4
1
6
3
6
0
6
4
2
0
6
4
2
7
0
7
6
5
4
3
2
1
2.1.3 Ring of Polynomials
The set of all polynomials with coefÔ¨Åcients deÔ¨Åned in Zq forms a ring under
the addition and multiplication operations. If we deÔ¨Åne two polynomials f(x) and
g(x) as f (x) = f0 + f1x + f2x2 + ¬∑ ¬∑ ¬∑ + fvxv and g(x) = g0 + g1x + g2x2 + ¬∑ ¬∑ ¬∑ +
gwxw, where fi and gi ‚ààZq, v and w are the degrees of both polynomials respectively
with v < w, then the addition of both polynomials is:
f (x) + g(x) =

i
( fi + gi) ¬∑ xi,
where fi + gi ‚ààZq.
Similarly, the product of the two polynomials is:
f (x) ¬∑ g(x) =

i
Ô£´
Ô£≠
i
j=0
f j ¬∑ gi‚àíj
Ô£∂
Ô£∏¬∑ xi
where

i
Ô£´
Ô£≠
i
j=0
f j ¬∑ gi‚àíj
Ô£∂
Ô£∏‚ààZq.
For example, if f (x) = 1 + 2x + 3x2 and g(x) = 3 + x + 2x2 + x3 are deÔ¨Åned in
Z4, then the sum of both polynomials is:
f (x) + g(x) =

1 + 2x + 3x2
+

3 + x + 2x2 + x3
= (1 + 3) + (2 + 1)x + (3 + 2)x2 + x3
= 3x + x2 + x3.
Similarly, the product of both polynomials is:
f (x) ¬∑ g(x) = (1 + 2x + 3x2) ¬∑ (3 + x + 2x2 + x3)
= (1 √ó 3) + (1 √ó 1 + 2 √ó 3)x + (1 √ó 2 + 2 √ó 1 + 3 √ó 3)x2
+ (1 √ó 1 + 2 √ó 2 + 3 √ó 1)x3 + (3 √ó 2 + 2 √ó 1)x4 + (3 √ó 1)x3
= 3 + 7x + 13x2 + 8x3 + 8x4 + 3x3
= 3 + 3x + x2 + 3x3.

54
BASIC PRINCIPLES OF NON-BINARY CODES
Table 2.3
Multiplication table of nonzero elements in GF(5).
¬∑
1
2
3
4
1
1
2
3
4
2
2
4
1
3
3
3
1
4
2
4
4
3
2
1
2.1.4 Fields
A Ô¨Åeld is similar to a ring as it also uses both binary operations. It has the following
conditions:
1. Commutativity under addition,
2. Distributivity,
3. Commutativity under multiplication when the additive identity element 0 is
removed,
4. Identity,
5. Inverse.
An example of a Ô¨Åeld is the set of real numbers. The set of integers is not a Ô¨Åeld
since not all integers have a multiplicative inverse. A Ô¨Ånite Ô¨Åeld has a Ô¨Ånite number of
elements and is known as a Galois Field, written as GF(q), where q is the cardinality
of the Ô¨Åeld and is a prime number or a power of a prime greater than 1. To be a
Ô¨Ånite Ô¨Åeld the elements {1, 2, 3, . . . , q ‚àí1} must form a group under multiplication
modulo-q.
For example, GF(5) = {0, 1, 2, 3, 4} is a Ô¨Ånite Ô¨Åeld because the elements
{1, 2, 3, 4} are a group under modulo-q multiplication. The group has closure because
no two elements multiplied together give 0, which is not in the group, as shown in
Table 2.3.
However, let us say that the set {0, 1, 2, 3, 4, 5} deÔ¨Ånes the Ô¨Ånite Ô¨Åeld GF(6). To
be a Ô¨Ånite Ô¨Åeld the elements {1, 2, 3, 4, 5} must form a group, but this is not the case.
It can be seen in Table 2.4 that some elements produce a 0 when multiplied together,
Table 2.4
Multiplication table of nonzero elements in GF(6),
proving it is not a Ô¨Ånite Ô¨Åeld.
¬∑
1
2
3
4
5
1
1
2
3
4
5
2
2
4
0
2
4
3
3
0
3
0
3
4
4
2
0
4
2
5
5
4
3
2
1

INTRODUCTION TO ALGEBRAIC CONCEPTS
55
Table 2.5
The order of each nonzero element in GF(5).
Element in GF(5)
Order
1
1
2
4
3
4
4
2
which is not in the set {1, 2, 3, 4, 5}, and this proves that GF(6) is not a Ô¨Ånite Ô¨Åeld.
Hence, only sets with a prime or a power of a prime number of elements can be Ô¨Ånite
Ô¨Åelds.
Every element Œ≤ in a Ô¨Ånite Ô¨Åeld has an order, denoted as ord(Œ≤). The order of an
element is the number of times the element is multiplied by itself until the produce
equals 1, that is the value of m which gives Œ≤m = 1. In a Ô¨Ånite Ô¨Åeld GF(q) the order
of an element divides q ‚àí1. For example, the order of the element 2 in GF(5) is 4
because 2 √ó 2 √ó 2 √ó 2 = 24 = 1. The order of each element in GF(5) is shown in
Table 2.5.
It can be seen that all the elements in GF(5) have an order which divides q ‚àí1 =
4. An element which has an order equal to q ‚àí1 is called a primitive element.
Therefore, from Table 2.5, the elements 2 and 3 are primitive elements in GF(5) since
they have an order of 4. This means that all the nonzero elements in a Ô¨Ånite Ô¨Åeld can
be expressed as powers of primitive elements. For example, in GF(5), 2 and 3 are the
primitive elements, meaning that all the nonzero elements in GF(5) can be expressed
using powers of 2 or 3, as shown in Table 2.6.
Finite Ô¨Åelds of the form GF( pm), where p is prime and m > 0, are called extension
Ô¨Åelds. They contain the elements

0, 1, Œ±, Œ±2, . . . , Œ± pm‚àí2	
, where Œ± is the primitive
element with order pm ‚àí1. This means that all the nonzero elements in GF( pm) can
Table 2.6
Using primitive elements to deÔ¨Åne all nonzero elements in GF(5).
Powers of primitive element 2
Element in GF(5)
21
2
22
4
23
3
24
1
Powers of primitive element 3
Element in GF(5)
31
3
32
4
33
2
34
1

56
BASIC PRINCIPLES OF NON-BINARY CODES
be represented as powers of Œ±. For example, GF(22) = {0, 1, Œ±, Œ±2}. Each element in
GF( pm) can be expressed as an m-tuple vector with elements from GF( p) and addition
between elements is done modulo-p.
2.1.5 Primitive Polynomials
A class of polynomials called primitive polynomials are used to deÔ¨Åne GF( pm). An
irreducible polynomial f(x) of degree m deÔ¨Åned over GF( p) is primitive if the smallest
positive integer n for which f(x) divides xn ‚àí1 is n = pm‚Äì 1 [2]. As an example, take
the irreducible polynomial
x4 + x + 1
and p = 2.
This polynomial will be primitive if it divides x24‚àí1 + 1 = x16‚àí1 + 1 = x15 + 1,
as ‚àí1 ‚â°1 when p = 2. It can be seen from Figure 2.1 that there is no remainder and
so x4 + x + 1 divides 1 + x15. Similar calculations will show that x4 + x + 1 will not
divide xn + 1, 1 ‚â§n < 15, which means that x4 + x + 1 is a primitive polynomial. If
a primitive element Œ± is the root of a primitive polynomial then a higher power of Œ±
1
0
1
1
0
0
0
0
0
0
0
0
0
0
1
0x
0x
0x
0x
0x
0x
0x
0x
0x
0x
0x
0x
0x
0x
1
2
3
5
7
8
11
4
4
2
5
2
4
5
2
3
6
3
4
5
6
3
4
7
5
6
7
5
6
9
7
9
7
8
11
8
9
11
8
9
12
11
12
11
12
15
2
3
4
5
6
7
8
9
10
11
12
13
14
15
4
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
Figure 2.1
Long division showing that the primitive polynomial x4 + x + 1 divides x15 ‚àí1.

INTRODUCTION TO ALGEBRAIC CONCEPTS
57
can be expressed as the sum of lower powers of Œ±. For example, if Œ± is a root of x4 +
x + 1, this implies Œ±4 + Œ± + 1 = 0 or Œ±4 = ‚àíŒ± ‚àí1. So, Œ±5 = ‚àíŒ±2 ‚àíŒ±, Œ±6 = ‚àíŒ±3
‚àíŒ±2, Œ±7 = ‚àíŒ±4 ‚àíŒ±3 = (‚àíŒ± ‚àí1) ‚àíŒ±3 and so on. In this way an extension Ô¨Åeld
GF( pm) can be deÔ¨Åned with the elements expressed in the form Œ±m‚àí1 + Œ±m‚àí2 + . . .
Œ± + 1, or equivalently an m-dimensional vector space over GF( p).
Example 2.1:
Constructing the extension Ô¨Åeld GF(23): The extension Ô¨Åeld
GF(23) can be constructed using the primitive polynomial f(x) = x3 + x + 1. If Œ±
is the root of f(x) then Œ±3 = Œ± + 1. The elements of GF(23) are shown in Table 2.7.
Table 2.7
Construction of GF(23).
Element in GF(23)
Element expressed as the sum of
lower powers of Œ±
Element expressed as 3-tuple
vector over GF(2)
0
0
000
1
1
001
Œ±
Œ±
010
Œ±2
Œ±2
100
Œ±3
Œ± + 1
011
Œ±4
Œ±2 + Œ±
110
Œ±5
Œ±3 + Œ±2 = Œ± + 1 + Œ±2
111
Œ±6
Œ±4 + Œ±3 = Œ±2 + Œ± + Œ± + 1 = Œ±2 + 1
101
To prove the Ô¨Åeld has closure, the element Œ±7 can be expressed as Œ±6. Œ± =
(Œ±2 + 1). Œ± = Œ±3 + Œ± = Œ± + 1 + Œ± = 1, that is Œ±c(q‚àí1) = 1 where c is a
nonnegative integer. Addition in GF(23) is accomplished by adding the vector rep-
resentation of the elements modulo-2. For example, Œ±5 + Œ±6 = (111) + (101) =
(010) = Œ±. Multiplication is accomplished by taking the powers of the elements
and adding them modulo-7. For example, Œ±4¬∑Œ±5 = Œ±(4+5)mod 7 = Œ±2. The ad-
dition and multiplication tables for GF(23) are shown in Tables 2.8 and 2.9,
respectively.
Table 2.8
Addition table for GF(23).
+
0
1
Œ±
Œ±2
Œ±3
Œ±4
Œ±5
Œ±6
0
0
1
Œ±
Œ±2
Œ±3
Œ±4
Œ±5
Œ±6
1
1
0
Œ±3
Œ±6
Œ±
Œ±5
Œ±4
Œ±2
Œ±
Œ±
Œ±3
0
Œ±4
1
Œ±2
Œ±6
Œ±5
Œ±2
Œ±2
Œ±6
Œ±4
0
Œ±5
Œ±
Œ±3
1
Œ±3
Œ±3
Œ±
1
Œ±5
0
Œ±6
Œ±2
Œ±4
Œ±4
Œ±4
Œ±5
Œ±2
Œ±
Œ±6
0
1
Œ±3
Œ±5
Œ±5
Œ±4
Œ±6
Œ±3
Œ±2
1
0
Œ±
Œ±6
Œ±6
Œ±2
Œ±5
1
Œ±4
Œ±3
Œ±
0

58
BASIC PRINCIPLES OF NON-BINARY CODES
Table 2.9
Multiplication table for GF(23).
¬∑
0
1
Œ±
Œ±2
Œ±3
Œ±4
Œ±5
Œ±6
0
0
0
0
0
0
0
0
0
1
0
1
Œ±
Œ±2
Œ±3
Œ±4
Œ±5
Œ±6
Œ±
0
Œ±
Œ±2
Œ±3
Œ±4
Œ±5
Œ±6
1
Œ±2
0
Œ±2
Œ±3
Œ±4
Œ±5
Œ±6
1
Œ±
Œ±3
0
Œ±3
Œ±4
Œ±5
Œ±6
1
Œ±
Œ±2
Œ±4
0
Œ±4
Œ±5
Œ±6
1
Œ±
Œ±2
Œ±3
Œ±5
0
Œ±5
Œ±6
1
Œ±
Œ±2
Œ±3
Œ±4
Œ±6
0
Œ±6
1
Œ±
Œ±2
Œ±3
Œ±4
Œ±5
Example 2.2:
Constructing the extension Ô¨Åeld GF(32): The extension Ô¨Åeld
GF(32) can be constructed using the primitive polynomial f(x) = x2 ‚àí2x ‚àí1. If
the primitive element Œ± is the root of f(x) then Œ±2 = 2Œ± + 1 and can be expressed
as a 2-dimensional vector space over GF(3), as shown in Table 2.10.
Table 2.10
Construction of GF(32).
Element in
GF(32)
Element expressed as the sum of lower powers of Œ±
Element expressed as
2-tuple vector over GF(3)
0
0
00
1
1
01
Œ±
Œ±
10
Œ±2
2Œ± + 1
21
Œ±3
2Œ±2 + Œ± = 2(2Œ± + 1) + Œ± = 4Œ± + 2+ Œ± = 5Œ± + 2
= 2Œ± + 2
22
Œ±4
2Œ±2 + 2Œ± = 2(2Œ± + 1) + 2Œ± = 4Œ± + 2 + 2Œ± = 6Œ±
+ 2 = 2
02
Œ±5
2Œ±
20
Œ±6
2Œ±2 = 2(2Œ± + 1) = 4Œ± + 2 = Œ± + 2
12
Œ±7
Œ±2 + 2Œ± = 2Œ± + 1 + 2Œ± = 4Œ± + 1 = Œ± + 1
11
To add two elements together, their corresponding vectors are added modulo-3,
for example Œ±4 + Œ±6 = (02) + (12) = (11) = Œ±7. To multiply two elements, their
powers are added modulo-8, for example Œ±6 + Œ±7 = Œ±(6+7)mod 8 = Œ±5.
2.1.6 Minimal Polynomials and Cyclotomic Cosets
It was stated that a primitive polynomial deÔ¨Åned in GF(2) was irreducible, implying
that it cannot be factorized, that is it has no roots in GF(2). However, that a polynomial
is irreducible in one Ô¨Ånite Ô¨Åeld usually does not mean that it will be irreducible in

INTRODUCTION TO ALGEBRAIC CONCEPTS
59
other Ô¨Ånite Ô¨Åelds. By substituting each element of GF(8) into the primitive polynomial
x3 + x + 1 it can be seen that this polynomial has three roots: X1 = Œ±, X2 = Œ±2 and
X3 = Œ±4. This means that x3 + x + 1 can be factorized to (x + Œ±)(x + Œ±2)(x + Œ±4).
In general, given one root Œ≤ ‚ààGF(2p), the remaining roots will be Œ≤2i, i = 1, 2, 3,
. . . , 2p‚àí1 ‚àí1. These roots are called the conjugates of Œ≤ and the polynomial
M(x) = (x + Œ≤)(x + Œ≤2)(x + Œ≤4) . . .

x + Œ≤2p‚àí1‚àí1
is called the minimal polynomial [2] of Œ≤, that is the degree of M(x) is minimal.
Therefore, the minimal polynomial of Œ±, Œ±2 and Œ±4 in GF(8) is M(x) = x3 + x + 1,
which is also a primitive polynomial. The minimal polynomials for the other elements
in GF(8) are calculated as follows.
For Œ≤ = 0 the minimal polynomial is M(x) = x, and for Œ≤ = 1, M(x) = x + 1. The
conjugates for Œ≤ = Œ±3 are Œ±6 and Œ±5. The minimal polynomial for Œ±3 is then:
M(x) = (x + Œ±3)(x + Œ±6)(x + Œ±5)
=

x2 + (Œ±3 + Œ±6)x + Œ±9
(x + Œ±5)
= (x2 + Œ±4x + Œ±2)(x + Œ±5)
= x3 + (Œ±5 + Œ±4)x2 + (Œ±9 + Œ±2)x + Œ±7
= x3 + x2 + 1.
The powers of Ô¨Ånite Ô¨Åeld elements associated with each minimal polynomial
form cyclotomic cosets. The minimal polynomials for GF(8) and their corresponding
cyclotomic cosets are summarized in Table 2.11.
2.1.7 SubÔ¨Åelds
Some elements within a Ô¨Ånite Ô¨Åeld GF( pm) can also belong to a smaller Ô¨Ånite Ô¨Åeld,
known as a subÔ¨Åeld. These elements will have a minimal polynomial that matches a
known primitive polynomial that can deÔ¨Åne a smaller Ô¨Ånite Ô¨Åeld. An example of a
subÔ¨Åeld can be found in GF(16), deÔ¨Åned by the primitive polynomial x4 + x + 1. The
element Œ±5 has only one conjugate, namely Œ±10. The minimal polynomial for Œ±5 in
Table 2.11
Elements in GF(8) and their corresponding minimal polynomials.
Finite Ô¨Åeld element in GF(8)
Minimal polynomial
Cyclotomic coset
0
x
‚Äî
1
x + 1
{0}
Œ±, Œ±2, Œ±4
x3 + x + 1
{1, 2, 4}
Œ±3, Œ±6, Œ±5
x3 + x2 + 1
{3, 6, 5}

60
BASIC PRINCIPLES OF NON-BINARY CODES
GF(16) is therefore:
M(x) = (x + Œ±5)(x + Œ±10)
= x2 + (Œ±5 + Œ±10)x + Œ±15
= x2 + ((Œ±2 + Œ±) + (Œ±2 + Œ± + 1))x + 1
= x2 + x + 1.
This minimal polynomial is known to be the primitive polynomial that deÔ¨Ånes
GF(4), and this implies that Œ≤ = Œ±5 is a primitive element for the subÔ¨Åeld GF(4)
within GF(16). This subÔ¨Åeld then consists of the elements {0, 1, Œ≤, Œ≤2} = {0, 1,
Œ±5, Œ±10}.
More examples of subÔ¨Åelds can be found in larger Ô¨Ånite Ô¨Åelds, such as GF(64),
deÔ¨Åned by the primitive polynomial x6 + x + 1. A list of minimal polynomials
for GF(64) and many others can be found in [1]. The element Œ±21 has a minimal
polynomial of x2 + x + 1, implying that it is the primitive element of the subÔ¨Åeld
GF(4) with elements {0, 1, Œ±21, Œ±42}. Also, the element Œ±9 has a minimal polynomial
of x3 + x2 + 1, implying that it is the primitive element of the subÔ¨Åeld GF(8) with
elements {0, 1, Œ≤, Œ≤2, Œ≤3, Œ≤4, Œ≤5, Œ≤6} = {0, 1, Œ±9, Œ±18, Œ±36, Œ±8, Œ±16, Œ±32}.
2.2 Algebraic Geometry Principles
2.2.1 Projective and AfÔ¨Åne Space
The construction of AG codes requires a set of points that satisfy an irreducible
afÔ¨Åne curve and a set of rational functions deÔ¨Åned on the curve. Projective space is
(n + 1)-dimensional and elements in this space deÔ¨Åned over some Ô¨Ånite Ô¨Åeld are
called projective points, that is [3]:
(c1, c2, c3, . . . , cn, cn+1) ,
where ci, i = 0, 1, . . . , n, n + 1, are elements in some Ô¨Ånite Ô¨Åeld.
AfÔ¨Åne space is n-dimensional. Elements in this space deÔ¨Åned over some Ô¨Ånite Ô¨Åeld
are called afÔ¨Åne points and are of the form [3] (c1, c2, c3, . . . , cn, 1).
Finally, when points are of the form (c1, c2, c3, . . . , cn, 0), the space is called the
hyperplane at inÔ¨Ånity [3]. All points in the hyperplane at inÔ¨Ånity are called points at
inÔ¨Ånity. For the construction of AG codes we are interested in Ô¨Ånding all afÔ¨Åne points
and points at inÔ¨Ånity that cause an irreducible smooth afÔ¨Åne curve to vanish.
2.2.2 Projective and AfÔ¨Åne Curves
A projective curve is an (n + 1)-dimensional curve deÔ¨Åned by projective points. It
is made up of n-dimensional afÔ¨Åne curves in the n + 1 different coordinate systems.

ALGEBRAIC GEOMETRY PRINCIPLES
61
For a 3-dimensional projective curve C(x, y, z) there are three afÔ¨Åne curves:
C(x, y, 1), C(x,1, z,) and C(1, y, z).
Algebraic‚Äìgeometric codes are constructed from irreducible afÔ¨Åne smooth curves.
An irreducible curve is a curve that cannot be expressed as the product of curves of
lower degrees. For example, the curve C(x, y) = x2 + y2 is not irreducible over GF(2)
because:
(x + y)(x + y)
= (x2 + xy + xy + y2)
and
xy + xy = 0.
= x2 + y2
Similarly, the curve C(x, y) = x3 + y3 is irreducible over GF(2) but is not irreducible
over GF(3) because:
(x + y)(x + y)(x + y)
= (x2 + 2xy + y2)(x + y)
= x3 + x2y + 2x2y + 2xy2 + xy2 + y3.
= x3 + 3x2y + 3xy2 + y3
and
3 ‚â°0 mod 3
= x3 + y3
A point on a curve is said to be nonsingular if all partial derivatives of the curve
do not vanish at this point. If all the points that satisfy the curve are nonsingular then
the curve is said to be nonsingular or smooth [4]. An important class of curve is the
Hermitian curve, which is deÔ¨Åned over square Ô¨Ånite Ô¨Åelds GF(w2). The projective
Hermitian curve is:
C(x, y, z) = xw+1 + ywz + yzw.
(2.1)
This is a smooth curve, and to prove it the three partial derivatives are calculated:
‚àÇC(x, y, z)
‚àÇx
= (w + 1)xw = xw
‚àÇC(x, y, z)
‚àÇy
= wyw‚àí1z + zw = zw.
‚àÇC(x, y, z)
‚àÇz
= yw + wyzw‚àí1 = yw
The only point that makes all three partial derivatives vanish is (0, 0, 0), but this is
not a point in projective space, and so all the points are nonsingular and the curve is
smooth.
2.2.3 Finding Points on an AfÔ¨Åne Curve
The projective points that satisfy the projective curve C(x, y, z) = 0 are of the form
(Œ±, Œ≤, Œ¥), where Œ±, Œ≤, Œ¥ are elements in a Ô¨Ånite Ô¨Åeld. For the construction of an AG

62
BASIC PRINCIPLES OF NON-BINARY CODES
code, only the afÔ¨Åne points of the form (Œ±, Œ≤, 1), that is z =1, and points at inÔ¨Ånity of
the form (Œ±, Œ≤, 0), that is z = 0, are required. To Ô¨Ånd all the afÔ¨Åne points the projective
curve is broken down into its afÔ¨Åne component curves. For each afÔ¨Åne curve, the
afÔ¨Åne points (Œ±, Œ≤, 1) and the points at inÔ¨Ånity (Œ±, Œ≤, 0) that cause the curve to vanish
are kept. By using all the afÔ¨Åne component curves more points can be found, resulting
in longer codes.
The afÔ¨Åne form of the Hermitian curve from (2.1) in the (x ‚àíy) coordinate system
(with z = 1) is:
C(x, y, 1) = xw+1 + yw + y.
(2.2)
In the (x ‚àíz) coordinate system (with y = 1) it is:
C(x, 1, z) = xw+1 + z + zw.
(2.3)
In the (y ‚àíz) coordinate system (with x = 1) it is:
C(1, y, z) = 1 + ywz + yzw.
(2.4)
The Hermitian curves are known to have w3 + 1 points. For GF(22), the Hermitian
curve will have 23 + 1 = 8 + 1 = 9 points and is deÔ¨Åned as:
C(x, y, z) = x3 + y2z + yz2,
(2.5)
with the afÔ¨Åne curve in the (x ‚àíy) system deÔ¨Åned as:
C(x, y) = x3 + y2 + y.
(2.6)
The afÔ¨Åne points that satisfy C(x, y, 1) = 0 are found by substituting all possible val-
ues of x and y (with z = 1) in GF(22), where Œ±2 = Œ± + 1, into (2.6). This is shown below:
C(0, 0, 1) = 03 + 02 + 0 = 0
C(0, 1, 1) = 03 + 12 + 1 = 0
C(0, Œ±, 1) = 03 + Œ±3 + Œ± = 1 + Œ± = Œ±2 = 0
C(0, Œ±2, 1) = 03 + Œ±6 + Œ±2 = 1 + Œ±2 = Œ± = 0
C(1, 0, 1) = 13 + 02 + 0 = 1 = 0
C(1, 1, 1) = 13 + 12 + 1 = 1 = 0
C(1, Œ±, 1) = 13 + Œ±2 + Œ± = 1 + 1 = 0
C(1, Œ±2, 1) = 13 + Œ±4 + Œ±2 = 1 + Œ± + Œ±2 = 1 + 1 = 0
C(Œ±, 0, 1) = Œ±3 + 02 + 0 = Œ±3 = 1 = 0
C(Œ±, 1, 1) = Œ±3 + 12 + 1 = Œ±3 = 1 = 0
C(Œ±, Œ±, 1) = Œ±3 + Œ±2 + Œ± = 1 + 1 = 0
C(Œ±, Œ±2, 1) = Œ±3 + Œ±4 + Œ±2 = 1 + Œ± + Œ±2 = 1 + 1 = 0

ALGEBRAIC GEOMETRY PRINCIPLES
63
Table 2.12
Eight projective points for the Hermitian curve in the ( x ‚àíy) system.
P1 = (0, 0, 1)
P2 = (0, 1, 1)
P3 = (1, Œ±, 1)
P4 = (1, Œ±2, 1)
P5 = (Œ±, Œ±, 1)
P6 = (Œ±, Œ±2, 1)
P7 = (Œ±2, Œ±, 1)
P8 = (Œ±2, Œ±2, 1)
C(Œ±2, 0, 1) = Œ±6 + 02 + 0 = 1 = 0
C(Œ±2, 1, 1) = Œ±6 + 12 + 1 = Œ±6 = 1 = 0
C(Œ±2, Œ±, 1) = Œ±6 + Œ±2 + Œ± = Œ±6 + 1 = 1 + 1 = 0.
C(Œ±2, Œ±2, 1) = Œ±6 + Œ±4 + Œ±2 = Œ±6 + Œ± + Œ±2 = Œ±6 + 1 = 1 + 1 = 0
Therefore, the afÔ¨Åne points that satisfy C(x, y, 1) = 0 are given in Table 2.12.
We must also Ô¨Ånd the points of the other two afÔ¨Åne curves, C(x, 1, z) = x3 + z
+ z2 and C(1, y, z) = 1 + y2z + yz2. There are eight projective points that satisfy
C(x, 1, z) = 0, as shown in Table 2.13.
However, P2 is the only afÔ¨Åne point, since it is of the form (x, y, 1), but it is also
in Table 2.12. There is also a point at inÔ¨Ånity, P1 = (0, 1, 0). Finally, there are four
projective points that satisfy C(1, y, z) = 0, as shown in Table 2.14. In this case, only
P1 and P3 are afÔ¨Åne points, since they are of the form (x, y, 1), and they are also
present in Table 2.12. Therefore, the projective Hermitian curve in (2.5) has eight
afÔ¨Åne points and one point at inÔ¨Ånity, Q = (0, 1, 0).
All codes constructed from curves with one point at inÔ¨Ånity are called one-point
AG or Goppa codes [4] and these are the most commonly used codes in the literature.
Elliptic and hyperelliptic curves are other examples of curves that have one point at
inÔ¨Ånity. An upper bound on the number of points N, including any points at inÔ¨Ånity,
that satisfy a curve over a Ô¨Åeld GF(q) is the Hasse-Weil bound, deÔ¨Åned as [3]:
|N| ‚â§(m ‚àí1)(m ‚àí2)‚àöq + 1 + q,
(2.7)
where m is the degree of the curve. Taking the Hermitian curve deÔ¨Åned in (2.5) with
degree m = 3 and q = 4, the upper bound from (2.7) is:
|N| ‚â§(3 ‚àí1)(3 ‚àí2)
‚àö
4 + 1 + 4
|N| ‚â§9.
Table 2.13
Eight projective points for the Hermitian curve in the ( x ‚àíz) system.
P1 = (0, 1, 0)
P2 = (0, 1, 1)
P3 = (1, 1, Œ±)
P4 = (1, 1, Œ±2)
P5 = (Œ±, 1, Œ±)
P6 = (Œ±, 1, Œ±2)
P7 = (Œ±2, 1, Œ±)
P8 = (Œ±2, 1, Œ±2)
Table 2.14
Four projective points for the Hermitian curve in the ( y ‚àíz) system.
P1 = (1, Œ±, 1)
P2 = (1, Œ±, Œ±2)
P3 = (1, Œ±2, 1)
P4 = (1, Œ±2, Œ±)

64
BASIC PRINCIPLES OF NON-BINARY CODES
Therefore, the maximum number of points a curve can have with m = 3 and deÔ¨Åned
over GF(22) is nine. For the Hermitian curve we have eight afÔ¨Åne points and one point
at inÔ¨Ånity, giving a total of nine points. Therefore, the Hermitian curves meet the
Hasse-Weil bound and are known as maximal curves. These curves are desirable
because they produce long codes. Other types of maximal curve include the elliptic
curves over certain Ô¨Ånite Ô¨Åelds. For example, the elliptic curve [3, 5]
C(x, y) = x3 + x2 + x + y2 + y + 1
is a maximal curve over GF(24). It has degree m = 3 and from (2.7) it has 25 points.
Compare this to the Hermitian curve deÔ¨Åned over GF(24)
C(x, y) = x5 + y4 + y,
which has m = 5 and from (2.7) has 65 points. We can see that codes from elliptic
curves will be much shorter than codes constructed from Hermitian curves, however
they are still longer that Reed‚ÄìSolomon codes, which are constructed from the afÔ¨Åne
line
y = 0
and have degree m = 1. For a Ô¨Ånite Ô¨Åeld GF(q), the Hasse-Weil bound in (2.7)
simpliÔ¨Åes to:
|N| ‚â§q + 1,
so for GF(24) the longest possible Reed‚ÄìSolomon code is 16 + 1 = 17, which is
shorter than a code constructed from an elliptic curve. However, it will be seen later
that maximizing the degree of a curve is not the only design criterion for constructing
good AG codes.
2.2.4 Rational Functions on Curves
To construct the generator matrix of an AG code, a basis of rational functions on the
curve must Ô¨Årst be deÔ¨Åned. Each rational function is evaluated at each of the n afÔ¨Åne
points to form a row of the generator matrix, where n is the block length of the code.
A rational function f(x, y, z) is the quotient of two other functions, g(x, y, z) and
h(x, y, z), that both have the same degree, that is f (x, y, z) = g(x,y,z)
h(x,y,z). DeÔ¨Åning a ratio-
nal function on a curve changes its behaviour. For example, the function f (x, y) =
x
y+1
deÔ¨Åned over the cubic curve C(x, y) = x3 + y3 + 1 =0 over GF(22) can be rewritten as:
x3 = y3 + 1
x = y3 + 1
x2
= (y + 1)(y2 + y + 1)
x2
.
‚à¥
x
y + 1 = y2 + y + 1
x2

ALGEBRAIC GEOMETRY PRINCIPLES
65
The cubic curve has six points: P1 = (0, 1), P2 = (0, Œ±), P3 = (0, Œ±2), P4 = (1, 0),
P5 = (Œ±, 0), P6 = (Œ±2, 0). In this case the function f (x, y) =
x
y+1 is deÔ¨Åned for all
the points on the cubic curve except for P1 = (0, 1), since f (0, 1) =
0
1+1 = 0
0.
If the function was not deÔ¨Åned on the curve it would have a zero of order 1 and a
pole of order 1 at this point. However, since it is on the curve the function evaluated
at (0, 1) gives:
f (x, y) = y2 + y + 1
x2
,
f (0, 1) = 12 + 1 + 1
02
= 1
0
which has no zeroes but has a pole of order 2 due to the x2 in the denominator. The
order of a function is denoted by ŒΩ(f(x, y, z)) and is the sum of its zero order and pole
order. To construct an AG code, the basis of the rational functions must have a pole
at the points of inÔ¨Ånity, but have no other poles at any of the afÔ¨Åne points.
Example 2.3: Rational functions on the projective line y = 0: The points in
the (x, y, 1) system are of the form (Œ±i, 0, 1) for a Ô¨Ånite Ô¨Åeld GF(q), where Œ±i is
a primitive element, so there are q points in this system. In the (x, 1, z) system
there are no points because y = 0. In the (1, y, z) system the points are of the
form (1, 0, Œ±i), but only (1, 0, 1) is of the form (Œ±, Œ≤, 1). In this system there is
also the point (1, 0, 0), which is in the hyperplane at inÔ¨Ånity (since z = 0) and
so is a point at inÔ¨Ånity. Therefore there are q afÔ¨Åne points and 1 point at inÔ¨Ånity.
Next, a sequence of rational functions on the projective line that have a pole only
at the point of inÔ¨Ånity Q = (1, 0, 0) must be found. The sequence of rational
functions { xi
zi }, i > 0, has a pole of order i at Q but has no poles at any of the
afÔ¨Åne points because z = 1. From this example, we can see that the number of
points on the projective line cannot exceed the cardinality of the Ô¨Ånite Ô¨Åeld the
line is deÔ¨Åned over. The codes constructed from the line y = 0 are actually the
well-known Reed‚ÄìSolomon codes described in Chapter 3, which can be viewed
as the simplest type of algebraic‚Äìgeometric code. The small number of points on
the projective line is the reason why Reed‚ÄìSolomon codes only have short code
lengths ‚Äì no greater than the size of the Ô¨Ånite Ô¨Åeld.
Example 2.4: Rational functions on the Hermitian curve: It is well known that
a sequence of rational functions on the projective Hermitian curve in (2.1) can be
formed from x
z and y
z [3]. The term x
z can be rewritten as:
xw+1 = ywz + yzw
x
z = yw + yzw‚àí1
xw
,

66
BASIC PRINCIPLES OF NON-BINARY CODES
which has an order of w at the point at inÔ¨Ånity Q = (0, 1, 0). Similarly, the term y
z
can be rewritten as:
xw+1 = ywz + yzw
ywz + yzw
xw+1
= 1
1
z = yw + yzw‚àí1
xw+1
y
z = yw+1 + y2zw‚àí1
xw+1
,
which has an order of w + 1 at the point at inÔ¨Ånity Q = (0, 1, 0). Therefore,
for the Hermitian curve deÔ¨Åned over GF(22) in (2.3) with r = 2, v
 x
z

= 2 and
v
 y
z

= 3, other rational functions on the curve are formed by combining the
products of different powers of x
z and y
z and adding their orders together. For
example, x
z ¬∑ x
z = x2
z2 has an order of 2 + 2 = 4, x
z ¬∑ y
z = xy
z2 has an order of 2 +
3 = 5, y
z ¬∑ y
z = y2
z2 has an order of 3 + 3 = 6 and so on.
In general, the rational function xi y j
zi+ j on the Hermitian curve has an order of [3]:
v
xi y j
zi+ j

= iw + j(w + 1).
(2.8)
This sequence of rational functions is denoted by L(G), where G is a divisor on
the curve. A divisor of a curve assigns an integer value to every point of the curve.
An AG code is deÔ¨Åned by two divisors, D and G. The divisor D assigns the value
D(P) = 1 to every afÔ¨Åne point and is the sum of all the afÔ¨Åne points [3, 4]:
D =
n

i=1
D(Pi)Pi =
n

i=1
Pi.
(2.9)
The sum n
i=1 D(P) is called the degree of divisor D, d(D).
Similarly, the divisor G assigns an integer value G(Q) to each point at inÔ¨Ånity
Q and is deÔ¨Åned as the sum of all points at inÔ¨Ånity. For curves with one point at
inÔ¨Ånity, the divisor G is just the point at inÔ¨Ånity multiplied by the degree of G,
d(G).
G =

i
G(Qi)Qi = d(G)Q
(2.10)
The space L(G) contains rational functions of order up to d(G). Continuing the
example above, if G = 7Q with d(G) = 7 then:
L(7Q) =

1, x
z , y
z , x2
z2 , xy
z2 , y2
z2 , x3
z3 , x2y
z3
%
.

ALGEBRAIC GEOMETRY PRINCIPLES
67
Notice that the order of x3
z3 is 6, which is the same as the order of y2
z2 . In this case
all functions with a power of x greater than or equal to 3 are removed to avoid
functions with the same order. Alternatively, all functions with a power of y greater
than 1 can be removed. Therefore, there are seven rational functions in L(G):
L(G) =

1, x
z , y
z , x2
z2 , xy
z2 , y2
z2 , x2y
z3
%
.
2.2.5 Riemann‚ÄìRoch Theorem
The Riemann‚ÄìRoch theorem can be used to calculate the number of rational functions
in L(G) with order up to and including d(G), and hence to determine the dimension
and minimum distance of the code. The number of rational functions in L(G) is called
the dimension of G, l(G). The theorem states that there exists a nonnegative integer Œ≥
such that [3, 4]:
l(G) ‚àíd(G) = 1 ‚àíŒ≥ ,
(2.11)
provided that d(G) > 2Œ≥ ‚àí2.
The nonnegative integer Œ≥ is called the genus and is deÔ¨Åned as [3, 4]:
Œ≥ = (m ‚àí1)(m ‚àí2)
2
,
(2.12)
where m is the degree of the curve, and plays an important part in the size of the code
parameters.
Example 2.5: The number of rational functions on the Hermitian curve with
pole order less than or equal to 21 at the point at inÔ¨Ånity Q = (0, 1, 0): The
rational function x
z has an order of w = 4, and y
z has an order of w + 1 = 5.
Therefore, the sequence of functions of order up to and including 21 is:
L(21Q)=

1, x
z , y
z , x2
z2 , xy
z2 , y2
z2 , x3
z3 , x2y
z3 , xy2
z3 , y3
z3 , x4
z4 , x3y
z4 , x2y2
z4 , xy3
z4 , y4
z4 , x4y
z5
%
.
All functions with a power of x greater than 4 have been removed to avoid
duplicate orders, so there is no x5
z5 in L(G) as its order is the same as œÖ( y4
z4 ). The
orders of these functions are:
{1, 4, 5, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21}
and there are 16 functions in L(G). These numbers are also known as nongaps.
Also, the orders 0, 2, 3, 6, 7 and 11 are not present, and these are known as gaps.
The number of gaps is equal to Œ≥ .

68
BASIC PRINCIPLES OF NON-BINARY CODES
The degree of the Hermitian curve is m = 5 and, from (2.12), the genus is
Œ≥ = 6 and d(G) = 21. Next, 2Œ≥ ‚àí2 = 10, which is less than d(G), so applying the
Riemman‚ÄìRoch theorem in (2.11) gives:
l(G) = d(G) + 1 ‚àíŒ≥
= 21 + 1 ‚àí6
= 16
,
which is equal to the number of functions in L(G).
2.2.6 The Zero Order of a Monomial
We can deÔ¨Åne a set Z of monomials œàpi,u(x, y) with a zero of order u at an afÔ¨Åne
point Pi as [6]:
œàpi,u(x, y) = œàpi,Œª+(w+1)Œ¥(x, y) = (x ‚àíxi)Œª[(y ‚àíyi) ‚àíxw
i (x ‚àíxi)]Œ¥,
(2.13)
where Œª, Œ¥ ‚ààN, 0 ‚â§Œª ‚â§w and Œ¥ ‚â•0. To evaluate the zero order, œàpi,Œ± is divided
by (x ‚àíxi) until a unit has been obtained. The zero order is equal to the number of
divisions.
Example 2.6: Determining the zero order of a monomial at the afÔ¨Åne point
(1, Œ±) on the Hermitian curve: The Ô¨Årst eight monomials with respect to the point
pi = (xi, yi) = (1, Œ±) from (2.13) are determined by:
œàpi,u(x, y) = œà(1,Œ±),Œª+3Œ¥(x, y) = (x ‚àí1)Œª[(y ‚àía) ‚àí12(x ‚àí1)]Œ¥,
where w = 2 and 0 ‚â§Œª ‚â§2.
Therefore:
œàpi,0(x, y) = (x ‚àí1)0[(y ‚àíŒ±) ‚àí12(x ‚àí1)] = 1
œàpi,1(x, y) = (x ‚àí1)1[(y ‚àíŒ±) ‚àí12(x ‚àí1)]0 = 1 + x
œàpi,2(x, y) = (x ‚àí1)2[(y ‚àíŒ±) ‚àí12(x ‚àí1)]0 = 1 + x2
œàpi,3(x, y) = (x ‚àí1)0[(y ‚àíŒ±) ‚àí12(x ‚àí1)]1 = y + Œ± + x + 1 = Œ±2 + x + y
œàpi,4(x, y) = (x ‚àí1)1[(y ‚àíŒ±) ‚àí12(x ‚àí1)]1 = (x + 1)(y + Œ± + x + 1)
= Œ±2 + Œ±x + y + x2 + xy
œàpi,5(x, y) = (x ‚àí1)2[(y ‚àíŒ±) ‚àí12(x ‚àí1)]1 = (x2 + 1)(y + Œ± + x + 1)
= Œ±2 + x + Œ±2x2 + y2 + x2y
œàpi,6(x, y) = (x ‚àí1)0[(y ‚àíŒ±) ‚àí12(x ‚àí1)]2 = (y + Œ± + x + 1)2 = Œ± + x2 + y2
œàpi,7(x, y) = (x ‚àí1)1[(y ‚àíŒ±) ‚àí12(x ‚àí1)]2 = (x + 1)(y + Œ± + x + 1)2
= Œ± + Œ±x + y + x2 + xy2.

ALGEBRAIC GEOMETRY PRINCIPLES
69
To evaluate a function‚Äôs zero order at an afÔ¨Åne point of the Hermitian curve xw+1
+ yw + y = 0, it is important to have the following equation associated with the
curve [6]:
y ‚àíyi
x ‚àíxi
= (x ‚àíxi)w + xi(x ‚àíxi)w‚àí1 + xw
i
e(y)
,
(2.14)
where e( y) = ( y ‚àíyi)w‚àí1 + 1. Notice that e( yi) = ( yi ‚Äì yi)w‚àí1 + 1 = 1.
It can be seen that œàpi,3(x, y) = (y ‚àíŒ±) ‚àí(x ‚àí1) and e( y) = ( y ‚àíŒ±) + 1.
Initialize œà(0)(x, y) = œàpi,3(x, y) = (y ‚àíŒ±) ‚àí(x ‚àí1).
The Ô¨Årst division:
œà(1)(x, y) = œà(0)(x, y)
x ‚àí1
= y ‚àíŒ±
x ‚àí1 ‚àí1 = (x ‚àí1)2 + (x ‚àí1) + 1
e(y)
‚àí1
= (x ‚àí1)2 + (x ‚àí1) + 1 ‚àí(y ‚àíŒ±) ‚àí1
e(y)
= (x ‚àí1)e(y)‚àí1 + (y ‚àíŒ±)e(y)‚àí1 + (x ‚àí1)2e(y)‚àí1.
We have œà(1)( pi) = (1 ‚àí1) ¬∑ 1 + (Œ± ‚àíŒ±) ¬∑ 1 + (1 ‚àí1)2 ¬∑ 1 = 0.
The second division:
œà(2)(x, y) = œà(1)(x, y)
x ‚àí1
= e(y)‚àí1 ‚àíy ‚àíŒ±
x ‚àí1 e(y)‚àí1 + (x ‚àí1)e(y)‚àí1
= e(y)‚àí1 ‚àí[(x ‚àí1)2 + (x ‚àí1) + 1]e(y)‚àí2 + (x ‚àí1)e(y)‚àí1
= (e(y)‚àí1 ‚àíe(y)‚àí2) ‚àí(x ‚àí1)(e(y)‚àí2 ‚àíe(y)‚àí1) ‚àí(x ‚àí1)2e(y)‚àí2.
We have œà(2)( pi) = (1 ‚Äì 1) ‚Äì (1 ‚àí1) ¬∑ (1 ‚Äì 1) ‚Äì (1 ‚Äì 1)2 ¬∑ 1 = 0.
The third division:
œà(3)(x, y) = œà(2)(x, y)
x ‚àí1
= e(y)‚àí1 ‚àíe(y)‚àí2
x ‚àí1
‚àí(e(y)‚àí2 ‚àíe(y)‚àí1) ‚àí(x ‚àí1)e(y)‚àí2
=
e(y) ‚àí1
(x ‚àí1)e(y)2 ‚àí(e(y)‚àí2 ‚àíe(y)‚àí1) ‚àí(x ‚àí1)e(y)‚àí2.
= y ‚àíŒ±
x ‚àí1 e(y)‚àí2 ‚àí(e(y)‚àí2 ‚àíe(y)‚àí1) ‚àí(x ‚àí1)e(y)‚àí2
= [(x ‚àí1)2 + (x ‚àí1) + 1]e(y)‚àí3 ‚àí(e(y)‚àí2 ‚àíe(y)‚àí1) ‚àí(x ‚àí1)e(y)‚àí2
= (e(y)‚àí3 ‚àíe(y)‚àí2 + e(y)‚àí1) + (x ‚àí1)(e(y)‚àí3
‚àíe(y)‚àí2) + (x ‚àí1)2e(y)‚àí3.
We have œà(3)( pi) = (1 ‚Äì 1 + 1) + (1 ‚àí1) ¬∑ (1 ‚Äì 1) + (1 ‚àí1)2 ¬∑ 1 = 1 = 0.

70
BASIC PRINCIPLES OF NON-BINARY CODES
There are 3 divisions in order obtain a unit. Therefore, the zero order of œàpi,3 at
pi is 3 as: v pi(œàpi,3) = 3. œàpi,3 can also be written as: œàpi,3 = (x ‚àí1)3[(e(y)‚àí3 ‚àí
e(y)‚àí2 + e(y)‚àí1) + (x ‚àí1)(e(y)‚àí3 ‚àíe(y)‚àí2) + (x ‚àí1)2e(y)‚àí3].
2.2.7 AG Code Parameters
There are two types of AG code: Functional Goppa codes, CL, and Residue Goppa
codes, C , which are the dual of functional Goppa codes. In both cases the block
length is the number of afÔ¨Åne points, n.
For functional Goppa codes the message length is the number of rational functions
in the divisor L( G), as given by the Reimann‚ÄìRoch theorem [3, 4]:
k = l(G) = d(G) + 1 ‚àíŒ≥.
(2.15)
An accurate value of the Hamming distance of AG codes cannot always be calcu-
lated, so a lower bound called the designed minimum distance d
‚àóis used. The genus of
the curve plays an important role in determining the distance of the code. The optimal
Hamming distance occurs when the Singleton bound is met [1, 2]:
d = n ‚àík + 1.
(2.16)
However, for AG codes the genus of the curve penalizes the minimum distance [4]:
d‚àó= n ‚àík ‚àíŒ≥ + 1.
(2.17)
In this case a large genus will reduce the designed minimum distance of the code,
but long codes such as the Hermitian codes have a large genus. Reed‚ÄìSolomon codes
are constructed from an afÔ¨Åne line, which has degree of one and a genus equal to zero.
Therefore they do not suffer any genus penalty and have optimal Hamming distances.
By substituting (2.15) into (2.16) the designed minimum distance of the code is [4]:
d‚àó= n ‚àí(d(G) + 1 ‚àíŒ≥ ) ‚àíŒ≥ + 1
= n ‚àíd(G)
.
(2.18)
For residue Goppa codes the message length k of the code is simply the code length
minus the message length of the functional Goppa codes, that is [4]:
k = n ‚àíl(G) = n ‚àíd(G) ‚àí1 + Œ≥.
(2.19)
Substituting (2.19) into (2.17) gives the designed minimum distance of the residue
Goppa code [4]:
d‚àó= n ‚àí(n ‚àíd(G) ‚àí1 + Œ≥ ) ‚àíŒ≥ + 1
= d(G) ‚àí2Œ≥ + 2
.
(2.20)

REFERENCES
71
2.3 Conclusions
This chapter has presented some of the more important mathematical concepts as-
sociated with non-binary error-correcting codes, explaining groups, rings and Ô¨Åelds,
minimal polynomials and cyclotomic cosets, and introducing algebraic geometry.
These are all important tools for the design of binary and non-binary error-correcting
codes and will be used often in the remaining chapters of this book.
Most of this chapter was dedicated to the principles of algebraic geometry since
the construction and decoding of algebraic‚Äìgeometric codes is more mathematically
intensive than the other non-binary coding schemes covered in this book.
References
[1] Lin, S. and Costello, D.J. Jr. (2004) Error Control Coding, 2nd edn, Pearson Prentice Hall, ISBN
0-13-017973-6.
[2] Wicker, S.B. (1995) Error Control Systems for Digital Communication and Storage, Prentice Hall,
Eaglewood Cliffs, NJ, ISBN 978-0132008099.
[3] Blake, I.F., Heegard, C., Hoholdt, T. and Wei, V. (1998) Algebraic geometry codes. IEEE Trans.
Inform. Theory, 44 (6), 2596‚Äì618.
[4] Pretzel, O. (1998) Codes and Algebraic Curves, Oxford Science Publications, Oxford University
Press (Oxford Lecture Series in Mathematics and its Applications 8), ISBN 0198500394.
[5] Driencourt, Y. (1985) Some properties of elliptic codes over a Ô¨Åeld of characteristic 2. Proceedings
of AAECC-3, 229, 185‚Äì93, Lecture Notes in Computer Science.
[6] H√∏holdt, T. and Nielsen, R.R. (1999) Decoding Hermitian codes with Sudan‚Äôs algorithm, in Applied
Algebra, Algebraic Algorithms and Error-Correcting Codes, Vol. 1719, Springer-Verlag, Berlin,
Germany, pp. 260‚Äì70 (Lecture Notes in Computer Science).


3
Non-Binary Block Codes
3.1 Introduction
The two previous chapters have covered the necessary mathematics to enable the
reader to understand the theory of error-correcting codes. From now on, this chapter
and succeeding chapters will present the different types and classes of binary and
non-binary error-correcting code, focusing on their design, construction and decoding
algorithms. In this chapter, we Ô¨Årst introduce binary block codes, beginning with the
Hamming code, one of the Ô¨Årst block codes presented in 1953 by Hamming [1]. It
will be seen that this coding scheme is not suitable for practical applications, but
these codes are simple enough to demonstrate encoding and decoding block codes.
Following this, a very important type of block code known as the cyclic code is
presented. The cyclic code has improved error-correction and its encoder can be
simply implemented using shift registers. The Ô¨Årst important class of cyclic code is
the Bose‚ÄìChaudhuri‚ÄìHocquengem (BCH) code [2, 3]. We describe the construction
of binary BCH codes and then move on to the construction of non-binary BCH codes
deÔ¨Åned over Ô¨Ånite Ô¨Åelds. This will lead to the most important and commonly used class
of non-binary BCH code, called the Reed‚ÄìSolomon code [4]. Two different decoding
algorithms are given to decode non-binary BCH codes: Euclid‚Äôs algorithm [5] and
the Berlekamp‚ÄìMassey algorithm [6, 7].
We then explain the concept of coded modulation presented by Ungerboeck [8],
whereby the encoding and modulation processes are treated as a single entity and the
constellation set is increased to compensate for the redundancy in a code word, thus
keeping the data rate constant without requiring the bandwidth use to be expanded.
Block Coded Modulation (BCM) employs simple block codes and maps the symbols
of the code word to an expanded constellation set, while at the same time maximizing
the Euclidean distance between adjacent symbols. Binary BCM codes are introduced,
covering their encoding procedure and their decoding by constructing the trellis
diagram of the BCM codes and using the well-known Viterbi algorithm [9] to recover
the original message. This work is then extended to non-binary BCM codes and we
Non-Binary Error Control Coding for Wireless Communication and Data Storage
Rolando Antonio Carrasco and Martin Johnston
C 2008 John Wiley & Sons, Ltd

74
NON-BINARY BLOCK CODES
demonstrate that the performance can be improved with only a small increase in
complexity.
The chapter Ô¨Ånishes by discussing the applications of non-binary block codes
related to wireless communications, such as a cellular environment and Ô¨Åxed and
wireless broadband access (WiMax), and to magnetic and optical data storage.
3.2 Fundamentals of Block Codes
In any error-correcting coding scheme a number of redundant bits must be included in
the original message in order for it to be possible to recover that message at the receiver.
In general, these redundant bits are determined by algebraic methods. All block codes
are deÔ¨Åned by their code word length n, their message length (or dimension) k and
their minimum Hamming distance d (described later). When referring to a block code,
it is written as a (n, k, d) block code.
One of the simplest types of block code is the parity check code, where for each
binary message an extra bit, known as a parity check bit, is appended to the message
to ensure that each code word has an even number of 1s. In this case the message is
k bits in length and therefore the code word length n = k + 1. For example, if k = 3
then the code word length is n = 4 and the code is:
000 ‚Üí0000
001 ‚Üí0011
010 ‚Üí0101
011 ‚Üí0110.
100 ‚Üí1001
101 ‚Üí1010
110 ‚Üí1100
111 ‚Üí1111
It can be seen that if a single error is added to any of the code words it will result
in an odd number of 1s in the received word and will be detected. Unfortunately, the
code will not know the location of the error in the received word. Hence, parity check
codes can only detect one error and can correct none. To obtain the parity check bit,
one simply adds the three message bits modulo-2. If the message bits are denoted as
u1, u2 and u3 and the parity check bit is denoted as p1 then the parity check equation
for the (4, 3) parity check code is:
p1 = u1 ‚äïu2 ‚äïu3.
(3.1)
A simple encoder for this is illustrated in Figure 3.1.
The number of errors that a block code can detect and correct is determined by its
minimum Hamming distance d. This is deÔ¨Åned as the minimum number of places
where any two code words differ. For example, by observation the minimum Hamming

FUNDAMENTALS OF BLOCK CODES
75
u1
u2
u3
u1
u2
u3
p1
Figure 3.1
A simple encoder for the (4, 3) parity check code.
distance of the above parity check code is d = 2. Hence, if any two bits of a code word
are Ô¨Çipped this will result in another valid code word, that is if two errors occurred
they would not be detected. In general, the number of errors v that can be detected for
a block code is:
v = d ‚àí1.
(3.2)
The error detection and correction of a block code can be improved by increasing
the number of parity check bits or equivalently increasing the number of parity check
equations. For example, take the following three parity check equations:
p1 = u1 ‚äïu3 ‚äïu4
p2 = u1 ‚äïu2 ‚äïu3.
p3 = u2 ‚äïu3 ‚äïu4
(3.3)
Each parity check bit in (3.3) is independent in order to ensure the minimum
Hamming distance is maximized. The corresponding encoder is given in Figure 3.2.
This is one of the Ô¨Årst block codes known as Hamming codes. These have the following
properties:
Codeword length n = 2m ‚àí1
Message length k = 2m ‚àím ‚àí1
Minimum Hamming distance d = 3,
where m ‚â•3 and is a positive integer.
u1
u2
u3
u4
u1
u2
u3
u4
p1
p2
p3
Figure 3.2
Encoder for the parity check equations of (3.3).

76
NON-BINARY BLOCK CODES
A list of the code words for the (7, 4, 3) Hamming code is shown below, with the
parity bits highlighted:
0000 ‚Üí0000000
0001 ‚Üí0001101
0010 ‚Üí0010111
0011 ‚Üí0011010
0100 ‚Üí0100011
0101 ‚Üí0101110
0110 ‚Üí0110100
0111 ‚Üí0111001
1000 ‚Üí1000110
1001 ‚Üí1001011
1010 ‚Üí1010001.
1011 ‚Üí1011100
1100 ‚Üí1100101
1101 ‚Üí1101000
1110 ‚Üí1110010
1111 ‚Üí1111111
From (3.2), the Hamming code can detect two errors, improving on the parity check
code. However, we do not know how many errors this code can correct. We know that
changing a code word in d positions can result in another valid code word. Each code
word can be interpreted as the centre of a circle of a certain radius t that contains all
other vectors that differ from the code word in t places or less. Therefore, if t = d
then two neighbouring code words will overlap, as shown in Figure 3.3. If t = d/2
then the two circles will meet at their circumferences. This implies that for t < d/2 the
circle will contain unique n-dimensional vectors that do not occur in any other circle.
Hence, if the received word matches any of these vectors we know that it can only be
one code word. So the number of errors t that a block code can correct up to is:
t <
&d
2
'
or
t ‚â§
&d ‚àí1
2
'
,
(3.4)
where  is the Ô¨Çoor function and any value inside is rounded down to the nearest
integer. This is important for the situation where d is even and the number of errors
that can be corrected would not be an integer. Since the Hamming code has a minimum
Hamming distance d = 3 it can correct one error.

FUNDAMENTALS OF BLOCK CODES
77
Figure 3.3
Representation of two code words that differ in d places. Any vector that differs from a
code word in less than d/2 places is unique and can therefore be corrected.
3.2.1 Generator and Parity Check Matrices
The encoders of Figures 3.1 and 3.2 can be represented as a (k √ó n) matrix for
convenience. Each column in the matrix corresponds to the connections from the
message and the code word, that is a connection is denoted by a 1 and no connection
by a 0. For example, the matrix for the parity check code is written as:
H =
Ô£Æ
Ô£∞
1
0
0
1
0
1
0
1
0
0
1
1
Ô£π
Ô£ª.
Taking the Ô¨Årst column, there is only one connection in Figure 3.1 between the Ô¨Årst
element of the code vector and the Ô¨Årst element of the message vector, and this is
denoted as 1. There are no connections between the Ô¨Årst elements of the code vector
and the second and third elements in the message vector so these are both denoted as 0.
Similarly, for the last column there are connections between the Ô¨Ånal element of the
code vector and all three elements of the message vector so this column contains all
1s. Multiplying a 3-bit message by this matrix will generate a code word, so the matrix
is called a generator matrix, denoted as G. The generator matrix of the Hamming
code encoder in Figure 3.2 is:
G =
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
0
0
0
1
1
0
0
1
0
0
0
1
1
0
0
1
0
1
1
1
0
0
0
1
1
0
1
Ô£π
Ô£∫Ô£∫Ô£ª.
(3.5)
It can be seen from (3.5) that every code word can be obtained by adding
different combinations of the rows in G. For example, encoding the message

78
NON-BINARY BLOCK CODES
u = [1 1 1 1] gives:
c = u ¬∑ G = 1 ¬∑
	1 0 0 0 1 1 0
‚äï1 ¬∑
	0 1 0 0 0 1 1
‚äï1 ¬∑
	0 0 1 0 1 1 1
‚äï
1 ¬∑
	0 0 0 1 1 0 1
=
	1 1 1 1 1 1 1
.
A second important matrix in error-correction coding is the parity check matrix,
denoted by H. It has the property that any row in H is orthogonal to the rows of G,
that is the inner product of a row in G and a row in H will be zero. This is important
as it allows us to validate any code word in a block code since multiplying a code
word by H should give an all-zero vector, indicating that the code word is valid, as
given by [10]:
H ¬∑ cT = 0,
(3.6)
where cT is the transpose of a code word and 0 is the (n ‚Äì k) √ó 1 column vector
containing all zeros. It can be seen that G is of the form:
G = [Ik|P],
(3.7)
where Ik is the k √ó k identity matrix and P is a parity matrix. This is called a systematic
generator matrix and generates a code word which contains the original message with
parity bits appended to it. The (n ‚Äì k) √ó n systematic parity check matrix is related to
G by:
H =
	
PT|In‚àík

,
(3.8)
where PT is the transpose of P and In‚àík is the (n ‚Äì k) √ó (n ‚Äì k) identity matrix. The parity
check matrix of the (4, 3, 2) parity check code is, by (3.8), simply H =
	1 1 1 1
.
Similarly, the parity check matrix for the (7, 4, 3) Hamming code is:
H =
Ô£Æ
Ô£∞
1
0
1
1
1
0
0
1
1
1
0
0
1
0
0
1
1
1
0
0
1
Ô£π
Ô£ª.
(3.9)
As an example, multiplying H by the transpose of the code word c =
	1 0 0 0 1 1 0
results in:
H ¬∑ cT =
Ô£Æ
Ô£∞
1
0
1
1
1
0
0
1
1
1
0
0
1
0
0
1
1
1
0
0
1
Ô£π
Ô£ª.
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
0
0
0
1
1
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£∞
0
0
0
Ô£π
Ô£ª.

FUNDAMENTALS OF BLOCK CODES
79
3.2.2 Decoding Block Codes
As well as validating code words, the parity check matrix can also be used to decode
a received word containing errors. It has been seen that multiplying the parity check
matrix by any code word results in an all-zero column vector. Therefore, if the parity
check matrix is multiplied by a received word containing errors, the column vector
will contain nonzero elements. These values are known as syndromes and the column
vector is a syndrome vector, s [10].
s = H ¬∑ rT,
(3.10)
where r is the received vector deÔ¨Åned as r = c + e and e is an error vector. Therefore,
(3.10) can be written as:
s = H ¬∑ (c + e)T = H ¬∑ cT + H ¬∑ eT = H ¬∑ eT.
(3.11)
We can see that the syndromes are only dependent on the error pattern. The
(7, 4, 3) Hamming code can correct a single error and so there are seven possible
error patterns:
	1 0 0 0 0 0 0
,
	0 1 0 0 0 0 0
,
	0 0 1 0 0 0 0
,
	0 0 0 1 0 0 0
,
	0 0 0 0 1 0 0
,
	0 0 0 0 0 1 0
and
	0 0 0 0 0 0 1
.
Multiplying H by the transpose of each of these error patterns gives the seven
unique syndrome vectors.
Now suppose the code word c =
	1 0 0 0 1 1 0
has been transmitted and the
received vector r =
	1 1 0 0 1 1 0
has a single error in its second position as high-
lighted. From (3.10) the syndrome vector is:
Ô£Æ
Ô£∞
1 0 1 1 1 0 0
1 1 1 0 0 1 0
0 1 1 1 0 0 1
Ô£π
Ô£ª.
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
1
0
0
1
1
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£∞
0
1
1
Ô£π
Ô£ª.
Looking up this syndrome vector in Table 3.1 shows that the associated error pattern
is e =
	0 1 0 0 0 0 0
. Therefore, adding this error pattern to the received word gives
the decoded code word ÀÜc:
1100110
‚äï0100000 .
1000110

80
NON-BINARY BLOCK CODES
Table 3.1
Error patterns and their associated syndrome
vectors for the (7, 4, 3) Hamming code.
Error pattern e
Syndrome vector s
1 0 0 0 0 0 0
1 1 0
0 1 0 0 0 0 0
0 1 1
0 0 1 0 0 0 0
1 1 1
0 0 0 1 0 0 0
1 0 1
0 0 0 0 1 0 0
1 0 0
0 0 0 0 0 1 0
0 1 0
0 0 0 0 0 0 1
0 0 1
However, for block codes that can correct more than one error this method becomes
prohibitive.
3.3 Cyclic Codes
A cyclic code is a block code which has the property that for any code word, a
cyclic shift of this code word results in another code word [11]. Cyclic codes have
the advantage that simple encoders can be constructed using shift registers, and low-
complexity decoding algorithms exist to decode them. A cyclic code is constructed
by Ô¨Årst choosing a generator polynomial g(x) and multiplying this by a message
polynomial m(x) to generate a code word polynomial c(x).
3.3.1 Polynomials
There are two conventions for representing polynomials. A polynomial f(x) can be
of the form f 0 + f 1x + f 2x2 + ¬∑ ¬∑ ¬∑ fnxn, where fi ‚ààGF(q) and i = 0, 1, . . . , n. In
this case the corresponding vector would be [f 0 f 1 f 2 . . . fn]. Alternatively, f(x) can
be of the form fnxn + fn‚àí1xn‚àí1 + ¬∑ ¬∑ ¬∑ + f0 and the corresponding vector is [fn fn‚àí1
fn‚àí2 . . . f 0]. Both conventions are acceptable but in this book the latter representation
is chosen. So for the vector [1 1 0 1] the polynomial would be x3 + x2 + 1.
The (7, 4, 3) Hamming code is actually also a cyclic code and can be constructed
using the generator polynomial g(x) = x3 + x2 + 1.
For example, to encode the binary message 1010 we Ô¨Årst write it as the message
polynomial m(x) = x3 + x and then multiply it with g(x):
c(x) = m(x)g(x)
= (x3 + x)(x3 + x2 + 1)
= x6 + x5 + x3 + x4 + x3 + x.
= x6 + x5 + x4 + x
This code word polynomial corresponds to 1 1 1 0 0 1 0. The complete list of code
words is shown in Table 3.2.

CYCLIC CODES
81
Table 3.2
Code words for the (7, 4, 3) cyclic Hamming code.
Binary
Message
Code word
Binary
message
polynomial m(x)
polynomial c(x)
code word
0000
0
0
0000000
0001
1
x3 + x2 + 1
0001101
0010
x
x4 + x3 + x
0011010
0011
x + 1
x4 + x2 + x + 1
0010111
0100
x2
x5 + x4 + x2
0110100
0101
x2 + 1
x5 + x4 + x3 + 1
0111001
0110
x2 + x
x5 + x3 + x2 + x
0101110
0111
x2 + x + 1
x5 + x + 1
0100011
1000
x3
x6 + x5 + x3
1101000
1001
x3 + 1
x6 + x5 + x2 + 1
1100101
1010
x3 + x
x6 + x5 + x4 + x
1110010
1011
x3 + x + 1
x6 + x5 + x4 + x3 + x2 + x + 1
1111111
1100
x3 + x2
x6 + x4 + x3 + x2
1011100
1101
x3 + x2 + 1
x6 + x4 + 1
1010001
1110
x3 + x2 + x
x6 + x2 + x
1000110
1111
x3 + x2 + x + 1
x6 + x3 + x + 1
1001011
Taking a code word from Table 3.2 and shifting each element results in another
code word. For example, the code word 0110100 shifted one place to the left gives
the vector 1101000, which is another valid code word in Table 3.2.
Multiplying a message polynomial by the generator polynomial is equivalent to
multiplying the binary message by a generator matrix G where each row contains the
coefÔ¨Åcients of g(x) shifted one place to the right with respect to the previous row.
For the (7, 4) cyclic Hamming code the generator matrix would be:
G =
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
1
0
1
0
0
0
0
1
1
0
1
0
0
0
0
1
1
0
1
0
0
0
0
1
1
0
1
Ô£π
Ô£∫Ô£∫Ô£ª.
Multiplying the message m =
	1 0 1 0
by G gives the code word:
	1 0 1 0
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
1
0
1
0
0
0
0
1
1
0
1
0
0
0
0
1
1
0
1
0
0
0
0
1
1
0
1
Ô£π
Ô£∫Ô£∫Ô£ª=
	1 1 1 0 0 1 0
,
which matches the code word in Table 3.2.
It can be seen that the code words of the cyclic code are nonsystematic and so
recovering messages is more difÔ¨Åcult. One method would be to divide the code word
polynomial by the generator polynomial to obtain the message polynomial, but this

82
NON-BINARY BLOCK CODES
is an unfeasible method for larger cyclic codes. However, it is desirable for the code
word to be in systematic form so that it is simpler to recover the transmitted message.
3.3.2 Systematic Cyclic Codes
A systematic cyclic code word is made up of two polynomials: a message polynomial
m(x) and a parity polynomial p(x), where the degree of p(x) is less than (n ‚àík). The
message polynomial is of the form:
m(x) = mkxk + mk‚àí1xk‚àí1 + ¬∑ ¬∑ ¬∑ + m1x + m0,
where k is the message length. It is Ô¨Årst multiplied by xn‚àík, resulting in:
xn‚àíkm(x) = mnxn + mn‚àí1xn‚àí1 + ¬∑ ¬∑ ¬∑ + mn‚àík+1xn‚àík+1 + mn‚àíkxn‚àík.
It is then divided by the generator polynomial g(x), giving [10]:
xn‚àíkm(x)
g(x)
= q(x)
   +
quotient
+ p(x)
g(x)
   +
remainder
‚áíxn‚àíkm(x) + p(x)
g(x)
= q(x)
,
(3.12)
where q(x) is a quotient polynomial. From (3.12) it can be seen that the term
xn‚àíkm(x) + p(x) is a valid code word polynomial since it is a factor of g(x).
Example 3.1:
Generating a systematic code word using the (7, 4, 3) cyclic
Hamming code: First multiply m(x) by xn‚àík = x3:
x3m(x) = x6 + x4.
Now divide this by the generator polynomial g(x) = x3 + x2 + 1:
x3 + x2 + 1
x3 + x2 + 1

x6
+ x4
x6 + x5
+ x3
x5 + x4 + x3
x5 + x4
+ x2
x3 + x2
x3 + x2 + 1
1

CYCLIC CODES
83
Switch 2
Input message symbol sequence
Switch 1
g0
g1
g2
g3
gn‚Äìk‚Äì2
gn‚Äìk‚Äì1
xn‚Äìk
xn‚Äìk‚Äì1
xn‚Äìk‚Äì2
x3
x2
x
x0
Figure 3.4
A general systematic cyclic code encoder.
In this case the quotient is q(x) = x3 + x2 + 1 and the remainder of parity
polynomial is p(x) = 1. Hence the systematic code word polynomial c(x) = m(x) +
p(x) = x6 + x4 + x2 + 1 and the code word is c =
	1 0 1 0 0 0 1
.
This operation can be achieved using an (n ‚Äì k)-stage shift register, as shown
in Figure 3.4. Initially, the shift register is initialized to zero and each one of the
k input bits is fed into the (n ‚Äì k)-stage shift register. At the same time switch
2 ensures that the output also has those k input bits. After the Ô¨Ånal bit is fed in,
switch 1 is open and switch 2 moves to its second position, emptying the contents
of the shift registers, which are the redundant parity bits. An example of the (7, 4, 3)
cyclic Hamming code systematic encoder is shown in Figure 3.5. This is a (n ‚Äì k) =
7 ‚Äì 4 = 3-stage shift register in which each delay element corresponds to a power
of x in the parity polynomial p(x).
Switch 1
Switch 2
x
x2
1
0
1
x3
Feedback
Input message symbol sequence
1
1010
Figure 3.5
The systematic encoder for the (7, 4, 3) cyclic Hamming code.

84
NON-BINARY BLOCK CODES
Table 3.3
Contents of the shift register for each cycle in the
systematic encoder.
Shift register contents
Input
Feedback
1
x
x2
1
1
1
0
1
0
1
1
1
1
1
0
0
1
1
0
1
1
0
0
If the input message is m = [1 0 1 0] then Table 3.3 shows the contents of the
shift register at each cycle in the systematic encoder.
The contents of the shift register for the Ô¨Ånal information bit show that the
parity polynomial is p(x) = 0x2 + 0x + 1 = 1 and the systematic code word is
c =
	1 0 1 0 0 0 1
.
3.4 Bose‚ÄìChaudhuri‚ÄìHocquenghem (BCH) Codes
One of the most important classes of cyclic code is the Bose‚ÄìChaudhuri‚Äì
Hocquenghem (BCH) code [2, 3]. When constructing a cyclic code the actual value
of the minimum Hamming distance is unknown, but BCH codes have a lower bound
on the minimum Hamming distance, called the designed minimum distance, denoted
as d
‚àó. This is due to the BCH bound, deÔ¨Åned as follows [12].
The BCH Bound: For any q-ary (n, k) cyclic code, choose a value of m that
ensures the extension Ô¨Åeld GF(qm) is minimal and contains a primitive nth root of
unity Œ±. Choose a generator polynomial g(x) of minimal degree in GF(q)x so that
g(Œ±) = g(Œ±2) = g(Œ±4) = ¬∑ ¬∑ ¬∑ = g(Œ±d‚àó‚àí2).
From the above, the cyclic code has minimum distance d ‚â•d
‚àó[12].
The value of d
‚àóis determined by deciding how many errors the BCH code should
be able to correct. This can be achieved by rearranging (3.4) to get:
d‚àó= 2t + 1.
(3.13)
Example 3.2:
Constructing a binary BCH code of length n = 15 bits: The
BCH bound tells us that we must choose the smallest extension Ô¨Åeld GF(2m) with
an nth root of unity, which in this case is a 15th root of unity. The smallest extension
Ô¨Åeld that satisÔ¨Åes this is GF(16). Next, we must decide how many errors the BCH
code will be able to correct; for this example it can correct t = 2 errors, giving a
designed minimum distance of d
‚àó= 5. Finally, we need a generator polynomial g(x)
where g(Œ±) = g(Œ±2) = g(Œ±3) = g(Œ±4) = 0. To obtain this the minimal polynomials

BOSE‚ÄìCHAUDHURI‚ÄìHOCQUENGHEM (BCH) CODES
85
of GF(16) are required, which are given in Table 2.11, but are repeated here for
convenience:
Cyclotomic Cosets
Minimal Polynomials M(x)
{1, 2, 4, 8}
M1(x) = x4 + x + 1
{3, 6, 12, 9}
M3(x) = x4 + x3 + x2 + x + 1
{5, 10}
M5(x) = x2 + x + 1
(7, 14, 13, 11}
M7(x) = x4 + x3 + 1
The minimal polynomial for Œ± and Œ±2 is M1(x), the minimal polynomial for Œ±3
is M3(x), and for Œ±4 it is also M1(x), so to satisfy the BCH bound the generator
polynomial must be g(x) = M1(x)M3(x).
g(x) = (x4 + x + 1)(x4 + x3 + x2 + x + 1)
= x8 + x7 + x6+(1+1)x5+(1+1+1)x4 + (1+1)x3 + (1+1)x2 + (1+1)x+1.
= x8 + x7 + x6 + x4 + 1
This means that the message length of the code is k = n ‚Äì deg(g(x)) = 15 ‚Äì
8 = 7, resulting in the (15, 7, 5) binary BCH code.
The design procedure for constructing non-binary BCH codes is the same as for
the binary case.
Example 3.3:
Constructing a non-binary BCH code over GF(4) of length
n = 15 symbols: In this case we must now Ô¨Ånd the smallest extension Ô¨Åeld GF(4m)
with a 15th root of unity. Again this is the extension Ô¨Åeld GF(16) with m = 2. The
elements {0, 1, Œ±5, Œ±10} in GF(16) form a subÔ¨Åeld of GF(4), so let Œ≤ = Œ±5 be a
primitive element in GF(4). The cyclotomic cosets and minimal polynomials of
GF(42) are given in Table 3.4.
Table 3.4
Cyclotomic cosets and minimal polynomials for GF(42).
Cyclotomic cosets
Minimal polynomials M(x)
{1, 4}
M1(x) = x2 + x + Œ≤
{2, 8}
M2(x) = x2 + x + Œ≤2
{3, 12}
M3(x) = x2 + Œ≤2x + 1
{5}
M5(x) = x + Œ≤
{6, 9}
M6(x) = x2 + Œ≤x + 1
{7, 13}
M7(x) = x2 + Œ≤x + Œ≤
{10}
M10(x) = x + Œ≤2
{11, 14}
M11(x) = x2 + Œ≤2x + Œ≤2

86
NON-BINARY BLOCK CODES
To obtain Table 3.4, it must Ô¨Årst be observed that the conjugates of Œ±i in GF(42)
are Œ±4i, i = 1, 2, . . . , that is, the conjugates of Œ± are Œ±4, Œ±16, Œ±64, . . . , but Œ±16 =
Œ± and Œ±64 = Œ±4 so the cyclotomic coset would be{1, 4}. The associate minimal
polynomial is:
M1(x) = (x + Œ±)(x + Œ±4) = x2 + (Œ± + Œ±4)x + Œ±5
= x2 + x + Œ≤.
To correct two errors, the generator polynomial will be g(x) = M1(x)M2(x)M3(x):
g(x) = (x2 + x + Œ≤)(x2 + x + Œ≤2)(x2 + Œ≤2x + 1)
= [x4 + (1 + 1)x3 + (Œ≤2 + 1 + Œ≤)x2 + (Œ≤2 + Œ≤)x + 1](x2 + Œ≤2x + 1)
= (x4 + x + 1)(x2 + Œ≤2x + 1)
.
= x6 + Œ≤2x5 + x4 + x3 + (Œ≤2 + 1)x2 + (Œ≤2 + 1)x + 1
= x6 + Œ≤2x5 + x4 + x3 + Œ≤x2 + Œ≤x + 1.
There are two important features of this non-binary BCH code. Firstly, the
message length of this code is k = n ‚Äì deg(g(x)) = 15 ‚Äì 6 = 9, which means there
is less redundancy in the code than in the binary BCH code in Example 3.2 (the
binary BCH code has code rate of 0.47, compared with the 4-ary BCH code which
has a code rate of 0.6). Secondly, although both the (15, 7, 5) binary BCH code and
the (15, 9, 5) 4-ary BCH code correct up to two errors it should be noted that the
4-ary BCH code corrects two symbol errors. In GF(4) each element is represented
by two bits, which implies that it can correct up to 4 bit errors, provided the bit
errors span two symbols, whereas the binary BCH code can only correct up to 2
bit errors. Of course it must be remembered that the actual minimum Hamming
distance of these codes could be greater than the designed minimum distance and
so they may be able to correct more than two errors.
From these two examples, we now begin to see the advantages of using non-binary
block codes.
3.5 Reed‚ÄìSolomon Codes
A BCH code deÔ¨Åned over GF(q) of length n can always be constructed provided
that there is an nth root of unity in some extension Ô¨Åeld GF(qm). A special case
occurs if we construct a BCH code of length n = qm ‚àí1 over GF(qm). Obviously,
GF(qm) is the smallest extension Ô¨Åeld with a (qm ‚Äì 1)th root of unity. In this case,
each cyclotomic coset will contain only one element. For example, the conjugates of
the primitive element Œ± will be Œ±qm, Œ±2qm, . . . , which is equal to Œ±. Therefore, each
minimal polynomial of Œ±i will be (x ‚àíŒ±i), i = 1, 2, 3. . ., qm ‚àí2.

REED‚ÄìSOLOMON CODES
87
Example 3.4:
Constructing a non-binary BCH code over GF(16) of length
n = 15 symbols: The 15th root of unity in GF(16) is the primitive element Œ±
and the cyclotomic cosets and their associated minimal polynomials are given in
Table 3.5.
Table 3.5
Cyclotomic cosets and minimal polynomials for GF(16).
Cyclotomic cosets
Minimal polynomials M(x)
{1}
M1(x) = x + Œ±
{2}
M2(x) = x + Œ±2
{3}
M3(x) = x + Œ±3
{4}
M4(x) = x + Œ±4
{5}
M5(x) = x + Œ±5
{6}
M6(x) = x + Œ±6
{7}
M7(x) = x + Œ±7
{8}
M8(x) = x + Œ±8
{9}
M9(x) = x + Œ±9
{10}
M10(x) = x + Œ±10
{11}
M11(x) = x + Œ±11
{12}
M12(x) = x + Œ±12
{13}
M13(x) = x + Œ±13
{14}
M14(x) = x + Œ±14
To correct up to t = 2 symbol errors the generator matrix will be g(x) =
M1(x)M2(x)M3(x)M4(x).
g(x) = (x + Œ±)(x + Œ±2)(x + Œ±3)(x + Œ±4)
= (x2 + Œ±5x + Œ±3)(x2 + Œ±7x + Œ±7)
= x4 + (Œ±7 + Œ±5)x3 + (Œ±7 + Œ±12 + Œ±3)x2 + (Œ±12 + Œ±10)x + Œ±10
= x4 + Œ±13x3 + Œ±6x2 + Œ±3x + Œ±10.
The message length of the code is k = n ‚Äì deg(g(x)) = 15 ‚Äì 4 = 11, resulting in
a (15, 11, 5) 16-ary BCH code. Once again it can be seen that the code rate of this
code is even higher than the 4-ary BCH code of Example 3.3, and it can correct
two symbol errors, where one symbol now represents 4 bits. This means that it can
correct up to 8 bit errors, provided they span two symbols.
Non-binary BCH codes deÔ¨Åned over GF(qm) of length n = qm ‚Äì 1 are called
Reed‚ÄìSolomon codes [4] and are the best-performing BCH codes. Hence, the
non-binary BCH code of Example 3.4 is the (15, 11, 5) Reed‚ÄìSolomon code.
Reed‚ÄìSolomon codes also differ from other BCH codes in that their minimum
Hamming distance is equal to the designed minimum distance.

88
NON-BINARY BLOCK CODES
An important upper bound on the minimum Hamming distance in error-correction
is the Singleton Bound, deÔ¨Åned as [10]:
d ‚â§n ‚àík + 1.
(3.14)
The Reed‚ÄìSolomon code of Example 3.4 meets this bound, and this is the case
for all Reed‚ÄìSolomon codes. Any code with a minimum Hamming distance d = n ‚Äì
k + 1 is known as maximum distance separable (MDS) and has optimal minimum
Hamming distance. Therefore, Reed‚ÄìSolomon codes can correct:
t =
&d ‚àí1
2
'
=
&n ‚àík + 1 ‚àí1
2
'
=
&n ‚àík
2
'
.
(3.15)
Rearranging (3.15) gives a message length of k = n ‚Äì 2t. In summary,
Reed‚ÄìSolomon codes deÔ¨Åned over GF(qm) have the following parameters [12]:
Codeword length n = qm ‚àí1
Message length k = n ‚àí2t
Minimum Hamming distance d = n ‚àík + 1,
and the generator matrix g(x) is of the form [12]:
g (x) =
2t
,
i=1
(x + Œ±i).
(3.16)
The parity check matrix of a Reed‚ÄìSolomon code is [12]:
H =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
Œ±
Œ±2
¬∑ ¬∑ ¬∑
Œ±qm‚àí2
1
Œ±2
Œ±4
¬∑ ¬∑ ¬∑
Œ±qm‚àí3
1
Œ±3
Œ±6
¬∑ ¬∑ ¬∑
Œ±qm‚àí4
...
...
...
...
...
1
Œ±2t
Œ±4t
¬∑ ¬∑ ¬∑
Œ±qm‚àí2t‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(3.17)
To obtain the 2t syndromes the parity check matrix is multiplied by the transpose of
the received vector, as in (3.10). This is equivalent to substituting successive powers
of Œ± into the received vector.
3.6 Decoding Reed‚ÄìSolomon Codes
In Section 3.2.2 it was shown how syndromes could be determined from the received
word and used to identify the correct error pattern and obtain the originally transmitted
code word. This method becomes too complex for larger codes and alternative de-
coding algorithms are needed. To decode binary codes only the locations of the errors

DECODING REED‚ÄìSOLOMON CODES
89
in the received word are required, since the value at these location can be Ô¨Çipped,
that is a ‚Äò1‚Äô becomes a ‚Äò0‚Äô and vice versa. However, for a non-binary code an error
value can be many different values and a secondary process is needed to evaluate the
error value. So, two algorithms are required to decode a non-binary block code: an
error-locating algorithm and an error-evaluation algorithm.
For decoding we will make use of two polynomials: an error-locating polynomial
(x) and an error-magnitude polynomial (x). These two polynomials are related to
each other by the key equation, given as [12]:
(x)[1 + S(x)] ‚â°(x) mod x2t+1,
(3.18)
where S(x) is the syndrome polynomial.
3.6.1 Euclid‚Äôs Algorithm
Euclid‚Äôs algorithm [5] is used to determine the greatest common divisor (GCD)
between two elements a and b, with a > b. If a and b are elements then a is divided by
b to obtain a remainder r. If the remainder is zero then GCD(a, b) = b, otherwise we
let a = b, b = r and repeat the division to obtain another remainder. This is continued
until the remainder r = 0. Euclid‚Äôs algorithm can be extended, by using the quotient q
from the division process, to also determine two further elements, u and v, that satisfy
the relationship:
ua + vb = gcd(a, b).
(3.19)
3.6.1.1 Euclid‚Äôs Extended Algorithm
1. Initialization: r(‚àí1) = a, r(0) = b, u(‚àí1) = 1, u(0) = 0, v(‚àí1) = 0, v(0) = 1.
2. k = 1.
3. r(k) = r(k ‚àí2)/r(k ‚àí1). Store the quotient q(k) from this division.
4. If r(k) = 0 terminate the algorithm, otherwise u(k) = u(k ‚àí2) ‚àíq(k)u(k ‚àí1) and v(k) =
v(k ‚àí2) ‚àíq(k)v(k ‚àí1).
5. k = k + 1.
6. Go to step 3.
As an example, take the elements a = 121 and b = 33 and determine their greater
common divisor. The values for each iteration are given in Table 3.6.
It can be seen that each row in Table 3.6 satisÔ¨Åes (3.19).
We can also use Euclid‚Äôs extended algorithm to decode Reed‚ÄìSolomon codes. The
key equation of (3.17) and (3.18) can be rewritten as:
(x)x2t+1 + (x)[1 + S(x)] = (x),
(3.20)

90
NON-BINARY BLOCK CODES
Table 3.6
Euclid‚Äôs algorithm to determine GCD(121, 33).
k
r(k)
q(k)
u(k)
v(k)
‚àí1
121
‚Äî
1
0
0
33
‚Äî
0
1
1
22
3
1
‚àí3
2
11
1
‚àí1
4
3
0
2
3
‚àí11
where (x) is a polynomial. It is now in the form of (3.19), with u = (x), a = x2t+1,
v = (x), b = 1 + S(x) and GCD(x2t+1, 1 + S(x)) = (x).
3.6.1.2 Euclid‚Äôs Algorithm to decode Reed‚ÄìSolomon Codes [5, 12]
1. Initialization: (‚àí1)(x) = x2t + 1, (0) = 1 + S(x), (‚àí1)(x) = 1, (0)(x) = 0, (‚àí1)
(x) = 0, (0)(x) = 1.
2. k = 1.
3. Find the remainder (k)(x) from the division of (k ‚àí2) by (k ‚àí1) and store the
quotient q(k).
4. If (k)(x) = 0 terminate the algorithm otherwise (k)(x) = (k ‚àí2)(x) ‚àíq(k)
(k ‚àí1)(x) and (k)(x) = (k-2)(x) ‚àíq(k) (k-1)(x).
5. k = k + 1.
6. Go to step 3.
The error locations are then determined by Ô¨Årst Ô¨Ånding the roots of the error-locating
polynomial. The locations are the inverses of these roots.
Example 3.5:
Determining (x) and (x) for the (7, 3, 5) Reed‚ÄìSolomon
code using Euclid‚Äôs algorithm: The generator polynomial for the (7, 3, 5)
Reed‚ÄìSolomon code deÔ¨Åned over GF(8) is g(x) = x4 + Œ±3x3 + x2 + Œ±x + Œ±3. Let
the message polynomial be m(x) = x2 + Œ±x + Œ±2; the code word polynomial is
c(x) = x6 + x5 + Œ±3x4 + Œ±5x3 + Œ±3x2 + Œ±6x + Œ±5. At the receiver it is assumed
that two errors have occurred in the received word polynomial r(x), in the third and
sixth positions, so that r(x) = x6 + x5 + Œ±x4 + Œ±5x3 + Œ±3x2 + Œ±2x + Œ±5.
The 2t = 4 syndromes are found by substituting Œ±, Œ±2, Œ±3 and Œ±4 into r(x) as
follows:
S1 = r(Œ±) = Œ±6 + Œ±5 + Œ± ¬∑ Œ±4 + Œ±5 ¬∑ Œ±3 + Œ±3 ¬∑ Œ±2 + Œ±2 ¬∑ Œ± + Œ±5
= Œ±6 + Œ±5 + Œ±5 + Œ± + Œ±5 + Œ±3 + Œ±5
= Œ±6 + Œ± + Œ±3
= (Œ±2 + 1) + Œ± + (Œ± + 1)
= Œ±2

DECODING REED‚ÄìSOLOMON CODES
91
S2 = r(Œ±2) = Œ±12 + Œ±10 + Œ± ¬∑ Œ±8 + Œ±5 ¬∑ Œ±6 + Œ±3 ¬∑ Œ±4 + Œ±2 ¬∑ Œ±2 + Œ±5
= Œ±5 + Œ±3 + Œ±2 + Œ±4 + 1 + Œ±4 + Œ±5
= Œ±3 + Œ±2 + 1
= (Œ± + 1) + Œ±2 + 1
= Œ±4
S3 = r(Œ±3) = Œ±18 + Œ±15 + Œ± ¬∑ Œ±12 + Œ±5 ¬∑ Œ±9 + Œ±3 ¬∑ Œ±6 + Œ±2 ¬∑ Œ±3 + Œ±5
= Œ±4 + Œ± + Œ±6 + 1 + Œ±2 + Œ±5 + Œ±5
= Œ±4 + Œ± + Œ±6 + 1 + Œ±2
= (Œ±2 + Œ±) + Œ± + (Œ±2 + 1) + 1 + Œ±2
= Œ±2
S4 = r(Œ±4) = Œ±24 + Œ±20 + Œ± ¬∑ Œ±16 + Œ±5 ¬∑ Œ±12 + Œ±3 ¬∑ Œ±8 + Œ±2 ¬∑ Œ±4 + Œ±5
= Œ±3 + Œ±6 + Œ±3 + Œ±3 + Œ±4 + Œ±6 + Œ±5
= Œ±3 + Œ±4 + Œ±5
.
= (Œ± + 1) + (Œ±2 + Œ±) + (Œ±2 + Œ± + 1)
= Œ±
Therefore, 1 + S(x) = 1 + Œ±2x + Œ±4x2 + Œ±2x3 + Œ±x4.
Euclid‚Äôs algorithm is initialized as follows:
(‚àí1)(x) = x5, (0)(x) = 1 + Œ±2x + Œ±4x2 + Œ±2x3 + Œ±x4, (‚àí1)(x) = 0,
(0)(x) = 1, (‚àí1)(x) = 1, (0)(x) = 0.
We must now divide (‚àí1)(x) by (0)(x) to determine the remainder (1)(x):
Œ±6x + 1
Œ±x4 + Œ±2x3 + Œ±4x2 + Œ±2x + 1

x5
x5 + Œ±x4 + Œ±3x3 + Œ±x2 + Œ±6x
Œ±x4 + Œ±3x3 + Œ±x2 + Œ±6x
Œ±x4 + Œ±2x3 + Œ±4x2 + Œ±2x + 1
Œ±5x3 + Œ±2x2 + x + 1
So, (1)(x) = Œ±5x3 + Œ±2x2 + x + 1 and the quotient q(1) = Œ±6x + 1. Hence:
(1)(x) = (‚àí1)(x) + q(1)(0)(x) = 1 + (Œ±6x + 1) ¬∑ 0 = 1
and
(1)(x) = (‚àí1)(x) + q(1)(0)(x) = 0 + (Œ±6x + 1) ¬∑ 1 = Œ±6x + 1.

92
NON-BINARY BLOCK CODES
For the second iteration we must now divide (0)(x) by (1)(x) to determine the
remainder (2)(x) and q(2):
Œ±3x + Œ±5
Œ±5x3 + Œ±2x2 + x + 1

Œ±x4 + Œ±2x3 + Œ±4x2 + Œ±2x + 1
Œ±x4 + Œ±5x3 + Œ±3x2 + Œ±3x
Œ±3x3 + Œ±6x2 + Œ±5x + 1 .
Œ±3x3 + x2 + Œ±5x + Œ±5
Œ±2x2 + Œ±4
So (2)(x) = Œ±2x2 + Œ±4 and the quotient q(2) = Œ±3x + Œ±5. Hence (2)(x) =
(0)(x) + q(2) (1)(x) = 0 + (Œ±3x + Œ±5).1 = Œ±3x + Œ±5 and (2)(x) = (0)(x) +
q(2) (1)(x) = 1 + (Œ±3x + Œ±5)(Œ±6x + 1) = Œ±2x2 + Œ±6x + Œ±4. The algorithm is
now terminated, since the iteration step k = t. The example is summarized in
Table 3.7.
Table 3.7
Euclid‚Äôs algorithm for Example 3.6.
k
(k)(x)
q(k)
(k)(x)
(k)(x)
‚àí1
x5
‚Äî
1
0
0
Œ±x4 + Œ±2x3 + Œ±4x2 + Œ±2x + 1
‚Äî
0
1
1
Œ±5x3 + Œ±2x2 + x + 1
Œ±6x + 1
1
Œ±6x + 1
2
Œ±2x2 + Œ±4
Œ±3x + Œ±5
Œ±3x + Œ±5
Œ±2x2 + Œ±6x + Œ±4
The error-locating polynomial is therefore (x) = (2)(x) = Œ±2x2 + Œ±6x + Œ±4
and the error-magnitude polynomial is (x) = (2)(x) = Œ±5x3 + Œ±2x2 + x + 1.
The error-locating polynomial can be factorized to (x) = (Œ±x + Œ±4)(Œ±x + 1) and
the two roots are x1 = Œ±3 and x2 = Œ±6. The error locations are the inverses of these
roots and are X1 = Œ±4 and X2 = Œ±, corresponding to the x4 and x terms in the
received word. It can be seen that generated polynomials from Euclid‚Äôs algorithm
satisfy (3.20).
(x)x5 + (x)(1 + S(x)) = (Œ±3x + Œ±5)x5 + (Œ±2x2 + Œ±6x + Œ±4)
√ó (1 + Œ±2x + Œ±4x2 + Œ±2x3 + Œ±x4)
= Œ±3x6 + Œ±5x5 + Œ±3x6 + (Œ±4 + 1)x5
+ (Œ±6 + Œ± + Œ±5)x4 + (Œ±4+Œ±3+Œ±6)x3
+ (Œ±2 + Œ± + Œ±)x2 + (Œ±6 + Œ±6)x + Œ±4
= Œ±2x2 + Œ±4 = (x).
Since this is equivalent to the key equation of (3.19) we actually do not need to
generate (x) to be able to obtain the error-locating and error-magnitude polyno-
mials.

DECODING REED‚ÄìSOLOMON CODES
93
3.6.2 Berlekamp‚ÄìMassey‚Äôs Algorithm
Berlekamp‚ÄìMassey‚Äôs algorithm [6, 7] is more difÔ¨Åcult to understand than Euclid‚Äôs
algorithm but has a more efÔ¨Åcient implementation.
3.6.2.1 Berlekamp‚ÄìMassey‚Äôs Algorithm
1. Initialization: k = 0, (0)(x) = 1, L = 0, T(x) = x.
2. k = k + 1.
3. Compute the discrepancy (k) = Sk ‚àí
L
i=1
(k‚àí1)
i
Sk‚àíi.
If (k) = 0, go to step 8.
4. Modify the connection polynomial: (k)(x) = (k‚àí1)(x) ‚àí(k)T (x).
5. If 2L ‚â•k, go to step 8.
6. Set L = k ‚àíL and T (x) = (k‚àí1)(x)
(k)
.
7. Set T(x) = xT(x).
8. If k < 2t, go to step 3.
9. Determine the roots of (x) = (2t)(x).
Example 3.6: Determining (x) and (x) for the (7, 3, 5) Reed‚ÄìSolomon code
using Berlekamp‚ÄìMassey‚Äôs algorithm: Assuming the same received word as in
Example 3.5, r(x) = x6 + x5 + Œ±x4 + Œ±5x3 + Œ±3x2 + Œ±2x + Œ±5, the syndromes
were found to be S1 = Œ±2, S2 = Œ±4, S3 = Œ±2 and S4 = Œ±. The algorithm proceeds
as follows:
First, initialize the algorithm variables: k = 0, (0)(x) = 1, L = 0 and T(x) = x.
(1) = S1 ‚àí
0

i=1
(0)
i S1‚àíi = S1 = Œ±2
(1) (x) = (0) (x) ‚àí(1)T (x)
= 1 ‚àíŒ±2x.
At step 5, 2L = 2 √ó 0 = 0, which is less than k, so go to step 6: L = k ‚àíL =
1 ‚àí0 = 1.
T (x) = (0) (x)
(1)
= 1
Œ±2 = Œ±7
Œ±2 = Œ±5.
At step 7, T(x) = xT(x) = Œ±5x. At step 8, k < 2t = 4, so go to step 3.
For k = 2, the discrepancy (2) and error-locating polynomial (2)(x) are:
(2) = S2 ‚àí
1

i=1
(1)
i S2‚àíi = Œ±4 ‚àíŒ±2Œ±2 = 0
(2) (x) = (1) (x) ‚àí(2)T (x) = (1) (x) = 1 ‚àíŒ±2x.

94
NON-BINARY BLOCK CODES
(2) = 0, meaning there is no discrepancy and the error-locating polynomial
does not need to be modiÔ¨Åed, so go to step 7: T(x) = xT(x) = Œ±5x2.
At step 8, k < 2t, so go to step 3.
For k = 3, the discrepancy (3) and error-locating polynomial (3)(x) are:
(3) = S3 ‚àí
1

i=1
(1)
i S3‚àíi = Œ±2 ‚àíŒ±2Œ±4 = Œ±2 + Œ±2 + 1 = 1
(3) (x) = (2) (x) ‚àí(3)T (x) = 1 ‚àía2x ‚àía5x2.
At step 5, 2L = 2, which is less than k, so go to step 6.
L = k ‚àíL = 3 ‚àí1 = 2.
T (x) = (2) (x)
(3)
= 1 ‚àíŒ±2x
1
= 1 ‚àíŒ±2x.
At step 7, T(x) = xT(x) = x ‚àíŒ±2x2. At step 8, k < 2t, so go to step 3.
Finally, for k = 4, the discrepancy (4) and error-locating polynomial (4)(x)
are:
(4) = S4 ‚àí
2

i=1
(3)
i S4‚àíi=S4‚àí
-
(3)
1 S3+(3)
2 S2
.
= Œ±+Œ±2Œ±2+Œ±5Œ±4 = Œ± + Œ±4 + Œ±2
= Œ± + Œ±2 + Œ± + Œ±2 = 0
(4) (x) = (3) (x) ‚àí(4)T (x) = (3) (x) ‚àí0 = 1 ‚àíŒ±2x ‚àíŒ±5x2
At step 3, (4) = 0, meaning there is no modiÔ¨Åcation to the error-locating
polynomial, so go to step 7: T(x) = xT(x) = x2 ‚àíŒ±2x3.
At step 8, k = 2t, so go to step 9 and end the algorithm.
The error-locating polynomial (x) = (4)(x) = 1 ‚àíŒ±2x ‚àíŒ±5x2 can be factorized
to (x) = (1 ‚àíŒ±4x)(1 ‚àíŒ±x), which has the roots X1 = Œ±3 and X2 = Œ±6. This
completes the Berlekamp‚ÄìMassey algorithm. A summary of the above example is
shown in Table 3.8.
Table 3.8
Summary of the Berlekamp‚ÄìMassey algorithm steps.
k
Sk
(k)(x)
(k)
L
T(x)
0
‚Äî
1
‚Äî
0
x
1
Œ±2
1 ‚àíŒ±2x
Œ±2
1
Œ±5x
2
Œ±4
1 ‚àíŒ±2x
0
1
Œ±5x2
3
Œ±2
1 ‚àíŒ±2x ‚àíŒ±5x2
1
2
x ‚àíŒ±2x2
4
Œ±
1 ‚àíŒ±2x ‚àíŒ±5x2
0
2
x2 ‚àíŒ±2x3
However, the decoding process is still unÔ¨Ånished as the error magnitudes at these
locations need to be calculated.

DECODING REED‚ÄìSOLOMON CODES
95
3.6.3 Determining the Error Magnitudes Using Forney‚Äôs Formula
Forney‚Äôs formula [13] is a very efÔ¨Åcient method for Ô¨Ånding the error magnitudes
and only requires the error locations, the error-locating polynomial and the error-
magnitude polynomial (x) given by (3.19). If errors have occurred at the positions
i1, i2, i3, . . . i2t, the error magnitudes are determined by Forney‚Äôs formula as [12]:
eik = ‚àíXk

X‚àí1
k

 
X‚àí1
k

,
(3.21)
where  (x) is the formal derivative of the error-locating polynomial. The formal
derivative of a function f (x) = f0 + f1x + f2x2 + ¬∑ ¬∑ ¬∑ + fnxn whose coefÔ¨Åcients
belong to some Ô¨Ånite Ô¨Åeld GF(q) is given as [12] :
f  (x) = f1 + 2 f2x + 3 f3x2 + ¬∑ ¬∑ ¬∑ + nfn‚àí1xn‚àí1.
(3.22)
Completing the previous example, the error locations were found to be at X1 = Œ±4
and X2 = Œ±. The error magnitude (x) is:
 (x) =  (x) [1 + S (x)] mod x2t+1
= (1 ‚àíŒ±2x ‚àíŒ±5x2)(1 + Œ±2x + Œ±4x2 + Œ±2x3 + Œ±x4) mod x5
= 1 + (Œ±2 + Œ±2)x + (Œ±4 + Œ±4 + Œ±5)x2+(Œ±2 + Œ±6 + 1)x3+(Œ± + Œ±4 + Œ±2)x4
= 1 + Œ±5x2 + (Œ±2 + Œ±2 + 1 + 1)x3 + (Œ± + Œ±2 + Œ± + Œ±2)x4
= 1 + Œ±5x2.
Applying (3.22) to (x), the formal derivative of the error-locating polynomial is:
f  (x) = f1 + 2 f2x + 3 f3x2 + ¬∑ ¬∑ ¬∑ + nfn‚àí1xn‚àí1
 (x) = Œ±2 + 2Œ±5x
= Œ±2 +

Œ±5 + Œ±5
x
= Œ±2.
Therefore, the error magnitude at the location X1 = Œ±4 is:
ei1=‚àíX1

X‚àí1
1

 
X‚àí1
1

=
Œ±4 
1 + Œ±5 
Œ±4‚àí2
Œ±2
=Œ±4(1+Œ±5Œ±6)
Œ±2
=Œ±4(1 + Œ±4)
Œ±2
= Œ±9
Œ±2 =Œ±7 = 1.
Similarly, the error magnitude at the location X2 = Œ± is:
ei2 = ‚àíX1

X‚àí1
1

 
X‚àí1
1

= Œ±(1 + Œ±5(Œ±)‚àí2)
Œ±2
= Œ±(1 + Œ±5Œ±5)
Œ±2
= Œ±

1 + Œ±3
Œ±2
= Œ±2
Œ±2 = 1.

96
NON-BINARY BLOCK CODES
This results in an error polynomial of e(x) = x4 + x. Therefore, the decoded code
word is r(x) + e(x), giving:
r(x) + e(x) =

x6 + x5 + Œ±x4 + Œ±5x3 + Œ±3x2 + Œ±2x + Œ±5
+

x4 + x

= x6 + x5 + (Œ± + 1) x4 + Œ±5x3 + Œ±3x2 +

Œ±2 + 1

x + Œ±5
= x6 + x5 + Œ±3x4 + Œ±5x3 + Œ±3x2 + Œ±6x + Œ±5,
which matches the transmitted code word polynomial c(x).
3.7 Coded Modulation
When conventional coding techniques are introduced in a transmission system, the
bandwidth of the coded signal after modulation is wider than that of the uncoded signal
for the same information rate and the same modulation scheme. In fact, the encoding
process requires a bandwidth expansion that is inversely proportional to the code
rate, being traded for a coding gain. This is the reason why, in the past, conventional
coding schemes have been very popular on power-limited channels (where bandwidth
is readily available) but not on bandwidth-limited channels.
The Ô¨Årst important contribution to coding on bandwidth-limited channels was made
by Ungerboeck in 1982 [8]. He presented trellis coded modulation (TCM) as a coded
modulation scheme that accommodates the redundancy of a code on an expanded
signal set, and showed that excellent coding gains over uncoded modulation can be
achieved with no bandwidth expansion required. TCM codes are explained in Chapter
7 of this book.
As well as combining convolutional codes with modulation, it is also possible
to combine block codes with modulation; this is known as block coded modulation
(BCM). BCM codes have the advantage of being simpler to design than TCM codes,
but the decoding complexity can become too high for larger BCM codes.
3.7.1 Block Coded Modulation (BCM) Codes
The encoding procedure involves adding redundant bits or symbols to the message.
This redundancy does not contain any message bits and transmitting it results in
the overall data rate being reduced. The Quadrature Phase Shift Keying (QPSK)
modulation scheme carries two information bits per QPSK symbol without coding.
However, if the information is encoded with encoder of code rate R = 0.5 then half
the bits in the code word are redundant bits and so each QPSK symbol only carries
one information bit. One solution is to increase the size of the constellation to 16-PSK
(or alternatively 16-QAM), where each 16-PSK symbol carries four information bits
without coding. Therefore, using an encoder with R = 0.5 and 16-PSK means that
each 16-PSK symbol carries two information bits, which is the same as uncoded
QPSK.

CODED MODULATION
97
One problem with this solution is that as the constellation size is increased the
Euclidean distance between constellation points decreases, resulting in poorer perfor-
mance. However, combining code design with the modulation scheme can ensure that
code words have a minimum Euclidean distance greater than the Euclidean distance
between neighbouring constellation points. The combination of convolutional codes
and modulation is known as trellis coded modulation (TCM), while the combination
of block codes with modulation is known as block coded modulation (BCM).
3.7.2 Multi-Level Block Coding
A general block diagram of a binary BCM encoder is shown in Figure 3.6. The
encoding process takes m parallel messages u(1), u(2), . . ., u(m) of lengths k1, k2, . . .,
km bits and each message is encoded by one of m parallel block encoders, resulting
in m code words all of length n bits with minimum Hamming distances d1, d2, . . .,
dm. This arrangement of block encoders is called multi-level block coding and was
proposed by Imai and Hirakawa [14] . Finally, each bit from the m encoders is mapped
onto a 2m-ary constellation [15].
From Figure 3.6, we can see that the code rate of the multi-level block code is:
R =
m
i=1
ki
mn .
(3.23)
An example of a BCM code using m = 3 block encoders which are mapped onto
a 23 = 8-PSK constellation is given in Figure 3.7. The top block encoder is an (8, 1,
8) repetition code, which takes a message of k1 = 1 bit and repeats that value eight
times, for example 0 ‚Üí00000000 and 1 ‚Üí11111111. The second block code is the
(8, 7, 2) parity check code, as explained in Section 3.2. It takes a 7 bit message and
appends a parity check bit, which is either zero if the number of ‚Äò1‚Äôs in the message
is even or one if the number of ‚Äò1‚Äôs in the message is odd. Finally, the last encoder
(n, k1, d1)
Block code
2m-ary 
signal
set
(n, k2, d2)
Block code
(n, km, dm)
Block code
(
)
(1)
(1)
2
(1)
1
(1)
1ku
u
u
=
u
(
)
(2)
(2)
2
(2)
1
(2)
2
ku
u
u
=
u
(
)
(m)
(m)
2
(m)
1
(m)
km
u
u
u
=
u
BCM codeword
c(1)
c(2)
c(m)
Figure 3.6
General multi-level BCM encoder.

0
0
1
1
0
1
1
0
1
1
0
1
1
1
0
1
(8, 1, 8) Repetition
(8, 7, 2) Parity
(8, 8, 1) Uncoded
1
1
1
1
1
1
1
1
1
0
1
1
1
0
1
1
0
0
1
1
0
1
1
0
0
3
1
7
7
3
5
7
3
1
2
3
4
5
6
7
Transmitted 8-PSK Symbols
Source Information Bits
Multi-Level
BCM Encoder
8-PSK Modulator
c1
c2
c3
Figure 3.7
A 3-level BCM encoder for 8-PSK.
98

CODED MODULATION
99
is not an encoder at all since k3 = n = 8 and so there is no redundancy added. From
(3.23) the code rate of this BCM code is R = k1 + k2 + k3
3n
= 1 + 7 + 8
24
= 16
24 = 2
3. All that
remains is to map each bit from the three encoders to the 8-PSK constellation, which
is explained next.
3.7.3 Set Partitioning
The BCM encoder output is a constrained sequence of constellation symbols and
the mapping of the multi-level block encoder to these symbols is very important
for ensuring that the minimum Euclidean distance, or free distance, of the BCM
code is maximized. To achieve this, Ungerboeck proposed the Set Partitioning [8]
of the constellation, whereby the points in the constellation are recursively divided
into subsets, with the Euclidean distance between neighbouring points in each subset
being increased. Figure 3.8 shows set partitioning of the 8-PSK constellation as an
example. This is called a partition tree.
Ungerboeck‚Äôs rules for set partitioning any constellation are [8]:
 The signals in the bottom level of the partition tree are assigned parallel transitions.
 Parallel transitions must appear in subsets separated by the largest Euclidean dis-
tance.
 All signal points should be used equally often.
0
1
2
3
4
5
6
7
0
2
4
6
1
3
5
7
0
4
1
5
2
6
3
7
0 4
2
6
1
5
3
7
Œµ
Œµ
2
Œµ
2
Œµ
2
Œµ
2
0
1
0
1
0
1
0
1
0
1
0
1
0
1
Œµ
0.765
c1
c1
c2
c2
c2
c2
c3
c3
c3
c3
c3
c3
c3
c3
000
001
010
011
100
101
110
111
Figure 3.8
The set partitioning of the 8-PSK constellation.

100
NON-BINARY BLOCK CODES
From Figure 3.8 we can see that the coded bit from the repetition code c1 in
Figure 3.7 selects one of two QPSK subsets with constellation points separated by
a Euclidean distance of 0.765‚àöŒµ. The coded bit from the parity check code c2 then
selects one of two subsets with constellation points separated at a greater distance of
2‚àöŒµ. Finally, the uncoded bits c3 select signals in the bottom level of the partition
tree.
It can be shown that the minimum squared Euclidean distance between any two
sequences of the BCM code, otherwise known as the free distance d2
free, is [15]:
d2
free = min

2
1d1, 2
2d2, 2
3d3, . . . , 2
mdm

,
(3.24)
where 2
i , i = 0, 1, 2, . . ., m is the squared Euclidean distance between neighbouring
points in a subset in the ith level of the partition tree and di is the minimum Hamming
distance of the ith block encoder of the BCM code. A measure of the performance
of the BCM code over an uncoded constellation can be determined by Ô¨Ånding the
asymptotic coding gain (ACG) Œ≥ [10]:
Œ≥ = d2
free/Œµ
d2min/Œµ ,
(3.25)
where d2
min is the squared Euclidian distance between points in the uncoded constel-
lation, Œµ is the energy of the coded constellation and Œµ is the energy of the uncoded
constellation. For the BCM code in Figure 3.7 the minimum Hamming distances are
d1 = 8, d2 = 2 and d3 = 1. The corresponding Euclidean distances from the subsets
of 8-PSK in Figure 3.8 are 2
1 = 0.585Œµ , 2
2 = 2Œµ and 2
3 = 4Œµ . Therefore, from
(3.24) the free distance of the BCM code is:
d2
free = min

0.585Œµ √ó 8, 2Œµ √ó 2, 4Œµ √ó 1

= min(4.68Œµ , 4Œµ , 4Œµ ) = 4Œµ .
Since the uncoded and coded modulation schemes are QPSK and 8-PSK respec-
tively, the energy of both constellations is the same; that is, Œµ = Œµ . The minimum
squared Euclidean distance between neighbouring points of QPSK constellation is
d2
min = 2Œµ, giving an ACG of:
Œ≥ = d2
free/Œµ
d2min/Œµ = 4Œµ /Œµ
2Œµ/Œµ = 4
2 = 2
or
3 dB.
3.7.4 Construction of a Block Code Trellis
The decoding of a BCM code can be achieved efÔ¨Åciently by representing it as a
trellis diagram and applying the Viterbi algorithm. Wolf [16] showed how a trellis
diagram of a linear block code could be constructed using its parity check matrix, and
this method was extended by Paravalos and Fleisher [17] to construct BCM trellis
diagrams.

CODED MODULATION
101
Since the BCM encoder is made up of a number of block codes, its trellis is a
combination of the trellis diagrams of the component codes. The state transitions in
the trellis are determined by [17]:
sŒæ(Œª + 1) = sl(Œª) +
m

i=1
Œ¥ihi,Œª+1,
(3.26)
where Œª is an index denoting the depth of the trellis, l ‚ààSŒª (a set containing the index
of all states created up to depth Œª), Œæ ‚ààSŒª+1 (a set containing the index of all states
in SŒª plus all new states formed between Œª and Œª + 1), hi,Œª+1 is a vector representing
column Œª + 1 of the parity check matrix of the ith block code and Œ¥i = {0, 1} is a
binary input.
The trellis construction consists of four steps:
1. Each trellis begins and terminates at the all-zero state.
2. The depth of each trellis spans from Œª = 0 to Œª = n.
3. The number of states depends on the number and type of block codes used and the
constellation size. It grows while proceeding further into the trellis. To begin the
construction, a number of 2m states is assumed. If a number of information bits
are presented to the mapper uncoded, the initial number of states reduces to 2m-r,
where r is the number of absent encoders.
4. Each transition is determined by (3.26).
The following should be observed:
1. When the construction is complete all trellis paths that do not terminate at the
all-zero state are removed as they do not represent valid code sequences.
2. Transitions in a trellis are assigned a symbol from the signal constellation. Each
path within the trellis corresponds to a possible BCM sequence.
3. Due to the variation in component code sizes, the parity check matrix vectors used
in (3.26) have different dimensions. When the size of one parity check vector is
less than the size of the state binary vector s, an appropriate number of zeroes is
appended at the top of the vector. Alternatively, a vector with more elements than
the state binary vector s is reduced from top to bottom using modulo-2 addition.
The given procedure yields the Ô¨Ånal BCM trellis only in cases where the top encoder
employs the repetition code and the remaining encoders are simple codes.
Example 3.7: BCM code trellis construction: Using the BCM code from Figure
3.7, an example of constructing a trellis is now given. The BCM encoder consists of
three block encoders: the (8, 1, 8) Repetition code, the (8, 7, 2) Parity Check code
and 8 bits of uncoded information. The parity check matrices of the Repetition

102
NON-BINARY BLOCK CODES
code, HR, and the Parity Check code, HP, are:
HR =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
1
0
0
0
0
0
0
1
0
1
0
0
0
0
0
1
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
1
0
0
0
0
0
1
0
1
0
0
0
0
0
0
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
and
HP = [1 1 1 1 1 1 1 1].
The trellis will consist of 2m‚àír = 23‚àí1 = 4 states with two subtrellises each of
two states. To accommodate two states per subtrellis, the parity check matrices
are adjusted to attain a row size equal to the size of the binary state vector. Thus,
the reduced vectors of HR are obtain by top-to-bottom modulo-2 addition and the
result is:
HR =
	1 1 1 1 1 1 1 1
HP =
	1 1 1 1 1 1 1 1
.
Now the state transitions will be determined by (3.26):
1. At depth Œª = 1, (3.26) will become:
sŒæ(1) = sl(0) + Œ¥1h1,1 + Œ¥2h2,1.
In the Ô¨Årst subtrellis, the values of Œ¥i for the Ô¨Årst path are Œ¥1 = 0, Œ¥2 = 0 and for
the second path Œ¥1 = 0, Œ¥2 = 1.
s0(1) = [0] + 0 √ó [1] + 0 √ó [1] = [0]
s1(1) = [0] + 0 √ó [1] + 1 √ó [1] = [1].
In the second subtrellis, the values of Œ¥i for the Ô¨Årst path are Œ¥1 = 1, Œ¥2 = 0 and
for the second path Œ¥1 = 1, Œ¥2 = 1.
s2(1) = [0] + 1 √ó [1] + 0 √ó [1] = [1]
s3(1) = [0] + 1 √ó [1] + 1 √ó [1] = [0].
Note that the uncoded information is transmitted on the third BCM encoder
level. This means that the parity check matrix does not exist and hence all
corresponding transitions lead to the same state, causing parallel transitions in
the trellis diagram.
2. At depth Œª = 2, (3.26) will become:
sŒæ(2) = sl(1) + Œ¥1h1,2 + Œ¥2h2,2.

CODED MODULATION
103
First Subtrellis
s0(2) = [0] + 0 √ó [1] + 0 √ó [1] = [0]
s1(2) = [0] + 0 √ó [1] + 1 √ó [1] = [1]
s2(2) = [1] + 0 √ó [1] + 0 √ó [1] = [1]
s3(2) = [1] + 0 √ó [1] + 1 √ó [1] = [0].
Second Subtrellis
s4(2) = [0] + 1 √ó [1] + 0 √ó [1] = [1]
s5(2) = [0] + 1 √ó [1] + 1 √ó [1] = [0]
s6(2) = [1] + 1 √ó [1] + 0 √ó [1] = [0]
s7(2) = [1] + 1 √ó [1] + 1 √ó [1] = [1].
3. At depth Œª = 3, (3.26) will become:
sŒæ(3) = sl(2) + Œ¥1h1,3 + Œ¥2h2,3.
First Subtrellis
s0(3) = [0] + 0 √ó [1] + 0 √ó [1] = [0]
s1(3) = [0] + 0 √ó [1] + 1 √ó [1] = [1]
s2(3) = [1] + 0 √ó [1] + 0 √ó [1] = [1]
s3(3) = [1] + 0 √ó [1] + 1 √ó [1] = [0].
Second Subtrellis
s4(3) = [0] + 1 √ó [1] + 0 √ó [1] = [1]
s5(3) = [0] + 1 √ó [1] + 1 √ó [1] = [0]
s6(3) = [1] + 1 √ó [1] + 0 √ó [1] = [0]
s7(3) = [1] + 1 √ó [1] + 1 √ó [1] = [1].
The construction continues until Œª = n = 8. The Ô¨Ånal trellis is shown in
Figure 3.9.
3.7.5 Non-Binary BCM Codes
In 1994, Baldini and Farrell [18] introduced a class of non-binary BCM code over
rings of integers suitable for M-PSK and M-QAM. The idea of non-binary BCM codes
is to transmit m bits per channel symbol by using a modulator with q > 2m waveforms

104
NON-BINARY BLOCK CODES
0
2
4
6
1
3
5
7
0
2
0
2
1
3
1
3
4
7
5
5
7
6
4
6
0
2
0
2
1
3
1
3
4
7
5
5
7
6
4
6
0
2
0
2
1
3
1
3
4
7
5
5
7
6
4
6
0
2
0
2
1
3
1
3
4
7
5
5
7
6
4
6
0
2
4
6
1
3
5
7
Figure 3.9
Trellis for a BCM code using 8-PSK modulation.
to accommodate the extra redundancy. The non-binary BCM encoder structure is
shown in Figure 3.10. The binary source generates m + 1 parallel bits, which are
Gray mapped onto one of 2m+1 channel symbols ai ‚ààZq, i = 0, 1, . . ., k ‚Äì 1. These
are then fed to the multi-level encoder to generate the BCM coded symbols xi ‚ààZq,
i = 0, 1, . . . n, which will increase the minimum Euclidean distance.
One class of block code that can be used in the BCM encoder is the systematic
linear circulant block code. Its generator matrix is of the form given in (3.7), where
its parity block P is [18]:
P =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
P1,1
Pk,1
Pk‚àí1,1 ¬∑ ¬∑ ¬∑ P3,1 P2,1
P2,1
P1,1
Pk,1
¬∑ ¬∑ ¬∑ P4,1 P3,1
...
...
...
...
...
...
Pk‚àí1,1 Pk‚àí2,1 Pk‚àí3,1 ¬∑ ¬∑ ¬∑ P1,1 Pk,1
Pk,1
Pk‚àí1,1 Pk‚àí2,1 ¬∑ ¬∑ ¬∑ P2,1 P1,1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(3.27)
Binary 
Source
ai =f( b1, b2, ..., bm+1
Mapping
Multilevel
Encoder
Multilevel Source
b1
b2
bm+1
ai
q
k
xi
‚àà
q
n
‚àà
Figure 3.10
Non-binary BCM encoder structure.

CODED MODULATION
105
Each column of P in (3.27) is a cyclic shift upwards of the preceding column. To
identify this type of code the values of the Ô¨Årst column of P are used, that is P1,1,
P2,1, . . . , Pk‚àí1,1, Pk,1. An example of a systematic linear circulant block code over
Z4 = {0, 1, 2, 3} is the (10, 5) block code denoted by 12 233 [18]. Its generator
matrix is:
G =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
0
0
0
0
1
3
3
2
2
0
1
0
0
0
2
1
3
3
2
0
0
1
0
0
2
2
1
3
3
0
0
0
1
0
3
2
2
1
3
0
0
0
0
1
3
3
2
2
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
and it has a minimum squared Euclidean distance of d2
free = 12. From (3.25) its
asymptotic coding gain over uncoded BPSK is 4.77 dB.
Another class of multi-level block code is the pseudocyclic multi-level code, which
has a generator matrix of the form [18]:
G =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
g1,1
g1,2
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
g1,r
0
0
¬∑ ¬∑ ¬∑
0
0
g1,1
g1,2
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
g1,r
0
¬∑ ¬∑ ¬∑
0
...
...
...
...
...
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
g1,1
g1,2
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
g1,r
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
(3.28)
where gi, j ‚ààZq and r depends on the coded q-PSK modulation scheme and the code
rate.
It is also possible to construct non-binary BCM codes that are invariant to phase
rotations, that is a phase shift of the received symbols results in another valid code
word. Codes that are phase invariant to rotations of (360/q)‚ó¶are called transparent
codes. A code is transparent if it contains the all-one sequence (1, 1, 1, . . ., 1) as a
valid code word. Since the BCM code is linear, adding any two code word modulo-q
will result in another code word. Adding the all-one code word to another code word
is equivalent to rotating all the coded symbols by 90‚ó¶, which produces yet another
valid code word.
To ensure that the all-one code word is present in the non-binary BCM code, the
generator matrix from (3.28) is modiÔ¨Åed so that the bottom row is all ones [18].
G =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
g1,1
g1,2
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
g1,r
0
0
¬∑ ¬∑ ¬∑
0
0
g1,1
g1,2
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
g1,r
0
¬∑ ¬∑ ¬∑
0
...
...
...
...
...
...
...
...
...
1
1
¬∑ ¬∑ ¬∑
1
1
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(3.29)
Now the pseudocyclic multi-level code is transparent. To remove phase rotations
from the received symbols a differential encoder can be added before the transparent

106
NON-BINARY BLOCK CODES
D
Transparent
Encoder
Channel
Transparent
Decoder
D
u(D)
a(D)
√¢(D)
√ª(D)
Differential 
Encoder
Differential 
Decoder
Œ£
Œ£
+
+
+
‚àí
Figure 3.11
A differential encoder and decoder added to the non-binary BCM system to remove phase
rotations.
encoder [18], as shown in Figure 3.11. Similarly, a differential decoder is added after
the transparent decoder to obtain the decoded message.
3.8 Conclusions
In this chapter an introduction to block coding was given, beginning with the simplest
binary block codes, such as the parity check codes. A class of block codes known
as cyclic codes was then presented, with particular attention given to BCH codes.
These codes have good parameters and, using BCH codes, it was demonstrated how
extending to a non-binary alphabet can improve these parameters, for example giving
large Hamming distance and higher code rates. A special class of non-binary BCH
codes is the Reed‚ÄìSolomon codes, which are the most commonly used error-correcting
codes today and can be found in optical and magnetic storage, high-speed modems
and wireless communications. Finally, the concept of block coded modulation was
introduced, whereby the block encoder and modulator are treated as a single entity
to avoid increasing bandwidth use. Proper mapping of the coded symbols to channel
symbols ensures that the loss in performance due to the expansion of the signal
set is more than compensated for by the coding gain of the BCM code. Further
performance gains can be achieved by using non-binary block codes in the coded
modulation scheme, but with an increase in the decoding complexity.
References
[1] Hamming, R. (1950) Error detecting and error correcting codes. Bell Systems Technical Journal,
29, 41‚Äì56.
[2] Hocquenghem, A. (1959) Codes correcteurs d‚Äôerruers. Chiffres, 2, 147‚Äì56.
[3] Bose, R. and Ray-Chaudhuri, D. (1960) On a class of error-correcting binary codes. Information
and Control, 3, 68‚Äì79.
[4] Reed, I. and Solomon, G. (1960) Polynomial codes over certain Ô¨Ånite Ô¨Åelds. Journal of the Society
of Industrial Mathematics, 8, 300‚Äì4.
[5] Sugiyama, Y., Kasahara, M., Hirasawa, S. and Namekawa, T. (1975) A method for solving key
equation for Goppa codes. Information and Control, 27, 87‚Äì99.

REFERENCES
107
[6] Berlekamp, E. (1967) Nonbinary BCH decoding. Proceedings of the International Symposium on
Information Theory, San Remo, Italy.
[7] Massey, J.L. (1969) Shift-register synthesis and BCH decoding. IEEE Transactions on Information
Theory, IT-15 (1), 122‚Äì7.
[8] Ungerboeck, G. (1982) Channel coding with multilevel/phase signals. IEEE Transactions on Infor-
mation Theory, 28 (1), 55‚Äì67.
[9] Viterbi, A.J. (1971) Convolutional codes and their performance in communication systems. IEEE
Transactions on Communications Technology, 19 (5), 751‚Äì72.
[10] Moon, T.K. (2005) Error Correction Coding. Mathematical Methods and Algorithms, Wiley Inter-
science, ISBN 0-471-64800-0.
[11] Prange, E. (1958) Cyclic Error-Correcting Codes in Two Symbols, Air Force Cambridge Research
Center, Cambridge, MA, Technical Report, TN-58-156.
[12] Wicker, S.B. (1995) Error Control Systems for Digital Communications and Storage, Prentice Hall,
Eaglewood Cliffs, NJ.
[13] Forney, G.D. (1965) On decoding BCH codes. IEEE Transactions on Information Theory, 11 (4),
549‚Äì57.
[14] Imai, H. and Hirakawa, S. (1977) A new multilevel coding method using error-correcting codes.
IEEE Transactions on Information Theory, 23 (3), 371‚Äì7.
[15] Lin, S. and Costello, D.J. Jr. (2004) Error Control Coding, 2nd edn, Pearson Prentice Hall, ISBN
0-13-017973-6.
[16] Wolf, J.K. (1978) EfÔ¨Åcient maximum likelihood decoding of linear block codes using a trellis. IEEE
Transactions on Information Theory, 24 (1), 76‚Äì80.
[17] Paravalos, E. and Fleisher, S. (1991) Block coded modulation: an application to frequency/phase
modulation and a procedure for the construction of trellis diagram. IEEE Proceedings of PaciÔ¨Åc
Rim Conference on Communications, Computers and Signal Processing, Victoria, USA, pp. 79‚Äì82.
[18] Baldini, F.R. and Farrell, P.G. (1994) Coded modulation based on rings of integers modulo-q. Part
1: block codes. IEE Proceedings Communications, 141 (3), 129‚Äì36.


4
Algebraic‚ÄìGeometric Codes
4.1 Introduction
Algebraic geometry is a powerful mathematical tool for constructing very long non-
binary block codes with excellent parameters, such as high code rate and large
Hamming distance. In 1981, Goppa showed how algebraic geometry could be used to
construct non-binary block codes called Goppa codes or algebraic‚Äìgeometric (AG)
codes [1]. These codes are constructed from the afÔ¨Åne points of an irreducible pro-
jective curve and a set of rational functions deÔ¨Åned on that curve. The simplest type
of AG code is the well-known Reed‚ÄìSolomon code, which can be constructed from
the afÔ¨Åne points of a projective line. The length of an AG code is equal to the number
of afÔ¨Åne points, which in the case of a line can be no greater than the cardinality of
the chosen Ô¨Ånite Ô¨Åeld. Hence, constructing codes from a line will result in short code
lengths, as is the case for Reed‚ÄìSolomon codes. More afÔ¨Åne points can be obtained
by instead choosing a projective curve, which can result in much longer codes with-
out increasing the size of the Ô¨Ånite Ô¨Åeld. The most desirable curves are those that
have the maximum possible number of afÔ¨Åne points, known as maximal curves, since
these produce the longest possible codes. However, constructing AG codes requires
an in-depth knowledge of the theory of algebraic geometry, which is very difÔ¨Åcult
to understand, and this could be one of the reasons why AG codes have yet to be
implemented.
In 1989, Justesen et al. [2] presented a construction method that only requires a
basic understanding of algebraic geometry, but this method can still produce AG codes.
This method has the disadvantage that less AG codes can be constructed than through
more complicated algebraic‚Äìgeometric approach, but many can still be produced. The
notation used by Justesen for the construction of AG codes discussed in this chapter
is also used in the hard-decision decoding algorithm explained later in this chapter.
In this chapter, the construction of AG codes is presented using the theory of
algebraic geometry discussed in Chapter 2. This is followed by Justesen‚Äôs simpliÔ¨Åed
construction method, which is described in detail. Comparisons are made between
Non-Binary Error Control Coding for Wireless Communication and Data Storage
Rolando Antonio Carrasco and Martin Johnston
C 2008 John Wiley & Sons, Ltd

110
ALGEBRAIC‚ÄìGEOMETRIC CODES
Reed‚ÄìSolomon and Hermitian codes, illustrating the limitations of Reed‚ÄìSolomon
codes in terms of code length and the number of codes that can be constructed.
The next section presents the construction of systematic AG codes. Unfortunately,
Hermitian codes are not cyclic like Reed‚ÄìSolomon codes, and so it is not possible
to use a multi-stage shift register to produce systematic AG codes. Traditionally,
systematic block codes are created by performing Gauss‚ÄìJordan elimination on the
nonsystematic generator matrix.
4.2 Construction of Algebraic‚ÄìGeometric Codes
The construction of AG codes can be accomplished using the theory described in
Chapter 2. Firstly, an irreducible afÔ¨Åne smooth curve over a Ô¨Ånite Ô¨Åeld must be
chosen. Curves that can be used to construct good AG codes are Hermitian curves,
elliptic curves, hyperelliptic curves and so on, and all have a single point at inÔ¨Ånity.
Next, all afÔ¨Åne points and the point at inÔ¨Ånity must be found, as explained in Chapter 2.
To determine the message length k and designed minimum distance d
‚àóof the code, the
degree a of the divisor aQ must be chosen, where Q is the point at inÔ¨Ånity, using [3]:
2Œ≥ ‚àí2 < a < n,
(4.1)
where Œ≥ is the genus of the curve and n is the number of afÔ¨Åne points (code length).
The code parameters can then be calculated using (2.13) and (2.16) for a functional
Goppa code or (2.17) and (2.18) for a residue Goppa code.
To construct the generator matrix of the code, a set of rational functions with pole
order up to and including a at Q must be determined, but these functions must not
have poles at any of the afÔ¨Åne points. From Chapter 2 we know the set of rational
functions
L(aQ) =
xi y j
zi+ j

, 0 ‚â§i < m,
j ‚â•0
(where m is the degree of the curve and the pole order of each rational function is
4i + 5j) satisfy this requirement for the Hermitian curve. There are k rational function
in L(aQ), which are evaluated at each of the n afÔ¨Åne points to generate the k rows of
the (k √ó n) generator matrix G. Now an example of AG code construction is given.
Example 4.1: Construction of the (8, 5, 3) Hermitian code using algebraic
geometry: In this example, the Hermitian curve over GF(22) deÔ¨Åned by (2.1) with
Œ≥ =1 is chosen to construct a residue Goppa code. The curve has eight afÔ¨Åne
points, shown in Table 2.12, and one point at inÔ¨Ånity, Q = (0, 1, 0). The degree a
of the divisor aQ is chosen using (4.1), which can vary between zero and eight.

CONSTRUCTION OF ALGEBRAIC‚ÄìGEOMETRIC CODES
111
In this example we choose a = 3 and from (2.19) the message length is:
k = n ‚àíl(G) = n ‚àíd(G) ‚àí1 + Œ≥
= 8 ‚àí3 ‚àí1 + 1 = 5
,
and from (2.18) the designed minimum distance d
‚àóis:
d‚àó= n ‚àí(n ‚àíd(G) ‚àí1 + Œ≥ ) + 1 ‚àíŒ≥
= d(G) ‚àí2Œ≥ + 2
= 3 ‚àí2 + 2 = 3
.
So we have the (8, 5, 3) Hermitian code. The k = 5 rational functions that have
pole orders up to and including a = 5 are:
L(5Q) =

1, x
z , y
z , x2
z2 , xy
z2

.
Since all the afÔ¨Åne points have z = 1, L(5Q) can be written as:
L(5Q) =

1, x, y, x2, xy

.
The code word is then obtained by evaluating a message polynomial at each
of the eight afÔ¨Åne points of the Hermitian curve. From Chapter 2, the eight afÔ¨Åne
points of the Hermitian curve over GF(4) are:
P0 = (0, 0), P1 = (0, 1), P2 = (1, Œ±), P3 = (1, Œ±2), P4 = (Œ±, Œ±),
P5 = (Œ±, Œ±2), P6 = (Œ±2, Œ±) and P7 = (Œ±2, Œ±2).
Let the message polynomial be f(x, y) = 1 + y + x2. Then each coded symbol
ci = f(Pi), i = 1, 2, . . . , 8.
c1 = f (P1) = f (0, 0) = 1 + 1 + 1 = 1,
c2 = f (P2) = f (0, 1) = 1 + 1 + 0 = 0,
c3 = f (P3) = f (1, Œ±) = 1 + 1 + Œ±2 = Œ±2,
c4 = f (P4) = f (1, Œ±2) = 1 + 1 + Œ±4 = Œ±,
c5 = f (P5) = f (Œ±, Œ±) = 1 + Œ± + Œ±2 = 0,
c6 = f (P6) = f (Œ±, Œ±2) = 1 + Œ±2 + Œ±2 = 1,
c7 = f (P7) = f (Œ±2, Œ±) = 1 + Œ± + Œ±4 = 1,
c8 = f (P8) = f (Œ±2, Œ±2) = 1 + Œ±2 + Œ±4 = 0,
.
Hence the code word is c = 1, 0, Œ±2, Œ±, 0, 1, 1, 0.
This is also equivalent to multiplying a message vector m by a generator matrix
G. To construct the rows of the generator matrix of an AG code, each monomial

112
ALGEBRAIC‚ÄìGEOMETRIC CODES
œÜi, i = 0, 1, . . . , k, in L(5Q) is evaluated at each afÔ¨Åne point, as shown in (4.2).
G =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
œÜ1(P1)
œÜ1(P2)
œÜ1(P3)
¬∑ ¬∑ ¬∑
œÜ1(Pn)
œÜ2(P1)
œÜ2(P2)
œÜ2(P3)
¬∑ ¬∑ ¬∑
œÜ2(Pn)
œÜ3(P1)
œÜ3(P2)
œÜ3(P3)
¬∑ ¬∑ ¬∑
œÜ3(Pn)
...
...
...
...
...
œÜk(P1)
œÜk(P2)
œÜk(P3)
¬∑ ¬∑ ¬∑
œÜk(Pn)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(4.2)
If the Ô¨Årst monomial is written as œÜ1 = x0y0 then evaluating it at each point
will give a row of all 1s due to powers of 0 in the x and y term. The remaining
monomials are evaluated at each of the eight points using Table 2.12.
The second row of the generator matrix is obtained by evaluating the monomial
f(x, y) = x at all eight afÔ¨Åne points. In this case each of the eight elements of this
row is just the x-coordinate of one of the eight afÔ¨Åne points.
f (0, 0) = 0, f (0, 1) = 0, f (1, Œ±) = 1, f (1, Œ±2) = 1, f (Œ±, Œ±) = Œ±,
f (Œ±, Œ±2) = Œ±, f (Œ±2, Œ±) = Œ±2, f (Œ±2, Œ±2) = Œ±2.
The third row of the generator matrix is obtained by evaluating the monomial
f(x, y) = y at all eight afÔ¨Åne points. In this case each of the eight elements of this
row is just the y-coordinate of one of the eight afÔ¨Åne points.
f (0, 0) = 0, f (0, 1) = 1, f (1, Œ±) = Œ±, f (1, Œ±2) = Œ±2, f (Œ±, Œ±) = Œ±,
f (Œ±, Œ±2) = Œ±2, f (Œ±2, Œ±) = Œ±, f (Œ±2, Œ±2) = Œ±2.
The fourth row of the generator matrix is obtained by evaluating the monomial
f(x, y) = x2 at all eight afÔ¨Åne points. In this case each of the eight elements of this
row is the square of the x-coordinate of one of the eight afÔ¨Åne points.
f (0, 0) = 0, f (0, 1) = 0, f (1, Œ±) = 1, f (1, Œ±2) = 1, f (Œ±, Œ±) = Œ±2,
f (Œ±, Œ±2) = Œ±2, f (Œ±2, Œ±) = Œ±, f (Œ±2, Œ±2) = Œ±.
The Ô¨Åfth row of the generator matrix is obtained by evaluating the monomial
f(x, y) = xy at all eight afÔ¨Åne points. In this case each of the eight elements of this
row is the product of the x-coordinate and y-coordinate of one of the eight afÔ¨Åne
points.
f (0, 0) = 0, f (0, 1) = 0, f (1, Œ±) = Œ±, f (1, Œ±2) = Œ±2, f (Œ±, Œ±) = Œ±2,
f (Œ±, Œ±2) = 1, f (Œ±2, Œ±) = 1, f (Œ±2, Œ±2) = Œ±.
Therefore, the generator matrix is:
G =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
1
1
1
1
1
1
1
0
0
1
1
Œ±
Œ±
Œ±2
Œ±2
0
1
Œ±
Œ±2
Œ±
Œ±2
Œ±
Œ±2
0
0
1
1
Œ±2
Œ±2
Œ±
Œ±
0
0
Œ±
Œ±2
Œ±2
1
1
Œ±
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.

CONSTRUCTION OF ALGEBRAIC‚ÄìGEOMETRIC CODES
113
The construction method for one-point AG codes is summarized below [3]:
1. Select an irreducible afÔ¨Åne smooth curve and a Ô¨Ånite Ô¨Åeld.
2. Find all the afÔ¨Åne points of the form (Œ±, Œ≤, 1) and point at inÔ¨Ånity Q = (Œ±, Œ≤,
0), where Œ± and Œ≤ are elements in the Ô¨Ånite Ô¨Åeld, that make the curve vanish.
3. Choose the degree a of the divisor aQ using (4.1) and calculate the message
length k and designed minimum distance d
‚àóof the code using (2.13) and (3.16)
for a functional Goppa code or (2.17) and (2.18) for a residue Goppa code.
4. Determine a set of rational functions L(aQ) that have pole orders up to and
including a at the point at inÔ¨Ånity Q, but do not have poles at any of the afÔ¨Åne
points.
5. Evaluate each rational function in L(aQ) at each afÔ¨Åne point to obtain the
nonsystematic generator matrix G of the AG code.
4.2.1 SimpliÔ¨Åed Construction of AG Codes
In this section, a simpliÔ¨Åed construction method attributed to Justesen [2] is described,
which only requires a basic knowledge of algebraic geometry to construct an AG code.
The Ô¨Årst difference of this method is that projective curves are not considered. Only
afÔ¨Åne curves in the (x ‚àíy) coordinate system are used and the point at inÔ¨Ånity is
excluded. Another difference is the use of a set of monomials in two dimensions
instead of a set of rational functions in three dimensions to obtain the generator
matrix.
To construct an algebraic‚Äìgeometric code using Justesen‚Äôs construction, a nonneg-
ative integer j is Ô¨Årst chosen that is bounded by [2]:
m ‚àí2 ‚â§j ‚â§
n ‚àí1
m

.
(4.3)
(4.3) is derived from (4.1). In the simpliÔ¨Åed construction the degree of the divisor
G is a multiple of the degree of the curve C(x, y), that is deg(G) = mj, where j is a
nonnegative integer. Substituting deg(G) = mj and (2.12) into (4.1) gives:
2(m ‚àí1)(m ‚àí2)
2
‚àí2 < mj < n
‚áím2 ‚àí3m + 2 ‚àí2 < mj < n
‚áím(m ‚àí3)
m
< j < n
m
‚à¥m ‚àí2 ‚â§j ‚â§
n ‚àí1
m

.
The codes obtained using Justesen‚Äôs simpliÔ¨Åed construction are residue Goppa
codes. Therefore, since deg(G) = mj, the code parameters from (2.17) and (2.18) can

114
ALGEBRAIC‚ÄìGEOMETRIC CODES
be written as:
k = n ‚àímj + Œ≥ ‚àí1
d‚àó= mj ‚àí2Œ≥ + 2
.
(4.4)
However, since deg(G) is limited to being a multiple of the degree of the curve, less
AG codes can be constructed than with the algebraic‚Äìgeometric construction. This
is the only disadvantage of the simpliÔ¨Åed method, but many AG codes can still be
constructed using it.
A monomial basis is deÔ¨Åned as:
f = {xi y j},
0 ‚â§i < m,
b ‚â•0,
(4.5)
which contains k monomials. Hence, as before, the monomials are evaluated at each
point to obtain the generator matrix of the code.
Example 4.2: Construction of the (8, 5, 3) Hermitian code over GF(22) using
the simpliÔ¨Åed method: In this example, the same Hermitian code is constructed
as in the previous example, but using Justesen‚Äôs simpliÔ¨Åed construction method.
Again, the Hermitian curve deÔ¨Åned in (2.1) is used. From the previous example,
we know that the Hermitian code over GF(22) has n = 8 points, but these are not
projective points and so there is no z component. The points are given in Table 4.1.
Table 4.1
Points of the Hermitian curve over GF(22).
P1 = (0, 0)
P2 = (0, 1)
P3 = (1, Œ±)
P4 = (1, Œ±2)
P5 = (Œ±, Œ±)
P6 = (Œ±, Œ±2)
P7 = (Œ±2, Œ±)
P8 = (Œ±2, Œ±2)
The degree of the Hermitian curve is m = 3, so from (4.3) the parameter j can
have values of:
3 ‚àí2 ‚â§j ‚â§
 8‚àí1
3
0
= 1 ‚â§j ‚â§2
.
The genus of the curve is Œ≥ = 1, and taking j = 1 the parameters of the code
from (4.4) are:
k = 8 ‚àí3 √ó 1 + 1 ‚àí1 = 5
d‚àó= 3 √ó 1 ‚àí2 √ó 1 + 2 = 3.
Since k = 5 there will be Ô¨Åve monomials in the monomial basis. These are
{1, x, y, x2, xy}. To construct the generator matrix G, each monomial is evaluated
at each of the eight points, and thus we obtain the same generator matrix as in
the previous example. This example shows that, using Justesen‚Äôs construction
method, all that is required is the degree and genus of the curve and the points
on the curve. It does not require an in-depth knowledge of algebraic geometry to
construct AG codes.

CONSTRUCTION OF ALGEBRAIC‚ÄìGEOMETRIC CODES
115
Table 4.2
Comparison of Reed‚ÄìSolomon and Hermitian code lengths for different
Ô¨Ånite Ô¨Åelds (Ô¨Ågures in parentheses are for doubly-extended Reed‚ÄìSolomon codes).
Finite Ô¨Åeld size q
Reed‚ÄìSolomon code length
Hermitian code length
4
3 (5)
8
16
15 (17)
64
64
63 (65)
512
256
255 (257)
4096
4.2.2 Comparison of AG Codes with Reed‚ÄìSolomon Codes
Reed‚ÄìSolomon codes are the most commonly used codes due to their efÔ¨Åcient decod-
ing algorithms and good performance over channels with burst errors. However, they
do have limitations, such as short code length and the fact that there are not many of
them. Table 4.2 shows how the lengths of Reed‚ÄìSolomon codes and Hermitian codes
vary for increasing Ô¨Ånite Ô¨Åeld size.
It can be seen that the length of the Hermitian codes increases dramatically for
increasing Ô¨Ånite Ô¨Åelds since for a Ô¨Ånite Ô¨Åeld size of q the length n = q3/2. However,
the length of the Reed‚ÄìSolomon code is restricted to the size of the Ô¨Ånite Ô¨Åeld and
has length n = q ‚àí1, or q + 1 if doubly extended.
The number of possible AG codes for a given Ô¨Ånite Ô¨Åeld can be found from (4.1).
If 2Œ≥ ‚àí2 < deg(G) < n then 2Œ≥ ‚àí1 ‚â§deg(G) ‚â§n ‚àí1. This means that there are
(n ‚àí1) ‚àí(2Œ≥ ‚àí1) = n ‚àí2Œ≥ possible AG codes for a given Ô¨Ånite Ô¨Åeld. As stated
previously, a Reed‚ÄìSolomon code is constructed from an afÔ¨Åne straight line which
has a genus Œ≥ = 0. This implies there are n possible Reed‚ÄìSolomon codes for a given
Ô¨Ånite Ô¨Åeld. A comparison of the numbers of Reed‚ÄìSolomon and Hermitian codes for
increasing Ô¨Ånite Ô¨Åeld size is given in Figure 4.1.
It can be seen that the number of possible Hermitian codes increases exponentially
with increasing Ô¨Åeld size, with a possible 3855 Hermitian codes deÔ¨Åned over GF(28).
In contrast, the number of Reed‚ÄìSolomon codes is again restricted by the size of the
Ô¨Ånite Ô¨Åeld.
4.2.3 Systematic Algebraic‚ÄìGeometric Codes
Evaluating the elements in the monomial basis at each point on the curve produces a
generator matrix in nonsystematic form. Traditionally, systematic codes are achieved
by applying Gauss‚ÄìJordan elimination [4] to the nonsystematic generator matrix.
However, this approach had not been used to construct systematic AG codes since
for large generator matrices the complexity could be too high. In the literature, only
Heegard et al. [5] have considered the construction of systematic AG codes. They show
how systematic encoding of algebraic‚Äìgeometric codes can be achieved in a method
analogous to the systematic encoding of Reed‚ÄìSolomon codes by using the cyclic
properties of the automorphisms of a curve. This method requires fewer operations

116
ALGEBRAIC‚ÄìGEOMETRIC CODES
15
63
255
51
455
3855
0
500
1000
1500
2000
2500
3000
3500
4000
256
64
16
Finite Field GF(q)
No. of Codes
Reed‚ÄìSolomon
Hermitian
Figure 4.1
A comparison of the numbers of Reed‚ÄìSolomon and Hermitian codes for increasing Ô¨Ånite
Ô¨Åeld size.
than applying Gauss‚ÄìJordan elimination on the nonsystematic generator matrix, but
it necessitates a good knowledge of algebraic geometry. However, since their paper
was written, computers have become much more powerful and it has been found that
Gauss‚ÄìJordan elimination can be performed quickly even on large generator matrices
from Hermitian codes deÔ¨Åned over GF(26). Gauss‚ÄìJordan elimination is made up of
two algorithms: Gaussian elimination and Jordan elimination. Gaussian elimination
converts the matrix into echelon form and Jordan elimination then ensures the matrix
is in row-reduced echelon form. For the case of a nonsquare matrix, such as a k √ó n
generator matrix, a k √ó k submatrix of the generator matrix in row-reduced echelon
form is the k √ó k identity matrix Ik. The remainder of the generator matrix is a k √ó (n
‚àík) parity matrix P. The generator matrix is now in systematic form, G = [Ik | P].
The algorithms for Gaussian elimination and Jordan elimination are described below.
4.2.3.1 Gaussian Elimination
The Gaussian elimination algorithm uses the following notation: Gj is used to denote
all the elements in row j of G, Gi,j denotes a single element located at (i, j) in G, and k
is the message length of the AG code. When Gaussian elimination is used on a matrix
containing real numbers, elimination of a column occurs by multiplying previous
rows by a factor and subtracting them from the row that is being operated on. In the
case of a matrix containing Ô¨Ånite Ô¨Åeld elements, Gaussian elimination is performed
in the same way, using modulo-q addition instead of subtraction. Modulo-q addition
can be used if the row to be subtracted is multiplied by the additional factor q ‚àí1.

CONSTRUCTION OF ALGEBRAIC‚ÄìGEOMETRIC CODES
117
Initialize: i = 0, j = 0
1. Check that column i does not contain all zeros. If it does then i = i + 1 and go to
step 1. Else go to step 2.
2. Find the row p with largest Ô¨Årst element.
3. Swap row p with largest Ô¨Årst element with row j.
4. Eliminate elements in column i from row j + 1 to row k ‚àí1.
For row = j + 1 to k ‚Äì 1:
Grow ‚ÜíGrow + (q ‚àí1)
Gi,row
Gi, j
G j

.
5. i = i + 1 and j = j + 1. If j = k go to step 6. Else go to step 2.
6. For row = 0 to k ‚àí1:
Divide Grow by the Ô¨Årst nonzero element in Grow so that the main diagonal of G
contains all 1s.
4.2.3.2 Jordan Elimination
1. Initialize: i = 0, j = 0.
2. If Gi,j = 0, swap column i with the nearest column that has a 1 in row j.
3. Eliminate elements in column i from row j ‚àí1 to row 0.
For row = j ‚àí1 to 0:
Grow ‚ÜíGrow + (q ‚àí1)
Gi,row
Gi, j
G j

.
4. i = i + 1 and j = j + 1. If j = k STOP. Else go to step 2.
Example 4.3: Construction of the systematic (27, 13, 12) Hermitian code over
GF(32): Using the primitive polynomial x2 = 2x + 1, the Ô¨Ånite Ô¨Åeld GF(32) is
shown in Table 2.10. The Hermitian code deÔ¨Åned over GF(32) is C(x, y) = x4 +
y3 + y and has 27 points that satisfy C(x, y) = 0, given in Table 4.3.
Table 4.3
The 27 points of the Hermitian curve over GF(32).
P1 = (0, 0)
P2 = (0, Œ±6)
P3 = (0, Œ±2)
P4 = (1, 1)
P5 = (1, Œ±)
P6 = (1, Œ±3)
P7 = (Œ±4, 1)
P8 = (Œ±4, Œ±)
P9 = (Œ±4, Œ±3)
P10 = (Œ±, Œ±4)
P11 = (Œ±, Œ±7)
P12 = (Œ±, Œ±5)
P13 = (Œ±7, Œ±4)
P14 = (Œ±7, Œ±7)
P15 = (Œ±7, Œ±5)
P16 = (Œ±6, 1)
P17 = (Œ±6, Œ±)
P18 = (Œ±6, Œ±3)
P19 = (Œ±5, Œ±4)
P20 = (Œ±5, Œ±7)
P21 = (Œ±5, Œ±5)
P22 = (Œ±2, 1)
P23 = (Œ±2, Œ±)
P24 = (Œ±2, Œ±3)
P25 = (Œ±3, Œ±4)
P26 = (Œ±3, Œ±7)
P27 = (Œ±3, Œ±5)

118
ALGEBRAIC‚ÄìGEOMETRIC CODES
The genus of the curve from (2.12) is:
Œ≥ = (4 ‚àí1)(4 ‚àí2)
2
= 3.
From (4.3), the parameter j can vary between 2 and 6. Taking j = 4, we get a
message length k = 13 and designed minimum distance d
‚àó= 12. The monomial
basis from (4.5) therefore has the 13 elements {1, x, y, x2, xy, y2, x3, x2y, xy2, y3,
x3y, x2y2, xy3} and the nonsystematic generator matrix is:
G =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1 1
1 1 1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
0 0
0 1 1
1 Œ±4 Œ±4 Œ±4 Œ±
Œ±
Œ± Œ±7 Œ±7 Œ±7 Œ±6 Œ±6 Œ±6 Œ±5 Œ±5 Œ±5 Œ±2 Œ±2 Œ±2 Œ±3 Œ±3 Œ±3
0 Œ±6 Œ±2 1 Œ± Œ±3 1
Œ± Œ±3 Œ±4 Œ±7 Œ±5 Œ±4 Œ±7 Œ±5 1
Œ± Œ±3 Œ±4 Œ±7 Œ±5 1
Œ± Œ±3 Œ±4 Œ±7 Œ±5
0 0
0 1 1
1
1
1
1 Œ±2 Œ±2 Œ±2 Œ±6 Œ±6 Œ±6 Œ±4 Œ±4 Œ±4 Œ±2 Œ±2 Œ±2 Œ±4 Œ±4 Œ±4 Œ±6 Œ±6 Œ±6
0 0
0 1 Œ± Œ±3 Œ±4 Œ±5 Œ±7 Œ±5 1 Œ±6 Œ±3 Œ±6 Œ±4 Œ±6 Œ±7 Œ±
Œ± Œ±4 Œ±2 Œ±2 Œ±3 Œ±5 Œ±7 Œ±2 1
0 Œ±4 Œ±4 1 Œ±2 Œ±6 1 Œ±2 Œ±6 1 Œ±6 Œ±2 1 Œ±6 Œ±2 1 Œ±2 Œ±6 1 Œ±6 Œ±2 1 Œ±2 Œ±6 1 Œ±6 Œ±2
0 0
0 1 1
1 Œ±4 Œ±4 Œ±4 Œ±3 Œ±3 Œ±3 Œ±5 Œ±5 Œ±5 Œ±2 Œ±2 Œ±2 Œ±7 Œ±7 Œ±7 Œ±6 Œ±6 Œ±6 Œ±
Œ±
Œ±
0 0
0 1 Œ± Œ±3 1
Œ± Œ±3 Œ±6 Œ± Œ±7 Œ±2 Œ±5 Œ±3 Œ±4 Œ±5 Œ±7 Œ±6 Œ± Œ±7 Œ±4 Œ±5 Œ±7 Œ±2 Œ±5 Œ±3
0 0
0 1 Œ±2 Œ±6 Œ±4 Œ±6 Œ±2 Œ± Œ±7 Œ±3 Œ±7 Œ±5 Œ± Œ±6 1 Œ±4 Œ±5 Œ±3 Œ±7 Œ±2 Œ±4 1 Œ±3 Œ± Œ±5
0 Œ±2 Œ±6 1 Œ±3 Œ±
1 Œ±3 Œ± Œ±4 Œ±5 Œ±7 Œ±4 Œ±5 Œ±7 1 Œ±3 Œ± Œ±4 Œ±5 Œ±7 1 Œ±3 Œ± Œ±4 Œ±5 Œ±7
0 0
0 1 Œ± Œ±3 Œ±4 Œ±5 Œ±7 Œ±7 Œ±2 1
Œ± Œ±4 Œ±2 Œ±2 Œ±3 Œ±5 Œ±3 Œ±6 Œ±4 Œ±6 Œ±7 Œ± Œ±5 1 Œ±6
0 0
0 1 Œ±2 Œ±6 1 Œ±2 Œ±6 Œ±2 1 Œ±4 Œ±6 Œ±4 1 Œ±4 Œ±6 Œ±2 Œ±2 1 Œ±4 Œ±4 Œ±6 Œ±2 Œ±6 Œ±4 1
0 0
0 1 Œ±3 Œ± Œ±4 Œ±7 Œ±5 Œ±5 Œ±6 1 Œ±3 Œ±4 Œ±6 Œ±6 Œ± Œ±7 Œ± Œ±2 Œ±4 Œ±2 Œ±5 Œ±3 Œ±7 1 Œ±2
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Applying Gaussian elimination to G gives:
G =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1 1 1
1 1 1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
0 1 Œ±4 Œ±6 Œ± Œ±7 Œ±6 Œ± Œ±7 Œ±2 Œ±3 Œ±5 Œ±2 Œ±3 Œ±5 Œ±6 Œ± Œ±7 Œ±2 Œ±3 Œ±5 Œ±6 Œ± Œ±7 Œ±4 Œ±3 Œ±5
0 0 1
Œ± 1 Œ±5 Œ±
1 Œ±5 Œ±3 1 Œ±7 Œ±3 1 Œ±7 Œ±
1 Œ±5 Œ±3 1 Œ±7 Œ±
1 Œ±5 Œ±3 1 Œ±7
0 0 0
1 1 1
1
1
1 Œ±4 Œ±4 Œ±4 Œ±4 Œ±4 Œ±4 1
1
1 Œ±4 Œ±4 Œ±4 1
1
1 Œ±4 Œ±4 Œ±4
0 0 0
0 1 Œ±4 Œ±6 Œ±7 Œ±
1 Œ±7 Œ±2 Œ±3 0 Œ±7 Œ±5 Œ±4 Œ±7 Œ±5 Œ±
0 Œ±3 Œ±
1 Œ±4 Œ±2 Œ±
0 0 0
0 0 1 Œ±6 Œ±6 Œ±7 Œ±2 Œ± Œ±6 Œ±7 Œ±2 Œ±6 Œ±5 Œ±5 Œ±4 0 Œ±7 Œ±6 Œ±3 Œ±3 Œ±
Œ±
0 Œ±6
0 0 0
0 0 0
1
1
1
0
1 Œ±4 Œ±4 0
1 Œ±4 0
1 Œ±4 1
0 Œ±4 1
0
0 Œ±4 1
0 0 0
0 0 0
0
1 Œ±4 Œ±6 Œ±3 Œ±4 Œ±7 1 Œ±2 Œ±7 Œ±6 Œ±
Œ± Œ±2 Œ±4 Œ± Œ±7 Œ±6 Œ±6 1 Œ±5
0 0 0
0 0 0
0
0
1
0 Œ±7 Œ±7 1
1
0 Œ±6 0
1 Œ±2 Œ±6 Œ±4 Œ±7 Œ±5 Œ±2 Œ±2 Œ±3 0
0 0 0
0 0 0
0
0
0
1
1
1 Œ±6 Œ±6 Œ±6 Œ±5 Œ±5 Œ±5 1
1
1 Œ±5 Œ±5 Œ±5 Œ±6 Œ±6 Œ±6
0 0 0
0 0 0
0
0
0
0
1 Œ±4 0
1 Œ±4 0
1 Œ±4 Œ± Œ±6 Œ±7 Œ±6 Œ±7 Œ± Œ±7 Œ± Œ±6
0 0 0
0 0 0
0
0
0
0
0
0
1
1
1 Œ±4 Œ±4 Œ±4 Œ±5 Œ±5 Œ±5 Œ±3 Œ±3 Œ±3 Œ±2 Œ±2 Œ±2
0 0 0
0 0 0
0
0
0
0
0
0
0
0
0
1
1
1 Œ±7 Œ±7 Œ±7 1
1
1 Œ±5 Œ±5 Œ±5
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
The swapping of rows also swaps the monomials but this does not affect the
code. After Gaussian elimination we can see that the main diagonal of 1s does not
continue at the twelfth column. Therefore, column 12 is swapped with column 13.
Next, column 16 is swapped with column 13 so that the main diagonal is complete.

DECODING ALGEBRAIC‚ÄìGEOMETRIC CODES
119
After applying Jordan elimination to G:
G =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1 0 0 0 0 0 0 0 0 0 0 0 0
Œ±5 Œ±4 Œ± Œ±6 Œ±5
0 Œ±4 Œ±7 Œ±5
1 Œ±7 Œ±7
0 Œ±7
0 1 0 0 0 0 0 0 0 0 0 0 0
Œ±7 Œ±7 Œ± Œ±5 Œ±6 Œ±3
1 Œ±5 Œ±2 Œ±5 Œ±2 Œ±3 Œ±6 Œ±2
0 0 1 0 0 0 0 0 0 0 0 0 0
Œ±4 Œ±5 Œ±
1
1
1 Œ±5 Œ±3 Œ±5 Œ± Œ±4 Œ±7 Œ±4 Œ±6
0 0 0 1 0 0 0 0 0 0 0 0 0
Œ±4 Œ±7 Œ±4
1 Œ±2 Œ±3
0
0 Œ±7 Œ±7 Œ±2 Œ±4 Œ±3 Œ±7
0 0 0 0 1 0 0 0 0 0 0 0 0
Œ±2 Œ±3 Œ±4 Œ±7 Œ±7 Œ±3 Œ±6 Œ±2
0 Œ±2 Œ±3 Œ±7
0 Œ±2
0 0 0 0 0 1 0 0 0 0 0 0 0
Œ±
0 Œ±4 Œ±2
1
0 Œ±4 Œ±5 Œ±6 Œ±2 Œ± Œ±2 Œ±6 Œ±4
0 0 0 0 0 0 1 0 0 0 0 0 0
Œ±6 Œ±5 Œ±3 Œ±3 Œ±2 Œ±5
1 Œ±7
1
Œ± Œ±7 Œ±7 Œ±2 Œ±7
0 0 0 0 0 0 0 1 0 0 0 0 0
Œ±4
0 Œ±3
1 Œ±6 Œ±6 Œ±4 Œ±2 Œ±2 Œ±3 Œ±6
0 Œ±7 Œ±4
0 0 0 0 0 0 0 0 1 0 0 0 0
Œ±3 Œ± Œ±3 Œ±
0 Œ±4
1 Œ±4 Œ±3 Œ±6 Œ±2 Œ±6 Œ±2 Œ±
0 0 0 0 0 0 0 0 0 1 0 0 0
Œ±4
1 Œ±4 Œ±4
1
Œ± Œ±7 Œ±6 Œ±4
0
1 Œ±5 Œ±2 Œ±3
0 0 0 0 0 0 0 0 0 0 1 0 0
1 Œ±4 Œ±4
1 Œ±4 Œ± Œ±6 Œ±7 Œ±6 Œ±7 Œ± Œ±7 Œ± Œ±6
0 0 0 0 0 0 0 0 0 0 0 1 0
1
1
0
0
0
1
1
1 Œ±5 Œ±5 Œ±5 Œ±7 Œ±7 Œ±7
0 0 0 0 0 0 0 0 0 0 0 0 1
0
0
0
1
1 Œ±7 Œ±7 Œ±7
1
1
1 Œ±5 Œ±5 Œ±5
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
The swapping of columns means that the points corresponding to those columns
are also swapped. For example, when column 12 was swapped with column 13,
P12 = (Œ±, Œ±5) was swapped with P13 = (Œ±7, Œ±4). The reordered points are shown
in Table 4.4.
Table 4.4
The reordered points from Table 4.3 after swapping columns.
P1 = (0, 0)
P2 = (0, Œ±6)
P3 = (0, Œ±2)
P4 = (1, 1)
P5 = (1, Œ±)
P6 = (1, Œ±3)
P7 = (Œ±4, 1)
P8 = (Œ±4, Œ±)
P9 = (Œ±4, Œ±3)
P10 = (Œ±, Œ±4)
P11 = (Œ±, Œ±7)
P12 = (Œ±7, Œ±4)
P13 = (Œ±6, 1)
P14 = (Œ±7, Œ±7)
P15 = (Œ±7, Œ±5)
P16 = (Œ±, Œ±5)
P17 = (Œ±6, Œ±)
P18 = (Œ±6, Œ±3)
P19 = (Œ±5, Œ±4)
P20 = (Œ±5, Œ±7)
P21 = (Œ±5, Œ±5)
P22 = (Œ±2, 1)
P23 = (Œ±2, Œ±)
P24 = (Œ±2, Œ±3)
P25 = (Œ±3, Œ±4)
P26 = (Œ±3, Œ±7)
P27 = (Œ±3, Œ±5)
4.3 Decoding Algebraic‚ÄìGeometric Codes
The decoding of algebraic‚Äìgeometric codes can be achieved in two parts: Ô¨Årst by
determining the error locations, and then by determining the error magnitudes at these
locations, which is similar to the decoding of Reed‚ÄìSolomon codes in Chapter 3.
Sakata‚Äôs algorithm was developed in 1988 [6] as a method of generating a set
of polynomials whose coefÔ¨Åcients formed recursive relationships among an array of
Ô¨Ånite Ô¨Åeld elements. This was an extension of the Berlekamp‚ÄìMassey algorithm to two
or more dimensions. In 1992, Justesen et al. [7] used Sakata‚Äôs algorithm to generate
a set of error-locating polynomials from a matrix containing syndrome values for
algebraic‚Äìgeometric codes in order to reduce decoding complexity. The syndromes

120
ALGEBRAIC‚ÄìGEOMETRIC CODES
Table 4.5
Variables used in the Sakata algorithm.
Variable
DeÔ¨Ånition
(a, b)
A point in the syndrome array
Sa,b
A syndrome in the array at the point (a, b)
F
The set containing the error-locating polynomials
f (i)(x, y)
The ith polynomial in the set F
G
The auxiliary set containing some of the polynomials that were in F earlier
in the algorithm
g(i)(x, y)
The ith polynomial in the set G
h(i)(x, y)
The ith polynomial after modiÔ¨Åcation
(ag, bg)
The point at which g(i)(x, y) was Ô¨Årst placed in the set G
df (dg)
The discrepancy in f (i)(x, y) (g(i)(x, y))
(t(i)
1 , t(i)
2 )
The powers of x and y respectively in the leading term of f (i)(x, y)
(u(i)
1 , u(i)
2 )
The powers of x and y respectively in the leading term of g(i)(x, y)
in the two-dimensional syndrome array are deÔ¨Åned as:
Sab =
n

i=1
rixa
i yb
i =
n

i=1
(ci + ei)xa
i yb
i =
n

i=1
eixa
i yb
i ,
(4.6)
where (a, b) is the location of the syndrome Sa,b in the array, ri is the ith received
element, ci is the ith coded symbol, ei is the error magnitude in the ith position and
(xi, yi) is the ith afÔ¨Åne point.
The majority voting scheme [8] (explained later) was added to [7] by Sakata
et al. [9] in order to increase the number of errors that could be corrected. The
decoding algorithm in [9] has many variables, so, before it is explained, a list of
deÔ¨Ånitions is provided in Table 4.5. Sakata‚Äôs algorithm takes a two-dimensional array
of syndromes calculated from the received word r = {r1,r2, . . . ,rn}, as given in (4.6).
From this array a set F of error-locating polynomials is generated, of the form:
f (i)(x, y) =

f (i)
k,l xk yl,
(4.7)
where i is the ith polynomial in F and f (i)
k,l are the coefÔ¨Åcients of the terms
xkyl in f (i)(x, y). Each syndrome from the array is read in and the polynomials in
F are modiÔ¨Åed so that the coefÔ¨Åcients in each polynomial f (i)(x, y) form recursive
equations among known syndromes, up to the current syndrome.
The recursive relationship that must be satisÔ¨Åed is [9]:

f (i)
k,l Sa‚àít(i)
1 +k,b‚àít(i)
2 +l = 0,
(4.8)
where t(i)
1
and t(i)
2
are the powers of x and y, respectively, of the leading term of
f (i)(x, y). A generalized syndrome array is shown in Figure 4.2. The syndromes are
expressed in terms of the parameter j deÔ¨Åned by (4.3). The known syndromes from

DECODING ALGEBRAIC‚ÄìGEOMETRIC CODES
121
Sj,0
S0,j
Sj+1,0
Sj‚Äì1,1
Sj,1
Sm,j+1‚Äìm
Sj+m‚Äì1,0
S0,j+m
Sj+m,0
S0,j+m‚Äì1
Sm-1,j+2‚Äìm
Sj+m‚Äì1,1
S1,j+m‚Äì1
S2,j+m‚Äì2
Sj+m‚Äì2,2
Known syndromes found from
the parity check matrix
Known syndromes found from
the recursive relationship among
the previous syndromes from the
Hermitian curve
Unknown syndromes found by the 
recursive relationship from the 
Hermitian curve or majority voting
S0,0
S1,0
S2,0
S0,1
S1,1
S2,1
S0,2
S2,1
S2,2
Figure 4.2
The generalized initial syndrome array.
S0,0 to S0,j are calculated from (4.6). Further known syndromes Sj+1,0 to Sm, j‚àím+1
can be calculated by substituting the curve into (4.7) to obtain a recursive relationship
among the known syndromes, in order to determine the value of all syndromes Sa,b
where a ‚â•m, the degree of the curve. The remaining unknown syndromes Sm‚àí1, j‚àím+2
to S0,j can be found using (4.8) or the majority voting scheme (explained later).
For the Hermitian curve, C(x, y) = xr+1 + yr + y with m = r + 1, the recursion
is:

k,l
Ck,lSa‚àít(i)
1 +k,b‚àít(i)
2 +l = 0
C0,1Sa‚àím+0,b‚àí0+1 + C0,m‚àí1Sa‚àím+0,b‚àí0+m‚àí1 + Cm,0Sa‚àím+m,b‚àí0+0 = 0
,
where Ck,l is a coefÔ¨Åcient of C(x, y), and k and l are the powers of the x and y term,
respectively, for each term in C(x, y). Since all the coefÔ¨Åcients of C(x, y) are equal to
1, this simpliÔ¨Åes to:
Sa,b = Sa‚àím,b+1 + Sa‚àím,b+m‚àí1.
(4.9)
(4.9) is important for the decoding procedure and is used to Ô¨Ånd the known syn-
dromes Sj+1,0 to Sm, j+1‚àím, as shown in Figure 4.2.

122
ALGEBRAIC‚ÄìGEOMETRIC CODES
If all the polynomials in F satisfy (4.8) then there is no need to modify F. If
a polynomial in F makes (4.7) nonzero, this polynomial is said to have a discrep-
ancy, df, and may be used in the modiÔ¨Åcation process. This is the same as  in the
Berlekamp‚ÄìMassey algorithm. The order in which each syndrome in the array is read
in is determined by the total graduated degree order, denoted by <T [6]. Let (a, b) be
a point in the syndrome array. The next point, (a, b), according to the total graduated
degree order, is [6]:
(a, b) =
(a ‚àí1, b + 1),
if a > 0
(b + 1, 0),
if a = 0.
(4.10)
This gives a degree ordering of {(0, 0) <T (1, 0) <T (0, 1) <T (2, 0) <T (1, 1) . . .},
meaning that the order the syndromes are read in from the array is S0,0, S1,0, S0,1, S2,0,
S1,1 and so on.
An important set is the auxiliary set G, which stores some of the polynomials
that had a nonzero discrepancy at an earlier point in the algorithm (ag, bg). Each
polynomial g(i)(x, y) in G has a quantity called the ‚Äòspan‚Äô, deÔ¨Åned as:
span(g(i)(x, y)) =

ag ‚àíu(i)
1 , bg ‚àíu(i)
2

,
(4.11)
where u(i)
1 and u(i)
2 are the degrees of x and y, respectively, of the leading term in g(i)(x,
y). The span of a polynomial means that there is no polynomial with leading term
xag‚àíu(i)
11 ybg‚àíu(i)
2 that can satisfy (4.8) at the point (a, b). The union of all sets less than or
equal to each span(g(i)(x, y)) in G deÔ¨Ånes the set span(G). If there are œï polynomials
in G then span(G) is deÔ¨Åned as:
span(G) =
œï

i=1

(k,l)|(k,l) ‚â§span (g(i)(x, y))

,
(4.12)
where (k, l) are a pair of nonnegative integers. The span of G has interior and exterior
corners. An interior corner is deÔ¨Åned as a maximal point inside span(G) with respect
to the partial ordering, and an exterior corner is deÔ¨Åned as a minimal point outside
span(G) with respect to the partial ordering denoted by <. For two integer pairs
a = (a1, a2) and b = (b1, b2) [6]:
a < b if a1 ‚â§b1 ‚àßa2 ‚â§b2 ‚àßa 	= b.
(4.13)
For example, (3, 5) < (4, 5) because 3 ‚â§5 and 4 ‚â§5 and (3, 5) 	= (4, 5). It is
easier to see the corners by drawing span(G). An example is shown in Figure 4.3 for
span(G) = {(0, 0), (1, 0), (0, 1), (2, 0)}.
The interior corners, shown as the large black circles, are (0, 1) and (2, 0), since
there are no points in span(G) greater than either of these points. Similarly, the exterior
corners, shown as the large white circles, are (3, 0), (1, 1) and (0, 2), since there are
no points outside span(G) less than these points.

DECODING ALGEBRAIC‚ÄìGEOMETRIC CODES
123
3
2
1
0
0
1
2
Figure 4.3
An example of span(G) showing interior (large black circles) and exterior (large white
circles) corners.
The number of exterior corners gives the number of polynomials in F, and their
values give the degrees of the polynomials. In this example there are three polynomials
in F with leading terms x3, xy and y2. As the set F changes there is also a corresponding
set, deÔ¨Åned as [6]:
 =
Œª‚àí1

k=1
k,
(4.14)
where Œª is the total number of polynomials in F and [6]:
k =
1
(k,l)|(k,l) ‚â§

t(k)
1
‚àí1, t(k+1)
2
‚àí1
2
,
(4.15)
where (k, l) are a pair of nonnegative integers. It turns out that the set  is equal
to span(G) and is used in this decoding algorithm for the majority-voting scheme
described later.
4.3.1 ModiÔ¨Åcation of the Sets F and G
In the Sakata algorithm the set G is Ô¨Årst modiÔ¨Åed by calculating the spans of the
polynomials currently in G, and also the polynomials in F that have a nonzero
discrepancy. The polynomials in F with df 	= 0 are placed in a set FN and are also added
to G to create a new set G, that is G = G ‚à™FN. The span of each polynomial in G is
calculated using (4.11) and then used to calculate span(G) from (4.12). The interior
corners are then found. A polynomial in G with a span equal to one of the interior
corners is kept in G and the others are removed. If more than one polynomial has a
span equal to the same interior corner then either one can be kept. The discrepancy
dg of the polynomials in G and the point in the syndrome array (ag, bg) where their
discrepancies were nonzero are also stored. The set G is now modiÔ¨Åed and will be
used for the next point in the syndrome array.
The modiÔ¨Åcation of F also uses span(G). The exterior corners are found and these
will give the number of polynomials and the leading terms of these polynomials in the

124
ALGEBRAIC‚ÄìGEOMETRIC CODES
modiÔ¨Åed set F. For each exterior corner (Œµ1, Œµ2) the polynomials in F are modiÔ¨Åed
using one of three possible cases [10, 11]:
Case 1
If there is a polynomial f (i)(x, y) in the difference set F/FN with (t(i)
1 , t(i)
2 ) =
(Œµ1, Œµ2) then the new polynomial h(i)(x, y) ‚ààF is unchanged:
h(i)(x, y) = f (i)(x, y).
(4.16)
Case 2
Else if Œµ1 > a or Œµ2 > b, Ô¨Ånd a polynomial f (i)(x, y) ‚ààFN with (t(i)
1 , t(i)
2 ) ‚â§
(Œµ1, Œµ2). The new minimal polynomial h(i)(x, y) ‚ààF is given by:
h(i)(x, y) = xŒµ1‚àít(i)
1 yŒµ2‚àít(i)
2 f (i)(x, y).
(4.17)
Case 3
Otherwise, Ô¨Ånd a polynomial f (i)(x, y) ‚ààFN with (t(i)
1 , t(i)
2 ) ‚â§(Œµ1, Œµ2) and
a polynomial g(i)(x, y) ‚ààG with span(g(i)(x, y)) ‚â§(a ‚àít(i)
1 , b ‚àít(i)
2 ). Let (p1, p2) =
span(g(i)(x, y)) ‚àí(a ‚àíŒµ1, b ‚àíŒµ2). The new minimal polynomial h(i)(x, y) ‚ààF is then
given by:
h(i)(x, y) = xŒµ1‚àít(i)
1 yŒµ2‚àít(i)
2 f (i)(x, y) ‚àíd f
dg
x p1 y p2g(i)(x, y).
(4.18)
The full algorithm can now be described [8, 11, 12]:
1. Initialize (a, b) = (0, 0), F = {1}, G = √ò, D = √ò.
2. FN = √ò, F = √ò.
Calculate the set  from (4.14) and (4.15).
If || exceeds the maximum number of errors the code can correct, terminate the
algorithm.
Else Ô¨Ånd the discrepancy df of each polynomial f (i)(x, y) ‚ààF for the syndrome
Sa,b using (4.8). Every f (i)(x, y) with df 	= 0 is placed in FN and G = G ‚à™FN.
If all f (i)(x, y) ‚ààF have no discrepancy then increment (a, b) with respect to the
total order <T and go to step 2.
3. Find the span of each g(i)(x, y) ‚ààG using (4.10), then Ô¨Ånd the span of G‚Äô using
(4.11).
4. Find the interior corners of span(G) and remove all g(i)(x, y) ‚ààG whose spans are
not equal to any of the interior corners. The modiÔ¨Åcation of G is now complete.
Also store the discrepancies dg of each g(i)(x, y) ‚ààG and the point in the Sakata
algorithm where each g(i)(x, y) had a nonzero discrepancy as (ag, bg).
5. Find the exterior corners of span(G) and apply Case 1, 2 or 3 to create the
modiÔ¨Åed minimal polynomials h(i)(x, y) in the set F.
6. Set F = F, G = G and increment (a, b) with respect to the total order <T.
7. If the last element in the syndrome array has been reached end the algorithm. The
set F contains the error-locating polynomials. Else go to step 2.

MAJORITY VOTING
125
In step 2, a stopping criterion was added by the authors [11] and the decoding algo-
rithm is terminated when the number of elements in the set  exceeds the maximum
number of errors that the code can correct. This is necessary because the majority
voting scheme is unreliable if  is too large and it can choose an incorrect value for
the unknown syndrome, affecting the remainder of the decoding algorithm.
All that remains is to substitute the points from the curve into one of the error-
locating polynomials in F. The points that make the error-locating polynomial vanish
are the error locations.
4.4 Majority Voting
The Berlekamp‚ÄìMassey algorithm is able to correct up to the maximum number of
errors, t, that the code is capable of correcting with the knowledge of 2t syndromes.
However, the Sakata algorithm can only correct up to:
t ‚â§
d‚àó‚àíŒ≥ ‚àí1
2

,
(4.19)
with only the known syndromes. Unknown syndromes of the type Sa,b, a ‚â§m can be
calculated by substituting the curve into (4.8) to obtain a recursive relationship among
the previous syndromes, such as for the Hermitian curve in (4.9), but this can only
be used if the previous syndromes are actually known. For example, for a Hermitian
code constructed from a Hermitian curve with degree m = 5, (4.9) can be used to
calculate the syndrome S7,0:
S7,0 = S7‚àí5,0 + S7‚àí5,0+5‚àí1
= S2,1 + S2,4
,
but only if the values of S2,0 and S2,4 are known. The unknown syndromes of the type
Sa,b, a < m, are determined using the majority voting scheme, presented by Feng and
Rao [8]. In this chapter, hard-decision decoding of AG codes is accomplished using the
decoding algorithm by Sakata et al. [9], which uses majority voting in the following
way: from [9] a candidate syndrome value vi from the ith minimal polynomial f (i)(x,
y) ‚ààF can be calculated using (4.8):

(k,l)‚â§T

t(i)
1 ,t(i)
2
 f (i)
k,l Sk+a‚àít(i)
1 ,l+b‚àít(i)
2 = ‚àívi.
(4.20)
However, it is not always possible to use (4.20) so a candidate syndrome value wi
can be calculated:

(k,l)‚â§T

t(i)
1 ,t(i)
2
 f (i)
k,l Sk+a+m‚àít(i)
1 ,l+b‚àím+1‚àít(i)
2 ‚àíSa,b‚àím+2 = ‚àíwi.
(4.21)

126
ALGEBRAIC‚ÄìGEOMETRIC CODES
In some cases both criteria are satisÔ¨Åed and f (i)(x, y) can give two candidate values,
vi and wi. If neither of these conditions is satisÔ¨Åed then the polynomial f (i)(x, y) is not
used to Ô¨Ånd a candidate syndrome value. Which equation is used to Ô¨Ånd candidate
values of the syndromes depends on two conditions:
1. If a = t(i)
1 and b = t(i)
2 then use (4.20).
2. If a + m = t(i)
1 and b ‚àít(i)
2 = m ‚àí1 then use (4.21).
Now deÔ¨Åne a set K = K1 ‚à™K2 [9]:
K1 = {(k,l)|0 ‚â§k ‚â§a ‚àß0 ‚â§l ‚â§b}
K2 = {(k,l)|0 ‚â§k < m ‚àß0 ‚â§l ‚â§b ‚àím + 1},
(4.22)
where (k, l) are a pair of nonnegative integers. Also deÔ¨Åne two further sets:
Ai =
1
(k,l) ‚ààK|k + t(i)
1 ‚â§a ‚àßl + t(i)
2 ‚â§b
2
Bi =
1
(k,l) ‚ààK|k + t(i)
1 ‚â§a + m ‚àßl + t(i)
2 ‚â§b ‚àím + 1
2.
(4.23)
Let Œ¥1, Œ¥2, Œ¥3, . . . be the candidate syndrome values obtained from (4.20) or (4.21)
with
Kj =
Ô£´
Ô£≠3
vi=Œ¥ j
Ai ‚à™
3
wi=Œ¥ j
Bi
Ô£∂
Ô£∏
4

(4.24)
associated with each candidate syndrome value. The set Kj containing the maximum
number of elements implies that the corresponding candidate syndrome value Œ¥j is the
correct value.
4.5 Calculating the Error Magnitudes
Previously, the error magnitudes were found by solving (4.6), but this can become too
complex. Sakata et al. [9] used a two-dimensional inverse discrete Fourier transform
(IDFT) that can calculate the error magnitudes with less operations but requires the
knowledge of many unknown syndromes. The error magnitude at the ith point on the
curve ei is given as:
ei =
q‚àí2

a=0
q‚àí2

b=0
Sa,bx‚àía
i
y‚àíb
i
,
(4.25)
where q is the size of the Ô¨Ånite Ô¨Åeld. However, (4.25) can only be used if an error
occurs at a point where both coordinates are nonzero. The Hermitian curves, for
example, also have points with a zero coordinate, so another method must be used

CALCULATING THE ERROR MAGNITUDES
127
when errors occurs at these points. Liu [13] addressed this problem for Hermitian
codes by using a one-dimensional IDFT, the properties of the Hermitian curve and
knowledge of more unknown syndromes up to Sq‚àí1,q‚àí1. The points of the Hermitian
curve are split into four types: all points with both terms nonzero are labelled P(x,x);
all points with a zero x term and a nonzero y term are labelled P(0,x); all points with
a nonzero x term and a zero y term are labelled P(x,0); the remaining point with both
terms zero is labelled P(0,0). The following mapping is deÔ¨Åned:
m ‚Üí
Œ±m,
0 ‚â§m ‚â§q ‚àí2
0,
m = q ‚àí1
.
(4.26)
For errors that occurred at points of type P(0,x) the one-dimensional IDFT is:
En =
q‚àí2

i=0
S0,q‚àí1‚àíiŒ±ni,
(4.27)
where En is the sum of all the error values at the points with y-coordinate Œ±n, and Œ±
is a primitive element in the Ô¨Ånite Ô¨Åeld. Fortunately, for Hermitian curves, if there
is a point (0, Œ±n) then there will be no points of the form (Œ±m, Œ±n), so En is actually
the error value that occurred at the point (0, Œ±n). Similarly, for errors that occurred at
points with a zero y-coordinate, the one-dimensional IDFT is:
Em =
q‚àí2

i=0
Sq‚àí1‚àíi,0Œ±mi,
(4.28)
where Em is the sum of all the error values at the points with x-coordinate Œ±m, and Œ±
is a primitive element in the Ô¨Ånite Ô¨Åeld. Again, for Hermitian curves, it turns out that
there are no other points with x-coordinate Œ±m so Em is the error value that occurred
at the point (Œ±m, 0). Finally, if an error occurred at the point (0, 0) we can simply use
the properties of the Hermitian curve. We know that the syndrome S0,0 is the sum of
all the errors that occurred. To Ô¨Ånd the error value at the point P(0,0) we subtract all
the error values that occurred at the points P(x,x), the error values that occurred at the
points P(0,x) and all the error values that occurred at the points P(x,0). For the Hermitian
curve we have the following relationships:

Pi‚ààP(x,x)
ei = Sq‚àí1,q‚àí1

Pi‚ààP(0,x)
ei = S0,q‚àí1 ‚àíSq‚àí1,q‚àí1

Pi‚ààP(x,0)
ei = Sq‚àí1,0 ‚àíSq‚àí1,q‚àí1
.
(4.29)

128
ALGEBRAIC‚ÄìGEOMETRIC CODES
Therefore, the error at the point P1 = (0, 0) is:
e1 =

i
ei ‚àí

Pi‚ààP(x,x)
ei ‚àí

Pi‚ààP(0,x)
ei ‚àí

Pi‚ààP(x,0)
ei
= S0,0 ‚àíSq‚àí1,q‚àí1 ‚àí(S0,q‚àí1 ‚àíSq‚àí1,q‚àí1) ‚àí(Sq‚àí1,0 ‚àíSq‚àí1,q‚àí1)
.
(4.30)
For Hermitian codes deÔ¨Åned over a Ô¨Ånite Ô¨Åeld with a Ô¨Åeld characteristic of 2, this
simpliÔ¨Åes to:
e1 = S0,0 + S0,q‚àí1 + Sq‚àí1,0 + Sq‚àí1,q‚àí1.
(4.31)
4.6 Complete Hard-Decision Decoding Algorithm for Hermitian Codes
The complete AG decoding algorithm used to obtain the simulation results presented
later in this chapter can be found in [11]:
1. Find the known syndromes S0,0 to S0,j using (4.6).
2. Use (4.9) to Ô¨Ånd more known syndromes Sj+1,0 to Sm, j+1‚àím.
3. Error location.
Apply the Sakata algorithm using known syndromes as the input:
For syndrome = S0,0 to Sm, j+1‚àím
{
Run Sakata Algorithm
if syndrome is of the form Sa,b, b ‚â•m ‚àí1 then Ô¨Ånd more unknown
syndromes using (4.9)
}
Apply the Sakata Algorithm using unknown syndromes as the input:
For syndrome = Sm‚àí1, j+2‚àím to S0, j+m
{
if syndrome is of the form Sa,b,a ‚â•m
{
Use (4.9) to Ô¨Ånd the value of the unknown syndrome
}
else if syndrome is of the form Sa,b,a < m

COMPLETE HARD-DECISION DECODING ALGORITHM FOR HERMITIAN CODES
129
{
Use the ‚ÄòMajority Voting‚Äô scheme
}
Run Sakata Algorithm
if syndrome is of the form Sa,b,b ‚â•m ‚àí1 then Ô¨Ånd more unknown
syndromes using (4.9)
}
/* End of Sakata algorithm and majority voting */
Find the error locations by substituting each point of the Hermitian curve into
one of the minimal (error-locating) polynomials in F. The roots of the minimal
polynomial are the error locations.
4. Error magnitudes:
Find the remaining unknown syndromes:
For syndrome = Sj+m+1,0 to Sq‚àí1,q‚àí1
{
if syndrome is of the form Sa,b,a ‚â•m
{
Use (4.9) to Ô¨Ånd the value of the unknown syndrome
}
else if syndrome is of the form Sa,b,a < m
{
Substitute the last minimal polynomial in the set F into (4.8)
to form a recursive relationship among the syndromes to the
Ô¨Ånd the value of the unknown syndrome
}
}
Use the inverse discrete Fourier transforms to Ô¨Ånd the error values:
 If the error location has both coordinates nonzero, use (4.25).
 Else if the error location has x-coordinate zero and y-coordinate nonzero, use
(4.27).
 Else if the error location has y-coordinate zero and x-coordinate nonzero, use
(4.28).
 Else if the error location occurred at the point (0, 0), use (4.31).
5. The error values found from step 4 at the error locations found from step 3 are
added to the received word to give the decoded code word. Since the code is
systematic, the original message can be extracted by taking the Ô¨Årst k symbols
from the decoded code word.

130
ALGEBRAIC‚ÄìGEOMETRIC CODES
Example 4.4: Using Sakata‚Äôs algorithm with majority voting and an IDFT to
decode a Hermitian code: In this example we use the (64, 44, 15) Hermitian code
deÔ¨Åned over GF(16), which can correct up to seven symbol errors. The Hermitian
curve used is:
C(x, y) = x5 + y4 + y,
(4.32)
which, from (2.12), has a genus Œ≥ = 6 and has 64 afÔ¨Åne points that satisfy
C(x, y) = 0, given in Table 4.6.
Table 4.6
The 64 afÔ¨Åne points that satisfy C(x, y) = 0.
P1 = (0, 0)
P2 = (0, 1)
P3 = (0, Œ±5)
P4 = (0, Œ±10)
P5 = (1, Œ±)
P6 = (1, Œ±4)
P7 = (1, Œ±2)
P8 = (1, Œ±8)
P9 = (Œ±, Œ±9)
P10 = (Œ±, Œ±7)
P11 = (Œ±, Œ±6)
P12 = (Œ±, Œ±13)
P13 = (Œ±4, Œ±9)
P14 = (Œ±4, Œ±7)
P15 = (Œ±4, Œ±6)
P16 = (Œ±4, Œ±13)
P17 = (Œ±2, Œ±3)
P18 = (Œ±2, Œ±14)
P19 = (Œ±2, Œ±11)
P20 = (Œ±2, Œ±12)
P21 = (Œ±8, Œ±3)
P22 = (Œ±8, Œ±14)
P23 = (Œ±8, Œ±11)
P24 = (Œ±8, Œ±12)
P25 = (Œ±5, Œ±3)
P26 = (Œ±5, Œ±14)
P27 = (Œ±5, Œ±11)
P28 = (Œ±5, Œ±12)
P29 = (Œ±10, Œ±9)
P30 = (Œ±10, Œ±7)
P31 = (Œ±10, Œ±6)
P32 = (Œ±10, Œ±13)
P33 = (Œ±3, Œ±)
P34 = (Œ±3, Œ±4)
P35 = (Œ±3, Œ±2)
P36 = (Œ±3, Œ±8)
P37 = (Œ±14, Œ±3)
P38 = (Œ±14, Œ±14)
P39 = (Œ±14, Œ±11)
P40 = (Œ±14, Œ±12)
P41 = (Œ±9, Œ±)
P42 = (Œ±9, Œ±4)
P43 = (Œ±9, Œ±2)
P44 = (Œ±9, Œ±8)
P45 = (Œ±7, Œ±9)
P46 = (Œ±7, Œ±7)
P47 = (Œ±7, Œ±6)
P48 = (Œ±7, Œ±13)
P49 = (Œ±6, Œ±)
P50 = (Œ±6, Œ±4)
P51 = (Œ±6, Œ±2)
P52 = (Œ±6, Œ±8)
P53 = (Œ±13, Œ±9)
P54 = (Œ±13, Œ±7)
P55 = (Œ±13, Œ±6)
P56 = (Œ±13, Œ±)
P57 = (Œ±11, Œ±3)
P58 = (Œ±11, Œ±14)
P59 = (Œ±11, Œ±11)
P60 = (Œ±11, Œ±12)
P61 = (Œ±12, Œ±)
P62 = (Œ±12, Œ±4)
P63 = (Œ±12, Œ±2)
P64 = (Œ±12, Œ±8)
From (4.3), the parameter j can have any value from j = 3 to j = 12. Choosing
j = 5 and substituting this into (4.4) we get a message length of k = 44 and a
designed minimum distance of d
‚àó= 15. It is assumed that the received word has
errors at the following afÔ¨Åne points:
e6 = Œ±12 at P6 = (1, Œ±4)
e13 = Œ±12 at P13 = (Œ±4, Œ±9)
e27 = Œ±12 at P27 = (Œ±5, Œ±11)
e32 = Œ±12 at P32 = (Œ±10, Œ±13)
e33 = Œ±12 at P33 = (Œ±3, Œ±)
e45 = Œ±12 at P45 = (Œ±7, Œ±9)
e51 = Œ±12 at P51 = (Œ±6, Œ±2)
The known syndromes Sa,b, where a + b ‚â§j = 5, can be calculated using (4.6).
The initial syndromes are shown in Figure 4.4. The syndromes S6,0 and S5,1 are
found using the recursive relationship of (4.9):
Sa,b = Sa‚àí5,b+1 + Sa‚àí5,b+4.

COMPLETE HARD-DECISION DECODING ALGORITHM FOR HERMITIAN CODES
131
Œ±3
Œ±9
0
Œ±6
0
0
Œ±2 Œ±10 Œ±14 Œ±3 Œ±4
Œ±12
Œ±7
Œ±4
Œ±8
Œ±8
Œ±11 Œ±4 Œ±12
Œ±2
Œ±10
Œ±2
2
1
0
5
4
3
6
0
1
2
3
4
5
a
b
Œ±12
Figure 4.4
Initial syndrome array.
Sa,b = Sa‚àí5,b+1 + Sa‚àí5,b+4
Therefore,
S6,0 = S1,1 + S1,4 = Œ±10 + Œ±10 = 0
S5,1 = S1,1 + S1,4 = Œ±7 + Œ±2 = (Œ±3 + Œ± + 1) + Œ±2 = Œ±12 .
The parameters in Sakata‚Äôs algorithm are then initialized as follows: (a, b) =
(0, 0), F = {1}, G = √ò and  = √ò, FN = √ò.
From (4.8), the discrepancy df of f (1) = 1 when Sa,b = Œ±12 is determined to be:
d f =

(k,l)‚â§T (t(1)
1 ,t(1)
2 )
f (i)
k,l Sa‚àít(i)
1 +k,b‚àít(i)
2 +l = f (1)
0,0 S0‚àí0+0,0‚àí0+0 = 1.S0,0 = Œ±12.
Hence, FN = {1} and G = G ‚à™FN = {1}, and we also store this discrepancy,
dg = df, and the point at which the discrepancy occurred, (ag, bg) = (0, 0).
Next, span(G) is found using (4.11) and (4.12). The x-degree and y-degree of
g(x, y) = 1 are u(1)
1 = 0 and u(1)
2 = 0, respectively, and so span(1) = (0 ‚àí0, 0 ‚àí
0) = (0, 0}.
Therefore,
span(G) =
œï

i=1

(k,l)|(k,l) ‚â§span(g(i)(x, y))

=
1

i=1
{(k,l)|(k,l) ‚â§(0, 0)} = {(0, 0)}.
If we represent span(G) graphically, as in Figure 4.5, we can see that it has only
one interior corner, at (0, 0), and two exterior corners, at (1, 0) and (0, 1).
Without performing any of the update procedures for sets F and G, we know that
since there is only one interior corner, (0, 0), G will contain a single polynomial

132
ALGEBRAIC‚ÄìGEOMETRIC CODES
0
2
1
0
1
2
k
l
Figure 4.5
The set  after the Ô¨Årst modiÔ¨Åcation to the polynomials in F.
with a span equal to (0, 0). Also, since there are two exterior corners, (1, 0) and
(0, 1), F will contain two polynomials with leading monomials x and y, respectively.
To update G, we keep only those polynomials in G that have spans equal to all in-
terior corners of span(G). In this case, the polynomial g = 1 is the only polynomial
in G and its span is equal to (0, 0), so this is kept for G. This completes the updating
of G.
To update F, we take each exterior corner of span(G) and apply Case 1, 2 or 3.
For the exterior corner (Œµ1, Œµ2) = (1, 0) we cannot apply Case 1 since the difference
set F/FN is empty. However, we can apply Case 2, since Œµ1 > a = 0, so:
h(1)(x, y) = xŒµ1‚àít(i)
1 yŒµ2‚àít(i)
2 f (1)(x, y) = x1‚àí0y0‚àí01 = x.
Similarly, for the exterior corner (Œµ1, Œµ2) = (0, 1), Case 2 can also be applied,
since Œµ2 > b, so:
h(2)(x, y) = xŒµ1‚àít(i)
1 yŒµ2‚àít(i)
2 f (1)(x, y) = x0‚àí0y1‚àí01 = y.
Therefore, for (a, b) = (1, 0) we have F = {x, y), G = {1} and  = {(0, 0)}. The
outputs of the algorithm for all the syndromes up to the Ô¨Årst unknown syndrome
S4,2 are given in Table 4.7.
Observations from Table 4.7:
 The set F always contains one more polynomial that the set G.
 For a t-error-correcting AG code, the number of elements in  never exceeds
t if there are no more that t errors in the received word.
 Once || = t, the number of polynomials in F and their leading monomials
never change. The increasing size of  is shown in Figure 4.6.
The polynomials in F are now used to determine the value of the Ô¨Årst unknown
syndrome, S42. The Ô¨Årst polynomial in F is f (1)(x, y) = Œ±14 + Œ±x + Œ±7y + Œ±4x2 +
xy + Œ±7x3 + Œ±x2y + x4, with leading degree (t(1)
1 , t(2)
2 ) = (4, 0). A candidate value

COMPLETE HARD-DECISION DECODING ALGORITHM FOR HERMITIAN CODES
133
Table 4.7
ModiÔ¨Åcation of the set of minimal polynomials using Sakata‚Äôs algorithm for each
syndrome in the array of Figure 4.4.
a, b
Sa,b
F
df

G
dg
ag, bg
0, 0
Œ±12
1
Œ±12
√¶
√¶
‚Äî
‚Äî
1, 0
Œ±3
x
y
Œ±3
0
{(0, 0)}
1
Œ±12
0, 0
0, 1
Œ±2
Œ±6+x
y
0
Œ±2
{(0, 0)}
1
Œ±12
0, 0
2, 0
Œ±9
Œ±6 + x
Œ±5 + y
0
0
{(0, 0)}
1
Œ±12
0, 0
1, 1
Œ±10
Œ±6 + x
Œ±5 + y
Œ±
Œ±
{(0, 0)}
1
Œ±12
0, 0
0, 2
Œ±7
Œ±6x + x2
Œ±4 + Œ±6y + xy
Œ±5y + y2
0
0
0
{(0, 0)(1, 0),
(0, 1)}
Œ±6 + x
Œ±5 + y
Œ±
Œ±
1, 1
1, 1
3, 0
0
Œ±6x + x2
Œ±4 + Œ±6y + xy
Œ±5y + y2
1
0
0
{(0, 0)(1, 0),
(0, 1)}
Œ±6 + x
Œ±5 + y
Œ±
Œ±
1, 1
1, 1
2, 1
Œ±14
Œ±4 + Œ±6x + Œ±14y + x2
Œ±4 + Œ±6y + xy
Œ±5y + y2
Œ±7
0
0
{(0, 0)(1, 0),
(0, 1)}
Œ±6 + x
Œ±5+y
Œ±
Œ±
1, 1
1, 1
1, 2
Œ±4
Œ±6 + Œ±14y + x2
Œ±4 + Œ±6y + xy
Œ±5y + y2
0
Œ±
Œ±
{(0, 0)(1, 0),
(0, 1)}
Œ±6 + x
Œ±5 + y
Œ±
Œ±
1, 1
1, 1
0, 3
Œ±11
Œ±6 + Œ±14y + x2
Œ±12 + x + Œ±6y + xy
Œ±5 + Œ±10y + y2
0
0
1
{(0, 0)(1, 0),
(0, 1)}
Œ±6 + x
Œ±5 + y
Œ±
Œ±
1, 1
1, 1
4, 0
Œ±6
Œ±6 + Œ±14y + x2
Œ±12 + x + Œ±6y + xy
Œ±14x + Œ±10y + y2
0
0
0
{(0, 0)(1, 0),
(0, 1)}
Œ±6 + x
Œ±5 + y
Œ±
Œ±
1, 1
1, 1
3, 1
Œ±3
Œ±6 + Œ±14y + x2
Œ±12 + x + Œ±6y + xy
Œ±14x + Œ±10y + y2
Œ±
Œ±
0
{(0, 0)(1, 0),
(0, 1)}
Œ±6 + x
Œ±5 + y
Œ±
Œ±
1, 1
1, 1
2, 2
Œ±8
Œ±6 + Œ±13x + Œ±14xy + x3
Œ±5 + Œ±13y + Œ±14y2 + x2y
Œ±14x + Œ±10y + y2
0
Œ±12
Œ±12
{(0, 0), (1, 0),
(0, 1), (2, 0),
(1, 1)}
Œ±6 + Œ±14y + x2
Œ±12 + x + Œ±6y
+ xy
Œ±
Œ±
3, 1
3, 1
1, 3
Œ±4
Œ±6 + Œ±13x + Œ±14xy + x3
Œ±5 + Œ±2x + Œ±13y + Œ±10xy
+ Œ±14y2 + Œ±11x3 + x2y
Œ±8 + Œ±10x + Œ±4y + Œ±11xy
+ y2
0
0
Œ±10
{(0, 0), (1, 0),
(0, 1), (2, 0),
(1, 1)}
Œ±6 + Œ±14y + x2
Œ±12 + x + Œ±6y
+ xy
Œ±
Œ±
3, 1
3, 1
(Continued)

134
ALGEBRAIC‚ÄìGEOMETRIC CODES
Table 4.7
(Continued)
a, b
Sa,b
F
df

G
dg
ag, bg
0, 4
Œ±11
Œ±6 + Œ±13x + Œ±14xy + x3
Œ±5 + Œ±2x + Œ±13y + Œ±10xy
+ Œ±14y2 + Œ±11x3 + x2y
Œ±2 + Œ±10x + Œ±5y + Œ±9x2
+ Œ±11xy + y2
0
0
0
{(0, 0), (1, 0),
(0, 1), (2, 0),
(1, 1)}
Œ±6 + Œ±14y + x2
Œ±12 + x + Œ±6y
+ xy
Œ±
Œ±
3, 1
3, 1
5, 0
0
Œ±6 + Œ±13x + Œ±14xy + x3
Œ±5 + Œ±2x + Œ±13y + Œ±10xy
+ Œ±14y2 + Œ±11x3 + x2y
Œ±2 + Œ±10x + Œ±5y + Œ±9x2
+ Œ±11xy + y2
Œ±8
0
0
{(0, 0), (1, 0),
(0, 1), (2, 0),
(1, 1)}
Œ±6 + Œ±14y + x2
Œ±12 + x + Œ±6y
+ xy
Œ±
Œ±
3, 1
3, 1
4, 1
Œ±4
Œ±12 + Œ±5x + Œ±13y + Œ±xy + x3
Œ±5 + Œ±2x + Œ±13y + Œ±10xy
+ Œ±14y2 + Œ±11x3 + x2y
Œ±2 + Œ±10x + Œ±5y + Œ±9x2
+ Œ±11xy + y2
Œ±8
Œ±4
0
{(0, 0), (1, 0),
(0, 1), (2, 0),
(1, 1)}
Œ±6 + Œ±14y + x2
Œ±12 + x + Œ±6y
+ xy
Œ±
Œ±
3, 1
3, 1
3, 2
Œ±8
Œ± + Œ±5x + y + Œ±7x2 + Œ±xy
+ x3
Œ±10 + Œ±6x + Œ±10y + Œ±12xy
+ Œ±14y2 + Œ±11x3 + x2y
Œ±2 + Œ±10x + Œ±5y + Œ±9x2
+ Œ±11xy + y2
Œ±4
Œ±4
Œ±4
{(0, 0), (1, 0),
(0, 1), 2, 0),
(1, 1)}
Œ±6 + Œ±14y + x2
Œ±12 + x + Œ±6y
+ xy
Œ±
Œ±
3, 1
3, 1
2, 3
Œ±12
Œ±x + Œ±5x2 + xy + Œ±7x3
+ Œ±x2y + x4
Œ±13 + Œ±6x + Œ±4y + Œ±3x2
+ Œ±12xy + Œ±14y2 + Œ±11x3
+ x2y
1 + Œ±6x + Œ±9y + Œ±10x2
+ Œ±11xy + Œ±9x3 + Œ±11x2y
+ xy2
Œ±2y + Œ±10xy + Œ±5y2 + Œ±9x2y
+ Œ±11xy2 + y3
0
Œ±
Œ±4
Œ±4
{(0, 0), (1, 0),
(0, 1), (2, 0),
(1, 1), (0, 2),
(3, 0)}
Œ± + Œ±5x + y
+ Œ±7x2 + Œ±xy
+ x3
Œ±6 + Œ±14y + x2
Œ±2 + Œ±10x
+ Œ±5y + Œ±9x2
+ Œ±11xy + y2
Œ±4
Œ±
Œ±4
3, 2
3, 1
3, 2
1, 4
Œ±10
Œ±x + Œ±5x2 + xy + Œ±7x3
+ Œ±x2y + x4
Œ±3x + Œ±6y + Œ±7x2 + Œ±xy
+ Œ±14y2 + x3 + x2y
Œ±7 + Œ±6x + Œ±11y + Œ±12x2
+ Œ±11xy + Œ±9x3 + Œ±11x2y
+ xy2
Œ±2x + Œ±2y + Œ±10x2 + xy
+ Œ±5y2 + Œ±9x3 + Œ±2x2y
+ Œ±12xy2 + y3
0
0
Œ±2
Œ±7
{(0, 0), (1, 0),
(0, 1), (2, 0),
(1, 1), (0, 2),
(3, 0)}
Œ± + Œ±5x + y
+ Œ±7x2 + Œ±xy
+ x3
Œ±6 + Œ±14y + x2
Œ±2 + Œ±10x
+ Œ±5y + Œ±9x2
+ Œ±11xy + y2
Œ±4
Œ±
Œ±4
3, 2
3, 1
3, 2
(Continued)

COMPLETE HARD-DECISION DECODING ALGORITHM FOR HERMITIAN CODES
135
Table 4.7
(Continued)
a, b
Sa,b
F
df

G
dg
ag, bg
0, 5
Œ±2
Œ±x + Œ±5x2 + xy + Œ±7x3
+ Œ±x2y + x4
Œ±3x + Œ±6y + Œ±7x2 + Œ±xy
+ Œ±14y2 + x3 + x2y
Œ± + Œ±2x + Œ±4y + Œ±14x2
+ Œ±10xy + Œ±10x3 + Œ±11x2y
+ xy2
Œ±12 + Œ±2x + Œ±y + Œ±7x2 + xy
+ Œ±5y2 + Œ±9x3 + Œ±2x2y
+ Œ±12xy2 + y3
0
0
0
Œ±7
{(0, 0), (1, 0),
(0, 1), (2, 0),
(1, 1), (0, 2),
(3, 0)}
Œ± + Œ±5x + y
+ Œ±7x2 + Œ±xy
+ x3
Œ±6 + Œ±14y + x2
Œ±2 + Œ±10x
+ Œ±5y + Œ±9x2
+ Œ±11xy + y2
Œ±4
Œ±
Œ±4
3, 2
3, 1
3, 2
6, 0
0
Œ±x + Œ±5x2 + xy + Œ±7x3
+ Œ±x2y + x4
Œ±3x + Œ±6y + Œ±7x2 + Œ±xy
+ Œ±14y2 + x3 + x2y
Œ± + Œ±2x + Œ±4y + Œ±14x2
+ Œ±10xy + Œ±10x3 + Œ±11x2y
+ xy2
Œ±6 + x + Œ±9y + Œ±6x2 + Œ±xy
+ Œ±5y2 + Œ±x3 + Œ±2x2y
+ Œ±12xy2 + y3
0
0
0
0
{(0, 0), (1, 0),
(0, 1), (2, 0),
(1, 1), (0, 2),
(3, 0)}
Œ± + Œ±5x + y
+ Œ±7x2 + Œ±xy
+ x3
Œ±6 + Œ±14y + x2
Œ±2 + Œ±10x
+ Œ±5y + Œ±9x2
+ Œ±11xy + y2
Œ±4
Œ±
Œ±4
3, 2
3, 1
3, 2
5, 1
Œ±12
Œ±x + Œ±5x2 + xy + Œ±7x3
+ Œ±x2y + x4
Œ±3x + Œ±6y + Œ±7x2 + Œ±xy
+ Œ±14y2 + x3 + x2y
Œ± + Œ±2x + Œ±4y + Œ±14x2
+ Œ±10xy + Œ±10x3 + Œ±11x2y
+ xy2
Œ±6 + x + Œ±9y + Œ±6x2 + Œ±xy
+ Œ±5y2 + Œ±x3 + Œ±2x2y
+ Œ±12xy2 + y3
Œ±9
Œ±
0
0
{(0, 0), (1, 0),
(0, 1), (2, 0),
(1, 1), (0, 2),
(3, 0)}
Œ± + Œ±5x + y
+ Œ±7x2 + Œ±xy
+ x3
Œ±6 + Œ±14y + x2
Œ±2 + Œ±10x
+ Œ±5y + Œ±9x2
+ Œ±11xy + y2
Œ±4
Œ±
Œ±4
3, 2
3, 1
3, 2
4, 2
?
Œ±14 + Œ±x + Œ±7y + Œ±4x2 + xy
+ Œ±7x3 + Œ±x2y + x4
Œ±14 + Œ±4x + Œ±3y + Œ±10x2
+ Œ±10xy + Œ±5y2 + x3 + x2y
Œ± + Œ±2x + Œ±4y + Œ±14x2
+ Œ±10xy + Œ±10x3 + Œ±11x2y
+ xy2
Œ±6 + x + Œ±9y + Œ±6x2 + Œ±xy
+ Œ±5y2 + Œ±x3 + Œ±2x2y
+ Œ±12xy2 + y3
?
?
?
?
{(0, 0), (1, 0),
(0, 1), (2, 0),
(1, 1), (0, 2),
(3, 0)}
Œ± + Œ±5x + y
+ Œ±7x2 + Œ±xy
+ x3
Œ±6 + Œ±14y + x2
Œ±2 + Œ±10x
+ Œ±5y + Œ±9x2
+ Œ±11xy + y2
Œ±4
Œ±
Œ±4
3, 2
3, 1
3, 2

136
ALGEBRAIC‚ÄìGEOMETRIC CODES
0
2
1
0
1
2
k
l
3
3
0
2
1
0
1
2
4
k
l
3
3
0
2
1
0
1
2
k
l
3
4
5
4
3
Figure 4.6
Graphical representation of the set  at different stages of Sakata‚Äôs algorithm from
Table 4.7.
for S42 can be obtained by using (4.20):

(k,l)‚â§T (4,0)
f (1)
k,l Sk+4‚àí4,l+2‚àí0 = ‚àív1
= Œ±14S0,2 + Œ±S1,2 + Œ±7S0,3 + Œ±4S2,2 + 1.S1,3 + Œ±7S3,2 + Œ±S2,3
= Œ±14Œ±7 + Œ±Œ±4 + Œ±7Œ±11 + Œ±4Œ±8 + Œ±4 + Œ±7Œ±8 + Œ±Œ±12
= Œ±6 + Œ±5 + Œ±3 + Œ±12 + Œ±4 + 1 + Œ±13
= (Œ±3 + Œ±2) + (Œ±2 + Œ±) + Œ±3 + (Œ±3 + Œ±2 + Œ± + 1) + (Œ± + 1) + 1
+(Œ±3 + Œ±2 + 1)
v1 = Œ±.
(4.20) can also be used to obtain candidate values for the second and third
polynomials in F; these are v2 = Œ± and v3 = Œ±.
However, the fourth polynomial in F, Œ±6 + x + Œ±9y + Œ±6x2 + Œ±xy + Œ±5y2 + Œ±x3 +
Œ±2x2y + Œ±12xy2 + y3, which has leading degree (t(1)
1 , t(2)
2 ) = (0, 3), cannot be used
to obtain a candidate value using either (4.20) or (4.21). Since the contributing
polynomials all give the same candidate value, the syndrome S42 = Œ±. For S33, all
contributing polynomials also give the same candidate value of Œ±6.
The Ô¨Årst unknown syndrome value where all contributing polynomials give
different candidate values is S24. Since the Ô¨Årst polynomial in F has leading degree

COMPLETE HARD-DECISION DECODING ALGORITHM FOR HERMITIAN CODES
137
(t(1)
1 , t(2)
2 ) = (4, 0), (4.20) can be used to obtain a candidate value:

(k,l)‚â§T (4,0)
f (1)
k,l Sk+2+5‚àí4,l+4‚àí5+1‚àí0 ‚àíS2,4‚àí5+2 = ‚àíw1
= Œ±14S3,0 + Œ±S4,0 + Œ±7S3,1 + Œ±4S5,0 + 1 ¬∑ S4,1 + Œ±7S6,0 + Œ±S5,1 + S2,1
= Œ±14 ¬∑ 0 + Œ± ¬∑ Œ±6 + Œ±7 ¬∑ Œ±3 + Œ±4 ¬∑ 0 + 1 ¬∑ Œ±4 + Œ±7 ¬∑ 0 + Œ± ¬∑ Œ±12 + Œ±14
= Œ±7 + Œ±10 + Œ±4 + Œ±13 + Œ±14
= (Œ±3 + Œ± + 1) + (Œ±2 + Œ± + 1) + (Œ± + 1) + (Œ±3 + Œ±2 + 1) + (Œ±3 + 1)
= Œ±3 + Œ± + 1
w1 = Œ±7.
For the other three contributing polynomials in F, (4.20) can be used to obtain
the candidate value of Œ±11. Now, from (4.23):
K1 = {(k,l) | 0 ‚â§k ‚â§2 ‚àß0 ‚â§l ‚â§4}
= {(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (1, 1), (1, 2), (1, 3), (1, 4),
√ó(2, 0), (2, 1), (2, 2), (2, 3), (2, 4)}
K2 = {(k,l) | 0 ‚â§k < 5 ‚àß0 ‚â§l ‚â§4 ‚àí5 + 1}
= {(0, 0), (1, 0), (2, 0), (3, 0), (4, 0)}
,
and the set K = K1 ‚à™K2 is:
K = {(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (1, 0), (1, 1), (1, 2), (1, 3), (1, 4),
√ó (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (3, 0), (4, 0)}.
From (4.22) the sets A and B are:
A1 = {(k,l) ‚ààK | k + 4 ‚â§2 ‚àßl + 0 ‚â§4} = √ò
A2 = {(k,l) ‚ààK | k + 2 ‚â§2 ‚àßl + 1 ‚â§4} = {(0, 0), (0, 1), (0, 2), (0, 3)}
A3 = {(k,l) ‚ààK | k + 1 ‚â§2 ‚àßl + 2 ‚â§4} = {(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2)}
A4 = {(k,l) ‚ààK | k + 0 ‚â§2 ‚àßl + 3 ‚â§4} = {(0, 0), (0, 1), (1, 0), (1, 1), (2, 0), (2, 1)}
B1 = {(k,l) ‚ààK | k + 4 ‚â§7 ‚àßl + 0 ‚â§0} = {(0, 0), (1, 0), (2, 0), (3, 0)}
B2 = {(k,l) ‚ààK | k + 2 ‚â§7 ‚àßl + 1 ‚â§0} = √ò
B3 = {(k,l) ‚ààK | k + 1 ‚â§7 ‚àßl + 2 ‚â§0} = √ò
B4 = {(k,l) ‚ààK | k + 0 ‚â§7 ‚àßl + 3 ‚â§0} = √ò
.
The candidate syndromes are Œ¥1 = Œ±7 and Œ¥2 = Œ±11. Finally, from (4.24):
K1 =
5
 
vi=Œ±7 Ai ‚à™ 
wi=Œ±7 Bi
!
/{(0, 0), (1, 0), (0, 1), (2, 0), (1, 1), (0, 2), (3, 0)}
= {(0, 0), (1, 0), (2, 0), (3, 0)}/{(0, 0), (1, 0), (0, 1), (2, 0), (1, 1), (0, 2), (3, 0)}
= √ò

138
ALGEBRAIC‚ÄìGEOMETRIC CODES
Œ±12 Œ±3
Œ±9
0
Œ±6
0
0
Œ±2 Œ±10 Œ±14
Œ±3
Œ±4 Œ±12
Œ±7
Œ±4
Œ±8
Œ±8
Œ±11 Œ±4
Œ±12
Œ±2
Œ±10
Œ±2
2
1
0
5
4
3
6
0
1
2
3
4
5
a
Œ±10
Œ±11
Œ±7
Œ±4
Œ±11
Œ±14
Œ±5
Œ±13
Œ±11
Œ±8
Œ±11
Œ±7
Œ±6
Œ±14
0
Œ±
Œ±
Œ±
Œ±4
Œ±8
Œ±14
Œ±9
Œ±12
Œ±13
Œ±9
1
Œ±7
Œ±12
Œ±8
Œ±2
Œ±12
Œ±2
Œ±4
0
1
Œ±8
Œ±8
Œ±11
Œ±10
7
10
9
8
6
7
8
9
10
Œ±10
0
0
1
b
Figure 4.7
The syndrome array after majority voting has been completed.
K2 =
5
 
vi=Œ±11 Ai ‚à™
 
wi=Œ±11 Bi
!
\{(0, 0), (1, 0), (0, 1), (2, 0), (1, 1), (0, 2), (3, 0)}
= A2 ‚à™A3 ‚à™A4\{(0, 0), (1, 0), (0, 1), (2, 0), (1, 1), (0, 2), (3, 0)}
= {(0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1)}/
{(0, 0), (1, 0), (0, 1), (2, 0), (1, 1), (0, 2), (3, 0)}
= {(0, 3), (1, 2), (2, 1)}
.
Therefore, there are | K1 | = 0 votes for the candidate value Œ±7 and | K2 | = 3
votes for the candidate value Œ±11, and so S24 = Œ±11. By employing majority voting
up to the last unknown syndrome, S0,10, we obtain the syndrome array shown in
Figure 4.7. The Ô¨Ånal set of minimal polynomials is:
F =
Ô£±
Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥
f (1)(x, y) = Œ±14 + Œ±x + Œ±7y + Œ±4x2 + xy + Œ±7x3 + Œ±x2y + x4
f (2)(x, y) = Œ±14 + Œ±4x + Œ±3y + Œ±10x2 + Œ±10xy + Œ±5y2 + x3 + x2y
f (3)(x, y) = Œ± + Œ±2x + Œ±4y + Œ±14x2 + Œ±10xy + Œ±10x3 + Œ±11x2y + xy2
f (4)(x, y) = Œ±6 + x + Œ±9y + Œ±6x2 + Œ±xy + Œ±5y2 + Œ±x3 + Œ±2x2y
+ Œ±12xy2 + y3
.
All four polynomials in F are error-locating polynomials, which means that
those afÔ¨Åne points that cause any of these polynomials to vanish are possible error
locations. If we take the polynomial f (4)(x, y), there are 12 points that cause it to
vanish:
P6 = (1, Œ±4)
P11 = (Œ±, Œ±6)
P13 = (Œ±4, Œ±9)
P27 = (Œ±5, Œ±11)
P31 = (Œ±10, Œ±6)
P32 = (Œ±10, Œ±13)
P33 = (Œ±3, Œ±)
P45 = (Œ±7, Œ±9)
P51 = (Œ±6, Œ±2)
P52 = (Œ±6, Œ±8)
P57 = (Œ±11, Œ±3)
P63 = (Œ±12, Œ±2)

COMPLETE HARD-DECISION DECODING ALGORITHM FOR HERMITIAN CODES
139
The remaining syndromes from S11,0 to S15,15 are found either by using (4.9) for
Sa,b, a ‚â•5, or by substituting the minimal polynomial f (4)(x, y) in F into (4.8) to
obtain the recursive equation:

(k,l)‚â§T (0,3)
f (4)
k,l Sa‚àí0+k,b‚àí3+l = 0
f0,0Sa,b‚àí3 + f1,0Sa+1,b‚àí3 + f0,1Sa,b‚àí2 + f2,0Sa+2,b‚àí3 + f1,1Sa+1,b‚àí2
+ f0,2Sa,b‚àí1 + f3,0Sa+3,b‚àí3 + f2,1Sa+2,b‚àí2 + f1,2Sa+1,b‚àí1 + f0,3Sa,b.
Making Sa,b the subject of the equation gives:
Sa,b = Œ±6Sa+1,b‚àí3 + Sa+1,b‚àí3 + Œ±9Sa,b‚àí2 + Œ±6Sa+2,b‚àí3 + Œ±Sa+1,b‚àí2
+ Œ±5Sa,b‚àí1 + Œ±Sa+3,b‚àí3 + Œ±2Sa+2,b‚àí2 + Œ±12Sa+1,b‚àí1.
(4.33)
So the syndromes S11,0, S10,1, S9,2, S8,3, S7,4, S6,5 and S5,6 are calculated using
(4.9) and have the values Œ±11, Œ±13, Œ±13, Œ±13, 0 and Œ±5, respectively. To calculate
S4,7 we would use (4.33):
S4,7 = Œ±6S4,4 + S5,4 + Œ±9S4,5 + Œ±6S6,4 + Œ±S5,5 + Œ±5S4,6 + Œ±S7,4
+ Œ±2S6,5 + Œ±12S5,6
= Œ±6Œ±4 + Œ±12 + Œ±9Œ±8 + Œ±6Œ±4 + Œ±Œ±2 + Œ±5Œ±12 + Œ± ¬∑ 0 + Œ±2Œ±5 + Œ±12Œ±8
= Œ±10 + Œ±12 + Œ±2 + Œ±10 + Œ±3 + Œ±2 + Œ±7 + Œ±5
= Œ±12 + Œ±3 + Œ±7 + Œ±5
= (Œ±3 + Œ±2 + Œ± + 1) + Œ±3 + (Œ±3 + Œ± + 1) + (Œ±2 + Œ±)
= Œ±3 + Œ±
= Œ±9
.
The complete syndrome array is given in Figure 4.8.
All that remains is to Ô¨Ånd the error magnitudes at the possible error locations.
Taking the point P6 = (1, Œ±4), we use (4.24) to determine the magnitude:
e6 =
14

a=0
14

b=0
Sa,bx‚àía
6 y‚àíb
6
=
q‚àí2

a=0
q‚àí2

b=0
Sa,b1‚àía ¬∑ Œ±‚àí4b
= S0,010Œ±0 + S1,010Œ±‚àí4 + S2,010Œ±‚àí8 + ¬∑ ¬∑ ¬∑ S14,121‚àí14Œ±‚àí3
+ S14,131‚àí14Œ±‚àí7 + S14,141‚àí14Œ±‚àí11
.
This gives a value of e6 = Œ±12. Similar calculations for the remaining points
give:
e6 = Œ±12 at P6 = (1, Œ±4)
e11 = 0 at P11 = (Œ±, Œ±6)
e13 =Œ±12 at P13 = (Œ±4, Œ±9)
e27 = Œ±12 at P27 = (Œ±5, Œ±11)
e31 = 0 at P31 = (Œ±10, Œ±6)
e32 = Œ±12 at P32 = (Œ±10, Œ±13)
e33 =Œ±12 at P33 = (Œ±3, Œ±)
e45 = Œ±12 at P45 = (Œ±7, Œ±9)
e51 = Œ±12 at P51 = (Œ±6, Œ±2)
e52 = 0 at P52 = (Œ±6, Œ±8)
e57 = 0 at P57 = (Œ±11, Œ±3)
e63 = 0 at P63 = (Œ±12, Œ±2)

140
ALGEBRAIC‚ÄìGEOMETRIC CODES
Œ±12 Œ±3
Œ±9
0
Œ±6
0
0
Œ±2 Œ±10 Œ±14
Œ±3
Œ±4
Œ±12
Œ±7
Œ±4
Œ±8
Œ±8
Œ±11 Œ±4
Œ±12
Œ±2
Œ±10
Œ±2
2
1
0
5
4
3
6
0
1
2
3
4
5
a
b
Œ±10
Œ±11
Œ±7
Œ±4
Œ±11
Œ±14
Œ±5
Œ±13
Œ±11
Œ±8
Œ±11
Œ±7
Œ±6
Œ±14
0
Œ±
Œ±
Œ±
Œ±4
Œ±8
Œ±14
Œ±9
Œ±12
Œ±13
Œ±9
1
Œ±7
Œ±12
Œ±8
Œ±2
Œ±12
Œ±2
Œ±4
0
1
Œ±8
Œ±8
Œ±11
Œ±10
7
10
9
8
6
7
8
9
10
Œ±4
Œ±8
Œ±8
Œ±10
Œ±11
0
Œ±4
Œ±8
Œ±10 Œ±10
Œ±2
Œ±13
1
Œ±3
Œ±
Œ±7
Œ±
Œ±13
Œ±
Œ±10
Œ±4
Œ±2
Œ±10
Œ±14
Œ±2
0
Œ±9
1
Œ±6
Œ±8
Œ±5
Œ±10
Œ±8
Œ±3
Œ±
Œ±3
Œ±4
Œ±
Œ±12
Œ±7
Œ±2 Œ±13 Œ±11 Œ±2
1
Œ±7
Œ±8
Œ±6
0
Œ±7
Œ±7
Œ±9
1
Œ±4
Œ±3
Œ±11
Œ±5
Œ±9
1
Œ±13
Œ±8
Œ±3
Œ±2
Œ±11 Œ±8
Œ±11
Œ±11
Œ±
Œ±9
1
Œ±4
Œ±11 Œ±9
1
Œ±
Œ±6
Œ±14
Œ±13
Œ±5
Œ±11
Œ±5
Œ±14
Œ±
Œ±14
Œ±14
Œ±10
Œ±
Œ±7
Œ±6
Œ±9
Œ±13
Œ±2
Œ±10
Œ±14
Œ±6
0
Œ±3
0
Œ±4
Œ±3
Œ±7
Œ±11
Œ±8
Œ±4
1
Œ±9
Œ±6
Œ±12
Œ±3
Œ±10 Œ±4
0
Œ±10
Œ±11
Œ±13
Œ±6
Œ±9
Œ±3
Œ±13 Œ±6
Œ±5
0
Œ±2
Œ±13 Œ±12
0
Œ±14 Œ±7
0
0
Œ±10
Œ±6
Œ±13
Œ±7
Œ±6
1
Œ±3
Œ±4
Œ±4
Œ±9
0
Œ±5
Œ±9
Œ±3
Œ±14
Œ±14 Œ±6
Œ±5
Œ±5
Œ±10 Œ±5
1
Œ±5
Œ±8
Œ±5
Œ±
Œ±11
14
13
12
11
11
12
13
14
Œ±5
15
15
Œ±12 Œ±3
Œ±9
0
Œ±6
0
0
Œ±10
1
0
0
Œ±11 0
Œ±4
Œ±8
Œ±10
0
Œ±12
Œ±7
Œ±8
Œ±8
Œ±10
Œ±2
Œ±7
Œ±11
Œ±2
Œ±2
Œ±10
Œ±11
Œ±7
Œ±4
Œ±12
0
1
Œ±13
Œ±4
Figure 4.8
The complete syndrome array.
4.7 Simulation Results
In this section, we present some simulation results evaluating the performance of
Hermitian codes in comparison with Reed‚ÄìSolomon codes. It is obvious that if we
compare Hermitian codes with Reed‚ÄìSolomon codes deÔ¨Åned over the same Ô¨Ånite
Ô¨Åeld then the Hermitian codes will always perform better due to their increased
length and larger Hamming distance. However, it is interesting to compare Hermi-
tian codes deÔ¨Åned over smaller Ô¨Ånite Ô¨Åelds and Reed‚ÄìSolomon codes deÔ¨Åned over
larger Ô¨Ånite Ô¨Åelds. A Hermitian code deÔ¨Åned over GF(64) has much better parameters
than a Reed‚ÄìSolomon code deÔ¨Åned over GF(64), but it also has better parameters
than a Reed‚ÄìSolomon code deÔ¨Åned over GF(256). For example, the (512, 314, 171)
Hermitian code deÔ¨Åned over GF(64) of rate 0.6 has a much larger minimum Ham-
ming distance than the (63, 39, 25) Reed‚ÄìSolomon code deÔ¨Åned over GF(64), and
also a larger Hamming distance than the (255, 153, 103) Reed‚ÄìSolomon code deÔ¨Åned
over GF(256). This illustrates another advantage of Hermitian codes in that improved
performance can be achieved with smaller alphabets, consequently reducing the com-
plexity of the Ô¨Ånite Ô¨Åeld arithmetic for hardware implementation.

SIMULATION RESULTS
141
1.00E‚Äì06
1.00E‚Äì05
1.00E‚Äì04
1.00E‚Äì03
1.00E‚Äì02
1.00E‚Äì01
1.00E+00
12
11
10
9
8
7
6
5
4
3
2
1
0
Eb/N0, dB
BER
Uncoded QPSK
Shannon limit
(512,314,171) Hermitian code
(255,153,103) Reed‚ÄìSolomon code
Figure 4.9
Performance comparison of the (512, 314, 171) Hermitian code deÔ¨Åned over GF(64) with
the (255, 153, 103) Reed‚ÄìSolomon code deÔ¨Åned over GF(256) on the AWGN channel.
A comparison of the (512, 314, 171) Hermitian code and (255, 153, 103)
Reed‚ÄìSolomon code is now presented in terms of simulation results on the AWGN
channel and a Rayleigh fading channel. In Figure 4.9, the performance of the two
codes is shown on the AWGN channel, with the Hermitian code achieving only a
0.3 dB coding gain over the Reed‚ÄìSolomon code.
However, under slow fading conditions where bursts of errors are more likely
to occur it can be shown that the Hermitian code can signiÔ¨Åcantly outperform the
Reed‚ÄìSolomon code. In Figure 4.10 the performance of the two codes is shown on
a Rayleigh fading channel that is frequency nonselective, with a carrier frequency
of 1.9 GHz, a velocity of 20 m/s and perfect channel estimation for a data rate of
30 kbps. It can be seen that the Hermitian code achieves a coding gain of 4.8 dB over
the Reed‚ÄìSolomon code.
In Figure 4.11 the performance of the two codes is shown on the same Rayleigh
fading channel, but with an increased data rate of 100 kbps. Under more harsh condi-
tions the Hermitian code still achieves a coding gain of 3.1 dB over the Reed‚ÄìSolomon
code, at a BER of 10‚àí6.
Finally, we also present the performance of the same Hermitian codes on magnetic
recording channels [12], as described in Chapter 1. In Figure 4.12, the performances of
the (64, 39, 20) Hermitian code and the (512, 314, 171) Hermitian code are compared
with the (15, 9, 7), (63, 39, 25) and (255, 153, 103) Reed‚ÄìSolomon codes on a magnetic
recording channel with a recording linear density of Ds = 2, which is a measure of the
intersymbol interference (ISI). The signal-to-noise ratio Et/N0 is the ratio of the energy

142
ALGEBRAIC‚ÄìGEOMETRIC CODES
1.00E‚Äì06
1.00E‚Äì05
1.00E‚Äì04
1.00E‚Äì03
1.00E‚Äì02
1.00E‚Äì01
1.00E+00
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
Eb/N0, dB
BER
Uncoded QPSK
(512,314,171) Hermitian code
(255,153,103) Reed‚ÄìSolomon code
Figure 4.10
Performance comparison of the (512, 314, 171) Hermitian code deÔ¨Åned over GF(64) with
the (255, 153, 103) Reed‚ÄìSolomon code deÔ¨Åned over GF(256) on a Rayleigh fading channel.
in the Lorentzian Pulse Et to the noise power spectral density N0 of the electronics
noise. As stated before, the Hermitian codes outperform the Reed‚ÄìSolomon codes
deÔ¨Åned over the same Ô¨Ånite Ô¨Åelds due to their increased lengths. Interestingly, the
(512, 314, 171) Hermitian code deÔ¨Åned over GF(64) still outperforms the (255, 153,
103) Reed‚ÄìSolomon code deÔ¨Åned over GF(256) on magnetic recording channels.
1.00E‚Äì06
1.00E‚Äì05
1.00E‚Äì04
1.00E‚Äì03
1.00E‚Äì02
1.00E‚Äì01
1.00E+00
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
Eb/N0, dB
BER
(512,314,171) Hermitian code
(255,153,103) Reed‚ÄìSolomon code
Uncoded QPSK
Figure 4.11
Performance comparison of the (512, 314, 171) Hermitian code deÔ¨Åned over GF(64) with
the (255, 153, 103) Reed‚ÄìSolomon code deÔ¨Åned over GF(256) on a Rayleigh fading channel.

SIMULATION RESULTS
143
1.00E‚Äì06
1.00E‚Äì05
1.00E‚Äì04
1.00E‚Äì03
1.00E‚Äì02
1.00E‚Äì01
1.00E+00
27
26
25
24
23
22
21
20
19
18
17
16
15
14
13
12
11
10
E  /N
t
0, dB
BER
EPR4 target, Ds=2
(15,9)RS code
(63,39)RS code
(64,39)AG code
(255,153)RS code
(512,314)AG code
Figure 4.12
Performance comparison of Hermitian codes and Reed‚ÄìSolomon codes on a magnetic
recording channel with a recording linear density of Ds = 2.
Figure 4.13 shows the performance of the same codes on a magnetic recording
channel with a recording linear density of Ds = 3, representing an increase in ISI.
Overall, the performance of all the codes is worse but the Hermitian codes still
outperform the Reed‚ÄìSolomon codes.
1.00E‚Äì06
1.00E‚Äì05
1.00E‚Äì04
1.00E‚Äì03
1.00E‚Äì02
1.00E‚Äì01
1.00E+00
29
28
27
26
25
24
23
22
21
20
19
18
17
16
15
14
13
12
11
10
E  /N
t
0, dB
BER
EPR4 target, Ds=3
(15,9)RS code
(63,39)RS code
(64,39)AG code
(255,153)RS code
(512,314)AG code
Figure 4.13
Performance comparison of Hermitian codes and Reed‚ÄìSolomon codes on a magnetic
recording channel with a recording linear density of Ds = 3.

144
ALGEBRAIC‚ÄìGEOMETRIC CODES
4.8 Conclusions
A class of non-binary block code called the algebraic‚Äìgeometric code has been
introduced. Such codes are constructed from the afÔ¨Åne points of a projective curve
and a set of rational functions deÔ¨Åned on that curve. Depending on the choice of
curve, very long AG codes can be constructed with very large minimum Hamming
distances. The well-known Reed‚ÄìSolomon codes are the simplest class of AG code,
constructed from the afÔ¨Åne points of the projective line. Consequently, they have
the shortest block lengths of all AG codes, and there are not many Reed‚ÄìSolomon
codes that can be constructed. However, Reed‚ÄìSolomon codes are maximum distance
separable (MDS) unlike other AG codes, where the genus of the curve reduces the
actual minimum Hamming distance.
Despite this genus penalty, AG codes still have much larger minimum Hamming dis-
tances than Reed‚ÄìSolomon codes deÔ¨Åned over the same Ô¨Ånite Ô¨Åeld and consequently
AG codes can correct much longer bursts of errors, which are common in data storage
channels and slow fading channels. A disadvantage of AG codes is their higher de-
coding complexity. Sakata‚Äôs algorithm is more complex than the Berlekamp‚ÄìMassey
algorithm, but Kotter recently showed how several Berlekamp‚ÄìMassey decoders could
be implemented in parallel, replacing Sakata‚Äôs algorithm and having a running time
equal to a single Berlekamp‚ÄìMassey decoder.
In this chapter, we have covered the construction of AG codes from Hermitian
curves. A detailed explanation of the decoding of AG codes has also been presented,
in particular the use of Sakata‚Äôs algorithm to locate errors, and the use of inverse
discrete Fourier transforms to determine the error magnitudes at these locations.
Finally, some simulation results have been presented on the AWGN and Rayleigh
fading channels, showing how a Hermitian curve deÔ¨Åned over GF(64) can outperform
a Reed‚ÄìSolomon code deÔ¨Åned over the much larger Ô¨Ånite Ô¨Åeld GF(256).
Reed‚ÄìSolomon codes are still the most commonly used coding scheme today,
but they are rapidly being replaced in communication systems with more powerful
coding schemes such as turbo and LDPC codes. However, in optical storage, the
latest devices, such as Blu-Ray and HD-DVD, still use error-correction schemes
involving Reed‚ÄìSolomon codes due to their good burst-error-correcting performance
and efÔ¨Åcient decoders. Eventually, as storage density increases and consequently the
effects of ISI become more severe, Reed‚ÄìSolomon codes will not be good enough
and they will need to be replaced. AG codes could be a possible candidate for the
error-correcting schemes in future data storage devices.
References
[1] Goppa, V.D. (1981) Codes on algebraic curves. Soviet Math. Dokl., 24, 75‚Äì91.
[2] Justesen, J., Larsen, K.J., Jensen, H. E. et al. (1989) Construction and decoding of a class of
algebraic geometry codes. IEEE Trans. Inform. Theory, IT-35, 811‚Äì21.
[3] Blake, I.F., Heegard, C., Hoholdt, T. and Wei, V. (1998) Algebraic geometry codes. IEEE Trans.
Inform. Theory, 44 (6), 2596‚Äì618.

REFERENCES
145
[4] Atkinson, K.A. (1989) An Introduction to Numerical Analysis, 2nd edn, John Wiley & Sons, Inc.,
New York, ISBN 0-471-50023-2.
[5] Heegard, C., Little, J. and Saints, K. (1995) Systematic encoding via Grobner bases for a class of
algebraic‚Äìgeometric Goppa codes. IEEE Trans. Inform. Theory, 41 (6), 1752‚Äì61.
[6] Sakata, S. (1988) Finding a minimal set of linear recurring relations capable of generating a given
Ô¨Ånite two-dimensional array. J. Symbolic Computation, 5, 321‚Äì37.
[7] Justesen, J., Larsen, K.J., Jensen, H.E. and Hoholdt, T. (1992) Fast decoding of codes from algebraic
plane curves. IEEE Trans. Inform. Theory, IT-38 (6), 1663‚Äì76.
[8] Feng, G.L. and Rao, T.R.N. (1993) Decoding algebraic geometric codes up to the designed minimum
distance. IEEE Trans. Inform. Theory, IT-39, 37‚Äì46.
[9] Sakata, S., Justesen, J., Madelung, Y. et al. (1995) Fast decoding of algebraic‚Äìgeometric codes up
to the designed minimum distance. IEEE Trans. Inform. Theory, IT-41 (5), 1672‚Äì7.
[10] Saints, K. and Heegard, C. (1995) Algebraic‚Äìgeometric codes and multidimensional cyclic codes: a
uniÔ¨Åed theory and algorithms for decoding using Grobner bases. IEEE Transactions on Information
Theory, 41 (6), 1733‚Äì51.
[11] Johnston, M. and Carrasco, R.A. (2005) Construction and performance of algebraic‚Äìgeometric
codes over the AWGN and fading channels. IEE Proc. Commun., 152 (5), 713‚Äì22.
[12] Carrasco, R.A. and Johnston, M. (2007) Hermitian codes on magnetic recording channels. 9th
International Symposium on Communication Theory and Applications, Ambleside, Lake District,
UK.
[13] Liu, C.-W. (1999) Determination of error values for decoding Hermitian codes with the inverse
afÔ¨Åne fourier transform. IEICE Trans. Fundamentals, E82-A (10), 2302‚Äì5.


5
List Decoding
5.1 Introduction
In this chapter, we discuss an alternative decoding algorithm which produces a list
of candidate messages instead of just one. This class of algorithm is known as a List
Decoding algorithm and was developed by Elias [1, 2] and Wozencraft [3]. A list
decoder has the advantage of being able to correct more errors than a conventional
decoding algorithm that only returns a single decoded message, from now on known
as a unique decoding algorithm. This is unfortunately at the expense of increased
decoding complexity.
The chapter begins by introducing the Guruswami‚ÄìSudan algorithm [4] for the list
decoding of Reed‚ÄìSolomon codes. This algorithm has two processes: interpolation
and factorization. It is shown that most of the decoding complexity of the list decoding
algorithm is due to the interpolation process, but we present a method to reduce this
complexity. This is followed by an explanation of the soft-decision list decoding of
Reed‚ÄìSolomon codes using the Kotter‚ÄìVardy algorithm [5].
We also describe how to modify the Guruswami‚ÄìSudan algorithm to list decode AG
codes and again show we can still reduce the complexity of the interpolation process.
The Kotter‚ÄìVardy algorithm is then extended for the soft-decision list decoding of AG
codes. The performance of hard- and soft-decision list decoding of Reed‚ÄìSolomon
codes and AG codes is evaluated, and simulation result are presented showing the
performance of both coding schemes on the AWGN and fading channels. The idea
of list decoding is as follows: given a received word R, reconstruct a list of all code
words with a distance œÑ to the received word R, in which œÑ can be greater than
œÑunique =  d‚àí1
2 .
This idea is illustrated in Figure 5.1, where c1, c2 and c3 are three independent code
words at a distance d from each other. A received word r1, which has distance less
than œÑ unique to code word c1, can be decoded by the unique decoding algorithm, which
results in c1. However, for received word r2, which has distance greater than œÑ unique
to any of the code words, the unique decoding algorithm will fail to decode it. But
Non-Binary Error Control Coding for Wireless Communication and Data Storage
Rolando Antonio Carrasco and Martin Johnston
C 2008 John Wiley & Sons, Ltd

148
LIST DECODING
d
‚àí
2
1
d
c1
c2
c3
r1
r2
Figure 5.1
The idea of list decoding.
using the list decoding algorithm, a list of possible transmitted code words will be
produced. For example, a decoded output list {c1, c2, c3} is produced by the decoder,
then the code word that has the minimal distance to r2 is chosen from the list and
decoding is completed.
5.2 List Decoding of Reed‚ÄìSolomon Codes Using
the Guruswami‚ÄìSudan Algorithm
5.2.1 Weighted Degrees and Lexicographic Orders
To apply the list decoding algorithm we must Ô¨Årst impose an ordering on the powers
of a set of monomials. If we deÔ¨Åne the weighted (u, v)-degree of a monomial xayb by:
degu,v(xayb) = ua + vb
(5.1)
then we can order a sequence of monomials by their weighted degrees. The lexico-
graphic order of the monomials is then deÔ¨Åned as:
xa1 yb1 < xa2 yb2
(5.2)
if degu,v(xa1 yb1) < degu,v(xa2 yb2) or degu,v(xa1 yb1) = degu,v(xa2 yb2) and a1 > a2.
This has already been used in Chapter 4 to order the monomial basis used to construct
algebraic‚Äìgeometric codes. The lexicographic order of the weighted (1, 1)-degree of
the sequence of monomials xayb is actually the Total Graduated Degree Order <T
deÔ¨Åned in (4.9).
To list decode a (n, k) Reed‚ÄìSolomon code we use the (1, k ‚àí1)-weighted degree.
For example, in order to decode a (7, 5) RS code deÔ¨Åned in GF(8), (1, 4)-lexicographic
order is used. The generation of this order is shown in Table 5.1. The entries in
Table 5.1a and 5.1b represent the (1, 4)-weighted degree and (1, 4)-lexicographic
order of monomials M with x degree a and y degree b, respectively. Applying (5.1)
with u = 1 and v = 4, we can generate the (1, 4)-weighted degree of monomials M
shown by Table 5.1a. For example, for monomial x2y, its (1, 4)-weighted degree is

LIST DECODING OF REED‚ÄìSOLOMON CODES
149
Table 5.1
(a) (1, 4)-weighted degree of monomial xayb; (b) (1, 4)-lexicographic order of monomial
xayb.
a/b
0
1
2
3
4
5
6
7
8
9
10
11
12
. . .
(a)
0
0
1
2
3
4
5
6
7
8
9
10
11
12
. . .
1
4
5
6
7
8
9
10
11
12
2
8
9
10
11
12
. . .
3
12
. . .
...
...
a/b
0
1
2
3
4
5
6
7
8
9
10
11
12
. . .
(b)
0
0
1
2
3
4
6
8
10
12
15
18
21
24
. . .
1
5
7
9
11
13
16
19
22
25
. . .
2
14
17
20
23
26
. . .
3
27
. . .
...
...
deg1,4(x2y) = 2 ¬∑ 1 + 4 = 6, which is shown in Table 5.1a. Based on Table 5.1a,
and applying the above lexicographic order rule, we can generate the (1, 4)-
lexicographic order of monomials M shown in Table 5.1b and denoted as ord(M). From
Table 5.1b, it can be observed that x4 < x2y < y2, since ord(x4) = 4, ord(x2y) = 9 and
ord(y2) = 14.
With the deÔ¨Ånition of the weighted degree and order of a monomial, we can deÔ¨Åne
the weighted degree of a nonzero bivariate polynomial in Fq[x, y] as the weighted
degree of its leading monomial, ML. Any nonzero bivariate polynomial Q(x, y) can be
written as:
Q(x, y) = Q0M0 + Q1M1 + ¬∑ ¬∑ ¬∑ + QL ML,
(5.3)
with bivariate monomial M = xayb ordered as M0 < M1 < ¬∑ ¬∑ ¬∑ < ML, Q0,
Q1, . . . , QL ‚ààGF(q) and QL = 0. The (1, k ‚àí1)-weighted degree of Q(x, y) can be
deÔ¨Åned as:
deg1,k‚àí1(Q(x, y)) = deg1,k‚àí1(ML)
(5.4)
L is called the leading order (lod) of polynomial Q(x, y), deÔ¨Åned as:
lod(Q(x, y)) = ord(ML) = L.
(5.5)
For example, given a polynomial Q(x, y) = 1 + x2 + x2y + y2, applying the above
(1, 4)-lexicographic order, it has leading monomial ML = y2. Therefore, deg1,4(Q(x,
y)) = deg1,4(y2) = 8 and lod(Q(x, y)) = ord(y2) = 14. Consequently, any two nonzero

150
LIST DECODING
polynomials Q and H (Q, H ‚ààFq[x, y]) can be compared with respect to their leading
order:
Q ‚â§H, if lod(Q) ‚â§lod(H)
(5.6)
Sx(T) and Sy(T) are denoted as the highest degree of x and y respectively under the
(1, k ‚àí1)-lexicographic order such that:
Sx(T ) = max{a : ord(xay0) ‚â§T }
(5.7)
Sy(T ) = max{b : ord(x0yb) ‚â§T },
(5.8)
where T is any nonnegative integer. The error-correction capability, œÑ m, and the
maximum number of candidate messages, lm, in the output list with respect to a
certain multiplicity, m, of the GS algorithm can be stated as [4]:
œÑm = n ‚àí1 ‚àí
& Sx(C)
m
'
(5.9)
lm = Sy(C),
(5.10)
in which T in (5.7) and (5.8) is replaced by the nonnegative integer C, deÔ¨Åned as:
C = n
m + 1
2

= n
(m + 1)!
(m + 1 ‚àí2)!2! = n(m + 1)m
2
(5.11)
C represents the number of iterations in the interpolation process, or alternatively
the maximum number of Hasse derivative evaluations of an individual interpolated
polynomial. These parameters will be proven in Section 5.2.5, when the factorization
theorem is presented. œÑ m and lm grow monotonically with multiplicity m [4]:
œÑm1 ‚â§œÑm2
(5.12)
lm1 < lm2,
(5.13)
if m1 < m2.
5.2.2 Interpolation Theorem
From an algebraic geometric point of view, the interpolation process in the GS algo-
rithm requires a hard decision to be made on the received word (r0, r1, . . . , rn‚àí1) and
the set of afÔ¨Åne points corresponding to the location of each symbol in the received
word. For Reed‚ÄìSolomon codes, the afÔ¨Åne points are simply the elements of the Ô¨Ånite
Ô¨Åeld over which it is deÔ¨Åned. Each afÔ¨Åne point and received symbol is paired up
to form a set of interpolation points (xi, ri), where i = 0, 1, . . . , n ‚àí1. The idea
of interpolation is to generate a polynomial Q(x, y) that intersects all these points a
given number of times. The number of times Q(x, y) intersects each point is called its
multiplicity, denoted as m, and this parameter determines the error-correcting capa-
bility of the GS algorithm. The interpolation polynomial Q(x, y) can be expressed

LIST DECODING OF REED‚ÄìSOLOMON CODES
151
as [4, 6]:
Q(x, y) =

a,b
Qabxayb,
(5.14)
where Qab ‚ààGF(q).
For Q(x, y) to be satisÔ¨Åed at the point (xi, ri), Q(xi, ri) = 0. We can write xa and yb
as:
xa = (x ‚àíxi + xi)a =

u‚â§a
a
u

xa‚àíu
i
(x ‚àíxi)u
and
yb = (y ‚àíri + ri)b
=

v‚â§b
b
v

rb‚àív
i
(y ‚àíri)v
and therefore:
Q(x, y) =

a,b
Qab

u‚â§a,v‚â§b

a
u

xa‚àíu
i
(x ‚àíxi)u

b
v

rv
i (y ‚àíri)v
=

u,v
Quv (x ‚àíxi)u (y ‚àíri)v
,
(5.15)
where:
Q(xi,ri)
uv
=

a‚â•u,b‚â•v

a
u
 
b
v

xa‚àíu
i
rb‚àív
i
.
(5.16)
(5.16) is the (u, v)-Hasse Derivative evaluation at the point (xi, ri) of Q(x, y)
and deÔ¨Ånes the constraints on the coefÔ¨Åcients of Q(x, y) in order to have a zero of
multiplicity m at that point.
Example 5.1: Given the polynomial Q(x, y) = Œ±5 + Œ±5x + y + xy deÔ¨Åned over
GF(8), we can use (5.16) to show that Q(x, y) has a zero of multiplicity of at least
m = 2 at the point (1, Œ±5). In order to have a zero of at least multiplicity m = 2, the
(u, v)-Hasse Derivative at this point must be zero for all u + v < m, that is (u, v)
can be (0, 0), (1, 0) and (0, 1).
Q(1,Œ±5)
00
= Q00
0
0
 0
0

10‚àí0(Œ±5)0‚àí0 + Q10
1
0
 0
0

11‚àí0(Œ±5)0‚àí0
+ Q01
0
0
 1
0

10‚àí0(Œ±5)1‚àí0 + Q11
1
0
 1
0

11‚àí0(Œ±5)1‚àí0
= 1 + 1 + Œ±5 + Œ±5 = 0
Q(1,Œ±5)
10
= Q10
1
1
 0
0

11‚àí1(Œ±5)0‚àí0 + Q11
1
1
 1
0

11‚àí1(Œ±5)1‚àí0 = Œ±5 + Œ±5 = 0
Q(1,Œ±5)
01
= Q01

0
0
 
1
1

10‚àí0(Œ±5)1‚àí1 + Q11

1
0
 
1
1

11‚àí0(Œ±5)1‚àí1 = 1 + 1 = 0.

152
LIST DECODING
Therefore,since Q(1,Œ±5)
00
= Q(1,Œ±5)
10
= Q(1,Œ±5)
01
= 0,Q(x,y)musthave a multiplicity
of at least m = 2.
We denote the Hasse derivative operator by Duv where [7]:
Duv Q(xi,ri) =

a‚â•u,b‚â•v
Qab
a
u
 b
v

xa‚àíu
i
rb‚àív
i
.
(5.17)
Therefore, the interpolation of the GS algorithm can be generalized as: Find a
minimal (1, k ‚àí1)-weighted degree polynomial Q(x, y) that satisÔ¨Åes:
Q(x, y) = min
lod(Q) {Q(x, y)|Duv Q(xi,ri) = 0
for
i = 0, 1, . . . , n
and
u + v < m} .
(5.18)
5.2.3 Iterative Polynomial Construction
To Ô¨Ånd the interpolated polynomial of (5.18), an iterative polynomial construction
algorithm [4, 8‚Äì12] is employed. In this algorithm a group of polynomials is ini-
tialized, tested by applying the Hasse derivative (5.16) and modiÔ¨Åed interactively.
The interactive modiÔ¨Åcation between two polynomials is based on the following two
properties of the Hasse derivative [7, 10]:
Property 1: Linear functional of Hasse derivative
If H, Q ‚ààFq[x, y], Œ¥1 and Œ¥2 ‚àà
GF(q), then:
D(Œ¥1H + Œ¥2Q) = Œ¥1D(H) + Œ¥2D(Q).
(5.19)
Property 2: Bilinear Hasse derivative
If H, Q ‚ààFq[x, y], then:
[H, Q]D = HD(Q) ‚àíQD(H).
(5.20)
If the Hasse derivative evaluation of D(Q) = Œ¥1 and of D(H) = Œ¥2 (d1, d2 = 0),
based on Property 1, it is straightforward to prove that the Hasse derivative evaluation
of (5.20) is zero, as follows:
D ([H, Q]D) = D (H D (Q) ‚àíQD (H)) = D (Œ¥1H ‚àíŒ¥2Q) .
Using Property 1:
D (Œ¥1H ‚àíŒ¥2Q) = Œ¥1D(H) ‚àíŒ¥2D(Q) = Œ¥1Œ¥2 ‚àíŒ¥2Œ¥1 = 0.

LIST DECODING OF REED‚ÄìSOLOMON CODES
153
Therefore:
D ([H, Q]D) = 0.
(5.21)
If lod(H) > lod(Q), the new constructed polynomial from (5.20) has leading order
lod(H). Therefore, by performing the bilinear Hasse derivative over two polynomials
with nonzero Hasse derivatives, we can reconstruct a polynomial which has a Hasse
derivative of zero. Based on this principle, the implementation of an algorithm for
interpolation will iteratively modify a set of polynomials through all n points and with
every possible (u, v) pair under each point.
With multiplicity m, there are
m + 1
2

pairs of (u, v), which are arranged as:
(u, v) = (0, 0), (0, 1), . . . , (0, m ‚àí1), (1, 0), (1, 1), . . . , (1, m ‚àí2), . . . , (m ‚àí1, 0).
Therefore, when decoding a (n, k) Reed‚ÄìSolomon code with multiplicity m, there are
C = n

m + 1
2

iterations required to construct a polynomial deÔ¨Åned by (5.18).
At the start of the algorithm, a group of polynomials is initialized as:
G0 = {Q0, j = y j
, j = 0, 1, . . . ,lm},
(5.22)
where lm is the maximum number of messages in the output list deÔ¨Åned by (5.10). If
ML denotes the leading monomial of polynomial Q, it is important to point out that:
Q0, j = min{Q(x, y) ‚ààFq[x, y]| deg y(ML) = j}.
(5.23)
Let ik denotes the iteration index of the algorithm, where ik = i

m + 1
2

+ r, i =
0, 1, . . . , n ‚àí1 and r = 0, 1, . . . ,
m + 1
2

‚àí1. For iteration ik of the algorithm,
each polynomial Qik, j in group Gik is tested by (5.17), and the value of each after
Hasse derivative evaluation is denoted by j as:
 j = Dik(Qik, j).
(5.24)
Those polynomials with j = 0 do not need to be modiÔ¨Åed. However, those
polynomials with j = 0 need to be modiÔ¨Åed based on (5.20). In order to construct a
group of polynomials which satisfy:
Qik+1, j = min

Q ‚ààFq[x, y]|Dik(Qik+1, j) = 0, Dik‚àí1(Qik+1, j) = 0, . . . ,
D0(Qik+1, j) = 0, and deg y(ML) = j

,
(5.25)

154
LIST DECODING
the minimal polynomial among those polynomials with j = 0 is chosen. Denote the
index of the minimal polynomial as j and record it as Q:
j = index(min{Qik, j| j = 0})
(5.26)
Q = Qik, j.
(5.27)
For the remaining polynomials with j = 0 but j = j, (5.20) is used to modify
them without the leading order being increased:
Qik+1, j = [Qik,, Q]Dik =  j Qik, j ‚àíQ.
(5.28)
Based on (5.21), we know that Dik(Qik+1, j) = 0. As lod(Qik, j) > lod(Q), it follows
that lod(Qik+1, j) = lod(Qik, j).
Q is modiÔ¨Åed by (5.20) with the leading order increasing:
Qik+1, j = [x Q, Q]Dik = (x ‚àíxi)Q,
(5.29)
where xi is the x-coordinate of current interpolating point (xi, ri).  j‚àó=
Dik(Q) = 0 and so, as Dik(x Q) = 0, Dik(Qik+1, j) = 0. As lod(xQ) > lod(Q),
lod(Qik+1, j) = lod(x Q) > lod(Qik, j). Therefore whenever (5.29) is performed, we
have: lod(Qik+1, j) > lod(Qik, j).
After C iterative modiÔ¨Åcations, the minimal polynomial in group Gc is the inter-
polated polynomial that satisÔ¨Åes (5.18), and it is chosen to be factorized in the next
step:
Q(x, y) = min{QC, j|QC, j ‚ààGC}.
(5.30)
5.2.4 Complexity Reduced ModiÔ¨Åcation
Based on the above analysis, it can be observed that when decoding a (n, k)
Reed‚ÄìSolomon code with multiplicity m, lm + 1 bivariate polynomials are being
interactively modiÔ¨Åed over C iterative steps in which Hasse derivative evaluation
(5.17) and bilinear Hasse derivative modiÔ¨Åcation (5.20) are being performed. This
process has a complexity of approximately O(n2m4) [4] and is responsible for the
GS algorithm‚Äôs high decoding complexity. Therefore, reducing the complexity of
interpolation is essential to improving the algorithm‚Äôs efÔ¨Åciency.
The leading order of the polynomial group Gik is deÔ¨Åned as the minimal leading
order (lod) among the group‚Äôs polynomials [13]:
lod(Gik) = min{lod(Qik, j)|Qik, j ‚ààGik}.
(5.31)
Based on the initialization deÔ¨Åned in (5.22), the leading order of polynomial group
G0 is lod(G0) = lod(Q0,0) = 0. In the ik modiÔ¨Åcation, if no polynomial needs to be
modiÔ¨Åed then the polynomial group is unchanged; lod(Gik+1) = lod(Gik). When a
polynomial needs to be modiÔ¨Åed, (5.29) must be used. If ML is the leading monomial

LIST DECODING OF REED‚ÄìSOLOMON CODES
155
of Q*, we have [13]:
lod(x Q‚àó) = lod(Q‚àó) +
&degx Q‚àó
k ‚àí1
'
+ degy(ML) + 1
(5.32)
when k is the dimension of the code. Based on (5.32), it can be seen that lod(Gik)
will be increased if Q* is the minimal polynomial in the group Gik. The leading order
increase guarantees that in the ik iterative step, the leading order of the polynomials
group Gik is always less than or equal to ik:
lod(Gik) ‚â§ik.
(5.33)
Based on (5.33), after C iterative steps we have:
lod(GC) ‚â§C.
(5.34)
From (5.30) we know that only the minimal polynomial is chosen from the polyno-
mial group GC as Q(x, y) = {Qc, j|Qc, j ‚ààGc and lod(Qc, j) = lod(Gc)}, therefore:
lod(Q(x, y)) ‚â§C,
(5.35)
which means the interpolated polynomial Q(x, y) has leading order less than or equal
to C. Those polynomials with leading order over C will not be candidates for Q(x, y).
Therefore, during the iterative process, we can modify the group of polynomials by
eliminating those with leading order greater than C, as [13]:
Gik = {Qik, j|lod(Qik, j) = C}.
(5.36)
We now prove this modiÔ¨Åcation will not affect the Ô¨Ånal result. In iteration ik, if
there is a polynomial Qik, j with lod(Qik, j) > C, it may be modiÔ¨Åed by either (5.28) or
(5.29), which will result in its leading order being unchanged or increased. Therefore,
at the end lod(Qc,j) > C, and based on (5.35) it cannot be Q(x, y). However, if Qik, j is the
minimalpolynomialdeÔ¨Ånedby(5.27),thisimplies thatthosepolynomialswithleading
order less than C do not need to be modiÔ¨Åed. If Qik, j is not the minimal polynomial
deÔ¨Åned by (5.27), Qik, j will not be chosen to perform bilinear Hasse derivative (5.28)
with other polynomials. Therefore, Q(x, y) has no information introduced from Qik, j,
since lod(Qik, j) > C. As a result, eliminating the polynomials with leading order
greater than C will not affect the Ô¨Ånal outcome.
This complexity modiÔ¨Åcation scheme can be generally applied to the iterative
interpolation process, for example to soft-decision list decoding of Reed‚ÄìSolomon
codes and hard/soft-decision list decoding of Hermitian codes. Based on the total
number of iterations C for interpolation, the interpolated polynomial‚Äôs leading order
always satisÔ¨Åes lod(Q(x, y)) ‚â§C. It implies that those polynomials in the group G
can be eliminated once their leading order is greater than C.

156
LIST DECODING
This modiÔ¨Åcation can reduce some unnecessary computation in terms of avoiding
Hasse derivative evaluation (5.24) and bilinear Hasse derivative modiÔ¨Åcation (5.28)
and (5.29) of polynomials with leading order over C. Based on the above analysis,
the modiÔ¨Åed interpolation process can be summarized as:
Algorithm 5.1: Interpolation for list decoding a (n, k) Reed‚ÄìSolomon code
[13, 14]
1. Initialize a group of polynomials by (5.22), and set the index of the interpolated
point i = 0.
2. Set interpolation point to (xi, ri).
3. For each (u, v) where u + v < m.
{
4. Modify the polynomial group by (5.36).
5. Perform Hasse derivative evaluation (5.24) for each polynomial in the group.
6. If all the polynomials‚Äô Hasse derivative evaluations are zero, choose another pair
(u, v) and go to step 3.
7. Find the minimal polynomial deÔ¨Åned by (5.26) and (5.27).
8. For the minimal polynomial, modify it by (5.29). For the other polynomials with
nonzero Hasse derivative evaluation, modify them by (5.28).
}
9. i = i + 1.
10. If i = n, stop the process and choose Q(x, y) deÔ¨Åned by (5.30). Else go to step 2.
Example 5.2 shows the modiÔ¨Åed interpolation process.
Example 5.2: Decoding the (7, 2) Reed‚ÄìSolomon code deÔ¨Åned over GF(8) with
multiplicity m = 2 As C = 7( 3
1) = 21, based on (5.9) and (5.10) we have œÑ 2 = 3
and l2 = 5. The transmitted code word is generated by evaluating the message
polynomial f(x) = Œ± + Œ±6x over the set of points x = (1, Œ±, Œ±3, Œ±2, Œ±6, Œ±4, Œ±5),
and the corresponding received word is R = (Œ±5, Œ±3, Œ±4, 0, Œ±6, Œ±2, Œ±2), where Œ±
is a primitive element in GF(8) satisfying Œ±3 + Œ± + 1 = 0. Construct a bivariate
polynomial that has a zero of multiplicity m = 2 over the n points (xi,ri)|n‚àí1
i=0 .
At the beginning, six polynomials are initialized as:
Q0,0 = 1, Q0,1 = y, Q0,2 = y2, Q0,3 = y3, Q0,4 = y4 and Q0,5 = y5. Their leading
orders are lod(Q0,0) = 0, lod(Q0,1) = 2, lod(Q0,2) = 5, lod(Q0,3) = 9, lod(Q0,4) =
14 and lod(Q0,5) = 20 respectively. lod(G0) = lod(Q0,0) = 0.
When i = 0 and (u, v) = (0, 0), ik = 0. No polynomial is eliminated from the
group G0.

LIST DECODING OF REED‚ÄìSOLOMON CODES
157
Perform Hasse derivative evaluation for each of the polynomials in G0 as:
0 = D(1,Œ±5)
(0,0) Q0,0 = 1, 1 = D(1,Œ±5)
(0,0) Q0,1 = Œ±5
2 = D(1,Œ±5)
(0,0) Q0,2 = Œ±3, 3 = D(1,Œ±5)
(0,0) Q0,3 = Œ±
4 = D(1,Œ±5)
(0,0) Q0,4 = Œ±7, 5 = D(1,Œ±5)
(0,0) Q0,5 = Œ±.
Find the minimal polynomial with j = 0 as:
j = 0
and
Q = Q0,0.
Modify polynomials in G0 with j = 0 as:
Q1,0 = 0(x ‚àíx0)Q = 1 + x, and lod(Q1,0) = 1
Q1,1 = 0Q0,1 ‚àí1Q = Œ±5 + y and lod(Q1,1) = 2
Q1,2 = 0Q0,2 ‚àí2Q = Œ±3 + y2 and lod(Q1,2) = 5
Q1,3 = 0Q0,3 ‚àí3Q = Œ± + y3 and lod(Q1,3) = 9
Q1,4 = 0Q0,4 ‚àí4Q = Œ±6 + y4 and lod(Q1,4) = 14
Q1,5 = 0Q0,5 ‚àí5Q = Œ±4 + y5 and lod(Q1,5) = 20
lod(G1) = lod(Q1,0) = 1.
When i = 0 and (u, v) = (0, 1), ik = 1. No polynomial is eliminated from the
group G1.
Perform Hasse derivative evaluation for each of the polynomial in G1 as:
0 = D(1,Œ±5)
(0,1) (Q1,0) = 0. 1 = D(1,Œ±5)
(0,1) (Q1,1) = 1
2 = D(1,Œ±5)
(0,1) (Q1,2) = 0. 3 = D(1,Œ±5)
(0,1) (Q1,3) = Œ±3
4 = D(1,Œ±5)
(0,1) (Q1,4) = 0. 5 = D(1,Œ±5)
(0,1) (Q1,5) = Œ±6.
Find the minimal polynomial with j = 0 as:
j = 1
and
Q = Q1,1.
As 0 = 2 = 4 = 0:
Q2,0 = Q1,0 = 1 + x, and lod(Q2,0) = 1
Q2,2 = Q1,2 = œÉ 3 + y2, and lod(Q2,2) = 5
Q2,4 = Q1,4 = œÉ 6 + y4, and lod(Q2,4) = 14.

158
LIST DECODING
Modify polynomials in G1 with j = 0 as:
Q2,1 = 1(x ‚àíx0)Q = Œ±5 + Œ±5x + y(1 + x), lod(Q2,1) = 4
Q2,3 = 1Q1,3 ‚àí3Q = Œ±3y + y3, lod(Q2,3) = 9
Q2,5 = 1Q2,5 ‚àí5Q = Œ±6y + y5, lod(Q2,5) = 20
lod(G2) = lod(Q2,0) = 1.
When i = 0 and (u, v) = (1, 0), ik = 2. No polynomial is eliminated from the
group G2.
Perform Hasse derivative evaluation for each of the polynomial in G2 as:
0 = D(1,Œ±5)
(1,0) (Q2,0) = 1. 1 = D(1,Œ±5)
(1,0) (Q2,1) = 0
2 = D(1,Œ±5)
(1,0) (Q2,2) = 0. 3 = D(1,Œ±5)
(1,0) (Q2,3) = 0
4 = D(1,Œ±5)
(1,0) (Q2,4) = 0. 5 = D(1,Œ±5)
(1,0) (Q2,5) = 0.
Find the minimal polynomial with j = 0 as:
j = 0 and Q = Q2,0.
As 1 = 2 = 3 = 4 = 5 = 0:
Q3,1 = Q2,1 = Œ±5 + Œ±5x + y(1 + x), lod(Q3,1) = 4
Q3,2 = Q2,2 = Œ±3 + y2, and lod(Q3,2) = 5
Q3,3 = Q2,3 = Œ±3y + y3, lod(Q3,3) = 9
Q3,4 = Q2,4 = Œ±6 + y4, and lod(Q3,4) = 14
Q3,5 = Q2,5 = Œ±6y + y5, lod(Q3,5) = 20.
Modify polynomials in G2 with j = 0 as:
Q3,0 = 0(x ‚àíx0)Q = 1 + x2, lod(Q3,0) = 3
lod(G3) = lod(Q3,0) = 3.
Based on the same process, interpolation is run through all the rest of the points
(xi, ri) (i = 1 to 6). In order to illustrate the complexity reducing modiÔ¨Åcation
scheme, Table 5.2 shows the whole iterative process with respect to the polynomi-
als‚Äô leading order.
From Table 5.2 we can see that the modiÔ¨Åed algorithm starts to take action at
ik = 10 when there is at least one polynomial with leading order over 21 and
eliminating such polynomials will not affect the Ô¨Ånal outcome. At the end, both
the original and the modiÔ¨Åed GS algorithm produce the same result: Q(x, y) =
min{G21} = Q21,2 = 1 + Œ±4x2 + Œ±2x4 + y2(Œ±5 + Œ±4x2). From this example we
can see that more computation can be reduced if the modiÔ¨Åed algorithm starts to
take action at earlier steps.

LIST DECODING OF REED‚ÄìSOLOMON CODES
159
Table 5.2
Iterative process of Example 5.2.
i (ik)
0 (0)
0 (1)
0 (2)
1 (3)
1 (4)
1 (5)
2 (6)
2 (7)
2 (8)
3 (9)
lod(Qik,0) 0
1
1
3
6
6
10
15
15
21
lod(Qik,1) 2
2
4
4
4
7
7
7
11
11
lod(Qik,2) 5
5
5
5
5
5
5
5
5
5
lod(Qik,3) 9
9
9
9
9
9
9
9
9
9
lod(Qik,4) 14
14
14
14
14
14
14
14
14
14
lod(Qik,5) 20
20
20
20
20
20
20
20
20
20
lod(Gik)
0
1
1
3
4
5
5
5
5
5
Original GS
i (ik)
3 (10) 3 (11) 4 (12) 4 (13) 4 (14) 5 (15) 5 (16) 5 (17) 6 (18) 6 (19) 6 (20) 7 (21)
lod(Qik,0) 28
28
36
45
45
55
55
55
55
66
66
78
lod(Qik,1) 11
16
16
16
22
22
22
22
22
22
29
29
lod(Qik,2) 5
5
5
5
5
5
8
8
12
12
12
12
lod(Qik,3) 9
9
9
9
9
9
9
13
13
13
13
13
lod(Qik,4) 14
14
14
14
14
14
14
14
14
14
14
14
lod(Qik,5) 20
20
20
20
20
20
20
20
20
20
20
20
lod(Gik)
5
5
5
5
5
5
8
8
12
12
12
12
ModiÔ¨Åed GS
i (ik)
3 (10) 3 (11) 4 (12) 4 (13) 4 (14) 5 (15) 5 (16) 5 (17) 6 (18) 6 (19) 6 (20) 7 (21)
lod(Qik,0) ‚Äî
‚Äî
‚Äî
‚Äî
‚Äî
‚Äî
‚Äî
‚Äî
‚Äî
‚Äî
‚Äî
‚Äî
lod(Qik,1) 11
16
16
16
‚Äî
‚Äî
‚Äî
‚Äî
‚Äî
‚Äî
‚Äî
‚Äî
lod(Qik,2) 5
5
5
5
5
5
8
8
12
12
12
12
lod(Qik,3) 9
9
9
9
9
9
9
13
13
13
13
13
lod(Qik,4) 14
14
14
14
14
14
14
14
14
14
14
14
lod(Qik,5) 20
20
20
20
20
20
20
20
20
20
20
20
lod(Gik)
5
5
5
5
5
5
8
8
12
12
12
12
Note: ‚Äî means the corresponding polynomial is eliminated;
means the corresponding
polynomial is chosen as Q(x, y).
5.2.5 Factorization
In this section, the factorization theorem is explained, followed by a detailed descrip-
tion of an efÔ¨Åcient algorithm known as Roth‚ÄìRuckenstein‚Äôs algorithm [15].
As mentioned in Section 5.2.2, given the interpolated polynomial Q(x, y), the
transmitted message polynomial f(x) can be found by determining Q(x, y)‚Äôs y roots.
Lemma 5.1
If Q(x, y) has a zero of multiplicity at least m over (xi, ri) and p(x) is a
polynomial in Fq[xk‚àí1] that p(xi) = ri, then (x ‚àíxi)m|Q(x, p(x)) [6].
DeÔ¨Åne (p, R) as the number of symbols in received word R that satisfy p(xi) = ri
as:
(p, R) = |{i : p(xi) = ri, i = 0, 1, . . . , n ‚àí1}| .
(5.37)

160
LIST DECODING
Lemma 5.2
p(x) is a polynomial in Fq[xk‚àí1] and p(xi) = ri for at least (p, R)
values. If m (p, R) > deg1,k‚àí1(Q(x, y)) then y ‚àíp(x)|Q(x, y), or Q(x, p(x)) =
0 [6].
Based on Lemma 5.1, if p(xi) = ri then (x ‚àíxi)m|Q(x, y). If S is the set of i
that satisÔ¨Åes p(xi) = ri. as |S| = (p, R) then  
i‚ààS
(x ‚àíxi)m|Q(x, p(x)). Assume
g1(x) =  
i‚ààS
(x ‚àíxi)m and g2(x) = Q(x, p(x)); therefore g1(x)|g2(x). It is obvious
that g1(x) has x-degree m (p, R) and g2(x) has x-degree equal to deg1,k‚àí1Q(x, y).
If m (p, R) > deg1,k‚àí1 Q(x, y) and g1(x)|g2(x), the only solution for these two
preconditions is g2(x) = 0. Therefore, if m (p, R) > deg1,k‚àí1(Q(x, y)), Q(x, p(x)) =
0, or equivalently, y ‚àíp(x)|Q(x, y).
As Q(x, y) is the interpolated polynomial from the last step, according to (5.35),
lod(Q) ‚â§C. Based on (5.9), deg1,k‚àí1(Q(x, y)) ‚â§Sx(C). If m (f, R) ‚â•Sx(C) then
m ( f, R) ‚â•deg1,k‚àí1(Q(x, y)). Based on Lemma 5.2, if ( f, R) ‚â•1 +
	 Sx(C)
m

then
the transmitted message polynomial f(x) can be found by factorizing Q(x, y). As (f,
R) represents the number of points that satisfy ri = f(xi) = ci, those points that do not
satisfy this equation are where the errors are located.
Therefore, the error-correction capability of the GS algorithm is œÑm = n ‚àí
	 Sx(C)
m

‚àí1, which is deÔ¨Åned by (5.10). Under (1, k ‚àí1)-lexicographic order, x0yj is
the maximal monomial with weighted degree (k ‚àí1)j. In polynomial Q(x, y) there
should not be any monomials with y-degree over Sy(C), otherwise lod(Q) > C. As a
result, max{degyQ(x, y)} ‚â§Sy(C). As the factorization output list contains the y-roots
of Q(x, y), and the number of y-roots of Q(x, y) should not exceed its y-degree, the
maximal number of candidate messages in the output list is lm = Sy(C), which is
deÔ¨Åned by (5.11).
5.2.6 Recursive CoefÔ¨Åcient Search
To Ô¨Ånd the y-roots of the interpolated polynomial Q(x, y), Roth and Ruckenstein [15]
introduced an efÔ¨Åcient algorithm for factorizing these bivariate polynomials.
In general, the factorization output p(x) ‚ààFq[xk‚àí1] can be expressed in the form of:
p(x) = p0 + p1x + ¬∑ ¬∑ ¬∑ + pk‚àí1xk‚àí1,
(5.38)
where p0, p1, . . . , pk‚àí1 ‚ààGF(q). In order to Ô¨Ånd the polynomials p(x), we must deter-
mine their coefÔ¨Åcients p0, p1, . . ., pk‚àí1, respectively. The idea of Roth‚ÄìRuckenstein‚Äôs
algorithm is to recursively deduce p0, p1, . . ., pk‚àí1 one at a time.
For any bivariate polynomial, if h is the highest degree such that xh|Q(x, y), we
can deÔ¨Åne [15]:
Q‚àó(x, y) = Q(x, y)
xh
.
(5.39)

LIST DECODING OF REED‚ÄìSOLOMON CODES
161
If we denote p0 = p(x) and Q0(x, y) = Q*(x, y), where Q(x, y) is the new interpolated
polynomial (5.30), we can deÔ¨Åne the recursive updated polynomials ps(x) and Qs(x,
y), where s ‚â•1, as [15]:
ps(x) = ps‚àí1(x) ‚àíps‚àí1(0)
x
= ps + ¬∑ ¬∑ ¬∑ + pk‚àí1xk‚àí1‚àís,
(s = k ‚àí1). (5.40)
Qs(x, y) = Q‚àó
s‚àí1(x, xy + ps‚àí1).
(5.41)
Lemma 5.3
With ps(x) and Qs(x, y) deÔ¨Åned by (5.40) and (5.41), when s ‚â•1,
(y ‚àíp(x))|Q(x, y) if and only if (y ‚àíps(x))|Qs(x, y) [4].
This means that if polynomial ps(x) is a y-root of Qs(x, y), we can trace back to
Ô¨Ånd the coefÔ¨Åcients ps‚àí1, . . . , p1, p0 to reconstruct the polynomial p(x), which is the
y-root of polynomial Q(x, y).
The Ô¨Årst coefÔ¨Åcient p0 can be determined by Ô¨Ånding the roots of Q0(0, y) = 0. If we
assume that Q(x, p(x)) = 0, p0(x) should satisfy Q0(x, p0(x)) = 0. When x = 0, Q0(0,
p0(0)) = 0. According to (5.38), p0(0) = p0, therefore p0 is the root of Q0(0, y) =
0. By Ô¨Ånding the roots of Q0(0, y) = 0, a number of different p0 can be determined.
For each p0, we can deduce further to Ô¨Ånd the rest of ps (s = 1, . . . , k ‚àí1) based on
the recursive transformation (5.40) and (5.41).
Assume that after s ‚àí1 deductions, polynomial ps‚àí1(x) is the y-root of Qs‚àí1(x,
y). Based on (5.40), ps‚àí1(0) = ps‚àí1 and a number of ps‚àí1 can be determined by
Ô¨Ånding the roots of Qs‚àí1(0, y) = 0. For each ps‚àí1, we can Ô¨Ånd ps. As Qs‚àí1(x,
ps‚àí1(x)) = 0, (y ‚àíps‚àí1(x))|Qs‚àí1(x, y). If we deÔ¨Åne y = xy + ps‚àí1, then (xy +
ps‚àí1 ‚àíps‚àí1(x))|Qs‚àí1(x, xy + ps‚àí1). Based on (5.40), xy + ps‚àí1 ‚àíps‚àí1(x) = xy ‚àí
xps(x). As Qs(x, y) = Q‚àó
s‚àí1(x, xy + ps‚àí1), (xy ‚àíxps(x))|Qs‚àí1(x, xy + ps‚àí1) and
(y ‚àíps(x))|Qs(x, y). Therefore, ps can again be determined by Ô¨Ånding the roots of
Qs(0, y) = 0. This root-Ô¨Ånding algorithm can be explained as a tree-growing process,
as shown in Figure 5.2. There can be an exponential number of routes for choosing
coefÔ¨Åcients ps (s = 0, 1, . . . , k ‚àí1) to construct p(x). However, the intended p(x) should
satisfy deg(p(x)) < k and (y ‚àíp(x))|Q(x, y). Based on (5.40), when s = k, pk(x) =
0. Therefore if Qk(x, 0) = 0, or equivalently Qk(x, pk(x)) = 0, (y ‚àípk(x))|Qk(x, y).
According to Lemma 5.3, (y ‚àíp(x))|Q(x, y) and p(x) is found.
Based on the above analysis, the factorization process can be summarized as [4, 15]:
Algorithm 5.2: Factorization of list decoding a (n, k) Reed‚ÄìSolomon code [15]
1. Initialize Q0(x, y) = Q‚àó(x, y), s = 0.
2. Find roots ps of Qs(0, y) = 0.
3. For each ps, perform Q transformation (5.41) to calculate Qs+1(x, y).
4. s = s + 1.
5. If s < k, go to (ii). If s = k and Qs(x, 0) = 0, stop this deduction route. If s = k and
Qs(x, 0) = 0, trace the deduction route to Ô¨Ånd ps‚àí1, . . . , p1, p0.
Example 5.3 demonstrates Roth‚ÄìRuckenstein‚Äôs algorithm.

162
LIST DECODING
Q(x, y)
p0
p0
p1......p1
p1
.
.
.
.
.
.p1
p2...p2
p2...p2
p2...p2
p2...p2
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.. 
‚Ä¶‚Ä¶‚Ä¶‚Ä¶
Figure 5.2
CoefÔ¨Åcients deduction in Roth‚ÄìRuckenstein‚Äôs algorithm.
Example 5.3: Based on polynomial Q(x, y) = 1 + Œ±4x2 + Œ±2x4 + y2(Œ±5 + Œ±4x2),
which is the interpolation result of Example 5.2, determine the factorization output
list L using Roth‚ÄìRuckenstein‚Äôs algorithm.
Initialize Q0(x, y) = Q*(x, y) = 1 + Œ±4x2 + Œ±2x4 + y2(Œ±5 + Œ±4x2) and s = 0.
Q0(0, y) = 1 + Œ±5y2 and p0 = Œ± is the root of Q0(0, y) = 0.
For p0 = Œ±, generate Q1(x, y) = Q0(x, xy + p0):
Q0(x, xy + Œ±) = 1 + Œ±4x2 + Œ±2x4 + (xy + Œ±)2(Œ±5 + Œ±4x2)
= 1 + Œ±4x2 + Œ±2x4 + (x2y2 + Œ±2)(Œ±5 + Œ±4x2)
= 1 + Œ±4x2 + Œ±2x4 + Œ±5x2y2 + Œ±4x4y2 + 1 + Œ±6x2
= Œ±3x2 + Œ±2x4 + y2(Œ±5x2 + Œ±4x4).
Now, from (5.39), we can see that x2 is the highest power of x that divides Q0(x,
xy + Œ±), so:
Q‚àó
0(x, xy + Œ±) = Q0(x, xy + Œ±)
xh
= Q0(x, xy + Œ±)
x2
= Œ±3 + Œ±2x2 + y2(Œ±5 + Œ±4x2).
s = s + 1 = 1. As s < k, go to step 2 of Algorithm 5.2.
Q1(0, y) = Œ±3 + Œ±5y2 and p1 = Œ±6 is a root of Q1(0, y) = 0.
For p1 = Œ±6, generate Q2(x, y) = Q‚àó
1(x, xy + p1) = y2(Œ±5 + Œ±4x2). s =
s + 1 = 2. As s = k and Q2(x, 0) = 0, trace this route to Ô¨Ånd its output p0 =
Œ± and p1 = Œ±6.
As a result, factorization output list L = {p(x) = Œ± + Œ±6x}. From Example 5.2,
p(x) matches the transmitted message polynomial f(x).

LIST DECODING OF REED‚ÄìSOLOMON CODES
163
Example 5.4: Factorizing an interpolation polynomial containing two
messages Using the same (7, 2) Reed‚ÄìSolomon code as in Example 5.3, assume
that the interpolated polynomial is Q(x, y) = Œ±x + Œ±6x2 + y(Œ±3 + Œ±3x) + Œ±2y2.
For s = 0, Q0(x, y) = Q‚àó(x, y) = Œ±x + Œ±6x2 + y(Œ±3 + Œ±3x) + Œ±2y2.
Setting x = 0, Q0(0, y) = Œ±3y + Œ±2y2 and has two roots, p0 = 0 and Œ±.
Taking p0 = 0:
Q1(x, y) = Q‚àó
0(x, xy + 0) = Œ± + Œ±6x + y(Œ±3 + Œ±3x) + Œ±2xy2.
s = s + 1 = 1. As s < k = 2 we go back to step 2 of Algorithm 5.2.
For s = 1, Q1(0, y) = Œ± + Œ±3y, which has one root, p1 = Œ±5.
Taking p1 = Œ±5:
Q2(x, y) = Q‚àó
1(x, xy + Œ±5) = y(Œ±3 + Œ±3x) + Œ±2x2y2
s = s + 1 = 2. Now s = k and Q2(x, 0), so this route is terminated and the
message is p(x) = 0 + Œ±5x = Œ±5x.
However, we must now determine whether there is another message contained
within the interpolation polynomial and so we now take the second root of
Q0(0, y).
Taking p0 = Œ±:
Q1(x, y) = Q‚àó
0(x, xy + Œ±) = Œ±2 + Œ±6x + y(Œ±3 + Œ±3x) + Œ±2xy2
s = s + 1 = 1. As s < k = 2 we go back to step 2 of Algorithm 5.2.
For s = 1, Q1(0, y) = Œ±2 + Œ±3y, which has one root, p1 = Œ±6.
Taking p1 = Œ±6:
Q2(x, y) = Q‚àó
1(x, xy + Œ±6) = y(Œ±3 + Œ±3x) + Œ±2x2y2
s = s + 1 = 2. Now s = k and Q2(x, 0), so this route is terminated and the
message is p(x) = Œ± + Œ±6x.
The roots of Q(x, y) are shown graphically in Figure 5.3. To complete the
decoding procedure, each message would be re-encoded and the code word with
the minimum Hamming distance from the received word would be chosen, along
with its corresponding message.
p0 = 0
p0 = Œ±
p1 = Œ±5
p1 = Œ±6
2
2
3
3
2
6
y
x)
y(
x
x
Q(x, y)
+ Œ±
+ Œ±
Œ±
+
+ Œ±
Œ±
=
Figure 5.3
Two messages from Roth‚ÄìRuckenstein‚Äôs algorithm for Example 5.3.

164
LIST DECODING
5.3 Soft-Decision List Decoding of Reed‚ÄìSolomon Codes Using the
K¬®otter‚ÄìVardy Algorithm
Increases in the performance of a Reed‚ÄìSolomon code can be achieved by taking into
consideration the soft values from the output of the demodulator. These can be used
to give a measure of the reliability of each symbol in the received word. To modify
the Guruswami‚ÄìSudan algorithm to use reliability values instead of hard values, there
needs to be a method of mapping reliability values to multiplicity values. Kotter and
Vardy presented a seminal paper in 2003 [5] which allowed the algebraic soft-decision
decoding of Reed‚ÄìSolomon codes. In this paper, they also gave an algorithm to convert
the reliability of each symbol in the received word to a multiplicity value of each point
(xj, œÅi) for the interpolation process, where œÅi can be one of q Ô¨Ånite Ô¨Åeld elements in
GF(q) = {œÅ0, œÅ1, œÅ2, . . . , œÅq‚àí1}. The reliability values of each received symbol are
arranged in a reliability matrix  and converted into a multiplicity matrix M. The
interpolation and factorization processes then follow in the same way as described
previously. The whole system model for soft-decision list decoding is illustrated in
Figure 5.4.
5.3.1 Mapping Reliability Values into Multiplicity Values
Instead of making a hard decision on the received symbols, the soft values of each
symbol are used to give a measure of reliability. Therefore, the received vector R =
(r0, r1, . . . , rn‚àí1) now contains soft values and not Ô¨Ånite Ô¨Åeld elements. The reliability
of each received symbol is denoted as œÄi,j, which gives the probability of the jth
transmitted coded symbol cj being the j = ith element œÅi in GF(q), given the soft value
of the jth received symbol rj.
pi, j = P(c j = œÅi|r j)(i = 0, 1, . . . , q ‚àí1 and j = 0, 1, . . . , n ‚àí1).
(5.42)
These reliabilities are entered into a q √ó n reliability matrix .
 =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
œÄ0,0
œÄ0,1
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
œÄ0,n‚àí1
œÄ1,0
œÄ1,1
œÄ1,n‚àí1
...
...
...
...
œÄi, j
...
...
...
...
œÄq‚àí1,0
œÄq‚àí1,1
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
œÄq‚àí1,n‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(5.43)
Reliability to
Multiplicity 
Œ† ‚ÜíM
Interpolation
Q(x, y)
Factorization
p(x)
Received
Symbol 
Reliabilities
Decoded
Messages
Figure 5.4
System model for soft-decision list decoding.

SOFT-DECISION LIST DECODING OF REED‚ÄìSOLOMON CODES
165
Referring to Figure 5.1, the matrix  is taken as an input to the soft-decision
decoder and converted to a multiplicity matrix M, followed by the interpolation and
factorization processes. An algorithm presented in [5] to convert the reliability matrix
 to a multiplicity matrix M is now given.
Algorithm 5.3: Convert reliability matrix  to multiplicity matrix M [5]
Input:
Reliability matrix  and a desired value of the sum of multiplicities in matrix M as:
s =
q‚àí1

i=0
n‚àí1

j=0
mi, j.
Initialization: Set * =  and q √ó n all-zero multiplicity matrix M:
1. While (s > 0)
{
2. Find the maximal entry œÄ‚àó
i, j in * with position (i, j).
3. Update œÄ‚àó
i, j in * as œÄ‚àó
i, j =
œÄi, j
mi, j+2.
4. Update mi, j in M as mi, j = mi, j + 1.
5. s = s ‚àí1.
}
Algorithm 5.3 results in a q √ó n multiplicity matrix M, which can be written as:
M =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
m0,0
m0,1 ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ m0,n‚àí1
m1,0
m1,1
m1,n‚àí1
...
...
...
...
mi, j
...
...
...
...
mq‚àí1,0 mq‚àí1,1 ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ mq‚àí1,n‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
(5.44)
The entry mi,j represents the multiplicity value of interpolated point (xj, œÅi) (j =
0, 1, . . . , n ‚àí1 and i = 0, 1, . . . , q ‚àí1). xj are the Ô¨Ånite Ô¨Åeld elements used in
the encoding process described in Chapter 3. In Algorithm 5.3, the desired value s
indicates the total value of multiplicity of all interpolated points. This algorithm gives
priority to those interpolated points which correspond to a higher reliability value œÄi,j,
to be assigned with a higher multiplicity value mi,j. For an illustration of the algorithm,
see Example 5.5.

166
LIST DECODING
Example 5.5: For soft-decision list decoding of the (7, 2) Reed‚ÄìSolomon code
deÔ¨Åned in GF(8), the following 8 √ó 7 reliability matrix  is obtained by the
receiver:
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0.959796
0.214170 0.005453
0.461070
0.001125
0.000505
0.691729
0.001749
0.005760 0.000000
0.525038
0.897551
0.025948
0.000209
0.028559
0.005205 0.000148
0.003293
0.000126
0.018571
0.020798
0.000052
0.000140 0.000000
0.003750
0.100855
0.954880
0.000006
0.009543
0.736533 0.968097 0.003180
0.000000
0.000000
0.278789
0.000017
0.019810 0.000006
0.003621
0.000307
0.000003
0.000084
0.000284
0.017900 0.026295
0.000023
0.000000
0.000002
0.008382
0.000001
0.000481 0.000000
0.000026
0.000035
0.000092
0.000003
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(Note: in the matrix  (*), the maximal entry is underlined).
Apply Algorithm 5.3 with a desired value s = 20.
Initialization: Set * =  and M = 0.
As s = 20 > 0, Ô¨Ånd the maximal entry œÄ‚àó
i, j = 0.968097 in * with position
(i, j) = (4, 2).
Update œÄ‚àó
4,2 as œÄ‚àó
4,2 =
œÄ4,2
m4,2 + 2 = 0.968097
0 + 2
= 0.484048.
Update m4,2 in M as m4,2 = 0 + 1 = 1
s = s ‚àí1 = 19.
Now the updated * is:
‚àó=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0.959796 0.214170
0.005453
0.461070
0.001125
0.000505
0.691729
0.001749
0.005760
0.000000
0.525038
0.897551
0.025948
0.000209
0.028559
0.005205
0.000148
0.003293
0.000126
0.018571
0.020798
0.000052
0.000140
0.000000
0.003750
0.100855
0.954880
0.000006
0.009543
0.736533
0.484048
0.003180
0.000000
0.000000
0.278789
0.000017
0.019810
0.000006
0.003621
0.000307
0.000003
0.000084
0.000284
0.017900
0.026295
0.000023
0.000000
0.000002
0.008382
0.000001
0.000481
0.000000
0.000026
0.000035
0.000092
0.000003
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
and the updated M is:
M =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
In the next iteration, as s = 19 > 0, Ô¨Ånd the maximal entry œÄ‚àó
i, j = 0.959696 in
* with position (i, j) = (0, 0).

SOFT-DECISION LIST DECODING OF REED‚ÄìSOLOMON CODES
167
Update œÄ‚àó
0,0 as œÄ‚àó
0,0 =
œÄ0,0
m0,0 + 2 = 0.959796
0 + 2
= 0.479898.
Update m0,0 in M as m0,0 = 0 + 1 = 1.
s = s ‚àí1 = 18.
Now the updated * is:
‚àó=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0.479898
0.214170 0.005453
0.461070
0.001125
0.000505
0.691729
0.001749
0.005760 0.000000
0.525038
0.897551
0.025948
0.000209
0.028559
0.005205 0.000148
0.003293
0.000126
0.018571
0.020798
0.000052
0.000140 0.000000
0.003750
0.100855
0.954880 0.000006
0.009543
0.736533 0.484048
0.003180
0.000000
0.000000
0.278789
0.000017
0.019810 0.000006
0.003621
0.000307
0.000003
0.000084
0.000284
0.017900 0.026295
0.000023
0.000000
0.000002
0.008382
0.000001
0.000481 0.000000
0.000026
0.000035
0.000092
0.000003
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
and the updated M is:
M =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Following the same process until s = 0, the updated * is:
‚àó=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0.239949
0.214170 0.005453
0.230535
0.001125
0.000505
0.230576
0.001749
0.005760 0.000000
0.175013
0.224388
0.025948
0.000209
0.028559
0.005205 0.000148
0.003293
0.000126
0.018571
0.020798
0.000052
0.000140 0.000000
0.003750
0.100855
0.238720
0.000006
0.009543
0.245511 0.242024
0.003180
0.000000
0.000000
0.139395
0.000017
0.019810 0.000006
0.003621
0.000307
0.000003
0.000084
0.000284
0.017900 0.026295
0.000023
0.000000
0.000002
0.008382
0.000001
0.000481 0.000000
0.000026
0.000035
0.000092
0.000003
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
and the updated M is:
M =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
3
0
0
1
0
0
2
0
0
0
2
3
0
0
0
0
0
0
0
0
0
0
0
0
0
0
3
0
0
2
3
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.

168
LIST DECODING
In the resulting multiplicity matrix M, it can be seen that the sum of its entries
7
i=0
6
j=0
mi, j = 20, which is the desired value s set at the beginning. From M we see
that there are nine nonzero entries, implying that there are now nine points used to
generate the interpolation polynomial Q(x, y). These points are:
(x0,r0) with m0,0 = 3, (x1, œÅ4) with m4,1 = 2, (x2,r4) with m4,2 = 3,
(x3,r0) with m0,3 = 1, (x3,r1) with m1,3 = 2, (x4,r1) with m1,4 = 3,
(x5,r3) with m3,5 = 3, (x6,r0) with m0,6 = 2, (x6,r4) with m4,6 = 1.
5.3.2 Solution Analysis for Soft-Decision List Decoding
Based on Section 5.2.2, to have a multiplicity of mij over interpolated point (xj, œÅi),
the Hasse derivative evaluation of an interpolation polynomial is now deÔ¨Åned as:
Duv Q(x j, œÅi) =

a‚â•u,b‚â•v
a
u
 b
v

Qabxa‚àíu
j
œÅb‚àív
i
, u + v < mi, j
(5.45)
where ri in (5.17) is replaced with œÅi. The total number of iterations for all points is:
CM = 1
2
q‚àí1

i=0
n‚àí1

j=0
mi, j(mi, j + 1).
(5.46)
CM is called the ‚Äòcost‚Äô of multiplicity matrix M, which also denotes the number of
iterations in the interpolation process.
Based on Lemma 5.1, if f(x) is the message polynomial that satisÔ¨Åes f(xj) = cj
(j = 0, 1, . . . , n ‚àí1), polynomial Q(x, f(x)) should satisfy:
(x ‚àíx0)m0(x ‚àíx1)m1 ¬∑ ¬∑ ¬∑ (x ‚àíxn‚àí1)mn‚àí1|Q(x, f (x)).
(5.47)
Again, if we let g1(x) = (x ‚àíx0)m0(x ‚àíx1)m1 ¬∑ ¬∑ ¬∑ (x ‚àíxn‚àí1)mn‚àí1 and g2(x) =
Q(x, f (x)), based on (5.12), g1(x)|g2(x). g1(x) has x-degree degx(g1(x)) = m0 +
m1 + ¬∑ ¬∑ ¬∑ + mn‚àí1. The x-degree of g1(x) is deÔ¨Åned as the code word score SM with
respect to multiplicity matrix M:
SM (c) = degx(g1(x)) = m0 + m1 + ¬∑ ¬∑ ¬∑ + mn‚àí1
=
n‚àí1

j=0
{mi, j|œÅi = c j, i = 0, 1, . . . , q ‚àí1}.
(5.48)
The x-degree of g2(x) is bounded by degx(g2(x)) ‚â§deg1,k‚àí1 Q(x, y). Therefore, if
SM(c) > deg1,k‚àí1 Q(x, y) then degx(g1(x)) > degx(g2(x)). To satisfy both degx(g1(x))
> degx(g2(x)) and g1(x)|g2(x), the only solution is g2(x) = 0, which indicates

SOFT-DECISION LIST DECODING OF REED‚ÄìSOLOMON CODES
169
Q(x, f(x)) = 0, or equivalently y ‚àíf (x)|Q(x, y), and the message polynomial f(x)
can be found by determining Q(x, y)‚Äôs y-roots. As a result, if the code word score with
respect to multiplicity matrix M is greater than the interpolated polynomial Q(x, y)‚Äôs
(1, k ‚àí1)-weighted degree
SM (c) > deg1,k‚àí1 Q(x, y)
(5.49)
then Q(x, f(x)) = 0, or equivalently y ‚àíf (x)|Q(x, y). Message polynomial f(x) can
be found by determining the y roots of Q(x, y).
Based on the (1, k ‚àí1)-weighted degree deÔ¨Ånition of monomial xayb given in
Section 5.2.1, let us deÔ¨Åne the following two parameters:
N1,k‚àí1(Œ¥) =
77{xayb : a, b ‚â•0 and deg1,k‚àí1(xayb) ‚â§Œ¥, Œ¥ ‚ààN}
77 ,
(5.50)
which represents the number of bivariate monomial xayb with (1, k ‚àí1)-weighted
degree not greater than a nonnegative integer Œ¥ [5]; and:
1,k‚àí1(v) = min{Œ¥ : N1,k‚àí1(Œ¥) > v, v ‚ààN},
(5.51)
which denotes the minimal value of Œ¥ that guarantees N1,k‚àí1(Œ¥) is greater than a
nonnegative integer v [5].
If the (1, k ‚àí1)-weighted degree of interpolated polynomial Q is Œ¥*, based on
(5.50), Q has at most N1,k‚àí1(Œ¥*) nonzero coefÔ¨Åcients. The interpolation procedure
generates a system of CM linear equations of type (5.45). The system will be solvable
if [5]:
N1,k‚àí1(Œ¥‚àó) > CM.
(5.52)
Based on (5.51), in order to guarantee the solution, the (1, k ‚àí1)-weighted degree
Œ¥* of the interpolated polynomial Q should be large enough that:
deg1,k‚àí1(Q(x, y)) = Œ¥‚àó= 1,k‚àí1(CM).
(5.53)
Therefore, according to (5.49), given the soft-decision code word score (5.48) and
the (1, k ‚àí1)-weighted degree of the interpolated polynomial Q (5.53), the message
polynomial f can be found if:
SM (c) > 1,k‚àí1(CM).
(5.54)
As the (1, k ‚àí1)-weighted degree of the interpolated polynomial Q(x, y) can
be determined by (5.53), while 1,k‚àí1(CM) can be realized by 1,k‚àí1(CM) =
deg1,k‚àí1(xayb|ord(xayb) = CM), a stopping
rule for Algorithm 5.3 based on the

170
LIST DECODING
designed length of output list l can be imposed. This is more realistic for assessing the
performance soft-decision list decoding. As the factorization outputs are the y-roots
of the interpolated polynomial Q, the maximal number of outputs lM based on the
interpolated polynomial Q is:
lM = deg0,1 Q(x, y) =
&deg1,k‚àí1 Q(x, y)
k ‚àí1
'
=
&1,k‚àí1(CM)
k ‚àí1
'
.
(5.55)
Therefore, after step 5 of Algorithm 5.3, the updated cost CM of the multiplicity
matrix M can be determined using (5.46). As CM has been determined, the interpolated
polynomial Q(x, y)‚Äôs (1, k ‚àí1)-weighted degree can be determined by (5.53). (5.55)
can then be applied to calculate the maximal number of factorization outputs, lM.
Based on a designed length of output list l, Algorithm 5.3 is stopped once lM is greater
than l.
In practice, due to the decoding complexity restriction, soft-decision list decoding
can only be performed based on a designed length of output list l. This output
length restriction in fact leads to practical decoding performance degradation. This
phenomenon will be seen later when the simulation results are discussed.
As mentioned in Section 5.2.5, to build interpolated polynomial Q(x, y) there are
in total CM (5.46) iterations. Therefore, the iteration index ik used in Algorithm 5.1
is: ik = 0, 1, . . . , CM. Based on a designed length of output list l, the initialization at
step 1 of Algorithm 5.1 can be modiÔ¨Åed as:
G0 = {Q0, j = y j, j = 0, 1, . . . ,l},
(5.56)
where l is the designed length of the output list. As there are in total CM iterations,
based on the complexity reducing scheme‚Äôs description given in Section 5.2.4, the
interpolated polynomial Q‚Äôs leading order is less than or equal to the total number of
iterations CM:
lod(Q(x, y)) ‚â§CM.
(5.57)
This indicates the fact that (5.53) is an upper bound for the interpolated polynomial‚Äôs
(1, k ‚àí1)-weighted degree:
deg1,k‚àí1 Q(x, y) ‚â§(CM).
(5.58)
Based on (5.56), those polynomials with leading order greater than CM will nei-
ther be chosen as the interpolated polynomial nor be modiÔ¨Åed with the interpolated
polynomial. Therefore they can be eliminated from the polynomial group and the
modiÔ¨Åcation at step 2 can be rewritten as:
Gik = {Qik, j|lod(Qik, j) = CM}.
(5.59)

LIST DECODING OF ALGEBRAIC‚ÄìGEOMETRIC CODES
171
With respect to interpolated point (xj, œÅi) and Hasse derivative parameter (u, v),
where u + v < mi,j, the Hasse derivative evaluation performed at step 3 of Algorithm
5.1 can be modiÔ¨Åed and determined by (5.45). The resulting process is the same
as Algorithm 5.1, with the exception that for polynomial modiÔ¨Åcation in (5.59) the
interpolated point‚Äôs x-coordinate xi should be replaced by the xj which is the current
interpolated point‚Äôs x-coordinate. Also, the index j should be used for the interpolated
point‚Äôs x-coordinate xj and polynomials in the group Qik, j.
5.3.3 Simulation Results
This section presents both hard-decision and soft-decision list decoding results for two
Reed‚ÄìSolomon codes: (63, 15) and (63, 31). They are simulated on both the AWGN
and Rayleigh fading channels. The Rayleigh fading channel is frequency nonselective
with Doppler frequency [16] 126.67 Hz and data rate 30 kb/s. The fading proÔ¨Åle is
generated using Jakes‚Äô method [16]. The fading coefÔ¨Åcients have mean value 1.55
and variance 0.60. On the Rayleigh fading channel, a block interleaver of size 63 √ó
63 is used to combat the fading effect. QPSK modulation is used and simulations are
run using the C programming language.
Comparisons between hard-decision and soft-decision are made based on output
length l. For an output length l, there are l + 1 polynomials taking part in the
iterative interpolation process. The total number of iterations (Cm in (5.11) for hard-
decision and CM in (5.46) for soft-decision) also grow with length l. The number of
polynomials l + 1 and the number of iterations (Cm, CM) are the important parameters
that determine the decoding complexity. Based on the same designed length, from
Figures 5.5 and 5.6 it can be seen that soft-decision can achieve signiÔ¨Åcant coding
gains over hard-decision list decoding, especially on the Rayleigh fading channel.
For example, with a designed length l = 2, soft-decision list decoding of the (63, 15)
Reed‚ÄìSolomon code can achieve about a 5.8 dB coding gain at BER = 10‚àí5 over
hard-decision decoding.
According to the analysis in [14], the performance improvement of soft-decision list
decoding over hard-decision list decoding is achieved with an insigniÔ¨Åcant complexity
penalty. This is because for the list decoding algorithm, the complexity is mainly
dominated by the interpolation process, and the complexity introduced by the a priori
process (Algorithm 5.3) is marginal. For the interpolation process, the important
parameter that determines its complexity is the iteration number. As the iteration
number of soft-decision does not vary much from hard-decision based on the same
designed output length, the complexity of soft-decision list decoding is not much
higher than that of hard-decision list decoding.
5.4 List Decoding of Algebraic‚ÄìGeometric Codes
The GS algorithm consists of two processes: interpolation and factorization. Given a
received word R = (r0, r1, . . . , rn‚àí1) (ri ‚ààGF(q), i = 0, 1, . . . , n ‚àí1), n interpolated

172
LIST DECODING
1.00E‚Äì06
1.00E‚Äì05
1.00E‚Äì04
1.00E‚Äì03
1.00E‚Äì02
1.00E‚Äì01
1.00E+00
12
11
10
9
8
7
6
5
4
3
2
1
0
SNR (Eb/N0)
BER
uncoded
GS (m=1, l=2)*
GS (m=2, l=4)**
GS (m=4, l=8)***
GS (m=6, l=13)****
GS (m=26, l=55)*****
KV (l=2)*
KV (l=3)
KV (l=4)**
KV (l=8)***
KV (l=13)****
KV (l=55)*****
KV (Optimal)
(a) over AWGN channel 
1.00E‚Äì06
1.00E‚Äì05
1.00E‚Äì04
1.00E‚Äì03
1.00E‚Äì02
1.00E‚Äì01
1.00E+00
20
18
16
14
12
10
8
6
4
2
0
SNR (Eb/N0)
BER
Uncoded
GS (m=1, l=2)*
GS (m=2, l=4)**
GS (m=4, l=8)***
GS (m=6, l=13)****
GS (m=26, l=55)*****
KV (l=1)
KV (l=2)*
KV (l=4)**
KV (l=8)***
KV (l=13)****
KV (l=55)*****
KV (Optimal)
(b) over Rayleigh fading channel 
Figure 5.5
Hard-decision and soft-decision list decoding Reed‚ÄìSolomon code (63, 15).
units can be formed by combining each received symbol with its respective afÔ¨Åne
point used in encoding, as: (p0, r0), (p1, r1), . . . , (pn‚àí1, rn‚àí1). Interpolation builds the
minimal polynomial Q ‚ààFq[x, y, z], which has a zero of multiplicity of at least m
over the n interpolated units. Q can be written as: Q = 
a,b QabœÜazb, where Qab ‚àà
GF(q) and œÜa is a set of rational functions with pole orders up to a [17‚Äì19]. If (pi, ri)

LIST DECODING OF ALGEBRAIC‚ÄìGEOMETRIC CODES
173
1.00E‚Äì06
1.00E‚Äì05
1.00E‚Äì04
1.00E‚Äì03
1.00E‚Äì02
1.00E‚Äì01
1.00E+00
12
11
10
9
8
7
6
5
4
3
2
1
0
SNR (Eb/N0)
BER
uncoded
GS (m=1, l=1)*
GS (m=3, l=4)**
GS (m=5, l=7)***
GS (m=13, l=19)****
KV (l=1)*
KV (l=2)
KV (l=3)
KV (l=4)**
KV (l=7)***
KV (l=19)****
KV (Optimal)
(a) over AWGN channel 
1.00E‚Äì06
1.00E‚Äì05
1.00E‚Äì04
1.00E‚Äì03
1.00E‚Äì02
1.00E‚Äì01
1.00E+00
20
18
16
14
12
10
8
6
4
2
0
SNR (Eb/N0)
BER
Uncoded
GS (m=1, l=1)*
GS (m=3, l=4)**
GS (m=5, l=7)***
GS (m=13, l=19)****
KV (l=1)*
KV (l=4)**
KV (l=7)***
KV (l=19)****
KV (Optimal)
(b) over Rayleigh fading channel 
Figure 5.6
Hard-decision and soft-decision list decoding Reed‚ÄìSolomon code (63, 31).
is the intended interpolated unit, it can also be written with respect to the zero basis
functions in Zw,pi deÔ¨Åned in Chapter 2 as [9]:
Q =

u,v
Q(pi,ri)
uv
œàpi,u(z ‚àíri)v,
(5.60)

174
LIST DECODING
where Q(pi,ri)
uv
‚ààGF(q). If Q(pi,ri)
uv
= 0 for u + v < m, polynomial Q has a
zero of multiplicity at least m at unit (pi, ri) [9, 11]. As zb = (z ‚àíri + ri)b =

v‚â§b ( b
v )rb‚àív
i
(z ‚àíri)v and œÜa = 
u Œ≥a,pi,uœàpi,u, substitute them into (5.15):
Q =

a,b
Qab

u
Œ≥a,pi,uœàpi,u
 
v‚â§b
b
v

rb‚àív
i
(z ‚àíri)v

=

u,v
 
a,b‚â•v
Qab
b
v

Œ≥a,pi,urb‚àív
i

œàpi,u(z ‚àíri)v
.
(5.61)
Therefore, the coefÔ¨Åcients Q(pi,ri)
uv
of (5.61) can be written as:
Q(pi,ri)
uv
=

a,b‚â•v
Qab

b
v

Œ≥a,pi,urb‚àív
i
.
(5.62)
(5.62) deÔ¨Ånes the zero condition constraints on the coefÔ¨Åcients Qab of polynomial
Q analogous to the Hasse derivative, so that Q has a zero of multiplicity at least m
over unit (pi, ri). Example 5.6 shows how to deÔ¨Åne the zero condition of a polynomial
in Fq[x, y, z] using (5.62).
Example 5.6: Given the polynomial Q(x, y, z) = 1 + Œ±y + Œ±x2 + z2(1 + Œ±2y)
deÔ¨Åned in GF(4)[x, y, z] justify the fact that it has a zero of multiplicity at least 2
over the unit (p, r) = ((1, Œ±), Œ±).
Polynomial Q(x, y, z) = 1 + Œ±y + Œ±x2 + z2(1 + Œ±2y) = Q00œÜ0z0 + Q20œÜ2z0
+ Q30œÜ3z0 + Q02œÜ0z2 + Q22œÜ2z2. Supporting the zero condition calculations, the
corresponding coefÔ¨Åcients Œ≥a,p,u are shown in Table 5.3.
Table 5.3
Corresponding coefÔ¨Åcients Œ≥a,p,u given p = (1, Œ±).
a \ u
0
1
2
3
. . .
0
1
1
Œ±
1
. . .
1
0
1
1
0
. . .
...
...
...
...
...
Based on the above description, to justify that Q has a zero of multiplicity m
over unit (p, r), its coefÔ¨Åcients Qab should satisfy Q(p,r)
uv
= 0 for u + v < 2 as:
Q(p,r)
00
= 0, Q(p,r)
01
= 0 and Q(p,r)
10
= 0.

LIST DECODING OF ALGEBRAIC‚ÄìGEOMETRIC CODES
175
Based on deÔ¨Ånition (5.62):
Q(p,r)
00
= Q00
0
0

Œ≥0,p,0Œ±0‚àí0 + Q20
0
0

Œ≥2,p,0Œ±0‚àí0 + Q30
0
0

Œ≥3,p,0Œ±0‚àí0
+ Q02
2
0

Œ≥0,p,0Œ±2‚àí0 + Q22
2
0

Œ≥2,p,0Œ±2‚àí0
= 1 + Œ±2 + Œ± + Œ±2 + Œ±2 = 0
Q(p,r)
01
= Q02

2
1

Œ≥0,p,0Œ±2‚àí1 + Q22

2
1

Œ≥2,p,0Œ±2‚àí1 = 0 + 0 = 0
Q(p,r)
10
= Q00

0
0

Œ≥0,p,1Œ±0‚àí0 + Q20

0
0

Œ≥2,p,1Œ±0‚àí0
+ Q30

0
0

Œ≥3,p,1Œ±0‚àí0 + Q02

2
0

Œ≥0,p,1Œ±2‚àí0 + Q22

2
0

Œ≥2,p,1Œ±2‚àí0
= Œ± + Œ± = 0.
Therefore, polynomial Q has a zero of multiplicity at least 2 over unit (p, r) =
(1, Œ±), Œ±).
If constraint (5.62) for the coefÔ¨Åcients of polynomial Q is denoted as D(pi,ri)
uv
(Q),
such that:
D(pi,ri)
uv
(Q) = Q(pi,ri)
uv
=

a,b‚â•v
Qab
b
v

Œ≥a,pi,urb‚àív
i
(5.63)
then interpolation builds a polynomial Q deÔ¨Åned as:
Q = min
lod(Q)

Q ‚ààFq[x, y, z]|D(pi,ri)
uv
(Q) = 0
for
i = 0, 1, . . . , n ‚àí1 ‚àßu
+ v < m (u, v ‚ààN)}

.
(5.64)
As there are ( m+1
2 ) permutations of (u, v) for u + v < m, there are in total:
C = n
m + 1
2

(5.65)
zero condition constraints that the coefÔ¨Åcients Qab of polynomial Q need to satisfy.
C also represents the number of iterations in the interpolation algorithm [9, 11], in
which each iteration imposes a zero condition constraint to Qab.
DeÔ¨Ånition: For monomial œÜazb, where œÜa ‚ààLw and Lw is the Hermitian curve‚Äôs pole
basis deÔ¨Åned in GF(w2), its (1, wz)-weighted degree is deÔ¨Åned as:
deg1,wz(œÜazb) = v p‚àû(œÜ‚àí1
a ) + b ¬∑ wz,

176
LIST DECODING
where wz is the weighted degree for variable z, and deÔ¨Åned as: wz = v p‚àû(z‚àí1) =
v p‚àû(œÜ‚àí1
k‚àí1). œÜk‚àí1 is the maximal term in the message polynomial. A (1, wz)-
lexicographic order (ord) can be deÔ¨Åned to arrange monomials œÜazb:
œÜa1zb1 < œÜa2zb2,
if deg1,wz(œÜa1zb1) < deg1,wz(œÜa2zb2), or deg1,wz(œÜa1zb1) = deg1,wz(œÜa2zb2) and b1 <
b2 [9]. If œÜazb is the maximal monomial in polynomial Q = 
a,b QabœÜazb as:
œÜazb = max{œÜazb|Qab = 0}.
œÜazb is called Q‚Äôs leading monomial (LM) and its coefÔ¨Åcient Qab is called f‚Äôs
leading coefÔ¨Åcient (LC), denoted as: LM( f ) = œÜazb and LC( f ) = fab. Polynomial
Q‚Äôs (1, wz)-weighted degree (deg1,wz(Q)) and leading order (lod(Q)) are deÔ¨Åned as:
deg1,wz(Q) = deg1,wz(œÜazb),
and
lod(Q) = ord(œÜazb).
The (1, wz)-weighted degree upper bound of polynomial Q is deÔ¨Åned as [9, 11]:
max{deg1,wz Q} = lmv p‚àû(z ‚àí1) + tm,
(5.66)
where lm is the maximal number of output candidates from factorization, deÔ¨Åned as:
lm = max

u|
u
2

v p‚àû(z ‚àí1) ‚àí(u ‚àí1)g ‚â§C

‚àí1,
(5.67)
and parameter tm is deÔ¨Åned as:
tm = max

u|(lm + 1)u ‚àí (u) +

lm + 1
2

v p‚àû(z ‚àí1) ‚àílmg ‚â§C

,
(5.68)
where g is the genus of the Hermitian curve, u ‚ààN and  (u) denotes the number of
gaps that are less than or equal to the nonnegative integer u [11].
If there exists a polynomial h ‚ààFuz
q [x, y] such that:
(h, R) = |{i|h(pi) = ri, i = 0, 1, . . . , n ‚àí1}|
(5.69)

LIST DECODING OF ALGEBRAIC‚ÄìGEOMETRIC CODES
177
then the total zero orders of polynomial Q(x, y, h) over all the interpolated units is:
n‚àí1

i=0
v pi(Q(x, y, h)) = m(h, R).
(5.70)
To deÔ¨Åne the total zero order of polynomial Q(x, y, h), the following lemma is
applied:
Lemma 5.4
If Q(x, y, z) has a zero of multiplicity m over unit (pi, ri) and h is a
polynomial in Fuz
q [x, y] that satisÔ¨Åes h(pi) = ri then Q(x, y, h) has a zero order of at
least m at pi, as v pi(Q(x, y, h)) ‚â•m [9, 11].
(5.69) deÔ¨Ånes the total number of afÔ¨Åne points that satisfy h(pi) = ri, and therefore
the total zero order of polynomial Q(x, y, h) over all the afÔ¨Åne points is deÔ¨Åned by
(5.70).
Theorem 5.1
If polynomial Q(x, y, h)‚Äôs total zero order is greater than its pole order,
as:
n‚àí1

i=0
v pi(Q(x, y, h)) > v p‚àû(Q(x, y, h)‚àí1),
(5.71)
then h is the z root of Q: Q(x, y, h) = 0, or equivalently z ‚àíh|Q(x, y, z) [6, 9, 11].
As h ‚ààFuz
q [x, y], v p‚àû(Q(x, y, h)‚àí1) = v p‚àû(Q(x, y, z)‚àí1) = deg1,wz(Q(x, y, z)).
Therefore, based on (5.69) and (5.70), Theorem 5.1 results in the following corollary:
Corollary 5.1
If there exists a polynomial h ‚ààFuz
q [x, y] such that:
m (h, R) > deg1,wz(Q(x, y, z))
(5.72)
then the list decoding outputs h can be found by factorizing the interpolated polynomial
Q(x, y, z) as: z ‚àíh|Q(x, y, z) [18].
If h = f, (5.69) deÔ¨Ånes the number of uncorrupted received symbols. Therefore,
the GS algorithm‚Äôs error-correction capability œÑ m is:
tm = n ‚àí(h, R) = n ‚àí
&deg1,wz Q
m
'
‚àí1.
(5.73)
Since the upper bound of deg1,wz Q is deÔ¨Åned by (5.70):
tm ‚â•n ‚àí
&lmv p‚àû(z‚àí1) + tm
m
'
‚àí1.
(5.74)
The GS algorithm‚Äôs error-correction capability upper bound for a (n, k) Hermitian
code is deÔ¨Åned by:
tGS = n ‚àí
8
n(n ‚àíd‚àó)
9
‚àí1.

178
LIST DECODING
5.5 Determining the Corresponding CoefÔ¨Åcients
Based on (5.62), the corresponding coefÔ¨Åcients Œ≥a,pi,u are critical for deÔ¨Åning the
zero condition of a polynomial in Fq[x, y, z]. Without knowing them, we have to
transfer a general polynomial written with respect to the zero basis functions and
Ô¨Ånd the coefÔ¨Åcients Q(pi,ri)
uv
, which is not efÔ¨Åcient during the iterative interpolation.
In fact, the corresponding coefÔ¨Åcients Œ≥a,pi,u can be determined independently of the
received word. Therefore, if they can be determined beforehand and applied during the
iterations, the interpolation efÔ¨Åciency can be greatly improved. This section proposes
an algorithm to determine them.
The problem we intend to solve can be simply stated as: given an afÔ¨Åne point
pi = (xi, yi) of curve Hw and a pole basis monomial œÜa, determine the corresponding
coefÔ¨Åcients Œ≥a,pi,u so that œÜa can be written as a sum of the zero basis functions
œàpi,u : œÜa = 
u Œ≥a,pi,uœàpi,u. For any two pole basis monomials œÜa1 and œÜa2 in Lw,
œÜa1œÜa2 = 
a‚ààN œÜa and the zero basis function œàpi,u (2.13) can be written as a sum of
pole basis monomials œÜa [9]:
œàpi,u =

a
Œ∂aœÜa,
(5.75)
where coefÔ¨Åcients Œ∂ a ‚ààGF(q). Partition œàpi,u(x, y) as:
œàpi,u = œà A
pi,u ¬∑ œà B
pi,u,
(5.76)
where œà A
pi,u = (x ‚àíxi)Œª and œà B
pi,u = [(y ‚àíyi) ‚àíxw
i (x ‚àíxi)]Œ¥ = [y ‚àíxw
i x ‚àí(yi ‚àí
xw+1
i
)]Œ¥. It is easy to recognize that œà A
pi,u has leading monomial LM(œà A
pi,u) = xŒ¥
and leading coefÔ¨Åcient LC(œà A
pi,u) = 1. As v p‚àû(y‚àí1) > v p‚àû(x‚àí1), œà B
pi,u has leading
monomial LM(œà B
pi,u) = yŒ¥ and leading coefÔ¨Åcient LC(œà B
pi,u) = 1. Based on (5.76),
œàpi,u has leading monomial LM(œà A
pi,u) ¬∑ LM(œà B
pi,u) = xŒªyŒ¥ and leading coefÔ¨Åcient
LC(œà A
pi,u) ¬∑ LC(œà B
pi,u) = 1. As 0 ‚â§Œª ‚â§w and Œ¥ ‚â•0, the set of leading monomials
of zero basis functions in Zw,pi contains all the monomials deÔ¨Åned in pole basis Lw.
Summarizing the above analysis, Corollary 5.2 is proposed as follows:
Corollary 5.2
If œÜL is the leading monomial of zero basis function œàpi,u as
LM(œàpi,u) = œÜL, the leading coefÔ¨Åcient of œàpi,u equals 1 and (5.75) can be writ-
ten as [18]:
œàpi,u =

a<L
Œ∂aœÜa + œÜL.
(5.77)
The set of leading monomials of zero basis functions in Zw,pi contains all the
monomials in Lw:
{LM(œàpi,u) = œÜL, œàpi,u ‚ààZw,pi} ‚äÜLw.
(5.78)

DETERMINING THE CORRESPONDING COEFFICIENTS
179
Following on, by identifying the second-largest pole basis monomial œÜL‚àí1 with
coefÔ¨Åcient Œ∂ L‚àí1 ‚ààGF(q) in œàpi,u, (5.77) can also be written as [18]:
œàpi,u =

a<L‚àí1
Œ∂aœÜa + Œ∂L‚àí1œÜL‚àí1 + œÜL.
(5.79)
Now it is sufÔ¨Åcient to propose the new efÔ¨Åcient algorithm in order to determine the
corresponding coefÔ¨Åcients Œ≥a,pi,u.
Algorithm 5.4: Determine the corresponding coefÔ¨Åcients Œ≥a,pi,u between a pole
basis monomial and zero basis functions [14, 18]
1. Initialize all corresponding coefÔ¨Åcients Œ≥a,pi,u = 0.
2. Find the zero basis function œàpi,u with LM(œàpi,u) = œÜa, and let Œ≥a,pi,u = 1.
3. Initialize function ÀÜœà = œàpi,u.
4. While ( ÀÜœà = œÜa)
{
5. Find the second-largest pole basis monomial œÜL‚àí1 with coefÔ¨Åcient Œ∂ L‚àí1 in ÀÜœà.
6. In Zw,pi, Ô¨Ånd a zero basis function œàpi,Œ± whose leading monomial LM(œàpi,u) =
œÜL‚àí1, and let the corresponding coefÔ¨Åcient Œ≥a,pi,u = Œ∂L‚àí1.
7. Update ÀÜœà = ÀÜœà + Œ≥a,pi,uœàpi,u.
}
Proof: Notice that functions œàpi,Œ± with LM(œàpi,u) > œÜa will not contribute to the
sum calculation of (5.62) and their corresponding coefÔ¨Åcients Œ≥a,pi,u = 0. The zero
basis function œàpi,u found at step 2 has leading monomial œÜL = œÜa. Based on (5.79),
it can be written as [18]:
œàpi,u =

a<L‚àí1
Œ∂aœÜa + Œ∂L‚àí1œÜL‚àí1 + œÜa
(5.80)
(5.80) indicates that the corresponding coefÔ¨Åcient between œÜa and œàpi,u is 1;
Œ≥a,pi,u = 1. Polynomial ÀÜœà, initialized by step 3, is an accumulated polynomial result-
ing in œÜa. While ÀÜœà = œÜa, in (5.80), the second-largest monomial œÜL‚àí1 with coefÔ¨Åcient
Œ∂ L‚àí1 is identiÔ¨Åed by step 5. Next Ô¨Ånd another zero basis function œàpi,u in Zw,pi such
that LM(œàpi,u) = œÜL‚àí1. According to Corollary 5.2, this zero basis function always
exists and it can be written as: œàpi,u = 
a<L‚àí1 Œ∂aœÜa + œÜL‚àí1. At step 6, the corre-
sponding coefÔ¨Åcient between monomial œÜa and the found zero basis function œàpi,u
can be determined as: Œ≥a,pi,u = Œ∂L‚àí1. As a result, the accumulated calculation of step
7 can be written as:
ÀÜœà =

a<L‚àí1
Œ∂aœÜa + Œ∂L‚àí1œÜL‚àí1 + œÜa + Œ≥a,pi,uœàpi,u
(5.81)
=

a<L‚àí1
Œ∂aœÜa + Œ∂L‚àí1œÜL‚àí1 + œÜa +

a<L‚àí1
Œ∂L‚àí1Œ∂aœÜa + Œ∂L‚àí1œÜL‚àí1.

180
LIST DECODING
Therefore, in the new accumulated ÀÜœà, Œ∂ L‚àí1œÜL‚àí1 is eliminated, while the leading
monomial œÜa is preserved. If the updated ÀÜœà = œÜa, its second-largest monomial œÜL‚àí1
is again eliminated, while œÜa is always preserved as a leading monomial by the
same process. The algorithm terminates after all monomials that are smaller than œÜa
have been eliminated and results in ÀÜœà = œÜa. This process is equivalent to the sum
calculation of (5.62). Example 5.7 illustrates Algorithm 5.4.
Example 5.7: Given pi = (Œ±2, Œ±2) is an afÔ¨Åne point on curve H2 and a pole basis
(L2) monomial œÜ5 = y2, determine the corresponding coefÔ¨Åcients Œ≥5,pi,u so that œÜ5
can be written as œÜ5 = 
u
Œ≥5,pi,uœàpi,u.
Based on (2.13), the Ô¨Årst eight zero basis functions in Z2,pi can be listed as:
œàpi,0 = (x ‚àíŒ±2)0 = 1
œàpi,1 = (x ‚àíŒ±2)1 = Œ±2 + x
œàpi,2 = (x ‚àíŒ±2)2 = Œ± + x2
œàpi,3 = (y ‚àíŒ±2) ‚àíŒ±(x ‚àíŒ±2) = Œ± + Œ±x + y
œàpi,4 = (x ‚àíŒ±2)[(y ‚àíŒ±2) ‚àíŒ±(x ‚àíŒ±2)] = 1 + Œ±2x + Œ±2y + Œ±x2 + xy
œàpi,5 = (x ‚àíŒ±2)2[(y ‚àíŒ±2) ‚àíŒ±(x ‚àíŒ±2)] = Œ±2 + Œ±2x + Œ±x2 + Œ±y2 + x2y
œàpi,6 = [(y ‚àíŒ±2) ‚àíŒ±(x ‚àíŒ±2)]2 = Œ±2 + Œ±2x2 + y2
œàpi,7 = (x ‚àíŒ±2)[(y ‚àíŒ±2) ‚àíŒ±(x ‚àíŒ±2)]2 = Œ± + Œ±2x + Œ±2y + Œ±x2 + xy2.
Initialize all Œ≥5,pi,u = 0. In Z2,pi, as LM(œàpi,6) = œÜ5, we let Œ≥5,pi,6 = 1 and
initialize the accumulated polynomial ÀÜœà = œàpi,6 = Œ±2 + Œ±2x2 + y2.
As ÀÜœà = œÜ5, its second-largest monomial œÜL‚àí1 = x2 with coefÔ¨Åcient Œ∂ L‚àí1 = Œ±2 is
identiÔ¨Åed. Among the zero basis functions in Z2,pi, we Ô¨Ånd œàpi,2 with LM(œàpi,2) =
œÜL‚àí1 = x2, and let Œ≥5,pi,2 = Œ∂L‚àí1 = Œ±2. Update ÀÜœà = ÀÜœà + Œ≥5,pi,2œàpi,2 = Œ± + y2.
As
ÀÜœà = œÜ5, again its second-largest monomial œÜL‚àí1 = 1 with coefÔ¨Åcient
Œ∂L‚àí1 = Œ± is identiÔ¨Åed. Among the zero basis functions in Z2,pi, we Ô¨Ånd
œàpi,0 with LM(œàpi,0) = œÜL‚àí1 = 1, and let Œ≥5,pi,0 = Œ∂L‚àí1 = Œ±. Update ÀÜœà = ÀÜœà +
Œ≥5,pi,0œàpi,0 = y2.
Now, ÀÜœà = œÜ5, we can stop the algorithm and output Œ≥5,pi,0 = Œ±, Œ≥5,pi,2 = Œ±2
and Œ≥5,pi,6 = 1. The rest of the corresponding coefÔ¨Åcients Œ≥5,pi,u = 0 (u = 0, 2, 6).
Before interpolation, monomials œÜa that exist in the interpolated polynomial Q
are unknown. However, the (1, wz)-weighted degree upper bound of polynomial Q
is deÔ¨Åned by (5.70), from which the largest pole basis monomial œÜmax that might
exist in Q can be predicted by v p‚àû(œÜ‚àí1
max) = max{deg1,wz Q}. Based on interpolation
multiplicity m, with parameter u < m, the corresponding coefÔ¨Åcients that might
be used in interpolation are Œ≥0,pi,u ‚àºŒ≥max,pi,u(u < m). Therefore Algorithm 5.4
can be used to determine all the corresponding coefÔ¨Åcients Œ≥0,pi,u ‚àºŒ≥max,pi,u and
only Œ≥0,pi,u ‚àºŒ≥max,pi,u (u < m) are stored for interpolation in order to minimize
the memory requirement. For example, to list decode the (8, 4, 4) Hermitian code

COMPLEXITY REDUCTION INTERPOLATION
181
with multiplicity m = 2, max{deg1,wz Q} = 13. Therefore, the largest pole basis
monomial that might exist in Q is œÜmax = œÜ12 = x2y3 and Algorithm 5.4 can be
applied to calculate all the corresponding coefÔ¨Åcients Œ≥0,pi,u ‚àºŒ≥12,pi,u (u < 2) that
are stored.
5.6 Complexity Reduction Interpolation
Interpolation determines a polynomial Q that intersects the points (pi, ri) (i = 0,
1, . . . , n ‚àí1). This can be implemented by an iterative polynomial construction
algorithm [4, 9, 11, 13]. At the beginning, a group of polynomials are initialized.
During the iterations, they are tested by different zero condition constraints and
modiÔ¨Åed interactively. As with the hard-decision list decoding of Reed‚ÄìSolomon
codes, there are from (5.11) a total of C iterations, after which the minimal polynomial
in the group is chosen as the interpolated polynomial Q. According to the iterative
process analysis given in Section 5.2.4 and also [13], the interpolated polynomial Q
has leading order lod(Q) ‚â§C. This indicates that those polynomials with leading order
greater than C will not be the chosen candidates. Also, if there is a polynomial in the
group with leading order greater than C during the iterations, the chosen polynomial
Q will not be modiÔ¨Åed with this polynomial, otherwise lod(Q) > C. Therefore, those
polynomials with leading order greater than C can be eliminated from the group
during iterations in order to save unnecessary computations.
If f ‚ààFq[x, y, z] has leading monomial LM( f ) = œÜazb, polynomials in Fq[x, y, z]
can be partitioned into the following classes according to their leading monomial‚Äôs
z-degree b and œÜa‚Äôs pole order v p‚àû(œÜ‚àí1
a ) [18]:
VŒª+wŒ¥ = { f ‚ààFq[x, y, z]|b = Œ¥ ‚àßv p‚àû(œÜ‚àí1
a ) = uw + Œª,
LM( f ) = œÜazb, (Œ¥, u, Œª) ‚ààN, Œª < w},
(5.82)
such that Fq[x, y, z] = 
Œª<w
VŒª+wŒ¥. The factorization outputs are the z-roots of Q.
Therefore, the z-degree of Q is less than or equal to the maximal number of the output
list lm (5.67) and Q is a polynomial chosen from the following classes:
Vj = VŒª+wŒ¥
(0 ‚â§Œª < w, 0 ‚â§Œ¥ ‚â§lm).
(5.83)
At the beginning of the iterative process, a group of polynomials are initialized to
represent each of the polynomial classes deÔ¨Åned by (5.83) as:
G = {Q j = QŒª+wŒ¥ = yŒªzŒ¥, Q j ‚ààVj}.
(5.84)
During the iterations, each polynomial Qj in the group G is the minimal polynomial
within its class Vj that satisÔ¨Åes all the tested zero conditions. At the beginning of each
iteration, the polynomial group G is modiÔ¨Åed by [18]:
G = {Q j|lod(Q j) ‚â§C}
(5.85)

182
LIST DECODING
in order to eliminate those polynomials with leading order greater than C. Then the
remaining polynomials in G are tested by the zero condition constraint deÔ¨Åned by
(5.62) as:
 j = D(pi,ri)
uv
(Q j).
(5.86)
The determined corresponding coefÔ¨Åcients Œ≥a,pi,u are applied for this calculation.
Those polynomials with  j = 0 satisfy the zero condition and do not need to be
modiÔ¨Åed. However, those polynomials with  j = 0 need to be modiÔ¨Åed. Among
them, the index of the minimal polynomial is found as j and the minimal polynomial
is recorded as Q:
j = index ( min
lod(Q j){Q j| j = 0})
(5.87)
Q = Q j.
(5.88)
Q j is modiÔ¨Åed as:
Q j = (x ‚àíxi)Q,
(5.89)
where xi is the x-coordinate of afÔ¨Åne point pi which is included in the current inter-
polated unit (pi, ri). The modiÔ¨Åed Q j satisÔ¨Åes D(pi,ri)
uv
(Q j) = 0. Based on Prop-
erties 1 and 2 mentioned in Section 5.2.3, D(pi,ri)
uv
[(x ‚àíxi)Q] = D(pi,ri)
uv
(x Q) ‚àí
xi D(pi,ri)
uv
(Q) = xi D j ‚àíxi j = 0. The rest of the polynomials with  j = 0 are
modiÔ¨Åed as:
Q j =  j Q j ‚àí j Q.
(5.90)
The modiÔ¨Åed Qj satisÔ¨Åes D(pi,ri)
uv
(Q j) = 0 because D(pi,ri)
uv
[ j Q j ‚àí j Q] =
 j D(pi,ri)
uv
(Q j) ‚àí j D(pi,ri)
uv
(Q) =  j ‚àí j = 0. After C iterations, the minimal
polynomial in the group G is chosen as the interpolated polynomial Q:
Q = min
lod(Q j){Q j|Q j ‚ààG}.
(5.91)
From the above description, it can be seen that by applying the complexity reduc-
tion scheme in (5.85) the zero condition calculation from (5.86) and modiÔ¨Åcations
from (5.89) and (5.90), for those polynomials Qj with lod(Qj) > C, can be avoided
and therefore the interpolation efÔ¨Åciency can be improved. According to [13], this
complexity reduction scheme is error-dependent, so that it reduces complexity more
signiÔ¨Åcantly in low-error-weight situations. This is because the modiÔ¨Åcation scheme
of (5.89) takes action in earlier iteration steps for low-error-weight situations, and
therefore computation can be reduced. Figure 5.7 shows interpolation complexity
reduction (with different multiplicity m) when applying the scheme in (5.85) to de-
code the (64, 19, 40) Hermitian code. It is shown that complexity can be reduced

COMPLEXITY REDUCTION INTERPOLATION
183
1.00E+05
1.00E+06
1.00E+07
1.00E+08
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
Number of Errors
Computational Complexity
original interpolation (m=1)
complexity reducing
interpolation (m=1)
original interpolation (m=2)
complexity reducing
interpolation (m=2)
original interpolation (m=3)
complexity reducing
interpolation (m=3)
48.83%
41.34%
34.08%
19.69%
11.92%
5.89%
2.61%
30.17%
22.05%
14.28%
11.29% 10.60%
Figure 5.7
Complexity analysis for the interpolation of GS decoding Hermitian code (64, 19, 40).
signiÔ¨Åcantly in low-error-weight situations, especially when m = 1 where complexity
can be reduced up to 48.83%. However, in high-error-weight situations, complexity
reduction is not as signiÔ¨Åcant. Based on Figure 5.7, it can also be observed that the
complexity reduction also depends on the interpolation multiplicity m. When m = 1,
complexity reduction is most signiÔ¨Åcant; when m = 2, complexity reduction is the
most marginal.
Summarizing Sections 5.5 and 5.6, the modiÔ¨Åed complexity reduction interpolation
process for GS decoding Hermitian codes can be stated as follows:
Initial computation: Apply Algorithm 5.4 to determine all the necessary correspond-
ing coefÔ¨Åcients Œ≥a,pi,u and store them for use by the iterative polynomial construction
algorithm (Algorithm 5.5).
Algorithm 5.5: Iterative polynomial construction [14, 18]
Initialization:: Initial-
ize the group of polynomials G with (5.84).
1. For each interpolated unit (pi, ri) (i = 0, 1, . . . , n ‚àí1)
{
2. For each pair of the zero condition parameters (u, v) (u + v < m)
{
3. Modify polynomial group G by (5.85).
4. Test the zero condition  j of each polynomial in G with (5.86).
5. For polynomials Qj with  j = 0
{

184
LIST DECODING
6. Denote the minimal polynomial‚Äôs index as j by (5.87) and record it as Q by (5.88).
7. If j = j, Qj is modiÔ¨Åed by (5.89).
8. If j = j, Qj is modiÔ¨Åed by (5.90).
}}}
At the end of the iterations, the minimal polynomial Q is chosen from the group G,
as in (5.91).
Example 5.8 illustrates this complexity reduction interpolation process.
Example 5.8: Decode the (8, 4, 4) Hermitian code deÔ¨Åned in GF(4) using the GS
algorithm with interpolation multiplicity m = 2.
The Hermitian code word is generated by evaluating the message polynomial
over the following afÔ¨Åne points: p0 = (0, 0), p1 = (0, 1), p2 = (1, Œ±), p3 = (1, Œ±2),
p4 = (Œ±, Œ±), p5 = (Œ±, Œ±2), p6 = (Œ±2, Œ±), p7 = (Œ±2, Œ±2). The received word is given
as R = (1, Œ±2, Œ±, Œ±2, Œ±, Œ±2, Œ±2, Œ±).
Applying (5.11), the iteration number C = 8
3
2

= 24. Based on C, the length
of the output list can be determined as l2 = 3, and of parameter t2 as t2 = 1, by using
(5.67) and (5.68) respectively. As a result, the (1, 4)-weighted degree upper bound
for the interpolated polynomial can be determined by (5.66) as max{deg1,4 Q} =
13. Therefore, the maximal pole basis (L2) monomial that might exist in the in-
terpolated polynomial is œÜmax = œÜ12 = x2y3. As the interpolation multiplicity
m = 2, Algorithm 5.4 is applied to determine the corresponding coefÔ¨Åcients
Œ≥0,pi,u ‚àíŒ≥12,pi,u and Œ≥0,pi,u ‚àíŒ≥12,pi,u (u < 2) are stored for the following in-
terpolation process. Table 5.4 lists all the resulted corresponding coefÔ¨Åcients
Œ≥0,pi,u ‚àºŒ≥12,pi,u (u < 2).
Following on, Algorithm 5.5 is performed to Ô¨Ånd the interpolated polynomial
Q(x, y, z). At the beginning, a group of polynomials is initialized as: Q0 = 1,
Q1 = y, Q2 = z, Q3 = yz, Q4 = z2, Q5 = yz2, Q6 = z3, Q7 = yz3. Their leading
orders are: lod(Q0) = 0, lod(Q1) = 2, lod(Q2) = 4, lod(Q3) = 9, lod(Q4) = 12,
lod(Q5) = 20, lod(Q6) = 24, lod(Q7) = 35.
For interpolated unit, (p0, r0) = ((0, 0), 1).
For the zero parameter, u = 0 and v = 0.
As lod(Q7) > C, polynomial Q7 is eliminated from the group.
Test the zero condition of the remaining polynomials in the group as:
0 = D(p0,r0)
00
(Q0) = 1, 1 = D(p0,r0)
00
(Q1) = 0, 2 = D(p0,r0)
00
(Q2) = 1,
3 = D(p0,r0)
00
(Q3) = 0,
4 = D(p0,r0)
00
(Q4) = 1, 5 = D(p0,r0)
00
(Q5) = 0, 6 = D(p0,r0)
00
(Q6) = 1.
Find the minimal polynomial with j = 0 as:
j = 0
and
Q = Q0.

COMPLEXITY REDUCTION INTERPOLATION
185
Table 5.4
Predetermined corresponding coefÔ¨Åcients for Example 5.8.
p0, Œ≥a,p0,u
a/u
0
1
2
3
4
5
6
7
8
9
10
11
12
0
1
0
0
0
0
0
0
0
0
0
0
0
0
1
0
1
0
0
0
0
0
0
0
0
0
0
0
p1, Œ≥a,p1,u
a/u
0
1
2
3
4
5
6
7
8
9
10
11
12
0
1
0
1
0
0
1
0
0
1
0
0
1
0
1
0
1
0
0
1
0
0
1
0
0
1
0
0
p2, Œ≥a,p2,u
a/u
0
1
2
3
4
5
6
7
8
9
10
11
12
0
1
1
Œ±
1
Œ±
Œ±2
Œ±
Œ±2
1
Œ±2
1
Œ±
1
1
0
1
1
0
Œ±2
0
1
Œ±2
Œ±2
0
Œ±
0
Œ±2
p3, Œ≥a,p3,u
a/u
0
1
2
3
4
5
6
7
8
9
10
11
12
0
1
1
Œ±2
1
Œ±2
Œ±
Œ±2
Œ±
1
Œ±
1
Œ±2
1
1
0
1
1
0
Œ±
0
1
Œ±
Œ±
0
Œ±2
0
Œ±
p4, Œ≥a,p4,u
a/u
0
1
2
3
4
5
6
7
8
9
10
11
12
0
1
Œ±
Œ±
Œ±2
Œ±2
Œ±2
1
1
1
Œ±
Œ±
Œ±
Œ±2
1
0
1
Œ±2
0
Œ±2
0
Œ±
Œ±2
Œ±
0
Œ±
0
1
p5, Œ≥a,p5,u
a/u
0
1
2
3
4
5
6
7
8
9
10
11
12
0
1
Œ±
Œ±2
Œ±2
1
Œ±
Œ±
Œ±2
1
1
Œ±
Œ±2
Œ±2
1
0
1
Œ±2
0
Œ±
0
Œ±
Œ±
1
0
Œ±2
0
Œ±2
p6, Œ≥a,p6,u
a/u
0
1
2
3
4
5
6
7
8
9
10
11
12
0
1
Œ±2
Œ±
Œ±
1
Œ±2
Œ±2
Œ±
1
1
Œ±2
Œ±
Œ±
1
0
1
Œ±
0
Œ±2
0
Œ±2
Œ±2
1
0
Œ±
0
Œ±
p7, Œ≥a,p7,u
a/u
0
1
2
3
4
5
6
7
8
9
10
11
12
0
1
Œ±2
Œ±2
Œ±
Œ±
Œ±
1
1
1
Œ±2
Œ±2
Œ±2
Œ±
1
0
1
Œ±
0
Œ±
0
Œ±2
Œ±
Œ±2
0
Œ±2
0
1
As 1 = 3 = 5 = 0:
Q1 = Q1 = y, and lod(Q1) = 2
Q3 = Q3 = yz, and lod(Q3) = 9
Q5 = Q5 = yz2, and lod(Q5) = 20.
Modify polynomials in the group with j = 0 as:
Q0 = (x ‚àí0)Q = x, and lod(Q0) = 1
Q2 = 0Q2 ‚àí2Q = 1 + z, and lod(Q2) = 4
Q4 = 0Q4 ‚àí4Q = 1 + z2, and lod(Q4) = 12
Q6 = 0Q6 ‚àí6Q = 1 + z3 and lod(Q6) = 24.
For the zero parameter, u = 0 and v = 1.
As there is no polynomial in the group with leading order over C, no polynomial
is eliminated in this iteration.

186
LIST DECODING
Test the zero condition of the remaining polynomials in the group as:
0 = D(p0,r0)
01
(Q0) = 0, 1 = D(p0,r0)
01
(Q1) = 0, 2 = D(p0,r0)
01
(Q2) = 1,
3 = D(p0,r0)
01
(Q3) = 0,
4 = D(p0,r0)
01
(Q4) = 0, 5 = D(p0,r0)
01
(Q5) = 0, 6 = D(p0,r0)
01
(Q6) = 1.
Find the minimal polynomial with j = 0 as:
j = 2
and
Q = Q2.
As 0 = 1 = 3 = 4 =5 = 0:
Q0 = Q0 = x, and lod(Q0) = 1
Q1 = Q1 = y, and lod(Q1) = 2
Q3 = Q3 = yz, and lod(Q3) = 9
Q4 = Q4 = 1 + z2, and lod(Q4) = 12
Q5 = Q5 = yz2 and lod(Q5) = 20.
Modify polynomials in the group with j = 0 as:
Q2 = (x ‚àí0)Q = x + xz and lod(Q2) = 7
Q6 = 2Q6 ‚àí6Q = z + z3 and lod(Q6) = 24.
For zero parameter, u = 1 and v = 0.
As there is no polynomial in the group with leading order over C, no polynomial
is eliminated in this iteration.
Test the zero condition of the remaining polynomials in the group as:
0 = D(p0,r0)
10
(Q0) = 1, 1 = D(p0,r0)
10
(Q1) = 0, 2 = D(p0,r0)
10
(Q2) = 0,
3 = D(p0,r0)
10
(Q3) = 0,
4 = D(p0,r0)
10
(Q4) = 0, 5 = D(p0,r0)
10
(Q5) = 0, 6 = D(p0,r0)
10
(Q6) = 0.
Find the minimal polynomial with j = 0 as:
j = 0
and
Q = Q0.
As 1 = 2 = 3 = 4 =5 = 6 = 0:
Q1 = Q1 = y, and lod(Q1) = 2
Q2 = Q2 = x + xz, and lod(Q2) = 7
Q3 = Q3 = yz, and lod(Q3) = 9
Q4 = Q4 = 1 + z2, and lod(Q4) = 12
Q5 = Q5 = yz2, and lod(Q5) = 20
Q6 = Q6 = z + z3, and lod(Q6) = 24.

GENERAL FACTORIZATION
187
Modify polynomials in the group with j = 0 as:
Q0 = (x ‚àí0)Q = x2, and lod(Q0) = 3.
Following the same process, interpolation runs through the rest of the interpo-
lated units (p1, r1) ‚àº(p7, r7) and with respect to all zero parameters (u, v) = (0,
0), (0, 1) and (1, 0). After C iterations, the chosen interpolated polynomial is: Q(x,
y, z) = Œ±2 + Œ±x + y + Œ±x2 + y2 + Œ±2x2y + Œ±2xy2 + Œ±2y3 + Œ±2x2y2 + z(x + xy +
xy2) + z2(Œ±2 + Œ±x + Œ±2y + Œ±x2), and lod(Q(x, y, z)) = 23. Polynomial Q(x, y, z)
has a zero of multiplicity at least 2 over the eight interpolated units.
5.7 General Factorization
Based on the interpolated polynomial, factorization Ô¨Ånds the polynomial‚Äôs z-roots
in order to determine the output list. Building upon the work of [15, 20], this sec-
tion presents a generalized factorization algorithm, or the so-called recursive coef-
Ô¨Åcient search algorithm, which can be applied to both Reed‚ÄìSolomon codes and
algebraic‚Äìgeometric codes. This section‚Äôs work is based on [21]. In general, the al-
gorithm is described with application to algebraic‚Äìgeometric codes. Therefore, it has
to be stated that when applying this algorithm to Reed‚ÄìSolomon codes, the rational
functions in an algebraic‚Äìgeometric code‚Äôs pole basis are simpliÔ¨Åed to univariate
monomials in the Reed‚ÄìSolomon code‚Äôs pole basis. As a consequence, polynomials
in Fq[x, y] are simpliÔ¨Åed to univariate polynomials with variable x.
Those polynomials h ‚ààFwz
q [x, y] will be in the output list if Q(x, y, h) = 0. The
outcome of the factorization can be written as:
Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥
h1 = h1,0œÜ0 + ¬∑ ¬∑ ¬∑ + h1,k‚àí1œÜk‚àí1
...
hl = hl,0œÜ0 + ¬∑ ¬∑ ¬∑ + hl,k‚àí1œÜk‚àí1
,
(5.92)
with l ‚â§lm. Rational functions œÜ0, . . . , œÜk‚àí1 are predetermined by the decoder;
therefore, Ô¨Ånding the list of polynomials is equivalent to Ô¨Ånding their coefÔ¨Åcients,
h1,0, . . . , h1,k‚àí1, . . . , hl,0, . . . , hl,k‚àí1, respectively. Substituting h into the interpolated
polynomial Q = 
a,b
QabœÜazb, we have:
Q(x, y, h) =

a,b
QabœÜahb =

a,b
QabœÜa(h0œÜ0 + ¬∑ ¬∑ ¬∑ + hk‚àí1œÜk‚àí1)b.
(5.93)
It is important to notice that:
(œÜiœÜ j) mod Œ∂ =

v
œÜv,
(5.94)

188
LIST DECODING
where œá is the algebraic curve (e.g. Hermitian curve Hw) and œÜi, œÜj and œÜv are
rational functions in the pole basis associated with the curve œá (for example the pole
basis Lw associated with curve Hw). Therefore (5.93) can be rewritten as a polynomial
in Fq[x, y]:
Q(x, y, h) =

a
QaœÜa,
(5.95)
where
coefÔ¨Åcients
Qa
are
equations
with
unknowns
h0, . . . ,
hk‚àí1.
If
T = |{Qa|Qa = 0}|, the rational functions œÜa with Qa = 0 can be arranged
as œÜa1 < œÜa2 < ¬∑ ¬∑ ¬∑ < œÜaT and (5.95) can again be rewritten as:
Q(x, y, h) = Qa1œÜa1 + Qa2œÜa2 + ¬∑ ¬∑ ¬∑ + QaT œÜaT .
(5.96)
Again, coefÔ¨Åcients Qa1, Qa2, . . . , and QaT are equations of unknowns h0, . . . , hk‚àí1.
To have Q(x, y, h) = 0, we need Qa1 = Qa2 = ¬∑ ¬∑ ¬∑ = QaT = 0. Therefore, h0, . . . , hk‚àí1
can be determined by solving the following simultaneous set of equations [21]:
Ô£±
Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£≥
Qa1(h0, . . . , hk‚àí1) = 0
Qa2(h0, . . . , hk‚àí1) = 0
...
QaT (h0, . . . , hk‚àí1) = 0
.
(5.97)
In order to solve (5.97), a recursive coefÔ¨Åcient search algorithm is applied to
determine h0, . . . , hk‚àí1 [20, 22]. Here a more general and efÔ¨Åcient factorization
algorithm is presented. Let us denote the following polynomials with respect to a
recursive index s (0 ‚â§s ‚â§k ‚àí1):
h(s)(x, y) = h0œÜ0 + ¬∑ ¬∑ ¬∑ + hk‚àí1‚àísk‚àí1‚àís,
(5.98)
which is a candidate polynomial with coefÔ¨Åcients h0, . . . , hk‚àí1‚àís undetermined. Up-
date Q(x, y, z) recursively as:
Q(s+1)(x, y, z) = Q(s)(x, y, z + hk‚àí1‚àísœÜk‚àí1‚àís),
(5.99)
with Q(0)(x, y, z) = Q(x, y, z), which is the interpolated polynomial (5.68). Substituting
hk‚àí1‚àísœÜk‚àí1‚àís into Q(s)(x, y, z), we have:
ÀúQ(s)(x, y) = Q(s)(x, y, hk‚àí1‚àísœÜk‚àí1‚àís).
(5.100)
ÀúQ(s) mod œá can be transferred into a polynomial in Fq[x, y] with coefÔ¨Åcients
expressed as 
i
ihi
k‚àí1‚àís where  i ‚ààGF(q). Denote ÀúQ(s)‚Äôs leading monomial with
its leading coefÔ¨Åcient as [21]:
œÜ(s)
L = LM( ÀúQ(s))
(5.101)
C(s)
L (hk‚àí1‚àís) = LC( ÀúQ(s)).
(5.102)

GENERAL FACTORIZATION
189
Based on (5.102), it can be seen that LM(h(s)) = œÜk‚àí1‚àís and LC(h(s)) = hk‚àí1‚àís.
Therefore, for any recursive polynomial Q(s)(x, y, z), we have:
LM(Q(s)(x, y, h(s))) = LM(Q(s)(x, y, hk‚àí1‚àísœÜk‚àí1‚àís)) = LM( ÀúQ(s)) = œÜ(s)
L ) (5.103)
LC(Q(s)(x, y, h(s))) = LC(Q(s)(x, y, hk‚àí1‚àís œÜk‚àí1‚àís)) = LC( ÀúQ(s)) = C(s)
L (hk‚àí1‚àís).
(5.104)
As all the candidate outputs should satisfy Q(x, y, h) = 0, and from the above
deÔ¨Ånitions it can be seen that h = h(0) and Q(0)(x, y, z) = Q(x, y, z), Q(x, y, h) = 0
is equivalent to Q(0)(x, y, h(0)) = 0. Based on (5.107) and (5.108), in order to have
Q(0)(x, y, h(0)) = 0 we need to Ô¨Ånd its leading monomial œÜ(0)
L with leading coefÔ¨Åcient
C(0)
L (hk‚àí1) and determine the values of hk‚àí1 that satisfy C(0)
L (hk‚àí1) = 0. As a result,
the leading monomial of Q(0)(x, y, h(0)) is eliminated. Based on each value of hk‚àí1 and
performing the polynomial update (5.99), Q(1)(x, y, z) is generated, in which œÜ(0)
L has
been eliminated. Now, Q(x, y, h) = 0 is equivalent to Q(1)(x, y, h(1)) = 0. Again, to have
Q(1)(x, y, h(1)) = 0, we need C(1)
L (hk‚àí2) = 0. Therefore, hk‚àí2 can be determined by
solving C(1)
L (hk‚àí2) = 0. Based on each value of hk‚àí2, we can trace further to Ô¨Ånd the
rest of the coefÔ¨Åcients. In general, after the coefÔ¨Åcients hk‚àí1‚àís (0 ‚â§s < k ‚àí1) have
been determined by solving C(s)
L (hk‚àí1‚àís) = 0, based on each value of them, perform
the polynomial update (5.99) to generate Q(s+1)(x, y, z). From Q(s+1)(x, y, z), ÀúQ(s+1)
can be calculated and hk‚àí1‚àí(s+1) can be determined by solving C(s+1)
L
(hk‚àí1‚àí(s+1)) = 0.
This process is illustrated in Figure 5.8.
From Figure 5.8 it can be seen that there might be an exponential number of routes
to Ô¨Ånd coefÔ¨Åcients hk‚àí1, . . . , h0. However, not every route will be able to reach h0, as
during the recursive process there may be no solution for C(s)
L (hk‚àí1‚àís) = 0 = 0. If h0
is produced and Q(k‚àí1)(x, y, h0œÜ0) = 0, this route can be traced to Ô¨Ånd the rest of the
Q updates
Q(5)(x, y, z)
hk, 1‚Äì5
hk, 1‚Äì(5+1)
hk, 1‚Äì(5+1)
hk, 1‚Äì(5+1)
hk, 1‚Äì(5+1)
hk, 1‚Äì5
Q(5+1)(x, y, z)
.
.
.
.
.
.
.
.
.
.
.
Figure 5.8
Recursive coefÔ¨Åcient search.

190
LIST DECODING
coefÔ¨Åcients h1, . . . , hk‚àí1 to construct polynomial h which will satisfy Q(x, y, h) = 0.
The correctness of this judgement will be proven later.
Based on the above description, the generalized factorization algorithm is summa-
rized in Algorithm 5.6.
Algorithm 5.6: Recursive coefÔ¨Åcient search [14, 21, 22]
Initialization: Q(0)(x, y,
z) = Q(x, y, z). The recursive index s = 0 and output candidate index l = 1.
Perform: Recursive coefÔ¨Åcient search (s) (RCS(s)).
Recursive coefÔ¨Åcient search (RCS):
Input parameter: s (0 ‚â§s ‚â§k ‚àí1).
1. Perform (5.100) to calculate ÀúQ(s)(x, y).
2. Find out œÜ(s)
L with its coefÔ¨Åcient C(s)
L (hk‚àí1‚àís).
3. Determine hk‚àí1‚àís by solving C(s)
L (hk‚àí1‚àís) = 0.
4. For each value of hk‚àí1‚àís, do
{
5. hl,k‚àí1‚àís = hk‚àí1‚àís.
6. If s = k ‚àí1, calculate Q(k‚àí1)(x, y, h0œÜ0) and go to step 7. Else go to step 8.
7. If Q(k‚àí1)(x, y, h0œÜ0) = 0, trace this route to Ô¨Ånd coefÔ¨Åcients hl,k‚àí1, hl,k‚àí2, . . . , and
hl,0 to construct the candidate polynomial hl and l = l + 1. Else stop this route.
8. Perform polynomial update (5.99) to generate Q(s+1)(x, y, z).
9. Perform RCS(s + 1).
}
If a number of hk‚àí1‚àís have been determined, the algorithm will choose one of them
to determine the rest of the coefÔ¨Åcients until all the possible routes starting from this
hk‚àí1‚àís have been traced. After this, it will choose another value of hk‚àí1‚àís and repeat
the process. This algorithm will terminate after all the possible routes started from
hk‚àí1 have been traced. To prove the correctness of this algorithm, we need to justify
the fact that the polynomial hl produced in step 7 satisÔ¨Åes Q(x, y, hl) = 0.
Proof: As Q(k‚àí1)(x, y, h0œÜ0) = 0 and h(k‚àí1)(x, y) = h0œÜ0, we have Q(k‚àí1)(x, y,
h(k‚àí1)) = 0. Assuming h1 is the previously-found coefÔ¨Åcient, Q(k‚àí1)(x, y, z) is
generated by (5.99): Q(k‚àí1)(x, y, z) = Q(k‚àí2)(x, y, z + h1œÜ1). From Q(k‚àí1)(x, y,
h(k‚àí1)) = 0 we have Q(k‚àí2)(x, y, h(k‚àí1) + h1œÜ1) = 0. Based on (5.98), it can be seen that
h(k‚àí2) = h0œÜ0 + h1œÜ1 = h(k‚àí1) + h1œÜ1. Therefore, Q(k‚àí2)(x, y, h(k‚àí2)) = 0. It can be
deduced further to get Q(k‚àí3)(x, y, h(k‚àí3)) = 0, . . . , and Q(0)(x, y, h(0)) = 0. As
Q(0)(x, y, z) = Q(z) and h(0) = h0œÜ0 + h1œÜ1 + ¬∑ ¬∑ ¬∑ + hk‚àí1œÜk‚àí1, whose coefÔ¨Åcients
have been traced as the coefÔ¨Åcients of the output candidate hl, it can be concluded that
Q(x, y, hl) = 0.
Here two worked examples are given to illustrate the generalized factorization
algorithm applied to both a Hermitian code and a Reed‚ÄìSolomon code.

GENERAL FACTORIZATION
191
Example 5.9: List decoding of a (8, 4, 4) Hermitian code deÔ¨Åned in GF(4)
Given the interpolated polynomial is Q(x, y, z) = Œ±2y + Œ±2x2 + Œ±xy + Œ±2y2 +
Œ±2x2y + x2y2 + xy3 + (Œ±x + Œ±xy + Œ±xy2)z + (x + x2)z2, apply Algorithm 5.6 to
Ô¨Ånd out its z-roots.
Initialization: Q(0)(x, y, z) = Q(x, y, z), s = 0 and l = 1.
RCS(0):
ÀúQ(0)(x, y) = Q(0)(x, y, h3x2) = (Œ±2 + Œ±h3)y + Œ±2x2 + Œ±xy + (Œ±2 + h2
3)y2 + (Œ±2
+ h2
3)x2y + (1 + h2
3)x2y2 + xy3 + (Œ±h3 + h2
3)y4, with œÜ(0)
L = y4 and C(0)
L (h3) = Œ±h3
+ h2
3. Solving C(0)
L (h3) = 0, we have h3 = 0 or h3 = Œ±.
For h3 = 0, h1,3 = h3 = 0. As s = 0 < 3, update Q(1)(x, y, z) = Q(0)(x, y, z +
0x2) = Q(x, y, z), and perform RCS(1). . .
Based on the same progress, the outcomes from RCS(1), RCS(2) and RCS(3) are
summarized in Table 5.5.
Table 5.5
Recursive coefÔ¨Åcient search from h3 = 0.
RCS(s)
œÜ(s)
L
C(s)
L (hk‚àí1‚àís)
h2,k‚àí1‚àís = hk‚àí1‚àís
RCS(1)
xy3
1 + Œ±h2
Œ±2
RCS(2)
x2y2
Œ±2 + Œ±h1
Œ±
RCS(3)
xy2
Œ±h0
0
After RCS(2) we have Q(3)(x, y, z) = (Œ±x + Œ±xy + Œ±xy2)z + (x + x2)z2. In
RCS(3), by solving C(3)
L (h0) = 0, we have h0 = 0. Therefore, h1,0 = h0 = 0. As
s = 3 and Q(3)(x, y, h0œÜ0) = Q(3)(x, y, 0¬∑1) = 0, this route can be traced to construct
candidate polynomial h1 = Œ±x + Œ±2y and update the candidate index l = l +
1 = 2.
Going back to the closest division point (when s = 0), we have:
For h3 = Œ±, h2,3 = h3 = Œ±. As s = 0 < 3, update Q(1)(x, y, z) = Q(0)(x, y, z + Œ±x2)
= Œ±2x2 + Œ±xy + Œ±x2y2 + xy3 + (Œ±x + Œ±xy + Œ±xy2)z + (x + x2)z2 and perform
RCS(1). . .
Again, the outcomes of RCS(1), RCS(2) and RCS(3) are summarized in Table
5.6.
Table 5.6
Recursive coefÔ¨Åcient search from h3 = Œ±.
RCS(s)
œÜ(s)
L
C(s)
L (hk‚àí1‚àís)
h2,k‚àí1‚àís = hk‚àí1‚àís
RCS(1)
xy3
1 + Œ±h2
Œ±2
RCS(2)
x2y2
Œ±h1
0
RCS(3)
xy2
Œ±2 + Œ±h0
Œ±
After RCS(2) we have Q(3)(x, y, z) = Œ±2x2 + Œ±2xy + Œ±2xy2 + (Œ±x + Œ±xy +
Œ±xy2)z + (x + x2)z2. In RCS(3), by solving C(3)
L (h0) = 0, we have h2,0 = h0 = Œ±.

192
LIST DECODING
As s = 3 and Q(3)(x, y, h0œÜ0) = Q(3)(x, y, Œ±¬∑1) = 0, this route can be traced to
construct the candidate polynomial h2 = Œ± + Œ±2y + Œ±x2. As all the possible routes
from h0 have been traced, the factorization process terminates and outputs: h1 =
Œ±x + Œ±2y, h2 = Œ± + Œ±2y + Œ±x2.
Example 5.10: List decoding of a (7, 2, 6) Reed‚ÄìSolomon code deÔ¨Åned in GF(8)
Œ± is a primitive element in GF(8) satisfying Œ±3 + Œ± + 1 = 0.
Given the interpolated polynomial Q(x, z) = Œ±x + Œ±6x2 + (Œ±3 + Œ±3x)z + Œ±2z2,
apply Algorithm 5.6 to determine its z-roots.
Initialization: Q(0)(x, z) = Q(x, z), s = 0 and l = 1.
RCS(0):
ÀúQ(0)(x, z) = Q(0)(x, h1x) = (Œ± + Œ±3h1)x + (Œ±6 + Œ±3h1 + Œ±2h2
1)x2, with œÜ(0)
L = x2
and C(0)
L (h1) = Œ±6 + Œ±3h1 + Œ±2h2
1. Solving C(0)
L (h1) = 0, we have h1 = Œ±5 or h1 =
Œ±6.
For h1 = Œ±5, h1,1 = h1 = Œ±5. As s = 0 < 1, update Q(1)(x, z) = Q(0)(x, z + Œ±5x)
= (Œ±3 + Œ±3x)z + Œ±2z2 and perform RCS(1).
In RCS(1), following the same progress, we have œÜ(1)
L = x and C(1)
L (h0) = Œ±3h0.
Solving C(1)
L (h0) = 0, we have h0 = 0.
For h0 = 0, h1,0 = h0 = 0. As s = 1 and Q(1)(x, h0œÜ0) = Q(1)(x, 0¬∑1) = 0, this
route can be traced to construct candidate polynomial h1 = Œ±5x. Update the output
candidate index as l = l + 1 = 2.
Going back to the closest division point (when s = 0), we have:
For h1 = Œ±6, h2,1 = h1 = Œ±6. As s = 0 < 1, update Q(1)(x, z) = Q(0)(x, z + Œ±6x)
= Œ±4x + (Œ±3 + Œ±3x)z + Œ±2z2, and perform RCS(1).
In RCS(1) we have œÜ(1)
L = x and C(1)
L (h0) = Œ±4 + Œ±3h0. Solving C(1)
L (h0) = 0,
we have h0 = Œ±.
For h0 = Œ±, h2,0 = h0 = Œ±. As s = 1 and Q(1)(x, h0œÜ0) = Q(1)(x, Œ±¬∑1) = 0, this
route can be traced to construct candidate polynomial h2 = Œ± + Œ±6x. As all the
possible routes from h0 have been traced, the factorization process terminates and
outputs: h1 = Œ±5x, h2 = Œ± + Œ±6x.
5.8 Soft-Decision List Decoding of Hermitian Codes
We can extend the hard-decision GS decoding algorithm for AG codes to soft-decision
by extending the Kotter‚ÄìVardy algorithm. As with Reed‚ÄìSolomon codes, soft-decision
list decoding is almost the same as hard-decision list decoding, with the addition of
a process to convert the reliability values of the received symbols into a multiplicity
matrix. In this section, a comparison of the code word scores is made between soft-
and hard-decision list decoding and conditions are derived to ensure that the equations
from the interpolation process are solvable.

SOFT-DECISION LIST DECODING OF HERMITIAN CODES
193
Based on wz, the two parameters Ô¨Årst deÔ¨Åned by (5.50) and (5.51) for analyzing
bivariate monomial xayb (a, b ‚ààN) can be extended to trivariate monomials œÜazb (a,
b ‚ààN):
N1,wz(d) = |{œÜazb : a, b ‚â•0 and deg1,wz(œÜazb) ‚â§Œ¥, Œ¥ ‚ààN}|,
(5.105)
which represents the number of monomials with (1, wz)-weighted degree not greater
than Œ¥, and:
1,wz(v) = min {Œ¥ : N1,wz(Œ¥) > v, v ‚ààN},
(5.106)
which represents the minimal value of Œ¥ that guarantees N1,wz(Œ¥) is greater than v.
The difference between soft-decision list decoding of Reed‚ÄìSolomon codes
and of Hermitian codes is in the size of the reliability matrix. For soft-
decision list decoding of Reed‚ÄìSolomon codes, the reliability matrix  has size
q √ó n, where n = q ‚àí1. For soft-decision list decoding of Hermitian codes, the
reliability matrix  has size q √ó n, where n = q3/2.
5.8.1 System Solution
The reliability matrix  is then converted to the multiplicity matrix M, for which
Algorithm 5.3 is applied. A version of Algorithm 5.3 introducing a stopping rule
based on the designed length of output list is presented later in this subsection, as
Algorithm 5.7.
In this subsection, the code word score with respect to matrix M is analyzed so
as to present the system solution for this soft-decision list decoder. It is shown that
the soft-decision scheme provides a higher code word score than the hard-decision
scheme.
The resulting multiplicity matrix M can be written as:
M =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
m0,0
m0,1
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
m0,n‚àí1
m1,0
m1,1
m1,n‚àí1
...
...
...
...
mi, j
...
...
...
...
mq‚àí1,0
mq‚àí1,1
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
mq‚àí1,n‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
(5.107)
where entry mi,j represents the multiplicity for unit (pj, œÅi). Interpolation builds the
minimal polynomial QM ‚ààFq[x, y, z], which has a zero of multiplicity at least mi,j

194
LIST DECODING
(mi,j = 0) over all the associated units (pj, œÅi). Following on from (5.62), with respect
to interpolated unit (pj, œÅi), QM‚Äôs coefÔ¨Åcients Qab should satisfy:

a,b‚â•v
Qab
b
v

Œ≥a,p j,uœÅb‚àív
i
= 0, ‚àÄu, v ‚ààN and u + v < mi, j.
(5.108)
For this soft-decision interpolation, the number of interpolated units covered by
QM is:
|{mi, j = 0|mi, j ‚ààM, i = 0, 1, . . . , q ‚àí1 and j = 0, 1, . . . , n ‚àí1}|
(5.109)
and the cost CM of multiplicity matrix M is:
CM = 1
2
q‚àí1

i=0
n‚àí1

j=0
mi, j(mi, j + 1),
(5.110)
which represents the number of constraints in (5.108) to QM‚Äôs coefÔ¨Åcients Qab. These
can be imposed by the iterative polynomial construction algorithm [9, 11, 17] in CM
iterations. Notice that (5.109)) and (5.110)) have the same expression as (5.45) and
(5.46), respectively, with the exception that n = q3/2.
Based on Lemma 5.1, the following units‚Äô multiplicities will contribute to the code
word score: (p0, c0), (p1, c1), . . . , and (pn‚àí1, cn‚àí1). Referring to the multiplicity matrix
in (5.107), the interpolated polynomial QM can be explained as passing through these
units with multiplicity at least m0 = mi,0 (œÅi = c0), m1 = mi,1 (œÅi = c1), . . . , and
mn‚àí1 = mi,n‚àí1 (œÅi = cn‚àí1), respectively. If f ‚ààFwz
q [x, y] is the transmitted message
polynomial such that f(pi) = ci, the total zero order of QM(x, y, f) over units {(p0, c0),
(p1, c1), . . . , (pn‚àí1, cn‚àí1)} is at least:
m0 + m1 + ¬∑ ¬∑ ¬∑ + mn‚àí1 =
n‚àí1

j=0
{mi, j|œÅi = c j, i = 0, 1, . . . , q ‚àí1}.
(5.111)
Therefore, the code word score SM(c) with respect to multiplicity matrix M is:
SM (c) =
n‚àí1

j=0
{mi, j|œÅi = c j, i = 0, 1, . . . , q ‚àí1}.
(5.112)
If
SM (c) > deg1,wz(QM(x, y, z))
then
n‚àí1
i=0 v pi(QM(x, y, f )) > v p‚àû(QM
(x, y, f )‚àí1) and QM(x, y, f) = 0. f can be found by determining QM(x, y, z)‚Äôs z-roots.
It results in the following corollary for successful soft-decision list decoding:

SOFT-DECISION LIST DECODING OF HERMITIAN CODES
195
Corollary 5.3
If the code word score with respect to multiplicity matrix M is greater
than the interpolated polynomial QM‚Äôs (1, wz)-weighted degree [23]:
SM (c) > deg1,wz(QM(x, y, z))
(5.113)
then QM(x, y, f) = 0 or z ‚àíf |QM(x, y, z).
To compare the soft-decision‚Äôs code word score SM (c) with the hard-decision‚Äôs
code word score SM (c), denote the index of the maximal element in each column of
 as:
i j = index (max{œÄi, j|i = 0, 1, . . . , q ‚àí1}),
(5.114)
such that œÄi j, j > œÄi, j(i = i j). The hard-decision received word R can be written as:
R = (r0,r1, . . . ,rn‚àí1) = (œÅi0, œÅi1, . . . , œÅin‚àí1).
(5.115)
For hard-decision list decoding, only those entries in the multiplicity matrix from
(5.107) that correspond to the reliability value œÄi j, j will be assigned a multiplicity, as
mi j, j = m, and therefore the score for hard-decision can also be written with respect
to multiplicity matrix M as:
Sm (c) = SM (c) =
n‚àí1

j=0
{mi j, j|œÅi j = c j}.
(5.116)
Comparing (5.112) and (5.116), the soft-decision list decoder gains its improve-
ments by increasing its code word score. This is done by increasing the total number
of interpolated units (5.111) so that the possibility of covering more interpolated units,
including the corresponding code word symbols, is also increased.
If the (1, wz)-weighted degree of interpolated polynomial QM is Œ¥*, based on (5.106),
QM has at most N1,wz(Œ¥*) nonzero coefÔ¨Åcients. The interpolation procedure generates
a system of CM linear equations of type (5.108). The system will be solvable if [5]:
N1,wz(Œ¥‚àó) > CM.
(5.117)
Based on (5.106), in order to guarantee the solution, the (1, wz)-weighted degree
Œ¥* of the interpolated polynomial QM should be large enough that:
deg1,wz(QM(x, y, z)) = d‚àó= 1,wz(CM).
(5.118)
Therefore, based on Corollary 5.3, given the soft-decision code word score (5.116)
and the (1, wz)-weighted degree of the interpolated polynomial QM (5.118), the mes-
sage polynomial f can be found if [23]:
SM (c) > 1,wz(CM).
(5.119)

196
LIST DECODING
The factorization output list contains the z-roots of polynomial QM. Therefore,
the maximal length of output list lM should be equal to polynomial QM‚Äôs z-degree
(degzQM), as:
lM = degz(QM(x, y, z)) =
&deg1,wz(QM(x, y, z))
wz
'
=
&1,wz(CM)
wz
'
.
(5.120)
When converting matrix  to matrix M, based on a designed length of output list
l, Algorithm 5.1 will stop once lM is greater than l. 1,wz(CM) can be determined by
Ô¨Ånding the monomial with (1, wz)-lexicographic order CM, as:
1,wz(CM) = deg1,wz(œÜazb|ord(œÜazb) = CM).
(5.121)
Therefore, in order to assess the soft-decision list decoding algorithm‚Äôs performance
with a designed length of output list l, a large enough value must be set for s when
initializing Algorithm 5.3. In the algorithm, after step 5, we can determine the cost
CM (5.110) of the updated matrix M and apply (5.121) to determine 1,wz(CM). The
maximal length of output list lM can then be determined by (5.120). Stop Algorithm
5.3 once lM is greater than l and output the updated matrix M.
Algorithm 5.7 is based on Algorithm 5.3 and includes this stopping rule. This
algorithm is used to obtain the simulation results shown later.
Algorithm 5.7: Convert reliability matrix  to multiplicity matrix M [14, 23]
Input: Reliability matrix , a high enough desired value of the sum of multiplicities
in matrix M:
s =
q‚àí1

i=0
n‚àí1

j=0
mi, j, and designed output length l.
Initialization: Set * =  and q √ó n all-zero multiplicity matrix M.
1. While (s > 0 or lM < l)
{
2. Find the maximal entry œÄ‚àó
i, j in * with position (i, j).
3. Update œÄ‚àó
i, j in * as œÄ‚àó
i, j =
œÄi, j
mi, j+2.
4. Update mi, j in M as mi, j = mi, j + 1.
5. s = s ‚àí1.
6. For the updated M, calculate its interpolation cost CM by (5.110).
7. Determine 1,wz(CM) by (5.121).
8. Calculate lM by (5.120).
}
Again, this algorithm gives priority to those interpolated points which correspond
to a higher reliability value œÄi,j, to be assigned with a higher multiplicity value mi, j.
For example, if œÄi1 j1 < œÄi2 j2 then mi1 j1 ‚â§mi2 j2.
The interpolation and factorization methods given in Algorithms 5.5 and 5.6 are
unchanged for soft-decision list decoding of AG codes. Again, as the total number of

SOFT-DECISION LIST DECODING OF HERMITIAN CODES
197
1.0E‚Äì06
1.0E‚Äì05
1.0E‚Äì04
1.0E‚Äì03
1.0E‚Äì02
1.0E‚Äì01
1.0E+00
10
9
8
7
6
5
4
3
2
1
0
Eb/N0 [dB]
BER
Uncoded
Sakata
list hard (l=1, m=1)
list hard (l=2, m=2)
list hard (Optimal)
list soft (l=1)
list soft (l=2)
list soft (l=5)
list soft (l=10)
list soft (l=20)
list soft (l=30)
list soft (Optimal)
(a) over AWGN channel 
1.0E‚Äì06
1.0E‚Äì05
1.0E‚Äì04
1.0E‚Äì03
1.0E‚Äì02
1.0E‚Äì01
1.0E+00
10
9
8
7
6
5
4
3
2
1
0
Eb/N0 [dB]
BER
uncoded
Sakata
list hard (l=1, m=1)
list hard (l=2, m=2)
list hard (Optimal)
list soft (l=1)
list soft (l=2)
list soft (l=5)
list soft (l=10)
list soft (l=20)
list soft (l=30)
list soft (Optimal)
(b) over Rayleigh fading channel 
Figure 5.9
Soft-decision list decoding of Hermitian code (64, 39, 20).

198
LIST DECODING
1.0E‚Äì06
1.0E‚Äì05
1.0E‚Äì04
1.0E‚Äì03
1.0E‚Äì02
1.0E‚Äì01
1.0E+00
10
9
8
7
6
5
4
3
2
1
0
Eb/N0 [dB]
BER
Uncoded
Sakata
list hard (l=1, m=1)
list hard (l=2, m=2)
list hard (Optimal)
list soft (l=1)
list soft (l=2)
list soft (l=5)
list soft (l=10)
list soft (l=20)
list soft (l=30)
list soft (Optimal)
(a) over AWGN channel 
1.0E‚Äì06
1.0E‚Äì05
1.0E‚Äì04
1.0E‚Äì03
1.0E‚Äì02
1.0E‚Äì01
1.0E+00
10
9
8
7
6
5
4
3
2
1
0
Eb/N0 [dB]
BER
uncoded
Sakata
list hard (l=1, m=1)
list hard (l=2, m=2)
list hard (Optimal)
list soft (l=1)
list soft (l=2)
list soft (l=5)
list soft (l=10)
list soft (l=20)
list soft (l=30)
list soft (Optimal)
(b) over Rayleigh fading channel 
Figure 5.10
Soft-decision list decoding of Hermitian code (512, 289, 196).

CONCLUSIONS
199
iterations CM has been determined before running the interpolation, those polynomials
with leading order greater than CM can be eliminated from the group during the
iterations [23].
5.8.2 Simulation Results
The performance of hard- and soft-decision list decoding for Hermitian codes is
evaluated by simulation results on the AWGN channel and a frequency nonselective
Rayleigh fading channel with Doppler frequency 126.67 Hz. The fading proÔ¨Åle is
generated using Jakes‚Äô method [16] and the fading coefÔ¨Åcients have a mean value of
1.55 and variance 0.60. During simulation, quasi-static fading is assumed in which
the fading amplitude changes for each code word block. For combating the fading
effect, 64 √ó 64 and 100 √ó 512 block interleavers are employed for codes deÔ¨Åned in
GF(16) and GF(64) respectively.
In Figure 5.9a the soft-decision list decoding performance of the (64, 39, 20)
Hermitian code over GF(16) is shown on the AWGN channel for different list lengths.
This is compared with the hard-decision list decoding performance with multiplicity
values m = 1 and 2, and the performance of the Sakata algorithm is included too. We
can see that there are small coding gains for soft-decision list decoding over hard-
decision list decoding up to approximately 1.3 dB at a BER of 10‚àí4. In Figure 5.9b,
more signiÔ¨Åcant coding gains are achieved on the fading channel, with a coding gain
greater than 3 dB over hard-decision list decoding.
In Figure 5.10a the soft-decision list decoding performance of the (512, 289, 196)
Hermitian code deÔ¨Åned over GF(64) is shown on the AWGN channel for different
output list lengths. Again, small coding gains are achieved over hard-decision decod-
ing. In Figure 5.10b the soft-decision list decoding performance on the fading channel
is shown, with more signiÔ¨Åcant coding gains over hard-decision decoding.
5.9 Conclusions
In this chapter we have introduced the concept of list decoding for Reed‚ÄìSolomon
and AG codes. We can see that list decoding allows more errors to be corrected than
with conventional algebraic decoding algorithms, but at a cost of higher complexity.
However, a method to reduce this complexity without degrading the performance is
given for both Reed‚ÄìSolomon and AG codes. The Kotter‚ÄìVardy algorithm for the soft-
decision list decoding of Reed‚ÄìSolomon codes was presented; this is almost identical
to the Guruswami‚ÄìSudan algorithm but with an extra process to convert the reliability
of the received symbols into a multiplicity matrix. The Kotter‚ÄìVardy algorithm was
also extended to soft decode AG codes. One interesting observation from the soft-
decision list decoding algorithm was that it could achieve the same performance as the
hard-decision list decoder but at a lower complexity. More signiÔ¨Åcant coding gains can
be achieved on a slow fading channel, particularly for AG codes, suggesting that soft-
decision list decoding is very suitable for channels where bursts of errors are common.

200
LIST DECODING
References
[1] Elias, P. (1957) List Decoding for Noisy Channels, Res. Lab. Electron, MIT, Cambridge, MA.
[2] Elias, P. (1991) Error-correcting codes for list decoding. Information Theory, IEEE Transactions,
37, 5‚Äì12.
[3] Wozencraft, J.M. (1958) List Decoding, Res. Lab. Electron, MIT, Cambridge, MA.
[4] McEliece, R.J. (2003) The Guruswami‚ÄìSudan Decoding Algorithm for Reed‚ÄìSolomon Codes,
California Institute. Tech, Pasadena, California, IPN Progress Rep, pp. 42‚Äì153.
[5] Koetter, R. and Vardy, A. (2003) Algebraic soft-decision decoding of Reed‚ÄìSolomon codes. IEEE
Trans. Inform. Theory, 49, 2809‚Äì25.
[6] Guruswami,
V.
and
Sudan,
M.
(1999)
Improved
decoding
of
Reed‚ÄìSolomon
and
algebraic‚Äìgeometric codes. IEEE Trans. Inform. Theory, 45, 1757‚Äì67.
[7] Hasse, H. (1936) Theorie der hoheren differentiale in einem algebraishen funcktionenkorper mit
vollkommenem konstantenkorper nei beliebeger charakteristic. J. Reine. Aug. Math, 175, 50‚Äì4.
[8] Koetter, R. (1996) On algebraic decoding of algebraic‚Äìgeometric and cyclic codes. Linkoping,
Sweden: University Linkoping.
[9] Nielsen, R.R. (2001) List decoding of linear block codes. Lyngby, Denmark: Tech. Univ. Denmark.
[10] Moon, T.K. (2005) Error Correction Coding ‚Äì Mathematical and Algorithms, Wiley Interscience.
[11] H√∏holdt, T. and Nielsen, R.R. (1999) Decoding Hermitian codes with Sudan‚Äôs algorithm, in Applied
Algebra, Algebraic Algorithms and Error-Correcting Codes (eds H.I.N. Fossorier, S. Lin, and A.
Pole), (Lecture Notes in Computer Science), Vol. 1719, Springer-Verlag, Berlin, Germany, pp.
260‚Äì70.
[12] Feng, G.-L. and Tzeng, K.K. (1991) A generalization of the Berlekamp-Massey algorithm for
multisequence shift-register synthesis with application to decoding cyclic codes. IEEE Trans.
Inform. Theory, 37, 1274‚Äì87.
[13] Chen, L., Carrasco, R.A. and Chester, E.G. (2007) Performance of Reed‚ÄìSolomon codes using the
Guruswami‚ÄìSudan algorithm with improved interpolation efÔ¨Åciency. IET Commun, 1, 241‚Äì50.
[14] Chen, L. (2007) Design of an efÔ¨Åcient list decoding system for Reed‚ÄìSolomon and
algebraic‚Äìgeometric codes, PhD Thesis School of electrical, electronic and computer engineer-
ing. Newcastle-upon-Tyne: Newcastle University.
[15] Roth, R. and Ruckenstein, G. (2000) EfÔ¨Åcient decoding of Reed‚ÄìSolomon codes beyond half the
minimum distance. IEEE Trans. Inform. Theory, 46, 246‚Äì57.
[16] Proakis, J.G. (2000) Digital Communications, 4th edn, McGraw-Hill International.
[17] Chen, L., Carrasco, R.A. and Johnston, M. (2006) List decoding performance of algebraic geometric
codes. IET Electronic Letters, 42, 986‚Äì7.
[18] Chen, L., Carrasco, R.A. and Johnston, M. (XXXX) Reduced complexity interpolation for list
decoding Hermitian codes. IEEE Trans. Wireless Commun, Accepted for publication.
[19] Chen,L. andCarrasco,R.A. (2007) EfÔ¨Åcient list decoder for algebraic‚Äìgeometriccodes. Presented at
9th International Symposium on Communication Theory and Application (ISCTA‚Äô07), Ambleside,
Lake district, UK.
[20] Wu, X.-W. and Siegel, P. (2001) EfÔ¨Åcient root-Ô¨Ånding algorithm with application to list decoding
of algebraic‚Äìgeometric codes. IEEE Trans. Inform. Theory, 47, 2579‚Äì87.
[21] Chen, L., Carrasco, R.A., Johnston, M. and Chester, E.G. (2007) EfÔ¨Åcient factorisation algorithm
for list decoding algebraic‚Äìgeometric and Reed‚ÄìSolomon codes. Presented at ICC 2007, Glasgow,
UK.
[22] Wu, X.-W. (2002) An algorithm for Ô¨Ånding the roots of the polynomials over order domains.
Presented at ISIT 2002, Lausanne, Switzerland.
[23] Chen, L., Carrasco, R.A. and Johnston, M. (XXXX) Soft-decision list decoding of Hermitian codes.
IEEE Trans. Commun, Submitted for publication.

6
Non-Binary Low-Density Parity
Check Codes
6.1 Introduction
The previous chapters have described non-binary block codes that have algebraic
decoders, but now a new type of block code is introduced that is decoded using a
graph. This is the well-known low-density parity check (LDPC) code, which was Ô¨Årst
presented by Gallager in 1962 [1] but only became popular when Mackay rediscovered
it in 1995 [2]. It has been shown that with iterative soft-decision decoding LDPC codes
can perform as well as or even better than turbo codes [3]. It is claimed that LDPC
codes will be chosen in future standards, such as 4G, later versions of WiMax and
magnetic storage devices.
In this chapter, binary LDPC codes are Ô¨Årst introduced with a discussion on ran-
dom and structured construction methods. The Belief Propagation algorithm is then
presented in detail to decode LDPC codes. We then expand binary LDPC codes to
non-binary LDPC codes deÔ¨Åned over a Ô¨Ånite Ô¨Åeld GF(q) and again discuss different
construction methods. Finally the Belief Propagation algorithm is extended to non-
binary symbols, which increases its complexity, but a method to reduce complexity
is introduced using fast Fourier transforms (FFT), allowing us to decode non-binary
LDPC codes deÔ¨Åned over large Ô¨Ånite Ô¨Åelds.
6.2 Construction of Binary LDPC Codes ‚Äì Random
and Structured Methods
A low-density parity check (LDPC) code is characterized by its sparse parity check
matrix, that is a matrix containing mostly zero elements and few nonzero elements.
Three important parameters are its code word length, n, its dimension, k, and its
number of parity bits, m = n ‚àík. The number of nonzero elements in a row of the
Non-Binary Error Control Coding for Wireless Communication and Data Storage
Rolando Antonio Carrasco and Martin Johnston
C 2008 John Wiley & Sons, Ltd

202
NON-BINARY LOW-DENSITY PARITY CHECK CODES
parity check matrix is called the row weight, denoted by œÅ. The number of nonzero
elements in a column of the parity check matrix is called the column weight, denoted
by Œ≥ . There are two general classes of LDPC code:
 If the row weights and column weights are constant for each row and column in the
parity check matrix then the LDPC code is said to be regular.
 If the row and column weights vary for each row and column in the parity check
matrix then the LDPC code is said to be irregular.
In general, irregular LDPC codes have a better performance than regular LDPC
codes. An example of a parity check matrix H for a small binary LDPC code is given
in (6.1):
H =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
1
0
1
0
0
0
1
1
0
1
0
1
0
0
0
1
1
0
0
1
1
0
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
(6.1)
We can see that H has a constant row weight œÅ = 3 and constant column weight Œ≥ =
2, implying that this is a regular LDPC code. Originally, LDPC codes were constructed
by Ô¨Årst choosing the row and column weights and then randomly determining where
the nonzero elements were located.
6.2.1 Tanner Graph Representation
A parity check matrix can be expressed in the form of a bipartite graph known as a
Tanner graph. Each row in the parity check matrix represents a parity check equation
zi, 1 ‚â§i ‚â§m, and each column represents a coded bit cj, 1 ‚â§j ‚â§n. For example,
the Ô¨Årst row of H in (6.1) is the parity check equation z1 = c1 ‚äïc2 ‚äïc4. The Tanner
graph connects the coded bits to each parity check equation. For each row of the parity
check matrix, if the coded bit is a 1 then there is a connection between that coded bit
and the corresponding parity check equation. The Tanner graph for H in (6.1) is given
in Figure 6.1.
The Tanner graph reveals the presence of cycles, that is a path starting and ending at
the same coded bit. In Figure 6.1 it can be seen that the smallest cycle has a length of
6. One such cycle is c1 ‚Üíz1‚Üíc4 ‚Üíz4 ‚Üíc6 ‚Üíz3 ‚Üíc1. The length of the smallest
cycle in a Tanner graph is known as its girth. It is desirable to avoid LDPC codes
with Tanner graphs with a girth of 4 as this degrades the performance, particularly for
short LDPC codes. From the Tanner graph, we introduce two sets:
 Mn is the set of indices of the parity checks connected to coded bit cn.
 Nm is the set of indices the coded bits that are connected to parity check zm.

CONSTRUCTION OF BINARY LDPC CODES
203
c1
c2
c3
c4
c5
c6
z1
z2
z3
z4
Figure 6.1
Tanner graph for the parity check matrix of (6.1).
So from the Tanner graph of Figure 6.1, the set M1 = {1, 3}, since parity checks
z1 and z3 are connected to coded bit c1. The set N1 = {1, 2, 4}, since coded bits
c1, c2 and c4 are connected to parity check z1. We also introduce the notation Mn/m,
which is the set Mn excluding the parity check zm that is connected to the coded
bit cn. Similarly, Nm/n is the set Nm excluding the coded bit cn that is connected
to the parity check zm. Therefore, M1/1 = {3} and N1/1 = {2, 4}. This nota-
tion will be useful when describing the Belief Propagation algorithm for decoding
LDPC codes.
6.2.2 Structured Construction Methods
There are many different methods for constructing a binary LDPC code [5‚Äì6], but in
this chapter we will discuss just one. This method is known as the Balanced Incomplete
Block Design (BIBD), which can be used to construct high-rate LDPC codes [4]. Let
X = {x1, x2, . . ., xŒΩ} be a set of v objects. A BIBD of X is a collection of n Œ≥ -subsets
of X, denoted by B1, B2, . . ., Bn, called blocks, such that the following conditions are
satisÔ¨Åed:
1. Each object appears in exactly œÅ of the n blocks.
2. Every two objects appear together in exactly Œª of the n blocks.
3. The number Œ≥ of objects in each block is small compared to the total number of
objects in X.
Instead of a list of the blocks, a BIBD can be efÔ¨Åciently described by a v √ó n matrix
Q over GF(2), as follows:
1. The rows of Q correspond to the v objects in X.
2. The columns of Q correspond to the n blocks of the design.

204
NON-BINARY LOW-DENSITY PARITY CHECK CODES
3. Each element Qi,j in Q is a 1 if and only if the ith object xi is contained in the jth
block Bj of the design; otherwise it is 0. This matrix is called the incidence matrix
of the design.
It follows from the structural properties of a BIBD that the incidence matrix Q has
constant row weight œÅ and constant column weight Œ≥ and any two rows of Q have
exactly Œª ‚Äò1-components‚Äô in common.
Example 6.1: Construction of an incidence matrix using BIBD: Let X = {x1,
x2, x3, x4, x5, x6, x7} be a set of seven objects. The blocks {x1, x2, x4}, {x2, x3,
x5}, {x3, x4, x6}, {x4, x5, x7}, {x5, x6, x1}, {x6, x7, x2},{x7, x1, x3} form a BIBD
for the set X. Every block consists of three objects, each object appears in three
blocks, and every two objects appear together in exactly one block, that is Œª = 1.
The incidence matrix Q is given below:
Q =
Ô£´
Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠
1
0
0
0
1
0
1
1
1
0
0
0
1
0
0
1
1
0
0
0
1
1
0
1
1
0
0
0
0
1
0
1
0
0
0
0
1
0
1
1
0
0
0
0
1
0
1
1
Ô£∂
Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
.
Note that each row of Q is a cyclic right-shift of the row above it and the Ô¨Årst
row is the cyclic shift of the last row. We also note that each column is a downward
cyclic shift of the column on its left and the Ô¨Årst column is the downward cyclic
shift of the last column. Therefore Q is a 7 √ó 7 square circulant matrix. Therefore,
the null space of Q gives a (Œ≥ , œÅ)-regular LDPC code, called a BIBD-LDPC code,
of length n. The Tanner graph of a BIBD-LDPC code is free of cycles of length 4
and hence its girth is at least 6. In fact, the second property of a BIBD with Œª =
1 ensures that the girth of the code‚Äôs Tanner graph is exactly 6. If Œª = 2 then the
incidence matrix will have a Tanner graph full of short cycles of length 4, which is
undesirable.
6.2.3 LDPC Codes from Class 1 Bose BIBD
There are four classes of Bose BIBD, each with different types producing many differ-
ent binary LDPC codes. However, in this book we only deal with LDPC constructed
from Class 1 Bose BIBDs of type 1 [4]. Choose a Ô¨Ånite Ô¨Åeld GF(2t + 1), where t is
selected so that GF(2t + 1) is prime, with a primate element x that satisÔ¨Åes x4t ‚àí1 =
xc, where c is an odd integer less than 2t + 1. The resulting LDPC code parameters

CONSTRUCTION OF BINARY LDPC CODES
205
will have the following values: n = t(12t + 1), v = 12t + 1, Œ≥ = 4, œÅ = 4t, Œª = 1.
The base blocks of this BIBD are:

0, x2i, x2i+4t, x2i+8t
,
(6.2)
for 0 ‚â§i < t. From each base block Bi, we can form 12t + 1 blocks by adding each
element of the Ô¨Åeld GF(12t + 1) in turn to the elements in Bi. This results in t(12t +
1) blocks. The incidence matrix Q of this BIBD is a (12t +1) √ó t(12t +1) matrix.
It can be written in cyclic form, which consists of t (12t + 1) √ó (12t + 1) circulant
submatrices in a row, as shown below:
Q =
Q1 Q2 ¬∑ ¬∑ ¬∑ Qt

,
(6.3)
where the ith circulant Qi is the incidence matrix formed by adding each element in
GF(12t + 1) in turn to the elements of the ith base block Bi. A circulant is a square
matrix which has the following structure:
1. Each row is a right-cyclic shift of the row above it and the Ô¨Årst row is the right-cyclic
shift of the last row.
2. Each column is a downward cyclic shift of the column on its left and the Ô¨Årst
column is the downward cyclic shift of the last column. The set of columns is the
same as the set of rows, except that each row reads from right to left.
Both the row and the column weight of each circulant Qi are 4. Therefore, the row
and column weights of Q are 4t and 4 respectively. The rank of Q (or a circulant Qi)
is observed to be 12t.
Example 6.2: Construction of an LDPC code using a class 1 Bose BIBD: Let
t = 1. Then 12t + 1 = 13. The element 6 of GF(13) = {0, 1, 2, . . ., 12} is a
primitive element so with c = 3 we can easily check that x4 ‚àí1 = 64 ‚àí1 = 9 ‚àí1
= 8 = 63, which is less than 13. Therefore, there is a BIBD with parameters, v =
13, n = 13, Œ≥ = 4, œÅ = 4 and Œª = 1. The base block for this design is {0, 60, 64,
68} = {0, 1, 9, 3}. By adding each element in GF(13) to the base block we get the
following 13 blocks:
B0 = {0, 1, 9, 3}, B1 = {1, 2, 10, 4}, B2 = {2, 3, 11, 5}, B3 = {3, 4, 12, 6}, B4
= {4, 5, 0, 7},
B5 = {5, 6, 1, 8}, B6 = {6, 7, 2, 9}, B7 = {7, 9, 3, 10}, B8 = {8, 9, 4, 11}, B9
= {9, 10, 5, 12},
B10 = {10, 11, 6, 0}, B11 = {11, 12, 7, 1}, B12 = {12, 0, 8, 2}.

206
NON-BINARY LOW-DENSITY PARITY CHECK CODES
Hence, the circulant matrix Q is:
Q =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
0
0
0
1
0
0
0
0
0
1
0
1
1
1
0
0
0
1
0
0
0
0
0
1
0
0
1
1
0
0
0
1
0
0
0
0
0
1
1
0
1
1
0
0
0
1
0
0
0
0
0
0
1
0
1
1
0
0
0
1
0
0
0
0
0
0
1
0
1
1
0
0
0
1
0
0
0
0
0
0
1
0
1
1
0
0
0
1
0
0
0
0
0
0
1
0
1
1
0
0
0
1
0
0
0
0
0
0
1
0
1
1
0
0
0
1
1
0
0
0
0
0
1
0
1
1
0
0
0
0
1
0
0
0
0
0
1
0
1
1
0
0
0
0
1
0
0
0
0
0
1
0
1
1
0
0
0
0
1
0
0
0
0
0
1
0
1
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
This example only produces a single circulant matrix, but the next value of t
that ensures the Ô¨Ånite Ô¨Åeld is prime is t = 6, giving GF(73). This will result in
an LDPC code with parameters v = 73, n = 438, Œ≥ = 4, œÅ = 292 and Œª = 1.
In this case, there will be six base blocks each containing four elements, and six
circulant matrices can be formed by adding every element in GF(73) to each of
the base blocks. Each circulant matrix will have dimensions 73 √ó 73, with Œ≥ =
4, œÅ = 4. The six circulant matrices are then concatenated to form the Ô¨Ånal parity
check matrix, H, with dimensions 73 √ó 438. The null space of this matrix will be
the (438, 365) LDPC code, assuming each row in H is independent.
6.2.4 Constructing the Generator Matrix of a Binary LDPC Code
The encoding of a binary message to form an LDPC code word is achieved by
multiplying the binary message vector by a systematic generator matrix, G. The
systematic generator matrix is obtained by Ô¨Årst performing Gauss‚ÄìJordan elimination
on the parity check matrix so that it is of the form H = [Im|P], where Im is the
m √ó m identity matrix and P is a parity matrix. The generator matrix is then G =
[PT|Ik], where PT is the matrix transpose of P, and Ik is the k √ó k identity matrix.
Gauss‚ÄìJordan elimination is related to Gaussian elimination in that it eliminates all
elements below and also above each pivot element. Gaussian elimination results in
a matrix in row echelon form, whereas Gauss‚ÄìJordan elimination results in a matrix
in reduced row echelon form. It can therefore be seen as a two-stage process, where
Gaussian elimination is Ô¨Årst performed to transform the matrix into row echelon form,
followed by elimination of elements above the pivot elements.

CONSTRUCTION OF BINARY LDPC CODES
207
The elimination procedure on a matrix containing Ô¨Ånite Ô¨Åeld elements is performed
in the same way as if the matrix contained real numbers. The only difference is that
elements are eliminated with modulo-2 addition instead of subtraction.
It is not unusual for a parity check matrix to have dependent rows. When performing
Gaussian elimination on the matrix, all dependent rows will appear as rows containing
all zeroes located at the bottom of the new matrix. This is the case if we apply Gaussian
elimination to the parity check matrix of (6.1):
H =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
1
0
1
0
0
0
1
1
0
1
0
1
0
0
0
1
1
0
0
1
1
0
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª‚ÜíH
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
0
0
0
1
1
0
1
1
0
1
0
0
0
1
1
0
1
0
0
0
0
0
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
We can see that row 4 in H is the modulo-2 sum of row 1, row 2 and row 3 and
hence it is a dependent row. Therefore, performing Gaussian elimination results in an
all-zero row in the new matrix, H. However, a generator matrix can still be obtained
by removing the all-zero rows and then eliminating elements above the pivot elements
in the resulting submatrix:
H =
Ô£Æ
Ô£∞
1
0
0
0
1
1
0
1
1
0
1
0
0
0
1
1
0
1
Ô£π
Ô£ª‚ÜíH =
Ô£Æ
Ô£ØÔ£∞
1
0
0
0
1
1
0
1
0
1
1
1
0
0
1
1
0
1
Ô£π
Ô£∫Ô£ª.
The systematic parity check matrix H is simply obtained by adding row 3 to row
2 in H. Finally, the generator matrix is:
G =
Ô£Æ
Ô£ØÔ£∞
0
1
1
1
0
0
1
1
0
0
1
0
1
1
1
0
0
1
Ô£π
Ô£∫Ô£ª.
We can see that dependent rows in the parity check matrix will increase the code
rate of the LDPC code. If all the rows were independent then the null space of the
parity check matrix of (6.1) would be the (6, 2) LDPC code with code rate of 1/3.
However, the dependent row means the null space is the (6, 3) LDPC code with code
rate of 1/2.
For larger parity check matrices, the swapping of columns is usually required during
Gaussian elimination to ensure that the pivot elements are on the main diagonal of
the matrix. However, this will obviously change the parity check equations and the
resulting systematic generator matrix will not be the null space of the original parity
check matrix. In this case, any column that is swapped must also be swapped on the
original parity check matrix, but this will not alter its column and row weights.

208
NON-BINARY LOW-DENSITY PARITY CHECK CODES
6.3 Decoding of Binary LDPC Codes Using the Belief
Propagation Algorithm
The decoding of LDPC codes using the sum product algorithm involves Ô¨Ånding a code
word of length n where each coded bit cn maximizes the probability of cn conditioned
on the parity check equations associated with cn being satisÔ¨Åed [7].
P(cn|{zm = 0, m ‚ààMn}).
(6.4)
The sum product algorithm involves calculating two probabilities. The Ô¨Årst is
the probability qmn, which is the probability of the nth code bit conditioned on
its associated parity checks being satisÔ¨Åed with the exception of the mth parity
check:
qmn(x) = P(cn = x

zm = 0, m ‚ààMn/m

.
(6.5)
The second probability is denoted rmn, which is the probability that the mth parity
check is satisÔ¨Åed conditioned on all the possible values of the coded bits c:
rmn(x) = P(zm = 0|c).
(6.6)
Figure 6.2 shows how these probabilities are exchanged in the Tanner graph of
Figure 6.1 for the connections between c1 and z1, and c6 and z4.
6.3.1 Initialization of the Belief Propagation Algorithm
The probabilities qmn(x) are initialized to the probability f (x)
n
that the nth received bit
is x, that is P(cn = x). For the additive white Gaussian noise (AWGN) channel the
probability g(x)
n
is [7]:
g(0)
n
‚àù
1
œÉ
‚àö
2œÄ
e‚àí(rn‚àí1)2/2œÉ 2
and
g(1)
n
‚àù
1
œÉ
‚àö
2œÄ
e‚àí(rn+1)2/2œÉ 2.
(6.7)
c1
c2
c3
c4
c5
c6
z1
z2
z3
z4
q64
r64
r 11
q11
Figure 6.2
Tanner graph showing the exchange of information between the coded bits and their parity
checks.

DECODING OF BINARY LDPC CODES
209
1
Q
I
00
01
11
10
Ô£∑
Ô£∏
Ô£∂
Ô£¨
Ô£≠
Ô£´
2
1
,
2
1
Ô£∑
Ô£∏
Ô£∂
Ô£¨
Ô£≠
Ô£´
‚àí
2
1
,
2
1
Ô£∑
Ô£∏
Ô£∂
Ô£¨
Ô£≠
Ô£´
‚àí
‚àí
2
1
,
2
1
Ô£∑
Ô£∏
Ô£∂
Ô£¨
Ô£≠
Ô£´‚àí
2
1
,
2
1
Figure 6.3
QPSK constellation.
In general, when the modulation scheme has an in-phase and quadrature component
the probabilities are determined by calculating the Euclidean distances for the in-phase
component from each constellation point. The coordinates of each constellation point
in the QPSK constellation are shown in Figure 6.3.
Therefore, the four probabilities g(00)
n
, g(01)
n
, g(11)
n
and g(10)
n
for the QPSK constella-
tion are:
g(00)
n
‚àù
1
œÉ
‚àö
2œÄ
e
‚àí

r(I)
n ‚àí1
‚àö
2
2
+

r(Q)
n
‚àí1
‚àö
2
2
2œÉ 2
,
g(01)
n
‚àù
1
œÉ
‚àö
2œÄ
e
‚àí

r(I)
n ‚àí1
‚àö
2
2
+

r(Q)
n
+ 1
‚àö
2
2
2œÉ 2
g(11)
n
‚àù
1
œÉ
‚àö
2œÄ
e
‚àí

r(I)
n + 1
‚àö
2
2
+

r(Q)
n
+ 1
‚àö
2
2
2œÉ 2
,
g(10)
n
‚àù
1
œÉ
‚àö
2œÄ
e
‚àí

r(I)
n + 1
‚àö
2
2
+

r(Q)
n
‚àí1
‚àö
2
2
2œÉ 2
,
(6.8)
where r(I)
n
and r(Q)
n
are the in-phase and quadrature components of the received
symbol. The likelihoods of the received bits are then placed in a vector f:
f =
Ô£Æ
Ô£∞g(0)
1
g(0)
2
¬∑ ¬∑ ¬∑
g(0)
n‚àí1
g(0)
n
g(1)
1
g(1)
2
¬∑ ¬∑ ¬∑
g(1)
n‚àí1
g(1)
n
Ô£π
Ô£ª.
(6.9)

210
NON-BINARY LOW-DENSITY PARITY CHECK CODES
To initialize the sum‚Äìproduct algorithm, the likelihoods in f are used to initialize a
matrix Q deÔ¨Åned as:
Q =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
g(0)
1
g(0)
2
g(0)
3
¬∑ ¬∑ ¬∑
g(0)
n‚àí1
g(0)
n
g(1)
1
g(1)
2
g(1)
3
¬∑ ¬∑ ¬∑
g(1)
n‚àí1
g(1)
n
...
...
...
...
...
...
...
...
...
...
...
...
g(0)
1
g(0)
2
g(0)
3
¬∑ ¬∑ ¬∑
g(0)
n‚àí1
g(0)
n
g(1)
1
g(1)
2
g(1)
3
¬∑ ¬∑ ¬∑
g(1)
n‚àí1
g(1)
n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(6.10)
The matrix Q has partitions containing the likelihoods that the ith received bit is a
0, g(0)
i
or a 1, g(1)
i .
6.3.2 The Horizontal Step
The horizontal step of the sum‚Äìproduct algorithm determines the probability rmn,
deÔ¨Åned as [7]:
rmn(x) =

c:cn=x
P(zm = 0|c)P(c|cn = x),
(6.11)
where, given that the nth coded bit cn = x, c is a binary vector of the remaining coded
bits of length œÅ ‚àí1, containing every possible combination of binary sequences.
There are therefore 2œÅ‚àí1 binary sequences that (6.9) is summed over. However, not
all these sequences will satisfy zm = 0 so we only consider those sequences that
do satisfy the parity check. For example, if x = 0 then only those binary sequences
with an even number of 1s are able to satisfy zm = 0. Therefore, the probability of a
sequence containing an odd number of 1s satisfying zm = 0 is zero. Therefore, rmn(x)
is the sum of all the probabilities of the binary sequences of length œÅ ‚àí1 that when
summed together equal x:
rmn(x) =

c:cn=x
P(zm = 0 |c)

n‚ààNm\n
qmn(x),
(6.12)

DECODING OF BINARY LDPC CODES
211
where P(zm = 0 | c) is either zero or one. The probabilities rmn(x) are placed in a matrix
R:
R =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
r11(0)
r12(0)
r13(0)
¬∑ ¬∑ ¬∑
r1(n‚àí1)(0)
r1n(0)
r11(1)
r12(1)
r13(1)
¬∑ ¬∑ ¬∑
r1(n‚àí1)(1)
r1n(1)
...
...
...
...
...
...
...
...
...
...
...
...
rm1(0)
rm2(0)
rm3(0)
¬∑ ¬∑ ¬∑
rm(n‚àí1)(0)
rmn(0)
rm1(1)
rm2(1)
rm3(1)
¬∑ ¬∑ ¬∑
rm(n‚àí1)(1)
rmn(1)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
(6.13)
where each element in the parity check matrix has a corresponding 1 √ó 2 column
vector containing the probabilities rmn(0) and rmn(1) (or a 1 √ó q column vector for an
LDPC code deÔ¨Åned over GF(q)).
6.3.3 The Vertical Step
The vertical step updates the probabilities qmn and is much simpler than the horizontal
step. From (6.5), qmn can be written using Baye‚Äôs Rule as [7]:
qmn(x) = P(cn = x

zm = 0, m ‚ààMn/m

= P(cn = x)P

zm = 0, m ‚ààMn/m
 |cn = x

P ({zm = 0, m ‚ààMn/m})
.
(6.14)
Using (6.6) and letting f x
n = P(cn = x), (6.10) can be written as [7]:
qmn(x) = Œ≤mn f x
n

m‚ààMn/m
rmn(x),
(6.15)
where Œ≤mn is a normalizing constant such that  qmn(x) = 1, that is:
Œ≤mn =
1

x
f xn

m‚ààMn\m
rmn(x).
(6.16)

212
NON-BINARY LOW-DENSITY PARITY CHECK CODES
The qmn(x) are placed in the matrix Q as shown in (6.17):
Q =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
q11(0)
q12(0)
q13(0)
¬∑ ¬∑ ¬∑
q1(n‚àí1)(0)
q1n(0)
q11(1)
q12(1)
q13(1)
¬∑ ¬∑ ¬∑
q1(n‚àí1)(1)
q1n(1)
...
...
...
...
...
...
...
...
...
...
...
...
qm1(0)
qm2(0)
qm3(0)
¬∑ ¬∑ ¬∑
qm(n‚àí1)(0)
qmn(0)
qm1(1)
qm2(1)
qm3(1)
¬∑ ¬∑ ¬∑
qm(n‚àí1)(1)
qmn(1)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
(6.17)
where each element in the parity check matrix has a corresponding 1 √ó 2 column
vector containing the probabilities qmn(0) and qmn(1) (or a 1 √ó q column vector for an
LDPC code deÔ¨Åned over GF(q)).
In this step, the pseudo posterior probabilities qn(x) are also determined by [7]:
qn(x) = Œ≤n f x
n

m‚ààMn
rmn(x),
(6.18)
where again Œ≤n is a normalizing constant such that  qn(x) = 1. The pseudo posterior
probabilities are place in a matrix Q:
Q =
q1(0)
q2(0)
q3(0)
¬∑ ¬∑ ¬∑
qn‚àí1(0)
qn(0)
q1(1)
q2(1)
q3(1)
¬∑ ¬∑ ¬∑
qn‚àí1(1)
qn(1)

.
(6.19)
From these pseudo posterior probabilities estimates of the transmitted code word
can be found by:
ÀÜcn = arg max Œ≤n f x
n
x

m‚ààNm
rmn(x).
(6.20)
Example 6.3: Decoding a binary LDPC code with the Belief Propagation
algorithm: In this example, we use the regular (10, 5) binary LDPC code with row
weight œÅ = 6 and column weight Œ≥ = 3. The parity check matrix H is given as:
H =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
1
1
0
0
1
1
0
0
1
1
0
1
0
1
1
0
1
1
0
0
0
1
1
1
0
1
0
1
1
0
1
0
1
1
1
0
1
0
1
1
1
0
1
0
0
1
1
1
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Let us assume that the transmitted code word is c = [0
0
0
1
0
1
0
1
0
1] and the received word is r = [0
0
0
1
1
1
0
1
0
1],

DECODING OF BINARY LDPC CODES
213
with a single error in the Ô¨Åfth position. The associated likelihoods are:
f =

0.78
0.84
0.81
0.52
0.45
0.13
0.82
0.21
0.75
0.24
0.22
0.16
0.19
0.48
0.55
0.87
0.18
0.79
0.25
0.76

.
We initialize the matrix Q with these likelihoods as follows:
Q =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0.78
0.84
0.81
0
0
0.13
0.82
0
0
0.24
0.22
0.16
0.19
0
0
0.87
0.18
0
0
0.76
0.78
0
0.81
0
0.45
0.13
0
0.21
0.75
0
0.22
0
0.19
0
0.55
0.87
0
0.79
0.25
0
0
0
0.81
0.52
0.45
0
0.82
0
0.75
0.24
0
0
0.19
0.48
0.55
0
0.18
0
0.25
0.76
0
0.84
0
0.52
0.45
0.13
0
0.21
0
0.24
0
0.16
0
0.48
0.55
0.87
0
0.79
0
0.76
0.78
0.84
0
0.52
0
0
0.82
0.21
0.75
0
0.22
0.16
0
0.48
0
0
0.18
0.79
0.25
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
The matrix Q containing all the probabilities qmn(x) is then used for the horizontal
step to determine the matrix R containing the probabilities rmn(x). To determine
the probability r11(1) we must calculate the probabilities of all the possible binary
sequences that satisfy z1 when c1 = 1. Hence, we must Ô¨Årst Ô¨Ånd all possible binary
values of c2, c3, c6, c7 and c10 that satisfy:
c2 + c3 + c6 + c7 + c10 = c1 = 1.
Since we are only working with binary values, the above equation can only be
satisÔ¨Åed when an odd number of the coded bits are equal to 1. The 16 possible bit
sequences and their probabilities are given in Table 6.1.
r11(1) =

q12(1)q13(0)q16(0)q17(0)q1,10(0)

+

q12(0)q13(1)q16(0)q17(0)q1,10(0)

+
+

q12(0)q13(0)q16(1)q17(0)q1,10(0)

+

q12(0)q13(0)q16(0)q17(1)q1,10(0)

+

q12(0)q13(0)q16(0)q17(0)q1,10(1)

+

q12(1)q13(1)q16(1)q17(0)q1,10(0)

+

q12(0)q13(1)q16(1)q17(1)q1,10(0)

+

q12(0)q13(0)q16(1)q17(1)q1,10(1)

+

q12(1)q13(1)q16(0)q17(0)q1,10(1)

+

q12(1)q13(1)q16(0)q17(1)q1,10(0)

+

q12(0)q13(1)q16(1)q17(0)q1,10(1)

+

q12(1)q13(0)q16(1)q17(0)q1,10(1)

+

q12(1)q13(0)q16(0)q17(1)q1,10(1)

+

q12(0)q13(1)q16(0)q17(1)q1,10(1)

+

q12(1)q13(0)q16(1)q17(1)q1,10(0)

+

q12(1)q13(1)q16(1)q17(1)q1,10(1)

= 0.00316 + 0.004038 + 0.116495 + 0.003821 + 0.055123 + 0.005205
+ 0.005998 + 0.080978 + 0.002463 + 0.000171 + 0.086533 + 0.070267
+ 0.002305 + 0.002838 + 0.004871 + 0.0036618r11(1) = 0.448086.

214
NON-BINARY LOW-DENSITY PARITY CHECK CODES
Table 6.1
Bit sequences and their associated probabilities of determining r11(1).
Bit sequence
Probability
10 000
q12(1) q13(0) q16(0) q17(0) q1,10(0) = 0.16 √ó 0.81 √ó 0.13 √ó 0.82 √ó 0.24 = 0.003316
01 000
q12(0) q13(1) q16(0) q17(0) q1,10(0) = 0.84 √ó 0.19 √ó 0.13 √ó 0.82 √ó 0.24 = 0.004083
00 100
q12(0) q13(0) q16(1) q17(0) q1,10(0) = 0.84 √ó 0.81 √ó 0.87 √ó 0.82 √ó 0.24 = 0.116495
00 010
q12(0) q13(0) q16(0) q17(1) q1,10(0) = 0.84 √ó 0.81 √ó 0.13 √ó 0.18 √ó 0.24 = 0.003821
00 001
q12(0) q13(0) q16(0) q17(0) q1,10(1) = 0.84 √ó 0.81 √ó 0.13 √ó 0.82 √ó 0.76 = 0.055123
11 100
q12(1) q13(1) q16(1) q17(0) q1,10(0) = 0.16 √ó 0.19 √ó 0.87 √ó 0.82 √ó 0.24 = 0.005205
01 110
q12(0) q13(1) q16(1) q17(1) q1,10(0) = 0.84 √ó 0.19 √ó 0.87 √ó 0.18 √ó 0.24 = 0.005998
00 111
q12(0) q13(0) q16(1) q17(1) q1,10(1) = 0.84 √ó 0.81 √ó 0.87 √ó 0.18 √ó 0.76 = 0.080978
11 001
q12(1) q13(1) q16(0) q17(0) q1,10(1) = 0.16 √ó 0.19 √ó 0.13 √ó 0.82 √ó 0.76 = 0.002463
11 010
q12(1) q13(1) q16(0) q17(1) q1,10(0) = 0.16 √ó 0.19 √ó 0.13 √ó 0.18 √ó 0.24 = 0.000171
01 101
q12(0) q13(1) q16(1) q17(0) q1,10(1) = 0.84 √ó 0.19 √ó 0.87 √ó 0.82 √ó 0.76 = 0.086533
10 101
q12(1) q13(0) q16(1) q17(0) q1,10(1) = 0.16 √ó 0.81 √ó 0.87 √ó 0.82 √ó 0.76 = 0.070267
10 011
q12(1) q13(0) q16(0) q17(1) q1,10(1) = 0.16 √ó 0.81 √ó 0.13 √ó 0.18 √ó 0.76 = 0.002305
01 011
q12(0) q13(1) q16(0) q17(1) q1,10(1) = 0.84 √ó 0.19 √ó 0.13 √ó 0.18 √ó 0.76 = 0.002838
10 110
q12(1) q13(0) q16(1) q17(1) q1,10(0) = 0.16 √ó 0.81 √ó 0.87 √ó 0.18 √ó 0.24 = 0.004871
11 111
q12(1) q13(1) q16(1) q17(1) q1,10(1) = 0.16 √ó 0.19 √ó 0.87 √ó 0.18 √ó 0.76 = 0.003618
By adding these probabilities we get r11(1) = 0.448086 and therefore r11(0) =
1 ‚àír11(1) = 0.551914. The complete matrix R is then:
R =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0.551914
0.542753
0.546890
0
0
0.460714
0.545425
0
0
0.444092
0.448086
0.457247
0.453110
0
0
0.539286
0.454575
0
0
0.555908
0.493347
0
0.493991
0
0.537255
0.505034
0
0.506423
0.493347
0
0506653
0
0.506009
0
0.462745
0.494966
0
0.493577
0.507451
0
0
0
0.500333
0.505158
0.497937
0
0.500322
0
0.500413
0.499603
0
0
0.499667
0.494842
0.502063
0
0.499678
0
0.499587
0.500397
0
0.500446
0
0.507588
0.496965
0.499590
0
0.499477
0
0.499416
0
0.499554
0
0.492412
0.503035
0.500410
0
0.500523
0
0.500584
0.497476
0.497921
0
0.464662
0
0
0.497791
0.502437
0.497173
0
0.502524
0.502079
0
0.535338
0
0
0.502209
0.497563
0.502827
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
The vertical step then updates the matrix Q using (6.15). To determine the
probability q11(x):
q11(0) = Œ≤11 f 0
1

m‚ààM1\1
rm1(0)
= Œ≤11 √ó 0.78 √ó (0.493347 √ó 0.497476) = 0.191434Œ≤11
q11(1) = Œ≤11 f 1
1

m‚ààM1\1
rm1(1)
= Œ≤11 √ó 0.22 √ó (0.506653 √ó 0.502524) = 0.056013Œ≤11
.

DECODING OF BINARY LDPC CODES
215
Since 0.191334Œ≤11 + 0.05601Œ≤11 = 1, Œ≤11 =
1
0.191334+0.056013 = 4.042903.
So:
q11(0) = 4.042903 √ó 0.78 √ó (0.493347 √ó 0.497476) = 0.773636
q11(1) = 4.042903 √ó 0.22 √ó (0.506653 √ó 0.502524) = 0.226364.
The remaining qmn are shown in Q:
Q =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0.773636
0.839121
0.806481
0
0
0.132106
0.818884
0
0
0.239285
0.226364
0.160879
0.193519
0
0
0.867894
0.181116
0
0
0.760715
0.812140
0
0.837461
0
0.444958
0.113039
0
0.211273
0.748185
0
0.187860
0
0.162539
0
0.555042
0.886961
0
0.788727
0.251815
0
0
0
0.833978
0.492203
0.484126
0
0.844187
0
0.742212
0.201076
0
0
0.166022
0.507797
0.515874
0
0.155813
0
0.257788
0.798924
0
0.860727
0
0.489773
0.485097
0.115241
0
0.215940
0
0.201196
0
0.139273
0
0.510227
0.514903
0.884759
0
0.784060
0
0.798804
0.809608
0.861934
0
0.532711
0
0
0.845514
0.213942
0.744684
0
0.190392
0.138066
0
0.467289
0
0
0.154486
0.786058
0.255316
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Finally the pseudo posterior probabilities qn are determined using (6.18). To Ô¨Ånd
q1(x):
q1(0) = Œ≤1 f 0
1

m‚ààM1
rm1(0)
= Œ≤1 √ó 0.78 √ó (0.551914 √ó 0.493347 √ó 0.500413) = 0.116898Œ≤1
q1(1) = Œ≤1 f 1
1

m‚ààM1
rm1(1)
= Œ≤1 √ó 0.22 √ó (0.448086 √ó 0.506653 √ó 0.502524) = 0.025099Œ≤1
‚à¥Œ≤1 =
1
0.116898 + 0.025099 = 7.042402
q1(0) = 7.042402 √ó 0.78 √ó (0.551914 √ó 0.493347 √ó 0.500413) = 0.808046
q1(1) = 7.042402 √ó 0.22 √ó (0.448086 √ó 0.506653 √ó 0.502524) = 0.191954
.
Applying (6.19), the decoded symbol ÀÜc1 = 0 since q1(0) has the higher proba-
bility. The pseudo posterior probabilities of the received word are:
Q =
 0.808046 0.860941 0.834162 0.497361 0.482065 0.115074 0.844356 0.215586 0.742528 0.200821
0.191954 0.139059 0.165838 0.502639 0.517935 0.884926 0.155644 0.784414 0.257472 0.799179

and the corresponding decoded code word is ÀÜc = [0
0
0
1
1
1
0
1
0
1]. We can see that this does not match the original transmitted code word
and so further iterations are required. It turns out that the decoded code word is

216
NON-BINARY LOW-DENSITY PARITY CHECK CODES
correct after the third iteration and the Ô¨Ånal matrices R, Q and Q are given along
with the correct decoded code word:
R =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0.549960
0.540086
0.544369
0
0
0.463092
0.542650
0
0
0.447890
0.450040
0.459914
0.455631
0
0
0.536908
0.457350
0
0
0.552110
0.493114
0
0.493650
0
0.545393
0.505532
0
0.507453
0.491301
0
0.506886
0
0.506350
0
0.454607
0.494468
0
0.492547
0.508699
0
0
0
0.499989
0.500176
0.502649
0
0.499989
0
0.499985
0.500012
0
0
0.500011
0.499824
0.497351
0
0.500011
0
0.500015
0.499988
0
0.499975
0
0.500415
0.503915
0.500023
0
0.500032
0
0.500030
0
0.500025
0
0.499585
0.496085
0.499977
0
0.499968
0
0.499970
0.496595
0.497094
0
0.457904
0
0
0.496955
0.503693
0.495668
0
0.503405
0.502906
0
0.542096
0
0
0.503045
0.496307
0.504332
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Q =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0.772854
0.838418
0.806053
0
0
0.132534
0.818189
0
0
0.240031
0.227146
0.161582
0.193947
0
0
0.867466
0.181811
0
0
0.759969
0.810391
0
0.835883
0
0.456507
0.114177
0
0.212482
0.746725
0
0.189609
0
0.164117
0
0.543493
0.885823
0
0.787518
0.253275
0
0
0
0.832375
0.478244
0.499266
0
0.842265
0
0.740099
0.203955
0
0
0.167625
0.521756
0.500734
0
0.157735
0
0.259901
0.796045
0
0.859034
0
0.478005
0.498000
0.116425
0
0.217493
0
0.203943
0
140966
0
0.521995
0.502000
0.883575
0
0.782507
0
0.796057
0.808242
0.860424
0
0.520590
0
0
0.843871
0.215010
0.743407
0
0.191758
0.139576
0
0.479410
0
0
0.156129
0.784990
0.256593
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Q =
 0.806122 0.859023 0.832369 0.478419 0.501915 0.116434 0.842260 0.217514 0.740088 0.203963
0.193878 0.140977 0.167631 0.521581 0.498085 0.883566 0.157740 0.782486 0.259913 0.796037

ÀÜc = [0
0
0
1
0
1
0
1
0
1].
The majority of the complexity of the sum‚Äìproduct algorithm is due to the
horizontal step, since the number of possible non-binary sequences can become
very large. However, it has been shown that fast Fourier transforms (FFTs) can be
used to replace this step, resulting in a signiÔ¨Åcant reduction in complexity [8].
6.3.4 Reducing the Decoding Complexity Using Fast Fourier Transforms
The horizontal step described previously involves Ô¨Ånding all possible binary sequences
that satisfy a parity check equation, determining the probability of each sequence and
adding them all together, as deÔ¨Åned in (6.10).

DECODING OF BINARY LDPC CODES
217
If we take the simple parity check equation z1 = c1 ‚äïc2 ‚äïc3 = 0 then to determine
r11(x) we Ô¨Årst need to Ô¨Ånd all the solutions of c2 ‚äïc3 = c1 = x. For x = 0 the solutions
are c2 = 0, c3 = 0 and c2 = 1, c3 = 1. Therefore:
r11(0) = q12(0)q13(0) + q12(1)q13(1).
For x = 1 the solutions are c2 = 0, c3 = 1 and c2 = 1, c3 = 0. Therefore:
r11(1) = q12(0)q13(1) + q12(1)q13(0).
In general, we can write this as the convolution operation:
r11(x) =
1

v=0
q12(v)q13(x ‚àív),
where v ‚ààGF(2).
This implies that the same result can be achieved by replacing the convolution
operation with Fourier transforms. Therefore, to determine rmn we must Ô¨Årst calculate
the product of the Fourier transforms of the other qmn and then apply the inverse
Fourier transform:
rmn(x) = F‚àí1
Ô£´
Ô£≠
n‚ààNm/n
F (qmn(x))
Ô£∂
Ô£∏,
(6.21)
where F( ) is the Fourier transform and F‚àí1 is the inverse Fourier transform. Since all
elements belong to the additive group Z2, the Fast Fourier transform reduces to the
Hadamard transform [9] W2, where:
W2 =
1
‚àö
2
1
1
1
‚àí1

.
(6.22)
A property of the Hadamard matrix is that its inverse is also the Hadamard matrix,
that is:
W2W2 = 1
2

1
1
1
‚àí1
 
1
1
1
‚àí1

= 1
2

2
0
0
2

= I,
where I is the identity matrix. Consequently, the inverse Fourier transform can be
obtained by also multiplying by the Hadamard matrix. We will now use FFTs to
determine r11(0) and r11(1) for the Ô¨Årst iteration of Example 3.2. From (6.20):
r11(x) = F‚àí1
Ô£´
Ô£≠
n‚ààN0\0
F (q1n(x))
Ô£∂
Ô£∏
= F‚àí1
 1
‚àö
2
1
1
1
‚àí1
 q13(0)
q13(1)

√ó 1
‚àö
2
1
1
1
‚àí1
 q14(0)
q14(1)


218
NON-BINARY LOW-DENSITY PARITY CHECK CODES
√ó
1
‚àö
2
1
1
1
‚àí1
 q16(0)
q16(1)

√ó 1
‚àö
2
1
1
1
‚àí1
 q17(0)
q17(1)

√ó
1
‚àö
2
1
1
1
‚àí1
 q1,10(0)
q1,10(1)
 
= F‚àí1
 1
‚àö
2
1
1
1
‚àí1
 0.84
0.16

√ó 1
‚àö
2
1
1
1
‚àí1
 0.81
0.19

√ó
1
‚àö
2
1
1
1
‚àí1
 0.13
0.87

√ó 1
‚àö
2
1
1
1
‚àí1
 0.82
0.18

√ó
1
‚àö
2
1
1
1
‚àí1
 0.24
0.76
 
= F‚àí1
 1
‚àö
2
 1
0.68

√ó 1
‚àö
2
 1
0.62

√ó 1
‚àö
2

1
‚àí0.74

√ó 1
‚àö
2
 1
0.64

√ó 1
‚àö
2

1
‚àí0.52
 
.
= 1
2
1
1
1
‚àí1
 
1
0.103828

=
0.551914
0.448086

Therefore, r11(0) = 0.551914 and r11(1) = 0.448086, which is identical to the result
obtained in the previous example. It is quite obvious by comparing these two methods
that FFTs are much simpler to perform. FFTs will also be applied to the decoding of
non-binary LDPC codes, explained later in this chapter.
6.4 Construction of Non-Binary LDPC Codes DeÔ¨Åned Over Finite Fields
A non-binary LDPC code is simply an LDPC code with a sparse parity check matrix
containing elements that could be deÔ¨Åned over groups, rings or Ô¨Åelds. In this book,
we concentrate solely on LDPC codes deÔ¨Åned over Ô¨Ånite Ô¨Åelds GF(2i), where i is a
positive integer greater than 1. In 1998, Mackay presented the idea of LDPC codes
over Ô¨Ånite Ô¨Åelds [10], proving that they can achieve increases in performance over
their binary counterparts with increasing Ô¨Ånite Ô¨Åeld size. Mackay also showed how
the sum‚Äìproduct algorithm could be extended to decode non-binary LDPC codes, but
the overall complexity was much higher. At that time it was therefore only feasible to
decode non-binary LDPC codes over small Ô¨Ånite Ô¨Åelds.
There are only a few papers in the literature on non-binary LDPC codes and most
of these codes are constructed by taking a known binary LDPC code and replacing
its nonzero elements with randomly-generated Ô¨Ånite Ô¨Åeld elements. Shu Lin has
presented several structured methods to construct good non-binary LDPC codes using
a technique known as array dispersion [11], one of which we now describe.

CONSTRUCTION OF NON-BINARY LDPC CODES DEFINED OVER FINITE FIELDS
219
6.4.1 Construction of Non-Binary LDPC Codes from Reed‚ÄìSolomon Codes
Shu Lin has demonstrated several array dispersion methods using tools such as Eu-
clidean and Ô¨Ånite geometries and also using a single code word from a very low-rate
Reed‚ÄìSolomon code [11]. Since this book is concerned with non-binary codes, this
section explains how to construct non-binary LPDC codes from a Reed‚ÄìSolomon
code word.
In the context of a non-binary code deÔ¨Åned over Ô¨Ånite Ô¨Åelds GF(q), array dispersion
is an operation applied to each nonzero element in a matrix whereby each element is
transformed to a location vector of length q ‚àí1. For an element Œ±i ‚ààGF(q), 0 ‚â§i ‚â§
q ‚àí2, it will be placed in the ith index of the location vector. For example, the element
Œ±5 in GF(8) would be placed in the Ô¨Åfth index of the location vector. The element in
this location vector is used to build an array with each row deÔ¨Åned as the previous
row cyclically shifted to the right and multiplied by the primitive element in GF(q),
resulting in a (q ‚àí1) √ó (q ‚Äì 1) array. Performing array dispersion on a matrix with
dimensions a √ó b would result in a larger matrix with dimensions a(q ‚àí1) √ó b(q ‚àí
1). Therefore, after array dispersion the element Œ±5 would become:
Ô£´
Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠
0
0
0
0
0
Œ±5
0
0
0
0
0
0
0
Œ±6
1
0
0
0
0
0
0
0
Œ±
0
0
0
0
0
0
0
Œ±2
0
0
0
0
0
0
0
Œ±3
0
0
0
0
0
0
0
Œ±4
0
0
Ô£∂
Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
where the top row is the original location vector. For the zero element, array dispersion
will result in a (q ‚àí1) √ó (q ‚Äì 1) zero matrix.
These non-binary LDPC codes are constructed from Reed‚ÄìSolomon codes with
message length k = 2. Since these code are maximum distance separable (MDS) their
minimum Hamming distance is d = n ‚àík + 1 = n ‚àí1. Since a Reed‚ÄìSolomon code
deÔ¨Åned over GF(q) has a block length of n = q ‚àí1, the Reed‚ÄìSolomon code will be a
(q ‚àí1, 2, q ‚àí2) Reed‚ÄìSolomon code. Since d = q ‚àí2, this means that the minimum
weight of the code word is also q ‚àí2, implying that the code word will contain q ‚àí
2 nonzero elements and a single zero. It can be shown that the vector containing all
1s, [1, 1, 1, . . ., 1], and the vector [1, Œ±, Œ±2, . . ., Œ±q‚àí2] are both valid code words. As
a Reed‚ÄìSolomon code is linear, adding these two code words will result in another
code word:
!1 1 1 ¬∑ ¬∑ ¬∑ 1"
+
!1 Œ± Œ±2 ¬∑ ¬∑ ¬∑ Œ±q‚àí2 "
=
!0 1 + Œ± 1 + Œ±2 ¬∑ ¬∑ ¬∑ 1 + Œ±q‚àí2 "
,
which has a weight of q ‚àí2. This code word is then used to build a (q ‚àí1) √ó (q ‚àí
1) array by cyclically shifting it to the right to form the next row of the array. Observe
that the main diagonal of this array will be zeroes.

220
NON-BINARY LOW-DENSITY PARITY CHECK CODES
Example 6.4: Array dispersion of a (7, 2, 6) Reed‚ÄìSolomon code word over
GF(8): The code word of weight q ‚àí2 = 6 is obtained by adding the two vectors:
!1 1 1 1 1 1 1"
+
!
1 Œ± Œ±2 Œ±3 Œ±4 Œ±5 Œ±6 "
=
!
0 Œ±3 Œ±6 Œ± Œ±5 Œ±4 Œ±2 "
.
This then is used to build the 7 √ó 7 array:
0
Œ±3
Œ±6
Œ±
Œ±
5
Œ±4
Œ±2
Œ±2
0
Œ±3
Œ±6
Œ±
Œ±
5
Œ±4
Œ±4
Œ±2
0
Œ±3
Œ±6
Œ±
Œ±
5
Œ±5
Œ±4
Œ±2
0
Œ±3
Œ±6
Œ±
Œ±
Œ±
5
Œ±4
Œ±2
0
Œ±3
Œ±6
Œ±
Œ±
Œ±
5
Œ±4
Œ±2
0
Œ±3
Œ±3
Œ±6
6
Œ±
Œ±
5
Œ±4
Œ±2
0
Since Reed‚ÄìSolomon codes are cyclic, each row is a code word and each column,
when read from bottom to top, is also a code word, all with weights equal to 6.
After applying array dispersion we will obtain a much larger 49 √ó 49 array. This
is too large to reproduce here but a small part of it is shown below, corresponding
to the 2 √ó 2 subarray circled.
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0
0
0
0
0
0
0
0
0
0
Œ±3
0
0
0
¬∑ ¬∑ ¬∑
0
0
0
0
0
0
0
0
0
0
0
Œ±4
0
0
¬∑ ¬∑ ¬∑
0
0
0
0
0
0
0
0
0
0
0
0
Œ±5
0
¬∑ ¬∑ ¬∑
0
0
0
0
0
0
0
0
0
0
0
0
0
Œ±6
¬∑ ¬∑ ¬∑
0
0
0
0
0
0
0
1
0
0
0
0
0
0
¬∑ ¬∑ ¬∑
0
0
0
0
0
0
0
0
Œ±
0
0
0
0
0
¬∑ ¬∑ ¬∑
0
0
0
0
0
0
0
0
0
Œ±2
0
0
0
0
¬∑ ¬∑ ¬∑
0
0
Œ±2
0
0
0
0
0
0
0
0
0
0
0
¬∑ ¬∑ ¬∑
0
0
0
Œ±3
0
0
0
0
0
0
0
0
0
0
¬∑ ¬∑ ¬∑
0
0
0
0
Œ±4
0
0
0
0
0
0
0
0
0
¬∑ ¬∑ ¬∑
0
0
0
0
0
Œ±5
0
0
0
0
0
0
0
0
¬∑ ¬∑ ¬∑
0
0
0
0
0
0
Œ±6
0
0
0
0
0
0
0
¬∑ ¬∑ ¬∑
1
0
0
0
0
0
0
0
0
0
0
0
0
0
¬∑ ¬∑ ¬∑
0
Œ±
0
0
0
0
0
0
0
0
0
0
0
0
¬∑ ¬∑ ¬∑
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.

DECODING NON-BINARY LDPC CODES WITH THE SUM‚ÄìPRODUCT ALGORITHM
221
6.5 Decoding Non-Binary LDPC Codes with the Sum‚ÄìProduct
Algorithm
The sum‚Äìproduct algorithm for binary LDPC codes can be extended to decode non-
binary LDPC codes, but with an increase in decoding complexity. Firstly, for a non-
binary LDPC code deÔ¨Åned over GF(q), each received symbol can be one of q different
elements in GF(q). Secondly, the horizontal step becomes more complicated as there
are now more possible non-binary sequences to satisfy the parity check equations.
The matrices Q and R used in the horizontal and vertical steps of the sum‚Äìproduct
algorithm are deÔ¨Åned in (6.23) and (6.24) respectively:
Q =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
q11(0)
q12(0)
q13(0)
¬∑ ¬∑ ¬∑
q1,n‚àí1(0)
q1n(0)
q11(1)
q12(1)
q13(1)
¬∑ ¬∑ ¬∑
q1,n‚àí1(1)
q1n(1)
q11(Œ±)
q12(Œ±)
q13(Œ±)
¬∑ ¬∑ ¬∑
q1,n‚àí1(Œ±)
q1n(Œ±)
...
...
...
¬∑ ¬∑ ¬∑
...
...
q11(Œ±q‚àí2)
q12(Œ±q‚àí2)
q13(Œ±q‚àí2)
¬∑ ¬∑ ¬∑
q1,n‚àí1(Œ±q‚àí2)
q1n(Œ±q‚àí2)
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
qm1(0)
qm2(0)
qm3(0)
¬∑ ¬∑ ¬∑
qm,n‚àí1(0)
qmn(0)
qm1(1)
qm2(1)
qm3(1)
¬∑ ¬∑ ¬∑
qm,n‚àí1(1)
qmn(1)
qm1(Œ±)
qm2(Œ±)
qm3(Œ±)
¬∑ ¬∑ ¬∑
qm,n‚àí1(Œ±)
qmn(Œ±)
...
...
...
¬∑ ¬∑ ¬∑
...
...
qm1(Œ±q‚àí2)
qm2(Œ±q‚àí2)
qm3(Œ±q‚àí2)
¬∑ ¬∑ ¬∑
qm,n‚àí1(Œ±q‚àí2)
qmn(Œ±q‚àí2)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(6.23)

222
NON-BINARY LOW-DENSITY PARITY CHECK CODES
R =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
r11(0)
r12(0)
r13(0)
¬∑ ¬∑ ¬∑
r1,n‚àí1(0)
r1n(0)
r11(1)
r12(1)
r13(1)
¬∑ ¬∑ ¬∑
r1,n‚àí1(1)
r1n(1)
r11(Œ±)
r12(Œ±)
r13(Œ±)
¬∑ ¬∑ ¬∑
r1,n‚àí1(Œ±)
r1n(Œ±)
...
...
...
¬∑ ¬∑ ¬∑
...
...
r11(Œ±q‚àí2)
r12(Œ±q‚àí2)
r13(Œ±q‚àí2)
¬∑ ¬∑ ¬∑
r1,n‚àí1(Œ±q‚àí2)
r1n(Œ±q‚àí2)
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
rm1(0)
rm2(0)
rm3(0)
¬∑ ¬∑ ¬∑
rm,n‚àí1(0)
rmn(0)
rm1(1)
rm2(1)
rm3(1)
¬∑ ¬∑ ¬∑
rm,n‚àí1(1)
rmn(1)
rm1(Œ±)
rm2(Œ±)
rm3(Œ±)
¬∑ ¬∑ ¬∑
rm,n‚àí1(Œ±)
rmn(Œ±)
...
...
...
¬∑ ¬∑ ¬∑
...
...
rm1(Œ±q‚àí2)
rm2(Œ±q‚àí2)
rm3(Œ±q‚àí2)
¬∑ ¬∑ ¬∑
rm,n‚àí1(Œ±q‚àí2)
rmn(Œ±q‚àí2)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(6.24)
It can be observed that each nonzero element deÔ¨Åned over GF(q) in the parity check
matrix H has q probabilities associated with it, instead of two probabilities as in the
binary case.
6.5.1 Received Symbol Likelihoods
In (6.8) it was shown how to determine the likelihoods of a demodulated symbol
for the AWGN channel. Assuming a non-binary LDPC code deÔ¨Åned over GF(q) and
that M-PSK modulation is chosen, then provided that q = M the likelihoods of the
demodulated symbols are also the likelihoods of the received symbols. However,
for the case where q > M and M divides q we must concatenate the likelihoods of
the demodulated symbols. As an example, take an LDPC code deÔ¨Åned over GF(16)
and QPSK modulation. We know that a Ô¨Ånite Ô¨Åeld element in GF(16) is made up
of four bits and a QPSK modulated symbol is made up of two bits. Therefore, two
consecutive QPSK modulated symbols contain a Ô¨Ånite Ô¨Åeld element. The element Œ±
in GF(16) is represented as 0010 in binary. Therefore, the likelihood of a received
symbol being Œ± is the product of the likelihood that one demodulated symbol is 00

DECODING NON-BINARY LDPC CODES WITH THE SUM‚ÄìPRODUCT ALGORITHM
223
and the likelihood that the neighbouring demodulated symbol is 10, that is, from (6.8),
P(ri = Œ±) = f Œ± = g00g10.
6.5.2 Permutation of Likelihoods
In general, the parity check equations are of the form:
zi =
n

j=1
hij cj,
(6.25)
where hij and cj ‚ààGF(q). The parity check equation is satisÔ¨Åed when:
h11c1 + h12c2 + ¬∑ ¬∑ ¬∑ + h1ncn = 0.
For the horizontal step we calculate the probabilities rij(x) from (6.10) by Ô¨Årst
substituting all possible non-binary elements into the coded symbols that satisfy the
parity check equation when cj = x, that is:
hi1c1 + hi2c2 + ¬∑ ¬∑ ¬∑ + hincn = hij cj,
and then determining the probability of each sequence. This is more complicated
than the binary case since we must now consider the non-binary parity check matrix
elements. Each coded symbol has q likelihoods associated with it. When the coded
symbol is multiplied by a non-binary parity check element we can compensate for this
by cyclically shifting downwards this column vector of likelihoods, with the exception
of the Ô¨Årst likelihood, corresponding to the probability of the coded symbol being
zero. The number of cyclic shifts is equal to the power of the primitive element that
is multiplied with the coded symbol. This is illustrated below:
cj ‚Üí
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
qij(0)
qij(1)
...
qij(Œ±q‚àí2)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Œ±c j ‚Üí
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
qij(0)
qij(Œ±q‚àí2)
qij(1)
...
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Œ±2c j ‚Üí
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
qij(0)
...
qij(Œ±q‚àí2)
qij(1)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
etc.
This cyclic shift of the likelihoods is known as a permutation [8] and transforms
the parity check equation from (6.25) to:
c1 + c2 + ¬∑ ¬∑ ¬∑ + cn = cj,
which is more similar to the binary parity check equations. The inverse of a permu-
tation is a depermutation, where the likelihoods are cyclically shifted upwards, again
with the exception of the Ô¨Årst likelihood.

224
NON-BINARY LOW-DENSITY PARITY CHECK CODES
CONV
‚àè
CONV
CONV
c1
h11
h21
f1
h11c1
h21c1
hm1
hm1c1
c2
h12
h22
f2
h12c2
h22c2
hm2
hm2c2
cn
h1n
h2n
fn
h1ncn
h2ncn
hmn
hmncn
qmn
rmn
rmn
qmn
Permuted
Depermuted
Figure 6.4
Generalized factor graph of a non-binary LDPC code [8].
6.5.3 Factor Graph of a Non-Binary LDPC Code
The factor graph of a non-binary LDPC code graphically shows the operation of the
sum‚Äìproduct decoding algorithm (Figure 6.4).
The factor graph for non-binary LPDC codes is similar to that for binary LPDC
codes, but we must now take into account the non-binary elements in the parity check
matrix, which are denoted as hij, i = 1, 2, . . ., m and j = 1, 2, . . ., n. In Figure 6.4
the number of parity check matrix elements connected to a coded symbol cj is the
column weight of the code and the number of connections to each parity check zi
is the row weight of the code. The likelihoods of each coded symbol fj are column
vectors containing the q likelihoods of the coded symbol being an element in GF(q).
The block labelled  connects the non-binary elements in each row to the parity
checks. In Figure 6.5 a generalized factor graph is shown, with FFT blocks replacing
the convolutional blocks, which reduces complexity.
A factor graph of the parity check matrix in (6.26) is show in Figure 6.6.
H =
Ô£Æ
Ô£ØÔ£∞
Œ±
0
1
Œ±
0
1
Œ±2
Œ±
0
1
1
0
0
Œ±
Œ±2
0
Œ±2
1
Ô£π
Ô£∫Ô£ª.
(6.26)
6.5.4 The Fast Fourier Transform for the Decoding of Non-Binary
LDPC Codes
In Section 6.3.4, the FFT was used to reduce the complexity of the horizontal step in the
sum‚Äìproduct algorithm. For binary decoding of LDPC codes, the matrix Q contains

DECODING NON-BINARY LDPC CODES WITH THE SUM‚ÄìPRODUCT ALGORITHM
225
c1
h11
h21
f1
h11c1
h21c1
hm1
hm1c1
c2
h12
h22
f2
h12c2
h22c2
hm2
hm2c2
cn
h1n
h2n
fn
h1ncn
h2ncn
hmn
hmncn
‚àè
qmn
rmn
rmn
qmn
Permuted
Depermuted
F
F
F
F
F
F
F
F
F
Figure 6.5
Generalized factor graph of a non-binary LDPC code showing the replacement of the
convolution operations with FFTs.
1 √ó 2 column vectors containing two probabilities, qmn(0) and qmn(1). The Fourier
transform of this column vector is then obtained by multiplying by the Hadamard
matrix deÔ¨Åned in (6.21).
However, for non-binary LDPC codes deÔ¨Åned over GF(q), Q contains 1 √ó q column
vectors and the Fourier transform is obtained by multiplying by the tensor product of
c1
c2
c3
c4
c5
c6
Œ±
Œ±2
Œ±
Œ±
1
Œ±2
Œ±
1
1
Œ±2
1
1
f1
f2
f3
f4
f5
f6
Œ±c1
Œ±2c1
Œ±c2
Œ±c2
c3
Œ±2c3
Œ±c4
c4
c5
Œ±2c5
c6
c6
F
F
F
F
F
F
F
F
F
F
F
F
Figure 6.6
Factor graph for the parity check matrix of (6.26).

226
NON-BINARY LOW-DENSITY PARITY CHECK CODES
+
+
+
‚àí
+
+
+
‚àí
+
+
+
‚àí
+
+
+
‚àí
qmn(0) ‚â°qmn(00)
qmn(1) ‚â°qmn(01)
qmn(Œ±) ‚â°qmn(10)
qmn(Œ±2) ‚â°qmn(11)
F(qmn(0))
F(qmn(1))
F(qmn(Œ±))
F(qmn(Œ±2))
Figure 6.7
Radix-2 butterÔ¨Çy diagram for GF(4).
Hadamard matrices. An example of the tensor product of two Hadamard matrices is
given below:
W4 = W2 ‚äóW2 =

W2 W2
W2 ‚àíW2

= 1
2
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
1
1
1
1
‚àí1
1
‚àí1
1
1
‚àí1
‚àí1
1
‚àí1
‚àí1
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
Therefore, for an LDPC code deÔ¨Åned over GF(4), the Fourier transform of the 1 √ó
4 column vector of probabilities qmn(0), qmn(1), qmn(Œ±) and qmn(Œ±2) is:
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
F (qmn(0))
F (qmn(1))
F (qmn(Œ±))
F

qmn(Œ±2)

Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª= 1
2
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
1
1
1
1
‚àí1
1
‚àí1
1
1
‚àí1
‚àí1
1
‚àí1
‚àí1
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
qmn(0)
qmn(1)
qmn(Œ±)
qmn(Œ±2)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
As before, the inverse FFT is achieved by multiplying by the Hadamard matrix, that
is W4W4 = I4. The FFT operation can be expressed in terms of its radix-2 butterÔ¨Çy
diagram. An example of the radix-2 butterÔ¨Çy diagram for GF(4) is shown in Figure
6.7.
The ordering of the probabilities qmn(x) is very important when determining the
FFT. It is essential that for each pair of qmn(x), the binary representation of the Ô¨Ånite
Ô¨Åeld elements x differ by one bit. In Figure 6.8, the radix-2 butterÔ¨Çy diagram for
GF(16) is also given.
Example 6.5: Decoding a non-binary LDPC code with the FFT sum‚Äìproduct
algorithm: In this example we decode the (6, 3) LDPC code deÔ¨Åned over
GF(4) with a parity check matrix given by (6.23). We assume that the trans-
mitted code word was c = [2 1 0 3 0 2 ] and the received word is r =
[2 1 0 2 2 2], with two errors highlighted. Furthermore, it is assumed that

DECODING NON-BINARY LDPC CODES WITH THE SUM‚ÄìPRODUCT ALGORITHM
227
+
+
+
‚àí
+
+
+
‚àí
+
+
+
‚àí
+
+
+
‚àí
+
+
+
‚àí
+
+
+
‚àí
+
+
+
‚àí
+
+
+
‚àí
+
+
+
‚àí
+
+
+
+
+
+
‚àí
‚àí
+
+
+
‚àí
+
+
+
‚àí
+
+
+
‚àí
+
+
+
‚àí
+
+
+
‚àí
+
+
+
‚àí
+
+
+
‚àí
+
+
+
‚àí
+
+
+
‚àí
+
+
+
‚àí
+
+
+
‚àí
+
+
+
‚àí
+
+
+
‚àí
+
+
‚àí
+
+
+
‚àí
+
+
+
+
‚àí
+
+
+
+
‚àí
+
+
+
+
‚àí
+
+
+
+
‚àí
+
+
+
+
‚àí
+
+
+
+
‚àí
+
qmn(0) ‚â°qmn(0000)
qmn(1) ‚â°qmn(0001)
qmn(Œ±) ‚â°qmn(0010)
qmn(Œ±4) ‚â°qmn(0011)
qmn(Œ±2) ‚â°qmn(0100)
qmn(Œ±8) ‚â°qmn(0101)
qmn(Œ±5) ‚â°qmn(0110)
qmn(Œ±10) ‚â°qmn(0111)
qmn(Œ±3) ‚â°qmn(1000)
qmn(Œ±9) ‚â°qmn(1010)
qmn(Œ±7) ‚â°qmn(1011)
qmn(Œ±6) ‚â°qmn(1100)
qmn(Œ±13) ‚â°qmn(1101)
qmn(Œ±11) ‚â°qmn(1110)
qmn(Œ±14) ‚â°qmn(1001)
qmn(Œ±12) ‚â°qmn(1111)
F(qmn(0))
F(qmn(1))
F(qmn(Œ±))
F(qmn(Œ±4))
F(qmn(Œ±2))
F(qmn(Œ±8))
F(qmn(Œ±5))
F(qmn(Œ±10))
F(qmn(Œ±3))
F(qmn(Œ±14))
F(qmn(Œ±9))
F(qmn(Œ±7))
F(qmn(Œ±6))
F(qmn(Œ±13))
F(qmn(Œ±11))
F(qmn(Œ±12))
Figure 6.8
Radix-2 butterÔ¨Çy diagram for GF(16).
the received symbols have the following likelihoods:
f =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
0.182512
0.149675
0.444988
0.044678
0.412355
0.320530
0.046118
0.723055
0.324187
0.030350
0.073539
0.079952
0.615774
0.021827
0.133538
0.550805
0.436298
0.479831
0.155596
0.105443
0.097286
0.374167
0.077809
0.119687
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
(6.27)

228
NON-BINARY LOW-DENSITY PARITY CHECK CODES
where each column in f contains the likelihoods of a received symbol being 0, 1, Œ±
or Œ±2. The matrix Q is then initialized as shown in (6.21). However, as described
earlier, the elements in Q must be permuted due to the received coded symbols
being multiplied by a non-binary element in the parity check matrix. The permuted
version of Q is given in (6.28):
Q =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0.182512
0
0.444988
0.044678
0
0.320530
0.046118
0
0.324187
0.030350
0
0.079952
0.615774
0
0.133538
0.550805
0
0.479831
0.155596
0
0.097286
0.374167
0
0.119687
0.182512
0.149675
0
0.044678
0.412355
0
0.046118
0.723055
0
0.030350
0.073539
0
0.615774
0.021827
0
0.550805
0.436298
0
0.155596
0.105443
0
0.374167
0.077809
0
0
0.149675
0.444988
0
0.412355
0.320530
0
0.723055
0.324187
0
0.073539
0.079952
0
0.021827
0.133538
0
0.436298
0.479831
0
0.105443
0.097286
0
0.077809
0.119687
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(6.28)
Permuted Q
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0.182512
0
0.444988
0.044678
0
0.320530
0.155596
0
0.324187
0.374167
0
0.079952
0.046118
0
0.133538
0.030350
0
0.479831
0.615774
0
0.097286
0.550805
0
0.119687
0.182512
0.149675
0
0.044678
0.412355
0
0.615774
0.105443
0
0.030350
0.073539
0
0.155596
0.723055
0
0.550805
0.436298
0
0.046118
0.021827
0
0.374167
0.077809
0
0
0.149675
0.444988
0
0.412355
0.320530
0
0.105443
0.133538
0
0.436298
0.079952
0
0.723055
0.097286
0
0.077809
0.479831
0
0.021827
0.324187
0
0.073539
0.119687
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(6.29)

DECODING NON-BINARY LDPC CODES WITH THE SUM‚ÄìPRODUCT ALGORITHM
229
We now perform the FFT operation on each of the 1 √ó 4 column vectors. For
example, the Fourier transform of the column vector in the top-left-hand corner of
the permuted Q matrix is determined by:
F
Ô£´
Ô£¨Ô£¨Ô£¨Ô£≠
q11(0)
q11(1)
q11(Œ±)
q11(Œ±2)
Ô£∂
Ô£∑Ô£∑Ô£∑Ô£∏= 1
2
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
1
1
1
1 ‚àí1
1 ‚àí1
1
1 ‚àí1 ‚àí1
1 ‚àí1 ‚àí1
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
0.182512
0.155596
0.046118
0.615774
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
= 1
2
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
0.182512 + 0.155596 + 0.046118 + 0.615774
0.182512 ‚àí0.155596 + 0.046118 ‚àí0.615774
0.182512 + 0.155596 ‚àí0.046118 ‚àí0.615774
0.182512 ‚àí0.155596 ‚àí0.046118 + 0.615774
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
F
Ô£´
Ô£¨Ô£¨Ô£¨Ô£≠
q11(0)
q11(1)
q11(Œ±)
q11(Œ±2)
Ô£∂
Ô£∑Ô£∑Ô£∑Ô£∏= 1
2
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
‚àí0.542740
‚àí0.323783
0.596571
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
The FFT of each column vector in the permuted Q matrix is given in (6.30):
F(Q) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
0
1
1
0
1
‚àí0.542740
0
0.157053
‚àí0.849944
0
0.600723
‚àí0.323783
0
0.538351
‚àí0.162310
0
‚àí0.199036
0.596571
0
0.084549
0.190965
0
‚àí0.119566
1
1
0
1
1
0
‚àí0.323783
0.745461
0
0.190965
0.697305
0
0.596571
‚àí0.489765
0
‚àí0.849944
‚àí0.028213
0
‚àí0.542740
‚àí0.656996
0
‚àí0.162310
‚àí0.019673
0
0
1
1
0
1
1
0
0.745461
0.084549
0
‚àí0.019673
0.600723
0
‚àí0.489765
0.157053
0
0.697305
‚àí0.199036
0
‚àí0.656996
0.538351
0
‚àí0.0282143
‚àí0.119566
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(6.30)
To complete the horizontal step we apply (6.20) to determine the probabilities
rmn(x). To determine r11(x) we take the product of the Fourier transformed column
vectors in F(Q) corresponding to the third, fourth and sixth received symbols and

230
NON-BINARY LOW-DENSITY PARITY CHECK CODES
then apply the inverse FFT to obtain a 1 √ó 4 column vector containing r11(x):
r11(x) = F‚àí1
# 
n‚ààN1/1
F (q1n(x))
$
= F‚àí1
Ô£´
Ô£¨Ô£¨Ô£¨Ô£≠
1
2
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
0.157053
0.538351
0.084549
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª√ó
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
‚àí0.849944
‚àí0.162310
0.190965
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª√ó
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
0.600723
‚àí0.199036
‚àí0.119566
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
Ô£∂
Ô£∑Ô£∑Ô£∑Ô£∏
= F‚àí1
Ô£´
Ô£¨Ô£¨Ô£¨Ô£≠
1
2
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
‚àí0.080188
0.0173917
‚àí0.003222
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
Ô£∂
Ô£∑Ô£∑Ô£∑Ô£∏
= 1
4
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
1
1
1
1
‚àí1
1
‚àí1
1
1
‚àí1
‚àí1
1
‚àí1
‚àí1
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
‚àí0.080188
0.0173917
‚àí0.003222
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
= 1
4
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
0.933982
1.100802
0.905642
1.059574
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
0.233818
0.274878
0.226088
0.265216
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
The remaining probabilities rmn(x) are given in (6.31):
R =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0.233818
0
0.313258
0.244365
0
0.277593
0.274878
0
0.181512
0.272982
0
0.236553
0.226088
0
0.325298
0.230033
0
0.258631
0.265216
0
0.179931
0.252620
0
0.227223
0.271356
0.242364
0
0.208230
0.286092
0
0.222772
0.264788
0
0.295891
0.338076
0
0.278277
0.236078
0
0.207616
0.190862
0
0.227595
0.256769
0
0.288262
0.184970
0
0
0.244755
0.264237
0
0.273865
0.238776
0
0.244347
0.269750
0
0.233789
0.234406
0
0.254745
0.231358
0
0.245066
0.260604
0
0.256153
0.234655
0
0.247279
0.266214
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(6.31)

DECODING NON-BINARY LDPC CODES WITH THE SUM‚ÄìPRODUCT ALGORITHM
231
The matrix R is now complete, but it must be depermuted before it can be used
in the vertical step. The depermuted R is given in (6.32):
Depermuted R
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0.233818
0
0.313258
0.244365
0
0.277593
0.226088
0
0.181512
0.230033
0
0.236553
0.265216
0
0.325298
0.252620
0
0.258631
0.274878
0
0.179931
0.272982
0
0.227223
0.271356
0.242364
0
0.208230
0.286092
0
0.227595
0.236078
0
0.295891
0.338076
0
0.222772
0.256769
0
0.207616
0.190862
0
0.278277
0.264788
0
0.288262
0.184970
0
0
0.244755
0.264237
0
0.273865
0.238776
0
0.254745
0.234655
0
0.247279
0.234406
0
0.256153
0.269750
0
0.233789
0.260604
0
0.244347
0.231358
0
0.245066
0.266214
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(6.32)
The vertical step is carried out in the same way as for the binary case. To
determine q11(x) we apply (6.13):
q11(0) = Œ≤11 f 0
1

m‚ààM1/1
rm1(0) = Œ≤11 √ó 0.182512 √ó 0.271356 = 0.049526Œ≤11
q11(1) = Œ≤11 f 1
1

m‚ààM1/1
rm1(1) = Œ≤11 √ó 0.046118 √ó 0.227595 = 0.010496Œ≤11
q11(Œ±) = Œ≤11 f Œ±
1

m‚ààM1/1
rm1(Œ±) = Œ≤11 √ó 0.615774 √ó 0.222772 = 0.137177Œ≤11
q11(Œ±2) = Œ≤11 f Œ±2
1

m‚ààM1/1
rm1(Œ±2) = Œ≤11 √ó 0.155596 √ó 0.278277 = 0.043299Œ≤11
Œ≤11 =
1
0.049256 + 0.010496 + 0.137177 + 0.043299 = 4.162712
q11(0) = 0.049526Œ≤11 = 0.205930
q11(1) = 0.010496Œ≤11 = 0.043644
q11(Œ±) = 0.137177Œ≤11 = 0.570388
q11(Œ±2) = 0.043299Œ≤11 = 0.180039
.

232
NON-BINARY LOW-DENSITY PARITY CHECK CODES
The complete matrix Q is given in (6.33):
Q =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0.205930
0
0.46625
0.038683
0
0.303488
0.043644
0
0.301653
0.037341
0
0.074315
0.570388
0
0.142839
0.475497
0
0.495852
0.180039
0
0.089252
0.448479
0
0.126345
0.164650
0.145266
0
0.042123
0.447806
0
0.040229
0.730398
0
0.026937
0.072108
0
0.630104
0.022170
0
0.536854
0.404473
0
0.165017
0.102165
0
0.394068
0.075612
0
0
0.150837
0.537825
0
0.490530
0.343296
0
0.709767
0.227035
0
0.103376
0.072970
0
0.023304
0.167601
0
0.346251
0.478806
0
0.116092
0.067538
0
0.059844
0.104928
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(6.33)
Finally, the pseudo posterior probabilities are determined using (6.18) and are
given in (6.34):
Q =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
0.205930 0.145266 0.466255 0.038683 0.447806 0.303488
0.043644 0.730398 0.301653 0.037341 0.072108 0.074315
0.570388 0.022170 0.142839 0.475497 0.404473 0.495852
0.180039 0.102165 0.089252 0.448479 0.075612 0.126345
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(6.34)
From (6.19), taking the highest likelihood in each column of (6.34) gives
a decoded code word of ÀÜc = [Œ±
1
0
Œ±
0
Œ±]. The Ô¨Årst iteration of the
sum‚Äìproduct algorithm has corrected the Ô¨Åfth received symbol but there is still
the error in the fourth received symbol. It turns out that only one more iteration is
required to correct this error too, and the relevant matrices from the second iteration
are given. The updated matrix Q in (6.33) is permuted and the FFT is applied to
each column vector, as shown in (6.35). The matrix R is depermuted and shown
in (6.36), and the updated matrix Q and pseudo posterior probabilities are given in

DECODING NON-BINARY LDPC CODES WITH THE SUM‚ÄìPRODUCT ALGORITHM
233
(6.37) and (6.38) respectively:
F(Q) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
0
1
1
0
1
‚àí0.500852
0
0.218190
‚àí0.847952
0
0.598679
‚àí0.228062
0
0.535817
‚àí0.025675
0
‚àí0.244394
0.552635
0
0.111015
0.028361
0
‚àí0.140334
1
1
0
1
1
0
‚àí0.340666
0.751328
0
0.157955
0.704559
0
0.589508
‚àí0.505138
0
‚àí0.861880
‚àí0.039829
0
‚àí0.590243
‚àí0.665128
0
‚àí0.127581
‚àí0.046837
0
0
1
1
0
1
1
0
0.721208
0.210727
0
0.100747
0.644204
0
‚àí0.466142
0.410854
0
0.673561
‚àí0.167468
0
‚àí0.651719
0.529721
0
0.187811
‚àí0.103553
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(6.35)
Depermuted R
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0.223039
0
0.312657
0.238958
0
0.274386
0.221579
0
0.186628
0.228330
0
0.227183
0.276740
0
0.314472
0.256737
0
0.271947
0.278642
0
0.186243
0.275975
0
0.226484
0.276232
0.236345
0
0.206548
0.291534
0
0.225755
0.244699
0
0.287521
0.336792
0
0.232438
0.265419
0
0.203285
0.188251
0
0.265575
0.253537
0
0.302645
0.183422
0
0
0.239258
0.278016
0
0.291432
0.205369
0
0.267581
0.228322
0
0.226443
0.230132
0
0.255591
0.248275
0
0.224605
0.302287
0
0.237570
0.245388
0
0.257521
0.262212
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(6.36)

234
NON-BINARY LOW-DENSITY PARITY CHECK CODES
Q =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0.205545
0
0.485609
0.037950
0
0.252543
0.042447
0
0.290544
0.035886
0
0.070589
0.583537
0
0.130139
0.460471
0
0.556467
0.168472
0
0.093708
0.465692
0
0.120401
0.153792
0.137779
0
0.040705
0.471531
0
0.038606
0.744379
0
0.026422
0.065340
0
0.643804
0.021464
0
0.539167
0.384507
0
0.163797
0.096378
0
0.393706
0.078622
0
0
0.144486
0.535638
0
0.498018
0.333508
0
0.722661
0.232931
0
0.102603
0.068878
0
0.023662
0.161675
0
0.340255
0.494822
0
0.109191
0.069757
0
0.059124
0.102792
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(6.37)
Q =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
0.205545
0.137779
0.485609
0.037950
0.471531
0.252543
0.042447
0.744379
0.290544
0.035886
0.065340
0.070589
0.583537
0.021464
0.130139
0.460471
0.384507
0.556467
0.168472
0.096378
0.093708
0.465692
0.078622
0.120401
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
(6.38)
From the matrix Q, the decoded code word is now ÀÜc = [Œ± 1
0 Œ±2 0 Œ±],
which matches the original transmitted code word.
6.6 Conclusions
A very important class of block code known as the low-density parity check (LDPC)
code has been explained, with discussions on some structured construction methods
for binary and non-binary LDPC codes. Additionally, a decoding algorithm called
the sum‚Äìproduct algorithm has been studied in detail for use with binary and non-
binary LDPC codes. A method to reduce the complexity of the sum‚Äìproduct algorithm
has been given, using fast Fourier transforms based on the Hadamard matrix, which
replaces the original horizontal step of the decoding algorithm.
The study of LDPC codes is very important as they are fast becoming one of the
more popular coding schemes for a number of future applications in wireless commu-
nications and eventually magnetic storage. The binary LDPC codes are well known
and perform as well as turbo codes, and in some cases can outperform turbo codes

REFERENCES
235
for large block lengths. Non-binary LDPC codes are less well known, but Mackay
showed that these codes outperform binary LDPC codes with further increases in
performance as the size of the Ô¨Ånite Ô¨Åeld increases [10]. This is of course at the ex-
pense of higher complexity, but non-binary LDPC codes also have better convergence
properties, requiring less iterations to decode a received word. With the inclusion of
the FFT proposed by Barnault et al. [8], the complexity is reduced and non-binary
LDPC codes can now be used practically in many future applications.
References
[1] Gallager, R.G. (1962) Low-density parity-check codes. IRE Transactions on Information Theory,
IT-8, 21‚Äì8.
[2] Mackay, D.J. and Neal, R.M. (1995) Good codes based on very sparse matrices. Cryptography and
Coding, 5th IMA Conference (ed. C. Boyd), Vol. 1025, pp. 100‚Äì11 (Lecture Notes in Computer
Science, Springer).
[3] Chung, S.Y., Forney, G.D. Jr., Richardson, T.J. and Urbanke, R. (2001) On the design of low-density
parity-check codes within 0.0045 dB of the Shannon limit. IEEE Communications Letters, 5 (2),
58‚Äì60.
[4] Ammar, B., Honary, B., Kou, Y. et al. (2004) Construction of low-density parity-check codes based
on balanced incomplete block designs. IEEE Transactions on Information Theory, 50 (6), 1257‚Äì69.
[5] Hu, X.-Y., Eleftheriou, E. and Arnold, D.-M. (2001) Progressive edge-growth tanner graphs. Pro-
ceedings of IEEE GlobeCom, San Antonio, Texas, pp. 995‚Äì1001.
[6] Kou, Y., Lin, S. and Fossorier, M.P.C. (2001) Low-density parity-check codes based on Ô¨Ånite
geometries: a rediscovery and new results. IEEE Transactions on Information Theory, 47 (7),
2711‚Äì36.
[7] Moon, T.K. (2005) Error Correction Coding. Mathematical Methods and Algorithms, Wiley Inter-
science, ISBN 0-471-64800-0.
[8] Barnault, L. and Declerq, D. (2003) Fast decoding algorithm for LDPC over GF(2q). IEEE Infor-
mation Theory Workshop, Paris, France, pp. 70‚Äì3.
[9] Sylvester, J.J. (1867) Thoughts on orthogonal matrices, simultaneous sign-successions, and tessel-
lated pavements in two or more colours, with applications to newton‚Äôs rule, ornamental tile-work,
and the theory of numbers. Phil. Mag., 34, 461‚Äì75.
[10] Davey, M.C. and Mackay, D.J. (1998) Low density parity check codes over GF(q). IEEE Information
Theory Workshop, Killarney, Ireland, pp. 70‚Äì1.
[11] Lin, S., Song, S., Zhou, B. et al. (2007) Algebraic constructions of non-binary quasi-cyclic LDPC
codes: array masking and dispersion. 9th International Symposium on Communication Theory and
Applications (ISCTA), Ambleside, Lake District, UK.


7
Non-Binary Convolutional Codes
7.1 Introduction
So far this book has dealt with many different types of binary and non-binary
block code, from simple parity check codes to the powerful low-density parity
check codes, but another important class of error-correcting code is the convolu-
tional code, developed by Elias in 1955 [1]. Convolutional codes differ from block
codes in that a block code takes a Ô¨Åxed message length and encodes it, whereas
a convolutional code can encode a continuous stream of data. Convolutional codes
also have much simpler trellis diagrams than block codes, and soft-decision decod-
ing can easily be realized using the Viterbi algorithm, explained later. However,
at higher code rates the performance of convolutional codes is poorer than that of
block codes.
In this chapter, the principles of convolutional codes are introduced. The convolu-
tional encoder is explained, followed by a description of its state table, tree diagram,
signal Ô¨Çow graph and trellis diagram. From the signal Ô¨Çow graph we can determine
the transfer function of the code, which gives us the Hamming distance of each code
word, beginning with the minimum Hamming distance of the code and the number of
nearest neighbours. This distance spectrum can be used to evaluate the performance
of the convolutional code.
The idea of trellis coded modulation(TCM) is then presented, whereby a convolu-
tional code is combined with the modulation process. By carefully partitioning the
constellation diagram, a bandwidth-efÔ¨Åcient TCM code can be constructed with a
signiÔ¨Åcant coding gain over an uncoded system.
Binary TCM coding is then extended to TCM codes deÔ¨Åned over rings of inte-
gers, known as ring-TCM codes. Ring-TCM encoding is explained and a method
to search for new ring-TCM codes using a genetic algorithm is presented. Finally,
ring-TCM codes are combined with spatial-temporal diversity to create space-time
ring-TCM (ST-RTCM) codes. The design criteria for good ST-RTCM codes are
Non-Binary Error Control Coding for Wireless Communication and Data Storage
Rolando Antonio Carrasco and Martin Johnston
C 2008 John Wiley & Sons, Ltd

238
NON-BINARY CONVOLUTIONAL CODES
D
D
Input
u = (u1, u2, ..., uk)
Output
c = (c1, c2, ..., c2k ‚Äì 1, c2k)
c2i ‚Äì 1, i = 1, 2, 3, ..., k
c2i , i = 1, 2, 3, ..., k
Figure 7.1
Nonsystematic convolutional encoder, R 1/2 with constraint length K = 3.
given and the performances of several ST-RTCM codes are presented on multiple-
input‚Äìmultiple-output (MIMO) fading channels, including urban environments such
as indoor, pedestrian and vehicular.
An example of a convolutional code is shown in Figure 7.1, where D denotes a
memoryelement.The encodertakes aninput uoflengthk bits andforeachbitproduces
two coded bits. These coded bits are collected using the switch to produce a code
word c of length n = 2k bits. In this case the input message cannot be easily recovered
from the code word since this is an example of a nonsystematic convolutional code.
The two memory elements of Figure 7.1 can only have four possible pairs of binary
values, 00, 01, 10 and 11. These values are called states and for a given input we
can construct a state table showing what the next state will be from the current state,
along with the corresponding output. The state table for the convolutional encoder of
Figure 7.1 is shown in Table 7.1.
We can write the encoding process of the convolutional code in terms of poly-
nomials, with U(D), G(D) and C(D) as the input, generator and output polynomials
respectively. For example, the message u = 110 100 is equivalent to U(D) = 1 + D +
D3. The convolutional encoder of Figure 7.1 has two generator polynomials, G1(D) =
1 + D + D2 and G2(D) = 1 + D2. Therefore, the code word polynomials are
Table 7.1
State table for the convolutional code of Figure 7.1.
Input
Current state
Next state
Output
0
00
00
00
1
00
10
11
0
01
00
11
1
01
10
00
0
10
01
10
1
10
11
01
0
11
01
01
1
11
11
10

INTRODUCTION
239
C1(D) = U(D)G1(D) and C2(D) = U(D)G2(D). For the message polynomial deÔ¨Åned
previously, the output will be:
C1(D) = U(D)G1(D)
= (1 + D + D3)(1 + D + D2) = 1 + D + D2 + D + D2 + D3
+D3 + D4 + D5
= 1 + D4 + D5 ‚â°100 011
C2(D) = U(D)G2(D)
= (1 + D + D3)(1 + D2) = 1 + D2 + D + D3 + D3 + D5
= 1 + D + D2 + D5 ‚â°111 001
.
Combining the two coded output gives c = 11 01 01 00 10 11.
Graphical displays of convolutional codes have proven invaluable over the years for
their understanding and analysis. A practically useful graphical representation is the
code tree or tree diagram of the code. A tree diagram is created by assuming zero initial
conditions for the encoder and considering all possible encoder input sequences. The
tree diagram for the convolutional code of Figure 7.1 is given in Figure 7.2.
Since the output of the convolutional encoder is determined by the input and the
state of the encoder, a more compact representation is the state diagram and signal
Ô¨Çow graph [2]. The state diagram is simply a graph of the possible states of the encoder
and the possible transmissions from one state to another. The signal Ô¨Çow graph for
the encoder of Figure 7.1 is shown in Figure 7.3.
The signal Ô¨Çow graph will be used to explain the method for obtaining the distance
properties of a convolutional code. We label the branches of the state diagram as D0,
0
1
00
00
00
00
11
10
01
11
00
01
10
00
11
10
01
11
00
01
10
11
10
01
11
00
01
10
11
10
01
11
Time
Input data bits
Figure 7.2
The tree diagram.

240
NON-BINARY CONVOLUTIONAL CODES
Xa
Xb
Xd
Xc
Xa‚Äô
D2
D
D
D
D0 = 1
D
D2
Figure 7.3
The signal Ô¨Çow graph.
D1 or D2, where the exponent of D denotes the Hamming distance between the coded
bits of each branch in the trellis and the coded bits of the all-zeros branch. If we
use the notation for S1, S2, S3 and S4 for state representation and replace Xa = S1,
Xb = S2, Xc = S3 and Xd = S2, we can write four state equations:
Xb = D2Xa + Xc.
(7.1)
Xc = DXb + DXd.
(7.2)
Xd = DXb + DXd.
(7.3)
Xa = D2Xc.
(7.4)
The transfer function for the code is deÔ¨Åned as:
T (D) = Xa
Xa
= D2Xc
Xa
.
(7.5)
Now we must express the numerator and denominator of T(D) in terms of the same
variable so that they will cancel out. First we can rearrange (7.3) to give:
Xd = DXb
1 ‚àíD .
We also observe that (7.2) and (7.3) are identical, so:
Xc = Xd
and therefore Xc = DXb
1‚àíD.
We now rearrange (7.1) so that:
Xa = Xb ‚àíXc
D2
=
Xb ‚àíDXb
1 ‚àíD
D2
=
 Xb(1 ‚àíD)
1 ‚àíD
‚àíDXb
1 ‚àíD

D2
= (1 ‚àí2D)Xb
D2 (1 ‚àíD) .

INTRODUCTION
241
We can also rewrite (7.4) as:
Xa = D2Xc = D3Xb
1 ‚àíD .
Finally, substituting Xa and Xa into (7.5) gives:
T (D) =
 D3Xb
1 ‚àíD

(1 ‚àí2D) Xb
D2 (1 ‚àíD)
 =
D3Xb
(1 ‚àí2D) Xb
D2
 =
D5Xb
(1 ‚àí2D) Xb
=
D5
(1 ‚àí2D).
We have expressed Xa and Xa in terms of Xb so that they cancel out. Evaluating
the transfer function gives:
D5 + 2D6 + 4D7 + 8D8 + ¬∑ ¬∑ ¬∑
1 ‚àí2D

D5
D5 ‚àí2D6
2D6
2D6 ‚àí4D7
4D7
4D7 ‚àí8D8
8D8 .
The transfer function tells us that there is one code word with a minimum Hamming
distance of 5, two code words with a Hamming distance of 6, four code words with a
Hamming distance of 7, and so on.
For a given input sequence, we can Ô¨Ånd the output by tracing through the trellis,
but without the exponential growth in branches as in the tree. Just as the encoding
operation for convolutional codes is quite different from the encoding operation for
linear block codes, the decoding operation for linear block codes and the decoding
process for convolutional codes proceed quite differently. Since we can represent
a transmitted code word for a convolutional code as a path through the trellis, the
decoding operation consists of Ô¨Ånding that path through a trellis which is ‚Äòalmost
like‚Äô the received binary sequence. As before with the linear block codes, we are
interested in hard-decision decoding and a decoder that immunizes the possibility of
error, and therefore, for a given received binary vector, the decoder Ô¨Ånds that path
through the trellis which has minimum Hamming distance from the received sequence.
Given a long received sequence of binary digits and a trellis similar to that in Figure
7.4, it would seem a quite a formidable task to search all possible paths for the best
path. However, there exists an illustrative procedure called the Viterbi algorithm [3, 4]
which simpliÔ¨Åes the process. This algorithm is a special case of what is called forward
dynamic programming.

242
NON-BINARY CONVOLUTIONAL CODES
00
11
00
11
10
01
00
11
11
00
10
01
10
01
00
11
10
01
00
11
States
Input is a 0
Input is a 1
Time
00
10
01
11
Figure 7.4
Trellis diagram for code.
A convolutional code can be made systematic [2] without affecting its minimum
distance properties by feeding back one of the outputs to the input. Such a code is
called a recursive systematic convolutional (RSC) code, and is the basic building block
for turbo codes, described in Chapter 8. An example of an RSC encoder, derived from
the nonsystematic encoder of Figure 7.1, is shown in Figure 7.5.
The decoder for the convolutional code Ô¨Ånds the most probable sequence of data
bits ÀÜu given the received sequence y:
ÀÜu = arg

max
u
P (u | y)

,
where y is the set of code symbols c observed through noise. The above equation can
be solved using the Viterbi algorithm [3, 4], explained next, in which case the solution
is termed the ‚Äòmaximum likelihood‚Äô or ML solution.
D
D
Input
Output
Figure 7.5
Recursive systematic convolutional (RSC) encoder.

INTRODUCTION
243
7.1.1 The Viterbi Algorithm
The Viterbi algorithm was proposed in 1967 [3] for decoding convolutional codes.
The application of the Viterbi algorithm to decoding convolutional codes is based on
the trellis diagram representing the convolutional encoder. As applied to our particular
problem, this principle states that the best (smallest Hamming distance) path through
the trellis that includes a particular node necessarily includes the best path from the
begging of the trellis to this node. What this means to us is that for each node in the
trellis we need to retain only the single best path to a node, this limiting the number
of retained paths at any time instant to the number of nodes in the trellis at that time.
The basic steps in the Viterbi algorithm are as follows [3, 4]:
1. Begin at time t = 0, at state 00.
2. Determine the distances between the Ô¨Årst received symbols associated with each
trellis path at the current time.
3. Select the path with the lowest distance at each state node and store both the path
and the distance.
4. If there is more than one path with the lowest distance, select one arbitrarily.
5. Increment time t and return to step 2. Add the lowest distance to the previous
lowest distance and store with the sequence of paths. This running score is called
the survivor score, and the corresponding sequence of paths, the survivor path.
6. Continue this process until all the symbols in the code word have been compared.
7. The resulting survivor path with the lowest survivor score is the most likely trans-
mitted code word.
7.1.2 Trellis Coded Modulation
In Chapter 3, the idea of coded modulation [5, 6] was introduced and applied to block
codes to form block coded modulation (BCM) codes. However, it is much more com-
mon to combine convolutional codes with modulation, resulting in trellis coded mod-
ulation (TCM) codes [6, 7]. The basic idea of TCM codes is illustrated in Figure 7.6,
u1
u2
1ku
1
1+
ku
2
1+
ku
2
1 k
ku
+
1
1
1
+
k
k
Convolutional
Encoder
Rate R =
1c
2c
1
1+
kc
Selects a 
subset of the
constellation
Selects a point
from the subset
Constellation 
Symbols
Input u of length k = k1 + k2
Figure 7.6
General idea of TCM.

244
NON-BINARY CONVOLUTIONAL CODES
which takes a binary message u = (u1, u2, . . . , uk1, uk1+1, . . . , uk1+k2) of length k =
k1 + k2 bits.
The BCM codes from Chapter 3 consisted of a multi-level block encoder, with the
component code with the largest minimum Hamming distance selecting the subset
of the constellation where the Euclidean distance between neighbouring points was
minimal. In the same way, a TCM code consists of a convolutional encoder, which
also selects a subset where the distance between points is minimal. The uncoded bits
then select points within the chosen subset in exactly the same way as the BCM code.
To Ô¨Ånd the free distance dfree of a TCM code we can draw its trellis diagram and
Ô¨Ånd the path with minimum Euclidean distance. Once determined, the asymptotic
coding gain (ACG) Œ≥ can be calculated by:
Œ≥ = d2
free/Œµ
d2min/Œµ ,
(7.6)
where d2
min is the squared Euclidian distance between points in the uncoded constel-
lation, Œµ is the energy of the coded constellation and Œµ is the energy of the uncoded
constellation. To maximize the free distance of the convolutional encoder, set parti-
tioning [6] must be applied to the chosen constellation, as explained in Chapter 3.
Figure 7.7 shows the set partitioning of the 8-PSK constellation.
An example of a TCM encoder for 8-PSK modulation is shown in Figure 7.8. In
this case, coded bit c1 selects a QPSK subset of the 8-PSK constellation, c2 selects a
BPSK subset of the QPSK subset and c3 selects a point within the BPSK subset.
0
1
2
3
4
5
6
7
0
2
4
6
1
3
5
7
0
4
1
5
2
6
3
7
0 4
2
6
1
5
3
7
Œµ
Œµ
2
Œµ
2
Œµ
2
Œµ
2
0
1
0
1
0
1
0
1
0
1
0
1
0
1
Œµ
0.765
c1
c1
c2
c2
c2
c2
c3
c3
c3
c3
c3
c3
c3
c3
000
001
010
011
100
101
110
111
Figure 7.7
The set partitioning of the 8-PSK constellation.

INTRODUCTION
245
D
D
8-PSK
u1
u2
c1
c2
c3
Output
Figure 7.8
A four-state TCM code for 8-PSK modulation.
The state table for the TCM code of Figure 7.8 is given in Table 7.2. From this we
can construct the trellis diagram of the TCM code, as shown in Figure 7.9.
From the trellis diagram we can see that there are two paths diverging from state
00 and converging at state 00 with minimum squared Euclidean distance d2
free = 4Œµ.
Since we are determining the ACG of a TCM code using 8-PSK modulation over
uncoded QPSK, the energies of both constellations are equal, that is Œµ = Œµ. The
minimum squared Euclidean distance between the points of QPSK is d2
min = 2Œµ and
so substituting into (7.6) gives:
Œ≥ = d2
free/Œµ
d2min/Œµ = 4Œµ/Œµ
2Œµ/Œµ = 4
2 = 2
or
3 dB.
From it can be seen that the large free distance of the TCM code more than
compensates for the smaller Euclidean distances between constellation points as a
result of expanding the size of the constellation.
Table 7.2
State table for the TCM code of Figure 7.8.
Inputs
Initial States
Next States
Outputs
u1
u2
S1
S2
S
1
S
2
c1
c2
c3
0
0
0
0
0
0
0
0
0
1
0
0
0
0
1
0
1
0
0
1
0
0
0
0
0
0
1
1
1
0
0
0
1
0
1
1
0
0
0
1
1
0
1
0
0
1
0
0
1
1
1
1
1
0
0
1
0
1
1
0
1
0
1
1
1
0
1
1
1
1
1
1
0
0
1
0
0
1
0
0
0
1
0
1
0
0
0
0
1
0
0
1
1
0
0
1
0
0
1
1
1
1
0
0
0
0
1
1
0
0
1
1
1
1
1
0
0
1
0
1
1
1
0
1
1
0
0
1
1
1
1
1
1
0
1
1
1
1
1
1
0
1
1
1

246
NON-BINARY CONVOLUTIONAL CODES
00
10
01
11
0
0
4
2
6
2
6
4
0
1
5
3
7
1
5
3
7
6
0426
2640
1537
3715
State
Figure 7.9
Trellis diagram of the TCM code of Figure 7.8.
7.1.3 TCM Based on Rings of Integers
Convolutional codes based on rings of integers modulo-M were Ô¨Årst presented by
Massey and Mittelholzer [8, 9]. They were followed by Baldini and Farrell [10,
11], who presented TCM codes based on rings of integers. Baldini and Farrell have
developed a number of modulo-M ring-TCM codes for M-PSK constellations and have
concluded that, due to the similarities between M-PSK signal sets and the algebraic
structure of rings of integers modulo-M, modulo-M ring-TCM codes are the natural
linear codes for M-PSK modulation.
Assuming that m information bits are transmitted per baud, the general structure
of a ring-TCM encoder suitable for M-PSK modulation, with M = 2m+1, is shown
in Figure 7.10. This ring-TCM encoder works as follows: Ô¨Årst, m + 1 information
bits, bi, are mapped into a modulo-M symbol, aj, according to a mapping function f
(for instance, f can be a Gray mapping function). Next, m modulo-M aj symbols are
introduced into a linear multi-level convolutional encoder (MCE), which generates
m + 1 modulo-M coded symbols, xk. Finally, each one of these coded symbols xk
is associated with a signal of the M-PSK signal set and is sent to the channel. As a
total of m + 1 modulo-M coded symbols xk are transmitted per single trellis branch,
ring-TCM codes can be considered as 2(m + 1)-dimensional TCM codes.
Propagation delays caused by a fading channel can result in phase shifting of the
transmitted signal. At the demodulator these phase shifts result in a rotation of the
received signals compared with the transmitted symbols. If, after these phase shifts,
the received word is another valid code word, that code is known as phase invariant.
m + 1 
xk
m
aj
m + 1 
bi
BINARY 
SOURCE 
MAPPING 
at = f(b1, b2, ‚Ä¶, bm+1)
MCE 
M-PSK 
SIGNAL SET 
Figure 7.10
General structure of a ring-TCM encoder suitable for M-PSK modulation.

INTRODUCTION
247
A multi-level convolutional code is 360/M phase invariant if and only if the all-one
code word can be found in the code [11]. A code with the all-one code word is said
to have a transparent encoder [11]. For example, an MCE deÔ¨Åned over Z4 is 360/4 =
90‚ó¶phase invariant if the all-one code word is present in the code.s
In general, transparent modulo-M ring-TCM codes can be readily designed, with
neither additional difÔ¨Åculty nor signiÔ¨Åcant decrease in performance with respect to
nontransparent ring-TCM codes. Furthermore, in addition to the transparency prop-
erty, ring-TCM codes present, in general, better coding gains than their nontransparent
Ungerboeck counterparts. However, the major drawback of these codes is that they
generally require a more computationally intensive decoding process. Therefore,
although it has already been established that ring-TCM schemes can constitute a pow-
erful alternative to conventional TCM schemes, their actual performance/complexity
tradeoff should be investigated.
There is also the necessity to extend the use of ring-TCM codes to other modulation
schemes than M-PSK constellations. In particular, the structure of a ring-TCM encoder
based on rings of integers modulo-4, which is suitable for any rectangular M-QAM
constellation, has been proposed [11]. This novel coded modulation scheme can result
in ring-TCM codes with excellent coding gains which are, in addition, transparent to
phase rotations of 360/4 = 90‚ó¶.
7.1.4 Ring-TCM Codes for M-PSK
This section describes a multi-level convolutional encoder (MCE) deÔ¨Åned over the
ring of integers modulo-M, ZM, which is especially suitable for combination with
signals of an M-PSK constellation. It is important to notice that we would have
preferred to deÔ¨Åne the MCE over a Ô¨Åeld of integers modulo-M rather than over a ring
of integers modulo-M. However, due to the fact that the Ô¨Åeld requires M to be a prime
integer, it is not suitable for most practical M-PSK constellations, which include a
composite number of signal points M = 2k, with k some positive integer.
A rate (m/p) MCE deÔ¨Åned over the ring of integers modulo-M, ZM, is a time-
invariant linear Ô¨Ånite-state sequential circuit that, having m information input symbols
(a1, a2, . . . , am) at a time deÔ¨Åned over ZM, generates p encoded output symbols
(x1, x2, . . . , x p) at a time deÔ¨Åned over ZM, where the coefÔ¨Åcients of the MCE also
belong to ZM, and all arithmetic operations satisfy the properties of the ring of integers
modulo-M. In order to achieve the same transmission rate as uncoded modulation, the
value of p is chosen so that p = m + 1. The modulo-M ring-TCM MCE [11] suitable
for M-PSK modulation is shown in Figure 7.11.
The information sequence of symbols, ai, is shifted into the encoder, beginning at
time zero, and continuing indeÔ¨Ånitely. The stream of incoming information symbols,
ai (with 0 ‚â§i < ‚àû), is broken into segments called information frames, each one con-
taining m symbols of ZM. At discrete time n, an information frame can be represented

248
NON-BINARY CONVOLUTIONAL CODES
g
g
g
2
1g
m
g1
1
sg
2
sg
m
sg
D
D
D
a1
a2
am
x1
x2
xm
xp
Figure 7.11
Ring-TCM MCE.
as the vector:
¬Øan = [a1, a2, . . . , am]n.
(7.7)
In order to keep the same transmission rate as uncoded (M/2)-PSK (that is, m
information bits per channel symbol) by using the expanded rectangular M-PSK signal
set, the rate of the MCE must be (m/p), where 2p = m. Therefore, each information
frame ¬Øan must generate an output coded frame ¬Øxn containing p symbols of ZM:
¬Øxn = [x1, x2, . . . , x p]n.
(7.8)
The MCE is characterized by the generator matrix G(D), which maps the input
information frames ¬Øan into the output coded frames ¬Øxn and which can be written in
the functional form [12, 13]:
¬Øxn(D) = ¬Øan(D) ¬∑ G(D),
(7.9)
where D is the delay operator and:
G(D) = [Im|P(D)] =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
Im
g(1)(D)/f (D)
g(2)(D)/f (D)
...
g(m)(D)/f (D)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
(7.10)

INTRODUCTION
249
where the entries of the matrix G(D):
g(i)(D) = g(i)
s Ds + ¬∑ ¬∑ ¬∑ + g(i)
2 D2 + g(i)
1 D + g(i)
0 (Feed forward polynomial)
f (D) = fs Ds + ¬∑ ¬∑ ¬∑ + f2D2 + f1D + 1
(Feedback polynomial)
(7.11)
are polynomials with coefÔ¨Åcients gi and fi belonging to ZM, and Im is the (m √ó m)
identity matrix, as the MCE is systematic.
The parity check matrix H(D) is a (1 √ó p) matrix that satisÔ¨Åes:
G(D) ¬∑ H(D)T = 0,
(7.12)
where H(D) is readily obtained from G(D) following:
H(D) = [‚àíP(D) : I1].
(7.13)
Every state q of the MCE is unique and can be deÔ¨Åned as:
q =
s

i=1
Mi‚àí1 ¬∑ v(n)
i ,
(7.14)
where v(n)
i , which belongs to ZM, represents the value of the ith memory element at
discrete time n, and is given by:
v(n)
i
= (modulo ‚àíM)
Ô£´
Ô£≠fi ¬∑ x p +
m

j=1
g( j)
i
¬∑ a j + v(n‚àí1)
i+1
Ô£∂
Ô£∏for
1 ‚â§i < s
v(n)
i
= (modulo ‚àíM)
Ô£´
Ô£≠fi ¬∑ x p +
m

j=1
g( j)
i
¬∑ a j
Ô£∂
Ô£∏
for
s = 1
,
(7.15)
with s denoting the constraint length of the MCE (number of memory elements). It
can be determined by examination of G(D) that:
s = max
i [degree g(i)(D)].
(7.16)
The number of states of the MCE, nst, represented in the trellis diagram as the
number of nodes in each column, is given by [14]:
nst ‚â§Ms.
(7.17)
The reason for the inequality in (7.17) is that every v(n)
i
in (7.15) can take on either
M or M/2 or M/4,. . ., or M/2m different values of ZM at a time, depending on the
coefÔ¨Åcients gi and fi (with 1 ‚â§i < s). So, nst = Ms only in the case that all v(n)
i
take
on the M possible values of ZM; otherwise, nst < Ms. For instance, as a clear example,
if all coefÔ¨Åcients gi and fi are even, then all v(n)
i
will also be even, and nst < Ms.

250
NON-BINARY CONVOLUTIONAL CODES
Each information frame ¬Øan causes the MCE to change state. This is represented by
a branch or transition to the next node in the trellis. There are Mm branches entering
each state and Mm branches leaving each state. Each branch is labelled with a coded
frame ¬Øxn. Therefore, any encoded frame sequence can be found by walking through
the appropriate path in the trellis. It becomes very common that trellis diagrams
of MCEs have a sizeable number of parallel transitions between states (especially
encoders with a low number of states), due to the sizeable number of total transitions
of their trellis codes. In fact, the minimum number of parallel branches, np, occurs for
a fully-connected trellis diagram; that is, every state of the trellis is connected to all
others and itself, and is given by [14]:
n p ‚â•Mm/M S = M(m‚àíS),
(7.18)
where (m ‚â•s). If s > m, the trellis diagram can never be fully connected.
7.1.5 Ring-TCM Codes Using Quadrature Amplitude Modulation
In this section, a novel 4D TCM technique deÔ¨Åned over Z4, suitable for rectangular
M-QAM signal sets, is described [12]. There are many real channels that require the
use of transparent, or at least RI TCM, codes. We have presented the design of transpar-
ent ring-TCM codes suitable for M-PSK constellations, which have a minimum phase
ambiguity of 360/M‚ó¶and require the proper combination of a (360/M)‚ó¶RI MCE with
a (360/M)‚ó¶code-to-signal mapping. Now the design of RI and transparent ring-TCM
codes for rectangular quadrature amplitude modulation (QAM) is explained. These
have a phase ambiguity of 90‚ó¶. The functional block diagram of the 4D ring-TCM
encoder [12] is shown in Figure 7.12. It is similar to the ring-TCM transmitter for
M-PSK signal sets in Figure 7.11, but there are two important differences [12]:
1. 4D ring-TCM codes suitable for rectangular M-QAM constellation (where M = 4i,
with i a positive integer greater than 0) are always deÔ¨Åned over the ring of integers
modulo-4, Z4. That is, both bit-to-symbol mapping (i.e. Gray mapping) and MCE
are deÔ¨Åned over Z 4. This particular ring deÔ¨Ånition is required in order to resolve
the phase ambiguities of rectangular M-QAM signal sets.
Linear
MCE
M-QAM
Signal
Mapping
M-QAM
Modulator
x1
xp/2
x(p/2)+1
xp
s1
s2
s(t)
ai
Figure 7.12
Block diagram of a 4D ring-TCM transmitter suitable for rectangular M-QAM.

INTRODUCTION
251
2. The code-to-signal mapping must properly assign the output of the MCE to the
signals of the 2D M-QAM constellation. In this particular case, the p encoded
output symbols of the MCE must now be split into two sets containing ( p/2)
symbols each, x(1) and x(2), where:
x(1) =

x1, x2, . . . , x p/2

x(2) =

x(p/2)+1, x(p/2)+2, . . . , x p
.
(7.19)
Each set of symbols is then mapped onto one signal of the 2D M-QAM signal
set, sk = f (x(k)), according to some set partitioning rules that guarantee a certain
minimum distance between coded sequences. The two coded 2D M-QAM signals,
s1 and s2, are then modulated and transmitted on the channel using time division,
forming the 4D coded signal, s(t).
Let us deÔ¨Åne the isomorphism œà(Z4) = P as the mapping of the elements of the ring
of integers modulo-4, Z4 = {0, 1, 2, 3}, onto the phase rotation they represent, P =
{0‚ó¶, 90‚ó¶, 180‚ó¶, 270‚ó¶}. Then, following (7.19), groups of ( p/2) output symbols of the
MCE (x1, x2, . . ., x(p/2)) must be mapped onto one signal of the expanded rectangular
M-QAM signal set. Also, all the symbols must be used in the mapping process with
equal frequency.
A 90‚ó¶RI code-to-signal mapping requires that any pair of signals in the rectangular
M-QAM constellation which have the same radius but are Œ±¬∑90‚ó¶apart, where Œ± ‚ààZ4,
be assigned the groups of symbols (x1, x2, . . ., x(p/2)) and Œ±.(x1, x2, . . ., x(p/2)).
Some possible transparent mappings for 16-QAM used by Tarokh [15], and 16-
QAM and 64-QAM from Carrasco and Farrell are shown in Figure 7.13a, b and c,
respectively [13].
7.1.6 Searching for Good Ring-TCM Codes
The choice of feed-forward and feedback coefÔ¨Åcients of the MCE can signiÔ¨Åcantly
change the parameters of the code, such as the free distance, dfree, and so it is important
to have methods for Ô¨Ånding good TCM codes. The most obvious method would be
to apply an exhaustive search algorithm where all values of the coefÔ¨Åcients are tried,
but this would become too time-consuming for larger codes deÔ¨Åned over larger rings.
There are two criteria for good TCM codes over a Gaussian channel: maximizing
the free distance and minimizing the number of paths in the code trellis with Euclidean
distance equal to the free distance, denoted as Nfree, subject to the feed-forward and
feedback coefÔ¨Åcients [16]:
max dfree

g(i)
s , . . . , g(i)
0 , fs, . . . , f1

(7.20)
min Nfree

g(i)
s , . . . , g(i)
0 , fs, . . . , f1

.
(7.21)

252
NON-BINARY CONVOLUTIONAL CODES
00
02
01
03
11
10
12
13
20
21
23
22
31
30
32
33
Q
I
(a)  
00
10
01
11
02
03
13
12
30
31
21
20
32
33
23
22
Q
I
(b)  
Figure 7.13
QAM constellation mapping (a) Tarokh‚Äôs 16-QAM, (b) Carrasco and Farrell 16-QAM,
(c) Carrasco and Farrell 64-QAM.

INTRODUCTION
253
122
123
132
133
121
120
131
130
112
113
102
103
111
110
101
100
201
200
211
210
212
213
202
203
221
220
231
230
233
222
223
232
022
023
032
033
021
020
031
030
012
013
002
003
011
010
001
000
311
310
301
300
312
313
302
303
321
320
331
330
322
323
332
333
Q
I
(c)
Figure 7.13
(Continued)
In the following section, a much faster search algorithm for Ô¨Ånding good ring-TCM
codes for M-PSK and M-QAM is presented.
7.1.7 Genetic Algorithm
For the solution of optimization problems, a genetic algorithm [17] has been inves-
tigated for several applications in signal processing, such as speech compression,
and has been shown to be effective at exploring a large and complex search space
in an adaptive way. Next, a formal deÔ¨Ånition of genetic algorithms is given, and the
operators used in this work are described.
A genetic algorithm can be deÔ¨Åned as the nine-tuple:
GA =

p0, S, Œª, L, f, s, c, m, T

, where
pt = (at
1, . . . , at
Œª) = population in the tth generation
S = search space in which the chromosome is encoded
a0
Œª ‚ààS= elements of the search space
Œª = population size
L = length of chromosome
f : S ‚ÜíR = Ô¨Åtness function

254
NON-BINARY CONVOLUTIONAL CODES
s : SŒª ‚ÜíS = selection operator
c : S √ó S ‚ÜíS √ó S = crossover operator
m : S ‚ÜíS = mutation operator
T : SŒª ‚Üí{0, 1} = termination criterion.
In searching for TCM codes, the genetic algorithm basically selects and classiÔ¨Åes
the generator code word with a high degree of adaptation as parents generate a new
generation by the combination of their components, and by the elimination of the
weakest generators from the population [18]. In genetic algorithms there are many
operators but the selection, crossover and mutation operators are only used for code
searching.
7.1.7.1 Selection Operator
Assuming that the initial population p0 = (a0
1, . . . , a0
Œª) can be obtained in a heuristic
or random way from the search space, S, in which the chromosome is encoded, the next
generation is obtained from members of the previous generation using a stochastic
process, which guarantees that the number of times that one structure is selected is
dependent on its performance compared with the rest of the population.
The parent-selection operation s, s : SŒª ‚ÜíS produces an intermediate population
pt = (at
1, . . . , at
Œª) from the population pt = (at
1, . . . , at
Œª) in the tth generation, s(pt) =
pt. Any at
i = at
q in the pt is selected by a given random number Œ±i satisfying the
following condition:
0 ‚â§Œ±i ‚â§
Œª

j=1
f

at
j

.
(7.22)
The index q is obtained from:
q = max
:
k
;
‚àÄk ‚àà{1, . . . , Œª} , s.t.Œ±i ‚â§
Œª

k=1
f

at
k

<
,
(7.23)
where Œª
k=1 f

at
k

is the summation of all the Ô¨Åtness from population pt for all
members of the population picking up the Ô¨Årst index k that reaches Œ±i ‚â§Œª
k=1
f (at
k).
7.1.7.2 Crossover Operator
For any selected chromosomes in a population pt, an associated real value 0 ‚â§œÅ ‚â§1
is generated randomly. If œÅ is greater than the deÔ¨Åned crossover threshold œÅc,
where 0 ‚â§œÅc ‚â§1, the crossover operator c : S √ó S ‚ÜíS √ó S is applied to this pair
of chromosomes. The strategy used in this work is the one-point crossover, cop,

INTRODUCTION
255
which produces an intermediate population pt from the population pt and is deÔ¨Åned
below:

at
i
at
(i+1)

= Cop

at
i
at
(i+1)

‚àÄat
i ‚ààpt/i ‚àà{1, . . . , Œª}
= cop
[ai,œÅ1, ai,œÅ2, . . . , ai,œÅj, ai,(œÅj+1), . . . , ai,œÅL]T
[a(i+1),œÅ1, a(i+1),œÅ2, . . . , a(i+1),œÅj, a(i+1),(œÅj+1), . . . , a(i+1),œÅL]T

= cop
[ai,œÅ1, ai,œÅ2, . . . , ai,œÅj, a(i+1),(œÅj+1), . . . , a(i+1),œÅL]T
[a(i+1),œÅ1, a(i+1),œÅ2, . . . , a(i+1),œÅj, ai,(œÅj+1), . . . , ai,œÅL]T

.
(7.24)
The pair of chromosomes is separated into two subchromosomes at œÅj, hence a new
pair is composed by swapping the second subchromosome, where each one is crossed
from œÅ j+1 to œÅL.
7.1.7.3 Mutation Operator
For any chromosome in a population pt, an associated real value 0 ‚â§œÅ ‚â§1 is gener-
ated randomly. If œÅ is less than the deÔ¨Åned mutation threshold œÅm, where 0 ‚â§œÅm ‚â§1,
the mutation operator is applied to the chromosome. The mutation operator simply al-
ters one bit in a chromosome from 0 to 1 (or 1 to 0). The mutation operator m : S ‚ÜíS
produces an intermediate population pt from the population pt, as below:
at
i = m(at
i )‚àÄi ‚àà{1, . . . , Œª}
at
i,œÅk =
ai,œÅk
for
k ‚àà{1, 2, . . . , p ‚àí1, p + 1, . . . , L}
¬Øai,œÅk
for
k = p
.
(7.25)
The reproduction and crossover operators give to the genetic algorithms the bulk
of their processing power. The mutation operator is needed because, even though
reproduction and crossover are the main operators, occasionally they may become
overzealous and lose some potentially useful genetic material.
Usually there are four parameters to control the evolution of the genetic algorithm.
They are: the population size, Œª; the crossover threshold, œÅc; the mutation threshold,
œÅm; and the number of generations. If three of these are kept Ô¨Åxed, an optimum value
for the free parameter can be found in order to produce the optimum code rate, dfree
and Nfree.
The disadvantage of the exhaustive search algorithm is the time taken to generate
codes, especially when a high number of states and a high-order constellation are
used. It is important to note that the exhaustive search for the rectangular M-QAM
constellation is more computationally intensive than that for M-PSK constellations,

256
NON-BINARY CONVOLUTIONAL CODES
because the rectangular M-QAM constellation requires that, to determine dfree and
Nfree, all paths in the trellis be examined and compared to each other. The Ô¨Åtness
function used in this work is an a priori function based on the computation of the
distances.
Once the code is obtained, a further check is required to investigate the performance
in terms of symbol error rate as a function of signal-to-noise ratio. The application
of the genetic algorithm to 16-QAM is shown with new codes for this particular
modulation scheme to demonstrate its feasibility, when the number of states is in-
creased to produce better constraint lengths. The application of the genetic algorithm
to the code-to-signal mapping for 16-QAM has produced a range of 180‚ó¶and 90‚ó¶
rotational invariant ring-TCM codes (RI ring-TCM), which are presented in Table 7.3
and compared with the exhaustive algorithm [18].
In Table 7.4, d2
free is the minimum squared Euclidean distance between coded
sequences of the ring-TCM code suitable for 16-QAM; g‚àûis the asymptotic coding
gain of the ring-TCM code suitable for 16-QAM over uncoded 8-AMPM modulation; ‚àó
means that the exhaustive search was stopped; and ‚ô¶means a new generated code
using the genetic algorithm.
Table 7.3
RI ring-TCM codes for 4-PSK on AWGN channel.
nst
Ring-TCM Code
Rot
d2
free
Nfree
2
320
90‚ó¶
8
5
4
230
90‚ó¶
8
5
4
032
90‚ó¶
8
5
4
212
90‚ó¶
8
1
8
030/02
90‚ó¶
8
5
8
012/02
90‚ó¶
8
1
16
032/31
90‚ó¶
12
6
16
221/31
90‚ó¶
12
2
16
232/33
90‚ó¶
16
14
32
0302/130
90‚ó¶
12
2
32
2032/110
90‚ó¶
16
8
64
0232/303
90‚ó¶
16
5
64
3103/213
90‚ó¶
16
4
64
1013/332
90‚ó¶
16
2
64
2122/321
90‚ó¶
16
1
128
21 112/0222
90‚ó¶
16
4
128
13 010/0112
90‚ó¶
16
2
128
11 230/0312
90‚ó¶
20
2
128
21 132/1210
90‚ó¶
20
6
256
32 130/3113
90‚ó¶
20
10
256
30 013/1122
90‚ó¶
20
8
256
33 012/2101
90‚ó¶
20
2
256
21 103/1221
90‚ó¶
24
29
256
23 121/0332
90‚ó¶
24
10

INTRODUCTION
257
Table 7.4
RI ring-TCM codes for 16-QAM on AWGN channel.
nst
Ring-TCM Code
Rot
d2
free
Nfree
g‚àûdB
2
(030 020/0)
90‚ó¶
4.0
19.187
3.01
4
(220 030/2)
90‚ó¶
4.0
5.937
3.01
‚ô¶4
(020 300/0)
90‚ó¶
4.0
6.5
3.01
‚àó8
(020 212 010/32)
90‚ó¶
4.0
1.031
3.01
‚àó8
(022 231 0130/03)
90‚ó¶
4.0
0.75
3.01
‚àó8
(010 232 020/21)
180‚ó¶
5.0
5.234
3.98
‚ô¶8
(000 311 010/03)
180‚ó¶
4.0
4.5
3.98
‚àó16
(010 232 010/22)
90‚ó¶
6.0
9.796
4.77
‚ô¶16
(233 311 213/20)
90‚ó¶
10.0
0.5
4.77
Algorithm 7.1: Algorithm for select operator
select()
{
Randomly choose Œ±i, in the range 0 ‚â§Œ±i ‚â§Œª
j=1 f

at
j

for {k/‚àÄk ‚àà{1, . . . , Œª}}
{
if

q = max{k.s.t.Œ±i ‚â§Œª
k=1 f

at
k

}

{
return(q)
}
}
}
Algorithm 7.2: Algorithm for crossover operator
crossing()
{
Randomly choose the coefÔ¨Åcient from g(i)
s , . . . , g(i)
0 , fs, . . . , f1 ‚ààS
if(œÅ ‚â•œÅc)
{
increment crossover pointer;
compute crossover pointer;
}
else
crossover pointer; = 0; /* no cross, just reproduction*/
gene counter = 0;
while(gene counter < crossover pointer)
{
Copy to child1 the genetic information of father;

258
NON-BINARY CONVOLUTIONAL CODES
Copy to child2 the genetic information of mother;
increment the gene counter;
}
while(gene counter < L)
{
Copy to child1 the genetic information of mother;
Copy to child2 the genetic information of father;
increment the gene counter;
}
share parentage information();
}
Algorithm 7.3: Algorithm for crossover operator
mutate()
{
k = ring elements() ;
for(k/‚àÄk ‚àà{1, . . . , Œª})
{
if(œÅ ‚â•œÅm)
{
increment mutation counter;
alter one bit k = p in the chromosome;
}
}
}
Example 7.1: A 4-state ring-TCM code deÔ¨Åned over 4: A good 4-state ring-
TCM code over Z4 is the 23/1 ring-TCM code shown in Figure 7.14.
1
3
2
Input
Figure 7.14
The 23/1 ring-TCM code.

INTRODUCTION
259
Table 7.5
State table for 23/1 ring-TCM code over Z4.
Input
Initial state
Next state
Output
0
0
0
00
1
0
1
12
2
0
2
20
3
0
3
32
0
1
1
01
1
1
2
13
2
1
3
21
3
1
0
33
0
2
2
02
1
2
3
10
2
2
0
22
3
2
1
30
0
3
3
03
1
3
0
11
2
3
1
23
3
3
2
31
Its state table is given in Table 7.5 and the corresponding trellis diagram is shown
in Figure 7.15. From the state table we are able to obtain the signal Ô¨Çow graph for
the 23/1 ring-TCM code, as shown in Figure 7.16. Comparing it to the signal Ô¨Çow
graph of Figure 7.3, we can see that the number of states is the same but there are
more connections. As before, state 0 is denoted Xa, state 1 is denoted Xb, state 2 is
denoted Xc and state 3 is denoted Xd. From the signal Ô¨Çow graph, it is possible to
determine the transfer function of the 23/1 ring-TCM code and hence generate the
weight distribution of the code.
0/00
1/12
2/20
3/32
0/00
1/12
2/20
3/32
3/33
0/01
1/13
2/21
0/02
2/22
3/30
1/10
0/03
1/11
2/23
3/31
0/00
1/12
2/20
3/32
3/33
0/01
1/13
2/21
0/02
2/22
3/30
1/10
0/03
1/11
2/23
3/31
0/00
1/12
2/20
3/32
3/33
0/01
1/13
2/21
0/02
2/22
3/30
1/10
0/03
1/11
2/23
3/31
0/00
3/33
2/22
1/11
0
1
2
3
States
Time
Figure 7.15
The trellis diagram of the 23/1 ring-TCM code.

260
NON-BINARY CONVOLUTIONAL CODES
Xa
Xb
Xc
Xd
Xa
D2
D
D2
D2
D2
D2
D
D2
D2
D
D
D
D2
D2
D
Figure 7.16
The signal Ô¨Çow graph of the 23/1 ring-TCM code.
From Figure 7.16, we can deÔ¨Åne the following set of equations:
Xb = D2Xa + DXb + DXc + D2Xd.
(7.26)
Xc = DXa + D2Xb + DXc + D2Xd.
(7.27)
Xd = D2Xa + D2Xb + DXc + D2Xd.
(7.28)
Xa = D2Xb + D2Xc + D2Xd.
(7.29)
An important step is to Ô¨Årst notice that Xb = Xd. Subtracting (7.28) from (7.29):
Xb ‚àíXd = (D ‚àíD2)Xb + (D2 ‚àíD)Xd

1 ‚àíD + D2
Xb =

1 ‚àíD + D2
Xd
‚à¥Xb = Xd.
The transfer function T(D) of the signal Ô¨Çow diagram is:
T (D) = Xa
Xa
= D2 (Xb + Xc + Xd)
Xa
= D2 (2Xb + Xc)
Xa
.
(7.30)
T (D) = D2 
2D2 + D

Xa + 2

2D2 + D

Xb + 3DXc

Xa
= 2D4 + D3 + 2D2 
2D2 + D
 Xb
Xa
+ 3D3 Xc
Xa
.
(7.31)
We can show that:
Xb = D2Xa + DXc
1 ‚àíD ‚àíD2 .
(7.32)
Also:
Xc = DXa + 2D2Xb
1 ‚àíD
.
(7.33)

INTRODUCTION
261
Substituting Xc into Xb gives:
Xb =
D2Xa
1 ‚àíD ‚àíD2 +
D
1 ‚àíD ‚àíD2
 DXa + 2D2Xb
1 ‚àíD

‚à¥Xb
Xa
=
2D2 ‚àíD3
1 ‚àí2D ‚àíD3 .
(7.34)
Similarly:
Xc
Xa
=
D
1 ‚àíD +
2D2Xb
(1 ‚àíD)Xa
= D ‚àíD2 + D3
1 ‚àí2D ‚àíD3 .
(7.35)
Now substituting (7.34) and (7.35) into the transfer function (7.31) gives:
T (D) = 2D4 + D3 + 2D2 
2D2 + D
 Xb
Xa
+ 3D3 Xc
Xa
= D3 + 3D4 ‚àí3D5 + 8D6 ‚àí6D7
1 ‚àí2D ‚àíD3
.
(7.36)
Evaluating (7.36) gives:
D3 + 5D4 + 7D5 + 23D6 + ¬∑ ¬∑ ¬∑
1 ‚àí2D ‚àíD3 
D3 + 3D4 ‚àí3D5 + 8D6 ‚àí6D7
D3 ‚àí2D4
‚àíD6
5D4 ‚àí3D5 + 9D6 ‚àí6D7
5D4 ‚àí10D5
‚àí5D7
7D5 + 9D6 ‚àíD7
7D5 ‚àí14D6
‚àí7D8
23D6 ‚àíD7 + 7D8
...
.
Therefore, the 23/1 ring-TCM code over Z4 has one path with a weight of 3,
Ô¨Åve paths with a weight of 4, seven paths with a weight of 5, and so on.
7.1.8 Performance of Ring-TCM Codes on Urban Fading Channels
Three simulation results are presented, showing the performances of three ring-TCM
codes on an indoor, a pedestrian and a vehicular channel, as deÔ¨Åned in Chapter 1. The
ring-TCM codes tested were transparent, and decoded using the soft-decision Viterbi
algorithm. The codes used were the 4-state 21/2, 8-state 213/30 and 16-state 212/31
ring-TCM codes deÔ¨Åned over Z4. The user velocities were 0 mph for indoor, 4 mph
for pedestrian and 70 mph for vehicular. The channel scenarios are easily modiÔ¨Åed

262
NON-BINARY CONVOLUTIONAL CODES
1.E‚Äì07
1.E‚Äì06
1.E‚Äì05
1.E‚Äì04
1.E‚Äì03
1.E‚Äì02
1.E‚Äì01
1.E+00
24
22
20
18
16
14
12
10
8
6
4
2
0
‚Äì2
‚Äì4
SNR (dB)
BER
Indoor Uncoded
Indoor (4-State)
Indoor (8-State)
Indoor (16-State)
Figure 7.17
Ring-TCM codes on the indoor channel.
by altering the delay, power proÔ¨Åle and Doppler spectra to create virtually any single-
input‚Äìsingle-output (SISO) environment based on measured data. This gives a much
more Ô¨Çexible channel model, which corresponds to actual measured data and produces
a time-varying frequency-selective channel that is much more realistic and is essential
for testing certain distortion mitigation techniques.
Figure 7.17 shows the performance of the ring-TCM codes on the indoor channel.
This is a slow-fading channel, but the least harsh of the three urban channel models
and good results are achieved, with the 16-state 212/31 ring-TCM code achieving a
coding gain of 1.2 dB over the 21/2 and 213/30 ring-TCM codes.
Figure 7.18 shows the performance of the ring-TCM codes on the pedestrian
channel. This channel is more harsh but the relative performance of the three ring-
TCM codes has not changed. The 212/31 ring-TCM code achieves a coding gain of
approximately 14 dB over uncoded performance.
Figure 7.19 shows the performance of the ring-TCM codes on the vehicular channel.
This channel is a time-varying fading channel and is very harsh. The performance
of all three ring-TCM codes is poor and there is no signiÔ¨Åcant coding gain over an
uncoded system.
7.2 Space-Time Coding Modulation
7.2.1 Introduction
Future-generation wireless communication systems require high-speed transmis-
sion rate for both indoor and outdoor applications. Space-time (ST) coding and

SPACE-TIME CODING MODULATION
263
1.E‚Äì07
1.E‚Äì06
1.E‚Äì05
1.E‚Äì04
1.E‚Äì03
1.E‚Äì02
1.E‚Äì01
1.E+00
34
32
30
28
26
24
22
20
18
16
14
12
10
8
6
4
2
0
‚Äì2
‚Äì4
SNR (dB)
BER
Pedestrian Uncoded
Pedestrian (4-State)
Pedestrian (8-State)
Pedestrian (16-State)
Figure 7.18
Ring-TCM codes on the pedestrian channel.
multiple-input‚Äìmultiple-output (MIMO) channels have been envisaged as the solution
for high-capacity levels [15]. A reliable high-speed communication is guaranteed due
to diversity, provided by space-time codes. This section describes the realization of
a space-time TCM coding scheme suitable for M-PSK [19] and M-QAM [20] over
fading channels. In this section, we also provide ST-ring TCM codes suitable for
1.E‚Äì03
1.E‚Äì02
1.E‚Äì01
1.E+00
20
18
16
14
12
10
8
6
4
2
0
‚Äì2
‚Äì4
SNR (dB)
BER
Vehicular Uncoded
Vehicular (4-State)
Vehicular (8-State)
Vehicular (16-State)
Figure 7.19
Ring-TCM codes on the vehicular channel.

264
NON-BINARY CONVOLUTIONAL CODES
Information 
source 
Space-time 
encoder & 
modulator 
Space-time 
receiver 
‚Ä¶
‚Ä¶
b
s1(t)   r1(t)
sn(t)   rm(t)
bÀÜ
Figure 7.20
Space-time system model.
QAM over Rayleigh fading channels. The performance of ST-ring TCM codes with
16-QAM in slow Rayleigh fading channels is evaluated with designs that involve rate
4 and 6 bits/s/Hz of 16 and 4 states.
7.2.2 Space-Time Codes Model
Consider a mobile communication system where the transmitter consists of n antennas
and the receiver has m antennas, as shown by Figure 7.20. The binary source data
are encoded by a space-time encoder that generates n streams, and each stream
is modulated and transmitted through a different antenna. Let si(t) be the symbol
transmitted by antenna i at time t for 1 ‚â§i ‚â§n and let T be the symbol period.
At a given instant t, all si(t) symbols are transmitted simultaneously in space-time
coding.
The signal received at each receiving antenna is the superposition of the n transmit-
ted signals corrupted by both Rayleigh and Rician fading and additive white Gaussian
noise (AWGN). Assuming that the elements of the signal constellations have an
average energy of Es equal to 1, the signal rj(t) received by antenna j at time t can be
described as [15]:
r j(t) =
n

i=1
h ji(t)si(t) + Œ∑ j(t)
with
1 ‚â§j ‚â§m, 1 ‚â§i ‚â§n, 1 ‚â§t ‚â§l,
(7.37)
where the noise Œ∑ j(t) at time t is modelled as independent samples of a zero-mean
complex Gaussian random variable with variance N0/2 per dimension. The coefÔ¨Åcients
h ji(t) are the channel gains from the transmit antenna, i, to the receive antenna,
j. These channel gains are usually modelled as independent samples of complex
Gaussian random variables with variance 0.5 per dimension.
(7.37) can be expanded to give a clear insight into the system model:
r j(t) = h1 j(t)s1(t) + h2 j(t)s2(t) + h3 j(t)s3(t) + ¬∑ ¬∑ ¬∑ + Œ∑ j(t).
(7.38)

SPACE-TIME CODING MODULATION
265
Likewise, the received signal sequence may be written as an array R of dimension
m √ó l, as well as the random AWGN variables in a matrix N of:
R =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
r1(1)
r1(2)
¬∑ ¬∑ ¬∑
r1(l)
r2(1)
r2(2)
¬∑ ¬∑ ¬∑
r2(l)
...
...
...
rm(1)
rm(2)
¬∑ ¬∑ ¬∑
rm(l)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
N =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
Œ∑1(1)
Œ∑1(2)
¬∑ ¬∑ ¬∑
Œ∑1(l)
Œ∑2(1)
Œ∑2(2)
¬∑ ¬∑ ¬∑
Œ∑2(l)
...
...
...
Œ∑m(1)
Œ∑m(2)
¬∑ ¬∑ ¬∑
Œ∑m(l)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
(7.39)
The system model expressed in (7.37) may also be expressed in matrix form:
R=H S+N,
(7.40)
where H is the m √ó n channel coefÔ¨Åcient matrix.
The information symbol b(t) at time t is encoded by the ST encoder as the sequence
of code symbols s1(t), s2(t), . . ., sn(t). In the receiver a maximum-likelihood decoder
receives the signals r1(t), r2(t), . . ., rm(t), each at a different receive antenna. For a
given code of length l, the transmitted sequence was:
S =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
s1(1)
s1(2)
¬∑ ¬∑ ¬∑
s1(l)
s2(1)
s2(2)
¬∑ ¬∑ ¬∑
s2(l)
...
...
...
sn(1)
sn(2)
¬∑ ¬∑ ¬∑
sn(l)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
(7.41)
The code symbols are generated by the ring-TCM encoder shown in Figure 7.14.
The outputs xi(i = 1, 2, 3, . . . , d + 1) are Z4 symbols and are mapped according to
the constellation used and the number of transmitting antennas, n. To decode the ST-
RTCM code using the Viterbi algorithm, the accumulated metrics are determined by:
M

r j(t), si(t)|hi j

=
l
t=1
m

j=1
r j(t) ‚àí
n

i=1
hi jsi(t)
.
(7.42)
Example 7.2: Construction and Viterbi decoding of a 4-state space-time ring-
TCM code: In this example, the 23/1 ring-TCM code from Figure 7.14 is combined
with spatial diversity to form a space-time ring-TCM code deÔ¨Åned over Z4 with
QPSK modulation. The The two QPSK outputs are transmitted from two antennas
simultaneously. The trellis diagram in Figure 7.21 shows how the Viterbi algorithm
can be used for the soft-decision decoding of the 23/1 space-time ring-TCM code,
with n = 2 and m = 2.

266
NON-BINARY CONVOLUTIONAL CODES
0/00
1/12
2/20
3/32
0/00
3/33
0/01
2/22
1/11
0
1
2
3
States
22.378
1.0917
21.437
41.135
12.071 + 22.378 = 34.449
2
2
22
1
12
2
2
2
21
1
11
1
)1(
)1(
)1(
)1(
)1(
)1(
)1(
)1(
)1(
)1(
q
h
q
h
r
q
h
q
h
r
‚àí
‚àí
+
‚àí
‚àí
Received
1st antenna
CSI
Transition 
signals
Received
2nd antenna
CSI Transition 
signals
22.952 + 1.0197 = 24.044
7.8235 + 21.437 = 29.2602
4.5824 + 41.137 = 45.7176
10.1054 + 1.0917 = 11.1971
Transition metrics
Accumulated 
metrics
Survivor path
Survivor
Figure 7.21
Soft-decision decoding example of a space-time ring-TCM code using the Viterbi
algorithm.
Observe how the transition metric is calculated for every state based on (7.38).
All the information required in the calculation is from the received signals, the
CSI and the transmitted signals speciÔ¨Åed by the respective branch label. At t = 2,
there are four competitive paths arriving at every state, whose accumulated metrics
are computed according to (7.42). In state 0, for instance, the second competitor
has the lowest accumulated metric, of 24.044; hence this will be the survivor path.
This procedure is repeated for every state and carried on to the continuing symbol
intervals. When the sequence Ô¨Ånalizes there will be four survivors, one for each
state. The survivor with the lowest resultant metric will be selected and the input
sequences traced back so that the original data can be reconstructed.
7.2.3 Performance of ST-RTCM Codes Using QPSK for MIMO
Urban Environments
Simulation results for ST-RTCM codes are now presented, evaluating their perfor-
mance on the indoor, pedestrian and vehicular MIMO fading channels. The codes
are the 4-state 21/3, the 16-state 212/31 and the 64-state 2103/132 ring-TCM codes.
Figure 7.22 shows that, as for the case of the ring-TCM codes in Figure 7.19, the
performance of the ST-RTCM codes on the vehicular MIMO channel is poor. The

SPACE-TIME CODING MODULATION
267
1.E‚Äì06
1.E‚Äì05
1.E‚Äì04
1.E‚Äì03
1.E‚Äì02
1.E‚Äì01
1.E+00
30
29
28
27
26
25
24
23
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
‚Äì1
‚Äì2
‚Äì3
‚Äì4
SNR (dB)
BER
Indoor (21/3)
Indoor (Delay Diversity)
Indoor (212/31)
Pedestrian (21/3) 
Pedestrian (Delay Diversity) 
Pedestrian (212/31) 
Vehicular (21/3) 
Vehicular (Delay Diversity) 
Vehicular (212/31) 
Indoor (2103/132)
Pedestrian (2103/132)
Vehicular (2103/132)
Figure 7.22
Performance of a 4-state, 16-state and 64-state ring-TCM code on urban MIMO fading
channels.
4-state and 16-state ST-RTCM codes on the pedestrian MIMO channel also perform
poorly with an error Ô¨Çoor, but the 64-state ST-RTCM code performs well with no
error Ô¨Çoor at a BER = 10‚àí6. The best performance is on the indoor MIMO channel,
as expected, with the 2103/132 ST-RTCM code performing very well with a BER =
10‚àí6 at a SNR of 6 dB.
7.2.4 Ideal CSI and Slow Fading
It is assumed here that the elements hij of matrix H are time-independent and the
receiver has full knowledge of them, so that there is no need to carry out any sort
of estimation process within the receiver. The probability of having received the
sequence R at the receiver when the code matrix S has been transmitted through
a fading channel H follows a multidimensional Gaussian distribution and shall be
described by the pdf:
p(R|S, H) =
l
t=1
m

j=1
1
‚àö2œÄ N0
exp
Ô£Æ
Ô£ØÔ£ØÔ£∞‚àí
|r j(t) ‚àí
n
i=1
hi jsi(t)|2
2N0
Ô£π
Ô£∫Ô£∫Ô£ª.
(7.43)
The above is due to the fact that, at a given symbol interval, the m-dimensional
noise array has a multivariate Gaussian complex distribution with zero mean and

268
NON-BINARY CONVOLUTIONAL CODES
covariance matrix 
Œ∑ equal to N0 ¬∑ Im, where Im is the m-order identity matrix. So,
if we consider the random vector  Œ∑(t) =
Œ∑1(t0) Œ∑2(t0) ¬∑ ¬∑ ¬∑ Œ∑m(t0) 
as a column of the
noise matrix N for a given time instant t0, for example, with 1 ‚â§t0 ‚â§l, then the joint
pdf of the Gaussian random variables Œ∑ j(t0) is deÔ¨Åned as [15]:
p

 Œ∑(t0)
 
=
1
(2œÄ)m/2!
det 
Œ∑
exp
"
‚àí1
2( r ‚àíH ¬∑  s) ‚àí1
Œ∑ ( r ‚àíH ¬∑  s)
#
,
(7.44)
where  r and  s are the column vectors at time t0 of matrices R and S respectively,
and ( x) denotes the transpose conjugate of vector  x. As the columns of the random
noise matrix N are statistically independent, we will have that the overall probability
p(R|S, H), which may be expressed as:
p(R|S, H) =
l
t0=1
p

 Œ∑(t0)
 
.
(7.45)
When combined with 7.44, and after some mathematical processing, this will yield
expression given in (7.43).
7.2.5 Log-Likelihood Function
The next step is to deÔ¨Åne the likelihood function that shall be used in the maximum
likelihood sequence decoding of ST codes. It is desirable that this function, f, has the
following additive property: the likelihood of the complete sequence is the sum of the
partial symbol likelihoods; that is to say [15]:
f =
l
t=1
fn,
(7.46)
where fn represents an individual instant likelihood. Having this in mind, the most
suitable function is the natural logarithm of p(R|S, H), namely the log-likelihood
function, which is obtained by taking the logarithm of (7.43):
ln[p(R|S, H)] =
l
t=1
m

j=1
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞ln

1
‚àö2œÄ N0

‚àí
r j(t) ‚àí
n
i=1
hi jsi(t)

2
2N0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
= ‚àíml
2 ln(2œÄ N0) ‚àí
l
t=1
m

j=1
r j(t) ‚àí
n
i=1
hi jsi(t)

2
2N0
,
(7.47)

SPACE-TIME CODING MODULATION
269
with the Ô¨Årst term being a constant, and the second dependent on the received signal
sequence, R, the transmitted code matrix, S, and the channel matrix, H. This function
is of vital important for decoding ST codes.
7.2.6 Maximum-Likelihood Sequence Detection
The maximum-likelihood (ML) sequence detection in ST codes is performed by
maximizing the above log-likelihood function, that is the task is to Ô¨Ånd the sequence
ÀÜsi(t) that maximizes ln[p(R|S, H)]:
ÀÜsi(t) = arg max
si
ln[p(R|S, H)].
(7.48)
The Ô¨Årst term of (7.47) is constant, so it is rendered irrelevant in the maximization
process. Furthermore, the denominator of the second term is also unnecessary. The
maximization of (7.48) turns into the following minimization problem:
ÀÜsi(t) = arg min
si
l
t=1
m

j=1
r j(t) ‚àí
n

i=1
hi jsi(t)

2
,
(7.49)
which can be easily achieved with the Viterbi algorithm, using the following metric
to compute the ML path through the trellis:
m(r j(t), si(t)|hi j) =
l
t=1
m

j=1
r j(t) ‚àí
n

i=1
hi jsi(t)

2
,
(7.50)
with 1 ‚â§i ‚â§n, 1 ‚â§j ‚â§m and 1 ‚â§t ‚â§l.
7.2.7 ST Codes Pairwise Error Probability
Consider that the code word s shown in (7.43) is initially transmitted but the decoder
decodes erroneously in favour of the following decoded sequence e:
e = e1(1)e2(1) . . . en(1)e1(2)e2(2) . . . en(2) . . . e1(l)e2(l) . . . en(l).
(7.51)
The probability of making such an erroneous decision, namely the pairwise error
probability (PEP), is given by [7]:
P(s ‚Üíe|hi j) = P{m(r j(t), ei(t)|hi j) ‚â§m(r j(t), si(t)|hi j)},
(7.52)
since the metric calculated for the erroneous sequence along the trellis must be less
than the metric worked out for the correct sequence in order to make the incorrect
decision.

270
NON-BINARY CONVOLUTIONAL CODES
By applying a Chernoff bound [21] to the above expression, it turns out that the
PEP is bounded as follows [7]:
P(s ‚Üíe|hi j) ‚â§E{exp(¬µ[m(r j(t), si(t)|hi j) ‚àím(r j(t), ei(t)|hi j)])},
(7.53)
where E{¬∑} represents the expectation and ¬µ is a nonnegative parameter that must be
optimized in order to produce the tightest upper bound possible.
Replacing (7.50) into the above expression will yield:
P(s ‚Üíe|hi j) ‚â§
l
t=1
m

j=1
E{exp(¬µ|r j(t) ‚àí
n

i=1
hi jsi(t)|2 ‚àí¬µ|r j(t) ‚àí
n

i=1
hi jei(t)|2)}.
(7.54)
After lengthy mathematical simpliÔ¨Åcations, which are presented in full detail in [7]
for the SISO channel (the only difference in this case is the different metric being
used), it is possible to reduce inequality (7.54) to an upper bound dependent on the
signal-to-noise ratio Es/N0 and the squared Euclidean distance between sequences s
and e only [15]:
P(s ‚Üíe|hi j) ‚â§exp

‚àíd2(s, e) Es
4N0

,
(7.55)
where d2(s, e) will from now on be called the space-time squared Euclidean distance
(ST-SED) between code words s and e and is given by [15]:
d2(s, e) =
m

j=1
l
t=1

n

i=1
hi j(si(t) ‚àíei(t))

2
.
(7.56)
By deÔ¨Åning v j = (h1 j, h2 j, . . . , hnj) as the vector of associated channel gains of
receive antenna j, (7.56) can be rewritten as follows:
d2(s, e) =
m

j=1
n

i=1
n

Àúi=1
hi jhÀúi j
l
t=1
(si(t) ‚àíei(t))(sÀúi(t) ‚àíeÀúi(t)),
(7.57)
where x indicates the conjugate of complex number x. After some manipulation, the
distance turns out to be:
d2(s, e) =
m

j=1
v j A(s, e)v
j,
(7.58)

SPACE-TIME CODING MODULATION
271
where v j is the transpose conjugate of complex matrix vj, the matrix A(s, e) has
dimension n √ó n and the element at row p and column q is given by:
Apq =
l
t=1
(sp(t) ‚àíep(t)(sq(t) ‚àíeq(t)).
(7.59)
We have from (7.55):
P(s ‚Üíe|hi j) ‚â§
m

j=1
exp

‚àív j A(s, e)v
j
Es
4N0

.
(7.60)
Matrix A has some linear algebra properties [22] that are extremely helpful in this
context and will allow a mathematical interpretation of this expression for the PEP.
First, A is a Hermitian matrix because it is equal to its transpose conjugate matrix,
that is to say:
A = A.
(7.61)
Furthermore, there is a unitary matrix U whose rows are eigenvectors of A and
constitute an orthonormal basis of Cn, the n-dimensional set of complex numbers.
There also exists a nondiagonal matrix, D, whose elements Œªi(i = 1, . . . , n) are
eigenvalues of A, counting multiples, and which veriÔ¨Åes the following equality test:
UA(s, e)U  = D.
(7.62)
At last, the matrix A can be decomposed in the following form:
A(s, e) = B(s, e)B(s, e),
(7.63)
where matrix B(s,e) has dimension nxl, and is made by the symbol difference between
the transmitted sequence s and the received code word e, as follows:
B(s, e) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
e1(1) ‚àís1(1)
e1(2) ‚àís1(2)
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
e1(l) ‚àís1(l)
e2(1) ‚àís2(1)
e2(2) ‚àís2(2)
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
e2(l) ‚àís2(l)
e3(1) ‚àís3(1)
e3(2) ‚àís3(2)
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
e3(l) ‚àís3(l)
...
...
...
...
...
en(1) ‚àísn(1)
en(2) ‚àísn(2)
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
en(l) ‚àísn(l)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
= E ‚àíS.
(7.64)
B is called the square root matrix of A. Another important linear algebra property
is that any matrix with a square root is always nonnegative-deÔ¨Ånite, so in this context
matrix A will verify the inequality:
x Ax ‚â•0,
‚àÄx ‚ààCn.
(7.65)

272
NON-BINARY CONVOLUTIONAL CODES
It also follows that the eigenvalues of a nonnegative deÔ¨Ånite Hermitian matrix are
nonnegative, therefore resulting in A‚Äôs eigenvalues Œªi being all nonnegative.
It is easy to see that the vectors vj as well as eigenvalues Œªi are time-dependant.
Now let‚Äôs deÔ¨Åne the vector Œ≤ j = (¬µ1 j, ¬µ2 j, . . . , ¬µnj) = v jU . We can see that ¬µij are
also independent complex Gaussian random variables of variance 0.5 per dimension.
Therefore, using (7.62):
v j A(s, v)v
j = v j(U DU)v
j = Œ≤ j DŒ≤
j
=
¬µi j ¬µ2 j ¬∑ ¬∑ ¬∑ ¬µnj

Ô£´
Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠
Œª1
0
¬∑ ¬∑ ¬∑
0
0
Œª2
¬∑ ¬∑ ¬∑
0
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
Œªn
Ô£∂
Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
Ô£´
Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠
¬Ø¬µ1 j
¬Ø¬µ2 j
...
¬Ø¬µnj
Ô£∂
Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
=
n

i=1
Œªi
¬µi j
2 .
(7.66)
Thus the probability of error is:
P(s ‚Üíe|hi j) ‚â§
m

j=1
exp

‚àíEs
4N0
n

i=1
Œªi
¬µi j
2

.
(7.67)
We now examine two different cases:
1. If the random variables ¬µij have nonzero mean, then their modulus
¬µi j
 follows a
Rician distribution and, by averaging inequality (7.67) with respect to these random
variables ¬µij, we get the following upper bound on the probability of error:
P(s ‚Üíe|hi j) ‚â§
m

j=1
Ô£´
Ô£¨Ô£¨Ô£≠
n
i=1
1
1 + Es
4N0
Œªi
exp
Ô£´
Ô£¨Ô£¨Ô£≠
Ki j
Es
4N0
Œªi
1 + Es
4N0
Œªi
‚àíEs
4N0
n

i=1
Œªi
¬µi j
2
Ô£∂
Ô£∑Ô£∑Ô£∏
Ô£∂
Ô£∑Ô£∑Ô£∏,
(7.68)
where Kij is the Rician factor of
¬µi j
. When taking the mean E (¬∑) of inequality
(7.67), it is necessary to work out the ‚Äòmoment-generating function‚Äô of the random
variable
¬µi j
2 which has a noncentral chi-square distribution.
2. If the random variables hij have zero mean then the random variables ¬µij also have
zero mean. This is the case of independent Rayleigh fading and Ki j = 0 for all i
and j. Hence, by replacing in 7.68, we now get:
P(s ‚Üíe|hi j) ‚â§
Ô£´
Ô£¨Ô£¨Ô£≠
n
i=1
1
1 + Es
4N0
Œªi
Ô£∂
Ô£∑Ô£∑Ô£∏
m
.
(7.69)

SPACE-TIME CODING MODULATION
273
Let œÅ be the rank of the matrix A; it follows that exactly n ‚àíœÅ eigenvalues of A are
zero. Considering that the nonzero eigenvalues of A are Œª1, Œª2, . . . , ŒªœÅ and assuming
the signal-to-noise ratio Es
&
N0 is high:
P(s ‚Üíe|hi j) ‚â§

i=1
Œªi
‚àím  Es
N0
‚àíœÅm
.
(7.70)
Let us deÔ¨Åne the diversity advantage of an ST code as the power of the SNR in the
denominator of the expression given for the PEP bound. So a diversity advantage of
œÅm is achieved in the above bound. Let us also deÔ¨Åne the coding gain of an ST code
as the approximate measure of the gain over an uncoded system that bears the same
diversity advantage. We can see that a coding gain of (Œª1Œª2 . . . ŒªœÅ)1/œÅ is obtained by
the multi-antenna system.
For the Ô¨Årst case where Rician fading is present, the upper bound for the pairwise
error probability is very similar to the above, except for a correction factor in the
coding gain that depends on Kij. The same diversity advantage is achieved. So, for
high SNR values in Rician channels, 7.68 is now reduced to:
P(s ‚Üíe|hi j) ‚â§

i=1
Œªi
‚àím  Es
N0
‚àíœÅm
Ô£´
Ô£≠
m

j=1
œÅ

i=1
e‚àíKi j
Ô£∂
Ô£∏.
(7.71)
Both matrices A and B have equal rank œÅ, so there is no need to calculate matrix A;
it is enough to know the code word-difference matrix B in order to get œÅ and hence
the diversity advantage.
7.2.8 ST Code Design Criteria for Slow Fading Channels
The code design criteria for slow Rayleigh fading channels can be summarized as
follows:
 Rank Criterion: In order to achieve the maximum diversity advantage nm, the matrix
B(s, e) must have full rank for any code words s and e. This will guarantee a tighter
upper bound on the PEP of inequality (7.70).
 Product Criterion: If the system has a diversity advantage of œÅm, the minimum of
the œÅth roots of the product of the product of the nonzero eigenvalues of A(s, e)
taken over all pairs of different code words s and e must be maximized. That is to
say, the target is to maximize the coding gain G, deÔ¨Åned as:
G = min
‚àÄs,e
s=e
Ô£´
Ô£≠œÅ
'
(
(
)
œÅ

i=1
Œªi
Ô£∂
Ô£∏.
(7.72)

274
NON-BINARY CONVOLUTIONAL CODES
These criteria are valid only when full channel state information is available at the
decoder. They will assure the tightest upper bound on the PEP possible. The product
criterion is often called the determinant criterion.
In slow Rician fading channels the criteria are the same, with the only difference
being that the coding advantage to be maximized according to the product criterion
is (from (7.70)):
G = min
‚àÄs,e
s=e
Ô£´
Ô£≠œÅ
'
(
(
)
œÅ

i=1
Œªi
Ô£∂
Ô£∏
Ô£´
Ô£≠
m

j=1
œÅ

j=1
e‚àíKi j
Ô£∂
Ô£∏
1/œÅm
.
(7.73)
These criteria are only intended to be a guideline for ST code design. They are
not laws, are based only on upper bounds, and tighter bounds may be found later
on. Besides, the criteria work on the basis of making the bound as tight as possible,
which only happens when a given transmitted sequence s is confused with a decoded
sequence e; it could be that this occurrence seldom happens in practice and it might
be necessary to study the PEP bounds of sequence pairs (s, e) that are actually more
likely to be produced.
7.2.9 ST Code Design Criteria for Fast Fading Channels
The code design criteria for time-varying Rayleigh fading channels are similar to
those shown for slow fading and are described as follows:
 Rank Criterion: Identical to criterion for slow Rayleigh fading channels.
Product Criterion: Slightly changes in the case of rapid fading. Here, in order to
achieve the highest coding gain possible, the minimum of the products:

t‚àà
(s,e)
 n

i=1
|ei(t) ‚àísi(t)|2

,
1 ‚â§t ‚â§l
taken over all distinct code words s and e must be maximized, where 
 (s, e) is the
set of time instances in which the code words s and e differ in at least one symbol.
If the system has a diversity advantage of œÅm, the coding gain G is:
G = min
‚àÄs,e
s=e

t‚àà
(s,e)
 n

i=1
|ei(t) ‚àísi(t)|2
1/œÅ
.
(7.74)
7.2.10 Space-Time Ring-TCM Codes for QAM in Fading Channels
Space-Time Coded Modulation (STCM) has been implemented with a wide variety of
modulation schemes. Most technical journals/books available nowadays only describe

SPACE-TIME CODING MODULATION
275
the case where binary STCM uses phase shift keying (PSK) modulation, especially
QPSK and 8-PSK. In this book we consider non-binary coding (ring-codes) combined
with quadrature amplitude modulation. We describe techniques to search for good ST
codes for 16-QAM for slow fading channels. The search was developed according to
a set of extended designed rules proposed in [7]. We assume QAM modulation with
sizes of 16 and 64 waveforms. Naguib et al. [23] employed the signal mapping shown
in Figure 7.13a for 16-QAM.
However, there are other mappings that yield increased Euclidean distances for ST
codes, as they provide higher values for the minimum determinant of the differential
code matrix taken over all pairs of possible code words. This improved 16-QAM
signal mapping is shown in Figure 7.13b and was originally proposed by Carrasco
and Farrell [13]. Figure 7.13c presents their proposed mapping for 64-QAM. The
improved mappings were designed by taking into account the isomorphism produced
by Z-numbers when considering phase rotations.
In Figure 7.13 every signal point is composed of a pair of Z-symbols, where the
leftmost symbol is the same for the four constituent signals of a given quadrant.
Within all quadrants the rightmost Z-number follows the same isomorphism. The
count goes up in one unit clockwise, starting at the upper-left corner point. The design
criteria for ST codes have been extended to reÔ¨Åne computer code search and make
it more accurate. This complementary set of rules takes into account the calculation
of other parameters that are relevant in ST code performance. These parameters are
the average number of competing paths with minimum determinant min associated
with a given path in the trellis ND, and the average determinant ¬Ø for all pairs of code
words. The design rules now have two new extra criteria. The four rules are presented
in hierarchical structure and are practical for use in code searches:
1. The minimum rank must be maximized. Ideally the code must have full rank n,
which is the number of transmitters. ST codes that do not possess full rank are
therefore discarded.
2. The codes selected are the ones that provide the highest minimum determinant
min of the matrix A(s, e) taken over all pairs of distinct code words, where the
matrix A(s, e) has dimension n √ó n and contains the symbol differences between
the transmitted sequence s and the received code e. The element at row p and
column q is given by [21]:
Apq =
l
i=1
(sp(t) ‚àíep(t)) (sq(t) ‚àíeq(t)),
(7.75)
where (‚Ä¢) represents complex conjugation. A hierarchical code list can thus be
made, ordering them according to min values.
3. In the case that two or more codes have the same min, the best code is the one
with the smallest N, so codes are prioritized based on N.

276
NON-BINARY CONVOLUTIONAL CODES
Table 7.6
Optimum ST-RTCM codes of modulo-16 for 16-QAM.
Code
States
min
N

8 9/8
16
4
1.625
61.3
0 9/0
16
4
1.625
27.2
0 9/8
16
4
1.625
27.2
8 9/0
16
4
1.625
27.2
8 3/8
16
1
0.094
62.4
7 12/3
16
1
0.316
51.7
Tarokh et al.
16
1
1.25
27.2
4. If any two good codes still have identical min and N, the one with the highest
average  is Ô¨Ånally chosen.
An exhaustive a priori search was conducted among all 16-state ST-RTCM codes
to Ô¨Ånd those that best satisfy these rules. Table 7.6 shows the best codes found with
their respective min, N and ; they are ordered from best at the top of the table to
worst at the bottom. All codes shown are of full rank, therefore yielding the maximum
diversity advantage nm, equal to four. All codes are written in the form g1g2/f1, where
g1 and g2 are the feed-forward coefÔ¨Åcients and f 1 is the feedback coefÔ¨Åcient.
Note that codes 0 9/0, 0 9/8 and 8 9/0 have the same parameter values and must
have identical performance as well. This is an indication of the existence of multiple
optimum codes. They are the best codes, together with 8 9/8, because their minimum
determinant is four. No other codes can achieve this, all having a minimum determinant
of one. In spite of it not being an optimum code from the determinant criterion point
of view, the code 7 12/3 has been included for performance comparison with Naguib
et al. [23] and optimum ST-RTCM codes. It has a smaller N than Naguib‚Äôs code and
a higher average determinant.
Figure 7.23 presents the simulation results for Naguib et al.‚Äôs code and the ST-
RTCM 8 9/8 and 7 12/3 codes for values of m equal to 2 and 4.The uncoded single-
channel 16-QAM curve is also indicated. For two receive antennas and a BER of 10‚àí3,
the 8 9/8 code has a coding gain of 1.8 dB in relation to Naguib et al.‚Äôs, while keeping a
gain of approximately 1.2 dB when compared with the 7 12/3 code. With four receive
antennas, the 8 9/8 now has a coding gain of 4 dB in relation to Naguib‚Äôs at 10‚àí4
BER. This proves that the higher min is for a particular code, the higher the coding
advantage will be when m increases. Another conclusion drawn from the graph is that
7 12/3 performance is better than Naguib et al.‚Äôs, accounting for a 3.3 dB margin at
SNRs greater than 20 dB in a system with two transmitters and two receivers.
Both codes have full rank and minimum determinant, however 7 12/3 performs
better due to its smaller ND and bigger ¬Ø, originating from the different code-to-
signal mapping used in the modulation of ST-RTCM codes. The ST-RTCM 8 9/8 is
the one with the best performance as it fully complies with the extended criteria: it
has the maximum min of four and higher determinant average than the other three

SPACE-TIME CODING MODULATION
277
100
0
4
8
12
16
20
24
SNR, dB
Uncoded,n=1, m=1
Tarokh‚Äôs,n=2, m=2
Tarokh‚Äôs, n=2, m=4
7 12 / 3, n=2, m=2
8 9 / 8, n=2, m=2
8 9 / 8, n=2, m=4
10‚àí2
10‚àí4
10‚àí6
BER
Figure 7.23
Error performance of ST-RTCM codes with 16-QAM over slow Rayleigh fading channels,
130 symbols per frame, 4 bits/s/Hz.
codes that also present the highest min. When more receiving antennas are employed,
Figure 7.23 shows a lower probability of error for a given high SNR value due to the
increased diversity. Therefore the respective curve sinks sooner, with a pronounced
slope.
Figure 7.24 shows that the ST-RTCM code 213 132/3 with n = m = 2 performs
on the Gaussian bound until an SNR of 18 dB. It outperforms code 25 12/47 by a
Figure 7.24
BER performance of ST-RTCM codes with QAM over slow Rayleigh fading channels, 65
symbols per frame, 6 bits/Hz, 25 12/47, 64-state, 64-QAM, 213 132/3, 4-state, 16-QAM.

278
NON-BINARY CONVOLUTIONAL CODES
Table 7.7
Comparison of ST codes with 16-QAM, 4 bit/s/Hz.
Code
SNR (dB)
Delay diversity, m = 2
26
ST-RTCM 7 12/3, m = 2
23.6
ST-RTCM 8 9/8, m = 2
21.4
Delay diversity, m = 4
17.2
ST-RTCM 8 9/8, m = 4
13.2
margin of 7 dB. This is due to the high coding gain provided by the former, being an
optimized code; the latter is not an optimum code. The 4-state code is not only superior
in performance but also has a lower complexity than the 64-state code. There must
be 64-QAM ST-RTCM codes of better performance than 25 12/47, but the computer
search is quite long due to the large number of different codes and is not of practical
interest since these 64-state codes are much more complex than 213 132/3.
Finally, 213 132/3 performance is also presented in a four-receiver conÔ¨Åguration,
which is even better due to higher diversity. It is 11.5 dB better off than the same code
used with m = 2.
Table 7.7 gives a brief summary of the simulation results obtained for ST-ring
TCM codes with QAM modulation over slow Rayleigh fading channels. Table 7.7
compares different ST codes that have a bandwidth efÔ¨Åciency of 4 bit/s/Hz, providing
SNR values that reach a BER of 10‚àí4 in 16-QAM simulations.
7.3 Conclusions
In this chapter we have introduced the concepts of convolutional coding and trellis
coded modulation. We have shown how TCM codes are not only bandwidth efÔ¨Åcient
but also achieve good performance. The idea of TCM has been extended for symbols
deÔ¨Åned over rings of integers, which outperform binary TCM codes with only a small
increase in decoding complexity. The criteria for constructing good ring-TCM codes
have been given and a genetic algorithm has been presented as a search method for
Ô¨Ånding good codes. We have shown how Ring-TCM codes are suitable for combining
with spatial-temporal diversity, resulting in space-time ring-TCM codes. The perfor-
mance of ST-RTCM codes were evaluated with simulation results over urban mobile
radio channels, and these performed well in the slow fading indoor and pedestrian
environments. Finally, the design criteria were presented for constructing good ST-
RTCM codes and the performance of some new ST-RTCM codes were evaluated with
simulation results for QAM on slow fading channels.
References
[1] Elias, P. (1955) Coding for Noise Channels, IRE Convention Report, Part 4, pp. 37‚Äì47.
[2] Moon, T.K. (2005) Error Correction Coding. Mathematical Methods and Algorithms, Wiley Inter-
science, ISBN 0-471-64800-0.

REFERENCES
279
[3] Viterbi, A.J. (1967) Error bounds for convolutional codes and asymptotically optimum decoding
algorithm. IEEE Transactions on Information Theory, 13, 260‚Äì9.
[4] Forney, G.D. (1973) The Viterbi algorithm. Proceedings of IEEE, 61, 268‚Äì78.
[5] Massey, J.L. (1974) Coding and modulation in digital communications. Presented at International
Zurich Seminar on Digital Communications, Zurich, Switzerland.
[6] Ungerboeck, G. (1982) Channel coding with multilevel/phase signals. Information Theory, IEEE
Transactions, 28, 55‚Äì67.
[7] Biglieri, E., Divisar, D., Melane, P.J. and Simon, M.K. (1991) Introduction to Trellis Coded Modu-
lation with Application, MacMillan, New York.
[8] Massey, J.L. and Mittelholzer, T. (1984) Codes over rings ‚Äì a practical necessity. Presented at
AAECC7 International Conference, Universite‚Äô P. Sabatier, Toulouse, France.
[9] Massey, J. and Mittelholzer, T. (1989) Convolutional codes over rings. Proceedings of the 4th Joint
Swedish Soviet Workshop Information Theory, pp. 14‚Äì8.
[10] Baldini, R.F. and Farrell, P.G. (1990) Coded modulation wit convolutional codes over rings.
Presented at Second IEE Bangor Symposium on Communications, Bangor, Wales.
[11] Baldini, R.F. and Farrell, P. (1994) Coded modulation based on rings of integers modulo-q part 2:
convolutional codes. IEE Proceedings: Communications, 141, 137‚Äì42.
[12] Carrasco, R., Lopez, F. and Farrell, P. (1996) Ring-TCM for M-PSK modulation: AWGN channels
and DSP implementation. Communications, IEE Proceedings, 143, 273‚Äì80.
[13] Carrasco, R. and Farrell, P. (1996) Ring-TCM for Ô¨Åxed and fading channels: land-mobile satellite
fading channels with QAM. Communications, IEE Proceedings, 143, 281‚Äì8.
[14] Lopez, F.J., Carrasco, R.A. and Farrell, P.G. (1992) Ring-TCM Codes over QAM. IEE Electronics
Letter, 28, 2358‚Äì9.
[15] Tarokh, V., Seshadri, N. and Calderbank, A. (1998) Space-time codes for high data rate wireless
communication: performance criterion and code construction. Information Theory, IEEE Transac-
tions on, 44, 744‚Äì65.
[16] Benedetto, S., Mondin, M. and Montorsi, G. (1994) Performance evaluation of trellis-coded mod-
ulation schemes. Proceedings of the IEEE, 82, 833‚Äì55.
[17] Goldberg, D. (1989) Genetic Algorithms in Search, Optimization and Machine Learning, Addison-
Wesley Longman Publishing Co., Inc., Boston, MA, USA.
[18] Soto, I. and Carrasco, A. (1997) Searching for TCM codes using genetic algorithms. Communica-
tions, IEE Proceedings, 144, 6‚Äì10.
[19] Pereira, A. and Carrasco, R. (2001) Space-time ring TCM codes for QPSK on time-varying fast
fading channels. Electronics Letters, 37, 961‚Äì2.
[20] Carrasco, R. and Pereira, A. (2004) Space-time ring-TCM codes for QAM over fading channels.
IEE Proceedings Communications, 151 (4), 316‚Äì21.
[21] Pereira, A. (2003) Space-Time Ring TCM Codes on Fading Channels. PhD Thesis, Staffordshire
University, Staffordshire, UK.
[22] Horn, R. and Johnson, C. (1990) Matrix Analysis, Cambridge University Press, Cambridge, UK,
ISBN 0-521-38632-2.
[23] Naguib, A., Tarokh, V., Seshadri, N. and Calderbank, A. (1998) A space-time coding modem for
high-data-rate wireless communications. Selected Areas in Communications, IEEE Journal, 16,
1459‚Äì78.


8
Non-Binary Turbo Codes
8.1 Introduction
One of the most important breakthroughs in coding theory was the development
of turbo codes by Berrou, Glavieux and Thitimajshima [1] in 1993. Turbo codes
are a parallel concatenation of recursive systematic convolutional codes separated
by an interleaver. They provide a practical way of achieving near-Shannon limit
performance by using an iterative decoder that contains two soft-input‚Äìsoft-output
component decoders in series, passing reliability information between them. Origi-
nally, the component decoders used the BCJR algorithm [2, 3] or MAP algorithm,
but this was considered too complex for practical applications. A reduction in the
complexity of the MAP algorithm with only a small degradation in performance was
achieved by working in the logarithmic domain, resulting in the log MAP algorithm.
Simpler trellis decoding algorithms also include the max-log MAP algorithm and the
Soft Output Viterbi Algorithm (SOVA). Both algorithms perform the same as one an-
other, but do not perform as well as the log MAP algorithm. Turbo codes are used for
error-correction in 3G mobile communications and are now also used for deep-space
communications.
In this chapter, the binary turbo encoding and decoding processes are introduced,
with detailed descriptions of log-likelihood ratios, the MAP algorithm and the iterative
log MAP algorithm. This is followed by an introduction to non-binary turbo coding,
which is an area of research that has received very little attention, with only a few
papers published. Finally, we complete this chapter with an explanation of the non-
binary turbo encoder and decoder structure.
8.2 The Turbo Encoder
A turbo code is a parallel concatenation of two recursive systematic convolutional
(RSC) codes separated by an interleaver, denoted by . A general turbo encoder is
shown in Figure 8.1.
Non-Binary Error Control Coding for Wireless Communication and Data Storage
Rolando Antonio Carrasco and Martin Johnston
C 2008 John Wiley & Sons, Ltd

282
NON-BINARY TURBO CODES
Interleaver
Œ†
RSC
Encoder 1
RSC
Encoder 2
Puncturing
(0)
tv
(2)
tv
(1)
tv
Encoder
Output
Message Bits xt
Œ†(x )
t
Figure 8.1
Turbo encoder.
The two encoders have code rates R1 and R2 respectively, and the overall code rate
R of the turbo encoder is given by:
R =
R1R2
R1 + R2 ‚àíR1R2
.
(8.1)
Usually the code rates of both encoders are the same and a parallel concatenation
of codes reduces the overall code rate. It is common to choose an RSC with code rate
1/2 as the component encoders result in an overall code rate of 1/3. We are not limited
to convolutional codes but could also choose block codes as the component encoders,
known as block turbo codes [2].
The upper encoder receives the data directly, while the lower encoder receives
the data after it has been interleaved by a permutation function, . The interleaver
is in general a pseudo random interleaver, that is it rearranges bits according to a
prescribed but randomly-generated rule. The interleaver operates in a block-wise
fashion, interleaving L bits at a time, and thus turbo codes can be viewed as block
codes. Since both encoders are systematic, the systematic output of the top encoder is
sent, while the systematic output of the lower encoder is not sent. However, the parity
bits of both encoders are transmitted. The overall code rate can be made higher by
puncturing, which operates only on the parity sequences ‚Äì the systematic bits are not
punctured. The code rate of a rate 1/3 turbo code is typically increased to 1/2 by only
transmitting the odd indexed parity bits from the upper encoder and the even indexed
parity bits from the lower encoder.
8.2.1 Notation
In this chapter, we use the following notation for the turbo encoding and decoding
processes:
 v(0)
t
‚àà{0, 1} is a turbo encoder output corresponding to an information bit at time t.
 v(1)
t
‚àà{0, 1} is a parity bit from the Ô¨Årst component RSC encoder.

THE TURBO ENCODER
283
 v(2)
t
‚àà{0, 1} is a parity bit from the second component RSC encoder.
 a(i)
t
‚àà{‚àí1, +1}, i = 0, 1, 2, is the mapping of v(i)
t
to the BPSK constellation, where
a(i)
t
= 2v(i)
t
‚àí1.
 v(0,s,s) ‚àà{0, 1} is the information bit corresponding to a transition from an initial
state s to the next state s.
 v(1,s,s) ‚àà{0, 1} is the parity bit from the Ô¨Årst RSC encoder corresponding to a
transition from state s to state s.
 v(2,s,s) ‚àà{0, 1} is the parity bit from the second RSC encoder corresponding to a
transition from state s to state s.
 a(i,s,s) ‚àà{‚àí1, +1} is equal to 2v(i,s,s) ‚àí1, i = 0, 1, 2.
 r(0)
t
is the received information bit, where r(0)
t
= a(0)
t
+ Œ∑t and Œ∑t is an additive white
Gaussian noise sample at time t.
 r(1)
t
is the received parity bit from the Ô¨Årst RSC encoder, where r(1)
t
= a(1)
t
+ Œ∑t.
 r(2)
t
is the received information bit from the second RSC encoder, where r(2)
t
= a(2)
t
+ Œ∑t.
Example 8.1: Binary turbo encoding: Figure 8.2 shows a turbo encoder with
(1, 5/7) RSC component codes with a pseudo random interleaver of length N =
5 bits. Therefore, the turbo encoder receives a 5 bit message xt, which is encoded
by the Ô¨Årst component encoder to obtain a 5 bit parity output v(1)
t . The message is
also interleaved (xt) and encoded by the second component encoder to obtain the
5 bit parity output v(2)
t . The turbo encoder output is therefore 15 bits long.
Let the message be:
xt = [x1x2, x3, x4, x5] = [1, 0, 1, 0, 1].
The state table for the Ô¨Årst component RSC encoder is given in Table 8.1.
The input sequence runs through the Ô¨Årst RSC encoder with transfer function
G(D), resulting in a parity sequence:
v(1)
t
=
%
v(1)
1 , v(1)
2 , v(1)
3 , v(1)
4 , v(1)
5
&
= [1, 1, 0, 1, 1].
Œ†
{3, 5, 1, 4, 2}
No Puncturing
)
0
(
tv
)
2
(
tv
)
1
(
tv
Encoder
Output
Message Bits xt
Œ†(xt)
Figure 8.2
Turbo encoder with (1, 5/7) RSC component codes.

284
NON-BINARY TURBO CODES
Table 8.1
State table for the Ô¨Årst component RSC encoder
xt
s
s
v(0,s,s)
t
v(1,s,s)
t
a(0,s,s)
t
a(1,s,s)
t
0
00
00
0
0
‚àí1
‚àí1
1
00
10
1
1
+1
+1
0
01
10
0
1
‚àí1
+1
1
01
00
1
0
+1
‚àí1
0
10
11
0
1
‚àí1
+1
1
10
01
1
0
+1
‚àí1
0
11
01
0
0
‚àí1
‚àí1
1
11
11
1
1
+1
+1
The sequence x is also passed through an interleaver or permuter of length
N = 5, denoted by , which produces the permuted output sequence x = (x) =
[1, 1, 1, 0, 0]. The sequence x is passed through another convolutional encoder
with transfer function G(D), which produces the output sequence:
v(2)
t
=
%
v(2)
1 , v(2)
2 , v(2)
3 , v(2)
4 , v(2)
5
&
= [1, 0, 1, 1, 1].
The three output sequences are multiplexed together to form the output sequence:
vt =
%
v(0)
1 , v(1)
1 , v(2)
1

,

v(0)
2 , v(1)
2 , v(2)
2

, . . . ,

v(1)
5 x, v(1)
5 , v(2)
5
&
= [(1, 1, 1), (0, 1, 0), (1, 0, 1), (0, 1, 1), (1, 1, 1)]
.
Assuming BPSK modulation with Es = 1, the mapped output at will be:
at = [(+1, +1, +1), (‚àí1, +1, ‚àí1), (+1, ‚àí1, +1), (‚àí1, +1, +1), (+1, +1, +1)] .
8.3 The Turbo Decoder
The near-Shannon limit performance of turbo codes is accomplished by their itera-
tive decoding algorithm. A general block diagram of the turbo decoder is shown in
Figure 8.3. The idea of the turbo decoding process is to extract extrinsic information
from the output of one decoder and pass it on to the second decoder in order to improve
the reliability of the second decoder‚Äôs output. Extrinsic information is then extracted
from the second decoder and passed on to the Ô¨Årst decoder to improve the reliability
of Ô¨Årst decoder‚Äôs output. This process is then repeated until no further gains in the
performance of the turbo decoder can be achieved.
The turbo decoder takes as its input the reliability values of the systematic in-
formation, r(0)
t , the parity bits from encoder 1, r(1)
t , and the interleaved parity bits
from encoder 2, r(2)
t . Turbo decoding is a two-stage process consisting of two soft-
input‚Äìsoft-output decoders in series. The Ô¨Årst decoder takes r(0)
t , r(1)
t
and prior in-
formation from decoder 2, which initially is zero. The output of decoder 1 is the
reliability of the systematic information. The prior information from decoder 2 and

THE TURBO DECODER
285
Decoder
1
Interleaver
Œ†
Decoder
2
Deinterleaver
Œ†-1
Interleaver
Œ†
Deinterleaver
Œ†-1
Œ£
Œ£
)
0
(
tr
)
2
(
tr
)1
(
tr
+
‚àí
‚àí
‚àí
‚àí
+
Extrinsic Information
A Priori Information
A Priori Information
Extrinsic Information
Decoded Output
Figure 8.3
Turbo decoder.
the reliability values of the systematic information are subtracted from the decoder
output, leaving extrinsic information, which is interleaved and becomes the prior
information for decoder 2.
Decoder 2 now takes the interleaved systematic information, the reliability values
of the interleaved parity bits r(2)
t
and the prior information from decoder 1. The output
is the reliability of the interleaved systematic information. The prior information and
the reliability values of the interleaved systematic information are subtracted from
the decoder output, resulting in extrinsic information, which is now deinterleaved
to become once again the prior information for decoder 1. This is called a decoder
iteration, where after each information the prior information to both decoders becomes
more reliable, thus improving the reliabilities of the decoder outputs. After each
iteration, the improvement in performance becomes less until it converges and no
further improvements can be achieved. Alternatively, instead of feeding back the
extrinsic information to decoder 1, the output of decoder 2 can be deinterleaved to
give the reliability of r(0)
t . A hard decision can then be made to recover the systematic
information.
8.3.1 Log-Likelihood Ratios (LLRs)
One measure of the reliability of a variable is to calculate its log-likelihood ratio
(LLR). For some variable u, its LLR L(u) is deÔ¨Åned as:
L(u) = ln
 P(u = +1)
P(u = ‚àí1)

,
(8.2)

286
NON-BINARY TURBO CODES
where P(u = +1) = 1 ‚àíP(u = ‚àí1). We can see that if P(u = +1) = P(u = ‚àí1) = 0.5
then L(u) = 0, that is the reliability of u is zero. As stated previously, the turbo decoder
takes as its inputs the reliabilities of the systematic information and parity bits from
encoders 1 and 2. If we deÔ¨Åne the received systematic information bit at time t as r(0)
t
then:
L

r(0)
t
| a(0)
t

= ln

P

r(0)
t
| a(0)
t
= +1

P

r(0)
t
| a(0)
t
= ‚àí1


.
(8.3)
For the AWGN channel and BPSK modulation with constellation points at ¬±‚àöEs:
P

r(0)
t
| a(0)
t
= +1

=
1
œÉ
‚àö
2œÄ e‚àí

r(0)
t ‚àí‚àöEs
2
/2œÉ 2
1
œÉ
‚àö
2œÄ e‚àí

r(0)
t +‚àöEs
2
/2œÉ 2 +
1
œÉ
‚àö
2œÄ e‚àí

r(0)
t ‚àí‚àöEs
2
/2œÉ 2
P

r(0)
t
| a(0)
t
= ‚àí1

=
1
œÉ
‚àö
2œÄ e‚àí

r(0)
t +‚àöEs
2
/2œÉ 2
1
œÉ
‚àö
2œÄ e‚àí

r(0)
t +‚àöEs
2
/2œÉ 2 +
1
œÉ
‚àö
2œÄ e‚àí

r(0)
t ‚àí‚àöEs
2
/2œÉ 2 .
Therefore:
ln

P

r(0)
t
| a(0)
t
= +1

P

r(0)
t
| a(0)
t
= ‚àí1


= ln
Ô£´
Ô£≠e‚àí

r(0)
t ‚àí‚àöEs
2
/2œÉ 2
e‚àí

r(0)
t +‚àöEs
2
/2œÉ 2
Ô£∂
Ô£∏= ln

e‚àí

r(0)
t ‚àí‚àöEs
2
/2œÉ 2
‚àíln

e‚àí

r(0)
t +‚àöEs
2
/2œÉ 2
= ‚àí

r(0)
t
‚àí‚àöEs
2
2œÉ 2
‚àí‚àí

r(0)
t
+ ‚àöEs
2
2œÉ 2
=
1
2œÉ 2

‚àí

r0
t
2 + 2

Esr(0)
t
‚àí1 +

r(0)
t
2 + 2

Esr(0)
t
+ 1

= 2‚àöEsr(0)
t
œÉ 2
.
The term 2‚àöEs
œÉ 2
is called the channel reliability and denoted by Lc. Therefore:
L

r(0)
t
| a(0)
t

= Lcr(0)
t
= 2‚àöEsr(0)
t
œÉ 2
.
(8.4)
The output of decoder 1 is deÔ¨Åned as:
L

a(0)
t
| r

= ln

P

a(0)
t
= +1 | r

P

a(0)
t
= ‚àí1 | r


,
(8.5)

THE TURBO DECODER
287
Decoder
1
Interleaver
Œ†
Decoder
2
Deinterleaver
Œ†-1
Interleaver
Œ†
Deinterleaver
Œ†-1
Œ£
Œ£
)
0
(
t
cr
L
)
2
(
t
cr
L
)
1
(
t
cr
L
+
‚àí
‚àí
‚àí
‚àí
+
)
|
(
)
0
(
r
ta
L
(
)
)
0
(
1
t
e a
L
(
)
(
)
r
|
)
0
(
ta
L
Œ†
(
)
)
0
(
t
cr
L
Œ†
(
)
(
)
)
0
(
2
t
e a
L
Œ†
(
)
(
)
)
0
(
2
ta
L
Œ†
(
)
)
0
(
1
ta
L
(
)
r
|
)
0
(
ta
L
Figure 8.4
A more detailed diagram of the turbo decoder.
where r is the received vector. Finally, the prior information into decoder 1 is the
deinterleaved extrinsic information from decoder 2.
L1

a(0)
t

= Le2



a(0)
t

= ln

P

a(0)
t
= +1

P

a(0)
t
= ‚àí1


.
(8.6)
Therefore, the extrinsic information from decoder 1 is:
Le1

a(0)
t

= L1

a(0)
t
|r

‚àíL1

a(0)
t

‚àíLcr(0)
t .
(8.7)
Similarly, the extrinsic information from decoder 2 is:


Le2

a(0)
t

= 

L2

a(0)
t
| r

‚àí

L2

a(0)
t

‚àí

Lcr(0)
t

.
(8.8)
A more detailed block diagram of the turbo decoder showing the different LLRs is
given in Figure 8.4.
When no more iterations are required, a hard decision is made on L(a(0)
t
| r) from
the deinterleaved output of decoder 2, depending on the sign of the LLR.
If
sign

L

a(0)
t
|r

< 0,
a(0)
t
= ‚àí1
‚à¥v(0)
t
= 0
If
sign

L

a(0)
t
|r

‚â•0,
a(0)
t
= +1
‚à¥v(0)
t
= 1.
(8.9)
8.3.2 Maximum A Posteriori (MAP) Decoding
The BCJR or MAP algorithm was originally presented coincidentally in [2] and
[5], and later in [3]. It is a modiÔ¨Åed version of this that is used by each of the
component decoders of the MAP turbo decoder. This algorithm is used to perform the

288
NON-BINARY TURBO CODES
symbol-by-symbol MAP decoding referred to earlier. The turbo decoder‚Äôs decision is
whether ÀÜv(0)
t
= 1 or 0.
It is presumed that ÀÜv(0)
t
= 1 if P(a(0)
t
= +1 | r) > P(a(0)
t
= ‚àí1 | r) and that ÀÜv(0)
t
= 0
if P(a(0)
t
= ‚àí1 | r) > P(a(0)
t
= +1 | r).
Put more simply, it can be said that the decision ÀÜv(0)
t
is given by:
ÀÜv(0)
t
= sign
%
L

a(0)
t
| r
&
.
(8.10)
The state transitions within the encoder‚Äôs trellis are governed by the transitional
probabilities:
P(s, s) = P(st+1 = s | st = s),
0 ‚â§s, s ‚â§Ms ‚àí1.
(8.11)
The posterior probabilities can be deÔ¨Åned as:
P

a(0)
t
= ‚àí1 | r

=

(s,s)‚ààS‚àí
P(st = s, st+1 = s | r)
P

a(0)
t
= +1 | r

=

(s,s)‚ààS+
P(st = s, st+1 = s | r),
which can be modiÔ¨Åed using Bayes‚Äô theorem to:
P

a(0)
t
= ‚àí1 | r

=

s‚àí
P(st = s, st+1 = s, r)
P(r)
P

a(0)
t
= +1 | r

=

s+
P(st = s, st+1 = s, r)
P(r)
,
(8.12)
where st ‚ààS is the state of the encoder at time t, S+ is the set of ordered pairs (s, s)
corresponding to all state transitions (st = s) ‚Üí(st+1 = s) brought about by the data
input v(0)
t
= 1, and S‚àíis deÔ¨Åned in the same way for v(0)
t
= 0. The received noisy
code word is expressed as r = (r1,r2, . . . ,rN). Therefore, (8.5) can be rewritten as:
L

a(0)
t
| r

= ln
Ô£´
Ô£¨Ô£≠

(s,s)‚ààS+ (P(st = s, st+1 = s, r)

(s,s)‚ààS‚àí‚àí(P(st = s, st+1 = s, r)
Ô£∂
Ô£∑Ô£∏.
(8.13)
Consequently, the joint probability:
P(s, s, r) = P(st+1 = s, st = s, r),
0 ‚â§s, s ‚â§Ms ‚àí1.
(8.14)
(8.13) can then be rewritten in the following form:
P(st = s, st+1 = s, r) = P(st = s, st+1 = s, r<t, rt, r>t).
(8.15)

THE TURBO DECODER
289
Applying Bayes‚Äô theorem, this becomes:
P(st = s, st+1 = s, r<t, rt, r>t) = P(st = s, st+1 = s, r<t, rt)
√ó P(r>t | st = s, st+1 = s, r<t, rt)
= P(st = s, st+1 = s, r<t, rt)P(r>t | st = s)
= P(st+1 = s, rt | st = s)P(st = s, r<t)
√ó P(r>t | st = s, st+1 = s, r<t, rt)
.
(8.16)
(8.16) is now expressed in terms of three probabilities:
Œ±t(s) = P(st = s, r<t),
(8.17)
Œ≤t+1(s) = P(r>t | st = s),
(8.18)
Œ≥t(s, s) = P(st+1 = s, rt | st = s).
(8.19)
This gives:
P(st = s, st+1 = s,r<t,rt,r>t) = Œ±t(s)Œ≤t+1(s)Œ≥t(s, s).
Therefore (8.12) can be written as:
L

a(0)
t
| r

= ln
Ô£´
Ô£¨Ô£≠

(ss)‚ààS+ Œ±t(s) ¬∑ Œ≤t+1(s) ¬∑ Œ≥t(s, s)

(s,s)‚ààS‚àí‚àíŒ±t(s) ¬∑ Œ≤t+1(s) ¬∑ Œ≥t(s, s)
Ô£∂
Ô£∑Ô£∏.
(8.20)
The values of Œ±t(s), Œ≤t+1(s) and Œ≥ t(s, s) can be determined by forming recursive
relationships. To determine Œ±t+1(s):
Œ±t+1(s) = P(st+1 = s, r<t+1) = P(st+1 = s, r<t, rt)
=
Ms‚àí1

s=0
P(st+1 = s, rt, st = s, r<t)
=
Ms‚àí1

s=0
P(st = s, r<t)P(st+1 = s, rt | st = s, r<t)
=
Ms‚àí1

s=0
P(st = s, r<t)P(st+1 = s, rt | st = s)
=
Ms‚àí1

s=0
Œ±t(s)Œ≥t(s, s)
.
(8.21)

290
NON-BINARY TURBO CODES
To determine Œ≤t(s):
Œ≤t(s) = P(r>t‚àí1 | st = s) = P(r>t,rt | st = s)
=
Ms‚àí1

s=0
P(r>t, rt, st+1 = s | st = s)
=
Ms‚àí1

s=0
P(rt, st+1 = s | st = s)P(r>t | rt, st+1 = s, st = s)
=
Ms‚àí1

s=0
P(rt, st+1 = s | st = s)P(r>t | st+1 = s)
=
Ms‚àí1

s=0
Œ≥t(s, s)Œ≤t+1(s)
.
(8.22)
Finally, to determine Œ≥ t(s, s):
Œ≥t(s, s) = P(st+1 = s, rt | st = s) = P(rt | st = s, st+1 = s)
¬∑ P(st = s, st+1 = s),
(8.23)
P(rt | st = s, st+1 = s) =
1
‚àö
2œÄœÉ 2 e

‚àí
1
2œÉ2
rt‚àí‚àöEsa(s,s)
2
,
(8.24)
Œ≥t(s, s) = P(st = s, st+1 = s)
1
‚àö
2œÄœÉ 2 e

‚àí
1
2œÉ2
rt‚àí‚àöEsa(s,s)
2
.
(8.25)
We can simplify (8.23) by assuming that if each information bit is equally likely
to be a 0 or a 1 then all states are also equally likely and P(st = s, st+1 = s) is a
constant. Therefore:
Œ≥ (s, s) =
1
‚àö
2œÄœÉ 2 e

‚àí
1
2œÉ2
rt‚àí‚àöEsa(s,s)
2
.
(8.26)
8.3.3 Max-Log MAP
The amount of memory required, and the number of operations involving exponential
values and multiplicative procedures, means that the complexity of the MAP algorithm
becomes prohibitive when implementing a turbo decoder. The system can be simpliÔ¨Åed
by employing the logarithms of the probabilities deÔ¨Åned in (8.16), (8.17) and (8.18)
and thus transforming any multiplicative operations to summations. This gives the
following:
¬ØŒ±t(s) = ln Œ±t(s),
(8.27)
¬ØŒ≤t(s) = ln Œ≤t(s),
(8.28)
¬ØŒ≥t(s, s) = ln Œ≥t(s, s).
(8.29)

THE TURBO DECODER
291
This means that, with reference to (8.20), Œ±t+1(s) becomes:
Œ±t+1(s) = ln
Ms‚àí1

s=0
Œ±t(s)Œ≥t(s, s)

= ln
Ms‚àí1

s=0
e(¬ØŒ±t(s)+ ¬ØŒ≥t(s,s))

.
(8.30)
To evaluate (8.28) we can use the following approximation:
ln(eŒ¥1 + eŒ¥2 + ¬∑ ¬∑ ¬∑ + eŒ¥n) ‚âà
max
i=1,2,...,n Œ¥i.
(8.31)
Therefore:
Œ±t+1(s) = ln
Ms‚àí1

s=0
e(¬ØŒ±t(s)+ ¬ØŒ≥t(s,s))

‚âàmax
s

¬ØŒ±t(s) + ¬ØŒ≥t(s, s)

.
(8.32)
Similarly, for Œ≤t(s):
Œ≤t(s) = ln
Ms‚àí1

s=0
Œ≥t(s, s)Œ≤t+1(s)

= ln
Ms‚àí1

s=0
e( ¬ØŒ≥t(s,s)+ ¬ØŒ≤t+1(s)

‚âàmax
s

¬ØŒ≥t(s, s) + ¬ØŒ≤t+1(s)

.
(8.33)
Finally, for Œ≥t(s, s):
¬ØŒ≥ (s, s) = ln

1
‚àö
2œÄœÉ 2 e

‚àí
1
2œÉ2
rt‚àí‚àöEsa(s,s)
2
= ln

1
‚àö
2œÄœÉ 2

+ ln e

‚àí
1
2œÉ2
rt‚àí‚àöEsa(s,s)
2
= ln

1
‚àö
2œÄœÉ 2

‚àí
1
2œÉ 2
rt ‚àí

Esa(s,s)2.
By neglecting constants we obtain:
¬ØŒ≥t(s, s) = ‚àí
rt ‚àí

Esa(s,s)2.
(8.34)
Inserting these values into (8.20) yields:
L

a(0)
t
| r

= ln

(s,s)‚ààS+ eŒ±t(s)+Œ≥ t(s,s)+Œ≤t+1(s)

(s,s)‚ààS‚àíeŒ±t(s)+Œ≥ t(s,s)+Œ≤t+1(s) .
(8.35)
(8.34) can be estimated as:
L

a(0)
t
| r

‚âà
max
(s,s)‚ààS+[Œ≥ t(s, s) + Œ±t(s) + Œ≤t+1(s)]
‚àímax
(s,s)‚ààS‚àí[Œ≥ t(s, s) + Œ±t(s) + Œ≤t+1(s)].
(8.36)

292
NON-BINARY TURBO CODES
8.3.4 Log MAP Algorithm
The max-log MAP algorithm approximates the log-likelihood ratio L(a(0)
t
| r), and is
therefore suboptimal. The log MAP algorithm instead uses the Jacobian logarithm [6]
to improve the log-likelihood ratio and therefore improve the decoding abilities of the
algorithm with only a small increase in complexity:
ln(eŒ¥1 + eŒ¥2) = max(Œ¥1, Œ¥2) + ln(1 ‚àíe‚àí| Œ¥2‚àíŒ¥1 | ).
(8.37)
8.3.5 Iterative Log MAP Decoding
If we assume that the encoder is systematic with a rate R = 0.5 then the received
vector r at time t can be expressed as rt = (r(0)
t ,r(1)
t ), where r(0)
t
is the information bit
and r(1)
t
is the parity bit.
(8.22) can then be written as [7]:
Œ≥t(s, s) = P(rt | st = s, st+1 = s)P(st+1 = s, st = s)
= P

r(0)
t ,r(1)
t
| st = s, st+1 = s

P(st+1 = s, st = s)
= P

r(0)
t ,r(1)
t
| st = s, st+1 = s

P

a(0)
t
= a(0,s,s)
.
(8.38)
Since st+1 is determined by a(0,s,s), P(st+1 = s | st = s) = P(a(0)
t
= a(0,s,s)).
Therefore:
P

r(0)
t ,r(1)
t
| st = s, st+1 = s

= P

r(0)
t ,r(1)
t
| st = s, a(0)
t
= a(0,s,s)
.
(8.39)
Since the encoder is systematic, r(0)
t
is independent of the state transitions and only
dependent on the input:
P

r(0)
t ,r(1)
t
| st = s, a(0)
t
= a(0,s,s)
= P

r(0)
t
| a(0)
t

P

r(1)
t
| st = s, st+1 = s

= P

r(0)
t
| a(0)
t

P

r(1)
t
| a(1)
t
= a(1,s,s)
.
(8.40)
Substituting (8.38) into (8.36) gives:
Œ≥t(s, s) = P

r(0)
t ,r(1)
t
| st = s, st+1 = s

P

a(0)
t
= a(0,s,s)
= P

r(0)
t
| v(0)
t

P

r(1)
t
| a(1)
t
= a(1,s,s)
P

a(0)
t
= a(0,s,s),
(8.41)
¬ØŒ≥t(s, s) = ln
%
P

r(0)
t
| a(0)
t

P

r(1)
t
| a(1)
t
= a(1,s,s)
P

a(0)
t
= a(0,s,s)&
= ln P

r(0)
t
| a(0)
t
= a(0,s,s)
+ ln P

r(1)
t
| a(1)
t
= a(1,s,s)
+ ln P

a(0)
t
= a(0,s,s)
.
(8.42)
From (8.5):
L1

a(0)
t

= ln

P

a(0)
t
= +1

P

a(0)
t
= ‚àí1


= ln

P

a(0)
t
= +1

1 ‚àíP

a(0)
t
= ‚àí1


,

THE TURBO DECODER
293
eL1

a(0)
t

=
P

a(0)
t
= +1

1 ‚àíP

a(0)
t
= +1
,
(8.43)
P

a(0)
t
= +1

=
eL1

a(0)
t

1 + eL1

a(0)
t
 =
1
1 + e‚àíL1

a(0)
t
 .
(8.44)
Similarly:
L1

v(0)
t

= ln

P

a(0)
t
= +1

P

a(0)
t
= ‚àí1


= ln

1 ‚àíP

a(0)
t
= ‚àí1

P

a(0)
t
= ‚àí1


eL1

a(0)
t

= 1 ‚àíP

a(0)
t
= ‚àí1

P

a(0)
t
= ‚àí1

,
(8.45)
P

a(0)
t
= ‚àí1

=
1
1 + eL1

a(0)
t
 .
(8.46)
In general:
P

a(0)
t
= a(0,s,s)
=
e
‚àíL1

a(0)
t

2
1 + e
‚àíL1

a(0)
t

2
e
a(0,s,s)L1

a(0)
t

2
.
(8.47)
The term
e
‚àíL1(a(0)
t
)
2
1+e
‚àíL1(a(0)
t
)
2
is independent of a(0,s,s) and can be taken out so we obtain:
ln

P

a(0)
t
= a0,s,s
= a(0,s,s)L1

a(0)
t

2
.
(8.48)
We can derive similar expressions to determine ln P(r(0)
t
| a(0)
t
= a(0,s,s)) and
ln P(r(1)
t
| a(1)
t
= a(1,s,s)) using (8.3):
ln

P

r(0)
t
| a(0)
t
= 1

P

r(0)
t
| a(0)
t
= 0


= ln

P

r(0)
t
| a(0)
t
= 1

1 ‚àíP

r(0)
t
| a(0)
t
= 1


= Lcr(0)
t
P

r(0)
t
| a(0)
t
= 1

=
eLcr(0)
t
1 + eLcr(0)
t
=
1
1 + e‚àíLcr(0)
t
.
Also, P(r(0)
t
| a(0)
t
= 0) =
1
1+eLcr(0)
t . Therefore:
P

r(0)
t
| a(0)
t
= a(0,s,s)
=
e
‚àíLcr(0)
t
2
1 + e
‚àíLcr(0)
t
2
e
a(0,s,s)Lcr(0)
t
2
.

294
NON-BINARY TURBO CODES
Again, ignoring the constant term we obtain:
ln

P

r(0)
t
| a(0)
t
= a0,s,s
= a(0,s,s)Lcr(0)
t
2
.
(8.49)
In the same way:
ln

P

r(1)
t
| a(1)
t
= a1,s,s
= a(1,s,s)Lcr(1)
t
2
.
(8.50)
Substituting (8.46), (8.47) and (8.48) into (8.40) gives [7]:
¬ØŒ≥t(s, s) = a(0,s,s)L1

a(0)
t

2
+ a(0,s,s)Lcr(0)
t
2
+ a(1,s,s)Lcr(1)
t
2
.
(8.51)
8.4 Non-Binary Turbo Codes
Extending binary turbo codes to non-binary turbo codes [8, 9, 10] can be considered
less complicated than the extension of binary LDPC codes to non-binary LDPC codes.
In particular, the principle of the non-binary turbo decoding algorithm remains the
same. One of the main differences is the trellis diagram associated with a non-binary
convolutional code, which has more branches leaving and entering nodes in the trellis,
resulting in more paths and higher decoding complexity. Secondly, an increase in the
size of the alphabet means that the reliabilities of these extra symbols must also be
considered. The non-binary turbo encoder has the same structure as the binary turbo
encoder in Figure 8.1, with the component encoders being replaced by RSC codes
deÔ¨Åned over a ring of integers Zq, where q is the cardinality of the ring. The non-binary
turbo encoder is given in Figure 8.5.
The message symbols xt and the turbo encoder output symbols v(0)
t , v(1)
t , v(2)
t
are
now deÔ¨Åned over Zq.
Interleaver
Œ†
q-ary RSC
Encoder 1
q-ary RSC
Encoder 2
Puncturing
)
0
(
tv
)
2
(
tv
)
1
(
tv
Encoder
Output
Message Symbols xt
Œ†(xt)
Figure 8.5
The non-binary turbo encoder.

NON-BINARY TURBO CODES
295
Example 8.2: Non-binary turbo encoding: A non-binary turbo encoder deÔ¨Åned
over Z4 with 23/1 RSC component encoders is shown in Figure 8.6.
Œ†
{3, 5, 1, 4, 2}
No Puncturing
)
0
(
tv
)
2
(
tv
)
1
(
tv
Encoder
Output
Message Symbols xt
3
2
1
Œ†(xt)
3
2
1
Figure 8.6
Non-binary turbo encoder with 23/1 RSC component codes.
Let the message be xt = [x1 x2, x3, x4, x5] = [1, 2, 3, 3, 3]. The state table for the
Ô¨Årst component 23/1 RSC encoder deÔ¨Åned over Z4 is given in Table 8.2.
Table 8.2
State table for the 23/1 RSC code.
xt
s
s
v(0,s,s)
v(1,s,s)
0
0
0
0
0
1
0
1
1
2
2
0
2
2
0
3
0
3
3
2
0
1
1
0
1
1
1
2
1
3
2
1
3
2
1
3
1
0
3
3
0
2
2
0
2
1
2
3
1
0
2
2
0
2
2
3
2
1
3
0
0
3
0
0
3
1
3
0
1
1
2
3
1
2
3
3
3
2
3
1

296
NON-BINARY TURBO CODES
The input sequence runs through the Ô¨Årst RSC encoder, resulting in a parity
sequence:
v(1)
t
=
%
v(1)
1 , v(1)
2 , v(1)
3 , v(1)
4 , v(1)
5
&
= [2, 1, 1, 0, 3].
The sequence xt is also passed through the interleaver of length N = 5, which
produces the permuted output sequence x
t = (xt) = [3, 3, 1, 3, 2]. The sequence
x
t is passed through the second convolutional encoder, which produces the output
sequence:
v(2)
t
=
%
v(2)
1 , v(2)
2 , v(2)
3 , v(2)
4 , v(2)
5
&
= [2, 1, 0, 1, 2].
The three output sequences are multiplexed together to form the output sequence:
vt =
%
v(0)
1 , v(1)
1 , v(2)
1

,

v(0)
2 , v(1)
2 , v(2)
2

, . . . ,

v(0)
5 , v(1)
5 , v(2)
5
&
= [(1, 2, 2), (2, 1, 1), (3, 1, 0), (3, 0, 1), (3, 3, 2)]
.
8.4.1 Multi-Dimensional Log-Likelihood Ratios
The binary log-likelihood ratio is a measure of reliability and is deÔ¨Åned as the natural
logarithm of the ratio of the likelihood of an event being 1 to the likelihood of the
same event being 0. However, if we expand to a ring of integers Zq we must consider
the reliabilities of the other symbols too. The LLRs for an event u being an element
in Z4 are [10]:
L(1)(u) = ln
 P(u = 1)
P(u = 0)

L(2)(u) = ln
 P(u = 2)
P(u = 0)

L(3)(u) = ln
 P(u = 3)
P(u = 0)

.
(8.52)
The output of decoder 1 is therefore deÔ¨Åned by the multi-dimensional LLR:

L(1)
v(0)
t
| r

= ln

P

v(0)
t
= 1 | r

P

v(0)
t
= 0 | r
)

,
L(2)
v(0)
t
| r

= ln

P

v(0)
t
= 2 | r

P

v(0)
t
= 0 | r


,
L(3)
v(0)
t
|r

= ln

P

v(0)
t
= 3 | r

P

v(0)
t
= 0 | r


,
(8.53)
where r is the received vector. In the binary case, the sign of the LLR was used to
make a hard decision on the decoded symbol. A negative LLR corresponded to a 0
and a positive LLR corresponded to a 1. This is also true for multi-dimensional LLRs.

NON-BINARY TURBO CODES
297
If ln( P(u=2)
P(u=0)) is negative, the hard decision will be u = 0; if it is positive the hard
decision will be u = 2.
In the non-binary turbo decoding procedure, the prior information into decoder 1
is the deinterleaved extrinsic information from decoder 2.

L(1)
1

v(0)
t

, L(2)
1

v(0)
t

, L(3)
1

v(0)
t

=

L(1)
e2

‚àí1
v(0)
t

, L(2)
e2

‚àí1
v(0)
t

, L(3)
e2

‚àí1
v(0)
t

.
(8.54)
Therefore, the extrinsic information from decoder 1 is:
L(1)
e1

v(0)
t

= L(1)
1

v(0)
t
| r

‚àíL(1)
1

v(0)
t

‚àíL(1)
r(0)
t
| v(0)
t

L(2)
e1

v(0)
t

= L(2)
1

v(0)
t
| r

‚àíL(2)
1

v(0)
t

‚àíL(2)
r(0)
t
| v(0)
t

L(3)
e1

v(0)
t

= L(3)
1

v(0)
t
| r

‚àíL(3)
1

v(0)
t

‚àíL(3)
r(0)
t
| v(0)
t

.
(8.55)
Similarly, the extrinsic information from decoder 2 is:
L(1)
e2

v(0)
t

= L(1)
2

v(0)
t
| r

‚àíL(1)
2

v(0)
t

‚àíL(1)
2



r(0)
t
| v(0)
t

L(2)
e2

v(0)
t

= L(2)
2

v(0)
t
| r

‚àíL(2)
2

v(0)
t

‚àíL(2)
2



r(0)
t
| v(0)
t

L(3)
e2

v(0)
t

= L(3)
2

v(0)
t
| r

‚àíL(3)
2

v(0)
t

‚àíL(3)
2



r(0)
t
| v(0)
t

.
(8.56)
8.4.2 Non-Binary Iterative Turbo Decoding
Now that all the necessary multi-dimensional LLRs are deÔ¨Åned we can present the
non-binary turbo decoder. The complete non-binary turbo decoder over Z4 is shown
in Figure 8.7.
A hard decision is made on the multi-dimensional LLRs from the deinterleaved
output of decoder 2.
If
sign

L(i)
v(0)
t
|r

< 0,
v(0)
t
= 0
If
sign

L(i)
v(0)
t
|r

‚â•0,
v(0)
t
= i,
where
i = 1, 2, . . . , q ‚àí1.
(8.57)
Hence, there are q ‚àí1 candidate values for the decoded symbol. The most likely
element is determined by comparing each LLR value L(i)(vt | r) and choosing the
LLR with the largest magnitude (the highest reliability).
One concern with regards to using q ‚àí1 LLR values of the form ln ( P(u=i)
P(u=0)),
i = 1, 2, . . . , q ‚àí1 is that we only have a measure of reliability of the nonzero
elements with respect to zero, and not to each other. For the case of element in Z4 we
would have the extra LLRs:
ln
 P(u = 2)
P(u = 1)

ln
 P(u = 3)
P(u = 1)

ln
 P(u = 3)
P(u = 2)

.

298
NON-BINARY TURBO CODES
Decoder
1
Œ£
(
)
(
)
(
)Ô£∑
Ô£∑
Ô£∑
Ô£∏
Ô£∂
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Ô£´
)1
(
)
3
(
)
1
(
)
2
(
)
1
(
)
1
(
|
|
|
t
t
t
v
L
v
L
v
L
r
r
r
(
)
(
)
(
)Ô£∑
Ô£∑
Ô£∑
Ô£∏
Ô£∂
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Ô£´
)
2
(
)
3
(
)
2
(
)
2
(
)
2
(
)
1
(
|
|
|
t
t
t
v
L
v
L
v
L
r
r
r
(
)
(
)
(
)Ô£∑
Ô£∑
Ô£∑
Ô£∏
Ô£∂
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Ô£´
)
0
(
)
3
(
)
0
(
)
2
(
)
0
(
)
1
(
|
|
|
t
t
t
v
L
v
L
v
L
r
r
r
(
)
(
)
(
)Ô£∑
Ô£∑
Ô£∑
Ô£∏
Ô£∂
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Ô£´
r
r
r
|
|
|
)
0
(
)
3
(
)
0
(
)
2
(
)
0
(
)
1
(
t
t
t
v
L
v
L
v
L
(
)
(
)
(
)Ô£∑
Ô£∑
Ô£∑
Ô£∏
Ô£∂
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Ô£´
)
0
(
)
3
(
1
)
0
(
)
2
(
1
)
0
(
)
1(
1
t
t
t
v
L
v
L
v
L
Decoder
2
Œ†
Œ†-1
(
)
(
)
(
)Ô£∑
Ô£∑
Ô£∑
Ô£∏
Ô£∂
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Ô£´
)
0
(
)
3
(
)
0
(
)
2
(
)
0
(
)
1
(
1
1
1
t
e
t
e
t
e
v
L
v
L
v
L
Œ£
Œ†
(
)
(
)
(
)
(
)
(
)
(
)Ô£∑
Ô£∑
Ô£∑
Ô£∏
Ô£∂
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Ô£´
Œ†
Œ†
Œ†
)
0
(
)
3
(
2
)
0
(
)
2
(
2
)
0
(
)
1(
2
t
t
t
v
L
v
L
v
L
(
)
(
)
(
)
(
)
(
)
(
)Ô£∑
Ô£∑
Ô£∑
Ô£∏
Ô£∂
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Ô£´
Œ†
Œ†
Œ†
r
r
r
|
|
|
)
0
(
)
3
(
)
0
(
)
2
(
)
0
(
)
1
(
t
t
t
v
L
v
L
v
L
(
)
(
)
(
)
(
)
(
)
(
)Ô£∑
Ô£∑
Ô£∑
Ô£∏
Ô£∂
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Ô£´
Œ†
Œ†
Œ†
r
r
r
|
|
|
)
0
(
)
3
(
)
0
(
)
2
(
)
0
(
)
1
(
t
t
t
v
L
v
L
v
L
(
)
(
)
(
)
(
)
(
)
(
)Ô£∑
Ô£∑
Ô£∑
Ô£∏
Ô£∂
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Ô£´
Œ†
Œ†
Œ†
)
0
(
)
3
(
)
0
(
)
2
(
)
0
(
)
1
(
2
2
2
t
e
t
e
t
e
v
L
v
L
v
L
‚àí
‚àí
‚àí
‚àí
+
+
Œ†-1
(
)
(
)
(
)Ô£∑
Ô£∑
Ô£∑
Ô£∏
Ô£∂
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Ô£´
r
r
r
|
|
|
)
0
(
)
3
(
)
0
(
)
2
(
)
0
(
)
1
(
t
t
t
v
L
v
L
v
L
Figure 8.7
The non-binary turbo decoder deÔ¨Åned over Z4.
Including these extra LLRs in the decoding procedure will provide more informa-
tion about the reliability of each symbol, but the number of extra LLRs will increase
exponentially with increasing alphabet size.
8.5 Conclusions
This chapter introduced the concept of non-binary turbo encoding and decoding
deÔ¨Åned over rings. It can be seen that the non-binary turbo encoder structure is the
same as the binary turbo encoder structure, with non-binary RSC codes replacing
the binary RSC codes. Non-binary turbo decoding was explained by Ô¨Årst introducing
multi-dimensional log-likelihood ratios. Essentially, the principle of non-binary turbo
decoding is the same, with the exception that we must now process an array of LLR
values for each nonzero element in the ring, instead of just one.
Non-binary turbo decoding is an area of coding theory that has not received much
attention in the literature, most likely due to the extra complexity in implementation.
However, with non-binary LDPC codes recently becoming more popular, we would
expect non-binary turbo codes to perform just as well and this would be an interesting
area of research for the future.

REFERENCES
299
References
[1] Berrou, C., Glaviuex, A. and Thitimijshima, P. (1993) Near shannon limit error-correcting coding
and decoding: turbo codes. Proceedings of the IEEE International Conference on Communications,
Geneva, Switzerland, pp. 1064‚Äì70.
[2] Bahl, L.R., Cocke, J., Jelinek, F. and Raviv, J. (1972) Optimal decoding of linear codes for
minimising symbol error rate. Abstracts of Papers, International Symposium on Information Theory,
p. 50.
[3] Bahl, L.R., Cocke, J., Jelinek, F. and Raviv, J. (1974) Optimal decoding of linear codes for
minimising symbol error rate. IEEE Transactions on Information Theory, IT-20 (2), 284‚Äì7.
[4] Pyndiah, R., Glavieux, A., Picart, A. and Jacq, S. (1994) Near optimum decoding of products codes.
Proceedings of the IEEE GlobeCom‚Äô94 Conference San Francisco, CA, Vol. 1/3, pp. 339‚Äì43.
[5] McAdam, P.L., Welch, L. and Weber, C. (1972) MAP bit decoding of convolutional codes. Abstracts
of Papers, International Symposium on Information Theory, p. 9.
[6] Erfanian, J.A., Pasupathy, S. and Gulot, G. (1994) Reduced complexity symbol detectors with
parallel structures for ISI channels. IEEE Transactions on Communications, 42 (234, Part 3),
1661‚Äì71.
[7] Moon, T.K. (2005) Error Correction Coding. Mathematical Methods and Algorithms, Wiley Inter-
science, ISBN 0-471-64800-0.
[8] Berrou, C., Jezequel, M., Douillard, C. and Kerouedan, S. (2001) The Advantages of Non-Binary
Turbo Codes, IEEE Information Theory Workshop, Australia.
[9] Berrou, C. and Jezequel, M. (1999) Non-binary convolutional codes for turbo coding. IET Elec-
tronics Letters, 35 (1), 39‚Äì40.
[10] Berkmann, J. (1998) On turbo decoding of non-binary turbo codes. IEEE Communications Letters,
2 (4), 94‚Äì6.


Index
afÔ¨Åne
curve, 60
points, 61
space, 60
algorithm
BCJR, 287
Belief Propagation, 208
Berlekamp-Massey, 93
Euclid‚Äôs, 89
Guruswami-Sudan, 148
Kotter-Vardy, 164
log MAP, 292
max-log MAP, 290
Roth-Ruckenstein, 159
Sakata, 119
Viterbi, 45, 265
angle of arrival, 28
array dispersion, 219
asymptotic coding gain (ACG), 100, 244
balanced incomplete block design (BIBD),
203
butterÔ¨Çy diagram, 226
channel
additive white Gaussian noise (AWGN),
17
binary erasure, 13
binary symmetric, 12
capacity, 11, 20
fading, 20
Ô¨Åxed wireless access, 34
indoor, 35, 39
magnetic storage, 43
multiple-input-multiple-output (MIMO),
40
pedestrian, 35, 39
quaternary symmetric
vehicular, 36, 40
code
algebraic-geometric, 70, 109
Bose-Chaudhuri-Hocquenghem (BCH),
84
block coded modulation (BCM), 96
convolutional, 238
cyclic, 80
functional Goppa, 70
Hamming, 75
Hermitian, 110
low density parity check (LDPC), 201
multi-level block, 97
parity check, 74
repetition, 97
Reed-Solomon, 86, 115, 148, 164
residue Goppa, 70
ring trellis coded modulation
(Ring-TCM), 246
ring BCM, 103
TCM, 243
turbo, 281
correlation, 42
curve
Hermitian, 61, 65, 110
irreducible, 61
Non-Binary Error Control Coding for Wireless Communication and Data Storage
Rolando Antonio Carrasco and Martin Johnston
C 2008 John Wiley & Sons, Ltd

302
INDEX
de-permutation, 223
decoder
hard-decision list, 148, 171
iterative, 208, 284
soft-decision list, 164, 192
turbo, 284
degree
curve, 64
divisor, 66
delay spread, 33
designed minimum distance, 84, 70,
114
differentiator, 44
distance
BCH bound, 84
Euclidean, 97, 99, 244
Free, 99, 244
Hamming, 74, 100, 241
divisor, 66
Doppler shift, 26
encoder
block, 75
BCM, 97
convolutional, 238
multilevel, 97, 98
systematic, 78, 82, 115, 242
TCM, 243
entropy, 5
erasure, 13
error
correction, 74, 76
detection, 75, 76
factor graph, 202, 224
factorisation, 159, 187
fast Fourier transform (FFT), 216, 224
Ô¨Ånite Ô¨Åelds
conjugate element, 59
construction, 54
cyclotomic cosets, 59, 85
extension, 55
minimal polynomial, 59
primitive element, 55
primitive polynomial, 56
subÔ¨Åelds, 59, 85
Forney‚Äôs formula, 95
Gauss-Jordan elimination, 116, 117
genetic algorithm, 253
genus, 67
group, 51
Hadamard transform, 217
horizontal step, 210
hyperplane at inÔ¨Ånity, 60
information
a priori, 284
average, 5
extrinsic, 284, 287
mutual, 9
iterative polynomial construction, 152
interleaving, 171, 199, 282
inter-symbol interference (ISI), 33, 44
interpolation theorem, 150, 181
inverse discrete Fourier transform (IDFT),
126
inverse fast Fourier transform (inverse FFT),
217
Justesen‚Äôs construction of AG codes, 113
key equation, 89
linear dependence/independence, 207
log likelihood ratio (LLR), 285
longitudinal magnetic recording, 44
Lorentzian pulse, 44
majority voting, 125
mapping
gray, 250
Carrasco and Farrell, 252
PSK, 247
QAM, 251
maximum distance separable (MDS), 88
maximum likelihood
decoding, 48, 265
sequence detection, 269
message length, 74, 70, 85, 207
matrix
circulant, 204
generator, 77, 112, 206
Hadamard, 217
incidence, 204

INDEX
303
multiplicity, 165, 193
parity check, 77, 88, 102, 202,
207
quasi-cyclic, 203, 219
reliability, 164
syndrome, 120
systematic, 116
monomial, 64, 68, 110, 118, 148
multi-dimensional LLR, 296
multiple-input-multiple-output (MIMO), 40,
263
multiplicity, 150
nearest neighbours, 251
noise
additive white Gaussian, 17
electronics, 44
non-binary
BCH code, 85
LDPC code, 218
turbo code, 294
order
leading, 149
lexicographic, 148
pole, 65, 110, 172
total graduated, 122
zero, 68, 173
parallel concatenation, 282
parity check equations, 74, 202
partial response equalisation, 47
partial response maximum likelihood
(PRML) detection, 45
permutation, 223
point at inÔ¨Ånity, 60, 110
polynomial
codeword, 80
error locating, 89, 120
error magnitude, 89
feed-back, 249
feed-forward, 249
generator, 80, 85, 87
interpolation, 151, 172
primitive, 56
remainder, 89
ring, 53
syndrome, 89
target, 46
power spectral density, 34
product criterion, 273
projective
curve, 60
points, 60
space, 60
puncturing, 282
rank criterion, 273
rational functions, 64, 110
recursive coefÔ¨Åcient search, 160
reliability, 285
Riemann-Roch theorem, 67
rings, 52
scattering, 28
set partitioning, 99, 244
signal Ô¨Çow graph, 240, 260
signal-to-noise (SNR) ratio, 18
Singleton bound, 88
space-time coding
design criteria for fast fading, 274
design criteria for slow fading, 273
pairwise error probability, 269
ring-TCM (ST-RTCM), 262
span, 122
state
diagram, 240
table, 238, 245
syndromes, 79, 89, 120
Tanner graph, 202
tapped delay line, 31
target response, 46
transfer function, 240, 260
tree diagram, 239
trellis
block code, 100
convolutional code, 241
urban environment (see indoor, pedestrian,
vehicular channels)
vertical step, 211
Walsh-Hadamard (see Hadamard matrix)


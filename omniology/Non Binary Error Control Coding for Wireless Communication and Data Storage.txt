
NON-BINARY ERROR
CONTROL CODING
FOR WIRELESS
COMMUNICATION
AND DATA STORAGE
Rolando Antonio Carrasco
Newcastle University, UK
Martin Johnston
Newcastle University, UK

NON-BINARY ERROR
CONTROL CODING
FOR WIRELESS
COMMUNICATION
AND DATA STORAGE


NON-BINARY ERROR
CONTROL CODING
FOR WIRELESS
COMMUNICATION
AND DATA STORAGE
Rolando Antonio Carrasco
Newcastle University, UK
Martin Johnston
Newcastle University, UK

This edition ﬁrst published 2008.
C 2008 John Wiley & Sons, Ltd.
Registered ofﬁce
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, United Kingdom
For details of our global editorial ofﬁces, for customer services and for information about how to apply for
permission to reuse the copyright material in this book please see our web site at www.wiley.com.
The right of the author to be identiﬁed as the author of this work has been asserted in accordance with the
Copyright, Designs and Patents Act 1988.
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in
any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the
UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be
available in electronic books.
Designations used by companies to distinguish their products are often claimed as trademarks. All brand names and
product names used in this book are trade names, service marks, trademarks or registered trademarks of their
respective owners. The publisher is not associated with any product or vendor mentioned in this book. This
publication is designed to provide accurate and authoritative information in regard to the subject matter covered. It
is sold on the understanding that the publisher is not engaged in rendering professional services. If professional
advice or other expert assistance is required, the services of a competent professional should be sought.
Library of Congress Cataloging-in-Publication Data
Carrasco, Rolando Antonio.
Non-binary error control coding for wireless communication and data storage / Rolando Antonio Carrasco,
Martin Johnston.
p. cm.
Includes bibliographical references and index.
ISBN 978-0-470-51819-9 (cloth)
1. Error-correcting codes (Information theory)
2. Wireless communicaton systems.
3. Data transmission
systems.
4. Computer storage devices.
5. Analog electronic systems.
I. Johnston, Martin, 1977–
II. Title.
TK5102.96.C37 2008
005.7 2–dc22
2008029104
A catalogue record for this book is available from the British Library.
ISBN 978-0-470-51819-9 (HB)
Typeset in 11/13pt Times by Aptara Inc., New Delhi, India.
Printed in Singapore by Markono Print Media Pte Ltd.

My wife Gladys, my daughters Gladys and Carolina, my grand children
Charles, Serena and Sophie, family and friends
Rolando Carrasco
My parents, grand parents, family and friends
Martin Johnston


Contents
Biographies
xiii
Preface
xv
Acknowledgements
xvii
1
Information, Channel Capacity and Channel Modelling
1
1.1 Introduction
1
1.1.1 Information Theory [1]
2
1.1.2 Deﬁnition of Information [1, 2]
2
1.2 Measure of Information [1–3]
2
1.2.1 Average Information
5
1.2.2 The Entropy of a Binary Source
6
1.2.3 Mutual Information
9
1.3 Channel Capacity
11
1.3.1 Binary Symmetric Input Channel
12
1.3.2 Binary Erasure Channel (BEC)
13
1.3.3 The 4-ary Symmetric Channel [1, 5]
15
1.3.4 Binary Input Capacity of a Continuous Channel (Channel Capacity)
17
1.3.5 Channel Capacity in Fading Environments
20
1.4 Channel Modelling
22
1.4.1 Classiﬁcation of Models
22
1.5 Deﬁnition of a Communications Channel and its Parameters
24
1.5.1 Doppler Shift
26
1.5.2 Scattering
28
1.5.3 Angle of Arrival and Spectra
30
1.5.4 Multipath Channel and Tapped Delay Line Model
31
1.5.5 Delay Spread
33
1.5.6 The Fixed Broadband Wireless Access Channel
34
1.5.7 UMTS Mobile Wireless Channel Model
35
1.5.8 Simulating the Fixed Broadband Wireless Access
Channel Model
37
1.5.9 Simulating the UMTS Mobile Radio Channel
38
1.6 (Multiple-Input–Multiple-Output) (MIMO) Channel
40

viii
CONTENTS
1.7 Magnetic Storage Channel Modelling
43
1.7.1 Longitudinal Recording
44
1.7.2 (Partial Response Maximum Likelihood) (PRML) Detection
45
1.8 Conclusions
48
References
48
2
Basic Principles of Non-Binary Codes
51
2.1 Introduction to Algebraic Concepts
51
2.1.1 Groups
51
2.1.2 Rings
52
2.1.3 Ring of Polynomials
53
2.1.4 Fields
54
2.1.5 Primitive Polynomials
56
2.1.6 Minimal Polynomials and Cyclotomic Cosets
58
2.1.7 Subﬁelds
59
2.2 Algebraic Geometry Principles
60
2.2.1 Projective and Afﬁne Space
60
2.2.2 Projective and Afﬁne Curves
60
2.2.3 Finding Points on an Afﬁne Curve
61
2.2.4 Rational Functions on Curves
64
2.2.5 Riemann–Roch Theorem
67
2.2.6 The Zero Order of a Monomial
68
2.2.7 AG Code Parameters
70
2.3 Conclusions
71
References
71
3
Non-Binary Block Codes
73
3.1 Introduction
73
3.2 Fundamentals of Block Codes
74
3.2.1 Generator and Parity Check Matrices
77
3.2.2 Decoding Block Codes
79
3.3 Cyclic Codes
80
3.3.1 Polynomials
80
3.3.2 Systematic Cyclic Codes
82
3.4 Bose–Chaudhuri–Hocquenghem (BCH) Codes
84
3.5 Reed–Solomon Codes
86
3.6 Decoding Reed–Solomon Codes
88
3.6.1 Euclid’s Algorithm
89
3.6.2 Berlekamp–Massey’s Algorithm
93
3.6.3 Determining the Error Magnitudes Using Forney’s Formula
95
3.7 Coded Modulation
96
3.7.1 Block Coded Modulation (BCM) Codes
96
3.7.2 Multi-Level Block Coding
97

CONTENTS
ix
3.7.3 Set Partitioning
99
3.7.4 Construction of a Block Code Trellis
100
3.7.5 Non-Binary BCM Codes
103
3.8 Conclusions
106
References
106
4
Algebraic–Geometric Codes
109
4.1 Introduction
109
4.2 Construction of Algebraic–Geometric Codes
110
4.2.1 Simpliﬁed Construction of AG Codes
113
4.2.2 Comparison of AG Codes with Reed–Solomon Codes
115
4.2.3 Systematic Algebraic–Geometric Codes
115
4.3 Decoding Algebraic–Geometric Codes
119
4.3.1 Modiﬁcation of the Sets F and G
123
4.4 Majority Voting
125
4.5 Calculating the Error Magnitudes
126
4.6 Complete Hard-Decision Decoding Algorithm for
Hermitian Codes
128
4.7 Simulation Results
140
4.8 Conclusions
144
References
144
5
List Decoding
147
5.1 Introduction
147
5.2 List Decoding of Reed–Solomon Codes Using the
Guruswami–Sudan Algorithm
148
5.2.1 Weighted Degrees and Lexicographic Orders
148
5.2.2 Interpolation Theorem
150
5.2.3 Iterative Polynomial Construction
152
5.2.4 Complexity Reduced Modiﬁcation
154
5.2.5 Factorization
159
5.2.6 Recursive Coefﬁcient Search
160
5.3 Soft-Decision List Decoding of Reed–Solomon Codes Using
the K¨otter–Vardy Algorithm
164
5.3.1 Mapping Reliability Values into Multiplicity Values
164
5.3.2 Solution Analysis for Soft-Decision
List Decoding
168
5.3.3 Simulation Results
171
5.4 List Decoding of Algebraic–Geometric Codes
171
5.5 Determining the Corresponding Coefﬁcients
178
5.6 Complexity Reduction Interpolation
181
5.7 General Factorization
187

x
CONTENTS
5.8 Soft-Decision List Decoding of Hermitian Codes
192
5.8.1 System Solution
193
5.8.2 Simulation Results
199
5.9 Conclusions
199
References
200
6
Non-Binary Low-Density Parity Check Codes
201
6.1 Introduction
201
6.2 Construction of Binary LDPC Codes – Random and Structured
Methods
201
6.2.1 Tanner Graph Representation
202
6.2.2 Structured Construction Methods
203
6.2.3 LDPC Codes from Class 1 Bose BIBD
204
6.2.4 Constructing the Generator Matrix of a Binary LDPC Code
206
6.3 Decoding of Binary LDPC Codes Using the Belief Propagation
Algorithm
208
6.3.1 Initialization of the Belief Propagation Algorithm
208
6.3.2 The Horizontal Step
210
6.3.3 The Vertical Step
211
6.3.4 Reducing the Decoding Complexity Using Fast Fourier
Transforms
216
6.4 Construction of Non-Binary LDPC Codes Deﬁned Over
Finite Fields
218
6.4.1 Construction of Non-Binary LDPC Codes from
Reed–Solomon Codes
219
6.5 Decoding Non-Binary LDPC Codes with the Sum–Product
Algorithm
221
6.5.1 Received Symbol Likelihoods
222
6.5.2 Permutation of Likelihoods
223
6.5.3 Factor Graph of a Non-Binary LDPC Code
224
6.5.4 The Fast Fourier Transform for the Decoding of Non-Binary
LDPC Codes
224
6.6 Conclusions
234
References
235
7
Non-Binary Convolutional Codes
237
7.1 Introduction
237
7.1.1 The Viterbi Algorithm
243
7.1.2 Trellis Coded Modulation
243
7.1.3 TCM Based on Rings of Integers
246
7.1.4 Ring-TCM Codes for M-PSK
247
7.1.5 Ring-TCM Codes Using Quadrature Amplitude Modulation
250
7.1.6 Searching for Good Ring-TCM Codes
251

CONTENTS
xi
7.1.7
Genetic Algorithm
253
7.1.8
Performance of Ring-TCM Codes on Urban Fading Channels
261
7.2 Space-Time Coding Modulation
262
7.2.1
Introduction
262
7.2.2
Space-Time Codes Model
264
7.2.3
Performance of ST-RTCM Codes Using QPSK for MIMO
Urban Environments
266
7.2.4
Ideal CSI and Slow Fading
267
7.2.5
Log-Likelihood Function
268
7.2.6
Maximum-Likelihood Sequence Detection
269
7.2.7
ST Codes Pairwise Error Probability
269
7.2.8
ST Code Design Criteria for Slow Fading Channels
273
7.2.9
ST Code Design Criteria for Fast Fading Channels
274
7.2.10 Space-Time Ring-TCM Codes for QAM in Fading Channels
274
7.3 Conclusions
278
References
278
8
Non-Binary Turbo Codes
281
8.1 Introduction
281
8.2 The Turbo Encoder
281
8.2.1
Notation
282
8.3 The Turbo Decoder
284
8.3.1
Log-Likelihood Ratios (LLRs)
285
8.3.2
Maximum A Posteriori (MAP) Decoding
287
8.3.3
Max-Log MAP
290
8.3.4
Log MAP Algorithm
292
8.3.5
Iterative Log MAP Decoding
292
8.4 Non-Binary Turbo Codes
294
8.4.1
Multi-Dimensional Log-Likelihood Ratios
296
8.4.2
Non-Binary Iterative Turbo Decoding
297
8.5 Conclusions
298
References
299
Index
301


Biographies
Rolando A. Carrasco was born in 1945 in Santiago, Chile. He obtained his Bachelors
BEng(Hons) degree in Electrical Engineering from Santiago University, Chile in
1969 and his PhD in Signal Processing from the University of Newcastle Upon
Tyne in 1980. He was awarded the IEE Heaviside Premium in 1982 for his work
in multiprocessor systems. Between 1982 and 1984 he was employed by Alfred
Peters Limited, Shefﬁeld (now Meditech) and carried out research and development
in signal processing associated with cochlear stimulation and response. He was with
Staffordshire University from 1984, and joined Newcastle University in 2004 as
Professor of Mobile Communications. His principle research interests are the digital
signal processing algorithm for data communication systems, mobile and network
communication systems, speech recognition and processing. Professor Carrasco has
over 200 scientiﬁc publications, ﬁve chapters in telecommunications reference texts
and a patent to his name. He has supervised 40 successful PhD students. He has
acted as principle investigator of several EPSRC projects, a BT research project and
Teaching Company schemes. He has been the local chairman on several international
conference organizing committees. He is a member of several organizing committees,
a member of the EPSRC College and a member of the EPSRC assessment panel. He
also has international collaborations with Chilean, Spanish and Chinese universities.
Professor Carrasco is a fellow of the Institution of Engineers and Technology (IET).
Martin Johnston was born in Wordsley, England in 1977. He received his BSc(Hons)
degree in Physics with Electronics from the University of Birmingham in 1999, his
MSc degree in Electronic Engineering from Staffordshire University in 2001 and was
awarded his PhD in the Design and Construction of Algebraic–Geometric Codes from
Newcastle University in 2006. His research interests include the design of non-binary
error-correcting codes for wireless and data storage applications and low-complexity
decoding algorithms. He is currently a Research Associate at Newcastle University
investigating the design of new error-correction coding schemes for high-density
magnetic storage channels. Dr Johnston is a member of the Institute of Engineering
and Technology (IET).


Preface
With the increasing importance of digital communications and data storage, there is a
need for research in the area of coding theory and channel modelling to design codes
for channels that are power limited and/or bandwidth limited. Typical examples of
such channels are found in cellular communications (GSM, UMTS), ﬁxed and mobile
broadband wireless access (WiMax) and magnetic storage media.
Our motivation to write this book is twofold. Firstly, it is intended to provide PhD
students and researchers in engineering with a sound background in binary and non-
binary error-correcting codes, covering different classes of block and convolutional
codes, band-width efﬁcient coded modulation techniques and spatial-temporal diver-
sity. Secondly, it is also intended to be suitable as a reference text for postgraduate
students enrolled on Master’s-level degree courses and projects in channel coding.
Chapter 1 of this book introduces the fundamentals of information theory and con-
cepts, followed by an explanation of the properties of fading channels and descriptions
of different channel models for ﬁxed wireless access, mobile communications systems
and magnetic storage.
In Chapter 2, basic mathematical concepts are presented in order to explain non-
binary error-correction coding techniques, such as Groups, Rings and Fields and their
properties, in order to understand the construction of ring trellis coded modulation
(ring-TCM), ring block coded modulation (ring-BCM), Reed–Solomon codes and
algebraic–geometric codes.
Binary and non-binary block codes and their application to wireless communica-
tions and data storage are discussed in Chapter 3, covering the construction of binary
and non-binary Bose–Chaudhuri–Hocquengem (BCH) codes, Reed–Solomon codes
and binary and non-binary BCM codes.
Chapter 4 introduces the construction methods of algebraic–geometric (AG) codes,
which require an understanding of algebraic geometry. The coding parameters of AG
codes are compared with Reed–Solomon codes and their performance and complexity
are evaluated. Simulation results showing the performance of AG and Reed–Solomon
codes are presented on fading channels and on magnetic storage channels.
Chapter 5 presents an alternative decoding algorithm known as list decoding, which
is applied to Reed–Solomon codes and AG codes. Hard-decision list decoding for
these codes is introduced ﬁrst, using the Guruswami–Sudan algorithm. This is then
followed by soft-decision list decoding, explaining the K¨otter-Vardy algorithm for

xvi
PREFACE
Reed–Solomon codes and modifying it for AG codes. The performance and complex-
ity of the list decoding algorithms for both Reed–Solomon codes and AG codes are
evaluated. Simulation results for hard- and soft-decision list decoding of these codes
on AWGN and fading channels are presented and it is shown that coding gains over
conventional hard-decision decoding can be achieved.
A more recent coding scheme known as the low-density parity check (LDPC)
code is introduced in Chapter 6. This is an important class of block code capable
of near-Shannon limit performance, constructed from a sparse parity check matrix.
We begin by explaining the construction and decoding of binary LDPC codes and
extend these principles to non-binary LDPC codes. Finally, the reduction of the
decoding complexity of non-binary LDPC codes using fast Fourier transforms (FFTs)
is explained in detail, with examples.
Chapter 7 begins with an explanation of convolutional codes and shows how they
can be combined with digital modulation to create a class of bandwidth-efﬁcient codes
called TCM codes. The construction of TCM codes deﬁned over rings of integers,
known as ring-TCM codes, is explained, and the design of good ring-TCM codes
using a Genetic algorithm is presented. It is then shown how ring-TCM codes can
be combined with spatial-temporal diversity, resulting in space-time ring-TCM (ST-
RTCM) codes, and design criteria are given to construct good ST-RTCM codes for
multiple-input–multiple-output (MIMO) fading channels. Simulation results of ST-
RTCM codes are presented for MIMO fading channels, including urban environments
such as indoor, pedestrian and vehicular scenarios.
Finally, Chapter 8 presents another recent coding scheme known as the turbo code.
This important class of code was the ﬁrst to achieve near-Shannon limit performance
by using a novel iterative decoding algorithm. Binary turbo encoding and decoding
are explained in this chapter, with a detailed description of the maximum a posteriori
(MAP) algorithm and simpler decoding algorithms derived from the MAP algorithm,
such as the max-log MAP and log MAP algorithms. The chapter ﬁnishes by introduc-
ing non-binary turbo codes, which are a new area of research that have received very
little attention in the literature. Non-binary turbo encoding is explained and the exten-
sion of the binary turbo decoder structure in order to decode non-binary turbo codes
is shown. For further information visit http://www.wiley.com/go/carrasco non-binary

Acknowledgements
We are deeply indebted to the many reviewers who have given their time to read
through this book, in part or in full. We are also very grateful for all the discussions,
support and encouragement we have had from Prof. Paddy Farrell during the writing
of this book. Our thanks go to our colleagues, to the many research assistants and
students over the years, to our families and friends, to Bahram Honary, Mario Blaum,
Mike Darnell, Garik Markarian, Ian Wassell and Fary Ghassemlooy for the useful
discussions at the conferences we participated in. We are grateful for the contributions
of Javier Lopez, Ismael Soto, Marcella Petronio, Alvero Pereira, Li Chen and Vajira
Ganepola to some of the chapters in the book. Finally, we would like to thank Sarah
Hinton and Sarah Tilley at John Wiley and Sons for their guidance in the initial
preparation and the submission of the ﬁnal version of the book.


1
Information, Channel Capacity
and Channel Modelling
1.1 Introduction
In this chapter, an introduction to the fundamental aspects of information theory is
given, with particular attention given to the derivation of the capacity of different
channel models. This is followed by an explanation of the physical properties of
fading channels and descriptions of different channel models for ﬁxed wireless access,
universal mobile telecommunication systems (UMTS) for single-input–single-output
(SISO) and multiple-input–multiple-output (MIMO), and ﬁnally magnetic recording
channels. Therefore, this chapter presents the prerequisites for evaluating many binary
and non-binary coding schemes on various channel models.
The purpose of a communication system is, in the broadest sense, the transmission
of information from one point in space and time to another. We shall brieﬂy explore
the basic ideas of what information is and how it can be measured, and how these
ideas relate to bandwidth, capacity, signal-to-noise ratio, bit error rate and so on.
First, we address three basic questions that arise from the analysis and design of
communication systems:
 Given an information source, how do we determine the ‘rate’ at which the source is
transmitting information?
 For a noisy communication channel, how do we determine the maximum ‘rate’ at
which ‘reliable’ information transmission can take place over the channel?
 How will we develop statistical models that adequately represent the basic properties
of communication channels?
For modelling purposes we will divide communication channels into two cate-
gories: analogue channels and discrete channels. We wish to construct a function that
Non-Binary Error Control Coding for Wireless Communication and Data Storage
Rolando Antonio Carrasco and Martin Johnston
C 2008 John Wiley & Sons, Ltd

2
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
measures the amount of information present in an event that occurs with probability p.
A lower probability means that we obtain more information about the event, whereas
if the event is entirely predictable ( p = 1) then we obtain no information about the
event since we already have knowledge of the event before we receive it.
1.1.1 Information Theory [1]
 How can the symbols of communication be transmitted? Technical or Syntactic
level?
 How precisely do the transmitted symbols carry the desired meaning? Semantic
problem.
 How effectively does the received meaning affect conduct in the desired manner?
1.1.2 Deﬁnition of Information [1, 2]
Information: Knowledge not precisely known by the recipient, or a measure of unex-
pectedness.
Given an information source, we evaluate the rate at which the source is emitting
information as:
Rate = symbols
second
OR
Rate =
bits
second.
However, given a noisy communication channel, how do we determine the maxi-
mum rate at which reliable information transmission can take place over the channel?
1.2 Measure of Information [1–3]
We should brieﬂy explore basic ideas about what information is, how it can be mea-
sured, and how these ideas relate to bandwidth and signal-to-noise ratio. The amount
of information about an event is closely related to its probability of occurrence. Mes-
sages containing knowledge of a high probability of occurrence (i.e. those indicating
very little uncertainty in the outcome) convey relatively little information. In contrast,
those messages containing knowledge of a low probability of occurrence convey rel-
atively large amounts of information. Thus, a measure of the information received
from the knowledge of occurrence of an event is inversely related to the probability
of its occurrence.
Assume an information source transmits one of nine possible messages M1, M2,
. . ., M9 with probability of occurrence P1, P2, . . ., P9, where P1 + P2 + · · · + P9 = 1,
as shown in Figure 1.1.
According to our intuition, the information content or the amount of information
in the ith message, denoted by I(Mi), must be inversely related to Pi. Also, to satisfy

MEASURE OF INFORMATION [1–3]
3
M1
P1
M2
P2
M3
P3
M4
P4
Mq
Pq
Message
Probability
Figure 1.1
Messages and their associated probabilities.
our intuitive concept of information, I(Mi) must satisfy:
I(Mi) > I(M j) if Pi < Pj
I(Mi) →0, Pi →1
I(Mi) ≥0 when 0 ≤Pi ≤1.
We can explain the concept of independent messages transmitting from the same
source. For example, the received message ‘It will be cold today and hot tomorrow’
is the same as the sum of information received in the two messages ‘It will be cold
today’ and ‘It will be hot tomorrow’ (assuming that the weather today does not affect
the weather tomorrow).
Mathematically, we can write this as:
I(Mi and M j)
= I(Mi, M j) = I(Mi) + I(M j),
where Mi and Mj are the two independent messages. We can deﬁne a measure of
information as the logarithmic function:
I(Mi) = logx
 1
Pi

,
where x is the base 2, e, 10, . . .
(1.1)
The base x for the logarithm in (1.1) determines the unit assigned to the information
content:
x = e
nats
x = 2
bits
x = 10
Hartley,
P1 + P2 + P3 + · · · + Pq = 1 where Pq is the probability of the message occurring
and q is the index value, that is 1, 2 . . .
The information content or the amount of information I(Mk) in the kth message
with the set kth probability (Pk) boundary values is:
1. I(Mk) →0
as
Pk →1.
Obviously, if we are absolutely certain of the outcome of an event, even before it
occurred.

4
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
2. I(Mk) ≥0
when
0 ≤Pk ≤1.
That is to say, the occurrence of an event M = Mk either provides some or no
information content.
3. I(Mk)>I(Mi)
if
Pk<Pi.
The less probable an event is, the more information we gain when it
occurs.
4. I(Mk Mi) = I(Mk) + I(Mi).
If I(Mk) and I(Mi) are statistically independent messages.
5. Mathematically we can prove that:
6. I(Mk and M j) = I(Mk) + I(M j),where Mk and Mj are two statistically indepen-
dent messages. The logarithmic measure is convenient.
Example 1.1: A source puts out ﬁve possible messages. The probabilities of these
messages are:
P1 = 1
2
P2 = 1
4
P3 = 1
8
P4 = 1
16
P5 = 1
16.
Determine the information contained in each of these messages.
Solution:
I(M1) = log2


1
1
2


= 1 bit
I(M2) = log2


1
1
4


= 2 bits
I(M3) = log2


1
1
8


= 3 bits.
I(M4) = log2


1
1
16


= 4 bits
I(M5) = log2


1
1
16


= 4 bits
Information total = 14 bits.

MEASURE OF INFORMATION [1–3]
5
1.2.1 Average Information
Text messages produced by an information source consist of sequences of symbols
but the receiver of a message may interpret the entire message as a single unit. When
we attempt to deﬁne the information content of symbols, we are required to deﬁne
the average information content of symbols in a long message, and the statistical
dependence of symbols in a message sequence will change the average information
content of symbols.
Let a transmitter unit consist of U possible symbols, s1, s2, . . . , su, in a statistically
independent sequence. The possibility of occurrence of a particular symbol during a
symbol time does not depend on the symbol transmitted by the source previous in
time. Let P1, P2, . . . , Pu be the probability of occurrence of the U symbols, in a long
message having N symbols. The symbol s1 will occur on average P1N times. The
symbol s2 will occur P2N times, and the symbol si will occur PiN times. Assuming
an individual symbol s to be a message of length 1, we can deﬁne the information
content of the ith symbol as
log2( 1
Pi ) bits. The PiN occurrence of si contributes
an information content of Pi N log2( 1
Pi ) bits.
The total information content of message is then the sum of each of the U symbols
of the source:
Itotal =
U
 
i=1
NPi log2
 1
Pi

bits.
(1.2)
The average information per symbol is measured by dividing the total information
by the number of symbols N in the message:
H = Itotal
N
=
U
 
i=1
Pi log2
 1
Pi

bits/symbol.
(1.3)
The average information H is called the source entropy (bits/symbol).
Example 1.2: Determine the entropy of a source that emits one of three symbols,
A, B, C, in a statistically independent sequence, with a probability of 1
2, 1
4 and 1
4,
respectively.
Solution: The information contents of the symbols are:
one bit for A
two bit for B
two bit for C
H = 1
2 log2


1
1
2


+ 1
4 log2


1
1
4


+ 1
4 log2


1
1
4


.

6
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
So the average information content per symbol on the source entropy is:
H = 1
2 log2(2) + 1
4 log2(4) + 1
4 log2(4).
H = 1.5 bits/symbols
If we have a ﬁxed time, say rs symbols/s, then by deﬁnition the average source
of information rate R in bits per second is the product of the average information
content per symbol and the symbol rate rs, as shown below:
R = rs · H bits/s.
1.2.2 The Entropy of a Binary Source
For ‘unbiased’ results, the probability of logic 0 or logic 1 is:
P(1) = P(0) = 0.5
Entropy = H = −
M
 
i=1
Pi log2 Pi
H = −P1 log2 (P1) −P2 log2 (P2)
H = −P1 log2
1
2

−P2 log2
1
2

H = 0.5 + 0.5 = 1.
This agrees with expectation. Now consider a system P1 = P2. We note that, for a
two-state system, since P1 + P2 = 1 we have P2 = 1 −P1. We can then write:
H = P1 log2
 1
P1

+ (1 −P1) log2

1
1 −P1

bits.
(1.4)
Table 1.1 shows how the entropy H varies for different values of P1 and P2.
Table 1.1
The variation of entropy for different values of P1 and P2.
P1
P2 = 1 −P1
P1 log2

1
P1

(1 −P1) log2

1
1−P1

H
0
1
0
0
0
0.2
0.8
0.46
0.25
0.72
0.4
0.6
0.528
0.44
0.97
0.5
0.5
0.5
0.5
1
0.6
0.4
0.44
0.528
0.97
0.8
0.2
0.25
0.46
0.72
1
0
0
0
0

MEASURE OF INFORMATION [1–3]
7
H(x), bits
1
0
1
0.5
Prob p
0.5
Figure 1.2
Entropy function of a binary source.
From Table 1.1 the following observations can be made:
The entropy H(x) attains its maximum value, Hmax = 1 bit, when P1 = P2 = 1/2;
that is, symbols 1 and 0 are equally probable.
When P1 = 0 or 1, the entropy value of H(x) = 0, resulting in no information.
The entropy is plotted in Figure 1.2.
The next two examples show how to determine the entropy functions for an event
with a uniform and Gaussian probability distribution.
Example 1.3:
Determine the entropy of an event x with a uniform probability
distribution deﬁned by:
P(x) =
1
2.X0
, −X0 < x < X0
P(x) = 0,
elsewhere
The uniform distribution is illustrated in Figure 1.3.
P(x)
1/2X0
0
–X0
X0
x
Figure 1.3
A uniform distribution.
Determine the entropy sources [1, 3]:
H(x) = −
 
x
P(x) log P(x) dx

8
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
H(x) =
X0
 
−X0
1
2X0
log (2X0) dx
H(x) = log (2X0)
2X0
X0
 
−X0
dx
.
H(x) = log (2X0)
2X0
(X0 −(−X0))
H(x) = log (2X0)
2X0
(2X0) = log (2X0)
Example 1.4:
Determine the entropy of an event x with a Gaussian probability
distribution deﬁned by:
P(x) =
1
√
2πσ
e−x2
2σ2
where σ is the standard deviation.
The distribution is normalized so that the area under the pdf is unity
∞
 
−∞
P(x) dx = 1.
The variance σ 2 of the distribution is given as:
∞
 
−∞
x2P(x) dx = σ 2.
Therefore, the entropy function is:
H(x) =
∞
 
−∞
1
σ
√
2π
e−x2
2σ2
 x2
2σ 2 + ln σ
√
2π

dx
H(x) = 1
2 + ln

σ
√
2π

.
H(x) = ln e
1
2 + ln

σ
√
2π

H(x) = ln

σ
√
2πe

nats or H(x) = log2

σ
√
2πe

bits

MEASURE OF INFORMATION [1–3]
9
1.2.3 Mutual Information
Given that we think of the channel output y as a noisy version of the channel input x
value, and the entropy H is a measure of the prior uncertainty about x, how can we
measure the uncertainty about x after observing the y value?
The conditional entropy of x is deﬁned as [4]:
H(X|Y = yk) =
 
i
P (xi|yk) log2

1
P (xi|yk)

,
k = 0, 1, 2, . . . , i = 0, 1, 2, . . .
(1.5)
This quantity is itself a random variable that takes on values:
H(X|y = y0), . . . , H(Y|y = yk−1)
with probabilities of:
P(y0), . . . , P(yk−1) respectively.
So the mean entropy is:
H =
 
k H (X|y = yk) P (yk)
H =
 
k
 
i P (xi|yk) P (yk) log2

1
P (xi|yk)

.
Or
H =
 
k
 
i P (xi, yk) log2

1
P (xi|yk)

(1.6)
This quantity H or H (X|Y) is called conditional entropy. It represents the amount
of uncertainty remaining about the input after the channel output has been observed.
Since the entropy H(X) represents our uncertainty about the channel input before
observing the channel output, it follows that the difference is:
H(X) −H(X|Y).
This must represent our uncertainty about the channel input, which is resolved by
observing the channel output. This important quantity is called the mutual information
of the channel. Denoting the mutual information by I (X, Y), we may thus write:
I(X, Y) = H(X) −H(X|Y).
(1.7)
Similarly:
I(Y, X) = H(Y) −H(Y|X),
(1.8)
where H(Y) is the entropy of the channel output and H(Y|X) is the conditional entropy
of the channel output given the channel input.

10
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
)
(x, y
I
)
(x
H
)
(y
H
)
(x, y
H
Figure 1.4
A Venn diagram illustrating the relationship between mutual information and entropy.
Property 1
The mutual information of a channel is symmetric, that is:
I(x, y) = I(y, x).
Property 2
The mutual information is always nonnegative, that is:
I(x, y) ≥0.
Property 3
The mutual information of a channel is related to the joint entropy of
the channel input and output by:
I(x, y) = H(x) + H(y) −H(x, y)
and is shown in Figure 1.4.
Example 1.5: Determine the mutual information of the binary symmetric channel
in Figure 1.5 where the associated probabilities are P(x = 1) = 0.7, P(x = 0) =
0.3, P(y = 1|x = 1) = 0.8, P(y = 1|x = 0) = 0.2, P(y = 0|x = 1) = 0.2 and
P(y = 0|x = 0) = 0.8.
Solution:
H(x) = P (x = 0) log2
1
P(x = 0) + P(x = 1) log2
1
P(x = 1)
H(x) = 0.3 × log2
1
0.3 + 0.7 × log2
1
0.7
.
H(x) = log2 10 −

0.3 × log2 3 + 0.7 × log2 7

H(x) = 0.881 bits
So, the conditional entropy of x:
H(x|y = 0) = 24
38 log2
38
24 + 14
38 log2
38
24

CHANNEL CAPACITY
11
H(x|y = 1) = 3
31 log2
31
2 + 28
31 log2
31
28
H(x|y) = 0.38 H(x|y = 0) + 0.62 H(x|y = 1)
= P(y = 0)H (x|y = 0) + P(y = 1)H(x|y = 1)
= 0.645.
Therefore the mutual information of the binary symmetric channel shown in
Figure 1.5 is:
I(x, y) = H(x) −H(X|y) = 0.881 −0.645 = 0.236 bits/symbol.
1.3 Channel Capacity
Successful electrical/optical communication systems depend on how accurately the
receiver can determine the transmitted signal. Perfect identiﬁcation could be possi-
ble in the absence of noise, but noise is always present in communication systems.
The presence of noise superimposed on signal limits the receiver ability to correctly
identify the intended signal and thereby limits the rate of information transmission.
The term ‘noise’ is used in electrical communication systems to refer to unwanted
electrical signals that accompany the message signals. These unwanted signals arise
from a variety of sources and can be classiﬁed as man-made or naturally occurring.
Man-made noise includes such things as electromagnetic pickup of other radiating
signals. Natural noise-producing phenomena include atmospheric disturbances, ex-
traterrestrial radiation and internal circuit noise.
Deﬁnition: The capacity C of the channel is the maximum mutual information, taken
over all input distribution of x. In symbols [3, 4]:
C = max
P(xi) I(x, y)
where
P(0) = P(1) = 1
2.
(1.9)
The units of the capacity C are bits per channel input symbols (bits/s).
1 
0.7 
0.8 
1
0
0
0.2 
0.2 
0.8 
Transmitter x
Receiver y
Figure 1.5
Binary symmetric channel of Example 1.5.

12
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
0
1 – p
1 – p
0
1
1
X
p
p
Y
Figure 1.6
A binary symmetric channel.
1.3.1 Binary Symmetric Input Channel
Figure 1.6 shows the binary symmetric channel (BSC).
If we assume a uniform input distribution P(x = 0) = P(x = 1) = 1/2 then H(x) = 1.
Furthermore, we have the set of transition probabilities [5]:
P(y = 0 | x = 0) = P(y = 1 | x = 1) = 1 −p
P(y = 0 | x = 1) = P(y = 1 | x = 0) = p.
The conditional entropy is given by:
H(x | y) = −
 
k
 
i
P(yi | xk)P(xk) log2 P(xk | yi)

H(x | y) = −
1
2(1 −p) log2 P (x = 0 | y = 0) + 1
2 p log2 P (x = 0 | y = 1)
+1
2 p log2 P (x = 1 | y = 0) + 1
2 (1 −p) log2 P (x = 1 | y = 1)

.
The distribution of y is determined as follows:
P(y = 0) = P(y = 0 | x = 0)P(x = 0) + P(y = 1 | x = 1)P(x = 1)
∴P(y = 0) = 1
2(1 −p) + 1
2 p = 1
2
,
and P(y = 1) is:
P(y = 1) = P(y = 1 | x = 0)P(x = 0) + P(y = 1 | x = 1)P(x = 1)
= 1
2
.
The result is not surprising since the channel is symmetric. The joint distribution is
determined as follows:
P(x = 0 | y = 0) = P(x = 0, y = 0)
P(y = 0)
= P(y = 0 | x = 0)p(x = 0)
P(y = 0)
P(x = 0 | y = 0) = P(y = 0 | x = 0) = 1 −p
.
P(x = 0 | y = 1) = P(x = 1 | y = 0) = p

CHANNEL CAPACITY
13
0
0.2
0.4
0.6
0.8
1
1.2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
p
I(x,y)
Figure 1.7
The mutual information of the BSC as a function of symbol error probability.
Thus:
H(x | y) = −
 1
2(1 −p) log2(1 −p) + 1
2 p log2 p + 1
2(1 −p) log2(1 −p)

= −p log2 p −(1 −p) log2(1 −p)
.
I(x, y) = 1 + p log2 p + (1 −p) log2(1 −p)
The mutual information as a function of the symbol error probability p is shown in
Figure 1.7.
1.3.2 Binary Erasure Channel (BEC)
The BEC output alphabets are 0 or 1, plus an additional element, denoted as e, called
the erasure. This channel corresponds to data loss. Each input bit is either transmitted
correctly with probability 1 −p or is erased with probability p. The BEC is shown in
Figure 1.8.
The channel probabilities are given by [1, 5]:
P(y = 0 | x = 0) = P(y = 1 | x = 1) = 1 −p
P(y = e | x = 0) = P(y = e | x = 1) = p.

14
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
0
1 – p
1 – p
0
1
1
p
p
e
Figure 1.8
The binary erasure channel.
The conditional entropy is given by:
H(x | y) = −
 
k
 
i
P(xk | yi)P(xk) log2 P(xk | yi)

H(x | y) = −
1
2(1 −p) log2 P (x = 0 | y = 0) + 1
2 p log2 P (x = 0 | y = e)
+1
2 p log2 P (x = 1 | y = 1) + 1
2 (1 −p) log2 P (x = 1 | y = e)

,
H(x | y) = −1
2(1 −p) log2 P (x = 0 | y = 0) −1
2 p log2 P (x = 0 | y = e)
where we used the fact that X is equiprobably distributed. The distribution of y is
determined as follows:
P(y = 0) = P(y = 0/x = 0)P(x = 0) = 1
2 (1 −p)
and similarly for P(y = e), we have
P(y = 0) = 1
2, P(y = e) = p
and:
P(x = 0 | y = 0) = P(x = 0, y = 0)
P(y = 0)
= P(y = 0 | x = 0)p(x = 0)
P(y = 0)
=
1
2(1 −p)
1
2(1 −p)
and similarly:
P(x = 0 | y = 0) = 1
P(x = 0 | y = e) = 1
2.
Thus,
H(x | y) = −(1 −p) log2 (1) −p log2
1
2

= p and
I(x, y) = H(x) −
H(x | y) = 1 −p.

CHANNEL CAPACITY
15
0
0.2
0.4
0.6
0.8
1
1.2
1.1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
p
I(x,y)
Figure 1.9
Mutual information of the binary erasure channel.
The mutual information of the binary erasure channel as a function of symbol error
probability is plotted in Figure 1.9.
1.3.3 The 4-ary Symmetric Channel [1, 5]
The 4-ary Symmetric Channel (SC) is shown in Figure 1.10.
Let us consider the case of the 4-SC, where both the input X and the output Y have
four possible values from the alphabet A = [α0 . . . α3]. Since we are sending 4-ary
symbols over the channel, we take the logarithm in equations [H(x), I(x, y)] to the
3p
1 –
p
p
p
p
p
p
p
p
p
p
p p
p
p
p
p
p
p
p
p
p
p
p p
3p
1 –
3p
1 –
3p
1 –
3p
1 –
3p
1 –
3p
1 –
3p
1 –
0
α
0
α
1
α
2
α
3
α
1
α
2
α
3
α
X
Y
Figure 1.10
The 4-ary symmetric channel.

16
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
base 4. The same holds for the capacity deﬁned previously. Assume a uniform input
distribution where P(x = a) = 1
4 for all a ∈A. Hence, H(x) = 1, and we can set the
transition probabilities:
P(y = a | x = a) = 1 −p, P(y = a | x = b) = p
3 for a = b, where a, b ∈A.
The conditional entropy is given by:
H(x | y) = −
 
k
 
i
P(yi | xk) P(xk) log4 P(xk | yi)

= −4
1
4 p log4 P (x = a | y = b) + 1
4 (1 −p) log4 P (x = a | y = a)

.
For a = b, where a, b ∈A, the distribution of y is determined as follows:
P(y = a) = 3P(y = a | x = b)P(x = b) + P(y = a | x = a)P(x = a)
= 3 × 1
4
p
3 + 1
4(1 −p) = 1
4.
The joint distribution is determined as follows:
P(x = a | y = a) = P(x = a, y = a)
P(y = a)
= P(y = a | x = a)P(x = a)
P(y = a)
= 1 −p.
Similarly:
P(x = a | y = b) = P(x = a, y = b)
P(y = b)
= P(y = b | x = a)P(x = a)
P(y = b)
= p
3 .
For all a, b ∈A, where a = b, we have:
H(x | y) = −4
1
4 p log4
p
3 + 1
4 (1 −p) log4(1 −p)

= −p log4
p
3 −(1 −p) log4(1 −p),
and:
I(x, y) = 1 + p log4
p
3 + (1 −p) log4(1 −p).
The mutual information as a function of the symbol error probability p is shown in
Figure 1.11.
Notice that when the capacity is 1, the 4-ary symbol per channel use for p = 0. The
mutual information is zero for p = 3
4.

CHANNEL CAPACITY
17
0
0.2
0.4
0.6
0.8
1
1.2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
p
I(x,y)
Figure 1.11
The mutual information of the 4-ary SC as function of symbol error probability.
1.3.4 Binary Input Capacity of a Continuous Channel (Channel Capacity)
Deﬁnition: The capacity C of a discrete memoryless channel is deﬁned as the maxi-
mum mutual information I (x, y) that can be transmitted through the channel:
I(X, Y) = H(Y) −H(Y | x) = H(x) −H(x | y)
= H(x) + H(y) −H(x, y).
The capacity is:
C = max I(x, y)
C = max
  
P(x, y) log2
P(x, y)
P(x)P(y)

.
If P(x) =
1
√
2π Se−x2
2S is a Gaussian distribution for the signal S then P(y) =
1
√2π(S+N)e
y2
2(S+N) is a Gaussian distribution for the signal S and the noise N.
We will make use of the expression for information I(x, y) = H(y) −H(y | x)
and hence we require to ﬁnd H(y | x). P(y | x) will be Gaussian distributed about a
particular value of x, as seen in Figure 1.12.
P(y | x) =
1
√
2π N
e
(y−x)2
2N .

18
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING



x
y
p
0
x
Figure 1.12
The Gaussian distribution of the conditional probability P(y | x).
It has been shown that the entropy is independent of the DC value and thus:
H(y) = log2
√
2πeN.
Since this is independent of x:
H(y) = log2

2πe(S + N)
I(x, y) = log2

2πe (S + N) −log2
√
2πeN
= log2

2πe (S + N)
2πeN
.
= log2

S + N
N
= log2

1 + S
N
This is the information per sample. Thus in time T information is:
Total information = 2BTI
= 2BT log2

1 + S
N = BT log2

1 + S
N

.
Maximum information rate is:
C = B log2

1 + S
N

bits/s,
where B is the bandwidth of the channel. The practical and non-practical channel
capacities of the AWGN channel are shown in Figure 1.13.
1. When the channel is noise free, permitting us to set p = 0, the channel capacity C
attains its maximum value of one bit per channel use.

CHANNEL CAPACITY
19
1.589
Figure 1.13
Channel capacity of the AWGN channel.
2. When the conditional probability of error p =1/2 due to noise, the channel capacity
C attains its minimum value of zero.
S = Average Power (Symbols)
N = Average Noise Power.
Example 1.6: Determine the capacity of a low-pass channel with usual bandwidth
of 3000 Hz and S/N = 10 dB (signal/noise) at the channel output. Assume the
channel noise to be Gaussian and white.
Solution:
C = B log2

1 + S
N

= 3000 log2 (1 + 10) ∼= 10 378 bits/s.
Signalling at rates close to capacity is achieved in practice by error correction
coding. An error correction code maps data sequences of k bits to code words of n

20
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
symbols. Because n > k, the code word contains structured redundancy. The code
rate, r = k/n, is a measure of the spectral efﬁciency of the code. In order to achieve
reliable communications, the code rate cannot exceed the channel capacity (r ≤c).
The minimum theoretical signal-to-noise ratio (S/N) required to achieve arbitrarily
reliable communications can be found by rearranging the equation for the capacity of
the AWGN channel [2].
S/N ≥1
2r

22r −1

.
This is the minimum S/N required for any arbitrary distinction for the input signal.
Shannon’s proof of the channel coding theorem [1, 2] used a random coding argu-
ment. Shannon showed that if one selects a rate r < c codes at random, the bit error
probability approaches zero as the block length n of the code approaches inﬁnity.
However, random codes are not practically feasible. In order to be able to encode and
decode with reasonable complexity, codes must possess some sort of a structure.
1.3.5 Channel Capacity in Fading Environments
In the case of a single input and single output (SISO) fading channel, the received
signal at the kth symbol instant is y(k) = h(k)x(k) + n(k), where h(k) is the impulse
response of the channel, x(k) is the input signal to the channel and n(k) is additive
white Gaussian noise. To ensure a compatible measure of power, set E(|h(k)|2) = 1
and E{|x(k)|2} ≤Es. The capacity in the case of the ﬁxed fading channel with random
but unchanging channel gain is given below [6]:
C = B log2

1 + |h|2 ρ

,
where ρ = Es
σ 2n
,
(1.10)
where Es is the symbol energy, σ 2
n is the n-dimensional variance and h is the gain
provided by the channel. An interesting point to note is that in a random but ﬁxed
fading channel the theoretical capacity can be zero, when the channel gain is close
enough to zero to make data rate impossible. In this case, the possible scenario is
determining what the chances are that a required capacity is available. This is deﬁned
by Outage probability Pout as the probability that the channel is above a threshold
capacity Cthres given by following equation [6]:
Pout = P(C > Cthres) = P

|h|2 > 2Cthres −1
ρ

(1.11)
Pout = 1 −e−2Cthres−1
ρ
,
(1.12)
where (1.13) is valid for Rayleigh fading.

CHANNEL CAPACITY
21
In the case of a time-varying channel, the channel is independent from one symbol
to the next and the average capacity of K data symbols is:
CK = 1
K
K
 
k=1

log2

1 + |h|2 ρ

.
(1.13)
Based on the law of large numbers, as K →∞the term on the right converges to
the average or expected value, therefore:
C = Eh

log2

1 + |h|2 ρ

,
(1.14)
where the expectation is taken over the channel values h. (1.14) is nonzero; therefore
with a ﬂuctuating channel it is possible to guarantee the existence of an error-free data
rate.
Increasing the transmitted and received antenna, the capacity of the channel in-
creases by NT NR-fold, where NT is number of transmit antenna and NR is num-
ber of receive antenna. The capacity of the AWGN channel in case of multiple-
input–multiple-output is approximately given by (1.15):
C ≈B log2 (1 + NT · NR · ρ)
where ρ = Es/σ 2
n .
(1.15)
Figure 1.14 shows the channel capacity with the increase in the number of trans-
mitted and received antennas plotted using (1.15).
0
2
4
6
8
10
12
14
0
5
10
15
20
25
30
Es/N0, dB
Capacity, bits/sec/Hertz
1 Tx, 1Rx
2 Tx, 2 Rx
3 Tx, 3 Rx
4 Tx, 4 Rx
5 Tx, 5 Rx
6 Tx, 6 Rx
7 Tx, 7 Rx
8 Tx, 8 Rx
9 Tx, 9 Rx
10 Tx, 10 Rx
Figure 1.14
Channel capacity in bits/s/Hz with increasing number of transmit and receive antennas.

22
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
1.4 Channel Modelling
This section begins by describing the fundamentals and characteristics of the channel
propagation models used in WiMAX or IEEE 802.16d (Fixed Broadband) systems
and UMTS cellular communications. Propagation models are the fundamental tools
for designing any wireless communication system. A propagation model basically
predicts what will happen to the transmitted signal while in transit to the receiver.
In general, the signal is weakened and distorted in particular ways and the receiver
must be able to accommodate for these changes if the transmitted information is to
be successfully received. The design of the transmitting and receiving equipment and
the type of communication service that is being provided will be affected by these
signal impairments and distortions. The role of propagation modelling is to predict
the performance of the system with these distortions and to determine whether it will
successfully meet its performance goals and service objectives.
The section ends with a description of magnetic storage channel modelling for lon-
gitudinal and perpendicular storage devices. In this situation, information is retrieved
not in a different location from where it was originally sent, as in radio communica-
tions, but at a different time from when it was originally stored.
1.4.1 Classiﬁcation of Models
In wireless channels, a narrowband implies that the channel under consideration is
sufﬁciently narrow that the fading across it is ﬂat (i.e. constant). It is usually used as an
idealizing assumption; no channel has perfectly ﬂat fading, but the analysis of many
aspects of wireless systems is greatly simpliﬁed if ﬂat fading can be assumed. Early
communication systems were narrowband systems in which median signal level pre-
diction and some description of the signal level variability (fading) statistics were the
only model parameters needed to adequately predict the system performance. How-
ever, modern communications systems use wideband, hence achieving higher data
rates.
In communications, wideband is a relative term used to describe a wide range
of frequencies in a spectrum. A system is typically described as wideband if its
bandwidth is much greater than its centre frequency or carrier frequency. For such
systems, narrowband prediction of signal levels and fading alone does not provide
enough information to predict system performance. In fact the concept of propagation
models is now enlarged to include the entire transfer function of the channel. These
new propagation models, known as channel models, now include parameters such as
signal time dispersion information and Doppler effect distortion arising from motion
of the mobile device. Time dispersion causes the signal fading to vary as a function
of frequency, so wideband channels are often known as frequency-selective fading
channels.
These propagation channel models are broadly classiﬁed into three main categories:
Theoretical, Empirical and Physical.

CHANNEL MODELLING
23
1.4.1.1 Theoretical Models
The channel models in this group are based on theoretical assumptions about the
propagation environment. The channel parameters based on these model assumptions
do not deﬁne any particular environment or model any speciﬁc channel conditions. The
theoretical models cannot be used for planning and developing any communication
systems as they do not reﬂect the exact propagation medium the signal would be
experiencing. The theoretical models can be nontime dispersive or time dispersive.
Nontime-dispersive channel models are those in which the duration of the transmitted
signal is the same on arriving at the receiving end. However, in time dispersion the
signal extends in time so that the duration of the received signal is greater than that of
the transmitted signal. The theoretical modelling of the time-dispersive channel has
been presented in [6–8]. The theoretical time-dispersive channel can also be modelled
by the tapped delay line structure, in which densely-spaced taps, multiplying constants
and tap-to-tap correlation coefﬁcients are determined on the basis of measurements
or some theoretical interpretation of how the propagation environment affects the
signal [7, 8].
1.4.1.2 Empirical Models
Empirical models are those based on observations and measurements alone. These
models are mainly used to predict the path loss, but models that predict rain fade
and multipath have also been proposed [7]. The problem can occur when trying to
use empirical models in locations that are broadly different from the environment in
which the data is measured. For example, the Hata Model [9] is based on the work of
Okumura, in which the propagation path loss is deﬁned for the urban, suburban and
open environments. But models like Hata and ITU-R are widely used because they
are simple and allow rapid computer calculations.
Empirical models can be subclassiﬁed in two categories, namely nontime dispersive
and time dispersive, as described. The time dispersive provides information relating
to the time-dispersive characteristics of the channel, that is the multipath delay spread
of the channel. Examples of this type are channel models developed by Stanford
University Interim (SUI) for use in setting up the ﬁxed broadband systems. These
types of channel model are extensively used for WiMAX or IEEE 802.16 system
development [10]. This chapter is mainly concerned with the time-dispersive empirical
channel model (WiMAX speciﬁcation), which will be explained in detail later on.
1.4.1.3 Physical Models
A channel can be physically modelled by attempting to calculate the physical processes
which modify the transmitted signal. These models rely on basic principles of physics
rather than statistical outcomes from the experiments. Physical channel models are
also known as deterministic models, which often require a complete 3D map of the

24
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
propagation environment. These models are not only divided into nontime dispersive
and time dispersive, but are also modelled with respect to the site speciﬁcation.
The works published in [6, 8] are examples of site-speciﬁc, time-dispersive chan-
nel models. These models are principally identiﬁed as ray-tracing, a high-frequency
approximation approach that traces the route of electromagnetic waves leaving the
transmitter as they interrelate with the objects in the propagation environment. A
deterministic ray-tracing propagation model was used to predict the time delay and
fading characteristics for the channel in a hypothetical urban area. Using this propa-
gation model, the channel response throughout the urban area was described in terms
of the signal level, root-mean-square (RMS) delay spread, and the fading statistics at
each point in the service area. These time-dispersive models provide not only multi-
path delay spread of the channel but also information related to the angle of arrival
(AoA) of the signal.
However, those physical models which do not have signal time delay information are
known as nontime-dispersive channel models. These propagation models are specif-
ically applicable to propagation prediction in the ﬁxed broadband wireless systems.
1.5 Deﬁnition of a Communications Channel and its Parameters
The main factor affecting the design of a ﬁxed wireless access system is the nature
of the channel available, which affects the behaviour of electromagnetic waves prop-
agating through it. The previous subsection presented the different types of channel
modelling procedures and their subcategories. Before dealing with the speciﬁcation
and technical parameters of this model, a general background of the channel is pre-
sented along with the deﬁnitions of parameters like delay spread, path loss, fading,
Doppler effect and so on.
The main processes of a communication system consist of a source, a transmitter,
a channel, a receiver and a destination, as shown in Figure 1.15, where bk are the
message bits, xn are the modulated symbols, rn are received symbols, ˆxn are the
demodulated symbols and ˆbk are the detected message bits. The transmitter takes
information from the source and converts it into a form suitable for transmission.
The wireless communication channel consists of the medium through which the RF
signal passes when travelling from the transmitting antenna to the receiving antenna.
The medium causes the transmitted signal to be distorted in several ways, as previously
mentioned.
In the absence of a line-of-sight between the transmitting antenna and the receiving
antenna, some of the transmitted signal ﬁnds a path to the receiving antenna by
Figure 1.15
Simple communication system.

DEFINITION OF A COMMUNICATIONS CHANNEL AND ITS PARAMETERS
25
reﬂecting or refracting from whatever is blocking the direct line-of-sight between
the two antennas. This action is known as a multipath signal scenario as the many
transmitted signals undergo different degrees of dispersion as they traverse multiple
paths to reach the receiving antenna.
Eventually some of the signal paths recombine vectorially at the receiving antenna,
producing a signal the amplitude of which is dependent upon the phases of the
individual component waveforms. This is the concept of signal fading, which is a
purely spatial phenomenon. If a mobile receiving antenna is moving relative to the
environment and/or the transmitting antenna, the incoming phases of the signals
will vary, producing a signal whose amplitude varies with spatial movement of the
mobile relative to the environment. Although fading is a spatial phenomenon, it is
often perceived as a temporal phenomenon as the mobile device moves through the
multipath signal ﬁeld over time.
To determine a channel model, mathematical descriptions of the transmitter, the
receiver and the effect of the environment (walls) on the signal must be known.
A linear channel can be totally described by its impulse response, that is by what
the received signal would look like if the transmitted signal were an impulse. The
impulse response is the response of the channel at all frequencies, that is, once the
impulse response of the channel is known, one can predict the channel response at all
frequencies.
Let x(t) be the signal transmitted from an antenna through the channel h(t), and y(t)
be the signal received at the receiving side. Assuming no delay, multipath signals and
no other noise present in the system, the channel can be considered as a linear system
with x(t) as an input and y(t) as an output. This relationship between the input and
output in the time domain is represented in (1.16) and shown in Figure 1.16.
y (t) = h (t) ⊗x (t) .
(1.16)
The channel impulse response h(t) is obtained by applying the impulse function to
the channel and can be represented as:
h (t) =
∞
 
i=0
Aie jφiδ (t −τi),
(1.17)
where Ai is the magnitude of the impulse response at delay τ i with phase φi, and
δ(t) is the Dirac delta function [5]. The system in Figure 1.16 is modelled as a linear
Figure 1.16
Channel input-output in time domain.

26
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
time variant ﬁlter with impulse response h (t, τ) and frequency response H( f, t). The
frequency response can be obtained by taking the Fourier transform of h(t):
H( f, t) =
 ∞
−∞
h(t)e−j2π f tdt.
(1.18)
The channel output y(t) at a particular time t = τ is the convolution of the impulse
response h(t) with the input signal x(t), that is:
y(t) = x (t) ⊗h (t) =
∞
 
τ=0
h(τ)x(t −τ),
(1.19)
where ⊗represents the convolution operation. The ﬁnal output y(t) can be obtained
by adding independent noise n(t) to (1.19):
y(t) = x (t) ⊗h (t) + n (t) =
∞
 
τ=0
h(τ)x(t −τ) + n(t).
(1.20)
However, in a wireless environment the transmitted signals are affected in different
ways, for example by the reﬂection that occurs when the signal hits a surface. At the
receiving side the signal arrives from different paths, which add delay at the receiver,
and it can be affected by moving objects in terms of a Doppler shift.
1.5.1 Doppler Shift
Doppler frequency effects occur whenever there is relative movement between the
transmitting and receiving antennas. It manifests itself as a change in the frequency of
the received signal. For a receiver tuned to a particular frequency, this phenomenon
has the effect of reducing the received signal energy as the receiver’s front end is
no longer tuned to the centre frequency of the signal and is therefore not operating
efﬁciently.
The Doppler frequency shift in each propagation path is caused by the rate of
change of signal phase (due to motion). Referring to Figure 1.17 below, if a mobile
device, moving from point A to point B at a velocity v, is receiving a signal from the
signal source, the distance travelled, d, can be found by vt, where t is the change
in time from point A to point B.
From this it can be shown geometrically that the extra distance that the wave has
to travel to get from the signal source to point B(l) with respect to point A is:
d cos (α).
The change in phase of the received signal at point B relative to point A is given
by [8]:
φ = −2π
λ l = −2πvt
λ
cos α.
(1.21)

DEFINITION OF A COMMUNICATIONS CHANNEL AND ITS PARAMETERS
27
Signal Source
B
A
∆l
d
α
Direction of
Motion
Figure 1.17
Example of the effects of Doppler shift [8].
With the change in frequency of the received signal at point B relative to point A
given by:
f = −1
2π
φ
t = v
λ cos α.
(1.22)
it can be seen that the change in path length is governed by the angle between the
direction of motion and the received wave. It should also be noted that when the
mobile antenna is moving closer to the signal source, a positive change in frequency
(Doppler) is caused, and conversely, if the mobile antenna is moving further away
from the signal source it causes a negative change in frequency.
If the mobile antenna is moving in the same plane as the signal source (either to or
from it) then the frequency shift is given by:
f = ± v/λ.
(1.23)
This information is required for modelling the channel using the ray-tracing method,
which calculates all the angles of the received signals in order to calculate the re-
ceived signal strength over time. This method becomes impractical for large numbers
of received signals due to the computational burden. The number of signal reﬂec-
tions also limits this method, and in practice only signals with two reﬂections are
considered.
The movement of the receiving antenna relative to the transmitting antenna, coupled
with the large number of reﬂections and received signal paths, causes the resultant
RF signal envelope at the receiving antenna to appear random in nature. There-
fore, statistical methods must be employed in order to produce a mathematically
tractable model, which produces results in accordance with the observed channel
properties.

28
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
1.5.2 Scattering
At the receiving antenna, the incoming signal consists of a superposition of individual
waves. Each individual wave has the following characteristics: amplitude, frequency,
phase, polarization angle, vertical angle of arrival and horizontal angle of arrival, all
relative to the signal at the transmitter.
If we assume that both the transmitting and receiving antennas are similarly polar-
ized and operating on the same frequency, only the amplitude (An), phase (φn), vertical
angle of arrival (βn) and horizontal angle of arrival (αn) need to be considered. Figure
1.18 shows an individual wave (n) relative to the point of reception.
The values for An, φn, βn and αn are all random and statistically independent. The
mean square value for the amplitude (An) is given by:
E

A2
n

= E0
N ,
(1.24)
where E0 is a positive constant and N is the number of received waves at the point of
reception.
Clarke [7] makes a generalization which assumes that the height of the transmitting
antenna is approximately the same as that of the receiving antenna, so (assuming
Clarke’s model) the vertical angles of arrival (βn) can be set to zero. In practice this
is found to be a good approximation.
Y
Z
X
α
β
Direction of
Received Wave
Figure 1.18
An individual wave relative to the point of reception.

DEFINITION OF A COMMUNICATIONS CHANNEL AND ITS PARAMETERS
29
The phase angles (φn) are uniformly distributed within the range of 0 to 2π, but
the probability density functions for the angles of arrival αn and βn are generally not
speciﬁed.
At the point of reception, the ﬁeld resulting from the superposition of the incoming
waves (n) is given by:
E(t) =
N
 
n=0
En (t).
(1.25)
If an unmodulated carrier is transmitted, the received signal (En(t)) can be expressed
at the point of reception (x0, y0, z0) as follows [8]:
E(t) = An cos

ω0t −2π
λ (x0 cos(αn) cos(βn) + y0 sin(αn) cos(βn)
+ z0 sin(βn)) + φn

.
(1.26)
If the receiving antenna moves in the xy plane at an angle γ (relative to the x-axis)
with a velocity v, then after a unit time, the coordinates of the received signal can be
expressed as:
v cos γ, v sin γ, z0.
(1.27)
Which means that the received signal E(t) can be expressed as:
E(t) = I (t) cos (ωct) −Q (t) sin (ωct) ,
(1.28)
where I(t) and Q(t) represent the in-phase and quadrature components of the signal
respectively, and can be expressed as:
I (t) =
N
 
n=1
An cos (ωnt + θn)
(1.29)
and:
Q (t) =
N
 
n=1
An sin (ωnt + θn),
(1.30)
where ωn equals 2πfn, and fn represents the Doppler shift in frequency experienced
by the individual wave n. The terms ωn and θn can be expressed as:
ωn = 2πv
λ
cos (γ −αn) cos (βn)
(1.31)
and:
θn = 2πz0
λ
sin (βn) + φn.
(1.32)

30
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
The previous equations reduce to Clarke’s two-dimensional model if β is taken to
be zero.
If the number of received waves (N) is very large (in practice 6, but theoretically
much larger), then according to the central limit theorem the components I(t) and Q(t)
are independent Gaussian processes and are completely characterized by their mean
and autocorrelation functions. Because I(t) and Q(t) both have zero mean values, it
follows that E{E(t)} is also zero. Also, I(t) and Q(t) have variance values ( ) that
are the same as the mean square value (average power), thus the probability density
function of I(t) and Q(t) can be expressed as:
px (x) =
1
σ
√
2π
exp

−x2
2σ 2

,
(1.33)
where x = I(t) or Q(t), and σ 2 = E{A2
n} = E0
N .
1.5.3 Angle of Arrival and Spectra
For a system in motion (that is, receiver antenna movement relative to the transmitting
antenna), the individual components of the received signal will experience a change
in frequency due to the Doppler effect, depending upon their individual angles of
arrival. This change in frequency is given by [8]:
fn = ωn
2π = v
λ cos (γ −αn) cos (βn) .
(1.34)
All frequency components within the received signal will experience this Doppler
shift in frequency and, as long as the signal bandwidth is relatively small (compared to
the receive signal path bandwidth), it can be assumed that all the individual component
waves will be affected in the same way.
The RF spectrum of the received signal can be obtained by using the Fourier
transform of the temporal autocorrelation function in terms of the time delay (τ):
E{E(t)E(t + τ)} = E{I(t)I(t + τ)} cos(ωcτ) −E{I(t)Q(t + τ)} sin(ωcτ)
= a(τ) cos(ωcτ) −c(τ) sin(ωcτ).
(1.35)
Aulin [11] showed that the correlation properties can therefore be expressed by
a(τ) and c(τ):
a (τ) = E0
2 E {cos (ωτ)}
c (τ) = E0
2 E {sin (ωτ)} .
(1.36)

DEFINITION OF A COMMUNICATIONS CHANNEL AND ITS PARAMETERS
31
In order to simplify further we make the assumption that all the signal waves arrive
in the horizontal plane (α) with equal probability, so that:
pα (α) = 1
2π .
(1.37)
Which means that by Fourier transforming the following, the power spectrum can
be obtained:
a (τ) = E0
2
 +π
−π
J0 (2π fmτ cos β)pβ (β) dβ.
(1.38)
1.5.4 Multipath Channel and Tapped Delay Line Model
The ﬁxed or mobile radio channel is characterized by the multipath propagation. The
signal offered to the receiver contains not only a direct line-of-sight radio wave, but
also a large number of reﬂected radio waves. This can be even worse in urban areas,
where obstacles often block the line-of-sight and a mobile antenna receives a collection
of various delayed waves. These reﬂected waves interfere with the direct wave, which
causes signiﬁcant degradation of the performance of the link. If the mobile user
moves, the channel varies with the location and time, because the relative phase and
amplitude of the reﬂective wave change. Multipath propagation seriously degrades
the performance of the communication system. The adverse effects produced by
the medium can be reduced by properly characterizing the medium in order to design
the transmitter and receiver to ﬁt the channel. Figure 1.19 shows the signal arriving
at the receiver from different paths, which include the direct line-of-sight (LOS) path
and nonline-of-sight (NLOS) paths.
As the different variants of the same signal arrive at different times, some are delayed
relative to one another. This time dispersion is known as multipath delay spread.
This delay spread is an important parameter in channel modelling and is commonly
measured as root mean square (RMS) delay spread. For reliable communication over
these channels without any interference reduction techniques, the transmitted data rate
should be much smaller than the coherence bandwidth. This type of channel, when
Figure 1.19
Signal arriving at mobile station from different paths.

32
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
Small-scale Fading
delay spread)
(Based on multipath
Flat Fading
Bandwidth of signal < coherence bandwidth 
1.
of the channel
Delay spread < symbol period
2.
Bandwidth of signal > coherence bandwidth 
1.
of the channel
Delay spread > symbol period
2.
Frequency-selective Fading
Figure 1.20
The two classes of fading channels.
the transmitted bandwidth of the signal is much smaller than the coherent bandwidth,
is known as a ﬂat channel or narrowband channel. When the transmitted bandwidth
of the signal is nearly equal to or larger than the coherent bandwidth, the channel
is known as a frequency-selective channel or broadband channel. The relationship
between ﬂat fading and frequency-selective fading is shown in Figure 1.20.
The multipath delay proﬁle is characterized by τ rms, which is deﬁned as [6]:
τ 2
rms =
 
j Pjτ 2
j −(τavg)2,
(1.39)
where τavg = 
j Pjτ j, τ j is the delay of the jth delay component of the proﬁle and
Pj = (power in the jth delay component)/(total power in all components).
The channel output y(t) of (1.22) can be realized as a tapped delay line, as shown
in Figure 1.21, where Ai(t) is the fading amplitude of the ith tap and Ti are the delays
of the ith tap. Each tap represents a scattered ray multiplied by a time-varying fading
proﬁle coefﬁcient Ai(t). The relative tap delays are dependent on the type of channel
T1
T2
T3
T4
T5
T6
T2 – T1
T3 – T2
T4 – T3
T5 – T4
T6 – T5
Σ
A1(t)
A2(t)
A3(t)
A4(t)
A5(t)
A6(t)
Output
Input
Discarded
sample
Figure 1.21
An example of a tapped delay line.

DEFINITION OF A COMMUNICATIONS CHANNEL AND ITS PARAMETERS
33
model. These values are given later for the ﬁxed broadband wireless-access channel
model and the urban UMTS mobile radio channel models.
1.5.5 Delay Spread
Because of the multipath reﬂections, the channel impulse response of a wireless
channel looks like a series of pulses. In practice the number of pulses that can be
distinguished is very large and depends on the time resolution of the communication
or measurement system.
The system evaluations process typically prefers to address a class of channels with
properties that are likely to be encountered, rather than one speciﬁc impulse response.
Hence it deﬁnes the (local-mean) average power, which is received with an excess
delay that falls within the interval (T, T + dt). Such characterization for all T gives the
‘delay proﬁle’ of the channel. The delay proﬁle determines the frequency dispersion;
that is, the extent to which the channel fading at two different frequencies f 1 and f 2 is
correlated (Figure 1.22).
The maximum delay time spread is the total time interval during which reﬂections
with signiﬁcant energy arrive. The RMS delay spread TRMS is the standard deviation
(or root-mean-square) value of the delay of reﬂections, weighted proportionally to the
energy in the reﬂected waves. For a digital signal with high bit rate, this dispersion
is experienced as frequency-selective fading and intersymbol interference (ISI). No
serious ISI is likely to occur if the symbol duration is longer than, say, ten times
the RMS delay spread. The RMS delay spread model published in [14] follows
a lognormal distribution and the median of this distribution grows as a power of
distance. This model was developed for rural, urban and mountainous environments
(Figure 1.23).
τrms = T1dey,
(1.40)
Amplitude 
Amplitude 
Figure 1.22
Example of the impulse response and frequency transfer function of a multipath channel.

34
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
Figure 1.23
Expected power per unit of time.
where τ rms is the RMS delay spread, d is the distance in km, T1 is the median value
of the τrms at d = 1 km, e is an exponent that lies between 0.5 and 1.0, and y is the
lognormal variable.
The narrowband received signal fading can be characterized by a Ricean distri-
bution. The K factor is the ratio between the constant component powers and the
scattered power. The model that represents the K factor by extensive experimental
data taken at 1.9 GHz is given below [12]:
K = Fs Fh FbKod?,
(1.41)
where Fs is a seasonal factor: Fs = 1.0 in summer (leaves) and 2.5 in winter (no leaves);
Fh is the receive antenna height factor, Fh = (h/3)0.46 (h is the receive antenna height);
Fb is the beamwidth factor, Fb = (b/17)−0.62 (b in degrees); Ko and γ are regression
coefﬁcients, Ko = 10; γ = −0.5; u is a lognormal variable with 0 dB mean and a
standard deviation of 8.0 dB.
1.5.6 The Fixed Broadband Wireless Access Channel
The Doppler spectrum of the ﬁxed wireless system is different from that in the
mobile wireless access systems. In ﬁxed wireless systems the Doppler spectrum of
the scattered component is mainly scattered around f = 0 Hz. The shape of the
spectrum is different from the mobile channels. The spectrum for mobile and ﬁxed
wireless channels is given below [10].
The above ﬁxed access channel power spectrum density (PSD) is calculated by
using the equation given below:
S( f ) =

1 −1.72 f 2
o + 0.785 f 4
o | fo| ≤1
0
| fo| > 1
where, fo =
f
fm
.
(1.42)
Figure 1.24 is based on (1.42) and is the rough approximation of the Doppler PSD,
which has the advantage that it is readily available in most existing RF simulators.
This Doppler spectrum is representative of ﬁxed mobile (point-to-point) wireless

DEFINITION OF A COMMUNICATIONS CHANNEL AND ITS PARAMETERS
35
–25
–20
–15
–10
–5
0
1.5
1
0.5
0
–0.5
–1
–1.5
f /f m
PSD, dB
Figure 1.24
Doppler spectrum of a ﬁxed wireless access channel.
channels and so does not represent the Doppler characteristics of a nonstationary
mobile wireless channel.
1.5.7 UMTS Mobile Wireless Channel Model
Three different propagation environments for the UMTS mobile wireless channel
model are considered: Indoor, Pedestrian and Vehicular. These environments were
chosen because they are the main scenarios that cellular mobile radios experience in
normal operation.
The Indoor environment model is characterized by having no Doppler frequency
shift, as the velocities concerned within the indoor environment are well below walk-
ing pace ( 4 miles per hour (MPH)), which produces either zero or a negligible shift
in frequency. Within this environment there is no direct line-of-sight path from trans-
mitter to receiver, so the signal propagates via many different paths as the signals are
reﬂected, refracted or diffracted. Only six paths are simulated as the power contained
within the strongest six rays are deemed strong enough to be included; the rest are
very low power and thus will not affect the results appreciably. If they were included
they would just serve to slow the simulation down.
The Pedestrian environment model is characterized by having a small Doppler
frequency shift, as the velocities concerned within the pedestrian environment are
around walking pace (≈4 MPH), which produces a small Doppler shift in frequency.
Within this environment there is limited/no direct line-of-sight path from transmitter to

36
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
receiver, so the signal propagates via many different paths as the signals are reﬂected,
refracted or diffracted. Only four paths are simulated as only the power contained
within the strongest four rays are deemed strong enough to be included; the rest are
very low power and thus will not affect the results appreciably.
The Vehicular environment model is characterized by having a larger Doppler
frequency shift as the velocities concerned within the vehicular environment are
reasonably large (≈70 MPH), producing a larger Doppler shift in frequency. Within
this environment there is no direct line-of-sight path from transmitter to receiver, so
the signal propagates via many different paths as the signals are reﬂected, refracted
or diffracted. Only six paths are simulated as only the power contained within the
strongest six rays are deemed strong enough to be included; the rest are very low
power and thus will not affect the results appreciably.
A Doppler spectrum is given in (1.43), which is the classic spectrum for mobile
radio channels, and is used by Jakes [8] and Clarke [7]:
S( f ) =



E0
4π fm
1

1 −
 f
fm
2
| f | ≤fm
0
elsewhere
,
(1.43)
where E0 = energy constant. Although this channel is representative of the mobile
wireless channels, there are problems with this representation as the power spectral
density becomes inﬁnite at fC ± fm. In order to ﬁnd a more realistic (and useable)
representation, Aulin [11] described the following:
S( f ) =



0
∀| f | > fm
E0
4 sin βm
1
fm
,
∀fm cos βm ≤| f | ≤fm
1
fm
π
2 −arcsin 2 cos2 βm −1 −( f/ fm)2
1 −( f/ fm)2

,
∀| f | < fm cos βm
.
(1.44)
This claimed to be realistic for small values of βm (angle of signal arrival) and
is particularly useful in providing analytic solutions. The problem with this model
is that there are sharp discontinuities at ±βm, which causes an unrealistic response.
Therefore a model is required which produces the classic Doppler shape for mobile
wireless channels but has no inﬁnities or sharp discontinuities. Such a model was
proposed by Parsons [6], and the PDF of the angle of arrival pβ (β) is given by:
pβ (β) =



π
4 |βm| cos
π
2 · β
βm

|β| ≤|βm| ≤π
2
0
elsewhere
.
(1.45)

DEFINITION OF A COMMUNICATIONS CHANNEL AND ITS PARAMETERS
37
0
–10
–20
–30
–40
dB
–fm
fm
–fm  cos βm
fm  cos βm
Clarke’s model
Aulin’s model
Parsons’ model
Figure 1.25
Doppler spectra of a mobile wireless access channel.
Using (1.45), the power spectral density can be expressed as:
S( f ) = FT
 E0
2
 +π
−π
J0 (2π fmτ cos β)pβ (β) dβ

,
(1.46)
where FT is the Fourier transform and J0 is the zero-order Bessel function of the ﬁrst
kind.
More information on the origin of these equations can be found in [7]. The Doppler
spectra of (1.43), (1.44) and (1.46) are shown in Figure 1.25.
1.5.8 Simulating the Fixed Broadband Wireless Access Channel Model
There are different SUI(Stanford University Interim) models presented in [13]. The
description of the SUI-3 model is explained here, and the model description and
parameters are given in Table 1.2. A three-tap model is described here to represent
the multipath scenario. Doppler PSD of ﬁxed wireless is used to model the real
environment.
Figure 1.26 shows the delay proﬁle of each tap. The magnitude of the channel
coefﬁcients of all the taps, generated and plotted versus time, are shown below in
Figure 1.27.

38
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
Table 1.2
SUI-3 channel parameters.
SUI-3 Channel
Tap 1
Tap 2
Tap 3
Units
Delay
0
0.5
1.0
µs
Power (Omni ant.)
0
−5
−10
dB
K Factor (Omni ant.)
1
0
0
Power (30◦ant.)
0
−11
−22
dB
K Factor (30◦ant.)
3
0
0
Doppler
0.4
0.4
0.4
Hz
Antenna Correlation:
ρENV = 0.4
Gain Reduction Factor:
GRF = 3 dB
Normalization Factor:
Fomni = −1.5113 dB, F30 = −0.3573 dB
1.5.9 Simulating the UMTS Mobile Radio Channel
The parameters for three different UMTS mobile channel models based on the stan-
dards put forward by ETSI, which model an indoor, a pedestrian and a vehicular
environment, are given in this chapter. Tables 1.3–1.5 give the relative delays between
taps and the average tap power for each scenario [14].
Simulation results evaluating the bit-error rate (BER) performance of QPSK mod-
ulation on the indoor, pedestrian and vehicular UMTS channels and BFWA channel
are presented in Figure 1.28.
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Power Delay Profile for SUI-3 Channel
Delay [us]
Normalized Power of each Tap
Figure 1.26
Delay proﬁle of SUI-3 channel model.

DEFINITION OF A COMMUNICATIONS CHANNEL AND ITS PARAMETERS
39
0
10
20
30
40
50
60
70
80
90
100
−40
−35
−30
−25
−20
−15
−10
−5
0
5
10
Fading Profile of SUI-3 Channel 
Magnitude of Channel Coefficients [dB]
Time [s]
Channel Tap 1
Channel Tap 2
Channel Tap 3
Figure 1.27
Fading proﬁle of SUI-3 channel model.
Table 1.3
Channel parameters for the indoor UMTS channel
model.
Tap
Relative delay (ns)
Average power (dB)
1
0
0
2
50
−3.0
3
110
−10.0
4
170
−18.0
5
290
−26.0
6
310
−32.0
Table 1.4
Channel parameters for the pedestrian UMTS channel
model.
Tap
Relative delay (ns)
Average power (dB)
1
0
0
2
110
−9.7
3
190
−19.2
4
410
−22.8

40
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
Table 1.5
Channel parameters for the vehicular UMTS channel
model.
Tap
Relative delay (ns)
Average power (dB)
1
0
0
2
310
−1.0
3
710
−9.0
4
1090
−10.0
5
1730
−15.0
6
2510
−20.0
1.6 (Multiple-Input–Multiple-Output) (MIMO) Channel
The performance analysis of the WiMAX system is extended from a SISO model to
the MIMO model. Let us consider a multi-antenna system with NT transmit and NR
receive antennas characterized by input/output in (1.47); the antenna conﬁguration is
shown in Figure 1.29.
ym (t) =
NT
 
n=1
 ∞
−∞
hmn (t, τ) xn (t −τ) dτ,
m = 1, 2, . . . , NR,
(1.47)
where xn(t) is the signal transmitted from the nth transmitted antenna, ym(t) is the
signal received at the mth receive antenna and hmn (t, τ) is the impulse response of the
channel between the nth transmitted and mth receive antennas. The above equation
1.E-07
1.E-06
1.E-05
1.E-04
1.E-03
1.E-02
1.E-01
1.E+00
24
22
20
18
16
14
12
10
8
6
4
2
0
SNR (dB)
BER
Indoor
Pedestrian
Vehicular
BFWA
Figure 1.28
BER performance of QPSK modulation on the indoor, pedestrian and vehicular and BFWA
channels.

(MULTIPLE-INPUT–MULTIPLE-OUTPUT) (MIMO) CHANNEL
41
Transmitter
Receiver
DATA
IN
DATA
OUT
MIMO
Channel
x(t)
y(t)
H(t,τ)
Figure 1.29
General MIMO channel with matrix H.
can be written in matrix form as follows:
y (t) =
 ∞
−∞
H (t, τ) x (t −τ) dτ,
(1.48)
where H(t, τ) is the NR × NT channel matrix and is given as:
H(t, τ) =


h11 (t, τ)
h12 (t, τ)
h13 (t, τ) ................... h1NT (t, τ)
h21 (t, τ)
h22 (t, τ)
h23 (t, τ) ................... h2NT (t, τ)
h31 (t, τ)
h32 (t, τ)
h33 (t, τ) ................... h3NT (t, τ)
.
.
.
...................
.
.
.
.
...................
.
hNR1 (t, τ) hNR2 (t, τ) hNR3 (t, τ) ................... hNR NT (t, τ)


, (1.49)
y(t) =
%
y1 (t) , y2 (t) , . . . , yNR (t)
&
is the NR size row vector containing the signals
received from NR antenna and x(t) =
%
x1 (t) , x2 (t) , . . . , xNT (t)
&
is the NT size row
vector containing signals transmitted from NT antennas. The impulse response in the
case of the MIMO channel can be explained using the same criteria as in the SISO
case, but the distinguishing feature of MIMO systems is the spatial correlation among
the impulse response composed of H (t, τ).
The channel matrix H deﬁnes the input-output relations of the MIMO system
and is known as the channel transfer function. Let us consider the two-transmit and
two-receive antennae conﬁguration, as shown in Figure 1.30. The system model
comprises two transmitting and two receiving antennae and the medium between
them is modelled as the MIMO channel.
The antenna correlations among the different paths are calculated and shown in
Table 1.6, showing small correlations between the paths of the same channel. Also
note that, for example, path 0 of channel A (A0) has correlation factor of 0.4 with
path 0 of channels B, C and D (i.e. B0, C0 and D0). However, the same path 0 (A0)
has a much lower correlation between other paths of the channels B, C and D.
Similarly, examples of correlation tables for the indoor, pedestrian and vehicular
UMTS MIMO channels used in simulations are given in Tables 1.7–1.9.

42
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
Tx1
Tx2
Rx2
Rx1
Channel A
Channel B
Channel C
Channel D
Figure 1.30
2 × 2 MIMO channel.
Table 1.6
Correlations between four paths, A, B, C and D, with subchannel paths 0, 1, 2 of a 2 × 2
MIMO system.
A0
A1
A2
B0
B1
B2
C0
C1
C2
D0
D1
D2
A0
1
0.017
0.008
0.38
0.013
0.011
0.39
0.008
0.005
0.38
0.007
0.004
A1
0.017
1
0.021
0.003
0.4
0.02
0.003
0.39
0.01
0.003
0.403
0.015
A2
0.008
0.021
1
0.009
0.003
0.398
0.012
0.008
0.39
0.01
0.006
0.402
B0
0.38
0.003
0.009
1
0.003
0.008
0.39
0.004
0.006
0.39
0.008
0.002
B1
0.001
0.4
0.003
0.003
1
0.015
0.008
0.39
0.01
0.008
0.39
0.012
B2
0.01
0.02
0.39
0.008
0.015
1
0.016
0.01
0.39
0.01
0.008
0.39
C0
0.39
0.003
0.01
0.39
0.008
0.016
1
0.002
0.008
0.4
0.009
0.0006
C1
0.008
0.39
0.008
0.004
0.39
0.01
0.002
1
0.008
0.007
0.4
0.009
C2
0.005
0.011
0.39
0.006
0.01
0.39
0.008
0.008
1
0.009
0.007
0.401
D0
0.38
0.003
0.01
0.39
0.008
0.01
0.4
0.007
0.009
1
0.008
0.0039
D1
0.007
0.4
0.006
0.008
0.39
0.008
0.009
0.4
0.007
0.008
1
0.012
D2
0.004
0.001
0.4
0.002
0.01
0.39
0.006
0.009
0.4
0.003
0.012
1
Table 1.7
Mean correlation values between each path of the indoor MIMO channel.
A
B
C
D
A
1
0.006874
0.006467
0.006548
B
0.006874
1
0.005903
0.006568
C
0.006467
0.005903
1
0.006312
D
0.006548
0.006568
0.006312
1
Table 1.8
Mean correlation values between each path of the pedestrian MIMO channel.
A
B
C
D
A
1
0.049111
0.054736
0.050264
B
0.049111
1
0.056464
0.057746
C
0.054736
0.056464
1
0.062907
D
0.050264
0.057746
0.062907
1

MAGNETIC STORAGE CHANNEL MODELLING
43
Table 1.9
Mean correlation values between each path of the vehicular MIMO channel.
A
B
C
D
A
1
0.057106
0.04603
0.052264
B
0.057106
1
0.040664
0.044029
C
0.04603
0.040664
1
0.061777
D
0.052264
0.044029
0.061777
1
In Chapter 7, the UMTS MIMO channels are used to evaluate the performance
of space-time ring-TCM codes. The simplest space-time code uses a delay diversity
code. Its performance on the indoor, pedestrian and vehicular MIMO channels is given
in Figure 1.31.
1.7 Magnetic Storage Channel Modelling
Another application of error-correcting codes can be found in magnetic data storage
devices. An error-correcting scheme in this situation must be able to correct long bursts
of errors and have an efﬁcient decoding algorithm, minimizing latency, to ensure high
data rates. In this section we present a simple channel model for longitudinal magnetic
recording, a method of writing data to a magnetic disc that is currently in use. Data is
written to the magnetic disc by magnetizing microscopic areas on the disc in one of
1.E–05
1.E–04
1.E–03
1.E–02
1.E–01
1.E+00
24
22
20
18
16
14
12
10
8
6
4
2
0
Eb/N0 (dB)
BER
Indoor
Pedestrian
Vehicular
Figure 1.31
Simulation results evaluating the performance of the delay diversity code on the indoor,
pedestrian and vehicular 2 × 2 MIMO channels.

44
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
two directions, representing either a ‘1’ or a ‘0’. The data is recovered by passing the
read head over the disc, which detects changes in the magnetic ﬁeld corresponding
to transitions from ‘0’ to ‘1’ or ‘1’ to ‘0’. Traditionally, data is written on the plane of
the magnetic disc; this is known as longitudinal recording. However, we have reached
the limits of storage capacity for this particular technique, and now data is written
perpendicular to the disc, known as perpendicular recording, which allows storage
capacity to be further increased. In this chapter, modelling a longitudinal magnetic
recording channel is explained, as this is still the most common recording method
used for most hard drives.
1.7.1 Longitudinal Recording
A simple linear channel model for a longitudinal magnetic recording system is given
in Figure 1.32 [15]. As stated previously, the read head measures the changes in
the direction of magnetization, and this can be modelled as a differentiator. The
differentiator with transfer function 1 – D, where D is a memory element, subtracts
the previous bit value from the current bit value.
The transition response for longitudinal magnetic recording can be modelled as a
Lorentzian pulse, given by [15]:
h(t) =
1
1 +

2t
PW50
2 ,
(1.50)
where PW50 is the pulse width of the Lorentzian pulse at half its peak value. Some
Lorentzian pulses and the effect of varying PW50 are shown in Figure 1.33. It will be
shown that increasing PW50 increases the level of intersymbol interference (ISI).
A useful measure of ISI is the recording linear density, denoted by Ds and given
by [15]:
Ds = PW50
τ
,
(1.51)
Binary
Source
1 – D
Differentiator
Lorentzian
Pulse
Electronics
Noise
{0, 1}
{–1, 0, 1}
ISI
To
Detector
Figure 1.32
Simple channel model for a longitudinal magnetic recording channel.

MAGNETIC STORAGE CHANNEL MODELLING
45
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
10
9
8
7
6
5
4
3
2
1
0
–1
–2
–3
–4
–5
–6
–7
–8
–9
–10
Time, t
h(t)
PW50 = 1
PW50 = 2
PW50 = 3
50% of peak amplitude
Figure 1.33
Lorentzian pulses with varying values of PW50.
where τ is the bit period. It gives the number of bits present within a time interval
equal to PW50. A higher value of Ds corresponds to a higher level of ISI.
The response of a positive transition from ‘1’ to a ‘0’ is called a dibit response,
expressed as:
d(t) = h(t) −h(t −τ).
(1.52)
An example of a dibit response where the Lorentzian pulse has PW50 = τ = 1
is given in Figure 1.34. A positive Lorentzian pulse is initiated at t = 0 and is followed
by a negative Lorentzian pulse at t = 1. The sum of the two pulses gives the dibit
response. The peak value of the dibit response is 80% of the peak value of the
Lorentzian pulses since PW50 is wide enough that both pulses overlap. Increasing
PW50 reduces the peak value of the dibit response further and this illustrates how ISI
occurs in a longitudinal magnetic recording channel.
1.7.2 (Partial Response Maximum Likelihood) (PRML) Detection
Partial response maximum likelihood (PRML) detection is a two-stage process con-
sisting of a transversal FIR ﬁlter concatenated with a Viterbi decoder (explained in
Chapter 7), as shown in Figure 1.35.
The idea is to design a partial response (PR) equalizer with coefﬁcients that shape
the frequency response of the channel output to a predetermined target response

46
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
–1
–0.5
0
0.5
1
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0
–0.5
–1
–1.5
–2
–2.5
–3
–3.5
–4
–4.5
–5
t
h(t)
Positive pulse
Negative pulse
Dibit response
PW50 = 1 
PW50 = 1 
0.8
–0.8
Figure 1.34
A positive Lorentzian pulse at t = 0 followed by a negative Lorentzian pulse at t = 1, both
with PW50 = 1. The sum of the two pulses results in the dibit response.
[15, 16]. It is well known that frequency response of the channel output for longitudinal
recording with varying values of Ds matches closely with the frequency response of
the polynomial of the form:
G(D) = (1 −D)(1 + D)n,
(1.53)
where D is a delay element and n is a positive integer. For n = 1, 2 and 3 the polyno-
mials are known as PR4, EPR4 and E2PR4 respectively. For example, a longitudinal
magnetic recording channel with Ds = 2 has a response that matches closely with the
response of G(D) = (1 – D)(1 + D) = 1 – D2, known as PR4. Given a binary input
PR Equaliser
Viterbi Algorithm
PRML Detector
Channel
Output
Recovered
Data
Figure 1.35
Block diagram showing the two processes in PRML detection.

MAGNETIC STORAGE CHANNEL MODELLING
47
error
Channel
F(D)
G(D)
ak
sk
zk
dk
Figure 1.36
PR equalizer design criterion.
ak, the output zk of the PR equalizer should closely match the output dk of the target
polynomial G(D), as shown in Figure 1.36 [16].
1.7.2.1 PR Equalizer Design
The PR equalizer polynomial F(D) and target polynomial G(D) are expressed as
vectors F and G respectively [16]:
F = {−fK −fK+1 · · · f0 · · · fK−1 fK},
(1.54)
where K is a positive integer and ± fi, i = 0, 1, . . ., K, are the coefﬁcients of F(D).
G = { g0 g1 · · · gL−1},
(1.55)
where L is a positive integer and gi, i = 0, 1, . . ., L – 1, are the coefﬁcients of G(D).
Two further matrices R and T are deﬁned as:
R = {Ri, j} = E{sk−isk−j},
−K ≤i, j ≤K,
(1.56)
where E{} is the expectation operator. R therefore contains autocorrelation values of
the channel output:
T = {Ti, j} = E{sk−iak−j},
−K ≤i ≤K, 0 ≤j ≤L −1,
(1.57)
T contains the cross correlation values of the channel output with the binary input.
The coefﬁcients of F can then be determined by [16]:
F = R−1TG.
(1.58)
After equalization the output should be similar to the desired output from G(D). A
diagram of the PR4 polynomial G(D) = 1 −D2 is given in Figure 1.37.
Σ
D
D
+
–
ak
dk
Figure 1.37
The PR4 target.

48
INFORMATION, CHANNEL CAPACITY AND CHANNEL MODELLING
0/0
1/0
00
01
10
11
States
Bit Period, τ
1/1
0/-1
1/0
0/0
1/1
0/-1
0/0
1/1
0/0
1/1
0/0
1/1
Time
0/0
1/0
1/1
0/-1
1/0
0/0
1/1
0/-1
0/0
1/0
1/1
0/-1
1/0
0/0
1/1
0/-1
0/0
1/0
1/1
0/-1
1/0
0/0
1/1
0/-1
0/0
1/0
1/1
0/-1
1/0
0/0
1/1
0/-1
Figure 1.38
Trellis diagram of the PR4 target polyomial.
Since it has two memory elements, this can be expressed as a trellis, as shown in
Figure 1.38, with four states and one input and one output for each branch.
The ﬁnal part of PRML detection involves ﬁnding the maximum likelihood path
through the trellis corresponding to the original binary input ak. This can be achieved
by using the soft-decision Viterbi algorithm.
1.8 Conclusions
In this chapter, an introduction to information theory and channel modelling was
presented. In particular, the capacities of various channel models, such as the AWGN
channel and SISO and MIMO fading channels modelling a ﬁxed wireless access
channel and UMTS cellular channels were examined. Simulation results were also
presented, showing the performance of uncoded schemes on the FWA and UMTS
channels for both MIMO and SISO situations. Finally, descriptions of different chan-
nel models were presented, allowing the performance for many coding schemes in
wireless and magnetic storage channels to be evaluated by computer simulation.
References
[1] Shannon, C.E. (1948) A mathematical theory of communication. Bell Systems Technical Journal,
27, 379–423, 623–56.
[2] Shannon, C.E. (1998) Communications in the presence of noise. Proceedings of IEEE, 86, 447–58.
[3] McEliece, R.J. (1977) The Theory of Information and Coding, Addison-Wesley, Massachusetts.
[4] Abramson, N. (1962) Information Theory and Coding, McGraw-Hill, New York.
[5] Proakis, J.G. (1989) Digital Communications, 2nd edn, McGraw-Hill, New York.
[6] Parsons, J.D. (2000) The Mobile Radio Propagation Channel, 2nd edn, John Wiley & Sons Inc.

REFERENCES
49
[7] Clarke, R.H. (1968) A statistical theory of mobile-radio reception. Bell Systems Technical Journal,
47, 957–1000.
[8] Jakes, W.C. (1994) Microwave Mobile Communications, John Wiley & Sons, Inc., New York.
[9] Hata, M. (1980) Empirical formula for propagation loss in land mobile radio services. IEEE
Transactions on Vehicular Technology, VT-29 (3), 317–25.
[10] IEEE 802.16 (2001) IEEE Standard for Local and Metropolitan Area Networks: part 1b, Air
Interface for Fixed Broadband Wireless Access Systems, April 8, 2002.
[11] Aulin, T. (1979) A modiﬁed model for the fading signal at a mobile radio channel. IEEE Transactions
on Vehicular Technology, VT-28 (3), 182–203.
[12] Greenstein, L.J., Erceg, V., Yeh, Y.S. and Clark, M.V. (1997) A new path-gain/delay-spread prop-
agation model for digital cellular channels. IEEE Transactions on Vehicular Technology, 46 (2),
477–85.
[13] IEEE 802.11 (1994) Wireless Access Method and Physical Layer Speciﬁcation, New York.
[14] Erceg, V. (1999) An empirically based path loss model for wireless channels in suburban environ-
ments. IEEE Journal in Selected Areas in Communications, 17 (7), 1205–11.
[15] Vasic, B. and Kurtas, E.M. (eds) (2005) Coding and Signal Processing for Magnetic Recording
Systems, CRC Press.
[16] Moon, J. and Zeng, W. (1995) Equalization for maximum likelihood detectors. IEEE Trans. Mag.,
31 (2), 1083–8.


2
Basic Principles of Non-Binary
Codes
2.1 Introduction to Algebraic Concepts
In this chapter, the basic mathematical concepts necessary for working with non-binary
error-correcting codes are presented. The elements of the non-binary codes described
in this book belong to a non-binary alphabet and so this chapter begins with a discus-
sion of the deﬁnition and properties of a Group, which will lead to an introduction to
Rings and Fields. Later in the book, knowledge of the properties of rings will be nec-
essary to understanding the design of ring trellis coded modulation (ring-TCM) codes
and ring block coded modulation (ring-BCM) codes. Similarly, a good understanding
of ﬁnite ﬁelds is needed to be able to construct and decode Bose, Ray-Chaudhuri,
Hocquenghem (BCH) codes, Reed–Solomon codes and Algebraic–Geometric codes.
2.1.1 Groups
A set contains any elements or objects with no conditions imposed on it and can
be either ﬁnite or inﬁnite. The number of elements or objects in a set is called the
cardinality. There are two binary operations that can operate on a set: multiplication
‘·’ and addition ‘+’. Certain conditions can be applied to the set under these binary
operations. The most common conditions are [1]:
 Commutativity: For two elements a and b in the set, a·b = b·a under multiplication
or a + b = b + a under addition.
 Identity: For any element a in the set there is an identity element b such that
a·b = a under multiplication or a + b = a under addition.
 Inverse: For any element a in the set its inverse a−1 must also be in the set. This
obeys a·a−1 = a−1·a = b (the identity element).
Non-Binary Error Control Coding for Wireless Communication and Data Storage
Rolando Antonio Carrasco and Martin Johnston
C 2008 John Wiley & Sons, Ltd

52
BASIC PRINCIPLES OF NON-BINARY CODES
 Associativity: For three elements a, b and c in the set, (a·b)·c = a·(b·c).
 Distributivity: For three elements a, b and c in the set, a·(b + c) = (a·b) + (a·c).
A group is deﬁned as a set with the multiplication operation. Multiplying any two
elements in the group must result in a third element that is also in the group. This is
known as closure. A group also has the following conditions:
 associativity under multiplication,
 identity under multiplication,
 inverse.
The group is called a commutative or abelian group if it also has commutativity
under multiplication.
2.1.2 Rings
If the two binary operations ‘+’ and ‘·’ are allowed then a ring can be deﬁned. A ring
must have the following conditions:
1. associativity,
2. distributivity,
3. commutativity under addition.
The ring is called a commutative ring if it also has commutativity under multiplica-
tion. If the ring has a multiplicative identity 1 then it is called a ring with identity. An
example of a ring is the ring of integers Zq under modulo-q addition and multiplica-
tion, where q is the cardinality of the ring. For example, Z4 is deﬁned as {0, 1, 2, 3}.
It is easy to see that the elements obey the three deﬁnitions of a ring. Also, all the
elements commute under multiplication and the multiplicative identity element 1 is
present, meaning that Z4 is a commutative ring with identity. Tables 2.1 and 2.2 show
the addition and multiplication tables respectively of the ring of integers Z 8 = {0, 1,
2, 3, 4, 5, 6, 7}.
Table 2.1
Addition table for Z8.
+
0
1
2
3
4
5
6
7
0
0
1
2
3
4
5
6
7
1
1
2
3
4
5
6
7
0
2
2
3
4
5
6
7
0
1
3
3
4
5
6
7
0
1
2
4
4
5
6
7
0
1
2
3
5
5
6
7
0
1
2
3
4
6
6
7
0
1
2
3
4
5
7
7
0
1
2
3
4
5
6

INTRODUCTION TO ALGEBRAIC CONCEPTS
53
Table 2.2
Multiplication table for Z8.
·
0
1
2
3
4
5
6
7
0
0
0
0
0
0
0
0
0
1
0
1
2
3
4
5
6
7
2
0
2
4
6
0
2
4
6
3
0
3
6
1
4
7
2
5
4
0
4
0
4
0
4
0
4
5
0
5
2
7
4
1
6
3
6
0
6
4
2
0
6
4
2
7
0
7
6
5
4
3
2
1
2.1.3 Ring of Polynomials
The set of all polynomials with coefﬁcients deﬁned in Zq forms a ring under
the addition and multiplication operations. If we deﬁne two polynomials f(x) and
g(x) as f (x) = f0 + f1x + f2x2 + · · · + fvxv and g(x) = g0 + g1x + g2x2 + · · · +
gwxw, where fi and gi ∈Zq, v and w are the degrees of both polynomials respectively
with v < w, then the addition of both polynomials is:
f (x) + g(x) =

i
( fi + gi) · xi,
where fi + gi ∈Zq.
Similarly, the product of the two polynomials is:
f (x) · g(x) =

i


i
j=0
f j · gi−j

· xi
where

i


i
j=0
f j · gi−j

∈Zq.
For example, if f (x) = 1 + 2x + 3x2 and g(x) = 3 + x + 2x2 + x3 are deﬁned in
Z4, then the sum of both polynomials is:
f (x) + g(x) =

1 + 2x + 3x2
+

3 + x + 2x2 + x3
= (1 + 3) + (2 + 1)x + (3 + 2)x2 + x3
= 3x + x2 + x3.
Similarly, the product of both polynomials is:
f (x) · g(x) = (1 + 2x + 3x2) · (3 + x + 2x2 + x3)
= (1 × 3) + (1 × 1 + 2 × 3)x + (1 × 2 + 2 × 1 + 3 × 3)x2
+ (1 × 1 + 2 × 2 + 3 × 1)x3 + (3 × 2 + 2 × 1)x4 + (3 × 1)x3
= 3 + 7x + 13x2 + 8x3 + 8x4 + 3x3
= 3 + 3x + x2 + 3x3.

54
BASIC PRINCIPLES OF NON-BINARY CODES
Table 2.3
Multiplication table of nonzero elements in GF(5).
·
1
2
3
4
1
1
2
3
4
2
2
4
1
3
3
3
1
4
2
4
4
3
2
1
2.1.4 Fields
A ﬁeld is similar to a ring as it also uses both binary operations. It has the following
conditions:
1. Commutativity under addition,
2. Distributivity,
3. Commutativity under multiplication when the additive identity element 0 is
removed,
4. Identity,
5. Inverse.
An example of a ﬁeld is the set of real numbers. The set of integers is not a ﬁeld
since not all integers have a multiplicative inverse. A ﬁnite ﬁeld has a ﬁnite number of
elements and is known as a Galois Field, written as GF(q), where q is the cardinality
of the ﬁeld and is a prime number or a power of a prime greater than 1. To be a
ﬁnite ﬁeld the elements {1, 2, 3, . . . , q −1} must form a group under multiplication
modulo-q.
For example, GF(5) = {0, 1, 2, 3, 4} is a ﬁnite ﬁeld because the elements
{1, 2, 3, 4} are a group under modulo-q multiplication. The group has closure because
no two elements multiplied together give 0, which is not in the group, as shown in
Table 2.3.
However, let us say that the set {0, 1, 2, 3, 4, 5} deﬁnes the ﬁnite ﬁeld GF(6). To
be a ﬁnite ﬁeld the elements {1, 2, 3, 4, 5} must form a group, but this is not the case.
It can be seen in Table 2.4 that some elements produce a 0 when multiplied together,
Table 2.4
Multiplication table of nonzero elements in GF(6),
proving it is not a ﬁnite ﬁeld.
·
1
2
3
4
5
1
1
2
3
4
5
2
2
4
0
2
4
3
3
0
3
0
3
4
4
2
0
4
2
5
5
4
3
2
1

INTRODUCTION TO ALGEBRAIC CONCEPTS
55
Table 2.5
The order of each nonzero element in GF(5).
Element in GF(5)
Order
1
1
2
4
3
4
4
2
which is not in the set {1, 2, 3, 4, 5}, and this proves that GF(6) is not a ﬁnite ﬁeld.
Hence, only sets with a prime or a power of a prime number of elements can be ﬁnite
ﬁelds.
Every element β in a ﬁnite ﬁeld has an order, denoted as ord(β). The order of an
element is the number of times the element is multiplied by itself until the produce
equals 1, that is the value of m which gives βm = 1. In a ﬁnite ﬁeld GF(q) the order
of an element divides q −1. For example, the order of the element 2 in GF(5) is 4
because 2 × 2 × 2 × 2 = 24 = 1. The order of each element in GF(5) is shown in
Table 2.5.
It can be seen that all the elements in GF(5) have an order which divides q −1 =
4. An element which has an order equal to q −1 is called a primitive element.
Therefore, from Table 2.5, the elements 2 and 3 are primitive elements in GF(5) since
they have an order of 4. This means that all the nonzero elements in a ﬁnite ﬁeld can
be expressed as powers of primitive elements. For example, in GF(5), 2 and 3 are the
primitive elements, meaning that all the nonzero elements in GF(5) can be expressed
using powers of 2 or 3, as shown in Table 2.6.
Finite ﬁelds of the form GF( pm), where p is prime and m > 0, are called extension
ﬁelds. They contain the elements

0, 1, α, α2, . . . , α pm−2	
, where α is the primitive
element with order pm −1. This means that all the nonzero elements in GF( pm) can
Table 2.6
Using primitive elements to deﬁne all nonzero elements in GF(5).
Powers of primitive element 2
Element in GF(5)
21
2
22
4
23
3
24
1
Powers of primitive element 3
Element in GF(5)
31
3
32
4
33
2
34
1

56
BASIC PRINCIPLES OF NON-BINARY CODES
be represented as powers of α. For example, GF(22) = {0, 1, α, α2}. Each element in
GF( pm) can be expressed as an m-tuple vector with elements from GF( p) and addition
between elements is done modulo-p.
2.1.5 Primitive Polynomials
A class of polynomials called primitive polynomials are used to deﬁne GF( pm). An
irreducible polynomial f(x) of degree m deﬁned over GF( p) is primitive if the smallest
positive integer n for which f(x) divides xn −1 is n = pm– 1 [2]. As an example, take
the irreducible polynomial
x4 + x + 1
and p = 2.
This polynomial will be primitive if it divides x24−1 + 1 = x16−1 + 1 = x15 + 1,
as −1 ≡1 when p = 2. It can be seen from Figure 2.1 that there is no remainder and
so x4 + x + 1 divides 1 + x15. Similar calculations will show that x4 + x + 1 will not
divide xn + 1, 1 ≤n < 15, which means that x4 + x + 1 is a primitive polynomial. If
a primitive element α is the root of a primitive polynomial then a higher power of α
1
0
1
1
0
0
0
0
0
0
0
0
0
0
1
0x
0x
0x
0x
0x
0x
0x
0x
0x
0x
0x
0x
0x
0x
1
2
3
5
7
8
11
4
4
2
5
2
4
5
2
3
6
3
4
5
6
3
4
7
5
6
7
5
6
9
7
9
7
8
11
8
9
11
8
9
12
11
12
11
12
15
2
3
4
5
6
7
8
9
10
11
12
13
14
15
4
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
Figure 2.1
Long division showing that the primitive polynomial x4 + x + 1 divides x15 −1.

INTRODUCTION TO ALGEBRAIC CONCEPTS
57
can be expressed as the sum of lower powers of α. For example, if α is a root of x4 +
x + 1, this implies α4 + α + 1 = 0 or α4 = −α −1. So, α5 = −α2 −α, α6 = −α3
−α2, α7 = −α4 −α3 = (−α −1) −α3 and so on. In this way an extension ﬁeld
GF( pm) can be deﬁned with the elements expressed in the form αm−1 + αm−2 + . . .
α + 1, or equivalently an m-dimensional vector space over GF( p).
Example 2.1:
Constructing the extension ﬁeld GF(23): The extension ﬁeld
GF(23) can be constructed using the primitive polynomial f(x) = x3 + x + 1. If α
is the root of f(x) then α3 = α + 1. The elements of GF(23) are shown in Table 2.7.
Table 2.7
Construction of GF(23).
Element in GF(23)
Element expressed as the sum of
lower powers of α
Element expressed as 3-tuple
vector over GF(2)
0
0
000
1
1
001
α
α
010
α2
α2
100
α3
α + 1
011
α4
α2 + α
110
α5
α3 + α2 = α + 1 + α2
111
α6
α4 + α3 = α2 + α + α + 1 = α2 + 1
101
To prove the ﬁeld has closure, the element α7 can be expressed as α6. α =
(α2 + 1). α = α3 + α = α + 1 + α = 1, that is αc(q−1) = 1 where c is a
nonnegative integer. Addition in GF(23) is accomplished by adding the vector rep-
resentation of the elements modulo-2. For example, α5 + α6 = (111) + (101) =
(010) = α. Multiplication is accomplished by taking the powers of the elements
and adding them modulo-7. For example, α4·α5 = α(4+5)mod 7 = α2. The ad-
dition and multiplication tables for GF(23) are shown in Tables 2.8 and 2.9,
respectively.
Table 2.8
Addition table for GF(23).
+
0
1
α
α2
α3
α4
α5
α6
0
0
1
α
α2
α3
α4
α5
α6
1
1
0
α3
α6
α
α5
α4
α2
α
α
α3
0
α4
1
α2
α6
α5
α2
α2
α6
α4
0
α5
α
α3
1
α3
α3
α
1
α5
0
α6
α2
α4
α4
α4
α5
α2
α
α6
0
1
α3
α5
α5
α4
α6
α3
α2
1
0
α
α6
α6
α2
α5
1
α4
α3
α
0

58
BASIC PRINCIPLES OF NON-BINARY CODES
Table 2.9
Multiplication table for GF(23).
·
0
1
α
α2
α3
α4
α5
α6
0
0
0
0
0
0
0
0
0
1
0
1
α
α2
α3
α4
α5
α6
α
0
α
α2
α3
α4
α5
α6
1
α2
0
α2
α3
α4
α5
α6
1
α
α3
0
α3
α4
α5
α6
1
α
α2
α4
0
α4
α5
α6
1
α
α2
α3
α5
0
α5
α6
1
α
α2
α3
α4
α6
0
α6
1
α
α2
α3
α4
α5
Example 2.2:
Constructing the extension ﬁeld GF(32): The extension ﬁeld
GF(32) can be constructed using the primitive polynomial f(x) = x2 −2x −1. If
the primitive element α is the root of f(x) then α2 = 2α + 1 and can be expressed
as a 2-dimensional vector space over GF(3), as shown in Table 2.10.
Table 2.10
Construction of GF(32).
Element in
GF(32)
Element expressed as the sum of lower powers of α
Element expressed as
2-tuple vector over GF(3)
0
0
00
1
1
01
α
α
10
α2
2α + 1
21
α3
2α2 + α = 2(2α + 1) + α = 4α + 2+ α = 5α + 2
= 2α + 2
22
α4
2α2 + 2α = 2(2α + 1) + 2α = 4α + 2 + 2α = 6α
+ 2 = 2
02
α5
2α
20
α6
2α2 = 2(2α + 1) = 4α + 2 = α + 2
12
α7
α2 + 2α = 2α + 1 + 2α = 4α + 1 = α + 1
11
To add two elements together, their corresponding vectors are added modulo-3,
for example α4 + α6 = (02) + (12) = (11) = α7. To multiply two elements, their
powers are added modulo-8, for example α6 + α7 = α(6+7)mod 8 = α5.
2.1.6 Minimal Polynomials and Cyclotomic Cosets
It was stated that a primitive polynomial deﬁned in GF(2) was irreducible, implying
that it cannot be factorized, that is it has no roots in GF(2). However, that a polynomial
is irreducible in one ﬁnite ﬁeld usually does not mean that it will be irreducible in

INTRODUCTION TO ALGEBRAIC CONCEPTS
59
other ﬁnite ﬁelds. By substituting each element of GF(8) into the primitive polynomial
x3 + x + 1 it can be seen that this polynomial has three roots: X1 = α, X2 = α2 and
X3 = α4. This means that x3 + x + 1 can be factorized to (x + α)(x + α2)(x + α4).
In general, given one root β ∈GF(2p), the remaining roots will be β2i, i = 1, 2, 3,
. . . , 2p−1 −1. These roots are called the conjugates of β and the polynomial
M(x) = (x + β)(x + β2)(x + β4) . . .

x + β2p−1−1
is called the minimal polynomial [2] of β, that is the degree of M(x) is minimal.
Therefore, the minimal polynomial of α, α2 and α4 in GF(8) is M(x) = x3 + x + 1,
which is also a primitive polynomial. The minimal polynomials for the other elements
in GF(8) are calculated as follows.
For β = 0 the minimal polynomial is M(x) = x, and for β = 1, M(x) = x + 1. The
conjugates for β = α3 are α6 and α5. The minimal polynomial for α3 is then:
M(x) = (x + α3)(x + α6)(x + α5)
=

x2 + (α3 + α6)x + α9
(x + α5)
= (x2 + α4x + α2)(x + α5)
= x3 + (α5 + α4)x2 + (α9 + α2)x + α7
= x3 + x2 + 1.
The powers of ﬁnite ﬁeld elements associated with each minimal polynomial
form cyclotomic cosets. The minimal polynomials for GF(8) and their corresponding
cyclotomic cosets are summarized in Table 2.11.
2.1.7 Subﬁelds
Some elements within a ﬁnite ﬁeld GF( pm) can also belong to a smaller ﬁnite ﬁeld,
known as a subﬁeld. These elements will have a minimal polynomial that matches a
known primitive polynomial that can deﬁne a smaller ﬁnite ﬁeld. An example of a
subﬁeld can be found in GF(16), deﬁned by the primitive polynomial x4 + x + 1. The
element α5 has only one conjugate, namely α10. The minimal polynomial for α5 in
Table 2.11
Elements in GF(8) and their corresponding minimal polynomials.
Finite ﬁeld element in GF(8)
Minimal polynomial
Cyclotomic coset
0
x
—
1
x + 1
{0}
α, α2, α4
x3 + x + 1
{1, 2, 4}
α3, α6, α5
x3 + x2 + 1
{3, 6, 5}

60
BASIC PRINCIPLES OF NON-BINARY CODES
GF(16) is therefore:
M(x) = (x + α5)(x + α10)
= x2 + (α5 + α10)x + α15
= x2 + ((α2 + α) + (α2 + α + 1))x + 1
= x2 + x + 1.
This minimal polynomial is known to be the primitive polynomial that deﬁnes
GF(4), and this implies that β = α5 is a primitive element for the subﬁeld GF(4)
within GF(16). This subﬁeld then consists of the elements {0, 1, β, β2} = {0, 1,
α5, α10}.
More examples of subﬁelds can be found in larger ﬁnite ﬁelds, such as GF(64),
deﬁned by the primitive polynomial x6 + x + 1. A list of minimal polynomials
for GF(64) and many others can be found in [1]. The element α21 has a minimal
polynomial of x2 + x + 1, implying that it is the primitive element of the subﬁeld
GF(4) with elements {0, 1, α21, α42}. Also, the element α9 has a minimal polynomial
of x3 + x2 + 1, implying that it is the primitive element of the subﬁeld GF(8) with
elements {0, 1, β, β2, β3, β4, β5, β6} = {0, 1, α9, α18, α36, α8, α16, α32}.
2.2 Algebraic Geometry Principles
2.2.1 Projective and Afﬁne Space
The construction of AG codes requires a set of points that satisfy an irreducible
afﬁne curve and a set of rational functions deﬁned on the curve. Projective space is
(n + 1)-dimensional and elements in this space deﬁned over some ﬁnite ﬁeld are
called projective points, that is [3]:
(c1, c2, c3, . . . , cn, cn+1) ,
where ci, i = 0, 1, . . . , n, n + 1, are elements in some ﬁnite ﬁeld.
Afﬁne space is n-dimensional. Elements in this space deﬁned over some ﬁnite ﬁeld
are called afﬁne points and are of the form [3] (c1, c2, c3, . . . , cn, 1).
Finally, when points are of the form (c1, c2, c3, . . . , cn, 0), the space is called the
hyperplane at inﬁnity [3]. All points in the hyperplane at inﬁnity are called points at
inﬁnity. For the construction of AG codes we are interested in ﬁnding all afﬁne points
and points at inﬁnity that cause an irreducible smooth afﬁne curve to vanish.
2.2.2 Projective and Afﬁne Curves
A projective curve is an (n + 1)-dimensional curve deﬁned by projective points. It
is made up of n-dimensional afﬁne curves in the n + 1 different coordinate systems.

ALGEBRAIC GEOMETRY PRINCIPLES
61
For a 3-dimensional projective curve C(x, y, z) there are three afﬁne curves:
C(x, y, 1), C(x,1, z,) and C(1, y, z).
Algebraic–geometric codes are constructed from irreducible afﬁne smooth curves.
An irreducible curve is a curve that cannot be expressed as the product of curves of
lower degrees. For example, the curve C(x, y) = x2 + y2 is not irreducible over GF(2)
because:
(x + y)(x + y)
= (x2 + xy + xy + y2)
and
xy + xy = 0.
= x2 + y2
Similarly, the curve C(x, y) = x3 + y3 is irreducible over GF(2) but is not irreducible
over GF(3) because:
(x + y)(x + y)(x + y)
= (x2 + 2xy + y2)(x + y)
= x3 + x2y + 2x2y + 2xy2 + xy2 + y3.
= x3 + 3x2y + 3xy2 + y3
and
3 ≡0 mod 3
= x3 + y3
A point on a curve is said to be nonsingular if all partial derivatives of the curve
do not vanish at this point. If all the points that satisfy the curve are nonsingular then
the curve is said to be nonsingular or smooth [4]. An important class of curve is the
Hermitian curve, which is deﬁned over square ﬁnite ﬁelds GF(w2). The projective
Hermitian curve is:
C(x, y, z) = xw+1 + ywz + yzw.
(2.1)
This is a smooth curve, and to prove it the three partial derivatives are calculated:
∂C(x, y, z)
∂x
= (w + 1)xw = xw
∂C(x, y, z)
∂y
= wyw−1z + zw = zw.
∂C(x, y, z)
∂z
= yw + wyzw−1 = yw
The only point that makes all three partial derivatives vanish is (0, 0, 0), but this is
not a point in projective space, and so all the points are nonsingular and the curve is
smooth.
2.2.3 Finding Points on an Afﬁne Curve
The projective points that satisfy the projective curve C(x, y, z) = 0 are of the form
(α, β, δ), where α, β, δ are elements in a ﬁnite ﬁeld. For the construction of an AG

62
BASIC PRINCIPLES OF NON-BINARY CODES
code, only the afﬁne points of the form (α, β, 1), that is z =1, and points at inﬁnity of
the form (α, β, 0), that is z = 0, are required. To ﬁnd all the afﬁne points the projective
curve is broken down into its afﬁne component curves. For each afﬁne curve, the
afﬁne points (α, β, 1) and the points at inﬁnity (α, β, 0) that cause the curve to vanish
are kept. By using all the afﬁne component curves more points can be found, resulting
in longer codes.
The afﬁne form of the Hermitian curve from (2.1) in the (x −y) coordinate system
(with z = 1) is:
C(x, y, 1) = xw+1 + yw + y.
(2.2)
In the (x −z) coordinate system (with y = 1) it is:
C(x, 1, z) = xw+1 + z + zw.
(2.3)
In the (y −z) coordinate system (with x = 1) it is:
C(1, y, z) = 1 + ywz + yzw.
(2.4)
The Hermitian curves are known to have w3 + 1 points. For GF(22), the Hermitian
curve will have 23 + 1 = 8 + 1 = 9 points and is deﬁned as:
C(x, y, z) = x3 + y2z + yz2,
(2.5)
with the afﬁne curve in the (x −y) system deﬁned as:
C(x, y) = x3 + y2 + y.
(2.6)
The afﬁne points that satisfy C(x, y, 1) = 0 are found by substituting all possible val-
ues of x and y (with z = 1) in GF(22), where α2 = α + 1, into (2.6). This is shown below:
C(0, 0, 1) = 03 + 02 + 0 = 0
C(0, 1, 1) = 03 + 12 + 1 = 0
C(0, α, 1) = 03 + α3 + α = 1 + α = α2 = 0
C(0, α2, 1) = 03 + α6 + α2 = 1 + α2 = α = 0
C(1, 0, 1) = 13 + 02 + 0 = 1 = 0
C(1, 1, 1) = 13 + 12 + 1 = 1 = 0
C(1, α, 1) = 13 + α2 + α = 1 + 1 = 0
C(1, α2, 1) = 13 + α4 + α2 = 1 + α + α2 = 1 + 1 = 0
C(α, 0, 1) = α3 + 02 + 0 = α3 = 1 = 0
C(α, 1, 1) = α3 + 12 + 1 = α3 = 1 = 0
C(α, α, 1) = α3 + α2 + α = 1 + 1 = 0
C(α, α2, 1) = α3 + α4 + α2 = 1 + α + α2 = 1 + 1 = 0

ALGEBRAIC GEOMETRY PRINCIPLES
63
Table 2.12
Eight projective points for the Hermitian curve in the ( x −y) system.
P1 = (0, 0, 1)
P2 = (0, 1, 1)
P3 = (1, α, 1)
P4 = (1, α2, 1)
P5 = (α, α, 1)
P6 = (α, α2, 1)
P7 = (α2, α, 1)
P8 = (α2, α2, 1)
C(α2, 0, 1) = α6 + 02 + 0 = 1 = 0
C(α2, 1, 1) = α6 + 12 + 1 = α6 = 1 = 0
C(α2, α, 1) = α6 + α2 + α = α6 + 1 = 1 + 1 = 0.
C(α2, α2, 1) = α6 + α4 + α2 = α6 + α + α2 = α6 + 1 = 1 + 1 = 0
Therefore, the afﬁne points that satisfy C(x, y, 1) = 0 are given in Table 2.12.
We must also ﬁnd the points of the other two afﬁne curves, C(x, 1, z) = x3 + z
+ z2 and C(1, y, z) = 1 + y2z + yz2. There are eight projective points that satisfy
C(x, 1, z) = 0, as shown in Table 2.13.
However, P2 is the only afﬁne point, since it is of the form (x, y, 1), but it is also
in Table 2.12. There is also a point at inﬁnity, P1 = (0, 1, 0). Finally, there are four
projective points that satisfy C(1, y, z) = 0, as shown in Table 2.14. In this case, only
P1 and P3 are afﬁne points, since they are of the form (x, y, 1), and they are also
present in Table 2.12. Therefore, the projective Hermitian curve in (2.5) has eight
afﬁne points and one point at inﬁnity, Q = (0, 1, 0).
All codes constructed from curves with one point at inﬁnity are called one-point
AG or Goppa codes [4] and these are the most commonly used codes in the literature.
Elliptic and hyperelliptic curves are other examples of curves that have one point at
inﬁnity. An upper bound on the number of points N, including any points at inﬁnity,
that satisfy a curve over a ﬁeld GF(q) is the Hasse-Weil bound, deﬁned as [3]:
|N| ≤(m −1)(m −2)√q + 1 + q,
(2.7)
where m is the degree of the curve. Taking the Hermitian curve deﬁned in (2.5) with
degree m = 3 and q = 4, the upper bound from (2.7) is:
|N| ≤(3 −1)(3 −2)
√
4 + 1 + 4
|N| ≤9.
Table 2.13
Eight projective points for the Hermitian curve in the ( x −z) system.
P1 = (0, 1, 0)
P2 = (0, 1, 1)
P3 = (1, 1, α)
P4 = (1, 1, α2)
P5 = (α, 1, α)
P6 = (α, 1, α2)
P7 = (α2, 1, α)
P8 = (α2, 1, α2)
Table 2.14
Four projective points for the Hermitian curve in the ( y −z) system.
P1 = (1, α, 1)
P2 = (1, α, α2)
P3 = (1, α2, 1)
P4 = (1, α2, α)

64
BASIC PRINCIPLES OF NON-BINARY CODES
Therefore, the maximum number of points a curve can have with m = 3 and deﬁned
over GF(22) is nine. For the Hermitian curve we have eight afﬁne points and one point
at inﬁnity, giving a total of nine points. Therefore, the Hermitian curves meet the
Hasse-Weil bound and are known as maximal curves. These curves are desirable
because they produce long codes. Other types of maximal curve include the elliptic
curves over certain ﬁnite ﬁelds. For example, the elliptic curve [3, 5]
C(x, y) = x3 + x2 + x + y2 + y + 1
is a maximal curve over GF(24). It has degree m = 3 and from (2.7) it has 25 points.
Compare this to the Hermitian curve deﬁned over GF(24)
C(x, y) = x5 + y4 + y,
which has m = 5 and from (2.7) has 65 points. We can see that codes from elliptic
curves will be much shorter than codes constructed from Hermitian curves, however
they are still longer that Reed–Solomon codes, which are constructed from the afﬁne
line
y = 0
and have degree m = 1. For a ﬁnite ﬁeld GF(q), the Hasse-Weil bound in (2.7)
simpliﬁes to:
|N| ≤q + 1,
so for GF(24) the longest possible Reed–Solomon code is 16 + 1 = 17, which is
shorter than a code constructed from an elliptic curve. However, it will be seen later
that maximizing the degree of a curve is not the only design criterion for constructing
good AG codes.
2.2.4 Rational Functions on Curves
To construct the generator matrix of an AG code, a basis of rational functions on the
curve must ﬁrst be deﬁned. Each rational function is evaluated at each of the n afﬁne
points to form a row of the generator matrix, where n is the block length of the code.
A rational function f(x, y, z) is the quotient of two other functions, g(x, y, z) and
h(x, y, z), that both have the same degree, that is f (x, y, z) = g(x,y,z)
h(x,y,z). Deﬁning a ratio-
nal function on a curve changes its behaviour. For example, the function f (x, y) =
x
y+1
deﬁned over the cubic curve C(x, y) = x3 + y3 + 1 =0 over GF(22) can be rewritten as:
x3 = y3 + 1
x = y3 + 1
x2
= (y + 1)(y2 + y + 1)
x2
.
∴
x
y + 1 = y2 + y + 1
x2

ALGEBRAIC GEOMETRY PRINCIPLES
65
The cubic curve has six points: P1 = (0, 1), P2 = (0, α), P3 = (0, α2), P4 = (1, 0),
P5 = (α, 0), P6 = (α2, 0). In this case the function f (x, y) =
x
y+1 is deﬁned for all
the points on the cubic curve except for P1 = (0, 1), since f (0, 1) =
0
1+1 = 0
0.
If the function was not deﬁned on the curve it would have a zero of order 1 and a
pole of order 1 at this point. However, since it is on the curve the function evaluated
at (0, 1) gives:
f (x, y) = y2 + y + 1
x2
,
f (0, 1) = 12 + 1 + 1
02
= 1
0
which has no zeroes but has a pole of order 2 due to the x2 in the denominator. The
order of a function is denoted by ν(f(x, y, z)) and is the sum of its zero order and pole
order. To construct an AG code, the basis of the rational functions must have a pole
at the points of inﬁnity, but have no other poles at any of the afﬁne points.
Example 2.3: Rational functions on the projective line y = 0: The points in
the (x, y, 1) system are of the form (αi, 0, 1) for a ﬁnite ﬁeld GF(q), where αi is
a primitive element, so there are q points in this system. In the (x, 1, z) system
there are no points because y = 0. In the (1, y, z) system the points are of the
form (1, 0, αi), but only (1, 0, 1) is of the form (α, β, 1). In this system there is
also the point (1, 0, 0), which is in the hyperplane at inﬁnity (since z = 0) and
so is a point at inﬁnity. Therefore there are q afﬁne points and 1 point at inﬁnity.
Next, a sequence of rational functions on the projective line that have a pole only
at the point of inﬁnity Q = (1, 0, 0) must be found. The sequence of rational
functions { xi
zi }, i > 0, has a pole of order i at Q but has no poles at any of the
afﬁne points because z = 1. From this example, we can see that the number of
points on the projective line cannot exceed the cardinality of the ﬁnite ﬁeld the
line is deﬁned over. The codes constructed from the line y = 0 are actually the
well-known Reed–Solomon codes described in Chapter 3, which can be viewed
as the simplest type of algebraic–geometric code. The small number of points on
the projective line is the reason why Reed–Solomon codes only have short code
lengths – no greater than the size of the ﬁnite ﬁeld.
Example 2.4: Rational functions on the Hermitian curve: It is well known that
a sequence of rational functions on the projective Hermitian curve in (2.1) can be
formed from x
z and y
z [3]. The term x
z can be rewritten as:
xw+1 = ywz + yzw
x
z = yw + yzw−1
xw
,

66
BASIC PRINCIPLES OF NON-BINARY CODES
which has an order of w at the point at inﬁnity Q = (0, 1, 0). Similarly, the term y
z
can be rewritten as:
xw+1 = ywz + yzw
ywz + yzw
xw+1
= 1
1
z = yw + yzw−1
xw+1
y
z = yw+1 + y2zw−1
xw+1
,
which has an order of w + 1 at the point at inﬁnity Q = (0, 1, 0). Therefore,
for the Hermitian curve deﬁned over GF(22) in (2.3) with r = 2, v
 x
z

= 2 and
v
 y
z

= 3, other rational functions on the curve are formed by combining the
products of different powers of x
z and y
z and adding their orders together. For
example, x
z · x
z = x2
z2 has an order of 2 + 2 = 4, x
z · y
z = xy
z2 has an order of 2 +
3 = 5, y
z · y
z = y2
z2 has an order of 3 + 3 = 6 and so on.
In general, the rational function xi y j
zi+ j on the Hermitian curve has an order of [3]:
v
xi y j
zi+ j

= iw + j(w + 1).
(2.8)
This sequence of rational functions is denoted by L(G), where G is a divisor on
the curve. A divisor of a curve assigns an integer value to every point of the curve.
An AG code is deﬁned by two divisors, D and G. The divisor D assigns the value
D(P) = 1 to every afﬁne point and is the sum of all the afﬁne points [3, 4]:
D =
n

i=1
D(Pi)Pi =
n

i=1
Pi.
(2.9)
The sum n
i=1 D(P) is called the degree of divisor D, d(D).
Similarly, the divisor G assigns an integer value G(Q) to each point at inﬁnity
Q and is deﬁned as the sum of all points at inﬁnity. For curves with one point at
inﬁnity, the divisor G is just the point at inﬁnity multiplied by the degree of G,
d(G).
G =

i
G(Qi)Qi = d(G)Q
(2.10)
The space L(G) contains rational functions of order up to d(G). Continuing the
example above, if G = 7Q with d(G) = 7 then:
L(7Q) =

1, x
z , y
z , x2
z2 , xy
z2 , y2
z2 , x3
z3 , x2y
z3
%
.

ALGEBRAIC GEOMETRY PRINCIPLES
67
Notice that the order of x3
z3 is 6, which is the same as the order of y2
z2 . In this case
all functions with a power of x greater than or equal to 3 are removed to avoid
functions with the same order. Alternatively, all functions with a power of y greater
than 1 can be removed. Therefore, there are seven rational functions in L(G):
L(G) =

1, x
z , y
z , x2
z2 , xy
z2 , y2
z2 , x2y
z3
%
.
2.2.5 Riemann–Roch Theorem
The Riemann–Roch theorem can be used to calculate the number of rational functions
in L(G) with order up to and including d(G), and hence to determine the dimension
and minimum distance of the code. The number of rational functions in L(G) is called
the dimension of G, l(G). The theorem states that there exists a nonnegative integer γ
such that [3, 4]:
l(G) −d(G) = 1 −γ ,
(2.11)
provided that d(G) > 2γ −2.
The nonnegative integer γ is called the genus and is deﬁned as [3, 4]:
γ = (m −1)(m −2)
2
,
(2.12)
where m is the degree of the curve, and plays an important part in the size of the code
parameters.
Example 2.5: The number of rational functions on the Hermitian curve with
pole order less than or equal to 21 at the point at inﬁnity Q = (0, 1, 0): The
rational function x
z has an order of w = 4, and y
z has an order of w + 1 = 5.
Therefore, the sequence of functions of order up to and including 21 is:
L(21Q)=

1, x
z , y
z , x2
z2 , xy
z2 , y2
z2 , x3
z3 , x2y
z3 , xy2
z3 , y3
z3 , x4
z4 , x3y
z4 , x2y2
z4 , xy3
z4 , y4
z4 , x4y
z5
%
.
All functions with a power of x greater than 4 have been removed to avoid
duplicate orders, so there is no x5
z5 in L(G) as its order is the same as υ( y4
z4 ). The
orders of these functions are:
{1, 4, 5, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21}
and there are 16 functions in L(G). These numbers are also known as nongaps.
Also, the orders 0, 2, 3, 6, 7 and 11 are not present, and these are known as gaps.
The number of gaps is equal to γ .

68
BASIC PRINCIPLES OF NON-BINARY CODES
The degree of the Hermitian curve is m = 5 and, from (2.12), the genus is
γ = 6 and d(G) = 21. Next, 2γ −2 = 10, which is less than d(G), so applying the
Riemman–Roch theorem in (2.11) gives:
l(G) = d(G) + 1 −γ
= 21 + 1 −6
= 16
,
which is equal to the number of functions in L(G).
2.2.6 The Zero Order of a Monomial
We can deﬁne a set Z of monomials ψpi,u(x, y) with a zero of order u at an afﬁne
point Pi as [6]:
ψpi,u(x, y) = ψpi,λ+(w+1)δ(x, y) = (x −xi)λ[(y −yi) −xw
i (x −xi)]δ,
(2.13)
where λ, δ ∈N, 0 ≤λ ≤w and δ ≥0. To evaluate the zero order, ψpi,α is divided
by (x −xi) until a unit has been obtained. The zero order is equal to the number of
divisions.
Example 2.6: Determining the zero order of a monomial at the afﬁne point
(1, α) on the Hermitian curve: The ﬁrst eight monomials with respect to the point
pi = (xi, yi) = (1, α) from (2.13) are determined by:
ψpi,u(x, y) = ψ(1,α),λ+3δ(x, y) = (x −1)λ[(y −a) −12(x −1)]δ,
where w = 2 and 0 ≤λ ≤2.
Therefore:
ψpi,0(x, y) = (x −1)0[(y −α) −12(x −1)] = 1
ψpi,1(x, y) = (x −1)1[(y −α) −12(x −1)]0 = 1 + x
ψpi,2(x, y) = (x −1)2[(y −α) −12(x −1)]0 = 1 + x2
ψpi,3(x, y) = (x −1)0[(y −α) −12(x −1)]1 = y + α + x + 1 = α2 + x + y
ψpi,4(x, y) = (x −1)1[(y −α) −12(x −1)]1 = (x + 1)(y + α + x + 1)
= α2 + αx + y + x2 + xy
ψpi,5(x, y) = (x −1)2[(y −α) −12(x −1)]1 = (x2 + 1)(y + α + x + 1)
= α2 + x + α2x2 + y2 + x2y
ψpi,6(x, y) = (x −1)0[(y −α) −12(x −1)]2 = (y + α + x + 1)2 = α + x2 + y2
ψpi,7(x, y) = (x −1)1[(y −α) −12(x −1)]2 = (x + 1)(y + α + x + 1)2
= α + αx + y + x2 + xy2.

ALGEBRAIC GEOMETRY PRINCIPLES
69
To evaluate a function’s zero order at an afﬁne point of the Hermitian curve xw+1
+ yw + y = 0, it is important to have the following equation associated with the
curve [6]:
y −yi
x −xi
= (x −xi)w + xi(x −xi)w−1 + xw
i
e(y)
,
(2.14)
where e( y) = ( y −yi)w−1 + 1. Notice that e( yi) = ( yi – yi)w−1 + 1 = 1.
It can be seen that ψpi,3(x, y) = (y −α) −(x −1) and e( y) = ( y −α) + 1.
Initialize ψ(0)(x, y) = ψpi,3(x, y) = (y −α) −(x −1).
The ﬁrst division:
ψ(1)(x, y) = ψ(0)(x, y)
x −1
= y −α
x −1 −1 = (x −1)2 + (x −1) + 1
e(y)
−1
= (x −1)2 + (x −1) + 1 −(y −α) −1
e(y)
= (x −1)e(y)−1 + (y −α)e(y)−1 + (x −1)2e(y)−1.
We have ψ(1)( pi) = (1 −1) · 1 + (α −α) · 1 + (1 −1)2 · 1 = 0.
The second division:
ψ(2)(x, y) = ψ(1)(x, y)
x −1
= e(y)−1 −y −α
x −1 e(y)−1 + (x −1)e(y)−1
= e(y)−1 −[(x −1)2 + (x −1) + 1]e(y)−2 + (x −1)e(y)−1
= (e(y)−1 −e(y)−2) −(x −1)(e(y)−2 −e(y)−1) −(x −1)2e(y)−2.
We have ψ(2)( pi) = (1 – 1) – (1 −1) · (1 – 1) – (1 – 1)2 · 1 = 0.
The third division:
ψ(3)(x, y) = ψ(2)(x, y)
x −1
= e(y)−1 −e(y)−2
x −1
−(e(y)−2 −e(y)−1) −(x −1)e(y)−2
=
e(y) −1
(x −1)e(y)2 −(e(y)−2 −e(y)−1) −(x −1)e(y)−2.
= y −α
x −1 e(y)−2 −(e(y)−2 −e(y)−1) −(x −1)e(y)−2
= [(x −1)2 + (x −1) + 1]e(y)−3 −(e(y)−2 −e(y)−1) −(x −1)e(y)−2
= (e(y)−3 −e(y)−2 + e(y)−1) + (x −1)(e(y)−3
−e(y)−2) + (x −1)2e(y)−3.
We have ψ(3)( pi) = (1 – 1 + 1) + (1 −1) · (1 – 1) + (1 −1)2 · 1 = 1 = 0.

70
BASIC PRINCIPLES OF NON-BINARY CODES
There are 3 divisions in order obtain a unit. Therefore, the zero order of ψpi,3 at
pi is 3 as: v pi(ψpi,3) = 3. ψpi,3 can also be written as: ψpi,3 = (x −1)3[(e(y)−3 −
e(y)−2 + e(y)−1) + (x −1)(e(y)−3 −e(y)−2) + (x −1)2e(y)−3].
2.2.7 AG Code Parameters
There are two types of AG code: Functional Goppa codes, CL, and Residue Goppa
codes, C , which are the dual of functional Goppa codes. In both cases the block
length is the number of afﬁne points, n.
For functional Goppa codes the message length is the number of rational functions
in the divisor L( G), as given by the Reimann–Roch theorem [3, 4]:
k = l(G) = d(G) + 1 −γ.
(2.15)
An accurate value of the Hamming distance of AG codes cannot always be calcu-
lated, so a lower bound called the designed minimum distance d
∗is used. The genus of
the curve plays an important role in determining the distance of the code. The optimal
Hamming distance occurs when the Singleton bound is met [1, 2]:
d = n −k + 1.
(2.16)
However, for AG codes the genus of the curve penalizes the minimum distance [4]:
d∗= n −k −γ + 1.
(2.17)
In this case a large genus will reduce the designed minimum distance of the code,
but long codes such as the Hermitian codes have a large genus. Reed–Solomon codes
are constructed from an afﬁne line, which has degree of one and a genus equal to zero.
Therefore they do not suffer any genus penalty and have optimal Hamming distances.
By substituting (2.15) into (2.16) the designed minimum distance of the code is [4]:
d∗= n −(d(G) + 1 −γ ) −γ + 1
= n −d(G)
.
(2.18)
For residue Goppa codes the message length k of the code is simply the code length
minus the message length of the functional Goppa codes, that is [4]:
k = n −l(G) = n −d(G) −1 + γ.
(2.19)
Substituting (2.19) into (2.17) gives the designed minimum distance of the residue
Goppa code [4]:
d∗= n −(n −d(G) −1 + γ ) −γ + 1
= d(G) −2γ + 2
.
(2.20)

REFERENCES
71
2.3 Conclusions
This chapter has presented some of the more important mathematical concepts as-
sociated with non-binary error-correcting codes, explaining groups, rings and ﬁelds,
minimal polynomials and cyclotomic cosets, and introducing algebraic geometry.
These are all important tools for the design of binary and non-binary error-correcting
codes and will be used often in the remaining chapters of this book.
Most of this chapter was dedicated to the principles of algebraic geometry since
the construction and decoding of algebraic–geometric codes is more mathematically
intensive than the other non-binary coding schemes covered in this book.
References
[1] Lin, S. and Costello, D.J. Jr. (2004) Error Control Coding, 2nd edn, Pearson Prentice Hall, ISBN
0-13-017973-6.
[2] Wicker, S.B. (1995) Error Control Systems for Digital Communication and Storage, Prentice Hall,
Eaglewood Cliffs, NJ, ISBN 978-0132008099.
[3] Blake, I.F., Heegard, C., Hoholdt, T. and Wei, V. (1998) Algebraic geometry codes. IEEE Trans.
Inform. Theory, 44 (6), 2596–618.
[4] Pretzel, O. (1998) Codes and Algebraic Curves, Oxford Science Publications, Oxford University
Press (Oxford Lecture Series in Mathematics and its Applications 8), ISBN 0198500394.
[5] Driencourt, Y. (1985) Some properties of elliptic codes over a ﬁeld of characteristic 2. Proceedings
of AAECC-3, 229, 185–93, Lecture Notes in Computer Science.
[6] Høholdt, T. and Nielsen, R.R. (1999) Decoding Hermitian codes with Sudan’s algorithm, in Applied
Algebra, Algebraic Algorithms and Error-Correcting Codes, Vol. 1719, Springer-Verlag, Berlin,
Germany, pp. 260–70 (Lecture Notes in Computer Science).


3
Non-Binary Block Codes
3.1 Introduction
The two previous chapters have covered the necessary mathematics to enable the
reader to understand the theory of error-correcting codes. From now on, this chapter
and succeeding chapters will present the different types and classes of binary and
non-binary error-correcting code, focusing on their design, construction and decoding
algorithms. In this chapter, we ﬁrst introduce binary block codes, beginning with the
Hamming code, one of the ﬁrst block codes presented in 1953 by Hamming [1]. It
will be seen that this coding scheme is not suitable for practical applications, but
these codes are simple enough to demonstrate encoding and decoding block codes.
Following this, a very important type of block code known as the cyclic code is
presented. The cyclic code has improved error-correction and its encoder can be
simply implemented using shift registers. The ﬁrst important class of cyclic code is
the Bose–Chaudhuri–Hocquengem (BCH) code [2, 3]. We describe the construction
of binary BCH codes and then move on to the construction of non-binary BCH codes
deﬁned over ﬁnite ﬁelds. This will lead to the most important and commonly used class
of non-binary BCH code, called the Reed–Solomon code [4]. Two different decoding
algorithms are given to decode non-binary BCH codes: Euclid’s algorithm [5] and
the Berlekamp–Massey algorithm [6, 7].
We then explain the concept of coded modulation presented by Ungerboeck [8],
whereby the encoding and modulation processes are treated as a single entity and the
constellation set is increased to compensate for the redundancy in a code word, thus
keeping the data rate constant without requiring the bandwidth use to be expanded.
Block Coded Modulation (BCM) employs simple block codes and maps the symbols
of the code word to an expanded constellation set, while at the same time maximizing
the Euclidean distance between adjacent symbols. Binary BCM codes are introduced,
covering their encoding procedure and their decoding by constructing the trellis
diagram of the BCM codes and using the well-known Viterbi algorithm [9] to recover
the original message. This work is then extended to non-binary BCM codes and we
Non-Binary Error Control Coding for Wireless Communication and Data Storage
Rolando Antonio Carrasco and Martin Johnston
C 2008 John Wiley & Sons, Ltd

74
NON-BINARY BLOCK CODES
demonstrate that the performance can be improved with only a small increase in
complexity.
The chapter ﬁnishes by discussing the applications of non-binary block codes
related to wireless communications, such as a cellular environment and ﬁxed and
wireless broadband access (WiMax), and to magnetic and optical data storage.
3.2 Fundamentals of Block Codes
In any error-correcting coding scheme a number of redundant bits must be included in
the original message in order for it to be possible to recover that message at the receiver.
In general, these redundant bits are determined by algebraic methods. All block codes
are deﬁned by their code word length n, their message length (or dimension) k and
their minimum Hamming distance d (described later). When referring to a block code,
it is written as a (n, k, d) block code.
One of the simplest types of block code is the parity check code, where for each
binary message an extra bit, known as a parity check bit, is appended to the message
to ensure that each code word has an even number of 1s. In this case the message is
k bits in length and therefore the code word length n = k + 1. For example, if k = 3
then the code word length is n = 4 and the code is:
000 →0000
001 →0011
010 →0101
011 →0110.
100 →1001
101 →1010
110 →1100
111 →1111
It can be seen that if a single error is added to any of the code words it will result
in an odd number of 1s in the received word and will be detected. Unfortunately, the
code will not know the location of the error in the received word. Hence, parity check
codes can only detect one error and can correct none. To obtain the parity check bit,
one simply adds the three message bits modulo-2. If the message bits are denoted as
u1, u2 and u3 and the parity check bit is denoted as p1 then the parity check equation
for the (4, 3) parity check code is:
p1 = u1 ⊕u2 ⊕u3.
(3.1)
A simple encoder for this is illustrated in Figure 3.1.
The number of errors that a block code can detect and correct is determined by its
minimum Hamming distance d. This is deﬁned as the minimum number of places
where any two code words differ. For example, by observation the minimum Hamming

FUNDAMENTALS OF BLOCK CODES
75
u1
u2
u3
u1
u2
u3
p1
Figure 3.1
A simple encoder for the (4, 3) parity check code.
distance of the above parity check code is d = 2. Hence, if any two bits of a code word
are ﬂipped this will result in another valid code word, that is if two errors occurred
they would not be detected. In general, the number of errors v that can be detected for
a block code is:
v = d −1.
(3.2)
The error detection and correction of a block code can be improved by increasing
the number of parity check bits or equivalently increasing the number of parity check
equations. For example, take the following three parity check equations:
p1 = u1 ⊕u3 ⊕u4
p2 = u1 ⊕u2 ⊕u3.
p3 = u2 ⊕u3 ⊕u4
(3.3)
Each parity check bit in (3.3) is independent in order to ensure the minimum
Hamming distance is maximized. The corresponding encoder is given in Figure 3.2.
This is one of the ﬁrst block codes known as Hamming codes. These have the following
properties:
Codeword length n = 2m −1
Message length k = 2m −m −1
Minimum Hamming distance d = 3,
where m ≥3 and is a positive integer.
u1
u2
u3
u4
u1
u2
u3
u4
p1
p2
p3
Figure 3.2
Encoder for the parity check equations of (3.3).

76
NON-BINARY BLOCK CODES
A list of the code words for the (7, 4, 3) Hamming code is shown below, with the
parity bits highlighted:
0000 →0000000
0001 →0001101
0010 →0010111
0011 →0011010
0100 →0100011
0101 →0101110
0110 →0110100
0111 →0111001
1000 →1000110
1001 →1001011
1010 →1010001.
1011 →1011100
1100 →1100101
1101 →1101000
1110 →1110010
1111 →1111111
From (3.2), the Hamming code can detect two errors, improving on the parity check
code. However, we do not know how many errors this code can correct. We know that
changing a code word in d positions can result in another valid code word. Each code
word can be interpreted as the centre of a circle of a certain radius t that contains all
other vectors that differ from the code word in t places or less. Therefore, if t = d
then two neighbouring code words will overlap, as shown in Figure 3.3. If t = d/2
then the two circles will meet at their circumferences. This implies that for t < d/2 the
circle will contain unique n-dimensional vectors that do not occur in any other circle.
Hence, if the received word matches any of these vectors we know that it can only be
one code word. So the number of errors t that a block code can correct up to is:
t <
&d
2
'
or
t ≤
&d −1
2
'
,
(3.4)
where  is the ﬂoor function and any value inside is rounded down to the nearest
integer. This is important for the situation where d is even and the number of errors
that can be corrected would not be an integer. Since the Hamming code has a minimum
Hamming distance d = 3 it can correct one error.

FUNDAMENTALS OF BLOCK CODES
77
Figure 3.3
Representation of two code words that differ in d places. Any vector that differs from a
code word in less than d/2 places is unique and can therefore be corrected.
3.2.1 Generator and Parity Check Matrices
The encoders of Figures 3.1 and 3.2 can be represented as a (k × n) matrix for
convenience. Each column in the matrix corresponds to the connections from the
message and the code word, that is a connection is denoted by a 1 and no connection
by a 0. For example, the matrix for the parity check code is written as:
H =


1
0
0
1
0
1
0
1
0
0
1
1

.
Taking the ﬁrst column, there is only one connection in Figure 3.1 between the ﬁrst
element of the code vector and the ﬁrst element of the message vector, and this is
denoted as 1. There are no connections between the ﬁrst elements of the code vector
and the second and third elements in the message vector so these are both denoted as 0.
Similarly, for the last column there are connections between the ﬁnal element of the
code vector and all three elements of the message vector so this column contains all
1s. Multiplying a 3-bit message by this matrix will generate a code word, so the matrix
is called a generator matrix, denoted as G. The generator matrix of the Hamming
code encoder in Figure 3.2 is:
G =


1
0
0
0
1
1
0
0
1
0
0
0
1
1
0
0
1
0
1
1
1
0
0
0
1
1
0
1

.
(3.5)
It can be seen from (3.5) that every code word can be obtained by adding
different combinations of the rows in G. For example, encoding the message

78
NON-BINARY BLOCK CODES
u = [1 1 1 1] gives:
c = u · G = 1 ·
	1 0 0 0 1 1 0
⊕1 ·
	0 1 0 0 0 1 1
⊕1 ·
	0 0 1 0 1 1 1
⊕
1 ·
	0 0 0 1 1 0 1
=
	1 1 1 1 1 1 1
.
A second important matrix in error-correction coding is the parity check matrix,
denoted by H. It has the property that any row in H is orthogonal to the rows of G,
that is the inner product of a row in G and a row in H will be zero. This is important
as it allows us to validate any code word in a block code since multiplying a code
word by H should give an all-zero vector, indicating that the code word is valid, as
given by [10]:
H · cT = 0,
(3.6)
where cT is the transpose of a code word and 0 is the (n – k) × 1 column vector
containing all zeros. It can be seen that G is of the form:
G = [Ik|P],
(3.7)
where Ik is the k × k identity matrix and P is a parity matrix. This is called a systematic
generator matrix and generates a code word which contains the original message with
parity bits appended to it. The (n – k) × n systematic parity check matrix is related to
G by:
H =
	
PT|In−k

,
(3.8)
where PT is the transpose of P and In−k is the (n – k) × (n – k) identity matrix. The parity
check matrix of the (4, 3, 2) parity check code is, by (3.8), simply H =
	1 1 1 1
.
Similarly, the parity check matrix for the (7, 4, 3) Hamming code is:
H =


1
0
1
1
1
0
0
1
1
1
0
0
1
0
0
1
1
1
0
0
1

.
(3.9)
As an example, multiplying H by the transpose of the code word c =
	1 0 0 0 1 1 0
results in:
H · cT =


1
0
1
1
1
0
0
1
1
1
0
0
1
0
0
1
1
1
0
0
1

.


1
0
0
0
1
1
0


=


0
0
0

.

FUNDAMENTALS OF BLOCK CODES
79
3.2.2 Decoding Block Codes
As well as validating code words, the parity check matrix can also be used to decode
a received word containing errors. It has been seen that multiplying the parity check
matrix by any code word results in an all-zero column vector. Therefore, if the parity
check matrix is multiplied by a received word containing errors, the column vector
will contain nonzero elements. These values are known as syndromes and the column
vector is a syndrome vector, s [10].
s = H · rT,
(3.10)
where r is the received vector deﬁned as r = c + e and e is an error vector. Therefore,
(3.10) can be written as:
s = H · (c + e)T = H · cT + H · eT = H · eT.
(3.11)
We can see that the syndromes are only dependent on the error pattern. The
(7, 4, 3) Hamming code can correct a single error and so there are seven possible
error patterns:
	1 0 0 0 0 0 0
,
	0 1 0 0 0 0 0
,
	0 0 1 0 0 0 0
,
	0 0 0 1 0 0 0
,
	0 0 0 0 1 0 0
,
	0 0 0 0 0 1 0
and
	0 0 0 0 0 0 1
.
Multiplying H by the transpose of each of these error patterns gives the seven
unique syndrome vectors.
Now suppose the code word c =
	1 0 0 0 1 1 0
has been transmitted and the
received vector r =
	1 1 0 0 1 1 0
has a single error in its second position as high-
lighted. From (3.10) the syndrome vector is:


1 0 1 1 1 0 0
1 1 1 0 0 1 0
0 1 1 1 0 0 1

.


1
1
0
0
1
1
0


=


0
1
1

.
Looking up this syndrome vector in Table 3.1 shows that the associated error pattern
is e =
	0 1 0 0 0 0 0
. Therefore, adding this error pattern to the received word gives
the decoded code word ˆc:
1100110
⊕0100000 .
1000110

80
NON-BINARY BLOCK CODES
Table 3.1
Error patterns and their associated syndrome
vectors for the (7, 4, 3) Hamming code.
Error pattern e
Syndrome vector s
1 0 0 0 0 0 0
1 1 0
0 1 0 0 0 0 0
0 1 1
0 0 1 0 0 0 0
1 1 1
0 0 0 1 0 0 0
1 0 1
0 0 0 0 1 0 0
1 0 0
0 0 0 0 0 1 0
0 1 0
0 0 0 0 0 0 1
0 0 1
However, for block codes that can correct more than one error this method becomes
prohibitive.
3.3 Cyclic Codes
A cyclic code is a block code which has the property that for any code word, a
cyclic shift of this code word results in another code word [11]. Cyclic codes have
the advantage that simple encoders can be constructed using shift registers, and low-
complexity decoding algorithms exist to decode them. A cyclic code is constructed
by ﬁrst choosing a generator polynomial g(x) and multiplying this by a message
polynomial m(x) to generate a code word polynomial c(x).
3.3.1 Polynomials
There are two conventions for representing polynomials. A polynomial f(x) can be
of the form f 0 + f 1x + f 2x2 + · · · fnxn, where fi ∈GF(q) and i = 0, 1, . . . , n. In
this case the corresponding vector would be [f 0 f 1 f 2 . . . fn]. Alternatively, f(x) can
be of the form fnxn + fn−1xn−1 + · · · + f0 and the corresponding vector is [fn fn−1
fn−2 . . . f 0]. Both conventions are acceptable but in this book the latter representation
is chosen. So for the vector [1 1 0 1] the polynomial would be x3 + x2 + 1.
The (7, 4, 3) Hamming code is actually also a cyclic code and can be constructed
using the generator polynomial g(x) = x3 + x2 + 1.
For example, to encode the binary message 1010 we ﬁrst write it as the message
polynomial m(x) = x3 + x and then multiply it with g(x):
c(x) = m(x)g(x)
= (x3 + x)(x3 + x2 + 1)
= x6 + x5 + x3 + x4 + x3 + x.
= x6 + x5 + x4 + x
This code word polynomial corresponds to 1 1 1 0 0 1 0. The complete list of code
words is shown in Table 3.2.

CYCLIC CODES
81
Table 3.2
Code words for the (7, 4, 3) cyclic Hamming code.
Binary
Message
Code word
Binary
message
polynomial m(x)
polynomial c(x)
code word
0000
0
0
0000000
0001
1
x3 + x2 + 1
0001101
0010
x
x4 + x3 + x
0011010
0011
x + 1
x4 + x2 + x + 1
0010111
0100
x2
x5 + x4 + x2
0110100
0101
x2 + 1
x5 + x4 + x3 + 1
0111001
0110
x2 + x
x5 + x3 + x2 + x
0101110
0111
x2 + x + 1
x5 + x + 1
0100011
1000
x3
x6 + x5 + x3
1101000
1001
x3 + 1
x6 + x5 + x2 + 1
1100101
1010
x3 + x
x6 + x5 + x4 + x
1110010
1011
x3 + x + 1
x6 + x5 + x4 + x3 + x2 + x + 1
1111111
1100
x3 + x2
x6 + x4 + x3 + x2
1011100
1101
x3 + x2 + 1
x6 + x4 + 1
1010001
1110
x3 + x2 + x
x6 + x2 + x
1000110
1111
x3 + x2 + x + 1
x6 + x3 + x + 1
1001011
Taking a code word from Table 3.2 and shifting each element results in another
code word. For example, the code word 0110100 shifted one place to the left gives
the vector 1101000, which is another valid code word in Table 3.2.
Multiplying a message polynomial by the generator polynomial is equivalent to
multiplying the binary message by a generator matrix G where each row contains the
coefﬁcients of g(x) shifted one place to the right with respect to the previous row.
For the (7, 4) cyclic Hamming code the generator matrix would be:
G =


1
1
0
1
0
0
0
0
1
1
0
1
0
0
0
0
1
1
0
1
0
0
0
0
1
1
0
1

.
Multiplying the message m =
	1 0 1 0
by G gives the code word:
	1 0 1 0


1
1
0
1
0
0
0
0
1
1
0
1
0
0
0
0
1
1
0
1
0
0
0
0
1
1
0
1

=
	1 1 1 0 0 1 0
,
which matches the code word in Table 3.2.
It can be seen that the code words of the cyclic code are nonsystematic and so
recovering messages is more difﬁcult. One method would be to divide the code word
polynomial by the generator polynomial to obtain the message polynomial, but this

82
NON-BINARY BLOCK CODES
is an unfeasible method for larger cyclic codes. However, it is desirable for the code
word to be in systematic form so that it is simpler to recover the transmitted message.
3.3.2 Systematic Cyclic Codes
A systematic cyclic code word is made up of two polynomials: a message polynomial
m(x) and a parity polynomial p(x), where the degree of p(x) is less than (n −k). The
message polynomial is of the form:
m(x) = mkxk + mk−1xk−1 + · · · + m1x + m0,
where k is the message length. It is ﬁrst multiplied by xn−k, resulting in:
xn−km(x) = mnxn + mn−1xn−1 + · · · + mn−k+1xn−k+1 + mn−kxn−k.
It is then divided by the generator polynomial g(x), giving [10]:
xn−km(x)
g(x)
= q(x)
   +
quotient
+ p(x)
g(x)
   +
remainder
⇒xn−km(x) + p(x)
g(x)
= q(x)
,
(3.12)
where q(x) is a quotient polynomial. From (3.12) it can be seen that the term
xn−km(x) + p(x) is a valid code word polynomial since it is a factor of g(x).
Example 3.1:
Generating a systematic code word using the (7, 4, 3) cyclic
Hamming code: First multiply m(x) by xn−k = x3:
x3m(x) = x6 + x4.
Now divide this by the generator polynomial g(x) = x3 + x2 + 1:
x3 + x2 + 1
x3 + x2 + 1

x6
+ x4
x6 + x5
+ x3
x5 + x4 + x3
x5 + x4
+ x2
x3 + x2
x3 + x2 + 1
1

CYCLIC CODES
83
Switch 2
Input message symbol sequence
Switch 1
g0
g1
g2
g3
gn–k–2
gn–k–1
xn–k
xn–k–1
xn–k–2
x3
x2
x
x0
Figure 3.4
A general systematic cyclic code encoder.
In this case the quotient is q(x) = x3 + x2 + 1 and the remainder of parity
polynomial is p(x) = 1. Hence the systematic code word polynomial c(x) = m(x) +
p(x) = x6 + x4 + x2 + 1 and the code word is c =
	1 0 1 0 0 0 1
.
This operation can be achieved using an (n – k)-stage shift register, as shown
in Figure 3.4. Initially, the shift register is initialized to zero and each one of the
k input bits is fed into the (n – k)-stage shift register. At the same time switch
2 ensures that the output also has those k input bits. After the ﬁnal bit is fed in,
switch 1 is open and switch 2 moves to its second position, emptying the contents
of the shift registers, which are the redundant parity bits. An example of the (7, 4, 3)
cyclic Hamming code systematic encoder is shown in Figure 3.5. This is a (n – k) =
7 – 4 = 3-stage shift register in which each delay element corresponds to a power
of x in the parity polynomial p(x).
Switch 1
Switch 2
x
x2
1
0
1
x3
Feedback
Input message symbol sequence
1
1010
Figure 3.5
The systematic encoder for the (7, 4, 3) cyclic Hamming code.

84
NON-BINARY BLOCK CODES
Table 3.3
Contents of the shift register for each cycle in the
systematic encoder.
Shift register contents
Input
Feedback
1
x
x2
1
1
1
0
1
0
1
1
1
1
1
0
0
1
1
0
1
1
0
0
If the input message is m = [1 0 1 0] then Table 3.3 shows the contents of the
shift register at each cycle in the systematic encoder.
The contents of the shift register for the ﬁnal information bit show that the
parity polynomial is p(x) = 0x2 + 0x + 1 = 1 and the systematic code word is
c =
	1 0 1 0 0 0 1
.
3.4 Bose–Chaudhuri–Hocquenghem (BCH) Codes
One of the most important classes of cyclic code is the Bose–Chaudhuri–
Hocquenghem (BCH) code [2, 3]. When constructing a cyclic code the actual value
of the minimum Hamming distance is unknown, but BCH codes have a lower bound
on the minimum Hamming distance, called the designed minimum distance, denoted
as d
∗. This is due to the BCH bound, deﬁned as follows [12].
The BCH Bound: For any q-ary (n, k) cyclic code, choose a value of m that
ensures the extension ﬁeld GF(qm) is minimal and contains a primitive nth root of
unity α. Choose a generator polynomial g(x) of minimal degree in GF(q)x so that
g(α) = g(α2) = g(α4) = · · · = g(αd∗−2).
From the above, the cyclic code has minimum distance d ≥d
∗[12].
The value of d
∗is determined by deciding how many errors the BCH code should
be able to correct. This can be achieved by rearranging (3.4) to get:
d∗= 2t + 1.
(3.13)
Example 3.2:
Constructing a binary BCH code of length n = 15 bits: The
BCH bound tells us that we must choose the smallest extension ﬁeld GF(2m) with
an nth root of unity, which in this case is a 15th root of unity. The smallest extension
ﬁeld that satisﬁes this is GF(16). Next, we must decide how many errors the BCH
code will be able to correct; for this example it can correct t = 2 errors, giving a
designed minimum distance of d
∗= 5. Finally, we need a generator polynomial g(x)
where g(α) = g(α2) = g(α3) = g(α4) = 0. To obtain this the minimal polynomials

BOSE–CHAUDHURI–HOCQUENGHEM (BCH) CODES
85
of GF(16) are required, which are given in Table 2.11, but are repeated here for
convenience:
Cyclotomic Cosets
Minimal Polynomials M(x)
{1, 2, 4, 8}
M1(x) = x4 + x + 1
{3, 6, 12, 9}
M3(x) = x4 + x3 + x2 + x + 1
{5, 10}
M5(x) = x2 + x + 1
(7, 14, 13, 11}
M7(x) = x4 + x3 + 1
The minimal polynomial for α and α2 is M1(x), the minimal polynomial for α3
is M3(x), and for α4 it is also M1(x), so to satisfy the BCH bound the generator
polynomial must be g(x) = M1(x)M3(x).
g(x) = (x4 + x + 1)(x4 + x3 + x2 + x + 1)
= x8 + x7 + x6+(1+1)x5+(1+1+1)x4 + (1+1)x3 + (1+1)x2 + (1+1)x+1.
= x8 + x7 + x6 + x4 + 1
This means that the message length of the code is k = n – deg(g(x)) = 15 –
8 = 7, resulting in the (15, 7, 5) binary BCH code.
The design procedure for constructing non-binary BCH codes is the same as for
the binary case.
Example 3.3:
Constructing a non-binary BCH code over GF(4) of length
n = 15 symbols: In this case we must now ﬁnd the smallest extension ﬁeld GF(4m)
with a 15th root of unity. Again this is the extension ﬁeld GF(16) with m = 2. The
elements {0, 1, α5, α10} in GF(16) form a subﬁeld of GF(4), so let β = α5 be a
primitive element in GF(4). The cyclotomic cosets and minimal polynomials of
GF(42) are given in Table 3.4.
Table 3.4
Cyclotomic cosets and minimal polynomials for GF(42).
Cyclotomic cosets
Minimal polynomials M(x)
{1, 4}
M1(x) = x2 + x + β
{2, 8}
M2(x) = x2 + x + β2
{3, 12}
M3(x) = x2 + β2x + 1
{5}
M5(x) = x + β
{6, 9}
M6(x) = x2 + βx + 1
{7, 13}
M7(x) = x2 + βx + β
{10}
M10(x) = x + β2
{11, 14}
M11(x) = x2 + β2x + β2

86
NON-BINARY BLOCK CODES
To obtain Table 3.4, it must ﬁrst be observed that the conjugates of αi in GF(42)
are α4i, i = 1, 2, . . . , that is, the conjugates of α are α4, α16, α64, . . . , but α16 =
α and α64 = α4 so the cyclotomic coset would be{1, 4}. The associate minimal
polynomial is:
M1(x) = (x + α)(x + α4) = x2 + (α + α4)x + α5
= x2 + x + β.
To correct two errors, the generator polynomial will be g(x) = M1(x)M2(x)M3(x):
g(x) = (x2 + x + β)(x2 + x + β2)(x2 + β2x + 1)
= [x4 + (1 + 1)x3 + (β2 + 1 + β)x2 + (β2 + β)x + 1](x2 + β2x + 1)
= (x4 + x + 1)(x2 + β2x + 1)
.
= x6 + β2x5 + x4 + x3 + (β2 + 1)x2 + (β2 + 1)x + 1
= x6 + β2x5 + x4 + x3 + βx2 + βx + 1.
There are two important features of this non-binary BCH code. Firstly, the
message length of this code is k = n – deg(g(x)) = 15 – 6 = 9, which means there
is less redundancy in the code than in the binary BCH code in Example 3.2 (the
binary BCH code has code rate of 0.47, compared with the 4-ary BCH code which
has a code rate of 0.6). Secondly, although both the (15, 7, 5) binary BCH code and
the (15, 9, 5) 4-ary BCH code correct up to two errors it should be noted that the
4-ary BCH code corrects two symbol errors. In GF(4) each element is represented
by two bits, which implies that it can correct up to 4 bit errors, provided the bit
errors span two symbols, whereas the binary BCH code can only correct up to 2
bit errors. Of course it must be remembered that the actual minimum Hamming
distance of these codes could be greater than the designed minimum distance and
so they may be able to correct more than two errors.
From these two examples, we now begin to see the advantages of using non-binary
block codes.
3.5 Reed–Solomon Codes
A BCH code deﬁned over GF(q) of length n can always be constructed provided
that there is an nth root of unity in some extension ﬁeld GF(qm). A special case
occurs if we construct a BCH code of length n = qm −1 over GF(qm). Obviously,
GF(qm) is the smallest extension ﬁeld with a (qm – 1)th root of unity. In this case,
each cyclotomic coset will contain only one element. For example, the conjugates of
the primitive element α will be αqm, α2qm, . . . , which is equal to α. Therefore, each
minimal polynomial of αi will be (x −αi), i = 1, 2, 3. . ., qm −2.

REED–SOLOMON CODES
87
Example 3.4:
Constructing a non-binary BCH code over GF(16) of length
n = 15 symbols: The 15th root of unity in GF(16) is the primitive element α
and the cyclotomic cosets and their associated minimal polynomials are given in
Table 3.5.
Table 3.5
Cyclotomic cosets and minimal polynomials for GF(16).
Cyclotomic cosets
Minimal polynomials M(x)
{1}
M1(x) = x + α
{2}
M2(x) = x + α2
{3}
M3(x) = x + α3
{4}
M4(x) = x + α4
{5}
M5(x) = x + α5
{6}
M6(x) = x + α6
{7}
M7(x) = x + α7
{8}
M8(x) = x + α8
{9}
M9(x) = x + α9
{10}
M10(x) = x + α10
{11}
M11(x) = x + α11
{12}
M12(x) = x + α12
{13}
M13(x) = x + α13
{14}
M14(x) = x + α14
To correct up to t = 2 symbol errors the generator matrix will be g(x) =
M1(x)M2(x)M3(x)M4(x).
g(x) = (x + α)(x + α2)(x + α3)(x + α4)
= (x2 + α5x + α3)(x2 + α7x + α7)
= x4 + (α7 + α5)x3 + (α7 + α12 + α3)x2 + (α12 + α10)x + α10
= x4 + α13x3 + α6x2 + α3x + α10.
The message length of the code is k = n – deg(g(x)) = 15 – 4 = 11, resulting in
a (15, 11, 5) 16-ary BCH code. Once again it can be seen that the code rate of this
code is even higher than the 4-ary BCH code of Example 3.3, and it can correct
two symbol errors, where one symbol now represents 4 bits. This means that it can
correct up to 8 bit errors, provided they span two symbols.
Non-binary BCH codes deﬁned over GF(qm) of length n = qm – 1 are called
Reed–Solomon codes [4] and are the best-performing BCH codes. Hence, the
non-binary BCH code of Example 3.4 is the (15, 11, 5) Reed–Solomon code.
Reed–Solomon codes also differ from other BCH codes in that their minimum
Hamming distance is equal to the designed minimum distance.

88
NON-BINARY BLOCK CODES
An important upper bound on the minimum Hamming distance in error-correction
is the Singleton Bound, deﬁned as [10]:
d ≤n −k + 1.
(3.14)
The Reed–Solomon code of Example 3.4 meets this bound, and this is the case
for all Reed–Solomon codes. Any code with a minimum Hamming distance d = n –
k + 1 is known as maximum distance separable (MDS) and has optimal minimum
Hamming distance. Therefore, Reed–Solomon codes can correct:
t =
&d −1
2
'
=
&n −k + 1 −1
2
'
=
&n −k
2
'
.
(3.15)
Rearranging (3.15) gives a message length of k = n – 2t. In summary,
Reed–Solomon codes deﬁned over GF(qm) have the following parameters [12]:
Codeword length n = qm −1
Message length k = n −2t
Minimum Hamming distance d = n −k + 1,
and the generator matrix g(x) is of the form [12]:
g (x) =
2t
,
i=1
(x + αi).
(3.16)
The parity check matrix of a Reed–Solomon code is [12]:
H =


1
α
α2
· · ·
αqm−2
1
α2
α4
· · ·
αqm−3
1
α3
α6
· · ·
αqm−4
...
...
...
...
...
1
α2t
α4t
· · ·
αqm−2t−1


.
(3.17)
To obtain the 2t syndromes the parity check matrix is multiplied by the transpose of
the received vector, as in (3.10). This is equivalent to substituting successive powers
of α into the received vector.
3.6 Decoding Reed–Solomon Codes
In Section 3.2.2 it was shown how syndromes could be determined from the received
word and used to identify the correct error pattern and obtain the originally transmitted
code word. This method becomes too complex for larger codes and alternative de-
coding algorithms are needed. To decode binary codes only the locations of the errors

DECODING REED–SOLOMON CODES
89
in the received word are required, since the value at these location can be ﬂipped,
that is a ‘1’ becomes a ‘0’ and vice versa. However, for a non-binary code an error
value can be many different values and a secondary process is needed to evaluate the
error value. So, two algorithms are required to decode a non-binary block code: an
error-locating algorithm and an error-evaluation algorithm.
For decoding we will make use of two polynomials: an error-locating polynomial
(x) and an error-magnitude polynomial (x). These two polynomials are related to
each other by the key equation, given as [12]:
(x)[1 + S(x)] ≡(x) mod x2t+1,
(3.18)
where S(x) is the syndrome polynomial.
3.6.1 Euclid’s Algorithm
Euclid’s algorithm [5] is used to determine the greatest common divisor (GCD)
between two elements a and b, with a > b. If a and b are elements then a is divided by
b to obtain a remainder r. If the remainder is zero then GCD(a, b) = b, otherwise we
let a = b, b = r and repeat the division to obtain another remainder. This is continued
until the remainder r = 0. Euclid’s algorithm can be extended, by using the quotient q
from the division process, to also determine two further elements, u and v, that satisfy
the relationship:
ua + vb = gcd(a, b).
(3.19)
3.6.1.1 Euclid’s Extended Algorithm
1. Initialization: r(−1) = a, r(0) = b, u(−1) = 1, u(0) = 0, v(−1) = 0, v(0) = 1.
2. k = 1.
3. r(k) = r(k −2)/r(k −1). Store the quotient q(k) from this division.
4. If r(k) = 0 terminate the algorithm, otherwise u(k) = u(k −2) −q(k)u(k −1) and v(k) =
v(k −2) −q(k)v(k −1).
5. k = k + 1.
6. Go to step 3.
As an example, take the elements a = 121 and b = 33 and determine their greater
common divisor. The values for each iteration are given in Table 3.6.
It can be seen that each row in Table 3.6 satisﬁes (3.19).
We can also use Euclid’s extended algorithm to decode Reed–Solomon codes. The
key equation of (3.17) and (3.18) can be rewritten as:
(x)x2t+1 + (x)[1 + S(x)] = (x),
(3.20)

90
NON-BINARY BLOCK CODES
Table 3.6
Euclid’s algorithm to determine GCD(121, 33).
k
r(k)
q(k)
u(k)
v(k)
−1
121
—
1
0
0
33
—
0
1
1
22
3
1
−3
2
11
1
−1
4
3
0
2
3
−11
where (x) is a polynomial. It is now in the form of (3.19), with u = (x), a = x2t+1,
v = (x), b = 1 + S(x) and GCD(x2t+1, 1 + S(x)) = (x).
3.6.1.2 Euclid’s Algorithm to decode Reed–Solomon Codes [5, 12]
1. Initialization: (−1)(x) = x2t + 1, (0) = 1 + S(x), (−1)(x) = 1, (0)(x) = 0, (−1)
(x) = 0, (0)(x) = 1.
2. k = 1.
3. Find the remainder (k)(x) from the division of (k −2) by (k −1) and store the
quotient q(k).
4. If (k)(x) = 0 terminate the algorithm otherwise (k)(x) = (k −2)(x) −q(k)
(k −1)(x) and (k)(x) = (k-2)(x) −q(k) (k-1)(x).
5. k = k + 1.
6. Go to step 3.
The error locations are then determined by ﬁrst ﬁnding the roots of the error-locating
polynomial. The locations are the inverses of these roots.
Example 3.5:
Determining (x) and (x) for the (7, 3, 5) Reed–Solomon
code using Euclid’s algorithm: The generator polynomial for the (7, 3, 5)
Reed–Solomon code deﬁned over GF(8) is g(x) = x4 + α3x3 + x2 + αx + α3. Let
the message polynomial be m(x) = x2 + αx + α2; the code word polynomial is
c(x) = x6 + x5 + α3x4 + α5x3 + α3x2 + α6x + α5. At the receiver it is assumed
that two errors have occurred in the received word polynomial r(x), in the third and
sixth positions, so that r(x) = x6 + x5 + αx4 + α5x3 + α3x2 + α2x + α5.
The 2t = 4 syndromes are found by substituting α, α2, α3 and α4 into r(x) as
follows:
S1 = r(α) = α6 + α5 + α · α4 + α5 · α3 + α3 · α2 + α2 · α + α5
= α6 + α5 + α5 + α + α5 + α3 + α5
= α6 + α + α3
= (α2 + 1) + α + (α + 1)
= α2

DECODING REED–SOLOMON CODES
91
S2 = r(α2) = α12 + α10 + α · α8 + α5 · α6 + α3 · α4 + α2 · α2 + α5
= α5 + α3 + α2 + α4 + 1 + α4 + α5
= α3 + α2 + 1
= (α + 1) + α2 + 1
= α4
S3 = r(α3) = α18 + α15 + α · α12 + α5 · α9 + α3 · α6 + α2 · α3 + α5
= α4 + α + α6 + 1 + α2 + α5 + α5
= α4 + α + α6 + 1 + α2
= (α2 + α) + α + (α2 + 1) + 1 + α2
= α2
S4 = r(α4) = α24 + α20 + α · α16 + α5 · α12 + α3 · α8 + α2 · α4 + α5
= α3 + α6 + α3 + α3 + α4 + α6 + α5
= α3 + α4 + α5
.
= (α + 1) + (α2 + α) + (α2 + α + 1)
= α
Therefore, 1 + S(x) = 1 + α2x + α4x2 + α2x3 + αx4.
Euclid’s algorithm is initialized as follows:
(−1)(x) = x5, (0)(x) = 1 + α2x + α4x2 + α2x3 + αx4, (−1)(x) = 0,
(0)(x) = 1, (−1)(x) = 1, (0)(x) = 0.
We must now divide (−1)(x) by (0)(x) to determine the remainder (1)(x):
α6x + 1
αx4 + α2x3 + α4x2 + α2x + 1

x5
x5 + αx4 + α3x3 + αx2 + α6x
αx4 + α3x3 + αx2 + α6x
αx4 + α2x3 + α4x2 + α2x + 1
α5x3 + α2x2 + x + 1
So, (1)(x) = α5x3 + α2x2 + x + 1 and the quotient q(1) = α6x + 1. Hence:
(1)(x) = (−1)(x) + q(1)(0)(x) = 1 + (α6x + 1) · 0 = 1
and
(1)(x) = (−1)(x) + q(1)(0)(x) = 0 + (α6x + 1) · 1 = α6x + 1.

92
NON-BINARY BLOCK CODES
For the second iteration we must now divide (0)(x) by (1)(x) to determine the
remainder (2)(x) and q(2):
α3x + α5
α5x3 + α2x2 + x + 1

αx4 + α2x3 + α4x2 + α2x + 1
αx4 + α5x3 + α3x2 + α3x
α3x3 + α6x2 + α5x + 1 .
α3x3 + x2 + α5x + α5
α2x2 + α4
So (2)(x) = α2x2 + α4 and the quotient q(2) = α3x + α5. Hence (2)(x) =
(0)(x) + q(2) (1)(x) = 0 + (α3x + α5).1 = α3x + α5 and (2)(x) = (0)(x) +
q(2) (1)(x) = 1 + (α3x + α5)(α6x + 1) = α2x2 + α6x + α4. The algorithm is
now terminated, since the iteration step k = t. The example is summarized in
Table 3.7.
Table 3.7
Euclid’s algorithm for Example 3.6.
k
(k)(x)
q(k)
(k)(x)
(k)(x)
−1
x5
—
1
0
0
αx4 + α2x3 + α4x2 + α2x + 1
—
0
1
1
α5x3 + α2x2 + x + 1
α6x + 1
1
α6x + 1
2
α2x2 + α4
α3x + α5
α3x + α5
α2x2 + α6x + α4
The error-locating polynomial is therefore (x) = (2)(x) = α2x2 + α6x + α4
and the error-magnitude polynomial is (x) = (2)(x) = α5x3 + α2x2 + x + 1.
The error-locating polynomial can be factorized to (x) = (αx + α4)(αx + 1) and
the two roots are x1 = α3 and x2 = α6. The error locations are the inverses of these
roots and are X1 = α4 and X2 = α, corresponding to the x4 and x terms in the
received word. It can be seen that generated polynomials from Euclid’s algorithm
satisfy (3.20).
(x)x5 + (x)(1 + S(x)) = (α3x + α5)x5 + (α2x2 + α6x + α4)
× (1 + α2x + α4x2 + α2x3 + αx4)
= α3x6 + α5x5 + α3x6 + (α4 + 1)x5
+ (α6 + α + α5)x4 + (α4+α3+α6)x3
+ (α2 + α + α)x2 + (α6 + α6)x + α4
= α2x2 + α4 = (x).
Since this is equivalent to the key equation of (3.19) we actually do not need to
generate (x) to be able to obtain the error-locating and error-magnitude polyno-
mials.

DECODING REED–SOLOMON CODES
93
3.6.2 Berlekamp–Massey’s Algorithm
Berlekamp–Massey’s algorithm [6, 7] is more difﬁcult to understand than Euclid’s
algorithm but has a more efﬁcient implementation.
3.6.2.1 Berlekamp–Massey’s Algorithm
1. Initialization: k = 0, (0)(x) = 1, L = 0, T(x) = x.
2. k = k + 1.
3. Compute the discrepancy (k) = Sk −
L
i=1
(k−1)
i
Sk−i.
If (k) = 0, go to step 8.
4. Modify the connection polynomial: (k)(x) = (k−1)(x) −(k)T (x).
5. If 2L ≥k, go to step 8.
6. Set L = k −L and T (x) = (k−1)(x)
(k)
.
7. Set T(x) = xT(x).
8. If k < 2t, go to step 3.
9. Determine the roots of (x) = (2t)(x).
Example 3.6: Determining (x) and (x) for the (7, 3, 5) Reed–Solomon code
using Berlekamp–Massey’s algorithm: Assuming the same received word as in
Example 3.5, r(x) = x6 + x5 + αx4 + α5x3 + α3x2 + α2x + α5, the syndromes
were found to be S1 = α2, S2 = α4, S3 = α2 and S4 = α. The algorithm proceeds
as follows:
First, initialize the algorithm variables: k = 0, (0)(x) = 1, L = 0 and T(x) = x.
(1) = S1 −
0

i=1
(0)
i S1−i = S1 = α2
(1) (x) = (0) (x) −(1)T (x)
= 1 −α2x.
At step 5, 2L = 2 × 0 = 0, which is less than k, so go to step 6: L = k −L =
1 −0 = 1.
T (x) = (0) (x)
(1)
= 1
α2 = α7
α2 = α5.
At step 7, T(x) = xT(x) = α5x. At step 8, k < 2t = 4, so go to step 3.
For k = 2, the discrepancy (2) and error-locating polynomial (2)(x) are:
(2) = S2 −
1

i=1
(1)
i S2−i = α4 −α2α2 = 0
(2) (x) = (1) (x) −(2)T (x) = (1) (x) = 1 −α2x.

94
NON-BINARY BLOCK CODES
(2) = 0, meaning there is no discrepancy and the error-locating polynomial
does not need to be modiﬁed, so go to step 7: T(x) = xT(x) = α5x2.
At step 8, k < 2t, so go to step 3.
For k = 3, the discrepancy (3) and error-locating polynomial (3)(x) are:
(3) = S3 −
1

i=1
(1)
i S3−i = α2 −α2α4 = α2 + α2 + 1 = 1
(3) (x) = (2) (x) −(3)T (x) = 1 −a2x −a5x2.
At step 5, 2L = 2, which is less than k, so go to step 6.
L = k −L = 3 −1 = 2.
T (x) = (2) (x)
(3)
= 1 −α2x
1
= 1 −α2x.
At step 7, T(x) = xT(x) = x −α2x2. At step 8, k < 2t, so go to step 3.
Finally, for k = 4, the discrepancy (4) and error-locating polynomial (4)(x)
are:
(4) = S4 −
2

i=1
(3)
i S4−i=S4−
-
(3)
1 S3+(3)
2 S2
.
= α+α2α2+α5α4 = α + α4 + α2
= α + α2 + α + α2 = 0
(4) (x) = (3) (x) −(4)T (x) = (3) (x) −0 = 1 −α2x −α5x2
At step 3, (4) = 0, meaning there is no modiﬁcation to the error-locating
polynomial, so go to step 7: T(x) = xT(x) = x2 −α2x3.
At step 8, k = 2t, so go to step 9 and end the algorithm.
The error-locating polynomial (x) = (4)(x) = 1 −α2x −α5x2 can be factorized
to (x) = (1 −α4x)(1 −αx), which has the roots X1 = α3 and X2 = α6. This
completes the Berlekamp–Massey algorithm. A summary of the above example is
shown in Table 3.8.
Table 3.8
Summary of the Berlekamp–Massey algorithm steps.
k
Sk
(k)(x)
(k)
L
T(x)
0
—
1
—
0
x
1
α2
1 −α2x
α2
1
α5x
2
α4
1 −α2x
0
1
α5x2
3
α2
1 −α2x −α5x2
1
2
x −α2x2
4
α
1 −α2x −α5x2
0
2
x2 −α2x3
However, the decoding process is still unﬁnished as the error magnitudes at these
locations need to be calculated.

DECODING REED–SOLOMON CODES
95
3.6.3 Determining the Error Magnitudes Using Forney’s Formula
Forney’s formula [13] is a very efﬁcient method for ﬁnding the error magnitudes
and only requires the error locations, the error-locating polynomial and the error-
magnitude polynomial (x) given by (3.19). If errors have occurred at the positions
i1, i2, i3, . . . i2t, the error magnitudes are determined by Forney’s formula as [12]:
eik = −Xk

X−1
k

 
X−1
k

,
(3.21)
where  (x) is the formal derivative of the error-locating polynomial. The formal
derivative of a function f (x) = f0 + f1x + f2x2 + · · · + fnxn whose coefﬁcients
belong to some ﬁnite ﬁeld GF(q) is given as [12] :
f  (x) = f1 + 2 f2x + 3 f3x2 + · · · + nfn−1xn−1.
(3.22)
Completing the previous example, the error locations were found to be at X1 = α4
and X2 = α. The error magnitude (x) is:
 (x) =  (x) [1 + S (x)] mod x2t+1
= (1 −α2x −α5x2)(1 + α2x + α4x2 + α2x3 + αx4) mod x5
= 1 + (α2 + α2)x + (α4 + α4 + α5)x2+(α2 + α6 + 1)x3+(α + α4 + α2)x4
= 1 + α5x2 + (α2 + α2 + 1 + 1)x3 + (α + α2 + α + α2)x4
= 1 + α5x2.
Applying (3.22) to (x), the formal derivative of the error-locating polynomial is:
f  (x) = f1 + 2 f2x + 3 f3x2 + · · · + nfn−1xn−1
 (x) = α2 + 2α5x
= α2 +

α5 + α5
x
= α2.
Therefore, the error magnitude at the location X1 = α4 is:
ei1=−X1

X−1
1

 
X−1
1

=
α4 
1 + α5 
α4−2
α2
=α4(1+α5α6)
α2
=α4(1 + α4)
α2
= α9
α2 =α7 = 1.
Similarly, the error magnitude at the location X2 = α is:
ei2 = −X1

X−1
1

 
X−1
1

= α(1 + α5(α)−2)
α2
= α(1 + α5α5)
α2
= α

1 + α3
α2
= α2
α2 = 1.

96
NON-BINARY BLOCK CODES
This results in an error polynomial of e(x) = x4 + x. Therefore, the decoded code
word is r(x) + e(x), giving:
r(x) + e(x) =

x6 + x5 + αx4 + α5x3 + α3x2 + α2x + α5
+

x4 + x

= x6 + x5 + (α + 1) x4 + α5x3 + α3x2 +

α2 + 1

x + α5
= x6 + x5 + α3x4 + α5x3 + α3x2 + α6x + α5,
which matches the transmitted code word polynomial c(x).
3.7 Coded Modulation
When conventional coding techniques are introduced in a transmission system, the
bandwidth of the coded signal after modulation is wider than that of the uncoded signal
for the same information rate and the same modulation scheme. In fact, the encoding
process requires a bandwidth expansion that is inversely proportional to the code
rate, being traded for a coding gain. This is the reason why, in the past, conventional
coding schemes have been very popular on power-limited channels (where bandwidth
is readily available) but not on bandwidth-limited channels.
The ﬁrst important contribution to coding on bandwidth-limited channels was made
by Ungerboeck in 1982 [8]. He presented trellis coded modulation (TCM) as a coded
modulation scheme that accommodates the redundancy of a code on an expanded
signal set, and showed that excellent coding gains over uncoded modulation can be
achieved with no bandwidth expansion required. TCM codes are explained in Chapter
7 of this book.
As well as combining convolutional codes with modulation, it is also possible
to combine block codes with modulation; this is known as block coded modulation
(BCM). BCM codes have the advantage of being simpler to design than TCM codes,
but the decoding complexity can become too high for larger BCM codes.
3.7.1 Block Coded Modulation (BCM) Codes
The encoding procedure involves adding redundant bits or symbols to the message.
This redundancy does not contain any message bits and transmitting it results in
the overall data rate being reduced. The Quadrature Phase Shift Keying (QPSK)
modulation scheme carries two information bits per QPSK symbol without coding.
However, if the information is encoded with encoder of code rate R = 0.5 then half
the bits in the code word are redundant bits and so each QPSK symbol only carries
one information bit. One solution is to increase the size of the constellation to 16-PSK
(or alternatively 16-QAM), where each 16-PSK symbol carries four information bits
without coding. Therefore, using an encoder with R = 0.5 and 16-PSK means that
each 16-PSK symbol carries two information bits, which is the same as uncoded
QPSK.

CODED MODULATION
97
One problem with this solution is that as the constellation size is increased the
Euclidean distance between constellation points decreases, resulting in poorer perfor-
mance. However, combining code design with the modulation scheme can ensure that
code words have a minimum Euclidean distance greater than the Euclidean distance
between neighbouring constellation points. The combination of convolutional codes
and modulation is known as trellis coded modulation (TCM), while the combination
of block codes with modulation is known as block coded modulation (BCM).
3.7.2 Multi-Level Block Coding
A general block diagram of a binary BCM encoder is shown in Figure 3.6. The
encoding process takes m parallel messages u(1), u(2), . . ., u(m) of lengths k1, k2, . . .,
km bits and each message is encoded by one of m parallel block encoders, resulting
in m code words all of length n bits with minimum Hamming distances d1, d2, . . .,
dm. This arrangement of block encoders is called multi-level block coding and was
proposed by Imai and Hirakawa [14] . Finally, each bit from the m encoders is mapped
onto a 2m-ary constellation [15].
From Figure 3.6, we can see that the code rate of the multi-level block code is:
R =
m
i=1
ki
mn .
(3.23)
An example of a BCM code using m = 3 block encoders which are mapped onto
a 23 = 8-PSK constellation is given in Figure 3.7. The top block encoder is an (8, 1,
8) repetition code, which takes a message of k1 = 1 bit and repeats that value eight
times, for example 0 →00000000 and 1 →11111111. The second block code is the
(8, 7, 2) parity check code, as explained in Section 3.2. It takes a 7 bit message and
appends a parity check bit, which is either zero if the number of ‘1’s in the message
is even or one if the number of ‘1’s in the message is odd. Finally, the last encoder
(n, k1, d1)
Block code
2m-ary 
signal
set
(n, k2, d2)
Block code
(n, km, dm)
Block code
(
)
(1)
(1)
2
(1)
1
(1)
1ku
u
u
=
u
(
)
(2)
(2)
2
(2)
1
(2)
2
ku
u
u
=
u
(
)
(m)
(m)
2
(m)
1
(m)
km
u
u
u
=
u
BCM codeword
c(1)
c(2)
c(m)
Figure 3.6
General multi-level BCM encoder.

0
0
1
1
0
1
1
0
1
1
0
1
1
1
0
1
(8, 1, 8) Repetition
(8, 7, 2) Parity
(8, 8, 1) Uncoded
1
1
1
1
1
1
1
1
1
0
1
1
1
0
1
1
0
0
1
1
0
1
1
0
0
3
1
7
7
3
5
7
3
1
2
3
4
5
6
7
Transmitted 8-PSK Symbols
Source Information Bits
Multi-Level
BCM Encoder
8-PSK Modulator
c1
c2
c3
Figure 3.7
A 3-level BCM encoder for 8-PSK.
98

CODED MODULATION
99
is not an encoder at all since k3 = n = 8 and so there is no redundancy added. From
(3.23) the code rate of this BCM code is R = k1 + k2 + k3
3n
= 1 + 7 + 8
24
= 16
24 = 2
3. All that
remains is to map each bit from the three encoders to the 8-PSK constellation, which
is explained next.
3.7.3 Set Partitioning
The BCM encoder output is a constrained sequence of constellation symbols and
the mapping of the multi-level block encoder to these symbols is very important
for ensuring that the minimum Euclidean distance, or free distance, of the BCM
code is maximized. To achieve this, Ungerboeck proposed the Set Partitioning [8]
of the constellation, whereby the points in the constellation are recursively divided
into subsets, with the Euclidean distance between neighbouring points in each subset
being increased. Figure 3.8 shows set partitioning of the 8-PSK constellation as an
example. This is called a partition tree.
Ungerboeck’s rules for set partitioning any constellation are [8]:
 The signals in the bottom level of the partition tree are assigned parallel transitions.
 Parallel transitions must appear in subsets separated by the largest Euclidean dis-
tance.
 All signal points should be used equally often.
0
1
2
3
4
5
6
7
0
2
4
6
1
3
5
7
0
4
1
5
2
6
3
7
0 4
2
6
1
5
3
7
ε
ε
2
ε
2
ε
2
ε
2
0
1
0
1
0
1
0
1
0
1
0
1
0
1
ε
0.765
c1
c1
c2
c2
c2
c2
c3
c3
c3
c3
c3
c3
c3
c3
000
001
010
011
100
101
110
111
Figure 3.8
The set partitioning of the 8-PSK constellation.

100
NON-BINARY BLOCK CODES
From Figure 3.8 we can see that the coded bit from the repetition code c1 in
Figure 3.7 selects one of two QPSK subsets with constellation points separated by
a Euclidean distance of 0.765√ε. The coded bit from the parity check code c2 then
selects one of two subsets with constellation points separated at a greater distance of
2√ε. Finally, the uncoded bits c3 select signals in the bottom level of the partition
tree.
It can be shown that the minimum squared Euclidean distance between any two
sequences of the BCM code, otherwise known as the free distance d2
free, is [15]:
d2
free = min

2
1d1, 2
2d2, 2
3d3, . . . , 2
mdm

,
(3.24)
where 2
i , i = 0, 1, 2, . . ., m is the squared Euclidean distance between neighbouring
points in a subset in the ith level of the partition tree and di is the minimum Hamming
distance of the ith block encoder of the BCM code. A measure of the performance
of the BCM code over an uncoded constellation can be determined by ﬁnding the
asymptotic coding gain (ACG) γ [10]:
γ = d2
free/ε
d2min/ε ,
(3.25)
where d2
min is the squared Euclidian distance between points in the uncoded constel-
lation, ε is the energy of the coded constellation and ε is the energy of the uncoded
constellation. For the BCM code in Figure 3.7 the minimum Hamming distances are
d1 = 8, d2 = 2 and d3 = 1. The corresponding Euclidean distances from the subsets
of 8-PSK in Figure 3.8 are 2
1 = 0.585ε , 2
2 = 2ε and 2
3 = 4ε . Therefore, from
(3.24) the free distance of the BCM code is:
d2
free = min

0.585ε × 8, 2ε × 2, 4ε × 1

= min(4.68ε , 4ε , 4ε ) = 4ε .
Since the uncoded and coded modulation schemes are QPSK and 8-PSK respec-
tively, the energy of both constellations is the same; that is, ε = ε . The minimum
squared Euclidean distance between neighbouring points of QPSK constellation is
d2
min = 2ε, giving an ACG of:
γ = d2
free/ε
d2min/ε = 4ε /ε
2ε/ε = 4
2 = 2
or
3 dB.
3.7.4 Construction of a Block Code Trellis
The decoding of a BCM code can be achieved efﬁciently by representing it as a
trellis diagram and applying the Viterbi algorithm. Wolf [16] showed how a trellis
diagram of a linear block code could be constructed using its parity check matrix, and
this method was extended by Paravalos and Fleisher [17] to construct BCM trellis
diagrams.

CODED MODULATION
101
Since the BCM encoder is made up of a number of block codes, its trellis is a
combination of the trellis diagrams of the component codes. The state transitions in
the trellis are determined by [17]:
sξ(λ + 1) = sl(λ) +
m

i=1
δihi,λ+1,
(3.26)
where λ is an index denoting the depth of the trellis, l ∈Sλ (a set containing the index
of all states created up to depth λ), ξ ∈Sλ+1 (a set containing the index of all states
in Sλ plus all new states formed between λ and λ + 1), hi,λ+1 is a vector representing
column λ + 1 of the parity check matrix of the ith block code and δi = {0, 1} is a
binary input.
The trellis construction consists of four steps:
1. Each trellis begins and terminates at the all-zero state.
2. The depth of each trellis spans from λ = 0 to λ = n.
3. The number of states depends on the number and type of block codes used and the
constellation size. It grows while proceeding further into the trellis. To begin the
construction, a number of 2m states is assumed. If a number of information bits
are presented to the mapper uncoded, the initial number of states reduces to 2m-r,
where r is the number of absent encoders.
4. Each transition is determined by (3.26).
The following should be observed:
1. When the construction is complete all trellis paths that do not terminate at the
all-zero state are removed as they do not represent valid code sequences.
2. Transitions in a trellis are assigned a symbol from the signal constellation. Each
path within the trellis corresponds to a possible BCM sequence.
3. Due to the variation in component code sizes, the parity check matrix vectors used
in (3.26) have different dimensions. When the size of one parity check vector is
less than the size of the state binary vector s, an appropriate number of zeroes is
appended at the top of the vector. Alternatively, a vector with more elements than
the state binary vector s is reduced from top to bottom using modulo-2 addition.
The given procedure yields the ﬁnal BCM trellis only in cases where the top encoder
employs the repetition code and the remaining encoders are simple codes.
Example 3.7: BCM code trellis construction: Using the BCM code from Figure
3.7, an example of constructing a trellis is now given. The BCM encoder consists of
three block encoders: the (8, 1, 8) Repetition code, the (8, 7, 2) Parity Check code
and 8 bits of uncoded information. The parity check matrices of the Repetition

102
NON-BINARY BLOCK CODES
code, HR, and the Parity Check code, HP, are:
HR =


1
1
0
0
0
0
0
0
1
0
1
0
0
0
0
0
1
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
1
0
0
0
0
0
1
0
1
0
0
0
0
0
0
1


and
HP = [1 1 1 1 1 1 1 1].
The trellis will consist of 2m−r = 23−1 = 4 states with two subtrellises each of
two states. To accommodate two states per subtrellis, the parity check matrices
are adjusted to attain a row size equal to the size of the binary state vector. Thus,
the reduced vectors of HR are obtain by top-to-bottom modulo-2 addition and the
result is:
HR =
	1 1 1 1 1 1 1 1
HP =
	1 1 1 1 1 1 1 1
.
Now the state transitions will be determined by (3.26):
1. At depth λ = 1, (3.26) will become:
sξ(1) = sl(0) + δ1h1,1 + δ2h2,1.
In the ﬁrst subtrellis, the values of δi for the ﬁrst path are δ1 = 0, δ2 = 0 and for
the second path δ1 = 0, δ2 = 1.
s0(1) = [0] + 0 × [1] + 0 × [1] = [0]
s1(1) = [0] + 0 × [1] + 1 × [1] = [1].
In the second subtrellis, the values of δi for the ﬁrst path are δ1 = 1, δ2 = 0 and
for the second path δ1 = 1, δ2 = 1.
s2(1) = [0] + 1 × [1] + 0 × [1] = [1]
s3(1) = [0] + 1 × [1] + 1 × [1] = [0].
Note that the uncoded information is transmitted on the third BCM encoder
level. This means that the parity check matrix does not exist and hence all
corresponding transitions lead to the same state, causing parallel transitions in
the trellis diagram.
2. At depth λ = 2, (3.26) will become:
sξ(2) = sl(1) + δ1h1,2 + δ2h2,2.

CODED MODULATION
103
First Subtrellis
s0(2) = [0] + 0 × [1] + 0 × [1] = [0]
s1(2) = [0] + 0 × [1] + 1 × [1] = [1]
s2(2) = [1] + 0 × [1] + 0 × [1] = [1]
s3(2) = [1] + 0 × [1] + 1 × [1] = [0].
Second Subtrellis
s4(2) = [0] + 1 × [1] + 0 × [1] = [1]
s5(2) = [0] + 1 × [1] + 1 × [1] = [0]
s6(2) = [1] + 1 × [1] + 0 × [1] = [0]
s7(2) = [1] + 1 × [1] + 1 × [1] = [1].
3. At depth λ = 3, (3.26) will become:
sξ(3) = sl(2) + δ1h1,3 + δ2h2,3.
First Subtrellis
s0(3) = [0] + 0 × [1] + 0 × [1] = [0]
s1(3) = [0] + 0 × [1] + 1 × [1] = [1]
s2(3) = [1] + 0 × [1] + 0 × [1] = [1]
s3(3) = [1] + 0 × [1] + 1 × [1] = [0].
Second Subtrellis
s4(3) = [0] + 1 × [1] + 0 × [1] = [1]
s5(3) = [0] + 1 × [1] + 1 × [1] = [0]
s6(3) = [1] + 1 × [1] + 0 × [1] = [0]
s7(3) = [1] + 1 × [1] + 1 × [1] = [1].
The construction continues until λ = n = 8. The ﬁnal trellis is shown in
Figure 3.9.
3.7.5 Non-Binary BCM Codes
In 1994, Baldini and Farrell [18] introduced a class of non-binary BCM code over
rings of integers suitable for M-PSK and M-QAM. The idea of non-binary BCM codes
is to transmit m bits per channel symbol by using a modulator with q > 2m waveforms

104
NON-BINARY BLOCK CODES
0
2
4
6
1
3
5
7
0
2
0
2
1
3
1
3
4
7
5
5
7
6
4
6
0
2
0
2
1
3
1
3
4
7
5
5
7
6
4
6
0
2
0
2
1
3
1
3
4
7
5
5
7
6
4
6
0
2
0
2
1
3
1
3
4
7
5
5
7
6
4
6
0
2
4
6
1
3
5
7
Figure 3.9
Trellis for a BCM code using 8-PSK modulation.
to accommodate the extra redundancy. The non-binary BCM encoder structure is
shown in Figure 3.10. The binary source generates m + 1 parallel bits, which are
Gray mapped onto one of 2m+1 channel symbols ai ∈Zq, i = 0, 1, . . ., k – 1. These
are then fed to the multi-level encoder to generate the BCM coded symbols xi ∈Zq,
i = 0, 1, . . . n, which will increase the minimum Euclidean distance.
One class of block code that can be used in the BCM encoder is the systematic
linear circulant block code. Its generator matrix is of the form given in (3.7), where
its parity block P is [18]:
P =


P1,1
Pk,1
Pk−1,1 · · · P3,1 P2,1
P2,1
P1,1
Pk,1
· · · P4,1 P3,1
...
...
...
...
...
...
Pk−1,1 Pk−2,1 Pk−3,1 · · · P1,1 Pk,1
Pk,1
Pk−1,1 Pk−2,1 · · · P2,1 P1,1


.
(3.27)
Binary 
Source
ai =f( b1, b2, ..., bm+1
Mapping
Multilevel
Encoder
Multilevel Source
b1
b2
bm+1
ai
q
k
xi
∈
q
n
∈
Figure 3.10
Non-binary BCM encoder structure.

CODED MODULATION
105
Each column of P in (3.27) is a cyclic shift upwards of the preceding column. To
identify this type of code the values of the ﬁrst column of P are used, that is P1,1,
P2,1, . . . , Pk−1,1, Pk,1. An example of a systematic linear circulant block code over
Z4 = {0, 1, 2, 3} is the (10, 5) block code denoted by 12 233 [18]. Its generator
matrix is:
G =


1
0
0
0
0
1
3
3
2
2
0
1
0
0
0
2
1
3
3
2
0
0
1
0
0
2
2
1
3
3
0
0
0
1
0
3
2
2
1
3
0
0
0
0
1
3
3
2
2
1


and it has a minimum squared Euclidean distance of d2
free = 12. From (3.25) its
asymptotic coding gain over uncoded BPSK is 4.77 dB.
Another class of multi-level block code is the pseudocyclic multi-level code, which
has a generator matrix of the form [18]:
G =


g1,1
g1,2
· · ·
· · ·
g1,r
0
0
· · ·
0
0
g1,1
g1,2
· · ·
· · ·
g1,r
0
· · ·
0
...
...
...
...
...
...
...
...
...
0
0
· · ·
g1,1
g1,2
· · ·
· · ·
· · ·
g1,r


,
(3.28)
where gi, j ∈Zq and r depends on the coded q-PSK modulation scheme and the code
rate.
It is also possible to construct non-binary BCM codes that are invariant to phase
rotations, that is a phase shift of the received symbols results in another valid code
word. Codes that are phase invariant to rotations of (360/q)◦are called transparent
codes. A code is transparent if it contains the all-one sequence (1, 1, 1, . . ., 1) as a
valid code word. Since the BCM code is linear, adding any two code word modulo-q
will result in another code word. Adding the all-one code word to another code word
is equivalent to rotating all the coded symbols by 90◦, which produces yet another
valid code word.
To ensure that the all-one code word is present in the non-binary BCM code, the
generator matrix from (3.28) is modiﬁed so that the bottom row is all ones [18].
G =


g1,1
g1,2
· · ·
· · ·
g1,r
0
0
· · ·
0
0
g1,1
g1,2
· · ·
· · ·
g1,r
0
· · ·
0
...
...
...
...
...
...
...
...
...
1
1
· · ·
1
1
· · ·
· · ·
· · ·
1


.
(3.29)
Now the pseudocyclic multi-level code is transparent. To remove phase rotations
from the received symbols a differential encoder can be added before the transparent

106
NON-BINARY BLOCK CODES
D
Transparent
Encoder
Channel
Transparent
Decoder
D
u(D)
a(D)
â(D)
û(D)
Differential 
Encoder
Differential 
Decoder
Σ
Σ
+
+
+
−
Figure 3.11
A differential encoder and decoder added to the non-binary BCM system to remove phase
rotations.
encoder [18], as shown in Figure 3.11. Similarly, a differential decoder is added after
the transparent decoder to obtain the decoded message.
3.8 Conclusions
In this chapter an introduction to block coding was given, beginning with the simplest
binary block codes, such as the parity check codes. A class of block codes known
as cyclic codes was then presented, with particular attention given to BCH codes.
These codes have good parameters and, using BCH codes, it was demonstrated how
extending to a non-binary alphabet can improve these parameters, for example giving
large Hamming distance and higher code rates. A special class of non-binary BCH
codes is the Reed–Solomon codes, which are the most commonly used error-correcting
codes today and can be found in optical and magnetic storage, high-speed modems
and wireless communications. Finally, the concept of block coded modulation was
introduced, whereby the block encoder and modulator are treated as a single entity
to avoid increasing bandwidth use. Proper mapping of the coded symbols to channel
symbols ensures that the loss in performance due to the expansion of the signal
set is more than compensated for by the coding gain of the BCM code. Further
performance gains can be achieved by using non-binary block codes in the coded
modulation scheme, but with an increase in the decoding complexity.
References
[1] Hamming, R. (1950) Error detecting and error correcting codes. Bell Systems Technical Journal,
29, 41–56.
[2] Hocquenghem, A. (1959) Codes correcteurs d’erruers. Chiffres, 2, 147–56.
[3] Bose, R. and Ray-Chaudhuri, D. (1960) On a class of error-correcting binary codes. Information
and Control, 3, 68–79.
[4] Reed, I. and Solomon, G. (1960) Polynomial codes over certain ﬁnite ﬁelds. Journal of the Society
of Industrial Mathematics, 8, 300–4.
[5] Sugiyama, Y., Kasahara, M., Hirasawa, S. and Namekawa, T. (1975) A method for solving key
equation for Goppa codes. Information and Control, 27, 87–99.

REFERENCES
107
[6] Berlekamp, E. (1967) Nonbinary BCH decoding. Proceedings of the International Symposium on
Information Theory, San Remo, Italy.
[7] Massey, J.L. (1969) Shift-register synthesis and BCH decoding. IEEE Transactions on Information
Theory, IT-15 (1), 122–7.
[8] Ungerboeck, G. (1982) Channel coding with multilevel/phase signals. IEEE Transactions on Infor-
mation Theory, 28 (1), 55–67.
[9] Viterbi, A.J. (1971) Convolutional codes and their performance in communication systems. IEEE
Transactions on Communications Technology, 19 (5), 751–72.
[10] Moon, T.K. (2005) Error Correction Coding. Mathematical Methods and Algorithms, Wiley Inter-
science, ISBN 0-471-64800-0.
[11] Prange, E. (1958) Cyclic Error-Correcting Codes in Two Symbols, Air Force Cambridge Research
Center, Cambridge, MA, Technical Report, TN-58-156.
[12] Wicker, S.B. (1995) Error Control Systems for Digital Communications and Storage, Prentice Hall,
Eaglewood Cliffs, NJ.
[13] Forney, G.D. (1965) On decoding BCH codes. IEEE Transactions on Information Theory, 11 (4),
549–57.
[14] Imai, H. and Hirakawa, S. (1977) A new multilevel coding method using error-correcting codes.
IEEE Transactions on Information Theory, 23 (3), 371–7.
[15] Lin, S. and Costello, D.J. Jr. (2004) Error Control Coding, 2nd edn, Pearson Prentice Hall, ISBN
0-13-017973-6.
[16] Wolf, J.K. (1978) Efﬁcient maximum likelihood decoding of linear block codes using a trellis. IEEE
Transactions on Information Theory, 24 (1), 76–80.
[17] Paravalos, E. and Fleisher, S. (1991) Block coded modulation: an application to frequency/phase
modulation and a procedure for the construction of trellis diagram. IEEE Proceedings of Paciﬁc
Rim Conference on Communications, Computers and Signal Processing, Victoria, USA, pp. 79–82.
[18] Baldini, F.R. and Farrell, P.G. (1994) Coded modulation based on rings of integers modulo-q. Part
1: block codes. IEE Proceedings Communications, 141 (3), 129–36.


4
Algebraic–Geometric Codes
4.1 Introduction
Algebraic geometry is a powerful mathematical tool for constructing very long non-
binary block codes with excellent parameters, such as high code rate and large
Hamming distance. In 1981, Goppa showed how algebraic geometry could be used to
construct non-binary block codes called Goppa codes or algebraic–geometric (AG)
codes [1]. These codes are constructed from the afﬁne points of an irreducible pro-
jective curve and a set of rational functions deﬁned on that curve. The simplest type
of AG code is the well-known Reed–Solomon code, which can be constructed from
the afﬁne points of a projective line. The length of an AG code is equal to the number
of afﬁne points, which in the case of a line can be no greater than the cardinality of
the chosen ﬁnite ﬁeld. Hence, constructing codes from a line will result in short code
lengths, as is the case for Reed–Solomon codes. More afﬁne points can be obtained
by instead choosing a projective curve, which can result in much longer codes with-
out increasing the size of the ﬁnite ﬁeld. The most desirable curves are those that
have the maximum possible number of afﬁne points, known as maximal curves, since
these produce the longest possible codes. However, constructing AG codes requires
an in-depth knowledge of the theory of algebraic geometry, which is very difﬁcult
to understand, and this could be one of the reasons why AG codes have yet to be
implemented.
In 1989, Justesen et al. [2] presented a construction method that only requires a
basic understanding of algebraic geometry, but this method can still produce AG codes.
This method has the disadvantage that less AG codes can be constructed than through
more complicated algebraic–geometric approach, but many can still be produced. The
notation used by Justesen for the construction of AG codes discussed in this chapter
is also used in the hard-decision decoding algorithm explained later in this chapter.
In this chapter, the construction of AG codes is presented using the theory of
algebraic geometry discussed in Chapter 2. This is followed by Justesen’s simpliﬁed
construction method, which is described in detail. Comparisons are made between
Non-Binary Error Control Coding for Wireless Communication and Data Storage
Rolando Antonio Carrasco and Martin Johnston
C 2008 John Wiley & Sons, Ltd

110
ALGEBRAIC–GEOMETRIC CODES
Reed–Solomon and Hermitian codes, illustrating the limitations of Reed–Solomon
codes in terms of code length and the number of codes that can be constructed.
The next section presents the construction of systematic AG codes. Unfortunately,
Hermitian codes are not cyclic like Reed–Solomon codes, and so it is not possible
to use a multi-stage shift register to produce systematic AG codes. Traditionally,
systematic block codes are created by performing Gauss–Jordan elimination on the
nonsystematic generator matrix.
4.2 Construction of Algebraic–Geometric Codes
The construction of AG codes can be accomplished using the theory described in
Chapter 2. Firstly, an irreducible afﬁne smooth curve over a ﬁnite ﬁeld must be
chosen. Curves that can be used to construct good AG codes are Hermitian curves,
elliptic curves, hyperelliptic curves and so on, and all have a single point at inﬁnity.
Next, all afﬁne points and the point at inﬁnity must be found, as explained in Chapter 2.
To determine the message length k and designed minimum distance d
∗of the code, the
degree a of the divisor aQ must be chosen, where Q is the point at inﬁnity, using [3]:
2γ −2 < a < n,
(4.1)
where γ is the genus of the curve and n is the number of afﬁne points (code length).
The code parameters can then be calculated using (2.13) and (2.16) for a functional
Goppa code or (2.17) and (2.18) for a residue Goppa code.
To construct the generator matrix of the code, a set of rational functions with pole
order up to and including a at Q must be determined, but these functions must not
have poles at any of the afﬁne points. From Chapter 2 we know the set of rational
functions
L(aQ) =
xi y j
zi+ j

, 0 ≤i < m,
j ≥0
(where m is the degree of the curve and the pole order of each rational function is
4i + 5j) satisfy this requirement for the Hermitian curve. There are k rational function
in L(aQ), which are evaluated at each of the n afﬁne points to generate the k rows of
the (k × n) generator matrix G. Now an example of AG code construction is given.
Example 4.1: Construction of the (8, 5, 3) Hermitian code using algebraic
geometry: In this example, the Hermitian curve over GF(22) deﬁned by (2.1) with
γ =1 is chosen to construct a residue Goppa code. The curve has eight afﬁne
points, shown in Table 2.12, and one point at inﬁnity, Q = (0, 1, 0). The degree a
of the divisor aQ is chosen using (4.1), which can vary between zero and eight.

CONSTRUCTION OF ALGEBRAIC–GEOMETRIC CODES
111
In this example we choose a = 3 and from (2.19) the message length is:
k = n −l(G) = n −d(G) −1 + γ
= 8 −3 −1 + 1 = 5
,
and from (2.18) the designed minimum distance d
∗is:
d∗= n −(n −d(G) −1 + γ ) + 1 −γ
= d(G) −2γ + 2
= 3 −2 + 2 = 3
.
So we have the (8, 5, 3) Hermitian code. The k = 5 rational functions that have
pole orders up to and including a = 5 are:
L(5Q) =

1, x
z , y
z , x2
z2 , xy
z2

.
Since all the afﬁne points have z = 1, L(5Q) can be written as:
L(5Q) =

1, x, y, x2, xy

.
The code word is then obtained by evaluating a message polynomial at each
of the eight afﬁne points of the Hermitian curve. From Chapter 2, the eight afﬁne
points of the Hermitian curve over GF(4) are:
P0 = (0, 0), P1 = (0, 1), P2 = (1, α), P3 = (1, α2), P4 = (α, α),
P5 = (α, α2), P6 = (α2, α) and P7 = (α2, α2).
Let the message polynomial be f(x, y) = 1 + y + x2. Then each coded symbol
ci = f(Pi), i = 1, 2, . . . , 8.
c1 = f (P1) = f (0, 0) = 1 + 1 + 1 = 1,
c2 = f (P2) = f (0, 1) = 1 + 1 + 0 = 0,
c3 = f (P3) = f (1, α) = 1 + 1 + α2 = α2,
c4 = f (P4) = f (1, α2) = 1 + 1 + α4 = α,
c5 = f (P5) = f (α, α) = 1 + α + α2 = 0,
c6 = f (P6) = f (α, α2) = 1 + α2 + α2 = 1,
c7 = f (P7) = f (α2, α) = 1 + α + α4 = 1,
c8 = f (P8) = f (α2, α2) = 1 + α2 + α4 = 0,
.
Hence the code word is c = 1, 0, α2, α, 0, 1, 1, 0.
This is also equivalent to multiplying a message vector m by a generator matrix
G. To construct the rows of the generator matrix of an AG code, each monomial

112
ALGEBRAIC–GEOMETRIC CODES
φi, i = 0, 1, . . . , k, in L(5Q) is evaluated at each afﬁne point, as shown in (4.2).
G =


φ1(P1)
φ1(P2)
φ1(P3)
· · ·
φ1(Pn)
φ2(P1)
φ2(P2)
φ2(P3)
· · ·
φ2(Pn)
φ3(P1)
φ3(P2)
φ3(P3)
· · ·
φ3(Pn)
...
...
...
...
...
φk(P1)
φk(P2)
φk(P3)
· · ·
φk(Pn)


.
(4.2)
If the ﬁrst monomial is written as φ1 = x0y0 then evaluating it at each point
will give a row of all 1s due to powers of 0 in the x and y term. The remaining
monomials are evaluated at each of the eight points using Table 2.12.
The second row of the generator matrix is obtained by evaluating the monomial
f(x, y) = x at all eight afﬁne points. In this case each of the eight elements of this
row is just the x-coordinate of one of the eight afﬁne points.
f (0, 0) = 0, f (0, 1) = 0, f (1, α) = 1, f (1, α2) = 1, f (α, α) = α,
f (α, α2) = α, f (α2, α) = α2, f (α2, α2) = α2.
The third row of the generator matrix is obtained by evaluating the monomial
f(x, y) = y at all eight afﬁne points. In this case each of the eight elements of this
row is just the y-coordinate of one of the eight afﬁne points.
f (0, 0) = 0, f (0, 1) = 1, f (1, α) = α, f (1, α2) = α2, f (α, α) = α,
f (α, α2) = α2, f (α2, α) = α, f (α2, α2) = α2.
The fourth row of the generator matrix is obtained by evaluating the monomial
f(x, y) = x2 at all eight afﬁne points. In this case each of the eight elements of this
row is the square of the x-coordinate of one of the eight afﬁne points.
f (0, 0) = 0, f (0, 1) = 0, f (1, α) = 1, f (1, α2) = 1, f (α, α) = α2,
f (α, α2) = α2, f (α2, α) = α, f (α2, α2) = α.
The ﬁfth row of the generator matrix is obtained by evaluating the monomial
f(x, y) = xy at all eight afﬁne points. In this case each of the eight elements of this
row is the product of the x-coordinate and y-coordinate of one of the eight afﬁne
points.
f (0, 0) = 0, f (0, 1) = 0, f (1, α) = α, f (1, α2) = α2, f (α, α) = α2,
f (α, α2) = 1, f (α2, α) = 1, f (α2, α2) = α.
Therefore, the generator matrix is:
G =


1
1
1
1
1
1
1
1
0
0
1
1
α
α
α2
α2
0
1
α
α2
α
α2
α
α2
0
0
1
1
α2
α2
α
α
0
0
α
α2
α2
1
1
α


.

CONSTRUCTION OF ALGEBRAIC–GEOMETRIC CODES
113
The construction method for one-point AG codes is summarized below [3]:
1. Select an irreducible afﬁne smooth curve and a ﬁnite ﬁeld.
2. Find all the afﬁne points of the form (α, β, 1) and point at inﬁnity Q = (α, β,
0), where α and β are elements in the ﬁnite ﬁeld, that make the curve vanish.
3. Choose the degree a of the divisor aQ using (4.1) and calculate the message
length k and designed minimum distance d
∗of the code using (2.13) and (3.16)
for a functional Goppa code or (2.17) and (2.18) for a residue Goppa code.
4. Determine a set of rational functions L(aQ) that have pole orders up to and
including a at the point at inﬁnity Q, but do not have poles at any of the afﬁne
points.
5. Evaluate each rational function in L(aQ) at each afﬁne point to obtain the
nonsystematic generator matrix G of the AG code.
4.2.1 Simpliﬁed Construction of AG Codes
In this section, a simpliﬁed construction method attributed to Justesen [2] is described,
which only requires a basic knowledge of algebraic geometry to construct an AG code.
The ﬁrst difference of this method is that projective curves are not considered. Only
afﬁne curves in the (x −y) coordinate system are used and the point at inﬁnity is
excluded. Another difference is the use of a set of monomials in two dimensions
instead of a set of rational functions in three dimensions to obtain the generator
matrix.
To construct an algebraic–geometric code using Justesen’s construction, a nonneg-
ative integer j is ﬁrst chosen that is bounded by [2]:
m −2 ≤j ≤
n −1
m

.
(4.3)
(4.3) is derived from (4.1). In the simpliﬁed construction the degree of the divisor
G is a multiple of the degree of the curve C(x, y), that is deg(G) = mj, where j is a
nonnegative integer. Substituting deg(G) = mj and (2.12) into (4.1) gives:
2(m −1)(m −2)
2
−2 < mj < n
⇒m2 −3m + 2 −2 < mj < n
⇒m(m −3)
m
< j < n
m
∴m −2 ≤j ≤
n −1
m

.
The codes obtained using Justesen’s simpliﬁed construction are residue Goppa
codes. Therefore, since deg(G) = mj, the code parameters from (2.17) and (2.18) can

114
ALGEBRAIC–GEOMETRIC CODES
be written as:
k = n −mj + γ −1
d∗= mj −2γ + 2
.
(4.4)
However, since deg(G) is limited to being a multiple of the degree of the curve, less
AG codes can be constructed than with the algebraic–geometric construction. This
is the only disadvantage of the simpliﬁed method, but many AG codes can still be
constructed using it.
A monomial basis is deﬁned as:
f = {xi y j},
0 ≤i < m,
b ≥0,
(4.5)
which contains k monomials. Hence, as before, the monomials are evaluated at each
point to obtain the generator matrix of the code.
Example 4.2: Construction of the (8, 5, 3) Hermitian code over GF(22) using
the simpliﬁed method: In this example, the same Hermitian code is constructed
as in the previous example, but using Justesen’s simpliﬁed construction method.
Again, the Hermitian curve deﬁned in (2.1) is used. From the previous example,
we know that the Hermitian code over GF(22) has n = 8 points, but these are not
projective points and so there is no z component. The points are given in Table 4.1.
Table 4.1
Points of the Hermitian curve over GF(22).
P1 = (0, 0)
P2 = (0, 1)
P3 = (1, α)
P4 = (1, α2)
P5 = (α, α)
P6 = (α, α2)
P7 = (α2, α)
P8 = (α2, α2)
The degree of the Hermitian curve is m = 3, so from (4.3) the parameter j can
have values of:
3 −2 ≤j ≤
 8−1
3
0
= 1 ≤j ≤2
.
The genus of the curve is γ = 1, and taking j = 1 the parameters of the code
from (4.4) are:
k = 8 −3 × 1 + 1 −1 = 5
d∗= 3 × 1 −2 × 1 + 2 = 3.
Since k = 5 there will be ﬁve monomials in the monomial basis. These are
{1, x, y, x2, xy}. To construct the generator matrix G, each monomial is evaluated
at each of the eight points, and thus we obtain the same generator matrix as in
the previous example. This example shows that, using Justesen’s construction
method, all that is required is the degree and genus of the curve and the points
on the curve. It does not require an in-depth knowledge of algebraic geometry to
construct AG codes.

CONSTRUCTION OF ALGEBRAIC–GEOMETRIC CODES
115
Table 4.2
Comparison of Reed–Solomon and Hermitian code lengths for different
ﬁnite ﬁelds (ﬁgures in parentheses are for doubly-extended Reed–Solomon codes).
Finite ﬁeld size q
Reed–Solomon code length
Hermitian code length
4
3 (5)
8
16
15 (17)
64
64
63 (65)
512
256
255 (257)
4096
4.2.2 Comparison of AG Codes with Reed–Solomon Codes
Reed–Solomon codes are the most commonly used codes due to their efﬁcient decod-
ing algorithms and good performance over channels with burst errors. However, they
do have limitations, such as short code length and the fact that there are not many of
them. Table 4.2 shows how the lengths of Reed–Solomon codes and Hermitian codes
vary for increasing ﬁnite ﬁeld size.
It can be seen that the length of the Hermitian codes increases dramatically for
increasing ﬁnite ﬁelds since for a ﬁnite ﬁeld size of q the length n = q3/2. However,
the length of the Reed–Solomon code is restricted to the size of the ﬁnite ﬁeld and
has length n = q −1, or q + 1 if doubly extended.
The number of possible AG codes for a given ﬁnite ﬁeld can be found from (4.1).
If 2γ −2 < deg(G) < n then 2γ −1 ≤deg(G) ≤n −1. This means that there are
(n −1) −(2γ −1) = n −2γ possible AG codes for a given ﬁnite ﬁeld. As stated
previously, a Reed–Solomon code is constructed from an afﬁne straight line which
has a genus γ = 0. This implies there are n possible Reed–Solomon codes for a given
ﬁnite ﬁeld. A comparison of the numbers of Reed–Solomon and Hermitian codes for
increasing ﬁnite ﬁeld size is given in Figure 4.1.
It can be seen that the number of possible Hermitian codes increases exponentially
with increasing ﬁeld size, with a possible 3855 Hermitian codes deﬁned over GF(28).
In contrast, the number of Reed–Solomon codes is again restricted by the size of the
ﬁnite ﬁeld.
4.2.3 Systematic Algebraic–Geometric Codes
Evaluating the elements in the monomial basis at each point on the curve produces a
generator matrix in nonsystematic form. Traditionally, systematic codes are achieved
by applying Gauss–Jordan elimination [4] to the nonsystematic generator matrix.
However, this approach had not been used to construct systematic AG codes since
for large generator matrices the complexity could be too high. In the literature, only
Heegard et al. [5] have considered the construction of systematic AG codes. They show
how systematic encoding of algebraic–geometric codes can be achieved in a method
analogous to the systematic encoding of Reed–Solomon codes by using the cyclic
properties of the automorphisms of a curve. This method requires fewer operations

116
ALGEBRAIC–GEOMETRIC CODES
15
63
255
51
455
3855
0
500
1000
1500
2000
2500
3000
3500
4000
256
64
16
Finite Field GF(q)
No. of Codes
Reed–Solomon
Hermitian
Figure 4.1
A comparison of the numbers of Reed–Solomon and Hermitian codes for increasing ﬁnite
ﬁeld size.
than applying Gauss–Jordan elimination on the nonsystematic generator matrix, but
it necessitates a good knowledge of algebraic geometry. However, since their paper
was written, computers have become much more powerful and it has been found that
Gauss–Jordan elimination can be performed quickly even on large generator matrices
from Hermitian codes deﬁned over GF(26). Gauss–Jordan elimination is made up of
two algorithms: Gaussian elimination and Jordan elimination. Gaussian elimination
converts the matrix into echelon form and Jordan elimination then ensures the matrix
is in row-reduced echelon form. For the case of a nonsquare matrix, such as a k × n
generator matrix, a k × k submatrix of the generator matrix in row-reduced echelon
form is the k × k identity matrix Ik. The remainder of the generator matrix is a k × (n
−k) parity matrix P. The generator matrix is now in systematic form, G = [Ik | P].
The algorithms for Gaussian elimination and Jordan elimination are described below.
4.2.3.1 Gaussian Elimination
The Gaussian elimination algorithm uses the following notation: Gj is used to denote
all the elements in row j of G, Gi,j denotes a single element located at (i, j) in G, and k
is the message length of the AG code. When Gaussian elimination is used on a matrix
containing real numbers, elimination of a column occurs by multiplying previous
rows by a factor and subtracting them from the row that is being operated on. In the
case of a matrix containing ﬁnite ﬁeld elements, Gaussian elimination is performed
in the same way, using modulo-q addition instead of subtraction. Modulo-q addition
can be used if the row to be subtracted is multiplied by the additional factor q −1.

CONSTRUCTION OF ALGEBRAIC–GEOMETRIC CODES
117
Initialize: i = 0, j = 0
1. Check that column i does not contain all zeros. If it does then i = i + 1 and go to
step 1. Else go to step 2.
2. Find the row p with largest ﬁrst element.
3. Swap row p with largest ﬁrst element with row j.
4. Eliminate elements in column i from row j + 1 to row k −1.
For row = j + 1 to k – 1:
Grow →Grow + (q −1)
Gi,row
Gi, j
G j

.
5. i = i + 1 and j = j + 1. If j = k go to step 6. Else go to step 2.
6. For row = 0 to k −1:
Divide Grow by the ﬁrst nonzero element in Grow so that the main diagonal of G
contains all 1s.
4.2.3.2 Jordan Elimination
1. Initialize: i = 0, j = 0.
2. If Gi,j = 0, swap column i with the nearest column that has a 1 in row j.
3. Eliminate elements in column i from row j −1 to row 0.
For row = j −1 to 0:
Grow →Grow + (q −1)
Gi,row
Gi, j
G j

.
4. i = i + 1 and j = j + 1. If j = k STOP. Else go to step 2.
Example 4.3: Construction of the systematic (27, 13, 12) Hermitian code over
GF(32): Using the primitive polynomial x2 = 2x + 1, the ﬁnite ﬁeld GF(32) is
shown in Table 2.10. The Hermitian code deﬁned over GF(32) is C(x, y) = x4 +
y3 + y and has 27 points that satisfy C(x, y) = 0, given in Table 4.3.
Table 4.3
The 27 points of the Hermitian curve over GF(32).
P1 = (0, 0)
P2 = (0, α6)
P3 = (0, α2)
P4 = (1, 1)
P5 = (1, α)
P6 = (1, α3)
P7 = (α4, 1)
P8 = (α4, α)
P9 = (α4, α3)
P10 = (α, α4)
P11 = (α, α7)
P12 = (α, α5)
P13 = (α7, α4)
P14 = (α7, α7)
P15 = (α7, α5)
P16 = (α6, 1)
P17 = (α6, α)
P18 = (α6, α3)
P19 = (α5, α4)
P20 = (α5, α7)
P21 = (α5, α5)
P22 = (α2, 1)
P23 = (α2, α)
P24 = (α2, α3)
P25 = (α3, α4)
P26 = (α3, α7)
P27 = (α3, α5)

118
ALGEBRAIC–GEOMETRIC CODES
The genus of the curve from (2.12) is:
γ = (4 −1)(4 −2)
2
= 3.
From (4.3), the parameter j can vary between 2 and 6. Taking j = 4, we get a
message length k = 13 and designed minimum distance d
∗= 12. The monomial
basis from (4.5) therefore has the 13 elements {1, x, y, x2, xy, y2, x3, x2y, xy2, y3,
x3y, x2y2, xy3} and the nonsystematic generator matrix is:
G =


1 1
1 1 1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
0 0
0 1 1
1 α4 α4 α4 α
α
α α7 α7 α7 α6 α6 α6 α5 α5 α5 α2 α2 α2 α3 α3 α3
0 α6 α2 1 α α3 1
α α3 α4 α7 α5 α4 α7 α5 1
α α3 α4 α7 α5 1
α α3 α4 α7 α5
0 0
0 1 1
1
1
1
1 α2 α2 α2 α6 α6 α6 α4 α4 α4 α2 α2 α2 α4 α4 α4 α6 α6 α6
0 0
0 1 α α3 α4 α5 α7 α5 1 α6 α3 α6 α4 α6 α7 α
α α4 α2 α2 α3 α5 α7 α2 1
0 α4 α4 1 α2 α6 1 α2 α6 1 α6 α2 1 α6 α2 1 α2 α6 1 α6 α2 1 α2 α6 1 α6 α2
0 0
0 1 1
1 α4 α4 α4 α3 α3 α3 α5 α5 α5 α2 α2 α2 α7 α7 α7 α6 α6 α6 α
α
α
0 0
0 1 α α3 1
α α3 α6 α α7 α2 α5 α3 α4 α5 α7 α6 α α7 α4 α5 α7 α2 α5 α3
0 0
0 1 α2 α6 α4 α6 α2 α α7 α3 α7 α5 α α6 1 α4 α5 α3 α7 α2 α4 1 α3 α α5
0 α2 α6 1 α3 α
1 α3 α α4 α5 α7 α4 α5 α7 1 α3 α α4 α5 α7 1 α3 α α4 α5 α7
0 0
0 1 α α3 α4 α5 α7 α7 α2 1
α α4 α2 α2 α3 α5 α3 α6 α4 α6 α7 α α5 1 α6
0 0
0 1 α2 α6 1 α2 α6 α2 1 α4 α6 α4 1 α4 α6 α2 α2 1 α4 α4 α6 α2 α6 α4 1
0 0
0 1 α3 α α4 α7 α5 α5 α6 1 α3 α4 α6 α6 α α7 α α2 α4 α2 α5 α3 α7 1 α2


.
Applying Gaussian elimination to G gives:
G =


1 1 1
1 1 1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
0 1 α4 α6 α α7 α6 α α7 α2 α3 α5 α2 α3 α5 α6 α α7 α2 α3 α5 α6 α α7 α4 α3 α5
0 0 1
α 1 α5 α
1 α5 α3 1 α7 α3 1 α7 α
1 α5 α3 1 α7 α
1 α5 α3 1 α7
0 0 0
1 1 1
1
1
1 α4 α4 α4 α4 α4 α4 1
1
1 α4 α4 α4 1
1
1 α4 α4 α4
0 0 0
0 1 α4 α6 α7 α
1 α7 α2 α3 0 α7 α5 α4 α7 α5 α
0 α3 α
1 α4 α2 α
0 0 0
0 0 1 α6 α6 α7 α2 α α6 α7 α2 α6 α5 α5 α4 0 α7 α6 α3 α3 α
α
0 α6
0 0 0
0 0 0
1
1
1
0
1 α4 α4 0
1 α4 0
1 α4 1
0 α4 1
0
0 α4 1
0 0 0
0 0 0
0
1 α4 α6 α3 α4 α7 1 α2 α7 α6 α
α α2 α4 α α7 α6 α6 1 α5
0 0 0
0 0 0
0
0
1
0 α7 α7 1
1
0 α6 0
1 α2 α6 α4 α7 α5 α2 α2 α3 0
0 0 0
0 0 0
0
0
0
1
1
1 α6 α6 α6 α5 α5 α5 1
1
1 α5 α5 α5 α6 α6 α6
0 0 0
0 0 0
0
0
0
0
1 α4 0
1 α4 0
1 α4 α α6 α7 α6 α7 α α7 α α6
0 0 0
0 0 0
0
0
0
0
0
0
1
1
1 α4 α4 α4 α5 α5 α5 α3 α3 α3 α2 α2 α2
0 0 0
0 0 0
0
0
0
0
0
0
0
0
0
1
1
1 α7 α7 α7 1
1
1 α5 α5 α5


.
The swapping of rows also swaps the monomials but this does not affect the
code. After Gaussian elimination we can see that the main diagonal of 1s does not
continue at the twelfth column. Therefore, column 12 is swapped with column 13.
Next, column 16 is swapped with column 13 so that the main diagonal is complete.

DECODING ALGEBRAIC–GEOMETRIC CODES
119
After applying Jordan elimination to G:
G =


1 0 0 0 0 0 0 0 0 0 0 0 0
α5 α4 α α6 α5
0 α4 α7 α5
1 α7 α7
0 α7
0 1 0 0 0 0 0 0 0 0 0 0 0
α7 α7 α α5 α6 α3
1 α5 α2 α5 α2 α3 α6 α2
0 0 1 0 0 0 0 0 0 0 0 0 0
α4 α5 α
1
1
1 α5 α3 α5 α α4 α7 α4 α6
0 0 0 1 0 0 0 0 0 0 0 0 0
α4 α7 α4
1 α2 α3
0
0 α7 α7 α2 α4 α3 α7
0 0 0 0 1 0 0 0 0 0 0 0 0
α2 α3 α4 α7 α7 α3 α6 α2
0 α2 α3 α7
0 α2
0 0 0 0 0 1 0 0 0 0 0 0 0
α
0 α4 α2
1
0 α4 α5 α6 α2 α α2 α6 α4
0 0 0 0 0 0 1 0 0 0 0 0 0
α6 α5 α3 α3 α2 α5
1 α7
1
α α7 α7 α2 α7
0 0 0 0 0 0 0 1 0 0 0 0 0
α4
0 α3
1 α6 α6 α4 α2 α2 α3 α6
0 α7 α4
0 0 0 0 0 0 0 0 1 0 0 0 0
α3 α α3 α
0 α4
1 α4 α3 α6 α2 α6 α2 α
0 0 0 0 0 0 0 0 0 1 0 0 0
α4
1 α4 α4
1
α α7 α6 α4
0
1 α5 α2 α3
0 0 0 0 0 0 0 0 0 0 1 0 0
1 α4 α4
1 α4 α α6 α7 α6 α7 α α7 α α6
0 0 0 0 0 0 0 0 0 0 0 1 0
1
1
0
0
0
1
1
1 α5 α5 α5 α7 α7 α7
0 0 0 0 0 0 0 0 0 0 0 0 1
0
0
0
1
1 α7 α7 α7
1
1
1 α5 α5 α5


.
The swapping of columns means that the points corresponding to those columns
are also swapped. For example, when column 12 was swapped with column 13,
P12 = (α, α5) was swapped with P13 = (α7, α4). The reordered points are shown
in Table 4.4.
Table 4.4
The reordered points from Table 4.3 after swapping columns.
P1 = (0, 0)
P2 = (0, α6)
P3 = (0, α2)
P4 = (1, 1)
P5 = (1, α)
P6 = (1, α3)
P7 = (α4, 1)
P8 = (α4, α)
P9 = (α4, α3)
P10 = (α, α4)
P11 = (α, α7)
P12 = (α7, α4)
P13 = (α6, 1)
P14 = (α7, α7)
P15 = (α7, α5)
P16 = (α, α5)
P17 = (α6, α)
P18 = (α6, α3)
P19 = (α5, α4)
P20 = (α5, α7)
P21 = (α5, α5)
P22 = (α2, 1)
P23 = (α2, α)
P24 = (α2, α3)
P25 = (α3, α4)
P26 = (α3, α7)
P27 = (α3, α5)
4.3 Decoding Algebraic–Geometric Codes
The decoding of algebraic–geometric codes can be achieved in two parts: ﬁrst by
determining the error locations, and then by determining the error magnitudes at these
locations, which is similar to the decoding of Reed–Solomon codes in Chapter 3.
Sakata’s algorithm was developed in 1988 [6] as a method of generating a set
of polynomials whose coefﬁcients formed recursive relationships among an array of
ﬁnite ﬁeld elements. This was an extension of the Berlekamp–Massey algorithm to two
or more dimensions. In 1992, Justesen et al. [7] used Sakata’s algorithm to generate
a set of error-locating polynomials from a matrix containing syndrome values for
algebraic–geometric codes in order to reduce decoding complexity. The syndromes

120
ALGEBRAIC–GEOMETRIC CODES
Table 4.5
Variables used in the Sakata algorithm.
Variable
Deﬁnition
(a, b)
A point in the syndrome array
Sa,b
A syndrome in the array at the point (a, b)
F
The set containing the error-locating polynomials
f (i)(x, y)
The ith polynomial in the set F
G
The auxiliary set containing some of the polynomials that were in F earlier
in the algorithm
g(i)(x, y)
The ith polynomial in the set G
h(i)(x, y)
The ith polynomial after modiﬁcation
(ag, bg)
The point at which g(i)(x, y) was ﬁrst placed in the set G
df (dg)
The discrepancy in f (i)(x, y) (g(i)(x, y))
(t(i)
1 , t(i)
2 )
The powers of x and y respectively in the leading term of f (i)(x, y)
(u(i)
1 , u(i)
2 )
The powers of x and y respectively in the leading term of g(i)(x, y)
in the two-dimensional syndrome array are deﬁned as:
Sab =
n

i=1
rixa
i yb
i =
n

i=1
(ci + ei)xa
i yb
i =
n

i=1
eixa
i yb
i ,
(4.6)
where (a, b) is the location of the syndrome Sa,b in the array, ri is the ith received
element, ci is the ith coded symbol, ei is the error magnitude in the ith position and
(xi, yi) is the ith afﬁne point.
The majority voting scheme [8] (explained later) was added to [7] by Sakata
et al. [9] in order to increase the number of errors that could be corrected. The
decoding algorithm in [9] has many variables, so, before it is explained, a list of
deﬁnitions is provided in Table 4.5. Sakata’s algorithm takes a two-dimensional array
of syndromes calculated from the received word r = {r1,r2, . . . ,rn}, as given in (4.6).
From this array a set F of error-locating polynomials is generated, of the form:
f (i)(x, y) =

f (i)
k,l xk yl,
(4.7)
where i is the ith polynomial in F and f (i)
k,l are the coefﬁcients of the terms
xkyl in f (i)(x, y). Each syndrome from the array is read in and the polynomials in
F are modiﬁed so that the coefﬁcients in each polynomial f (i)(x, y) form recursive
equations among known syndromes, up to the current syndrome.
The recursive relationship that must be satisﬁed is [9]:

f (i)
k,l Sa−t(i)
1 +k,b−t(i)
2 +l = 0,
(4.8)
where t(i)
1
and t(i)
2
are the powers of x and y, respectively, of the leading term of
f (i)(x, y). A generalized syndrome array is shown in Figure 4.2. The syndromes are
expressed in terms of the parameter j deﬁned by (4.3). The known syndromes from

DECODING ALGEBRAIC–GEOMETRIC CODES
121
Sj,0
S0,j
Sj+1,0
Sj–1,1
Sj,1
Sm,j+1–m
Sj+m–1,0
S0,j+m
Sj+m,0
S0,j+m–1
Sm-1,j+2–m
Sj+m–1,1
S1,j+m–1
S2,j+m–2
Sj+m–2,2
Known syndromes found from
the parity check matrix
Known syndromes found from
the recursive relationship among
the previous syndromes from the
Hermitian curve
Unknown syndromes found by the 
recursive relationship from the 
Hermitian curve or majority voting
S0,0
S1,0
S2,0
S0,1
S1,1
S2,1
S0,2
S2,1
S2,2
Figure 4.2
The generalized initial syndrome array.
S0,0 to S0,j are calculated from (4.6). Further known syndromes Sj+1,0 to Sm, j−m+1
can be calculated by substituting the curve into (4.7) to obtain a recursive relationship
among the known syndromes, in order to determine the value of all syndromes Sa,b
where a ≥m, the degree of the curve. The remaining unknown syndromes Sm−1, j−m+2
to S0,j can be found using (4.8) or the majority voting scheme (explained later).
For the Hermitian curve, C(x, y) = xr+1 + yr + y with m = r + 1, the recursion
is:

k,l
Ck,lSa−t(i)
1 +k,b−t(i)
2 +l = 0
C0,1Sa−m+0,b−0+1 + C0,m−1Sa−m+0,b−0+m−1 + Cm,0Sa−m+m,b−0+0 = 0
,
where Ck,l is a coefﬁcient of C(x, y), and k and l are the powers of the x and y term,
respectively, for each term in C(x, y). Since all the coefﬁcients of C(x, y) are equal to
1, this simpliﬁes to:
Sa,b = Sa−m,b+1 + Sa−m,b+m−1.
(4.9)
(4.9) is important for the decoding procedure and is used to ﬁnd the known syn-
dromes Sj+1,0 to Sm, j+1−m, as shown in Figure 4.2.

122
ALGEBRAIC–GEOMETRIC CODES
If all the polynomials in F satisfy (4.8) then there is no need to modify F. If
a polynomial in F makes (4.7) nonzero, this polynomial is said to have a discrep-
ancy, df, and may be used in the modiﬁcation process. This is the same as  in the
Berlekamp–Massey algorithm. The order in which each syndrome in the array is read
in is determined by the total graduated degree order, denoted by <T [6]. Let (a, b) be
a point in the syndrome array. The next point, (a, b), according to the total graduated
degree order, is [6]:
(a, b) =
(a −1, b + 1),
if a > 0
(b + 1, 0),
if a = 0.
(4.10)
This gives a degree ordering of {(0, 0) <T (1, 0) <T (0, 1) <T (2, 0) <T (1, 1) . . .},
meaning that the order the syndromes are read in from the array is S0,0, S1,0, S0,1, S2,0,
S1,1 and so on.
An important set is the auxiliary set G, which stores some of the polynomials
that had a nonzero discrepancy at an earlier point in the algorithm (ag, bg). Each
polynomial g(i)(x, y) in G has a quantity called the ‘span’, deﬁned as:
span(g(i)(x, y)) =

ag −u(i)
1 , bg −u(i)
2

,
(4.11)
where u(i)
1 and u(i)
2 are the degrees of x and y, respectively, of the leading term in g(i)(x,
y). The span of a polynomial means that there is no polynomial with leading term
xag−u(i)
11 ybg−u(i)
2 that can satisfy (4.8) at the point (a, b). The union of all sets less than or
equal to each span(g(i)(x, y)) in G deﬁnes the set span(G). If there are ϕ polynomials
in G then span(G) is deﬁned as:
span(G) =
ϕ

i=1

(k,l)|(k,l) ≤span (g(i)(x, y))

,
(4.12)
where (k, l) are a pair of nonnegative integers. The span of G has interior and exterior
corners. An interior corner is deﬁned as a maximal point inside span(G) with respect
to the partial ordering, and an exterior corner is deﬁned as a minimal point outside
span(G) with respect to the partial ordering denoted by <. For two integer pairs
a = (a1, a2) and b = (b1, b2) [6]:
a < b if a1 ≤b1 ∧a2 ≤b2 ∧a 	= b.
(4.13)
For example, (3, 5) < (4, 5) because 3 ≤5 and 4 ≤5 and (3, 5) 	= (4, 5). It is
easier to see the corners by drawing span(G). An example is shown in Figure 4.3 for
span(G) = {(0, 0), (1, 0), (0, 1), (2, 0)}.
The interior corners, shown as the large black circles, are (0, 1) and (2, 0), since
there are no points in span(G) greater than either of these points. Similarly, the exterior
corners, shown as the large white circles, are (3, 0), (1, 1) and (0, 2), since there are
no points outside span(G) less than these points.

DECODING ALGEBRAIC–GEOMETRIC CODES
123
3
2
1
0
0
1
2
Figure 4.3
An example of span(G) showing interior (large black circles) and exterior (large white
circles) corners.
The number of exterior corners gives the number of polynomials in F, and their
values give the degrees of the polynomials. In this example there are three polynomials
in F with leading terms x3, xy and y2. As the set F changes there is also a corresponding
set, deﬁned as [6]:
 =
λ−1

k=1
k,
(4.14)
where λ is the total number of polynomials in F and [6]:
k =
1
(k,l)|(k,l) ≤

t(k)
1
−1, t(k+1)
2
−1
2
,
(4.15)
where (k, l) are a pair of nonnegative integers. It turns out that the set  is equal
to span(G) and is used in this decoding algorithm for the majority-voting scheme
described later.
4.3.1 Modiﬁcation of the Sets F and G
In the Sakata algorithm the set G is ﬁrst modiﬁed by calculating the spans of the
polynomials currently in G, and also the polynomials in F that have a nonzero
discrepancy. The polynomials in F with df 	= 0 are placed in a set FN and are also added
to G to create a new set G, that is G = G ∪FN. The span of each polynomial in G is
calculated using (4.11) and then used to calculate span(G) from (4.12). The interior
corners are then found. A polynomial in G with a span equal to one of the interior
corners is kept in G and the others are removed. If more than one polynomial has a
span equal to the same interior corner then either one can be kept. The discrepancy
dg of the polynomials in G and the point in the syndrome array (ag, bg) where their
discrepancies were nonzero are also stored. The set G is now modiﬁed and will be
used for the next point in the syndrome array.
The modiﬁcation of F also uses span(G). The exterior corners are found and these
will give the number of polynomials and the leading terms of these polynomials in the

124
ALGEBRAIC–GEOMETRIC CODES
modiﬁed set F. For each exterior corner (ε1, ε2) the polynomials in F are modiﬁed
using one of three possible cases [10, 11]:
Case 1
If there is a polynomial f (i)(x, y) in the difference set F/FN with (t(i)
1 , t(i)
2 ) =
(ε1, ε2) then the new polynomial h(i)(x, y) ∈F is unchanged:
h(i)(x, y) = f (i)(x, y).
(4.16)
Case 2
Else if ε1 > a or ε2 > b, ﬁnd a polynomial f (i)(x, y) ∈FN with (t(i)
1 , t(i)
2 ) ≤
(ε1, ε2). The new minimal polynomial h(i)(x, y) ∈F is given by:
h(i)(x, y) = xε1−t(i)
1 yε2−t(i)
2 f (i)(x, y).
(4.17)
Case 3
Otherwise, ﬁnd a polynomial f (i)(x, y) ∈FN with (t(i)
1 , t(i)
2 ) ≤(ε1, ε2) and
a polynomial g(i)(x, y) ∈G with span(g(i)(x, y)) ≤(a −t(i)
1 , b −t(i)
2 ). Let (p1, p2) =
span(g(i)(x, y)) −(a −ε1, b −ε2). The new minimal polynomial h(i)(x, y) ∈F is then
given by:
h(i)(x, y) = xε1−t(i)
1 yε2−t(i)
2 f (i)(x, y) −d f
dg
x p1 y p2g(i)(x, y).
(4.18)
The full algorithm can now be described [8, 11, 12]:
1. Initialize (a, b) = (0, 0), F = {1}, G = Ø, D = Ø.
2. FN = Ø, F = Ø.
Calculate the set  from (4.14) and (4.15).
If || exceeds the maximum number of errors the code can correct, terminate the
algorithm.
Else ﬁnd the discrepancy df of each polynomial f (i)(x, y) ∈F for the syndrome
Sa,b using (4.8). Every f (i)(x, y) with df 	= 0 is placed in FN and G = G ∪FN.
If all f (i)(x, y) ∈F have no discrepancy then increment (a, b) with respect to the
total order <T and go to step 2.
3. Find the span of each g(i)(x, y) ∈G using (4.10), then ﬁnd the span of G’ using
(4.11).
4. Find the interior corners of span(G) and remove all g(i)(x, y) ∈G whose spans are
not equal to any of the interior corners. The modiﬁcation of G is now complete.
Also store the discrepancies dg of each g(i)(x, y) ∈G and the point in the Sakata
algorithm where each g(i)(x, y) had a nonzero discrepancy as (ag, bg).
5. Find the exterior corners of span(G) and apply Case 1, 2 or 3 to create the
modiﬁed minimal polynomials h(i)(x, y) in the set F.
6. Set F = F, G = G and increment (a, b) with respect to the total order <T.
7. If the last element in the syndrome array has been reached end the algorithm. The
set F contains the error-locating polynomials. Else go to step 2.

MAJORITY VOTING
125
In step 2, a stopping criterion was added by the authors [11] and the decoding algo-
rithm is terminated when the number of elements in the set  exceeds the maximum
number of errors that the code can correct. This is necessary because the majority
voting scheme is unreliable if  is too large and it can choose an incorrect value for
the unknown syndrome, affecting the remainder of the decoding algorithm.
All that remains is to substitute the points from the curve into one of the error-
locating polynomials in F. The points that make the error-locating polynomial vanish
are the error locations.
4.4 Majority Voting
The Berlekamp–Massey algorithm is able to correct up to the maximum number of
errors, t, that the code is capable of correcting with the knowledge of 2t syndromes.
However, the Sakata algorithm can only correct up to:
t ≤
d∗−γ −1
2

,
(4.19)
with only the known syndromes. Unknown syndromes of the type Sa,b, a ≤m can be
calculated by substituting the curve into (4.8) to obtain a recursive relationship among
the previous syndromes, such as for the Hermitian curve in (4.9), but this can only
be used if the previous syndromes are actually known. For example, for a Hermitian
code constructed from a Hermitian curve with degree m = 5, (4.9) can be used to
calculate the syndrome S7,0:
S7,0 = S7−5,0 + S7−5,0+5−1
= S2,1 + S2,4
,
but only if the values of S2,0 and S2,4 are known. The unknown syndromes of the type
Sa,b, a < m, are determined using the majority voting scheme, presented by Feng and
Rao [8]. In this chapter, hard-decision decoding of AG codes is accomplished using the
decoding algorithm by Sakata et al. [9], which uses majority voting in the following
way: from [9] a candidate syndrome value vi from the ith minimal polynomial f (i)(x,
y) ∈F can be calculated using (4.8):

(k,l)≤T

t(i)
1 ,t(i)
2
 f (i)
k,l Sk+a−t(i)
1 ,l+b−t(i)
2 = −vi.
(4.20)
However, it is not always possible to use (4.20) so a candidate syndrome value wi
can be calculated:

(k,l)≤T

t(i)
1 ,t(i)
2
 f (i)
k,l Sk+a+m−t(i)
1 ,l+b−m+1−t(i)
2 −Sa,b−m+2 = −wi.
(4.21)

126
ALGEBRAIC–GEOMETRIC CODES
In some cases both criteria are satisﬁed and f (i)(x, y) can give two candidate values,
vi and wi. If neither of these conditions is satisﬁed then the polynomial f (i)(x, y) is not
used to ﬁnd a candidate syndrome value. Which equation is used to ﬁnd candidate
values of the syndromes depends on two conditions:
1. If a = t(i)
1 and b = t(i)
2 then use (4.20).
2. If a + m = t(i)
1 and b −t(i)
2 = m −1 then use (4.21).
Now deﬁne a set K = K1 ∪K2 [9]:
K1 = {(k,l)|0 ≤k ≤a ∧0 ≤l ≤b}
K2 = {(k,l)|0 ≤k < m ∧0 ≤l ≤b −m + 1},
(4.22)
where (k, l) are a pair of nonnegative integers. Also deﬁne two further sets:
Ai =
1
(k,l) ∈K|k + t(i)
1 ≤a ∧l + t(i)
2 ≤b
2
Bi =
1
(k,l) ∈K|k + t(i)
1 ≤a + m ∧l + t(i)
2 ≤b −m + 1
2.
(4.23)
Let δ1, δ2, δ3, . . . be the candidate syndrome values obtained from (4.20) or (4.21)
with
Kj =

3
vi=δ j
Ai ∪
3
wi=δ j
Bi


4

(4.24)
associated with each candidate syndrome value. The set Kj containing the maximum
number of elements implies that the corresponding candidate syndrome value δj is the
correct value.
4.5 Calculating the Error Magnitudes
Previously, the error magnitudes were found by solving (4.6), but this can become too
complex. Sakata et al. [9] used a two-dimensional inverse discrete Fourier transform
(IDFT) that can calculate the error magnitudes with less operations but requires the
knowledge of many unknown syndromes. The error magnitude at the ith point on the
curve ei is given as:
ei =
q−2

a=0
q−2

b=0
Sa,bx−a
i
y−b
i
,
(4.25)
where q is the size of the ﬁnite ﬁeld. However, (4.25) can only be used if an error
occurs at a point where both coordinates are nonzero. The Hermitian curves, for
example, also have points with a zero coordinate, so another method must be used

CALCULATING THE ERROR MAGNITUDES
127
when errors occurs at these points. Liu [13] addressed this problem for Hermitian
codes by using a one-dimensional IDFT, the properties of the Hermitian curve and
knowledge of more unknown syndromes up to Sq−1,q−1. The points of the Hermitian
curve are split into four types: all points with both terms nonzero are labelled P(x,x);
all points with a zero x term and a nonzero y term are labelled P(0,x); all points with
a nonzero x term and a zero y term are labelled P(x,0); the remaining point with both
terms zero is labelled P(0,0). The following mapping is deﬁned:
m →
αm,
0 ≤m ≤q −2
0,
m = q −1
.
(4.26)
For errors that occurred at points of type P(0,x) the one-dimensional IDFT is:
En =
q−2

i=0
S0,q−1−iαni,
(4.27)
where En is the sum of all the error values at the points with y-coordinate αn, and α
is a primitive element in the ﬁnite ﬁeld. Fortunately, for Hermitian curves, if there
is a point (0, αn) then there will be no points of the form (αm, αn), so En is actually
the error value that occurred at the point (0, αn). Similarly, for errors that occurred at
points with a zero y-coordinate, the one-dimensional IDFT is:
Em =
q−2

i=0
Sq−1−i,0αmi,
(4.28)
where Em is the sum of all the error values at the points with x-coordinate αm, and α
is a primitive element in the ﬁnite ﬁeld. Again, for Hermitian curves, it turns out that
there are no other points with x-coordinate αm so Em is the error value that occurred
at the point (αm, 0). Finally, if an error occurred at the point (0, 0) we can simply use
the properties of the Hermitian curve. We know that the syndrome S0,0 is the sum of
all the errors that occurred. To ﬁnd the error value at the point P(0,0) we subtract all
the error values that occurred at the points P(x,x), the error values that occurred at the
points P(0,x) and all the error values that occurred at the points P(x,0). For the Hermitian
curve we have the following relationships:

Pi∈P(x,x)
ei = Sq−1,q−1

Pi∈P(0,x)
ei = S0,q−1 −Sq−1,q−1

Pi∈P(x,0)
ei = Sq−1,0 −Sq−1,q−1
.
(4.29)

128
ALGEBRAIC–GEOMETRIC CODES
Therefore, the error at the point P1 = (0, 0) is:
e1 =

i
ei −

Pi∈P(x,x)
ei −

Pi∈P(0,x)
ei −

Pi∈P(x,0)
ei
= S0,0 −Sq−1,q−1 −(S0,q−1 −Sq−1,q−1) −(Sq−1,0 −Sq−1,q−1)
.
(4.30)
For Hermitian codes deﬁned over a ﬁnite ﬁeld with a ﬁeld characteristic of 2, this
simpliﬁes to:
e1 = S0,0 + S0,q−1 + Sq−1,0 + Sq−1,q−1.
(4.31)
4.6 Complete Hard-Decision Decoding Algorithm for Hermitian Codes
The complete AG decoding algorithm used to obtain the simulation results presented
later in this chapter can be found in [11]:
1. Find the known syndromes S0,0 to S0,j using (4.6).
2. Use (4.9) to ﬁnd more known syndromes Sj+1,0 to Sm, j+1−m.
3. Error location.
Apply the Sakata algorithm using known syndromes as the input:
For syndrome = S0,0 to Sm, j+1−m
{
Run Sakata Algorithm
if syndrome is of the form Sa,b, b ≥m −1 then ﬁnd more unknown
syndromes using (4.9)
}
Apply the Sakata Algorithm using unknown syndromes as the input:
For syndrome = Sm−1, j+2−m to S0, j+m
{
if syndrome is of the form Sa,b,a ≥m
{
Use (4.9) to ﬁnd the value of the unknown syndrome
}
else if syndrome is of the form Sa,b,a < m

COMPLETE HARD-DECISION DECODING ALGORITHM FOR HERMITIAN CODES
129
{
Use the ‘Majority Voting’ scheme
}
Run Sakata Algorithm
if syndrome is of the form Sa,b,b ≥m −1 then ﬁnd more unknown
syndromes using (4.9)
}
/* End of Sakata algorithm and majority voting */
Find the error locations by substituting each point of the Hermitian curve into
one of the minimal (error-locating) polynomials in F. The roots of the minimal
polynomial are the error locations.
4. Error magnitudes:
Find the remaining unknown syndromes:
For syndrome = Sj+m+1,0 to Sq−1,q−1
{
if syndrome is of the form Sa,b,a ≥m
{
Use (4.9) to ﬁnd the value of the unknown syndrome
}
else if syndrome is of the form Sa,b,a < m
{
Substitute the last minimal polynomial in the set F into (4.8)
to form a recursive relationship among the syndromes to the
ﬁnd the value of the unknown syndrome
}
}
Use the inverse discrete Fourier transforms to ﬁnd the error values:
 If the error location has both coordinates nonzero, use (4.25).
 Else if the error location has x-coordinate zero and y-coordinate nonzero, use
(4.27).
 Else if the error location has y-coordinate zero and x-coordinate nonzero, use
(4.28).
 Else if the error location occurred at the point (0, 0), use (4.31).
5. The error values found from step 4 at the error locations found from step 3 are
added to the received word to give the decoded code word. Since the code is
systematic, the original message can be extracted by taking the ﬁrst k symbols
from the decoded code word.

130
ALGEBRAIC–GEOMETRIC CODES
Example 4.4: Using Sakata’s algorithm with majority voting and an IDFT to
decode a Hermitian code: In this example we use the (64, 44, 15) Hermitian code
deﬁned over GF(16), which can correct up to seven symbol errors. The Hermitian
curve used is:
C(x, y) = x5 + y4 + y,
(4.32)
which, from (2.12), has a genus γ = 6 and has 64 afﬁne points that satisfy
C(x, y) = 0, given in Table 4.6.
Table 4.6
The 64 afﬁne points that satisfy C(x, y) = 0.
P1 = (0, 0)
P2 = (0, 1)
P3 = (0, α5)
P4 = (0, α10)
P5 = (1, α)
P6 = (1, α4)
P7 = (1, α2)
P8 = (1, α8)
P9 = (α, α9)
P10 = (α, α7)
P11 = (α, α6)
P12 = (α, α13)
P13 = (α4, α9)
P14 = (α4, α7)
P15 = (α4, α6)
P16 = (α4, α13)
P17 = (α2, α3)
P18 = (α2, α14)
P19 = (α2, α11)
P20 = (α2, α12)
P21 = (α8, α3)
P22 = (α8, α14)
P23 = (α8, α11)
P24 = (α8, α12)
P25 = (α5, α3)
P26 = (α5, α14)
P27 = (α5, α11)
P28 = (α5, α12)
P29 = (α10, α9)
P30 = (α10, α7)
P31 = (α10, α6)
P32 = (α10, α13)
P33 = (α3, α)
P34 = (α3, α4)
P35 = (α3, α2)
P36 = (α3, α8)
P37 = (α14, α3)
P38 = (α14, α14)
P39 = (α14, α11)
P40 = (α14, α12)
P41 = (α9, α)
P42 = (α9, α4)
P43 = (α9, α2)
P44 = (α9, α8)
P45 = (α7, α9)
P46 = (α7, α7)
P47 = (α7, α6)
P48 = (α7, α13)
P49 = (α6, α)
P50 = (α6, α4)
P51 = (α6, α2)
P52 = (α6, α8)
P53 = (α13, α9)
P54 = (α13, α7)
P55 = (α13, α6)
P56 = (α13, α)
P57 = (α11, α3)
P58 = (α11, α14)
P59 = (α11, α11)
P60 = (α11, α12)
P61 = (α12, α)
P62 = (α12, α4)
P63 = (α12, α2)
P64 = (α12, α8)
From (4.3), the parameter j can have any value from j = 3 to j = 12. Choosing
j = 5 and substituting this into (4.4) we get a message length of k = 44 and a
designed minimum distance of d
∗= 15. It is assumed that the received word has
errors at the following afﬁne points:
e6 = α12 at P6 = (1, α4)
e13 = α12 at P13 = (α4, α9)
e27 = α12 at P27 = (α5, α11)
e32 = α12 at P32 = (α10, α13)
e33 = α12 at P33 = (α3, α)
e45 = α12 at P45 = (α7, α9)
e51 = α12 at P51 = (α6, α2)
The known syndromes Sa,b, where a + b ≤j = 5, can be calculated using (4.6).
The initial syndromes are shown in Figure 4.4. The syndromes S6,0 and S5,1 are
found using the recursive relationship of (4.9):
Sa,b = Sa−5,b+1 + Sa−5,b+4.

COMPLETE HARD-DECISION DECODING ALGORITHM FOR HERMITIAN CODES
131
α3
α9
0
α6
0
0
α2 α10 α14 α3 α4
α12
α7
α4
α8
α8
α11 α4 α12
α2
α10
α2
2
1
0
5
4
3
6
0
1
2
3
4
5
a
b
α12
Figure 4.4
Initial syndrome array.
Sa,b = Sa−5,b+1 + Sa−5,b+4
Therefore,
S6,0 = S1,1 + S1,4 = α10 + α10 = 0
S5,1 = S1,1 + S1,4 = α7 + α2 = (α3 + α + 1) + α2 = α12 .
The parameters in Sakata’s algorithm are then initialized as follows: (a, b) =
(0, 0), F = {1}, G = Ø and  = Ø, FN = Ø.
From (4.8), the discrepancy df of f (1) = 1 when Sa,b = α12 is determined to be:
d f =

(k,l)≤T (t(1)
1 ,t(1)
2 )
f (i)
k,l Sa−t(i)
1 +k,b−t(i)
2 +l = f (1)
0,0 S0−0+0,0−0+0 = 1.S0,0 = α12.
Hence, FN = {1} and G = G ∪FN = {1}, and we also store this discrepancy,
dg = df, and the point at which the discrepancy occurred, (ag, bg) = (0, 0).
Next, span(G) is found using (4.11) and (4.12). The x-degree and y-degree of
g(x, y) = 1 are u(1)
1 = 0 and u(1)
2 = 0, respectively, and so span(1) = (0 −0, 0 −
0) = (0, 0}.
Therefore,
span(G) =
ϕ

i=1

(k,l)|(k,l) ≤span(g(i)(x, y))

=
1

i=1
{(k,l)|(k,l) ≤(0, 0)} = {(0, 0)}.
If we represent span(G) graphically, as in Figure 4.5, we can see that it has only
one interior corner, at (0, 0), and two exterior corners, at (1, 0) and (0, 1).
Without performing any of the update procedures for sets F and G, we know that
since there is only one interior corner, (0, 0), G will contain a single polynomial

132
ALGEBRAIC–GEOMETRIC CODES
0
2
1
0
1
2
k
l
Figure 4.5
The set  after the ﬁrst modiﬁcation to the polynomials in F.
with a span equal to (0, 0). Also, since there are two exterior corners, (1, 0) and
(0, 1), F will contain two polynomials with leading monomials x and y, respectively.
To update G, we keep only those polynomials in G that have spans equal to all in-
terior corners of span(G). In this case, the polynomial g = 1 is the only polynomial
in G and its span is equal to (0, 0), so this is kept for G. This completes the updating
of G.
To update F, we take each exterior corner of span(G) and apply Case 1, 2 or 3.
For the exterior corner (ε1, ε2) = (1, 0) we cannot apply Case 1 since the difference
set F/FN is empty. However, we can apply Case 2, since ε1 > a = 0, so:
h(1)(x, y) = xε1−t(i)
1 yε2−t(i)
2 f (1)(x, y) = x1−0y0−01 = x.
Similarly, for the exterior corner (ε1, ε2) = (0, 1), Case 2 can also be applied,
since ε2 > b, so:
h(2)(x, y) = xε1−t(i)
1 yε2−t(i)
2 f (1)(x, y) = x0−0y1−01 = y.
Therefore, for (a, b) = (1, 0) we have F = {x, y), G = {1} and  = {(0, 0)}. The
outputs of the algorithm for all the syndromes up to the ﬁrst unknown syndrome
S4,2 are given in Table 4.7.
Observations from Table 4.7:
 The set F always contains one more polynomial that the set G.
 For a t-error-correcting AG code, the number of elements in  never exceeds
t if there are no more that t errors in the received word.
 Once || = t, the number of polynomials in F and their leading monomials
never change. The increasing size of  is shown in Figure 4.6.
The polynomials in F are now used to determine the value of the ﬁrst unknown
syndrome, S42. The ﬁrst polynomial in F is f (1)(x, y) = α14 + αx + α7y + α4x2 +
xy + α7x3 + αx2y + x4, with leading degree (t(1)
1 , t(2)
2 ) = (4, 0). A candidate value

COMPLETE HARD-DECISION DECODING ALGORITHM FOR HERMITIAN CODES
133
Table 4.7
Modiﬁcation of the set of minimal polynomials using Sakata’s algorithm for each
syndrome in the array of Figure 4.4.
a, b
Sa,b
F
df

G
dg
ag, bg
0, 0
α12
1
α12
æ
æ
—
—
1, 0
α3
x
y
α3
0
{(0, 0)}
1
α12
0, 0
0, 1
α2
α6+x
y
0
α2
{(0, 0)}
1
α12
0, 0
2, 0
α9
α6 + x
α5 + y
0
0
{(0, 0)}
1
α12
0, 0
1, 1
α10
α6 + x
α5 + y
α
α
{(0, 0)}
1
α12
0, 0
0, 2
α7
α6x + x2
α4 + α6y + xy
α5y + y2
0
0
0
{(0, 0)(1, 0),
(0, 1)}
α6 + x
α5 + y
α
α
1, 1
1, 1
3, 0
0
α6x + x2
α4 + α6y + xy
α5y + y2
1
0
0
{(0, 0)(1, 0),
(0, 1)}
α6 + x
α5 + y
α
α
1, 1
1, 1
2, 1
α14
α4 + α6x + α14y + x2
α4 + α6y + xy
α5y + y2
α7
0
0
{(0, 0)(1, 0),
(0, 1)}
α6 + x
α5+y
α
α
1, 1
1, 1
1, 2
α4
α6 + α14y + x2
α4 + α6y + xy
α5y + y2
0
α
α
{(0, 0)(1, 0),
(0, 1)}
α6 + x
α5 + y
α
α
1, 1
1, 1
0, 3
α11
α6 + α14y + x2
α12 + x + α6y + xy
α5 + α10y + y2
0
0
1
{(0, 0)(1, 0),
(0, 1)}
α6 + x
α5 + y
α
α
1, 1
1, 1
4, 0
α6
α6 + α14y + x2
α12 + x + α6y + xy
α14x + α10y + y2
0
0
0
{(0, 0)(1, 0),
(0, 1)}
α6 + x
α5 + y
α
α
1, 1
1, 1
3, 1
α3
α6 + α14y + x2
α12 + x + α6y + xy
α14x + α10y + y2
α
α
0
{(0, 0)(1, 0),
(0, 1)}
α6 + x
α5 + y
α
α
1, 1
1, 1
2, 2
α8
α6 + α13x + α14xy + x3
α5 + α13y + α14y2 + x2y
α14x + α10y + y2
0
α12
α12
{(0, 0), (1, 0),
(0, 1), (2, 0),
(1, 1)}
α6 + α14y + x2
α12 + x + α6y
+ xy
α
α
3, 1
3, 1
1, 3
α4
α6 + α13x + α14xy + x3
α5 + α2x + α13y + α10xy
+ α14y2 + α11x3 + x2y
α8 + α10x + α4y + α11xy
+ y2
0
0
α10
{(0, 0), (1, 0),
(0, 1), (2, 0),
(1, 1)}
α6 + α14y + x2
α12 + x + α6y
+ xy
α
α
3, 1
3, 1
(Continued)

134
ALGEBRAIC–GEOMETRIC CODES
Table 4.7
(Continued)
a, b
Sa,b
F
df

G
dg
ag, bg
0, 4
α11
α6 + α13x + α14xy + x3
α5 + α2x + α13y + α10xy
+ α14y2 + α11x3 + x2y
α2 + α10x + α5y + α9x2
+ α11xy + y2
0
0
0
{(0, 0), (1, 0),
(0, 1), (2, 0),
(1, 1)}
α6 + α14y + x2
α12 + x + α6y
+ xy
α
α
3, 1
3, 1
5, 0
0
α6 + α13x + α14xy + x3
α5 + α2x + α13y + α10xy
+ α14y2 + α11x3 + x2y
α2 + α10x + α5y + α9x2
+ α11xy + y2
α8
0
0
{(0, 0), (1, 0),
(0, 1), (2, 0),
(1, 1)}
α6 + α14y + x2
α12 + x + α6y
+ xy
α
α
3, 1
3, 1
4, 1
α4
α12 + α5x + α13y + αxy + x3
α5 + α2x + α13y + α10xy
+ α14y2 + α11x3 + x2y
α2 + α10x + α5y + α9x2
+ α11xy + y2
α8
α4
0
{(0, 0), (1, 0),
(0, 1), (2, 0),
(1, 1)}
α6 + α14y + x2
α12 + x + α6y
+ xy
α
α
3, 1
3, 1
3, 2
α8
α + α5x + y + α7x2 + αxy
+ x3
α10 + α6x + α10y + α12xy
+ α14y2 + α11x3 + x2y
α2 + α10x + α5y + α9x2
+ α11xy + y2
α4
α4
α4
{(0, 0), (1, 0),
(0, 1), 2, 0),
(1, 1)}
α6 + α14y + x2
α12 + x + α6y
+ xy
α
α
3, 1
3, 1
2, 3
α12
αx + α5x2 + xy + α7x3
+ αx2y + x4
α13 + α6x + α4y + α3x2
+ α12xy + α14y2 + α11x3
+ x2y
1 + α6x + α9y + α10x2
+ α11xy + α9x3 + α11x2y
+ xy2
α2y + α10xy + α5y2 + α9x2y
+ α11xy2 + y3
0
α
α4
α4
{(0, 0), (1, 0),
(0, 1), (2, 0),
(1, 1), (0, 2),
(3, 0)}
α + α5x + y
+ α7x2 + αxy
+ x3
α6 + α14y + x2
α2 + α10x
+ α5y + α9x2
+ α11xy + y2
α4
α
α4
3, 2
3, 1
3, 2
1, 4
α10
αx + α5x2 + xy + α7x3
+ αx2y + x4
α3x + α6y + α7x2 + αxy
+ α14y2 + x3 + x2y
α7 + α6x + α11y + α12x2
+ α11xy + α9x3 + α11x2y
+ xy2
α2x + α2y + α10x2 + xy
+ α5y2 + α9x3 + α2x2y
+ α12xy2 + y3
0
0
α2
α7
{(0, 0), (1, 0),
(0, 1), (2, 0),
(1, 1), (0, 2),
(3, 0)}
α + α5x + y
+ α7x2 + αxy
+ x3
α6 + α14y + x2
α2 + α10x
+ α5y + α9x2
+ α11xy + y2
α4
α
α4
3, 2
3, 1
3, 2
(Continued)

COMPLETE HARD-DECISION DECODING ALGORITHM FOR HERMITIAN CODES
135
Table 4.7
(Continued)
a, b
Sa,b
F
df

G
dg
ag, bg
0, 5
α2
αx + α5x2 + xy + α7x3
+ αx2y + x4
α3x + α6y + α7x2 + αxy
+ α14y2 + x3 + x2y
α + α2x + α4y + α14x2
+ α10xy + α10x3 + α11x2y
+ xy2
α12 + α2x + αy + α7x2 + xy
+ α5y2 + α9x3 + α2x2y
+ α12xy2 + y3
0
0
0
α7
{(0, 0), (1, 0),
(0, 1), (2, 0),
(1, 1), (0, 2),
(3, 0)}
α + α5x + y
+ α7x2 + αxy
+ x3
α6 + α14y + x2
α2 + α10x
+ α5y + α9x2
+ α11xy + y2
α4
α
α4
3, 2
3, 1
3, 2
6, 0
0
αx + α5x2 + xy + α7x3
+ αx2y + x4
α3x + α6y + α7x2 + αxy
+ α14y2 + x3 + x2y
α + α2x + α4y + α14x2
+ α10xy + α10x3 + α11x2y
+ xy2
α6 + x + α9y + α6x2 + αxy
+ α5y2 + αx3 + α2x2y
+ α12xy2 + y3
0
0
0
0
{(0, 0), (1, 0),
(0, 1), (2, 0),
(1, 1), (0, 2),
(3, 0)}
α + α5x + y
+ α7x2 + αxy
+ x3
α6 + α14y + x2
α2 + α10x
+ α5y + α9x2
+ α11xy + y2
α4
α
α4
3, 2
3, 1
3, 2
5, 1
α12
αx + α5x2 + xy + α7x3
+ αx2y + x4
α3x + α6y + α7x2 + αxy
+ α14y2 + x3 + x2y
α + α2x + α4y + α14x2
+ α10xy + α10x3 + α11x2y
+ xy2
α6 + x + α9y + α6x2 + αxy
+ α5y2 + αx3 + α2x2y
+ α12xy2 + y3
α9
α
0
0
{(0, 0), (1, 0),
(0, 1), (2, 0),
(1, 1), (0, 2),
(3, 0)}
α + α5x + y
+ α7x2 + αxy
+ x3
α6 + α14y + x2
α2 + α10x
+ α5y + α9x2
+ α11xy + y2
α4
α
α4
3, 2
3, 1
3, 2
4, 2
?
α14 + αx + α7y + α4x2 + xy
+ α7x3 + αx2y + x4
α14 + α4x + α3y + α10x2
+ α10xy + α5y2 + x3 + x2y
α + α2x + α4y + α14x2
+ α10xy + α10x3 + α11x2y
+ xy2
α6 + x + α9y + α6x2 + αxy
+ α5y2 + αx3 + α2x2y
+ α12xy2 + y3
?
?
?
?
{(0, 0), (1, 0),
(0, 1), (2, 0),
(1, 1), (0, 2),
(3, 0)}
α + α5x + y
+ α7x2 + αxy
+ x3
α6 + α14y + x2
α2 + α10x
+ α5y + α9x2
+ α11xy + y2
α4
α
α4
3, 2
3, 1
3, 2

136
ALGEBRAIC–GEOMETRIC CODES
0
2
1
0
1
2
k
l
3
3
0
2
1
0
1
2
4
k
l
3
3
0
2
1
0
1
2
k
l
3
4
5
4
3
Figure 4.6
Graphical representation of the set  at different stages of Sakata’s algorithm from
Table 4.7.
for S42 can be obtained by using (4.20):

(k,l)≤T (4,0)
f (1)
k,l Sk+4−4,l+2−0 = −v1
= α14S0,2 + αS1,2 + α7S0,3 + α4S2,2 + 1.S1,3 + α7S3,2 + αS2,3
= α14α7 + αα4 + α7α11 + α4α8 + α4 + α7α8 + αα12
= α6 + α5 + α3 + α12 + α4 + 1 + α13
= (α3 + α2) + (α2 + α) + α3 + (α3 + α2 + α + 1) + (α + 1) + 1
+(α3 + α2 + 1)
v1 = α.
(4.20) can also be used to obtain candidate values for the second and third
polynomials in F; these are v2 = α and v3 = α.
However, the fourth polynomial in F, α6 + x + α9y + α6x2 + αxy + α5y2 + αx3 +
α2x2y + α12xy2 + y3, which has leading degree (t(1)
1 , t(2)
2 ) = (0, 3), cannot be used
to obtain a candidate value using either (4.20) or (4.21). Since the contributing
polynomials all give the same candidate value, the syndrome S42 = α. For S33, all
contributing polynomials also give the same candidate value of α6.
The ﬁrst unknown syndrome value where all contributing polynomials give
different candidate values is S24. Since the ﬁrst polynomial in F has leading degree

COMPLETE HARD-DECISION DECODING ALGORITHM FOR HERMITIAN CODES
137
(t(1)
1 , t(2)
2 ) = (4, 0), (4.20) can be used to obtain a candidate value:

(k,l)≤T (4,0)
f (1)
k,l Sk+2+5−4,l+4−5+1−0 −S2,4−5+2 = −w1
= α14S3,0 + αS4,0 + α7S3,1 + α4S5,0 + 1 · S4,1 + α7S6,0 + αS5,1 + S2,1
= α14 · 0 + α · α6 + α7 · α3 + α4 · 0 + 1 · α4 + α7 · 0 + α · α12 + α14
= α7 + α10 + α4 + α13 + α14
= (α3 + α + 1) + (α2 + α + 1) + (α + 1) + (α3 + α2 + 1) + (α3 + 1)
= α3 + α + 1
w1 = α7.
For the other three contributing polynomials in F, (4.20) can be used to obtain
the candidate value of α11. Now, from (4.23):
K1 = {(k,l) | 0 ≤k ≤2 ∧0 ≤l ≤4}
= {(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (1, 1), (1, 2), (1, 3), (1, 4),
×(2, 0), (2, 1), (2, 2), (2, 3), (2, 4)}
K2 = {(k,l) | 0 ≤k < 5 ∧0 ≤l ≤4 −5 + 1}
= {(0, 0), (1, 0), (2, 0), (3, 0), (4, 0)}
,
and the set K = K1 ∪K2 is:
K = {(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (1, 0), (1, 1), (1, 2), (1, 3), (1, 4),
× (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (3, 0), (4, 0)}.
From (4.22) the sets A and B are:
A1 = {(k,l) ∈K | k + 4 ≤2 ∧l + 0 ≤4} = Ø
A2 = {(k,l) ∈K | k + 2 ≤2 ∧l + 1 ≤4} = {(0, 0), (0, 1), (0, 2), (0, 3)}
A3 = {(k,l) ∈K | k + 1 ≤2 ∧l + 2 ≤4} = {(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2)}
A4 = {(k,l) ∈K | k + 0 ≤2 ∧l + 3 ≤4} = {(0, 0), (0, 1), (1, 0), (1, 1), (2, 0), (2, 1)}
B1 = {(k,l) ∈K | k + 4 ≤7 ∧l + 0 ≤0} = {(0, 0), (1, 0), (2, 0), (3, 0)}
B2 = {(k,l) ∈K | k + 2 ≤7 ∧l + 1 ≤0} = Ø
B3 = {(k,l) ∈K | k + 1 ≤7 ∧l + 2 ≤0} = Ø
B4 = {(k,l) ∈K | k + 0 ≤7 ∧l + 3 ≤0} = Ø
.
The candidate syndromes are δ1 = α7 and δ2 = α11. Finally, from (4.24):
K1 =
5
 
vi=α7 Ai ∪ 
wi=α7 Bi
!
/{(0, 0), (1, 0), (0, 1), (2, 0), (1, 1), (0, 2), (3, 0)}
= {(0, 0), (1, 0), (2, 0), (3, 0)}/{(0, 0), (1, 0), (0, 1), (2, 0), (1, 1), (0, 2), (3, 0)}
= Ø

138
ALGEBRAIC–GEOMETRIC CODES
α12 α3
α9
0
α6
0
0
α2 α10 α14
α3
α4 α12
α7
α4
α8
α8
α11 α4
α12
α2
α10
α2
2
1
0
5
4
3
6
0
1
2
3
4
5
a
α10
α11
α7
α4
α11
α14
α5
α13
α11
α8
α11
α7
α6
α14
0
α
α
α
α4
α8
α14
α9
α12
α13
α9
1
α7
α12
α8
α2
α12
α2
α4
0
1
α8
α8
α11
α10
7
10
9
8
6
7
8
9
10
α10
0
0
1
b
Figure 4.7
The syndrome array after majority voting has been completed.
K2 =
5
 
vi=α11 Ai ∪
 
wi=α11 Bi
!
\{(0, 0), (1, 0), (0, 1), (2, 0), (1, 1), (0, 2), (3, 0)}
= A2 ∪A3 ∪A4\{(0, 0), (1, 0), (0, 1), (2, 0), (1, 1), (0, 2), (3, 0)}
= {(0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1)}/
{(0, 0), (1, 0), (0, 1), (2, 0), (1, 1), (0, 2), (3, 0)}
= {(0, 3), (1, 2), (2, 1)}
.
Therefore, there are | K1 | = 0 votes for the candidate value α7 and | K2 | = 3
votes for the candidate value α11, and so S24 = α11. By employing majority voting
up to the last unknown syndrome, S0,10, we obtain the syndrome array shown in
Figure 4.7. The ﬁnal set of minimal polynomials is:
F =



f (1)(x, y) = α14 + αx + α7y + α4x2 + xy + α7x3 + αx2y + x4
f (2)(x, y) = α14 + α4x + α3y + α10x2 + α10xy + α5y2 + x3 + x2y
f (3)(x, y) = α + α2x + α4y + α14x2 + α10xy + α10x3 + α11x2y + xy2
f (4)(x, y) = α6 + x + α9y + α6x2 + αxy + α5y2 + αx3 + α2x2y
+ α12xy2 + y3
.
All four polynomials in F are error-locating polynomials, which means that
those afﬁne points that cause any of these polynomials to vanish are possible error
locations. If we take the polynomial f (4)(x, y), there are 12 points that cause it to
vanish:
P6 = (1, α4)
P11 = (α, α6)
P13 = (α4, α9)
P27 = (α5, α11)
P31 = (α10, α6)
P32 = (α10, α13)
P33 = (α3, α)
P45 = (α7, α9)
P51 = (α6, α2)
P52 = (α6, α8)
P57 = (α11, α3)
P63 = (α12, α2)

COMPLETE HARD-DECISION DECODING ALGORITHM FOR HERMITIAN CODES
139
The remaining syndromes from S11,0 to S15,15 are found either by using (4.9) for
Sa,b, a ≥5, or by substituting the minimal polynomial f (4)(x, y) in F into (4.8) to
obtain the recursive equation:

(k,l)≤T (0,3)
f (4)
k,l Sa−0+k,b−3+l = 0
f0,0Sa,b−3 + f1,0Sa+1,b−3 + f0,1Sa,b−2 + f2,0Sa+2,b−3 + f1,1Sa+1,b−2
+ f0,2Sa,b−1 + f3,0Sa+3,b−3 + f2,1Sa+2,b−2 + f1,2Sa+1,b−1 + f0,3Sa,b.
Making Sa,b the subject of the equation gives:
Sa,b = α6Sa+1,b−3 + Sa+1,b−3 + α9Sa,b−2 + α6Sa+2,b−3 + αSa+1,b−2
+ α5Sa,b−1 + αSa+3,b−3 + α2Sa+2,b−2 + α12Sa+1,b−1.
(4.33)
So the syndromes S11,0, S10,1, S9,2, S8,3, S7,4, S6,5 and S5,6 are calculated using
(4.9) and have the values α11, α13, α13, α13, 0 and α5, respectively. To calculate
S4,7 we would use (4.33):
S4,7 = α6S4,4 + S5,4 + α9S4,5 + α6S6,4 + αS5,5 + α5S4,6 + αS7,4
+ α2S6,5 + α12S5,6
= α6α4 + α12 + α9α8 + α6α4 + αα2 + α5α12 + α · 0 + α2α5 + α12α8
= α10 + α12 + α2 + α10 + α3 + α2 + α7 + α5
= α12 + α3 + α7 + α5
= (α3 + α2 + α + 1) + α3 + (α3 + α + 1) + (α2 + α)
= α3 + α
= α9
.
The complete syndrome array is given in Figure 4.8.
All that remains is to ﬁnd the error magnitudes at the possible error locations.
Taking the point P6 = (1, α4), we use (4.24) to determine the magnitude:
e6 =
14

a=0
14

b=0
Sa,bx−a
6 y−b
6
=
q−2

a=0
q−2

b=0
Sa,b1−a · α−4b
= S0,010α0 + S1,010α−4 + S2,010α−8 + · · · S14,121−14α−3
+ S14,131−14α−7 + S14,141−14α−11
.
This gives a value of e6 = α12. Similar calculations for the remaining points
give:
e6 = α12 at P6 = (1, α4)
e11 = 0 at P11 = (α, α6)
e13 =α12 at P13 = (α4, α9)
e27 = α12 at P27 = (α5, α11)
e31 = 0 at P31 = (α10, α6)
e32 = α12 at P32 = (α10, α13)
e33 =α12 at P33 = (α3, α)
e45 = α12 at P45 = (α7, α9)
e51 = α12 at P51 = (α6, α2)
e52 = 0 at P52 = (α6, α8)
e57 = 0 at P57 = (α11, α3)
e63 = 0 at P63 = (α12, α2)

140
ALGEBRAIC–GEOMETRIC CODES
α12 α3
α9
0
α6
0
0
α2 α10 α14
α3
α4
α12
α7
α4
α8
α8
α11 α4
α12
α2
α10
α2
2
1
0
5
4
3
6
0
1
2
3
4
5
a
b
α10
α11
α7
α4
α11
α14
α5
α13
α11
α8
α11
α7
α6
α14
0
α
α
α
α4
α8
α14
α9
α12
α13
α9
1
α7
α12
α8
α2
α12
α2
α4
0
1
α8
α8
α11
α10
7
10
9
8
6
7
8
9
10
α4
α8
α8
α10
α11
0
α4
α8
α10 α10
α2
α13
1
α3
α
α7
α
α13
α
α10
α4
α2
α10
α14
α2
0
α9
1
α6
α8
α5
α10
α8
α3
α
α3
α4
α
α12
α7
α2 α13 α11 α2
1
α7
α8
α6
0
α7
α7
α9
1
α4
α3
α11
α5
α9
1
α13
α8
α3
α2
α11 α8
α11
α11
α
α9
1
α4
α11 α9
1
α
α6
α14
α13
α5
α11
α5
α14
α
α14
α14
α10
α
α7
α6
α9
α13
α2
α10
α14
α6
0
α3
0
α4
α3
α7
α11
α8
α4
1
α9
α6
α12
α3
α10 α4
0
α10
α11
α13
α6
α9
α3
α13 α6
α5
0
α2
α13 α12
0
α14 α7
0
0
α10
α6
α13
α7
α6
1
α3
α4
α4
α9
0
α5
α9
α3
α14
α14 α6
α5
α5
α10 α5
1
α5
α8
α5
α
α11
14
13
12
11
11
12
13
14
α5
15
15
α12 α3
α9
0
α6
0
0
α10
1
0
0
α11 0
α4
α8
α10
0
α12
α7
α8
α8
α10
α2
α7
α11
α2
α2
α10
α11
α7
α4
α12
0
1
α13
α4
Figure 4.8
The complete syndrome array.
4.7 Simulation Results
In this section, we present some simulation results evaluating the performance of
Hermitian codes in comparison with Reed–Solomon codes. It is obvious that if we
compare Hermitian codes with Reed–Solomon codes deﬁned over the same ﬁnite
ﬁeld then the Hermitian codes will always perform better due to their increased
length and larger Hamming distance. However, it is interesting to compare Hermi-
tian codes deﬁned over smaller ﬁnite ﬁelds and Reed–Solomon codes deﬁned over
larger ﬁnite ﬁelds. A Hermitian code deﬁned over GF(64) has much better parameters
than a Reed–Solomon code deﬁned over GF(64), but it also has better parameters
than a Reed–Solomon code deﬁned over GF(256). For example, the (512, 314, 171)
Hermitian code deﬁned over GF(64) of rate 0.6 has a much larger minimum Ham-
ming distance than the (63, 39, 25) Reed–Solomon code deﬁned over GF(64), and
also a larger Hamming distance than the (255, 153, 103) Reed–Solomon code deﬁned
over GF(256). This illustrates another advantage of Hermitian codes in that improved
performance can be achieved with smaller alphabets, consequently reducing the com-
plexity of the ﬁnite ﬁeld arithmetic for hardware implementation.

SIMULATION RESULTS
141
1.00E–06
1.00E–05
1.00E–04
1.00E–03
1.00E–02
1.00E–01
1.00E+00
12
11
10
9
8
7
6
5
4
3
2
1
0
Eb/N0, dB
BER
Uncoded QPSK
Shannon limit
(512,314,171) Hermitian code
(255,153,103) Reed–Solomon code
Figure 4.9
Performance comparison of the (512, 314, 171) Hermitian code deﬁned over GF(64) with
the (255, 153, 103) Reed–Solomon code deﬁned over GF(256) on the AWGN channel.
A comparison of the (512, 314, 171) Hermitian code and (255, 153, 103)
Reed–Solomon code is now presented in terms of simulation results on the AWGN
channel and a Rayleigh fading channel. In Figure 4.9, the performance of the two
codes is shown on the AWGN channel, with the Hermitian code achieving only a
0.3 dB coding gain over the Reed–Solomon code.
However, under slow fading conditions where bursts of errors are more likely
to occur it can be shown that the Hermitian code can signiﬁcantly outperform the
Reed–Solomon code. In Figure 4.10 the performance of the two codes is shown on
a Rayleigh fading channel that is frequency nonselective, with a carrier frequency
of 1.9 GHz, a velocity of 20 m/s and perfect channel estimation for a data rate of
30 kbps. It can be seen that the Hermitian code achieves a coding gain of 4.8 dB over
the Reed–Solomon code.
In Figure 4.11 the performance of the two codes is shown on the same Rayleigh
fading channel, but with an increased data rate of 100 kbps. Under more harsh condi-
tions the Hermitian code still achieves a coding gain of 3.1 dB over the Reed–Solomon
code, at a BER of 10−6.
Finally, we also present the performance of the same Hermitian codes on magnetic
recording channels [12], as described in Chapter 1. In Figure 4.12, the performances of
the (64, 39, 20) Hermitian code and the (512, 314, 171) Hermitian code are compared
with the (15, 9, 7), (63, 39, 25) and (255, 153, 103) Reed–Solomon codes on a magnetic
recording channel with a recording linear density of Ds = 2, which is a measure of the
intersymbol interference (ISI). The signal-to-noise ratio Et/N0 is the ratio of the energy

142
ALGEBRAIC–GEOMETRIC CODES
1.00E–06
1.00E–05
1.00E–04
1.00E–03
1.00E–02
1.00E–01
1.00E+00
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
Eb/N0, dB
BER
Uncoded QPSK
(512,314,171) Hermitian code
(255,153,103) Reed–Solomon code
Figure 4.10
Performance comparison of the (512, 314, 171) Hermitian code deﬁned over GF(64) with
the (255, 153, 103) Reed–Solomon code deﬁned over GF(256) on a Rayleigh fading channel.
in the Lorentzian Pulse Et to the noise power spectral density N0 of the electronics
noise. As stated before, the Hermitian codes outperform the Reed–Solomon codes
deﬁned over the same ﬁnite ﬁelds due to their increased lengths. Interestingly, the
(512, 314, 171) Hermitian code deﬁned over GF(64) still outperforms the (255, 153,
103) Reed–Solomon code deﬁned over GF(256) on magnetic recording channels.
1.00E–06
1.00E–05
1.00E–04
1.00E–03
1.00E–02
1.00E–01
1.00E+00
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
Eb/N0, dB
BER
(512,314,171) Hermitian code
(255,153,103) Reed–Solomon code
Uncoded QPSK
Figure 4.11
Performance comparison of the (512, 314, 171) Hermitian code deﬁned over GF(64) with
the (255, 153, 103) Reed–Solomon code deﬁned over GF(256) on a Rayleigh fading channel.

SIMULATION RESULTS
143
1.00E–06
1.00E–05
1.00E–04
1.00E–03
1.00E–02
1.00E–01
1.00E+00
27
26
25
24
23
22
21
20
19
18
17
16
15
14
13
12
11
10
E  /N
t
0, dB
BER
EPR4 target, Ds=2
(15,9)RS code
(63,39)RS code
(64,39)AG code
(255,153)RS code
(512,314)AG code
Figure 4.12
Performance comparison of Hermitian codes and Reed–Solomon codes on a magnetic
recording channel with a recording linear density of Ds = 2.
Figure 4.13 shows the performance of the same codes on a magnetic recording
channel with a recording linear density of Ds = 3, representing an increase in ISI.
Overall, the performance of all the codes is worse but the Hermitian codes still
outperform the Reed–Solomon codes.
1.00E–06
1.00E–05
1.00E–04
1.00E–03
1.00E–02
1.00E–01
1.00E+00
29
28
27
26
25
24
23
22
21
20
19
18
17
16
15
14
13
12
11
10
E  /N
t
0, dB
BER
EPR4 target, Ds=3
(15,9)RS code
(63,39)RS code
(64,39)AG code
(255,153)RS code
(512,314)AG code
Figure 4.13
Performance comparison of Hermitian codes and Reed–Solomon codes on a magnetic
recording channel with a recording linear density of Ds = 3.

144
ALGEBRAIC–GEOMETRIC CODES
4.8 Conclusions
A class of non-binary block code called the algebraic–geometric code has been
introduced. Such codes are constructed from the afﬁne points of a projective curve
and a set of rational functions deﬁned on that curve. Depending on the choice of
curve, very long AG codes can be constructed with very large minimum Hamming
distances. The well-known Reed–Solomon codes are the simplest class of AG code,
constructed from the afﬁne points of the projective line. Consequently, they have
the shortest block lengths of all AG codes, and there are not many Reed–Solomon
codes that can be constructed. However, Reed–Solomon codes are maximum distance
separable (MDS) unlike other AG codes, where the genus of the curve reduces the
actual minimum Hamming distance.
Despite this genus penalty, AG codes still have much larger minimum Hamming dis-
tances than Reed–Solomon codes deﬁned over the same ﬁnite ﬁeld and consequently
AG codes can correct much longer bursts of errors, which are common in data storage
channels and slow fading channels. A disadvantage of AG codes is their higher de-
coding complexity. Sakata’s algorithm is more complex than the Berlekamp–Massey
algorithm, but Kotter recently showed how several Berlekamp–Massey decoders could
be implemented in parallel, replacing Sakata’s algorithm and having a running time
equal to a single Berlekamp–Massey decoder.
In this chapter, we have covered the construction of AG codes from Hermitian
curves. A detailed explanation of the decoding of AG codes has also been presented,
in particular the use of Sakata’s algorithm to locate errors, and the use of inverse
discrete Fourier transforms to determine the error magnitudes at these locations.
Finally, some simulation results have been presented on the AWGN and Rayleigh
fading channels, showing how a Hermitian curve deﬁned over GF(64) can outperform
a Reed–Solomon code deﬁned over the much larger ﬁnite ﬁeld GF(256).
Reed–Solomon codes are still the most commonly used coding scheme today,
but they are rapidly being replaced in communication systems with more powerful
coding schemes such as turbo and LDPC codes. However, in optical storage, the
latest devices, such as Blu-Ray and HD-DVD, still use error-correction schemes
involving Reed–Solomon codes due to their good burst-error-correcting performance
and efﬁcient decoders. Eventually, as storage density increases and consequently the
effects of ISI become more severe, Reed–Solomon codes will not be good enough
and they will need to be replaced. AG codes could be a possible candidate for the
error-correcting schemes in future data storage devices.
References
[1] Goppa, V.D. (1981) Codes on algebraic curves. Soviet Math. Dokl., 24, 75–91.
[2] Justesen, J., Larsen, K.J., Jensen, H. E. et al. (1989) Construction and decoding of a class of
algebraic geometry codes. IEEE Trans. Inform. Theory, IT-35, 811–21.
[3] Blake, I.F., Heegard, C., Hoholdt, T. and Wei, V. (1998) Algebraic geometry codes. IEEE Trans.
Inform. Theory, 44 (6), 2596–618.

REFERENCES
145
[4] Atkinson, K.A. (1989) An Introduction to Numerical Analysis, 2nd edn, John Wiley & Sons, Inc.,
New York, ISBN 0-471-50023-2.
[5] Heegard, C., Little, J. and Saints, K. (1995) Systematic encoding via Grobner bases for a class of
algebraic–geometric Goppa codes. IEEE Trans. Inform. Theory, 41 (6), 1752–61.
[6] Sakata, S. (1988) Finding a minimal set of linear recurring relations capable of generating a given
ﬁnite two-dimensional array. J. Symbolic Computation, 5, 321–37.
[7] Justesen, J., Larsen, K.J., Jensen, H.E. and Hoholdt, T. (1992) Fast decoding of codes from algebraic
plane curves. IEEE Trans. Inform. Theory, IT-38 (6), 1663–76.
[8] Feng, G.L. and Rao, T.R.N. (1993) Decoding algebraic geometric codes up to the designed minimum
distance. IEEE Trans. Inform. Theory, IT-39, 37–46.
[9] Sakata, S., Justesen, J., Madelung, Y. et al. (1995) Fast decoding of algebraic–geometric codes up
to the designed minimum distance. IEEE Trans. Inform. Theory, IT-41 (5), 1672–7.
[10] Saints, K. and Heegard, C. (1995) Algebraic–geometric codes and multidimensional cyclic codes: a
uniﬁed theory and algorithms for decoding using Grobner bases. IEEE Transactions on Information
Theory, 41 (6), 1733–51.
[11] Johnston, M. and Carrasco, R.A. (2005) Construction and performance of algebraic–geometric
codes over the AWGN and fading channels. IEE Proc. Commun., 152 (5), 713–22.
[12] Carrasco, R.A. and Johnston, M. (2007) Hermitian codes on magnetic recording channels. 9th
International Symposium on Communication Theory and Applications, Ambleside, Lake District,
UK.
[13] Liu, C.-W. (1999) Determination of error values for decoding Hermitian codes with the inverse
afﬁne fourier transform. IEICE Trans. Fundamentals, E82-A (10), 2302–5.


5
List Decoding
5.1 Introduction
In this chapter, we discuss an alternative decoding algorithm which produces a list
of candidate messages instead of just one. This class of algorithm is known as a List
Decoding algorithm and was developed by Elias [1, 2] and Wozencraft [3]. A list
decoder has the advantage of being able to correct more errors than a conventional
decoding algorithm that only returns a single decoded message, from now on known
as a unique decoding algorithm. This is unfortunately at the expense of increased
decoding complexity.
The chapter begins by introducing the Guruswami–Sudan algorithm [4] for the list
decoding of Reed–Solomon codes. This algorithm has two processes: interpolation
and factorization. It is shown that most of the decoding complexity of the list decoding
algorithm is due to the interpolation process, but we present a method to reduce this
complexity. This is followed by an explanation of the soft-decision list decoding of
Reed–Solomon codes using the Kotter–Vardy algorithm [5].
We also describe how to modify the Guruswami–Sudan algorithm to list decode AG
codes and again show we can still reduce the complexity of the interpolation process.
The Kotter–Vardy algorithm is then extended for the soft-decision list decoding of AG
codes. The performance of hard- and soft-decision list decoding of Reed–Solomon
codes and AG codes is evaluated, and simulation result are presented showing the
performance of both coding schemes on the AWGN and fading channels. The idea
of list decoding is as follows: given a received word R, reconstruct a list of all code
words with a distance τ to the received word R, in which τ can be greater than
τunique =  d−1
2 .
This idea is illustrated in Figure 5.1, where c1, c2 and c3 are three independent code
words at a distance d from each other. A received word r1, which has distance less
than τ unique to code word c1, can be decoded by the unique decoding algorithm, which
results in c1. However, for received word r2, which has distance greater than τ unique
to any of the code words, the unique decoding algorithm will fail to decode it. But
Non-Binary Error Control Coding for Wireless Communication and Data Storage
Rolando Antonio Carrasco and Martin Johnston
C 2008 John Wiley & Sons, Ltd

148
LIST DECODING
d
−
2
1
d
c1
c2
c3
r1
r2
Figure 5.1
The idea of list decoding.
using the list decoding algorithm, a list of possible transmitted code words will be
produced. For example, a decoded output list {c1, c2, c3} is produced by the decoder,
then the code word that has the minimal distance to r2 is chosen from the list and
decoding is completed.
5.2 List Decoding of Reed–Solomon Codes Using
the Guruswami–Sudan Algorithm
5.2.1 Weighted Degrees and Lexicographic Orders
To apply the list decoding algorithm we must ﬁrst impose an ordering on the powers
of a set of monomials. If we deﬁne the weighted (u, v)-degree of a monomial xayb by:
degu,v(xayb) = ua + vb
(5.1)
then we can order a sequence of monomials by their weighted degrees. The lexico-
graphic order of the monomials is then deﬁned as:
xa1 yb1 < xa2 yb2
(5.2)
if degu,v(xa1 yb1) < degu,v(xa2 yb2) or degu,v(xa1 yb1) = degu,v(xa2 yb2) and a1 > a2.
This has already been used in Chapter 4 to order the monomial basis used to construct
algebraic–geometric codes. The lexicographic order of the weighted (1, 1)-degree of
the sequence of monomials xayb is actually the Total Graduated Degree Order <T
deﬁned in (4.9).
To list decode a (n, k) Reed–Solomon code we use the (1, k −1)-weighted degree.
For example, in order to decode a (7, 5) RS code deﬁned in GF(8), (1, 4)-lexicographic
order is used. The generation of this order is shown in Table 5.1. The entries in
Table 5.1a and 5.1b represent the (1, 4)-weighted degree and (1, 4)-lexicographic
order of monomials M with x degree a and y degree b, respectively. Applying (5.1)
with u = 1 and v = 4, we can generate the (1, 4)-weighted degree of monomials M
shown by Table 5.1a. For example, for monomial x2y, its (1, 4)-weighted degree is

LIST DECODING OF REED–SOLOMON CODES
149
Table 5.1
(a) (1, 4)-weighted degree of monomial xayb; (b) (1, 4)-lexicographic order of monomial
xayb.
a/b
0
1
2
3
4
5
6
7
8
9
10
11
12
. . .
(a)
0
0
1
2
3
4
5
6
7
8
9
10
11
12
. . .
1
4
5
6
7
8
9
10
11
12
2
8
9
10
11
12
. . .
3
12
. . .
...
...
a/b
0
1
2
3
4
5
6
7
8
9
10
11
12
. . .
(b)
0
0
1
2
3
4
6
8
10
12
15
18
21
24
. . .
1
5
7
9
11
13
16
19
22
25
. . .
2
14
17
20
23
26
. . .
3
27
. . .
...
...
deg1,4(x2y) = 2 · 1 + 4 = 6, which is shown in Table 5.1a. Based on Table 5.1a,
and applying the above lexicographic order rule, we can generate the (1, 4)-
lexicographic order of monomials M shown in Table 5.1b and denoted as ord(M). From
Table 5.1b, it can be observed that x4 < x2y < y2, since ord(x4) = 4, ord(x2y) = 9 and
ord(y2) = 14.
With the deﬁnition of the weighted degree and order of a monomial, we can deﬁne
the weighted degree of a nonzero bivariate polynomial in Fq[x, y] as the weighted
degree of its leading monomial, ML. Any nonzero bivariate polynomial Q(x, y) can be
written as:
Q(x, y) = Q0M0 + Q1M1 + · · · + QL ML,
(5.3)
with bivariate monomial M = xayb ordered as M0 < M1 < · · · < ML, Q0,
Q1, . . . , QL ∈GF(q) and QL = 0. The (1, k −1)-weighted degree of Q(x, y) can be
deﬁned as:
deg1,k−1(Q(x, y)) = deg1,k−1(ML)
(5.4)
L is called the leading order (lod) of polynomial Q(x, y), deﬁned as:
lod(Q(x, y)) = ord(ML) = L.
(5.5)
For example, given a polynomial Q(x, y) = 1 + x2 + x2y + y2, applying the above
(1, 4)-lexicographic order, it has leading monomial ML = y2. Therefore, deg1,4(Q(x,
y)) = deg1,4(y2) = 8 and lod(Q(x, y)) = ord(y2) = 14. Consequently, any two nonzero

150
LIST DECODING
polynomials Q and H (Q, H ∈Fq[x, y]) can be compared with respect to their leading
order:
Q ≤H, if lod(Q) ≤lod(H)
(5.6)
Sx(T) and Sy(T) are denoted as the highest degree of x and y respectively under the
(1, k −1)-lexicographic order such that:
Sx(T ) = max{a : ord(xay0) ≤T }
(5.7)
Sy(T ) = max{b : ord(x0yb) ≤T },
(5.8)
where T is any nonnegative integer. The error-correction capability, τ m, and the
maximum number of candidate messages, lm, in the output list with respect to a
certain multiplicity, m, of the GS algorithm can be stated as [4]:
τm = n −1 −
& Sx(C)
m
'
(5.9)
lm = Sy(C),
(5.10)
in which T in (5.7) and (5.8) is replaced by the nonnegative integer C, deﬁned as:
C = n
m + 1
2

= n
(m + 1)!
(m + 1 −2)!2! = n(m + 1)m
2
(5.11)
C represents the number of iterations in the interpolation process, or alternatively
the maximum number of Hasse derivative evaluations of an individual interpolated
polynomial. These parameters will be proven in Section 5.2.5, when the factorization
theorem is presented. τ m and lm grow monotonically with multiplicity m [4]:
τm1 ≤τm2
(5.12)
lm1 < lm2,
(5.13)
if m1 < m2.
5.2.2 Interpolation Theorem
From an algebraic geometric point of view, the interpolation process in the GS algo-
rithm requires a hard decision to be made on the received word (r0, r1, . . . , rn−1) and
the set of afﬁne points corresponding to the location of each symbol in the received
word. For Reed–Solomon codes, the afﬁne points are simply the elements of the ﬁnite
ﬁeld over which it is deﬁned. Each afﬁne point and received symbol is paired up
to form a set of interpolation points (xi, ri), where i = 0, 1, . . . , n −1. The idea
of interpolation is to generate a polynomial Q(x, y) that intersects all these points a
given number of times. The number of times Q(x, y) intersects each point is called its
multiplicity, denoted as m, and this parameter determines the error-correcting capa-
bility of the GS algorithm. The interpolation polynomial Q(x, y) can be expressed

LIST DECODING OF REED–SOLOMON CODES
151
as [4, 6]:
Q(x, y) =

a,b
Qabxayb,
(5.14)
where Qab ∈GF(q).
For Q(x, y) to be satisﬁed at the point (xi, ri), Q(xi, ri) = 0. We can write xa and yb
as:
xa = (x −xi + xi)a =

u≤a
a
u

xa−u
i
(x −xi)u
and
yb = (y −ri + ri)b
=

v≤b
b
v

rb−v
i
(y −ri)v
and therefore:
Q(x, y) =

a,b
Qab

u≤a,v≤b

a
u

xa−u
i
(x −xi)u

b
v

rv
i (y −ri)v
=

u,v
Quv (x −xi)u (y −ri)v
,
(5.15)
where:
Q(xi,ri)
uv
=

a≥u,b≥v

a
u
 
b
v

xa−u
i
rb−v
i
.
(5.16)
(5.16) is the (u, v)-Hasse Derivative evaluation at the point (xi, ri) of Q(x, y)
and deﬁnes the constraints on the coefﬁcients of Q(x, y) in order to have a zero of
multiplicity m at that point.
Example 5.1: Given the polynomial Q(x, y) = α5 + α5x + y + xy deﬁned over
GF(8), we can use (5.16) to show that Q(x, y) has a zero of multiplicity of at least
m = 2 at the point (1, α5). In order to have a zero of at least multiplicity m = 2, the
(u, v)-Hasse Derivative at this point must be zero for all u + v < m, that is (u, v)
can be (0, 0), (1, 0) and (0, 1).
Q(1,α5)
00
= Q00
0
0
 0
0

10−0(α5)0−0 + Q10
1
0
 0
0

11−0(α5)0−0
+ Q01
0
0
 1
0

10−0(α5)1−0 + Q11
1
0
 1
0

11−0(α5)1−0
= 1 + 1 + α5 + α5 = 0
Q(1,α5)
10
= Q10
1
1
 0
0

11−1(α5)0−0 + Q11
1
1
 1
0

11−1(α5)1−0 = α5 + α5 = 0
Q(1,α5)
01
= Q01

0
0
 
1
1

10−0(α5)1−1 + Q11

1
0
 
1
1

11−0(α5)1−1 = 1 + 1 = 0.

152
LIST DECODING
Therefore,since Q(1,α5)
00
= Q(1,α5)
10
= Q(1,α5)
01
= 0,Q(x,y)musthave a multiplicity
of at least m = 2.
We denote the Hasse derivative operator by Duv where [7]:
Duv Q(xi,ri) =

a≥u,b≥v
Qab
a
u
 b
v

xa−u
i
rb−v
i
.
(5.17)
Therefore, the interpolation of the GS algorithm can be generalized as: Find a
minimal (1, k −1)-weighted degree polynomial Q(x, y) that satisﬁes:
Q(x, y) = min
lod(Q) {Q(x, y)|Duv Q(xi,ri) = 0
for
i = 0, 1, . . . , n
and
u + v < m} .
(5.18)
5.2.3 Iterative Polynomial Construction
To ﬁnd the interpolated polynomial of (5.18), an iterative polynomial construction
algorithm [4, 8–12] is employed. In this algorithm a group of polynomials is ini-
tialized, tested by applying the Hasse derivative (5.16) and modiﬁed interactively.
The interactive modiﬁcation between two polynomials is based on the following two
properties of the Hasse derivative [7, 10]:
Property 1: Linear functional of Hasse derivative
If H, Q ∈Fq[x, y], δ1 and δ2 ∈
GF(q), then:
D(δ1H + δ2Q) = δ1D(H) + δ2D(Q).
(5.19)
Property 2: Bilinear Hasse derivative
If H, Q ∈Fq[x, y], then:
[H, Q]D = HD(Q) −QD(H).
(5.20)
If the Hasse derivative evaluation of D(Q) = δ1 and of D(H) = δ2 (d1, d2 = 0),
based on Property 1, it is straightforward to prove that the Hasse derivative evaluation
of (5.20) is zero, as follows:
D ([H, Q]D) = D (H D (Q) −QD (H)) = D (δ1H −δ2Q) .
Using Property 1:
D (δ1H −δ2Q) = δ1D(H) −δ2D(Q) = δ1δ2 −δ2δ1 = 0.

LIST DECODING OF REED–SOLOMON CODES
153
Therefore:
D ([H, Q]D) = 0.
(5.21)
If lod(H) > lod(Q), the new constructed polynomial from (5.20) has leading order
lod(H). Therefore, by performing the bilinear Hasse derivative over two polynomials
with nonzero Hasse derivatives, we can reconstruct a polynomial which has a Hasse
derivative of zero. Based on this principle, the implementation of an algorithm for
interpolation will iteratively modify a set of polynomials through all n points and with
every possible (u, v) pair under each point.
With multiplicity m, there are
m + 1
2

pairs of (u, v), which are arranged as:
(u, v) = (0, 0), (0, 1), . . . , (0, m −1), (1, 0), (1, 1), . . . , (1, m −2), . . . , (m −1, 0).
Therefore, when decoding a (n, k) Reed–Solomon code with multiplicity m, there are
C = n

m + 1
2

iterations required to construct a polynomial deﬁned by (5.18).
At the start of the algorithm, a group of polynomials is initialized as:
G0 = {Q0, j = y j
, j = 0, 1, . . . ,lm},
(5.22)
where lm is the maximum number of messages in the output list deﬁned by (5.10). If
ML denotes the leading monomial of polynomial Q, it is important to point out that:
Q0, j = min{Q(x, y) ∈Fq[x, y]| deg y(ML) = j}.
(5.23)
Let ik denotes the iteration index of the algorithm, where ik = i

m + 1
2

+ r, i =
0, 1, . . . , n −1 and r = 0, 1, . . . ,
m + 1
2

−1. For iteration ik of the algorithm,
each polynomial Qik, j in group Gik is tested by (5.17), and the value of each after
Hasse derivative evaluation is denoted by j as:
 j = Dik(Qik, j).
(5.24)
Those polynomials with j = 0 do not need to be modiﬁed. However, those
polynomials with j = 0 need to be modiﬁed based on (5.20). In order to construct a
group of polynomials which satisfy:
Qik+1, j = min

Q ∈Fq[x, y]|Dik(Qik+1, j) = 0, Dik−1(Qik+1, j) = 0, . . . ,
D0(Qik+1, j) = 0, and deg y(ML) = j

,
(5.25)

154
LIST DECODING
the minimal polynomial among those polynomials with j = 0 is chosen. Denote the
index of the minimal polynomial as j and record it as Q:
j = index(min{Qik, j| j = 0})
(5.26)
Q = Qik, j.
(5.27)
For the remaining polynomials with j = 0 but j = j, (5.20) is used to modify
them without the leading order being increased:
Qik+1, j = [Qik,, Q]Dik =  j Qik, j −Q.
(5.28)
Based on (5.21), we know that Dik(Qik+1, j) = 0. As lod(Qik, j) > lod(Q), it follows
that lod(Qik+1, j) = lod(Qik, j).
Q is modiﬁed by (5.20) with the leading order increasing:
Qik+1, j = [x Q, Q]Dik = (x −xi)Q,
(5.29)
where xi is the x-coordinate of current interpolating point (xi, ri).  j∗=
Dik(Q) = 0 and so, as Dik(x Q) = 0, Dik(Qik+1, j) = 0. As lod(xQ) > lod(Q),
lod(Qik+1, j) = lod(x Q) > lod(Qik, j). Therefore whenever (5.29) is performed, we
have: lod(Qik+1, j) > lod(Qik, j).
After C iterative modiﬁcations, the minimal polynomial in group Gc is the inter-
polated polynomial that satisﬁes (5.18), and it is chosen to be factorized in the next
step:
Q(x, y) = min{QC, j|QC, j ∈GC}.
(5.30)
5.2.4 Complexity Reduced Modiﬁcation
Based on the above analysis, it can be observed that when decoding a (n, k)
Reed–Solomon code with multiplicity m, lm + 1 bivariate polynomials are being
interactively modiﬁed over C iterative steps in which Hasse derivative evaluation
(5.17) and bilinear Hasse derivative modiﬁcation (5.20) are being performed. This
process has a complexity of approximately O(n2m4) [4] and is responsible for the
GS algorithm’s high decoding complexity. Therefore, reducing the complexity of
interpolation is essential to improving the algorithm’s efﬁciency.
The leading order of the polynomial group Gik is deﬁned as the minimal leading
order (lod) among the group’s polynomials [13]:
lod(Gik) = min{lod(Qik, j)|Qik, j ∈Gik}.
(5.31)
Based on the initialization deﬁned in (5.22), the leading order of polynomial group
G0 is lod(G0) = lod(Q0,0) = 0. In the ik modiﬁcation, if no polynomial needs to be
modiﬁed then the polynomial group is unchanged; lod(Gik+1) = lod(Gik). When a
polynomial needs to be modiﬁed, (5.29) must be used. If ML is the leading monomial

LIST DECODING OF REED–SOLOMON CODES
155
of Q*, we have [13]:
lod(x Q∗) = lod(Q∗) +
&degx Q∗
k −1
'
+ degy(ML) + 1
(5.32)
when k is the dimension of the code. Based on (5.32), it can be seen that lod(Gik)
will be increased if Q* is the minimal polynomial in the group Gik. The leading order
increase guarantees that in the ik iterative step, the leading order of the polynomials
group Gik is always less than or equal to ik:
lod(Gik) ≤ik.
(5.33)
Based on (5.33), after C iterative steps we have:
lod(GC) ≤C.
(5.34)
From (5.30) we know that only the minimal polynomial is chosen from the polyno-
mial group GC as Q(x, y) = {Qc, j|Qc, j ∈Gc and lod(Qc, j) = lod(Gc)}, therefore:
lod(Q(x, y)) ≤C,
(5.35)
which means the interpolated polynomial Q(x, y) has leading order less than or equal
to C. Those polynomials with leading order over C will not be candidates for Q(x, y).
Therefore, during the iterative process, we can modify the group of polynomials by
eliminating those with leading order greater than C, as [13]:
Gik = {Qik, j|lod(Qik, j) = C}.
(5.36)
We now prove this modiﬁcation will not affect the ﬁnal result. In iteration ik, if
there is a polynomial Qik, j with lod(Qik, j) > C, it may be modiﬁed by either (5.28) or
(5.29), which will result in its leading order being unchanged or increased. Therefore,
at the end lod(Qc,j) > C, and based on (5.35) it cannot be Q(x, y). However, if Qik, j is the
minimalpolynomialdeﬁnedby(5.27),thisimplies thatthosepolynomialswithleading
order less than C do not need to be modiﬁed. If Qik, j is not the minimal polynomial
deﬁned by (5.27), Qik, j will not be chosen to perform bilinear Hasse derivative (5.28)
with other polynomials. Therefore, Q(x, y) has no information introduced from Qik, j,
since lod(Qik, j) > C. As a result, eliminating the polynomials with leading order
greater than C will not affect the ﬁnal outcome.
This complexity modiﬁcation scheme can be generally applied to the iterative
interpolation process, for example to soft-decision list decoding of Reed–Solomon
codes and hard/soft-decision list decoding of Hermitian codes. Based on the total
number of iterations C for interpolation, the interpolated polynomial’s leading order
always satisﬁes lod(Q(x, y)) ≤C. It implies that those polynomials in the group G
can be eliminated once their leading order is greater than C.

156
LIST DECODING
This modiﬁcation can reduce some unnecessary computation in terms of avoiding
Hasse derivative evaluation (5.24) and bilinear Hasse derivative modiﬁcation (5.28)
and (5.29) of polynomials with leading order over C. Based on the above analysis,
the modiﬁed interpolation process can be summarized as:
Algorithm 5.1: Interpolation for list decoding a (n, k) Reed–Solomon code
[13, 14]
1. Initialize a group of polynomials by (5.22), and set the index of the interpolated
point i = 0.
2. Set interpolation point to (xi, ri).
3. For each (u, v) where u + v < m.
{
4. Modify the polynomial group by (5.36).
5. Perform Hasse derivative evaluation (5.24) for each polynomial in the group.
6. If all the polynomials’ Hasse derivative evaluations are zero, choose another pair
(u, v) and go to step 3.
7. Find the minimal polynomial deﬁned by (5.26) and (5.27).
8. For the minimal polynomial, modify it by (5.29). For the other polynomials with
nonzero Hasse derivative evaluation, modify them by (5.28).
}
9. i = i + 1.
10. If i = n, stop the process and choose Q(x, y) deﬁned by (5.30). Else go to step 2.
Example 5.2 shows the modiﬁed interpolation process.
Example 5.2: Decoding the (7, 2) Reed–Solomon code deﬁned over GF(8) with
multiplicity m = 2 As C = 7( 3
1) = 21, based on (5.9) and (5.10) we have τ 2 = 3
and l2 = 5. The transmitted code word is generated by evaluating the message
polynomial f(x) = α + α6x over the set of points x = (1, α, α3, α2, α6, α4, α5),
and the corresponding received word is R = (α5, α3, α4, 0, α6, α2, α2), where α
is a primitive element in GF(8) satisfying α3 + α + 1 = 0. Construct a bivariate
polynomial that has a zero of multiplicity m = 2 over the n points (xi,ri)|n−1
i=0 .
At the beginning, six polynomials are initialized as:
Q0,0 = 1, Q0,1 = y, Q0,2 = y2, Q0,3 = y3, Q0,4 = y4 and Q0,5 = y5. Their leading
orders are lod(Q0,0) = 0, lod(Q0,1) = 2, lod(Q0,2) = 5, lod(Q0,3) = 9, lod(Q0,4) =
14 and lod(Q0,5) = 20 respectively. lod(G0) = lod(Q0,0) = 0.
When i = 0 and (u, v) = (0, 0), ik = 0. No polynomial is eliminated from the
group G0.

LIST DECODING OF REED–SOLOMON CODES
157
Perform Hasse derivative evaluation for each of the polynomials in G0 as:
0 = D(1,α5)
(0,0) Q0,0 = 1, 1 = D(1,α5)
(0,0) Q0,1 = α5
2 = D(1,α5)
(0,0) Q0,2 = α3, 3 = D(1,α5)
(0,0) Q0,3 = α
4 = D(1,α5)
(0,0) Q0,4 = α7, 5 = D(1,α5)
(0,0) Q0,5 = α.
Find the minimal polynomial with j = 0 as:
j = 0
and
Q = Q0,0.
Modify polynomials in G0 with j = 0 as:
Q1,0 = 0(x −x0)Q = 1 + x, and lod(Q1,0) = 1
Q1,1 = 0Q0,1 −1Q = α5 + y and lod(Q1,1) = 2
Q1,2 = 0Q0,2 −2Q = α3 + y2 and lod(Q1,2) = 5
Q1,3 = 0Q0,3 −3Q = α + y3 and lod(Q1,3) = 9
Q1,4 = 0Q0,4 −4Q = α6 + y4 and lod(Q1,4) = 14
Q1,5 = 0Q0,5 −5Q = α4 + y5 and lod(Q1,5) = 20
lod(G1) = lod(Q1,0) = 1.
When i = 0 and (u, v) = (0, 1), ik = 1. No polynomial is eliminated from the
group G1.
Perform Hasse derivative evaluation for each of the polynomial in G1 as:
0 = D(1,α5)
(0,1) (Q1,0) = 0. 1 = D(1,α5)
(0,1) (Q1,1) = 1
2 = D(1,α5)
(0,1) (Q1,2) = 0. 3 = D(1,α5)
(0,1) (Q1,3) = α3
4 = D(1,α5)
(0,1) (Q1,4) = 0. 5 = D(1,α5)
(0,1) (Q1,5) = α6.
Find the minimal polynomial with j = 0 as:
j = 1
and
Q = Q1,1.
As 0 = 2 = 4 = 0:
Q2,0 = Q1,0 = 1 + x, and lod(Q2,0) = 1
Q2,2 = Q1,2 = σ 3 + y2, and lod(Q2,2) = 5
Q2,4 = Q1,4 = σ 6 + y4, and lod(Q2,4) = 14.

158
LIST DECODING
Modify polynomials in G1 with j = 0 as:
Q2,1 = 1(x −x0)Q = α5 + α5x + y(1 + x), lod(Q2,1) = 4
Q2,3 = 1Q1,3 −3Q = α3y + y3, lod(Q2,3) = 9
Q2,5 = 1Q2,5 −5Q = α6y + y5, lod(Q2,5) = 20
lod(G2) = lod(Q2,0) = 1.
When i = 0 and (u, v) = (1, 0), ik = 2. No polynomial is eliminated from the
group G2.
Perform Hasse derivative evaluation for each of the polynomial in G2 as:
0 = D(1,α5)
(1,0) (Q2,0) = 1. 1 = D(1,α5)
(1,0) (Q2,1) = 0
2 = D(1,α5)
(1,0) (Q2,2) = 0. 3 = D(1,α5)
(1,0) (Q2,3) = 0
4 = D(1,α5)
(1,0) (Q2,4) = 0. 5 = D(1,α5)
(1,0) (Q2,5) = 0.
Find the minimal polynomial with j = 0 as:
j = 0 and Q = Q2,0.
As 1 = 2 = 3 = 4 = 5 = 0:
Q3,1 = Q2,1 = α5 + α5x + y(1 + x), lod(Q3,1) = 4
Q3,2 = Q2,2 = α3 + y2, and lod(Q3,2) = 5
Q3,3 = Q2,3 = α3y + y3, lod(Q3,3) = 9
Q3,4 = Q2,4 = α6 + y4, and lod(Q3,4) = 14
Q3,5 = Q2,5 = α6y + y5, lod(Q3,5) = 20.
Modify polynomials in G2 with j = 0 as:
Q3,0 = 0(x −x0)Q = 1 + x2, lod(Q3,0) = 3
lod(G3) = lod(Q3,0) = 3.
Based on the same process, interpolation is run through all the rest of the points
(xi, ri) (i = 1 to 6). In order to illustrate the complexity reducing modiﬁcation
scheme, Table 5.2 shows the whole iterative process with respect to the polynomi-
als’ leading order.
From Table 5.2 we can see that the modiﬁed algorithm starts to take action at
ik = 10 when there is at least one polynomial with leading order over 21 and
eliminating such polynomials will not affect the ﬁnal outcome. At the end, both
the original and the modiﬁed GS algorithm produce the same result: Q(x, y) =
min{G21} = Q21,2 = 1 + α4x2 + α2x4 + y2(α5 + α4x2). From this example we
can see that more computation can be reduced if the modiﬁed algorithm starts to
take action at earlier steps.

LIST DECODING OF REED–SOLOMON CODES
159
Table 5.2
Iterative process of Example 5.2.
i (ik)
0 (0)
0 (1)
0 (2)
1 (3)
1 (4)
1 (5)
2 (6)
2 (7)
2 (8)
3 (9)
lod(Qik,0) 0
1
1
3
6
6
10
15
15
21
lod(Qik,1) 2
2
4
4
4
7
7
7
11
11
lod(Qik,2) 5
5
5
5
5
5
5
5
5
5
lod(Qik,3) 9
9
9
9
9
9
9
9
9
9
lod(Qik,4) 14
14
14
14
14
14
14
14
14
14
lod(Qik,5) 20
20
20
20
20
20
20
20
20
20
lod(Gik)
0
1
1
3
4
5
5
5
5
5
Original GS
i (ik)
3 (10) 3 (11) 4 (12) 4 (13) 4 (14) 5 (15) 5 (16) 5 (17) 6 (18) 6 (19) 6 (20) 7 (21)
lod(Qik,0) 28
28
36
45
45
55
55
55
55
66
66
78
lod(Qik,1) 11
16
16
16
22
22
22
22
22
22
29
29
lod(Qik,2) 5
5
5
5
5
5
8
8
12
12
12
12
lod(Qik,3) 9
9
9
9
9
9
9
13
13
13
13
13
lod(Qik,4) 14
14
14
14
14
14
14
14
14
14
14
14
lod(Qik,5) 20
20
20
20
20
20
20
20
20
20
20
20
lod(Gik)
5
5
5
5
5
5
8
8
12
12
12
12
Modiﬁed GS
i (ik)
3 (10) 3 (11) 4 (12) 4 (13) 4 (14) 5 (15) 5 (16) 5 (17) 6 (18) 6 (19) 6 (20) 7 (21)
lod(Qik,0) —
—
—
—
—
—
—
—
—
—
—
—
lod(Qik,1) 11
16
16
16
—
—
—
—
—
—
—
—
lod(Qik,2) 5
5
5
5
5
5
8
8
12
12
12
12
lod(Qik,3) 9
9
9
9
9
9
9
13
13
13
13
13
lod(Qik,4) 14
14
14
14
14
14
14
14
14
14
14
14
lod(Qik,5) 20
20
20
20
20
20
20
20
20
20
20
20
lod(Gik)
5
5
5
5
5
5
8
8
12
12
12
12
Note: — means the corresponding polynomial is eliminated;
means the corresponding
polynomial is chosen as Q(x, y).
5.2.5 Factorization
In this section, the factorization theorem is explained, followed by a detailed descrip-
tion of an efﬁcient algorithm known as Roth–Ruckenstein’s algorithm [15].
As mentioned in Section 5.2.2, given the interpolated polynomial Q(x, y), the
transmitted message polynomial f(x) can be found by determining Q(x, y)’s y roots.
Lemma 5.1
If Q(x, y) has a zero of multiplicity at least m over (xi, ri) and p(x) is a
polynomial in Fq[xk−1] that p(xi) = ri, then (x −xi)m|Q(x, p(x)) [6].
Deﬁne (p, R) as the number of symbols in received word R that satisfy p(xi) = ri
as:
(p, R) = |{i : p(xi) = ri, i = 0, 1, . . . , n −1}| .
(5.37)

160
LIST DECODING
Lemma 5.2
p(x) is a polynomial in Fq[xk−1] and p(xi) = ri for at least (p, R)
values. If m (p, R) > deg1,k−1(Q(x, y)) then y −p(x)|Q(x, y), or Q(x, p(x)) =
0 [6].
Based on Lemma 5.1, if p(xi) = ri then (x −xi)m|Q(x, y). If S is the set of i
that satisﬁes p(xi) = ri. as |S| = (p, R) then  
i∈S
(x −xi)m|Q(x, p(x)). Assume
g1(x) =  
i∈S
(x −xi)m and g2(x) = Q(x, p(x)); therefore g1(x)|g2(x). It is obvious
that g1(x) has x-degree m (p, R) and g2(x) has x-degree equal to deg1,k−1Q(x, y).
If m (p, R) > deg1,k−1 Q(x, y) and g1(x)|g2(x), the only solution for these two
preconditions is g2(x) = 0. Therefore, if m (p, R) > deg1,k−1(Q(x, y)), Q(x, p(x)) =
0, or equivalently, y −p(x)|Q(x, y).
As Q(x, y) is the interpolated polynomial from the last step, according to (5.35),
lod(Q) ≤C. Based on (5.9), deg1,k−1(Q(x, y)) ≤Sx(C). If m (f, R) ≥Sx(C) then
m ( f, R) ≥deg1,k−1(Q(x, y)). Based on Lemma 5.2, if ( f, R) ≥1 +
	 Sx(C)
m

then
the transmitted message polynomial f(x) can be found by factorizing Q(x, y). As (f,
R) represents the number of points that satisfy ri = f(xi) = ci, those points that do not
satisfy this equation are where the errors are located.
Therefore, the error-correction capability of the GS algorithm is τm = n −
	 Sx(C)
m

−1, which is deﬁned by (5.10). Under (1, k −1)-lexicographic order, x0yj is
the maximal monomial with weighted degree (k −1)j. In polynomial Q(x, y) there
should not be any monomials with y-degree over Sy(C), otherwise lod(Q) > C. As a
result, max{degyQ(x, y)} ≤Sy(C). As the factorization output list contains the y-roots
of Q(x, y), and the number of y-roots of Q(x, y) should not exceed its y-degree, the
maximal number of candidate messages in the output list is lm = Sy(C), which is
deﬁned by (5.11).
5.2.6 Recursive Coefﬁcient Search
To ﬁnd the y-roots of the interpolated polynomial Q(x, y), Roth and Ruckenstein [15]
introduced an efﬁcient algorithm for factorizing these bivariate polynomials.
In general, the factorization output p(x) ∈Fq[xk−1] can be expressed in the form of:
p(x) = p0 + p1x + · · · + pk−1xk−1,
(5.38)
where p0, p1, . . . , pk−1 ∈GF(q). In order to ﬁnd the polynomials p(x), we must deter-
mine their coefﬁcients p0, p1, . . ., pk−1, respectively. The idea of Roth–Ruckenstein’s
algorithm is to recursively deduce p0, p1, . . ., pk−1 one at a time.
For any bivariate polynomial, if h is the highest degree such that xh|Q(x, y), we
can deﬁne [15]:
Q∗(x, y) = Q(x, y)
xh
.
(5.39)

LIST DECODING OF REED–SOLOMON CODES
161
If we denote p0 = p(x) and Q0(x, y) = Q*(x, y), where Q(x, y) is the new interpolated
polynomial (5.30), we can deﬁne the recursive updated polynomials ps(x) and Qs(x,
y), where s ≥1, as [15]:
ps(x) = ps−1(x) −ps−1(0)
x
= ps + · · · + pk−1xk−1−s,
(s = k −1). (5.40)
Qs(x, y) = Q∗
s−1(x, xy + ps−1).
(5.41)
Lemma 5.3
With ps(x) and Qs(x, y) deﬁned by (5.40) and (5.41), when s ≥1,
(y −p(x))|Q(x, y) if and only if (y −ps(x))|Qs(x, y) [4].
This means that if polynomial ps(x) is a y-root of Qs(x, y), we can trace back to
ﬁnd the coefﬁcients ps−1, . . . , p1, p0 to reconstruct the polynomial p(x), which is the
y-root of polynomial Q(x, y).
The ﬁrst coefﬁcient p0 can be determined by ﬁnding the roots of Q0(0, y) = 0. If we
assume that Q(x, p(x)) = 0, p0(x) should satisfy Q0(x, p0(x)) = 0. When x = 0, Q0(0,
p0(0)) = 0. According to (5.38), p0(0) = p0, therefore p0 is the root of Q0(0, y) =
0. By ﬁnding the roots of Q0(0, y) = 0, a number of different p0 can be determined.
For each p0, we can deduce further to ﬁnd the rest of ps (s = 1, . . . , k −1) based on
the recursive transformation (5.40) and (5.41).
Assume that after s −1 deductions, polynomial ps−1(x) is the y-root of Qs−1(x,
y). Based on (5.40), ps−1(0) = ps−1 and a number of ps−1 can be determined by
ﬁnding the roots of Qs−1(0, y) = 0. For each ps−1, we can ﬁnd ps. As Qs−1(x,
ps−1(x)) = 0, (y −ps−1(x))|Qs−1(x, y). If we deﬁne y = xy + ps−1, then (xy +
ps−1 −ps−1(x))|Qs−1(x, xy + ps−1). Based on (5.40), xy + ps−1 −ps−1(x) = xy −
xps(x). As Qs(x, y) = Q∗
s−1(x, xy + ps−1), (xy −xps(x))|Qs−1(x, xy + ps−1) and
(y −ps(x))|Qs(x, y). Therefore, ps can again be determined by ﬁnding the roots of
Qs(0, y) = 0. This root-ﬁnding algorithm can be explained as a tree-growing process,
as shown in Figure 5.2. There can be an exponential number of routes for choosing
coefﬁcients ps (s = 0, 1, . . . , k −1) to construct p(x). However, the intended p(x) should
satisfy deg(p(x)) < k and (y −p(x))|Q(x, y). Based on (5.40), when s = k, pk(x) =
0. Therefore if Qk(x, 0) = 0, or equivalently Qk(x, pk(x)) = 0, (y −pk(x))|Qk(x, y).
According to Lemma 5.3, (y −p(x))|Q(x, y) and p(x) is found.
Based on the above analysis, the factorization process can be summarized as [4, 15]:
Algorithm 5.2: Factorization of list decoding a (n, k) Reed–Solomon code [15]
1. Initialize Q0(x, y) = Q∗(x, y), s = 0.
2. Find roots ps of Qs(0, y) = 0.
3. For each ps, perform Q transformation (5.41) to calculate Qs+1(x, y).
4. s = s + 1.
5. If s < k, go to (ii). If s = k and Qs(x, 0) = 0, stop this deduction route. If s = k and
Qs(x, 0) = 0, trace the deduction route to ﬁnd ps−1, . . . , p1, p0.
Example 5.3 demonstrates Roth–Ruckenstein’s algorithm.

162
LIST DECODING
Q(x, y)
p0
p0
p1......p1
p1
.
.
.
.
.
.p1
p2...p2
p2...p2
p2...p2
p2...p2
…………………………………………………….. 
…………
Figure 5.2
Coefﬁcients deduction in Roth–Ruckenstein’s algorithm.
Example 5.3: Based on polynomial Q(x, y) = 1 + α4x2 + α2x4 + y2(α5 + α4x2),
which is the interpolation result of Example 5.2, determine the factorization output
list L using Roth–Ruckenstein’s algorithm.
Initialize Q0(x, y) = Q*(x, y) = 1 + α4x2 + α2x4 + y2(α5 + α4x2) and s = 0.
Q0(0, y) = 1 + α5y2 and p0 = α is the root of Q0(0, y) = 0.
For p0 = α, generate Q1(x, y) = Q0(x, xy + p0):
Q0(x, xy + α) = 1 + α4x2 + α2x4 + (xy + α)2(α5 + α4x2)
= 1 + α4x2 + α2x4 + (x2y2 + α2)(α5 + α4x2)
= 1 + α4x2 + α2x4 + α5x2y2 + α4x4y2 + 1 + α6x2
= α3x2 + α2x4 + y2(α5x2 + α4x4).
Now, from (5.39), we can see that x2 is the highest power of x that divides Q0(x,
xy + α), so:
Q∗
0(x, xy + α) = Q0(x, xy + α)
xh
= Q0(x, xy + α)
x2
= α3 + α2x2 + y2(α5 + α4x2).
s = s + 1 = 1. As s < k, go to step 2 of Algorithm 5.2.
Q1(0, y) = α3 + α5y2 and p1 = α6 is a root of Q1(0, y) = 0.
For p1 = α6, generate Q2(x, y) = Q∗
1(x, xy + p1) = y2(α5 + α4x2). s =
s + 1 = 2. As s = k and Q2(x, 0) = 0, trace this route to ﬁnd its output p0 =
α and p1 = α6.
As a result, factorization output list L = {p(x) = α + α6x}. From Example 5.2,
p(x) matches the transmitted message polynomial f(x).

LIST DECODING OF REED–SOLOMON CODES
163
Example 5.4: Factorizing an interpolation polynomial containing two
messages Using the same (7, 2) Reed–Solomon code as in Example 5.3, assume
that the interpolated polynomial is Q(x, y) = αx + α6x2 + y(α3 + α3x) + α2y2.
For s = 0, Q0(x, y) = Q∗(x, y) = αx + α6x2 + y(α3 + α3x) + α2y2.
Setting x = 0, Q0(0, y) = α3y + α2y2 and has two roots, p0 = 0 and α.
Taking p0 = 0:
Q1(x, y) = Q∗
0(x, xy + 0) = α + α6x + y(α3 + α3x) + α2xy2.
s = s + 1 = 1. As s < k = 2 we go back to step 2 of Algorithm 5.2.
For s = 1, Q1(0, y) = α + α3y, which has one root, p1 = α5.
Taking p1 = α5:
Q2(x, y) = Q∗
1(x, xy + α5) = y(α3 + α3x) + α2x2y2
s = s + 1 = 2. Now s = k and Q2(x, 0), so this route is terminated and the
message is p(x) = 0 + α5x = α5x.
However, we must now determine whether there is another message contained
within the interpolation polynomial and so we now take the second root of
Q0(0, y).
Taking p0 = α:
Q1(x, y) = Q∗
0(x, xy + α) = α2 + α6x + y(α3 + α3x) + α2xy2
s = s + 1 = 1. As s < k = 2 we go back to step 2 of Algorithm 5.2.
For s = 1, Q1(0, y) = α2 + α3y, which has one root, p1 = α6.
Taking p1 = α6:
Q2(x, y) = Q∗
1(x, xy + α6) = y(α3 + α3x) + α2x2y2
s = s + 1 = 2. Now s = k and Q2(x, 0), so this route is terminated and the
message is p(x) = α + α6x.
The roots of Q(x, y) are shown graphically in Figure 5.3. To complete the
decoding procedure, each message would be re-encoded and the code word with
the minimum Hamming distance from the received word would be chosen, along
with its corresponding message.
p0 = 0
p0 = α
p1 = α5
p1 = α6
2
2
3
3
2
6
y
x)
y(
x
x
Q(x, y)
+ α
+ α
α
+
+ α
α
=
Figure 5.3
Two messages from Roth–Ruckenstein’s algorithm for Example 5.3.

164
LIST DECODING
5.3 Soft-Decision List Decoding of Reed–Solomon Codes Using the
K¨otter–Vardy Algorithm
Increases in the performance of a Reed–Solomon code can be achieved by taking into
consideration the soft values from the output of the demodulator. These can be used
to give a measure of the reliability of each symbol in the received word. To modify
the Guruswami–Sudan algorithm to use reliability values instead of hard values, there
needs to be a method of mapping reliability values to multiplicity values. Kotter and
Vardy presented a seminal paper in 2003 [5] which allowed the algebraic soft-decision
decoding of Reed–Solomon codes. In this paper, they also gave an algorithm to convert
the reliability of each symbol in the received word to a multiplicity value of each point
(xj, ρi) for the interpolation process, where ρi can be one of q ﬁnite ﬁeld elements in
GF(q) = {ρ0, ρ1, ρ2, . . . , ρq−1}. The reliability values of each received symbol are
arranged in a reliability matrix  and converted into a multiplicity matrix M. The
interpolation and factorization processes then follow in the same way as described
previously. The whole system model for soft-decision list decoding is illustrated in
Figure 5.4.
5.3.1 Mapping Reliability Values into Multiplicity Values
Instead of making a hard decision on the received symbols, the soft values of each
symbol are used to give a measure of reliability. Therefore, the received vector R =
(r0, r1, . . . , rn−1) now contains soft values and not ﬁnite ﬁeld elements. The reliability
of each received symbol is denoted as πi,j, which gives the probability of the jth
transmitted coded symbol cj being the j = ith element ρi in GF(q), given the soft value
of the jth received symbol rj.
pi, j = P(c j = ρi|r j)(i = 0, 1, . . . , q −1 and j = 0, 1, . . . , n −1).
(5.42)
These reliabilities are entered into a q × n reliability matrix .
 =


π0,0
π0,1
· · ·
· · ·
· · ·
π0,n−1
π1,0
π1,1
π1,n−1
...
...
...
...
πi, j
...
...
...
...
πq−1,0
πq−1,1
· · ·
· · ·
· · ·
πq−1,n−1


.
(5.43)
Reliability to
Multiplicity 
Π →M
Interpolation
Q(x, y)
Factorization
p(x)
Received
Symbol 
Reliabilities
Decoded
Messages
Figure 5.4
System model for soft-decision list decoding.

SOFT-DECISION LIST DECODING OF REED–SOLOMON CODES
165
Referring to Figure 5.1, the matrix  is taken as an input to the soft-decision
decoder and converted to a multiplicity matrix M, followed by the interpolation and
factorization processes. An algorithm presented in [5] to convert the reliability matrix
 to a multiplicity matrix M is now given.
Algorithm 5.3: Convert reliability matrix  to multiplicity matrix M [5]
Input:
Reliability matrix  and a desired value of the sum of multiplicities in matrix M as:
s =
q−1

i=0
n−1

j=0
mi, j.
Initialization: Set * =  and q × n all-zero multiplicity matrix M:
1. While (s > 0)
{
2. Find the maximal entry π∗
i, j in * with position (i, j).
3. Update π∗
i, j in * as π∗
i, j =
πi, j
mi, j+2.
4. Update mi, j in M as mi, j = mi, j + 1.
5. s = s −1.
}
Algorithm 5.3 results in a q × n multiplicity matrix M, which can be written as:
M =


m0,0
m0,1 · · · · · · · · · m0,n−1
m1,0
m1,1
m1,n−1
...
...
...
...
mi, j
...
...
...
...
mq−1,0 mq−1,1 · · · · · · · · · mq−1,n−1


,
(5.44)
The entry mi,j represents the multiplicity value of interpolated point (xj, ρi) (j =
0, 1, . . . , n −1 and i = 0, 1, . . . , q −1). xj are the ﬁnite ﬁeld elements used in
the encoding process described in Chapter 3. In Algorithm 5.3, the desired value s
indicates the total value of multiplicity of all interpolated points. This algorithm gives
priority to those interpolated points which correspond to a higher reliability value πi,j,
to be assigned with a higher multiplicity value mi,j. For an illustration of the algorithm,
see Example 5.5.

166
LIST DECODING
Example 5.5: For soft-decision list decoding of the (7, 2) Reed–Solomon code
deﬁned in GF(8), the following 8 × 7 reliability matrix  is obtained by the
receiver:
=


0.959796
0.214170 0.005453
0.461070
0.001125
0.000505
0.691729
0.001749
0.005760 0.000000
0.525038
0.897551
0.025948
0.000209
0.028559
0.005205 0.000148
0.003293
0.000126
0.018571
0.020798
0.000052
0.000140 0.000000
0.003750
0.100855
0.954880
0.000006
0.009543
0.736533 0.968097 0.003180
0.000000
0.000000
0.278789
0.000017
0.019810 0.000006
0.003621
0.000307
0.000003
0.000084
0.000284
0.017900 0.026295
0.000023
0.000000
0.000002
0.008382
0.000001
0.000481 0.000000
0.000026
0.000035
0.000092
0.000003


.
(Note: in the matrix  (*), the maximal entry is underlined).
Apply Algorithm 5.3 with a desired value s = 20.
Initialization: Set * =  and M = 0.
As s = 20 > 0, ﬁnd the maximal entry π∗
i, j = 0.968097 in * with position
(i, j) = (4, 2).
Update π∗
4,2 as π∗
4,2 =
π4,2
m4,2 + 2 = 0.968097
0 + 2
= 0.484048.
Update m4,2 in M as m4,2 = 0 + 1 = 1
s = s −1 = 19.
Now the updated * is:
∗=


0.959796 0.214170
0.005453
0.461070
0.001125
0.000505
0.691729
0.001749
0.005760
0.000000
0.525038
0.897551
0.025948
0.000209
0.028559
0.005205
0.000148
0.003293
0.000126
0.018571
0.020798
0.000052
0.000140
0.000000
0.003750
0.100855
0.954880
0.000006
0.009543
0.736533
0.484048
0.003180
0.000000
0.000000
0.278789
0.000017
0.019810
0.000006
0.003621
0.000307
0.000003
0.000084
0.000284
0.017900
0.026295
0.000023
0.000000
0.000002
0.008382
0.000001
0.000481
0.000000
0.000026
0.000035
0.000092
0.000003


and the updated M is:
M =


0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0


.
In the next iteration, as s = 19 > 0, ﬁnd the maximal entry π∗
i, j = 0.959696 in
* with position (i, j) = (0, 0).

SOFT-DECISION LIST DECODING OF REED–SOLOMON CODES
167
Update π∗
0,0 as π∗
0,0 =
π0,0
m0,0 + 2 = 0.959796
0 + 2
= 0.479898.
Update m0,0 in M as m0,0 = 0 + 1 = 1.
s = s −1 = 18.
Now the updated * is:
∗=


0.479898
0.214170 0.005453
0.461070
0.001125
0.000505
0.691729
0.001749
0.005760 0.000000
0.525038
0.897551
0.025948
0.000209
0.028559
0.005205 0.000148
0.003293
0.000126
0.018571
0.020798
0.000052
0.000140 0.000000
0.003750
0.100855
0.954880 0.000006
0.009543
0.736533 0.484048
0.003180
0.000000
0.000000
0.278789
0.000017
0.019810 0.000006
0.003621
0.000307
0.000003
0.000084
0.000284
0.017900 0.026295
0.000023
0.000000
0.000002
0.008382
0.000001
0.000481 0.000000
0.000026
0.000035
0.000092
0.000003


and the updated M is:
M =


1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0


.
Following the same process until s = 0, the updated * is:
∗=


0.239949
0.214170 0.005453
0.230535
0.001125
0.000505
0.230576
0.001749
0.005760 0.000000
0.175013
0.224388
0.025948
0.000209
0.028559
0.005205 0.000148
0.003293
0.000126
0.018571
0.020798
0.000052
0.000140 0.000000
0.003750
0.100855
0.238720
0.000006
0.009543
0.245511 0.242024
0.003180
0.000000
0.000000
0.139395
0.000017
0.019810 0.000006
0.003621
0.000307
0.000003
0.000084
0.000284
0.017900 0.026295
0.000023
0.000000
0.000002
0.008382
0.000001
0.000481 0.000000
0.000026
0.000035
0.000092
0.000003


and the updated M is:
M =


3
0
0
1
0
0
2
0
0
0
2
3
0
0
0
0
0
0
0
0
0
0
0
0
0
0
3
0
0
2
3
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0


.

168
LIST DECODING
In the resulting multiplicity matrix M, it can be seen that the sum of its entries
7
i=0
6
j=0
mi, j = 20, which is the desired value s set at the beginning. From M we see
that there are nine nonzero entries, implying that there are now nine points used to
generate the interpolation polynomial Q(x, y). These points are:
(x0,r0) with m0,0 = 3, (x1, ρ4) with m4,1 = 2, (x2,r4) with m4,2 = 3,
(x3,r0) with m0,3 = 1, (x3,r1) with m1,3 = 2, (x4,r1) with m1,4 = 3,
(x5,r3) with m3,5 = 3, (x6,r0) with m0,6 = 2, (x6,r4) with m4,6 = 1.
5.3.2 Solution Analysis for Soft-Decision List Decoding
Based on Section 5.2.2, to have a multiplicity of mij over interpolated point (xj, ρi),
the Hasse derivative evaluation of an interpolation polynomial is now deﬁned as:
Duv Q(x j, ρi) =

a≥u,b≥v
a
u
 b
v

Qabxa−u
j
ρb−v
i
, u + v < mi, j
(5.45)
where ri in (5.17) is replaced with ρi. The total number of iterations for all points is:
CM = 1
2
q−1

i=0
n−1

j=0
mi, j(mi, j + 1).
(5.46)
CM is called the ‘cost’ of multiplicity matrix M, which also denotes the number of
iterations in the interpolation process.
Based on Lemma 5.1, if f(x) is the message polynomial that satisﬁes f(xj) = cj
(j = 0, 1, . . . , n −1), polynomial Q(x, f(x)) should satisfy:
(x −x0)m0(x −x1)m1 · · · (x −xn−1)mn−1|Q(x, f (x)).
(5.47)
Again, if we let g1(x) = (x −x0)m0(x −x1)m1 · · · (x −xn−1)mn−1 and g2(x) =
Q(x, f (x)), based on (5.12), g1(x)|g2(x). g1(x) has x-degree degx(g1(x)) = m0 +
m1 + · · · + mn−1. The x-degree of g1(x) is deﬁned as the code word score SM with
respect to multiplicity matrix M:
SM (c) = degx(g1(x)) = m0 + m1 + · · · + mn−1
=
n−1

j=0
{mi, j|ρi = c j, i = 0, 1, . . . , q −1}.
(5.48)
The x-degree of g2(x) is bounded by degx(g2(x)) ≤deg1,k−1 Q(x, y). Therefore, if
SM(c) > deg1,k−1 Q(x, y) then degx(g1(x)) > degx(g2(x)). To satisfy both degx(g1(x))
> degx(g2(x)) and g1(x)|g2(x), the only solution is g2(x) = 0, which indicates

SOFT-DECISION LIST DECODING OF REED–SOLOMON CODES
169
Q(x, f(x)) = 0, or equivalently y −f (x)|Q(x, y), and the message polynomial f(x)
can be found by determining Q(x, y)’s y-roots. As a result, if the code word score with
respect to multiplicity matrix M is greater than the interpolated polynomial Q(x, y)’s
(1, k −1)-weighted degree
SM (c) > deg1,k−1 Q(x, y)
(5.49)
then Q(x, f(x)) = 0, or equivalently y −f (x)|Q(x, y). Message polynomial f(x) can
be found by determining the y roots of Q(x, y).
Based on the (1, k −1)-weighted degree deﬁnition of monomial xayb given in
Section 5.2.1, let us deﬁne the following two parameters:
N1,k−1(δ) =
77{xayb : a, b ≥0 and deg1,k−1(xayb) ≤δ, δ ∈N}
77 ,
(5.50)
which represents the number of bivariate monomial xayb with (1, k −1)-weighted
degree not greater than a nonnegative integer δ [5]; and:
1,k−1(v) = min{δ : N1,k−1(δ) > v, v ∈N},
(5.51)
which denotes the minimal value of δ that guarantees N1,k−1(δ) is greater than a
nonnegative integer v [5].
If the (1, k −1)-weighted degree of interpolated polynomial Q is δ*, based on
(5.50), Q has at most N1,k−1(δ*) nonzero coefﬁcients. The interpolation procedure
generates a system of CM linear equations of type (5.45). The system will be solvable
if [5]:
N1,k−1(δ∗) > CM.
(5.52)
Based on (5.51), in order to guarantee the solution, the (1, k −1)-weighted degree
δ* of the interpolated polynomial Q should be large enough that:
deg1,k−1(Q(x, y)) = δ∗= 1,k−1(CM).
(5.53)
Therefore, according to (5.49), given the soft-decision code word score (5.48) and
the (1, k −1)-weighted degree of the interpolated polynomial Q (5.53), the message
polynomial f can be found if:
SM (c) > 1,k−1(CM).
(5.54)
As the (1, k −1)-weighted degree of the interpolated polynomial Q(x, y) can
be determined by (5.53), while 1,k−1(CM) can be realized by 1,k−1(CM) =
deg1,k−1(xayb|ord(xayb) = CM), a stopping
rule for Algorithm 5.3 based on the

170
LIST DECODING
designed length of output list l can be imposed. This is more realistic for assessing the
performance soft-decision list decoding. As the factorization outputs are the y-roots
of the interpolated polynomial Q, the maximal number of outputs lM based on the
interpolated polynomial Q is:
lM = deg0,1 Q(x, y) =
&deg1,k−1 Q(x, y)
k −1
'
=
&1,k−1(CM)
k −1
'
.
(5.55)
Therefore, after step 5 of Algorithm 5.3, the updated cost CM of the multiplicity
matrix M can be determined using (5.46). As CM has been determined, the interpolated
polynomial Q(x, y)’s (1, k −1)-weighted degree can be determined by (5.53). (5.55)
can then be applied to calculate the maximal number of factorization outputs, lM.
Based on a designed length of output list l, Algorithm 5.3 is stopped once lM is greater
than l.
In practice, due to the decoding complexity restriction, soft-decision list decoding
can only be performed based on a designed length of output list l. This output
length restriction in fact leads to practical decoding performance degradation. This
phenomenon will be seen later when the simulation results are discussed.
As mentioned in Section 5.2.5, to build interpolated polynomial Q(x, y) there are
in total CM (5.46) iterations. Therefore, the iteration index ik used in Algorithm 5.1
is: ik = 0, 1, . . . , CM. Based on a designed length of output list l, the initialization at
step 1 of Algorithm 5.1 can be modiﬁed as:
G0 = {Q0, j = y j, j = 0, 1, . . . ,l},
(5.56)
where l is the designed length of the output list. As there are in total CM iterations,
based on the complexity reducing scheme’s description given in Section 5.2.4, the
interpolated polynomial Q’s leading order is less than or equal to the total number of
iterations CM:
lod(Q(x, y)) ≤CM.
(5.57)
This indicates the fact that (5.53) is an upper bound for the interpolated polynomial’s
(1, k −1)-weighted degree:
deg1,k−1 Q(x, y) ≤(CM).
(5.58)
Based on (5.56), those polynomials with leading order greater than CM will nei-
ther be chosen as the interpolated polynomial nor be modiﬁed with the interpolated
polynomial. Therefore they can be eliminated from the polynomial group and the
modiﬁcation at step 2 can be rewritten as:
Gik = {Qik, j|lod(Qik, j) = CM}.
(5.59)

LIST DECODING OF ALGEBRAIC–GEOMETRIC CODES
171
With respect to interpolated point (xj, ρi) and Hasse derivative parameter (u, v),
where u + v < mi,j, the Hasse derivative evaluation performed at step 3 of Algorithm
5.1 can be modiﬁed and determined by (5.45). The resulting process is the same
as Algorithm 5.1, with the exception that for polynomial modiﬁcation in (5.59) the
interpolated point’s x-coordinate xi should be replaced by the xj which is the current
interpolated point’s x-coordinate. Also, the index j should be used for the interpolated
point’s x-coordinate xj and polynomials in the group Qik, j.
5.3.3 Simulation Results
This section presents both hard-decision and soft-decision list decoding results for two
Reed–Solomon codes: (63, 15) and (63, 31). They are simulated on both the AWGN
and Rayleigh fading channels. The Rayleigh fading channel is frequency nonselective
with Doppler frequency [16] 126.67 Hz and data rate 30 kb/s. The fading proﬁle is
generated using Jakes’ method [16]. The fading coefﬁcients have mean value 1.55
and variance 0.60. On the Rayleigh fading channel, a block interleaver of size 63 ×
63 is used to combat the fading effect. QPSK modulation is used and simulations are
run using the C programming language.
Comparisons between hard-decision and soft-decision are made based on output
length l. For an output length l, there are l + 1 polynomials taking part in the
iterative interpolation process. The total number of iterations (Cm in (5.11) for hard-
decision and CM in (5.46) for soft-decision) also grow with length l. The number of
polynomials l + 1 and the number of iterations (Cm, CM) are the important parameters
that determine the decoding complexity. Based on the same designed length, from
Figures 5.5 and 5.6 it can be seen that soft-decision can achieve signiﬁcant coding
gains over hard-decision list decoding, especially on the Rayleigh fading channel.
For example, with a designed length l = 2, soft-decision list decoding of the (63, 15)
Reed–Solomon code can achieve about a 5.8 dB coding gain at BER = 10−5 over
hard-decision decoding.
According to the analysis in [14], the performance improvement of soft-decision list
decoding over hard-decision list decoding is achieved with an insigniﬁcant complexity
penalty. This is because for the list decoding algorithm, the complexity is mainly
dominated by the interpolation process, and the complexity introduced by the a priori
process (Algorithm 5.3) is marginal. For the interpolation process, the important
parameter that determines its complexity is the iteration number. As the iteration
number of soft-decision does not vary much from hard-decision based on the same
designed output length, the complexity of soft-decision list decoding is not much
higher than that of hard-decision list decoding.
5.4 List Decoding of Algebraic–Geometric Codes
The GS algorithm consists of two processes: interpolation and factorization. Given a
received word R = (r0, r1, . . . , rn−1) (ri ∈GF(q), i = 0, 1, . . . , n −1), n interpolated

172
LIST DECODING
1.00E–06
1.00E–05
1.00E–04
1.00E–03
1.00E–02
1.00E–01
1.00E+00
12
11
10
9
8
7
6
5
4
3
2
1
0
SNR (Eb/N0)
BER
uncoded
GS (m=1, l=2)*
GS (m=2, l=4)**
GS (m=4, l=8)***
GS (m=6, l=13)****
GS (m=26, l=55)*****
KV (l=2)*
KV (l=3)
KV (l=4)**
KV (l=8)***
KV (l=13)****
KV (l=55)*****
KV (Optimal)
(a) over AWGN channel 
1.00E–06
1.00E–05
1.00E–04
1.00E–03
1.00E–02
1.00E–01
1.00E+00
20
18
16
14
12
10
8
6
4
2
0
SNR (Eb/N0)
BER
Uncoded
GS (m=1, l=2)*
GS (m=2, l=4)**
GS (m=4, l=8)***
GS (m=6, l=13)****
GS (m=26, l=55)*****
KV (l=1)
KV (l=2)*
KV (l=4)**
KV (l=8)***
KV (l=13)****
KV (l=55)*****
KV (Optimal)
(b) over Rayleigh fading channel 
Figure 5.5
Hard-decision and soft-decision list decoding Reed–Solomon code (63, 15).
units can be formed by combining each received symbol with its respective afﬁne
point used in encoding, as: (p0, r0), (p1, r1), . . . , (pn−1, rn−1). Interpolation builds the
minimal polynomial Q ∈Fq[x, y, z], which has a zero of multiplicity of at least m
over the n interpolated units. Q can be written as: Q = 
a,b Qabφazb, where Qab ∈
GF(q) and φa is a set of rational functions with pole orders up to a [17–19]. If (pi, ri)

LIST DECODING OF ALGEBRAIC–GEOMETRIC CODES
173
1.00E–06
1.00E–05
1.00E–04
1.00E–03
1.00E–02
1.00E–01
1.00E+00
12
11
10
9
8
7
6
5
4
3
2
1
0
SNR (Eb/N0)
BER
uncoded
GS (m=1, l=1)*
GS (m=3, l=4)**
GS (m=5, l=7)***
GS (m=13, l=19)****
KV (l=1)*
KV (l=2)
KV (l=3)
KV (l=4)**
KV (l=7)***
KV (l=19)****
KV (Optimal)
(a) over AWGN channel 
1.00E–06
1.00E–05
1.00E–04
1.00E–03
1.00E–02
1.00E–01
1.00E+00
20
18
16
14
12
10
8
6
4
2
0
SNR (Eb/N0)
BER
Uncoded
GS (m=1, l=1)*
GS (m=3, l=4)**
GS (m=5, l=7)***
GS (m=13, l=19)****
KV (l=1)*
KV (l=4)**
KV (l=7)***
KV (l=19)****
KV (Optimal)
(b) over Rayleigh fading channel 
Figure 5.6
Hard-decision and soft-decision list decoding Reed–Solomon code (63, 31).
is the intended interpolated unit, it can also be written with respect to the zero basis
functions in Zw,pi deﬁned in Chapter 2 as [9]:
Q =

u,v
Q(pi,ri)
uv
ψpi,u(z −ri)v,
(5.60)

174
LIST DECODING
where Q(pi,ri)
uv
∈GF(q). If Q(pi,ri)
uv
= 0 for u + v < m, polynomial Q has a
zero of multiplicity at least m at unit (pi, ri) [9, 11]. As zb = (z −ri + ri)b =

v≤b ( b
v )rb−v
i
(z −ri)v and φa = 
u γa,pi,uψpi,u, substitute them into (5.15):
Q =

a,b
Qab

u
γa,pi,uψpi,u
 
v≤b
b
v

rb−v
i
(z −ri)v

=

u,v
 
a,b≥v
Qab
b
v

γa,pi,urb−v
i

ψpi,u(z −ri)v
.
(5.61)
Therefore, the coefﬁcients Q(pi,ri)
uv
of (5.61) can be written as:
Q(pi,ri)
uv
=

a,b≥v
Qab

b
v

γa,pi,urb−v
i
.
(5.62)
(5.62) deﬁnes the zero condition constraints on the coefﬁcients Qab of polynomial
Q analogous to the Hasse derivative, so that Q has a zero of multiplicity at least m
over unit (pi, ri). Example 5.6 shows how to deﬁne the zero condition of a polynomial
in Fq[x, y, z] using (5.62).
Example 5.6: Given the polynomial Q(x, y, z) = 1 + αy + αx2 + z2(1 + α2y)
deﬁned in GF(4)[x, y, z] justify the fact that it has a zero of multiplicity at least 2
over the unit (p, r) = ((1, α), α).
Polynomial Q(x, y, z) = 1 + αy + αx2 + z2(1 + α2y) = Q00φ0z0 + Q20φ2z0
+ Q30φ3z0 + Q02φ0z2 + Q22φ2z2. Supporting the zero condition calculations, the
corresponding coefﬁcients γa,p,u are shown in Table 5.3.
Table 5.3
Corresponding coefﬁcients γa,p,u given p = (1, α).
a \ u
0
1
2
3
. . .
0
1
1
α
1
. . .
1
0
1
1
0
. . .
...
...
...
...
...
Based on the above description, to justify that Q has a zero of multiplicity m
over unit (p, r), its coefﬁcients Qab should satisfy Q(p,r)
uv
= 0 for u + v < 2 as:
Q(p,r)
00
= 0, Q(p,r)
01
= 0 and Q(p,r)
10
= 0.

LIST DECODING OF ALGEBRAIC–GEOMETRIC CODES
175
Based on deﬁnition (5.62):
Q(p,r)
00
= Q00
0
0

γ0,p,0α0−0 + Q20
0
0

γ2,p,0α0−0 + Q30
0
0

γ3,p,0α0−0
+ Q02
2
0

γ0,p,0α2−0 + Q22
2
0

γ2,p,0α2−0
= 1 + α2 + α + α2 + α2 = 0
Q(p,r)
01
= Q02

2
1

γ0,p,0α2−1 + Q22

2
1

γ2,p,0α2−1 = 0 + 0 = 0
Q(p,r)
10
= Q00

0
0

γ0,p,1α0−0 + Q20

0
0

γ2,p,1α0−0
+ Q30

0
0

γ3,p,1α0−0 + Q02

2
0

γ0,p,1α2−0 + Q22

2
0

γ2,p,1α2−0
= α + α = 0.
Therefore, polynomial Q has a zero of multiplicity at least 2 over unit (p, r) =
(1, α), α).
If constraint (5.62) for the coefﬁcients of polynomial Q is denoted as D(pi,ri)
uv
(Q),
such that:
D(pi,ri)
uv
(Q) = Q(pi,ri)
uv
=

a,b≥v
Qab
b
v

γa,pi,urb−v
i
(5.63)
then interpolation builds a polynomial Q deﬁned as:
Q = min
lod(Q)

Q ∈Fq[x, y, z]|D(pi,ri)
uv
(Q) = 0
for
i = 0, 1, . . . , n −1 ∧u
+ v < m (u, v ∈N)}

.
(5.64)
As there are ( m+1
2 ) permutations of (u, v) for u + v < m, there are in total:
C = n
m + 1
2

(5.65)
zero condition constraints that the coefﬁcients Qab of polynomial Q need to satisfy.
C also represents the number of iterations in the interpolation algorithm [9, 11], in
which each iteration imposes a zero condition constraint to Qab.
Deﬁnition: For monomial φazb, where φa ∈Lw and Lw is the Hermitian curve’s pole
basis deﬁned in GF(w2), its (1, wz)-weighted degree is deﬁned as:
deg1,wz(φazb) = v p∞(φ−1
a ) + b · wz,

176
LIST DECODING
where wz is the weighted degree for variable z, and deﬁned as: wz = v p∞(z−1) =
v p∞(φ−1
k−1). φk−1 is the maximal term in the message polynomial. A (1, wz)-
lexicographic order (ord) can be deﬁned to arrange monomials φazb:
φa1zb1 < φa2zb2,
if deg1,wz(φa1zb1) < deg1,wz(φa2zb2), or deg1,wz(φa1zb1) = deg1,wz(φa2zb2) and b1 <
b2 [9]. If φazb is the maximal monomial in polynomial Q = 
a,b Qabφazb as:
φazb = max{φazb|Qab = 0}.
φazb is called Q’s leading monomial (LM) and its coefﬁcient Qab is called f’s
leading coefﬁcient (LC), denoted as: LM( f ) = φazb and LC( f ) = fab. Polynomial
Q’s (1, wz)-weighted degree (deg1,wz(Q)) and leading order (lod(Q)) are deﬁned as:
deg1,wz(Q) = deg1,wz(φazb),
and
lod(Q) = ord(φazb).
The (1, wz)-weighted degree upper bound of polynomial Q is deﬁned as [9, 11]:
max{deg1,wz Q} = lmv p∞(z −1) + tm,
(5.66)
where lm is the maximal number of output candidates from factorization, deﬁned as:
lm = max

u|
u
2

v p∞(z −1) −(u −1)g ≤C

−1,
(5.67)
and parameter tm is deﬁned as:
tm = max

u|(lm + 1)u − (u) +

lm + 1
2

v p∞(z −1) −lmg ≤C

,
(5.68)
where g is the genus of the Hermitian curve, u ∈N and  (u) denotes the number of
gaps that are less than or equal to the nonnegative integer u [11].
If there exists a polynomial h ∈Fuz
q [x, y] such that:
(h, R) = |{i|h(pi) = ri, i = 0, 1, . . . , n −1}|
(5.69)

LIST DECODING OF ALGEBRAIC–GEOMETRIC CODES
177
then the total zero orders of polynomial Q(x, y, h) over all the interpolated units is:
n−1

i=0
v pi(Q(x, y, h)) = m(h, R).
(5.70)
To deﬁne the total zero order of polynomial Q(x, y, h), the following lemma is
applied:
Lemma 5.4
If Q(x, y, z) has a zero of multiplicity m over unit (pi, ri) and h is a
polynomial in Fuz
q [x, y] that satisﬁes h(pi) = ri then Q(x, y, h) has a zero order of at
least m at pi, as v pi(Q(x, y, h)) ≥m [9, 11].
(5.69) deﬁnes the total number of afﬁne points that satisfy h(pi) = ri, and therefore
the total zero order of polynomial Q(x, y, h) over all the afﬁne points is deﬁned by
(5.70).
Theorem 5.1
If polynomial Q(x, y, h)’s total zero order is greater than its pole order,
as:
n−1

i=0
v pi(Q(x, y, h)) > v p∞(Q(x, y, h)−1),
(5.71)
then h is the z root of Q: Q(x, y, h) = 0, or equivalently z −h|Q(x, y, z) [6, 9, 11].
As h ∈Fuz
q [x, y], v p∞(Q(x, y, h)−1) = v p∞(Q(x, y, z)−1) = deg1,wz(Q(x, y, z)).
Therefore, based on (5.69) and (5.70), Theorem 5.1 results in the following corollary:
Corollary 5.1
If there exists a polynomial h ∈Fuz
q [x, y] such that:
m (h, R) > deg1,wz(Q(x, y, z))
(5.72)
then the list decoding outputs h can be found by factorizing the interpolated polynomial
Q(x, y, z) as: z −h|Q(x, y, z) [18].
If h = f, (5.69) deﬁnes the number of uncorrupted received symbols. Therefore,
the GS algorithm’s error-correction capability τ m is:
tm = n −(h, R) = n −
&deg1,wz Q
m
'
−1.
(5.73)
Since the upper bound of deg1,wz Q is deﬁned by (5.70):
tm ≥n −
&lmv p∞(z−1) + tm
m
'
−1.
(5.74)
The GS algorithm’s error-correction capability upper bound for a (n, k) Hermitian
code is deﬁned by:
tGS = n −
8
n(n −d∗)
9
−1.

178
LIST DECODING
5.5 Determining the Corresponding Coefﬁcients
Based on (5.62), the corresponding coefﬁcients γa,pi,u are critical for deﬁning the
zero condition of a polynomial in Fq[x, y, z]. Without knowing them, we have to
transfer a general polynomial written with respect to the zero basis functions and
ﬁnd the coefﬁcients Q(pi,ri)
uv
, which is not efﬁcient during the iterative interpolation.
In fact, the corresponding coefﬁcients γa,pi,u can be determined independently of the
received word. Therefore, if they can be determined beforehand and applied during the
iterations, the interpolation efﬁciency can be greatly improved. This section proposes
an algorithm to determine them.
The problem we intend to solve can be simply stated as: given an afﬁne point
pi = (xi, yi) of curve Hw and a pole basis monomial φa, determine the corresponding
coefﬁcients γa,pi,u so that φa can be written as a sum of the zero basis functions
ψpi,u : φa = 
u γa,pi,uψpi,u. For any two pole basis monomials φa1 and φa2 in Lw,
φa1φa2 = 
a∈N φa and the zero basis function ψpi,u (2.13) can be written as a sum of
pole basis monomials φa [9]:
ψpi,u =

a
ζaφa,
(5.75)
where coefﬁcients ζ a ∈GF(q). Partition ψpi,u(x, y) as:
ψpi,u = ψ A
pi,u · ψ B
pi,u,
(5.76)
where ψ A
pi,u = (x −xi)λ and ψ B
pi,u = [(y −yi) −xw
i (x −xi)]δ = [y −xw
i x −(yi −
xw+1
i
)]δ. It is easy to recognize that ψ A
pi,u has leading monomial LM(ψ A
pi,u) = xδ
and leading coefﬁcient LC(ψ A
pi,u) = 1. As v p∞(y−1) > v p∞(x−1), ψ B
pi,u has leading
monomial LM(ψ B
pi,u) = yδ and leading coefﬁcient LC(ψ B
pi,u) = 1. Based on (5.76),
ψpi,u has leading monomial LM(ψ A
pi,u) · LM(ψ B
pi,u) = xλyδ and leading coefﬁcient
LC(ψ A
pi,u) · LC(ψ B
pi,u) = 1. As 0 ≤λ ≤w and δ ≥0, the set of leading monomials
of zero basis functions in Zw,pi contains all the monomials deﬁned in pole basis Lw.
Summarizing the above analysis, Corollary 5.2 is proposed as follows:
Corollary 5.2
If φL is the leading monomial of zero basis function ψpi,u as
LM(ψpi,u) = φL, the leading coefﬁcient of ψpi,u equals 1 and (5.75) can be writ-
ten as [18]:
ψpi,u =

a<L
ζaφa + φL.
(5.77)
The set of leading monomials of zero basis functions in Zw,pi contains all the
monomials in Lw:
{LM(ψpi,u) = φL, ψpi,u ∈Zw,pi} ⊆Lw.
(5.78)

DETERMINING THE CORRESPONDING COEFFICIENTS
179
Following on, by identifying the second-largest pole basis monomial φL−1 with
coefﬁcient ζ L−1 ∈GF(q) in ψpi,u, (5.77) can also be written as [18]:
ψpi,u =

a<L−1
ζaφa + ζL−1φL−1 + φL.
(5.79)
Now it is sufﬁcient to propose the new efﬁcient algorithm in order to determine the
corresponding coefﬁcients γa,pi,u.
Algorithm 5.4: Determine the corresponding coefﬁcients γa,pi,u between a pole
basis monomial and zero basis functions [14, 18]
1. Initialize all corresponding coefﬁcients γa,pi,u = 0.
2. Find the zero basis function ψpi,u with LM(ψpi,u) = φa, and let γa,pi,u = 1.
3. Initialize function ˆψ = ψpi,u.
4. While ( ˆψ = φa)
{
5. Find the second-largest pole basis monomial φL−1 with coefﬁcient ζ L−1 in ˆψ.
6. In Zw,pi, ﬁnd a zero basis function ψpi,α whose leading monomial LM(ψpi,u) =
φL−1, and let the corresponding coefﬁcient γa,pi,u = ζL−1.
7. Update ˆψ = ˆψ + γa,pi,uψpi,u.
}
Proof: Notice that functions ψpi,α with LM(ψpi,u) > φa will not contribute to the
sum calculation of (5.62) and their corresponding coefﬁcients γa,pi,u = 0. The zero
basis function ψpi,u found at step 2 has leading monomial φL = φa. Based on (5.79),
it can be written as [18]:
ψpi,u =

a<L−1
ζaφa + ζL−1φL−1 + φa
(5.80)
(5.80) indicates that the corresponding coefﬁcient between φa and ψpi,u is 1;
γa,pi,u = 1. Polynomial ˆψ, initialized by step 3, is an accumulated polynomial result-
ing in φa. While ˆψ = φa, in (5.80), the second-largest monomial φL−1 with coefﬁcient
ζ L−1 is identiﬁed by step 5. Next ﬁnd another zero basis function ψpi,u in Zw,pi such
that LM(ψpi,u) = φL−1. According to Corollary 5.2, this zero basis function always
exists and it can be written as: ψpi,u = 
a<L−1 ζaφa + φL−1. At step 6, the corre-
sponding coefﬁcient between monomial φa and the found zero basis function ψpi,u
can be determined as: γa,pi,u = ζL−1. As a result, the accumulated calculation of step
7 can be written as:
ˆψ =

a<L−1
ζaφa + ζL−1φL−1 + φa + γa,pi,uψpi,u
(5.81)
=

a<L−1
ζaφa + ζL−1φL−1 + φa +

a<L−1
ζL−1ζaφa + ζL−1φL−1.

180
LIST DECODING
Therefore, in the new accumulated ˆψ, ζ L−1φL−1 is eliminated, while the leading
monomial φa is preserved. If the updated ˆψ = φa, its second-largest monomial φL−1
is again eliminated, while φa is always preserved as a leading monomial by the
same process. The algorithm terminates after all monomials that are smaller than φa
have been eliminated and results in ˆψ = φa. This process is equivalent to the sum
calculation of (5.62). Example 5.7 illustrates Algorithm 5.4.
Example 5.7: Given pi = (α2, α2) is an afﬁne point on curve H2 and a pole basis
(L2) monomial φ5 = y2, determine the corresponding coefﬁcients γ5,pi,u so that φ5
can be written as φ5 = 
u
γ5,pi,uψpi,u.
Based on (2.13), the ﬁrst eight zero basis functions in Z2,pi can be listed as:
ψpi,0 = (x −α2)0 = 1
ψpi,1 = (x −α2)1 = α2 + x
ψpi,2 = (x −α2)2 = α + x2
ψpi,3 = (y −α2) −α(x −α2) = α + αx + y
ψpi,4 = (x −α2)[(y −α2) −α(x −α2)] = 1 + α2x + α2y + αx2 + xy
ψpi,5 = (x −α2)2[(y −α2) −α(x −α2)] = α2 + α2x + αx2 + αy2 + x2y
ψpi,6 = [(y −α2) −α(x −α2)]2 = α2 + α2x2 + y2
ψpi,7 = (x −α2)[(y −α2) −α(x −α2)]2 = α + α2x + α2y + αx2 + xy2.
Initialize all γ5,pi,u = 0. In Z2,pi, as LM(ψpi,6) = φ5, we let γ5,pi,6 = 1 and
initialize the accumulated polynomial ˆψ = ψpi,6 = α2 + α2x2 + y2.
As ˆψ = φ5, its second-largest monomial φL−1 = x2 with coefﬁcient ζ L−1 = α2 is
identiﬁed. Among the zero basis functions in Z2,pi, we ﬁnd ψpi,2 with LM(ψpi,2) =
φL−1 = x2, and let γ5,pi,2 = ζL−1 = α2. Update ˆψ = ˆψ + γ5,pi,2ψpi,2 = α + y2.
As
ˆψ = φ5, again its second-largest monomial φL−1 = 1 with coefﬁcient
ζL−1 = α is identiﬁed. Among the zero basis functions in Z2,pi, we ﬁnd
ψpi,0 with LM(ψpi,0) = φL−1 = 1, and let γ5,pi,0 = ζL−1 = α. Update ˆψ = ˆψ +
γ5,pi,0ψpi,0 = y2.
Now, ˆψ = φ5, we can stop the algorithm and output γ5,pi,0 = α, γ5,pi,2 = α2
and γ5,pi,6 = 1. The rest of the corresponding coefﬁcients γ5,pi,u = 0 (u = 0, 2, 6).
Before interpolation, monomials φa that exist in the interpolated polynomial Q
are unknown. However, the (1, wz)-weighted degree upper bound of polynomial Q
is deﬁned by (5.70), from which the largest pole basis monomial φmax that might
exist in Q can be predicted by v p∞(φ−1
max) = max{deg1,wz Q}. Based on interpolation
multiplicity m, with parameter u < m, the corresponding coefﬁcients that might
be used in interpolation are γ0,pi,u ∼γmax,pi,u(u < m). Therefore Algorithm 5.4
can be used to determine all the corresponding coefﬁcients γ0,pi,u ∼γmax,pi,u and
only γ0,pi,u ∼γmax,pi,u (u < m) are stored for interpolation in order to minimize
the memory requirement. For example, to list decode the (8, 4, 4) Hermitian code

COMPLEXITY REDUCTION INTERPOLATION
181
with multiplicity m = 2, max{deg1,wz Q} = 13. Therefore, the largest pole basis
monomial that might exist in Q is φmax = φ12 = x2y3 and Algorithm 5.4 can be
applied to calculate all the corresponding coefﬁcients γ0,pi,u ∼γ12,pi,u (u < 2) that
are stored.
5.6 Complexity Reduction Interpolation
Interpolation determines a polynomial Q that intersects the points (pi, ri) (i = 0,
1, . . . , n −1). This can be implemented by an iterative polynomial construction
algorithm [4, 9, 11, 13]. At the beginning, a group of polynomials are initialized.
During the iterations, they are tested by different zero condition constraints and
modiﬁed interactively. As with the hard-decision list decoding of Reed–Solomon
codes, there are from (5.11) a total of C iterations, after which the minimal polynomial
in the group is chosen as the interpolated polynomial Q. According to the iterative
process analysis given in Section 5.2.4 and also [13], the interpolated polynomial Q
has leading order lod(Q) ≤C. This indicates that those polynomials with leading order
greater than C will not be the chosen candidates. Also, if there is a polynomial in the
group with leading order greater than C during the iterations, the chosen polynomial
Q will not be modiﬁed with this polynomial, otherwise lod(Q) > C. Therefore, those
polynomials with leading order greater than C can be eliminated from the group
during iterations in order to save unnecessary computations.
If f ∈Fq[x, y, z] has leading monomial LM( f ) = φazb, polynomials in Fq[x, y, z]
can be partitioned into the following classes according to their leading monomial’s
z-degree b and φa’s pole order v p∞(φ−1
a ) [18]:
Vλ+wδ = { f ∈Fq[x, y, z]|b = δ ∧v p∞(φ−1
a ) = uw + λ,
LM( f ) = φazb, (δ, u, λ) ∈N, λ < w},
(5.82)
such that Fq[x, y, z] = 
λ<w
Vλ+wδ. The factorization outputs are the z-roots of Q.
Therefore, the z-degree of Q is less than or equal to the maximal number of the output
list lm (5.67) and Q is a polynomial chosen from the following classes:
Vj = Vλ+wδ
(0 ≤λ < w, 0 ≤δ ≤lm).
(5.83)
At the beginning of the iterative process, a group of polynomials are initialized to
represent each of the polynomial classes deﬁned by (5.83) as:
G = {Q j = Qλ+wδ = yλzδ, Q j ∈Vj}.
(5.84)
During the iterations, each polynomial Qj in the group G is the minimal polynomial
within its class Vj that satisﬁes all the tested zero conditions. At the beginning of each
iteration, the polynomial group G is modiﬁed by [18]:
G = {Q j|lod(Q j) ≤C}
(5.85)

182
LIST DECODING
in order to eliminate those polynomials with leading order greater than C. Then the
remaining polynomials in G are tested by the zero condition constraint deﬁned by
(5.62) as:
 j = D(pi,ri)
uv
(Q j).
(5.86)
The determined corresponding coefﬁcients γa,pi,u are applied for this calculation.
Those polynomials with  j = 0 satisfy the zero condition and do not need to be
modiﬁed. However, those polynomials with  j = 0 need to be modiﬁed. Among
them, the index of the minimal polynomial is found as j and the minimal polynomial
is recorded as Q:
j = index ( min
lod(Q j){Q j| j = 0})
(5.87)
Q = Q j.
(5.88)
Q j is modiﬁed as:
Q j = (x −xi)Q,
(5.89)
where xi is the x-coordinate of afﬁne point pi which is included in the current inter-
polated unit (pi, ri). The modiﬁed Q j satisﬁes D(pi,ri)
uv
(Q j) = 0. Based on Prop-
erties 1 and 2 mentioned in Section 5.2.3, D(pi,ri)
uv
[(x −xi)Q] = D(pi,ri)
uv
(x Q) −
xi D(pi,ri)
uv
(Q) = xi D j −xi j = 0. The rest of the polynomials with  j = 0 are
modiﬁed as:
Q j =  j Q j − j Q.
(5.90)
The modiﬁed Qj satisﬁes D(pi,ri)
uv
(Q j) = 0 because D(pi,ri)
uv
[ j Q j − j Q] =
 j D(pi,ri)
uv
(Q j) − j D(pi,ri)
uv
(Q) =  j − j = 0. After C iterations, the minimal
polynomial in the group G is chosen as the interpolated polynomial Q:
Q = min
lod(Q j){Q j|Q j ∈G}.
(5.91)
From the above description, it can be seen that by applying the complexity reduc-
tion scheme in (5.85) the zero condition calculation from (5.86) and modiﬁcations
from (5.89) and (5.90), for those polynomials Qj with lod(Qj) > C, can be avoided
and therefore the interpolation efﬁciency can be improved. According to [13], this
complexity reduction scheme is error-dependent, so that it reduces complexity more
signiﬁcantly in low-error-weight situations. This is because the modiﬁcation scheme
of (5.89) takes action in earlier iteration steps for low-error-weight situations, and
therefore computation can be reduced. Figure 5.7 shows interpolation complexity
reduction (with different multiplicity m) when applying the scheme in (5.85) to de-
code the (64, 19, 40) Hermitian code. It is shown that complexity can be reduced

COMPLEXITY REDUCTION INTERPOLATION
183
1.00E+05
1.00E+06
1.00E+07
1.00E+08
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
Number of Errors
Computational Complexity
original interpolation (m=1)
complexity reducing
interpolation (m=1)
original interpolation (m=2)
complexity reducing
interpolation (m=2)
original interpolation (m=3)
complexity reducing
interpolation (m=3)
48.83%
41.34%
34.08%
19.69%
11.92%
5.89%
2.61%
30.17%
22.05%
14.28%
11.29% 10.60%
Figure 5.7
Complexity analysis for the interpolation of GS decoding Hermitian code (64, 19, 40).
signiﬁcantly in low-error-weight situations, especially when m = 1 where complexity
can be reduced up to 48.83%. However, in high-error-weight situations, complexity
reduction is not as signiﬁcant. Based on Figure 5.7, it can also be observed that the
complexity reduction also depends on the interpolation multiplicity m. When m = 1,
complexity reduction is most signiﬁcant; when m = 2, complexity reduction is the
most marginal.
Summarizing Sections 5.5 and 5.6, the modiﬁed complexity reduction interpolation
process for GS decoding Hermitian codes can be stated as follows:
Initial computation: Apply Algorithm 5.4 to determine all the necessary correspond-
ing coefﬁcients γa,pi,u and store them for use by the iterative polynomial construction
algorithm (Algorithm 5.5).
Algorithm 5.5: Iterative polynomial construction [14, 18]
Initialization:: Initial-
ize the group of polynomials G with (5.84).
1. For each interpolated unit (pi, ri) (i = 0, 1, . . . , n −1)
{
2. For each pair of the zero condition parameters (u, v) (u + v < m)
{
3. Modify polynomial group G by (5.85).
4. Test the zero condition  j of each polynomial in G with (5.86).
5. For polynomials Qj with  j = 0
{

184
LIST DECODING
6. Denote the minimal polynomial’s index as j by (5.87) and record it as Q by (5.88).
7. If j = j, Qj is modiﬁed by (5.89).
8. If j = j, Qj is modiﬁed by (5.90).
}}}
At the end of the iterations, the minimal polynomial Q is chosen from the group G,
as in (5.91).
Example 5.8 illustrates this complexity reduction interpolation process.
Example 5.8: Decode the (8, 4, 4) Hermitian code deﬁned in GF(4) using the GS
algorithm with interpolation multiplicity m = 2.
The Hermitian code word is generated by evaluating the message polynomial
over the following afﬁne points: p0 = (0, 0), p1 = (0, 1), p2 = (1, α), p3 = (1, α2),
p4 = (α, α), p5 = (α, α2), p6 = (α2, α), p7 = (α2, α2). The received word is given
as R = (1, α2, α, α2, α, α2, α2, α).
Applying (5.11), the iteration number C = 8
3
2

= 24. Based on C, the length
of the output list can be determined as l2 = 3, and of parameter t2 as t2 = 1, by using
(5.67) and (5.68) respectively. As a result, the (1, 4)-weighted degree upper bound
for the interpolated polynomial can be determined by (5.66) as max{deg1,4 Q} =
13. Therefore, the maximal pole basis (L2) monomial that might exist in the in-
terpolated polynomial is φmax = φ12 = x2y3. As the interpolation multiplicity
m = 2, Algorithm 5.4 is applied to determine the corresponding coefﬁcients
γ0,pi,u −γ12,pi,u and γ0,pi,u −γ12,pi,u (u < 2) are stored for the following in-
terpolation process. Table 5.4 lists all the resulted corresponding coefﬁcients
γ0,pi,u ∼γ12,pi,u (u < 2).
Following on, Algorithm 5.5 is performed to ﬁnd the interpolated polynomial
Q(x, y, z). At the beginning, a group of polynomials is initialized as: Q0 = 1,
Q1 = y, Q2 = z, Q3 = yz, Q4 = z2, Q5 = yz2, Q6 = z3, Q7 = yz3. Their leading
orders are: lod(Q0) = 0, lod(Q1) = 2, lod(Q2) = 4, lod(Q3) = 9, lod(Q4) = 12,
lod(Q5) = 20, lod(Q6) = 24, lod(Q7) = 35.
For interpolated unit, (p0, r0) = ((0, 0), 1).
For the zero parameter, u = 0 and v = 0.
As lod(Q7) > C, polynomial Q7 is eliminated from the group.
Test the zero condition of the remaining polynomials in the group as:
0 = D(p0,r0)
00
(Q0) = 1, 1 = D(p0,r0)
00
(Q1) = 0, 2 = D(p0,r0)
00
(Q2) = 1,
3 = D(p0,r0)
00
(Q3) = 0,
4 = D(p0,r0)
00
(Q4) = 1, 5 = D(p0,r0)
00
(Q5) = 0, 6 = D(p0,r0)
00
(Q6) = 1.
Find the minimal polynomial with j = 0 as:
j = 0
and
Q = Q0.

COMPLEXITY REDUCTION INTERPOLATION
185
Table 5.4
Predetermined corresponding coefﬁcients for Example 5.8.
p0, γa,p0,u
a/u
0
1
2
3
4
5
6
7
8
9
10
11
12
0
1
0
0
0
0
0
0
0
0
0
0
0
0
1
0
1
0
0
0
0
0
0
0
0
0
0
0
p1, γa,p1,u
a/u
0
1
2
3
4
5
6
7
8
9
10
11
12
0
1
0
1
0
0
1
0
0
1
0
0
1
0
1
0
1
0
0
1
0
0
1
0
0
1
0
0
p2, γa,p2,u
a/u
0
1
2
3
4
5
6
7
8
9
10
11
12
0
1
1
α
1
α
α2
α
α2
1
α2
1
α
1
1
0
1
1
0
α2
0
1
α2
α2
0
α
0
α2
p3, γa,p3,u
a/u
0
1
2
3
4
5
6
7
8
9
10
11
12
0
1
1
α2
1
α2
α
α2
α
1
α
1
α2
1
1
0
1
1
0
α
0
1
α
α
0
α2
0
α
p4, γa,p4,u
a/u
0
1
2
3
4
5
6
7
8
9
10
11
12
0
1
α
α
α2
α2
α2
1
1
1
α
α
α
α2
1
0
1
α2
0
α2
0
α
α2
α
0
α
0
1
p5, γa,p5,u
a/u
0
1
2
3
4
5
6
7
8
9
10
11
12
0
1
α
α2
α2
1
α
α
α2
1
1
α
α2
α2
1
0
1
α2
0
α
0
α
α
1
0
α2
0
α2
p6, γa,p6,u
a/u
0
1
2
3
4
5
6
7
8
9
10
11
12
0
1
α2
α
α
1
α2
α2
α
1
1
α2
α
α
1
0
1
α
0
α2
0
α2
α2
1
0
α
0
α
p7, γa,p7,u
a/u
0
1
2
3
4
5
6
7
8
9
10
11
12
0
1
α2
α2
α
α
α
1
1
1
α2
α2
α2
α
1
0
1
α
0
α
0
α2
α
α2
0
α2
0
1
As 1 = 3 = 5 = 0:
Q1 = Q1 = y, and lod(Q1) = 2
Q3 = Q3 = yz, and lod(Q3) = 9
Q5 = Q5 = yz2, and lod(Q5) = 20.
Modify polynomials in the group with j = 0 as:
Q0 = (x −0)Q = x, and lod(Q0) = 1
Q2 = 0Q2 −2Q = 1 + z, and lod(Q2) = 4
Q4 = 0Q4 −4Q = 1 + z2, and lod(Q4) = 12
Q6 = 0Q6 −6Q = 1 + z3 and lod(Q6) = 24.
For the zero parameter, u = 0 and v = 1.
As there is no polynomial in the group with leading order over C, no polynomial
is eliminated in this iteration.

186
LIST DECODING
Test the zero condition of the remaining polynomials in the group as:
0 = D(p0,r0)
01
(Q0) = 0, 1 = D(p0,r0)
01
(Q1) = 0, 2 = D(p0,r0)
01
(Q2) = 1,
3 = D(p0,r0)
01
(Q3) = 0,
4 = D(p0,r0)
01
(Q4) = 0, 5 = D(p0,r0)
01
(Q5) = 0, 6 = D(p0,r0)
01
(Q6) = 1.
Find the minimal polynomial with j = 0 as:
j = 2
and
Q = Q2.
As 0 = 1 = 3 = 4 =5 = 0:
Q0 = Q0 = x, and lod(Q0) = 1
Q1 = Q1 = y, and lod(Q1) = 2
Q3 = Q3 = yz, and lod(Q3) = 9
Q4 = Q4 = 1 + z2, and lod(Q4) = 12
Q5 = Q5 = yz2 and lod(Q5) = 20.
Modify polynomials in the group with j = 0 as:
Q2 = (x −0)Q = x + xz and lod(Q2) = 7
Q6 = 2Q6 −6Q = z + z3 and lod(Q6) = 24.
For zero parameter, u = 1 and v = 0.
As there is no polynomial in the group with leading order over C, no polynomial
is eliminated in this iteration.
Test the zero condition of the remaining polynomials in the group as:
0 = D(p0,r0)
10
(Q0) = 1, 1 = D(p0,r0)
10
(Q1) = 0, 2 = D(p0,r0)
10
(Q2) = 0,
3 = D(p0,r0)
10
(Q3) = 0,
4 = D(p0,r0)
10
(Q4) = 0, 5 = D(p0,r0)
10
(Q5) = 0, 6 = D(p0,r0)
10
(Q6) = 0.
Find the minimal polynomial with j = 0 as:
j = 0
and
Q = Q0.
As 1 = 2 = 3 = 4 =5 = 6 = 0:
Q1 = Q1 = y, and lod(Q1) = 2
Q2 = Q2 = x + xz, and lod(Q2) = 7
Q3 = Q3 = yz, and lod(Q3) = 9
Q4 = Q4 = 1 + z2, and lod(Q4) = 12
Q5 = Q5 = yz2, and lod(Q5) = 20
Q6 = Q6 = z + z3, and lod(Q6) = 24.

GENERAL FACTORIZATION
187
Modify polynomials in the group with j = 0 as:
Q0 = (x −0)Q = x2, and lod(Q0) = 3.
Following the same process, interpolation runs through the rest of the interpo-
lated units (p1, r1) ∼(p7, r7) and with respect to all zero parameters (u, v) = (0,
0), (0, 1) and (1, 0). After C iterations, the chosen interpolated polynomial is: Q(x,
y, z) = α2 + αx + y + αx2 + y2 + α2x2y + α2xy2 + α2y3 + α2x2y2 + z(x + xy +
xy2) + z2(α2 + αx + α2y + αx2), and lod(Q(x, y, z)) = 23. Polynomial Q(x, y, z)
has a zero of multiplicity at least 2 over the eight interpolated units.
5.7 General Factorization
Based on the interpolated polynomial, factorization ﬁnds the polynomial’s z-roots
in order to determine the output list. Building upon the work of [15, 20], this sec-
tion presents a generalized factorization algorithm, or the so-called recursive coef-
ﬁcient search algorithm, which can be applied to both Reed–Solomon codes and
algebraic–geometric codes. This section’s work is based on [21]. In general, the al-
gorithm is described with application to algebraic–geometric codes. Therefore, it has
to be stated that when applying this algorithm to Reed–Solomon codes, the rational
functions in an algebraic–geometric code’s pole basis are simpliﬁed to univariate
monomials in the Reed–Solomon code’s pole basis. As a consequence, polynomials
in Fq[x, y] are simpliﬁed to univariate polynomials with variable x.
Those polynomials h ∈Fwz
q [x, y] will be in the output list if Q(x, y, h) = 0. The
outcome of the factorization can be written as:



h1 = h1,0φ0 + · · · + h1,k−1φk−1
...
hl = hl,0φ0 + · · · + hl,k−1φk−1
,
(5.92)
with l ≤lm. Rational functions φ0, . . . , φk−1 are predetermined by the decoder;
therefore, ﬁnding the list of polynomials is equivalent to ﬁnding their coefﬁcients,
h1,0, . . . , h1,k−1, . . . , hl,0, . . . , hl,k−1, respectively. Substituting h into the interpolated
polynomial Q = 
a,b
Qabφazb, we have:
Q(x, y, h) =

a,b
Qabφahb =

a,b
Qabφa(h0φ0 + · · · + hk−1φk−1)b.
(5.93)
It is important to notice that:
(φiφ j) mod ζ =

v
φv,
(5.94)

188
LIST DECODING
where χ is the algebraic curve (e.g. Hermitian curve Hw) and φi, φj and φv are
rational functions in the pole basis associated with the curve χ (for example the pole
basis Lw associated with curve Hw). Therefore (5.93) can be rewritten as a polynomial
in Fq[x, y]:
Q(x, y, h) =

a
Qaφa,
(5.95)
where
coefﬁcients
Qa
are
equations
with
unknowns
h0, . . . ,
hk−1.
If
T = |{Qa|Qa = 0}|, the rational functions φa with Qa = 0 can be arranged
as φa1 < φa2 < · · · < φaT and (5.95) can again be rewritten as:
Q(x, y, h) = Qa1φa1 + Qa2φa2 + · · · + QaT φaT .
(5.96)
Again, coefﬁcients Qa1, Qa2, . . . , and QaT are equations of unknowns h0, . . . , hk−1.
To have Q(x, y, h) = 0, we need Qa1 = Qa2 = · · · = QaT = 0. Therefore, h0, . . . , hk−1
can be determined by solving the following simultaneous set of equations [21]:



Qa1(h0, . . . , hk−1) = 0
Qa2(h0, . . . , hk−1) = 0
...
QaT (h0, . . . , hk−1) = 0
.
(5.97)
In order to solve (5.97), a recursive coefﬁcient search algorithm is applied to
determine h0, . . . , hk−1 [20, 22]. Here a more general and efﬁcient factorization
algorithm is presented. Let us denote the following polynomials with respect to a
recursive index s (0 ≤s ≤k −1):
h(s)(x, y) = h0φ0 + · · · + hk−1−sk−1−s,
(5.98)
which is a candidate polynomial with coefﬁcients h0, . . . , hk−1−s undetermined. Up-
date Q(x, y, z) recursively as:
Q(s+1)(x, y, z) = Q(s)(x, y, z + hk−1−sφk−1−s),
(5.99)
with Q(0)(x, y, z) = Q(x, y, z), which is the interpolated polynomial (5.68). Substituting
hk−1−sφk−1−s into Q(s)(x, y, z), we have:
˜Q(s)(x, y) = Q(s)(x, y, hk−1−sφk−1−s).
(5.100)
˜Q(s) mod χ can be transferred into a polynomial in Fq[x, y] with coefﬁcients
expressed as 
i
ihi
k−1−s where  i ∈GF(q). Denote ˜Q(s)’s leading monomial with
its leading coefﬁcient as [21]:
φ(s)
L = LM( ˜Q(s))
(5.101)
C(s)
L (hk−1−s) = LC( ˜Q(s)).
(5.102)

GENERAL FACTORIZATION
189
Based on (5.102), it can be seen that LM(h(s)) = φk−1−s and LC(h(s)) = hk−1−s.
Therefore, for any recursive polynomial Q(s)(x, y, z), we have:
LM(Q(s)(x, y, h(s))) = LM(Q(s)(x, y, hk−1−sφk−1−s)) = LM( ˜Q(s)) = φ(s)
L ) (5.103)
LC(Q(s)(x, y, h(s))) = LC(Q(s)(x, y, hk−1−s φk−1−s)) = LC( ˜Q(s)) = C(s)
L (hk−1−s).
(5.104)
As all the candidate outputs should satisfy Q(x, y, h) = 0, and from the above
deﬁnitions it can be seen that h = h(0) and Q(0)(x, y, z) = Q(x, y, z), Q(x, y, h) = 0
is equivalent to Q(0)(x, y, h(0)) = 0. Based on (5.107) and (5.108), in order to have
Q(0)(x, y, h(0)) = 0 we need to ﬁnd its leading monomial φ(0)
L with leading coefﬁcient
C(0)
L (hk−1) and determine the values of hk−1 that satisfy C(0)
L (hk−1) = 0. As a result,
the leading monomial of Q(0)(x, y, h(0)) is eliminated. Based on each value of hk−1 and
performing the polynomial update (5.99), Q(1)(x, y, z) is generated, in which φ(0)
L has
been eliminated. Now, Q(x, y, h) = 0 is equivalent to Q(1)(x, y, h(1)) = 0. Again, to have
Q(1)(x, y, h(1)) = 0, we need C(1)
L (hk−2) = 0. Therefore, hk−2 can be determined by
solving C(1)
L (hk−2) = 0. Based on each value of hk−2, we can trace further to ﬁnd the
rest of the coefﬁcients. In general, after the coefﬁcients hk−1−s (0 ≤s < k −1) have
been determined by solving C(s)
L (hk−1−s) = 0, based on each value of them, perform
the polynomial update (5.99) to generate Q(s+1)(x, y, z). From Q(s+1)(x, y, z), ˜Q(s+1)
can be calculated and hk−1−(s+1) can be determined by solving C(s+1)
L
(hk−1−(s+1)) = 0.
This process is illustrated in Figure 5.8.
From Figure 5.8 it can be seen that there might be an exponential number of routes
to ﬁnd coefﬁcients hk−1, . . . , h0. However, not every route will be able to reach h0, as
during the recursive process there may be no solution for C(s)
L (hk−1−s) = 0 = 0. If h0
is produced and Q(k−1)(x, y, h0φ0) = 0, this route can be traced to ﬁnd the rest of the
Q updates
Q(5)(x, y, z)
hk, 1–5
hk, 1–(5+1)
hk, 1–(5+1)
hk, 1–(5+1)
hk, 1–(5+1)
hk, 1–5
Q(5+1)(x, y, z)
.
.
.
.
.
.
.
.
.
.
.
Figure 5.8
Recursive coefﬁcient search.

190
LIST DECODING
coefﬁcients h1, . . . , hk−1 to construct polynomial h which will satisfy Q(x, y, h) = 0.
The correctness of this judgement will be proven later.
Based on the above description, the generalized factorization algorithm is summa-
rized in Algorithm 5.6.
Algorithm 5.6: Recursive coefﬁcient search [14, 21, 22]
Initialization: Q(0)(x, y,
z) = Q(x, y, z). The recursive index s = 0 and output candidate index l = 1.
Perform: Recursive coefﬁcient search (s) (RCS(s)).
Recursive coefﬁcient search (RCS):
Input parameter: s (0 ≤s ≤k −1).
1. Perform (5.100) to calculate ˜Q(s)(x, y).
2. Find out φ(s)
L with its coefﬁcient C(s)
L (hk−1−s).
3. Determine hk−1−s by solving C(s)
L (hk−1−s) = 0.
4. For each value of hk−1−s, do
{
5. hl,k−1−s = hk−1−s.
6. If s = k −1, calculate Q(k−1)(x, y, h0φ0) and go to step 7. Else go to step 8.
7. If Q(k−1)(x, y, h0φ0) = 0, trace this route to ﬁnd coefﬁcients hl,k−1, hl,k−2, . . . , and
hl,0 to construct the candidate polynomial hl and l = l + 1. Else stop this route.
8. Perform polynomial update (5.99) to generate Q(s+1)(x, y, z).
9. Perform RCS(s + 1).
}
If a number of hk−1−s have been determined, the algorithm will choose one of them
to determine the rest of the coefﬁcients until all the possible routes starting from this
hk−1−s have been traced. After this, it will choose another value of hk−1−s and repeat
the process. This algorithm will terminate after all the possible routes started from
hk−1 have been traced. To prove the correctness of this algorithm, we need to justify
the fact that the polynomial hl produced in step 7 satisﬁes Q(x, y, hl) = 0.
Proof: As Q(k−1)(x, y, h0φ0) = 0 and h(k−1)(x, y) = h0φ0, we have Q(k−1)(x, y,
h(k−1)) = 0. Assuming h1 is the previously-found coefﬁcient, Q(k−1)(x, y, z) is
generated by (5.99): Q(k−1)(x, y, z) = Q(k−2)(x, y, z + h1φ1). From Q(k−1)(x, y,
h(k−1)) = 0 we have Q(k−2)(x, y, h(k−1) + h1φ1) = 0. Based on (5.98), it can be seen that
h(k−2) = h0φ0 + h1φ1 = h(k−1) + h1φ1. Therefore, Q(k−2)(x, y, h(k−2)) = 0. It can be
deduced further to get Q(k−3)(x, y, h(k−3)) = 0, . . . , and Q(0)(x, y, h(0)) = 0. As
Q(0)(x, y, z) = Q(z) and h(0) = h0φ0 + h1φ1 + · · · + hk−1φk−1, whose coefﬁcients
have been traced as the coefﬁcients of the output candidate hl, it can be concluded that
Q(x, y, hl) = 0.
Here two worked examples are given to illustrate the generalized factorization
algorithm applied to both a Hermitian code and a Reed–Solomon code.

GENERAL FACTORIZATION
191
Example 5.9: List decoding of a (8, 4, 4) Hermitian code deﬁned in GF(4)
Given the interpolated polynomial is Q(x, y, z) = α2y + α2x2 + αxy + α2y2 +
α2x2y + x2y2 + xy3 + (αx + αxy + αxy2)z + (x + x2)z2, apply Algorithm 5.6 to
ﬁnd out its z-roots.
Initialization: Q(0)(x, y, z) = Q(x, y, z), s = 0 and l = 1.
RCS(0):
˜Q(0)(x, y) = Q(0)(x, y, h3x2) = (α2 + αh3)y + α2x2 + αxy + (α2 + h2
3)y2 + (α2
+ h2
3)x2y + (1 + h2
3)x2y2 + xy3 + (αh3 + h2
3)y4, with φ(0)
L = y4 and C(0)
L (h3) = αh3
+ h2
3. Solving C(0)
L (h3) = 0, we have h3 = 0 or h3 = α.
For h3 = 0, h1,3 = h3 = 0. As s = 0 < 3, update Q(1)(x, y, z) = Q(0)(x, y, z +
0x2) = Q(x, y, z), and perform RCS(1). . .
Based on the same progress, the outcomes from RCS(1), RCS(2) and RCS(3) are
summarized in Table 5.5.
Table 5.5
Recursive coefﬁcient search from h3 = 0.
RCS(s)
φ(s)
L
C(s)
L (hk−1−s)
h2,k−1−s = hk−1−s
RCS(1)
xy3
1 + αh2
α2
RCS(2)
x2y2
α2 + αh1
α
RCS(3)
xy2
αh0
0
After RCS(2) we have Q(3)(x, y, z) = (αx + αxy + αxy2)z + (x + x2)z2. In
RCS(3), by solving C(3)
L (h0) = 0, we have h0 = 0. Therefore, h1,0 = h0 = 0. As
s = 3 and Q(3)(x, y, h0φ0) = Q(3)(x, y, 0·1) = 0, this route can be traced to construct
candidate polynomial h1 = αx + α2y and update the candidate index l = l +
1 = 2.
Going back to the closest division point (when s = 0), we have:
For h3 = α, h2,3 = h3 = α. As s = 0 < 3, update Q(1)(x, y, z) = Q(0)(x, y, z + αx2)
= α2x2 + αxy + αx2y2 + xy3 + (αx + αxy + αxy2)z + (x + x2)z2 and perform
RCS(1). . .
Again, the outcomes of RCS(1), RCS(2) and RCS(3) are summarized in Table
5.6.
Table 5.6
Recursive coefﬁcient search from h3 = α.
RCS(s)
φ(s)
L
C(s)
L (hk−1−s)
h2,k−1−s = hk−1−s
RCS(1)
xy3
1 + αh2
α2
RCS(2)
x2y2
αh1
0
RCS(3)
xy2
α2 + αh0
α
After RCS(2) we have Q(3)(x, y, z) = α2x2 + α2xy + α2xy2 + (αx + αxy +
αxy2)z + (x + x2)z2. In RCS(3), by solving C(3)
L (h0) = 0, we have h2,0 = h0 = α.

192
LIST DECODING
As s = 3 and Q(3)(x, y, h0φ0) = Q(3)(x, y, α·1) = 0, this route can be traced to
construct the candidate polynomial h2 = α + α2y + αx2. As all the possible routes
from h0 have been traced, the factorization process terminates and outputs: h1 =
αx + α2y, h2 = α + α2y + αx2.
Example 5.10: List decoding of a (7, 2, 6) Reed–Solomon code deﬁned in GF(8)
α is a primitive element in GF(8) satisfying α3 + α + 1 = 0.
Given the interpolated polynomial Q(x, z) = αx + α6x2 + (α3 + α3x)z + α2z2,
apply Algorithm 5.6 to determine its z-roots.
Initialization: Q(0)(x, z) = Q(x, z), s = 0 and l = 1.
RCS(0):
˜Q(0)(x, z) = Q(0)(x, h1x) = (α + α3h1)x + (α6 + α3h1 + α2h2
1)x2, with φ(0)
L = x2
and C(0)
L (h1) = α6 + α3h1 + α2h2
1. Solving C(0)
L (h1) = 0, we have h1 = α5 or h1 =
α6.
For h1 = α5, h1,1 = h1 = α5. As s = 0 < 1, update Q(1)(x, z) = Q(0)(x, z + α5x)
= (α3 + α3x)z + α2z2 and perform RCS(1).
In RCS(1), following the same progress, we have φ(1)
L = x and C(1)
L (h0) = α3h0.
Solving C(1)
L (h0) = 0, we have h0 = 0.
For h0 = 0, h1,0 = h0 = 0. As s = 1 and Q(1)(x, h0φ0) = Q(1)(x, 0·1) = 0, this
route can be traced to construct candidate polynomial h1 = α5x. Update the output
candidate index as l = l + 1 = 2.
Going back to the closest division point (when s = 0), we have:
For h1 = α6, h2,1 = h1 = α6. As s = 0 < 1, update Q(1)(x, z) = Q(0)(x, z + α6x)
= α4x + (α3 + α3x)z + α2z2, and perform RCS(1).
In RCS(1) we have φ(1)
L = x and C(1)
L (h0) = α4 + α3h0. Solving C(1)
L (h0) = 0,
we have h0 = α.
For h0 = α, h2,0 = h0 = α. As s = 1 and Q(1)(x, h0φ0) = Q(1)(x, α·1) = 0, this
route can be traced to construct candidate polynomial h2 = α + α6x. As all the
possible routes from h0 have been traced, the factorization process terminates and
outputs: h1 = α5x, h2 = α + α6x.
5.8 Soft-Decision List Decoding of Hermitian Codes
We can extend the hard-decision GS decoding algorithm for AG codes to soft-decision
by extending the Kotter–Vardy algorithm. As with Reed–Solomon codes, soft-decision
list decoding is almost the same as hard-decision list decoding, with the addition of
a process to convert the reliability values of the received symbols into a multiplicity
matrix. In this section, a comparison of the code word scores is made between soft-
and hard-decision list decoding and conditions are derived to ensure that the equations
from the interpolation process are solvable.

SOFT-DECISION LIST DECODING OF HERMITIAN CODES
193
Based on wz, the two parameters ﬁrst deﬁned by (5.50) and (5.51) for analyzing
bivariate monomial xayb (a, b ∈N) can be extended to trivariate monomials φazb (a,
b ∈N):
N1,wz(d) = |{φazb : a, b ≥0 and deg1,wz(φazb) ≤δ, δ ∈N}|,
(5.105)
which represents the number of monomials with (1, wz)-weighted degree not greater
than δ, and:
1,wz(v) = min {δ : N1,wz(δ) > v, v ∈N},
(5.106)
which represents the minimal value of δ that guarantees N1,wz(δ) is greater than v.
The difference between soft-decision list decoding of Reed–Solomon codes
and of Hermitian codes is in the size of the reliability matrix. For soft-
decision list decoding of Reed–Solomon codes, the reliability matrix  has size
q × n, where n = q −1. For soft-decision list decoding of Hermitian codes, the
reliability matrix  has size q × n, where n = q3/2.
5.8.1 System Solution
The reliability matrix  is then converted to the multiplicity matrix M, for which
Algorithm 5.3 is applied. A version of Algorithm 5.3 introducing a stopping rule
based on the designed length of output list is presented later in this subsection, as
Algorithm 5.7.
In this subsection, the code word score with respect to matrix M is analyzed so
as to present the system solution for this soft-decision list decoder. It is shown that
the soft-decision scheme provides a higher code word score than the hard-decision
scheme.
The resulting multiplicity matrix M can be written as:
M =


m0,0
m0,1
· · ·
· · ·
· · ·
m0,n−1
m1,0
m1,1
m1,n−1
...
...
...
...
mi, j
...
...
...
...
mq−1,0
mq−1,1
· · ·
· · ·
· · ·
mq−1,n−1


,
(5.107)
where entry mi,j represents the multiplicity for unit (pj, ρi). Interpolation builds the
minimal polynomial QM ∈Fq[x, y, z], which has a zero of multiplicity at least mi,j

194
LIST DECODING
(mi,j = 0) over all the associated units (pj, ρi). Following on from (5.62), with respect
to interpolated unit (pj, ρi), QM’s coefﬁcients Qab should satisfy:

a,b≥v
Qab
b
v

γa,p j,uρb−v
i
= 0, ∀u, v ∈N and u + v < mi, j.
(5.108)
For this soft-decision interpolation, the number of interpolated units covered by
QM is:
|{mi, j = 0|mi, j ∈M, i = 0, 1, . . . , q −1 and j = 0, 1, . . . , n −1}|
(5.109)
and the cost CM of multiplicity matrix M is:
CM = 1
2
q−1

i=0
n−1

j=0
mi, j(mi, j + 1),
(5.110)
which represents the number of constraints in (5.108) to QM’s coefﬁcients Qab. These
can be imposed by the iterative polynomial construction algorithm [9, 11, 17] in CM
iterations. Notice that (5.109)) and (5.110)) have the same expression as (5.45) and
(5.46), respectively, with the exception that n = q3/2.
Based on Lemma 5.1, the following units’ multiplicities will contribute to the code
word score: (p0, c0), (p1, c1), . . . , and (pn−1, cn−1). Referring to the multiplicity matrix
in (5.107), the interpolated polynomial QM can be explained as passing through these
units with multiplicity at least m0 = mi,0 (ρi = c0), m1 = mi,1 (ρi = c1), . . . , and
mn−1 = mi,n−1 (ρi = cn−1), respectively. If f ∈Fwz
q [x, y] is the transmitted message
polynomial such that f(pi) = ci, the total zero order of QM(x, y, f) over units {(p0, c0),
(p1, c1), . . . , (pn−1, cn−1)} is at least:
m0 + m1 + · · · + mn−1 =
n−1

j=0
{mi, j|ρi = c j, i = 0, 1, . . . , q −1}.
(5.111)
Therefore, the code word score SM(c) with respect to multiplicity matrix M is:
SM (c) =
n−1

j=0
{mi, j|ρi = c j, i = 0, 1, . . . , q −1}.
(5.112)
If
SM (c) > deg1,wz(QM(x, y, z))
then
n−1
i=0 v pi(QM(x, y, f )) > v p∞(QM
(x, y, f )−1) and QM(x, y, f) = 0. f can be found by determining QM(x, y, z)’s z-roots.
It results in the following corollary for successful soft-decision list decoding:

SOFT-DECISION LIST DECODING OF HERMITIAN CODES
195
Corollary 5.3
If the code word score with respect to multiplicity matrix M is greater
than the interpolated polynomial QM’s (1, wz)-weighted degree [23]:
SM (c) > deg1,wz(QM(x, y, z))
(5.113)
then QM(x, y, f) = 0 or z −f |QM(x, y, z).
To compare the soft-decision’s code word score SM (c) with the hard-decision’s
code word score SM (c), denote the index of the maximal element in each column of
 as:
i j = index (max{πi, j|i = 0, 1, . . . , q −1}),
(5.114)
such that πi j, j > πi, j(i = i j). The hard-decision received word R can be written as:
R = (r0,r1, . . . ,rn−1) = (ρi0, ρi1, . . . , ρin−1).
(5.115)
For hard-decision list decoding, only those entries in the multiplicity matrix from
(5.107) that correspond to the reliability value πi j, j will be assigned a multiplicity, as
mi j, j = m, and therefore the score for hard-decision can also be written with respect
to multiplicity matrix M as:
Sm (c) = SM (c) =
n−1

j=0
{mi j, j|ρi j = c j}.
(5.116)
Comparing (5.112) and (5.116), the soft-decision list decoder gains its improve-
ments by increasing its code word score. This is done by increasing the total number
of interpolated units (5.111) so that the possibility of covering more interpolated units,
including the corresponding code word symbols, is also increased.
If the (1, wz)-weighted degree of interpolated polynomial QM is δ*, based on (5.106),
QM has at most N1,wz(δ*) nonzero coefﬁcients. The interpolation procedure generates
a system of CM linear equations of type (5.108). The system will be solvable if [5]:
N1,wz(δ∗) > CM.
(5.117)
Based on (5.106), in order to guarantee the solution, the (1, wz)-weighted degree
δ* of the interpolated polynomial QM should be large enough that:
deg1,wz(QM(x, y, z)) = d∗= 1,wz(CM).
(5.118)
Therefore, based on Corollary 5.3, given the soft-decision code word score (5.116)
and the (1, wz)-weighted degree of the interpolated polynomial QM (5.118), the mes-
sage polynomial f can be found if [23]:
SM (c) > 1,wz(CM).
(5.119)

196
LIST DECODING
The factorization output list contains the z-roots of polynomial QM. Therefore,
the maximal length of output list lM should be equal to polynomial QM’s z-degree
(degzQM), as:
lM = degz(QM(x, y, z)) =
&deg1,wz(QM(x, y, z))
wz
'
=
&1,wz(CM)
wz
'
.
(5.120)
When converting matrix  to matrix M, based on a designed length of output list
l, Algorithm 5.1 will stop once lM is greater than l. 1,wz(CM) can be determined by
ﬁnding the monomial with (1, wz)-lexicographic order CM, as:
1,wz(CM) = deg1,wz(φazb|ord(φazb) = CM).
(5.121)
Therefore, in order to assess the soft-decision list decoding algorithm’s performance
with a designed length of output list l, a large enough value must be set for s when
initializing Algorithm 5.3. In the algorithm, after step 5, we can determine the cost
CM (5.110) of the updated matrix M and apply (5.121) to determine 1,wz(CM). The
maximal length of output list lM can then be determined by (5.120). Stop Algorithm
5.3 once lM is greater than l and output the updated matrix M.
Algorithm 5.7 is based on Algorithm 5.3 and includes this stopping rule. This
algorithm is used to obtain the simulation results shown later.
Algorithm 5.7: Convert reliability matrix  to multiplicity matrix M [14, 23]
Input: Reliability matrix , a high enough desired value of the sum of multiplicities
in matrix M:
s =
q−1

i=0
n−1

j=0
mi, j, and designed output length l.
Initialization: Set * =  and q × n all-zero multiplicity matrix M.
1. While (s > 0 or lM < l)
{
2. Find the maximal entry π∗
i, j in * with position (i, j).
3. Update π∗
i, j in * as π∗
i, j =
πi, j
mi, j+2.
4. Update mi, j in M as mi, j = mi, j + 1.
5. s = s −1.
6. For the updated M, calculate its interpolation cost CM by (5.110).
7. Determine 1,wz(CM) by (5.121).
8. Calculate lM by (5.120).
}
Again, this algorithm gives priority to those interpolated points which correspond
to a higher reliability value πi,j, to be assigned with a higher multiplicity value mi, j.
For example, if πi1 j1 < πi2 j2 then mi1 j1 ≤mi2 j2.
The interpolation and factorization methods given in Algorithms 5.5 and 5.6 are
unchanged for soft-decision list decoding of AG codes. Again, as the total number of

SOFT-DECISION LIST DECODING OF HERMITIAN CODES
197
1.0E–06
1.0E–05
1.0E–04
1.0E–03
1.0E–02
1.0E–01
1.0E+00
10
9
8
7
6
5
4
3
2
1
0
Eb/N0 [dB]
BER
Uncoded
Sakata
list hard (l=1, m=1)
list hard (l=2, m=2)
list hard (Optimal)
list soft (l=1)
list soft (l=2)
list soft (l=5)
list soft (l=10)
list soft (l=20)
list soft (l=30)
list soft (Optimal)
(a) over AWGN channel 
1.0E–06
1.0E–05
1.0E–04
1.0E–03
1.0E–02
1.0E–01
1.0E+00
10
9
8
7
6
5
4
3
2
1
0
Eb/N0 [dB]
BER
uncoded
Sakata
list hard (l=1, m=1)
list hard (l=2, m=2)
list hard (Optimal)
list soft (l=1)
list soft (l=2)
list soft (l=5)
list soft (l=10)
list soft (l=20)
list soft (l=30)
list soft (Optimal)
(b) over Rayleigh fading channel 
Figure 5.9
Soft-decision list decoding of Hermitian code (64, 39, 20).

198
LIST DECODING
1.0E–06
1.0E–05
1.0E–04
1.0E–03
1.0E–02
1.0E–01
1.0E+00
10
9
8
7
6
5
4
3
2
1
0
Eb/N0 [dB]
BER
Uncoded
Sakata
list hard (l=1, m=1)
list hard (l=2, m=2)
list hard (Optimal)
list soft (l=1)
list soft (l=2)
list soft (l=5)
list soft (l=10)
list soft (l=20)
list soft (l=30)
list soft (Optimal)
(a) over AWGN channel 
1.0E–06
1.0E–05
1.0E–04
1.0E–03
1.0E–02
1.0E–01
1.0E+00
10
9
8
7
6
5
4
3
2
1
0
Eb/N0 [dB]
BER
uncoded
Sakata
list hard (l=1, m=1)
list hard (l=2, m=2)
list hard (Optimal)
list soft (l=1)
list soft (l=2)
list soft (l=5)
list soft (l=10)
list soft (l=20)
list soft (l=30)
list soft (Optimal)
(b) over Rayleigh fading channel 
Figure 5.10
Soft-decision list decoding of Hermitian code (512, 289, 196).

CONCLUSIONS
199
iterations CM has been determined before running the interpolation, those polynomials
with leading order greater than CM can be eliminated from the group during the
iterations [23].
5.8.2 Simulation Results
The performance of hard- and soft-decision list decoding for Hermitian codes is
evaluated by simulation results on the AWGN channel and a frequency nonselective
Rayleigh fading channel with Doppler frequency 126.67 Hz. The fading proﬁle is
generated using Jakes’ method [16] and the fading coefﬁcients have a mean value of
1.55 and variance 0.60. During simulation, quasi-static fading is assumed in which
the fading amplitude changes for each code word block. For combating the fading
effect, 64 × 64 and 100 × 512 block interleavers are employed for codes deﬁned in
GF(16) and GF(64) respectively.
In Figure 5.9a the soft-decision list decoding performance of the (64, 39, 20)
Hermitian code over GF(16) is shown on the AWGN channel for different list lengths.
This is compared with the hard-decision list decoding performance with multiplicity
values m = 1 and 2, and the performance of the Sakata algorithm is included too. We
can see that there are small coding gains for soft-decision list decoding over hard-
decision list decoding up to approximately 1.3 dB at a BER of 10−4. In Figure 5.9b,
more signiﬁcant coding gains are achieved on the fading channel, with a coding gain
greater than 3 dB over hard-decision list decoding.
In Figure 5.10a the soft-decision list decoding performance of the (512, 289, 196)
Hermitian code deﬁned over GF(64) is shown on the AWGN channel for different
output list lengths. Again, small coding gains are achieved over hard-decision decod-
ing. In Figure 5.10b the soft-decision list decoding performance on the fading channel
is shown, with more signiﬁcant coding gains over hard-decision decoding.
5.9 Conclusions
In this chapter we have introduced the concept of list decoding for Reed–Solomon
and AG codes. We can see that list decoding allows more errors to be corrected than
with conventional algebraic decoding algorithms, but at a cost of higher complexity.
However, a method to reduce this complexity without degrading the performance is
given for both Reed–Solomon and AG codes. The Kotter–Vardy algorithm for the soft-
decision list decoding of Reed–Solomon codes was presented; this is almost identical
to the Guruswami–Sudan algorithm but with an extra process to convert the reliability
of the received symbols into a multiplicity matrix. The Kotter–Vardy algorithm was
also extended to soft decode AG codes. One interesting observation from the soft-
decision list decoding algorithm was that it could achieve the same performance as the
hard-decision list decoder but at a lower complexity. More signiﬁcant coding gains can
be achieved on a slow fading channel, particularly for AG codes, suggesting that soft-
decision list decoding is very suitable for channels where bursts of errors are common.

200
LIST DECODING
References
[1] Elias, P. (1957) List Decoding for Noisy Channels, Res. Lab. Electron, MIT, Cambridge, MA.
[2] Elias, P. (1991) Error-correcting codes for list decoding. Information Theory, IEEE Transactions,
37, 5–12.
[3] Wozencraft, J.M. (1958) List Decoding, Res. Lab. Electron, MIT, Cambridge, MA.
[4] McEliece, R.J. (2003) The Guruswami–Sudan Decoding Algorithm for Reed–Solomon Codes,
California Institute. Tech, Pasadena, California, IPN Progress Rep, pp. 42–153.
[5] Koetter, R. and Vardy, A. (2003) Algebraic soft-decision decoding of Reed–Solomon codes. IEEE
Trans. Inform. Theory, 49, 2809–25.
[6] Guruswami,
V.
and
Sudan,
M.
(1999)
Improved
decoding
of
Reed–Solomon
and
algebraic–geometric codes. IEEE Trans. Inform. Theory, 45, 1757–67.
[7] Hasse, H. (1936) Theorie der hoheren differentiale in einem algebraishen funcktionenkorper mit
vollkommenem konstantenkorper nei beliebeger charakteristic. J. Reine. Aug. Math, 175, 50–4.
[8] Koetter, R. (1996) On algebraic decoding of algebraic–geometric and cyclic codes. Linkoping,
Sweden: University Linkoping.
[9] Nielsen, R.R. (2001) List decoding of linear block codes. Lyngby, Denmark: Tech. Univ. Denmark.
[10] Moon, T.K. (2005) Error Correction Coding – Mathematical and Algorithms, Wiley Interscience.
[11] Høholdt, T. and Nielsen, R.R. (1999) Decoding Hermitian codes with Sudan’s algorithm, in Applied
Algebra, Algebraic Algorithms and Error-Correcting Codes (eds H.I.N. Fossorier, S. Lin, and A.
Pole), (Lecture Notes in Computer Science), Vol. 1719, Springer-Verlag, Berlin, Germany, pp.
260–70.
[12] Feng, G.-L. and Tzeng, K.K. (1991) A generalization of the Berlekamp-Massey algorithm for
multisequence shift-register synthesis with application to decoding cyclic codes. IEEE Trans.
Inform. Theory, 37, 1274–87.
[13] Chen, L., Carrasco, R.A. and Chester, E.G. (2007) Performance of Reed–Solomon codes using the
Guruswami–Sudan algorithm with improved interpolation efﬁciency. IET Commun, 1, 241–50.
[14] Chen, L. (2007) Design of an efﬁcient list decoding system for Reed–Solomon and
algebraic–geometric codes, PhD Thesis School of electrical, electronic and computer engineer-
ing. Newcastle-upon-Tyne: Newcastle University.
[15] Roth, R. and Ruckenstein, G. (2000) Efﬁcient decoding of Reed–Solomon codes beyond half the
minimum distance. IEEE Trans. Inform. Theory, 46, 246–57.
[16] Proakis, J.G. (2000) Digital Communications, 4th edn, McGraw-Hill International.
[17] Chen, L., Carrasco, R.A. and Johnston, M. (2006) List decoding performance of algebraic geometric
codes. IET Electronic Letters, 42, 986–7.
[18] Chen, L., Carrasco, R.A. and Johnston, M. (XXXX) Reduced complexity interpolation for list
decoding Hermitian codes. IEEE Trans. Wireless Commun, Accepted for publication.
[19] Chen,L. andCarrasco,R.A. (2007) Efﬁcient list decoder for algebraic–geometriccodes. Presented at
9th International Symposium on Communication Theory and Application (ISCTA’07), Ambleside,
Lake district, UK.
[20] Wu, X.-W. and Siegel, P. (2001) Efﬁcient root-ﬁnding algorithm with application to list decoding
of algebraic–geometric codes. IEEE Trans. Inform. Theory, 47, 2579–87.
[21] Chen, L., Carrasco, R.A., Johnston, M. and Chester, E.G. (2007) Efﬁcient factorisation algorithm
for list decoding algebraic–geometric and Reed–Solomon codes. Presented at ICC 2007, Glasgow,
UK.
[22] Wu, X.-W. (2002) An algorithm for ﬁnding the roots of the polynomials over order domains.
Presented at ISIT 2002, Lausanne, Switzerland.
[23] Chen, L., Carrasco, R.A. and Johnston, M. (XXXX) Soft-decision list decoding of Hermitian codes.
IEEE Trans. Commun, Submitted for publication.

6
Non-Binary Low-Density Parity
Check Codes
6.1 Introduction
The previous chapters have described non-binary block codes that have algebraic
decoders, but now a new type of block code is introduced that is decoded using a
graph. This is the well-known low-density parity check (LDPC) code, which was ﬁrst
presented by Gallager in 1962 [1] but only became popular when Mackay rediscovered
it in 1995 [2]. It has been shown that with iterative soft-decision decoding LDPC codes
can perform as well as or even better than turbo codes [3]. It is claimed that LDPC
codes will be chosen in future standards, such as 4G, later versions of WiMax and
magnetic storage devices.
In this chapter, binary LDPC codes are ﬁrst introduced with a discussion on ran-
dom and structured construction methods. The Belief Propagation algorithm is then
presented in detail to decode LDPC codes. We then expand binary LDPC codes to
non-binary LDPC codes deﬁned over a ﬁnite ﬁeld GF(q) and again discuss different
construction methods. Finally the Belief Propagation algorithm is extended to non-
binary symbols, which increases its complexity, but a method to reduce complexity
is introduced using fast Fourier transforms (FFT), allowing us to decode non-binary
LDPC codes deﬁned over large ﬁnite ﬁelds.
6.2 Construction of Binary LDPC Codes – Random
and Structured Methods
A low-density parity check (LDPC) code is characterized by its sparse parity check
matrix, that is a matrix containing mostly zero elements and few nonzero elements.
Three important parameters are its code word length, n, its dimension, k, and its
number of parity bits, m = n −k. The number of nonzero elements in a row of the
Non-Binary Error Control Coding for Wireless Communication and Data Storage
Rolando Antonio Carrasco and Martin Johnston
C 2008 John Wiley & Sons, Ltd

202
NON-BINARY LOW-DENSITY PARITY CHECK CODES
parity check matrix is called the row weight, denoted by ρ. The number of nonzero
elements in a column of the parity check matrix is called the column weight, denoted
by γ . There are two general classes of LDPC code:
 If the row weights and column weights are constant for each row and column in the
parity check matrix then the LDPC code is said to be regular.
 If the row and column weights vary for each row and column in the parity check
matrix then the LDPC code is said to be irregular.
In general, irregular LDPC codes have a better performance than regular LDPC
codes. An example of a parity check matrix H for a small binary LDPC code is given
in (6.1):
H =


1
1
0
1
0
0
0
1
1
0
1
0
1
0
0
0
1
1
0
0
1
1
0
1


(6.1)
We can see that H has a constant row weight ρ = 3 and constant column weight γ =
2, implying that this is a regular LDPC code. Originally, LDPC codes were constructed
by ﬁrst choosing the row and column weights and then randomly determining where
the nonzero elements were located.
6.2.1 Tanner Graph Representation
A parity check matrix can be expressed in the form of a bipartite graph known as a
Tanner graph. Each row in the parity check matrix represents a parity check equation
zi, 1 ≤i ≤m, and each column represents a coded bit cj, 1 ≤j ≤n. For example,
the ﬁrst row of H in (6.1) is the parity check equation z1 = c1 ⊕c2 ⊕c4. The Tanner
graph connects the coded bits to each parity check equation. For each row of the parity
check matrix, if the coded bit is a 1 then there is a connection between that coded bit
and the corresponding parity check equation. The Tanner graph for H in (6.1) is given
in Figure 6.1.
The Tanner graph reveals the presence of cycles, that is a path starting and ending at
the same coded bit. In Figure 6.1 it can be seen that the smallest cycle has a length of
6. One such cycle is c1 →z1→c4 →z4 →c6 →z3 →c1. The length of the smallest
cycle in a Tanner graph is known as its girth. It is desirable to avoid LDPC codes
with Tanner graphs with a girth of 4 as this degrades the performance, particularly for
short LDPC codes. From the Tanner graph, we introduce two sets:
 Mn is the set of indices of the parity checks connected to coded bit cn.
 Nm is the set of indices the coded bits that are connected to parity check zm.

CONSTRUCTION OF BINARY LDPC CODES
203
c1
c2
c3
c4
c5
c6
z1
z2
z3
z4
Figure 6.1
Tanner graph for the parity check matrix of (6.1).
So from the Tanner graph of Figure 6.1, the set M1 = {1, 3}, since parity checks
z1 and z3 are connected to coded bit c1. The set N1 = {1, 2, 4}, since coded bits
c1, c2 and c4 are connected to parity check z1. We also introduce the notation Mn/m,
which is the set Mn excluding the parity check zm that is connected to the coded
bit cn. Similarly, Nm/n is the set Nm excluding the coded bit cn that is connected
to the parity check zm. Therefore, M1/1 = {3} and N1/1 = {2, 4}. This nota-
tion will be useful when describing the Belief Propagation algorithm for decoding
LDPC codes.
6.2.2 Structured Construction Methods
There are many different methods for constructing a binary LDPC code [5–6], but in
this chapter we will discuss just one. This method is known as the Balanced Incomplete
Block Design (BIBD), which can be used to construct high-rate LDPC codes [4]. Let
X = {x1, x2, . . ., xν} be a set of v objects. A BIBD of X is a collection of n γ -subsets
of X, denoted by B1, B2, . . ., Bn, called blocks, such that the following conditions are
satisﬁed:
1. Each object appears in exactly ρ of the n blocks.
2. Every two objects appear together in exactly λ of the n blocks.
3. The number γ of objects in each block is small compared to the total number of
objects in X.
Instead of a list of the blocks, a BIBD can be efﬁciently described by a v × n matrix
Q over GF(2), as follows:
1. The rows of Q correspond to the v objects in X.
2. The columns of Q correspond to the n blocks of the design.

204
NON-BINARY LOW-DENSITY PARITY CHECK CODES
3. Each element Qi,j in Q is a 1 if and only if the ith object xi is contained in the jth
block Bj of the design; otherwise it is 0. This matrix is called the incidence matrix
of the design.
It follows from the structural properties of a BIBD that the incidence matrix Q has
constant row weight ρ and constant column weight γ and any two rows of Q have
exactly λ ‘1-components’ in common.
Example 6.1: Construction of an incidence matrix using BIBD: Let X = {x1,
x2, x3, x4, x5, x6, x7} be a set of seven objects. The blocks {x1, x2, x4}, {x2, x3,
x5}, {x3, x4, x6}, {x4, x5, x7}, {x5, x6, x1}, {x6, x7, x2},{x7, x1, x3} form a BIBD
for the set X. Every block consists of three objects, each object appears in three
blocks, and every two objects appear together in exactly one block, that is λ = 1.
The incidence matrix Q is given below:
Q =


1
0
0
0
1
0
1
1
1
0
0
0
1
0
0
1
1
0
0
0
1
1
0
1
1
0
0
0
0
1
0
1
0
0
0
0
1
0
1
1
0
0
0
0
1
0
1
1


.
Note that each row of Q is a cyclic right-shift of the row above it and the ﬁrst
row is the cyclic shift of the last row. We also note that each column is a downward
cyclic shift of the column on its left and the ﬁrst column is the downward cyclic
shift of the last column. Therefore Q is a 7 × 7 square circulant matrix. Therefore,
the null space of Q gives a (γ , ρ)-regular LDPC code, called a BIBD-LDPC code,
of length n. The Tanner graph of a BIBD-LDPC code is free of cycles of length 4
and hence its girth is at least 6. In fact, the second property of a BIBD with λ =
1 ensures that the girth of the code’s Tanner graph is exactly 6. If λ = 2 then the
incidence matrix will have a Tanner graph full of short cycles of length 4, which is
undesirable.
6.2.3 LDPC Codes from Class 1 Bose BIBD
There are four classes of Bose BIBD, each with different types producing many differ-
ent binary LDPC codes. However, in this book we only deal with LDPC constructed
from Class 1 Bose BIBDs of type 1 [4]. Choose a ﬁnite ﬁeld GF(2t + 1), where t is
selected so that GF(2t + 1) is prime, with a primate element x that satisﬁes x4t −1 =
xc, where c is an odd integer less than 2t + 1. The resulting LDPC code parameters

CONSTRUCTION OF BINARY LDPC CODES
205
will have the following values: n = t(12t + 1), v = 12t + 1, γ = 4, ρ = 4t, λ = 1.
The base blocks of this BIBD are:

0, x2i, x2i+4t, x2i+8t
,
(6.2)
for 0 ≤i < t. From each base block Bi, we can form 12t + 1 blocks by adding each
element of the ﬁeld GF(12t + 1) in turn to the elements in Bi. This results in t(12t +
1) blocks. The incidence matrix Q of this BIBD is a (12t +1) × t(12t +1) matrix.
It can be written in cyclic form, which consists of t (12t + 1) × (12t + 1) circulant
submatrices in a row, as shown below:
Q =
Q1 Q2 · · · Qt

,
(6.3)
where the ith circulant Qi is the incidence matrix formed by adding each element in
GF(12t + 1) in turn to the elements of the ith base block Bi. A circulant is a square
matrix which has the following structure:
1. Each row is a right-cyclic shift of the row above it and the ﬁrst row is the right-cyclic
shift of the last row.
2. Each column is a downward cyclic shift of the column on its left and the ﬁrst
column is the downward cyclic shift of the last column. The set of columns is the
same as the set of rows, except that each row reads from right to left.
Both the row and the column weight of each circulant Qi are 4. Therefore, the row
and column weights of Q are 4t and 4 respectively. The rank of Q (or a circulant Qi)
is observed to be 12t.
Example 6.2: Construction of an LDPC code using a class 1 Bose BIBD: Let
t = 1. Then 12t + 1 = 13. The element 6 of GF(13) = {0, 1, 2, . . ., 12} is a
primitive element so with c = 3 we can easily check that x4 −1 = 64 −1 = 9 −1
= 8 = 63, which is less than 13. Therefore, there is a BIBD with parameters, v =
13, n = 13, γ = 4, ρ = 4 and λ = 1. The base block for this design is {0, 60, 64,
68} = {0, 1, 9, 3}. By adding each element in GF(13) to the base block we get the
following 13 blocks:
B0 = {0, 1, 9, 3}, B1 = {1, 2, 10, 4}, B2 = {2, 3, 11, 5}, B3 = {3, 4, 12, 6}, B4
= {4, 5, 0, 7},
B5 = {5, 6, 1, 8}, B6 = {6, 7, 2, 9}, B7 = {7, 9, 3, 10}, B8 = {8, 9, 4, 11}, B9
= {9, 10, 5, 12},
B10 = {10, 11, 6, 0}, B11 = {11, 12, 7, 1}, B12 = {12, 0, 8, 2}.

206
NON-BINARY LOW-DENSITY PARITY CHECK CODES
Hence, the circulant matrix Q is:
Q =


1
0
0
0
1
0
0
0
0
0
1
0
1
1
1
0
0
0
1
0
0
0
0
0
1
0
0
1
1
0
0
0
1
0
0
0
0
0
1
1
0
1
1
0
0
0
1
0
0
0
0
0
0
1
0
1
1
0
0
0
1
0
0
0
0
0
0
1
0
1
1
0
0
0
1
0
0
0
0
0
0
1
0
1
1
0
0
0
1
0
0
0
0
0
0
1
0
1
1
0
0
0
1
0
0
0
0
0
0
1
0
1
1
0
0
0
1
1
0
0
0
0
0
1
0
1
1
0
0
0
0
1
0
0
0
0
0
1
0
1
1
0
0
0
0
1
0
0
0
0
0
1
0
1
1
0
0
0
0
1
0
0
0
0
0
1
0
1
1


.
This example only produces a single circulant matrix, but the next value of t
that ensures the ﬁnite ﬁeld is prime is t = 6, giving GF(73). This will result in
an LDPC code with parameters v = 73, n = 438, γ = 4, ρ = 292 and λ = 1.
In this case, there will be six base blocks each containing four elements, and six
circulant matrices can be formed by adding every element in GF(73) to each of
the base blocks. Each circulant matrix will have dimensions 73 × 73, with γ =
4, ρ = 4. The six circulant matrices are then concatenated to form the ﬁnal parity
check matrix, H, with dimensions 73 × 438. The null space of this matrix will be
the (438, 365) LDPC code, assuming each row in H is independent.
6.2.4 Constructing the Generator Matrix of a Binary LDPC Code
The encoding of a binary message to form an LDPC code word is achieved by
multiplying the binary message vector by a systematic generator matrix, G. The
systematic generator matrix is obtained by ﬁrst performing Gauss–Jordan elimination
on the parity check matrix so that it is of the form H = [Im|P], where Im is the
m × m identity matrix and P is a parity matrix. The generator matrix is then G =
[PT|Ik], where PT is the matrix transpose of P, and Ik is the k × k identity matrix.
Gauss–Jordan elimination is related to Gaussian elimination in that it eliminates all
elements below and also above each pivot element. Gaussian elimination results in
a matrix in row echelon form, whereas Gauss–Jordan elimination results in a matrix
in reduced row echelon form. It can therefore be seen as a two-stage process, where
Gaussian elimination is ﬁrst performed to transform the matrix into row echelon form,
followed by elimination of elements above the pivot elements.

CONSTRUCTION OF BINARY LDPC CODES
207
The elimination procedure on a matrix containing ﬁnite ﬁeld elements is performed
in the same way as if the matrix contained real numbers. The only difference is that
elements are eliminated with modulo-2 addition instead of subtraction.
It is not unusual for a parity check matrix to have dependent rows. When performing
Gaussian elimination on the matrix, all dependent rows will appear as rows containing
all zeroes located at the bottom of the new matrix. This is the case if we apply Gaussian
elimination to the parity check matrix of (6.1):
H =


1
1
0
1
0
0
0
1
1
0
1
0
1
0
0
0
1
1
0
0
1
1
0
1

→H


1
0
0
0
1
1
0
1
1
0
1
0
0
0
1
1
0
1
0
0
0
0
0
0

.
We can see that row 4 in H is the modulo-2 sum of row 1, row 2 and row 3 and
hence it is a dependent row. Therefore, performing Gaussian elimination results in an
all-zero row in the new matrix, H. However, a generator matrix can still be obtained
by removing the all-zero rows and then eliminating elements above the pivot elements
in the resulting submatrix:
H =


1
0
0
0
1
1
0
1
1
0
1
0
0
0
1
1
0
1

→H =


1
0
0
0
1
1
0
1
0
1
1
1
0
0
1
1
0
1

.
The systematic parity check matrix H is simply obtained by adding row 3 to row
2 in H. Finally, the generator matrix is:
G =


0
1
1
1
0
0
1
1
0
0
1
0
1
1
1
0
0
1

.
We can see that dependent rows in the parity check matrix will increase the code
rate of the LDPC code. If all the rows were independent then the null space of the
parity check matrix of (6.1) would be the (6, 2) LDPC code with code rate of 1/3.
However, the dependent row means the null space is the (6, 3) LDPC code with code
rate of 1/2.
For larger parity check matrices, the swapping of columns is usually required during
Gaussian elimination to ensure that the pivot elements are on the main diagonal of
the matrix. However, this will obviously change the parity check equations and the
resulting systematic generator matrix will not be the null space of the original parity
check matrix. In this case, any column that is swapped must also be swapped on the
original parity check matrix, but this will not alter its column and row weights.

208
NON-BINARY LOW-DENSITY PARITY CHECK CODES
6.3 Decoding of Binary LDPC Codes Using the Belief
Propagation Algorithm
The decoding of LDPC codes using the sum product algorithm involves ﬁnding a code
word of length n where each coded bit cn maximizes the probability of cn conditioned
on the parity check equations associated with cn being satisﬁed [7].
P(cn|{zm = 0, m ∈Mn}).
(6.4)
The sum product algorithm involves calculating two probabilities. The ﬁrst is
the probability qmn, which is the probability of the nth code bit conditioned on
its associated parity checks being satisﬁed with the exception of the mth parity
check:
qmn(x) = P(cn = x

zm = 0, m ∈Mn/m

.
(6.5)
The second probability is denoted rmn, which is the probability that the mth parity
check is satisﬁed conditioned on all the possible values of the coded bits c:
rmn(x) = P(zm = 0|c).
(6.6)
Figure 6.2 shows how these probabilities are exchanged in the Tanner graph of
Figure 6.1 for the connections between c1 and z1, and c6 and z4.
6.3.1 Initialization of the Belief Propagation Algorithm
The probabilities qmn(x) are initialized to the probability f (x)
n
that the nth received bit
is x, that is P(cn = x). For the additive white Gaussian noise (AWGN) channel the
probability g(x)
n
is [7]:
g(0)
n
∝
1
σ
√
2π
e−(rn−1)2/2σ 2
and
g(1)
n
∝
1
σ
√
2π
e−(rn+1)2/2σ 2.
(6.7)
c1
c2
c3
c4
c5
c6
z1
z2
z3
z4
q64
r64
r 11
q11
Figure 6.2
Tanner graph showing the exchange of information between the coded bits and their parity
checks.

DECODING OF BINARY LDPC CODES
209
1
Q
I
00
01
11
10






2
1
,
2
1






−
2
1
,
2
1






−
−
2
1
,
2
1





−
2
1
,
2
1
Figure 6.3
QPSK constellation.
In general, when the modulation scheme has an in-phase and quadrature component
the probabilities are determined by calculating the Euclidean distances for the in-phase
component from each constellation point. The coordinates of each constellation point
in the QPSK constellation are shown in Figure 6.3.
Therefore, the four probabilities g(00)
n
, g(01)
n
, g(11)
n
and g(10)
n
for the QPSK constella-
tion are:
g(00)
n
∝
1
σ
√
2π
e
−

r(I)
n −1
√
2
2
+

r(Q)
n
−1
√
2
2
2σ 2
,
g(01)
n
∝
1
σ
√
2π
e
−

r(I)
n −1
√
2
2
+

r(Q)
n
+ 1
√
2
2
2σ 2
g(11)
n
∝
1
σ
√
2π
e
−

r(I)
n + 1
√
2
2
+

r(Q)
n
+ 1
√
2
2
2σ 2
,
g(10)
n
∝
1
σ
√
2π
e
−

r(I)
n + 1
√
2
2
+

r(Q)
n
−1
√
2
2
2σ 2
,
(6.8)
where r(I)
n
and r(Q)
n
are the in-phase and quadrature components of the received
symbol. The likelihoods of the received bits are then placed in a vector f:
f =

g(0)
1
g(0)
2
· · ·
g(0)
n−1
g(0)
n
g(1)
1
g(1)
2
· · ·
g(1)
n−1
g(1)
n

.
(6.9)

210
NON-BINARY LOW-DENSITY PARITY CHECK CODES
To initialize the sum–product algorithm, the likelihoods in f are used to initialize a
matrix Q deﬁned as:
Q =


g(0)
1
g(0)
2
g(0)
3
· · ·
g(0)
n−1
g(0)
n
g(1)
1
g(1)
2
g(1)
3
· · ·
g(1)
n−1
g(1)
n
...
...
...
...
...
...
...
...
...
...
...
...
g(0)
1
g(0)
2
g(0)
3
· · ·
g(0)
n−1
g(0)
n
g(1)
1
g(1)
2
g(1)
3
· · ·
g(1)
n−1
g(1)
n


.
(6.10)
The matrix Q has partitions containing the likelihoods that the ith received bit is a
0, g(0)
i
or a 1, g(1)
i .
6.3.2 The Horizontal Step
The horizontal step of the sum–product algorithm determines the probability rmn,
deﬁned as [7]:
rmn(x) =

c:cn=x
P(zm = 0|c)P(c|cn = x),
(6.11)
where, given that the nth coded bit cn = x, c is a binary vector of the remaining coded
bits of length ρ −1, containing every possible combination of binary sequences.
There are therefore 2ρ−1 binary sequences that (6.9) is summed over. However, not
all these sequences will satisfy zm = 0 so we only consider those sequences that
do satisfy the parity check. For example, if x = 0 then only those binary sequences
with an even number of 1s are able to satisfy zm = 0. Therefore, the probability of a
sequence containing an odd number of 1s satisfying zm = 0 is zero. Therefore, rmn(x)
is the sum of all the probabilities of the binary sequences of length ρ −1 that when
summed together equal x:
rmn(x) =

c:cn=x
P(zm = 0 |c)

n∈Nm\n
qmn(x),
(6.12)

DECODING OF BINARY LDPC CODES
211
where P(zm = 0 | c) is either zero or one. The probabilities rmn(x) are placed in a matrix
R:
R =


r11(0)
r12(0)
r13(0)
· · ·
r1(n−1)(0)
r1n(0)
r11(1)
r12(1)
r13(1)
· · ·
r1(n−1)(1)
r1n(1)
...
...
...
...
...
...
...
...
...
...
...
...
rm1(0)
rm2(0)
rm3(0)
· · ·
rm(n−1)(0)
rmn(0)
rm1(1)
rm2(1)
rm3(1)
· · ·
rm(n−1)(1)
rmn(1)


,
(6.13)
where each element in the parity check matrix has a corresponding 1 × 2 column
vector containing the probabilities rmn(0) and rmn(1) (or a 1 × q column vector for an
LDPC code deﬁned over GF(q)).
6.3.3 The Vertical Step
The vertical step updates the probabilities qmn and is much simpler than the horizontal
step. From (6.5), qmn can be written using Baye’s Rule as [7]:
qmn(x) = P(cn = x

zm = 0, m ∈Mn/m

= P(cn = x)P

zm = 0, m ∈Mn/m
 |cn = x

P ({zm = 0, m ∈Mn/m})
.
(6.14)
Using (6.6) and letting f x
n = P(cn = x), (6.10) can be written as [7]:
qmn(x) = βmn f x
n

m∈Mn/m
rmn(x),
(6.15)
where βmn is a normalizing constant such that  qmn(x) = 1, that is:
βmn =
1

x
f xn

m∈Mn\m
rmn(x).
(6.16)

212
NON-BINARY LOW-DENSITY PARITY CHECK CODES
The qmn(x) are placed in the matrix Q as shown in (6.17):
Q =


q11(0)
q12(0)
q13(0)
· · ·
q1(n−1)(0)
q1n(0)
q11(1)
q12(1)
q13(1)
· · ·
q1(n−1)(1)
q1n(1)
...
...
...
...
...
...
...
...
...
...
...
...
qm1(0)
qm2(0)
qm3(0)
· · ·
qm(n−1)(0)
qmn(0)
qm1(1)
qm2(1)
qm3(1)
· · ·
qm(n−1)(1)
qmn(1)


,
(6.17)
where each element in the parity check matrix has a corresponding 1 × 2 column
vector containing the probabilities qmn(0) and qmn(1) (or a 1 × q column vector for an
LDPC code deﬁned over GF(q)).
In this step, the pseudo posterior probabilities qn(x) are also determined by [7]:
qn(x) = βn f x
n

m∈Mn
rmn(x),
(6.18)
where again βn is a normalizing constant such that  qn(x) = 1. The pseudo posterior
probabilities are place in a matrix Q:
Q =
q1(0)
q2(0)
q3(0)
· · ·
qn−1(0)
qn(0)
q1(1)
q2(1)
q3(1)
· · ·
qn−1(1)
qn(1)

.
(6.19)
From these pseudo posterior probabilities estimates of the transmitted code word
can be found by:
ˆcn = arg max βn f x
n
x

m∈Nm
rmn(x).
(6.20)
Example 6.3: Decoding a binary LDPC code with the Belief Propagation
algorithm: In this example, we use the regular (10, 5) binary LDPC code with row
weight ρ = 6 and column weight γ = 3. The parity check matrix H is given as:
H =


1
1
1
0
0
1
1
0
0
1
1
0
1
0
1
1
0
1
1
0
0
0
1
1
1
0
1
0
1
1
0
1
0
1
1
1
0
1
0
1
1
1
0
1
0
0
1
1
1
0


.
Let us assume that the transmitted code word is c = [0
0
0
1
0
1
0
1
0
1] and the received word is r = [0
0
0
1
1
1
0
1
0
1],

DECODING OF BINARY LDPC CODES
213
with a single error in the ﬁfth position. The associated likelihoods are:
f =

0.78
0.84
0.81
0.52
0.45
0.13
0.82
0.21
0.75
0.24
0.22
0.16
0.19
0.48
0.55
0.87
0.18
0.79
0.25
0.76

.
We initialize the matrix Q with these likelihoods as follows:
Q =


0.78
0.84
0.81
0
0
0.13
0.82
0
0
0.24
0.22
0.16
0.19
0
0
0.87
0.18
0
0
0.76
0.78
0
0.81
0
0.45
0.13
0
0.21
0.75
0
0.22
0
0.19
0
0.55
0.87
0
0.79
0.25
0
0
0
0.81
0.52
0.45
0
0.82
0
0.75
0.24
0
0
0.19
0.48
0.55
0
0.18
0
0.25
0.76
0
0.84
0
0.52
0.45
0.13
0
0.21
0
0.24
0
0.16
0
0.48
0.55
0.87
0
0.79
0
0.76
0.78
0.84
0
0.52
0
0
0.82
0.21
0.75
0
0.22
0.16
0
0.48
0
0
0.18
0.79
0.25
0


.
The matrix Q containing all the probabilities qmn(x) is then used for the horizontal
step to determine the matrix R containing the probabilities rmn(x). To determine
the probability r11(1) we must calculate the probabilities of all the possible binary
sequences that satisfy z1 when c1 = 1. Hence, we must ﬁrst ﬁnd all possible binary
values of c2, c3, c6, c7 and c10 that satisfy:
c2 + c3 + c6 + c7 + c10 = c1 = 1.
Since we are only working with binary values, the above equation can only be
satisﬁed when an odd number of the coded bits are equal to 1. The 16 possible bit
sequences and their probabilities are given in Table 6.1.
r11(1) =

q12(1)q13(0)q16(0)q17(0)q1,10(0)

+

q12(0)q13(1)q16(0)q17(0)q1,10(0)

+
+

q12(0)q13(0)q16(1)q17(0)q1,10(0)

+

q12(0)q13(0)q16(0)q17(1)q1,10(0)

+

q12(0)q13(0)q16(0)q17(0)q1,10(1)

+

q12(1)q13(1)q16(1)q17(0)q1,10(0)

+

q12(0)q13(1)q16(1)q17(1)q1,10(0)

+

q12(0)q13(0)q16(1)q17(1)q1,10(1)

+

q12(1)q13(1)q16(0)q17(0)q1,10(1)

+

q12(1)q13(1)q16(0)q17(1)q1,10(0)

+

q12(0)q13(1)q16(1)q17(0)q1,10(1)

+

q12(1)q13(0)q16(1)q17(0)q1,10(1)

+

q12(1)q13(0)q16(0)q17(1)q1,10(1)

+

q12(0)q13(1)q16(0)q17(1)q1,10(1)

+

q12(1)q13(0)q16(1)q17(1)q1,10(0)

+

q12(1)q13(1)q16(1)q17(1)q1,10(1)

= 0.00316 + 0.004038 + 0.116495 + 0.003821 + 0.055123 + 0.005205
+ 0.005998 + 0.080978 + 0.002463 + 0.000171 + 0.086533 + 0.070267
+ 0.002305 + 0.002838 + 0.004871 + 0.0036618r11(1) = 0.448086.

214
NON-BINARY LOW-DENSITY PARITY CHECK CODES
Table 6.1
Bit sequences and their associated probabilities of determining r11(1).
Bit sequence
Probability
10 000
q12(1) q13(0) q16(0) q17(0) q1,10(0) = 0.16 × 0.81 × 0.13 × 0.82 × 0.24 = 0.003316
01 000
q12(0) q13(1) q16(0) q17(0) q1,10(0) = 0.84 × 0.19 × 0.13 × 0.82 × 0.24 = 0.004083
00 100
q12(0) q13(0) q16(1) q17(0) q1,10(0) = 0.84 × 0.81 × 0.87 × 0.82 × 0.24 = 0.116495
00 010
q12(0) q13(0) q16(0) q17(1) q1,10(0) = 0.84 × 0.81 × 0.13 × 0.18 × 0.24 = 0.003821
00 001
q12(0) q13(0) q16(0) q17(0) q1,10(1) = 0.84 × 0.81 × 0.13 × 0.82 × 0.76 = 0.055123
11 100
q12(1) q13(1) q16(1) q17(0) q1,10(0) = 0.16 × 0.19 × 0.87 × 0.82 × 0.24 = 0.005205
01 110
q12(0) q13(1) q16(1) q17(1) q1,10(0) = 0.84 × 0.19 × 0.87 × 0.18 × 0.24 = 0.005998
00 111
q12(0) q13(0) q16(1) q17(1) q1,10(1) = 0.84 × 0.81 × 0.87 × 0.18 × 0.76 = 0.080978
11 001
q12(1) q13(1) q16(0) q17(0) q1,10(1) = 0.16 × 0.19 × 0.13 × 0.82 × 0.76 = 0.002463
11 010
q12(1) q13(1) q16(0) q17(1) q1,10(0) = 0.16 × 0.19 × 0.13 × 0.18 × 0.24 = 0.000171
01 101
q12(0) q13(1) q16(1) q17(0) q1,10(1) = 0.84 × 0.19 × 0.87 × 0.82 × 0.76 = 0.086533
10 101
q12(1) q13(0) q16(1) q17(0) q1,10(1) = 0.16 × 0.81 × 0.87 × 0.82 × 0.76 = 0.070267
10 011
q12(1) q13(0) q16(0) q17(1) q1,10(1) = 0.16 × 0.81 × 0.13 × 0.18 × 0.76 = 0.002305
01 011
q12(0) q13(1) q16(0) q17(1) q1,10(1) = 0.84 × 0.19 × 0.13 × 0.18 × 0.76 = 0.002838
10 110
q12(1) q13(0) q16(1) q17(1) q1,10(0) = 0.16 × 0.81 × 0.87 × 0.18 × 0.24 = 0.004871
11 111
q12(1) q13(1) q16(1) q17(1) q1,10(1) = 0.16 × 0.19 × 0.87 × 0.18 × 0.76 = 0.003618
By adding these probabilities we get r11(1) = 0.448086 and therefore r11(0) =
1 −r11(1) = 0.551914. The complete matrix R is then:
R =


0.551914
0.542753
0.546890
0
0
0.460714
0.545425
0
0
0.444092
0.448086
0.457247
0.453110
0
0
0.539286
0.454575
0
0
0.555908
0.493347
0
0.493991
0
0.537255
0.505034
0
0.506423
0.493347
0
0506653
0
0.506009
0
0.462745
0.494966
0
0.493577
0.507451
0
0
0
0.500333
0.505158
0.497937
0
0.500322
0
0.500413
0.499603
0
0
0.499667
0.494842
0.502063
0
0.499678
0
0.499587
0.500397
0
0.500446
0
0.507588
0.496965
0.499590
0
0.499477
0
0.499416
0
0.499554
0
0.492412
0.503035
0.500410
0
0.500523
0
0.500584
0.497476
0.497921
0
0.464662
0
0
0.497791
0.502437
0.497173
0
0.502524
0.502079
0
0.535338
0
0
0.502209
0.497563
0.502827
0


.
The vertical step then updates the matrix Q using (6.15). To determine the
probability q11(x):
q11(0) = β11 f 0
1

m∈M1\1
rm1(0)
= β11 × 0.78 × (0.493347 × 0.497476) = 0.191434β11
q11(1) = β11 f 1
1

m∈M1\1
rm1(1)
= β11 × 0.22 × (0.506653 × 0.502524) = 0.056013β11
.

DECODING OF BINARY LDPC CODES
215
Since 0.191334β11 + 0.05601β11 = 1, β11 =
1
0.191334+0.056013 = 4.042903.
So:
q11(0) = 4.042903 × 0.78 × (0.493347 × 0.497476) = 0.773636
q11(1) = 4.042903 × 0.22 × (0.506653 × 0.502524) = 0.226364.
The remaining qmn are shown in Q:
Q =


0.773636
0.839121
0.806481
0
0
0.132106
0.818884
0
0
0.239285
0.226364
0.160879
0.193519
0
0
0.867894
0.181116
0
0
0.760715
0.812140
0
0.837461
0
0.444958
0.113039
0
0.211273
0.748185
0
0.187860
0
0.162539
0
0.555042
0.886961
0
0.788727
0.251815
0
0
0
0.833978
0.492203
0.484126
0
0.844187
0
0.742212
0.201076
0
0
0.166022
0.507797
0.515874
0
0.155813
0
0.257788
0.798924
0
0.860727
0
0.489773
0.485097
0.115241
0
0.215940
0
0.201196
0
0.139273
0
0.510227
0.514903
0.884759
0
0.784060
0
0.798804
0.809608
0.861934
0
0.532711
0
0
0.845514
0.213942
0.744684
0
0.190392
0.138066
0
0.467289
0
0
0.154486
0.786058
0.255316
0


.
Finally the pseudo posterior probabilities qn are determined using (6.18). To ﬁnd
q1(x):
q1(0) = β1 f 0
1

m∈M1
rm1(0)
= β1 × 0.78 × (0.551914 × 0.493347 × 0.500413) = 0.116898β1
q1(1) = β1 f 1
1

m∈M1
rm1(1)
= β1 × 0.22 × (0.448086 × 0.506653 × 0.502524) = 0.025099β1
∴β1 =
1
0.116898 + 0.025099 = 7.042402
q1(0) = 7.042402 × 0.78 × (0.551914 × 0.493347 × 0.500413) = 0.808046
q1(1) = 7.042402 × 0.22 × (0.448086 × 0.506653 × 0.502524) = 0.191954
.
Applying (6.19), the decoded symbol ˆc1 = 0 since q1(0) has the higher proba-
bility. The pseudo posterior probabilities of the received word are:
Q =
 0.808046 0.860941 0.834162 0.497361 0.482065 0.115074 0.844356 0.215586 0.742528 0.200821
0.191954 0.139059 0.165838 0.502639 0.517935 0.884926 0.155644 0.784414 0.257472 0.799179

and the corresponding decoded code word is ˆc = [0
0
0
1
1
1
0
1
0
1]. We can see that this does not match the original transmitted code word
and so further iterations are required. It turns out that the decoded code word is

216
NON-BINARY LOW-DENSITY PARITY CHECK CODES
correct after the third iteration and the ﬁnal matrices R, Q and Q are given along
with the correct decoded code word:
R =


0.549960
0.540086
0.544369
0
0
0.463092
0.542650
0
0
0.447890
0.450040
0.459914
0.455631
0
0
0.536908
0.457350
0
0
0.552110
0.493114
0
0.493650
0
0.545393
0.505532
0
0.507453
0.491301
0
0.506886
0
0.506350
0
0.454607
0.494468
0
0.492547
0.508699
0
0
0
0.499989
0.500176
0.502649
0
0.499989
0
0.499985
0.500012
0
0
0.500011
0.499824
0.497351
0
0.500011
0
0.500015
0.499988
0
0.499975
0
0.500415
0.503915
0.500023
0
0.500032
0
0.500030
0
0.500025
0
0.499585
0.496085
0.499977
0
0.499968
0
0.499970
0.496595
0.497094
0
0.457904
0
0
0.496955
0.503693
0.495668
0
0.503405
0.502906
0
0.542096
0
0
0.503045
0.496307
0.504332
0


Q =


0.772854
0.838418
0.806053
0
0
0.132534
0.818189
0
0
0.240031
0.227146
0.161582
0.193947
0
0
0.867466
0.181811
0
0
0.759969
0.810391
0
0.835883
0
0.456507
0.114177
0
0.212482
0.746725
0
0.189609
0
0.164117
0
0.543493
0.885823
0
0.787518
0.253275
0
0
0
0.832375
0.478244
0.499266
0
0.842265
0
0.740099
0.203955
0
0
0.167625
0.521756
0.500734
0
0.157735
0
0.259901
0.796045
0
0.859034
0
0.478005
0.498000
0.116425
0
0.217493
0
0.203943
0
140966
0
0.521995
0.502000
0.883575
0
0.782507
0
0.796057
0.808242
0.860424
0
0.520590
0
0
0.843871
0.215010
0.743407
0
0.191758
0.139576
0
0.479410
0
0
0.156129
0.784990
0.256593
0


Q =
 0.806122 0.859023 0.832369 0.478419 0.501915 0.116434 0.842260 0.217514 0.740088 0.203963
0.193878 0.140977 0.167631 0.521581 0.498085 0.883566 0.157740 0.782486 0.259913 0.796037

ˆc = [0
0
0
1
0
1
0
1
0
1].
The majority of the complexity of the sum–product algorithm is due to the
horizontal step, since the number of possible non-binary sequences can become
very large. However, it has been shown that fast Fourier transforms (FFTs) can be
used to replace this step, resulting in a signiﬁcant reduction in complexity [8].
6.3.4 Reducing the Decoding Complexity Using Fast Fourier Transforms
The horizontal step described previously involves ﬁnding all possible binary sequences
that satisfy a parity check equation, determining the probability of each sequence and
adding them all together, as deﬁned in (6.10).

DECODING OF BINARY LDPC CODES
217
If we take the simple parity check equation z1 = c1 ⊕c2 ⊕c3 = 0 then to determine
r11(x) we ﬁrst need to ﬁnd all the solutions of c2 ⊕c3 = c1 = x. For x = 0 the solutions
are c2 = 0, c3 = 0 and c2 = 1, c3 = 1. Therefore:
r11(0) = q12(0)q13(0) + q12(1)q13(1).
For x = 1 the solutions are c2 = 0, c3 = 1 and c2 = 1, c3 = 0. Therefore:
r11(1) = q12(0)q13(1) + q12(1)q13(0).
In general, we can write this as the convolution operation:
r11(x) =
1

v=0
q12(v)q13(x −v),
where v ∈GF(2).
This implies that the same result can be achieved by replacing the convolution
operation with Fourier transforms. Therefore, to determine rmn we must ﬁrst calculate
the product of the Fourier transforms of the other qmn and then apply the inverse
Fourier transform:
rmn(x) = F−1


n∈Nm/n
F (qmn(x))

,
(6.21)
where F( ) is the Fourier transform and F−1 is the inverse Fourier transform. Since all
elements belong to the additive group Z2, the Fast Fourier transform reduces to the
Hadamard transform [9] W2, where:
W2 =
1
√
2
1
1
1
−1

.
(6.22)
A property of the Hadamard matrix is that its inverse is also the Hadamard matrix,
that is:
W2W2 = 1
2

1
1
1
−1
 
1
1
1
−1

= 1
2

2
0
0
2

= I,
where I is the identity matrix. Consequently, the inverse Fourier transform can be
obtained by also multiplying by the Hadamard matrix. We will now use FFTs to
determine r11(0) and r11(1) for the ﬁrst iteration of Example 3.2. From (6.20):
r11(x) = F−1


n∈N0\0
F (q1n(x))


= F−1
 1
√
2
1
1
1
−1
 q13(0)
q13(1)

× 1
√
2
1
1
1
−1
 q14(0)
q14(1)


218
NON-BINARY LOW-DENSITY PARITY CHECK CODES
×
1
√
2
1
1
1
−1
 q16(0)
q16(1)

× 1
√
2
1
1
1
−1
 q17(0)
q17(1)

×
1
√
2
1
1
1
−1
 q1,10(0)
q1,10(1)
 
= F−1
 1
√
2
1
1
1
−1
 0.84
0.16

× 1
√
2
1
1
1
−1
 0.81
0.19

×
1
√
2
1
1
1
−1
 0.13
0.87

× 1
√
2
1
1
1
−1
 0.82
0.18

×
1
√
2
1
1
1
−1
 0.24
0.76
 
= F−1
 1
√
2
 1
0.68

× 1
√
2
 1
0.62

× 1
√
2

1
−0.74

× 1
√
2
 1
0.64

× 1
√
2

1
−0.52
 
.
= 1
2
1
1
1
−1
 
1
0.103828

=
0.551914
0.448086

Therefore, r11(0) = 0.551914 and r11(1) = 0.448086, which is identical to the result
obtained in the previous example. It is quite obvious by comparing these two methods
that FFTs are much simpler to perform. FFTs will also be applied to the decoding of
non-binary LDPC codes, explained later in this chapter.
6.4 Construction of Non-Binary LDPC Codes Deﬁned Over Finite Fields
A non-binary LDPC code is simply an LDPC code with a sparse parity check matrix
containing elements that could be deﬁned over groups, rings or ﬁelds. In this book,
we concentrate solely on LDPC codes deﬁned over ﬁnite ﬁelds GF(2i), where i is a
positive integer greater than 1. In 1998, Mackay presented the idea of LDPC codes
over ﬁnite ﬁelds [10], proving that they can achieve increases in performance over
their binary counterparts with increasing ﬁnite ﬁeld size. Mackay also showed how
the sum–product algorithm could be extended to decode non-binary LDPC codes, but
the overall complexity was much higher. At that time it was therefore only feasible to
decode non-binary LDPC codes over small ﬁnite ﬁelds.
There are only a few papers in the literature on non-binary LDPC codes and most
of these codes are constructed by taking a known binary LDPC code and replacing
its nonzero elements with randomly-generated ﬁnite ﬁeld elements. Shu Lin has
presented several structured methods to construct good non-binary LDPC codes using
a technique known as array dispersion [11], one of which we now describe.

CONSTRUCTION OF NON-BINARY LDPC CODES DEFINED OVER FINITE FIELDS
219
6.4.1 Construction of Non-Binary LDPC Codes from Reed–Solomon Codes
Shu Lin has demonstrated several array dispersion methods using tools such as Eu-
clidean and ﬁnite geometries and also using a single code word from a very low-rate
Reed–Solomon code [11]. Since this book is concerned with non-binary codes, this
section explains how to construct non-binary LPDC codes from a Reed–Solomon
code word.
In the context of a non-binary code deﬁned over ﬁnite ﬁelds GF(q), array dispersion
is an operation applied to each nonzero element in a matrix whereby each element is
transformed to a location vector of length q −1. For an element αi ∈GF(q), 0 ≤i ≤
q −2, it will be placed in the ith index of the location vector. For example, the element
α5 in GF(8) would be placed in the ﬁfth index of the location vector. The element in
this location vector is used to build an array with each row deﬁned as the previous
row cyclically shifted to the right and multiplied by the primitive element in GF(q),
resulting in a (q −1) × (q – 1) array. Performing array dispersion on a matrix with
dimensions a × b would result in a larger matrix with dimensions a(q −1) × b(q −
1). Therefore, after array dispersion the element α5 would become:


0
0
0
0
0
α5
0
0
0
0
0
0
0
α6
1
0
0
0
0
0
0
0
α
0
0
0
0
0
0
0
α2
0
0
0
0
0
0
0
α3
0
0
0
0
0
0
0
α4
0
0


where the top row is the original location vector. For the zero element, array dispersion
will result in a (q −1) × (q – 1) zero matrix.
These non-binary LDPC codes are constructed from Reed–Solomon codes with
message length k = 2. Since these code are maximum distance separable (MDS) their
minimum Hamming distance is d = n −k + 1 = n −1. Since a Reed–Solomon code
deﬁned over GF(q) has a block length of n = q −1, the Reed–Solomon code will be a
(q −1, 2, q −2) Reed–Solomon code. Since d = q −2, this means that the minimum
weight of the code word is also q −2, implying that the code word will contain q −
2 nonzero elements and a single zero. It can be shown that the vector containing all
1s, [1, 1, 1, . . ., 1], and the vector [1, α, α2, . . ., αq−2] are both valid code words. As
a Reed–Solomon code is linear, adding these two code words will result in another
code word:
!1 1 1 · · · 1"
+
!1 α α2 · · · αq−2 "
=
!0 1 + α 1 + α2 · · · 1 + αq−2 "
,
which has a weight of q −2. This code word is then used to build a (q −1) × (q −
1) array by cyclically shifting it to the right to form the next row of the array. Observe
that the main diagonal of this array will be zeroes.

220
NON-BINARY LOW-DENSITY PARITY CHECK CODES
Example 6.4: Array dispersion of a (7, 2, 6) Reed–Solomon code word over
GF(8): The code word of weight q −2 = 6 is obtained by adding the two vectors:
!1 1 1 1 1 1 1"
+
!
1 α α2 α3 α4 α5 α6 "
=
!
0 α3 α6 α α5 α4 α2 "
.
This then is used to build the 7 × 7 array:
0
α3
α6
α
α
5
α4
α2
α2
0
α3
α6
α
α
5
α4
α4
α2
0
α3
α6
α
α
5
α5
α4
α2
0
α3
α6
α
α
α
5
α4
α2
0
α3
α6
α
α
α
5
α4
α2
0
α3
α3
α6
6
α
α
5
α4
α2
0
Since Reed–Solomon codes are cyclic, each row is a code word and each column,
when read from bottom to top, is also a code word, all with weights equal to 6.
After applying array dispersion we will obtain a much larger 49 × 49 array. This
is too large to reproduce here but a small part of it is shown below, corresponding
to the 2 × 2 subarray circled.


0
0
0
0
0
0
0
0
0
0
α3
0
0
0
· · ·
0
0
0
0
0
0
0
0
0
0
0
α4
0
0
· · ·
0
0
0
0
0
0
0
0
0
0
0
0
α5
0
· · ·
0
0
0
0
0
0
0
0
0
0
0
0
0
α6
· · ·
0
0
0
0
0
0
0
1
0
0
0
0
0
0
· · ·
0
0
0
0
0
0
0
0
α
0
0
0
0
0
· · ·
0
0
0
0
0
0
0
0
0
α2
0
0
0
0
· · ·
0
0
α2
0
0
0
0
0
0
0
0
0
0
0
· · ·
0
0
0
α3
0
0
0
0
0
0
0
0
0
0
· · ·
0
0
0
0
α4
0
0
0
0
0
0
0
0
0
· · ·
0
0
0
0
0
α5
0
0
0
0
0
0
0
0
· · ·
0
0
0
0
0
0
α6
0
0
0
0
0
0
0
· · ·
1
0
0
0
0
0
0
0
0
0
0
0
0
0
· · ·
0
α
0
0
0
0
0
0
0
0
0
0
0
0
· · ·
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...


.

DECODING NON-BINARY LDPC CODES WITH THE SUM–PRODUCT ALGORITHM
221
6.5 Decoding Non-Binary LDPC Codes with the Sum–Product
Algorithm
The sum–product algorithm for binary LDPC codes can be extended to decode non-
binary LDPC codes, but with an increase in decoding complexity. Firstly, for a non-
binary LDPC code deﬁned over GF(q), each received symbol can be one of q different
elements in GF(q). Secondly, the horizontal step becomes more complicated as there
are now more possible non-binary sequences to satisfy the parity check equations.
The matrices Q and R used in the horizontal and vertical steps of the sum–product
algorithm are deﬁned in (6.23) and (6.24) respectively:
Q =


q11(0)
q12(0)
q13(0)
· · ·
q1,n−1(0)
q1n(0)
q11(1)
q12(1)
q13(1)
· · ·
q1,n−1(1)
q1n(1)
q11(α)
q12(α)
q13(α)
· · ·
q1,n−1(α)
q1n(α)
...
...
...
· · ·
...
...
q11(αq−2)
q12(αq−2)
q13(αq−2)
· · ·
q1,n−1(αq−2)
q1n(αq−2)
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
qm1(0)
qm2(0)
qm3(0)
· · ·
qm,n−1(0)
qmn(0)
qm1(1)
qm2(1)
qm3(1)
· · ·
qm,n−1(1)
qmn(1)
qm1(α)
qm2(α)
qm3(α)
· · ·
qm,n−1(α)
qmn(α)
...
...
...
· · ·
...
...
qm1(αq−2)
qm2(αq−2)
qm3(αq−2)
· · ·
qm,n−1(αq−2)
qmn(αq−2)


.
(6.23)

222
NON-BINARY LOW-DENSITY PARITY CHECK CODES
R =


r11(0)
r12(0)
r13(0)
· · ·
r1,n−1(0)
r1n(0)
r11(1)
r12(1)
r13(1)
· · ·
r1,n−1(1)
r1n(1)
r11(α)
r12(α)
r13(α)
· · ·
r1,n−1(α)
r1n(α)
...
...
...
· · ·
...
...
r11(αq−2)
r12(αq−2)
r13(αq−2)
· · ·
r1,n−1(αq−2)
r1n(αq−2)
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
rm1(0)
rm2(0)
rm3(0)
· · ·
rm,n−1(0)
rmn(0)
rm1(1)
rm2(1)
rm3(1)
· · ·
rm,n−1(1)
rmn(1)
rm1(α)
rm2(α)
rm3(α)
· · ·
rm,n−1(α)
rmn(α)
...
...
...
· · ·
...
...
rm1(αq−2)
rm2(αq−2)
rm3(αq−2)
· · ·
rm,n−1(αq−2)
rmn(αq−2)


.
(6.24)
It can be observed that each nonzero element deﬁned over GF(q) in the parity check
matrix H has q probabilities associated with it, instead of two probabilities as in the
binary case.
6.5.1 Received Symbol Likelihoods
In (6.8) it was shown how to determine the likelihoods of a demodulated symbol
for the AWGN channel. Assuming a non-binary LDPC code deﬁned over GF(q) and
that M-PSK modulation is chosen, then provided that q = M the likelihoods of the
demodulated symbols are also the likelihoods of the received symbols. However,
for the case where q > M and M divides q we must concatenate the likelihoods of
the demodulated symbols. As an example, take an LDPC code deﬁned over GF(16)
and QPSK modulation. We know that a ﬁnite ﬁeld element in GF(16) is made up
of four bits and a QPSK modulated symbol is made up of two bits. Therefore, two
consecutive QPSK modulated symbols contain a ﬁnite ﬁeld element. The element α
in GF(16) is represented as 0010 in binary. Therefore, the likelihood of a received
symbol being α is the product of the likelihood that one demodulated symbol is 00

DECODING NON-BINARY LDPC CODES WITH THE SUM–PRODUCT ALGORITHM
223
and the likelihood that the neighbouring demodulated symbol is 10, that is, from (6.8),
P(ri = α) = f α = g00g10.
6.5.2 Permutation of Likelihoods
In general, the parity check equations are of the form:
zi =
n

j=1
hij cj,
(6.25)
where hij and cj ∈GF(q). The parity check equation is satisﬁed when:
h11c1 + h12c2 + · · · + h1ncn = 0.
For the horizontal step we calculate the probabilities rij(x) from (6.10) by ﬁrst
substituting all possible non-binary elements into the coded symbols that satisfy the
parity check equation when cj = x, that is:
hi1c1 + hi2c2 + · · · + hincn = hij cj,
and then determining the probability of each sequence. This is more complicated
than the binary case since we must now consider the non-binary parity check matrix
elements. Each coded symbol has q likelihoods associated with it. When the coded
symbol is multiplied by a non-binary parity check element we can compensate for this
by cyclically shifting downwards this column vector of likelihoods, with the exception
of the ﬁrst likelihood, corresponding to the probability of the coded symbol being
zero. The number of cyclic shifts is equal to the power of the primitive element that
is multiplied with the coded symbol. This is illustrated below:
cj →


qij(0)
qij(1)
...
qij(αq−2)


αc j →


qij(0)
qij(αq−2)
qij(1)
...


α2c j →


qij(0)
...
qij(αq−2)
qij(1)


etc.
This cyclic shift of the likelihoods is known as a permutation [8] and transforms
the parity check equation from (6.25) to:
c1 + c2 + · · · + cn = cj,
which is more similar to the binary parity check equations. The inverse of a permu-
tation is a depermutation, where the likelihoods are cyclically shifted upwards, again
with the exception of the ﬁrst likelihood.

224
NON-BINARY LOW-DENSITY PARITY CHECK CODES
CONV
∏
CONV
CONV
c1
h11
h21
f1
h11c1
h21c1
hm1
hm1c1
c2
h12
h22
f2
h12c2
h22c2
hm2
hm2c2
cn
h1n
h2n
fn
h1ncn
h2ncn
hmn
hmncn
qmn
rmn
rmn
qmn
Permuted
Depermuted
Figure 6.4
Generalized factor graph of a non-binary LDPC code [8].
6.5.3 Factor Graph of a Non-Binary LDPC Code
The factor graph of a non-binary LDPC code graphically shows the operation of the
sum–product decoding algorithm (Figure 6.4).
The factor graph for non-binary LPDC codes is similar to that for binary LPDC
codes, but we must now take into account the non-binary elements in the parity check
matrix, which are denoted as hij, i = 1, 2, . . ., m and j = 1, 2, . . ., n. In Figure 6.4
the number of parity check matrix elements connected to a coded symbol cj is the
column weight of the code and the number of connections to each parity check zi
is the row weight of the code. The likelihoods of each coded symbol fj are column
vectors containing the q likelihoods of the coded symbol being an element in GF(q).
The block labelled  connects the non-binary elements in each row to the parity
checks. In Figure 6.5 a generalized factor graph is shown, with FFT blocks replacing
the convolutional blocks, which reduces complexity.
A factor graph of the parity check matrix in (6.26) is show in Figure 6.6.
H =


α
0
1
α
0
1
α2
α
0
1
1
0
0
α
α2
0
α2
1

.
(6.26)
6.5.4 The Fast Fourier Transform for the Decoding of Non-Binary
LDPC Codes
In Section 6.3.4, the FFT was used to reduce the complexity of the horizontal step in the
sum–product algorithm. For binary decoding of LDPC codes, the matrix Q contains

DECODING NON-BINARY LDPC CODES WITH THE SUM–PRODUCT ALGORITHM
225
c1
h11
h21
f1
h11c1
h21c1
hm1
hm1c1
c2
h12
h22
f2
h12c2
h22c2
hm2
hm2c2
cn
h1n
h2n
fn
h1ncn
h2ncn
hmn
hmncn
∏
qmn
rmn
rmn
qmn
Permuted
Depermuted
F
F
F
F
F
F
F
F
F
Figure 6.5
Generalized factor graph of a non-binary LDPC code showing the replacement of the
convolution operations with FFTs.
1 × 2 column vectors containing two probabilities, qmn(0) and qmn(1). The Fourier
transform of this column vector is then obtained by multiplying by the Hadamard
matrix deﬁned in (6.21).
However, for non-binary LDPC codes deﬁned over GF(q), Q contains 1 × q column
vectors and the Fourier transform is obtained by multiplying by the tensor product of
c1
c2
c3
c4
c5
c6
α
α2
α
α
1
α2
α
1
1
α2
1
1
f1
f2
f3
f4
f5
f6
αc1
α2c1
αc2
αc2
c3
α2c3
αc4
c4
c5
α2c5
c6
c6
F
F
F
F
F
F
F
F
F
F
F
F
Figure 6.6
Factor graph for the parity check matrix of (6.26).

226
NON-BINARY LOW-DENSITY PARITY CHECK CODES
+
+
+
−
+
+
+
−
+
+
+
−
+
+
+
−
qmn(0) ≡qmn(00)
qmn(1) ≡qmn(01)
qmn(α) ≡qmn(10)
qmn(α2) ≡qmn(11)
F(qmn(0))
F(qmn(1))
F(qmn(α))
F(qmn(α2))
Figure 6.7
Radix-2 butterﬂy diagram for GF(4).
Hadamard matrices. An example of the tensor product of two Hadamard matrices is
given below:
W4 = W2 ⊗W2 =

W2 W2
W2 −W2

= 1
2


1
1
1
1
1
−1
1
−1
1
1
−1
−1
1
−1
−1
1

.
Therefore, for an LDPC code deﬁned over GF(4), the Fourier transform of the 1 ×
4 column vector of probabilities qmn(0), qmn(1), qmn(α) and qmn(α2) is:


F (qmn(0))
F (qmn(1))
F (qmn(α))
F

qmn(α2)


= 1
2


1
1
1
1
1
−1
1
−1
1
1
−1
−1
1
−1
−1
1




qmn(0)
qmn(1)
qmn(α)
qmn(α2)

.
As before, the inverse FFT is achieved by multiplying by the Hadamard matrix, that
is W4W4 = I4. The FFT operation can be expressed in terms of its radix-2 butterﬂy
diagram. An example of the radix-2 butterﬂy diagram for GF(4) is shown in Figure
6.7.
The ordering of the probabilities qmn(x) is very important when determining the
FFT. It is essential that for each pair of qmn(x), the binary representation of the ﬁnite
ﬁeld elements x differ by one bit. In Figure 6.8, the radix-2 butterﬂy diagram for
GF(16) is also given.
Example 6.5: Decoding a non-binary LDPC code with the FFT sum–product
algorithm: In this example we decode the (6, 3) LDPC code deﬁned over
GF(4) with a parity check matrix given by (6.23). We assume that the trans-
mitted code word was c = [2 1 0 3 0 2 ] and the received word is r =
[2 1 0 2 2 2], with two errors highlighted. Furthermore, it is assumed that

DECODING NON-BINARY LDPC CODES WITH THE SUM–PRODUCT ALGORITHM
227
+
+
+
−
+
+
+
−
+
+
+
−
+
+
+
−
+
+
+
−
+
+
+
−
+
+
+
−
+
+
+
−
+
+
+
−
+
+
+
+
+
+
−
−
+
+
+
−
+
+
+
−
+
+
+
−
+
+
+
−
+
+
+
−
+
+
+
−
+
+
+
−
+
+
+
−
+
+
+
−
+
+
+
−
+
+
+
−
+
+
+
−
+
+
+
−
+
+
−
+
+
+
−
+
+
+
+
−
+
+
+
+
−
+
+
+
+
−
+
+
+
+
−
+
+
+
+
−
+
+
+
+
−
+
qmn(0) ≡qmn(0000)
qmn(1) ≡qmn(0001)
qmn(α) ≡qmn(0010)
qmn(α4) ≡qmn(0011)
qmn(α2) ≡qmn(0100)
qmn(α8) ≡qmn(0101)
qmn(α5) ≡qmn(0110)
qmn(α10) ≡qmn(0111)
qmn(α3) ≡qmn(1000)
qmn(α9) ≡qmn(1010)
qmn(α7) ≡qmn(1011)
qmn(α6) ≡qmn(1100)
qmn(α13) ≡qmn(1101)
qmn(α11) ≡qmn(1110)
qmn(α14) ≡qmn(1001)
qmn(α12) ≡qmn(1111)
F(qmn(0))
F(qmn(1))
F(qmn(α))
F(qmn(α4))
F(qmn(α2))
F(qmn(α8))
F(qmn(α5))
F(qmn(α10))
F(qmn(α3))
F(qmn(α14))
F(qmn(α9))
F(qmn(α7))
F(qmn(α6))
F(qmn(α13))
F(qmn(α11))
F(qmn(α12))
Figure 6.8
Radix-2 butterﬂy diagram for GF(16).
the received symbols have the following likelihoods:
f =


0.182512
0.149675
0.444988
0.044678
0.412355
0.320530
0.046118
0.723055
0.324187
0.030350
0.073539
0.079952
0.615774
0.021827
0.133538
0.550805
0.436298
0.479831
0.155596
0.105443
0.097286
0.374167
0.077809
0.119687

,
(6.27)

228
NON-BINARY LOW-DENSITY PARITY CHECK CODES
where each column in f contains the likelihoods of a received symbol being 0, 1, α
or α2. The matrix Q is then initialized as shown in (6.21). However, as described
earlier, the elements in Q must be permuted due to the received coded symbols
being multiplied by a non-binary element in the parity check matrix. The permuted
version of Q is given in (6.28):
Q =


0.182512
0
0.444988
0.044678
0
0.320530
0.046118
0
0.324187
0.030350
0
0.079952
0.615774
0
0.133538
0.550805
0
0.479831
0.155596
0
0.097286
0.374167
0
0.119687
0.182512
0.149675
0
0.044678
0.412355
0
0.046118
0.723055
0
0.030350
0.073539
0
0.615774
0.021827
0
0.550805
0.436298
0
0.155596
0.105443
0
0.374167
0.077809
0
0
0.149675
0.444988
0
0.412355
0.320530
0
0.723055
0.324187
0
0.073539
0.079952
0
0.021827
0.133538
0
0.436298
0.479831
0
0.105443
0.097286
0
0.077809
0.119687


.
(6.28)
Permuted Q
=


0.182512
0
0.444988
0.044678
0
0.320530
0.155596
0
0.324187
0.374167
0
0.079952
0.046118
0
0.133538
0.030350
0
0.479831
0.615774
0
0.097286
0.550805
0
0.119687
0.182512
0.149675
0
0.044678
0.412355
0
0.615774
0.105443
0
0.030350
0.073539
0
0.155596
0.723055
0
0.550805
0.436298
0
0.046118
0.021827
0
0.374167
0.077809
0
0
0.149675
0.444988
0
0.412355
0.320530
0
0.105443
0.133538
0
0.436298
0.079952
0
0.723055
0.097286
0
0.077809
0.479831
0
0.021827
0.324187
0
0.073539
0.119687


.
(6.29)

DECODING NON-BINARY LDPC CODES WITH THE SUM–PRODUCT ALGORITHM
229
We now perform the FFT operation on each of the 1 × 4 column vectors. For
example, the Fourier transform of the column vector in the top-left-hand corner of
the permuted Q matrix is determined by:
F


q11(0)
q11(1)
q11(α)
q11(α2)

= 1
2


1
1
1
1
1 −1
1 −1
1
1 −1 −1
1 −1 −1
1




0.182512
0.155596
0.046118
0.615774


= 1
2


0.182512 + 0.155596 + 0.046118 + 0.615774
0.182512 −0.155596 + 0.046118 −0.615774
0.182512 + 0.155596 −0.046118 −0.615774
0.182512 −0.155596 −0.046118 + 0.615774


F


q11(0)
q11(1)
q11(α)
q11(α2)

= 1
2


1
−0.542740
−0.323783
0.596571

.
The FFT of each column vector in the permuted Q matrix is given in (6.30):
F(Q) =


1
0
1
1
0
1
−0.542740
0
0.157053
−0.849944
0
0.600723
−0.323783
0
0.538351
−0.162310
0
−0.199036
0.596571
0
0.084549
0.190965
0
−0.119566
1
1
0
1
1
0
−0.323783
0.745461
0
0.190965
0.697305
0
0.596571
−0.489765
0
−0.849944
−0.028213
0
−0.542740
−0.656996
0
−0.162310
−0.019673
0
0
1
1
0
1
1
0
0.745461
0.084549
0
−0.019673
0.600723
0
−0.489765
0.157053
0
0.697305
−0.199036
0
−0.656996
0.538351
0
−0.0282143
−0.119566


.
(6.30)
To complete the horizontal step we apply (6.20) to determine the probabilities
rmn(x). To determine r11(x) we take the product of the Fourier transformed column
vectors in F(Q) corresponding to the third, fourth and sixth received symbols and

230
NON-BINARY LOW-DENSITY PARITY CHECK CODES
then apply the inverse FFT to obtain a 1 × 4 column vector containing r11(x):
r11(x) = F−1
# 
n∈N1/1
F (q1n(x))
$
= F−1


1
2


1
0.157053
0.538351
0.084549

×


1
−0.849944
−0.162310
0.190965

×


1
0.600723
−0.199036
−0.119566




= F−1


1
2


1
−0.080188
0.0173917
−0.003222




= 1
4


1
1
1
1
1
−1
1
−1
1
1
−1
−1
1
−1
−1
1




1
−0.080188
0.0173917
−0.003222


= 1
4


0.933982
1.100802
0.905642
1.059574

=


0.233818
0.274878
0.226088
0.265216


The remaining probabilities rmn(x) are given in (6.31):
R =


0.233818
0
0.313258
0.244365
0
0.277593
0.274878
0
0.181512
0.272982
0
0.236553
0.226088
0
0.325298
0.230033
0
0.258631
0.265216
0
0.179931
0.252620
0
0.227223
0.271356
0.242364
0
0.208230
0.286092
0
0.222772
0.264788
0
0.295891
0.338076
0
0.278277
0.236078
0
0.207616
0.190862
0
0.227595
0.256769
0
0.288262
0.184970
0
0
0.244755
0.264237
0
0.273865
0.238776
0
0.244347
0.269750
0
0.233789
0.234406
0
0.254745
0.231358
0
0.245066
0.260604
0
0.256153
0.234655
0
0.247279
0.266214


.
(6.31)

DECODING NON-BINARY LDPC CODES WITH THE SUM–PRODUCT ALGORITHM
231
The matrix R is now complete, but it must be depermuted before it can be used
in the vertical step. The depermuted R is given in (6.32):
Depermuted R
=


0.233818
0
0.313258
0.244365
0
0.277593
0.226088
0
0.181512
0.230033
0
0.236553
0.265216
0
0.325298
0.252620
0
0.258631
0.274878
0
0.179931
0.272982
0
0.227223
0.271356
0.242364
0
0.208230
0.286092
0
0.227595
0.236078
0
0.295891
0.338076
0
0.222772
0.256769
0
0.207616
0.190862
0
0.278277
0.264788
0
0.288262
0.184970
0
0
0.244755
0.264237
0
0.273865
0.238776
0
0.254745
0.234655
0
0.247279
0.234406
0
0.256153
0.269750
0
0.233789
0.260604
0
0.244347
0.231358
0
0.245066
0.266214


.
(6.32)
The vertical step is carried out in the same way as for the binary case. To
determine q11(x) we apply (6.13):
q11(0) = β11 f 0
1

m∈M1/1
rm1(0) = β11 × 0.182512 × 0.271356 = 0.049526β11
q11(1) = β11 f 1
1

m∈M1/1
rm1(1) = β11 × 0.046118 × 0.227595 = 0.010496β11
q11(α) = β11 f α
1

m∈M1/1
rm1(α) = β11 × 0.615774 × 0.222772 = 0.137177β11
q11(α2) = β11 f α2
1

m∈M1/1
rm1(α2) = β11 × 0.155596 × 0.278277 = 0.043299β11
β11 =
1
0.049256 + 0.010496 + 0.137177 + 0.043299 = 4.162712
q11(0) = 0.049526β11 = 0.205930
q11(1) = 0.010496β11 = 0.043644
q11(α) = 0.137177β11 = 0.570388
q11(α2) = 0.043299β11 = 0.180039
.

232
NON-BINARY LOW-DENSITY PARITY CHECK CODES
The complete matrix Q is given in (6.33):
Q =


0.205930
0
0.46625
0.038683
0
0.303488
0.043644
0
0.301653
0.037341
0
0.074315
0.570388
0
0.142839
0.475497
0
0.495852
0.180039
0
0.089252
0.448479
0
0.126345
0.164650
0.145266
0
0.042123
0.447806
0
0.040229
0.730398
0
0.026937
0.072108
0
0.630104
0.022170
0
0.536854
0.404473
0
0.165017
0.102165
0
0.394068
0.075612
0
0
0.150837
0.537825
0
0.490530
0.343296
0
0.709767
0.227035
0
0.103376
0.072970
0
0.023304
0.167601
0
0.346251
0.478806
0
0.116092
0.067538
0
0.059844
0.104928


.
(6.33)
Finally, the pseudo posterior probabilities are determined using (6.18) and are
given in (6.34):
Q =


0.205930 0.145266 0.466255 0.038683 0.447806 0.303488
0.043644 0.730398 0.301653 0.037341 0.072108 0.074315
0.570388 0.022170 0.142839 0.475497 0.404473 0.495852
0.180039 0.102165 0.089252 0.448479 0.075612 0.126345


.
(6.34)
From (6.19), taking the highest likelihood in each column of (6.34) gives
a decoded code word of ˆc = [α
1
0
α
0
α]. The ﬁrst iteration of the
sum–product algorithm has corrected the ﬁfth received symbol but there is still
the error in the fourth received symbol. It turns out that only one more iteration is
required to correct this error too, and the relevant matrices from the second iteration
are given. The updated matrix Q in (6.33) is permuted and the FFT is applied to
each column vector, as shown in (6.35). The matrix R is depermuted and shown
in (6.36), and the updated matrix Q and pseudo posterior probabilities are given in

DECODING NON-BINARY LDPC CODES WITH THE SUM–PRODUCT ALGORITHM
233
(6.37) and (6.38) respectively:
F(Q) =


1
0
1
1
0
1
−0.500852
0
0.218190
−0.847952
0
0.598679
−0.228062
0
0.535817
−0.025675
0
−0.244394
0.552635
0
0.111015
0.028361
0
−0.140334
1
1
0
1
1
0
−0.340666
0.751328
0
0.157955
0.704559
0
0.589508
−0.505138
0
−0.861880
−0.039829
0
−0.590243
−0.665128
0
−0.127581
−0.046837
0
0
1
1
0
1
1
0
0.721208
0.210727
0
0.100747
0.644204
0
−0.466142
0.410854
0
0.673561
−0.167468
0
−0.651719
0.529721
0
0.187811
−0.103553


.
(6.35)
Depermuted R
=


0.223039
0
0.312657
0.238958
0
0.274386
0.221579
0
0.186628
0.228330
0
0.227183
0.276740
0
0.314472
0.256737
0
0.271947
0.278642
0
0.186243
0.275975
0
0.226484
0.276232
0.236345
0
0.206548
0.291534
0
0.225755
0.244699
0
0.287521
0.336792
0
0.232438
0.265419
0
0.203285
0.188251
0
0.265575
0.253537
0
0.302645
0.183422
0
0
0.239258
0.278016
0
0.291432
0.205369
0
0.267581
0.228322
0
0.226443
0.230132
0
0.255591
0.248275
0
0.224605
0.302287
0
0.237570
0.245388
0
0.257521
0.262212


.
(6.36)

234
NON-BINARY LOW-DENSITY PARITY CHECK CODES
Q =


0.205545
0
0.485609
0.037950
0
0.252543
0.042447
0
0.290544
0.035886
0
0.070589
0.583537
0
0.130139
0.460471
0
0.556467
0.168472
0
0.093708
0.465692
0
0.120401
0.153792
0.137779
0
0.040705
0.471531
0
0.038606
0.744379
0
0.026422
0.065340
0
0.643804
0.021464
0
0.539167
0.384507
0
0.163797
0.096378
0
0.393706
0.078622
0
0
0.144486
0.535638
0
0.498018
0.333508
0
0.722661
0.232931
0
0.102603
0.068878
0
0.023662
0.161675
0
0.340255
0.494822
0
0.109191
0.069757
0
0.059124
0.102792


.
(6.37)
Q =


0.205545
0.137779
0.485609
0.037950
0.471531
0.252543
0.042447
0.744379
0.290544
0.035886
0.065340
0.070589
0.583537
0.021464
0.130139
0.460471
0.384507
0.556467
0.168472
0.096378
0.093708
0.465692
0.078622
0.120401

.
(6.38)
From the matrix Q, the decoded code word is now ˆc = [α 1
0 α2 0 α],
which matches the original transmitted code word.
6.6 Conclusions
A very important class of block code known as the low-density parity check (LDPC)
code has been explained, with discussions on some structured construction methods
for binary and non-binary LDPC codes. Additionally, a decoding algorithm called
the sum–product algorithm has been studied in detail for use with binary and non-
binary LDPC codes. A method to reduce the complexity of the sum–product algorithm
has been given, using fast Fourier transforms based on the Hadamard matrix, which
replaces the original horizontal step of the decoding algorithm.
The study of LDPC codes is very important as they are fast becoming one of the
more popular coding schemes for a number of future applications in wireless commu-
nications and eventually magnetic storage. The binary LDPC codes are well known
and perform as well as turbo codes, and in some cases can outperform turbo codes

REFERENCES
235
for large block lengths. Non-binary LDPC codes are less well known, but Mackay
showed that these codes outperform binary LDPC codes with further increases in
performance as the size of the ﬁnite ﬁeld increases [10]. This is of course at the ex-
pense of higher complexity, but non-binary LDPC codes also have better convergence
properties, requiring less iterations to decode a received word. With the inclusion of
the FFT proposed by Barnault et al. [8], the complexity is reduced and non-binary
LDPC codes can now be used practically in many future applications.
References
[1] Gallager, R.G. (1962) Low-density parity-check codes. IRE Transactions on Information Theory,
IT-8, 21–8.
[2] Mackay, D.J. and Neal, R.M. (1995) Good codes based on very sparse matrices. Cryptography and
Coding, 5th IMA Conference (ed. C. Boyd), Vol. 1025, pp. 100–11 (Lecture Notes in Computer
Science, Springer).
[3] Chung, S.Y., Forney, G.D. Jr., Richardson, T.J. and Urbanke, R. (2001) On the design of low-density
parity-check codes within 0.0045 dB of the Shannon limit. IEEE Communications Letters, 5 (2),
58–60.
[4] Ammar, B., Honary, B., Kou, Y. et al. (2004) Construction of low-density parity-check codes based
on balanced incomplete block designs. IEEE Transactions on Information Theory, 50 (6), 1257–69.
[5] Hu, X.-Y., Eleftheriou, E. and Arnold, D.-M. (2001) Progressive edge-growth tanner graphs. Pro-
ceedings of IEEE GlobeCom, San Antonio, Texas, pp. 995–1001.
[6] Kou, Y., Lin, S. and Fossorier, M.P.C. (2001) Low-density parity-check codes based on ﬁnite
geometries: a rediscovery and new results. IEEE Transactions on Information Theory, 47 (7),
2711–36.
[7] Moon, T.K. (2005) Error Correction Coding. Mathematical Methods and Algorithms, Wiley Inter-
science, ISBN 0-471-64800-0.
[8] Barnault, L. and Declerq, D. (2003) Fast decoding algorithm for LDPC over GF(2q). IEEE Infor-
mation Theory Workshop, Paris, France, pp. 70–3.
[9] Sylvester, J.J. (1867) Thoughts on orthogonal matrices, simultaneous sign-successions, and tessel-
lated pavements in two or more colours, with applications to newton’s rule, ornamental tile-work,
and the theory of numbers. Phil. Mag., 34, 461–75.
[10] Davey, M.C. and Mackay, D.J. (1998) Low density parity check codes over GF(q). IEEE Information
Theory Workshop, Killarney, Ireland, pp. 70–1.
[11] Lin, S., Song, S., Zhou, B. et al. (2007) Algebraic constructions of non-binary quasi-cyclic LDPC
codes: array masking and dispersion. 9th International Symposium on Communication Theory and
Applications (ISCTA), Ambleside, Lake District, UK.


7
Non-Binary Convolutional Codes
7.1 Introduction
So far this book has dealt with many different types of binary and non-binary
block code, from simple parity check codes to the powerful low-density parity
check codes, but another important class of error-correcting code is the convolu-
tional code, developed by Elias in 1955 [1]. Convolutional codes differ from block
codes in that a block code takes a ﬁxed message length and encodes it, whereas
a convolutional code can encode a continuous stream of data. Convolutional codes
also have much simpler trellis diagrams than block codes, and soft-decision decod-
ing can easily be realized using the Viterbi algorithm, explained later. However,
at higher code rates the performance of convolutional codes is poorer than that of
block codes.
In this chapter, the principles of convolutional codes are introduced. The convolu-
tional encoder is explained, followed by a description of its state table, tree diagram,
signal ﬂow graph and trellis diagram. From the signal ﬂow graph we can determine
the transfer function of the code, which gives us the Hamming distance of each code
word, beginning with the minimum Hamming distance of the code and the number of
nearest neighbours. This distance spectrum can be used to evaluate the performance
of the convolutional code.
The idea of trellis coded modulation(TCM) is then presented, whereby a convolu-
tional code is combined with the modulation process. By carefully partitioning the
constellation diagram, a bandwidth-efﬁcient TCM code can be constructed with a
signiﬁcant coding gain over an uncoded system.
Binary TCM coding is then extended to TCM codes deﬁned over rings of inte-
gers, known as ring-TCM codes. Ring-TCM encoding is explained and a method
to search for new ring-TCM codes using a genetic algorithm is presented. Finally,
ring-TCM codes are combined with spatial-temporal diversity to create space-time
ring-TCM (ST-RTCM) codes. The design criteria for good ST-RTCM codes are
Non-Binary Error Control Coding for Wireless Communication and Data Storage
Rolando Antonio Carrasco and Martin Johnston
C 2008 John Wiley & Sons, Ltd

238
NON-BINARY CONVOLUTIONAL CODES
D
D
Input
u = (u1, u2, ..., uk)
Output
c = (c1, c2, ..., c2k – 1, c2k)
c2i – 1, i = 1, 2, 3, ..., k
c2i , i = 1, 2, 3, ..., k
Figure 7.1
Nonsystematic convolutional encoder, R 1/2 with constraint length K = 3.
given and the performances of several ST-RTCM codes are presented on multiple-
input–multiple-output (MIMO) fading channels, including urban environments such
as indoor, pedestrian and vehicular.
An example of a convolutional code is shown in Figure 7.1, where D denotes a
memoryelement.The encodertakes aninput uoflengthk bits andforeachbitproduces
two coded bits. These coded bits are collected using the switch to produce a code
word c of length n = 2k bits. In this case the input message cannot be easily recovered
from the code word since this is an example of a nonsystematic convolutional code.
The two memory elements of Figure 7.1 can only have four possible pairs of binary
values, 00, 01, 10 and 11. These values are called states and for a given input we
can construct a state table showing what the next state will be from the current state,
along with the corresponding output. The state table for the convolutional encoder of
Figure 7.1 is shown in Table 7.1.
We can write the encoding process of the convolutional code in terms of poly-
nomials, with U(D), G(D) and C(D) as the input, generator and output polynomials
respectively. For example, the message u = 110 100 is equivalent to U(D) = 1 + D +
D3. The convolutional encoder of Figure 7.1 has two generator polynomials, G1(D) =
1 + D + D2 and G2(D) = 1 + D2. Therefore, the code word polynomials are
Table 7.1
State table for the convolutional code of Figure 7.1.
Input
Current state
Next state
Output
0
00
00
00
1
00
10
11
0
01
00
11
1
01
10
00
0
10
01
10
1
10
11
01
0
11
01
01
1
11
11
10

INTRODUCTION
239
C1(D) = U(D)G1(D) and C2(D) = U(D)G2(D). For the message polynomial deﬁned
previously, the output will be:
C1(D) = U(D)G1(D)
= (1 + D + D3)(1 + D + D2) = 1 + D + D2 + D + D2 + D3
+D3 + D4 + D5
= 1 + D4 + D5 ≡100 011
C2(D) = U(D)G2(D)
= (1 + D + D3)(1 + D2) = 1 + D2 + D + D3 + D3 + D5
= 1 + D + D2 + D5 ≡111 001
.
Combining the two coded output gives c = 11 01 01 00 10 11.
Graphical displays of convolutional codes have proven invaluable over the years for
their understanding and analysis. A practically useful graphical representation is the
code tree or tree diagram of the code. A tree diagram is created by assuming zero initial
conditions for the encoder and considering all possible encoder input sequences. The
tree diagram for the convolutional code of Figure 7.1 is given in Figure 7.2.
Since the output of the convolutional encoder is determined by the input and the
state of the encoder, a more compact representation is the state diagram and signal
ﬂow graph [2]. The state diagram is simply a graph of the possible states of the encoder
and the possible transmissions from one state to another. The signal ﬂow graph for
the encoder of Figure 7.1 is shown in Figure 7.3.
The signal ﬂow graph will be used to explain the method for obtaining the distance
properties of a convolutional code. We label the branches of the state diagram as D0,
0
1
00
00
00
00
11
10
01
11
00
01
10
00
11
10
01
11
00
01
10
11
10
01
11
00
01
10
11
10
01
11
Time
Input data bits
Figure 7.2
The tree diagram.

240
NON-BINARY CONVOLUTIONAL CODES
Xa
Xb
Xd
Xc
Xa’
D2
D
D
D
D0 = 1
D
D2
Figure 7.3
The signal ﬂow graph.
D1 or D2, where the exponent of D denotes the Hamming distance between the coded
bits of each branch in the trellis and the coded bits of the all-zeros branch. If we
use the notation for S1, S2, S3 and S4 for state representation and replace Xa = S1,
Xb = S2, Xc = S3 and Xd = S2, we can write four state equations:
Xb = D2Xa + Xc.
(7.1)
Xc = DXb + DXd.
(7.2)
Xd = DXb + DXd.
(7.3)
Xa = D2Xc.
(7.4)
The transfer function for the code is deﬁned as:
T (D) = Xa
Xa
= D2Xc
Xa
.
(7.5)
Now we must express the numerator and denominator of T(D) in terms of the same
variable so that they will cancel out. First we can rearrange (7.3) to give:
Xd = DXb
1 −D .
We also observe that (7.2) and (7.3) are identical, so:
Xc = Xd
and therefore Xc = DXb
1−D.
We now rearrange (7.1) so that:
Xa = Xb −Xc
D2
=
Xb −DXb
1 −D
D2
=
 Xb(1 −D)
1 −D
−DXb
1 −D

D2
= (1 −2D)Xb
D2 (1 −D) .

INTRODUCTION
241
We can also rewrite (7.4) as:
Xa = D2Xc = D3Xb
1 −D .
Finally, substituting Xa and Xa into (7.5) gives:
T (D) =
 D3Xb
1 −D

(1 −2D) Xb
D2 (1 −D)
 =
D3Xb
(1 −2D) Xb
D2
 =
D5Xb
(1 −2D) Xb
=
D5
(1 −2D).
We have expressed Xa and Xa in terms of Xb so that they cancel out. Evaluating
the transfer function gives:
D5 + 2D6 + 4D7 + 8D8 + · · ·
1 −2D

D5
D5 −2D6
2D6
2D6 −4D7
4D7
4D7 −8D8
8D8 .
The transfer function tells us that there is one code word with a minimum Hamming
distance of 5, two code words with a Hamming distance of 6, four code words with a
Hamming distance of 7, and so on.
For a given input sequence, we can ﬁnd the output by tracing through the trellis,
but without the exponential growth in branches as in the tree. Just as the encoding
operation for convolutional codes is quite different from the encoding operation for
linear block codes, the decoding operation for linear block codes and the decoding
process for convolutional codes proceed quite differently. Since we can represent
a transmitted code word for a convolutional code as a path through the trellis, the
decoding operation consists of ﬁnding that path through a trellis which is ‘almost
like’ the received binary sequence. As before with the linear block codes, we are
interested in hard-decision decoding and a decoder that immunizes the possibility of
error, and therefore, for a given received binary vector, the decoder ﬁnds that path
through the trellis which has minimum Hamming distance from the received sequence.
Given a long received sequence of binary digits and a trellis similar to that in Figure
7.4, it would seem a quite a formidable task to search all possible paths for the best
path. However, there exists an illustrative procedure called the Viterbi algorithm [3, 4]
which simpliﬁes the process. This algorithm is a special case of what is called forward
dynamic programming.

242
NON-BINARY CONVOLUTIONAL CODES
00
11
00
11
10
01
00
11
11
00
10
01
10
01
00
11
10
01
00
11
States
Input is a 0
Input is a 1
Time
00
10
01
11
Figure 7.4
Trellis diagram for code.
A convolutional code can be made systematic [2] without affecting its minimum
distance properties by feeding back one of the outputs to the input. Such a code is
called a recursive systematic convolutional (RSC) code, and is the basic building block
for turbo codes, described in Chapter 8. An example of an RSC encoder, derived from
the nonsystematic encoder of Figure 7.1, is shown in Figure 7.5.
The decoder for the convolutional code ﬁnds the most probable sequence of data
bits ˆu given the received sequence y:
ˆu = arg

max
u
P (u | y)

,
where y is the set of code symbols c observed through noise. The above equation can
be solved using the Viterbi algorithm [3, 4], explained next, in which case the solution
is termed the ‘maximum likelihood’ or ML solution.
D
D
Input
Output
Figure 7.5
Recursive systematic convolutional (RSC) encoder.

INTRODUCTION
243
7.1.1 The Viterbi Algorithm
The Viterbi algorithm was proposed in 1967 [3] for decoding convolutional codes.
The application of the Viterbi algorithm to decoding convolutional codes is based on
the trellis diagram representing the convolutional encoder. As applied to our particular
problem, this principle states that the best (smallest Hamming distance) path through
the trellis that includes a particular node necessarily includes the best path from the
begging of the trellis to this node. What this means to us is that for each node in the
trellis we need to retain only the single best path to a node, this limiting the number
of retained paths at any time instant to the number of nodes in the trellis at that time.
The basic steps in the Viterbi algorithm are as follows [3, 4]:
1. Begin at time t = 0, at state 00.
2. Determine the distances between the ﬁrst received symbols associated with each
trellis path at the current time.
3. Select the path with the lowest distance at each state node and store both the path
and the distance.
4. If there is more than one path with the lowest distance, select one arbitrarily.
5. Increment time t and return to step 2. Add the lowest distance to the previous
lowest distance and store with the sequence of paths. This running score is called
the survivor score, and the corresponding sequence of paths, the survivor path.
6. Continue this process until all the symbols in the code word have been compared.
7. The resulting survivor path with the lowest survivor score is the most likely trans-
mitted code word.
7.1.2 Trellis Coded Modulation
In Chapter 3, the idea of coded modulation [5, 6] was introduced and applied to block
codes to form block coded modulation (BCM) codes. However, it is much more com-
mon to combine convolutional codes with modulation, resulting in trellis coded mod-
ulation (TCM) codes [6, 7]. The basic idea of TCM codes is illustrated in Figure 7.6,
u1
u2
1ku
1
1+
ku
2
1+
ku
2
1 k
ku
+
1
1
1
+
k
k
Convolutional
Encoder
Rate R =
1c
2c
1
1+
kc
Selects a 
subset of the
constellation
Selects a point
from the subset
Constellation 
Symbols
Input u of length k = k1 + k2
Figure 7.6
General idea of TCM.

244
NON-BINARY CONVOLUTIONAL CODES
which takes a binary message u = (u1, u2, . . . , uk1, uk1+1, . . . , uk1+k2) of length k =
k1 + k2 bits.
The BCM codes from Chapter 3 consisted of a multi-level block encoder, with the
component code with the largest minimum Hamming distance selecting the subset
of the constellation where the Euclidean distance between neighbouring points was
minimal. In the same way, a TCM code consists of a convolutional encoder, which
also selects a subset where the distance between points is minimal. The uncoded bits
then select points within the chosen subset in exactly the same way as the BCM code.
To ﬁnd the free distance dfree of a TCM code we can draw its trellis diagram and
ﬁnd the path with minimum Euclidean distance. Once determined, the asymptotic
coding gain (ACG) γ can be calculated by:
γ = d2
free/ε
d2min/ε ,
(7.6)
where d2
min is the squared Euclidian distance between points in the uncoded constel-
lation, ε is the energy of the coded constellation and ε is the energy of the uncoded
constellation. To maximize the free distance of the convolutional encoder, set parti-
tioning [6] must be applied to the chosen constellation, as explained in Chapter 3.
Figure 7.7 shows the set partitioning of the 8-PSK constellation.
An example of a TCM encoder for 8-PSK modulation is shown in Figure 7.8. In
this case, coded bit c1 selects a QPSK subset of the 8-PSK constellation, c2 selects a
BPSK subset of the QPSK subset and c3 selects a point within the BPSK subset.
0
1
2
3
4
5
6
7
0
2
4
6
1
3
5
7
0
4
1
5
2
6
3
7
0 4
2
6
1
5
3
7
ε
ε
2
ε
2
ε
2
ε
2
0
1
0
1
0
1
0
1
0
1
0
1
0
1
ε
0.765
c1
c1
c2
c2
c2
c2
c3
c3
c3
c3
c3
c3
c3
c3
000
001
010
011
100
101
110
111
Figure 7.7
The set partitioning of the 8-PSK constellation.

INTRODUCTION
245
D
D
8-PSK
u1
u2
c1
c2
c3
Output
Figure 7.8
A four-state TCM code for 8-PSK modulation.
The state table for the TCM code of Figure 7.8 is given in Table 7.2. From this we
can construct the trellis diagram of the TCM code, as shown in Figure 7.9.
From the trellis diagram we can see that there are two paths diverging from state
00 and converging at state 00 with minimum squared Euclidean distance d2
free = 4ε.
Since we are determining the ACG of a TCM code using 8-PSK modulation over
uncoded QPSK, the energies of both constellations are equal, that is ε = ε. The
minimum squared Euclidean distance between the points of QPSK is d2
min = 2ε and
so substituting into (7.6) gives:
γ = d2
free/ε
d2min/ε = 4ε/ε
2ε/ε = 4
2 = 2
or
3 dB.
From it can be seen that the large free distance of the TCM code more than
compensates for the smaller Euclidean distances between constellation points as a
result of expanding the size of the constellation.
Table 7.2
State table for the TCM code of Figure 7.8.
Inputs
Initial States
Next States
Outputs
u1
u2
S1
S2
S
1
S
2
c1
c2
c3
0
0
0
0
0
0
0
0
0
1
0
0
0
0
1
0
1
0
0
1
0
0
0
0
0
0
1
1
1
0
0
0
1
0
1
1
0
0
0
1
1
0
1
0
0
1
0
0
1
1
1
1
1
0
0
1
0
1
1
0
1
0
1
1
1
0
1
1
1
1
1
1
0
0
1
0
0
1
0
0
0
1
0
1
0
0
0
0
1
0
0
1
1
0
0
1
0
0
1
1
1
1
0
0
0
0
1
1
0
0
1
1
1
1
1
0
0
1
0
1
1
1
0
1
1
0
0
1
1
1
1
1
1
0
1
1
1
1
1
1
0
1
1
1

246
NON-BINARY CONVOLUTIONAL CODES
00
10
01
11
0
0
4
2
6
2
6
4
0
1
5
3
7
1
5
3
7
6
0426
2640
1537
3715
State
Figure 7.9
Trellis diagram of the TCM code of Figure 7.8.
7.1.3 TCM Based on Rings of Integers
Convolutional codes based on rings of integers modulo-M were ﬁrst presented by
Massey and Mittelholzer [8, 9]. They were followed by Baldini and Farrell [10,
11], who presented TCM codes based on rings of integers. Baldini and Farrell have
developed a number of modulo-M ring-TCM codes for M-PSK constellations and have
concluded that, due to the similarities between M-PSK signal sets and the algebraic
structure of rings of integers modulo-M, modulo-M ring-TCM codes are the natural
linear codes for M-PSK modulation.
Assuming that m information bits are transmitted per baud, the general structure
of a ring-TCM encoder suitable for M-PSK modulation, with M = 2m+1, is shown
in Figure 7.10. This ring-TCM encoder works as follows: ﬁrst, m + 1 information
bits, bi, are mapped into a modulo-M symbol, aj, according to a mapping function f
(for instance, f can be a Gray mapping function). Next, m modulo-M aj symbols are
introduced into a linear multi-level convolutional encoder (MCE), which generates
m + 1 modulo-M coded symbols, xk. Finally, each one of these coded symbols xk
is associated with a signal of the M-PSK signal set and is sent to the channel. As a
total of m + 1 modulo-M coded symbols xk are transmitted per single trellis branch,
ring-TCM codes can be considered as 2(m + 1)-dimensional TCM codes.
Propagation delays caused by a fading channel can result in phase shifting of the
transmitted signal. At the demodulator these phase shifts result in a rotation of the
received signals compared with the transmitted symbols. If, after these phase shifts,
the received word is another valid code word, that code is known as phase invariant.
m + 1 
xk
m
aj
m + 1 
bi
BINARY 
SOURCE 
MAPPING 
at = f(b1, b2, …, bm+1)
MCE 
M-PSK 
SIGNAL SET 
Figure 7.10
General structure of a ring-TCM encoder suitable for M-PSK modulation.

INTRODUCTION
247
A multi-level convolutional code is 360/M phase invariant if and only if the all-one
code word can be found in the code [11]. A code with the all-one code word is said
to have a transparent encoder [11]. For example, an MCE deﬁned over Z4 is 360/4 =
90◦phase invariant if the all-one code word is present in the code.s
In general, transparent modulo-M ring-TCM codes can be readily designed, with
neither additional difﬁculty nor signiﬁcant decrease in performance with respect to
nontransparent ring-TCM codes. Furthermore, in addition to the transparency prop-
erty, ring-TCM codes present, in general, better coding gains than their nontransparent
Ungerboeck counterparts. However, the major drawback of these codes is that they
generally require a more computationally intensive decoding process. Therefore,
although it has already been established that ring-TCM schemes can constitute a pow-
erful alternative to conventional TCM schemes, their actual performance/complexity
tradeoff should be investigated.
There is also the necessity to extend the use of ring-TCM codes to other modulation
schemes than M-PSK constellations. In particular, the structure of a ring-TCM encoder
based on rings of integers modulo-4, which is suitable for any rectangular M-QAM
constellation, has been proposed [11]. This novel coded modulation scheme can result
in ring-TCM codes with excellent coding gains which are, in addition, transparent to
phase rotations of 360/4 = 90◦.
7.1.4 Ring-TCM Codes for M-PSK
This section describes a multi-level convolutional encoder (MCE) deﬁned over the
ring of integers modulo-M, ZM, which is especially suitable for combination with
signals of an M-PSK constellation. It is important to notice that we would have
preferred to deﬁne the MCE over a ﬁeld of integers modulo-M rather than over a ring
of integers modulo-M. However, due to the fact that the ﬁeld requires M to be a prime
integer, it is not suitable for most practical M-PSK constellations, which include a
composite number of signal points M = 2k, with k some positive integer.
A rate (m/p) MCE deﬁned over the ring of integers modulo-M, ZM, is a time-
invariant linear ﬁnite-state sequential circuit that, having m information input symbols
(a1, a2, . . . , am) at a time deﬁned over ZM, generates p encoded output symbols
(x1, x2, . . . , x p) at a time deﬁned over ZM, where the coefﬁcients of the MCE also
belong to ZM, and all arithmetic operations satisfy the properties of the ring of integers
modulo-M. In order to achieve the same transmission rate as uncoded modulation, the
value of p is chosen so that p = m + 1. The modulo-M ring-TCM MCE [11] suitable
for M-PSK modulation is shown in Figure 7.11.
The information sequence of symbols, ai, is shifted into the encoder, beginning at
time zero, and continuing indeﬁnitely. The stream of incoming information symbols,
ai (with 0 ≤i < ∞), is broken into segments called information frames, each one con-
taining m symbols of ZM. At discrete time n, an information frame can be represented

248
NON-BINARY CONVOLUTIONAL CODES
g
g
g
2
1g
m
g1
1
sg
2
sg
m
sg
D
D
D
a1
a2
am
x1
x2
xm
xp
Figure 7.11
Ring-TCM MCE.
as the vector:
¯an = [a1, a2, . . . , am]n.
(7.7)
In order to keep the same transmission rate as uncoded (M/2)-PSK (that is, m
information bits per channel symbol) by using the expanded rectangular M-PSK signal
set, the rate of the MCE must be (m/p), where 2p = m. Therefore, each information
frame ¯an must generate an output coded frame ¯xn containing p symbols of ZM:
¯xn = [x1, x2, . . . , x p]n.
(7.8)
The MCE is characterized by the generator matrix G(D), which maps the input
information frames ¯an into the output coded frames ¯xn and which can be written in
the functional form [12, 13]:
¯xn(D) = ¯an(D) · G(D),
(7.9)
where D is the delay operator and:
G(D) = [Im|P(D)] =


Im
g(1)(D)/f (D)
g(2)(D)/f (D)
...
g(m)(D)/f (D)


,
(7.10)

INTRODUCTION
249
where the entries of the matrix G(D):
g(i)(D) = g(i)
s Ds + · · · + g(i)
2 D2 + g(i)
1 D + g(i)
0 (Feed forward polynomial)
f (D) = fs Ds + · · · + f2D2 + f1D + 1
(Feedback polynomial)
(7.11)
are polynomials with coefﬁcients gi and fi belonging to ZM, and Im is the (m × m)
identity matrix, as the MCE is systematic.
The parity check matrix H(D) is a (1 × p) matrix that satisﬁes:
G(D) · H(D)T = 0,
(7.12)
where H(D) is readily obtained from G(D) following:
H(D) = [−P(D) : I1].
(7.13)
Every state q of the MCE is unique and can be deﬁned as:
q =
s

i=1
Mi−1 · v(n)
i ,
(7.14)
where v(n)
i , which belongs to ZM, represents the value of the ith memory element at
discrete time n, and is given by:
v(n)
i
= (modulo −M)

fi · x p +
m

j=1
g( j)
i
· a j + v(n−1)
i+1

for
1 ≤i < s
v(n)
i
= (modulo −M)

fi · x p +
m

j=1
g( j)
i
· a j


for
s = 1
,
(7.15)
with s denoting the constraint length of the MCE (number of memory elements). It
can be determined by examination of G(D) that:
s = max
i [degree g(i)(D)].
(7.16)
The number of states of the MCE, nst, represented in the trellis diagram as the
number of nodes in each column, is given by [14]:
nst ≤Ms.
(7.17)
The reason for the inequality in (7.17) is that every v(n)
i
in (7.15) can take on either
M or M/2 or M/4,. . ., or M/2m different values of ZM at a time, depending on the
coefﬁcients gi and fi (with 1 ≤i < s). So, nst = Ms only in the case that all v(n)
i
take
on the M possible values of ZM; otherwise, nst < Ms. For instance, as a clear example,
if all coefﬁcients gi and fi are even, then all v(n)
i
will also be even, and nst < Ms.

250
NON-BINARY CONVOLUTIONAL CODES
Each information frame ¯an causes the MCE to change state. This is represented by
a branch or transition to the next node in the trellis. There are Mm branches entering
each state and Mm branches leaving each state. Each branch is labelled with a coded
frame ¯xn. Therefore, any encoded frame sequence can be found by walking through
the appropriate path in the trellis. It becomes very common that trellis diagrams
of MCEs have a sizeable number of parallel transitions between states (especially
encoders with a low number of states), due to the sizeable number of total transitions
of their trellis codes. In fact, the minimum number of parallel branches, np, occurs for
a fully-connected trellis diagram; that is, every state of the trellis is connected to all
others and itself, and is given by [14]:
n p ≥Mm/M S = M(m−S),
(7.18)
where (m ≥s). If s > m, the trellis diagram can never be fully connected.
7.1.5 Ring-TCM Codes Using Quadrature Amplitude Modulation
In this section, a novel 4D TCM technique deﬁned over Z4, suitable for rectangular
M-QAM signal sets, is described [12]. There are many real channels that require the
use of transparent, or at least RI TCM, codes. We have presented the design of transpar-
ent ring-TCM codes suitable for M-PSK constellations, which have a minimum phase
ambiguity of 360/M◦and require the proper combination of a (360/M)◦RI MCE with
a (360/M)◦code-to-signal mapping. Now the design of RI and transparent ring-TCM
codes for rectangular quadrature amplitude modulation (QAM) is explained. These
have a phase ambiguity of 90◦. The functional block diagram of the 4D ring-TCM
encoder [12] is shown in Figure 7.12. It is similar to the ring-TCM transmitter for
M-PSK signal sets in Figure 7.11, but there are two important differences [12]:
1. 4D ring-TCM codes suitable for rectangular M-QAM constellation (where M = 4i,
with i a positive integer greater than 0) are always deﬁned over the ring of integers
modulo-4, Z4. That is, both bit-to-symbol mapping (i.e. Gray mapping) and MCE
are deﬁned over Z 4. This particular ring deﬁnition is required in order to resolve
the phase ambiguities of rectangular M-QAM signal sets.
Linear
MCE
M-QAM
Signal
Mapping
M-QAM
Modulator
x1
xp/2
x(p/2)+1
xp
s1
s2
s(t)
ai
Figure 7.12
Block diagram of a 4D ring-TCM transmitter suitable for rectangular M-QAM.

INTRODUCTION
251
2. The code-to-signal mapping must properly assign the output of the MCE to the
signals of the 2D M-QAM constellation. In this particular case, the p encoded
output symbols of the MCE must now be split into two sets containing ( p/2)
symbols each, x(1) and x(2), where:
x(1) =

x1, x2, . . . , x p/2

x(2) =

x(p/2)+1, x(p/2)+2, . . . , x p
.
(7.19)
Each set of symbols is then mapped onto one signal of the 2D M-QAM signal
set, sk = f (x(k)), according to some set partitioning rules that guarantee a certain
minimum distance between coded sequences. The two coded 2D M-QAM signals,
s1 and s2, are then modulated and transmitted on the channel using time division,
forming the 4D coded signal, s(t).
Let us deﬁne the isomorphism ψ(Z4) = P as the mapping of the elements of the ring
of integers modulo-4, Z4 = {0, 1, 2, 3}, onto the phase rotation they represent, P =
{0◦, 90◦, 180◦, 270◦}. Then, following (7.19), groups of ( p/2) output symbols of the
MCE (x1, x2, . . ., x(p/2)) must be mapped onto one signal of the expanded rectangular
M-QAM signal set. Also, all the symbols must be used in the mapping process with
equal frequency.
A 90◦RI code-to-signal mapping requires that any pair of signals in the rectangular
M-QAM constellation which have the same radius but are α·90◦apart, where α ∈Z4,
be assigned the groups of symbols (x1, x2, . . ., x(p/2)) and α.(x1, x2, . . ., x(p/2)).
Some possible transparent mappings for 16-QAM used by Tarokh [15], and 16-
QAM and 64-QAM from Carrasco and Farrell are shown in Figure 7.13a, b and c,
respectively [13].
7.1.6 Searching for Good Ring-TCM Codes
The choice of feed-forward and feedback coefﬁcients of the MCE can signiﬁcantly
change the parameters of the code, such as the free distance, dfree, and so it is important
to have methods for ﬁnding good TCM codes. The most obvious method would be
to apply an exhaustive search algorithm where all values of the coefﬁcients are tried,
but this would become too time-consuming for larger codes deﬁned over larger rings.
There are two criteria for good TCM codes over a Gaussian channel: maximizing
the free distance and minimizing the number of paths in the code trellis with Euclidean
distance equal to the free distance, denoted as Nfree, subject to the feed-forward and
feedback coefﬁcients [16]:
max dfree

g(i)
s , . . . , g(i)
0 , fs, . . . , f1

(7.20)
min Nfree

g(i)
s , . . . , g(i)
0 , fs, . . . , f1

.
(7.21)

252
NON-BINARY CONVOLUTIONAL CODES
00
02
01
03
11
10
12
13
20
21
23
22
31
30
32
33
Q
I
(a)  
00
10
01
11
02
03
13
12
30
31
21
20
32
33
23
22
Q
I
(b)  
Figure 7.13
QAM constellation mapping (a) Tarokh’s 16-QAM, (b) Carrasco and Farrell 16-QAM,
(c) Carrasco and Farrell 64-QAM.

INTRODUCTION
253
122
123
132
133
121
120
131
130
112
113
102
103
111
110
101
100
201
200
211
210
212
213
202
203
221
220
231
230
233
222
223
232
022
023
032
033
021
020
031
030
012
013
002
003
011
010
001
000
311
310
301
300
312
313
302
303
321
320
331
330
322
323
332
333
Q
I
(c)
Figure 7.13
(Continued)
In the following section, a much faster search algorithm for ﬁnding good ring-TCM
codes for M-PSK and M-QAM is presented.
7.1.7 Genetic Algorithm
For the solution of optimization problems, a genetic algorithm [17] has been inves-
tigated for several applications in signal processing, such as speech compression,
and has been shown to be effective at exploring a large and complex search space
in an adaptive way. Next, a formal deﬁnition of genetic algorithms is given, and the
operators used in this work are described.
A genetic algorithm can be deﬁned as the nine-tuple:
GA =

p0, S, λ, L, f, s, c, m, T

, where
pt = (at
1, . . . , at
λ) = population in the tth generation
S = search space in which the chromosome is encoded
a0
λ ∈S= elements of the search space
λ = population size
L = length of chromosome
f : S →R = ﬁtness function

254
NON-BINARY CONVOLUTIONAL CODES
s : Sλ →S = selection operator
c : S × S →S × S = crossover operator
m : S →S = mutation operator
T : Sλ →{0, 1} = termination criterion.
In searching for TCM codes, the genetic algorithm basically selects and classiﬁes
the generator code word with a high degree of adaptation as parents generate a new
generation by the combination of their components, and by the elimination of the
weakest generators from the population [18]. In genetic algorithms there are many
operators but the selection, crossover and mutation operators are only used for code
searching.
7.1.7.1 Selection Operator
Assuming that the initial population p0 = (a0
1, . . . , a0
λ) can be obtained in a heuristic
or random way from the search space, S, in which the chromosome is encoded, the next
generation is obtained from members of the previous generation using a stochastic
process, which guarantees that the number of times that one structure is selected is
dependent on its performance compared with the rest of the population.
The parent-selection operation s, s : Sλ →S produces an intermediate population
pt = (at
1, . . . , at
λ) from the population pt = (at
1, . . . , at
λ) in the tth generation, s(pt) =
pt. Any at
i = at
q in the pt is selected by a given random number αi satisfying the
following condition:
0 ≤αi ≤
λ

j=1
f

at
j

.
(7.22)
The index q is obtained from:
q = max
:
k
;
∀k ∈{1, . . . , λ} , s.t.αi ≤
λ

k=1
f

at
k

<
,
(7.23)
where λ
k=1 f

at
k

is the summation of all the ﬁtness from population pt for all
members of the population picking up the ﬁrst index k that reaches αi ≤λ
k=1
f (at
k).
7.1.7.2 Crossover Operator
For any selected chromosomes in a population pt, an associated real value 0 ≤ρ ≤1
is generated randomly. If ρ is greater than the deﬁned crossover threshold ρc,
where 0 ≤ρc ≤1, the crossover operator c : S × S →S × S is applied to this pair
of chromosomes. The strategy used in this work is the one-point crossover, cop,

INTRODUCTION
255
which produces an intermediate population pt from the population pt and is deﬁned
below:

at
i
at
(i+1)

= Cop

at
i
at
(i+1)

∀at
i ∈pt/i ∈{1, . . . , λ}
= cop
[ai,ρ1, ai,ρ2, . . . , ai,ρj, ai,(ρj+1), . . . , ai,ρL]T
[a(i+1),ρ1, a(i+1),ρ2, . . . , a(i+1),ρj, a(i+1),(ρj+1), . . . , a(i+1),ρL]T

= cop
[ai,ρ1, ai,ρ2, . . . , ai,ρj, a(i+1),(ρj+1), . . . , a(i+1),ρL]T
[a(i+1),ρ1, a(i+1),ρ2, . . . , a(i+1),ρj, ai,(ρj+1), . . . , ai,ρL]T

.
(7.24)
The pair of chromosomes is separated into two subchromosomes at ρj, hence a new
pair is composed by swapping the second subchromosome, where each one is crossed
from ρ j+1 to ρL.
7.1.7.3 Mutation Operator
For any chromosome in a population pt, an associated real value 0 ≤ρ ≤1 is gener-
ated randomly. If ρ is less than the deﬁned mutation threshold ρm, where 0 ≤ρm ≤1,
the mutation operator is applied to the chromosome. The mutation operator simply al-
ters one bit in a chromosome from 0 to 1 (or 1 to 0). The mutation operator m : S →S
produces an intermediate population pt from the population pt, as below:
at
i = m(at
i )∀i ∈{1, . . . , λ}
at
i,ρk =
ai,ρk
for
k ∈{1, 2, . . . , p −1, p + 1, . . . , L}
¯ai,ρk
for
k = p
.
(7.25)
The reproduction and crossover operators give to the genetic algorithms the bulk
of their processing power. The mutation operator is needed because, even though
reproduction and crossover are the main operators, occasionally they may become
overzealous and lose some potentially useful genetic material.
Usually there are four parameters to control the evolution of the genetic algorithm.
They are: the population size, λ; the crossover threshold, ρc; the mutation threshold,
ρm; and the number of generations. If three of these are kept ﬁxed, an optimum value
for the free parameter can be found in order to produce the optimum code rate, dfree
and Nfree.
The disadvantage of the exhaustive search algorithm is the time taken to generate
codes, especially when a high number of states and a high-order constellation are
used. It is important to note that the exhaustive search for the rectangular M-QAM
constellation is more computationally intensive than that for M-PSK constellations,

256
NON-BINARY CONVOLUTIONAL CODES
because the rectangular M-QAM constellation requires that, to determine dfree and
Nfree, all paths in the trellis be examined and compared to each other. The ﬁtness
function used in this work is an a priori function based on the computation of the
distances.
Once the code is obtained, a further check is required to investigate the performance
in terms of symbol error rate as a function of signal-to-noise ratio. The application
of the genetic algorithm to 16-QAM is shown with new codes for this particular
modulation scheme to demonstrate its feasibility, when the number of states is in-
creased to produce better constraint lengths. The application of the genetic algorithm
to the code-to-signal mapping for 16-QAM has produced a range of 180◦and 90◦
rotational invariant ring-TCM codes (RI ring-TCM), which are presented in Table 7.3
and compared with the exhaustive algorithm [18].
In Table 7.4, d2
free is the minimum squared Euclidean distance between coded
sequences of the ring-TCM code suitable for 16-QAM; g∞is the asymptotic coding
gain of the ring-TCM code suitable for 16-QAM over uncoded 8-AMPM modulation; ∗
means that the exhaustive search was stopped; and ♦means a new generated code
using the genetic algorithm.
Table 7.3
RI ring-TCM codes for 4-PSK on AWGN channel.
nst
Ring-TCM Code
Rot
d2
free
Nfree
2
320
90◦
8
5
4
230
90◦
8
5
4
032
90◦
8
5
4
212
90◦
8
1
8
030/02
90◦
8
5
8
012/02
90◦
8
1
16
032/31
90◦
12
6
16
221/31
90◦
12
2
16
232/33
90◦
16
14
32
0302/130
90◦
12
2
32
2032/110
90◦
16
8
64
0232/303
90◦
16
5
64
3103/213
90◦
16
4
64
1013/332
90◦
16
2
64
2122/321
90◦
16
1
128
21 112/0222
90◦
16
4
128
13 010/0112
90◦
16
2
128
11 230/0312
90◦
20
2
128
21 132/1210
90◦
20
6
256
32 130/3113
90◦
20
10
256
30 013/1122
90◦
20
8
256
33 012/2101
90◦
20
2
256
21 103/1221
90◦
24
29
256
23 121/0332
90◦
24
10

INTRODUCTION
257
Table 7.4
RI ring-TCM codes for 16-QAM on AWGN channel.
nst
Ring-TCM Code
Rot
d2
free
Nfree
g∞dB
2
(030 020/0)
90◦
4.0
19.187
3.01
4
(220 030/2)
90◦
4.0
5.937
3.01
♦4
(020 300/0)
90◦
4.0
6.5
3.01
∗8
(020 212 010/32)
90◦
4.0
1.031
3.01
∗8
(022 231 0130/03)
90◦
4.0
0.75
3.01
∗8
(010 232 020/21)
180◦
5.0
5.234
3.98
♦8
(000 311 010/03)
180◦
4.0
4.5
3.98
∗16
(010 232 010/22)
90◦
6.0
9.796
4.77
♦16
(233 311 213/20)
90◦
10.0
0.5
4.77
Algorithm 7.1: Algorithm for select operator
select()
{
Randomly choose αi, in the range 0 ≤αi ≤λ
j=1 f

at
j

for {k/∀k ∈{1, . . . , λ}}
{
if

q = max{k.s.t.αi ≤λ
k=1 f

at
k

}

{
return(q)
}
}
}
Algorithm 7.2: Algorithm for crossover operator
crossing()
{
Randomly choose the coefﬁcient from g(i)
s , . . . , g(i)
0 , fs, . . . , f1 ∈S
if(ρ ≥ρc)
{
increment crossover pointer;
compute crossover pointer;
}
else
crossover pointer; = 0; /* no cross, just reproduction*/
gene counter = 0;
while(gene counter < crossover pointer)
{
Copy to child1 the genetic information of father;

258
NON-BINARY CONVOLUTIONAL CODES
Copy to child2 the genetic information of mother;
increment the gene counter;
}
while(gene counter < L)
{
Copy to child1 the genetic information of mother;
Copy to child2 the genetic information of father;
increment the gene counter;
}
share parentage information();
}
Algorithm 7.3: Algorithm for crossover operator
mutate()
{
k = ring elements() ;
for(k/∀k ∈{1, . . . , λ})
{
if(ρ ≥ρm)
{
increment mutation counter;
alter one bit k = p in the chromosome;
}
}
}
Example 7.1: A 4-state ring-TCM code deﬁned over 4: A good 4-state ring-
TCM code over Z4 is the 23/1 ring-TCM code shown in Figure 7.14.
1
3
2
Input
Figure 7.14
The 23/1 ring-TCM code.

INTRODUCTION
259
Table 7.5
State table for 23/1 ring-TCM code over Z4.
Input
Initial state
Next state
Output
0
0
0
00
1
0
1
12
2
0
2
20
3
0
3
32
0
1
1
01
1
1
2
13
2
1
3
21
3
1
0
33
0
2
2
02
1
2
3
10
2
2
0
22
3
2
1
30
0
3
3
03
1
3
0
11
2
3
1
23
3
3
2
31
Its state table is given in Table 7.5 and the corresponding trellis diagram is shown
in Figure 7.15. From the state table we are able to obtain the signal ﬂow graph for
the 23/1 ring-TCM code, as shown in Figure 7.16. Comparing it to the signal ﬂow
graph of Figure 7.3, we can see that the number of states is the same but there are
more connections. As before, state 0 is denoted Xa, state 1 is denoted Xb, state 2 is
denoted Xc and state 3 is denoted Xd. From the signal ﬂow graph, it is possible to
determine the transfer function of the 23/1 ring-TCM code and hence generate the
weight distribution of the code.
0/00
1/12
2/20
3/32
0/00
1/12
2/20
3/32
3/33
0/01
1/13
2/21
0/02
2/22
3/30
1/10
0/03
1/11
2/23
3/31
0/00
1/12
2/20
3/32
3/33
0/01
1/13
2/21
0/02
2/22
3/30
1/10
0/03
1/11
2/23
3/31
0/00
1/12
2/20
3/32
3/33
0/01
1/13
2/21
0/02
2/22
3/30
1/10
0/03
1/11
2/23
3/31
0/00
3/33
2/22
1/11
0
1
2
3
States
Time
Figure 7.15
The trellis diagram of the 23/1 ring-TCM code.

260
NON-BINARY CONVOLUTIONAL CODES
Xa
Xb
Xc
Xd
Xa
D2
D
D2
D2
D2
D2
D
D2
D2
D
D
D
D2
D2
D
Figure 7.16
The signal ﬂow graph of the 23/1 ring-TCM code.
From Figure 7.16, we can deﬁne the following set of equations:
Xb = D2Xa + DXb + DXc + D2Xd.
(7.26)
Xc = DXa + D2Xb + DXc + D2Xd.
(7.27)
Xd = D2Xa + D2Xb + DXc + D2Xd.
(7.28)
Xa = D2Xb + D2Xc + D2Xd.
(7.29)
An important step is to ﬁrst notice that Xb = Xd. Subtracting (7.28) from (7.29):
Xb −Xd = (D −D2)Xb + (D2 −D)Xd

1 −D + D2
Xb =

1 −D + D2
Xd
∴Xb = Xd.
The transfer function T(D) of the signal ﬂow diagram is:
T (D) = Xa
Xa
= D2 (Xb + Xc + Xd)
Xa
= D2 (2Xb + Xc)
Xa
.
(7.30)
T (D) = D2 
2D2 + D

Xa + 2

2D2 + D

Xb + 3DXc

Xa
= 2D4 + D3 + 2D2 
2D2 + D
 Xb
Xa
+ 3D3 Xc
Xa
.
(7.31)
We can show that:
Xb = D2Xa + DXc
1 −D −D2 .
(7.32)
Also:
Xc = DXa + 2D2Xb
1 −D
.
(7.33)

INTRODUCTION
261
Substituting Xc into Xb gives:
Xb =
D2Xa
1 −D −D2 +
D
1 −D −D2
 DXa + 2D2Xb
1 −D

∴Xb
Xa
=
2D2 −D3
1 −2D −D3 .
(7.34)
Similarly:
Xc
Xa
=
D
1 −D +
2D2Xb
(1 −D)Xa
= D −D2 + D3
1 −2D −D3 .
(7.35)
Now substituting (7.34) and (7.35) into the transfer function (7.31) gives:
T (D) = 2D4 + D3 + 2D2 
2D2 + D
 Xb
Xa
+ 3D3 Xc
Xa
= D3 + 3D4 −3D5 + 8D6 −6D7
1 −2D −D3
.
(7.36)
Evaluating (7.36) gives:
D3 + 5D4 + 7D5 + 23D6 + · · ·
1 −2D −D3 
D3 + 3D4 −3D5 + 8D6 −6D7
D3 −2D4
−D6
5D4 −3D5 + 9D6 −6D7
5D4 −10D5
−5D7
7D5 + 9D6 −D7
7D5 −14D6
−7D8
23D6 −D7 + 7D8
...
.
Therefore, the 23/1 ring-TCM code over Z4 has one path with a weight of 3,
ﬁve paths with a weight of 4, seven paths with a weight of 5, and so on.
7.1.8 Performance of Ring-TCM Codes on Urban Fading Channels
Three simulation results are presented, showing the performances of three ring-TCM
codes on an indoor, a pedestrian and a vehicular channel, as deﬁned in Chapter 1. The
ring-TCM codes tested were transparent, and decoded using the soft-decision Viterbi
algorithm. The codes used were the 4-state 21/2, 8-state 213/30 and 16-state 212/31
ring-TCM codes deﬁned over Z4. The user velocities were 0 mph for indoor, 4 mph
for pedestrian and 70 mph for vehicular. The channel scenarios are easily modiﬁed

262
NON-BINARY CONVOLUTIONAL CODES
1.E–07
1.E–06
1.E–05
1.E–04
1.E–03
1.E–02
1.E–01
1.E+00
24
22
20
18
16
14
12
10
8
6
4
2
0
–2
–4
SNR (dB)
BER
Indoor Uncoded
Indoor (4-State)
Indoor (8-State)
Indoor (16-State)
Figure 7.17
Ring-TCM codes on the indoor channel.
by altering the delay, power proﬁle and Doppler spectra to create virtually any single-
input–single-output (SISO) environment based on measured data. This gives a much
more ﬂexible channel model, which corresponds to actual measured data and produces
a time-varying frequency-selective channel that is much more realistic and is essential
for testing certain distortion mitigation techniques.
Figure 7.17 shows the performance of the ring-TCM codes on the indoor channel.
This is a slow-fading channel, but the least harsh of the three urban channel models
and good results are achieved, with the 16-state 212/31 ring-TCM code achieving a
coding gain of 1.2 dB over the 21/2 and 213/30 ring-TCM codes.
Figure 7.18 shows the performance of the ring-TCM codes on the pedestrian
channel. This channel is more harsh but the relative performance of the three ring-
TCM codes has not changed. The 212/31 ring-TCM code achieves a coding gain of
approximately 14 dB over uncoded performance.
Figure 7.19 shows the performance of the ring-TCM codes on the vehicular channel.
This channel is a time-varying fading channel and is very harsh. The performance
of all three ring-TCM codes is poor and there is no signiﬁcant coding gain over an
uncoded system.
7.2 Space-Time Coding Modulation
7.2.1 Introduction
Future-generation wireless communication systems require high-speed transmis-
sion rate for both indoor and outdoor applications. Space-time (ST) coding and

SPACE-TIME CODING MODULATION
263
1.E–07
1.E–06
1.E–05
1.E–04
1.E–03
1.E–02
1.E–01
1.E+00
34
32
30
28
26
24
22
20
18
16
14
12
10
8
6
4
2
0
–2
–4
SNR (dB)
BER
Pedestrian Uncoded
Pedestrian (4-State)
Pedestrian (8-State)
Pedestrian (16-State)
Figure 7.18
Ring-TCM codes on the pedestrian channel.
multiple-input–multiple-output (MIMO) channels have been envisaged as the solution
for high-capacity levels [15]. A reliable high-speed communication is guaranteed due
to diversity, provided by space-time codes. This section describes the realization of
a space-time TCM coding scheme suitable for M-PSK [19] and M-QAM [20] over
fading channels. In this section, we also provide ST-ring TCM codes suitable for
1.E–03
1.E–02
1.E–01
1.E+00
20
18
16
14
12
10
8
6
4
2
0
–2
–4
SNR (dB)
BER
Vehicular Uncoded
Vehicular (4-State)
Vehicular (8-State)
Vehicular (16-State)
Figure 7.19
Ring-TCM codes on the vehicular channel.

264
NON-BINARY CONVOLUTIONAL CODES
Information 
source 
Space-time 
encoder & 
modulator 
Space-time 
receiver 
…
…
b
s1(t)   r1(t)
sn(t)   rm(t)
bˆ
Figure 7.20
Space-time system model.
QAM over Rayleigh fading channels. The performance of ST-ring TCM codes with
16-QAM in slow Rayleigh fading channels is evaluated with designs that involve rate
4 and 6 bits/s/Hz of 16 and 4 states.
7.2.2 Space-Time Codes Model
Consider a mobile communication system where the transmitter consists of n antennas
and the receiver has m antennas, as shown by Figure 7.20. The binary source data
are encoded by a space-time encoder that generates n streams, and each stream
is modulated and transmitted through a different antenna. Let si(t) be the symbol
transmitted by antenna i at time t for 1 ≤i ≤n and let T be the symbol period.
At a given instant t, all si(t) symbols are transmitted simultaneously in space-time
coding.
The signal received at each receiving antenna is the superposition of the n transmit-
ted signals corrupted by both Rayleigh and Rician fading and additive white Gaussian
noise (AWGN). Assuming that the elements of the signal constellations have an
average energy of Es equal to 1, the signal rj(t) received by antenna j at time t can be
described as [15]:
r j(t) =
n

i=1
h ji(t)si(t) + η j(t)
with
1 ≤j ≤m, 1 ≤i ≤n, 1 ≤t ≤l,
(7.37)
where the noise η j(t) at time t is modelled as independent samples of a zero-mean
complex Gaussian random variable with variance N0/2 per dimension. The coefﬁcients
h ji(t) are the channel gains from the transmit antenna, i, to the receive antenna,
j. These channel gains are usually modelled as independent samples of complex
Gaussian random variables with variance 0.5 per dimension.
(7.37) can be expanded to give a clear insight into the system model:
r j(t) = h1 j(t)s1(t) + h2 j(t)s2(t) + h3 j(t)s3(t) + · · · + η j(t).
(7.38)

SPACE-TIME CODING MODULATION
265
Likewise, the received signal sequence may be written as an array R of dimension
m × l, as well as the random AWGN variables in a matrix N of:
R =


r1(1)
r1(2)
· · ·
r1(l)
r2(1)
r2(2)
· · ·
r2(l)
...
...
...
rm(1)
rm(2)
· · ·
rm(l)

,
N =


η1(1)
η1(2)
· · ·
η1(l)
η2(1)
η2(2)
· · ·
η2(l)
...
...
...
ηm(1)
ηm(2)
· · ·
ηm(l)

.
(7.39)
The system model expressed in (7.37) may also be expressed in matrix form:
R=H S+N,
(7.40)
where H is the m × n channel coefﬁcient matrix.
The information symbol b(t) at time t is encoded by the ST encoder as the sequence
of code symbols s1(t), s2(t), . . ., sn(t). In the receiver a maximum-likelihood decoder
receives the signals r1(t), r2(t), . . ., rm(t), each at a different receive antenna. For a
given code of length l, the transmitted sequence was:
S =


s1(1)
s1(2)
· · ·
s1(l)
s2(1)
s2(2)
· · ·
s2(l)
...
...
...
sn(1)
sn(2)
· · ·
sn(l)

.
(7.41)
The code symbols are generated by the ring-TCM encoder shown in Figure 7.14.
The outputs xi(i = 1, 2, 3, . . . , d + 1) are Z4 symbols and are mapped according to
the constellation used and the number of transmitting antennas, n. To decode the ST-
RTCM code using the Viterbi algorithm, the accumulated metrics are determined by:
M

r j(t), si(t)|hi j

=
l
t=1
m

j=1
r j(t) −
n

i=1
hi jsi(t)
.
(7.42)
Example 7.2: Construction and Viterbi decoding of a 4-state space-time ring-
TCM code: In this example, the 23/1 ring-TCM code from Figure 7.14 is combined
with spatial diversity to form a space-time ring-TCM code deﬁned over Z4 with
QPSK modulation. The The two QPSK outputs are transmitted from two antennas
simultaneously. The trellis diagram in Figure 7.21 shows how the Viterbi algorithm
can be used for the soft-decision decoding of the 23/1 space-time ring-TCM code,
with n = 2 and m = 2.

266
NON-BINARY CONVOLUTIONAL CODES
0/00
1/12
2/20
3/32
0/00
3/33
0/01
2/22
1/11
0
1
2
3
States
22.378
1.0917
21.437
41.135
12.071 + 22.378 = 34.449
2
2
22
1
12
2
2
2
21
1
11
1
)1(
)1(
)1(
)1(
)1(
)1(
)1(
)1(
)1(
)1(
q
h
q
h
r
q
h
q
h
r
−
−
+
−
−
Received
1st antenna
CSI
Transition 
signals
Received
2nd antenna
CSI Transition 
signals
22.952 + 1.0197 = 24.044
7.8235 + 21.437 = 29.2602
4.5824 + 41.137 = 45.7176
10.1054 + 1.0917 = 11.1971
Transition metrics
Accumulated 
metrics
Survivor path
Survivor
Figure 7.21
Soft-decision decoding example of a space-time ring-TCM code using the Viterbi
algorithm.
Observe how the transition metric is calculated for every state based on (7.38).
All the information required in the calculation is from the received signals, the
CSI and the transmitted signals speciﬁed by the respective branch label. At t = 2,
there are four competitive paths arriving at every state, whose accumulated metrics
are computed according to (7.42). In state 0, for instance, the second competitor
has the lowest accumulated metric, of 24.044; hence this will be the survivor path.
This procedure is repeated for every state and carried on to the continuing symbol
intervals. When the sequence ﬁnalizes there will be four survivors, one for each
state. The survivor with the lowest resultant metric will be selected and the input
sequences traced back so that the original data can be reconstructed.
7.2.3 Performance of ST-RTCM Codes Using QPSK for MIMO
Urban Environments
Simulation results for ST-RTCM codes are now presented, evaluating their perfor-
mance on the indoor, pedestrian and vehicular MIMO fading channels. The codes
are the 4-state 21/3, the 16-state 212/31 and the 64-state 2103/132 ring-TCM codes.
Figure 7.22 shows that, as for the case of the ring-TCM codes in Figure 7.19, the
performance of the ST-RTCM codes on the vehicular MIMO channel is poor. The

SPACE-TIME CODING MODULATION
267
1.E–06
1.E–05
1.E–04
1.E–03
1.E–02
1.E–01
1.E+00
30
29
28
27
26
25
24
23
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
–1
–2
–3
–4
SNR (dB)
BER
Indoor (21/3)
Indoor (Delay Diversity)
Indoor (212/31)
Pedestrian (21/3) 
Pedestrian (Delay Diversity) 
Pedestrian (212/31) 
Vehicular (21/3) 
Vehicular (Delay Diversity) 
Vehicular (212/31) 
Indoor (2103/132)
Pedestrian (2103/132)
Vehicular (2103/132)
Figure 7.22
Performance of a 4-state, 16-state and 64-state ring-TCM code on urban MIMO fading
channels.
4-state and 16-state ST-RTCM codes on the pedestrian MIMO channel also perform
poorly with an error ﬂoor, but the 64-state ST-RTCM code performs well with no
error ﬂoor at a BER = 10−6. The best performance is on the indoor MIMO channel,
as expected, with the 2103/132 ST-RTCM code performing very well with a BER =
10−6 at a SNR of 6 dB.
7.2.4 Ideal CSI and Slow Fading
It is assumed here that the elements hij of matrix H are time-independent and the
receiver has full knowledge of them, so that there is no need to carry out any sort
of estimation process within the receiver. The probability of having received the
sequence R at the receiver when the code matrix S has been transmitted through
a fading channel H follows a multidimensional Gaussian distribution and shall be
described by the pdf:
p(R|S, H) =
l
t=1
m

j=1
1
√2π N0
exp

−
|r j(t) −
n
i=1
hi jsi(t)|2
2N0

.
(7.43)
The above is due to the fact that, at a given symbol interval, the m-dimensional
noise array has a multivariate Gaussian complex distribution with zero mean and

268
NON-BINARY CONVOLUTIONAL CODES
covariance matrix 
η equal to N0 · Im, where Im is the m-order identity matrix. So,
if we consider the random vector  η(t) =
η1(t0) η2(t0) · · · ηm(t0) 
as a column of the
noise matrix N for a given time instant t0, for example, with 1 ≤t0 ≤l, then the joint
pdf of the Gaussian random variables η j(t0) is deﬁned as [15]:
p

 η(t0)
 
=
1
(2π)m/2!
det 
η
exp
"
−1
2( r −H ·  s) −1
η ( r −H ·  s)
#
,
(7.44)
where  r and  s are the column vectors at time t0 of matrices R and S respectively,
and ( x) denotes the transpose conjugate of vector  x. As the columns of the random
noise matrix N are statistically independent, we will have that the overall probability
p(R|S, H), which may be expressed as:
p(R|S, H) =
l
t0=1
p

 η(t0)
 
.
(7.45)
When combined with 7.44, and after some mathematical processing, this will yield
expression given in (7.43).
7.2.5 Log-Likelihood Function
The next step is to deﬁne the likelihood function that shall be used in the maximum
likelihood sequence decoding of ST codes. It is desirable that this function, f, has the
following additive property: the likelihood of the complete sequence is the sum of the
partial symbol likelihoods; that is to say [15]:
f =
l
t=1
fn,
(7.46)
where fn represents an individual instant likelihood. Having this in mind, the most
suitable function is the natural logarithm of p(R|S, H), namely the log-likelihood
function, which is obtained by taking the logarithm of (7.43):
ln[p(R|S, H)] =
l
t=1
m

j=1

ln

1
√2π N0

−
r j(t) −
n
i=1
hi jsi(t)

2
2N0


= −ml
2 ln(2π N0) −
l
t=1
m

j=1
r j(t) −
n
i=1
hi jsi(t)

2
2N0
,
(7.47)

SPACE-TIME CODING MODULATION
269
with the ﬁrst term being a constant, and the second dependent on the received signal
sequence, R, the transmitted code matrix, S, and the channel matrix, H. This function
is of vital important for decoding ST codes.
7.2.6 Maximum-Likelihood Sequence Detection
The maximum-likelihood (ML) sequence detection in ST codes is performed by
maximizing the above log-likelihood function, that is the task is to ﬁnd the sequence
ˆsi(t) that maximizes ln[p(R|S, H)]:
ˆsi(t) = arg max
si
ln[p(R|S, H)].
(7.48)
The ﬁrst term of (7.47) is constant, so it is rendered irrelevant in the maximization
process. Furthermore, the denominator of the second term is also unnecessary. The
maximization of (7.48) turns into the following minimization problem:
ˆsi(t) = arg min
si
l
t=1
m

j=1
r j(t) −
n

i=1
hi jsi(t)

2
,
(7.49)
which can be easily achieved with the Viterbi algorithm, using the following metric
to compute the ML path through the trellis:
m(r j(t), si(t)|hi j) =
l
t=1
m

j=1
r j(t) −
n

i=1
hi jsi(t)

2
,
(7.50)
with 1 ≤i ≤n, 1 ≤j ≤m and 1 ≤t ≤l.
7.2.7 ST Codes Pairwise Error Probability
Consider that the code word s shown in (7.43) is initially transmitted but the decoder
decodes erroneously in favour of the following decoded sequence e:
e = e1(1)e2(1) . . . en(1)e1(2)e2(2) . . . en(2) . . . e1(l)e2(l) . . . en(l).
(7.51)
The probability of making such an erroneous decision, namely the pairwise error
probability (PEP), is given by [7]:
P(s →e|hi j) = P{m(r j(t), ei(t)|hi j) ≤m(r j(t), si(t)|hi j)},
(7.52)
since the metric calculated for the erroneous sequence along the trellis must be less
than the metric worked out for the correct sequence in order to make the incorrect
decision.

270
NON-BINARY CONVOLUTIONAL CODES
By applying a Chernoff bound [21] to the above expression, it turns out that the
PEP is bounded as follows [7]:
P(s →e|hi j) ≤E{exp(µ[m(r j(t), si(t)|hi j) −m(r j(t), ei(t)|hi j)])},
(7.53)
where E{·} represents the expectation and µ is a nonnegative parameter that must be
optimized in order to produce the tightest upper bound possible.
Replacing (7.50) into the above expression will yield:
P(s →e|hi j) ≤
l
t=1
m

j=1
E{exp(µ|r j(t) −
n

i=1
hi jsi(t)|2 −µ|r j(t) −
n

i=1
hi jei(t)|2)}.
(7.54)
After lengthy mathematical simpliﬁcations, which are presented in full detail in [7]
for the SISO channel (the only difference in this case is the different metric being
used), it is possible to reduce inequality (7.54) to an upper bound dependent on the
signal-to-noise ratio Es/N0 and the squared Euclidean distance between sequences s
and e only [15]:
P(s →e|hi j) ≤exp

−d2(s, e) Es
4N0

,
(7.55)
where d2(s, e) will from now on be called the space-time squared Euclidean distance
(ST-SED) between code words s and e and is given by [15]:
d2(s, e) =
m

j=1
l
t=1

n

i=1
hi j(si(t) −ei(t))

2
.
(7.56)
By deﬁning v j = (h1 j, h2 j, . . . , hnj) as the vector of associated channel gains of
receive antenna j, (7.56) can be rewritten as follows:
d2(s, e) =
m

j=1
n

i=1
n

˜i=1
hi jh˜i j
l
t=1
(si(t) −ei(t))(s˜i(t) −e˜i(t)),
(7.57)
where x indicates the conjugate of complex number x. After some manipulation, the
distance turns out to be:
d2(s, e) =
m

j=1
v j A(s, e)v
j,
(7.58)

SPACE-TIME CODING MODULATION
271
where v j is the transpose conjugate of complex matrix vj, the matrix A(s, e) has
dimension n × n and the element at row p and column q is given by:
Apq =
l
t=1
(sp(t) −ep(t)(sq(t) −eq(t)).
(7.59)
We have from (7.55):
P(s →e|hi j) ≤
m

j=1
exp

−v j A(s, e)v
j
Es
4N0

.
(7.60)
Matrix A has some linear algebra properties [22] that are extremely helpful in this
context and will allow a mathematical interpretation of this expression for the PEP.
First, A is a Hermitian matrix because it is equal to its transpose conjugate matrix,
that is to say:
A = A.
(7.61)
Furthermore, there is a unitary matrix U whose rows are eigenvectors of A and
constitute an orthonormal basis of Cn, the n-dimensional set of complex numbers.
There also exists a nondiagonal matrix, D, whose elements λi(i = 1, . . . , n) are
eigenvalues of A, counting multiples, and which veriﬁes the following equality test:
UA(s, e)U  = D.
(7.62)
At last, the matrix A can be decomposed in the following form:
A(s, e) = B(s, e)B(s, e),
(7.63)
where matrix B(s,e) has dimension nxl, and is made by the symbol difference between
the transmitted sequence s and the received code word e, as follows:
B(s, e) =


e1(1) −s1(1)
e1(2) −s1(2)
· · ·
· · ·
e1(l) −s1(l)
e2(1) −s2(1)
e2(2) −s2(2)
· · ·
· · ·
e2(l) −s2(l)
e3(1) −s3(1)
e3(2) −s3(2)
· · ·
· · ·
e3(l) −s3(l)
...
...
...
...
...
en(1) −sn(1)
en(2) −sn(2)
· · ·
· · ·
en(l) −sn(l)


= E −S.
(7.64)
B is called the square root matrix of A. Another important linear algebra property
is that any matrix with a square root is always nonnegative-deﬁnite, so in this context
matrix A will verify the inequality:
x Ax ≥0,
∀x ∈Cn.
(7.65)

272
NON-BINARY CONVOLUTIONAL CODES
It also follows that the eigenvalues of a nonnegative deﬁnite Hermitian matrix are
nonnegative, therefore resulting in A’s eigenvalues λi being all nonnegative.
It is easy to see that the vectors vj as well as eigenvalues λi are time-dependant.
Now let’s deﬁne the vector β j = (µ1 j, µ2 j, . . . , µnj) = v jU . We can see that µij are
also independent complex Gaussian random variables of variance 0.5 per dimension.
Therefore, using (7.62):
v j A(s, v)v
j = v j(U DU)v
j = β j Dβ
j
=
µi j µ2 j · · · µnj



λ1
0
· · ·
0
0
λ2
· · ·
0
...
...
...
...
0
0
· · ·
λn




¯µ1 j
¯µ2 j
...
¯µnj


=
n

i=1
λi
µi j
2 .
(7.66)
Thus the probability of error is:
P(s →e|hi j) ≤
m

j=1
exp

−Es
4N0
n

i=1
λi
µi j
2

.
(7.67)
We now examine two different cases:
1. If the random variables µij have nonzero mean, then their modulus
µi j
 follows a
Rician distribution and, by averaging inequality (7.67) with respect to these random
variables µij, we get the following upper bound on the probability of error:
P(s →e|hi j) ≤
m

j=1


n
i=1
1
1 + Es
4N0
λi
exp


Ki j
Es
4N0
λi
1 + Es
4N0
λi
−Es
4N0
n

i=1
λi
µi j
2



,
(7.68)
where Kij is the Rician factor of
µi j
. When taking the mean E (·) of inequality
(7.67), it is necessary to work out the ‘moment-generating function’ of the random
variable
µi j
2 which has a noncentral chi-square distribution.
2. If the random variables hij have zero mean then the random variables µij also have
zero mean. This is the case of independent Rayleigh fading and Ki j = 0 for all i
and j. Hence, by replacing in 7.68, we now get:
P(s →e|hi j) ≤


n
i=1
1
1 + Es
4N0
λi


m
.
(7.69)

SPACE-TIME CODING MODULATION
273
Let ρ be the rank of the matrix A; it follows that exactly n −ρ eigenvalues of A are
zero. Considering that the nonzero eigenvalues of A are λ1, λ2, . . . , λρ and assuming
the signal-to-noise ratio Es
&
N0 is high:
P(s →e|hi j) ≤

i=1
λi
−m  Es
N0
−ρm
.
(7.70)
Let us deﬁne the diversity advantage of an ST code as the power of the SNR in the
denominator of the expression given for the PEP bound. So a diversity advantage of
ρm is achieved in the above bound. Let us also deﬁne the coding gain of an ST code
as the approximate measure of the gain over an uncoded system that bears the same
diversity advantage. We can see that a coding gain of (λ1λ2 . . . λρ)1/ρ is obtained by
the multi-antenna system.
For the ﬁrst case where Rician fading is present, the upper bound for the pairwise
error probability is very similar to the above, except for a correction factor in the
coding gain that depends on Kij. The same diversity advantage is achieved. So, for
high SNR values in Rician channels, 7.68 is now reduced to:
P(s →e|hi j) ≤

i=1
λi
−m  Es
N0
−ρm


m

j=1
ρ

i=1
e−Ki j

.
(7.71)
Both matrices A and B have equal rank ρ, so there is no need to calculate matrix A;
it is enough to know the code word-difference matrix B in order to get ρ and hence
the diversity advantage.
7.2.8 ST Code Design Criteria for Slow Fading Channels
The code design criteria for slow Rayleigh fading channels can be summarized as
follows:
 Rank Criterion: In order to achieve the maximum diversity advantage nm, the matrix
B(s, e) must have full rank for any code words s and e. This will guarantee a tighter
upper bound on the PEP of inequality (7.70).
 Product Criterion: If the system has a diversity advantage of ρm, the minimum of
the ρth roots of the product of the product of the nonzero eigenvalues of A(s, e)
taken over all pairs of different code words s and e must be maximized. That is to
say, the target is to maximize the coding gain G, deﬁned as:
G = min
∀s,e
s=e

ρ
'
(
(
)
ρ

i=1
λi

.
(7.72)

274
NON-BINARY CONVOLUTIONAL CODES
These criteria are valid only when full channel state information is available at the
decoder. They will assure the tightest upper bound on the PEP possible. The product
criterion is often called the determinant criterion.
In slow Rician fading channels the criteria are the same, with the only difference
being that the coding advantage to be maximized according to the product criterion
is (from (7.70)):
G = min
∀s,e
s=e

ρ
'
(
(
)
ρ

i=1
λi




m

j=1
ρ

j=1
e−Ki j


1/ρm
.
(7.73)
These criteria are only intended to be a guideline for ST code design. They are
not laws, are based only on upper bounds, and tighter bounds may be found later
on. Besides, the criteria work on the basis of making the bound as tight as possible,
which only happens when a given transmitted sequence s is confused with a decoded
sequence e; it could be that this occurrence seldom happens in practice and it might
be necessary to study the PEP bounds of sequence pairs (s, e) that are actually more
likely to be produced.
7.2.9 ST Code Design Criteria for Fast Fading Channels
The code design criteria for time-varying Rayleigh fading channels are similar to
those shown for slow fading and are described as follows:
 Rank Criterion: Identical to criterion for slow Rayleigh fading channels.
Product Criterion: Slightly changes in the case of rapid fading. Here, in order to
achieve the highest coding gain possible, the minimum of the products:

t∈
(s,e)
 n

i=1
|ei(t) −si(t)|2

,
1 ≤t ≤l
taken over all distinct code words s and e must be maximized, where 
 (s, e) is the
set of time instances in which the code words s and e differ in at least one symbol.
If the system has a diversity advantage of ρm, the coding gain G is:
G = min
∀s,e
s=e

t∈
(s,e)
 n

i=1
|ei(t) −si(t)|2
1/ρ
.
(7.74)
7.2.10 Space-Time Ring-TCM Codes for QAM in Fading Channels
Space-Time Coded Modulation (STCM) has been implemented with a wide variety of
modulation schemes. Most technical journals/books available nowadays only describe

SPACE-TIME CODING MODULATION
275
the case where binary STCM uses phase shift keying (PSK) modulation, especially
QPSK and 8-PSK. In this book we consider non-binary coding (ring-codes) combined
with quadrature amplitude modulation. We describe techniques to search for good ST
codes for 16-QAM for slow fading channels. The search was developed according to
a set of extended designed rules proposed in [7]. We assume QAM modulation with
sizes of 16 and 64 waveforms. Naguib et al. [23] employed the signal mapping shown
in Figure 7.13a for 16-QAM.
However, there are other mappings that yield increased Euclidean distances for ST
codes, as they provide higher values for the minimum determinant of the differential
code matrix taken over all pairs of possible code words. This improved 16-QAM
signal mapping is shown in Figure 7.13b and was originally proposed by Carrasco
and Farrell [13]. Figure 7.13c presents their proposed mapping for 64-QAM. The
improved mappings were designed by taking into account the isomorphism produced
by Z-numbers when considering phase rotations.
In Figure 7.13 every signal point is composed of a pair of Z-symbols, where the
leftmost symbol is the same for the four constituent signals of a given quadrant.
Within all quadrants the rightmost Z-number follows the same isomorphism. The
count goes up in one unit clockwise, starting at the upper-left corner point. The design
criteria for ST codes have been extended to reﬁne computer code search and make
it more accurate. This complementary set of rules takes into account the calculation
of other parameters that are relevant in ST code performance. These parameters are
the average number of competing paths with minimum determinant min associated
with a given path in the trellis ND, and the average determinant ¯ for all pairs of code
words. The design rules now have two new extra criteria. The four rules are presented
in hierarchical structure and are practical for use in code searches:
1. The minimum rank must be maximized. Ideally the code must have full rank n,
which is the number of transmitters. ST codes that do not possess full rank are
therefore discarded.
2. The codes selected are the ones that provide the highest minimum determinant
min of the matrix A(s, e) taken over all pairs of distinct code words, where the
matrix A(s, e) has dimension n × n and contains the symbol differences between
the transmitted sequence s and the received code e. The element at row p and
column q is given by [21]:
Apq =
l
i=1
(sp(t) −ep(t)) (sq(t) −eq(t)),
(7.75)
where (•) represents complex conjugation. A hierarchical code list can thus be
made, ordering them according to min values.
3. In the case that two or more codes have the same min, the best code is the one
with the smallest N, so codes are prioritized based on N.

276
NON-BINARY CONVOLUTIONAL CODES
Table 7.6
Optimum ST-RTCM codes of modulo-16 for 16-QAM.
Code
States
min
N

8 9/8
16
4
1.625
61.3
0 9/0
16
4
1.625
27.2
0 9/8
16
4
1.625
27.2
8 9/0
16
4
1.625
27.2
8 3/8
16
1
0.094
62.4
7 12/3
16
1
0.316
51.7
Tarokh et al.
16
1
1.25
27.2
4. If any two good codes still have identical min and N, the one with the highest
average  is ﬁnally chosen.
An exhaustive a priori search was conducted among all 16-state ST-RTCM codes
to ﬁnd those that best satisfy these rules. Table 7.6 shows the best codes found with
their respective min, N and ; they are ordered from best at the top of the table to
worst at the bottom. All codes shown are of full rank, therefore yielding the maximum
diversity advantage nm, equal to four. All codes are written in the form g1g2/f1, where
g1 and g2 are the feed-forward coefﬁcients and f 1 is the feedback coefﬁcient.
Note that codes 0 9/0, 0 9/8 and 8 9/0 have the same parameter values and must
have identical performance as well. This is an indication of the existence of multiple
optimum codes. They are the best codes, together with 8 9/8, because their minimum
determinant is four. No other codes can achieve this, all having a minimum determinant
of one. In spite of it not being an optimum code from the determinant criterion point
of view, the code 7 12/3 has been included for performance comparison with Naguib
et al. [23] and optimum ST-RTCM codes. It has a smaller N than Naguib’s code and
a higher average determinant.
Figure 7.23 presents the simulation results for Naguib et al.’s code and the ST-
RTCM 8 9/8 and 7 12/3 codes for values of m equal to 2 and 4.The uncoded single-
channel 16-QAM curve is also indicated. For two receive antennas and a BER of 10−3,
the 8 9/8 code has a coding gain of 1.8 dB in relation to Naguib et al.’s, while keeping a
gain of approximately 1.2 dB when compared with the 7 12/3 code. With four receive
antennas, the 8 9/8 now has a coding gain of 4 dB in relation to Naguib’s at 10−4
BER. This proves that the higher min is for a particular code, the higher the coding
advantage will be when m increases. Another conclusion drawn from the graph is that
7 12/3 performance is better than Naguib et al.’s, accounting for a 3.3 dB margin at
SNRs greater than 20 dB in a system with two transmitters and two receivers.
Both codes have full rank and minimum determinant, however 7 12/3 performs
better due to its smaller ND and bigger ¯, originating from the different code-to-
signal mapping used in the modulation of ST-RTCM codes. The ST-RTCM 8 9/8 is
the one with the best performance as it fully complies with the extended criteria: it
has the maximum min of four and higher determinant average than the other three

SPACE-TIME CODING MODULATION
277
100
0
4
8
12
16
20
24
SNR, dB
Uncoded,n=1, m=1
Tarokh’s,n=2, m=2
Tarokh’s, n=2, m=4
7 12 / 3, n=2, m=2
8 9 / 8, n=2, m=2
8 9 / 8, n=2, m=4
10−2
10−4
10−6
BER
Figure 7.23
Error performance of ST-RTCM codes with 16-QAM over slow Rayleigh fading channels,
130 symbols per frame, 4 bits/s/Hz.
codes that also present the highest min. When more receiving antennas are employed,
Figure 7.23 shows a lower probability of error for a given high SNR value due to the
increased diversity. Therefore the respective curve sinks sooner, with a pronounced
slope.
Figure 7.24 shows that the ST-RTCM code 213 132/3 with n = m = 2 performs
on the Gaussian bound until an SNR of 18 dB. It outperforms code 25 12/47 by a
Figure 7.24
BER performance of ST-RTCM codes with QAM over slow Rayleigh fading channels, 65
symbols per frame, 6 bits/Hz, 25 12/47, 64-state, 64-QAM, 213 132/3, 4-state, 16-QAM.

278
NON-BINARY CONVOLUTIONAL CODES
Table 7.7
Comparison of ST codes with 16-QAM, 4 bit/s/Hz.
Code
SNR (dB)
Delay diversity, m = 2
26
ST-RTCM 7 12/3, m = 2
23.6
ST-RTCM 8 9/8, m = 2
21.4
Delay diversity, m = 4
17.2
ST-RTCM 8 9/8, m = 4
13.2
margin of 7 dB. This is due to the high coding gain provided by the former, being an
optimized code; the latter is not an optimum code. The 4-state code is not only superior
in performance but also has a lower complexity than the 64-state code. There must
be 64-QAM ST-RTCM codes of better performance than 25 12/47, but the computer
search is quite long due to the large number of different codes and is not of practical
interest since these 64-state codes are much more complex than 213 132/3.
Finally, 213 132/3 performance is also presented in a four-receiver conﬁguration,
which is even better due to higher diversity. It is 11.5 dB better off than the same code
used with m = 2.
Table 7.7 gives a brief summary of the simulation results obtained for ST-ring
TCM codes with QAM modulation over slow Rayleigh fading channels. Table 7.7
compares different ST codes that have a bandwidth efﬁciency of 4 bit/s/Hz, providing
SNR values that reach a BER of 10−4 in 16-QAM simulations.
7.3 Conclusions
In this chapter we have introduced the concepts of convolutional coding and trellis
coded modulation. We have shown how TCM codes are not only bandwidth efﬁcient
but also achieve good performance. The idea of TCM has been extended for symbols
deﬁned over rings of integers, which outperform binary TCM codes with only a small
increase in decoding complexity. The criteria for constructing good ring-TCM codes
have been given and a genetic algorithm has been presented as a search method for
ﬁnding good codes. We have shown how Ring-TCM codes are suitable for combining
with spatial-temporal diversity, resulting in space-time ring-TCM codes. The perfor-
mance of ST-RTCM codes were evaluated with simulation results over urban mobile
radio channels, and these performed well in the slow fading indoor and pedestrian
environments. Finally, the design criteria were presented for constructing good ST-
RTCM codes and the performance of some new ST-RTCM codes were evaluated with
simulation results for QAM on slow fading channels.
References
[1] Elias, P. (1955) Coding for Noise Channels, IRE Convention Report, Part 4, pp. 37–47.
[2] Moon, T.K. (2005) Error Correction Coding. Mathematical Methods and Algorithms, Wiley Inter-
science, ISBN 0-471-64800-0.

REFERENCES
279
[3] Viterbi, A.J. (1967) Error bounds for convolutional codes and asymptotically optimum decoding
algorithm. IEEE Transactions on Information Theory, 13, 260–9.
[4] Forney, G.D. (1973) The Viterbi algorithm. Proceedings of IEEE, 61, 268–78.
[5] Massey, J.L. (1974) Coding and modulation in digital communications. Presented at International
Zurich Seminar on Digital Communications, Zurich, Switzerland.
[6] Ungerboeck, G. (1982) Channel coding with multilevel/phase signals. Information Theory, IEEE
Transactions, 28, 55–67.
[7] Biglieri, E., Divisar, D., Melane, P.J. and Simon, M.K. (1991) Introduction to Trellis Coded Modu-
lation with Application, MacMillan, New York.
[8] Massey, J.L. and Mittelholzer, T. (1984) Codes over rings – a practical necessity. Presented at
AAECC7 International Conference, Universite’ P. Sabatier, Toulouse, France.
[9] Massey, J. and Mittelholzer, T. (1989) Convolutional codes over rings. Proceedings of the 4th Joint
Swedish Soviet Workshop Information Theory, pp. 14–8.
[10] Baldini, R.F. and Farrell, P.G. (1990) Coded modulation wit convolutional codes over rings.
Presented at Second IEE Bangor Symposium on Communications, Bangor, Wales.
[11] Baldini, R.F. and Farrell, P. (1994) Coded modulation based on rings of integers modulo-q part 2:
convolutional codes. IEE Proceedings: Communications, 141, 137–42.
[12] Carrasco, R., Lopez, F. and Farrell, P. (1996) Ring-TCM for M-PSK modulation: AWGN channels
and DSP implementation. Communications, IEE Proceedings, 143, 273–80.
[13] Carrasco, R. and Farrell, P. (1996) Ring-TCM for ﬁxed and fading channels: land-mobile satellite
fading channels with QAM. Communications, IEE Proceedings, 143, 281–8.
[14] Lopez, F.J., Carrasco, R.A. and Farrell, P.G. (1992) Ring-TCM Codes over QAM. IEE Electronics
Letter, 28, 2358–9.
[15] Tarokh, V., Seshadri, N. and Calderbank, A. (1998) Space-time codes for high data rate wireless
communication: performance criterion and code construction. Information Theory, IEEE Transac-
tions on, 44, 744–65.
[16] Benedetto, S., Mondin, M. and Montorsi, G. (1994) Performance evaluation of trellis-coded mod-
ulation schemes. Proceedings of the IEEE, 82, 833–55.
[17] Goldberg, D. (1989) Genetic Algorithms in Search, Optimization and Machine Learning, Addison-
Wesley Longman Publishing Co., Inc., Boston, MA, USA.
[18] Soto, I. and Carrasco, A. (1997) Searching for TCM codes using genetic algorithms. Communica-
tions, IEE Proceedings, 144, 6–10.
[19] Pereira, A. and Carrasco, R. (2001) Space-time ring TCM codes for QPSK on time-varying fast
fading channels. Electronics Letters, 37, 961–2.
[20] Carrasco, R. and Pereira, A. (2004) Space-time ring-TCM codes for QAM over fading channels.
IEE Proceedings Communications, 151 (4), 316–21.
[21] Pereira, A. (2003) Space-Time Ring TCM Codes on Fading Channels. PhD Thesis, Staffordshire
University, Staffordshire, UK.
[22] Horn, R. and Johnson, C. (1990) Matrix Analysis, Cambridge University Press, Cambridge, UK,
ISBN 0-521-38632-2.
[23] Naguib, A., Tarokh, V., Seshadri, N. and Calderbank, A. (1998) A space-time coding modem for
high-data-rate wireless communications. Selected Areas in Communications, IEEE Journal, 16,
1459–78.


8
Non-Binary Turbo Codes
8.1 Introduction
One of the most important breakthroughs in coding theory was the development
of turbo codes by Berrou, Glavieux and Thitimajshima [1] in 1993. Turbo codes
are a parallel concatenation of recursive systematic convolutional codes separated
by an interleaver. They provide a practical way of achieving near-Shannon limit
performance by using an iterative decoder that contains two soft-input–soft-output
component decoders in series, passing reliability information between them. Origi-
nally, the component decoders used the BCJR algorithm [2, 3] or MAP algorithm,
but this was considered too complex for practical applications. A reduction in the
complexity of the MAP algorithm with only a small degradation in performance was
achieved by working in the logarithmic domain, resulting in the log MAP algorithm.
Simpler trellis decoding algorithms also include the max-log MAP algorithm and the
Soft Output Viterbi Algorithm (SOVA). Both algorithms perform the same as one an-
other, but do not perform as well as the log MAP algorithm. Turbo codes are used for
error-correction in 3G mobile communications and are now also used for deep-space
communications.
In this chapter, the binary turbo encoding and decoding processes are introduced,
with detailed descriptions of log-likelihood ratios, the MAP algorithm and the iterative
log MAP algorithm. This is followed by an introduction to non-binary turbo coding,
which is an area of research that has received very little attention, with only a few
papers published. Finally, we complete this chapter with an explanation of the non-
binary turbo encoder and decoder structure.
8.2 The Turbo Encoder
A turbo code is a parallel concatenation of two recursive systematic convolutional
(RSC) codes separated by an interleaver, denoted by . A general turbo encoder is
shown in Figure 8.1.
Non-Binary Error Control Coding for Wireless Communication and Data Storage
Rolando Antonio Carrasco and Martin Johnston
C 2008 John Wiley & Sons, Ltd

282
NON-BINARY TURBO CODES
Interleaver
Π
RSC
Encoder 1
RSC
Encoder 2
Puncturing
(0)
tv
(2)
tv
(1)
tv
Encoder
Output
Message Bits xt
Π(x )
t
Figure 8.1
Turbo encoder.
The two encoders have code rates R1 and R2 respectively, and the overall code rate
R of the turbo encoder is given by:
R =
R1R2
R1 + R2 −R1R2
.
(8.1)
Usually the code rates of both encoders are the same and a parallel concatenation
of codes reduces the overall code rate. It is common to choose an RSC with code rate
1/2 as the component encoders result in an overall code rate of 1/3. We are not limited
to convolutional codes but could also choose block codes as the component encoders,
known as block turbo codes [2].
The upper encoder receives the data directly, while the lower encoder receives
the data after it has been interleaved by a permutation function, . The interleaver
is in general a pseudo random interleaver, that is it rearranges bits according to a
prescribed but randomly-generated rule. The interleaver operates in a block-wise
fashion, interleaving L bits at a time, and thus turbo codes can be viewed as block
codes. Since both encoders are systematic, the systematic output of the top encoder is
sent, while the systematic output of the lower encoder is not sent. However, the parity
bits of both encoders are transmitted. The overall code rate can be made higher by
puncturing, which operates only on the parity sequences – the systematic bits are not
punctured. The code rate of a rate 1/3 turbo code is typically increased to 1/2 by only
transmitting the odd indexed parity bits from the upper encoder and the even indexed
parity bits from the lower encoder.
8.2.1 Notation
In this chapter, we use the following notation for the turbo encoding and decoding
processes:
 v(0)
t
∈{0, 1} is a turbo encoder output corresponding to an information bit at time t.
 v(1)
t
∈{0, 1} is a parity bit from the ﬁrst component RSC encoder.

THE TURBO ENCODER
283
 v(2)
t
∈{0, 1} is a parity bit from the second component RSC encoder.
 a(i)
t
∈{−1, +1}, i = 0, 1, 2, is the mapping of v(i)
t
to the BPSK constellation, where
a(i)
t
= 2v(i)
t
−1.
 v(0,s,s) ∈{0, 1} is the information bit corresponding to a transition from an initial
state s to the next state s.
 v(1,s,s) ∈{0, 1} is the parity bit from the ﬁrst RSC encoder corresponding to a
transition from state s to state s.
 v(2,s,s) ∈{0, 1} is the parity bit from the second RSC encoder corresponding to a
transition from state s to state s.
 a(i,s,s) ∈{−1, +1} is equal to 2v(i,s,s) −1, i = 0, 1, 2.
 r(0)
t
is the received information bit, where r(0)
t
= a(0)
t
+ ηt and ηt is an additive white
Gaussian noise sample at time t.
 r(1)
t
is the received parity bit from the ﬁrst RSC encoder, where r(1)
t
= a(1)
t
+ ηt.
 r(2)
t
is the received information bit from the second RSC encoder, where r(2)
t
= a(2)
t
+ ηt.
Example 8.1: Binary turbo encoding: Figure 8.2 shows a turbo encoder with
(1, 5/7) RSC component codes with a pseudo random interleaver of length N =
5 bits. Therefore, the turbo encoder receives a 5 bit message xt, which is encoded
by the ﬁrst component encoder to obtain a 5 bit parity output v(1)
t . The message is
also interleaved (xt) and encoded by the second component encoder to obtain the
5 bit parity output v(2)
t . The turbo encoder output is therefore 15 bits long.
Let the message be:
xt = [x1x2, x3, x4, x5] = [1, 0, 1, 0, 1].
The state table for the ﬁrst component RSC encoder is given in Table 8.1.
The input sequence runs through the ﬁrst RSC encoder with transfer function
G(D), resulting in a parity sequence:
v(1)
t
=
%
v(1)
1 , v(1)
2 , v(1)
3 , v(1)
4 , v(1)
5
&
= [1, 1, 0, 1, 1].
Π
{3, 5, 1, 4, 2}
No Puncturing
)
0
(
tv
)
2
(
tv
)
1
(
tv
Encoder
Output
Message Bits xt
Π(xt)
Figure 8.2
Turbo encoder with (1, 5/7) RSC component codes.

284
NON-BINARY TURBO CODES
Table 8.1
State table for the ﬁrst component RSC encoder
xt
s
s
v(0,s,s)
t
v(1,s,s)
t
a(0,s,s)
t
a(1,s,s)
t
0
00
00
0
0
−1
−1
1
00
10
1
1
+1
+1
0
01
10
0
1
−1
+1
1
01
00
1
0
+1
−1
0
10
11
0
1
−1
+1
1
10
01
1
0
+1
−1
0
11
01
0
0
−1
−1
1
11
11
1
1
+1
+1
The sequence x is also passed through an interleaver or permuter of length
N = 5, denoted by , which produces the permuted output sequence x = (x) =
[1, 1, 1, 0, 0]. The sequence x is passed through another convolutional encoder
with transfer function G(D), which produces the output sequence:
v(2)
t
=
%
v(2)
1 , v(2)
2 , v(2)
3 , v(2)
4 , v(2)
5
&
= [1, 0, 1, 1, 1].
The three output sequences are multiplexed together to form the output sequence:
vt =
%
v(0)
1 , v(1)
1 , v(2)
1

,

v(0)
2 , v(1)
2 , v(2)
2

, . . . ,

v(1)
5 x, v(1)
5 , v(2)
5
&
= [(1, 1, 1), (0, 1, 0), (1, 0, 1), (0, 1, 1), (1, 1, 1)]
.
Assuming BPSK modulation with Es = 1, the mapped output at will be:
at = [(+1, +1, +1), (−1, +1, −1), (+1, −1, +1), (−1, +1, +1), (+1, +1, +1)] .
8.3 The Turbo Decoder
The near-Shannon limit performance of turbo codes is accomplished by their itera-
tive decoding algorithm. A general block diagram of the turbo decoder is shown in
Figure 8.3. The idea of the turbo decoding process is to extract extrinsic information
from the output of one decoder and pass it on to the second decoder in order to improve
the reliability of the second decoder’s output. Extrinsic information is then extracted
from the second decoder and passed on to the ﬁrst decoder to improve the reliability
of ﬁrst decoder’s output. This process is then repeated until no further gains in the
performance of the turbo decoder can be achieved.
The turbo decoder takes as its input the reliability values of the systematic in-
formation, r(0)
t , the parity bits from encoder 1, r(1)
t , and the interleaved parity bits
from encoder 2, r(2)
t . Turbo decoding is a two-stage process consisting of two soft-
input–soft-output decoders in series. The ﬁrst decoder takes r(0)
t , r(1)
t
and prior in-
formation from decoder 2, which initially is zero. The output of decoder 1 is the
reliability of the systematic information. The prior information from decoder 2 and

THE TURBO DECODER
285
Decoder
1
Interleaver
Π
Decoder
2
Deinterleaver
Π-1
Interleaver
Π
Deinterleaver
Π-1
Σ
Σ
)
0
(
tr
)
2
(
tr
)1
(
tr
+
−
−
−
−
+
Extrinsic Information
A Priori Information
A Priori Information
Extrinsic Information
Decoded Output
Figure 8.3
Turbo decoder.
the reliability values of the systematic information are subtracted from the decoder
output, leaving extrinsic information, which is interleaved and becomes the prior
information for decoder 2.
Decoder 2 now takes the interleaved systematic information, the reliability values
of the interleaved parity bits r(2)
t
and the prior information from decoder 1. The output
is the reliability of the interleaved systematic information. The prior information and
the reliability values of the interleaved systematic information are subtracted from
the decoder output, resulting in extrinsic information, which is now deinterleaved
to become once again the prior information for decoder 1. This is called a decoder
iteration, where after each information the prior information to both decoders becomes
more reliable, thus improving the reliabilities of the decoder outputs. After each
iteration, the improvement in performance becomes less until it converges and no
further improvements can be achieved. Alternatively, instead of feeding back the
extrinsic information to decoder 1, the output of decoder 2 can be deinterleaved to
give the reliability of r(0)
t . A hard decision can then be made to recover the systematic
information.
8.3.1 Log-Likelihood Ratios (LLRs)
One measure of the reliability of a variable is to calculate its log-likelihood ratio
(LLR). For some variable u, its LLR L(u) is deﬁned as:
L(u) = ln
 P(u = +1)
P(u = −1)

,
(8.2)

286
NON-BINARY TURBO CODES
where P(u = +1) = 1 −P(u = −1). We can see that if P(u = +1) = P(u = −1) = 0.5
then L(u) = 0, that is the reliability of u is zero. As stated previously, the turbo decoder
takes as its inputs the reliabilities of the systematic information and parity bits from
encoders 1 and 2. If we deﬁne the received systematic information bit at time t as r(0)
t
then:
L

r(0)
t
| a(0)
t

= ln

P

r(0)
t
| a(0)
t
= +1

P

r(0)
t
| a(0)
t
= −1


.
(8.3)
For the AWGN channel and BPSK modulation with constellation points at ±√Es:
P

r(0)
t
| a(0)
t
= +1

=
1
σ
√
2π e−

r(0)
t −√Es
2
/2σ 2
1
σ
√
2π e−

r(0)
t +√Es
2
/2σ 2 +
1
σ
√
2π e−

r(0)
t −√Es
2
/2σ 2
P

r(0)
t
| a(0)
t
= −1

=
1
σ
√
2π e−

r(0)
t +√Es
2
/2σ 2
1
σ
√
2π e−

r(0)
t +√Es
2
/2σ 2 +
1
σ
√
2π e−

r(0)
t −√Es
2
/2σ 2 .
Therefore:
ln

P

r(0)
t
| a(0)
t
= +1

P

r(0)
t
| a(0)
t
= −1


= ln

e−

r(0)
t −√Es
2
/2σ 2
e−

r(0)
t +√Es
2
/2σ 2

= ln

e−

r(0)
t −√Es
2
/2σ 2
−ln

e−

r(0)
t +√Es
2
/2σ 2
= −

r(0)
t
−√Es
2
2σ 2
−−

r(0)
t
+ √Es
2
2σ 2
=
1
2σ 2

−

r0
t
2 + 2

Esr(0)
t
−1 +

r(0)
t
2 + 2

Esr(0)
t
+ 1

= 2√Esr(0)
t
σ 2
.
The term 2√Es
σ 2
is called the channel reliability and denoted by Lc. Therefore:
L

r(0)
t
| a(0)
t

= Lcr(0)
t
= 2√Esr(0)
t
σ 2
.
(8.4)
The output of decoder 1 is deﬁned as:
L

a(0)
t
| r

= ln

P

a(0)
t
= +1 | r

P

a(0)
t
= −1 | r


,
(8.5)

THE TURBO DECODER
287
Decoder
1
Interleaver
Π
Decoder
2
Deinterleaver
Π-1
Interleaver
Π
Deinterleaver
Π-1
Σ
Σ
)
0
(
t
cr
L
)
2
(
t
cr
L
)
1
(
t
cr
L
+
−
−
−
−
+
)
|
(
)
0
(
r
ta
L
(
)
)
0
(
1
t
e a
L
(
)
(
)
r
|
)
0
(
ta
L
Π
(
)
)
0
(
t
cr
L
Π
(
)
(
)
)
0
(
2
t
e a
L
Π
(
)
(
)
)
0
(
2
ta
L
Π
(
)
)
0
(
1
ta
L
(
)
r
|
)
0
(
ta
L
Figure 8.4
A more detailed diagram of the turbo decoder.
where r is the received vector. Finally, the prior information into decoder 1 is the
deinterleaved extrinsic information from decoder 2.
L1

a(0)
t

= Le2



a(0)
t

= ln

P

a(0)
t
= +1

P

a(0)
t
= −1


.
(8.6)
Therefore, the extrinsic information from decoder 1 is:
Le1

a(0)
t

= L1

a(0)
t
|r

−L1

a(0)
t

−Lcr(0)
t .
(8.7)
Similarly, the extrinsic information from decoder 2 is:


Le2

a(0)
t

= 

L2

a(0)
t
| r

−

L2

a(0)
t

−

Lcr(0)
t

.
(8.8)
A more detailed block diagram of the turbo decoder showing the different LLRs is
given in Figure 8.4.
When no more iterations are required, a hard decision is made on L(a(0)
t
| r) from
the deinterleaved output of decoder 2, depending on the sign of the LLR.
If
sign

L

a(0)
t
|r

< 0,
a(0)
t
= −1
∴v(0)
t
= 0
If
sign

L

a(0)
t
|r

≥0,
a(0)
t
= +1
∴v(0)
t
= 1.
(8.9)
8.3.2 Maximum A Posteriori (MAP) Decoding
The BCJR or MAP algorithm was originally presented coincidentally in [2] and
[5], and later in [3]. It is a modiﬁed version of this that is used by each of the
component decoders of the MAP turbo decoder. This algorithm is used to perform the

288
NON-BINARY TURBO CODES
symbol-by-symbol MAP decoding referred to earlier. The turbo decoder’s decision is
whether ˆv(0)
t
= 1 or 0.
It is presumed that ˆv(0)
t
= 1 if P(a(0)
t
= +1 | r) > P(a(0)
t
= −1 | r) and that ˆv(0)
t
= 0
if P(a(0)
t
= −1 | r) > P(a(0)
t
= +1 | r).
Put more simply, it can be said that the decision ˆv(0)
t
is given by:
ˆv(0)
t
= sign
%
L

a(0)
t
| r
&
.
(8.10)
The state transitions within the encoder’s trellis are governed by the transitional
probabilities:
P(s, s) = P(st+1 = s | st = s),
0 ≤s, s ≤Ms −1.
(8.11)
The posterior probabilities can be deﬁned as:
P

a(0)
t
= −1 | r

=

(s,s)∈S−
P(st = s, st+1 = s | r)
P

a(0)
t
= +1 | r

=

(s,s)∈S+
P(st = s, st+1 = s | r),
which can be modiﬁed using Bayes’ theorem to:
P

a(0)
t
= −1 | r

=

s−
P(st = s, st+1 = s, r)
P(r)
P

a(0)
t
= +1 | r

=

s+
P(st = s, st+1 = s, r)
P(r)
,
(8.12)
where st ∈S is the state of the encoder at time t, S+ is the set of ordered pairs (s, s)
corresponding to all state transitions (st = s) →(st+1 = s) brought about by the data
input v(0)
t
= 1, and S−is deﬁned in the same way for v(0)
t
= 0. The received noisy
code word is expressed as r = (r1,r2, . . . ,rN). Therefore, (8.5) can be rewritten as:
L

a(0)
t
| r

= ln



(s,s)∈S+ (P(st = s, st+1 = s, r)

(s,s)∈S−−(P(st = s, st+1 = s, r)

.
(8.13)
Consequently, the joint probability:
P(s, s, r) = P(st+1 = s, st = s, r),
0 ≤s, s ≤Ms −1.
(8.14)
(8.13) can then be rewritten in the following form:
P(st = s, st+1 = s, r) = P(st = s, st+1 = s, r<t, rt, r>t).
(8.15)

THE TURBO DECODER
289
Applying Bayes’ theorem, this becomes:
P(st = s, st+1 = s, r<t, rt, r>t) = P(st = s, st+1 = s, r<t, rt)
× P(r>t | st = s, st+1 = s, r<t, rt)
= P(st = s, st+1 = s, r<t, rt)P(r>t | st = s)
= P(st+1 = s, rt | st = s)P(st = s, r<t)
× P(r>t | st = s, st+1 = s, r<t, rt)
.
(8.16)
(8.16) is now expressed in terms of three probabilities:
αt(s) = P(st = s, r<t),
(8.17)
βt+1(s) = P(r>t | st = s),
(8.18)
γt(s, s) = P(st+1 = s, rt | st = s).
(8.19)
This gives:
P(st = s, st+1 = s,r<t,rt,r>t) = αt(s)βt+1(s)γt(s, s).
Therefore (8.12) can be written as:
L

a(0)
t
| r

= ln



(ss)∈S+ αt(s) · βt+1(s) · γt(s, s)

(s,s)∈S−−αt(s) · βt+1(s) · γt(s, s)

.
(8.20)
The values of αt(s), βt+1(s) and γ t(s, s) can be determined by forming recursive
relationships. To determine αt+1(s):
αt+1(s) = P(st+1 = s, r<t+1) = P(st+1 = s, r<t, rt)
=
Ms−1

s=0
P(st+1 = s, rt, st = s, r<t)
=
Ms−1

s=0
P(st = s, r<t)P(st+1 = s, rt | st = s, r<t)
=
Ms−1

s=0
P(st = s, r<t)P(st+1 = s, rt | st = s)
=
Ms−1

s=0
αt(s)γt(s, s)
.
(8.21)

290
NON-BINARY TURBO CODES
To determine βt(s):
βt(s) = P(r>t−1 | st = s) = P(r>t,rt | st = s)
=
Ms−1

s=0
P(r>t, rt, st+1 = s | st = s)
=
Ms−1

s=0
P(rt, st+1 = s | st = s)P(r>t | rt, st+1 = s, st = s)
=
Ms−1

s=0
P(rt, st+1 = s | st = s)P(r>t | st+1 = s)
=
Ms−1

s=0
γt(s, s)βt+1(s)
.
(8.22)
Finally, to determine γ t(s, s):
γt(s, s) = P(st+1 = s, rt | st = s) = P(rt | st = s, st+1 = s)
· P(st = s, st+1 = s),
(8.23)
P(rt | st = s, st+1 = s) =
1
√
2πσ 2 e

−
1
2σ2
rt−√Esa(s,s)
2
,
(8.24)
γt(s, s) = P(st = s, st+1 = s)
1
√
2πσ 2 e

−
1
2σ2
rt−√Esa(s,s)
2
.
(8.25)
We can simplify (8.23) by assuming that if each information bit is equally likely
to be a 0 or a 1 then all states are also equally likely and P(st = s, st+1 = s) is a
constant. Therefore:
γ (s, s) =
1
√
2πσ 2 e

−
1
2σ2
rt−√Esa(s,s)
2
.
(8.26)
8.3.3 Max-Log MAP
The amount of memory required, and the number of operations involving exponential
values and multiplicative procedures, means that the complexity of the MAP algorithm
becomes prohibitive when implementing a turbo decoder. The system can be simpliﬁed
by employing the logarithms of the probabilities deﬁned in (8.16), (8.17) and (8.18)
and thus transforming any multiplicative operations to summations. This gives the
following:
¯αt(s) = ln αt(s),
(8.27)
¯βt(s) = ln βt(s),
(8.28)
¯γt(s, s) = ln γt(s, s).
(8.29)

THE TURBO DECODER
291
This means that, with reference to (8.20), αt+1(s) becomes:
αt+1(s) = ln
Ms−1

s=0
αt(s)γt(s, s)

= ln
Ms−1

s=0
e(¯αt(s)+ ¯γt(s,s))

.
(8.30)
To evaluate (8.28) we can use the following approximation:
ln(eδ1 + eδ2 + · · · + eδn) ≈
max
i=1,2,...,n δi.
(8.31)
Therefore:
αt+1(s) = ln
Ms−1

s=0
e(¯αt(s)+ ¯γt(s,s))

≈max
s

¯αt(s) + ¯γt(s, s)

.
(8.32)
Similarly, for βt(s):
βt(s) = ln
Ms−1

s=0
γt(s, s)βt+1(s)

= ln
Ms−1

s=0
e( ¯γt(s,s)+ ¯βt+1(s)

≈max
s

¯γt(s, s) + ¯βt+1(s)

.
(8.33)
Finally, for γt(s, s):
¯γ (s, s) = ln

1
√
2πσ 2 e

−
1
2σ2
rt−√Esa(s,s)
2
= ln

1
√
2πσ 2

+ ln e

−
1
2σ2
rt−√Esa(s,s)
2
= ln

1
√
2πσ 2

−
1
2σ 2
rt −

Esa(s,s)2.
By neglecting constants we obtain:
¯γt(s, s) = −
rt −

Esa(s,s)2.
(8.34)
Inserting these values into (8.20) yields:
L

a(0)
t
| r

= ln

(s,s)∈S+ eαt(s)+γ t(s,s)+βt+1(s)

(s,s)∈S−eαt(s)+γ t(s,s)+βt+1(s) .
(8.35)
(8.34) can be estimated as:
L

a(0)
t
| r

≈
max
(s,s)∈S+[γ t(s, s) + αt(s) + βt+1(s)]
−max
(s,s)∈S−[γ t(s, s) + αt(s) + βt+1(s)].
(8.36)

292
NON-BINARY TURBO CODES
8.3.4 Log MAP Algorithm
The max-log MAP algorithm approximates the log-likelihood ratio L(a(0)
t
| r), and is
therefore suboptimal. The log MAP algorithm instead uses the Jacobian logarithm [6]
to improve the log-likelihood ratio and therefore improve the decoding abilities of the
algorithm with only a small increase in complexity:
ln(eδ1 + eδ2) = max(δ1, δ2) + ln(1 −e−| δ2−δ1 | ).
(8.37)
8.3.5 Iterative Log MAP Decoding
If we assume that the encoder is systematic with a rate R = 0.5 then the received
vector r at time t can be expressed as rt = (r(0)
t ,r(1)
t ), where r(0)
t
is the information bit
and r(1)
t
is the parity bit.
(8.22) can then be written as [7]:
γt(s, s) = P(rt | st = s, st+1 = s)P(st+1 = s, st = s)
= P

r(0)
t ,r(1)
t
| st = s, st+1 = s

P(st+1 = s, st = s)
= P

r(0)
t ,r(1)
t
| st = s, st+1 = s

P

a(0)
t
= a(0,s,s)
.
(8.38)
Since st+1 is determined by a(0,s,s), P(st+1 = s | st = s) = P(a(0)
t
= a(0,s,s)).
Therefore:
P

r(0)
t ,r(1)
t
| st = s, st+1 = s

= P

r(0)
t ,r(1)
t
| st = s, a(0)
t
= a(0,s,s)
.
(8.39)
Since the encoder is systematic, r(0)
t
is independent of the state transitions and only
dependent on the input:
P

r(0)
t ,r(1)
t
| st = s, a(0)
t
= a(0,s,s)
= P

r(0)
t
| a(0)
t

P

r(1)
t
| st = s, st+1 = s

= P

r(0)
t
| a(0)
t

P

r(1)
t
| a(1)
t
= a(1,s,s)
.
(8.40)
Substituting (8.38) into (8.36) gives:
γt(s, s) = P

r(0)
t ,r(1)
t
| st = s, st+1 = s

P

a(0)
t
= a(0,s,s)
= P

r(0)
t
| v(0)
t

P

r(1)
t
| a(1)
t
= a(1,s,s)
P

a(0)
t
= a(0,s,s),
(8.41)
¯γt(s, s) = ln
%
P

r(0)
t
| a(0)
t

P

r(1)
t
| a(1)
t
= a(1,s,s)
P

a(0)
t
= a(0,s,s)&
= ln P

r(0)
t
| a(0)
t
= a(0,s,s)
+ ln P

r(1)
t
| a(1)
t
= a(1,s,s)
+ ln P

a(0)
t
= a(0,s,s)
.
(8.42)
From (8.5):
L1

a(0)
t

= ln

P

a(0)
t
= +1

P

a(0)
t
= −1


= ln

P

a(0)
t
= +1

1 −P

a(0)
t
= −1


,

THE TURBO DECODER
293
eL1

a(0)
t

=
P

a(0)
t
= +1

1 −P

a(0)
t
= +1
,
(8.43)
P

a(0)
t
= +1

=
eL1

a(0)
t

1 + eL1

a(0)
t
 =
1
1 + e−L1

a(0)
t
 .
(8.44)
Similarly:
L1

v(0)
t

= ln

P

a(0)
t
= +1

P

a(0)
t
= −1


= ln

1 −P

a(0)
t
= −1

P

a(0)
t
= −1


eL1

a(0)
t

= 1 −P

a(0)
t
= −1

P

a(0)
t
= −1

,
(8.45)
P

a(0)
t
= −1

=
1
1 + eL1

a(0)
t
 .
(8.46)
In general:
P

a(0)
t
= a(0,s,s)
=
e
−L1

a(0)
t

2
1 + e
−L1

a(0)
t

2
e
a(0,s,s)L1

a(0)
t

2
.
(8.47)
The term
e
−L1(a(0)
t
)
2
1+e
−L1(a(0)
t
)
2
is independent of a(0,s,s) and can be taken out so we obtain:
ln

P

a(0)
t
= a0,s,s
= a(0,s,s)L1

a(0)
t

2
.
(8.48)
We can derive similar expressions to determine ln P(r(0)
t
| a(0)
t
= a(0,s,s)) and
ln P(r(1)
t
| a(1)
t
= a(1,s,s)) using (8.3):
ln

P

r(0)
t
| a(0)
t
= 1

P

r(0)
t
| a(0)
t
= 0


= ln

P

r(0)
t
| a(0)
t
= 1

1 −P

r(0)
t
| a(0)
t
= 1


= Lcr(0)
t
P

r(0)
t
| a(0)
t
= 1

=
eLcr(0)
t
1 + eLcr(0)
t
=
1
1 + e−Lcr(0)
t
.
Also, P(r(0)
t
| a(0)
t
= 0) =
1
1+eLcr(0)
t . Therefore:
P

r(0)
t
| a(0)
t
= a(0,s,s)
=
e
−Lcr(0)
t
2
1 + e
−Lcr(0)
t
2
e
a(0,s,s)Lcr(0)
t
2
.

294
NON-BINARY TURBO CODES
Again, ignoring the constant term we obtain:
ln

P

r(0)
t
| a(0)
t
= a0,s,s
= a(0,s,s)Lcr(0)
t
2
.
(8.49)
In the same way:
ln

P

r(1)
t
| a(1)
t
= a1,s,s
= a(1,s,s)Lcr(1)
t
2
.
(8.50)
Substituting (8.46), (8.47) and (8.48) into (8.40) gives [7]:
¯γt(s, s) = a(0,s,s)L1

a(0)
t

2
+ a(0,s,s)Lcr(0)
t
2
+ a(1,s,s)Lcr(1)
t
2
.
(8.51)
8.4 Non-Binary Turbo Codes
Extending binary turbo codes to non-binary turbo codes [8, 9, 10] can be considered
less complicated than the extension of binary LDPC codes to non-binary LDPC codes.
In particular, the principle of the non-binary turbo decoding algorithm remains the
same. One of the main differences is the trellis diagram associated with a non-binary
convolutional code, which has more branches leaving and entering nodes in the trellis,
resulting in more paths and higher decoding complexity. Secondly, an increase in the
size of the alphabet means that the reliabilities of these extra symbols must also be
considered. The non-binary turbo encoder has the same structure as the binary turbo
encoder in Figure 8.1, with the component encoders being replaced by RSC codes
deﬁned over a ring of integers Zq, where q is the cardinality of the ring. The non-binary
turbo encoder is given in Figure 8.5.
The message symbols xt and the turbo encoder output symbols v(0)
t , v(1)
t , v(2)
t
are
now deﬁned over Zq.
Interleaver
Π
q-ary RSC
Encoder 1
q-ary RSC
Encoder 2
Puncturing
)
0
(
tv
)
2
(
tv
)
1
(
tv
Encoder
Output
Message Symbols xt
Π(xt)
Figure 8.5
The non-binary turbo encoder.

NON-BINARY TURBO CODES
295
Example 8.2: Non-binary turbo encoding: A non-binary turbo encoder deﬁned
over Z4 with 23/1 RSC component encoders is shown in Figure 8.6.
Π
{3, 5, 1, 4, 2}
No Puncturing
)
0
(
tv
)
2
(
tv
)
1
(
tv
Encoder
Output
Message Symbols xt
3
2
1
Π(xt)
3
2
1
Figure 8.6
Non-binary turbo encoder with 23/1 RSC component codes.
Let the message be xt = [x1 x2, x3, x4, x5] = [1, 2, 3, 3, 3]. The state table for the
ﬁrst component 23/1 RSC encoder deﬁned over Z4 is given in Table 8.2.
Table 8.2
State table for the 23/1 RSC code.
xt
s
s
v(0,s,s)
v(1,s,s)
0
0
0
0
0
1
0
1
1
2
2
0
2
2
0
3
0
3
3
2
0
1
1
0
1
1
1
2
1
3
2
1
3
2
1
3
1
0
3
3
0
2
2
0
2
1
2
3
1
0
2
2
0
2
2
3
2
1
3
0
0
3
0
0
3
1
3
0
1
1
2
3
1
2
3
3
3
2
3
1

296
NON-BINARY TURBO CODES
The input sequence runs through the ﬁrst RSC encoder, resulting in a parity
sequence:
v(1)
t
=
%
v(1)
1 , v(1)
2 , v(1)
3 , v(1)
4 , v(1)
5
&
= [2, 1, 1, 0, 3].
The sequence xt is also passed through the interleaver of length N = 5, which
produces the permuted output sequence x
t = (xt) = [3, 3, 1, 3, 2]. The sequence
x
t is passed through the second convolutional encoder, which produces the output
sequence:
v(2)
t
=
%
v(2)
1 , v(2)
2 , v(2)
3 , v(2)
4 , v(2)
5
&
= [2, 1, 0, 1, 2].
The three output sequences are multiplexed together to form the output sequence:
vt =
%
v(0)
1 , v(1)
1 , v(2)
1

,

v(0)
2 , v(1)
2 , v(2)
2

, . . . ,

v(0)
5 , v(1)
5 , v(2)
5
&
= [(1, 2, 2), (2, 1, 1), (3, 1, 0), (3, 0, 1), (3, 3, 2)]
.
8.4.1 Multi-Dimensional Log-Likelihood Ratios
The binary log-likelihood ratio is a measure of reliability and is deﬁned as the natural
logarithm of the ratio of the likelihood of an event being 1 to the likelihood of the
same event being 0. However, if we expand to a ring of integers Zq we must consider
the reliabilities of the other symbols too. The LLRs for an event u being an element
in Z4 are [10]:
L(1)(u) = ln
 P(u = 1)
P(u = 0)

L(2)(u) = ln
 P(u = 2)
P(u = 0)

L(3)(u) = ln
 P(u = 3)
P(u = 0)

.
(8.52)
The output of decoder 1 is therefore deﬁned by the multi-dimensional LLR:

L(1)
v(0)
t
| r

= ln

P

v(0)
t
= 1 | r

P

v(0)
t
= 0 | r
)

,
L(2)
v(0)
t
| r

= ln

P

v(0)
t
= 2 | r

P

v(0)
t
= 0 | r


,
L(3)
v(0)
t
|r

= ln

P

v(0)
t
= 3 | r

P

v(0)
t
= 0 | r


,
(8.53)
where r is the received vector. In the binary case, the sign of the LLR was used to
make a hard decision on the decoded symbol. A negative LLR corresponded to a 0
and a positive LLR corresponded to a 1. This is also true for multi-dimensional LLRs.

NON-BINARY TURBO CODES
297
If ln( P(u=2)
P(u=0)) is negative, the hard decision will be u = 0; if it is positive the hard
decision will be u = 2.
In the non-binary turbo decoding procedure, the prior information into decoder 1
is the deinterleaved extrinsic information from decoder 2.

L(1)
1

v(0)
t

, L(2)
1

v(0)
t

, L(3)
1

v(0)
t

=

L(1)
e2

−1
v(0)
t

, L(2)
e2

−1
v(0)
t

, L(3)
e2

−1
v(0)
t

.
(8.54)
Therefore, the extrinsic information from decoder 1 is:
L(1)
e1

v(0)
t

= L(1)
1

v(0)
t
| r

−L(1)
1

v(0)
t

−L(1)
r(0)
t
| v(0)
t

L(2)
e1

v(0)
t

= L(2)
1

v(0)
t
| r

−L(2)
1

v(0)
t

−L(2)
r(0)
t
| v(0)
t

L(3)
e1

v(0)
t

= L(3)
1

v(0)
t
| r

−L(3)
1

v(0)
t

−L(3)
r(0)
t
| v(0)
t

.
(8.55)
Similarly, the extrinsic information from decoder 2 is:
L(1)
e2

v(0)
t

= L(1)
2

v(0)
t
| r

−L(1)
2

v(0)
t

−L(1)
2



r(0)
t
| v(0)
t

L(2)
e2

v(0)
t

= L(2)
2

v(0)
t
| r

−L(2)
2

v(0)
t

−L(2)
2



r(0)
t
| v(0)
t

L(3)
e2

v(0)
t

= L(3)
2

v(0)
t
| r

−L(3)
2

v(0)
t

−L(3)
2



r(0)
t
| v(0)
t

.
(8.56)
8.4.2 Non-Binary Iterative Turbo Decoding
Now that all the necessary multi-dimensional LLRs are deﬁned we can present the
non-binary turbo decoder. The complete non-binary turbo decoder over Z4 is shown
in Figure 8.7.
A hard decision is made on the multi-dimensional LLRs from the deinterleaved
output of decoder 2.
If
sign

L(i)
v(0)
t
|r

< 0,
v(0)
t
= 0
If
sign

L(i)
v(0)
t
|r

≥0,
v(0)
t
= i,
where
i = 1, 2, . . . , q −1.
(8.57)
Hence, there are q −1 candidate values for the decoded symbol. The most likely
element is determined by comparing each LLR value L(i)(vt | r) and choosing the
LLR with the largest magnitude (the highest reliability).
One concern with regards to using q −1 LLR values of the form ln ( P(u=i)
P(u=0)),
i = 1, 2, . . . , q −1 is that we only have a measure of reliability of the nonzero
elements with respect to zero, and not to each other. For the case of element in Z4 we
would have the extra LLRs:
ln
 P(u = 2)
P(u = 1)

ln
 P(u = 3)
P(u = 1)

ln
 P(u = 3)
P(u = 2)

.

298
NON-BINARY TURBO CODES
Decoder
1
Σ
(
)
(
)
(
)









)1
(
)
3
(
)
1
(
)
2
(
)
1
(
)
1
(
|
|
|
t
t
t
v
L
v
L
v
L
r
r
r
(
)
(
)
(
)









)
2
(
)
3
(
)
2
(
)
2
(
)
2
(
)
1
(
|
|
|
t
t
t
v
L
v
L
v
L
r
r
r
(
)
(
)
(
)









)
0
(
)
3
(
)
0
(
)
2
(
)
0
(
)
1
(
|
|
|
t
t
t
v
L
v
L
v
L
r
r
r
(
)
(
)
(
)









r
r
r
|
|
|
)
0
(
)
3
(
)
0
(
)
2
(
)
0
(
)
1
(
t
t
t
v
L
v
L
v
L
(
)
(
)
(
)









)
0
(
)
3
(
1
)
0
(
)
2
(
1
)
0
(
)
1(
1
t
t
t
v
L
v
L
v
L
Decoder
2
Π
Π-1
(
)
(
)
(
)









)
0
(
)
3
(
)
0
(
)
2
(
)
0
(
)
1
(
1
1
1
t
e
t
e
t
e
v
L
v
L
v
L
Σ
Π
(
)
(
)
(
)
(
)
(
)
(
)









Π
Π
Π
)
0
(
)
3
(
2
)
0
(
)
2
(
2
)
0
(
)
1(
2
t
t
t
v
L
v
L
v
L
(
)
(
)
(
)
(
)
(
)
(
)









Π
Π
Π
r
r
r
|
|
|
)
0
(
)
3
(
)
0
(
)
2
(
)
0
(
)
1
(
t
t
t
v
L
v
L
v
L
(
)
(
)
(
)
(
)
(
)
(
)









Π
Π
Π
r
r
r
|
|
|
)
0
(
)
3
(
)
0
(
)
2
(
)
0
(
)
1
(
t
t
t
v
L
v
L
v
L
(
)
(
)
(
)
(
)
(
)
(
)









Π
Π
Π
)
0
(
)
3
(
)
0
(
)
2
(
)
0
(
)
1
(
2
2
2
t
e
t
e
t
e
v
L
v
L
v
L
−
−
−
−
+
+
Π-1
(
)
(
)
(
)









r
r
r
|
|
|
)
0
(
)
3
(
)
0
(
)
2
(
)
0
(
)
1
(
t
t
t
v
L
v
L
v
L
Figure 8.7
The non-binary turbo decoder deﬁned over Z4.
Including these extra LLRs in the decoding procedure will provide more informa-
tion about the reliability of each symbol, but the number of extra LLRs will increase
exponentially with increasing alphabet size.
8.5 Conclusions
This chapter introduced the concept of non-binary turbo encoding and decoding
deﬁned over rings. It can be seen that the non-binary turbo encoder structure is the
same as the binary turbo encoder structure, with non-binary RSC codes replacing
the binary RSC codes. Non-binary turbo decoding was explained by ﬁrst introducing
multi-dimensional log-likelihood ratios. Essentially, the principle of non-binary turbo
decoding is the same, with the exception that we must now process an array of LLR
values for each nonzero element in the ring, instead of just one.
Non-binary turbo decoding is an area of coding theory that has not received much
attention in the literature, most likely due to the extra complexity in implementation.
However, with non-binary LDPC codes recently becoming more popular, we would
expect non-binary turbo codes to perform just as well and this would be an interesting
area of research for the future.

REFERENCES
299
References
[1] Berrou, C., Glaviuex, A. and Thitimijshima, P. (1993) Near shannon limit error-correcting coding
and decoding: turbo codes. Proceedings of the IEEE International Conference on Communications,
Geneva, Switzerland, pp. 1064–70.
[2] Bahl, L.R., Cocke, J., Jelinek, F. and Raviv, J. (1972) Optimal decoding of linear codes for
minimising symbol error rate. Abstracts of Papers, International Symposium on Information Theory,
p. 50.
[3] Bahl, L.R., Cocke, J., Jelinek, F. and Raviv, J. (1974) Optimal decoding of linear codes for
minimising symbol error rate. IEEE Transactions on Information Theory, IT-20 (2), 284–7.
[4] Pyndiah, R., Glavieux, A., Picart, A. and Jacq, S. (1994) Near optimum decoding of products codes.
Proceedings of the IEEE GlobeCom’94 Conference San Francisco, CA, Vol. 1/3, pp. 339–43.
[5] McAdam, P.L., Welch, L. and Weber, C. (1972) MAP bit decoding of convolutional codes. Abstracts
of Papers, International Symposium on Information Theory, p. 9.
[6] Erfanian, J.A., Pasupathy, S. and Gulot, G. (1994) Reduced complexity symbol detectors with
parallel structures for ISI channels. IEEE Transactions on Communications, 42 (234, Part 3),
1661–71.
[7] Moon, T.K. (2005) Error Correction Coding. Mathematical Methods and Algorithms, Wiley Inter-
science, ISBN 0-471-64800-0.
[8] Berrou, C., Jezequel, M., Douillard, C. and Kerouedan, S. (2001) The Advantages of Non-Binary
Turbo Codes, IEEE Information Theory Workshop, Australia.
[9] Berrou, C. and Jezequel, M. (1999) Non-binary convolutional codes for turbo coding. IET Elec-
tronics Letters, 35 (1), 39–40.
[10] Berkmann, J. (1998) On turbo decoding of non-binary turbo codes. IEEE Communications Letters,
2 (4), 94–6.


Index
afﬁne
curve, 60
points, 61
space, 60
algorithm
BCJR, 287
Belief Propagation, 208
Berlekamp-Massey, 93
Euclid’s, 89
Guruswami-Sudan, 148
Kotter-Vardy, 164
log MAP, 292
max-log MAP, 290
Roth-Ruckenstein, 159
Sakata, 119
Viterbi, 45, 265
angle of arrival, 28
array dispersion, 219
asymptotic coding gain (ACG), 100, 244
balanced incomplete block design (BIBD),
203
butterﬂy diagram, 226
channel
additive white Gaussian noise (AWGN),
17
binary erasure, 13
binary symmetric, 12
capacity, 11, 20
fading, 20
ﬁxed wireless access, 34
indoor, 35, 39
magnetic storage, 43
multiple-input-multiple-output (MIMO),
40
pedestrian, 35, 39
quaternary symmetric
vehicular, 36, 40
code
algebraic-geometric, 70, 109
Bose-Chaudhuri-Hocquenghem (BCH),
84
block coded modulation (BCM), 96
convolutional, 238
cyclic, 80
functional Goppa, 70
Hamming, 75
Hermitian, 110
low density parity check (LDPC), 201
multi-level block, 97
parity check, 74
repetition, 97
Reed-Solomon, 86, 115, 148, 164
residue Goppa, 70
ring trellis coded modulation
(Ring-TCM), 246
ring BCM, 103
TCM, 243
turbo, 281
correlation, 42
curve
Hermitian, 61, 65, 110
irreducible, 61
Non-Binary Error Control Coding for Wireless Communication and Data Storage
Rolando Antonio Carrasco and Martin Johnston
C 2008 John Wiley & Sons, Ltd

302
INDEX
de-permutation, 223
decoder
hard-decision list, 148, 171
iterative, 208, 284
soft-decision list, 164, 192
turbo, 284
degree
curve, 64
divisor, 66
delay spread, 33
designed minimum distance, 84, 70,
114
differentiator, 44
distance
BCH bound, 84
Euclidean, 97, 99, 244
Free, 99, 244
Hamming, 74, 100, 241
divisor, 66
Doppler shift, 26
encoder
block, 75
BCM, 97
convolutional, 238
multilevel, 97, 98
systematic, 78, 82, 115, 242
TCM, 243
entropy, 5
erasure, 13
error
correction, 74, 76
detection, 75, 76
factor graph, 202, 224
factorisation, 159, 187
fast Fourier transform (FFT), 216, 224
ﬁnite ﬁelds
conjugate element, 59
construction, 54
cyclotomic cosets, 59, 85
extension, 55
minimal polynomial, 59
primitive element, 55
primitive polynomial, 56
subﬁelds, 59, 85
Forney’s formula, 95
Gauss-Jordan elimination, 116, 117
genetic algorithm, 253
genus, 67
group, 51
Hadamard transform, 217
horizontal step, 210
hyperplane at inﬁnity, 60
information
a priori, 284
average, 5
extrinsic, 284, 287
mutual, 9
iterative polynomial construction, 152
interleaving, 171, 199, 282
inter-symbol interference (ISI), 33, 44
interpolation theorem, 150, 181
inverse discrete Fourier transform (IDFT),
126
inverse fast Fourier transform (inverse FFT),
217
Justesen’s construction of AG codes, 113
key equation, 89
linear dependence/independence, 207
log likelihood ratio (LLR), 285
longitudinal magnetic recording, 44
Lorentzian pulse, 44
majority voting, 125
mapping
gray, 250
Carrasco and Farrell, 252
PSK, 247
QAM, 251
maximum distance separable (MDS), 88
maximum likelihood
decoding, 48, 265
sequence detection, 269
message length, 74, 70, 85, 207
matrix
circulant, 204
generator, 77, 112, 206
Hadamard, 217
incidence, 204

INDEX
303
multiplicity, 165, 193
parity check, 77, 88, 102, 202,
207
quasi-cyclic, 203, 219
reliability, 164
syndrome, 120
systematic, 116
monomial, 64, 68, 110, 118, 148
multi-dimensional LLR, 296
multiple-input-multiple-output (MIMO), 40,
263
multiplicity, 150
nearest neighbours, 251
noise
additive white Gaussian, 17
electronics, 44
non-binary
BCH code, 85
LDPC code, 218
turbo code, 294
order
leading, 149
lexicographic, 148
pole, 65, 110, 172
total graduated, 122
zero, 68, 173
parallel concatenation, 282
parity check equations, 74, 202
partial response equalisation, 47
partial response maximum likelihood
(PRML) detection, 45
permutation, 223
point at inﬁnity, 60, 110
polynomial
codeword, 80
error locating, 89, 120
error magnitude, 89
feed-back, 249
feed-forward, 249
generator, 80, 85, 87
interpolation, 151, 172
primitive, 56
remainder, 89
ring, 53
syndrome, 89
target, 46
power spectral density, 34
product criterion, 273
projective
curve, 60
points, 60
space, 60
puncturing, 282
rank criterion, 273
rational functions, 64, 110
recursive coefﬁcient search, 160
reliability, 285
Riemann-Roch theorem, 67
rings, 52
scattering, 28
set partitioning, 99, 244
signal ﬂow graph, 240, 260
signal-to-noise (SNR) ratio, 18
Singleton bound, 88
space-time coding
design criteria for fast fading, 274
design criteria for slow fading, 273
pairwise error probability, 269
ring-TCM (ST-RTCM), 262
span, 122
state
diagram, 240
table, 238, 245
syndromes, 79, 89, 120
Tanner graph, 202
tapped delay line, 31
target response, 46
transfer function, 240, 260
tree diagram, 239
trellis
block code, 100
convolutional code, 241
urban environment (see indoor, pedestrian,
vehicular channels)
vertical step, 211
Walsh-Hadamard (see Hadamard matrix)


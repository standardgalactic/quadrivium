OXJORD 
i 
l \ JL l i W x i 
L_ C O 
v/ I 
C O M P U T E R 
HARDWARE 
F O U R T H 
E D I T I O N 
A L an 
C L ement s 
I N C L U D E S ^ | 
FREE 
"M 
CD-ROM 
W 
online 
resource 
centre 


Principles of Computer Hardware 

I dedicate this edition to all those who have helped me run the IEEE Computer Society's 
International Design Competition since 2001. In particular, I express my gratitude 
to the following who have become my friends and mentors. 
Andy Bernat 
Simon Ellis 
Jerry Engle 
Robert Graham 
David Hennage 
Ivan Joseph 
Anne Marie Kelly 
Kathy Land 
Mike Lutz 
Fernando Maymi 
Stacy Saul 
Deborah Scherrer 
Janie Schwark 
Steve Seidman 

P R I N C I P L E S OF 
C O M P U T E R 
HARDWARE 
Alan Clements 
School of Computing 
University of Teesside 
Fourth Edition 
OXFORD 
UNIVERSITY PRESS 

OXFORD 
UNIVERSITY PRESS 
Great Clarendon Street, Oxford OX2 6DP 
Oxford University Press is a department of the University of Oxford. 
It furthers the University's objective of excellence in research, scholarship, 
and education by publishing worldwide in 
Oxford New York 
Auckland Cape Town Dares Salaam Hong Kong Karachi 
Kuala Lumpur Madrid Melbourne Mexico City Nairobi 
New Delhi Shanghai Taipei Toronto 
With offices in 
Argentina Austria Brazil Chile Czech Republic France Greece 
Guatemala Hungary Italy Japan Poland Portugal Singapore 
South Korea Switzerland Thailand Turkey Ukraine Vietnam 
Oxford is a registered trade mark of Oxford University Press 
in the UK and certain other countries 
Published in the United States 
by Oxford University Press Inc., New York 
© Alan Clements, 2006 
The moral rights of the author have been asserted 
Database right Oxford University Press (maker) 
First published 1985 
Second edition 1991 
Third edition 2000 
Fourth edition 2006 
All rights reserved. No part of this publication may be reproduced, 
stored in a retrieval system, or transmitted, in any form or by any means, 
without the prior permission in writing of Oxford University Press, 
or as expressly permitted by law, or under terms agreed with the appropriate 
reprographics rights organization. Enquiries concerning reproduction 
outside the scope of the above should be sent to the Rights Department, 
Oxford University Press, at the address above 
You must not circulate this book in any other binding or cover 
and you must impose the same condition on any acquirer 
British Library Cataloguing in Publication Data 
Data available 
Library of Congress Cataloging in Publication Data 
Data available 
Typeset by Newgen Imaging Systems (P) Ltd., Chennai, India. 
Printed in Great Britain 
on acid-free paper by 
Bath Press Ltd, Bath 
ISBN 0-19-927313-8 
978-0-19-927313-3 
10 9 8 7 6 5 4 3 2 1 

PREFACE 
Principle of Computer Hardware is aimed at students taking 
an introductory course in electronics, computer science, or 
information technology. The approach is one of breadth 
before depth and we cover a wide range of topics under the 
general umbrella of computer hardware. 
I have written Principles of Computer Hardware to achieve 
two goals. The first is to teach students the basic concepts on 
which the stored-program digital computer is founded. 
These include the representation and manipulation of infor-
mation in binary form, the structure or architecture of a com-
puter, the flow of information within a computer, and the 
exchange of information between its various peripherals. We 
answer the questions, 'How does a computer work', and 'How 
is it organized?' The second goal is to provide students with a 
foundation for further study. In particular, the elementary 
treatment of gates and Boolean algebra provides a basis for 
a second-level course in digital design, and the introduction 
to the CPU and assembly-language programming provides a 
basis for advanced courses on computer architecture/organi-
zation or microprocessor systems design. 
This book is written for those with no previous knowledge 
of computer architecture. The only background information 
needed by the reader is an understanding of elementary alge-
bra. Because students following a course in computer science 
or computer technology will also be studying a high-level 
language, we assume that the reader is familiar with the con-
cepts underlying a high-level language. 
When writing this book, I set myself three objectives. By 
adopting an informal style, I hope to increase the enthusiasm 
of students who may be put off by the formal approach of 
more traditional books. I have also tried to give students an 
insight into computer hardware by explaining why things are 
as they are, instead of presenting them with information to be 
learned and accepted without question. I have included sub-
jects that would seem out of place in an elementary first-level 
course. Topics like advanced computer arithmetic, timing 
diagrams, and reliability have been included to show how the 
computer hardware of the real world often differs from that 
of the first-level course in which only the basics are taught. 
I've also broadened the range of topics normally found in 
first-level courses in computer hardware and provided sec-
tions introducing operating systems and local area networks, 
as these two topics are so intimately related to the hardware of 
the computer. Finally, I have discovered that stating a formula 
or a theory is not enough—many students like to see an 
actual application of the formula. Wherever possible I have 
provided examples. 
Like most introductory books on computer architecture, 
I have chosen a specific microprocessor as a vehicle to illustrate 
some of the important concepts in computer architecture. The 
ideal computer architecture is rich in features and yet easy to 
understand without exposing the student to a steep learning 
curve. Some microprocessors have very complicated architec-
tures that confront the students with too much fine detail early 
in their course. We use Motorola's 68K microprocessor because 
it is easy to understand and incorporates many of the most 
important features of a high-performance architecture. This 
book isn't designed to provide a practical assembly language 
programming course. It is intended only to illustrate the oper-
ation of a central processing unit by means of a typical assem-
bly language. We also take a brief look at other microprocessors 
to show the range of computer architectures available. 
You will see the words computer, CPU, processor, micro-
processor, and microcomputer in this and other texts. The part 
of a computer that actually executes a program is called a 
CPU (central processing unit) or more simply a processor. 
A microprocessor is a CPU fabricated on a single chip of sili-
con. A computer that is constructed around a microprocessor 
can be called a microcomputer. To a certain extent, these terms 
are frequendy used interchangeably. 

READING GUIDE 
We've already said that this book provides a traditional 
introductory course in computer architecture plus additional 
material to broaden its scope and fill in some of the gaps left 
in such courses. To help students distinguish between fore-
ground and background material, the following guide will 
help to indicate the more fundamental components of the 
course. 
Chapter 2 introduces the logic of computers and deals with 
essential topics such as gates, Boolean algebra, and Karnaugh 
maps. Therefore this chapter is essential reading. 
Chapter 3 introduces sequential circuits such as the counter 
that steps through the instructions of a program and demon-
strates how sequential circuits are designed. We first intro-
duce fiie bistable (flip-flop) used to construct sequential 
circuits such as registers and counters. We don't provide a 
comprehensive introduction to the design of sequential cir-
cuits; we show how gates and flip-flops can be used to create 
a computer. 
Chapter 4 deals with the representation of numbers and 
shows how arithmetic operations are implemented. Apart 
from some of the coding theory and details of multiplication 
and division, almost all this chapter is essential reading. 
Multiplication and division can be omitted if the student is 
not interested in how these operations are implemented. 
Chapter 5 is the heart of the book and is concerned with the 
structure and operation of the computer itself. We examine 
the instruction set of a processor with a sophisticated 
architecture. 
Chapter 6 provides an overview of assembly language pro-
gramming and the design of simple 68K assembly language 
programs. This chapter relies heavily on the 68K cross-
assembler and simulator provided with the book. You can use 
this software to investigate the behavior of the 68K on a PC. 
Chapter 7 begins with a description of the functional units 
that make up a computer and the flow of data during the exe-
cution of an instruction. We then describe the operation of 
the computer's control unit, which decodes and executes 
instructions. The control unit may be omitted on a first read-
ing. Although the control unit is normally encountered in a 
second- or third-level course, we've included it here for the 
purpose of completeness and to show how the computer 
turns a binary-coded instruction into the sequence of events 
that carry out the instruction. 
Chapter 8 is concerned with the quest for performance. We 
look at how performance is measured and describe three 
techniques used to accelerate processors. All students should 
read about the first two acceleration techniques, pipelining 
and cache memory, but may omit parallel processing. 
Chapter 9 describes two contrasting computer architectures. 
Introductory texts on computer architecture are forced to 
concentrate on one processor because students do not have 
the time to plow through several different instruction sets. 
However, if we don't cover other architectures, students can 
end the course with a rather unbalanced view of processors. 
In this chapter we provide a very brief overview of several 
contrasting processors. We do not expect students to learn 
the fine details of these processors. The purpose of this chap-
ter is to expose students to the range of processors that are 
available to the designer. 
Chapter 10 deals with input/output techniques. We are inter-
ested in the way in which information is transferred between 
a computer and peripherals. We also examine the buses, or 
data highways, along which data flows. This chapter is essen-
tial reading. 
Chapter 11 introduces some of the basic peripherals you'd 
find in a typical PC such as the keyboard, display, printer, and 
mouse, as well as some of the more unusual peripherals that, 
for example, can measure how fast a body is rotating. 
Although these topics are often omitted from courses in com-
puter hardware, students should scan this chapter to get some 
insight into how computers control the outside world. 
Chapter 12 looks at the memory devices used to store data in 
a computer. Information isn't stored in a computer in just 
one type of storage device. It's stored in DRAM and on disk, 
CD-ROM, DVD, and tape. This chapter examines the operat-
ing principles and characteristics of the storage devices found 
in a computer. There's a lot of detail in this chapter. Some 
readers may wish to omit the design of memory systems (for 
example, address decoding and interfacing) and just concen-
trate on the reasons why computers have so many different 
types of memory. 
Chapter 13 deals with hardware topics that are closely related 
to the computer's operating system. The two most important 
elements of a computer's hardware that concern the operating 
system are multiprogramming and memory management. 
These topics are intimately connected with interrupt handling 

Reading guide 
vii 
and data storage techniques and serve as practical examples of 
the use of the hardware described elsewhere. Those who 
require a basic introduction to computer hardware may omit 
this chapter, although it best illustrates how hardware and 
software come together in the operating system. 
Chapter 14 describes how computers can communicate with 
each other. The techniques used to link computers to create 
computer networks are not always covered by first-level texts 
on computer architecture. However, the growth of both local 
area networks and the Internet have propelled computer 
communications to the forefront of computing. For this rea-
son we would expect students to read this chapter even if 
some of it falls outside the scope of their syllabus. 

THE HISTORY OF THIS BOOK 
Like people, books are born. Principles of Computer Hardware 
was conceived in December 1980. At the end of their first semes-
ter our freshmen were given tests to monitor their progress. The 
results of the test in my'Principles of computer hardware' course 
were not as good as I'd hoped, so I decided to do something 
about it. I thought that detailed lecture notes written in a style 
accessible to the students would be the most effective solution. 
Having volunteered to give a course on computer commu-
nications to the staff of die Computer Center during the 
Christmas vacation, I didn't have enough free time to produce 
me notes. By accident I found that the week before Christmas 
was the cheapest time of the year for vacations. So I went to 
one of the Canary Islands for a week, sat down by the pool, 
surrounded by folders full of reference material, with a bottle of 
Southern Comfort, and wrote the core of this book—number 
bases, gates, Boolean algebra, and binary arithmetic. Shortly 
afterwards I added the section on the structure of the CPU. 
These notes produced the desired improvement in the 
end-of-semester exam results and were well received by the 
students. In the next academic year my notes were transferred 
from paper to a mainframe computer and edited to include 
new material and to clean up the existing text. 
I decided to convert the notes into a book. The conversion 
process involved adding topics, not covered by our syllabus, 
to produce a more rounded text. While editing my notes, I 
discovered what might best be called the inkblot effect. Text 
stored in a computer tends to expand in all directions because 
it's so easy to add new material at any point; for example, you 
might write a section on disk drives. When you next edit the 
section on disks, you can add more depth or breadth. 
The final form of this book took a breadth before depth 
approach. That is, I covered a large number of topics rather 
man treating fewer topics in greater depdi. It was my intention 
to give students taking our introductory hardware/architecture 
course a reasonably complete picture of the computer system. 
The first edition of Principles of Computer Hardware 
proved successful and I was asked to write a second edition, 
which was published in 1990. The major change between the 
first and second editions was the adoption of the 68K micro-
processors as a vehicle to teach computer architecture. I have 
retained this processor in the current edition. Although 
members of the Intel family have become the standard 
processors in the PC world, Motorola's 68K family of micro-
processors is much better suited to teaching computer archi-
tecture. In short, it supports most of the features that 
computer scientists wish to teach students, and just as impor-
tantly, it's much easier to understand. The 68K family and its 
derivatives are widely used in embedded systems. 
By the mid-1990s the second edition was showing its age. 
The basic computer science and the underlying principles 
were still fine, but the actual hardware had changed dramati-
cally over a very short time. The most spectacular progress 
was in the capacity of hard disks—by the late 1990s disk 
capacity was increasing by 60% per year. 
This third edition included a 68K cross-assembler and 
simulator allowing students to create and run 68K programs 
on any PC. It also added details of interesting microprocessor 
architecture, the ARM, which provides an interesting con-
trast to the 68K. 
When I used the second edition to teach logic design to my 
students, they built simple circuits using logic trainers—boxes 
with power supplies and connectors that allow you to wire a 
handful of simple chips together. Dave Barker, one of my for-
mer students, has constructed a logic simulator program as 
part of his senior year project called Digital Works, which 
runs under Windows on a PC. Digital Works allows you to 
place logic elements anywhere within a window and to wire 
the gates together. Inputs to the gates can be provided manu-
ally (via the mouse) or from clocks and sequence generators. 
You can observe the outputs of the gates on synthesized LEDs 
or as a waveform or table. Moreover, Digital Works permits 
you to encapsulate a circuit in a macro and then use this 
macro in other circuits. In other words, you can take gates 
and build simple circuits, and take the simple circuits and 
build complex circuits, and so on. 
I began writing a fourth edition of this text in late 2003. 
The fundamental principles have changed little since the 
third edition, but processors had become faster by a factor of 
10 and the capacity of hard disks has grown enormously. This 
new edition is necessary to incorporate some of the advances. 
After consultation with those who adopt this book, we have 
decided to continue to use the 68K family to introduce the 
computer instruction set because this processor still has one 
of the most sophisticated of all instruction set architectures. 

ACKNOWLEDGEMENTS 
Few books are entirely the result of one person's unaided 
efforts and this is no exception. I would like to thank all those 
who wrote the books about computers on which my own 
understanding is founded. Some of these writers conveyed the 
sheer fascination of computer architecture that was to change 
the direction of my own academic career. It really is amazing 
how a large number of gates (a circuit element whose opera-
tion is so simple as to be trivial) can be arranged in such a way 
as to perform all the feats we associate computers with today. 
I am grateful for all the comments and feedback I've 
received from my wife, colleagues, students, and reviewers 
over the years. Their feedback has helped me to improve the 
text and eliminate some of the errors I'd missed in editing. 
More importantly, their help and enthusiasm has made the 
whole project worthwhile. 
Although I owe a debt of gratitude to a lot of people, I would 
like to mention four people who have had a considerable 
impact. Alan Knowles of Manchester University read drafts of 
both the second and third editions with a precision well 
beyond that of the average reviewer. Paul Lambert, one of my 
colleagues at The University of Teesside, wrote the 68K cross-
assembler and simulator that I use in my teaching. In this 
edition we have used a Windows-based graphical 68K 
simulator kindly provided by Charles Kelly. 
Dave Barker, one of my former students and an excellent 
programmer, wrote the logic simulator called Digital Works 
that accompanies this book. I would particularly like to thank 
Dave for providing a tool that enables students to construct 
circuits and test them without having to connect wires 
together. 
One of the major changes to the third edition was the 
chapter on the ARM processor. I would like to thank Steve 
Furber of Manchester University (one of the ARM's design-
ers) for encouraging me to use this very interesting device. 


CONTENTS 
1 Introduction to computer hardware 
1.1 What is computer hardware? 
1.2 Why do we teach computer hardware? 
1.2.1 Should computer architecture remain in the 
CS curriculum? 
1.2.2 Supporting the CS curriculum 
1.3 An overview of the book 
1.4 History of computing 
1.4.1 Navigation and mathematics 
1.4.2 The era of mechanical computers 
1.4.3 Enabling technology—the telegraph 
1.4.4 The first electromechanical computers 
1.4.5 The first mainframes 
1.4.6 The birth of transistors, ICs, and microprocessors 
1.4.7 Mass computing and the rise of the Internet 
1.5 The digital computer 
1.5.1 The PC and workstation 
1.5.2 The computer as a data processor 
1.5.3 The computer as a numeric processor 
1.5.4 The computer in automatic control 
1.6 The stored program computer—an overview 
1.7 The PC—a naming of parts 
SUMMARY 
PROBLEMS 
2 Gates, circuits, and combinational logic 
6 
6 
6 
8 
10 
11 
12 
14 
15 
15 
15 
16 
17 
19 
22 
23 
23 
25 
2.1 Analog and digital systems 
26 
2.2 Fundamental gates 
28 
2.2.1 The AND gate 
28 
2.2.2 The OR gate 
30 
2.2.3 The NOT gate 
31 
2.2.4 The NAND and NOR gates 
31 
2.2.5 Positive, negative, and mixed logic 
32 
2.3 Applications of gates 
34 
2.4 Introduction to Digital Works 
40 
2.4.1 Creating a circuit 
41 
2.4.2 Running a simulation 
45 
2.4.3 The clock and sequence generator 
48 
2.4.4 Using Digital Works to create embedded circuits 
50 
2.4.5 Using a macro 
52 
2.5 An introduction to Boolean algebra 
56 
2.5.1 Axioms and theorems of Boolean algebra 
56 
2.5.2 De Morgan's theorem 
63 
2.5.3 Implementing logic functions in NAND or NOR two 
logic only 
65 
2.5.4 Karnaugh maps 
67 
2.6 Special-purpose logic elements 
83 
2.6.1 The multiplexer 
84 
2.6.2 The demultiplexer 
84 
2.7 Tri-state logic 
87 
2.7.1 Buses 
88 
2.8 Programmable logic 
91 
2.8.1 The read-only memory as a logic element 
91 
2.8.2 Programmable logic families 
93 
2.8.3 Modern programmable logic 
94 
2.8.4 Testing digital circuits 
96 
SUMMARY 
98 
PROBLEMS 
98 
3 Sequential logic 
3.1 The RS flip-flop 
3.1.1 Analyzing a sequential circuit by assuming initial 
conditions 
3.1.2 Characteristic equation of an RS flip-flop 
3.1.3 Building an RS flip-flop from NAND gates 
3.1.4 Applications of the RS flip-flop 
3.1.5 The clocked RS flip-flop 
3.2 The D flip-flop 
3.2.1 Practical sequential logic elements 
3.2.2 Using D flip-flops to create a register 
3.2.3 Using Digital Works to create a register 
3.2.4 A typical register chip 
3.3 Clocked flip-flops 
3.3.1 Pipelining 
3.3.2 Ways of clocking flip-flops 
3.3.3 Edge-triggered flip-flops 
3.3.4 The master-slave flip-flop 
3.3.5 Bus arbitration—an example 
3.4 The JK flip-flop 
3.5 Summary of flip-flop types 
101 
103 
104 
105 
106 
106 
108 
109 
110 
110 
111 
112 
113 
114 
115 
116 
117 
118 
120 
121 
1 
1 
2 
3 
4 
5 

xii 
Contents 
3.6 Applications of sequential elements 
3.6.1 Shift register 
3.6.2 Asynchronous counters 
3.6.3 Synchronous counters 
3.7 An introduction to state machines 
3.7.1 Example of a state machine 
3.7.2 Constructing a circuit to implement 
the state table 
SUMMARy 
PROBLEMS 
4 Computer arithmetic 
4.1 Bits, bytes, words, and characters 
4.2 Number bases 
4.3 Number base conversion 
4.3.1 Conversion of integers 
4.3.2 Conversion of fractions 
4.4 Special-purpose codes 
4.4.1 BCD codes 
4.4.2 Unweighted codes 
4.5 Error-detecting codes 
4.5.1 Parity EDCs 
4.5.2 Error-correcting codes 
4.5.3 Hamming codes 
4.5.4 Hadamard codes 
4.6 Data-compressing codes 
4.6.1 Huffman codes 
4.6.2 Quadtrees 
4.7 Binary arithmetic 
4.7.1 The half adder 
4.7.2 The full adder 
4.7.3 The addition of words 
4.8 Signed numbers 
4.8.1 Sign and magnitude representation 
4.8.2 Complementary arithmetic 
4.8.3 Two's complement representation 
4.8.4 One's complement representation 
4.9 Floating point numbers 
4.9.1 Representation of floating point numbers 
4.9.2 Normalization of floating point numbers 
4.9.3 Floating point arithmetic 
4.9.4 Examples of floating point calculations 
4.10 Multiplication and division 
4.10.1 Multiplication 
4.10.2 Division 
SUMMARY 
PROBLEMS 
122 
122 
128 
132 
134 
136 
138 
139 
140 
5 The instruction set architecture 
203 
145 
146 
148 
150 
150 
152 
153 
153 
154 
156 
158 
158 
160 
161 
163 
164 
167 
169 
170 
171 
173 
175 
176 
176 
177 
180 
181 
182 
183 
186 
188 
189 
189 
194 
198 
198 
5.1 What is an instruction set architecture? 
5.2 Introduction to the CPU 
5.2.1 Memory and registers 
5.2.2 Register transfer language 
5.2.3 Structure of the CPU 
5.3 The 68K family 
5.3.1 The instruction 
5.3.2 Overview of addressing modes 
5.4 Overview of the 68K's instructions 
5.4.1 Status flags 
5.4.2 Data movement instructions 
5.4.3 Arithmetic instructions 
5.4.4 Compare instructions 
5.4.5 Logical instructions 
5.4.6 Bit instructions 
5.4.7 Shift instructions 
5.4.8 Branch instructions 
5UMMARY 
PROBLEMS 
6 Assembly language programming 
6.1 Structure of a 68K assembly language program 
6.1.1 Assembler directives 
6.1.2 Using the cross-assembler 
6.2 The 68K's registers 
6.2.1 Data registers 
6.2.2 Address registers 
6.3 Features of the 68K's instruction set 
6.3.1 Data movement instructions 
6.3.2 Using arithmetic operations 
6.3.3 Using shift and logical operations 
6.3.4 Using conditional branches 
6.4 Addressing modes 
6.4.1 Immediate addressing 
6.4.2 Address register indirect addressing 
6.4.3 Relative addressing 
6.5 The stack 
6.5.1 The 68K stack 
6.5.2 The stack and subroutines 
6.5.3 Subroutines, the stack, and parameter 
passing 
6.6 Examples of 68K programs 
6.6.1 A circular buffer 
SUMMARY 
PROBLEMS 
204 
206 
207 
208 
209 
210 
210 
215 
217 
217 
218 
218 
220 
220 
221 
221 
223 
226 
226 
228 
228 
229 
232 
234 
235 
236 
237 
237 
241 
244 
244 
249 
249 
250 
259 
262 
263 
266 
271 
280 
282 
287 
287 

Contents 
xiii 
7 Structure of the CPU 
293 
7.1 The CPU 
7.1.1 The address path 
7.1.2 Reading the instruction 
7.1.3 The CPU's data paths 
7.1.4 Executing conditional instructions 
7.1.5 Dealing with literal operands 
7.2 Simulating a CPU 
7.2.1 CPU with an 8-bit instruction 
7.2.2 CPU with a 16-bit instruction 
7.3 The random logic control unit 
7.3.1 Implementing a primitive CPU 
7.3.2 From op-code to operation 
7.4 Microprogrammed control units 
7.4.1 The microprogram 
7.4.2 Microinstruction sequence control 
7.4.3 User-microprogrammed processors 
SUMMARY 
PROBLEMS 
8 Accelerating performance 
8.1 Measuring performance 
8.1.1 Comparing computers 
8.2 The RISC revolution 
8.2.1 Instruction usage 
8.2.2 Characteristics of RISC architectures 
8.3 RISC architecture and pipelining 
8.3.1 Pipeline hazards 
8.3.2 Data dependency 
8.3.3 Reducing the branch penalty 
8.3.4 Implementing pipelining 
8.4 Cache memory 
8.4.1 Effect of cache memory on computer 
performance 
8.4.2 Cache organization 
8.4.3 Considerations in cache design 
8.5 Multiprocessor systems 
8.5.1 Topics in Multiprocessor Systems 
8.5.2 Multiprocessor organization 
8.5.3 MIMD architectures 
SUMMARY 
PROBLEMS 
9 Processor architectures 
294 
294 
295 
296 
298 
300 
300 
301 
304 
308 
308 
312 
315 
316 
319 
320 
322 
322 
325 
326 
326 
327 
328 
329 
335 
336 
338 
339 
341 
344 
345 
346 
350 
350 
352 
353 
356 
362 
362 
365 
9.1.2 Instruction formats 
9.1.3 Instruction types 
9.1.4 Addressing modes 
9.1.5 On-chip peripherals 
9.2 The microcontroller 
9.2.1 TheM68HC12 
9.3 The ARM—an elegant RISC processor 
9.3.1 ARM'S registers 
9.3.2 ARM instructions 
9.3.3 ARM branch instructions 
9.3.4 Immediate operands 
9.3.5 Sequence control 
9.3.6 Data movement and memory reference 
instructions 
9.3.7 Using the ARM 
SUMMARY 
PROBLEMS 
9.1 Instruction set architectures and their resources 
365 
9.1.1 Register sets 
365 
11.1 Simple input devices 
11.1.1 The keyboard 
11.1.2 Pointing devices 
11.2 CRT, LED, and plasma displays 
11.2.1 Raster-scan displays 
11.2.2 Generating a display 
11.2.3 Liquid crystal and plasma displays 
11.2.4 Drawing lines 
11.3 The printer 
11.3.1 Printing a character 
11.3.2 The Inkjet printer 
11.3.3 The laser printer 
366 
366 
367 
367 
367 
368 
375 
375 
377 
380 
381 
381 
382 
385 
397 
398 
10 Buses and input/output mechanisms 
399 
10.1 The bus 
400 
10.1.1 Bus architecture 
400 
10.1.2 Key bus concepts 
400 
10.1.3 The PC bus 
404 
10.1.4 The IEEE 488 bus 
407 
10.1.5 The USB serial bus 
411 
10.2 I/O fundamentals 
412 
10.2.1 Programmed I/O 
413 
10.2.2 Interrupt-driven I/O 
415 
10.3 Direct memory access 
422 
10.4 Parallel and serial interfaces 
423 
10.4.1 The parallel interface 
424 
10.4.2 The serial interface 
428 
SUMMARY 
433 
PROBLEMS 
433 
11 Computer Peripherals 
435 
436 
436 
440 
444 
445 
445 
447 
450 
452 
453 
453 
455 

xiv 
Contents 
11.4 Color displays and printers 
11.4.1 Theory of color 
11.4.2 Color CRTs 
11.4.3 Color printers 
11.5 Other peripherals 
11.5.1 Measuring position and movement 
11.5.2 Measuring temperature 
11.5.3 Measuring light 
11.5.4 Measuring pressure 
11.5.5 Rotation sensors 
11.5.6 Biosensors 
11.6 The analog interface 
11.6.1 Analog signals 
11.6.2 Signal acquisition 
11.6.3 Digital-to-analog conversion 
11.6.4 Analog-to-digital conversion 
11.7 Introduction to digital signal processing 
11.7.1 Control systems 
11.7.2 Digital signal processing 
SUMMARY 
PROBLEMS 
457 
457 
458 
460 
461 
461 
463 
464 
464 
464 
465 
466 
466 
467 
473 
477 
486 
486 
488 
491 
492 
12 Computer memory 
493 
12.1 Memory hierarchy 
493 
12.2 What is memory? 
496 
12.3 Memory technology 
496 
12.3.1 Structure modification 
496 
12.3.2 Delay tines 
496 
12.3.3 Feedback 
496 
12.3.4 Charge storage 
497 
12.3.5 Magnetism 
498 
12.3.6 Optical 
498 
12.4 Semiconductor memory 
498 
12.4.1 Static semiconductor memory 
498 
12.4.2 Accessing memory—timing diagrams 
499 
12.4.3 Dynamic memory 
501 
12.4.4 Read-only semiconductor memory devices 
505 
12.5 Interfacing memory to a CPU 
506 
12.5.1 Memory organization 
507 
12.5.2 Address decoders 
508 
12.6 Secondary storage 
515 
12.6.1 Magnetic surface recording 
515 
12.6.2 Data encoding techniques 
521 
12.7 Disk drive principles 
524 
12.7.1 Disk drive operational parameters 
527 
12.7.2 High-performance drives 
529 
12.7.3 RAID systems 
12.7.4 The floppy disk drive 
12.7.5 Organization of data on disks 
12.8 Optical memory technology 
12.8.1 
Storing and reading information 
12.8.2 
Writable CDs 
SUMMARY 
PROBLEMS 
13 The operating system 
13.1 The operating system 
13.1.1 Types of operating system 
13.2 Multitasking 
13.2.1 What is a process? 
13.2.2 Switching processes 
13.3 Operating system support from the CPU 
13.3.1 Switching states 
13.3.2 The 68K's two Stacks 
13.4 Memory management 
13.4.1 Virtual memory 
13.4.2 Virtual memory and the 68K family 
SUMMARY 
PROBLEMS 
14 Computer communications 
14.1 Background 
14.1.1 Local area networks 
14.1.2 LAN network topology 
14.1.3 History of computer communications 
14.2 Protocols and computer communications 
14.2.1 Standards bodies 
14.2.2 Open systems and standards 
14.3 The physical layer 
14.3.1 Serial data transmission 
14.4 The PSTN 
14.4.1 Channel characteristics 
14.4.2 Modulation and data transmission 
14.4.3 High-speed transmission over the PSTN 
14.5 Copper cable 
14.5.1 Ethernet 
14.6 Fiber optic links 
14.7 Wireless links 
14.7.1 Spread spectrum technology 
531 
532 
533 
536 
537 
540 
543 
543 
547 
547 
548 
550 
551 
551 
554 
555 
556 
561 
563 
565 
568 
568 
569 
570 
571 
572 
574 
576 
578 
578 
584 
584 
587 
587 
588 
591 
592 
593 
595 
596 
598 

Contents 
x v 
14.8 The data link layer 
599 
14.8.1 Bit-oriented protocols 
599 
14.8.2 The Ethernet data link layer 
603 
14.9 Routing techniques 
604 
14.9.1 Centralized routing 
607 
14.9.2 Distributed routing 
607 
14.9.3 IP (Internet protocol) 
607 
SUMMARY 
609 
PROBLEMS 
610 
Appendix:The 68000 instruction set 
611 
Bibliography 
641 
Index 
643 
Contents and installation instructions for the CD-Rom 
653 


INTRODUCTION 
In this chapter we set the scene for the rest of the book. We define what we mean by computer 
hardware, explain just why we teach computer hardware to computer science students, provide a 
very brief history of computing, and look at the role of the computer. 
1.1 What is computer hardware? 
To begin with I feel we ought to define the terms hardware 
and software. I could give a deeply philosophical definition, 
but perhaps an empirical one is more helpful. If any part of a 
computer system clatters on the floor when dropped, it's 
hardware. If it doesn't, it's software. This is a good working 
definition, but it's incomplete because it implies that hardware 
and software are unrelated entities. As we will discover, soft-
ware and hardware are often intimately related. Moreover, the 
operation of much of today's hardware is controlled by 
firmware (software embedded in the structure of the hardware). 
A computer's hardware includes all the physical compon-
ents that make up the computer system. These components 
HARDWARE, ARCHITECTURE, AND ORGANIZATION 
Hardware means all the parts of the computer that are not 
software. It includes the processor, its memory, the buses that 
connect devices together, and the peripherals. 
Architecture describes the internal organization of a 
computer in an abstract way; that is, it defines the capabilities 
of the computer and its programming model. You can have 
range from the CPU to the memory and input/output 
devices. The programs that control the operation of the com-
puter are its software. When a program is inside a computer 
its physical existence lies in the state of electronic switches, 
the magnetization of tiny particles on magnetic disk, or 
bumps on the surface of a CD or DVD. We can't point to a 
program in a computer any more than we can point to 
a thought in the brain. 
Two terms closely related to hardware are architecture and 
organization. A computer's architecture is an abstract view of 
the computer, which describes what it can do. A computer's 
architecture is the assembly language programmer's view of 
the machine. You could say that architecture has a similar 
meaning to functional specification. The architecture is an 
two computers that have been constructed in different ways 
with different technologies but with the same architecture. 
Organization describes how a computer is implemented. 
Organization is concerned with a computer's functional 
components and their interrelationship. Organization is about 
buses, timing, and circuits. 
Introduction to computer hardware 
CHAPTER MAP 
1 Introduction to 
computer hardware 
2 Logic elements and 
Boolean algebra 
Digital computers are 
constructed from millions of very 
simple logic elements called 
gates. In this chapter we 
introduce the fundamental gates 
and demonstrate how they can 
be combined to create circuits 
that carry out the basic functions 
required in a computer. 
Sequential logic 
We can classify logic circuits into 
two groups: the combinational 
circuit we described in Chapter 2 
and the sequential circuit which 
forms the subject of this chapter. 
A sequential circuit includes 
memory elements and its current 
behavior is governed by its past 
inputs. Typical sequential circuits 
are counters and registers. 
4 Computer arithmetic 
In Chapter 4 we demonstrate 
how numbers are represented in 
binary form and look at binary 
arithmetic. We also demonstrate 
how the properties of binary 
numbers are exploited to create 
codes that compress data or even 
detect and correct errors. 

2 
Chapter 1 Introduction to computer hardware 
abstraction of the computer. A computer's organization 
describes how the architecture is implemented; that is, it 
defines the hardware used to implement the architecture. 
Let's look at a simple example that distinguishes between 
architecture and organization. A computer with a 32-bit 
architecture performs operations on numbers that are 32 bits 
wide. You could build two versions of this computer. One is 
a high-performance device that adds two 32-bit numbers in a 
single operation. The other is a low-cost processor that gets 
a 32-bit number by bringing two 16-bit numbers from mem-
ory one after the other. Both computers end up with the same 
result, but one takes longer to get there. They have the same 
architecture but different organizations. 
Although hardware and software are different entities, 
there is often a trade-off between them. Some operations can 
be carried out either by a special-purpose hardware system or 
by means of a program stored in the memory of a general-
purpose computer. The fastest way to execute a given task is 
to build a circuit dedicated exclusively to the task. Writing a 
program to perform the same task on an existing computer 
may be much cheaper, but the task will take longer, as the 
computer's hardware wasn't optimized to suit the task. 
Developments in computer technology in the late 1990s 
further blurred the distinction between hardware and soft-
ware. Digital circuits are composed of gates that are wired 
together. From the mid-1980s onward manufacturers were 
producing large arrays of gates that could be interconnected 
electronically to create a particular circuit. As technology 
progressed it became possible to reconfigure the connections 
between gates while the circuit was operating. We now have 
the technology to create computers that can repair errors, 
restructure themselves as the state of the art advances, or even 
evolve. 
1.2 Why do we teach computer 
hardware? 
A generation ago, school children in the UK had to learn 
Latin in order to enter a university. Clearly, at some point it 
was thought that Latin was a vital prerequisite for everyone 
going to university. When did they realize that students could 
still benefit from a university education without a prior 
knowledge of Latin? Three decades ago students taking a 
degree in electronics had to study electrodynamics, the dance 
of electrons in magnetic fields, a subject so frightening that 
older students passed on its horrors to the younger ones in 
hushed tones. Today, electrodynamics is taught only to stu-
dents on specialist courses. 
We can watch a television program without understanding 
how a cathode ray tube operates, or fly in a Jumbo jet without 
ever knowing the meaning of thermodynamics. Why then 
should the lives of computer scientists and programmers be 
made miserable by forcing them to learn what goes on inside 
a computer? 
If topics in the past have fallen out of the curriculum with no 
obviously devastating effect on the education of students, what 
about today's curriculum? Do we still need to teach computer 
science students about the internal operation of the computer? 
Computer architecture is the oldest component of the 
computer curriculum. The very first courses on computer 
science were concerned with the design and construction of 
computers. At that time programming was in its infancy and 
compilers, operating systems, and databases did not exist. 
In the 1940s, working with computers meant building com-
puters. By the 1960s computer science had emerged as a 
discipline. With the introduction of courses in program-
ming, numerical methods, operating systems, compilers, and 
databases, the then curriculum reflected the world of the 
mainframe. 
In the 1970s computer architecture was still, to a considerable 
extent, an offshoot of electronics. Texts were more concerned 
with the circuits in a computer than with the fundamental prin-
ciples of computer architecture as now encapsulated by the 
expression instruction set architecture (ISA). 
Computer architecture experienced a renaissance in the 
1980s. The advent of the low-cost microprocessor-based sys-
tems and the single-board computer meant that computer 
science students could study and even get hands-on experi-
ence of microprocessors. They could build simple systems, 
test them, interface them to peripherals such as LEDs and 
switches, and write programs in machine code. Bill Gates 
himself is a product of this era. 
Assembly language programming courses once mirrored 
high-level language programming courses—students were 
taught algorithms such as sorting and searching in assembly 
language, as if assembly language were no more than the poor 
person's C. Such an approach to computer architecture is 
now untenable. If assembly language is taught at all today, it is 
used as a vehicle to illustrate instruction sets, addressing 
modes, and other aspects of a processor's architecture. 
In the late 1980s and early 1990s computer architecture 
underwent another change. The rise of the RISC micro-
processor turned the focus of attention from complex 
instruction set computers to the new high-performance, 
highly pipelined, 32-bit processors. Moreover, the increase in 
the performance of microprocessors made it harder and 
harder for classes to give students the hands-on experience 
they had a few years earlier. In the 1970s a student could con-
struct a computer with readily available components and 
simple electronic construction techniques. By the 1990s clock 
rates rose to well over 100 MHz and buses were 32 bits wide 
making it difficult for students to construct microprocessor-
based systems as they did in the 1980s. High clock rates 
require special construction techniques and complex chips 

1.2 Why do we teach computer hardware? 
3 
have hundreds of connections rather than the 40- or 64-pin 
packages of the 8086/68K era. 
In the 1990s computer architecture was largely concerned 
with the instruction set architecture, pipelining, hazards, 
superscalar processors, and cache memories. Topics such as 
microprocessor systems design at the chip level and micro-
processor interfacing had largely vanished from the CS cur-
riculum. These topics belonged to the CEng and EE curricula. 
In the 1990s a lot was happening in computer science; for 
example, the introduction of new subject areas such as 
object-oriented programming, communications and net-
works, and the Internet/WWW. The growth of the computer 
market, particularly for those versed in the new Internet-
based skills, caused students to look at their computing 
curricula in a rather pragmatic way. Many CS students will 
join companies using the new technologies, but very few of 
them indeed will ever design chips or become involved with 
cutting-edge work in computer architecture. At my own uni-
versity, the demand for courses in Internet-based computing 
has risen and fewer students have elected to take computer 
architecture when it is offered as an elective. 
1.2.1 Should computer architecture 
remain in the CS curriculum? 
Developments in computer science have put pressure on 
course designers to remove old material to make room for the 
new. The fraction of students that will ever be directly 
involved in computer design is declining. Universities pro-
vide programs in multimedia-based computing and visual-
ization at both undergraduate and postgraduate levels. 
Students on such programs do not see the point of studying 
computer architecture. 
Some have suggested that computer architecture is a prime 
candidate for pruning. It is easy to argue that computer archi-
tecture is as irrelevant to computer science as, say, Latin is to 
the study of contemporary English literature. If a student 
never writes an assembly language program or designs an 
instruction set, or interfaces a memory to a processor, why 
should we burden them with a course in computer architec-
ture? Does the surgeon study metallurgy in order to under-
stand how a scalpel operates? 
It's easy to say that an automobile driver does not have to 
understand the internal combustion engine to drive an auto-
mobile. However, it is patently obvious that a driver who 
understands mechanics can drive in such a way as to enhance 
the life of the engine and to improve its performance. The 
same is true of computer architecture; understanding com-
puter systems can improve the performance of software if the 
software is written to exploit the underlying hardware. 
The digital computer lies at the heart of computer science. 
Without it, computer science would be little more than a branch 
of theoretical mathematics. The very idea of a computer science 
program that did not provide students with an insight into the 
computer would be strange in a university that purports to edu-
cate students rather than to merely train them. 
Those supporting the continued teaching of computer 
architecture employ several traditional arguments. First, 
education is not the same as training and CS students are not 
simply being shown how to use commercial computer pack-
ages. A course leading to a degree in computer science should 
also cover the history and the theoretical basis for the subject. 
Without an appreciation of computer architecture, the com-
puter scientist cannot understand how computers have 
developed and what they are capable of. 
However, there are concrete reasons why computer archi-
tecture is still relevant in today's world. Indeed, I would 
maintain that computer architecture is as relevant to the 
needs of the average CS student today as it was in the past. 
Suppose a graduate enters the industry and is asked to select 
the most cost-effective computer for use throughout a large 
organization. Understanding how the elements of a com-
puter contribute to its overall performance is vital—is it 
better to spend $50 on doubling the size of the cache or $100 
on increasing the clock speed by 500 MHz? 
Computer architecture cannot be divorced entirely from 
software. The majority of processors are found not in PCs or 
workstations but in embedded1 applications. Those designing 
multiprocessors and real-time systems have to understand 
fundamental architectural concepts and limitations of com-
mercially available processors. Someone developing an auto-
mobile electronic ignition system may write their code in C, 
but might have to debug the system using a logic analyzer that 
displays the relationship between interrupt requests from 
engine sensors and the machine-level code. 
There are two other important reasons for teaching com-
puter architecture. The first reason is that computer architec-
ture incorporates a wealth of important concepts that appear 
in other areas of the CS curriculum. This point is probably 
least appreciated by computer scientists who took a course in 
architecture a long time ago and did little more than learn 
about bytes, gates, and assembly language. The second reason 
is that computer architecture covers more than the CPU; it is 
concerned with the entire computer system. Because so many 
computer users now have to work with the whole system 
(e.g. by configuring hard disks, by specifying graphics cards, 
by selecting a SCSI or FireWire interface), a course covering 
the architecture of computer systems is more a necessity than 
a luxury. 
Some computer architecture courses cover the architecture 
and organization of the processor but make relatively little 
' An embedded computer is part of a product (digital camera, cell 
phone, washing machine) that is not normally regarded as a computing 
device. The end user does not know about the computer and does not 
have to program it. 

4 
Chapter 1 Introduction to computer hardware 
reference to buses, memory systems, and high-performance 
peripherals such as graphics processors. Yet, if you scan the 
pages of journals devoted to personal/workstation comput-
ing, you will rapidly discover that much attention is focused 
on aspects of the computer system other than the CPU itself. 
Computer technology was once driven by the paperless-
office revolution with its demand for low-cost mass storage, 
sufficient processing power to rapidly recompose large docu-
ments, and low-cost printers. Today, computer technology is 
being driven by the multimedia revolution with its insatiable 
demand for pure processing power, high bandwidths, low 
latencies, and massive storage capacities. 
These trends have led to important developments in com-
puter architecture such as special hardware support for mul-
timedia applications. The demands of multimedia are being 
felt in areas other than computer architecture. Hard disks 
must provide a continuous stream of data because people can 
tolerate a degraded picture much better than a picture with 
even the shortest discontinuities. Such demands require 
efficient track-seeking algorithms, data buffering, and high-
speed, real-time error correction and detection algorithms. 
Similarly, today's high data densities require frequent recal-
ibration of tracking mechanisms due to thermal effects. Disk 
drives now include SMART technologies from the AI world 
that are able to predict disk failure before it occurs. These 
developments have as much right to be included in the archi-
tecture curriculum as developments in the CPU. 
122 Supporting the CS curriculum 
It is in the realm of software that you can most easily build a 
case for the teaching of assembly language. During a student's 
career, they will encounter abstract concepts in areas ranging 
from programming languages to operating systems to real-
time programming to AI. The foundation of many of these 
concepts lies in assembly language programming and computer 
architecture. Computer architecture provides bottom-up 
support for the top-down methodology taught in high-level 
languages. Consider some of the areas where computer 
architecture can add value to the CS curriculum. 
The operating system Computer architecture provides a 
firm basis for students taking operating system courses. In 
computer architecture students learn about the hardware 
that the operating system controls and the interaction 
between hardware and software; for example, in cache sys-
tems. Consider the following two examples of the way in 
which the underlying architecture provides support for 
operating system facilities. 
Some processors operate in either a privileged or a user 
mode. The operating system runs in the privileged or pro-
tected mode and all applications run in the user mode. This 
mechanism creates a secure environment in which the effects 
of an error in an application program can be prevented from 
crashing the operating system or other applications. Covering 
these topics in an architecture course makes the student 
aware of the support the processor provides for the operating 
system and enables those teaching operating system courses 
to concentrate more on operating system facilities than on 
the mechanics of the hardware. 
High-level languages make it difficult to access peripherals 
directly. By using an assembly language we can teach students 
how to write device drivers that directly control interfaces. 
Many real interfaces are still programmed at machine level by 
accessing registers within them. Understanding computer 
architecture and assembly language can facilitate the design 
of high-performance interfaces. 
Programming and data structures Students encounter the 
notion of data types and the effect of strong and weak data 
typing when they study high-level languages. Because 
computer architecture deals with information in its most 
primitive form, students rapidly become familiar with the 
advantages and disadvantages of weak typing. They learn the 
power that you have over the hardware by being able to apply 
almost any operations to binary data. Equally, they learn 
the pitfalls of weak typing as they discover the dangers of 
inappropriate operations on data. 
Computer architecture is concerned with both the type of 
operations that act on data and the various ways in which the 
location of an operand can be accessed in memory. Computer 
addressing modes and the various means of accessing data 
naturally lead on to the notion of pointers. Students learn 
about how pointers function at machine level and the sup-
port offered for pointers by various architectures. This aspect 
is particularly important if the student is to become a C 
programmer. 
An understanding of procedure call and parameter passing 
mechanisms is vital to anyone studying processor perform-
ance. Programming in assembly language readily demon-
strates the passing of parameters by value and by reference. 
Similarly, assembly language programming helps you to 
understand concepts such as the use of local variables and 
re-entrant programming. 
Students sometimes find the concept of recursion difficult. 
You can use an assembly language to demonstrate how recur-
sion operates by tracing through the execution of a program. 
The student can actually observe how the stack grows as 
procedures are called. 
Computer science fundamentals Computer architecture is 
awash with concepts that are fundamental to computer science 
generally and which do not appear in other parts of the 
undergraduate curriculum. A course in computer architecture 
can provide a suitable forum for incorporating fundamental 
principles in the CS curriculum. For example, a first course in 
computer architecture introduces the student to bits and 
binary encoding techniques. A few years ago much time 
would have been spent on special-purpose codes for BCD 

1.3 An overview of the book 
5 
arithmetic. Today, the professor is more likely to introduce 
error-correcting codes (important in data communications 
systems and secure storage mechanisms) and data-compression 
codes (used by everyone who has ever zipped a file or used a 
JPEG-encoded image). 
1.3 An overview of the book 
It's difficult to know just what should be included in an intro-
ductory course on computer architecture, organization, and 
hardware—and what should be excluded. Any topic can be 
expanded to an arbitrary extent; if we begin with gates and 
Boolean algebra, do we go on to semiconductor devices and 
then semiconductor physics? In this book, we cover the mater-
ial specified by typical computer curricula. However, I have 
included a wider range of material because the area of influ-
ence encompassed by the digital computer has expanded 
greatly in recent years. The major subject areas dealt with in 
this book are outlined below. 
Computer arithmetic Our system of arithmetic using the 
base 10 has evolved over thousands of years. The computer car-
ries out its internal operations on numbers represented in the 
base two. This anomaly isn't due to some magic power inher-
ent in binary arithmetic but simply because it would be uneco-
nomic to design a computer to operate in denary (base 10) 
arithmetic. At this point I must make a comment. Time and 
time again, I read in the popular press that the behavior of 
digital computers and their characteristics are due to the fact 
that they operate on bits using binary arithmetic whereas we 
humans operate on digits using decimal aridimetic. That idea 
is nonsense. Because there is a simple relationship between 
binary and decimal numbers, the fact that computers represent 
information in binary form is a mere detail of engineering. It's 
the architecture and organization of a computer that makes it 
behave in such a different way to the brain. 
Basic logic elements and Boolean algebra Today's techno-
logy determines what a computer can do. We introduce the 
basic logic elements, or gates, from which a computer is made 
up and show how these can be put together to create more 
complex units such as arithmetic units. The behavior of these 
gates determines both the way in which the computer carries 
out arithmetic operations and the way in which the func-
tional parts of a computer interact to execute a program. We 
need to understand gates in order to appreciate why the com-
puter has developed in the way it has. The operation of cir-
cuits containing gates can be described in terms of a formal 
notation called Boolean algebra. An introduction to Boolean 
algebra is provided because it enables designers to build cir-
cuits with the least number of gates. 
As well as gates, computers require devices called flip-flops, 
which can store a single binary digit. The flip-flop is the 
basic component of many memory units. We provide an 
introduction to flip-flops and their application to sequential 
circuits such as counters, timers, and sequencers. 
Computer architecture and assembly language The prim-
itive instructions that directly control the operation of a com-
puter are called machine-code instructions and are composed 
of sequences of binary values stored in memory. As program-
ming in machine code is exceedingly tedious, an aid to 
machine code programming called assembly language has 
been devised. Assembly language is shorthand permitting the 
programmer to write machine-code instructions in a simple 
abbreviated form of plain language. High-level languages 
(Java, C, Pascal, BASIC) are sometimes translated into a series 
of assembly-language instructions by a compiler as an inter-
mediate step on the way to pure machine code. This interme-
diate step serves as a debugging tool for programmers who 
wish to examine the operation of the compiler and the output 
it produces. Computer architecture is the assembly language 
programmer's view of a computer. 
Programmers writing in assembly language require a 
detailed knowledge of the architecture of their machines, 
unlike the corresponding programmers operating in high-
level languages. At this point I must say that we introduce 
assembly language to explain the operation of the central pro-
cessing unit. Apart from certain special exceptions, programs 
should be written in a high-level language whenever possible. 
Computer organization This topic is concerned with how a 
computer is arranged in terms of its building blocks (i.e. the 
logic and sequential circuits made from gates and flip-flops). 
We introduce the architecture of a simple hypothetical com-
puter and show how it can be organized in terms of func-
tional units. That is, we show how the computer goes about 
reading an instruction from memory, decoding it, and then 
executing it. 
Input/output It's no good having a computer unless it can 
take in new information (programs and data) and output the 
results of its calculations. In this section we show how 
information is moved into and out of the computer. The 
operation of three basic input/output devices is described: 
the keyboard, the display, and the printer. 
We also examine the way in which analog signals can be 
converted into digital form, processed digitally by a com-
puter, and then converted back into analog form. Until the 
mid-1990s it was uneconomical to process rapidly changing 
analog signals (e.g. speech, music, video) digitally. The advent 
of high-speed low-cost digital systems has opened up a new 
field of computing called digital signal processing (DSP). We 
introduce DSP and outline some of the basic principles. 
Memory devices A computer needs memory to hold pro-
grams, data, and any other information it may require at 
some point in the future. We look at the immediate access 
store and the secondary store (sometimes called backing 
store). An immediate access store provides a computer with 
the data it requires in approximately the same time as it takes 

6 
Chapter 1 Introduction to computer hardware 
the computer to execute one of its machine-level operations. 
The secondary store is very much slower and it takes thou-
sands of times longer to access data from a secondary store 
than from an immediate access store. However, secondary 
storage is used because it is immensely cheaper than an 
immediate access store and it is also non-volatile (i.e. the data 
isn't lost when you switch the computer off). The most pop-
ular form of secondary store is the disk drive, which relies on 
magnetizing a moving magnetic material to store data. 
Optical storage technology in the form of the CD and DVD 
became popular in the 1990s because it combines the rela-
tively fast access time of the disk with the large capacity and 
low cost of the tape drive. 
Operating systems and the computer An operating system 
coordinates all the functional parts of the computer and pro-
vides an interface for the user. We can't cover the operating 
system in detail here. However, because the operating system 
is intimately bound up with the computer's hardware, we do 
cover two of its aspects—multiprogramming and memory 
management. Multiprogramming is the ability of a computer 
to appear to run two or more programs simultaneously. 
Memory management permits several programs to operate 
as though each alone occupied the computer's memory and 
enables a computer with a small, high-speed random access 
memory and a large, low-speed serial access memory (i.e. 
hard disk) to appear as if it had a single large high-speed ran-
dom access memory. 
Computer communications Computers are networked when 
they are connected together. Networking computers has 
many advantages, not least of which is the ability to share 
peripherals such as printers and scanners. Today we have two 
types of network—the local area network (LAN), which 
interconnects computers within a building, and the wide area 
network, which interconnects computers over much greater 
distances (e.g. the Internet). Consequently, we have devoted a 
section to showing how computers communicate with each 
other. Three aspects of computer communications are exam-
ined. The first is the protocols or rules that govern the way in 
which information is exchanged between systems in an 
orderly fashion. The second is the way in which digital 
information in a computer is encoded in a form suitable for 
transmission over a serial channel, the various types of 
channel, the characteristics of the physical channel, and how 
data is reconstituted at the receiver. The third provides a 
brief overview of both local area and wide area networks. 
1.4 History of computing 
The computer may be a marvel of our age, but it has had a long 
and rich history. Writing a short introduction to computer 
history is difficult because there is so much to cover. Here we 
provide some of die milestones in the computer's development. 
1.4.1 Navigation and mathematics 
The development of navigation in the eighteenth century was 
probably the most important driving force behind auto-
mated computation. It's easy to tell how far north or south of 
the equator you are—you measure the height of the sun 
above the horizon at midday and then use the elevation to 
work out your latitude. Unfortunately, calculating your lon-
gitude relative to the prime meridian through Greenwich in 
England is very much more difficult. Longitude is determined 
by comparing your local time (obtained by observing the 
angle of the sun) with the time at Greenwich. 
The mathematics of navigation uses trigonometry, which 
is concerned with the relationship between the sides and 
angles of a triangle. In turn, trigonometry requires an accur-
ate knowledge of the sine, cosine, and tangent of an angle. 
Those who originally devised tables of sines and other math-
ematical functions (e.g. square roots and logarithms) had to 
do a lot of calculation by hand. If x is expressed in radians 
(where 2TT radians = 360°) and x<l, 
the expression for 
sin(x) can be written as an infinite series of the form 
vJ 
„5 
v7 
v2n+l 
«n(x)=x-g + f r - f f + - + (-l)-^T- f T r 
Although the calculation of sin(x) requires the summation 
of an infinite number of terms, we can obtain a reasonably 
accurate approximation to sin(x) by adding just a handful of 
terms together because x" tends towards zero as n increases 
f o r x « l . 
An important feature of the formula for sin(x) is that it 
involves nothing more than the repetition of fundamental 
arithmetic operations (addition, subtraction, multiplication, 
and division). The first term in the series is x itself. The sec-
ond term is —x3/3!, which is derived from the first term by 
multiplying it by — x2 and dividing it by 1 X 2 X 3. Each new 
term is formed by multiplying the previous term by — x2 and 
dividing it by 2n(2n +1), where n is number of the term. It 
would eventually occur to people that this process could be 
mechanized. 
1.4.2 The era of mechanical computers 
During the seventeenth century major advances were made in 
watch making; for example, in 1656 Christiaan Huygens 
designed the first pendulum clock. The art of watch making 
helped develop the gear wheels required by the first mechanical 
calculators. In 1642 the French scientist Blaise Pascal designed 
a simple mechanical adder and subtracter using gear wheels 
with 10 positions marked on them. One complete rotation of 
a gear wheel caused the next wheel on its left to move one posi-
tion (a bit like the odometer used to record an automobile's 
mileage). Pascal's most significant contribution was the use of 
a ratchet device that detected a carry (i.e. a rotation of a wheel 
vJ 
„5 
v7 
v2n+l 
= x - — + — - — + - • • + ( - 1 V — ^ 
3! 
5! 
7! 
( 
l> (2n + 1)! 
sin(x) 

from 9 to 0) and nudged the next wheel on the left one digit. 
In other words, if two wheels show 58 and the right-hand 
wheel is rotated two positions forward, it moves to the 0 posi-
tion and advances the 5 to 6 to get 60. Pascal's calculator, the 
Pascaline, could perform addition only. 
In fact, Wilhelm Schickard, rather than Pascal, is now 
generally credited with the invention of the first mechanical 
calculator. His device, created in 1623, was more advanced 
than Pascal's because it could also perform partial multiplica-
tion. Schickard died in a plague and his invention didn't 
receive the recognition it merited. Such near simultaneous 
developments in computer hardware have been a significant 
feature of the history of computer hardware. 
Within a few decades, mechanical computing devices 
advanced to the stage where they could perform addition, 
subtraction, multiplication, and division—all the operations 
required by armies of clerks to calculate the trigonometric 
functions we mentioned earlier. 
The industrial revolution and early 
control mechanisms 
If navigation provided a requirement for mechanized com-
puting, odier developments provided important steps along 
the path to the computer. By about 1800 the industrial 
revolution in Europe was well under way. Weaving was one 
of the first industrial processes to be mechanized. A weaving 
loom passes a shuttle pulling a horizontal thread to and fro 
between vertical threads held in a frame. By changing the 
color of the thread pulled by the shutde and selecting whether 
the shuttle passes in front of or behind the vertical threads, 
you can weave a particular pattern. Controlling the loom 
manually is tedious and time consuming. In 1801 Joseph 
Jacquard designed a loom that could automatically weave a 
predetermined pattern. The information necessary to control 
die loom was stored in the form of holes cut in cards—the 
presence or absence of a hole at a certain point controlled the 
behavior of the loom. Information was read by rods that 
pressed against the card and either went fhrough a hole or 
were stopped by the card. Some complex patterns required as 
many as 10 000 cards strung together in the form of a tape. 
Babbage and the computer 
Two of the most significant advances in computing were 
made by Charles Babbage, a UK madiematician born in 1792: 
his difference engine and his analytical engine. Like other 
mathematicians of his time, Babbage had to perform all 
calculations by hand and sometimes he had to laboriously 
correct errors in published mathematical tables. Living in the 
age of steam, it was quite natural that Babbage asked himself 
whether mechanical means could be applied to arithmetic 
calculations. 
The difference engine was a complex array of intercon-
nected gears and linkages that performed addition and 
1.4 History of computing 
7 
Number 
Number 
First 
Second 
squared 
difference 
difference 
1 
1 
2 
4 
3 
3 
9 
5 
2 
4 
16 
7 
2 
5 
25 
9 
2 
6 
36 
11 
2 
7 
49 
13 
2 
Table 1.1 The use of finite differences to calculate squares. 
subtraction rather like Pascal's mechanical adder. Its purpose 
was to mechanize the calculation of polynomial functions 
and automatically print the result. It was a calculator rather 
than a computer because it could carry out only a set of 
predetermined operations. 
Babbage's difference engine employed finite differences to 
calculate polynomial functions. Trigonometric functions can 
be expressed as polynomials in the form a0x + a,x' + 
a-p? + • • • The difference engine can evaluate such expres-
sions automatically. Table 1.1 demonstrates how you can use 
the method of finite differences to create a table of squares 
without having to use multiplication. The first column con-
tains the natural integers 1, 2, 3 , . . . The second column 
contains the squares of these integers (i.e. 1,4,9,...). Column 
3 contains the first difference between successive pairs of 
numbers in column 2; for example, the first value is 4 — 1 = 3, 
the second value is 9 — 4 = 5, and so on. The final column is 
the second difference between successive pairs of first differ-
ences. As you can see, the second difference is always 2. 
Suppose we want to calculate the value of 82 using finite 
differences. We simply use Table 1.1 in reverse by starting 
with the second difference and working back to the result. If 
the second difference is 2, the next first difference (after 72) is 
13 + 2 = 15. Therefore, the value of 82 is die value of 72 plus 
the first difference; diat is, 49 + 15 = 64. We have generated 
82 without using multiplication. This technique can be 
extended to evaluate many other mathematical functions. 
Babbage's difference engine project was cancelled in 1842 
because of increasing costs. He did design a simpler differ-
ence engine using 31-digit numbers to handle seventh-order 
differences, but no one was interested in financing it. In 1853 
George Scheutz in Sweden constructed a working difference 
engine using 15-digit arithmetic and fourth-order differ-
ences. Incidentally, in 1991 a team at the Science Museum in 
London used modern construction techniques to build 
Babbage's difference engine. It worked. 
Charles Babbage went on to design the analytical engine, 
which was to be capable of performing any mathematical 

8 
Chapter 1 Introduction to computer hardware 
operation automatically. This truly remarkable and entirely 
mechanical device was nothing less than a general-purpose 
computer that could be programmed. The analytical engine 
included many of the elements associated with a modern elec-
tronic computer—an arithmetic processing unit that carries 
out all the calculations, a memory that stores data, and input 
and output devices. Unfortunately, the sheer scale of the ana-
lytical engine rendered its construction, at that time, impos-
sible. However, it is not unreasonable to call Babbage the 
father of the computer because his machine incorporated 
many of the intellectual concepts at the heart of the computer. 
Babbage envisaged that his analytical engine would be 
controlled by punched cards similar to those used to control 
the operation of the Jacquard loom. Two types of punched 
card were required. Operation cards specified the sequence of 
operations to be carried out by the analytical engine and vari-
able cards specified the locations in the store of inputs and 
outputs. 
One of Babbage's collaborators was Ada Gordon2, a math-
ematician who became interested in the analytical engine when 
she translated a paper on it from French to English. When 
Babbage discovered the paper he asked her to expand the 
paper. She added about 40 pages of notes about the machine 
and provided examples of how the proposed analytical engine 
could be used to solve mathematical problems. Gordon 
worked closely with Babbage and it's been reported that she 
even suggested the use of the binary system to store data. She 
noticed that certain groups of operations are carried out over 
and over again during the course of a calculation and pro-
posed that a conditional instruction be used to force the ana-
lytical engine to perform the same sequence of operations 
many times. This action is the same as the repeat or loop func-
tion found in most of today's high-level languages. 
Gordon devised algorithms to perform the calculation of 
Bernoulli numbers, making her one of the founders of numer-
ical computation. Some regard Gordon as the world's first 
computer programmer, who was constructing algorithms a 
century before programming became a recognized discipline— 
and long before any real computers were constructed. 
Mechanical computing devices continued to be used in 
compiling mathematical tables and performing the arithmetic 
operations used by everyone from engineers to accountants 
until about the 1960s. The practical high-speed computer had 
to await the development of the electronics industry. 
1.4.3 Enabling technology— 
the telegraph 
Many of the technological developments required to con-
struct a practical computer took place at the end of the 
nineteenth century. The most important of these events was 
the invention of the telegraph. We now provide a short history 
of the development of telecommunications. 
One of the first effective communication systems was the 
optical semaphore, which passed visual signals from tower to 
tower across Europe. Claude Chappe in France developed a 
system with two arms, each of which could be in one of seven 
positions. The Chappe telegraph could send a message across 
France in about half an hour (good weather permitting). The 
telegraph was used for commercial purposes, but it also 
helped Napoleon to control his army. 
King Maximilian had seen how the French visual sema-
phore system had helped Napoleon's military campaigns and 
in 1809 he asked the Bavarian Academy of Sciences to devise 
a scheme for high-speed communication over long distances. 
Samuil T. von Sommering suggested a crude telegraph using 
35 conductors, one for each character. Sommering's tele-
graph transmits electricity from a battery down one of these 
35 wires where, at the receiver, the current is passed through 
a tube of acidified water. Passing a current through the water 
breaks it down into oxygen and hydrogen. To use the 
Sommering telegraph you detected the bubbles that appeared 
in one of the 35 glass tubes and then wrote down the cor-
responding character. Sommering's telegraph was ingenious 
but too slow to be practical. 
In 1819 Hans C. Oersted made one of the greatest discover-
ies of all time when he found that an electric current creates a 
magnetic field round a conductor. This breakthrough allowed 
you to create a magnetic field at will. In 1828 Cooke exploited 
Oersted's discovery when he invented a telegraph that used 
the magnetic field round a wire to deflect a compass needle. 
The growth of the railway networks in the early nineteenth 
century spurred the development of the telegraph because you 
had to warn stations down the line that a train was arriving. By 
1840 a 40-mile stretch between Slough and Paddington in 
London had been linked using the telegraph of Charles 
Wheatstone and William Cooke. The Wheatstone and Cooke 
telegraph used five compass needles that normally hung in a 
vertical position. The needles could be deflected by coils to 
point to the appropriate letter. You could transmit one of 
20 letters (J, C, Q, U, X, and Z were omitted). 
The first long-distance data links 
We take wires and cables for granted. In the early nineteenth 
century, plastics hadn't been invented and the only material 
available for insulation waterproofing was a type of pitch 
called asphaltum. In 1843 a form of rubber called gutta 
percha was discovered. The Atlantic Telegraph Company cre-
ated an insulated cable for underwater use containing a single 
copper conductor made of seven twisted strands, surrounded 
by gutta percha insulation and protected by a ring of 18 iron 
wires coated with hemp and tar. 
2 Ada Gordon married William King in 1835. King inherited the title 
Earl of Lovelace and Gordon became Countess of Lovelace. Gordon is 
often considered the founder of scientific computing. 

1.4 History of computing 
9 
Submarine cable telegraphy began with a cable crossing 
the English Channel to France in 1850. The cable failed after 
only a few messages had been exchanged and a more success-
ful attempt was made the following year. Transatlantic cable 
laying from Ireland began in 1857 but was abandoned when 
the strain of the cable descending to the ocean bottom caused 
it to snap under its own weight. The Atlantic Telegraph 
Company tried again in 1858. Again, the cable broke after 
only 3 miles but the two cable-laying ships managed to splice 
the two ends. The cable eventually reached Newfoundland in 
August 1858 after suffering several more breaks and storm 
damage. 
It soon became clear tliat this cable wasn't going to be a 
commercial success. The receiver used the magnetic field from 
the current in the cable to deflect a magnetized needle. 
Unfortunately, after crossing the Atlantic the signal was too 
weak to be detected reliably. The original voltage used to drive 
a current down the cable was approximately 600 V. So, they 
raised the voltage to about 2000 V to drive more current along 
the cable and improve the detection process. Unfortunately, 
such a high voltage burned through the primitive insulation, 
shorted the cable, and destroyed the first transatlantic tele-
graph link after about 700 messages had been transmitted in 
3 months. 
In England, the Telegraph Construction and Maintenance 
Company developed a new 2300-mile-long cable weighing 
9000 tons, which was three times the diameter of the failed 
1858 cable. Laying this cable required the largest ship in the 
world, the Great Eastern. After a failed attempt in 1865 a 
transatlantic link was finally established in 1866. It cost $100 
in gold to transmit 20 words across the first transatlantic 
cable at a time when a laborer earned $20/month. 
Telegraph distortion and the theory of 
transmission lines 
The telegraph hadn't been in use for very long before people 
discovered that it suffered from a problem called telegraph 
distortion. As the length of cables increased it became appar-
ent that a sharply rising pulse at the transmitter end of a cable 
was received at the far end as a highly distorted pulse with 
long rise and fall times. This distortion meant that the 1866 
transatlantic telegraph cable could transmit only eight words 
per minute. The problem was eventually handed to William 
Thomson at the University of Glasgow. 
Thomson, who later became Lord Kelvin, was one of the 
nineteenth century's greatest scientists. He published more 
than 600 papers, developed the second law of thermodynam-
ics, and created the absolute temperature scale. In 1855 
Thomson presented a paper to the Royal Society analyzing 
the effect of pulse distortion, which became the cornerstone 
of what is now called transmission line theory. The transmis-
sion line effect reduces the speed at which signals can change 
state. The cause of the problems investigated by Thomson 
lies in the physical properties of electrical conductors 
and insulators. Thomson's theories enabled engineers to 
construct data links with much lower levels of distortion. 
Thomson contributed to computing by providing the the-
ory that describes the flow of pulses in circuits, which enabled 
the development of the telegraph and telephone networks. In 
turn, the switching circuits used to route messages through 
networks were used to construct the first electromechanical 
computers. 
Developments in communications networks 
Although the first telegraph systems operated from point to 
point, the introduction of the telephone led to the develop-
ment of switching centers. First-generation switching centers 
employed a telephone operator who manually plugged a sub-
scriber's line into a line connected to the next switching center 
in the link. By the end of the nineteenth century, the infra-
structure of computer networks was already in place. 
In 1897 an undertaker called Almon Strowger was annoyed 
to find that he was not getting the trade he expected because 
the local telephone operator was connecting prospective 
clients to Strowger's competitor. So, Strowger cut out the 
human factor by inventing the automatic telephone exchange 
that used electromechanical devices to route calls between 
exchanges. When you dial a number using a rotary dial, a 
series of pulses are sent down the line to a rotary switch. If 
you dial, for example, '5', the five pulses move a switch five 
steps clockwise to connect you to line number five, which 
routes your call to the next switching center. Consequently, 
when you phoned someone using Strowger's technology die 
number you dialed determined the route your call took 
though the system. 
By the time the telegraph was well established, radio was 
being developed. James Clerk Maxwell predicted radio waves 
in 1864 following his study of light and electromagnetic 
waves. Heinrich Hertz demonstrated the existence of radio 
waves in 1887 and Guglielmo Marconi is credited with being 
the first to use radio to span the Atlantic in 1901. 
The light bulb was invented by Thomas A. Edison in 1879. 
Investigations into its properties led Ambrose Fleming to 
discover the diode in 1904. A diode is a light bulb surrounded 
by a wire mesh that allows electricity to flow only one way 
between the filament (the cathode) and the mesh (the anode). 
The flow of electrons from the cathode gave us the term 
'cathode ray tube'. In 1906 Lee de Forest modified Fleming's 
diode by placing a wire mesh between the cathode and anode. 
By changing the voltage on this mesh, it was possible to 
change the flow of current between the cathode and anode. 
This device, called a triode, could amplify signals. Without 
the vacuum tube to amplify weak signals, modern electronics 
would have been impossible. The term electronics refers to 
circuits with amplifying or active devices such as tubes or tran-
sistors. The first primitive computers using electromechanical 

10 
Chapter 1 Introduction to computer hardware 
devices did not use vacuum tubes and, therefore, these 
computers were not electronic computers. 
The telegraph, telephone, and vacuum tube were all steps 
on the path to the development of the computer and, later, 
computer networks. As each of these practical steps was 
taken, there was a corresponding development in the accom-
panying theory (in the case of radio, the theory came before 
the discovery). 
Typewriters, punched cards, and tabulators 
Another important part of computer history is the humble 
keyboard, which is still the prime input device of most 
personal computers. As early as 1711 Henry Mill, an 
Englishman, described a mechanical means of printing text 
on paper a character at a time. In 1829 the American William 
Burt was granted the first US patent for a typewriter, 
although his machine was not practical. It wasn't until 1867 
that three Americans, Christopher Sholes, Carlos Glidden, 
and Samuel Soule, invented their Type-Writer, the forerun-
ner of the modern typewriter. One of the problems encoun-
tered by Sholes was the tendency of his machine to jam when 
digraphs such as 'th' and 'er' were typed. Hitting the't' and 'h' 
keys at almost the same time caused the letters 't' and 'h' to 
strike the paper simultaneously and jam. His solution was to 
arrange the letters on the keyboard to avoid the letters of 
digraphs being located side by side. This layout has continued 
until today and is now described by the sequence of the first 
six letters on the left of the top row—QWERTY. Because the 
same digraphs do not occur in different languages, the layout 
of a French keyboard is different to that of an English key-
board. It is reported that Sholes made it easy to type 'Type-
Writer' by putting all these characters on the same row. 
Another enabling technology that played a key role in the 
development of the computer was the tabulating machine, a 
development of the mechanical calculator that processes data 
on punched cards. One of the largest data processing opera-
tions carried out in the USA during the nineteenth century 
was the US census. A census involves taking the original data, 
sorting and collating it, and tabulating the results. 
In 1879 Herman Hollerith became involved in the evaluation 
of the 1880 US Census data. He devised an electric tabulating 
system that could process data stored on cards punched by 
clerks from the raw census data. Hollerith's electric tabulating 
machine could read cards, process the information on the cards, 
and then sort them. The tabulator helped lay the foundations of 
the data processing industry. 
Three threads converged to make the computer possible: 
Babbage's calculating machines, which performed arithmetic 
calculations; communications technology, which laid the 
foundations for electronics and even networking; and the 
tabulator because it and the punched card media provided a 
means of controlling machines, inputting data into them, 
and storing information. 
1.4.4 The first electromechanical 
computers 
The forerunner of today's digital computers used electro-
mechanical components called relays, rather than electronic 
circuits such as vacuum tubes and transistors. A relay is con-
structed from a coil of wire wound round an iron cylinder. 
When a current flows through the coil, it generates a mag-
netic field that causes the iron to act like a magnet. A flat 
springy strip of iron is located close to the iron cylinder. 
When the cylinder is magnetized, the iron strip is attracted, 
which, in turn, opens or closes a switch. Relays can perform 
any operation that can be carried out by the logic gates mak-
ing up today's computers. You cannot construct fast com-
puters from relays because they are far too slow, bulky, and 
unreliable. However, the relay did provide a technology that 
bridged the gap between the mechanical calculator and the 
modern electronic digital computer. 
One of the first electromechanical computers was built by 
Konrad Zuse in Germany. Zuse's Z2 and Z3 computers were 
used in the early 1940s to design aircraft in Germany. The 
heavy bombing at the end of the Second World War 
destroyed Zuse's computers and his contribution to the 
development of the computer was ignored for many years. 
He is mentioned here to demonstrate that the notion of a 
practical computer occurred to different people in different 
places. The Z3 was completed in 1941 and was the World's 
first functioning programmable mechanical computer. 
Zuse's Z4 computer was finished in 1945, was later taken to 
Switzerland, and was used at the Federal Polytechnical 
Institute in Zurich until 1955. 
As Zuse was working on his computer in Germany, Howard 
Aiken at Harvard University constructed his Harvard Mark I 
computer in 1944 with both financial and practical support 
from IBM. Aiken was familiar with Babbage's work and his 
electromechanical computer, which he first envisaged in 1937, 
operated in a similar way to Babbage's proposed analytical 
engine. The original name for the Mark I was the Automatic 
Sequence Controlled Calculator, which, perhaps, better 
describes its nature. 
Aiken's machine was a programmable calculator that was 
used by the US Navy until the end of the Second World 
War. Just like Babbage's machine, the Mark I used decimal 
counter wheels to implement its main memory consisting of 
72 words of 23 digits plus a sign. The program was stored on 
a paper tape (similar to Babbage's punched cards), although 
operations and addresses (i.e. data) were stored on the same 
tape. Input and output operations used punched cards or an 
electric typewriter. Because the Harvard Mark I treated data 
and instructions separately, the term Harvard architecture is 
now applied to any computer with separate paths for data 
and instructions. The Harvard Mark I didn't support condi-
tional operations and therefore is not strictly a computer. 

1.4 History of computing 
11 
However, it was later modified to permit multiple paper tape 
readers with a conditional transfer of control between 
the readers. 
1.4.5 The first mainframes 
Relays have moving parts and can't operate at very high 
speeds. It took the invention of the vacuum tube by John A. 
Fleming and Lee de Forest to make possible the design of 
high-speed electronic computers. John V. Atanasoff is now 
credited with the partial construction of the first completely 
electronic computer. Atanasoff worked with Clifford Berry at 
Iowa State College on their computer from 1937 to 1942. 
Their machine used a 50-bit binary representation of num-
bers and was called the ABC (Atanasoff-Berry Computer). It 
was designed to solve linear equations and wasn't a general 
purpose computer. Atanasoff and Berry abandoned their 
computer when they were assigned to other duties because of 
the war. 
ENIAC 
The first electronic general purpose digital computer was 
John W. Mauchly's ENIAC (Electronic Numerical Integrator 
and Calculator), completed in 1945 at the University of 
Pennsylvania. ENIAC was intended for use at the Army 
Ordnance Department to create firing tables that relate the 
range of a field gun to its angle of elevation, wind conditions, 
and so on. For many years, ENIAC was regarded as the first 
electronic computer, although credit was later given to 
Atanasoff and Berry because Mauchly had visited Atanasoff 
and read his report on the ABC machine. 
ENIAC used 17 480 vacuum tubes and weighed about 301. 
ENIAC was a decimal machine capable of storing 20 10-digit 
decimal numbers. IBM card readers and punches imple-
mented input and output operations. ENIAC was pro-
grammed by means of a plug board that looked like an old 
pre-automatic telephone switchboard; that is, a program was 
set up manually by means of wires. In addition to these wires, 
the ENIAC operator had to manually set up to 6000 muti-
position mechanical switches. Programming ENIAC was 
very time consuming and tedious. 
ENIAC did not support dynamic conditional operations 
(e.g. IF . . . THEN). An operation could be repeated a fixed 
number of times by hard wiring the loop counter to an 
appropriate value. Because the ability to make a decision 
depending on the value of a data element is vital to the 
operation of all computers, ENIAC was not a computer in 
today's sense of the word. It was an electronic calculator. 
John von Neumann, EDVAC and IAS 
The first US computer to use the stored program concept was 
EDVAC (Electronic Discrete Variable Automatic Computer). 
EDVAC was designed by some of the same team that designed 
the ENIAC at the Moore School of Engineering at the 
University of Pennsylvania. 
John von Neumann, one of the leading mathematicians of 
his age, participated in EDVAC's design. He wrote a docu-
ment entitled 'First draft of a report on the EDVAC, which 
compiled the results of various design meetings. Before von 
Neumann, computer programs were stored either mechan-
ically or in separate memories from the data used by the pro-
gram. Von Neumann introduced the concept of the stored 
program—an idea so commonplace today that we take it for 
granted. In a stored program von Neumann machine both the 
program that specifies what operations are to be carried out 
and the data used by the program are stored in the same 
memory. The stored program computer consists of a memory 
containing instructions coded in binary form. The control 
part of the computer reads an instruction from memory, 
carries it out, then reads the next instruction, and so on. 
Although EDVAC is generally regarded as the first stored pro-
gram computer, this is not stricdy true because data and 
instructions did not have a common format and were not 
interchangeable. 
EDVAC promoted the design of memory systems. The 
capacity of EDVAC's mercury delay line memory was 1024 
words of 44 bits. A mercury delay line operates by converting 
data into pulses of ultrasonic sound that continuously retic-
ulate in a long column of mercury in a tube. 
EDVAC was not a great commercial success. Its construc-
tion was largely completed by April 1949, but it didn't run its 
first applications program until October 1951. Because of its 
adoption of the stored program concept, EDVAC became a 
topic in the first lecture course given on computers. These 
lectures took place before EDVAC was actually constructed. 
Another important early computer was IAS constructed by 
von Neumann and his colleagues at the Institute for 
Advanced Studies in Princeton. IAS is remarkably similar to 
modern computers. Main memory was IK words and a mag-
netic drum was used to provide 16K words of secondary stor-
age. The magnetic drum was the forerunner of today's disk 
drive. Instead of recording data on the flat platter found in a 
hard drive, data was stored on the surface of a rotating drum. 
In the late 1940s the Whirlwind computer was produced 
at MIT for the US Air Force. This was the first computer 
intended for real-time information processing. It employed 
ferrite-core memory (the standard form of mainframe mem-
ory until the semiconductor integrated circuit came along in 
the late 1960s). A ferrite core is a tiny bead of a magnetic mar-
tial diat can be magnetized clockwise or counterclockwise to 
store a one or a zero. Ferrite core memory is no longer widely 
used today, although the term remains in expressions such as 
core dump, which means a printout of the contents of a region 
of memory. 
One of the most important centers of early computer 
development in the 1940s was Manchester University in 

12 
Chapter 1 Introduction to computer hardware 
England. In 1948 Tom Kilburn created a prototype computer 
called the Manchester Baby. This was a demonstration 
machine that tested the concept of the stored program com-
puter and the Williams store, which stored data on the surface 
of a cathode ray tube. Some regard the Manchester Baby as 
the world's first true stored program computer. 
IBM's place in computer history 
No history of the computer can neglect the giant of the com-
puter world, IBM, which has had such an impact on the 
computer industry. Although IBM grew out of the Computing-
Tabulating-Recording (C-T-R) Company founded in 1911, 
its origin dates back to the 1880s. The C-T-R Company was 
the result of a merger between the International Time 
Recording (ITR) Company, the Computing Scale Company 
of America, and Herman Hollerith's Tabulating Machine 
Company (founded in 1896). In 1914 Thomas J. Watson, 
Senior, left the National Cash Register Company to join the 
C-T-R company and soon became President. In 1917, a 
Canadian unit of the C-T-R company called International 
Business Machines Co. Ltd was set up. Because this name was 
so well suited to the C-T-R company's role, they adopted it 
for the whole organization in 1924. IBM bought Electromatic 
Typewriters in 1933 and the first IBM electric typewriter was 
marketed 2 years later. 
IBM's first contact with computers was via its relationship 
with Aiken at Harvard University. In 1948 Watson Senior at 
IBM gave the order to construct the Selective Sequence 
Control Computer. Although this was not a stored program 
computer, it was IBM's first step from the punched card 
tabulator to the computer. 
Thomas. J. Watson, Junior, was responsible for building the 
Type 701 EDPM (Electronic Data Processing Machine) in 
1953 to convince his father that computers were not a threat 
to IBM's conventional business. The 700 series was successful 
and dominated the mainframe market for a decade. In 1956 
IBM launched a successor, the 704, which was the world's first 
supercomputer. The 704 was largely designed by Gene 
Amdahl who later founded his own supercomputer company 
in the 1990s. 
IBM's most important mainframe was the System/360, 
which was first delivered in 1965. The importance of the 
32-bit System/360 is that it was a member of a series of com-
puters, each with the same architecture (i.e. programming 
model) but with different performance; for example, the 
System/360 model 91 was 300 times faster than the model 20. 
IBM developed a common operating system, OS/360, for their 
series. Other manufactures built their own computers diat 
were compatible with System/360 and thereby began the slow 
process towards standardization in the computer industry. 
In 1960 the Series/360 model 85 became the first computer 
to implement cache memory. Cache memory keeps a copy of 
frequently used data in very high-speed memory to reduce 
the number of accesses to the slower main store. Cache 
memory has become one of the most important features of 
today's high performance systems. 
In August 1980 IBM became the first major manufacturer 
to market a PC. IBM had been working on a PC since about 
1979 when it was becoming obvious that IBM's market would 
eventually start to come under threat from the PC manufac-
turers such as Apple and Commodore. IBM not only sold 
mainframes and personal computers—by the end of 1970s 
IBM had introduced the floppy disk, computerized super-
market checkouts, and the first automatic teller machines. 
1.4.6 The birth of transistors, ICs, and 
microprocessors 
Since the 1940s computer hardware has become smaller and 
faster. The power-hungry and unreliable vacuum tube was 
replaced by the smaller, reliable transistor in the 1950s. The 
transistor plays the same role as a thermionic tube; the only 
real difference is that a transistor switches a current flowing 
through a crystal rather than a beam of electrons flowing 
through a vacuum. The transistor was invented by William 
Shockley, John Bardeen, and Walter Brattain at AT8cT's Bell 
Lab in 1948. 
If you can put one transistor on a slice of silicon, you can 
put two or more transistors on the same piece of silicon. The 
idea occurred to Jack St Clair Kilby at Texas Instruments 
in 1958. Kilby built a working model and filed a patent 
early in 1959. In January of 1959, Robert Noyce at Fairchild 
Semiconductor was also thinking of the integrated circuit. He 
too applied for a patent and it was granted in 1961. Today, 
both Noyce and Kilby are regarded as the joint inventors 
ofthelC. 
The minicomputer era 
The microprocessor was not directly derived from the main-
frame computer. Between the mainframe and the micro-
processor lies the minicomputer, a cut-down version of the 
mainframe, which appeared in the 1960s. By the 1960s many 
departments of computer science could afford their own 
minicomputers and a whole generation of students learned 
computer science from PDP-1 Is and NOVAs in the 1960s and 
1970s. Some of these minicomputers were used in real-time 
applications (i.e. applications in which the computer has to 
respond to changes in its inputs within a specified time). 
One of the first minicomputers was Digital Equipment 
Corporation's PDP-5, introduced in 1964. This was followed 
by the PDP-8, in 1966 and the very successful PDP-11, in 
1969. Even the PDP-11 would be regarded as a very basic 
machine by today's standards. Digital Equipment built on 
their success with the PDP-11 series and introduced their 
VAX architecture in 1978 with the VAX-11/780, which 
dominated the minicomputer world in the 1980s. The VAX 

1.4 History of computing 
13 
EARLY MICROPROCESSOR SPINOFFS 
The first two major microprocessors were the 8080 and 
the 6800 from Intel and Motorola, respectively. Other 
microprocessor manufacturers emerged when engineers 
left Intel and Motorola to start their own companies. 
Federico Faggin, one of the founders of Intel, left the 
company and founded Zilog in 1974. Zilog made the 
Z80, which was compatible with Intel's 8080 at the 
machine-code level. The Z80 has a superset of the 8080's 
instructions. 
A group of engineers left Motorola to form MOS 
Technologies in 1975. They created the 6502 microprocessor, 
which was similar to the 6800 but not software compatible 
with it. The 6502 was the first low-cost microprocessor and 
was adopted by Apple and several other early PCs. 
range was replaced by the 64-bit Alpha architecture (a high-
performance microprocessor) in 1991. The Digital Equipment 
Corporation, renamed Digital, was taken over by Compaq 
in 1998. 
Microprocessor and the PC 
Credit for creating the world's first microprocessor, the 4040, 
goes to Ted Hoff and Fagin at Intel. Three engineers from 
Japan worked with Hoff to implement a calculator's digital 
logic circuits in silicon. Hoff developed a general purpose 
computer that could be programmed to carry out calculator 
functions. Towards the end of 1969 the structure of a pro-
grammable calculator had emerged. The 4004 used about 
2300 transistors and is considered the first general purpose 
programmable microprocessor, even though it was only a 
4-bit device. 
The 4004 was rapidly followed by the 8-bit 8008 micro-
processor, which was originally intended for a CRT applica-
tion. By using some of the production techniques developed 
for the 4004, Intel was able to manufacture the 8008 as early 
as March 1972. The 8008 was soon replaced by a better 
version, the first really popular general purpose 8-bit micro-
processor, the 8080 (in production in early 1974). Shortly 
after the 8080 went into production, Motorola created its 
own competitor, the 8-bit 6800. 
Six months after the 8008 was introduced, the first ready-
made computer based on the 8008, the Micral, was designed 
and built in France. The term microcomputer was coined to 
refer to the Micral, although the Micral was not successful in 
the USA. In January 1975 Popular Electronics magazine pub-
lished an article on microcomputer design by Ed Roberts 
who had a small company called MITS. Roberts' computer 
was called Altair and was constructed from a kit. 
Although the Altair was intended for hobbyists, it had a 
significant impact and sold 2000 kits in its first year. In March 
1976, Steve Wozniak and Steve Jobs designed a 6502-based 
computer, which they called the Apple 1. A year later in 1977 
they created the Apple II with 16 kbytes of ROM, 4 kbytes of 
RAM, and a color display and keyboard. Although unsoph-
isticated, this was the first practical PC. 
As microprocessor technology improved, it became pos-
sible to put more and more transistors on larger and larger 
chips of silicon. Microprocessors of the early 1980s were not 
only more powerful than their predecessors in terms of the 
speed at which they could execute instructions, they were also 
more sophisticated in terms of the facilities they offered. Intel 
took the core of their 8080 microprocessor and converted it 
from an 8-bit into a 16-bit machine, the 8086. Motorola did 
not extend their 8-bit 6800 to create a 16-bit processor. 
Instead, they started again and did not attempt to achieve 
either object or source code compatibility with earlier 
processors. By beginning with a clean slate, Motorola was 
able to create a 32-bit microprocessor with an exceptionally 
clean architecture in 1979. 
Several PC manufacturers adopted the 68K; Apple used it 
in the Macintosh and it was incorporated in the Atari and 
Amiga computers. All three of these computers were regarded 
as technically competent and had many very enthusiastic 
followers. The Macintosh was sold as a relatively high-priced 
black box with die computer, software, and peripherals from a 
single source. This approach could not compete widi the IBM 
PC, launched in 1981, with an open system architecture that 
allowed the user to purchase hardware and software from the 
supplier with the best price. The Atari and Amiga computers 
suffered because they had the air of the games machine. 
Although the Commodore Amiga in 1985 had many of the 
hallmarks of a modern multimedia machine, it was derided 
as a games machine because few then grasped the importance 
of advanced graphics and high-quality sound. 
The 68K developed into the 68020, 68030, 68040, and 
68060. Versions were developed for the embedded processor 
market and Motorola played no further role in the PC market 
until Apple adopted Motorola's PowerPC processor. The 
PowerPC came from IBM and was not a descendent of the 
68K family. 
Many fell in love with the Apple Mac. It was a sophisticated 
and powerful PC, but not a great commercial success. Apple's 
commercial failure demonstrates that those in the semi-
conductor industry must realize that commercial factors 
are every bit as important as architectural excellence and 
performance. Apple failed because their processor, from 
hardware to operating system, was proprietary. Apple didn't 
publish detailed hardware specifications or license their BIOS 
and operating system. IBM adopted open standards and 
anyone could build a copy of the IBM PC. Hundreds of 
manufacturers started producing parts of PCs and an entire 

14 
Chapter 1 Introduction to computer hardware 
industry sprang up. You could buy a basic system from one 
place, a hard disk from another, and a graphics card from yet 
another supplier. By publishing standards for the PC's bus, 
anyone could create a peripheral for the PC. What IBM lost in 
the form of increased competition, they more than made up 
for in the rapidly expanding market. IBM's open standard 
provided an incentive for software writers to generate soft-
ware for the PC market. 
The sheer volume of PCs and their interfaces (plus the 
software base) pushed PC prices down and down. The Apple 
was perceived as over-priced. Even though Apple adopted 
the PowerPC, it was too late and Apple's role in the PC world 
was marginalized. However by 2005, cut-throat competition 
from PC manufacturers was forcing IBM to abandon its PC 
business, whereas Apple was flourishing in a niche market 
that rewarded style. 
A major change in direction in computer architecture took 
place in the 1980s when the RISC or Reduced Instruction Set 
Computer first appeared. Some observers expected the RISC to 
sweep away all CISC processors like the 8086 and 68K families. 
It was the work carried out by David Paterson at the 
University of Berkley in the early 1980s that brought the RISC 
philosophy to a wider audience. Paterson was also respons-
ible for coining the term 'RISC' in 1980. The Berkeley RISC 
was constructed at a university (like many of the first main-
frames such as EDSAC) and required only a very tiny fraction 
of the resources consumed by these early mainframes. Indeed, 
the Berkeley RISC is hardly more than an extended graduate 
project. It took about a year to design and fabricate the RISC I 
in silicon. By 1983 the Berkeley RISC II had been produced 
and that proved to be both a testing ground for RISC ideas 
and the start of a new industry. Many of the principles of 
RISC design were later incorporated in Intel's processors. 
1.4.7 Mass computing and the rise of 
the Internet 
The Internet and digital multimedia have driven the evolu-
tion of the PC. The Internet provides interconnectivity and 
the digital revolution has extended into sound and vision. 
The cassette-based personal stereo system has been displaced 
by the minidisk and the MP3 players with solid state memory. 
The DVD with its ability to store an entire movie on a single 
disk first became available in 1996 and by 1998 over one 
million DVD players had been sold in the USA. The digital 
video camera that once belonged to the world of the profes-
sional filmmaker is now available to anyone with a modest 
income. 
All these applications have had a profound effect on the 
computer world. Digital video requires vast amounts of stor-
age. Within 5 years, low-cost hard disk capacities grew from 
about 1 Gbyte to 400 Gbytes or more. The DVD uses very 
sophisticated signal processing techniques that require very 
high-performance hardware to process the signals in real-
time. The MP3 player requires a high-speed data link to 
download music from the Internet. 
The demand for increasing reality in video games and real-
time image processing has spurred development in special-
purpose video subsystems. Video processing requires the 
ability to render images, which means drawing vast numbers 
of polygons on the screen and filling them with a uniform 
color. The more polygons used to compose an image, the 
more accurate the rendition of the image. 
The effect of the multimedia revolution had led to the com-
moditization of the PC, which is now just another commodity 
like a television or a stereo player. Equally, the growth of multi-
media has forced the development of higher speed processors, 
low-cost high-density memory systems, multimedia-aware 
operating systems, data communications, and new processor 
architectures. 
The Internet revolution 
Just as the computer itself was the result of a number of inde-
pendent developments (the need for automated calculation, 
the theoretical development of computer science, the 
enabling technologies of communications and electronics, 
the keyboard and data processing industries), the Internet 
was the fruit of a number of separate developments. 
The principal ingredients of the Internet are communica-
tions, protocols, and hypertext. Communications systems 
have been developed throughout human history as we have 
already pointed out when discussing the enabling technology 
behind the computer. The USA's Department of Defense cre-
ated a scientific organization, ARPA (Advanced Research 
Projects Agency) in 1958 at the height of the Cold War. ARPA 
had some of the characteristics of the Manhattan project, 
which had preceded it during the Second World War. A large 
group of talented scientists was assembled to work on a pro-
ject of national importance. From its early days ARPA con-
centrated on computer technology and communications 
systems; moreover, ARPA was moved into the academic area 
which meant that it had a rather different ethos from that of 
the commercial world because academics cooperate and 
share information. 
One of the reasons why ARPA concentrated on networking 
was the fear that a future war involving nuclear weapons 
would begin with an attack on communications centers lim-
iting the capacity to respond in a coordinated manner. By 
networking computers and ensuring that a message can take 
many paths through the network to get from its source to its 
destination, the network can be made robust and able to cope 
with the loss of some of its links of switching centers. 
In 1969 ARPA began to construct a testbed for networking, 
a system that linked four nodes: University of California at 
Los Angeles, SRI (in Stanford), University of California at 
Santa Barbara, and University of Utah. Data was sent in the 

1.5 The digital computer 
15 
form of individual packets or frames rather than as complete 
end-to-end messages. In 1972 ARPA was renamed DARPA 
(Defense Advances Research Projects Agency). 
In 1973 the TCP/IP (transmission control protocol/Internet 
protocol) was developed at Stanford; this is the set of rules that 
govern the routing of a packet through a computer network. 
Another important step on the way to the Internet was Robert 
Metcalfe's development of the Ethernet, which enabled com-
puters to communicate with each other over a local area net-
work based on a low-cost cable. The Ethernet made it possible 
to link computers in a university together and the ARPANET 
allowed the universities to be linked together. Ethernet was, 
however, based on techniques developed during the construc-
tion of the University of Hawaii's radio-based packet-switching 
ALOHAnet, another ARPA-funded project. 
Up to 1983 ARPANET users had to use a numeric IP 
address to access other users on the Internet. In 1983 the 
University of Wisconsin created the Domain Name System 
(DNS), which routed packets to a domain name rather than 
an IP address. 
The World's largest community of physicists is at CERN in 
Geneva. In 1990 Tim Berners-Lee implemented a hypertext-
based system to provide information to other the members of 
the high-energy physics community. This system was 
released by CERN in 1993 as the World-Wide Web (WWW). 
In the same year, Marc Andreessen at the University of Illinois 
developed a graphical user interface to the WWW, a browser 
called Mosaic. All that the Internet and the WWW had to do 
now was to grow. 
1.5 The digital computer 
Before beginning the discussion of computer hardware 
proper, we need to say what a computer is and to define a few 
terms. If ever an award were to be given to those guilty of mis-
information in the field of computer science, it would go to 
the creators of HAL in 2001, R2D2 in Star Wars, K9 in Doctor 
Who, and Data in Star Trek. These fictional machines have 
generated the popular myth that a computer is a reasonably 
close approximation to a human brain, which stores an infinite 
volume of data. 
The reality is a little more mundane. A computer is a 
machine that takes in information from the outside world, 
processes it according to some predetermined set of opera-
tions, and delivers the processed information. This definition 
of a computer is remarkably unhelpful, because it attempts to 
define the word computer in terms of the equally complex 
words information, operation, and process. Perhaps a better 
approach is to provide examples of what computers do by 
looking at the role of computers in data processing, numerical 
computation (popularly called number crunching), work-
stations, automatic control systems, and electronic systems. 
1.5.1 The PC and workstation 
The 1980s witnessed two significant changes in computing— 
the introduction of the PC and the workstation. PCs bring 
computing power to people in offices and in their own 
homes. Although primitive PCs have been around since the 
mid 1970s, the IBM PC and Apple Macintosh transformed 
the PC from an enthusiast's toy into a useful tool. Software 
such as word processors, databases, and spreadsheets revolu-
tionized the office environment, just as computer-aided 
design packages revolutionized the industrial design envir-
onment. Today's engineer can design a circuit and simulate 
its behavior using one software package and then create a lay-
out for a printed circuit board (PCB) with another package. 
Indeed, the output from the PCB design package may be 
suitable for feeding directly into the machine that actually 
makes the PCBs. 
In the third edition of this book in 1999 I said 
Probably the most important application of the personal computer 
is in word processing.. . Today's personal computers have immensely 
sophisticated word processing packages that create a professional-
looking result and even include spelling and grammar checkers to 
remove embarrassing mistakes. When powerful personal computers 
are coupled to laser printers, anyone can use desktop publishing 
packages capable of creating manuscripts that were once the 
province of the professional publisher. 
Now, all that's taken for granted. Today's PCs can take video 
from your camcorder, edit it, add special effects, and then burn 
it to a DVD that can be played on any home entertainment 
system. 
Although everyone is familiar with the PC, the concept of 
the workstation is less widely understood. A workstation can 
be best thought of as a high-performance PC that employs 
state-of-the-art technology and is normally used in industry. 
Workstations have been produced by manufacturers such as 
Apollo, Sun, HP, Digital, Silicon Graphics, and Xerox. They 
share many of the characteristics of PCs and are used by 
engineers or designers. When writing the third edition, I 
stated that the biggest difference between workstations and 
PCs was in graphics and displays. This difference has all but 
vanished with the introduction of high-speed graphics cards 
and large LCD displays into the PC world. 
1.5.2 The computer as a data processor 
The early years of computing were dominated by the main-
frame, which was largely used as a data processor. Figure 1.1 
describes a computer designed to deal with the payroll of a 
large factory. We will call the whole thing a computer, in 
contrast with those who would say that the CPU (central 
processing unit) is the computer and all the other devices 
are peripherals. Inside the computer's immediate access 
memory is a program, a collection of primitive machine-code 

16 
Chapter 1 Introduction to computer hardware 
Display 
Computer (central processing unit) 
Disk drives 
Figu re 1.1 The computer as a 
data processor. 
operations, whose purpose is to calculate an employee's pay 
based on the number of hours worked, the basic rate of pay, 
and the overtime rate. Of course, this program would also 
deal with tax and any other deductions. 
Because the computer's immediate access memory is relat-
ively expensive, only enough is provided to hold the program 
and the data it is currently processing. The mass of informa-
tion on the employees is normally held in secondary store as 
a disk file. Whenever the CPU requires information about a 
particular employee, the appropriate data is copied from the 
disk and placed in the immediate access store. The time taken 
to perform this operation is a small fraction of a second but is 
many times slower than reading from the immediate access 
store. However, the cost of storing information on disk is very 
low indeed and this compensates for its relative slowness. 
The tape transport stores data more cheaply than the disk 
(tape is called tertiary storage). Data on the disks is copied 
onto tape periodically and the tapes stored in the basement 
for security reasons. Every so often the system is said to crash 
and everything grinds to a halt. The last tape dump can be 
reloaded and the system assumes the state it was in a short 
time before the crash. Incidentally, the term crash had the 
original meaning of a failure resulting from a read/write head 
in a disk drive crashing into the rotating surface of a disk and 
physically damaging the magnetic coating on its surface. 
The terminals (i.e. keyboard and display) allow operators 
to enter data directly into the system. This information could 
be the number of hours an employee has worked in the cur-
rent week. The terminal can also be used to ask specific ques-
tions, such as 'How much tax did Mr XYZ pay in November?' 
To be a little more precise, the keyboard doesn't actually ask 
questions but it allows the programmer to execute a program 
containing the relevant question. The keyboard can be used 
to modify the program itself so that new facilities may be 
added as the system grows. Computers found in data process-
ing are often characterized by their large secondary stores and 
their extensive use of printers and terminals. 
1.5.3 The computer as a numeric 
processor 
Numeric processing or number crunching refers to computer 
applications involving a very large volume of mathematical 
operations—sometimes billions of operations per job. 
Computers used in numeric processing applications are fre-
quently characterized by powerful and very expensive CPUs, 
very high-speed memories, and relatively modest quantities 
of input/output devices and secondary storage. Some super-
computers are constructed from large arrays of microproces-
sors operating in parallel. 
Most of the applications of numeric processing are best 
described as scientific. For example, consider the application 
of computers to the modeling of the processes governing the 
weather. The atmosphere is a continuous, three-dimensional 
medium composed of molecules of different gases. The sci-
entist can't easily deal with a continuous medium, but can 
make the problem more tractable by considering the atmo-
sphere to be composed of a very large number of cubes. Each 
of these cubes is considered to have a uniform temperature, 
density, and pressure. That is, the gas making up a cube shows 
no variation whatsoever in its physical properties. Variations 
exist only between adjacent cubes. A cube has six faces and 
the scientist can create a model of how the cube interacts with 
each of its six immediate neighbors. 
Printer 
Display 
. Line printer 
Tape drive 
Keyboard 
Plotter 

1.5 The digital computer 
17 
The scientist may start by assuming that all cubes are 
identical (there is no initial interaction between cubes) and 
then consider what happens when a source of energy, the sun, 
is applied to the model. The effect of each cube on its neigh-
bor is calculated and the whole process is repeated cyclically 
(iteration). In order to get accurate results, the size of the 
cubes should be small, otherwise the assumption that the 
properties of the air in the cube are uniform will not be valid. 
Moreover, the number of iterations needed to get the results 
to converge to a steady-state value is often very large. 
Consequently, this type of problem often requires very long 
runs on immensely powerful computers, or supercomputers 
as they are sometimes called. The pressure to solve complex 
scientific problems has been one of the major driving forces 
behind the development of computer architecture. 
Numeric processing also pops up in some real-time 
applications of computers. Here, the term real-time indicates 
that the results of a computation are required within a given 
time. Consider the application of computers to air-traffic 
control. A rotating radar antenna sends out a radio signal that 
is echoed back from a target. Because radio waves travel at a 
fixed speed (the speed of light), radar can be used to measure 
the bearing and distance (range) of each aircraft. At time t, 
target iat position Pu returns an echo giving its range ru„ and 
bearing bit. Unfortunately, because of the nature of radar 
receivers, a random error is added to the value of each echo 
from a target. 
The computer obtains data from the radar receiver for n 
targets, updated p times a minute. From this raw data that 
is corrupted by noise, the computer computes the position of 
each aircraft and its track and warns air traffic control of 
possible conflicts. All this requires considerable high-speed 
numerical computation. 
Supercomputers are also used by the security services to 
crack codes and to monitor telecommunications traffic for 
certain words and phrases. 
1.5.4 The computer in automatic control 
The majority of computers are found neither in data process-
ing nor in numeric processing activities. The advent of the 
microprocessor put the computer at the heart of many auto-
matic control systems. When used as a control element, the 
computer is embedded in a larger system and is invisible to the 
observer. By invisible we mean that you may not be aware of 
the existence of the computer. Consider a computer in a 
pump in a gas station that receives cash in a slot and delivers 
a measured amount of fuel. The user doesn't care whether the 
pump is controlled by a microprocessor or by a clockwork 
mechanism, as long as it functions correctly. 
A good example of a computer in automatic control is an 
aircraft's automatic landing system, illustrated in Fig. 1.2. The 
aircraft's position (height, distance from touch down, and 
Figure 1.2 The computer as a control element in a flight 
control system. 
distance off the runway centerline) and speed are determined 
by radio techniques in conjunction with a ground-based 
instrument-landing system. Information about the aircraft's 
position is fed to the three computers, which, individually, 
determine the error in the aircraft's course. The error is 
the difference between the aircraft's measured position and the 
position it should be in. The output from the computer is the 
signals required to move the aircraft's control surfaces 
(ailerons, elevator, and rudder) and adjust the engine's thrust. 
In this case the computer's program is held in ROM, a mem-
ory that can be read from but not written to. Once the 
program to land the aircraft has been developed, it requires 
only occasional modification. 
The automatic-landing system requires three computers, 
each working on the same calculation with the same inputs. 
The outputs of the computers are fed to a majority logic 
circuit called a voting network. If all three inputs to the major-
ity logic circuit are the same, its output is identical to its 
inputs. If one computer fails, the circuit selects its output to 
be the same as that produced by the two good computers. 
This arrangement is called triple modular redundancy and 
makes the system highly reliable. 
Another example of the computer as a controller can be 
found in the automobile. Car manufacturers want to increase 
the efficiency and performance of the internal combustion 
engine and reduce the emission of harmful combustion 
products. Figure 1.3 illustrates the structure of a computer-
ized fuel injection system that improves the performance of 
an engine. The temperature and pressure of the air, the angle 
of the crankshaft, and several other variables have to be meas-
ured thousands of times a second. These input parameters 
are used to calculate how much fuel should be injected into 
each cylinder. 
The glass cockpit provides another example of the computer 
as a controller. Until the mid 1980s the flight instrumentation 
of commercial aircraft was almost entirely electromechanical. 
Aileron control (roll) 
| 
Rudder control (yaw) 
Elevator control (pitch)  
Majority ~~" 
logic 
—i 
netwok 
T — i — i - * 
J
Position 
sensors 
I 
1__^ 
1 
CPU 
CPU 
CPU 
A
B
C 
1 
1 
1 

18 
Chapter 1 Introduction to computer hardware 
Figure 1.3 The computerized fuel injection system. 
Today the mechanical devices that display height, speed, 
the late 1990s. GPS provides another interesting application 
engine performance, and the altitude of the aircraft are being 
of the computer as a component in an electronic system. The 
replaced by electronic displays controlled by microcomputers, 
principles governing GPS are very simple. A satellite in 
These displays are based on the cathode ray tube or LED, 
medium Earth orbit at 20 200 km contains a very accurate 
hence the expression 'glass cockpit'. Electronic displays are 
atomic clock and it broadcasts both the time and its position, 
easier to read and more reliable than their mechanical coun- 
Suppose you pick up the radio signal from one of these 
terparts, but they provide only the information required by 
Navstar satellites, decode it, and compare the reported time 
the flight crew at any instant. 
with your watch. You may notice that the time from the satel-
Figure 1.4 illustrates an aircraft display that combines a 
lite is inaccurate. That doesn't mean that the US military has 
radar image of clouds together with navigational informa- 
wasted its tax dollars on faulty atomic clocks, but that the sig-
tion. In this example the pilot can see that the aircraft is 
nal has been traveling through space before it reaches you. 
routed from radio beacon WCO to BKP to BED and will miss 
Because the speed of light is 300 000 km/s, you know that the 
the area of storm activity. Interestingly enough, this type of 
satellite must 20 000 km away. Every point that is 20 000 km 
indicator has been accused of deskilling pilots, because they 
from the satellite falls on the surface of a sphere whose center 
no longer have to create their own mental image of the posi- 
is the satellite. 
tion of their aircraft with respect to the World from much 
If you perform the same operation with a second satellite, 
cruder instruments. 
you know that you are on the surface of another sphere. 
In the 1970s the USA planned a military navigation system 
These two spheres must intersect. Three-dimensional geo-
based on satellite technology called GPS (global positioning 
metry tells us that the points at which two spheres merge is 
system), which became fully operational in the 1990s. The 
a ring. If you receive signals from three satellites, the three 
civilian use of this military technology turned out to be 
spheres intersect at just two points. One of these points is 
one of the most important and unexpected growth areas in 
normally located under the surface of the Earth and can be 
Injectionl 
1 volume 
Throttle1* 
opening^ 
Engine rpm 
[Main refays 
CFI 
computer 
, Pump 
J relay 
Injection 
h volume 
Boosr* 
pressure* 
Engine rpm 
PB-Ne basic fuel 
injection volume map 
/ 
Fuel 
filter 
'Fuel 
pump 
KillswitchN 
combination 
switch 
Battery 
Ignition timing map 
Ignition 
timing [ 
Boost ^ 
pressure 
Engine 
rpm 
Ignition 
control 
unit 
Throttle sensor 
Air valve 
J " rr 
TW 
sensor 
sReed 
"valve 
I Surge 
iflltank 
Resonance 
chamber 
Temperature 
sensor 
/ 
i 
Air cleaner 
Compressor 
Turbine. 
Turbocharger 
Wastegate valve 
actuator 
Muffler 
'Pulser 
Pulser 
^Injector 
^ ^ 
Pressure 
\ 
regulator 
Ignition Coil 
Throttle valve yi 
0th-Ne basic fuel 
. injection volume map 
n 
Turbometer 
Pressure sensor 
Pressure sensor 
Pressure sensors 

1.6 The stored program computer—an overview 
19 
Figure 1.4 Computer-controlled displays in the glass cockpit. 
This figure illustrates the primary navigation display (or 
horizontal situation indicator) that the pilot uses to determine 
the direction in which the aircraft is traveling (in this case 
231°—approximately south-west). In addition to the heading, 
the display indicates the position and density of cloud and the 
location of radio beacons.The three arcs indicate range from the 
aircraft (30,60,90 nautical miles). 
disregarded. You can therefore work out your exact position 
on the surface of the Earth. This scheme relies on you having 
access to the exact time (i.e. your own atomic clock). 
However, by receiving signals from a fourth satellite you can 
calculate the time as well as your position. 
Several companies produce small low-cost GPS receivers 
that receive signals from the 24 Navstar satellites, decode the 
timing signals and the ephemeris (i.e. satellite position), and 
calculate the position in terms of latitude and longitude. By 
embedding a microprocessor in the system, you can process 
the position data in any way you want. For example, by com-
paring successive positions you can work out your speed and 
direction. If you enter the coordinates of a place you wish to 
go to, the processor can continually give you a bearing to 
head, a distance to your destination, and an estimated time of 
arrival. 
By adding a liquid crystal display and a map stored in a 
read-only memory to a GPS receiver, you can make a hand-
held device that shows where you are with respect to towns, 
roads, and rivers. By 2000 you could buy a device for about 
$100 that showed exactly where you were on the surface of 
the Earth to an accuracy of a few meters. 
The combination of a GPS unit plus a microprocessor plus 
a display system became a major growth area from about 
2000 because there are so many applications. Apart from its 
obvious applications to sailing and aviation, GPS can be 
included in automobiles (the road maps are stored on CD 
ROMs). GPS can even be integrated into expensive systems 
that aren't intended to move—unless they are stolen. If the 
system moves, the GPS detects the new position and reports 
it to the police. 
1.6 The stored program 
computer—an overview 
Before discussing the stored program computer, consider first 
the human being. It's natural to compare today's wonder, the 
computer, with the human just as the Victorians did with 
their technology. They coined expressions like, 'He has a 
screw loose', or 'He's run out of steam', in an endeavor to 
describe humans in terms of their mechanical technology. 
Figure 1.5 shows how a human can be viewed as a system 
with inputs, a processing device, and outputs. The inputs are 
sight (eyes), smell (nose), taste (tongue), touch (skin), sound 
(ear), and position (muscle tension). The brain processes 
information from its sensors and stores new information. 
The storage aspect of the brain is important because it mod-
ifies the brain's operation by a process we call learning. 
Because the brain learns from new stimuli, it doesn't always 
exhibit the same response to a given stimulus. Once a child 
has been burned by a flame the child reacts differently the 
next time they encounter fire. 
The brain's ability to both store and process information is 
shared by the digital computer. Computers can't yet mimic 
the operation of the brain and simplistic comparisons 
between the computer and the brain are misleading at best 
and mischievous at worst. A branch of computer science is 
devoted to the study of computers that do indeed share some 
of the brain's properties and attempt to mimic the human 
brain. Such computers are called neural nets. 
The output from the brain is used to generate speech or to 
control the muscles needed to move the body. 
Figure 1.6 shows how a computer can be compared with a 
human. A computer can have all the inputs a human has plus 
inputs for things we can't detect. By means of photoelectric 
devices and radio receivers, a computer can sense ultraviolet 
light, infrared, X-rays, and radio waves. The computer's out-
put is also more versatDe than that of humans. Computers 
can produce mechanical movement (by means of motors) 
and generate light (TV displays), sound (loudspeakers), or 
even heat (by passing a current through a resistor). 
The computer's counterpart of the brain is its central pro-
cessing unit plus its storage unit (memory). Like the brain, the 
computer processes its various inputs and produces an output. 
We don't intend to write a treatise on the differences 
between the brain and the computer, but we should make a 
comment here to avoid some of the misconceptions about 
digital computers. It is probable that the brain's processing 

20 
Chapter 1 Introduction to computer hardware 
Eyes 
Nose 
Tongue 
Skin 
Ears 
Muscle 
tension 
Mouth 
(sound) 
puts 
Outputs 
Muscle 
(movement) 
Figure 1.5 The organization of a human being. 
Keyboard 
Mouse 
Modem 
Scanner 
Central processing unit 
nputs 
Outputs 
Memory 
Figure 1.6 The organization of a computer. 
and memory functions are closely interrelated, whereas in the 
computer they are distinct. Some scientists believe that a 
major breakthrough in computing will come only when 
computer architecture takes on more of the features of the 
brain. In particular, the digital computer 
is serially organized and performs a 
single instruction at a time, whereas the 
brain has a highly parallel organization 
and is able to carry out many activities at 
the same time. 
Somewhere 
in 
every 
computer's 
memory is a block of information that 
we call a program. The word program has 
the same meaning as it does in the 
expression program of studies, or program 
of music. A computer program is a collec-
tion of instructions defining the actions 
to be carried out by the computer 
sequentially. The classic analogy with a 
computer program is a recipe in a cook-
ery book. The recipe is a sequence of 
commands that must be obeyed one by 
one in the correct order. Our analogy 
between the computer program and the 
recipe is particularly appropriate because 
the cookery instructions involve opera-
tions on ingredients, just as the com-
puter carries out operations on data 
stored in memory. 
Figure 1.7 describes how a digital 
computer can be divided into two parts: 
a central processing unit (CPU) and a 
memory system. The CPU reads the pro-
gram from memory and executes the 
operations specified by the program. 
The word execute means carry out; for 
example, the instruction add A to B 
causes the addition of a quantity called 
A to a quantity called B to be carried out. 
The actual nature of these instructions 
does not matter here. What is important 
is that the most complex actions carried 
out by a computer can be broken down 
into a number of more primitive opera-
tions. But then again, the most sublime 
thoughts of Einstein or Beethoven can 
be reduced to a large number of impulses 
transmitted across the synapses of the 
cells in their brains. 
The memory system stores two types 
of information; the program and the 
data acted on or created by the program. It isn't necessary to 
store both the program and data in the same memory. Most 
computers store programs and data in a single memory 
system and are called von Neumann machines. 
A computer is little more than a black box that moves 
information from one point to another and processes the 
Printer 
Sound system 
Video display 
?.rain 

1.6 The stored program computer—an overview 
21 
Input 
} 
Output 
Central 
processing 
unit 
Memory 
Data 
Progrorr 
Figure 1.7 Structure of the general purpose digital computer. 
Address 
(i.e. a location 
in the memory) 
0 
Get [4] * ^ J 
1 
Add it to [5] 
2 
Put result in [6] 
Instruction t 
be executed 
3 
Stop 
Instruction t 
be executed 
4 
2 * ^ 
5 
7 
6 
1 
Data element 
in memory 
/ 
information as it goes along. When we say information we 
mean the data and the instructions held inside the computer. 
Figure 1.7 shows two information-carrying paths connecting 
the CPU to its memory. The lower path with the single 
arrowhead from the memory to the CPU (heavily shaded in 
Fig. 1.7) indicates the route taken by the computer's program. 
The CPU reads the sequence of commands that make up a 
program one by one from its memory. 
The upper path (lightly shaded in Fig. 1.7) with arrows at 
both its ends transfers data between the CPU and memory. 
The program controls the flow of information along the data 
path. This data path is bidirectional, because data can flow in 
two directions. During a write cycle data generated by the 
program flows from the CPU to the memory where it is 
stored for later use. During a read cycle the CPU requests the 
retrieval of a data item from memory, which is transferred 
from the memory to the CPU. 
Suppose the instruction x = y + z is stored in memory. 
The CPU must first fetch the instruction from memory and 
bring it to the CPU. Once the CPU has analyzed or decoded 
the instruction it has to get the values of y and z from memory. 
The CPU adds these values and sends die result, x, back to 
memory for storage. 
Figure 1.8 demonstrates how the instructions making up a 
program and data coexist in the same memory. In this case 
the memory has seven locations, numbered from 0 to 7. 
Memory is normally regarded as an array of storage locations 
(boxes or pigeonholes). Each of these boxes has a unique 
location or address containing data. For example, in the 
simple memory of Fig. 1.8, address 5 contains the number 7. 
One difference between computers and people is that we 
number m items from 1 to m, whereas the computer numbers 
them from 0 to m — 1. This is because the computer regards 0 
(zero) as a valid identifier. Unfortunately, people often 
confuse 0 the identifier with 0 meaning nothing. 
Information in a computer's memory is accessed by pro-
viding the memory with the address (i.e. location) of die 
desired data. Only one memory location is addressed at a 
time. If we wish to search through memory for a particular 
item because we don't know its address, we have to read the 
items one at a time until we find the desired item. It appears 
that the human memory works in a very different way. 
Figure 1.8 The program and data in memory. 
Throughout this book square brackets denote 'the contents of 
so that in this figure, [4] is read as the contents of memory 
location number 4 and is equal to 2. 
Information is accessed from our memories by applying a key 
to all locations within the memory (brain). This key is related 
to the data being accessed (in some way) and is not related to 
its location within the brain. Any memory locations contain-
ing information that associates with the key respond to the 
access. In other words, the brain carries out a parallel search 
of its memory for the information it requires. 
Accessing many memory locations in parallel permits 
more than one location to respond to the access and is there-
fore very efficient. Suppose someone says 'chip' to you. The 
word chip is the key that is fed to all parts of your memory for 
matching. Your brain might produce responses of chip (silicon), 
chip (potato), chip (on shoulder), and chip (gambling). 
The program in Fig. 1.8 occupies consecutive memory 
locations 0-3 and the data locations 4-6. The first instruc-
tion, get [4], means fetch the contents of memory location num-
ber 4 from the memory. We employ square brackets to denote 
the contents of die address they enclose, so that in this 
case [4] = 2. The next instruction, at address 1, is add it to [5] 
and means add the number brought by the previous instruction 
to the contents of location 5. Thus, the computer adds 2 and 
7 to get 9. The third instruction, put result in [6], tells 
the computer to put the result (i.e. 9) in location 6. The 1 that 
was in location 6 before this instruction was obeyed is 
replaced by 9. The final instruction in location 3 tells die 
computer to stop. 
We can summarize the operation of a digital computer by 
means of a little piece of pseudocode (pseudocode is a method 
of writing down an algorithm in a language that is a cross 
between a computer language such as C, Pascal, or Java and 
plain English). We shall meet pseudocode again. 
DO 
BEGIN 
Read an instruction from memory 
Execute the instruction 
END 
REPEAT FOREVER 

22 
Chapter 1 Introduction to computer hardware 
Figure 1.9 The microcontroller SBC. 
1.7 The PC—a naming of 
parts 
The final part of this chapter looks at the computer 
with which most readers will be familiar, the 
PC. As we have not yet covered many of the ele-
ments of a computer, all we can do here is provide 
an overview and to name some of the parts of a 
typical computer system to help provide a context 
for following chapters. 
Figure 1.9 shows a typical single-board computer 
(SBC). As its name suggests, the SBC consists of 
one printed circuit board containing the micro-
processor, memory, peripherals, and everything 
T~r.Tr T T .... 
Memory slots 
Figure 1.10 The PC motherboard. 
Basic I/O 
CPU slot 
Video 
slot 
Disk 
-~ connectors 
PCI slots 
—-v 

1.7 The PC—a naming of parts 
23 
else it needs to function. Such a board can be embedded in 
systems ranging from automobile engines to cell phones. The 
principal characteristic of the SBC is its lack of expandability 
or flexibility. Once you've made it, the system can't be 
expanded. 
The PC is very different from the single-board computer 
because each user has their own requirements; some need 
lots of memory and fast video processing and some need 
several peripherals such as printers and scanners. 
One way of providing flexibility is to design a system with 
slots into which you can plug accessories. This allows you to 
buy a basic system with functionality that is common to all 
computers with that board and then you can add specific 
enhancements such as a video card or a sound card. 
Figure 1.10 shows a PC motherboard. The motherboard 
contains the CPU and all the electronics necessary to connect 
the CPU to memory and to provide basic input/output such 
as a keyboard and mouse interface and an interface to floppy 
and hard disk drives (including CD and DVD drives). 
The motherboard in Fig. 1.10 has four areas of expandabil-
ity. Program and data memory can be plugged into slots 
allowing the user to implement enough memory for their 
application (and their purse). You can also plug a video card 
into a special graphics slot, allowing you to use a basic system 
for applications such as data processing or an expensive state-
of-the-art graphics card for a high-performance games 
machine with fast 3D graphics. 
The CPU itself fits into a rectangular slot and is not per-
manently installed on the motherboard. If you want a faster 
processor, you can buy one and plug it in your motherboard. 
This strategy helps prevent the computer becoming out of 
date too soon. 
The motherboard has built-in interfaces that are common 
to nearly all systems. A typical motherboard has interfaces to 
a keyboard and mouse, a floppy disk drive, and up to four 
hard disks or CD ROMs. Over the last few years, special-
purpose functions have migrated from plug-in cards to the 
motherboard. For example, the USB serial interface, the local 
area network, and the audio system have been integrated on 
some of the high-performance motherboards. 
The motherboard in Fig. 1.10 has five PCI connectors. 
These connectors allow you to plug cards into the mother-
board. Each connector is wired to a bus, a set of parallel 
conductors that carry information between the cards and 
the CPU and memory. One of the advantages of a PC is its 
expandability because you can plug such a wide variety of 
cards into its bus. There are modems and cards that capture 
and process images from camcorders. There are cards that 
contain TV receivers. There are cards that interface a PC to 
industrial machines in a factory. 
In this book we will be looking at all these aspects of a 
computer. 
• 
SUMMARY 
We began this chapter with a discussion of the role of computer 
architecture in computer science education. Computer 
architecture provides the foundation of computing; it helps you 
to get the best out of computers and it aids in an understanding 
of a wide range of topics throughout computing. 
We provided a brief history of computing. We can't do justice 
to this topic in a few pages. What we have attempted to do is to 
demonstrate that computing has had a long history and is the 
result of the merging of the telegraph industry, the card-based 
data processing industry, and the calculator industry. 
In this chapter we have considered how the computer can be 
looked at as a component or, more traditionally, as part of a 
large system. Besides acting in the obvious rote as a computer 
system, computers are now built into a wide range of everyday 
items from toys to automobile ignition systems. In particular, 
we have introduced some of the topics that make up a 
first-level course in computer architecture or computer 
organization. 
We have introduced the notion of the von Neumann 
computer, which stored instructions and data in the same 
memory.The von Neumann computer reads instructions from 
memory, one by one and then executes them in turn. 
The final part of this chapter provided an overview of the 
computer system with which most students will be 
familiar—the PC.This computer has a motherboard into which 
you can plug a Pentium microprocessor, memory, and 
peripherals. You can create a computer that suits your own 
price-performance ratio. 
As we progress through this book, we are going to examine 
how the computer is organized and how it is able to step 
through instructions in memory and execute them. We will also 
show how the computer communicates with the world outside 
the CPU and its memory. 
m PROBLEMS 
Unlike the problems at the end of other chapters, these 
problems are more philosophical and require further 
background reading if they are to be answered well. 
1.1 I have always claimed you cannot name the inventor of the 
computer because what we now call a computer emerged after 
a long series of incremental steps. Am I correct? 
1.2 If you have to name one person as inventor of the 
computer, who would you choose? And why? 
1.3 What is the difference between computer architecture and 
computer organization? 
1.4 A Rolls-Royce is not a Volkswagen Beetle. Is the difference a 
matter of architecture or organization? 
1.5 List 10 applications of microprocessors you can think of 
and classify them into the groups we described (e.g. computer 
as a component). Your examples should cover as wide a range 
of applications as possible. 

24 
Chapter 1 Introduction to computer hardware 
1.6 Do you think that a digital computer could ever be capable 
of feelings, free will, original thought, and self-awareness in a 
similar fashion to humans? If not, why not? 
1.7 Some of the current high-performance civil aircraft such as 
the A320 AirBus have fly-by-wire control systems. In a 
conventional aircraft, the pilot moves a yoke that provides 
control inputs that are fed to the flying control surfaces and 
engines by mechanical linkages or hydraulic means. In the A320 
the pilot moves the type of joystick normally associated with 
computer games. The pilot's commands from the joystick (called 
a sidestick) are fed to a computer and the computer interprets 
them and carries them out in the fashion it determines is most 
appropriate. For example, if the pilot tries to increase the speed 
to a level at which the airframe might be overstressed, the 
computer will refuse to obey the command. Some pilots and 
some members of the public are unhappy about this 
arrangement. Are their fears rational? 
1.8 The computer has often been referred to as a high-speed 
moron. Is this statement fair? 
1.9 Computers use binary arithmetic (i.e. all numbers are 
composed of 1s and Os) to carry out their operations. Humans 
normally use decimal arithmetic (0-9) and have symbolic 
means of representing information (e.g. the Latin alphabet or 
the Chinese characters). Does this imply a fundamental 
difference between people and computers? 
1.10 Shortly after the introduction of the computer, someone 
said that two computers could undertake all the computing in 
the World. At that time the best computers were no more 
powerful than today's pocket calculators. The commentator 
assumed that computers would be used to solve a few scientific 
problems and little else. As the cost and size of computers has 
been reduced, the role of computers has increased. Is there a 
limit to the applications of computers? Do you anticipate any 
radically new applications of computers? 
1.11 A microprocessor manufacturer, at the release of their new 
super chip, was asked the question, 'What can your 
microprocessor do?' He said it was now possible to put it in 
washing machines so that the user could tell the machine what 
to do verbally, rather than by adjusting the settings manually. 
At the same time we live in a world in which many of its 
inhabitants go short of the very basic necessities of life: water, 
food, shelter, and elementary health care. Does the computer 
make a positive contribution to the future well-being of the 
World's inhabitants? Is the answer the same if we ask about the 
computer's short-term effects or its long-term effects? 
1.12 The workstation makes it possible to design and to test 
(by simulation) everything from other computers to large 
mechanical structures. Coupled with computer communications 
networks and computer-aided manufacturing, it could be 
argued that many people in technologically advanced societies 
will be able to work entirely from home. Indeed, all their 
shopping and banking activities can also be performed from 
home. Do you think that this step will be advantageous or 
disadvantageous? What will be the effects on society of a 
population that can, largely, work from home? 
1.13 In a von Neumann machine, programs and data share the 
same memory.The operation 'get [4]' reads the contents of 
memory location number 4 and you can then operate on the 
number you've just read from this location. However, the 
contents of this location may not be a number. It may be an 
instruction itself. Consequently, a program in a von Neumann 
machine can modify itself. Can you think of any implications 
this statement has for computing? 
1.14 When discussing the performance of computers we 
introduced the benchmark, a synthetic program whose 
execution time provides a figure of merit for the performance of 
a computer. If you glance at any popular computer magazine, 
you'll find computers compared in terms of benchmarks. 
Furthermore, there are several different benchmarks. A computer 
that performs better than others when executing one 
benchmark might not do so well when executing a different 
benchmark. What are the flaws in benchmarks as a test of 
performance and why do you think that some benchmarks favor 
one computer more than another? 
1.15 The von Neumann digital computer offers just one 
computing paradigm. Other paradigms are provided by analog 
computers and neural networks. What are the differences 
between these paradigms and are there others? 

Gates, circuits, and 
combinational logic 
2 Logic elements and 
Boolean algebra 
Digital computers are 
constructed from millions of very 
simple logic elements called 
gales. In this chapter we 
introduce the fundamental gales 
and demonstrate how they can 
be combined to create circuits 
that carry out the basic functions 
required in a computer. 
INTRODUCTION 
We begin our study of the digital computer by investigating the elements from which it is 
constructed. These circuit elements are gates and flip-flops and are also known as combinational 
and sequential logic elements, respectively. A combinational logic element is a circuit whose 
output depends only on its current inputs, whereas the output from a sequential element 
depends on its past history (i.e. a sequential element remembers its previous inputs) as well as 
its current input. We describe combinational logic in this chapter and devote the next chapter to 
sequential logic. 
Before we introduce the gate, we highlight the difference between digital and analog systems 
and explain why computers are constructed from digital logic circuits. After describing the 
properties of several basic gates we demonstrate how a few gates can be connected together to 
carry out useful functions in the same way that bricks can be put together to build a house or a 
school. We include a Windows-based simulator that lets you construct complex circuits and then 
examine their behavior on a PC. 
The behavior of digital circuits can be described in terms of a formal notation called Boolean 
algebra.We include an introduction to Boolean algebra because it allows you to analyze circuits 
containing gates and sometimes enables circuits to be constructed in a simpler form. Boolean 
algebra leads on to Karnaugh maps, a graphical technique for the simplification and manipulation 
of Boolean equations. 
The last circuit element we introduce is the tri-state gate, which allows you to connect lots of 
separate digital circuits together by means of a common highway called a bus. A digital computer 
is composed of nothing more than digital circuits, buses, and sequential logic elements. 
By the end of this chapter, you should be able to design a wide range of circuits that can 
perform operations as diverse as selecting between one of several signals to implementing simple 
arithmetic operations. 
Real circuits can fail. The final part of this chapter takes a brief look at how you test digital 
circuits. 
3 Sequential logic 
We cari classify logic circuits into 
.two groups: the combinational 
circuit we described in Chapter 2 
and the sequential circuit which : 
forms the subject of this chapter. 
A sequential circuit includes 
memory elements and its current 
behavior is governed by Its past 
inputs. Typical sequential circuits 
• are counters and registers. 
4 Computer arithmetic 
In Chapter 4 we demonstrate 
how numbers are represented in 
binary form and look at binary 
arithmetic. We also demonstrate 
how the properties of binary 
numbers are exploited to create • 
codes that compress data or 
even detect and correct errors. 
5 The instruction set 
architecture 
In Chapter 5 we introduce the 
computer's instruction set 
architecture (ISA), which defines 
the machine-level programmer's 
View of the computer.The ISA 
describe the type of operations a 
computer carries out We are 
interested in three aspects of the 
ISA: the nature of the 
instructions, the resources used 
by the instructions (registers and 
memory), and the ways in which 
the instructions access data 
( addressing modes). 
CHAPTER MAP 

26 
Chapter 2 Gates, circuits, and combinational logic 
2.1 Analog and digital systems 
Before we can appreciate the meaning and implications of 
digital systems, it's necessary to look at the nature of analog 
systems. The term analog is derived from the noun analogy 
and means a quantity that is related to, or resembles, or 
corresponds to, another quantity; for example, the length of 
a column of mercury in a thermometer is an analog of the 
temperature because the length of the mercury is propor-
tional to the temperature. Analog electronic circuits repres-
ent physical quantities in terms of voltages or currents. 
An analog variable can have any value between its max-
imum and minimum limits. If a variable X is represented by a 
voltage in the range — 10 V to +10 V, X may assume any one 
of an infinite number of values within this range. We can say 
that X is continuous in value and can change its value by an 
arbitrarily small amount. Fig. 2.1 plots a variable X as a con-
tinuous function of time; that is, X doesn't jump instanta-
neously from one value to another. In Fig. 2.1, a fragment of 
the graph of X is magnified to reveal fluctuations that you 
can't see on the main graph. No matter how much you 
magnify this graph, the line will remain continuous and 
unbroken. 
A*W 
Figure 2.1 Characteristics of an analog variable. 
A digital signal must have one of a fixed 
number of values and change from one 
value to another instantaneously 
Figure 2.2 Characteristics of an ideal digital variable. 
The design of analog circuits such as audio amplifiers is a 
demanding process, because analog signals must be processed 
without changing their shape. Changing the shape of an 
analog signal results in its degradation or distortion. 
Information inside a computer is represented in digital 
form. A digital variable is discrete in both value and in time, as 
Fig. 2.2 demonstrates. The digital variable Fmust take one of 
four possible values. Moreover, Y changes from one discrete 
value to another instantaneously. In practice, no physical (i.e. 
real) variable can change instantaneously and a real signal 
must pass through intermediate values as it changes from one 
discrete state to another. 
All variables and constants in a digital system must take a 
value chosen from a set of values called an alphabet. In 
decimal arithmetic the alphabet is composed of the symbols 0, 
1, 2 , . . . 9 and in Morse code the alphabet is composed of the 
four symbols dot, dash, short space, and long space. Other 
digital systems are Braille, semaphore, and the days of the week. 
A major advantage of representing information in digital 
form is that digital systems are resistant to error. A digital 
symbol can be distorted, but as long as the level of distortion 
is not sufficient for the symbol to be confused with a different 
symbol, the original symbol can always be recognized and 
reconstituted. For example, if you write the letter K by hand, 
most readers will be able to recognize 
it as a K unless it is so badly formed 
that it looks like another letter such as 
an R or C. 
Digital computers use an alphabet 
composed of two symbols called 0 and 
1 (sometimes called false and true, or 
low and high, or off and on). A digital 
system with two symbols is called a 
binary system. The physical repres-
entation of these symbols can be made 
as unlike each other as possible to give 
the maximum discrimination between 
the two digital values. Computers 
once stored binary information on 
paper tape—a hole represented one 
binary value and no hole represented 
the other. When reading paper tape 
the computer has only to distinguish 
between a hole and no-hole. Suppose 
we decided to replace this binary 
computer by a decimal computer. 
Imagine that paper tape were to be 
used to store the 10 digits 0-9. A 
number on the tape would consist of 
no-hole or a hole in one of nine sizes 
(10 symbols in all). How does this 
computer distinguish between a size 
six hole and a size five or a size seven 
Time-varying 
analog signal 
Time 
Time 
vXMagnification 
Vtf) 
A 
3 
2 
1 
0 

2.1 Analog and digital systems 
27 
NOTES ON LOGIC VALUES 
1. Every logic input or output must assume one of two 
discrete states. You cannot have a state that is neither 
1 nor 0. 
2. Each logic input or output can exist in only one state at any 
one time. 
3. Each logic state has an inverse or complement that is the 
opposite of its current state. The complement of a true or 
one state is a false or zero state, and vice versa. 
4. A logic value can be a constant or a variable. If it is a 
constant, it always remains in that state. If it is a variable, it 
may be switched between the states 0 and 1.A Boolean 
variable is also called a literal. 
5. A variable is often named by the action it causes to take 
place. The following logical variables are all self-evident: 
START, STOP, RESET, COUNT, and ADD. 
6. The signal level (i.e. high or low) that causes a variable to 
carry out a function is arbitrary. If a high voltage causes the 
action, the variable is called active-high. If a low voltage 
causes the action, the variable is called active-low. Thus, if 
an active-high signal is labeled START, a high level will 
initiate the action. If the signal is active-low and labeled 
START, a low level will trigger the action. 
7. By convention, a system of logic that treats a low level as a 
0 or false state and a high-level as a 1 or true state is called 
positive logic. Most of this chapter uses positive logic. 
8. The term asserted is used to indicate that a signal is placed 
in the level that causes its activity to take place. If we say 
that START is asserted, we mean that it is placed in a high 
state to cause the action determined by START. Similarly, if 
we say that LOAD is asserted, we mean that it is placed in a 
low state to trigger the action. 
-5-Y 
LOGIC VALUES AND SIGNAL LEVELS 
In a system using a 5 V power supply you might think that a 
bit is represented by exactly 0 V or 5 V. Unfortunately, we 
can't construct such precise electronic devices cheaply. We can 
construct devices that use two ranges of voltage to represent 
the binary values 0 and 1. For example, one logic family 
represents a 0 state by a signal in the range 0-0.4 V and a 
1 state by a signal in the range 2.8-5 V. 
This diagram 
illustrates the ranges 
of voltage used to 
represent 0 and 1 
states. Digital 
component 
manufacturers make 
several promises to 
users. First, they 
guarantee that the 
output of a gate in a 
logical 0 state shall be 
in the range 0-0.4V 
and that the output of 
a gate in a logical 1 
-2BV 
Output range 
for a 
logical 1 
.2.4 V 
-0.8 V 
t 
Forbidden 
zone 
state shall be in the range 2.8-5.0V. Similarly, they 
guarantee that the input circuit of a gate shall 
recognize a voltage in the range 0-0.8V as a logical 
0 and a voltage in the range 2.4-5.0 V as a logical 1. 
Here, two gates are wired together so that the 
output of gate 1 becomes the input of gate 2. The 
signal at the output of gate 1 is written Vout and 
the input to gate 2 is written Vin. 
An adder (represented by the circle with a' + ') is placed 
between the two gates so that the input voltage to the second 
gate is given by Vin = VM + Vnoise. that is, a voltage called 
Vnol5e is added to the output from gate 1. In a real circuit there 
is, of course, no such adder. The adder is fictitious and 
demonstrates how the output voltage may be modified by the 
addition of noise or interference. All electronic circuits are 
subject to such interference; for example, the effect of noise 
on a weak TV signal is to create snow on the screen. 
Note that the range of input signals that are recognized as 
representing a 1 state (i.e. 2.4-5 V) is greater than the 
range of output signals produced by a gate in a 1 state (i.e. 
2.8-5 V). By making the input range greater than the output 
range, the designer compensates for the effect of noise or 
unwanted signals. Suppose a noise spike of -0.2 V is added to 
a logical 1 output of 2.8 V to give a total input signal of 2.6 V. 
This signal, when presented to the input circuit of a gate, is 
greater than 2.4 V and is still guaranteed to be recognized 
as a logical I.The difference between the input and output 
ranges for a given logic value known as the gate's guaranteed 
noise immunity. 
0.4 V 
Output range 
for a 
0\|f logical 0 
1 / 
"noise 
\ I 
Output from/ 
. 
\ 
element 1 
N o l s e 
\ Input to 
element 2 
Input range 
for a 
logical 1 
Input range 
for a 
logical 0 
Logic element 1 
Logic element 2 
1 
Noise 
kut 
Jjn, 

28 
Chapter 2 Gates, circuits, and combinational logic 
a system would require extremely precise 
2 2 1 
T h e A N D 2 3 t e 
hole? Such 
electronics. 
A single binary digit is known as a bit (Binary digiT) and is 
the smallest unit of information possible; that is, a bit can't be 
subdivided into smaller units. Ideally, if a computer runs off, 
say, 3 V, a low level would be represented by 0.0 V and a high 
level by 3.0 V. 
22 Fundamental gates 
The digital computer consists of nothing more than the inter-
connection of three types of primitive elements called AND, 
OR, and NOT gates. Other gates called NAND, NOR, and 
EOR gates can be derived from these gates. We shall see that 
all digital circuits, may be designed from the appropriate 
interconnection of NAND (or NOR) gates alone. In other 
words, the most complex digital computer can be reduced 
to a mass of NAND gates. This statement doesn't devalue 
the computer any more than saying that the human brain is 
just a lot of neurons joined in a particularly complex way 
devalues the brain. 
We don't use gates to build computers because we like 
them or because Boolean algebra is great fun. We use gates 
because they provide a way of mass producing cheap and 
reliable digital computers. 
The AND gate is a circuit with two or more inputs and a sin-
gle output. The output of an AND gate is true if and only if 
each of its inputs is also in a true state. Conversely, if one or 
more of the inputs to the AND gate is false, the output will 
also be false. Figure 2.3 provides the circuit symbol for both a 
two-input AND gate and a three-input AND gate. Note that 
the shape of the gate indicates its AND function (this will 
become clearer when we introduce the OR gate). 
An AND gate is visualized in terms of an electric circuit or 
a highway as illustrated in Fig. 2.4. Electric current (or traffic) 
flows along the circuit (road) only if switches (bridges) A and 
B are closed. The logical symbol for the AND operator is a 
dot, so that A AND B can be written A • B. As in normal alge-
bra, the dot is often omitted and A • B can be written AB. The 
logical AND operator behaves like the multiplier operator in 
conventional algebra; for example, the expression (A + B) • 
(C + D) = A C + A D + B C + B D in both Boolean 
and conventional algebra. 
C = A.B 
C = A.B.C 
(a) Two-input AND gate 
Figure 2.3 The AND gate. 
(b) Three-input AND gate 
WHAT IS A GATE? 
The word gate conveys the idea of a two-state device—open 
or shut. A gate may be thought of as a black box with one or 
more input terminals and an output terminal. The gate 
processes the digital signals at its input terminals to produce a 
digital signal at its output terminal. The particular type of the 
gate determines the actual processing involved. The output C 
of a gate with two input terminals A and B can be expressed in 
conventional algebra as C = F{A,B), where A, B, and Care two-
valued variables and F is a logical function. 
The output of a gate is a function only of its inputs. When 
we introduce the sequential circuit, we will discover that the 
sequential circuit's output depends on its previous output as 
well as its current inputs. We can demonstrate the concept of 
a gate by means of an example from the analog world. 
Consider the algebraic expression y= F(x) = 2.x2 + x + 1. If 
we think of x as the input to a black box and y its output, the 
block diagram demonstrates how y is generated by a sequence 
of operations on x.The operations performed on the input are 
those of addition, multiplication, and squaring, variable x 
enters the 'squarer' and comes out as xa.The output from the 
squarer enters a multiplier (along with the constant 2) and 
comes out as 2X2, and so on. By applying all the operations to 
input x, we end up with output 2X2 + x + 1. The boxes 
carrying out these operations are entirely analogous to gates 
in the digital world—except that gates don't do anything as 
complicated as addition or multiplication. 
Squarer | 
xz Multiplier 
X 
2x2 
The input signal x is 
acted on by four functional 
units to create a signal 
Squarer | 
xz Multiplier 
X 
r 
The input signal x is 
acted on by four functional 
units to create a signal 
t 
1 r 
y=2x2 + x+ 1. 
Input 
Z 
Adder 
+ 
2x2 + x _ 
Adder 
+ 
2x* + x+1 
Adder 
+ 
Adder 
+ 
t 
1 
+ Output 
y 
A -
B-
A " 
B -
C -

2.2 Fundamental gates 
29 
CIRCUIT CONVENTIONS 
Because we write from left to right, many logic circuits are also 
read from left to right; that is, information flows from left to 
right with the inputs of gates on the left and the outputs on 
the right. 
Because a circuit often contains many signal paths, some of 
these paths may have to cross over each other when the 
diagram is drawn on two-dimensional paper. We need a means 
of distinguishing between wires that join and wires that 
simply cross each other (rather like highways that merge and 
highways that fly over each other). The standard procedure is 
to regard two lines that simply cross as not being connected as 
the diagram illustrates.The connection of two lines is denoted 
by a dot at their intersection. 
The voltage at any point along a conductor is constant and 
therefore the logical state is the same everywhere on the line. 
If a line is connected to the input of several gates, the input to 
each gate is the same. In this diagram, the value of Xand P 
must be the same because the two lines are connected. 
These two lines are connected 
These two lines are not connected 
and cross over at this point 
A corollary of the statement that the same logic state exists 
everywhere on a conductor is that a line must not be 
connected to the output of more than one circuit—otherwise 
the state of the line will be undefined if the outputs differ. At 
the end of this chapter we will introduce gates with special 
tri-state outputs that can be connected together without 
causing havoc. 
The circuit is completed only 
if switch A and switch B is closed 
/ 
/ 
Switch A 
Switch B 
Figure 2.4 The representation of an AND gate. 
A useful way of describing the relationship between the 
inputs of a gate and its output is the truth table. In a truth 
table the value of each output is tabulated for every possible 
combination of the inputs. Because the inputs are two valued 
(i.e. binary with states 0 and 1), a circuit with n inputs has 2" 
lines in its truth table. The order in which the 2" possible 
inputs are taken is not important but by convention the order 
corresponds to the natural binary sequence (we discuss 
binary numbers in Chapter 4). Table 2.1 describes the natural 
binary sequences for values of n from 1 to 4. 
Table 2.2 illustrates the truth table for a two-input AND 
gate, although there's no reason why we can't have any num-
ber of inputs to an AND gate. Some real gates have three or 
four inputs and some have 10 or more inputs. However, it 
doesn't matter how many inputs an AND gate has. Only one 
line in the truth table will contain a 1 entry because all inputs 
must be true for the output to be true. 
When we introduce computer arithmetic, computer archi-
tecture, and assembly language programming, we will see 
that computers don't operate on bits in isolation. Computers 
process entire groups of bits at a time. These groups are called 
words and are typically 8, 16, 32, or 64 bits wide. The AND 
Table 2.1 The 2" possible values of an n-bit variable 
for n = 1 to 4. 
Inputs 
A 
B 
Output 
F=AB 
0 
0 
0 
0 
1 
0 
1 
0 
0 
1 
1 
1 
False because one or 
more inputs is false 
True because both 
inputs are true 
Table 2.2 Truth table for the AND gate. 
n = 1 
n = 2 
n = 3 
n = ^ 
0 
oo 
ooo 
ooo;.1 
1 
01 
001 
0001 
10 
010 
0010 
11 
Oil 
001" 
iQO 
0100 
10" 
OiO! 
n o 
c'i'O 
' • < • 
CM: 
IdOi! 
ux:i 
nil;: 
10''I 
noo 
I'OI 
11:1 
^A 
^ / B 

30 
Chapter 2 Gates, circuits, and combinational logic 
operation, when applied to words, is called a logical operation 
to distinguish it from an arithmetic operation such as addi-
tion, subtraction, or multiplication. When two words take 
part in a logical operation such as an AND, the operation 
takes place between the individual pairs of bits; for example 
bit a, of word A is ANDed with bit b, of word B to produce bit 
C; of word C. Consider the effect of ANDing of the following 
two 8-bit words A = 11011100 andB = 01100101.x 
1 
0 
1 
1 
0 
1 
1 
0 
1 
0 
1 
1 
0 
0 
0 <— word A 
1 -^— word B 
0 1 0 0 0 1 0 0 <— C = A-B 
In this example the result C = A • B is given by 01000100. 
Why should anyone want to AND together two words? If you 
AND bit x with 1, the result is x (because Table 2.2 demon-
strates that 1.0 = Oand 1.1 = 1). If you AND bit x with 0 the 
result is 0 (because the output of an AND gate is true only if 
both inputs are true). Consequently, a logical AND is used to 
mask certain bits in a word by forcing them to zero. For 
example, if we wish to clear the leftmost four bits of an 8-bit 
word to zero, ANDing the word with 00001111 will do the 
trick. The following example demonstrates the effect of an 
AND operation with a 00001111 mask. 
1 1 0 1 1 0 1 1 -4— source word 
0 0 0 0 1 1 1 1 - * — mask 
0 0 0 0 1 0 1 1 -*— result 
2.2.2 The OR gate 
The output of an OR gate is true if any one (or more than 
one) of its inputs is true. Notice the difference between AND 
and OR operations. The output of an AND is true only if all 
inputs are true whereas the output of an OR is true if at least 
one input is true. The circuit symbol for a two-input and a 
three-input OR gate is given in Fig. 2.5. The logical symbol 
for an OR operation is an addition sign, so that the logical 
operation A OR B is written as A + B. The logical OR opera-
tor is the same as the conventional addition symbol because 
the OR operator behaves like the addition operator in algebra 
(the reasons for this will become clear when we introduce 
Boolean algebra). Table 2.3 provides the truth table for a two-
input OR gate. 
The behavior of an OR gate can be represented by the 
switching circuit of Fig. 2.6. A path exists from input to out-
put if either of the two switches is closed. 
•C=A+S 
•D=A+B+C 
(a) Two-input OR gate. 
(b) Three-input OR gate. 
Inputs 
A 
B 
Output 
F=A+B 
0 
0 
0 
1 
1 
0 
1 
1 
0 
1 
1 
1 
False because 
no input is true 
True because at least 
one input is true 
Table 2.3 Truth table for the OR gate. 
Switch A 
B 
. / 
The circuit is 
complete if either 
switch A or B 
is closed 
Switch B' 
Figure 2.6 The representation of an OR gate. 
The use of the term OR here is rather different from the 
English usage of or. The Boolean OR means (either A or B) or 
(both A and B), whereas the English usage often means A or 
B but not (A and B). For example, consider the contrasting 
use of the word or in the two phrases: 'Would you like tea 
or coffee?' and 'Reduced fees are charged to members who 
are registered students or under 25'. We shall see that the 
more common English use of the word or corresponds to 
die Boolean function known as the EXCLUSIVE OR, an 
important function that is frequently abbreviated to EOR 
orXOR. 
A computer can also perform a logical OR on words as the 
following example illustrates. 
1 0 0 1 1 1 0 0 
<— word A 
0 0 1 0 0 1 0 1 
<— word B 
1 0 1 1 1 1 0 1 
-4— C = A + B 
The logical OR operation is used to set one or more bits in 
a word to a logical 1. The term set means make a logical one, 
just as clear means reset to a logical zero. For example, the 
least-significant bit ofaword is set byORingitwith 00 . . . 01. 
By applying both AND and OR operations to a word we can 
selectively clear or set its bits. Suppose we have an 8-bit binary 
word and we wish to clear bits 6 and 7 and set bits 4 and 5. If 
the bits of the word are d% to d7, we can write: 
Figure 2.5 The OR gate. 
A -
B-
C -
A -
B -
d7 
db 
d5 
d4 
d3 
d2 
dx 
d0 
Source word 
0 
0 
1
1
1
1
1 
1 
AND mask 
0 
0 
d5 
d4 
di 
d2 
di 
d0 
First result 
0 
0 
1
1
0 
0 
0 
0 
OR mask 
0 
0 
1 
1 
d5 
d2 
dx 
d0 
Final result 
0 
word A 
word B 
C = A + B 
1 0 
0 
1 0 
1 
1 0 
1 
O i l 
1 0 
0 
1 1 1 
1 0 
0 0 
1 0 
Switch A 

2.2 Fundamental gates 
31 
2.2.3 The NOT gate 
The NOT gate is also called an inverter or a complementer and 
is a two-terminal device with a single input and a single out-
put. If the input of an inverter is X, its output is NOT X which 
is written X or X". Figure 2.7 illustrates the symbol for an 
inverter and Table 2.4 provides its truth table. Some teachers 
vocalize X as 'not X' and others as 'X not'. The inverter is the 
simplest of gates because the output is the opposite of 
the input. If the input is 1 the output is 0 and vice versa. By the 
way, the triangle in Fig. 2.7 doesn't represent an inverter. 
The small circle at the output of the inverter indicates the 
inversion operation. We shall see that this circle indicates 
logical inversion wherever it appears in a circuit. 
We can visualize the operation of the NOT gate in terms of 
the relay illustrated in Fig. 2.8. A relay is an electromechanical 
switch (i.e. a device that is partially electronic and partially 
mechanical) consisting of an iron core around which a coil of 
wire is wrapped. When a current flows through a coil, it gen-
erates a magnetic field that causes the iron core to act as a 
magnet. Situated close to the iron core is a pair of contacts, 
the lower of which is mounted on a springy strip of iron. If 
switch A is open, no current flows through the coil and the 
iron core remains unmagnetized. The relay's contacts are 
lie output is 
le logical 
complement of 
the input 
Figure 2.7 The NOT gate or inverter. 
Input 
A 
Output 
F=A 
Table 2.4 Truth table for the inverter. 
Contacts 
(switch A)—-—». A ! 
ron strip Output 
A 
normally closed so that they form a switch that is closed when 
switch A is open. 
If switch A is closed, a current flows through the coil to 
generate a magnetic field that magnetizes the iron core. The 
contact on the iron strip is pulled toward the core, opening 
the contacts and breaking the circuit. In other words, closing 
switch A opens the relay's switch and vice versa. The system in 
Fig. 2.8 behaves like a NOT gate. The relay is used by a com-
puter to control external devices and is described further 
when we deal with input and output devices. 
Like both the AND and OR operations, the NOT function 
can also be applied to words: 
1 1 0 1 1 1 0 0 * ~ word A 
0 0 1 0 0 0 1 1 * ~ B = A 
2.2.4 The NAND and NOR gates 
The two most widely used gates in real circuits are the NAND 
and NOR gates. These aren't fundamental gates because the 
NAND gate is derived from an AND gate followed by an 
inverter (Not AND) and the NOR gate is derived from an OR 
gate followed by an inverter (Not OR), respectively. The circuit 
symbols for the NAND and NOR gates are given in Fig. 2.9. 
The little circle at the output of a NAND gate represents the 
symbol for inversion or complementation. It is this circle that 
converts the AND gate to a NAND gate and an OR gate to a 
NOR gate. Later, when we introduce the concept of mixed 
logic, we will discover that this circle can be applied to the 
inputs of gates as well as to their outputs. 
Table 2.5 gives the truth table for the NAND and the NOR 
gates. As you can see, the output columns in the NAND and 
NOR tables are just the complements of the outputs in the 
corresponding AND and OR tables. 
We can get a better feeling for the effect that different gates 
have on two inputs, A and B, by putting all the gates together 
in a single table (Table 2.6). We have also included the 
EXCLUSIVE OR (i.e. EOR) and its complement the 
EXCLUSIVE NOR (i.e. EXNOR) in Table 2.6 for reference. 
The EOR gate is derived from AND, OR, and NOT gates and 
is described in more detail later in this chapter. It should be 
noted here that A-B is not the same as AB, just as A+B is 
not the same as A+B. 
Iron core 
Figure 2.8 The operation of a relay. 
-E- Battery 
T 
AND gate followed by an inverter 
NAND gate 
OR gate followed by an inverter 
NOR gate 
Figure 2.9 Circuit symbols for the NAND and NOR gates. 
< = A.B 
^ 
A -
B -
A-
B-
-C = A.B 
-C = A+B 
A. 
B-
-C = A + B 
A + B 
A -
B-
Coil 
1 
0 
0 
1 
A 
A 
^ - A 
A.B 

32 
Chapter 2 Gates, circuits, and combinational logic 
2.2.5 Positive, negative, and mixed logic 
At this point we introduce the concepts of positive logic, 
negative logic, and mixed logic. Some readers may find that 
this section interrupts their progress toward a better under-
standing of the gate and may therefore skip ahead to the next 
section. 
Up to now we have blurred the distinction between two 
unconnected concepts. The first concept is the relationship 
between low/high voltages in a digital circuit, 0 and 1 logical 
levels, and true/false logic values. The second concept is the 
logic function; for example, AND, OR, and NOT. So far, we 
have used positive logic in which a high-level signal represents 
a logical one state and this state is called true. 
Table 2.7 provides three views of the AND function. The 
leftmost column provides the logical truth table in which the 
output is true only if all inputs are true (we have used T and F 
to avoid reference to signal levels). The middle column 
describes the AND function in positive logic form in which 
the output is true (i.e. 1) only if all inputs are true (i.e. 1). 
The right hand column in Table 2.7 uses negative logic in 
which 0 is true and 1 is false. The output A • B is true (i.e. 0) 
only when both inputs are true (i.e. 0). 
As far as digital circuits are concerned, there's no funda-
mental difference between logical 1 s and 0s and it's as sensible 
to choose a logical 0 level as the true state as it is to choose a 
logical 1 state. Indeed, many of the signals in real digital 
systems are active-low which means that their function is 
carried out by a low-level signal. 
NAND 
C = A • B 
NOR 
C = A + B 
0 
0 
1 
0 
0 
1 
0 
1 
1 
0 
1 
0 
1 
0 
1 
1 
0 
0 
1 
1 
0 
1 
1 
0 
Suppose we regard the low level as true and use negative 
logic, Table 2.7 shows that we have an AND gate whose out-
put is low if and only if each input is low. It should also be 
apparent that an AND gate in negative logic functions as an 
OR gate in positive logic. Similarly, a negative logic OR gate 
functions as an AND gate in positive logic. In other words, the 
same gate is an AND gate in negative logic and an OR gate in 
positive logic. Figure 2.10 demonstrates the relationship 
between positive and negative logic gates. 
For years engineers used the symbol for a positive logic 
AND gate in circuits using active-low signals with the result 
that the reader was confused and could only understand the 
Logical form 
Positive logic 
Negative logic 
A B 
A B 
A B 
A B 
A B 
A B 
F F 
F 
0 0 
0 
1 1 
1 
F T 
F 
0 1 
0 
1 0 
1 
T F 
F 
1 0 
0 
0 1 
1 
T T 
T 
1 1 
1 
0 0 
0 
Table 2.7 Truth table for AND gate in positive and negative 
logic forms. 
C is high if A or B is high 
C is low if A and B is low 
Table 2.5 Truth table for the NAND and NOR gates. 
C is high if A and B is high 
C is low if A or B is low 
Figure 2.10 Positive and negative logic. 
Inputs 
Output 
A B 
AND A B 
ORA+B 
NAND A.B 
NORA + B 
EORA©B 
EXNOR A © B 
0 
0 
0 
0 
1 
1 
0 
1 
0 
1 
0 
1 
1 
0 
1 
0 
1 
0 
0 
1 
1 
0 
1 
0 
1 
1 
1 
1 
0 
0 
0 
1 
Table 2.6 Truth table for six gates. 
A 
B 
A 
B 
C 
r 
A-
B 
A-
B-
C 
•C 
w 
A-
B-
-C 
A 
B 

2.2 Fundamental gates 
33 
GATES AS TRANSMISSION ELEMENTS 
We can provide more of an insight into what gates do by 
treating them as transmission elements that control the flow 
of information within a computer. 
We are going to take three two-input gates (i.e. AND, OR, 
EOR) and see what happens when we apply a variable to one 
input and a control signal to the other input. The figure 
illustrates three pairs of gates. Each pair demonstrates the 
situation in which the control input C is set to a logical 0 and a 
(a) AND gate 
Control input C=0 
X • 
C=0 
Gate disabled (output low) 
(c) OR gate 
Control input C=0 
X 
C=0' 
Gate enabled (output=X) 
(e) EOR gate 
Control input C=0 
X 
C=0 
Gate acts as pass-through element (output=X) 
(a) AND gate 
Control input C= 1 
C=1 • 
Gate enabled (output=Xj 
X' 
C=1 • 
Gate disabled (output high) 
Gate acts as inverter (output=X) 
logical 1 state. The other input is a variable X and we wish to 
determine the effect the gate has on the transmission of X 
through it. 
Figures (a) and (b) demonstrate the behavior of an AND 
gate. When C = 0, an AND gate is disabled and its output is 
forced into a logical zero state. When C = 1, the AND gate is 
enabled and its X input is transmitted to the output 
unchanged. We can think of an AND gate as a simple switch 
that allows or inhibits the passage of 
a logical signal. Similarly, in Fig (c) 
and (d) an OR gate is enabled by 
C = 0 and disabled by C = 1. 
However, when the OR gate is 
disabled, its output is forced into a 
logical one state. 
The EOR gate in Fig (e) and (f) is a 
more interesting device. When its 
control input is 0, it transmits the 
other input unchanged. But when 
C = 1, it transmits the complement 
of X.The EOR gate can best be 
regarded as a programmable inverter. 
Later we shall make good use of this 
property of an EOR gate. 
The reason we've introduced the 
concept of a gate as a transmission 
element is that digital computers can 
be viewed as a complex network 
through which information flows and 
this information is operated on by 
gates as it flows round the system. 
fd) OR gate 
Control input G=1 
(f) EOR gate 
Control input C=1 
X 
C=1 
circuit by mentally transforming the positive logic gate into 
its negative logic equivalent. In mixed logic both positive 
logic and negative logic gates are used together in the same 
circuit. The choice of whether to use positive or negative logic 
is determined only by the desire to improve the clarity of a 
diagram or explanation. 
Why do we have to worry about positive and negative 
logic? If we stick to positive logic, life would be much simpler. 
True, but life is never that simple. Many real electronic sys-
tems are activated by low-level signals and that makes it sens-
ible to adopt negative logic conventions. Let's look at an 
example. Consider a circuit that is activated by a low-level 
signal only when input A is a low level and input B is a low 
level. Figure 2.11 demonstrates the circuit required to imple-
ment this function. Note that die bubble at the input to die 
circuit indicates that it is activated by a low level. 
In Fig. 2.11(a) we employ positive logic and draw an OR 
gate because the output of an OR gate is 0 only when both its 
inputs are 0. There's nothing wrong with this circuit, but it's 
<1UkU:! 
(a) Positive logic system 
The circuit is activated when 
A is low and B is low 
Figure 2.11 Mixed logic. 
(b) Negative logic system 
The circuit is activated when 
A is low and B is low 
confusing. When you see a gate with an OR shape you think 
of an OR function. However, in this case, the gate is actually 
performing an AND operation on low-level signals. 
What we need is a means of preserving the AND shape and 
indicating we are using negative logic signals. Figure 2.11(b) 
does just that. By placing inverter circles at the AND gate's 
inputs and output we immediately see that the output of the 
gate is low if and only if both of its inputs are low. 
- 0 
-X 
•X 
X 
1 
X 
A-
B-
A-
B 
Circt.it I 

34 
Chapter 2 Gates, circuits, and combinational logic 
^ 
Active-low output 
Figure 2.12 Using mixed logic. 
There is no physical difference between the circuits of 
Figs. 2.11 (a) and 2.11 (b). They are both ways of representing 
the same thing. However, the meaning of the circuit in 
Fig. 2.11 (b) is clearer. 
Consider another example of mixed logic in which we use 
both negative and positive logic concepts. Suppose a circuit is 
activated by a low-level signal if input A is low and input B 
high, or input D is high, or input C is low. Figure 2.12 shows 
how we might draw such a circuit. For most of this book we 
will continue to use positive logic. 
2.3 Applications of gates 
We now look at four simple circuits to demonstrate that a few 
gates can be connected together in such a way as to create a 
circuit whose function and importance may readily be appre-
ciated by the reader. Following this informal introduction to 
circuits we introduce Digital Works, a Windows-based pro-
gram that lets you construct and simulate circuits containing 
gates on a PC. We then return to gates and provide a more 
formal section on the analysis of logic circuits by means of 
Boolean algebra. 
Circuits are constructed by connecting gates together. The 
output from one gate can be connected (i.e. wired) to the 
input of one or more other gates. However, two outputs can-
not be connected together. 
Example 1 Consider the circuit of Fig. 2.13 that uses three 
two-input AND gates labeled Gl, G2, and G3, and a three-
input OR gate labeled G4. This circuit has three inputs A, B, 
and C, and an output F. What does it do? 
We can tackle this problem in several ways. One approach 
is to create a truth table that tabulates the output F for all the 
eight possible combinations of the three inputs A, B, and C. 
Table 2.8 corresponds to the circuit of Fig. 2.13 and includes 
columns for the outputs of the three AND gates as well as the 
output of the OR gate, F. 
The three intermediate signals P, Q, and R are denned by 
P = A B , Q = B C . a n d R = A C . Figure 2.13 tells us that 
we can write down the output function, F, as the logical OR of 
the three intermediate signals P, Q, and R; that is, 
F = P + Q + R. 
We can substitute the expressions for P, Q, and R to get 
F = A B + B C + A-C. This is a Boolean equation, but it 
A, B, and C are 
inputs 
r i; 
" P, Q, and R are 
intermediate 
variables 
F is the output 
Figure 2.13 The use of gates—Example 1. 
Inputs 
Intermediate values 
Output 
A B c 
P = A B 
Q = B c 
R - A C 
F - P + Q + R 
0 
0 0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
1 0 
0 
0 
0 
0 
0 
1 
1 
0 
1 
0 
1 
1 
0 0 
0 
0 
0 
0 
1 0 
1 
0 
0 
1 
1 
1 
1 0 
1 
0 
0 
1 
1 
1 1 
1 
1 
1 
1 
Table 2.8 Truth table for Fig. 2.13. 
doesn't help us a lot at this point. However, by visually 
inspecting the truth table for F we can see that the output is 
true if two or more of the inputs A, B, and C, are true. That is, 
this circuit implements a majority logic function whose out-
put takes the same value as the majority of inputs. We have 
already seen how such a circuit is used in an automatic land-
ing system in an aircraft by choosing the output from three 
independent computers to be the best (i.e. majority) of three 
inputs. Using just four basic gates, we've constructed a circuit 
that does something useful. 
Example 2 The circuit of Fig. 2.14 has three inputs, one out-
put, and three intermediate values (we've also included a 
mixed logic version of this circuit on the right hand side of 
Fig. 2.14). By inspecting the truth table for this circuit 
(Table 2.9) we can see that when the input X is 0, the output, 
F, is equal to Y. Similarly, when X is 1, the output is equal to Z. 
The circuit of Fig. 2.14 behaves like an electronic switch, con-
necting the output to one of two inputs, Y or Z, depending on 
the state of a control input X. 
The circuit of Fig. 2.14 is a two-input multiplexer that can 
be represented by the arrangement of Fig. 2.15. Because the 
word multiplexer appears so often in electronics, it is 
frequently abbreviated to MUX. 
A -
B-
D -
C -
A -
B-
C 
CI 
G2 
C3 
£-
^Q 
iR 
IG4 
-F 

2.3 Applications of gates 
35 
Figure 2.14 The use of gates-
Example 2. 
Table 2.9 Truth table for Fig. 2.14. 
>. Output F 
Control input X 
select Y or Z 
Figure 2.15 The logical representation of Figure 2.14. 
We can derive an expression for F in terms of inputs X, Y, and 
Z in two ways. From the circuit diagram of Fig. 2.14, we can 
get an equation for F by writing the output of each gate in 
terms of its inputs. 
F = CH* 
Q = Yl> 
P = X 
Therefore 
Q = Y-X 
R = X^Z 
by substituting for P 
Therefore 
F = Y-X-X-Z 
When we introduce Boolean algebra we will see how this 
type of expression can be simplified. Another way of obtain-
ing a Boolean expression is to use the truth table. Each time a 
logical one appears in the output column, we can write down 
Out 
t 
the set of inputs that cause the output to be true. In Table 2.9 
the output is true when 
(1) X = 0, Y = 1, Z = 0 
(X-Y-Z) 
(2) X = 0, Y = 1, Z = 1 
(X-Y-Z) 
(3) X = 1, Y = 0, Z = 1 
(X-Y-Z) 
(4) X = 1, Y = 1, Z = 1 
(X-Y-Z) 
There are four possible combinations of inputs that make the 
output true. Therefore, the output can be expressed as the 
logical sum of the four cases (l)-(4) above; that is, 
F = X-Y-Z + X-Y-Z + X-Y-Z + X-Y-Z 
This function is true if any of the conditions (l)-(4) are 
true. A function represented in this way is called a sum-of-
products (S-of-P) expression because it is the logical OR (i.e. 
sum) of a group of terms each composed of several of vari-
ables ANDed together (i.e. products). A sum-of-products 
expression represents one of the two standard ways of writing 
down a Boolean expression. 
An alternative way of writing a Boolean equation is called 
a product-of-sums (P-of-S) expression and consists of several 
terms ANDed together. The terms are made up of variables 
ORed together. A typical product-of-sums expression has 
the form 
F = (A + B + C)-(A + B + C)-(A + B + C) 
Later we shall examine ways of converting sum-of-products 
expressions into product-of sums expressions and vice versa. 
Each of the terms (1 )-(4) in Example 2, is called a minterm. 
A minterm is an AND (product) term that includes each of 
the variables in either its true or complemented form. For 
example, in the case above X - Y • Z is a minterm, but if we had 
had the term X • Y that would not be a minterm, because X • Y 
includes only two of the three variables. When an equation is 
expressed as a sum of minterms, it is said to be in its canonical 
form. Canonical is just a fancy word that means standard. 
As the output of the circuit in Fig. 2.14 must be the same 
whether it is derived from the truth table or from the logic 
diagram, the two equations we have derived for F must be 
equivalent, with the result that 
Y-X-X-Z = X-Y-Z + X-Y-Z + X-Y-Z + X-Y-Z 
* Electronic 
* 
switch 
z 
Input Y — 
Input Z — 
Y-
Z-
Xl 
L Q 
vS-
•F 
Y-
Pl 
<£.:' 
z-
xl 
J 
-F 
Mixed logic version 
Inputs 
X Y Z 
0 
0 
0 
0 0 1 
0 
1 0 
0 
1 1 
1 0 0 
1 0 1 
1 
1 0 
1 
1 1 
Intermediate values 
P ---X 
Q ~- P Y 
R - X Z 
1 
1 
1 
1 
1 
1 
1 
0 
1 
1 
0 
1 
0 
1 
1 
0 
1 
0 
0 
1 
1 
0 
1 
0 
F 
Q R 
0 
0 
1 
1 
0 
1 
0 
1 

36 
Chapter 2 Gates, circuits, and combinational logic 
This equation demonstrates that a given Boolean function 
can be expressed in more than one way. 
The multiplexer of Fig. 2.14 may seem a very long way from 
computers and programming. However, multiplexers are 
found somewhere in every computer because computers oper-
ate by modifying the flow of data within a system. A multi-
plexer allows one of two data streams to flow through a switch 
that is electronically controlled. Let's look at a highly simplified 
example. The power of a digital computer (or a human brain) 
lies in its ability to make decisions. Decision taking in a com-
puter corresponds to the conditional branch; for example, 
IF Day = Weekday 
THEN update stock 
ELSE print stock list 
We can't go into the details of how such a construct is imple-
mented here. What we would like to do is to demonstrate that 
something as simple as a multiplexer can implement some-
thing as sophisticated as a conditional branch. Consider 
the system of Fig. 2.16. Two numbers P and Q are fed to a 
comparator where they are compared. If they are the same, 
the output of the comparator is 1 (otherwise it's 0). The same 
output is used as the control input to a multiplexer that 
selects between two values X and Y. In practice, such a system 
would be rather more complex (because P, Q, X, and Y are all 
multi-bit values), but the basic principles are the same. 
Example 3 Figure 2.17 describes a simple circuit with three 
gates: an OR gate, an AND gate, and a NAND gate. This circuit 
Implementing 
IF P = Q 
THENY 
ELSEX 
Output 
Comparator Control, 
The output from 
the comparator is 
true if P = Q 
Multiplexer 
J 
The output from 
the multiplexer is 
Y if the control input is 
true, otherwise X 
has two inputs, two intermediate values, and one output. 
Table 2.10 provides its truth table. 
The circuit of Fig. 2.17 represents one of the most 
important circuits in digital electronics, the exclusive or (also 
called EOR or XOR). The exclusive or corresponds to the 
normal English use of the word or (i.e. one or the other but 
not both). The output of an EOR gate output is true if one of 
the inputs is true but not if both inputs are true. 
An EOR circuit always has two inputs (remember that 
AND and OR gates can have any number of inputs). Because 
the EOR function is so widely used, the EOR gate has its own 
special circuit symbol (Fig. 2.18) and the EOR operator its 
own special logical symbol '©'; for example, we can write 
F = A E O R B = A © B 
The EOR is not a fundamental gate because it is constructed 
from basic gates. 
Because the EOR gate is so important, we will discuss it a 
little further. Table 2.10 demonstrates that F is true when 
A = 0 and B = 1, or when A = 1 and B = 0. Consequently, 
the output F = A B + AB. From the circuit in Fig. 2.17 we 
can write 
F = P-Q 
P = A + B 
Q = AT3 
Therefore 
F = (A + B) • AT3 
As these two equations (i.e. F = A-B + A-B 
and F = (A + B)-AB are equivalent, we can 
therefore also build an EOR function in the 
manner depicted in Fig. 2.19. 
It's perfectly possible to build an EOR with 
four NAND gates (Fig. 2.20). We leave it as an 
exercise for the reader to verify that Fig. 2.20 does 
indeed represent an EOR gate. To demonstrate 
that two different circuits have the same func-
tion, all you need do is to construct a truth table 
for each circuit. If the outputs are the same for 
each and every possible input, the circuits are 
equivalent. 
Figure 2.16 Application of the multiplexer. 
Inputs 
Intermediate values 
Output 
CI 
C2 
C3 
-+-F 
A B 
P = A + B 
Q =• - A B 
F = P-Q 
0 0 
0 
1 
0 
0 
1 
1 
1 
1 
1 0 
1 
1 
1 
1 1 
1 
0 
0 
Figure 2.17 The use of gates—Example 3. 
Table 2.10 Truth table for the circuit of Fig. 2.17. 
p 
Q 
Y 
X 
A-
B-
h2-
P 

2.3 Applications of gates 
37 
The EOR is a remarkably versatile logic element that pops 
up in many places in digital electronics. The output of an 
EOR is true if its inputs are different and false if they are the 
same. As we've already stated, unlike the AND, OR, NAND 
and NOR gates the EOR gate can have only two inputs. The 
EOR gate's ability to detect whether its inputs are the same 
C = AeB 
Figure 2.18 Circuit symbol for an EOR gate. 
t~ 
G3 
F = A.B + A.B 
H>-
Figure 2.19 An alternative circuit for an EOR gate. 
= A0B 
Figure 2.20 An EOR circuit constructed with NAND gates only. 
allows us to build an equality tester that indicates whether or 
not two words are identical (Fig. 2.21). 
In Fig. 2.21 two m-bit words (Word 1 and Word 2) are fed 
to a bank of m EOR gates. Bit i from Word 1 is compared with 
bit i from Word 2 in the ith EOR gate. If these two bits are the 
same, the output of this EOR gate is zero. 
If the two words in Fig. 2.21 are equal, the outputs of all 
EORs are zero and we need to detect this condition in order 
to declare that Word 1 and Word 2 are identical. An AND gate 
will give a 1 output when all its inputs are 1. However, in this 
case, we have to detect the situation in which all inputs are 0. 
We can therefore connect all m outputs from the m EOR gates 
to an m-input NOR gate (because the output of a NOR gate 
is 1 if all inputs are 0). 
If you look at Fig. 2.21 you can see that the outputs from 
the EOR gates aren't connected to a NOR gate but to an 
m-input AND gate with inverting inputs. The little bubbles at 
the AND gate's inputs indicate inversion and are equivalent to 
NOT gates. When all inputs to the AND gate are active-low, 
the AND gate's output will go active-high (exactly what we 
want). In mixed logic we can regard an AND gate with active-
low inputs and an active-high output as a NOR gate. 
Remember that we required an equality detector (i.e. com-
parator) in Fig. 2.21 (Example 2) to control a multiplexer. 
We've just built one. 
Example 4 The next example of an important circuit con-
structed from a few gates is the prioritizer whose circuit is 
given in Fig. 2.22. As this is a rather more complex circuit 
than the previous three examples, we'll explain what it does 
first. A prioritizer deals with competing requests for attention 
and grants service to just one of those requesting attention. 
The prioritizer is a device with n inputs and n outputs. Each 
of the inputs is assigned a priority from 0 to n— 1 (assume 
that the highest priority is input n— 1, and the lowest is 0). 
Wordl 
Bitm-1 
Bit 1 Bit 0 
Each EOR gate compares 
a pair of bits 
Word 2 
Bitm-1 
1 
1 
F (high if Word 1 = Word 2) 
m-input AND gate 
with active-tow inputs 
Bit 1 BitO 
Figure 2.21 The application of 
EOR gates in an equality tester. 
A -
B -
A -
B-
B 
JL. C2 
}A.B 
A.B 
CI 
A 
• 
B 

38 
Chapter 2 Gates, circuits, and combinational logic 
*o 
A 
*2 
*3 
A 
A 
C1 
G2 
-*-yo 
- * y i 
G3 
x4 
A 
G4 
-*• y2 
- > y 3 
-+• y4 
Figure 2.22 Example 4—the priority circuit. 
If two or more inputs are asserted simultaneously, only the 
output corresponding to the input with the highest priority 
is asserted. Computers use this type of circuit to deal with 
simultaneous requests for service from several peripherals 
(e.g. disk drives, the keyboard, the mouse, and the modem). 
Consider the five-input prioritizer circuit in Fig. 2.22. The 
prioritizer's five inputs x„ to x4 are connected to the outputs 
of five devices that can make a request for attention (input x4 
has the highest priority). That is, device i can put a logical 
1 on input X; to request attention at priority level i. If several 
inputs are set to 1 at the same time, the prioritizer sets only 
one of its outputs to 1, all the other outputs remain at 0. 
For example, if the input is x4,x3,x2,x„x0 = 00110, the output 
y4>y3>y2>yi>yo = OOIOO, because the highest level of input is x2. 
Table 2.11 provides a truth table for this prioritizer. 
If you examine the circuit of Fig. 2.22, you can see that out-
put y4 is equal to input x4 because there is a direct connection. 
If x4 is 0, then y4 is 0; and if x4 is 1 then y4 is 1. The value of x4 
is fed to the input of the AND gates G3, G2, and Gl in the 
lower priority stages via an inverter. If X4 is 1, the logical level 
at the inputs of the AND gates is 0, which disables them and 
forces their outputs to 0. If x4 is 0, the value fed back to the 
AND gates is 1 and therefore they are not disabled by x4. 
Similarly, when x} is 1, gates G3, G2 and Gl are disabled, 
and so on. 
Example 5 Our final example looks at two different circuits 
that do the same thing. This is a typical exam question. 
(a) Using AND, OR, and NOT gates only, draw circuits to 
generate Pjind 
Q from inputs X, Y, and Z, where 
P = (X + Y)(Y@Z)andQ 
= YZ + 
XYZ. 
(b) By means of a truth table establish a relationship between 
PandQ. 
Table 2.11 Truth table for the priority circuit of Fig. 2.22. 
(c) Compare the circuit diagrams ofP and Q in terms of speed 
and cost of implementation. 
(a) The circuit diagram for P = (X + Y)(Y©Z) is given by 
Fig. 2.23 and the circuit diagram for Q = Y Z + X-Y-Z 
is give by Fig. 2.24. 
(b) The truth table for functions P and Q is given in 
Table 2.12 from which it can be seen that P = Q. 
(c) We can compare the two circuits in terms of speed 
and cost. 
P = (X + Y)(Y@Z)andQ = Y-Z + X-Y-Z. 
*1 " 
Inputs 
Outputs 
x4 
*3 
><2 
x, 
x 0 
y 4 
y 3 
y 2 
y , 
y 0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
1 
1 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
1 
1 
1 
0 
0 
0 
0 
1 
1 
1 
1 
0 
0 
0 
0 
1 
1 
1 
1 
0 
0 
0 
0 
1 
1 
1 
1 
0 
0 
1 
1 
0 
0 
1 
1 
0 
0 
1 
1 
0 
0 
1 
1 
0 
0 
1 
1 
0 
0 
1 
1 
0 
0 
1 
1 
0 
0 
1 
1 
0 
1 
0 
1 
0 
1 
0 
1 
0 
1 
0 
1 
0 
1 
0 
1 
0 
1 
0 
1 
0 
1 
0 
1 
0 
1 
0 
1 
0 
1 
0 
1 

2.3 Applications of gates 
39 
COMPARING DIFFERENT DIGITAL CIRCUITS WITH THE SAME FUNCTION 
Different combinations of gates may be used to implement 
the same function. This isn't the place to go into the detailed 
design of logic circuits, but it is interesting to see how the 
designer might go about selecting one particular 
implementation in preference to another. Some of the basic 
criteria by which circuits are judged are listed below. In general, 
the design of logic circuits is often affected by other factors 
than those described here. 
Speed The speed of a circuit (i.e. how long it takes the output 
to respond to a change at an input) is approximately governed 
by the maximum number of gates through which a change of 
state must propagate (i.e. pass). The output of a typical gate 
might take 5 ns to change following a logic change at its input 
(5 ns = 5 X 1(T9s). Figs 2.17 and 2.19 both implement an 
EOR function. In Fig. 2.17 there are only two gates in series, 
whereas in Fig. 2.19 there are three gates in series. Therefore 
the implementation of an EOR function in Fig. 2.17 is 50% 
faster. All real gates don't have the same propagation delay, 
because some gates respond more rapidly than others. 
Number of interconnections It costs money to wire gates 
together. Even if a printed circuit is used, somebody has to 
design it and the more interconnections used the more it will 
cost. Increasing the number of interconnections in a circuit 
also increases the probability of failure due to a faulty 
connection. One parameter of circuit design that takes 
account of the number of interconnections is the total number 
of inputs to gates. In Fig. 2.17 there are six inputs, whereas in 
Fig. 2.19 there are eight inputs. 
Number of packages Simple gates of the types we describe 
here are available in 14-pin packages (two pins of which are 
needed for the power supply). As it costs virtually nothing to 
add extra gates to the silicon chip, only the number of pins 
(i.e. external connections to the chip) limits the total number 
of gates in a physical package. Thus, an inverter requires two 
pins, so that six inverters are provided on the chip. Similarly, a 
two-input AND/NAND/OR/NOR gate needs three pins, so 
four of these gates are put on the chip. Because each of these 
circuits uses three different types of gate, both circuits 
require three 14 pin integrated circuits. Even so, the circuit of 
Fig. 2.17 is better than that of Fig. 2.19 because there are 
more unused gates left in the ICs, freeing them for use by 
other parts of the computer system. Note that the circuit of 
Fig. 2.20 uses only one package because all gates are the 
same type. 
You should appreciate that this is an introductory text and 
what we have said is appropriate only to logic circuits 
constructed from basic logic elements. Computer-aided design 
techniques are used to handle more complex systems with 
hundreds of gates. Indeed, complex circuits are largely 
constructed from programmable digital elements. 
YZ+YZ 
Figure 2.23 Circuit diagram for P. 
•W 
•£>oi 
Figure 2.24 Circuit diagram for Q. 
Propagation delay The maximum delay in the circuit for 
P is four gates in series in the Y path (i.e. NOT gate, AND gate, 
OR gate, AND gate). The maximum delay in the circuit for 
Q is three gates in series in both Y and Z paths (i.e. NOT gate, 
AND gate, OR gate). Therefore the circuit for Q is 33% faster 
than that for P. 
Cost Total number of gates needed to implement P is 7. 
Total number of gates needed to implement Q is 5. Total 
inputs in the circuit for P is 12. Total inputs in the circuit for 
Q is 9. Clearly, the circuit for Q is better than that for P both 
in terms of the number of gates and the number of inputs to 
the gates. 
X-
Y-
Z-
_Y 
'W 
z 
hi 
X + Y 
>P 
Y 
Z-
X-
iYZ 
——*Q 
XYZ+ YZ 
^XYZ 

40 
Chapter 2 Gates, circuits, and combinational logic 
X 
Y 
z 
X T Y 
YVT;Z 
P -= (X + Y)(Y@Z) 
Y Z 
X-Y-Z 
Q = Y Z f X-Y-Z 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
1 
1 
1 
1 
1 
0 
1 
0 
1 
0 
0 
1 
0 
0 
0 
0 
0 
1 
1 
0 
0 
0 
0 
0 
0 
1 
0 
0 
1 
0 
0 
0 
0 
0 
1 
0 
1 
1 
1 
1 
1 
0 
1 
1 
1 
0 
1 
1 
1 
0 
1 
1 
1 
1 
1 
1 
0 
0 
0 
0 
0 
Table 2.12 Truth table for Figs 2.23 and 2.24. 
fDigital W o * s 95!5 Untitled 
File 
Edit Circuit View Tools 
Help 
-lDJ*i 
I X > D 
|> 
fr 
a 
® 
i t 
i 
I 
o B 
The pointer toot allows 
you to select an object 
in the window. You can 
click the right-hand 
mouse button to alter 
the object's properties 
or act on it in some 
way. 
This is the push 
button input tool 
that lets you create 
a 0 or a 1 input. 
This is the LED 
symbol that lets you 
see the state of a 
point in the circuit. 
This is the hand 
tool that allows 
you to set the 
state of a switch. 
This is a typical gate symbol. Click on 
it and move the mouse to where you 
want the gate to be positioned. Click 
again and the gate is placed there. 
This is the wiring 
tool that lets you 
wire gates together. 
Figure 2.25 Digital Works—the initial screen. 
2.4 Introduction to Digital Works 
We now introduce a Windows-based logic simulator called 
Digital Works that enables you to construct a logic circuit 
from simple gates (AND, OR, NOT, NAND, NOR, EOR, 
XNOR) and to analyze the circuit's behavior. Digital Works 
also supports the tri-state logic gate that enables you to con-
struct systems with buses. In the next chapter we will discover 
that Digital Works simulates both simple 1-bit storage 
elements called flip-flops and larger memory components 
such as ROM and RAM. 
After installing Digital Works on your system, you can run 
it to get the initial screen shown in Fig. 2.25. We have anno-
tated six of the most important icons on the toolbars. A cir-
cuit is constructed by using the mouse to place gates on die 
screen or workspace and a wiring tool to connect the gates 

2.4 Introduction to Digital Works 
41 
together. The input to your circuit may come from a clock 
generator (a continuous series of alternating Is and Os), a 
sequence generator (a user-defined sequence of Is and Os), or 
a manual input (from a switch that you can push by means of 
the mouse). You can observe the output of a gate by connect-
ing it to a display, LED. You can also send the output of the 
LED to a window that displays either a waveform or a 
sequence of binary digits. 
Digital Works has been designed to be consistent with the 
Windows philosophy and has a help function that provides 
further information about its facilities and commands. The 
File command in the top toolbar provides the options you 
would expect (e.g. load, save, save as). 
2.4.1 Creating a circuit 
We are going to design and test an EOR circuit that has the 
logic function A-B + A-B. This function can be imple-
mented with two inverters, two AND gates, and an OR gate. 
Figure 2.26 shows three of the icons we are going to use to 
create this circuit. The first icon is the new circuit icon that 
creates a fresh circuit (which Digital Works calls a macro). 
The second icon is the pointer tool used to select a gate (or 
other element) from the toolbars. The third icon is a gate that 
can be planted in the work area. 
Let's start by planting some gates on the work area. The 
EOR requires two AND gates, an OR gate, and two inverters. 
First click on the pointer tool on the bottom row of icons. If it 
hasn't already been selected, it will become depressed when 
you select it. The pointer tool remains selected until another 
tool is selected. 
You select a gate from the list on the second row of icons by 
first left clicking on the gate with the pointer tool and then left 
clicking at a suitable point in the workspace as Fig. 2.27 
demonstrates. If you hold the control key down when placing 
a gate, you can place multiple copies of the gate in the work-
space. The OR gate is shown in broken outline because we've 
just placed it (i.e. it is currently selected). Once a gate has been 
placed, you can select it with the mouse by clicking the left 
button and drag it wherever you want. You can click the right 
button to modify the gate's attributes (e.g. the number of 
inputs). 
You can tidy up the circuit by moving the gates within the 
work area by left clicking a gate and dragging it to where you 
want it. Figure 2.28 shows the work area after we've moved 
the gates to create a symmetrical layout. You can even drag 
gates around the work area after they've been wired up and 
reposition wires by left clicking and dragging any node 
(a node is a point on a wire that consists of multiple sections 
or links). 
Digital Works displays a grid to help you position the gates. 
The grid can be turned on or off and the spacing of the grid 
lines changed. Objects can be made to snap to the grid. These 
functions are accessed via the View command in the top line. 
Before continuing, we need to save the circuit. Figure 2.29 
demonstrates how we use the conventional File function in 
the toolbar to save a circuit. We have called this circuit 
O U P J O R l and Digital Works inserts the extension .dwm. 
The next step is to wire up the gates to create a circuit. First 
select the wiring tool from the tool bars by left clicking on it 
(Fig. 2.30). Then position the cursor over the point at which 
you wish to connect a wire and left click. The cursor changes 
to wire when it's over a point that can legally be connected to. 
Left click to attach a wire and move the cursor to the point 
you wish to connect. Left click to create a connection. Instead 
of making a direct connection between two points, you can 
^Digital Work<v95 -Untitled 
File 
Edit 
Circuit 
View lools 
Help 
P|x| 
This is one of the gates 
that you can select and put 
in the work area. 
This icon creates a new 
macro. If a circuit is 
already open, you will be 
invited to save it. 
This is the pointer tool and 
is the most important icon 
because you use it to 
select objects. 
Figure 2.26 Beginning a session with Digital Works. 

42 
Chapter 2 Gates, circuits, and combinational logic 
giMfiiiiiiisiiiiiiai 
File 
Edit Circuit View Tools 
Help 
D 
GS 
fit 
•• .. 
3 
& 
O 
DO 
B> 
" j ^ 
-Inlxj 
mm 0 
i 
+ 
A /? 
This is the icon for 
a NOT gate 
(inverter). Select it 
with the pointer 
tool and left click in 
the workspace at 
the point you wish 
to place it. 
- [ » 
H>o 
This was the last 
item placed. Note 
that it remains 
dotted until the 
next item is 
selected. 
Figure 2.27 Placing gates in the work area. 
SllSilSilSSilsijttftJiitii 
File 
Edit 
Circul| View Jools 
Help 
D 
& 
b I > D I > 9 ^ D 
O 
«• 
e 
a 
s 
® 
1 
o Q 
e> 
o 
sM 
• m 
The View 
command is used 
to set up the grid, 
display it, or select 
snap-to-grid. 
^>c^ 
-fx> 
You can select a 
gate by left clicking 
on it and then drag 
it to a suitable point 
in the work area. 
Figure 2.28 Tidying up the circuit. 
click on the workspace to create a node (i.e. the connection is 
series if straight lines.) 
You can make the wiring look neat by clicking on interme-
diate points to create a signal path made up of a series of 
straight-line segments. If you select the pointer tool and left 
click on a wire, you can drag any of its nodes (i.e. the points 
between segments on a line). If you right click on a wire you 
can delete it or change its color. Once a wire has been 

2.4 Introduction to Digital Works 
43 
'W$^^^a^M^f§^^&$li 
-lof: 
E H Edit Cjreuit Yjew Tools Help 
New Macro 
• 
a 
Open\ 
= 
C
l
>
f
r
i
l
l
S
S
B
l O 0 
33 i t 
m 
^ r 
+• 
O 
A 
9 
Sgve As 
\ 
* fc 
Print Setup... 
\ 
A 
Print Preview... 
\ 
, 
Print 
Exit 
\ 
-oo-
U> 
3> 
Click on the file 
command to 
U> 
3> 
<i 
bring down the 
menu. 
I > 
<i 
I > 
•r 
Figure 2.29 Saving the circuit. 
liiiliiiSiii^BittiiiiiHiii 
File Edit Circuit View Tools Help 
D & a 
• 
•- 
& 
j> 
! > D I > C « D 
|> 
fr 
@ e n s 
i 
1 
o 
£3 
f> 
O 
10 
B> 
t^ 
^ ) 
HolKt 
STEP 1 Click on 
the output of the 
gate we wish to 
connect. 
STEP 2 Click 
on the input we 
wish to 
connect the 
output to. 
AQ) 
Click on the 
wiring tool to 
connect gates 
together. 
Figure 2.30 Wiring gates together. 
connected to another wire (or an input or output), the con-
nection point can't be moved. To move a connection you have 
to delete the wire and connect a new one. 
Digital Works permits a wire to be connected only between 
two legal connections. In Fig. 2.30 the inputs to the two 
inverters and the circuit's outputs aren't connected anywhere. 
This is because each wire must be connected between two 
points—it can't just be left hanging. In order to wire up the 
inputs and output we need points we can connect the wire to. 
In this case we are going to use the interactive input device 
to provide an input signal from a push button and the LED to 
show the state of the output. 
In Fig. 2.31 we've added two interactive inputs and an LED 
to the circuit. When we run the simulator, we can set the 
states of the inputs to provide a 0 or a 1 and we can observe 
the state of the output on the LED. 

44 
Chapter 2 Gates, circuits, and combinational logic 
WmMMB^MB$WM!^^&mi 
File 
Edit 
Circuit View loots 
Help 
D § 
I 
: - 
* 
:r- 
B 
E > I > D l > i X ' D l > ' f ? - @ s s J B l o E I 
O 
O 
00 
0> 
sE 
Figure 2.31 Adding inputs and outputs to the circuit. 
:i^0M^i§^:M^G^^^I^§^i 
File 
Edit 
Circuit View 
Tools 
Help 
D 
G£ Q 
•• 
;^- * 
s 
D I> £> E» »° D O - t r - i s u s s r s r s i I 
a 
a 
t> 
o 
no 
B> 
fe 
I5 
r-t>o^ 
-{X> 
~C> 
-o 
The text tool is 
used to add labels 
and comments to 
the work area. 
Figure 2.32 Completing the circuit. 
We can now wire up the inputs and the output and com-
plete the rest of the wiring as shown in Fig. 2.32. At this stage 
we could run the circuit if we wanted. However, we will use 
the text tool (indicated by the letter A on the middle toolbar) 
to give the circuit a title. Click on the A and then click on the 
place at which you wish to add the text to open the text 
window. This brings down a text box. Enter the text and click 
ok to place it on the screen. 
We also wish to label the circuit's inputs and outputs. 
Although you can use the text tool to add text at any point, 
input and output devices (e.g. clocks, switches, LEDs) can be 
given names. We will use this latter technique because the 
The interactive 
tool allows you 
to generate 
digital inputs. 
The LED is selected 
like any other device 
and placed on the 
workspace. 
The hand 
tool is used 
to operate 
the input. 

2.4 Introduction to Digital Works 
45 
'ig^s^^^^^M^Mmi^mi 
File Edit Circuit yiew lools 
Help 
D t> D 
I> »« 
D |> 
fr 
S3 ® 
® 
l i 
1 
o B 
O 
O 
DJ 
H> 
[^ 
^ ) 
-Inlxl 
• i 
+ . o 
A 
^ 
EOR circuit 
Right dick on the LED 
to bring down the 
menu and dick on Text 
to select the text box. 
Change Color 
• 
Iext... 
Figure 2.33 Labeling the circuit and inputs and outputs. 
names attached to input and output devices are automatically 
used to label the timing diagrams we will introduce later. 
Figure 2.33 shows the circuit with annotation. The label 
EOR circuit has been added by the text tool, and inputs A and 
B have been labeled by right clicking on the input devices. In 
Fig. 2.33 we have right clicked on the LED to bring down 
a menu and then selected Text to invoke the text box (not 
shown). You enter the name of the output (in this case Sum) 
into the text box and click ok. This label is then appended to 
the LED on the screen. You can change the location of the 
label by right clicking on its name, selecting Text Style from 
the menu, and then selecting the required position (Left, 
Right, Top, Bottom). 
2.4.2 Running a simulation 
We are now ready to begin simulation. The bottom row of 
icons is concerned with running the simulation. The leftmost 
icon (ringed in Fig. 2.34) is left clicked to begin the simulation. 
The next step is to change the state of the interactive input 
devices. If you click on the hand tool icon, the cursor changes 
to a hand when positioned anywhere over the work area. 
By putting the hand cursor over one of the input devices, 
you can left click the mouse to change the status of the input 
(i.e. input 0 or input 1). When the input device is supplying 
a 1, it becomes red. Figure 2.34 shows the situation input 
A = 1, B = 0, and the Sum = 1 (the output LED becomes 
red when it is connected to a 1 state). You can change the 
states of the input devices to generate all the possible input 
values 0,0,0,1,1,0, and 1,1 to verify that the circuit is an EOR 
(the output LED should display the sequence 0,1,1,0). 
Just observing the outputs of the LEDs is not always 
enough to get a picture of the circuit's behavior. We need a 
record of the states of the inputs and outputs. Digital Works 
provides a Logic History function that records and displays 
inputs and outputs during a simulator run. Any input or out-
put device can be added to Logic History. If you select input 
A with the pointer tool and then right click, you get a pull 
down menu from which you can activate the Add to Logic 
History function to record the value of input A. When this 
function is selected (denoted by a tick on the menu), all input 
is copied to a buffer (i.e. store). As we have two inputs, A and 
B, we will have to assign them to the Logic History function 
independently. 
To record the output of the LED, you carry out the same 
procedure you did with the two inputs A and B (i.e. right 
click on the LED and select Add to Logic History) (see 
Fig. 2.35). 
In order to use the Logic History function, you have to 
activate it from the Tools function on the toolbar. Selecting 
Tools pulls down a menu and you have to select the Logic 
History window. Figure 2.36 shows the logic history window 
after a simulation run. Note that the inputs and outputs have 
the labels you gave them (i.e. A, B, and Sum). 
We now need to say something about die way the simulator 
operates. The simulator uses an internal clock and a record of 
the state of inputs and outputs is taken at each clock pulse. 
Figure 2.37 shows how you can change the clock speed from 

46 
Chapter 2 Gates, circuits, and combinational logic 
siffiiiilitSiiiiiiliMiiiil 
File 
Edit Circuit View Tools 
Help 
D & H 
6 
D I> D 1> w D |> if- B s tu i 
1 o Q 
f t> JO 
Bll B> 
t i ( | j 
nix 
am @ 
The run 
tool is used 
to begin a 
simulation. 
The LED connected to the 
output becomes red when 
the output is a 1. 
The hand tool is used to 
operate the input switches. 
First click on the hand tool to 
select it. Then move the 
hand cursor to the switch 
(interactive input device) you 
wish to operate. Each click 
changes the input state. A 
logical one state is shown by 
the switch becoming red. 
Figure 2.34 Running the simulator. 
-^prp-r;-^!*-.;";-^—;."-}-;^ 
File 
Edit 
Circuit View loots 
Help 
D & H 
3 
I X > | 3 l > f r B B t 3 a j O E l 
33 Jill 13 
i> O . II B>/ 
k 
M3 
Change Color 
lext... 
Text Style 
ilHlBIgJfflSolI 
fielete 
Use the pointer tool to 
select and right click 
on the output LED. 
This brings down the 
attributes menu. Click 
on Add to Logic 
History to record the 
output values 
generated during a 
simulation. 
Figure 2.35 Recording inputs and output. 
the toolbar by pulling down the Circuit menu and selecting 
the signals are read and recorded at each clock pulse, the entire 
Clock Speed. 
simulation is over in a second or so. Blink and you miss it. 
We're not interested in clocks at this stage because we are 
We need to stop the clock to perform a manual simulation, 
looking at a circuit that doesn't have a clock. However, because 
The Logic History window contains a copy of the run, stop, 
EOR circuit 
-4-#Sum) 
~—fcQRxiiUJit 

2.4 Introduction to Digital Works 
47 
File 
Edit 
Circuit 
View/ Tools \Help 
D & 
H 
> o 
ii no fc 
- P x 
File 
Options 
Help 
Cycle 11 completed 
Select the Tools 
function to pull down 
a menu and then 
select Logic History 
window. 
This is the Logic 
History window that 
displays the inputs 
and outputs as a 
waveform. 
!> 
O 
ii » 
M 1 
iJ 
A 
B 
Sam 
I 
L 
r — 
A 
B 
Sam 
I 
L 
— 
— 
A 
B 
Sam 
A 
B 
Sam 
"" 
T 
A 
B 
Sam 
Figure 2.36 The logic history window. 
Figure 2.37 Changing the clock rate. 
pause, and single-step icons to allow you to step through the 
simulation. Fig. 2.38 provides details of the Logic History 
window. The waveform in Fig. 2.38 was created by putting the 
simulator in the pause mode and executing a single cycle at a 
time by clicking on the single-step button. Between each cycle 
we have used the hand tool to change the inputs to the EOR 
gate. We can use the hand tool to both change the state of the 
inputs and to single step (you don't have to use the pointer 
tool to perform a single step). 
The logic history can be displayed either as a waveform as 
in Fig. 2.38 or as a binary sequence as in Fig. 2.39 by clicking 
on the display mode icon in the Logic History window. You 
can also select the number of states to be displayed in this 
window. 
-OSurn 
EOR circuit 
Ete 
Edit e n l 
View Ioote 
Help 
Run 
Stop 
Eause 
Step Circuit 
1 Hertz 
: 
2 Hertz 
EOR circuit 
10 Hertz 
Digilul Works 3!i - OUP EORI riwm 

48 
Chapter 2 Gates, circuits, and combinational logic 
The run 
button begins 
a simulation. 
The stop 
button 
terminates a 
simulation. 
The pause 
button 
suspends 
simulation. 
The single 
step button 
executes a 
single clock 
cycle. 
Figure 2.38 Controlling the simulation. 
IgjidMiltiiltoivl 
File 
Options 
Help 
t> 
O 
18 
II> 
A 
B 
Sum 
000001111 noooocrsmm 
0000000000011111111TKQ 
00000111111111110000001 
Cycle 12 completed 
The toggle display 
button switches 
between waveform 
and binary display 
modes. 
These buttons 
allow you to 
specify the 
number of clock 
cycles in the run. 
Figure 2.39 Viewing the simulation as a binary sequence. 
2.43 The clock and sequence generator 
Inputting data into a digital circuit by using the hand tool to 
manipulate push buttons and switches is suitable for simple 
circuits, but not for more complex systems. Digital Works 
provides two means of generating signals automatically. One 
is a simple clock generator, which produces a constant stream 
of alternating Is and Os and the other is a sequence generator, 
which produces a user-defined stream of Is and Os. The 
sequence generator is controlled by Digital Works' own clock 
and a new 1 or 0 is output at each clock pulse. Figure 2.40 
shows the icons for the clock and pulse generator and 
demonstrates how they appear when placed in the work area. 
Figure 2.41 demonstrates how you can define a sequence of 
pulses you wish to apply to one of the inputs of a circuit 
(in this example, a single AND gate). One of the inputs to the 
AND gate comes from the clock generator and the other from 
the sequence generator. We've added LEDs to the gate's inputs 
and output to make it easy to observe the state of all signals. 
Let's go through the operations required to place and set 
up a sequence generator (called a bit generator by Digital 
Works). First left click on the sequencer icon on the toolbar 
and then move the cursor to the point at which you wish to 
locate this device in the workspace. Then right click to both 
place it in the workspace and bring down the menu that con-
trols the bit generator. From the pull-down menu, select Edit 
Sequence and the window shown in Fig. 2.41 appears. You 
can enter a sequence either from the computer's keyboard or 
by using the mouse on the simulated keyboard in the Edit 
Sequence window. You can either enter the sequence in 
File 
Options 
Help 
Cycle Vl cornpletad 
St/m 

2.4 Introduction to Digital Works 
lgi|^|^irj®sii)|Htit!|i[S 
File Edit 
Stop 
ifiew Jools Help 
Pause 
t> 
o /« 
Step Circuit 
The Cjrcuit menu allows 
you to change the 
speed of the clock. 
^ e a is a 1 o Q fsTpjt j 
1 Hertz 
' 
5 Hertz 
TO Hertz 
The sequencer icon is 
used to place a pattern 
generator in the work 
area. You can set this to 
generate any sequence 
of Is and Os. 
sequence'. 
-1QJX| 
The dock icon 
is used to 
place a clock 
generator in 
the work area. 
ClOCfiO-
Figure 2.40 The clock generator and sequencer. 
D I > H > » O D > « - a B a i i 
o o w •> 
1^ ^5 
O £3 
s 
A t) 
The sequence 
device has been 
selected and right 
clicked. From the 
pull down menu 
'Edit Sequence' is 
selected and the 
calculator appears 
The calculator lets 
you enter a sequence 
in either binary or 
hexadecimal form. 
When you run the 
simulation, the bits of 
this sequence are fed 
to the input. 
EStt|?eqHfen4i; 
QCDQ 
CDSQ 
The sequence generator 
tool we have used to 
provide user-defined 
input. 
Cancel 
.r1 
j Start By Microsoft Woid - Do..|% Digital Works SS 
Figure 2.41 Setting up the sequence generator. 
•secjuenceo 
\t 
e Jocko—y~*— 
Ble Edit fitcurt i?ew Ioole Help 

50 
Chapter 2 Gates, circuits, and combinational logic 
binary or hexadecimal form (see Chapter 4 for a discussion of 
hexadecimal numbers). 
We can run the simple circuit by clicking on the run icon. 
When the system runs you will see the LEDs turn on and off. 
The speed of the clock pulses can be altered by clicking on 
Circuit in the toolbar to pull down a menu that allows you to 
set the clock speed. 
2.4.4 Using Digital Works to create 
embedded circuits 
Up to now, we have used Digital Works to create simple cir-
cuits composed from fundamental gates. You could create an 
entire microprocessor in this manner, but it would rapidly 
become too complex to use in any meaningful way. Digital 
Works allows you to convert a simple circuit into a logic ele-
ment itself. The new logic element can be used as a building 
block in the construction of more complex circuits. These 
complex circuits can be converted into new logic elements, 
and so on. Turning circuits into re-usable black boxes is anal-
ogous to the use of subroutines in a high-level language. 
Let's take the simple two-input multiplexer described in 
Fig. 2.42 and convert it into a black box with four terminals: 
two inputs A and B, a control input C whose state selects one 
of the inputs, and an output. When we constructed this cir-
cuit with Digital Works, we used the macro tag icon to place 
macro tags at the circuit's inputs and outputs. A macro tag can 
be wired up to the rest of the circuit exactly like an input or 
output device. You left-click on the macro tag icon to select it 
and then move the cursor to the place on the workspace you 
wish to insert the macro tag (i.e. the input or output port). 
Then you wire the macro tag to the appropriate input or out-
put point of the circuit. Note that you can't apply a macro tag 
to the input or output of a gate directly—you have to connect 
it to an input or output by a wire. 
You can also place a macro tag anywhere within the work-
space by right clicking the mouse when using the wiring tool. 
Right clicking terminates the wiring process, inserts a macro 
tag, and activates a pull-down menu. 
We are going to take the circuit of Fig. 2.42 and convert it 
into a black box with four terminals (i.e. the macro tags). This 
new circuit is just a new means of representing the old 
circuit—it is not a different entity. Indeed, this circuit doesn't 
have a different file name and is saved in the same file as the 
original circuit. 
The first step is to create the macro (i.e. black box) itself. 
This is a slightly involved and repetitive process because you 
have to repeat the procedure once for each of the macro tags. 
Place the cursor over one of the macro tags in Fig. 2.43 and 
right click to pull down the menu. Select Template Editor 
from the menu with a left click. A new window called 
Template Editor appears (Fig. 2.43). You create a black box 
representation of the circuit in this window. Digital Works 
allows you to draw a new symbol to represent the circuit (in 
Fig. 2.43 we've used a special shape for the multiplexer). 
Wi$B^IXo!&&i^MF&&im^ 
'WSMSS^MWSW^^^S^^^^^i 
m^L.r--. 
,-- M 
File Edit Circuit View J_ools Help 
D g g 
-, -. 
a. 
m m m I 
o f BJ ss m m ± 
+ 
D E » I > S > i > o D l > - ^ « s t 
m m m I 
o f BJ ss m m ± 
+ 
A 
9 
t> 
O 
01 
B> 
& 
^5 
V 
—i 
The macro tag 
tool is selected 
to place four 
—i 
The macro tag 
tool is selected 
to place four 
—i 
f 
The macro tag 
tool is selected 
to place four 
—i 
1 :. 
tags at the 
inputs and 
outputs of the 
circuit. 
The macro tag 
allows you to 
define an 
interface 
between a 
circuit and the 
/
'
• 
* 
^~y (°v 
tags at the 
inputs and 
outputs of the 
circuit. 
The macro tag 
allows you to 
define an 
interface 
between a 
circuit and the 
/ 
n 
tags at the 
inputs and 
outputs of the 
circuit. 
The macro tag 
allows you to 
define an 
interface 
between a 
circuit and the 
J^ 
The macro tag 
allows you to 
define an 
interface 
between a 
circuit and the 
J^ 
The macro tag 
allows you to 
define an 
interface 
between a 
circuit and the 
outside world 
d i 
i 
<l 
1 
•j 
•j 
Figure 2.42 Converting the two-input multiplexer circuit into a black box. 

2.4 Introduction to Digital Works 
51 
Figure 2.43 shows the Template Editor window. We have 
used the simple polyline drawing tool provided by Digital 
Works to create a suitable shape for the representation of the 
multiplexer. You just click on this tool in the Template Editor 
window and draw the circuit by clicking in the workspace at 
the points you wish to draw a line. You exit the drawing mode 
by double clicking. You can also add text to the drawing by 
using the text tool. Figure 2.43 shows the shape we've drawn 
j^||i|i|ill|i|E|(itoi|' 
Help 
Pin icon. This is used 
to place the pins on 
the representation of 
the circuit 
Text tool. 
Use this to 
label the 
symbol. 
D Multiplexer' 
Polyline drawing tool. 
Use this tool to create a 
suitable symbol for your 
circuit. 
This is the 
label we've 
added to the 
multiplexer. 
This is the 
symbol we've 
drawn for the 
multiplexer. 
Figure 2.43 Drawing a symbol for the new circuit. 
i||implBBB|^itg5ri 
File View 
Options 
Help 
D G£ y 
k O a a v 
The pin tool 
creates an 
interface point 
between the black 
box and circuit. 
Pin 1 Selected 
This is the location of the 
first pin. Right click it to 
associate it with the 
macro tag in the circuit 
diagram.  
Figure 2.44 Creating an interface point in the black box. 
for the multiplexer and the label we've given it. To add a label 
or text to the circuit, select the text tool and click on the point 
you wish to insert the text. This action will pull down the Edit 
Text box. 
The next step is to add pins to the black box in the 
Template Editor window and associate them with the macro 
tags in the original circuit of Fig. 2.42. Once this has been 
done, you can use the black box representation of the multi-
plexer in other circuits. The pins you 
have added to the black box are the 
connections to the circuit at the 
macro tags. 
Click on the pin icon in the 
Template Editor and then left click in 
the workspace at the point you wish 
to locate this pin—see Fig. 2.44. You 
then right click on this new pin 
and select Associate with Tag. This 
operation associates the pin you have 
just placed with a macro tag in the 
circuit diagram. Each new pin placed 
on the circuit in the Template Editor 
window is automatically numbered 
in sequence. 
We add additional pins to the black 
box by closing the Template Editor, 
going back to the circuit, clicking on 
one of the unassigned pins, and 
selecting Associate with Tag again. 
Remember that Digital Works auto-
matically numbers the pins in the 
circuit diagram as you associate them 
with tags. We can finish the process by 
using the text tool to add labels to the 
four pins see Fig. 2.45. We have now 
created a new element that behaves 
exactly like the circuit from which it 
was constructed and which can be 
used itself as a circuit element. 
Figure 2.46 shows the original or 
expanded version of the circuit. Note 
how the pins have been numbered 
automatically. 
To summarize, you create a black 
box representation of a circuit by car-
rying out the following sequence of 
operations. 
• In Digital Works add and connect 
(i.e. wire up) a macro tag to your 
circuit. 
• Right click the macro tag to enter 
the template editor. 
Multiplexer 
File 
View 
Options 

52 
Chapter 2 Gates, circuits, and combinational logic 
Use the Template Editor to add a pin to the circuit 
representation 
In the Template Editor, select this pin and right click to 
associate it with the macro tag in the circuit diagram. 
IftiSSiilBiifts? 
File 
D 
View Options 
o a a> A 
Help 
• Close the Template Editor. 
• Repeat these operations, once for each macro tag. 
When you exit Digital Works, saving your circuit also saves 
its black box representation. You can regard these two circuits 
as being bound together—with one 
representing a short-hand version of 
the other. Note that the Template 
Editor also has a save function. Using 
this save function simply saves the 
drawing you've created but not the 
pins, the circuit, or its logic. 
This icon is a switch. 
When down, the macro 
will display the pins. 
When up, an embedded 
macro will not show pins. 
This logic element 
will behave exactly 
like the circuit of the 
multiplexer. It is a 
multiplexer with all 
its internal details 
hidden. 
Figure 2.45 The completed black box representation. 
2.4.5 Using a macro 
Having created a black box circuit 
(i.e. a macro), we can now use it as a 
building block just like any other 
logic element. We will start a new cir-
cuit in Digital Works and begin with 
an empty work area. The macro for a 
two-input multiplexer we have just 
created and saved is used like other 
circuit elements. You click on the 
embed macro icon (see Fig. 2.47) and 
move the pointer to the location in 
the workspace where you wish to 
File 
Edit 
Circuit View Tools 
Help 
D e£ y 
- "._. ...• 
B 
t> 
o 
m oe> fe ^3 
Multiplexer 
1 B 
2 Q -
3fl 
The pins (macro tags) 
are automatically 
numbered as they are 
associated with pins in 
the Template Editor). 
Figure 2.46 The original circuit with the macro tags numbered. 
C~_^ 
Output 
J Multiplexer 
C 
Control 
• B ^JL-
Unjital Workf. 3b - OUI>._MPI.X (iwm 

2.4 Introduction to Digital Works 
53 
Spagiti||^rl«^|ign|i!Ba| 
File £dit 
Circuit View Tools Help 
D & H 
- B 
D> D° I> I> e» 
D 
D° -t^ l^t S3 OB Hi 
i> 
o 
no m> . [^ 
^ 5 
nsH 
* + 
This is one of the 
two macros we 
have placed in 
this window. 
This is the embed 
macro icon. Click 
on it and then 
place the cursor 
at the point you 
wish to place the 
macro. Left dick 
and then select 
the appropriate 
macro from the 
file list. 
Figure 2.47 Embedding a macro in a circuit. 
place the macro. Then you left click and select the appropri-
ate macro from the pull-down menu that appears. 
The macro is automatically placed at the point you clicked on 
and can be used exactly like a circuit element placed from one of 
the circuit icons. Remember that the macro is the same as the 
circuit—the only difference is its on-screen representation. 
In Fig. 2.47 we have placed two of the multiplexers in the 
workspace prior to wiring them together. Figure 2.48 demon-
strates how we can wire these two macros together, add a gate, 
and provide inputs and LED displays. 
Modifying a circuit 
Suppose you build a circuit that contains one or more macros 
(e.g. Fig. 2.48) and wish to modify it. A circuit can be modi-
fied in the usual way by opening its file in Digital Works and 
making any necessary changes. Digital Works even allows you 
to edit (i.e. modify) a circuit while it's running. 
In order to modify a macro itself, you have to return to the 
macros expanded form (i.e. the circuit that the macro repre-
sents). A macro is expanded by right clicking on the macro's 
symbol and selecting the Edit Macro function from the pull-
down menu that appears. Figure. 2.49 shows die system of 
Fig. 2.48 in which the macro representation of the multi-
plexer in the upper left-hand side of the workspace has been 
right clicked on. 
Selecting the Edit Macro function converts the black box 
macro representation into the original circuit as Fig. 2.50 
demonstrates. You can now edit this circuit in the normal 
way. When editing has been completed, you select the Close 
Macro icon that appears on the lower toolbar. Closing this 
window returns to the normal circuit view, which contains 
the macro that has now been changed. 
There are two macros in the circuit diagram of Fig. 2.48. If 
we edit one of them what happens to the other and what 
happens to the original circuit? Digital Works employs object 
embedding rather than object linking. When a macro is 
embedded in a circuit, a copy ofthemacroisembeddedinthe 
circuit. If you modify a macro only that copy is changed. The 
original macro is not altered. Moreover, if you have embed-
ded several copies of a macro in a circuit, only the macro that 
you edit is changed. 
Figure 2.51 demonstrates the effect of editing the macro 
version of a two-input multiplexer. Figure 2.51(a) shows the 
modified expanded macro. An OR gate has been wired to 
the A and B inputs on pins 1 and 2 and a macro tag added 
to the output of the OR gate. By clicking on the macro tag, the 
Template Editor window is invoked. You can add a pin and 
assign it to the macro tag. When you exit the Template Editor 
and close the macro, the final circuit of Fig. 2.51 (b) appears 
(we have added an LED to the output of the new macro). 
HA 
~~~— 
- _ 
Output 
JMultiplexer 
C 
, | 
Control 
3A 
^ ~ ~ ~ 1 
-_ 
Output 
[Multiplexer 
C 
Control 
3 B _ j i ^ — - — • 

54 
Chapter 2 Gates, circuits, and combinational logic 
Mm^^M^^S&^SiW&^im 
File Edit Circuit View loots 
Help 
D S S 
'. 
• 
a 
t> o 
oi s> 
& 
K 
-Inlxl 
•ir 
4* 
o 
-# 
We have used the 
embed macro tool twice 
to place two full adders 
on the work area. 
Figure 2.48 Embedding two macros, wiring them, and creating a new macro. 
^|j!gfi^iyfll^J©li®Z!ip^3iiin| 
File 
Edit 
Circuit View 
Tools 
Help 
D & H 
\ 
3 
P D» D I> w D o -tj- ai ® to ID 
> 
o 
no 
JO 
(&} 
•H 
o 
E 
* + 
In order to edit a 
macro, select the 
pointer tool and right 
click on the macro to 
pull down a menu 
with the Edit Macro 
option. 
O 
Figure 2.49 Editing a macro in a circuit. 
DA 
- ~ , 
-», 
Output 
[Multiplexer 
E — 
Control 
•*• 
C ^ 
Output 
JMultiplexer 
G — 
I 
Control 
.-
Output! 
; Multiplexer 
'.#— 
! 
Control 
' 
Edit Macro... 
i 
- 
— 
itput 
' 
Delete 
C — 
Control 
B— 
®-
o— 
ra-
i l — 
n— 

2.4 Introduction to Digital Works 
fSMm^^aMmS^mMMimMMmr, 
File 
Edit 
Circuit View 
Tools 
Help 
D 
D» D 
I > c» 
D 
> 
-If- ® 
S 
® 
I I 
1 
O H 
nsra 
± 
+ 
t> 
O 
JO i> 
$ 
^ r h Close Macro 
This is the 
expanded 
macro. It 
can be 
modified 
just like any 
other. 
3H 
Once the 
expanded version 
of the macro has 
been edited, you 
can return to the 
circuit that 
embeds the macro 
by clicking on 
Close Macro. 
Figure 2.50 Editing the expanded form of the macro. 
ffliiitSllMrMiSl&PfSMRiSS^wml 
File 
Edit 
Circuit 
View 
Tools 
Help 
D 
: 
a 
I> I> I) I> » 
D I> P- » 
03 IB HI 
> 
O 
10 «t> 
tk h 
O Close Macro 
5 B -
li 
Multiplexer 
(a) The modified macro. 
Figure 2.51 Example of editing a macro. 
WE 
O 
E 
-a 4 
mM^0^a^mi^mggMPi^^^ 
File Edit Circuit View Tools Help 
D G* O 
' . 
3 
P 
! > I > I > o < . D I > * ? - i s i a i t s f s i o E l 
t> 
o 
Hi •> 
t i ?3 
l - | o | x | 
(b) The circuit with the modified macro. 
Multiplexer 
C_ 
O.utputj 
. . 
jMulIlpiexer 
3 
1 
-
f" 
Control QRGJ 
O 
U——~^~^ 
t 
r-
. 
bjA • 
~ ""——, • 
U_ 
Output . : . ,V 
JMuttiplsKer 
G—-* 
L ' Control 
o— 
a— 
a— 
a— 
m— 
2Q-»— 
3A 
1 B-J-
1 B— 
B4 
2 Q 
55 

56 
Chapter 2 Gates, circuits, and combinational logic 
2.5 An introduction to Boolean 
algebra 
We've already seen that you can describe circuits containing 
gates in terms of variables and AND, OR, and NOT operators. 
Consider an AND gate with input variables A and B, and an 
output C. We can write the Boolean equation C = A • B which 
uses variables A, B, and C and the AND operator. In this sec-
tion we introduce Boolean algebra1, show how equations are 
manipulated, and demonstrate how logic circuits can be con-
structed with only one type of gate. Students requiring only a 
very basic knowledge of Boolean algebra can omit some of 
the fine detail that appears later in this section. 
George Boole was an English mathematician (1815-1864) 
who developed a mathematical analysis of logic and pub-
lished it in his book An Investigation of the Laws of Thought in 
1854. Boole's algebra of logic would probably have remained 
a tool of the philosopher, had it not been for the development 
of electronics in the Twentieth Century. 
In 1938 Claude Shannon published a paper entitled 'A 
symbolic analysis of relays and switching circuits', which 
applied Boolean algebra to switching circuits using relays. 
Such circuits were widely used in telephone exchanges and 
later in digital computers. Today, Boolean algebra is used to 
design digital circuits and to analyze their behavior. 
Digital design is concerned with the conversion of ideas or 
specifications into hardware and Boolean algebra is a tool 
that facilitates this process. In particular, Boolean algebra 
permits an idea to be expressed in a mathematical form and 
the resulting expression to be simplified and then translated 
into the real hardware of gates and other logic elements. 
Let's begin with a formal definition just in case this book 
falls into the hands of a mathematician. Boolean algebra (or 
any other algebra) consists of a set of elements E, a set of 
functions F that operate on members of £, and a set of basic 
laws called axioms that define the properties of E and F. The 
set of elements making up a Boolean algebra are variables and 
constants that have fixed values of 0 or 1. A Boolean algebra 
with « variables has a set of 2" possible permutations of these 
variables. 
Only three functions or operations are permitted in 
Boolean algebra. The first two are the logical OR represented 
by a plus (e.g. A + B) and the logical AND represented by a 
dot (e.g. A-B). Some texts use a u (cup) or a v to denote the 
logical OR operator and a n (cap) or a A to denote a logical 
AND operator. 
The use of the plus and dot symbols is rather confusing 
because die same symbols are used for addition and multipli-
cation in everyday life. One reason that these particular sym-
bols have been chosen is that they behave rather like 
conventional addition and multiplication. Another possible 
reason Boole chose + and • to represent the logical OR and 
AND functions is that Boole's background was in probability 
theory. The chance of throwing a 1 or a 2 with two throws of 
a single die is 1/6 + 1/6, whereas the chance of throwing a 1 
and a 2 is 1/6 X 1/6; that is, the or and and in probability the-
ory also behave like addition and multiplication, respectively. 
The third operation permitted in Boolean algebra is that of 
negation or complementation and is denoted by a bar over a 
constant or a variable. The complement of 0 (i.e. 0) is 1 and vice 
versa. The equation X + Y • Z = A is read as 'X or Y and not Z 
equals A'. The priority of an AND operator is higher than that 
of an OR operator so that the expression means A = X + 
(Y • Z) and not A = (X + Y)Z. Some texts use an asterisk to 
denote negation and some use a stroke. Thus, we can write 
NOT(X)asXorX*or/X. 
The arithmetic operations of subtraction and division do 
not exist in Boolean algebra. For example, the Boolean 
expression X + Y = X + Z, cannot be rearranged in the 
form (X + Y) - X = (X + Z) - X, which would lead to 
Y = Z. If you don't believe this, then consider the case X = 1, 
Y = 1, and Z = 0. The left-hand side of the equation yields 
X + Y = l + l = l, 
and 
the 
right-hand 
side 
yields 
X + Z = l + 0 = 1 . That is, the equation is valid even 
though Y is not equal to Z. 
2.5.1 Axioms and theorems of 
Boolean algebra 
An axiom or postulate is a fundamental rule that has to be 
taken for granted (i.e. the axioms of Boolean algebra define 
the framework of Boolean algebra from which everything 
else can be derived). The first axiom is called the closure 
property, which states that Boolean operations on Boolean 
variables or constants always yield Boolean results. If vari-
ables A and B belong to a set of Boolean elements, the opera-
tions A • B, A + B, and NOT A and NOT B also belong to the 
set of Boolean elements. 
Boolean variables obey the same commutative, distributive, 
and associative laws as the variables of conventional algebra. 
We take these laws for granted when we do everyday arith-
metic; for example, the commutative law states that 
6 X 3 = 3 X 6. Table 2.13describes the commutative, distribu-
tive, and associative laws of Boolean algebra. 
We approach Boolean algebra by first looking at the action 
of NOT, OR, and AND operations on constants. The effect of 
these three operations is best illustrated by means of the truth 
table given in Table 2.14. These rules may be extended to any 
number of variables. 
We can extend Table 2.14, which defines the relationship 
between the Boolean operators and the constants 0 and 1, to 
' There are, in fact, an infinite number of Boolean algebras. We are 
interested only in the Boolean algebra whose variables have binary two-
state values. 

2.5 An Introduction to Boolean algebra 
57 
A + B = B + A 
A B = B A 
A • (B • C) = (A • B) • C 
A + (B + C) = (A + B) + C 
A(B + C) = A B + A-C 
A + B • C = (A + B)(A + C) 
The AND and OR operators are commutative so that the order of the 
variables in a sum or product group does not matter. 
The AND and OR operators are associative so that the order in which 
sub-expressions are evaluated does not matter. 
The AND operator behaves like multiplication and the OR operator like 
addition. The first distributive property states that in an expression 
containing both AND and OR operators the AND operator takes precedence 
over the OR. The second distributive law, A + B • C = (A + B)(A + C), is not 
valid in conventional algebra. 
Table 2.13 Commutative, distributive, and associative laws of Boolean algebra. 
NOT 
AND 
OR 
0 = 1 
0 0 = 0 
0 + 0 = 0 
1 = 0 
0-1 = 0 
0 + 1 = 0 
1 0 = 0 
1 + 0 = 1 
1-1 = 1 
1 + 1 = 1 
AND 
OR 
NOT 
0 X = 0 
o + x = x 
X=X 
1-X = X 
1 + X = 1 
X X = X 
x + x = x 
x x = o 
X + X= 1 
Table 2.14 Basic axioms of Boolean algebra. 
Table 2.15 Boolean operations on a 
constant and a variable. 
the relationship between a Boolean operator, a variable, and a 
literal (see Table 2.15). 
We can prove the validity of the equations in Table 2.15 by 
substituting all the possible values for X (i.e. 0 or 1). For 
example, consider the axiom 0 • X = 0. If X = 1 we have 
0.1 = 0 , which is correct because by definition the output of 
an AND gate is true if and only if all its inputs are true. 
Similarly, if X = 0 we have 0 0 = 0, which is also correct. 
Therefore, the expression 0 • X = 0 is correct for all possible 
values of X. A proof in which we test a theorem by examining 
all possibilities is called proof by perfect induction. 
The axioms of Boolean algebra could be used to simplify 
equations, but it would be too tedious to keep going back 
to first principles. Instead, we can apply the axioms of 
Boolean algebra to derive some theorems to help in the sim-
plification of expressions. Once we have proved a theorem 
by using the basic axioms, we can apply the theorem to 
equations. 
Theorem 1 
Proof 
Theorem 2 
Proof 
Theorem 3 
Proof 
X + X-Y 
X + X-Y 
X I + X Y 
X(l + Y) 
X(l) 
X 
X + X Y = X + Y 
X + X Y = (X + X-Y) +JK-Y 
= X + X Y + X-Y 
= X + Y(X + X) 
= X +Y(l) 
= X + Y 
X Y + X Z + Y Z = X Y + X Z 
X Y + X-Z + Y Z = X Y + X Z + Y Z ( X + X) 
= X-Y + X Z + X-Y-Z + X Y -
= XY(1 + Z) + X Z ( 1 +Y) 
= XY(1)_+XZ(1) 
= X Y + X Z 
Using 1 • X = X and commutativity 
Using distributivity 
Because 1 + Y = 1 
ByTheoremlX = X + X Y 
Remember that X + X = 1 
Remember that (X + X) = 1 
Multiply bracketed terms 
Apply distributive rule 
Because (1 + Y) = 1 
= x 

58 
Chapter 2 Gates, circuits, and combinational logic 
Inputs 
0 
0 
0 
1 
0 
0 
0 
0 
0 
1 
1 
0 
1 
0 
0 
1 
0 
1 
0 
0 
0 
0 
1 
1 
1 
0 
1 
1 
1 
0 
0 
0 
0 
0 
0 
1 
0 
1 
0 
0 
0 
0 
1 
1 
0 
0 
1 
0 
0 
1 
1 
1 
0 
1 
0 
1 
Table 2.16 Proof of Theorem 3 by perfect induction. 
We can also prove Theorem 3 by the method of perfect 
induction. To do this, we set up a truth table and demonstrate 
that the theorem holds for all possible values of X, Y, and Z 
(Table 2.16). Because the columns labeled X • Y + X • Z and 
X Y + X Z + Y Z i n Table 2.16 are identical for all possible 
inputs, these two expressions must be equivalent. 
Theorem 4 
X(X + Y) = X 
Proof 
X(X + Y) = X • X + X • Y 
= X + X Y 
= X 
Theorem 5 
X(X + Y) = X • Y 
Proof 
X(X + Y) = X X + X Y 
= 0 + X Y 
= X Y 
Theorem6 
(X + Y)(X + Y) = X 
Proof 
(X + Y)(X + Y) = X-X + X-Y + X-Y + Y-Y 
= X + X-_Y + X-Y 
= X(1 + Y +Y) 
= X 
Theorem7 
(X + Y)(X + Z) = X Z + X Y 
Proof 
(X + Y)(X + Z) = X'X + X-Z +X-Y + Y-Z 
= X Z + X Y + Y Z 
= X Z + X Y 
Theorem 8 
(X + Y)(X + Z)(Y + Z) = (X + Y)(X + Z) 
Proof 
(X + Y)(X + Z)(Y + Z) = (X • Z + X • Y)(Y + Z)_ 
= X Y Z + X Z Z + X Y : Y + X Y ' Z 
= X Y Z + X Z + X Y + X Y Z 
= X Z ( Y + 1) + X-Y(1 + Z ) 
= X • Z + X Y 
= (X + Y)(X + Z) 
Multiply by X 
Because X X = X 
By Theorem 1 
Because X • X = 0 
BecauseXX = X,YY = 0 
Multiply brackets 
BecauseXX = 0 
By Theorem 3 
By Theorem 7 
Because X • X = 1 
By Theorem 7 
We provide an alternative proof for Theorem 8 when we look at de Morgan's theorem later in this chapter. 
X 
Y 
Z 
X 
X Y 
X Z 
Y Z 
X Y + X Z 
X Y + X Z + Y Z 
0 
0 
1 
1 
0 
0 
0 
0 
J 
L 
-* 
same 
^ -

2.5 An Introduction to Boolean algebra 
59 
Theorem 9 
X-Y-Z = X + Y + Z 
Proof 
To prove that X-Y-Z = X + Y + Z,we assume that the 
expression is true and test its consequences. 
If X + Y + Z is the complement of X • Y • Z, then from the 
basic axioms of Boolean algebra, we have 
(X + Y + Z)- (X-Y-Z) = 0and (X + Y + Z) + (X-Y-Z) = 1 
Subproofl 
(XJ-Y + Z) X Y-Z = X X Y Z + Y X Y Z + Z-X-Y Z 
= X-X-(Y-Z) + Y-Y-(Y-Z) + Z-Z(X-Y) 
= 0 
Subproof2 
(X + Y + Z) + X - Y-Z = Y-Z-(X) +X + Y + Z 
= Y_-Z + X + Y + Z _ 
= (Y + Y-Z)+ X + Z 
= Y + Z + Z + X 
= Y + 1 + X = 1 
As we have demonstrated that 
(X + Y + Z)-X-Y-Z = Oandthat 
(X + Y + Z) + X-Y-Z = 1, it follows that X + Y + Zisthe 
complement of X • Y • Z. 
Re-arrange equation 
UseA B + B = A + B 
Re-arrange equation 
Theorem 10 
X-Y-Z = X + Y + Z 
Proof 
One possible way of proving Theorem 10 is to use the method 
we used to prove Theorem 9. For the sake of variety, we will 
prove Theorem 10 by perfect induction (see Table 2.17). 
Inputs 
X 
Y 
z 
X + Y + Z 
X -+ Y + Z 
X 
Y 
z 
X - Y - Z 
0 
0 
0 
0 
M 
1 
1 
1 
A 
0 
0 
0 
1 
1 
0 
l°\ 
I ° 1 
1 
1 
1 
0 
0 
1 
l°\ 
0 
0 
1 
1 
0 
1 
0 
0 
0 
1 
0 
0 
0 
0 
1 
1 
0 
1 
0 
1 
0 
0 
1 
0 
0 
1 
1 
0 
o / 
0 
0 
1 
\ 0 / 
1 
1 
1 
V 
0 
0 
0 
w 
fc 
Table 2.17 Proof of Theorem 10 by perfect induction. 
Theorems 9 and 10 are collectively called de Morgan's 
theorem. This theorem can be stated as an entire function is 
complemented by replacing AND operators by OR operators, 
replacing OR operators by AND operators, and complement-
ing variables and literals. We make extensive use of de 
Morgan's theorem later. 
An important rule in Boolean algebra is called the principle 
of duality. Any expression that is true is also true if AND is 
replaced by OR (and vice versa) and 1 replaced by 0 (and vice 
versa). Consider the following examples of duals. 
Expression 
Dual 
x = x + x 
x = x x 
(replace + by-) 
1 = X + 1 
o = xo 
(replace + by and 
replace 1 by 0) 
X = X(X + Y) 
X = X + X Y 
(replace • by + and 
replace + by) 
As you can see, the dual of each expression is also true. 
Use Z + Z = 1 

60 
Chapter 2 Gates, circuits, and combinational logic 
OBSERVATIONS 
When novices first encounter Boolean algebra, it is not 
uncommon for them to invent new theorems that are incorrect 
(because they superficially look like existing theorems). We 
include the following observations because they represent the 
most frequently encountered misconceptions. 
Observation 1 
X • Y + X • Y is not equal to 1 
X • Y + X • Y cannot be simplified 
Observation 2 
Observation 3 
Observation 4 
X Y + X Y i s not equal to 1 
X • Y + X • Y cannot be simplified 
X^YisnotequaltoX-Y 
OBSERVATIONS 
When novices first encounter Boolean algebra, it is not 
uncommon for them to invent new theorems that are incorrect 
(because they superficially look like existing theorems). We 
include the following observations because they represent the 
most frequently encountered misconceptions. 
Observation 1 
X • Y + X • Y is not equal to 1 
X • Y + X • Y cannot be simplified 
Observation 2 
Observation 3 
Observation 4 
X + Y is not equal to X + Y 
ALL FUNCTIONS OF TWO VARIABLES — A L L POSSIBLE GATES 
This table provides all possible functions of two variables A and 
of two variables; that is, there are only 16 possible types of 
B. These two variables have 22 = 4 possible different 
two-input gate Some of the functions corresponc to 
combinations. We can associate a different function with 
functions we've already met. Some functions are 
each of these 4Z = 16 values to create all possible functions 
meaningless. 
Inputs 
Functions 
A 
B 
F0 
F, 
F2 
F3 
F4 
F5 
F6 
F7 
F8 
F9 
Fio 
Fn 
Fiz 
F13 
Fi4 
F15 
0 
0 
0 
1
0 
1 
0 
1
0 
1
0 
1 
0 
1 
0 
1 
0 
1 
0 
1
0 
0 
1 
1 
0 
0 
1
1
0 
0 
1 
1 
0 
0 
1 
1 
1
0 
0 
0 
0 
0 
1 
1
1
1
0 
0 
0 
0 
1 
1 
1 
1 
1
1
0 
0 
0 
0 
0 
0 
0 
0 
1
1 
1 
1 
1 
1 
1 
1 
Function 
Expression 
Name 
Fo 
Fi 
0 
NOR 
Fo 
Fi 
A + B 
NOR 
F2 
h 
F4 
Fs 
F6 
F7 
F8 
F9 
Fio 
Fi, 
A B 
A 
A B 
B 
A©B 
A~B 
A B 
AS)! 
B 
A B + A B + A B = A-B = A + B 
NOT 
NOT 
EOR 
NAND 
AND 
ENOR 
Fi2 
F13 
F,4 
A 
A - B + A - B + A-B = A-B = A + B 
A + B 
OR 
F1S 
1 
Examples of the use of Boolean algebra in 
simplifying equations 
Having presented the basic rules of Boolean algebra, the next 
step is to show how it's used to simplify Boolean expressions. 
By simplifying these equations you can sometimes produce 
a cheaper version of the logic circuit. The following equations 
are generally random functions chosen to demonstrate the 
rules of Boolean algebra. 
(a) X + Y + X-Y + (X + Y) XY 
(b) X Y Z + X Y Z + X Y Z + X Y Z 

2.5 An Introduction to Boolean algebra 
61 
(f)W-X-Z + X-Y-Z + W-X-Y + X-Y-Z + W-Y-Z 
(g) W-X-Z + W Z + X Y Z + 
W X Y 
(h) (X + Y + Z)(X + Y + Z)(X + Y + Z) 
Solutions 
When I simplify Boolean expressions, I try to keep the order of the variables alphabetical, making it easier to pick out logical 
groupings. 
(a) X+Y + X-Y + (X + Y)-X-Y = X + Y + X-Y + X X-Y + X-Y-Y 
= X + Y + X Y 
AsA-A = 0 
= X + Y + Y 
a s A + A B = A + B 
= 1 
asA+ A = 1 
Note: When a Boolean expression can be reduced to the constant 1, the expression is always true and is independent of the 
variables. 
( b ) X Y Z + X Y Z + X Y Z + X Y Z = X Y ( Z + Z) + X Z ( Y + Y) 
= X-Y-(1) + X-Z-(1) 
= X Y + X Z 
By Theorem 9 
AsF = F 
Note: Both expressions in examples (b) and (c) simplify to X • Y + X • Z, demonstrating that these two expressions are equiv-
alent. These equations are those of the multiplexer with (b) derived from the truth table (Table 2.9) and (c) from the circuit 
diagram of Fig. 2.14. 
(d) (X +Y)(X + Z)(Y + Z) = (X-X + X-Z + X-Y+Y-Z)-(Y + Z) 
= (X-Z + X-Y + Y-Z)-(Y + Z) 
= (X-Z + X-Y)-(Y + Z) 
= X Y Z + X Z Z + X Y Y + X Y Z 
= X Y Z + X Y Z 
(e) (W + X +Y-Z)(W + X)(X +Y) = (W-W + W-X + W-Y-Z + W-X + X-X + X • Y-Z)(X + Y) 
= (W-X + W-Y-Z + W-X + X + XY-Z)(X +Y) 
= (X +WYZ)(X +Y) 
= X X + X Y + W X Y Z + W Y Y Z 
= X-Y +W-X-Y-Z +W-Y-Z 
= X Y + W Y Z ( X + 1) 
= X-Y + W-Y-Z 
(f) WXZ + XYZ + WXY + XYZ + WYZ = WXZ + YZ(X + X + W) + WXY 
= WXZ + YZ + WXY 
= WX(Y + Z) + YZ 
Note that YZ = Y + Z so we can write 
= W-X(Y + Z) + Y + Z 
= W-X + Y-Z 
BecauseA + A-B = A + B 
(c) X-Y-X-Z 
(d) (X + Y)(X + Z)(Y + Z) 
(e) (W + X + Y-Z)(W + X)(X + Y) 
(c) X-Y-X-Z = X-Y+ X-Z 
= X Y + X Z 
AsX X = 0 
By Theorem 3 

62 
Chapter 2 Gates, circuits, and combinational logic 
(g) WXZ + WZ + XYZ + WXY = Z(WX + W) + XYZ + WXY 
= Z(X + W) + XYZ + WXY 
= XZ + WZ + XYZ + WXY 
= X(Z + YZ) + WZ + WXY 
= X(Z + Y) + WZ + WXY 
= XZ + XY + WZ + WXY 
= XZ + XY(1 + W) + WZ 
= XZ + XY + WZ 
(h) (X + Y + Z)(X + Y + Z)(X + Y + Z) = (Y + Z)(X + Y + Z) 
= Z(X +Y) + Y Z 
= X Z + Y Z + Y Z 
= X-Z + Y(Z + Z) 
= X Z +Y 
as (A + B)(A + B) = A 
as(A + B)(A + C) = A C + A B 
Input *o-
X 
XT-
Input Yo. 
For example, 1.0 = 2 
2-bit by 2-bit 
multiplier 
For example, 1,1=3 
In this example, 2x3 = 6 = 0110 
/ 
— f t 
* z ° 
* 
1 4-bit product 
J »i 
Figure 2.52 A 2-bit multiplier 
Inputs 
Output 
X 
Y 
z 
XxY = Z 
X, 
X0 
V, 
Yo 
z3 
z2 
z, 
Zo 
0 X 0 = 0 
0 
0 
0 
0 
0 
0 
0 
0 
0 X 1 = 0 
0 
0 
0 
1 
0 
0 
0 
0 
0 X 2 = 0 
0 
0 
1 
0 
0 
0 
0 
0 
0 X 3 = 0 
0 
0 
1 
1 
0 
0 
0 
0 
1 X 0 = 0 
0 
1 
0 
0 
0 
0 
0 
0 
1 X 1 = 1 
0 
1 
0 
1 
0 
0 
0 
1 
1 X 2 = 2 
0 
1 
1 
0 
0 
0 
1 
0 
1 X 3 = 3 
0 
1 
1 
1 
0 
0 
1 
1 
2 X 0 = 0 
0 
0 
0 
0 
0 
0 
0 
2 X 1 = 2 
0 
0 
1 
0 
0 
1 
0 
2 X 2 = 4 
0 
1 
0 
0 
1 
0 
0 
2 X 3 = 6 
0 
1 
1 
0 
1 
1 
0 
3 X 0 = 0 
1 
0 
0 
0 
0 
0 
0 
3 X 1 = 3 
1 
0 
1 
0 
0 
1 
1 
3 X 2 = 6 
1 
1 
0 
0 
1 
1 
0 
3 X 3 = 9 
1 
1 
1 
1 
0 
0 
1 
Table 2.18 Truth table for a 2-bit by 2-bit multiplier. 

2.5 An Introduction to Boolean algebra 63 
These examples illustrate the art of manipulating Boolean 
expressions. It's difficult to be sure we have reached an optimal 
solution. Later we study Karnaugh maps, which provide an 
approach that gives us confidence that we've reached an opti-
mal solution. 
The Design of a 2-bit Multiplier 
The following example illustrates how Boolean algebra is 
applied to a practical problem. A designer wishes to produce 
a 2-bit by 2-bit binary multiplier. The two 2-bit inputs are X„ 
X„ and Y,, Y0 and the four-bit product at the output terminals 
is Z3, Z2, Z1? Z0. We have not yet introduced binary arithmetic 
(see Chapter 4), but nothing difficult is involved here. We 
begin by considering the block diagram of the system 
(Fig. 2.52) and constructing its truth table. 
Z2 — X^Xg-Yi-Y,) + X[-XQ'Y,-Y0 + X|-Xo-Y,-Y0 
= X1-X0-Y1(Y, +Y 0) + X,-VY.-Yo 
= X,-X,)-Yi + X^XO'Y^YQ 
= X,-Y1(Xo + X0Y0) 
= X.-Y.tXo + Y0) 
= XJ-XQ-Y, + X,-Y,Y0 
Z3 = X! •X0
-Y[ 'Yd 
We have now obtained four simplified sum of products 
expressions for Z0 to Z3; that is, 
Zo = Xfl-Yo 
Zj = ^,-Xo-Y! + XQ-Y^YQ + XI-XQ-YQ + X^Y^YQ 
Z2 = X,-Xo-Yi + XI-YJ-YQ 
Z3 = X^XO-YJ-YQ 
It's interesting to note that each of the above expressions is 
symmetric in X and Y. This is to be expected—if the problem 
The multiplier has four inputs, X,, X,,, Y,, Y0, (indicat-
ing a 16-line truth table) and four outputs. Table 2.18 pro-
vides a truth table for the binary multiplier. Each 4-bit 
input represents the product of two 2-bit numbers so that, 
for example, an input of X,, X^, Yj, Y0 = 1011 represents 
the product 102 X ll 2 or 2 X 3. The corresponding out-
put is a 4-bit product, which, in this case, is 6 or 0110 in 
binary form. 
From Table 2.18, we can derive expressions for the four 
outputs, Z0 to Z3. Whenever a truth table has m output 
columns, a set of m Boolean equations must be derived. One 
equation is associated with each of the m columns. To derive 
an expression for Z0, the four minterms in the Z0 column are 
ORed logically. 
itself is symmetric in X and Y (i.e. 3 X 1 = 1X3), then the 
result should also demonstrate this symmetry. There are 
many ways of realizing the expressions for Z0 to Z3. The 
circuit of Fig. 2.53 illustrates one possible way. 
2.5.2 De Morgan's theorem 
Theorems 9 and 10 provide the designer with a powerful tool 
because they enable an AND function to be implemented by 
Z0 ~ X^XO-Y^YQ + Xi-Xfl-Y^Yo + X|-X„-Y,-Y0 + XJ-XQ-Y^Y,, 
= X,-X0-Y0(Y1 + YJ) + X.-Xo-YotY, + Y,) 
= XJ-XQ-YQ + XJ-XO-YQ 
= Xo-YofX, + X,) 
= Xo-Y0 
Zi = X^XQ-YI-YQ + XJ-XO-YJ-YD + X^XO-YJ-YQ-I- XJ-XQ-YJ-Y,, -I- XJ-X^-Y^YQ + X^XQ-YJ-YQ 
= X.-Xj-Y.CYo + Y„) + X,-Xo-Y0(Y, + Y.) + W V Y o + X rVY,-Y 0 
= X^Xo-Y, + X|-Xo-Y0 + XJ-XQ-Y,-Y0 + XI-X0
-Y1-Y0 
= VY,(X, + X,-Y0) + X,-Y0(Xo + Xo-Y,) 
= VY,(X, + Y0) + X1-Y0(X0 + Y,) 
= XrXn-Y, + Xo-Y.-Yo + X,-X,,-Y0 + X,-Y.-Y0 

6 4 
Chapter 2 Gates, circuits, and combinational logic 
X-j 
XQ 
Y-| 
Y0 
Figure 2.53 Circuit for the two-bit multiplier. 
an OR gate and inverter. Similarly, these theorems enable an 
OR gate to be implemented by an AND gate and inverter. 
We first demonstrate how de Morgan's theorem is applied 
to Boolean expressions and then show how circuits can be 
converted to NAND-only or NOR-only forms. You may 
wonder why anyone should wish to implement circuits in 
NAND (or NOR) logic only. There are several reasons for 
this, but, in general, NAND gates operate at a higher speed 
than AND gates and NAND gates can be built with fewer 
components (at the chip level). Later we shall examine in 
more detail how a circuit can be designed entirely with 
NAND gates only. 
To apply de Morgan's theorem to a function the ANDs are 
changed into ORs, ORs and the into ANDs and variables (and 
literals) are complemented. The following examples illustrate 
the application of de Morgan's theorem. 
1. F = X-Y + X-Z 
= X-Y-X-Z 
= (X + Y)(X + Z) 
We wish to apply de Morgan's 
theorem to the right-hand side 
The + becomes • and variables 
'X • Y' and 'X • Z' complemented 
Variables M 
and X^Y are 
themselves complemented 
As you can see, the first step is to replace the OR by an AND 
operator. The compound variables X • Y and X • Z are comple-
mented to get X-Y and X-Z. The process is continued by 
applying de Morgan to the two complemented groups (i.e. 
X T becomes X + Y and 3oZ becomes X + Z). 
fc 
Yi 
X 0 
V 
\ 
xoYo 
— • 
Zo 
i XIXQY-J 
X O Y I Y O 
1 
\ 
X-JXQYQ 
• Z l 
V X I Y I Y Q 
, XTXQY-) 
*z 2 
V ] ¥ o f 
| XiX0Y-[Y0 
-*• z3 

2.5 An Introduction to Boolean algebra 
65 
2. F = A-B + C D + A D 
= A ^ B O D - A I ) 
= (A + B)(C + D)(A + D) 
3. F = A-B-(C + E-D) 
A + B + C + E-D 
A + B + C-ifD 
A + B + C- ( 1 + D ) 
Replace + by • and complement the product terms 
Expand the complemented product terms 
This is a product term with three elements. 
Replace • by + and complement variables 
Evaluate the complemented expression (change + to •) 
Final step, evaluate E-D 
This example demonstrates how you have to keep applying de Morgan's theorem until there are no complemented terms left 
to evaluate. 
4. A proof of Theorem 8 by de Morgan's theorem 
(X + Y)-(X + Y)-(Y + Z) = (X + Y)-(X + Z)-(Y + Z) 
X + Y + X + Z + Y + Z 
X-Y + X-Z + Y Z 
= X-Y + X-Z 
= X-Y-X-Z 
= (X + Y)(X + Z) 
Complement twice because X = X. 
Remove inner bar by applying de Morgan 
Complement the three two-variable groups 
Use Theorem 3 to simplify 
Remove outer bar, change + to • 
Remove bars over two-variable groups 
2.5.3 Implementing logic functions in 
NAND or NOR two logic only 
Some gates are better than others; for example, the NAND 
gate is both faster and cheaper than the corresponding AND 
gate. Consequently, it's often necessary to realize a circuit 
using one type of gate only. Engineers sometimes implement 
a digital circuit with one particular type of gate because there 
is not a uniform range of gates available. For obvious eco-
nomic reasons manufacturers don't sell a comprehensive 
range of gates (e.g. two-input AND, three-input AND,..., 
10-input AND, two-input OR,...). For example, there are 
many types of NAND gate, from the quad two-input NAND 
to the 13-input NAND, but there are few types of AND gates. 
NAND logic We first look at the way in which circuits can 
be constructed from nothing but NAND gates and then 
demonstrate that we can also fabricate circuits with NOR 
gates only. To construct a circuit solely in terms of NAND 
gates, de Morgan's theorem must be invoked to get rid of all 
OR operators in the expression. For example, suppose we 
wish to generate the expression F = A + B + C using NAND 
gates only. We begin by applying a double negation to the 
expression, as this does not alter the expression's value but it 
does give us the opportunity to apply de Morgan's theorem. 
F = A + B + C 
F = F = A + B + C 
F = A B C 
The original expression using 
OR logic 
Double negation has no effect on 
the value of a function 
Apply de Morgan's theorem 
We've now converted the OR function into a NAND func-
tion. The three NOT functions that generate A, B, and C can 
be implemented in terms of NOT gates, or by means of two-
input NAND gates with their inputs connected together. 
Figure 2.54 shows how the function F = A + B + C can 
be implemented in NAND logic only. If the inputs of a 
NAND gate are A and B, and the output is C, then C = A-B. 
But if A = B, then C = A • A or C = A. You can better under-
stand this by looking at the truth table for the NAND gate, 
and imagining the effect of removing the lines A, B = 0,1 and 
A,B= 1,0. 
It's important to note that we are not using de Morgan's 
theorem here to simplify Boolean expressions. We are using 
de Morgan's theorem to convert an expression into a form 
suitable for realization in terms of NAND (or NOR) gates. 
ABC=A+B+C 
Figure 2.54 Implementing F = A + B + C with NAND logic only. 
A 
B 
-n 
A -
B-
C -

66 
Chapter 2 Gates, circuits, and combinational logic 
By applying the same techniques to the 2-bit by 2-bit 
multiplier we designed earlier we can convert the expressions 
for the four outputs into NAND-only logic. 
Z0 = X„-Y0 = XoY0 
(i.e. NAND gate followed by NOT 
gate = AND gate) 
Zi = XiXflY, + XoY,Y0 + X ^ Y Q + X,Y|Y0 
— X ^ Y Q + XoYjYo + X[XoY0 + X,Y[Y0 
— X^Y^XoYiYo-X^Yo-X^Y,, 
Z2 - XiXflY, + X|Y|Y0 
— X ^ Y ! 
+ XJY^Q 
= X ^ Y j • XiY^o 
Z3 = X1X0Y1Y0 
— X]XflYiY0 
Figure 2.55 shows the implementation of the multiplier in 
terms of NAND logic only. Note that this circuit performs 
exactly the same function as the circuit of Fig. 2.53. 
NOR logic The procedures we've just used may equally be 
applied to the implementation of circuits using NOR gates 
Xo 
7T 
Yi 
o 
J© 
• Z3 
Figure 2.55 Implementing the multiplier circuit in NAND logic only. 
I 
I 
Yo 
I 
F 
|x0 
?• 
-#• 
z o 
- * Z i 
— • Z z 

2.5 An Introduction to Boolean Algebra 
67 
only. By way of illustration, the value of Z3 in the 2-bit multi-
plier can be converted to NOR logic form in the following way 
Z3 — X^Xfl-Y, -Y0 
Note that negation may be implemented by an inverter or by 
a NOR gate with its inputs connected together. 
As a final example of NAND logic consider Fig. 2.56. A 
Boolean expression can be expressed in sum-of-products 
form as A • B + C • D. This expression can be converted to 
NAND logic as 
A-B-C-D 
Note how the three-gate circuit in Fig. 2.56(a) can be 
converted into the three-gate NAND circuit of Fig. 2.56(b). 
Fig. 2.57 shows the construction of the two versions of 
AB + CD in Digital Works. We have provided an LED at each 
output and manually selectable inputs to enable you to inves-
tigate the circuits. 
2.5.4 Karnaugh maps 
When you use algebraic techniques to simplify a Boolean 
expression you sometimes reach a point at which you 
can't proceed, because you're unable to find 
further 
simplifications. The Karnaugh map, or more simply the 
K-map, is a graphical technique for the representation and 
simplification of a Boolean expression that shows unambigu-
ously when a Boolean expression has been reduced to its 
most simple form. 
Although the Karnaugh map can simplify Boolean equa-
tions with five or six variables, we will use it to solve problems 
(A) Realization of AB + CD 
(AND/OR logic). 
AB.CD = AB + CD = AB + CD 
(b) Realization of AB + CD 
(NAND logic). 
Figure 2.56 Implementing 
A B + C D i n AND/OR and 
NAND logic. 
W8^§^^MSm:^^B^t0^MMS€% 
Z^v;:/^:^^H'^^v^^ siill 
• -In|x| 
File 
Edit Circuit 
View Jools 
Help 
0 & 0 
.'..••- , H 
D l > D I > » « D l > f r s a t s 
m 1 o a -S 
H s * 
+ 
o 
A 
9 
E> 
O 
U 
HO 
f ^ 
^ 5 
HO 
l_j 
x 
HO 
l_j 
x 
^ ^ ~ T _ ^ 
[5] 
*~1 
-^ 
— 
S—l__l 
-v 
P ^ " ^ 
S—l__l 
-v 
P ^ " ^ 
s ^ - ^ v 
|3| 
r-l 
/ 
a—r-l—/ 
AND-OR circuit 
NAND circuit 
« ( 
j-j-
Figure 2.57 Using Digital Works to investigate two circuits. 
— Xj-Xo-Y^Yo 
= X, + X„ + Y, + Y0 
A—I—\3 
C 
1 N | L_y=r^ 
VJ_ 
AB.CD 
D 
1 /CD 
X 
A 
1 "NAB 
D — L A D 
r 
AB.CD 
)
—
• 
/AB+CD 
A -
B-
C -
D-
A-
B-
C 
D 
NAB 
A — 
B — 

68 
Chapter 2 Gates, circuits, and combinational logic 
EXAMPLE 
Show that the exclusive or, EOR, operator is associative, so 
thatA©(B©C) = (A©B) © C . 
A © ( B © C ) = A©(B-C + BC) 
(A©B)©C = (A-B + AB)©C 
= (A-B + AB)C + (A-B + A-B)C 
= A-B-C + A-B-C + (A-BA-B)C 
= A(B-C + B-C) + A(B-C + B-C) 
= A-B-C + A-B-C + (A + B)-(A + B)C 
= A(B + C)(B + C) + A-B-C + A-B-C 
= A-B-C + A-B-C + (A-B + AB)C 
= A(BC + B-C) + A-B-C + A-B-C 
= A-B-C + A-B-C + A-B-C + A-B-C 
Both these expressions are equal and therefore the © 
operator is associative. 
EFFECT OF FINITE PROPAGATION DELAYS ON LOGIC ELEMENTS 
We have assumed that if signals are applied to the input 
terminals of a circuit, the correct output will appear 
instantaneously at the output of the circuit. In practice, this is 
not so. Real gates suffer from an effect called propagation 
delay and it takes about 1 ns for a change in an input signal to 
affect the output. One nanosecond is an unbelievably short 
period of time in human terms—but not in electronic terms. 
The speed of light is 300 X 108 cm/s and electrical signals in 
computers travel at about 70% of the speed of light. In 1 ns 
a signal travels about 20 cm. 
The propagation delay introduced by logic elements is one 
of the greatest problems designers have to contend with. The 
diagram illustrates the effect of propagation delay on a single 
inverter where a pulse with sharp (i.e. vertical) rising and 
falling edges is applied to the input of an inverter. An inverted 
pulse is produced at its output and is delayed with respect to 
the input pulse. Moreover, the edges of the output pulse are no 
longer vertical. The time tHL represents the time delay 
between the rising edge of the input pulse and the point at 
which the output of the gate has reached Vol. Similarly, tw 
represents the time between the falling edge of the input and 
the time at which the output reaches VOH. 
You might think that the effect of time delays on the passage 
of signals through gates simply reduces the speed at which a 
digital system may operate. Unfortunately, propagation delays 
have more sinister effects as demonstrated by the diagram. By 
the rules of Boolean algebra the output of the AND gate is X • X 
and should be permanently 0. Now examine its timing diagram. 
At point A the input, X, rises from 0 to 1. However, the X 
input to the AND gate does not fall to 0 for a time equal to 
the propagation delay of the inverter. Consequently, for a short 
time the inputs of the AND gate are both true, and its output 
rises to a logical 1 from points B to C (after its own internal 
delay).The short pulse at the output of the AND gate is called 
a glitch, and can be very troublesome in digital systems. There 
are two solutions to this problem. One is to apply special 
design techniques to the Boolean logic to remove the glitch. 
The other is to connect the output to a flip-flop, and to clock 
the flip-flop after any glitches have died away. 
l—T^J-^r> 
-*. F = X-X=0 
Input waveform 
Negated and delayed 
output from invertor 
IglitcH 
Output from AND gate 
is a glitch 
A
B
C 
with only three or four variables. Other techniques such as 
the Quine-McCluskey method can be applied to the simplifi-
cation of Boolean expressions in more than six variables. 
However, these techniques are beyond the scope of this book. 
The Karnaugh map is just a two-dimensional form of the 
truth table, drawn in such a way that the simplification of 
a Boolean expression can immediately be seen from the loca-
tion of 1 s on the map. A system with n variables has 2" lines in 
its truth table and 2n squares on its Karnaugh map. Each 
square on the Karnaugh map is associated with a line (i.e. 
minterm) in the truth table. Figure 2.58 shows Karnaugh 
maps for one to four variables. 
X 
:L 
X 
Output F 
:L 
X 
[>o 
*-X 
Input 
Output 
Input i t 
j 
| 
I 
x oi__n 
! i 
nt-HJ-h 
X -
Output F 
1 A 
fHL 
(LH 

2.5 An Introduction to Boolean algebra 
69 
A 
A 
(a) One-variable Karnaugh map. 
VAB 
C \ 
00 
01 
11 
10 
ABC 
ABC 
ABC 
ABC 
ABC 
ABC 
ABC 
ABC 
c) Three-variable K arnaugh n nap. 
AB 
AB 
AB 
AB 
(b) Two-variable Karnaugh map. 
v AB 
C D \ 
00 
01 
11 
00 
01 
11 
10 
10 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
(b) Four-variable Karnaugh map. 
Figure 2.58 The Karnaugh map. 
CD1 
00 
01 
11 
10 
.AB 00 
01 
11 
10 
sAB 00 
01 
11 
10 
ABCD 
ABCD 
ABCD 
ABCD 
CD\ 
00 
01 
11 
10 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
CD\ 
00 
01 
11 
10 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
CD\ 
00 
01 
11 
10 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
ABCD 
CD\ 
00 
01 
11 
10 
ABCD 
ABCD 
ABCD 
ABCD 
(a) The region for which A is true. 
s.AB 00 
01 
11 
10 
(b) The region for which B is true. 
vAB 00 
01 
11 
10 
CDS 
ABCD 
ABCD 
ABCD 
CD> 
00 
01 
11 
10 
ABCD 
ABCD 
ABCD 
ABCD 
00 
ABCD 
ABCD 
ABCD 
ABCD 
CD> 
00 
01 
11 
10 
ABCD 
ABCD 
ABCD 
ABCD 
01 
ABCD 
ABCD 
ABCD 
ABCD 
CD> 
00 
01 
11 
10 
ABCD 
ABCD 
ABCD 
ABCD 
11 
ABCD 
SBCD 
ABCD 
ABCD 
CD> 
00 
01 
11 
10 
ABCD 
ABCD 
ABCD 
ABCD 
10 
ABCD 
ABCD 
ABCD 
ABCD 
CD> 
00 
01 
11 
10 
ABCD 
ABCD 
ABCD 
ABCD 
(c) The region for which C is true. 
Figure 2.59 Regions of a Karnaugh map. 
(d) The region for which D is true. 
As you can see from Fig. 2.58, each 
line in a truth table is mapped onto a 
Karnaugh map; for example, in four 
variables each logical combination 
from A B C D to A B C D has a 
unique location. However, the key to 
the Karnaugh map is the layout of the 
squares. Adjacent squares differ by only 
one variable. By adjacent we mean 
horizontally and vertically adjacent, 
but not diagonally adjacent. For exam-
ple, if you look the three-variable map 
of Fig. 2.58(c) you will see that the left-
most two terms on the top line are A • B 
•C and A B C . The only difference 
between these terms is B and B. 
Figure 2.59 demonstrates the struc-
ture of a four-variable Karnaugh map 
with variables A, B, C, and D. This map 
has been repeated four times and, in each 
case, the region in which the selected 
variable is true has been shaded. The 
unshaded portion of each map repre-
sents the region in which the chosen 
variable is false. 
We will soon see that you need to 
develop three skills to use a Karnaugh 
map. The first is to plot terms on die map 
(i.e. transfer a truth table or a Boolean 
0 
_J 
A 
-> 
o 
0 
1 
0 
1 
1 
A 
B 

70 
Chapter 2 Gates, circuits, and combinational logic 
expression onto the map). The second skill is the ability to 
group the Is you've plotted on the map. The third skill is to 
read the groups of Is on the map and express each group as a 
product term. 
We now use a simple three-variable map to demonstrate 
how a truth table is mapped onto a Karnaugh map. One-and 
two-variable maps represent trivial cases and aren't consid-
ered further. Figure 2.60 shows the truth table for a three-
variable function and the corresponding Karnaugh map. 
Each of the three Is in the truth table is mapped onto its 
appropriate square on the Karnaugh map. 
A three-variable Karnaugh map has four vertical columns, 
one for each of the four possible values of two out of the three 
variables. For example, if the three variables are A, B, and C, 
the four columns represent all the combinations of A and B. 
The leftmost column is labeled 00 and represents the region for 
which A = 0, B = 0. The next column is labeled 01, and repre-
sents the region for which A = 0, B = 1. The next column is 
labeled 11 (not 10), and represents the region for which A = 1, 
B = 1. Remember that adjacent columns differ by only one 
variable at a time. The fourth column, 10, represents the region 
for which A = 1, B = 0. In fact, a Karnaugh map is made up of 
all possible 2n minterms for a system with n variables. 
The three-variable Karnaugh map in Fig. 2.60 has two 
horizontal rows, the upper row corresponding to C = 0 and 
the lower to C = 1. Any square on this Karnaugh map repre-
sents a unique combination of the three variables, from A • B 
• C t o A B C. 
Figure 2.60 demonstrates how a function of three variables, 
F = A B C + A B C + A B C is plotted on a Karnaugh 
Figure 2.60 Relationship between a Karnaugh map and 
truth table. 
map. If it isn't clear how the entries in the table are plotted on 
the Karnaugh map, examine Fig. 2.60 and work out which cell 
on the map is associated with each line in the table. A square 
containing a logical 1 is said to be covered by a 1. 
At this point it's worth noting that no two Is plotted on the 
Karnaugh map of Fig. 2.60 are adjacent to each other, and 
that the function F = A B C + A B C + A B C cannot 
be simplified. To keep the Karnaugh maps as clear and 
uncluttered as possible, squares that do not contain a 1 are left 
unmarked even though they must, of course, contain a 0. 
Consider Fig. 2.61 in which the function F, = A • B • C + 
A • B • C is plotted on the left-hand map. The two minterms in 
this function are A B C and A B C and occupy the cells for 
which A = 1, B = 1, C = 0, and A = 1, B = 1, C = 1, 
respectively. If you still have difficulty plotting minterms, just 
think of them as coordinates of squares; for example, A B C 
has the coordinates 1,1,0 and corresponds to the square 
ABC= 110. 
In the Karnaugh map for Fj two separate adjacent squares 
are covered. Now look at the Karnaugh map for F2 = A • B at 
the right-hand side of Fig. 2.61. In this case a group of two 
squares is covered, corresponding to the column A = 1, 
B = 1. As the function for F2 does not involve the variable C, 
a 1 is entered in the squares for which A = B = 1 and C = 0, 
and A = B = 1 and C = 1; that is, a 1 is entered for all values 
of C for which AB = 1 1 . When plotting a product term like 
A • B on the Karnaugh map, all you have to do is to locate the 
region for which AB = 11. 
It is immediately obvious that both Karnaugh maps in 
Fig. 2.61 are identical, so that Fj = F2 and A B C + A B C 
= A • B. From the rules of Boolean algebra A B C + A B C 
= A • B (C + C) = A • B( 1) = A • B. It should be apparent that 
two adjacent squares in a Karnaugh map can be grouped 
together to form a single simpler term. It is this property that 
the Karnaugh map exploits to simplify expressions. 
Simplifying Sum-of-Product expressions with a 
Karnaugh map 
The first step in simplifying a Boolean expression by means of 
a Karnaugh map is to plot all the Is (i.e. minterms) in the 
function's truth table on the Karnaugh map. The next step is 
to combine adjacent Is into groups of one, two, four, eight, or 
00 
01 
11 
10 
FT=ABC + ABC 
< ~ \ 
00 
01 
11 
ABC 
o 
ABC 
1 
F2 = AB 
10 
I®) 
V 
AB 
Figure 2.61 Plotting two functions 
on Karnaugh maps. 
00 
01 
11 
10 
0 
A B C| F 
o o o <3>-
0 0 1 0 
0 1 0 0 
0 1 1 ® 
1 0 0 0 s 
1 o 1 <3>~ 
1 1 0 0 
1 1 1|0 
\AB 
1 
0 
1 
o 
e: 
<~\B 
oo 
01 
11 
0 
1 
1-

2.5 An Introduction to Boolean algebra 
71 
16. The groups of minterms should be as large as possible—a 
single group of four minterms yields a simpler expression 
than two groups of two minterms. The final stage in simplify-
ing an expression is reached when each of the groups of 
minterms (i.e. the product terms) are ORed together to form 
the simplified sum-of-products expression. This process is 
best demonstrated by means of examples. In what follows, a 
four-variable map is chosen to illustrate the examples. 
Transferring a truth table to a Karnaugh map is easy 
because each 1 in the truth table is placed in a unique square 
on the map. We now have to demonstrate how the product 
terms of a general Boolean expression are plotted on the map. 
Figures 2.62-2.67 present six functions plotted on Karnaugh 
maps. In these diagrams various sum-of-products expressions 
have been plotted directly from the equations themselves, 
rather than from the minterms of the truth table. The follow-
ing notes should help in understanding these diagrams. 
1. For a four-variable Karnaugh map 
one-variable product term covers 8 squares 
two-variable product terms cover 4 squares 
three-variable product terms cover 2 squares 
four-variable product terms cover 1 square. 
2. A square covered by a 1 may belong to more than one term 
in the sum-of-products expression. For example, in 
Fig. 2.63 the minterm A B C D belongs to two groups, 
A • B and C•D. If a 1 on the Karnaugh map appears in two 
groups, it is equivalent to adding the corresponding 
minterm to the overall expression for the function plotted 
on the map twice. Repeating a term in a Boolean expression 
does not alter the value of the expression, because one of 
the axioms of Boolean algebra is X + X = X. 
3. The Karnaugh map is not a square or a rectangle as it 
appears in these diagrams. A Karnaugh map is a torus or 
doughnut shape. That is, the top edge is adjacent to the 
bottom edge and, the left-hand edge is adjacent to the 
right-hand edge. For example, in Figure 2.65 the term A • 
D covers the two minterms A B C D and A • B • C • D at 
the top, and the two minterms A B C D and A - B • C • D 
at the bottom of the map. Similarly, in Fig. 2.66 the term 
B • D covers all four corners of the map. Whenever a group 
CD< B 00 
01 
11 
10 
ABCD 
ACD 
The two-variable term A • D covers four 
squares (the region A = 0 and D = 1).The 
term A B C D covers one square and is part 
of the same group as A • D. 
Figure 2.62 Plotting F = AD +ACD + ABCD on a Karnaugh map. 
.AB 
CDN 
00 
01 
11 
10 
00 
01 
11 
10 
t—1 
1 
1 
c c 
1 
1 
•u 
c 
1 
AB 
The two-variable term A • B covers four squares 
(the region A = 0 and B = 0).The two-variable 
term C • D covers four squares (the region C = 1 
and D = 1). The term A • B • C D is common to 
both groups. 
Figure 2.63 Plotting F = AB + CD on a Karnaugh map. 
• AD 
1 
OX. 
01 
11 
CD 
01 
11 
10 
00 

72 
Chapter 2 Gates, circuits, and combinational logic 
>AB 
ABCD 
The one-variable term A covers four squares 
(the region A = 0). 
Figure 2.64 Plotting F = A + BD + ABCD on a Karnaugh map. 
CD^ 
00 
01 
11 
10 
vAB 00 
01 
11 
10 
w 
<L 
•ABCD 
•ACD 
The four-variable term A D covers four squares 
(the region A = 0, D =0). Note that two squares 
are at the top (A = 0, C = 0, D = 0) and two are 
at the bottom (A = 0, C = 1, D = 0). 
Figure 2.65 Plotting F = A D + A C D + A B C D on a Karnaugh map. 
00 
01 
11 
10 
The four-variable term B D covers four squares 
(the region B = 0, D = 0). In this case the 
adjacent squares are the corner squares. If you 
examine any pair of horizontally or vertically 
adjacent corners, you will find that they differ in 
one variable only. 
Figure 2.66 Plotting F = B D + ABCD + ACD on a Karnaugh map. 
of terms extends across the edge of a Karnaugh map, we 
have shaded it to emphasize the wraparound nature of 
the map. 
4. In order either to read a product term from the map, or to 
question, 'what minterms (squares) are covered by this 
term?' Consider the term A • D in Fig. 2.62. This term covers 
all squares for which A = 0 and D = 1 (a group of 4). 
Having shown how terms are plotted on the Karnaugh 
plot a product term on the map, it is necessary to ask the 
map, the next step is to apply the map to the simplification of 
\AB 
C D \ 
00 
01 
11 
10 
-> -> 
o 
o 
_> -> 
o 
o 
8/ 
kABCD 
1 
A 
i 
n 
1 
i 
1 
i 
01 
11 
10 
00 
>A 
•AD 
1 
1 
1 
c: 
GX 

2.5 An Introduction to Boolean algebra 
73 
,AB 
C D \ 
00 
00 
01 
11 
10 
'AC 
•BCD 
Three two-variable groups overlap. In 
this example, the square (i.e. minterm) 
A B C D belongs to groups B C D , 
AB-C,andA-C-D. 
Figure 2.67 Plotting F = ACD + ABC + BCD + AC on a Karnaugh map. 
.AB 
C D \ 
00 
01 
11 
10 
00 
01 
11 
10 
A 0 
0 
i 
0 
i 
i 
VAB 
C D \ 
00 
01 
11 
10 
00 
01 
11 
10 
(I 1 
1 
(I 
1 
' 1 
1 
1 
1 
1 
1 
Figure 2.68 Karnaugh map for Example 1. 
.AB 
CD^ 
00 
01 
11 
10 
00 
01 
11 
10 
AB 
0 
0 
(1 
1) 
1 
1 
T 
1 
^ 
Cff-
00 
01 
11 
10 
00 
01 
11 
10 
0 
o 
0, 
1) 
1 
-n 
1 
V 
U 
Figure 2.69 Karnaugh map for Example 2. 
the expressions. Once again, we demonstrate this process by 
Example 2 F = A C D + A B C + A C D + A B D (Fig. 
means of examples. In each case, the original function is plot- 
2.69). In this case only one regrouping is possible. The simpli-
ted on the left-hand side ofthe figure and the regrouped ones fied function is F = B D + A C D + A C D + A B C . 
(i.e. minterms) are plotted on the right-hand side. 
— — — 
— 
— „ 
v 
B 
Example 
3 
F = A B C D + A-B-CD -j^A-B-C_-D +_ 
Example 1 Figure 2.68 gives a Karnaugh map for theexpres- 
A B C D + A B C D + A B C - D + A - B C D + A B C - D 
sion F = A-B + A B - C D + A B C D + A B - C D . The 
(Fig. 2.70). Thisfunction can be simplified to two product 
simplified function is F = A B + B D + A C D . 
termswithF = B D + B D . 
->ABC 
*ACD 
01 
11 
10 
1 
1 
1 
1 
1 
1 
1 
1 

74 
Chapter 2 Gates, circuits, and combinational logic 
AB 
CDN 
00 
01 
11 
10 
00 
01 
11 
10 
.AB 
CD> 
00 
01 
11 
10 
0 
© 
00 
01 
11 
10 
\ 
i 
0 0 
00 
01 
11 
10 
(i 
i>l 
0 © 
00 
01 
11 
10 
b 
V 
0 
© 
00 
01 
11 
10 
r 
' l 
Figure 2.70 Karnaugh map for Example 3. 
.AB 00 
01 
11 
10 
1 
1 
1 
1 
1 
(a) Ones placed. 
c \ oo oi 11 10 
0 
1 
(b) Ones grouped. 
T 
ft _J) T 
II 
j ^ 
X 
oo oi ii 10 
G a u 
(c) Alternate grouping. 
Figure 2.71 Karnaugh map for Example 4. 
.AB 
CDX 
00 
01 
11 
10 
00 
01 
11 
10 
.AB 
CDN 
00 
01 
11 
10 
00 
01 
11 
10 
(j 
1 
1 
1 
1 i] 
00 
01 
11 
10 
(j 
1 
1 i] 
00 
01 
11 
10 
i 
1 
00 
01 
11 
10 
i 
1 
00 
01 
11 
10 
0 
0 
i 
1 
00 
01 
11 
10 
0 
0 
i 
1 
00 
01 
11 
10 
i 
n 
00 
01 
11 
10 
i 
1 
1 
n 
00 
01 
11 
10 
Figure 2.72 Example 5—using a 
Karnaugh map to obtain the 
complement of a function. 
Example 4 F = A B C + A B C + A B C + A B C + 
A • B • C (Fig. 2.71). We can group the minterms together in two 
ways, both of which are equally valid; that is, there are two 
equally correct simplifications of this expression. We can write 
either F = A B + A-C + A-BorF = A B + B-C + A-B. 
Applications of Karnaugh maps 
Karnaugh maps can also be used to convert sum-of-products 
expressions to the corresponding product-of-sums form. The 
first step in this process involves the generation of the com-
plement of the sum-of-products expression. 
Example 5 The Karnaugh map in Fig. 2.72 demonstrates how 
we can obtain the complement of a sum-of-products expression. 
Consider the expression F = C-D + A-B -I- A-B + C-D 
(left-hand side of Fig. 2.72). If the squares on a Karnaugh 
map covered by 1 s represent the function F, then the remain-
ing squares covered by 0s must represent F, the complement 
of F. In the right-hand side of Fig. 2.72, we have plotted the 
complement of this function. The group of four Os corre-
sponds to the expression F = B-D. 
Example 6 We can use a Karnaugh map to convert of sum-
of-products expression into a product-of-sums expression. 
In Example 5, we used the Karnaugh map to get the comple-
ment of a function in a product-of-sums form. If we then 
complement the complement, we get the function but in a 
sum-of-products form (because de Morgan's theorem allows 
us to step between SoP and PoS forms). Let's convert 
F = A-B-C + C-D + A-B-D into product of sums form 
(Fig. 2.73). 
The complement of F is defined by the zeros on the map 
and may be read from the right-hand map as 
F = C D + B-C + A D 
F = C D + B-C + A D 
= (C + D)(B + C)(A + D) 
We now have an expression for F in product-of-sums form. 
0 
1 
1 
1 
c 
0 
1 

2.5 An Introduction to Boolean algebra 
75 
VAB 
CDN 
00 
01 
11 
10 
00 
01 
11 
10 
1 
d 
1 
1 
0 
1 
J 
? 
1 
\AB 
c r K 
00 
01 
11 
10 
00 
01 
11 
10 
& -
oj 
0 
°) 
0 
0 
o 
1 
0 
o 
Figure 2.73 Example 6—using a 
Karnaugh map to convert an expression 
from SoP to PoS form. 
Using the Karnaugh map to design a circuit 
with NAND logic 
Now that we've demonstrated how Karnaugh maps are used 
to simplify and transform Boolean expressions, we're going 
to apply the Karnaugh map to the design of a simple logic cir-
cuit using NAND logic only. 
A fire detection system protects a room against fire by means 
of four sensors. These sensors comprise a flame detector, a 
smoke detector, and two high-temperature detectors located at 
the opposite ends of the room. Because such sensors are prone 
to errors (i.e. false alarms or the failure to register a fire), the fire 
alarm is triggered only when two or more of the sensors indi-
cate the presence of a fire simultaneously. The output of a sen-
sor is a logical 1 if a fire is detected, otherwise a logical 0. 
The output of the fire alarm circuit is a logical 1 whenever 
two or more of its inputs are a logical one. Table 2.19 gives the 
truth table for the fire detector circuit. The inputs from the 
four sensors are labeled A, B, C, and D. Because it is necessary 
only to detect two or more logical 1 s on any of the lines, the 
actual order of A, B, C, and D columns doesn't matter. The 
circuit is to be constructed from two-input and three-input 
NAND gates only. 
The output of the circuit, F, can be written down directly 
from Table 2.19 by ORing the 11 minterms to get the expression 
F = A-BC-D + A-B-C-D + A-B-C-D + A-B-CD 
+ A-B-C-D + A-B-C-D + A-BC-D + A-B-C-D 
+ A-B-C-D + A-B-CD + A-B-C-D 
Plotting these 11 minterms terms on a Karnaugh map we 
get Fig. 2.74(a). The next step is to group these terms together 
into six groups of four minterms (Fig. 2.74(b)). Note that the 
minterm A-B-C-D belongs to all six groups. 
Therefore, the simplified sum-of-products form of F is 
given by 
F = A B + A-C + A-D + B-C + B-D + C-D 
This expression is (as you might expect) the sum of all possible 
two-variable combinations. 
Table 2.19 Truth table for a fire detector. 
In order to convert the expression into NAND logic only 
form, we have to eliminate the five logical OR operators. We 
do that by complementing F twice and then using de Morgan's 
theorem. 
F = F = A B + A C + A D + B-C + B-D + C D 
A-B-A-C-A-D-B-C-B-D-C-D 
Although we have realized the expression in NAND logic 
as required, it calls for a six-input NAND gate. If the expres-
sion for F is examined, it can be seen that six terms are 
NANDed together, which is the same as ANDing them and 
then inverting the result. Because of the associative property 
Inputs 
Output 
A 
B 
C 
D 
F 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
1
0 
0 
0 
0 
1
1 
1 
0 
1
0 
0 
0 
0 
1
0 
1 
1 
0 
1
1
0 
1 
0 
1
1
1 
1 
1
0 
0 
0 
0 
1
0 
0 
1 
1 
1
0 
1
0 
1 
1
0 
1
1 
1 
1
1
0 
0 
1 
1
1
0 
1 
1 
1
1
1
0 
1 
1
1
1
1 
1 

76 
Chapter 2 Gates, circuits, and combinational logic 
.AB 
CD^ 
00 
01 
11 
10 
00 
01 
11 
10 
XAB 
c r j \ 
00 
01 
11 
10 
© 
© 
© 
© 
© 
0 
© 
© 
0 
© 
© 
00 
D 
01 
1 
1 
1 
11 0 
[1 
-
n 
10 
1 U \ 
(a) Location of the 1s. 
(b) After grouping the 1s 
Figure 2.75 NAND-only circuit for fire detector. 
of Boolean variables, we can write X (Y • Z) = (X • Y)Z and 
hence extending this to our equation we get 
F = A B A C A D B - C B D C D 
Figure 2.74 Karnaugh map corresponding 
to Table 2.19. 
Figure 2.75 shows how this expression can be implemented 
in terms of two- and three-input NAND gates. 
Using Karnaugh Maps—an example 
A circuit has four inputs, A, B, C, and D, representing the 16 
natural binary integers from 0000 to 1111 (i.e. 0 to 15). The 
output of the circuit, F, is true if the input is divisible by a 
multiple of 4,5,6, or 7, with the exception of 15, in which case 
the output is false. Zero is not divisible by 4,5,6, or 7. Suppose 
we wish to design a logic circuit to implement F using NAND 
gates only. 
We can obtain a sum-of-products expression for F from 
Table 2.20 by writing down the sum of the minterms (i.e. the 
lines with a 1). 
F = A B C D + A B C D + A B C D + A B C D 
+ AB-CD + A B C D + A B C D + A B C D 
In puts 
Number 
F 
A 
B 
C 
D 
Number 
F 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
1 
0 
0 
0 
1 
0 
2 
0 
0 
0 
1 
1 
3 
0 
0 
1 
0 
0 
4 
1 
Divisible by 4 
0 
1 
0 
1 
5 
1 
Divisible by 5 
0 
1 
1 
0 
6 
1 
Divisible by 6 
0 
1 
1 
1 
7 
1 
Divisible by 7 
0 
0 
0 
8 
1 
Divisible by 4 
0 
0 
1 
9 
0 
0 
1 
0 
10 
1 
Divisible by 5 
0 
1 
1 
11 
0 
1 
0 
0 
12 
1 
Divisible by 6 
1 
0 
1 
13 
0 
1 
1 
0 
14 
1 
Divisible by 7 
1 
1 
1 
15 
0 
False by definition 
Table 2.20 Truth table for example. 
By means of Boolean algebra the expression can be simpli-
fied to 
F = ABC(D + D) + ABC(D + D) + ABD(C + C) 
+ ABD(C + C) 
= A B C + A B C + A-B-D + A-B-D 
= AB(C + C) + A-D(B + B) 
= AB + AD 
Figure 2.76 gives the Karnaugh map for F. In Fig. 2.77 the 
squares covered by Is are formed into two groups of four. 
This gives F = A B + A-D, which is reassuringly the same as 
the result obtained above. 
A . 
B -
C . 
D . 

2.5 An Introduction to Boolean algebra 
77 
c ^ \ 
00 
01 
11 
10 
00 
01 
11 
10 
1 
1 
1 
1 
1 
1 
1 
1 
AD AB 
Figure 2.78 NAND-only circuit. 
Figure 2.76 Karnaugh map for F. 
Figure 2.77 Karnaugh map after regrouping the minterms. 
To obtain a product-of-sums expression, it's necessary to 
generate the complement of F in a sum-of-products form 
and then complement it. 
F = A B + A-D 
F = A-B + A D 
(A + B)(A + D) 
Get the complement of F 
Complement of F in 
product-of-sums form 
A-A + A-D + A-B + B-D 
Multiply out sum terms 
= A-D + A B + B D 
= A-D + A-B 
F = A-D + A-B 
= (A + D)(A + B) 
Complement of F in 
sum-of-products form 
Complement in 
simplified sum-of-
products form 
Complement the 
complement to get F 
Function in required 
product-of-sums form 
Note that the complement of F in sum-of-products form 
could have been obtained directly from the Karnaugh map of 
F by considering the squares covered by zeros. 
To convert the expression F = A-B + A-D into NAND 
logic form, the' +' must be eliminated. 
F = F = A-B + A-D = A-B-A-D 
The inverse functions A and D can be generated by two-
input NAND gates with their inputs connected together. 
Figure 2.78 implements F in NAND logic only. 
Karnaugh maps and don't care conditions 
We now demonstrate how Karnaugh maps can be applied 
to problems in which the truth table isn't fully specified; that 
is, for certain input conditions the output is undefined. 
Occasionally, a system exists in which a certain combination 
of inputs can't happen; or, if it does, we don't care what the 
output is. In such cases, the output may be defined as either 
true or false. 
Consider 
the 
Karnaugh 
map 
of 
Fig. 
2.79 
for 
F = A-B-D + A-B-C-D. Now suppose that the input condi-
tions A • B • C • D and A • B • C • D cannot occur. We have marked 
these two inputs on the map with an X. The value of X is 
undefined (if the input can't occur then the value of the out-
put is undefined). 
If an input can't occur and the output is undefined, we can 
cover that square with either a 0 or a 1. In Fig. 2.79(b) we have 
made one of the Xs a 1 and one of the Xs a zero. We can 
express the output function as F = B • D, which is simpler 
than the function in Fig. 2.79(a). 
A don't care condition is set to a 0 or a 1 in order to simplify 
the solution. There is an important exception. Although an 
impossible input can't occur in normal circumstances, it 
could under fault conditions (e.g. when an input circuit 
fails). No designer would assign an output to an impossible 
input condition that might lead to an unsafe or dangerous 
situation. However, the ultimate aim is to cover all the Is in 
J? 
A A 
fB 
' A D 
( - ^ 
00 
01 
11 
10 
00 
01 
11 
10 
A-
B 
D-
1> 
1 
1 
J, 
1 
"71 
iJ 
. 1 
AD 
/ 
-A-E 

78 
Chapter 2 Gates, circuits, and combinational logic 
the map and to incorporate them in the smallest number of 
large groups. 
The following example demonstrates the concept of 
impossible input conditions. An air conditioning system has 
two temperature control inputs. One input, C, from a cold-
sensing thermostat is true if the temperature is below 15°C 
and false otherwise. The other input, H, from a hot-sensing 
thermostat is true if the temperature is above 22°C and false 
otherwise. Table 2.21 lists the four possible logical conditions 
for the two inputs. 
c r K 
00 
01 
11 
10 
00 
01 
11 
10 
n 
1 
X 
X 
j , 
0 
\AB 
C D \ 
00 
01 
00 
The input condition C = 1,H = 1 in Table 2.20 has no real 
meaning, because it's impossible to be too hot and too cold 
simultaneously. Such an input condition could arise only if at 
least one of the thermostats failed. Consider now the example 
of an air conditioning unit with four inputs and four outputs. 
Table 2.22 defines the meaning of the inputs to the controller. 
The controller has four outputs P, Q, R, and S. When P = 1 
a heater is switched on and when Q = 1 a cooler is switched 
on. Similarly, a humidifier is switched on by R = 1 and a 
dehumidifier by S = 1. In each case a logical 0 switches off the 
appropriate device. The relationship 
between the inputs and outputs is as 
follows. 
11 
10 
01 
11 
10 
X 
r l 
*l 
X 
I1 
V 
(a) The function F = ABD + ABCD. 
(b) The function F 
Note that the inputs ABCD and ABCD 
Minterm ABCD is 
cannot occur 
the expression 
Figure 2.79 The effect of don't care conditions. 
= BD. 
included to simplify 
Inputs 
H 
Meaning 
0 
0 
0 
1 
1 
0 
1 
1 
Temperature OK 
Too hot 
Too cold 
Impossible condition 
Table 2.21 Truth table for a pair of temperature sensors. 
If the temperature and humidity are 
both within limits, switch off the 
heater and the cooler. The humidifier 
and dehumidifier are both switched 
off unless stated otherwise. 
If the humidity is within limits, 
switch on the heater if the tempera-
ture is too low and switch on the 
cooler if the temperature is too high. 
If the temperature is within limits, 
switch on the heater if the humidity is 
too low and the cooler if the humidity 
is too high. 
• If the humidity is high and the tem-
perature low, switch on the heater. If 
the humidity is low and the tempera-
ture high, switch on the cooler. 
• If both the temperature and humidity are high switch on 
the cooler and dehumidifier. 
• If both the temperature and humidity are too low switch on 
the heater and humidifier. 
The relationship between the inputs and outputs can now be 
expressed in terms of a truth table (Table 2.23). We can draw 
Karnaugh maps for P to S, plotting a 0 for a zero state, a 1 for a 
one state, and an X for an impossible state. Remember that an 
X on the Karnaugh map corresponds to a state that cannot 
exist and therefore its value is known as a don't care condition. 
Input 
Name 
H 
Hot 
C 
Cold 
W 
Wet 
D 
Dry 
Meaning when input = 0 
Meaning when input = 1 
Temperature < upper limit 
Temperature > lower limit 
Humidity < upper limit 
Humidity > lower limit 
Temperature > upper limit 
Temperature < lower limit 
Humidity > upper limit 
Humidity < lower limit 
Table 2.22 Truth table for a climate controller. 
c 

2.5 An Introduction to Boolean algebra 
79 
1 nputs 
Condition 
Outputs 
H 
c 
W 
D 
Condition 
P 
Q 
cooler 
R 
humidifier 
s 
heater 
Q 
cooler 
R 
humidifier 
dehumidifier 
0 
0 
0 
0 
OK 
0 
0 
0 
0 
0 
0 
0 
1 
Dry 
1 
0 
0 
0 
0 
0 
1 
0 
Wet 
0 
1 
0 
0 
0 
0 
1 
1 
Impossible 
X 
X 
X 
X 
0 
1 
0 
0 
Cold 
1 
0 
0 
0 
0 
1 
0 
1 
Cold and dry 
1 
0 
1 
0 
0 
1 
1 
0 
Cold and wet 
1 
0 
0 
0 
0 
1 
1 
1 
Impossible 
X 
X 
X 
X 
0 
0 
0 
Hot 
0 
1 
0 
0 
0 
0 
1 
Hot and dry 
0 
1 
0 
0 
0 
1 
0 
Hot and wet 
0 
1 
0 
1 
0 
1 
1 
Impossible 
X 
X 
X 
X 
1 
0 
0 
Impossible 
X 
X 
X 
X 
1 
0 
1 
Impossible 
X 
X 
X 
X 
1 
1 
0 
Impossible 
X 
X 
X 
X 
1 
1 
1 
Impossible 
X 
X 
X 
X 
Table 2.23 Truth table for a climate controller. 
vHC 
WD\ 
00 
01 
11 
10 
00 
01 
11 
10 
1 
X 
1 
1 
X 
X 
X 
X 
X 
1 
X 
Figure 2.80 Karnaugh map for P (the heater). 
Figure 2.80 provides a Karnaugh map corresponding to 
output P, the heater. We have marked all the don't care condi-
tions with an X. We could replace the Xs by 1 s or 0s. However, 
by forcing some of the don't care outputs to be a 1, we can 
convert a group of Is into a larger group. 
Figure 2.81 provides Karnaugh maps for outputs P, Q, R, 
and S. In each case we have chosen the don't care conditions 
to simplify the output function. For example, the Karnaugh 
map of Fig. 2.81(a) corresponds to output P where we have 
included six of the don't care conditions within the groupings 
togetP = C + H D . 
You should appreciate that by taking this approach we have 
designed a circuit that sets the output 1 for some don't care 
inputs and 0 for other don't care inputs. You cannot avoid 
this. The output of any digital circuit must always be in a 0 or 
a 1 state. As we said at the beginning of this chapter, there is 
no such state as an indeterminate state. It is up to the designer 
to choose what outputs are to be assigned to don't care 
inputs. 
Exploiting don't care conditions—constructing a 
seven-segment decoder 
We now design a BCD-to-seven-segment decoder (BCD 
means binary-coded decimal). The decoder has a 4-bit nat-
ural binary BCD input represented by D, C, B, A, where A is 
the least-significant bit. Assume that the BCD input can 
never be greater than 9 (Chapter 4 describes BCD codes). The 
seven-segment decoder illustrated by Fig. 2.82 has seven out-
puts (a to g), which are used to illuminate any combination of 
bars a to g of a seven-segment display; for example, if the code 
for 2 (i.e. 0010) is sent to the decoder, segments a, b, d, e, and 
g are illuminated to form a '2'. 
The truth table for this problem is given in Table 2.24. This 
table has four inputs and seven outputs (one for each of the 
segments). 
We can now solve the equation for segments a to g. By 
using Karnaugh maps the don't care conditions can be 
catered for. 

8 0 
Chapter 2 Gates, circuits, and combinational logic 
J-IC 
WD> 
00 
01 
11 
10 
00 
01 
11 
10 
wHC 
1 
X 
X 
1 
1 
X 
X 
X 
J 
X 
X 
X 
X 
1 
X 
J 
WD^1 
00 
01 
11 
10 
00 
01 
11 
10 
' X 
1] 
X 
1 
X 
X 
X 
X 
1 
X A 
(a)P = C + HD. 
(b) Q = H + CW. 
MC 
WD 
00 
01 
11 
10 
00 
01 
11 
10 
MQ 
X 
' 1 
x" 
X 
, x 
X 
J 
X 
X 
X 
X 
W D \ 
00 
01 
11 
10 
00 
01 
11 
10 
X 
X 
X 
X 
X 
X 
'x 
x ) 
. x 0 
(c) R = CD. 
(d) S = HW. 
Figure 2.81 Karnaugh maps for outputs 
P,Q,R,andS. 
Display 
Decoder 
a 
b 
c 
d 
e 
f 
g 
a 
b 
c 
d 
e 
f 
g 
a 
b 
c 
d 
e 
f 
g 
a 
b 
c 
d 
e 
f 
g 
a 
b 
c 
d 
e 
f 
g 
a 
b 
c 
d 
e 
f 
g 
a 
b 
c 
d 
e 
f 
g 
a 
b 
c 
d 
e 
f 
g 
T he decoder convert s 
a 4-bit binary numeric 
code on 0, C, B, A into 
the signals that light up 
segments a to g of the 
display 
Figure 2.82 The seven-segment display. 
Figure 2.83 gives the Karnaugh map for segment a. From 
the Karnaugh map we can write down the expression for 
a = D + B + C-A + C-A. 
An alternative approach is to obtain a by considering 
the zeros on the map to get the complement of a. From 
the 
Karnaugh 
map 
in 
Fig. 
2.84 
we 
can 
write 
a = D-C-B-A -I- C-B-A. Therefore, 
a = D-C-B-A + C-B-A 
= (D + C + B + A) (C + B + A) 
= D C + D-B + D-A + C-C + C B + C-A + C B 
+ B-B + B-A + C-A + B-A + A-A 
= D-C + D-B + D-A + C B + C-A + C B + B 
+ B-A + C-A + B-A 
= D-C + D-A + C-A + B + C-A 
= D-C + C-A + B + C-A 
1 offers no improvement over the first realiza-
This expression i 
riguic i.oj provides the Miuaugn map IOI aegiiieiiL u, 
which gives b = C + B-A + B-A. We can proceed as we did 
for segment a and see what happens if we use b. Plotting zeros 
on the Karnaugh map for b we get b = C + B-A-C-B-A 
Fig. 2.86. Therefore, 
b = C-B-A + C-B-A 
= (C + B + A)(C + B + A) 
= C + B-A + B-A 

2.5 An Introduction to Boolean algebra 
81 
Inputs 
Character 
Outputs 
D C B A 
Character 
a 
b 
c 
d 
e 
f 
g 
0 0 0 0 
n 
LI 
1 
1 
1 
1 
1 
0 
0 0 0 1 
0 
1 
0 
0 
0 
0 
0 0 1 0 
C 
1 
1 
0 
1 
1 
0 
1 
0 0 1 1 
-J 
1 
1 
1 
0 
0 
1 
0 1 0 0 
'"I 
0 
1 
0 
0 
1 
1 
0 1 0 1 
1 
0 
1 
0 
1 
1 
0 1 1 0 
CI 
1 
0 
1 
1 
1 
1 
0 1 1 
1 0 0 
1 0 0 
1 
0 
1 
a 
I_I 
n 
1 
1 
1 
1 
1 
1 
o -> o 
0 
1 
0 
0 
1 
1 
0 
1 
1 
1 0 1 0 
Forbidden code 
X 
X 
X 
X 
X 
X 
X 
1 0 1 1 
X 
X 
X 
X 
X 
X 
X 
1 1 0 0 
X 
X 
X 
X 
X 
X 
X 
1 1 0 1 
X 
X 
X 
X 
X 
X 
X 
1 1 1 0 
X 
X 
X 
X 
X 
X 
X 
1 1 1 1 
X 
X 
X 
X 
X 
X 
X 
Table 2.24 Truth table for a seven—segment display. 
N.DC 
B A \ 
00 
01 
11 
10 
00 
01 
11 
10 
VDC 
B X 
00 
01 
11 
10 
1 
X 
1 
1 
X 
1 
1 
1 
X 
X 
1 
1 
X 
X 
00 
1J 
fx 
11 
fx 
1 
01 
6 1 
1 
1 
11 
1 
k 
X ; 
X 
1 
i
X 
; X 
10 
1 
V 
1 
i
X 
; X 
T* 
Figure 2.83 Karnaugh map for the 
segment a control signal. 
B A \ 
00 
01 
11 
10 
00 
01 
11 
10 
\ D C 
B X 
00 
01 
11 
10 
0 
X 
00 
01 
11 
10 
0 
X 
00 
01 
11 
10 
(° 
* ) 
0 
X 
00 
01 
11 
10 
© 
X 
X 
X 
00 
01 
11 
10 
X 
X 
X 
X 
00 
01 
11 
10 
X 
X 
Figure 2.84 Karnaugh map for the 
complement of segment a. 

82 
Chapter 2 Gates, circuits, and combinational logic 
vDC 
BAN 
00 
01 
11 
10 
00 
01 
11 
10 
<DC 
1 
1 
X 
1 
1 
X 
1 
1 
1 
X 
X 
1 
X 
X 
BAN 
00 
01 
11 
10 
00 
01 
11 
10 
\ 
,„, r~ 
1 
1 
X 
1 
1 
X 
1 
1 
1 
X 
X 
1 
X 
l
x 
Figure 2.85 Karnaugh map for 
segment b. 
vDC 
BA^ 
00 
01 
11 
10 
00 
01 
11 
10 
.DC 
X 
0 
X 
X 
X 
0 
X 
X 
BAV 
00 
01 
11 
10 
00 
01 
11 
10 
X 
( 0 
X ) 
X 
X 
X 
( 0 
x) 
X 
X 
Figure 2.86 Karnaugh map for the 
complement of segment b. 
This expression yields the same result as that obtained 
directly by considering the Is on the Karnaugh map. The 
equations for the remaining five segments can be considered 
in a similar way. 
Example of the use of Karnaugh maps to 
implement a circuit 
A logic circuit has four inputs D, C, B, A, which represent two 
pairs of bits (D,C) and (B,A). Bits (B,A) are subtracted from bits 
(D,C) to give a result Fv F0 and an n-bit that indicates a negative 
result. Table 2.25 provides a truth table for this problem. 
We first construct three Karnaugh maps for the outputs 
and use them to obtain simplified sum-of-product expres-
sions (Table 2.25). 
Figure 2.87 provides the three Karnaugh maps cor-
responding to outputs n, F,, and F0 in the truth table. The Is 
have been regrouped under each truth table to provide the 
minimum number of large groups. 
We can write down expressions for n, F[, and F0 from 
Fig. 2.87 as 
n = D B + C-B-A + D-C-A 
F, = D-C-B + D C B + D B A + D B A 
F0 = C-A + C-A 
Inputs 
Number 
Outputs 
n 
F, 
F0 
D C B A 
0 0 0 0 
0 - 0 = 0 
0 
0 
0 
0 0 0 
1 
0 - 1 = - 1 
1 
0 
1 
0 0 
1 0 
0 - 2 = - 2 
1 
1 
0 
0 0 
1 1 
0 - 3 = - 3 
1 
1 
1 
0 
1 0 0 
1 - 0 = 1 
0 
0 
1 
0 
1 0 
1 
1 - 1 = 0 
0 
0 
0 
0 
1 1 0 
1 - 2 = - 1 
1 
0 
1 
0 
1 1 1 
1 - 3 = - 2 
1 
1 
0 
1 0 0 0 
2 - 0 = 2 
0 
1 
0 
1 0 0 
1 
2 - 1 = 1 
0 
0 
1 
1 0 
1 0 
2 - 2 = 0 
0 
0 
0 
1 0 
1 1 
2 - 3 = - 1 
1 
0 
1 
1 1 0 0 
3 - 0 = 3 
0 
1 
1 
1 1 0 
1 
3 - 1 = 2 
0 
1 
0 
1 1 1 0 
3 - 2 = 1 
0 
0 
1 
1 1 1 1 
3 - 3 = 0 
0 
0 
0 
Table 2.25 Truth table for a two-bit subtractor. 

2.6 Special-purpost logic elements 
83 
.DC 
BAN 
00 
01 
11 
10 
00 
01 
11 
10 
1 
1 
1 
1 
1 
1 
BAN 
00 
01 
11 
10 
(a) Karnaugh map for n. 
.DC 
00 
01 
11 
10 
i 
l « TO-™**"**^ 
1 
(L 
I 1 
\ 
\ D C 
B K 
00 
01 
11 
10 
00 
01 
11 
10 
(b) Karnaugh map for F-, 
.DC 
1 
1 
1 
1 
1 
1 
vDC 
BA^ 
00 
01 
11 
10 
00 
01 
11 
10 
0 s 
1 
0 
1) 
1 
(a) Regrouped Karnaugh map for n. 
(b) Regrouped Karnaugh map for F, 
Figure 2.87 The Karnaugh maps for the subtractor. 
BAN 
00 
01 
11 
10 
00 
01 
11 
10 
1 
1 
1 
1 
1 
1 
1 
1 
(c) Karnaugh map for F0 
DC 
BAN 
00 
01 
11 
10 
00 
01 
11 
10 
ll J 
~^l 
M 
1 
I 1 
r n 
(c) Regrouped Karnaugh map for F0 
2.6 Special-purpose logic 
elements 
So far, we've looked at the primitive logic elements from 
which all digital systems can be constructed. As technology 
progressed, more and more components were fabricated on 
single chips of silicon to produce increasingly complex cir-
cuits. Today, you can buy chips with tens of millions of gates 
that can be interconnected electronically (i.e. the chip pro-
vides a digital system whose structure can be modified elec-
tronically by the user). Indeed, by combining microprocessor 
technology, electronically programmable with arrays of 
gates, we can now construct self-modifying (self-adaptive) 
digital systems. 
Let's briefly review the development of digital circuits. The 
first digital circuits contained a few basic NAND, NOR, AND 
gates, and were called small-scale integration (SSI). Basic SSI 
gates were available in 14-pin dual-in-line (DIL) packages. 
Dual-in-line simply means that there are two parallel rows of 
pins (i.e. contacts) forming the interface between the chip 
and the outside world. The rows are 0.3 inches apart and the 
pins are spaced by 0.1 inch. Two pins are used for the power 
supply (Vcc = +5.0 V and ground = 0 V). These devices are 
often called 74-series logic elements because the part number 
of each chip begins with 74; for example, a 7400 chip contains 
four NAND gates. Today, the packaging of such gates has 
shrunk to the point where the packages are very tiny and are 
attached to circuit boards by automatic machines. 
It soon became possible to put tens of gates on a chip and 
manufacturers connected gates together to create logic func-
tions such as a 4-bit adder, a multiplexer, and a decoder. Such 
circuits are called medium-scale integration (MSI). By the 
1970s entire systems began to appear on a single silicon chip, 
of which the microprocessor is the most spectacular example. 
The technology used to make such complex systems is called 
large-scale integration (LSI). In the late 1980s LSI gave way to 
very-large-scale integration (VLSI), which allowed designers 
to fabricate millions of transistors on a chip. Initially, VLSI 
technology was applied to the design of memories rather 
than microprocessors. Memory systems are much easier to 
design because they have a regular structure (i.e. a simple 
memory cell is replicated millions of times). 

84 
Chapter 2 Gates, circuits, and combinational logic 
A major change in digital technology occurred in the mid 
1990s. From the 1970s to the 1990s, digital logic had largely 
used a power supply of + 5 V. As the number of gates per chip 
approached the low millions, the problem of heat manage-
ment created a limit to complexity. It was obvious that more 
and more transistors couldn't be added to a chip without 
limit because the power they required would destroy the chip. 
Radiators and fans were used to keep chips cool. Improvements 
in silicon technology in the 1990s provided digital logic ele-
ments that could operate at 3 V or less and, therefore, create 
less heat. A further impetus to the development of low-power 
systems was provided by the growth of the laptop computer 
market. 
We now look at the characteristics of some of the simple 
digital circuits that are still widely available—even though 
VLSI systems dominate the digital world, designers often 
have to use simple gates to interface these complex chips to 
each other. 
2.6.1 The multiplexer 
A particularly common function arising regularly in digital 
design is the multiplexer, which we met earlier in this chapter. 
Figure 2.88 shows the 74157, a quad two-input multiplexer, 
which is available in a 16-pin MSI circuit. The prefix quad 
simply means that there are four multiplexers in one package. 
Each of the four Y outputs is connected to the correspond-
ing A input pin when SELECT = 0 and to the B input when 
SELECT = 1. The multiplexer's STROBE input forces all Y 
outputs into logical 0 states whenever STROBE = 1. We have 
already described one use of the multiplexer when we looked 
at some simple circuits. 
The STROBE input enables the 
multiplexer. When STROBE = 1 
all Y outputs are set to zero. 
When STROBE = 0 the outputs 
are either the A or B inputs 
The SELECT inputs determines 
whether the output is connected 
to the A input or the B input 
Figure 2.89 illustrates the structure of a l-of-8 data 
multiplexer, which has eight data inputs, D0, D[, D 2,..., D7, 
an output Y, and three data select inputs, S0, Sly S2. When S0, 
SL S2 = 0,0,0 the output is Y = D0, and when S0, S„ S2 = 1, 
0, 0 the output Y = D[, etc. That is, if the binary value at the 
data select input is i, the output is given by Y = Dy. 
A typical application of the l-of-8 multiplexer is in the 
selection of one out of eight logical conditions within a digital 
system. Figure 2.90 demonstrates how the 1 -of-8 multiplexer 
might be used in conjunction with a computer's flag register 
to select one of eight logical conditions. We cover registers in 
the next chapter—all we need know at this points that a regis-
ter is a storage unit that holds the value of 1 or more bits. 
The flag register in Fig. 2.90 stores the value of up to eight 
so-called flags or marker bits. When a computer performs an 
operation (such as addition or subtraction) it sets a zero flag 
if the result was zero, a negative flag if the result was negative, 
and so on. These flags define the state of the computer. In 
Fig. 2.90 the eight flag bits are connected to the eight inputs of 
the multiplexer. The 3-bit code on S0 to S2 determines which 
flag bit is routed to the multiplexer's Y output. This code 
might be derived from the instruction that the computer is 
currently executing. That is, the bits of the instruction can be 
used to select a particular flag (via the multiplexer) and the 
state of this flag bit used to determine what happens next. 
Suppose a computer instruction has the form I F x = 0 
THEN do something. The computer compares x with 0, which 
sets the zero flag if x is equal to zero. The bits that encode this 
instruction provide the code on S0 to S2 that routes the Z flag to 
the Y output. Finally, the computer uses the value of the Y output 
to 'do something' or not to 'do something'. Later we shall see how 
alternative courses of action are implemented by a computer. 
2.6.2 The demultiplexer 
The inverse function of the multiplexer is the demultiplexer, 
which converts a binary code on n inputs into an asserted 
*• Y output 
Three-bit select 
input connects one 
of eight inputs to 
the Y output 
Figure 2.88 The 74157 quad two-input multiplexer. 
Figure 2.89 The l-of-8 multiplexer. 
Inputs 
Multiplexer 
Outputs 
1A 
J > 
A 
IB 
> j.« 
riwiichl 
* Y 1 
2A 
»• | ' ; T " t ^ l 
2B 
• I •lr_-t1witchj 
3 A 
" I * • • - • " ! _ _ _ ^ 
• 
Y 3 
« 
!~ r____r_^_.| 
„ Y4 
4 B 
• 1*T1..„J switch) 
SELECT 
r—* 
^ 
? 
 
STROBE 
^
^ 
T 
* 
The SELECT inputs determines 
Multiplexer 
Do 
J A ~ ^ 
D-f 
»B . •* 
D2 
• C • 
^ N . 
D3 
• D • 
!
\ 
D4 
• E . 
^ 
Ds 
> F • 
D6 
• C • 
D7 
» H - 
I 
Eight inputs 
s0 S, S 2 ^ - - ^ 
Y2 

2.6 Special-purpost logic elements 
85 
Flag register 
Z 
N 
C 
The flag register contains _ 
eight status bits such as the 
zero, carry, and negative 
The 3-bit code from 
the select register 
determines which flag 
bit appears at the output. 
Figure 2.90 The 1-of-8 multiplexer. 
8-way multiplexer 
Do 
Di 
D2 
D3 
D4 
D5 
D6 
Select 
So ST 
SZ 
- • Output 
o-
D-
For each value on C, B, A 
from 000 to 111, one of 
• Y4 . Y0 to Y7 goes high 
o-
Figure 2.91 The demultiplexer (3-line to 8-line decoder). 
level on one of 2" outputs. The demulti-
plexer circuit of Fig. 2.91 has three inputs 
A, B, and C and eight outputs Y0 to Y7. 
The three inverters generate the comple-
ments of the inputs A, B, and C. Each of 
the eight AND gates is connected to three 
of the six lines A, A, B, B, C, C (each of the 
three variables must appear in either its 
true or complemented forms). 
The output of first gate, Y„, is A-B-C 
and is 1 if all inputs to the AND gates are 
1 (i.e. A = 1, B = 1, C = 1). Therefore, 
Y0 is 1 when A = 0, B = 0, C = 0. If you 
examine the other AND gates, you will 
see that each gate is enabled by one of the 
eight possible combinations of A, B, C. 
This circuit is called a 3-line to 8-line 
demultiplexer, because it converts a 3-bit 
binary value, A, B, C, into one of 23 = 8 
outputs. Table 2.26 provides a truth table 
for this circuit, which is also called a 
decoder because it can take, for example, 
the bits that define a computer instruc-
tion and decode it into individual 
instructions as Fig. 2.92 demonstrates. 
Let's look at an actual demultiplexer, 
the 74138 3-line to 8-line demultiplexer 
(Fig. 2.93). The 74138's eight outputs, Y0 
to Y7, are active-low and remain in a high 
state unless the corresponding input is 
selected. The device has three enable 
inputs, El, E2, E3, which must be 0, 0, 1 
respectively, for the chip to be selected. 
When the chip is selected, one (and only 
one) of the eight outputs is forced into a 
0 state by the 3-bit code at the select 
inputs, A, B, C. Remember that the 
74138's outputs are active-low. 
One application of this circuit is as a 
device selector. Suppose that a system has 
eight devices and only one can be active 
(in use) at any instant. If each device is 
enabled by a 0 at its input, the binary 
code applied to the 74138's C, B, A inputs 
will determine which device is selected 
(assumingthat the 74138 is enabled by 0, 
0,1 at its IT, E2, E3 enable inputs). 
The demultiplexer generates the 2" 
minterms of an n-bit function. Why? 
Because a three-variable function has 
eight minterms and the demultiplexer 
converts a 3-bit code into one of eight 
values. For example, if you present a 
Select register 
Y o l 
Yl 
y 2 
Y3 
I 
Y s . 
Y 6 i 
_ Y 7 . 
C 
B 
A 
flags 
A 
B 
C 
A 
B 
c 

86 
Chapter 2 Gates, circuits, and combinational logic 
Inputs 
Outputs 
A 
B 
c 
Yo 
Y, 
Yz 
Y3 
Y4 
Y5 
Y6 
Y, 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
1 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
1 
1 
0 
0 
0 
1 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
1 
0 
1 
0 
0 
0 
0 
0 
1 
0 
0 
1 
1 
0 
0 
0 
0 
0 
0 
0 
1 
0 
1 
1 
1 
0 
0 
0 
0 
0 
0 
0 
1 
Table 2.26 Truth table for a 3-line demultiplexer. 
Instruction register 
Op-code 
~n 
The 3-bit op-code 
in the instruction register 
is decoded in to one of 
eight actions. 
Decoded instruction 
{^)-+ Add 
_J—+ 
Subtract 
; ~J—+- Load 
jy~• 
Store 
' 
J—• 
Branch on zero 
o 
Branch on not zero 
j — • 
Branch unconditonally 
~ ~ V > Stop 
Figure 2.92 Application of a demultiplexer as an instruction 
decoder. 
74138 3-line to 8-line 
demultiplexer 
_ Eight active-low 
4 outputs 
74138 with the code 101 (representing C-B-A), output Y5 will 
be asserted low. 
By ORing together the appropriate minterms we can gen-
erate an arbitrary sum of products expression in n variables. 
In other words, any function can be implemented by a 
demultiplexer and OR gate. 
Figure 2.94 demonstrates how a 3-line to 8-line decoder 
can be used to implement a full-adder that adds three bits 
to generate a sum and a carry. Chapter 4 discusses binary 
arithmetic and adders—all we need say here is that the sum of 
bits A, B, and Cin is given by the Boolean expression Cjn • A • B 
+ Cin- A B + CinA_B + Cln- A-Band the carry by C i nB- A 
+ C i n B A + C i n A B + C l n A B . 
Note that the outputs of the 74LS138 are active-low and 
therefore it is necessary to employ a NAND gate to generate 
the required sum-of-products expression. 
Another application of the demultiplexer is in decoding 
binary characters. Consider the ISO/ASCII character code (to 
be described in Chapter 4) which represents the alpha-
numeric characters (A-Z, 0-9, and symbols such as !, @, #, $, 
% ...) together with certain non-printing symbols such 
as the back space and carriage return. The ASCII codes for 
some of these non-printing control codes are given in 
Table 2.27. 
Suppose we receive an ASCII code from a keyboard 
and wish to decode its function in hardware. First note 
that all the codes of interest start with 00001. We can use 
the most-significant five bits to enable a 74LS138 3-line 
to 8-line decoder and then decode the three least-significant 
bits of the word OOOOldjdjdo to distinguish between the 
control codes. Figure 2.95 demonstrates how this is 
achieved. Each output from die decoder can be fed to a cir-
cuit to perform the appropriate action (e.g. carriage 
return). 
Medium-scale logic devices like the 74138 make it easy to 
design circuits with just a handful of chips. However, many 
Figure 2.93 The 74138 3-line to 8-line decoder. 
Control 
inputs 
0-*-C 
Enab'eo-K: 
inputs 
D 
• Y2 
3 
* 
Y4 01 
D 
• Y^ 
F1 
E3 
A 
B 
C 
0 
0 
1 
Tl 
Tl 
5 

2.7 Tri-state logic 
87 
M133 3-lhclo 
8-line 
iei:i-.:l:iple>er 
• i ! 
msi.ls 
The circuit adds bits 
A + B + Cinto 
generate a sum and 
a carry out 
Eight rninterms 
Carry 
Sum 
Figure 2.94 Generating a 
logic function with a 
demultiplexer. 
Mnemonic 
Name 
Back space 
Value 
BS 
Name 
Back space 
00001000 
LF 
Line feed 
00001010 
CR 
Carriage return 
00001101 
HT 
Horizontal tabulate 
00001001 
VT 
Vertical tabulate 
00001011 
Table 2.27 ASCII control characters. 
Bits d7 to d 3 must be 
' 00001 and bits 
d 0to d2 are decoded 
into one of eight control 
values 
74138 3-line to 
8-line 
demultiplexer 
Figure 2.95 Decoding ASCII control characters with a demultiplexer. 
circuits are now constructed from special-purpose user-pro-
grammable logic elements. Indeed, today's very low cost sin-
gle-chip microprocessors sometimes make it feasible to 
program the microprocessor to carry out the required logic 
function. These microprocessors are called microcontrollers 
to distinguish them from their more powerful relatives in PCs 
and workstations. 
2.7 Tri-state logic 
The logic elements we introduced at the beginning of this 
chapter are used to create functional units in which one or 
more logical outputs are generated from 
several inputs. A computer is composed 
of the interconnection of such func-
tional units together with the storage 
elements (registers) to be described in 
Chapter 3. We now examine a special 
type of gate that enables the various 
functional units of a computer to be 
interconnected. This new gate can be any 
of the gates we've already described—it's 
not the gate's logical function that's dif-
ferent, it's the behavior of its output. A 
logic element with a tri-state output has 
the special property that the output can 
be in a 0 state, a 1 state, or an unconnected 
state (hence the term tri-state). Before 
we can explain the operation of tri-state 
gates, we have to introduce the reason for 
their existence—the bus. 
Decoded control 
characters 
d 7 
d 6 
d 5 
d 4 
d3 
d2 
<*, 
d 0 
1 
— • BS 
— • HT 
—*• LF 
—*• VT 
— • CR 
• El 
Y o " 
•E2 
Y J -
• c 
! ! 
Y_T 
• A 
! i " 
Y7-
-•C^BA 
+ 
CJ3A 
+ 
C^BA 
-» C|„BA 
- • 
C i n B A 
-»C~BA 
+ C^A 
• 
A 
-
B 
-
C i n -
0 
• £! 
0 
• K 
1 
• ES 
ASCII code 
CinBA 

88 
Chapter 2 Gates, circuits, and combinational logic 
2.7.1 Buses 
A computer is like a city. Just as roads link homes, shops, and 
factories, buses link processing units, storage devices, and 
interfaces. Figure 2.96 shows a digital system composed of 
five functional units, A, B, C, D, and E. These units are linked 
together by means of two data highways (or buses), P and Q, 
permitting data to be moved from one unit to another. Data 
can flow onto the bus from a device connected to it and off 
Qbus-
Pbus 
o 
Logical unit 
or storage dev it t-
Figure 2.96 Functional units and buses. 
m-bit data bus 
Figure 2.97 Connecting systems to the bus. 
the bus to any other device. Buses may be unidirectional (i.e. 
data always flows the same way) or bidirectional (i.e. data can 
flow in two directions—but not simultaneously). 
A bus is normally represented diagrammatically by a single 
thick line or a wide shaded line as in Fig. 2.96. Real buses are 
composed of several individual wires (i.e. electrical connec-
tions). Modern computer buses have 100 or more lines, 
because a bus has to carry data, addresses, control signals, and 
even the power supply. Indeed the nature of a bus can be an 
important factor in the choice of a computer (consider the 
PC with its USB, and PCI buses). 
Figure 2.97 demonstrates how a bus is arranged. Logical 
units A and B are connected to an m-bit data bus and can 
transmit data to the bus or receive data from it. We are not 
concerned with the nature of the processes A and B here, but 
simply wish to show how they communicate with each other 
via the bus. For clarity, the connections to only one line of the 
bus are shown. Similar arrangements exist for bits dj to dm_,. 
Suppose unit A wishes to send data to unit B. The system in 
unit A puts data on the bus via gate A„ut and B receives the 
data from the bus via gate Bin. These two gates look like 
inverters but they aren't because they don't have bubbles at 
their output. Such a gate is called a buffer and it just copies the 
signal at its input terminal to its output terminal (i.e. the gate 
doesn't change the state of the data passing through it). We 
will soon see why such a gate is needed. 
Such an arrangement is, in fact, unworkable and a glance at 
Fig. 2.98 shows why. In Fig. 2.98(a) the outputs of two AND 
gates are connected together. Figure 2.98(b) shows the same 
circuit as Fig. 2.98(a) except that we've 
included the internal organization of 
the two gates. Essentially, a gate's out-
put circuit consists of two electronic 
switches that can connect the output to 
the + 5 V power supply or to the 0 V (i.e. 
ground) power supply. These switches 
are transistors that are either conduct-
ing or non-conducting. Because only 
one switch is closed at a time, the out-
put of a gate is always connected either 
to + 5 V or to ground. 
In Figure 2.98(b) the output from 
gate Gl is in a logical 1 state and is 
pulled up towards + 5 V by a switch 
inside the gate. Similarly, the output 
from G2 is a logical 0 state and is pulled 
down towards 0 V. Because the two out-
puts are wired together and yet their 
states differ, two problems exist. The 
first is philosophical. The logical level at 
all points along a conductor is constant, 
because the voltage along the conduc-
tor is constant. Because the two ends of 
C,l 
Data path between 
functional unit and 
system bus (only one 
bit shown) 

2.7 Tri-state logic 
8 9 
Bus 
— I I 
Output = 5 V 
Bus 
Gate C2 = logical 0 
o 
+ S V 
1} 
Switch ' 
/ 
open 
f ' 
] \ 
[ 
Output = 
Switch j 
i 
closed | 
i 
= ov 
0 V 
(a) Logical arrangement 
Two outputs connected together 
(b) Physical arrangement 
Two outputs connected together 
Figure 2.98 Connecting two 
outputs together. 
-+5V 
_^. Logical 0 
output 
.0 V 
+5 V 
^. Logical 1 
output 
0 V 
. +5 V 
/ 
. Floating 
output 
.0 V 
(a) Lower switch closed. 
Output connected to ground 
(b) Upper switch closed. 
Output connected to +5V 
(c) Both switches open. 
Output disconnected 
Figure 2.99 The operation of 
the tri-state output. 
the bus in Fig. 2.98(b) are connected to different voltages, the 
logical level on the conductor is undefined and breaks one of 
the rules of Boolean algebra. We have stated that in a Boolean 
system there is no such thing as a valid indeterminate state 
lying between a logical 1 and a logical 0. Secondly, and more 
practically, a direct physical path exists between the + 5 V 
power supply and ground (0 V). This path represents is a 
short circuit and the current flowing through the two output 
circuits could even destroy the gates. 
The tri-state gate lets you connect outputs together. Tri-
state logic is not, as its name might suggest, an extension of 
Boolean algebra into ternary or three-valued logic. It is a 
method of resolving the conflict that arises when two outputs 
are connected as in Fig. 2.98. Tri-state logic disconnects from 
the bus all those gates not actively engaged in transmitting 
data. In other words, a lot of tri-state outputs may be wired to 
a bus, but only one of them may be actively connected to the 
bus internally. We shouldn't speak of tri-state logic or tri-
state gates, we should speak of (conventional) gates with 
tri-state outputs. 
Figure 2.99 illustrates the operation of a gate with a tri-state 
enable output. In fact, any type of gate can have a tri-state 
output. All tri-state gates have a special ENABLE input. When 
ENABLE = 1. the gate behaves normally and its output is 
either a logical 1 or a logical 0 depending on its input 
(Fig. 2.99(a) shows a 0 state and Fig. 2.99(b) a 1 state). 
When ENABLE = 0, both switches in the output circuit of 
the gate are open and the output is physically disconnected 
from the gate's internal circuitry (Fig. 2.99(c)). If I were to ask 
what state the output is in when ENABLE = 0, the answer 
should be that the question is meaningless. In fact, because 
the output of an un-enabled tri-state gate is normally 
Gated = logical 1 
Switch 
| 
closed 
Switch 
/ 
open 
/ 
1 
0 V 

90 
Chapter 2 Gates, circuits, and combinational logic 
connected to a bus, the logic level at the output terminal is the 
same as that on the bus to which it is connected. For this rea-
son, the output of a tri-state gate in its third state is said to be 
floating. It floats up and down with the bus traffic. 
Most practical tri-state gates do, in fact, have active-low 
enable inputs rather than active-high enable inputs. 
Figure 2.100 provides the circuit symbols for four tri-state 
buffers, two of which are inverting buffers (i.e., NOT gates) 
and two of which are non-inverting buffers. Two of these 
gates have active-low enable inputs and two have active-high 
enable inputs. The truth table of an inverter with a tri-state 
output is given in Table 2.28. 
Figure 2.101 demonstrates how tri-state buffers imple-
ment a bused structure. The buffers connect or disconnect 
the three networks A, B, and C, to the bus. The outputs of net-
works A, B, and C are placed on the bus by three tri-state 
buffers Ao, Bo, and Co, which are enabled by signals EAo, EBo, 
and ECo, respectively. If any network wishes to put data on to 
the bus it sets its enable signal (e.g. EBo) to a 1. It is vital that 
no more than one of EAo, EBo, and ECo be at a 1 level at any 
instant. 
Each of the networks receives data from the bus via its own 
input buffers (Ai, Bi, and Ci). If a network wishes to receive 
(a) Non-inverting buffer. 
Active-high enable 
(b) Inverting buffer. 
Active-high enable 
(c) Non-inverting buffer. 
Active-low enable 
(d) Inverting buffer. 
Active-low enable 
Figure 2.100 Logic symbol for the tri-state buffer. 
ENABLE 
Input 
Output 
0 
0 
X 
Output floating 
0 
1 
X 
Output floating 
1 
0 
0 
Output same as input 
1 
1 
1 
Output same as input 
Table 2.28 Truth table for the non-inverting tri-state 
buffer with an active-high enable input. 
data, it enables its input buffer by asserting one of EAi, EBi, or 
ECj, as appropriate. For example, if network C wishes to trans-
mit data to network A, all that is necessary is for Eco and EAI 
to be set to a logical 1 simultaneously. All other enable signals 
remain in a logical 0 state for the duration of the information 
transfer. 
Input buffers (Ai, Bi, Ci) are not always necessary. If the 
data flowing from the bus into a network goes only into the 
input of one or more gates, a buffer is not needed. If however, 
the input data is placed on an internal bus (local to the net-
work) on which other gates may put their output, the buffer 
is necessary to avoid conflict between the various other out-
puts that may drive the local bus. 
The bus in Fig. 2.101 is bidirectional; that is, data can flow 
onto the bus or off the bus. The pairs of buffers are arranged 
back to back (e.g. Ai and Ao) so that one buffer reads data 
from the bus and the other puts data on the bus—but not at 
the same time. 
In the description of the bused system in Fig. 2.101 the 
names of the gates and their control signals have been care-
fully chosen. Ao stands for Aou„ and Ai for Ain. This labels the 
gate and the direction in which it transfers data with respect to 
the network it is serving. Similarly, EAo stands for enable gate 
A out, and EAi for enable gate A in. By 
choosing consistent and meaningful 
names, the reading of circuit diagrams 
and their associated text is made easier. 
Further details of a bused system will 
be elaborated on in Chapter 3, and 
Chapter 7 on the structure of the CPU 
makes extensive use of buses in its 
description of how the CPU actually 
carries out basic computer operations. 
Digital Works supports tri-state 
buffers. The device palette provides a 
simple non-inverting tristate buffer with 
an active-high enable input. Figure 2.102 
shows a system with a single bus to 
which three tri-state buffers are con-
nected. One end of the bus is connected 
to an LED to show the state of the bus. 
Digital Works requires you to con-
nect a wire between two points so we've 
added a macro tag to the bottom of the 
bus to provide an anchor point (we don't use the macro tag 
for its normal purpose in this example). 
The input of each tri-state gate in Fig. 2.102 is connected to 
the interactive input tool that can be set to a 0 or a 1 by the 
hand tool. Similarly, the enable input of each gate is con-
nected to an interactive input tool. 
By clicking on the run icon and then using the hand tool to 
set the input and enable switches, we can investigate the oper-
ation of the tristate buffer. In Fig. 2.102 inputs 1 and 3 are set 
X 
. P 
P 
P _ 
P _ 
- P 
P 

2.8 Programmable logic 
91 
d0 
d l 
dm-1 
Figure 2.101 Interconnecting logic elements with a bus and tri-state buffers. 
to 1 and only buffer 3 is enabled. Consequently, the output of 
buffer 3 is placed on the bus and the bus LED is illuminated. 
We have stated that you shouldn't enable two or more of 
the tri-state gates at the same time. If you did, that would cre-
ate bus contention as two devices attempted to put data on 
the bus simultaneously. In Fig. 2.103 we have done just that 
and used the hand tool to enable buffer 2 as well as buffer 3. 
As you can see, the simulation has stopped (the run button is 
in the off state) and an error message has been generated at 
the buffer we've attempted to enable. 
2.8 Programmable logic 
In this short section we introduce some of the single-chip 
programmable logic elements that can be configured by the 
user to perform any function they require. In the earlier days 
of logic design, systems were constructed with lots of basic 
logic elements; for example, the two-input OR gate, the five-
input NAND gate, and so on. The introduction of medium 
scale integration by the major semiconductor manufacturers 
generated a range of basic building blocks from multiplexers 
to digital multiplier circuits and allowed the economic design 
of more complex systems. We now introduce the next step 
in the history of digital systems—programmable logic that can 
be configured by the user. 
2.8.1 The read-only memory as a logic 
element 
Semiconductor manufactures find it easier to design regular 
circuits with repeated circuit elements than special-purpose 
highly complex systems. A typical regular circuit is the read 
only memory or ROM. We deal with memory in a later 
chapter. All we need say here is that a ROM is a device with n 
address input lines specifying 2" unique locations within it. 
Each location, when accessed, produces an m-bit value on its 
m output lines. It is called read only because the output corre-
sponding to a given input cannot be modified (i.e. written 
into) by the user. A ROM is specified by its number of locations 
— > 
Ao^> 
1> 
fr —c 
<j 
^TBi ——^— 
y 
< 
1» 
B o ^ 
• 
, 
, % 
, 
•'"• 
.o-.-.. 1^11 
~ ^ C i 
" — " ' ^ 
g 
* 
— * 
Co^> 
& 
EAi i 
EAo I 
EBi I 
EBOI 
ECi 
ECo 
C 
B 
A 

^^^^^i^^ii^M^^^^: 
File 
Edit 
Circuit 
View 
Tools 
Help 
D & O • 
S 
D 
I> 
D 
E> l>° 
D 
[> (.VJ^EI EU GO [m] |j 
O 
B 
: = 
• 
lei 
E> 
O 
DO m> fe ^ ) 
Figure 2.102 Using tri-state buffers in Digital Works. 
^0i^^^)i^:^'-:S^^0^^jd^^; 
hie 
bait 
Circuit 
View 
Tools 
Help 
D ^ S 
: :. 3 
E> I > ) D i > e * = D 
O - C f - S U E H I I S I M l l 
O S 
t> 
o 
DO BO 
tj 
^5 
mi 
a 
* 
+ 
Input 1 l l -
Enabie 1 El-
Input 2 
Enable 3 ® -
Bus 
Buffer 1 
Buffer 2 
- ! > 
Enable 2 111 
1 Bus contention, process 
halted 
7 yroiren 
Input 3 I I 
£ * > 
Figure 2.103 Attempting to enable two tri-state drivers simultaneously. 
Icon for non-inverting 
tri-state buffer. 
-*» 
Tri-state buffer. 
Bus (to which 
three tri-state 
buffers are 
connected). 
: : : : : : : : : : : : : : : : : : : Buffer i : ; : . 
• Input 1 I I 
P r ^ ^ Z 
'.'.'.'.'. 
Enable i E H — — — 
; : ; ; : : : : : 
Buffer 2 
• • 
• • Input 2 • H 
£ j > 
: : Enable 2 E H — — — ' 
: : : : : : : : : 
Buffer 3. . . 
: : input3:a: : : ; : : : ; [ ^ > : : ; ; ; ; ; 
Enable 3 H 
• 
' 
Bus ; 

2.8 Programmable logic 
93 
x width of each location; for example, a 16 X 4 ROM has 
16 locations each containing 4 bits. 
An alternative approach to the design of digital systems 
with basic gates or MSI elements is to use ROMs to imple-
ment the required function as a look-up table. Figure 2.104 
shows how a 16 X 4 ROM implements the 4-bit multiplier 
we designed earlier in this chapter using AND, OR, and NOT 
gates. The binary code, X„ XQ, Y„ Y0, at the four address 
inputs selects one of the 16 possible locations, each contain-
ing a 4-bit word corresponding to the desired result. The 
manufacturer or user of the ROM writes the appropriate out-
put into each of these 16 locations; for example, the location 
1011, corresponding to 10 X 11 (i.e. 2 X 3), has 0110 (i.e. 6) 
written into it. 
The ROM directly implements not the circuit but the truth 
table. The value of the output is stored for each of the possible 
inputs. The ROM look-up table doesn't even require Boolean 
algebra to simplify the sum-of-products expression derived 
from the truth table. Not only does a ROM look-up table save 
a large number of logic elements, but the ROMs themselves 
can be readily replaced to permit the logic functions to be 
modified (to correct errors or to add improved facilities). 
Unfortunately, the ROM look-up table is limited to about 20 
inputs and eight outputs (i.e. 220 X 8 = 8 Mbits). The ROM 
can be programmed during its manufacture or a PROM (pro-
grammable ROM) can be programmed by means of a special 
device. 
2.8.2 Programmable logic families 
Because ROM requires a very large number of bits to imple-
ment moderately complex digital circuits, semiconductor 
manufacturers have created much simpler logic elements 
than ROMs containing a regular structure of AND and OR 
gates that can be interconnected by the user to generate the 
required logical function. 
Figure 2.105 provides a simplified picture of how pro-
grammable logic devices operate. The three inputs on the 
left-hand side of the diagram are connected to six vertical 
lines (three lines for the inputs and three for their comple-
ments). On the right of the diagram are three two-input AND 
gates whose inputs run horizontally. The key to programma-
ble logic is the programmable link between each horizontal 
and vertical conductor. 
Fusible links between gates are broken by passing a suffi-
ciently large current through the link to melt it. By leaving a 
link intact or by blowing it, the outputs of the AND gates can 
be determined by the designer. Modern programmable logic 
devices have electrically programmed links that can be made 
and un-made many times. 
A real programmable device has many more inputs vari-
ables than in Fig. 2.105 and the AND gates can have an input 
for each of the variables and their complements. The digital 
designer selects the appropriate programmable device from a 
manufacturer's catalogue and adapts the Boolean equations 
to fit the type of gates on the chip. The engineer then plugs 
the chip into a special programming machine that intercon-
nects the gates in the desired way. 
Programmable logic elements enable complex systems to be 
designed and implemented without requiring large numbers 
of chips. Without the present generation of programmable 
logic elements, many of the low-cost microcomputers would 
be much more bulky, consume more power, and cost consid-
erably more. 
Today's designers have several types of programmable logic 
element at their disposal; for example, the PAL (programmable 
ROM with 16 locations 
Address 
X n 
A% 
0000 
0000 
0001 
0010 
0011 
0100 
0101 
0110 
0111 
1000 
1001 
1010 
1011 
1100 
1101 
1110 
1111 
Input 
X n 
A% 
0000 
0000 
0001 
0010 
0011 
0100 
0101 
0110 
0111 
1000 
1001 
1010 
1011 
1100 
1101 
1110 
1111 
X 
X 1 
As 
0000 
0000 
0001 
0010 
0011 
0100 
0101 
0110 
0111 
1000 
1001 
1010 
1011 
1100 
1101 
1110 
1111 
X 1 
As 
0000 
0000 
0001 
0010 
0011 
0100 
0101 
0110 
0111 
1000 
1001 
1010 
1011 
1100 
1101 
1110 
1111 
Yr 
% 
0000 
0000 
0001 
0010 
0011 
0100 
0101 
0110 
0111 
1000 
1001 
1010 
1011 
1100 
1101 
1110 
1111 
Input Yr 
% 
0001 
0000 
0001 
0010 
0011 
0100 
0101 
0110 
0111 
1000 
1001 
1010 
1011 
1100 
1101 
1110 
1111 
Y 
Y 
% 
0010 
0000 
0001 
0010 
0011 
0100 
0101 
0110 
0111 
1000 
1001 
1010 
1011 
1100 
1101 
1110 
1111 
Y 
Y 
% 
0011 
0000 
0001 
0010 
0011 
0100 
0101 
0110 
0111 
1000 
1001 
1010 
1011 
1100 
1101 
1110 
1111 
T1 
Address input 
Data output 
0000 
0000 
0001 
0010 
0011 
0100 
0101 
0110 
0111 
1000 
1001 
1010 
1011 
1100 
1101 
1110 
1111 
T1 
Address input 
Data output 
0010 
0000 
0001 
0010 
0011 
0100 
0101 
0110 
0111 
1000 
1001 
1010 
1011 
1100 
1101 
1110 
1111 
T1 
Address input 
Data output 
0100 
0000 
0001 
0010 
0011 
0100 
0101 
0110 
0111 
1000 
1001 
1010 
1011 
1100 
1101 
1110 
1111 
T1 
Address input 
Data output 
0110 
0000 
0001 
0010 
0011 
0100 
0101 
0110 
0111 
1000 
1001 
1010 
1011 
1100 
1101 
1110 
1111 
T1 
Address input 
Data output 
0000 
0000 
0001 
0010 
0011 
0100 
0101 
0110 
0111 
1000 
1001 
1010 
1011 
1100 
1101 
1110 
1111 
T1 
Address input 
Data output 
0011 
0000 
0001 
0010 
0011 
0100 
0101 
0110 
0111 
1000 
1001 
1010 
1011 
1100 
1101 
1110 
1111 
T1 
Address input 
Data output 
0110 
0000 
0001 
0010 
0011 
0100 
0101 
0110 
0111 
1000 
1001 
1010 
1011 
1100 
1101 
1110 
1111 
T1 
Address input 
Data output 
1001 
0000 
0001 
0010 
0011 
0100 
0101 
0110 
0111 
1000 
1001 
1010 
1011 
1100 
1101 
1110 
1111 
T1 
Address input 
Data output 
D 3 
D 2 
D i 
D o 
T1 
Address input 
Data output 
D 3 
D 2 
* 
z o 
T1 
Address input 
Data output 
D 3 
w *-1 
4-bit product 
T1 
Address input 
Data output 
* 
Z2 
Z 
Figure 2.104 Using a ROM 
T1 
Address input 
Data output 
* 
<-3 
implement a multiplier. 

94 
Chapter 2 Gates, circuits, and combinational logic 
Input 1 
Input 2 
Input 3 
0 
1 2 
3 
0 -
1 -
-£>^ 
4>^ 
•O-
) 
» Output ' 
Output 2 
Output 3 
Figure 2.105 Conceptual 
A connection can be made 
s t r u c t u r e o f a programmable 
3t oarh hnri7nnr3l anil 
• 
*-* 
logic device. 
'at each horizontal and 
vertical crosspoint 
array logic), the PLA (programmable logic array), and the 
PROM (programmable read-only memory). The PROM and 
the PAL are special cases of the PLA. The difference between 
the various types of programmable logic element depends 
on whether one or both of the AND and OR arrays are 
programmable. 
Programmable Logic Array 
The programmable logic array (PLA) was one of the first field 
programmable logic elements to become widely available. It 
has an AND-OR gate structure with a programmable array of 
AND gates whose inputs may be variables, their comple-
ments, or don't care states. The OR gates are also program-
mable, which means that you can define each output as the 
sum of any of the product terms. A typical PLA has 48 AND 
gates (i.e. 48 product terms) for 16 input variables, compared 
with the 65 536 required by a 16-input PROM. Figure 2.106 
provides a simple example of a PLA that has been pro-
grammed to generate three outputs (no real PLA is this 
simple). Because thePLAhasa program mable address decoder 
implemented by the AND gates, you can create product terms 
containing between one and n variables. 
Programmable array logic 
A more recent programmable logic element is the program-
mable array logic (PAL), which is not to be confused with the 
PLA we discussed earlier. The PAL falls between the simple 
gate array that contains only programmable AND gates and 
the more complex programmed logic array. The PLA has 
both programmable AND and OR arrays, whereas the PAL 
has a programmable AND array but a fixed OR array. In 
short, the PAL is an AND gate array whose outputs are ORed 
together in a way determined by the device's programming. 
Consider a hypothetical PAL with three inputs XQ to x2 and 
three outputs y0 to y2. Assume that inputs XQ to x2, generate six 
product terms P0 to P5. These product terms are, of course, 
user programmable and may include an input variable in a 
true, complement, or don't care form. In other words, you 
can generate any six product terms you want. 
The six product terms are applied to three two-input OR 
gates to generate the outputs y0 to y2 (Fig. 2.107). Each output 
is the logical OR of two product terms. Thus, y0 = P0 + P^ 
yj = P2 + P3, and y2 = P4 + P5. We have chosen to OR three 
pairs of products. We could have chosen three triplets so that 
y0 = P! + P2 + P3, yi = P4 + P5 + P6> etc. In other words, 
the way in which the product terms are ORed together is a 
function of the device and is not programmable by the user. 
2.8.3 Modern programmable logic 
Over the years, logic systems have evolved. Once the designer 
was stuck with basic gates and MSI building blocks. The 
1980s were the era of the programmable logic element with 
PROMs, PALs, PLAs, and so on. Today's programmable logic 
elements are constructed on a much grander scale. Typical 
programmable logic devices extend the principles of the PLA 
2 
3 
4 
5 
4 — 
5 — 

2.8 Programmable logic 
95 
The inputs are 
used to generate 
user-programmable 
product terms 
These OR gates combine 
product terms to generate 
user-programmable sum 
terms 
Outputs 
Figure 2.106 Example of a circuit built with a PLA. 
nputs 
Y 
"*Yo 
->Y 2 
- • Y 3 
Outputs 
t is, 
' 
r^o 
"*Yo 
->Y 2 
- • Y 3 
Outputs 
t is, 
L>° 
xo 
"*Yo 
->Y 2 
- • Y 3 
Outputs 
t is, 
I 
No 
'M 
"*Yo 
->Y 2 
- • Y 3 
Outputs 
t is, 
X, 
1 
| 
I 1 
, 
x 1 
"*Yo 
->Y 2 
- • Y 3 
Outputs 
t is, 
h-n 
1 1 J 1 
A 2 
"*Yo 
->Y 2 
- • Y 3 
Outputs 
t is, 
l>° 
c P-
X 2 
Each input used to 
generate a product 
term can be open 
or closed 
"*Yo 
->Y 2 
- • Y 3 
Outputs 
t is, 
I A r r A k r 
X 2 
Each input used to 
generate a product 
term can be open 
or closed 
"*Yo 
->Y 2 
- • Y 3 
Outputs 
t is, 
Programmable arra> 
)y- "*Yo 
->Y 2 
- • Y 3 
Outputs 
t is, 
«£—'•' 
"*Yo 
->Y 2 
- • Y 3 
Outputs 
t is, 
) > -
"*Yo 
->Y 2 
- • Y 3 
Outputs 
t is, 
( 
Fixed array 
"*Yo 
->Y 2 
- • Y 3 
Outputs 
t is, 
( 
" ) 
TV-
"*Yo 
->Y 2 
- • Y 3 
Outputs 
t is, 
Po 
F i 
\ 
p s 
F >4 
These 
you c 
i OR gates are fixed; the 
annot program them 
"*Yo 
->Y 2 
- • Y 3 
Outputs 
t is, 
Figure 2.107 Structure of 
the PAL. 
and employ macro cells that implement more complex build-
ing blocks containing storage elements as well as AND, OR, 
and EOR gates. 
A more recent innovation in programmable logic is 
which can be programmed, erased, and reprogrammed. 
Reprogrammable logic elements represent a considerable 
saving at the design stage. Moreover, they can be used to con-
struct systems that can be reconfigured by downloading data 
the electrically programmable and erasable logic element from disk. 
Inputs 
X 0 
_ 
X, 
_ 
x 2 _ 
— X_0 
— 
xo 
— x, 
- x 2 
- Y 0 
• 
Y , 
P 0 
P, 
P2 
P 3 
• 
Y , 

96 
Chapter 2 Gates, circuits, and combinational logic 
Design techniques for modern logic 
There's little point in developing massively complex 
programmable logic elements if they can't easily be used. 
Although the Boolean algebra and logic construction meth-
ods we've described earlier in this chapter are perfectly good 
for simple circuits, more efficient design techniques and tools 
are needed for complex circuits. 
Device manufacturers have developed logic languages that 
run on PCs and make it possible to configure these program-
mable logic elements. You can express the required functions 
in Boolean form and the software will generate the data nec-
essary to program the device. 
Just as high-level languages have replaced assembly lan-
guage in computer programming, circuit designers use high-
level design languages. One such language is called VHDL 
(VHSIC hardware description language, where VHSIC is an 
acronym for very-high-speed integrated circuit), which per-
mits you to specify a digital circuit in a high-level abstract 
language. VHDL started out as a US Department of Defense 
project to specify complex circuits and evolved into a general-
purpose design tool. VHDL became an IEEE standard in 1987 
with the number IEEE 1076. 
A designer armed with VHDL can specify a circuit in 
VHDL code and then simulate the circuit's behavior on a PC 
(or a workstation under Unix). The software can even cope 
with the problems of delays in the circuit. Because the device 
can be simulated, the engineer is reasonably certain that 
the final circuit will work when it is constructed. This soft-
ware can even drive the devices that program these logic 
elements. 
About three decades ago, the engineer built digital circuits 
on breadboards with hundreds of small-scale and medium-
scale integrated circuits—and then spent weeks debugging 
the circuit. Today, the engineer can express complex logical 
operations in a high-level notation, design a circuit, simulate 
its behavior, and then program a real device knowing that it 
will probably work first time. 
The following fragment of VHDL code is 
taken from VHDL of Programmable Logic 
by Kevin Skahill (Addison Wesley, 1996) 
and demonstrates how a quad 4-bit multi-
plexer can be specified. This device has four 
4-bit inputs a to d and a 4-bit output x. A 
2-bit input s determines which of the four 
inputs is connected to the output. 
Readers who have programmed in 
almost any high-level language would 
probably be able to follow this fragment of 
VHDL. It consists of a declaration block 
that defines the inputs and outputs and a 
process block that defines what the circuit 
is to do. 
2.8.4 Testing digital circuits 
A significant part of the cost of a digital system is its testing. 
Why should testing be so expensive? After all, a system either 
works or it doesn't. If it doesn't work it can often be scrapped 
and replaced more economically than repairing it. 
Although it's easy to test a light bulb by plugging it into a 
socket, it's much more difficult to test all but the most primi-
tive of digital systems. Consider a small memory element 
with 10 address lines and eight data outputs (i.e. 1 kbyte). 
How many tests do we need to perform to verify that the 
memory is working correctly? Obviously the memory can be 
tested by writing a pattern into each of its 210 = 1024 loca-
tions and then reading the pattern back. That is, the test 
requires a total of 1024 read and 1024 write cycles. 
But wait a moment. How do we know that the memory will 
store every possible data pattern in each possible word loca-
tion? The test must be extended by writing all possible data 
values into a location before testing the next location. In this 
case there are 28 = 256 tests per location, or 28 X 2'° = 218 
tests altogether. 
At last we have now thoroughly tested the memory com-
ponent. No we have not! Some memories display a fault 
called pattern sensitivity in which writing data to one location 
affects the contents of another location. You can test for pat-
tern sensitivity by writing a data pattern to the location we 
wish to test and then filling all other locations with a different 
data pattern. We then reread the data in the location under 
test to see whether it has changed. So for each of our 218 tests, 
we must write a different pattern in all the other 210— 1 word 
cells. This gives us a total of 218 X 210 or 22S tests. If we were to 
consider a 64 Mbyte memory, it would require 28 X 226 X 
226 = 252 tests (a gigantic number). 
This example demonstrates that it's effectively impossible 
to test any reasonably complex digital system with external 
inputs and internal states. Even if tests could be carried out at 
a rate of over 100 million/s, most complex digital systems 
library ieee; 
use ieee.std_logic_1164.all; 
entity mux is port( 
a, b, c, d: 
in std_logic_vector(3 downto 0); 
s: 
in std_logic_vector(1 downto 0) ; 
x: 
out std_logic_vector(3 downto 0)); 
end mux; 
architecture archmux of mux is 
begin 
with s select 
x <= a when "00", 
b when "01", 
c when "10", 
d when others; 
end archmux; 

2.8 Programmable logic 
97 
(e.g. a microprocessor chip) would take longer to test than 
the anticipated life of the entire universe. A way out of this 
dilemma is to perform a test that provides a reasonable level 
of confidence in its ability to detect a large fraction of possible 
faults without requiring an excessive amount of time. 
The first step in devising such a test is to distinguish 
between the idea of a defect and a fault. A real system fails 
because of a defect in its manufacture. For example, a digital 
system may fail because of a defect at the component level (a 
crystal defect in a silicon chip), or at the system level (a solder 
splash joining together two adjacent tracks on a printed cir-
cuit board). The observed failure is termed A fault. 
Although there are an infinite number of possible defects 
that might cause a system to fail, their effects (i.e. faults) are 
relatively few. In simpler terms, an automobile may suffer 
from many defects, but many of these defects result in a 
single observable fault—the car doesn't move. That is, a fault 
is the observable effect due to a defect. A digital system can be 
described in terms of a fault model (i.e. the list of observable 
effects of defects). Typical faults are given below. 
Stuck-at-one The input or output of a circuit remains in a 
logical 1 state independently of all other circuit conditions. 
This is usually written s_a_l. 
Stuck-at-zero In this case the input or output is permanently 
stuck in a 0 state (i.e. s_a_0). 
Bridging faults Two inputs or outputs of a circuit are effect-
ively connected together and cannot assume independent 
logic levels. That is, they must both be Os or 1. 
It is possible to devise a longer list of fault models, but the 
stuck-at fault model is able to detect a surprisingly large 
(a) A simple three-gate digital circuit. 
Set high to propagate 
„ ^ , . , 
. . , 
A through gate1 8 
S e t h l8 h t o
c
 
S e t l o w ° H 
s 
5 
propagate E 
propagate H 
through gate 2 
through gate 3 
(b) Establishing a sensitive path between input A and output K. 
Figure 2.108 Using sensitive path analysis to test digital circuits. 
number of defects. In other words, if we test a system by 
considering all possible stuck-at-1 and stuck-at-0 faults, we 
are likely to detect almost all of the probable defects. 
The sensitive path test 
A sensitive path between an input and an output is con-
structed to make the output a function of the input being 
tested (i.e. the output is sensitive to a change in the input). 
Figure 2.108(a) illustrates a circuit with three gates and six 
inputs A, B, C, F, I and J. The sensitive path to be tested is 
between input A and output K. 
Figure 2.108(b) demonstrates how we have chosen the sen-
sitive path by ensuring that a change in input A is propagated 
through the circuit. By setting AND gate l's B and C inputs 
high, input A is propagated through this gate to the E input of 
AND gate 2. The second input of AND gate 2, F, must be set 
high to propagate E through gate 2. Output G of AND gate 2 
is connected to input H of the three-input OR gate 3. In this 
case, inputs I and / must be set low to propagate input H 
(i.e. A) through OR gate 3. 
By setting inputs B, C, F, I, and /to 1,1,1,0, and 0, the out-
put becomes K = A and, therefore, by setting A to 0 and then 
to 1, we can test the sensitive path between A and JCand deter-
mine whether any A_stuck_at fault exists. 
A fault-list can be prepared for the circuit, which, in this 
case, might consist of A s_a_0, A s_a_ 1, B s_a_0, B s_a_ 1 , . . . . 
A convenient notation for the fault list is A/0, A/1, B/0, 
B/l,... etc. The'/' is read as'stuck at'. 
To test for A s_a_0 (i.e. A/0), the other inputs are set to the 
values necessary to create a sensitive path and A is switched 
from 0 to 1. If the output changes state, A is not stuck at zero. 
The same test also detects A/1. 
Fault tests are designed by engineers 
(possibly using CAD techniques) and can be 
implemented either manually or by means 
K 
of computer-controlled automatic test equip-
ment (ATE). This equipment sets up the 
appropriate input signals and tests the out-
put against the expected value. We can spec-
ify the sensitive path for A in the circuit of 
Fig. 2.108(b) as B-C-F-1 J. 
It's not always possible to test digital cir-
cuits by this sensitive path analysis because 
of the topological properties of some digital 
X 
circuits. For example, a digital signal may 
take more than one route through a circuit 
and certain faults may lead to a situation in 
which an error is cancelled at a particular 
node. Similarly, it's possible to construct 
logic circuits that have an undetectable 
fault. Figure 2.109 provides an example of 
such a circuit. This type of undetectable 
A -
B -
C -
F -
I 
-
J -
O ^ D ^ 
G 
H 
| D _ E 
t 
F 
A 
B 
C 
I 
2 
s 

98 
Chapter 2 Gates, circuits, and combinational logic 
Node to tested 
1 uy-;± 
In order to establish a sensitive path for internal node D 
to external node H, it is necessary to set inputs G and F to 
OR gate 5 low. C is set low by setting inputs B and 
E to NAND gate 3 high. Input E is derived from NOT gate 
2 and is set high by setting input A low. Similarly, output F 
of NAND gate 4 is set low by setting inputs A and C to 
gate 4 high. Unfortunately, in order to set G and F low 
requires that input A be both 0 and 1 simultaneously. 
This condition is a contradiction and therefore node D 
Figure 2.109 Circuit with an 
undetectable fault. 
fault is due to redundancy in the circuit and can be eliminated 
by redesigning the circuit. Alternatively, a circuit can be made 
easier to test by connecting some of its internal nodes to pins 
so that they can be directly examined. 
• 
SUMMARY 
In this chapter we have looked at the basic set of logic elements 
used to create any digital system—the AND, OR, and NOT 
gates. We have demonstrated how simple functions can be 
generated from gates by first converting a problem in words into 
a truth table and then using either graphical or algebraic 
methods to convert the truth table into a logical expression and 
finally into a circuit made up of gates. At the end of this chapter 
we briefly mentioned the new families of programmable logic 
elements and their design tools that have revolutionized the 
creation of today's complex digital systems. 
We have introduced Digital Works, a design tool that enables 
you to create digital circuits and to observe their behavior. We 
also introduced the tri-state buffer, a device that enables you to 
connect logic subsystems to each other via a common data 
highway called a bus. 
In the next chapter we look at sequential circuits built from 
flip-flops. As the term sequential suggests, these circuits involve 
the time factor, because the logical state of a sequential device is 
determined by its current inputs and its past history (or 
behavior). Sequential circuits form the basis of counters and data 
storage devices. Once we have covered sequential circuits, we 
will have covered all the basic building blocks necessary to design 
a digital system of any complexity (e.g. the digital computer). 
• 
PROBLEMS 
2.1 Explain the meaning of the following terms. 
(a) Sum-of-products 
(b) Product of sums 
A B CD 
- > -=3> 
—E>°- I> 
O a>-
Figure 2.110 Circuit for Question 2.2. 
(c) Minterm 
(d) Truth table 
(e) Literal 
(f) Constant 
(g) Variable 
2.2 Tabulate the values of the variables P, Q, R, S, T, and U in the 
circuit of Fig. 2.110 for all possible input variables A, B, C, and D. 
The truth table for this question should be expressed in the form 
of Table 2.29. 
2.3 For the circuit of Fig. 2.110 in Question 2.2 obtain a 
Boolean expression for the output U, in terms of the inputs A, B, 
C, and D. You should obtain an expression for the output U by 
considering the logic function of each gate. 
2.4 For the truth table in Question 2.2 (Table 2.29) obtain a 
sum-of-minterms expression for U and use Boolean algebra to 
obtain a simplified sum-of-products expression for U. 
Contradiction 
A 
B 
C 
F 0 
ml 
.E 
— H 
1-*- 0 
0 
1 
1 
0 
2 
1 
i 
1 

2.8 Programmable Logic 
99 
Inputs 
Output 
A 
B C D 
0 
0 0 0 
0 
0 0 
1 
0 
0 
1 0 
0 
0 
1 1 
0 
1 0 
0 
0 
1 0 
1 
P = B K 
Q 
P A 
R - C - D 
S - B R 
T - B • D 
U - Q r S + T 
1 1 1 1 
Table 2.29 Truth table for Question 2.2. 
Q n> 
I 
|>o-
->X 
J> 
Figure 2.111 Circuit for Question 2.5. 
2.5 Use a truth table to obtain the relationship between 
outputs X and Y and the input variables A, B, and C for the circuit 
in Fig. 2.111. From the truth table write down Boolean 
expressions for X and Y. Derive expressions for X and Y by 
considering the Boolean equations of the gates. 
Demonstrate that the two results (i.e. those derived from the 
truth table and those derived from the Boolean equations) are 
equivalent by substituting literals (000,001, etc.) for A, B, and C 
in the Boolean equations. 
2.6 Draw logic diagrams, using AND, OR, and NOT gates only, to 
implement the following Boolean expressions. In each case draw 
the diagrams directly from the equations and do not attempt to 
simplify the expressions. 
(a) F = A-B + A-B 
(b) F = (A + B + C)(A-B + A-C) 
(c) F = (A + C)(A + B-D) 
(d) F = A + C-A + B-D 
(e) F = (A-B + A-B + A-C)(A-B + A-B + AC) 
2.7 Plot the following functions on a Karnaugh map. 
(a) F = A-B-C + A-B-C 
(b) F = A B C + A B C + A B C 
(c) F = A + A B + A C + A B C 
(d) F = A + B-A-C + D 
(e) F = A-B-C-D + A-B-C-D + B-D 
2.8 How would you plot the following expressions on a 
Karnaugh map? 
(a) (A + B + C)(D + B)(A + C) 
(b) (A-B + A-B + A-C)(A-D + A-B + A-C) 
2.9 Simplify the following expressions by means of Boolean 
algebra. That is, do not use Karnaugh maps. 
(a) A B C + A B C + A B C + A B C 
(b)A-B LC + A-B-C + A-B-C + A-B-C + A-B-C 
+A-B-C 
(c) A-B-C + A_-B-^ + A_-B-_C + A-B-C + A-B-C 
+ A-B-C + A-B-C + A-B-C 
2.10 Simplify the following expressions. 
(a) (A + B + C)(A + B + C) 
(b) (A-B + A-B + A-C)(A-B + A-B + A-C) 
(c) A + B + (A + C • D) (A + B • C) 
(d) A-B + B-C + A-B-C + A-B-C-D 
(e) (A + B)(A + B + C) (A + B + C) (A + B + C) 
2.11 Use de Morgan's theorem to complement the following 
expressions. Do not simplify the expressions either before or 
after you complement them. 
(a) B-C + B-C 
(b) B-C-D + B-C_ 
(c) B(C + D)+B-C 
(d) A-B(C-D + C-D) 
(e) AB(ALD_+C-D) 
(f) B C + B C ( A D + A D ) 
0 
0 
0 
1 
0 
1 
1 
1 
1 
1 
1 
1 
A • 
B • 
C • 

100 
Chapter 2 Gates, circuits, and combinational logic 
2.12 Convert the following expressions to sum-of-products form. 
(a) (A + B) (B 4- C) (A + C) 
(b) (C + D)(AB + A C ) ( A C + B) 
(c) (A + B + C) (A + C • D) (D + F) 
2.13 Simplify 
(a) A © B © C 
(b) A-B(C@A) 
2.14 Convert the following expressions to product-of-sums form. 
(a) A B + A B + B C 
(b) AB + BC + B C D 
(c) AB + AC + BC 
(d) A B C + A B C + A B C + A B C 
2.15 A circuit has four inputs, P, Q, R, and S, representing the 
natural binary numbers 0000 = 0, to 1111 = 15. P is the 
most-significant bit.The circuit has one output, X, which is true 
if the number represented by the input is divisible by three 
(regard zero as being indivisible by three.) Design a truth table 
for this circuit and hence obtain an expression for X in terms 
of P, Q, R, and S. Give the circuit diagram of an arrangement of 
AND, OR, and NOT gates to implement this circuit. Design a 
second circuit to implement this function using NAND 
gates only. 
2.16 A device accepts natural binary numbers in the range 
0000 to 1111 which represent 0 to 15. The output of the circuit 
is true if the input to the circuit represents a prime number and 
is false otherwise. Design a circuit using AND, OR, and NOT 
gates to carry out this function. A prime number is an integer 
that is greater than 1 and is divisible only by itself and 1. Zero 
and 1 are not prime numbers. 
2.17 Demonstrate how you would use a 4-line to 16-line 
demultiplexer to implement the system in Question 2.16. 
2.18 A logic circuit accepts a natural binary number DCBA in 
the range 0 to 15 (the least-significant bit is bit A). The output is 
the square of the input; for example, if DCBA = 01012 = 510, the 
output is 00010101; = 2510. Design a circuit to implement this 
function. 
2.19 A logic circuit has three inputs C, B, and A, where A is the 
least-significant bit.The circuit has three outputs R, Q, and P. For 
any binary code applied to the input terminals (A, B, and C) the 
output is given by the input plus 1; for example, if C, B,A = 0,1, 
1, the output R, Q, P is 1,0,0. Note that 111 + 1 = 000 (i.e. 
there is no carry out). Design a circuit to implement this system. 
2.20 A 4-bit binary number is applied to a circuit on four lines 
D, C, B, and A. The circuit has a single output, F, which is true if 
the number is in the range 3 to 12, inclusive. Draw a truth table 
for this problem and obtain a simplified expression for F in 
terms of the inputs. Implement the circuit 
(a) in terms of NAND gates only 
(b) in terms of NOR gates only 
2.21 A circuit has four inputs D, C, B, and A encoded in 8421 
natural binary form.The inputs in the range 0000z = 0 to 
10112= 11 represent the months of the year from January (0) 
to December (11). Inputs in the range 1100 to 1111 (i.e. 12 to 
15) cannot occur. The output of the circuit is a logical one if the 
month represented by the input has 31 days. Otherwise the 
output is false. The output for inputs in the range 1100 to 1111 
is undefined. 
(a) Draw a truth table to represent the problem and use it to 
construct a Karnaugh map. 
(b) Use the Karnaugh map to obtain a simplified 
expression for the function. 
(c) Construct a circuit to implement the function using AND, 
OR, and NOT gates. 
(d) Construct a circuit to implement the function using NAND 
gates only. 
2.22 A multiplexer has eight inputs Y0 to Y7 and a single output 
Z. A further three inputs A, B, and C (A = least-significant bit) 
determine which output the single input X is connected to. For 
example, if A, B, C = 110, the output Y6 = X and all other 
outputs are low. Design a circuit to implement this function. 
2.23 What is tri-state logic and why is it used in digital 
systems? 
2.24 Use Digital Works to construct a circuit that realizes the 
expression 
A B - C + A B C + A B C + A B - C 
Simplify the above expression and use Digital Works to 
construct a new circuit. Demonstrate that the two circuits are 
equivalent (by comparing their outputs for all inputs). 
2.25 Use Digital Works to construct the system of 
Question 2.20 and demonstrate that your system works. 

Sequential Logic 
INTRODUCTION 
We now introduce a new type of circuit that is constructed from devices that remembertheir 
previous inputs. The logic circuits in Chapter 2 were all built with combinational elements whose 
outputs are functions of their inputs only. Given a knowledge of a combinational circuit's inputs 
and its Boolean function, we can always calculate the state of its outputs. The output of a 
sequential circuit depends not only on its current inputs, but also on its previous inputs. Even if 
we know a sequential circuit's Boolean equations, we can't determine its output state without 
knowing its past history (i.e. its previous internal states). The basic building blocks of sequential 
circuits are the flip-flop, bistable, and latch just as the basic building block of the combinational 
circuit is the gate. 
It's not our intention to deal with sequential circuits at anything other than an introductory 
level, as their full treatment forms an entire branch of digital engineering. Sequential circuits can't 
be omitted from introductory texts on computer hardware because they are needed to implement 
registers, counters, and shifters, all of which are fundamental to the operation of the central 
processing unit. 
Figure 3.1 describes the conceptual organization of a sequential circuit. An input is applied 
to a combinational circuit using AND, OR, and NOT gates to generate an output that is fed to 
a memory circuit that holds the value of the output.The information held in this memory is 
called the internal state of the circuit. The sequential circuit uses its previous output together 
with its current input to generate the next output. This statement contains a very important 
implicit concept, the idea of a next state. Sequential circuits have a clock input, which triggers 
the transition from the current state to the next state. The counter is a good example of a 
sequential machine because it stores the current count that is updated to become the next 
count. We ourselves are state machines because our future behavior depends on our past 
CHAPTER MAP 
Logic elements and 
Boolean algebra 
We begin our introduction to the 
computer with the basic building 
block from which we construct 
alt computers, the gate. 
A combinational digital circuit 
such as an adder is composed 
of gates and its output is a 
Boolean (logical) function of 
its inputs only. 
3 Sequential logic 
The output of a sequential circuit 
is a function of both its current 
inputs and its past inputs; that is, 
a sequential circuit has memory. 
The building blocks used to 
construct devices that store data 
are called flip-flops. In this 
chapter we look at basic 
sequential elements and the 
counters, registers, and shifters 
that are constructed from 
flip-flops. 
4 Computer arithmetic 
Computer arithmetic concerns 
the representation of numbers in 
a computer and the arithmetic 
used by digital computers. We 
look at how decimal numbers are 
converted into binary form and 
the properties of binary numbers 
and we demonstrate how 
operations like addition and 
subtraction are carried out. We 
also look at how computers deal 
with negative numbers and 
fractional numbers. 
5 The instruction set 
architecture 
In this chapter we introduce the 
computer's instruction set 
architecture (ISA), which 
describes the low-level 
programmer's view of the 
computer. The ISA describe the 
type of operations a computer 
carries out. We are interested in 
three aspects of the ISA: the 
nature of the instructions, the 
resources used by the 
instructions (registers and 
memory), and the ways in which 
the instructions access data 
(addressing modes). The 68K 
microprocessor is used to 
illustrate the operation of a real 
device. 
1 i tiSmt 

102 
Chapter 3 Sequential logic 
Sequential logic circuit 
Input 
^> Output 
The combinational logic 
is composed of conventional 
AND, OR, and NOT gates 
The memory holds 
the previous output 
(i.e. state) and uses it 
to generate the next 
output 
Figure 3.1 The sequential circuit. 
WHAT IS SEQUENTIAL LOGIC? 
Sequential logic elements perform as many different functions as combinational logic 
elements; however, they do carry out certain well-defined functions, which have been 
given names. 
Latch A latch is a 1-bit memory element. You can capture a single bit in a latch at one instant 
and then use it later; for example, when adding numbers you can capture the carry-out in a 
latch and use it as a carry-in in the next calculation. 
Register The register is just m latches in a row and is able to store an m-bit word; that is, the 
register is a device that stores one memory word. A computer's memory is just a very large 
array of registers. 
Shift register A shift register is a special-purpose register that can move the bits of the word it 
holds left or right; for example the 8-bit word 00101001 can be shifted left to give 01010010. 
Counter A counter is another special-purpose register that holds an m-bit word. However, 
when a counter is triggered (i.e. clocked) its contents increase by 1; for example, if a counter 
holding the binary equivalent of 42 is clocked, it will hold the value 43. Counters can count up 
or down, by 1 or any other number, or they can count through any arbitrary sequence. 
State machines A state machine is a digital system that moves from one state to another 
each time it is triggered. You can regard a washing machine controller as a state machine 
that steps though all the processes involved in washing (at a rate depending on the load, 
the temperature, and its preselected functions). Ultimately, the computer itself is a nothing 
more than a state machine controlled by a program and its data. 
inputs—if you burn yourself getting something out of the oven, you approach the oven with 
more care next time. 
We begin our discussion of sequential circuits with the bistable or flip-flop. A bistable is so called 
because its output can remain in one of two stable states indefinitely, even if the input changes. 
For a particular input, the bistable's output may be high or low, the actual value depending on the 
Combinational logic 
Memory 

3.1 The RS flip-flop 
103 
previous inputs. Such a circuit remembers what has happened to it in the past and is therefore 
a form of memory element. A more detailed discussion of memory elements is given in 
Chapter 8. A bistable is the smallest possible memory cell and stores only a single bit of 
information.The term flip-flop, which is synonymous with bistable, gives the impression of the 
circuit going flip into one state and then flop into its complement. Bistables were constructed 
from electromagnetic relays that really did make a flip-flop sound as they jumped from one 
state into another. 
The term latch is also used to describe certain types of flip-flop. A latch is a flip-flop that is 
unclocked (i.e. its operation isn't synchronized with a timing signal called a clock).The RS 
flip-flop that we describe first can also be called a latch. 
Sequential systems can be divided into two classes: synchronous and asynchronous. 
Synchronous systems use a master clock to update the state of all flip-flops periodically. 
The speed of a synchronous system is determined by its slowest device and all signals must 
have settled to steady-state values by the time the system is clocked. In an asynchronous 
system a change in an input signal triggers a change in another circuit and this change ripples 
through the system (an asynchronous system is rather like a line of closely spaced dominoes 
on edge—when one falls it knocks its neighbor over and so on). Reliable asynchronous systems 
are harder to design than synchronous systems, although they are faster and consume less 
power. We will return to some of these topics later. 
We can approach flip-flops in two ways. One is to demonstrate what they do by defining 
their characteristics as an abstract model and then show how they are constructed. That is, we 
say this is a flip-flop and this is how it behaves—now let's see what it can do. The other way 
of approaching flip-flops is to demonstrate how they can be implemented with just two gates 
and then show how their special properties are put to work. We intend to follow the latter 
path. Some readers may prefer to skip ahead to the summary of flip-flops at the end of this 
section and then return when they have a global picture of the flip-flop. 
3.1 The RS flip-flop 
We begin our discussion of the flip-flop with the simplest 
member of the family, the RS flip-flop. Consider the circuit of 
Fig. 3.2. What differentiates this circuit from the combina-
tional circuits of Chapter 2 is that the gates are cross-coupled 
and the output of a gate is fed back to its input. Although 
Fig. 3.2 uses no more than two two-input NOR gates, its 
operation is not immediately apparent. 
The circuit has two inputs, A and B, and two outputs, X 
and Y. A truth table for the NOR gate is provided alongside 
Fig. 3.2 for reference. From the Boolean equations governing 
the NOR gates we can readily write down expressions for out-
puts X and Y in terms of inputs A and B. 
1. X = A + Y 
2. Y = W+~X 
If we substitute the value for Y from equation (2) in equation 
(1), we get 
By de Morgan's theorem 
Two negations cancel 
Expand the expression 
. X = A + B + X 
= A-B + X 
= A(B + X) 
= A-B + A X 
• X 
*• Y 
A 
B 
A + B 
0 
0 
1 
0 
1 
0 
1 
0 
0 
1 
1 
0 
Figure 3.2 Two cross-coupled NOR 
gates. 
A Gate"" 
I CM 
J 
• s 
IF) 
A — 
B -

104 
Chapter 3 Sequential logic 
Because Boolean algebra doesn't define the operations 
of division or subtraction we can't simplify this equation 
any further and are left with an expression in which the 
output is a function of the output; that is, the value of X 
depends on X. Equation (3) is correct but its meaning 
isn't obvious. We have to look for another way of analyz-
ing the behavior of cross-coupled gates. Perhaps a better 
approach to understanding this circuit is to assume a value 
for output X and for the inputs A and B and then see 
where it leads us. 
3.1.1 Analyzing a sequential circuit by 
assuming initial conditions 
Figure 3.3(a) shows the cross-coupled NOR gate circuit with 
the initial condition X = 1 and A = B = 0 and Fig. 3.3(b) 
shows the same circuit redrawn to emphasize the way in 
which data flows between the gates. 
Because the inputs to gate G2 are X = 1, B = 0, its output, 
Y = X + B, must be 0. The inputs to gate G, are Y = 0 and 
A = 0, so that its output, X, is Y + A, which is 1. Note that 
this situation is self-consistent. The output of gate G[ is X = 1, 
which is fed back to the input of gate G! to keep X in a logical 
1 state. That is, the output actually maintains itself. It should 
now be a little clearer why equation (3) has X on both sides 
(i.e.X = A B + A X ) . 
Had we assumed the initial state of X to be 0 and inputs 
A = B = 0, we could have proceeded as follows. The inputs 
to G2 are X = 0, B = 0 and therefore its output is 
Y = X + B = 0 + 0 = 1. The inputs to G) are Y = 1 and 
A = 0, and its output isX = Y + A = l + 0 = 0. Once 
more we can see that the circuit is self-consistent. The output 
can remain indefinitely in either a 0 or a 1 state for the inputs 
A = B = 0. 
The next step in the analysis of the circuit's behavior is to 
consider what happens if we change inputs A or B. Assume 
that the X output is initially in a logical 1 state. If input B to 
gate G2 goes high while input A remains low, the output of 
gate G2 (i.e. Y) is unaffected, because the output of a NOR 
gate is low if either of its inputs are high. As X is already high, 
the state of B has no effect on the state of Y. 
If now input A goes high while B remains low, the output, 
X, of gate G[ must fall to a logical 0 state. The inputs to gate G2 
are now both in logical 0 states and its output Y rises to a 
logical 1. However, because Y is fed back to the input of gate 
G,, the output X is maintained at a logical 0 even if A returns 
to a 0 state. 
The effect of setting A to a 1 causes output X to flip over 
from a 1 to a 0 and to remain in that state when A returns to 
a 0. We call an RS flip-flop a latch because of its ability to 
capture a signal. Table 3.1 provides a truth table for the circuit 
of Fig. 3.2. Two tables are presented—one appropriate to the 
circuit we have described and one with its inputs and outputs 
relabeled. 
Table 3.1(a) corresponds exactly to the two-NOR gate 
circuit of Fig. 3.2 and Table 3.1 (b) to the idealized form of this 
circuit that's called an RS flip-flop. There are two differences 
between Tables 3.1(a) and 3.1(b). Table 3.1(b) uses the 
conventional labeling of an RS flip-flop with inputs R and S 
and an output Q. The other difference is in the entry for the 
case in which A = B = 1 and R = S = 1. The effect of these 
differences will be dealt with later. 
We've already stated that Fig. 3.2 defines its output in terms 
of itself (i.e. X = A- B + A • X). The truth table gets round 
this problem by creating a new variable, X+ (or Q+), where 
X+ is the new output generated by the old output X and 
the current inputs A and B. We can write X+ = A • B + A • X. 
The input and output columns of the truth table are now not 
only separated in space (e.g. input on the left and output on 
the right) but also in time. The current output X is combined 
with inputs A and B to generate a new output X f. The value 
of X that produced X+ no longer exists and belongs only to 
the past. 
Labels R and S in the Table 3.1(b) correspond to reset 
and set, respectively. The word reset means make 0 (clear has 
the same meaning) and set means make 1. The output of all 
flip-flops is called Q by a historical convention. Examining 
the truth table reveals that whenever R = 1, the output Q 
is reset to 0. Similarly, when S = 1 the output is set to 1. 
(a) 
A 
°- 
V - N 
( b ) A — r ~ ^ 
/ 
0 I :->\J^ 
T?X 
/ 0 • Jr-\ 
GateV Y 
Assume t h a t / 
I 
^
^ 
I 
' — - 
I Cz 
J\ 
A and B are 
^ - ~ ^ ^ _ ^ ^ ^ — - * 
^ ^ 
I 
l 
| 
i n i t i a " y 0 
\ 
.
^
-
-
X
-
^ 
S
*
t
X 
Note that the gates are cross-coupled 
\ 
I 
1 r 
^ 
| 
initially 
with the output of one gate connected 
\ 
» GateV. 0 I 
0 
t 0 t h e i nP u t o f t h e o t h ef gate 
B o_ 
M ^ 2 y 
Analyzing the circuit by assuming initial conditions. 
An alternative view of the circuit. 
Figure 3.3 Analyzing the behavior of cross-coupled NOR gates. 

3.1 The RS Flip-flop 
105 
(a) Truth table for Fig. 3.2. 
Inputs 
Output 
A 
B 
X 
X 
0 
0 
0 
0 
0 
0 
1 
1 
0 
1 
0 
1 
0 
1 
1 
1 
1 
0 
0 
0 
1 
0 
1 
0 
1 
1 
0 
0 
1 
1 
1 
0 
T 
t 
OldX 
NewX 
(b) Truth table for relabeled Fig. 3.2. 
Inputs 
Output 
R 
S 
Q 
Q 
0 
0 
0 
0 
No change 
0 
0 
1 
1 
No change 
0 
1 
0 
1 
Set 
0 
1 
1 
1 
Set 
1 
0 
0 
0 
Clear 
1 
0 
1 
0 
Clear 
1 
1 
0 
? 
Undefined 
1 
1 
1 
T 
OldQ 
7 
T 
NewQ 
undefined 
The truth table is interpreted as follows. The output of the circuit is currently X (or Q) and the new inputs to be applied to the input terminals are A, B 
(or R, S). When these new inputs are applied to the circuit, its output is given by X+ (or Q+). For example, if the current output X is 1 and the new 
values of A and B are A = 1, B = 0, then the new output, X+, will be O.This value of X+ then becomes the next value of X when new inputs A and B 
are applied to the circuit. 
Table 3.1 Truth table for the circuit in Fig. 3.2. 
When R and S are both 0, the output does not change; that 
is,Qf = Q . 
If both R and S are simultaneously 1, the output is concep-
tually undefined (hence the question marks in Table 3.1(b), 
because the output can't be set and reset at the same time. In 
the case of the RS flip-flop implemented by two NOR gates, 
the output X does, in fact, go low when A = B = 1. In prac-
tice, the user of an RS flip-flop should avoid the condition 
R = S = 1. 
The two-NOR gate flip-flop of Fig. 3.2 has two outputs X 
and Y. An examination of the circuit for all inputs except 
A = B = 1 reveals that X and Y are complements. Because of 
the symmetric nature of flip-flops, almost all flip-flops have 
two outputs, Q and its complement Q. The complement of Q 
may not always be available to the user of the flip-flop 
because many commercial devices leave Q buried on the chip 
and not brought out to a pin. Figure 3.4 gives the circuit 
representation of an RS flip-flop. 
We can draw the truth table of the RS or any other flip-
flop in two ways. Up to now we've presented truth tables 
with two output lines for each possible input, one line 
for Q = 0 and one for Q = 1. An alternative approach is to 
employ the algebraic value of Q and is illustrated by 
Table 3.2. 
When R = S = 0 the new output Q+ is simply the old 
output Q. In other words, the output doesn't change state and 
remains in its previous state as long as R and S are both 0. 
The inputs R = S = 1 result in the output Q+ = X. The 
symbol X is used in truth tables to indicate an indeterminate 
or undefined condition. In Chapter 2 we used the same symbol 
R 
Q 
S 
Q 
R 
Q 
S 
Q 
R 
Q 
S 
Q 
R 
S 
Inputs 
Outputs 
Figure 3.4 Circuit representation of the RS flip-flop as a black box. 
Inputs 
Output 
Description 
R 
i/i 
Q 
0 
0 
Q 
No change 
0 
1 
1 
Set output to 1 
1 
0 
0 
Reset output to 0 
1 
1 
X 
Forbidden 
Table 3.2 An alternative truth table for the RS flip-flop. 
to indicate a don't care condition. An indeterminate condi-
tion is one whose outcome can't be calculated, whereas a 
don't care condition is one whose outcome does not matter to 
the designer. 
3.1.2 Characteristic equation of 
an RS flip-flop 
We have already demonstrated that you can derive an equa-
tion for a flip-flop by analyzing its circuit. Such an equation is 
called the flip-flop's characteristic equation. Instead of using 
an actual circuit, we can derive a characteristic equation from 

106 
Chapter 3 Sequential logic 
^R 
Q\OO OI n io 
o 
i j ) 
Figure 3.5 Karnaugh map for the characteristic equation of an 
RS flip-flop. 
Figure 3.6 RS flip-flop constructed from two cross-coupled 
NAND gates. 
the flip-flop's truth table. Figure 3.5 plots Table 3.1(b) on a 
Karnaugh map. We have indicated the condition R = S = 1 
by X because it is a forbidden condition. From this truth table 
we can write Q+ = S + Q • R. 
Note that this equation is slightly different from the one 
we derived earlier because it treats R = S = 1 as a don't care 
condition. 
3.1.3 Building an RS flip-flop from 
NAND gates 
An RS flip-flop can be constructed from two cross-coupled 
NAND gates just as easily as from two NOR gates. Figure 3.6 
illustrates a two-NAND gate flip-flop whose truth table is 
given in Table 3.3. 
The only significant difference between the NOR gate flip-
flop of Fig. 3.2 and the NAND gate flip-flop of Fig. 3.6 is that 
the inputs to the NAND gate flip-flop are active-low. If we 
were to place inverters at the R and S inputs to the NAND gate 
flip-flop, it would then be logically equivalent to the NOR 
gate flip-flop of Fig. 3.2. 
The no change input to the NAND gate flip-flop is R, S = 1, 
1; the output is cleared by forcing R = 0 and set by forcing 
S = 0; the forbidden input state is R, S = 0, 0. Suppose that 
we did set the inputs of a NAND gate RS flip-flop to 0,0 and 
then released the inputs to 1,1 to enter the no change state. 
What would happen? The answer is that we can't predict the 
final outcome. Initially, when both inputs are 0s, both outputs 
of the RS flip-flop must be Is (because the output of a NAND 
gate is a 1 if either of its inputs are a 0). The real problem 
arises when the inputs change state from 0,0 to 1,1. Due to 
tiny imperfections, either one or the other input would go 
high before its neighbor and cause the flip-flop to be set or 
reset. 
Inputs 
Output 
Q 
Comment 
R 
S 
Output 
Q 
0 
0 
X 
Forbidden 
0 
1 
1 
Reset output to 0 
1 
0 
0 
Set output to 1 
1 
1 
Q 
No change 
Table 3.3 Truth table for an RS flip-flop constructed from 
NAND gates. 
Inputs 
Output Q 
Figure 3.7 Timing diagram of the effect of pulses on an 
RS flip-flop's inputs. 
Real applications of RS flip-flops may employ either two 
NAND or two NOR gates depending only on which gates 
provide the simpler solution. In practice, the majority of RS 
flip-flops are often constructed from NAND gates because 
most circuits use active-low signals. We began our discussion 
of RS flip-flops with the NOR gate circuit (unlike other texts 
that introduce first the more common NAND gate flip-flop) 
because we have discovered that many students find it hard to 
come to terms with negative logic (i.e. logic in which the low 
state is the active state). 
3.1.4 Applications of the RS flip-flop 
An important application of RS flip-flops is in the recording 
of short-lived events. If the Q output of a flip-flop is in a zero 
state, a logical 1 pulse at its S input (assuming the R input is 0) 
will cause Q to be set to a 1, and to remain at a 1, until the R 
input resets Q. The effect of a pulse at the S input followed by 
a pulse at the R input of an RS flip-flop is illustrated in Fig. 3.7. 
Consider the following application of RS flip-flops to an 
indicator circuit. If an aircraft is flown outside its perfor-
mance envelope no immediate damage may be apparent, but 
its structure might be permanently weakened. To keep things 
/Rising edge 
fof SsetsQ 
~T 
Rising edge 
lof R resets Q 
/
GateV 
_ — 
CTJ°—T 
*Q 
Active-Sow 
"~ 
"""--—^ 
inputs 
I 
X 7 
ypy-1—>* 
S 
0_ 
1 
R 
0 -
1 
Cat« 
Ci 
Gate\~ 
Active-Sow 
inputs 

3.1 The RS Flip-flop 
107 
Pressure 
sensing 
head 
Pressure to 
voltage 
transducer 
Comparator 
V„ 
Comparator 
Accelerometer 
measures 
|—i 
g-force 
Flap selection 
switch 
Crr 
Comparator 
Test warning 
lights 
Master reset 
Figure 3.8 Application of RS flip-flops in a warning system. 
S 
Q 
FF, 
S 
Q 
FF2 
R 
S 
Q 
FF3 
R 
Overspeed 
warning light 
Flap extension 
warning light 
Overstress 
warning light 
From other 
warning circuits 
Master 
warning 
simple, we will consider three possible events that are consid-
ered harmful and might endanger the aircraft. 
1. Exceeding the maximum permissible speed Vne. 
2. Extending the flaps above the flap-limiting speed Vfl. That is, 
the flaps must not be lowered if the aircraft is going faster 
than Vfl. 
3. Exceeding the maximum acceleration (g-force) Gmax. 
If any of the above parameters are exceeded (even for only 
an instant), a lasting record of the event must be made. 
Figure 3.8 shows the arrangement of warning lights used 
to indicate that one of these conditions has been violated. 
Transducers that convert acceleration or velocity into a 
voltage measure the acceleration and speed of the aircraft. 
The voltages from die transducers are compared with the three 
threshold values (Vne, Vfl, Gmax) in comparators, whose outputs 
are true if the threshold is exceeded, otherwise false. In order 
to detect the extension of flaps above the flap-limiting 
speed, the output of the comparator is ANDed with a signal 
from the flap actuator circuit that is true when the flaps 
are down. 
The three signals from the comparators are fed, via OR 
gates, to the S inputs of three RS flip-flops. Initially, on 
switching on the system, the flip-flops are automatically reset 
by applying a logical 1 pulse to all R inputs simultaneously. If 
at any time one of the S inputs becomes true, the output of 
that flip-flop is set to a logical 1 and triggers an alarm. All 
outputs are ORed together to illuminate a master warning 
light. A master alarm signal makes it unnecessary for the pilot 
to have to scan all the warning lights periodically. An addi-
tional feature of the circuit is a test facility. When the warning 
test button is pushed, all warning lights should be illumin-
ated and remain so until the reset button is pressed. A test 
facility verifies the correct operation of the flip-flops and the 
warning lights. 
A pulse-train generator 
Figure 3.9 gives the circuit of a pulse-train generator that 
generates a sequence of N pulses each time it is triggered by a 
positive transition at its START input. The value of N is user 
supplied and is fed to the circuit by three switches to select the 
values of Cc, Cb, Ca. This circuit uses the counter that we will 
meet later in this chapter. 
The key to this circuit is the RS flip-flop, G6, used to start 
and stop the pulse generator. Assume that initially the R and 
S inputs to the flip-flop are R = 0 and S = 0 and that its 
output Q is a logical 0. Because one of the inputs to AND gate 
G, is low, the pulse train output is also low. 
When a logical 1 pulse is applied to the flip-flop's START 
input, its Q output rises to a logical 1 and enables AND gate 
G,. A train of clock pulses at the second input of G, now 
appears at the output of the AND gate. This gated pulse train 
is applied to the input of a counter (to be described later), 
which counts pulses and generates a three-bit output on Qa, 
Qb> Qc corresponding to the number of pulses counted in the 
range 0 to 7. The outputs of the counter are fed to an equality 
detector composed of three EOR gates, G2 to G4, plus NOR 
gate G5. A second input to the equality detector is the user-
supplied count value Ca, Cb, Cc. The outputs of the EOR gates 
are combined in NOR gate G5 (notice that it's drawn in 
negative logic form to emphasize that the output is 1 if all its 
inputs are 0). 

Clock J 
START 
Output 
Counter 
output 
RESET 
Figure 3.10 Timing diagram of pulse train generator. 
Figure 3.10 gives a timing diagram for the pulse generator. 
Initially the counter is held in a reset state (Qa = Qb = Qc = 0). 
When the counter is clocked, its output is incremented by 1 on 
the falling edge of each clock pulse. The counter counts upward 
from 0 and the equality detector compares the current count on 
Qa> Qb> Qc output with the user-supplied inputs Ca, Q, Cc. When 
the output of the counter is equal to the user-supplied input, the 
output of gate G5 goes high and resets both the counter and the 
RS flip-flop. Resetting the counter forces the counter output to 0. 
Resetting the RS flip-flop disables AND gate G, and no further 
clock pulses appear at the output of G,. In this application of the 
RS flip-flop, its S input is triggered to start an action and its 
R input is triggered to terminate the action. 
3.1.5 The clocked RS flip-flop 
The RS flip-flop of Fig. 3.2 responds to signals applied to its 
inputs according to its truth table. There are situations when 
108 
Chapter 3 Sequential logic 
_J~ 
l 
1 
START 
c 
Start/stop flip-flop 
. 
» „ 
Q 
1 
. 
Pu'se train 
^— a 
£) 
.
 
outP
ut » J U L T L 
Clock 
L 
I 
iUUUUl 
I 
± 
- * RESET 
Counter G7 
*- ^ ^ 
The counter's /
^ 
Q, 
Qb 
Qa 
-The counter 
RESET input ^ 
^ 
- p 
" P 
P 
resets its outputs 
I—CMC rCMC rC?
4\C *\ Gates Czi °
3, GA' and Cs 
— 
^ 
^ s . constitute a comparator 
that compares QcQbQa 
RESET S~~]_ F 
Q J 
Cb 
* 
Ca 
withCcCbCa 
\ 
The values of Cc, Cb, Ca are 
RESET asserted when the counter 
user selected to determine 
_ 
reaches the preselected value of 
the length of the pulse train 
Figure 3.9 Pulse train 
Cc, Cb, Ca 
generator. 
V 
Clock 
RESET 
Counter 
G7 
*• 
Qc 
Qb 
Qa 
- I 
* 
» H 4 • 
Q 
0 

3.2 The D flip-flop 
109 
we want the RS flip-flop to ignore its inputs until a particular 
time. The circuit of Fig. 3.11 demonstrates how this is accomp-
lished by turning the RS flip-flop into a clocked RS flip-flop. 
A normal, unmodified, RS flip-flop lies in the inner box in 
Fig. 3.11. Its inputs, R' and S', are derived from the external 
inputs R and S by ANDing them with a clock input C—some 
texts call these two AND gates 'steering gates'. As long as 
C = 0, the inputs to the RS flip-flop, R' and S', are forced to 
remain at 0, no matter what is happening to the external 
\ llhe AND gates ensure that 
the inputs to the RS flip-flop 
are tow unless C is high 
Figure 3.11 The clocked RS flip-flop. 
Figure 3.12 Building a clocked RS flip-flop with NAND gates. 
R and S inputs. The output of the RS flip-flop remains 
constant as long as these R' and S' inputs are both 0. 
Whenever C = 1, the external R and S inputs to the 
circuit are transferred to the flip-flop so that R' = R and 
S = S, and the flip-flop responds accordingly. The clock 
input may be thought of as an inhibitor, restraining the flip-
flop from acting until the right time. Figure 3.12 demon-
strates how we can build a clocked RS flip-flop from NAND 
gates. Clocked flip-flops are dealt with in more detail later 
in this chapter. 
3.2 The D flip-flop 
Like the RS flip-flop, the D flip-flop has two inputs, one called 
D and the other C. The D input is referred to as the data input 
and C as the clock input. The D flip-flop is, by its nature, a 
clocked flip-flop and we will call the act of pulsing the C input 
high and then low clocking the D flip-flop. 
When a D flip-flop is clocked, the value at its D input is 
transferred to its Q output and the output remains constant 
until the next time it is clocked. The D flip-flop is a staticizer 
because it records the state of the D input and holds it con-
stant until it's clocked. Others call it a delay element because, 
if the D input changes state at time T but the flip-flop is 
clocked t seconds later, the output Q doesn't change state 
until t seconds after the input. I think of the D flip-flop as a 
census taker because it takes a census of the input and remem-
bers it until the next census is taken. The truth table for a 
D flip-flop is given in Table 3.4. 
The circuit of a D flip-flop is provided in Fig. 3.13 and 
consists of an RS flip-flop plus a few gates. The two AND 
gates turn the RS flip-flop into a clocked RS flip-flop. As long 
as the C input to the AND gates is low, the R and S inputs are 
clamped at 0 and Q cannot change. 
Full form 
Algebraic form 
Inputs 
D 
Q 
Output 
0/ 
Inputs 
Output 
Q 
C 
D 
Q 
Output 
0/ 
C 
D 
Output 
Q 
0 
0 
0 
0 
Q +^Q 
No change 
0 
0 
Q 
0 
0 
1 
1 
Q +^Q 
No change 
0 
1 
Q 
0 
1 
0 
0 
Q+^-Q 
No change 
1 
0 
0 
0 
1 
1 
1 
Q+<-Q 
No change 
1 
1 
i 
1 
0 
0 
0 
Q+<-D 
1 
0 
1 
0 
Q+<-D 
1 
1 
0 
1 
Q+<-D 
1 
1 
1 
1 
Q+<-D 
Table 3.4 Truth table for a D flip-flop. 
Q 
Q 
•Q 
Q 
R-
S-
C-
R-
S-
C-
R' 
RS flip-flop 
S' 

110 
Chapter 3 Sequential logic 
Vcc 
2CLR 
2D 
2Clk 2PRE 
2Q 
2Q 
14 
13 
12 
11 
10 
? ) 
8 
•-a1— 
TZ 
J 
D 
PRE 
Q 
>Ctk 
CLR 
Q 
TZ 
J 
—C 
1 
D 
PRE 
Q 
>Ctk 
CLR 
Q 
TZ 
J 
A 
—C 
1 
D 
PRE 
Q 
>Ctk 
CLR 
Q 
TZ 
J 
CLR 
Q -
rOClk 
—C 
1 
D 
PRE 
Q 
>Ctk 
CLR 
Q 
TZ 
J 
CLR 
Q -
rOClk 
V 
CLR 
Q -
rOClk 
1 
rD 
PRE 
rD 
PRE 
v ~ 
h v-. 
1 
J 
3 
% 
5 
5 
7 
Figure 3.13 Circuit of a D flip-flop. 
1CLR 
1D 
Klk 
1PRE 
1Q 
1Q 
GND 
When C goes high, the S input is connected to D and the 
R input to D. Consequently, (R, S) must either be (0, 1) if 
D = 1, or (1, 0) if D = 0. Therefore, D = 1 sets the RS flip-
flop, and D = 0 clears it. 
3.2.1 Practical sequential logic 
elements 
Just as semiconductor manufacturers have provided combi-
national logic elements in single packages, they have done the 
same with sequential logic elements. Indeed, there are more 
special-purpose sequential logic elements than combina-
tional logic elements. Practical flip-flops are more complex 
than those presented hitherto in this chapter. Real circuits 
have to cater for real-world problems. We have already said 
that the output of a flip-flop is a function of its current inputs 
and its previous output. What happens when a flip-flop is 
first switched on? The answer is quite simple. The Q output 
takes on a random state, assuming no input is being applied 
that will force Q into a 0 or 1 state. 
Random states may be fine at the gaming tables in Las 
Vegas; they're less helpful when the control systems of a 
nuclear reactor are first energized. Many flip-flops are pro-
vided with special control inputs that are used to place them 
in a known state. Figure 3.14 illustrates the 74LS74, a dual 
positive-edge triggered D flip-flop that has two active-low 
control inputs called preset and clear (abbreviated PRE and 
CLR). In normal operation both PRE and CLR remain in 
logical 1 states. If PRE = 0 the Q output is set to a logical 1 
and if CLR = 0 the Q output is cleared to a logical 0. As in 
the case of the RS flip-flop, the condition PRE = CLR = 0 
should not be allowed to occur. 
These preset and clear inputs are unconditional in the sense 
that they override all activity at the other inputs of this flip-
flop. For example, asserting PRE sets Q to 1 irrespective of 
the state of the flip-flop's C and D inputs. When a digital 
system is made up from many flip-flops that must be set or 
cleared at the application of power, their PRE or CLR lines 
are connected to a common RESET line and this line is 
Figure 3.14 The 74LS74 D flip-flop. 
dm-1 d l d0 
Register 
D 
Q 
C 
D 
Q 
C 
m D flip-flops 
- • Q o 
• • Q i 
• + Q * 
m-bit data bus 
Clock 
Figure 3.15 Using D flip-flops to create a register. 
momentarily asserted active-low by a single pulse shortly 
after the power is switched on. 
3.2.2 Using D flip-flops to 
create a register 
Later we shall discover that a computer is composed of little 
more than combinational logic elements, buses, and groups of 
flip-flops called registers that transmit data to and receive data 
from buses. A typical example of the application of D flip-
flops is provided by Fig. 3.15 in which an m-bit wide data bus 
transfers data from one part of a digital system to another. 
Data on the bus is constantly changing as different devices use 
it to transmit their data from one register to another. 
The D inputs of a group of m D flip-flops are connected to 
the m lines of the bus. The clock inputs of all flip-flops are 
Q 
Q 
D-
C-
D 
D 
S 
Q 
D 
C 

3.2 The D filp-flop 
111 
connected together, allowing them to be clocked simultan-
eously. As long as C = 0, the flip-flops ignore data on the bus 
and their Q outputs remain unchanged. Suppose some device 
wishes to transfer its data to the flip-flops. It first puts its data 
on the bus and then the flip-flops are clocked, latching the 
data into them. When the clock has returned to zero, the data 
remains frozen in the flip-flops. 
3.2.3 Using Digital Works to 
create a register 
We are now going to use Digital Works to create a simple 
bused system using D flip-flops. Although Digital Works 
implements both RS and D flip-flops, we'll construct a D flip-
flop from basic gates. Figure 3.16 shows a single 1 -bit cell in a 
register (we can construct an m-bit register by using m iden-
tical elements in parallel). 
If you examine Fig. 3.16 you will find that the flip-flop is 
more complex than the simple D flip-flop of Fig. 3.13. We 
have added a tri-state gate to the Q output to allow the flip-
flop to drive a bus or to be disconnected from the bus. 
Furthermore, we've added an input multiplexer to allow the 
D input to be connected to one of two sources A and B. The 
inputs and output of Fig. 3.16 are 
• A input 
• B input 
• A/B select input 
• Clock input 
• Enable output 
• Q output. 
In Fig. 3.17 we've used Digital Work's macro facility to 
convert the circuit in Fig. 3.16 into a black box macro that 
can be used as a circuit element to build more complex 
systems. 
Figure 3.18 provides a test bed for three of the register slices. 
We have provided one bit of three registers and three buses 
(input bus A, input bus B, and output bus C). Each register 
slice is connected to all three buses. We've added input 
devices to all the control inputs to enable us to experiment 
with this circuit. 
The system in Fig. 3.18 can't do a lot, but what it can do is 
very important. Because we've added input devices to buses A 
and B, we can force our own data on bus A and B. We can 
select whether each register slice gets its input from bus A or 
bus B by setting the value of the Input select input to 1 (bus A) 
or 0 (bus B). Data is clocked into any of the register slices by 
clocking it (i.e. setting its clock input to 1 to capture the data 
and then setting the clock input to 0 to latch and retain the 
data). Finally, data from any of the three register slices can be 
put on bus C by asserting the appropriate output. 
This circuit is important because it forms the heart of a 
computer. All we need to create an ALU (arithmetic and logic 
unit) are the circuits that take data from bus C, process it, and 
copy the result to the A or B bus. 
Digital Vfoiks 95^OLjPcelidwm 
File 
Edit 
Circuit 
View 
l o o l s 
Help 
D
^
l 
- 
•'•_ 
;• 
<S 
D D ° D I > ! x > D 
fr 
-if m m m 
m 
> 
o 
ii 
i> • ! 
^5 
EU5J 
O 
Q 
35 
Input multiplexer 
Input 
Output enable 
3 EJ Select input 
Clock register 
Figure 3.16 Using D 
flip-flops to create one cell of 
a register. 
: RS flip-flop; 
.' . output: 
= 
08-
InputA 

112 
Chapter 3 Sequential logic 
!p^ei%)]aifefEclitQ^ 
File 
View 
options 
iieip 
ik o a a 
A a >]ia 
Register Cell 
Output I 
I Input A 
I Input B 
a 
. 
Output enable 
Iselettlnput 
Clock register 
Figure 3.17 Converting the circuit of Fig. 3.16 
into a macro (i.e. black box representation). 
»pigitalWdtks S5 rOUPcellBBqidwm 
0le £dit Qicuit View Tools i^eip 
D £# B 
"• 
' - ' & 
J > t > P I > » D I > f r S B t D H 
l> O 1! O 
li 
5) 
a 
0 
S 3 
SetAffi-
SetBB-
We can jam data 
on the A or B bus 
via the Set A and 
Set B switches. 
H-
3lnputA 
Output £ . 
3 Input B 
EnableC-
0 Input select 
3 Clock register 
Register eel! 
• 3lnputA 
Output 
-3 Input B 
EnableE-
. 3 Input select 
•D Clock register 
O 
O 
Data from a register 
is put on the C bus 
by enabling the 
appropriate register. 
LEDs on the A, B, 
and C buses show 
the state of the bus. 
Figure 3.18 Using D flip-flops to create a register in Digital Works. 
3.2.4 A typical register chip 
You can obtain a single package containing the flip-flops that 
implement a register. Figure 3.19 illustrates the 74LS373, an 
octal register composed of D flip-flops that is available in a 
20-pin package with eight inputs, eight outputs, two power 
simultaneously. 
supply pins, and two control inputs. The clock input, G, is a 
level-sensitive clock, which, when high, causes the value at D, 
to be transferred to Q,. All eight clock inputs are connected 
together internally so that the G input clocks each flip-flop 
Q 
• Register celt 
• • 
tj 
, —. 
rjinputA 
Output £ 
—DlnputB 
' 
. E n a & l e E 
— 
— 3 input select 
0 Clock register 
Register tell 
: 
BusC 
Bu&&. BusB 

3.3 Clocked flip-flops 
113 
Figure 3.19 The 74LS373 octal register. 
The 74LS373's other control input is active-low OE (out-
put enable), which controls the output of all flip-flops. When 
OE = 0, the flip-flop behaves exactly as we would expect. 
When OE = 1, the eight Q outputs are internally discon-
nected from the output pins of the device; that is, the 
74LS373 has tri-state outputs and OE is used to turn off the 
chip's output circuits when it is not driving a bus. 
Figure 3.20 demonstrates the 74LS373 octal register in a 
digital system where four registers are connected to a com-
mon data bus. Each register is arranged with both its outputs 
and its inputs connected to the same bus allowing it to trans-
mit data onto the bus or to receive data from it. 
Each register has tri-state outputs controlled by an output 
enable pin. When OE is asserted low, the corresponding reg-
ister drives the bus. Registers are clocked by an active-high 
clock input labeled G. 
IC5a is a 2-line to 4-line decoder; that is, a demultiplexer 
of the type we described in Chapter 2. When this device 
is enabled, the 2-bit binary source code at the input of IC5a 
causes one of its output lines, Y0 to Y3, to go low. These out-
puts are connected to the respective OE inputs of the four 
registers. Each of the four possible source codes enables one 
of the registers; for example, if the source code at the input to 
IC5a is 01, the output of register 1 is enabled and the contents 
of register 1 are placed on the bus. The outputs of all other 
registers remain internally disconnected from the bus. 
The 74LS139 contains two complete 2-line to 4-line 
decoders in a single 16-pin package. The second half of this 
package acts as a destination decoder. Each of the four out-
puts from IC5b is connected to one of the clock inputs, G, of 
the four registers. Because the clock inputs are active-high 
and the outputs of the decoder are active-low, it's necessary to 
invert these outputs. Four inverters, IC6, perform this func-
tion. When IC5b is enabled, one of its outputs is asserted and 
the corresponding register clocked. Clocking a register 
latches data from the data bus. 
Suppose the contents of register 1 are to be copied into reg-
ister 3. The source code at IC5a is set to 01 and the destination 
code at IC5b is set to 11. This puts the data from register 1 on 
the bus and latches the data into register 3. We can easily 
relate the example of Fig. 3.20 to the digital computer. One of 
the most fundamental operations in computing is the assign-
ment that can be represented in a high-level language as 
B = A and in a low-level language as MOVE A, B. The action 
MOVE A, B (i.e. transfer the contents of A to B) is imple-
mented by specifying A as the source and B as the destination. 
Note that throughout this text we put the destination of a 
data transfer in bold font to stress the direction of data 
transfer. 
3.3 Clocked flip-flops 
Before we introduce the JK flip-flop we look more closely at 
the idea of clocking sequential circuits. Clocked circuits allow 
logic elements to respond to their inputs only when the 
inputs are valid. Some writers use the term trigger rather than 
clock, because triggering a flip-flop gives the impression of 
causing an event to take place at a discrete instant. We begin 
by examining the effect of delays on the passage of signals 
through systems. 
Figure 3.21 demonstrates the effect of circuit delays on a 
system with two inputs, A and B, that are acted upon by 
processes A, B, and C to produce an output. The nature of the 
processes is not important because we're interested only in 
the way in which they delay signals passing through them. 
Imagine that at time f = 0, the inputs to processes A and B 
become valid (i.e. these are the correct inputs to be operated 
on by the processes). Assume that process A in Fig. 3.21 intro-
duces a two-unit delay, process B a one-unit delay, and 
process C a two-unit delay. 
Although the output from process B becomes valid at 
f = 1, it's not until f = 2 that the output of process A has 
become valid. The outputs of processes A and B are fed to 
process C, which has a two-unit delay. Clearly, the desired 
output from C due to inputs A and B is not valid until at least 
four time units after f = 0. The output from process C 
changes at least once before it settles down to its final value 
(Why? Because of the different delays through processes A 
and B). This poses a problem. How does an observer at the 
output of process C know when to act upon the data from C? 
What we need is some means of capturing data only when 
we know that it's valid—see Fig. 3.22. If a D flip-flop is placed 
Output enable 
Ol 
o|>_^ 
ID 
i5~n —-j 
j—dG Ql 
cp—MQ 
2D 
ID _| 
— n 
-olC Ql 
c£>—»-2Q 
3D 
l i T l 
1 
-dG QU-O[>—»3Q 
4D 
ID _j 
1 
-cJG_Qj 
o[>-^4Q 
5D 
|D^n 
—n 
-dG Ql 
cfp>—»-5Q 
6D 
icrn 
—-i 
-dG Ql 
C0>—»-6Q 
7D 
|D _| —~\ 
- d G Ql 
o|/>>—»-7Q 
8D 
W3 
—"I 
-dG Ql 
c£>—»-8Q 
dock _ ^ r ^ ^ - ^ -

114 
Chapter 3 Sequential logic 
IC, Register 3 
74LS373 
d0 d. 
d7 
Source code 
Enable source 
DE, DE0 
Destination code 
Enable destination 
OE = Output enable 
G = Clock 
8-bit parallel data bus 
Figure 3.20 Using the 
74LS373 octal register in a 
bused system. 
at the output of process C and is clocked four units of time 
after f = 0, the desired data will be latched into the flip-flop 
and held constant until the next clock pulse. Clocked systems 
hold digital information constant in flip-flops while the infor-
mation is operated on by groups of logic elements, analogous 
to the processes of Fig. 3.21. Between clock pulses, the outputs 
of the flip-flops are processed by the logic elements and the 
new data values are presented to the inputs of flip-flops. 
After a suitable time delay (longer than the time taken for 
the slowest process to be completed), the flip-flops are clocked. 
The outputs of the processes are held constant until the next 
time the flip-flops are clocked. A clocked system is often called 
synchronous, as all processes are started simultaneously on each 
new clock pulse. An asynchronous system is one in which the 
end of one process signals (i.e. triggers) the start of the next. 
Obviously, an asynchronous system must be faster than the 
corresponding synchronous system. Asynchronous systems 
are more complex and difficult to design than synchronous 
systems and popular wisdom says that they are best avoided 
because they are inherently less reliable than synchronous 
circuits. The 1990s saw a renewed interest in asynchronous 
systems because of their speed and lower power consumption. 
3.3.1 Pipelining 
Now consider the effect of placing D flip-flops at the outputs of 
processes A, B, and C in the system of Fig. 3.23. Figure 3.23 
shows the logical state at several points in a system as a function 
of time. The diagram is read from left to right (the direction of 
time flow). Signals are represented by parallel lines to demon-
strate that the signal values may be Is or Os (we don't care). 
What matters is the time at which signals change. Changes are 
shown by the parallel lines crossing over. Lines with arrow-
heads are drawn between points to demonstrate cause and 
effect; for example, the line from Input A to Output A shows 
that a change in Input A leads to a change in Output A. 
IC2 Register 2 
74LS373 
IC3 Register 1 
74LS373 
IC4 Register 0 
74LS373 
\ I C 6 74LS04 
' I L L . 
.J 1 1 1 -
v 
\ 
Y 
V 
V 
Y 
^ , - 
r ' ™- i S5S**" 
' 
s 
2 
' 
I 
' 
T 
A " " • 
' 
OE 
G 
OE 
G 
G 
OE 
G 

3.3 Clocked flip-flops 
115 
Two-unit delay 
Input A 
Input B 
»- Output C 
One-unit delay 
Two signals, A and B are operated on 
by process A and process B respectively. 
The outputs of these two processes, are 
the inputs to process C 
Figure 3.21 Processes and 
delays. 
In this example we assume that each of the processes intro-
duces a single unit of delay and the flip-flops are clocked 
simultaneously every unit of time. Figure 3.23 gives the tim-
ing diagram for this system. Note how a new input can be 
accepted every unit of time, rather than every two units of 
time as you might expect. The secret of our increase in 
throughput is called pipelining because we are operating on 
different data at different stages in the pipeline. For example, 
when process A and process B are operating on data i, process 
C is operating on data i - 1 and the latched output from 
process C corresponds to data i — 2. 
When we introduce the RISC processor we will discover 
that pipelining is a technique used to speed up the operation 
of a computer by overlapping consecutive operations. 
3.3.2 Ways of clocking flip-flops 
A clocked flip-flop captures a digital value and holds it 
constant. There are, however, three ways of clocking a 
flip-flop. 
1. Whenever the clock is asserted (i.e. a level-sensitive flip-flop). 
2. Whenever the clock is changing state (i.e. an edge-sensitive 
flip-flop). 
3. Capture data on one edge of the clock and transfer it to the 
output on the following edge (i.e. a master-slave flip-flop). 
A level-sensitive clock triggers a flip-flop whenever the 
clock is in a particular logical state (some flip-flops are 
clocked by a logical 1 and some by a logical 0). The clocked RS 
flip-flop of Fig. 3.11 is level sensitive because the RS flip-flop 
responds to its R and S inputs whenever the clock input is 
high. A level-sensitive clock is unsuitable for certain 
applications. Consider the system of Fig. 3.24 in which the 
output of a D flip-flop is fed through a logic network and 
then back to the flip-flop's D input. If we call the output of the 
flip-flop the current Q, then the current Q is fed through the 
logic network to generate a new input D. When the flip-flop 
is clocked, the value of D is transferred to the output to 
generate Q+. 
If the clock is level sensitive, the new Q+ can rush through 
the logic network and change D and hence the output. This 
chain of events continues in an oscillatory fashion with the 
dog chasing its tail. To avoid such unstable or unpredictable 
behavior, we need an infinitesimally short clock pulse to 
capture the output and hold it constant. As such a short pulse 
can't easily be created, the edge-sensitive clock has been intro-
duced to solve the feedback problem. Level-sensitive clocked 
D flip-flops are often perfectly satisfactory in applications 
such as registers connected to data buses, because the dura-
tion of the clock is usually small compared to the time for 
which the data is valid. 
Process A 
Two-unit delay 
Process C 
Process B 
4—\ 
Input A valid 
Input A 
1 
_ i ^ 
1 — 
4—.. 
\ 
Input B valid 
Input B 
1 X 
1 
1 
_ 
j 
I TwoXjnit delay 
_ 
\ 
I 
^*7~N 
Output A valid 
Output A 
j 
1 
' \ 
_ 
; 
One-unit delay \ 
H 
V - * , 
—4 
_ t 
,. „ 
j 
jf 
._ 
\ 
Output B valid 
Output B 
; 
' 
^ 
\ 
v 
 
! 
,, Njworunit delay 
: 
< 
^v ^v.^—p-: J 
i 
^ - - — 
».;/ 
Output C valid 
Output C l__j 
t 
1 
*J 
S 
1 
2 
3 
f 
5 
6 
Time 
Delay before C is valid 
< 
*— 
• 
Output C I 
Output B valid 
Tworunit delay 

116 
Chapter 3 Sequential logic 
Input A 
U 
Process A 
Process C 
Input I 
Process B 
Output Q 
iThe input to the 
j D flip-flop is sampled 
I at this point 
Figure 3.22 Latching the 
output of a system. 
3.3.3 Edge-triggered flip-flops 
An edge-triggered flip-flop is clocked not by the level or state 
of the clock (i.e. high or low), but by the transition of the 
clock signal from zero to one, or one to zero. The former case 
is called a positive or rising-edge sensitive clock and the latter 
is called a negative or falling-edge sensitive clock. As the ris-
ing or falling edge of a pulse may have a duration of less than 
1 ns, an edge-triggered clock can be regarded as a level-
sensitive clock triggered by a pulse of an infinitesimally short 
duration. A nanosecond (ns) is a thousand millionth (10~9) 
of a second. The feedback problem described by Fig. 3.24 
ceases to exist if you use an edge-sensitive flip-flop because 
there's insufficient time for the new output to race back to the 
input within the duration of a single rising edge. 
There are circumstances when edge-triggered flip-flops are 
unsatisfactory because of a phenomenon called clock skew. 
If, in a digital system, several edge-triggered flip-flops are 
clocked by the same edge of a pulse, the exact times at which 
the individual flip-flops are clocked vary. Variation in the 
arrival time of pulses at each clock input is called clock skew 
and is caused by the different paths by which clock pulses 
reach each flip-flop. Electrical impulses move through 
circuits at somewhat less than the speed of light, which is 
30 cm/ns. Unless each flip-flop is located at the same distance 
from the source of the clock pulse and unless any additional 
delays in each path due to other logic elements are identical, 
the clock pulse will arrive at the flip-flops at different 
instants. Moreover, the delay a signal experiences going 
through a gate changes with temperature and even the age of 
the gate. Suppose that the output of flip-flop A is connected 
to the input of flip-flop B and they are clocked together. 
Ideally, at the moment of clocking, the old output of A is 
clocked into B. If, by bad design or bad luck, flip-flop A is trig-
gered a few nanoseconds before flip-flop B, B sees the new 
output from A, not the old (i.e. previous) output—it's as if 
A were clocked by a separate and earlier clock. 
Figure 3.25 gives the circuit diagram of a positive edge-
triggered D flip-flop that also has unconditional preset and 
clear inputs. Edge triggering is implemented by using the 
active transition of the clock to clock latches 1 and 2 and then 
feeding the output of latch 2 back to latch 1 to cut off the 
clock in the NAND gate. That is, once the clock has been 
detected, the clock input path is removed. 
Input A 4 
Input B • 
Output A 4 
Output B * 
Output C * 
Clock | 
* D 
Q 
• 
Output 
^ 
Q 
The output of 
process C is latched 
by a D flip-flop and 
Clock 
held constant 
-»• Time 
i 
f - 1 
;'-1 
/ - I 
^ 
X^D 
;+2 

3.3 Clocked flip-flops 
1 1 7 
Input A -
Input B 
Clock 
Input A 
Output A 
Latched A 
Latched B 
Output C 
Latched C 
/"-1 
i-4 
Process A 
Output 
The outputs from processes 
A and B are captured and 
latched and held constant 
as the inputs to process C 
-J->Ti 
K 
i r \ 
«+i X uz X~TT3 K »4 f T T T l 
^K 
/-i^K 
>A 
/-n X ' ^ 1 
'+3 r T T T l 
Tij] i-z r~/-^i 
' Ki ^i K '-+2 in^Ht 
^ 
/-z K f-i K 
' A 
'+i K '+2 K~~^3l 
rrinr^T-n^rT-^r^nnTFi 
r?] i-4 i /-3 K i-2 x >^~p* / r r r r x 
Time 
Figure 3.23 Latching the 
input and output of processes 
to implement pipelining. 
the input can be 
fed to the output 
Figure 3.24 Feedback and the level-sensitive clock. 
3.3.4 The master-slave flip-flop 
The master-slave (MS) flip-flop has the external appearance 
of a single flip-flop, but internally is arranged as two flip-flops 
operating in series. One of these flip-flops is called the master 
and the other the slave. The term slave is used because the 
slave flip-flop follows the master. Figure 3.26 describes a 
simple RS master-slave flip-flop composed of two RS flip-
flops in series. Note that the master flip-flop is enabled when 
the clock is high and the slave flip-flop is enabled when the 
clock is low. 
When the clock pulse goes high, the input data at the R and 
S input terminals of the master flip-flop is copied into the 
master flip-flop. At this point, the output terminals of the 
master-slave flip-flop aren't affected and don't change state 
because die output comes from the slave flip-flop that is in a 
hold state because its clock is low. 
Because the master flip-flop of Fig. 3.26 uses a level-
sensitive RS flip-flop, the master responds to data at its RS 
inputs as long as the clock is asserted high. The data at the RS 
inputs of the master is latched by the master at the instant the 
clock input goes low. On the falling edge of the clock, the 
slave's clock input goes high and data from the master flip-
flop's outputs is copied into the save flip-flop. Only now may 
the output terminals change state. Figure 3.27 provides a tim-
ing diagram for the master-slave RS flip-flop. 
Master-slave flip-flops totally isolate their input terminals 
from their output terminals simply because the output of the 
slave flip-flop does not change until after the input conditions 
have been sampled and latched internally in the master. 
Conceptually, the master-slave flip-flop behaves like an air 
Process C 
Process B 
Clock I 
Output from 
the flip-flop 
is the input to 
the network 
Loop 
Logic network 
Clock— 
D = f(Q)\ 
Q 
D 
Q 
D 
C 
Q 
D 
C 

input. To operate an air lock in a submarine, 
divers in the water open the air lock, enter, and 
close the door behind them. The divers are 
now isolated from both the water outside and 
the air inside. When the divers open the door 
into the submarine, they step inside and close 
the air lock door behind diem. 
In order to demonstrate how the different 
types of clocked flip-flop behave, Fig. 3.28 
presents the output waveforms for four 
clocked D flip-flops when presented with the 
same input. 
3.3.5 Bus arbitration-
example 
-an 
Figure 3.25 Circuit of an edge-triggered flip-flop. 
Figure 3.26 The master-slave RS flip-flop. 
Input sampled 
by master 
Clock 
Clock to master 
'goes low and 
clock to slave 
2oes high 
Master flip-flop 
may change as 
long as the clock 
is high 
Input 
Output 
Input valid 
J 
Output valid 
Figure 3.27 Timing diagram of a master-slave RS flip-flop. 
lock in a submarine or spacecraft. An air lock exists to transfer 
people between regions of different pressure (air-to-vacuum 
or air-to-water) without ever permitting a direct path between 
the two pressure regions. A flip-flop is analogous to an air 
lock because its output must not be fed directly back to its 
We now look at a more advanced application 
of flip-flops in a bus arbitration circuit that 
decides which of two processors get to access 
a block of common memory, called dual-
ported RAM, when both processors want the 
memory at the same time. Students may omit 
this section on a first reading. 
Let's look at a system with two processors 
that communicate via a common block of 
RAM called DPRAM (dual-ported RAM). 
Figure 3.29 describes such an arrangement. 
You could regard the DPRAM as a bridge 
between two buses. 
Because both processors 1 and 2 operate 
independently, either processor may access 
the common memory at any time. We need a means of 
requesting control of the common memory and getting 
access to the memory even if both processors make near-
simultaneous requests. 
Figure 3.30 describes an arbiter with a clock input, two 
request inputs, and two grant outputs. The request and grant 
inputs and outputs are all active-low. The memory-request 
inputs, Requestl and Request2, are sampled by two positive-
edge triggered latches. The arbiter clocks latch la on the ris-
ing edge of the clock and latch 2a on the falling edge of the 
clock. This arrangement ensures that the two request inputs 
are not sampled simultaneously. 
Figure 3.31 provides a timing diagram for the case in which 
both processors request the bus simultaneously. As we can 
see, processor 2 wins the request and processor 1 must wait 
until processor 2 has relinquished the bus. That is, processor 
1 does not have to try again—it simply waits for the memory 
to become free. Processor 1 determines that the bus is once 
more free. 
Initially, the arbiter is in an idle state with both request 
inputs inactive-high. Therefore, both D inputs to latches la 
and 2a are high and in a steady-state condition. Outputs AA, 
BB, Grant 1, and Grant2 are all high. 
1 1 8 
Chapter 3 Sequential logic 
Unconditonal PRE 
' 
1 v 
preset 
| 
— 
P-i 
Unconditonal rrr 
| 
"V 
I 
V 
_ _ 
clear 
C L R - | j 
\_^P 
" [_jP~] 
* Q 
I 
RSlatcnl 
[ 
Clock 
1 
~V-| 
L-y3"^ 
*" 
I 
1 ' 
I 
RS latch 3 
RS latch 2 
Tiie master capture; the input 
i he sia\'e holds the ouipui 
s 
_J) p Q]"H V-1—p 
Q]—*"Q 
R ^ ~ l O 
1R 
Qrrj-r^vj—|R 
Qr- 
• Q 
Master 
' H ' 
Slave 
Clock——1 
[ > o 1 Clock 

3.3 Clocked flip-flops 
119 
D input to 
flip-flop 
Clock input 
_ 
Q input of 
level-sensitive 
flip-flop 
— 
Q output of 
positive-edge 
triggered 
_ 
flip-flop 
Q output of 
negative-edge 
triggered 
_ 
flip-flop 
Q output of 
master-slave 
flip-flop 
— 
u 
f] 
Figure 3.28 Comparison of 
flip-flop clocking modes. 
Address 
Data 
Requestl 
Grantl 
Processor 1 
JLLT. 
Local 
memory 1 
DP RAM 
Requestl Request2 
Grantl Grant2 
IA; 
Local 
memory 2 
Address 
Data 
Request2 
Grant2 
Processor 2 
Figure 3.29 Two processors communicating via 
dual-ported RAM. 
Requestl 
Clock 
Request2 
£> 
_r 
-*D 
-+>CLK 
Latch 1a 
AA 
CLK 
£> 
~L 
{> 
~*D 
-*>CLK 
Latch 2a 
CLK 
BB 
-*D 
-+>CLK 
Pre 
Latch 1b 
CLK 
D 
*>CLK 
Pre 
Latch 2b 
CLK 
Grantl 
. Grant2 
Figure 3.30 An 
arbiter circuit. 

Figure 3.31 Timing diagram for 
Fig. 3.30. 
Suppose that Requestl and Request2 are asserted almost 
simultaneously when the clock is in a high state. This results in 
the outputs of both OR gates (A and B) going low simultan-
eously. The cross-coupled feedback inputs to the OR gates 
(Grantl and Grant2) are currently both low. 
On the next rising edge of the clock, the Q output of latch 
la (i.e. AA) and the Q output of latch 2a (i.e. BB) both go low. 
However, as latch 2a sees a rising edge clock first, its Q output 
goes low one half a clock cycle before latch l's output also 
goes low. 
When a latch is clocked at the moment its input is chang-
ing, it may enter a metastable1 state lasting for up to about 
75 ns before the output of the latch settles into one state or 
the other. For this reason a second pair of latches is used to 
sample the input latches after a period of 80 ns. 
One clock cycle after Request2 has been latched and out-
put BB forced low, the output of latch 2b, Grant2 goes low. Its 
complement, Grant2 is fed back to OR gate 1, forcing input A 
high. After a clock cycle AA also goes high. Because Grant2 is 
connected to latch lb's active-low preset input, latch lb is 
held in a high state. 
At this point, Grantl is negated and Grant2 asserted, per-
mitting processor 2 to access the bus. 
When processor 1 relinquishes the memory, Request2 
becomes inactive-high, causing first B, then BB and finally 
Grant2 to be negated as the change ripples through the 
arbiter. Once Grant2 is high, Grant2 goes low, causing the 
output of OR gate 1 (i.e. A) to go low. This is clocked through 
latches la and lb to force Grantl low and therefore permit 
processor 1 to access the memory. Of course, once Grantl is 
asserted, any assertion of Request2 is ignored. 
3.4 The JK flip-flop 
The JK flip-flop can be configured, or programmed, to oper-
ate in one of two modes. All JK flip-flops are clocked and the 
majority of them operate on the master-slave principle. The 
truth table for a JK flip-flop is given in Table 3.5 and Fig. 3.32 
gives its logic symbol. A bubble at the clock input to a flip-
flop indicates that the flip-flop changes state on the falling 
edge of a clock pulse. 
Table 3.5 demonstrates that for all values of J and K, except 
J = K = 1, the JK flip-flop behaves exactly like an RS flip-flop 
with J acting as the set input and K acting as the reset input. 
When J and K are both true, the output of the JK flip-flop 
1 If a latch is clocked at the exact moment its input is changing state, it 
can enter a metastable state in which its output is undefined and it may 
even oscillate for a few nanoseconds. You can avoid the effects of metasta-
bility by latching a signal, waiting for it to settle, and then capturing it in 
a second latch. 
120 
Chapter 3 Sequential logic 
"JlJlJTJOJlJTJTJlJTJm^ 
». 4 
». 4 
» < 
». 4 
• ^ 
y 
Idle 
Contention 
Requests gets bus 
Idle 
Requestl 
Idle 
gets bus 
Requestl 
-N 
S~ 
^
—
^
<
> 
j 
' 
RequestZ 
A 
1 
A 
I 
M f s \ ^ 
r~^! 
N— 
\ ^\ 
\ \ \ 
r 
««—^k \ 
S r i — \ — — 
Grantl 
/ 
[_ 
— 
cv 
v^J 
Crant2 
V ^ | 
x 
A 
B 
AA 
BB 
Grant 1 
Crant2 
CLK 
CLK 

3.5 Summary of flip-flop types 
121 
Full form 
Algeb raic form 
Inputs 
Output 
Input; 
Output 
J 
K 
Q 
Q 
J 
K 
Q 
0 
0 
0 
0 
No change 
0 
0 
Q 
No change 
0 
0 
1 
1 
No change 
0 
1 
0 
Clear 
0 
1 
0 
0 
Reset Q 
1 
0 
1 
Set 
0 
1 
1 
0 
Reset Q 
1 
1 
Q 
Toggle 
1 
0 
0 
1 
SetQ 
1 
0 
1 
1 
SetQ 
1 
1 
0 
1 
Q+<-Q 
1 
1 
1 
0 
Q+<-Q 
Table 3.5 Truth table for a JK flip-flop. 
Falling-edge 
clock 
J 
*>Clk 
K 
Positive-edge triggered 
JK flip-flop. 
(b) 
Rising-edge 
f 
clock 
] 
>Clk 
K 
Negatitive-edge triggered 
JK flip-flop. 
Figure 3.32 Representation of the JK flip-flop. 
next section. Note that the T flip-flop is a JK flip-flop with 
J = K = 1, which changes state on each clock pulse (we don't 
deal with T flip-flops further in this text). 
We can derive the characteristic equation for a JK flip-flop 
by plotting Table 3.5 on a Karnaugh map, Fig. 3.33. This gives 
Q + = J Q + K Q . 
Figure 3.34 demonstrates how a JK flip-flop can be 
constructed from NAND gates and Fig. 3.35 describes a 
master-slave JK flip-flop. 
Q \ 
oo 
oi 
11 10 
0 
1 
Figure 3.33 Deriving the characteristic equation of a 
JK flip-flop. 
(1 
1) 
1 ) 
G 
Figure 3.34 Construction of a basic JK flip-flop. 
toggles, or changes state, each time the flip-flop is clocked. 
That is, if Q was a 0 it becomes a 1 and vice versa. It is this 
property that puts the JK flip-flop at the heart of many 
counter circuits, the operation of which is dealt with in the 
3.5 Summary of flip-flop types 
To understand flip-flops, it's necessary to appreciate that, 
unlike combinational circuits, they have internal states as 
well as external inputs; that is, the output of a flip-flop 
depends on the previous inputs of the flip-flop. Flip-flops 
are therefore memory elements. The most common forms of 
flip-flop are the D flip-flop, the RS flip-flop, and the JK flip-
flop. Each flip-flop has two outputs, Q and its complement 
Q, although the complementary output is not always con-
nected to a pin in an integrated circuit. Most flip-flops are 
clocked and have a clock input that is used to trigger the flip-
flop. Flip-flops often have unconditional preset and clear 
inputs that can be used the set or clear the output, respect-
ively. The term unconditional means that these inputs 
override any clock input. 
The D flip-flop D flip-flops have two inputs, a D (data) input 
and a C (clock) input. The output of a D flip-flop remains in 
its previous state until its C input is clocked. When its C input 
is clocked, the Q output becomes equal to D until the next 
time it is clocked. 
The RS flip-flop An RS flip-flop has two inputs, R (reset) and 
S (set). As long as both R and S are 0, the Q output of the 
RS flip-flop is constant and remains in its previous state. 
When R = 1 and S = 0, the Q output is forced to 0 (and 
RS flip-flop 
Q 
Q 
K -
C -
J " 

122 
Chapter 3 Sequential logic 
The master stage captures the input The slave stage copies the previous 
and holds it constant 
captured input to the output terminals 
and holds it constant while the next 
input is being captured 
Clock 
The invertor ensures that the master stage 
operates on a rising edge and the slave stage 
on a falling edge 
Figure 3.35 Circuit diagram of a 
master-slave JK flip-flop. 
remains at zero when R returns to 0). When S = 1 and R = 0, 
the Q output is forced to one (and remains at one when S 
returns to 0). The input conditions R = S = 1 produce an 
indeterminate state and should be avoided. Clocked RS flip-
flops behave as we have described, except that their R and S 
inputs are treated as zero until the flip-flop is clocked. When 
the RS flip-flop is clocked, its Q output behaves as we have 
just described. 
The JK flip-flop The JK flip-flop always has three inputs, J, K, 
and a clock input C. As long as a JK flip-flop is not clocked, its 
output remains in the previous state. When a JK flip-flop is 
clocked, it behaves like an RS flip-flop (where J = S, K = R) 
for all input conditions except J = K = 1. If J = K = 0, the 
output does not change state. If K = 1 and J = 0, the Q out-
put is reset to zero. If J = 1 and K = 0, the Q output is set to 
1. If both J and K are 1, the output changes state (or toggles) 
each time it is clocked. 
The T flip-flop The T flip-flop has a single clock input. Each 
time it is clocked, its output toggles or changes state. A T flip-
flop is functionally equivalent to a JK flip-flop with 
J = K = 1 . 
3.6 Applications of 
sequential elements 
Just as the logic gate is combined with other gates to form 
combinational circuits such as adders and multiplexers, flip-
flops can be combined together to create a class of circuits 
called sequential circuits. Here, we are concerned with two 
particular types of sequential circuit: the shift register, which 
moves a group of bits left or right and the counter, which steps 
through a sequence of values. 
3.6.1 Shift register 
By slightly modifying the circuit of the register we can build a 
shift register whose bits can be moved one place right every 
time the register is clocked. For example, the binary pattern 
01110101 
becomes 
00111010 after the shift register is clocked once 
and 
00011101 after it is clocked twice 
and 
00001110 after it is clocked three times, and so on. 
Note that after the first shift, a 0 has been shifted in from 
the left-hand end and the 1 at the right-hand end has been 
lost. We used the expression binary pattern because, as we 
shall see later, the byte 01110101 can represent many things. 
However, when the pattern represents a binary number, shift-
ing it one place right has the effect of dividing the number by 
two (just as shifting a decimal number one place right divides 
it by 10). Similarly, shifting a number one place left multiplies 
it by 2. Later we will see that special care has to be taken when 
shifting signed two's complement binary numbers right (the 
sign-bit has to be dealt with). 
Figure 3.36 demonstrates how a shift register is con-
structed from D flip-flops. The Q output of each flip-flop is 
connected to the D input of the flip- flop on its right. All clock 
inputs are connected together so that each flip-flop is clocked 
simultaneously. When the ith stage is clocked, its output, Qj, 
takes on the value from the stage on its left, that is, Qj <- Q,+). 
Data presented at the input of the left-hand flip-flop, D^, is 
shifted into the (m-l)th stage at each clock pulse. 
Figure 3.36 describes a right-shift register—we will look at 
registers that shift the data sequence left shortly. 
The flip-flops in a shift register must either be edge-
triggered or master-slave flip-flops, otherwise if a level-sensitive 
flip-flop were used, the value at the input to the left-hand 
• Q 
• Q 
Slave 
Master 
J-
K-

3.6 Applications of sequential elements 
123 
Clock 
Qm-1 
On each clock pulse 
data is copies to the 
next stage on the right 
Qo 
1 . 
i i 
, 
i 
t L 
D 
Q 
1 
D 
Q 
D 
Q 
i 
D 
0 
D 
Q 
1 
D 
Q 
D 
Q 
D 
0 
_ • C 
i \ 
— • C 
p* C 
r-* C 
V 
) 
Figure 3.36 The right-shift 
register. 
Clock 
Q4 
Q3 
Qo 
i . 
-
L 
i . 
' t 
, 
i 
D 
Q 
C 
D 
Q 
C 
D 
Q 
C 
D 
Q 
C 
D 
Q 
C 
j
—
* 
D 
Q 
C 
r—^ 
D 
Q 
C 
, D 
Q 
C 
D 
Q 
C 
p-*-
D 
Q 
C 
— 
i 
p 
Ilock 
1 
Q4 
Q3 
Q, 
Qi 
On 
01101 
00011 00001 
State 11010 01101 00110 00011 00001 00000 
Figure 3.38 Example of a five-stage 
shift-right register. 
stage would ripple through all stages as soon as the clock went 
high. We can construct a shift register from JK flip-flops just 
as easily as from RS flip-flops as Fig. 3.37 demonstrates. 
Figure 3.38 shows a five-stage shift register that contains 
the initial value 01101. At each clock pulse the bits are shifted 
right and a 0 enters the most-significant bit stage. This figure 
also provides a timing diagram for each of the five Q outputs. 
The output of the right-hand stage, Q0, consists of a series of 
five sequential pulses, corresponding to the five bits of the 
word in the shift register (i.e. 11010). 
The invertor ensures that Q m_i 
Qm-2 
Qm-3 
Qo 
the j , K input is 0, 1 or 1, 0 A 
A 
A 
Din-» 
J l 
o]—L-+TJ 
o]—i—>{1 
Q]—L-»- 
>T\ 
Q ] — L * 
|—*C 
p-fr C 
[-». C 
[-• C 
- p » -*• K 
Q 
»K 
Q 
• K 
Q 
•- 
• K 
Q 
Figure 3.37 Shift register 
Shift clock 
I 
composed of JK flip-flops. 
Qm-2 
Qm-3 

Figure 3.39 Serial to parallel converter. 
The D input of each 
stage in the shift register 
comes either from the 
previous stage or from 
an external input 
Figure 3.40 Shift register with a parallel load capability. 
A shift register can be used to convert a parallel word of 
m bits into a serial word of m consecutive bits. Such a circuit 
is called a parallel to serial converter. If the output of an m-bit 
parallel to serial converter is connected to the Din input of an 
m-bit shift register, after m clock pulses the information in 
the parallel to serial converter has been transferred to the 
second (right-hand) shift register. Such a shift register is 
called a serial to parallel converter and Fig. 3.39 describes a 
simplified version. In practice, a means of loading parallel 
data into the parallel-to-serial converter is necessary (see 
Fig. 3.40). There is almost no difference between a parallel to 
serial converter and a serial to parallel converter. 
A flaw in our shift register (when operating as a parallel to 
serial converter) is the lack of any facilities for loading it with 
m bits of data at one go, rather than by shifting in m bits 
through Din. Figure 3.40 shows a right-shift register with a 
parallel load capacity. A two-input multiplexer, composed of 
two AND gates, an OR gate, and an inverter switches a flip-
flop's D input between the output of the previous stage to the 
left (shift mode) and the load input (load mode). The control 
inputs of all multiplexers are connected together to provide 
the mode control, labeled load/shift. When we label a variable 
namel/name2, we mean that when the variable is high it 
carries out action namel and when it is low it carries out 
action name!. If load/shift = 0 the operation performed 
is a shift and if load/shift = 1 the operation performed is a 
load. 
Constructing a left-shift register with 
JK flip-flops 
Although we've considered the right-shift register, a left-shift 
register is easy to design. The input of the ith stage, Dp is 
Parallel load inputs 
Shift clock 
Load/shift 
1 2 4 
Chapter 3 Sequential logic 
Parallel output 
4 
x. 
». 
Serial data 
Qm-l 
Q/n-z 
Q 0 
\ 
t 
4 
t 
npUt 
>D 
Q 
>D 
Q 
—* D 
Q 
3 4 
• D 
Q —A—>D 
Q —A- 
—• D 
Q •* 
|-frC 
r - * C 
[-+• C 
r—>C 
r - * C 
[-+• C 
Shift 
, 
clock 
Parallel to serial converter 
\L 
Serial to parallel converter  
Note: A real parallel to serial register would 
^ - - ^ 
have a means of loading parallel data into it 
^ 
Only two lines are required 
to transmit serial data 
Input 
D 
Q -
C 
{D 
Q 
• c 
JD 
Q -
• C 
4D 
QT 
• C 
*[D 
Q 
•C 
Serial data 
Parallel output 
4 
X. 
». 
Qm-1 
Q^2 
Q0 
t 
t 
t 
QM 
D 
0: 
c 
Multiplexer 
0 
n 
c 

3.6 Applications of sequential elements 
125 
Stage (+1 
Stage / 
Stage i'-1 
Stage 1-2 
J 
Q 
C 
K 
Q 
„ J 
Q 
C 
K 
Q 
J 
Q 
C 
K 
Q 
J 
Q 
c 
K 
Q 
J 
Q 
C 
K 
Q 
J 
Q 
C 
K 
Q 
- • 
J 
Q 
C 
K 
Q 
J 
Q 
c 
K 
Q 
J 
Q 
C 
K 
Q 
J 
Q 
C 
K 
Q 
J 
Q 
C 
K 
Q 
J 
Q 
c 
K 
Q 
t 
_, 
\ 
\ 
Shift clock 
\ 
The input to stage / 
comes from the register 
on the right (i.e. stage /-1) 
Figure 3.41 The left-shift 
register. 
Shift type 
Shift left 
Shift right 
Original bit pattern before shift 
11010111 
11010111 
Logical shift 
10101110 
01101011 
Arithmetic shift 
10101110 
11101011 
Circular shift 
10101111 
11101011 
Table 3.6 The effect of logical, arithmetic, and circular shifts. 
connected to the output of the (i — i)th stage so that, at each 
clock pulse, Qj <— D; _,. In terms of the previous example 
01110101 
becomes 
11101010 after one shift left 
and 
11212100 after two shifts left 
The structure of a left-shift register composed of JK flip-
flops is described in Fig. 3.41. 
When we introduce the instruction set of a typical computer 
we'll see that there are several types of shift (logical, arithmetic, 
circular). These operations all shift bits left or right—the only 
difference between them concerns what happens to the bit 
shifted in. So far we've described the logical shift where a 0 is 
shifted in and the bit shifted out at the other end is lost. In an 
arithmetic shift the sign of 2's complement number is preserved 
when it is shifted right (this will become clear when we intro-
duce the representation of negative numbers in the next chap-
ter). In a circular shift the bit shifted out of one end becomes the 
bit shifted in at the other end. Table 3.6 describes what happens 
when the 8-bit value 11010111 undergoes three types of shift. 
A typical shift register 
Figure 3.42 gives the internal structure of a 74LS95 parallel-
access bidirectional shift register chip. You access the shift regis-
ter through its pins and cannot make connections to the 
internal parts of its circuit. Indeed, its actual internal imple-
mentation may differ from the published circuit. As long as it 
behaves like its published circuit, the precise implementation of 
its logic function doesn't matter to the end user. The 74LS95 is a 
versatile shift register and has the following functions. 
Parallel load The four bits of data to be loaded into the shift 
register are applied to its parallel inputs, the mode control 
input is set to a logical one, and a clock pulse applied to the 
clock 2 input. The data is loaded on the falling edge of the 
clock 2 pulse. 
Right-shift A shift right is accomplished by setting the mode 
control input to a logical zero and applying a pulse to the 
clock 1 input. The shift takes place on the falling edge of the 
clock pulse. 
Left-shift A shift left is accomplished by setting the mode con-
trol input to a logical one and applying a pulse to the clock 2 
input. The shift takes place on the falling edge of the clock 
pulse. A left shift requires that the output of each flip-flop be 
connected to the parallel input of the previous flip-flop and 
serial data entered at the D input. 
Table 3.7 provides a function table for this shift register 
(taken from the manufacturer's literature). This table 
describes the behavior of the shift register for all combina-
tions of its inputs. Note that the table includes don't care 
values of inputs and the effects of input transitions (indicated 
by I andT). 
Designing a versatile shift register—an example 
Let's design an 8-bit shift register to perform the following 
operations. 
(a) Load each stage from 
an 8-bit data bus 
(b) Logical shift left 
(c) Logical shift right 
(d) Arithmetic shift left 
(e) Arithmetic shift right 
(f) Circular shift left 
(g) Circular shift right 
(parallel load) 
(0 in, MSB lost) 
(0 in, LSB lost) 
(same as logical shift left) 
(MSB replicated, LSB lost) 
(MSB moves to LSB position) 
(LSB moves to MSB position) 

1 2 6 
Chapter 3 Sequential logic 
Parallel inputs 
A 
Clock 1 
right shift 
Clock2 
left shift 
Outputs 
Figure 3.42 The left-shift register. 
Inputs 
Clocks 
Outputs 
Mode 
Clocks 
Serial 
Parallel inputs 
Outputs 
control 
X 
2(L) 
1(R) 
X 
A 
B 
c 
D 
Qa 
Qb 
Qc 
Qd 
1 
1 
X 
X 
X 
X 
X 
X 
QaO 
QbO 
QcO 
Qdo 
1 
i 
X 
X 
A 
B 
c 
D 
A 
B 
c 
D 
1 
i 
X 
X 
Qb 
Qc 
Qd 
D 
Qb„ 
Qc„ 
Qd, 
D 
0 
0 
1 
X 
X 
X 
X 
X 
QaO 
Qbo 
QcO 
Qdo 
0 
X 
1 
1 
X 
X 
X 
X 
1 
Qa„ 
Qb„ 
Qc, 
0 
X 
4-
0 
X 
X 
X 
X 
0 
Qan 
Qb„ 
Qc„ 
T 
0 
0 
X 
X 
X 
X 
X 
QaO 
Qbo 
QcO 
Qdo 
1 
0 
0 
X 
X 
X 
X 
X 
QaO 
Qbo 
QcO 
Qdo 
I 
0 
1 
X 
X 
X 
X 
X 
QaO 
Qbo 
QcO 
Qdo 
T 
1 
0 
X 
X 
X 
X 
X 
QaO 
Qbo 
QcO 
Qdo 
t 
1 
1 
X 
X 
X 
X 
X 
QaO 
Qbo 
QcO 
Qdo 
Notes 
1. Left-shift operations assume that Qb is connected to A, Qc to B, and Qd to C. 
2.x = don't care. 
3. i and T indicate high-to-low and low-to-high transitions, respectively. 
4. QJQ indicates the level at Qa before the indicated inputs were established. 
5. Q,„ indicates the level of Qa before the I transition of the clock. 
Table 3.7 Function table for a 74LS95 shift register. 
Mode 
control 
Serial 
input 
I
l
l 
I 
Qa 
Qb 
Qc 
Qd 
R 
Clk 
Q 
S 
Q 
s 
>Clk 
R 
R 
Clk 
Q 
s 
IS' 
Q 
Clk 
R 

3.6 Applications of sequential elements 
127 
Most-significant 
bit stage 
Three middle stages ; + 1, /, /-1 
h 
• 
Shift clock 
J 
J;+r 
ro>c 
K 
Q 
K. 
J 
Q 
r°t>c 
K 
Q 
2/+i 
> 
K, 
J 
Q 
rO>C 
K 
Q 
s-
J 
Q 
K 
Q 
9w 
2 / - 1 , 
Least-significant 
bit stage 
J 
j—oJ>C 
K 
Q 
-go 
D; 
v 
Parallel input 
Figure 3.43 End and middle stages of a shift register. 
The circuit is composed of eight master-slave JK flip-flops 
Stage i 
and has a clock input that causes operations (a)-(g) above to 
Parallel load 
be carried out on its falling edge. The circuit has five control 
Shift right 
inputs: 
Shift left 
J, = D, 
S = 0 
Ij = Qj+, S = 1, R = 1 
J,- = Qi-, S = 1, R = 0 
R 
When R = 1 shift right, when R = 0 shift left. 
S 
When S = 1 perform a shift operation, when S = 0 
a parallel load. 
L When L = 1 perform a logical shift (if S = 1). 
A 
When A = 1 perform an arithmetic shift (if S =1). 
C 
When C = 1 perform a circular shift (if S = 1). 
Assume that illegal combinations of L, A, and C cannot 
occur because only one type of shift can be performed at a 
time. Therefore, more than one of L, A, and C, will never be 
true simultaneously. 
For all eight stages of the shift register obtain algebraic 
expressions for J and K in terms of control inputs R, S, L, A, 
and C and the outputs of the flip-flops. 
Figure 3.43 illustrates five stages of the shift register. These 
are the end stages Q7 and Q0, the most-significant and least-
significant bit stages, respectively. A non-end stage Q„ 
together with its left-hand neighbor Qi+, and its right-hand 
neighbor Q,-_,, must also be considered. 
All stages except 0 and 7 perform the same functions: par-
allel load, shift right, and shift left. As the JK flip-flops always 
load from an external input or another stage, only the inputs 
J = 1, K = 0, or _J = 0, K = 1 have to be considered. 
Consequently, J = K and we need only derive expressions for 
J, as the corresponding values for K can be obtained from an 
inverter. 
Therefore, J,= SD, + S(RQ,+1 + R Q r l ) 
Stage 0 (LSB) 
Parallel load 
Shift right logical 
J„ = D0 S = 0 
J0 = Q, S = 1, R = 1, L 
Shift left 
1 
arithmetic J0 = Q, S = 1, R = 1, A = 1 
circular 
J„ = Q, S = 1, R = 1, C = 1 
logical 
Jo = 0 
S = 1, R = 0, L = 1 
arithmetic J0 = 0 
S = 1, R 
circular 
J„ = Q7 S = 1, R 
0, A = 1 
0, C = 1 
Therefore, J0 = S-Dj + S(R-L-Q, + RAQ, + R-C-Q, 
+ R-L-0 + RAO + RCQ 7) 
= S-Do + S(R LQ, + RAQ, + R-C-Q, 
+ RCQ 7) 
= SD0 + S(R-Q,(L + A + C)+ RC-Q7) 
Note: L + A -I- C = 1 
= S-D0 + S(R-Q, + R-C-Q7). 
Stage 7 (MSB) 
Parallel load 
Shift right 
logical 
arithmetic 
circular 
Shift left 
logical 
arithmetic 
circular 
J7 = D7 S = 0 
J7 = 0 
S = 1, R •• 1, L = 1 
J7 = Q7 S = 1, R = 1, A = 1 
J7 = Q0 S = 1, R = 1, C = 1 
J7 = Q6 S = 1, R = 0, L = 1 
J7 = Q6 S = 1, R = 0, A = 1 
J7 = Q6 S = 1, R = 0, C = 1 
I 
, D 7 
I 
Df-1 
_°!J 

128 
Chapter 3 Sequential logic 
Therefore, J7 = SD, + S(R-Lj) + RAQ 7 + R C Q 0 
+ RLQ„ + R AQ 6 + RCQ 6) 
= S D7 + S(RAQ7 + R C Q 0 
+ RQ 6 (L + A + C)) 
= SD7 + S(R(AQ7 + CQ0) + RQ6) 
3.6.2 Asynchronous counters 
A counter is a sequential circuit with a clock input and 
m outputs. Each time the counter is clocked, one or more of 
its outputs change state. These outputs form a sequence with 
N unique values. After the Nth value has been observed at the 
counter's output terminals, the next clock pulse causes 
the counter to assume the same output as it had at the start of 
the sequence; that is, the sequence is cyclic. For example, a 
counter may displaythesequence01234501234501 . . . or the 
sequence 9731097310973... 
A counter composed of m flip-flops can generate an arbit-
rary sequence with a length of not greater than 2m cycles 
before the sequence begins to repeat itself. 
One of the tools frequently employed to illustrate the oper-
ation of sequential circuits is the state diagram. Any system 
with internal memory and external inputs such as the flip-
flop can be said to be in a state that is a function of its internal 
and external inputs. A state diagram shows some (or all) of 
the possible states of a given system. A labeled circle repres-
ents each of the states and the states are linked by unidirec-
tional lines showing the paths by which one state becomes 
another state. 
Figure 3.44 gives the state diagram of a JK flip-flop that has 
just two states, S0 and S,. S0 represents the state Q = 0 and 
Sj represents the state Q = 1. The transitions between states 
S0 and S, are determined by the values of the JK inputs at the 
time the flip-flop is clocked. In Fig. 3.44 we have labeled the 
flip-flop's input states C, to C4. Table 3.8 defines the four pos-
sible input conditions, C,, C2, C3, and C4, in terms of J and K. 
From Fig. 3.44 it can be seen that conditions C3 or C4 cause 
a transition from state S0 to state S,. Similarly, conditions 
C2 or C4 cause a transition from state S, to state S0. Condition 
C4 causes a change of state from S0 to St and also from St to S0. 
This is, of course, the condition J = K = 1, which causes the 
JK flip-flop to toggle its output. Some conditions cause a state 
to change to itself, that is, there is no overall change. Thus, 
conditions C, or C2, when applied to the system in state S0, 
have the effect of leaving the system in state S0. 
The binary up-counter 
The state diagram of a simple 3-bit binary up-counter is given 
in Fig. 3.45 (an up-counter counts upward 0, 1, 2, 3 , . . . in 
contrast with a down-counter, which counts downward 
3,2,1,0). In this state diagram, there is only a single path from 
each state to its next higher neighbor. As the system is clocked, 
it cycles through the states S0 to S7 representing the natural 
binary numbers 0 to 7. The actual design of counters in gen-
eral can be quite involved, altJiough the basic principle is to 
ask 'What input conditions are required by the flip-flops to 
cause them to change from state S, to state Sj+1?' 
The design of an asynchronous natural binary up-counter 
is rather simpler than the design of a counter for an arbitrary 
sequence. Figure 3.46 gives the circuit diagram of a 3-bit 
binary counter composed of JK flip-flops and Fig. 3.47 pro-
vides its timing diagram. The J and K inputs to each flip-flop 
J 
K 
Condition 
0 
0 
c. 
0 
1 
Q 
1 
0 
c3 
1 
1 
Q 
Table 3.8 Relationship between J K inputs 
and conditions C, to C4. 
C, + C. 
Ci + C; 
A line from a state back to 
itself indicates that the 
corresponding condition 
does not cause a change 
of state 
C1 + C3 
Lines with arrows indicate a change of state. 
The boolean equation indicates the condition 
that causes this state transition 
Figure 3.44 The state diagram of a JK flip-flop. 
(Q = 0) 
(Q = 0) 
c2+c4 • 

3.6 Applications of sequential elements 
129 
Figure 3.45 The state diagram of; 
binary 3-bit up-counter. 
Qo 
i Clock-
»o>C 
- 1 -
4 
Q - 1 
*o>C 
1 - * 
/ 
J 
Q 
J 
Q 
r 
>c 
K 
Q 
The J and K inputs 
of each filp-flop 
are connected to 
logical 1 levels to force 
the flip-flop to toggle 
on each clock pulse 
The Q output of each 
The flip-flops are 
flip-flop is connected 
negative-edge triggered 
to the clock input of the and change state on 
next stage 
the falling edge of 
the clock 
Figure 3.46 Circuit of an asynchronous binary up-counter. 
are connected to constant logical 1 levels. Consequently, 
whenever a flip-flop is clocked, its output changes state. The 
flip-flops are arranged so that the Q output of one stage trig-
gers the clock input of the next higher stage (i.e. the output Q; 
of stage i triggers the clock input of stage i +1). The flip-flops 
in Fig. 3.46 are master-slave clocked and their outputs change 
on the negative edge of the clock pulse. 
Consider the first stage of this counter. When the clock 
input makes a complete cycle (0 to 1 to 0), the Q output 
changes state on the falling edge of the clock. It takes two 
clock cycles to make the Q output execute one cycle; that is, 
the flip-flop divides the clock input by 2. 
The asynchronous binary counter of Fig. 3.46 is called a 
ripple counter because the output of the first stage triggers the 
input of the second stage, the output of the second stage trig-
gers the input of the third stage, and so on. Consequently, 
a change of state at the output of the first stage ripples through 
the counter until it clocks the final stage. The propagation 
delay through each stage of the counter determines its max-
imum speed of operation. The timing diagram of Fig. 3.47 
doesn't show the ripple effect—when one stage changes state, 
there's a short delay before stages to its right change state. 
Figure 3.48 demonstrates the construction of a four-stage 
binary up-counter in Digital Works. We have wired all J and 
K inputs together and connected them to Vcc (the positive 
power supply that provides a logical 1 to cause the JK flip-
flops to toggle when clocked). We have labeled each of the 
Q outputs and used the Logic History function to capture 
the output waveform. Digital Works clears all flip-flops at the 
start of each run. However, the flip-flops have two unlabeled 
set and clear inputs that can be used to preset outputs to 1 or 
0, respectively (these are not used in this application). 
The binary down-counter 
We can also create a binary down-counter that counts 
backwards from 7 to 0. Figure 3.49 demonstrates the effect of 
connecting the Q output of each stage in a ripple counter to the 
clock input of the next stage. You can also create a binary down-
counter by using JK flip-flops that are clocked on the positive or 
rising edge of the clock pulse by connecting Q; to Clk,+,. 
Designing an asynchronous decimal counter 
Let's design a 4-bit asynchronous ripple-through decimal 
counter to count from 0 to 9 cyclically. We use JK 
master-slave flip-flops with an unconditional active-low 
clear input. A decimal counter can be derived from a binary 
counter by resetting the counter to zero at the appropriate 
point. A four-stage binary counter counts from 0000 to 1111 
A 
s° i 
^-•H ooo 
r - \ 
I m i 
I ooi J 
V. no y 
I 010 J 
\y-—v. 
. 
^sV At each clock pulse, the system 
f 
e 
\ 
f 
c 
\ changes state; for example, if 
I 
* 
J 
I 
_A 
J the current state is S4 with the 
V 
J 
^ 
V 
J 
output 100, the next state will 
^ 
^ " ^ ^ f 
\ _ ^ - " " / — 
be S5 with the output 101 
\^ 100 J 
100 
^ 
y 
101 
I 
110 ) 
s7 
111 
So 
000 
001 
^—--/\ 
\ 
010 
011 j 

130 
Chapter 3 Sequential logic 
State 
Clock 
Qo 
Qi 
s 
0 
1 
5 1 
s 
2 
1 
3 
S 
1 
T 
4 
S 
1' 
5 
5 
1 ' 
6 
5 7 
1 
1 ' 
1 ' 
' 
' 
1 ' 
1 
1 
' 
' 
1 
' 
1 
' 
' 
' 
' 
' 
' 
' 
100 
101 
110 
1 
111 
' 
i oc K) 
001 
01 0 
Oil 
100 
101 
110 
1 
111 
000 
Figure 3.47 Timing diagram of an 
asynchronous 3-bit binary up-counter. 
l^^$^00^^Q\^f^wj^dwifii 
File 
Edit 
Circuit View 
Tools 
Help 
D 
E5 B 
- 
•.•;•• : 3 
: 
D I > H > E > i > ° 
D [> 
fr 
EH a 
E J 
n 
> 
O 
OB 00 
[^ 
| j 
o 
H 
as juui m 
?
•
•
•
•
•
• 
' 
-
File 
Options 
Help 
t> 
O 
OD 
m> 
z). 
"±j. 
A 
0 
cfock 
Off 
or 
02 
OS 
Cycle 28 completed 
JTJTJTJTJTJTJTJTJTJTJTJXnjTJTJT^^ 
Digital Works 
initializes flip-flops to 
Q = 0 at the start of 
a simulation. 
This is the symbol for 
a connection to a high 
level. When placed in 
the work area, it 
appears as Vcc and 
can be used to provide 
a logical 1 level. 
- • X1 
Figure 3.48 Using Digital Works to create a binary up-counter. 
U0® 
Q1« 
Q20 
Q3»-
'.'.'.'.'.,.'.'.'.'.'.'.'.r-jj 
, Qt4—-i i——jj 
QI-4—-j : r - i j 
G H - I : r — - j j 
, Q H - H : 
. , . clock*—: 
- — clock-, . 
.' 
clock- , , <-r -HcTock _ 
<--—clock-
•—l,K, 
Q.k . . .> —IK" 
a I- . . . ,• H,K" 
Q t . . . —I,K" 
Q -; . . . 
VCC—* 
* 
e 
•* 
.,.. 
Q2 
Count 

3.6 Applications of sequential elements 
131 
/Digital ^prk^?95r-GyPddwnGountciwm^ 
File 
Edit 
Circuit 
View 
Tools 
Help 
D (3 a : 
a 
D j> E> l> c*> D |> fr 
EI a 
in a 
i 
o H 
.:• 
O 
DD 
Dt> 
( ^ 
^ ) 
E H 
jUUl 
1 1 
+ 
° 
A 
9 
ciockO-
VCC-i-
clock -
K 
0. 
QOO: 
Q10 
Q20 
~nT—' ' 
,——F~i 
nT—' ' 
r—Tl 
Al' " ' i—— 
Q3' 
^clock _ 
flock _ 
flock -
K 
& 
. 
" 
X i 
File 
Options 
Help 
O 
O 
OD 
Di> 
£i 
"±J 
clock 
Off 
OB" 
Of 
Ol" 
Q2 
Of 
OS 
OS" 
JTJTJTJTJTJTJTJTJTJTJTJTJTJTJTJTJ^ 
Cycle 33 completed 
Figure 3.49 Using Digital Works to create a binary down-counter. 
(i.e. 0 to 15). To create a decade counter the state 10 (1010) 
must be detected and used to reset the flip-flops. Fig. 3.50 
provides a possible circuit. 
The binary counter counts normally from 0 to 9. On the 
tenth count Q3 = landQ, = 1. This condition is detected by 
the NAND gate whose output goes low, resetting the flip-
flops. The count of 10 exists momentarily as Fig. 3.51 demon-
strates. We could have detected the state 10 with Q3, Q2, Qp 
Q0 = 1010. However, that would have required a four-input 
gate and is not strictly necessary. Although Q3 = 1 and 
Q, = 1 corresponds to counts 10,11,14, and 15, the counter 
never gets beyond 10. 
The reset pulse must be long enough to reset all flip-flops 
to zero. If the reset pulse were too short and, say, Q, was reset 
before Q3, the output might be reset to 1000. The counting 
sequence would now be: 0, 1, 2, 3,4, 5,6, 7, 8,9, (10), 8,9, 8, 
9, 
However, such a problem is unlikely to occur in this 
case, because the reset pulse is not removed until at least the 
output of one flip-flop and the NAND gate has changed state. 
The combined duration of flip-flop reset time plus a gate 
delay will normally provide sufficient time to ensure that all 
flip-flops are reset. 
It is possible to imagine situations in which the circuit 
would not function correctly. Suppose that the minimum 
reset pulse required to guarantee the reset of a flip-flop were 
50 ns. Suppose also that the minimum time between the 
application of a reset pulse and the transition Q <— 0 were 
10 ns and that the propagation delay of a NAND gate were 
10 ns. It would indeed be possible for the above error to 
occur. This example demonstrates the dangers of designing 
asynchronous circuits! 
The pulse generator revisited 
When we introduced the RS flip-flop we used it to start and 
stop a simple pulse generator that created a train of n pulses. 
Figure 3.52 shows a pulse generator in Digital Works. This 
system is essentially the same as that in Fig. 3.9, except that 
we've built the counter using JK flip-flops and we've added 

132 
Chapter 3 Sequential logic 
Clock 
' When the counter reaches 1010, both 
Q-i and Q3 are 1, The NAND gate detects 
this condition and resets all flop-flops to 0 
Figure 3.50 Circuit of a 
decimal counter. 
Figure 3.51 Timing diagram of a decimal counter. 
LEDs to examine the signals produced when the system runs. 
Note also that the RS flip-flop can be set only when the 
flip-flop is in the reset mode. 
3.6.3 Synchronous counters 
Synchronous counters are composed of flip-flops that are all 
clocked at the same time. The outputs of all stages of a syn-
chronous counter become valid at the same time and the 
ripple-through effect associated with asynchronous counters 
is entirely absent. Synchronous counters can be easily 
designed to count through any arbitrary sequence just as well 
as the natural sequence 0,1, 2 , 3 , . . . . 
We design a synchronous counter by means of a state dia-
gram and the excitation table for the appropriate flip-flop 
(either RS or JK). An excitation table is a version of a flip-
flop's truth table arranged to display the input states required 
to force a given output transition. Table 3.9 illustrates the 
excitation table of a JK flip-flop. Suppose we wish to force the 
Q output of a JK flip-flop to make the transition from 0 to 1 
the next time it is clocked. Table 3.9 tells us that the J, K input 
should be 1, d (where d = don't care). 
*lc rniml-nr 
-*Ti 
o~-
4s counter 
— 
m 
Qo 
Qi 
Q2 
Q3 
u 
u 
——a 
' 
' 
o 
' 
CLR 
CLR 
I CLR 
I CLR 
Q1Q3I 
1 
1> 
1 
State ! S0 
S1 
S2 
S3 
S4 
S5 
S6 
S7 
S8 I 
S9 
S0 
aock [ n 
,r n 
Qo 
| 
- 
J 
Ql I 
I 
] 
I 
I t 
I xl~lWi\ 
; 
| 
I 
, 
I 
I 
| 
/ 
\ 
1/ 
\ J 
Although Q goes high 
! 
/ 
f 
\ 
when the counter 
i 
| 
n 
! / 
!\ 
reaches 10, it is reset 
Q2 
" 
\f 
\\ 
by the NANDgate 
^ C / 
The NAND gate 
Count i 0000 
0001 I 0010 
0011 
0100 
0101 
0110 
I 0111 
1000 
1001 
0000 
detects when the 
counter reaches 10 
Although Q goes high 
when the counter 
reaches 10, it is reset 
by the NAND gate 
Q 
>c 
1,1 rt 
*n 
Q"-
+ K 
Q 
—r . 
v -
*K3>C 
- • -' 
Q 
CLR 
Q 3Ql 
Q3 

3.6 Applications of sequential elements 
133 
i00ig^^QT^$5:g^&Ma^BGefi<^m?i 
file 
Edit 
Circuit View Tools 
Help 
DiS I 
' -. : <9 
j> I> I) D > » 
D r> -q- ES ea EJ ®] H o 
S 
O 
O ' II 
1> 
fo 
. K • 
l-lnlxl 
A f 
file 
Options 
Help 
iaiiij 
"ij 
Start 
PUISB 
train 
Oa 
Ob 
Oc 
Bun 
Cycle 31 completed 
-TTJTJTXLriTlJTJTJTJTJTJ-LriJTJT^^ 
n 
i—i 
n 
i 
1 
_n_n_n_ 
n _ _ 
JXTLTL-
Figure 3.52 Using Digital Works to design a pulse generator. 
Inputs 
Transition 
0->0 
0-»1 
1->0 
1-»1 
Table 3.9 Excitation table of a JK flip-flop. 
Why is the K input a don't care condition when we want a 
0 -» 1 transition? If we set J = 1 and K = 0, the flip-flop is set 
when it's clocked and Q+ becomes 1. If we set J = 1 and 
K = 1, the flip-flop is toggled when it's clocked and the out-
put Q = 0 is toggled to Q = 1. Clearly, the state of the K input 
doesn't matter when we wish to set Q + to 1 given that Q = 0 
and J = 1. It should now be clear why all the transitions in the 
JK's excitation table have a don't care input—a given state can 
be reached from more than one starting point. 
The next step in designing a synchronous counter is to 
construct a truth table for the system to determine the JK 
inputs required to force a transition to the required next state 
for each of the possible states in the table. It is much easier to 
explain this step by example rather than by algorithm. 
Let's design a synchronous binary-coded decimal or 
modulo-10 counter to count through the natural sequence 0, 
1,2,3,4,5,6,7,8,9,0, 
As there are 10 states, we require 
four JK flip-flops because 23 <10 <24. Table 3.10 provides 
a truth table for this counter. 
To understand Table 3.10 it's necessary to look along a line 
and to say, 'Given this state, what must the inputs of the flip-
flops be to force the transition to the next state?' For example, 
in the first line the current state is 0,0,0,0 and the next state 
is 0, 0, 0, 1. The values for the four pairs of J, K inputs are 
obtained from the excitation table in Table 3.9. Three of these 
outputs cause the transition 0 —» 0 and one causes the 
transition 0 -» 1. The J, K inputs required are 0, d for the 0 to 
0 transitions and 1, d for the 0 to 1 transition. 
From the truth table of the synchronous counter we can 
write down eight Karnaugh maps for the Js and Ks. 
clocks 
1 Pulse train O 
QaO 
QbQ 
Qcp 
i—fR 
(31—f— 
J 
I 
N , Qh' I—I J , 0 1 " ' I—I J , Ql—' 
" 
u 
' 
'—: 
clock 
clock _ 
' 
clock _ 
IS 
Qk 
!—IK, 
Qr- 
•—IK 
Qh 
—l,K, 
Qh 
V c c — I — 
1 
1 
A A 
.—px^ 
1 
L 
1 
1 l | 
RunO 
^
| 
1 
| i L-^ r A r V r ^ 
m 
H 
la 
Start 
Set pulse train length 
1—B 
J 
K 
0 
d 
1 
d 
d 
1 
d 
0 
Q 
Q 

134 
Chapter 3 Sequential logic 
Count 
Output 
Next state 
J,K inputs req uired to force transition 
Qd 
Qc 
Qb 
Qa 
Q„ 
Qc 
Qb 
Qa 
Jd 
Kd 
Jc 
Kc 
Jb 
Kb 
Ja 
Ka 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
d 
0 
d 
0 
d 
1 
d 
1 
0 
0 
0 
1 
0 
0 
1 
0 
0 
d 
0 
d 
1 
d 
d 
1 
2 
0 
0 
1 
0 
0 
0 
1 
1 
0 
d 
0 
d 
d 
0 
1 
d 
3 
0 
0 
1 
1 
0 
1 
0 
0 
0 
d 
1 
d 
d 
1 
d 
1 
4 
0 
1 
0 
0 
0 
1 
0 
1 
0 
d 
d 
0 
0 
d 
1 
d 
5 
0 
1 
0 
1 
0 
1 
1 
0 
0 
d 
d 
0 
1 
d 
d 
1 
6 
0 
1 
1 
0 
0 
1 
1 
1 
0 
d 
d 
0 
d 
0 
1 
d 
7 
0 
1 
1 
1 
1 
0 
0 
0 
1 
d 
d 
1 
d 
1 
d 
1 
8 
1 
0 
0 
0 
1 
0 
0 
1 
d 
0 
0 
d 
0 
d 
1 
d 
9 
1 
0 
0 
1 
0 
0 
0 
0 
d 
1 
0 
d 
0 
d 
d 
1 
10 
1 
0 
1 
0 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
11 
1 
0 
1 
1 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
12 
1 
1 
0 
0 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
13 
1 
1 
0 
1 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
14 
1 
1 
1 
0 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
15 
1 
1 
1 
1 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
The ds in the table correspond to don't care conditions in the excitation table of the JK flip-flop.The x's correspond to don't care conditions due to 
unused states; for example, the counter never enters states 1010 to 1111. There is, of course, no fundamental difference between x and d. We've 
chosen different symbols in order to distinguish between the origins of the don't care states. 
Table 3.10 Truth table for a synchronous counter. 
The clock triggers all flip-flops simultaneously 
Figure 3.54 Circuit diagram for 
a 4-bit synchronous BCD 
counter. 
Figure 3.53 gives the Karnaugh maps for this counter. These 
maps can be simplified to give 
Jd = Qc' Qb' Qa 
Kd = Qa 
Jc = Qb- Qa 
Kc = Q„ Q 
Jb = Qd" Qa 
Kb = Qa 
J a = l 
K a = l 
We can now write down the circuit diagram of the 
synchronous counter (Fig. 3.54). Remember that d denotes a 
don't care condition and indicates that the variable marked 
by a d may be a 0 or a 1 state. The same technique can be 
employed to construct a counter that will step through any 
arbitrary sequence. We will revisit this technique when we 
look at state machines. 
3.7 Introduction to state machines 
No discussion of sequential circuits would be complete with-
out at least a mention of state machines. The state machine 
offers the designer a formal way of specifying, designing, test-
ing, and analyzing sequential systems. Because the detailed 
study of state machines is beyond the scope of this intro-
ductory text, we shall simply introduce some of the basic 
concepts here. 
It would be impossible to find a text on state machines 
without encountering the general state machines called 
Mealy machines and Moore machines (after G. H. Mealy and 
E. Moore). Figure 3.55 illustrates the structure of a Mealy 
1 
1 
K 
Qa 
FFl 
C 
J 
Qa! 
K 
Qb 
FF2 
C 
Qb 
J 
J 
Qc 
FF3 
C 
K 
Qc 
K 
Qd 
FF4 
C 
J 
Q4 

3.7 Introduction to state machines 
135 
>PdQc 
QbQ\ 
00 
0 1 
11 
00 
01 
11 
10 
Jd=QcQbQa 
^QdQc 
QbQa 
00 
01 
11 
10 
Jc = QbQa 
>9dQc 
QbQa\ 
00 
01 
11 
10 
Jb = QdQ 
\QdQc 
QbQa 
00 
01 
11 
00 
01 
11 
10 
J a = 1 
10 
^ d Q c 
X 
d 
X 
d 
0 0 
X 
X 
X 
00 
01 
11 
10 
d 
X 
d 
X 
0 
d 
X 
x ) 
d 
X 
X 
00 
01 
11 
10 
X 
X 
1 
> 
1 
X 
ld 
d 
J 
X 
X 
X 
X 
d 
d 
X 
X 
10 
1 
1 
X 
"i 
• 
1 
<i 
X 
x 
; 
i 
. d 
c 
A 
A 
'•• 
' V 
i 
N 
x 
.••' 
' V 
QbQ>v 
00 
01 
11 
00 
01 
11 
QbQa 
10 
Kd = Qa 
\ Q d Q c 00 
01 
11 
00 
01 
11 
10 
Kc = QbQa 
\QdQc 
QbQa 
00 
01 
11 
10 
Kb = Qa 
sPdQc 
QbQa 
00 
01 
11 
10 
K 
= 1 
10 
d 
d 
X 
f' 
d 
X 
1 ^ 
l
d 
d 
X 
V 
d 
d 
X 
X 
10 
d 
X 
d 
d 
X 
d 
(< 
1 
X 
x) 
d 
X 
X 
00 
01 
11 
10 
d 
d 
X 
d 
' d 
d 
X 
d ^ 
K} 
1 
X 
V 
X 
X 
00 
01 
11 
10 
fr-
X 
• i 
• 
i 
1 
A 
! 
1 
Y 
:< 
\ d 
,i 
>. 
X • 
Figure 3.53 Karnaugh maps for a 
synchronous counter. 

136 
Chapter 3 Sequential logic 
Inputs 
Input logic 
(combinational) 
z 
/ 
This logic generates 
the next state 
Memory 
Clock 
-N 
Output logic 
(combinational) 
Outputs 
^> 
Figure 3.55 The Mealy state 
machine. 
This logic generates 
the next state 
Clock 
Figure 3.56 The Moore state 
machine. 
state machine and Fig. 3.56 the structure of a Moore state 
machine. Both machines have a combinational network that 
operates on the machine's inputs and on its internal states to 
produce a new internal state. The output of the Mealy 
machine is a function of the current inputs and the internal 
state of the machine, whereas the output of a Moore machine 
is a function of the internal state of the machine only. 
3.7.1 Example of a state machine 
As we have already said, the state machine approach to the 
design of sequential circuits is by no means trivial. Here, we 
will design a simple state machine by means of an example. 
Suppose we require a sequence detector that has a serial 
input X and an output Y. If a certain sequence of bits appears 
at the input of the detector, the output goes true. Sequence 
detectors are widely used in digital systems to split a stream of 
bits into units or frames by providing special bit patterns 
between adjacent frames and then using a sequence detector 
to identify the start of a frame. 
In the following example we design a sequence detector 
that produces a true output Y whenever it detects the 
sequence 010 at its X input. 
For example, if the input sequence is 
the output sequence will be 
000110011010110001011, 
000000000000100000010 
(the output generates a 1 in the state following the detection 
of the pattern). 
Figure 3.57 shows a black box state machine that detects 
the sequence 010 in a bit stream. We have provided input and 
output sequences to demonstrate the machine's action. 
We solve the problem by constructing a state diagram as 
illustrated in Fig. 3.58. Each circle represents a particular state 
of the system and transitions between states are determined 
by the current input to the system at the next clock pulse. 
A state is marked name/value, where name is the label we 
use to describe the state (e.g. states A, B, C, and D in Fig. 3.58) 
and value is the output corresponding to that state. The trans-
ition between states is labeled a/b, where a is the input condi-
tion and b the output value after the next clock. For example, 
the transition from state A to state B is labeled 0/0 and 
indicates that if the system is in state A and the input is 0, the 
next clock pulse will force the system into state B and set 
the output to 0. 
Figure 3.59 provides a partial state diagram for this 
sequence detector with details of the actions that take place 
during state transitions. State A is the initial state in Fig. 3.59. 
Suppose we receive an input while in state A. If input X is a 0 
we may be on our way to detecting the sequence 010 and 
therefore we move to state B along the line marked 0/0 (the 
output is 0 because we have not detected the required 
sequence yet). If the input is 1, we return to state A because we 
have not even begun to detect the start of the sequence. 
From state B there are two possible transitions. If we detect 
a 0 we remain in state B because we are still at the start of the 
desired sequence. If we detect a 1, we move on to state C (we 
have now detected 01). From state C a further 1 input takes us 
Input logic 
(combinational) 
N j Memory H 
Output logic 
(combinational) 
Outputs 
, 
h 

3.7 Introduction to state machines 
137 
This machine detects the 
input sequence 010 
Serial input X 
0 0 1 1 0 10 1 1 0 0 0 1 0 1 1 
j i _ n 
State machine 
Output Y 
0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 
n 
n_ 
Clock 
Figure 3.57 State machine to detect 
the sequence 010. 
Start 
Figure 3.58 State diagram for a 
010 sequence detector (X is the 
current input). 
Start 
If the system is 
in state A and the 
input is X = 0, the 
system moves to 
state B 
X = 0 
0/0 
If the system is 
in state A and the 
input is X = 1, the 
system remains 
in state A 
This is the initial state 
called A. The initial 
output is 0 
If the next input is 
X=1, we have detected 
the sequence 01 and 
we move to state C 
where we look for 0 
to complete the 
sequence 010 
The notation B/0 
indicates that this is 
state B with output 0 
The notation 0/0 
means that the 
input is 0 and the 
output is 0 
Figure 3.59 Details of the 
state counter diagram of 
Fig. 3.58. 
A/0 \ 
x = o 
0/0 
B/0 
X = 1 
1/0 
X = 1 
1/0 
C/0 J 
— 
W D/1 
x = o 
^
^
^
] 
__ 0/0 ^ ^ - - " ^ 
j 
X=1 
1/0 
x = o 
0/0 
x= f — ' 
1/0 
0/0 * \ 
{ B/0 
A/0 
X = 1 
1/0 
X=1 
1/0 

138 
Chapter 3 Sequential logic 
Current state 
Output 
A 
B 
C 
D 
Next state 
X - 0 
B 
B 
D 
B 
X = 1 
A 
C 
A 
A 
Table 3.11 State table for a 010 sequence detector. 
Current state 
Flip-flop outputs 
Output 
Next state 
Q, 
X - 0 
X - 1 
A 
B 
C 
D 
Qi 
0 
0 
1 
1 
0 
0 
1 
0 
0 
0 
1 
1 
0,1 
0,0 
0,1 
1,0 
1,1 
0,0 
0,1 
0,0 
Table 3.12 Modified state table for a sequence detector. 
Current state 
Next state 
Output 
Qi 
Qz 
X 
Qi 
0 
Qa 
1 
J, 
Ki 
h 
1 
Ka 
0 
0 
0 
Qi 
0 
Qa 
1 
0 
d 
h 
1 
d 
0 
0 
1 
0 
0 
0 
d 
0 
d 
0 
1 
0 
0 
1 
0 
d 
d 
0 
0 
1 
1 
1 
0 
1 
d 
d 
1 
1 
0 
0 
1 
1 
d 
0 
1 
d 
1 
0 
1 
0 
0 
d 
1 
0 
d 
1 
1 
0 
0 
1 
d 
1 
d 
0 
1 
1 
1 
0 
0 
d 
1 
d 
1 
Ji = Qi + QrX 
J2 = x 
K, = Q2 + X 
K2 = X 
Table 3.13 Determining the JK outputs of the sequence detector. 
Output = Q,QZ 
Figure 3.60 Circuit to detect 
the sequence 010. 
right back to state A (because we have received Oil). 
However, if we detect a 0 we move to state D and set die out-
put to 1 to indicate that the sequence has been detected. From 
state D we move back to state A if the next input is a 1 and 
back to state B if it is a 0. From the state diagram we can con-
struct a state table that defines die output and the next state 
corresponding to each current state and input. Table 3.11 
provides a state table for Fig. 3.58. 
3.7.2 Constructing a circuit to implement 
the state table 
The next step is to go about constructing the circuit itself. If a 
system can exist in one of several states, what then defines the 
current state? In a sequential system flip-flops are used to hold 
state information—in this example there are four states, 
which requires two flip-flops. 
Input X • 
Clock • 
K 
x-Qj 
^ x + (^ 
• c 
-fl 
, 0 
X 
• c 
•J<_2_ 
x_ 
Clock 
-
0 
0 
0 
1 

3.7 Introduction to state machines 
139 
f3m^M4^:M?^^^^MMWMc 
fjle 
Edit 
Circuit View Toots 
Help 
D & H ; , m m \ 3 \ 
D r> !> I> <>> D D° -cj- SB ® ts3 m i 
o s 
rTsra 
+ : ° 
A § 
t> 
o 
a> k bl 
• 1 
r> 7 
• 1 - 1 23^-31 
^ 
J1 
Q1 
' 
X 
/ 
X 
Q2 
23^-31 
^ 
J1 
Q1 
' 
X 
/ 
X 
Q2 
^ 
- J 
Q 
' 
X 
- J 
Q 
4 
. .^ 
' ^ A ' W 
•i 
. 1 , 1 , 
. 
, 
1 t 
4 
• 
/ 
/ 
4 
4 , 
< 
4 , 
< 
Q2< 
} nput)5»- -f 
Q 1 0 
Q2< • 
OutputO 
File 
.Options 
Help 
- • Ix; 
t> 
o 
JTJTJTJTJTJlJTJXiajTJXrTJTJTJTJTJ^ 
Cycle 32 oampleted 
Figure 3.61 Using Digital Works to implement the sequence detector. 
Table 3.12 expands Table 3.11 to represent internal states A 
to D by flip-flop outputs Q„ Q2 = 0, 0 to 1, 1. We next 
construct Table 3.13 to determine the JK input of each JK 
flip-flop that will force the appropriate state transition, given 
the next input X. Table 3.13 is derived by using the excitation 
table of the JK flip-flop (see Table 3.9). The final step is to cre-
ate a circuit diagram from Table 3.13 (i.e. Fig. 3.60). 
Figure 
3.61 demonstrates 
the construction 
of the 
sequence detector in Digital Works. We've added LEDs to show 
the state of the flip-flop outputs and control signals and have 
provided an example of a run. Note the output pulse after the 
sequence 010. We used the programmable sequence generator 
to provide a binary pattern for the test. 
Ml SUMMARY 
In this chapter we've looked at the flip-flop, which provides data 
storage facilities in a computer and which can be used to create 
counters and shift registers as well as more general forms of 
state machine. We have introduced the RS, D, and JK flip-flops. 
All these flip-flops can capture data and the JK flip-flop is able 
to operate in a toggle mode in which its output changes state 
each time it is clocked. Any of these flip-flops can be converted 
into the other two flip-flops by the addition of a few gates. 
We have also introduced the idea of clocking or triggering 
flip-flops. A flip-flop can be triggered by a clock at a given level 
or by the change in state of a clock. The master-slave flip-flop 
latches data at its input when the clock is high (or low) and 
transfers data to the output (slave) when the clock changes 
state. 
We have looked at the counter and shift register. The counter 
counts through a predetermined sequence such as the natural 
integers 0,1,2,3,.... A shift register holds a word of data and 
is able to shift the bits one or more places left or right. Shift 
registers are used to divide and multiply by two and to 
manipulate data in both arithmetic and logical operations. 
Counters and shift registers can be combined with the type of 
JnputX 
or 
GZ 
Output 

140 
Chapter 3 Sequential logic 
combinational logic we introduced in the previous chapter to 
create a digital computer. 
Sequential machines fall into two categories. Asynchronous 
sequential machines don't have a master clock and the output 
from one flip-flop triggers the flip-flop it's connected to. In a 
synchronous sequential machine all the flip-flops are triggered 
at the same time by means of a common master clock. 
Synchronous machines are more reliable. In this chapter we 
have briefly demonstrated how you can construct a 
synchronous counter and a machine that can detect a specific 
binary pattern in a stream of serial data. 
PROBLEMS 
3.1 What is a sequential circuit and in what way does it differ 
from a combinational circuit? 
R input 
S input 
Q output 
3.2 Explain why it is necessary to employ clocked flip-flops in 
sequential circuits (as opposed to unclocked flip-flops)? 
3.3 What are the three basic flip-flop clocking modes and why 
is it necessary to provide so many clocking modes? 
3.4 The behavior of an RS flip-flop is not clearly defined when 
R = 1 and S = 1. Design an RS flip-flop that does not suffer from 
this restriction. (Note: What assumptions do you have to 
make?) 
3.5 For the waveforms in Fig. 3.62 draw the Q and Q outputs 
of an RS flip-flop constructed from two NOR gates (as in 
Fig. 3.2). 
3.6 For the input and clock signals of Fig. 3.63, provide a 
timing diagram for the Q output of a D flip-flop. Assume that 
the flip-flop is 
(a) Level sensitive 
(b) positive edge triggered 
(c) negative-edge triggered 
(d) a master-slave flip-flop 
3.7 What additional logic is required to 
convert a JK flip-flop into a D flip-flop? 
Q output 
Figure 3.62 R and S inputs to an RS flip-flop. 
3.8 Assuming that the initial state of the 
circuit of Fig. 3.64 is given by C = 1, D = 1, 
P = 1, and Q = 0, complete the table. This 
question should be attempted by calculating 
the effect of the new C and D on the inputs to 
both cross-coupled pairs of NOR gates and 
therefore on the outputs P and Q.As P and Q 
are also inputs to the NOR gates, the change 
in P and Q should be taken into account when 
calculating the effect of the next inputs C and 
D. Remember that the output of a NOR is 1 if 
both its inputs are 0, and is 0 otherwise. 
D input to 
flip-flop 
Clock input 
Jl 
Figure 3.63 Timing diagram of a clock and data signal. 
Figure 3.64 Circuit for Question 3.8. 
Figure 3.65 Circuit for Question 3.9. 
D-
C-
Q 
• p 

3.7 Introduction to state machines 
141 
1 
1 
1 
0 
0 
0 
1 
1 
0 
1 
1 
1 
0 
1 
0 
0 
1 
0 
Modify the circuit to provide a new input S which, when 1, will 
at any time set P to 1 and Q to 0. Provide another input R that 
will similarly set P to 0 and Q to 1. Note that R and S cannot 
both be a 1 at the same time and therefore the condition 
R = S = 1 need not be considered. 
3.9 Demonstrate that the flip-flops in Fig. 3.65 are equivalent. 
Are they exactly equivalent? 
3.10 Many flip-flops have unconditional preset and clear inputs. 
What do these inputs do and why are they needed in sequential 
circuits? 
3.11 AT flip-flop has a single clock input and outputs Q and Q. 
Its Q output toggles (changes state) each time it is clocked. The 
T flip-flop behaves exactly like a JK flip-flop with its j and K 
inputs connected permanently to a logical one. Design a T flip-
flop using a D flip-flop. 
3.12 Why haven't D and RS flip-flops been replaced by the JK 
flip-flop, because the JK flip-flop can, apparently, do everything a 
D flip-flop or an RS flip-flop can do? 
3.13 What is a shift register and why is it so important in digital 
systems? 
3.14 Design a shift register that has two inputs, a clock input 
and a shift input. Whenever this register receives a pulse at its 
shift input, it shifts its contents two places right. 
3.15 Analyze the operation of the circuit of Fig. 3.66 by 
constructing a timing diagram (assume that Q0 and QT are 
initially 0). Construct the circuit using Digital Works and observe 
its behavior. 
3.16 Analyze the operation of the circuit of Fig. 3.67 by 
constructing a timing diagram (assume any initial value for Q0 to 
Q3). Construct the circuit using Digital Works and observe its 
behavior.This type of circuit is an important circuit in digital 
systems because it can be used to generate a pseudo random 
sequence; that is, the sequence of bits at its Q0 output look (to an 
Input 
Clock 
p- Output 
Figure 3.66 Circuit diagram for 
Question 3.15. 
r-K3>c 
4>— 
Shift clock 
Q3 
r*o>c 
<r 
J 
Q 2 
K 
Q 
r-K»c 
J 
Qi 
>c 
K 
Q 
r+°?c 
Qo 
Q 
Figure 3.67 Circuit diagram for 
Question 3.16. 
Clock 
_3>H 
Figure 3.68 Circuit diagram for 
Question 3.17. 
J 
Q 
K 
Ji 
Qi 
c 
h 
Q2 
K2 
c 
Q2 
h 
1 
h 
Q3 
c 
Q3 
K3 
C 
D 
P 
Q 
1 
0 
Jo 
Qo 
c 
Qo 
h 
K1 
Qi 
c 
Qi 
Ji 

142 
Chapter 3 Sequential logic 
Clock 
Figure 3.69 Circuit diagram for Question 3.18. 
Q 3 
Q2 
*c>c 
K 
Q 
Shift dock 
r * C > C 
K 
Q 
r * C > C 
K 
Q 
Qo 
I-M:>C 
K 
Q 
Figure 3.70 Circuit diagram of a Johnson 
counter. 
Load 
Data A 
DataB 
Enable 
inputs 
* Carry out 
Figure 3.71 Organization of a 74162 synchronous decade counter. 
Clock — 
Data C -
Data D -
Clear — 
1 -
1 -
•+j 
Q 
1 
* J 
Q —I 
• ] 
Q —I 
i 
• J 
Q — 
0 > C 
FF1 
i - K 5 > C 
FF2 
| - * 0 > C 
FF3 
j-KD>C 
FF4 
+-J 
Q" 
' 
>J 
~Q 
' 
M 
Q ' 
' 
M 
Q" 
QA 
QB 
Qc 
QD 

3.7 Introduction to state machines 
143 
*-Qb 
Clock 
Figure 3.72 Circuit diagram of a 
sequence processor. 
Figure 3.73 Circuit diagram of a sequence processor. 
observer) as if they constitute a random series of Is and Os. Longer 
sequences of random numbers are generated by increasing the 
number of stages in the shift register.The input is the exclusive OR 
of two or more outputs. 
3.17 Use Digital Works to construct the circuit of Fig. 3.68 and 
then investigate its behavior. 
3.18 Investigate the behavior of the circuit in Fig. 3.69. 
3.19 Explain the meaning of the terms asynchronous and 
synchronous in the context of sequential logic systems. What is 
the significance of these terms? 
3.20 Design an asynchronous base 13 counter that counts 
through the natural binary sequence from 0 (0000) to 
12 (1100) and then returns to zero on the next count. 
3.21 Design a synchronous binary duodecimal (i.e. base 12) 
counter that counts through the natural binary sequence 
from 0 (0000) to 11 (1011) and then returns to zero on 
the next count. The counter is to be built from four JK 
flip-flops. 
3.22 Design a synchronous modulo 9 counter using 
(a) JK flip-flops 
(b) RS flip-flops (with a master-slave clock). 
3.23 Design a programmable modulo 10/modulo 12 
synchronous counter using JK flip-flops. The counter 
has a control input,TEN/TWELVE, which when high, causes 
the counter to count modulo 10. When low, TEN/TWELVE 
causes the counter to count 
modulo 12. 
3.24 How would you determine the maximum rate at which a 
synchronous counter could be clocked? 
3.25 The circuit in Fig. 3.70 represents a Johnson counter. 
This is also called a twisted ring counter because 
feedback from the last (rightmost) stage is fed back 
to the first stage by crossing over the Q and Q connections. 
Investigate the operation of this 
circuit. 
3.26 Design a simple digital time of day clock that can display 
the time from 00:00:00 to 23:59:59. Assume that 
you have a clock pulse input derived from the public 
electricity supply of 50 Hz (Europe) or 
60 Hz (USA). 
3.27 Figure 3.71 gives the internal organization of a 74162 
synchronous decade (i.e. modulo 10) counter. 
Investigate its operation. Explain the function of the various 
control inputs. Note that the flip-flops are master-slave J Ks 
with asynchronous (i.e. unconditional) clear inputs. 
3.28 Design a modulo 8 counter with a clock and a 
control input UP. When UP = 1, the counter counts 0,1,2 
7. When UP = 0, the counter counts down 7,6, 5 
0. This 
circuit is a programmable up-/down-counter. 
3.29 Design a counter using JK flip-flops to count through the 
following sequence. 
Qa 
Qi 
Qo 
0 
0 
1 
0 
1 
0 
0 
1 
1 
1 
1 
0 
1 
1 
1 
sequence repeats 
1/0 
vo/ 
0 / 1 ^ 
0/1 
1/0 
1/0 
p/o 
0/0/" 
\ i y o 
0/0 
; ) c i y 
*\i 
FFi Q[—»-Qa 
| 
M 
FF2 Q -
-j>0 
fr-lK 
Q 
| _ _ V 
'—• * 
Q 
0 
0 
1 
n/o 
jvo 
0/1 
0/0 
input 
X 
so 
S4 
% 
s6 
S3 
s2 
Si 

144 
Chapter 3 Sequential logic 
i^KgiJal'^or^lSSSQUPquespZfiiywm? 
File 
Edit 
Circuit 
View 
Toois 
Help 
D & H 
:.• 
3 . 
D
^
D
^
^
D
f
r
^
^
S
E
a
l
o
Q 
S
I
B 
E> 
O 
BE l!f> 
fe 
^ 5 
IsH 
~- 
o 
inputo-
•-px> 
4-
3> 
clocks -
- J 
. Q 
Hciock -
H, Q 
-OOutput 
-OQa 
J , Q 
clock _ 
-©Qb 
j 
o 
MP* 5 
-OQc 
File 
Options JHelp 
> 
o 
B« C!> 
si 
-Ib'fxi 
~i] 
clock 
input 
Output 
Oc 
Ob 
Oa 
Cycle 30 completed 
jrriJTJTJTJxnjTJTJxruTJTjTJTJxru^^ 
Figure 3.74 A sequential circuit constructed with Digital Works. 
3.30 Investigate the action of the circuit in Fig. 3.72 when it is 
presented with the input sequence 111000001011111, where 
the first bit is the rightmost bit. Assume that all flip-flops are 
reset to Q = 0 before the first bit is received. 
3.31 Design a state machine to implement the state diagram 
defined in Fig. 3.73. 
3.32 Figure 3.74 provides a screen shot of a session using 
Digital Works. Examine the behavior of the circuit both by 
constructing it and by analyzing it. 

Computer arithmetic 
Because of the ease with which binary logic elements are manufactured and because of their 
remarkably low price, it was inevitable that the binary number system was chosen to represent 
numerical data within a digital computer. This chapter examines how numbers are represented in 
digital form, how they are converted from one base to another, and how they are manipulated 
within the computer.We begin with an examination of binary codes in general and demonstrate 
how patterns of ones and zeros can represent a range of different quantities. 
We demonstrate how computers use binary digits to implement codes that detect errors in 
stored or transmitted data and how some codes can even correct bits that have been corrupted. 
Similarly, we show how codes can be devised that reduce the number of bits used to encode 
information (e.g. the type of codes used to zip files). 
The main theme of this chapter is the class of binary codes used to represent numbers in digital 
computers. We look at how numbers are converted from our familiar decimal (or denary) form to 
binary form and vice versa. Binary arithmetic is useless without the hardware needed to 
implement it, so we examine some of the circuits of adders and subtractors. We also introduce 
error-detecting codes, which enable the computer to determine whether data has been corrupted 
(i.e. inadvertently modified). Other topics included here are ways in which we represent and 
handle negative as well as positive numbers. We look at the way in which the computer deals with 
very large and very small numbers by means of a system called floating point arithmetic. Finally, 
we describe how computers carry out multiplication and division—operations that are much 
more complex than addition or subtraction. 
We should stress that error-detecting codes, data compressing codes, and computer arithmetic 
are not special properties of the binary representation of data used by computers. All these 
applications are valid for any number base. The significance of binary arithmetic is its elegance and 
simplicity. 
CHAPTER MAP 
2 Logic elements and 
Boolean algebra 
This chapter introduces the basic 
component of the digital 
computer, the gate. We show 
how a few simple gates can be 
used to create circuits that 
perform useful functions. We also 
demonstrate how Boolean 
algebra and Karnaugh maps can 
be used to design and even 
simplify digital circuits. 
3 Sequential logic 
Computers use sequential 
circuits such as counters to step 
through the instructions of a 
program. This chapter 
demonstrates how sequential 
circuits are designed using the 
flip-flop. 
S The instruction set 
architecture 
This is the heart of the book and 
is concerned with the structure 
and operation of the computer 
itself. We examine the instruction 
set of a processor with a 
sophisticated architecture. 
4 Computer arithmetic 
We show lio-.v both positive and 
negative; numbers are 
represented in binary and how 
simple arithmetic operations arc 
implemented We also look at 
other aspects oi binary 
information such as 
error-detecting codes and fldta 
compression. Part of this c.hapre: 
IL devoted to the way in which 
multiplication and division is 
carried out. 
INTRODUCTION 

146 
Chapter 4 Computer arithmetic 
ACCURACY AND WORD LENGTH 
If I ask students what the advantages of a 32-bit wordlength 
over an 8-bit wordlength are, some will say that computers 
with long wordlengths are more accurate than computers 
with short wordlengths. 
This answer is incorrect. All computers are completely 
accurate unless they have failed. The answer confuses 
precision with accuracy. A computer with an 8-bit 
wordlength can represent one of 256 values, whereas a 
computer with a 32-bit wordlength can represent one 
of 4,294,967,296 values. The number of bits in a 
word indicates how precisely a value can be 
represented. 
An 8-bit computer can deal with any arbitrary wordlength. 
If you wish to represent integers to a precision of 1 in 32 bits 
on an 8-bit machine, you have to take four 8-bit words and 
concatenate them to form a 32-bit entity. When you add two 
32-bit values, you have to add each of the two pairs of four 
bytes. An m-bit computer can simulate an n-bit computer for 
any values of m and n and achieve exactly the same results as 
the n-bit machine. However, simulating, say, a 32-bit 
computer on a real 8-bit machine may seriously reduce 
performance (i.e. speed). A real 32-bit machine is faster than a 
simulated 32-bit machine. The trend toward longer 
wordlengths is about performance and not accuracy. 
4.1 Bits, bytes, words, and characters 
The smallest quantity of information that can be stored and 
manipulated inside a computer is the bit, which can take the 
value 0 or 1. Digital computers store information in the form 
of groups of bits called words. The number of bits per word 
varies from computer to computer. A computer with a 4-bit 
word is not less accurate than a computer with a 64-bit word; 
the difference is one of performance and economics. 
Computers with small words are cheaper to construct than 
computers with long words. Typical word lengths of com-
puter both old and new are 
Cray-1 supercomputer 
64 bits 
ICL 1900 series mainframe 
24 bits 
UNIVAC 1100 mainframe 
36 bits 
PDP-11 minicomputer 
16 bits 
VAX minicomputer 
32 bits 
The first microprocessor (4004) 
4 bits 
First-generation microprocessors 
8 bits 
8086 microprocessor 
16 bits 
Third-generation microprocessors 
32 bits 
Fourth-generation microprocessors 
64 bits 
Special-purpose graphics processors 
128 bits 
A group of 8 bits has come to be known as a byte. 
Today's microprocessors and minicomputers are byte ori-
ented with word lengths that are integer multiples of 8 bits 
(i.e. their data elements and addresses are 8,16,32, or 64 bits). 
A word is spoken of as being 2,4, or 8 bytes long, because its 
bits can be formed into two, four, or eight groups of 8 bits, 
respectively.1 
An «-bit word can be arranged into 2" unique bit patterns 
as Table 4.1 demonstrates for n — 1,2, 3, and 4. So, what do 
the n bits of a word represent? The simple and correct answer 
is nothing, because there is no intrinsic meaning associated 
with a pattern of Is and 0s. The meaning of a particular 
pattern of bits is the meaning given to it by the programmer. 
As Humpty Dumpty said to Alice in Through the Looking 
Glass, 'When I use a word,' Humpty Dumpty said, 'in a rather 
scornful tone, 'it means just what I choose it to mean— 
neither more nor less.' 
The following are some of the entities that a word may 
represent. 
An instruction An instruction or operation to be per-
formed by the CPU is represented by a binary pattern such as 
00111010111111111110000010100011. 
The 
relationship 
between the instruction's bit pattern and what it does is arbit-
rary and is determined by the computer's designer. A partic-
ular sequence of bits that means add A to B on one computer 
might have an entirely different meaning on another com-
puter. Instructions vary in length from 8 to about 80 bits. 
A numeric quantity A word, either alone or as part of a 
sequence of words, may represent a numerical quantity. 
Bits (n) Patterns 2" 
Values 
1 
2 
0,1 
2 
4 
00,01,10,11 
3 
8 
000,001,010,011,100,101,110,111 
4 
16 
0000,0001,0010,0011,0100,0101,0110, 
0111,1000,1001,1010,1011,1100,1101, 
1110,1111 
Table 4.1 The relationship between the number of bits in a word 
and the number of patterns. 
1 Some early computers grouped bits into sixes and called them bytes. 
Computer science uses flexible jargon where a term sometimes has differ-
ent meanings in different contexts; for example, some employ the term 
word to mean a 16-bit value and longword to mean a 32-bit value. Others 
use the term word to refer to a 32-bit value and halfword to refer to a 
16-bit value. Throughout this text we will use word to mean the basic 
unit of information operated on by a computer except when we are 
describing the 68K microprocessor. 

4.1 Bits, bytes, words, and characters 
147 
Numbers can be represented in one of many formats: BCD 
integer, unsigned binary integer, signed binary integer, BCD 
floating point, binary floating point, complex integer, com-
plex floating point, double precision integer, etc. The mean-
ing of these terms and the way in which the computer carries 
out its operations in the number system represented by the 
term is examined later. Once again we stress that the byte 
10001001 may represent the value —119 in one system, 137 in 
another system, and 89 in yet another system. We can think of 
a more human analogy. What is GIFT7. To a Bulgarian it 
might be their login password; to an American it might be 
something to look forward to on their birthday; to a German 
it is something to avoid because it means poison. Only the 
context in which GIFT is used determines its meaning. 
A character The alphanumeric characters (A to Z, a to z, 
0 to 9) and the symbols *,—,+,!,?, etc. are assigned binary 
patterns so that they can be stored and manipulated within 
the computer. The ASCII code (American Standard Code 
for Information Interchange) is widely used throughout 
the computer industry to encode alphanumeric characters. 
Table 4.2 defines the relationship between the bits of the 
ASCII code and the character it represents. This is also called 
the ISO 7-bit character code. 
The ASCII code represents a character by 7 bits, allowing a 
maximum of 27 =128 different characters. 96 characters are 
the normal printing characters. The remaining 32 characters 
are non-printing characters that carry out special functions, 
such as carriage return, backspace, line feed, etc. 
0 
1 
2 
000 
001 
01 
0 0000 
NULL 
DCL 
SP 
1 0001 
SOH 
DC1 
I 
2 0010 
STX 
DC2 
" 
3 0011 
ETX 
DC3 
# 
4 0100 
EOT 
DC4 
$ 
5 0101 
ENQ 
NAK 
% 
6 0110 
ACK 
SYN 
& 
7 0111 
BEL 
ETB 
' 
8 1000 
BS 
CAN 
( 
9 1001 
HT 
EM 
) 
A 1010 
LF 
SUB 
* 
B 1011 
VT 
ESC 
+ 
C 1100 
FF 
FS 
, 
D 1101 
CR 
cs 
-
E 1110 
SO 
RS 
F 1111 
SI 
US 
/ 
To convert an ASCII character into its 7-bit binary code, 
you read the upper-order three bits of the code from the col-
umn in which the character falls and the lower-order four bits 
of code from the row. Table 4.2 numbers the rows and 
columns in both binary and hexadecimal forms (we'll intro-
duce hexadecimal numbers shortly); for example, the ASCII 
representation of the letter 'Z' is given by 5A]6 or 10110102. 
Because most computers use 8-bit bytes, the ASCII code for 
'Z' would be 01011010. If you wish to print the letter 'Z' on a 
printer, you send the ASCII code for Z, 01011010, to the 
printer. 
The ASCII codes for the decimal digits 0,1,2,3,4,5,6,7,8, 
and 9, are 3016, 3116, 3216, 3316, 34i6, 3516, 3616, 3716, 38]6, and 
3916, respectively. For example, the symbol for the number 4 is 
represented by the ASCII code 001101002, whereas the binary 
value for 4 is represented by 000001002. When you hit the key 
'4' on a keyboard, the computer receives the input 00110100 
and not 00000100. Input from a keyboard or output to a dis-
play must be converted between the codes for the numbers 
and the values of the numbers. In high-level language this 
translation takes place automatically. 
The two left-hand columns of Table 4.2, representing 
ASCII codes 0000000 to 0011111, don't contain letters, num-
bers, or symbols. These columns are non-printing codes that 
are used either to control printers and display devices or to 
control data transmission links. Data link control characters 
such as ACK (acknowledge) and SYN (synchronous idle) are 
associated with communications systems that mix the text 
3 
4 
5 
6 
7 
011 
100 
101 
110 
111 
@ 
P 
1 
P 
A 
Q 
a 
q 
B 
R 
b 
r 
C 
S 
c 
s 
D 
T 
d 
t 
E 
U 
e 
u 
F 
V 
f 
V 
G 
W 
S 
w 
H 
X 
h 
X 
I 
Y 
i 
y 
J 
Z 
J 
z 
K 
[ 
k 
} 
L 
\ 
1 
1 
M 
1 
m 
} 
N 
A 
n 
-
O 
0 
DEL 
Table 4.2 The ASCII code. 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
< 
> 
7 

148 
Chapter 4 Computer arithmetic 
being transmitted with the special codes used to regulate the 
flow of the information. 
The 7-bit ASCII code has been extended to the 8-bit 
ISO 8859-1 Latin code to add accented characters such as 
A, 6, and e. Although suited to Europe and the USA, 
ISO 8859-1 can't deal with many of the World's languages. A 
16-bit code, called Unicode, has been designed to represent 
the characters of most of the World's written languages such 
as Chinese and Japanese. The first 256 characters of Unicode 
map onto the ASCII character set, making ASCII to Unicode 
conversion easy. The programming language Java has 
adopted Unicode as the standard means of character 
representation. 
A picture element One of the many entities that have to be 
digitally encoded is the picture or graphical display. Pictures 
vary widely in their complexity and there are a correspond-
ingly large number of ways of representing pictorial informa-
tion. For example, pictures can be parameterized and stored 
as a set of instructions that can be used to recreate the image 
(i.e. the picture is specified in terms of lines, arcs, and poly-
gons and their positions within the picture). When the pic-
ture is to be displayed or printed, it is recreated from its 
parameters. 
A simple way of storing pictorial information is to employ 
symbols that can be put together to make a picture. Such an 
approach was popular with the low-cost microprocessor sys-
tems associated with computer games where the symbols 
were called sprites. 
Complex pictures can be stored as a bit-map (an array of 
pixels or picture elements). By analogy with the bit, a pixel is 
the smallest unit of information of which a picture is com-
posed. Unlike a bit, the pixel can have attributes such as color. 
If we wish to store a 10 in X 8 in image at a reasonably high 
resolution of 300 pixels/in in both the horizontal and vertical 
axes, we require (10 X 300) X (8 X 300) = 7 200 000 pixels. 
If the picture is in color and each pixel has one of 256 differ-
ent colors, the total storage requirement is 8 X 7 200 000 bits, 
WHAT ARE NUMBERS? 
Before we introduce binary numbers, we need to say what we 
mean by numbers. The numbers we use to count things 
(i.e. 1,2,3,4,...) are called natural numbers and are whole 
numbers or integers. Natural numbers are so called because 
they don't depend on our mathematics (there are three stars 
in Orion's belt whether or not there are humans on the Earth 
to count them). The way in which we count is defined by the 
number system, which uses 10 special symbols to represent 
numbers. 
Not all numbers are natural. For example, we have invented 
negative numbers to handle certain concepts. We have real 
numbers, which describe non-integer values including 
or approximately 8 Mbytes. Typical high-quality color video 
displays have a resolution of 1600 by 1 200 (i.e. 221 pixels) per 
frame. These values explain why high-quality computer 
graphics requires such expensive hardware to store and 
manipulate images in real time. There are techniques for com-
pressing the amount of storage required by a picture. Some 
techniques operate by locating areas of a constant color and 
intensity and storing the shape and location of the area and its 
color. Other techniques such as JPEG work by performing a 
mathematical transformation on an image and deleting data 
that contributes little to the quality of the image. 
4.2 Number bases 
Our modern number system, which includes a symbol to 
represent zero, was introduced into Europe from the 
Hindu-Arabic world in about 1400. This system uses a posi-
tional notation to represent decimal numbers. By positional 
we mean that the value or weight of a digit depends on its 
location within a number. In our system, when each digit 
moves one place left, it is multiplied by 10 (the base or radix). 
Thus, the 9 in 95 is worth 10 times the 9 in 49. Similarly, a 
digit is divided by 10 when moved one place right (e.g. con-
sider 0.90 and 0.09). 
If the concept of positional notation seems obvious and not 
worthy of mention consider the Romans. They conquered 
most of the known world, invented Latin grammar, wrote the 
screenplays of many Hollywood epics, and yet their math-
ematics was terribly cumbersome. Because the Roman World 
did not use a positional system to represent numbers, each 
new large number had to have its own special symbol. Their 
number system was one of give and take so that if X = 10 and 
I = 1, then XI = 11 (i.e. 10+ l)andIX = 9 (i.e. 1 0 - l).The 
decimal number 1970 is represented in Roman numerals by 
MCMLXX (i.e. 1000 + (1000-100) + (50 + 10 + 10)). 
The Romans did not have a symbol for zero. 
fractions. Real numbers themselves are divided into rational 
and irrational numbers. A rational number can be expressed as 
a fraction (e.g. 7/12), whereas an irrational number can't be 
expressed as one integer divided by another. Paradoxically we 
can draw a line that has a finite length but we can't write 
down the length as a real number.Jf a square measures one 
inch by one inch, its diagonal is Vz inchesjong. You can draw 
the diagonal, but the irrational value of V 2 cannot be 
expressed by a finite number of digits in our number system. 
We have introduced these basic concepts because they 
have implications for the way in which computers process 
numeric information. 

4.2 Number bases 
149 
The number base lies at the heart of both conventional and 
computer arithmetic. Humans use base 10 and computers 
use base 2. We sometimes use other bases even in our every-
day lives; for example, we get base 60 from the Babylonians 
(60 seconds = 1 minute and 60 minutes = 1 hour). We can 
express the time 1:2:3 (1 hour 2 minutes 3 seconds) as 
1 X 60 X 60 + 2 X 60 + 3 seconds. Similarly, we occasion-
ally use the base 12 (12 = 1 dozen, 1 2 X 1 2 = 1 gross). 
Indeed, the Docenal Society of America exists to promote the 
base 12 (also called duodecimal). 
We now examine how a number is represented in a general 
base using positional notation. Integer N, which is made up 
of n digits can be written in the form 
The a;S that make up the number are called digits and can take 
one of b values (where b is the base in which the number is 
expressed). Consider the decimal number 821 686, where the 
six digits are aa = 6, ax = 8, a2 = 6, a3 = 1, a4 = 2, and 
a5 = 8, and these digits are taken from a set of 10 symbols 
{0to9}. 
The same notation can be used to express real values by 
using a radix point (e.g. decimal point in base 10 arithmetic or 
binary point in binary arithmetic) to separate the integer and 
fractional parts of the number. The following real number 
uses n digits to the left of the radix point and m digits to the 
right. 
an-l an-2 - a l 
a 0 a l a - 2 a ~ m 
The value of this number, expressed in positional notation in 
the base b is written, is defined as 
N = «?„_!&"-'... + a,b' + aob° + a_,b~' 
+ a_2b-2... + a _ b -
i = n - 1 
= 2a.b-
i = — m 
The value of a number is equal to the sum of its digits, each 
of which is multiplied by a weight according to its position in 
the number. Let's look at some examples of how this 
formula works. The decimal number 
1982 is equal 
to 1 X 103 + 9 X 102 + 8 X 101 + 2 X 10° (i.e. one thou-
sand + nine hundreds + eight tens + two). Similarly, 12.34 
is equal to 1 X 101 + 2 X 10° + 3 X 10_1 + 4 X 0^2. The 
value of the binary number 
10110.11 is given by 
1 X 24 + 0 X 23 + 1 X 22 +1 X 21 + 0 X 2° + 1 X 2"1 + 
1 X 2~2, or, in decimal, 16 + 4 + 2 + 0.5 + 0.25 = 22.75. 
Remember that the value of r° is 1 (i.e. any number to the 
power zero is 1). 
In base seven arithmetic, the number 123 is equal to the 
decimal 
number 
1 X 72 + 2 X 71 + 3 X 7° = 49 + 14 + 
3 = 66. Because we are talking about different bases in this 
chapter, we will sometimes use a subscript to indicate the base; 
for example, 1237 = 6610. 
We should make it clear that we're talking about natural 
positional numbers with positional weights of 1, 10, 100, 
1000,... (decimal) or 1, 2, 4, 8, 16, 32,... (binary). The 
weight of a number is the value by which it is multiplied by 
virtue of its position in the number. It's perfectly possible to 
have weightings that are not successive powers of an integer; 
for example, we can choose a binary weighting of 2, 4, 4, 2 
which means that the number 1010 is interpreted as 
1 X 2 + 0 X 4 + 1 X 4 + 0 X 2 = 6. 
We are interested in three bases: decimal, binary, and hexa-
decimal (the term hexadecimal is often abbreviated to hex). 
Although some texts use base-8 octal arithmetic, this base is 
ill-fitted to the representation of 8- or 16- bit binary values. 
Octal numbers were popular when people used 12-, 24- or 
36-bit computers. We do not discuss octal numbers further. 
Table 4.3 shows the digits used by each of these bases. Because 
the hexadecimal base has 16 digits, we use the letters A to F to 
indicate decimal values between 10 and 15. 
People work in decimal and computers in binary. We use 
base 10 because we have 10 fingers and thumbs. The hexa-
decimal system is used by people to handle computer arithm-
etic. By converting binary numbers to hexadecimal form (a 
very easy task), the shorter hexadecimal numbers can be 
more readily remembered. For example, the 8-bit binary 
number 10001001 is equivalent to the hexadecimal number 
89 which is easier to remember than 10001001. Because hexa-
decimal numbers are more compact than binary numbers (1 
hexadecimal digit = 4 binary digits), they are used in com-
puter texts and core-dumps. The latter term refers to a print-
out of part of the computer's memory, an operation normally 
performed as a diagnostic aid. 
There are occasions where binary numbers offer people 
advantages over other forms of representation. Suppose a 
computer-controlled chemical plant has three heaters, three 
valves, and two pumps, which are designated HI, H2, H3, VI, 
V2, V3, P1, P2, respectively. An 8-bit word from the computer 
is fed to an interface unit that converts the binary ones and 
zeros into electrical signals to switch on (logical one) or 
switch off (logical zero) the corresponding device. For 
example, the binary word 01010011 has the effect described 
in Table 4.4 when presented to the control unit: 
Decimal 
6=10 
a = {0,1,2,3,4, 5,6, 7,8,9} 
Binary 
6 = 2 
a = {0,1} 
Hexadecimal 
6 = 1 6 
a = {0,1,2,3,4, 5,6, 7,8,9,A,B,C,D, E,F} 
Table 4.3 Three number bases. 
fl,l-lfl^-2-«ia0 

150 
Chapter 4 Computer arithmetic 
Bit 
Value 
Component 
Action 
7 
0 
Heater 1 
off 
6 
1 
Heater 2 
on 
5 
0 
Heater 3 
off 
4 
1 
Valve 1 
on 
3 
0 
Valve 2 
off 
2 
0 
Valve 3 
off 
1 
1 
Pump 1 
on 
0 
1 
Pump 2 
on 
Table 4.4 Decoding the binary sequence 01010011. 
By inspecting the binary value of the control word, the sta-
tus of all devices is immediately apparent. If the output had 
been represented in decimal (83) or hexadecimal (53) the 
relationship between the number and its intended action 
would not have been so obvious. 
Now that we've looked at the structure of binary and 
decimal numbers, the next step is to consider how we convert 
a number in one base into its equivalent value in another 
base. 
4.3 Number base conversion 
It's sometimes necessary to convert numbers from one base 
to another by means of a pencil-and-paper method. This 
statement is particularly true when working with micro-
processors at the assembly language or machine code level. 
Computer users don't concern themselves with conversion 
between number bases, as the computer will have software to 
convert a decimal input into the computer's own internal 
binary representation of the input. Once the computer has 
done its job, it converts the binary results into decimal form 
before printing them. 
A knowledge of the effect of number bases on arithmetic 
operations is sometimes quite vital, as, for example, even the 
simplest of decimal fractions (say 1/10 = 0.1) has no exact 
binary equivalent. That is, a rational number expressed in a 
finite number of digits in one base may require an infinite 
number of digits in another base. Suppose the computer were 
asked to add the decimal value 0.110 to itself and stop when 
the result reaches 1. The computer may never stop because 
the decimal value 0.110 cannot be exactly represented by a 
binary number, with the result that the sum of 10 binary rep-
resentations of 0.1 is never exactly 1. The sum may be 
1.0000000000001 or 0.99999999999, which is almost as good 
as 1, but it is not the same as 1, and a test for equality with 1 
will always fail. 
4.3.1 Conversion of integers 
In this section we are going to demonstrate how integers are 
converted from one base to another. 
Decimal to binary To convert a decimal integer to binary, 
divide the number successively by 2, and after each division 
record the remainder, which is either 1 or 0. The process is 
terminated only when the result of the division is 0 remain-
der 1. In all the following conversions R represents the 
remainder after a division. 
RULES OF ARITHMETIC 
If there's one point that we would like to emphasize here, it's 
that the rules of arithmetic are the same in base x as they are 
in base y. All the rules we learned for base 10 arithmetic can 
be applied to base 2, base 16, or even base 5-arithmetic. For 
example, the base 5 numbers 1235 and 2215 represent, in 
decimal, 1 X 52 + 2 X 51 + 3 X 5° = 3810 and 
2 X 52 + 2 X 51 + 1 X 5° = 6110, respectively. Let's add both 
pairs of numbers together using the conventional rules of 
arithmetic in base 5 and base 10. 
Base 5 
Base 10 
123 
38 
+221 
+61 
344 
99 
If we add 1235 to 2215 we get 3445, which is equal to the decimal number 3 X 5 2 + 4 X 5 1 + 4 X 5 ° = 9910. Adding the 
decimal numbers 3810 and 6110 also gives us 9910. 

4.3 Number base conversion 
151 
HOW MANY BITS DOES IT REQUIRE TO REPRESENT A 
If we represent decimal numbers in binary form, we need 
to know how many binary digits are required to express an 
n-digit decimal number in binary form. For example, how 
many bits does it take to represent numbers up to 
90 000 000? The following explanation requires an 
understanding of logarithms. 
Suppose we require m bits to represent the largest n-digit 
decimal number, which is, of course, 99... 999 or 10"- I.We 
require the largest binary number in m bits (i.e. 1 1 . . . 111) to 
be equal to or greater than the largest decimal number in 
n bits (i.e. 99... 999) that is, 
1 0 n - 1 ^ 2 m - 1 , 
that is, 1 0 n « 2 m 
For example, 24510 becomes 
245 -- 2 = 122 
R = 1A 
122 -- 2 = 61 
R = 0 
61 -- 2 = 30 
R = 1 
30 -- 2 = 15 
R = 0 
15 -- 2 = 
7 
R = 1 
7 -- 2 = 
3 
R = 1 
3 -- 2 = 
1 
R = 1 
1 -- 2 = 
0 
R = 1 
Most-significant bit 
The result is read from the most-significant bit (the last 
remainder) upwards to give 24510 =111101012. 
Decimal to hexadecimal Decimal numbers are converted 
from decimal into hexadecimal form in exactly the same way 
that decimal numbers are converted into binary form. 
However, in this case the remainder lies in the decimal range 
0 to 15, corresponding to the hexadecimal range 0 to F. 
For example, 5324110 becomes 
53241 -=- 16 = 3327 
R = 9 
3327 4- 16 = 
207 
R = 1510 = F16 
207 4 16 = 
12 
R = 1510 = F16 
12 4 16 = 
0 
R = 1210 = Cie 
Therefore, 5324110 = CFF9I6. 
Binary to decimal It is possible to convert a binary num-
ber to decimal by adding together the requisite powers of two. 
This technique is suitable for relatively small binary numbers 
up to about 7 or 8 bits. 
For example, 110001112 is represented by 
128 64 32 16 
8 
4 
2 
1 
128 
1
1
0
0
0
1
1
1 
= 
64 
4 
2 
+ _1 
199 
CIMAL NUMBER? 
Taking logarithms to base 10 we get 
logl010"«logl02ro 
Note: log1010" = nlog1010 and log1010 = 1 
nlog1010=£mlog102 
n s mlog102 
m « 3.322„ 
In other words, it takes approximately 3.3n bits to 
represent an n-bit decimal number. For example, if we wish 
to represent decimal numbers up to 1 000 000 in binary, we 
must use at least 6 x 3.3 bits, which indicates a 20-bit word 
length. 
A more methodical technique is based on a recursive algo-
rithm as follows. Take the leftmost non-zero bit, double it, 
and add it to the bit on its right. Now take this result, double 
it, and add it to the next bit on the right. Continue in this way 
until the least-significant bit has been added in. The recursive 
procedure may be expressed mathematically as 
(flo+2(fl,+2(a2+---))) 
where the least-significant bit of the binary number 
isa0. 
For example, 10101112 becomes 
1 0 
1
0 
1
1
1 
2 L 
L 
•-•10 
10 
'—•20 
21 
43 
L+.86 
87 
Therefore, 10101112 = 8710. 
Hexadecimal to decimal The method is identical to the 
procedure for binary except that 16 is used as a multiplier. 
1 
1 
1 
0 
1 
0 
1 
21 
20 
42 
43 

152 
Chapter 4 Computer arithmetic 
For example, 24E16 becomes 
2 
4 
E 
L 
3_2 
36 
L 
576 
590 
Therefore, 24E16 = 59010. 
Binary to hexadecimal The binary number is formed into 
groups of 4 bits starting at the binary point. Each group is 
replaced by a hexadecimal digit from 0 to 9, A, B, C, D, E, F. 
For example, 11001011101 becomes 
Therefore, 110010111012 = 65D16. 
Hexadecimal to binary Each hexadecimal digit is replaced 
by its four bit binary equivalent. 
For example, 1234AF0C16 becomes 
Therefore, 
1234AF0C,, 
00010010001101001010111100001100,. 
4.3.2 Conversion of fractions 
The conversion of fractions from one base to another is car-
ried out in a similar way to the conversion of integers, 
although it's rather more tedious to manipulate fractions 
manually. Fortunately, it's rare to have to perform actual pen-
cil and paper conversion of fractions outside the classroom. 
One way of effectively abolishing fractions is to treat all frac-
tions as integers scaled by an appropriate factor. For example, 
the binary fraction 0.10101 is equal to die binary integer 
101012 divided by 25 (i.e. 32), so that, for example, 0.101012 is 
the same as 101012/25 = 21/32 = 0.65625. 
Converting binary fractions to decimal fractions 
The algorithm for converting binary fractions to their deci-
mal equivalent is based on the fact that a bit in one column is 
worth half the value of a bit in the column on its left. Starting 
at the rightmost non-zero bit, take that bit and halve it. Now 
add the result to the next bit on its left. Halve this result and 
add it to the next bit on the left. Continue until the binary 
point is reached. 
For example, consider the conversion of 0.011012 
decimal form. 
into 
0 
0 
1/2 
1/2 
1/4 
5/4 
5/6 
13/E 
13/16 
13/16 
13/32 
Therefore, 0.011012 = 13/32. 
Converting decimal fractions to binary fractions 
The decimal fraction is multiplied by 2 and the integer part 
noted. The integer, which will be either 1 or 0, is then stripped 
from the number to leave a fractional part. The new fraction 
is multiplied by two and the integer part noted. We continue 
in this way until the process ends or a sufficient degree of pre-
cision has been achieved. The binary fraction is formed by 
reading the integer parts from the top to the bottom as illus-
trated below. 
For example, 0.687510 becomes 
0 . 6 8 7 5 
X 2 -> 1.3750 
0.3750 
X 2 -> 0.7500 
0.7500 
X 2 -> 1.5000 
0 . 5 0 0 0 
X 2 —> 1.0000 
0.0000 
X 2 
ends the process 
Therefore, 0.6875 10 
= 0.10112. 
Now consider the conversion of 0.110 
0.1000 
X 2 -» 
0.2000 
0.2000 
X 2 -> 0 4000 
0.4000 
X 2 -» o 8000 
0.8000 
X 2 —» 
1 6000 
0.6000 
X 2 -y 
1 2000 
0.2000 
X 2 -> o 4000 
0 . 4 0 0 0 
X 2 —> o 8000 
0 . 8 0 0 0 
X 2 -» I 6000 
0.6000 
X 2 -» 
1 2000 
0.2000 
X 2 -> o 4000 
0.4000 
X 2 -> o 8000 
etc. 
Therefore, 0.110 = 0.00011001100 . . . 2 etc. As we pointed 
out before 0.110 cannot be expressed exactly in binary form 
with a finite number of bits. 
Converting between hexadecimal fractions and 
decimal fractions 
We can convert between hexadecimal fractions and decimal 
fractions using the same algorithms we used for binary con-
versions. All we have to change is die base (i.e., 2 to 16). 
110 
0101 
1101 
6 
5 
D 
0001 0010 0011 0100 1010 1111 0000 1100 
nto binary form. 
1 
1 
0. 

4.4 Special-purpose codes 
153 
Consider the following example where we convert 0.12316 
into a decimal fraction. 
3 
16 
3_5 
16 
j 
U 
3_5_, 
2 5 6 
2_9JL 
2 5 6 
4 0 9 6 J 
4.4.1 BCD codes 
A common alternative to natural binary arithmetic is called 
BCD or binary-coded decimal. In theory BCD is a case of hav-
ing your cake and eating it. We have already stated that com-
puter designers use two-state logic elements on purely 
economic grounds. This, in turn, leads to the world of binary 
arithmetic and the consequent problems of converting 
between binary and decimal representations of numeric 
quantities. Binary-coded decimal numbers accept the 
inevitability of two-state logic by coding the individual deci-
mal digits into groups of four bits. Table 4.5 shows how the 
10 digits, 0 to 9, are represented in BCD, and how a decimal 
number is converted to a BCD form. 
BCD arithmetic is identical to decimal arithmetic and 
differs only in the way the 10 digits are represented. The 
following example demonstrates how a BCD addition is 
carried out. 
Binary to hexadecimal fraction conversion and 
vice versa 
The conversion of binary fractions to hexadecimal bases is as 
easy as the corresponding integer conversions. The only point 
worth mentioning is that when binary digits are split into 
groups of four, we start grouping bits at the binary point and 
move to the right. Any group of digits remaining on the right 
containing fewer than 4 bits must be made up to 4 bits by the 
addition of zeros to the right of the least-significant bit. The 
following examples illustrate this point. 
Binary to hexadecimal 
0.101011002->0.1010 11002->0.AC16 
Binary to hexadecimal 
0.101011001-> 0.1010 1100 1(000)->O.AC816 
Hexadecimal to binary 
0.ABCi6->0.1010 1011 1100 —>0.1010101111002 
Numbers containing an integer part and a fraction part 
(e.g. 110101.11010 in base 2 or 123.125 in decimal) are con-
verted from one base to another in two stages. The integer 
part is converted and then the fractional part. 
4.4 Special-purpose codes 
Throughout this book a group of binary digits generally 
represents one of three things: a numerical quantity, an 
instruction, or a character. However, many different codes 
exist in the world of computing and digital systems, each of 
which is best suited to the particular job for which it was 
designed. 
Decimal 
BCD 
2634 
0010 0110 0011 0100 
1-3825 
+0011 1000 0010 0101 
6459 
0110 0100 0101 1001 
As you can see, the arithmetic is decimal with the digits 0 to 
9 represented by 4-bit codes. When 6 is added to 8 (i.e. 0110 
to 1000), the result is not the binary value 1110, but the 
decimal 6 + 8 = 14 = 01002 (i.e. 4) carry 1. 
Binary code 
BCD value 
0000 
0 
0001 
1 
0010 
2 
0011 
3 
0100 
4 
0101 
5 
0110 
6 
0111 
7 
1000 
8 
1001 
9 
1010 
Forbidden code 
1011 
Forbidden code 
1100 
Forbidden code 
1101 
Forbidden code 
1110 
Forbidden code 
1111 
Forbidden code 
Table 4.5 Binary-coded decimal. 

154 
Chapter 4 Computer arithmetic 
Although BCD makes decimal to binary conversion easy, it 
suffers from two disadvantages. The first is that BCD arith-
metic is more complex than binary arithmetic simply 
because the binary tables (i.e. addition, subtraction, multipli-
cation, and division) can be implemented in hardware by a 
few gates. The decimal tables involve all combinations of the 
digits 0 to 9 and are more complex. Today's digital technology 
makes these disadvantages less evident than in the early days 
of computer technology where each gate was an expensive 
item. 
BCD uses storage inefficiently. A BCD digit requires 4 bits 
of storage but only 10 symbols are mapped onto 10 of the 16 
possible binary codes making the binary codes 1010 to 1111 
(10 to 15) redundant and wasting storage. As we demon-
strated earlier, natural binary numbers require an average of 
approximately 3.3 bits per decimal digit. In spite of its dis-
advantages, BCD arithmetic can be found in applications 
such as pocket calculators or digital watches. Some micro-
processors have special instructions to aid BCD operations. 
There are other ways of representing BCD numbers in 
addition to the BCD code presented above. Each of these 
codes has desirable properties making it suitable for a partic-
ular application (e.g. the representation of negative num-
bers). These BCD codes are not relevant to this text. 
4.4.2 Unweighted codes 
The binary codes we've just described are called pure binary, 
natural binary, or 8421 weighted binary because the 8, 4, 2, 
and 1 represent the weightings of each of the columns in the 
positional code. These are not the only types of code avail-
able. Some positional codes don't have a natural binary 
weighting, other codes are called unweighted because the 
value of a bit doesn't depend on its position within a number. 
Each of the many special-purpose codes has properties that 
make it suitable for a specific application. One such 
unweighted code is called a unit distance code. 
HAMMING DISTANCE 
The Hamming distance between two words is the number of pi; 
following five pairs of words. 
Wordl 
00101101 
00101101 
Word 2 
00101100 
11101100 
Places different 
• 
• • 
• 
Hamming distance 
1 
3 
In a unit distance code, the Hamming distance between 
consecutive code words is equal to one, and no two consecu-
tive code words differ in more than one bit position. Natural 
binary numbers are not unit distance codes; for example, the 
sequential values 01112 = 7 and 10002 = 8 differ by a 
Hamming distance of four. The most widely encountered 
unit distance code is the Gray code, the first 16 values of 
which are given in Table 4.6. Figure 4.1 illustrates the timing 
diagrams of 4-bit binary and 4-bit Gray counters. As you can 
see, only one bit makes a change at each new count of the 
Gray counter. 
Decimal value 
Natural binary value 
Gray code 
DCBA 
DCBA 
0 
0000 
0000 
1 
0001 
0001 
2 
0010 
0011 
3 
0011 
0010 
4 
0100 
0110 
5 
0101 
0111 
6 
0110 
0101 
7 
0111 
0100 
8 
1000 
1100 
9 
1001 
1101 
10 
1010 
1111 
11 
1011 
1110 
12 
1100 
1010 
13 
1101 
1011 
14 
1110 
1001 
15 
1111 
1000 
Table 4.6 The 4-bit Cray code (an unweighted unit distance 
code). 
(i.e. positions) in which their bits differ. Consider the 
00101101 
00101101 
00101101 
11101101 
00100101 
11010010 
/ • 
/ 
/ • / / / • • • 
2 
1 
8 
Two m-bit words have a zero Hamming distance if they are the same and an m-bit distance if they are logical complements. 

(a) Binary code 
4.4 Special-purpose codes 
155 
A 
B 
C 
D 
(b) Cray code 
A 
B 
C 
D 
The Gray code is used by the optical encoder, a mechanism 
for converting the angle of a shaft or spindle into a binary 
value. An optical encoder allows you to measure the angular 
position of a shaft electronically without any physical con-
nection between the shaft and the measuring equipment. A 
typical example of an optical encoder is found in an auto-
mated weather reporting system. The direction from which 
the wind is blowing is measured by one of the World's oldest 
instruments, the weather vane. The weather vane is mounted 
on a shaft connected to an optical encoder, which provides 
the angle of rotation (i.e. wind direction) as a digital signal. 
Figure 4.2 shows an optical encoder using a natural binary 
code and Figure 4.3 shows the same arrangement but with 
a Gray-encoded disk. A transparent glass or plastic disk is 
attached to the shaft whose angular position is to be measured. 
As you can see, the disk is covered with concentric tracks, one 
for each of the bits in the code representing the position of the 
shaft. A 4-bit code might be suitable for a wind direction indi-
cator, whereas a 10-bit code may be required to indicate the 
position of a shaft in a machine. Each of these tracks is divided 
into sectors that are either opaque or transparent. 
A light source is located on one side of the disk over each 
track. A photoelectric sensor is located on the other side 
Figure 4.1 Sequencing 
though a 4-bit binary and 
a 4-bit Gray code. 
directly opposite each light source. For any position of the 
disk, a particular combination of the photoelectric cells 
detects a light beam, depending on whether or not 
there is a transparent sector between the light source and 
detector. 
A natural binary code can create problems when more 
than one bit of the output code changes as the shaft rotates 
from one code to the next. The photoelectric cells may not be 
perfectly aligned; the light source isn't a point source; and the 
edges of the sectors don't have perfectly straight edges. When 
the disk rotates from one sector to the next and two or three 
bits change state, one bit may change slightly before the other. 
For example, the change from the natural binary code 001 to 
010 might be observed as the sequence 001,000,010. Because 
the least-significant bit changes before the middle bit, the spu-
rious code 000 is generated momentarily. In some applica-
tions this can be very troublesome. Figure 4.3 demonstrates 
that a Gray-encoded disk has the property that only one bit at 
a time changes, solving the problems inherent in the natural 
binary system. Once the Gray code has been read into a digi-
tal system it may be converted into a natural binary code for 
processing in the normal way. The EOR gate logic of Fig. 4.4 
converts between Gray codes and natural binary codes. 

156 
Chapter 4 Computer arithmetic 
Bit 2 
Bit 1 
Binary 
output 
Sector 
Angle 
Binary code 
0 
0 - 45 
0 0 0 
^ 
1 
45- 90 
0 0 1 
j 
2 
90-135 
0 1 0 
' 
3 
135-180 
0 1 1 
4 
180-225 
1 0 0 
5 
225-270 
1 0 1 
6 
270-315 
1 1 0 
7 
315-360 
1 1 1 
Figure 4.2 A natural 
binary-encoded optical 
encoder. 
Binary 
output 
Sector 
Angle 
Binary code 
0 - 45 
45- 90 
90-135 
135-180 
180-225 
225-270 
270-315 
315-360 
0 0 0 
0 0 1 
0 1 1 
0 1 0 
1 1 0 
1 1 1 
1 0 1 
1 0 0 
Bit 1 
Figure 4.3 A Cray-encoded 
optical encoder. 
Cray-encoded output 
Figure 4.4 Converting binary codes to Gray codes and vice versa. 
Gray-encoded output 
J 3 
"2 
"1 
Natural binary input 
4.5 Error-detecting codes 
In an ideal world, errors don't occur. In reality a bit that 
should be a 1 sometimes gets changed into a 0, and a bit that 
should be a 0 sometimes gets changed into a 1. In any 
electronic system there are always unwanted random signals, 
collectively called noise which may interfere with the correct 
operation of the system. These random signals arise from a 
variety of causes, ranging from the thermal motion of elec-
trons in a digital system, to electromagnetic radiation from 
Spindle 
BitO 
Binary-encoded disk 
LIglll 
sources , 
0 
1 
2 
3 
4 
5 
6 
7 
Light 
sources 
BitO 
Bit 2 
g 4 
8 3 
82 
§1 
So 
b4 
b3 
b2 
b1 
b0 
b4 
b3 
b2 
bn 
b0 
g4 
§3 
§2 
§1 
So 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
Natural binary input 

4.5 Error-detecting codes 
157 
nearby lightning strikes and power line transients caused 
by the switching of inductive loads (e.g. starting motors in 
vacuum cleaners or elevators). The magnitude of these 
unwanted signals is generally tiny compared with digital sig-
nals inside the computer. The two electrical signal levels 
representing the zero and one binary states are so well sepa-
rated that one level is almost never spontaneously converted 
into the other level inside a digital computer under normal 
operating conditions. 
We can use some of the properties of binary numbers to 
detect errors, or even to correct errors. Suppose we take the 
binary pattern 01101011 and ask whether there is an error in 
it. We can't answer this question, because one binary pattern 
is just as good as another. Now consider the word "Jamuary. 
You will immediately realize that it should be spelt January, 
because there is no word 'Jamuary' in the English language. 
You can correct this spelling error because the closest valid 
word to 'Jamuary' is January. It's exactly the same with binary 
codes. 
Error-detecting codes, (EDCs), can detect that a word has 
been corrupted (i.e. changed). The subject of error-detecting 
codes is large enough to fill several textbooks. Here we look at 
threes codes. We also introduce the error-correcting code 
(ECC), which can correct one or more errors in a corrupted 
word. Of course the ECC is also an EDC whereas the EDC is 
not necessarily an ECC. 
Before we can discuss EDCs and ECCs we must introduce 
two terms: source word and code word. A source word is an 
unencoded string of bits and a code word is a source word 
that has been encoded. For example, the source code 10110 
might be transformed into the code word 111000111111000 
by triplicating each bit. 
In order to create an error-detecting code, we have to con-
struct a code in such a way that an error always leaves 
a noticeable trace or marker. An error-correcting code 
increases the length of the source code by adding one (or 
more) redundant bits, so called because they carry no new 
information. Figure 4.5 demonstrates how r redundant bits 
are added to an m-bit source word to create an (m + r)-bit 
code word. The redundant bits are also called check bits 
because they are used to check whether the code word is valid 
or not. Note that the check bits can be interleaved throughout 
the word and don't have to be located together as Fig. 4.5 
shows. 
WHERE DO WE FIND ECCs AND EDCs? 
Whenever digital signals are transmitted over a long distance 
by cables, their magnitude is diminished, making it possible for 
external noise signals to exceed the level of the digital signals 
and thus corrupt them.The effect of electrical noise is familiar 
to anyone who has tuned a radio or television to a distant 
station—the sound or picture is of a lower quality than when 
a local station is received. Whenever an error occurs in the 
reception of digital signals, it is important for the event to be 
detected so that a request for retransmission of the corrupted 
data can be made. 
ECCs and EDCs are also required by data storage 
technology. Some of the techniques used to store 
digital data are prone to errors (albeit with a very low 
probability); for example, DRAM chips are susceptible 
to errors caused by alpha particles due to radioactivity. 
ECCs and EDCs can determine whether data has been 
corrupted during the process of storage and 
retrieval. 
Error detection can be implemented by transmitting the 
desired digital information (i.e. the source word) plus one or 
more check bits whose value is a function of the information 
bits. Because check bits convey no new information, they are 
called redundant bits. 
At the receiving end of a data link the information bits are 
used to recalculate the check bits. If the received check bits 
are the same as those locally generated, error-free transmis-
sion is assumed—otherwise the receiver sends a message 
back to the transmitter asking it to repeat the lost data. 
If an error is detected in a word stored in memory, the error 
can't be corrected by asking the computer what the stored 
word should have been, because there is no other copy of the 
word. Consequently the operating system must be informed 
of the error and then left to take appropriate action (usually 
by aborting the current task). Memories that use error-
correcting codes are able to repair the damage done by an 
error before the word is passed to the computer. 
Source word 
Cll 
di|do 
Code word 
r redundant bits 
m data bits 
<tn-l 
di do 
The redundant bits 
are a function of the 
m data bits. 
Figure 4.S Encoding a source 
word. 

158 
Chapter 4 Computer arithmetic 
Source code 
Eight data bits 
«-= 
• 
00 1 1 0 1 0 0 1 
Figure 4.6 Creating a code word with even parity. 
110 
Hamming distance =1 
The black circles 
represent valid code 
words with even 
parity 
The blue circles 
represent invalid 
code words (error 
states) with 
odd parity 
Figure 4.7 A 3-bit error detecting code 
4.5.1 Parity EDCs 
The simplest error-detecting code is called A parity check code. 
We take an m-bit source word and append a parity bit to the 
source word to produce an (m + l)-bit codeword. The parity 
bit is chosen to make the total number of Is in the code word 
even (an even parity bit) or odd (an odd parity bit). We will 
assume an even parity bit here. 
Figure 4.6 shows an 8-bit source word, 01101001, which is 
converted into a 9-bit code with even parity. This binary 
string has a total of four Is, so the parity bit must be selected 
as 0 to keep the total number of Is even. Assuming that the 
parity bit is appended to the left-hand end of the string, the 
9-bit code word is 001101001. If, when this value is stored in 
memory or transmitted over a data link, any one of the bits is 
changed, the resulting parity will no longer be even. Imagine 
that bit 2 is changed from 0 to 1 and the code word becomes 
001101101. This word now contains five Is, which indicates 
an error, because the parity is odd. 
Figure 4.7 provides a graphical representation of a 2-bit 
source word with an even parity bit. Although 3 bits provide 
eight possible binary values, only four of them are valid code 
words with an even parity (i.e. the black circles representing 
codes 000,101,110,011). As you can see, a valid code word is 
separated from another nearest valid code word by two unit 
lengths. In Fig. 4.7 a unit length corresponds to an edge of the 
cube. If one of the valid codewords suffers a single error (i.e. 
Message 
Code word (even par ty) 
Code word (odd parity) 
000 
0 
000 
1 
000 
001 
1 
001 
0 
001 
010 
1 
010 
0 
010 
011 
0 
011 
1 
011 
100 
1 
100 
0 
100 
101 
0 
101 
1 
101 
110 
0 
110 
1 
110 
111 
1 
111 
0 
111 
x Odd partity bit 
Even parity bit 
111 
x Odd partity bit 
Table 4.7 Odd and even parity codes. 
only one bit changes), the codeword changes from one of the 
black circles to one of the white circles one unit length away. 
Because these code words all have an odd parity, you can 
always detect a single error. Two errors (or any even number 
of errors) cannot be detected because you move from one 
valid code word to another valid codeword. Fortunately, if 
one error is a rare event, two errors are correspondingly rarer 
(unless the nature of the error-inducing mechanism affects 
more than one bit at a time). 
If you detect an error, you must ask for the correct data to 
be retransmitted (if the corruption occurred over a data 
link). If the data was stored in memory, there's little you can 
do other than to tell the operating system that something has 
gone wrong. 
Table 4.7 gives the eight valid code words for a three-bit 
source word, for both even and odd parities. In each case the 
parity bit is the most-significant bit. 
As an example of the application of check bits consider a 
simple two-digit decimal code with a single decimal check 
digit. The check digit is calculated by adding up the two 
source digits modulo 10 (modulo 10 simply means that we 
ignore any carry when we add the digits; for example, the 
modulo 10 value of 6 + 7 is 3). If the two source digits are 4 
and 9, the code word is 493 (the check digit is 3). Suppose that 
during transmission or storage the code word is corrupted 
and becomes 463. If we re-evaluate the check digit we get 4 + 
6 = 1 0 = 0 (modulo 10). As the recorded check digit is 3, we 
know that an error must have occurred. 
4.5.2 Error-correcting codes 
We can design error-detecting and-correcting codes to both 
locate and fix errors. Figure 4.8 illustrates the simplest 3-bit 
error detecting and correcting code where only code words 
000 and 111 are valid. The Hamming distance between these 
two valid code words is 3. Suppose that the valid code word 
Parity bits 
HOO 
1 1 1 
o 
oifs 
000 
010\ 
001 
V 101 
0 1 1 0 10 0 1 

4.5 Error-detecting codes 
1 5 9 
000 
001 
The black circles represent 
the two valid code words 
000 and 111 
101 
The blue circles represent 
invalid codewords. Each 
invalid code word is one 
unit from the nearest valid 
code word and two units 
from the next valid code 
word 
possible code words 
2" possible code words 
Figure 4.8 A 3-bit error-correcting code. 
Figure 4.9 The principle of the EDC. 
3 unit distances between valid code words 
1 unit 
distance 
Valid code word 
1 unit 
. u distance ,,. 
>(•< 
~ 
K -
1 unit 
distance 
A sphere represents a 
region that encloses 
code words that are 
closer to the valid 
code word (the black 
circle) than to all other 
valid code words. 
Valid code word 
State A is closer to code word X 
than code word Y 
Figure 4.10 Minimum condition 
required to correct a single bit error. 
111 is stored in memory and later read back as 110. This code 
word is clearly invalid, because it is neither 000 nor 111. If you 
examine Fig. 4.8, you can see that the invalid code word 110 
has a Hamming distance of 1 from the valid code word 111 
and a Hamming distance 2 from the valid code word 000. An 
error correcting code selects the correct code as the nearest 
valid code word to the invalid code word. We assume that the 
valid code word was 111—we have now both detected an 
error and corrected it. 
How error-detecting codes work 
The idea behind EDCs is simple, as Fig. 4.9 demonstrates. An 
incorrect code word has to be made to reveal itself. Assume 
we transmit n-bit messages, where m bits are data bits and 
r=n~m 
bits are redundant check bits. Imagine an n-dimen-
sional space in which each point is represented by the value of 
an n-bit signal. This n-dimensional space contains 2" possible 
elements (i.e. all the possible combinations of n bits). 
However, an m-bit source code can convey 2m unique mes-
sages. In other words, only 2'" signals are valid out of the 2" 
possible signals. Should a code word be received that is not 
one of these 2'" values, an error may be assumed. 
If r check bits are added to the m message digits to create an 
n-bit code word, there are 2" = 2"'+r possible code words. 
The n-dimensional space will contain 2'" valid code words, 2" 
possible code words and 2"-2 m = 2m(2"-'"-l) = 2ra(2r-l) 
error states. 
If we read a word from memory or from a communication 
system, we can check its location within the n-dimensional 
space. If the word is located at one of the 2m valid points we 
assume that it's error free. If it falls in one of the 2"—2m error 
states, we can reject it. 
Error-correcting codes require that all valid code words be 
separated from each other by a Hamming distance of at least 3. 
An error-correcting code tries to correct an error by selecting 
the nearest valid code to the code word in error. Because valid 
codes are separated by a minimum of three units from each 
other, a single error moves a code word one unit from its cor-
rect value, but it remains two units from any other valid code 
word. Figure 4.10 illustrates this concept. 
Invalid states 
A, and B 
110 
111 
100 
011" 
010 

160 
Chapter 4 Computer arithmetic 
Block parity error-correcting codes 
The single parity-bit, error-detecting code can be extended to 
create a block EDC (also called a matrix EDC). A block EDC 
uses two types of parity check bit: a vertical parity bit and a 
horizontal (or longitudinal) parity bit. Imagine a block of 
data composed of a sequence of source words. Each source 
word can be written vertically to form a column and the 
sequence of source words can be written one after another to 
create a block. Figure 4.11 demonstrates a simple block of six 
3-bit source words. 
The source words are 110,101,001,110,101,and010 and 
have been written down as a block or matrix. We can generate 
a parity bit for each source word (i.e. column) and append it 
to the bottom of each column to create a new row. Each of 
these parity bits is called a vertical parity bit. Since a block of 
source words is made up of a number of columns, a parity 
word can be formed by calculating the parity across the bits. 
Each code word (i.e. column) in Fig. 4.12 is composed of four 
bits: D0, D|, D2, and D3 (where D3 is the vertical parity bit). 
We can now derive a horizontal parity bit by calculating the 
parity across the columns. That is, we create a parity bit across 
all the D0s. Horizontal parity bits for D1; D2, and the vertical 
parity bits, D3, can be generated in a similar way. Figure 4.12 
shows how the source words of Fig. 4.11 are transformed into 
a block error-detecting code. 
A vertical even parity bit has been appended to each col-
umn to create a new row labeled D3. Similarly, a horizontal 
parity bit has been added to each row to create a new column 
labeled word 7. 
Figure 4.13 demonstrates the action of a block error-
detecting code in the presence of a single error. A tick marks 
each row or column where the parity is correct and a cross 
marks where it is not. In this example, the bit in error is 
detected by the intersection of the row and column in which 
it creates a parity violation. Thus, although the word 1001 is 
received incorrectly as 1101 it can be corrected. Although the 
block parity code can detect and correct single errors, it can 
detect (but not correct) certain combinations of multiple 
error. Block EDCs/ECCs are sometimes found in data trans-
mission systems and in the storage of serial data on magnetic 
tape. 
By detecting a parity error in a row, we can detect the posi-
tion of the bit in error (i.e. in this case bit D,). By detecting a 
parity error in a column, we can detect the word in error 
(i.e. in this case word 3). Now we can locate the error, which 
is bit D, of word 3. The error can be corrected by inverting 
this bit. 
4.5.3 Hamming codes 
Hamming codes are the simplest class of error-detecting 
and-correcting codes that can be applied to a single code 
word (in contrast with a block error-correcting code that is 
applied to a group of words). A Hamming code takes an 
wi-bit source word and generates r parity check bits to create 
an «-bit code word. The r parity check bits are selected so that 
a single error in the code word can be detected, located, and 
therefore corrected. 
Bit 
Wordl 
Word 2 
Word3 
Word 4 
WordS 
Word 6 
Do 
0 
1 
1 
0 
1 
0 
D i 
1 
0 
0 
1 
0 
1 
D2 
1 
1 
0 
1 
1 
0 
Bit 
Wordl 
Word 2 
Word 3 
Word 4 
Words 
Word 6 
Word 7 
Do 
0 
1 
1 
0 
1 
0 
1 
D, 
1 
0 
0 
1 
0 
1 
1 
Dz 
1 
1 
0 
1 
1 
0 
0 
D3 
0 
0 
1 
0 
0 
1 
0 
Bit 
Wordl 
Word 2 
Word 3 
Word 4 
Word 5 
Word6 
Word 7 
Do 
0 
1 
1 
0 
1 
0 
1 
• 
D, 
1 
0 
1 
1 
0 
1 
1 
* 
D2 
1 
1 
0 
1 
1 
0 
0 
' 
0 
' 
D3 
0 
0 
1 
0 
0 
1 
0 
' 
0 
' 
• 
• 
X 
• 
• 
• 
y 
Figure 4.11 Six 3-bit words. 
Figure 4.12 Creating a block 
error-detecting code. 
Figure 4.13 Detecting and 
correcting an error in a block 
code. 

4.5 Error-detecting codes 
161 
Hamming codes are designated H„? m where, for example, 
H74 represents a Hamming code with a code word of 7 bits 
and a source word of 4 bits. The following sequence of bits 
represents a H7 4 code word: 
Bit p o s i t i o n 
7 
6 
5 
4 
3 
2 
1 
Code b i t 
I 4 
I 3 
i 2 
C3 
l x 
C2 
Cx 
I, = source bit i, Cj = check bit j . 
The information (i.e. source word) bits are numbered 
I,, I2, I3, and I4, and the check bits are numbered C,, C2, 
and C3. Similarly, the bit positions in the code word are 
numbered from 1 to 7. The check bits are located in 
binary positions 2' in the code word (i.e. positions 1, 2, 
and 4). Note how the check bits are interleaved with the 
source code bits. 
The three check bits are generated from the source word 
according to the following parity equations. 
C, = I 2©I 3©I 4 
C2 = I 1©I 3©I 4 
C1 = I 1©I 2©I 4 
For example, C3 is the parity bit generated by information 
bits I2,13, and I4, etc. Suppose we have a source word equal to 
14,1,, I2, ^ = 1,1,0,1. The check bits are calculated as 
c 3=o©i©i=o 
C 2=1©1©1=1 
C 1=1©0©1=0 
The code word is therefore 
I 4, 
I 3, 
I 2, C3, 11, 
C2, C1 = 1, 1, 0, 0, 1, 1, 0 
Suppose now that the code word is corrupted during storage 
(or transmission). Assume that the value of I3 is switched 
from 1 to 0. The resulting code word is now 1000110. Using 
the new code word we can recalculate the check bits 
to give 
C 3=0©0©1=1 
C2 = 1©0©1 = 0 
C1 = 1©0©1 = 0 
The new check bits are 1,0,0 and the stored check bits are 0, 
1, 0. If we take the exclusive OR of the old and new check 
bits we get 1 © 0,0 © 1,0 © 0 = 1,1,0. The binary value 110 
expressed in decimal form is 6 and points to bit position 6 in 
the code word. It is this bit that is in error. How does a 
Hamming code perform this apparent magic trick? The 
answer can be found in the equations for the parity check 
bits. The check bits are calculated in such a way that any 
single bit error will change the particular combination of 
check bits that points to its location. 
The Hamming code described above can detect and cor-
rect a single error. By adding a further check bit we can create 
a Hamming code that can detect two errors and correct one 
error. 
4.5.4 Hadamard codes 
Computer and communication systems designers employ a 
wide range of error-correcting codes and each code has its 
own particular characteristics. As the mathematics of error-
correcting codes is not trivial, we will demonstrate the con-
struction of an error-correcting code that can be appreciated 
without any math. 
A Hadamard matrix of order n is written [H]„ and has 
some very interesting properties. All elements in a Hadamard 
matrix are either 1 or — 1 (this is still a binary or two-state 
system because we can write —1 and 1 instead ofOand 1 with-
out loss of generality). The simplest Hadamard matrix is 
written [H]2 and has the following value: 
+ 1 
+1 
+1 
- 1 
An interesting property of the Hadamard matrix is that 
a In x 2M Hadamard matrix [H]2„ can be derived from the 
n x n Hadamard matrix [H]„ by means of the expansion 
+ [H]„ 
+[H]„ 
+ [H]„ 
-fH]„ 
That is, you just write down the matrix [HJ„ four times, with 
the appropriate signs in front of each matrix. Let's use this 
relationship to construct the Hadamard matrix of the order 
four, [H]4, from [H]2. All we have to do is write down [H]2 
four times. Each time a + [H]„ appears in the expression for 
[H]2„, we write the value of [H]2 and each time a — [H]„ 
appears in the expression we write the value of — [H]2 (i.e. 
[H]2 with all elements reversed). 
+1 
+1 
+1 
+1 
+1 
- 1 
+1 
- 1 
+1 
+1 
- 1 
- 1 
+1 
- 1 
- 1 
+1 
Can you see any pattern in the matrix emerging? Let's 
construct a Hadamard matrix of the order eight, [H]8, by 
taking the value of [H]4 and using the expansion to 
generate [H]g. 
+1 
+1 +1 +1 
+1 +1 
+1 +1 
+1 
-1 +1 
-1 
+1 
-1 + 1 -1 
+1 
+1 
-1 
-1 
+1 
+1 -1 
-1 
+1 -1 
-1 +1 
+1 
-1 
-1 
+1 
+1 +1 
+1 
+1 
-1 
-1 -1 
-1 
+1 
-1 
+1 
-1 
-1 +1 
-1 
+1 
+1 
+1 -1 
„1 
-1 
-1 
+1 
+1 
+1 
-1 
-1 +1 
-1 
+1 +1 
-1 

1 6 2 
Chapter 4 Computer arithmetic 
Source code 
Row 
8-bit code word 
000 
0 
+ 1 
+ 1 
+ 1 
+ 1 
+ 1 
+ 1 
+ 1 
+ 1 
001 
1 
+ 1 
- 1 
+ 1 
- 1 
+ 1 
- 1 
+ 1 
- 1 
010 
2 
+ 1 
+ 1 
- 1 
- 1 
+ 1 
+ 1 
- 1 
- 1 
Oil 
3 
+ 1 
- 1 
- 1 
+ 1 
+ 1 
- 1 
- 1 
+ 1 
100 
4 
+ 1 
+ 1 
+ 1 
+ 1 
- 1 
- 1 
- 1 
- 1 
101 
5 
+ 1 
- 1 
+ 1 
- 1 
- 1 
+ 1 
- 1 
+ 1 
110 
6 
+ 1 
+ 1 
- 1 
- 1 
- 1 
- 1 
+ 1 
+ 1 
111 
7 
+ 1 
- 1 
-1 
+ 1 
- 1 
+ 1 
+ 1 
- 1 
Table 4.8 Using a Hadamard matrix to generate an 8-bit codeword from a 
3-bit source code. 
1 unit 
1 unit 
1 unit 
1 unit 
.distancej.distance.|,distance, .distance^ 
4 unit distances between valid code words 
A sphere represents a 
region that encloses 
code words that are 
closer to the valid 
code word (the black 
circle)than to all other 
valid code words. 
Valid code word 
State A is closer to code word X 
than code word Y 
Valid code word 
If you inspect the rows of this Hadamard matrix of the order 
eight, you find that each row has a Hamming distance of 4 
from each of the other seven rows. The Hamming distance 
between any row and all other rows of a Hadamard matrix of 
the order n is nil. 
Let's now use the Hadamard matrix of the order eight to 
transform a 3-bit source code into an 8-bit code word. The 
matrix for [H]8 has eight rows, so that a 3-bit source code can 
be used to select one of the eight possible rows. Table 4.8 is a 
simple copy of an [H]8 matrix in which the rows of the matrix 
are numbered 0 to 7 to represent source codes 000 to 111. 
Suppose you want to encode the 3-bit source code 011 
using [H]8. The source word is 011, which corresponds to the 
8-bit code word +1 -1 -1 +1 +1 - 1 -1 +1 (or 10011001 
expressed in conventional binary form) on row 3 of Table 4.8. 
This code word has three information bits and jive redundant 
bits. These five redundant bits enable us to both detect and 
correct an error in the code word. 
The most important property of the Hadamard matrix of 
the order n is that each row differs from all other rows in 
exactly nil bit positions. The rows of an [H]8 matrix differ 
from each other in four bit positions; that is, the minimum 
Hamming distance between code words is 4. We have already 
demonstrated that a code with a minimum Hamming dis-
tance of 3 can both detect and correct errors. Consider, for 
example, the valid code words +1—1 —1 +1 +1 —1 —1 +1 
Figure 4.14 Adjacent code 
words in a 4-unit code. 
and +1 +1 +1 +1 —1 - 1 —1 —1. The first code word has a 
unit distance of 4 from the second code word. If the first code 
word were converted into the second code word, it might go 
through the intermediate error states +1 +1 —1 +1 
+1 - 1 -1 +1 , +1 +1 +1 +1 +1 -1 - 1 +1, +1 +1 +1 
+1 - 1 - 1 - 1 +l,and +1 +1 +1 +1 -1 - 1 - 1 - 1 . 
Let's look at this code in more detail. Figure 4.14 illustrates 
two adjacent valid codewords, X and Y, generated by a [H]8 
matrix, which are, of course, separated by a Hamming dis-
tance of 4. The intermediate error states between X and Y (i.e. 
the invalid codewords) are labeled A, B, and C. Each state is 
separated by 1 unit distance from its immediate neighbors. As 
you can see, error state A is closer to valid code word X than 
to the next nearest code word, Y. Similarly, error state C is 
closer to valid code word Y than to any other valid code word. 
Error state B is equidistant from the two valid code words and 
cannot be used to perform error correction. Two errors in a 
code word are therefore detectable but not correctable, 
because the resulting error state has a Hamming distance of 2 
from the correct state and 2 from at least one other valid state. 
Suppose that the code word 011 is transformed into the 
8 -bit Hadamard code 10011001 and an error occurs in storage 
(or transmission) to give a new incorrect value 10011101 (we 
have made an error in bit 2). We detect and correct the error 
by matching the new code word against all the valid code 
words. 
W / 
Invalid states 
A. B. and C 
are numbered 0 to 7 to represent source codes 000 to 111. 

4.6 Data-compressing codes 
163 
Code 
Code word 
000 
1 1 1 1 1 1 1 1 
10 0 1 1 1 0 1 Distance 3 
001 
10 10 10 10 
10 0 1 1 1 0 1 Distance 5 
010 
1 1 0 0 1 1 0 0 
10 0 1110 1 Distance 3 
011 
1 0 0 1 1 0 0 1 
10 0 1110 1 Distance 1 
100 
1 1 1 1 0 0 0 0 
10 0 1110 1 Distance 5 
101 
1 0 1 0 0 1 0 1 
10 0 1110 1 Distance 3 
110 
1 1 0 0 0 0 1 1 
10 0 1 1 1 0 1 
Distance 5 
111 
1 0 0 1 0 1 1 0 
10 0 1 1 1 0 1 Distance 3 
The word 10011101 has 
a Hamming distance of 
1 from the valid code 
10011001. Therefore, we 
assume that the code was 
011. 
Table 4.9 Correcting an error in a 
Hadamard code. 
Table 4.9 gives the Hamming distance between the code 
word 10011101 and each row of the Hamming matrix. The 
smallest Hamming distance is 1, which corresponds to a 
source code of 011 (i.e. the correct source code). All other 
Hamming distances are either 3 or 5; that is, the new error 
state is closer to some of the valid states and further away 
from some of the valid states. 
Having looked at [H]8 let's continue and construct a 
Hadamard matrix [H] 16, which is even more interesting. The 
16X16 Hadamard matrix [H] I6 is given by 
In this case there are 16 rows and each row has a minimum 
Hamming distance of 8 from all other rows. If a 4-bit source 
word is used to select one of the 16 rows, the resulting 16-bit 
code word will differ from all other valid code words in eight 
bit positions. Figure 4.15 illustrates two adjacent valid code 
words. Up to three errors in a code word can still be corrected 
because a code word with three errors is 3 Hamming units 
away from the correct code word and 5 Hamming units away 
from the next nearest valid code word. 
Because this 16-bit Hadamard code can correct up to three 
errors in a code word, it is used in applications in which 
errors occur relatively frequently (e.g. when transmitting 
digital data between spacecraft and receiving stations on Earth). 
Error-correcting codes are widely used to minimize the error 
rate in digital storage systems (e.g. hard disk and CD-ROM). 
4.6 Data-compressing codes 
Whoever said 'the best things in life are free' was wrong—very 
little in life is free. Encoding information into binary form 
costs time and money; time because data takes time to process 
and money because both random access memory and sec-
ondary storage systems are not free. Consequently, computer 
scientists have attempted to squeeze as much information 
into as few bits as possible. 
Most computer users will have heard of data compression. 
Data compression operates by encoding data in such a way 
that the encoded version occupies fewer bits than the original 
version. Although you can't compress random numbers, you 
can compress data that contains redundancy. Consider the 
sentence T am going to Washington in January 2007'. If you 
were taking this down in note form, you might write 'I'm gng 
to W'shtn Jan 07'. You have now compressed 40 ASCII char-
acters into 24 ASCII characters. Digital data can also be com-
pressed if it contains redundant information that can later be 
restored when the data is decompressed—e.g. English text, 
diagrams, and pictures. 
Let's look at how you might go about compressing data. 
Figure 4.16 shows a radar screen with three targets. Suppose 
that the radar image is converted into a 16 X 16 block of pix-
els, and a 1 is used to represent a target and a 0 no target—see 
Table. 4.10. The three targets are represented by Is in this 
block of 16 X 16 elements. 
The information in Table 4.10 can be stored as a string of 
16 X16 = 256 bits. Or, we could just store the locations of the 
three targets as conventional x, /coordinates: 4,6 10,10 13,3 
(the data block begins at location 0, 0 at the top left hand 
position and the location is given as row, column). These 
three coordinates can be stored as the string of six values 4,6, 
10,10, 13,3 or 010001101010101011010011 in binary form 
(each coordinate is a 4-bit value). We have just compressed a 
256-bit table into 24 bits. As you can imagine, this type of 
data compression is effective only if the table is sparse and 
contains relatively few targets. 
We can approach the coding in another way. If you look at 
the data,you can regard it as strings of 0s and Is (even though 
there are no multiple Is in this example). If we regard the 
start of the string as the top left-hand bit, it can be expressed 
as 70 zeros, 1 one, 99 zeros, 1 one, 40 zeros, 1 one, 44 zeros. 
The information needed to store this block of data is 70,1,99, 
1,40,1,44; i.e. 7 bytes (i.e. 56 bits). 
These two examples demonstrate that some data can be 
greatly compressed, although the type of data found in real 
+1 
+ 1 +1 
+1 
tl 
tl 
tl 
ll 
+ 1 
(1 
4l +1 
tl 
41 
+1 tl 
-II 
- 1 (-I - 1 +1 -1 
+1 - 1 
+ 1 -1 
+"L -1 +1 
- 1 +1 - 1 
n 
*i 
-i 
-l 
+i 
+i 
-l 
- i 
+i 
+i 
- i 
- l 
+i 
t i 
- l 
- l 
II 
-1 
-I 
+1 
+1 
-1 
-1 
+1 
+1 
-1 
-1 
+1 
+1 
-1 
-1 
+1 
II 
+1 
4-1 +1 
-1 
-1 
-1 
-1 
tl 
+1 
tl 
tl 
-1 
-1 
-1 
-1 
tl 
-1 
4-1 
-1 
-1 
tl 
-1 
+1 
tl 
-1 
tl 
-1 
-1 
4-1 
-1 
tl 
II 
tl 
-1 
-1 
-1 
-1 
tl 
tl 
tl 
tl 
-1 
-1 
-1 
-1 
tl 
tl 
tl 
-1 
-1 
tl 
-1 
41 
II 
-1 
tl 
-1 
-1 
II 
-1 
41 
tl 
-1 
+ 1 
tl 
4-1 +1 
+1 
+1 
tl 
+1 
-1 
-j 
-1 
+1 
-1 
-1 
-1 
-1 
+ 1 
-1 
+1 
-1 
+1 
-1 
tl 
-1 
-1 
tl 
-1 
tl 
-1 
*1 
-1 
+1 
tl 
tl 
-1 
-1 
tl 
tl 
-1 
-1 
-1 
-1 
tl 
tl 
-1 
-1 
tl 
tl 
tl 
-1 
-1 
tl 
tl 
-1 
-1 
tl 
-1 
t] 
tl 
-1 
-1 
tl 
tl 
-1 
+ 1 
41 
tl 
tl 
-1 
-1 
-1 
-1 
-1 
-1 
-1 
-1 
tl 
+1 
tl 
tl 
+ 1 
-1 
-tl -1 
-1 
tl 
-1 
+1 
-1 
tl 
-1 
+1 
tl 
-1 
tl 
-1 
tl 
tl 
-1 
-1 
-1 
-1 
tl 
tl 
-1 
-1 
tl 
tl 
tl 
+1 
-1 
-1 
+ 1 
-1 
-1 
tl 
-1 
+1 
tl 
-1 
-1 
tl 
+1 
+1 
tl 
-1 
-1 
tl 

164 
Chapter 4 Computer arithmetic 
Units distance of 8 between valied code words 
Figure 4.15 Adjacent code 
words in an 8-unit code. 
Item 
Code 
Figure 4.16 A radar image. 
000000 
000000 
000000 
000000 
000000 
000000 
000000 
000000 
000000 
000000 
000000 
000000 
000000 
000100 
000000 
000000 
00000 
00000 
00000 
00000 
00000 
10000 
00000 
00000 
00000 
00000 
00001 
00000 
00000 
00000 
00000 
00000 
00000 
00000 
00000 
00000 
00000 
00000 
00000 
00000 
00000 
00000 
00000 
00000 
00000 
00000 
00000 
00000 
Target 
Table4.10 Radar data (0 = no target, 1 = target). 
computers can't be compressed to such an extent. The figure 
of merit of a data compression system is its compression 
ratio, which is defined as uncompressed sizexompressed size. 
The compression ratio for a typical mixture of code, text, and 
images found in a PC is approximately 2:1. 
Potatoes 
00 
Onions 
01 
Beans 
10 
Avocado pears 
11 
Table 4.11 Coding four items with a 2-bit code. 
4.6.1 Huffman codes 
Huffman codes employ a variable-length code word. The idea 
of a Huffman code isn't new. When Samuel Morse devised his 
famous code he sent his assistant to a printer to count the 
number of letters in each storage bin. Morse argued that the 
printer's storage bins would provide a crude estimate of how 
much each letter was used on average. The letter E appears so 
frequently in English language text that there were many Es, 
whereas there were relatively few Qs. Morse constructed a 
code that assigned frequently used letters short codes, and 
infrequently used letters longer codes. 
A similar arrangement can be extended to binary codes. 
Huffman codes are applied only to information in which 
some elements appear more frequently than others. Plain text 
(e.g. written English) is such a case. To keep things simple, we 
provide a simple example before looking at Huffman codes in 
more depth. A grocer sells only four items (we did say we were 
keeping things simple), potatoes, onions, beans, and avocado 
pears. Being a thoroughly modern trader and a computer sci-
entist the grocer has a computerized business. Every time an 
item is bought, it is encoded in binary form and stored on 
disk. The grocer wishes to code transactions in such a way as 
to use the least possible storage. Initially the grocer tried fiie 
2-bit binary code described in Table 4.11. 
/Code words with a 
/ 
unit distance of 3 or 
' 
less are ctoser to 
valid code word X 
than to any other 
valid code word. 
Valid codeword 
\ 
x 
Valied code word 
Invalid state; 
A, B, C, D, 
F F and f, 

4.6 Data-compressing codes 
165 
If there are n transactions, the total storage required to 
record them is 2n bits. At first sight it would seem that there's 
no way the grocer can get away with less than two bits to 
encode each transaction. However, after a little thought, the 
grocer realizes that most customers buy potatoes and 
therefore devises the encoding scheme of Table 4.12. 
Table 4.12 uses codes of different lengths. One code has a 
1 -bit length, one has a 2-bit length, and two have 3-bit lengths. 
After a week's trading, the total storage space occupied will be 
the number of transactions for each item multiplied by the 
length of its code. The average code length will be: 
l X j + 2 X | + 3 x i + 3 X ^ = l . 375 
By adopting this code, a Huffman code, the average storage 
has been reduced from 2 bits per transaction to 1.375 bits per 
transaction, a saving of 31.25%. A Huffman code is often 
represented in the form of a binary tree, the tree in Fig. 4.17 
corresponding to the grocer's example. 
The diagram in Fig. 4.17 is sometimes called a trellis and is 
read from left to right. From the left, each of the four terminal 
nodes (labeled node 0, node 10, node 110, and node 111) can 
be reached by following the marked paths. These paths are 
indicated by a 1 or a 0 depending on the bit to be decoded. 
Let's look at how a Huffman code is interpreted. Suppose that 
the grocer's disk contains the following string of bits, 
001100101110. What codes does this string correspond to? 
Item 
Percent of transactions 
Code 
Potatoes 
75 
Onions 
12.5 
Beans 
6.25 
Avocado pears 
6.25 
0 
10 
110 
111 
Table 4.12 A Huffman code for four items. 
The first (leftmost) bit of the string is 0. From the trellis we 
can see that a first bit 0 leads immediately to the terminal 
node 0. Thus, the first code is 0. Similarly, the second code is 
also 0. The third code begins with a 1, which takes us to a 
junction rather than to a terminal. We must examine another 
bit to continue. This is also a 1, and yet another bit must be 
read. The third bit is a 0 leading to a terminal node 110. This 
process can be continued until the string is broken down into 
the sequence: 00110 0101110 = potatoes, potatoes, beans, 
potatoes, beans, avocados, potatoes. 
Variations of this type code are used in data and program 
compression algorithms to reduce the size of files (e.g. the 
widely used ZIP program). 
Statistical encoding 
The type of encoding performed by Samuel Morse employs a 
probabilistic model of the source words to generate code 
words. Frequently occurring source words are assigned short 
code words, and infrequently used source words are assigned 
long code words. We can easily calculate the average length of 
a message that has been Huffman encoded. Suppose that a 
source word st has a probability P; of appearing as the next 
element in the information to be encoded. The average length 
of a Huffman-encoded message is therefore given by the sum 
of the probability of each source word in the message multi-
plied by the length of its code word, that is 
n2p,s, for i = 1 to n symbols. 
If a system employs four code words with the lengths 1,2,4, 
and 5 and the probability of each code word is 0.4,0.3,0.2, and 
0.1, respectively, the average length of a message with n code 
words is 1 X 0.4 + 2 X 0.3 + 4 X 0.2 + 5 X 0.1 = 2.3n. 
Let's look at a simple example of Huffman encoding. 
Consider an alphabet of five characters A, B, C, D, and E. 
Table 4.13 provides the relative frequency of the occurrence 
0 
Terminal node 
(Potatoes) 
Start 
j 10 Terminal node 
(Onions) 
110 Terminal node 
(Beans) 
111 Terminal node 
(Avocados) 
Figure 4.17 The Huffman code 
corresponding to Table 4.12. 

166 
Chapter 4 Computer arithmetic 
of these letters in a long message. Such a table can be derived 
by obtaining the statistics from many messages. The values in 
this table are hypothetical and have been chosen to provide a 
simple example. 
Symbol A is the most common symbol (it occurs eight 
times more frequently than symbol D) and we will give it the 
shortest possible code—a single bit. It doesn't matter whether 
we choose a I or a 0. We'll represent A by 0. If symbol A is 
represented by a single-bit code 0, what is represented by a 1? 
The answer is, all the remaining symbols. We therefore have 
to qualify the code 1 by other bits in order to distinguish 
between the remaining four symbols. 
We will represent the next most common symbol B by the 
code 10, leaving the code 11 to be shared among symbols C, 
D, and E. Continuing in this manner, the code for symbol C is 
110, for symbol D is 1110, and for symbol E is 1111. 
Figure 4.18 provides a trellis to illustrate how the symbols are 
encoded. As you can see, we have now constructed a code in 
which symbol A is represented by a single bit, whereas symbol 
E is represented by four bits. 
Consider encoding the string BAEAABDA. We begin at the 
point labeled start in Fig. 4.18 and follow the tree until we get 
to the terminal symbol (i.e. A, B, C, D, or E); for example, the 
bit 0 takes us immediately to the terminal symbol A, whereas 
you have to take the path 1, 1, 1, 0 to reach the terminal 
symbol D. The encoding of the sequence BAEAABDA is 
therefore B = 10, A = 0, E = 1111, A = 0, A = 0, B = 10, 
D = 1110.A = 0, to give the string 1001111001011100. 
In this example there are five symbols and the average 
length of a message is 1 X 0.5 + 2 X 0.25 + 3 X 0.125 + 
Symbol 
A 
B 
C 
D 
E 
Relative frequency 
8 
4 
2 
1 
1 
Relative probability 
0.5 
0.25 
0.125 
0.0625 
0.0625 
Table 4.13 The relative frequency of symbols in an alphabet. 
A 
O 
S 
B 
Start / 
-O 
\1 
os 
Symbol Code 
N. 
/ 
D
A 
0 
\ l 
o/ 
C 
11° 
\ 
y 
D 
mo 
\X 
E 
1111 
o 
Figure 4.18 A Huffman encoding tree. 
4 X 0.0625 + 4 X 0.0625 = 1.875 bits per symbol. If the same 
five symbols had been coded conventionally, 3 bits would 
have been required to represent 000 = A to E = 100. Huffman 
encoding has reduced the average storage requirement from 
3 to less than 2 bits per symbol. 
Now let's look at a more complex example of Huffman 
encoding. In this case we will use a wider range of symbols 
and avoid the easy numbers of the previous example (did you 
notice that all the probabilities were binary fractions?). In this 
case, we take 16 letters, A to P, and produce a table of relative 
frequencies, (Table. 4.14). We have not used all 26 letters, to 
keep the example reasonably simple. The relative frequencies 
are made up. 
Figure 4.19 shows how we can construct a Huffman tree 
for this code. The letters (i.e. symbols) are laid out along the 
top with the relative frequencies underneath. The task is to 
draw a tree whose branches always fork left (a 1 path) or right 
(a 0 path). The two paths are between branches of equal (or as 
nearly equal as possible) relative frequency. At each node in 
the tree, a shaded box shows the cumulative relative fre-
quency of all the symbols above that node. The node at the 
bottom of the tree has a relative frequency equal to the sum of 
all the symbols. 
Consider the right-hand end of the tree. The symbols G 
and J each have a relative frequency 1 and are joined at a 
node whose combined relative frequency is 2. This node is 
combined with a path to symbol K that has a frequency 2 
(i.e. G and J are as relatively frequent as I). You derive the 
Symbol 
A B C D 
E F G H I J 
K L M N O P 
Relative frequency 10 3 4 4 
12 2 1 
3 3 1 2 6 4 
5 5 3 
Table 4.14 The relative probability of symbols in a 16-symbol 
alphabet. 
Figure 4.19 A Huffman encoding tree for a 16-symbol code. 
E
A
L
N
O
C
D
M
B
H
I
P
F
K
G
J 
12 10 
6
5
5 
4 4 
4
3
3
3
3
2
2
1 
1 
\ 
li jo 
\i/o 
Vi/o 
\i/o 
\i/o 
\i/o 
\i 
V y i 
\ij 
p l a U3 jfni ua YB 
\\l 
X r 
\1 1° 
x f° 
A 
B 
C 
D 
E 
0 
10 
110 
1110 
1111 
1 
0 
A 
0 
Start 
c 
D 
E 

4.6 Data-compressing codes 
167 
Symbol 
A
B
C
D
E
F
C
H
I
J 
K
L
M
N
O
P 
Code 
1011 
0110 
10001 
10000 
11 
0010 
00001 
0101 
0100 
00000 
0001 
1010 
0111 
10011 
10010 
0011 
Table 4.15 The Huffman encoding of a 16-symbol alphabet. 
(b) Dividing the block into four quadrants 
(a) Block of 8 x i i pixels 
F 
P 
r r 
P 
r 
P 
r 
2 
3 
0 
1 
Quadrant 
numbering 
F = full (all elements 1) 
E = empty (all elements 0) 
P = partially filled 
(c) The top right-hand quadrant of (b) 
(d) The top right-hand quadrant of (c) 
is divided into four quadrants 
is divided into four quadrants 
P 
E 
c 
E 
F 
(e) Bottom right-hand quadrant of (a) 
(f) Top right-hand quadrant of (e) 
is divided into four quadrants 
is divided into four quadrants 
T> 
F 
F 
Figure 4.20 The quadtree. 
codes for the letters by starting at the bottom-most node and 
working back to the symbol (see Table. 4.15). 
4.6.2 Quadtrees 
An interesting data compression technique employs a data 
structure called the quadtree, which is used to encode two-
dimensional images. Figure 4.20 illustrates an 8 X 8 pixel 
image. This image can be divided into four quadrants, as 
Fig. 4.20(b) demonstrates (hence the term quadtree). As you 
can see from Fig. 4.20(b), the four quadrants have different 
properties. 
In the top left-hand quadrant, all the pixels are black and 
are marked 'F' for full. In the bottom left-hand quadrant, all 
the pixels are white and are marked 'E' for empty. Each of the 
two right-hand quadrants contains a mixture of black and 
white pixels—these quadrants are marked 'P' for partially 
occupied. 
I 
E 
E 
E 
E 

168 
Chapter 4 Computer arithmetic 
E, (E, F, (E, E, F, F), (E, F, F, F)), F, (E, (E, F, E, F), E, (E, F, E, F)) 
Initial image 
(E, (E, F, E, F), E, {E, F, E, F)) 
(E.F.E.F) 
" E 
F 
E 
r 
( L . . L •") 
f 
E 
r 
E 
: 
(E, E, F, F) 
Figure 4.21 The complete quadtree expansion of Fig. 4.20. 
The picture of Fig. 4.20(b) can be represented by its four 
quadrants 0, 1, 2, 3 as E, P, F, P (see figure 4.20 for the 
quadrant numbering scheme). We can partially regenerate 
the image because we know that one quadrant is all black and 
another is all white. However, we don't know anything about 
the two quadrants marked 'P'. 
We can, however, subdivide partially filled quadrants 1 and 
3 into further quadrants. Consider the upper right-hand 
quadrant of Fig. 4.20(b) (i.e. quadrant 3). This can be divided 
into four quadrants as Fig. 4.20(c) demonstrates. We can 
describe the structure of Fig. 4.20(c) by E, P, E, P. If we substi-
tute this expansion of quadrant 3 in the original expression 
for the image, we get: E, P, F, (E, P, E, P). 
We haven't yet completely defined quadrant 3 of the image 
because there are still subdivisions marked 'P'. Figure 4.20(d) 
demonstrates how the top right-hand quadrant for Fig. 4.20(c) 
can be subdivided into the quadrants E, F, E, F. If we now 
substitute this in the expression for the image we get E, P, F, (E, 
(E, F, E, F), E, P). We can do the same thing to quadrant 1 of 
Fig. 4.20(c) to get: E, P, F, (E, (E, F, E, F), E, (E, F, E, F)). Now we 
have completely defined quadrant 3 of the original image. 
Continuing in this way and expanding quadrant 1 of 
the original image, we get the expression E, (E, F, (E, E, F, F), 
(E, F, F, F)), F, (E, (E, F, E, F), E, (E, F, E, F)). All we have done 
is to divide an image into four quadrants and successively 
divided a quadrant into four quadrants until we reach that 
point at which each quadrant contains only one color. 
Because many areas of an image contain the same color, the 
quadtree structure can compress the image. In the case of 
Fig. 4.20 we have compressed a 64-element block into a string 
of 29 elements (the elements may be E, F, left bracket, or right 
bracket). 
Figure 4.21 demonstrate the complete quadtree expansion 
of Fig. 4.20. 
(E, (E, F, E, F), E, (E, F, E, F)) 
Initial image 
(E, F, (E, E, F, F), (E, F, F, F)) 
(E.F.F.F) 
F 
< f 
E 
E 
E 
E 
F 
E • . 

4.7 Binary arithmetic 
169 
The quad tree and the other compression techniques we've 
described are lossless encoding techniques because a file can 
be compressed and restored with no loss of information (i.e. 
compress and decompress yields the original source). Some 
compressing techniques are lossy because information is lost 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
Table 4.16 The decimal addition tables. 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
1 
1 
2 
3 
4 
5 
6 
7 
8 
9 
2 
2 
4 
6 
8 
10 
12 
14 
16 
18 
3 
3 
6 
9 
12 
15 
18 
21 
24 
27 
4 
4 
8 
12 
16 
20 
24 
28 
32 
36 
5 
5 
10 
15 
20 
25 
30 
35 
40 
45 
6 
6 
12 
18 
24 
30 
36 
42 
48 
54 
7 
7 
14 
21 
28 
35 
42 
49 
56 
63 
8 
8 
16 
24 
32 
40 
48 
56 
64 
72 
9 
9 
18 
27 
36 
45 
54 
63 
72 
81 
Table 4.17 The decimal multiplication tables. 
0 
0 1 
02 
03 
04 
05 
06 
07 
1 
01 
02 
03 
04 
05 
06 
07 
2 
02 
04 
06 
08 
0A 
0C 
E 
3 
03 
06 
09 
OC 
OF 
12 
15 
4 
04 
08 
OC 
10 
14 
18 
1C 
5 
05 
0A 
OF 
14 
19 
IE 
23 
6 
06 
OC 
12 
18 
IE 
24 
2A 
7 
07 
0E 
15 
1C 
23 
2A 
3 1 
8 
08 
10 
18 
20 
28 
30 
38 
9 
09 
12 
IB 
24 
2D 
36 
3F 
A 
0A 
14 
IE 
28 
32 
3C 
46 
B 
0B 
16 
2 1 
2C 
37 
42 
4D 
C 
OC 
18 
24 
30 
3C 
48 
54 
D 
0D 
1A 
27 
34 
4 1 
4E 
5B 
E 
0E 
1C 
2A 
38 
46 
54 
62 
F 
OF 
IE 
2D 
3C 
4B 
5A 
69 
during compression and the original information can't be 
restored. Lossy compression technology is used to compress 
images and sound because humans can lose a lot of detail in 
an image (or piece of music) without noticing missing it. 
Typical lossy compression techniques are MP3 (sound), 
JPEG (still images), and MPEG (video). 
4.7 Binary arithmetic 
Now that we've introduced binary numbers and demon-
strated how it's possible to convert between binary and deci-
mal formats, the next step is to look at how binary numbers 
are manipulated. Binary arithmetic follows exactly the same 
rules as decimal arithmetic and all that we have to do to work 
with binary numbers is to learn the binary tables. Table 4.16 
gives the decimal addition tables and Table 4.17 gives the dec-
imal multiplication table. Table 4.18 gives the hexadecimal 
multiplication table. Table 4.19 gives the binary addition, 
subtraction, and multiplication tables. As you can see, these 
are much simpler than their decimal equivalents. 
A remarkable fact about binary arithmetic revealed by 
Table 4.19 is that if we didn't worry about the carry in addition 
and the borrow in subtraction, then the operations of addition 
and subtraction would be identical. Such an arithmetic in which 
addition and subtraction are equivalent does exist and has some 
important applications; this is called modulo-2 arithmetic. 
Table 4.19 tells us how to add two single digits. We need to 
add longer words. The addition of n-bit numbers is entirely 
straightforward, except that when adding the two bits in each 
column, a carry bit from the previous stage must also be 
added in. Each carry bit results from a carry-out from the 
column on its right. In the following example, we present the 
08 
9 
0A 
0B 
OC 
0D 
0E 
OF 
08 
09 
0A 
0B 
OC 
0D 
0E 
OF 
10 
12 
14 
16 
18 
1A 
1C 
IE 
18 
IB 
IE 
2 1 
24 
27 
2A 
2D 
20 
24 
28 
2C 
30 
34 
38 
3C 
28 
2D 
32 
37 
3C 
4 1 
46 
4B 
30 
36 
3C 
42 
48 
4E 
54 
5A 
38 
40 
46 
4D 
54 
5B 
62 
69 
40 
48 
50 
58 
60 
68 
70 
78 
48 
51 
5A 
63 
6C 
75 
7E 
87 
50 
5A 
64 
6E 
78 
82 
8C 
96 
58 
63 
6E 
79 
84 
8F 
9A 
A5 
60 
6C 
78 
84 
90 
9C 
A8 
B4 
68 
75 
82 
8F 
9C 
A9 
B6 
C3 
70 
7E 
8C 
9A 
A8 
B6 
C4 
D2 
78 
87 
96 
A5 
B4 
C3 
D2 
E l 
Table 4.18 The hexadecimal multiplication tables. 

170 
Chapter 4 Computer arithmetic 
numbers to be added on the left, and on the right we include 
the carry bits that must be added in. 
00110111 
-» 
00110111 
+01010110 
-» 
01010110 
10001101 
10001101 
We can carry out hexadecimal addition in the same way 
that we carry out binary addition. All we have to remember is 
that if we add together two digits whose sum is greater than 
15, we must convert the result into a carry digit whose value 
is 16 plus the remainder, which is the sum less 16. For exam-
ple, if we add E and 7 in hexadecimal arithmetic, we get 1516 
which is 5 carry 1 (i.e. 5 plus 16). 
Consider the addition EA34816 + 6701916. 
EA348 -» 
67019 -> 
151361 -» 
EA348 
67019 
11 
1^_ 
151361 
carries 
Subtraction can also be carried out in a conventional 
fashion, although we shall see later that a computer does not 
subtract numbers in the way we do because negative numbers 
are not usually represented in a sign plus magnitude form but 
by means of their complements. 
01010110 
86 
-00101010 
-42. 
1 1 
<— borrows 
44 
0101100 
The multiplication of binary numbers can be done by the 
pencil and paper method of shifting and adding, although in 
practice the computer uses a somewhat modified technique. 
01101 
13 
x 01010 
x H^ 
00000 
130 
01101 
00000 
01101 
00000 
0010000010 
4.7.1 The Half adder 
We now design circuits to add binary numbers. The most 
primitive circuit is called the half adder or HA which 
adds together two bits to give a sum, S, and a carry, C as 
described in Table 4.20. The sum, S, is given by their exclus-
ive OR; that is, S = A • B + A • A. The carry is given by 
C = A B . 
From the chapter on gates we know that this circuit may 
be realized in at least three different ways as Fig. 4.22 
demonstrate. This circuit, whatever its implementation, 
is often represented in many texts by the symbol in 
Figure 4.23. 
We can use Digital Works to construct a half adder and 
to simulate its behavior. Figure 4.24 shows a screen in 
which we've constructed a half adder and used the push-
button (i.e. input device) to provide inputs A and B. We 
have connected the adder's sum and carry outputs to 
LEDs and have linked all inputs and outputs to the Logic 
History function. 
As you can see from fig. 4.24, we've run the simulation in 
the single step mode, applied all possible inputs to the circuit, 
and observed the output waveforms. 
Table 4.19 Binary tables. 
A 
0 
0 
1 
1 
B 
S (sum) 
Addition 
Subtraction 
Multiplication 
0 X 0 = 0 
A 
0 
0 
1 
1 
B 
S (sum) 
C(carry) 
Subtraction 
Multiplication 
0 X 0 = 0 
A 
0 
0 
1 
1 
0 
0 
0+0 = 0 
0 - 0 = 0 
Multiplication 
0 X 0 = 0 
A 
0 
0 
1 
1 
0 
0 
0 
0+1 = 1 
0 - 1 = 1 borrow 1 
0 X 1 = 0 
A 
0 
0 
1 
1 
1 
1 
0 
1 + 0 = 1 
1 - 0 = 1 
1 X 0 = 0 
A 
0 
0 
1 
1 
0 
1 
0 
1 
1 + 1 =0carry 1 
1-1 = 0 
1 X 1 = 1 
A 
0 
0 
1 
1 
1 
0 
0 
1 
1-1 = 0 
1 X 1 = 1 
Table 4.20 Truth table for a half adder. 
PERFORMING ADDITION 
The following sequence demonstrates binary addition. Each 
step shows the current carry in (light blue) and the carry out 
generated in the next column to the right (bold blue). Only 
seven stages are shown in order to fit on the page. 
Stepl 
1 
0 
© 
Step 2 
11 
10 
© 
1 
01 
Step 3 
111 
110 
101 
Step 4 
0111 
+ 
0110 
1101 
Step 5 
10111 
+ 
10110 
0 
1 
01101 
Step 6 
110111 
+ 
010110 
001101 
Step 7 
0110111 
+ 1010110 
0001101 
<- carries 

Sum 
Carry 
p. 
Figure 4.23 The circuit representation of a half adder. 
Sum 
Carry 
— • 
Carry 
Figure 4.22 Three ways of implementing a half adder. 
4.7.2 TheFullAdder 
Unfortunately, the half adder is of little use as it stands. When 
two n-bit numbers are added together we have to take 
account of any carry bits. Adding bits a, of A and b, of B 
together must include provision for adding in the carry bit 
c,_, from the results of the addition in the column to the right 
of a, and bt. This is represented diagrammatically as 
a n-i 
t>n-l 
S-2 
b2 
ai 
bi 
a0 
bo 
a„_i .. 
a2 
&i 
a0 
b n - i 
•• 
b 2 
b i 
b 0 
C„-2 •• 
C i 
Co 
This row consists of the 
carry bits generated by 
the columns on the right. 
A 
B 
c,„ 
S (sum) 
Cout (carry-out) 
0 
0 
0 
0 
0 
0 
1 
0 
1 
0 
1 
0 
0 
1 
0 
1 
1 
0 
0 
1 
0 
0 
1 
1 
0 
0 
1 
1 
0 
1 
1 
0 
1 
0 
1 
1 
1 
1 
1 
1 
Table 4.21 Truth table for full adder. 
When people perform an addition they deal with the carry 
automatically, without thinking about it. More specifically they 
say, 'If a carry is generated we add it to the next column, if it is 
not we do nothing.' In human terms doing nothing and adding 
zero are equivalent. As far as the logic necessary to carry out the 
addition is concerned, we always add in the carry from the 
previous stage, where the carry bit has the value 0 or 1. 
The full adder, represented by the circuit symbol of 
Fig. 4.25, adds together two bits A and B, plus a carry-in C,„ 
from the previous stage, to generate a sum S and a carry-out 
Cout. In other words, the full adder is a 3-bit adder. Table 4.21 
provides the truth table for a full adder. 
You can realize the circuit for a full adder by connecting two 
half adders in tandem. Conceptually, a full adder requires that 
the two bits of A and B be added together and then 
the carry-in is added to the result. Figure 4.26 shows a possible 
representation of the full adder in terms of two half adders. 
The sum output of the full adder is provided by the sum out-
put of the second half adder, HA2. The carry-out from the full 
adder, Cout, is given by ORing the carries from both half adders. 
To demonstrate that the circuit of Fig. 4.26 does indeed per-
form the process of full addition a truth table may be used. 
Table 4.22 provides a truth table for the circuit of Fig. 4.26. 
As the contents of the S2 and Cout columns are identical to 
those of the corresponding columns of die truth table for the 
full adder (Table 4.22), we must conclude that the circuit of 
Fig. 4.26 is indeed that of a full adder. Figure 4.27 demonstrates 
A 
B 
1 i 
HA 
T 
T 
5 
^out 
4.7 Binary arithmetic 
171 
A 
B 
B 
A 
B 
A 

172 
Chapter 4 Computer arithmetic 
s;DrqitM:5«o<KfcSCii**l£l 93wrn 
£fie £dft 0rcuit i/isw Jsote 
ftelp 
O <c? S3 
''' 7 
i9 
£> > 
I> I> w> D l> *- B 
B tB 
B 
1> O 
II 
t> 
ti 
-fr, 
a 
B 
gie 
Qpfions Help 
'•JIG 
> o 
II so 
^ 
f 
-|J 
C 
1 
I 
1 
I 
C 
i 
i 
C 
I 
1 
.Cyc!e19comp!eted 
SB Start] %, Digital Works 
j*fMicrosof!Worti-Da„ 
^4- a-8 11-55 PM 
Figure 4.24 Using Digital Works to implement and test a half adder circuit. 
A 
B 
Cir 
Figure 4.25 The circuit representation of a full adder. 
cin 
A 
B 
s, 
c, 
s2 
c2 
^-out 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
1 
0 
1 
0 
0 
0 
1 
0 
1 
0 
1 
0 
0 
0 
1 
1 
0 
1 
0 
0 
1 
1 
0 
0 
0 
0 
1 
0 
0 
1 
0 
1 
1 
0 
0 
1 
1 
1 
1 
0 
1 
0 
0 
1 
1 
1 
1 
1 
0 
1 
1 
0 
1 
/ \ 
i 
C in 
' ' 
' 
' 
HA1 
Full adder 
' 
( 
s 
' 
1 
i ' 
i ' 
HA2 
1 ' 
' 1 
Table 4.22 Truth table for a full adder implemented by two half 
adders. 
Fig. 4.26 Implementing a full adder using two half adders. 
FA 
input A 
m—| 
-f>o 
1
—
•
% 
Ls;—^ 
Sum 
J 
) 
—-OS 
inputs 
j ~ * — 
BO—r~ —L—[>0 
j 
\ 
I 
D
Cafry 
Oc 
^-out 
s 

4.7 Binary arithmetic 
173 
^BjgHoIiWqrks^OTt^^.awro/S 
File Edit Circuit Mtsw Iools Help 
D I > I > I > B < > D l > - f r - e s s t B B i o B 
sa 
TSIxl 
A 
<? 
^3 
E> 
O 
II 
Help 
a> 
j ^ J 
-7-} 
. • ; ; • - - • . 
- .JflJiii 
A 
B 
C 
Sum 
Carry out 
Cycle 15 comp 
r—1 
r~} r ~ - 
n 
A 
B 
C 
Sum 
Carry out 
Cycle 15 comp 
, 
j 
; 
j 
j 
1 
A 
B 
C 
Sum 
Carry out 
Cycle 15 comp 
A 
B 
C 
Sum 
Carry out 
Cycle 15 comp 
A 
B 
C 
Sum 
Carry out 
Cycle 15 comp 
A 
B 
C 
Sum 
Carry out 
Cycle 15 comp 
A 
B 
C 
Sum 
Carry out 
Cycle 15 comp 
A 
B 
C 
Sum 
Carry out 
Cycle 15 comp 
A 
B 
C 
Sum 
Carry out 
Cycle 15 comp eted 
Figure 4.27 Using Digital Works to implement and test a full adder built from two half adders. 
the use of Digital Words to construct and simulate a full adder 
built from two half adders. 
In practice the full adder is not implemented in this way 
because the propagation path through the two half adders 
involves six units of delay. An alternative full adder circuit 
may be derived directly from the equations for the sum and 
the carry from the truth table. Let the sum be S, the carry-out 
COJ and the carry-in C. 
S = C-A-B + "C-A-B + C~-A-~B + C-A-B 
and C0 = C-A-B + C-A-B + C-A-B + C-A-B 
= C-A-B + C-A-B + C-A(B + B) 
= C-A-B + C-A-B + C-A 
= C"-A-B + C (A-B + A) 
= C"-A-B + C-B + C-A 
= A(C"-B + C) 
+ C-B = A ( B + C) 
+ C-B 
= C-A + C-B + A-B 
The carry-out represents a majority logic function that is true 
if two or more of the tree inputs are true. The circuit diagram of 
the full adder corresponding to the above equations is given in 
Fig. 4.28. This circuit contains more gates than the equivalent 
realization in terms of half adders (12 against 9) but it is faster. 
The maximum propagation delay is three gates in series. 
4.7.3 The addition of words 
Even a full adder on its own is not a great deal of help, as we 
normally wish to add two n-bit numbers together. We now 
look at ways in which two n-bit numbers can be added 
together. We begin with the serial full adder and then describe 
the parallel full adder. 
It is perfectly possible to add two n-bit numbers, A and B, 
together, serially, a bit at a time by means of the scheme given 
in Fig. 4.29. The contents of the shift registers containing the 
n-bit words A and B are shifted into the full adder a bit at a 
time. The result of each addition is shifted into a result (i.e. 
sum) register. A single flip-flop holds the carry bit so that the 
old carry-out becomes the next carry-in. After n clock pulses, 
the sum register, S, contains the sum of A and B. Serial adders 
aren't used today because parallel adders are much faster. 
Parallel adders 
A parallel adder adds the n bits of word A to the n bits of word 
B in one simultaneous operation. Figure 4.30 describes a 
parallel adder constructed from n full adders. The carry-out 
from each full adder provides the carry-in to the stage on its 
left. The term parallel implies that all n additions take place at 
the same time and it's tempting to think that the parallel 
adder is n times faster than the corresponding serial adder. In 
practice a real parallel adder is slowed down by the effect 
of the carry-bit propagation through the stages of the full 
adder. 
Several points are worth noting in Fig. 4.30. You might 
think that a half adder could replace the least-significant bit 
stage because this stage doesn't have a carry-in. However, 
OCarry out 
OSum 
as 
> i 
I 
ro 
> I 
Halfadder / 
I HalfSdder 
/ 
\—cp—Q—/ 
L_m • 
/ 
H 
Bl 
CI 

174 
Chapter 4 Computer arithmetic 
Carry 
Sum 
Figure 4.28 A possible circuit for the full 
adder. 
Shift register A 
Shift register B 
Shift dock 
n pulses per addition 
The full adder adds two bits and 
the carry-in from the previous 
addition at each clock pulse. 
Shift register S (sum) 
The carry flip-flop stores the 
carry-out from the previous 
Carry 
addition to generate the carry-in 
flip-flop t 0 t'1e n e x t addition. 
Figure 4.29 The serial adder. 
*1 
A 
B 
C„ 
Full adder 
Cout Sum 
A 
B 
Cir 
Full adder 
Cout Sum 
A 
B 
q 
Full adder 
Cout Sum 
by using a full adder for this stage, the carry-in may be set to 
zero for normal addition, or it may be set to 1 to generate 
A + B + 1. If input B is set to zero, A + 1 is generated and 
the circuit functions as an incrementer. A facility to add in 1 
to the sum of A plus B will prove very useful when we come to 
complementary arithmetic. 
>0 
Carry-in to 
first stage 
A 
B 
Cjr 
Full adder 
% 
Figure 4.30 The parallel adder. 
Another feature of this circuit concerns the carry-out from 
the most-significant bit stage. If two o-bit words are added 
and the result is greater than 111 . . . 1, then a carry-out is 
generated. As the computer cannot store words longer than 
n bits, the sum cannot be stored in the memory as a single 
entity. The carry-out of the most-significant stage may be 
A 
B 
^-m 
i \ 
1 
I i 
1 
( l — | 
Y V Y 
c l n -
A -
B -
an-l 
^n-1 
I 
I 
a2 
b2 
I 
1 
ao 
1 
* i 
'-out 
-sn-1 
S2 
Si 
^ 
' 
Full 
adder 
—• bi 
si 
• Mil 
'"out 
— Q D 
- * C 

4.8 Signed numbers 
175 
latched into a flip-flop (normally forming part of the com-
puter's condition code register). When addition is performed 
by software as part of a program, it is usual for the program-
mer to test the carry bit to check whether the result has gone 
out of range. 
A final point about the parallel adder concerns the mean-
ing of the term parallel. The first stage can add a0 to b0 to get 
S0 as soon as A and B are presented to the input terminals of 
the full adder. However, the second stage must wait for the 
first stage's carry-out to be added in to ax plus bx before it can 
be sure that its own output is valid. In the worst case inputs of 
111 . . . 1 + 1, the carry must ripple through all the stages. 
This type of adder is referred to as a ripple-carry adder. 
The full adder we have described here is parallel in the sense 
that all the bits of A are added to all the bits of B in a single 
operation without the need for a number of separate clock 
cycles. Once the values of A and B have been presented to the 
inputs of the full adders, the system must wait until the circuit 
has had time to settle down and for all carries to propagate 
before the next operation is started. Figure 4.31 shows a ripple 
adder in more detail using the circuits we've developed before. 
As you can see, the carry in has to ripple through successive 
stages until it reaches the most-significant bit position. 
Real full adders in computers are much more complicated 
than those we have shown here. The fundamental principles 
are the same, but the effect of the ripple-through carry from 
first to last stage cannot be tolerated. A mechanism called 
carry look ahead circuits can be used to anticipate a carry over 
a group of say four full adders. That is, the carry out to stage 
i + 5 is calculated by examining the inputs to stages i + 4, 
i + 3, (' + 2, and i + 1, and the carry in to stage i + 1, by 
means of a special high-speed circuit. This anticipated carry 
is fed to the fifth stage to avoid the delay that would be 
incurred if a ripple-through carry were used. The exact 
nature of these circuits is beyond the scope of this book. 
4.8 Signed numbers 
Any real computer must be able to deal with negative 
numbers as well as positive numbers. Before we examine how 
the computer handles negative numbers, we should consider 
how we deal with them. I believe that people don't, in fact, 
actually use negative numbers. They use positive numbers 
(the 5 in —5 is the same as in +5), and place a negative sign 
in front of a number to remind them that it must be treated 
in a special way when it takes part in arithmetic operations. In 
other words, we treat all numbers as positive and use a sign 
(i.e. + or —) to determine what we have to do with the num-
bers. For example, consider the following two operations. 
Carry to 
next stage 
Carry-in to 
first stage 
Full adder 
s0 
Figure 4.31 Ripple carry. 
Full adder 
~~QJ 
Full adder 
'-out 
Cm 
a0 
b0 
i 
_i 
. 
8 
8 
+ 5 and 
_Jj 
13 
3 
T 
IHA 
*1 
*1 
— I — 
I |HA 
N 
a2 
&2 
_l 
[HA 
Y 
s 1 

176 
Chapter 4 Computer arithmetic 
In both these examples the numbers are the same, but the 
operations we performed on them were different; in the 
first case we added them together and in the second case we 
subtracted them. This technique can be extended to 
computer arithmetic to give the sign and magnitude represen-
tation of a negative number. 
4.8.1 Sign and magnitude representation 
An «-bit word can have 2" possible different values from 0 to 
2"—1; for example, an 8-bit word can represent 0 , 1 , . . . ,254, 
255. One way of indicating a negative number is to take the 
most-significant bit and reserve it to indicate the sign of the 
number. The usual convention is to choose the sign bit as 0 
to represent positive numbers and 1 to represent negative 
numbers. We can express the value of a sign and magnitude 
number mathematically in the form ( — l) s X M, where 5 is 
the sign bit of the number and M is its magnitude. If S = 0, 
( —1)°= +1 and the number is positive. If S = 1, 
( — l)1 = —1 and the number is negative. For example, in 
8 bits we can interpret the two numbers 00001101 and 
10001101 as 
000011012=+13_10 
numbeT 
magnitude | 
! 
0 0 0 0 1 1 0 1[ 
t 
100011012 =-13,0 
number 
I 
I 
tnagrutude J 
I 
1 0 0 0 1 1 0 V 
t. 
sign bit 
Negative: 
: sign bit 
Positive! 
Using a sign bit to represent signed numbers is not widely 
used in integer arithmetic. The range of a sign and magnitude 
number inn bits is given by — (2"_1 — l)to+(2" - 1 — 1). 
All we've done is to take an n bit number, use 1 bit to 
represent the sign, and let the remaining n — 1 bits represent 
the number. Thus, an 8-bit number can represent from — 127 
(11111111) to +127 (01111111). One of the objections to 
this system is that it has two values for zero: 
00000000 = + 0 
and 
10000000 = - 0 
Another reason for rejecting this system is that it requires 
separate adders and subtractors. The are other ways of 
representing negative numbers that remove the need for 
subtractor circuits. 
Examples of addition and subtraction in sign and 
magnitude arithmetic are given below. Remember that the 
most-significant bit is a sign bit and does not take part in the 
calculation itself. This is in contrast with two's complement 
arithmetic (see later) in which the sign bit forms an integral 
part of the number when it is used in calculations. In each of 
the four examples below, we perform the calculation by first 
converting the sign bit to a positive or to a negative sign. Then 
we perform the calculation and, finally, convert the sign of 
the result into a sign bit. 
Sign and 
magnitude 
value 
1. 001011 
+001110 
2. 001011 
+100110 
3. 001011 
+110110 
4. 001011 
-001001 
Number 
with sign 
bit converted 
into sign 
+01011 
+11001 
-
+01011 
-00110 
+00101 
• 
+01011 
-10110 
-01011 
+01011 
*• -01001 
+00010 
• 
Result 
with sign 
converted 
into sign bit 
-*• 
011001 
- • 
000101 
- • 101011 
•> 000010 
4.8.2 Complementary arithmetic 
In complementary arithmetic the negativeness of a number is 
contained within the number itself. Because of this, the 
concept of signs (+ and —) may, effectively, be dispensed 
with. If we add X to Kthe operation is that of addition if X is 
positive and Y is positive, but if Y is negative the end result is 
that of subtraction (assuming that Y is represented by its 
negative form). It is important to point out here that comple-
mentary arithmetic is used to represent and to manipulate 
both positive and negative numbers. To demonstrate that 
there is nothing magical about complementary arithmetic, 
let's examine decimal complements. 
Ten's complement arithmetic 
The ten's complement of an n-digit decimal number, JV, is 
defined as 10"—N. The ten's complement may also be 
calculated by subtracting each of the digits of N from 9 and 
adding 1 to the result; for example, if n = 1, the value of — 1 is 
represented in ten's complement by 9. Consider the four-digit 
decimal number 1234. Its ten's complement is: 
(a) 10" - 1234 = 8766 or 
(b) 9999 
-1234 
8765 + 1 = 8766 
Suppose we were to add this complement to another 
number (say) 8576. We get 
8576 
+8766 
17342 
Now let's examine the effect of subtracting 1234 from 8576 
by conventional means. 
8576 
-1234 
7342 

4.8 Signed numbers 
177 
Notice that the results of the two operations are similar in 
the least-significant four digits, but differ in the fifth digit 
by 104. The reason for this is not hard to find. Consider the 
subtraction of V from X. We wish to calculate Z = X — Y, 
which we do by adding the ten's complement of Y to X. The 
ten's complement of Y is defined as 104 — Y. Therefore we get 
Z = X + (104 - Y) = 104 + (X - Y). 
In other words, we get the desired result, X — Y, together with 
an unwanted digit in the leftmost position. This digit may be 
discarded. 
Complementing a number twice results in the original num-
ber; for example, -1234 is 10" - 1234 = 8876. Complementing 
twice,weget - ( -1234) = -8876 = 104 - 8876 = 1234. 
4.8.3 Two's complement representation 
The equivalent of ten's complement in binary arithmetic is 
two's complement. To form the two's complement of an «-bit 
binary number, N, we evaluate 2" — N. For example, in 5 bits, 
if N = 5 = 00101 then the two's complement of Nis given by 
25 - 00101 = 100000 - 00101 = 11011. It is important to 
note here that 11011 represents-00101 (-5) 
or+27 
depending only on whether we interpret the bit pattern 11011 
as a two's complement integer or as an unsigned integer. 
If we add the two's complement of N (i.e. 11011) to 
another binary number, we should execute the operation of 
subtraction. In the following demonstration we add 11011 to 
01100 (i.e. 12). 
01100 
+ 11011 
100111 
12 
7 
As in the case of ten's complement arithmetic, we get 
the correct answer together with the 2" = 25 term, which is 
LetX = 9 = 01001 and Y= 6 = 00110 
- X = 100000 - 01001 = 10111 
-Y = 100000 -00110 = 11010 
1. + X + 9 
01001 
2. +X +9 
01001 
+ Y + 6 + 00110 
01111 = 15 
-Y ^6 + 11010 
.100011 = +3 
3. -X -9 
10111 
4. -X -9 
10111 
+Y +6 + 00110 
11101 = -3 
-Y -6 + 11010 
110001 = -ia 
discarded. Before continuing further, it is worthwhile exam-
ining the effect of adding all the combinations of positive and 
negative values for a pair of numbers. 
All four examples give the result we'd expect when the 
result is interpreted as a two's complement number. However, 
examples 3 and 4 give negative results that require a little fur-
ther explanation. Example 3 calculates — 9 + 6 by adding the 
two's complement of 9 to 6 to get —3 expressed in two's com-
plement form. The two's complement representation of —3 is 
given by 100000 - 00011 = 11101. 
Example 4 evaluates — X H— Y to get a result of —15 but 
with the addition of a 2" term. The two's complement repre-
sentation of-15 is given by 100000 - 01111 = 10001. In 
example 4, where both numbers are negative, we have 
(2" -X) 
+ {2"-Y) 
= 2" + {2n -X-Y). 
The first part of 
this expression is the redundant 2" and the second part is the 
two's complement representation of ~X — Y. The two's com-
plement system works for all possible combinations of posi-
tive and negative numbers. 
Calculating two's complement values 
The two's complement system would not be so attractive if it 
weren't for the ease with which two's complements can be 
formed. Consider the two's complement of N, which is 
defined as: 2" — N. 
Suppose we re-arranged the equation by subtracting 1 
from the 2" and adding it to the result. 
2" - 1 -N+ 
1 = 111... 1 - N + 1 
n places 
For example, in 8 bits (n = 8) we have 
28 - N = 100000000 - N = 100000000 - 1 - N + 1 
(after rearranging) = 11111111 — N + 1 
In practice, it's easy to evaluate the two's complement of N. 
All you have to do is invert the bits and add 1. Why? Because 
the previous expression demonstrates that 1 — N{ = N;. If bit 
i of N is 0, subtracting bit i from 1 gives 1, and if the bit is 1, 
subtracting bit i from 1 gives 0. For example, in 5 bits we have 
7 = 00111 
- 7 = 0011166111+ 1 = 11000+ 1 = 11001 
Evaluating two's complement numbers in this fashion is 
attractive because it's easy to perform with hardware. 
Figure 4.32 demonstrate how an adder/subtractor is imple-
mented. All you need is a little extra logic to convert a parallel 
binary adder into an adder/subtractor for two's complement 
numbers. Each of the EOR gates has two inputs b{ (where 
i = 0 to « — 1) and C, a control signal. The output of the 
EOR gate is b{ • C + ¥( • C. If C is 0 then C = 1 and the output 
is by If C is 1 then C = 0 and the output is br The n EORs 
form a chain of programmable invertors, complementing the 
input if C = 1 and passing the input unchanged if C = O.The 
carry-in input to the first full adder is C. When addition is 
being performed, C = 0 and the carry-in is 0. However, when 
we perform subtraction, C = 1 so that 1 is added to the result 
of the addition. We have already inverted B's bits so that 

178 
Chapter 4 Computer arithmetic 
am-1 fyn-1 
^ 
A 
B 
q, 
Full adder 
Cout Sum 
az 
bz 
V 
A 
B 
q. 
Full adder 
V 
A 
B 
Cir 
Full adder 
Q>ut Sm_-| 
s2 
Fig. 4.32 The binary adder/subtractor. 
do 
vo 
V 
A 
B 
q, 
Full adder 
Com Sum 
% 
- C 
The control input 
adds A and B when 
C = 0 and subtracts 
A - B when C = 1. 
adding this 1 forms the two's complement of B enabling the 
subtraction of B from A to take place. 
Properties of two's complement numbers 
1. The two's complement system is a true complement system in 
that +X + (-X) = 0. For example, in 5 bits +1310 = 
0110110 and - 132 = 100112. The sum of+ 13 and - B i s 
01101 
+ 10011 
100000 = 0 
2. There is one unique zero 00 ... 0. 
3. If the number is positive the most-significant bit is 0, 
and if it is negative the most-significant bit is 1. Thus, the 
most-significant bit is a sign bit. 
4. The range of two's complement numbers in n bits is 
from -2" ' to +2" " ' - 1. For n = 5, this range is 
from -16 to +15. Note that the total number of different 
numbers is 32 (16 negative, zero and 15 positive). What this 
demonstrates is that a 5-bit number can uniquely describe 
32 items, and it is up to us whether we choose to call these 
items the natural binary integers 0 to 31, or the signed two's 
complement numbers —16 to +15. 
5. The complement of the complement of X is X (i.e. 
- ( - X ) =X).In5bits +12 = 01100and -12 = 10011 + 
1 = 10100. If we form the two's complement of -12 (i.e. 
10100) in the usual fashion by inverting the bits and adding 1, 
we get lOlOO + 1 = 01011 + 1 = 01100,whichisthesameas 
the number we started with. 
Let's now see what happens if we violate the range of two's 
complement numbers. That is, we will carry out an operation 
whose result falls outside the range of values that can be 
represents by two's complement numbers. 
Case 2 
12 = 01100 
+ 13 = 01101 
25 
11001 = - 7 , 0 (as a 
two's complement number) 
If we choose a 5-bit representation, we know that the range 
of valid signed numbers is -16 to +15. Suppose we first add 
5 and 6 and then try 12 and 13. 
Case 1 
5 = 00101 
+6 = 00110 
11 
01011 = ll,o 
In case 1 we get the expected answer of +11 m, but in case 2 
we get a negative result because the sign bit is' 1'. If the answer 
were regarded as an unsigned binary number it would 
be +25, which is, of course, the correct answer. However, 
once the two's complement system has been chosen to 
represent signed numbers, all answers must be interpreted in 
this light. 
Similarly, if we add together two negative numbers whose 
total is less than -16, we also go out of range. For example, if 
we add - 9 = 101112 and -12 = 101002, we get 
- 
9 
- 1 2 
- 2 7 
10111 
+ 10100 
101011 
gives a positive result 01011,2= +H10 
Both these cases represent an out-of-range condition 
called arithmetic overflow. Arithmetic overflow occurs during 
a two's complement addition if the result of adding two 
positive numbers yields a negative result, or if adding two 
negative numbers yields a positive result.2 If the sign bits of 
A and B are the same but the sign bit of the result is different, 
arithmetic overflow has occurred. If a„_, is the sign bit of A, 
2 Some define overflow more generally as 'A condition that occurs 
when the result of an operation does not fit the number representation 
in use'. 
Full adder 
Cput Sum 
Full adder 
Cout Sum 
u- 
. 
1 
Full adder 
Out Sum 
n 
1 

4.8 Signed numbers 
179 
Considering 
both 
cases, 
overflow 
occurs 
if 
Cn'Cn-\ 
< cn'cn-l 
= 
' . 
Alternative view of two's complement numbers 
We have seen that a binary integer, N, lying in the range 
0 s N < 2" — 1, is represented in a negative form in n bits by 
the expression 2" - N. We have also seen that this expression 
can be readily evaluated by inverting the bits of N and adding 
1 to the result. 
Another way of looking at a two's complement number is 
to regard it as a conventional binary number represented 
in the positional notation but with the sign of the most-
significant bit negative. That is, 
- N = - rf„_,2n"1 + d„-22"-2 + ... + d02° 
where dn l7 dn._,, • • • d0 are the bits of the two's complement 
number D. Consider the binary representation of 1410 and 
the two's complement form of —14, in 5 bits. 
+ 1410 = 011102 
-14 = 2" - N = 25 - 14 = 32 - 14 = 18 = 10010 
o r - 1 4 = 0 T T T 0 + l = 10001 + 1 = 100102. 
We can regard the two's complement representation 
of-14 (i.e. 10010) as 
- 1 X 2 4 + OX23 + O X 2 2 + 1 X 2 ' + OX2° 
(this is a con-
ventional 8421-coded binary number with a negative 
weight for the most-significant bit) 
= - 1 6 + (0 + 0 + 2 + 0) 
= - 1 6 + 2 = -14 
We can demonstrate that a two's complement number is 
indeed represented in this way. In what follows N represents a 
positive integer, and D the two's complement form of —N. 
We wish to prove that —N = D. 
That is, - N = - 2 n _' + 2 d,2' 
(1) 
>=i 
In terms of the bits of N and D we have 
-(N„_,N„_2... N,N0) = dn.A-i---dA 
= D 
(2) 
The bits of D are formed from the bits of AT by inverting and 
adding 1. 
N^~iN^2...~NiNv+l 
= d„_A-2---<iA 
(3) 
Substituting equation 3 in equation 1 to eliminate D we get 
n - 2 _ 
-N = -2"'1 + 2 ^ 2 ' ' + 1 
ButN, = 1 -Ni, so that 
_ 
n-2 
~N= -2"~' + 2(1 ~ W + 1 
n-2 
n-2 
= -2"-1 + 2 2' - S Ni2' + 1 
i=o 
;=o 
n-2 
= 2"_1 + (2""1 - 1) - 2 N;2'' + 1 
i=0 
f»„_i is the sign bit of B, and sn_, is the sign bit of the sum of 
A and B, then overflow is defined by 
V = a ^ r ^ - i - s ^ , + 
a„.i-bn-rs„-i 
Arithmetic overflow is a consequence of two's complement 
arithmetic and shouldn't be confused with carry-out, which 
is the carry bit generated by the addition of the two most-
significant bits of the numbers. 
In practice, real systems detect overflow from Cin + Cou, to 
the last stage. That is, we detect overflow from 
V = cn-cn-x + c„-c„-i 
We now demonstrate that this expression is correct. This 
proof has been included to improve your understanding of 
the nature of two's complement arithmetic. 
Figure 4.33 illustrates the most-significant stage of a paral-
lel adder that adds together bits a„_,, b„_ 1? and c„ , to gener-
ate a sum bit, sn_ „ and a carry-out, cn. There are four possible 
combinations of A and B that can be added together 
+ A + +B 
+A+ - B 
- A + + B 
- A + - B 
As adding two numbers of differing sign cannot result in 
arithmetic overflow, we need consider only the cases where 
A and B are both positive, or both negative. 
Case 1 A and B positive a„_, = 0, b„_[ = 0 
The final stage adds a„_1 + £>„_! + cn_{ to get cnA, because 
a„_, and fr„_, are both 0 (by definition if the numbers are pos-
itive). That is, the carry-out, c„, is 0 and s„_i = c„_,. 
We know overflow occurs if s„.., = 1, therefore overflow 
occurs if the sum is negative and c~n-c„-x = 1. 
Case 2 A and B negative a„_j = l,fo„_i = 1. 
The final stage adds <J„_, + fr„_, + c„_, = 1 + 1 + <:„-,, to 
get a sum, s„_1 = c„__j and a carry-out c„ = 1. Overflow 
occurs if the sum is positive and s„^[ = 0. That is, if c„_j = 0, 
orifc„-ET, = 1. 
an-l V l 
cn-1 
t 
t 
T 
Demonstrating that 
FA 
_ 
T 
' 
'' 
Fig. 4.33 Most-significant stage of a full adder. 
Demonstrating that 
Fig. 4.33 Most-significant stage of a full adder. 
an-l V l 
cn-1 
FA 
t 
V - 
crfn-'\ + 
cncn-1 
ButN, = 1 -Ni, so that 
_ 
n-2 
~N= 
-2-'1 + 2 ( 1 ~ Ni)2' + 1 
n-2 
n-2 
= -2"-1 + 2 2' - S Ni2' + 1 
i=n 
;=n 
n-2 
= 2"_1 + (2""1 - 1) - 2 N;2'' + ! 
i=0 
n-2_ 
- N = -2"' 1 + 2 ^ 2 ' ' + 1 
i"=0 
N„_, N„_ 2... Nj N0 + 1 = d„_,d„_2... rf^o 

180 
Chapter 4 Computer arithmetic 
= - 2 " " ' + (2""1 - 1) + 1 - N (because 
n-2 
^ N , 2 ' = Nas the most-significant bit of N is zero 
for N to be within its stated range) 
= -2"" 1 + 2""1 - N 
= -N 
Representing two's complement numbers graphically 
We can visualize numbers in the two's complement system by 
arranging numbers around a circle. Figure 4.34 demonstrates 
such an arrangement for 4-bit numbers in which a circle has 
been divided up by 16 radials numbered from 0000 to 1111. 
Suppose we number the radials according to their two's com-
plement values, so that radials 0000 to 0111 are numbered 
0 to 7 and radials 1000 to 1111 are numbered —8 to — 1. 
Notice how stepping one place clockwise increases a number 
and stepping one place counterclockwise decreases a number. 
We can now see how adding numbers to a negative value 
causes the result to move in the direction toward zero. For 
example, if we add 0011 ( + 3) to 1100 (-4) we get 1111 
which is - 1 . If we had added 0101 (+5) to 1100, we would 
have got (1 )0001 which is +1 and lies to the right of zero. 
4.8.4 One's complement representation 
An alternative to two's complement arithmetic is one's com-
plement arithmetic in which the representation of a negative 
number, N, in n bits is given by 2" — N— 1. The one's com-
plement representation of a number is one less than the cor-
responding two's complement representation and is formed 
more simply by inverting the bits of N. For example, for 
n = 5 consider the subtraction of 4 from 9. The binary value 
of 4 is 00100 and its one's complement is 11011. 
Fig. 4.34 Visualizing two's complement numbers. 
+9 = +01001 
• 
01001 
-4 = -00100 
• + 11011 
100100 -4 
result after addition 
00101 •* 
result after adding 
in the end-
around-carry 
After the addition has been completed, the leftmost bit of 
the result is added to the least-significant bit of the result in 
an arrangement called end-around-carry. This provides us 
with the final and correct result. Note that if the result of the 
initial addition yields a carry-out of zero, the result is negative 
and adding in the carry-out (i.e. zero) also gives the correct 
answer. Consider the following example. 
-9 = -01001 
• 10110 
+ 4 = +00100 
• + 00100 
011010 <—result after addition 
11010 A—result after adding 
in the end-
around-carry 
In one's complement 11010 represents the value —00101 
(i.e. - 5 ) . 
We can demonstrate that end-around-carry works in one's 
complement arithmetic as follows. Suppose we wish to com-
pute X — Y. We add the one's complement of Y to X to get 
X + (2" - Y - 1) = 2" + (X - Y) - 1. By transferring the 
carry-out, 2", to the least-significant bit, we correct the result 
(X - Y) - 1 to (X - Y) by canceling the - 1 term. 
If we add together two one's complement negative numbers 
weget-X+ - Y = (2" - X - 1) + ( 2 " - Y- 1) = 2" — 1 + 
2" — 1 — (X + Y). If we apply end-around-carry to this 
result, the first 2" term cancels the first — 1 term to leave 
2" — 1 — (X + Y). This is, of course, the correct result in one's 
complement form. 
The one's complement system is not a true complement as 
the value of X + (—X) is not zero. Furthermore, there are two 
representations for zero: 00 . . . 0 and 11 . . . 1. Today, the 
one's complement system is rarely used to represent signed 
numbers. 
It's instructive to compare the various ways of representing 
numbers we have encountered so far. Table 4.23 shows the 
sequence of 5-bit binary numbers for n — 5 for pure binary 
numbers, sign and magnitude, one's complement, and two's 
complement 
representations. The rightmost 
column 
includes the biased representation of signed numbers—a 
system that we will use when we describe floating point 
numbers. In this case the biased representation of a number 
iiii 
7 « n 
mo S / I T 
o +1V/ 0 0 1 0 
HOI-/ 
V-oon 
/ — 3 
+ 3 \ 
1100 — 
4 Negative 
Positive 
+4——0100 
l O I T V , 
7*^0101 
/ \ 
-7 
Q +7 
X 
ioio 
\ y ^ | _ \ ^ 
011° 
1001 
' 
0111 
1000 
+6, 
+5 

4.9 Floating point numbers 
181 
Binary code 
Natural binary 
0 
Sign; 
00000 
Natural binary 
0 
0 
00001 
1 
1 
00010 
2 
2 
00011 
3 
3 
00100 
4 
4 
00101 
5 
5 
00110 
6 
6 
00111 
7 
7 
01000 
8 
8 
01001 
9 
9 
01010 
10 
10 
01011 
11 
11 
01100 
12 
12 
01101 
13 
13 
OHIO 
14 
14 
01111 
15 
15 
10000 
16 
- 0 
10001 
17 
- 1 
10010 
18 
- 2 
10011 
19 
- 3 
10100 
20 
- 4 
10101 
21 
- 5 
10110 
22 
- 6 
10111 
23 
- 7 
11000 
24 
- 8 
11001 
25 
- 9 
11010 
26 
- 1 0 
11011 
27 
- 1 1 
11100 
28 
- 1 2 
11101 
29 
- 1 3 
11110 
30 
- 1 4 
11111 
31 
- 1 5 
Table 4.23 The representation of negative numbers. 
is 15 greater than the actual number; for example, 7 is repre-
sented by 7 + 15 = 22 = 101102. 
4.9 Floating point numbers 
So far, we've largely dealt with integer values. Let's look at a 
simple way of handling numbers with both integer and 
fractional parts (e.g. 13.7510 or 1101.112). Fortunately, a 
binary (or decimal) fraction presents no problems. Consider 
the following two calculations in decimal arithmetic. 
One's complement 
Two's complement 
Biased form 
0 
0 
-15 
1 
1 
-14 
2 
2 
-13 
3 
3 
-12 
4 
4 
-11 
5 
5 
-10 
6 
6 
- 9 
7 
7 
- 8 
8 
8 
- 7 
9 
9 
- 6 
10 
10 
- 5 
11 
11 
- 4 
12 
12 
- 3 
13 
13 
- 2 
14 
14 
- 1 
15 
15 
0 
-15 
-16 
1 
- 1 4 
-15 
2 
-13 
- 1 4 
3 
-12 
-13 
4 
-11 
-12 
5 
-10 
-11 
6 
- 9 
-10 
7 
- 8 
- 9 
8 
- 7 
- 8 
9 
- 6 
- 7 
10 
- 5 
- 6 
11 
- 4 
- 5 
12 
- 3 
- 4 
13 
- 2 
- 3 
14 
- 1 
- 2 
15 
- 0 
- 1 
16 
Case 1 Integer arithmetic Case 2 Fixed point arithmetic 
7632135 
763.2135 
+1794821 
+179.4821 
9426956 
942.6956 
Although case 1 uses integer arithmetic and case 2 uses 
fractional arithmetic, the calculations are entirely identical. 
The only difference is in the location of the decimal point. We 
can extend this principle to computer arithmetic. All the 
computer programmer has to do is to remember where the 
binary point is assumed to lie. Input to the computer is scaled 
to match this convention and the output is similarly scaled. 
The internal operations themselves are carried out as if the 
and magnitude 

182 
Chapter 4 Computer arithmetic 
numbers were in integer form. This arrangement is called 
fixed point arithmetic, because the binary point is assumed to 
remain in the same position. That is, there is always the same 
number of digits before and after the binary point. The 
advantage of the fixed point representation of numbers is 
that no complex software or hardware is needed to imple-
ment it. 
A simple example should make the idea of fixed point arith-
metic clearer. Consider an 8-bit fixed point number with the 
four most-significant bits representing the integer part and 
the four least-significant bits representing the fractional part. 
Let's see what happens if we wish to add the two numbers 
3.625 and 6.5 and print the result. An input program first 
converts these numbers to binary form. 
3.625 -> 11.101 -» 0011.1010 (in8bits) 
6.5 
-> 110.1 
-> 0110.1000 (in8bits) 
The computer now regards these numbers as 00111010 
and 01101000, respectively. Remember that the binary point 
is only imaginary. These numbers are added in the normal 
way to give 
00111010 
+01101000 
10100010 
This result would be equal to 162,0 if we were to regard it as 
an unsigned natural binary integer. But it isn't. We must 
regard it as a fixed point value. The output program now 
takes the result and splits it into an integer part 1010 and a 
fractional part 0010. The integer part is equal to 1010 and the 
fractional part is 0.12510. The result would be printed as 
10.125. 
In practice, a fixed point number may be spread over several 
words to achieve a greater range of values than allowed by a 
single word. The fixed point representation of fractional num-
bers is very useful in some circumstances, particularly for 
financial calculations. For example, the smallest fractional 
part may be (say) 0.1 of a cent or 0.001 of a dollar. The largest 
integer part may be $ 1 000 000. To represent such a quantity in 
BCD a total of 6 X 4 + 3 X 4 = 36 bits are required. A byte-
oriented computer would require 5 bytes for each number. 
Fixed point numbers have their limitations. Consider the 
astrophysicist who is examining the sun's behavior. They are 
confronted with numbers ranging from the mass of the sun 
(1990000000000000000000000000000000 grams) to the mass 
of an electron 
(0.000000000000000000000000000910956 
grams). 
If astrophysicists were to resort to fixed point arithmetic, 
they would require an extravagantly large number of bits to 
represent the range of numbers used in their trade. A single 
byte represents numbers in the range 0 to 255. If the physicist 
wanted to work with astronomically large and microscop-
ically small numbers, roughly 14 bytes would be required for 
the integer part of the number and 12 bytes for the fractional 
part; that is, they would need a 26-byte (208 bit) number. A 
clue to a way out of our dilemma is to note that both figures 
contain a large number of zeros but few significant digits. 
4.9.1 Representation of floating point 
numbers 
We can express a decimal number such as 1234.56 in the form 
0.123456X104 which is called the floating point format or 
scientific notation. The computer handles large and small 
binary values in a similar way; for example, 1101101.1101101 
may be represented internally as 0.11011011101101 X 27 
(the 7 is, of course, also stored in a binary format). Before 
looking at floating point numbers in more detail we should to 
consider the ideas of range, precision, and accuracy, which are 
closely related to the way numbers are represented in floating 
point format. 
Range 
A number's range tells us how big or how small 
it can be; for example, the astrophysicist was dealing with 
numbers as large as 2 X 1033 and those as small as 
9 X 10-28, representing a range of approximately 1061, or 61 
decades. The range of numbers capable of representation by 
a computer must be sufficient for the calculations that are 
likely to be performed. If the computer is employed in a 
dedicated application where the range of data to be handled 
is known to be quite small, then the range of valid numbers 
may be restricted, simplifying the hardware/software 
requirements. 
Precision 
The precision of a number is a measure of its 
exactness and corresponds to the number of significant 
figures used to represent it. For example, the constant -rr 
may be written as 3.142 or 3.141592. The latter value is 
more precise than the former because it represents IT to one 
part in 107 whereas the former value represents IT to one part 
in 104. 
Accuracy Accuracy has been included here largely to con-
trast it with precision, a term often incorrecdy thought to 
mean the same as accuracy. Accuracy is the measure of the 
correctness of a quantity. For example, we can say IT — 3.141 
or 77 = 3.241592. The first value is a low-precision number 
but is more accurate than the higher precision value, which 
has an error in the second digit. In an ideal world accuracy and 
precision would go hand in hand. It's the job of the computer 
programmer to design algorithms that preserve the accuracy 
that the available precision allows. One of the potential 
hazards of computation is calculations that take the form 
A + B 
A - 
B 
For example, 
1234.5687 + 1234.5678 
= 2469.1365 
1234.5687 - 1234.5678 
= 
0.0009 

4.9 Floating point numbers 
183 
Exponent 
Mantissa 
This floating point number 
represents a x 2e. 
Floating point number 
Fig. 4.35 Storing a floating-
point number. 
When the denominator is evaluated we are left with 
0.0009, a number with only one decimal place of precision. 
Although the result might show eight figures of precision, it 
may be very inaccurate indeed. 
A floating point number can be represented in the form 
a X f where a is the mantissa (also called the argument)-, r is 
the radix or base, and e is the exponent or characteristic. The 
computer stores a floating point number by splitting the 
binary sequence representing the number into the two fields 
illustrated in Fig. 4.35. The radix r is not stored explicitly by 
the computer. 
Throughout the remainder of this section the value of the 
radix in all floating point numbers is assumed to be two. 
Before the IEEE format became popular, some computers 
used an octal or hexadecimal exponent, so that the mantissa 
is multiplied by 8e or 16e, respectively. For example, if a float-
ing-point number has a mantissa 0.101011 and an octal 
exponent of 4 (i.e. 0100 in 4 bits), the number is equal to 
0.101011 X 84 or 0.101011 X 212, which is 1010110000002. 
It's not necessary for a floating point number to occupy a 
single storage location. Indeed with an 8-bit word, such a rep-
resentation would be useless. Several words are grouped to 
form a floating point number (the number of words required 
is bits-in-floating-point-representation/computer-word-
length). The split between exponent and mantissa need not 
fall at a word boundary. That is, a mantissa might occupy 3 
bytes and the exponent 1 byte of a two 16-bit word floating-
point number. 
When constructing a floating point representation for 
numbers, the programmer must select the following. 
1. The total number of bits. 
2. The representation of the mantissa (two's complement etc.). 
3. The representation of the exponent (biased etc.). 
4. The number of bits allocated to the mantissa and exponent. 
5. The location of the mantissa (exponent first or mantissa first). 
Point 4 is worthy of elaboration. Once you've decided on the 
total number of bits in the floating point representation, the 
number must be split into a mantissa and exponent. 
Dedicating a large number of bits to the exponent lets you rep-
resent numbers with a large range. Gaining exponent bits at the 
expense of the mantissa reduces the precision of the floating 
point number. Conversely, increasing the bits available for the 
mantissa improves the precision at the expense of the range. 
Once, almost no two machines used the same format. 
Things improved with the introduction of microprocessors. 
Today, the IEEE standard for floating-point numbers 
dominates the computer industry. Accordingly, we concen-
trate on this standard. 
4.9.2 Normalization of floating 
point numbers 
By convention a floating point mantissa is always normalized 
unless it is equal to zero and is expressed in the form l.F 
where F is the fractional part.3 Because a normalized IEEE 
floating pint mantissa always begins with a 1, this is called 'the 
leading 1'. A normalized mantissa is therefore in the range 
1.00... 00 to 1.11 . . . 1 1 ; that is 
— 2< x < — 1, or x = 0, or 1 < x < 2. 
If the result of a calculation were to yield 11.010 . . . X 2e, 
the result would be normalized to give 1.1010... X2 e + 1. 
Similarly, the result 0.1001 . . . X 2e would be normalized to 
1.001 . . . x r 1 . 
By normalizing a mantissa, the greatest possible advantage 
is taken of the available precision. For example, the unnor-
malized 8-bit mantissa 0.00001010 has only four significant 
bits, whereas the normalized 8-bit mantissa 1.0100011 has 
eight significant bits. It is worth noting here that there is a 
slight difference between normalized decimal numbers as 
used by engineers and scientists, and normalized binary 
numbers. By convention, a decimal floating point number is 
normalized so that its mantissa lies in the range 1.00 ... 0 to 
9.99 . . . 9. 
A special exception has to be made in the case of zero, as 
this number cannot, of course, be normalized. 
Because the IEEE floating-point format uses a sign and 
magnitude format, a sign-bit indicates the sign of a mantissa. 
A negative floating point mantissa is stored in the form 
x = —1.11 ... 1 to -1.00...0 
A floating point number is limited to one of the three 
ranges — 2 < x < — 1, or x = 0, or l ^ x < 2 described by 
Fig. 4.36. 
Biased exponents 
A floating-point representation of numbers must make 
provision for both positive and negative numbers, and 
3 Before the advent of the IEEE standard, floating point numbers were 
often normalized in the form 0.1 ...x 2' and constrained to the range 
>A£x< \or-'A>x» 
- ] . 

184 
Chapter 4 Computer arithmetic 
-1.1111 
-1.000. 
1.000. 
0.000 . 
/ 
Valid negative 
mantissas 
Valid positive 
mantissas 
- 2 
Figure 4.36 Range of valid normalized two's complement mantissas. 
Binary value 
True exponent 
Biased form 
0000 
- 8 
0 
0001 
- 7 
1 
0010 
- 6 
2 
0011 
- 5 
3 
0100 
- 4 
4 
0101 
- 3 
5 
0110 
- 2 
6 
0111 
- 1 
7 
1000 
0 
8 
1001 
1 
9 
1010 
2 
10 
1011 
3 
11 
1100 
4 
12 
1101 
5 
13 
1110 
6 
14 
1111 
7 
15 
For example, if « = 1010.1111,we normalize it to+1.0101111 X23. The 
true exponent is + 3, which is stored as a biased exponent of 3 + 8, which is 
1110 or 1011 in binary form. 
Table 4.24 Relationship between true and biased exponents. 
representing zero by a 
zero exponent and mantissa 
Exponent 
Mantissa 
S* 
• ' 
0 000 
0 J 00 0 
0 
Fig. 4.37 Representing zero in floating point arithmetic. 
positive and negative exponents. The following example in 
decimal notation demonstrates this concept. 
+0.123 X 1012, -0.756 X 109,+0.176 X 10"3, 
-0.459 X 10 7 
The mantissa of an IEEE format floating point number is 
represented in sign and magnitude form. The exponent, how-
ever, is represented in a biased form. An m-bit exponent pro-
vides 2m unsigned integer exponents from 0 0 . . . 0 to 
11 . . . 1. Suppose that we relabel these 2m values from - 2 m - 1 
1.1111... 
to +2'" ' —1 by subtracting a constant value 
(or bias) of B = 2m~l from each of the num-
bers. We get a continuous natural binary series 
from 0 to N representing exponents from — B 
toN- 
B. 
If we use a 3-bit decimal biased exponent 
with 5 = 4, the biased exponents are 0,1,2, 3, 
4, 5, 6, 7 and represent the actual expo-
nents-4, - 3 , - 2 , - 1 , 0, 1, 2, 3. We've 
invented a way of representing negative num-
bers by adding a constant to the most negative 
number to make it equal to zero. In this exam-
ple, we've added 4 to each true number so that —4 is repre-
sented by the biased values 0, and — 3 by +1, etc. 
We create a biased exponent by adding a constant to the 
true exponent so that the biased exponent is given by 
b' = b + B, where V is the biased exponent, b the true expo-
nent, and B a weighting. The weighting B is frequently either 
2m~1 or 2m~1 — 1. Consider what happens for the case where 
m = 4 and B = 23 = 8. (See Table 4.24). 
The true exponent ranges from —8 to +7, allowing us to 
represent powers of 2 from 2~8 to 2+7, while the biased expo-
nent ranges from 0 to +15. The advantage of the biased rep-
resentation of exponents is that the most negative exponent is 
represented by zero. Conveniently, the floating-point value of 
zero is represented by 0.0 . . . 0 X 2most ne»a,ive exPonem (see 
Figure 4.37). By choosing the biased exponent system we 
arrange that zero is represented by a zero mantissa and a zero 
exponent as Figure 4.36 demonstrates. 
The biased exponent representation of exponents is also 
called excess n, where n is typically 2m~'. For example, a 6-bit 
exponent is called excess 32 because the stored exponent 
exceeds the true exponent by 32. In this case, the smallest 
true exponent that can be represented is — 32 and is stored as 
an excess 32 value of 0. The maximum true exponent that can 
be represented is 31 and this is stored as 63. 
A second advantage of the biased exponent representation 
is that the stored (i.e. biased) exponents form a natural binary 
sequence. This sequence is monotonia so that increasing the 
exponent by 1 involves adding 1 to the binary exponent, and 
decreasing the exponent by 1 involves subtracting one from 
the binary exponent. In both cases the binary biased expo-
nent can be considered as behaving like an unsigned binary 
number. Consequently, you can use relatively simple logic to 
compare two exponents. Remember that in 4-bit signed arith-
metic the number 0110 is larger than 1110 because the 
second number is negative. If these were biased exponents, 
1110 would be larger than 0110. 
IEEE floating point format 
The Institute of Electronics and Electrical Engineers (IEEE) 
has defined a standard floating point format for arithmetic 
operations called ANSI/IEEE standard 754-1985. To cater for 
I 
-
1
0 
1
2 

4.9 Floating point numbers 
185 
different applications, the standard specifies three basic 
formats, called single, double, and quad. Table 4.25 defines the 
principal features of these three floating point formats. 
An IEEE format floating point number X is formally 
defined as 
X= - 1 S X 2E~BX l.F, 
where S = sign bit, 0 = positive mantissa, 1 = negative 
mantissa, E = exponent biased by B, F = fractional mantissa 
(note that the mantissa is 1. F and has an implicit leading 1). 
A single-format 32-bit floating-point number has a bias of 
127 and a 23-bit fractional mantissa. A sign and magnitude 
representation has been adopted for the mantissa; if S = 1 
the mantissa is negative and if S = 0 it is positive. 
The mantissa is always normalized and lies in the range 
1.000 . . . 00 to 1.111 . . . 11. If the mantissa is always normal-
ized, it follows that the leading 1, the integer part, is redun-
dant when the IEEE format floating point number is stored in 
Single 
Double 
Quad 
precision 
precision 
precision 
Field width in bits 
5 = sign 
1 
1 
1 
E = exponent 
8 
11 
15 
L = leading bit 
1 
1 
1 
F = fraction 
23 
52 
111 
Total width 
32 
64 
128 
Exponent 
Maximum E 
255 
2047 
32 767 
Minimum E 
0 
0 
0 
Bias 
127 
1023 
16 383 
Notes 
5 = sign bit (0 for a negative number, 1 for a positive number). 
L = leading bit (always 1 in a normalized, non-zero mantissa). 
F = fractional part of the mantissa. 
The range of exponents is from Min E + 1 to Max E — 1 
The number is represented by - 1
5 x 2E exP°nent x L • F. 
A signed zero is represented by the minimum exponent, L = 0, and 
F = 0, for all three formats. 
The maximum exponent has a special function that represents signed 
infinity for all three formats. 
memory. If we know that a 1 must be located to the left of the 
fractional mantissa, there is no need to store it. In this way 1 
bit of storage is saved, permitting the precision of the man-
tissa to be extended by 1 bit. The format of the number when 
stored in memory is given in Fig. 4.38. 
As an example of the use of the IEEE 32-bit format, con-
sider the representation of the decimal number —2345.125. 
-2345.12510= -100100101001.0012 (as 
an 
equivalent 
binary number) 
= -1.00100101001001X2" 
(as a nor-
malized binary number) 
The mantissa is negative so the sign bit S is 1. The biased 
exponent is given by +11 + 127 = 138 = 100010102. The 
fractional 
part 
of 
the 
mantissa 
is 
.00100101001001000000000 (in 23 bits). Therefore, the IEEE 
single format representation of —2345.125 is: 
11000101000100101001001000000000 
In order to minimize storage space in computers where the 
memory width is less than that of the floating point number, 
floating point numbers are packed so that the sign bit, expo-
nent and mantissa share part of two or more machine words. 
When floating point operations are carried out, the numbers 
are first unpacked and the mantissa separated from the expo-
nent. For example, the basic single precision format specifies 
a 23-bit fractional mantissa, giving a 24-bit mantissa when 
unpacked and the leading 1 reinserted. If the processor on 
which the floating point numbers are being processed has a 
16-bit word length, the unpacked mantissa will occupy 24 
bits out of the 32 bits taken up by two words. 
If, when a number is unpacked, the number of bits in its 
exponent and mantissa is allowed to increase to fill the avail-
able space, the format is said to be extended. By extending the 
format in this way, the range and precision of the floating 
Imaginary leading bit 
1 bit 
+ 
• - * -
8 bits 
31 30 
23 
Biased exponent 
23 bits 
22 
Fractional mantissa 
32-bit floating point number 
Table 4.25 Basic IEEE floating point formats. 
Figure 4.38 Format of the IEEE 32-bit floating point format. 
IEEE SPECIAL NUMBERS 
When we said that floating point numbers are always 
normalized, we were being economical with the truth. 
The IEEE floating point standard does allow for 
denormalized numbers called denormals. A denormal can 
be used to represent a value that is less than 
1.000... 00 X 2E™". The denormal permits the representation 
of numbers below the minimum at the cost of a reduced 
precision; for example, you can have 
0.000001011101... x 2 ™n, where we have lost six places of 
precision. 
The not-a-number (NaN) can be used to speed the processing 
of chained calculations. For example, if we have Z = A • B • C and C 
is a NaN, the computer can immediately say that Zis also a NaN. 
The special exponent Emax + 1 is used to represent infinity if 
the fractional part is zero or a NaN if the fractional part is 
non-zero. 

186 
Chapter 4 Computer arithmetic 
point number are considerably increased. For example, a 
single format number is stored as a 32-bit quantity. When it is 
unpacked the 23-bit fractional mantissa is increased to 24 bits 
by including the leading 1 and then the mantissa is extended 
to 32 bits (either as a single 32-bit word or as two 16-bit 
words). All calculations are then performed using the 32-bit 
extended precision mantissa. This is particularly helpful 
when trigonometric functions (e.g., sin x, cos x) are evalu-
ated. After a sequence of floating operations have been car-
ried out in the extended format, the floating point number is 
repacked and stored in memory in its basic form. 
In 32-bit single IEEE format, the maximum exponent Emax 
is +127 and the minimum exponent Emin is —126 rather 
than +128 to +127 as we might expect. The special value 
Emm-1 (i.e- —127) is used to encode zero and Emax + 1 is 
used to encode plus or minus infinity or a NaN. A NaN is a 
special entity catered for in the IEEE format and is not a 
number. The use of NaNs is covered by the IEEE standard and 
they permit the manipulation of formats outside the IEEE 
standard. 
4.9.3 Floating point arithmetic 
Unlike integer and fixed point number representations, float-
ing point numbers can't be added in one simple operation. A 
moment's thought should demonstrate why this is so. 
Consider an example in decimal arithmetic. Let A = 12345 
and B = 567.89. In decimal floating point form these num-
bers can be represented by 
A = 0.12345 X 105 and 
B = 0.56789 X 103 
If these numbers were to be added by hand, no problems 
would arise. 
12345 
+ 567.89 
12912.89 
However, as these numbers are held in a normalized float-
ing point format we have the following problem. 
0.12345 x 105 
+0.56789 x 103 
Addition can't take place as long as the exponents are differ-
ent. To perform a floating-point addition (or subtraction) 
the following steps must be carried out. 
1. Identify the number with the smaller exponent. 
2. Make the smaller exponent equal to the larger exponent by 
dividing the mantissa of the smaller number by the same 
factor by which its exponent was increased. 
3. Add (or subtract) the mantissas. 
4. If necessary, normalize the result (post-normalization). 
In the above example we have A = 0.12345 X 105 and 
B = 0.56789 X 103. 
The exponent of B is smaller than that of A which results 
an increase of 2 in B's exponent and a corresponding division 
of B's mantissa by 102 to give 0.0056789 X 105. We can now 
add A to the denormalized B. 
A = 0.1234500 X 105 
+B = 0.0056789 X 105 
0.1291289 X 105 
The result is already in a normalized form and doesn't need 
post-normalizing. Note that the answer is expressed to a pre-
cision of seven significant figures whereas A and B are each 
expressed to a precision of five significant figures. If the result 
were stored in a computer, its mantissa would have to be 
reduced to five figures after the decimal point (because we 
were working with five-digit mantissas). 
When people do arithmetic they often resort to what may 
best be called floating precision. If they want greater preci-
sion they simply use more digits. Computers use a fixed rep-
resentation for floating point numbers so that the precision 
may not increase as a result of calculation. Consider the 
following binary example of floating point addition. 
A = 0.11001 X 2' 
B = 0 . 1 0 0 0 1 X 23 
The exponent of B must be increased by 1 and the mantissa of 
B divided by 2 (i.e. shifted one place right) to make both 
exponents equal to 4. 
A = 0.11001 X 2" 
B = 0.010001 X 24 
1.000011 X 24 
Because the result is no longer normalized, we have to shift 
its mantissa one place right (i.e. divide it by 2) and add 1 to 
the exponent to compensate; that is, the result becomes 
0.1000011 X 25. We've gained two extra places of precision. 
We can simply truncate the number to get 
A + B = 0.10000 X 25 
A more formal procedure for the addition of floating point 
numbers is given in Fig. 4.39 as a flowchart. A few points to 
note about this flowchart are given below. 
1. Because the exponent shares part of a word with the 
mantissa, it's necessary to separate them before the process 
of addition can begin. As we pointed out before, this is called 
unpacking. 
2. If the two exponents differ by more than p + l,wherepisthe 
number of significant bits in the mantissa, the smaller num-
ber is too small to affect the larger and the result is effectively 
equal to the larger number. For example, there's no point in 
adding 0.1234 X 1020 to 0.4567 X 102, because adding 
0.4567 X 102 to 0.1234 X 1020 has no effect on a four-digit 
mantissa. 
3. During the post-normalization phase the exponent is 
checked to see if it is less than its minimum possible value or 

4.9 Floating point numbers 
187 
Unpack the 
numbers 
4 = ax2e' 
B=6x2e2 
Mantissas a and b are expressed in p bits 
STOP 
Yes 
Shift mantissa a right 
one place. 
Compensate by 
e^e-,+1 
_li<e 2 
e2<e1 
Shift mantissa b right 
one place. 
Compensate by 
e2 = e2+1 
-1 = e2 
Add a to b 
Shift mantissa right 
Over range 
Under range 
Shift mantissa left 
e^e-,-1 
Yes 
ERROR 
Exponent overflow 
y \ . 
END 
ERROR 
Exponent underflow 
Fig. 4.39 Flowchart for floating point addition and subtraction. 
/ 
e-|-e2 >p + 1^v 
or 
\ . e2-e-]>p + 1 
/ 
Compare 
> 
e1 and e2 
x 
Test resulting 
mantissa 
Is 
\ 
minimum? 
/ 
Is 
\ ^ 
e1 > maximum? 
/ 
No 
ei = e-i + i 
No 
Yes 

188 
Chapter 4 Computer arithmetic 
greater than its maximum possible value. This corresponds to 
testing for exponent underflow and overflow, respectively. Each 
of these cases represents conditions in which the number is 
outside the range of numbers that the computer can handle. 
Exponent underflow would generally lead to the number 
being made equal to zero, whereas exponent overflow would 
result in an error condition and may require the intervention 
of the operating system. 
Floating point multiplication is easier than addition or 
subtraction because we simply multiply mantissas and add 
exponents. For example if x — s, X 2e' and y = s2 X 2Cj then 
x'y = s,«s2 X 2(c»+e2). The multiplication can be done with 
an integer multiplier (we don't even have to worry about 
signed numbers). Of course, multiplication of two p-bit 
numbers yields a 2p-bit product and we therefore have to 
round down the result of the multiplication. When we add 
the two exponents, we have to remember to subtract the bias 
because each exponent is Etrue + b. 
Rounding and truncation 
We have seen that some of the operations involved in floating 
point arithmetic lead to an increase in the number of bits in 
the mantissa and that some technique must be invoked to 
keep the number of bits in the mantissa constant. The sim-
plest technique is called truncation and involves nothing 
more than dropping unwanted bits. For example, if we trun-
cate 0.1101101 to four significant bits we get 0.1101. 
Truncating a number creates an error called an induced error 
(i.e. an error has been induced in the calculation by an oper-
ation on the number). Truncating a number causes a biased 
error because the number after truncation is always smaller 
than the number that was truncated. 
A much better technique for reducing the number of bits 
in a word is rounding. If the value of the lost digits is greater 
than half the least-significant bit of the retained digits, 1 is 
added to the least-significant bit of the remaining digits. We 
have been doing this with decimal numbers for years—the 
decimal number 12.234 is rounded to 12.23, whereas 13.146 
is rounded to 13.15. Consider rounding to four significant 
bits the following numbers. 
0.1101011 -> 0.1101 
The three bits removed are 011, so do 
nothing 
0.1101101 —>0.1101 + 1 = 0.1110 
The three bits removed 
are 101, so add 1 
Rounding is always preferred to truncation partially 
because it is more accurate and partially because it gives 
rise to an unbiased error. Truncation always undervalues 
the result leading to a systematic error whereas rounding 
sometimes reduces the result and sometimes increases it. 
The major disadvantage of rounding is that it requires 
a further arithmetic operation to be performed on the 
result. 
4.9.4 Examples of floating point 
calculations 
Because handling floating point numbers can be tricky, we 
provide several examples. An IEEE standard 32-bit floating-
point number has the format N = - I s X l.F X 2E~'27, 
where S is the sign bit, F is the fractional mantissa, and E the 
biased exponent. 
Convert the decimal numbers 123.5 and 100.25 into the IEEE 
format for floating point numbers. Then carry out the subtrac-
tion of 123.5—100.25 and express the result as a normalized 
32-bit floating point value. 
123.510 = 1111011.1 
= 1.1110111 X 26 
The mantissa is positive, so S = 0. The exponent is + 6, which 
is stored in biased form as 6 + 127 = 13310 = 100001012. The 
mantissa is 1.1110111, which is stored as 23 bits with the lead-
ing 1 suppressed. The number is stored in IEEE format as 
010000101 10010001000000000000000. 
We can immediately write down the IEEE value for 100.25 
because it is so close to the 123.5 we have just calculated; that 
is, 0 10000101 10010001000000000000000. 
The two IEEE-format floating point numbers taking 
part in the operation are first unpacked. The sign, the expo-
nent, and the mantissa (with the leading 1 restored) must be 
reconstituted. 
The two exponents are compared. If they are the same, the 
mantissas are added. If they are not, the number with the 
smaller exponent is denormalized by shifting its mantissa 
right (i.e. dividing by 2) and incrementing its exponent (i.e. 
multiplying by 2) until the two exponents are equal. Then the 
numbers are added. 
If the mantissa of the result is out of range (i.e. greater than 
1.11111 . . . 1 or less than 1.0000 . . . 0) it must be re-normal-
ized. If the exponent goes out of range (bigger than its largest 
value or smaller than its smallest value) exponent overflow 
occurs and an error is flagged. The result is then repacked and 
the leading 1 in front of the normalized mantissa removed. 
IEEE number 
123.510 = 001000101111011100000 00000000000 
IEEE number 
100.2510 = 0 0100010110010001000000000000000 
These floating-point numbers have the same exponent, so 
we can subtract their mantissas (after inserting the leading 1). 
1.11101110000000000000000 
-1.10010001000000000000000 
0.01011101000000000000000 

4.10 Multiplication and division 189 
The result is not normalized and must be shifted left twice to 
get 1.01110100000000000000000. The exponent must be 
decreased by 2 to get 01000011. The result expressed in floating 
point format is 
0 0100011 01110100000000000000000 
2., 
Carry out the operation 42.6875 — 0.09375 by first converting 
these numbers to the IEEE 32-bit format. Use these floating 
point numbers to perform the subtraction and then calculate 
the new floating point value. 
42.687510 = 101010.10112 
= 1.010101011 X25 
This number is positive and S = 0. The true exponent is 5 
and, therefore, the biased exponent is 5 + 127 (i.e. actual 
exponent + bias) = 132 = 100001002 in 8 bits. The frac-
tional exponent is 010101011(00000000000000). Therefore 
42.6875 is represented as an IEEE floating point value by 
01000010001010101100000000000000. 
Similarly, -0.09375,„ = -0.000112 = -1.1 X 2 4. The 
sign-bit S = 1 because the number is negative and the biased 
exponent E = - 4 + 127 = 123 = 011110112. The frac-
tional mantissa is F = 10000000000000000000000. The 
representation of -0.09375 is therefore 
1011110111 
0000000000000000000000. These two numbers are stored as 
01000010001010101100000000000000 and 
10111101110000000000000000000000, respectively. 
In order to perform the addition we have to unpack these 
numbers to sign + biased exponent + mantissa. 
First number 
0 10000100 01010101100000000000000 
Second number 1 01111011 10000000000000000000000 
We must insert the leading 1 into the fractional mantissa to 
get the true mantissa. 
First number 
0 10000100 101010101100000000000000 
Second number 1 01111011 110000000000000000000000 
In order to add or subtract the numbers, the exponents must 
be the same (we can work with biased exponents). The second 
number's 
exponent 
is 
smaller 
by 
10000100 — 
0111011 = 000010012 = 9,0.We increase the second exponent 
by 9 and shift the mantissa right 9 times to get 
First number 
0 10000100 101010101100000000000000 
Second number 1 10000100 000000000110000000000000000000000 
We 
can 
now 
subtract 
mantissas 
to 
get 
10101010011000000000000. The result is positive with a 
biased 
exponent 
of 
10000100 and a mantissa of 
1.0101010011000000000000. This number would be stored as 
0 10000100 0101010011000000000000 (we've dropped the 
leading 1 mantissa). 
This 
number 
is 
equal 
to + 25 X 1.0101010011 = 
101010.10011 =42.59375. 
Let's perform a floating point multiplication. We'll use two dec-
imal floating point numbers that can be converted into binary 
form without a calculator. Assume we wish to calculate 
X = 256.5x4.652. 
We can immediately write 256.5 = 100000000.12 = 
1.000000001 X 28 and 4.625 = 100.1012 = 1.00101 X 22. 
In IEEE 32-bit format, these two numbers are represented by 
0 10000111 00000000100000000000000 and 
0 10000001 00101000000000000000000 
To multiply the numbers, we unpack the fractional 
mantissas, insert the leading Is, and multiply them. Then we 
add the two biased exponents and subtract one bias. The new 
mantissa is 
1.000000001 X 1.00101 = 1.001010001001012 
If we add the biased mantissas and subtract one bias we get 
10000111 + 10000001-01111111 = 100010012. The final 
IEEE format result is 
0 10001001 00101000100101000000000 = 44944A0,6. 
The decimal result is+1.00101000100101000000000 X 
2 I O O O I O O I - I I H I I I 
= 
1.00101000100101 X 2 1 0 = 
10010100010.01012 = 1186.3125]0. 
4.10 Multiplication and division 
We've looked at addition and subtraction—now we consider 
multiplication and division. Other mathematical functions 
can be derived from multiplication. Division itself will later 
be defined as an iterative process involving multiplication. 
4.10.1 Multiplication 
Binary multiplication is no more complex than decimal 
multiplication. In many ways it's easier as the whole binary 
multiplication table can be reduced to 
0 X 0 = 0 
0 X 1 = 0 
1 X 0 = 0 
1 X 1 = 1 
The multiplication of two bits is identical to their logical 
AND. When we consider the multiplication of strings of bits, 
things become more complex and the way in which multipli-
cation is carried out, or mechanized, varies from machine to 
machine. The faster and more expensive the computer, the 
2: 
EXANPLE 3 

190 
Chapter 4 Computer arithmetic 
more complex the hardware used to implement multiplica-
tion. Some high-speed computers perform multiplication in 
a single operation by means of a very large logic array involv-
ing hundreds of gates. 
Unsigned binary multiplication 
The so-called pencil and paper algorithm used by people to cal-
culate the product of two multidigit numbers, involves the 
multiplication of an tt-digit number by a single digit followed 
by shifting and adding. We can apply the same approach to 
unsigned binary numbers in the following way. The multiplier 
bits are examined, one at a time, starting with the least-signif-
icant bit. If the current multiplier bit is one the multiplicand is 
written down, if it is zero then n zeros are written down 
instead. Then the next bit of the multiplier is examined, but 
this time we write the multiplicand (or zero) one place to the 
left of the last digits we wrote down. Each of these groups of n 
digits is called A partial product. When all partial products have 
been formed, they are added up to give the result of the multi-
plication. An example should make this clear. 
1 0 X 1 3 
Multiplier 
= 11012 
Multiplicand = 10102 
1010 
the algorithm of Table 4.26. The mechanization of the prod-
uct of 1101 X 1010 is presented in Table 4.27. 
Signed multiplication 
The multiplication algorithm we've just discussed is valid 
only for unsigned integers or unsigned fixed point numbers. 
As computers represent signed numbers by means of two's 
complement notation, it is necessary to find some way of 
forming the product of two's complement numbers. It is, of 
course, possible to convert negative numbers into a modulus-
only form, calculate the product, and then convert it into a 
two's complement form if it is negative. That approach wastes 
time. 
We first demonstrate that the two's complement represen-
tation of negative numbers can't be used with the basic shift-
ing and adding algorithm. That is, two's complement 
arithmetic works for addition and subtraction, but not for 
multiplication or division (without using special algo-
rithms). Consider the product of X and —Y. The two's com-
plement representation of — Yis 2"—Y. 
If we use two's complement arithmetic, the product X( — Y) 
is given by X(2" -Y) = 2"X-XY. 
X1101 
1010 
0000 
1010 
1010 
10000010 
Step 1 first multiplier bit = 1, writedown multiplicand 
Step 2 second multiplier bit = 0, write down zeros shifted left 
Step 3 third multiplier bit = 1, write down multiplicand shifted left 
Step 4 fourth multiplier bit = 1, write down multiplicand shifted left 
Step 5 add together four partial products 
The result, 100000102 = 13010, is 8 bits long. The multiplica-
tion of two n-bit numbers yields a 2«-bit product. 
Digital computers don't implement the pencil and paper 
algorithm in the above way, as this would require the storing 
of n partial products, followed by the simultaneous addition 
of n words. A better technique is to add up the partial prod-
ucts as they are formed. An algorithm for the multiplication 
of two n-bit unsigned binary numbers is given in Table 4.26. 
We will consider the previous example of 1101 X 1010 using 
(a) Set a counter to n. 
(b) Clear the 2n-bit partial product register. 
(c) Examine the rightmost bit of the multiplier (initially the least-
significant bit). If it is one add the multiplicand to the n most-
significant bits of the partial product. 
(d) Shift the partial product one place to the right. 
(e) Shift the multiplier one place to the right (the rightmost bit is, of 
course, lost). 
(f) Decrement the counter. If the result is not zero repeat from step c. If 
the result is zero read the product from the partial product register. 
The expected result, — XY, is represented in two's 
complement form by 22"—XY. The most-significant bit is 22" 
(rather than 2") because multiplication automatically yields a 
Multipl ier = 11012 
Multiplicand = 10102 
Step 
Counter 
Multiplier 
Partial product 
Cycle 
aandb 
c 
4 
4 
1101 
1101 
00000000 
10100000 
1 
dande 
4 
0110 
01010000 
1 
f 
3 
0110 
01010000 
1 
c 
3 
0110 
01010000 
2 
dande 
3 
0011 
00101000 
2 
f 
2 
0011 
00101000 
2 
c 
2 
0011 
11001000 
3 
d and e 
2 
0001 
01100100 
3 
f 
1 
0001 
01100100 
3 
c 
1 
0001 
100000100 
4 
dande 
1 
0000 
10000010 
4 
f 
0 
0000 
10000010 
4 
Table 4.26 An algorithm for multiplication. 
Table 4.27 Mechanizing unsigned multiplication. 

4.10 Multiplication and division 
191 
X = 
15 = 01111 
Y = - 1 3 = 10011 
9 
8 
7 
6 
5 
4 
3 
2 
1 
0 
29 
2e 
27 
2 6 
2b 
24 
23 
2l 
21 
2° 
0 
1 
1 
1 
1 
1 
0 
0 
1 
1 
0 
1 
1 
1 
1 
0 
1 
1 
1 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
1 
1 
1 
0 
1 
o 
0 
0 
1 
0 
] 
1 
0 
0 
0 
1 
(Two's complement form) 
First partial product 
Second partial product 
Third partial product 
Fourth partial product 
Fifth partial product 
Uncorrected result 
Correction factor 
Corrected result 
double-length product. In order to get the correct two's com-
plement result we have to add a correction factor of 
22" - 2"X = 2"(2" - X) 
This correction factor is the two's complement of X scaled 
by 2". As a further illustration consider the product of X = 15 
and Y = —13 in 5 bits. 
The final result in 10 bits, 11001111012 = -195,„, is cor-
rect. Similarly, when X is negative and Y is positive, a correc-
tion factor of 2"(2"- Y) must be added to the result. 
When both multiplier and multiplicand are negative the 
following situation exists. 
(2" - X)(2" - Y) = 22" - 2"X - TY + XY 
In this case correction factors of 2"X and 2"Y must be 
added to the result. The 22" term represents a carry-out bit 
from the most-significant position and can be neglected. 
Booth's algorithm 
One approach to the multiplication of signed numbers in 
two's complement form is provided by Booth's algorithm. 
This algorithm works for two positive numbers, one negative 
and one positive, or both negative. Booth's algorithm is 
broadly similar to conventional unsigned multiplication but 
with the following differences. In Booth's algorithm two bits 
of the multiplier are examined together, to determine which 
of three courses of action is to take place next. The algorithm 
is defined below. 
1. If the current multiplier bit is 1 and the next lower order 
multiplier bit is 0, subtract the multiplicand from the 
partial product. 
2. If the current multiplier bit is 0 and the next lower order 
multiplier bit is 1, add the multiplicand to the partial 
product. 
3. If the current multiplier bit is the same as the next lower 
order multiplier bit, do nothing. 
Notel. When adding in the multiplicand to the partial 
product, discard any carry bit generated by the 
addition. 
Note 2. When the partial product is shifted, an arithmetic 
shift is used and the sign bit propagated. 
Note 3. Initially, when the current bit of the multiplier is its 
least-significant bit, the next lower-order bit of the 
multiplier is assumed to be zero. 
The flowchart for Booth's algorithm is given in Fig. 4.40. In 
order to illustrate the operation of Booth's algorithm, 
consider the three products 
13 X 15, -13 X 15, and 
- 1 3 X ( - 1 5 ) . Table 4.28 demonstrates how Booth's 
algorithm mechanizes these three multiplications. 
High-speed multiplication 
We don't intend to delve deeply into the subject of high-speed 
multiplication as large portions of advanced textbooks are 
devoted to this topic alone. Here two alternative ways of 
forming products to the method of shifting and adding are 
explained. 
We have seen in Chapter 2 that you can construct a 2-bit by 
2-bit multiplier by means of logic gates. This process can be 
extended to larger numbers of bits. Figure 4.41 illustrates the 
type of logic array used to directly multiply two numbers. 
An alternative approach is to use a look-up table in which 
all the possible results of the product of two numbers are 
stored in a ROM read-only-memory. Table 4.29 shows how 
two four-bit numbers may be multiplied by storing all 
28 = 256 possible results in a ROM. 
The 4-bit multiplier and 4-bit multiplicand together form 
an 8-bit address that selects one of 256 locations within the 
ROM. In each of these locations the product of the multiplier 
(most-significant four address bits) and the multiplicand 
: 
_L 
O 
O 
j - 
.J. 
- 
L 
'J 
L 
X 
Y 

192 
Chapter 4 Computer arithmetic 
Unpack the 
numbers 
*r 
Set cycle counter 
Clear partial product 
1 
1 ' 
Test current multiplier bit 
and next lower-order bit 
1 r 
i ' 
Multiplier bit = 1 
Next lower bit = 0 
Both multiplier and next 
lower-order bit the same 
Multiplier bit = 0 
Next lower bit = 1 
* 
r 
i r 
i ' 
Subtract multiplicand from 
partial product 
Do nothing 
Add multiplicand to 
partial product 
' r 
Shift partial product right and 
select next pair of multiplier bits 
* r 
Decrement cycle counter 
~^X 
Stop 
« 
Ys s 
Result zero? 
s~ 
t~ to 
Figure 4.40 Flowchart for Booth's algorithm. 
(least-significant four address bits) are stored. For example, 
the product of 2 and 3 is given by the contents of location 
0010 0011, which contains 00000110. 
The disadvantage of this technique is the rapid increase in 
the size of the ROM as the number of bits in the multiplier 
and multiplicand increases. Table 4.30 provides the relation-
ship between the size of a multiplier and the number of bits a 
ROM requires to hold the appropriate multiplication table. 
The multiplication of two 8-bit numbers requires a 
memory capacity of 1048 576 bits. Forming the product of 
large numbers direcdy by look-up table is impracticable. 
Fortunately, it's possible to calculate the product of two 
2n-bit numbers by using an n-bit multiplier. 
Before showing how we proceed with binary numbers, let's 
take a look at the product of two 2-digit decimal numbers, 
and then extend the technique to binary arithmetic. 
— -»on„D„ -r iiv^D] -r IW\JUU -r 
^^ 
34 X 27 = (3 X 10 + 4)(2 X 10 + 7) 
= 3 X 2 X 1 0 2 + 3 X 7 X 1 0 + 4 X 2 X 1 0 + 4 X 7 
= 6 X 102 + 21 X 10 + 8 X 10 + 28 
= 6 X 102 + 29 X 10 + 28 
= 600 + 290 + 28 
= 918 
Now consider the generation of the product of two 8-bit 
numbers by means of 4-bit multipliers. Let the two 8-bit 
numbers A and B be represented by the structure in Fig. 4.42. 
Au represents the four most-significant bits of A, and Aj the 
four least-significant bits. We have already encountered the 
idea of splitting up numbers when we performed 64-bit 
addition on a 32-bit microprocessor. Eight-bit numbers 
A and B can be represented algebraically as follows 
A = Au X 16 + A, and B = Bu X 16 + B[ 
Consequendy, A X B = (Au X 16 + A,)(BU X 16 + B,) 

4.10 Multiplication and division 
193 
1. Multiplicand 
Multiplier 
= 01111 = +15 
= 01101 = +13 
Step 
Multiplier bits 
Partial product 
0000000000 
Subtract multiplicand 
011010. 
1000100000 
Shift partial product right 
1100010000 
Add multiplicand 
0 1 1 0 1 
10011110000 
Shift partial product right 
0001111000 
Subtract multiplicand 
0110.1 
1010011000 
Shift partial product right 
1101001100 
Do nothing 
0 1 1 0 1 
1101001100 
Shift partial product right 
1110100110 
Add multiplicand 
0 1 1 0 1 
10110000110 
Shift partial product right 
0 0 1 1 0 0 0 0 1 1 
The final result is 00110000112, which is equal to +195. Note that the under-
lined numbers represent the bits to be examined at each stage. 
LSB 
LSB 
&S 
S i > 
C1.3 
^ 
C0,2 
% 
2. Multiplicand 
=01111 = +15 
Multiplier 
= 10011 = = - 1 3 
Multiplier bits 
Partial product 
iJa i d 
Step 
= - 1 3 
Multiplier bits 
Partial product 
M u l t i p l i e r 
M u l t i p l . icancl 
Result 
0000000000 
0000 
0000 
00000000 
Subtract multiplicand 
100110. 
1000100000 
0000 
0001 
00000000 
Shift partial product right 
1100010000 
Do nothing 
1 0 0 1 1 
1100010000 
Shift partial product right 
1110001000 
0000 
1111 
00000000 
Add multiplicand 
Shift partial product right 
Do nothing 
10011 
l O j m 
10101101000 
0010110100 
0010110100 
0001 
0001 
0000 
0001 
00000000 
00000001 
Shift partial product right 
0001011010 
Subtract multiplicand 
Shift partial product right 
10.011 
1001111010 
1100111101 
0001 
0010 
1111 
0000 
00001111 
00000000 
0001 
0010 
1111 
0000 
00001111 
00000000 
The result is 11001111012, which corresponds to —195. 
0010 
0001 
00000010 
3. Multiplicand 
= 10001 
Multiplier 
=10011 
= X15 
= X13 
0010 
1111 
00011110 
Step 
Multiplier bits 
Partial product 
0000000000 
Subtract multiplicand 
100110 
0111100000 
1111 
0000 
00000000 
Shift partial product right 
0011110000 
1111 
0001 
00001111 
Do nothing 
1 0 0 U 
0011110000 
Shift partial product right 
0001111000 
Add multiplicand 
1 0 0 1 1 
10_QH 
1010011000 
1101001100 
1101001100 
1111 
1111 
11100001 
Shift partial product right 
Do nothing 
1 0 0 1 1 
10_QH 
1010011000 
1101001100 
1101001100 
Table 4.29 Multiplication by means of a look-up table. 
Shift partial product right 
Subtract multiplicand 
Shift partial product right 
m o i l 
1110100110 
10110000110 
0011000011 
This expression requires the evaluation of four 4-bit prod-
ucts (AUBU, A^BL A]B U, A ^ I ) , the shifting of the products by 
eieht or four positions (i.e. multiplication bv 256 or 16), and 
The result is 00110000112> which corresponds to +195. 
Table 4.28 Three examples of mechanizing Booth's algorithm. 
Figure 4.41 The multiplier array. 
oooo 
oooo 
oooooooo 
oooo 
oooi 
oooooooo 
0000 
m i 
oooooooo 
oooi 
oooo 
oooooooo 
0001 
0001 
00000001 
0001 
m i 
00001111 
ooio 
oooo 
oooooooo 
0010 
0001 
00000010 
0010 
m i 
00011110 
mi 
oooo 
oooooooo 
m i 
oooi 
oooomi 
m i 
m i 
mooooi 
Table 4.29 Multiplication by means of a look-up table. 
This expression requires the evaluation of four 4-bit prod-
ucts (AUBU, A^BL, A]BU, A^I), the shifting of the products by 
eight or four positions (i.e. multiplication by 256 or 16), and 
the addition of four partial products. Figure 4.43 shows how 
this may be achieved. 
Y3 
Y2 
Y, 
I 
I 
I 
N-Po 
^ P 2 
x2 
Q.o S ; y } \ 
Ki,o S{^\^ 
co,o S o ) j \ 
x3 
Po 
Pi 
P3 
P2 
t.z-
$22 
<=2,2 
Ci.i S > 
iCo.i So> 
fc.1 S2> 
co.o S0> 
Cro S ^ 
C2n S2> 
C2.3 ^.^ 
Q>,3 
Xi 
x2 

194 
Chapter 4 Computer arithmetic 
Multiplier bits Address bits Lines in table Total of bits in ROM 
(22") 
(2n x 22") 
16 
64 
64 
384 
256 
1024 
1024 
10240 
4096 
49152 
16384 
229376 
65536 
1048576 
Table 4.30 Relationship between multiplier size and array size. 
4.10.2 Division 
Division is the inverse of multiplication and is performed by 
repeatedly subtracting the divisor from the dividend until the 
result is either zero or less than the divisor. The number of times 
the divisor is subtracted is called the quotient, and the number 
left after the final subtraction is the remainder. That is 
dividend/divisor = quotient + remainder/ divisor 
Alternatively, we can write 
dividend = quotient X divisor + remainder 
Before we consider binary division let's examine decimal 
division using the traditional pencil and paper technique. 
The following example illustrates the division of 575 by 25. 
quotient 
divisor JdMdend 
25)575 
The first step is to compare the two digits of the divisor 
with the most-significant two digits of the dividend and ask 
how many times the divisor goes into these two digits. The 
answer is 2 (i.e. 2 X 25 = 50), and 2 X 25 is subtracted 
from 57. The number 2 is entered as the most-significant 
digit of the quotient to produce the situation below. 
2 
25)575 
50 
7 
The next digit of the dividend is brought down, and the 
divisor is compared with 75. As 75 is an exact multiple of 25, 
a three can be entered in the next position of the quotient to 
give the following result. 
23 
25)575 
50 
75 
75 
00 
As we have examined the least-significant bit of the 
dividend and the divisor was an exact multiple of 75, the 
division is complete, and the quotient is 23 with a zero 
remainder. 
A difficulty associated with division lies in estimating how 
many times the divisor goes into the partial dividend (i.e. 57 
was divided by 25 to produce 2 remainder 7). Although peo-
ple do this mentally, some way has to be found to mechanize 
it for application to computers. Luckily this process is easier 
in binary arithmetic. Consider, the above example using 
unsigned binary arithmetic. 
25 = 110012 
575 = 10001111112 
11001)1000111111 
11001 
The 5 bits of the divisor do not go into the first 5 bits of the 
dividend, so a zero is entered into the quotient and the divi-
sor is compared with the first 6 bits of the dividend. 
01 
11001)1000111111 
11001 
001010 
The divisor goes into the first 6 bits of the dividend once, to 
leave a partial dividend 001010(1 111). 
The next bit of the dividend is brought down to give 
010 
11001)1000111111 
11001 
010101 
11001 
The partial dividend is less than the divisor, and a zero is 
entered into the next bit of the quotient. The process contin-
ues as follows. 
010111 
11001)1000111111 
11001 
00101011 
11001 
000100101 
11001 
000011001 
11001 
0000000000 
In this case the partial quotient is zero, so that the final 
result is 10111, remainder 0. 
Restoring division 
The traditional pencil and paper algorithm we've just dis-
cussed can be implemented in digital form with little modifi-
cation. The only real change is to the way in which the divisor 
is compared with the partial dividend. People do the compar-
ison mentally whereas computers must perform a subtrac-
tion and test the sign of the result. If the subtraction yields a 
positive result, a one is entered into the quotient, but if the 
2 
4 
3 
6 
4 
8 
5 
10 
6 
12 
7 
14 
8 
16 
Zn 
n 

4.10 Multiplication and division 
195 
8b )itS 
8 bits 
Au 
A l 
BU 
B[ 
4 bits 
4 bits 
4 bits 
4 bits 
Fig, 4,42 High-speed multiplication. 
Au 
Al 
Bu 
B| 
i i 
0 
I 
% 
l! 
Si 
t 
V 
Ay 
Bu 
4-bit 
4-bit 
multip ier 
4-bit 
4-bit 
multiplier 
V 
A, 
Bl 
4-bit 
4-bit 
multip ier 
AUBU 
Partial product adder 
AUB, 
A|BU 
A,B, 
AUBU 
Partial product adder 
AUB, 
i 
A,B, 
AUBU 
Partial product adder 
AUB, 
AUBU 
Partial product adder 
j 
A,BU 
AUBU 
Partial product adder 
AUBU 
Partial product adder 
| 
i 
AUB, 
AUBU 
Partial product adder 
| 
i 
J 
r~ 
AUBU 
J J 
2S6AUBU + 16A„B I + 16A,BU + A,B, 
16-bit product 
Fig. 4.43 High-speed multiplication. 
result is negative a zero is entered in the quotient and the divi-
sor added back to the partial dividend to restore it to its pre-
vious value. 
A suitable algorithm for restoring division is as follows. 
1. Align the divisor with the most-significant bit of the 
dividend. 
2. Subtract the divisor from the partial dividend. 
3. If the resulting partial dividend is negative, place a zero 
in the quotient and add back the divisor to restore the 
partial dividend. 
If the resulting partial dividend is positive, place a one in 
the quotient. 
Perform a test to determine end of division. If the divisor 
is aligned so that its least-significant bit corresponds to the 
least-significant bit of the partial dividend, stop. The final 
partial product is the remainder. Otherwise, continue 
with step 6. 
6. Shift the divisor one place right. Repeat from 
step 2. 
The flowchart corresponding to this algorithm 
is given in Fig. 4.44. Consider the division of 
011001112 by 10012, which corresponds to 103 
divided by 9 and should yield a quotient 11 and a 
remainder 4. Figure 4.45 illustrates the division 
process, step by step. 
Non-restoring division 
It's possible to modify the restoring division algo-
rithm of Fig. 4.44 to achieve a reduction in the 
time taken to execute the division process. The 
non-restoring division algorithm is almost identi-
cal to the restoring algorithm. The only difference 
is that the so-called restoring operation is elimi-
nated. From the flowchart for restoring division, 
it can be seen that after a partial dividend has 
been restored by adding back the divisor, one-half 
the divisor is subtracted in the next cycle. This is 
because each cycle includes a shift-divisor-right 
operation, which is equivalent to dividing the 
divisor by two. The restore divisor operation in the 
current cycle followed by the subtract half the 
divisor in the following cycle is equivalent to a 
single operation of add half the divisor to the par-
tial dividend. That is, D - D/2 = +D/2, where D 
is the divisor. 
Figure 4.46 gives the flowchart for non-restoring 
division. After the divisor has been subtracted 
from the partial dividend, the new partial divi-
dend is tested. If it is negative, zero is shifted into 
the least-significant position of the quotient and 
half the divisor is added back to the partial divi-
dend. If it is positive, one is shifted into the least-significant 
position of the quotient and half the divisor is subtracted 
from the partial dividend. Figure 4.47 repeats the example of 
Fig. 4.4 using non-restoring division. 
Division by multiplication 
Because both computers and microprocessors perform 
division less frequently than multiplication, some processors 
implement multiplication but not division. It is, however, possi-
ble to perform division by means of multiplication, addition, 
and shifting. 
A u 
B, 
4-bit 
4-bit 
multiplier 
4. 
5. 
Bu 
Ai 

196 
Chapter 4 Computer arithmetic 
Start 
Align most-significant 
bits of divisor and dividend 
Subtract divisor from 
partial dividend 
No / . 
, 
. . ?X Yes 
Is result positive? 
Shift 0 in quotient 
Shift 1 in quotient 
Add divisor to partial 
dividend to restore divisor 
Fig. 4.44 The flowchart for restoring division. 
If we now repeat the process with K = (1 + Z2), we get 
N(l + Z) 
l + Z2 _ M l + Z)(l + Z1) 
1 - Z2 
1 + Z2 
1 - Z 4 
* 
Shift divisor one place right 
/ 
Is \ . 
Yes / ° M » f alignedX^ NO 
~ ^ C 
withLSBof ~^>-^. 
^-^dividend?/'^ 
End 
Suppose we wish to divide a dividend N by a divisor D to 
obtain a quotient Q, so that Q = N/D. The first step is to scale 
D so that it lies in the range 
1/2 < D < 1 
This operation is carried out by shifting D left or right and 
recording the number of shifts—rather like normalization in 
floating point arithmetic. We define a new number, Z, in 
terms of D as Z = 1 — D. Because D lies between 'A and unity, 
it follows that Z lies between zero and H. That is, 0 < Z < 'A. 
An elementary rule of arithmetic states that the value of 
the fraction remains unaltered if the top and bottom of a 
fraction are multiplied by the same number. 
Thus, Q = N/D = KNIKD. Suppose that K = 1 + Z, then 
_N 
_N(l 
+ Z) = 
N(l + Z) 
= N(l + Z) 
Q ~ D ~ D(l + Z) ~ (1 - Z)(l + Z) 
l - Z2 
This process may be repeated « times with the result that 
N 
N(l +Z)(1 +Z 2)(1 +Z 4)---(1 +Z2"") 
Because Z is less than unity, the value of Z2 
rapidr 
approaches zero as n is increased. Consequendy, the approxi 
mate value of Q is given by 
Q = N(l + Z)(l + X2)(l + Z4) • • • (1 + Z2 ) 
For 8-bit precision n need be only 3, and if « = 5 the 
quotient yields a precision of 32 bits. As the divisor was scaled 
to lie between H and unity, the corresponding quotient, Q, 
Q 
Q = J 
. N 
= 
D 
Q 

4.10 Multiplication and division 
197 
Step 
Description 
Partial dividend 
Divisor 
Quotient 
1 
2 
4 
5 
6 
2 
3 
5 
6 
2 
4 
5 
6 
2 
4 
5 
01100111 
00001001 
00000000 
Align 
01100111 
01001000 
00000000 
Subtract divisor from partial dividend 
00011111 
01001000 
00000000 
Result positive—shift in 1 in quotient 
00011111 
01001000 
00000001 
Test for end 
Shift divisor one place right 
00011111 
00100100 
00000001 
Subtract divisor from partial dividend 
-00000101 
00100100 
00000001 
Restore divisor, shift in 0 in quotient 
00011111 
00100100 
00000010 
Test for end 
Shift divisor one place right 
00011111 
00010010 
00000010 
Subtract divisor from partial dividend 
00001101 
00010010 
00000010 
Result positive—shift in 1 in quotient 
00001101 
00010010 
00000101 
Test for end 
Shift divisor one place right 
00001101 
00001001 
00000101 
Subtract divisor from partial dividend 
00000100 
00001001 
00000101 
Result positive—shift in 1 in quotient 
00000100 
00001001 
00001011 
Test for end 
Figure 4.45 Example of restoring division for 1001)01100111. 
Start 
Align most-significant bits 
of divisor and dividend 
Subtract divisor from 
dividend 
Shift divisor one place right 
Positive 
Negative 
Shift 1 in LSB of quotient. 
Subtract divisor from 
partial dividend. 
Shift 0 in LSB of quotient 
Add divisor to 
partial dividend. 
Restore final divisor to 
get remainder 
End 
Figure 4.46 Flowchart for non-restoring division. 
Test partial 
v^ dividend 
/ S 
IS ^ v 
Yes ^/divisor aligned^^ No 
^ _ < \ 
withLSBof 
>—-
^^dividend?/''^ 

198 
Chapter 4 Computer arithmetic 
Step 
Description 
Partial dividend 
Divisor 
Quotient 
01100111 
00001001 
00000000 
1 
Align divisor 
01100111 
01001000 
00000000 
2 
Subtract divisor from partial dividend 
00011111 
01001000 
00000000 
3 
Shift divisor right 
00011111 
00100100 
00000000 
4 
Test partial dividend—enter 1 in quotient 
and subtract divisor from partial dividend 
-00000101 
00100100 
00000001 
6 
Test for end of process 
-00000101 
00100100 
00000001 
3 
Shift divisor right 
-00000101 
00010010 
00000001 
5 
Test partial dividend—enter 0 in quotient 
and add divisor to partial dividend 
00001101 
00010010 
00000010 
6 
Test for end of process 
00001101 
00010010 
00000010 
3 
Shift divisor right 
00001101 
00001001 
00000010 
4 
Test partial dividend enter—1 in quotient 
and subtract divisor from partial dividend 
00000100 
00001001 
00000101 
6 
Test for end of process 
00000100 
00001001 
00000101 
3 
Shift divisor right 
00000100 
0000100.1 
00000101 
4 
Test partial dividend—enter 1 in quotient 
and subtract divisor from partial dividend 
-00000000.1 
0000100.1 
00001011 
6 
Test for end of process 
-00000000.1 
0000100.1 
00001011 
7 
Restore last divisor 
00000100 
0000100.1 
00001011 
Fig. 4.47 An example of non-restoring division for 1001)0110011. 
calculated from the above formula must be scaled by the 
same factor to produce the desired result. 
J SUMMARY 
In this chapter we have looked at how numerical information is 
represented inside a digital computer. We have concentrated on 
the binary representation of numbers, because digital 
computers handle binary information efficiently. Because both 
positive and negative numbers must be stored and manipulated 
by a computer, we have looked at some of the ways in which 
digital computers represent negative numbers. The two's 
complement system is used to represent negative integers, 
whereas a biased representation is used to represent negative 
exponents in floating point arithmetic and a floating point 
mantissa uses a sign and magnitude representation. 
Because digital computers sometimes have to work with 
very large and very small numbers, we have covered some of 
the ways in which the so-called scientific notation is used 
to encode both large and small numbers. These numbers 
are stored in the form of a mantissa and a magnitude 
(i.e. the number of zeros before/after the binary point) and are 
called floating point numbers. Until recently, almost every 
computer used its own representation of floating point numbers. 
Today, the IEEE standard for the format of floating point numbers 
has replaced most of these ad hoc floating point formats. 
At the end of this chapter we have briefly introduced the 
operations of multiplication and division and have 
demonstrated how they are mechanized in digital computers. 
Special hardware has to be used to implement signed 
multiplication because the two's complement system cannot be 
used for signs and unsigned multiplication. 
• 
PROBLEMS 
4.1 Convert the following decimal integers to their natural 
binary equivalents. 
(a) 12 
(d) 4090 
(b) 42 
(e) 40900 
(c) 255 
(f) 65530 
4.2 Convert the following natural binary integers to their 
decimal equivalents. 
(a) 110 
(c) 110111 
(b) 1110110 
(d) 11111110111 
4.3 Complete the table below. 
Decimal 
Binary 
Hexadecimal 
Base 7 
37 
99 
10101010 
11011011101 
256 
CAB 
12 
666 
4.4 Convert the following base 5 numbers into base 9 
equivalents. For example, 235 = 149. 
(a) 14 
(b) 144 (c) 444 
(d) 431 

4.10 Multiplication and division 
199 
4.5 Convert the following decimal numbers to their binary 
equivalents. Calculate the answer to five binary places and 
round the result up or down as necessary. 
(a) 1.5 
(d) 1024.0625 
(b) 1.1 
(e) 3.141592 
(c) 1/3 
(f) 1/V2 
4.6 Convert the following binary numbers to their decimal 
equivalents. 
(a) 1.1 
(d) 11011.111010 
(b) 0.0001 
(e) 111.111111 
(c) 111.101 
(f) 10.1111101 
4.7 Complete the following table. Calculate all values to four 
places after the radix point. 
Decimal 
Binary 
Hexadecimal 
Base 7 
0.25 
0.35 
11011.0111 
111.1011 
2.08 
AB.C 
1.2 
66.6 
4.8 Calculate the error (both absolute and as a percentage) if 
the following decimal fractions are converted to binary 
fractions, correct to 5 binary places. Convert the decimal 
number to six binary digits and then round up the fifth bit if the 
sixth bit is a 1. 
(a) 0.675 
(b) 0.42 
(c) 0.1975 
4.9 An electronics engineer has invented a new logic device 
that has three states: - 1 , 0 , and +1. These states are 
represented by 1, 0, and 1, respectively. This arrangement may 
be used to form a balanced ternary system with a radix 3, but 
where the trits represent -1,0, +1 instead of 0,1,2. The 
following examples illustrate how this system works. 
Ternary 
11 
Balanced ternary 
11 
Decimal 
4(3+1) 
12 
111 
5 ( 9 - 3 - 1 ) 
22 
101 
8 ( 9 - 1 ) 
1012 
11TT 
32 (27 + 9 - 3 --1) 
Write down the first 15 decimal numbers in the balanced 
ternary base. 
4.10 The results of an experiment fall in the range 
- 4 to +9. A scientist reads the results into a computer and 
then processes them. The scientist decides to use a 4-bit 
binary code to represent each of the possible inputs. Devise a 
4-bit code capable of representing numbers in the range 
- 4 to+9. 
4.11 Convert the following decimal numbers into BCD form, 
(a) 1237 
(b) 4632 
(c) -9417 
4.12 Perform the following additions on the BCD numbers 
using BCD arithmetic. 
(a) 0010100010010001 
(b) 1001100101111000 
0110100001100100 
1001100110000010 
4.13 The 16-bit hexadecimal value C123l6 can represent many 
things. What does this number represent, assuming that it is the 
following: 
(a) an unsigned binary integer 
(b) a signed two's complement binary integer 
(c) a sign and magnitude binary integer 
(d) an unsigned binary fraction 
4.14 Convert the following 8-bit natural binary values into their 
Cray code equivalents. 
(a) 10101010 
(b) 11110000 
(c) 00111011 
4.15 Convert the following 8-bit Gray code values into their 
natural binary equivalents. 
(a) 01010000 
(b) 11110101 
(c) 01001010 
4.16 What are the Hamming distances between the following 
pairs of binary values? 
(a) 00101111 
(b) 11100111 
01011101 
01110101 
(c) 01010011 
(d) 11111111 
00011011 
00000111 
(e) 11011101 
(f) 0011111 
11011110 
0000110 
4.17 Decode the Huffman code below, assuming that the valid 
codes are P = 0, Q = 10, R = 110, and S = 111. How many bits 
would be required if P, Q, R, and 5 had been encoded as 00,01, 
10, and 11, respectively? 
000001110111000000101111111101010001111100010 
4.18 The hexadecimal dump from part of a microcomputer's 
memory is as follows. 
0000 
4265 6769 6EFA 47FE BB87 0086 3253 7A29 
0010 
698F E000 
The dump is made up of a series of strings of characters, each 
string being composed of nine groups of four hexadecimal 
characters. The first four characters in each string provide the 
starting address of the following 16 bytes. For example, the first 
byte in the second string (i.e. $C9) is at address $0010 and the 
second byte (i.e. $8F) is at address $0011. 
The 20 bytes of data in the two strings represent the 
following sequence of items (starting at location 0000): 
(a) five consecutive ASCII-encoded characters 
(b) one unsigned 16-bit integer 
(c) one two's complement 16-bit integer 

200 
Chapter 4 Computer arithmetic 
(d) one unsigned 16-bit fraction 
(e) one six-digit natural BCD integer 
(f) one 16-bit unsigned fixed-point number with a 12-bit 
integer part and a 4-bit fraction 
(g) One 4-byte floating point number with a sign bit and true 
fraction plus an exponent biased by 64 
s 
E + 64 
Mantissa 
> 
• < 
8 bits 
24 bits 
Decode the hexadecimal data, assuming that it is interpreted as 
above. 
4.19 A message can be coded to protect it from unauthorized 
readers by EORing it with a binary sequence of the same length 
to produce an encoded message. The encoded message is 
decoded by EORing it with the same sequence that was used to 
decode it. If the ASCII-encoded message used to generate the 
code is ALANCLEMENTS, what does the following encoded 
message (expressed in hexadecimal form) mean? 
09 09 OD 02 OC 6C 12 02 17 02 10 73 
4.20 A single-bit error-detecting code appends a parity bit to a 
source word to produce a code word. An even parity bit is 
chosen to make the total number of ones in the code word even 
(this includes the parity bit itself). For example the source words 
0110111 and 1100110 would be coded as 01101111 and 
1100110, respectively. In these cases the parity bit has been 
located in the LSB position. Indicate which of the following 
hexadecimal numbers have parity errors. 
$00, $07, $FF, $A5, $5A, $71, $FE. 
4.21 A checksum digit is the least-significant digit formed 
when a series of numbers are added together. For example, the 
decimal checksum of the sequence 98731 is 8 because 
9 + 8 + 7 + 3 + 1 = 28 and 8 is the least-significant digit. 
Similarly, the checksum of the hexadecimal sequence A3,02,49, 
FF is ED because A3 + 02 + 49 + FF = 1ED. 
The purpose of a checksum is to detect errors in a sequence 
of digits after they have been transmitted or stored in memory 
or on tape. The following hexadecimal sequences are terminated 
by a checksum. Which, if any, are in error? 
(a) 0001020304050F 
(b) 11223344556675 
(c) FFA32415751464 
The position of the checksum in the above three strings is the 
right-most byte. Does it matter where the checksum is located? 
What happens if there is an error in the checksum itself? 
4.22 What is the meaning of Hamming distance? 
4.23 The H74 Hamming code is written l413 \z C31, C2 C„ where 
I, = source bit /, and C, = check bit /.The three Hamming check 
bits are calculated from the parity equations. 
C3 = I20l3®l4 
C2 = 1,013014 
Q = I,©I2®I4 
Show that no two valid code words differ by less than 3 bits. 
Demonstrate that an error in any single bit can be used to locate 
the position of the error and, therefore, to correct it. 
4.24 Examine the following H74 Hamming code words and 
determine whether the word is a valid code word. If it isn't valid, 
what should the correct code word have been (assuming that 
only 1 error is present)? 
(a) 0000000 
(b) 1100101 
(c) 0010111 
4.25 Convert the following image into a quadtree. 
F = full (all elements'!) 
E = empty (all elementsO) 
P = partially filled 
2 
3 
0 
1 
Quadrant 
numbering 
4.26 Convert the following image into a quadtree. 
4.27 Almost all computer hardware courses include a section 
on number bases and the conversion of numbers between bases. 
Does the base in which a computer represents numbers really 
matter to the computer user or even to the student of 
computer science? 
4.28 Perform the following binary additions: 
(a) 
10110 
(b) 
+ 101 
100111 
(c) 
11011011 
111001 
10111011 
+101101 
00101011 
+01111111 
4.29 Perform the following octal additions. We have not 
covered octal arithmetic (base 8). You must determine the rules 
of addition, 
(a) 
42 
(b) 
3357 
(c) 
777 
(d) 
+ 53 
+2741 
543 
+420 
437 
426 
772 
+ 747 
4.30 Perform the following hexadecimal additions: 
(a) 
42 
(b) 
3357 
(c) 
777 
(d) 
ABCD 
+ 53 
+2741 
543 
+420 
FE10 
+ 123A 

4.10 Multiplication and division 
201 
4 31 Using 8-bit arithmetic throughout, express the following 
decimal numbers in two's complement binary form: 
(a) - 4 
(d) - 2 5 
(g) -127 
(b) - 5 
(e) - 4 2 
(h) -111 
(c) 0 
(f) -128 
4.32 Perform the following decimal subtractions in 8-bit two's 
complement arithmetic. Note that some of the answers will 
result in arithmetic overflow. Indicate where overflow has 
occurred. 
(a) 
20 
(b) 
127 
(c) 
127 
(d) 
5 
^ 5 
-126 
-128 
^ 2 0 
(e) 69 
(0 -20 
(g) -127 
(h) 
-42 
-42 
-111 
= 1 
+69 
+ 120 
4.33 Using two's complement binary arithmetic with a 12-bit 
word, write down the range of numbers capable of being 
represented (both in decimal and binary formats) by giving the 
smallest and largest numbers. What happens when the smallest 
and largest numbers are 
(a) incremented? 
(b) decremented? 
4.34 Distinguish between overflow and carry when these terms 
are applied to two's complement arithmetic on n-bit words. 
4.35 Write down an algebraic expression giving the value of the 
n-bit integer N = an_v a„_2,... ,av a0 for the case where N 
represents a two's complement number. 
Hence prove that (in two's complement notation) the 
representation of a signed binary number in n+ 1 bits may be 
derived from its representation in n bits by repeating the 
leftmost bit. For example, if n = - 1 2 = 10100 in 5 bits, 
n= - 1 2 = 110100 in 6 bits. 
4.36 Perform the additions below on 4-bit binary numbers. 
(a) 
0011 
+ 1100 
(b) 
1111 
+0001 
(c) 
0110 
+0111 
(d) 
1100 
+ 1010 
In each case, regard the numbers as being (i) unsigned integer, 
(ii) two's complement integer, and (iii) sign and magnitude 
integer. Calculate the answer and comment on it where 
necessary. 
4.37 Add together the following pairs of numbers. Each number 
is represented in a 6-bit sign-and-magnitude format. Your 
answer should also be in sign-and-magnitude format. Convert 
each pair of numbers (and result) into decimal form in order to 
check your answer. 
(a) 
000111 
010101 
(c) 
010101 
000111 
(e) 
110111 
110111 
(b) 
100111 
OlpJOl 
(d) 
111111 
000001 
(f) 
011111 
000110 
4.38 Write down the largest base 5 positive integer in n digits 
and the largest base 7 number in m digits. It is necessary to 
represent n-digit base 5 numbers in base 7. What is the 
minimum number m of digits needed to represent all possible 
n-digit base 5 numbers? Hint—the largest m-digit base-7 
number should be greater than, or equal to, the largest n-digit 
base 5 number. 
4.39 A 4-bit binary adder adds together two 4-bit numbers, A 
and 6, to produce a 4-bit sum, S, and a single-bit carry-out C. 
What is the range of outputs (i.e. largest and smallest values) 
that the adder is capable of producing? Give your answer in 
both binary and decimal forms. 
An adder is designed to add together two binary coded 
decimal (BCD) digits to produce a single digit sum and a 1-bit 
carry-out. What is the range of valid outputs that this circuit 
may produce? 
The designer of the BCD adder decides to use a pure binary 
adder to add together two BCD digits as if they were pure 4-bit 
binary numbers. Under what circumstances does the binary 
adder give the correct BCD result? Under what circumstances is 
the result incorrect (i.e. the 4-bit binary result differs from the 
required BCD result)? 
What algorithm must the designer apply to the 4-bit output 
of the binary adder to convert it to a BCD adder? 
4.40 Design a 4-bit parallel adder to add together two 4-bit 
natural binary-encoded integers. Assume that the propagation 
delay of a signal through a gate is r ns. For your adder, calculate 
the time taken to add 
(a) 0000 to 0001 
(b) 0001 to 0001 
(c) 0001 to 1111 
4.41 Design a full subtractor circuit that will subtract bit X 
together with a borrow-in bit 6, from bit Yto produce a 
difference bit D = Y— X— B„ and a borrow-out B0. 
4.42 In the negabinary system an /-bit binary integer, N, is 
expressed using positional notation as 
N = fl0 X -1° X 2° + fl! X - 1 ' X 21 + 
a,_! X -V'1 
X 2'"1 
+ 
This is the same as conventional natural 8421 binary 
weighted numbers, except that alternate positions have the 
additional weighting +1 and — 1 . 
For example, 1101 = (-1 X 1 X 8) + (+1 X 1 X 4) + 
(-1 X OX 2) + (+1 x 1 x 1) = - 8 + 4 + 1 = - 3 
The following 4-bit numbers are represented in 
negabinary form. Convert them into their decimal 
equivalents. 
(a) 0000 
(b) 0101 
(c) 1010 
(d) 1111 

202 
Chapter 4 Computer arithmetic 
4.43 Perform the following additions on 4-bit negabinary 
numbers. The result is a 6-bit negabinary value. You must 
construct your own algorithm. 
(a) 0000 
(b) 1010 
(c) 1101 
(d) 1111 
+0001 
+0101 
1011 
1111 
4.44 Convert the following signed decimal numbers into their 
6-bit negabinary counterparts. 
(a) 4 
(b) - 7 
(c) +7 
(d) 10 
4.45 What is the range of values that can be expressed as an 
n-bit negabinary value? That is, what is the largest positive 
decimal number and what is the largest negative decimal 
number that can be converted into an n-bit negabinary form? 
4.46 A computer has a 24-bit word length, which, for the 
purpose of floating point operations, is divided into an 8-bit 
biased exponent and a 16-bit two's complement mantissa. 
Write down the range of numbers capable of being represented 
in this format and their precision. 
4.47 Explain the meaning of the following terms (in the context 
of floating point arithmetic): 
(a) biased exponent 
(b) fractional part 
(c) packed 
(d) unpacked 
(e) range 
(f) precision 
(g) normalization 
(h) exponent overflow/underflow 
4.48 An IEEE standard 32-bit floating point number has the 
formatN= - 1 S X 1.FX 2E~127, where Sis the sign bit, Fis the 
fractional mantissa, and E the biased exponent. 
(a) (i) Convert the decimal number 1000.708 into the IEEE 
format for floating point numbers. 
(ii) Convert the decimal number 100.125 into the IEEE 
format for floating point numbers. 
(b) Describe the steps that take place when two IEEE floating 
point numbers are added together. You should start with the 
two packed floating point numbers and end with the packed 
sum. 
(c) Perform the subtraction of 1000.708 - 100.25 using the 
two IEEE-format binary floating point numbers you 
obtained for 1000.708 and 100.25 in part (a) of this 
question. You should begin the calculation with the packed 
floating-point representations of these numbers and end 
with the packed result. 
4.49 Convert the 32-bit IEEE format number C33BD00016 into 
its decimal representation. 
4.50 Explain why floating point numbers have normalized 
mantissas. 
4.51 What is the difference between a truncation error and a 
rounding error? 
4.52 The following numbers are to be represented by three 
significant digits in the base stated. In each case perform the 
operation by both truncation and rounding and state the 
relative error created by the operation. 
(a) 0.11001002 
(b) 0.1A3416 
(c) b. 0.00110112 
(d) d.0.12AAl6 
4.53 We can perform division by multiplication to calculate 
Q = N/D.Jbe iterative expression for Q is given by 
Q = N{\ + Z)(l + Z2)(l + ZV-(1 + Zr ') 
where Z= 1 — D. 
If N = 5010 and D = 0.7410, calculate the value of Q. Evaluate 
Q using 1,2,3, and 4 terms in the expression. 
4.54 For each of the following calculations (using 4-bit 
arithmetic) calculate the value of the 1 (zero), C (carry), N 
(negative), and V (overflow) flag bits at the end of the operation. 
(a) 1010- 1010 
(b) 1111 -0001 
(c) 1111 +0001 
(d) 0110 + 0110 
(e) 1010- 1110 
(f) 1110+ 1010 

The instruction set architecture 
INTRODUCTION 
There are two ways of introducing the processor. One is to explain how a computer works at the 
level of its internal information flow by describing the way in which information is transmitted 
between registers and internal units and showing how an instruction is decoded and interpreted 
(i.e. executed).The other approach is to introduce the native language, or machine code, of a 
computer and show what computer instructions can do. In practice no-one writes programs in 
machine code; instead they use assembly language which is a human-readable representation of 
machine code (see the box 'The assembler'). 
Both approaches to the description of a computer are valid. Beginning with how a computer 
works by examining its internal operation is intuitive. Once you understand how information flows 
from place to place through adders and subtractors, you can see how instructions 
are constructed and then you can examine how sequences of instructions implement 
programs. 
Unfortunately, beginning with the hardware and looking at very primitive operations hides the 
big picture. You don't immediately see where you are going or understand why we need the 
primitive operations in the first place. This bottom-up approach is rather like studying cellular 
biochemistry as the first step in a course on sociology. Knowing how a brain cell works doesn't tell 
you anything about human personality. 
5 The instruction set 
architecture 
!'i I'VS chapu-r v.c iritiodt.;* thi! 
computer's instrut t:on id 
iJ.-c.,:/tcrtu.n'(ISA'n,vhvli 
eestiibestlielow ipvui 
programmer's vis1/.- of :he 
compute.- rne ISA deiini-i tiir-
type- of operation* d computor 
cernos ,-iu! V,'e E'I- .-itere.'ir'd m 
thrre .-ispeits or the ibA-lhc 
nature of the ns::iic::ons *.*M 
leMnirceiiisftlby the 
•ns'.rutt'ons (•easier;, and 
memory), .mcl the ways in ivnicri 
t"ie instructions access dald 
(adfrcssinf -i-oae.j.Trie 68K 
mi:ropiowssor is used to 
ilijaiMtr iii«r operi-.t'ijn t:! a --*3l 
6 Assembly language 
programming 
Having introduced the basic 
operations that a computer can 
carry, the next step is to show 
how they are used to construct 
entire programs.We look at how 
the 6BK processor uses machine-
tevel instructions to implement 
some simple algorithms. 
CHAPTER MAP 
2 Logic elements and 
i Boolean algebra 
The basic building blocks, gates, . 
f: from which we construct the 
I computer. 
3 Sequential logic 
'.. The building blocks, flip-flops, 
••" used to construct devices that • 
"store data and counters. 
4 Computer arithmetic 
The representation of numbers in. 
a computer and the arithmetic 
Used by digital computers. 
7 Structure of the CPU 
Having described what a 
computer does, thenextstep is 
i to show howM operates. Here we 
examine the internal organization 
of a computer and demonstrate 
how it reads instructions from 
memory, decodes them, and 
executes them. 
8 Other processors 
We have used the 68K to 
introduce the CPU and assembly 
language programming. Here we 
provide a brief overview of two 
other processors: a jimpie 8-bit 
microcontroller and a 32-bit RISC 
processor. 

204 
Chapter 5 The instruction set architecture 
Beginning a course with the computer's instruction set gives you a better idea of what a 
computer does in terms of its capabilities. Once you know what a computer does, you can look 
inside it and explain how it implements its machine code operations. 
In the previous edition of Principles of Computer Hardware I began with the internal 
organization of a computer and explained the steps involved in the execution of an instruction. 
Later we looked at the nature of instructions. In this edition I've reversed the order and we begin 
with the instruction set and leave the internal organization of the computer until later. This 
sequence enables students to take lab classes early in the semester and build up practical 
experience by writing assembly language programs. 
We begin this chapter by introducing the notion of computer architecture, the instruction 
set, and the structure of a computer. We describe a real processor, the Motorola 68K. 
This processor is a contemporary of the Intel 8086 but has a more sophisticated 
architecture and its instruction set is easier for students to understand. This processor 
has evolved like the corresponding Intel family and its variants are now called the 
ColdFire family. 
THE ASSEMBLER 
An assembly language program starts off as a text file written by a programmer (or created by 
a compiler). An assembler takes the text file together with any library functions required by 
the program and generates the binary code that the target executes. 
The addresses of code and data generated by the assembler are not absolute (i.e. actual), but 
refer to the tocations with respect to the start of the program. Another program called a linker 
takes one or more code modules generated by the assembler, puts them together, and creates 
the actual addresses of data in memory. The output of the linker is the binary that can be exe-
cuted by the actual computer.This mechanism allows you to write a program in small chunks 
and to put them together without having to worry about addresses in the different chunks. 
Assembly language 
program 
Assembler 
Linker 
Source code 
MOVE.B #4,DO 
MOVE.B #9,D1 
ADD.B 
D0.D1 
MULD 
#5,D1 
The assembler 
translates assembly 
language into binary 
code (machine code). 
The linker takes one 
or more separately 
assembled modules 
and puts them together 
to generate the final 
binary code. 
Source code 
MOVE.B #4,DO 
MOVE.B #9,D1 
ADD.B 
D0.D1 
MULD 
#5,D1 
The assembler 
translates assembly 
language into binary 
code (machine code). 
The linker takes one 
or more separately 
assembled modules 
and puts them together 
to generate the final 
binary code. 
Library 
i 
The library contains 
commonly used 
functions such as 
I/O routines. 
• Binary 
5.1 What is an instruction set 
architecture? 
An instruction set architecture (ISA) is an abstract model of a 
computer that describes what it does, rather than how it does 
it. You could say that a computer's architecture is its func-
tional definition. The notion of an architecture dates back to 
the 1960s and IBM's 360 series mainframes. Each mainframe 
had the same architecture, but the performance varied from 
model to model. By adopting a common architecture for all 
members of the 360 series, users were able to upgrade a 
model and still use the same software. 
An object can be viewed in more than one way. Consider 
the airline pilot. Passengers see the pilot as an agent responsi-
ble for transferring them from one airport to another. The 

5.1 What is an instruction set architecture? 
205 
pilot's crew sees them as a colleague with whom they relate at 
the personal level. The pilot's doctor sees a complex biologi-
cal mechanism. It's exactly the same with computers—you 
can view them in different ways. 
Suppose you run a spreadsheet on a computer. As far as 
you're concerned, the machine is a spreadsheet machine that 
behaves exactly as if it were an electronic spreadsheet doing 
nothing other than spreadsheet calculations. You could con-
struct an electronic device to directly handle spreadsheets, but 
no one does. Instead they construct a computer and run a 
program to simulate a spreadsheet. 
ARCHITECTURE AND ORGANIZATION 
Architecture describes the functionality of a system, whereas 
organization describes how it achieves that functionality. 
Consider the automobile as a good example of the distinction 
between architecture and organization. The architecture of an 
automobile covers its steering, acceleration, and braking. An 
automobile's gearbox is part of its organization rather than its 
architecture. Why? Because the gearbox is a device that 
facilitates the operation of an automobile—it is there only 
because we can't create engines that drive wheels directly. 
Figure 5.1 illustrates how a computer can be viewed in differ-
ent ways. The outer level is the applications layer that the end 
user sees. This level provides a virtual spreadsheet or any 
other user-application because, to all intents and purposes, 
the machine looks like a spreadsheet machine that does noth-
ing else other than implement spreadsheets. 
A spreadsheet, a word processor, or a game is invariably 
implemented by expressing its behavior in a high-level lan-
guage such as C or Java. You can view a computer as a 
machine that directly executes the instructions of a high-level 
language. In Fig. 5.1 the layer below the application level is 
the high-level language layer. 
It's difficult to construct a computer that executes a high-
level language like C. Computers execute machine code, a 
primitive language consisting of simple operations such as 
addition and subtraction, Boolean operations, and data 
movement. The statements and constructs of a high-level 
language are translated into sequences of machine code 
instructions by a compiler. The machine code layer in Fig. 5.1 
is responsible for executing machine code; it's this layer that 
defines the computer's architecture. 
Figure 5.1 shows two layers between the machine level and 
high-level language levels. The assembly language level sits on 
Word processor 
Database 
Came 
Hardware/software 
nterface 
Figure 5.1 Machine levels and virtual architectures. 
c 
— Java 
LISP 
Windows 
• 
"" 
Unix 
Application 
High-level 
language 
Operating 
system 
-""""Assembly 
"""--
Language 
Machine level 
/^icrcprcgram 
' Digi:al 
Uoglc 

206 
Chapter 5 The instruction set architecture 
top of the machine level and represents the human-readable 
form of the machine code; for example, the binary 
string 00000010100000010001000000000011 might be the 
machine code instruction represented in assembly language 
as MOVE D2, Dl (move the number in register D2 to the 
register Dl).1 
To say that assembly language is just a human-readable 
version of machine code is a little simplistic. An assembly lan-
guage contains facilities that make it easier for a human to 
write a program. Moreover, an assembly language allows you 
to determine where data and code is loaded into memory. We 
will not use sophisticated assembly language mechanisms 
and it is reasonably true to say that assembly language 
instructions are human-readable versions of the strings of Is 
and 0s that represent the machine-code binary instructions. 
The conventions we will adopt in the structure and layout of 
assembly language programs are, normally, those of the 
Motorola assembler. 
In Fig. 5.1 there is an additional layer between the assembly 
language layer and the high-level language layer called the operat-
ing system level. Strictly speaking, this layer isn't like the other lay-
ers. The operating system runs on top of the machine code and 
assembly language layers and provides facilities required by 
higher-level layers (e.g. memory management and the control of 
peripherals such as the display and disk drives). 
Below the machine-level layer is the microprogram layer. A 
heavy line separates the machine level and microprogram 
layers because you can access all the layers above this line. The 
two innermost layers (microprogram and digital logic) are 
not accessible to the programmer. 
The microprogram layer is concerned with the primitive 
operations that take place inside the computer during the 
execution of a machine code operation. For example, a MOVE 
D2, Dl machine-level instruction might be interpreted by 
executing a sequence of micro-operations inside the com-
puter. These micro-operations transfer information between 
functional units such as registers and buses. The sequences of 
micro-operations that interpret each machine level instruc-
tion are stored in firmware within the computer. Firmware is 
the term for read-only memory containing programs or 
other data that controls the processor's operation. Firmware 
cannot normally be modified, although modern systems can 
update their firmware from time to time. 
Some modern computers don't have a microprogram layer. 
If an instruction set is very regular and all instructions involve 
a simple, single-step operation, there is no need for a micro-
program to translate instructions into primitive operations. 
Where mere's a simple relationship between the binary code 
of an instruction and what it does, the microprogram layer 
direcdy translates a machine-level instruction into the control 
signals required to implement the instruction. 
The innermost level of the computer is the digital logic level 
which consists of the gates, flip-flops, and buses. At this level 
the individual logic elements are hardwired to each other by 
fixed connections. You can't program or modify the behavior 
of components at this level. This statement isn't strictly true. 
Programmable logic elements whose functionality can be 
modified do exist; for example, it is possible to reconfigure 
internal connections using the same technology found in 
flash memory. In the future we may incorporate such comp-
onents in processors to enable manufacturers to update a 
processor's instruction set or to fix hardware bugs. 
You could, in fact, go even deeper into the hierarchy of 
Fig. 5.1 because there is a physical layer below the digital logic 
layer. This physical layer is concerned with the individual 
transistors and components of the computer that are used to 
fabricate gates, registers, and buses. Below the physical layer 
exists the individual atoms of the transistors themselves. 
We're not interested in the physical layer and the atomic 
layers, because that's the province of the semiconductor 
engineer and physicist. In this chapter we are concerned with 
the machine-level and microprogram layers. 
5.2 Introduction to the CPU 
Before we look at what a CPU does or how it works, it is 
important to understand the relationship between the CPU, 
the memory, and the program. Let's take a simple program to 
calculate the area of a circle and see how the computer deals 
with it. In what follows the computer is a hypothetical 
machine devoid of all the complications associated with real-
ity. Throughout this section we assume that we are operating 
at the machine level. 
The area of a circle, A, can be calculated from the 
formula A = irr2. When people evaluate the area of a circle, 
they automatically perform many of the calculations at a sub-
conscious level. However, when they come to write programs, 
they must tell the computer exactly what it must do, step by 
step. To illustrate this point, take a look at the expression TIT2. 
We write r2, but we mean a number, which we have given the 
symbol r, multiplied by itself. We never confuse the symbol r 
with the value that we give to r when we evaluate the expres-
sion. This may seem an obvious point, but students some-
times have great difficulty when they encounter the concepts 
of an address and data in assembly language. Although 
people never confuse the symbol for the radius (i.e. r) and its 
value, say 4 cm, you must remember that an address (i.e. the 
place where the value of r is stored) and data (i.e. the value 
of r) are both binary quantities inside the computer. 
1 Throughout this chapter we adopt the convention used by the 68K 
microprocessor diat the rightmost register in an instruction is the desti-
nation operand (i.e. where the result goes). To help you remember this, 
we will use a bold face to indicate the destination operand. 

5.2 Introduction to the CPU 
2 0 7 
Memory 
Program 
getr 
square r 
multiply i2 by TC 
output the result 
read 
A 
Processor 
(CPU) 
j 
Constants 
n 
read 
A 
Processor 
(CPU) 
j 
Constants 
n 
Processor 
(CPU) 
j 
Variables 
A 
r 
Processor 
(CPU) 
j 
INPUT 
Processor 
(CPU) 
j 
Figure 5.2 The relationship 
between the memory, 
processor, and program. 
External 
INPUT 
Processor 
(CPU) 
j 
Figure 5.2 The relationship 
between the memory, 
processor, and program. 
system <s 
OUTPUT 
Processor 
(CPU) 
j 
Figure 5.2 The relationship 
between the memory, 
processor, and program. 
Figure 5.2 The relationship 
between the memory, 
processor, and program. 
Figure 5.2 illustrates the relationship between the 
program, memory, and processor. The memory has been 
divided into five parts: program, constants, variables, input, 
and output. The program is composed of the sequence of 
operations to be carried out, or executed. The constants 
(in this case there is only one—TT) are numbers used by 
the program but which do not change during its execution. 
The variables represent numbers created and modified by the 
program. When the program squares r, it reads the value of 
the number in the memory location it has called r, squares it, 
and puts the result back in the same location. Thus the 
original value of r is lost. If the programmer wishes to retain 
the original value of r, rather than by overwriting it with r2, 
memory locations must be reserved for both r and r2. 
Although the variables (i.e. the values held in memory 
locations) are often numerical quantities, there is no reason 
why this must always be so. For example, the variables used by 
a word processor are the letters and symbols of the text being 
manipulated. Indeed, it is perfectly possible for the variable to 
be another program. That is, one program can operate on, or 
modify, another program. 
A program must be able to communicate with the outside 
world, otherwise all its efforts are to no effect. We have labeled 
two memory locations in Fig. 5.2 input and output. Reading 
from the input location causes information to be taken from 
an input device (say a keyboard) and writing to the output 
location causes information to be moved from the computer 
to an output device (say a display). Treating input and output 
as memory locations is not entirely fictional—we'll later 
discover that many computers really do perform all 
input/output transactions via the memory by means of a 
mechanism called memory-mapped I/O. 
The processor may either read data from a memory loca-
tion or write data to a memory location. Of the five regions of 
memory described above, three are read-only, one is write-
only, and one can be read from or written to. 
5.2.1 The memory and registers 
We now introduce two important concepts. The first is the 
notion of a memory that holds programs and data. The 
second concept is the algebraic notation we use to define 
operations taking place within a computer. 
Figure 5.3 illustrates the so-called random access memory 
system (i.e. RAM). This memory appears as a block of 
sequential locations, each of which contains a data element. 
Each location has a unique address that defines the location 
of the data; for example, in Fig. 5.3 we can say that location 
number 5 contains the value 7. 
The memory in Fig. 5.3 is interfaced to the rest of the 
computer via three buses (i.e. information paths). The 
address bus is a one-way path from the computer to the 
memory, which specifies the location of the memory element 
being accessed. The data bus is a bidirectional (i.e. two-way) 
data path along which data flows into memory when it is 
stored and out of memory when it is retrieved. The control 
bus includes signals that control the operation of the 
memory (e.g. read data or write data commands). 
Registers 
A register is a storage device that holds a word exactly like a 
memory location. In principle, there's no difference between 
a location in memory and a register because they both do 
the same thing. The real difference is one of accessibility. 
Registers are located within the CPU and can be accessed 
faster than memory locations. Moreover, there are few regis-
ters in a computer and millions of address locations, which 
means that you need far fewer bits to specify a register than a 
memory location. For example, the 68K has eight data regis-
ters and an instruction requires 3 bits to specify which one of 
the eight registers is to be used by an instruction. If you spec-
ify a memory location, you need 32 bits to select one out of 
232 possible locations. 

2 0 8 
Chapter 5 The instruction set architecture 
Processor 
Address 
Data 
Control 
signals 
Address port 
Address bus 
Data port 
Data bus^ 
Control bus 
The control bus 
determines the 
direction of information 
transfer 
Memory system 
0 
7 
1 
3 
2 
15 
3 
2 CL-^-
4-* - " ~ ~ 3 « 
5 
7 
6 
3 
7 
8 
8 
8 
9 
42 
10 
12 
11 
19 
Address 
(location 4) 
Memory cell 
containing the 
value 3 
Figure 5.3 The random 
access memory system. 
Registers are not a necessary component of computers— 
we could use memory to store all variables and temporary 
data. Registers exist because they are required to construct 
practical computers. We could not build cheap, fast, and 
effective computers without registers. 
Registers are used as temporary storage places to hold fre-
quently used data. Much of a computer's activity is of the 
form copy data from a memory location to a register, operate on 
the data in the register, send the data back to memory. This 
sequence involves a lot of moving data from one place to 
another place. 
The size of a register (its width in bits) is normally the same 
size as memory locations and the size of the arithmetic and 
logical operations in the CPU. If you have a computer with 
32-bit words, they are held in 32-bit memory locations and 
32-bit registers and are processed by 32-bit adders, and so on. 
Some registers are said to be visible and some are invisible 
to the programmer. In this chapter we are interested only in 
programmer-visible registers. A register is visible if it can be 
directly accessed by the programmer though a computer 
operation. A register is invisible if it is required for internal 
operations but cannot be directly accessed by the program-
mer. The 68K's visible register set consists of 
• eight 32-bit data registers (DO to D7) 
• eight 32-bit address registers (pointers) (A0 to A7) 
• a 16-bit status register (SR) 
• a 32-bit program counter (PC). 
The address and data registers are used by the programmer 
to hold temporary data. The status register defines the 
processor's current operating mode and keeps track of things 
like the carry-out when you do addition. The program 
counter contains the location of the next instruction to be 
executed and, therefore, keeps track of where the computer is 
up to in a program. 
5.2.2 Register transfer language 
Throughout this book we adopt shorthand called register 
transfer language (RTL) to help us to explain how the CPU 
operates. RTL is an algebraic notation that describes how 
information is accessed from memories and registers and 
how it is operated on. You should appreciate that RTL is just a 
notation and not a programming language. 
RTL uses variables like algebra or computer languages; for 
example, one or more letters (or letters followed by numbers) 
to denote registers or storage locations. 
It's very important to distinguish between a memory loca-
tion and its contents. RTL uses square brackets to indicate the 
contents of a memory location; for example, the expression 
[ 6 ] 
= 
3 
is interpreted as the contents of memory location 6 contains the 
value 3. If we were using symbolic names, we might write 
[Time] = HoursWorked 
When dealing with registers, we use their name rather than 
an address; for example, 
[D4] = PQR 
A left or backward arrow (<—) indicates the transfer of data. 
The left-hand side of an expression denotes the destination of 
the data defined by the source of the data defined on the right-
hand side of the expression. For example, the expression 
[MAR] 
<- 
[PC] 
indicates that the contents of the program counter (PC) are 
transferred (i.e. copied) into the memory address register 
(MAR). The program counter is the register that holds the 
location (i.e. address) of the next instruction to be executed. 
The MAR is a register that holds the address of the next item 
to be read from memory or written to memory. Note that the 
contents of the PC are not modified by this operation. 

5.2 Introduction to the CPU 
209 
The operation 
[3] <- 
[5] 
means copy the contents of memory location 5 to location 3. 
In previous editions of this book we used the notation 
[M (5) ] to indicate the contents of memory location 5. We 
have simplified the notation because the meaning of the 
notation [ 5 ] is clear and it's easier to read than [M (5) ]. 
If we were writing a program, memory locations 3 and 5 
would have been given symbolic names, say, x and y, respec-
tively. A symbolic name is the name given to a number by the 
programmer—people like to deal with meaningful names 
rather than, say, the actual numeric addresses of data in 
memory. The operation [3] <- [5] tells us what's happening 
at the micro level—at the high level this operation might be 
written in the rather more familiar form 
x = y; 
Consider the RTL expression 
[PC] 
<- 
[PC] 
+ 4 
which indicates that the number in the PC is increased by 4; 
that is, the contents of the program counter are read, 4 is 
added, and the result is copied into die PC. 
Suppose the computer executes an operation that stores 
the contents of the PC in location 2000 in the memory. We 
can represent this action in RTL as 
[ 2 0 0 0 ] 
[PC] 
Occasionally, we wish to refer to the individual bits of a 
register or memory location. We will do this by means of the 
subscript notation (p:q) to mean bits p to q inclusive; for 
example if we wish to indicate that bits 0 to 7 of a 32-bit reg-
ister are set to zero, we write2 
[ R 6 ( 0 : 7 ) ] 
<- 
0 
Numbers are assumed to be decimal, unless indicated oth-
erwise. Computer languages adopt conventions such as 
0xl2AC or $12AC to indicate hexadecimal values. In RTL we 
will use a subscript; that is, 12AC16. 
As a final example of RTL notation, consider the following 
RTL expressions. 
(a) [20] = 6 
(b) [20] <- 6 
(c) [20] <r- [6] 
(d) [20] <- [6] + 3 
The first example states that memory location 20 contains 
the value 6. The second example states that the number 6 is 
copied or loaded into memory location 20. The third example 
indicates that die contents of memory location 6 are copied 
into memory location 20. The last example reads the contents 
of location 6, adds 3 to it, and stores the result in location 20. 
The RTL symbol '<-' is equivalent to the assignment symbol 
in high-level languages. Remember that RTL is not a 
computer language; it is a notation used to define computer 
operations. 
Later in this chapter we use the 68K's processor's assembly 
language. The typographic conventions in an assembly 
language differ from those of RTL. We use RTL to define the 
meaning of assembly language instructions. Consider the 
following examples. 
Processor 
Instruction 
RTL definition 
family 
mnemonic 
1. 68K 
MOVE DO, (A5) 
[A5] <- [DO] 
2. ARM 
ADD R1,R2,R3 
[Rl] <- [R2] + [R3] 
3. IA32 
MOV ah, 6 
[ah] <- 6 
3. PowerPC 
l i 
r25, 10 
[r25] <r- 10 
5.2.3 Structure of the CPU 
Figure 5.4 provides a more detailed view of the central 
processing unit and memory system. The same memory 
system stores both the program and the data acted on or 
created by the program. It isn't necessary to store both the 
program and data in the same memory. However, for largely 
economic reasons, most computers do store programs and 
data in a single memory system. Such computers are called 
von Neumann machines in honor of John von Neumann. 
A computer is a black box that moves information from 
one point to another and processes the information as it goes 
along. By information we mean the data and the instructions 
held inside the computer. Figure 5.4 shows two information-
carrying paths between the CPU and its memory. The lower 
path (dark blue) with the single arrowhead from the memory 
to the CPU indicates the route taken by the computer's pro-
gram. The CPU reads the sequence of commands that make 
up a program one by one from its memory. 
The upper path (light blue in Fig. 5.4) with arrows at 
both its ends transfers data between the CPU and memory. The 
program being executed controls the flow of information along 
the data path. This data path is bidirectional, because data can 
Input 
Output 
C 
Central 
processing 
unit 
(CPU) 
A -
V -
-N 
A -
Memory 
Data 
Pic-j-r;, 
Figure 5.4 The general-purpose digital computer. 
2 In the previous editions of this book, we used the notation 
[R3 (0:7) ] to indicate bits 0 to 7 of R3. However, by using the 
subscript [ R3 ,0.7, ] we make it easier to indicate a register's subsection. 

210 
Chapters The instruction set architecture 
RISC AND CISC PROCESSORS 
Processors fall into two classes: CISC (complex instruction set 
computer) and RISC (reduced instruction set computer). 
Intel's Pentium and Motorola's 68K families are CISC 
processors with large, irregular instruction sets. CSIC 
processors can perform operations directly on the contents of 
memory locations, whereas RISC processors perform 
operations only on the contents of registers. 
The acronym RISC is misleading. Before the advent of RISC 
technology, processors had complex and elaborate instruction 
sets with instruction lengths varying from 16 bits to 80 bits. 
These processors are retrospectively called CISCs. RISC instruc-
tions are the same length with few variations in their formats. 
Until the mid-1970s the notation of a RISC processor didn't 
exist. IBM investigated ways of accelerating computer perfor-
mance in the 1970s and the computers incorporating IBM's 
flow in two directions. During a write cycle data generated by the 
program flows from the CPU to the memory where it is stored 
for later use. During a read cycle the CPU requests the retrieval 
of a data item that is transferred from the memory to the CPU. 
Suppose the instruction ADD x, Y, z corresponding to the 
operation X = Y + Z is stored in memory.3 The CPU must 
first fetch this instruction from memory and bring it to the 
CPU. Once the CPU has analyzed or decoded the instruction, 
the CPU has to get the values of X and Y from memory. The 
actual values of X and Y are read from the memory and sent to 
the CPU. The CPU adds these values and sends the result, Z, 
back to memory for storage. Remember that x, Y, and z are 
symbolic names for the locations of data in memory. 
Few computers are constructed with two independent 
information paths between the CPU and its memory as 
Fig. 5.4 suggests. Most computers have only one path along 
which information flows between the CPU and its memory— 
data and instructions have to take turns flowing along this 
path. Two paths are shown in Fig. 5.4 simply to emphasize 
that there are two types of information stored in the memory 
(i.e. the instructions that make up a program and the data 
used by the program). Indeed, forcing data and instructions 
to share the same path sometimes creates congestion on 
the data bus between the CPU and memory that slows the 
computer down. This effect is called the von Neumann 
bottleneck. 
5.3 The 68K family 
Anyone introducing computer architecture and the ISA has to 
make an important choice: should the target architecture used 
to illustrate the course be a real machine or a hypothetical 
teaching machine? A hypothetical machine reduces the stu-
dent's learning curve because you can simplify it. A real 
acceleration techniques later become known as RISCs.The 
term RISC was coined in the early 1980s by John Hennessey 
at Stanford University. 
RISC processors have lots of on-chip registers and do not 
allow you to perform operations directly on data in memory. 
You have to load data into a register, process it, and then store 
it in memory. For this reason, RISC processors are also called 
load/store processors. 
The goal of RISC processor design was to execute an aver-
age of one instruction per clock cycle by overlapping the exe-
cution of consecutive instructions (i.e. starting executing the 
next instruction before the current instruction has finished). In 
order to do this, it is necessary that there is a simple relation-
ship between the bit pattern of an instruction and what the 
instruction does. 
machine is harder to learn, but it does illustrate the real-world 
constraints faced by its designer. 
There's no perfect solution to this dilemma. We've chosen 
a real machine, the 68K, to introduce an assembly language 
and machine-level instructions. The 68K is a classic CISC 
processor and is easer to understand than the Pentium family 
because the 68K has a more regular instruction set. Another 
reason for using the 68K processor to illustrate the ISA is its 
interesting architectural features. 
The architecture of a processor is defined by its register set, 
its instruction set, and its addressing modes (the way in which 
the location of data in memory is specified). Figure 5.5 
describes the 68K's register set. There are eight data registers 
used to hold temporary variables, eight address registers used 
to hold pointers to data, a status register, and a program 
counter, which determines the next instruction to be executed. 
Data registers are 32 bits wide and but can be treated as if 
they were 8 or 16 bits wide. Address registers always hold 32-
bit values and are always treated as 32-bit registers that hold 
two's complement values. However, you can perform an 
operation on the low-order 16 bits of an address register and 
the result will be sign-extended to 32 bits automatically. 
5.3.1 The instruction 
We now look at the instructions executed by the 68K proces-
sor. There has been remarkably little progress in instruction 
set design over the last few decades and computers do today 
almost exactly what they did in the early days.4 Much of the 
3 Strictly speaking, we should write this operation in RTL as [X] <— 
[ Y] + [Z] to demonstrate that X, Y, and Z refer to memory locations. 
4 An exception to this is multimedia technology. Processors such as 
Intel's Pentium family introduced a special instruction to handle the type 
of data used in audio and video processing (e.g. the MMX instruction set). 

5.3 The 68K family 
211 
32 bits 
16 bits 
The eight data registers 
hold scratchpad 
information and are used 
by data processing 
instructions. You can treat 
data registers as 8-bit, 
16-bit, or 32-bit entities. 
The eight address registers 
hold 32-bit address or pointers. 
Address registers take part 
only in 32-bit operations and 
are used only in accessing 
data. 
Note that address register 
A7 is the system stack 
pointer because it points to 
the top of the stack. 
The program counter 
contains the address of 
the next instruction to 
be executed. 
Suits 
« 
k 
CCR 
The status register contains information about 
the state (operating mode) of the computer. 
DO 
D1 
D2 
D3 
D4 
DS 
D6 
D7 
AO 
A1 
A2 
A3 
A4 
A5 
A6 
A7 SP 
PC 
SR 
Figure 5.5 The 68K register set. 
progress over the last six decades has been in computer tech-
nology, organization, and implementation rather than in 
computer architecture. 
Computer instructions are executed sequentially, one by 
one in turn, unless a special instruction deliberately changes 
the flow of control or unless an event called an exception 
(interrupt) takes place. 
The structure of instructions varies from machine to 
machine. The format of an instruction running on a Pentium is 
different to the format of an instruction running on a 68K 
(even though both instructions might do the same thing). 
Instructions are classified by type (what they do) and by the 
number of operands they take. The three basic instruction types 
are data movement which copies data from one location to 
another, data processing, which operates on data, and flow con-
trol, which modifies the order in which instructions are exe-
cuted. Instruction formats can take zero, one, two, or three 
operands. Consider the following examples of instructions with 
zero to three operands. In these examples operands P, Q, and 
Effect 
Add P to Q and put the result in R 
Add P to Q and put the result in Q 
Add P to an accumulator 
Add the top two items on the stack 
Operands Instruction 
Three 
ADD P Q,R 
Two 
ADD P, Q 
One 
ADD P 
Zero 
ADD 
R may be memory locations or registers. The two-address 
instruction is in blue because that is the format used by the 68K. 
Let's begin with three operands because it's intuitively easy 
to understand. A three-address computer instruction can be 
written 
operation 
s o u r e e l , s o u r c e 2 , d e s t i n a t i o n 
where operation defines the nature of the instruction, 
sourcel is the location of the first operand, source2 is the 
location of the second operand, and destination is the 
location of the result. The instruction ADD p, Q, R adds P and 
Q to get R (remember that we really means that the instruc-
tion adds the contents of location P to the contents of loca-
tion Q and puts the sum in location R). Having reminded you 
that when we mention a variable we mean the contents of the 
memory location or register specified by that variable, we will 
not emphasize it again. 
Modern microprocessors don't implement three-address 
instructions exactly like this. It's not the fault of the instruc-
tion designer, but it's a limitation imposed by the practicali-
ties of computer technology. Suppose that a computer has a 
32-bit address that allows a total of 232 bytes of memory to be 
accessed. The three address fields, P, Q, and R would each be 
32 bits, requiring 3 X 32 = 96 bits to specify operands. 
Assuming a 16-bit operation code (allowing up to 216 = 65 536 
instructions), 
the 
total 
instruction 
size 
would 
be 

2 1 2 
Chapter 5 The instruction set architecture 
112 bits 
16 bits 
32 bits 
32 bits 
32 bits 
*.* 
Op-code 
Destination 
address 
Source 1 
address 
Source 2 
address 
(a) Format of a hypothetical instruction with three address fields 
32 bits  
17 bits 
5 bits 
5 bits 
5 bits 
• - < 
*~* 
The op-code and control 
bits together define the 
instruction. The op-code 
selects the class of 
instruction and the 
control bits select 
options that specify how 
the instruction operates. 
Op-code 
Control bits 
Destination 
register 
Source 1 
register 
Source 2 
register 
(b) Format of a hypothetical instruction with a register-to-register architecture 
Figure 5.6 Possible three-address instruction formats. 
Memory 
Registers 
ADD R1,R2,R3 
The instruction A D D R l , R2, R3 in 
memory is read by the computer and 
interpreted.The computer reads 
registers R2 and R3 to obtain the two 
source operands, sends these operands 
to the adder in the ALU, and then 
writes the sum from the adder to 
register R1. 
Figure 5.7 Implementing a three-address instruction. 
96 + 16 = 112 bits or 14 bytes. Figure 5.6(a) illustrates a 
hypothetical three-address instruction. 
Computer technology developed when memory was very 
expensive indeed. Implementing a 14-byte instruction was 
not cost effective in the 1970s. Even if memory had been 
cheap, it would have been too expensive to implement 112-
bit-wide data buses to move instructions from point to point 
in the computer. Finally, main memory is intrinsically slower 
than on-chip registers. 
The modern RISC processor allows you to specify three 
addresses in an instruction by providing three 5-bit operand 
address fields. This restriction lets you select from one of only 
32 different operands that are located in registers within the 
CPU itself.5 By using on-chip registers to hold operands, the 
time taken to access data is minimized because no other stor-
age mechanism can be accessed as rapidly as a register. An 
instruction with three 32-bit operands requires 3 X 5 bits to 
specify the operands, which allows a 32-bit instruction to use 
the remaining 32 — 15 = 17 bits to specify the instruction, as 
Fig. 5.6(b) demonstrates. Figure 5.7 illustrates the operation 
of an instruction with three register addresses. 
We'll use the ADD instruction to add together four values 
in registers R2, R3, R4, and R5. In the following fragment of 
code, die semicolon indicates the start of a comment field, 
which is not part of the executable code. This code is typical 
of RISC processors like the ARM. 
ADD 
R1,R2,R3 
ADD 
Rl, Rl, R4 
ADD 
R1,R1,R5 
;R1 = R2 + R3 
;R1 = Rl + R4 
;R1 = Rl + R5 
= R2 + R3 + R4 + R5 
5 I will use RISC very loosely to indicate the class of computers that 
have a register-to-register architecture such as the ARM, MIPS, 
PowerPC, and SPARC. The Motorola 68K and the Intel Pentium are not 
members of this group. 
RO [ 
R1 I 
R2 I 
R3 
R4 [ 
Wdderi 

5.3 The 68K family 
213 
REGISTER-TO-REGISTER ARCHITECTURES 
Computers act on data in registers or memory locations. Many 
data processing operations operate on two operands; for 
example, X + Y or X - Y or XY or Z©Y. These operations are 
said to be dyadic because they require two operands. The 
result of such a dyadic operation generates a third operand, 
called the destination operand; for example, Z = A + B. 
First-generation microprocessors of the 1970s and 1980s 
allowed one source operand to be in memory and one 
source operand to be in a register in the CPU. A separate desti-
nation address was not permitted, forcing you to use one of 
the source operands as a destination. This restriction means 
that one of the source operands is destroyed by the 
instruction. 
A typical two-address instruction is ADD DO, P. This adds 
the contents of memory location P to the contents of register 
DO and deposits the result in location P. The original contents 
of P are destroyed. 
Register-to-register architectures permit operations only on 
the contents of on-chip registers such as ADD Rl, R2, R3.The 
source or destination of an operand is never a memory loca-
tion. Consequently, registers must first be loaded from mem-
ory and the results of an operation transferred to memory. 
Two-address machines 
A CISC machine like the 68K has a two-address instruction 
format. Clearly, you can't execute P = Q + R with two 
operands. You can execute Q <— P + Q. One operand appears 
twice, first as a source and then as a destination. The opera-
tionADD P, Q performs the operation [Q] <— [P] + [QJ.The 
price of a two-operand instruction format is the destruction, 
by overwriting, of one of the source operands. 
Most computer instructions can't directly access two 
memory locations. Typically, the operands are either two reg-
isters or one register and a memory location; for example, the 
68K ADD instruction can be written 
Instruction 
RTL definition 
Mode 
ADD D 0 , D 1 
[ D l ] <- [ D l ] + [DO] 
Register to register 
ADD P,D2 
[ D 2 ] < - [ D 2 ] + [P] 
Memory to register 
ADD D7,P 
[P] 
< - [ P ] 
+ [ D 7 ] 
Register to memory 
The 68K has seven general-purpose registers, DO to D7; there 
are no restrictions on the way in which you use these 
registers; that is, if you can use Di you can also use Dj for any 
iorjfromO to 7. 
One-address machines 
A one-address machine specifies one operand in the instruction. 
The second operand is a fixed register called an accumulator, 
which doesn't have to be specified. For example, the operation 
one-address instruction ADD p means [A] <- [A] + [P]. 
The notation [A] indicates the contents of the accumulator. 
A simple operation R = P + Q can be implemented by the 
following fragment of 8-bit code (from a 6800 processor). 
LDA P 
;load accumulator with P 
ADD Q 
;add Q to accumulator 
STA R 
;store accumulator in R 
Eight-bit machines of the Intel 8080 and Motorola 6800 
eras have one-address architectures. As you can imagine, 
8-bit code is verbose because you have to load data into the 
accumulator, process it, and then store it to avoid it being 
overwritten by the next data processing instruction. 
One-address machines are still widely used in embedded 
controllers in low-cost, low-performance systems such as 
toys. We look at an 8-bit processor in Chapter 9. 
Zero-address machines 
A zero-address machine doesn't specify the location of 
an operand because the operand's location is fixed. A 
zero-address machine uses a stack, which is a data structure in 
the form of a queue where all items are added and removed from 
the same end. An ADD instruction would pop the top two items 
off the stack, add them together, and push the result on the stack. 
Although stack machines have been implemented to execute 
languages like FORTH, processors with stack-based architec-
tures have been largely confined to the research lab. There is one 
exception. The language JAVA is portable because it is complied 
into bytecode, which runs on a stack machine, which is simu-
lated on the real target machine. We will return to the stack later. 
68K instruction format 
We will look at 68K instruction in detail when we've covered 
more of the basics. The 68K has a two-address instruction 
format. An operand may be a register or a memory location. 
The following are valid 68K instructions. 
I n s t r u c t i o n 
RTL definition 
ADD 
D 4 , D 1 
[ D l ] <-
[ D l ] 
+ 
[D4] 
SUB 
P , D 6 
[D6] <— [D6] 
-
[P] 
AND 
D 7 , P 
[P] 
<-
[P] 
A 
[D7] 
MOVE 
D 3 , D 1 
[ D l ] <-
[D3] 
Register to register 
MOVE 
X , D 1 
[ D l ] 
4-
[X] 
Memory to register 
MOVE 
D 2 , Y 
[Y] 
<-
[D2] 
Register to memory 
MOVE X , Z 
[Z] 
<— [X] 
The only memory to 
memory operation 
CLR 
DO 
[DO] <~ 0 
Only one operand 
required for the clear 
instruction 

214 
Chapter 5 The instruction set architecture 
Consider the 68K's ADD instruction ADD $00345678, 
D2. 
This instruction performs the operation 
[D2] <-
[D2] + [345678,6]. The two source operands provide the 
addresses: one address is a memory location and the other a 
data register. This instruction format is sometimes called 
'one-and-a-half address' because you can specify only a hand-
ful of registers. 
CISC processors use variable-length instructions. The 
minimum 68K instruction size is 16 bits and instructions can 
be constructed by chaining together successive 16-bit values in 
memory. For example, die 68K is one of the few processors to 
provide a memory-to-memory MOVE instruction that supports 
absolute 32-bit addressing. You can write MOVE $12345678, 
$ABCDDCBA, which takes 10 consecutive bytes in memory and 
moves the contents of one memory location to another. 
Subword operations 
First-generation microprocessors had 8-bit data wordlengths 
and operations acted on 8-bit values to produce 8-bit results. 
When 16-bit processors appeared, operations were applied to 
16-bit values to create 16-bit results. However, the byte did not 
go away because some types of data such as ASCII-encoded 
characters map naturally on to 8-bit data elements. 
If you wish to access bytes in a 16- or 32-bit processor, you 
need special instructions. The Motorola 68K family deals with 
8-bit, 16-bit, and 32-bit data by permitting most data process-
ing instructions to act on an 8-bit or a 16-bit slice of a register 
as well as the full 32 bits. RISC processors do not (generally) 
support 8- or 16-bit operations on 32-bit registers, but they do 
support 8-bit and 16-bit memory accesses. 
Suppose a processor supports operations that act on a 
subsection of a register. This raises the interesting ques-
tion, 'What happens to the bits that do not take part in 
the operation?' Figure 5.8 demonstrates how we can handle 
operations shorter than 32 bits. Assume that a register is 
partitioned as Fig. 5.8(a) demonstrates. In this example, we 
are going to operate on data in the least-significant byte. 
We can do three things, as Fig. 5.10(b) and (c) demon-
strates. In (b) the bits not acted on remain unchanged—this 
is the option implemented by the 68K when it operates on 
data registers. In (c) the bits that do not take part in the oper-
ation are cleared to zero. In (d) the bits that do not take part 
in the operation are set to the value of the most-significant 
bit (the sign bit) of the bits being operated on. This option 
preserves the sign of two's complement values. Most proces-
sors implement options (c) or (d). 
RISC processors like the ARM do not allow general data 
processing operations on fever than 32 bits. However, they do 
support 8-bit and 16-bit load instructions with a zero or sign 
extension. 
The 68K calls 32-bit values longwords, 16-bit values words, 
and 8-bit values bytes. Motorola's terminology is not univer-
sal. Others use the term word to mean 32 bits and halfword to 
means 16 bits. The 68K is an unusual processor because it 
allows variable size operations on most of its data processing 
instructions. By appending . B after an instruction, you per-
form an operation on a byte. Appending .w performs the 
operation on a 16-bit word and appending . L performs the 
operation on a 32-bit longword. Omitting a size suffix selects 
a 16-bit default. Consider the following. 
Unused bits 
Data to be modified 
- • < 
1 
(a) This represents the data before 
the operation. An operation takes 
place on a slice of the register. 
No change 
fb) The simplest arrangement 
(implemented by the 68K) is to 
leave bits not taking part in the 
operation unchanged. 
0 0 
0 0 0 
(c) Some processors perform an 
operation on a subsection of a 
register and clear all bits not taking 
part in the operation to zero. 
s s 
s s s 
s 
(d) If the data in a register is 
a signed integer, it is necessary 
to expand the number by sign, 
extending it to 32 bits after the 
operation. 
Figure 5.8 Extending data. 

5.3 The 68K family 
215 
Instruction 
RTL definition 
ADD.B 
D0,D1 
[Dl ( 0 :,,] 
< - [ D l ( 0 : 7 ) ] 
+ [D4 ( 0 : 7 )] 
ADD.W 
D0,D1 
[Dl, 0 : l b l] 
<- [Dl ( 0 : 1 5 l] 
+ [D4 i 0 : 1 5 )] 
ADD.L 
D0,D1 
[Dl(o:3i|] <- [Dl ( 0 : 3 1 )] 
+ [D4(0:3i,] 
ADD 
D0,D1 
[Dl(o:i5)] <- [Dl(0:i5)] 
+ [D4 ( 0 : 1 5 )] 
Same as ADD.W 
D0,D1 
5.3.2 Overview of addressing modes 
A key concept in computing in both high- and low-level lan-
guages is the addressing mode. Computers perform opera-
tions on data and you have to specify where the data comes 
from. The various ways of specifying the source or destina-
tion of an operand are called addressing modes. 
We can't discuss instructions, the ISA, or low-level pro-
gramming until we have introduced three fundamental 
concepts in addressing: 
• Absolute addressing 
(the operand specifies the location 
of the data) 
• Immediate addressing 
(the operand provides the 
operand itself) 
• Indirect addressing 
(the operand provides a pointer 
to the location of the data). 
In absolute addressingyou 
specify an operand by providing 
its location in memory or in a register. For example, ADD 
P, Dl uses absolute addressing because die location of the 
operand P is specified as a memory location. Another example 
of absolute addressing is the instruction CLR 1234, which 
means set the contents of memory location 1234 to zero. 
When you specify a data register as an operand, that is also 
an example of absolute addressing, although some call it reg-
ister direct addressing. 
In immediate 
addressing the operand is an actual value 
rather than a reference to a memory location. The 68K 
assembler indicates immediate addressing by prefixing the 
operand with the '#' symbol; for example, ADD # 4, DO means 
add the value 4 to the contents of register DO and put the 
result in register DO. Immediate addressing lets you specify a 
constant, rather than a variable. This addressing mode is 
called immediate because the constant is part of the instruc-
tion and is immediately available to the computer. The 
addressing mode is also called immediate 
because the 
operand is immediately available from the instruction and 
you don't have to fetch it from memory or a register. When 
you specify the absolute address of a source operand, the 
computer has to get the address from the instruction and 
then read the data at that location. 
Indirect addressing specifies a pointer 
to the actual 
operand, which is invariably in a register. For example, the 
instruction, MOVE (AO) , Dl first reads die contents of regis-
ter AO to obtain a pointer that gives you the address of 
the operand. Then it reads the memory location specified by 
the pointer in AO to get the actual data. This addressing mode 
requires three memory accesses; the first is to read the 
instruction to identify the register containing the pointer, and 
the second is to read the contents of the register to get the 
pointer, the third is to get the desired operand at the location 
specified by the pointer. 
You can easily see why this addressing mode is called indi-
rect because the address register specifies the operand indi-
rectiy by telling you where it is, rather than what it is. 
Motorola calls this mode of address register indirect address-
ing, because the pointer to the actual operand is in an address 
register. Figure 5.9 illustrates the effect of executing the oper-
ation MOVE (AO) ,D0. 
In Fig. 5.9 address register AO points to a memory location; 
that is, the value it contains is the address of an operand in 
memory. In this case AO contains 1234 and is, therefore, 
pointing at memory location 1234. When the instruction 
MOVE (AO) ,D0 is executed, the contents of the memory 
location pointed at by AO (i.e. location 1234) are copied 
into data register DO. In this example, DO will be loaded 
with 3254. 
DATA AND ADDRESS REGISTERS 
The 68K has eight data registers, DO to D7. It also has eight 
address registers, AO to A7. Data and address registers are 
similar. Both types of registers are 32 bits wide. The principal 
difference between these registers lies in their function. A data 
register holds any data (including addresses). An address register 
is used only to hold the address of an operand in memory. 
All data processing operations can be applied to any data 
register, but not to all address registers. Address registers take 
part in operations only of relevance to the processing of 
addresses. 
All data registers are equal in the sense that you can use 
any one in any way. This is not true of address registers. You 
can use AO to A6 in any way you want but A7 is special. A7 has 
a system function called the stack pointer. Address register A7 
points to the top of the system stack (the place where subrou-
tine return addresses are stored). 

216 
Chapters The instruction set architecture 
Executing a MOVE (AO) ,DO instruction 
AO 
1234 
Pointer 
• 
1232 
1234 
Address register AO is a p o i n t e r . " " 
It contains the value 1234 and, 
therefore, points to address location 
1234. If you use AO to access memory, 
you will access location 1234. 
Memory 
3254 
The effect of MOVE 
is [DO] «- [[AO]] 
Figure 5.9 Address register indirect addressing. 
Why do we implement this addressing mode? Consider the 
following two operations. 
MOVE 
ADD.L 
(AO),D0 
#2,A0 
; copy t h e item p o i n t e d a t by AO i n t o DO 
; increment AO t o p o i n t t o t h e n e x t 
item 
The first operation loads DO with the 16-bit element pointed 
at by address register AO. The second instruction increments 
AO by 2 to point to the next element. The increment is 2 
because the elements are 2 bytes (i.e. 16 bits) wide and 
successive elements are 2 bytes apart in memory. 
Address register indirect addressing allows you to step 
though an array or table of values accessing consecutive ele-
ments. Suppose we have a table of 20 consecutive bytes that 
we have to add together. We can write 
The first three instructions set up the initial 
values. We load AO with the address of the 
numbers. The location has the symbolic name 
'Table'. The # symbol precedes 'Table' because 
Do 
AO is being loaded with the address table and 
not the contents of that address. Data register DO 
is used to hold the sum of the numbers and is 
cleared prior to its first use. Finally, we put the 
( AO ), DO 
number 20 into Dl to count the elements as we 
add them. 
The body of the code is in blue. The first 
instruction fetches the byte pointed at by AO 
and adds it to the running total in DO and the second instruc-
tion points to the next byte element in the list. Note that when 
we increment the pointer we use a longword operation 
because all pointers are 32 bits. 
The last part of the program decrements the element count 
by one and then branches back to 'Next' if we haven't reached 
zero. We look at the branching operations in more detail later. 
The three addressing modes form a natural progression. 
Consider their definitions in RTL. 
Addressing mode 
Assembly 
RTL 
Memory 
form 
accesses 
Immediate addressing 
MOVE # 4 , D 1 
[Dl] < - 4 
l 
Absolute addressing 
MOVE P , D 1 
[Dl] <- [P] 
2 
Indirect addressing 
MOVE ( A l ) , D 1 
[Dl] <- [ [ A l ] ] 
3 
MOVE.L •Table,AO 
CLR.B 
DO 
MOVE.B # 2 0 , D l 
ADD.B 
(AO),D0 
ADD.L 
# i, ao 
SUB.B 
#1,D1 
BNE 
Next 
;A0 points to the table (AO has the address of Table) 
;Use DO to hold the sum - clear it first 
;There are 20 numbers to add 
;fldci a number to che total in DO 
;Point to the next number in the list 
;Decrement the counter 
;Repeat until all added in 
IMPORTANT POINTS 
The fragment of code to add the 20 numbers is, in principle, 
very straightforward. However, it contains aspects that many 
beginners find confusing. Indeed, I would say that probably 
90% of the errors made by beginners are illustrated by this 
fragment of code. Consider the following points. 
1. Data register DO is used to hold the running total. At the 
machine level, registers and memory locations are not set 
to zero before they are used. Therefore, the programmer 
must initialize their contents either by clearing them or by 
loading a value into them. 
2. We are working with byte-wide data elements throughout. 
Therefore all operations on data in this problem have a . B 
suffix. All operations on pointers have an . L suffix. Do not 
confuse operations on a pointer with operations on the 
data elements at which they point! 
Understand the meaning of the # symbol, which indicates 
a literal value, MOVE 1234, DO puts the contents of mem-
ory location 1234 in register DO.MOVE #1234, DO puts 
the number 1234 in DO. This is the single most common 
mistake my students make. 
An address register used to hold a pointer has to be loaded 
with the value of the pointer. This value is a memory 
location where the data lies. If the symbolic name for the 
address of a table is PQR, then you point to PQR with 
MOVE. L #PQR, A0. You are putting an actual address in 
A0 and not the contents of a memory location. 
3. 
4. 

5.4 Overview of the 68K's instructions 
217 
MOVE #4,D1 
Literal addressing 
The operand is part 
the instruction. 
MOVE 1234,Dl 
Memory 
Absolute addressing 
The operand is a register 
or memory location. 
1234 
MOVE (AO),D1 
Figure 5.10 The three addressing modes. 
Dl 
, 
Memory 
^ 
AO 
Pointer 
\ 
Operand 
Pointer 
\ 
Operand 
Indirect addressing 
The operand is specified 
by a pointer. In this case 
pointer is in AO. 
Figure 5.10 illustrates these three addressing modes 
graphically. 
5.4 Overview of the 68K's 
instructions 
We now look at the type of operations that the 68K and 
similar processors carry out on data. Here we are interested in 
general principles. In the next chapter we demonstrate how 
the instructions can be used. A typical two-operand 
memory-to-register instruction has the format 
ADD 
P,D0 
and is interpreted as [DO] 
<- 
[DO] + [P]. The source 
operand appears first (left to right), then the destination 
operand. Instructions can be divided into various categories. 
For our current purposes, we will consider the following 
broad categories. 
Data movement These instructions copy data from one 
place to another; for example, from memory to a register or 
from one register to another. 
Arithmetic Arithmetic instructions perform operations on 
data in numeric form. In this chapter we assume data is either 
a signed or an unsigned integer. 
Logical A logical operation treats data as a string of bits and 
performs a Boolean operation on these bits; for example, 
11000111 AND 10101010 yields 10000010. 
Shift A shift instruction moves the bits in a register one or 
more places left or right; for example, shifting 00000111 one 
place left yields 00001110. 
Bit A bit instruction acts on an individual bit in a register, 
rather than the entire contents of a register. Bit instructions 
allow you to test a single bit in a word (for 1 or 0), to set a bit, 
to clear a bit, or to flip a bit into its complementary state. 
Compare These instructions compare two operands and set 
the processor's status flags accordingly; for example, a 
compare operation allows you to carry out the test. 
Control Control instructions modify the flow of control; that 
is, they change the normal sequential execution of instructions 
and permit instructions to be executed out of order. 
5.4.1 Status flags 
Before we continue we have to introduce the notion of the proces-
sor status register because its contents can be modified by the 
execution of most instructions. The processor status register 
records the outcome of an instruction and it can be used to imple-
ment conditional behavior by selecting one of two courses of 
action. Some processors call this register a condition code register. 
\ 
Operand 
D1 
D1 
4 

218 
Chapter 5 The instruction set architecture 
Conditional behavior is the feature of computer languages that 
lets us implement high-level language operations such as 
if 
(x == 4) then 
or 
(i = 0; i < 20; i++). 
A processor register contains at least four bits, Z, N, C, and 
V, whose values are set or cleared after an instruction has been 
executed. These four flags, or status bits, and their interpreta-
tions are as follows. 
Z-bit Set if the result of the operation is zero. 
N-bit Set if the result is negative in a two's complement 
sense; that is, the leftmost bit is zero. 
C-bit Set if the result yields a carry-out. 
V-bit Set if the result is out of range in a two's complement 
sense. 
Typical CISC processors update these flags after each 
operation (see box for more details). 
Consider the following example using 8-bit arithmetic. 
Suppose DO contains 001101012 and Dl contains 011000112. 
The effect of adding these two values together with 
ADD DO, Dl would result in 
001101012 
+011000112 
100110002 
The result is 100110002, which is deposited in Dl. If we inter-
pret these numbers as two's complement values, we have added 
two positive values and got a negative result. Consequently, the 
V-bit is set to indicate arithmetic overflow. The result is not 
zero, so the Z-bit is cleared. The carry-out is 0. The most-
significant bit is 1, so the N-bit is set. Consequendy, after this 
operation C = 0,Z = 0,N = 1,V = 1. 
5.4.2 Data movement instructions 
The most frequently executed computer operation is data 
movement. The data movement instruction is incorrectly 
CONDITION CODE FLAGS 
One of the biggest variations between processor families is 
the treatment of condition code flags .There are three aspects 
to the way in which these flags are updated. 
Update always The condition code flags are updated after 
each and every instruction has been executed. 
Update sometimes The condition code flags are updated 
after some instructions have been executed but not others. 
Generally the flags are updated after instructions that might 
be used in a comparison such as 'is X < Y' but not after 
instructions that perform routine housekeeping tasks. 
named because the one thing it does not do is move data. Data 
movement instructions copy data; for example, the instruc-
tion MOVE y , x copies the contents of Y to X but does not 
modify the value of Y. You could say that a data movement 
instruction is a data propagate or data copy instruction. 
Some processors have a load instruction, a store instruc-
tion, and a move instruction. A load copies data from mem-
ory to a register, a store copies data from a register to memory, 
and a move instruction copies data from one register to 
another. As we already know, the 68K has a single MOVE 
instruction, which copies data from anywhere to anywhere. 
There are other types of move operation; for example, the 
68K has an exchange instruction that swaps the contents of 
two registers; for example, 
EXG Dl, A2 has the effect [A2 ] <- [Dl ] ,- [Al ] <- [A2 ] 
The purpose of the semicolon in the above RTL indicates 
that the two operations happen simultaneously. 
5.4.3 Arithmetic instructions 
Arithmetic operations are those that act on numeric data (i.e. 
signed and unsigned integers). Table 5.1 lists the 68K's arith-
metic instructions. Let's look at these in detail. 
Add The basic ADD instruction adds the contents of two 
operands and deposits the result in the destination operand. 
One operand may be in memory. There's nothing to stop you 
using the same source operand twice and writing ADD DO, DO 
to load DO with the value of 2 X [DO]. 
All addition and subtraction instructions update the con-
tents of the condition code register unless the destination 
operand is an address register. 
Add with carry The add with carry instruction, ADC, is 
almost the same as the ADD instruction. The only different is 
that ADC adds the contents of two registers together with 
the carry bit; that is, ADC DO,Dl performs [Dl] <-[Dl] + 
[DO] + C, where C is the carry bit generated by a previous 
operation. 
Update on demand The condition code register is updated 
only when the programmer requires it. This mode is indicated 
by appending a suffix to an instruction to indicate an update 
flags request. 
The 68K updates its status bits after most instructions 
are executed. You simply have to learn which instrucion 
update the flags and which don't. Instructions that affect 
the flow of control such as subroutine calls and instructions 
that act on address registers do not update the conditon 
code flags. 

5.4 Overview of the 68K's instructions 
219 
ADD 
D 0 , D 1 
Add 
[ D l ] <-
[ D l ] 
+ 
[DO] 
ADC 
D 0 , D 1 
Add with carry 
[ D l ] <-
[ D l ] 
+ 
[DO] 
+ C 
SUB 
D 0 , D 1 
Subtract 
[ D l ] <-
[ D l ] -
[DO] 
SBC 
D 0 , D 1 
Subtract with carry 
[ D l ] 
i— 
[ D l ] -
[DO] 
- C 
MULU D 0 , D 1 
Multiply (unsigned) 
[ D l ] 
<r-
[ D l ] 
X 
[DO] 
MULS D 0 , D 1 
Multiply (signed) 
[ D l ] <-
[ D l ] 
X 
[DO] 
DIVU D 0 , D 1 
Divide (unsigned) 
[ D l ] <- [ D l ] 
H-
[DO] 
DIVS D 0 , D 1 
Divide (signed) 
[ D l ] <~ [ D l ] ^ [DO] 
Table 5.1 The 68K's arithmetic instructions. 
Most-significant word 
Least-significant word 
^upper 
^lower 
1 upper 
"-upper 
Mower 
Figure 5.11 Extended addition. 
This instruction is used in extended or compound arith-
metic. Suppose you wish to add two 64-bit numbers using 
the 32-bit 68K. Assume that the most-significant 32 bits of X 
are in DO and the least-significant 32 bits are in Dl, and the 
most-significant 32 bits of Y are in D2 and the least-signifi-
cant bits are in D3. We can perform the 64-bit addition 
X + Yby 
Multiplication 
Unfortunately, 
two's 
complement arithmetic works only for 
addition and subtraction; that is, you 
don't have to use a different addition 
operation for unsigned and signed inte-
gers. The same is not true for multipli-
cation and division. If you are using 
unsigned integers you have to do multi-
plication one way and if you are using 
signed two's complement integers you 
have to do multiplication in a different 
way. Consequently, the 68K has two 
multiplication (and division) instruc-
tions and you have to choose the one to 
reflect the type of numbers you are 
using. MULU multiplies two unsigned 
integers and MULS multiplies two signed 
integers. 
The 68K's multiplications do not 
provide the same flexibility as the 68K's 
addition instructions. You can multiply 
only two 16-bit integers to get a 32-bit 
result (remember that multiplying two n-bit values yields a 
2«-bit product). The operation MULU DO, Dl performs the 
operation 
The two lower words are 
added to generate a sum 
and a carry out. 
The two higher words are 
added together with any 
carry bit generated from 
adding the low-order words. 
[Dl, 
[01,0:15)1 
X [DO,0:15)] 
Division The 68K's division instructions are a little more 
complicated because division results in a quotient plus a 
remainder; for example, 17 + 3 = 5 remainder 2. The 68K 
ADD.L D0,D2 ;Add the low-order 32 bits, update carry flag 
ADC.L D1,D3 ;Add the high-order 32 bits plus any carry 
In this example we use ADD . L to add the two low-order 32-
bit words. Remember that the . L suffix indicates a 32-bit 
operation. An addition records any carry bit generated by the 
addition and moves it to the C-bit. The following instruction 
ADC adds the high-order longwords together with any carry 
that was generated by adding the low-order longwords. 
Figure 5.11 illustrates the addition Z = X + Y where X, Y, 
and Z are 64-bit values and the addition is to be performed 
with 32-bit arithmetic. Each of the operands is divided into 
an upper and lower 32-bit word. 
Subtract The subtract instruction subtracts the source 
operand from the destination operand and puts the result in 
the destination register; for example, SUB DO, Dl performs 
[Dl]«-[Dl] - [DO]. A special subtraction operation that 
facilitates multiple length subtraction, SBC DO , Dl, performs 
the action [D1]<-[D1] - [DO] - C (the carry bit is also 
subtracted from the result). 
divides a 32-bit value by a 16-bit value to produce a 16-bit 
quotient and a 16-bit remainder. In order to avoid using an 
instruction with three operands, the quotient and remainder 
are packed in the same register. For example, DIVU DO, Dl 
divides the 32-bit contents of Dl by the 16-bit lower-order 
16-bit word in DO and puts the 16-bit quotient in the low-
order word of D1 and the 16-bit remainder in the high-order 
word of Dl. We can express this as 
[01,0:15)1 
[01,15:31)1 
[Dl(0:31)l 
^ 
[ D 0 , 0 : 1 5 ) ] 
remainder 
If DO contains 4 and Dl contains 1234516, the operation 
DIVU DO, Dl results in Dl = 000148D116. Consider the fol-
lowing fragment of code where we divide P by Q and put the 
quotient in D2 and the remainder in D3. 
'lower 
0:31 

220 
Chapter 5 The instruction set architecture 
MOVE.L P , D 1 
MOVE.W Q,DO 
DIVU 
D 0 , D 1 
C L R . L 
D2 
MOVE.W D 1 , D 2 
C L R . L 
D3 
SWAP 
D l 
MOVE.W D 1 , D 3 
;get P 
;get Q 
/divide P by Q 
/clear D2 ready to receive quotient 
/put quotient in D2 
/clear D3 ready to receive remainder (clear = make 0) 
/exchange upper and lower words in Dl 
/put remainder in D3 
This code is more complex than you would think and 
demonstrates the pitfalls of assembly language. First we have to 
remember that P is a 32-bit value and that Q is a 16-bit value. 
The divide instruction divides a 32-bit value by a 16-bit value. 
Because we get the quotient and remainder in Dl, we have to 
split them and copy them to D2 and D3 respectively. A MOVE 
instruction always operates on the low-order word in a register, 
which means that we don't have to worry about the remainder 
bits in bits 16 to 31 of D1 when we copy the quotient. However, 
because D2 is a 32-bit register, we should ensure that the upper 
order bits are zero before we do the transfer. We use CLR . L to set 
all the bits of D2 to zero before transferring the 16-bit quotient. 
We can use the SWAP instruction, which exchanges the 
upper and lower order words of a register to get the remain-
der in the low-order 16-bits of Dl before we transfer the 
remainder to D3. 
When writing 68K instructions, you always have to ask 
yourself'How? many bits are we operating on? and' What are 
we going to do about the bits not taking part in the operation1. 
Some processors take away that choice; for example, the ARM 
and similar RISC processors require that all operations be 
applied to all bits of a register. 
5.4.4 Compare instructions 
High-level languages provide conditional constructs of the form 
i f 
(x == y) 
{a = b * c } / 
We examine how these constructs are implemented later. 
At this stage we are interested in the comparison part of the 
above construct, (x ==y), which tests two variables for 
equality. We can also test for greater than or less than. The 
operation that performs the test is called comparison. 
The 68K provides a compare instruction CMP source, 
destination, which evaluates [Rd]-[Rs] and updates the 
bits in the condition code register accordingly. 
A compare instruction is inevitably followed by a branch 
instruction that chooses one of two courses of action 
depending only on the outcome of the comparison. Here we 
simply demonstrate a (compare, branch) pair because we will 
soon look at branch instructions in greater detail. 
Consider the high-level construct i f (x = = 5 ) 
{x = x + 10} 
We can write the following fragment of code: 
MOVE X,D0 
/ g e t 
X i n DO 
CMP 
#5,DO 
/ i s X == 5? 
BNE 
E x i t 
/ i f 
n o t e q u a l t h e n go t o 
" e x i t " 
ADD 
#10,DO 
/ e l s e 
add 10 t o X 
MOVE D0,X 
/ r e s t o r e X t o memory 
In this example the branch instruction BNE Exit forces a 
branch (jump) to the line labeled by Exit if the outcome of 
the compare operation yields 'not zero'. 
5.4.5 Logical instructions 
Logical operations allow you to directly manipulate the indi-
vidual bits of a word. When a logical operation is applied to 
two 32-bit values, the logical operation is applied (in parallel) 
to each of the 32 pairs of bits; for example, a logical AND 
between A and B would perform q = a{b{ for all values of i. 
Table 5.2 illustrates the 68K's logical operations using an 8-
bit example. 
The AND operation is dyadic and is applied to two source 
operands. Bit i of the source is ANDed with bit i of the desti-
nation and the result is stored in bit i of the destination. If 
[Dl] = 110010102, the operation 
AND #»11110000,D1 
results in [Dl] = 110000002. Remember that the symbol # 
indicates a literal or actual operand and the symbol % 
indicates a binary value. We can represent this operation 
more conventionally as 
11001010 
11110000 
11000000 
D2 
10101010 
D l 
10101010 
Operation 
CMP D1,D2 
Processor status flags 
Z = 1,C = 0,N = 0,V = 0 
10101010 
00000000 
CMP D1,D2 
Z = 0,C = 0,N = 1,V = 0 
10101010 
11000001 
CMP D1,D2 
Z = 0,C= 1,N= 1,V = 0 
Exit . 

5.4 Overview of the 68K's instructions 
221 
Mnemonic 
Opera t ion 
Definition 
 
AND DO, Dl 
Logical AND 
[D1] <- [D1] A [DO] 
OR 
D0,D1 
Logical OR 
[D1] <-[D1] v [DO] 
EOR DO, Dl 
Exclusive EOR 
[D1] +- [D1] e [DO] 
NOT Dl 
Logical NOT 
[D1]<-fDlJ 
Table 5.2 The 68K's logical instructions. 
The AND operation is used to mask the bits of a word. If 
you AND bit x with bit y, the result is 0 if y = 0, and x if/ = 1. 
A typical application of the AND instruction is to strip the 
parity bit off an ASCII-encoded character. That is, 
AND 
#%01111111,D1 
clears bit 7 of Dl to zero and leaves bits 0 to 6 unchanged. 
The OR operation is used to set one or more bits of a word 
to 1. ORing a bit with 0 has no effect and ORing the bit with 
1 sets it. For example, if [Dl] = 110010102, the operation 
OR 
#%11110000,D1 
results in [Dl] = 111110102. 
The EOR operation is used to toggle (i.e. invert) one 
or more bits of a word. EORing a bit with 0 has no effect 
and EORing it with 1 inverts it. For example, if [Dl] = 
110010102, the operation 
EOR #%11110000,D1 
results in [Dl] =001110102. 
By using the NOT, AND, OR, and EOR instructions, you 
can perform any logical operations on a word. Suppose you 
wish to clear bits 0,1, and 2, set bits 3,4, and 5, and toggle bits 
6 and 7 of the byte in DO. You could write 
AND 
#%11111000,D1 
Clear b i t s 0, 1, and 2 
OR 
#%00111000,D1 
Set b i t s 3, 4, and 5 
EOR 
#%11000000,D1 
Toggle b i t s 6 and 7. 
If [Dl ] initially contains 010101012, its final contents will 
be 101110002. We will look at a more practical application of 
bit manipulation after we have covered branch operations in 
a little more detail. 
5.4.6 Bit instructions 
The 68K provides bit instructions that operate on the indivi-
dual bits of a word. Bit instructions are not strictly necessary, 
because you can use logical operations to do the same thing. 
The 68K's bit instructions can be used to set, clear, or toggle 
(complement) a single bit in a word. Moreover, the bit instruc-
tions also test the state of the bit they have tested and set or clear 
the Z-bit of the condition control register accordingly. Consider 
BTST #4,DO 
;test bit number 4 of 
BSET #4,DO 
;test bit 4 of DO, se 
BSET D1,D0 
;test the bit of DO w 
Example 
11110000 • 10101010 = 10100000 
11110000 + 10101010 = 11111010 
11110000 e 10101010 = 01011010 
11110000 = 00001111 
5.4.7 Shift instructions 
A shift operation moves a group of bits one or more places 
left or right; for example, consider the following examples 
Source 
After shift left 
After shift right 
00110011 
01100110 
00011001 
11110011 
11100110 
01111001 
10000001 
00000010 
01000000 
Although there are only two shift directions, left and right, 
there are several variations on the basic shift operation. These 
variations depend on whether we are treating the value being 
shifted as an integer or a signed value and whether we include 
the carry bit in the shifting. 
Shift operations are used to multiply or divide by a power 
of 2, to rearrange the bits of a word, and to access bits in 
a specific location of a word. 
Figure 5.12 illustrates the various types of shift operation. 
Suppose the 8-bit value 110010102 is shifted one place right. 
What is the new value? A logical shift right operation, LSR 
introduces a zero into the leftmost bit position vacated by the 
shift and the new value is 011001012. 
Arithmetic shifts treat the data shifted as a signed two's 
complement value. Therefore, the sign bit is propagated by 
an arithmetic shift right. In this case, the number 
110010102 = —54 is negative and, after an arithmetic right 
shift, ASR, the new result is 111001012 (i.e. -27). 
When a word is shifted right, the old least-significant bit 
has been shifted out and 'lost'. Figure 5.12 shows that this bit 
isn't lost because it's copied into the carry flag bit. 
An arithmetic shift left is equivalent to multiplication by 2 
and an arithmetic shift right is equivalent to division by 2. 
Some computers allow you to shift one bit position at a 
time. Others let you shift any number of bits. The number of 
bits to be shifted can be a constant; that is, it is defined in the 
program and the shift instruction always executes the same 
number of shifts. Some computers let you specify the 
number of bits to be shifted as the contents of a register. This 
allows you to implement dynamic shifts because you can 
DO and set the Z-bit accordingly 
the Z-bit accordingly, set bit 4 
iose position is in Dl 

2 2 2 
Chapter 5 The instruction set architecture 
H 
LSL 
Operand 4 
Operand 4 
0 
• 
4 
0 
LSR 
Operand 
• 
•B 
Logical shift left 
A zero enters the least-significant bit and the 
most-significant bit is copied to the carry flag. 
Logical shift right 
A zero enters the most-significant bit and the 
least-significant bit is copied to the carry flag. 
0-
MSB 
ASL 
Operand •+-
•* 
0 
ASR 
Operand 
• 
•H 
Arithmetic shift left 
A zero enters the least-significant bit and the 
most-significant bit is copied to the carry flag. 
Arithmetic shift right 
The old most-significant bit is copied into the new 
most-significant bit and the least-significant bit is 
copied to the carry flag. 
LH 
ROL 
-4 
* 
Operand 
-*-
ROR 
Operand 
:U[7| 
Rotate left 
The most-significant bit is copied into the 
least-significant bit and the carry flag. 
Rotate right 
The least-significant bit is copied into the 
most-significant bit and the carry flag. 
0 
A 
» 
ROLC 
Operand 
-4-
U r n -
RORC 
- • I 
Operand 
Figure 5.12 The four classes of shift instruction. 
s 
^B 
Rotate left through carry 
The most-significant bit is copied into the carry flag 
and the old C-bit copied into the least-significant bit. 
Rotate right through carry 
The least-significant bit is copied into the C flag 
and the old C-bit copied into the most-significant bit. 
change the contents of the register that specifies the number 
of shifts. The 68K lets you write LSL #4, DO to shift the con-
tents of data register DO left by four places or LSL Dl, DO to 
shift the contents of DO left by the number in Dl. 
Figure 5.12 also describes circular shifts or rotates. A circ-
ular shift operation treats the data being shifted as a ring with 
the most-significant bit adjacent to the least-significant bit. 
Circular shifts result in the most-significant bit being shifted 
into the least-significant bit position (left shift), or vice versa 
for a right shift. No data is lost during a circular shift. 
Consider the following examples. 
Shift type 
Before circular shift 
After circular shift 
Rotate left, ROL 
11001110 
10011101 
Rotate right, ROR 
11001110 
01100111 
The last pair of shift operations in Fig. 5.12 are called rotate 
though carry. These operations treat the carry bit as part of 
the shift operation. A circular shift is performed with the old 
carry bit being shifted into the register and the bit lost from 
the carry register being shifted into the carry bit. Suppose that 
the carry bit is currently 1 and that the 8-bit value 111 100002 
is to be shifted one place right through carry. The final result 

5.4 Overview of the 68K's instructions 
2 2 3 
is 111110002 and the carry bit is 0. A circular shift is a non-
destructive shift because no information is lost (bits don't fall 
off the end). 
The 68K's shift instructions are as follows. 
LSL The operand is shifted left by 0 to 31 places. The vacated 
bits at the least-significant end of the operand are filled with 
zeros. 
LSR The operand is shifted right 0 to 31 places. The vacated 
bits at the most-significant end of the operand are filled with 
zeros. 
ASL The arithmetic shift left is identical to the logical 
shift left. 
ROXR The operand is rotated by 0 to 31 places right. The bit 
shifted out of the least-significant end of the operand is 
shifted into the C-bit. The old value of the C-bit is copied into 
the most-significant end of the operand; that is, shifting takes 
place over 33 bits (i.e. the operand plus the C-bit). 
Shift operations can be used to multiply or divide a num-
ber by a power of two. They can be used for several other pur-
poses such as re-arranging binary patterns; for example, 
suppose register D2 contains the bit pattern Oaaaaxxxbbbb, 
and we wish to extract the xxx field (we're using 12-bit arith-
metic for simplicity). We could write 
LSR 
#4,D2 
;this will give us OOOOOaaaaxxx 
ASR The operand is shifted right 0 to 31 places. The vacated 
bits at the most-significant end of the operand are filled with 
zeros if the original operand was positive, or with Is if it was 
negative (i.e. the sign-bit is replicated). This divides a number 
by 2 for each place shifted. 
ROL The operand is rotated by 0 to 31 places left. The bit 
shifted out of the most-significant end is copied into the 
least-significant end of the operand. This shift preserves all 
bits. No bit is lost by the shifting. 
ROR The operand is rotated by 0 to 31 places right. The bit 
shifted out of the least-significant end is copied into the 
most-significant end of the operand. This shift preserves all 
bits. No bit is lost by the shifting. 
ROXL The operand is rotated by 0 to 31 
places left. The bit shifted out of the most 
significant end of the operand is shifted 
into the C-bit. The old value ofthe C-bit is 
Here 
ADD 
R0,D1 
copied into the least-significant end ofthe 
operand; that is, shifting takes place over 33 bits (i.e. the 
operand plus the C-bit). 
If we want to ensure that we just have the xxx field, we can use 
a logical AND to clear the other bits by 
AND 
#%111,D1 
; t h i s w i l l give us OOOOOOOOOxxx 
5.4.8 Branch instructions 
A branch instruction modifies the flow of control and causes 
the program to continue execution at the target address speci-
fied by the branch. The simplest branch instruction is the 
unconditional branch instruction, BRA 
target, which 
always forces a jump to the instruction at the target address. 
In the following fragment of code, the BRA Here instruction 
Here 
;jump to the line that begins "Here" 
forces the 68K to execute next the instruction on the line 
which is labeled by Here. 
1000 
Instruction 1 
1004 
Instruction 2 
1008 
Instruction 3 
100C 
Instruction 4 
1010 
Instruction 5 
1014 
Instruction 6 
1018 
Instruction 7 
101C 
Instruction 8 
' BRA 2000 
1020 
Instruction 9 
1024 
Instruction 10 
1028 
Instruction 11 
102C 
Instruction 12 
1030 
Instruction 13 
1034 
Instruction 14 
1038 
Instruction 15 
103C 
Instruction 16 
1040 
Instruction 17 I 
1044 
Instruction 18 
, I 
The branch instruction forces the 
instruction at 2000 to be executed 
next. 
>2000 
2004 
2008 
200c 
2010 
-BRA 1040 2014 
Instruction N 
Instruction A/+ 1 
Instruction N + 2 
Instruction N+ 3 
Instruction /V+4 
Instruction A/+ 5 
Figure 5.13 The unconditional branch. 
BRA 

224 
Chapter 5 The instruction set architecture 
1000 
1004 
1008 
100C 
1010 
1014 
1018 
Instruction 1 
Instruction 2 
Instruction 3 
Instruction 4 
Instruction 5 
Instruction 6 
Instruction 7 
1 o 1C <TnSrucrJon8 
False 
X^ 
Block 1 
1020 
Instruction 9 
1024 
Instruction 10 
1028 
Instruction 11 
102C 
Instruction 12 
1030 
Instruction 13 
1034 
Instruction 14 
1038 
Instruction 15 
103C 
Instruction 16 
^ r 
1040 
Instruction 17 
1044 
Instruction 18 
When instruction 8 is executed, 
the next instruction is at 1020 if 
the condition is false and 2000 if the 
condition is true. 
EEQ 
True 
Block 2 
2000 
Instruction N 
2004 
Instruction N+1 
2008 
Instruction N +2 
200c 
Instruction N+3 
2010 
Instruction N +4 
2014 
Instruction N+ 5 
T 
B 1040 
Instruction 8 is BEQ 2000 and is interpreted 
as 'branch to instruction N if die last result was 
zero, odierwise continue'. Consequendy, there 
is a fork at instruction 8 between the path if the 
last result was not zero and a path (to instruc-
tion N) if the result was zero. 
We can imagine that the test for zero leads 
to the execution of code block 1 or code block 
2. At the end of code block 2 a branch back to 
the main stream of instructions is made. 
Let's look at this conditional behavior in 
high-level language. Consider the following 
example of the high-level construct 
if 
(x 
3) then y 
We can translate this construct into the 
following 68K code. 
Figure 5.14 The conditional branch. 
CMP #3,D1 
BNE exit 
MOVE #4,D2 
exit . . . 
;<x == 3>? 
; if x is not 3 then leave 
;if x is 3 then y - 4 
Mnemonic 
Condition 
Flags 
BEQ 
equal 
Z = 1 
BNE 
not equal 
Z = 0 
BCS/BHS 
carry set/higher or same 
C = 1 
BCC/BLO 
carry clear/lower 
C = 0 
BMI 
negative 
N = 1 
BPL 
positive or zero 
N = 0 
BVS 
overflow set 
V = 1 
BVC 
overflow clear 
V = 0 
BHI 
higher than (signed) 
(C = 1).(Z = 0) 
BLS 
lower or same (signed) 
(C = 0)+(Z = 1) 
BGE 
greater than or equal (signed) 
N = V 
BLT 
less than (signed) 
N * V 
BGT 
greater than (signed) 
(Z = 0).( N = V) 
BLE 
less than or equal (signed) 
(Z= 1 ) + ( N * V ) 
Table 5.3 The 68K's conditional branches. 
Figure 5.13 demonstrates how an unconditional branch 
can modify the flow of control. In this example, execution 
continues sequentially from instruction 1 to instruction 8, 
which is BRA 2 000 (branch to instruction N at location 
200016). The address of the first instruction is 100016 and each 
instruction takes 4 bytes. Execution then continues with the 
instruction at location N. Instruction N + 5 is BRA 1040 
(branch to instruction 17 at location 104016) and a change of 
flow takes place again. 
The most important feature of any computer is its abil-
ity to implement conditional behavior by carrying out a 
test and then branching on the result of the text. 
Figure 5.14 demonstrates the flow of control with a condi-
tional branch. 
The instruction CMP #3 , Dl compares the 
contents of register Dl with the literal 3 by 
evaluating [Dl] — 3 and setting the status 
flags. If the result of the operation is zero, the 
Z-bit is set to 1. If the result is not zero (i.e. 
Dl does not contain 3), the Z-bit is set to 0. 
The key instruction is BNE exit, which 
means 'branch on not zero to the instruction 
labeled exit'. The effect of this instruction is 
to test the Z-bit of the status flags and then 
branch to the instruction with the label 'exit' 
if Z = 0 (i.e. Dl is not 3). If Dl is 3, Z = 1, 
the branch is not taken, and the MOVE #4, D2 
instruction is executed. 
The 68K provides 16 branch instructions of 
the form Bcc where CC defines the branch condition. These 
16 conditions, described in Table 5.3, are virtually die same 
as tiiose provided by many other microprocessors. We will see 
what the 4 bits in the first column mean later. 
Let's look at another application of conditional branching. 
You can implement a loop construct in the following way 
MOVE 
#20, DO 
Next 
SUB 
BNE 
#1,D0 
Next 
,-load the loop counter DO with 20 
;body of loop 
;decrement loop counter 
;repeat until loop count = zero 
Let's look at another example of the use of branching. 
Suppose A and B are two n-component vectors. As we have 
already stated, the inner product of A and B is the scalar value 
s = AB = a^b, + a2-b2 + a 3 b 3 . . . anbn. 
We 
can 
now 
write the code 

5.4 Overview of the 68K's instructions 
225 
Next 
CLR.L 
D6 
MOVE 
#24,D5 
MOVE.L 
#A,AO 
MOVE.L 
#B,A1 
MOVE 
(AO),D2 
ADD.L 
#2,A0 
MOVE 
(A1),D3 
ADD.L 
#2,A1 
MULU 
D2,D3 
ADD.L 
D3,D6 
SUB 
#1,D5 
BNE 
Next 
clear initial sum in D6 
load loop counter with n (assume 24 here) 
AO points at vector A 
Al points at vector B 
Repeat: get At and update pointer to A 
point to next element in A 
get Bi and update pointer to B 
point to next element in B 
Ai x Bi 
s = s + Ai x Bi 
decrement loop counter 
repeat n times 
Subroutine calls 
A subroutine is a piece of code that is called and executed and 
a return is made to the calling point. Subroutines are very 
important because they implement the function or procedure 
at the high-level language level. We look at subroutines in 
more detail in the next chapter. Here we are interested only in 
the principle of the subroutine call and return. 
Figure 5.15 demonstrates the subroutine call. Code is exe-
cuted sequentially until a subroutine call is encountered. The 
current place in the code sequence is saved and control is then 
transferred to the subroutine; that is, the first instruction in 
the subroutine is executed and the processor continues exe-
cuting instructions in the subroutine until a return instruc-
tion is encountered. Then, control is transferred back to the 
point immediately after the subroutine call by retrieving the 
saved return address. 
Figure 5.16 illustrates this concept with a simple subroutine 
called ABC that calculates the value of Ix2 (where x is a 16-bit 
value passed in DO). This subroutine is called by the instruction 
BSR ABC (branch to subroutine), and a return from subroutine 
is made by an RTS (return from subroutine) instruction. 
Figure 5.17 displays the program of Fig. 5.16 in the form of 
a memory map and demonstrates the flow of control 
between the calling program and the subroutine ABC. 
Figure 5.18 extends the example in Fig. 5.15 by demon-
strating a multiple call. We have used the instruction 
BSR ABC to implement the subroutine 
call. The main body of the code calls 
subroutine ABC. At the end of the sub-
routine, a return instruction makes a 
return to the instruction following the 
calling point. As you can see, the subrou-
tine is called from two different places 
and yet a return is made to the correct 
point in each case. 
In chapter 6 we look at how assembly 
language programs are constructed. We 
will also look at a data structure called 
the stack, which is used to hold subrou-
tine return addresses. 
Code 
Subroutine 
Figure 5.15 The subroutine call. 
MOVE.W #4,DO-
BSR 
ABC • 
. Set up a parameter in DO 
• The subroutine call 
Return point 
(next instruction after 
the subroutine) 
ABC 
MULU 
DO, DO 
ASL.L 
RTS * -
#1,D0 
— The subroutine 
• Return from subroutine 
Figure 5.16 The subroutine. 
Body of the code 
Subroutine 
Return 
Figure 5.17 Memory map of a subroutine call and flow of control. 
Call 
"\Return 
MOVE.W 
# 4 , DO 
BSR 
ABC 
—~• 
ABC MULU 
DO,DO 
ASL.L 
#l,DO 
~~~""~- RTS 

226 
Chapter 5 The instruction set architecture 
Figure 5.18 Multiple subroutine calls. 
II SUMMARY 
We have introduced the CPU and its native language, the 
assembly language, which is a human-readable representation 
of machine code. Unfortunately, assembly languages are not 
portable; each family of microprocessors has its own unique 
assembly language that is incompatible with any other 
processor. You can run a C program on most computers with a C 
compiler.A program written in Pentium assembly language will 
run only on machines with a Pentium at their core. 
We introduced the concept of an architecture, the assembly 
language programmer's view of the computer in terms of its 
functionality rather than performance or implementation. To 
illustrate the characteristics of an architecture we selected the 
elegant 68K processor, which is, paradoxically, simpler than 
many of its contemporaries while, at the same time, 
incorporating a number of sophisticated facilities such as the 
ability to shift an operand as part of a data processing 
instruction and the ability to execute an instruction only if 
certain conditions are met (predication). 
An architecture consists of a set of instructions, a set of 
resources (the registers), and the addressing modes used to 
access data. 
In this chapter we have laid the foundations for the next 
chapter where we look at how programs can be constructed to 
run on the instruction set architecture we introduced here. 
M PROBLEMS 
5.1 What's the difference between an assembly language and 
machine code? In order to answer this question fully, you should 
use the Internet to find out more about assemblers. 
5.2 Books and articles on the computer make a clear distinction 
between architecture and organization. Do you think that this is a 
useful distinction? Can you think of other areas (i.e. non-computer 
example) where such a distinction would be appropriate? 
5.3 What are the advantages and disadvantages of dividing a 
computer's registers into data and address registers like the 68K? 
5.4 What are the relative advantages and disadvantages of one-
address, two-address, and three-address instruction formats? 
5.5 What's a register-to-register 
architecture and why is such an 
architecture also called a load and store 
computer? 
5.6 What are the three fundamental 
addressing modes? Are they all 
necessary? What is the minimum number 
of addressing modes required? Can you 
think of other possible addressing 
modes? 
5.7 The 68K has two add instructions: 
ADD and ADC. What is the difference 
between them? If the processor lacked 
an ADC instruction, how would you 
synthesize it (i.e. what other instructions would you use to 
achieve the same effect)? 
5.8 The 68K has an exchange registerpa/r instruction, EXG. 
Why would you want such an instruction? 
5.9 The SWAP Di instruction swaps the upper- and lower-order 
words of data register Di. Why would you want such an 
instruction? If the 68K's instruction set lacked a SWAP, how 
would you swap the two halves of a data register? 
5.10 Why are so many variations on a shift operation provided 
by the 68K and many other processors? 
5.11 What is the largest memory space (i.e. program) that can 
be addressed by processors with the following number of 
address bits? 
(a) 12 bits 
(b) 16 bits 
(c) 24 bits 
(d) 32 bits 
(e) 48 bits 
(f) 64 bits 
5.12 The von Neumann stored program computer locates 
program and data in the same memory. What are the 
advantages and disadvantages of a system with a combined 
program and data memory? 
5.13 The gear lever is part of an automobile's organization 
rather than its architecture. Are the brakes part of a car's 
architecture or organization? 
5.14 What does the RTL expression [ 1 0 0 ] < - [ 5 0 ] + 2 mean? 
5.15 What does the RTL expression [ 1 0 0 ] < - [ 5 0 + 2] + 2 
mean? 
5.16 What is an operand? 
5.17 In the context of an instruction register, what is a field? 
5.18 What is a literal operand? 
5.19 What is the effect on the C-.V-, Z-, and N-bits when the 
following 8-bit operations are carried out? 
(a) 01110001 
(b) 11110101 
(c) 10100011 
+01111001 
+11000111 
+01011101 
5.20 Some microprocessors have one general-purpose data 
register, some two, some eight, and so on. What determines the 
BSR 
ABC 
F,v5* „ „ . 
Subroutine 
N 
*
^
^
^ 
BSR 
ABC - " seco^0 ^ 
^^______ 
• 
M 
.. 
i
^
^
^
6 
' 
Return 
Subroutine 
N 

5.4 Overview of the 68K's instructions 
227 
number of such general-purpose data registers in any given 
computer? 
5.21 What is the difference between a dedicated and a general-
purpose computer? 
5.22 What is a subroutine and how is it used? 
5.23 What is the so-called von Neumann bottleneck? 
5.24 For the following memory map explain the meaning of the 
following RTL expressions in plain English. 
(a) 
(b) 
(c) 
(d) 
(e) 
(f) 
1000 
I00I 
1002 
1003 
1004 
1005 
[1000] 
[1003] 
[1004] 
[1000] 
[1001] 
= 120 
= [1001] 
<- 5 
<- [1005] 
<- [1002] 
+ 1 
+ 
[1003] 
[1000] 
<- [1003 + 1] 
120 
1003 
8 
1004 
0 
23 
5.25 Suppose a problem in a high-school algebra text says 'Let 
x= 5'. What exactly is x? Answer this question from the point 
of view of a computer scientist. 
5.26 In the context of a CPU, what is the difference between a 
data path and an address path? 
5.27 Why is the program counter a pointer and not a counter? 
5.28 What's the difference between a memory location and a 
data register? 
5.29 Does a computer need data registers? 
5.30 Some machines have a one-address format, some a 
two-address format, and some a three-address format; for 
example, 
ADD PI 
ADD P1,P2 
ADD P 1 , P 2 , P 3 
5.31 What is the difference between the C, Z, V, and N flags in a 
computer's status register (or condition code register)? 
5.32 What is the difference between machine code and 
assembly language? 
5.33 What is the advantage of a computer with many registers 
over one with few registers? 
5.34 Translate the following algorithm into assembly language. 
IF X > 12 THEN X = 2*X + 4 ELSE X = X + Y 
5.35 For the memory map below, evaluate the following 
expressions, where [N] means the contents of the memory 
location whose address is N. All addresses and their contents are 
decimal values. 
(a) [7] 
(b) [[[4]] 
(c) [[[0}]]] 
(d) [2 + 10] 
(e) [[9] + 2] 
00 
12 
01 
17 
02 
7 
03 
4 
04 
8 
05 
4 
06 
4 
07 
6 
08 
0 
09 
5 
10 
12 
11 
12 
_ 7 
6 
13 
3 
14 
2 
(f) [[9]+ [2]] 
(g)[[5] + [13] + 2*[14]] 
(h) [0)]'3 + [1]*4 
(i) [9]* [10] 
What are the relative merits of each of these instruction formats? 
5.36 The most frequently executed class of instruction is the 
data move instruction. Why is this? 

Assembly language programming 
INTRODUCTION 
We introduced the processor and its machine-level language via the 68K CISC processor in the previous 
chapter. Now we demonstrate how 68K assembly language programs are written and debugged. 
Because assembly language programming is a practical activity, we provide a 68K cross-assembler 
and simulator with this book. Previous editions of this book used the DOS-based Teesside simulator. 
In this edition we use a more modern Windows-based system called Easy68K.We provide a copy of 
the EASy68K simulator on the CD accompanying this book, as well as a copy of the Teesside 
simulator and its documentation for those who wish to maintain compatibility with earlier editions. 
Both simulators run on a PC and allow you to execute 68K programs. You can execute a program 
instruction by instruction and observe the effect of each instruction on memory and registers as it 
is executed. 
6.1 Structure of a 68K assembly 
language program 
Figure 6.1 provides the listing of a simple assembly language 
program written to run on the 68K cross-assembler. This 
program implements the high-level language operation 
R = P + Q (where variables P = 2 and Q = 4). Few rules gov-
ern the layout of a 68K assembly language program. The left-
most column is reserved for user-defined, labels—in this case, 
P, Q, and R. If a line begins with an asterisk in the first col-
umn, the assembler ignores the rest of the line. You put an 
asterisk in column 1 to create a comment. Another rule is that 
the mnemonic and its operand must be separated by at least 
one space and that no embedded spaces may be located 
within either the mnemonic or operand fields. 
Recall that numbers prefixed by the $ symbol are hexadec-
imal, whereas numbers prefixed by % indicate that the fol-
lowing number is expressed in binary form; for example, tiie 
following three instructions 
MOVE 25,DO 
load DO with 25i0 
MOVE $19,DO 
load DO with 2510 
MOVE %11001,D0 
load DO with 2510 
are equivalent. The assembler translates each of these into 
exactly the same machine code. 
CHAPTER MAP 
5 The instruction set 
architecture 
' Chapter 5 introduces the 
computer's instruction set 
architecture, which defines the 
tow-level programmer's view of 
the computer and describes the 
type of operations a computer, 
carries out. We are interested in 
three aspects of the ISA: the 
nature of the instructions, the 
resources used by the 
instructions (registers and 
memory), and the way in which 
the instructions access data 
(addressing modes). 
6 Assembly language 
programming 
Having introduced the basic 
operations that a computer can 
carry out, the next step is to 
show how instructions are used 
to construct entire programs. We 
introduce the 68K's 
programming environment via a 
simulator that runs on a PC and 
demonstrate how to implement 
some basic algorithms. 
7 Structure of the CPU 
Now we know vvftat a computer 
does, the next step is to show • 
. /low it operates, fn Chapter 7 we 
examine the internal 
• organization of a computer and 
' demonstrate how it reads, 
instructions from memory, 
decodes them, and executes ' 
•' them: • 
8 Other processors •..: 
We have used the 68K to 
introduce the CPU and assembly 
language programming. Here we • 
provide a brief overview of some • 
of the features of other ••.. 
processors. 

6.1 Structure of a 68K assembly language program 229 
THE 68K ARCHITECTURE REVIEW 
The 68K has 16 general-purpose 32-bit user accessible 
registers. DO to D7 are data registers and AO to A7 are address 
registers. An address register holds a pointer and is used in 
address register indirect addressing. The only instructions that 
can be applied to the contents of address registers are add, 
subtract, move, and compare. Operations on the contents of 
an address register always yield a 32-bit value whereas 
operations on data registers can be 8,16, or 32 bits. 
Most 68K instructions are register to register, register to 
memory, or memory to register. The following defines some 
68K instructions. 
MOVE.B 
D0,D1 
Copy the low-order byte in DO to D1(0:7). 
MOVE.W 
ABC,D1 
Copy the 16-bit word in memory location ABC to D1(0:l5). 
MOVE.L 
DO, (AO) 
Copy the 32-bit value in DO to the memory location pointed at by address register AO. 
ADD . B 
D l , ABC 
Add the low-order byte in register D1 to the contents of memory location ABC. 
S U B . B 
#4, (A3) 
Subtract 4 from the contents of the byte-wide memory location pointed at by A3. 
ADD. L 
A l , D5 
Add the 32-bit contents of address register A1 to data register D5. 
The # symbol indicates the immediate addressing mode; that is, the operand is a literal value. 
Figure 6.1 Structure of an assembly 
language program. 
Figure 6.2 shows the structure of a typical 68K instruction. 
Instructions with two operands are always written in the 
form source, destination, where source is where the 
operand comes from and d e s t i n a t i o n is where the result 
goes to. 
6.1.1 Assembler directives 
Assembly language statements are divided into executable 
instructions and assembler directives. An executable instruction 
is translated into the machine code of the target microproces-
sor and executed when the program is loaded into memory. In 
the example in Fig. 6.1, the executable instructions are 
We've already encountered the first three instructions. The 
last instruction, STOP #$2700, terminates the program by 
halting further instruction execution. This instruction also 
loads the 68K's status register with the value 270016, a special 
code that initializes the 68K. We use this STOP instruction to 
terminate programs running on the simulator. 
An assembler directive tells the assembler something it needs 
to know about the program; for example, the assembler direc-
tive ORG means origin and tells the assembler where instruc-
tions or data are to be loaded in memory. The expression 
ORG $ 10 0 0 tells the assembler to load instructions in memory 
MOVE P,D0 
Copy contents of P to DO 
ADD Q,D0 Add contents of Q to DO 
MOVE D0,R 
Store contents of DO in memory location R 
STOP #$2700 Stop executing instructions1 
Remember that in the instruction STOP 
#$2700 the operand 
is #$2700. The '#' indicates a literal operand and 
the '$' indicates hexadecimal. The literal operand 
00100111000000002 is loaded into the 68K's status 
register after it stops. The 68K remains stopped until it it 
receives an interrupt. 
.ORG 
$ 1 0 0 0 
^ _ _ 6 8 K machine 
An asterisk in 
fri^TE? 
H~i^>*-—-—""""^ 
l e v e l i n s t m c t l 0 n 
the first column 
/ ( g O V E 
P , D Q ) * - ^ _ 
^
^ 
indicates the line 
/ C A D D ) 
Q f O O ^ _ 
operand 
is a comment 
/ 
\ 
^ r „ ~~x 
——___ 
\ 
/ 
M O V E \ C D 0 _ , _ R j ) 
^ ^ D e s t i n a t i o n 
\ ^ 
/ 
STOP 
\ # $ 2 7 00 \ . 
operand 
* This i s the en\l of t h e cede area 
/ 
<ORG 
$2^00 
\ „ . . 
T 
\ 
-This \s an operand 
P 
/ 
/4 D C . W 
2 
\ 
that describes the 
Q / 
// 
D C . W 
4 
\ 
d a t a t o t h e C P U 
XR/ 
//Arf'-'SuWj 
1 
This is a mnemonic 
/"-'I 
/// 
/ 
r.»ir^^ 
t 1nnn 
that describes the 
/ 
/ / / / / 
' \ $ 1 0 0 0 
operation t 0 b e 
, / 
/ ///// 
^ x 
carried out 
Label 
/ / / / / 
A. W suffix indicates that 
Assembler directives 
the data size is 16 bits 
(these directives tell the assembler 
' 
things it needs to perform its job) 
I 
^This is an operand 
that describes the 
data to the CPU 
This is a mnemonic 
. „ 
that describes the 
operation to be 
carried out 
A . W suffix indicates that 
the data size is 16 bits 

230 
Chapter 6 Assembly language programming 
; A 
.Edge of screen/paper (column 1) 
The fields of an instruction are 
separated by at least one space 
The first operand, P, defines the 
source of the data used by the instruction. 
In this case the source is the contents 
of a memory location 
istart 
t 
Anything beginning in 
column 1 is a user-supplied 
label that allows the 
programmer to refer to 
this line 
MOVE.B 
This is a mnemonic 
that indicates the 
instruction to be 
carried out 
Figure 6.2 Anatomy of an assembly language instruction. 
P,D0 
GetP 
DO is the destination 
operand (in this case 
a data register) 
Anything following the 
instruction is treated as a 
comment and ignored 
by the assembler 
Many instruction are terminated by 
. B . W or . L. These suffixes define 
the size of the operand (byte, 
world, or longword) 
starting at address 100016. We've used the value 100016 because 
the 68K reserves memory locations 0 to 3FF16 for a special pur-
pose and 1000 is an easy number to remember. 
The second origin assembler directive, ORG $2000, is 
located after the code and defines the starting point of the 
data area. We don't need this assembler directive; without it 
data would immediately follow the code. WeVe used it 
because it's easy to remember that the data starts at memory 
location $2000. 
An important role of assembler directives is in reserving 
memory space for variables, presetting variables to initial val-
ues, and binding variables to symbolic names. Languages like C 
call these operations declaring variables. We will be performing 
assembly level actions similar to the following C declarations. 
Figure 6.3 demonstrates what's happening when the 68K 
program in Fig. 6.1 is assembled by looking at the output pro-
duced by EASy68K. This listing has seven columns. The first 
column is a 32-bit value expressed in hexadecimal form, 
which contains the current memory address in which 
instructions or data will be loaded. The next two columns are 
the hexadecimal values of instructions or data loaded into the 
current memory location. These are the values produced by 
the assembler from instructions, addresses, and data in the 
assembly language program. The fourth column contains the 
line number that makes it easy to locate a particular line in 
the program. The remaining right-hand columns in Fig. 6.3 
are the instructions or assembler directives themselves 
followed by any comment field. 
int • • time; •;' 
i n t 
x , . y ; 
' . • • • ' • • ' 
.int ' 23 := ;42; •' -
':char:/tefin.-= ;*/@' ';•• 
,/t- declare integer "time" -.' 
/. */ 
/* declare two integers- x and y 
*/ 
/* declare an integer z3 and assign the value 42 */ 
•/•*,,-declare' • a--character-.with the'initial/value'
 v@'. *"/•• 
In this fragment of code the operation i n t z3 =42; 
reserves a 16-bit memory location for the variable called z3 
and then stores the binary equivalent of 42,0 in that location. 
Whenever you use the variable z3 in the program, the com-
piler will automatically select its appropriate address in 
memory. All this is invisible to the programmer. The follow-
ing demonstrates the relationship between 68K assembler 
directives and the C code. 
As you can see, the instruction MOVE DO,R is located 
on line 12 and is stored in memory location 100C16. 
This instruction is translated into the machine code 
33C000002004,6, where the operation code is 33C016 and the 
address of operand R is 00002004J6. 
The symbol table below the program relates symbolic 
names to their value. This information is useful when you are 
debugging a program; for example, you can see that variable 
P has the address 2000. 
The assembler maintains a variable called the location 
counter, which keeps track of where the next instruction or data 
element is to be located in memory. When you write an ORG 
directive, you preset the value of the location counter to that 
specified; for example, ORG $ 12 3 4 means load the following 
instruction or data into memory at location 1234I6. Let's look as 
some of the other assembler directives in this program. 
68K assembler directive 
C code equivalent 
Time ' 
x-- •' 
Y-
2 3 . •'•• 
: Term' 
DS.W' 
•DS.W 
-DS.W• 
DC', W -
: DC. B 
- 1 
'' 1 
" -1 ' -
-42 
int 
':,'iht: ' 
• ' • • • i ' r i t ••;.'•' 
"v.cha'r' 
.'.time; ,;. 
.x/...y; . 
• z ' 3 : - . = 
4
2
; 
•/••••' 
• ; t e r m . = ;...,@'; 

6.1 Structure of a 68K assembly language program 
231 
0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 
0 0 0 0 1 0 0 0 
0 0 0 0 1 0 0 0 
00001000 
3039 
00001006 
D079 
0000100C 
33C0 
00001012 
4E72 
00001016 
0 0 0 0 2 0 0 0 
0 0 0 0 2 0 0 0 
01 
000H2Ii02 
02 
^ 0 0 02o'o4^L. 
OCTo"TT2TT06 ^ S . 
No errors detected 
No warnings generate^ 
00002000 
00002002 
00002004 
2700 
SYMBOL TABLE INFORMATION 
Symbol-name 
V a l u e 
P 
2000 
Q 
2002 
R 
2004 
* Program Number 
* Written by 
* Date Created 
* D e s c r i p t i o n 
OUP1 
A. Clements 
10 October 2003 
Use of assembler 
d i r e c t i v e s 
8 
ORG 
$io"o~cT~——• 
9 
1 0 
MOVE 
P , D 0 
1 1 
ADD 
Q , D 0 
12 
MOVE 
D 0 , R 
13 
STOP 
# $ 2 7 0 0 
14 
15 
ORG 
$ 2 0 0 0 
16 
P 
DC.W 
2 
17 
Q 
DC.W 
4 
18 
R 
DS .W 
1 
1 9 
END 
$ 1 0 0 0 
The code and data 
Address in memory 
The line number 
The source code 
The symbol table relates symbolic 
names to their addresses 
Figure 6.3 Assembling a program. 
The define constant assembler directive DC loads a constant 
in memory before the program is executed; that is, it provides a 
means of presetting memory locations with data before a 
program runs. This directive is written DC. B to store a byte, 
DC. W to store a word, and DC. L to store a longword. In the 
program of Fig. 6.3, the assembler directive P DC. W 2 places 
the value 2 in memory and labels this location 'P'. Because 
this directive is located immediately after the ORG $2000 
assembler directive, the integer 2 is located at memory loca-
tion 200016. This memory location (i.e. 200016) can be 
referred to as P. When you wish to read the value of P (i.e. the 
contents of memory location 200016), you use P as a source 
operand; for example, MOVE P,D0. Because the size of the 
operand is a word, the value 00000000000000102 is stored in 
location 200016. Figure 6.4 demonstrates the effect of this 
assembler directive. 
The next assembler directive, Q DC. W 4, loads the con-
stant 4 in the next available location—200216. Why 2002i6 
and not 200116? Because the operands are word sized (i.e. 16 
bits) and the 68K's memory is byte addressed. Each word 
occupies two bytes—P takes up 2000i6 and 2001lf). 
The define storage directive (DS) tells the assembler to 
reserve memory space and also takes a . B, . w, or . h qualifier. 
For example, R DC. w 1 tells the assembler to reserve a word 
in memory and to equate the name of the word with 'R'. The 
difference between DC. B N and DS. B N is that the former 
stores the 8-bit value N in memory, whereas the latter reserves 
N bytes of memory by advancing the location counter by N. 
The final assembler directive, END $ 10 0 0, tells the assem-
bler that the end of the program has been reached and that 
there's nothing else left to assemble. The parameter taken by 
the END directive is the address of the first instruction of the 
program to be executed. In this case, execution begins with 
the instruction at address 100016. 
The assembler directive EQU equates a symbolic name to a 
numeric value. If you write Tuesday EQU 3, you can use the 
symbolic name 'Tuesday' instead of its actual value, 3. For 
example, ADD #Tuesday,DO is identical to ADD #3,DO. 
1 
2 
3 
4 
5 
6 5 
7 

232 
Chapter 6 Assembly language programming 
We now provide another example of the use of assembler 
directives. 
Half the fun in writing assembly language programs is run-
ning and debugging them. In this chapter we will be using the 
2000 
2002 
2004 
Memory 
2000 
2002 
2004 
Memory 
$0002 
2000 
EASy68K simulator, which allows you to cross-assemble a 68K 
program on a PC and then execute it on a PC. The PC simu-
lates the behavior of a 68K processor and the basic operating 
system function required to perform simple input and output 
activities such as reading from the 
keyboard and writing to the screen. 
Figure 6.5 gives a screen dump of a 
session with the simulator. 
Memory 
$0002 
(a) ORG $2000 sets 
the location counter 
to 2000. 
(b)P DC.W 2 
puts $0002 in the 
current location 
and moves the 
location counter to 
the next free location. 
(c) Location $2000 
has the symbolic 
value P. Using 
'P' in the program 
is the same as using 
$2000. 
Figure 6.4 The effect of a define constant assembly directive. 
v^mt^mtmmmimmmmssmKm 
0e B^f tit™ 2ptK»ss l&fc 
BioTtal -.-I- 'lal <B| 
6.1.2 Usingthe 
cross-assembler 
The following 68K assembly language 
program illustrates what an assembler 
does. This program is designed only to 
demonstrate the use of assembler direc-
tives; it does not perform any useful 
Registers' 
T S IHT 
XTOUC 
Cycles.. 
O»-|O00O0005 M-J00OO000O »B-|O0000S02 •"•"(OOOOOOOO" 
M-fo3o5Ho7r os-foooooobV «i-[oo5S55o" «s«|SBHo5oo" ^"f 5" 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 
B2=[0O0Q0000 B4«|o0000000 <t2-|00000000 «6-|O0000000 l*C»p[)00040E 
ra«[oooooooo D7-|oooooooo *3»joooooooo OT-joooooooo~ ss-joioooooo 
126 
QesjCyc&ss 
<l I 
00000400 Start 3 
ng Address 
A 
Assembler used EA5y68K Editor vl 9 2 
treated On 10-Oct-03 9 54 37 PM 
00000000 
oooooooo 
1 
2 
00000000 
oooooooo 
1 
2 * Program Number 
00000000 
3 * Written by 
oooooooo 
4 * Date Created 
oooooooo 
5 * Description 
oooooooo 
6 * 
oooooooo 
00000400 
oooooooo 
00000400 
8 
ORG 
S000400 
Define the origin for data 
00000400 41FA 00FE 
9 
TEA 
BUFFER (PC) A0 Preset A0 as a pointer register 
00000404 6100 0022 
10 KEXTIH 
BSR 
GET CHAR 
Get a character 
00000408 10C1 
11 
HOVE B D1.(A0)+ 
Store character and move pointer to next 
0000040A B23C 
41FA 
0040 
00EE 
12 
14 PRINT 
CMP B 
LEA 
#'9\D1 
BUFFER(PC) 
IF character = '#' THEN print 
00000410 
B23C 
41FA 
0040 
00EE 
12 
14 PRINT 
CMP B 
LEA 
#'9\D1 
BUFFER(PC) A0 Reset pointer to start of buffer 
00000414 1218 
15 KEXTOUT MOVE B (A0J+.D1 
Get a character and update pointer 
00000416 B23C 0040 
16 
CMP B *'»'.D1 
IF character - '9' THEN EXIT 
0000041A 6700 0008 
17 
BEQ 
DONE 
0000041E 6100 0010 
18 
BSR 
PUT CHAR 
ELSE print character 
00000422 60F0 
19 
BRA 
MEXTOUT 
Repeat 
00000424 4E72 2700 
20 DONE 
STOP 
#62700 
Halt the 68K 
00000428 
21 * 
00000428 103C 0005 
22 GET CHAR MOVE B #5, DO 
Input routine 
0000042C 4E4F 
23 
TRAP 
*15 
Load input command in DO and call O/S 
0000042E 4E75 
24 
RTS 
Return 
00000430 
25 * 
00000430 103C 0006 
26 POT CHAK MOVE B #6. DO 
Output routine 
00000434 4E4F 
27 
TRAP 
#15 
Load output command in DO and call O/S 
00000436 4E75 
28 
RTS 
Return 
00000438 
29 * 
00000500 
30 
ORG 
S500 
00000500 
31 BUFFER 
DS.B 
40 
Reserve 40 bytes of storage 
00000528 
32 
END 
S400 
lio errors detected 
Ho earnings generated 
SYMBOL TABLE INFORMATION 
Symbol-name 
Value 
BUFFER 
500 
DOME 
424 
GET CHAR 
428 
J 
2i. 
Figure 6.5 Output from the Easy68K simulator. 

6.1 Structure of a 68K assembly language program 233 
Dummy equates 
Save two words of storage 
Put the longword $12345678 in memory 
Put an ASCII string in memory 
Store a 32-bit constant 
Put 4 in memory 
Start of code 
Pick up a character 
Test for end of string 
And exit on terminator 
Print a character 
Repeat 
Halt the 68K 
Dummy subroutine 
Return 
END needed ("Begin" is start of code) 
Test 
Alan 
XXX 
YYY 
Name 
Begin 
Next 
Exit 
* 
Print 
* 
ORG 
$400 
EQU 
6 
EQU 
7 
DS.W 
2 
DC.L 
$12345678 
DC.B 
'Clements' 
DC.B 
$FF 
DC.L 
Test+Name 
DC.B 
4 
MOVE.L #Name,A0 
MOVE.B (A0)+,D0 
CMP.B #$FF,D0 
BEQ 
Exit 
BSR 
Print 
BRA 
Next 
STOP 
#$2700 
NOP 
NOP 
RTS 
END 
Begin 
computation. The source program is followed by its assem-
bled listing file. Examine both the source code and listing file, 
and try to follow what is happening. 
The following listing file was produced by a cross-assembler 
from the above source code. 
1 00000400 
ORG 
$400 
2 
00000006 
TEST: EQU 
6 
;Dummy equates 
3 
00000007 
ALAN: EQU 
7 
4 00000400 00000004 
XXX: 
DS.W 
2 
;Save two words of storage 
5 00000404 12345678 
YYY: 
DC.L 
$12345678 
;Put the longword $12345678 in memory 
6 00000408 436C656D656E NAME: DC.B 
'Clements' 
;Put an ASCII string in memory 
7473 
7 00000410 FF 
DC.B 
$FF 
8 00000412 0000040E 
DC.L 
TEST+NAME 
/Store a 32-bit constant 
9 00000416 04 
DC.B 
4 
;Put 4 in memory 
10 00000418 207C00000408 BEGIN: MOVE.L #NAME,A0 
11 0000041E 1018 
NEXT: MOVE.B (A0)+,D0 
;Pick up a character 
12 00000420 0C0000FF 
CMP.B #$FF,D0 
/Test for end of string 
13 00000424 67000008 
BEQ 
EXIT 
/And exit on terminator 
14 00000428 61000008 
BSR 
PRINT 
/Print a character 
15 0000042C 60F0 
BRA 
NEXT 
/Repeat 
16 0000042E 4E722700 
EXIT: STOP 
#$2700 
/Halt the 68K 
17 
* 
18 00000432 4E71 
PRINT: NOP 
/Dummy subroutine 
19 00000434 4E71 
NOP 
20 00000436 4E75 
RTS 
/Return 
21 
22 
00000418 
END 
BEGIN 
/END needed ("Begin" is start of code) 
Lines: 22, Errors: 0, Warnings: 0. 
The first column provides the line number. The second 
used as a label and refers to the location in memory of the 
column defines the location in memory into which data and 
code on this line. This address is $0408. Therefore, the 
instructions go. The third column contains the instructions 
constant to be stored is 6 + $0408 = $040E. You can see that 
and the constants generated by the assembler. The remainder 
this really is the value stored from column 3 in line 8. Note 
is the original assembly language program. Consider line 8. 
that line 10 has the location $0418 and not $0417 because all 
8 
00000412 0000040E 
DC.L 
TEST+NAME 
/Store a 32-bit constant 
The constant stored in memory is TEST+NAME. In line 2 
TEST was equated to 6 (i.e. the assembler automatically sub- 
word and longword addresses must be even. The following 
stitutes 6 for TEST). But what is 'NAME'? On line 6, NAME is 
notes will help you understand the assembly process. 

234 
Chapter 6 Assembly language programming 
The simulator system requires an ORG statement at the 
beginning of the program to define the point at which code is 
loaded into the simulated memory. 
You can halt a 68K by executing the STOP #data instruc-
tion, which stops the 68K and loads the 16-bit value data 
into its status register. By convention we use the constant 
$2700 (this puts the processor in the supervisor mode, turns 
off interrupt requests, and clears the condition code flags). 
Operations on address registers always yield longword results 
because an address register holds a 32-bit pointer. A . W opera-
tion is permitted on an address register but the result is treated 
as a two's complement value and sign-extended to 32 bits. 
Because the END address assembler directive terminates 
the assembly process, no instructions beyond END point are 
assembled. 
6.2 The 68K's registers 
The 68K has a byte-addressable architecture. Successive bytes 
are stored at consecutive byte addresses 0,1, 2, 3 . . . , succes-
sive words are stored at consecutive even addresses 0, 2, 4, 
6 , . . . , and successive 32-bit longwords are stored at addresses 
BEGINNER'S ERRORS 
1. Embedded data 
You should not locate data in the middle of a section of code. 
The microprocessor executes instructions sequentially and will 
regard embedded data as instructions. Put data between the 
end of the executable instructions of a program and the END 
assembler directive as the following demonstrates. 
3. Subroutine call 
You call a subroutine with a BSR or a JSR instruction. This is 
the only way you call a subroutine. You cannot call a 
subroutine with a conditional branch (e.g. BEQ, BNE, BCC, 
etc.). 
The last instruction 
This stops the 68K dead in its tracks 
MOVE. B D3 , D4 
STOP 
#$2700 
Datal 
DC.B 
'This is data' 
Test 
DS.B 
4 
Save 4 bytes of storage 
END 
$400 
The END directive is that last item in a program 
The only way that you can locate data in the middle of a pro-
gram is by jumping past it like this: 
MOVE.B D3 , D4 
BRA 
Datal 
DC. B 
Test 
DS.B 
Continue ADD.B 
Continue 
'This is data' 
4 
#1,D6 
An instruction 
Jump past the data 
Put the string here 
Save 4 bytes of storage 
Back to the instructions 
Although this code is legal, it is not good practice (for the 
beginner) to mix code and data. 
2. Initialization 
I saw a program beginning with the operation 
MOVE . B (A0) , DO which loads DO with the byte pointed at 
by address register AO. It failed because the student had not 
defined an initial value of AO. You have to set up AO before you 
can use it or any other variable by, for example. 
4. Misplaced END directives 
The END directive indicates the end of the program. No 
instruction or assembler directive may be placed after the END 
directive. The END must be followed by the address of the first 
instruction to be executed. 
Table 
LEA 
Table, A0 
MOVE.B 
(A0),D0 
DC.B 
1,2,3,7,2 
A0 points to "Table" 
Pick up a byte from Table 
Here's the table 

6.2 The 68K's registers 
235 
8 bits 
16 bits 
32 bits 
1000 
1002 
1004 
1006 
1008 
100A 
100C 
100E 
1010 
1012 
1000 
1004 
1008 
100C 
1010 
1014 
1018 
101C 
1020 
1024 
*~ 
1000 
1002 
1004 
1006 
1008 
100A 
100C 
100E 
1010 
1012 
( 
I 
~1 
1000 
1004 
1008 
100C 
1010 
1014 
1018 
101C 
1020 
1024 
" 
| 
I 
1000 
1000 
1002 
1004 
1006 
1008 
100A 
100C 
100E 
1010 
1012 
1000 
! 
1001 
1000 
1004 
1008 
100C 
1010 
1014 
1018 
101C 
1020 
1024 
1000 
! 
1001 
j 
1002 
1003 
1001 
1000 
1002 
1004 
1006 
1008 
100A 
100C 
100E 
1010 
1012 
1002 
| 
1003 
1000 
1004 
1008 
100C 
1010 
1014 
1018 
101C 
1020 
1024 
1004 
! 
1005 
I 
1006 
1007 
1002 
1000 
1002 
1004 
1006 
1008 
100A 
100C 
100E 
1010 
1012 
1000 
1004 
1008 
100C 
1010 
1014 
1018 
101C 
1020 
1024 
1003 
1000 
1002 
1004 
1006 
1008 
100A 
100C 
100E 
1010 
1012 
1000 
1004 
1008 
100C 
1010 
1014 
1018 
101C 
1020 
1024 
1004 
1000 
1002 
1004 
1006 
1008 
100A 
100C 
100E 
1010 
1012 
1000 
1004 
1008 
100C 
1010 
1014 
1018 
101C 
1020 
1024 
1005 
1000 
1002 
1004 
1006 
1008 
100A 
100C 
100E 
1010 
1012 
1000 
1004 
1008 
100C 
1010 
1014 
1018 
101C 
1020 
1024 
1006 
1000 
1002 
1004 
1006 
1008 
100A 
100C 
100E 
1010 
1012 
1000 
1004 
1008 
100C 
1010 
1014 
1018 
101C 
1020 
1024 
1007 
1000 
1002 
1004 
1006 
1008 
100A 
100C 
100E 
1010 
1012 
1000 
1004 
1008 
100C 
1010 
1014 
1018 
101C 
1020 
1024 
1008 
1000 
1002 
1004 
1006 
1008 
100A 
100C 
100E 
1010 
1012 
1000 
1004 
1008 
100C 
1010 
1014 
1018 
101C 
1020 
1024 
1009 
1000 
1002 
1004 
1006 
1008 
100A 
100C 
100E 
1010 
1012 
1000 
1004 
1008 
100C 
1010 
1014 
1018 
101C 
1020 
1024 
1000 
1002 
1004 
1006 
1008 
100A 
100C 
100E 
1010 
1012 
Word-wide 
memory 
1000 
1004 
1008 
100C 
1010 
1014 
1018 
101C 
1020 
1024 
—Vi____L 
T 
Byte-wide 
memory 
1000 
1002 
1004 
1006 
1008 
100A 
100C 
100E 
1010 
1012 
Word-wide 
memory 
1000 
1004 
1008 
100C 
1010 
1014 
1018 
101C 
1020 
1024 
^
— 
Longword-wide 
memory 
Figure 6.6 The 68K's memory 
space. 
0,4,8,.... Figure 6.6 illustrates how the 68K's memory space 
is organized. 
Figure 6.6 poses an interesting question. If you store a 
32-bit longword at, say, memory location $1000, where do 
the 4 bytes go? For example, if the longword is $12345678, 
does byte $12 go into address $1000 or does byte $78 go into 
address $1000? 
The 68K stores the most-significant byte of an operand at 
the lowest address (in this case $12 is stored at $1000). This 
storage order is called Big Endian (because the 'big end' of a 
number goes in first). The term Big Endian has been bor-
rowed from Gulliver's Travels. Intel processors are Little 
Endian and store bytes in the reverse order to the 68K 
family. 
The 68K stores the most-significant byte of a word in bits 
d08 to d15 at an even address and the least-significant byte in 
bits d00 to d07 at an odd address. Executing MOVE.W 
DO, 1234, stores bits d00 to d07 of DO at byte address 1235 
and bits dos to d15 of DO at byte address 1234. To avoid con-
fusion between registers and bits, we use 'D' to indicate a reg-
ister and 'd' to indicate a bit. We introduced the 68K's 
registers in the previous chapter; now we examine some 
of their features. In Fig. 6.6 we've labeled the individual bytes 
of the 16-bit and 32-bit memory space in blue to demon-
strate that the most-significant byte of a word or longword is 
at the low address. 
of their registers. In such cases, learning assembly language is 
rather like learning to conjugate irregular foreign verbs. 
The 68K's data registers are written DO to D7. To refer to 
the sequence of consecutive bits i to j in register DM we write 
Dni:j. For example, we indicate bits 8 to 31, inclusive, of D4 by 
D4(831). This notation is an extension of RTL and is not part 
of the 68K's assembly language. 
When a byte operation is applied to the contents of a data 
register, only bits d00 to d07 of the register are affected. 
Similarly, a word operation affects bits d^ to d,5 of the regis-
ter. Only the lower-order byte (word) of a register is affected 
by a byte (word) operation. For example, applying a byte 
operation to data register D1 affects only bits 0 to 7 and leaves 
bits 8 to 31 unchanged, CLR . B Dl forces the contents of D1 
to XXXXXXXXXXXXXXXXXXXXXXXX00000000, where 
the Xs represent the old bits of Dl before the CLR . B Dl was 
executed. If [Dl] = $12345678 before the CLR. B Dl, then 
[Dl] = $12345600 after it. 
Further examples should clarify the action of byte, word, 
and longword operations. In each case we give the 68K form 
of the instruction and its definition in RTL. We use slice nota-
tion to indicate a range of bits. 
Assembly from 
ADD.L 
D0,D1 
ADD.W 
D0,D1 
ADD.B 
D0,D1 
RTL definition 
[01(0:31)1 *-
[Dl ( C ! l 6 )] <-
[DlrJ:7>] 
<" 
[Dl ( 0 
[Dl,„ 
[Dl,,, 
31,1 + 
[D1, C ; 3„] 
Hll + 
[00,0:16,] 
Vl 
+ 
[DO,;,:-,] 
6.2.1 Data registers 
The 68K has eight general-purpose data registers, numbered 
DO to D7. Any operation that can be applied to data register Di 
can also be applied to D;. No special-purpose data registers 
are reserved for certain types of instruction. Some micro-
processors do not permit all instructions to be applied to each 
If the initial contents of DO and Dl are $12345678 and 
$ABCDEF98, respectively, the ADD operation has the follow-
ing effects on the contents of D1 and the carry bit, C. 
ADD.L 
D0,D1 results in [Dl] = BK0246I 0 and [C] = 0 
ADD. 5-; D0,D1 results in [Dl] = ABCD4510 and [C] = 1 
ADD . 2 
DO, D l 
results in [ Dl ] = ABCDEF10 and [C] = 1 

236 
Chapter 6 Assembly language programming 
The state of the carry bit and other bits of the CCR are 
determined only by the result of operations on bits 0-7 for a 
byte operation, by the result of operations on bits 0—15 for a 
word operation, and by the result of operations on bits 0-31 
for a longword operation. 
One of the most common errors made by 68K program-
mers is using inconsistent size operations on a data register, as 
the following example demonstrates. 
MOVE.B XYZ,D0 
SUB.B 
#5,DO 
CMP.W 
#12,DO 
BGT 
Test 
Get the 8-bit contents of memory 
and subtract 5 to get [XYZ] - 5 
Is ([XYZ] - 5) > 12 
This example implements the operation IF ([ XYZ ] -5) > 
12 THEN 
But note that the operand XYZ is created as a 
byte value and yet it is compared with a word value. This frag-
ment of code might work correctly sometimes if the contents 
of bits 8 to 15 of DO are zero. However, if these bits are not 
zero, this code will not operate correctly. 
6.2.2 Address registers 
An address register holds the location of a variable. Registers 
A0-A6 are identical in that whatever we can do to Ai, we can 
also do to A;'. Address register A7 is also used as a stack 
pointer to keep track of subroutine return addresses. We 
describe the use of the stack point in detail later in this 
chapter. 
Address registers sometimes behave like data registers. For 
example, we can move data to or from address registers and 
we can add data to them. There are important differences 
between address and data registers; operations on address 
registers don't affect the status of the condition code register. 
If you are in the process of adding up a series of numbers, you 
shouldn't have to worry about modifying the CCR every time 
you use an address register to calculate the location of the 
next number in the series. 
Because the contents of an address register are considered 
to be a pointer to an item in memory, the concept of separate 
independent fields within an address register is quite mean-
ingless. All operations on address registers yield longword 
values. You can apply a . L operation to an address register 
but not a . B operation. No instruction may operate on the 
low-order byte of an address register. However, word opera-
tions are permitted on the contents of 
location XYZ 
address registers because the 16-bit result 
of a . w operation is automatically sign-
extended to 32 bits. For example, the oper-
ation MOVEA.W 
#$8022,A3 has the 
effect: 
[A3] 
$FFFF8022 
The 16-bit value $8022 is sign extended to $FFFF8022. 
Similarly, MOVEA .W #$7022, A3 has the effect: 
[A3] 
$00007022 
The concept of a negative address may seem strange. If you 
think of a positive address as meaning forward and a negative 
address as meaning backward, everything becomes clear. 
Suppose address register Al contains the value 1280. If 
address register A2 contains the value —40 (stored as the 
appropriate two's complement value), adding the contents of 
Al to the contents of A2 by ADDA. L Al, A2 to create a com-
posite address results in the value 1240, which is 40 locations 
back from the address pointed at by A1. 
We conclude with an example of the use of address 
registers. Address register A0 points to the beginning of a 
data structure made up of 50 items numbered from 0 to 49. 
Each of these 50 items is composed of 12 bytes and data 
register DO contains the number of the item we wish 
MNEMONICS FOR OPERATIONS ON ADDRESS REGISTERS 
Although some of the operations that can be applied to the 
contents of data registers can also be applied to the contents 
of address registers, the 68K's assembler employs special 
mnemonics for operations that modify the contents of an 
address register. The following examples illustrate some of 
these mnemonics. In each case, the destination operand is an 
address register 
ADDA.L D1,A3 ADDA = add to address register 
MOVEA.L D1,A2 MOVEA = move to an address regi 
SUBA.W D1,A3 SUBA = subtract from an addres 
CMPA.L A2,A3 SUBA = compare with an address 
Some assemblers for the 68K permit only the use of the ADD 
mnemonic for both ADD . w A l , D l and for ADD . w D1,A1. 
Other assemblers demand that the programmer write ADDA . w 
D l , A l and will reject ADD . w D l , A l . The purpose of forcing 
programmers to write MOVEA, ADDA, and SUBA instead of 
MOVE , ADD , and SUB when specifying address registers as 
destinations is to remind them that they are dealing with 
addresses and that these addresses are 
treated differently to data values (e.g. 
s t e r 
because of sign extension). Practical applica-
s r e g i s t e r 
tions of the 68K's address registers are pro-
r e g i s t e r 
vided when we discuss addressing modes. 

6.3 Features of the 68K's instruction set 
237 
AO 
AO points to the start 
of the data structure 
A1 
e start 
A1 points to th e start 
of the /th element 
DO 
/ 
12-byte data block 
(Item 0) 
Offset 12 x / 
12-byte data block 
(Item /) 
12-byte data block 
(Item 49) 
Data 
structure 
Figure 6.7 Using an 
address register to access 
a data element. 
Name 
Exchange 
Swap 
Load effective address 
Assembly form 
to access. Figure 6.7 illustrates this data structure. Suppose 
we need to put the address of this item in Al. In what 
follows use the operation MULU #n, DO, which multiplies the 
16-bit low-order word in DO by n and puts the 32-bit product 
in DO. 
We need to find 
where the required 
item falls within 
the data structure. 
In order to do this 
we multiply 
the 
contents of DO by 12 (because each item takes up 12 bytes). 
Then we add this offset to the contents of AO and deposit the 
result in Al. That is, 
EXG 
SWAP 
LEA 
Di,Dj 
Di 
P,Ai 
processing operations, you can perform any data manipula-
tion you require. However, the 68K provides some special-
purpose data movement instructions to generate more 
compact and efficient code. The following three instructions 
provide enhanced data movement capabilities. 
RTL definition 
[Temp] «- [Di], 
[Di(0:15)l 
<~ 
[ D i ( 
[ A i ] 
<- 
p 
IDi] <r- [Dj} [Dj] <- [Temp] 
[Di, 
:31)3 
* -
[Di, 
The EXG instruction is intrinsically a longword opera-
tion that exchanges the contents of two registers (see 
Fig. 6.8(a)). EXG maybe used to transfer the contents of an 
MOLD 
#12,DO 
MOVEA.L A0,A1 
ADDA.L D0,A1 
Calculate the offset 
Copy AO to Ai 
Add the offset to Al 
into the data structure 
6.3 Features of the 68K's 
instruction set 
We've already described the 68K's basic operations. We now 
introduce some of the 68K's other instructions and demon-
strate how they are used and what happens as they are exe-
cuted on a simulator. 
6.3.1 Data movement instructions 
The MOVE instruction is the most common data movement 
instruction. Indeed, by using a MOVE in conjunction with data 
address register into a data register and vice versa. SWAP 
exchanges the upper- and lower-order words of a given data 
register. The LEA (load effective address) instruction gener-
ates an address and puts it in an address register. 
Let's write a program that executes some of these data 
movement instructions and then use the simulator to observe 
what happens as we trace through it. This program is just a 
random selection of data movement instructions—it doesn't 
actually do anything. 

238 
Chapter 6 Assembly language programming 
Before 
A2 
D3 
AfterEXG A2,D3 
A2 
D3 
(a) Effect of an exchange operation. 
Before 
D4 
After SWAP D4 
(b) Effect of a swap operation. 
Figure 6.8 The EXG and SWAP 
instructions. 
Data 
ORG 
$400 
MOVE.L #$1234567 
MOVE.B D0,D1 
MOVE.W D0,D2 
MOVE.L D0,D3 
EXG 
D0,AO 
SWAP 
D3 
MOVEA.L Data,Al 
LEA 
Data,Al 
STOP 
#$2700 
DC.L 
$ABCDDCBA 
END 
$400 
,D0 Copy a 32-bit literal to a register 
Copy a byte from a register to a register 
Copy a word from a register to a register 
Copy a longword from a register to a register 
Exchange the 32-bit contents of two registers 
Swap the upper and lower words of a register 
Copy contents of memory location into an address register 
Copy the address "Data" into an address register 
Store a longword constant in memory at address "Data" 
This source file produces the following listing file when 
assembled. 
1 00000400 
2 00000400 203C12345678 
3 00000406 1200 
4 00000408 3400 
5 0000040A 2600 
6 0000040C C188 
7 0000040E 4843 
8 00000410 227900000420 
9 00000416 43F900000420 
10 0000041C 4E722700 
11 00000420 ABCDDCBA 
DATA 
12 
00000400 
ORG 
$400 
MOVE.L 
#$12345678,DO 
MOVE.B 
D0,D1 
MOVE.W 
D0,D2 
MOVE.L 
D0,D3 
EXG 
D0,A0 
SWAP 
D3 
MOVEA.L 
DATA,A1 
LEA 
DATA,A1 
STOP 
#$2700 
DC.L 
$ABCDDCBA 
END 
$400 
We are going to use the simulator to run this program and 
observe the contents of the simulated 68K's registers as the 
instructions are executed one by one. Figure 6.9 displays the 
contents of the simulated computer's registers immediately 
after the program is loaded.2 Note that the 68K has two A7 
address registers labeled SS and US in all simulator output. 
SS is the supervisor state stack pointer A7 and US is the user 
state stack pointer A7. When the 68K is first powered up, the 
supervisor stack pointer is selected (we will discuss the differ-
ence between these later). Throughout this chapter, all refer-
ences to the stack pointer refer to the supervisor stack pointer, 
2 This is the output from the Teesside simulator, which is a text-based 
simulator unlike EASy68K which is Windows based. This text uses both 
simulators. EASy68K is better for running programs in a debug mode, the 
Teesside simulator is better for creating files that can be used in a book. 

6.3 Features of the 68K's instruction set 
239 
The DF command is entered by the user. 
What follows is the simulator's respor" 
The simulator displays the current 
contents of the program counter (PC), the 
status register (SR), the supervisor stack 
pointer (SS), and the user stack pointer (US) 
£>C^)00400 (SR=2000 (SSJ=OOAOOOOO*(US>00000000 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 
'A4=00000000 A5=00000000 A6=00000000 A7=00A00000 
D0=00000000 D1=00000000 D2=00000000 
D3=00000000 
D4 = 00000000_D5=00000000 D6=0000O_Q00 D7=00000000 
>JlOVE.L 
#31234 5 678, DO) 
The simulator prints the contents of 
data registers DO to D7 and address 
registers AO to A7 
X=0 
N=0 
Z=0 
V=0: 
C=0! 
This is the next instruction 
to be executed 
These are the current 
flag bits of the condition 
code register (i.e. The 
lower byte of the SR) 
The simulator displays all 
values in hexadecimal form 
Figure 6.9 Structure of the output from 
the simulator. 
SP (i.e. A7). In Fig. 6.9, PC defines the current value of the pro-
gram counter, SR the status register containing the 68K's CCR, 
and X, N, Z, V, and C are the CCR's flag bits. 
The last line of the block of data in Fig. 6.9 is the 
mnemonic of the next instruction to be executed. Because the 
simulator doesn't use symbolic names, all addresses, data val-
ues, and labels are printed as hexadecimal values. In Fig. 6.9, 
the program counter is pointing at location 40016 and the 
instruction at this address is MOVE . L #$12345678,DO. 
We now execute this program, instruction by instruction. 
The purpose of this exercise is to demonstrate the use of the 
simulator and to show how each instruction affects the 68K's 
internal registers as it is executed. To help you appreciate what 
is happening, registers that have changed are depicted in blue. 
PC=000406 SR=2000 SS=0OA0O000 US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=O0AO0000 Z=0 
D0=12345678 D1=00000000 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.B 
D0,D1 
The first instruction, MOVE.L 
#$12345678,DO, has been 
executed. The only registers that have changed are DO and the 
program counter. The PC has increased by 6 because the 
instruction had a 2-byte op-code and a 4-byte immediate 
value. The shortest 68K instruction is 2 bytes and the longest 
is 10 bytes. The next instruction to be executed, MOVE.B 
DO , D l , copies the low-order byte of DO to D1. 
PC=000408 SR=2000 SS=0OAO0000 US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00A00OOO Z=0 
D0=12345678 Dl=00000078 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.W 
D0,D2 

240 
Chapter 6 Assembly language programming 
As you can see, only the least-significant byte of DO has 
been copied to Dl. The next two instructions, MOVE.W 
DO, D2 and MOVE . L DO , D3, demonstrate the transfer of a 
word and a longword, respectively. 
PC=00040A SR=2000 SS=00A00000 US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00A00000 Z=0 
D0=12345678 Dl=00000078 D2-C0005678 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.L 
D0,D3 
FC=00040C SR=2000 SS=00A00000 US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=OOAOOOO0 Z=0 
D0=12345678 Dl=00000078 D2=00005678 D3-12345678 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=0000000O C=0 
>EXG 
D0,A0 
The next instruction, EXG DO, AO, exchanges the contents of 
a pair of registers to give 
P O 0 0 0 4 0 E SR=2000 SS=00A00000 US=00000000 
X=0 
A0=12345678 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00A0OOOO Z=0 
DO-00000000 Dl=00000078 D2=00005678 D3=12345678 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>SWAP 
D3 
The SWAP instruction swaps the upper and lower order 
words of a data register. 
PC=000410 SR=2000 SS=O0A00OOO US=00000000 
X=0 
A0=12345678 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=OOAO00OO Z=0 
D0=00000000 Dl=00000078 D2=00005678 D3-56781234 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVEA.L 
$0420, Al 
TheMOVEA.L D a t a , A l and LEA D a t a , A l instructions 
have the following effects. 
PC=000416 SR=2000 SS=OOA00OOO US=00000000 
X=0 
A0=12345678 A1=ABCDDCBA A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=OOA000OO Z=0 
D0=00000000 Dl=00000078 D2=00005678 D3=56781234 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>LEA.L 
$ 0 4 2 0 , A l 
PC=00041C SR=2000 SS=O0AO0000 US=00000000 
X=0 
A0=12345678 Al=00000420 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00A0OO00 Z=0 
D0=00000000 Dl=00000078 D2=00005678 03=56781234 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>STOP 
#$2700 

6.3 Features of the 68K's instruction set 241 
A MOVEA. L Data, A l instruction loads address register Al 
with the contents of the operand Data, whereas LEA Data, 
A l loads Al with the address of the operand (the significance 
of this will become clear later). We now look at instructions 
that do more than move data from one place to another. 
6.3.2 Using arithmetic operations 
We've already encountered arithmetic instructions. Now we 
demonstrate their use by writing a program to calculate 
Z = (X2 + Y2)/(X - Y) where X and Y are 16-bit unsigned 
integers. The 68K provides 16-bit x 16-bit multiplication 
with a 32-bit product. The following program uses unsigned 
multiplication and division (MULU and Divu). We will 
assume that X and Y are positive values and that X > Y. 
':%•'•• 
ORG 
MOVE . V. 
MtfUJ 
MOVE.W 
MULU 
ADD. h 
HOVE.K 
SUb.W 
DiVU 
MOVE.W 
STOP 
ORG 
DC.W 
DC.W 
DS.W 
END 
LIU. X ait IJK. 
$400 
X,D0 
DO, DO 
Y,D1 
D1,D1 
D0,D1 
X,D2 
Y,D2 
D2,D1 
m,z 
#$2700 
$500 
50 
12 
1 
$400 
Start of the program 
Fut the value of X in DO 
Calculate X' (16-bit operands, 32-bit result) 
Put the value of Y in Dl 
Calculate Y' 
Add >1 to Y and put the 32-bit result in Dl 
Put the value of X in 02 
Subtract Y from D2 to get D2 = X - i 
Divide Dl by D2 to get (X:' -t T')/(X - Y)3 
Put the result now in Dl into Z 
Put the data here 
I n i t i a l dummy value for variable X 
I n i t i a l dummy value for variable'Y 
Reserve space for the r e s u l t 
End of program and address of entry poirit 
Remember that you can't perform a memory-to-memory 
subtraction—you have to load one of the operands onto a 
data register. We assemble this program to create a listing file. 
1 
00000400 
2 
3 
00000400 303900000500 
4 
OO0Q040S COCO 
5 
00000408 323900000502 
6 
0000040E C2C1 
7 
00000410 D280 
8 
00000412 343900000500 
9 
00000418 947900000502 
10 
0000041E 82C2 
11 
00000420 33C100000504 
12 00000426 .4E722700 
14 00000500 , 
15 
16 00000500 0032 
' X; 
17 00000502 O00C 
Y; 
18 .00000504 00000002 
Z; , 
13 
20 
000Q0400 „ 
ORG 
$400 
;Start of the program 
MOVE.W 
X,DO 
;Put the value of X in DO 
MUUJ 
DO,, DO 
; Calculate Xz 
, 
' • 
MOVE.W 
Y,D1 
;Put the value o f Y in Dl 
MULU , 
DI,DJ 
/Calculate1 ,Y2 
' . ' ' ' . ' ' 
: ' ,'•' 
ADD.L 
D0,D1 
;Add X1' to Y2 and put result in Dl 
MOVE.W 
X,D2 
;Put the value Of ,X in,D2 ' 
,' 
,' 
SOB.'w 
Y,D2 
/Subtract Y from D2 to get x - Y , 
DIVD 
D2,D1 
,-Diylde Dl toy D2 to'get (X4 + YS)/(X - Y) ' 
MOVE.W 
D1,Z 
?Put the result now in Dl into Z 
STOP . 
#$2700 
' • 
: ' 
. 
• 
• 
• 
ORG 
$500 
;Put the data here • 
DC.W 
50 - 
; I n i t i a l dummy value for X 
- 
, •. 
DC.W 
12 
• ,-initial dummy value l o r Y 
DS.W 
1 
- ,'.* Reserve' space £ or, the result ," 
, ,.-..p , ,,,, . '• 
ESD 
• $400 
';tend,of program and address-of entry, paint, . .- v 
3 The DIVU D2, D l instruction divides the 32-bit value in Dl by the 
16-bit value in D2. The 16-bit quotient is placed in the low-order word of 
D1 and the 16-bit remainder is placed in the upper-order word of D1. 
ORG 
$500 
;Put the data here • 
;Initial dummy value for X 
- 
, • 
• ;initial dummy value lor y 
/.•Reserve'space for, the result ," 
- • •... , 
;fend,of program and address-of entry pqi»t'< . 
50 • 
12 
1 
$4,00 

242 
Chapter 6 Assembly language programming 
We can use the simulator to run this program line by line 
and observe its execution. If we examine memory initially, 
we get. 
000500 00 32 00 0C 00 00 00 00 00 00 00 00 00 00 00 00 
The first six digits 000500 give the first memory location 
on the line, and the following 16 pairs of digits give the con-
tents of 16 consecutive bytes starting at the first location. 
Location 50015 contains 3216 = 50, and location 50216 con-
tains 0C16 = 12. These values were set up by the two DC. W 
(define constant) assembler directives. 
The state of the system prior to the execution of the first 
instruction is 
PC=000400 SR=2000 SS=00A00000 US=00000000 
• X=0 
A0=00000000 Al=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00A00000 Z=0 
D0=00000000 D1=00000000 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.W 
$0500,DO 
We are going to step through this program a line at a time 
and display the contents of the registers as we execute each 
instruction. Values that change are displayed in blue to make 
the program easier to follow. 
PC=000406 SR=2000 SS=00A00000 US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=0OAOO0OO Z=0 
D0=00000032 D1=00000000 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MULU.W 
DO,DO 
The instruction MOVE.w $0500, DO has been executed 
and the contents of memory location 50016 have been copied 
into data register DO. 
PC=000408 SR=2000 SS=00A00000 US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=OOA0OOOO Z=0 
DO=000009C4 Dl=00000000 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.W 
$0502,Dl 
We have just executed MULU DO , DO and the contents of DO 
is 9C416. This is 50 X 50 = 2500 = 9C4,6. 
PC=00040E SR=2000 SS=00A00000 US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00A00000 Z»0 
DO=000009C4 Dl=0000000c D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MULU.W 
D1,D1 

PC=000410 SR=2000 SS=O0A0OOO0 US=OO0000OO 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=O0A00OO0 Z=0 
DO=000009C4 Dl=00000090 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>ADD.L 
DO,Dl 
At this stage D l contains C16
2 = 12 X 12 = 144 = 90l6. 
PC=000412 SR=2000 SS=OOAOO00O US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=0OAOO00O Z=0 
DO=000009C4 Dl=OO0O0A54 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.W 
$0500,D2 
We have now calculated X2 + Y2 and deposited the result 
in data register Dl. 
PC=000418 SR=2000 SS=00A00000 US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=0OA0OOOO Z=0 
DO=000009C4 D1=00000A54 D2=00000032 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>SUB.W 
$0502,D2 
PC=00041E SR=2000 SS=OOAOO0OO US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00A00000 Z=0 
D0=000009C4 D1=00000A54 D2=00000026 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>DIVU.W 
D2,D1 
PC=OO0420 SR=2000 SS=00A00000 US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=OOA0OOO0 Z=0 
D0=000009C4 Dl=00160045 D2=00000026 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.W 
D l , $ 0 5 0 4 
The 68K instruction DIVU D2 , D l divides the 32-bit con-
tents of data register Dl by the lower-order 16 bits in data 
register D2. The result is a 16-bit quotient in the lower-order 
word of Dl and a 16-bit remainder in the upper-order word 
of D l . That is, A5416/9C416 = 4516 remainder 1616. The con-
tents of Dl are $00160045. 
PC=000426 SR=2000 SS=O0A0OOOO US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=OOA0OO0O Z=0 
D0=000009C4 Dl=00160045 D2=00000026 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>STOP 
#$2700 
6.3 Features of the 68K's instruction set 243 

244 
Chapter 6 Assembly language programming 
The MOVE, w Dl, $0504 stores the low-order 16-bit result 
in Dl in memory location 504If) (i.e. Z). We've used a 
wordlength operation and have discarded the remainder in 
the upper-order word of D1. Now we look at the contents of 
memory location 500 onward. 
000500 00 32 00 0C 00 45 00 00 00 00 00 00 00 00 00 00. 
As you can see, memory location 50416 now contains the 
integer result of (502 + 122)(50 - 12) = 45,6 = 69. 
6.3.3 Using shift and logical operations 
We now demonstrate the use of shift and logical operations. 
Logical shifts enable you to extract specific bits in a word. 
Consider an 8-bit byte in DO with the format xxyyyzzz, where 
the xs, ys, and zs are three groups of bits that have been 
packed into a byte. Suppose we wish to extract the three ys 
from this byte. 
logical operation AND. B # % 0 0 0 0 0111, DO to get OOOOOyyy 
in DO. 
By using the NOT, AND, OR, and EOR instructions, you can 
perform any logical operations on a word. Suppose you wish 
to clear bits 0,1, and 2, set bits 3,4, and 5, and toggle bits 6 and 
7 of the byte in DO. You could write 
AND.B 
#%11111000,DO 
OR.B 
#%00111000,DO 
EOR.B 
#%11000000,DO 
C l e a r b i t s 0, 1, and 2 
Set b i t s 3, 4, and 5 
Toggle b i t s 
6 and 7. 
If [DO] initially contains 01010101, its final contents will 
be 10111000. We will look at a more practical application of 
bit manipulation after we have covered branch operations in 
a little more detail. 
6.3.4 Using conditional branches 
You can't write programs without using the conditional 
branches required to implement loops and other control 
constructs. We now look at the branch again and demonstrate 
LSR.B #3,DO 
Shift DO right 3 places to get OOOxxyyy in DO 
AND.B #%0000011l,D0 
Clear bits 3 to 7 of DO to get OOOOOyyy 
The first instruction LSR. B #3 , DO shifts xxyyyzzz right 
its use. Consider the following example of an addition fol-
to get OOOxxyyy in DO. We remove the xs by means of the 
lowed by a branch on negative (minus). 
SHIFT OPERATIONS— A REMINDER 
The assembly language forms of the 68K's shift instructions are 
LSL #n,D0 
s h i 
LSR #n,D0 
sh 
ASL #n,D0 
sh-
ASR #n,D0 
sh 
f t 
c o n t e n t s of DO n p l a c e s 
Lft c o n t e n t s of DO n p l a c e s 
Lft c o n t e n t s of DO n p l a c e s 
Lft c o n t e n t s of DO n p l a c e s 
l e f t 
l o g i c a l l y 
r i g h t 
l o g i c a l l y 
l e f t 
a r i t h m e t i c a l l y 
r i g h t 
a r i t h m e t i c a l l y 
The integer n indicates the number of places to be shifted. 
These instructions can be applied to bytes, words, and long-
words. If you shift a word by more than one place, the end 
value of the carry bit is determined by the final shift. Consider 
the following examples: 
Initial contents of DO 
Operation 
Final contents of DO 
11001100 
11001100 
LSL.B #1,D0 
1 0 0 1 1 0 0 0 
LSR.B #1,D0 
0 1 1 0 0 1 1 0 
11001100 
11001100 
ASL.B #1,D0 
1 0 0 1 1 0 0 0 
ASR.B #1,D0 
1 1 1 0 0 1 1 0 
1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 
0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1 
ASR.W #3,DO 
1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 
ASR.W #4,DO 
0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 
11001110 
11001110 
11110000 
ROL.B #1,D0 
1 0 0 1 1 1 0 1 
ROR.B #l,DO 
0 1 1 0 0 1 1 1 
ROL.B #2,DO 
1 1 0 0 0 0 1 1 

6.3 Features of the 68K's instruction set 
245 
SUB D1,D2 
BMI ERROR 
Subtract Dl from D2 and branch IF the result in negative 
ERROR 
ELSE p a r t 
THEN p a r t 
Beware 
Here be bugs 
THIS CODE IS W R O N G ! ! ! 
The operation SUB Dl, D2 subtracts the contents of Dl 
from D2, deposits the results in D2, and updates the condi-
tion code register accordingly. 
When the BMI instruction is executed, the branch is taken 
(the THEN part) if the N-bit of the CCR is set because the 
addition gave a negative result. The 
branch target is the line labeled by ERROR 
and the intervening code between BMI 
ERROR and ERROR . . . is not executed. 
If the branch is not taken because the 
result of SUB Dl, D2 was positive, the 
code immediately following the BMI 
ERROR is executed. This code corresponds 
to the ELSE part of the I F 
THEN 
ELSE 
construction. 
Unfortunately, there's an error in this example. Suppose 
that the subtraction yields a positive result and the ELSE part 
is executed. Once the ELSE code has been executed, we fall 
through to the THEN part and execute that too, which is not 
what we want to do. After the ELSE part has been executed, it's 
necessary to skip round the THEN part by means of an BRA 
instruction. The unconditional branch instruction, BRA 
EXIT forces the computer to execute the next instruction at 
ADD 
D1,D2 
BMI 
ERROR 
ADD 
D1,D2 
EXG 
D3, A4 
EXG 
D5,A6 
BMI 
ERROR 
SUB 
BMI 
BRA 
D1,D2 
ERROR 
EXIT 
ERROR 
EXIT 
) 
ELSE part 
) 
Skip past the THEN part 
) 
) 
THEN part 
) 
Start 
EXIT and skips past the ' ERROR ' clause. Figure 6.10 demon-
strates the flow of control for this program. 
Remember we said earlier that not all the 68K's instruc-
tions affect die CCR. For example, consider the following two 
examples. 
Add the contents of Dl to the contents of D2 
If the result is negative, branch to ERROR 
and 
Add the contents of Dl to the contents of D2 
Exchange the contents of D3 and A4 
Exchange the contents of D5 and A6 
If the r e s u l t of the addition was negative, 
branch to ERROR 
Both these fragments of code have the same effect as far as 
the BMI ERROR is concerned. However, the second case might 
prove confusing to the reader of the program who may well 
imagine that the state of the CCR prior to the BMI ERROR is 
determined by the EXG D3 , DA instruction. 
Example 1 Suppose you want to write a subroutine to con-
vert a 4-bit hexadecimal value into its ASCII equivalent. 
ASCII character 
He) 
0 
30 
1 
31 
2 
32 
3 
33 
4 
34 
5 
35 
6 
36 
7 
37 
8 
38 
9 
39 
A 
41 
B 
42 
C 
43 
D 
44 
E 
45 
F 
46 
Hexadecimal value 
Binary value 
0000 
0001 
0010 
0011 
0100 
0101 
0110 
0111 
1000 
1001 
1010 
1011 
1100 
1101 
1110 
1111 
Figure 6.10 Flow of control for an IF ... THEN ... ELSE construct. 
Table 6.1 Relationship between ISO/ASCII characters and hexa-
decimal values. 
1 
true / 
x^ 
false 
[ . 
X 
^ S ^ / ' 
• 
THEN, 
ELSE 
T 

246 
Chapter 6 Assembly language programming 
Table 6.1 illustrates the relationship between the binary 
value of a number (expressed in hexadecimal form) and its 
ASCII equivalent (also expressed in hexadecimal form). For 
example, if the internal binary value in a register is 
00001010, its hexadecimal equivalent is A16. In order to 
print the letter 'A' on a terminal, you have to transmit the 
ASCII code for the letter 'A' (i.e. $41) to it. Once again, 
please note that there is a difference 
between the internal binary repre-
sentation of a number within a com-
puter and the code used represent the 
symbol for that number. The number 
six is expressed in 8 bits by the binary 
pattern 00000110 and is stored in the 
computer's memory in this form. On 
the other hand, the symbol for a six 
(i.e. '6') is represented by the binary 
pattern 00110110 in the ASCII code. 
If we want a printer to make a mark on paper corresponding 
to '6', we must send the binary number 00110110 to 
it. Consequently, numbers held in the computer must 
be converted to their ASCII forms before they can be 
printed. 
From Table 6.1 we can derive an algorithm to convert a 
4-bit internal value into its ASCII form. A hexadecimal value 
in the range 0 to 9 is converted into ASCII form by adding 
hexadecimal 30 to the number. A hexadecimal value in the 
range $A to $F is converted to ASCII by adding hexadecimal 
$37. If we represent the number to be converted by HEX and 
the number to be converted by ASCII, we can write down a 
suitable algorithm in the form 
The CMP source, destination subtracts the source 
operand from the destination operand and sets the flag bits of 
the CCR accordingly; that is, a CMP is the same as a SUB except 
that the result is not recorded. 
Example 2 Consider the following algorithm. 
* IF x = y THEN y = 6 
* IF x < y THEN y = y + 
* 
1 
CMP.B 
D1,D0 
Assume that x is in DO and y in : 
BNE 
NotEqu 
IF x = y THEN 
MOVE.B #6,D1 
y = 6 
BRA 
exit 
and leave the algorithm 
NotEqu BGE 
exit 
IF x < y THEN exit 
ADD.B 
#1,D1 
ELSE y = y + 1 
exit 
exit point for the algorithm 
Dl 
IF HEX < 9 
THEN ASCII 
ELSE ASCII 
HEX 
HEX 
30 1 6 
37 1 6 
We can rewrite the algorithm as 
ASCII = HEX + $30 
IF ASCII > $39 THEN ASCII 
ASCII + 7 
This algorithm can be translated into low-level language as 
We perform two tests after the comparison CMP .B DO, Dl. 
One is a BNE and the other a BGE. We can carry out the two 
tests in succession because there isn't an intervening instruc-
tion that modifies the state of the CCR. 
Although conditional tests performed by high-level 
languages can be complex (e.g. IF x+Y-z > 3t), the condi-
tional test at the assembly language level is rather more basic 
as this example demonstrates. 
Templates for control structures 
We now represent some of the control structures of 
high-level languages as templates in assembly language. A 
template is a pattern or example that can be modified to suit 
the actual circumstances. In each of the following examples, 
the high-level construct is provided as a comment to the 
assembly language template by means of asterisks in the 
first column. The condition tested is [DO] = [Dl] and the 
actions to be carried out are Actionl or Action2. The 
templates can be used by providing the appropriate test 
instead of CMP 
D0,D1 and providing the appropriate 
sequence of assembly language statements instead of 
Actionl or Action2. 
ADD.B #$30,DO 
CMP.B #$39,DO 
BLE 
EXIT 
ADD.B #7, DO 
EXIT RTS 
Note: D0.B holds HEX value on subroutine entry 
D0.B holds the ASCII character code on return 
No other register is modified by this subroutine 
ASCII = HEX + $30 
IF ASCII < $39 THEN EXIT 
ELSE ASCII = ASCII + 7 

6.3 Features of the 68K's instruction set 
247 
Actionl 
EXIT 
Actionl 
Action2 
EXIT 
IF [DO] = [Dl] THEN Actionl 
CMP 
D0.D1 
BNE 
EXIT 
Perform test 
IF [DO] * [Dl] THEN exit 
ELSE execute Actionl 
Exit point for construct 
IF [DO] = [Dl] THEN Actionl ELSE Action2 
CMP 
D0,D1 
BNE 
Action2 
BRA 
EXIT 
Compare DO with Dl 
IF [DO] * [Dl] perform Action2 
Fall through to actionl if [DO] = [Dl] 
Skip round Action2 
Action2 
Exit point for construct 
Actionl 
FOR K = I TO J 
ENDFOR 
MOVE 
#1,D2 
Load loop counter, 
Perform Actionl 
D2. with I 
ADD 
#1, D2 
CMP 
#J+1,D2 
BNE 
Actionl 
EXIT 
* 
WHILE [DO] = 
Repeat 
CMP 
DO, Dl 
BNE 
EXIT 
Actionl 
Increment loop counter 
Test for end of loop 
IF not end THEN go round again 
ELSE exit 
EXIT 
BRA 
Repeat 
[Dl] Perform Actionl 
Perform test 
IF [DO] * [Dl] THEN exit 
ELSE carry out Actionl 
REPEAT 'loop 
Exit from construct 
Actionl 
EXIT 
REPEAT Actionl UNTIL [DO] = [Dl] 
Perform Actionl 
CMP 
D0,D1 
BNE 
Actionl 
Carry out test 
REPEAT as long as [DO] 
Exit from loop 
* [Dl] 
CASE OF I 
1 = 0 ActionO 
1 = 1 Actionl 
1 = 2 Action2 
1 = 3 Action3 
I = N ActionN 
I > N Exception 
CMP.B 
#N,I 
BGT 
EXCEPTION 
MOVE.W I, DO 
MULU 
#4,DO 
LEA 
Table,A0 
LEA 
(A0,D0), AO 
MOVEA.L (A0),A0 
JMP 
(A0) 
Test for I out of range 
IF I > N THEN exception 
Pick up value of I in DO 
Each address is a longword 
A0 points to table of addresses 
A0 now points to case I in table 
A0 contains address of case I handler 
Execute case I handler 

248 
Chapter 6 Assembly language programming 
Table 
ORG 
ActionO DC.L 
Actionl DC.L 
Action2 DC.L 
<address> 
Here is the table of exceptions 
<addressO> 
Address of case 0 handler 
<addressl> 
Address of case 1 handler 
<address2) 
Address of case 2 handler 
ActionN 
DC.L 
<addressN> 
Address of case N handler 
EXCEPTION 
... 
Exception handler here 
The case number I stored in DO must be multiplied by 4 
before it can be added to the address in AO. This action is 
necessary because the cases numbers are consecutive integers 
0,1,2,3 while the addresses of the case handlers are consecu-
tive longword addresses (i.e.AO + 0,A0 + 4,A0 + 8,. ..). 
Putting it all together 
Consider a system with eight single-bit inputs (P, Q, R, S, T, U, 
V, W) and eight single-bit outputs (A, B, C, D, E, F, G, H). We're 
not interested in the details of input/output techniques here 
and assume that reading a memory location whose address is 
INPUT loads the values of P to W into a data register. Similarly, 
writing the contents of a data register to memory location 
OUTPUT sets up the eight output bits A to H. The formats of 
the input and output control words are defined in Fig. 6.11. 
Suppose that a system has to implement the following 
control operation. 
OR ( (P = 0) AND (S 1>) 
IF ( (P = 1) AND (Q = 0) ) 
THEN C = 1; E = 0 
ELSE C = 0; E = 1 
ENDIF 
We have to translate this algorithm into 68K code. 
The above action involves the testing of three bits of INPUT 
(P, Q, and S), and then setting or clearing two bits of 
OUTPUT (C and E). The bits of OUTPUT not involved in 
the algorithm must not be affected in any way by operations 
on bits C and E. 
FALSE 
TRUE 
EXIT 
MOVE.B INPUT,DO 
AND.B #%11000000 DO 
CMP.B #%10000000 DO 
BEQ 
TRUE 
MOVE.B INPUT,DO 
AND.B #%10010000 DO 
CMP.B #%00010000 DO 
BEQ 
TRUE 
MOVE.B OUTPUT,DO 
AND.B #%11011111 DO 
OR.B 
#%00001000 DO 
MOVE.B DO,OUTPUT 
BRA 
EXIT 
MOVE.B OUTPUT,DO 
AND.B #%11110111 DO 
OR.B 
#%00100000 DO 
MOVE.B DO,OUTPUT 
Figure 6.11 The memory map of two input/output ports. 
Let's look again at the compare instruction, CMP, that com-
pares two operands and sets the bits of the CCR accordingly. 
CMP.B #%0001000,DO compares the contents of DO with 
the value 000100002 by evaluating [DO] - 00010000. The 
result is discarded, leaving the contents of DO unaffected by 
flie CMP operation. Only the bits of the CCR are modified. If 
DO contains 00010000, the subtraction yields zero, setting 
the Z (zero) flag of the CCR. The following operation, 
BEQ TRUE, results in a branch to the instruction whose address 
is labeled TRUE. Comparison instructions are of the form 
CMP s o u r c e , d e s t i n a t i o n . The difference between CMP 
Di, D j and SUB Di, D j is that the former evaluates Di — D; 
Get input status 
Mask out all bits but P and Q 
Test for P = 1, Q = 0 
Goto action on test true 
Get input status again 
Mask out all bits but P and S 
Test for P = 0, S = 1 
Goto action on test true 
Get the output control word 
Clear bit C 
Set bit E 
Set up new output control word 
Branch past actions on test true 
Get the output control word 
Clear bit E 
Set bit C 
Set up new output control word 
Continue 
« 
Sbjts 
„ 
Memory 
INPUT ±_o_^__^_j_j^_y_^_ 
OUTPUT 
_A__B_^_D_J_J__G__H_ 

6.4 Addressing modes 
249 
POINTS TO REMEMBER 
The assembly language symbol % indicates that the following 
number is interpreted as a binary value and the symbol $ 
means that the following number is interpreted as a 
hexadecimal value, AND . B #%11000000, DO tells you much 
more than the hexadecimal and decimal forms of the operand, 
AND.B #$C0,D0andAND.B #192,DO, respectively. 
The symbol # informs the assembler that the following 
value is not the address of a memory location containing the 
operand, but the actual operand itself, AND . B 
#%11000000, DO means calculate the logical AND 
between the binary value 11000000 and the contents of DO. 
If we had made a mistake in the program and had written 
A N D . B %11000000,DO (ratherthan A N D . B 
#%11000000,DO),the instruction would have AN Ded DO 
with the contents of memory location %11000000 
(i.e. location 192). 
and throws away the result, whereas the latter evaluates 
Di — Dj and puts the result in Dj. 
The label FALSE is a dummy label and is not in any way 
used by the assembly program. It merely serves as a reminder 
to the programmer of the action to be taken as a result of the 
test being false. At the end of this sequence is an instruction 
BRA EXIT. A BRA (branch) is equivalent to a GOTO in a high-
level language and causes a branch round the action taken if 
the result of the test is true. 
6.4 Addressing modes 
guage programming and introduce a wealth of variations on 
address register indirect, addressing. 
6.4.1 Immediate addressing 
Application of immediate addressing 
As an arithmetic constant 
+ 22 
MOVE 
NUM,D0 
[DO] 
<-
[NUM 
ADD 
# 2 2 , D O 
[DO] 
<-
[DO] 
MOVE 
D0,NTJM 
[NUM] 
<-
[DO] 
Addressing modes include all the ways of specifying the loca-
tion of an operand used by an instruction. We encountered 
the fundamental addressing modes, absolute, immediate, and 
address register indirect, in the previous chapter. Now we 
demonstrate how some of these are used in assembly lan-
This sequence of instructions is equivalent to the high-level 
language construct NUM = NUM + 22 and increases the value 
in memory location NUM by 22. That is, [NUM] <- [NUM] 
+ 
22. The 68K can, in fact, add an immediate operand to a 
memory location directly without using a data register by 
REVIEW OF IMMEDIATE ADDRESSING 
Immediate addressing allows the programmer to specify a 
constant as an operand. The value following the op-code in an 
instruction is not a reference to the address of an operand but 
is the actual operand itself. The symbol # precedes the 
operand to indicate immediate addressing.The four instructions 
below demonstrate how absolute and immediate addressing 
modes are represented in assembly language and in RTL 
Name 
number is binary.The instructions MOVE #25, DO, 
MOVE #$19, DO, and MOVE #%00011001, DO have identical 
effects. 
Immediate addressing is used when the value of the 
operand required by an instruction is known at the time the 
program is written; that is, it is used to handle constants as 
opposed to variables. Immediate addressing is faster than 
absolute addressing, because only one memory 
Assembly language form 
RTL form 
MOVE 1234,DO 
[DO] <- [1234] 
MOVE #1234,DO 
[DO] <-1234 
ADD 
1234,DO 
[DO] <-[D0] + [l234] 
ADD 
#1234,DO 
[DO] <- [D0] + 1234 
absolute addressing 
immediate addressing 
absolute addressing 
immediate addressing 
The symbol # is not part of the instruction. It is a message to 
the assembler telling it to select that code for MOVE that uses 
the immediate addressing mode. Don't confuse the symbol # 
with the symbols $ or %.The $ indicates only that the following 
number is hexadecimal and the % indicates that the following 
reference is required to read the instruction during the fetch 
phase. When the instruction MOVE #5, DO is read from 
memory in a fetch cycle, the operand, 5, is available 
immediately without a further memory access to 
location 5, to read the actual operand. 

250 
Chapter 6 Assembly language programming 
SPECIAL 68K IMMEDIATE OPERATIONS 
The 68K provides three instructions with immediate source 
operands, which allow you to perform an operation directly on 
the contents of a memory location. These operations can be 
called data-to-memory and are indicated by appending an I 
to the instruction mnemonic. The three instructions with this 
facility are A D D I , S U B I , and CMPI. Consider 
CMPI.B #42,$1234 
SUB.W #$F23D,(AO) 
Compare the contents of location $1234 with 42 
Subtract F23D16 from the contents of the 
memory location pointed at by AO 
means of the special add immediate instruction ADDI. For 
example, ADDI #22 , NUM adds the constant value 22 to the 
contents of the location called NUM. 
In a comparison with a constant 
Consider the test on a variable, NUM, to determine whether it 
lies in the range 7 < NUM < 25. 
MOVE NUM,DO 
Get NUM in DO 
CMP 
#8,DO 
Compare it with 8 
BMI 
FALSE 
IF negative NUM < 7 
CMP 
#25,DO 
Compare it with 25 
BPL 
FALSE 
IF positive NUM > 24 
TRUE 
FALSE 
As a method of terminating loop structures. 
A typical loop structure in both Java and C is illustrated 
below. 
such as N+1, the assembler evaluates it and replaces it by the 
calculated value—in this example #N+1 is replaced by 11. We 
use the comparison with N + 1, because the counter is 
incremented before it is tested. On the last time round the 
loop, the variable I becomes N + 1 after incrementing and the 
branch to NEXT is not taken, allowing the loop to be 
exited. This loop construct can be written in a more elegant 
fashion, but at this point we're interested only in the applica-
tion of immediate addressing as a means of setting up 
counters. 
6.4.2 Address register indirect 
addressing 
In address register indirect addressing, the operand is 
specified indirectly via the contents of a pointer register. RISC 
processors allow any register to act as a pointer, whereas the 
68K reserves address registers for this function. The box 
provides a review of this addressing mode. 
f o r 
( i = 0 ; i < N+l; i++) 
The high-level language FOR construct may readily be trans-
lated into 68K assembly language. In the following example, 
the loop counter is stored in data register DO. 
Using address register indirect addressing 
to access a table 
Figure 6.12 demonstrates how you'd add up a sequence 
of numbers using ADD. B (AO), DO . The computer reads 
the contents of address register AO (i.e. 1000l6) and then reads 
the contents of memory location 100016 (i.e. 25). This 
operand is then added to die contents of DO. 
EQU 
10 
Define the loop size - we've used N = 10 here 
MOVE #1,D0 
NEXT 
ADD #1,D0 
CMP 
#N+1,D0 
BNE 
NEXT 
Load DO with initial value of the loop counter 
Start of loop 
Body of loop 
Increment the loop counter 
Test for the end of the loop 
IF not end THEN repeat loop 
At the end of the loop, the counter is incremented by 
ADD #1,D0. The counter, DO, is then compared with its 
terminal value by CMP #N+1,D0. If you write an expression 
We can add 100 numbers by means of address register indi-
rect addressing in the following way. This isn't efficient 
code—we'll write a better version later. 

6.4 Addressing modes 
251 
REVIEW OF ADDRESS REGISTER INDIRECT ADDRESSING 
Register indirect addressing specifies the address of an 
operand by the contents of an address register. The diagram 
illustrates the effect of MOVE . B ( AO ) , DO when 
[AO] = 100016.The computer first reads the contents of 
address register AO and then reads the contents of memory 
pointed at by AO.The contents of AO are 1000, so the 
processor reads the contents of memory location 1000 to get 
the actual operand, 25. 
Address register 
AO points to the location 
of the operand 
The following instructions illustrate address register 
indirect addressing and provide RTL definitions for the 
action to be carried out, together with a plain language 
description. 
Assembly language 
RTL definition 
MOVE (A0),D0 
[DO] <- [[AO]] 
M0VED1,(A2) 
[[A2]] <r- [Dl] 
ADD 
(A1),D2 
[D2] <- [D2] + [ [Al] 
Some texts call this addressing mode indexed addressing or 
modifier-based addressing.The manufacturers of the 68K 
reserve the term indexed addressing to indicate a particular 
variant of address register indirect addressing in which the 
effective address of an operand is calculated by adding the 
contents of two registers. 
OFFE 
OFFF 
1000 
1 0 0 l \ 
1002 
\ 
1003 
\ 
u 
This is the 
<004 
location pointed 
1005 
atbyAO. 
MOVE (Al), (A2) 
[[A2]] 
[ [ A H ; 
Description 
Move the contents of the memory pointed at by AO to DO 
Move the contents of D1 to the location pointed at by A2 
Add the contents of the location pointed at by Al to the 
contents of D2 
Move the contents of the location pointed at by Al to the 
location pointed at by A2 
LOOP 
NtJMl 
ORG 
$001000 
CLR 
DO 
MOVEA #NUM1,A0 
CLR 
Dl 
ADD 
(A0),D1 
ADDA 
#2,A0 
ADD 
#1,D0 
CMP 
#100,DO 
BNE 
LOOP 
STOP 
#$2700 
ORG 
$2000 
DC.W 
1,2,3,4,5 
END 
$1000 
Start of the program 
DO is the number counter 
AO points to the first number 
Clear the total in Dl 
Add in the number pointed at by A0 
Point to the next number in the list 
Increment the number counter 
Have we added the 100 numbers? 
If not then repeat 
Halt the program 
Data region 
Dummy data (only 5 numbers given here) 
The pointer in address register A0 is incremented by 2 on each 
memory elements are words and each word occupies 2 bytes in 
pass round the loop—the increment of 2 is required because the 
memory. We can express this program in a more compact way. 
AO 
DO 
25 
25 

252 
Chapter 6 Assembly language programming 
OFFE 
AO 
OFFF 
1000 
Offset 
r 
1001 
+0 
1002 
+1 
1003 
+2 
r 
1001 
+0 
1002 
+1 
1003 
+2 
Displacement = + 4 
r 
1001 
+0 
1002 
+1 
1003 
+2 
DO 
r 
1004 
+3 
1 
1 
J 
r 
15 
1 0 0 5 * 
+4 
ffect Of MOVE . B 4 (AC ) ,D0 
The location accessed 
is 4 bytes on from that 
pointed at by AO 
Figure 6.13 An illustration of address register indirect 
addressing with displacement. 
AO 
1000 
r 
"~ 
~*w'i 
OFFF 
1000 
OFFF 
1000 
p 
1001 
Q 
1002 
R 
1003 
1004 
^-——-~__ 
Offset with respect 
to AO 
LOOP 
LEA 
NUM1,A0 
CLR 
DO 
MOVE 
# 9 9 , D l 
ADD 
(AO),D0 
ADDA.L #2,A0 
DBRA 
Dl,LOOP 
AO points at the list of numbers" 
Clear the total 
Set up the counter for 100 cycles 
Add in a number 
Point to next number 
Repeat until all numbers added 
Figure 6.14 Using address register indirect addressing with 
displacement. 
dl6 (Ai), where dl6 is a 16-bit constant and Ai an address 
register. The effective address of an operand is calculated by 
adding the contents of the address register specified by 
the instruction to the signed two's complement constant that 
forms part of the instruction. Figure 6.13 illustrates how the 
effective 
address is calculated 
for the instruction 
MOVE.B 4 (AO) ,D0. Some 68K simulators permit you to 
write either MOVE. B 4 (AO) , DO orMOVE.B (4,A0),D0. 
We can 
define 
MOVE 
dl6(A0),DO 
in RTL as 
[D0]<-[dl6 + [AO]], where dl6 is a 16-bit two's comple-
ment value in the range — 32K to 32K. This constant is called 
a displacement or offset because it indicates how far the 
operand is located from the location pointed at by AO. 
The 
displacement 
can be 
negative; 
for 
example; 
MOVE . B -4 (AO), DO specifies an operand 4 bytes back from 
the location pointed at by AO. 
Why would you wish to use this addressing mode? 
Consider the data structure of Fig. 6.14 where three variables 
P, Q, and R, have consecutive locations on memory. If we load 
address register AO with the address of the first variable, P, we 
can access each variable via the pointer 
inAO. 
In this fragment of code we define 
the displacements P, Q, and R as 0, 1, 
and 2, respectively. 
The 68K's decrement and branch instruc-
tion DBRA Dl, LOOP implements a loop. This 
instruction subtracts 1 from the contents of Dl 
and branches back to the line labeled by LOOP. 
If, however, Dl is decremented and goes from 0 
to — 1, the loop is not taken and the next instruc-
tion in sequence is executed. Because the branch 
terminates on — 1 rather than 0, loading D1 with 
N causes DBRA Dl, LOOP to execute N+1 times. 
Address register indirect addressing with displacement 
A more general form of the 68K's address register indirect 
addressing mode is called the address register indirect address-
ing mode with displacement. The effective address is written 
p 
EQU 
0 
Q 
EQO 
1 
R 
EQU 
2 
LEA 
Block,A0 
AO p o i n t s t o "Block" 
MOVE B 
P(A0),D0 
Evaluate R = P + Q 
ADD.B 
Q(A0),D0 
MOVE B 
D0,R(A0) 
4 The LEA or load effective address instruction loads an address regis-
ter with an address. The effect of LEA NUM1, A0 is to load A0 with the 
address NUM1 (and not the contents of NUM1). This instruction is 
equivalent to MOVEA.L 
#NUM1,A0. The LEA instruction doesn't 
require a # symbol to indicate a literal operand because the source 
operand is always an address. 
OFFE 
OFFF 
Address register AO points I 
• 
25 
1000 
at memory location 1000 
1 
-|QQ-| 
AO 
/ 
1002 
1000 
| 
1 
^ / 
1003 
initial value of DO X 
1 0 0 4 
DO 
/ 
f 
U 
^ _ J 1005 
77* 
I 
/ 
The effect of 
— 
1 
I 
ADD.B 
(A0),DO 
^ 
.r 
when AO contains 1000, 
A37 
I ^("\ 
DO contains 12 and 
- ^ \ 
v 7 
memory location 1000 
Final value of D 0 \ ^ 
J 
contains 25. 
Figure 6.12 Using address register indirect addressing. 
0 1 
1 
2 

6.4 Addressing modes 
2 5 3 
This code adds two numbers and stores their sum in 
memory. But where in memory? The location of the three 
numbers is Block + P, Block + Q, and Block + R, respec-
tively. Because the value of Block can be changed by the pro-
grammer, we can locate the variables P, Q, and R in any three 
consecutive locations anywhere in memory. Why would we 
want to do that? If we access variables by specifying their 
location with respect to a pointer, we can move the program 
about in memory without having to recalculate all addresses. 
s 
VEC1 
VEC2 
EQO 
ORG 
CLR 
SUBA.L 
MOVE 
MULU 
ADD 
ADDA.L 
CMPA.L 
BNE 
MOVE 
ORG 
DS 
DS 
DS 
$10 
$001000 
DO 
A0,A0 
VEC1(A0) 
VEC2(A0) , 
D1,D0 
#2,A0 
#2*N,A0 
LOOP 
D0,S 
$002000 
1 
$20 
$20 
16 components (N = 16) 
Origin of program 
Dl 
Dl 
[DO] 
[A0] 
[Dl] 
[Dl] 
[DO] 
[A0] 
[A0] 
IF Z 
[S] < 
0 
0 
[[A0 
[Dl] 
[DO] 
[A0] 
- 2N <-
= 0 THEN 
- [DO] 
Origin of data 
Reserve a word for the product 
Reserve 16 words for Vector1 
Reserve 16 words for Vector2 
AO 
Before 
Effect of MOVE . B (A0 ) + , DO 
After 
N-1 
N+, tqr 
DO 
(a)'Initially, address register AO points 
at element P in memory which is accessed 
and loaded into DO. 
Figure 6.15 Address register indirect addressing with postincrementing. 
The instruction MULU <ea>,Di multiplies the 16-bit word 
at the effective address specified by <ea> by the lower-order 
word in D,. The 32-bit longword product is loaded into 
D,(0.31). MULU operates on unsigned values and uses two 16-bit 
source operands to yield a 32-bit destination operand. As the 
68K lacks a clear address register instruction, we have to use 
either MOVEA.L 
#0,AO or the faster SUBA.L 
A0,A0to 
clear A0. 
Note the instruction CMPA.L 
#2*N,A0 containing the 
expression 2*N, which is automatically evaluated by the 
assembler. The assembler looks up the value of N (equated to 
$10) and multiples it by 2 to get $20. Consequently, the 
assembler treats CMPA. L #2*N,A0 as CMPA.L #$2 0,AO. 
Variations on a theme 
The 68K supports two variations on address register indirect 
addressing. One is called address register indirect addressing 
with predecrementing and the other is called address register 
indirect addressing with postincrementing. The former 
addressing mode is written in assembly language as - (Ai) 
and the latter (Ai) + . Both these addressing modes use 
address register indirect addressing to access an operand 
exactly as we've described. However, the postincrementing 
mode automatically increments the address register after it's 
been used, whereas the predecrementing mode automatically 
decrements the address register before it's used. Figure 6.15 
demonstrates the operation 
ADD.B (A0)+,D0. 
This 
instruction adds the contents of 
the location pointed at by A0 
(i.e. P) to the contents of data 
register DO. After A0 has been 
used to access P, the value of A0 
is incremented to point at the 
next element, Q. 
Address 
register 
indirect 
addressing is used to access 
tables. If we access an item by 
MOVE.B (A0),D0, the next 
item (i.e. byte) in the table can 
be accessed by first updating 
the address pointer, A0, by 
ADDA #1, A0 and then repeat-
ing the MOVE.B (A0),DO.The 
68K's 
automatic 
postincre-
menting mode increments an 
address register after it has been 
used to access an operand. This 
addressing mode is indicated by 
(Ai) +. Consider the following 
examples of address register 
indirect addressing with post 
incrementing. 
+ VECl] 
* [[A0] + VEC2] 
+ [Dl] 
+ 2 
note N words = 
[PC] <r~ LOOP 
2N bytes 
(b) After accessing elementP, A0 is incremented 
to point at the next element, Q. 
Using address register indirect addressing with 
displacement 
Let's look at an example of this addressing mode that involves 
vectors. A vector is composed of a sequence of components; 
for example, the vector X might be composed of four elements 
XQI X\t XT.* 
x3. One of the most common of all mathematical 
calculations (because it crops up in many different areas— 
particularly graphics) is the evaluation of the inner or scalar 
product of two vectors. Suppose A and B are two n-compo-
nent vectors; the inner product S, of A and B, is given by 
S = ^arbi 
= a0-b0 + a r b , + ••• + a„_,-fr„_i 
If A = (1, 3, 6) and B = (2, 3, 5), the inner product S is 
given byl-2 + 3-3 + 6-5 = 41. Consider the case in which 
the components of vectors A and B are 16-bit integers. 
N 
* 
AO 
N-1 
N + 1 [—| 
P 
N 
*—» Q 
N+1 
| 
|DO 
LOOP 
XQI X{> XT.* ^ 3 ' One of the most common of all mathematical 
calculations 
K 
9 
m 
p 
Q 

254 
Chapter 6 Assembly language programming 
LEGALAND ILLEGAL ADDRESSING MODES 
Because of the real-world constraints imposed by the encoding 
don't support an offset in the calculation of an effective 
of instructions and the design of the chips themselves, not all 
address; that is, only (Ai)+ and ~(Ai) are legal. To make this 
possible variations on addressing modes are supported. Both 
clear, we present several 68K instructions—some of these 
the predecrementing and postincrementing addressing modes 
represent legal and some represent illegal addressing modes. 
ADD.W 
( A 2 ) - D2 
illegal—postdecrementing not allowed 
MOVE.W 
D O , ( 1 2 , A 1 ) + 
illegal—offset not allowed with postincrementing 
MOVE.W 
~(A2) 
(A3) + 
legal 
SUB.W 
D 3 , + ( A 4 ) 
illegal—preincrementing not allowed 
CMP.W 
(A6) + D3 
legal 
Assembly language form 
ADD.B 
(A2)+,D2 
RTL definition 
[D2] 
<- 
[D2] 
+ [ [ A 2 ] ] ; 
[A2] «- 
[A2] 
+ 1 
MOVE.B DO, ( A l ) + 
[ [ A l ] ] 
<-
[DO] ; 
[ A l ] 
<- - [ A l ] 
+ 1 
CLR.W 
(A0) + 
[ [ A O ] ] 
<- 0 ; 
[AO] <- 
[AO] 
+ 2 
MOVE.W 
( A 2 ) + , (A3) + 
[ [ A 3 ] ] 
<-
[ [ A 2 ] ] ; 
[A2] 
<-
[A2] 
+ 2 ; 
[A3] 
<- 
[A3] 
+ 2 
C L R . L 
(A0) + 
[[AO] ] <-
0 ; 
[AO] 
i- 
[AO] 
+ 4 
The pointer register is automatically incremented by 1 for 
( A 2 > +, D5. Postincrementing leaves A2 pointing to the new 
byte operands, 2 for word operands, and 4 for longword 
(0p item on the stack, 
operands. Consider the following examples. 
MOVE.B 
(A0)+,D5 
<- 
[D5(0:7)] 
MOVE.L 
(A0)+,D5 
i- 
[05,0:3!)] 
[[AO] ] ; [AO] <r- [AO] + 1 
[ [AO] ) ; [AO] <- [AO] + 4 
Examples of register indirect 
addressing with postincrementing 
The 68K provides a predecrementing address register 
Let's revisit the program to add together 100 numbers stored 
indirect addressing mode with the assembly language form 
in consecutive locations. 
LEA 
NUM1,A0 
A0 points at list of numbers 
CLR.W 
DO 
Clear the total 
MOVE.W #99,Dl 
Set up the counter for 100 cycles 
LOOP ADD.W 
(A0)+,D0 
Add in a number and move the pointer to next 
DBRA 
Dl,LOOP 
Repeat until all numbers added 
- (Ai), where the contents of Ai are decremented before 
they are used to access the operand at the address pointed at 
by Ai. As above, the predecrement is by 1,2, or 4, depending 
on whether the operand is a byte, word, or longword, 
respectively. 
Predecrementing and postincrementing are complemen-
tary operations because one undoes the other. Suppose we 
use MOVE D3,—(A2) to store the contents of D3 on a 
stack in memory. MOVE D3,— (A2) decrements A2 
and then copies D3 to the top of the stack pointed at by 
A2. After this instruction has been executed, A2 is 
pointing to the top item on the stack. We can remove 
D3 from the stack and put it in D5 by executing MOVE 
The instruction ADD.W (A0)+, DO adds the number 
pointed at by A0 to the contents of DO and then moves the 
pointer to point to the next number in the sequence. 
Let's look at another example of this postincrementing 
addressing mode. Suppose we have a table of N unsigned 
integer bytes and wish to locate the value of the largest. The 
number of bytes is less than 256. A simple pseudocode algo-
rithm to do this is 
largest = 0 
FOR i = 0 to N-l 
read number^ 
if (number! > largest) THEN largest = numbert 
END FOR 

6.4 Addressing modes 
2 5 5 
This pseudocode uses the notation number, to indicate the 
ith element in a sequence. We can express this in 68K assem-
bly language as 
If we use MOVE.B 
(AO) +, DO, the contents of address 
register AO incremented to 100116 after the character 
located at 1000 has been accessed and we are ready to access 
Next 
Last 
ORG 
EQO 
CLR.B 
MOVE.W 
LEA 
MOVE.B 
CMP.B 
BPL 
MOVE 
DBRA 
STOP 
ORG 
DC.B 
END 
B 
$400 
10 
DO 
#N-1,D1 
List,A0 
<A0)+,D2 
D0,D2 
Last 
D2,D0 
Dl,Next 
#52700 
$1000 
1,4,8,6,2, 
$400 
Assume a dummy value of N = 10 {numbe 
Use DO as largest and set it to 0 
Use Dl as a counter and preset it to 
Use A0 as a pointer to the list 
Read a number 
Is new number > largest? 
If it isn't, go and check for end of 
It is, record the new largest number 
Repeat until count exhausted 
r of elements) 
N-l 
loop 
MOVE.B 
MOVE.B 
ADD.B 
CMP.B 
BNE 
Addressing modes and strings 
A string is a sequence of consecutive characters. We will 
assume the characters are 8-bit ASCII-encoded values. It's 
necessary to indicate a string's size in order to process it. You 
could store a string as n, char_l, char_2,..., char_n, where n 
is the length of the string. For example, the ASCII-encoded 
string 'ABC might be stored in memory as the sequence $03, 
$41, $42, $43. 
You can also use a special termi-
nator or marker to indicate the end 
Again 
of a string. Of course, the termina-
tor must not occur naturally in the 
string. If the terminator is the null 
byte, the string 'ABC would be 
stored as the sequence $41, $42, $43, $00. Some strings use the 
terminator $0D because this is the ASCII code for a carriage 
return. 
The address of a string in memory is usually of the first 
character in the string. Figure 6.16 shows a 10-character 
string located at location 100016 in memory and terminated 
by a null byte (i.e. 0). 
Most microprocessors don't permit direct operations on 
strings (e.g. you can't compare two strings using a single 
instruction). You have to process a string by using byte oper-
ations to access individual characters, one by one. The char-
acters of a string can be accessed by means of address register 
indirect addressing. In Fig. 6.16, address register A0 contains 
the value 1000!6, which is the address or location of the first 
character in the string. 
The operation MOVE . B (A0), DO copies the byte pointed 
at by the contents of address register A0 into data register DO. 
Applying this instruction to Fig. 6.16 would copy the charac-
ter 'T' into data register DO (the actual data loaded into DO is, 
of course, the ASCII code for a letter T ) . 
the next character in the string. Consider the following 
example. 
Counting characters 
Suppose we want to count the 
number of characters in the string pointed at by address 
register A0 and return the string length in D3. The string is 
terminated by the null character, which is included in the 
character count. 
#0,D3 
Preset the character counter to 0 
(A0)+,DO 
Get a character into DO 
#1,D3 
Increment the character counter 
#0,DO 
Is the character a null? 
Again 
Repeat until zero found 
At the end of this code, address register A0 will be pointing 
at the next location immediately following the string. We can 
rewrite this fragment of code. 
Address register 
AO 
1000 
This string begins in memory 
location 1000 and extends 
to location 100A. 
1000 
1001 
1002 
1003 
1004 
1005 
1006 
1007 
1008 
1009 
100A 
Figure 6.16 Example of a string. 
List 
Let's put some test values here 
i,7,6,9,3 
> 
T 
h 
T 
11 
rt 
f >  
mil. 
h 
e 
s 
t 
r 
i 
n 
ft 
£ i  
nul. 

256 
Chapter 6 Assembly language programming 
AO 
i 
1000 
1001 
1002 
1003 
1004 
1005 
1006 
1007 
1008 
1009 
100A 
• 
T 
i 
1000 
1001 
1002 
1003 
1004 
1005 
1006 
1007 
1008 
1009 
100A 
• 
T 
2000 
!> 
1000 
1001 
1002 
1003 
1004 
1005 
1006 
1007 
1008 
1009 
100A 
h 
2001 
tf 
1000 
1001 
1002 
1003 
1004 
1005 
1006 
1007 
1008 
1009 
100A 
e 
2002 
Address register 
1000 
1001 
1002 
1003 
1004 
1005 
1006 
1007 
1008 
1009 
100A 
Address register 
2003 
1000 
* 
1000 
1001 
1002 
1003 
1004 
1005 
1006 
1007 
1008 
1009 
100A 
A1 
2000 
S 
1000 
* 
1000 
1001 
1002 
1003 
1004 
1005 
1006 
1007 
1008 
1009 
100A 
A1 
2000 
S 
2004 
t 
1000 
1001 
1002 
1003 
1004 
1005 
1006 
1007 
1008 
1009 
100A 
t 
2005 
This string begins in memory 
location 1000 and extends 
.-
1000 
1001 
1002 
1003 
1004 
1005 
1006 
1007 
1008 
1009 
100A 
This string begins in memory 
location 2000 and extends 
to location 200A. 
r 
2006 
This string begins in memory 
location 1000 and extends 
! 
1000 
1001 
1002 
1003 
1004 
1005 
1006 
1007 
1008 
1009 
100A 
This string begins in memory 
location 2000 and extends 
to location 200A. 
i 
2007 
to location 100A. 
It 
1000 
1001 
1002 
1003 
1004 
1005 
1006 
1007 
1008 
1009 
100A 
This string begins in memory 
location 2000 and extends 
to location 200A. 
n 
2008 
>: 
1000 
1001 
1002 
1003 
1004 
1005 
1006 
1007 
1008 
1009 
100A 
This string begins in memory 
location 2000 and extends 
to location 200A. 
g 
2009 
nil:. 
1000 
1001 
1002 
1003 
1004 
1005 
1006 
1007 
1008 
1009 
100A 
This string begins in memory 
location 2000 and extends 
to location 200A. 
null 
200A 
Figure 6.17 Comparing two 
strings. 
Again 
CLR.B 
D3 
MOVE.B 
(A0)+,D0 
ADD.B 
#1,D3 
TST.B 
DO 
BNE 
Again 
Preset the character counter to 0 
Get a character into DO 
Increment the character counter 
Is the character a null? 
Repeat until zero found 
The new instruction, TST, tests an operand by comparing 
it with zero and setting the flag bits in the CCR accordingly. 
Counting A's Suppose we want to count the number of times 
'A' occurs in a string that starts at address Find_A. 
Comparing strings Suppose we wish to test whether two 
strings are identical. Figure 6.17 shows two strings in mem-
ory. One is located at 100016 and the other at 200016. In this 
case both strings are identical. 
Next 
Test 
LEA 
F i n d A,A0 
CLR B 
Dl 
MOVE :.B 
(A0)+,DO 
CMP B 
# ' A ' , D 0 
BNE 
T e s t 
ADD B 
#1,D1 
BRA 
Next 
TST B 
DO 
BNE 
Next 
A0 points at the start of the string 
Clear the A's counter 
REPEAT Get a character 
IF 'A' 
THEN increment A's counter 
UNTIL terminator found 
The instruction CMP. B #' A', DO compares the contents 
of DO (i.e. the last character read from the string) with the 
source operand, # ' A'. The # symbol means the actual value 
and the 'A' means the number whose value is the ASCII code 
for the letter A. If you omit the # symbol, the processor will 
read the contents of memory location 41 )6 (because 
'A' = 4116). Because the MOVE instruction sets the CCR, we 
can test for the terminator as soon as we pick up a character, 
as the following code demonstrates. 
In order to compare the strings we have to read a character 
at a time from each string. If, at any point, the two characters 
do not match, the strings are not identical. If we reach two 
null characters, the strings are the same. A0 points at one 
string and Al points at the other. We will set D7 to a zero if the 
strings are not the same, and one if they are the same. 
Next 
LEA 
Find A,A 
CLR.B 
Dl 
MOVE.B 
<A0)+,DO 
BEQ 
E x i t 
CMP.B 
# ' A ' , D 0 
BNE 
Next 
ADD.B 
#1,D1 
BRA 
Next 
AO points at the start of the string 
Clear the A's counter 
REPEAT Get a character 
Exit on null character 
IF 'A' 
THEN increment A's counter 
END REPEAT 
Exit 

6.4 Addressing modes 
257 
Next 
CLR.B 
D7 
MOVE.B 
(A0)+,DO 
CMP.B 
(A1)+,D0 
BNE 
NotSame 
TST.B 
DO 
BNE 
N e x t 
MOVE.B 
# 1 , D 7 
Assume the strings are not the same 
Read a character from the first string 
Compare it with a char from the second string 
IF not the same THEN exit 
REPEAT unit terminator found 
IF terminator found then strings are equal 
NotSame 
Removing spaces A common string manipulation problem is 
the removal of multiple spaces in text If you enter a command 
into a computer like delete X, Y, Z the various component parts 
(i.e. fields) of the command are first analyzed. A command line 
processor might remove multiple spaces before processing the 
command. Figure 6.18 shows how we might go about dealing 
with this problem. On the left, the string has three spaces. On the 
right, the same string has been rewritten with only one space. 
Because the final string will be the same size or shorter than 
the original string, we can simply move up characters when we 
find a multiple space. We can use two pointers, one to point at 
the original string and one to point at the final string. 
We read characters from the string and copy them to their 
destination until a space is encountered. The first space encoun-
tered is copied across. We continue to read characters from the 
source string but do not copy them across if they are further 
spaces. This algorithm requires some care. If we are searching 
for multiple spaces, we will move one character beyond the 
space because of the autoincrementing addressing mode. 
Therefore, we have to adjust the pointer before continuing. 
Figure 6.19 demonstrates the operation of this algorithm. 
By the way, there is a flaw in this program. What happens if 
the end of the string is a space followed by a null? How can 
you fix the problem? 
Next 
Loop 
Exit 
LEA 
LEA 
MOVE.B 
MOVE.B 
TST.B 
BEQ 
CMP.B 
BNE 
MOVE.B 
CMP.B 
BEQ 
LEA 
BRA 
String,AO 
String,Al 
(A0)+,DO 
DO,(Al)+ 
DO 
Exit 
#' ',DO 
Next 
(A0)+,DO 
#' ',D0 
Loop 
-1(AO),A0 
Next 
Set both pointers to the start of the string 
Get a character from the source string 
Copy it to its destination 
If it is a null, then exit 
Was the character a space? 
If not a space, continue copying across 
Get another character from the source string 
Is this a space too? 
Continue until a non-space is found 
Wind the source pointer back to the last char 
Go back to the string 
Exit point 
Address register 
AO 
The string contains three 
spaces between 'The' and 
'test' 
(a) Register AO points 
to the source string. 
null 
Figure 6.18 Removing spaces from a string. 
T 
h 
e 
Address register 
A1 
t 
A1 
t 
e 
s 
t 
null 
(b) Register A1 points 
to the destination string. 
We move only the first 
space in a group. 
T 
h 
e 
t 
e 
s 
t 

258 
Chapter 6 Assembly language programming 
Source string 
AOI 
AO 
A0[_ 
AO 
AOf 
A0|_ 
Destination string 
AO 
A1L 
A1 
A1C 
A 1 [ 
A1L 
AI 
Figure 6.19 Deleting multiple spaces. 
Indexed addressing 
The 68K provides a variant on the address register indirect 
addressing mode called indexed addressing., which uses two 
registers to calculate the effective address of an operand. The 
assembly language form of the effective address is written 
d8 (Ai, xj), where d8 is an 8-bit signed constant forming 
part of the instruction, Ai is one of the eight address registers, 
and Xi is either one of D0-D7 or A0-A7. The effective 
address is calculated from the expression d8 + [Ai] + [Xi]; for 
example, CLR 
28(A3,D6) clears the contents of the 
location whose effective address is given by the contents of A3 
AO points to the head 
of a data structures 
DO 
DO selects a given 
data block in the structure. 
Offset 
The offset selects an 
item within a data Mock. 
Data block 1 
Data block 2 
Data block 3 
Figure 6.20 Indexed addressing—executing 
MOVE.B O f f s e t ( A O , D O ) , D l . 
plus the contents of D6 plus 28, that is, [28 + [A3] + 
[D6]] <- 0. Note that modern 68K assemblers permit 
you to write either TUESDAY (AO, DO) 
or (TUESDAY, 
A0,D0). 
Indexed addressing is a variation on address register indi-
rect addressing. Instead of using one pointer register, the 
effective address is given by the sum of the contents of two 
registers and a displacement. The displacement in indexed 
addressing lies in the range—128 to+127, whereas the 
displacement in address register indirect addressing is 
—32K to +32K. Indexed addressing can be used to access 
two-dimensional tables in which the location of an element is 
specified by its row and its column position. 
Figure 6.20 illustrates MOVE. B Of fset (AO, DO) ,Dl.You 
can regard AO as pointing at the beginning of a data structure. 
In this example, we've shown three blocks of data. By adding 
the contents of DO to AO we can select a specific data block in 
the structure. In the example of Fig. 6.20, the contents of DO 
would be 6 (if each data block occupied 3 bytes). 
By adding a constant to the effective address created by 
adding the two registers, we can access a particular element of 
one of the data blocks. In Fig. 6.20, the offset is 1. 
Consider a data structure representing a diary tfiat consists 
of several weeks, each of which is divided into 7 days. An item 
of data is accessed by locating the head of the data structure, 
counting off the appropriate number of weeks, and then 
accessing the required day. If the location of the array in 
memory is called DIARY and we wish to access the location 
corresponding to Tuesday of week five, we need to access 
location DIARY + (WEEK-1)*7 + Tuesday. If Tuesday = 2, 
the location of the required element is DIARY + (5—1) * 
1 + 2 = DIARY + 30. 
The data structure can be accessed using indexed address-
ing by loading A0 with DIARY and DO with the location of the 
X 
Y 
X 
Y 
X 
Y 
X 
Y 
X 
Y 
X 
Y 
Y 
X 
X 
X 
X 
X 
X 
D1 

6.4 Addressing modes 
259 
PC 
PC 
e 
d l 6 
MOVE fcil&fPC) ,DO 
The operand is 
d16 bytes from 
instruction. 
DO 
Address -' 
[PC] + d16 
Figure 6.21 Program counter relative addressing-
MOVE dl6(PC) ,D0. 
-the effect of 
start of the desired week, and then using the desired day as a 
constant as demonstrated in the following fragment of code. 
dl6 (PC), for example, the operation 'Load data register DO 
relative' is written 
MOVE dl6(PC),D0 
and is defined as [DO] <- [[PC] + dl6]. Asbefore, dl6 is 
a 16-bit two's complement offset that is normally written in 
symbolic form and whose value is calculated by the assem-
bler. Figure 6.21 demonstrates the relationship between the 
PC, the instruction, and the operand address. Figure 6.21 is 
slightly simplified because the 68K's program counter is 
automatically incremented by 2 after the instruction fetch 
phase. 
Relative addressing lets you write position-independent 
code (PIC), which avoids absolute addresses. The machine 
code version of a program written in PIC is independent of 
the physical location of the program in memory. You can 
SUNDAY 
EQU 
0 
MONDAY 
EQU 
1 
TUESDAY 
EQU 
2 
WEDNESDAY EQU 
3 
LEA 
DIARY, AO 
MOVE.L WEEK,DO 
SUB.L 
#1,D0 
MULU 
#7,DO 
MOVE. I 
AO points to head of structure 
DO contains week number 
Calculate (Week - 1)*7 
DO now contains number of days 
TUESDAY(AO,DO),D1 Access the required item 
6.4.3 Relative addressing 
Before we introduce this addressing mode, we'll pose a prob-
lem. Consider the operation MOVE $ 12 3 4, DO, which spec-
ifies the absolute address $1234 as a source operand location. 
If you were to take the program containing this instruction 
and its data and locate it in a different region of memory, 
would it work? No. Why not? Because the data accessed by the 
instruction is no longer in location $1234. The only way to 
run this program is to change all operand addresses to their 
new locations. Relative addressing provides a means of relo-
cating programs without changing addresses. 
Relative addressing is similar to address register indirect 
addressing because the effective address of an operand is 
given by the contents of a register plus a displacement. 
However, relative addressing uses the program counter to cal-
culate the effective address rather than an address register; 
that is, the location of the operand is specified relative to the 
current instruction. The syntax of a 68K relative address is 
move (i.e. relocate) PIC programs in memory without mod-
ifying them, MOVE 3 6 ( PC ), DO means load data register DO 
with the contents of the memory location 36 locations on 
from this instruction. It doesn't matter where the operation 
MOVE 3 6 (PC), DO lies in memory, because the data associ-
ated with it will always be stored in the 36th location follow-
ing the instruction. 
Calculating the displacement required by an instruction 
using program counter relative addressing is difficult. 
Fortunately, you never have to perform this calculation—the 
assembler does it for you. Consider the following example. 
ORG 
$400 
MOVE.B Valuel,DO 
Put Valuel in DO 
MOVE.B Valuel(PC),D1 Put Valuel in Dl 
STOP 
#$2700 
Valuel DC.B 
$23 
END 
$400 
Let's assemble this code and see what happens. 
1 00000400 
ORG 
$400 
2 00000400 10390000040E 
MOVE.B 
Valuel,DO 
3 00000406 123A0006 
MOVE.B 
Valuel(PC),D1 
4 0000040A 4E722700 
STOP 
#$2700 
5 0000040E 23 
Valuel: DC.B 
$23 
6 
00000400 
END 
$400 
;Put Valuel in DO 
;Put Valuel in Dl 

260 
Chapter 6 Assembly language programming 
The address of operand Valuel is $0000040E (as a 32-bit 
longword). The instruction on line 2, MOVE. B Valuel, DO, 
contains an opcode ($1039) and the absolute address of the 
operand ($0000040E). 
Now look at the instruction on line 3, MOVE.B 
Valuel (PC) ,Dl. The opcode is $123A and the operand is 
the 16-bit value $0006. When the 68K reads an instruction, 
the program counter is automatically incremented by 2. Once 
MOVE. B Valuel (PC), Dl has been read from memory, the 
program counter is incremented by 2 from $00000406 to 
$00000408. If we add the offset $0006 to $00000408, we get 
$0000040E, which is the address of the operand Va lue 1. 
You can use relative addressing for source operands, but 
not for destination operands. You can specify where an 
operand comes from but not where it is going to by means of 
relative addressing (this restriction is a 68K design decision 
and not a fundamental limitation). The instructions MOVE 
12(PC),D3 and ADD 
8(PC),D2 are legal instructions, 
whereas MOVE D3,12(PC) and ADD D2,8(PC) are illegal. 
We can write completely position-independent code for 
the 68K by loading the address of the operand into an address 
register using position-independent code and then using 
address register indirect addressing to access operands in 
memory. We use the LEA (load effective address) instruction to 
load an address into an address register. Note that the instruc-
tions MOVEA. L #Temp7 , AO and LEA Temp7, A0 are equiva-
lent. Consider the following examples of this instruction. 
When the instruction LEA Valuel (PC) , A0 is assembled, 
the assembler takes the value of Valuel and subtracts the 
current value of the program counter from it to evaluate the 
offset required by the instruction. 
We now look at one of the most important applications of 
program counter relative addressing, relative branching. 
Relative branching 
We've already met the branch instructions (e.g. BEQ, BNE), 
which can force a branch to the target address. What we 
haven't said is that the target address is expressed relative to 
the current value of the program counter. Most microproces-
sors have a relative branching mode in which the destination 
of a branch instruction is expressed with reference to the cur-
rent address in the program counter. Figure 6.22 illustrates 
relative addressing by means of a memory map; Fig. 6.22(a) 
illustrates the instruction BRA XYZ, Fig. 6.22(b) shows how 
the instruction is encoded, and Fig. 6.22(c) shows a jump 
instruction that performs the same function with an absolute 
address. 
Figure 6.22(a) illustrates BRA XYZ, where XYZ is the target 
address. The machine code form (Fig. 6.22(b)), shows that 
the offset corresponding to XYZ is stored as 4 because the tar-
get address is 4 bytes beyond the end of the branch instruc-
tion. Remember that the program counter is automatically 
incremented by two after the BRA instruction is read during 
an instruction fetch. The programmer doesn't have to worry 
about short and long branches, or about calculating the 
branch offset. If you write BRA ABC, the assembler computes 
the offset as ABC — [PC] — 2. Figure 6.22(c) demonstrates 
the JMP XYZ instruction, which uses an absolute address; that 
is, XYZ is stored as $1006. 
The offset used by a relative branch is an 8-bit signed two's 
complement number in the range —128 to +127. As 2 is 
branch). The 68K also supports a long branch with a 16-bit 
offset that provides a range of —32K to +32K bytes. 
Figure 6.22 also illustrates the importance of relative 
branching in the production of position-independent code. 
The program containing the instruction BRA 
XYZ can be 
relocated merely by moving it in memory, whereas the 
Assembly language form 
LEA $1000,A0 
LEA (AD ,A0 
LEA 10(Al),A0 
LEA 10(PC),A0 
RTL definition 
[A0] 
<- 1000 1 6 
[A0] <- 
[Al] 
[AO] <- 
[Al] 
+ 10 
[A0] 
<- 
[PC] 
+ 10 This lets us generate PIC 
The following fragment of code demonstrates how the 
automatically added to the PC at the start of an instruction, 
LEA instruction can be used to support position independ- 
relative branching is possible within the range -126 to +129 
entcode. 
bytes from the start of the current instruction (i.e. the 
LEA 
Valuel(PC),A0 
Calculate the relative address of 
VALUE1 and store it in A0 
MOVE 
D2, (A0) 
MOVE (A0),D3 
Store D2 at the address pointed at by A0 
Move the word pointed at by A0 to D3 
Valuel DS 
Reserve one word of memory for data 

6.4 Addressing modes 
261 
1000 
1001 
1002 
1003 
1004 
1005 
•XYZ 1006 
1007 
BRA XYZ 
Target 
(a) Assembly language form of 
the branch instruction. A branch 
is made to the instruction at 
target address XYZ. 
60 
04 
Target 
BRA op-code 
+J Offset in 
+;>instructioi 
+3 
+4 
+5 
* 6 
Offset from 
current 
instruction 
Next instruction 
to be executed 
(b) Machine code form of the 
instruction. The stored offset 
is 4 because the destination is 
4 bytes from the end of the 
current instruction. 
1000 
1001 
1002 
1003 
1004 
1005 
1006 
1007 
JMP XYXi 
0000-
1006 
Target 
XYZ is stored 
as the absolute 
'value 00001006 
XYZ 
(c) Equivalent form using 
the JMP instruction with 
an absolute address. 
Figure 6.22 Absolute and 
relative branching. 
program containing JMP XYZ must be modified if it is 
relocated. 
The following program moves a block of data from one 
region of memory to another and provides examples of both rel-
ative branching and relative addressing. The first location of the 
block to be moved is FROM and the first location of its destination 
is TO. The number of words to be moved is given by SI ZE. 
tion is $00 0410 and the address of the operation 
MOVE 
(AO) +, (Al) + is $00 040C. We therefore have to 
branch four locations from the start of the BNE, or six loca-
tions from the end of the BNE. As the CPU always increments 
the PC by 2 at the start of a branch, the stored offset is 
—6. In two's complement form this is $FA (the code is 
$66FA). 
ORG 
$ 4 0 0 
S I Z E 
EQU 
16 
LEA 
FROM(PC) , A0 
LEA 
T O ( P C ) , A 1 
MOVE.B # S I Z E , D 0 
REPEAT MOVE.B 
( A 0 ) + , ( A l ) + 
SUB . B 
# 1 , D 0 
BNE 
REPEAT 
STOP 
#$2700 
ORG 
$001000 
FROM 
D S . B 
16 
TO 
D S . B 
16 
END 
$400 
Start of program to move a data block 
Let's use a 16-byte block 
AO points to the source of the sate 
Al points to the destination 
DO is the loop counter 
REPEAT Move byte from source to destination 
UNTIL all bytes moved 
Locate source and destination blocks here 
In Fig. 6.23 the instruction BNE REPEAT causes a branch 
backwards to instruction MOVE (AO) +, (Al) + in the event 
of the zero bit in the CCR not being set. From the memory 
map of Fig. 6.23, we see that the address of the branch opera-
Note how we use relative addressing to load the address of 
the source and destination blocks into address registers A0 
and Al, respectively. This program can be assembled to give 
the following. 
1 00000400 
ORG 
$400 
2 
00000010 
SIZE: 
EQU 
16 
3 00000400 41FA0BFE 
LEA 
FROM(PC),A0 
4 00000404 4 3FA0C0A 
LEA 
TO(PC),Al 
5 00000408 103C0010 
MOVE.B 
#SIZE,D0 
6 0000040C 12D8 
REPEAT: 
MOVE.B 
(A0)+, (Al) + 
7 0000040E 5300 
SUB . B 
#1,D0 
8 00000410 6 6FA 
BNE 
REPEAT 
9 00000412 4E722700 
STOP 
#$2700 
10 
* 
11 00001000 
ORG 
$001000 
12 00001000 00000010 
FROM: 
DS.B 
16 
13 00001010 00000010 
TO: 
DS.B 
16 
14 
00000400 
END 
$400 
Start of program 
Let's use a 16-byte block 
A0 points to the source 
Al points to the destination 
DO is the loop counter 
REPEAT move byte from source to destination 
;UNTIL all bytes moved 
rLocate source and destination blocks here 

262 
Chapter 6 Assembly language programming 
000400 
^ ^ ^ - 0 0 0 4 0 2 ) 
/
^ 
000404 
00040$/ 
V „ (T\ 
4 
JXXM0& 
/ 
00040A 
/REPEAT 00040C 
/ 
00040E 
/ 
000410 
000412 
000414 
^ — • 0 0 1 0 0 0 
001010 
41 FA 
LEA 
LEA 
MOVE.B 
MOVE.B 
SUB.B 
BNE 
STOP 
K^FROM 
FROM(PC),AO 
000400 
^ ^ ^ - 0 0 0 4 0 2 ) 
/
^ 
000404 
00040$/ 
V „ (T\ 
4 
JXXM0& 
/ 
00040A 
/REPEAT 00040C 
/ 
00040E 
/ 
000410 
000412 
000414 
^ — • 0 0 1 0 0 0 
001010 
^JBFjJ) 
LEA 
LEA 
MOVE.B 
MOVE.B 
SUB.B 
BNE 
STOP 
K^FROM 
000400 
^ ^ ^ - 0 0 0 4 0 2 ) 
/
^ 
000404 
00040$/ 
V „ (T\ 
4 
JXXM0& 
/ 
00040A 
/REPEAT 00040C 
/ 
00040E 
/ 
000410 
000412 
000414 
^ — • 0 0 1 0 0 0 
001010 
/ 
43FA 
LEA 
LEA 
MOVE.B 
MOVE.B 
SUB.B 
BNE 
STOP 
K^FROM 
TO(PC),A1 
000400 
^ ^ ^ - 0 0 0 4 0 2 ) 
/
^ 
000404 
00040$/ 
V „ (T\ 
4 
JXXM0& 
/ 
00040A 
/REPEAT 00040C 
/ 
00040E 
/ 
000410 
000412 
000414 
^ — • 0 0 1 0 0 0 
001010 
OCOA 
LEA 
LEA 
MOVE.B 
MOVE.B 
SUB.B 
BNE 
STOP 
K^FROM 
000400 
^ ^ ^ - 0 0 0 4 0 2 ) 
/
^ 
000404 
00040$/ 
V „ (T\ 
4 
JXXM0& 
/ 
00040A 
/REPEAT 00040C 
/ 
00040E 
/ 
000410 
000412 
000414 
^ — • 0 0 1 0 0 0 
001010 
103C 
LEA 
LEA 
MOVE.B 
MOVE.B 
SUB.B 
BNE 
STOP 
K^FROM 
#SIZE,D0 
000400 
^ ^ ^ - 0 0 0 4 0 2 ) 
/
^ 
000404 
00040$/ 
V „ (T\ 
4 
JXXM0& 
/ 
00040A 
/REPEAT 00040C 
/ 
00040E 
/ 
000410 
000412 
000414 
^ — • 0 0 1 0 0 0 
001010 
0010 
LEA 
LEA 
MOVE.B 
MOVE.B 
SUB.B 
BNE 
STOP 
K^FROM 
When LEA FROM ( PC S , AO 
is executed, the PC contains 
000400 
^ ^ ^ - 0 0 0 4 0 2 ) 
/
^ 
000404 
00040$/ 
V „ (T\ 
4 
JXXM0& 
/ 
00040A 
/REPEAT 00040C 
/ 
00040E 
/ 
000410 
000412 
000414 
^ — • 0 0 1 0 0 0 
001010 
12D8 
LEA 
LEA 
MOVE.B 
MOVE.B 
SUB.B 
BNE 
STOP 
K^FROM 
( A 0 ) + , ( A l ) + 
When LEA FROM ( PC S , AO 
is executed, the PC contains 
000400 
^ ^ ^ - 0 0 0 4 0 2 ) 
/
^ 
000404 
00040$/ 
V „ (T\ 
4 
JXXM0& 
/ 
00040A 
/REPEAT 00040C 
/ 
00040E 
/ 
000410 
000412 
000414 
^ — • 0 0 1 0 0 0 
001010 
5300 
LEA 
LEA 
MOVE.B 
MOVE.B 
SUB.B 
BNE 
STOP 
K^FROM 
#1,D0 
00000402. The offset 'FROM' 
is OBFE. These are added 
000400 
^ ^ ^ - 0 0 0 4 0 2 ) 
/
^ 
000404 
00040$/ 
V „ (T\ 
4 
JXXM0& 
/ 
00040A 
/REPEAT 00040C 
/ 
00040E 
/ 
000410 
000412 
000414 
^ — • 0 0 1 0 0 0 
001010 
66FA 
LEA 
LEA 
MOVE.B 
MOVE.B 
SUB.B 
BNE 
STOP 
K^FROM 
REPEAT 
to get 00000402+ 0BFE = 
00001000, which is loaded 
000400 
^ ^ ^ - 0 0 0 4 0 2 ) 
/
^ 
000404 
00040$/ 
V „ (T\ 
4 
JXXM0& 
/ 
00040A 
/REPEAT 00040C 
/ 
00040E 
/ 
000410 
000412 
000414 
^ — • 0 0 1 0 0 0 
001010 
4E72 
LEA 
LEA 
MOVE.B 
MOVE.B 
SUB.B 
BNE 
STOP 
K^FROM 
#$2700 
into AO 
000400 
^ ^ ^ - 0 0 0 4 0 2 ) 
/
^ 
000404 
00040$/ 
V „ (T\ 
4 
JXXM0& 
/ 
00040A 
/REPEAT 00040C 
/ 
00040E 
/ 
000410 
000412 
000414 
^ — • 0 0 1 0 0 0 
001010 
2700 
LEA 
LEA 
MOVE.B 
MOVE.B 
SUB.B 
BNE 
STOP 
K^FROM 
000400 
^ ^ ^ - 0 0 0 4 0 2 ) 
/
^ 
000404 
00040$/ 
V „ (T\ 
4 
JXXM0& 
/ 
00040A 
/REPEAT 00040C 
/ 
00040E 
/ 
000410 
000412 
000414 
^ — • 0 0 1 0 0 0 
001010 
- 
- ^ ^ i 
LEA 
LEA 
MOVE.B 
MOVE.B 
SUB.B 
BNE 
STOP 
K^FROM 
000400 
^ ^ ^ - 0 0 0 4 0 2 ) 
/
^ 
000404 
00040$/ 
V „ (T\ 
4 
JXXM0& 
/ 
00040A 
/REPEAT 00040C 
/ 
00040E 
/ 
000410 
000412 
000414 
^ — • 0 0 1 0 0 0 
001010 
LEA 
LEA 
MOVE.B 
MOVE.B 
SUB.B 
BNE 
STOP 
K^FROM 
000400 
^ ^ ^ - 0 0 0 4 0 2 ) 
/
^ 
000404 
00040$/ 
V „ (T\ 
4 
JXXM0& 
/ 
00040A 
/REPEAT 00040C 
/ 
00040E 
/ 
000410 
000412 
000414 
^ — • 0 0 1 0 0 0 
001010 
LEA 
LEA 
MOVE.B 
MOVE.B 
SUB.B 
BNE 
STOP 
K^FROM 
•./•; AC 
000400 
^ ^ ^ - 0 0 0 4 0 2 ) 
/
^ 
000404 
00040$/ 
V „ (T\ 
4 
JXXM0& 
/ 
00040A 
/REPEAT 00040C 
/ 
00040E 
/ 
000410 
000412 
000414 
^ — • 0 0 1 0 0 0 
001010 
*TO 
•./•; AC 
000400 
^ ^ ^ - 0 0 0 4 0 2 ) 
/
^ 
000404 
00040$/ 
V „ (T\ 
4 
JXXM0& 
/ 
00040A 
/REPEAT 00040C 
/ 
00040E 
/ 
000410 
000412 
000414 
^ — • 0 0 1 0 0 0 
001010 
*TO 
000400 
^ ^ ^ - 0 0 0 4 0 2 ) 
/
^ 
000404 
00040$/ 
V „ (T\ 
4 
JXXM0& 
/ 
00040A 
/REPEAT 00040C 
/ 
00040E 
/ 
000410 
000412 
000414 
^ — • 0 0 1 0 0 0 
001010 
*TO 
000400 
^ ^ ^ - 0 0 0 4 0 2 ) 
/
^ 
000404 
00040$/ 
V „ (T\ 
4 
JXXM0& 
/ 
00040A 
/REPEAT 00040C 
/ 
00040E 
/ 
000410 
000412 
000414 
^ — • 0 0 1 0 0 0 
001010 
*TO 
^ \ A 1 ...'••'. 
000400 
^ ^ ^ - 0 0 0 4 0 2 ) 
/
^ 
000404 
00040$/ 
V „ (T\ 
4 
JXXM0& 
/ 
00040A 
/REPEAT 00040C 
/ 
00040E 
/ 
000410 
000412 
000414 
^ — • 0 0 1 0 0 0 
001010 
^ \ A 1 ...'••'. 
000400 
^ ^ ^ - 0 0 0 4 0 2 ) 
/
^ 
000404 
00040$/ 
V „ (T\ 
4 
JXXM0& 
/ 
00040A 
/REPEAT 00040C 
/ 
00040E 
/ 
000410 
000412 
000414 
^ — • 0 0 1 0 0 0 
001010 
The LEA instructions 
load pointer registers 
AO and A1 with the target 
addresses. 
Figure 6.23 Moving a block of data in memory. 
RELATIVE ADDRESSING—A SUMMARY 
Relative addressing is used to specify the location of an 
operand with respect to the program counter. This addressing 
mode means that code and its data can be moved in memory 
without having to recompute operand addresses because the 
data is the same distance from the code it accesses 
irrespective of where the code is located. Relative addressing 
is also used with branch instructions because the target 
address is expressed as the number of bytes from the current 
instruction. 
6.5 The stack 
We now look at one of the most important data structures in 
computer science, the stack, and describe the facilities pro-
vided by the 68K to support the stack (we provided a basic 
introduction in the previous chapter). A stack is a last-
in-first-out queue with a single end, where items are added or 
removed. Unlike a conventional first-in-first-out queue 
(FIFO), the stack has only one end. The stack expands as 
items are added to it and contracts as they are removed. Items 
are removed from the stack in the reverse order to which they 
are entered. The point at which items are added to, or 
removed from, the stack is called the top of stack (TOS). The 
next position on the stack is referred to as next on stack 
(NOS). When an item is added to the stack it is said to be 
pushed on to the stack, and when an item is removed from the 
stack it is said to be pulled (or popped) off the stack. 
Figure 6.24 presents a series of diagrams illustrating the 
operation of a stack as items A, B, C, D, and E, are added to it 
and removed from it. 
Before we look at the stack's role in subroutines, we must 
mention die stack-based architecture that has been imple-
mented by some special-purpose computers and by some 
experimental machines. Suppose a computer can transfer 
data between memory and the stack and perform monadic 
operations on the top item of the stack, or dyadic operations 
on the top two items of the stack. A dyadic operation (e.g. +, 
*, AND, OR) removes the top two items on die stack and 
pushes the result of the operation. 
Figure 6.25 shows how an ADD instruction is executed by a 
stack-based computer. Figure 6.25(a) demonstrates a system 

6.5 The stack 
2 6 3 
stack 
C 
B 
B 
A 
A 
A 
(a) Stack empty, 
(b) Push A. 
(c) Push B. 
(d) Push C. 
(e) Push D. 
D 
E 
C 
C 
C 
C 
B 
B 
B 
B 
A 
A 
A 
A 
(f) Pull D. 
(i) Pull C. 
(j) puH 1 
Figure 6.24 The stack. 
(g) Push E. 
(stack empty) 
(k) Pull A. 
(h) Pull E. 
with four data elements on the stack. When the ADD is exe-
cuted, the element at the top of the stack is pulled 
(Fig. 6.25(b)) and sent to the adder. The next element (i.e. C, 
the old NOS) is now the new TOS. In Fig. 6.25(c) the element 
at the top of stack is pulled and sent to the adder. Finally, the 
output of the adder, D + C, is pushed onto the stack to create 
a new TOS. 
Note how this ADD instruction doesn't have an operand 
unlike all the instructions we've described so far. A stack-
based computer has so-called addressless instructions 
because they act on elements at the top of the stack. 
The following example illustrates the evaluation of the 
expression (A + B ) ( C - D ) on a hypothetical stack-based 
computer. We assume that the instruction PUSH pushes the 
contents of DO onto the stack, ADD , SUB, and MULU all act on 
the top two items on the stack, and PULL places the top item 
on the stack in DO. 
Current top of stack 
contains element D 
(a) Initial state of the stack with four items. 
New top of stack 
(b) First element pulled off the stack. 
(c) Second element pulled off the stack. 
1. 
2. 
MOVE 
PUSH 
A, DO 
3. 
4. 
MOVE 
PUSH 
B, DO 
5. ADD 
6. MOVE C DO 
7. PUSH 
8. MOVE D DO 
9. PUSH 
10. SUB 
11. MULU 
12. PULL 
Get A in DO 
Push it on the stack 
Get B in DO 
Push it on the stack 
Pull the top two items off the stack, add them, and push the result 
Get C in DO 
Push it on the stack 
GetD 
Push it on the stack 
Pull the top two items off the stack, subtract them, and push the result 
Pull the top two items off the stack, multiply them, and push the result 
Pull the result off the stack and put it in DO 
(d) Result pushed on the stark 
Figure 6.25 Executing an A D D operation on a stack machine. 
on the stack in the way we've just described (e.g. ADD , SUB, 
MULU), special-purpose microprocessors have been designed 
to support stack-based languages. The 68K implements 
instructions enabling it to access a stack, although it's not a 
stack machine. Pure stack machines 
do exist, although they have never 
been developed to the same extent as 
the two-address machines like the 
68K and Pentium. 
6.5.1 The 68K stack 
Figure 6.26 represents the state of the stack at various 
stages in the procedure. The number below each diagram 
corresponds to the line number in the program. Although the 
68K and similar microprocessors do not permit operations 
A hardware stack can be implemented 
as a modified shift register. When 
such a stack is implemented in hard-
ware, the addition of a new item to the top of stack causes all 
other items on the stack to be pushed down. Similarly, when 
an item is removed from the stack, the NOS becomes TOS 
and all items move up. 
C 
' 
B 
A 
B 
A 
D 
(Adder 
' I 
1 
_D__J—-
c 
—* 
C+D 
• 
B 
A 
r " 
Adder 
i 
H Adder 
D 
— 
D. 
C 
B 
A 
B 
A 
•A 
G 

2 6 4 
Chapter 6 Assembly language programming 
D 
B 
C 
C 
C--D 
A 
A 
A + B 
A + 3 
A+B 
A + B 
(A + B){C-D) 
Step 2 
Step 4 
Push A 
Push B 
StepS 
Add 
Step 7 
Push C 
Step 9 
Step 10 
Push D 
Subtract 
Step 11 
Multiply 
Figure 6.26 Executing a 
program on a stack machine. 
SP 
im 
Memory 
Step 2 
Memory 
r*-
D 
SP 
r*-
C 
JSP-2) 
r*-
A+B 
Step 9 
Memory 
SP 
B 
fSP-1] — • 
A 
Step 4 
Memory 
SP 
["*" 
JSP-1J -> 
C-D 
A+B 
Step 10 
SP 
Memory 
(SP] | — ¥ 
A+B 
Step 5 
Memory 
SP 
[SP] —» <A+B)x 
fc+D) 
Step 11 
Memory 
SP 
r * 
ISP—11 H 
A+B 
Step 7 
Figure 6.27 Executing the 
program of Fig. 6.26 on a 
machine with a stack pointer. 
THE T W O 68K STACK POINTERS 
The 68K has t w o A7 registers and, therefore, t w o system stack 
pointers. One A7 is the supervisor stack pointer and is 
associated with the operating system. The other is the user 
stack pointer and is associated with programs running under 
the operating system. The operating system controls the 
allocation of the computer's resources (memory and I/O), and 
is protected from errors caused by the less reliable user 
programs. A stack pointer dedicated solely to the operating 
system prevents user programs accessing and possibly 
corrupting the operating system's stack. Only one of these 
two A7s is accessible at a time, because the 68K is either 
running an operating system or it isn't. 
Microprocessors don't implement a stack in this way and 
the items already on the stack don't move as new items 
are pushed and old ones pulled. The stack is located in a 
region of the main store and a stack pointer points to the 
top of the stack. This stack pointer points at the top of 
stack as the stack grows and contracts. In some micro-
processors, the stack pointer points to the next free location 
on the stack, whereas in others, it points to the current top 
of stack. 
Figure 6.27 demonstrates how the program illustrated in 
Fig. 6.26 is executed by a computer with a stack in memory 
and a stack pointer, SP. 
The 68K doesn't have a special system stack pointer—it 
uses address register A7. We call A7 the system stack pointer 
because the stack pointed at by A7 stores return addresses 
during subroutine calls. Assemblers let you write either A7 or 
SP; for example, MOVE, w DO, (A7) and MOVE, w DO, (SP) 
are equivalent. The 68K can maintain up to eight stacks 
simultaneously, because all its address registers can be used as 
stack pointers. 
In what follows, we use the 68K's stack pointer to illustrate 
the operation of a stack. You might expect the assembly lan-
guage instruction that pushes the contents of DO on the stack 
to be PUSH DO, and the corresponding instruction to pull an 
item from the stack and put it in DO to be PULL DO. Explicit 
PUSH and PULL instructions are not provided by the 68K. You 
can use address register indirect with predecrementing 
addressing mode to push, and address register indirect with 
postincrementing addressing mode to pull. 
Figure 6.28 illustrates the effect of a PUSH DO instruction, 
which is implemented by MOVE.W DO, — (SP), and PULL 
DO, which is implemented by MOVE, w (SP) +, DO. The 68K's 
stack grows towards lower addresses as data is pushed on it; 
for example, if the stack pointer contains $80014C and a 
word is pushed onto the stack, the new value of the stack 
pointer will be $80014A. 

6.5 The stack 
265 
The 68K's push operation MOVE. w DO, - (SP) is denned 
in RTL as 
[SP] «- [SP] - 2 
[[SP]] <- [DO] 
Predecrement stack pointer t o point to next free element 
Copy contents of DO to the stack 
and the 68K's pull operation 
MOVE.W (SP) + ,D0 is 
defined as 
[DO] <- [[SP]] 
[SP] <- [SP] + 2 
Copy t h e element on top of t h e s t a c k t o DO 
Postincrement t h e s t a c k p o i n t e r t o p o i n t t o t h e new TOS 
Push and pull operations use word or longword operands. 
A longword operand automatically causes the SP to be 
decremented or incremented by 4. Address registers AO to A6 
may be used to push or pull byte, . B, operands—but not the 
N - 4 
A7 
N-2 
N 
N 
, Top of stack 
N 
Top of stack 
Stack pointe r 
N + 2 
N + 6 
N + 8 
Stack 
. 
N - 4 
A7 
, - J ^ i * Top of stack 
N - 2 
J N 
Stack pointe r 
N + 2 
N + 6 
N + 8 
U—: 
^ 
(a) Snapshot of the 68K's stack. 
Figure 6.28 The 68K's stack. 
(b) State of the stack after pushing a 
word by MOVE.W D 0 , - ( A 7 ) . 
A7 
Memory 
Stack pointer 
' 
2200 
—*• 
Top of stack 
— 
-— ^ 
Memory 
2200 
Stack 
Stack pointer 
A7 
21DC 
DO 
2 IDC 
/ 
Dl 
21E0 
Registers DO to D5/ 
D2 
21E4 
and A2 to A5 are 
D3 
21E8 
dumped on the stack 
D4 
21EC 
D5 
21F0 
A2 
21F4 
A3 
21F8 
A4 
21FC 
3 
A5 
2200 
(a) Initial state of stack. 
Figure 6.29 The 68K's stack. 
(b) State of stack after MOVEM . L DO -D5 / 
A2-A5,-(A7). 
system stack pointer, A7. The reason for this restriction is 
that A7 must always point at a word boundary on an even 
address (this is an operational restriction imposed by the 
68K's hardware). 
The 68K's stack pointer is decre-
mented before a push and incremented 
after a pull. Consequently, the stack 
pointer always points at the item at the 
top of the stack; for example, 
MOVE ( SP ) +, D3 pulls the top item off 
the stack and deposits it in D3. Note 
that MOVE (SP) ,D3 copies the TOS 
into D3 without modifying the stack 
pointer. 
When the stack shrinks after a 
MOVE . w 
( SP ) +, DO operation, items 
on the stack are not physically deleted; 
they are still there in the memory until 
overwritten 
by, for 
example, a 
MOVE.W DO, - (SP) operation. 
The stack can be used as a temp-
orary 
data 
store. 
Executing 
a 
MOVE.W DO, — (SP) saves the contents 
of DO on the stack, and executing a 
MOVE . W ( SP) +, DO returns the con-
tents of DO. The application of the stack 
as a temporary storage location avoids 
storing data in explicitly named mem-
ory locations. More importantly, if fur-
ther data is stored on the stack, it does 
not overwrite the old data. 
The 68K has a special instruction 
called move multiple registers (MOVEM), 
which saves or retrieves an entire group 
of registers. For example MOVEM.L 
D0-D7/A0-A7, - (A7) pushes all 
registers on the stack pointed at by A7. 
The register list used by MOVEM is writ-
ten in the form Di-Dj/ Ap-Aq and 

266 
Chapter 6 Assembly language programming 
SIMULATED INPUT AND OUTPUT 
Microprocessors cannot perform input operations from the 
keyboard or display data on the screen. To do that requires an 
operating system, peripherals, and their drivers. 
In order to allow the assembly language programmer to 
write programs that do have input or output operations, 
simulators provide an I/O mechanism. Both EASy68K and the 
Teesside simulator use the 68K's TRAP #15 instruction.This 
instruction is a request to the operating system to provide a 
facility such as input or output. Because there are many 
operations that the operating system can perform, you need 
to tell the operating system what you want. EASy68K and the 
Teesside simulator use data register DO to hold a parameter 
that defined the requested operation; for example, if you load 
DO with 5, the request is for input. 
This I/O mechanism is specific to EASy68K and the Teesside 
simulator. It is part of their environment and will not work 
with any other 68K system. 
specifies data registers Di to Dj inclusive and address registers 
Ap to Aq inclusive. Groups of registers are pulled off the stack 
by, for example, MOVEM.L 
(A7) +.D0-D2/D4/A4-A6. 
The most important applications of the stack are in the 
implementation of subroutines (discussed in the following 
section) and in the handling of interrupts. When autodecre-
menting is used, registers are stored in the order A7 to AO 
then D7 to DO with the highest numbered address register 
being stored at the lowest address. Figure 6.29 illustrates the 
effect of MOVEM.L D 0 - D 5 / A 2 - A 5 , - (A7). 
6.5.2 The stack and subroutines 
A subroutine is called by the instruction BSR < label > or JSR 
<label>, where BSR means branch to subroutine and JSR 
means jump to subroutine. The difference between BSR and 
JSR is that BSR uses a relative address and JSR an absolute 
address. Remember that the programmer simply supplies the 
label of the subroutine and the assembler automatically calcu-
lates the appropriate relative or absolute address. To call a sub-
routine ABC, all we have to do is write either BSR ABC or JSR 
ABC. The BSR is preferred to J S R because it permits the use of 
position-independent code. The range of branching with BSR is 
—32 kbytes to +32 kbytes from the present instruction. JSR 
uses an absolute address and cannot therefore be used to gen-
erate position-independent code. JSR may use an address reg-
ister indirect address; for example, JSR 
(AO) calls the 
subroutine whose address is in AO. 
Using subroutines—an example 
We now look at an example of how subroutines are used. The 
following program inputs text from the keyboard and stores 
successive characters in a buffer in memory until an @ sym-
bol is typed. When an @ is encountered, the text is displayed 
on the screen. In this simple example, we don't test for buffer 
overflow. 
In this example we use the character input and output 
mechanisms built into both EASy68K and the Teesside 68K 
simulators. All I/O is performed by means of a TRAP #15 
instruction, which is a call to the operating system. We have't 
yet covered the 68K's TRAP instructions, but all we need say 
here is that a TRAP calls a function that forms part of the 
computer's operating system. Before the TRAP is executed, 
you have to tell the O/S what operation you want by putting a 
parameter in data register DO. A '5' indicates character input 
and a '6' indicates character output. When a character is 
input, it is deposited in Dl. Similarly, the character in Dl is 
displayed by the output routine. 
We can express the algorithm is pseudocode as follows. 
Initialize a pointer to point to the top of the character buffer 
REPEAT 
Read a character from the keyboard 
Store it in the buffer at the address given by the pointer 
Update the pointer 
UNTIL character = "@" 
Reset the pointer to point to the top of the character buffer 
REPEAT 
Read a character from the buffer at the address given by the pointer 
IF character = "8" THEN exit 
Display the character on the screen 
Update the pointer 
UNTIL exit 

6.5 The stack 
267 
In the following program, the BUFFER is a region of memory 
reserved for the data to be stored. 
When an RTS instruction is encountered at the end of a 
subroutine, the longword address on the top of the stack is 
NEXTIN 
PRINT 
NEXTOUT 
DONE 
* 
GET CHAR 
ORG 
LEA 
BSR 
MOVE.B 
CMP.B 
BNE 
LEA 
MOVE.B 
CMP.B 
BEQ 
BSR 
BRA 
STOP 
$000400 
BUFFER(PC) 
GET_CHAR 
Dl, (A0) + 
#•e•,DI 
NEXTIN 
BUFFER(PC) 
(A0)+,D1 
#'@',Dl 
DONE 
PUT CHAR 
NEXTOUT 
#$2700 
MOVE.B #5,DO 
TRAP 
#15 
RTS 
Define the origin for data 
A0 Preset A0 as a pointer register 
Get a character 
Store character and move pointer to next 
IF character = '@* THEN print 
ELSE repeat 
A0 Reset pointer to start of buffer 
Get a character and update pointer 
IF character = '@' THEN EXIT 
ELSE print character 
Repeat 
Halt the 68K 
Input routine (code = 5) 
Load input command in DO and call O/S 
Return 
PUT CHAR MOVE.B #6, DO 
TRAP 
#15 
RTS 
Output routine (code = 6) 
Load output command in DO and call O/S 
Return 
BUFFER 
ORG 
$500 
DS.B 
40 
END 
$400 
Reserve 40 bytes of storage 
The instruction CMP. B #' @', Dl compares the contents 
of the lower-order byte of data register Dl with the byte 
whose ASCII code corresponds to the symbol @. The instruc-
tion LEA BUFFER (PC) , AO generates 
position-independent 
code because it calculates the address of the buffer relative to 
the program counter. Had we written LEA BUFFER, AO, the 
code would not have been position independent. 
pulled and placed in the program counter in order to force a 
return to the calling point. The following code is produced 
by assembling this program. We will need this output 
when we trace the program (in particular the addresses 
of the subroutines and the return addresses of subroutine 
calls). 
1 00000400 
ORG 
$000400 
; 
2 00000400 41FA0OFE 
LEA 
BUFFER(PC),A0 
; 
3 00000404 61000022 
NEXTIN: 
BSR 
GET_CHAR 
4 00000408 10C1 
MOVE.B 
Dl, (A0) + 
5 0000040A 0C010040 
CMP.B 
#'@',Dl 
6 0000040E 66F4 
BNE 
NEXTIN 
7 00000410 41FA00EE 
PRINT: 
LEA 
BUFFER(PC),A0 
; 
8 00000414 1218 
NEXTOUT: MOVE.B 
(A0)+,D1 
9 00000416 0C010040 
CMP.B 
# ' @',Dl 
10 0000041A 67000008 
BEQ 
DONE 
11 0000041E 61000010 
BSR 
PUT_CHAR 
12 00000422 60F0 
BRA 
NEXTOUT 
; 
13 
14 
15 
00000424 4E722700 
DONE: 
STOP 
#$2700 
13 
14 
15 
00000428 103C0005 
GET_CHAR: MOVE. B 
#5, DO 
16 0000042C 4E4F 
TRAP 
#15 
17 0000042E 4E75 
RTS 
; 
18 
* 
19 00000430 103C0006 
PUT_CHAR: MOVE.B 
#6, DO 
20 00000434 4E4F 
TRAP 
#15 
21 00000436 4E75 
RTS 
; 
22 
* 
23 00000500 
ORG 
$500 
24 00000500 00000028 
BUFFER: 
DS.B 
40 
25 
00000400 
END 
$400 
Define the origin for data 
Preset A0 as pointer 
register 
Get a character 
Store character and move pointer to next 
IF character = '%' THEN print 
ELSE repeat 
Reset pointer to start of buffer 
Get a character and update pointer 
IF character = '@' THEN EXIT 
ELSE print character 
Repeat 
Halt the 68K 
Input routine 
Load input command in DO and call O/S 
Return 
Output routine 
Load output command in DO and call O/S 
Return 
/Reserve 40 bytes of storage 

268 
Chapter 6 Assembly language programming 
The following trace output demonstrates the flow of 
control as subroutine calls and subroutine returns are 
made—when you read the trace, look at the program counter 
and the stack pointer (A7 = SS). Remember that the PC is 
incremented between 2 and 10 bytes after each instruction. 
We've set A0 to point to the buffer for input data. The next 
instruction calls the subroutine to input a character. Note the 
change in the PC to $428. 
PC=000428 SR=2000 SS=009FFFJC US=00000000 
X=0 
A0=00000500 A1=00000000 A2=00TTtn»&e—£3^00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7^CTD?FFFEC_^0 
D0=00000000 D1=00000000 D2=00000000 D3=00000000 V^0~" 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.B 
#$05,DO 
PC=00042C SR=2000 SS=009FFFFC US=00000000 
X=0 
A0=00000500 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=009FFFFC Z=0 
D0=00000005 D1=00000000 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>TRAP 
#$0F 
0 0 9FFFFC:00000408 s 
Note the stack pointer 
has moved up 4 bytes 
and the return address 
is on the stack. 
009FFFFC:00000408 s 
This is the character entered from the keyboard and captured by the TRAP #15. 
C=00042E SR=2000 SS=009FFFFC US=00000000 
X=0 
A0=00000500 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=009FFFFC Z=0 
D0=00000005 D1=0000005A D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>RTS 
009FFFFC:00000408 s 
Having got the input (in this case Z) in Dl, we return from 
the subroutine. Watch the program counter again. It is 
currently $42E and will be replaced by $408 (i.e. the address 
of the instruction after the subroutine call. 
PC=000408 SR=2000 SS=O0AO0O0O US=00000000 
X=0 
A0=00000500 A1=00000000 A2-00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=O0A00O00 Z=0 
D0=00000005 D1=0000005A D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.B 
D1,(A0)+ 
PC=000400^5g=2000 SS=00AO00OO US=00000000 
X=0 
AO = OOOOOOOOAT5l>eAQiiOOO A2 = 00000000 A3 = 00000000 N=0 
A4=00000000 A 5 = 0 0 0 0 0 0 ? t r f t ^ i a £ 0 0 0 0 0 0 
A7=00A00000 Z=0 
D0 = 00000000 
D1 = 00000000 D2 = 0OO0TJtT&«-J2^=0000000O V=n 
. 
D4 = 00000000 D5=00000000 
D6 = 00000000 D7^ITCrfr9*LCLQ0C= Note the position independent 
>LEA.L 
$FE(PCJ 
AO 
" 
code. The value of the PC when 
^~ i 
, 
LEA $FE(PC),AO is executed 
PC=OOO404 
SR=20OO SS=OOAOOOOO us=oooooooo 
x= '^f402^ Th^°"se*,s°0FEi6
/;so 
^ 
the value loaded into AO is 
a n - n n n n n R n n « r n 
n n n n n n n n 
i n . n n n n n n n n 
3 V f l 0 0 0 0 0 0 0 
N= 
^2 
+FE =500,6 
A4 = 00000000 A5 = 00000000 A6=00000000 A7 = 00AOOOO0 Z = L _ _ ! ! _ J _ _ 
D0=00000000 D1=00000000 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>BSR.L 
$0428 

6.5 The stack 
269 
We now store the character in Dl in memory and 
increment the pointer in AO. 
PC=00040A SR=2000 SS=00A00000 US=00000000 
X=0 
A0=00000501 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00A00000 Z=0 
D0=00000005 D1=0000005A D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>CMPI.B 
#$40,Dl 
PC=00040E SR=2000 SS=00A00000 US=00000000 
X=0 
A0=00000501 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00A00000 Z=0 
D0=00000005 D1=0000005A D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 0 0 
>BNE.S 
$0404 
We test the character in D1 for equality with '@' = $40 and 
then branch back to $0404 if we haven't input an '@'. 
PO000404 SR=2000 SS=00A00000 US=00000000 
A0=00000501 A1=00000000 A2=00000000 A3=00000000 
A4=00000000 A5=00000000 A6=00000000 A7=0OAOO0OO 
D0=00000005 D1=0000005A D2=00000000 D3=00000000 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 
>BSR.L 
$0428 
We haven't, so we continue by reading another character. 
PC-000428 SR=2000 SS=009FFFFC US=00000000 
A0=00000501 A1=00000000 A2=00000000 A3=00000000 
A4=00000000 A5=00000000 A6=00000000 A7=009FFFFC 
D0=00000005 D1=0000005A D2=00000000 D3=00000000 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 
>MOVE.B 
#$05,DO 
To avoid more tracing, we'll jump ahead to the point at 
which a '@' has been input in DO. 
PO00042C SR=2000 SS=009FFFFC US=00000000 
X=0 
009FFFFC:00000408 s 
A0=00000501 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=009FFFFC Z=0 
D0=00000005 D1=0000005A D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>TRAP 
#$0F 
PC=OO^M2X^R^2TTO«--SS^I1£9TFFFC US=00000000 
X=0 
009FFFFC: 00000408 s 
A0=00000502 Al=00000000~A2^0lKTTO^e«-Aa£00000000 N=0 
Here's the '@' that 
we entered and its 
ASCII value in Dl. 
PC=000408 SR=2000 SS^OOAOOOOO OS=00000000 
X=0 
A0=00000502 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00A00000 Z=0 
D0=00000005 Dl=00000040 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.B 
D1,(A0)+ 
X=0 
N=0 
Z=0 
v=o 
c=o 
X=0 
009FFFFC:00000408 s 
N=0 
Z=0 
V=0 
C=0 
A4=00000000 A5=00000000 A6=00000000 A7=009FFF 
D0=00000005 m=nnnnnn/in.-,n->-n^nnnnnn n^nnnnnnnn V=o 
D4=00000000 D5=00000000 D6=00000000 D7=0 
>RTS 

270 
Chapter 6 Assembly language programming 
PC=00040A SR=2000 SS=OOAO00O0 US=00000000 
X=0 
A0=00000503 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=OOAOO0OO Z=0 
D0=00000005 Dl=00000040 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>CMPT.B 
#540,Dl 
PC=00040E SR=2004 SS=00A000O0 US=O0OO0000 
X=0 
A0=00000503 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=0OA0O00O Z=l 
D0=00000005 Dl=00000040 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>BNE.S 
$0404 
Because Dl contains the ASCII code for'@', the test for equal-
ity will yield true and we will not take the branch back to $0404. 
P O 0 0 0 4 1 0 SR=2004 SS=00A00000 US=00000000 
X=0 
A0=00000503 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00A00000 
Z=l 
D0=00000005 Dl=00000040 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>LEA.L 
$EE(PC),A0 
The next instructions reset the pointer to the top of the 
buffer, read a character, and compare it to '@'. 
PC=000414 SR=2004 SS=O0AOO0OO US=00000000 
X=0 
A0=00000500 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=O0AOO000 Z=l 
D0=00000005 Dl=00000040 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.B 
(A0)+,D1 
PC=000416 SR=2000 SS=OOA0O0OO US=00000000 
X=0 
A0=00000501 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=O0AO0OOO Z=0 
D0=00000005 D1=0000005A D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>CMPI.B 
#$40,Dl 
PC=00041A SR=2000 SS=00A00000 US=00000000 
X=0 
A0=00000501 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00A00000 Z=0 
D0=00000005 D1=0000005A D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>BEQ.L 
$0424 
If it isn't an '<§>', we will print it by calling the output routine. 
PC=00041E SR=2000 SS=0OAOO00O OS=00000000 
X=0 
A0=00000501 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=O0AOOO0O Z=0 
D0=00000005 D1=0000005A D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>BSR.L 
$0430 

6.5 The stack 271 
PC=000430 SR=2000 SS=009FFFFC US=00000000 
X=0 
A0=00000501 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=009FFFFC Z=0 
D0=00000005 D1=0000005A D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.B 
#$06,DO 
009FFFFC:00000422 s 
In this case we have branched to address $0430. 
PO000434 SR=2000 SS=009FFFFC US=00000000 
X=0 
A0=00000501 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=009FFFFC Z=0 
D0=00000006 D1=0000005A D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>TRAP 
#$0F 
009FFFFC:00000422 s 
We call the operating system with the TRAP. Note that the 
contents of D1 will be printed as the ASCII character Z. Then 
we return to the body of the program. 
© 
PC=000436 SR=2000 SS=009FFFFC US=00000000 
X=0 
A0=00000501 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=009FFFFC Z=0 
D0=00000006 D1=0000005A D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>RTS 
009FFFFC-.00000422 s 
Note the change in the value of the PC following the RTS. 
PC=000422 SR=2000 SS=O0AO0OOO US=00000000 
X=0 
A0=00000501 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00A00000 Z=0 
D0=00000006 D1=0000005A D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>BRA.S 
$0414 
And so on .. 
6.5.3 Subroutines, the stack, and 
parameter passing 
In order for a subroutine to carry out its function, it is almost 
always necessary to transfer data between the calling program 
and the subroutine. Up to now we have passed data to and from 
the subroutine via data registers. In the previous example, we 
called the subroutine GET_CHAR to input a character from the 
keyboard. When this subroutine is invoked by the operation 
BSR GET_CHAR, a branch is made to the entry point of the sub-
routine. This subroutine reads the keyboard until a key is 
pressed. A return to the calling point is made with the ASCII 
code of the character in data register D1. 
You can even use the C-bit in the CCR to pass informa-
tion from a subroutine to its calling program; for example, 
to indicate an error state. Suppose a subroutine has been 
called to read data from a terminal and the terminal is faulty 
or not switched on. By setting the carry bit prior to a return 
from subroutine, the calling program can be informed that 
an error exists as the following fragment of a program 
demonstrates. 

272 
Chapter 6 Assembly language programming 
You can't use registers to transfer large quantities of data to 
and from subroutines, due to the limited number of registers. 
You can pass parameters to a subroutine by means of a mail-
box in memory. Consider the following. 
above the top of the stack). If an interrupt occurs or you call 
a subroutine, the new return address will be pushed on the 
top of the stack overwriting the old return address. Never 
move the stack pointer below the top of stack. 
MOVE.W Paraml,Mboxl 
MOVE.W Param2,Mbox2 
BSR 
Sub 
Put first parameter in mail box 1 
Put second parameter in mail box 2 
Now call the subroutine 
Return here... 
Sub 
MOVE.W Mboxl,DO 
MOVE.W Mbox2,Dl 
Retrieve the first parameter 
Retrieve the second parameter 
RTS 
Return to the calling program 
Such a solution is poor, because the subroutine can't be 
interrupted or called by another program. Any data stored in 
explicitly named locations could be corrupted by the inter-
rupting program (see the box on interrupts). Let's look at 
how data is transferred between a subroutine and its calling 
program by many high-level languages. 
Passing parameters on the stack 
An ideal way of passing information between the subroutine 
and calling program is via the stack. Suppose two 16-bit 
parameters, PI and P2, are needed by the subroutine 
ABC(P1,P2). The parameters are pushed on the stack 
immediately before the subroutine call by the following code: 
MOVE.W 
P 1 , - ( A 7 ) 
MOVE. W P 2 , - ( A 7 ) 
BSR 
ABC 
Push the first parameter 
Push the second parameter 
Call ABC(P1,P2) 
The state of the stack prior to the subroutine call and 
immediately after it is given in Fig. 6.30. Note that the return 
address is a longword and takes up two words on the stack. 
On entering the subroutine, you can retrieve the parame-
ters from the stack in several ways. However, you must never 
change the stack pointer in such a way that you move it down 
the stack. Consider Fig. 6.30(c) where the stack pointer is 
pointing at the return address. If you add 4 to the stack 
pointer, it will point to parameter P2 on the stack. You can 
now get P2 with, say, MOVE. W (A7) , DO. However, the return 
address is no longer on the stack (it's still there in memory 
You can avoid using the stack pointer by copying it to 
another address register with LEA (A7) , AO. NOW you can 
use AO to get the parameters; for example, PI can be loaded 
into Dl by MOVE.W 6(A0) ,D1. The offset 6 is required 
because the parameter PI is buried under the return address 
(4 bytes) and PI (2 bytes). Similarly, P2 can be loaded into D2 
by MOVE.W 4(AO) ,D2. 
After returning from the subroutine with RTS, the contents of 
the stack pointer are [A7] — 4, where A7 is the value of the stack 
pointer before PI and P2 were pushed on the stack. The stack 
pointer can be restored to its original value or cleaned up by exe-
cuting LEA 4 (A7) , A7 to move the stack pointer down by two 
words. Note that LEA 4 (A7) , A7 is the same as ADD . L # 4, A7. 
PI and P2 are, of course, still in the same 
locations in memory but they will be over-
written as new data is pushed on the stack. 
By using the stack to pass parameters to 
a subroutine, the subroutine may be inter-
rupted and then used by the interrupting program without 
the parameters being corrupted. As the data is stored on the 
stack, it is not overwritten when the subroutine is interrupted 
because new data is added at the top of the stack, and then 
removed after the interrupt has been serviced. 
Let's look at another example of parameter passing in detail. 
In the following program two numbers are loaded into DO and 
Dl, and then the contents of these registers are pushed on the 
stack. A subroutine, Addup, is called to add these two numbers 
together. In this case the result is pushed on the stack. We've used 
blue to highlight code that performs the parameter passing. 
on the stack 
BSR 
GETDATA Call subroutine and return with data in DO 
BCS 
ERROR 
IF carry flag set THEN something went wrong 
ELSE deal with the data 
Recover from error condition 
ERROR 

(a) Initial state of the stack. 
Stack 
Stack pointer 
TOS 
(b) Immediately before BSR. 
Stack 
(c) immediately after BSR. 
Stack pointer 
n - 4 
J>2 
PI 
Stack 
Address with 
respect to the 
stack pointer 
Address with 
respect to the 
stack pointer 
• 
return 
address ° / 
P2 
+y 
P1 
+6 
+8 
figure 6.30 Passing parameters on the stack (all values on the stack are words or longwords). 
THE INTERRUPT 
An interrupt is a method of diverting the processor from its 
intended course of action, and is employed to deal with errors 
and external events that must be attended to as soon as they 
occur. Whenever a processor receives an interrupt request 
from a device, the processor finishes its current instruction and 
then jumps to the program that deals with the cause of the 
interrupt. After the interrupt has been serviced, a return is 
made to the point immediately following the last instruction 
before the interrupt was dealt with. The return mechanism of 
the interrupt is almost identical with that of the subroutine— 
the return address is saved on the stack. 
Suppose a subroutine is interrupted during the course of its 
execution. If the interrupt-handling routine also wishes to use 
the same subroutine (yes, that's possible), any data stored in 
explicitly named memory locations will be overwritten and 
corrupted by the re-use of the subroutine. If the data had been 
stored in registers and the content of the registers pushed on 
the stack by the interrupt-handling routine, no data in the 
subroutine would have been lost by its re-use. After the sub-
routine has been re-used by the interrupt-handling routine, the 
contents of the registers stored on the stack are restored and a 
return from interrupt is made with the state of the registers 
exactly the same as at the instant the interrupt was serviced. 
Interrupts may originate in hardware or software. A 
hardware interrupt may occur when you move the mouse. 
A software interrupt may occur when you perform an illegal 
operation or even when you generate one with a TRAP #15 
instruction. 
AcidUp 
ORG 
$ 4 0 0 
LEA 
$ 1 0 0 0 , A 7 
MOVE W 
# l , D O 
MOVE W 
# 2 , D 1 
MOVE W 
D 0 , - ( A 7 ) 
MOVE w D 1 , - ( A 7 ) 
BSR 
AcidUp 
MOVE w 
( A 7 ) + , D 2 
LEA 
2<A7) , A 7 
STOP 
# $ 2 7 0 0 
MOVE w 4 ( A 7 ) , D 2 
MOVE w 6(A7) , D 3 
ADD.W 
D2,D3 
MOVE w D 3 , 4 ( A 7 ) 
RTS 
Set up the stack pointer 
Set up two parameters in DO and Dl 
Push parameter 1 on the stack 
Push parameter 2 on the stack 
Call adder routine 
Read the result from the stack 
Clean up the stack 
Stop 
Get parameter 2 from the stack 
Get parameter 1 from the stack 
Add them 
Store the result in the parameter 2 slot 
If we assemble this program, we get the following. 
1 0 0 0 0 0 4 0 0 
ORG 
$ 4 0 0 
2 0 0 0 0 0 4 0 0 
4 F F 8 1 0 0 0 
LEA 
$ 1 0 0 0 , A 7 
3 0 0 0 0 0 4 0 4 
3 0 3 C 0 0 0 1 
MOVE W 
# 1 , D 0 
4 0 0 0 0 0 4 0 8 
3 2 3 C 0 0 0 2 
MOVE W 
# 2 , D 1 
5 0 0 0 0 0 4 0 C 
3 F 0 0 
MOVE .W 
D 0 , - ( A 7 ) 
;Set up the stack pointer 
;Set up two parameters in DO and Dl 
;Push parameter 1 
6.5 The stack 
273 
Stack pointer 

274 
Chapter 6 Assembly language programming 
6 0000040E 3F01 
MOVE .W 
D1,-(A7) 
7 00000410 6100000C 
BSR 
ADDUP 
8 00000414 381F 
MOVE w 
(A7)+,D4 
9 00000416 4FEF0002 
LEA 
2(A7),A7 
10 
11 
12 
0000041A 4E722700 
STOP 
#$2700 
10 
11 
12 0000041E 342F0004 ADDUP: MOVE w 
4(A7),D2 
13 00000422 362F0006 
MOVE w 
6(A7),D3 
14 00000426 D642 
ADD 
D2,D3 
15 00000428 3F430004 
MOVE w 
D3,4(A7) 
16 0000042C 4E75 
RTS 
17 
18 
;Push parameter 2 
;Call adder routine 
;Read the result 
;Clean up stack 
;Stop 
Get parameter 2 
Get parameter 1 
Add them 
Store result in the parameter 2 slot 
00000400 
END 
$400 
Figure 6.31 shows the state of the stack at various points 
during the execution of this program. We will now load the 
program and trace it line by line. 
PC=000400 SR=2000 SS=00A00000 US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=OOA0OOO0 Z=0 
D0=00000000 D1=00000000 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>LEA.L 
$1000,SP 
PC=000404 SR=2000 SS=00001000 US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00001000 Z=0 
D0=00000000 D1=00000000 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.W 
#$01,DO 
00001000:00000000 s 
00001004:00000000 s+4 
00001008:00000000 s+8 
0000100C:00000000 s+12 
00001010:00000000 s+16 
Note the five new entries to the right of the register display. 
These lines display the five longwords at the top of the stack. 
Each line contains the stack address, the longword in that 
address, and the address with respect to the current stack 
pointer. 
Stack 
Stack pointer 
$1000 
TOS 
1000 
Stack pointer 
$0FF£ 
P1=1 
OFFE 
1000 
(a) Initial state of the stack. 
Stack 
(b) After pushing P1 with 
MOVE.W 
D 0 , - ( A 7 ) . 
Stack 
Stack pointer 
$0FF8 
Return 
/address 
r*t»000414 0FF8 
0FFC 
P2=2 
P1=1 
OFFE 
OFFE 
1000 
Stack 
30000414 0 
0FFC 
OFFE 
pointer 
S0FF8 
P2=2 
P1=1 
Stack 
Stack pointer 
SOFFE 
P2=2 
0FFC 
OFFE 
1000 
(c) After pushing P2 with 
MOVE.W D 0 , - ( A 7 ) . 
Stack 
Offset from 
SP 
'*S 
+4 
+6 
+8 
Stack pointer 
$0FFC 
P2=2 
P1=1 
0FF8 
0FFC 
OFFE 
1000 
(d) After calling the subroutine. 
(d) Accessing parameters 
in the subroutine. 
(d) After returning from 
the subroutine. 
Figure 6.31 The state of the 
stack during the execution of a 
program. 

PC=000408 SR=2000 SS=00001000 US=00OO0OO0 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00001000 Z=0 
DO-00000001 Dl=O0O0OO0O D2=00000000 D3=00000000 V=0 
D4=0OO000O0 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.W 
#$02,Dl 
00001000:00000000 s 
00001004:00000000 s+4 
00001008:00000000 s+8 
0000100C:00000000 s+12 
00001010:00000000 s+16 
PC=00040C SR=2000 SS=00001000 US=00000000 
X=0 00001000:00000000 s 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 00001004:00000000 s+4 
A4=00000000 A5=00000000 A6=00000000 A7=00001000 Z=0 00001008:00000000 s+8 
D0=00000001 Dl=00000002 D2=00000000 D3=00000000 V=0 0000100C:00000000 s+12 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 00001010:00000000 s+16 
>MOVE.W 
DO,-(SP) 
PC=00040E SR=2000 SS=O0000FFE US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=0OOOOFFE Z=0 
D0=00000001 Dl=00000002 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.W 
D1,-(SP) 
00000FFE:OOO10O00 
s 
00001002:00000000 
s+4 
00001006:00000000 
s+8 
0000100A:00000000 
s+12 
0000100E:00000000 
s+16 
Note how the instruction MOVE. w DO,— (SP) has mod-
ified the stack. The top of the stack is no longer $1000, but 
$0FFE. You can also see that the contents of DO.W (i.e. 0001) 
has been pushed on the stack. 
PC=000410 SR=2000 SS=00000FFC US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00000FFC Z=0 
D0=00000001 Dl=00000002 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>BSR.L 
$041E 
00000FFC:OO020001 
s 
00001000:00000000 
s+4 
00001004:00000000 
s+8 
00001008:00000000 
s+12 
0000100C:00000000 
s+16 
PC=00041E SR=2000 SS=00O0OFF8 US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7-0OOOOFF8 Z=0 
D0=00000001 Dl=00000002 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.W 
$04(SP),D2 
O0000FF8:OOOOO414 
s 
00000FFC.-00020001 s + 4 
00001000:00000000 
s+8 
00001004:00000000 
s+12 
00001008:00000000 
s+16 
At this point the return address, $00000414, has been 
pushed on the stack and the stack pointer is now pointing at 
$00000FF8. 
PC=000422 SR=2000 SS=00000FF8 US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=0000OFF8 Z=0 
D0=00000001 Dl=00000002 D2=00000002 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.W 
$06(SP),D3 
00000FF8:00000414 s 
00000FFC:00020001 s+4 
00001000:00000000 s+8 
00001004:00000000 s+12 
00001008:00000000 s+16 
PC=000426 SR=2000 SS=O0OOOFF8 US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00000FF8 Z=0 
D0=00000001 Dl=00000002 D2=00000002 D3=00000001 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>ADD.W 
D2,D3 
OOOOOFF8:00000414 s 
OOOOOFFC:00020001 s+4 
00001000:00000000 s+8 
00001004:00000000 s+12 
00001008:00000000 s+16 
6.5 The stack 
275 

276 
Chapter 6 Assembly language programming 
PC=000428 SR=2000 SS=00000FF8 US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00000FF8 Z=0 
D0=00000001 Dl=00000002 D2=00000002 D3=00000003 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.W 
D3,$04(SP) 
OOOOOFF8:00000414 s 
00000FFC:00020001 s+4 
00001000:00000000 s+8 
00001004:00000000 s+12 
00001008:00000000 s+16 
PC=00042C SR=2000 SS=00000FF8 US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00000FF8 Z=0 
D0=00000001 Dl=00000002 D2=00000002 D3=00000003 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>RTS 
OOOOOFF8:00000414 s 
OOOOOFFC:00030001 s+4 
00001000:00000000 s+8 
00001004:00000000 s+12 
00001008:00000000 s+16 
PO000414 SR=2000 SS=00000FFC US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=OOO0OFFC Z=0 
D0=00000001 Dl=00000002 D2=00000002 D3=00000003 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.W 
(SP)+,D4 
O00O0FFC:0OO3O001 s 
00001000:00000000 s+4 
00001004:00000000 s+8 
00001008:00000000 s+12 
0000100C:00000000 s+16 
PC=000416 SR=2000 SS=0O0OOFFE US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00000FFE Z=0 
D0=00000001 Dl=00000002 D2=00000002 D3=00000003 V=0 
D4=00000003 D5=00000000 D6=00000000 D7=00000000 C=0 
>LEA.L 
$02(SP),SP 
OOCOOFFE:00010000 s 
00001002:00000000 s+4 
00001006:00000000 s+8 
0000100A:00000000 s+12 
0000100E:00000000 s+16 
PC=00041A SR=2000 SS=00001000 US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00001000 Z=0 
D0=00000001 Dl=00000002 D2=00000002 D3=00000003 V=0 
D4=00000003 D5=00000000 D6=00000000 D7=00000000 C=0 
>STOP 
#$2700 
00001000:00000000 s 
00001004:00000000 s+4 
00001008:00000000 s+8 
0000100C:00000000 s+12 
00001010:00000000 s+16 
The result 1 + 2 = 3 is in data register D3, and the stack 
pointer is the same as its starting value $1000. Passing a para-
meter to a subroutine by value is easy. Getting a result back 
from the subroutine is trickier, as we'll soon see. 
Passing parameters by reference 
We have passed a parameter by value to the subroutine by 
pushing a copy of its value on the stack. There are two copies 
of the parameter, the original in the calling program and its 
copy on the stack. If a parameter is passed by value, changing 
it within the subroutine doesn't change its value in the calling 
program—as the next example demonstrates. 
Program to call a subroutine that swaps two numbers A and B 
ORG 
$400 
LEA 
$1000,A7 
MOVE. 
• W A,-(A7) 
MOVE. 
.W B,-(A7) 
BSR 
SWAP 
LEA 
4(A7),A7 
STOP 
#$2700 
Set up the stack pointer 
Push value of parameter A 
Push value of parameter B 
Call subroutine to swap A and B 
Clean up the stack 
Stop 
SWAP 
MOVE.W 4(A7),D1 
MOVE.W 6(A7),4(A7) 
MOVE.W D1,6(A7) 
RTS 
Get first parameter in DO 
Copy second parameter to first parameter 
Copy first parameter to second parameter 
A 
B 
DC.W 
DC.W 
$1234 
$5678 
END 
$400 

This program calls a subroutine to swap two numbers, 
A and B, which are first pushed on the stack in the main pro-
gram. In subroutine SWAP the two parameters are retrieved 
from their locations on the stack and swapped over. Once a 
return from subroutine is made and the stack cleaned up, the 
parameters on the stack are lost. Parameters A and B in the 
main program were never swapped. 
6.5 The stack 
277 
In this case, there is only one copy of the parameter. We repeat 
the example in which we added two numbers together, and, this 
time, pass the parameters to the subroutine by reference. 
The following program introduces a new instruction, 
push effective address PEA, which pushes an address in the 
stack; for example, the operation PEA PQR pushes the address 
PQR on the stack. The instruction PEA PQR is equivalent to 
MOVE.L #PQR, - (A7). 
* 
AddUp 
* 
X 
Y 
Z 
Set up the stack pointer 
Push address of variable X 
Push address of variable Y 
Push address of variable 2 (the- result;': 
Call adder routine 
Read the result (a dummy operation) 
Clean up stack 
Stop 
Get address of parameter X 
Get address of parameter Y 
Get value of X 
Get value of Y 
Add them 
Get address of parameter Z 
Put result in variable Z 
ORG 
LEA 
PEA 
PEA 
PEA 
BSR 
MOVE.W 
LEA 
STOP 
HOVEA.L 
MOVKA.L 
MOVE. Vi 
MOVE.W 
ADD 
MOVEA.L 
MOVE.W 
RTS 
ORG 
DC.W 
DC.W 
DS.W 
$400 
$1000,A7 
X 
X 
Z 
AddUp 
Z,D2 
12(A7),A7 
#$2700 
12(A7),A0 
S(A7),A1 
(AD) ,D2 
(All,D3 
D2,D3 
4(A7),A3 
D3, (A3) 
$500 
1 
2 
i 
You can pass a parameter to a subroutine by reference by pass-
ing its address on the stack. This is, you don't say'Here's a para-
meter'. Instead you say, 'Here's where the parameter is located'. 
The following is the assembled version of this program and 
Fig. 6.32 provides snapshots of memory and registers during 
the execution of the code. 
1 
00000400 
2 
00000400 4FF81000 
3 
00000404 487900000500 
4 
0000040A 487900000502 
5 
00000410 487900000504 
6 
00000416 61000010 
7 
0000041A 343900000504 
8 
00000420 4FEF000C 
9 
00000424 4E722700 
10 
* 
11 
00000428 206F0O0C 
ADDDP: 
12 
0000042C 226F0008 
13 
00000430 3410 
14 
00000432 3611 
15 . 00000434 D642 
16 
00000436 266F0004 
17 
0000Q43A 3683 
18 
0000043C 4E75 
1 9 
. ,-• 
* 
20 .00000500 
21 . 00000500 0001 
X: : 
22, 00000502 0002 . 
T: . 
23 00000504 0,0000002: 
Z: 
2 4 
• . . . • ; 
': 
' 
* . 
25 • 
'•' 00000400,,' ' • 
ORG 
LEA 
PEA 
PEA 
PEA 
BSR 
MOVE.W 
LEA 
STOP 
' MOVEA.L 
MOVEA.L 
MOVE.W 
MOVE.W 
ADD 
MOVEA.L 
.MOVE.W ' 
RTS 
O R G '•• 
• ' DC.W :, 
DP.W. : 
DS.W . 
'END 
$400 
$1000,A7 
X 
Y 
Z 
ADD0P 
Z,D2 
12(A7),A7 
#$2700 
12(A7),A0 
8{A7),A1 
<A0),D2 
(A1),D3 
D2,D3 
4(A7),A3 
D3, (A3.) 
$500 
'1 
. 2 
1 
• 
$400 
;Set up the stack pointer 
;Push address of X 
,-Push address, of Y 
;Push address of Z 
/Call adder routine 
/Read the result •• 
;Clean up stack 
•/Stop 
;Get address of parameter 
;Get address pf parameter • 
;Get value of X 
./Get value of ;Y 
/Add them 
/Get address of parameter 
;Put. result in variable .2 
return from subroutine is made and the stack cleaned up, the 

278 
Chapter 6 Assembly language programming 
Figure 6.32 Example of parameter passing by reference. 
We can now run this program line by line. Note 
We will use the simulator command MD 500 to view the 
how the addresses of the variables are pushed on the stack 
data area. Initially it contains the two 16-bit constants 
and then loaded in address registers in the subroutine. 
1 and 2. 
000500 00 01 00 02 00 00 00 00 00 00 00 00 00 00 00 00. 
PO000400 SR=2000 SS=OOA0OOOO US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00A00000 Z=0 
D0=00000000 D1=00000000 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>LEA.L 
51000,SP 
PC=000404 SR=2000 SS=00001000 US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00001000 Z=0 
D0=00000000 D1=00000000 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>PEA 
$0500 
00001000:00000000 s 
00001004:00000000 s+4 
00001008:00000000 s+8 
0000100C:00000000 s+12 
00001010:00000000 s+16 
PC=00040A SR=2000 SS=O00O0FFC US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=O0O00FFC Z=0 
D0=00000000 D1=00000000 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>PEA 
$0502 
OOOOOFFC:00000500 s 
00001000:00000000 s+4 
00001004:00000000 s+8 
00001008:00000000 s+12 
0000100C:00000000 s+16 
_ Memory 
x 
osoo 
Y 
0502 
Z 
0504 
Stack 
Stack 
Stack 
r > 
Return OFFO 
r > __T___ 
0 F F 4 
0-i 1 
:- ••••••' 
O F F S 
••'-•' 
c.-:;-. 
r 
, 
. 
c •::. 
OFFC 
;::•:.:.: 
or-,: 
Stack pointer 
— — — 
Stack pointer 
Stack pointer 
A7[E™3~Mz=d 
A7[i5if]J 1 
j
1 0 0 0 Ajfi^y 
I 
[100° 
(a) Initial state of the stack. 
(b) After pushing &X, &Y 
(c) A f t e r c a l l i n g A c U U p 
and &Z with PEA X, PEA Y, 
with BSR addup. 
PEAZ. 
v 
Memory 
Memory 
X 
S f ^ p 0 
Addresses of 
/~ x ^ ~ TSEOO 
V <—. rcA"> 
parameters loaded 
! 
, 
Addresses of 
' 
-<^u< 
i n a < j < | r e s s registers 
\ 
jY ^—• 050£ 
parameters loaded 
-, 
\ 
\ 
A1 and AO 
\ 
^ 
\ 
in address registers 
_ _ £ _ _ 
0504\ 
_\ ^ 
0504 \ 
AlandAO. 
' 
-I 
\ H' J , o o s l ] AO 
n Z 3 Z ] 
\ V.|ooooo5Qj)j A0 
Stack 
V I 
f l 
Sta-ckX 
\ i 
Li 
" 
i 
^Quoojjp|A1 
I——V-A 
"~- oooqgsy AI 
• • ° 
Return 
OFFO 
J~l 
- + . 0 R eturn\ OF-0 
* 
/ 
4 
•;•;• •: 
CFF4 
./ 
I 
4 ~ 7 ~ 
W-i\ 
y 
I 
8 
'-;'--- 
- -dTS 
/ 
g 
CLXt2 - Off!s, 
\ _ 
nnfntr 
12 
C1,Q:; - U f € — ^ 
Stack 
1 2 
05Ct J ^ - F G X 
| ~~ V 
^ 
P o i n t e r 
pointer 
'^ — — — 
s \ 
, , „ r f , . | 
1000 
I— 
I 
TUQQ 
x < ; 
1 
A7| 
SOFFO |—' 
[ 
- 
j 
A 7 j $OFFO |—' 
— - — — 
j ~ ~ ~ X 
| ^ ? 
(d) In subroutine AddUp after 
, », 
, 
.. 
. ..., 
^ 
'uc 
I 
,c 
MOVEA L 12 (A7) AO 
(e) In subroutine AddUp after 
l „ t d < , 
MOVEA L 
B ( A 7 ) ' A 1 
MOVE.W (AO) , D2 
memory toOata 
MOVbA.I, 
B ( A 7 ) , A 1 . 
MOVE.W 
( A 1 ) , D 3 . 
registers. 
0500 
0502 
0504 
Stack pointer 
A7[T^0~]-^] 
T° S J1000 
(a) Initial state of the stack. 
_Stack 
Stack 
r > 
Return OFFO 
0S04 
0FF4 
:-• 
:J:\ l 
'-'St'2 
0FF8 
•.'•:•. .-• c.-:;-. 
OSGO 
OFFC 
;.'::_0 
OF'-,." 
~ 
Stack pointer 
1000 
[—r^ 
1 
IOOO 
———— 
A7 
SOFFO —I 
(c) After calling Addllp 
with BSR addup. 
Memory 
/-X^-BSOO 
f 
^ 
Addresses of 
\ 
iY ~*— 050£ 
parameters loaded 
\ 
^ 
\ 
in address registers 
_\ 2, 
0504 \ 
AlandAO. 
_SZ3Zj 
'', vjooooosoo I i n 
\ \ 
\ I 
4 I ™ 
Z _ r — ] \ 
^ oooqgsog AI 
"*0 
RetumX cAo 
J j 
8 
''•-'(»- - Off ik 
N ^ 
Stack 
, 2 
05Ct'Ji)F-FGX 
| ~~ Y 
| -
pointer 
'c 
\ _ 
A 7 | 
50FF0 | - J 
[_ 
10C'° 
")~~- X 
\„? 
(e) In subroutine AddUp after 
t
 y ' t!j't 
, l S 
MOVE.W (A0),D2 
i.wmory tuOata 
MOVE . W 
(Al) , D3 . 
registers. 
Memory 
X ^~~ -o~n~i 
1 
, 
Addresses of 
V <—. rcA-> 
parameters loaded 
' 
-<f u< 
i n a < j < | r e s s registers 
., 
\ 
\ 
Al and AO 
Z 
05CM\ 
J 
\ M i J W 5 ' J A„ 
\ 
I 
1-1 AO 
Stack 
\ 
. 
-L, 
——1 
v - OvOOMiZ 
A 1 
•*• 0 Return 
OFFO 
1~~1 
A 
;•;• .' 
CFF4 
yS 
I 
Stack 
. J 
e:-f-' -• O f * S — ^ ^ ^ 
pointer 
" M 
— — 
A7J 
SOFFO I—i 
••• 
(d) In subroutine AddUp after 
MOVEA.L 12<A7),A0 
MOVEA.L 
8 ( A 7 ) , A 1 . 

The operation PEA 
$0500 has pushed the address 
$00000500 on the stack and moved the stack pointer up by 4. 
PO000410 SR=2000 SS-OO000FF3 US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00000FF8 Z=0 
D0=00000000 D1=00000000 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>PEA 
$0504 
G0000FFS:0CC00502 s 
00000FFC:00000500 s+4 
00001000:00000000 s+8 
00001004:00000000 s+12 
00001008:00000000 s+16 
PO000416 SR=2000 3S=OOO00FF4 US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00O00FF4 2=0 
D0=00000000 D1=00000000 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>BSR.L 
$0428 
OOOOOFF4:00000504 s 
00OOOFF8:OO0005O2 s+4 
0000OFFC:0O0O05O0 s+8 
00001000:00000000 s+12 
00001004:00000000 s+16 
PO000423 SR=2000 SS = 0O0O0FFO US=00000000 
X=0 
A0=00000000 A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=0OO00FF0 Z=0 
D0=00000000 D1=00000000 D2=00000000 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVEA.L $0C (SP) ,A0 
0C000FF0:000C041A s 
00000FF4:00000504 s+4 
00000FF8:00000502 s+8 
00000FFC:00000500 s+12 
00001000:00000000 s+16 
PO00042C SR=2000 SS=00000FF0 US=00000000 
X=0 
AO^OOOOOSOO A1=00000000 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=OOOO0FF0 Z=0 
D0=00000000 D1=00000000 D2=00000000 D3=00000000 V=0 
D4-00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVEA.L $08(SP),Al 
0O0OOFF0:000OO41A s 
OOOOOFF4-.00000504 s + 4 
0000OFF8:O00O05O2 S+8 
00000FFC:00000500 s+12 
00001000:00000000 s+16 
0 0 0 0 0 4 30 SR=2000 SS=D0000FF0 US=00000000 
X=0 
A0=00000500 A1=0C000502 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=000O0FF0 Z=0 
D0=00000000 D1=00000000 D2=00000000 D3=00000000 V=0 
D4-00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.W 
(A0),D2 
OOOOOFF0:00O0041A S 
00O00FF4:OO000504 s+4 
OOOOOFF8:00000502 s+8 
0OOO0FFC:O00005O0 s+12 
00001000:00000000 s+16 
PC=000432 SR=2000 SS=00000FF0 US=00000000 
X=0 
A0=00000500 Al=00000502 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00000FF0 Z=0 
D0=00000000 D1=00000000 D2-=00000001 D3=00000000 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.W 
(Al),D3 
OOOOOFFO:0000041A s 
0000OFF4:00O0O5O4 S+4 
00000FF8:00000502 S+8 
OOOOOFFC:00000500 s+12 
00001000:00000000 s+16 
PO000434 SR=2000 SS=00000FF0 US=00000000 
X=0 
A0=00000500 Al=00000502 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6-00000000 A7=0O0OOFF0 Z=0 
D0=00000000 D1=00000000 D2=00000001 D3=000O0002 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>ADD.W 
D2,D3 
0OOOOFF0:0000041A s 
OOOOOFF4:00000504 s+4 
OOOOOFF8:00000502 S+8 
OOOOOFFC:00000500 s+12 
00001000:00000000 s+16 
PO0004 36 SR=2000 SS=00000FF0 US=00000000 
X=0 
A0=00000500 Al=00000502 A2=00000000 A3=00000000 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00000FF0 Z=0 
D0=00000000 D1=00000000 D2=00000001 03=00000003 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVEA.L $04(SP),A3 
OOOOOFF0:OOO0041A s 
OOOOOFF4:00000504 s+4 
00000FF8:00000502 s+8 
00000FFC:00000500 s+12 
00001000:00000000 s+16 
PC=00043A SR=2000 SS=00000FF0 US=00000000 
X=0 
A0=00000500 Al=00000502 A2=00000000 A3=00000504 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=0OOOOFFO Z=0 
D0=00000000 D1=00000000 D2=00000001 D3=00000003 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.W 
D3, (A3) 
00000FF0:000004lA s 
OOOOOFF4:00000504 s+4 
00000FF8:00000502 s+8 
OOOOOFFC:00000500 s+12 
00001000:00000000 s+16 
PC=00043C SR=2000 SS=O0OOOFF0 US=00000000 
X=0 
A0=00000500 Al=00000502 A2=00000000 A3=00000504 N=0 
A4=00000000 A5»00000000 A6=00000000 A7=O0000FF0 Z=0 
D0=00000000 Dl=00000000 D2=00000001 D3=00000003 V=0 
D4=0O000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>RTS 
0O000FF0:OO00041A s 
0O000FF4:00000504 s+4 
O00OOFF8:00000502 s+8 
OOOOOFFC-.00000500 s+12 
00001000:00000000 s+16 
6.5 The stack 
279 

280 
Chapter 6 Assembly language programming 
EOC0041A SR=2000 SS=00C0CFF4 OS=00000000 
X=0 
A0=00000500 Al=00000502 A2=00000000 A3=00000504 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00000FF4 Z=0 
D0=00000000 D1=00000000 D2=00000001 D3=00000003 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>MOVE.W 
$05C4,D2 
PC=000420 SR=2000 SS=00000FF4 US=OO000OOO 
X=0 
A0=00000500 Al=00000502 A2=00000000 A3=00000504 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00000FF4 Z=0 
D0=00000000 D1=00000000 D2-00000003 D3=00000003 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
M.SA.L 
$OC(SP),SP 
PO000424 SR=2000 3S=C0001000 US=00000000 
X=0 
A0=00000500 Al=00000502 A2=00000000 A3=00000504 N=0 
A4=00000000 A5=00000000 A6=00000000 A7=00001000 Z=0 
D0=00000000 D1=00000000 D2=00000003 D3=00000003 V=0 
D4=00000000 D5=00000000 D6=00000000 D7=00000000 C=0 
>STOP 
#$2700 
00000FF4:C0000504 s 
000OOFF8:OO0005O2 s+4 
00000FFC:00000500 s+8 
00001000:00000000 s+12 
00001004:00000000 s+16 
00000FF4:00000504 s 
OOOOOFF8:00000502 s+4 
OOOOOFFC:00000500 s+8 
00001000:00000000 s+12 
00001004:00000000 s+16 
00001000:00000000 s 
00001004:00000000 S+4 
00001008:00000000 s+8 
0000100C:00000000 s+12 
00001010:00000000 s+16 
If we look at memory again, we will find that the sum of X 
and Y has been stored in location Z. 
000500 00 01 00 02 00 03 00 00 00 00 00 00 
We have passed the parameters by reference. In practice, a 
programmer would pass parameters that aren't changed in 
the subroutine by value, and only pass parameters that are to 
be changed by reference. 
6.6 Examples of 68K programs 
We now put together some of the things we've learned about 
the 68K's instruction set and write a simple program to 
implement a text matching algorithm that determines 
whether a string contains a certain substring. The problem 
can be solved by sliding the substring along 
00 00 00 00 
the string until each character of the sub-
string matches with the corresponding 
character of the string, as illustrated in Fig. 6.33. 
The string starts at address $002000 and is terminated by a 
carriage return (ASCII code $0D). The substring is stored at 
location $002100 onwards and is also terminated by a car-
riage return. In what follows, the string of characters is 
referred to as STRING, and the substring as TEXT. 
We will construct a main program that calls a subroutine, 
MATCH, to scan the string for the first occurrence of the 
String 
Substring 
T H I S 
T H E N 
T H A T 
T H E 
T H E N 
T H E 
O T H E R 
Step 
Matches 
T H I S 
T H A T 
T H E N 
T H E 
O T H E R 
1 
5 
2 
0 
3 
0 
4 
1 
5 
0 
6 
6 
7 
0 
8 
0 
9 
1 
10 
0 
11 
8 
12 
0 
13 
0 
T H E 
N 
T H E 
T H E N 
T H E 
T H 
T 
E N 
H E 
T H 
T 
N 
E 
H 
T 
T 
N 
E 
H 
T 
H 
T 
N 
E 
H 
T 
E 
H 
T 
N 
E 
H 
T 
ui I 
1— 
Z ul X 1— 
E 
H 
T 
N 
E 
H 
T 
E 
H 
T 
N 
E 
H 
T 
E 
H 
T 
N 
E 
H 
E 
H 
T 
N 
E 
E 
H 
T 
N 
E 
H 
T 
E 
H 
T 
E 
H E 
Figure 6.33 Matching a string 
and a substring. 

6.6 Examples of 68K programs 
281 
substring. Because STRING and TEXT are both strings of con-
secutive characters, we will pass them to MATCH by reference. 
The subroutine should return the address of the first character 
CRET 
EQU 
$0D 
ORG 
$400 
LEA 
$1000,A7 
PEA 
STRING 
PEA 
TEXT 
LEA 
-4(A7) ,A7 
BSR 
MATCH 
MOVE.L 
(A7)+,D0 
LEA 
8(A7),A7 
STOP 
#$2700 
in the string matching the first character of the substring. This 
address is to be returned on the stack. If the match is unsuc-
cessful, the null address, $00000000, is pushed on the stack. 
ASCII code for carriage return 
Start of the main program 
Set up the stack pointer 
Push the address of the string 
Push the address of the substring 
Make room on the stack for the result 
Perform the match 
Let's have a look at the result 
Clean up the stack (remove the 2 parameters) 
MATCH 
NEXT 
MATCH matches the substring whose location is pointed at by 
A0 with the string whose location is pointed at by Al. 
Both strings are terminated by a carriage return. 
The match is carried out by comparing the first character of 
the substring with the characters of the string, one by one. 
If a match is found, the rest of the characters of the 
substring are matched with the corresponding characters of 
the string. If they all match up to the substring terminator, 
the search is successful. As soon as a mismatch is found, we 
return to matching the first character of the substring with 
a character from the string. If the terminator of the string 
is reached, the search has been unsuccessful. 
MOVEM.L D0/A0-A3,-(A7) Save all working registers 
MOVEA.L 32(A7),A0 
Get STRING address off the stack 
MOVEA.L 28(A7),A1 
Get TEXT (substring) address off the stack 
MOVE.B (A0)+,DO 
CMP.B 
#CRET,D0 
BEQ 
FAIL 
CMP.B 
(A1),D0 
BNE 
NEXT 
Get a character from the string 
Is this character a carriage return? 
If carriage return then no match so exit 
Match character with char from substring 
If no match then move along the string 
We have found the first match. 
We have to save the two pointers before performing the 
submatch in case we have to return to matching the pairs 
of first characters. 
MOVEA.L A0,A2 
MOVEA.L A1,A3 
ADDA.L #1,A1 
* 
LOOP 
MOVE.B (A1)+,D0 
CMP.B 
#CRET,D0 
BEQ 
SUCCESS 
CMP.B 
(A0)+,DO 
BEQ 
LOOP 
Save A0 in A2 in case of no full match 
Save Al in A3 
Increment pointer to substring 
Get next character from substring 
If terminator found then success 
Else compare it with next char from string 
Repeat while they match 
No submatch found so prepare to continue matching pairs 
of first characters. 
SUCCESS 
FAIL 
* 
RETURN 
STRING 
TEXT 
MOVEA.L A2,A0 
MOVEA.L A3,A1 
BRA 
NEXT 
SUBA.L #1,A2 
MOVE.L A2,24(A7) 
BRA 
RETURN 
Restore A0 and Al 
to their values before the submatch 
Try again 
Undo work of auto increment 
Push address of match on stack 
MOVE.L f0,24(A7) 
Push null address on stack for fail 
MOVEM.L (A7)+,DO/A0-A3 Restore all working registers 
RTS 
ORG 
$002000 
Location of the string 
DC.B 
'THIS THAT THEN',$0D 
ORG 
$002100 
Location of substring to be matched 
DC.B 
"THEN THE',$0D 
END 
$400 

282 
Chapter 6 Assembly language programming 
6.6.1 A circular buffer 
Many computers can't process data as they receive it; the 
data is placed in a store until it's ready for use by the com-
puter. This store is a buffer and is analogous to a doctor's 
waiting room. The number of people in the waiting room 
increases as new patients enter and decreases as patients are 
treated by the doctor. Consider a 68K-based system with a 
software module that is employed by both input and output 
routines, and whose function is to buffer data. When it's 
called by the input routine, it adds a character to the buffer. 
When it's called by the output routine, it removes a charac-
ter from the buffer. Below are the operational parameters of 
the subroutine. 
• Register DO is to be used for character input and output. 
The character is an 8-bit value and occupies the lowest-order 
byte of DO. 
• Register Dl contains the code 0, 1, or 2 on entering the 
subroutine. 
• Code 0 means clear the buffer and reset all pointers. 
• Code 1 means place the character in DO into the buffer. 
• Code 2 means remove a character from the buffer and place it 
in DO. 
• We assume that a higher-level module ensures that only one of 
0, 1, or 2 is passed to the module (i.e. invalid operation codes 
cannot occur). 
• The location of the first entry in the buffer is at $010000 and 
the buffer size is 1024 bytes. Pointers and storage may be placed 
after the end of the buffer. 
• If the buffer is full, the addition of a new character overwrites 
the oldest character in the buffer. In this case, bit 31 of DO is set 
to indicate overflow and cleared otherwise. 
• If the buffer is empty, removing a new character results in the 
contents of the lower byte of DO being set to zero and its most-
significant bit set. 
• Apart from DO, no other registers are to be modified by calling 
the subroutine. 
The first step in solving the problem is to construct a dia-
gram (Fig. 6.34) to help us visualize the buffer. Figure 6.34 
shows the memory map corresponding to the buffer. A region 
of 1024 bytes ($400) is reserved for the buffer together with 
two 32-bit pointers. INLptr points to the location of the next 
free position into which a new character is to be placed and 
OUT_ptr points to the location of the next character to be 
removed from the buffer. At the right-hand side of the dia-
gram is the logical arrangement of the circular buffer. This 
arrangement provides the programmer with a better mental 
image of how the process is to operate. 
The first level of abstraction in pseudocode is to determine 
the overall action the module is to perform. This can be writ-
ten as follows. 
Module Circular_buffer 
Save working registers 
CASE OF: 
Dl = 0: Initialize system 
Dl = 1: Input a character 
Dl = 2 : Output a character 
END_CASE 
Restore working registers 
End Circular_buffer 
At this, the highest level of abstraction, we have provided 
no indication of how any action is to be carried out and the 
only control structure is the selection of one of three possible 
functions. The next step is to elaborate on some of these 
actions. 
Start 010000 
End0103FF 
010400 
010404 
010408 
!N_ptr 
OUTLptr 
Count 
Direction of 
pointer movement 
1024-byte 
buffer 
(a) Buffer memory map. 
Figure 6.34 The circular buffer. 
(b) Logical arrangement of buffer. 
\ s t a r t 
\ End 
1 
OUT_ptr 
IN_ptf 

6.6 Examptes of 68K programs 283 
Module Circular_buffer 
Save working registers 
IF [Dl] = 0 THEN Initialize END_IF 
IF [Dl] = 1 THEN Input_character ENDIF 
IF [Dl] = 2 THEN Output_character END_IF 
Restore working registers 
End Circular_buffer 
Initialize 
Count = 0 
IN_ptr = Start 
OUT_ptr = Start 
End Initialize 
Input_character 
Store new character 
Deal with any overflow 
End Input_character 
Output_character 
IF buffer NOT empty THEN Get_characterJrombuffer 
ELSE return null character 
END_IF 
End Output__character 
The pseudocode is now fairly detailed. Both the module 
selection and the initialization routines are complete. We still 
have to work on the input and output routines because of the 
difficulty in dealing with the effects of overflow and under-
flow in a circular buffer. 
We can determine the state of the buffer by means of a 
variable, Count, which indicates the number of characters in 
the buffer. If Count is greater than zero and less than its max-
imum value, a new character can be added or one removed 
without any difficulty. If Count is zero, the buffer is empty 
and we can add a character but not remove one. If Count is 
equal to its maximum value and therefore the buffer is full, 
each new character must overwrite the oldest character as 
specified by the program requirements. This last step is tricky 
because the next character to be output (the oldest character 
in the buffer) is overwritten by the latest character. Therefore, 
the next character to be output will now be the oldest survi-
Input_character 
Store new character at IN_ptr 
IN_ptr = IN_ptr + 1 
IF IN_ptr > End THEN IN_ptr = Start END_IF 
IF Count < Max THEN Count = Count + 1 
ELSE 
Set overflow flag 
OOT_ptr = OUT_ptr + 1 
IF OUT_ptr > End THEN OOT_ptr = Start END_IF 
END_IF 
End Input_character 
Output_character 
IF Count = 0 THEN return null and set underflow flag 
ELSE 
Count = Count - 1 
Get character pointed at by OUT_ptr 
OUT_ptr = OOT_ptr + 1 
IF OUT_ptr > End THEN OUT_ptr = Start END_IF 
END_IF 
End Output_character 
ving character and the pointer to the output must be moved 
to reflect this. 
Sometimes it is helpful to draw a simplified picture of the 
system to enable you to walk through the design. Figure 6.35 
shows a buffer with four locations. Initially, in state (a), the 
buffer is empty and both pointers point to the same location. 
At state (b), a character is entered, the counter incremented, 
and the input pointer moved to the next free position. States 
(c) to (e) show the buffer successively filling up to its maxi-
mum count of 4. If another character is now input, as in state 
(f), the oldest character in the buffer is overwritten. 
It is not necessary to rewrite the entire module in pseudocode. 
We will concentrate on the input and output routines and then 
begin assembly language coding. Because the logical buffer is cir-
cular while the physical buffer is not, we must wrap the physical 
buffer round. That is, when the last location in the physical 
buffer is filled, we must move back to the start of the buffer. 

284 
Chapter 6 Assembly language programming 
out 
in J 
(a) Count = 0 
(b) Count = 1 
(c) Count = 2 
y. 
3 
\ 
\ 
V i 
out_J 
in 
f 4 
3 \ 
out_J 
in 
2 J 
(d) Count = 3 
(e) Count = 4 
The program design language has now done its job and we 
can translate the routines into the appropriate assembly 
language. 
(f) Count 
Figure 6.35 Operation of the 
circular buffer. 
CIRC 
EQU 
* 
MOVEM.L A0-A1,-(SP) 
BCLR.L #31,DO 
CMPI.B #0,D1 
BNE.S 
CIRC1 
BSR.S 
INITIAL 
BRA.S 
CIRC3 
CIRC1 
CMPI.B #1,D1 
BNE.S 
CIRC2 
BSR.S 
INPUT 
BRA.S 
CIRC3 
CIRC2 
BSR.S 
OUTPUT 
CIRC3 
MOVEM.L (SP)+,A0-A1 
* 
RTS 
INITIAL EQU * 
This module sets u 
CLR.W 
COUNT 
MOVE.L •BUFFER,IN PTR 
MOVE.L #BUFFER,OOT_PTR 
* 
RTS 
INPUT 
EQU * 
This module stores 
MOVEA.L IN PTR,A0 
MOVE.B DO,(A0)+ 
CMPA.L #END+1,A0 
BNE.S 
INPUT1 
MOVEA.L #START,AO 
INPUT1 MOVE.L A0,IN_PTR 
CMPI.W #MAX,COUNT 
BEQ.S 
INPUT2 
ADDQ.B 
RTS 
#l,COOHT 
INP0T2 BSET.L #31,DO 
MOVEA.L OUT PTR.AO 
LEA 
1(A0),A0 
CMPA. L #END+1,A0 
This module implements a circular buffer 
Save working registers 
Clear bit 31 of DO (no error) 
Test for initialize request 
IF not 0 THEN next test 
IF 0 THEN perform initialize 
and exit 
Test for input request 
IF not input THEN must be output 
IF 1 THEN INPUT 
and exit 
By default OUTPUT 
Restore working registers 
End CIRCULAR 
the circular buffer 
Initialize counter and pointers 
Set up IN_ptr 
Set up OUT_ptr 
a character in the buffer 
Get pointer to input 
Store char in buffer, update pointer 
Test for wrap-round 
IF not end THEN skip reposition 
Reposition input pointer 
Save updated pointer 
Is buffer full? 
IF full THEN deal with 
ELSE increment 
and return 
Set overflow flag 
Get output pointer 
Increment OUT_ptr 
Test for wrap-round 
overflow 
character count 
V 
1 
2 
/ 
out I 
in 
t" 
1 
out_J 
in 
/ 
4 
3 
\ 
Y 5 
2 
/ 
A A 
out 
I 
in 

6.6 Examples of 68K programs 
285 
BNE.S 
INPUT3 
MOVEA.L #START, AO 
INPUT3 
MOVE.L 
RTS 
A0,OUT_PTR 
OUTPUT 
TST.W 
COUNT 
BNE.S 
OUTPUT1 
CLR.B 
DO 
BSET.L 
RTS 
#31,DO 
OUTPUT1 SUBI.W 
#1,COUNT 
MOVEA.L OUT PTR,A0 
MOVE.B 
( A 0 ) + , D 0 
CMPA.L 
#END+l,AO 
BNE.S 
OUTPUT2 
MOVEA.L #START,AO 
OUTPUT2 MOVE.L 
A0,OUT PTR 
IF not wrap-round THEN skip fix 
ELSE wrap-round OUT_ptr 
Update OUT_ptr in memory 
and return 
Examine state of buffer 
IF buffer not empty output char 
ELSE return null output 
set underflow flag 
and exit 
Decrement COUNT for removal 
Point to next char to be output 
Get character and update pointer 
Test for wrap-round 
IF not wrap-round THEN exit 
ELSE wrap-round OUT_ptr 
Restore OUT_ptr in memory 
RTS 
Now that we've designed and coded the buffer, the next 
step is to test it. The following code is the assembled circular 
buffer program with the necessary driver routines. The pro-
gram inputs two characters at a time, and implements an 
8-byte buffer. The first character of a pair is the control char-
acter (i.e. 0 = initialize, 1 = input, and 2 = output). For 
example, to initialize the buffer you type OX, where X is any 
character. If you type 1Y, the character Y is stored in the next 
free place in the buffer. If you type 2Z, the next character to be 
output is displayed. After each operation, the contents of the 
buffer are printed and the current value of the variable count 
displayed. 
1 
00000400 
ORG 
$400 
2 
00000400 4DF900001012 
LEA 
HEADER, A6 
3 
00000406 6100015E 
BSR 
PRINT 
4 
0000040A 61000104 
NEXT: 
BSR 
GETCHAR 
5 
0000040E 04010030 
SUB.B • 
#530,Dl 
6 
00000412 1E01 
MOVE.B 
D1,D7 
7 
00000414 
610000FA 
BSR 
GETCHAR 
8 
00000418 1001 
MOVE.B 
D1,D0 
9 
0000041A 1207 
MOVE.B 
D7,D1 
10 
0000041C 6100002A 
BSR 
CIRC 
11 
00000420 OC010002 
CMP.B 
#2,D1 
12 
00000424 66000018 
BNE 
CONT 
13 
00000428 1C00 
MOVE.B 
D0,D6 
14 
0000042A 610000F4 
BSR 
NEWLINE 
15 
0000042E 4DF900001023 
LEA 
OUT.A6 
16 
00000434 
61000130 
BSR 
PRINT 
17 
00000438 1206 
MOVE.B 
D6,D1 
18 
0000043A 610000DC 
BSR 
PUTCHAR 
19 
0000043E 610000EO 
CONT: 
BSR 
NEWLINE 
20 
00000442 610000E8 
BSR 
DISPLAY 
21 
00000446 60C2 
BRA 
NEXT 
22 
* 
23 
00000448 
CIRC: 
EQU 
* 
24 
00000448 48E700C0 
MOVEM.L 
A 0 - A 1 , - ( S P ) 
25 
0000044C 0880001F 
BCLR 
#31,DO 
26 
00000450 0C010000 
CMPI.B 
»0,D1 
27 
00000454 
6604 
BNE.S 
CIRC1 
28 
00000456 6114 
BSR.S 
INITIAL 
29 
00000458 600C 
BRA.S 
CIRC3 
30 
0000045A 0C010001 
CIRC1: 
CMPI.B 
#1,D1 
31 
0000045E 6604 
BNE.S 
CIRC2 
32 
00000460 6126 
BSR.S 
INPUT 
33 
00000462 6002 
BRA.S 
CIRC3 
34 
00000464 6174 
CIRC2: 
BSR.S 
OUTPUT 
35 
00000466 4CDF0300 
CIRC3: 
MOVEM.L 
(SP)+,A0-A1 
36 
37 
38 
0000046A 4E75 
RTS 
36 
37 
38 
0000046C 
INITIAL: 
EQU 
* 
39 
0000046C 427900001008 
CLR.W 
COUNT 
40 
00000472 23FC00001000 
0000100A 
MOVE.L 
•BUFFER,IN_POINT 
41 
0000047C 23FC00001000 
0000100E 
MOVE.L 
•BUFFER,OUT_POIN 
42 
00000486 4E75 
RTS 
43 
* 
44 
00000488 
INPUT: 
EQU 
* 
45 
00000488 20790000100A 
MOVEA. L 
IN POINT,A0 
46 
0000048E 10C0 
MOVE.B 
DO,(A0)+ 
47 
00000490 B1FC00001008 
CMPA.L 
#LAST+1,A0 
48 
00000496 6606 
BNE.S 
INPUT1 
;Print heading 
REPEAT 
Get number (0, 1, or 2) 
Save it 
Get character for input routine 
Restore number 
Call the buffer routine 
IF we did output THEN print the char 
;Save character from output 
;Print "output" 
;Now print the character 
;Display the buffer contents 
;FOREVER 
;Implement a circular buffer 
;Save working registers 
;Clear bit 31 of DO (no error) 
;Test for initialize request 
;IF not 0 THEN next test 
;IF 0 THEN perform initialize 
;and exit 
;Test for input request 
;IF not input THEN must be output 
;IF 1 THEN INPUT 
;and exit 
;By default OUTPUT 
;Restore working registers 
;End CIRCULAR 
;Set up the circular buffer 
;Initialize counter and pointers 
;Set up IN_ptr 
;Set up OUT_ptr 
;Store a char in the buffer 
;Get pointer to input 
;Store char in buffer, update ptr 
;Test for wrap-round 
;IF not end THEN skip reposition 

286 
Chapter 6 Assembly language programming 
49 
00000498 207C00001000 
MOVEA.L 
•START,A0 
Reposition input pointer 
50 0000049E 23C80000100A INPUT1: 
MOVE.L 
A0, IN POINT 
Save updated pointer 
51 000004A4 OC7900080000 
1008 
CMPI.W 
#MAX,COUNT 
Is buffer full? 
52 000004AC 6708 
BEQ.S 
INPUT2 
IF full THEN deal with overflow 
53 
000004AE 527900001008 
ADDQ.W 
#1,COUNT 
ELSE increment character count 
54 000004B4 4E75 
RTS 
and return 
55 000004B6 08C0001F 
INPUT2: 
BSET 
#31,DO 
Set overflow flag 
56 
000004BA 20790000100E 
MOVEA.L 
OUT POINT, A0 
Get output pointer 
57 
000004C0 41E80001 
LEA 
1(A0),A0 
Increment OUT ptr 
58 000004C4 B1FC00001008 
CMPA.L 
#LAST+1,A0 
Test for wrap-round 
59 000004CA 6606 
BNE.S 
INPUT3 
IF not wrap-round THEN skip fix 
60 000004CC 207C00001000 
MOVEA.L 
• START, A0 
ELSE wrap-round OUT ptr 
61 
000004D2 23C80000100E INPUT3: 
MOVE.L 
A0,OUT_POINT 
Update OUT ptr in memory 
62 
63 
64 
000004D8 4E75 
RTS 
and return 
62 
63 
64 000004DA 4A7900001008 OUTPUT: 
TST.W 
COUNT 
Examine state of buffer 
65 000004E0 6608 
BNE.S 
OUTPUT1 
IF buffer not empty output char 
66 000004E2 4200 
CLR.B 
DO 
ELSE return null output 
67 000004E4 08C0001F 
BSET 
#31, DO 
set underflow flag 
68 000004E8 4E75 
RTS 
and exit 
69 
000004EA 047900010000 
1008 
GUTPUT1: 
SUBI.W 
#1,COUNT 
Decrement COUNT for removal 
70 000004F2 20790000100E 
MOVEA.L 
OUT POINT,A0 
Point to next char to be output 
71 000004F8 1018 
MOVE.B 
(A0)+,D0 
Get character and update pointer 
72 000004FA B1FC00001008 
CMPA. L 
#LAST+1,A0 
Test for wrap-round 
73 
00000500 6606 
BNE.S 
OUTPUT2 
IF not wrap-round THEN exit 
74 
00000502 207C00001000 
MOVEA.L 
•START,A0 
ELSE wrap-round OUT_ptr 
75 
00000508 23C80000100E OUTPUT2: 
MOVE.L 
A0,OUT_POINT 
Restore OUT ptr in memory 
76 
77 
78 
0000050E 4E75 
RTS 
76 
77 
78 00000510 103C0005 
GETCHAR: MOVE.B 
#5,DO 
Read an ASCII character into Dl 
79 
00000514 4E4F 
TRAP 
#15 
80 
00000516 4E75 
RTS 
81 
00000518 103C0006 
PUTCHAR: MOVE.B 
#6,DO 
Print the ASCII character in Dl 
82 0000051C 4E4F 
TRAP 
#15 
83 
0000051E 4E75 
RTS 
84 
00000520 123C000D 
NEWLINE: MOVE.B 
#$0D, Dl 
New Line 
85 
00000524 61F2 
BSR 
PUTCHAR 
86 00000526 123C000A 
MOVE.B 
#$0A,D1 
87 0000052A 60EC 
BRA 
PUTCHAR 
88 
0000052C 48E7FFFE 
DISPLAY: MOVEM.L 
A0-A6/D0-D7,-(A7) 
Display the buffer contents 
89 
00000530 61EE 
BSR 
NEWLINE 
90 
00000532 43F900001000 
LEA 
BUFFER,Al 
91 
00000538 3E3C0007 
MOVE.W 
#7,D7 
92 0000053C 1219 
DIS1: 
MOVE.B 
(A1)+,D1 
93 0000053E 61D8 
BSR 
PUTCHAR 
•Print a character 
94 
00000540 123C0020 
MOVE.B 
#$20,Dl 
and a space 
95 
00000544 51CFFFF6 
DBRA 
D7,DIS1 
96 00000548 4DF900001037 
LEA 
COUNTER,A6 
Print header before current count 
97 0000054E 61000016 
BSR 
PRINT 
98 00000552 323900001008 
MOVE.W 
COUNT,Dl 
Display count 
99 O00O0S58 06010030 
ADD.B 
#$30,Dl 
100 
0000055C 61BA 
BSR 
PUTCHAR 
101 
0000055E 61C0 
BSR 
NEWLINE 
102 
00000560 4CDF7FFF 
MOVEM.L 
(A7)+,A0-A6/D0-D7 
103 
00000564 4E75 
RTS 
104 
* 
105 
00000566 48E7FFFC 
PRINT: 
MOVEM. L 
A0-A5/D0-D7,-(A7) 
Print the string pointed at by A6 
106 
0000056A 121E 
PRINT1: 
MOVE.B 
(A6) +,D1 
107 
0000056C 6700000A 
BEQ 
PRINT2 
108 
00000570 103C0006 
MOVE.B 
#6,DO 
109 
00000574 4E4F 
TRAP 
#15 
110 
00000576 60F2 
BRA 
PRINT1 
111 
00000578 4CDF3FFF 
PRINT2: 
MOVEM.L 
(A7)+,A0-A5/D0-D7 
112 
0000057C 4E75 
RTS 
113 
* 
114 
00001000 
ORG 
$1000 
115 
00001000 
START: 
EQU 
* 
116 
00001007 
LAST: 
EQU 
*+7 
117 
00000008 
MAX: 
EQU 
8 
118 
00001000 00000008 
BUFFER: 
DS.B 
8 
119 
00001008 00000002 
COUNT: 
DS.W 
1 
120 
0000100A 00000004 
INPOINT: DS.L 
1 
121 
0000100E 00000004 
OUT POINT : DS.L 
1 
122 
00001012 43697263756C 
617220427566 
6665722000 
HEADER: 
DC.B 
'Circular Buffer ' 0 
123 00001023 436861726163 
746572206F75 
74707574203D 
2000 
OUT: 
DC.B 
'Character output = \ 0 
124 
00001037 2020436F756E 
74203D2000 
COUNTER: 
DC.B 
1 
Count = \ 0 
125 
00000400 
END 
$400 

6.6 Examples of 68K programs 
287 
This example concludes our overview of 68K assembly 
language programming. We have only touched the surface of 
this topic. Real assemblers are far more complex and include 
facilities to design large programs such as the separate com-
pilation of modules. However, our intention was never to cre-
ate an assembly language programmer; it was to give you 
some insight into how machine-level instructions are used to 
achieve high-level actions. 
SUMMARY 
In this chapter we've looked at how you can use a 
microprocessor's instruction set to write simple 
programs. 
This is not an assembly language programming text and we 
cannot delve deeply into assembly language programming. We 
have, however, demonstrated that the assembly language 
programmer has to appreciate the programming environment 
that requires the use of assembly directives to allocate memory 
space for variables and constants. 
We have examined some of the 68K's instructions and 
demonstrated how to write a program. More importantly, we 
have looked at the addressing modes supported by the 68K and 
shown how data structures like lists, tables, and arrays can be 
accessed. 
Finally, we have introduced the stack used by the 68K to keep 
track of subroutine return addresses and have demonstrated 
how the stack can be used to pass parameters to and from a 
subroutine. 
« 
PROBLEMS 
6.1 Describe the action of the following 68K assembly language 
instructions in RTL (register transfer language). That is, translate 
the assembly language syntax of the 68K instruction into the 
RTL notation that defines the action of the instruction. 
(a) MOVE 
(b) MOVE 
(c) MOVE 
(d) MOVE 
(e) MOVE 
(f) MOVE #4000,5000 
3000,4000 
D0,D4 
3000,DO 
DO,3000 
#4000,D4 
(g) MOVE 
(h) MOVE 
(i) MOVE 
(j) ADD 
(k) ADD 
(I) ADD 
(A0),D3 
# 1 2 , ( A 0 ) 
( A l ) , ( A 2 ) 
D2,D1 
#13,D4 
( A 3 ) , 1 2 3 4 
6.2 Explain why the following assembly language and RTL 
constructs are incorrect. 
(a) MOVE D3 , #4 
(b) MOVE 
[D3] ,D2 
(c) MOVE 
(D3) ,D2 
(d) [D3] <- A0 + 
(e) [D3] <- #3 
(f) 3 <- [D3] 
6.3 Create a simple 68K program called ADDER. Your program 
should add together the numbers 6,4,12,16,17, and 50. The 
program is to be assembled with the 68K cross-assembler and 
then run on the 68K simulator (either Easy68K or the Teesside 
assembler/simulator). Run the binary file you have created in 
the simulation mode. 
6.4 Give examples of valid 68K assembly language instructions 
that use 
(a) register-to-register 
addressing 
(b) register-to-memory 
addressing 
(c) memory-to-register 
addressing 
(d) memory-to-memory 
addressing 
6.5 The following 68K assembly language program illustrates 
what an assembler does and is designed only to illustrate some 
points. The source program is followed by its assembled listing. 
Examine both the source code and listing file, and try to follow 
what is happening. 
ORG 
$400 
Test 
EQU 
6 
Alan 
EQU 
7 
XXX 
DS.W 
2 
YYY 
DC.L 
$12345678 
Name 
DC.B 
'Clements 
DC.B 
$FF 
DC.L 
Test+Name 
DC.B 
4 
MOVE.L #Name,A0 
Next 
MOVE.B (A0)+,D0 
CMP.B #$FF,D0 
BEQ 
Exit 
BSR 
Print 
BRA 
Next 
Exit 
* 
STOP 
#$2700 
Print 
NOP 
NOP 
RTS 
END 
$400 
Dummy equates 
Save two words of storage 
Put the longword $12345678 in memory 
Put an ASCII string in memory 
Store a 32-bit constant 
Put 4 in memory 
Pick up a character 
Test for end of string 
And exit on terminator 
Print a character 
Repeat 
Halt the 68K 
Dummy subroutine 
Return 
END needed ($400 is start address) 

288 
Chapter 6 Assembly language programming 
The following code was produced by a 68K cross-assembler 
from the above source code. 
(c) What is the effect of the assembler directive DC . w 12 34? 
(d) What is the effect of the assembler directive DC. w $1234? 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
00001000 
00001000 
00001004 
00001008 
00001010 
00001012 
00001016 
00001018 
0000101E 
00001020 
00001024 
00001028 
0000102C 
0000102E 
00001032 
00001034 
00001036 
00000006 
00000007 
00000004 
12345678 
TEST: 
ALAN: 
XXX: 
YYY: 
436C656D656E NAME: 
7473 
FF 
0000100E 
04 
207C00001008 
1018 
0C0000FF 
67000008 
61000008 
60FO 
4E722700 
4E71 
4E71 
4E75 
00000400 
NEXT: 
EXIT: 
PRINT: 
ORG 
EQU 
EQU 
DS.W 
DC.L 
DC.B 
DC.B 
DC.L 
DC.B 
MOVE.L 
MOVE.B 
CMP.B 
BEQ 
BSR 
BRA 
STOP 
NOP 
NOP 
RTS 
END 
$400 
6 
7 
2 
$12345678 
'Clements' 
$FF 
TEST+NAME 
4 
#NAME,A0 
(A0)+,D0 
#$FF,DO 
EXIT 
PRINT 
NEXT 
#$2700 
$400 
;Dummy equates 
;Save two words of storage 
;Put longword $12345678 in memory 
;Put an ASCII string in memory 
/Store a 32-bit constant 
;Put 4 in memory 
/Pick up a character 
/Test for end of string 
/And exit on terminator 
/Print a character 
/Repeat 
/Halt the 68K 
/Dummy subroutine 
/Return 
/END needed 
6.6 By means of a memory map explain the effect of the 
following sequence of 68K assembly language directives. 
ORG 
$600 
DS.L 2 
DC.L 2 
DC.B '1234' 
Time DC.B 6 
Top 
DS.B 6 
BScl EQU 
2 
ITl 
EQU 
3 
SE1 
DS.B ITl+BScl 
6.7 What would the following 68K assembly 
language fragment store at address $1002? 
THEN 
ELSE 
EXIT 
ORG 
$1000 
p 
EQU 
5 
Q 
DS.B 2 
One 
DC.W P+Q 
6.8 What is wrong with each of the following 68K assembly 
language operations? 
(b) 
(c) 
(d) 
(e) 
(0 
6.9 Answer the following questions about 68K assembler 
conventions. 
(a) What is the effect of the assembler directive ORG $400? 
(b) What is the effect of the assembler directive DS. W 20? 
MOVE 
Temp,#4 
ADD.B 
#1,A3 
CMP.L 
DO,#9 
MOVE.B #500,D5 
DS.B 
1,2 
ADD.W 
+(A2),D3 
ORG 
#400 
BEQ.B 
Loop 3 
(e) What is the effect of the'+' in the effective address 
(A0)+? 
(f) What is the effect of the' - ' in the effective 
address -(A0)? 
(g) Why'ADDA.L #4, 
A0',but'ADD.L#4,D0'? 
6.10 What is wrong with the following fragment of 68K assem-
bly language (the error is one of semantics)? 
CMP.W 
#4,Q 
IF Q = 4 THEN X = 5 ELSE X = Y 
BNE 
ELSE 
IF Q 
4 THEN goto 'ELSE' 
MOVE.W #5,X 
IF Q = 4 THEN X = 5 
MOVE.W Y,X 
ELSE part (i.e. X = Y) 
Leave the program 
6.11 Translate the following fragment of high-level language 
into 68K assembly language. 
I F T = 5 
THEN X = 4 
END_IF 
Assume that T and X are in memory. Write a program to imple-
ment this fragment of code and run it on the 68K simulator. 
Select your own values for variables T and X. Use the simulator's 
trace mode to observe the behavior of the program. 
6.12 Translate the following fragment of high-level language into 
68K assembly language. Use a 68K simulator to test your program. 
I F T = 5 
THEN X = 4 
ELSE Y = 6 
END_IF 
6.13 The 68K can operate with byte, word, and longword 
operands. What does this mean? Which type of operand do you 
use in any particular circumstance? 
a 
& 

6.6 Examples of 68K programs 
289 
6.14 Explain what the following 68K program does. Use the 68K 
simulator to test your observations. 
Again 
MOVE.B 
#20,DO 
MOVEA.L 
#$1000 
CLR.B 
(AO) 
ADDA.L 
#1,A0 
SUB.B 
#1,D0 
BNE 
Again 
AO 
6.15 A sequence, or string of 1-byte ASCII characters is stored 
at memory location $600 onward. A second sequence of equal 
length is stored at memory location $700 onward. Each 
sequence ends with the character $0D (i.e. the ASCII value for a 
carriage return). Write a 68K assembly language program to 
determine whether or not these two strings are identical. If they 
are identical, place the value $00 in data register DO. If they are 
not, place the value $FF in DO. 
Use the 68K cross-assembler and simulator to write the 
program and test it. 
Modify your program to use the simulator's character input 
routine to input the two strings into memory. The simulator's 
character input code that puts a byte into D1 is 
Get char 
MOVE.B 
TRAP 
#5,DO 
#15 
6.16 A sequence of ASCII-encoded characters is stored at mem-
ory location $600 onwards and is terminated by a $0D. Write a 
program to reverse the order of the sequence (i.e. the first value 
at location $600 will now be SOD, which was the old end of the 
string). 
Use the 68K simulator to input a string and print it in reverse 
order. 
The simulator's character output code that prints the byte in 
D1is 
Put char 
MOVE.B 
TRAP 
#6, DO 
#15 
6.17 The following program contains both syntax and semantic 
errors. What is the difference between these two types of error? 
Locate both types of error. 
* This is a program to designed locate the smallest number in a table 
ORG 
Numbers 
DS.B 
#400 
42,3,060,20,8,9,$A,$C,7,2,$F,5AF,8,600,0A,9,4 0,6,#FF 
MOVE.B 
#Numbers,A0 
CLR.B 
DO 
N_2 
MOVE.B 
(A0),D1 
ADD.B 
#1,A1 
CMP.B 
D0,D1 
BGE 
N_l 
MOVE . B 
Dl, DO 
N_l 
CMP.B 
D1,#$FF 
BNE 
N_2 
BRA 
Print 
END 
$400 
AO points to table of numbers 
Current smallest number 
Get a number 
Point to next number 
Is new number lower? 
Keep new low number 
Check for end 
Print it 
Print 
MOVE.B 
#5,D1 
TRAP 
#15 
RTS 
STOP 
#$2700 

290 
Chapter 6 Assembly language programming 
6.18 Examine the following fragment of pseudocode and its 
translation into 68K assembly language. Work through this code 
and ensure that you understand it. Is the program correct? Can 
you improve it? 
Next 
Else 
End 
X = 5 
Y = 7 
FOR 1= 1 TO 9 
Y = = Y + I 
IF T(I) = J(I) f X THEN J(I) = T(I) * 4 - Y 
ELSE J(I) = J(D - T(I) 
END_FOR 
ORG 
$400 
MOVEA 
#T,A0 
A0 points at base of array T 
MOVEA 
#J,A1 
Al points at base of array J 
MOVE 
#1,D0 
Use DO as a counter to hold '. 
MOVE 
#5,D1 
X = 5; Dl is X 
MOVE 
#7,D2 
Y = 7; • D2 is Y 
ADD 
D0,D2 
Y = Y + I 
MOVE 
(A0) ,D3 
Read T(I) into D3 
MOVE 
(Al),D4 
Read J (I) into D4 
MOVE 
D4,D5 
D5 is a temp copy of J(I) 
ADD 
D1,D5 
Compute J(I) + X in D5 
CMP 
D5,D3 
IF T(I) = J(I) + X 
BNE 
Else 
MOLU 
#4,D3 
THEN compute T(I) * 4 - Y 
SUB 
D2,D3 
MOVE 
D3,(Al) 
J(I) = T(I) * 4 - Y 
BRA 
End Loop 
SUB 
D3, D4 
J(I) = J(I) - T(I) 
MOVE 
D4,(Al) 
>op ADDA 
#2,A0 
Point to ] 
lext element in T 
ADDA 
#2,A1 
Point to next element in J 
ADD 
#1,D0 
Increment the loop counter 
CMP 
#10,DO 
Repeat until I = 10 
BNE 
Next 
6.19 Write a 68K program to run on the simulator, which 
• inputs a single ASCII-encoded numeric character 
• converts the character into binary numeric form 
• inputs and converts a second character 
• adds the two numbers 
• converts the numeric result into character form 
• prints the result as ASCII characters. 
If the input characters are 5 and 7, the displayed result 
should be 12. 
6.20 Write a program to arrange a sequence of eight numbers 
in descending order. You can store the numbers in memory 
before the program is executed by means of the DC. B assem-
bler directive. For example 
L i s t 
DC.B 1 , 2 , 5 , 4 , 8 , 5 , 4 , 2 
There are many ways of performing this sorting operation. One 
of the simplest is to search the list for the largest number and 
put it at the top of the list, then do the same to the remaining 
numbers, and so on. Use the 68K simulator to test your 
program. 
6.21 Why is it best to pass parameters to and from a subrou-
tine by means of the stack? 
6.22 Write a subroutine to carry out the operation X*(Y+Z), 
where X,Y, and Z are all wordlength (i.e. 16-bit) values.The three 
parameters, X, Y, and Z, are to be passed on the stack to the 
procedure.The subroutine is to return the result of the 
calculation via the stack. Remember 
that the 68K instruction MULU 
DO, Dl multiplies the 16-bit 
unsigned integer in DO by the 16-bit 
unsigned integer in D1 and puts the 
32-bit product in Dl. 
Write a subroutine, call it, and 
pass parameters X, Y, and Z on the 
stack. Test your program by using 
the 68K simulator's debugging 
facilities. 
6.23 Write a subroutine ADDABC 
that performs the operation 
C = A + B. Variables A, B, and C are 
all word (i.e. 16-bit) values.Test your 
program on the 68K simulator. 
Your calling code and subroutine 
should have the following 
features. 
• Parameters A and B should be 
passed on the stack to the 
procedure by reference (i.e. by 
address). 
• Because parameters A and Bare 
adjacent in memory, you need pass 
only the address of parameter A to 
the subroutine (because the 
address of parameter B is 2 bytes 
on from parameter A). 
Parameter C should be passed back to the calling program on 
the stack by value. 
Before you call the subroutine, make room on the stack for the 
returned parameter (i.e. parameter C). 
After calling the subroutine, read the parameter off the stack 
into data register DO (i.e. DO should end up containing the 
value of A+B). 
The subroutine ADDABC must not corrupt any registers. 
Save all working registers on the stack on entry to the 
subroutine and restore them before returning from the 
subroutine. 
When you write your code, preset the stack pointer to a 
value like $1500 (by using either MOVEA . L #$1500, A7 
or LEA $1500, A7). Doing this will make it easier to follow 
the movement of the stack while your program is 
running. 
Make certain that you are operating with the correct operand 
sizes! Use .w for data values and . L for addresses/pointers. 
Some of the important instructions you might need are 
provided below. Make sure you understand exactly what they 
do. 

6.6 Examples of 68K programs 291 
MOVEM.L RegList,-(A7) 
MOVEM.L 
(A7)+,RegList 
LEA 
X(Ai),Aj 
MOVEA.L (Ai),Aj 
Push a group of registers on stack 
Pull a group of registers off stack 
Load Aj with the contents of register Ai + X 
Load Aj with longword pointed at by Ai 
Your program should be of the general form 
ORG 
$400 
LEA 
$1500,A7 
BSR ADDABC 
STOP #$2700 
ADD? iBC ... 
Set up the stack pointer with an easy value 
Pass the parameters 
Call the subroutine 
Get the result, C, in DO 
Clean up the stack 
Halt execution 
RTS 
ORG 
$1200 
A 
DC.W $1234 
B 
DC.W $ABAB 
END 
$400 
Put the test data here 
This is the first parameter 
This is the second parameter 
This is not an easy or trivial problem. You will need to draw a 
map of the stack at every stage and take very great care not to 
confuse pointers (addresses) and actual parameters. 
6.24 Suppose you wish to pre-load memory with the value 
1234 before executing a program. Which of the following opera-
tions is correct? 
(a) DC.B 
(b) DC.W 
(c) DC.W 
(d) DS.B 
(e MOVE.W 
#1234 
1234 
#1234 
$1234 
#1234,Location 
6.25 Which of the following defines MOVE. B (A2)+,D3? 
(a) D3 
<r- [[A2]]; [A2] <r- [A2] + 1 
(b) [D3] <r- [[A2]]; [A2] ^ [A2 ] + 1 
(c) D3] <- [[A2]]; [A2] <- [A2] + 1 
(d) [A2] <- [A2] + 1 ; [D3] <- [A2]; 
6.26 Which of the following statements is true when a parameter 
is passed to a subroutine by reference (i.e. not by value). 
(a) The parameter can be put in an address register. 
(b) The address of the parameter can be put in an address 
register. 
(c) The address of the parameter can be pushed on the stack. 
(d) The parameter can be pushed on the stack. 
(e) Parts (a) and (d) are correct. 
(f) Parts (b) and (c) are correct. 
6.27 Consider the following code: 
MOVE.W X,-(A7) 
Push X 
MOVE.L Y,-(A7) 
Push Y 
BSR 
PQR 
C a l l PQR 
Clean__up 
Clean up t h e s t a c k 
(a) Why do you have to clean up the stack after returning from 
the subroutine? 
(b) What code would you use to clean up the stack? 
(c) Draw a memory map of the stack immediately before exe-
cuting the RTS in the subroutine PQR. 
6.28 Write an assembly language program to reverse the bits 
of a byte. 
6.29 Explain why the following assembly language and RTL 
constructs are incorrect 
(a) MOVE D4, #$64 
(b) MOVE 
(D3) ,D2 
(c) [D3] <- A0 + 3 
(d) [D3] <- #3 
6.30 The 68K has both signed and unsigned conditional 
branches. What does this statement mean? 
6.31 You cannot (should not?) exit a subroutine by jumping out 
of it by means of a branch instruction. You must exit it with an 
RTS instruction.Why? 
6.32 Assume that a string of ASCII characters is located in 
memory starting at location $2000. The string ends with the 
character 'Z'. Design and write a 68K assembly language pro-
gram to count the number of 'E's, if any, in the string. 
6.33 Express the following sequence of 68K assembly language 
instructions in register transfer language and explain in plain 
English what each instruction does. 
(a) LEA 
4(A2) ,A1 
(b) MOVEA.L A3,A2 
(c) MOVE.B 
(AI) ,D3 
(d) MOVE.B #5, (Al) 
(e) BCS 
ABC 
(f) MOVE.B (Al)+,-(A3) 
6.34 The following fragment of 68K assembly language has 
several serious errors. Explain what the errors are. Explain how 
you would correct the errors. 

292 
Chapter 6 Assembly language programming 
MOVE.B 
X,D0 
CMP.B 
#4,DO 
BEQ 
Add_6 
MOVE.B 
D0,X 
PEA 
X 
BSR 
Sqr 
X 
DS.W 
1 
* 
STOP 
#$2700 
ADD 6. 
ADD.B 
#6,DO 
RTS 
Sqr 
MOVE.L 
(A7)+,D2 
MULU 
D2,D2 
MOVE.L 
RTS 
D2,-(A7) 
Get X in a data register 
IF X = 4 THEN X = X + 6 
Restore X in memory 
Push X on the stack 
Calculate X2 
Save space for X 
X = X + 6 
Return 
Get X 
Square X 
Put X*X on the stack 
Return 
6.35 Suppose you are given an algorithm and asked to design 
and test a program written in 68K assembly language. How 
would you carry out this activity? Your answer should include 
considerations of program design and testing, and the necessary 
software tools. 
6.36 Suppose that DO contains SF12C4689 and D1 contains 
$21 IDOFFI.What is the result of 
(a) ADD.B D0.D1 
(b) ADD.W D0,D1 
(c) ADD.L D0,D1 
In each case, give the contents of D1 after the operation and 
the values of the C, Z, N, and V flags. 
6.37 Suppose that A0 contains $F12CE600.What is the result 
of 
(a) ADDA.L #$1234, A0 
(b) ADDA.W #$1234,A0 
(c) ADDA.W #$4321,A0 
6.38 What is the effect of the following code? 
XXX 
CLR 
DO 
MOVE.B D0,D1 
MOVE.B #10,D2 
ADD.B 
D2,D0 
ADD.B 
#1,D1 
ADD.B 
D1,D0 
SUB.B 
#1,D2 
BNE 
XXX 
STOP 
#$2700 

Structure of the CPU 
7 Structure of the CPU 
Chapters 5 and 6 describe what a 
computer does; in this chapter 
we show how a computer 
converts an instruction op-code 
into the actions that implement 
the op-code. We build on some 
of the material we covered in the 
section on digital logic. In this 
chapter we demonstrate how a 
computer is organized internally 
and how it reads instructions 
from memory, decodes them, 
and executes them. 
INTRODUCTION 
In Chapters 2 and 3 we introduced combinational and sequential logic elements and 
demonstrated how to build functional circuits. In Chapters 5 and 6 we introduced the instruction 
set architecture and low-level programming.This chapter bridges the gap between digital circuits 
and the computer by demonstrating how we can construct a computer from simple circuits; that 
is, we show how a computer instruction is interpreted (i.e. executed). 
We begin by describing the structure of a simple generic CPU. Once we see how a computer 
operates in principle, we can look at how it may be implemented. We describe the operation of a 
very simple one-and-a-half address machine whose instructions have two operands; one in 
memory and one a register. Instructions are written in the form ADD A , B that adds A to B and 
puts the result in B. Either A or B must be a register. 
Some readers will read this introduction to the CPU before the previous two chapters on 
assembly language programming. Consequently, some topics will be re-introduced. 
Instead of introducing the computer all at once, we will keep things simple and build up a CPU 
step by step. This approach helps demonstrate how an instruction is executed because the 
development of the computer broadly follows the sequence of events taking place during the 
execution of an instruction. In the next chapter we will find that this computer is highly simplified; 
real computers don't execute an instruction from start to finish.Today's computers overlap the 
execution of instructions. As soon as one instruction is fetched from memory, the next instruction 
is fetched before the previous instruction has completed its execution. This mechanism is called 
pipelining and we examine it more closely in the next chapter. 
.CHAPTER MAP 
\ S The instruction set 
~ architecture 
".The computer's instruction set 
' architecture (ISA) describes the 
; -tow-level programmer's view of 
. .the computer and defines the 
." type of operations a computer 
:• parries out We are interested in 
•' three aspects of the ISA: the 
.'• nature of the instructions, the 
.resources such as registers and 
.-.memory used by the instructions 
'• and the ways in which the 
•' iipstructlons access data 
• '{addressing modes). 
6 Assembly language 
programming 
Chapter 6 builds on the previous 
chapter by demonstrating how 
instructions are used to 
construct entire programs. We 
introduce the programming 
environment via a simulator thai' 
runs on a PC and demonstrate 
how to implement some basic 
algorithms. 
8 Accelerating 
performance 
The previojs chapter described 
the structure of a simple 
computer. Here we describe how 
the performance of computers 
can be increased by overlapping 
the execution of instructions 
(pipelining). We also look at 
cache memory and introduce 
parallel processing. 

294 
Chapter 7 Structure of the CPU 
7.1 The CPU 
A von Neumann stored program digital computer operates by 
reading instructions from memory and executing them one 
by one in sequence. If you wish to evaluate X2 + 1 on a 68K 
processor where X is a memory location, you may write 
MOVE.W X,DO 
MOLU 
DO,DO 
ADD.W 
I I , DO 
MOVE.W D0,X 
Get X in a register 
Square it 
Add 1 
Update X in memory 
We now demonstrate how instructions like MOVE. W X, DO 
are read from memory and how the sequence of actions that 
implement this operation are carried out inside the CPU. 
Because the CPU is such a complex device, we have 
decided to walk through the execution of an instruction, step 
by step, on a very primitive hypothetical computer. We 
begin with the address paths that are used to locate the next 
instruction to be executed. 
7.1.1 The address path 
Figure 7.1 provides the block diagram of part of a CPU. In 
this diagram only the address paths and the paths needed to 
read an instruction from memory are shown for clarity. We 
have omitted most of the data paths required to execute 
instructions to avoid clutter. Address paths are shown in blue 
and data paths are in light blue. 
The address paths are the highways along which addresses 
flow from one part of the CPU to another. An address repre-
sents the location of a data element within the memory. There 
are three types of information flow in a computer: address, 
A HYPOTHETICAL COMPUTER 
Anyone describing the internal operation of a computer 
must select an architecture and an organization for their 
target machine. We have two choices: register to memory 
or register to register. The register-to-memory model fits 
the architecture of processors like the Pentium and 68K, 
whereas the register-to-register model corresponds to 
processors like the ARM, which we introduce later. When 
describing the internal structure of a computer, we could 
describe either a system that executes an instruction to 
completion before beginning the next instruction, or a 
computer that overlaps or pipelines the execution of 
instructions. 
I have decided to begin this chapter with the description 
of a register-to-memory, non-pipelined processor. 
A non-pipelined organization is easier to describe than 
one that overlaps the execution of instructions. 
PC 
MAR 
Program counter 
(ncrementer 
/ 
The first stage in the execution of 
any instruction is to fetch it from 
memory. The program counter 
contains the address of the next 
instruction to be executed 
Op-code 
Address 
The contents of the 
PC are incremented 
each time it is used 
Instruction 
address 
instruction 
op-code 
Memory address register 
- When you read an instruction, 
it is moved to the instruction 
register where it is decoded 
Address 
Memory 
Data 
Memory buffer register 
The main memory or 
immediate access store 
contains both instructions 
and data 
MBR 
_Data gaffis, 
When you read from memory, 
the contents of the memory 
location accessed are loaded 
into the MBR 
Address paths. _J) 
Control signals 
Figure 7.1 The CPU's address paths. 
! 
: Ac* >dss of the 
/ 
i I ne-,j instruction /_ 
Control unit 
FT] 

7.1 The CPU 
295 
CONSTANTS, VARIABLES. POINTERS—A REMINDER 
A constant or literal is a value that does not change during the 
execution of a program. Its value is set at compile time or 
assemble time. Literal addressing is used to handle constants. 
A variable is a value that changes during the execution of 
a program. 
A pointer is a variable that contains the address of a 
variable; that is, a pointer points to a variable in memory. 
An address register is a pointer register. 
The following code implements the expression X = 5 + Y + Z3 
and illustrates the nature of constants, variables, and pointers. 
;load DO with the variable Y 
;add the constant 5 
;load AO with a pointer to Z 
; add the variable Z-. 
/store the result in variable X 
MOVE.W Y,DO 
ADD.W 
4*5, DO 
LEA 
Z,A0 
ADD.W 
3(AO),D0 
MOVE.W D0,X 
data, and control. Data comprises the instructions, constants, 
and variables that are stored in memory and registers. 
Control paths comprise the signals that trigger events, 
provide clocks, and control the flow of data and addresses 
throughout the computer. 
7.1.2 Reading the instruction 
Before the CPU can execute an instruction, the instruction 
must first be brought to the CPU from the computer's 
memory. We begin our description of the way in which a 
program is executed with the CPU's program counter (also 
called instruction counter or location counter). The expression 
program counter is a misnomer. The program counter contains 
the address of the next instruction in memory to be executed 
and it doesn't count programs or anything else. 
The program counter points to the next instruction to be 
executed. If, for example, [PC] = 1234 (i.e. the PC contains 
the number 1234), the next instruction to be executed is to be 
found in memory location 1234. 
Fetching an instruction begins with the contents of the 
program counter being moved to the memory address 
register (i.e. [MAR] <— [PC]). Once the contents of the pro-
gram counter have been transferred to the memory address 
register, the contents of the program counter are incremented 
and moved back to the program counter; that is, 
[PC] 
[PC] 
+ 4 
At the end of this operation, the program counter points to 
the next instruction while the current instruction is being 
executed. The program counter in incremented by 4 rather 
than 1 because many of today's high-performance computers 
have 32-bit instructions. Computers are byte-addressed but 
have 32-bit architectures; that is, they can address individual 
bytes in memory, although instructions and data normally 
fall on 32-bit (i.e. 4-byte) boundaries.1 
The memory address register (MAR), holds the address of 
the location in the memory into which data is being written in 
a write cycle, or from which data is being read in a read cycle. 
At this stage, the MAR contains a copy of the (previous) con-
tents of the PC. When a memory read cycle is performed, the 
contents of the memory location specified by the MAR are read 
from the memory and transferred to the memory buffer register 
(MBR). We can represent this read operation in RTL terms as 
[MBR] 
[ [MAR] : 
We interpret the expression [ [MAR] ] as the contents of the 
memory whose address is given by the contents of the MAR. The 
memory buffer register is a temporary holding place for data 
received from memory in a read cycle, or for data to be trans-
ferred to memory in a write cycle. Some texts refer to the 
MBR as the memory data register (MDR). At this point in the 
execution of an instruction, the MBR contains the bit pattern 
of the instruction to be executed. 
The instruction is next moved from the MBR to the 
instruction register (IR) where it is divided into two fields. 
One field in the IR contains the operation code (op-code) that 
tells the CPU what operation is to be carried out. The other 
field, called the operand field, contains the address of the data 
to be used by the instruction. The operand field can also 
provide a constant to be employed by the operation code 
when immediate or literal addressing is used. Note that we 
are assuming a one-address instruction format. 
The control unit (CU) takes the op-code from the instruc-
tion register together with a stream of clock pulses and 
generates signals that control all parts of the CPU. The time 
between individual clock pulses is in the range 0.3 ns to 100 ns 
(i.e. 3 X 10"10 to 10~7s). The control unit is responsible for 
moving the contents of the program counter into the MAR, 
executing a read cycle, and moving the contents of the MBR 
to the IR. Later we look at the control unit in more detail and 
demonstrate how it goes about interpreting an op-code. 
1 Remember that the 68K has variable-length instruction and its PC 
is incremented by 2, 4, 6, 8, or 10; that is instruction fall on 16-bit 
boundaries For the purpose of this chapter, we assume that instructions 
are all 4 bytes long. 

296 
Chapter 7 Structure of the CPU 
THE INSTRUCTION REGISTER 
A modern RISC processor like the ARM, which we introduce 
in Chapter 9 has a 32-bit instruction register which is 
sufficient for an op-code and three register addresses; 
for example, ADD Rl, R2, R3. Processors like the Pentium 
and 68K have very complex variable-length instruction 
formats and you cannot use a simple fixed-length 
instruction register. The processor has to fetch the first 
16 bits of an instruction from memory, decode it, and 
execute successive fetches to assemble the rest of the 
instruction. 
FETCH [MAR] «- [PC] 
[PC] *- [PC] + 4 
[MBR] 4-
[[MAR]] 
[IR] <- [MBR] 
CU 
<-
[ I R (op-code) 
Copy contents of the PC to the MAR 
Increment the contents of the PC 
Read the instruction from memory 
Move the instruction to the IR 
Transmit the op-code to the control unit 
Table 7.1 The FETCH phase expressed in register transfer language. 
All instructions are executed in a two-phase operation 
called a fetch-execute cycle. During the fetch phase, the 
instruction is read from memory and decoded by the control 
unit. The fetch phase is followed by an execute phase in which 
the control unit generates all the signals necessary to execute 
the instruction. Table 7.1 describes the sequence of opera-
tions taking place in a fetch phase. In Table 7.1 FETCH is a 
label that serves to indicate a particular line in the sequence of 
operations. The notation iR(op_code) means the operation-
code field of the instruction register. 
7.1.3 The CPU's data paths 
Now that we've sorted out the fetch phase, let's see what else 
we need to actually execute instructions. Figure. 7.2 adds new 
data paths to the simplified CPU of Fig. 7.1, together with an 
address path from the address field of the instruction register 
to the memory address register. Other modifications to 
Fig. 7.1 included in Fig. 7.2 are the addition of a data register, 
DO, and an arithmetic and logical unit, ALU. 
The data register called DO holds temporary results during 
a calculation. You need a data register in a one-address 
machine because dyadic operations with two operands such 
as ADD x,DO take place on one operand specified by the 
instruction and the contents of the data register. This instruc-
tion adds the contents of memory location X to the contents 
of the data register and deposits the result of the addition in 
the data register, destroying one of the original operands. 
The arrangement of Fig. 7.2 has only one general-purpose 
data register, which we've called DO for compatibility with the 
68K we used in the previous chapters. Some first-generation 
8-bit microprocessors had only one general-purpose data 
register, which was called the accumulator. 
We can represent an ADD X, DO instruction2 by the RTL 
expression 
[DO] 
[DO] 
+ 
[X] 
The arithmetic and logic unit (ALU) is the workhorse of 
the CPU because it carries out all the calculations. Arithmetic 
and logical operations are applied either to the contents of 
the data register or MBR alone, or to the contents of the data 
register and the contents of the MBR. The output of the ALU 
is fed back to the data register or to the MBR. 
Two types of operation are carried out by the ALU—arith-
metic and logical. The fundamental difference between arith-
metic and logical operations is that logical operations don't 
generate a carry when bit a; of word A and bit b; of B are oper-
ated upon. Table 7.2 provides examples of typical arithmetic 
and logical operations. A logical shift treats an operand as a 
string of bits that are moved left or right. An arithmetic shift 
treats a number as a signed two's complement value and 
propagates the sign bit during a right shift. Most of these 
operations are implemented by computers like the 68K, 
Pentium, and ARM. 
Having developed our computer a little further, we can 
now execute an elementary program. Consider the high-level 
language operation P = Q + R. Here the plus symbol means 
arithmetic addition. The assembly language program 
required to carry out this operation is given below. 
Remember that P, Q, and R are symbolic names that refer to 
the locations of the variables in memory. 
LOAD Q, DO Load data register DO with the contents of 
memory location Q3 
ADD R, DO 
Add the contents of memory location R to data 
register DO 
STORE DO, P Store the contents of data register DO in memory 
location P 
2 A machine with a single accumulator does not need to specify it explic-
itly; for example, an 8-bit microprocessor may use ADD P to indicate add 
the contents ofP to the accumulator. We write ADD P, DO and make DO 
explicit to be consistent with the notation we used for 68K instructions. 
3 We have denned explicit LOAD and STORE operations to be 
consistent with the CPU we construct later in this chapter. The 68K uses 
a single MOVE operation to indicate LOAD and STORE 

7.1 The CPU 
2 9 7 
PC 
MAR 
Mi-i.-or/ ."Kicrirss regirii-r 
IncrenivittT 
Kemnij 
Address of next 
location to be 
accessed 
Op-code 
Address 
Mi-noiy buffer resj:s:er 
MBR 
J" 
Control unit 
Clock 
Control unit 
Clock 
Control unit 
" ' i 
' " ' i ' 
' r 
Data read from 
memory or to be 
written to memory 
Control signals 
CZ 
Address paths 
Data paths 
Output from 
ALU goes to 
DO or MBR 
Data is transmitted 
to the ALU where it 
s operated on 
Arithmetic and 
logic units performs 
data processin; 
operations 
Figure 7.2 The CPU's address and data paths. 
Operation 
Class 
Addition 
Subtraction 
Negation 
Multiplication 
Division 
Divide by 2 
Multiply by 2 
AND 
OR 
NOT 
EOR 
Shift left 
Shift right 
Arithmetic 
Arithmetic 
Arithmetic 
Arithmetic 
Arithmetic 
Arithmetic 
Arithmetic 
Logical 
Logical 
Logical 
Logical 
Logical 
Logical 
Table 7.2 Typical arithmetic and logical operations. 
~ 
~ 
The one-address machine requires a rather 
Typical mnemonic c u m b e r s o m e 
seqUence of operations just to 
carry out the simple action of adding two num-
bers. If we had a computer with a three-address 
format, we could have written 
ADD Q, R, P Add the contents of Q to the con-
tents of R and put the result in P 
Three-address machines are potentially* faster 
than one-address machines, because they can do 
in one instruction things that take other machines 
three operations. However, the power of three-
address machines can be achieved only by means 
of a more complex and expensive CPU and mem-
ory system. 
We can demonstrate how the CPU operates 
by examining the execution of ADD R, DO in 
terms of register transfer language. Table 7.3 
ADD 
SUB 
NEG 
MULU 
DIVU 
ASR 
ASL 
AND 
OR 
NOT 
EOR 
LSL 
LSR 
Operand address 
to memory address 
register 
-_^^ 
instruction read 
from memory 
Vr:.-\ rogisH"1 DO 
••(•\,ui 
ALJ 
r 
Data 
Afldreii 
^ 
IRJ 
Program counter 

298 
Chapter 7 Structure of the CPU 
FETCH [MAR] <- [PC] 
[PC] <- [PC] + 4 
[MBR] <- [ [MAR] ] 
[IR] <— [MBR] 
CU 
<-
[IR(op-code) 1 
ADD 
[MAR] *-
[ ^(address) ] 
[MBR] <r~ [ [MAR] ] 
ALU 
<- [MBR], ALU 
[DO] 
4-
ALU 
[DO] 
Move the contents of the PC to the MAR 
Increment the contents of the PC 
Read the current instruction from the memory 
Move the contents of the MBR to the IR 
Move the op-code from the IR to the CU 
Move the operand address to the MAR 
Read the data from memory 
Perform the addition 
Move the output of ALU to the data register 
Operations sharing the same line are executed simultaneously. ALU <- [MBR]andALU <— [DO] are executed simultaneously. 
Table 7.3 Expressing the FETCH/EXECUTE cycle for an ADD instruction in RTL. 
MICROPROGRAMMING 
The terms microprogram, microprogramming, microcode, 
microinstruction, and micro-operation have nothing to do 
with microprocessor ox microcomputer. A micro-operation 
is the smallest event that can take place within a computer; 
typical micro-operations are the clocking of data into 
a register, a memory read operation, putting data on a bus, 
or adding two numbers together. 
A microprogram consist of a sequence of microinstructions 
that, when executed, implement a machine-level instruction. 
For example, the machine-level or macro-level operation 
ADD P , DO can be implemented by executing a sequence 
of micro-operations.The instructions that comprise a 
microprogram are called microcode. 
gives the sequence of operations carried out during the fetch 
and execute phases of an ADD R, DO instruction. These oper-
ations tell us what is actually going on inside the computer. 
During the fetch phase the op-code is fed to the control 
unit by CU <- 
[lR(op-code) ] and used to generate all the 
internal signals required to perform the addition—this 
includes programming the ALU to do addition by adding 
together the data at its two input terminals to produce a sum 
at its output terminals. 
Operations of the form [PC] <- [MAR] or [DO] <- [DO] 
+ [MBR] are often referred to as microinstructions. Each 
assembly level instruction (e.g. LOAD, ADD) is executed 
as a series of microinstructions. Microinstructions and 
microprogramming are the province of the computer 
designer. In the 1970s some machines were user-micropro-
grammable; that is, you could define your own instruction 
set. We take a further look at microinstructions later in this 
chapter. 
7.1.4 Executing conditional 
instructions 
So far, we've considered the architecture of the single-instruction 
single-data CPU capable of executing programs in a purely 
sequential mode; that is, the computer can execute only a 
stream of instructions, one by one in strict order. We covered 
conditional behavior in the previous chapter and we require 
a means of implementing instructions such as BEQ Target 
(branch on zero flag set to Target). 
The computer in Fig. 7.2 lacks a mechanism for making 
choices or repeating a group of instructions. To do this, the 
CPU must be able to execute conditional branches or jumps 
such as BEQ XYZ. We've already met the branch that forces 
the CPU to execute an instruction out of the normal 
sequence. The block diagram of Fig. 7.3 shows the new 
address and data paths required by the CPU to execute con-
ditional branches. 
Three items have been added to our computer in Fig. 7.3: 
• a condition code register, CCR 
• a path between the CCR and the control unit 
• a path between the address field of the instruction register 
and the program counter. 
4 A three-address machine is faster than a machine with register-
to-register instructions only if the access time of memory is comparable 
to the access time of register. Memory takes much longer to access than 
internal registers and, therefore, a three-address machine is not currently 
practical. 

7.1 The CPU 
2 9 9 
PC  
Program counter 
MAR 
Incrementer 
The IR's operand field 
provides the branch -— 
target address 
IR 
Op-code 
Address 
i 
Control signals 
The control units uses the 
CCR output to decide whether 
to execute the next instruction 
or to force a branch 
The next instruction may be the 
next instruction in sequence 
or the instruction at the branch 
target address 
. The arthimetic and 
logic unit performs 
data processing 
operations 
program flow control paths 
CCR 
Condition code register 
The word in the CCR indicates whether the 
last operation gave a zero or a negative result 
or whether a carry was generated 
Detail of the 
' ALU's result are 
sent to the CCR 
Figure 7.3 Information paths in the CPU and conditional instructions. 
The condition code register or processor status register takes 
a snapshot of the state of the ALU after each instruction has 
been executed and records the state of the carry, negative, 
zero, and overflow flag bits. A conditional branch instruc-
tion interrogates the CCR's current state. The control unit 
then either forces the CPU to execute the next instruction 
in series or to branch another instruction somewhere in 
the program. Let's look at the details of the conditional 
branch. 
The CPU updates the bits of its condition code register 
after it carries out an arithmetic or a logical operation to 
reflect the nature of the result. The following is a reminder of 
the operations of the condition code bits. 
C = carry Set if a carry was generated in the last operation. 
The C-bit is, of course, the same as the carry bit in the carry 
flip-flop. 
Z = zero Set if the last operation generated a zero 
result. 
N = negative Set if the last result generated a negative result. 
V = overflow Set if the last operation resulted in an arith-
metic overflow, that is, an operation on one or two two's com-
plement values gave a result that was outside its allowable 
range (an arithmetic overflow occurs during addition if the 
sign bit of the result is different from the sign bit of both 
operands). 
Memory address register 
~~~-~-Address 
Memory 
Data 
Memory buffer register 
I 
MBR 
Data register DO 
ALU*f 
Control unit 
Clock -

300 
Chapter 7 Structure of the CPU 
The condition code register is connected to the control 
unit, enabling instructions to interrogate the CCR. For exam-
ple, some instructions test whether the last operation per-
formed by the central processor yielded a positive result, or 
whether the carry bit was set, or whether arithmetic overflow 
occurred. 
There's no point in carrying out an interrogation unless 
the results are acted upon. We need a mechanism that does 
one thing if the result of the test is true and does another 
thing if the result of the test is false. 
The final modification included in Fig. 7.3 is the addition 
of a path between the operand field (i.e. target address) of the 
instruction register and the program counter. It's this feature 
that enables the computer to respond to the result of its inter-
rogation of the CCR. 
A conditional branch instruction tests a bit of the CCR and, 
if the bit tested is clear, the next instruction is obtained from 
memory in the normal way. But if the bit tested is set, the next 
instruction is obtained from the location whose target address 
is in the instruction register. In the above description we said 
that a branch is made if a certain bit of the CCR is set; equally 
a branch can be made if the bit is clear (branches can also be 
made on the state of several CCR bits). 
The precise way in which conditional branches are actu-
ally implemented inside the computer is discussed later 
when we deal with the design of the control unit. Branch 
operations can be expressed in register transfer language in 
the form 
IF condition THEN action 
Typical machine-level conditional operations expressed 
in RTL are 
1. Branch on carry clear (jump to the target address if the 
carry bit in the CCR is 0) 
BCC t a r g e t : 
IF [C] = 0 THEN [PC] ^ [IR ! a d d r e s s ) J 
2. Branch on equal (jump to the target address if the Z-bit 
in the CCR is 1) 
BEQ target: IF [2j = l THEN IPC] f-[IR(ad,,rMS, ] 
An example of a conditional branch is as follows. 
SOB 
X,D0 
Subtract X from contents of DO 
BEQ 
Las t 
If the result was zero then branch to 
. 
Last, otherwise continue 
Last 
Target address of branch (if taken) 
7.1.5 Dealing with literal operands 
The CPU of Fig. 7.2 assumes all instructions (e.g. ADD and 
BEQ etc.) refer to an operand somewhere within the CPU's 
memory. Sometimes we wish to use instructions such as ADD 
#12 , DO, where the source operand supplies the actual value 
of the data being referred to by the op-code part of the 
instruction. Although the symbol '#' appears as part of 
the operand when this instruction is written in mnemonic 
form, the assembler uses a different op-code code for 
ADD # l i t e r a l , D 0 than it does for ADD address, DO. 
The instruction ADD . B # 12 , DO is defined in RTL as 
[DO] <- [DO] + 12. 
Figure 7.4 shows that an additional data path is required 
between the operand field of the IR and the data register and 
ALU to deal with literal operands. In fact, the architecture of 
Fig. 7.4 can execute any computer program. Any further 
modifications to this structure improve the CPU's perfor-
mance without adding any fundamentally new feature. 
Figure 7.5 completes the design of the computer. We 
have added a second general-purpose data register Dl and a 
pointer register A0. In principle, there is nothing stopping us 
adding any number of registers. As you can see, three buses, A, 
B, and C are used to transfer data between the registers and 
ALU. 
The structure of Fig. 7.5 can implement instructions with 
more complex addressing modes than the simple direct 
(absolute) addressing we have used so far; for example MOVE 
(A0) , Dl can be implemented by the following sequence of 
micro-operations. 
[MAR] <- [A0] 
Move source operand address to MAR 
[MBR] <r- [ [MAR] ] Read the actual operand from memory 
[DO] 
*- [MBR] 
CopydatatoDl 
This sequence has been simplified because, you will see 
from Fig. 7.5, that there is no direct path between register AO 
and the MBR. You would have to put the contents of AO onto 
bus A, pass the contents of bus A through the ALU to bus C, 
and then copy bus C to the MAR. We will return to this theme 
when we look at the detailed design of computers. 
We have now demonstrated the flow of information that 
takes place during the execution of a single address computer 
instruction. In the next section we reinforce some of the 
things we have covered by showing how you can simulate a 
computer architecture in C. 
7.2 Simulating a CPU 
One way of learning how a processor operates is to build one. 
We present a program in C that simulates a very simple 8-bit 
CPU. In order to make the simulator as accessible to as many 
readers as possible, we have written the simulator in C but 

7.2 Simulating a CPU 
3 0 1 
PC  
Program counter 
MAR 
Memory address register 
Incrementer 
Address 
Memory 
Op-code 
Operand 
• T y 
Control signals 
The operand field of the , 
IR is fed to the data 
register or ALU to provide 
a titeral operand 
Literal data paths 
The literal from the 
instruction register 
can by loaded into 
the data register 
The literal from the 
instruction register can 
be supplied to the ALU 
Condition code register 
Figure 7.4 Modifying the CPU to deal with literal operands. 
have avoided all but C's most basic elements. All data types 
are 8 bits and the only C constructs we use are the while, the 
if . . . then . . . else, and the switch constructs, which 
select one of several courses of action. 
We are going to construct two simulators—the first 
is a very primitive CPU with an 8-bit instruction that 
simply demonstrates the fetch/execute cycle, and the 
second is not too dissimilar to typica first-generation 8-bit 
microprocessors. 
branch instructions. Only the store instruction performs a 
write to memory. 
Choosing an instruction set requires many compromises; 
for example, if the number of bits in an instruction is fixed, 
increasing the number of different instructions reduces the 
number of bits left for other functions such as addressing 
modes or register selection. 
We can define an instruction set for our primitive 8-bit 
machine as 
7.2.1 CPU with an 8-bit 
instruction 
Instruction 
Load DO from memory 
Store DO in memory 
Our first computer has a single data register (i.e. Add memory to DO 
accumulator) called DO and all instructions are Branch to location N 
memory to register apart from the store and the If [DO] = 0 then branch to N 
Mnemonic 
RTLd efinition 
LDA N 
[DO] 
4- 
[N] 
STA N 
tN] 
4- 
[DO] 
ADD N 
[DO] 
<- 
[D0] + [N] 
BRA N 
[PC] 
<- 
N 
BEQ N 
IF [DO] =0THEN [PC] 
%A 
Memory buffer register 
Data 
—^ 
Literal to ALU" 
Data register DO 
L_ 
MBR 
Control unit 
Clock 
f(A,B) 
ALU J 
B 
A k 
CCR 

3 0 2 
Chapter 7 Structure of the CPU 
PC 
Program counter 
MAR 
Memory address register 
L 
Incrementer 
Path between 
operand field of 
IR and bus A to "" 
literals 
_
^ 
. . . _ • 
Address 
Memory 
Data 
MBR 
Op-code 
Address 
Memory buffer register 
—* 
Clock —* 
Control unit 
—* 
Control unit 
" '' 
' " '' ' 
1 ' 
Data register DO 
Data register Dl 
Address register AO 
Control signals 
The output from 
the ALU is fed via 
bus C to the registeis 
< 
Bus A 
Bus! 
CCR 
Condition code register 
Figure 7.5 Processor with multiple registers. 
We have provided only five instructions because these are 
illustrative of all instructions. This computer has an 8-bit 
instruction format that includes both die op-code and the 
operand. If we chose a 3-bit op-code (eight instructions) and 
a 4-bit operand (a 16-bit 
memory), the remaining bit 
can be used to specify the 
addressing mode (absolute 
or literal). Real 8-bit micro-
processors solve the prob-
lem of instruction set design 
by using 1 byte to provide an 
operation code and then 0, 
1, or 2 succeeding bytes to 
provide an operand. 
Figure 7.6 defines the structure of an 8-bit instruction for 
our simulated machine. 
The first step in constructing a simulator is to describe the 
action of the computer in pseudocode. 
PC = 0 
REPEAT 
Read the instruction pointed at by the PC 
Increment the program counter 
Split the instruction into 
1. operation code 
2. addressing mode 
3. operand 
IF the addressing mode is direct THEN get the operand from memory 
ELSE the operand is a literal 
Execute the instruction 
FOREVER 
BusC 
f(A,B) 
ALU 
• • • • _ > _ _ 
"~<r...r-
A 
B 

7.2 Simulating a CPU 
303 
8-bit instruction format 
7 
6 
5 
4 
3 
2 
1 
0 
1 1 
Op-code 
000 = LDA 
001 = STA 
010 = ADD 
011 = BRA 
100 = BEQ *r 
Operand 
Addressing mode 
0 = absolute 
1 = literal (immediate) 
Figure 7.6 Format of an 8-bit instruction. 
C allows you to operate on individual bits of a byte; for 
example, the operator > > n performs a right shift by n bits. 
The opcode is obtained from the three most-significant bits of 
the IR by shifting right five times. A bitwise logical AND can be 
performed between a variable and a hexadecimal value; for 
example IR & OxOF ANDs the IR with 000011112 to extract 
the operand bits in the four least-significant bit positions. 
Once we've extracted the addressing mode (bit 4 of the 
instruction register) with amode = (IR & 0x10) >> 4, we 
can calculate the source operand for the load and add instruc-
tion by 
We can now write a program 
to implement this algorithm. The 
following fragment of code is largely self-explanatory. 
The instruction in the 8-bit instruction register (IR) is 
decoded by the three operations 
if (amode == 0) source = memory[operand]; else source = operand; 
opcode = IR >> 5; 
amode 
= (IR & 0x10) 
operand = IR & OxOF; 
/* get the op-code 
*/ 
>> 4; ' /* extract the address mode bit 
*/ 
/* extract the operand 
*/ 
•define LDA 
0 
#define STA 
1 
•define ADD 
2 
•define BRA 
3 
•define BEQ 
4 
•define STOP 8 
void main(void) 
The following listing provides the C code for this CPU 
simulator. 
unsigned short int PC = 0; 
/* program counter 
*/ 
unsigned short int DO = 0; 
/* data register 
V 
unsigned short int MAR; 
/* memory address register 
*/ 
unsigned short int MBR; 
/* memory buffer register 
*/ 
unsigned short int IR; 
/* instruction register 
*/ 
unsigned •short int operand; 
/* the 8-bit operand from the IR */ 
unsigned short int source; 
/* source operand 
*/ 
unsigned short int opcode; 
/* the 3-bit op-code from the IR */ 
unsigned short int amode; 
/* the 1-bit addressing mode 
*/ 
unsigned short int memory[16]; /* the memory 
*/ 
unsigned short int run = 1; 
/* execute program while run is 1 */ 
/* Instruction format: 
/* 
7
6
5
4
3
2
1
0 
/* Bits 3 to 0 4-bit operand 
/* Bit 4 
1-bit addressing mode 
/* Bits 7 to 5 3-bit instruction code 
/* main loop */ 
while (run) 
{ 
MAR = PC; 
/* PC to MAR 

304 
Chapter 7 Structure of the CPU 
PC = PC + 1; 
MBR = memory[MAR]; 
IR = MBR; 
opcode = IR >> 5; 
amode 
= (IR & 0x10) 
operand = IR & OxOF; 
/* increment PC 
/* get next instruction 
/* copy MBR to IR 
/* get the op-code 
*/ 
>> 4; /* extract the address mods bit 
*/ 
/* extract the operand 
*/ 
if (amode == 0) source = memory[operand]; else source = operand; 
switch (opcode) /* now execute the instruction */ 
case LDA 
case STA 
case ADD 
case BRA 
case BEQ 
case STOP: 
{ DO = source; 
break; 
{ memory [operand] = DO; 
break,-; 
( DO = DO + source; 
break;; 
{ PC = operand; 
break;] 
{ if (DO == 0) PC = operand; break;] 
{ run = 0; 
break;; 
Op-code 
Bit 3 
not used 
Most of the work done in the simulator takes place in the 
switch construct at the end of this program where each 
instruction is interpreted. 
7.2.2 CPU with a 16-bit instruction 
We now describe a CPU that is much closer to the architec-
ture of typical 8-bit microprocessors. The simulator uses an 
8-bit memory with 256 locations. Each instruction occupies 
two consecutive memory locations—an 8-bit instruction 
followed by an 8-bit operand. This arrangement provides us 
with a much richer instruction set than the previous 
example. However, each fetch cycle requires two memory 
accesses. The first access is to fetch the op-code and the sec-
onds to fetch the operand; that is; 
FETCH [MAR] 
<- [pc] 
Copy contents of the PC to the MAR 
Increment contents of the PC 
Read the instruction from memory 
Move the instruction to the IR 
Save the op-code 
Copy contents of the PC to the MAR 
Increment contents of the PC 
Read the operand from memory 
Move the operand to the IR 
Save the operand 
This multibyte instruction format is used by 8-bit and 
16-bit microprocessors. Indeed, the 68K has one 10-byte 
instruction. 
The architecture of this computer is memory to register or 
register to memory; for example, it supports both ADD DO , M 
and ADD M, DO instructions. In addition to the direct and lit-
eral addressing modes, we have provided address register 
indirect addressing with a single A0 register. We have also 
Instruction 
Operand 
5
4
3
2
1
0
7
6
5
4
3
2
1
0 
. Addressing mode 00 = absolute 
01 = literal (immediate) 
10 = indexed 
11 = PC relative 
- Direction 0 = register to memory 
1 = memory to register 
[MAR] 
«- [PC] 
[PC] 
<- [PC] + 
[MBR] 
*-
[[MAR] 
[IR] 
<- [MBR] 
opcode <- [IR] 
[MAR] 
<— [PC] 
[PC] 
<- [PC] + 
[MBR] 
<- [ [MAR] 
(IR] 
<— [MBR] 
operand <-
[IR] 
Figure 7.7 Format of the CPU's instruction. 
provided program counter relative addressing (discussed in 
the next chapter) in which the operand is specified with 
respect to the current value of the program counter; for 
example, MOVE DO, 12 (PC) means store the con-
tents of data register DO 12 bytes on from the loca-
tion pointed at by the program counter. 
The instruction itself is divided into four fields, 
as Fig. 7.7 demonstrates. A 4-bit op-code in bits 7, 
6, 5, 4 provides up to 16 instructions. A 2-bit 
addressing mode in bits 1, 0 selects the way in 
which the current operand is treated. When the 
addressing mode is 00, the operand provides the 
address of the data to be used by the current 
instruction. When the addressing mode is 01 the 
operand provides the actual (i.e. literal) operand. Modes 10 
and 11 provide indexed and program counter relative 
addressing respectively (i.e. the operand is added to the A0 
register or the PC, respectively). 
Bit 2 of the instruction is a direction bit that determines 
whether the source operand is in memory or is provided by 
the data register; for example, the difference between 
MOVE DO, 123 and MOVE 12 3, DO is determined by the value 
of the direction bit. 

72 Simulating a CPU 
We can express the basic fetch cycle and decode instruction 
phase in C as 
while (run) 
MAR = PC; 
PC = PC + 1; 
MBR = memory[MAR]; 
IR = MBR; 
opcode = IR; 
MAR = PC-
PC = PC + 1; 
MBR = memory[MAR]; 
IR = MBR; 
operand = IR; 
amode 
= opcode & 0x03 
direction = (opcode & 0x04 
opcode = opcode » 4; 
Fetch instruction 
PC to MAR 
/* increment PC 
/* get next instruction 
/* copy MBR to IR 
/* store the op-code bits 
/* PC to MAR 
/* increment PC 
/* get the operand 
/* copy MBR to IR 
/* store the operand bits 
/* extract the address mode bits 
) » 2; /* get data direction 0 = 
1 = 
/* get the 4-bit instruction code 
*/ 
*/ 
*/ 
*/ 
V 
*/ 
*/ 
V 
*/ 
*/ 
*/ 
*/ 
register to memory 
memory to register 
/* use the address mode to get the operand */ 
switch (amode) 
( case 0: {source = memory[operand]; 
} 
break; /* absolute */ 
case 1: {source = operand; 
break; /* literal */ 
case 2: {source = memory[A0 + operand]; break; /* indexed */ 
} 
case 3: {source = memory[PC + operand]; break; /* PC relative */ 
} 
Each instruction is executed by means of the switch 
construct. Note that the CCR has only a zero flag (it 
would have been more complex to have provided a C- and 
V-bit). The following provides the complete code for the 
processor. 
tdefine MOVE 
•define ADD 
tdefine SUB 
•define BRA 
tdefine CMP 
tdefine BEQ 
tdefine BNE 
tdefine EXG 
tdefine STOP 
0 
1 
2 
3 
4 
5 
6 
7 
15 
/ *EXG exchanges the contents of two registers */ 
void main(void) 
{ 
unsigned short int 
unsigned short int 
unsigned short int 
unsigned short int 
unsigned short int 
unsigned short int 
unsigned short int 
unsigned short int 
unsigned short int 
PC = 0 
DO = 0 
AO = 0 
CCR = 0 
MAR; 
MBR; 
IR; 
operand; 
source; 
/* program counter 
*/ 
/* data register 
*/ 
/* address register 
*/ 
/* condition code register 
*/ 
/* memory address register 
*/ 
/* memory buffer register 
*/ 
/* instruction register 
*/ 
/* the 8-bit operand from the IR */ 
/* source operand 
*/ 

306 
Chapter 7 Structure of the CPU 
unsigned short int destination; 
unsigned short int 
unsigned short int 
unsigned short int 
unsigned short int 
unsigned short int 
/* Instruction format: 
/* the destination value 
"I 
opcode; 
amode; 
/ 
/ 
direction; 
/ 
memory[256]; 
run = 1; 
/ 
/ 
the 4-bit op-code from the IR */ 
the 2-bit addressing mode 
*/ 
the 1-bit data direction flag */ 
the memory */ 
execute program while run is 1 */ 
/* 7 5 5 4 
/* Bit 1 and 0 
/* 
/* 
/* 
/* 
/* Bit 2 
/* Bit 3 
/* Bit 7 to 4 
*/ 
3 2 1 0 
*/ 
2-bit address mode 
*/ 
00 address mode = absolute 
*/ 
01 address mode = literal 
*/ 
10 address mode = indexed 
*/ 
11 address mode = relative 
*/ 
1-bit direction (source/operand) 
*/ 
not used 
*/ 
4-bit instruction code 
*/ 
/* main loop */ 
while (run) 
i 
MAR = PC; 
/ 
PC = PC + 1; 
/ 
MBR = memory[MAR]; 
/ 
IR = MBR, 
/ 
opcode = IR; 
/ 
MAR = PC; 
/ 
PC = PC + 1; 
/ 
MBR = memory[MAR] ; 
/ 
IR = MBR, 
/ 
operand = IR; 
/ 
amode 
= opcode & Dx03, / 
direction = (opcode & 0x04) 
opcode = opcode » 4; 
PC to MAR 
increment PC 
/* get next instruction 
copy MBR to IR 
store the op-code bits 
PC to MAR 
increment PC 
get the operand 
copy MBR to IR 
store the operand bits 
extract the address mode bits 
» 2; /* get data direction 0 = 
1 = 
/* get the 4-bit instruction code 
*/ 
*/ 
V 
V 
*/ 
*/ 
*/ 
*/ 
*/ 
*/ 
*/ 
register to memory 
memory to register */ 
/* use the address mode to get the source operand */ 
switch (amode) 
{ case 0 
case 1 
case 2 
case 3 
{source = memory[operand]; 
{source = operand; 
{source = memory[A0 + operand]; 
{source = memory[PC + operand]; 
break;} /* absolute */ 
break;] /* literal "/ 
break;} /* indexed */ 
break;} /* PC relative */ 
/* now execute the instruction */ 
switch (opcode) 
{case MOVE: {if (direction == 0) destination = DO; 
else 
DO = source; 
if (DO == 0) CCR = 1; else CCR = 0; 
/* update CCR */ 
break; 
} 
case ADD: 
{if (direction == 0) 
{ destination = DO + source; 
if (destination == 0) CCR = 1; else CCR = 0; 
} 
else 
{ DO = DO + source; 
if (DO == 0 ) CCR = 1; else CCR = 0; 
} 
break; 
} 

7.2 Simulating a CPU 
307 
case SUB: 
{if (direction == 0) 
{ destination = DO - source; 
if (destination == 0) CCR = 1; else CCR = 0; 
} 
else 
{ DO = DO - source; 
if (DO == 0 ) CCR = 1; else CCR = 0; 
} 
break; 
} 
case BRA: 
{ if (amode = = 0 ) PC = operand; 
if (amode = = 1 ) PC = PC + operand; break; 
} 
case CMP: 
{ MBR = DO - source; 
if (MBR == 0) CCR = 1; 
else CCR = 0; break; 
} 
case BEQ: 
{if (CCR == 1) 
{ if (amode = = 0 ) PC = operand; 
if (amode = = 1 ) PC = PC + operand; 
} break; 
} 
case BNE: 
{if (CCR != 1) 
{ if (amode = = 0 ) PC = operand; 
if (amode == 1) PC = PC + operand; 
} break; 
} 
case EXG: 
{MBR = DO; DO = A0; A0 = MBR; break; 
} 
case STOP: 
{run = 0; break; 
} 
} 
/* save result in memory if register to memory */ 
if (direction == 0) 
switch (amode) 
{ case 0: { memory[operand] = destination; 
case 1: 
case 2: 
case 3: 
break; /* absolute */ 
break; /* l i t e r a l 
*/ 
memory[A0 + operand] = destination; 
break; /* indexed 
*/ 
memory[PC + operand] = destination; 
break; /* PC r e l a t i v e */ 
Now that we have examined the sequence of events 
implement the instruction. In the next two sections we 
that take place during the execution of an instruction, 
describe two different types of control unit; the micropro-
the next step is to demonstrate how the binary code 
grammed control unit and the so-called random logic control 
of an instruction is translated into the actions that 
unit. 

308 
Chapter 7 Structure of the CPU 
7.3 The random logic control unit 
We've demonstrated how you can write a program using 
assembly language instructions. We've shown how an assem-
bly language instruction can be reduced to the flow of infor-
mation between registers and functional units in a CPU. 
We've shown how register, counters, and logical units can be 
created from primitive gates and flip-flops. What's left? 
What we have not yet demonstrated is how a binary pat-
tern such as, say, 11011011101010012 can be turned into the 
sequence of operations that causes a machine-level operation 
like ADD $A9, DO to be executed. We now make good this 
omission and complete the final link in the chain between 
gate and computer. 
There are two ways of transforming an instruction into the 
operations that interpret it. One is to create the logic that 
directly transforms instructions into control signals. The 
other is to design a special computer that takes a machine-
level instruction as a program and generates control signals 
as an output. We first describe the random logic unit, which 
uses gates and flip-flops to generate control signals. 
When engineers design a random logic control unit (RALU) 
they ask 'What sequence of microinstructions is needed 
to execute each machine code instruction and what logic 
elements do we need to implement them?' In other words, 
designers resort to the Boolean techniques we described in 
Chapter 2. The word random in the expression random logic 
element is employed in a specific sense and implies that the 
arrangement of gates from which the control unit is con-
structed varies widely from computer to computer. The same 
microprogrammed control unit can readily be adapted to suit 
many different computers with relatively little modification, 
whereas the random logic control unit is dedicated to a spe-
cific CPU and cannot easily be modified. 
7.3.1 Implementing a primitive CPU 
Before designing a random logic control unit, let's look at the 
structure of a very simple CPU in order to determine what 
control signals we need to synthesize. Figure 7.8 illustrates a 
computer with a single bus that is connected to all registers 
and the memory. This arrangement reduces the amount of 
logic required to implement the processor, but it is inefficient 
because only one word at a time can be copied from a source 
to a destination. The ALU has two inputs P and Q. The P 
input comes only from data register DO and the Q input 
i 
, 
JGMSR 
s y s t e
n
m b u s 
Data out 
^ 
• [ ] 
R 
• 
R e a d 
\^L 
^-^^^ 
[• 
• ^MSR 
^ ~ ~ ~ ~ ; r - ^ 
W 
• 
Write 
.. _ 
jj 
~~~~~ 
Memory 
_ 
p Each tri-state buffer 
Address 
Data in < 
jj p u f s <-jata o n t^e ^ u s 
' L 
I when enabled 
r 
I 
I 
\ 
"-MAR 
t> 
MAR 
« 
h 
= 
H>—i 
CMBR 
ft- 
| vi 3r 
T 
jj 
/
fcMBR 
ft 
r« 
p 
i 
H ^ — 4 
When clocked / 
C,R 
¥ 
_R 
^ 
| B u s 
a register latches 
^ 
If 
| 
data from the bus 
r»Jjpc 
I 
I 
I 1? 
*l 
CPC 
„ 
pc 
life 
I 
(< 
1 
1 
h^DQ 
1 
The inputs to the ALU 
r 
•-. . 
*% 
I L - X are fro register DO 
D° 
^-• M 
a n c' t n e system bus, 
. 
. 
^ - ^ \ s ^ 
I 
^ e output °f the ALU is a 
F0 and F, select the ALU F0 
» 
P < | iT 
/ 
%• function of inputs P and Q 
function and CALU docks f 
J 
A L U r e p i , t e r 
L _ 
ALU 
-' 
^ ^ ^ 
§ 
a n d is determined by control 
the ALU output into the 
H 
! 
T " " 
< 
< A" 
X 
J signals F0 and F r The result 
ALU register 
CALU 
». | 
f(P, Q) 
Q^ 
i . - < £ A L u 
| 
is latched into the ALU 
I 
L ^ 
register by CALU 
Figure 7.8 Structure of a singte-bus CPU. 
CMBR 
» 
/ 
When clocked / 
Cm 
^ 
a register latches 
data from the bus 
CPC 
&, 
C D 0 
^ 
F0 and F, select the ALU F0 
& 
function and CALU clocks , 
the ALU output into the 
' 
ALU register 
CALU 
j , 
R 
• 
W 
• 
CMAR 
t> 
The inputs to the ALU 
^ 
are fro register DO 
and the systembus, 
The output of the ALU is a 
function of inputs P and Q 
and is determined by control 
signals F0 and F-,. The result 
is latched into the ALU 
register by C A L U 
Each tri-state buffer 
puts data on the bus 
when enabled 
DO 
PC 
i«. 
;MBR 
MAR 
ALU register 
ALU 

7.3 The random logic control unit 
309 
Signal number 
Signal 
Type 
Operation 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
R 
M e m o r y control 
W 
M e m o r y control 
CMAR 
Register clock 
CMBR 
Register clock 
CPC 
Register clock 
MR 
Register clock 
Cpo 
Register clock 
Qw 
Register clock 
EMSR 
Bus control 
EMBR 
Bus control 
E,R 
Bus control 
EPC 
Bus control 
EDO 
Bus control 
EALU 
Bus control 
Fo 
ALU control 
F, 
ALU control 
Read from memory 
Write to memory 
Clock data into MAR 
Clock data into MBR 
Clock data into program counter 
Clock data into instruction register 
Clock data into data register 
Clock data into ALU register 
Gate data from the memory onto the bus 
Gate data from the MBR onto the bus 
Gate operand from IR onto the bus 
Gate program counter onto the bus 
Gate data register onto the bus 
Gate ALU function register onto the bus 
Select ALU function bit 0 
Select ALU function bit 1 
Table 7.4 The control signals in the CPU of Fig. 7.8. 
comes only from the system bus. Note that this structure 
allows the memory to transfer data directly to or from any 
register; that is, all data does not have to pass through 
the MBR. 
The memory receives the address of the location to be 
accessed directly from the MAR, whose output is perma-
nently connected to the memory's address input. A dedicated 
connection between the MAR and memory is possible 
because the memory never receives an address input from a 
source other than the memory address register. A permanent 
connection removes the need for bus control circuits. 
Two data patiis link the memory to the system bus. In a 
read cycle when memory control input R is asserted, data is 
transferred from the memory to the system bus via tri-state 
gate GMSR. During a memory write cycle when memory con-
trol input W is asserted, data is transferred from the system 
bus directly to the memory. 
The MBR, data register, program counter, and instruction 
register are each connected to the system bus in the same way. 
When one of these registers wishes to place data on the bus, 
its tri-state gate is enabled. Conversely, data is copied into a 
register from the bus by clocking the register. The instruction 
register (IR) receives data from the memory directly, without 
the data having to pass through the MBR. 
The ALU receives data from two sources, the system bus 
and data register DO, and places its own output on the system 
bus. This arrangement begs the question, 'If the ALU gets 
data from the system bus how can it put data on the same bus 
at the same time it is receiving data from this bus?' Figure 7.8 
shows that the ALU contains an internal ALU register. When 
Fi 
Fo 
function 
0 
0 
0 
1 
1 
0 
1 
1 
add P to Q 
subtract Q 
increment Q 
decrement Q 
Table 7.5 Decoding the ALU control code, F0, Fv 
this register is clocked by C^u, the output from the ALU 
is captured and can be put on the system bus by enabling 
gate GALU. 
Table 7.4 defines the 16 control signals in Fig. 7.8. 
Instruction decoding takes an instruction and uses it to create 
a sequence of 16-bit signals that control the system in Fig. 7.8. 
The ALU is controlled by a two-bit code, F^ F0, which 
determines its functions as denned in Table 7.5. These opera-
tions are representative of real instructions, although a prac-
tical ALU would implement, typically, 16 different functions. 
In order to keep the design of a random logic control unit 
as simple as possible, we will construct a 3-bit operation code 
giving a total of eight instructions. This instruction set 
defined in Table 7.6 presents a very primitive instruction set 
indeed, but it does include the types of instruction found in 
real first-generation processors. We have defined explicit 
LOAD and STORE instructions rather than a single MOVE 
instruction which does the work of both LOAD and STORE. 
Having constructed an instruction set, we define each 
of the instructions in terms of RTL and determine the 

310 
Chapter 7 Structure of the CPU 
Op-code 
000 
001 
010 
Oil 
100 
101 
110 
111 
Mnemonic 
Operation 
LOAD 
N 
STORE N 
ADD 
N 
SUB 
N 
INC 
N 
DEC 
N 
BRA 
N 
BEQ 
N 
[DO] 
<- 
[N] 
[N] 
<- 
[DO] 
[DO] 
<- 
[DO] 
+ 
[N] 
[DO] 
<- 
[DO] 
- 
[N] 
[N] 
<- 
[N] 
+ 1 
[N] 
<- 
[N] 
- 
1 
[PC] 
<- N 
I F Z = 1 THEN 
[PC] 
<- N 
Note that N is the operand field used by the instruction. 
Table 7.6 A primitive instruction set for the CPU of Fig. 7.8. 
sequence of operations neces-
sary to carry them out on the 
computer in Fig. 7.8. Table 7.7 
gives the micro-operations for 
each instruction including the 
fetch phase. The symbol Z is 
the zero-flag bit from the 
CCR, which is assumed to be 
part of the ALU. 
Instruction Op-code Operations (RTL) 
Control actions 
F e t c h 
[MAR] 
<— [PC] 
EPC 
= 1 
CMAR 
[ I R ] 
4r-
[ [ M A R ] ] 
R 
= 1, EMSR = 1, 
C,R 
[ A L U ] 
4-
[PC] 
EPC 
= 1, F1,F0 = 1,0, 
CALU 
[PC] 
<— [ A L U ] 
EALU = 1 
CPC 
LOAD 
0 0 0 
[MAR] 
<-
[ I R ] 
E K 
= 1 
CMAR 
[DO] 
•*-
[ [MAR] ] 
R 
= 1i EMSR 
= 1, 
CDO 
STORE 
0 0 1 
[MAR] 
<— [ I R ] 
E|R 
= 1 
CMAR 
[ [ M A R ] ] <— [DO] 
EDO 
= 1 
W = 1 
ADD 
0 1 0 
[MAR] 
<-
[ I R ] 
E« 
= 1 
CMAR 
[MBR] 
<r-
[ [ M A R ] ] 
R 
= 1. EMSR = 1, 
CMBR 
[ALU] 
<-
[MBR] 
EMBR = 1, F1;F0 = 0,0, 
CALU 
[DO] 
<-
[ A L U ] 
EALU = 1 
CDO 
SUB 
0 1 1 
[MAR] 
«-
[ I R ] 
E,R 
= 1 
CMAR 
[MBR] 
<-
[ [ M A R ] ] 
R 
= 
'• EMSR = 1, 
CMBR 
[ A L U ] 
<-
[MBR] 
EMBR = 1.Fl.Fo = 0,1, 
CALU 
[DO] 
<-
[ A L U ] 
EALU = 1 
CDO 
INC 
100 
[MAR] 
<-
[ I R ] 
EIR 
= 1 
CMAR 
[MBR] 
<— [ [ M A R ] ] 
R 
= 1i EMSR = 1, 
CMBR 
[ A L U ] 
<r-
[MBR] 
EMBR = 1, F 1 lF 0 = 0,1, 
CALU 
[ [ M A R ] ] 
<r-
[ A L U ] 
EALU = 1 
W = 1 
DEC 
1 0 1 
[MAR] 
<-
[ I R ] 
EIR 
= 1 
CMAR 
[MBR] 
<— [ [ M A R ] ] 
R 
= 
'• EMSR 
= 1, 
CMBR 
[ A L U ] 
<— [MBR] 
E MBR = 1. F1#F0 = 1,1, 
CALU 
[ [ M A R ] ] <-
[ A L U ] 
EALU = 1 
W = 1 
BRA 
110 
[PC] 
<r-
[ I R ] 
E,R 
= 1 
CPC 
BEQ 
1 1 1 
I F 
Z = 1 
THEN 
EIR 
= 1 
IF Z = 1 THEN Cf,c 
[PC] <— [ I R ] 
Table 7.7 Interpreting the instruction set of Table 7.6 in RTL and microinstructions. 

7.3 The random logic control unit 
311 
Consider the load DO from memory operation; this requires 
Then we have to put the memory in read mode, put the data 
the following two steps: 
[MAR] <- [IR] 
EIR = 1 
[DO] 
<- [[MAR]] R = 1 , 
CMAR 
EMSR = 1, 
-DO 
from the memory onto the bus by enabling the GMSR gate, and 
finally capture the data in DO by clocking register DO. 
Copy operand address to MAR 
Read memory and copy to DO 
We have to send the operand address in the instruction reg- 
Table 7.7 tells us what signals have to be asserted to execute 
ister to the memory address register by enabling the GIR gate 
the two operations required to interpret LOAD N. Table 7.8 
and then clocking die data into the memory address register. 
Instruction 
Operations (RTL) 
Control action: 
Operations (RTL) 
R w 
CMAR 
CMBR 
CPC 
Cm 
CDO 
CALU 
EMSR 
EMBR 
E,R 
EPC 
EDO 
EALU 
Fi 
Fo 
F e t c h 
[MAR] 
<— PC] 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
[ I R ] 
<— 
[MAR] ] 
1 
0 
0 
0 
0 
1 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
[ALU] 
<— PC] 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
1 
0 
0 
1 
0 
[PC] 
<-
ALU] 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
LOAD 
[MAR] 
<— I R ] 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
[DO] 
i— 
[MAR] ] 
1 
0 
0 
0 
0 
0 
1 
0 
1 
0 
0 
0 
0 
0 
0 
0 
STORE 
[MAR] 
<— I R ] 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
[[MAR] <-
DO] 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
ADD 
[MAR] 
<-
I R ] 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
[MBR] 
<-
[MAR] ] 
1 
0 
0 
0 
0 
0 
0 
0 
1 
1 
0 
0 
0 
0 
0 
0 
[ALU] 
<-
MBR] 
0 
0 
0 
0 
0 
0 
0 
1 
0 
1 
0 
0 
0 
0 
0 
0 
[DO] 
<— ALU] 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
1 
0 
0 
SUB 
[MAR] 
<— I R ] 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
[MBR] 
<— 
[MAR] ] 
1 
0 
0 
0 
0 
0 
0 
0 
1 
1 
0 
0 
0 
0 
0 
0 
[ALU] 
*-
MBR] 
0 
0 
0 
0 
0 
0 
0 
1 
0 
1 
0 
0 
0 
0 
0 
1 
[DO] 
<-
ALU] 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
1 
0 
0 
INC 
[MAR] 
<— I R ] 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
[MBR] 
<-
[MAR] ] 
1 
0 
0 
0 
0 
0 
0 
0 
1 
1 
0 
0 
0 
0 
0 
0 
[ALU] 
<— MBR] 
0 
0 
0 
0 
0 
0 
0 
1 
0 
1 
0 
0 
0 
0 
1 
0 
[[MAR] «-
ALU] 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
DEC 
[MAR] 
<r-
I R ] 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
[MBR] 
4 -
[ [MAR] ] 
1 
0 
0 
0 
0 
0 
0 
0 
1 
1 
0 
0 
0 
0 
0 
0 
[ALU] 
«-
[MBR] 
0 
0 
0 
0 
0 
0 
0 
1 
0 
1 
0 
0 
0 
0 
1 
1 
[[MAR] <-
[ALU] 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
BRA 
[PC] 
*-
[ I R ] 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
BEQ 
I F Z = 1 
Tl IEN 
[PC] <r-
[ I P J 
0 
0 
0 
0 
z 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
Table 7.8 Interpreting the micro-operations of Table 7.7 as microinstructions. 

312 
Chapter 7 Structure of the CPU 
gives all the signals in the form of a 16-component vector; 
that is, the two vectors are 
0010000000100000 and 
1000000010001000 
Figure 7.9 shows the timing of the execution phase of this 
instruction. We have included only five of the 16 possible 
control signals because all the other 12 signals remain inac-
tive during these two micro-operations. 
7.3.2 From op-code to operation 
In order to execute an instruction we have to do two things. 
The first is to convert the 3-bit op-code into one of eight 
possible sequences of action and the second is to cause these 
actions to take place. 
Figure 7.10 shows how the instructions are decoded and is 
similar in operation to the 3-line to 8-line decoder described 
in Chapter 2. For each of the eight possible three-bit 
op-codes, one and only one of the eight outputs is placed in 
an active-high condition. For example, if the op-code corres-
ponding to ADD (i.e. 010) is loaded into the instruction 
register during a fetch phase, the ADD line 2 from the AND 
gate array is asserted high while all other AND gate outputs 
remain low. 
It's no good simply detecting and decoding a particular 
instruction. The control unit has to carry out the sequence of 
microinstructions that will execute the instruction. To do this 
Fetch phase 
Execute LOAD N 
Fetch phase 
Micro-operation 2 
Micro-operation 1 
Read memory, 
_ Send IR to MAR J^ latch into DO 
Clock 
Put [IR on bus 
E.R 
Enable IR to bus 
Capture [ R] in MAR 
CMAR 
Clock data into MAR 
Read operand 
R 
Read memory 
Put operand on bus 
EMSR 
Enable memory to bus 
Capture c ata in DO 
Cpo 
i 
i 
i 
Clock data i -itoDO 
we require a source of signals to trigger each of the microin-
structions. A circuit that produces a stream of trigger signals 
is called a sequencer. Figure 7.11 provides the logic diagram of 
a simplified eight-step sequencer. 
The outputs of three JK flip-flops arranged as a 3-bit 
binary up-counter counting 000, 001, 010,... ,111, are con-
nected to eight three-input AND gates to generate timing sig-
nals T0 to T7. Figure 7.12 illustrates the timing pulses created 
by this circuit. Note that the timing decoder is similar to the 
instruction decoder of Fig. 7.11. As not all macro instructions 
require the same number of microinstructions to interpret 
them, the sequencer of Fig. 7.11 has a reset input that can be 
used to reset the sequencer by returning it to state T0. 
The sequencer of Fig. 7.11 is illustrative rather than practi-
cal, because, as it stands, the circuit may generate spurious 
timing pulses at the timing pulse outputs due to the use of an 
asynchronous counter. All outputs of an asynchronous 
counter don't change state at the same instant and therefore 
the bit pattern at its output may pass through several states (if 
only for a few nanoseconds) before it settles down to its final 
value. Unfortunately, these transient states or glitches may last 
long enough to create spurious timing signals, which, in turn, 
may trigger undesired activity within the control unit. A solu-
tion to these problems is to disable the output of the timing 
pulse generator until the counter has settled down (or to use 
a synchronous counter). 
The next step in designing the control unit is to combine 
the signals from the instruction decoder with the timing sig-
nals from the sequencer to generate the actual control signals. 
Figure 7.13 shows one possible approach. 
There are nine vertical lines in the decoder 
of Fig. 7.13 (only three are shown). One 
vertical line corresponds to the fetch phase 
and each of the other eight lines is assigned to 
one of the eight instructions. At any instant 
one of the vertical lines from the instruction 
decoder (or fetch) is in a logical one state, 
enabling the column of two-input AND gates 
to which it is connected. The other inputs to 
the column of AND gates are the timing 
pulses from the sequencer. 
As the timing signals, T0 to T7, are gener-
ated, the outputs of the AND gates enabled by 
the current instruction synthesize the control 
signals required to implement the random 
logic control unit. The output of each AND 
gate corresponding to a particular microin-
struction (e.g. CMAR) triggers the actual 
microinstruction (i.e. micro-operation). As 
we pointed out earlier, not all macroinstruc-
tions require eight clock cycles to execute 
them. 
Figure 7.9 Timing of the execute phase of a LOAD N instruction. 

7.3 The random logic control unit 
313 
3 op-code bits 
Bit 2 
Bit 1 
BitO 
Operand address N 
o 
-*• 000 LOAD N, DO 
-*001 STORE D0,N 
-*-010 ADD 
N,D0 
111 BEQ 
N 
The 3-bit op-code 
is decoded into 
eight operations 
Figure 7.10 The instruction decoder. 
CLR 
Clock -
Qh 
1 -
CLR 
zn 
CLR 
J 
Q 
>C 
_ K 
Q 
J 
Q 
>c 
-i—IK 
Q 
_Ciear counter 
to state T 
o 
-*r „ " 
-+T, 
Eight timing pulses 
(Only 4 outputs 
shown for simpiicity) 
Figure 7.11 The timing pulse 
generator (sequencer). 
One machine cycle (eight clock states) 
I~L 
Figure 7.12 The outputs from the timing pulse 
generator. 
To -
T, -
T2 -
T3 -
T4 -
Ts -
T6 -
h -

3 1 4 
Chapter 7 Structure of the CPU 
Op-code 
—I 
N 
E m - 1 , Cy 
Op-code 
Rr1. E M S R-1, CD0 
|— 
STORE N 
EXECUTE 
E]R=1.C„ 
ED0=1,W=1 
Figure 7.13 Combining control 
signals. 
-*-E. 
-*-E„ 
I>^ 
- * E DO 
-+-E. 
-+C, MAR 
-*-c„ 
j 
^C|R 
•>c, PC 
-*-C n 
-+C„ 
-*F n 
-+E 
-+-R 
-+W 
- * E MSR 
Figure 7.14 The OR gate array used to generate the actual 
microinstructions. 
Each microinstruction is activated by one or more control 
signals from the nine columns of AND gates. Figure 7.14 
shows the array of OR gates that combine the outputs 
from the AND gates to generate the control signals. The inputs 
from these OR gates come from the nine columns of AND 
gates. 
The fetch-execute flip-flop 
So far we have devised a mechanism to interpret each 
macroinstruction but have not looked at how we implement 
the two-phase fetch-execute cycle. As the control unit is 
always in one of two states (fetch or execute), an RS flip-flop 
provides a convenient way of switching from one state to 
another. When Q = 0 the current operation is a fetch phase, 
and when Q = 1 an execute phase is being performed. 
Figure 7.15 is an extension of Figure 7.13 and demonstrates 
how the instruction decoder is enabled by the Q output of 
the fetch-execute flip-flop, and the fetch decoder by the Q 
output. 
At the end of each fetch phase, a clock pulse from the tim-
ing generator sets the fetch-execute flip-flop, permitting the 
current op-code to be decoded and executed. The timing-
pulse generator is reset at the end of each fetch. At the end of 
each execute phase, the fetch-execute flip-flop is cleared and 
the sequencer reset, enabling the next fetch phase to begin. 
Table 7.9 shows how the machine-level instructions can be 
represented in terms of both timing signals and microin-
structions. Note that we've included the micro-operation 
[MAR] <- [IR] in the fetch phase. 
The microinstructions are the enable signals to the bus 
drivers, the register clocks, the ALU function select bits, the 
memory controls (R and W), and the reset and set inputs of 
the fetch-execute flip-flop. For each of the microinstructions 
we can write down a Boolean expression in terms of the 
machine-level instruction and the sequence of timing pulses. 
For example, consider expressions for EMBR, EIR, and C^^. 
EMBR = ADD- T, + SUB- T, + INC- Tj + DEC- ^ 
E1R = Fetch • T4 + BRA • T0 + BEQ • T0 
CMAR = Fetch • T0 + Fetch • T4 
We should note, of course, that this CPU and its micropro-
gram are very highly simplified and illustrate the nature of 
the random logic CU rather than its exact design. 
FETCH 
—j 
N
 
E
PC=1, CMAR 
T0- 4 _ J ** 
Ti 
LJ 
- r ~ ~ V _ E P C = 1 , Fv F0=1, 0, CALU 
T2 
\_J 
'—I 
\ E A L U = 1 ' Qc 
LOAD N 
E | R - 1 , C M A R 
1 
_ ^ = 1 . E M S R = 1 , C D 0 
T 
LOAD N 
To-
T i -
h-
T 3 -
E p j - 1 , C M A R 
£=1. EMSR=1, C|R 
_Epc=1.FvFo=1.0,CAl 

7.4 Microprogrammed control units 
3 1 5 
Other. 
reset 
inputs- £>l Fetch-execute 
flip-flop 
EXECUTE 
FETCH 
Select execute 
phase 
At the end of each 
operation the last 
j 
timing pulse is used 
3 
to reset the 
feth-execute flip-flop 
T 
ft 
LOAD N 
Control 
signals to 
the OR gate 
array 
Op-code 
{from IR) 
Select fetch 
phase 
STORE N 
Select fetch 
phase 
Figure 7.15 The fetch-execute flip-flop. 
In the next section we look at how the design of a control 
unit can be simplified by putting the sequence of micro-
operations in a table and then reading them from a table, 
rather than by synthesizing them in hard logic. 
7.4 Microprogrammed 
control units 
Before we describe the microprogrammed control unit, let's 
remind ourselves of the macro-level instruction, micro-level 
instruction, and interpretation. The natural or native lan-
guage of a computer is its machine code whose mnemonic 
representation is called assembly language. Machine-level 
instructions are also called macroinstructions. Each macroin-
struction is executed by means of a number of primitive 
actions called microinstructions. The process whereby a 
macroinstruction is executed by carrying out a series of 
microinstructions is called interpretation. 
Let's begin with another simple computer. Consider 
Fig. 7.16. The internal structure of this primitive CPU differs 
slightly from that of Fig. 7.8 because there's more than one 
bus. The CPU in Fig. 7.16 includes the mechanisms by which 
information is moved within the CPU. Each of the registers 
(program counter, MAR, data register, etc.) is made up of 
D flip-flops. When the clock input to a register is pulsed, the 
data at the register's D input terminals is transferred to its 
output terminals and held constant until the register is 
clocked again. The connections between the registers are by 
means of m-bit wide data highways, which are drawn as a sin-
gle bold line. The output from each register can be gated onto 
the bus by enabling the appropriate tri-state buffer. We have 
used a multiplexer, labeled MPLX, to select the program 
counter's input from either the incrementer or the operand 
field of the instruction register. The multiplexer is controlled 
by the 1-bit signal Mux, where Mux = 0 selects the incre-
menter path, and Mux = 1 selects the branch target address 
from the address/operand field of the instruction register, 
ress* 
Suppose our computer performs a fetch-execute cycle in 
which the op-code is ADD N,DO. This instruction adds the 
contents of the memory location specified by the operand 
field N to the contents of the data register (i.e. DO) and 
deposits the result in DO. We can write down the sequence of 
operations that take place during the execution of ADD not 
only in terms of register transfer language, but also in terms 
of the enabling of gates and the clocking of flip-flops. 
Table 7.10 illustrates the sequence of microinstructions exe-
cuted during the fetch-execute cycle of an ADD instruction. It 
should be emphasized that the fetch phase of all instructions 
is identical and it is only the execute phase that varies 
according to the nature of the op-code read during the fetch 
phase. 
To 
Ti 
Tz 
To 
T, 
To 
Ti 
R 
:Q 
e 
s 

316 
Chapter 7 Structure of the CPU 
Instruction 
Time 
Memory 
Gate enables 
Register clocks 
ALU Fetch--execute 
Time 
R 
w 
MBR 
IR 
PC 
DO 
MSR 
ALU 
MAR 
MBR 
IR 
PC 
DO 
ALU 
F, Fo 
X 
R 
S 
F e t c h 
To 
0 
0 
0 
0 
1 
0 
0 
0 
1 
0 
0 
0 
0 
0 
X 
Fo 
X 
0 
0 
T, 
1 
0 
0 
0 
0 
0 
1 
0 
0 
0 
1 
0 
0 
0 
X X 
0 
0 
h 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
1 
1 0 
0 
0 
T3 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
1 
0 
0 
X X 
0 
0 
T4 
0 
0 
0 
1 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
X X 
0 
1 
LOAD 
To 
1 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
1 
0 
X X 
1 
0 
STORE 
To 
0 
1 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
X X 
1 
0 
ADD 
To 
1 
0 
0 
0 
0 
0 
1 
0 
0 
1 
0 
0 
0 
0 
X X 
0 
0 
T, 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
Tz 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
1 
0 
X X 
1 
0 
SUB 
To 
1 
0 
0 
0 
0 
0 
1 
0 
0 
1 
0 
0 
0 
0 
X X 
0 
0 
T, 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
1 
0 
0 
T2 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
1 
0 
X X 
1 
0 
INC 
To 
1 
0 
0 
0 
0 
0 
1 
0 
0 
1 
0 
0 
0 
0 
X X 
0 
0 
T, 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
1 
0 
0 
0 
T2 
0 
1 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
X X 
1 
0 
DEC 
To 
1 
0 
0 
0 
0 
0 
1 
0 
0 
1 
0 
0 
0 
0 
X X 
0 
0 
T, 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
1 
1 
0 
0 
T2 
0 
1 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
X X 
1 
0 
BRA 
To 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
X X 
1 
0 
BEQ 
To 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
z 
0 
0 
X X 
1 
0 
Table 7.9 The interpretation of machine code instructions. 
7.4.1 The microprogram 
Imagine that the output of the control unit in Fig. 7.16 con-
sists of 10 signals that enable gates G, to G10> the PC input 
multiplexer, two signals that control the memory, and five 
clock signals that pulse the clock inputs of the PC, MAR, 
MBR, IR, and DO registers. Table 7.11 presents the 17 outputs 
of the control unit as a sequence of binary values that are gen-
erated during the fetch and execute phases of an ADD instruc-
tion. We have not included the ALU function signals in this 
table. 
When the memory is accessed by E = 1, a memory read or 
write cycle may take place. The R/W (i.e. read/write) signal 
determines the nature of the memory access when E = 1. 
When R/W = 0 the cycle is a write cycle, and when R/W = 1 
the cycle is a read cycle. 
If, for each of the seven steps in Table 7.11, the 17 signals 
are fed to the various parts of the CPU in Fig. 7.16, then the 
fetch-execute cycle will be carried out. Real microprogram-
med computers might use 64 to 200 control signals rather 
than the 17 in this example. One of the most significant dif-
ferences between a microinstruction and a macroinstruction 
is that the former contains many fields and may provide 
several operands, while the macroinstruction frequendy 
specifies only an op-code and one or two operands. 
The seven steps in Table 7.11 represent a micro-
program that interprets a fetch phase followed by an ADD 
instruction. 
We have demonstrated that a macroinstruction is inter-
preted by executing a microprogram, which comprises a 
sequence of microinstructions. Each of the CPU's instruc-
tions has its own microprogram. We now look at the 

7.4 Microprogrammed control units 
3 1 7 
Clock 
PC 
PC 
Q-
D 
H 
MPLX 
'Mux 
Incrementer •*-
Branch 
path 
Clock 
PC 
. 
Control 
units 
Ml 
Control signals 
Clock 
MAR . 
R/W-
MAR 
D 
2. 
Address 
Memory 
Data 
A 
-A 
L ^ 
I feral 
path 
BusC 
out 
Clock 
MBR 
MBR 
• + T > — * 
Clock 
DO . 
DO 
j 
*<3*~C ALU 
" TTT 
< < ™ 
Fi F, F, 
" 
^ " 
Copy bus A 
to bus C 
Copy bus A 
tolR 
Bus A 
BusB 
Figure 7.16 Controlling the flow of 
information in a computer. 
Step 
Register 
[MAR] 
tran 
< -
sfer language 
1 
Register 
[MAR] 
tran 
< -
[PC] 
1a 
INC 
<-
[PC] 
2 
[PC] 
<— INC 
3 
[MBR] <— [[MAR]] 
4 
[ I R ] 
<-
[MBR] 
4a 
CU 
<-
[ IR(op-c-ode) J 
5 
[MAR] <-
[ IR(address) ] 
6 
[MBR] <-
[ [MAR] ] 
7 
ALU 
<-
[MBR] 
7a 
ALU 
<-
[DO] 
7b 
[DO] 
«- ALU 
Operations required 
enable C1( clock MAR 
Mux = 0, clock PC _ 
enable memory, R/W=1, enable C3 enable G9, clock MBR 
enable C4, clock IR 
enable G2, clock MAR 
enable memory, R/W=1, enable C3, enable G9, clock MBR 
enable C4, set ALU function to add 
enable G7 
enable G8, clock data register DO 
Note 1 Where there is no entry in the column labeled 'Operations required', that operation happens automatically. For example, the output 
of the program counter is always connected to the input of the incrementer and therefore no explicit operation is needed to move 
the contents of the PC to the incrementer. 
Note 2 Any three-state gate not explicitly mentioned is not enabled. 
Note 3 Steps 1,1a are carried out simultaneously, as are 4,4a and 7, 7a, 7b. 
Table 7.10 Interpreting a fetch-execute cycle for an A D D N , DO instruction in terms of RTL. 
Ps 
,G6 
9l 
« 
B 
E' 
G2 

318 
Chapter 7 Structure of the CPU 
Step Gate control signals and MPLX control 
Memory 
Regi 
PC 
ster clocks 
MAR 
MBR 
DO 
c. 
G2 
G3 
G4 
G5 
G6 
G7 
G8 
c9 
G10 
Mux 
E 
R/W 
Regi 
PC 
ster clocks 
MAR 
MBR 
DO 
IR 
1 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
2 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
3 
0 
0 
1 
0 
0 
0 
0 
0 
1 
0 
0 
1 
1 
0 
0 
1 
0 
0 
4 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
5 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
6 
0 
0 
1 
0 
0 
0 
0 
0 
1 
0 
0 
1 
1 
0 
0 
1 
0 
0 
7 
0 
0 
0 
1 
0 
0 
1 
1 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
Table 7.11 Control signals generated during the fetch and execution phases of an ADD instruction. 
Address mapper 
ni 
Incrementer 
Iz 
iz 
Microprogram counter 
i£ 
Clock 
Address 
Microprogram memory 
Data 
Next micro-
instruction 
address 
Load 
control 
3E 
Microinstruction register 
Condition 
select 
Branch condition 
H 
CPU control field 
>
Control signals 
to other parts 
of the CPU 
Multiplexer 
Branch on zero 
Branch on not zero 
Branch never (logical zero) 
Branch always (logical one) 
These signals 
are from the ALU 
in the CPU 
Figure 7.17 The micro-
programmed control unit. 
microprogram itself and consider the hardware required to 
execute it. The microprogram is executed by the same type of 
mechanism used to execute the macroprogram (i.e., machine 
code) itself. This is a good example of the common expres-
sion wheels within wheels. 
Figure 7.17 describes the basic structure of a micropro-
grammed control unit that has a microprogram counter, a 
microprogram memory, and a microinstruction register (this 
structure is typical of the 1980s). The microinstruction 
address from the microprogram counter is applied to the 
address input of the microprogram memory and the data 
output of the memory fed to the microinstruction register. 
As we've said, the structure of the control unit that executes 
the macroinstruction is very much like the structure of the 
CPU itself. However, there is one very big difference between 
the macroinstruction world and the microinstruction 
world—the microinstruction register is very much longer 
than the macroinstruction register and the microinstruction's 
f"^ 
This is part of the CPU 
~~ 
~^v 
\ 
Operation code 
Address 
Instruction register 
J 

7.4 Microprogrammed control units 
319 
structure is much more complex that of the macro-
instruction. 
Information in the microinstruction register is divided 
into four fields: next microinstruction address field, micro-
program counter load control field, condition select field, 
and CPU control field. Most of the bits in the microinstruc-
tion register belong to the CPU control field, which controls 
the flow of information within the CPU by enabling tri-state 
gates and clocking registers as we've described; for example, 
all the control signals in Table 7.11 belong to this field. Our 
next task is to describe one of the principal differences 
between the micro- and macroinstruction. Each microin-
struction is also a conditional branch instruction that deter-
mines the location of the next microinstruction to be 
executed. We will now explain how microinstructions are 
sequenced. 
7.4.2 Microinstruction sequence 
control 
If the microprogram counter were to step through the micro-
program memory in the natural sequence, 0,1,2,3,... etc., a 
stream of consecutive microinstructions would appear in the 
microinstruction register, causing the CPU to behave in the 
way described by Table 7.11. The CPU control bits of each 
microinstruction determine the flow of information within 
the CPU. However, just as in the case of the macroprogram 
control unit, it is often necessary to modify the sequence in 
which microinstructions are executed. For example, we 
might wish to repeat a group of microinstructions n times, or 
we may wish to jump from a fetch phase to an execute phase, 
or we may wish to call a (microinstruction) procedure. 
Microinstruction sequence control is determined by the 
three left-hand fields of the microinstruction register in 
Fig. 7.17, enabling the microprogram counter to implement 
both conditional and unconditional branches to locations 
within the microprogram memory. We shall soon see that 
this activity is necessary to execute macroinstructions such as 
BRA, BCC, BCS, BEQ, etc. 
In normal operation, the microprogram counter steps 
through microinstructions sequentially and the next micro-
program address is the current address plus one. By loading 
This field selects the 
Location of the 
This field determines where condition to be tested 
next microinstruction 
the next microinstruction 
when making a 
to execute if a branch 
address comes from 
conditional branch 
is taken 
+ 
+ 
• 
Next address field 
Load control 
Conditional select 
the contents of the next microinstruction address field of the 
current microinstruction field into the microprogram 
counter, a branch can be made to any point in the micropro-
gram memory. In other words each microinstruction deter-
mines whether the next microinstruction is taken in 
sequence or whether it is taken from the next address field of 
the current microinstruction. The obvious question to ask is, 
'What determines whether the microprogram counter con-
tinues in sequence or is loaded from the next microinstruc-
tion address field of the current microinstruction?' 
The microprogram load control field in the microinstruc-
tion register tells the microprogram counter how to get the 
next microinstruction address. This next address can come 
from the incrementer and cause the microprogram to con-
tinue in sequence. The next address can also be obtained 
from the address mapper (see below) or from the address in 
the next microinstruction address field of the microinstruc-
tion register. 
The condition select field in the microinstruction register 
implements conditional branches at the macroinstruction 
level by executing a conditional branch at the microinstruc-
tion level. In the simplified arrangement of Fig. 7.17, the con-
dition select field directly controls a 4-to-l multiplexer that 
selects one of four flag bits representing the state of the CPU. 
These flag bits are obtained from the ALU and are usually the 
flag bits in the condition code register (e.g. Z, N, C, V). The 
condition select field selects one of these flag bits for testing 
(in this example only the Z-bit is used). If the output of the 
multiplexer is true, a microprogram jump is made to the 
address specified by the contents of the next microinstruc-
tion address field, otherwise the microprogram continues 
sequentially. In Fig. 7.17 two of the conditions are obtained 
from the CCR and two bits are permanently true and false. A 
false condition implies branch never (i.e. continue) and a true 
condition implies branch always (i.e. goto). 
To emphasize what we've just said, consider the hypotheti-
cal microinstruction of Fig. 7.18. This microinstruction is 
interpreted as: 
IF Z = 1 THEN [PC] «- ADD3 ELSE [PC] <- [PC] + 1 
where PC indicates the microprogram counter. 
A conditional branch at the macroinstruction level (e.g. 
BEQ) is interpreted by microinstructions in the following 
This field provide the CPU control 
signals that select source and 
destination registers, control buses, 
and determine the ALU function 
CPU control fields 
ADD3 
Conditional 
Branch on zero 
Figure 7,18 Structure of a 
microinstruction. 

320 
Chapter 7 Structure of the CPU 
way. The condition select field of the microinstruction selects 
the appropriate status bit of the CCR to be tested. For exam-
ple, if the macroinstruction is BEQ the Z-bit is selected. The 
microprogram counter load control field contains the 
operation 'branch to the address in the microinstruction reg-
ister on selected condition true'. Thus, if the selected condi-
tion is true (i.e. Z = 1), a jump is made to a point in the 
microprogram that implements the corresponding jump in 
the macroprogram. If the selected condition is false (i.e. 
Z = 0), the current sequence of microinstructions is termi-
nated by the start of a new fetch-execute cycle. 
Implementing the fetch-execute cycle 
The first part of each microprogram executed by the control 
unit corresponds to a macroinstruction fetch phase that ends 
with the macroinstruction op-code being deposited in the 
instruction register. The op-code from the instruction regis-
ter is first fed to the address mapper, which is a look-up table 
containing the starting address of the microprogram for 
each of the possible op-codes. That is, the address mapper 
translates the arbitrary bit pattern of the op-code into the 
location of the corresponding microprogram that will exe-
cute the op-code. After this microprogram has been executed, 
an unconditional jump is made to the start of the micropro-
gram that interprets the macroinstruction execute phase, and 
the process continues. 
7.4.3 User-microprogrammed 
processors 
Before the advent of today's powerful microprocessors, engi-
neers in the 1980s requiring high performance sometimes 
constructed their own microprogrammed computers; that is, 
the engineer designed a CPU to their own specifications. This 
was fun because you could create your own architecture and 
instruction set. On the other hand, you ended up with a com-
puter without an off-the-shelf operating system, compilers, 
or any of the other tools you take for granted when you use a 
mainstream CPU. 
At the heart of many of these systems was the bit-slice com-
ponent, which provided a middle path between microcom-
puter and mainframe. Bit-slice components, as their name 
suggests, are really subsections of a microprocessor that can 
be put together to create a custom CPU. For example, a 64-bit 
computer is made by putting together eight 8-bit bit-slice 
chips. 
Bit-slice components are divided into two types corre-
sponding to the functional division within the microproces-
sor (i.e. the microprogram control and ALU). By using 
several ALU and microprogram controller bit-slices plus 
some additional logic and a microprogram in ROM, a CPU 
with a user-defined instruction set and wordlength may be 
created. Of course, the designer doesn't have to construct a 
new CPU out of bit-slice components. You can emulate an 
existing microprocessor or even add machine-level instruc-
tions to enhance it. 
Figure 7.19 describes a typical bit-slice arithmetic logic 
unit that can generate one of eight functions of two inputs R 
and S. These functions vary from R plus S to the exclusive 
NOR of R and S. The values of R and S may be selected from 
a register file of 16 general-purpose data registers, an external 
input, a Q register, or zero. 
The bit-slice ALU is controlled (i.e. programmed) by a 
9-bit input, which selects the source of the data taking part in 
an arithmetic or logical operation, determines the particular 
operation to be executed, and controls the destination 
(together with any shifting) of the result. Typical ALU 
operations are 
[R7] <- [R7] + [Rl] 
[R6] <- [R6] - 
[R5] 
[R9] i- 
[R9]-[R2] 
[R7] <- [R7] 
+ 1 
An arithmetic unit of any length (as long as it is a multiple 
of 4) is constructed by connecting together bit-slice ALUs. 
Designers can use the ALU's internal registers in any way they 
desire. For example, they may choose to implement eight 
addressable data registers, two stack pointers (described 
later), two index registers, a program counter, and three 
scratchpad registers. Flexibility is the most powerful feature 
of bit-slice microprocessors. 
This description of the microprogrammed control unit is 
highly simplified. In practice the microprogram might 
include facilities for dealing with interrupts, the memory 
system, input/output, and so on. 
One of the advantages of a microprogrammed control unit 
is that it is possible to alter the content of the microprogram 
memory (sometimes called the control store) and hence 
design your own machine-level instructions. In fact it is per-
fectly possible to choose a set of microprograms that will exe-
cute the machine code of an entirely different computer. In 
this case the computer is said to emulate another computer. 
Such a facility is useful if you are changing your old computer 
to a new one whose own machine code is incompatible with 
your old programs. Emulation applies to programs that exist 
in binary (object) form on tape or disk. By writing micropro-
grams (on the new machine) to interpret the machine code of 
the old machine, you can use the old software and still get the 
advantages of the new machine. 
One of the greatest problems in the design of a bit-slice 
computer lies in the construction and testing of the 

7.4 Microprogrammed control units 
321 
RAM0 RAM shifter 
RAM^ 
4-bit register 
selects inputs 
from control 
unit 
Carry_in 
Data input 
RAM 
16 addressable registers 
A data B data 
output output 
Logical zero 
3L 
i i 
Literal data from 
control unit or 
main store 
JE 
Q o Q shifter 
^ 
Si. 
F 
Q 
Q register 
A 
B 
D 
ALU data source selector 
iz. 
it 
8-function ALU 
Carry 
Zero 
Sign 
Overflow 
£ 
1-
Bit slice control functions: 
ALU control, source and 
destination control 
Data output to memory 
address register or 
memory data register 
Figure 7.19 The micro-
programmed ALU. 
microprogram. You can, or course, write a program to 
emulate the bit-slice processor on another computer. A pop-
ular method of developing a microprogram is to replace the 
microprogram ROM with read/write memory and to access 
this memory with a conventional microprocessor. That is, the 
microprogram memory is common to both the bit-slice 
system and the microprocessor. In this way, the microproces-
sor can input a microprogram in mnemonic form, edit it, 
assemble it, and then pass control to the bit-slice system. The 
microprocessor may even monitor the operation of the 
bit-slice system. 
Such a microprogram memory is called a writable 
control store and once a writable control store was 
regarded as a big selling point of microprogrammed 
minicomputers and mainframes. However, we have already 
pointed out that a microprogrammable control store is 
of very little practical use due to the lack of applications 
software. Even if a computer user has the expertise to design 
new microprogrammed macroinstructions, it is unlikely that 
the system software and compilers will be able to make use of 
these new instructions. Finally, RISC technology (as we shall 
see) does not use microprogramming and interest in micro-
programming is much less than it once was. 
In the next chapter we look at how the performance of 
computers can be enhanced by three very different tech-
niques. We begin with a brief introduction to the RISC revo-
lution of the 1970s and 1980s and show how processors with 
regular instruction sets lend themselves to pipelining (the 
overlapping of instruction execution). We also look at cache 
memory and explain how a small quantity of very-high-speed 
random access memory can radically improve a computer's 
performance. Finally, we describe the multiprocessor—a 
system that uses more than one processing unit to accelerate 
performance. 
- • c c 
z 
N 
A 

322 
Chapter 7 Structure of the CPU 
M SUMMARY 
H 
PROBLEMS 
We have taken a step back from the complex CPU 
architecture we described in the previous chapters and 
have looked at how a simple processor can read an instruction 
from memory, decode it, and execute it.We did this by 
considering the sequence of events that takes place when 
an instruction is executed and the flow of information 
within the computer. 
In principle, the computer is a remarkably simple device.The 
program counter contains the address of the next instruction to 
be executed. The computer reads the instruction from memory 
and decodes it. We have demonstrated that a typical instruction 
requires a second access to memory to fetch the data used by 
the instruction. 
We have demonstrated how a simple computer that 
can execute only instructions that load and store data 
or perform arithmetic operations can implement the 
conditional behavior required for loop and if... then... 
else constructs. 
The second part of this chapter looked at two ways of 
implementing a computer's control unit. We started with a 
simple computer structure and demonstrated the control signals 
required to implement several machine-level instructions.Then 
we showed how you can use relatively simple logic and a timing 
sequencer to generate the signals required to interpret an 
instruction. 
Random logic control units are faster than their micro-
programmed counterparts. This must always be so because the 
random logic control unit is optimized for its particular 
application. Moreover, a microprogrammed control unit is 
slowed by the need to read a microinstruction from the 
microprogram memory. Memory accesses are generally 
slower than basic Boolean operations. 
Microprogramming offers a flexible design. As the micro-
program lives in read-only memory, it can easily be modified at 
either the design or the production stage. A random logic 
control unit is strictly special purpose and cannot readily be 
modified to incorporate new features in the processor 
(e.g. additional machine-level instructions), and sometimes it 
is difficult to remove design errors without considerable 
modification of the hardware. 
The highpoint of microprogramming was the early 1970s 
when main memory had an access time of 1-2 /xs and the 
control store used to hold microprograms had an access 
time of 50-100 ns. It was then sensible to design complex 
machine level instructions that were executed very rapidly as 
microcode.Today, things have changed and memories with 
access times of below 50 ns are the norm rather than the 
exception. Faster memory makes microprogramming less 
attractive because hard-wired random logic control units 
execute instructions much more rapidly than microcoded 
control units.Today's generation of RISC (reduced instruction 
set computers) and post-RISC architectures are not 
microprogrammed. 
7.1 Within a CPU, what is the difference between an address 
path and a data path? 
7.2 In the context of a machine-level instruction, what is an 
operand? 
7.3 What is a literal operand? 
7.4 How does a computer 'know' whether an operand in its 
instruction register is a literal or a reference to memory (i.e. an 
address)? 
7.5 Why is the program counter a pointer and not a 
counter? 
7.6 Explain the function of the following registers in a CPU: 
(a) PC 
(b) MAR 
(c) MBR 
(d) IR 
7.7 What is the CCR? 
7.8 Does a computer need data registers? 
7.9 Some microprocessors have one general-purpose data 
register, some two, some eight, and so on. What do you think 
determines the number of such general-purpose data registers 
in any given computer? 
7.10 What is the significance of the fetch-execute cycle? 
7.11 What is the so-called von Neumann bottleneck? 
7.12 Design a computer (at the register and bus level) to 
implement a zero address instruction set architecture. 
7.13 In the context of CPU design, what is a random logic 
control unit? What is the meaning of the word random in this 
expression? 
7.14 What is a microprogrammed control unit? 
7.15 Microprogramming has now fallen into disfavor. Why do 
you think this is so? 
7.16 For the computer structure of Fig. 7.20, state the 
sequence of micro-operations necessary to carry out the 
following instruction. Assume that the current instruction is in 
the IR. 
ADDsquare DO, Dl 
This instruction reads the contents of register DO, squares that 
value, and then adds it to the contents of register DLThe result 

7.4 Microprogrammed control units 
323 
Figure 7.20 A microprogrammed CPU. 
is put in register Dl.The function codes F2,F-,, and F0 are given 
below. 
Operation 
Copy P to F 
Add P to Q 
Subtract Q from P 
Add 1 to P 
Add 1 to Q 
Multiply P by Q 
F = P 
F = P + Q 
F = P-Q 
F = P + 1 
F = Q + 1 
F = Qx1 
7.17 For the structure of Fig. 7.20 write a microprogram to 
implement the operation 
D 1 = [ A ] + [ B ] + [ C ] + 1 
Assume that only one operand, A, is required by the instruction 
and that operands B and C are in the next consecutive two 
memory locations, respectively. 
7.18 For the architecture of the hypothetical two-bus 
computer of Fig. 7.20, derive a microprogram to carry out the 
operation 
MOVE D O , [ D l ] 
This operation copies the contents of register DO into the 
memory location whose address is given by the contents of 
register Dl. 
You should describe the actions that occur in plain English 
(e.g.'Put data from this register on the B bus') and as a sequence 
of events (e.g. Read = 1, EM5R).The table in Question 16 defines 
the effect of the ALU's function code. All data has to pass 
through the ALU to get from bus B to bus A. 
Note that the ALU has two input latches. Data has to be 
loaded into these latches before an ALU operation takes place. 
Abus 
GMSR 
B bus 
Data out 
1!>— 
• 
Read 
» 
Uf 
Write 
• 
" a l n s t o r e 
Address 
D a t a in 4 
'-MAR 
• 
MAR 
This computer has 
" 
' 
two internal buses 
- 
, 
f 
J^MAR 
A and B. 
MBR 
* 
L i 5 
*• All registers capture 
^ 
M B R 
' EMBR 
data from a bus when 
\ 
I 
they are clocked. 
ciR 
H 
I 
L ^ * ^ 
* 
A t l tri-state gates can 
IR 
y E 
put data on a bus when 
* 
IR 
they are enabled. 
C p c 
»| 
1 
r ^ > P C 
» 
T h e f u n c t i o n o f t h e A L U 
p r 
1^1 
is controlled by inputs 
•] 
| 
' E p c 
FaFLFo. 
I 
1 
hv^DO 
This memory is controlled 
CDO 
• 
\s£ 
by a read signal and a 
. 
D 0 
T E M 
Write signal. 
^ _ ^ _ _ _ ^ _ ^ ^ ^ 
hspDi 
T^ e A L U performs an 
CD1 
^ 
1 ^ 
* operation on one or both 
D1 
' E m 
'ts P and Q inputs 
* 
depending on the state 
*--B_to k^> 
° f i t s control inputs. 
V J 
~ 
Data iputs for the ALU 
EB t 
A 
I 
L1 
come from two registers, 
~°~ 
• 
latch 1 and latch 2. 
ALU 
r 
p 4— 
Latch 1 
4 
L A L U ^ 
*-y}- 
f(p' Q) 
^ Z Z Z Z : 
EALU 
Function 
Q 4— 
Latch 2 
4 
| 
select 
 
F2, 
F„ 
F0 
Fz 
F, 
F0 
0 
0 
0 
0 
0 
1 
0 
1 
0 
0 
1 
1 
1 
0 
0 
1 
0 
1 
Data out 
MAR 
MBR 
IR 
PC 
DO 
D1 
cL2 
H 
F„ F0 

324 
Chapter 7 Structure of the CPU 
7.19 For the computer of Fig. 7.20, what is the effect of the 
following sequence of micro-operations? 
E D 0 
= 1, 
cL1 
E D 0 
= 1, 
cL2 
F 2,F 1 ; ,F0 
= 0, -0, , 1 , 
E A L U ' 
(-MBR 
EMBR 
= 1, 
cL1 
E D 0 
= 1, 
cL2 
F 2,F 1, F0 
= 0, 0, . 1 , 
E A L U ' 
CDI 
Your answer should explain what each of the micro-
operations does individually. You should also state what 
these actions achieve collectively; that is, what is the effect 
of the equivalent assembly language operation? 

Accelerating performance 
INTRODUCTION 
We want faster computers. In this chapter we examine three very different ways in which we can 
take the conventional von Neumann machine described in the last chapter and increase its 
performance with little or no change in the underlying architecture or its implementation.1 
The development of the computer comprises three threads: computer architecture, computer 
organization, and peripheral technology. Advances in each of these threads have contributed to 
increasing the processing power of computers over the years. The least progress has been made in 
computer architecture and the programming model of a modern microprocessor would probably 
not seem too strange to someone who worked with computers in the 1950s. They, would, however, 
be astonished by developments in internal organization such as pipelining and instruction-level 
parallelism. Similarly, someone from the 1940s would be utterly amazed by the development of 
peripherals such as disk and optical storage. In 1940 people were struggling to store hundreds or 
thousands of bits, whereas some home computers now have storage capacities of about 241 bits. 
We look at the way in which three particular techniques have been applied to computer design 
to improve throughput. We begin with pipelining, a technique that increases performance by 
overlapping the execution of instructions. Pipelining is the electronic equivalent of Henry Ford's 
production line where multiple units work on a stream of instructions as they flow through a 
processor. We then look at the way in which the apparent speed of memory has been improved 
by cache memory, which keeps a copy of frequently used data in a small, fast memory. Finally, 
we provide a short introduction to multiprocessing where a problem can be subdivided into 
several parts and run on an array of computers. 
Before discussing how we speed up computers, we need to introduce the notion of computer 
performance.We need to be able to measure how fast a computer is if we are to quantify the 
effect of enhancements. 
1 Although we introduce some of the factors that have made computers 
so much faster, we can't cover the advances in semiconductor physics and 
manufacturing technology that have increased the speed of processors, 
improved the density of electronic devices, and reduced the power con-
sumption per transistor. These topics belong to the realm of electronic 
engineering. 
RAFTER MAP 
6 Assembly language 
'•: programming 
-'•Assembly language programming 
'•• is concerned with the way in 
-which individual maehine-ievel 
:.- instructions are used to construct 
'.^entire program's.We introduce 
•' the programming environment 
.?'..-v:a a simulator that urns on a PC ' 
!•": and demonstrate how to 
:•..' implement basic-algorithms. 
7 Structure of the CPU: 
Chapter 7 demonstrates how a' 
computer is organized internally 
| and how it reads instructions 
from memory, decodes them, 
| anawecutesthem.WeiooScat 
j the fetch-execute cycle and 
' demonstrate how both random 
logk arid microprogrammed 
I centre! units operate • ' 
8 Accelerating 
Performance 
The p'tvioiis chapif-r describe"; 
ho.v d computer woiks in 
pnnc'ple; nO.V Vn" 0«1pi:ri; « rp<= 
i>: ;iu> >clin;3ues us.-d to mjiki-
tin'; emputer opcrj'c faster. WP 
begin !'v discussing how we rsn 
nea-siii1 comtiuu-i ns-.iarma.-Ki" 
one. \\—- Ctur bo riui', the 
perfoi iTi.in'.e ot computers has 
boon enhanced by pipelining. th>' 
use of c?( he mono- y. jnd 
pawUi-1. r.rnrc-is"'i»-
9 Processor families 
We have used the 68K to 
introduce computer architecture 
because of its elegance and 
simplicity. However, students 
should appreciate that there arc 
many computer architectures 
available. Here we introduce two 
alternatives to the 68K. We leak 
at a simple 3-bit microcontroller 
found in consumer products and 
a high-performance RISC 
processor. 

326 
Chapter 8 Accelerating performance 
8.1 Measuring performance 
How do we measure performance? The answer is quite 
simple—'With difficulty'. A computer's performance is a 
measure of its throughput2 or the time to execute a program. 
However, because performance depends on the computer's 
operating system, disk drives, memory, cache memory, bus 
structure, architecture, internal processor organization, and 
clock rate, it is very difficult to compare two computer 
systems. When you do compare systems, just what are you 
actually comparing? 
There are many ways of measuring a computer's perfor-
mance. A technique widely used in the computer industry 
is the benchmark. A benchmark is a figure-of-merit, which is 
usually the time taken to execute a set of programs. These 
benchmark programs are chosen to mimic the type of work 
that will be performed by the computer. A common bench-
mark is called SPEC and was devised by a vender-indepen-
dent organization that serves the computer industry. A 
computer's SPEC benchmark is calculated by running a 
series of different tasks (i.e. programs) on the computer 
under test and then dividing the time each task takes by the 
time the same program takes on a reference machine. These 
figures constitute a set of normalized execution times. The 
geometric mean3 of the individual normalized times is then 
taken to provide a single benchmark for the computer: its 
SPEC-mark. The SPEC benchmark tests the entire system 
including CPU, cache, memory, buses, and system software. 
Suppose a machine executes programs A, B, and C in 45, 20, 
and 60 s, respectively. If, say, the times for these targets on the 
reference machine are 60, 25, and 50 s, the normalized times 
are 0.75, 0.80, and 1.10, respectively, corresponding to a 
benchmark of 0.87. 
Although the speed of a machine is dependent on all its 
components including the hardware, we are first going to 
look at the CPU and neglect the contribution to performance 
made by memory, I/O, and software. Considering only the 
CPU, we can say that the time taken to execute a program is 
given by the expression 
^execute 
= N-M X 1/Winst X CPI X T 
^executeis the t i m e taken to execute a program, Ninst, is the num-
ber of instructions in the program, Winst is the work carried out 
per instruction, CPI is the average number of clock cycles per 
instruction, and T^ is the clock period. Each of these terms 
plays a role in die equation and each of them is determined by 
a different factor in the computer design process. 
Ninst tells us how many instructions we need to implement 
die program. The size of a program is dependent on both the 
architecture of the processor and the algorithm used to solve 
the problem. This term is also dependent on die efficiency 
of the compiler because some compilers generate more 
compact code than others. The Nimt term is determined by 
the programmer and compiler writer. 
The Winst term4 tells us how much computation an instruc-
tion performs and is a function of the CPU's architecture. 
A simple architecture has a low value of Wjnst, because many 
instructions are required to perform a certain action. A com-
plex architecture has a high value of Winst because individual 
instructions perform quite sophisticated operations. Consider 
the 68020's BFFFO (bit field find first one) instruction, which 
scans an arbitrary sequence of 1 to 32 bits and returns the 
location of the first bit in the string that is set to 1; for example, 
the instruction BFFFO (A0){63:23},D0 scans the string of 
23 bits pointed at by address register A0. This 23-bit-wide 
string starts at 63 bits from the most-significant bit of the 
byte pointed at by A0. The position of the first bit in this string 
set to 1 (plus the string offset 63) is loaded into register DO. 
Without this instruction you'd need a handful of primitive 
machine-level instructions to implement it. The value of a 
processor's W^, term is determined by the computer architect. 
The CPI (cycles per instruction) term depends on the 
internal organization of the computer and expresses how 
many clock cycles are needed to execute an instruction. 
First- and second-generation processors had large CPI 
values. Modern RISC processors are much better with CPIs 
approaching the ideal value, 1. Some processors, called super-
scalars, have multiple processing units and execute instruc-
tions in parallel; these have CPIs less than unity. This term is 
determined by the chip designer. 
All digital operations take an integer number of clock 
cycles. The T ^ term expresses the processor's clock speed, 
which is determined both by device physics and the internal 
organization of the chip. 
We derived the expression Texecute = Ninst X 1/Winst X 
CPI X Tcyc to demonstrate that the speed of a CPU is deter-
mined by the combined efforts of the programmer, compiler 
writer, computer architect, chip designer, and semiconductor 
physicist. It is important to appreciate that a processor's clock 
speed cannot be used to compare it with a different micro-
processor; for example, you cannot directly compare an AMD 
processor with an Intel processor on the basis of clock speed 
alone because the internal organizations of these two processor 
families are radically different. 
8.1.1 Comparing computers 
Before we describe ways of speeding up computers, we need 
to explain why we have to be able to compare the speeds of 
1 The term 'throughput' implies the number of programs that can be 
executed in a given time. 
3 The geometric mean of n numbers is the n-th root of their product. 
4 This is a made-up term. It would be very difficult to create a parameter 
that defines the work done by an instruction. I've included it here to 
demonstrate that some instructions are more powerful than others. 
^execute 
= N-M X 1/Winst X CPI X T 
T 
x execute 
N 
i/winst 
CPI 
T 
••eye 

8.2 The RISC revolution 
327 
different computers. Broadly speaking, there are three types 
of computer user. The first is the home or small business user; 
the second is the large corporate user such as the bank, 
hospital, government agency, or university; the third is the 
specialist such as the aircraft designer, weather forecaster, or 
nuclear physicist. Members of each of these groups have to 
select the computers they use; a process that requires an 
understanding of performance. 
The domestic or small business user falls into one of two 
classes: the casual user and the sophisticated user. The casual 
user is someone who knows little of computing and who is 
very much at the mercy of advertising and personal advice. 
Casual users lack the training to understand computer litera-
ture and may well select a computer on spurious advertising 
claims. Fortunately, the increasing performance of comput-
ers means that almost anything they buy will be satisfactory 
for most purposes. The sophisticated user will look at reviews 
published in popular computing magazines and read the 
detailed specifications of any computer they are thinking of 
buying. Reviews in the popular computing press may use 
suites of programs such as computer games, the type of pro-
grams that many users will run. 
The corporate buyer of computers is in a different 
situation. They may be buying thousands of computers for an 
organization where it's possible to set down criteria by which 
the competing computers can be judged. For example, sup-
pose two computers are contenders for a large contract but 
computer A uses an AMD processor and computer B uses an 
Intel processor. These processors have, essentially, the same 
architectures but radically different internal organizations. 
How do you decide which to buy if, say, professor A costs $50 
more that processor B and A's manufacture claims it has a 
better performance? 
The situation with high-end personal computers and 
workstations is more complex. Here, the machine is being 
used to perform massive amounts of computing and it is very 
important to specify the best machine. Performance becomes 
a critical issue. 
We begin our discussion of accelerating computer perfor-
mance with the notion of pipelining. Before we do that, we 
need to say a little about the RISC revolution, which spear-
headed the drive toward performance. 
RISC—REDUCED OR REGULAR? 
What does the R in RISC stand for? The accepted definition of 
RISC is reduced instruction set computer. First-generation 
experimental RISC processors were much simpler devices than 
existing CISC processors like the Intel 8086 family or the 
Motorola 68K family.These RISCs had very simple instruction 
set architectures with limited addressing modes and no 
complex special-purpose instructions. 
8.2 The RISC revolution 
Microprocessor manufacturers looked anew at processor 
architectures in the 1980s and started designing simpler, 
faster machines. Some designers turned their backs on the 
conventional complex instruction set computer (CISC) and 
started producing reduced instruction set computers (RISCs). 
By the mid-1990s some of these RISC processors were con-
siderably more complex than the CISCs they replaced. This 
isn't a paradox. The RISC processor is not really a cut-down 
computer architecture—it represents a new approach to 
architecture design. In fact, the distinction between CISC and 
RISC is now so blurred that all modern processors incorpo-
rate RISC features even if they are officially CISCs. 
From the introduction of the microprocessor in the mid-
1970s to the mid 1980s there was an almost unbroken trend 
towards more and more complex architectures. Some micro-
processor architectures developed like a snowball rolling 
downhill; each advance in the chip fabrication process 
allowed designers to add more to the microprocessor's cen-
tral core. Intel's 8086 family illustrates this trend particularly 
well, because Intel took their original 16-bit processor and 
added more features in each successive generation. This 
approach to chip design leads to cumbersome architectures 
and inefficient instruction sets, but it has the tremendous 
commercial advantage that the end users don't have to buy 
new software when they use the latest reincarnation of a 
microprocessor. Intel's 8086 appeared in the 1970s and yet 
the Pentium 4 that powers many of today's PCs is a direct 
descendent of the 8086. 
Although processors were advancing in terms architec-
tural sophistication in the late 1970s, a high price was being 
paid for this progress in terms of efficiency. Complex instruc-
tions required complex decoders and a lot of circuitry to 
implement. There was no guarantee that these instructions 
would be used in actual programs. An instruction such as 
ADD Rl, R2 is relatively easy to decode and interpret. You 
simply clock Rl and R2 on to buses to the ALU, select the ALU 
function for addition, and then clock the output from the 
ALU into Rl. Couldn't be simpler. 
Consider the implementation of the 68K instruction MOVE 
(12, A2, DO), ( A l ) + . Although this instruction copies 
However, as time passed, RISC instruction sets grew in 
complexity; by the time the PowerPC was introduced, it had 
more variations on the branching instruction than some CISCs 
had instructions. However, RISC processors are still character-
ized by the regularity of their instruction sets; there are very 
few variations in the format of instructions. 

328 
Chapter 8 Accelerating performance 
data from A to B, it is not easy to implement. The source 
operand 
is 
in 
the 
memory 
location 
given 
by 
12 + [A2] + [DO]. The processor has to extract the constant 
12 and the register identifiers A2 and DO from the op-code. 
Two registers have to be read and their values added to the lit-
eral 12 to get the address used to access memory (i.e. there is 
a memory access cycle to get the source operand). The value 
at this location is stored at the destination address pointed at 
by address register Al. Getting the destination address 
requires more instruction decoding and the reading of regis-
ter Al. Finally, the destination operand uses autoincrement-
ing, so the contents of register Al have to be incremented by 
2 and restored to A1. All this requires a large amount of work. 
A reaction against the trend toward greater complexity 
began at IBM with their 801 architecture and continued at 
Berkeley where David Patterson and Divid Ditzel coined the 
term RISC to describe a new class of architectures that 
reversed earlier trends in microcomputer design. RISC archi-
tectures redeploy to better effect some of the silicon real 
estate used to implement complex instructions and elaborate 
addressing modes in conventional microprocessors of the 
68K and 8086 generation. 
Those who designed first-generation 8-bit architectures in 
the 1970s were striving to put a computer on a chip, rather 
than to design an optimum computing engine. The designers 
of 16-bit machines added sophisticated addressing modes 
and new instructions and provided more general-purpose 
registers. The designers of RISC architectures have taken the 
design process back to fundamentals by studying what many 
computers actually do and by starting from a blank sheet (as 
opposed to modifying an existing chip a la Intel). 
Two factors that influenced the architecture of first- and 
second-generation microprocessors were microprogram-
ming and the complex instruction sets created to help pro-
grammers. By complex instructions we mean operations like 
MOVE 12 (A3, DO) ,D2 and ADD (A6) + ,D3. 
Microprogramming achieved its highpoint in the 1970s 
when ferrite core memory had a long access time of 1 p-s or 
more and semiconductor high-speed random access mem-
ory was very expensive. Quite naturally, computer designers 
used the slow main store to hold the complex instructions 
that made up the machine-level program. These machine-
level instructions are interpreted by microcode in the much 
faster microprogram control store within the CPU. Today, 
main stores use semiconductor memory with an access time 
of 40 ns or less and cache memory with access times below 
Instruction group 
1
2 
3 
Mean value 
45.28 
28.73 
10.75 
5 ns. Most of the advantages of microprogramming have 
evaporated. The goal of RISC architectures is to execute an 
instruction in a single machine cycle. A corollary of this state-
ment is that complex instructions cannot be executed by pure 
RISC architectures. Before we look at RISC architectures 
themselves, we provide an overview of the research that led to 
the hunt for better architectures. 
8.2.1 Instruction usage 
Computer scientists carried out extensive research over a 
decade or more in the late 1970s into the way in which com-
puters execute programs. Their studies demonstrated that 
the relative frequency with which different classes of instruc-
tions are executed is not uniform and that some types of 
instruction are executed far more frequently than others. 
Fairclough divided machine-level instructions into eight groups 
according to type and compiled the statistics described by 
Table 8.1. The mean value represents the results averaged over 
both program types and computer architecture. 
The eight instruction groups are 
• data movement 
• program modification (i.e. branch, call, return) 
• arithmetic 
• compare 
• logical 
• shift 
• bit manipulation 
• input/output and miscellaneous. 
This data demonstrates that the most common instruction 
type is the data movement primitive of the form p = Q in a 
high-level language or MOVE Q, P in a low-level language. 
The program modification group which includes conditional 
and unconditional branches together with subroutine calls 
and returns, is the second most common group of instruc-
tions. The data movement and program modification groups 
account for 74% of all instructions. A large program may 
contain only 26% of instructions that are not data movement 
or program modification primitives. These results apply to 
measurements taken in the 1970s and those measurements 
were the driving force behind computer architecture devel-
opment; more modern results demonstrate similar trends as 
Table 8.2 shows. 
4 
5 
6 
7 
8 
5.92 
3.91 
2.93 
2.05 
0.44 
Table 8.1 Frequency of instruction usage (very old data). 

8.2 The RISC revolution 
329 
Benchmark 
Branch 
22.9% 
Integer 
Load 
Store 
O08.espresso 
Branch 
22.9% 
46.8% 
21.1% 
5.3% 
022.li 
20.7% 
34.2% 
25.6% 
15.5% 
023.eqntott 
27.9% 
42.8% 
27.0% 
0.9% 
O26.compress 
19.5% 
53.8% 
17.6% 
9.0% 
072.sc 
23.5% 
40.1% 
20.1% 
10.7% 
085.gcc 
21.1% 
42.0% 
21.7% 
11.1% 
average 
22.1% 
39.7% 
23.8% 
10.7% 
Note: The PowerPC is a RISC machine with a load/store architecture. All data 
processing operations act on internal registers. The only memory accesses are 
via loads and stores. 
Table 8.2 Instruction usage figures for the PowerPC processor.5 
An inescapable inference from such results is that proces-
sor designers might be better employed devoting their time to 
optimizing the way in which machines handle instructions in 
groups one and two, than in seeking new powerful instruc-
tions that are seldom used. In the early days of the micro-
processor, chip manufacturers went out of their way to 
provide special instructions that were unique to their prod-
ucts. These instructions were then heavily promoted by the 
company's sales force. Today, we can see that their efforts 
should have been directed towards the goal of optimizing the 
most frequently used instructions. RISC architectures have 
been designed to exploit the programming environment in 
which most instructions are data movement or program 
control instructions. 
Constants, parameters, and local storage 
Another aspect of computer architecture that was investi-
gated was the optimum size of literal operands (i.e. con-
stants). Tanenbaum reported the remarkable result that 56% 
of all constant values lie in the range —15 to +15 and that 
98% of all constant values lie in the range —511 to +511. 
Consequently, the inclusion of a 5-bit constant field in an 
instruction would cover over half the occurrences of a literal. 
RISC architectures have sufficiently long instruction lengths 
to include a literal field as part of the instruction that caters 
for the majority of literals. 
Programs use subroutines heavily and an effective archi-
tecture should optimize the way in which subroutines are 
called, parameters are passed to and from subroutines, and 
workspace provided for local variables created by sub-
routines. Research showed that in 95% of cases 12 words of 
storage are sufficient for parameter passing and local storage; 
that is, an architecture with 12 words of on-chip register stor-
age should be able to handle all the operands required by most 
subroutines without accessing main store. Such an arrangement 
reduces the processor-memory bus traffic associated with 
subroutine calls. 
8.2.2 Characteristics of 
RISC architectures 
We begin by summarizing the characteristics of a classic RISC 
architecture of the 1980s. These characteristics don't define 
the RISC architecture; they are general attributes of proces-
sors that were called RISC. 
1. RISC processors have sufficient on-chip memory in the 
form of registers to overcome the worst effects of the 
processor-memory bottleneck. On-chip memory can be 
accessed more rapidly than off-chip main store. 
2. RISC processors have three-address, register-to-register 
architectures. Instructions are of the form OPERATION 
Ra, Rb, Re, where Ra, Rb, and Re are general-purpose reg-
isters. 
3. Because subroutine calls are so frequently executed, RISC 
architectures facilitate the passing of parameters between 
subroutines. 
4. Instructions that modify the flow of control (e.g. branch 
instructions) are implemented efficiently because they 
comprise about 20 to 30% of a typical program. 
5. RISC processors don't attempt to implement infrequendy 
used instructions. Complex instructions waste space on a 
chip. Moreover, the inclusion of complex instructions 
increases the time taken to design, fabricate, and test a 
processor. 
6. RISC processors aim to execute on average one instruc-
tion per clock cycle. This goal imposes a limit on the max-
imum complexity of instructions. 
7. A corollary of point 6 is that an efficient architecture 
should not be microprogrammed, as microprogramming 
interprets an instruction by executing microinstructions. 
In the limit, a RISC processor is close to a micropro-
grammed architecture in which the distinction between 
machine cycle and microcycle has vanished. 
8. An efficient processor should have a single instruction for-
mat. A typical CISC processor has variable-length instruc-
tions (e.g. from 2 to 10 bytes). By providing a single 
instruction format, the decoding of an instruction into its 
component fields can be performed by a minimum level 
of decoding logic. A RISC's instruction length should be 
sufficient to accommodate the operation code field and 
one or more operand fields. Consequently, a RISC proces-
sor may not utilize memory space as efficiently as a con-
ventional CISC microprocessor. 
5 From IBM's Power PC compiler writer's guide Appendix C. 

330 
Chapter 8 Accelerating performance 
We now look at two of the fundamental aspects of the 
RISC architecture—its register set and pipelining. Multiple 
overlapping register windows have been implemented to 
reduce the need to transfer parameters between subroutines. 
Pipelining is a mechanism that permits the overlapping of 
instruction execution (i.e. internal operations are carried out 
in parallel). Note that many of the features of RISC pro-
cessors are not new. They have been employed long before the 
advent of the microprocessor. The RISC revolution happened 
when all these performance-enhancing techniques were 
brought together and applied to microprocessor design. 
The Berkeley RISC, SPARC, and MIPS 
Although the CISC processors came from the large semicon-
ductor manufacturers, one of the first RISC processors came 
from the University of California at Berkeley.6 The Berkeley 
RISC was not a commercial machine, but it had a tremen-
dous impact on the development of other RISC architectures. 
Figure 8.1 describes the format of a Berkeley RISC instruc-
tion. Each of the 5-bit operand fields permits one of 32 inter-
nal registers to be accessed. 
The Sec field determines whether the condition code bits 
are updated after the execution of an instruction; if Sec = 1, 
the condition code bits are updated after an instruction. The 
source 2 field uses an IM (immediate mode) bit to select one 
of two functions. When IM = 0, bits 5 to 12 are zeros and bits 
0 to 4 provide the second source operand register. When 
IM = 1, the second source operand is a literal and bits 0 to 12 
provide a 13-bit constant (i.e. immediate value). 
Because five bits are allocated to each operand field, it fol-
lows that this RISC has 25 = 32 internal registers. This last 
statement is emphatically not true, because the Berkeley 
RISC has 138 user-accessible general-purpose internal regis-
ters. The reason for the discrepancy between the number of 
registers directly addressable and the actual number of regis-
ters is due to a mechanism called windowing, which gives the 
programmer a view of only a subset of all registers at any 
instant. 
The Berkeley RISC and several other RISC processors 
hardwire register R0 to zero. Although this loses a register 
because you can't change the contents of R0, it gains a con-
stant. By specifying register R0 in an instruction, you force 
the value zero; for example, ADD RI, RI, R2 implements MOVE 
RI,R2. 
The experimental Berkeley led to the development of 
the commercial SPARC processor (Scalable Processor 
ARChitecture) by Sun Microsystems. SPARC is an open 
architecture and is also manufactured by Fujitsu. Similarly, a 
RISC project at Stanford led to the design of another classic 
RISC machine, the MIPS. Figure 8.2 illustrates the format of 
the MIPS instruction, which has three basic formats, a regis-
ter-to-register format for all data processing instructions, an 
immediate format for either data processing instructions 
with a literal or load/store instructions with an offset, and a 
branch/jump instruction with a 26-bit literal that is con-
catenated with the six most-significant bits of the program 
counter to create a 32-bit address. 
Register windows 
An important feature of the Berkeley RISC architecture is the 
way in which it allocates new registers to subroutines; that is, 
when you call a subroutine, you get some new registers. 
Suppose you could create 12 registers out of thin air each 
time you call a subroutine. Each subroutine would have its 
own workspace for temporary variables, thereby avoiding 
relatively slow accesses to main store. Although only 12 or so 
registers are required by each invocation of a subroutine, the 
successive nesting of subroutines rapidly increases the total 
number of on-chip registers assigned to subroutines. You 
might think that any attempt to dedicate a set of registers to 
each new procedure is impractical, because the repeated call-
ing of nested subroutines will require an unlimited amount 
of storage. 
Although subroutines can be nested to any depth, research 
has demonstrated that on average subroutines are not nested 
to any great depth over short periods. Consequently, it is 
32 bits 
31 
25 24 23 
19 18 
14 13 12 
5 4 
0 
Op-code 
Sec 
Destination 
Source 1 
0 
0 0 0 0 0 0 0 0 
s4s3s2slSo 
Op-code 
Sec 
Destination 
Source 1 
1 'u'n'io'g'aV'e's 
U '3 >2 H 'o 
IM 
Source 2 
4 
* 
< 
^ 4 
*..« 
* 4 
* 4 
^ 4 
p. 
7 bits 
1 bit 
5 bits 
5 bits 
9 bits 
5 bits 
Figure 8.1 Format of the Berkeley 
RISC instruction. 
6 It would be unfair to imply that RISC technology came entirely from 
academia. As early at 1974 John Cocke was working on RISC-like archi-
tectures at IBM's Thomas J. Watson Research Center. The project was 
called '801' after then number of the building in which the researchers 
worked. Cocke's work led to IBM's RISC System/6000 and the PowerPC. 

8.2 The RISC revolution 
3 3 1 
R-type instruction 
(register to register) 
l-type instruction 
(register with 
immediate operand) 
J-type instruction 
(jump to target) 
32 bits 
31 
26 25 
21 20 
16 15 
1110 
6 5 
0 
Op-code 
Source S 
Source T 
Destination 
Shift amount 
Function 
6 bits 
5 bits 
5 bits 
5 bits 
5 bits 
6 bits 
Op-code 
Source S 
Source T 
Immediate value 
Op-code 
Target 
Figure 8.2 Format of the MIPS instruction. 
Nesting 
depth 
During this period the depth of nested 
function calls is not great. 
Call 
Return 
Time 
During this period 
the depth of nesting 
does not vary widely 
Figure 8.3 Depth of subroutine nesting as 
a function of time. 
feasible to adopt a modest number of local register sets for a 
sequence of nested subroutines. 
Figure 8.3 provides a graphical representation of the exe-
cution of a program in terms of the depth of nesting of sub-
routines as a function of time. The trace goes up each time a 
subroutine is called and down each time a return is made. 
Even though subroutines may be nested to considerable 
depths, there are long runs of subroutine call that do not 
require a nesting level of greater than about five. 
An ingenious mechanism for implementing local variable 
work space for subroutines was adopted by the Berkeley 
RISC. Up to eight nested subroutines could be handled using 
on-chip work space for each subroutine. Any further nesting 
forces the CPU to dump registers to main memory. Before we 
demonstrate the Berkeley RISC's windowing mechanism, we 
describe how the memory used by subroutines can be divided 
into four types. 
Global space is directly accessible by all subroutines that hold 
constants and data that may be required from any point 
within the program. Most conventional microprocessors 
have only global registers. 
Local space is private to the subroutine. That is, no other sub-
routine can access the current subroutine's local address 
space 
from 
outside 
the 
subroutine. 
Local 
space 
is employed as temporary working space by the current 
subroutine. 
Imported parameter space holds the parameters imported by 
the current subroutine from its parent. In RISC terminology 
these are called the high registers. 
Exported parameter space holds the parameters exported by 
the current subroutine to its child. In RISC terminology these 
are called the low registers. 
Consider the following fragment of C code. Don't worry if 
you aren't a C programmer—the fine details don't matter. 
What we are going to do is to demonstrate the way in which 
memory is allocated to parameters. The main program 
creates three variables x, y, and z. Copies of x and y are passed 
Depth of nesting 
1 
• 

332 
Chapter 8 Accelerating performance 
to the function (i.e. subroutine) calc. The result is returned 
to the main program and assigned to z. Figure 8.4 illustrates a 
possible memory structure for the program. 
Parameters x, y, and z are local to function main, and 
copies of x and y are sent to function calc as imported para-
meters. We will assume that copies of these parameters are 
placed on the stack before calc is called. The value returned 
by function calc is an exported parameter, and sum and 
di f f are local variables in calc. 
Windows and parameter passing 
One reason for the high frequency of data movement opera-
tions is the need to pass parameters to subroutines and to 
receive them from subroutines. The Berkeley RISC architec-
ture improves parameter passing by means of multiple over-
lapped windows. A window is die set of registers visible to the 
current subroutine. Figure 8.5 illustrates the structure of the 
RISC's overlapping windows. 
void main (void) 
( 
int x = 204, y = 25, z; 
z = calc (x, y) 
/* let's calculate (x + y) / (x - y) */ 
int calc (int a, int b) 
{ 
int sum, diff; 
sum = a + b; 
diff = a - b; 
return (sum/diff) 
} 
/* this function calculates (a + b)/(a - b) */ 
/* calculate a + b */ 
/* calculate a - b */ 
. 
• -
Stack pointer 
diff 
Stack pointer 
diff 
sum 
Frame pointer 
old frame pointer 
Frame pointer 
old frame pointer 
return address 
y 
X 
y 
X 
, 
Function calc with local workspace 
Parameters passed on the stack 
Function main 
Figure 8.4 Parameter space. 
Register name Register type 
RO to R9 
The global register set is always accessible 
RIO to R15 
Six registers used by the subroutine to receive 
parameters from its parent and to pass results 
back to its parent 
R16 to R2 5 
10 local registers accessed only by the current 
subroutine that cannot be accessed directly by any 
other subroutine 
R26 to R31 
Six registers used by the subroutine to pass 
parameters to and from its own child (i.e., a 
subroutine called by itself). 
Table 8.3 Berkeley RISC register types. 
Suppose that the processor is currently 
using the ith window set. A special-pur-
pose register, called the window pointer 
(WP), indicates the current active win-
dow. In this case the WP contains the 
value i. Each window is divided into four 
parts as described by Table 8.3. 
All windows consist of 32 addressable 
registers, RO to R31. A Berkeley RISC 
instruction of the form ADD R3,R12,R25 
implements [R25] <- [R3] 
+ [R12], 
where R3 is within the window's global 
address space, R12 is within its import from 
(or export to) parent subroutine space, and 
R25 is within its local address space. RISC 
arithmetic and logical instructions always 
involve 32-bit values (there are no 8-bit or 
16-bit operations). 
Whenever a subroutine is invoked by an instruction of the 
form CALLR Rd, address, the contents of the window 
pointer are incremented by 1 and the current value of the 
program counter saved in register Rd of the new window. The 
Berkeley RISC does not employ a conventional stack in exter-
nal main memory to save subroutine return addresses. 
Once a new window has been invoked (in Fig. 8.3 this is 
window i), the new subroutine sees a different set of registers 
to the previous window. Global registers RO to R9 are an 
exception since they are common to all windows. Window 
RIO of the child subroutine corresponds to (i.e. is the same 
as) window R26 of the calling (i.e. parent) subroutine. The 
Memory 

8.2 The RISC revolution 
3 3 3 
Window /-1 
0 
RO 
9 
R9 
Window / 
Window /+1 
RO 
R9 
RO 
R9 
Global registers can be 
accessed oy any subroutine 
R10;_, 
R15:_, 
R16;_T 
Local registers R16 to R25 
^_^__~——•—" a r e unique to each window 
~~ 
and can t be accessed from 
other windows 
Local • -
Local registers R16 to R25 
^_^__~——•—" a r e unique to each window 
~~ 
and can t be accessed from 
other windows 
R25 M 
Window /-1 
R26;_-| 
R10; 
R26;_-| 
R10; 
R31;_T 
R15; 
R31;_T 
R15; 
R16; 
Local 
R25; 
Window /+1 
R26, 
R10,+l 
R26, 
R10,+l 
R31; 
R15,+1 
R31; 
R15,+1 
R16,tl 
Local 
R25/+1 
R26/+1 
mM 
Registers R26 in window / 
is the same as register R10 
in window H-1. the group of 
6 registers overlap 
Figure 8.5 Berkeley windowed register sets. 
total number of registers required to implement the Berkeley 
windowed register set are 
10 global + 8 X 1 0 local 
+ 8 X 6 parameter transfer registers = 138 registers 
Although windowed register sets are a good idea, there are 
flaws and only one commercial processor implements win-
dowing, the SPARC. The major problem with windows is that 
the number of registers is finite. If there are more nested calls 
than register sets, then old register sets have to be moved from 
windowed registers to main store and later restored. When all 
windowed registers are in use, a subroutine call results in regis-
ter overflow and the system software has to intervene. Register 
sets also increase the size of a task's environment. If the operat-
ing system has to switch tasks or deal with an exception, it may 
be necessary to save a lot of program context to main store. 

334 
Chapter 8 Accelerating performance 
The Berkeley RISC instruction set 
Although the Berkeley RISC is not a commercial computer, 
we briefly look at its instruction set because it provides a tem-
plate for later RISC-like processors such as the MIPS, the 
SPARC, and the ARM. The instruction set is given below. The 
effect of most instructions is self-explanatory. As you can see, 
instructions have a three-operand instruction format and the 
only memory operations are load and store. In the absence of 
byte, word, and longword operations, this RISC includes 
several memory reference operations designed to access 
Register to register operations 
AND 
Rs,S2,Rd 
OR 
Rs,S2,Rd 
XOR 
RS,S2,Rd 
ADD 
Rs,S2,Rd 
ADDC 
Rs,S2,Rd 
SUB 
Rs,S2,Rd 
SUBC 
Rs,S2,Rd 
SUBI 
Rs,S2,Rd 
SUBCI 
Rs,S2,Rd 
SLL 
Rs,S2,Rd 
SRA 
Rs,S2,Rd 
SRL 
Rs,S2,Rd 
Load instructions 
LDXW 
(Rx)S2,Rd 
LDXHU 
(Rx)S2,Rd 
LDXHS 
(Rx)S2,Rd 
LDXBU 
(Rx)S2,Rd 
LDXBS 
(Rx)S2,Rd 
LDRW 
Y,Rd 
LDRHU 
Y,Rd 
LDRHS 
Y,Rd 
LDRBU 
Y,Rd 
LDRBS 
Y,Rd 
Store instructions 
logical 
logical 
exclusive 
add 
add with carry 
subtract Rd = Rs - S2 
subtract with borrow Rd = S2 
subtract reverse 
subtract reverse with borrow 
shift left logical 
shift right arithmetic 
shift right logical 
load long 
load short unsigned 
load short signed 
load byte unsigned 
load byte signed 
load relative long 
load relative short unsigned 
load relative short signed 
load relative byte unsigned 
load relative byte signed 
Rs 
STXW 
R m , ( R x ) S 2 
store long 
STXH 
R m , ( R x ) S 2 
store short 
STXB 
R m , ( R x ) S 2 
store byte 
STRW 
Rm,Y 
store relative long 
STRH 
Rm,Y 
store relative short 
STRB 
Rm,Y 
store relative byte 
Control transfer instructions 
JMPX 
C O N D , ( R x ) S 2 
conditional jump 
JMPR 
COND,Y 
conditional relative jump 
CALLX 
R d , ( R x ) S 2 
call and change window 
CALLR 
R d , Y 
call relative and change window 
RET 
C O N D , ( R x ) S 2 
return and change window 
CALLI 
Rd 
call an interrupt 
RETI 
C O N D , ( R x ) S 2 
return from interrupt 
Miscellaneous instructions 
LDHI 
Rd,Y 
GETLPC 
Rd 
GETPSW 
Rd 
PUTPSW 
Rm 
load immediate high 
load PC into register 
load PSW into register 
put contents of register Rm in PSW 
This is a very simple 
instruction processing set of 
operations covering logical, 
arithmetic, and shift 
operations. 
Load instructions use address 
relative addressing. There are 
32-bit, 16-bit, and 8-bit 
versions. 
Store instructions use 
address register indirect 
addressing and are available 
for 8-, 16-, and 32-bit stores. 
These are the subroutine call 
and return instructions. 
Note that program counter 
relative adddressing is 
supported. 
These perform special 
register accesses to the PC 
and processor status register. 

8.3 RISC architecture and pipelining 
335 
8-, 16-, and 32-bit values (bytes, half words, and words, 
respectively). 
Load 
and 
store 
instructions 
use 
register indirect 
addressing with a constant and a pointer; for example, LDXW 
(Rx)S2,Rd loads destination register Rd with the 32-bit 
value at the address pointed at by register Rx plus offset S2. 
The value of the second source operand S2 is either a register 
or a literal. Because register RO is always zero, we can write 
LDXW (R0)S2,R3 to generateLDXW S2,R3. 
8.3 RISC architecture and pipelining 
Historically, the two key attributes of RISC architectures are 
their uniform instruction sets and the use of pipelining to 
increase throughput by overlapping instruction execution. 
We now look at pipelining. 
Figure 8.6 illustrates the machine cycle of a hypothetical 
microprocessor executing an ADD Rl, R2 , R3 instruction. 
Imagine that this instruction is executed in the following five 
phases. 
Instruction fetch Read the instruction from the system 
memory and increment the program counter. 
Instruction decode Decode the instruction read from mem-
ory during the previous phase. The nature of the instruction 
decode phase is dependent on the complexity of the instruc-
tion encoding. A regularly encoded instruction might be 
decoded in a few nanoseconds with two levels of gating 
whereas a complex instruction format might require ROM-
based look-up tables to implement the decoding. 
Operand fetch The operand specified by the instruction is 
read from the system memory or an on-chip register and 
loaded into the CPU. In this example, we have two operands. 
Execute The operation specified by the instruction is 
carried out. 
One instruction 
-4—. 
_ 
_ 
^ 
: Instruction 
Instruction 
: Operand 
Execute 
; 
Operand 
fetch 
. decode 
: fetch 
i 
Store 
IF 
ID 
OF 
E 
OS 
IF 
ADD R l r R 2 , R 3 Read this instruction from memory 
ID 
A D D R1,R2,R3 Decode this instruction into "ADD" and registers 1,2, 3 
OF 
A D D R1,R2,R3 Read operands R2 and R3 from the register file 
E 
ADD R l , R2 , R3 Add the two operands together 
OS 
ADD R l , R2, R3 Store the result in R1 
Figure 8.6 Instruction execution. 
Operand store 
The 
result 
obtained 
during 
the 
execution phase is written into the operand destination. 
This may be an on-chip register or a location in external 
memory. 
Each of these five phases may take a specific time (although 
the time taken is an integer multiple of the system's master 
clock period). Some instructions may require phases; for 
example, the CMP Rl, R2 instruction, which compares Rl 
and R2 by subtracting Rl from R2, does not need an operand 
store phase. 
The inefficiency in the arrangement of Fig. 8.6 is clear. 
Consider the execution phase of an instruction. This takes 
one-fifth of an instruction cycle leaving the instruction exe-
cution unit idle for the remaining 80% of the time. The same 
applies to the other functional units of the processor that 
also lie idle for 80% of the time. A technique called pipelining 
can be employed to increase the effective speed of the 
processor by overlapping the various stages in the 
execution of an instruction. For example, when a pipelined 
processor is executing one instruction, it is fetching the next 
instruction. 
The way in which a RISC processor implements pipelining 
is described in Fig. 8.7. Consider the execution of two 
instructions. At time i instruction 1 begins execution with its 
instruction fetch phase. At time (+1 instruction 1 enters its 
instruction decode phase and instruction 2 begins its instruc-
tion fetch phase. This arrangement makes sense because it 
ensures that the functional units in a computer are used more 
efficiently. 
Figure 8.8 illustrates the execution of five instructions in a 
pipelined system. We use a four-stage pipeline for the rest of 
this section because RISC processors don't need an instruc-
tion decode phase because their encoding is so simple. As you 
can see, the total execution time is eight cycles. After instruc-
tion 4 has entered the pipeline, the pipeline is said to be full 
and all stages are active. 
Pipelining considerably speeds up a processor. Suppose an 
unpipelined processor has four stages and each operation 
takes 10 ns. It takes 4 X 10 ns = 40 ns to execute an instruc-
tion. If pipelining is used and a new instruction enters the 
pipeline every 10 ns, a completed instruction leaves the 
pipeline every 10 ns. That's a speed up of 400% without 
improving the underlying semiconductor technology. 
Consider the execution of n instructions in a processor 
with an m-stage pipeline. It will take m clock cycles for the 
first instruction to be competed. This leaves n — 1 instruc-
tions to be executed at a rate of one instruction per cycle. The 
total time to execute the n instructions is, therefore, 
m + (n — 1) cycles. 
If we do not use pipelining, it takes n • m cycles to 
execute n instructions, assuming that each instruction is 

3 3 6 
Chapter 8 Accelerating performance 
Time i 
Time /+1 
time -
Time ;'+1 
Time /+1 
Time /+1 
Instruction 
1 
Instruction 
fetch 
— • Instruction 
decode 
— • Operand 
fetch 
—»• Execute 
— • 
Operand 
store 
Time ;'+1 
Instruction 
2 
/ 
Instruction 1 is fetched while 
instruction 2 is being decoded. 
Instruction 
fetch,? 
— • Instruction 
decode 
— • Operand 
fetch 
Execute 
— • Operand 
store 
Figure 8.7 Pipelining and 
instruction overlap. 
Time 1 
Time 2 
Time 3 
Instruction 1 
Instruction 4 
Instruction 5 
Instruction 
fetch 
Operand 
fetch 
Instruction 
fetch 
Execute 
Operand 
fetch 
Instruction 
fetch 
Time 4 
Time 5 
Operand 
store 
Execute 
Operand 
fetch 
Instruction 
ltftch 
In timg slot 3, instruction 1 
is beirjg executed, instruction 2 
is in t i e operand fetch phase, 
and instruction 3 is being fetched 
from jiemory 
Execute 
Operand 
fetch 
Time 6 
Time 7 
Time 8 
In time slot 4, instruction jl 
is being cofnpleted, just as 
instruction! 4 is being fetched 
from rnembry. At this poirit, all 
stages are bctive and a ne\V 
instruction! is completed et'ery cycle 
Operand 
store 
Execute 
Operand 
store 
Instruction 
fetch 
Operand 
fetch 
Execute 
Operand 
store 
Figure 8.8 Pipelining and instruction overlap. 
executed in m phases. The speedup due to pipelining is, 
therefore, 
n • m 
S = m + (n - 1) 
Let's put some numbers into this equation and see what 
happens when we vary the values of n (the code size) and m 
(the number of stages in the pipeline). Table 8.4 gives the 
results for m = 3, 6, and 12 with instruction blocks ranging 
from 4 to 1000 instructions. 
Table 8.4 demonstrates that pipelining produces a speedup 
that is the same as the number of stages when the number of 
instructions in a block is large. Small blocks of instructions 
running on computers with large pipelines do not demon-
strate a dramatic performance improvement; for example, a 
12-stage pipeline with four-instruction blocks has a speedup 
ratio of 3.2 rather than 12. We will return to the implications 
of this table when we've introduced the notion of the pipeline 
hazard. 
8.3.1 Pipeline hazards 
Table 8.4 demonstrates that pipelining can provide a substan-
tial performance acceleration as long at the block of instruc-
tions being executed is much longer than the number of 
Operand 
store 
Instruction 2 j 
Instruction 3 | 

8.3 RISC architecture and pipelining 
3 3 7 
Block size 
Three-stage 
pipeline 
Six-stage 
pipeline 
12-stage 
pipeline 
4 
2.0000 
2.6667 
3.2000 
8 
2.4000 
3.6923 
5.0526 
20 
2.7272 
4.8000 
7.7419 
100 
2.9411 
5.7143 
10.810 
1000 
2.9940 
5.9701 
11.8694 
QC 
3.0000 
6.0000 
12.0000 
Table 8.4 Pipelining efficiency as a function of pipeline length 
and block size. 
stages in the pipeline. Unfortunately, machine code is divided 
into blocks with breaks between blocks that have come to be 
known as hazards (hazards possibly a misnomer) or pipeline 
stalls. We are interested in two types of hazard, the bubble 
created by branch instructions and the data dependency 
caused by certain combinations of instructions. 
A pipeline is an ordered structure that thrives on regularity. 
At any stage in the execution of a program, a pipeline contains 
components of two or more instructions at varying stages in 
their execution. Consider Fig. 8.9 in which a sequence of 
instructions is being executed in a pipelined processor. The 
second instruction, i", enters the pipeline and begins execution. 
Let's assume that this is a simple unconditional branch 
instruction, BRA N. The branch instruction enters the pipeline 
during its fetch phase. In the next clock cycle, any operands 
required by the branch instruction are fetched and the next 
instruction in sequence is dragged into die pipeline. In the 
next phase the branch instruction is executed and 
another instruction brought into the pipeline. 
However, when the branch instruction is exe-
cuted, the program counter is reloaded with the 
branch target address, in this case N. 
Because the program counter is loaded with a new value 
when the processor encounters a branch instruction, any 
ADD Rl R2,R3 
JMPXN 
ADD R2 R4,R5 
ADD R7 R8,R9 
instructions loaded into the pipeline immediately following 
the branch are not executed. All the work performed by the 
pipeline on these instructions must be thrown away, because 
the instructions are not executed. When data in a pipeline is 
rejected or the pipeline is held up by the introduction of idle 
states, we say that a bubble has been introduced. Of course, 
the longer the pipeline the more instructions that must be 
rejected once the branch is encountered. 
Because program control instructions are so frequent, any 
realistic processor using pipelining must do something to 
overcome the problem of bubbles caused by this class of 
instructions. The Berkeley RISC reduces the effect of bubbles 
by refusing to throw away the instruction immediately fol-
lowing a branch; that is, the instruction immediately after a 
branch is always executed. Consider the effect of the follow-
ing sequence of instructions: 
[R3] <- [Rl] + [R2] 
[PC] <- [N] 
Goto address N 
[R5] <- [R2] + [R4] This is executed 
Not executed because of branch taken 
The processor calculates R5 = R2 + R4 before executing 
the branch. This sequence of instructions is most strange to 
, 
Four-stage pipeline 
^ 
|F 
= i n s t r u c t i o n 
f e t c h 
, 
, 
1 
, 
OF = operand fetch 
i'-1 
IF 
OF 
OE 
OS 
E 
= operand execute 
OS = operand store 
(T)| „ IF 
OF 
| 
OE 
| 
OS 
A 
i + 1 \ 
IF 
OF 
OE 
OS 
/ 
\ 
\ 
„ . . . 
~l 
These two instructions 
/ 
\ 
\ 
Bubble 
| 
are not executed 
/ 
\ 
l\/+2| 
IF 
OF 
| OE J OS 1 
Instruction i is 
X^^^ 
" ' 
BRA N and the next ^ H — — _ 
^/Z5\\ 
Z- 
I 
^7 
I 
~Z~r 1 TZ 
instruction should be 
'Vy 
IF 
| 
OF 
| 
OE 
| 
OS 
fetched from address 
' 
N rather than i + l . 
I 
1 
1 
1 
1 
Figure 8.9 The pipeline 
N +11 
IF I OT I OE I os I 
bubble. 
Figure 8.9 The pipeline 
bubble. 

3 3 8 
Chapter 8 Accelerating performance 
Fetch 1-1 
OF ] 
•! OE | 
• [ OS 
Fetch i 
OF 
OE 
OS 
This instruction is executed out of sequence 
Fetch i+1r 
This instruction 
is not executed 
This instruction 
is at the branch -
target address 
OF :M OE 
•\ Fetch i+2 
OF 
OS 
OE 
OS 
^-»|Fetch;N | 
*\ 
OF 
| — • ] 
OE 
| — » | OS 
Figure 8.10 Delayed branch. 
Time 1 
Time 2 
Time 3 
Time 4 
Time 5 
Time 6 
7 
Time 7 
Operand R5 
atthispoint 
Time 8 
is saved only 
Time 9 
Instruction 1 
IF 
OF 
E 
OS 
Time 5 
Time 6 
7 
Time 7 
Operand R5 
atthispoint 
Time 8 
is saved only 
ADD R1,R2,R3 
Get R2.R3 
Add R2.R3 
R1 = R2+R3 
Time 5 
Time 6 
7 
Time 7 
Operand R5 
atthispoint 
Time 8 
is saved only 
Time 5 
Time 6 
7 
Time 7 
Operand R5 
atthispoint 
Time 8 
is saved only 
IF 
OF 
E 
OS 
Time 6 
7 
Time 7 
Operand R5 
atthispoint 
Time 8 
is saved only 
ADD R5,R2,R4 
Get R2.R4 
Add R2.R4 
R5=R2 + R4 
Time 6 
7 
Time 7 
Operand R5 
atthispoint 
Time 8 
is saved only 
> 
Time 6 
7 
Time 7 
Operand R5 
atthispoint 
Time 8 
is saved only 
IF 
Bubble 
/ 
OF 
E 
OS 
Instruction 3 ; 
SUB R6.R7.R5 
Bubble 
/ 
Get R7.R5 
Sub R7-R5 
R6=R7-R5 
Instruct on 3 has to wa t for the 
save R5 
Instruct on 3 has to wa t for the 
save R5 
IF 
OF 
E 
OS 
Instruction 4 ! 
Instruct on 3 has to wa t for the 
save R5 
AND R2.R2.R3 
Get R3.R4 
AND R3.R4 
R2= R3.R4 
previous instruction to 
t for the 
save R5 
Figure 8.11 Data dependency. 
the eyes of a conventional assembly language programmer, 
who is not accustomed to seeing an instruction executed after 
a branch has been taken. 
Unfortunately, it's not always possible to arrange a pro-
gram in such a way as to include a useful instruction immedi-
ately after a branch. Whenever this happens, the compiler 
must introduce a no operation (NOP) instruction after the 
branch and accept the inevitability of a bubble. This mecha-
nism is called a delayed jump or a branch-and-execute 
Figure 8.10 is a computed branch whose target address is cal-
culated during the execute phase of the instruction cycle. 
8.3.2 Data dependency 
Another problem caused by pipelining is data dependency in 
which an instruction cannot be executed because it requires a 
result from a previous operation that has not yet left the 
pipeline. Consider the following sequence of operations. 
ADD R l , R 2 , R 3 
[Rl] <- [R2] + [R3] 
ADD R5,R2,R4 
[R5] 
<r-
[R2] + [R4] 
SUB R6,R7,R5 
[R6] *-
[R7] - 
[R5] 
AND R2,R3,R4 
[R2] <-
[R3] 
. 
[R4] 
Note: R5 may not have been computed 
technique. Figure 8.10 demonstrates how a RISC processor 
These instructions are executed sequentially. However, a 
implements a delayed jump. The branch described in 
problem arises when die third instruction, SUB R 6 , R7, R5, 
Instruction 2 j 

8.3 RISC architecture and pipelining 
339 
Time 1 
Time 2 
Time 3 
Instruction 1 
Instruction 2 
Instruction 3 
Instruction 4 
Time 4 
Time 5 
Time 6 
Time 7 
Fetch 1 
OF 
fc 
OE 
OS 
In this case, R5 is 
Fetch 1 
OF 
OE 
OS 
In this case, R5 is 
ADDR1.R2.R3 
Fetch 2 
OF 
OE 
^ 
OS 
execution unit where it 
15 needed in the next cycle 
Fetch 2 
OF 
OE 
^ 
OS 
ADD R5,R2,R 4 
(internal forwarding 
ADD R5,R2,R 
Fetch 3 
OF 
k " * 
OE 
OS 
ADD R5,R2,R 
Fetch 3 
OF 
" * 
OE 
OS 
ADD R5,R2,R 
ADD R6.R7.R 
ADD R5,R2,R 
Fetch 4 
^ 
OF 
^ 
OE 
OS 
ADD R5,R2,R 
Fetch 4 
OF 
OE 
OS 
ADD R2,R1,R4 . 
Although this operation uses 
register R1 as a source operand that 
^was generated by instruction 1, there 
is no need for infernal forwarding 
because R1 was stored in time slot 4 
Figure 8.12 Internal forwarding. 
is executed on a pipelined machine. This instruction uses R5, 
which is calculated by the preceding instruction, as a source 
operand. Clearly, the value of R5 will not have been stored by 
the previous instruction by the time it is required by the 
current instruction. Figure 8.11 demonstrates how data 
dependency occurs. 
Figure 8.11 demonstrates that the pipeline is held up or 
stalled after the fetch phase of instruction 3 for two clock 
cycles. It is not until the end of time slot 5 that operand R5 is 
ready and execution can continue. Consequently a bubble 
must be introduced in the pipeline while an instruction waits 
for its data generated by the previous instruction. 
Figure 8.12 demonstrates a technique called internal for-
warding designed to overcome the effects of data dependency. 
The example provided corresponds to a three-stage pipeline 
like the RISC. The following sequence of operations is to be 
executed. 
1. 
ADD R1,R2,R3 
[Rl] <- [R2] + 
[R3] 
2. 
ADD R5,R2,R4 
[R5] <-
[R2] + [R4] 
3. 
SUB R6,R7,R5 
[R6] <-
[R7] - 
[R5] 
4. 
ADD R2,R1,R4 
[R2] 
<r-
[Rl] 
+ [R4] 
Instruction 2 generates a destination operand R5 that is 
required as a source operand by the next instruction. If the 
processor were to read the source operand requested by 
instruction 3 directly from the register file, it would see the 
old value of R5. By means of internal forwarding the proces-
sor transfers R5 from instruction 2's execution unit directly 
to the execution unit of instruction 3 (see Fig. 8.12). 
In this example, instruction 4 uses an operand generated 
by an instruction 1 (i.e. the contents of register Rl). However, 
because of the intervening instructions 2 and 3, the destina-
tion operand generated by instruction 1 has time to be 
written into the register file before it is read as a source 
operand by instruction 4. 
8.3.3 Reducing the branch penalty 
If we're going to reduce the effect of branches on the perfor-
mance of RISC processors, we need to determine the effect of 
branch instructions on the performance of the system. 
Because we cannot know how many branches a given pro-
gram will contain, or how likely each branch is to be taken, we 
have to construct a probabilistic model for the system. We will 
make the following assumptions. 
1. Each non-branch instruction is executed in one cycle. 
2. The probability that a given instruction is a branch ispb. 
3. The probability that a branch instruction will be 
taken is pt. 
4. If a branch is taken, the additional penalty is b cycles. 
5. If a branch is not taken, there is no penalty. 
The average number of cycles executed during the execu-
tion of a program is the sum of the cycles taken for non-
branch instructions, plus the cycles taken by branch 
instructions that are taken, plus the cycles taken by branch 
instructions that are not taken. 
If the probability of an instruction being a branch is pb, the 
probability that an instruction is not a branch is 1—pb 
because the two probabilities must add up to 1. Similarly, if pt 
is the probability that a branch will be taken, the probability 
that a branch will not be taken is 1 —pt. 
The total cost (i.e. time) of an instruction is 
Tme = (1 - pb)-l + pb-p,-(l +b)+ 
j y ( l - p,)-l 
= 1 + pb-pt-fe. 

340 
Chapter 8 Accelerating performance 
The expression, 1 + pb-pt-b, tells us that the number of 
branch instructions, the probability that a branch is taken, 
and the overhead per branch instruction all contribute to the 
branch penalty. We are now going to examine some of the 
ways in which pt,-px-b can be reduced. 
Branch prediction 
If we can predict the outcome of the branch instruction 
before it is executed, we can start filling the pipeline with 
instructions from the branch target address if the branch is 
going to be taken. For example, if the instruction is BRA N, the 
processor can start fetching instructions at locations N, 
N + 1, N + 2 etc., as soon as the branch instruction is 
fetched from memory. In this way, the pipeline is always filled 
with useful instructions. 
This prediction mechanism works well with an uncondi-
tional branch like BRA N. Unfortunately, conditional branches 
pose a problem. Consider a conditional branch of the form 
BCC N (branch to N on carry bit clear). Should the RISC 
processor make the assumption that the branch will not be 
taken and fetch instructions in sequence, or should it make 
the assumption that the branch will be taken and fetch 
instruction at the branch target address N? 
Conditional branches are required to implement various 
types of high-level language construct. Consider the follow-
ing fragment of high-level language code. 
IF 
(J < K) I = I + L; 
(FOR T = 1; T <= I ; 
T++) 
{ 
} 
The first conditional operation compares J with K. Only the 
nature of the problem will tell us whether J is often less than K. 
The second conditional in this fragment of code is pro-
vided by the FOR construct, which tests a counter at the end 
of the FOR and then decides whether to jump back to the 
body of the construct or to terminate the loop. In this case, 
you could bet that the loop is more likely to be repeated than 
exited. Some loops are executed thousands of times before 
they are exited. Therefore, it might be a shrewd move to look 
at the type of conditional branch and then either fill the 
pipeline from the branch target if you think that the branch 
will be taken, or fill the pipeline from the instruction after the 
branch if you think that it will not be taken. 
If we attempt to predict the behavior of a system with two 
outcomes (branch taken or branch not taken), there are four 
possibilities. 
1. Predict branch taken and branch taken—successful outcome. 
2. Predict branch taken and branch not taken—unsuccess-
ful outcome. 
3. Predict branch not taken and branch not taken—success-
ful outcome. 
4. Predict branch not taken and branch taken—unsuccess-
ful outcome. 
Suppose we apply a branch penalty to each of four these 
possible outcomes. The penalty is the number of cycles 
taken by that particular outcome, as Table 8.5 demonstrates. 
For example, if we think that a branch will not be taken and 
get instructions following the branch and the branch is actu-
ally taken (forcing the pipeline to be loaded with instructions 
at the target address), the branch penalty in Table 8.5 is c 
cycles. 
We now need to calculate the average penalty for a partic-
ular system. To do this we need more information about the 
system. The first thing we need to know is the probability that 
an instruction will be a branch (as opposed to any other cat-
egory of instruction). Assume that the probability that an 
instruction is a branch is pb. The next thing we need to know 
is the probability that the branch instruction will be taken, pt. 
Finally, we need to know the accuracy of the prediction. Letpc 
be the probability that a branch prediction is correct. These 
values can be obtained by observing the performance of real 
programs. Figure 8.13 illustrates all the possible outcomes of 
an instruction. We can immediately write 
(1 — pb) = probability that an instruction is not a branch 
(1 — pt) = probability that a branch will not be taken 
(1 — pc) = probability that a prediction is incorrect 
These equations are obtained by using the principle that if 
one event or another must take place, their probabilities must 
add up to unity. The average branch penalty per branch 
instruction, Cave, is therefore 
Cave 
.pred i cted_takcn_and_taken / 
+ 
^(Pbranch. _predicted_taken__bul_not_taken/ 
^Vrbranch_predicted_not_taken_but_taken/ 
_predicted_not_taken_and__not_taken/ 
C« = a-(p,-pc) + MP, - D-(i -p c) 
+ c-pt-(l -p c) + d-(l - pd'Pc 
Prediction 
Result 
Branch penalty 
Branch taken 
Branch taken 
a 
Branch taken 
Branch not taken 
b 
Branch not taken 
Branch taken 
c 
Branch not taken 
Branch not taken 
d 
Table 8.5 The branch penalty. 
Cave ~ 
a (/"branch, .pred i cted_takcn_and_taken / 
_predicted_taken__bul_not_taken/ 
^Vrbranch_predicted_not_taken_but_taken/ 
_predicted_not_taken_and_not_taken/ 
C« = «-(p,-pc) + *•(?, - D-(l - P C ) 
+ c •/>,•(! -/>J + <*•(! -/>,)-pc 

8.3 RISC architecture and pipelining 
341 
Figure 8.13 Branch prediction. 
The average number of cycles taken by a branch 
instruction is CZV!.-ph 
• M«-(Pt-pc) + 
&'Pt-0 ~Pc) 
+ c-(l -p,)-(l -pc) 
+ <*•(! 
~pt)-pc). 
We can make two assumptions to help us to simplify the 
first expression. The first is that a = d = N (i.e. if the predic-
tion is correct the number of cycles is N). The other simplifi-
cation is that b = c — B (i.e. if the prediction is wrong the 
number of cycles is B). The average number of cycles per 
branch instruction is therefore 
static prediction of branches. This bit is set 
or cleared by the compiler depending on 
whether the compiler estimates that the 
branch is most likely to be taken. This 
technique provides branch prediction 
accuracy in the range 74 to 94%. 
Dynamic branch prediction techniques 
operate at run-time and use the past 
behavior of the program to predict its 
future behavior. Suppose the processor 
maintains a table of branch instructions. 
This branch table contains information 
about the likely behavior of each branch. 
Each time a branch is executed, its out-
come (i.e. taken or not taken) is used to update the entry in 
the table. The processor uses the table to determine whether 
to take the next instruction from the branch target address 
(i.e. branch predicted taken) or from the next address in 
sequence (branch predicted not taken). 
Single-bit branch predictors provide an accuracy of over 
80% and 5-bit predictors provide an accuracy up to 98%. 
A typical branch prediction algorithm uses the last two 
outcomes of a branch to predict its future. If the last two out-
comes are X, the next branch is assumed to lead to outcome 
X. If the prediction is wrong it remains the same the next time 
the branch is executed (i.e. two failures are needed to modify 
the prediction). After two consecutive failures, the prediction 
is inverted and the other outcome assumed. This algorithm 
responds to trends and is not affected by the occasional single 
different outcome. 
Pb-(N-p,-pc + B-P.-0 -p c) + B-(1 -p t)-(l ~pc) 
+ N-(l ~p,)-pc) = pb-(N-pc + 
B-(l-pc)). 
There are several ways of implementing branch prediction 
(i.e. increasing the value of pc). Two basic approaches are 
static branch prediction and dynamic branch prediction. Static 
branch prediction makes the assumption that branches are 
always taken or never taken. Because observations of real 
code have demonstrated that branches have a greater than 
50% chance of being taken, the best static branch prediction 
mechanism would be to fetch the next instruction from the 
branch target address as soon as the branch instruction is 
detected. 
A better method of predicting the outcome of a branch is 
by observing its op-code, because some branch instructions 
are taken more or less frequently than other branch instruc-
tions. Using the branch op-code to predict that the branch 
will or will not be taken results in a 75% accuracy. An exten-
sion of this technique is to devote a bit of the op-code to the 
8.3.4 Implementing pipelining 
We demonstrated how the fetch-execute cycle operated at the 
logic level in Chapter 7. Now we show how the basic model is 
extended to incorporate pipelining. 
In principle, pipelining is straightforward. Information is 
passed though a system (e.g. a logic unit or a memory) and 
then captured in a latch at the output of the system on the 
next clock cycle. Once the information has been captured in a 
latch, it can be held constant and used by the next processing 
stage. 
Consider the highly simplified pipelined processor of 
Fig. 8.14 (we have omitted all but the basic detail—there is no 
data memory or facilities for dealing with literal operations 
and conditional behavior). 
We are going to look at the operation of Fig. 8.14 cycle by 
cycle using the timing diagram of Fig. 8.15. All data is latched 
on the rising edge of a clock signal. Assume that the first cycle 
is timeslot T0 and that the program counter contains the 
address of instruction i at the start of T0. 
Instruction 
not branch 
® 
1 — P b ^ 
„ 
, 
^rMk 
Predict taken 
J > ^ 
Branch 
^ ^ * m 
b r a n c h t a k e n 
, - ^ 
taken 
^^-p 
^ ~ \ . 
Branch 
Px^^ 
V"""""---^ 
c 
p ^ - v i n s t r u c t i o n ^ ^ ^ 
' ~Pc ^~~-*-» 
Predict not taken 
^ ^ v l 
^
^ 
branch taken 
^ v - 
»ML 
Predict not taken 
1 - p 1 " ^ - 
^ — - " - ~ w 
branch n o t taken 
Branch not~~^~^_^ 
5 
taken 
1 - / V ^ ~ ^ - * ^ 
Predict taken 
branch not taken 

342 
Chapter 8 Accelerating performance 
ACCESSING EXTERNAL MEMORY 
CISC processors have a wealth of addressing modes that can 
be used with memory reference instructions. The 68K 
implements A D D DO , - (A5) which adds the contents of DO 
to the top of the stack pointed at by A5 and then pushes the 
result on to this stack. Designers of the Berkeley RISC 
restricted the way in which it accesses external memory. 
The Berkeley RISC permits only two types of reference to 
external memory: a load and a store. Similarly, it provides a 
limited number of addressing modes with which to access an 
operand. It's not hard to find the reason for these restrictions 
on external memory accesses—an external memory reference 
takes longer than an internal operation. 
Consider a LOAD (Rx)S2, Rd which implements 
[Rd]«- [ [Rx] + S2].The diagram demonstrates a 
possible sequence of actions performed during the 
execution of this instruction. In the source operand 
fetch phase, Rx is read from the register file to calculate 
the effective address of the operand.The processor can't 
progress beyond the execute phase to the store operand 
phase, because the operand has not been read from the main 
store. A bubble is introduced in the pipeline until the operand 
has been read from memory. Because memory accesses 
introduce bubbles into the pipeline, they are avoided wherever 
possible. 
The effective address in the program counter relative mode 
is given by 
EA 
[PC] + S2 
where PC represents the contents of the program counter and 
S2 is an offset as above. 
These addressing modes provide zero, one, or two pointers, 
and a constant offset. If you wonder how we can use an 
addressing mode without a pointer register, recall that RO 
contains the constant zero.There is a difference between 
addressing modes permitted by load and store operations. A 
load instruction permits the second source, S2, to be either an 
immediate value or a second register, whereas a store instruc-
tion permits S2 to be a 13-bit immediate value only. 
The Berkeley RISC instruction has two formats. The short 
immediate format provides a 5-bit destination, a 5-bit source 
1 operand and a 14-bit short source 2 operand. The short 
immediate format has two variations: one that specifies 
a 13-bit literal for source 2 and one that specifies a 5-bit 
source 2 register address. Bit 13 is used to specify whether the 
source 2 operand is a 13-bit literal of a 5-bit register pointer. 
The long immediate format provides a 19-bit source 
operand by concatenating the two source operand fields. 
Fetch. r| 
»j 
OF 
| 
H OE | 
H OS 
Calculate operand 
address 
JFetch /+1|—•) 
OF 
)—>j 
OE 
Load instruction 
|Fetch/+2[—•[ 
OF 
Memory access 
OS 
-H 
OE 
The Berkeley RISC implements two addressing 
modes: indexed and program counter relative. All other 
addressing modes must be synthesized from these two 
primitives. The effective address in the indexed mode is 
given by 
EA = [PC] + S2 
1. Absolute addressing 
2. Register indirect 
3. Indexed addressing 
4. Two-dimensional addressing 
EA= 13-bit offset: 
EA = [Rx]; 
EA = [Rx] + Offset 
EA = [Rx] + [Ry] 
where Rx is the index register (one of the 32 
general purpose registers) and S2 is an offset. The 
offset can be either a general-purpose register or a 13-bit 
constant. 
Thirteen-bit and 19-bit immediate fields may sound a little 
strange at first sight. However, because 13 + 19 = 32, RISC 
permits a full 32-bit value to be loaded into a window register 
in two operations. A typical microprocessor might take the 
same number of instruction bits to perform the same action 
(i.e. a 32-bit operation code field followed by a 32-bit literal). 
The following describes some of the addressing modes that 
can be synthesized from the RISC's basic addressing modes. 
implemented by setting Rx = RO = 0, S2 = 13-bit constant 
implemented by setting S2 = RO = 0 
implemented by setting S2 = 13-bit constant (i.e. offset) 
implemented by setting S2 = [Ry] (LOAD instructions only) 
Conditional instructions do not require a destination address 
and therefore the 5 bits, 19 to 23, normally used to specify a 
destination register are used to specify the condition (one of 
16 because bit 23 is not used by conditional instructions). 
-»r • os i 
( Bobbie due to wterna: J 

8.3 RISC architecture and pipelining 
3 4 3 
Figure 8.14 Using latches to 
implement pipelining. 
Clock 
i 
Time slot 7"0 
i 
Time slot 7"-, 
i 
Time slot T2 
i 
Time slot 7"3 
i 
i 
i 
i 
i 
i 
J 
PQ output 
output 
Source , 
operands 
/ + 1 
/ + 2 
t=H3 
Instruction /' in IR1 
;' + 1 
/ - 1 
/ + 3 
/' + 2 
/ + 1 
/ + 4 
/ + 3 
; + Z 
Result 
(ALU output)_ 
/ - I 
/ + 1 
i Read instruction i Read operands 
i Generate result i 
Latch result 
i 
Figure 8.15 Timing diagram for a 
pipelined computer. 
During cycle T0 the output of the program counter interro-
gates the program memory and instruction i is read from 
memory. 
When the next clock pulse appears at the beginning of 
cycle T\, instruction i is latched in the instruction register and 
held constant for a clock cycle. The program counter is incre-
mented and instruction i + 1 is read from memory. The 
instruction in the instruction register is decoded and used to 
read its two source operands during cycle Tv 
At the end of cycle Tlt the two source operands for instruc-
tion i appear at the operand latches (just as instruction /' + 1 
appears at the IR). 
During cycle T2, the operands currently in the operand 
latches are processed by the ALU to produce a result that is 
captured by the result latch at the beginning of cycle T3. At 
this point, instruction i has been executed. Note that in time 
slot T2, the program counter contains the address of instruc-
tion i + 2 and the instruction register contains the op-code 
for instruction i + 1. 
In Fig. 8.14 there is a block marked T delay in the path 
between the op-code field of the IR and the ALU, and a block 
marked 2T delay between the destination field of the op-code 
and the register file. These delays are necessary to ensure that 
data arrives at the right place at the right time. For example, 
the operand data that goes to the ALU passes through the 
operand latches, which create a one-cycle delay. Consequently, 
the op-code has to be delayed for a cycle to avoid the data 
for instruction i getting to the ALU at the same time as the 
op-code for instruction i + 1. 
During cycle T3 the result of instruction i from the ALU is 
latched into the register file. In cycle T3, instruction i + 3 is in 
the program counter, instruction i + 2 is in the instruction 
Stage 1 
I 
Stage 2 
J 
Stage 3 
jStage 4 
Clpck 
| 
j 
i 
I 
Registers 
. 
I 
I* IR 
I I 
I 
[ 
T 
I 
I 
Operand 
I 
i 
I 
I 
Larches 
I 
. 
Solirce 1 
5 
Q 
i—Li 
1 
i 
. 
' 
I 
—' 
• 
1 > S, llip-flop 
J 
\ 
I ., 
! J., , 
. lL7h 
fc 
o2 rlE^J^HSl-i 
I 
| 
• 
• S2 |ip-nop —*\ 
/ 
| 
— 
ar 
-»» Instruction 
i 
I—r 
1 
I 
— / 
i 
*"- 
memory 
Op-code 
' 
' 
' 
' 
^ [ i n a t i o n ^
^
^ 
^
_
^ 
j 
1 
—©J 
— 1 
I — I 
I 
I 
I 
J 
Op-code 
— E — T delay 
1 
1 
. 
i 
4 
U—I 
i 
i 
• 
Read instruction from memory 
; 
Fetch source operands 
] 
Calculate result 
, Store result 

344 
Chapter 8 Accelerating performance 
register, instruction i + 1 is being executed, and the result of 
instruction i is being written back into the register file. A new 
instruction is completed (or retired) on each further clock 
pulse. 
There is little point in increasing the speed of the process-
ing if memory cannot deliver data and instructions when 
they are needed. This is a particularly critical issue in com-
puter design because memory speed has not kept up with 
processor speed. In the next section we look at how the effect-
ive speed of main store can be increased. 
8.4 Cache memory 
We now look at the cache memory that can dramatically 
increase the performance of a computer system at relatively 
little cost. 
Cache memory provides system designers with a way of 
exploiting high-speed processors without incurring the cost 
of large high-speed memory systems. The word cache is pro-
nounced 'cash' or 'cash-ay' and is derived from the French 
word meaning hidden. Cache memory is hidden from the 
programmer and appears as part of the system's memory 
space. There's nothing mysterious about cache memory—it's 
simply a quantity of very-high-speed memory that can be 
accessed rapidly by the processor. The element of magic 
stems from the ability of systems with a tiny cache memory 
(e.g. 512 kbytes of cache memory in a system with 2 Gbytes 
of DRAM) expecting the processor to make over 95% of its 
accesses to the cache rather than the slower DRAM. 
First-generation microprocessors had truly tiny cache 
memories; for example, 256 bytes. Up to the mid-1990s, 
cache sizes of 8 to 32 kbytes were common. By the end of the 
1990s, PCs had internal on-chip caches of 128 kbytes and 
external second-level caches of up to 1 Mbyte and in 2004 
on-chip cache memories of 2 Mbytes and up to 4 Gbytes of 
main store. 
Cache memory can be understood in everyday terms by its 
analogy with a diary or notebook used to jot down telephone 
numbers. A telephone directory contains hundreds of thou-
sands of telephone numbers and nobody carries a telephone 
directory around with them. However, lots of people have a 
notebook with a hundred or so telephone numbers that they 
keep with them. Although the fraction of all possible tele-
phone numbers in someone's notebook might be less than 
0.01%, the probability that their next call will be to a number 
in the notebook is high because they frequently call the same 
people. Cache memory operates on exactly the same prin-
ciple, by locating frequently accessed information in the 
cache memory rather than in the much slower main memory. 
Unfortunately, unlike the personal notebook, the computer 
cannot know, in advance, what data is most likely to be 
accessed. You could say that computer caches operate on a 
learning principle. By experience they learn what data is most 
frequently used and then transfer it to the cache. 
The general structure of a cache memory is provided in 
Fig. 8.16. A block of cache memory sits on the processor's 
address and data buses in parallel with the much larger main 
memory. The implication of parallel in the previous sentence 
is that data in the cache is also maintained in the main mem-
ory. To return to the analogy with the telephone notebook, 
writing a friend's number in the notebook does not delete 
their number in the directory. 
Cache memory relies on the same principle as the note-
book with telephone numbers. The probability of accessing 
the next item of data in memory isn't a random function. 
Because of the nature of programs and their attendant data 
structures, the data required by a processor is often highly 
clustered. This aspect of memories is called the locality of refer-
ence and makes the use of cache memory possible (it is of 
course the same principle that underlies virtual memory). 
A cache memory requires a cache controller to determine 
whether the data currendy being accessed by the CPU resides 
in the cache or whether it must be obtained from the main 
Data ^ 
"Data bus 
^ 
A 
/>-N 
LJ 
K 
Main store 
Address 
t 
J 
Address ht« 
^ ) 
CPU 
r r ^ — — 
• 
— 
1 
I / 
, 
\> 
, 
, 
V V , 
I 
i 
1 
X 
H. 
/ 
S u l S i r ^ h 
conquer 
~
* 
£ £ y 
Typicaily 64 Mbytes to 4Gbytes 
the cache and main 
\ 
memory 
\ 
* \ ^ 
\ 
-Typically 64 Kbytes to 2 Mbytes 
1 
If the data is in the cache, 
it is fetched from there rather 
than the main store 
Figure 8.16 Structure of a 
cache memory. 

8.4 Cache memory 
345 
memory. When the current address is applied to the cache 
controller, the controller returns a signal called hit, which is 
asserted if the data is currently in the cache. Before we look at 
how cache memories are organized, we will demonstrate 
their effect on a system's performance. 
8.4.1 Effect of cache memory on 
computer performance 
A key parameter of a cache system is its hit ratio (h), which 
defines the ratio of hits to all accesses. The hit ratio is deter-
mined by statistical observations of a real system and cannot 
readily be calculated. Furthermore, the hit ratio is dependent 
on the specific nature of the programs being executed. It is 
possible to have some programs with very high hit ratios and 
others with very low hit ratios. Fortunately, the effect of local-
ity of reference usually means that the hit ratio is very high— 
often in the region of 95%. Before calculating the effect of a 
cache memory on a processor's performance, we need to 
introduce some terms. 
Access time of main store 
tm 
Access time of cache memory 
tc 
Hit ratio 
h 
Miss ratio 
m 
Speedup ratio 
S 
The figure of merit of a computer with cache is called the 
speedup ratio, which indicates how much the cache acceler-
ates the memory's access time. The speedup ratio is defined as 
the ratio of the memory system's access time without cache to 
its access time with cache. 
N accesses to a system without cache memory requires Ntm 
seconds. N accesses to a system with cache requires 
N{htc + mtm) seconds; that is, the time spent in accessing the 
cache plus the time spent accessing the main memory multi-
plied by the total number of memory accesses. We can express 
m in terms of h as m = (1 — h), because if an access is not a 
hit it must be a miss. Therefore the total access time for a sys-
tem with cache is given by N(htc + (1 — h)tm). 
The speedup ratio is therefore given by 
S = 
NL 
N(htc + (1 - h)tj 
htc + (1 - h)tm 
We can introduce a new parameter, k, which defines the 
ratio of the access time of cache memory to main memory. 
That is, k = tjtm. Typical values for tm and fc might be 50 ns 
and 10 ns, respectively, which gives a value for k of 0.2. 
Therefore, 
S = 
tjtm 
Figure 8.17 provides a plot of S as a function of the hit ratio 
{h). As you might expect, when h = 0 and all accesses are 
made to the main memory, the speedup ratio is 1. Similarly, 
when h = 1 and all accesses are made to the cache the 
speedup ratio is 1/fc. The most important conclusion to be 
drawn from Fig. 8.17 is that the speedup ratio is a sensitive 
function of the hit ratio. Only when h approaches about 90% 
does the effect of the cache memory become really signific-
ant. This result is consistent with common sense. If h drops 
below about 90%, the accesses to main store take a dis-
proportionate amount of time and accesses to the cache have 
little effect on system performance. 
Life isn't as simple as these equations suggest. Computers 
are clocked devices and run at a speed determined by the 
clock. Memory accesses take place in one or more whole clock 
cycles. If a processor accesses main store in one clock cycle, 
adding cache memory is not going to make the system faster. 
If we assume that a computer has a clock cycle time t^, and 
accesses cache memory in p clock cycles (i.e. access 
time = pfcyc) and main store in q clock cycles, its speedup 
ratio is 
S = 
<?<a 
htc + (1 - h)tm 
pht,yc + (1 - h)t^c 
= 
q 
ph + (1 - h) 
If q = 4 and p = 2, the speedup ratio is given by 
1/(2W4 + 1 -h) = 2/(2 - h). 
In practice, we are more concerned with the performance 
of the entire system. A computer doesn't spend all its time 
accessing memory. The following expression gives a better 
picture of the average cycle time of a computer because it 
takes into account the number of cycles the processor spends 
performing internal (i.e. non-memory reference) operations. 
^average- " internal' ^^cyc """^Memory' *cycv*cache ' \* — "•Hfcache'" *delay// 
Speedu 
ratio ft 
_This figure assumes k = 0.2 
"The maximum value of 5 is Mk 
Hit ratio [h) 
1 
htjtm + ((l - h)tjtj 
hk+(i-h) 
Figure 8.17 Speedup as a function of hit ratio. 
Cache memory is effective 
only at high hit ratios—~_ 
0 i 
. — 
o 
os 
10 
5 
4 
3 
2 
1 

3 4 6 
Chapter 8 Accelerating performance 
where 
^internal = fraction of cycles the processor spends doing inter-
nal operations 
N 
= average number of cycles per internal operation 
(„,- 
= processor cycle time 
FMemory
= fraction of cycles processor spends doing memory 
accesses 
fdday 
~ additional penalty clock cycles required caused by a 
cache miss 
h 
= hit ratio 
'cache 
= cache memory access time (in clock cycles) 
Note that, by convention, the main memory access time is 
given by the number of cycles to access cache plus the addi-
tional number of cycles (i.e. the penalty) to access main store. 
If we put some figures into this equation, we get 
'average = 40% X 2 X 20 ns + 60% X 20 ns 
X 0 . 9 X 1 + 0.1(1 + 3) 
16 ns + 26 ns 
42 ns 
The effect of cache memory on the performance of a com-
puter depends on many factors including the way in which 
the cache is organized and the way in which data is written to 
main memory when a write access takes place. We will return 
to some of these considerations when we have described how 
cache systems are organized. 
8.4.2 Cache organization 
There are at least three ways of organizing a cache memory— 
direct-mapped, associative-mapped, and set associative-
mapped cache. Each of these systems has its own 
performancexost trade-off. 
Direct-mapped cache 
The easiest way of organizing a cache memory employs direct 
mapping, which relies on a simple algorithm to map data 
block i from the main memory into data block j in the cache. 
For the purpose of this section we will regard the smallest 
unit of data held in a cache as a line that is made up of typ-
ically two or four consecutive words. The line is the basic unit 
of data that is transferred between the cache and main store 
and varies between 4 and 32 bytes. 
Figure 8.18 illustrates the structure of a highly simplified 
direct-mapped cache. As you can see, the memory space is 
divided into sets and the sets into lines. This memory is com-
posed of 32 words and is accessed by a 5-bit address bus from 
the CPU. For the purpose of this discussion we need only 
consider the set and line (as it doesn't matter how many 
words there are in a line). The address in this example has a 
2-bit set field, a 2-bit line field, and a 1-bit word field. The 
cache memory holds 22 = 4 lines of two words. When the 
processor generates an address, the appropriate line in 
the cache is accessed. For example, if the processor generates 
the 5-bit address 101002, word 0 of line 2 in set 2 is accessed. 
A glance at Fig. 8.18 reveals that there are four possible 
lines numbered two—a line 2 in set 0, a line 2 in set 1, a line 2 
in set 2, and a line 2 in set 3. In this example the processor 
accessed line 2 in set 2. The obvious question is, 'How does 
the system know whether the line 2 accessed in the cache is 
the line 2 from set 2 in the main memory?' 
Figure 8.19 shows how a direct-mapped cache resolves the 
contention between lines. Each line in the cache memory has 
a tag or label, which identifies which set this particular line 
belongs to. When the processor accesses line 2, the tag 
belonging to line 2 in the cache is sent to a comparator. At the 
same time the set field from the processor is also sent to 
the comparator. If they are the same, the line in the cache is 
the desired line and a hit occurs. 
If they are not the same, a miss occurs and the cache must 
be updated. The old line 2 from set 1 is either simply dis-
carded or rewritten back to main memory depending on how 
the updating of main memory is organized. 
5-bit address from CPU 
•< 
• 
2 bits 2 bits 
1 bit 
Main store 
SetO 
Set1 
Set 2 
Set 3 
Set 
Line 
Word 
The line address 
selects the same 
line (line 2) in each 
of the four sets 
Cache memory 
LineO 
Line 1 
Line 2 
^ 
Line 3 
/ 
A line in the 
cache may 
come from one 
of the four sets 
Figure 8.18 The direct-mapped cache. 
LineO 
Line 1 
Line 2 
Line 3 
LineO 
Line 1 
Line 2 
Line 3 
LineO 
Line 1 
Line 2 
Lir= ? 
L"-e 0 
L "'i* i 
Line 3 

8.4 Cache memory 
3 4 7 
5-bit address from CPU 
« 
• 
2 bits 
2 bits 
1 bit 
Set address 
used to check 
tag at current 
line 
Tag field 
indicates 
ownership 
of the line 
Word selects the 
word in a tine 
Main store 
LineO 
Line 1 
Line 2 
Line 3 
LineO 
Line 1 
Line 2 
Line 3 
LineO 
Line 1 
Line 2 
Line 3 
LineO 
Line' 
Line 2 
Line 3 
SetO 
Set 1 
Set 2 
Set 3 
Figure 8.19 Resolving contentions between 
lines in a direct-mapped cache. 
Figure 8.20 provides a skeleton structure of a direct-
mapped cache memory system. The cache memory itself is 
nothing more than a block of very-high-speed random access 
read/write memory. The cache tag RAM is a fast combined 
memory and comparator that receives both its address and 
data inputs from the processor's address bus. The cache tag 
RAM's address input is the line address from the processor 
that is used to access a unique location (one for each of the 
possible lines). The data in the cache tag RAM at this location 
is the tag associated with that location. The cache tag RAM 
also has a data input that receives the tag field from the 
processor's address bus. If the tag field from the processor 
matches the contents of the tag (i.e. set) field being accessed, 
the cache tag RAM returns a hit signal. 
As Fig. 8.20 demonstrates, the cache tag RAM is nothing 
more than a high-speed random access memory with a built-
in data comparator. Some of the major semiconductor 
manufacturers have implemented single-chip cache tag 
RAMs. 
The advantage of the directly mapped cache is almost self-
evident. Both the cache memory and the cache tag RAM are 
widely available devices which, apart from their speed, are no 
more complex than other mainstream devices. Moreover, die 
direct-mapped cache requires no complex line replacement 
algorithm. If line x in set y is accessed and a miss takes place, 
line x from set y in the main store is loaded into the frame for 
line x in the cache memory and the tag set to y. That is, there 
is no decision concerning which line has to be rejected when 
a new line is to be loaded. 
Another important advantage of direct-mapped cache is 
its inherent parallelism. Because the cache memory holding 
the data and the cache tag RAM are entirely independent, 
they can both be accessed simultaneously. Once the tag has 
been matched and a hit has occurred, the data from the cache 
will also be valid (assuming the two cache data and cache tag 
memories have approximately equal access times). 
The disadvantage of direct-mapped cache is almost a 
corollary of its advantage. A cache with n lines has one 
restriction—at any instant it can hold only one line num-
bered x. What it cannot do is hold a line x from set p and a line 
x from set q. This restriction exists because there is one page 
frame in the cache for each of the possible lines. Consider the 
following fragment of code: 
REPEAT 
Get_data 
Compare 
UNTIL match OR end of data 
This fragment of code reads data and compares it with 
another string until a match is found. Suppose that the 
Get_data routine is in set x, line y and that part of the 
Compare routine is in set z, line y. Because a direct-mapped 
cache can hold only one line y at a time, the frame cor-
responding to line y must be reloaded twice for each path 
through the loop. Consequently, the performance of a direct-
mapped cache can sometimes be poor. Statistical measure-
ments on real programs indicate that the very poor 
J 
\ 
Line address 
selects a tine 
in the cache 
/ 
Data field of 
cache 
memory 
Cache memory 
[~0~ 
line 0 
— 
I 3 
Line 1 
-
EE 
Line 2 
x 
Q _ 
LineS 4 
— 
i 
Set 
Line 
Word 
i 
__j 
— i — 
1 
address 

348 
Chapter 8 Accelerating performance 
Line address 
Cache tag 
memory 
Tag associated 
with the line 
being accessed 
Address bus 
Cache 
memory 
i Set address 
Main store 
Line and word 
address 
A hit occurs if the tag associated 
with the currently addressed line 
read from the cache tag RAM 
matches the set address from 
the CPU 
Figure 8.20 Implementation 
of direct-mapped cache. 
worst-case behavior of direct-mapped caches has no signific-
ant impact on their average behavior. 
Suppose a cache is almost empty and most of its lines have 
not yet been loaded with active data. Certain lines may have 
to be swapped out of the cache frequently because data in the 
main store just happens to share the same line numbers. In 
spite of this objection to direct-mapped cache, it is very 
popular because of its low cost of implementation and high 
speed. 
Associative-mapped cache 
One way of organizing a cache memory that overcomes the 
limitations of direct-mapped cache is described in Fig. 8.21. 
Ideally, we would like a cache that places no restrictions on 
what data it can contain. The associative cache is such a 
memory. 
An address from the processor is divided into three fields: 
the tag, the line, and the word. Like the direct-mapped cache, 
the smallest unit of data transferred into and out of the cache 
is the line. Unlike the direct-mapped cache, there's no pre-
determined relationship between the location of lines in the 
cache and lines in the main memory. Line p in the memory 
can be put in line q in the cache with no restrictions on the 
values of p and q. Consider a system with 1 Mbyte of main 
store and 64 kbytes of associatively mapped cache. If the size 
of a line is four 32-bit words (i.e. 16 bytes), the main memory 
is composed of 220/16 = 64K lines and the cache is composed 
of 216/16 = 4096 lines. Because an associative cache permits 
any line in the main store to be loaded into one of its lines, 
line i in the associative cache can be loaded with any one of 
the 64K possible lines in the main store. Therefore, line i 
Address from CPU 
< 
• 
Tag 
word 
Main store 
Cache memory 
| Tag 
|Tag 
|Tag 
JTag 
| Tag 
*t 
|Tag 
\ 
The tag from the 
address bus is 
compared with all 
tags in the cache 
simultaneously 
LineO 
Line 1 
Line 2 
Line 3 
Line n 
A line in the cache may 
come from any line in 
the main store 
Figure 8.21 Associative-mapped cache. 
requires a 16-bit tag to uniquely label it as being associated 
with line i from the main store. 
When the processor generates an address, the word bits 
select a word location in both the main memory and the 
cache. Unlike the direct-mapped cache memory, the line 
Data ,'1 
Address 
'v_. 
CPU 
.0 
1. 
2 
3 
line 
tag 
k 

8.4 Cache memory 
349 
address from the processor can't be used to address a line in 
the associative cache. Why? Because each line in the direct-
mapped cache can come only from one of n lines in the main 
store (where n is the number of sets). The tag resolves which 
of the lines is actually present. In an associative cache, any of 
the 64K lines in the main store can be located in any of the 
lines in the cache. Consequently, the associative cache 
requires a 16-bit tag to identify one of the 216 lines from the 
main memory. Because the cache's lines are not ordered, the 
tags are not ordered and cannot be stored in a simple look-up 
table like the direct-mapped cache. In other words, when the 
CPU accesses line i, it may be anywhere in the cache or it may 
not be in the cache. 
Associative cache systems employ a special type of mem-
ory called associative memory. An associative memory has an 
n-bit input but not necessarily 2" unique internal locations. 
The n-bit address input is a tag that is compared with a tag 
field in each of its locations simultaneously. If the input tag 
matches a stored tag, the data associated with that location is 
output. Otherwise the associative memory produces a miss 
output. An associative memory is not addressed in the same 
way that a computer's main store is addressed. Conventional 
computer memory requires the explicit address of a location, 
whereas an associative memory is accessed by asking, 'Do you 
have this item stored somewhere?' 
Associative cache memories are efficient because they 
place no restriction on the data they hold. In Fig. 8.21 the tag 
that specifies the line currently being accessed is compared 
with the tag of each entry in the cache simultaneously. 
In other words, all locations are accessed at once. 
Unfortunately, large associative memories are not yet cost 
effective. Once the associative cache is full, a new line can be 
brought in only by overwriting an existing line that requires 
a suitable line replacement policy (as in the case of virtual 
memories). 
Set associative-mapped cache 
Most computers employ a compromise between the direct-
mapped cache and the fully associative cache called a set asso-
ciative cache. A set associative cache memory is nothing more 
than multiple direct-mapped caches operated in parallel. The 
simplest arrangement is called a two-way set associative 
cache and consists of two direct-mapped cache memories so 
that each line in the cache system is duplicated. For example, 
a two-way set associative cache example has two line 5s and 
it's possible to store one line 5 from set x and one line 5 
from set y. 
If the cache has n parallel sets, an rc-way comparison is per-
formed in parallel against all members of the set. Because n in 
small (typically 2 to 16), the logic required to perform the 
comparison is not complex. 
Figure 8.22 describes the common four-way set associa-
tive cache. When the processor accesses memory, the 
Address from CPU 
Each of the 
caches is direct 
mapped 
Composite hit 
A hit occurs if any one 
of the four caches 
responds to an access 
Figure 8.22 Set associative-mapped cache. 
appropriate line in each of four direct-mapped caches is 
accessed simultaneously. Because there are four lines, a 
simple associative match can be used to determine which 
(if any) of the lines in cache are to supply the data. In 
Fig. 8.22 the hit output from each direct-mapped cache is 
fed to an OR gate, which generates a hit if any of the caches 
generate a hit. 
Level 2 cache 
The memory hierarchy can be expanded further by dividing 
the cache memory into a level 1 and a level 2 cache. A level 1 
cache is normally located on the same chip as the CPU itself; 
that is, it is integrated with the processor. Over the years, 
level 1 caches have grown in size as semiconductor technol-
ogy has advanced and more memory devices can be inte-
grated on a chip. A level 2 cache was once invariably located 
off the processor chip but modern high-performance 
devices have on-chip level 1 and level 2 caches. By 2005 Intel 
Pentium processors were available with 2 Mbyte level 2 
caches. 
When the processor makes a memory access, the level 1 
cache is first searched. If the data isn't there, the level 2 cache 
is searched. If it isn't in the level 2 cache, the main store is 
accessed. The average access time is given by 
fave = M e l 
+ (1 - 
KdKlhl 
+ (1 - JlL1)(l - 
hu)tmmory 
where hu and hu
 a r e the hit rates of the level 1 and level 2 
caches, and fcl and tc2 are the access times of the LI and L2 
caches, respectively. 
Consider a system with a hit ratio of 0.90, a single-level 
cache access time of 4 ns, and a main store access time of 
50 ns. The speedup ratio is given by ll(hk + 1 — h) = 5.81. 
I >£ 
1 
......A*. •-- 
. V 
1 
| A£ 1 
Cache 1 
Cache 2 
Cache 3 
Cache 4 
Hit 
Hit 
Hit 
Hit 
1 
1 ' 
' 1 ' 
' 1 ' 
' 1 

350 
Chapter 8 Accelerating performance 
Suppose we add a level 2 cache to this system and that the 
level 2 cache has a hit ratio of 0.7 and an access time of 8 ns. 
In this case, the access time is 
fave 
=hUtcl + (l~hu)hutc2 
+ 
(l~hu)(\"hL1) 
fmemory = 0 . 9 X 4 + 0 . 1 X 0 . 7 X 8 + 0 . 1 X 0 . 3 X 5 0 = 5 . 6 6 n S 
The 
speedup 
ratio 
with 
a 
level 
2 
cache 
is 
50 ns/ 5.66 ns = 8.83. 
8.4.3 Considerations in cache design 
Apart from choosing the structure of a cache system and the 
line replacement policy, the designer has to consider how 
write cycles are to be treated. Should write accesses be made 
only to the cache and then the main store updated when the 
line is replaced (a writeback policy)? Should the main mem-
ory also be updated each time a word in the cache is modified 
(a writethrough policy)? The writethrough policy allows the 
cache to be written to rapidly and the main memory updated 
over a longer span of time (if there is write buffer to hold 
the data until the bus becomes free). A writethrough 
policy can lead to more memory write accesses than are 
strictly necessary. 
When a cache miss occurs, a line of data is fetched from the 
main store. Consequently, the processor may read a byte from 
the cache and then the cache requires a line of, say, 8 bytes 
from the main store. As you can imagine, the cost of a miss on 
an access to cache carries an additional penalty because an 
entire line has to be filled from memory. Fortunately, modern 
memories, CPUs, and cache systems support a burst-fill 
mode in which a burst of consecutive data elements can be 
transferred between the main store and cache memory. Let's 
look at cache access times again. 
If data is not in the cache, it must be fetched from memory 
and loaded in the cache. If t\ is the time taken to reload the 
cache on a miss, the effective average access time is given by 
(ave = htc + (1 - h)tm + (1 - /j)f, 
The term (1 — h)tt is the additional time required to 
reload the cache following each miss. This expression can be 
rewritten as 
W = K + 0 - >>)(fl + O 
The term {tt + tm) corresponds to the time taken to access 
main memory and to load a line in the cache following a miss. 
However, because both accessing the element that caused the 
miss and filling the cache take place in parallel, we can note 
that ti > tm and simplify the equation to get 
fave = htc + (1 - h)t, 
The performance of cache memory systems is also deter-
mined by the relative amounts of read and write accesses and 
the different ways in which read and write cache accesses are 
treated. Relatively few memory accesses are write operations 
(about 5 to 30% of memory accesses). If we take into account 
the action taken on a miss during a read access and on a miss 
during a write access, the average access time for writethough 
memory is given by 
fave = K 
+ (1 - 
fc)(l 
- 
W)t, + (1 " 
h)wtc 
where w is the fraction of write accesses and t\ is the time 
taken to reload the cache on a miss. The (1 — h)(l — w)t\ 
term is the time taken to reload the cache on a read access and 
(1 — h)wtc represents the time taken to access the cache on a 
write miss. This equation is based on the assumption that 
writes occur infrequently and therefore the main store has 
time to store writethrough data between two successive write 
operations. 
Another aspect of cache memories that has to be taken into 
account in sophisticated systems is cache coherency. As we 
know, data in the cache also lives in the main memory. When 
the processor modifies data it must modify both the copy in 
the cache and the copy in the main memory (although not 
necessarily at the same time). There are circumstances when 
the existence of two copies (which can differ) of the same 
item of data causes problems. For example, an I/O controller 
using DMA might attempt to move an old line of data from 
the main store to disk without knowing that the processor has 
just updated the copy of the data in the cache but has not yet 
updated the copy in the main memory. Cache coherency is 
also known as data consistency. 
8.5 Multiprocessor systems 
One way of accelerating the performance of a computer with-
out resorting to either new technology or to a new architec-
ture is to use a multiprocessor system; that is, you take two or 
more CPUs and divide the work between them. Here we 
introduce some basic concepts of multiprocessing hardware 
and the topology of multiprocessors. 
The speedup ratio, Sp> of a multiprocessor system using p 
processors is defined as Sp = r,/Tp, where Tj is the time taken 
to perform the computation on a single processor and Tp is 
the time taken to perform the same computation on p pro-
cessors. The value of Sp must fall in the range 1 < Sp < />. The 
lower limit on 5p corresponds to a situation in which the par-
allel system cannot be exploited and only one processor can 
be used. The upper limit on Sp corresponds to a problem that 
can be divided equally between the p processors. The effi-
ciency, £., of a multiprocessor system is defined as the ratio 
between the speedup factor and the number of processors; 
that is Ep = Sp/p = T^pTp. The efficiency, Ep, must fall in the 
range 1 (all processors used fully) to 1/p (only one processor 
out of p used). 
Whenever I think of multiprocessing I think of air travel. 
Suppose you want to get from central London to downtown 

8.5 Microprocessor systems 
351 
Manhattan. The time taken is the sum of the time to travel to 
Heathrow airport, the check-in time, the transatlantic jour-
ney, the baggage reclaim time, and the time to travel from IFK 
to downtown Manhattan. The approximate figures (in 
hours) are 0.9 + 1.5 + 6.5 + 0.5 + 1 = 10.4. Suppose you 
now decide to speed things up and travel across the Atlantic 
in a supersonic aircraft that takes only 3 hours; the new times 
are 0.9 + 1.5 + 3 + 0.5 + 1 = 6.9 hours. The speedup ratio 
between these two modes of travel is 10.4/6.9 = 1.51. 
Increasing the speed of the aircraft by a factor of 2.17 has 
resulted in a speedup ratio of only 1.51, because all the other 
delays have not been changed. 
The same problem affects multiprocessing—the speedup 
ratio is profoundly affected by the parts of a problem that 
cannot be computed in parallel. Consider, for example, the 
product (P + Q)(P - Q). The operations P + Q and P - Q 
can be carried out simultaneously in parallel, whereas their 
product can be carried out serially only after P + Q and 
P — Q have been evaluated. 
Figure 8.23 shows how a task may have components that 
must be executed serially, Ps, and tasks that can be executed in 
parallel, Pp. If each task in Fig. 8.23 requires f seconds, the 
total time required by a serial processor is 8f. Because three 
pairs of tasks can be carried out in parallel, the total time 
taken on a parallel system is 4f. 
Suppose a task consists of a part that must be computed 
serially and a part that can be computed by processors in par-
allel. Let the fraction of the task executed serially be/and the 
fraction executed in parallel be (1 — f). The time taken to 
process the task on a parallel processor is/Ti + (1 —/) TJp, 
where t is the time required to execute the task on a single 
processor and p is the number of processors. The speedup 
ratio is S„ = lytfT, + (1 - f)TJp) 
= p/(l + (p - 1)/). 
This equation is Amdahl's law and tells us that increasing the 
number of processors in a system is futile unless the value of 
/is very low. 
Figure 8.24 demonstrates the relationship between the 
speedup ratio, S(f), and/(the fraction of serial processing) 
for a system with 16 processors. The horaontal axis is 
the fraction of a task that is executed in parallel, 1 — /. As 
you can see, the speedup ratio rises very rapidly as 1 — / 
approaches 1. 
Parallel process 
/ 
r v i PJ 
_ _ 
Figure 8.23 Executing a task in serial and 
If each process takes t seconds, the total time taken is At 
parallel. 
16 
"I 
"I 
I 
I 
I 
I 
I 
I 
i 
i 
i 
12 -
/ 
8 -
J-
4 
I 
I 
I 
I 
I 
I 
L 
1 
1 
1 
1 
-0.2 -0.1 
0 
0.1 
0.2 
0.3 
0.4 
1-f 
0.5 
0.6 
0.7 
0.8 
0.9 
Figure 8.24 The effect of fon the speedup ratio 
(p=16). 
Serial process 
Start 
>(^y 
& 
na 
^ > 
&
>
• 
s@ 
<5 
-0-
PP 
PP 
PP 

352 
Chapter 8 Accelerating performance 
You can't just plug an extra processor into an existing sys-
tem to convert it into a multiprocessor. The global implica-
tions for the system hardware and its software are not trivial, 
because the individual processors have to share the available 
resources (i.e. memory and input/output). An effective mul-
tiprocessor system must be able to allocate resources to con-
tending 
processors 
without 
seriously degrading 
the 
performance of the system. 
Some multiprocessor systems are termed reconfigurable, 
because the structure of the hardware itself can be modified 
by the operating system. For example, the way in which mem-
ory is distributed between the individual processors or the 
paths between the processors can be changed dynamically 
under software control. Similarly, interrupt handling can be 
dynamically partitioned between the various processors to 
maximize efficiency. We do not discuss reconfigurable archi-
tectures further here. 
Although the architecture of a stored-program computer 
(i.e. a von Neumann machine) can be defined quite precisely, 
there is no similar definition of a multiprocessor system. 
Multiprocessor systems come in many different flavors and a 
configuration suitable for one particular application may be 
useless for another. The only really universal characteristic 
common to all multiprocessor systems is that they have more 
than one processor. We shall soon examine the various classes 
of multiprocessor system. 
Multiprocessor systems design is not easy; there are a lot 
of factors to take into account; for example, the distribution 
of tasks between processors, the interconnection of the 
processors (i.e. the topology of the multiprocessor system), 
the management of the memory resources, the avoidance of 
deadlock, and the control of input/output resources. 
Deadlock occurs when two or more processors cannot 
continue because each is blocking the other. 
The distribution of tasks between processors is of crucial 
importance in selecting the architecture of the processor sys-
tem itself. In turn, the distribution of tasks is strongly deter-
mined by the nature of the problem to be solved by the 
computer. In other words, the architecture of a multipro-
cessor system can be optimized for a certain type of problem. 
Conversely, a class of programs that runs well on one multi-
processor system may not run well on another. 
A classic problem that can be solved by multiprocessing 
belongs to the world of air-traffic control. A radar system 
receives a periodic echo from the targets (i.e. aircraft) being 
tracked. Each echo E, is a function of the bearing, 9 and 
distance or range, r, of the target. Due to noise and imperfec-
tions in the system, there is an uncertainty or error, s, associ-
ated with each echo. A new echo is received every few 
millisecond. Given this stream of inputs, £r e T + er_ e T, the 
computer connected to the radar receiver has to calculate the 
current positions of the targets and then estimate the future 
track of each target and report any possible conflicts. Such a 
system requires very large amounts of computer processing 
power with relatively little I/O activity or disk access. 
Obviously it is reasonable to try to solve the problem by 
means of multiprocessing. For example, as one processor is 
updating a target's current position, another processor can be 
calculating its future position. 
The preceding problem is described as classic, because it is 
so well suited to multiprocessing. There are several ways of 
allocating the mathematics involved in the radar calculations 
to the various processors. It is, unfortunately, much less easy 
to decompose a general task into a number of subtasks that 
can be run in parallel. Often it is necessary for the program-
mer to write programs in such a way that they involve the 
greatest amount of parallel activity. Other problems well 
suited to parallel processing are the simulation of complex 
dynamic systems such as the atmosphere or the motion of 
liquids. 
8.5.1 Topics in multiprocessor systems 
A key parameter of a multiprocessor system is its topology, 
which defines how the processors are arranged with respect 
to each other and how they communicate. A more important 
parameter of a multiprocessor system is the degree of cou-
pling between the various processors. We will discuss proces-
sor coupling first and then look at multiprocessor topologies. 
Processors with facilities for exchanging large quantities of 
data very rapidly are said to be tightly coupled. Such com-
puters share resources like buses or blocks of memory. The 
advantage of tightly coupled systems is their potential speed, 
because one processor doesn't have to wait long periods of 
time while data is transferred from another. Their disadvan-
tage arises from the complexity of the hardware and software 
necessary to coordinate the processors. If they share a bus or 
memory, an arbiter is needed to determine which processor is 
permitted to access the resource at anytime. 
Although not a problem associated entirely with multi-
processors, the avoidance of deadlock must feature in the 
design of some classes of multiprocessor. Deadlock describes 
the situation in which two tasks are unable to proceed 
because each task holds something needed by the other. In a 
real-time system, the sequential tasks (i.e. the software) 
require resources (memory, disk drives, I/O devices, etc.), 
whereas in a multiprocessor system these resources are 
required by the individual processors. 
Every multiprocessor system, like every single-processor 
system, has facilities for input or output transactions. We 
therefore have the problem of how I/O transactions are to be 
treated in a multiprocessor system. Does each processor have 
its own I/O arrangements? Is the I/O pooled between the 
processors, with each processor asking for I/O facilities as 
they are needed? Finally, is it possible to dedicate one or more 
processors solely to the task of I/O processing? 

8.S Microprocessor systems 
353 
In a similar vein, the designer of a multiprocessor may 
need to construct an appropriate interrupt-handling system. 
When an I/O device interrupts a processor in a single-proces-
sor system, there is not a lot to decide. Either the processor 
services the interrupt or it is deferred. In a multiprocessor 
system we have to decide which processor will service an 
interrupt, which in turn begs the question, 'Do we pool inter-
rupts or do we allocate certain types of interrupt to specific 
processors?' If interrupts are pooled, the interrupt-handling 
software must also be pooled, as processor A must deal with 
an interrupt from device X in exactly the same way that 
processor B would deal with the same interrupt. In addition 
to interrupts generated by I/O devices, it is possible for one 
processor to interrupt another processor. 
Like any other computer, the multiprocessor requires an 
operating system. There are two basic approaches to the 
design of operating systems for multiprocessors. One of the 
simplest arrangements is the master-slave operating system 
in which a single operating system runs on the master proces-
sor and ail other processors receive tasks that are handed 
down from the master. The master-slave operating system is 
little more than the type of operating system found in con-
ventional single-processor systems. 
The distributed operating system provides each processor 
with its own copy of the operating system (or at least a 
processor can access the common operating system via 
shared memory). Distributed operating systems are more 
robust than their master-slave counterparts because the fail-
ure of a single processor does not necessarily bring about a 
complete system collapse. 
The problems we have just highlighted serve to emphasize 
that a multiprocessor system cannot easily be built in a vac-
uum. Whenever we are faced with the design of a multi-
processor system, it is necessary to ask, 'Why do we need the 
multiprocessor system and what are its objectives?' and then 
to configure it accordingly. In other words, almost all design 
aspects of a multiprocessor system are very much problem 
dependent. 
8.5.2 Multiprocessor organization 
Although there is an endless variety of multiprocessor archi-
tectures, we can identify broad groups whose members have 
certain features in common. One possible approach to the 
classification of multiprocessor systems, attributed to 
Michael J. Flynn, is to consider the type of the parallelism (i.e. 
architecture or topology) and the nature of the interpro-
cessor communication. Flynn's four basic multiprocessor 
architectures are referred to by the abbreviations SISD, 
SIMD, MISD, and MIMD and are described later. However, 
before continuing, we must point out that Flynn's topological 
classification of multiprocessor systems is not the only one 
possible, as multiprocessors may be categorized by a number 
of different parameters. One broad classification of multi-
processors depends on the processor's relationship to mem-
ory and to other processing elements. Multiprocessors can be 
classified as processor to memory (P to M) structures or as 
processing element to processing element (PE to PE) struc-
tures. Figure 8.25 describes these two structures. A P to M 
architecture has n processors, an interconnection network, 
and n memory elements. The interconnection network allo-
cates processor X to memory Y. The more general PE to PE 
architecture uses n processors, each with its own memory, 
and permits processor element X to communicate with PE Y 
via an interconnection network. The multiprocessors 
described in this chapter best fit the PE to PE model. 
SISD (single instruction single data-stream) 
The SISD machine is nothing more than the conventional 
single-processor system. It is called single instruction because 
only one instruction is executed at a time, and single data-
stream because there is only one task being executed at any 
instant. 
SIMD (single instruction multiple data-stream) 
The SIMD architecture executes instructions sequentially, 
but on data in parallel. The idea of a single instruction oper-
ating on parallel data is not as strange as it may sound. 
Consider vector mathematics. A vector is a multicomponent 
data structure; for example, the four-component vector A 
might be 0.2, 4.3,0.2,0.1. A very frequent operation in most 
branches of engineering is the calculation of the inner product 
of two n-component vectors, A and B: 
s = A-B = aibi 
For example, if A is (1,4,3,6) and B is (4,6,2,3), the inner product 
A-B is 1X4 + 4 X 6 + 3X2 + 6 X 3 = 4 + 24 + 6 + 18 = 52. 
The inner product can be expressed as single operation (i.e. 
s = (A-B), but involves multiple data elements (i.e. the 
{a,-bj. Such calculations are used extensively in computer 
graphics and image processing. One way of speeding up the 
calculation of an inner product is to assign a processor to the 
generation of each of the individual elements, the [^ • bj The 
simultaneous calculation of a, • bt for i = 1 to n requires 
« processors, one for each component of the vector. 
Such an arrangement consists of a single controller that 
steps through the program (i.e. the single instruction-stream) 
and an array of processing elements (PEs) acting on the com-
ponents of a vector in parallel (i.e. the multiple data-stream). 
Often, such PEs are number crunchers or high-speed ALUs, 
rather than the general-purpose microprocessors we have 
been considering throughout this text. 
The SIMD architecture, or array processor, has a high per-
formance/cost ratio and is very efficient, as long as the task 
running on it can be decomposed largely into vector opera-
tions. Consequently, the array processor is best suited to the 

3 5 4 
Chapter 8 Accelerating performance 
Processor 
0 
Processor 
1 
Processor 
2 
Processor 
n-1 
Interconnection network 
Memory 
0 
Memory 
1 
Memory 
2 
(a) Processor to memory multiprocessor organization (P to M). 
Memory 
n-1 
Processor 
0 
Memory 
0 
Y 
Processor 
1 
Memory 
1 
Processor 
2 
Memory 
2 
Interconnection network 
Processor 
n-1 
Memory 
n-1 
(b) Processing element to processing element multprocessor organization (PE to PE). 
-r 
Figure 8.25 Processor to 
-J 
memory, and processor to 
processor structures. 
air-traffic control problem discussed earlier, to the processing 
of weather information (this involves partial differential 
equations), and to computerized tomography where the out-
put of a body scanner is processed almost entirely by vector 
arithmetic. As the SIMD architecture is generally built 
around a central processor controlling an array of special-
purpose processors, the SIMD architecture is not discussed in 
any further detail here. However, we provide an example of a 
SIMD architecture to illustrate one of its applications. 
Figure 8.26 demonstrates the application of a SIMD architec-
ture to image smoothing—an operation performed on images 
to remove noise (we discuss image processing further when 
we introduce computer graphics).7 In this example, an image 
from a noisy source such as a spacecraft camera is smoothed 
or filtered to yield a relatively noise-free image. Consider an 
input array,I,of512 X 512 pixels, which is to be smoothed to 
produce an output array S. 
A pixel is an 8-bit unsigned integer representing one of 256 
gray levels from 0 (white) to 255 (black). Each pixel Suj in the 
smoothed array, S, is the average of the gray levels of its eight 
nearest neighbors. By averaging the pixels in this way, the 
effects of noise bursts are reduced. The neighbors of Sy in the 
input array are I,-i,j-„ I,-i,j> I;-i,j+i> kj-\> hj+i> h+ij-v h+ijan<* 
Ii+w+1. The top, bottom, and left- and right-edge pixels of S 
are set to zero, because their corresponding pixels in I do not 
have eight adjacent neighbors. 
The following example demonstrates how this smoothing 
algorithm operates. The left-hand array represents the near 
neighbors of a pixel before it is smoothed. The right-hand 
array shows the pixel after smoothing. As you can see the 
value 6 has been reduced to 3.2. 
Before smoothing 
After smoothing 
If the smoothing algorithm were performed serially, it 
would be necessary to compute the value for each of the 
512 X 512 pixels by looking at its eight nearest neighbors. 
Parallel processing allows us to process groups of pixels at the 
same time. 
Assume that an SIMD array has 1024 processing elements 
(PEs), logically arranged as an array of 32 X 32 PEs as shown 
in Fig. 8.26. Each PE stores a 16 X 16 pixel sub-image block 
of the 512 X 512 pixel image I. For example, PE„ stores a 
16 X 16 pixel sub-image block composed of columns 0 to 15 
and rows 0 to 15; PE, stores the pixels in columns 16 to 31 of 
rows 0 to 15, etc. Each PE smoothes its own subimage, with all 
7 Example 
from 
Large-scale Parallel 
Processing Systems, 
Microprocessors and Microsystems, January 1987, by Howard J. Siegel 
et ah pp. 3-20. 
2 
3 
3 
2 
3 
3 
2 
6 
3 
2 
3.2 
3 
2 
1
2 
2 
1
2 

8.5 Microprocessor systems 
355 
16 pixels 
16 pixels 
Detail showing inter-PE data transfer 
Figure 8.26 Using a SIMD architecture in image processing. 
PEs operating on their subimages concurrently. At the edges 
of each 16X16 subimage, data must be transferred between 
adjacent PEs in order to calculate the smoothed value. The 
necessary data transfers for PEj are shown in Fig. 8.26. 
Transfers between different PEs can take place simultan-
eously. For example, when P E ^ sends its upper right corner 
pixel to PE;, PE; can send its own upper right corner pixel to 
PE; + !, and so on. 
To perform a smoothing operation on a 512 X 512 pixel 
image by the parallel smoothing of 1024 subimage blocks of 
16 X 16 pixels, 256 parallel smoothing operations are 
executed. However, the neighbors of each subimage edge pixel 
must be transferred between adjacent PEs and the total num-
ber of parallel data transfers required is (4 X 16) + 4 = 68 
(i.e. 16 for each of the top, bottom and left- and right-side 
edges). The corresponding serial algorithm needs no data 
transfers between PEs but 5122 = 264 144 smoothing calcu-
lations must be executed. If no data transfers were needed, 
the parallel algorithm would be faster than the serial algo-
rithm by a factor of 262 144/256 = 1024. If the inter-PE data 
transfer time is included and it is assumed that each parallel 
data transfer requires at most as much time as one smoothing 
operation, 
then 
the 
time 
factor 
improvement 
is 
262 144/(256 + 68) = 809. 
The last step in the smoothing process is to set the edge 
pixels of S to zero. This creates an additional (although negli-
gible) overhead, which is to enable only the appropriate PEs 
when the zero values are stored for these edge pixels (only 
enabled PEs execute the instructions broadcast to the PEs). 
Serially, this would require (4 X 512) - 4 = 2044 parallel 
stores. The SIMD architectures can be implemented by 
means of arrays of relatively primitive processing elements 
(e.g. ALUs). It is not usually necessary to make each process-
ing element as complex as a CPU. 
MISD (multiple instruction single data-stream) 
The MISD architecture performs multiple operations con-
currently on a single stream of data and is associated with the 
pipeline processor. We described the concept of the pipeline 
when we introduced the RISC processor. The difference 
between a MISD pipeline and a RISC pipeline is one of scale. 
In multiprocessor terms, the various processors are 
arranged in line and are synchronized so that each processor 
accepts a new input every r seconds. If there are n processors, 
the total execution time of a task is n • f seconds. At each epoch, 
a processor takes a partially completed task from a down-
stream processor and hands on its own task to the next 
upstream processor. As a pipeline processor has N processors 
operating concurrently and each task may be in one of the N 
stages, it requires a total of N-t + {K - 1) time slots to 
process K tasks. The MISD architecture is not suited to mul-
tiprocessor systems based on general-purpose microproces-
sors. MISD systems are highly specialized and require 
special-purpose architectures; they have never been devel-
oped to the same extent as SIMD and MIMD architectures. 
MIMD (multiple instruction multiple data-stream) 
The MIMD architecture is really the most general-purpose 
form of multiprocessor system and is represented by systems 
in which each processor has its own set of instructions oper-
ating on its own data structures. In other words, the pro-
cessors are acting in a largely autonomous mode. Each 
individual processor may be working on part of the main task 
and does not necessarily need to get in touch with its 
1 pixel 
- 1 6 
~~ ' pixels 
1 pixel 
16 
pixels 
1 pixel 
PEy 
_ | 
L 
^ ^ - 1 6 pixels 
T— 
16 pixels 
512 
pixels 
512 
pixels 
"" "H023 
1 pixel 
PE 9 9 2 ' 
PEo PE, 
PE32 

3 5 6 
Chapter 8 Accelerating performance 
Computers 
Serial 
SISD 
Parallel 
MISD 
Overlapped 
operations 
Multi-ALU 
von Neumann 
machines 
I SIMD i 
Pipeline 
processors 
^ ^ 
Array 
processors 
Associative 
processors 
MIMD 
Tightly 
Moderately 
coupled 
coupled 
Loosely 
coupled 
Distributed 
systems 
Multiprocessor 
systems 
Computer 
networks 
Figure 8.27 
Multiprocessor 
categories. 
(a) The unconstrained topology. 
(b) The fully connected topology. 
Figure 8.28 The unconstrained topology. 
neighbors until it has finished its subtask. The PE to PE archi-
tecture described in Fig. 8.25 can be thought of as a generic 
MIMD machine. 
Because of the generality of the MIMD architecture, it can 
be said to encompass the relatively tightly coupled arrange-
ments to be discussed shortly and the very loosely coupled 
geographically distributed LANs. Figure 8.27 provides a 
graphical illustration of the classification of multiprocessor 
systems according to E. T. Fathi and A. M. Krieger (Multiple 
Microprocessor Systems: What, Why and When, Computer, 
March 1983, pp. 23-32). 
8.5.3 MIMD architectures 
Although the array processor or the pipeline processor is 
likely to be constructed from very special units, the more gen-
eral MIMD architecture is much more likely to be built from 
widely available off-the-shelf microprocessors. Therefore, the 
major design consideration in the production of such a mul-
tiprocessor concerns the topology of the system, which 
describes the arrangement of the communications paths 
between the individual processors. 
Figures 8.28 to 8.32 depict the five classic MIMD topo-
logies. Multiprocessor structures are described both by their 
topology and by their interconnection level. The level of inter-
connection is a measure of the number of switching units 
through which a message must pass when going from 
processor X to processor Y. The four basic topologies are the 
unconstrained topology, the bus, the ring, and the star, 
although, of course, there are many variants of each of these 
pure topologies. 
The unconstrained topology 
The unconstrained topology is so called because it is a 
random arrangement in which a processor is linked directly 
to each processor with which it wishes to communicate 
(Fig. 8.28(a)). The unconstrained topology is not practicable 
for any but the simplest of systems. As the number of pro-
cessors grows, the number of buses between processors 
becomes prohibitive. Figure 8.28(b) shows the limiting case 
of this topology, called the fully connected topology, because 
each processor is connected to each other processor. The 
advantage of the unconstrained topology system is the very 
high degree of coupling that can be achieved. As all the buses 
are dedicated to communication between only two proces-
sors, there is no conflict between processors waiting to access 
the same bus. 
The bus topology 
The bus (Fig. 8.29) is the simplest of topologies because each 
processor is connected to a single common data highway— 
the bus. The bus is a simple topology; not least because it 
avoids the problem of how to route a message from processor 
X to processor Y. All traffic between processors must use the 

8.5 Microprocessor systems 
357 
5 
o 
r\ 
Bus 
o 
o 
Figure 8.29 The bus topology. 
~5 
6 
Figure 8.30 The ring topology. 
bus. The disadvantage of the bus as a method of implement-
ing a multiprocessor system lies in the problem of controlling 
access to the bus. As only one processor at a time can use the 
bus, it is necessary to design an arbiter to determine which 
processor may access the bus at any time. Arbitration 
between two or more contending processors slows down the 
system and leads to bottlenecks. A bus offers a relatively high 
degree of coupling but is more suitable for schemes in which 
the quantity of data exchanged between the individual 
processors is small. 
The symmetric multiprocessor is a practical realization of a 
bus-based multiprocessor. In a system with symmetric multi-
processing, all processors are of equal priority and each 
processor is given access to the common bus in sequence. 
Symmetric multiprocessing is used on PCs and the Pentium 
family implements mechanisms to support symmetric multi-
processing. However, it is not a scalable technology and the 
symmetric multiprocessors are limited to about eight 
processors. 
The ring topology 
The ring topology of Fig. 8.30 is arranged so that each pro-
cessor is connected only to its two nearest neighbors. One 
neighbor is called the upstream neighbor and the other the 
downstream neighbor. A processor receives information 
from its downstream neighbor and passes it on to its 
upstream neighbor. In this way, information flows round the 
ring in one direction only and a packet of information passes 
through each of the processors in the ring. The information 
Figure 8.31 The star topology. 
passed to a processor contains a destination address. When a 
processor receives a packet, it checks the address and, if the 
packet address corresponds to the processor's own address, 
the processor reads the packet. Similarly, a processor is able to 
add packets of its own to the stream of information flowing 
round the ring. 
The ring topology offers certain advantages for some 
classes of loosely coupled multiprocessor networks and rep-
resents one of the most popular forms of local area network. 
It is less widely used as a method of interconnecting proces-
sors in a tightly-coupled MIMD architecture. A ring network 
is vulnerable to a break in the ring. Some systems employ a 
double ring that does not fail if one of the rings fails. 
The star topology 
The star topology of Fig. 8.31 employs a central processor as 
a switching network, rather like a telephone exchange, 
between the other processors that are arranged logically (if 
not physically) around the central node. The advantage of the 
star is that it reduces bus contention, as there are no shared 
communication paths, moreover, the star does not require 
the large number of buses needed by unconstrained 
topologies. 
On the other hand, the star network is only as good as its 
central node. If this node fails, the entire system fails. 
Consequently, the star topology does not display any form of 
graceful degradation. The central network must be faster 
than the nodes using its switching facilities if the system is to 
be efficient. In many ways, both the ring and the star topo-
logies is are better suited to local area networks, where the 
individual signal paths are implemented by serial data chan-
nels, rather than by the parallel buses of the tightly-coupled 
multiprocessor. 
The hypercube topology 
An n-dimensional hypercube multiprocessor connects 
N = 2" processors in the form of an n-dimensional binary 
cube. Each corner (vertex or node) of the hypercube consists 
of a processing element and its associated memory. Because 
of the topology of a hypercube, each node is directly 
/Central 
node 

358 
Chapter 8 Accelerating performance 
© 
© 
no. 
(a) Hypercube with n= 1. 
(b) Hypercube with n=Z. 
(c) Hypercube with n= 3. 
(d) Hypercube with n = 4. 
Figure 8.32 The hypercube. 
connected to exactly n other neighbors. Figure 8.32 illustrates 
the hypercube for n = 1,2,3, and 4. 
Each processor in a hypercube has an «-bit address in the 
range 0 ... 00 to 1 . . . 11 (i.e. 0 to 2" — 1) and has « nearest 
neighbors with an address that differs from the node's address 
by only 1 bit. If n = 4 and a node has an address 0100, its four 
nearest neighbors have addresses 1100,0000,0110, and 0101. 
A hypercube of dimension n is constructed recursively by 
taking a hypercube of dimension n — 1, prefixing all its node 
addresses by 0, and adding to this another hypercube of 
dimension n — 1 whose node addresses are all prefixed by 1. 
In other words, a hypercube of dimension n can be subdivided 
into two hypercubes of dimension «— 1, and these two sub-
cubes can, in turn, be divided into four subcube of dimension 
n—2 and so on. 
The hypercube is of interest because is has a topology that 
makes it relatively easy to map certain groups of algorithm 
onto the hypercube. In particular, the hypercube is well 
suited to problems involving the evaluation of fast Fourier 
transforms (FFTs)—used in sound and video signal pro-
cessing. The first practical hypercube multiprocessor was 
built at Caltech in 1983. This was called the Cosmic Cube and 
was based on 64 8086 microprocessors plus 8087 floating 
point coprocessors. 
.0110 
0111 
1111 
1110 
0100 
1101 
0101 
1100 
1010 
1011 
1001 
1000 
0011 
J3010 
0001 
0000 
11 
01 
00 
110 
111 
L100 
101 
.011 
pio. 
.000 
001 

8.5 Microprocessor systems 
359 
0 
4 
» 
-Bus A 
6 
-BusB 
[ PE ) 
( PE ) 
(PE 
(a) Twin-bus multiprocessor with a switch between buses. 
• Processing element 
or memory element 
.Bus A 
.BusB 
© 
© 
Q 
^ 
(b) Twin-bus multiprocessor with dual bus access from each processor. 
Hybrid topologies 
In addition to the above pure network topologies, there are 
very many hybrid topologies, some of which are described in 
Fig. 8.33 to 8.36. Figure 8.33(a) and (b) both illustrate the 
dual-bus multiprocessor, although this topology may be 
extended to include any number of buses. In Fig. 8.33(a) the 
processors are split into two groups, with one group con-
nected to bus A and one connected to bus B. A switching unit 
connects bus A to bus B and therefore allows a processor on 
one bus to communicate with a processor on the other. The 
advantage of the dual-bus topology is that the probability of 
bus contention is reduced, because both buses can be oper-
ated in parallel (i.e. simultaneously). Only when a processor 
connected to one bus needs to transfer data to a processor on 
the other does the topology become equal to a single-bus 
topology. 
The arrangement of Fig. 8.33(b) also employs two buses, 
but here each processor is connected directly to both buses 
via suitable switches. Two communication paths always exist 
Figure 8.33 The dual-bus 
multiprocessor. 
between any pair of processors, one using bus A and one 
using bus B. Although the provision of two buses reduces the 
bottleneck associated with a single bus, it requires more con-
nections between the processors and the two buses and more 
complex hardware is needed to determine which bus a 
processor is to use at any time. 
The crossbar network 
Another possible topology, described in Fig. 8.34, is the so-
called crossbar switching architecture, which has its origin in 
the telephone exchange where it is employed to link sub-
scribers to each other. 
The processors are arranged as a single column (processors 
Pcl to PCJ and a single row (processors Prl to Pr„). That is, 
there are a total of m + n processors. Note that the processors 
may be processing elements or just simple memory elements. 
Each processor in a column is connected to a horizontal bus 
and each processor in a row is connected to a vertical bus. 
A switching network, Src, connects the processor on row r to 
PE 
PE 
PE 
PE 
PE 
PE 
PE 

360 
Chapter 8 Accelerating performance 
Figure 8.34 The crossbar switching 
network. 
the processor on column c. This arrangement requires m + n 
switching networks for them + n processors. 
The advantage of the crossbar matrix is the speed at which 
the interconnection between two processors can be set 
up. Furthermore, it can be made highly reliable by 
providing alternative connections between nodes, should 
one of the switch points fail. Reliability is guaranteed only 
if the switches are failsafe and always fail in the off or no-
connection position. 
If the switches at the crosspoints are made multiway 
(vertical to vertical, horizontal to horizontal or horizontal, to 
vertical), we can construct a number of simultaneous path-
ways through the matrix. The provision of multiple pathways 
considerably increases the bandwidth of the system. 
In practice, the crossbar matrix is not widely found in gen-
eral-purpose systems, because of its high complexity. 
Another penalty associated with this arrangement is its 
limited expandability. If we wish to increase the power of the 
system by adding an extra processor, we must also add 
another bus, together with its associated switching units. 
The binary tree 
An interesting form of multiprocessor topology is illustrated 
in Fig. 8.35. For obvious reasons this structure is called a 
binary tree, although I am not certain whether it is really 
a special case of the unconstrained topology of Fig. 8.26, or a 
trivial case of the star topology (using three processors), 
repeatedly iterated! Any two processors (nodes) in the tree 
communicate with each other by traversing the tree right to 
left until a processor common to both nodes is found, and 
then traversing the tree left to right. For example, Fig. 8.35 
shows how processor P0110 communicates with processor 
P0100, by establishing backward links from P0110 to P01 and 
then forward links from P0i to P010 to P0ioo-
The topology of the binary tree has the facility to set up 
multiple simultaneous links (depending on the nature of 
each of the links), because the whole tree is never needed to 
link any two points. In practice, a real system would imple-
ment additional pathways to relieve potential bottlenecks 
and to guard against the effects of failure at certain switching 
points. The failure of a switch in a right-hand column, for 
example, P0010, causes the loss of a single processor, whereas 
the failure of a link at the left-hand side, for example, P0, 
immediately removes half the available processors from the 
system. 
Cluster topology 
Figure 8.36 illustrates the cluster topology, which is a hybrid 
star-bus structure. The importance of this structure lies in its 
application in highly reliable systems. Groups of processors 
and their local memory modules are arranged in the form of 
a cluster. Figure 8.36 shows three processors per cluster in an 
arrangement called triple modular redundancy. The output 
of each of the three processors is compared with the output of 
the other two processors in a voting network. The output of 
the voting circuit (or majority logic circuit) is taken as two 
out of three of its inputs, on the basis that the failure of a 
single module is more likely than the simultaneous failure of 
two modules. 
The design of a clustered triple modular redundancy 
system is not as easy as might be first thought. One of the 
major problems associated with modular redundancy arises 
Row of processors 
© 
©
© 
© 
Column of 
processors 
© 
0 
® 
(9 
Q 
In this example 
processor Pr1 is 
connected to 
processor Pc2 
(£> 
© 
© 
© 
© 
<s>-V€)—©—©—© 
Switch between 
row 1 and column 2 

8.5 Microprocessor systems 
3 6 1 
Processor P0100 
p 
\ 
communicates with 
0100 7 
processor P0110. 
Figure 8.35 The binary tree topology 
Processor 
Bus 
Figure 8.36 The cluster topology. 
from a phenomenon called divergence. Suppose that three 
identical processors have identical hardware and software 
and that they receive identical inputs and start with the same 
initial conditions at the same time; therefore, unless one 
processor fails, their outputs are identical, as all elements of 
the system are identical. 
In actual fact, the above statement is not entirely true. In 
order to create truly redundant systems, each of the three 
processors in a cluster must have its own independent clock 
and I/O channels. Therefore, events taking place externally 
will not be seen by each processor at exactly the same time. If 
these events lead to conditional branches, the operation of a 
processor in the cluster may diverge from that of its neigh-
bors quite considerably after even a short period of opera-
tion. In such circumstances, it becomes very difficult to tell 
whether the processors are suffering from divergence or 
whether one of them has failed. 
The problem of divergence can be eliminated by providing 
synchronizing mechanisms between the processors and by 
comparing their outputs only when they all wish to access the 
system bus for the same purpose. Once more it can be seen 
that, although the principles behind the design of multi-
processor systems are relatively straightforward, their 
detailed practical design is very complex due to a consider-
able degree of interaction between hardware and software. As 
we have already pointed out, topologies for multiprocessor 
systems are legion. 
Coupling 
Up to now we have been looking at the topology of multi-
processor systems with little or no consideration of the nuts 
and bolts of the actual connections between the processors. 
Possibly more than any other factor, the required degree of 
coupling between processors in a multiprocessor system 
determines how the processors are to be linked. A tightly 
coupled multiprocessor system passes data between pro-
cessors either by means of shared memory or by allowing one 
processor to access the other processor's data, address, and 
control buses directly. When shared memory, sometimes 
called dual-port RAM, is employed to couple processors, a 
block of read/write memory is arranged to be common to 
both processors. One processor writes data to the block and 
•. Memory 
(Switch] 
© 
© 
© 
(?) 
(?) 
(?)•*" 
® 
© ©K. 
"{"oooo/ 
OOOIJ 
•yooio) 
V0011I 
vWoJ 
^ 
0101 J 
o 
t Offll 
''(yz. 
<5> 
©c 
o 
\T 
ef 
i£C 
S.-.iti-l-
Sv.i:.h 
p 
p 
p 

362 
Chapter 8 Accelerating performance 
the other reads that data. Data can be transferred as fast as 
each processor can execute a memory access. 
The degree of coupling between processors is expressed in 
terms of two parameters: the transmission bandwidth and the 
latency of the interprocessor link. The transmission band-
width is defined as the rate at which data is moved between 
processors and is expressed in bits/s. For example, if a micro-
processor writes a byte of data to an 8-bit parallel port every 
1 u.s, the bandwidth of the link is 8 bits/1 u-s or 8 Mbits/s. 
However, if a 32-bit port is used to move words at the same 
rate, the bandwidth rises to 32 Mbits/s. 
The latency of an interprocessor link is defined as the time 
required to initiate a data transfer. That is, latency is the time 
that elapses between a processor requesting a data transfer 
and the time at which the transfer actually takes place. A high 
degree of coupling is associated with large transmission 
bandwidths and low latencies. As might be expected, tightly 
coupled microprocessor systems need more complex hard-
ware than loosely coupled systems. 
• 
SUMMARY 
It's taken a concerted attempt to make computers run as fast as 
they do today. This chapter has demonstrated three ways in 
which the performance of the computer has been enhanced 
over the years. We have concentrated on three aspects of 
computer acceleration: pipelining to improve the throughput of 
the CPU, cache memory to reduce the effective access time of 
the memory system, and increased parallelism to improve 
performance without modifying the instruction set 
architecture. 
The movement towards RISC processors in the 1980s was 
driven by a desire to exploit instruction level parallelism by 
overlapping, or pipelining, the execution of instructions. We have 
seen how pipelining can be very effective with an n-stage 
pipeline providing an n-fold increase in performance—in theory. 
In practice, the ultimate performance of pipelined architectures 
is degraded by the branch penalty and data dependency. 
Instructions that alter the flow of control (branches, subroutine 
calls, and returns) throw away the instructions that are already 
in the pipeline. 
The second part of this chapter introduced the cache 
memory, which can radically improve the performance of a 
computer system for relatively little cost. Cache memory uses a 
small amount of high-speed memory to hold frequently used 
data. Cache memory is effective because most programs may 
have large program or data sets but, for 80% of the time, they 
access only 20% of the data. We looked at how the performance 
of cache memory can be calculated and how cache memory is 
organized. We described the three forms of cache organization: 
directly mapped, associative, and set associative. Direct-mapped 
cache is easy to design but is limited by its restriction on what 
data can be stored. Associative memory provides the optimum 
performance in theory but is impossible to construct and you 
have to implement an algorithm to replace old data once the 
cache is full. Set associative cache is only slightly more complex 
than direct-mapped cache and achieves a performance close to 
associative cache. 
The final part of this chapter introduced the multiprocessor, 
which uses two or more computers operating in parallel to 
improve performance.The speedup ratio of a parallel processor 
(i.e. multiprocessor) is the ratio of the time taken by one pro-
cessor to solve a task to the time taken by p processors to 
solve the same task. Ideally the speedup factor is p. However, 
Amdahl's law states that the speedup ratio in a multiprocessor 
system with p processors is given by p/(1 + (p - I)/), where fis 
the fraction of the code that is executed serially. 
We introduced the topology of multiprocessor systems, 
which describes the way the individual processors are inter-
connected. Multiprocessor topologies like the hypercube, the 
crossbar switching network, and the binary tree are well suited 
for solving particular classes of problem. However, a class of 
problem that is well suited to one type of topology may be ill 
suited to a different type of topology. 
* 
PROBLEMS 
8.1 The power of computers is often quoted in MIPS and 
megaflops. Some computer scientists believe that such fig-
ures of merit are, at best, misleading and, at worst, down-
right dishonest. Why? 
Why can't you compare two different processors on the 
basis of their clock speeds? Surely, a processor with a 4 GHz 
clock is twice as fast as a processor with a 2 GHz clock. 
8.2 What are the characteristics of a CISC processor? 
8.3 The most frequently executed class of instruction is the 
data move instruction. Why is this? What are the implica-
tions for computer design? 
8.4 The 68020 microprocessor has a BFFFO (bit field find first 
one) bit-field instruction. This instruction scans a string of 
up to 32 bits at any point in memory (i.e. the string does 
not have to start on any 8-bit boundary) and returns the 
location of the first bit set to 1 in the instruction. For exam-
ple, BFFFO (A0) ( D l : D2}, DO takes the byte at the 
address pointed at by address register A0 and locates the 
start of the bit string at the number of bits in D1 away from 
the most-significant bit at this address. The string, whose 
length is in register D2, is scanned and the location of the 
first 1 is deposited in DO (this is a simplified description of 
the BFFFO instruction). 
In order to demonstrate the complexity of a BFFFO 
instruction, write the equivalent 68K assembly language 
code to implement BFFFO (A0) {Dl:D2},D0. 
8.5 The Berkeley RISC has a 32-bit architecture and yet provides 
only a 13-bit literal.Why is this and does it really matter? 
8.6 What are the advantages and disadvantages of register 
windowing? 
8.7 Some RISC processors with 32 registers, rO to r31, force 
register rO to contain zero. That is, if you read the contents 
of rO, the value returned is always 0. Why have the design-
ers wasted a register by making it read-only and perma-
nently setting its content to 0? 

8.5 Microprocessor systems 
363 
8.8 What is pipelining and how does it increase the 
performance of a computer? 
8.9 Consider the expression 
(A+1)(A+2)(A+B)(A+B+C)(A+B+C+1) 
(A+B+C-1)(D + E)(A-B)(A-B-C) 
Assuming a simple three-operand format with instruc-
tions ADD, SUB, MULT, and DIV (and that all data is in 
registers), write the assembly language code to implement 
this expression with the minimum data dependency 
(assuming that dependency extends to the next 
instruction only). 
8.10 The code of a computer is examined and it is found that, 
on average, for 70% of the time the runlength between 
instructions that change the flow of control is 15 instruc-
tions. For the remainder of the time, the runlength is 6 
instructions. Other cases can be neglected. 
This computer has a five-stage pipeline and no special 
techniques are used to handle branches. 
What is the speedup ratio of this computer? 
8.11 A pipeline is defined by its length (i.e. the number of 
stages that can operate in parallel). A pipeline can be short 
or long. What do you think are the relative advantages of 
long and short pipelines? 
8.12 What is data dependency in a pipelined system and how 
can its effects be overcome? 
8.13 RISC architectures don't permit operations on operands in 
memory other than load and store operations. Why? 
8.14 The average number of cycles required by a RISC to exe-
cute an instruction is given by 
7"ave = 1 + PbPtb = Pet-
where 
the probability that a given instruction is a branch is pt, 
the probability that a branch instruction will be 
taken is pt 
if a branch is taken, the additional penalty is b cycles 
if a branch is not taken, there is no penalty 
pe is the effective probability of a branch (pb-Pt) 
The efficiency of a pipelined computer is defined as the 
average number of cycles per instruction without branches 
divided by the average number of instructions with 
branches.This is given by 1/Tave. 
Draw a series of graphs of the average number of cycles 
per instruction as a function of pe for b = 1,2, 3, and 4. The 
horizontal axis is the effective probability of a branch 
instruction and ranges from 0 to 1. 
8.15 What is branch prediction and how can it be used to 
reduce the so-called branch penalty in a pipelined system? 
8.16 A computer has main memory with an access time of 
60 ns and cache memory with an access time of 15 ns. If 
the average hit ratio is 92%, what is the maximum theo-
retical speedup ratio? 
8.17 A computer has main memory with an access time of 
60 ns and cache memory with an access time of 15 ns.The 
computer has a 50 Mhz clock and all operations require at 
least two clock cycles. If the hit ratio is 92%, what is the 
theoretical speedup ratio for this system? 
8.18 A computer has main memory with an access time of 
60 ns and cache memory with an access time of 15 ns.The 
computer has a 50 Mhz clock and all operations require 
two clock cycles. On average the computer spends 40% of 
its time accessing memory and 60% performing internal 
operations (an internal operation is a non-memory 
access). If the hit ratio is 92%, what is the speedup ratio 
for this system? 
8.19 What is the fundamental limitation of a direct-mapped 
cache? 
8.20 How can the performance of a direct-mapped cache 
memory be improved? 
8.21 A computer has main memory with an access time of 
50 ns and cache memory with an access time of 10 ns.The 
cache has a line size of 16 bytes and the computer's mem-
ory bus is 32 bits wide.The cache controller operates in a 
burst mode and can transfer 32 bytes between cache and 
main memory in 80 ns. Whenever a miss occurs the cache 
must be reloaded with a line. If the average hit ratio is 
90%, what is the speedup ratio? 
8.22 What is cache coherency and why is it important only in 
sophisticated systems? 
8.23 What are the similarities and differences between mem-
ory cache and so-called disk cache? 
8.24 For the following ideal systems, calculate the hit 
ratio (/)) required to achieve the stated speedup 
ratio S. 
(a) tm = 60ns 
t c= 10 ns 
S= 1.1 
(b) tm = 60ns 
t c=10ns 
S= 1.5 
(c) fm = 60 ns 
fc = 10 ns 
5 = 3.0 
(d) tm = 60 ns 
tc = 10 ns 
S = 4.0 
8.25 Draw a graph of the speedup ratio for an ideal system for 
k = 0.5, k = 0.2, k = 0.1 (plot the three lines on the same 
graph). The value of fc defines the ratio of cache to main 
store access times 
(tjtm). 
8.26 What is the meaning of speedup ratio and efficiency in the 
context of multiprocessor systems? 
8.27 In a multiprocessor with p processors, the ideal speedup 
factor is p and the efficiency is 1. In practice, both of these 
ideal values are not achieved. Why? 
8.28 What is Amdahl's law and why is it so important? Is it the 
same as 'the law of diminishing returns'? 
8.29 If a system has 128 processors and the fraction of code 
that must be carried out serially is 0.1, what is the speedup 
ratio of the system? 
8.30 A computer system has 32 microprocessors and the frac-
tion of code that is carried out serially is 5%. Suppose you 
wish to run the same code on a system with 24 proces-
sors. What fraction of the code may be executed serially to 
maintain the same speedup ratio? 

364 
Chapter 8 Accelerating performance 
8.31 In the context of a multiprocessor system, define the 
meaning of the following terms. 
(a) Topology 
(b) Deadlock 
(c) Tightly coupled 
(d) Arbitration 
(e) Latency 
8.32 What are the relative advantages and disadvantages of the 
unconstrained topology, the bus, and the ring multi-
processor topologies? 
8.33 A fully connected multiprocessor topology is one in which 
each of the p processors is connected directly to each of 
the other processors. Show that the number of connec-
tions between processors is given by p(p — 1)/2. 

Processor architectures 
9 Processor 
architectures 
Any c o w i ' on compute; 
a.'chitectuie has to concentrate 
on £• sjn-ffi: miTcnrcr^sici 
bocaose students do-s't haw 
:i!n& to le<irn the details of 
••-teial arch-tortures Wo i.'-od 
the 6f>.< tn leach computer 
architecture because of its 
elegari e md simp'." I:>. Student* 
should appreciate that many 
rcmpiilei architecture-1; are 
avaiir-bi.e Hero we j:mv'dp <tn 
overview of two alternatives to 
the 6SK: a simple roic rocnnlrollei 
found in ct,:is'jire' fircdurts cirri 
a high performance RISC' 
piccwor. 
INTRODUCTION 
When we introduced the CPU and assembly language programming, we were forced to limit the 
discussion to one processor to avoid confusing readers by presenting a range of different processor 
architectures. Because students should at least appreciate some of the differences between 
architectures, we now look at two variations on the von Neumann architecture. We begin with the 
microcontroller, which is a descendant of the first-generation 8-bit microprocessor. Our aim is 
only to provide students with an idea of how the instruction set of a microcontroller differs from 
that of more sophisticated architectures like the 68K. 
The second processor to be introduced here is the ARM, a high-performance RISC processor. This 
device has a 32-bit architecture with the characteristics of traditional RISC processors like MIPS 
but which has some very interesting architectural facilities. We look at the ARM in greater detail 
than the microcontroller and provide development tools on the disk accompanying this text. 
This chapter is not essential reading in for all computer architecture courses—but it is worth 
skimming though just to appreciate some of the architectural differences between processors. 
9.1 Instruction set architectures 
and their resources 
All mainstream computer architectures have remarkably simi-
lar architectures. Differences between families are often a matter 
of detail rather than substance. Before we look at competing 
families, we list some of the differences between processors. 
9.1.1 Register sets 
Microprocessors use registers to hold temporary data. The 
more registers you have, the less you need to access slower 
external memory. More registers require more silicon to 
implement them and more bits to specify them in an 
instruction. Consequently, early microprocessors had very 
few registers. Modern processors such as Intel's Itanium have 
| Chapter 6 shows how 
[6 Assembly language 
(programming 
! Chapter 6 shows how 
! machine-level instructions are 
!'• used to construct entire 
f programs. We introduce the 
programming environment via a 
simulator that runs on a PC and j 
demonstrate how to implement 
basic algorithms in assembly 1 
language. 
7 Structure of the CPU 
Here, we show how a computer 
is organized internally.and how 
it reads instructions from 
memory; decodes them, and 
executes them. 
8 Enhancing 
; performance 
Chapter 8 describes some of the 
techniques we can use to make 
the computer work faster. We 
examine how the performance 
of computers has been enhanced 1 
by pipelining, the use of cache 
memory, and parallel processing. I 

366 
Chapter 9 Processor architectures 
WHY DO WE HAVE DIFFERENT PROCESSOR FAMILIES? 
Life would be simpler if there were only one microprocessor. 
All software would be compatible across all platforms and 
manufacturers would compete on price and performance 
rather than the architecture itself. 
Different professor families exist for various reasons. First, 
competing companies working independently of each other 
developed the microprocessor. Trade secrets and product 
confidentiality guaranteed that processors from, for example, 
Intel, Motorola, and Tl would be different. Developments in 
technology also help create diversity. First-generation 
processors were 8-bit machines because it was not 
economically feasible to create 16- and 32-bit processors in 
the 1970s. Technological progress forced companies to 
jump from 8-bit processors to 16-bit processors. Some 
companies moved from register-to-memory architectures to 
register-to-register architectures to exploit developments in 
computer organization such as pipelining. Finally, economics 
required that some processors be cheap and cheerful 
whereas others could command premium prices because of 
their power. 
large register sets (the Itanium has 128 general-purpose 
64-bit registers). 
Processors with few registers use special instructions to 
indicate the register such as LDX or LDY to specify load the X 
or Y register. Processors with many registers number their 
registers sequentially and implement instructions such as 
ADD R l , R 2 , R 3 . 
9.1.2 Instruction formats 
We have already seen that computer architectures can be 
specified by the number of operands used by the instruction; 
for example, 
or more of the operands must be a register; mat is, memory-
to-memory operations are not generally permitted. 
The three-address format is used by RISC processors such 
as MIPS, the PowerPC, and the ARM. Real processors require 
three register addresses. Typically, the only memory accesses 
permitted by RISC processors are load and store. 
9.1.3 Instruction types 
The instruction sets of most computers are very similar. All 
processors have arithmetic, logical, shift, data transfer, data 
movement, and program flow control instructions. Some 
processors have a richer instruction set than others; for 
Format 
Zero address 
One address 
Two address 
Three address 
Typical operation 
ADD 
ADD P 
ADD 
R l , P 
ADD 
R 1 , R 2 , R 3 
Description 
Pull the top two words off the stack, add them, and push the result. 
Add the contents of memory location P to the accumulator 
Add the contents of memory location P to register Rl 
Add the contents of register R2 to register R3 and put the result in register Rl 
The zero address format doesn't require operand addresses 
because operations are applied to the element or elements 
at the top of the stack. This format is used only by some 
calculators designed for arithmetic operations and some 
experimental processors; for example, performing the opera-
tion (Y + Z) -X might be implemented by the following 
hypothetical code 
example, a processor may not include a multiplication 
instruction, which means that you have to write a routine to 
perform multiplication by shifting and adding. 
Some processors have instructions that are not necessary; 
for example, one processor might implement CLR DO to load the 
contents of DO with 0, whereas another processor may require 
you to write MOVE # 0, DO or SUB DO, DO to do the same thing. 
Push x 
Save X on the stack 
Push Y 
Save Y on the stack (stack = Y, X) 
Push z 
Save Z on the stack (stack = Z,Y,X) 
ADD 
Pull top two elements Z and Y, add them, and push the result Y+Z (stack = Y+Z, X) 
MUL 
Pull top two elements Y+Z and X, multiply them and push the result (Y+Z) • X. 
The one-address format was adopted by 
first-generation 
processors and is still used by microcontrollers. 
The two-address instruction format is used by the main-
stream CISC processors such as the 68K or the Pentium. This 
is often called a 'one-and-a-half-address' format because one 
The trend to complex instruction sets in the 1980s led to 
instructions such as the 68020's B F F F O which scans a sequence 
of bits and returns the location first bit that was set to 1. Such 
complex instructions appeared to be dying out with the advent 
of the high-speed, streamlined RISC architectures of the 1980s. 

9.2 The microcontroller 
367 
MICROCONTROLLER FAMILIES 
High-performance microcomputers are like jet aircraft; 
their development costs are so high that there are relatively few 
different varieties.The same is not true of microcontrollers and 
there are more members of microcontroller families than 
varieties of salad dressing. Because microcontrollers are a very-
low-cost circuit element, they have been optimized for very 
specific applications. You can select a particular version of a 
microcontroller family with the RAM, ROM, and I/O you require 
for your application. 
The generic Motorola microcontroller families are as follows. 
6800 This was the original 8-bit Motorola microprocessor.The 
6800 is not a microcontroller because it lacks internal 
memory and peripherals. 
In recent years, the trend towards simplicity has reversed 
with the advent of the so-called SIMD (single instruction, 
multiple data) instruction. Such an instruction acts on multi-
ple data elements at the same time; for example, a 64-bit 
register may contain eight 8-bit bytes that can be added in 
parallel to another eight bytes at the same time. These 
instructions are used widely in multimedia applications 
where large numbers of data elements representing sound or 
video are processed (e.g. Intel's MMX extensions). 
9.1.4 Addressing modes 
An important element of an instruction set is the addressing 
mode used to access operands. First-generation microproces-
sors used absolute addressing, literal addressing, and indexed 
(register indirect) addressing. The generation of 16- and 
32-bit CISC processors widened addressing, modes by 
providing a richer set of indexed addressing modes; for exam-
ple, the 68K provided autoincrementing with (A0)+, and 
double indexing with a literal displacement with 12 (A0, DO). 
9.1.5 On-chip peripherals 
Microprocessor families also differ in terms of the facilities 
they offer. The processor intended for use in workstations or 
high-end PCs is optimized for performance. Peripherals such 
as I/O ports and timers are located on the motherboard. 
A microcontroller intended of use in an automobile, cell 
phone, domestic appliance, or toy is a one-chip device that 
contains a wealth of peripherals as well as the CPU itself. For 
example, a microcontroller may contain an 8-bit CPU, ran-
dom access memory, user-programmable read-only mem-
ory, read/write RAM, several timers, parallel and serial I/O 
devices, and even analog-to-digital converters. The micro-
controller can be used to implement a complete computer 
system costing less than a dollar. 
6805 Motorola's 6805 was their first microcomputer with an 
architecture very similar to the 6800. It was initially aimed at 
the automobile market. 
68HCllThe 68HC11 is one of the best-selling 
microcontrollers of all time. It has a 6800-like architecture but 
includes ROM, RAM, and peripherals. 
68HC12 The 68HC12 is an extension of the 68HC11. It has 
more instructions, enhanced addressing modes and some 
16-bit capabilities. 
68HC16 The 68HC16 has a 16-bit architecture 
and is an enhanced 68HC12 rather than a new 
architecture. 
I 
We look at two processors. We begin with the 68HC12 
, 
microcontroller to demonstrate what an embedded con-
troller looks like. Then we introduce the ARM, a RISC 
t 
processor with a simple instruction set and some interesting 
l 
architectural features. 
r 9.2 The microcontroller 
One of the first major competitors to Intel's 8080 8-bit 
microprocessor was the Motorola 6800, which has a signifi-
cantly simpler register model than the 8080. The 6800 has a 
J 
single 8-bit accumulator and 16-bit index register, which lim-
its its performance because you have to load the accumulator, 
I 
perform a data operation, and then store the result before you 
I 
can reuse the accumulator. 
f 
First-generation microprocessors had 16-bit program 
counters that supported only 64 kbytes of directly address-
i 
able memory. Although 64 kbytes is tiny by today's 
standard's, in the mid-1970s 64 kbytes was considered as 
positively gigantic. 
Motorola later introduced the 6809, an architecturally 
advanced 8-bit processor, to overcome the deficiencies of the 
s 
6800. Unfortunately, the 6809 appeared just as the 68K was 
r 
about to enter the market; few wanted a super 8-bit processor 
i 
when they could have a 16- or 32-bit device.1 
Motorola created a range of microcontrollers aimed at the 
II 
low-cost high-volume industrial microcontroller market. We 
t 
are going to describe the architecture of the popular 8-bit 
' 'Better late than never'. No way! Motorola seems to have been very 
unlucky. The high-performance 68K with a true 32-bit architecture lost 
out to Intel's 16-bit 8086 when IBM adopted the Intel architecture 
because IBM couldn't wait for the 68K. Similarly, the 6809 appeared just 
as the world of high-performance computing was moving from 8 bits to 
16/32 bits. 

368 
Chapter 9 Processor architectures 
M68HC12, which is object code compatible with Motorola's 
8-bit MC68HC11 but with more sophisticated addressing 
modes and 16-bit arithmetic capabilities. 
Before we look at microcontroller register sets, we will say 
a few words about one of the differences between the Intel-
style processors and Motorola-style processors. The bits of an 
instruction are precious, because we wish to cram as many 
different instructions into an 8-bit instruction set as possible. 
Intel processors reduce the number of bits required to specify 
a register by using dedicated registers. For example, if arith-
metic operations can be applied only to one accumulator, it's 
not necessary to devote op-code bits to specifying the register. 
On the other hand, this philosophy makes life difficult for 
programmers who have to remember what operations can be 
applied to what registers. Moreover, programmers with lim-
ited registers have to spend a lot of time moving data between 
registers. 
Motorola-style processors employed fewer specialized 
registers than Intel-style processors. This approach reduced 
the number of different instructions, because more bits have 
to be devoted to specifying which register is to take part in an 
operation. Equally, it makes it easier to write assembly 
language code. 
9.2.1 TheM68HC12 
In this chapter we are interested in the instruction set 
architecture of microprocessors. The MC68HC12 microcon-
troller family provides an interesting contrast with the 68 K 
because of its simplicity and its similarity to first-generation 
8-bit microprocessors. We are not able to discuss the 
microprocessor's most important features—its on-chip 
peripherals that let you implement a complete computer in 
one single chip. Figure 9.1 provides an indication of the 
- PADO 
- PAD1 
- PAD2 
- PAD3 
- PAD4 
- PAD5 
• PAD6 
• PAD7 
• PTO 
• PT1 
• PT2 
• PT3 
• PT4 
• PT5 
• PT6 
• PT7 
' PSO 
• PS1 
• PS2 
• PS3 
• PS4 
- PS5 
• PS6 
• PS7 
PPO 
PP1 
PP2 
PP3 
PP4 
PPS 
PP6 
PP7 
RxCAN 
• TxCAN 
PCAN2 
PCAN3 
• PCAN4 
PCAN5 
• PCAN6 
Figure 9.1 TheMC68HC12 
structure. 
V f p 
• 
32-KBYTE FLASH EEPROM/ROM 
1-KBYTE RAM 
768-KBYTEPROM 
CPU 12 
BKGD *—\ 
SMOON/TAGHI 
1
P E R I O D I C INTERRUPT 
SINGLE-WIRE 
CCPWATCHDOC 
 
BACKGROUND 
CLOCK MONITOR 
DEBUG MODULE 
| 
BREAKPOINTS 
EXTAL 
• 
XTAL 
•* 
RESET 
*-*\ 
PEO 
• ! 
I 
•XIRQ 
PE1 
• 
— • IRQ/Vpp 
PE2 
+ - * 
^ 
*-*• 
R/W 
LITE 
PE3 
•»-»• 
H- *-*• 
LSTRB/TAGLO 
INTEGRATION 
PE4 +-* 
o 
« - * ECLK 
MODULE 
PES *~• 
°" <—• PIPEO/MODA 
( y M ) 
PE6 * - * 
* - • PIPE1/M0DB 
PE7 
•*—•! 
r*~• DBE 
V D D x 2 — • 
I 
| 
VssX2^ 
t m m t 
m t m i 
POWER FOR 
MULTIPLEXED ADDRESS/DATA BUS 
INTERNAL 
I 
- 
I 
iimni immt 
V 
* 2 
• 
DDRA 
DDRB 
VDDX 
c 
 
v s s x * 2 
~^\_ 
PORTA 
PORTB 
ESSJS* timm mum 
h--*i>LO ,*rO(NJ<— O 
N ID I T f 
f l N r- 
O 
a . a . a . a . a . a . a . a . 
0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 
i n f 
f f l N i - O 
_ 
„ 
r - T - r - r - r - i - O I 
CO 
r * - . l D U 1 f m f M i — 
O 
a a i X Q i a i c t t t 
cc 
a:occ£a:a:(£c£C£: 
Q Q Q Q Q O Q Q 
0 0 0 0 0 0 0 0 
O O O O O O O O 
0 0 0 0 0 0 0 0 
<<<<<<<< 
<<<<<<<< 
WIDE 
< < < < < < < < 
< < < < < < < < 
BUS 
< < < < < < < < 
< < < < < < < < 
0 0 0 0 0 0 °
Q 
0 0 0 0 0 0 0 0 
s toifl f m M t- o 
< < < < < < < < 
»— 1—»— I— h— I— H-*— 
<<<<<<<< 
o o a 
Q a 
Q a o 
NARROWBUS 
VRH 
* 
V R H 
V R t 
< 
V R L 
V D D A 
•* 
VfjrjA 
VSSA \* 
V S S A 
ANO 
+ 
< 
PADO 
AN1 
A 
< 
PAD1 
™„ 
r L ™ 
A N 2 
A 
Q 
* 
P A D 2 
CONVERTER 
A N 3 
« 
< < _ 
p A D 3 
AN4 
< 
g 
< 
PAD4 
AN5 
< 
a. 4 
RAD5 
AN6 
4 
-* 
PAD6 
AN7 
* 
« 
PAD7 
v- 
IOC0 •*—*' 
•*—*- PTO 
IOC1 •*—> 
•«—•+ PT1 
TIMER AND 
IOC2 < ^ » 
- 
<—+ PT2 
PULSE 
CC7 
I O C 3 * " " * g 
£ 
* ~ ~ * P T 3 
A m M . A T n D 
IOC4 + — • g 
O 
«—*• PT4 
ACCUMULATOR 
| Q C 5 J # _ > . " 
°- ^ _ ^ 
p T 5 
IOC6 « — • 
* — • PT6 
L- 
PAI •*—*• 
< — • PT7 
q r , 
RxO «—» 
* — • PSO 
_ 
TxO_*—» 
•*—»• PS1 
I/O 
l / 0 
~ 
< ^ P S 2 
I/O 
« — • v, 
*" - < — • PS3 
i 
1 
^ 
| _ 
I 
1 
" 
o 
SDI/MISO «—» ° 
a! 4 — • PS4 
c-p, 
SDO/MOSI «—» 
« — • PS5 
SCK « — • 
<—» PS6 
CSSS < — • 
< — • PS7 
PWO 
*—• 
< — • PPO 
PW1 
* — • 
* — • PP1 
PWM 
PW2 
« — • 
« — • PP2 
PW3 
« — • £ 
£ < — • PP3 
Q 
O 
I/O 
4 — • 
=- « — • pp4 
I/O 
<—• 
< — • PPS 
1 / 0 
I/O 
<—• 
<—• PP6 
I/O 
<—• 
< — • PP7 
™c ^ 
RXCAN |- 
J H ^ 
^ 
TxCAN | 
• 
• TxCAN 
I 
I 
z 
I/O 
<—• 
J 
< — • PCANi 
I/O 
<—• i 
K - < — • PCAN; 
I/O 
I/O 
« — • ^ 
O •<—• PCAN^ 
I/O 
«—• g 
"• < — • PCAN! 
I/O 
<—» 
< — • pcANe 

9.2 The microcontroller 
369 
WHAT'S IN A NAME? PART I 
You access variable locations in memory by means of a 
pointer-based addressing mode. The register that supplies 
the address of an operand was called a modifier register. 
Some microprocessors call these registers index register, 
the term used by Motorola in its microcontroller 
literature. 
The 68K family provides eight 32-bit pointer registers 
called address registers, although their function is identical to 
that of index registers. The 68k family has an addressing mode 
that uses the sum of two pointer registers to provide an 
effective address; for example, MOVE (AO , D2), D7. 
Motorola calls this addressing mode Indexed addressing. 
15 
CCR 
15 
Accumulator D 
0 
Accumulator A 
Accumulator B 
7 
0 7 
0 
Index register X 
15 
0 
index register Y 
15 
0 
Stack pointer 
15 
0 
Program counter 
The 68HC12 has two 8-bit accumulators 
that can be concated to create a single 
Q
1 16-bit D-register. 
Q1 The two 16-bit X and Y registrs are pointer 
registers (called index registers) that are 
userd to access momory. 
The 16 bit stack pointer is used to manage 
the system stack holds subroutine return 
"Q 
and interrupt retur addresses. 
The 16-bit program counter contains the 
address of the next instruction to be 
0 
executed. 
The 8-bit condition code register contains 
the processor's staus flags (e.g. N-, C-, V-, 
0 
and N-bits) 
Figure 9.2 The M68HC12 register set. 
number of accesses the M68HC12 has 
to make to the main store. The two 
8-bit accumulators, A and B, can be 
concatenated to create a single 16-bit 
data accumulator D, where D(0:7) = A, 
D(8:i5> = B- Because the D register can 
take part in 16-bit operations (albeit to 
limited extent), it can be used for some 
of the applications that would nor-
mally require a 16-bit pointer register 
(e.g.X,Y,SP). 
We've covered sufficient computer 
architecture and assembly language to 
write a simple program in M68HC12 
code that you can follow with little dif-
ficult. In what follows, A and B are 8-bit 
registers and X is a 16-bit pointer. 
The following are some M68HC12 
instructions. 
Assembly form 
LDX #P 
STX P 
LDAB P 
LDAA 0,X 
INX 
STAA 0,X 
DECB 
BNE P 
Description in words 
Load X register with a literal 
Store X register in P 
Load B register 
Load A register indexed 
Increment X register by 1 
Store A register indexed 
Decrement B register 
Branch on not zero 
RTL description 
[X] t -
P 
[P] <-
[X] 
[B] «-
[P] 
[A] <-
[ [ X ] ] 
[X] <— [X] + 1 
[ I X ] ] 
<- 
[A] 
[B] <— [B] - 
1 
I F 
[Z] =1 [PC] 
«-
[PC] 
+ P 
MC68HC12's capabilities. The microcontroller contains 
both read-write memory for scratchpad storage and flash 
EPROM to hold programs and fixed data. There is a wealth of 
input and output ports, serial interfaces, and counter timers. 
Such a chip can be used to control a small system (e.g. a 
digital camera or a cell phone) with very little additional 
hardware. 
Figure 9.2 illustrates a user register model of the 
M68HC12 microcomputer family. Unlike the first 8-bit 
processors, it has two general-purpose 8-bit accumulators A 
and B. The inclusion of a second accumulator reduces the 
Note how the 8-bit mnemonic combines the operation 
and the operand; for example, INX increments the X register 
and INCA and INCB increment the A and B accumulators. 
The M68HC12 uses load and store mnemonics to move data 
between memory and registers, rather than the 68K's more 
generic MOVE instruction; for example, LDX and STX load and 
store data in the X register. 
Eight-bit code uses variable-length instructions. Common 
operations like INCA are encoded as a single byte. The 
equivalent operation, ADDA # 1, takes two bytes—one for the 
op-code and one for the literal. 

370 
Chapter 9 Processor architectures 
WHAT'S IN A NAME? PART II 
Modem RISC processors and the 68K family have lots of 
internal registers that use sequential alpha-numeric names 
such as D0-D7,A0-A7,and R0-R31. 
Microcontrollers have few general-purpose registers and use 
special names; for example, the A and B accumulators or the X 
and Y index registers.The operation load accumulatorA might be 
written LDAA and load accumulator B might be written LDAB. 
Operations such as increment and decrement are imple-
mented by the 68K as additions and subtractions by ADD 
# 1 , DO or SUB #4, A2. Microcontrollers avoid an explicit lit-
eral by using increment and decrement; for example, I N X 
increments the contents of the M68HC12's X index register. 
Typical M68HC2 mnemonics that apply to a specific 
register are 
I N X 
increment X register 
DEX 
decrement X register 
LDX 
load X register 
STX 
store X register 
LDS 
load stack pointer 
TXS 
transfer X register to stack pointer 
INY 
increment Y register 
DEY 
decrementY register 
LDY 
load Y register 
STY 
store Y register 
STS 
store stack pointer 
TSX 
transfer stack pointer to X register 
Assembly language 
RTL form 
Comment 
LDAA #d8 
[A] <r-
d8 
LDAA EA 
[A] <r-
[EA] 
LDAA d8,X 
[A] <— [[X] + d8] 
LDAA dl6,X 
[A] <— [[X] + dl6] 
LDAA B,X 
[A] <— [[X] + [B]] 
LDAA D,X 
[A] <— [[X] + [D]] 
LDAA i,-x 
[X] <— [X]-l; [A] <-
[[X]] 
LDAA i,x-
[A] 4-
[[X]] ; [X] <-[X]-l 
LDAA 1,+X 
[X] «— [X]+l; [A] <-
[[X]] 
LDAA 1,X + 
[A] i— [[X]]; [X] <-
[X]+l 
LDAA 
[dl6,X] 
[A] <— [[dl6 + [X]]] ; 
LDAA 
[D,X] 
[A] <r-
[[[D] + [X]]] ; 
Literal addressing 
Absolute addressing 
Indexed addressing with an 8-bit offset 
Indexed addressing with a 16-bit offset 
Indexed addressing with a variable 8-bit offset 
Indexed addressing with a variable 16-bit offset 
Indexed addressing with predecrement 
Indexed addressing with postdecrement 
Indexed addressing with preincrement 
Indexed addressing with postincrement 
Memory indirect addressing with index register and offset 
Memory indirect addressing with two index registers 
Table 9.1 The M68HC11 's indexed addressing modes (you can replace A by B and X by Y). 
Consider the following fragment of M68HC12 code, 
which uses two pointers, the X and Y registers, to copy a string 
from its source to its destination. 
of source> 
destination> 
ytes to move> 
TABLE1 EQU 
<address 
TABLE2 EQU 
<address 
N 
EQU 
<number o 
LDX 
tTABLEl 
LDY 
#TABLE2 
LOOP 
LDAA 0,X 
STAA 
INX 
INY 
0,Y 
CPX 
#TABLE1+N 
BNE 
LOOP 
X points to source string [X] «— Tablel 
Y points to destination string. 
Read source byte, store it in accumulator 
Copy accumulator A to destination 
Update X pointer 
Update Y pointer 
Test for end of loop 
This fragment of code is not too difficult to understand. All 
you have to remember is that one of the operands is specified 
by the mnemonic (e.g. A, B, X, or Y registers). 
M68HC12 addressing modes 
The M68HC12 has 16-bit pointer registers and can address up to 
64 kbytes of memory. Memory addressing modes are absolute 
(the operand is specified by a 
16-bit address), literal (either 
an 8-bit or a 16-bit offset), and 
indexed. Table 9.1 illustrates 
the M68HC12's 
addressing 
modes. 
The M68HC12's indexed 
addressing modes are partic-
ularly extensive. Like the 
68K, indexed addressing with 
both a literal and a variable 
offset is supported. Auto-incrementing and -decrementing 
modes are provided; indeed both pre- and postdecrementing 
and pre- and postincrementing modes are supported. Note 
that the increment size is user selectable in the range 1 to 8. 

9.2 The microcontroller 
371 
Unusually, the MC68HC12 provides a memory indirect 
addressing mode. The operation LDDA [12, x] adds 12 to 
the contents of the X register to create a pointer. The memory 
location at that address is accessed to yield a second 16-bit 
pointer. This pointer is used to access the target operand. 
Memory indirect addressing allows you to index into a table 
of pointers which can be useful when implementing two-
dimensional data structures. 
The M68HC12 has a full complement of pointer register 
transfer instructions. Pointer registers can be pushed on the 
stack or pulled off it. The stack operation uses a 2-byte 
instruction, the second byte specifying the list of registers to 
be pulled or pushed. It is also possible to push multiple regis-
ters on the stack with one instruction. 
M68HC12 instruction set 
Table 9.2 lists some of the M68HC12's mnemonics and indi-
cates the actions carried out by the instructions (note that M 
is a memory location). All we need here is an indication of the 
types of operation performed by these computers. The 
M68HC12 microprocessor is reasonably complete in its data 
movement, arithmetic, logical, shift, and branch instructions. 
Complex arithmetic operations such as multiply or divide 
except for the M68HC12's unsigned 8-bit multiply instruc-
tion are absent. 
Eight-bit microprocessors offer very effective ways of 
manipulating 
character-oriented 
data, 
implementing 
input/output 
operations, 
and 
designing 
embedded 
Operation 
RTLdefin tion 
Mnemonic 
Arithmetic group 
tion 
Add M to A 
[A] <— [A] + [EA] 
ADDA 
Add M to A 
[B] <— [B] + [EA] 
ADDB 
Add M to D 
[D] <— [D] + [EA] 
ADDD 
Add B to X 
[X] <— [X] + [B] 
ABX 
AddBtoY 
[Y] <— [Y] + [B] 
ABY 
Add B to A 
[A] <-
[A] + [B] 
ABA 
Add M to A with carry 
[A] <— [A] + [EA] + 
[C] 
ADCA 
Add M to B with carry 
[B] <-
[B] + [EA] + 
[C] 
ADCB 
Subtract M from A 
[A] <— [A] -
[EA] 
SUBA 
Subtract M from B 
[B] <-
[B] -
[EA] 
SUBB 
Subtract M from D 
[D] <-
[D] -
[EA] 
SUBD 
Subtract B from A 
[A] <— [A] -
[B] 
SBA 
Subtract M from A with carry 
[A] «— [A] -
[EA] 
- 
[c] 
SBCA 
Subtract M from B with carry 
[B] <— [B] -
[EA] 
" 
[ C ] 
SBCB 
Clear M 
[EA 
<- 0 
CLR 
Clear A 
[A] 
i— 0 
CLRA 
Clear B 
[B] 
«- 0 
CLRB 
Negate M 
[EA 
<" o -
[EA] 
NEG 
Negate A 
[A] 
<- o -
[A] 
NEGA 
Negate B 
[B] 
<- o -
[B] 
NEGB 
Multiply A by B 
[D] 
«-
[A] 
X 
[B] 
MUL 
Compare A with M 
[A] -
[EA] 
CMPA 
Compare B with M 
[B] -
[EA] 
CMPB 
Compare D with M 
[D] -
[EA] 
CMPD 
Compare A with B 
[A] -
[B] 
CBA 
TestM 
[EA] 
-
0 
TST 
Test A 
[A] -
0 
TSTA 
TestB 
[B] -
0 
TSTB 
Sign extend [B] into [D] 
I F 
[B 7] = 1 THEN 
SEX 
[A (0:7) 1 _ 1 1 1 1 1 1 1 1 
ELSE [A (0:7) 1 = 0 
Table 9.2 (Continues) 

372 
Chapter 9 Processor architectures 
Table 9.2 {Continued) 
Operation 
RTL definition 
[EA] <— [EA] - 1 
[A] <- [A] - 1 
[B] <— [B] - 1 
[S] <— [S] - 1 
[X] <— [X] - 1 
[Y] «- [Y] - 1 
[EA] «- [EA] + 1 
[A] <— [A] + 1 
[B] <— [B] + 1 
[S] <— [S] + 1 
[X] «— [X] + 1 
[Y] «— [Y] + 1 
[A] . [EA] 
[B] . [EA] 
[A] <— [A] . [EA] 
[B] <~ [B] . [EA] 
[EA] «- [EA] 
[A] <— [A] 
[B] <- [B] 
[A] <- [A] 0 [EA] 
[B] <— [B] 0 [EA] 
[A] <- [A] + [EA] 
[B] <— [B] + [EA] 
Mnemonic 
Decrement and increment group 
Decrement M 
Decrement A 
Decrement B 
Decrement S 
Decrement X 
Decrement Y 
Increment M 
Increment A 
Increment B 
Increment S 
Increment X 
IncrementY 
Logical group 
Bit A with M 
Bit B with M 
AND A with M 
AND B with M 
Complement M 
Complement A 
Complement B 
EOR A with M 
EOR B with M 
OR A with M 
OR B with M 
Shift and rotate group 
Arithmetic shift M left 
Arithmetic shift A left 
Arithmetic shift B left 
Arithmetic shift M right 
Arithmetic shift A right 
Arithmetic shift B right 
Logical shift M left 
Logical shift A left 
Logical shift B left 
Logical shift M right 
Logical shift A right 
Logical shift B right 
Rotate M left 
Rotate A left 
Rotate B left 
Rotate M right 
Rotate A right 
Rotate B right 
DEC 
DECA 
DECB 
DES 
DEX 
DEY 
INC 
INCA 
INCB 
INS 
INX 
INY 
BITA 
BITB 
ANDA 
ANDB 
COM 
COMA 
COMB 
EORA 
EORB 
ORAA 
ORAB 
ASL 
ASLA 
ASLB 
ASR 
ASRA 
ASRB 
LSL 
LSLA 
LSLB 
LSR 
LSRA 
LSRB 
ROL 
ROLA 
ROLB 
ROR 
RORA 
RORB 

Table 9.2 (Continued} 
9.2 The microcontroller 
373 
Operation 
RTL definition 
Mnemonic 
Data movement group 
Exchange register pair 
[ R i ] <-
[ R j ] ; 
[ R j ] 
<-
[ R i ] 
EXG 
Load A with M 
[A] 
<-
[EA] 
LDA 
Load B with M 
[B] 
<— [EA] 
LDB 
Load D with M 
[D] 
<— [EA] 
LDD 
Store A in M 
[EA] <-
[A] 
STA 
Store B in M 
[EA] <— [B] 
STB 
Store D in M 
[EA] «-
[D] 
STD 
Transfer regj to regi 
[ R i ] <-
[ R j ] 
TFR 
Transfer B to A 
[A] 
<— [B] 
TAB 
Transfer A to B 
[B] 
<— [A] 
TBA 
Push A on system stack 
[SP] <-
[ S P ] - 1 ; [ [ S P ] ] <- 
[A] 
PSHSA 
Push B on system stack 
[SP] «-
[ S P ] - 1 ; [ [ S P ] ] <- 
[B] 
PSHSB 
Push register list on system stack 
PSHS 
Push register list on user stack 
PSHU 
Pull A off system stack 
[A] 
<— [ [ S P ] ] ; [ S P ] 
<-
[ S P ] + 1 
PULSA 
Pull B off system stack 
[B] 
<-
[ [SP] ] ; [SP] 
<-
[ S P ] + 1 
PULSB 
Pull register list off system stack 
PULS 
Pull register list off user stack 
PULU 
Branch group 
Branch on equal 
I F 
[ Z ] = 0 THEN [ P C ] < - | ;PC] + T 
BEQ 
Branch on not equal 
I F 
[ Z ] = 1 THEN 
[ P C ] < - | [PC] + T 
BNE 
Branch unconditionally 
[PC] «-
[PC] 
+ 
a d d r e s s 
BRA 
Branch to subroutine 
[SP] 
i S- 
[ S P J - 2 ; 
[ [ S P ] ] 
<- 
[PC] ; 
[PC] 
«- 
[ P C ] + d 8 
BSR 
Jump 
[PC] 
6— a d d r e s s 
JMP 
Jump to subroutine 
[SP] 
e- 
[ S P ] - 2 ; 
[ [ S P ] ] 
<- [PC] ; 
[PC] 
t - 
a d d r e s s 
JSR 
Return from interrupt 
R T I 
Return from subroutine 
[PC] <— [ [ S P ] ] ; 
[SP] 
<- 
[ S P ] + 2 
RTS 
Pointer register load, store, and manipulation group 
Add A to X 
[X] 
<— [X] 
+ 
[ A ] 
ABX 
Decrement X 
[X] 
<— [X] 
- 
1 
DEY 
Decrement stack pointer 
[S] 
<— [ S ] 
- 
1 
DES 
Increment Y 
[X] 
<— [X] 
+ 
1 
INX 
Increment X 
[ Y ] 
<-
[Y] 
+ 
1 
INY 
Increment stack pointer 
[S] 
<— [S] 
+ 
1 
INS 
Load X with M 
[X] 
<— [EA] 
LDX 
Load Y with M 
[Y] 
<-
[EA] 
LDY 
Load stack pointer with M 
[SP] <-
[EA] 
LDS 
Load user SP with M 
[U] 
<— [EA] 
LDU 
Store X in M 
[EA] <— [X] 
STX 
Store Y in M 
[EA] <— [ Y ] 
STY 

374 
Chapter 9 Processor architectures 
Table 9.2 (Continued) 
Operation 
RTL definition 
Mnemonic 
Store stack pointer in M 
[EA 
^ 
[ S ] 
STS 
Store user SP in M 
[EA 
<- [U] 
STU 
Transfer SP to M 
[X] 
<- [ S ] 
TSX 
Transfer X to SP 
[S] 
<r- [ X ] 
TXS 
Transfer A to X 
[X] 
<- [ A ] 
TAX 
Transfer X to A 
[A] 
<- [ X ] 
TXA 
Transfer A to Y 
[Y] 
<- [ A ] 
TAY 
Transfer Y to A 
[A] 
<- [ Y ] 
TYA 
Load X with EA 
[X] 
<- EA 
LEAX 
Load Y with EA 
[Y] 
<- EA 
LEAY 
Load system SP with EA 
[S] 
<- EA 
LEAS 
Load user SP with EA 
[U] 
<- EA 
LEAU 
Compare X with M 
[X] 
- [EA] 
CMPX 
Compare Y with M 
[Y] 
" [EA] 
CMPY 
Compare system SP with M 
[S] 
- [EA] 
CMPS 
Compare user SP with M 
[U] 
- IEA] 
CMPU 
CCR manipulation group 
Clear carry 
[C] 
<- 0 
CLC 
Set carry 
[C] <- 1 
SEC 
Clear interrupt 
[ I ] 
<- 0 
C L I 
Set interrupt 
[ I ] 
<- 1 
S E I 
Clear overflow 
[V] <- 0 
CLV 
Set overflow 
[V] <~ 1 
SEV 
Set decimal mode 
SED 
Clear decimal mode 
CLD 
TransferAtoCCR 
[CCR] <- [ A ] 
TAP 
Transfer CCR to A 
[A] 
4 - [CCR] 
TPA 
AND CCR with M 
[CCR] «- [CCR] AND [EA] 
ANDCC 
OR CCR with M 
[CCR] <- [CCR] OR [EA] 
ORCC 
Push CCR 
PHP 
Pull CCR 
PLP 
Table 9.2 Summary of the MC68HC12 instruction set. 
controllers. They are rather less effective when asked to per-
form numeric operations on floating point data or when they 
attempt to compile programs 
in 
modern 
high-level 
EMAXM 0 , X 
[ [X] ] <-
languages. 
EMAXD 0 , X 
[D] <-
The MC68HC12's instruc-
tion set is conventional with 
just a few special instructions intended to accelerate some 
applications; for example, instructions are provided to 
extract the maximum and minimum of two values. Consider 
the following example, which compares the memory location 
pointed at by the X register and the unsigned contents of the 
16-bit D register and puts the larger value in the memory 
location or D register. 
max([[X]],[D]) 
max([[X]],[D]) 
maximum value in memory 
maximum value in D register 
Similarly, EMIND 0, x puts the lower of the memory loca-
tion and the D register in the D register. 
Consider the following fragment of code, which uses the 
8-bit signed minimum function and indexed addressing with 
postincrementing to find the minimum value in a four-
element vector. 

9.3 TheARM—an elegant RISC processor 
375 
LDX #List ;X register points to List 
LDAA #$FF ;dummy minimum in A register 
MINA 1,X+ ;min(A,Listl) in A 
MINA 1,X+ ,-min(A,List2) in A 
MINA 1,X+ ;min(A,List3) in A 
MINA 1,X+ ;min(A,List4) in A. We now have the smallest element 
Sample MC68HC12 code 
In many ways, MC68HC12 code is not too dissimilar to 68K 
code; it's just rather more verbose. The lack of on-chip registers 
means that you have to frequently load one of the two accumu-
lators from memory, perform an operation, and then restore 
the element to memory. The following example demonstrates a 
program to find the maximum of a table of 20 values. 
develop further generations of RISC processors (called the 
ARM family). In 1998 the company was floated on the stock 
market and became ARM Ltd. We are going to use the ARM 
processor to illustrate the RISC philosophy because it is easy to 
understand and it incorporates some interesting architectural 
features. 
EQU 
LDAA 
STAA 
LDX 
LDAB 
LDAA 
CMPA 
BLE 
LDAA 
STAA 
TsTLast DEX 
DBNE 
Next 
64 
#0 
Maximum 
#Table+N-l 
#N-1 
Maximum 
0,X 
TstLast 
0,x 
Maximum 
B,Next 
Maximum db 0 
Table 
db 7,2,5,3, 
size of table 
clear A 
set up dummy maximum value of 0 
X points to the end of the table 
register B is the element counter set to count down 
get the current largest value 
compare maximum with tablei 
IF new not bigger then test for end 
ELSE update maximum 
Decrement table pointer 
Decrement loop count in B, branch if not zero 
memory location to hold array max 
dummy data 
The following code presents the core of a 68K version of 
this program. Notice that it is more compact. 
CLR 
DO 
LEA 
Table,A0 
MOVE.B 
#N-1,D1 
Next 
CMP.B 
DO,(A0)+ 
BLE 
TsTLast 
MOVE.B 
-l(AO),D0 
TsTLast DBRA 
Dl,Next 
DO contains the maximum - preset to 0 
A0 points to table 
Dl is the loop counter (set to size - 1) 
Test the next element 
If not larger than check for end 
IF bigger then record new largest element 
Repeat until all done 
9.3 TheARM—an elegant RISC 
processor 
One of the strongest arguments made by the supporters of 
RISC processors in the 1980s was that they were easy to design 
and fabricate. From 1983 onward Acorn Computers created a 
series of chips solidly based on RISC principles, which 
demonstrated the proof of this statement. The same company 
later set up Advanced RISC Machines, of Cambridge, UK, to 
9.3.1 ARM's registers 
Like most mainstream RISC architectures, the ARM is a 
32-bit machine with a register-to-register, three-operand 
instruction set. First-generation ARMs supported 32-bit 
words and unsigned bytes, whereas later members of the 
ARM family provide 8-bit signed bytes and 16-bit signed 
and unsigned half-words. The ARM processor doesn't 
implement delayed branches and therefore the instruction 
following a branch is not automatically executed. 

376 
Chapter 9 Processor architectures 
SHADOW REGISTERS 
When an interrupt occurs, a processor is forced to suspend the 
current program and to carry out a task defined by the 
operating system. This means that registers being used by the 
pre-interrupt program might be overwritten by the 
interrupting program. 
A simple solution is for interrupt handlers to save data in 
registers, use the registers, and then restore the data before 
returning from the interrupt. This process takes time. 
All operands are 32 bits wide, except for some multiplication 
instructions that generate a 64-bit product in two registers, and 
byte and halfword accesses (64-bit and halfword accesses are 
available only on some members of the ARM family). The 
ARM has 16 user-accessible general-purpose registers called rO 
to rl5 and a current program status register (CPSR), that's 
similar to the condition code register we've described earlier. 
The ARM doesn't divide registers into address and data 
registers like the 68K—you can use any register as an address 
register or a data register. Most 32-bit RISC processors have 
32 general-purpose registers, which require a 5-bit operand 
field in the instruction. By reducing the number of bits in an 
instruction used to specify a register, the ARM has more bits 
available to select an op-code. The ARM doesn't provide lots 
of different instructions like a CISC processor. Instead, it pro-
vides flexibility by allowing instructions to do two or more 
things at once (as we shall soon see). In some ways, the ARM 
is rather like a microprogrammed CPU. 
The ARM's registers are not all general purpose because 
two serve special purposes. Register rl5 contains the program 
counter and register rl4 is used to save subroutine return 
addresses. In ARM programs you can write pc for rl5 and lr 
(link register) for rl4. 
Because rl 5 is as accessible to the programmer as any other 
register, you can easily perform computed gotos; that is, 
MOV pc, r i o forces a jump to the address in register rlO. 
By convention, ARM programmers reserve register rl3 as a 
stack pointer, although tJiat is not mandatory. 
The ARM has more than one program status register 
(CPSR—see Fig. 9.3). In normal operation the CPSR 
contains the current values of the condition code bits 
(N, Z, C, and V) and eight system status bits. The I and F bits 
are used to disable interrupt requests and fast interrupt 
requests, respectively. Status bits MO to M4 indicate the 
processor's current operating mode. The T flag is imple-
mented only by the Thumb-compatible versions of the ARM 
family. Such processors implement two instruction sets, the 
32-bit ARM instruction set and a compressed 16-bit Thumb 
instruction set.2 
When an interrupt occurs, die ARM saves die pre-exception 
value of the CPSR in a stored program status register (there's 
one for each of the ARM's five interrupt modes). 
Some devices like the ARM provide shadow registers. These 
are copies of a register that are associated with an 
interrupt. When an interrupt occurs and the processor 
handles it, an old register is 'switched out' and a new one 
switched in. When the interrupt has completed, the old 
register is switched in again. In this way, no data has to be 
moved. 
The ARM runs in its user mode except when it switches to 
one of its other five operating modes. These modes corres-
pond to interrupts and exceptions and are not of interest to 
us in this chapter. Interrupts and exceptions switch in new 
rl3 and rl4 registers (the so-called fast interrupt switches in 
new r8 to rl4 registers as well as rl3 and rl4). When a mode 
switch occurs, registers rO to rl2 are unmodified. For our cur-
rent purposes we will assume that there are just 16 user-
accessible registers rO to rl5. Figure 9.3 describes the ARM's 
register set. 
The current processor status register is accessible to the pro-
grammer in all modes. However, user-level code can't modify 
the I, F, and MO to M4 bits (this restriction is necessary to 
enable die ARM to support a protected operating system). 
When a context switch occurs between states, the CPSR is 
saved in the appropriate SPSR (savedprocessor state register). 
In this way, a context switch does not lose die old value of 
the CPSR. 
Summary of the ARM's register set 
• The ARM has 16 accessible 32-bit registers called rO to rl5. 
• Register r 15 acts as the program counter, and r 14 (called the 
link register) stores die subroutine return address. 
• You can write PC for rl5 in ARM assembly language, lr for 
rl4, andspforrl3. 
• By convention, register rl3 is used as a stack pointer. 
However, there is no hardware support for die stack 
pointer. 
• The ARM has a current program status register (CPSR), 
which holds condition codes. 
• Some registers are not unique because processor exceptions 
create new instances of rl3 and rl4. 
• Because die return address is not necessarily saved on the 
stack by a subroutine call, die ARM is very fast at imple-
menting subroutine return calls. 
As most readers will have read the chapter on the CISC 
processor and are now familiar with instruction sets and 
2 The ARM's Thumb mode is designed to make the processor look 
like a 16-bit device in order to simplify memory circuits and bus design 
in low-cost applications such as cell phones. 

9.3 The ARM—an elegant RISC processor 
3 7 7 
User 
registers 
Supervisor 
Abort 
registers 
registers 
Undefined 
Interrupt 
Fast interrupt 
registers 
registers 
registers 
rO 
rO 
rO 
rO 
rO 
rO 
r1 
rl 
r1 
r1 
r1 
rl 
r2 
r2 
r2 
r2 
r2 
r2 
r3 
r3 
r3 
r3 
r3 
r3 
r4 
r4 
r4 
r4 
r4 
r4 
r5 
r5 
r5 
r5 
r5 
r5 
r6 
r6 
r6 
r6 
r6 
r6 
r7 
r7 
r7 
r7 
r7 
r7 
r8 
r8 
r8 
r8 
r8 
r8_FIQ 
r9 
r9 
r9 
r9 
r9 
r9_FIQ 
r19 
r19 
r19 
r19 
r19 
r19_FIQ 
r11 
r11 
r l l 
rll 
r l l 
r11_FIQ 
r12 
r12 
r12 
r12 
r12 
r12_FIQ 
r13 
r13_SCV 
r!3_abort 
r13_undef 
r13_1RQ 
r!3_FIQ 
r14 
r14_SCV 
r14_abort 
r14_undef 
r14_lRQ 
r14_FIQ 
r15=PC 
r15=PC 
r15=PC 
r15=PC 
r!5=PC 
r15=PC 
Gray registers 
are banked 
Processor status register 
31 30 29 28 27 26 25 24 23... 
9 
8 7 
6 
5 4 
3 
2 
1 0 
N Z 
C V 
1 F T M4 M3 M2 MO M1 
Condition 
flag bits 
Interrupt control bits 
Figure 9.3 The ARM's register set 
addressing modes, we provide only a short introduction to the 
ARM's instruction set before introducing some of its develop-
ment tools and constructing simple ARM programs. 
9.3.2 ARM instructions 
The basic ARM instruction set is not, at first sight, exciting. A 
typical three-operand register-to-register instruction has the 
format 
ADD 
r l , r 2 , r 3 
and is interpreted as [ r 1 ] <- [ r2 ] + [r3 ]. Note the order 
of the operands—the destination appears first (left to right), 
then the first source operand, and finally the second source 
operand. Table 9.3 describes some of the ARM's data process-
ing instructions. 
The ARM has a reverse subtract; for example, SUB r l , r 2 , 
r 3 is defined as [ r l ] <- [r2] - [r 3], whereas the reverse 
[ R J <_ [ R J X [RJ + [ R J 
subtract operation RSB r l , r 2 , r 3 is defined as [rl]<-
[r3] - [r2]. A reverse subtract operation is useful because 
you can do things like 
SUB r l , r 2 , # 5 
;[rl] = [r2]-5 
RSB r l , r 2 , # 5 
;[rl] = 5-[r2] 
The bit clear instruction BIC performs the operation 
AND NOT so that BIC 
r l , r2, r3 is defined as [rl] 
<-[r2]-[r3]. Consider the effect of BIC 
r l , r 2 , r 3 on 
operands [r2] = 11001010 and [r3] = 11110000. The 
result loaded into r l is 00001010 because each bit in the sec-
ond operand set to 1 clears the corresponding bit of the first 
operand. 
The multiply instruction, MUL, has two peculiarities. First, 
the destination (i.e. result) register must not be the same as 
the first source operand register; for example, MUL rO, r 0, 
r l is illegal whereas MUL rO, r l , rO is legal. Second, the MUL 
instruction may not specify an immediate value as the second 
operand. 
The multiply and accumulate instruction MLA performs a 
multiplication and adds the result to a running total. It has 
the four-operand form MLA Rd, Rm, Rs, Rn. The RTL defini-
tion of MLA is 
The result is truncated to 32 bits. 
note how the normal order of the source operands is reversed 

378 
Chapter 9 Processor architectures 
Mnemonic 
Operation 
Definition 
ADD 
Add 
[Rd] <- Op1 + Op2 
ADC 
Add with carry 
[Rd] <- Op1 + Op2 + C 
SUB 
Subtract 
[Rd] <- Op1 - Op2 
SBC 
Subtract with carry 
[Rd] <- Op1 - Op2 + C - 1 
RSB 
Reverse subtract 
[Rd] <- Op2 - Op1 
RSC 
Reverse subtract with carry 
[Rd] <- Op2 - Op1 + C - 1 
MUL 
Multiply 
[Rd] <- Opl x Op2 
MLA 
Multiply and accumulate 
[Rd] <- Rm x Rs + Rn 
AND 
Logical AND 
[Rd] <- Opl A Op2 
ORR 
Logical OR 
[Rd] <- Opl v Op2 
EOR 
Exclusive OR 
[Rd] <- Opl © Op2 
BIC 
Logical AND NOT 
[Rd] <- Op1 A Op2 
CMP 
Compare 
Set condition codes on Op1 — Op2 
CMN 
Compare negated 
Set condition codes on Opl + Op2 
TST 
Test 
Set condition codes on Op1 A Op2 
TEQ 
Test equivalence 
Set condition codes on Op1 © Op2 
MOV 
Move 
[Rd] <- Op2 
MVN 
Move negated 
[Rd] <- Op2 
LDR 
Load register 
[Rd] <- [M] 
STR 
Store register 
[M] <- [Rd] 
LDM 
Load register multiple 
Load a block of registers from memory 
STM 
Store register multiple 
Store a block of registers in memory 
SWI 
Software interrupt 
[PC] <— [r!4], [PC] «- 8, enter supervisor mode 
Table 9.3 The ARM data processing and data move instructions. 
The ARM has two compare instructions. The conventional 
CMP Rn, Rs evaluates [Rn] - [Rd] and sets the condition 
codes in the CPSR register. The compare negated instruc-
tion, CMN Rn, Rs, also performs a comparison, except that 
the second operand is negated before the comparison 
is made. 
The ARM has a test equivalence instruction, TEQ Rn, Rs, 
which tests whether two values are equivalent. If the two 
operands are equivalent, the Z-bit is set to 1. This instruction 
is very similar to the CMP, except that the V-bit isn't modified 
by a TEQ. 
The test instruction, TST Rn, Rs, tests two operands by 
ANDing their operands bit by bit and then setting the condi-
tion code bits. The TST instruction allows you to mask out 
bits of the operand you wish to test. For example, if rO con-
tains 0 . . . 00011112, the effect of TST r l , rO is to mask the 
contents of rl to four least-significant bits and then to 
compare those bits with 0. 
The ARM's built-in shift mechanism 
ARM data processing instructions can combine an arith-
metic or logical operation with a shift operation. The shift is 
applied to operand 2 rather than the result. For example, the 
ARM instruction 
ADD r l , r 2 , r 3 , 
LSL #4 
shifts the 32-bit operand in register r3 left by four places 
before adding it to the contents of register r2 and depositing 
the result in register rl. In RTL terms, this instruction is 
defined as 
[ r l ] <- [r2] + [r3] x 16 
Figure 9.4 illustrates the format of a data processing 
instruction. As you can see, the encoding of an ARM instruc-
tion follows the general pattern of other RISC architectures: 
an opcode, some control bits, and three operands. Operands 
Rn and Rd specify registers. Operand 2 in bits 0 to 11 of the 

93 The ARM—an elegant RISC processor 
379 
EXPLICIT CONDITION CODE REGISTER UPDATING 
The 68K automatically updates the CCR register after most 
operations. The ARM and several other RISC processors allow 
the programmer to update the condition codes only when 
needed. 
If an ARM instruction has the suffix'S', the CPSR is 
updated—otherwise it is not; for example, ADDS r 3 , r 1, r2 
the condition code register, CPSR. 
adds r1 to r2, puts the result in r3, and sets the condition code 
flags accordingly. 
However, ADD r 3 , r l , r2 performs exactly the same 
addition but does not update the condition codes in the CPSR. 
Bit 20 of an instruction, the S-bit, is used to force an update of 
31 
28 27 26 25 24 
21 20 19 
16 IB 
12 11 
cond 
0 0 # 
op-code 
Rn 
Rd 
If bit 25 is zero 
operand 2 is a 
shifted register. 
operand 2 
7 6 
5 
4 3 
0 
number of shifts 
shifts 
0 
Rm 
Figure 9.4 Format of the 
ARM's data processing 
instructions. 
op-code in Fig. 9.4 selects either a third register or a literal. 
The ARM's designers use this field to provide a shift function 
on all data processing instructions. 
When bit 25 of an op-code is 0, operand 2 both selects a 
second operand register and a shift operation. Bits 5 to 11 
specify one of five types of shift and the number of places to 
be shifted. The shifts supported by the ARM are LSL (logical 
shift left), LSR (logical shift right), ASR (arithmetic shift 
right), ROR (rotate right), and RRX (rotate right extended by 
one place). The RRX shift is similar to the 68K's ROXL (rotate 
right extended) in which the bits are rotated and the carry bit 
is shifted into the vacated position. These shifts are similar to 
the corresponding 68K shifts and are defined as: 
shifts the second operand in r3 three places left to multiply it 
by 8. This value is added to operand 1 (i.e. r3) to generate 
8 X R3 + R3 = 9 X R3. However, instructions such as 
ADD r 3 , r 3 , r3 , LSL #3 take an extra cycle to complete 
because the ARM can read only two registers from the regis-
ter file in a single cycle. 
This ability to scale operands is useful when dealing with 
tables. Suppose that a register contains a pointer to a table 
of 4-byte elements in memory and we wish to access ele-
ment number i. What is the address of element i? The 
address of the ith element is the pointer plus 4 X i. If we 
assume that the pointer is in register rO and the offset is in 
rl, the pointer to the required element, r2, is given by 
LSL 
LSR 
ASL 
ASR 
ROR 
RRX 
The operand is shifted left by 0 to 31 places. The vacated bits at the least-significant end of the 
operand are filled with zeros. 
The operand is shifted right 0 to 31 places. The vacated bits at the most-significant end of the 
operand are filled with zeros. 
The arithmetic shift left is identical to the logical shift left. This multiplies a number by 
2 for each shift. 
The operand is shifted right 0 to 31 places. The vacated bits at the most-significant end of the 
operand are filled with zeros if the original operand was positive, or with Is if it was negative (i.e. the 
sign-bit is replicated). This divides a number by 2 for each place shifted. 
The operand is rotated by 0 to 31 places right. The bit shifted out of the least-significant end is 
copied into the most-significant end of the operand. This shift preserves all bits. No bit is lost by the 
shifting. 
The operand is rotated by one place right. The bit shifted out of the least-significant end of the 
operand is shifted into the C-bit. The old value of the C-bit is copied into the most-significant end of 
the operand; that is, shifting takes place over 33 bits (i.e. the operand plus the C-bit). 
You can use this shifting facility to perform clever short 
cuts; for example, suppose you want to multiply the contents 
[rO] + 4 X [rl ]. In ARM assembly language, we can load 
of r3 by 9. The operation 
t h i s P o i n t e r i n t o r 2 by 
ADD r 3 , r 3 , r 3 , LSL #3 
A D D r 2 , r 0 , r l , LSL #2 
[rO] 
4 
rl 
In ARM assembly 
this pointer into r2 by 
ADD r 2 rO r l 
LSL 
•2 
11 
0 
0 

380 
Chapter 9 Processor architectures 
We have been able to scale the offset by 4 (because 
each integer requires 4 bytes) before adding it to rO in a 
conventional way. This instruction performs the operation 
[r2] <- [rO] + [rl] X 4. 
The ARM permits dynamic shifts in which the number of 
places shifted is specified by the contents of a register. In this 
case the instruction format is similar to that of Fig. 9.4, except 
that bits 8 to 11 specify the register that defines the number of 
shifts, and bit 4 is 1 to select the dynamic shift mode. If regis-
ter r4 specifies the number of shifts, we can write 
ADD r l , r 2 , r 3 , LSL r4 
which has the RTL definition [rl] «- [r2] + [r3] x 2|ri| 
How do you shift an operand itself without using a data 
processing operation such as an addition? You can apply a 
shift to the source operand of the move instruction; for 
example, 
MOV rO,rl,LSL #2 
MOV rO,rl,LSL #6 
MOV rO,rl,ASR #2 
shift the contents of r1 left twice and 
multiply [r1] by 64 and copy result to 
divide [r1] by 4 and copy result to rO 
The 16 conditions described in Table 9.4 are virtually the 
same as those provided by many other microprocessors. One 
condition is the default case always and means that the cur-
rent instruction is to be executed. The special case never is 
reserved by ARM for future expansion and should not be 
used. In order to indicate the ARM's conditional mode to the 
assembler, all you have to do is to append the appropriate 
condition to a mnemonic. Consider the following example in 
which the suffix EQ is appended to the mnemonic ADD to get 
ADDEQ r l , r 2 , r 3 
The addition is now performed only if the Z-bit in the CPSR 
is set. The RTL form of this operation is 
IF Z = 1 THEN [rl] <- [r2] + [r3] 
Consider the high-level expression 
IF x = y THEN p = q + r 
If we assume, that x, y, p, q, and r are in 
copy result to rO 
registers rO, rl, r2, r3, and r4, respectively, 
we can express this algorithm as 
CMP 
r O , r l 
ADD;:j r 2 , r 3 , r 4 
9.3.3 ARM branch instructions 
One of the ARM's most interesting features is that each 
instruction is conditionally executed. Bits 28 to 31 of each 
ARM instruction provide a condition field 
CMP 
r g 
rx 
that defines whether the current instruc- 
ADDEQ r 2 , r 3 , 
tion is to be executed—see Table 9.4. 
SUBLS r 2 , r 3 . 
The ARM's ability to make the execution of each instruction 
conditional makes it easy to write compact code. Consider 
die following extension of the previous example 
/compare x and y 
r4 
;IF x = y THEN p = 
r4 
; 
ELSE IF 
q + r 
x < y THEN p = q - r 
Op-code bits 31-28 
Mnemonic prefix 
Condition 
Flags 
0000 
EQ 
equal 
Z = 1 
0001 
NE 
not equal 
Z = 0 
0010 
CS/HS 
carry set/higher or same 
C = 1 
0011 
CC/LO 
carry clear/lower 
C = 0 
0100 
M I 
negative 
N = 1 
0101 
PL 
positive or zero 
N = 0 
0110 
VS 
overflow set 
V = 1 
0111 
VC 
overflow clear 
V = 0 
1000 
H I 
higher than (signed) 
( C = 1 ) ( Z = 0) 
1001 
LS 
lower or same (signed) 
(C = 0) + (Z = 1) 
1010 
GE 
greater than or equal (signed) 
N = V 
1011 
LT 
less than (signed) 
N =£V 
1100 
GT 
greater than (signed) 
(Z = 0 ) ( N = V ) 
1101 
LE 
less than or equal (signed) 
(Z=1) + (NV) 
1110 
AL 
always (default) 
don't care 
1111 
NV 
never (do not use) 
none 
Table 9.4 The ARM's condition codes. 

9.3 The ARM—an elegant RISC processor 
381 
There is, of course, nothing to stop you combining condi-
tional execution and shifting because the branch and shift 
fields of an instruction are independent. You can write 
ADDCC r l , r 2 , r 3 LSL r4 
which is interpreted as 
IF C = 0 THEN [rl] 
[r2] + [r3] x 2' 
The following example from Steve Furber demonstrates 
the ARM's ability to generate very effective code for the 
construct 
IF (a = b) AND (c = d) 
THEN e f- e + 1; 
Assume that a is in register rO, b is in register rl, c is in reg-
ister r2, d is in register r3, and e is in register r4. 
c CMP 
rO,rl 
( CMPL\.. r2,r3 
! ADD:-'.; r4,r4,#l 
Compare a and b 
If a = b THEN compare c 
if c = d then increment 
In this example, the first instruction, CMP rO, r l , compares 
a and b. The next instruction, CMPEQ r2, r 3, performs a com-
parison only if the result of the first line was true (i.e. a = b). 
8-bit literal is N and the 4-bit alignment is n in the range 0 to 12, 
the value of the literal is given by N X 22". Note that the scale 
factor is In. This mechanism is, of course, analogous to the way 
in which floating point numbers are represented. Scaling is 
performed automatically by the assembler. If you write 
ADD rl,r2,#65536 
This assembler deals with the out-of-range literal by scaling 
it. That is, the assembler converts a literal into an alignment 
and a literal (when that is possible). 
Summary of data processing instructions 
The ARM's instruction set is both simple and powerful. It's sim-
ple because instructions are regular and the instruction set is 
very small. The instruction set is powerful because you can 
apply three attributes to each instruction. You can 
and d 
choose whether to update the condition codes by 
e by 1 
appending an S to the op-code. You can make the 
instruction's execution conditional by appending 
the condition to the instruction. You can specify a register or a 
literal as operand 2 and then shift the operand before it is used. 
Consider the following examples: 
Op-code 
Operation 
ADD 
r l , r 2 , r 3 
[ r l ] 
<- 
[r2] 
+ [r3] 
ADDS 
r l , r 2 , r 3 
[ r l ] 
<- 
[r2] 
+ [r3] , update flags 
ADDEQ 
r l , r 2 , r 3 
IF Z = 1 THEN [ r l ] 
<- [r2] 
+ 
[ r 3 
ADDEQS r l , r 2 , r 3 
IF Z = 1 THEN [ r l ] 
<- [r2] 
+ 
[ r 3 
ADD 
r l , r 2 , r 3 , 
LSL 12 
[ r l ] 
<- 
[r2] 
+ [r3] 
x 4 
ADD 
r l , r 2 , r 3 , 
LSL 
r4 
[ r l ] 
<- 
[r2] 
+ [ r 3 ] 
x 2 l r 4 ] 
ADD 
r l , r 2 , 
#125 
[ r l ] 
<- 
[r2] 
+ 125 
ADD 
r l , r 2 , 
#0xFF00 
[ r l ] 
<- 
[r2] 
+ 255 * 28 
ADDCSS r l , r 2 , r 3 , 
LSL 
r4 
IF C = 1 THEN [ r l ] 
<- [r2] 
+ 
[ r 3 
update flags 
] x 2 tr4] 
update flags 
The third line, ADDEQ r4, r 4 , # 1 , is evaluated only if the 
previous line was true (i.e. c = d). The third line adds the literal 
1 to r4 to implement the e <- e + 1 part of the expression. 
9.3.4 Immediate operands 
ARM instructions can specify an immediate operand as well as 
a register. Figure 9.5 demonstrates how an immediate operand 
is encoded. When bit 25 of an instruction is 0, the ARM spec-
ifies a register for use as operand 2. When bit 25 is 1, the 12-bit 
operand 2 field could provide a 12-bit literal. But it doesn't. 
Those who designed the ARM argued 
that range is more important than preci-
sion and provided an 8-bit literal in the 
range 0 to 255 that can be scaled to pro-
vide a 32-bit value. 
In Fig. 9.5 the four most-significant bits 
of the operand 2 field specify the literal's 
alignment within a 32-bit frame. If the 
9.3.5 Sequence control 
The ARM implements a conventional branching mechanism 
using the conditions described in Table 9.4. For example, the 
instruction BEN LOOP forces a branch if the Z-bit of the con-
dition code register (i.e. CPSR) is clear. The branch instruc-
tion is encoded in 32 bits, which includes an 8-bit op-code 
and a 24-bit signed offset, which is added to the contents of 
the program counter. The signed offset is a 26-bit value, 
which is stored as a word offset in 24 bits because ARM 
31 2827 26 25 24 
2120 19 
16 15 
12 11 
cond 0 0 # 
op-cone S 
Rn 
Rd 
operand 2 
If bit 25 is one 
operand 2 is a 
shifted register. 
87 
alignment 
8-bit literal 
Figure 9.5 Format of the ARM's instructions with immediate operands. 
11 
0 
0 
LJi 

382 
Chapter 9 Processor architectures 
instructions are word aligned on a 32-bit 
boundary. 
Consequently, the byte and halfword parts of the offset do 
not have to be stored as they will always be zero. 
The simple unconditional branch has the single-letter 
mnemonic B, as the following demonstrates 
B 
Next 
; b r a n c h t o 
" N e x t " 
You can implement a loop construct in the following way 
Because 
the branch 
with 
link 
instruction 
can 
be 
made conditional, the ARM implements a full set of 
conditional subroutine calls. You can write, for example, 
CMP 
r 9 , r 4 
; i f 
r9 < r4 
BLLT ABC 
; t h e n c a l l 
s u b r o u t i n e ABC 
The 
mnemonic 
BLLT 
is made 
up 
of 
B 
(branch 
unconditionally), L (branch with link), and L T (execute on 
condition less than). 
MOV 
r 0 , # 2 0 
Next 
; l o a d t h e l o o p c o u n t e r 
rO w i t h 
; b o d y of 
l o o p 
20 
SUBS 
BNE 
r O , r 0 , # l 
Next 
9.3.6 Data movement and 
memory reference 
instructions 
This fragment of code is exactly like that of many CISC 
processors, but note that you have to explicitly update the 
condition codes when you decrement the loop counter with 
SUBS r O , r 0 , # 1 . 
The ARM also implements a so-called branch with link 
instruction, which is similar to the subroutine call. A branch 
operation can be transformed into a 'branch with link' instruc-
tion by appending L to its mnemonic. Consider the following 
BL 
Next 
The ARM copies the program 
counter held in register r l 5 into 
the link register rl4. That is, 
the branch with link preserves 
the return address in rl4. We 
can express this instruction in 
RTLas 
;decrement loop counter 
;repeat until loop count = zero 
ARM processors support register-to-reg-
ister operations and load and store operations between regis-
ters and memory. The ARM implements two instructions that 
copy data from one register to another (or a literal to a regis-
ter). MOV r i , r j copies the contents of register r, into register 
ri.MVN r i , r j copies the logical complement of the contents of 
register r; into register r,. The logical complement is calculated 
by inverting its bits (i.e. it's the one's complement rather than 
the arithmetic two's complement). 
The MOV instruction can be used conditionally and combined 
with a shifted literal. Consider the following examples. 
] <r- 0; Clear rO 
] <- [ r l ] 
* 16 
Z = 0 THEN [r3] 
<- 
[ r 2 ] / 3 2 
] <- [ r l ] 
* 16; update condition codes 
] <- - 1 ; the 1's complement of 0 is 111... 1 
] <- [ r O ] ; complement the bits of rO 
1 <- OxFFFFFFFO 
,-branch to "Next" with link 
MOV 
r0,#0 
; [rO 
MOV 
rO,rl, LSL #4 
; [rO 
MOVNF. r3,r2, ASR #5 
;IF 
MOVS 
r0,rl, LSL #4 
; [rO 
MVN 
r0,#0 
; [rO 
MVN 
r0,r0 
; [rO 
MVN 
r0,#0x F 
; [rO 
[rl4] <— [PC] 
;copy program counter to link register 
[rl5] <r- Next 
;jump to "Next" 
The value loaded into r l 4 is actually [PC] - 4 because 
the PC actually points to the instruction being fetched into 
the pipeline, rather than the instruction being currently 
executed. 
A return from subroutine is made by copying the saved 
value of the program counter to the program counter. You 
can use the move instruction, MOV, to achieve this: 
The ARM provides a move instruction 
that lets you examine or modify the con-
tents of the current processor status register 
(CPSR). The operation MRS Rd, CPSR copies the value of 
the CPSR into general register Rd. Similarly, MSR_f CPSR, 
Rm copies general register Rm into the CPSR (note that bits 
28,29, 30, and 31 of the CPSR holds the V, C, Z, and N flags, 
respectively). This instruction is privileged and can't be exe-
cuted in the user mode (to prevent users changing to a pri-
MOV 
P C , r l 4 
vileged mode). 
; c o p y r l 4 t o r l 5 
( r e s t o r e t h e p r o g r a m 
c o u n t e r ) 
Because the subroutine return address is stored in r l 4 
rather than on the stack in external memory, the ARM's sub-
routine call and return mechanism is very fast indeed. 
However, you have to be careful not to accidentally overwrite 
the return address in rl4. Moreover, if a subroutine calls 
another subroutine, you have to save the previous return 
address in r!4 on the stack. 
Loading an address into a register 
Up to now, we have assumed that an address is already in a 
register. We cannot load a 32-bit literal value into a register 
because 32-bit literals aren't supported and the ARM doesn't 
implement multiple-length instructions. However, we can load 
an 8-bit literal shifted by an even power of 2 into a register. 
The ARM assembly language programmer 
can use the ADR 
(load address into register) instruction to load a register with 

9.3 TheARM—an elegant RISC processor 383 
a 32-bit address; for example, 
ADR 
r O , t a b l e 
loads the contents of register rO with the 32-bit address 
' t a b l e ' . The ARM assembler treats the ADR as a pseudo 
instruction and then generates the code that causes the appro-
priate action to be carried out. The ADR instruction attempts 
to generate a MOV, MVN, ADD, or SUB instruction to load the 
address into a register. 
Fi gure 9.6 demonstrates how the ARM assembler treats an 
ADR instruction. We have used ARM's development system to 
show the source code, the disassembled code, and the regis-
ters during the execution of the program (we'll return to this 
system later). As you can see, the instruction ADR r 5 , t a b l e l 
has been assembled into the instruction ADD r 5 , p c , 0 x l 8 , 
because t a b l e l is 1816 bytes onward from the current con-
tents of the program counter in rl5. That is, the address 
t a b l e l has been synthesized from the value of the PC plus 
the constant 1816. 
The ARM assembler also supports a similar pseudo opera-
tion. The construct LDR rd, = value is used to load value 
into register rd. The LDR 
„„ 
A 
• i 
L D R r O , [ r l ] 
;load rO w 
pseudo instruction uses 
„„„ „ 
, 
' 
. , 
STR r2,[r3] /store the 
the MOV or MOV instructions, or it places the constant in 
memory and uses program counter relative addressing to 
load the constant. 
Accessing memory 
The ARM implements two flexible memory-to-register and 
register-to-memory data transfer operations, LDR and STR. 
Figure 9.7 illustrates the structure of the ARM's memory ref-
erence instructions. Like all ARM instructions, the memory 
access operations LDR and STR have a conditional field and 
can, therefore, be executed conditionally. 
The ARM's load and store instructions use address register 
indirect addressing to access memory. ARM literature refers to 
this as indexed addressing. Any of the ARM's 16 registers can 
act as an address (i.e. index) register. 
Bit 20 of the op-code determines whether the instruction is 
a load or a store, and bit 25, the # bit, determines the type 
of the offset used by indexed addressing. Let's look at some of 
the various forms of these instructions. Simple versions of the 
load and store operations that provide indexing can be written 
ith the word pointed at by rl 
word in r2 in the location pointed at by r3 
^B^MfSssmSM^s^^&m^^^KSmi^MSi 
Ble 
Ejciii Se&rch View Execute Qftfions ^mdo# fcjelp 
^1 -al I IP! M 1 1 I 1 1 I glalalole 
MM 
,JstJ 
Stop 
AREA ARMtesT. CODE. READONLY 
EKTTEY 
MOV 
rll,#20 
MOV 
rI,#0xET 
ADD 
r 2 , r 0 , r l 
ADD 
r 3 , r 0 , r l . LSL #4 
ADD 
r l , r 2 , # 6 5 5 3 6 
ADR 
r S . t o b l e l . 4 -
ADR 
r 6 . t a b l e 2 
MOV 
r0,#0xl8 
LDR 
1-1,-0x20026 
SOT 
0x123456 
AREA xyz. DATA, READWRITE 
tablel DCB "test" 
AREA pqr, DATA, READWRITE 
table? DCB "test2" 
This is the 
actual code 
stored in 
memory 
Lilj 
±lii 
1 
Tnis is the 
ADR 
inslruciion in 
the source 
code 
BiilSiiSilaiiiiSiSliSili 
0x00008084 
0x00008088 
0x0000608c 
0x00008090 
0x00008094 
OxClfl 
op 
oxooooeoao 
0x00008Qo4 
0::000080a 8 
t a b l e Z 
OxOOOOBOhO 
xyz 
edata 
ill 
-dbU 
snov 
KlOV 
add 
add 
add 
• add 
sdd 
mov 
ldr 
swi 
andeg 
Idrvcbt 
endeg 
ldrvcbt 
andeq 
*M*i 
0x00000018 
0x00020026 
0x00000113 
0x00001004 
0x00000000 
0x000080b4 
OxOOODSQac 
0x00000000 
0x00000000 
0x00000000 
0x00000000 
0x00000000 
0x00000000 
0x00000000 
•3x00000000 
0x000080o4 
MCCvift_User32 
1 
122 
r0.#0xl4 
rl,#0::ff 
r2,r0.rl 
r3,r0.rl,lsl #4 
rl,r2,#0x10000 
rS,pc.#0xl8 
r6,pc.#0xc 
r0.#0xlS 
rl.OxOOOOBOaa ; -
0x123456 
r0,r2,r6,lsr #32 
r6,fr3J,#-0x5?4 
r 0 , r 0 . r 2 , l s r rO 
r 6 . [ r 3 ] , # - 0 x 5 7 4 
r 0 , r O , r 0 
^ 
Program iemwiaied normally 
Figure 9.6 Effect of the ADR pseudo instruction. 
' f i
-
: 2 
: 3 
; 4 
! 5 
6 
j 
i:8 
i'9 
!:io 
;. n 
:'12 
13 
. 14 
15 
16 
1? 
18 
19 
:20 
: 21 
i 22 
;• 23 
124 
• 
,'jStart 
END 
rl 
r2 
r3 
: T 4 
l~5 
r& 
r7 
r8 
> 9 
rlO 
;rll 
:rl2 
rl3 
|rl4 
pc 
i; cpsr 

3 8 4 
Chapter 9 Processor architectures 
31 
28 21 2i. 
Condition 
0 1 
7? 
21 
?•? 19 
16 15 
12 11 
rba: 
Offset 
immediate offset 
Figure 9.7 Format of the ARM's memory reference instructions. 
register-based offset 
-+• Source/destination register 
- • Base register 
-fr. Data direction (Load/store) 
0 = store in memory 
1 = load into register 
- • Pointer update (Write-back) 
0 = don't write back adjusted pointer 
1 = write back adjusted pointer 
- • Operand size (Byte/word) 
0 = word access 
1 = byte access 
-^- Pointer direction (Up/down) 
0 = decrement pointer 
1 = increment pointer 
-P- Pointer adjust (Pre/post-increment) 
0 = post-index operation: use pointer then adjust 
1 = pre-index operation: adjust pointer then use pointer 
11 
12-bit immediate value 
11 
7 6 
5 4 3 
shift length 
type 
register 
These addressing modes correspond exacdy to the 68k's 
the pointer register is also incremented by 8. By modifying 
address register indirect addressing modes MOVE . L (Al), DO 
the above syntax slightly, we can perform post-indexing by 
and MOVE . L D2, (A3), respectively. 
accessing the operand at the location pointed at by the base 
The simple indexed addressing mode can be extended by 
register and then incrementing the base register, as the fol-
providing an offset to the base register; for example, 
lowing demonstrates. 
LDR r 0 , [ r l , # 8 ] 
;load rO with the word pointed a t by [ r l ] + 8 
The ARM goes further and permits the off- 
LDR rO, [ r l ] , # 8 
;load rO with the word pointed at by r l 
set to be permanently added to the base regis- 
<-and post-index by adding 8 to r l 
ter in a form of auto-indexing (rather like die 
We can summarize these three forms as 
eSK'spredecrementingandpostin- 
L D R rQ 
[ r l f # 8 ] 
; e f f e c t i v e address = [ r l ] + 8, r l i s unchanged 
crementing 
addressing 
modes). 
L D R r 0 r [ r l / # 8 ] ! 
e f f e c t i v e address = [ r l ] + 8, [ r l ] <- [ r l ] + 8 
This mode is indicated by using the 
L D R 
r 0 , [ r l ] , # 8 
/ e f f e c t i v e address = [ r l ] , 
[ r l ] <~ [ r l ] + 8 
! suffix as follows: 
Let's look at Fig. 9.7 in greater 
LDR r0,[rl,#8]! ;load rO with the word pointed at by [rl] + 8 
;and post-index by adding 8 to rl 
detail. The base register, r„, acts as a 
memory pointer (much like other 
In this example, the effective address of the operand is 
RJSC processors) and the U-bit defines whether the final 
given by the contents of register rl plus the offset 8. However, 
address should be calculated by adding or subtracting the 
25 
24 
23 
• 
0 
I 
''transfer 
I 
» 
« 
0 
0 

9.3 The ARM—an elegant RISC processor 
385 
offset. The B-bit can be set to force a byte operation rather 
than a word. Whenever a byte is loaded into a 32-bit register, 
bits 8 to 31 are set to zero (i.e. the byte is not sign-extended). 
The P- and W-bits control the ARM's auto-indexing 
modes. When W = 1 and P = 1, pre-indexed addressing is 
performed. When W = 0, P = 0, post-indexed addressing is 
performed. 
Consider the following example, which calculates the total 
of a table of bytes terminated by zero. 
MOV 
rO,#Table 
MOV 
r2,#0 
Next LDRB rl,[rO],#1 
ADD 
r2,rl,r2 
CMP 
rl,#0 
BNE 
Next 
There is no clear register instruction so you use 
SUB r 2 , r 2 , r 2 o r MOV r2,#0. 
The example consolidates some of die things we've 
learned. Let's calculate the inner product of two n-component 
of vectors A and B; that is, s = A • B = aY• bt + a2- b2 + 
«3 • b3 + • • • + an • bn. The code is as follows. 
MOV r4,#0 
MOV r5,#24 
ADR rO,A 
ADR rl,B 
LDR r2,[r0],#4 
LDR r3,[rl],#4 
MLA r4, r2, r3, r4 
SUBS r5,r5,#l 
BNE Next 
This block of ARM RISC code is not too dissimilar to the 
corresponding 68k CISC code. 
All ARM processors can operate with 32-bit values. 
The ARMv4 also supports byte and halfword (i.e. 16-bit) 
operations. A 16-bit unsigned word can be loaded into a 
register and stored in memory, or a 16-bit or 8-bit value 
can be loaded and sign-extended. Typical load/store 
instructions are 
LDHR 
Load unsigned halfword (i.e., 16 bits) 
LDRSB 
Load signed byte 
LDHSH 
Load signed halfword 
STHR 
Store halfword 
9.3.7 Using the ARM 
We are now going to look at ARM's development system, 
which allows you to write programs in assembly language, 
assemble them, and then run the programs. The software 
needed to carry out these operations is provided on the CD 
that accompanies this book. This software consists of three 
parts: an assembler, a linker (which generates binary code), 
and a simulator (which lets you execute the binary code on a 
PC). Let's begin with its assembly language. 
The ARM assembler 
All assembly languages and their assemblers are roughly sim-
ilar (there isn't the same difference between the 68K and the 
;r0 points to Table 
;clear the running total 
;get a byte and increment the pointer 
/calculate the new total 
;test for end 
clear initial sum in r4 
load loop counter with n (assume 24 here) 
rO points at vector A 
rl points at vector B 
Repeat: get Ai and update pointer to A 
get Bi and update pointer to B 
s = s + Ai x Bj 
decrement loop counter 
repeat n times 
CLR.B D2 
MOVE.B #24,DO 
LEA 
A,A0 
LEA 
B,A1 
MOVE.W (A0)+,D1 
MULU 
<A1)+,D1 
ADD.L D1,D2 
SUB 
#1,D0 
BNE 
Next 
clear initial sum in D2 
load loop counter with n (assume 24 here) 
A0 points at vector A 
AI points at vector B 
Repeat: get At and update pointer to A 
Ai x Bi and update pointer to B 
s = s + Ai x Bi (sum in D2) 
decrement loop counter 
repeat n times 
Next 
Next 

386 
Chapter 9 Processor architectures 
MULTIPLE REGISTER MOVEMENT 
The ARM supports a powerful set of multiple register 
movement instructions that allow you to copy any subset of 
the ARM's 16 registers to or from memory. 
Memory can be treated as a stack that can grow up or 
down. We do not go into the details of these instructions here. 
However, the instruction 
LDMIA 
r l ! , { r 2 - r 5 , r7-rl0} 
copies registers r2 to r5 and r7 to r10 inclusive from memory, 
using r1 as a pointer with auto-indexing. 
Figure 9.8 Structure of an ARM 
assembly language program. 
ARM assembly languages as there is between, for example, 
Pascal and LISP). Most assemblers follow the layout 
label mnemonic operand comment 
There are, however, differences in the conventions they 
employ and in the way in which assembler directives are 
implemented. Figure 9.8 shows a simple ARM assembly lan-
guage program (which does nothing other than manipulate a 
few numbers). 
As you can see, the program in Fig. 9.8 begins with the 
assembler directive 
AREA TestProg, CODE, READONLY 
This directive provides the name of the section of code and 
describes its properties. The ENTRY directive on the next line 
provides the code's unique entry point. An END directive ter-
minates the code. 
We have used the software interrupt, swi, an operating 
system call to terminate the program. 
Once a program has been written, you can assemble it with 
the ARM assembler. An ARM assembly language program 
has the extension . s. If the program is called ARMtestl. s, 
you enter the command (from the DOS prompt) 
armasm -g ARMtestl.s 
Assembling the program produces an object file called 
ARMtestl. o. The ARM development system requires that a 
program be linked before you can execute its code. Linking is 
performed in both high-level language compilation and low-
level language assembly and involves bringing together 
separately compiled/assembled units of code. To link the 
object file generated by the assembler you enter 
armlink ARMtestl.o -o ARMtestl 
The command armlink takes the object file ARMtestl. o 
and creates an executable file ARMtestl. 
Now that we've created the binary code, we can run it in the 
debugger that can be called from DOS or from Windows. 
We've used the Windows version (see Fig. 9.9). 
After invoking the simulator, we've loaded the program 
ARMtestl and opened a window that displays the disassem-
bled source code and shows the contents of the registers. In 
Fig. 9.8 we have stepped through the program line by line by 
clicking on the single-step icon. The program has ended with 
an error message caused by die SWI instruction. Note the 
values of the registers. We are now going to provide a tutorial 
on the use of the ARM development system. The full develop-
ment system provides a very powerful set of tools and is avail-
able from ARM Ltd. Here we are interested only in writing an 
assembly language program and observing its execution. 
Using the ARM development system 
Let's go through the steps necessary to develop and debug a 
program written in ARM assembly language. We begin by 
writing a simple program to determine whether a given string 
is a palindrome or not. A palindrome is a string that reads the 
same from left to right as from right to left—in this example 
the string is 'ANNA1 All we have to do is to remove a character 
from each end of the string and compare this pair of charac-
ters. If they are the same the string might be a palindrome; if 
AREA TestProg, CODE, READONLY 
ENTRY 
•«^ s^ 
S t a r t 
/MOV r O , # l \ 
^ * * \ ^ ^ 
/ M O V r l , # 2 
X. 
^ " ^ - v ^ ^ 
CODE defines a block 
/ 
ADD r 2 , r l , r 0 
N . 
*—^^ of program and 
/ 
MOV r 3 , r 2 , L S L #4 ^ V 
READ ONLY defines 
/ 
ADD r 4 , r 2 , r 3 , L S L #3 N. 
the block as readonly 
/ 
SWI 0x123456 
Nv 
/ 
E N D 
AREA defines a unit of 
/ 
code or data; in this 
' ENTRY defines the 
c a s e c a l [ e d 'TestProg' 
pointat which 
execution begins 

9.3 The ARM—an elegant RISC processor 
387 
. | g | xi 
File 
Edit 
Search 
VIEW 
Execute Options Item Windo.-; Help 
•3.1 ._i_L'i?l *l=:| IM^iiti g|Bjn|q|g| 
Start MOV rO,#l 
MOV rl.#2 
ADD r2.rl,r0 
MOV r3,r2,LSL #4 
ADD r4.rZ,r3.LSL #3 
SWI 0x123456 
END 
E>§JPJ^M|^GS5J|J|;§%1SSJ 
-ln|x| 
OxOOOCQOCi 
•1 
0x00300002 
0x00000003 
0x00000030 
0x00000183 
0x00000000 
0x00000000 
OzOOOOOOOO 
0x00000000 
0x00000000 
ra 
0 
OxOODOOOOO 
1 
0x00000000 
2 
0x00008098 
3 
0x00000000 
4 
0x00008010 
0x00008094 
^ 
S.r 
>.nZCvift 
User32 
^ 
^J:T
! 
Start 
:r.ov 
r O , * : 
0x00008084 
mov 
r l , # 2 
0x00008088 
add 
r 2 , r l , r 0 
0x0000808c 
mov 
r 3 , r 2 , l e 
0x00008090 
add 
r 4 . r 2 , r 3 
0x00008094 
SWI 
0x123456 
i'lp. 
R 
Figure 9.9 Running the simulator. 
they differ the string isn't a palin-
drome. We then repeat the same 
operation on the remaining string 
(i.e. substring), and so on. If we 
reach the middle of the string, the 
string is a palindrome. 
The algorithm 
to 
determine 
whether a string is a palindrome 
requires at least three variables. We 
need a pointer to the character at the left-hand end of the 
string, a pointer to the character at the right-hand end of the 
string, and a flag that indicates whether the string is a palin-
drome or not. 
The following fragment of pseudocode provides a 
first-level 
solution 
to 
the 
problem. 
The 
variables 
lef t _ p o i n t e r and right_pointer point at the charac-
ters at the ends of the string currently being examined and 
Palindrome is true if the string is a palindrome and false 
otherwise. We begin by assuming that the string is a palin-
drome and set the flag palindrome false if ever a pair of 
characters don't match. 
Palindrome = true 
Set left_pointer to point at leftmost character 
Set right pointer to point at rightmost character 
REPEAT 
Get l e f t character at l e f t _ p o i n t e r 
p o s i t i o n 
Get r i g h t character at right_pointer 
p o s i t i o n 
IF l e f t character * r i g h t character THEN Palindrome = f a l s e 
left_pointer 
= l e f t _ p o i n t e r + 1 
right_pointer = right_pointer - 1 
UNTIL middle of s t r i n g OR Palindrome = f a l s e 
The only tricky part of this problem is determining when 
we reach the middle. Consider the palindromes ABCCBA 
and ABCBA. The first palindrome has an even number of 
letters and the second an odd number of letters. Consider the 
following: 
Step Even length 
Pointers at 
end of test 
Odd 
length 
Pointers at 
end of test 
1 
ABCCBA 
ABCBA 
2 
ABCCBA 
ABCBA 
3 
ABCCBA 
Left_pointer 
ABCBA 
= right_pointer+ 1 
Left_pointer 
= right_pointer 
i_ 
it 
2 
3 
4 
S 
6 
7 
8 
9 
10 
11 
AREA TestProg. C0DE7~RSAD0NI 
ENTRY 
^"--
jjjg Start) j j | MS-DOS Prompt 
[ IjjExpioring-£:\ARyg...[[^£;\ARM200\BiN\,__ 
Executing 
Program stopped: SWI 
lZ..J¥ J 
Single step 
control 

388 
Chapter 9 Processor architectures 
The middle of the string is located when either the left 
pointer is one less than the right pointer or the left pointer is 
equal to the right pointer. 
We can easily write a fragment of code that scans the string. 
In the following code (written in the form of a subroutine), 
register rO points to the left-hand end of the string and regis-
ter rl points to the right hand end of the string. Remember 
character-fetch operations and therefore we have to take 
account of this when comparing the pointers). We can fix the 
problem in three ways: update the pointers only after the test for 
die middle, take copies of the pointers and move them back 
before comparing them, or perform a new test on the copies for 
left_ pointer = right_pointer + 2 and lef t_pointer = 
right_po i n t e r + 1. We will use the first option to get 
pal 
MOV 
rlO,#0x0 
again 
LDRB r3,[rO] 
LDRB r4, [rl] 
CMP 
r3, r4 
BNE 
notpal 
CMP 
rO,rl 
BEQ 
waspal 
ADD 
r2,r0,#l 
CMP 
r2,rl 
BEQ 
waspal 
ADD 
rO,rO,#l 
SUB 
rl,rl,#l 
B 
again 
;rO points at left hand end of string 
;rl points at right hand end of string 
;rlO = success/fail flag 
;get left hand character 
;get right hand character 
/compare the ends of the string 
;if different then fail 
;test for middle of string 
;if rO = rl then odd length palindrome 
;if same then exit with palindrome found 
;copy left pointer to r2 and move right 
;if r2 = rl then even length palindrome 
;if same then exit with palindrome found 
;move left pointer right 
;move right pointer left 
;REPEAT 
waspal MOV 
rlO,#0xl 
notpal MOV 
pc,lr 
;rlO = 1 
;return 
success flag 
that the ARM instruction LDRB r 3 , [ r 0 ] , # 1 means 'load 
register r3 with the byte pointed at by rO and add 1 to the 
contents of rO'. 
again 
LDRB 
r3,[rO],#1 
LDRB 
r4,[rl],#-1 
CMP 
r3,r4 
BNE 
notpal 
;get left hand character and update pointer 
;get right hand character and update pointer 
;compare characters the at ends of the string 
;if characters different then fail 
;test for middle of string 
BNE 
again 
waspal 
notpal 
MOV 
pc,lr 
,-if middle not found then repeat 
;end up here if string is palindrome 
;return from subroutine 
We can test for the middle of a string in the following way: 
CMP 
rO,rl 
BEQ 
waspal 
ADD 
r2,r0,#l 
CMP 
r2,rl 
BEQ 
waspal 
;if r2 = rl then odd length palindrome 
;if same then exit with palindrome found 
;copy left pointer to r2 and move right 
;if r2 = rl then even length palindrome 
;if same then exit with palindrome found 
The code we wrote to scan the palindrome automatically 
updates the pointers when they are used to fetch characters (e.g. 
the left pointer is used and updated by LDRB r 3, [ r 0 ] , # 1 and 
the right pointer is updated by LDRB r 3 , [rl] ,#-l) 
This means that both pointers are updated during the 
The following code provides the complete program to test 
a string. We begin by scanning the string (which is terminated 
by a 0) to find the location of the right-hand character. The 
subroutine either returns 0 in rlO (not palindrome) or 1 (is 
palindrome). 

9.3 The ARM—an elegant RISC processor 
389 
AREA palindrome, CODE, READONLY 
ENTRY 
start 
loop 
«t op 
pal 
again 
waspal 
notpal 
LDR 
MOV 
LDRB 
CMP 
BNE 
SUB 
BL 
MOV 
f,DR 
Gl'71 
MOV 
LDRB 
LDRB 
CMP 
BNE 
CMP 
BEQ 
ADD 
CMP 
BEQ 
ADD 
SUB 
B 
MOV 
MOV 
rO,=string 
rl,rO 
r2,[rl],#l 
r2,#0 
loop 
rl,rl,#2 
pal 
rO,10x18 
rl,-'0x2002c 
0>;1234'.)6 
n o , #0x0 
r3,[rO] 
r4, [rl] 
r3,r4 
notpal 
r0,rl 
waspal 
r2,r0,#l 
r2,rl 
waspal 
rO,r0,#l 
rl,rl,#l 
again 
rl0,#0xl 
pc, lr 
;locate the ends of the string 
;r0 points to start of string to test 
;copy left pointer to right pointer in rl 
;get char and update right pointer 
/repeat until terminator located 
/fix right pointer to point to end of string 
;call subroutine to test for palindrome 
;:;top program execution by calling r.he C/3 
/the three J.me:; of "n.agii c" code in thin 
;biOck are an operating system rail 
;test for palindrome 
;r0 points at left hand end of string 
;rl points at right hand end of string 
;rl0 = Palindrome = 0 = set fail flag 
;get left hand character 
;get right hand character 
/compare the ends of the string 
;if different then fail 
;test for middle of string 
;if rO = rl then odd length palindrome 
;if same then exit with palindrome found 
;copy left pointer to r2 and move right 
;if r2 = rl then even length palindrome 
;if same then exit with palindrome found 
;move left pointer right 
;move right pointer left 
/REPEAT 
;rl0 = 1 = success flag 
/return 
Note the three lines of code labeled by stop. I copied the 
code from ARM's literature because it offers a means of 
halting program execution by calling an operating system 
function. Other versions of the ARM simulator may require 
different termination mechanisms. You can always terminate 
a program by implementing an infinite loop: 
Finish 
B 
Finish 
Having written the program (using an ASCII editor), we 
assemble it with the command ARMASM. If the program is 
called PROG1. s, it is assembled by 
ARMASM -g PROG1.s 
The assembly process produces a new object file called 
PROG1. o. The '-g' option generates debugging information 
for later use. If no errors are found during the assembly 
phase, the object code must be linked to produce the binary 
code that can be executed by an ARM processor (or simulated 
on a PC). The command used to link a program is ARMLINK. 
In this case we write 
ARMLINK PROGl.o -o PROG1 
The linker creates a new file called PROG1, which can be 
loaded into the ARM simulator. 
Once we have created a file to run, we can call die 
Windows-based ARM debugger by clicking on the ADW icon 
(assuming you've loaded ARM's package on your system). 
This loads the development system and creates the window 
shown in Fig. 9.10. By selecting the F i l e item on the top 
toolbar, you get a pull-down menu whose first item is Load 
image (see Fig. 9.11). Clicking on Load Image invokes the 
window used to open a file and lists the available files (see 
Fig. 9.12). In this case, we select the file called Progl. 
Figure 9.13 shows the situation after this program has been 
loaded. 
The Execution window in Fig. 9.13 shows the code 
loaded into the debugger. Note that the ARM development 
system creates a certain amount of header code in addition 
to your program. We are not interested in this code. 
Figure 9.13 shows address 0 X 00008008 highlighted—this 
is the point at which execution is to begin (i.e. the initial 
value of the program counter). However, you can also start 

390 
Chapter 9 Processor architectures 
L i V . r ^ 
JU 
uxuuuuuuuc 
i a r 
pt.UXUUUUUdlC 
= 
WUSUUUUUiJllU 
0x00000010 
I d r 
pcOzOOOOOaSO 
- #0xQ0000bcG 
0x00000014 
l d r 
p c . 0 x 0 0 0 0 0 a 5 4 
- #OxOG000bd0 
0x00000018 
I d r 
pc,0K0D000a58 
= #OxOG0OGbeQ 
0x0000001c 
l d r 
pcOsOOQOOaSc 
= #Gx0DO00fafD 
0x00000020 
, 
a n d e q 
r O , r O , r O 
0x00000024 
a n d e q 
r O . r O . r O 
0x00000026 
a n d e q 
r O , r O , r O 
0x0000002c 
a n d e q 
r O , r O , r O 
0x00000030 
a n d e q 
r O , r O , r Q 
0x00000031 
andeq 
r O ^ r O . r O 
0x00000038 
a n d e q 
r Q , r O , r O 
0x0000003c 
a n d e q 
r O , r O , r O 
0x00000040 
a n d e q 
r O , r O , r O 
0x00000044 
a n d e q 
r O , r 0 . r O 
0x00000048 
a n d e q 
r O , r O , r O 
0x0000004o 
a n d e q 
r O , r O , r O 
OxOOOOOOSO 
a n d e q 
r O , r O , r O 
0x00000054 
a n d e q 
r O , r O , r O 
0x00000058 
a n d e q 
r O , r O , r O 
0x0000005c 
a n d e q 
r G . r O , r O 
0x00000060 
a n d e q 
r D , r O , r G 
0x00000064 
a n d e q 
r O , r O , r O 
0x00000068 
a n d e q 
r O . r O . r G 
0x0000006c 
a n d e q 
r O , r O , r O 
0x00000070 
a n d e q 
r O , r O , r O 
0x00000074 
andeq 
r O , r O , r O 
0x00000078 
a n d e q 
r O , r 0 . r O 
0x0000007c 
a n d e q 
r O , r O , r O 
0x00000080 
, 
a n d e q 
r O „ r O , r O 
0x00000084 
andeq 
r O . r O . r O 
!T"""°%j «rr 
• 
For Help, press Ft 
«$3 Start) Ig'MicmsotsWord-AR-
I ARM Debugger.. 
Figure 9.10 The initial window after loading the ARM debugger. 
ARMjbeliBgj«j«W6%^E^^ 
ma 
0x00000030 
a n d e q 
r O . U . r 3 
0x00000034 
a n d e q 
rO.rfl.-.C 
0x00000038 
andeq 
r Q . i ' i . i - J 
0x0000003c 
a n d e q 
r O . r J . r 2 
0x00000040 
a n d e q 
r O . r . l . r J 
0x00000044 
a n d e q 
r O . i n . r J 
0x00000048 
andeq 
r O . r J . r J 
0x0000004c 
ondeq 
r O . T L . r J 
0x00000050 
a n d e q 
r O . i u . r d 
0x00000054 
a n d e q 
r O . r r . r f l 
0x00000058 
a n d e q 
r O . s l , i U 
0 x 0 0 0 0 0 0 5 c 
a n d e q 
r O . t O . i - 0 
0x00000060 
a n d e q 
r O . j O . r i l 
0x00000064 
a n d e q 
r O . r l , : 0 
0x00000068 
a n d e q 
r 0 , r 0 . r 3 
0x0000006c 
a n d e q 
r O . r O . r : 
0x00000070 
a n d e q 
r O . i C - . r : 
0x00000074 
a n d e q 
r O . i J . r C 
0x00000078 
a n d e q 
r O . r . i . r J 
0x0000007c 
a n d e q 
r O . i . ' . r j 
0x00000080 
a n d e q 
r O . i P . r f ) 
0 
a n d e q 
r O . i C . i O 
.«.: 
>\ *rr 
3fl Start JgyMier osoftWofd- AR j]0[AKM Debugger 
pc.OsOOOOOalO ; - #OxOODOObBO 
0x1414 
pc,0x00000a48 ; - #0>iOOOOObaO 
pc,0x00000a4cr ; - SOxOOOOObbO 
pc.0x00000a50 ; - #0j:0OO00bcCI 
pc.0x00000a54 ; - «teOOOOObdQ 
pc.0K000D0a53 ; - #0sO0OO0beO 
pc.0xt)0000a5c ; - #0xOO0OObfO 
rO,rO,xO 
rO.rO.rO 
rO.rO.rO 
rO.rO.rO 
l^'0Qmmw&fflmdffi0t 
:0?xK$Sii •l-lnlxl 
(ikMod CoEtir.i.d 
11.-or. 
Celmg : 
a c e 
4 
ULJ 
&•$. 33 B«W 
Figure 9.11 The K i l e pull-down menu. 
K i K i U U U U 
I 
l d r 
pc,UjiUUUUUa4U : - #UXUUUUUUHU 
* 
1x00000004 
b 
0x1414 
~ 
1x00000008 
l d r 
pc,0x00000a48 ; - #0x00000ba0 
WS 
fidit 
Ss^d: 
Via* 
Exiicute 
Options 
Window 
Haip 
Load symbols arty. 
Qui life. 
Bit WE. 
Save EoniLie conisnls. 
SovsMILug 
.1 prog1 
fcKt 
Ofe 
£<*t Search 
\£tew £\ecuie 
CJpSons ^fmdow 
Help 
ARMulala 
i & M S 8:48 PM 

9.3 The ARM—an elegant RISC processor 
391 
OxOOQOOOlO 
Idr 
pc ,Ox00O0Qa5Q ; - #0x00D00bc0 
| I 
0x00000014 
0x00000018 
: 
0x0000001c , 
0x00000020 
: 
0x00000024 
0x00000028 
Idr 
Idr 
Idr 
andeq 
aiideq 
andeq 
I 
I 
.O;:0OO00aS4 : - #Ox00OOObd0 
! 
0x00000014 
0x00000018 
: 
0x0000001c , 
0x00000020 
: 
0x00000024 
0x00000028 
Idr 
Idr 
Idr 
andeq 
aiideq 
andeq 
I 
I 
Look in: 
J 
0x00000014 
0x00000018 
: 
0x0000001c , 
0x00000020 
: 
0x00000024 
0x00000028 
Idr 
Idr 
Idr 
andeq 
aiideq 
andeq 
I 
I 
Look in: 
J _,j Progs 
••dslslisal 
0x00000014 
0x00000018 
: 
0x0000001c , 
0x00000020 
: 
0x00000024 
0x00000028 
Idr 
Idr 
Idr 
andeq 
aiideq 
andeq 
I 
0x0000002c 
andeq 
I 
0x00000030 
andeq 
I *Jprog1.s 
0x00000034 • 
andeq 
I 
*Jprog1.s 
0x00000038 
andeq 
I 
0x0000003c 
andeq 
I 
0x00000040 
andeq 
X 
0x00000044 
andeq 
I 
0x00000048 
andeq 
2 
0x0000004c 
andeq 
I 
OxDDOOOOSO 
andeq 
I 
0x00000056 
0x0000005c 
0x00000060 
andeq 
andeq 
andeq 
* file cams-
Fries of &pe. 
progl 
Op 
0x00000056 
0x0000005c 
0x00000060 
andeq 
andeq 
andeq 
* file cams-
Fries of &pe. 
All Files r*) 
""" -3 
0x00000064 
0x00000068 
andeq 
andeq 
rO 
0x00000064 
0x00000068 
andeq 
andeq 
rO ,rO,rO 
0x0000006c 
andeq 
rO.rO.rO 
0x00000070 
andeq 
rO.rO.rO 
0x00000074 
andeq 
rO,rO,rO 
0x00000078 
andeq 
rO,rO,rO 
0x0000007c 
andeq 
rO„rO,rO 
0x00000080 
andeq 
r0.rO,rO 
0x00000084 
andeq 
rO,rO,rO 
T J 
m
nn""°^ k r 
* I 
, 
. .,* 
„, _ 
For Help, press F1 
^gStartl ^MicjosoftWord-AR | 
I ARM Debugger.. 
Figure 9.12 Loading a program into the simulator. 
! S i ARM JDeMggBr AD£»BM2,1 ffiRrogS^proal S 
File 
J~dtf Search 
M'SW 
Erectile 
Options 
W.r.dovv 
Help 
"EH 
'^E^ecutToh '.Wind oWl 
0x00007fc4 
0x00007fc8 
0x00007fcc 
0x00007fd0 
0x00007fd4 
0x00007fd8 
0x00007fdc 
0x0OO07fe0 
0x0000?fe4 
0x0OOO7feB 
0x00007fec 
0x00007ff0 
0x00007ff4 
0x00007ff8 
0x00D07ffc 
0x00008000 
0x00008004 
OxOOODaODS 
0000800c 
0x00008010 
0x00008014 
0x00008018 
0x0000801c 
0x00008020 
0x00008024 
0x00008028 
0x0000802c 
0x00008030 
0x00008034 
0x00008036 
0x0000803c 
0x00008040 
0x00008044 
0x00008048 
Efl-
±1 
andeq 
andeq 
andeq 
andeq 
andeq 
andeq 
andeq 
andeq 
andeq 
andeq 
andeq 
andeq 
endeq 
andeq 
andeq 
nop 
nop 
bl 
bl 
swi 
andeq 
andeq 
streqh 
andeq 
andeq 
andeq 
andeq 
andeq 
andeq 
andeq 
andeq 
nop 
sub 
add 
r0,r0.rC 
rO,rO,rl. 
r0,r0,rC 
r0,r0,r!_ 
r0„r0,r0 
r0,r0,r': 
r0.rO,rJ 
rO.rO.r.1 
r0.r0,rj 
r0,r0.r.l 
r0.r0,i: 
r0,r0,rJ 
r0,rQ,r0 
rD,r0,r:i 
rO,rO,rJ 
0x8040 
s t a r t 
0x11 
r 0 , r 0 , r 0 , r o r #1 
rO.rO.r8 
r 0 , [ r 0 ] . - r t 2 
rO.rO.rO 
r 2 . r 0 , r 3 . 1 s l #7 
r 8 , r 0 , r 0 
rO,rO,rO 
rO,rO,rO,lsr #32 
rO.rO.rO 
rO,rO,rO 
rO,rO,rO 
r l 2 , r l 4 , p c 
r l 2 , p c , r l 2 
OH 
"3 
^ 
B 
JuUj 
i?.K-d Cmrai.H. 1, 
DHI.-.H : 
=2 
±i_i 
J 
Ftte Loaded 
jSlStBrt] IB?MicrosoftWo:d-AH 
iARM rtebugqp.i 
.jjj.-jg CblPt.f 
Figure 9.13 The screen after loading p r o g l . 
'(OrtOOaOOOO 
I Idr 
pc,0x00000a40 ; - MxOOOOObBQ 
Til 
10x00000004 
I b 
0x1414 
;zj 
0x00000008 
Idr 
pc.OxOOOODa48 ; - #0x00000ba0 
I 
fiRHsd Command Interface 
Debug: 

3 9 2 
Chapter 9 Processor architectures 
the program by setting a breakpoint to 0 X 8080 and then 
running the code to the breakpoint. Doing this executes the 
start up code and then stops simulation at the appropriate 
point. 
We can view other windows beside the Execution 
window. In Fig. 9.14 we have selected the view command 
on the top toolbar and have chosen Registers from 
the pull-down list to give a second pull-down list of 
registers. 
Figure 9.15 shows the debugger with the register window 
active. You can modify the contents of any register in this 
window by double clicking on the appropriate register. 
Figure 9.16 shows how the current contents of a register 
appear in a Modify item window. In this diagram the PC 
contains 0 X 00008008, which we alter to 0 X 00008080 (the 
address of the start of progl). This address (i.e. 8080) is a fea-
ture of the ARM development system I used. 
Figure 9.17 shows the state of die system after the PC has 
been reloaded. As you can see, the code that we originally 
entered is now displayed. 
In Fig. 9.17 we have resized the windows to make best use 
of the available space in order to see as much as possible of the 
program's comment field (without losing die Registers 
window). The first instruction to be executed is highlighted. 
We can now begin to execute the program's instructions to 
test whether a string is a palindrome. There are several ways 
of running a program in the ARM debugger; for example, we 
can run the whole program until it terminates, execute a group 
of instructions, or execute a single instruction at a time. If you 
click on the step-in icon on the toolbar, a single instruction at 
a time is executed. The effect of program execution can be 
observed by monitoring the contents of the registers in the 
Registers window. 
In Fig. 9.18 we have begun execution and have reached the 
second instruction of the subroutine 'pal'. In Fig. 9.19 we 
have executed some more instructions and have reached line 
number 25 in the code. 
Let's return to the View pull-down menu on the tool bar to 
display more information about the program. In Fig. 9.20 we 
have pulled down the menu and in Fig. 9.21 we have selected 
the Disassembly mode and have been given the disassem-
bly address window. 
Figure 9.22 shows the Disassembly window. You can see 
the contents of the memory locations starting at 0 X 00008080. 
i i i ^ M 3 S H W 9 o > R 0 » B M E 1 l J R r g g ^ r o g 1 S 
File £ d 1 £ s e r f , 
Execute 
Qp'.'orts 
v^rdnw 
Help 
0 x 0 0 0 0 7 f u 4 
0>:00007fcB 
0::00007£ce 
0::03D07fd-l 
03Di<7faD 
0 : : M 0 ' J 7 f d c 
DxCGOO'/feO 
0 x 0 U 0 n 7 f « 4 
0;:G0007fe3 
0 i . 0 0 0 0 7 f e c 
0:-.00007ff0 
0OOD7ff4 
Q::n0007f£8 
3 x n 0 0 0 7 t - f c 
Q::000l(800U 
xCconaoo4 
OsODOOBDOS 
ooooeooc 
OxODOOBOlO 
OiOOOOBOM 
O^OOOOSuifi 
OxQOOQOOlt 
3:130008020 
0^0000602-1 
r0006028 
0x000OB02c 
0x00000030 
OzOOOOB034 
0>.OGOC803S 
J:.00008fl3c.-
0KO000804fl 
XC0000044 
0:;0000804B 
'".Li 
Source Files 
Function NamBS 
Backtrace 
Memory 
Osassembfy 
L,O«Y Level Symbols CW-Z 
Or-P 
Ctil-F 
CUi-N 
Qrt-T 
CtrtM 
Qrt-D 
greakpomts 
Vjjafctipoints 
Console 
CQtnmami 
FlDiPretaco Log 
Debugger internals 
' Sto:us Bai 
Clrt-B 
Crt-W 
andoq 
rtrtdpq 
= tl'oqh 
andt'g 
andvq 
?ndeq 
n n d p q 
sndeq 
dlld«'q 
a n d f q 
andpq 
nop 
sub 
add 
lO.rO.rO.Tnr »1 
r 0 . r ' , , r 3 
rO,[rO].-rl2 
rO.rO.rO 
r ? , r 0 . r 3 . l ! , l #7 
rS,r0,rC 
r0.i-0.r0 
rO.rO.rO 
rO.rO.rO 
rO.i-O.rO 
rO.rO.rO 
r l 2 . r l 4 . p c 
rl,7 ,pc,r:2 
— iMi.^n^:111 
Visv crcerc* registets 
l<iT #32 
jJARM 
D e b u g g e r . 
i M - - I 3 
852 PM 
Figure 9.14 Selecting the set of registers to view. 
• I.'.. : • 
S . l . : -
! ! ' . ' • <:• 
- - 
'•?. .'• 
•v.jil 
••!•!: 
• 
- . . : • : • • ' 
.• 
ARMsd Command Interface 
Debug: 

9.3 The ARM—an elegant RISC processor 
393 
(l,vARM;:Debugg8Mp;VARM2JBVPrq'gs^roglJ 
Vi«,v 
E>E?CL:P 
QpLons 
Window 
yelp 
l(')|'»-[ir-| [ q|ra|a|ci|g 
t^tt 
rs 
• 
=MU 
jxooot:... 
3 
3xOooc7tve 
ar.anq 
: 3 . r 0 . r 0 
3 
0x0CDC7fcc 
a n u e q 
r O . r O . r O 
3xOQ007fdO 
a n d e q 
r O . r O . r O 
3xO00G7fd4 
a n d e q 
r 0 . r 0 . r 0 
3x00007fd8 
s n d e q 
r O . r O . r O 
0 x 0 0 0 0 7 f d c 
a n d e q 
r O . r O . r O 
3x00007feO 
a n d e q 
r O . r O . r O 
3x0000?fe4 
a n d e q 
r O . r O . r O 
3 x 0 0 0 0 7 f e 8 
a n d e q 
r O . r O . r O 
Ox0000?fec 
a n d e q 
r O . r O . r O 
0xOOO07ffO 
a n d e q 
r O . r O . r O 
Qx00007ff4 
a n d e q 
r O . r O . r O 
0 x 0 0 0 0 7 f f 8 
s n d e q 
r O . r O . r O 
? 0 x 0 0 0 0 7 f f c 
0x00008000 
0x00008004 
a n d e q 
nop 
nop 
r O . r O . r O 
9x00008008 
b l 
0x8040 
—I 
Oxooooaooc 
b l 
s t a r t 
0x00008010 
S W l 
O x l l 
^ 
0x00008014 
a n d e q 
r O . r O . r Q . r o r 
#1 
i 0x00008018 
a n d e q 
r O . r O . r S 
i 0x0000801c 
s t r e q h 
r O , [ r O J . - r l 2 
i 0x00008020 
a n d e q 
r O . r O . r O 
i 0x00008024 
a n d e q 
r 2 . r 0 . r 3 . l s l 
#7 
i 0x00006028 
a n d e q 
r 8 . r O . r O 
i 0 x 0 0 0 0 8 0 2 c 
a n d e q 
r O . r O . r O 
1 0x00008030 
a n d e q 
r O . r O . r O . l s r 
#32 
I 0x00008034 
a n d e q 
r O . r O . r O 
< 
0x00008038 
a n d e q 
r O . r O . r O 
1 
0 x 0 0 0 0 8 0 3 c 
0x00008040 
a n d e q 
nop 
r O . r O . r O 
s 0x00008044 
s u b 
r l 2 , r l 4 , p c 
0x00008048 
add 
r l 2 , p c . r l 2 
zl 
F"'"ii <rr 
kl'y.id 
'.'orr.-rand 
DLU 
2i 
For Hefp. press Fl 
^ S t e r t | gyMicrooftWord-AR. |||jJARM Debugger -
Jxffffffff 
)x00000a44 
0x00000000 
0x00000000 
0x00000000 
3x00000000 
5x00000000 
0x00000000 
OxOOOOODOO 
3x00000000 
0x00000000 
0x00000000 
0x00000000 
OxOOOOODOO 
OxOOOOODOO 
0x00008008 
aizcvift_us 
'
1 
• • • ' . : . " 
1 
.r1 
[J 'i3 8 53 PM 
Figure 9.15 Viewing the ARM'S registers. 
F & Earl Seercrr 
igj 
M 
|is| 
B i a s 
Opl.ons M n c « 
Ue:p 
J |H|MfflJ d|iaiq«P|Sl 
a- . 
' ~-\'n\ 1 
3 x 0 0 0 0 7 f c 4 
a n d e q 
r J . r C . r 3 
d 
0 x 0 0 0 0 7 f c 8 
a n d e q 
r O . r O . r O 
0xOO0O7fce 
a n d e q 
r O . r O . r O 
0x000O7fdO 
a n d e q 
r O . r O . r O 
Ox00007Fd4 
a n d e q 
r O . r O . r O 
3x00007fd8 
a n d e q 
r O . r O . r O 
0 x 0 0 0 0 7 f d c 
s n d e q 
r O . r O . r O 
0 x 0 0 0 0 7 f e 0 
a n d e q 
r O . r O . r O 
0 x 0 0 0 0 7 f a 4 
a n d e q 
r O . r O . r O 
0 x 0 0 0 0 7 f e 8 
a n d e q 
r O . r O . r O 
0 x 0 0 0 0 7 f e c 
a n d e q 
r O . r O . r O 
0 x 0 0 0 0 7 f f 0 
a n d e q 
r O . r O . r O 
0 x 0 0 0 0 7 f f 4 
s n d e q 
r O . r O . r O 
0 x 0 0 0 0 7 f f 8 
a n d e q 
r O . r O . r O 
0 x 0 0 0 0 7 f f c 
0x00008000 
0x00008004 
fn 
r O . r O . r O 
• » 0 0 8 
b l 
0x8040 
—1 
0x0000800c 
b l 
s t a r t 
0x00008010 
swx 
O x l l 
0x00008014 
a n d e q 
r O . r O . r O . r o r 
# 1 
0x00008018 
a n d e q 
r 0 . r 0 . r 8 
0x0000801c 
s t r e q h 
r O , ( r O J . - r l 2 
0x00008020 
a n d e q 
r O . r O . r O 
0x00008024 
a n d e q 
r 2 . r 0 , r 3 . 1 s l 
#7 
0x00008028 
a n d e q 
r 8 . r 0 . r 0 
0xO00OB02c 
' 
a n d e q 
r O . r O . r O 
0x00008030 
a n d e q 
r O . r O . r O . l s r 
#32 
0x00008034 
a n d e q 
r O . r O . r O 
0x00008038 
• 
a n d e q 
r O . r O . r O 
0 x 0 0 0 0 8 0 3 c 
a n d e q 
r O . r O . r O 
8 0x00008040 
nop 
; 
0x00008044 
s u b 
r l 2 . r l 4 , p c 
> 
0x00008048 
add 
r l 2 . p c . r i 2 
~*\ 
* "£$"""""-_•) <rr 
> |"7^ 
sxxsiMsissmw^mrf^it. 
srwprttSiSnwra t^iu. 
, 
U - - ^ • • ^ ~ » - * - . — - 
- * -
fcSltea Con=iand 
H 
rO 
Oxfff 
. 
r l 
CxOOO 
~ 
r 2 
3x030 
r j 
OxOUO 
r 4 
3x003 
: i 
GsOOC- JUJu 
r b 
0XO0CO0O3C 
! 7 
3x00000033 
T 3 
3x00300000 
r 3 
0x00300000 
r l D 
GxOIJCGOOOO 
i l l 
0x000300110 
r l 2 
0x00000003 
r l 3 
0x00000003 
r l 4 
OxCOCOOOOO 
PC 
0x00033008 
a : Z C v i f t 
J s o r j 2 
'"I 
SJ 
J Liu 
_»j 
Vod^/ths contents Qi 
pc 
|rMMMMR 
j 
Of. 
| 
Cancel 
j 
For Heip. press Fl 
^ S t a r t j ByMteosof>Word-AR.j 
• ARM Debugger.. 
^ • i g 
B5'PM 
Figure 9.16 Reloading the simulated PC. 
rO 
rl 
r2 
r3 
r4 
r5 
r6 
r? 
r8 
r9 
rlO 
rll 
rl2 
rl3 
rl4 
po 
cpsr 
ill 

394 
Chapter 9 Processor architectures 
•j'/SSg'Sgi^^jyj^'g 
miEmmmmgm%M 
loop 
LDR 
rO,=string 
MOV 
rl.rO 
LBRB 
r2,[rl].#l 
CMP 
r2.#0 
BNE 
loop 
SUB 
rl,rl.#2 
BL 
pal 
MOV 
r0,#0xi8 
LDR 
rl.-0x20026 
SHI 
0x123456 
pal 
MOV 
rl0,#0x0 
again 
LDRB 
r3,[r0] 
LDRB 
r4.[rl] 
CMP 
r3,r4 
BNE 
notpal 
CMP 
rO.rl 
BEQ 
waspal 
ADD 
r2.rB,#l 
CMP 
r2,rl 
BEQ 
waspal 
ADD 
r0,r0,#l 
SUB 
ri,rl.#l 
8 
again 
JLiL 
For Help, press F1 
^gStart] I^MicsosoftWoid-Ne 
; locate the ends of the s t n n c 
;r0 points to start of string 
;copy left pointer to right pc 
;get char and update right pal 
;repeat until terminator local 
;fix right pointer to point tc 
;call subroutine to test for f. 
;stop program execution 
;test for palindrome 
;r0 points at left hand end of 
;rl points at right hand end c 
;r!0 = Palindrrae - 0 = set it 
;get left hand character 
;get right hand character 
;compare the ends of the strir 
;if different then fail 
;test for middle of string 
;if rO » rl then odd length p£ 
;if same then exit with p a l m c 
;copy left pointer to r2 and a 
;if r2 = rl then even length p 
;if same then exit with p a l m c 
;move left pointer right 
;ntove right pointer left 
;REPEAT 
< It 
JLl 
rl 
0x00O0Oa44 
r2 
OxOOOOOOOO 
r3 
oxoooooaoo 
r4 
0x00000000 
r5 
0x00000000 
r6 
0x00000000 
r7 
0x00000000 
r8 
0x00000000 
x9 
0x00000000 
rlO 
0x00000000 
rll 
0x00000000 
rl2 
0x00000000 
rl3 
0x00000000 
rl4 
0x00000000 
3C 
0x00008080 
cpsr 
!HZCvift_User32 
^ 
I ABM Debugger.. 
a v i g 21 CPU 
Figure 9.17 The system after resetting the PC. 
».AHM;;DibiigWtSPiV«^?Jll.ffi^isip(no1i-
filo 
Ed?. Ssairn 
V I B * 
fcsenfe 
Optnns 
Window He'p 
izl 
I I UEI^UJ 
|Ti|gltP|-o|C!|e|n|g|i55| 
EH3 
ESSE HHi 
W^^MJ^^^^^^MMWM-^-^ \ 
AREA palindrome, CUL'E READONLY 
A 
ENTRY 
READONLY 
A 
start 
ilocate the ends of the strim 
LDR 
r0,*string 
;r0 points to start of string 
MOV 
rl,r0 
;copy left pointer to right pc 
loop 
LDRB 
r2,[rl],#l 
;get char and update right poi 
CMP 
r2.#0 
;repeat until terminator locat 
BNE 
loop 
SUB 
rl,rl,#2 
;fix right pointer to point tc 
BL 
pal 
;call subroutine to test for \ 
stop 
MOV 
r0,#0xl8 
;stop program execution 
LDR 
rl, -0x20026 
SWI 
0x123456 
;test for palindrome 
;r0 points at left hand end o£ 
;rl points st right hand end c 
pal 
MOV 
rl0,#Qx0 
;rl0 * Palindme = 0 = set fc 
again 
LDRB 
r3,[r0] 
;get left hand character 
LDRB r4,frl] 
;get right hand character 
CMP 
r3,r4 
;compare the ends of the si 
BHE 
notpal 
;if different then fail 
;t<?st for middle of string 
CMP 
rO.rl 
;if rO «• rl then odd leagti 
BEQ 
waspal 
;if same then exit with pa] 
ADD 
r2,r0.#l 
;copy left pointer to r2 ar 
CMP 
r2,rl 
;if r2 = rl then uvea lengl 
BEQ 
waspal 
;if same then exit with pa3 
ADD 
rG,rQ,n 
;mave 
left pointer right 
SUB 
rl,rl,#l 
;move right pointer left 
B 
again 
;REPEAT 
< \ 
1 
- T Kl^! 
. ri r, ^ 1 ^ -.„_,-«„ fi„„ 
^ 
msSM 
rO 
rl 
r2 
r3 
r4 
r5 
r6 
r7 
rS 
r9 
rlO 
rll 
rl2 
rl3 
14 
pc 
;psr 
:^MM 
OxOOOOBOec 
OxOOOOBOef 
0x00000000 
0x00000000 
0x00000000 
0x00000000 
0x00000000 
0x00000000 
0x00000000 
0x00000000 
0x00000000 
0x00000000 
0x00000000 
0x00000000 
0x0000809c 
OxOOOOBOac 
SinZCVi f t„User32 
~':«p C3np:?{tfc 
iSfiSlnrtj i»/'r.'i_ra;=r.>/ara-Nc 
I I Q A H M 
Onhuggn 
Figure 9.18 The situation after executing several instructions. 
IT 
2 
3 
4 
S 
6 
7 
8 
9 
i 10 
i 11 
! 12 
i 13 
; 14 
I 15 
i 16 
i 17 
) IB 
I 19 
! 20 
i 21 
i 22 
I 23 
! 24 
I 25 
: 26 
27 
! 28 
i 29 
I 30 
31 
32 
33 
IE 
2 
! 3 
r4 
5 
I 6 
: 7 
S 
: 9 
10 
! 11 
I 12 
I 13 
i 14 
I 15 
1 16 
17 
IB 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
llu 
•t-1 f| = 1 = q t i r r e a s 
f l a t i 
MOV 
r 1 n #n.y 1 
B!e 
Edit 
Search 
View 
Execute 
Options 
^Vmdow 
jHetp 
g) 
i- |--|gi| j&i»t| :j?»|ffi|tFN| a|B|a|pla|_ 
ARWuiote 
AREA palindrome, 
ENTRY 
CODE, READONLY 
start 
stop 

9.3 The ARM—an elegant RISC processor 
395 
K^McpehugaertDaARMZJIIiRrdgsVpipgl;; 
Bte 
£dif 
Search 
^tew Egsajfe 
Options 
Wmdow 
£jeip 
l-|g|xl 
^^x6cMliph)Wipddw-s 'imgl^y^ ^yM^SUirlt/'/Srt'^^iy «a;a*lit*#WSIK«SaS!?iBHH 
i 
AREA palindrome, CODE. READONLY 
* f 
23 
2 
3 
4 
EOTRY 
READONLY 
* f 
23 
2 
3 
4 
start 
;locate the ends of the strinc 
S 
LDR 
r0,^string 
;r0 points to start of string 
6 
MOV 
rl.rO 
;copy left pointer to right pc 
7 
loop 
LDRB 
r2.[rl],#l 
;get char and update right poi 
8 
CMP 
r2.#0 
;repeet until terminator locat 
9 
BNE 
loop 
10 
SUB 
rl,rl.#2 
;fxx right pointer to point tc 
11 
12 
13 
BL 
pal 
;call subroutine to test for i 
11 
12 
13 
stop 
MOV 
rO.MxlB 
;stop program execution 
14 
LDR 
rl,-0x20026 
15 
SWI 
0x123456 
16 
;test for palindrome 
1? 
;r0 points at left hand end ot 
18 
;rl points at right hand end c 
19 
pal 
MOV 
r10,#0x0 
;r!0 = Palindnfie - 0 = set fe 
20 
again 
LDRB 
r3.[r0] 
;get left hand character 
21 
LDRB 
r4.[rl] 
;get right hand character 
22 
CMP 
r3,r4 
;ccmpare the ends of the s t n r 
23 
BSE 
notpal 
;if different then fail 
24 
;test for middle of string 
25 
CMP 
rO.rl 
;if rO = rl then odd length pe 
26 
BEQ 
waspal 
;if same then exit with p e l m c 
27 
ADD 
r2.r0.#l 
;copy left pointer to r2 and n 
28 
CMP 
r2.rl 
;if r2 = rl then even length i 
29 
BEQ 
waspal 
;if same then eKit with p a l m c 
30 
31 
ADD 
r0,r0,#l 
;move left pointer right 
32 
SUB 
rl.rl.#l 
;move right pointer left 
33 
B 
again 
;REPEAT 
34 
r „ - = n^ 1 Mnv 
±n 
JJ <l 
1 
j±S 
Step completed 
SBSlartl gyMiaosoftWord-NB.. 
M' 
I A R M Debugger.. 
r9 
no 
rll 
rl2 
rl3 
rl4 
pc 
cpsr 
^JsJiSl 
:-x;oc.u=caf 
OxOOOOSQed 
0x0000004a 
0x0000004e 
,0x00000000 
0x00000000 
0x00000000 
0x00000000 
0x00000000 
0x00000000 
0x00000000 
0x00000000 
0x00000000 
0x0000809c 
OxOOOOBObc 
'•inZCvi f t„User32 
3 
Figure 9.19 The situation after executing part of the subroutine pal. 
SB ARM -DeBUqqiM SD;^ABM21 mProgsVprpril;; 
E3M3 
File Edit Search 
m 
i-i r 
Execute QpSons Window Help 
CODE, READONLY 
; locate the ends of the strint 
;rQ points to start of string 
;copy left pointer to right pr 
;get char and update right poi 
;repeat until terminator lucat 
;fix right pointer to point tc 
;call subroutine to test for j: 
;stop program execution 
test for palindrome 
r0 points ot left hand end of 
rl points at right hand end c 
rlO = Palindrme = 0 = set it 
get left hand character 
get right hand character 
compare the ends of the s 
if different then fail 
test for middle of string 
if rO = rl then odd lengt 
if same then exit with ^3 
copy left pointer to r2 a 
if r2 = rl then even leng 
if same then exit with pa 
move left pointer right 
move right pointer left 
r& 
r? 
rB 
r9 
no 
rll 
rl2 
pc 
cpsr 
0x00006Oed 
OsOOOOBDee 
0%000080ed 
0sOOOO0D4« 
:0x0000a03e 
O K O O O D O O O O 
0x00000000 
OsOOOOGOOO 
•OsOOOOOOOO 
0x00000000 
OsQOOOOOOO 
0200000000 
0x00000000 
OKOOOOOODO 
0ii0000809c 
.:Qs000Q8Dbe 
?inZCvift_User32 
""*! 
J 
jSStartjigARM Debugger.. ayMigoso«Word-te | 
Figure 9.20 Using the View function to select the Disassembly display 
p ^ * " i i ! 
Search Paths 
Ctt-P 
2 
Source Files 
Grf-F 
3 
Fraction Names 
Gri-N 
4 
= 
Backtrace 
Ctri-T 
5 
o 
Memory. 
Grf-M 
7 
1 
gisessembry- 
CH-D 
q 
Low Level Symbofs CW-5 
j ° 
greakpoirts 
OfrB 
12 
^alchpDinfs 
GW-W 
13 
3 
14 
Console 
, 
15 
Command 
; IS 
RDi Protocol Log 
Debugger Internals 
H 
P v Status Bsr 
20 
a , T 
,. 
-. j 
^ looibar 
22 
CMP 
r 3 , r 4 
I 23 
BKE 
n o t p a l 
24 
; 2S 
. 
CMP 
r 0 , r l 
\- 26 
. 
BEQ 
waspal 
127 
ADD 
r 2 , r 0 , # i 
\ 28 
CMP 
r 2 , r l 
: 29 
BEQ 
waspal 
; 30 
: 
! 31 
ADD 
r O , r Q , # l 
32 
SUB 
r l , r l . # l 
i 33 
B 
a g a i n 
34 
m 
un = nr*l 
MflV 
-rm ^flv 1 
liii 
_ilm 
i 
 

396 
Chapter 9 Processor architectures 
b'o Ed.' Seo-Ci ••£-" ErtaH 
Op-=r.s » i j » 
= f p 
HI 
hil 
.El. 
IftMfrl-pl aiBlaldlBl 
i: 
^J 
LDR 
MOV 
LORB 
CMP 
BME 
SUB 
BL 
MOV 
LDR 
SWI 
rO,=string 
rl,rO 
r2,[rlj.#l 
r2,#0 
loop 
rl„rl,#2 
rO.#0xlS 
rl,=0x2002* 
0x123456 
;locste the ends of the strmc 
rO points to start of string 
copy left pointer to right pc 
get char and update right poi 
repeat until terminator loeat 
;fi 
Enter eddiess for dissasembV 
r i g h t p o i n t e r t o p o i n t tc 
MOV 
r l 0 , # 0 x Q 
LDRB 
r 3 , [ r O J 
LDRB 
r 4 , [ r l ] 
r 3 . r 4 
n o t p a l 
m" -" 
- " . _ ia?x| 
10 
nxOOOOBOed 
d 
rJ 
OxOOOOSOef* 
d 
r2 
OxQOOOBOed 
r 3 
0x0000004e 
r4 
OxQ000004w 
rS 
0x00000000 
r6 
0x00000000 
r7 
0x00000000 
r 8 
0x00000000 
r9 
0x00000000 
rlO 
0x00000000 
r l l 
3x00000000 
r l 2 
0x00000000 
r l 3 
0x00000000 
r H 
0x0000809= 
pc 
0x000080bc 
s p s r 
^nZCvif t J J s e : 
zl 
CMP 
BNE 
CMP 
BEQ 
ADD 
CMP 
BEQ 
ADD 
SUB 
MilV 
r O , r l 
waspal 
r 2 , r D , # l 
r 2 . r l 
wagpa1 
r0,rQ-#l 
rl,rl,#l 
again 
;if different then fail 
;test for middle of string 
;if r0 " rl then odd length 
;i£ same then exit with pali 
;copy left pointer to r2 and 
;if r2 = rl then even length 
;if same then exit with pali 
;move left pointer right 
;move right pointer left 
;REPEAT 
For Help, press Ft 
^3Siartj|g|ARM Debugger.-. 
^MiaosoSWord-Ne... 
Figure 9.21 Selecting the point at which to start disassembly. 
(S^^f^^gSmSi^i^imw'miB^^oMomiOii'i 
[ 0 8 
£tiii 
Search 
Jjfiaw 
Execute 
Options 
W^dow 
Help 
LJM*J 
e*l ••i-)rm 
« M :;|ft|o>|#IM O|0|D|C3|.S| -i| 
s t a r t 
l d r 
r 0 , 0 x 0 0 0 0 8 0 a 4 
; - 
# _ e t o x t 
d 
0 x 0 0 0 0 8 0 8 4 
KlOV 
r i , r 0 
d 
l o o p 
Xdrb 
r 2 . [ r l ] , # l 
0 x 0 0 0 0 8 0 8 c 
cmV 
r 2 , # 0 
0 x 0 0 0 0 8 0 9 0 
b n e 
l o o p 
0 x 0 0 0 0 8 0 9 4 
s u b 
r l . r l . # 2 
0 x 0 0 0 0 8 0 9 8 
M 
p a l 
s t o p 
ffiOV 
r 0 . # 0 x l 8 
OxOOOOBOaO 
l d r 
rl.OxOOOOBOeB : - 
# 0 x 0 0 0 2 0 0 2 6 
0 x 0 0 0 0 8 0 a 4 
s w i 
0 x 1 2 3 4 5 6 
p a l 
mov 
r l O . # 0 
a g a i n 
l d r b 
r 3 , [ r 0 , # 0 ] 
Ox000080bO 
Idrfo 
r 4 , [ r l , # 0 ] 
0 x 0 0 0 0 8 0 b 4 
cmp 
r 3 , r 4 
0 x 0 0 0 0 8 0 b 8 
b n e 
n o t p a l 
0 x 0 0 0 0 8 0 b c 
cmp 
r O . r t 
OxOOOOBOcO 
b©q 
w a s p a l 
0 x 0 0 0 0 8 0 c 4 
add 
r 2 . r 0 . « 
OXOOOOBDCB 
: 
cmp 
r 2 , r l 
OxOQOOBOcc 
beef 
w a s p a l 
_ J 
OxOOOOSOdO 
a d d 
r O . r O . M 
0 x 0 0 0 0 8 0 d 4 
i 
s u b 
r l . r l , # l 
OxOOOOBOdS 
b 
a g a i n 
w a s p a l 
mov 
r l 0 . « 
n o t p a l 
ffiOV 
p c . r H 
0 x 0 0 0 0 8 0 e 4 
a n d e q 
r 8 . r 0 , r l 2 . r o r 
# 1 
0 x 0 0 0 0 8 0 e 8 
a n d e q 
r 0 , r 2 , r 6 . 1 s r # 3 2 
e t e x t 
rarsmi 
r f . s p s r 
; ( ? ) 
OxOOOOBOfO 
a n d e q 
r O . r O . r O 
_emd 
a n d e q 
r 0 , r 0 , r 0 
OxOOOOBOfB 
a n d e q 
r 0 , r 0 , r 0 
OxOOOOSOfc 
a n d e q 
r O . r O . r O 
0 x 0 0 0 0 8 1 0 0 
a n d e q 
r 0 . i - 0 . r 0 
0 x 0 0 0 0 8 1 0 4 
a n d e q 
r O . r O . r O 
0 x 0 0 0 0 8 1 0 8 
a n d e q 
r O . r O . r O 
0 x 0 0 0 0 8 1 0 c 
a n d e q 
r O . r O . r O 
0 x 0 0 0 0 8 1 1 0 
a n d e q 
r O . r O . r O 
0 x 0 0 0 0 8 1 1 4 
a a d e q 
r O . r O . r O 
-rf 
ill 
2l .«l '.. 
. . 
JjJ 
FofHe!p. press F1 
a S « n t | | ^ A R M Debuggar..- j g Microsoft Word-Ne...| 
iilrf.33 Z'«PM 
Figure 9.22 The disassembled code. 
! 1 
2 
; 3 
! 4 
! 5 
6 
7 
i-8 
9 
' 10 
M l 
12 
13 
: 14 
15 
16 
17 
I IS 
19 
20 
21 
22 
23 
24 
Sill 
26 
I 2 7 
28 
29 
30 
; 31 
32 
33 
: 34 
start 
loop 
stop 
pal 
again 
A R E A p a l i n d r o m e , C O D E , R E A D O N L Y 
FKrrBV 
-rift = 1 = ^nr-f-05^ Plfl« 
0K 1 
Cancet 
ARMutete 
'•&$.•% 
tarn 

9.3 The ARM—an elegant RISC processor 
397 
Note that the symbolic labels are displayed, although the text 
string is interpreted as instructions. 
Simplifying the code 
We can simplify the code we've developed to test for a palin-
drome; that's one of the advantages of writing a program in 
assembly language. The following provides an improved ver-
sion (without the header, data, and termination mechanism, 
which don't change). 
We test two characters and then branch to notpal if they 
aren't the same. From notpal, we perform a return by plac-
ing the return address in the link register into the pc. Steve 
uses conditional execution to combine these two instruc-
tions; that is, 
CMP 
r3,r4 
;compare the ends of the string 
MOVNE pc,lr /if not same then return 
start 
LDR 
rO,=string 
MOV 
rl,rO 
loop 
LDRB 
rlO,[rl],#l 
CMP 
rlO,#0 
BNE 
loop 
SUB 
rl,rl,#2 
BL 
pal 
stop 
pal 
LDRB 
r3,[rO],#l 
LDRB 
r4,[rl],#-l 
CMP 
r3,r4 
BNE 
notpal 
SUBS 
r3,rl,r0 
BEQ 
waspal 
BMI 
waspal 
B 
pal 
waspal 
MOV 
rlO,#Oxl 
notpal 
MOV 
pc, lr 
,-rO points to start of string to test 
;copy left pointer to right pointer in rl 
;get character and update right pointer 
/repeat until terminator located 
;fix right pointer to point to end of string 
;call subroutine to test for palindrome 
;stop program execution 
;get left hand character 
;get right hand character 
/compare the ends of the string 
;if different then fail 
;get difference between pointers 
;if same then exit with palindrome found 
;if left pointer past right then palindrome 
;REPEAT 
;rlO = 1 = success flag 
;return 
We've used two improvements. The first is to use rlO (the 
success/fail flag) to test for the terminator at the end of the 
string. In this way, we begin the sub-
routine with [rlO] = 0 and save an 
P a l 
instruction. The major change is in the 
test for the middle of the string. If we 
automatically 
increment 
the 
left 
pointer and decrement the right 
pointer when they are used, we will 
have one of two situations when we 
reach the middle. If the string is even, 
the left and the right hand pointers will 
have swapped over. If the string is odd, the two pointers will be 
pointing at the same character. The code subtracts the left 
pointer from the right pointer and stops on zero or negative. 
Further simplification 
Steve Furber at Manchester University pointed out that the 
code can be simplified even further. Look at the way I handled 
a return if the string wasn't a palindrome. 
Steve's final version is 
LDRB 
r3,[rO],#l 
LDRB 
r4,[rl],#-l 
CMP 
r3,r4 
MOVNE 
pc, lr 
CMP 
rO,rl 
BMI 
pal 
MOV 
rlO,#l 
MOV 
pc, lr 
ers will be • SUMMARY 
CMP 
BNE 
notpal MOV 
r3,r4 /compare the ends of the string 
notpal /if different then fail 
pc,lr /return 
;get left hand character 
/get right hand character 
/compare the ends of the string 
/if not same then return 
/compare pointers 
/not finished 
/return (success) 
When we first introduced the computer, we used Motorola's 
68K as a teaching vehicle because it is both powerful and easy 
to understand. In this chapter, we have looked at two contrast-
ing microprocessors; a simple 8-bit device used in devices rang-
ing from toys to cell phones, and a more sophisticated 32-bit 
RISC processor, the ARM. 
The 8-bit M68HC12 looks very much like the first-generation 
processors that go back to the late 1970s. These 
processors have relatively few internal registers and 
you have only two general-purpose accumulators. 
However, their processors have a wealth of on-chip 
I/O ports, which means that they provide a single-
chip solution to many computing problems. 

398 
Chapter 9 Processor architectures 
The ARM processor is a 32-bit machine with a register-
to-register (or load/store) architecture with instructions tike 
ADD r l , r 2 , r3.We introduced the ARM because it has some 
very interesting features. The program counter is one of the 
processor's general-purpose registers, which means that the pro-
grammer can access the PC like any other register. This feature 
can be exploited in returning from a subroutine because you can 
transfer the return address to the PC without having to perform 
a memory access. 
Another feature of the ARM is its ability to shift the second 
operand as part of a normal data processing instruction. This 
mechanism provides a limited degree of parallel processing 
because you can execute two instructions at once (provided one 
is a shift). 
One of the most interesting features of the ARM is its condi-
tional execution, where an instruction is executed only if a con-
dition is met.This facility makes it possible to generate very 
compact code. 
* 
PROBLEMS 
9.1 What are the advantages and disadvantages of 
microprocessor wordlengths that are not powers of 2 
(e.g. 12 bits and 24 bits)? 
9.2 We said that all processors permit register-to-memory, 
memory-to-register, and register-to-register moves, whereas 
few microprocessors permit direct memory-to-
memory moves. What are the advantages and dis-
advantages of direct memory-to-memory moves? 
L D M I A , 
9.3 Some computers have a wide range of shift 
operations (e.g. logical, arithmetic, and rotate). Some computers 
have very few shift operations. Suppose that your computer had 
only a single logical shift left operation. How would you synthe-
size all the other shifts using this instruction and other appropri-
ate operations on the data? 
9.4 Some microprocessors implement simple unconditional 
procedure (i.e. subroutine) calls with a BSR (branch to subrou-
tine) instruction. Other microprocessors have a conditional 
branch to subroutine instruction that let's you call a subroutine 
conditionally. What are the relative merits and disadvantages of 
these two approaches to instruction design? 
9.5 Some registers in a microprocessor are part of its architec-
ture which is visible to the programmer, whereas other registers 
belong to the processor's organization and are invisible to the 
programmer. Explain what this statement means. 
9.6 The MC68HC12 instruction set of Table 9.2 has a very large 
number of instructions. Design a new instruction set that per-
forms the same operations but uses fewer instruction types (e.g. 
employ a MOVE instruction to replace many of the 6809's exist-
ing data transfer instructions). 
9.7 What are the relative advantages and disadvantages of vari-
able-length instructions (in contrast with fixed-length instructions). 
9.8 In what significant ways does the ARM differ from the 68K? 
9.9 Most RISC processors have 32 user-accessible registers, 
whereas the ARM has only 16. Why is this so? 
9.10 Construct an instruction set that has the best features of a 
CISC processor like the 68K and a RISC processor like the ARM. 
Write some test programs for your architecture and compare 
them with the corresponding pure 68K and ARM programs. 
9.11 All ARM instructions are conditional, which means that 
they are executed only if a defined condition is met; for exam-
ple, ADDEQ means 'add if the last result set the zero flag'. 
Explain how this feature can be exploited to produce very 
compact code. Give examples of the use of this feature to 
implement complex conditional constructs. 
9.12 What is the effect of the followingARM instructions? 
(a) MOV rl, #0xFF 
(b) MVN rl, #0xFF 
(c) MVN rl, #25 
(d) MVN rl, #0xF 
(e) MOVS rl, #0xF 
(f) MLA r3, r5, r6 
(g) LDR rl, [r3,#8 
(h) LDR rl, [r3,#8 
r2 
9.13 The ARM has a wealth of move multiple register instruc-
tions, which copy data between memory and several registers. 
The load versions of these instructions are 
LDMIB, LDMDA, LDMDB, LDMFD, LDMFA, LDMED, LDMEA 
What do these instructions do? You will need to look up ARM 
literature to answer this question. 
9.14 How are subroutines handled in ARM processors? 
9.15 Implement a jump table in ARM assembly language. A 
jump table is used to branch to one of a series of addresses 
stored in a table. For example, if register r3 contains the value /, 
a jump (i.e. branch) will be made to the address of the /th entry 
in the table. Jump tables can be used to implement the case or 
switch construct in high-level languages. 
9.16 Consider the fragment of C-code i f 
(p = 
= 0 ) 
q = q l l ; e l s e q = 
q * 4 ; 
How can conditional execution be exploited by the compiler for 
this code? 
9.17 A 32-bit IEEE floating point number is packed and con-
tains a sign bit, biased exponent, and fractional mantissa. Write 
an ARM program that takes a 32-bit IEE floating point number 
and returns a sign-bit (most significant bit of r1), a true expo-
nent in r3, and a mantissa with a leading 1 in register r3. 
Write a program to convert an unsigned 8-digit decimal inte-
ger into a 32-bit IEEE floating point number.The 8-digit decimal 
integer is stored at the memory location pointed at by r1 and 
the result is to be returned in r2.The decimal number is right 
justified and leading digits are filled with zeros; for example, 
1234 would be stored at 00001234. 

Buses and input/output mechanisms 
INTRODUCTION 
Computers receive data from a wide variety of sources such as the keyboard and mouse, the 
modem, the scanner, and the microphone. Similarly, computers transmit data to printers, 
displays, and modems. Computer peripherals can be discussed under two headings. The first 
is the techniques or strategies whereby information is moved into and out of a computer 
(or even within the computer).The second is the peripherals themselves; their characteristics, 
operating modes, and functions. We first look at the way in which information is moved 
into and out of the computer and in the next chapter we describe some important 
peripherals. 
We begin with the bus, the device that distributes information within a computer and between 
a computer and external peripherals. We describe both high-speed parallel buses and slower, 
low-cost buses such as the USB bus that connects keyboards and similar devices to the computer. 
We introduce a very unusual bus, the IEEE488 bus, which illustrates many important aspects of 
I/O technology. 
The middle part of this chapter looks at the strategies used to implement I/O such as programmed 
I/O and interrupt-driven I/O. 
This chapter concludes with a description of two peripheral chips that automate the 
transmission of data between a computer and peripheral. One interface chip handles parallel 
data and the other serial data. The precise details of these chips are not important. Their 
operating principles are because these chips demonstrate how a lot of the complexity 
associated with input and output transactions can be moved from the CPU to an interface. 
CH-fTiKHA!1 
9 Processor f 
i architectures 
Chapter 9 provides a brief j 
overview of two contrasting i 
processors; the purpose of this > 
• chapter is to expose students to | 
the range of processors that ate { 
. , available to the d 
11 Computer peripherals 
The power of a computer is much 
a function of as p*rlphwals as its 
data processing capabilities. We 
examine several peripherals 
found in a typical PC such as the 
keyboard, display, printer, and 
mouse, as well as some of the 
more unusual peripherals that, 
for example, car. measure how 
fast a body is rotating. 
2 Computer memory 
Information isn't stcrei! in a 
computer in ji«i crse type cf 
storage device; it's stored in 
CRAM arid on disk. CD-RO"", 
DVD, and tape, lliis chapter 
examines the opcialing principles 
and characteristics of ine storage 
devices found in a computer 
10 Buses and input/ 
output mechanisms 
This ch<spiei ciea!> wiih 
inpul/outpui tetlviiquei.\Wc are 
interested n ho.v nfo'nijtitri := 
transferred bt-lwce-i a conputei 
and peripherals and bor-.v<..?n the 
pen;me'ais ihpmsoivc^.vVc: look 
at interna', tiuws. which link 
c!e>iC£S v.-^ihin t>*e ccn-oi.ier and 
external bii>.e5. whi'h l;nk remov 
de-.'rics inch as srinteis v;.th the 
computer. We aiso describe two 
pe>i?herri'. i:iterfa:i" components 
that perform :/G ctr-r-H'iin.-. 
autonomously by performing 
nia.ny 2' the ac: v'-iie-, required to 
mo-.v cieta between a computer 
and oMemul pel iphc-rals 

400 
Chapter 10 Buses and input/output mechanisms 
10.1 The bus 
We've examined the internal structure and operation of the 
computer's central processing unit. The next step is to show 
how the computer communicates with the outside world. 
In this chapter we look at how information gets into and out 
of a computer; in the next chapter we turn our attention to 
devices like the printer and the display that are connected to 
the computer. 
This chapter begins with the bus that distributes informa-
tion both within a computer and between a computer and 
external devices. We then demonstrate how the CPU imple-
ments input and output transactions—the CPU doesn't 
dirty its hands with the fine details of input/output (I/O) 
operations. The CPU hands over I/O operations to special-
purpose interface chips; for example, the computer sends 
data to one of these chips and logic within the chip handles 
the transfer of data between the chip and the external device. 
We describe the operation of two typical interface chips— 
one that handles I/O a byte (or a word) at a time and one that 
handles I/O a bit at a time. 
access to the bus, and an interrupt bus that deals with requests 
for attention from peripherals. The data transfer bus is, itself, 
composed of sub-buses; for example, there's an address bus 
to communicate the address of the memory location being 
accessed, a data bus to carry data between memory and CPU, 
and a control bus, which determines the sequence of opera-
tions that take place during a data transfer. 
Buses are optimized for their specific application; for 
example, speed (throughput), functionality, or cost (e.g. the 
USB bus). A computer such as the PC may have several buses. 
Figure 10.2 illustrates the structure of a PC with buses that 
are linked by bridges (i.e. circuits) that control the flow 
of traffic between buses that might have widely different 
parameters. 
In Fig. 10.2 a system bus links together the processor and 
its memory. This is the fastest bus in the system because the 
computer cannot afford to wait for instructions or data from 
memory. The system bus is connected to a local bus that deals 
with data transfers between slower devices such as audio 
subsystems or interfaces to external peripherals. A logic 
system that may be as complex as a CPU is used to connect 
the system bus to the local bus. 
10.1.1 Bus architecture 
A bus is used by the processor and peripherals to move data 
from point to point in a computer. An important feature 
of some buses is their ability to allow several devices with 
different characteristics to communicate with each other 
over a common highway. 
Figure 10.1 illustrates a generic computer bus that is 
composed of several sub-buses; there's a data transfer bus that 
transfers data between the CPU and memory or peripherals, 
an arbitration bus that allows one or more CPUs to request 
10.1.2 Key bus concepts 
Before we look at buses in greater detail, we need to introduce 
concepts that are intimately bound up with the way in which 
both buses and other I/O mechanisms control the flow of 
data. The most important concept is that of the open- and 
closed-loop data transfer. 
Irrespective of the strategy by which data is moved 
between the processor and peripheral, all data transfers fall 
into one of two classes: open-ended or closed-loop. In an open-
ended I/O transaction the data is sent on its way and its safe 
Data transfer bus 
transfers information between 
CPU and memory 
The data transfer bus 
consists of three 
sub-buses 
System bus 
Interrupt bus 
Figure 10.1 Structure of a generic bus. 
\ 
Arbitration bus 
Address 
Data 
Control 

10.1 The Bus 
401 
reception assumed. Open-ended data transfers correspond to 
the basic level of service offered by the mail system. A letter is 
written and dropped into a mailbox. The sender believes that 
after a reasonable delay, the letter will be received. However, 
the sender doesn't know whether the letter was received. 
In many circumstances the open-ended transfer of data is 
perfectly satisfactory. The probability of data getting lost or 
corrupted is very small and its loss may be of little importance. 
If Aunt Mabel doesn't get a birthday card, the world doesn't 
come to an end. Consider now the following exchange of 
information between a control tower and an aircraft. 
Approach control 'Cherokee Nine Four Six November 
cleared for straight in approach to runway 25. Wind 270 
degrees 10 knots. Altimeter 32 point 13. Report field in sight.' 
Aircraft 'Straight in runway 25. 32 point 13. Cherokee Nine 
Four Six November.' 
Processor 
c 
Cache 
memory 
Main 
memory 
Video 
System bus 
The system bus is fast 
because it has to handle 
CPU to memory transfers 
} 
Scanner 
Audio 
V 
Bridge 
c 
0 If 
The bridge allows 
signals on one bus 
to be transferred 
to another bus. 
Locat Bus 
t> 
•-The local bus handles 
slower data transfers 
between the CPU 
and peripherals 
Figure 10.2 A system with multiple buses, 
(a) Physical arrangement. 
Data 
The aircraft acknowledges receipt of the message and reads 
back any crucial data (i.e. the identification of the runway 
is 25 and the altimeter pressure setting is 32.13 inches of 
mercury). This data transfer demonstrates the operation of 
a closed-loop system. In the computer world, a closed-loop 
data transfer simply indicates that data has been received 
(the data itself isn't read back). 
Open-loop data transfer 
Figure 10.3 illustrates an open-loop data transfer between 
a computer and a peripheral. Figure 10.3(a) shows a computer 
and peripheral with a data path and a 1-bit control signal, 
DAV, Fig. 10.3(b) gives a timing diagram for an open-loop 
write in which data is sent from die computer to the peripheral, 
and Fig. 10.3(c) provides a transaction of protocol diagram 
that presents the sequence of actions in the form of messages. 
At point A data from the computer becomes valid (the 
shading before point A indicates that the data is invalid). 
At point B the computer asserts the DAV (data valid) control 
signal to indicate that the data from the computer is valid. 
The peripheral must read the data before it vanishes at point 
D. DAV is negated at point C to inform the peripheral that 
the data is no longer valid. This data transfer is called open 
loop because the peripheral doesn't communicate with the 
CPU and doesn't indicate that it has received the data. 
Closed-loop data transfer 
In a closed-loop data transfer, the device receiving the data 
acknowledges its receipt. Figure 10.4 illustrates a closed-loop 
data transfer between a computer and peripheral. Initially, 
the computer (i.e. originator of the data) makes the data 
available and then asserts data DAV at point B to indicate that 
the data is valid just as in an open-loop data transfer. The 
peripheral receiving the data sees that DAV has been asserted, 
indicating that new data is ready. The peripheral asserts its 
acknowledgement, DAC (data accepted), at point C and reads 
Computer 
DAV 
J Peripheral 
T 
DAV tells the peripheral 
that data is valid 
Data 
DAV 
(b) Timing diagram. 
B 
•time 
C 
Data valid 
r^ X 
Data 
' removed 
Computer 
A 
(c) Transaction (protocol) diagram. 
Peripheral 
Data 
available 
Data 
invalid 
time 
Figure 10.3 Open-loop data 
transfer between computer 
and peripheral 
Data is/ 
valid (_ 
______Data 
•" 
•—2£Vasserted^ 
—_°d^HSgated^ 
-—Pata removed 
B 
C 
D 

402 
Chapter 10 Buses and input/output mechanisms 
(a) Physical arrangement 
Computer 
Data 
DAV 
Peripheral 
DAC 
(c) Transaction (protocol) diagram 
Data 
accepted 
Figure 10.4 Closed-loop data transfer between computer and peripheral 
(b) Transaction (protocol) diagram 
time 
the data. The data accepted signal is a reply to the computer 
informing it that the data has been accepted. Once the data 
has been read by the peripheral, the DAV and DAC signals 
may be negated and the data removed. This sequence of 
events is known as handshaking. Apart from indicating the 
receipt of data, handshaking also caters to slow peripherals, 
because the transfer is held up until the peripheral indicates 
its readiness by asserting DAC. 
Figure 10.5 shows how the handshaking process can be 
taken a step further in which the acknowledgement is itself 
acknowledged, to create a fully interlocked data transfer. The term 
fully interlocked means that each stage in the handshaking 
Computer 
Peripheral 
A I 
— 
gata_____^ 
B 
DAVasserted_____^ 
DAC_asserted 
c 
DAC negated 
D 
time 
«— 
" 
E 
___£AV£egated___^ 
F 
-—_^ata_rernoved~ 
" 
* 
Computer 
Peripheral 
A Y~ 
_Data____^ 
B 
2AVasserted______^ 
DAC asserted 
c 
*— 
D • 
DAVnegated 
° 
DAC neaarted 
E 
+ 
— 
' 
F 
SfiiCernoved 
(a) Timing diagram 
•Time 
A 
B 
C 
D 
E 
F 
Data 
X-N 
Data valid 
X 
Data is/ 
Data is 
_., , 1 valid V. 
_ 
removed 
DAV o 
K 
r-"^-
Data 
j 
\ Datja valid 
available 
\negatfed 
1 
V_ J I 
DAC 
^ - - — • - ^ 
V_^. 
0 
Data 
Data accepted 
accepted 
negated 
Figure 10.5 Fully interlocked handshaking. 
(b) Timing diagram 
•Time 
A 
B 
C 
D E 
F 
Data ^ ( 3 
Data valid 
~ 1 ( ~ 
Data is/ 
1 valid V. 
Asserted 
DAV 
• - . 
0 
1 | 
Negated 
Data 
available\ 
1 
\ 
Asserted 
DAC 
• 
0 
1 
Negated 
Data 
accented 
Data 
Data valid 

10.1 The Bus 
403 
HANG UPS 
In data transfers with handshaking, a problem arises when the 
transmitter asserts DAV, but DAC isn't asserted by the receiver 
in turn (because the equipment is faulty or the receiver is not 
switched on). When the transmitter wishes to send data, it 
starts a timer concurrently with the assertion of DAV. If the 
receiver doesn't assert DAC after a given time has passed, the 
operation is aborted. The period of time between the start of 
an action and the declaration of a failure state is called a 
timeout. 
When a timeout occurs, an interrupt (see Section 10.2.2) is 
generated, forcing the computer to take action. In a poorly 
designed system without a timeout mechanism, the 
non-completion of a handshake causes the transmitter to 
wait for DAC forever and the system is then said to hang up. 
procedure can continue only when the previous stage has 
been acknowledged. At point A in Fig. 10.5 the data becomes 
valid and at point B the transmitter asserts DAV indicating 
the availability of data. At C the receiver asserts DAC indicat-
ing that DAV has been observed and the data accepted. So far 
this is the same procedure as in Fig. 10.4. 
The transmitter sees that DAC is asserted and de-asserts 
(i.e. negates) DAV at D, indicating that data is no longer valid 
and that it is acknowledging that the receiver has accepted 
the data. Finally, at E the receiver de-asserts (i.e. negates) DAC 
to complete the cycle, and to indicate that it has seen the 
transmitter's acknowledgement of its receipt of data. 
The difference between the handshaking and fully inter-
locked handshaking of Figs. 10.4 and 10.5 should be stressed. 
Handshaking merely involves an acknowledgement of data, 
which implies that the assertion of DAV is followed by the 
assertion of DAC. What happens after this is undefined. In 
fully interlocked handshaking, each action (i.e. the assertion 
or negation of a signal) takes place in a strict sequence 
that ends only when all signals have finally been negated. 
Interlocked handshaking is a two-way process because the 
receiver acknowledges the assertion of DAV by asserting DAC 
whereas the transmitter acknowledges the assertion of DAC 
by negating DAV. Moreover, because fully interlocked hand-
shaking also acknowledges negations, it is said to be delay 
insensitive. 
Many real systems employing closed-loop data transfers 
make the entire handshaking sequence automatic in the 
sense that it is carried out by special-purpose hardware. The 
computer itself doesn't get involved in the process. Only if 
something goes wrong does the processor take part in the 
handshaking. 
How fast should an interface operate? As fast as it can—any 
faster and it wouldn't be able to keep up with the data—any 
slower and it would waste time waiting for data. Unfortunately, 
most real interfaces don't transfer data at anything like an 
optimum speed. In particular, data can sometimes arrive so 
fast that it's impossible to process one element before the next 
is received. 
Doctors have a similar problem. If a doctor took exactly m 
minutes to treat a patient and a new patient arrived every m 
minutes, all should be well. However, even if patients arrive 
on average every m minutes and a consultation takes on 
average m minutes, the system wouldn't work because some 
patients arrive at approximately the same time. Doctors have 
solved this problem long ago by putting new patients in a 
waiting room until they can be dealt with. Sometimes the 
waiting room becomes nearly full when patients enter more 
rapidly than average. 
The solution used by doctors can be applied to any I/O 
process. Data is loaded into a FIFO (first-in first-out) memory 
that behaves almost exactly like a waiting room. Data arrives at 
the memory's input port and is stored in the same sequence 
in which it arrives. Data leaves the memory's output port when 
it is required. Like the doctor's waiting room, the FIFO can fill 
with data during periods in which data arrives faster than it can 
be processed. It's up to the designer to provide a FIFO with 
sufficient capacity to deal with the worst case input burst. There 
is, however, one significant difference between the FIFO and 
the waiting room. FIFOs aren't littered with piles of battered 
10-year-old copies of National Geographical. Saving data in a 
store until it is required is called buffering and the FIFO store is 
often called a buffer. Some interfaces incorporate a buffer into 
their input or output circuits to control the flow of data. 
Bus terminology 
Bus technology has its own vocabulary. Before we continue it's 
necessary to introduce some of the concepts and terminology 
associated with computer buses. 
Arbitration Arbitration is a process whereby a device on the 
bus competes with other devices for control of the bus and 
is granted access to the bus. A simple bus-based system with 
only one processor and no other bus master doesn't require bus 
arbitration because the CPU permanently controls the bus. 
Backplane Parallel buses fall into two groups: passive back-
planes and motherboards. A motherboard is a printed circuit 
board that includes the CPU and its associated circuitry; for 
example, the motherboard found in the PC. A backplane 
contains the bus and slots (sockets) into which modules such 
as memory cards, processors, and peripherals can be plugged. 
The backplane is passive because it provides information 
paths but not functionality; that is, there is no CPU or other 
subsystem on the backplane. A backplane is more versatile 
than a motherboard and is generally found in commercial or 
professional systems. 

404 
Chapter 10 Buses and input/output mechanisms 
Bandwidth The bandwidth of a bus is a measure of its 
throughput, the rate at which data is transmitted over the 
bus. Bandwidth is normally expressed in bytes/s and is pro-
portional to the width of the data bus; for example, if an 8-bit 
data bus can transfer 200 Mbytes/s, increasing the bus's width 
to 64 bits increases the bandwidth to 1.6 Gbytes/s. 
Bus architecture Just as we speak about processor architecture 
or memory architecture, we can refer to a bus's architecture. 
The architecture of a bus (by analogy with the CPU) is an 
expression of its functionality and how it appears to the user. 
Bus architecture includes a bus's topology, its data exchange 
protocols, and its functionality such as its arbitration and 
interrupt-handling capabilities. 
Bus contention When two or more devices attempt to 
access a common bus at the same time, bus contention takes 
place. This situation is resolved by arbitration, the process 
that decides which of the contenders is going to gain access to 
the bus. 
Bus driver Logic systems are wired to bus lines via 
gates. Special gates called bus drivers have been designed to 
interface the CPU to a bus or other logic. A bus driver is a dig-
ital circuit with the added property that its output terminal 
can provide the necessary voltage swing and current neces-
sary to drive a bus up to a 1 state or down to a 0 state. Bus dri-
vers are required because of the electrical characteristics of 
bus lines. 
Bus master A bus master is a device that can actively take 
control of a bus and use it to transfer data. CPUs are bus 
masters. A bus slave, on the other hand, is a device that is 
attached to a bus but which can only be accessed from a bus 
master. A bus slave cannot initiate a bus access. 
Bus protocol A bus is defined by the electrical characteristics 
of its signals (i.e. what levels are recognized as Is and 0s) and 
the sequence of signals on the various lines of the bus used to 
carry out some transaction. The rules governing the sequenc-
ing of signals during the exchange of data are known as a 
protocol. 
Bus termination A bus can be quite long and extend the 
width of a computer system. Signals put on the bus propagate 
along the bus at close to the speed of light (the actual speed is 
given by the electrical properties of the bus lines and insulators 
between them). When a pulse reaches the end of a bus, it may 
be reflected back towards its source just like a wave that hits 
the side of a swimming pool. If you place a terminating net-
work across the ends of a bus, it can absorb reflections and 
stop them bouncing from end to end and triggering spurious 
events. 
Bus topology The topology of a bus is a description of the 
paths that link devices together. 
Latency A bus's latency is the time the bus takes to respond 
to a request for a data transfer. Typically, a device requests the 
bus for a data transfer (or a burst of data transfers) and then 
waits until the bus has signaled that it is ready to perform 
the transfer. This waiting period is the bus's latency. 
Motherboard A motherboard is similar to a backplane 
because it contains a bus and sockets that accept modules 
such as memory and peripherals. The difference between a 
backplane and motherboard is that the motherboard is 
active; it contains a processor and control logic. Modern PCs 
have such sophisticated motherboards that they can operate 
without any cards plugged into the system bus because the 
motherboard implements I/O, sound, and even the video 
display. 
Multiplexed bus Some data transfer buses have separate 
address and data sub-buses; that is, the address bus sends 
the location of the next word to be accessed in memory and 
the data bus either transmits information to the memory 
in a write cycle or receives information in a read cycle. 
Some computer buses use the same lines to carry both 
addresses and data. This arrangement, called multiplexing, 
reduces the number of lines required by the bus at the 
expense of circuit complexity. A multiplexed bus works 
in two or more phases; an address is transmitted on the 
common address/data lines and then the same lines are 
used to transfer data. 
10.1.3 The PC bus 
You might think it would be easier to wire peripherals and 
memory direcdy to a PC's own address and data bus. Indeed, 
some single-board microcontrollers do take this approach. 
Connecting the processor to memory and peripherals is not 
viable in sophisticated systems for several reasons. First, a 
processor chip cannot provide the electrical energy to drive 
lots of memory or peripheral chips. Second, a bus can be 
standardized and equipment from different manufacturers 
plugged into it. Third, if we didn't have buses, all interface 
circuits would have to be modified whenever a new processor 
were introduced. 
A bus makes a computer system independent of processor, 
memory, or peripheral characteristics and allows independent 
development of CPU or processor technology. 
The history of the IBM PC and its clones is as much the 
history of its bus as its central processing unit. Indeed, the 
PC's bus structure has advanced more radically than its 

10.1 The Bus 
405 
Width 
64 
32 
16 
8 
Serial 
ISA 
PCI 
PCI-X 
MCA 
EISA 
VESA 
PCI 
ACP 
PCI-X 
MCA 
ISA 
PCI 
express 
1980 
Figure 10.6 PC bus history. 
1990 
>• Time 
2000 
2004 
processor architecture. Figure 10.6 describes some of the 
steps along the path of the PC's bus architecture. 
When the PC was first created, its bus was very limited 
in terms of its speed, width, and functionality. The original 
XT bus supported the Intel 8088, a processor with a 16-bit 
internal architecture and an 8-bit external data bus. The 
XT bus operated with a 4.77 MHz clock and could access 
1 Mbytes of memory. The 8088 was soon replaced by the 
8086, a processor with an identical architecture but with 
a true 16-bit data bus. A new version of the PC with a 
16-bit bus called ISA (Industrial Standard Architecture,) 
was created. 
As performance increased, the ISA bus rapidly became 
obsolete and was replaced by three competing buses forcing 
PC users to choose between one of these mutually incompat-
ible systems. IBM produced its high-performance propriety 
Micro Channel Architecture bus, which was protected by 
patents. This bus died because it was uncompetitive. Two 
other PC buses were the VESA and EISA buses. 
In 1992 Intel announced the PCI (peripheral interconnect) 
bus to provide higher performance, to provide a path for 
future expansion, and to gain control of the bus market. The 
PCI 2.0 bus was 32 bits wide and had a speed of 33 MHz. The 
original PCI 2.0 specification was replaced by the PCI 2.1 
specification and the PCI bus was so successful that it rapidly 
replaced all other buses in PCs.1 
In 2004 the PCI express bus was introduced. This is a major 
departure from conventional backplane buses because it 
uses a pair of serial data paths operating at 2.5 Gbytes/s in 
each direction. Such a pair of buses is called a lane and the 
PCI express may use multiple lanes to increase the overall 
data rate. 
Figure 10.7 illustrates the structure of the PCI bus in a 
typical PC system. The processor bus is also called the host 
bus, or in PC terminology, the front side bus. The logic system 
that connects the processor bus to the PCI bus is called a 
north bridge. The circuits that implement inter-bus interfaces 
in a PC environment have come to be known colloquially 
as chipsets. 
Figure 10.8 describes Intel's 875 chipset, which uses an 
82875 MCH chip to provide a north bridge interface between 
the processor and memory and AGP (the advanced graphics 
card slot that provides a fast dedicated interface to a video 
card) and an ICH5 chip, which provides an interface to the 
PCI bus, LAN, and other subsystems. This chipset provides 
much of the functionality that was once provided on plug-in 
PCI cards such as an audio interface, a USB bus interface, and 
a LAN interface. 
The PCI bus operates at clock speeds of 33 or 66 MHz 
and supports both 32- and 64-bit systems. Data can be 
transferred in an efficient high-speed burst mode by sending 
an address and then transferring a sequence of data bytes. 
A 64-bit-wide bus operating at 66 MHz can transfer data at 
a maximum rate of 66 X 8 = 528 Mbytes/s. 
The PCI supports arbitration; that is, a PCI card can take 
control of the PCI bus and access other cards on the PCI bus. 
We now look at the IEEE488 bus, which was designed for 
use in professional systems in commercial environments such 
' Computer buses did not originate in the PC world. Professional 
systems had long since used standardized computer buses such as 
Motorola's VMEbus or the Multibus. 

406 
Chapter 10 Buses and input/output mechanisms 
c 
c 
LAN 
adapter 
c 
Processor 
Cache 
memory 
A 
This hi^h "peed bus 
ut.- * , K.CPU. 
Processor bus 
v i> 
Host to PC 
bridge 
y 
Memory bus 
J> 
7\ 
PCi bus 
SCSI 
controller 
SI 
ISA bridge 
ISA bus 
ISA device 
DRAM 
o 
:> 
Audio 
The PC) bus is used 
to provide an interface 
to plug-in cards. 
The legacy bus is used to 
support older peripherals 
{.-''- O'cte "'"J.",). 
:> 
ISA device 
Figure 10.7 The PCI bus in a PC. 
Serial ATA ports 
_150 Mbyte/-; 
* • 
LAN interface 
Legacy ATA100 
(for compatibility with 
older systems) 
ICHS 
BIOS interface 
DDR 
Main memory 
6 channel audio 
133Mbytes/s 
PCI bus 
USB 2.0 ports 
RAID interface 
Figure 10.8 The Intel 875 PCI chipset. 
Pentium 4 
processor 
x 
6.4 Cbytes/s 
6.-1 Gbvtes/s/ 
( 
y..O Cbytes/s . 
« 
J 
AGP8X 
* 
• 
8E875P 
V 
I 
) 
MCH 
AGP vioeo port 
North bridge 
8E875P 
MCH 
r:or*M. • -

10.1 The Bus 
407 
LEGACY DEVICES 
The term legacy device describes facilities that were 
incorporated in all PCs but which have now become largely 
obsolete. For example, the ISA bus is obsolete. However, 
because there are many ISA cards such as modems still in 
existence, some PCs contain both PCI and ISA buses to 
enable users to keep their old modem cards. As time passes, 
fewer and fewer systems have ISA buses. 
Similarly, the growth of the high-performance and flexible 
USB interface has largely rendered the traditional serial 
and parallel PC interfaces used by modems and printers 
unnecessary.These interfaces are also called legacy 
devices and are omitted from many modern high-
performance PCs. 
as instrumentation and control. Some students may omit this 
section because the IEEE488 bus is very specialized—we have 
included it because it illustrates several important aspects 
of bus design and operation. 
10.1.4 The IEEE 488 bus 
The IEEE 488 bus dates from 1967 when the Hewlett Packard 
Company began to look for a standard bus to link together 
items of control and test instrumentation2 in automatic test 
environments in industry. We cover it here because it has two 
interesting facets. First, it implements an unusual patented 
three-line data transfer protocol. You have to have a license 
from the patent holders to use the IEEE 488 bus. Second, it 
transmits control messages in two ways: via special control 
signals and via encoded data messages. 
Figure 10.9 illustrates the relationship between the IEEE 
bus, the IEEE interface, and the devices that communicate 
with each other via the bus. As this diagram demonstrates, 
the IEEE standard covers only the bus and the interfaces but 
not the devices connected to the interfaces. This distinction is 
important because we shall soon discover that the IEEE bus 
implements different communication methods between 
devices and between interfaces. 
The IEEE bus supports three types of device: the con-
troller, the talker, and the listener. A talker (transmitter) can 
put data on the bus, a listener (receiver) can read data from 
the bus, and a controller is a device that manages the bus and 
determines which device may talk and which may listen. Only 
one controller may be active at any given time. An active con-
troller can give up control of the bus by permitting another 
controller to take control. In general, the controller is part of 
the host computer on which the applications program is 
being run. Furthermore, this computer invariably has the 
functions of controller, talker, and listener. 
At any instant only one talker can send messages over the 
IEEE bus, although several listeners may receive the messages 
from the talker. The ability to support a single talker and mul-
tiple listeners simultaneously demonstrates a fundamental 
difference between typical backplane buses and the IEEE bus. 
Backplane buses transfer data between a master and a single 
slave, whereas the IEEE bus is able to transfer data between a 
master (talker) and several slaves (listeners) in a broadcast 
mode of operation. 
The IEEE bus uses 16 information lines that are divided 
into three distinct groups—the data bus, the data bus 
control lines and the bus management lines (see Fig. 10.9). 
The data lines, carry two types of information: bus control 
information and information sent from one bus user to 
another. The IEEE bus supports the following three data 
transmission modes. 
1. A byte of user data is called a multiline message and is 
transmitted over the 8-bit data bus. The message doesn't 
directly affect the operation of the bus itself or the IEEE 
bus interface and its meaning depends only on the nature 
of the devices sending and receiving it. 
2. A byte of IEEE bus interface control information can be 
transmitted over the data bus. Control information acts 
on the interfaces in the devices connected to the bus or 
affects the operation of the devices in some predetermined 
fashion defined in the IEEE 488 standard. 
3. A single bit of information can be transmitted over one of 
the five special-purpose bus management lines. Certain 
bus management lines may be used concurrently with the 
operations on the data bus. 
Information flow on DIOl to DI08 is managed by three 
control lines, NRFD, DAV, and NDAC (i.e. not ready for data, 
data available, and not data accepted). All data exchanges 
between a talker and one or more listeners are fully interlocked, 
and, if a talker is sending information to several listeners, 
the data is transmitted at a rate determined by the slowest 
listener. The operation of the three data bus control lines is 
controlled by the bus interfaces in the devices connected to 
the bus, and is entirely transparent to the user. 
The bus management lines, IFC, ATN, SRQ, REN, and 
EOI, perform functions needed to enhance the operation of 
2 The IEEE standard was introduced in 1976 and revised in 1978. An 
updated version of the standard IEEE 488.2 includes changes to the soft-
ware environment but no significant modifications to the underlying 
physical layer. The IEEE 488 bus is known by several names: the General 
Purpose Interface Bus (GPIB), the Hewlett Packard Instrument Bus 
(HPIB) the, IEC 625-1 bus, the ANSI MC1-1 bus, or, more simply, the 
IEEE bus. 

408 
Chapter 10 Buses and input/output mechanisms 
K 
Controller 
DIOl-8 
Sr-
Talker 
DIOI-8 
V-
Listener 
DIOI-8 
Device connected to the bus 
Host 
CPU 
IEEE bus 
interface 
4 
• 
\ 
Bidirectional 
transceivers £ DIOI-8 
This device may be a 
talker or listener 
-Data bus 
DIOI-8 
Device byte 
transfer control 
(handshake) 
General 
interface 
management 
DAV 
NRFD 
NDAC 
IFC 
ATN 
SRQ 
REN 
EOI 
Data transfer 
management 
Bus control 
Figure 10.9 The IEEE 488 bus. 
the bus. In a minimal implementation of the IEEE 488 bus, 
only ATN is absolutely necessary. The functions of the bus 
management lines are summarized as follows. 
ATN (attention) The ATN line distinguishes between data 
and control messages on the eight data lines. When ATN is 
true (i.e. electrically low), the information on DIOl to DI08 
is interpreted as a control message. When ATN is false 
(i.e. electrically high) the message is a device-dependent 
message from a talker to one or more listeners. The expression 
device-dependent data means that the data is in a format that 
has a meaning only to the device using the IEEE bus. Only the 
controller can assert the ATN line (or the IFC or REN lines). 

10.1 The Bus 409 
IFC (interface clear) The controller uses the IFC line to 
place the bus in a known state. Asserting IFC resets the IEEE 
bus interfaces but not the devices connected to them. After an 
IFC message has been transmitted by a controller for at least 
100 ms, any talker and all listeners are disabled and the serial 
poll mode (if active) is aborted. 
SRQ (service request) The SRQ line performs the same role 
as an interrupt request and is used by a device to indicate to 
the controller that it wants attention. The controller must 
perform a serial poll to identify the device concerned, using a 
specified protocol. 
REN (remote enable) The REN line is used by the controller 
to select between two alternative sources of device control. 
When REN is true a device is controlled from the IEEE bus, 
and when false it is controlled locally. In general, local control 
implies that the device is operated manually from its front 
panel. The REN line allows a device to be attached to the IEEE 
bus, or to be removed from it. In the world of automated 
testing, the assertion of REN turns a manually controlled 
instrument into one that is remotely controlled. 
EOI (end or identify) The EOI line serves two, mutually 
exclusive, purposes. Although the mnemonic for this line is 
EOI, it is frequently written END (end) or IDY (identify), 
depending on the operation being carried out. When asserted 
by a talker, END indicates the end of a sequence of device-
dependent messages. When a talker is transmitting a string of 
device-dependent messages on DIOl to DI08, the talker 
asserts EOI concurrently with the last byte to indicate that 
it has no more information to transmit. When asserted by 
the controller in conjunction with the ATN line, the EOI line 
performs the identify (IDY) function and causes a parallel 
poll in which up to eight devices (or groups of devices) may 
indicate simultaneously whether they require service. 
Data transfer 
Data transfers on the IEEE data bus, DIOl to DI08, are 
interesting because they involve a patented three-line, fully 
interlocked handshaking procedure. The signals used by 
the IEEE bus are all active-low, with an electrically high level 
representing a negated level and an electrical low level repre-
senting an asserted level. Active-low signal levels make it 
possible to take advantage of the wired-OR property of the 
open-collector bus driver (i.e. if any open-collector circuit 
pulls the line down to ground, the state of the line is a logical 
one). The definitions of the three signals controlling data 
movement on the IEEE bus are as follows. 
DAV (data valid) When true (i.e. electrically low), DAV 
indicates to a listener or listeners that data is available on the 
eight data lines. 
NRFD (not ready for data) When true, NRFD indicates that 
one or more listeners are not ready to accept data. 
NDAC (Not Data Accepted) When true, NRFD indicates that 
one or more listeners have not accepted data. 
The timing diagram of a data transfer between a talker and 
several listeners is given in Fig. 10.10. Suppose the bus is 
initially quiet with no transmitter activity and that three active 
receivers are busy and have asserted NRFD to inform the 
transmitter that they are busy. In this state, the NRFD line will 
be pulled down by open-collector bus drivers into a logical 
one state (remember that the IEEE bus uses negative logic in 
which the true or asserted state is the electrically low state). 
When one of the listeners becomes free, it releases (i.e. 
negates) its NRFD output. The negation of NRFD by a listener 
has no effect on the state of the NRFD line, as other listeners 
are still holding it down. This situation is shown by dotted 
lines in Fig. 10.10. When, at last, all listeners have released 
their NRFD outputs, the NRFD line is negated, signifying 
that the listeners are all not 'not ready for data'—that is, they 
are ready for data. Now the talker can go ahead with a data 
transfer. 
The talker places data on DIOl to DI08 and asserts DAV. 
As soon as the listeners detect DAV asserted, they assert 
NRFD to indicate that they are once more busy. 
DI01-D108 
(Talker) 
X 
X 
2.4 V 
0.8 V 
0 
1 
Arrival of new 
data stops some 
-''devices being ready 
24 V 
NRFD 
(Listener) 
0 
1 
1 \7T r"L 
Arrival of new 
data stops some 
-''devices being ready 
0.8 V 
,4 
c k / ^ 
?4V 
DAV 
(Talker) 
0 
1 
0 
1 
/ 
^ 
All devices 
ready for 
data 
\ c 
"1 
0.8 V 
0 
1 
0 
1 
/ 
^ 
All devices 
ready for 
data 
XX 
24V 
NDAC 
(Listener) 
0 
1 
0 
1 
/ 
^ 
All devices 
ready for 
data 
Lastde 
accept 
0.8 V 
Lastde 
accept 
vice 
s data 
Figure 10.10 The three-wire 
handshake. 

410 
Chapter 10 Buses and input/output mechanisms 
Meanwhile, the listeners assert their NDAC outputs 
electrically low to indicate that they have not accepted data. 
When a listener detects that DAV has been asserted, it reads 
the data off D101 to DI08 and negates its NDAC output. 
That is, if its 'not data' accepted output is negated, then it 
must be signifying data accepted. 
Because all listeners must negate their NDAC outputs 
before the NDAC line can rise to an electrical high state, the 
talker does not receive a composite data-accepted signal until 
the last listener has released NDAC. The talker terminates the 
data transfer cycle when it releases DAV and the receivers 
release NDAC in turn. 
Configuring the IEEE bus 
Before the IEEE bus can be used by the devices connected 
to it, the controller must first assign one device as a talker and 
one or more devices as listeners The controller communicates 
with all other devices either by uniline messages (asserting 
one of the bus management lines), or by multiline messages 
(asserting ATN and transmitting a message via DIOl to 
DI08). Multiline messages can be further subdivided into 
those intended for all devices (universal commands) and 
those intended for specific devices (addressed commands). 
Remember that all messages use only 7 bits of an 8-bit byte, 
enabling 7-bit ISO characters to be assigned to the control 
messages. 
Three multiline messages are used by the controller to 
configure talkers and listeners on the bus: MLA (my listen 
address), MTA (my talk address), and MSA (my secondary 
address). Consider first the action of the MLA command. 
Before a device may listen to device-dependent traffic on 
the bus, it must be addressed to listen by the controller. The 
31 my listen address codes from 00100000 to 00111110 select 
31 unique listener addresses. Each listener has its own address, 
determined either at the time of its manufacture or by 
manually setting switches, generally located on its rear panel. 
By sending a sequence of MLAs, a group of devices can 
be configured as active listeners. The 32nd listener address, 
00111111, has a special function called unlisten (UNL). 
Whenever the UNL command is transmitted by the controller, 
all active listeners are disabled. An unlisten command is issued 
before a string of MLAs to disable any listeners previously 
configured for some other purpose. 
Having set up the listeners, the next step is to configure a 
talker, which is done by transmitting an MTA. There are 
31 my talk address codes from 01000000 to 01011110. As only 
one device can be the active talker at any given time, the act 
of issuing a new MTA has the effect of automatically disabling 
the old (if any) talker. The special code 01011111 is called UNT 
(untalk) and deactivates the current talker. Once a talker 
and one or more listeners have been configured, data can be 
transmitted from the talker to the listener(s) at the rate of 
the slowest device taking part in the exchange and without 
the aid (or intervention) of the controller. The format and 
interpretation of this data is outside the scope of the IEEE 488 
standard, but, as we have said, is frequently represented by 
ISO (ASCII) characters. Note that the controller is acting as 
an intermediary between talkers and listeners, in contrast 
to other buses in which potential talkers and listeners are 
usually autonomous. 
Serial and parallel polling 
Like many other buses, the IEEE 488 bus provides facilities 
for devices to request service from controllers (i.e. an inter-
rupt mechanism). The IEEE bus supports two forms of 
supervisor request—the serial poll and the parallel poll, 
although the parallel poll cannot strictly be classified as an 
interrupt. 
A device connected to the IEEE bus can request attention 
by asserting the SRQ (service request) bus management line. 
The controller detects the service request and may respond 
by initiating a serial poll. A service request, in IEEE bus 
terminology, corresponds to an interrupt request in conven-
tional computer terminology. As the controller does not know 
which device initiated the service request, it must poll all 
devices sequentially. The recommended sequence of actions 
that should be carried out by the controller in response to a 
service request is 
S e r i a l - p o l l 
Unlisten a l l a c t i v e l i s t e n e r s with UNL 
Enable s e r i a l p o l l with SPE 
REPEAT 
Transmit a MTA t o a device to be p o l l e d 
Read the response from the polled 
device 
UNTIL a l l devices 
p o l l e d 
Disable s e r i a l p o l l with SPD 
Untalk a l l devices with UNT 
End 
s e r i a l _ p o l l 
After entering the serial poll mode the controller transmits 
successive talk addresses (MTAs) and examines the service 
messages from each of the devices addressed to talk, until an 
affirmative response is obtained. The controller ends the polling 
sequence by an SPD (serial poll disable) command. 
A parallel poll is initiated by the controller and involves 
several devices concurrently. The controller sets up the 
parallel poll by assigning individual data bus lines to devices 
(or groups of devices). For example, device 5 may be told 
to respond to a parallel poll by asserting DI03. Then, the 
controller initiates the parallel poll and the configured 
devices respond. 
The controller asserts the ATN and IDY (identify) lines 
simultaneously to carry out a parallel poll. Whenever the IEEE 
bus is in this state with ATN and IDY asserted, the predeter-
mined devices place their response outputs on the assigned 
data lines and the controller then reads the contents of the 
data bus. A parallel poll can be completed in only a few 
microseconds unlike the serial poll. 

10.1 The Bus 
411 
10.1.5 The USB serial bus 
First-generation PCs suffered from poor connectivity. PCs 
had an RS232C serial port for modems and a parallel port for 
printers. All external systems had to be interfaced to these 
relatively slow interfaces that had not been designed to be 
flexible. You could plug a special card into the PC's mother-
board to support a particular interface or use the expensive 
SCSI bus designed for hard-disk interfaces. 
Two of the greatest advances in PC technology were the 
USB interface and the plug-and-play philosophy. The USB, or 
universal serial bus, interface is a low-cost plug and socket 
arrangement that allows you to connect devices from printers 
and scanners to digital cameras and flash-card readers to a PC 
with minimal effort. Moreover, the USB is expandable—you 
can connect a USB port to a hub and that hub can provide 
other USB connectors. A processor with a USB port lets you 
connect up to 127 devices to the computer. Plug-and-play 
allows the device connected to die USB port to negotiate with 
the operating system running on the host and to supply the 
necessary drivers and set-up parameters. 
The first-generation USB implementation supported a data 
rate of 11 Mbps whereas the USB 2.0 replacement that emerged 
in 2000 supports data transfer rates of 1.5,12, and 480 Mbps. 
A USB connector has four pins. Two provide a 5 V power 
supply and two transmit the data. The power supply can be 
used by a USB device as long as its power requirements are 
modest. This arrangement allows devices like keyboards, mice, 
flashcard readers, etc. to be connected to a USB port without 
the need for their own power supply or batteries. 
Data on the USB is transmitted differentially, that is, the 
signal on the two data lines is transmitted as the pair (+0.1 V 
-0.1 V) or (-0.1 V,+0.1 V) so that the information content 
lies in the potential difference between the data terminals, 
which is either 0.2 V or -0.2 V. Information encoding is called 
NRZ1 (non-return to zero 1) where the voltage between the 
data lines is unchanged to transmit a 1 and it is switched to 
transmit a 0; that is, information is transmitted by switching 
polarity whenever there is a 0 in the data stream. 
Information is transmitted without a reference clock 
leaving the receiver to extract data from the incoming stream 
of pulses. If you transmit a long string of Is, there are no 
transitions in the data stream from which you can extract 
timing information. Consequently, whenever six Is are 
transmitted, a 0 is automatically transmitted to force a data 
transition to help create a synchronizing signal. If you recover 
six 1 s, you know the next bit must be a 0 and you simply drop 
it. This mechanism is called bit stuffing. 
The individual bits transmitted across the USB bus are 
grouped into units called packets or frames. Figure 10.11 
illustrates four of the 10 USB packets. Packets begin with a 
synchronizing field followed by a packet identification field, 
Packet identition (PID) Field, which defines the type of the 
Sync 
PID 
Address 
ENDP 
CRC 
EOP 
Token packet 
Sync 
PID 
Data 
Data packet 
Sync 
PID 
Handshake packet 
Sync 
PID 
Frame number 
CRC 
EOP 
Start-of-frame packet 
Figure 10.11 USB packets. 
current packet. Packets are terminated by an end-of-packet 
(EOP) field. 
Other packet fields in Fig. 10.11 are the data field used to 
transport applications-oriented data between the host computer 
and USB device, and the CRC field, which is used to detect 
retransmission errors. The ENDP field defines the packet's 
endpoint, which provides a destination (one of four) for the 
packet within a USB device. This arrangement allows the 
USB to treat packets as belonging to four different types 
of stream or pipe. The four pipes supported by the USB are 
the default or control pipe, the bulk pipe used for raw data 
transmission, the interrupt pipe, and the isochronous pipe 
for streaming video or audio. Note that each pipe consists of 
two pipes in opposite directions for host-to-USB device and 
USB device-to-host data transfers. 
Most data transfers use the bulk data pipe where informa-
tion is sent in units of up to 64 bytes. Isochronous data transfers 
provide a guaranteed bandwidth that is needed for video 
or audio links. These data transfers don't use error checking 
because there's nothing that can be done if an error occurs in 
a real-time video or audio stream. 
Setting up the USB 
The universal serial bus is a dynamic system that can adapt to 
changing circumstances; that is, you can hot-plug devices 
into the USB bus at any time without powering down and 
you can remove devices from the bus at any time. 
When a device is plugged into the USB, the host detects that 
a new device has been connected and then waits 100 ms to 
ensure that the new device has had time to be properly inserted 
and powered up. The host then issues a reset command 
to place the new device in its default state and allow it to 
respond to address zero (the initial default address). 
CRC 
mm 

412 
Chapter 10 Buses and input/output mechanisms 
The host then asks the newly connected device for the first 
64 bytes of its device descriptor. Each USB device is able to 
supply a device descriptor that defines the device to the host 
processor; for example, the descriptor includes information 
about the product and its vendor, its power requirements, the 
number of interfaces it has, endpoint information, and so on. 
Once the full device descriptor has been transmitted, the host 
is able to communicate with the USB device using the appro-
priate device drivers. The host can now assign an address to 
the new device. 
10.2 I/O fundamentals 
Computer I/O covers several topics because input and output 
transactions involve the host processor, its software, and the 
peripherals sending or receiving data. We can divide I/O into 
three areas. 
1. The strategy by which data is moved into or out of the 
computer. 
2. The interface circuit that actually moves the data into or 
out of the computer. 
3. The input/output devices themselves that convert data 
into a form that can be used by an external system or that 
take data from the outside world and convert it into a form 
that can be processed digitally. Data maybe converted into 
an almost infinite number of representations, from a close 
approximation to human speech to a signal that opens or 
closes a valve in a chemical factory. Input/output devices 
are frequently called peripherals. 
The difference between these three aspects of I/O can 
be illustrated by two examples. Consider first a computer 
connected to a keyboard and an LCD display. Data is moved 
into or out of the computer by a strategy called programmed 
data transfer. Whenever the computer wants to send data to 
the display, an instruction in the program writes data into the 
output port that communicates with the display. Similarly, 
when the computer requires data, an instruction reads data 
from the input port connected to the keyboard. The term port 
indicates a gateway between the computer and an external 
I/O device. Programmed data transfer or programmed I/O 
represents the strategy by which the information is moved 
but tells us nothing about how the data is moved—that is 
handled by the interface between the computer and external 
peripheral. In this example the keyboard and display are the 
I/O devices proper (i.e. peripherals). 
Consider data that's sent from a computer to a remote 
display terminal (see Fig. 10.12). When the computer sends 
data to its output port, the output port transmits that data 
to the display. The output port is frequently a sophisticated 
integrated circuit whose complexity may approach that 
of the CPU itself. Such a semi-intelligent device relieves the 
computer of the tedious task of communicating with the 
LCD display directly, and frees it to do useful calculations. 
The connection between a computer and a display may 
consist of a twisted pair (two parallel wires twisted at regular 
intervals). Because the data written into the output port by 
the CPU is in parallel form, the output port must serialize the 
data and transmit it a bit at a time over the twisted pair to the 
display. Moreover, the output port must supply start and stop 
bits to enable the display to synchronize itself with the stream 
of bits from the computer. Chapter 14 deals in more detail 
with serial data transmission. We can now see that the output 
port is the device that is responsible for moving the data 
between the processor and the peripheral. 
The display terminal is the output device proper. It accepts 
serial data from die computer, reconstitutes it into a parallel 
form, and uses the data to select a character from a table 
of symbols. The symbols are then displayed on a screen. 
Sometimes the transmitted character performs a control 
Computer 
Parallel to serial 
converter 
Serial data 
*-0 o~Lrui_jn^t 
Transmission path 
Display 
Display controller 
0 + 
Serial to parallel 
converter 
Figure 10.12 Relationship 
between a computer and a 
peripheral. 
\ 
The program 
I 
f MOVE.B d a t a , 
DO 
\ 
\MOVE.B DO, 
o u t p u t ^ 

10.2 I/O fundamentals 
413 
function (e.g. carriage return, line-feed, or backspace) that 
determines the layout of the display. 
Figure 10.13 illustrates the relationship between the CPU, 
the peripheral interface chip, and the peripheral device itself. 
As you can see, the peripheral interface chip looks just like a 
memory location to the CPU (i.e. you read or write data to 
it). However, this chip contains specialized logic that allows it 
to communicate with the peripheral. 
The way in which a block of data is written to a disk drive 
provides another example of the relationship between I/O 
strategy, the I/O interface, and the peripheral. It's impractical 
to use programmed data transfers for disk I/O because that is 
too slow. The I/O strategy most frequently used is direct 
memory access (DMA) in which the data is transferred from 
the computer's memory to a peripheral, or vice versa, without 
passing through the CPU's registers. The CPU tells the DMA 
hardware to move a block of data and the DMA hardware gets 
on with the task, allowing the CPU to continue its main func-
tion of information processing. This strategy (i.e. DMA) 
requires special hardware to implement it. 
An interface chip called a DMA controller (DMAC) is 
responsible for moving the data between the memory and the 
peripheral. The DMAC provides addresses for the source or 
destination of data in memory, and informs the peripheral 
that data is needed or is ready. Furthermore, the DMAC must 
grab the computer's internal data and address buses for the 
duration of a data transfer. Data transfer by DMA must be 
performed while avoiding a conflict with the CPU for the 
possession of the buses. In this example the peripheral is a 
disk drive—a complex mixture of electronics and high-preci-
sion mechanical engineering designed to store data by locally 
affecting the magnetic properties of the surface of a disk 
rotating at a high speed. 
10.2.1 Programmed I/O 
Programmed I/O takes place when an instruction in the pro-
gram performs the data transfer; for example, a programmer 
writes MOVE . B Keyboard, DO to read a byte of data from 
the keyboard and puts it in DO. Some microprocessors have 
special instructions that are used only for I/O; for example, 
when a microprocessor executes an OUT 123 operation, the 
contents of a data register are placed on the data bus. At the 
same time the number 123 is placed on the eight least-
significant bits of the address bus and a pulse is generated on 
the system's I/O write line. Each of the I/O ports in such a 
system monitors the address lines. When an I/O interface sees 
its own address together with a read-port or a write-port 
signal, the interface acts on that signal and executes an I/O 
data transfer. 
Memory-mapped I/O 
Many microprocessors lack explicit I/O instructions like the 
OUT <por t > we've just described and have no special input 
or output instructions whatsoever. Microprocessors without 
special I/O instruction must use memory-mapped I/O in 
which the processor treats interface ports as an extension to 
memory. That is, part of the CPU's normal memory space is 
dedicated to I/O operations and all I/O ports look exactly like 
normal memory locations. 
Memory-mapped I/O ports are accessed by memory refer-
ence instructions like MOVE DO, IO_PORT (to output data) 
and MOVE IO_PORT,D0 (to input data). A disadvantage of 
memory-mapped I/O is that memory space available to 
programs and data is lost to the I/O system. 
Figure 10.14 describes the organization and memory map 
of an I/O port. An output port located at address 800016 is 
connected to a display device. Data is transmitted to the dis-
play by storing it in memory location 8000,6. As far as the 
processor is concerned, it's merely storing data in memory. 
The program in Table 10.1 sends 128 characters (starting at 
200016) to the display. Note that we've provided both conven-
tional comments and RTL definitions of the instructions. 
The numbers in the right-hand column in Table 10.1 give 
the time to execute each instruction in microseconds, assum-
ing a clock rate of 8 MHz. To output the 128 characters takes 
approximately 128 + (8 + 8 + 8 + 10)/8 = 544 (JLS, which is 
a little over 'A thousandth of a second. Data is transferred at a 
rate of one character per 4'A u.s. 
Although the program in Table 10.1 looks as if it should 
work, it's unsuited to almost all real situations involving pro-
grammed output. Most peripherals connected to an output 
CPU 
A-
V 
A-
V -
Address bus 
Data bu 
Control bui~ 
Periph 
interf 
CPU side 
erai 
ce chip 
Peripheral side 
Part of the computer 
--*•<-
An external device 
Figure 10.13 Relationship 
between a computer and a 
peripheral. 
AjJMgMifb 
Peripheral 
^> 
device 
\ 
Peripheral 
bus 

414 
Chapter 10 Buses and input/output mechanisms 
Address bus 
Memory map 
000000 
000400 
0007FF 
002000 
0020FF • 
008000 
008003 
SHI 
Output port 
Figure 10.14 Memory 
mapped I/O. 
Data 
Address 
Output port 
Output lines 
i 
i 
i 
i 
+ 1 1 * 
To peripheral 
Data 
Address 
Memory 
^ ^ 
mm 
Data 
Address! 
:CPU 
;j::;Data
;b'usv.'::',; 
* FOR i = 1 to 128 
* 
Move data from Tablet to outputjport 
* ENDFOR 
PORT 
COUNT: 
* 
* 
LOOP 
EQU 
$008000 
EQU 
128 
ORG 
$000400 
MOVE 
#COUNT,Dl 
LEA 
TABLE, AO 
LEA 
PORT,Al 
MOVE.B ,(A0) + ,D0 
MOVE.B DO/(Al) 
SUB , #1,D1 
BNE ' LOOP 
Location of memory-mapped port 
Size of block to be output 
Origin of program 
[Dl] 
128 
Set up loop counter 
[A0] 
TABLE 
A0 points to the table 
[Al] 
Portl 
Al points to the port 
. ORG 
$002000 
Origin for data area 
TABLE DS.B 
128 
Reserve 128 bytes for the table of data 
Table 10.1 A hypothetical example of a programmed output transfer. 
port are slow devices and sending data to them at this rate 
would simply result in almost all the data being lost. Some 
interfaces can deal with short bursts of high-speed data 
because they store data in a buffer; they can't deal with a 
continuous stream of data at high speeds because the buffer 
fills up and soon overflows. 
You can deal with a mismatch in speed between the 
computer and a peripheral by asking the peripheral if it's 
ready to receive data, and not sending data to it until it is ready 
to receive it. That is, we introduce a software handshaking 
procedure between the peripheral and the interface. 
Almost all memory-mapped I/O ports occupy two or more 
memory locations. One location is reserved for the actual data 
to be input or output, and one holds a status byte associated 
with the port. For example, let 80001(i be the location of the 
port to which data is sent and let 800216 be the location of 
the status byte. Suppose that bit 0 of the status byte is a 1 if 
the port is ready for data and a 0 if it is busy. The fragment of 
program in Table 10.2 implements memory-mapped output 
at a rate determined by the peripheral. The comments at 
the beginning of the program describe the data transfer in 
pseudocode. 
[DO] <- HA0JI 
Get data to be output 
B 
[A0] <~ [A0] + 1 
IIA3J] <- [DO] 
Output the data 
8 
[Dl] i- [Dl] - ] 
Decrement counter 
8 
Repeat until counter = 0 
10 
MOVE.B ,(A0) + ,D0 
MOVE.B DO/(Al) 
SDB , #1,D1 
BNE ' LOOP 
* 
LOOP 
mmm. 

10.2 I/O fundamentals 415 
FOR i = 1 TO 128 
REPEAT 
Read Port_status_byte 
UNTIL Port^notjausy 
Move data from Tablex to output_port 
* ENDFOR 
1. 
PORTDATA 
EQU 
$008000 
2. 
PORTSTAT 
EQU 
$008002 
3. COUNT 
EQU 
128 
4. 
ORG 
$000400 
5. 
MOVE 
#C0UNT,D1 
6. 
LEA 
TABLE,AO 
7. 
LEA 
PORTDATA,Al 
8. 
LEA 
PORTSTAT, A2 
9. 
LOOP 
MOVE.B (A0)+,DO 
10. WAIT 
MOVE.B (A2) ,D2 
11 . 
AND.B 
#1,D2 
12. 
BEQ 
WAIT 
13. 
MOVE.B DO, (Al) 
14. 
SUB 
fl,Dl 
15. 
BNE 
LOOP 
16. * 
17. 
ORG 
$002000 
18. TABLE 
DS.B 
128 
Location of memory-mapped port 
Location of port's status byte 
Size of block to be output 
Origin of program 
Set up character counter in Dl 
A0 points to table in memory 
Al points to data port 
A2 points to port status byte 
Get a byte from the table 
REPEAT Read the port's status 
Mask all but Isb of status 
Until port ready 
Store data in peripheral 
Decrement loop counter 
Repeat until COUNT = 0 
Start of data area 
Reserve 128 bytes of data 
Table 10.2 Using the polling loop to control the flow of data. 
The program in Table 10.2 is similar to the previous 
example in Table 10.1 except for lines 8 to 12 inclusive. In line 
8 an address register, A2, is used to point to the status byte of 
the interface at address 800216. In line 10 the status byte of 
the interface is read into D2 and masked down to the least-
significant bit (by the action of AND.B #l,D2 in line 11). 
If the least-significant bit of the status byte is zero, a branch 
back to line 10 is made by the instruction in line 12. When the 
interface becomes free, the branch to WAIT is not taken and 
the program continues exactly as in Table 10.1. 
Lines 10, 11, and 12 constitute a polling loop, in which the 
output device is continually polled (questioned) until it 
indicates it is free, allowing the program to continue. A slow 
mechanical printer might operate at 30 characters/second, or 
approximately 1 character per 33 000 u.s. Because the polling 
loop takes about 3 (jis, the loop is executed 11 000 times per 
character. 
Operating a computer in a polled input/output mode is 
grossly inefficient because so much of the computer's time is 
wasted waiting for the port to become free. If the micro-
computer has nothing better to do while it is waiting for a 
peripheral to become free (i.e. not busy) polled I/O is per-
fectly acceptable. Many first-generation PCs, were operated 
in this way. However, a more powerful computer working in 
a multiprogramming environment can attend to another task 
program during the time the I/O port is busy. In this case a 
better I/O strategy is to ignore the peripheral until it is ready 
for a data transfer and then let the peripheral ask the CPU 
for attention. Such a strategy is called interrupt-driven I/O. 
Note mat all the I/O strategies we are describing use 
memory-mapped I/O. 
By the way, if you are designing a computer with memory-
mapped I/O and a memory cache,3 you have to tell the cache 
controller not to cache the port's status register. If you don't 
do this, the cache memory would read the status once, cache 
it, and then return the cached value on successive accesses to 
the status. Even if the status register in the peripheral 
changes, the old value in the cache is frozen. 
10.2.2 Interrupt-driven I/O 
A computer executes instructions sequentially unless a jump 
or a branch is made. There is, however, an important excep-
tion to this rule called an interrupt, an event that forces the 
CPU to modify its sequence of actions. This event may be 
a signal from a peripheral (i.e. a hardware interrupt) or an 
internally generated call to the operating system (i.e. a software 
interrupt). The term exception describes both hardware and 
software interrupts. 
Most microprocessors have an active-low interrupt 
request input, IRQ, which is asserted by a peripheral to 
request attention. The word request implies that the interrupt 
request may or may not be granted. Figure 10.15 illustrates 
the organization, of a system with a simple interrupt-driven 
I/O mechanism. 
3 Cache memory is very fast memory that contains a copy of frequently 
accessed data. We looked at cache memory in Chapter 8. 

416 
Chapter 10 Buses and input/output mechanisms 
In Figure 10.15 an active-low interrupt request lineconnects 
all peripherals to the CPU. A peripheral asserts its IRQ output 
when it requires attention. This system is analogous to the 
emergency handle in a train. When the handle is pulled in one 
of the carriages, the driver knows that a problem has arisen but 
doesn't yet know who pulled the handle. Similarly, the CPU 
doesn't know which peripheral caused the interrupt or why. 
When the CPU detects that its IRQ input has been asserted, 
the following sequence of events takes place. 
• The CPU finishes its current instruction because micro-
processors cannot be stopped in mid-instruction. Individual 
machine code instructions are indivisible and must always 
be executed to completion.4 
• The contents of the program counter and the processor 
status word are pushed onto the stack. The processor status 
must be saved because the interrupt routine will almost 
certainly modify the condition code bits. 
• Further interrupts are disabled to avoid an interrupt being 
interrupted (we will elaborate on this partially true state-
ment later). 
• The CPU deals with the interrupt by executing a program 
called an interrupt handler. 
• The CPU executes a return from interrupt instruction at 
the end of the interrupt handler. Executing this instruction 
pulls the PC and processor status word off the stack and 
execution then continues normally—as if the interrupt had 
never happened. 
Figure 10.16 illustrates the sequence of actions taking place 
when an interrupt occurs. In a 68K system the processor 
status word consists of the system byte plus the condition 
code register. The system byte is used by the operating system 
and interrupt processing mechanism. 
Interrupt-driven I/O requires a more complex program 
than programmed I/O because the information transfer takes 
place not when the programmer wants or expects it, but when 
the data is available. The software required to implement 
interrupt-driven I/O is frequently part of the operating system. 
A fragment of a hypothetical interrupt-driven output routine 
in 68K assembly language is provided in Table 10.3. Each time 
the interrupt handling routine is called, data is obtained from 
a buffer and passed to the memory-mapped output port at 
$008000. In a practical system some check would be needed 
to test for the end of the buffer. 
Because the processor executes this code only when a peri-
pheral requests an I/O transaction, interrupt-driven I/O is very 
much more efficient than the polled I/O we described earlier. 
Although the basic idea of interrupts is common to most 
computers, there are considerable variations in the precise 
nature of the interrupt-handling structure from computer to 
computer. We are now going to look at how the 68K deals 
with interrupts because this microprocessor has a particularly 
comprehensive interrupt handling facility. 
Prioritized interrupts 
Computer interrupts are almost exactly analogous to inter-
rupts in everyday life. Suppose two students interrupt me when 
I'm lecturing—one with a question and the other because they 
feel unwell. I will respond to the more urgent of the two 
requests. Once I've dealt with the student who's unwell, I 
answer the other student's question and then continue my 
teaching. Computers behave in the same way. 
4 This statement is not true of all microprocessors. It is possible to 
design microprocessors that can save sufficient state information to inter-
rupt an instruction and then continue from the point at which execution 
had reached. 
Address bus 
Data bus 
K} 1 1 
V 
V 
O 
O 
Port 
I Data register 
I Interrupt registers 
Memory 
Status register 
reread by the CPU 
£ 
to determine the 
I 
!Y5 
I peripheral's status 
IRQ 
1 
I 
I 
I 
IRQ 
'' 
Informs CPU that the 
I w a n t attention 
• c 3 
peripheral wants attention 
, r 
Interrupt request to CPU 
Figure 10.15 Interrupt organization. 
CPU 

10.2 I/O fundamentals 
417 
Normal processing 
Interrupt^L 
Interrupt handlin; 
Stack & » * * • « 
- ^ l l H H ^ d r e s s 
Restore PC and 
processor status 
Save working 
registers 
Interrupt handling 
routine 
Restore working 
registers 
Stack before 
interrupt 
Stack 
Figure 10.16 Interrupt 
sequence. 
Pick up pointer to next free entry in the table (buffer) 
Read a byte from the table and transmit it to the interface 
Move the pointer to the next entry in the table and save 
the pointer in memory 
Return from interrupt 
OUTPUT EQU 
$008000 
* 
ORG 
$000400 
LEA 
POINTER,AO 
MOVE B (A0)f,OUTPUT 
MOVE 
RTE 
L A0,POINTER 
ORG 
$002000 
BUFFER DS.B 
1024 
POINTER DS.L 
1 
Location of memory-mapped output port 
Start of the program fragment 
Load A0 with the pointer to the buffer 
Read character from buffer and output 
Save the updated pointer 
Return from interrupt 
Data origin 
Reserve 1024 bytes for the table 
Reserve a longword for the pointer 
Table 10.3 A simple interrupt handler. 
Most computers have more than one interrupt request input. 
Some interrupt request pins are connected to peripherals 
requiring immediate attention (e.g. a disk drive), whereas 
others are connected to peripherals requiring less urgent 
attention (e.g. a keyboard). For the sake of accuracy, we 
should point out that the processor's interrupt request input 
is connected to the peripheral's interface, rather than the 
peripheral itself. If the disk drive is not serviced when its data 
is available, the data will be lost because it will be replaced by 
new data. In such circumstances, it is reasonable to assign a 
priority to each of the interrupt request pins. 
The 68K supports seven interrupt request inputs, from 
IRQ7, the most important, to IRQ1, the least important. 
Suppose an interrupt is caused by the assertion of IRQ3 and 
no other interrupts are pending. The interrupt on IRQ3 will 
be serviced. If an interrupt at a level higher than IRQ3 occurs, 
it will be serviced before the level 3 interrupt service routine 
is completed. However, interrupts generated by IRQ1 or 
IRQ2 will be stored pending the completion of IRQ3's service 
routine. 
The 68K does not have seven explicit IRQ1 to IRQ7 
interrupt request inputs (simply because such an arrange-
ment would require seven precious pins). Instead, the 68K 
has a 3-bit encoded interrupt request input, IPL0 to IPL2. 
The 3-bit value on IPL0 to IPL2 reflects the current level of 
interrupt request from 0 (i.e. no interrupt request) to 7 (the 
highest level corresponding to IRQ7). Figure 10.17 illustrates 
some of the elements involved in the 68K's interrupt handling 
structure. A priority encoder chip is required to convert an 
interrupt request on IRQ1 to IRQ7 into a 3-bit code in IPL0 
. 
Stack after 
\ 
interrupt 
Stack\^ 
> Return 
Status 
Old TOS 
y 
SP 
/ S f 
—*• TOS 

418 
Chapter 10 Buses and input/output mechanisms 
to IPL2. The priority encoder automatically prioritizes inter-
rupt requests and its output reflects the highest interrupt 
request level asserted. 
The 68K doesn't automatically service an interrupt request. 
The processor status byte in the CPU in Fig. 10.17 controls the 
way in which the 68K responds to an interrupt. Figure 10.18 
describes the status byte in more detail. The 3-bit interrupt 
mask field in the processor status byte, I2, Ip I0, determines 
how the 68K responds to an interrupt. The current interrupt 
request is serviced if its level is greater than that of the inter-
rupt mask; otherwise the request is ignored. For example, 
if the interrupt mask has a current value of 4, only interrupt 
requests on IRQ5 to IRQ7 will be serviced. 
When the 68K services an interrupt, the interrupt mask 
bits are reset to make them equal to the level of the interrupt 
currently being serviced. For example, if the interrupt mask 
bits were set to 2 and an interrupt occurred at level IRQ5, the 
mask bits would be set to 5. Consequently, the 68K can now 
be re-interrupted only by interrupt levels 6 and 7. After the 
interrupt has been serviced, the old value of the processor 
status byte saved on the stack, and therefore the interrupt 
mask bits, are restored to their original level. 
Non-maskable interrupts 
Microprocessors sometimes have a special interrupt request 
input called a non-maskable interrupt request. The term 
Priority 
encoder 
IACK 
encoder 
Memory 
Stack pointer 
Reset vector 
Vector 255 
7T 
7> 
IRQ 
IACK 
IVEC 
Peripheral 
7 y 
IRQ1 
IRQ2 
IRQ3 
IRQ4 
IRQ5 
IRQ6 
IRQ7 
Interrupt request 
inputs from 
peripherals 
•*• IACK1 
IACK2 
TACKS 
l n t e r r u P t 
acknowledge 
I A C K 4 outputs to 
IACK5 peripherals 
-+- IACK6 
• * 
IACK7 
i want attention 
~~~~-You've got it 
IRQ 
IACK 
IVEC 
Peripheral 
IVEC is returned 
- by a peripheral to 
acknowledge on 
interrupt 
:> 
:> 
Figure 10.17 The 68K's interrupt structure. 
68000 microprocessor 
Encoded 
I P L 0 ' 
interrupt 
— - 
( 
request 
i nP u t 
|PL2 « 
function code FCO • 
indicates type 
of bus cycle. 
FC1 -
1,1,1 =IACK 
cycle 
FC2 • 
in an lACK cycle the 
A01 " 
CPU puts the level 
, 
of the iACK on the 
^>2 
address bus 
A 
Processor status byte 
M i l l \h\h\k\ 
Interrupt mask 
bits set the level 
below which 
interrupts will 
not be processed 
Address bus ". 
Data bus '-

10.2 I/O fundamentals 
419 
Status byte (used by operating system) 
^ 
' 
'•" 
&4 
Condition code register 
Trace bit 
Supervisor bit 
(this bit reflects the 
state of the CPU) 
S = 0 user mode 
S = 1 supervisor mode 
Interrupt mask 
(an interrupt is serviced 
only if it is at a higher level 
than the interrupt mask) 
The user-mode (application) 
programmer can access only 
these bits in the CCR part 
of the status register. 
Figure 10.18 The68K's 
status word. 
Figure 10.19 A memory-mapped data and status port. 
non-maskable means that the interrupt cannot be turned off 
(i.e. delayed or suspended) and must be serviced immediately. 
Non-maskable interrupts are necessary when the interrupt is 
caused by a critical event that must not be missed; for example, 
an interruption of the power supply. When power is lost, the 
system still functions for a few milliseconds on energy stored 
in capacitors (devices found in all power supplies). A non-
maskable interrupt generated at the first sign of a power loss 
is used to shut down the computer in an orderly fashion so 
that it can be restarted later with little loss of data and no 
corruption of disk files. 
A second application of non-maskable interrupts is in real-
time systems. Suppose that the temperature and pressure at 
various points in a chemical process must be measured peri-
odically. If these points aren't polled on a programmed basis, 
a stream of regularly spaced non-maskable interrupts will do 
the trick. At each interrupt, the contents of a counter register 
are updated and, if a suitable span of time has elapsed, the 
required readings are taken. 
The 68K reserves its level 7 interrupt ( IRQ7)_as a non-
maskable interrupt, because an interrupt on IRQ7 is always 
serviced by the 68000. If a level 7 interrupt is currently being 
serviced by the 68K, a further active transition on IRQ7 (i.e. a 
high-to-low edge) results in the 68K servicing the new level 7 
interrupt. 
Vectored interrupts 
Following the detection and acceptance 
of an interrupt, the appropriate interrupt-
handling routine must be executed. You 
can test each of the possible interrupters, 
in turn, to determine whether they were 
responsible for the interrupt. This opera-
tion is called polling and is the same 
mechanism used for programmed I/O. 
We now look at how the 68 K deals 
with the identification of an interrupt 
request that came from one of several 
possible devices. However, before we 
do this it's instructive to consider how 
first-generation microprocessors performed the task of 
isolating the cause of an interrupt request. 
Figure 10.19 shows the structure of a memory-mapped 
I/O port with a data port at address 800016 and a status byte 
at location 800216. We have defined 3 bits in the status byte: 
• RDY (ready) indicates that the port is ready to take part in a 
data transaction 
• IRQ indicates that the port has generated an interrupt 
• ERR indicates that an error has occurred (i.e. the input or 
output is unreliable). 
The RDY bit of a peripheral is tested until it is ready to take 
part in an I/O transaction. A system with interrupt-driven 
I/O and device polling waits for an interrupt and then reads 
the IRQ bit in the status register of each peripheral. This techn-
ique is fairly efficient as long as there are few devices capable 
of generating an interrupt. 
Because the programmer chooses the order in which the 
interfaces are polled following an interrupt, a measure of pri-
oritization is built into the polling process. However, a well-
known law of the universe states that when searching through 
a pile of magazines for a particular copy, the desired issue is 
always at the opposite end to the point at which the search 
was started. Likewise, the device that generated the interrupt 
is the last device to be polled. A system with polled interrupts 
could lead to the situation in which a device requests service 
Memory map 
8 0 0 0 
Peripheral data 
Data port 
8002 
RDY 
IRQ ERR Status byte 
Ti 
8 V 
/ 
/o 
Re.a^h-agH 
Interrupt flag / 
. . 
/ 
, K 
set rf this device 
jf £ 
d ^ 
Errot status 
is ready to take 
^
™
d
™ 
set to tnd.cate error 
part in an I/O 
* 
t 
transaction 
r 
T 
S 
. I2 . 
I v 
l0 
N 
X 
Z 
V 
C 
T - — » 
A 
' 
• 
* 
- 
'* 
* 
' 
' 
" J 
"• 
' 

420 
Chapter 10 Buses and input/output mechanisms 
but never gets it. We next demonstrate how some processors 
allow the peripheral that requested attention to identify itself 
by means of a mechanism called the vectored interrupt. 
In a system with vectored interrupts the interface itself 
identifies its interrupt-handling routine, thereby removing 
the need for interrupt polling. Whenever the 68K detects an 
interrupt, the 68K acknowledges it by transmitting an inter-
rupt acknowledge (called IACK) message to all the interfaces 
that might have originated the interrupt. 
The 68K uses function code outputs, FCO, FC1, FC2, to 
inform peripherals that it's acknowledging an interrupt (see 
Fig. 10.17). These three function code outputs tell external 
devices what the 68K is doing. For example, the function code 
tells the system whether the 68K is reading an instruction or 
an operand from memory. The special function code 1,1,1 
indicates an interrupt acknowledge. 
Because the 68K has seven levels of interrupt request, 
it's necessary to acknowledge only the appropriate level of 
Figure 10.20 Responding to a level 6 vectored interrupt. 
0 0 1 = 1 1 0 
level 6 IRQ 
68K microprocessor 
/ 
ZTI /3f 
I 
|4 
IRQT 
IPLZ ii— 
* 
^ 
IPLI * M 
Priorjty 
* 
: jjg§ 
Thsf.nc.tjo., 
T ^ A y 
6 n C O d e r 
< 
IRQ5 
| ^ r ° ° " 
^ " " \ ^ 
I 
H 
IRQ7 
F C° 0 3 
\ _ _ _ 
^ — " High during 
^ 
^ j 3 
y 
^
"
^
^ 
IACK cycle 
I 
E 
I 
* 
• 
IACK1 
A 
• 
IACK2 
A03 f—\ 
• 
3-line to 
• 
IACK3 
A 
1 
f, 
8 4 i n e 
• 
IACK4 
02 l o l
 
d e C °
d e r 
1 
** 1ACK5 
A01 i - J 
f 
- —
—
—
™
—
. 
—~- -j 
> IACK6 
^V. 
I 
l 
• IACK7 
\ 
IACK at 
c " 
~ Interrupt request 
level 6 
a* >eve' 6 
^-P———.Interrupt acknowledge 
Memory 
i 
W 
a t l e v e l 6 
I 
i A 
I 
I Z I _ 
, 
IRQ 
IACK 
Interrupt 
vector 
Peripheral 
.! . An . i m 
,. 
Interrupt vector 
^ - " ' 
l f " ' 
1£ 
100« ,*• 00001234 
table 
register supplies 
/ 
. / 
v 
I 
1 
, ^ interrupt vector 
/ 
, / 
IVEC 40 1 6* 
during IACK cycle 
Tiio Fntern.irii• wr'-f>r 
1234iC 
Interrupt handler 
1 j 
| | 
Jf^HSSE 
iT 
Tr 
\ 
\ 
Address bus _ _ _ _ ^ 
y 
" 
Data bus ^ 
- 
^ _ 
I^> 
' 
^^-Interrupt vector 
supplied on data bus 
in IACK cycle 
• • . 
68K microprocessor 
IPL2 
IPL? 
The function 
jpm 
code 1,1,1 
indicates an ~"""--~^^ 
!ACK 
^ " ~ ~ \ ^ 
FCO 
FC1 
FC2 
A03 
A02 
Aoi 
4>:40 1 6 = 10016 
The interrupt vector 
is used to access the 
address of the level 
6 interrupt-handling 
routine in memory 
\ 
Address bus 
Data bus 
0 0 1 = 1 1 0 
level 6 IRQ 
i 
y* 
Interrupt request 
I 
at level 6 
I 
c ^———. Interrupt acknowledge 
| 
w 
at level 6 
IRQ 
lACK 
Peripheral 
interrupt vector 
register supplies 
I 
1 
, ^ interrupt vector 
IVEC 
4016 * 
during lACK cycle 
Memory  
1 A 
Interrupt 
vector 
10016 _M 00001234 
table 
123416 
Interrupt handler 
l_ 
^J 
Priority 
encoder 
— High during 
lACK cycle 
^•~~- Interrupt vector 
supplied on data bus 
in IACK cycle 
3-line to 
8-line 
decoder 

10.2 I/O fundamentals 
421 
interrupt. It would be unfair if a level 2 and a level 6 interrupt 
occurred nearly simultaneously and the interface requesting 
a level 2 interrupt thought that its interrupt was about to be 
serviced. The 68K indicates which level of interrupt it's acknow-
ledging by providing the level on the three least-significant 
bits of its address bus (A^ to Ao3). External logic detects FCO, 
FC1, FC2 = 1,1,1 and uses A^ to A^ to generate seven inter-
rupt acknowledge signals IACKO to IACK7. 
After issuing an interrupt request, the interface waits for an 
acknowledgement on its IACK input. When the interface 
detects IACK asserted, it puts out an interrupt vector number 
on data lines doo to d07. That is, the interface responds with a 
number ranging from 0 to 255. When the 68K receives this 
interrupt vector number, it multiplies it by 4 to get an entry 
into the 68K's interrupt vector table; for example, if an inter-
face responds to an IACK cycle with a vector number of 100, 
the CPU multiplies it by 4 to get 400. In the next step, the 68K 
reads the contents of memory location 400 to get a pointer to 
the location of the interrupt-handling routine for the inter-
face that initiated the interrupt. This pointer is loaded into 
the 68K's program counter to start interrupt processing. 
Because an interface can supply one of 256 possible vector 
numbers, it's theoretically possible to support 256 unique 
interrupt-handling routines for 256 different interfaces. We 
say theoretically, because it's unusual for 68K systems to 
dedicate all 256 vector numbers to interrupt handling. In 
fact, the 68K itself uses vector numbers 0 to 63 for purposes 
other than handling hardware interrupts (these vectors are 
reserved for other types of exception). 
The 68K multiplies the vector number by 4 because each 
vector number is associated with a 4-byte pointer in memory. 
The interrupt vector table itself takes up 4 X 256 = 1024 
bytes of memory. Figure 10.20 illustrates the way in which the 
68K responds to a level 6 vectored interrupt. 
Daisy-chaining 
The vectored interrupt scheme we've just described has a 
flaw. Although there are 256 interrupt vector numbers, the 
68K supports only seven levels of interrupt. A mechanism 
called daisy-chaining provides a means of increasing the 
number of interrupt levels by linking the peripherals together 
in a line. When the CPU acknowledges an interrupt, a message 
is sent to the first peripheral in the daisy chain. If this peri-
pheral doesn't require attention, it passes the IACK down the 
line to the next peripheral. 
Figure 10.21 shows how interrupt requesters at a given 
priority level are prioritized by daisy chaining. Each periph-
eral has an IACK_IN input and an IACK_OUT output. The 
IACK_OUT pin of a peripheral is wired to the IACK_JN pin 
of the peripheral on its right. Suppose an interrupt request 
at level 6 is issued and acknowledged by the 68K. The inter-
face at the left-hand side of the daisy chain closest to the 
68K receives the IACK signal first from the CPU. If this 
interface generated the interrupt, it responds with an inter-
rupt vector. If the interface did not request service, it passes 
the IACK signal to the device on its right. That is, IACK_IN 
is passed out on IACK_OUT. The IACK signal ripples down 
the daisy chain until a device responds with an interrupt 
vector. 
Daisy-chaining interfaces permit an unlimited number of 
interfaces to share the same level of interrupt and each inter-
face to have its own interrupt vector number. Individual 
IRQ to CPU 
IRQ 
P1 
BYPASS1 
IACK IN 
IACK OUT 
IRQ 
P2 
BYPASS2 
lACKJN 
IACK OUT 
IRQ 
P3 
BYPASS3 
*>H 
IACK IN 
IACK OUT 
IF PI has not requested service, PI 
asserts BYPASS1 and passes 
lACKJN out on TACiToOt 
to other peripherals lower down 
the chain 
Figure 10.21 Daisy-chaining 
interrupts at the same level of 
priority. 

422 
Chapter 10 Buses and input/output mechanisms 
interfaces are prioritized by their position with respect to the 
CPU. The closer to the CPU an interface is, the more chance 
it has of having its interrupt request serviced in the event of 
multiple interrupt requests at this level. 
10.3 Direct memory access 
The third I/O strategy, called direct memory access (DMA), 
moves data between a peripheral and the CPU's memory 
without the direct intervention of the CPU itself. DMA 
provides the fastest possible means of transferring data 
between an interface and memory, as it requires no CPU 
overhead and leaves the CPU free to do useful work. DMA is 
complex to implement and requires a relatively large amount 
of hardware. Figure 10.22 illustrates the operation of a system 
with DMA. 
DMA works by grabbing the data and address buses from 
the CPU and using them to transfer data directly between the 
peripheral and memory. During normal operation of the 
computer in Fig. 10.22, bus switch 1 is closed and bus switches 
2 and 3 are open. The CPU controls the buses, providing an 
address on the address bus and reading data from memory or 
writing data to memory via the data bus. 
When a peripheral wishes to take part in an I/O transac-
tion it asserts the TransferRequest input of the DMA 
controller (DMAC). In turn, the DMA controller asserts 
DMArequest to request control of the buses from the CPU; 
that is the CPU is taken offline. When the CPU returns 
DMAgrant to the DMAC, a DMA transfer takes place. 
Bus switch 1 is opened and switches 2 and 3 closed. The 
DMAC provides an address to the address bus and hence to 
the memory. At the same time, the DMAC provides a 
TransferGrant signal to the peripheral, which is then able to 
write to, or read from, the memory directly. When the DMA 
operation has been completed, the DMAC hands back con-
trol of the bus to the CPU. 
A real DMA controller is a very complex device. It has 
several internal registers with at least one to hold the address 
of the next memory location to access and one to hold the 
number of words to be transferred. Many DMACs are able 
to handle several interfaces, which means that tlieir registers 
must be duplicated. Each interface is referred to as a channel 
and typical single-chip DMA controllers handle up to four 
channels (i.e. peripherals) simultaneously. 
Figure 10.23 provides a protocol flowchart for the sequence 
of operations taking place during a DMA operation. This figure 
shows the sequence of events that takes place in the form of 
a series of transactions between the peripheral, DMAC, and 
the CPU. 
DMA operates in one of two modes: burst mode or cycle 
stealing. In the burst mode the DMA controller seizes the 
system bus for the duration of the data transfer operation (or 
at least for the transfer of a large number of words). Burst 
mode DMA allows data to be moved into memory as fast as 
the weakest link in the chain memory/bus/interface permits. 
The CPU is effectively halted in the burst mode because it 
cannot use its data and address buses. 
In the cycle steal mode described by Fig. 10.24, DMA 
operations are interleaved with the computer's normal 
memory accesses. As the computer does not require access to 
the system buses for 100% of the time, DMA can take place 
when they are free. This free time occurs while the CPU is busy 
generating an address ready for a memory read or write cycle. 
i> 
Bus switch 1 
Address bus 
Data bus" 
Enable CPU 
Address Data 
CPU 
V 
\Z. 
Address Data 
Memory 
DMArequest 
DMAgrant 
Bus switch 2 
Enable 
DMA 
Address 
DMA controller 
(DMAC) 
Address register 
Byte count 
Control register 
^> 
1Z. 
Bus switch 3 
Enable 
DMA 
\ z 
Data 
Peripheral 
(e.g. disk) 
•f TransferRequest 
TransferGrant 
Figure 10.22 Input/Output 
by means of DMA. 

10.4 Parallel and serial interfaces 
423 
CPU 
Grant DMA cycle 
DMA controller 
Request DMA cycle 
*• 
Grant data transfer 
Peripheral 
Request data transfer 
Time 
*>• 
Transfer data 
Figure 10.23 Protocol 
flowchart for a DMA 
operation. 
One machine cycle 
•
*
•
-
«
-
One machine cycle 
System clock 
L 
Address bus . 
DMA address 
CPU address 
DMA address 
CPU address 
1 
Data bus 
DMA data 
CPU data 
DMA data 
CPU data 
? 
Figure 10.24 DMA by cycle stealing. 
When the system clock is low, the CPU doesn't need 
the buses, so the DMAC grabs them and carries out a data 
transfer. When the clock goes high the CPU carries out its 
normal memory access cycle. DMA by cycle stealing is said to 
be transparent because the transfer is invisible to the com-
puter and no processing time is lost. A DMA operation is ini-
tiated by the CPU writing a start address and the number of 
words to be transferred into the DMAC's registers. When the 
DMA operation has been completed, the DMAC generates an 
interrupt, indicating to the CPU that the data transfer is over 
and that a new one may be initiated or results of the current 
transfer made use of. 
In systems with a cache memory, DMA can take place 
in parallel with normal CPU activity; that is, the CPU can 
access data and code that's been cached while the I/O inter-
face is copying data between a peripheral and the main 
memory. 
10.4 Parallel and serial interfaces 
Having described how I/O transactions can be programmed, 
be interrupt driven, or use DMA, we now look at typical 
interfaces between the CPU and the outside world. These 
devices look like a block of memory locations to the CPU and 
implement the protocol required to communicate with the 
external system. Although we describe two actual devices, 
the general principles apply to all interface devices. Readers 
not interested in the fine details of I/O systems may skip this 
section. 
The first interface to be described is the peripheral interface 
adapter, which transfers data between an external system and 
a processor, and the second interface is the asynchronous 
communications adapter, which transfers data on a single-bit 
serial highway. These devices are typical first-generation 

424 
Chapter 10 Buses and input/output mechanisms 
circuits with 8-bit data interfaces allowing them to be used 
with 8-bit processors. 
10.4.1 The parallel interface 
The peripheral interface adapter (PIA) is an integrated circuit 
with two independent 8-bit ports. It contains all the logic 
needed to control the flow of data between an external periph-
eral and a computer. A port's eight pins may be programmed 
individually to act as inputs or outputs; for example, an 8-bit 
port can be configured with two input lines and six output 
lines. The PIA can automatically perform handshaking with 
devices connected to its ports. 
Figure 10.25 gives a block diagram of the PIA from which 
it can be seen that the two I/O ports, referred to as the A side 
and the B side, appear symmetrical. In general this is true, 
but small differences in the behavior of these ports are 
described when necessary. Each port has two control pins 
that can transform the port from a simple I/O latch into a 
device capable of performing a handshake or initiating inter-
rupts, as required. 
The interface between the PIA and the CPU is conven-
tional; the PIA's CPU-side looks like a block of four locations 
in RAM to the CPU. CPU-side pins comprise a data bus 
and its associated control circuits. Two register-select pins RSO 
and RSI are connected to the lower-order bits of the CPU's 
address bus and discriminate between the PIA's internal 
registers. 
The PIA has two independent interrupt request outputs, 
one for each port. When the PIA is powered up, the contents 
of all its internal registers are put in a zero state. In this 
mode the PIA is in a safe state with all its programmable pins 
configured as inputs. It would be highly dangerous to permit 
the PIA to assume a random initial configuration, because 
any random output signals might cause havoc elsewhere. 
To appreciate how the PIA operates, we have to understand 
the function of its six internal registers. The PIA has two 
peripheral data registers (PDRA and PDRB), two data-direction 
registers (DDRA, and DDRB), and two control registers (CRA 
and CRB). The host computer accesses a location within 
the PIA by putting the appropriate 2-bit address on register 
select lines RSO and RSI. Because RSO and RSI can directly 
Data bus 
> 
CPU side 
interface 
Chip-select 
and 
read-write 
control 
Data bus 
buffers 
Control register A (CRA) 
7 
6 
5 I 4 | 
3 
2 
1 I 
0 
IRQ 
A1 
IRQ 
A2 
CA2 
control 
DDR 
A 
CA1 
control 
Output register A 
7 
6 
5 
4 
3 
2 
1 
0 
d7 
d6 
d5 
d4 
d3 
d2 
di 
do 
Output register B 
7 
6 
5 
4 
3 
2 
1 
0 
d7 
d6 
d5 
d4 
d3 
d2 
di 
do 
Control register B (CRB) 
7 
6 
5 1 4 | 
3 
2 
1 I 
0 
IRQ 
B1 
IRQ 
B2 
CB2 
control 
DDR 
B 
CB1 
control 
Interrupt status 
register A 
Data direction 
register A (DDRA) 
3E 
Peripheral 
interface A 
Peripheral 
interface B 
ZE 
Data direction 
register B (DDRB) 
Interrupt status 
register B 
-CA-, 
•CA2 
PAQ 
PA, 
PA2 
PA3 
PA4 
PA5 
PA6 
PA7 
PB0 
PBi 
PB2 
PB3 
PB4 
PB5 
PB6 
PB7 
-OB,' 
• CB2 
Figure 10.25 Structure of the PIA. 

10.4 Parallel and serial interfaces 
425 
distinguish between only four of the six internal registers, we 
need a means of accessing the other registers. The PI A uses bit 
2 in the control registers (CRA2 or CRB2) as a pointer to 
either the data register or the data-direction register. 
Table 10.4 demonstrates how this arrangement works. 
Register select input RSI determines which of the two 8-bit 
I/O ports of the PIA is selected and RSO determines whether 
the control register or one of the pair of registers formed by the 
peripheral data register and the data register, is selected. The 
control registers can always be unconditionally accessed 
when RSO = 1, but to select a peripheral data register or a 
data-direction register, bit 2 of the appropriate control regis-
ter must be set or cleared, respectively. 
RS1 RSO 
CRA2 
CRB2 
Location selected 
Address 
0 
0 
1 
X 
0 
0 
0 
X 
0 
1 
X 
X 
1 
0 
X 
1 
1 
0 
X 
0 
1 
1 
X 
X 
Peripheral data register A 
BASE 
Data direction register A 
BASE 
Control register A 
BASE+2 
Peripheral data register B 
BASE+4 
Data direction register B 
BASE+4 
Control register B 
BASE+6 
X = don't care 
BASE = base address of the memory-mapped PIA 
RSO = register select 0 
RSI = register select 1 
CRA2 = bit 2 of control register A CRB2 = bit 2 of control register B 
Table 10.4 The register selection scheme of the PIA. 
The peripheral data registers provide an interface between 
the PIA and the outside world. When one of the PIA's 16 I/O 
pins is programmed as an input, data is moved from that pin 
through the peripheral data register onto the CPU's data bus 
during a read cycle. Conversely, when acting as an output, the 
CPU latches a 1 or 0 into the appropriate bit of the peripheral 
data register to determine the state of the corresponding 
output pin. 
The data-direction 
registers determine the direction of data 
transfer at the PIA's I/O pins. Writing a zero into bit i of 
DDRA configures bit /' of the A side peripheral data register as 
an input. Conversely, writing a one into bit i of DDRA con-
figures bit i of the A side peripheral data register as an output. 
The pins of the PIA's A side or B side ports may be defined as 
inputs or outputs by writing an appropriate code into DDRA 
or DDRB, respectively. The PIA's I/O pins can be configured 
dynamically and the direction of data transfer altered during 
the course of a program. The DDR's bits are cleared during a 
power-on-reset to avoid accidentally forcing any pin into an 
output mode. 
Table 10.5 demonstrates how side A of a PIA memory-
mapped at address $80 0000 is configured as an input and 
side B as an output. The registers are accessed at $80 0000, 
$80 0002, $80 0004, and $80 0006. Consecutive addresses 
differ by 2 rather than 1 because the 68K's data bus is 16 bits 
wide (2 bytes) whereas the PIA is 8 bits wide. 
Once the PIA has been configured, data can be read from 
side A of the PIA into data register DO by a MOVE. B PDRA, DO 
instruction, and data may written into side B by writing to the 
PIA with a MOVE. B D0,PDRB instruction. 
PDRA 
EQU 
$800000 
Base address of PIA (data r e g i s t e r side A) 
DDRA 
EQU 
$800000 
Data d i r e c t i o n r e g i s t e r shares PDRA address 
CRA 
EQU 
$800002 
Control r e g i s t e r address (side A) 
PDRB 
EQU 
$800004 
Side B r e g i s t e r s 
DDRB 
EQU 
$800004 
CRB 
EQU 
$800006 
* 
Select side A DDR by s e t t i n g CRA2 to 0 (we c l e a r a l l b i t s ) 
CLR.B CRA 
Configure side A data r e g i s t e r as input by writing Os i n t o DDRA 
MOVE.B #0,DDRA (this i s the same as CLR.B DDRA) 
Select side B data d i r e c t i o n r e g i s t e r B by s e t t i n g CRB2 to 0 
CLR.B CRB 
Select side B as an output by writing Is i n t o DDRB 
MOVE.B #%11111111,DDRB 
Select PDRA as an input port by s e t t i n g b i t CRA2 to 1 
OR.B 
#%00000100,CRA 
Select PDRB as an output port by s e t t i n g b i t CRB2 to 1 
OR.B 
#%00000100,CRB 
Table 10.5 Configuring 
a PIA. 

426 
Chapter 10 Buses and input/output mechanisms 
Controlling the PIA 
The control registers control the special-purpose pins associated 
with each port of the PIA. Pins CA1 and CA2 control the flow of 
information between the peripheral's A side and the PIA by pro-
viding any required handshaking between the peripheral and 
PIA. Similarly, side B has control pins CB1 and CB2. 
The bits of control register A (CRA) can be divided into 
four groups according to their function. Bits CRAO to CRA5 
define the PIA's operating mode (Fig. 10.26). Bits CRA6 and 
CRA7 are interrupt status bits that are set or cleared by the 
PIA itself. Bit CRA6 is interrupt request flag 1 (IRQA1), 
which is set by an active transition at the CA1 input pin. 
Similarly, CRA7 corresponds to the IRQA2 interrupt request 
flag and is set by an active transition at the CA2 input pin. We 
now examine the control register in more detail. 
7 
6 
5 
4 
3 
2 
1 
0 
IRQA1 IRQA2 
CA2control 
DDRA 
CA1 control 
IR 3A 
CA2 
CA1 
i ' 
A k 
0 
Interrupt request 
to CPU 
Handshake controls 
to peripheral 
8-bit data bus 
Figure 10.26 Structure of the PIA's side A control register. 
CRA1 CRAO Transition of CA1 
control input 
IRQA1 interrupt 
flag status 
negative edge 
negative edge 
positive edge 
positive edge 
set on negative edge 
set on negative edge 
set on positive edge 
set on positive edge 
Status of side A 
interrupt request 
masked 
enabled 
masked 
enabled (asserted) 
Table 10.6 Effect of CA1 control bits. 
CRA5 CRA4 CRA3 Transition of CA2 
control input 
IRQA2 interrupt 
flag status 
Status interrupt 
request 
0 
0 
0 
negative edge 
set on negative 
edge 
masked 
0 
0 
1 
negative edge 
set on negative 
edge 
enabled 
(asserted) 
0 
1 
0 
positive edge 
set on positive 
edge 
masked 
0 
1 
1 
positive edge 
set on positive 
edge 
enabled 
(goes low) 
Table 10.7 Effect of CA2 control bits when CRA5 = 0 (note that E is the 
PIA's clock). 
CA1 control Bits CRAO and CRA1 determine how the PIA 
responds to a change of level (0-to-l or l-to-0) at the CAl 
control input. The relationship between the CAl control 
input, CRAO, CRA1, and the interrupt flag IRQA1 is 
described in Table 10.6. CRA1 determines the sense (i.e. up or 
down) of the transition on CAl that causes the CRA7 inter-
rupt flag (i.e. IRQA1) to be set. CRAO determines whether an 
active transition on CAl generates an interrupt request by 
asserting the IRQA output. CAl can be used as an auxiliary 
input if bit CRAO is clear, or as an interrupt request input if 
bit CRAO is set. 
Whenever an interrupt is caused by an active transition on 
CAl, the interrupt flag in the control register, IRQA1, is set 
and the IRQA output pin goes low. After the CPU has read 
the contents of peripheral data register A, interrupt flag 
IRQA1 is automatically reset. In a typical applica-
tion of the PIA, CAl is connected to a peripheral's 
RDY output so that the peripheral can request 
attention when it is ready to take part in a data 
transfer. 
For example, if CRA 1, CRAO is set to 0,1, a neg-
ative (falling) edge at the CA1 control input sets the 
IRQA1 status flag in control register CRA to 1, and 
the PIA's IRQA interrupt request output is asserted 
to interrupt the host processor. CRAl determines 
the sense of the transition on CAl that sets the 
interrupt flag status and CRAO determines 
whether the PIA will interrupt the host processor 
when the interrupt flag is set. 
Data direction access control (CRA2) When regis-
ter select input RS0 is 0, the data-direction access 
control bit determines whether data-direction regis-
ter A or peripheral data register A is selected. When 
the PIA is reset, CRA2 is 0 so that the data-direction 
register is always available after a reset. 
CA2 control (CRA3, CRA4, CRA5) The CA2 con-
trol pin maybe programmed as an input that gener-
ates an interrupt request in a similar way to CAl, or 
it may be programmed as an output. Bit 5 of the 
control register determines CA2's function. If bit 5 is 
0, CA2 is an interrupt request input (Table 10.7) 
and if bit 5 is 1, CA2 is an output (Table 10.8). 
Table 10.7 demonstrates that the behavior of CA2, 
when acting as an interrupt-request input, is 
entirely analogous to that of CAl. 
When CA2 is programmed as an output with 
CRA5 = 1 it behaves in the manner defined in 
Table 10.8. 
1. Case 1 (CRA5 = 1, CRA4 = 0, CRA3 = 0). This 
is the handshake mode used when a peripheral is 
transmitting data to the CPU via the PIA. A tim-
ing diagram of the action of the handshake mode 
of CA2 is given in Fig. 10.27, together with an 
0 
0 
0 
1 
1 
0 
1 
1 

10.4 Parallel and serial interfaces 427 
Case 
0 
CRA5 
CRA4 
CRA3 
Output CA2 
Low on the falling edge of the 
E clock after a CPU read side A 
data operation 
Low on the falling edge of E after 
a CPU read side A data operation 
Low when CRA3 goes low as a 
result of a CPU write to CRA. 
Always high as long as CRA3 is 
high. Will be cleared on a CPU 
write to CRA that clears CRA3 
High when interrupt flag bit CRA7 is set 
by an active transition of CA1 input 
High on the negative edge of the first E 
pulse occurring during a deselect state 
Always low as long as CRA3 is low. 
Will go high on a CPU write to CRA that 
changes CRA3 to a 1 
High when CRA23 goes high as a result 
of a CPU write to CRA 
Table 10.8 Effect of CA2 control bits when CRA5 = 1. 
PIA clock' 
CA1 interrupt 
request input 
j Following a read side-A 
I data operation 
• 
Peripheral 
• * 
, CA1 
PIA 
CA2 
Peripheral 
CA1 
PIA 
CA2 
Peripheral 
CA1 
PIA 
CA2 
Peripheral 
• CA1 
PIA 
CA2 
Peripheral 
CA1 
PIA 
CA2 
Peripheral 
• CA1 
PIA 
CA2 
At A the peripheral causes an active 
transition on CA1. This tells the 
CPU that data is available. 
At B the PIA responds to the transition 
on CA1 by setting CA2 high. 
At C the PIA brings CA2 low after the 
CPU has read the data. This tells the peripheral 
that the data has been accepted. 
Figure 10.27 The PIA 
input handshake mode 
(case 0 in Table 10.8). 
explanation of the steps involved. In handshake mode 
CA2 goes high whenever a peripheral has data ready for 
reading and remains high until the CPU has read the data 
from the PIA's data register. 
Case 2 (CRA.5 = 1, CRA4 = 0, CRA3 = 1). This is the 
autohandshaking mode and is illustrated in Fig. 10.28. CA2 
automatically produces a single pulse at a low level after 
the side A peripheral data register has been read by the 
CPU. Because the peripheral receives a pulse on CA2 after 
the CPU has read the PIA, the peripheral knows that its 
data has been received and that the PIA is ready for 
new data. 
Case 3 (CRA5 = 1, CRA4 = 1, CRA3 = 0). In this mode 
CA2 is set low and remains in that state until CRA3 is set. 
That is, CA2 is cleared under program control. 
This is the handshake pulse 
CA2 control 
< 
• 
(normally high) 
^ 
• 
Clock 
Goes 
A side 
ov 
dc 
• afte 
ta in 
r a reac 
structio n 
Goes 
edge 
after 
A ins 
high or 
of the r 
the rea 
tructior 
the negative 
lext E clock 
d data side 
Figure 10.28 The PIA autohandshake input mode. 
CAZ 
output. 
1 
1
0 
1 
2 
1
1
0 
3 
1
1
1 
2. 
3. 
0 
1 
1 
1 
0 
1 
1 
1 
1 
B 
A 

428 
Chapter 10 Buses and input/output mechanisms 
4. Case 4 (CRA5 = 1, CRA4 = 1, CRA3 = 1). Now CA2 is 
set to a high level and remains in that state until CRA3 is 
cleared. Cases 3 and 4 demonstrate the use of CA2 as an 
additional output, set or cleared under program control. 
10.4.2 The serial interface 
We now describe the serial interface device that connects a 
computer to a modem or a similar device. Although the serial 
interface was once used to connect PCs to a wide range of 
external peripherals, the USB and FireWire interfaces have 
largely rendered the serial interface obsolete in modern PCs. 
Serial data transmission is used by data transmission sys-
tems that operate over distances greater than a few meters 
and Chapter 14 will have more to say on the subject of data 
transmission. Here we're more interested in the asynchronous 
communications adapter (ACIA) interface, which connects a 
CPU to a serial data link. 
The serial interface transfers data into and out of the CPU 
a bit at a time along a single wire; for example, the 8-bit value 
101100012 would be sent in the form of eight or more pulses 
one after the other. Serial data transfer is slower than the par-
allel data transfer offered by a PIA, but is inexpensive because 
it requires only a single connection between the serial inter-
face and the external world (apart from a ground-return). 
We are not concerned with fine details of the ACIA's inter-
nal operation, but rather in what it does and how it is used to 
transmit and receive serial data. When discussing serial trans-
mission we often use the term character to refer to a unit of 
data rather than byte, because many transmission systems are 
designed to transmit information in the form of ISO/ASCII-
encoded characters. 
Figure 10.29 demonstrates how a 7-bit character is trans-
mitted bit by bit asynchronously. During a period in which 
no data is being transmitted from an ACIA, the serial output 
is at a high level, which is called the mark condition. When a 
character is to be transmitted, the ACIA's serial output is put 
in a low state (a mark-to-space transition) for a period of one 
bit time. The bit time is the reciprocal of the rate at which 
successive serial bits are transmitted and is measured in Baud. 
In the case of a two-level binary signal, the Baud corresponds 
to bits/s. The initial bit is called the start bit and tells the 
receiver that a stream of bits, representing a character, is 
about to be received. If data is transmitted at 9600 Baud, each 
bit period is 1/9600 = 0.1042 ms. 
During the next seven time slots (each of the same dura-
tion as the start bit) the output of the ACIA depends on the 
value of the character being transmitted. The character is 
transmitted bit by bit. This data format is called non-return to 
zero (NRZ) because the output doesn't go to zero between 
individual bits. After the character has been transmitted, a 
further two bits (a parity bit and a stop bit) are appended to 
the end of the character. 
At the receiver, a parity bit is generated locally from the 
incoming data and then compared with the received parity 
bit. If the received and locally generated parity bits differ, an 
error in transmission is assumed to have occurred. A single 
parity bit can't correct an error once it has occurred, nor 
detect a pair of errors in a character. Not all serial data trans-
mission systems employ a parity bit error detector. 
The stop bit (or optionally two stop bits) indicates the end 
of the character. Following the reception of the stop bit(s), 
the transmitter output is once more in its mark state and is 
ready to send the next character. The character is composed 
of 10 bits but contains only 7 bits of useful information. 
The key to asynchronous data transmission is that once the 
receiver has detected a start bit, it has to maintain synchroniza-
tion only for the duration of a single character. The receiver 
examines successive received bits by sampling the incoming 
Mark 
Space 
Start 
bit 
7 data bits 
« 
^4 
1 
Parity 
Stop 
bit 
bit 
One character 
Example: Letter M = 1001101 (even parity) 
Mark 
Space 
- K -
7 data bits 
-*-«-
-*4-
Start 
1 
bit 
1 
1 
0 
Stop 
Parity bit bit 
Figure 10.29 Format of 
an asynchronous serial 
character. 

10.4 Parallel and serial interfaces 
4 2 9 
CPU side 
interface 
Chip-select 
and 
read-write 
control 
Data bus 
buffers 
Transmitter data register 
7 
6 
5 
4 
3 
2 
1 
0 
d7 
d6 
ds d4 
d3 
£?„„. di 
do 
Status register 
7 
IRQ 3VRN PE FE CTS 
1 
DCDTDRE RDRE 
Control register 
7 
6 
5 4 
3 
2 
1 
0 
RfE 
TX control 
Word select 
Clock control 
Receiver data register 
7 
6 
s 
4 
3 
2 
1 
0 
an d6 
ds d4 d3 d, d, do 
Serial interface 
Clock 
generator 
Parity 
generator 
Transmitter shift register 
Transmitter control 
interrupt logic 
Receiver control 
Parity check 
senerator 
Receiver 
shift register 
Receiver 
clock generator 
Synchronizing logic 
TxD 
• TxClk 
•CTS 
RTS 
Transmitter 
control 
IRQ 
rRxD 
RxClk 
DCD 
Receiver 
control 
Figure 10.30 Organization 
oftheACIA. 
signal at the center of each pulse. Because the clock at the 
receiver is not synchronized with the clock at the transmitter, 
each received data bit will not be sampled exactly at its center. 
Figure 10.30 provides the internal arrangement of a typical 
ACIA, a highly programmable interface whose parameters 
can be defined under software control. The ACIA has a single 
receiver input pin and a single transmitter output pin. 
The ACIA's Peripheral side pins 
The ACIA communicates with a peripheral via seven pins, 
which maybe divided into three groups: receiver, transmitter, 
and modem control. At this point, all we need say is that the 
modem is a black box that interfaces a digital system to the 
public switched telephone network and therefore permits 
digital signals to be transmitted across the telephone system. 
A modem converts digital signals into audio (analog) tones. 
We'll look at the modem in more detail in Chapter 14. 
Receiver The receiver part of the ACIA has a clock input and 
a serial data input. The receiver clock is used to sample the 
incoming data bits and may be 64, 16, or 1 times that of the 
bit rate of the received data; for example, an ACIA operating 
at 9600 bits/s might use a 16X receiver clock of 153 600 Hz. 
The serial data input receives data from the peripheral to 
which the ACIA is connected. Most systems require a special 
interface chip between the ACIA and the serial data link to 
convert the signal levels at the ACIA to the signal levels found 
on the data link. 
Transmitter The transmitter part of the ACIA has a clock 
input from which it generates the timing of the transmitted 
data pulses. 
Modem control The ACIA communicates with a modem or 
similar equipment via three active-low pins (two inputs and 
one output). The ACIA's request to send (RTS) output may be 
set or cleared under software control and is used by the ACIA 
to tell the modem that it is ready to transmit data to it. 
The two active-low inputs to the ACIA are clear-to-send 
(CTS) and data-carrier-detect (DCD). The CTS input is a sig-
nal from the modem to the ACIA that inhibits the ACIA from 
transmitting data if the modem is not ready (because the tele-
phone connection has not been established or has been bro-
ken). If the CTS input is high, a bit is set in the ACIA's status 
register, indicating that the modem (or other terminal equip-
ment) is not ready for data. 
The modem uses the ACIA's DCD input to tell the ACIA 
that the carrier has been lost (i.e. a signal is no longer being 
received) and that valid data is no longer available at the 
receiver's input. A low-to-high transition at the DCD input 
sets a bit in the status register and may also initiate an inter-
rupt if the ACIA is so programmed^ applications of the 
ACIA that don't use a modem, the CTS and DCD inputs are 
connected to a low level and not used. 
The ACIA's internal registers 
The ACIA has four internal registers: a transmitter data regis-
ter (TDR), a receiver data register (RDR), a control register 
(CR), and a status register (SR). Because the ACIA has a sin-
gle register-select input RS, only two internal registers can be 
directly accessed by the CPU. Because the status and receiver 
data registers are always read from, and the transmitter data 
register and control register are always written to, the ACIA's 
R/W input 
distinguishes between 
the two pairs of 
registers. The addressing arrangement of the ACIA is given in 
Table 10.9. 
The control register is a write-only register that defines the 
operational properties of the ACIA, particularly the format of 

430 
Chapter 10 Buses and input/output mechanisms 
the transmitted or received data. Table 10.10 defines the con-
trol register's format. The counter division field, CRO and 
CR1, determines the relationship between the transmitter and 
receiver bit rates and their respective clocks (Table 10.11). 
When CR1 and CRO are both set to one, the ACIA is reset 
and all internal status bits, with the exception of the CTS and 
DCD flags, are cleared. The CTS and DCD flags are entirely 
dependent on the signal level at the respective pins. The ACIA 
RS 
R/W 
Type of register 
ACIA register 
0 
0 
Write only 
Control 
0 
1 
Read only 
Status 
1 
0 
Write only 
Transmitter data 
1 
1 
Read only 
Receiver data 
Table 10.9 Register selection scheme of the ACIA. 
7 
6 
5 
4 
3 
2 
1
0 
Receive 
Transmitter control 
Word select 
Counter division 
interrupt 
enable 
Table 10.10 Format of the ACIA's control register. 
CR1 
CRO 
Division ratio 
0 
0
-
1 
0 
1 
-16 
1 
0 
-64 
1 
1 
Master reset 
Table 10.11 Relationship between CR1, CRO, and the division ratio. 
CR4 CR3 
CR2 
Word length 
Parity 
Stop bits Total bits 
0 
0 
0 
7 
Even 
2 
11 
0 
0 
1 
7 
Odd 
2 
11 
0 
1 
0 
7 
Even 
1 
10 
0 
1 
1 
7 
Odd 
1 
10 
1 
0 
0 
8 
None 
2 
11 
1 
0 
1 
8 
None 
1 
10 
1 
1 
0 
8 
Even 
1 
11 
1 
1 
1 
8 
Odd 
1 
11 
is initialized by first writing ones into bits CR1 and CRO of the 
control register, and then writing one of the three division 
ratio codes into these positions. In the majority of systems 
CR1 = 0 and CRO = 1 for a divide by 16 ratio. 
The word select field, CR2, CR3, CR4, defines the format of 
the received or transmitted characters. These three bits allow the 
selection of eight possible arrangements of number of bits per 
character, type of parity, and number of stop bits (Table 10.12). 
For example, if you require a word with 8 bits, no parity, and 1 
stop bit, control bits CR4, CR3, CR2 must set to 1,0,1. 
The transmitter control field, CR5 and CR6, determines 
the level of the request to send (RTS) output, and the genera-
tion of an interrupt by the transmitter portion of the ACIA. 
Table 10.13 gives the relationship between these controls bits 
and their functions. RTS can be employed to tell the modem 
that the ACIA has data to transmit. 
The transmitter interrupt mechanism can be enabled or 
disabled depending on whether you are operating the ACIA in 
an interrupt-driven or in a polled data mode. If the transmit-
ter interrupt is enabled, a transmitter interrupt is generated 
whenever the transmitter data register (TDR) is empty, signi-
fying the need for new data from the CPU. If the ACIA's clear-
to-send input is inactive-high, the TDR empty flag bit in the 
status register is held low, inhibiting any transmitter interrupt. 
The effect of setting both CR6 and CR5 to a logical one 
requires some explanation. If both these bits are high, a break 
(space level) is transmitted until the bits are altered under 
software control. A break can be used to generate an interrupt 
at the receiver because the asynchronous format of the serial 
data precludes the existence of a space level for more than 
about 10 bit periods. 
The receiver interrupt enable field consists of bit CR7 
which, when clear, inhibits the generation of interrupts by the 
receiver portion of the ACIA. Whenever bit CR7 is set, a 
receiver interrupt is generated by the receiver data register 
(RDR) flag of the status byte going high, indicating the pres-
ence of a new character ready for the CPU to read. A receiver 
interrupt can also be generated by a low-to-high transition at 
the data-carrier-detect (DCD) input, signifying the loss of a 
carrier. CR7 is a composite interrupt enable bit. It is impossi-
ble to enable either an interrupt caused by the RDR being 
empty or an interrupt caused by a positive transition on the 
DCD pin alone. 
CR6 
CR5 
$RTS$ 
Transmitter interrupt 
0 
0 
Low 
Disabled 
0 
1 
Low 
Enabled 
1 
0 
High 
Disabled 
1 
1 
Low 
Disabled—a break level is placed on 
the transmitter output 
Table 10.12 The word select bits. 
Table 10.13 Function of transmitter control bits CR5, CR6. 

10.4 Parallel and serial interfaces 
431 
Configuring the ACIA 
The following 68000 assembly language listing demonstrates 
how the ACIA is initialized before it can be used to transmit 
and receive serial data. 
Setting up an ACIA 
ACIA 
EQU 
$800000 
CR 
EQU 
0 
LEA 
ACIA,A0 
Perform a software reset by writing 1,1 to CR1, 
MOVE.B #%00000011,CR(AO) 
0,1 
Select counter division ratio as clk/16 CR1,CR0 
Select character format CR4,CR3,CR2 = 1,0,1 
Select operating mode 
CR6,CR5 = 0,1 = assert RTS and enable transmitter interrupt 
Select receiver interrupt mode CR7 = 1 to enable Rx interrupt 
MOVE.B niQ110101,CR(A0) Set up ACIA 
Bit 1—transmitter data register empty (TDRE) This flag is 
the transmitter counterpart of RDRF. A logical 1 in TDRE 
indicates that the contents of the transmitter data register 
(TDR) have been transmitted and the register is now ready for 
new data. The IRQ bit is 
also set whenever the 
TDRE flag is set if the trans-
mitter interrupt is enabled. 
The TDRE bit is 0 when 
the TDR is full, or when the 
CTS input is high, indicat-
ing that the terminal equip-
ment is not ready for data. 
The fragment of code 
below demonstrates how 
the TDRE flag is used when 
the ACIA is operated in a 
polled output mode. 
Location of ACIA in memory 
Control register offset 
A0 points to ACIA 
CR0 
Subroutine to transmit a character 
REPEAT 
Read ACIA status 
UNTIL TDRE = 1 
Write data to ACIA 
ACIA 
EQU 
$800000 
TDRE 
EQU 
1 
SR 
EQU 
2 
DR 
EQU 
0 
LEA 
ACIA,A0 
POLL 
BTST 
#TDRE,SR(AQ) 
BEQ 
POLL 
MOVE.I 
RTS 
3 D0,DR(AO) 
The status register The 
status register has the 
same address as the 
control register, but is 
distinguished from it by 
being a read-only regis-
ter. Table 10.14 gives the 
format of the status 
register. Let's look at the 
function of these bits. 
Bit 0—receiver data 
register full (RDRF) 
When set the RDRF bit 
indicates 
that 
the 
receiver data register is full and a character has been received. 
If the receiver interrupt is enabled, the interrupt request flag, 
bit 7, is also set whenever RDRF is set. Reading the data in the 
receiver data register clears the RDRF bit. Whenever the DCD 
input is high, the RDRF bit remains at a logical zero, indicat-
ing the absence of any valid input. 
The RDRF bit is used to detect the arrival of a character 
when the ACIA is operated in a polled input mode. 
* Subroutine to receive a character 
* 
REPEAT 
* 
Read ACIA status 
* 
UNTIL RDRF = 1 
* Read ACIA data 
ACIA 
EQU 
$800000 
RDRF 
EQU 
0 
SR 
EQU 
2 
DR 
EQU 
0 
.
:
, 
L
E
A 
.
•
•
•
.
/
.
;
•
. ,AciAjA0;;,,:.; 
POLL'.'.;"' , ;BTST" •„'. ':#RDRF,SR(A0) 
':."':'"BEQ ;'"; ;"POLL';.'';:'; ; 
'""'"':"MOVE';B;' 
• RTS 
:'
;;"b'R
;:(A'&')VD0"
:V:"
:'
:. 
Rx data ready = bit 0 of SR 
Offset for status register 
Offset for data register 
: 
A0 points to ACIA 
REPEAT, Test. R.x status ;bit•;;•;; 
UNTIL character ^received'.:;"'; 
"M6ve
;;'input"
:"
;;f rom''.
:,ACIA'
;;t6
;,'.'D0''"' 
Transmitter data register empty = bit:1 
Offset for status register 
Offset for data register 
A0 points to ACIA base 
Test: transmitter for empty state. 
Repeat until transmitter ready 
.,.....,, 
Move byte from DO to ACIA 
Bit 2—data carrier detect (DCD) The DCD bit is set when-
ever the DCD input is high, indicating that a carrier is not 
present. The DCD pin is normally employed only in conjunc-
tion with a modem. When the signal at the DCD input makes 
a low-to-high transition, the DCD bit in the status register is 
set and the IRQ bit is also set if the receiver interrupt is 
enabled. The DCD bit remains set even if the DCD input 
returns to a low state. To clear the DCD 
bit, the CPU must read the contents of 
the ACIA's status register and then the 
contents of the data register. 
Bit 3—clear to send (CTS) The CTS bit 
directly reflects the status of the ACIA's 
CTS input. A low level on the CTS input 
indicates that the modem is ready for 
data. If the CTS bit is set, the transmitter 
data register empty bit is inhibited 
(clamped at zero) and no data may be 
transmitted by the ACIA. 

432 
Chapter 10 Buses and input/output mechanisms 
7 
6 
5 
4 
3 
2 
1 
0 
IRQ 
PE 
OVRN 
FE 
CTS 
DCD 
TDRE 
RDRF 
Table 10.14 Format of the ACIA's control register. 
Bit 4—framing error (FE) The FE bit is set whenever a 
received character is incorrectly framed by a start bit and a 
stop bit. A FE is detected by the absence of the first stop bit and 
indicates a synchronization (timing) error, a faulty transmis-
sion, or a break condition. The FE flag is set or cleared during 
receiver data transfer time and is present throughout the time 
that the associated character is available. 
Bit 5—receiver overrun (OVRN) The OVRN flag bit is set 
when a character is received, but hasn't been read by the CPU 
before a subsequent character is 
received. The new character over-
writes the previous character, 
which is now lost. Consequently, 
the OVRN bit indicates that one 
or more characters in the data 
stream 
have 
been 
lost. 
Synchronization is not affected by 
an overrun error—the error is 
caused by the CPU not reading a 
character, rather than by a fault in 
the transmission process. The 
overrun bit is cleared after reading 
the data from the RDR or by a 
master reset. Modern ACIAs usu-
ally have FIFO buffers to hold sev-
eral characters to give the CPU 
more time to read them. 
Bit 6—parity error (PE) The PE is 
set whenever the received parity 
bit does not agree with the parity 
bit generated locally at the receiver 
from the preceding data bits. Odd 
or even parity may be selected by 
writing the appropriate code into 
bits 2,3, and 4 of the control regis-
ter. If no parity is selected, then 
OVERR 
both the transmitter parity gener-
ator and the receiver 
parity 
checker are disabled. Once a PE 
has been detected and the PE bit 
EXIT 
RTS 
set, it remains set as long as a char-
acter with a PE is in the receiver data register. 
Bit 7—interrupt request (IRQ) The IRQ bit is a composite 
interrupt request flag because it is set whenever the ACIA 
AC I AC 
EQU 
$800000 
ACIAD 
EQU 
ACIAC+2 
RDRF 
EQU 
0 
TDRE 
EQU 
1 
DCD 
EQU 
2 
CTS 
EQU 
3 
FE 
EQU 
4 
OVRN 
EQU 
5 
PE 
EQU 
6 
INPUT 
MOVE. .B 
ACIAC,D0 
BTST 
#RDRF,D0 
BNE 
ERR CK 
BTST 
#DCD,D0 
BEQ 
INPUT 
BRA 
DCD ERR 
ERR_CK 
BTST 
#FE,D0 
BNE 
FE ERR 
BTST 
#OVRN,DO 
BNE 
OV ERR 
BTST 
#PE,D0 
BNE 
PE_ERR 
MOVE. ,B 
ACIAD,DO 
* 
BRA 
EXIT 
DCD_ERR 
* 
BRA 
EXIT 
FE_ERR 
Deal with framing 
BRA 
EXIT 
wishes to interrupt the CPU, for whatever reason. The IRQ 
bit may be set by any of the following: 
• receiver data register full (SR bit 0 set) 
• transmitter data register empty (SRbit 1 set) 
• DCD bit set (SRbit 2). 
Whenever IRQ = 1 the ACIA's IRQ pin is forced active-
low to request an interrupt from the CPU. The IRQ bit is 
cleared by a read from the receiver data register or a write to 
the transmitter data. 
Programming the ACIA 
We are now going to look at a more complete program that 
uses some of the AClA's error-detecting facilities when 
receiving data. 
Base address of ACIA 
Address of data register 
Receiver data register full 
Transmitter data register empty 
Data carrier detect 
Clear to send 
Framing error 
Over run 
Parity error 
Get status from ACIA 
Test for received character 
If character received then test 
Else test for loss of signal 
Repeat loop while DCD clear 
Else deal with loss of signal 
Test for framing error 
If framing error, deal with it 
Test for overrun 
If overrun, deal with it 
Test for parity error 
If parity error deal with it 
Load the input into DO 
Deal with loss of signal 
SR 
Deal 
BRA 
with overrun error 
EXIT 
PE_ERR 
Deal with parity error 
So far we've examined how information in digital form 
is read by a computer, processed in the way dictated by a 
program and then output in digital form. We haven't yet 
error 

10.4 Parallel and serial interfaces 
433 
considered how information is converted between real-world 
form and digital form. In the next chapter we describe some 
of the most frequently used computer interfaces such as the 
keyboard, the display, and the printer. 
s* SUMMARY 
A computer is not just a device that executes instructions; it 
is a complete system with subsystems that process data, 
store data, and move data between the computer and 
outside world. 
In this chapter we began by examining the bus, the data 
highway that moves information between a computer's 
various parts. There is not, in fact, a single bus. Like CPUs, 
buses have developed over the years and different computers 
use different buses. Even within a computer there may be a 
family of buses, each of which is optimized for a specific 
application. 
We have described the characteristics of a computer bus and 
introduced some of the members of the bus family. As well as 
the conventional parallel bus, we've looked at the USB serial bus 
used to provide a low-cost high-performance solution to the 
interconnection of peripherals ranging from the keyboard to the 
printer or scanner. 
We have also looked at the IEEE488 bus that was designed for 
use in a computer-controlled automated laboratory environ-
ment. This bus incorporates some very interesting principles 
such as the ability to transfer data between groups of devices 
with very different characteristics. In particularly, it is able to 
send data between serial devices simultaneously using a 
three-wire handshake that ensures the transfer does not 
continue until each device has completed the data transfer. 
Moreover, it uses two different methods of sending control 
messages to devices connected to the bus. One method 
uses encoded messages on the 8-bit data bus and the other 
method used single-line control signals on a special 
control bus. 
There are three broad strategies for moving information onto 
or out of a computer. One is called programmed I/O in which 
the programmer copies data to or from the peripheral directly 
by means of appropriate instructions in the program. One is 
called interrupt driven I/O in which an external device requests 
a data transfer when it is ready and then the operating system 
handles the transfer. The third technique is called direct memory 
access in which a subsystem that is almost as complex as the 
CPU itself takes over and transfers data directly between the 
peripheral and memory. We have looked at how all these three 
I/O strategies are implemented and their advantages and 
disadvantages. 
The final part of this chapter described two special-purpose 
integrated circuits that are designed to facilitate the transfer of 
data between a computer and external peripherals such as 
modems and printers. Both these devices control the flow of 
information between the computer and peripheral. One, the 
parallel interface, uses handshaking to sequence the flow of 
information and the other, the serial device, uses flow control 
signals between the peripheral and external modem. 
« 
PROBLEMS 
10.1 Why is the bus such an important element of a computer 
system? 
10.2 Why do computers use families of buses rather than a 
single bus? 
10.3 In the context of buses, what is the meaning of arbitration? 
10.4 It takes 1 |j,s for a computer to take control of a 64-bit 
bus. Suppose it takes 20 ns to set up a data transfer and 40 ns to 
terminate a transfer and that a transfer consists of sending eight 
64-bit values in 32 ns.What is the average data rate in bytes/s 
that this bus can support? 
10.5 What is the difference between an open-loop and a 
closed-loop data transfer? 
10.6 What is special (different) about the IEEE 488 bus 
three-wire data handshake? 
10.7 What factors determine the ultimate rate at which data 
can be transported by a bus? 
10.8 Although most microprocessors implement 
memory-mapped I/O, are there any advantages in 
implementing dedicated I/O mechanisms with dedicated I/O 
instructions and appropriate control signals? 
10.9 What is an input/output strategy, as opposed to and 
input/output device? 
10.10 What is programmed I/O? 
10.11 Define the meaning of the following terms in the context 
of I/O operations. 
(a) Port 
(b) Peripheral 
(c) FIFO 
(d) Handshake 
(e) Interlocked handshake 
(f) Polling loop 
10.12 What is the difference between an unintelligent I/O 
device (e.g. a flip-flop) and an intelligent I/O device 
(e.g. a PI A)? 
10.13 Why does the CPU have to save the processor 
status (i.e. status byte and CCR) before responding to 
an interrupt? 
10.14 What is a non-maskable interrupt and how is it used? 
10.1 S Explain how daisy-chaining can be used to improve the 
way in which interrupts are handled in a system with many 
peripherals. 
10.16 What is the role of the 68K's interrupt mask in its 
prioritized interrupt handling system? 
10.17 What is a prioritized interrupt? 
10.18 What is a vectored interrupt? 
10.19 To what extent are interrupts and subroutines the same 
and to what extent do they differ? 
10.20 In a particular computer, the overhead involved in an 
interrupt call is 5 u.s and the overhead involved in a return 
from interrupt is 6 p.s. Suppose that this computer executes 

434 
Chapter 10 Buses and input/output mechanisms 
10 instructions/|xs. How many instructions can be used in an 
interrupt handling routine if the overall interrupt handling 
efficiency is to be greater than 70%? 
10.21 What is DMA and why is it so important in high 
performance systems? 
10.22 What are the advantages and disadvantages of 
memory-mapped I/O in comparison with dedicated I/O that 
uses special instructions and signals? 
10.23 The PIA has six internal registers and two register select 
lines. How does the PIA manage to select six registers with only 
two lines? 
10.24 Can you think of any other way of implementing a 
register select scheme (other than the one used by the PIA)? 
10.25 In the context of the PIA, what is a data direction register 
(DDR), and how is it used? 
10.26 How does the PIA use its CA1 and CA2 control lines to 
implement handshaking? 
10.27 How are the characters transmitted over a serial data link 
divided into individual characters and bits? 
10.28 What are the functions of the ACIA's DCD and CTS 
inputs? 
10.29 What is the difference between a framing error and an 
overrun error? 
10.30 The 68K's status register (SR) contains the value $2601. 
How is this interpreted? 

Computer peripherals 
INTRODUCTION 
Humans communicate with each other by auditory and visual stimuli; that is, we speak, gesticulate, 
write to each other, and use pictures. You would therefore expect humans and computers to 
communicate in a similar way. Computers are good at communicating visually with people; they 
can generate sophisticated images, although they are rather less good at synthesizing natural-
sounding speech. Unfortunately, computers can't yet reliably receive visual or sound input directly 
from people. Hardware and software capable of reliably understanding speech or recognizing visual 
input does not yet exist—there are systems that can handle speech input and systems that can 
recognize handwriting, but the error rate is still too large for general-purpose use.1 Consequently, 
people communicate with computers in a different way than they communicate with other people. 
The keyboard and video display are the principal input and output devices used by personal 
computers. The terms input and output refer here to the device as seen from the CPU; that is, a 
keyboard provides an output, which, in turn, becomes the CPU's input. 
The CRT (cathode ray tube) display is an entirely electronic device that's inexpensive to produce. 
It is cheap because it relies on semiconductor technology for its electronics and on tried-and-tested 
television technology for its display. By 2000 the more compact but expensive LCD panel was 
beginning to replace the CRT display. Less than 4 years later, the trend had accelerated and large, high-
quality, high-resolution LCD displays were widely available. By 2005 CRT displays were in decline. 
This chapter looks at keyboards, pointers, displays, and printers. We also look at input devices 
that do more than communicate with people; we show how physical parameters from 
temperature and pressure to the concentration of glucose in blood can be measured. 
The second part of this chapter demonstrates how the digital computer interacts with the 
analog world by converting analog values into digital representations and vice versa. At the end of 
this chapter we provide a very brief insight into how a computer can be used to control real-world 
analog systems with the aid of digital signal processing (DSP). 
' You can buy speech recognition programs but they have to be 
trained to match a particular voice. Even then, their accuracy is less than 
perfect. Similarly, hand-held computers provide handwriting recogni-
tion provided you write in a rather stylized way. 
CHAPTER MAP 
9 Processor 
architectures 
Chapter 9 provides a brief 
overview of contrasting 
processors; its purpose is to 
expose students to some of the 
processors that are available to 
the designer. 
0 Buses and input/ 
output mechanisms 
Chapter 10 deals with 
input/output techniques and 
shows how information is 
transferred between a computer 
and its peripherals. We look at 
internal buses that link devices 
within the computer and 
external buses that link remote 
devices such as printers with the 
computer. 
11 Computer peripherals 
The power ol a computer is as 
much a function of its peripherals 
as i)' if; data processing 
capabilities. Chapiei 11 
introduces some of the 
peripherals you'd find in a typical 
PC such as the keyboard, display, 
printer, and mouse, as well as 
some of the more unusual 
peripherals that, for example, can 
measure how fas: a body is 
rotating. 
12 Computer memory 
Information isn't stored in a 
computer in just one type of 
storage device; it's stored in 
DRAM, on disk, on CD-ROM, DVD 
and on tape. Chapter 12 
examines the operating principles 
and characteristics of the storage 
devices found in a computer. 

436 
Chapter 11 Computer peripherals 
11.1 Simple input devices 
We begin this chapter by introducing some of the simplest 
computer peripherals, the input devices used by personal 
computers. We describe both the keyboard and the pointing 
device. 
11.1.1 The keyboard 
The keyboard is an efficient interface—especially if you're a 
professional typist entering text at 60 words/minute. 
Sometimes the keyboard provides a poor interface; for exam-
ple, you can't easily use it to control a flight simulator. 
Figure 11.1 illustrates the layout of the ubiquitous 
QWERTY keyboard. The term QWERTY isn't an acronym 
but the sequence of letters on the back row of characters on a 
keyboard. When the first mechanical typewriters were 
constructed, the sequence of letters was chosen to reduce the 
probability of letters jamming. If Y and 'h' were next to each 
other, typing 'the' would sometimes cause the letters't' and 'h' 
to collide and jam. The anti-jamming property of the 
QWERTY keyboard is optimum only for the English 
language. 
Because today's keyboards are electronic with no moving 
parts except the keys themselves, there's no longer any need 
for a QWERTY layout. You could devise a much better layout 
for the keys that would make it easier to type by reducing the 
distance a typist's fingers have to move. Indeed, a keyboard 
was developed in the 1920s to make it easier to type English. 
Studies demonstrate that a typist using a Dvorak keyboard 
can achieve a 10 to 15% improvement. The Dvorak is biased 
in favor of right-handed typists. However, so many typists 
and programmers have been trained on the QWERTY 
keyboard, that it would be difficult to retrain them to use a 
new layout. The Dvorak keyboard has therefore failed to 
topple the QWERTY standard. 
Some systems designed for infrequent computer users and 
non-typists have a simple ABCDE keyboard in which the keys 
are laid out in alphabetic order—this keyboard makes it easy 
for users to locate keys, but prevents experienced users enter-
ing data rapidly (because they will have been trained on a 
QWERTY layout). 
A radically different form of keyboard is the chord key-
board, which has only a few (typically 4 or 5) keys. You enter 
a letter by hitting a subgroup of keys simultaneously; it's 
rather like using Morse code or Braille. The chord keyboard is 
very small indeed and can be used with one hand. Chord key-
boards have found a niche market for people who operate in 
cramped spaces or for those who want a pocket-sized device 
that they can use to make notes as they move about. 
Figure 11.2 illustrates the structure of the cord keyboard. 
Special purpose keys 
In order to provide the total number of keys necessary for 
efficient computer operation a keyboard would have to be 
gigantic. In practice, most keys have a multiple function; that 
is, the meaning of a given key can be modified by pressing 
another key at the same time. The shift key selects lower case 
characters as the default mode and upper case characters 
when it's pressed at the same time as a letter. The shift key also 
selects between pairs of symbols that share the same key 
(e.g.: and;, @ and ', + and = , etc.) and between numbers 
and symbols (e.g. 4 and $, 5 and %, 8 and *, etc.). 
Although the layout of the letters and numbers on a 
QWERTY keyboard is standard throughout the English-
speaking world, the layout of other keys (e.g. symbols) is 
not—in particular, there is a difference between keyboards 
designed for use in the USA and those designed for use in the 
F1 |F2 F3 F4 i F5|F6 F7 F8 ' 
J' 
1 . . 
F9 
Flfl F11 F 1 2
1 :'.•• Scroll Ita' ^ 
• 
; :• 
4 — ; Ins Home 
End 
R]Upi M 
F^Dnj j 1 8 
- H 
•• 
( 
i 
="• ^
1 . 
; . Del 
J 
Home 
End 
R]Upi M 
F^Dnj j 1 8 
2 3 
-—, 
—^fater; 
Figure 11.1 Layout of the QWERTY keyboard. 
i ; 
.. 
• 
: 
• 
: 
fJ 
Mi \ 
! o 
4 
5 6 
± 
9 

11.1 Simple input devices 
437 
Figure 11.2 Layout of the cord keyboard. 
UK. Consequently, software has to be configured for the 
specific version of the keyboard currently in use. 
Computer keyboards also include a control (ctrl) key, 
which behaves like a shift key and gives a key a different 
meaning when control is pressed at the same time. Computer 
texts indicate the act of pressing the control key and, say, the 
letter D at the same time by the notation CTRL-D. 
Why do we need all these special keys? When we commu-
nicate with a computer we need to provide it with two types 
of information. One type of information is the data the com-
puter is going to process (e.g. the text entered into a word 
processor, or a booking entered into an airline's database). 
The other type of information entered into a computer is the 
commands that you want it to execute. Suppose that you are 
entering text into a word processor and wish to save the file. 
You can't simply type Save file because the computer cannot 
distinguish between the command you want to carry out and 
the words you are entering into the document. By typing, for 
example, CTRL-S, you are telling the computer unambigu-
ously that you are entering the command to save a file.2 
PC keyboards also provide an alternative (alt) key to give 
yet another set of meanings to the keys. Consequendy, you 
can enter a key unshifted, on with shift, control, alternative, 
or any combination of the three function-modifier keys. In 
addition to the shift, control, and alternative keys, the PC 
keyboard contains 12 function keys labeled Fl to F12 that 
perform special functions; e.g. function key Fl is normally 
used to invoke a program's 'Help' function. Finally, keyboards 
have several dedicated keys like home, end, PgDn, PgUp, Del, 
Ins, and so on. 
Computer displays invariably have a cursor—a marker on 
the screen indicating die currently active position; that is, if 
you enter a character, it will appear at the position indicated 
by the cursor. Cursors can be vertical or horizontal lines, 
small blocks, highlighted text, or even reversed text (i.e. white 
on black). Modern applications frequendy make use of 
several different types of cursor; for example, a solid line 
indicates where text can be entered, an arrow points at a 
command, and a cross indicates the edge of a picture or a 
table. Cursors sometimes blink because human vision can 
more easily detect a change in a static picture). Computer 
keyboards also contain cursor control keys that move the cur-
sor on the screen up, down, left, or right, by one unit—either 
a character position horizontally or a line position vertically. 
Several technologies can be used to detect a keystroke 
(e.g. mechanical, magnetic, capacitive, etc.). The difference 
between keyboards is often a matter of cost and personal 
preference—some typists prefer to hear a satisfying click 
when they depress a key, others don't. Important keys like 
enter, shift, control, and space are often made larger than 
other keys to make it easy to hit them. If you are a real sadist, 
keyboard design is just for you because you can guarantee a 
maximum level of user misery by locating a key that has 
a potentially destructive function (e.g. delete text) next to a 
normal key such as the space bar. Good practice would ensure 
that it is difficult to enter a potentially fatal command by acci-
dent. Consider the following two examples of safe operation: 
you can't start a VCR recording without pressing two buttons 
simultaneously, and the master engine switches in some air-
craft are under a metal bar that blocks access to the switch to 
ensure that you can't switch an engine off accidentally. 
Character codes 
The ASCII code for the upper-case letter3 we call 'A' is 
010000012. In order to convert this computer representation 
of'A into the actual letter'A' on paper we have to use a periph-
eral called a printer. Similarly, we have to strike a key on a 
keyboard with the symbol 'A on it in order to get the code 
010000012 into the computer. We now look at peripherals in 
typical personal computers that input and output data— 
beginning with the keyboard. Following the keyboard we 
describe the video display and printer, respectively. This 
chapter concludes with some of die interfaces associated with 
computers used in more general applications. 
The switch 
A keyboard is composed of two parts, a set of keys that detect 
the pressure of a finger and an encoder, which converts die out-
put of a key into a unique binary code representing that key. 
The keyswitch, which detects the pressure of a finger, called 
a keystroke, is often a mechanical device (see Fig. 11.3(a). 
A typical keyswitch contains a plunger that is moved by a finger 
2 The Windows environment has much reduced the need for special-
purpose keys because you can use a ponting device to puD down a menu 
and select the required option. 
3 A character as represented on paper or a screen is called a glyph. 
A, A, and A are all glyphs. 

438 
Chapter 11 Computer peripherals 
against the pressure of a spring. As it moves down, the 
plunger forces two wires together to make a circuit—the 
output of this device is inherently binary (on or off). A small 
stainless steel snap-disk located between the plunger and base 
of the switch produces an audible click when bowed down-
wards by the plunger. A similar click is made when the 
plunger is released. This gives depressing a keyswitch a 
positive feel because of its tactile feedback. 
Figure 11.3(b) describes the membrane switch, which 
provides a very-low-cost mechanical switch for applications 
such as microwave oven control panels. A thin plastic 
membrane is coated with a conducting material and spread 
over a printed circuit board. Either by forming the plastic 
membrane into slight bubbles or by creating tiny pits in the 
PCB, it is possible to engineer a tiny gap between contacts on 
the PCB and the metal-coated surface of the membrane. 
Pressure on the surface of the membrane due to a finger 
pushes the membrane against a contact to close a circuit. The 
membrane switch can be hermetically sealed for ease of 
cleaning and is well suited to applications in hazardous or 
dirty environments (e.g. mines). Equally, the membrane 
switch suffers all the disadvantages of other types of low-cost 
mechanical switch. 
Another form of mechanical switch employs a plunger 
with a small magnet embedded in one end. As this magnet is 
pushed downwards, it approaches a reed relay composed of 
two gold-plated iron contacts in a glass tube. These contacts 
become magnetized by the field from the magnet, attract 
each other, and close the circuit (Fig. 11.3(c)). Because the 
contacts are in a sealed tube, the reed relay is one of the most 
reliable types of mechanical switches. 
Non-mechanical switches 
Non-mechanical switches have been devised to overcome 
some of the problems inherent in mechanical switches. Three 
of the most commonly used non-mechanical switches are the 
THE DEBOUNCED SWITCH 
-5V 
-sv 
Although the mechanical switch has some excellent ergonomic properties, it has 
rather less good electrical properties. In particular, the contacts get dirty and 
make intermittent contact or they tend to bounce when brought together, 
producing a series of pulses rather than a single, clean make. This effect is called 
contact bounce. You can eliminate the effects of contact bounce by connecting 
the switch to the S input of an RS flip-flop. When the switch first closes, the flip-
flop is set and its Q output goes high. Even if the contacts bounce and S goes low, 
the Q output remains high. 
Bounce 
Plunger 
Contacts 
Spring 
(a) The switch. 
Press 
Conductor-coated 
membrane 
Contacts 
Direction 
of motion 
^^e-
(b) The membrane switch. 
Class tube 
(c) The reed relay. 
Contacts 
Figure 11.3 The mechanical switch: (a) basic switch, (b) membrane switch, (c) reed relay. 
Q 
switch 
ON 
I 
OFF 
ON 
OFF 
Q 
~ * R 
fcateVQ 
r — l _ £ L > | 
_ 
GateV 1 
. 
S 
G2 
f 
-f 
L i - - ' 
gnd 
—ON 
OFF 
—ON 
OFF 
' C 
0 
1 CateV 
J
GC> 
GateV 
Md{r:ei 
PCB 

11.1 Simple input devices 
439 
Hall-effect switch, the elastomeric switch, and the capacitive 
switch. The Hall-effect switch consists of a magnet that is 
pushed against the force of a spring towards a Hall cell. The 
Hall cell is a semiconductor device through which a steady 
current flows. When a magnetic field is applied at right angles 
to the current, a voltage is produced across the terminals of 
the cell at right angles to both the magnetic field and the 
current flow. Figure 11.4 illustrates the operation of such a 
switch. By detecting the Hall-effect voltage due to the mag-
netic field you can generate a digital output corresponding to 
the state 'switch-open' or 'switch-closed'. The Hall-effect 
switch does not suffer from contact bounce, but is relatively 
expensive. 
The capacitive switch relies on the change in capacitive 
coupling between two metallic contacts when a finger is 
pressed against them. The great advantage of a capacitive 
switch keyboard is its extremely low cost and small size 
because it is often nothing more than a printed-circuit board 
with contacts etched on the surface. Some capacitive switches 
use a single contact to sense a keystroke and rely on the capac-
itive coupling between the contact and ground via the finger. 
Unfortunately, the capacitive switch keyboard has no tac-
tile feedback and is rather unpleasant to use. Designers can 
get round the lack of tactile feedback by providing audio 
feedback. Each time a keystroke is made, a short audio bleep 
is sounded from a loudspeaker. The capacitive switch is 
found in some very-low-cost PCs, in television touch-
sensitive tuners, and in professional equipment that must 
be hermetically sealed 
for 
operation 
in 
hazardous 
environments. 
Elastomeric switches employ certain types of material that 
change their electrical resistance when subjected to pressure. 
When a finger is pressed against the material, the drop in its 
electrical resistance is detected by a suitable interface. This 
type of switch lacks any tactile feedback and its feel is said to 
be mushy and ill defined. 
The keyboard encoder 
The conversion of a keystroke into its ISO/ASCII-encoded 
equivalent can be performed by a special purpose chip called 
a keyboard encoder. Figure 11.5 illustrates the operation of 
such a chip, which contains all the circuitry necessary to 
convert a signal from an array of switches into a binary 
code together with a strobe (i.e. a pulse that indicates a 
new character). 
Figure 11.5 demonstrates how the keyboard encoder oper-
ates. Eight horizontal lines are connected to the pins of an 
8-bit output port. Similarly, eight vertical lines are connected 
to an 8-bit input port. A switch is located at each of the 
8 X 8 = 64 cross-points between horizontal and vertical 
lines. When a key is pressed, a connection is made between 
the vertical line and the corresponding horizontal line. As 
long as no key is pressed, there is no connection between any 
Zl 
Magnet • 
Magnet 
1—"--4, 
Current / 
Hall voltage 
Figure 11.4 The Hall-effect switch. 
+5V 
64 push-button 
switches 
Figure 11.5 Structure of the keyboard encoder. 
vertical and any horizontal line. If a switch is pressed we can 
determine which key it was by determining its row and its 
column. 
The eight vertical input lines are each terminated in a resis-
tor connected to +5 V, so that these lines are pulled up to a 
high level. That is, if a byte were read from the input port, it 
would be 11111111. Suppose now the output port puts the 
binary value 11111110 onto its eight output lines, as illus-
trated in Fig. 11.6. If the CPU reads from its input port with, 
say, the top right-hand key pressed, it will see 11111110. If the 
next key to the left on the same row is pressed it will see 
11111101. Pressing a key on the topmost row causes a 0 to be 
read into the vertical position corresponding to that key. 
Pressing a key in any other row has no effect on the data read. 
The CPU next outputs the byte 11111101 and reads the 
input lines to interrogate the second row of keys. This process 
is continued cyclically with the CPU outputting 11111110, 
11111101, 11111011, 11110111 . . . t o 01111 111, as the 0 is 
shifted one place left each time. In this way all eight rows of 
the switch matrix are interrogated one by one. The assembly 
language program in Table 11.1 gives an idea of the software 
necessary to operate the keyboard. 
< Pull-up 
/ resistors 
I I I * + + * I 
d 7 d 6 d 5 d 4 d 3 d 2 d^ d 0 
Input port 
Output port > ^ 
u 0 -
d , -
d 2 -
d 3 -
d 4 -
d s . 

Figure 11.6 State of the keyboard encoder with one key pressed. 
11.1.2 Pointing devices 
Although the keyboard is excellent for inputting text, it can't 
be used efficiently as a pointing device to select an arbitrary 
point on the screen. PCs invariably employ one of three 
pointing devices: the joystick, the mouse, and the trackball 
(Fig. 11.7). Portable computers normally use either an eraser 
pointer or a trackpad as a pointing device in order to conserve 
space. 
The joystick is so called because it mimics the pilot's 
joystick. A joystick consists of a vertical rod that can be 
moved simultaneously in a left-right or front-back direction. 
The computer reads the position of the stick and uses it to 
move a cursor on the screen in sympathy. You don't look at 
the joystick when moving it; you look at the cursor on the 
Set X to 11111110 
Set X counter to -1 
REPEAT 
Rotate X left 
Output X 
Read Y 
UNTIL Y * 0 (i.e 
a key is pressed) 
Compress Y value to to 3-bit code 
Set Y counter to -1 
* REPEAT 
* Incremen 
* Compare 
* Shift Y 
* UNTIL Y 
* Concatena 
ORG 
XLINES EQU 
YLINES EQU 
t Y counter 
Y with 11111110 
right 
11111110 
te X and Y to get 
$002000 
$008000 
$008002 
XLOOP 
YLOOP 
MOVE 
MOVE 
ROL. 
ADD. 
AND. 
MOVE 
MOVE 
CMP. 
BEQ 
CLR. 
CMP. 
BEQ 
ROR. 
ADD. 
BRA 
.B #%01111111,DO 
.B #-l,Dl 
B 
#1,D0 
B 
#1,D1 
B 
#%00000111,D1 
B DO,XLINES 
B YLINES,D2 
B 
#%11111111,D2 
XLOOP 
B 
D3 
B 
#%11111110,D2 
CONCAT 
B 
#1,D2 
B 
#1,D3 
YLOOP 
CONCAT LSL.E 
OR.B 
RTS 
#3,D2 
D2,D1 
6-bit value key location 
Subroutine origin 
Output port for horizontal lines 
Input port for vertical lines 
Preset the initial value of X output 
Preset X counter to -1 
Rotate value of X output one place left 
Increment X counter in step 
X counter value is modulo 8 
Send X value to output port 
Read value of Y from input lines 
Has a key been pressed? 
Repeat if Y is all ones 
Preset Y counter to 0 
Test for Y = 0 
Exit to concatenate X and Y values 
Rotate value of Y one place right 
Increment Y counter 
Repeat 
Shift value of Y counter 3 places left 
Add in value of X counter 
Return with key value in Dl 
Table 11.1 Reading data by scanning a keyboard. 
440 
Chapter 11 Computer peripherals 
I T 
t 
t 
T t 
T T * 
+ 5 V 
Output port > > > > > > > 
> resistors 
3QQ 
_ ^ . 0 L S B 
d-| J 
•• T ~ ~ 
, -i 
This button is pushed 
°2 
*• 1 
and the corresponding 
d J 
h -I 
input bit pulled low 
d43 
M 
d 5 l 
M 
d6J 
M 
d7p 
M MSB 
d7 d6 d5 d4 d3 d2 d! d0 
• + » • • 
+ + + 
1 1 1 1 1 1 1 0 
M S B 
Input port sees 
L S B 
 
11111110  

11.1 Simple input devices 
441 
<£- y.f 
P*'V 
(a) The mouse. 
(b) The joystick. 
(c) The Trackball. 
figure 11.7 Pointing devices. 
screen. Without this visual feedback between the hand 
and the eye, people would not be able to use this, or similar, 
pointing devices. Joysticks and mice have one or more 
buttons that can be used to enter commands. The joystick is 
well suited to computer games. 
Although the joystick is similar to the mouse and trackball, 
there is one difference. When the mouse is not being moved, 
there is no signal from them and the computer unambigu-
ously interprets this as no input. However, the joystick 
continually transmits a position, which means that it is very 
difficult to centralize or neutralize its output. Consequently, 
joysticks often have a dead zone around their neutral position. 
Until you move the joystick out of the dead zone, the cursor 
on the screen doesn't move. 
The mouse 
The mouse, invented by Douglas Engelbart in 1964, is the 
PC's most popular pointing device. A mechanical mouse 
consists of a housing that fits comfortably in the hand and a 
ball that rotates in contact with the surface of a desk—you 
could say that a mouse is a larger version of the ball-point 
pen. As the mouse is dragged along the desk, the ball rotates. 
Circuits in the mouse translate the ball's movement into a 
signal that can be read by the computer. 
When the computer receives a signal from the mouse, the 
signal is processed and used to move a cursor on the screen. 
The software that controls the mouse may allow you to 
modify the mouse's sensitivity; that is, 
you can determine how much the cur-
sor moves for a given movement of the 
mouse. 
A modern mouse is comfortable to 
hold and can be used to move the 
cursor rapidly to any point on the 
screen. Once the mouse is at the cor-
rect point, you depress one of two but-
tons that fit naturally under your 
fingers as you move the mouse. Some 
versions of the mouse have one or 
three buttons. Pressing a button 
activates some predefined applica-
tion-dependent 
function 
on 
the 
screen. Typical mouse-based systems 
require you to click the button once to 
select an application (i.e. highlight it) 
and twice to launch an application (i.e. 
run it). Clicking a button twice in this 
way is called double-clicking and is 
not always easy to perform because the 
interval between the clicks must fall 
within a given range. 
Figure 11.8 demonstrates the princi-
ple of an opto-mechanical mouse.4 As the ball rotates due to 
the friction between itself and the desk, its motion is resolved 
into two axes by means of the rollers located at right angles to 
each other. If the mouse is moved upwards or from left to 
right, only one roller rotates. If the mouse is moved diago-
nally both rollers rotate and their relative speed is a function 
of the diagonal angle. 
Each roller is connected to a shaft that rotates an optical 
encoder (i.e. an opaque disc with holes in its surface). The 
rotation of the encoder interrupts a beam of light between an 
LED and a detector. Each pulse is fed to the computer and by 
adding up the number of pulses received, it is possible to 
determine how far the mouse has moved along that axis. In 
practice, two beams of light are necessary because a single 
beam would make it impossible to determine whether the 
mouse was moving in a positive or a negative direction. A sec-
ond detector is required to produce a quadrature signal that 
is out of phase with the main signal. If the ball is rotated one 
way, the quadrature signal leads the main signal and if the 
rotation is reversed the quadrature signal lags the main 
signal. 
An entirely optical mouse doesn't rely on a moving ball. 
First-generation optical mice used a special pad with a fine 
grid of horizontal and vertical lines. This mouse reflects a 
4 It is called opto-mechanical because it has moving parts but uses 
optical techniques to sense the position of a rotating disk. 

442 
Chapter 11 Computer peripherals 
MODERN MICE 
Nothing stands still in the world of the PC.The humble mouse 
has developed in three ways. Its sensing mechanism has 
developed from crude mechanical motion sensors to precise 
optical sensors. Its functionality has increased with the 
addition of two buttons (left click and right click) to permit 
selection followed by a third button and a rotating wheel that 
let you scroll through menus or documents. Some mice let you 
move the wheel left or right to provide extra capacities. Finally, 
the mouse interface has changed from the original PS2 serial 
interface to the USB serial interface to wireless interfaces that 
allow cordless mouse operation. 
The gyro mouse is the most sophisticated of all computer 
mice and is both cordless and deskless. You can use it in space 
simply by waving it about (as you might do when giving a pre-
sentation to an audience). The gyro mouse employs solid-
state gyroscopic sensors to determine the mouse's motion in 
space. Consequently, you are not restricted to dragging it 
across a surface. 
Phototransistor 
(a) Ball and disk arrangement 
The time difference 
between the main and 
quadrature outputs determines 
the direction of rotation. 
P 
J 
o 
Main 
output 
Quadrature 
output 
-•Time 
(b) Detecting the direction of motion 
Figure 11.8 Operation of the optical mouse. 
Surface 
Figure 11.9 The optical mouse. 
light beam off the grid and counts the number of horizontal 
and vertical lines crossed as the mouse moves about the pad. 
An optical mouse does not require intimate contact and 
friction with the surface, although it does require that the 
surface have an optical texture—you can't use a mirror. The 
resolution of an optical mouse is higher then that of a 
mechanical mouse.5 
Second-generation optical mouse technology uses a 
light-emitting diode to illuminate the surface underneath the 
mouse. Light reflected back from the surface is picked up and 
focused on a sensor array (Fig. 11.9). The image from the 
array consists of a series of pixels. As the mouse moves, the 
image changes unless the surface beneath the mouse is blank. 
A special-purpose signal-processing chip in the mouse com-
pares consecutive surface images and uses the difference 
between them to calculate the movement of the mouse in the 
x and/ planes. A new image is generates once every 1/2000 s. 
This type of optical mouse has no moving parts to wear out 
or become clogged with dust. The second-generation optical 
mouse is a miracle of modern engineering because it has an 
image sensing system, an image processing system, and a 
computer interface in a package that sells for a few dollars. 
The joystick 
The joystick is a low-cost pointing device found largely in 
games applications. Its principal advantage over the mouse or 
trackball is that it can provide three motion axes—the con-
ventional left-right and up-down axes, plus a twist or rotate 
axis that you can control by rotating the joystick. 
Figure 11.10 illustrates the operation of the joystick, which 
uses two potentiometers to sense the position of the control 
column. A potentiometer consists of a thin film of a partially 
conducting material and a metallic contact (the slider) that 
can move along the thin film. Mechanical linkages between 
the joystick and potentiometers move the arms of the two slid-
ers. The potentiometers employ a fixed resistance (i.e. the thin 
5 The resolution of a mouse is the minimum distance you need to 
move it for the output to change. The resolution of a typical optical 
mouse is 800 dpi, which means that the mouse generates 800 steps when 
you move it one inch and it can detect a movement of 1/800 inch. 
LED 
Mirror 
Image sensor^ 
Baseplate, 
Dal 
/ 
LED 

11.2 CRT, LED, and plasma displays 
443 
+V supply 
Slider connected to 
joystick by mechanical 
inkages 
-V supply 
X-axis 
output 
Figure 11.10 Operation of the joystick. 
film) that has a constant voltage across its two terminals. If the 
resistance is linear, the voltage at any point along the resistance 
is proportional to its distance from its end. Consequently, the 
slider supplies an analog voltage output that is an approxi-
mately linear function of its position. Two analog-to-digital 
converters are needed to transform the two analog outputs 
into X and Y digital position inputs required by the computer. 
A joystick usually has a dead zone of up to 10% of its full-
scale output about its neutral position. The dead zone is a 
property of a joystick's construction, which means that small 
movements of the joystick about its central neutral position 
generate no change in its X and Y outputs. This effect has the 
advantage that the joystick produces a null output in its 
'hands off' neutral position and is unaffected by small move-
ments. However, a dead zone makes it harder to make precise 
movements when the joystick is near to its neutral position. 
Joysticks can be produced using non-contact technology 
such as optical or even magnetic sensing and it is now possi-
ble to obtain high-precision joysticks. Indeed, some commer-
cial aircraft such as the Airbus 320 have removed the 
conventional control column or yoke and have replaced it by 
a side-mounted joystick that they call a sidestick. 
Another innovation in joystick design is the addition of 
force feedback. In first-generation aircraft, the control column 
directly moved the ailerons and elevators by wire links. As the 
aircraft flew faster, the force needed to move the control 
column grew and it could be quite difficult to maneuver an 
aircraft at high speeds unless you were very strong. When 
hydraulics were used to move control surfaces, the pilot 
didn't need the same effort at high speeds. Unfortunately, 
making large control inputs at high speeds can put an aircraft 
in a maneuver that breaks it up. Hydraulic systems were 
designed that gave the pilot an artificial feel; as the speed 
increases and the forces on the aircraft grow, it becomes 
increasingly harder to move the control column. Replacing 
the control column with a joystick in a fly-by-wire computer-
controlled aircraft removes the natural tactile feedback 
between the aircraft and pilot. The force feedback joystick 
combines a joystick with motors or solenoids that generate a 
computer-controlled force on the joystick as you move it. 
Now, the computer can restore the tactile feedback lacking in 
conventional joysticks. 
The trackball 
A trackball is an upside-down mouse—it remains stationary 
on the desk and you rotate a 1" to 4" ball to move the cursor 
on the screen. Unlike the mouse, the trackball requires no 
desk space and can be fitted on the keyboard of a laptop 
portable. Trackballs are often built into electronic equipment 
that requires an operator to select a point on a screen (e.g. a 
target on a radar screen). 
Other input devices 
Two other input sensors worth mentioning are the eraser tip 
pointer (or pointing stick) and trackpad used by some laptop 
computers. The eraser tip pointer is a tiny joystick that juts 
out of a keyboard. It is operated like a joystick except that 
it doesn't move. It senses pressure and uses a pressure-to-
voltage converter to move the cursor. The track pad consists 
of a small flat region that senses the position of a finger and 
uses the finger as if it were a mouse. Both these devices are not 
as precise as the mouse and are used simply because they 
require no desk space. 
Three other pointing devices are the touch screen, the 
light-pen, and the tablet. The touch screen, found on all 
PDAs, uses a display coated with transparent conductors to 
detect the location of a finger on the surface of the screen. 
Some touch screens sense the finger's position by ultrasonic 
or infra-red beams. The touch screen can be used as an input 
device simply by touching the point you want to activate. 
A typical system displays a menu of commands. Touch-
sensitive screens are still relatively expensive and are found 
only in specialized applications. The finger is a rather course 
pointer and cannot be used as precisely as a mouse or joystick 
(PDAs require the use of a pencil-like stylus). Touch screens 
are useful when the operator has no computer experience 
whatsoever (e.g. a user-controlled guide in a shopping mall) 
or when the system must be hermetically sealed (e.g. in 
potentially explosive atmospheres). 
The light-pen uses a stylus with a light-sensitive device in 
its tip. When placed against the computer's screen, the light-
pen sends a signal to the computer when the beam passes 
under it. The light-pen is just a much more precise form of 
the touch screen and is cheaper to implement. Sophisticated 
algorithms can be used to convert the light-pen's movement 
over the screen (i.e. handwriting) into an ASCII-encoded text 
format for internal storage and manipulation. Unfortunately, 
it's not easy to convert the output from a light-pen into text, 
because the way in which one person writes, say, a letter'a', is 
often different from the way another person writes it. 
X motion 
Y motion 
t 
t < 
i i 
V-axis 
output 

444 
Chapter 11 Computer peripherals 
Heater 
Cathode 
Glass envelope 
Phosphor 
coating 
Figure 11.11 Structure of the CRT. 
The document scanner was once a very expensive device 
and has now become a very-low-cost but high-precision 
input device. A document is placed on a glass sheet that forms 
the top of a largely empty box. A light source on tracks is 
moved along the length of the document and photo sensors 
read light intensity along a scan line. The scanner is able to 
create a high-resolution color image of the document being 
scanned (typically 2400 dpi). Indeed, the accuracy of some 
document scanners is sufficient to give national mints a 
headache because of the potential for forgery. Some bank-
notes are designed with color schemes that are difficult to 
copy precisely by document copiers. 
11.2 CRT, LED, and plasma displays 
General-purpose computers communicate with people via a 
screen, which may be a conventional CRT or a liquid crystal 
display. We begin by describing the cathode ray tube (CRT) 
display that lies at the heart of many display systems. A CRT 
is little more than a special type of the vacuum tube that was 
once used in all radios and TVs before they were replaced by 
transistors. 
The way in which human visual perception operates is 
important to those designing displays; for example, we can 
see some colors better than others, we cannot read text if it is 
too small nor can we read it rapidly if it is too large. Colors 
themselves are described in terms of three parameters: hue is 
determined by the wavelength of the light, saturation is deter-
mined by the amount of white light present in the color, and 
intensity is determined by the brightness of the light. Objects 
on a screen are viewed against background objects—the 
luminosity of an object in comparison with its background is 
called its contrast. All these factors have to be taken into 
account when designing an effective display. 
Conductive coating 
Figure 11.11 describes the construction of the cathode ray 
tube (CRT).6 It is a remarkably simple device that uses a tech-
nology discovered early in the twentieth century. The CRT is 
a glass tube from which all the air has been removed. A wire 
coil, called a heater, is located at one end of the CRT and 
becomes red-hot when a sufficiently large current flows 
through it—exactly like the element in an electric fire. The 
heater raises the temperature of a cylinder, called the cathode, 
which is coated with a substance that gives off electrons when 
it is hot. The negatively charged electrons leaving the surface 
of the cathode are launched into space unimpeded by air 
molecules because of the high vacuum in the CRT. 
When the negatively charged electrons from the CRT's 
cathode boil off into space, they don't get very far because 
they are pulled back to the positively charged cathode. To 
overcome the effect of the positive charge on the cathode, the 
surface and sides of the glass envelope at the front of the CRT 
are coated with a conducting material connected to a very 
high positive voltage with respect to the cathode. The high 
positive voltage (over 20 000 V attracts electrons from the 
cathode to the screen. As the electrons travel along the length 
of the CRT, they accelerate and gain kinetic energy. When 
they hit phosphors coating the front of the screen, their energy 
is dissipated as light. The color and intensity of the light 
depend on chemical characteristics of the phosphor coating 
and the speed and quantity of the electrons. For the time 
being, we will assume that the composition of the phosphor 
gives out a white light; that is, the display is black and white or 
monochrome. 
The beam of electrons from the cathode flows through a 
series of cylinders and wire meshes located near the cathode. 
Using the principle that like charges repel and unlike charges 
6 The cathode ray tube was originally invented in Germany by Karl 
Braun in 1910 and later developed by Vladimir Zworykin in 1928. 
Deflection coils \ 
\ Electron 
beam 
\ 
Focus 
electrodes 
\ 
Grid 

11.2 CRT, LED, and plasma displays 
445 
attract, various electrical potentials are applied to these 
cylinders and meshes to control the flow of the beam from the 
cathode to the screen and to focus the electrons to a tight 
spot—the smaller the spot, the better the resolution of the dis-
play. The cathode and focusing electrodes are called a gun. 
A wire mesh called a control grid is placed in the path of the 
electron beam and connected to a negative voltage with 
respect to the cathode. The stronger the negative voltage on 
the grid, the more the electrons from the cathode are 
repelled, and the fewer get through to the screen. By changing 
or modulating the voltage on the grid, the number of elec-
trons hitting the screen and, therefore, the brightness of the 
spot, can be controlled. 
11.2.1 Raster-scan displays 
Two scanning coils at right angles (called a yoke) are placed 
around the neck of the CRT. Passing a current through one 
coil creates a magnetic field that deflects the beam along the 
horizontal axis and passing a current through the other coil 
causes a deflection along the vertical axis. These coils let you 
deflect the beam up-down and left-right to strike any point 
on the screen. 
The magnetic field in the coil that deflects the beam in the 
horizontal axis is increased linearly to force the spot to trace 
out a horizontal line across the face of the CRT. This line is 
called a scan line or a raster. When the beam reaches the right-
hand side, it is rapidly moved to the left-hand edge, ready for 
the next horizontal scan. 
While the beam is being scanned in the horizontal direc-
tion, another linearly increasing current is applied to the 
vertical deflection coils to move the beam downward. The rate 
at which the beam moves vertically is a fraction of 
the rate at which it moves horizontally. During the time it 
takes the beam to scan from top to bottom, it makes hundreds 
of scans in the horizontal plane. A scan in the vertical axis is 
called a frame. Figure 11.12(a) shows the combined effects of 
the fast horizontal and slow vertical scans—eventually, the 
beam covers or scans the entire surface of the screen. 
As the beam scans the surface of the screen, the voltage on 
the grid is varied to change the brightness of the spot to draw 
an image. Figure 11.12(b) demonstrates how the letter 'A' can 
be constructed by switching the beam on and off as it scans 
the screen. The scanning process is carried so rapidly that the 
human viewer cannot see the moving spot and perceives a 
continuous image. Typically, the horizontal scan rate is in the 
region of 31000 lines/s and the vertical scan rate is 50 to 
100 fields/s. We will return to the scanning process later when 
we describe the system used to store images. 
The simplest CRT screen would be a hemisphere, because 
any point on its surface is a constant distance from the focus-
ing mechanism. Such a screen is unacceptable, and, over the 
years, the CRT screens have become both flatter and squarer 
at the cost of ever more sophisticated focusing mechanisms. 
The CRT's screen is not square; its width:height or aspect 
ratio is the same as a television, 4:3. 
The CRT is an analog device employing electrostatic and 
electromagnetic fields to focus an electron beam to a point on 
a screen. The engineering problems increase rapidly with the 
size of the screen and large CRTs are difficult to construct and 
expensive. The weight of the CRT also increases dramatically 
with screen size. In the early 1990s the cost of a 17-inch screen 
was about four times that of a 14-inch screen and a 19-inch 
screen cost over 10 times as much as a 14-inch screen. The 
CRT was one of the last components of the computer to expe-
rience falling prices. However, by the late 1990s the price of 
CRTs had dramatically fallen; not least because of competi-
tion with LCD displays. By 2003, the LCD was beginning to 
replace the CRT in domestic TV displays. 
11.2.2 Generating a display 
The next step is to explain how an image is generated. 
Figure 11.13 provides a more detailed description of the raster-
scan display based on the CRT. A sawtooth waveform is applied 
to the vertical scan coils of a CRT to cause the spot to move from 
the top of the screen to the bottom of the screen at a constant 
linear rate. When the spot reaches the bottom of the screen, it 
rapidly flies back to the top again. At the same time, a second 
sawtooth waveform is applied to the horizontal scanning coils 
to cause the beam to scan from the left-hand side to the right-
hand side before flying back again. A negative pulse is applied to 
the grid during the flyback period to turn off the beam. 
As the beam is scanned across the surface of the screen, 
passing every point, the voltage on the grid can be changed to 
modify the spot's brightness. Although at any instant a TV 
screen consists of a single spot, the viewer perceives a com-
plete image for two reasons. First, the phosphor coating con-
tinues to give out light for a short time after the beam has 
struck it, and, second, a phenomenon called the persistence of 
vision causes the brain to perceive an image for a short time 
after it has been removed. 
Raster scan 
(a) The raster scan display. 
(b) Modulating the beam 
to create an image. 
Figure 11.12 The raster scan. 
One frame 

446 
Chapter 11 Computer peripherals 
A raster-scan display system can be constructed by 
mapping the screen onto memory. As the beam scans the 
physical display screen, the corresponding location in the 
computer memory is interrogated and the resulting value 
used to determine the brightness of the spot. Figure 11.14 
provides a highly simplified arrangement of a system that 
generates an n column by m row display. 
In Fig. 11.14a clock called a dot clock produces pulses at the 
dot rate (i.e. it generates a pulse for each dot or pixel on the 
Raster track 
L 
Brightness 
Vertical scan 
Horizonta 
circuit 
scan circuit 
Figure 11.13 Details of the raster-scan display. 
display). The dot clock is fed into a divide-by-H circuit that pro-
duces a single pulse every time n dots along a row are counted. 
It also produces a dot number in the range 0, 1, 2,... n — 1. 
The output of the divide-by-n circuit is a pulse at the row (i.e. 
raster) rate, which is fed to a divide-by-m circuit. 
The output of the divide-by-m circuit is a pulse at the 
frame rate (i.e. a pulse for each complete scan of the screen). 
This pulse is fed to the CRT's control circuits and is used to 
lock or synchronize the scanning circuits in the CRT unit with 
the dot clock. The divide-by- m circuit 
produces an output in the range 0, 1, 
2, m — 1 corresponding to the current 
row. The column and row address 
combiner takes the current column 
and row addresses from the two 
dividers and generates the address of 
the corresponding pixel in the video 
memory. The pixel at this address is 
fed to the CRT to either turn the beam 
on (a white dot), or to turn the beam 
off (no dot). 
A real display system differs from 
that of Fig. 11.14 in several ways. 
Probably the most important compo-
nent of the display generator is the 
One frami 
Dot 
clock 
Dot rate 
Divide by n 
Dot number 
(column address! 
Line rate 
Divide by m 
Field rate •Sync to CRT 
jr 
IT 
Row and column 
address combiner 
Line number 
(row address) 
Screen 
Address 
Video memory 
Data 
R 
-••Video to CRT 
n columns 
Figure 11.14 The display controller. 
TV DISPLAYS 
Because the computer display grew out of the television, let's 
look at some of the details of a TV display. In the USA a TV 
image uses 60 vertical scans a second and each vertical scan 
(called a field) is composed of 26272 lines. A frame is made up 
of two consecutive fields containing 262 V2 odd-numbered 
lines and 262 V2 even-numbered lines. The total number of 
lines per frame is 2 X 262 V2 = 525. In Europe there are 50 
vertical scans a second and each vertical scan is composed 
of 3121/2 lines. The total number of lines per frame is 
2 X 31272 = 625. 
A display composed of consecutive fields of odd and even 
lines is called an interlaced display and is used to reduce 
the rate at which lines have to be transmitted. However, 
interlaced displays are effective only with broadcast TV and 
generate unacceptable flicker when used to display text in a 
computing environment. 
Deflection 
coils/ 
Screen 
/ 
^ 
/ 
v 
Grid 
CRT 
rri rows 

11.2 CRT, LED, and plasma displays 
447 
Address of 
next group 
of pixels 
Display generator 
address 
Dot dock 
Address 
CPU 
Data 
Video address 
Shift dock 
Shift register 
Video memory 
Video DAC 
Video to CRT 
Figure 11.15 The video memory. 
video memory (sometimes called VRAM), which holds the 
image to be displayed. Figure 11.15 shows the structure of a 
dual-ported video memory. We call this memory dual ported 
because it can be accessed by both an external CPU and the 
display generator simultaneously. The CPU needs to access 
the video memory in order to generate and modify the image 
being displayed. 
One of the problems of video display design is the high rate 
at which its pixels are accessed. Consider a SuperVGA display 
with a resolution of 1024 X 768 pixels and a refresh rate 
(frame rate) of 70 Hz. In one second, the system must access 
1024 X 768 X 70 = 55 050 240 pixels. The time available to 
access a single pixel is approximately 1/55 000 000 s = 18 ns, 
which is too short for typical video memory. In practice even 
less time is available to access pixels, because some time is lost 
to left- and right-hand margins and the flyback. 
A practical video display system reads a group of pixels 
from video memory at a time and then sends them to the 
CRT one at a time. Figure 11.15 shows how the video mem-
ory performs this operation. The address from the display 
generator selects a row of pixels that are loaded into a shift 
register once per row clock. This arrangement means that the 
video memory is accessed by the display generator only once 
per row, rather than once per pixel. Consequently, the mem-
ory doesn't require such a low access time. The individual 
pixels of a row are read out of the shift register at the dot (i.e. 
pixel) rate. A shift register is capable of much higher speed 
operation than a memory. 
Modern display systems permit more sophisticated 
images than the simple on/off dot displays of a few years ago. 
Several video memories (called planes) are operated in 
parallel, with each memory plane contributing one bit of 
the current pixel. If there are q memory planes, the q bits 
can be fed to a q-bit digital-to-analog converter to generate 
one of 2q levels of brightness (i.e. a gray scale), or they can be 
used to select one of 2q different colors (we discuss color later). 
PC display systems 
You can integrate a PC's display electronics onto the mother-
board or you can locate the display subsystem on a plug-in 
J 
Video memory 
II 
II 
II 
1 
Dedicated 
video 
processor 
BIOS 
Dedicated 
video 
processor 
—1 
1 
II 
1 
Figure 11.16 The video display card. 
board. High-performance computers use a plug-in display 
card because a card can be optimized for video applications. 
Figure 11.16 illustrates the organization of a typical display 
card. Because it's necessary to transfer large volumes of data 
between the CPU and the video display, PCs have a special 
interface slot that provides a very-high-speed data bus 
between the display card and the CPU; this is called the 
AGP bus. 
It is impossible to cover display cards in any depth in this 
book. They are very complex devices that contain their own 
massively powerful special-purpose processors with 128-bit-
wide internal buses. These processors free the host processor 
on the motherboard from display processing. 
The video display memory forms part of the display card 
and is not normally part of the processor's memory space. 
This means that video and processor memory can be accessed 
in parallel. Some low-cost computers integrate the video con-
troller on the motherboard and use system memory for dis-
play memory. 
The original PC display was 640 X 480 pixels. Over the 
years, the average size of mainstream PC displays has 
increased. By the 1990s most PCs had 1024 X 768 displays 
and Web applications frequently used 800 X 600 display for-
mats. Today, displays with resolutions of 1280 X 1024 are 
common and some LCD displays have a resolution of 
1600 X 1200. 
In practical terms, a 640 X 480 display can present a page 
of rather chunky text. A SXGA display with 1600 X 1200 pix-
els can display two word-processed pages side by side. 
Figure 11.17 illustrates the growth in the size (resolution) of 
displays for the PC. 
11.2.3 Liquid crystal and plasma 
displays 
For a long time, the CRT remained the most popular display 
mechanism. The late 1980s witnessed the rapid growth of a 
rival to the CRT, the liquid crystal display or LCD. By the 
mid-1990s color LCD displays were widely found in laptop 

448 
Chapter 11 Computer peripherals 
POLARIZING MATERIAL 
The polarizing materials we use today were first produced by 
Edwin Land in 1932. Dr Land embedded crystals of idoquinine 
sulfate in a transparent film. These crystals are all oriented in 
the same direction and form a grating that allows only light 
polarized in a specific direction to pass though the film. If you 
place two crossed polarizing films in series with one rotated at 
90° with respect to the other, no light passes through the pair, 
because one filter stops the passage of vertically polarized light 
and the other stops the passage of horizontally polarized light. 
Dr Land marketed his filters under the trade name Polaroid 
and later went on to develop the instant picture camera that 
took pictures and developed the film while in the camera. 
VGA 640 x 480 
SVGA 800 x 600 
XGA 1024x768 
SXGA 1280x1024 
UXVGA 1600x1200 
Figure 11.17 Video display formats. 
Alignment at top 
The liquid crystal is sandwiched 
between two surfaces with parallel 
ridges. The crystals align with these 
ridges 
Because the ridges are at right angles, 
the liquid crystal molecules gradually 
rotate through 90° between top 
and bottom plates 
Alignment at bouoin 
Figure 11.18 The LCD cell with no applied field. 
portables. First-generation LCDs had a resolution of 
640 X 480 pixels and they cost much more than CRTs. The 
800 X 600 pixel displays were rapidly replaced by the 14-inch 
1024 X 768 pixel display. By 2004 top-of-the-range LCDs 
had 21-inch screens with resolutions of 1600 X 1200 pixels. 
In order to understand how LCDs operate, we need to 
introduce polarization. Light is a form of electromagnetic 
radiation that shares some of the properties of waves; 
for example, the vibration of a light wave is at right angles 
to the direction of its propagation. Light from an incandes-
cent bulb is composed of large numbers of individual light 
waves, all arranged with their axes of vibration at random. 
Such light is said to be unpolarized because the vibrations do 
not favor a specific axis. If the vibrations are all along the 
same axis, the light is polarized. When light passes through a 
polarizing filter, only light whose axis of vibration has a cer-
tain angle is transmitted through the filter. The polarization 
of light and the polarizing filter can be used as the basis of 
a display system—all you need is a material that can be 
polarized on demand. 
Unlike the molecules in a solid crystal, which are arranged 
in a fixed pattern, the molecules of a liquid are arranged 
at random. Liquid crystals are liquid but their molecules have a 
preferred alignment; that is, they are liquids that share some of 
the properties of solids. The intermolecular forces between 
molecules in a liquid crystal are not the same in all directions. 
The molecules are arranged at random in 
one plane but are organized or structured 
in another plane. You can imagine a liquid 
crystal as sheets or layers of molecules 
where the orientation of molecules within 
individual layers is random. 
Figure 11.18 illustrates a liquid crystal 
cell where the liquid crystal film is placed 
between two plates. Each plate has a series 
of ridges scored on it that cause the long, 
rod-like molecules of the liquid crystal to 
align with the ridges. The two plates in 
Fig. 11.18 are crossed so that the molecules 
gradually rotate through 90° between the 
top and bottom surfaces. The polariza-
tion of light passing though such a struc-
ture is rotated by 90°. 
A particular group of substances 
known as nematic liquid crystals are affected by an electric 
field. When an electric field is applied to a nematic liquid 
crystal, the molecules twist or rotate under the influence of 
the field. Figure 11.19 illustrates the same arrangement in 
which an electric field has been applied across the plates. This 
field overcomes the preferred alignment of the molecules. 
The polarization of light passing though such a twisted liquid 
crystal is either unmodified or rotated through 90°, depending 
on whether a field is applied across the liquid crystal. 

11.2 CRT, LED, and plasma displays 
449 
The liquid crystal display mimics the behavior of the CRT; 
that is, it creates a pixel that can be switched on or off. All we 
have to do is to make a sandwich of a polarizing substance 
and a liquid crystal—light will pass through it if the liquid 
crystal is polarized in the same plane as the polarizing mater-
ial. Otherwise, the two polarizing filters block the transmis-
sion of light. 
We now have all the ingredients of a flat panel display: a 
polarizing filter that transmits light polarized in one plane 
only and a liquid crystal cell that can rotate the polarization 
of light by 90° (or more) electronically. Figure 11.20 demon-
strates the structure and operation of a single-pixel LCD cell. 
In Fig. 11.20 light is passed first through a polarizer 
arranged with its axis of polarization at 0°, then through the 
liquid crystal, and finally through a second polarizer at 90°. If 
the liquid crystal does not rotate the plane of polarization of 
the light, all the light is blocked by the second polarizer. If, 
however, an electrostatic field is applied to the two electrodes, 
the liquid crystal rotates the polarization of the light by 90° and 
the light passes through the second polarizer. Consequently, a 
light placed behind the cell will be visible or invisible. 
4.'gnment of ridges at top 
An entire LCD display is made by creating rows and 
columns of cells like those of Fig. 11.20. Each cell is selected 
or addressed by applying a voltage to the row and the column 
in which it occurs. The voltage is connected to each cell by 
depositing transparent conductors on the surface of the glass 
sandwich that holds the liquid crystals. 
Because an LCD cell can be only on or off, it's impossible to 
achieve directly different levels of light transmission (i.e. you 
can display only black or white). However, because you can 
rapidly switch a cell on and off, you can generate intermedi-
ate light levels by modulating the time for which a cell trans-
mits light. Typical LCDs can achieve 64 gray levels. 
LCD displays can be operated in one of two modes. The 
reflective mode relies on the ambient light that falls on the 
cell, whereas the transmissive mode uses light that passes 
through the cell. Ambient light displays have a very low con-
trast ratio (i.e. there's not a lot of difference between the on 
state and the off state) and are often difficult to read in poor 
light, but they do consume very little power indeed. 
Reflective LCD displays are found in pocket calculators, 
low-cost toys, and some personal organizers. Displays operat-
ing in a transmissive mode using back-lighting are easier to 
read, but require a light source (often a 
fluorescent tube) that consumes a con-
siderable amount of power. Ultimately, 
it's the power taken by this fluorescent 
tube that limits the life of a laptop com-
puter's battery. 
When an electric field is applied 
between the plates, the liquid crystal 
molecules change their alignment and 
line up in the direction of the field 
?nment of ridges at bottom 
Figure 11.19 The LCD cell with an applied electric field. 
Plasma displays 
Plasma technology was invented in the 
1960s. The plasma display uses a flat 
panel that shares many similarities with 
the LCD display. Both displays consist 
of an array of pixels that are addressed 
by x, y coordinates. The LCD cell uses a 
liquid crystal light-gate to switch a beam 
Cell 
r * 
Light 
Electrode 
Polarizing filter 0° 
Nematic liquid crystal 
\ 
Polarizing fitter 90° 
V = 0 
(a) The liquid crystal cell has no effect on the 
light passing through it. The polarizing filters 
stop light passing through them. 
Light 
iffy y t f ^ V VT 
r^ 
Polarizing filter 0° 
Nematic liquid crystal 
Polarizing fi ter 90° 
t t t t f t y 
I 
V = Polarize 
(b) The liquid crystal cell rotates the polarization 
of the light by 90°. This allows the light to pass 
through the lower polarizing filter. 
Figure 11.20 Displaying a 
pixel. 

450 
Chapter 11 Computer peripherals 
of light on or off, whereas the plasma panel uses a tiny flores-
cent light to generate a pixel. 
Each pixel in a plasma display is a tiny neon light. A eel] 
(i.e. pixel) contains a gas at a low pressure. Suppose a voltage 
can be applied to a pair of electrodes across the cell. At low volt-
ages, nothing happens. As the voltage increases, electrons are 
pulled off the atoms of the gas in the cell by the electric field. As 
the voltage increases further, these electrons collide with other 
atoms and liberate yet more electrons. At this point an 
avalanche effect occurs and a current passes through the cell. 
The passage of a current through the gas in a cell generates 
light or UV radiation. First-generation plasma display panels 
used cells containing neon, which glows orange-red when 
energized. By about 2000 color plasma panels were beginning 
to appear; these use cells with phosphors, which glow red, 
Electrode 
Discharge 
Figure 11.21 The plasma display cell 
Contrast 
ratio 
Dark 
_„„ 
room 
500 
400 
Living 
room 
Office 
300 
200 
100 
green or blue when bombarded by UV light. By 2003 plasma 
panel production was rapidly increasing and plasma panels 
offered a realistic alternative to large CRT displays. 
Plasma technology offers significant advantages over LCD 
technology; in particular, plasma displays have a higher con-
trast ratio (i.e. the ratio of black to white) and plasma panels 
are brighter because they operate by generating light rather 
than by gating it through filters and a liquid crystal. 
Figure 11.21 illustrates the structure of a plasma cell. The 
display is constructed from two sheets of glass separated by a 
few hundred microns. Ribs are molded on one of the plates to 
provide the cell structure. The phosphors that define the 
colors are deposited on the surface of the glass sheet. When 
the unit is assembled it is filled with xenon at low pressure. 
A display is initiated by applying a voltage across two elec-
trodes to break down the gas and start the discharge. Once the 
cell has been activated, a lower voltage is 
applied to the keep-alive electrode to main-
tain the discharge. 
Although plasma panels have advantages 
over LCD displays, they also have disadvan-
tages. They consume more power than LCDs, 
which can cause problems with unwanted 
heat. The phosphors gradually degrade and 
the display slowly loses its brightness over a 
few years. Finally, the individual cells sufferer 
a memory effect. If a cell is continuously ener-
gized for a long time, it can lose some inten-
sity. This means that a static picture can be 
burnt onto the screen; you see this effect with 
plasma displays in airports where the 
ghostly details of some flights are perma-
nently burnt onto the screen. 
The contrast ratio of a display is the 
ratio of the lightest and darkest parts of 
the display. Although many regard the 
plasma panel as having a better contrast 
ratio than an LCD, the situation is more 
complicated. Contrast ratio is affected 
not only by the transmissivity of an LCD 
or the on/off light ratio of a plasma cell, 
but also by ambient light. Figure 11.22 
gives the contrast ratios of LCDs and 
plasma displays as a function of ambient 
light. This demonstrates that the plasma 
display is far superior in a dark room but 
is less good in bright light. 
Data electrode 
Outdoors 
10 
100 
1000 
Ambient illumination (cc/mm2) 
10 000 
Figure 11.22 Contrast LDC and plasma displays. 
11.2.4 Drawing lines 
Having come so far in describing display 
systems, we'd just like to demonstrate 
how dots are transformed into lines by 
Plasma \ 
display 
\* 
.LCD 
Phosphor 
J 
I 
• 
. < _ — _ _ _ _ _ _ _ _ 
Back plate 
Face plate 

11.2 CRT, LED, and plasma displays 
451 
ORGANIC DISPLAYS 
Conventional electronics uses semiconductors fabricated from 
silicon or compounds such as gallium arsenide. Organic 
semiconducting materials have been discovered that have 
many interesting properties. Unfortunately, organic 
semiconducting materials are often difficult to work with. 
They suffer from low electron and hole mobility and are 
limited to low-speed applications. From a manufacturing 
standpoint, they are sensitive to high temperatures, cannot be 
soldered into conventional circuits, and degrade when 
exposed to atmospheric moisture and oxygen. 
On the other hand, organic electronics have remarkable 
commercial potential. They offer the promise of flexible, 
paper-thin circuits and displays because they can be deposited 
on flexible plastic foils rather than the heavy and fragile glass 
surfaces required by LCDs. OLEDs (organic light-emitting 
diodes) have far lower power consumption than LCDs. 
The OLED pioneered by Kodak in the mid-1980s, is already 
in large-scale production. The first applications of this 
technology were in automobile instrument panels, digital 
watches, and small, low-power displays in mobile phones. 
The OLED uses organic materials that have weak intermole-
cular bonds that give them the properties of both semicon-
ductors and insulators. The organic molecules of an OLED are 
sandwiched between conductors (anode and cathode). When 
a current flows through the molecules of the OLED, electron 
and hole charge carriers recombine to give off light in a 
process called fluorescence. 
OLEDs can be constructed in the form of TOLEDs 
(transparent OLEDs) for use in automobile or aircraft wind-
shields to provide 'head-up' displays, or in the form of 
FOLEDs (flexible OLEDs), which can be bent or rolled into 
any shape. 
By vertically stacking TOLEDs in layers with each 
layer having a different color, you can create the SOLED 
(stacked OLED), which forms the basis of a multicolor 
display. 
Figure 11.23 Drawing a line. 
software. There are two types of image: the bit-mapped image 
and the parameterized image. All images in the video memory 
are bit mapped in the sense that each pixel in the display cor-
responds to a pixel in the video memory (strictly speaking, 
perhaps we should use the term pixel mapped). Photographs 
and TV pictures are examples of bit-mapped images. 
A parameterized image is defined in terms of an algorithm; 
for example, you might describe a line as running from point 
(—4,12) to point (30,45), or as the equationy = 4x + 25 for 
—9 < x < 70. The graphics software is responsible for taking 
the parameterized image and converting it into a bit-mapped 
image in the display's memory. We now look at the parame-
terized image because it can be specified with relatively few 
bits and it can easily be manipulated. 
Figure 11.23 demonstrates how a line is mapped onto a 
display by evaluating the equation of the line and then select-
= 0.5* 
Q A pixel above the line 
^ ) A pixel on the line 
(~\h pixel below the line 
Figure 11.24 The equation of a straight line. 
ing every pixel that passes through the line. In a practical sys-
tem, relatively few pixels will lie exactly on the line and it is 
necessary to select pixels close to the line. 
Jack Bresenham invented a classic line-drawing algorithm 
in 1965. A straight line can be expressed as ay = bx + c, 
where x and y are variables, and «, b, and c constants that 
define the line. The line's slope is given by b/a and the point at 
which the line goes through the x origin is given by y = c/a. 
For the sake of simplicity, consider a line that goes through 
the origin, so that ay = bx, where a = 1 and b = +0.5. 
Figure 11.24 illustrates how the line corresponding to this 
equation goes through some pixels (shown black in 
Fig. 11.24). All other pixels are either above or below the line. 
If the equation of the line is rearranged in the form 
ay — bx = 0, the pixels above the line (light in Fig. 11.24) sat-
isfy the relation ay — bx > 0, and those below the line (dark in 
Fig. 11.24) satisfy the relation ay— bx < 0. 

452 
Chapter 11 Computer peripherals 
The Bresenham algorithm draws lines with a slope 
m = b/a in the range 0 to 1. This algorithm evaluates the sign 
of ay— bx at regular intervals. By monitoring the sign of the 
function, you can determine whether you are above or below 
the line. The line is drawn from its starting point from, say, 
left to right. Suppose we select a pixel somewhere along this 
line. Bresenham's algorithm tells us how to go about selecting 
the next pixel. Figure 11.25 demonstrates that the new pixel is 
selected to be either the pixel directly to the right of the 
current pixel or the pixel both above and to the right of 
the current pixel. 
The algoridim evaluates the value of the function at the mid-
point between the two candidates for the new pixel along the 
line. The pixel selected to be the next pixel is the one that lies 
closest to die line, as Fig. 11.25 demonstrates. The details of me 
Bresenham algorithm are more complex dian we've described. 
The algorithm must handle lines that don't pass through the 
origin and lines that don't have slope in die range 0 to 1. 
The following fragment of pseudocode implements the 
Bresenham line-drawing algorithm. Assume that a straight 
line with a slope between 0 and 1 is to be drawn from x,, y, to 
x2,x2. At each step in drawing the line we increment the value 
of the x coordinate by x_step. The corresponding change in 
y is x_step * {yl — y\ )/(x2 — xl). 
The Bresenham algorithm eliminates this calculation by 
either making or not making a fixed step along they axis. 
DrawLine(xl yl,x2, y2) 
int x, y, e dx, dy; 
( 
dx = x2 - xl; 
dy = y2 - yi; 
e = = 2*dy -- dx; 
y = = yl; 
for (x = xl; xl d"x 
( 
plot(x y); 
while 
i 
(e e"0) 
e = 
y++ 
e - 2* dx; 
e = e + 2*dy, 
Current pixel 
(a) Select next pixel up and right. 
Figure 11.2S Searching along a line. 
Current pixel 
(b) Select next pixel 
If the line's slope is greater than 1, we can use the same 
algorithm by simply swapping x and y. 
f~~\ 
Antialiasing 
^ r^ 
The Bresenham and similar algorithms 
generate lines that suffer from a step-like 
appearance due to the nature of the line-
following mechanism. These steps are 
often termed jaggies and spoil the line's 
*f~^\ 
appearance. The effect of finite pixel 
size and jaggies is termed aliasing, a 
term taken from the world of signal 
processing to indicate the error intro-
duced when analog signals are sampled 
at too low a rate to preserve fidelity. 
Figure 11.26 demonstrates how we can 
reduce the effects of aliasing. 
The antialiasing technique in Fig. 11.26 uses pixels 
with gray-scale values to create a line that appears less jagged 
to the human eye. A pixel that is on the line is made fully black. 
Pixels that are partially on the line are displayed as less than 
100% black. When the human eye sees the line from a dis-
tance, the eye-brain combination perceives a line free of jag-
gies. That is, the brain averages or smoothes the image. 
Now that we've looked at how images are created on a 
screen, we examine how they are printed on paper. 
11.3 The printer 
Figure 11.26 Improving a line by antialiasing. 
The printer produces a hard-copy output from a computer by 
converting digital information into marks on paper. Because 
printers rely on precisely machined moving mechanical 
1 Midpoint 
Next pixel 
L i n e ^ 
i Midpoint 
Next pixel 
i 
I ine 
el right. 
x 2 ; 
x++) 

11.3 The printer 
453 
parts, they are less reliable than the purely electronic devices 
such as CRT and LCD displays. 
Like all the other subsystems of a PC, the printer has seen a 
remarkable drop in its price over the last three decades. In 
1976 a slow, unreliable dot matrix printer cost about $800 
and by 2005 you could buy a laser printer for $150 and a 
photo-quality inkjet printer for about $60. 
Printers come in a wider range than other display devices 
because there are more printer technologies. Mechanical 
printers must perform the following basic functions. 
1. Move the paper to a given line. 
2. Move the print-head to a given point along a line. 
3. Select a character or symbol to be printed. 
4. Make a mark on the paper corresponding to that 
character. 
The first and last of these functions are relatively easy 
to explain and are dealt with first. Depending on the 
application, paper is available in single sheet, continuous roll, 
or fan-fold form. Paper can be moved by friction feed, in 
which the paper is trapped between a motor-driven roller 
and pressure rollers that apply pressure to the surface of the 
paper. As the roller (or platen) moves, the paper is dragged 
along with it. An alternative paper feeding mechanism is the 
tractor or sprocket feed where a ring of conical pins round 
the ends of the platen engage in perforations along the 
paper's edge. As the platen rotates, the paper is accurately and 
precisely pulled through the printer. The rise of the PC has 
seen the decline of fan-fold paper. Today's printers found in 
the home or small office use plain paper. Some banks and 
similar institutions still employ fan-fold paper to print state-
ments (and other pre-printed forms). 
11.3.1 Printing a character 
Printers are described by the way in which marks on the 
paper are made; for example, the matrix printer, the inkjet 
printer, and the laser printer. The earliest method of marking 
paper used the impact of a hard object against an ink-coated 
ribbon, to make an imprint in the shape of the object. This is 
how the mechanical office typewriter operates. The tremen-
dous reduction in the cost of laser and inkjet printers in the 
early 1990s rendered impact printers obsolete, except in 
specialized applications. 
Non-impact printers form characters without physically 
striking the paper. The thermal printer employs special paper 
coated with a material that turns black or blue when heated to 
about 110°C. A character is formed by heating a combination 
of dots within a matrix of, typically, 7 by 5 points. Thermal 
printers are very cheap, virtually silent in operation, and are 
used in applications such as printing receipts in mobile ticket 
dispensers. A similar printing mechanism uses black paper 
coated with a thin film of shiny aluminum. When a needle 
electrode is applied to the surface and a large current passed 
through it, the aluminum film is locally vaporized to reveal 
the dark coating underneath. 
Another method of printing involves spraying a fine jet of 
ink at the paper. As this technique also includes the way in 
which the character is selected and formed, it will be dealt 
with in detail later. 
The hardware that actually prints the character is called the 
print head. There are two classes of print head: the single print 
head and the multiple print head found in line printers. 
Typewriters employ a fixed print head and the paper and 
platen move as each new character is printed. A fixed print 
head is unsuitable for high-speed printing, as the platen and 
paper have a large mass and hence a high inertia, which 
means that the energy required to perform a high-speed car-
riage return would be prohibitive. Because the mass of the 
print head is very much less than that of the platen, most 
printers are arranged so that the paper stays where it is and 
the print head moves along the line. 
One way of moving the print head is to attach it to a nut on 
a threaded rod (the lead screw). At the end of the rod is a step-
ping motor, which can rotate the rod through a fixed angle at 
a time. Each time the rod rotates the print head is moved left 
or right (depending on the direction of rotation). In another 
arrangement the print head is connected to a belt, moved by 
the same technique as the paper itself. The belt passes 
between two rollers, one of which moves freely and one of 
which is controlled by a stepping motor. 
11.3.2 The inkjet printer 
Inkjet printers were originally developed for professional 
applications and the technology migrated to low-cost PC 
applications. The original continuous inkjet owes more to the 
CRT for its operation than the impact printer. A fine jet of ink 
is emitted from a tiny nozzle to create a high-speed stream of 
ink drops. The nozzle is vibrated ultrasonically so that the ink 
stream is broken up into individual drops. As each drop 
leaves the nozzle it is given an electrical charge, so that the 
stream of drops can be deflected electrostatically, just like the 
beam of electrons in a CRT. By moving the beam, characters 
can be written on to the surface of the paper. The paper is 
arranged to be off the central axis of the beam, so that when 
the beam is undeflected, the ink drops do not strike the paper 
and are collected in a reservoir for re-use. 
Continuous inkjet printers are high-speed devices, almost 
silent in operation, and are used in high-volume commercial 
applications. The original inkjet printer was very expensive 
and was regarded with suspicion because it had suffered a 
number of problems during its development. In particular, 

454 
Chapter 11 Computer peripherals 
they were prone to clogging of the nozzle. Many of the early 
problems have now been overcome. 
Drop-on-demand printing 
The modern drop-on-demand inkjet printer is much simpler 
than its continuous jet predecessor. In fact, it's identical to a 
dot matrix printer apart from the print head itself. The print 
head that generates the inkjet also includes the ink reservoir. 
When the ink supply is exhausted the head assembly is 
thrown away and a new head inserted. Although this looks 
wasteful, it reduces maintenance requirements and increases 
reliability. Some inkjet printers do have permanent print 
heads and just change the ink cartridge. 
In the 1980s inkjet printers had maximum resolution of 
300 dpi (dots per inch) and by the late 1990s inkjet printers 
with resolution of over 1400 dpi were available at remarkably 
low cost. In 2004 you could buy inkjet printers with a resolu-
tion of 5700 dpi that could print photographs that were 
indistinguishable from conventional color photographs. 
Later we will look at the color inkjet printer, which created 
the mass market in desktop digital photography. 
The drop-on-demand print head uses multiple nozzles, one 
for each of the dots in a dot matrix array. The head comes into 
contact with the paper and there's no complex ink delivery 
and focusing system. The holes or capillary nozzles through 
which the ink flows are too small to permit the ink to leak out. 
Ink is forced through the holes by creating a shock wave in the 
reservoir that expels a drop of ink though the nozzle to 
the paper. 
One means of creating a shock wave is to place a thin film of 
piezoelectric crystal transducer in the side of the reservoir (see 
Fig. 11.27(a)). When an electronic field is applied across a 
piezoelectric crystal, the crystal flexes. By applying an electrical 
pulse across such a crystal in a print head, it flexes and creates 
the shock wave that forces a single drop of ink through one of 
the holes onto the paper (see Fig. 11.27(b)). Note that there is a 
separate crystal for each of the holes in the print head. 
Exit point 
Ink reservoir 
Transducer 
flexes and 
expels ink 
drop 
(a) Structure of head assembly 
(b) Voltage pulse applied to 
(only one nozzle shown). 
transducer to eject a drop 
of ink. 
Figure 11.27 Structure of the inkjet. 
OLD PRINTER TECHNOLOGIES 
Dot matrix printer A dot matrix printer forms characters from 
a matrix of dots in much the same way as a CRT. The dots are 
generated by a needle pressing an inked ribbon onto the paper 
or the needles may be used with spark erosion techniques or 
may be replaced by heating elements in a thermal printer. The 
dot matrix printer was very popular in the 1970s and 1980s 
when it offered the only low-cost means of printing. 
Cylinder, golf-ball and daisy-wheel printers The cylinder print 
head is a metallic cylinder with symbols embossed around it.The 
ribbon and paper are positioned immediately in front of the 
cylinder, and a hammer is located behind it.The cylinder is 
rotated about its vertical axis and is moved up or down until the 
desired symbol is positioned next to the ribbon. A hammer, 
driven by a solenoid, then strikes the back of the cylinder, forcing 
the symbol at the front onto the paper through the inked ribbon. 
The golf-ball head was originally used in IBM electric 
typewriters. Characters are embossed on the surface of a 
metallic sphere. The golf-ball rotates in the same way as a 
cylinder, but is tilted rather than moved up or down to access 
different rows of characters. The golf-ball is propelled towards 
the ribbon and the paper by a cam mechanism, rather than by 
a hammer striking it at the back. 
The daisy-wheel printer has a disk with slender petals 
arranged around its periphery. An embossed character is 
located at the end of each of these spokes. The wheel is made 
of plastic or metal and is very lightweight, giving it a low 
inertia. A typical daisy wheel has 96 spokes, corresponding to 
the upper and lower case subsets of the ISO/ASCII code. 
The daisy wheel rotates in the vertical plane in front of the 
ribbon. As the wheel rotates, each of the characters passes 
between a solenoid-driven hammer and the ribbon. When the 
desired character is at a print position, the hammer forces the 
spoke against the ribbon to mark the paper. 
Line printer A line printer prints a whole line of text at one go. 
Line printers are expensive, often produce low quality output, 
and are geared to high-volume, high-speed printing. 
A metal drum extends along the entire width of the paper 
in front of the ribbon. The character set to be printed is 
embossed along the circumference of the drum. This character 
set is repeated, once for each of the character positions, along 
the drum. A typical line printer has 132 character positions 
and a set of 64 characters. As the drum rotates, the rings of 
characters pass over each of the 132 print positions, and a 
complete set of characters passes each printing point once per 
revolution. A mark is made on the paper by a hammer hitting 
the paper and driving it into the head through the ribbon. By 
controlling the instant at which the hammer is energized, any 
particular character may be printed. As there is one hammer 
per character position, a whole line may be printed during the 
course of a single revolution of the drum. 
" \ 
/ 
Piezoelectric i* 
transducer 
aroP 

11.3 The printer 
455 
XEROGRAPHY 
Xerography has a long history. In 1935 a patent attorney, 
Carlton Chester, had an idea for a cheap copying process that 
didn't require the wet and messy chemicals then used in 
photography. While looking for a dry process that allowed 
photocopying, Chester turned his attention to the 
phenomenon of photoconductivity (i.e. the relationship 
between light and the electrical conductivity of materials). He 
was awarded a patent on electrophotography in 1937. 
Chester's first experiments used a metal plate covered with 
sulfur (a photoconductive material).The sulfur was electrically 
charged by rubbing it, and then a glass plate was placed over it. 
Chester wrote on the glass. In the next step, a bright light was 
shone on the assembly for several seconds. The effect of the 
light was to cause the sulfur to conduct and permit the electro-
static charge to leak away to the metal plate under the sulfur. 
The glass was removed and a fine power dusted on the sul-
fur-coated plate. This powder clung to the regions that 
retained their charge because they hadn't been illuminated by 
the light (i.e. the writing). Finally, a sheet of waxed paper was 
placed on the powder-covered plate and pressed against it. 
A copy of the writing on the glass plate was now impressed 
on the wax paper. 
It took until 1944 for Chester to find someone who was 
interested in his invention—the Battelle Memorial Institute. 
Battelle's scientists rapidly discovered that selenium had far 
better photoconductive properties than sulfur and that a 
fine-pigmented resin could easily be fused onto the surface of 
paper to create the copy. 
Battelle develop the process further in conjunction with the 
Haloid Company. They decided that Chester's electrophotog- 
| 
raphy was too cumbersome a term and asked a professor of 
Creek to come up with a better name. He suggested 'dry 
writing' because the process did not involve liquids. The 
corresponding Creek word was xerography. Haloid changed its 
name to Haloid Xerox in 1958 and then to the Xerox 
Corporation in 1961. 
Some inkjet printers employ a fine wire behind the nozzle 
to instantaneously heat the ink to 300°C, well above its boil-
ing point, which creates bubble that force out a tiny drop. 
These printers are called bubble jet printers. 
Although inkjet printers are capable of high resolution 
with over 1000 dpi, the ink drops spread out on paper due to 
the capillary action of fibers on the paper's surface (this effect 
is called wicking). Specially coated paper considerably 
reduces the effect of wicking, although such paper is expen-
sive. Canon's photo paper has a four-layered structure with a 
mirror-finished surface. The outer surface provides an 
ink-absorption layer, consisting of ultrafine inorganic 
particles. By instantly absorbing the ink, this layer prevents 
ink from spreading and preserves the round ink dots. The 
second layer reflects light. The third layer is the same material 
used in conventional photographic paper. The bottom layer 
is a back coating, which counteracts the stresses placed on the 
paper by the upper layers, to prevent curling. 
11.3.3 The laser printer 
The dot matrix printer brought word processing to the masses 
because it produced acceptable quality text at a low cost. The 
laser printer has now brought the ability to produce high-quality 
text and graphics to those who, only a few years ago, could 
afford no more than a medium-quality dot matrix printer. In 
fact, the quality of the laser printer is sufficiently high to enable 
a small office to create artwork similar to that once produced by 
the professional typesetter; that is, desktop publishing (DTP). 
The laser printer is just a photocopier specially modified to 
accept input from a host computer. The principle of the pho-
tocopier and the laser printer is both elegant and simple. At 
the heart of a laser printer lies a precisely machined drum, 
which is as wide as the sheet of paper to be printed. The secret 
of the drum lies in its selenium coating.7 Selenium is an 
electrical insulator with an important property—when 
selenium is illuminated by light, it becomes conductive. 
A photocopier works by first charging up the surface of the 
drum to a very high electrostatic potential (typically 1000 V 
with respect to ground). By means of a complex arrangement 
of lenses and mirrors, the original to be copied is scanned by a 
very bright light and the image projected onto the rotating 
drum. After one rotation, the drum contains an invisible 
image of the original document. If the image is invisible we are 
entitled to ask ourselves, 'What form does this image take?' 
Black regions of the source document reflect little light and 
the corresponding regions on the drum receive no light. The 
selenium coating in these regions is not illuminated, doesn't 
become conducting, and therefore retains its electrical charge. 
Light regions of the document reflect light onto the drum, 
causing the coating to become conducting and to lose its 
charge. In other words, the image on the drum is painted with 
an electrostatic charge, ranging from high voltage (black) to 
zero voltage (white). 
One of the effects of an electrostatic charge is its ability to 
attract nearby light objects. In the next step the drum is 
rotated in close proximity to a very fine black powder called 
the toner. Consequently, the toner is attracted to those parts 
of the drum with a high charge. Now the drum contains a 
true positive image of the original. The image is a positive 
image because black areas on the original are highly charged 
and pick up the black toner. 
7 Modern drums don't use selenium; they use complex organic sub-
stances that have superior photo-electric properties. 

456 
Chapter 11 Computer peripherals 
The drum is next rotated in contact with paper that has an 
even higher electrostatic charge. The charge on the paper 
causes the toner to transfer itself from the drum to the paper. 
Finally, the surface of the paper is heat-treated to fix the toner 
on to it. Unfortunately, not all toner is transferred from the 
drum to the paper. Residual toner is scraped off the drum by 
rotating it in contact with a very fine blade. Eventually, the 
drum becomes scratched or the selenium no longer functions 
properly and it must be replaced. In contrast with other print-
ers, the laser printer requires the periodic replacement of 
some of its major components. Low-cost laser printers some-
times combine the drum and the toner, which means that the 
entire drum assembly is thrown away once the toner has been 
exhausted. This approach to printer construction reduces the 
cost of maintenance while increasing the cost of consumables. 
(a) 
Cleaning blade 
(removes unused toner) 
Corona wire 
(chares drum) 
Feeder paper 
hopper 
Mirror 
(b) 
taser light 
source 
Modulator 
Rotating mirror drum 
Figure 11.28 The laser printer. 
Unlike the photocopier, the laser printer has no optical 
imaging system. The image is written directly onto the 
drum by means of an electromechanical system. As the drum 
rotates, an image is written onto it line by line in very much the 
same way that a television picture is formed in a cathode ray 
tube. 
Figure 11.28(a) illustrates the organization of the laser 
scanner and Fig. 11.28(b) provides details of the scanning 
mechanism. A low-power semiconductor laser and optical 
system produces a very fine spot of laser light. By either vary-
ing the intensity of the current to the laser or by passing the 
beam through a liquid crystal whose opacity is controlled 
electronically (i.e. modulated), the intensity of the light spot 
falling on the drum can be varied. 
The light beam strikes a multi-sided rotating mirror. As the 
mirror turret rotates, the side currently in the path of the light 
beam sweeps the beam across the surface of the selenium-
coated drum. By modulating the light as the beam sweeps 
across the drum, a single line is 
drawn. This scanning process is 
rather like a raster-scan mechanism 
found in a CRT display. After a line 
has been drawn, the next mirror in 
the rotating turret is in place and a 
new line is drawn below the previ-
ous line, because the selenium 
drum has moved by one line. 
The combined motions of the 
rotating mirror turret and the 
rotating selenium drum allow the 
laser beam to scan the entire sur-
face of the selenium drum. Of 
course, the optical circuits required 
to perform the scanning are very 
precise 
indeed. The 
resolution 
imposed by the optics and the 
laser beam size provided low-cost 
first-generation laser printers with a 
resolution of about 300 dots per 
inch. Such a resolution is suitable for 
moderately high-quality text but 
is not entirely suitable for high-
quality graphics. Second-generation 
laser printers with resolutions of 
600 or 1200 dpi became available in 
the mid-1990s. 
Not all laser printers employ the 
same optical arrangement, because 
the 
rotating 
mirror 
turret 
is 
complex 
and 
requires 
careful 
alignment. An alternative tech-
nique designed by Epson uses an 
incandescent light source behind 
Light from 
optical system 
Toner 
Drum 
\ 
Direction 
of scan 
Printed paper 
hopper 
Heater 
Drum 

11.4 Color displays and printers 
457 
a stationary liquid crystal shutter. The liquid crystal shutter 
has a linear array of 2000 dots, each of which can be turned 
on and off to build up a single line across the drum. By writ-
ing a complete line in one operation, the only major moving 
part involved in the scanning process is the photosensitive 
drum itself. Technically, a laser printer without a laser scan-
ner isn't really a laser printer. However, the term laser printer 
is used to describe any printer that generates an image by 
using an electrostatic charge to deposit a fine powder (the 
toner) on paper. 
Other ways of avoiding the complex rotating drum mirror 
turret are a linear array of light-emitting diodes (LEDs) in an 
arrangement similar to the liquid crystal shutter, or a CRT 
projection technique that uses a CRT to project a line onto 
the photosensitive drum. 
Laser printers can print dot-map pictures; that is, each 
pixel of the picture is assigned a bit in the printer's memory. 
A linear resolution of 300 dpi requires 300 X300 = 90 000 
dots/square inch. A sheet of paper measuring 11 inches by 
8 inches (i.e. 88 square inches) can hold up to 88 X 90 
000 = 7 720 000 dots or just under 1 Mbyte of storage. 
Having introduced the principles of monochrome displays 
and printers, we are going to look at how color displays and 
printers are constructed. 
11.4 Color displays and printers 
It's been possible to print color images for a long time, 
although color printers were astronomically expensive until 
relatively recently. Low-cost printers began to appear in the 
early 1990s (largely based on inkjet technology) although the 
quality was suitable only for draft work. By the late 1990s 
high-quality low-cost color printers were widely available 
and the new term photorealistic was coined to describe that 
they were almost able to match the quality of color pho-
tographs. Before we discuss color printers we need to say a 
little about the nature of color. 
11.4.1 Theory of color 
Light is another type of electromagnetic radiation just like 
X-rays and radio waves. The eye is sensitive to electromag-
netic radiation in the visible spectrum and light waves of dif-
ferent frequencies are perceived as different colors. This 
visible spectrum extends from violet to red (i.e. wavelengths 
from 400 nm to 700 nm). Frequencies lower than red are 
called infra-red and higher than violet are called ultra-violet. 
Both these frequencies are invisible to the human eye, 
although they play important roles in our life. 
A single frequency has a pure color or hue and we perceive 
its intensity in terms of is brightness. In practice, we see few 
pure colors in everyday life. Most light sources contain visible 
radiation over a broad spectrum. If a light source contains 
approximately equal amounts of radiation across the 
entire visual spectrum we perceive the effect as white light. 
In practice light often consists of a mixture of white light 
together with light containing a much narrower range of fre-
quencies. The term saturation describes the ratio of colored 
light to white light; for example, pink is unsaturated red at 
about 700 nm plus white light. An unsaturated color is some-
times referred to as a pastel shade. 
Most light sources contain a broad range of frequencies 
(e.g. sunlight and light from an incandescent lamp). Sources 
that generate a narrow band of visible frequencies are gas dis-
charge lamps and LEDs; for example, the sodium light used 
to illuminate streets at night emits light with two very closely 
spaced wavelengths at about 580 nm (i.e. yellow). 
COLOR TERMINOLOGY 
Hue This describes the color of an object. The hue is largely 
dependent on the dominant wavelength of light emitted from 
or reflected by an object. 
Saturation This describes the strength of a color. A color may 
be pure or it may be blended with white light; for example, 
pink is red blended with white. 
Luminance This measures the intensity of light per unit area 
of its source. 
Gamma This expresses the contrast range of an image. 
Color space This provides a means of encoding the color. The 
following color spaces are used to define the color of objects. 
RGB The red, green, blue color space defines a color as the 
amount of its red, blue, and green components. This color 
space is based on the additive properties of colors. 
CMYThe cyan, magenta, yellow color space is used in 
situations in which color is applied to a white background 
such as paper. For example, an object appears yellow 
because it absorbs blue but reflects red and green. Suppose 
you wanted to create blue using a CMY color space. Blue is 
white light with red and green subtracted. Because green is 
absorbed by cyan and red is absorbed by magenta, combin-
ing cyan and magenta leads to the absorption of green and 
red; that is, blue. 
HSBThe HSB model defines light in the way we perceive it 
(H = hue or color, S = saturation, B = brightness or intensity). 
Pantone matching system This is an entirely arbitrary and a 
proprietary commercial system. A very large number of colors 
are defined and given reference numbers. You define a color 
by matching it against the colors in the Pantone system. 

458 
Chapter 11 Computer peripherals 
Shadow mask 
Blue gun 
Red gun 
Green gun 
(a) Three independent electron guns produce 
three electron beams. 
Figure 11.29 Generating a color image. 
Blue 
Red 
Green 
(b) Each beam hits only its own phosphors 
\ 256 entries each holding 
a 12-bit value 
Figure 11.30 The color look-up table. 
Whatever jumble of frequencies the eye detects, the brain 
perceives a single color. Suppose a particular light source con-
tains the colors red and green. We don't see these two colors, 
we perceive a single color whose frequency is intermediate 
between red and green; that is, we see yellow. By mixing vari-
ous quantities of the three primary colors red, green, and blue 
we can create any other color. Moreover, because we can add 
red, green, and blue to create white light, we can control the 
level of saturation. 
11.4.2 Color CRTs 
The majority of general-purpose computers employ color 
displays—partially because we see the world in color, and 
partially because we already have color TV and the cinema. In 
principle the operation of a color CRT is very simple as 
Fig. 11.29 demonstrates. The color CRT is similar to a mono-
chrome CRT. Instead of having a single gun assembly, the 
color CRT has three gun assemblies, one for each of the pri-
mary colors red, green, and blue (Fig. 11.29(a)). The focusing 
system aims each beam at the same spot on the screen and the 
three beams are deflected in unison by the scanning coils. 
The key to the color CRT is the shadow mask located 
immediately in front of the screen. The shadow mask is made of 
metal and has a very large number of tiny holes punched in it 
(Fig. 11.29(b)). Millions of tiny dots of phosphor are deposited 
behind the shadow mask on the surface of the CRT's screen. 
These phosphors are arranged as triplets. One dot produces a 
green 
light 
when 
bombarded 
by 
electrons, one a red light, and one a blue 
light. Because of the geometry of the 
shadow mask, the phosphor dots on the 
screen, and the electron guns, the elec-
tron beam from the green gun can hit 
only green phosphors (the red and blue 
phosphors are shielded by die shadow 
mask). The same is true for beams from 
the other two guns. Some CRTs employ 
a different type of shadow mask; for 
example, the trinitron shadow mask 
employs vertical stripes rather than dots 
to generate a brighter image. 
By applying three independent con-
trol voltages to each of the three guns, 
you can control the intensities of each 
of the beams. Consequently, you can 
control the intensity of each of the 
three pixels—red, green, and blue. Any 
color can be generated by adding suit-
able intensities of green, red, and blue 
light, and, therefore, the human eye 
sees each pixel not as three different 
colors, but as a single color. 
Figure 11.30 illustrates the interface between a color CRT and 
a computer which lets you generate a large number of colors 
specified by relatively few bits. The output of the video memory 
specifies the current pixel's attributes. In the example of 
Fig. 11.30, the pixel is defined by an 8-bit integer that provides 
one of 28 = 256 possible values. The 8-bit pixel value is sent to a 
look-up table mat produces one of 256 12-bit outputs; that is, 
each of the 256 locations contains a 12-bit value. Each 12-bit 
output is divided into three 4-bit fields representing the pri-
mary colors red, green, and blue. In turn, each of these 4-bit val-
ues is fed to a DAC to generate one of 24 = 16 levels of 
brightness. This system can select 1 of 16 levels of red, 
1 of 16 levels of green, and 1 of 16 levels of blue. The pixel can 
therefore have any one of 16 X 16 X 16 = 4096 values 
(i.e. colors). 
An 8-bit pixel code from the video memory can perform 
the apparently impossible act of selecting one of 4096 differ-
ent pixels. The paradox is resolved by the nature of the look-
up table. It has 256 entries, each of which can select one of 
4096 colors; that is, at any instant the look-up table allows the 
display to address 256 colors out of a palette of 4096 colors. 
The look-up table is loaded with the values appropriate to the 
application being run. 
Because the surface of die screen is made up of dots, the res-
olution of a color CRT is limited—the smallest element that 
can be displayed on the screen is a single dot. The resolution of 
color CRTs is often specified in terms of die pitch or distance 
between clusters of die three dots. Typical CRT resolutions lie 
-4 bits 
'H-bitDACI 
^Red signal 
;[ Red 
| 
t 0 C R T 
Video memory 
\ Look-up 
— ' , 4-bit DAC 
Green signal 
system 
~ *. — • table 
— — 
,1 Green 
I 
to CRT 
8-bit pixel 
4 bits- ~"FbiFDAC] 
Blue signal 
value 
J 
J"4 * * - - i B l y g 
" 
to CRT 
Video memor 
system 
-4 bits 
'H-bitDAC| 
^Red signal 
;[ Red 
| 
t 0 C R T 
v Look-up 
|4-bit DACl 
Green signal 
- *. — • table 
A-J~ 
,1 Green 
I 
to CRT 
8-bit pixel 
4 6its- ~"FbiFDACl 
Blue signal 
value 
J 
J"4 " ^ i B U i e 
" 
to CRT 
[Green | 
I Blue"! 

11.4 Color displays and printers 
459 
Monitor size 
Image size 
Resolution 
(pixels) 
Dots 
per inch 
15-inch 
17-inch 
21-inch 
270 X 200 mm 
315 X 240 mm 
385 X 285 mm 
640 X 480 
800 X 600 
1024X768 
640 X 480 
800 X 600 
1024 X 768 
640X480 
800 X 600 
1024 X 768 
1280 X 1024 
1600 X 1200 
60 
75 
96 
51 
63 
85 
42 
52 
85 
84 
106 
Table 11.2 Monitor size, image size, and resolution. 
rowO 
row 1 
:d~b 
2V\ 
row 2 
row 3 
voltage across this cell 
— The voltage across this cell 
21/+ v=3V 
Selected cells 
colO 
coll 
col 2 
col 3 
Figure 11.31 The passive matrix. 
Row line 
Figure 11.32 The active matrix. 
in the range 0.28 to 0.31 mm. Table 11.2 provides relates 
image size to resolution for some popular configurations. 
Color LCDs 
Color LCDs operate on a similar principle to the color CRT 
and generate a color pixel from three cells with three primary 
colors. The individual cells of a color LCD display include 
red, green, or blue filters to transmit light of only one color. 
As in the case of the color CRT, the three primary colors are 
clustered to enable other colors to be synthesized by combin-
ing red, green, or blue. Color LCD displays are divided into 
two types: active and passive. Both active and passive displays 
employ the same types of LCD cells—the difference lies in the 
ways in which the individual cells of a matrix are selected. 
The so-called passive liquid crystal matrix of Fig. 11.31 
(this arrangement applies to both monochrome and color 
displays) applies a large pulse to all the cells (i.e. pixels) of a 
given row. This pulse is marked 2Vin Fig. 11.31 and is cur-
rently applied to row 2. A smaller pulse that may be either 
positive or negative is applied to each of the columns in the 
array. The voltage from the row and the column pulses are 
applied across each cell in a row, and are summed to either 
polarize the cell or to leave it unpolarized. 
This arrangement displays an entire row of 
pixels at a time. Once a row has been drawn, a 
pulse is applied to the next row, and so on. 
Each cell is connected to one row line and 
to one column line. In Fig. 11.32 a pulse of 
level 2V is applied to one terminal of all the 
cells in row 2 of the matrix. A pulse of 
level +V or — V is then applied in turn to 
each of the column cells, 0, 1,2, and 3. The 
voltage across each cell in the matrix must be 
either 0 (neither row nor column selected), V 
(row selected with +2V, column selected 
with +V),or3V(row 
selected with +2V, col-
umn selected with — V). The matrix is 
designed so that the 3 V pulse across a cell is 
sufficient to polarize the liquid crystal and 
turn the cell on. 
The passive matrix suffers from cross-talk 
caused by the pulse on one row leaking into 
cells on adjacent rows. Furthermore, if the 
matrix has m rows, each row is driven (i.e. 
accessed) for only Mm of the available time. 
These limitations produce a display that has 
low contrast, suffers from smearing when mov-
ing images are displayed, and has a less bright 
image than the TFT active matrix alternative 
to be described next. Although passive 
matrix displays were popular in the 1990s, 
improvements in active matrix manufacturing 
technology have rendered them obsolete. 
Transistor 
switch 
X C e " / 
Detail 
I he v 
^ 
IV-
Column line 
+Cell 
rCell 
+C.1T 
Transistor 
switch 
J I I ' " , 
fCell 
£Cell 
4= Cell 
TFT 
TFT 
TFT 
TFT 
TFT 
TFT 

460 
Chapter 11 Computer peripherals 
A better arrangement is the active matrix of Fig. 11.32; the 
cell structure is exactly the same as that of a passive display, 
only the means of addressing a cell is different. A transistor, 
which is simply an electronic switch, is located at the junction of 
each row and column line; that is, there is one transistor for 
each cell. The transistor can be turned on or off by applying a 
pulse to its row and column lines. However, the electrical 
capacitance of each cell is able to store a charge and maintain 
the cell in the on or off condition while die matrix is address-
ing anodier transistor. That is, a transistor can be accessed and 
turned on or off, and that transistor will maintain its state until 
the next time it is accessed. The active matrix array produces a 
sharper and more vivid picture. The lack of cross-talk between 
adjacent cells means that the active matrix suffers less smearing 
than the passive array equivalent. 
The transistors that perform the switching are not part of a 
silicon chip but are laid down in thin films on a substrate— 
hence the name TFT (thin film transistor). It takes 
3 X 1024 X 768 thin film transistors to make a XVGA active 
matrix display, and, if just a few of these transistors are faulty, 
the entire display has to be rejected. The manufacturing yield 
of good arrays is not 100%, which means that the cost of a 
TFT active matrix array is considerably higher than the 
passive equivalent. 
11.4.3 Color printers 
Color printers don't employ the same RGB (red, green, blue) 
principle used by the color CRT. Suppose we look at an object 
that we call red, which is illuminated by white light. The red 
object absorbs part of the white light and reflects some of the 
light to the observer. If all the light is reflected we call the 
object white and if all the light is absorbed we call the object 
Mack. However, if all frequencies are absorbed except red, we 
call if the object red. In other words, if we wish to print images 
we have to consider what light is absorbed rather than what 
light is generated (as in a CRT). 
The RGB model is called additive because a color is created 
by adding three primary colors. The CMY (cyan, magenta, 
yellow) color model used in printing is called subtractive 
because a color is generated by subtracting the appropriate 
components from white light. Cyan (blue-green) is the 
absence of red, magenta the absence of green, and yellow the 
absence of blue. Mixing equal amounts of cyan, magenta, and 
yellow subtracts all colors from white light to leave black. To 
create a color such as purple the printer generates a pattern of 
0% black 
2S% black 
50% Mack 
75% black 
100% black 
EE SB EB El H 
Figure 11.33 Dithering. 
magenta and cyan dots. The saturation can be controlled by 
leaving some of the underlying white paper showing through. 
Adding the three subtractive primaries together doesn't 
produce a satisfactory black; it creates a dark muddy looking 
color. Although the human eye is not terribly sensitive to slight 
color shifts, it is very sensitive to any color shift in black (black 
must be true black). Printers use a four-color model CMYK, 
where K indicates black. Including a pure black as well as the 
three subtractive primaries considerably improves the image. 
Printing color is much more difficult than displaying it on 
a CRT. Each of the red, green, and blue beams can be modu-
lated to create an infinite range of colors (although, in 
practice, a digital system is limited to a finite number of 
discrete colors). When you print an image on paper, you have 
relatively little control over the size of the dot. Moreover, it's 
not easy to ensure that the dots created from the different 
subtractive primaries are correctly lined up (or registered). 
You can generate different levels or shades of a color by dither-
ing (a technique that can also be applied to black and white 
printers to create shades of gray). 
Dithering operates by dividing the print area into an array 
of, say, 2-by-2 matrices of 4 dots. Figure 11.33 provides a sim-
ple example of dithering in black and white. Because the dots 
in the matrices are so small, the eye perceives a composite 
light level and the effect is to create one of five levels of gray 
from black to white. 
Didiering isn't free. If you take a 3 X 3 matrix to provide 
10 levels of intensity, the effective resolution of an image is 
divided by diree; for example, a 300 dpi printer can provide a 
resolution of 300 dpi or a resolution of 100 dpi with a 10-level 
gray scale. In other words, there's a trade-off between resolu-
tion and the range of tones that can be depicted. 
The principle of dithering can be extended to error diffusion 
where dots are placed at random over a much larger area than 
the 2 by 2 matrix. This technique is suited to printing areas of 
color that require subde changes of gradation (e.g. skin tones). 
An alternative approach to dithering that provides more 
tones without reducing resolution is to increase the number 
of colors. This technique was introduced by some manufac-
turers to provide the photorealism required to print the out-
put from digital cameras. One enhanced subtractive inkjet 
printer uses six inks: cyan, magenta, yellow, light magenta, 
light cyan, and black. The lighter colors make it possible to 
render skin tones more realistically. Another printer uses 
cyan, magenta, yellow, two types of black, plus red and blue. 
Color inkjet printers 
The color inkjet printer is virtually the same as the black and 
white counterpart. The only difference lies in the multiple 
heads. Typical color printers use a separate black cartridge 
and a combined color cartridge. Because the head and ink 
reservoirs form a combined unit, the cartridge has to be 
thrown away when the first of the color inks runs out. Some 

11.5 Other peripherals 461 
printers use separate print heads and reservoirs and only the 
ink cartridge need be replaced. 
Inkjet printer ink can be dye based or pigment based. A dye 
is a soluble color dissolved in a solvent and is used by most 
printers. A pigment is a tiny insoluble particle that is carried in 
suspension. Pigment-based inks are superior to dyes because 
pigments are more resistant to fading and can provide more 
highly saturated colors. Pigment-based inks have less favorable 
characteristics from the point of view of the designer; for 
example, the pigments can separate out of the liquid. 
Inkjet printers can be prone to banding, an effect where 
horizontal marks appear across the paper due to uneven ink 
distribution from the print head. 
Apart from its cost, the advantage of color inkjet printing 
is the bright, highly saturated colors. However, good results 
can be achieved only with suitable paper. The type of plain 
paper used in laser printers gives poor results. The drops of 
ink hit the paper and soak into its surface to mix with adja-
cent drops. This mixing effect reduces the brightness and 
quality of the colors. 
By about 2000, advances in inkjet printing, ink technology, 
and inkjet papers and the advent of the digital camera had 
begun to wipe out the large photographic industry based on 
traditional silver halide photographic paper, the optical camera, 
and the developing enlarging, and printing process. Moreover, 
the availability of digital-image processing programs such as 
Photoshop gave amateurs a level of control over the photo-
graphic image that only professionals had a few years ago. 
Thermal wax and dye sublimation printers 
The thermal wax printer is rather like the dot matrix printer 
with heat-sensitive paper. The print head extends the length 
of the paper and contains a matrix of thousands of tiny pixel-
size heaters. Instead of a ribbon impregnated with ink, a sheet 
of material coated with colored wax is placed between the 
head and the paper. When the individual elements are heated 
to about 80°C, the wax is melted and sticks to the paper. An 
entire line of dots is printed at a time. The paper must make 
three or more passes under the print head to print dots in 
each of the primary (subtractive) colors. The sheet contain-
ing the wax consists of a series of bands of color. 
Dye sublimation is similar to the thermal wax technique 
but is more expensive and is capable of a higher quality result. 
Electrical elements in the print head are heated to 400°C, 
which vaporizes the wax. These special waxes undergo 
sublimation when heated; that is, they go directly from the 
solid state to the gaseous state without passing through 
the liquid state. 
By controlling the amount of heating, the quantity of wax 
transferred to the paper can be modified making it possible to 
generate precise colors without having to resort to techniques 
such as dithering. Unlike the thermal wax process, which 
deposits drops of wax on the paper, dye sublimation 
impregnates the paper with the wax. Dye sublimation can 
create very-high-quality images on special paper. The cost of 
the consumables (waxed sheets and special paper) make sub-
limation printing much more expensive than inkjet printing. 
The phase-change printer 
The phase-change printer falls somewhere between the inkjet 
printer and the thermal wax printer. The basic organization is 
that of the inkjet printer. The fundamental difference is that 
the print head contains a wax that is heated to about 90°C to 
keep it in liquid form. The wax is bought in the form of sticks, 
which are loaded into the print head. 
The print head itself uses a piezo-electric crystal to create a 
pressure wave that expels a drop of the molten wax onto the 
paper. The drops freeze on hitting the paper, causing them to 
adhere well without spreading out. You can print highly satu-
rated colors on plain paper. Because the paper is covered with 
lumpy drops, some phase-change printers pass the paper 
through pressure rollers to flatten the drops. 
The color laser 
Color laser printers are almost identical to monochrome 
printers. Instead of using a black toner, they use separate 
toners in each of the subtractive primary colors. The image is 
scanned onto a drum using a toner with the first primary 
color and then transferred to paper. The same process is 
repeated three more times using a different color toner after 
each scan. Advances in color laser technology have produced 
color lasers that cost as much today in real terms as mono-
chrome lasers did a decade ago. However, the consumables 
for color lasers (i.e. three tones plus a block tone) are still 
expensive. 
11.5 Other peripherals 
We've looked at several peripherals found in a personal com-
puter. We now describe some of the peripherals that can be 
connected to a digital computer. Computers are used mainly 
in embedded control systems—the PC is just the tip of a very 
large iceberg. Embedded controllers take information from 
the outside world, process it, and control something. We 
begin by looking at some of the sensors that can measure 
properties such as temperature and pressure. 
11.5.1 Measuring position and 
movement 
The most fundamental measurement is that of position; for 
example, the position of the gas pedals in a car or the position 
of the arm in a crane. Position is measured either as rotation 
(i.e. the position of a dial or knob) or as a linear, position (i.e. 
the position along a line). 

462 
Chapter 11 Computer peripherals 
+v 
Wiper 
OV 
Resistor 
(a) The angle-sensing potentiometer. 
Figure 11.34 Position transducers. 
AC source 
Three coils 
Primary 
coil 
Secondary 
J 
coil 
High permeability 
nickel iron core 
$—$ 
Output 
(b) The magnetic position sensor. 
Figure 11.34(a) describes a simple position transducer 
called the potentiometer, which can measure linear or angular 
movement. An arm or wiper moves along the surface or a 
resistor with a voltage of V between its ends. The voltage 
picked up by the sliding arm is proportional to its position 
along the resistor, The potentiometer is cheap, gives a large 
electrical output that is easy to measure, but is unreliable 
because it wears out due to friction. Another position 
transducer uses a magnetic field. Figure 11.34(b) demon-
strates a transformer where one coil generates a magnetic field 
and the other coil detects it. The amount of magnetic field 
picked up is dependent on the position of the magnetic core. 
Measuring very small distances requires a device that gen-
erates a signal when it moved by even the tiniest amount. The 
strain gauge consists of a zigzag path of wire embedded in a 
substrate such as plastic. When the strain gauge is deformed 
by bending, the resistance of the wire increases slightly 
because it has been stretched. The change in resistance in 
response to strain is usually very small indeed—the resistance 
of a 200 ft strain gauge might change by only a few millionths 
of an ohm. A strain gauge might be bonded to, say, the wing 
of an aircraft to measure how much it flexes in flight. 
An alternative to the resistive strain gauge employs the 
piezo-electric effect; certain crystals generate a voltage across 
their faces when they are flexed. 
A modern form of the pressure sensor is constructed with 
semiconductor technology. Four resistors are deposited on a 
1 mm diameter wafer of silicon in the form of a bridge. When 
the silicon flexes due to stress, the voltage across two of the ter-
minals changes. If one side of the wafer is adjacent to a vacuum 
(created beneath the silicon disk during the manufacturing 
process), the device measures the absolute pressure on the 
other side. This type of pressure transducer is very versatile and 
can measure very tiny pressures or very high pressures. It is 
used in some electronic engine management systems to mea-
sure the manifold pressure, which is required to calculate 
engine power and to control optimum spark timing. 
Velocity can be measured either indirectly by measuring 
the rate of change of position, or more directly with a 
Toothed metallic 
wheel 
Shaft connecting 
to rotating device 
Output signa 
Figure 11.35 The tachometer. 
transducer or tachometer. A simple means of measuring 
velocity is to measure the speed of a magnet traveling down a 
long coil because the voltage generated is proportional to the 
velocity of the magnet. A tachometer measures speed of rota-
tion (e.g. the speed of a car's wheels) by either an optical disc 
or a toothed wheel and proximity sensor (see Fig. 11.35). 
You can measure the speed of a liquid or a gas by placing a 
turbine in the path of the liquid and then using a tachometer 
to measure the speed of the turbine. Figure 11.36 describes an 
alternative flowmeter that measures the flow rate of a liquid 
or gas by using the pressure differential between a pipe and a 
constriction. 
An interesting flow rate device is the thermal anemometer. If 
your coffee is too hot, you blow on it because the forced airflow 
carries heat away from the coffee. The thermal anemometer 
uses a heater in a tube though which a gas is flowing. If the tem-
perature of the heating element is kept constant, it requires 
more current to keep the heater at a constant temperature as 
the gas flow increases. By measuring the current being supplied 
to the heater, you can determine the gas flow. 
Finally, you can measure the rate of flow of a liquid by 
ultrasonic techniques, which are non-invasive and do not 
require direct contact with the fluid. A beam of ultrasound is 
~ Magnetic core 
,—Coil 

11.5 Other peripherals 
463 
Flow of 
gas or 
liquid 
Extrance 
Exit 
Figure 11.38 The Venturi flowmeter. 
Copper 
t- 
— 
Digital 
output 
to CPU 
Chromel 
Temperature-
measuring junction 
Compensating 
junctions 
Figure 11.37 Temperature-compensated thermocouple. 
directed through the pipe into the flowing liquid. You can 
measure the rate of flow either by determining the transit 
time of the ultrasound beam or by measuring the Doppler 
frequency shift in the sound reflected by particles and bub-
bles in the liquid. This is the technique used by ultrasound 
scanners to measure the flow of blood within the heart. 
11.52 Measuring temperature 
One of the most common sensors is the temperature probe. 
The everyday temperature transducer is the thermometer, 
which converts the temperature into the length of a column 
of liquid. Mercury or alcohol in a glass bulb expands with 
temperature and is driven up a fine transparent tube against 
a scale calibrated in degrees. 
Computers don't measure temperature via the thermal 
expansion of a liquid. There are several ways of converting 
temperature into a voltage that can be measured by an ana-
log-to-digital converter. The first is the thermoelectric effect in 
which two wires of dissimilar metals are connected together 
to form a thermocouple (e.g. copper and constantan, or 
chromel and alumel). If the junction between the wires is 
heated, a voltage appears across it (about 50 fi V/°C). This 
technique is used to measure a wide range of temperatures; 
for example, a platinum/platinum-rhodium alloy thermo-
couple can measure furnace temperatures up to 1500°C. The 
thermocouple operating range is about -270°C to 2300°C, 
although no single thermocouple can 
cover this whole range. 
Figure 11.37 illustrates the struc-
ture of a temperature-compensated 
thermocouple where the potential 
difference between a junction com-
posed of chromel and alumel alloy 
wires is measured. Because these two 
wires must be connected to the mea-
suring system, two further junctions 
are used and put in an ice bath to pro-
vide a reference junction. 
The output of a thermocouple is 
not a linear function of temperature. 
Thermocouple manufacturers pub-
lish algorithms and the associated 
coefficients to convert the output of a 
thermocouple into an accurate tem-
perature. 
Another temperature-measuring 
technique uses the resistance of a 
metal that changes with tempera-
ture. A platinum resistance tempera-
ture detector (RTD) has a resistance 
of 10 11 that changes by 0.385%/°C. 
The RTD is more linear than the thermocouple but is 
unsuited to the measurement of temperatures over about 
800°C. 
Semiconductor devices, called thermistors, have an electrical 
resistance that varies with temperature. These devices are very 
sensitive to small changes in temperature. The temperature 
coefficient is negative so that the thermistor conducts more as 
the temperature rises. Because of their high sensitivity (e.g. a 
change of 100 fl/°C) thermistors can be used to measure tem-
perature precisely over a small range. Unfortunately, the ther-
mistor cannot be used above about 200° C. 
Another temperature-measuring element is the band-gap 
device, which employs a semiconductor junction (i.e. the region 
between two differently doped8 areas of silicon in a transistor). 
The band-gap detector operates over a relatively narrow range 
of temperatures but can provide an accuracy of 0.1 °C. All the 
temperature sensors we've just described can be made very 
small indeed (e.g. 1 mm or less across). 
Yet another means of measuring temperature exploits the 
radiation from a body. Any body whose temperature is above 
absolute zero radiates energy in the infra-red region of the spec-
trum. Figure 11.38 gives the radiation intensity produced by an 
ideal object, called a black body, as a function of temperature 
and radiation wavelength. A perfect black body generates 
8 Doping a semiconductor refers to the addition of an element in 
amounts as low as one part in 106 to change the semiconductor's electri-
cal properties. 
I Pressure 
_ ^differential 
Throat 
^ - \ ^ \ 
(constriction) 
Alumel 
Copper 
Analog-to-
digital converter 
Ice bath 

464 
Chapter 11 Computer peripherals 
radiation proportional to the fourth power of the body's tem-
perature (Stefan's law) and the wavelength of the radiation with 
the greatest amplitude falls as the temperature rises (Wein's law). 
Once the temperature rises sufficiently, the radiation falls into 
the visible band and we say that the object has become red hot. 
Real materials don't have ideal black body radiation char-
acteristics. If the emissivity9 of a body is less than that of a 
black body, it is called a gray body. If the emissivity varies with 
temperature, it is called a non-gray body. 
The temperature of a body can be measured by examining 
the radiation it radiates. This temperature measurement 
technique is non-contact because it does not affect or disturb 
the temperature of the object whose temperature is being 
recorded. Moreover, you can measure the temperature of 
moving objects. In recent years this technique has been used 
to measure human body temperatures by measuring the 
radiation produced inside the ear. 
11.5.3 Measuring light 
Light intensity has been measured for hundreds of years. When 
it was noticed that compounds of silver and iodine darken on 
Black body radiation 
exposure to light, the effect was quickly exploited to create 
photography. In the early 1900s Max Planck suggested that 
light consists of individual packets containing a discrete 
amount of energy called photons. When a photon hits an atom, 
an electron may be knocked out of its orbit round the nucleus. 
If this atom is metallic, the movement of electrons generates a 
current that flows in the metal. Some light detectors operate by 
detecting the flow of electrons when light interacts with the 
atoms of a material. 
The photodiode is a semiconductor photosensor compris-
ing a junction between two differently doped regions of 
silicon. The photons of light falling on the junction create a 
current in the device. These devices are sensitive to light in 
the region 400 nm to 1000 nm (this includes infra-red as well 
as visible light). Another means of measuring light intensity 
exploits the photovoltaic effect in silicon and selenium. 
Light intensity can also be measured by the photoresistor. 
Certain substances such as cadmium sulfide change their 
electrical resistance when illuminated. 
11.5.4 Measuring pressure 
The effect of pressure or stress on a 
material is to deform it by compres-
sion or expansion, an effect we call 
strain. Strain is defined as the change 
in length per unit length. We can 
measure stress (pressure) from the 
effect of the strain it causes. 
600 
800 
1000 
1200 
Wavelength (nm) 
Figure 11.38 Radiation emitted by an ideal black body. 
N 
„ 
Rotation away from the 
equator (600 mph) 
1600 
w 
•• 
Air flowing north from 
the equatot is deflected 
to the west 
Rotation at equator 
1000 mph) 
Figure 11.39 The Coriolis force. 
11.5.5 Rotation sensors 
If you sometimes feel you're going 
round in circles, a rotation sensor can 
at least confirm it. At first sight it's 
hard to imagine a sensor that can tell 
you how fast something is turning. 
One way of measuring rotation is 
to exploit the mechanism that creates 
the World's weather patterns—the 
Coriolis force. Air at the equator is 
heated by the sun and flows north and south. Figure 11.39 
shows the direction of flowing to the north. It doesn't travel 
due north. Let's consider what happens to the northward-
moving stream of air. The Earth rotates on its north-south 
axis once every 24 hours. Because the circumference of the 
Earth is 24 000 miles, the Earth is moving at 1000 mph at the 
equator. And so is the air above the equator because the Earth 
drags the air around with it. 
9 Emissivity is the ratio of a body's radiation at a given frequency to 
the radiation given off by a perfect black body at the same temperature. 
A material-like rock may have an emissivity of 0.9, whereas a metal might 
have an emissivity of 0.1. 
jndjno XSjauj 
f jS 
^-. 
300K 
/-—-->. 
280K \ 
r ^ ^ 
260K 
N. 
' 
240K ^v 
\. 
''""~^"~>"220K 
X. 
\^ 
\ 
\ 
\ 
1 
1 
1 
1 
1 
1 
1 
200 
400 
600 
800 
1000 
1200 
1400 

11.5 Other peripherals 
465 
Piezoelectric transducer 
(picks up the vibrations^ 
Prism 
Piezoelectric transducer 
(picks up the vibrations) 
Output 
,. (proportional to 
speed of rotation) 
Piezoelectric transducer 
(generates sound waves) 
Oscillator 
(a) Left channel greater than reference. 
(b) Right channel greater than reference. 
Figure 11.40 Exploiting the Coriolis effect to measure rotation. 
The circumference of the Earth gets less as you move away 
from the equator and the speed at which the Earth's surface 
moves reduces. If you live in the USA you're moving at about 
600 mph. The stream of air flowing north from the equator it 
is also moving west to east at 1000 mph (because that's the 
speed it had at the equator). Because the rotational speed of 
the Earth drops as you go north, an observer on die ground 
sees the air deflected toward the east. That is, the air appears to 
come from the south-west. Because it takes a force to deflect 
an object, we say that the wind has been deflected by a force 
called the Coriolis force (this force does not actually exist). 
Figure 11.40 demonstrates how the Coriolis force is used to 
measure the rate of rotation of a body. A triangular prism has 
diree piezoelectric transducers bonded to its sides. A piezo-
electric crystal flexes when a voltage is applied across its face; 
equally a voltage is generated when it is flexed (i.e. the piezo-
electric effect is reversible). An alternating voltage is applied 
to one of the transducers to create a high-frequency sound 
wave that propagates through the prism. The two facing 
piezoelectric transducers convert the vibrations into two 
equal alternating voltages when the prism is stationary. 
When die prism rotates die vibrations spread out from the 
transducer through the medium that is rotating. The motion 
of the vibrations and the motion of the rotating prism inter-
act and the direction of the vibrations is altered by the 
Coriolis force. Consequently, the two sensors don't pick up 
equal amounts of vibration in a rotating prism. The differ-
ence between the outputs of the sensors can be used to 
generate a signal whose amplitude depends on the rate of 
rotation. Figure 11.40 illustrates the effect of a left rotation. 
The output from the left transducer 
(a) is greater than that from right 
transducer (b). 
You might wonder who would 
want to sense rotation. As in many 
other areas of electronics, the intro-
duction of a new device often leads to 
applications that couldn't have been 
imagined in die past. Low-cost rota-
tion sensors can be used in automatic 
applications to follow the motion of 
the car. If you know how far the car 
has traveled (from the odometer) and 
the angular velocity (from which you 
can calculate the angular position), 
you can work out where the car is (if 
you know its initial position). This 
technology can be used to provide 
in-car navigation systems—especially 
when coupled with GPS. Another 
application of a rotation sensor is in 
stabilizing platforms; for example, the 
video camera. By detecting the motion of the camera, you can 
move the image sensor or lens in the opposite direction to 
obtain a stable image free of camera shake and jitter. 
11.5.6 Biosensors 
One of the greatest revolutions in the late twentieth 
Century was biotechnology, which may eventually dwarf 
the computer revolution. If computers are to be used in 
biotechnology systems, it's necessary to be able to detect bio-
logical parameters electronically. Until relatively recently, the 
only way of measuring a biological quantity such as the level 
of glucose or oxygen in blood was to take a sample and send 
it to a laboratory for chemical analysis. In circumstances 
where time is critical such as during an operation, a know-
ledge of a biological parameter like the blood oxygen level is 
vital. 
It's difficult to describe biosensors in a few paragraphs 
because it requires an understanding of biochemical reac-
tions. Here, we describe only the underlying principles. The 
first biosensor was developed by Clark in the 1950s to meas-
ure the amount of dissolved oxygen in blood. 
Suppose you connect two electrodes to a voltage supply 
and immerse them in a liquid (see Fig. 11.41(a)). A current 
will flow between the electrodes if there's a means of trans-
porting electrons. Unfortunately, blood contains many mole-
cules capable of carrying a charge and their combined effect 
would swamp the current carried by oxygen molecules. 
Clark's solution was to surround the electrodes with a plastic 
gas-permeable membrane (a form of molecular sieve). Now 
only oxygen is able to pass through the membrane and carry 
Comparator 

466 
Chapter 11 Computer peripherals 
, / 
7 
Silver anode Noble metal 
cathode 
K2Hh 
Gas permeable 
membrane 
(a) A current flows between two 
(b) The electrodes are enclosed by 
electrodes in a cell. 
a membrane that is permeable 
to oxygen. 
Glucose oxidase 
(c) A second permeable mem-
brane surrounds the inner 
membrane. Glucose oxidase 
gel fills the space between 
the membranes. 
Figure 11.41 The biosensor. 
V(t) 
The analog signal 
is continuous in 
time and value 
>Time 
The digital signal 
is discrete in 
time and value 
/! 
•••Time 
Figure 11.42 Analog and digital signals. 
a charge between the electrodes. By measuring the current 
flow you 
can 
determine 
the 
oxygen 
concentration 
(Fig. 11.41(b)) in the liquid surrounding the cell. 
This technique can be extended to detect more exotic 
molecules such as glucose. A third membrane can be used to 
surround the membrane containing the electrodes. Between 
the outer and inner membranes is a gel of glucose oxidase 
enzyme that reacts with glucose and oxygen to generate glu-
conic acid. The amount of glucose in the liquid under test is 
inversely proportional to the amount of oxygen detected 
(Fig. 11.41(c)). 
Because these techniques were developed in the 1960s the 
number of detectors has vastly increased and the size of the 
probes reduced to the point at which they can be inserted 
into veins. 
11.6 The analog interface 
We now look at analog systems and their interface to the 
digital computer. In an analog world, measurable quantities 
are not restricted to the binary values 0 and 1; they may 
take one of an infinite number of val-
ues within a given range. For example, 
the temperature of a room changes 
from one value to another by going 
through an infinite number of incre-
ments on its way. Similarly, air pres-
sure, speed, sound intensity, weight, 
and time are all analog quantities. 
When computers start to control their 
environment, or generate speech or 
music, or process images, we have to 
understand the relationship between 
the analog and digital worlds. 
We first examine analog signals and demonstrate how they 
are captured and processed by a digital computer. Then we 
look at the hardware that converts analog signals into digital 
form, and digital values into analog signals. 
A full appreciation of the relationship between and analog 
and digital signals and the transformation between them 
requires a knowledge of electronics; this is particularly true 
when we examine analog-to-digital and digital-to-analog 
converters. Readers without an elementary knowledge of 
electronics may wish to skip these sections. 
11.6.1 Analog signals 
In Chapter 2 we said that a signal is said to be analog if it falls 
between two arbitrary levels, Vx and Vy, and can assume any 
one of an infinite number of values between Vx and Vy. If the 
analog signal, V(t), is time dependent, it is a continuous func-
tion of time, so that its slope, dVldt, is never infinite, which 
would imply an instantaneous change of value. Figure 11.42 
illustrates how both an analog voltage and a digital voltage 
vary with time. 
V(t) 
t 

11.6 The analog interface 
4 6 7 
Analog signals are processed by analog circuits. The princi-
pal feature of an analog circuit is its ability to process an ana-
log signal faithfully, without distorting it—hence the 
expression hi-fidelity. A typical analog signal is produced at 
the output terminals of a microphone as someone speaks 
into it. The voltage varies continuously over some finite 
range, depending only on the loudness of the speech and on 
the physical characteristics of the microphone. An amplifier 
is used to increase the amplitude of this time-varying signal 
to a level suitable for driving a loudspeaker. If the voltage gain 
of the amplifier is A, and the voltage from the microphone 
V(t), the output of the amplifier is equal to A -V(t). The out-
put signal from the amplifier, like the input, has an infinite 
range of values, but within a range A times that of the signal 
from the microphone. 
Because digital signals in computers fall into two ranges 
(e.g. 0 to 0.4 V for logical 0 and 2.4 to 5.0 V for logical 1 levels 
in LS TTL logic systems], small amounts of noise and cross-
talk have no effect on digital signals as long as the noise is less 
than about 0.4 V. Life is much more difficult for the analog 
systems designer. Even small amounts of noise in the millivolt 
or even microvolt region can seriously affect the accuracy of 
analog signals. In particular, the analog designer has to worry 
about power-line noise and digital noise picked up by analog 
circuits from adjacent digital circuits. 
11.6.2 Signal acquisition 
At first sight it might appear that the analog and digital 
worlds are mutually incompatible. Fortunately a gateway 
exists between the analog and digital worlds called quantiza-
tion. The fact that an analog quantity can have an infinite 
range of values is irrelevant. If somebody says they will arrive 
at 9.0 a.m., they are not being literal—9.0 a.m. exists for an 
Physical_ 
variable 
Physical 
variable 
Physical 
variable 
Transducer 
Amplifiei 
Filter 
Transducer 
Amplifiei 
Filter 
Transducer 
Amplifiei 
Filter 
Analog signal processing 
(n channels) 
Channel 1 
Channel 2 
n-channel 
analog 
multiplexer 
Channel n 
7% 
infinitesimally short period. Of course, what they really mean 
is that they will arrive at approximately 9.0 a.m. In other 
words, if we measure an analog quantity and specify it to a 
precision sufficient for our purposes (i.e. quantization), the 
error between the actual analog value and its measured value 
is unimportant. Once the analog value has been measured, it 
exists in a numeric form that can be processed by a computer. 
The conversion of an analog quantity into a digital value 
requires two separate operations; the extraction of a sample 
value of the signal to be processed and the actual conversion 
of that sample value into a binary form. Figure 11.43 gives the 
block diagram of an analog signal acquisition module. As the 
analog-to-digital converter (ADC) at the heart of this mod-
ule may be rather expensive, it is not unusual to provide a 
number of different analog channels, all using the same ADC. 
The cost of an ADC also depends on its speed of conversion. 
Each analog channel in Fig. 11.43 begins with a transducer 
that converts an analog quantity into an electrical value. 
Transducers are almost invariably separate from the signal 
acquisition module proper. Sometimes the transducer is a 
linear device, so that a change in the physical input produces 
a proportional change in the electrical output. All too often, 
the transducer is highly non-linear and the relationship 
between the physical input and the voltage from the trans-
ducer is very complex; for example, the output of a trans-
ducer that measures temperature might be V = V0e"kT. In 
such cases it is usual to perform the linearization of the input 
in the digital computer after the signal has been digitized. It is 
possible to perform the linearization within the signal acqui-
sition module by means of purely analog techniques. 
The electrical signal from the transducer is frequently very 
tiny (sometimes only a few microvolts) and must be amplified 
before further processing in order to bring it to a level well 
above the noise voltages present in later circuits. Amplification 
Digital 
output 
—»-d0 
- • d 2 
Sample 
and hold 
circuit 
Analog-
to-digital 
converter 
START 
SAMPLE 
Channel select 
STOP 
System 
control 
logic 
Control 
input 
Figure 11.43 An analog signal acquisition module. 
dm-1 

468 
Chapter 11 Computer peripherals 
is performed by an analog circuit called an op-amp (opera-
tional amplifier). Some transducers have an internal amplifier. 
After amplification comes filtering, a process designed to 
restrict the passage of certain signals through the circuit. 
Filtering blocks signals with a frequency above or below a 
cut-off point; for example, if the signal from the transducer 
contains useful frequency components only in the range 0 to 
20 Hz (as one might expect from, say, an electrocardiogram), 
it is beneficial to filter out all signals of a higher frequency. 
These out of band signals represent unwanted noise and have 
no useful effect on the interpretation of the electrocardio-
gram. Moreover, it is necessary for the filter to cut out all fre-
quencies above one-half the rate at which the analog signal is 
sampled. The reasons for this are explained later. 
The outputs of the filters are fed to an electronic switch 
called a multiplexer., which selects one of the analog input 
channels for processing. The multiplexer is controlled by the 
digital system to which the signal acquisition module is con-
nected. The only purpose of the multiplexer is to allow one 
analog-to-digital converter to be connected to several inputs. 
The analog output of the multiplexer is applied to the input 
of the last analog circuit in the acquisition module, the sample 
and hold (S/H) circuit. The sample and hold circuit takes an 
almost instantaneous sample of the incoming analog signal and 
holds it constant while the analog-to-digital converter (ADC) is 
busy determining the digital value of the signal. 
The analog-to-digital converter (ADC) transforms the 
voltage at its input into an m-bit digital value, where m varies 
Binary code 
111 
110 
101 
100 
011 
010 
001 
ooo I 
' ^ v 
o.o\o.5 LONi ; 
Initially, the input is 0 V 
and the output code 000 
When the input reaches 0.5 V, 
the output code jumps to 001 
Figure 11.44 The transfer function of an ideal 3-bit A/D converter. 
from typically 4 to 16 or more. Several types of analog-
to-digital converter are discussed at the end of this section. 
We now look at the relationship between the analog signal 
and the analog-to-digital conversion process. 
Signal quantization 
Two fundamental questions have to be asked when consider-
ing any analog-to-digital converter. Into how many levels or 
values should the input signal be divided and how often 
should the conversion process be carried out? The precise 
answer to both these questions requires much mathematics. 
Fortunately, they both have simple conceptual answers and in 
many real situations a rule-of-thumb can easily be applied. 
We look at how analog signals are quantized in value and then 
how they are quantized or sampled in time. 
When asked how much sugar you want in a cup of coffee, 
you might reply: none, half a spoon, one spoon, one-and-
a-half spoons, etc. Although a measure of sugar can be 
quantized right down to the size of a single grain, the practi-
cal unit chosen by those who add sugar to coffee is the 
half-spoon. This unit is both easy to measure out and offers 
reasonable discrimination between the quanta (i.e. half-
spoons). Most drinkers could not discriminate between, say, 
13/27 and 14/27 of a spoon of sugar. As it is with sugar, so it is 
with signals. The level of quantization is chosen to be the 
minimum interval between successive values that carries 
meaningful information. You may ask, 'Why doesn't everyone 
use an ADC with the greatest possible resolution?' The answer 
is perfectly simple. The cost of an ADC 
rises steeply with resolution. A 16-bit 
ADC is very much more expensive 
than an 8-bit ADC (assuming all other 
parameters to be equal). Therefore, 
engineers select the ADC with a resolu-
tion compatible with the requirements 
of the job for which it is intended. 
Let's look at an ideal 3-bit analog-to-
digital converter that converts a voltage 
into a binary code. As the analog input 
to this ADC varies in the range 0 V to 
7.5 V, its digital output varies from 000 
to 111. Figure 11.44 provides a transfer 
function for this ADC. 
Consider the application of a linear 
voltage ramp input from 0.0 V to 7.5 V 
to this ADC (a ramp is a signal mat 
increases at a constant rate). Initially 
the analog input is 0.0 V and the digi-
tal output 000. As the input voltage 
rises, the output remains at 000 until 
the input passes 0.5 V, at which point 
the output code jumps from 000 to 
001. The output code remains at 001 
Analog 
nput(V) 
Analog 
| A n a l of- 
I 
•doi ^ X 
inmit 
• to-digital 
• d A 
\ 
l nP u t 
*\ 
. J foutput\ 
converter 
*"a2J 
0.0\0.5 l.oNl 5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 
The maximum 
change of input 
f-equ'tfed-tepreckjee 
a 1 bit change in the 
output is 1.0 V 

11.6 The analog interface 
469 
until the input rises above 1.5 V. Clearly, for each 1.0 V change 
in the input, the output code changes by one unit. 
Figure 11.44 shows that the input can change in value by up 
to 1 V without any change taking place in the output code. 
The resolution of an ADC, Q, is the largest change in its 
input required to guarantee a change in the output code and is 
1.0 V in this example. The resolution of an ADC is expressed 
indirectly by the number of bits in its output code, where 
resolution = Vmaximum/2" - 1. For example, an 8-bit ADC 
with an input in the range 0 V to +8.0 V has a resolution of 
8.0V/255 = 0.03137V= 31.37 mV. Table 11.3 gives the basic 
characteristics of ADCs with digital outputs ranging from 4 to 
16 bits. The figures in Table 11.3 represent the optimum values 
for perfect ADCs. In practice, real ADCs suffer from imperfec-
tions such as non-linearity, drift, offset error, and missing 
codes, which are described later. Some ADCs are unipolar and 
handle a voltage in the range 0 to Vand some are bipolar and 
handle a voltage in the range — V/2 to + V/2. 
The column labeled value of Q for 10 V FS in Table 11.3 
indicates the size of the step (i.e. Q) if the maximum input of 
the ADC is 10 V. The abbreviation 'FS' means full-scale. 
Figure 11.45 provides a graph of the difference or error 
between the analog input of a 3-bit ADC and its digital out-
put. Suppose that the analog input is 5.63 V. The correspond-
ing digital output is 110, which represents 6.0 V; that is, the 
digital output corresponds to the quantized input, rather 
than the actual input. The difference between the actual input 
and the idealized input corresponds to an error of 0.37 V. 
Figure 11.45 shows that the maximum error between the 
input and output is equal to Q/2. This error is called the 
quantization error. 
The output from a real ADC can be represented by the out-
put from a perfect ADC whose input is equal to the applied 
signal plus a noise component. The difference between the 
input and the quantized output (expressed as an analog 
value) is a time-varying signal between +Q/2 and —Q/2 and 
is called the quantization noise of the ADC. 
Because the quantization noise is a random value, engineers 
characterize it by its RMS (root mean square)—the RMS value 
Resolution (bits) 
Discrete states 
Binary weight 
4 
16 
0.062S 
6 
64 
0.0156 
8 
256 
0.00391 
10 
1024 
0.000977 
12 
4096 
0.000244 
14 
16 384 
0.0000610 
16 
65 536 
0.0000153 
expresses the power of the signal. The RMS value of a signal is 
obtained by squaring it, taking the average, and then taking 
the square root of the average. The RMS of the quantization 
noise of an analog-to-digital converter is equal to Q/V12. 
Increasing the resolution of the converter reduces the ampli-
tude of the quantization noise as Table 11.3 demonstrates. 
A figure-of-merit of any analog system is its signal-to-noise 
ratio, which measures the ratio of the wanted signal to the 
unwanted signal (i.e. noise). The signal-to-noise ratio (SNR) 
of a system is expressed in units called decibels, named after 
Graham Bell, a pioneer of the telephone. The SNR ratio of 
two signals is defined as 201og(Vsignal/Vnois(,). The signal-
to-noise ratio of an ideal «-bit ADC is given by 
SNR (in dB) = 201og(2"Q )/Q/ V\2 
= 20log(2") + 10log(12) 
= 6.02« + 10.8 
This expression demonstrates that the signal-to-noise 
ratio of the ADC increases by 6.02 dB for each additional bit 
of precision. Table 11.3 gives the signal-to-noise ratio of 
ADCs from 4 to 16 bits. An 8-bit ADC has a signal-to-noise 
ratio similar to that of some low-quantity audio equipment, 
whereas a 10-bit ADC approaches the S/N ratio of high-
fidelity equipment. 
Another figure-of-merit of an analog system is its dynamic 
range. The dynamic range of an ADC is given by the ratio of 
its full-scale range (FSR) to its resolution, Q, and is expressed 
in decibels as 201og(2") = 20«log2 = 6.02n. Table 11.3 also 
gives the dynamic range of the various ADCs. Once again you 
can see that a 10- to 12-bit ADC is suitable for moderately 
high-quality audio signal processing. Because of other 
impairments in the system and the actual behavior of a real 
ADC, high-quality audio signal processing is normally done 
with a 16-bit ADC. 
Sampling a time-varying signal 
What is the minimum rate at which a signal should be 
sampled to produce an accurate digital representation of it? 
Value of Q for 10 V FS 
SNR (dB) 
Dynamic range (dB) 
0.625 V 
34.9 
24.1 
0.156V 
46.9 
36.1 
39.1 mV 
58.1 
48.2 
9.76 mV 
71.0 
60.2 
2.44 mV 
83.0 
72.2 
610 uV 
95.1 
84.3 
153 |iV 
107.1 
96.3 
Table 11.3 The performance of ideal analog-to-digital converters. 

470 
Chapter 11 Computer peripherals 
Binary code 
111 
110 
101 
100 
011 
010 
001 
000 
0.0 
015 1! 
Quantization error 
.0 1: 5 Z .0 Z 5 3.0 
3.5 
4.0 
4.5 
5.0 
5.5 
6.0 
6.5 
7.0 
7.5 
Analog 
** input (V) 
». Analog 
input (V) 
Maximum negative 
Zero quantization 
quantization error 
error 
Figure 11.45 The error 
function of an ideal 3-bit A/D 
converter. 
We need to know the minimum rate at which a signal must be 
sampled, because we want to use the slowest and cheapest 
ADC that does the job we require. 
Intuitively, we would expect the rate at which a signal must 
be sampled to be related to the rate at which it is changing; for 
example, a computer controlling the temperature of a swim-
ming pool might need to sample the temperature of the water 
once every 10 minutes. The thermal inertia of such a large 
body of water doesn't permit sudden changes in temperature. 
Similarly, if a microcomputer is employed to analyze human 
speech with an upper frequency limit of 3000 Hz, it is reason-
able to expect that the input from a microphone must be 
sampled at a much greater rate than 3000 times a second, 
simply because in the space of 1/3000 second the signal can 
execute a complete sine wave. 
A simple relationship exists between the rate at which a sig-
nal changes and the rate at which it must be sampled if it is 
to be reconstituted from the samples without any loss of 
information content. The Sampling Theorem states 'If a 
continuous signal containing no frequency components 
higher than £ is sampled at a rate of at least 2fc, then the orig-
inal signal can be completely recovered from the sampled 
value without distortion'. This minimum sampling rate is 
called the Nyquist rate. 
The highest frequency component in the signal means just 
that and includes any noise or unwanted signals present 
together with the desired signal. For example, if a signal con-
tains speech in the range 300 to 3000 Hz and noise in the 
range 300 to 5000 Hz, it must be sampled at least 10 000 times 
a second. One of the purposes of filtering a signal before 
sampling it is to remove components whose frequencies are 
higher than the signals of interest, but whose presence would 
nevertheless determine the lower limit of the sampling rate. 
If a signal whose maximum frequency component is fc, 
is sampled at less than 2fc times a second, some of the high-
frequency components in it are folded back into the spectrum of 
the wanted signal. In other words, sampling a speech signal in 
the range 300 to 3000 Hz containing noise components up to 
0 
- Q / 2 , 

Amplitude 
M i 
11.6 The analog interface 
471 
Amplitude 
-•Frequency (f) 
-"—• Frequency (f) 
(a) Spectrum of input signal. 
(b) Spectrum of sampled signal. 
Figure 11.46 Sampling a signal at more than the Nyquist rate. 
Amplitude 
(V) 
Amplitude 
( V ) * 
-*• Frequency (f) 
*• Frequency (f) 
(a) Spectrum of input signal. 
(b) Spectrum of sampled signal 
(shaded region indicates overlap in spectra). 
Figure 11.47 Sampling a signal at slightly less than the Nyquist rate. 
5000 Hz at only 6000 times a second would result in some of 
this noise appearing within the speech band. This effect is called 
frequency folding and, once it has occurred, there is no way in 
which the original, wanted, signal can be recovered. 
Figures 11.46 and 11.47 illustrate the effect of sampling an 
analog signal at both below and above the Nyquist rate. In Fig. 
11.46 the input signal consists of a band of frequencies from 
zero t o / , sampled at a rate equal to/s times a second, where/, 
is greater than 2/c. The spectrum of the sampled signal contains 
components in the frequency range/ -fc to/s 4- fc that do not 
fall within the range of the input signal. Consequently, you can 
recover the original signal from the sampled signal. 
In Fig. 11.47 the input signal has a maximum frequency 
component of/c and is sampled at/si, where/ < 2/c. Some 
energy in the region/ - / to fc falls in the range of the input 
frequency and is represented by the gray region in 
Fig. 11.47. This situation results in frequency folding and a loss 
of information; that is, you cannot recover the original 
information from the sampled signal. 
The classic example of sampling at too low a rate is the 
wagon wheel effect seen in movies. A cine film runs at 24 
frames/s and each frame samples the image. If the spokes of 
a rotating wheel are sampled (i.e. photographed) at too low 
a rate, the wheel appears to move backward. Why? Suppose a 
wheel rotates 10° clockwise between each frame. The eye 
perceives this as a clockwise rotation. Now suppose the wagon 
is moving rapidly and the wheel rotates 350° between each 
frame. The eye perceives this as a 10° counterclockwise rotation. 
It is difficult to appreciate the full implications of the sam-
pling theorem without an understanding of the mathematics 
of sampling and modulation. However, all we need say here is 
that the overlap in spectra caused by sampling at too low a 
frequency results in unwanted noise in the sampled signal. 
Another way of looking at the relationship between a sig-
nal and its sampling rate is illustrated by Figs 11.48 and 11.49. 
Figure 11.48(a) gives the continuous input waveform of an 
analog signal and Fig. 11.48(b) its sampled form. These sam-
pled amplitudes are, of course, stored in a digital computer 
numerically. Figure 11.48(c) shows the output of a circuit, 
called a filter, fed from the digital inputs of Fig. 11.48(b). The 
simplest way of describing this circuit is to say that it joins up 
the dots of the sampled signal to produce a smooth output. As 
you can see, the reconstituted analog signal is virtually a copy 
of the original analog signal. 
Figure 11.49 is similar to Fig. 11.48, except that the input sig-
nal is sampled at less than 2/. A glance at the sampled values of 
Fig. 11.49(b) is enough to show that much of the detail in the 
input waveform has been lost. When this sampled signal is 
reconstituted into a continuous signal (Fig. 11.49(c)) its fre-
quency is not the same as the input signal. The erroneous signal 
' 
• 
i 
a 
h-fz 
fc 
h 
fs+fc 

472 
Chapter 11 Computer peripherals 
(b) Sampled signal. 
V 
(c) Reconstituted signal. 
Figure 11.48 Sampling at fs > 2fc. 
(a) Input signal. 
The input is sampled less 
than twice per cycle 
->-t 
(b) Sampled signal. 
The output is a highly 
distorted version of 
the input. 
c) Reconstituted signal. 
Figure 11.49 The aliasing effect (fs < 2Q. 
of Fig. 11.49(c) is called an alias. Once more, it must be stressed 
that if frequencies greater than l/2 fs appear in the input signal 
they can play havoc with the results of sampling. 
Most signal acquisition modules have low-pass filters with 
a sharp cut-off frequency to attenuate signals and noise 
outside the band of interest. As it is impossible to construct a 
perfect filter that passes frequencies in the range 0 to fc and 
Input 
voltage 
During the time 
that the input 
is sampled, it 
changes by 5V 
• ( 
Aperture time defines the 
period required to measure 
the signal 
Figure 11.50 The effect of a finite measurement time on the 
A/D conversion process. 
which attenuates all frequencies above fc infinitely, it is usual 
to sample a signal at a much greater rate than 2fc in order to 
reduce the effects of aliasing to an acceptable level. Typically, 
a signal may be sampled at up to five times the rate of its 
maximum frequency component. 
Aperture time 
As well as the sampling frequency, we also have to think about 
the time taken by the sampling process itself. Signals of interest 
are time dependent. One question we should ask is, 'What 
happens if a signal changes while it is being measured?' 
Figure 11.50 illustrates the problem of trying to measure a 
dynamic quantity where the quantization process takes ta sec-
onds, which is called the aperture time. The term aperture time 
suggests an analogy with the camera—a image is captured 
when the camera's aperture (i.e. shutter) is open and any move-
ment of the subject blurs the image. During the aperture time, 
the input voltage being measured changes by 8V, where8Vis 
given by 
dV 
dt 
The value of d V7dt is the slope of the graph. The change in 
the input, S V, is called the amplitude uncertainty. A perfect, 
instantaneous digitizer has a zero aperture time and 8 V = 0, 
resulting in a spot-sample of the input. 
Suppose we apply a linearly rising ramp voltage to the 
input of an analog-to-digital converter that has a full-scale 
range of 5 V. Let's imagine that the input changes by 5 V in 
100 ms, corresponding to a rate-of-change of 5 V in 0.1 s = 
50 V/s. If the analog-to-digital converter takes 1 ms to per-
form a conversion, we can calculate the amount by which the 
input changes while it's being converted. 
5 V = t, -dV(t)/df = 1 ms X 50 V/s 
= 1 X 1(T3 X 50 V/s = 0.05 V 
(a) Input signal. 

11.6 The analog interface 
473 
That is, the input changes by 0.05 V during the period that 
the A/D conversion is taking place. Consequently, there's lit-
tle point in using an ADC with a resolution of better than 
0.05 V. This resolution corresponds to 1 in 100, and a 7-bit 
ADC would be suitable for this application. 
In order to get a feeling for the importance of aperture 
time, let's consider a data acquisition system in processing 
human speech. Suppose a system with an 8-bit analog-to-
digital converter is required to digitize an input with an upper 
frequency limit of 4000 Hz. We need to know the maximum 
aperture time necessary to yield an accuracy of one least sig-
nificant bit in the digitized output. Assuming a sinusoidal 
input, V(t) = Vsin wt, the amplitude uncertainty is given by 
SV = fa-d(Vsi„wf)/df = fa-wV- coscof 
The differential of sin ait is wcos wt, where to is defined as 
2nf. The maximum rate-of-change of V(t) occurs at the zero-
crossing of the waveform when f = 0 (i.e. the maximum value 
of cos (Otis 1). Therefore, the worst case value of S Vis 
8V= tt-V-a> 
and 
SV7V = ra«i 
ta-2Tf-/ 
We can substitute 1/256 for 5V7Vand 4000 Hz for/in the 
above equation to calculate the desired aperture time as 
follows: 
8V/V = 1/256 = fa2irf = ( , X 2 X 3.142 X 4000 
t3 = 1/(256 X 2 X 3.142 X 4000) s = 0.146 u.s 
An aperture time of 0.146 |AS (i.e. 146 ns) is very small, 
although not too small to be achieved by the some ADCs. 
Fortunately, we can use a sample and hold circuit to capture a 
sample of the input and hold it constant while a relatively 
slow and cheap ADC performs the conversion. Of course, 
even a sample and hold circuit is itself subject to the effects of 
aperture uncertainty. Although an aperture time of 1 u,s is 
relatively small for an analog-to-digital converter, a sample 
and hold circuit can achieve an aperture time of 50 ns with 
little effort. We look at the sample and hold circuit in more 
detail later. 
11.6.3 Digital-to-analog conversion 
Beginning with digital-to-analog converters (DACs) may 
seem strange. It's more logical to discuss analog-to-digital 
(ADC) conversion first and then deal with the inverse 
process. There are two reasons for disregarding this natural 
sequence. The first is that the DAC is less complex than the 
corresponding ADC, and the second is that some analog-to-
digital converters, paradoxically, have a digital-to-analog 
converter at their heart. 
Conceptually, the DAC is a simple device. To convert a 
binary value into analog form, all we have to do is to generate 
an analog value proportional to each bit of the digital word 
and then add these values to give a composite analog sum. 
Figure 11.51 illustrates this process. An w-bit digital signal is 
latched by m D flip-flops and held constant until the next 
value is ready for conversion. The flip-flops constitute a 
digital sample and hold circuit. Each of the m bits operates an 
electronic switch that passes either zero or V, volts to an 
analog adder, where V{ is the output of the ith switch. The 
output of this adder is 
v=d0v0 + d,v1 + --- + d_1vm^ 
The m{dj in this equation represent binary values 0 or 1 
and the {VJ represent binary powers of the form (1,1/2,1/4, 
1/8,...). 
Figure 11.52 gives a possible (but not practical) implemen-
tation of a digital-to-analog converter. The total current 
flowing into the inverting terminal of the operational amplifier 
is equal to the linear sum of the currents flowing through the 
individual resistors (the panel describes how the operational 
Parallel 
digital 
input 
Latch 
Figure 11.51 The digital-
to-analog converter. 
m-bit latch 
•To 
Q J - ^2 
•Switch) 
v 
* C 
\ e ^ 
\ 
I 
1 
'Vo 
\ 
I 
1 di 
/ ^ \ 
\ 
* D 
Q 
>-(SwitchJ 
> 
\ 
* c 
*{ 
\ 
\ 
—:— 
Vi 
\x^ 
/Analog\ 
^ Analog 
-Udder J 
output 
J D 
Q|_ _ftl 
Xvitcr] 
' 
h 
1 
> c 
^-r^ 

474 
Chapter 11 Computer peripherals 
Analog 
' 
Operational 
" 
amplifier 
Figure 11.52 A possible implementation of the D/A converter. 
THE OPERATIONAL AMPLIFIER 
The operational amplifier is a simple circuit that is widely used 
in many applications. In the figure below, an amplifier has two 
input terminals, one called the inverting input marked 
b y ' - ' and one called the non-inverting input marked by'+'. 
The output of the amplifier is ~AV„ where V, is the voltage 
difference between the two input terminals and A is its gain 
(amplification). 
To analyze the operational amplifier, all you need know is 
Ohm's law, which states 'the current /' flowing through a resis-
tor R is given by VIR, where Vis the voltage across the ends o f 
the resistor 
From the diagram we can immediately write down 
'. = (K -vt //?! 
h = (v, - V0JIR2 
and 
Vou, = --AV; 
If we assume that the current flowing into the inverting 
terminal of the amplifier is zero (approximately true in 
practice), we have i, = i2.That is 
(l<n - Vj/R, = (V, - V0M)/RZ 
We can substitute forV, = - V0JA in this equation to get 
(Vm + KJA)IR, = (~V0JA - Kj//? 2 
Re-arranging this equation gives 
- / y / V O + (1 + RtIR^)IA) 
VaJVm 
In a practical operational amplifier, the gain of the amplifier, -A, 
approaches infinity and (1 + /?2//?,)/A approaches zero. 
Therefore we can write the gain of the operational amplifier as 
VnJV, = w 
This remarkable result shows that the gain is dependent 
only on the value of the components /?, and R2 and not on the 
amplifier itself (as long as the value of A is very large). 
amplifier works). As each of the resistors in Fig. 11.52 can be 
where d; represents the state of the rth switch. The voltage at 
connected to ground or to a precisely maintained reference 
voltage, Vref, the current flowing through each resistor is either 
zero or V„^2'R, where i = 0,1,2,..., m — 1. The total current 
flowing into the operational amplifier is given by 
R k 2-' 
the output terminal of the operational amplifier is given by 
V„ = - 2Vlef X RfIR X [dm_, X 2-' + dm^2 X 2-2 
+ ••• + d j X 2~m] 
Real digital-to-analog converters implement the m 
switches, typically, by field-effect transistors (a field-effect 
transistor behaves as a fast electronic switch—the voltage at 
m digital 
switches 
\ 
dm-l 
dm~2 
^nrf 
d0 
Kn 
A/VW—IN. 
• 
I T 
2% 
4 f i / 
2m-lA 
/?/ 
- W W 
'i 
-Km 
Kn-
*K 

11.6 The analog interface 
475 
Analog 
Operational 
o u tP u t 
amplifier 
di 
d2 
Figure 11.53 The R-2R ladder D/A converter. 
its gate determines whether the path between the other two 
terminals is open or closed). By switching the control gate of 
these transistors between two logic levels, the resistance 
between their source and drain terminals is likewise switched 
between a very high value (the off or open state) and a very 
low value (the on or closed state). A perfect field-effect tran-
sistor switch has off and on values of infinity and zero, respec-
tively. Practical transistor switches have small but finite 
on-resistances that degrade the accuracy of the DAC. 
Although the circuit of Fig. 11.52 is perfectly reasonable 
for values of m below six, larger values create manufacturing 
difficulties associated with the resistor chain. Suppose a 
10-bit DAC is required. The ratio between the largest and 
smallest resistor is 210:1 or 1024:1. If the device is to be 
accurate to one LSB, the precision of the largest resistor must 
be at least one-half part in 1024, or approximately 0.05%. 
Manufacturing resistors to this absolute level of precision is 
difficult and costly with thin-film technology, and virtually 
impossible with integrated circuit technology. 
The R-2R ladder 
An alternative form of digital-to-analog converter is given in 
Fig. 11.53, where the DAC relies on the R-2R ladder 
(pronounced R two R). This DAC is so called because all 
resistors in the ladder have either the value R or 2R. Although 
it's difficult to produce highly accurate resistors over a wide 
range of values, it is much easier to produce pairs of resistors 
with a precise 2:1 ratio in resistance. 
As the current from the reference source, VKf, flows down 
the ladder (from left to right in Fig. 11.53), it is divided at each 
junction (i.e. the node between the left R, right R, and 2R resis-
tors) into two equal parts, one flowing along the ladder to the 
right and one flowing down the 2R shunt resistor. The net-
work forms a linear circuit and we can apply the Superposition 
Theorem. This theorem states that, in a linear system, the effect 
is the sum of all the causes. Consequently, the total current 
flowing into the inverting terminal of the operational ampli-
fier is equal to the sum of all the currents from the shunt (i.e. 
2R) resistors, weighted by the appropriate binary value. 
A digital-to-analog converter based on the R-2R ladder 
has three advantages over the type described in Fig. 11.54. 
1. All resistors have a value of either R or 2R, making it easy 
to match resistors and to provide a good measure of 
temperature tracking between resistors. Furthermore, the 
residual on-resistance of the transistor switches can 
readily be compensated for. 
2. By selecting relatively low values for R in the range 
2.5 kfl to 10 kfl, it is both easy to manufacture the DAC 
and to achieve a good response time because of the low 
impedance of the network. 
3. Due to the nature of the R-2R ladder, the operational 
amplifier always sees a constant impedance at its input, 
regardless of the state of the switches in the ladder, which 
improves the accuracy of the operational amplifier circuit. 
The R-2R ladder forms the basis of many commercially 
available DACs. Real circuits are arranged slightly differently 
to that of Fig. 11.53 to reduce still further the practical prob-
lems associated with a DAC. 
DACs based on the potentiometric network 
Another form of digital-to-analog converter is called the 
potentiometric or tree network. Figure 11.54 describes a 3-bit 
arrangement of such a network where a chain of n resistors is 
placed in series between the reference supply and ground. 
The value of « is given by 2ra, where m is the resolution of the 
DAC. In the example of Fig. 11.54, m = 3 and n = 8. An 8-bit 
DAC requires 256 resistors in series. The voltage between 
ground and the lower end of the ith resistor is given by 
V = VJRInR = VrJln 
fori = Otow - 1. 
2/?/ 
ZR> 
R 
R 
R 
2R 
Rf 
2R7 
0 m 
0 • 
Oi 
i*a 
]sQ 
1<« 
i 
1 
1> 
* 
4 
lsb do 
di 
d2 
d m ^ 

476 
Chapter 11 Computer peripherals 
"ref- T 
gnd • 
S07 
S06 
S05 
S04 
S03 
S02 
S01 
soo 
S13 
S12 
S21 
S11 
S10 
S20 
Tree network 
of switches 
Analog 
Operational 
" 
amplifier 
Figure 11.54 The tree-
network D/A converter. 
The value of the resistors, R, does not appear in this equa-
tion. All that matters is that the resistors are of equal value. 
Because the flow of current through the resistors is constant, 
the effects of resistor heating found in some forms of R-2R 
ladder are eliminated. 
The switch tree serves only to connect the input terminal 
of the operational amplifier to the appropriate tap (i.e. node) 
in the resistor network. In fact, this switching network is 
nothing but an n:\ demultiplexer. Moreover, because the 
switches do not switch a current (as in the case of the R-2R 
network), the values of their on and off resistances are rather 
less critical. 
A DAC based on a switch tree is also inherendy monotonia 
That is, as the digital input increases from 00 . . . 0 to 11 . . . 1, 
the analog output always increases for each increment in the 
input. 
Before we look at analog-to-digital converters, we need to 
say something about errors in digital-to-analog converters. 
Errors in DACs 
Real DACs differ from the ideal DACs described above. 
Differences between input code and output voltages are 
out 
4 
tOffsgfr-
Actual output 
• Ideal output 
For any given code 
the output voltage has 
a constant offset error. 
(input code) 
Figure 11.55 The constant offset error. 
caused by errors that originate in the DACs analog circuits. 
Figures 11.55 to 11.59 give five examples of errors in DACs. 
We have drawn the outputs of Figs 11.55 to 11.59 as straight 
lines for convenience—in practice they are composed of steps 
because the input is a binary code. 
In Fig. 11.55, the DACs output voltage differs from its ideal 
value by a constant offset. If the input is a binary value X, the 
output is equivalent to that of a perfect DAC plus a constant 
error signal e, that is Vou[ = KX + e. A constant error is easy to 
deal with because it can be trimmed out by adding a compensat-
ing voltage of equal magnitude but of opposite sign to the error. 
R 
R 
R 
R 
R 
R 
R 
R 

11.6 The analog interface 
477 
(output voltage) 
- Actual output 
...- Ideal output 
Output error 
Figure 11.56 The gain error. 
For any given input code 
the error in the output 
voltage is proportional 
to the input code. 
• * 
(input code) 
111 
110 
101 
100 
011 
010 
001 
000 
(output code) 
4> 
..*-" 
Missing code 
Figure 11.60 The missing code. 
Actual output 
As the input voltage increases, 
the output steps through the 
codes 000 to 111. However, in 
this case, the output step from 
010 to 100 misses the code 011. 
There is no input voltage that 
generates the code 011. 
.... Ideal output 
• Actual output 
For any given input code 
the error in the output 
voltage is proportional 
to the input code plus 
a constant offset. 
^ X 
(input code) 
Figure 11.57 The combined effect of offset and gain errors. 
^Actual output 
•Ideal output 
The error between the 
input code and the 
output code is non-linear 
(input code) 
Figure 11.58 The non-linear error. 
"out 
Actual output 
Ideal output 
The error between the 
input code and the 
output code is non-linear 
and non-monotonic. 
000 
Input code 011 
produces a lower 
output than code 010 
(input code) 
Figure 11.59 Non-monotonicity. 
corrected by passing the DAC's output through an amplifier 
with a gain factor of \/k. 
Real DACs suffer from both offset and gain errors as illus-
trated in Fig. 11.57. The combined offset and gain errors can 
both be removed separately by injecting a negative offset and 
passing the output of the DAC through a compensating 
amplifier as we've just described. 
A more serious error is the non-linear response illustrated 
in Fig. 11.58 where the change in the output, Q, for each step 
in the input code is not constant. The error between the input 
code and the output voltage is a random value. Non-linear 
errors cannot easily be corrected by simple circuitry. Many 
DACs are guaranteed to have a maximum non-linearity less 
than one-half Q, the quantization error; i.e. the DAC's output 
error is always less than Q/2 for any input. 
Figure 11.59 illustrates a non-monotonic response, a form 
of nonlinearity in which the output voltage does not always 
increase with increasing input code. In this example, the ana-
log output for the code 011 is less than that for the code 010. 
Non-monotonic errors can be dangerous in systems using 
feedback. For example, if an increasing input produces a 
decreasing output, the computer controlling the DAC may 
move the input in the wrong direction. 
Analog-to-digital converters suffer from similar errors to 
DACs—only the axes of the graphs in Figs 11.55 to 11.59 are 
changed. An interesting form of an ADC error is called the 
missing code where the ADC steps from code X to code X + 2 
without going through code X + 1. Code X + 1 is said to be a 
missing code, because there is no input voltage that will gener-
ate this code. Figure 11.60 demonstrates the transfer function 
of an ADC with a missing code. As the input voltage to the 
ADC is linearly increased, the output steps through its codes 
one by one in sequence. In Fig. 11.60 the output jumps from 
010 to 100 without passing through 011. 
Figure 11.56 illustrates a gain error in which the difference 
between the output of the DAC and its ideal value is a linear 
function of the digital input. In this case, if the ideal output is 
Vou( = KX, the actual output is given by V0M = k • KX, where 
k is the gain error (ideally k = 1). The gain error can be 
11.6.4 Analog-to-digital conversion 
Although converting a digital value into an analog signal is 
relatively easy, converting an analog quantity into a digital 
value is rather more difficult. In fact, apart from one special 
^out 
001 010 
011 
100 
Wi 
111 
11Q, 
^out 
• fin 

478 
Chapter 11 Computer peripherals 
''hold 
R 
-vVW 
The circuit acts as 
an electronic switch 
controlled by Vhold. 
Analog 
input 
o 
vin 
vA/VV-t 
Diode bridge 
Analog 
+ / " 
output 
The output follows the input 
when VhQld is 0 or holds (freezes) 
the input when Vho|d= 1. 
Figure 11.61 The sample 
and hold circuit. 
Analog 
input /? 
Vin 
AAA" 
The diode bridge 
is a switch that can 
be opened or 
closed electronically. 
(a) Equivalent circuit. 
Operational 
amplifier 
* y Analog 
output 
Analog 
input 
# 
Vin 
The output is 
a copy of the 
input. 
-*V„ A n a l 0§ 
output 
(b) Equivalent circuit when switch closed. 
Operational 
amplifier 
(c) Equivalent circuit when switch open. 
The capacitor 
holds the output 
constant. 
*.Vn Analog 
output 
Operational 
amplifier 
Figure 11.62 Operation of the sample and hold circuit. 
type of A/D converter, analog-to-digital conversion is per-
formed in a roundabout way. In this section, we describe 
three types of A/D converter: the parallel converter (the only 
direct A/D converter), the feedback converter, and the inte-
grating converter. 
Before we describe ADCs in detail, we look 
at the sample and hold circuit used to freeze 
time-varying analog signals prior to their 
conversion. This circuit is sometimes called a 
follow and hold circuit. We mentioned this 
circuit when we discussed aperture time. 
The sample and hold circuit 
Like many other analog circuits, the sample 
and hold (S/H) circuit is simple in principle 
but very complex in practice. The divergence 
between theory and practice stems from the 
effect of second-or even third-order non-
linearities of analog circuits. Such problems 
don't affect digital circuits. 
Figure 11.61 gives the circuit of a sample 
and hold amplifier. Readers without a back-
ground in electronics may skip the details of 
this circuit's operation—all it does is to 
charge a capacitor to the same level as the 
input signal, and then connect the capacitor 
to its output terminals. For a short time, the 
voltage on the capacitor remains constant, 
allowing the ADC to perform a conversion 
with a relatively constant input. 
If we forget the diode bridge and regard the 
input resistor, R, as being directly connected 
to the inverting terminal of the operational 
amplifier, we have a simple inverting buffer 
with unity gain (see Fig. 11.62(a)). That is, 
Km = ~~ Vin- Assume also that the capacitor 
C has negligible effect on the circuit. 
The diode bridge in Fig. 11.61 acts as an on/off switch that 
either connects the analog input to the inverting terminal of 
the op-amp via R, or isolates the inverting terminal from die 
input. When the switch is in the closed position, the S/H 
Operational 
Karnpiifier 
gnd — 
gnd — 
gnd 
gnd — 

11.6 The analog interface 
479 
Input 
Sample 1 
Hold 0 
Output 
-••time 
; 
Sample 
: 
• time 
y—*-time 
Aperture time 
Figure 11.63 Timing details of the sample and hold circuit. 
Analog pulses 
Digital signal 
Sampte 
and hold 
circuit 
Hold clock 
<-*• t 
uncertainty time. We've already met this parameter, which 
defines the period during which the input must not change 
by more than, say, a least-significant bit. Aperture times vary 
from about 50 ns to 50 ps, or less. One pico second (ps), is 
10~12 seconds. 
In the hold mode, the capacitor discharges and the output 
begins to droop. Droop rates vary, typically, between 5 u,V/u,s 
and 0.01 u.V7u.s. The parameters of the S/H circuit are often 
interrelated and optimizing one parameter may degrade the 
values of other parameters. 
Sample and hold circuits are vital when analog-to-digital 
converters with appreciable conversion times are to be con-
nected to time-varying inputs. Sample and hold circuits must 
sometimes be used with digital-to-analog converters. A sam-
ple and hold circuit can be fed from a DAC and used to turn 
the sequence of analog values from the DAC into a continu-
ous analog signal. In this mode the S/H circuit is called a zero-
order hold filter and its output 
consists of steps between the analog 
values see Fig. 11.64. Another advan-
tage of the S/H circuit is that it 
deglitches the DAC and removes any 
spikes in its output. 
Now that we have described how 
an analog signal can be captured, the 
next step is to show how it can be 
converted into a digital value. 
-+• Analog output 
LtrQ +t 
Analog pulses 
Figure 11.64 The sample and hold circuit as a filter. 
circuit operates in its sample mode and Vout = - Vin 
(Fig. 11.62(b)); that is, the output follows the input. At the 
same time, the capacitor, C, is charged up to the output volt-
age because its other terminal is at ground potential (the 
inverting terminal of the op-amp is a virtual ground). 
When the diode bridge switch is opened, the output of the 
op-amp is held constant by the charge on the capacitor 
(Fig. 11.62(c)). The charge stored in the capacitor will even-
tually leak away and the output will fall to zero. However, in 
the short term the output remains at the level the input was in 
at the instant the diode bridge switch was opened. 
Figure 11.63 illustrates the timing parameters of a sample 
and hold amplifier. When the diode switch is closed and the 
circuit goes into its sample mode, the capacitor begins to 
charge up to the level of the input. The period in which the 
capacitor is charged is called the acquisition time and is about 
3 /u,s for a low-cost S/H circuit. The output now tracks the input 
up to the maximum slew rate of the S/H circuit. Slew rate defines 
the fastest rate at which the output of a circuit can change. 
When the S/H circuit is switched into its hold mode and the 
diode switch turned off, there's a finite delay during which the 
capacitor is disconnected from the input called the aperture 
Filtered analog output 
The parallel analog-to-digital 
converter 
The parallel A/D converter is called the flash converter 
because of its great speed of conversion when compared with 
the two indirect techniques described later. It works by 
simultaneously comparing the analog input with 2m - 1 
equally spaced reference voltages. Figure 11.65 illustrates a 
3-bit flash A/D converter (real flash ADCs are typically 6- to 
8-bit devices). A chain of 2m equal-valued resistors forms a 
tapped potentiometer between two reference voltages. The 
voltage between consecutive taps in the chain of resistors dif-
fers by 1/2"' of the full-scale analog input. Each of the 2'" - 1 
taps is connected to the inverting input of a high-speed dif-
ferential comparator, whose output depends on the sign of 
the voltage difference between its two inputs. The non-
inverting inputs of the comparators are all wired together and 
connected to the analog input of the ADC. The output of the 
!th comparator in Fig. 11.65 is given by 
sign(V i n- Vref//8). 
For any given analog input voltage, the outputs of the com-
parators, whose reference input is below that of the analog 
input to be converted into digital form, are at a logical 1 level. 
All other outputs are at a logical 0. The seven outputs are fed 
Droop 
i 
jAcquisition 
! 
i 
! Follow; 
;* 
H* 
^ : 
DAC 

4 8 0 
Chapter 11 Computer peripherals 
Analog 
v. 
' nP ut 
The chain of 
resistors creates 
7 reference 
voltages between 
0 and V f. 
R 
The binary output of 
the encoder is the 
3-bit value of the highest 
"nput that is true. 
3-bit latch 
-•Digital output 
Latch output 
Figure 11.65 The flash AID 
converter. 
to a priority encoder that generates a 3-bit output correspond-
ing to the number of logical Is in the input. 
The parallel A/D converter is very fast and can digitize ana-
log signals at over 30 million samples per second. High con-
version rates are required in real-time signal processing in 
applications such as radar data processing and image process-
ing. As an illustration of the speeds involved consider digitiz-
ing a television picture. The total number of samples required 
to digitize a TV signal with 500 pixels/line in real-time is 
samples = pixels per line X lines per field X fields per second 
= 500 X 312'/2 X 50 = 7812500 samples per 
second (UK) 
= 500 X 265 72 X 60 = 7875500 samples per 
second (USA) 
Because the flash converter requires so many comparators, 
it is difficult to produce with greater than about 8-bit preci-
sion. Even 6-bit flash ADCs are relatively expensive. 
The feedback analog-to-digital converter 
The feedback analog-to-digital converter, paradoxically, uses 
a digital-to-analog converter to perform the required conver-
sion. Figure 11.66 illustrates the basic principle behind this 
Analog input 
The control logic uses 
the error signal to generate 
an m-bit digital value 
Control logic 
Error signal 
-*• d^-i m-bit 
- * d, 
d'8 i t a l 
-> d0 
output 
Local digital-
to-analog 
converter 
Figure 11.66 The feedbackADC. 
class of converter. A local digital-to-analog converter trans-
forms an m-bit digital value, D = d„, d,,..., d,„_[, into an 
analog voltage, Vout. The value of the w-bit digital word D is 
determined by the block labeled control logic in one of the 
ways to be described later. 
Vou, from the DAC is applied to the inverting input of an 
operational amplifier and the analog input to be converted is 
applied to its non-inverting input. The output of the opera-
tional amplifier corresponds to an error signal, Ve, and is 
equal to A times (Vom — Vin), where A is the gain of the 
amplifier. This error signal is used by the control logic 
7-line 
, 
to 3-line 
do 
encoder 
<j 
-* 
—— 
< 
^ref 
1 
R 
R 
R 
R 
R 
R 

11.6 The analog interface 
481 
Analog input 
Comparator 
c 
Digital 
output 
(m bits) 
Output 
latches 
Latch 
signt^out-Vy 
Start count 
Stop count I 
m-bit DAC 
m-bit binary 
up counter 
Reset 
Enable 
counter 
Clock 
End of count 
• START 
-+• End of conversion 
Figure 11.67 The ramp feedbackADC. 
START = R 
Q 
(gate clock) 
Clock 
Count clock 
(gated clock) 
Comparator 
output = S 
EOC = Q 
Conversion phase 
Hold result 
Figure 11.68 Timing diagram of a ramp feedbackADC. 
network to modify the digital data, D, to minimize the error 
signal A(Vout - VJ. When the difference between Vin and 
Vout is less than that between two quantized signal levels 
(i.e. Q), the conversion process is complete. 
In plain English, the digital signal is 
varied by trial and error until the 
locally generated analog voltage is as 
close to the analog input as it is possi-
ble to achieve. The next step is to 
examine ways of implementing this 
trial and error process. 
The ramp converter 
The simplest feedback A/D converter 
is the ramp converter of Fig. 11.67, 
which uses a binary counter to gener-
ate the digital output, D. Initially, the 
binary counter is cleared to 0. A new 
conversion process starts with the 
resetting of the RS flip-flop. When Q 
goes high following a reset, the AND 
gate is enabled and clock pulses are 
fed to the m-bit binary up-counter. 
These pulses cause the output of the 
counter, D, to increase monotonically 
from zero (i.e. 0,1,2,..., 2'"'1). 
The output from the counter is 
applied to both an m-bit output latch 
and a D/A converter. As the counter is 
clocked, the output of the local D/A 
converter ramps upwards in the 
manner shown in the timing diagram 
of Fig. 11.68. The locally generated 
analog signal is compared with the 
input to be converted in a digital 
comparator, whose output is the sign 
of the local analog voltage minus the 
input; that is, sign(V0U, - Vin).When 
this value goes positive, the flip-flop 
is set. At the same time, its Q output 
goes low, cutting off the stream of 
clock pulses to the counter and its Q 
output goes high, providing an 
End_of_conversion (EOC) out-
put and latching the contents of the 
binary counter into the output 
latches. 
The ramp feedback A/D converter 
has a variable conversion time. If the 
analog input is close to the maximum 
(i.e. full-scale) value, approximately 2'" clock pulses are 
required before the locally generated analog signal reaches 
the unknown input. The maximum conversion time of an 
8-bit ADC is 256 times the DAC's settling time plus associ-
ated delays in the comparator and counter. The ramp feed-
back converter produces a biased error in its output, because 
the counter stops only when the local DAC output is higher 
Next conversion 
0 
1 
0 
1 
0 
1 
0 
^out 
Vi„ 
0 
1 
0 
R 
S 
Q 
£. 
"out 

482 
Chapter 11 Computer peripherals 
End of 
conversion 3 
Start of 
conversion 1 
Start of 
conversion 2 
Start of 
conversion 3 
Figure 11.69 The ramp converter using an up/down counter. 
Code Input (V)' 
k 
1111 0.9375 
1110 0.8750 
1101 0.8125 
Input 0.64 V 
1100 0.7500 
0.6875 
0.6250 
0.5625 
/ 
/ 
1011 
0.7500 
0.6875 
0.6250 
0.5625 
/ 
/ 
0.7500 
0.6875 
0.6250 
0.5625 
/ 
/ 
I 
: ' : 
1010 
0.7500 
0.6875 
0.6250 
0.5625 
I 
1001 
0.7500 
0.6875 
0.6250 
0.5625 
1000 0.5000 
0.4375 
0.3750 
\ Locally gen 
voltage 
irated 
0111 
0110 
0.5000 
0.4375 
0.3750 
"sPk 
\ Locally gen 
voltage 
irated 
0101 0.3125 
0100 0.2500 
0011 0.1875 
0010 0.1250 
0001 0.0625 
0000 0.0000 
. 
Ik-
Iteration 1 
Iteration 2 
Iteration 3 
Iteration 4 
Figure 11.70 The operation of a successive approximation A/D converter. 
than the input to be converted. This local analog value is not 
necessarily closest to the true digital equivalent of the analog 
input. The advantage of the ramp A/D converter is its sim-
plicity and low hardware cost. 
The tracking converter is a ramp converter with the addi-
tion of a bidirectional (i.e. up/down) counter and slightly 
more complex control logic. At the start of each new conver-
sion process, the comparator determines whether the analog 
input is above or below the feedback voltage from the local 
DAC. If the analog input is greater, the counter is clocked up 
and if it is lower the counter is clocked down. Thus, the 
counter ramps upwards or downwards until the output of the 
comparator changes state, at which point the analog input is 
said to be acquired by the converter. Figure 11.69 demon-
strates the operation of this type of convertor by showing 
how three successive conversions are performed. 
If the analog input is constant, the conversion time of the 
counter is effectively zero once the input has been initially 
acquired. As long as the input changes slowly with respect 
to the rate at which the output of the local DAC can 
ramp upward or downward, the tracking counter faithfully 
converts the analog input into the appropriate digital output. 
If the analog input rapidly changes rapidly, the local analog 
voltage may not be able to track the input and acquisition 
is lost. 
The tracking A/D converter is most useful when the input 
is changing slowly and is highly auto-correlated. Human 
speech represents such a signal. If the converter is subject to 
essentially random inputs (e.g. it is fed 
from a multiplexer), it offers little or no 
advantage over a ramp converter. 
The successive approximation 
converter 
Intuitively, it would seem reasonable to 
take very large steps in increasing the 
analog signal from the local DAC early 
in the conversion process, and then to 
reduce the step size as the conversion 
proceeds and the local analog voltage 
approaches the analog input. Such an 
A/D converter is known as a successive 
approximation A/D converter and uses 
a binary search algorithm to guarantee 
an m-bit conversion in no more than 
m iterations (i.e. clock cycles). 
The 
structure 
of 
a 
successive 
approximation D/A converter is ade-
quately illustrated by the generic feed-
back converter of Fig. 11.67. Only the 
strategy used to generate successive 
steps makes the successive approxima-
tion converter different from a ramp converter. At the start of 
a new conversion process, the digital logic sets the most-sig-
nificant bit (MSB), of the input of the local D/A converter to 
a logical 1 level and all other bits to 0 (i.e. D = 1000 . . . 0). In 
other words, the first guess is equal to one-half the full-scale 
output of the converter. 
If the analog input is greater than half the full-scale output 
from the local D/A converter, the MSB is retained at a logical 
1 level, otherwise it is cleared. On the second iteration, the 
next most significant bit (i.e. dm_2 in an m-bit word) is set to 
a logical 1 and retained at 1 if the output of the D/A converter 
is less than the analog input, or cleared if it is not. This process 
is repeated m times until the LSB of the D/A converter has 
End of 
End of 
conversion 1 
conversion 2 
i 
i r 

11.6 The analog interface 
483 
First cycle 
Second cycle 
Third cycle 
Bit set and tested 
DAC output (V) 
0.5000 
0.7500 
0.6250 
0.6875 
Analog l/P - DAC O/P 
+0.1400 
-0.1100 
+ 0.0150 
-0.0475 
Bit pattern at start of current cycle 
1000 
1100 
1010 
1011 
Bit retained 
Yes 
No 
Yes 
No 
DAC output after iteration 
1000 
1000 
1010 
1010 
Figure 11.71 The decision tree for a successive approximation ADC. 
been set and then retained or cleared. After the LSB has been 
dealt with in this way, the process is at an end and the final 
digital output may be read by the host microprocessor. 
Figure 11.70 illustrates the operation of a 4-bit successive 
approximation A/D converter whose full-scale input is nom-
inally 1.000 V. The analog input to be converted into digital 
form is 0.6400 V. As you can see, a conversion is complete 
after four cycles. 
Figure 11.71 provides another way of looking at the suc-
cessive approximation process described in Fig. 11.70. 
Figure 11.71 takes the form of a decision tree that shows every 
possible sequence of events that can take place when an ana-
log signal is converted into a 4-bit digital value. The path 
taken through the decision tree when 0.6400 V is converted 
into digital form is shown by a heavy line. 
Figure 11.72 illustrates the structure of a 68K-controlled 
successive approximation A/D converter. The microprocessor 
is connected to a memory mapped D/A converter that 
responds only to a write access to the lower byte of the base 
address chosen by the address decoder. The analog output 
Code 
— 1111 
•— 1110 
1101 
—. 1100 
—" 1011 
•k- lioioj 
•— 1001 
"~~ 1000 
— 
0111 
•— 0110 
—- 0101 
"~~ 0100 
~~~ 0011 
~"~ 0010 
-~ oooi 
-"• 0000 
t 
Fourth cycle 
_^--mi 
< 
""~"-~- 1101 < : 
^ypioii|' < 
1001 " ^ 
^-— om 
< 
"""^0101 
< 
^^,0011 
< 
~ \ 0 0 0 1 «_-
t 
/ 1110 
< ^ 
/0110 < C ^ 
^ 
0010 < C ^ 
J33 
^ 0100< 
|iooo/ 
t \ 
Start 
1
2 
3 
4 
2 
101 

484 
Chapter 11 Computer peripherals 
of the converter is compared with the unknown analog input 
in a comparator, whose output is gated onto data line Di5) 
whenever a read access is made to the upper byte of the base 
address. The software to operate the A/D converter of 
Fig. 11.72 is 
The integrating analog-to-digital converter 
The integrating, or more specifically, the dual-slope integrat-
ing analog-to-digital converter, transforms the problem of 
measuring an analog voltage into the more tractable problem 
of measuring another analog quantity—time. An integrating 
operational amplifier circuit converts the analog input into a 
charge stored on a capacitor, and then evaluates the charge by 
measuring the time it takes to discharge the capacitor. The 
block diagram of a dual-slope integrating A/D converter is 
given in Fig. 11.73 and its timing diagram in Fig. 11.74. 
A typical integrating converter operates in three phases: 
auto-zero, integrate the unknown analog signal, and integrate 
the reference voltage. The first phase, auto-zero, is a feature of 
many commercial dual-slope converters, which reduces any 
offset error in the system. As it isn't a basic feature of the dual-
slope process, we won't deal with it here. During the second 
phase of the conversion, the unknown analog input linearly 
charges the integrating capacitor C. In this phase, the input of 
the electronic switch connects the integrator to the voltage to 
be converted, Vin. 
Figure 11.74 shows how the output from the integrator, 
Vou„ ramps upward linearly during phase 2 of the conversion 
process. At the start of phase 2, a counter is triggered that 
counts upwards from 0 to its maximum value 2" — I. After a 
fixed period T, = 2"/fc where fc is the frequency of the 
converter's clock, the counter overflows (i.e. passes its maxi-
mum count). The electronic switch connected to the integra-
tor then connects the integrator's input to — Vref, the negative 
reference supply. The output of the integrator now ramps 
downwards to 0, while the counter runs up from 0. 
Eventually, the output of the integrator reaches zero and the 
conversion process stops—we'll assume that the counter con-
tains M at the end of this phase. 
Readers without a knowledge of basic electronics may skip 
the following analysis of the dual slope integrating ADC. 
At the end of phase 2 the capacitor is charged up to a level 
The voltage rise during the second phase is equal to the fall 
in the third phase because the output of the integrator begins 
at zero volts and ends up at zero volts. Therefore, the follow-
ing equation holds: 
Successive_approximation 
DAC_output = 0 
Increment = H full-scale output {100... 
FOR I = 1 TO Number_of_bits 
DAC_output = DAC_output + Increment 
Error_sign = sign(Vin — DAC_output) 
IF Error_sign negative THEN 
DAC^output = 
ENDIF 
Increment := Increment/2 
ENDFOR 
End successive_approximation 
00) 
DAC output — Increment 
DO contains the increment 
Dl is the DAC output 
D2 is the cycle counter 
ORG 
$0OFO00 
DAC_IN DS.B 
1 
DAC_OT DS.B 
1 
ORG 
$001000 
CONV 
MOVE.B #$80,DO 
MOVE.B D0,D1 
MOVE.W #7,D2 
AGAIN MOVE.B D1,DAC OT 
BTST 
#7,DAC_IN 
BPL 
NEXT 
SUB.B D0,D1 
NEXT 
LSR.B #1,D0 
ADD.B D0,D1 
DBRA 
RTS 
D2,AGAIN 
Base address of DAC 
Reserve byte for sign input from DAC 
Reserve byte for output to DAC 
Program origin 
Set the half-scale increment 
Setup initial value for the output 
We are going to do 8 cycles 
Transmit output to DAC 
Examine output from comparator 
IF positive THEN add next increment 
ELSE remove the increment 
Increment := increment/2 
Add increment to output 
Repeat for 8 cycles END 
End of conversion 
^!v'»dt 
^ / , V ™d f 
= 
^ / ,
V -
d
f 

11.6 The analog interface 
4 8 5 
Analog input 
doo-do 
68K 
CPU 
RAV 
LDS 
UDS 
AS 
Aoi-A a 
d15 
DTACK 
^ Data 
V°M 
Memory-mapped 
digital-to-analog 
converter 
CS 
jv, Address 
• decoder 
t>n 
OiSVin>Vm 
Write 
Sign(Vin-V0l 
Read 
<r-
Memory map 
16 bits 
don't care  
^i^4 
dpaldp/ 
dp. 
MSB 
LSB 
read-only 
write-only 
Figure 11.72 The circuit of a successive approximation A/D converter. 
Integrator 
Analog 
m P
u t 
Electronic switch 
Kn 
Vre,^ 
gnd-
-O, 
^
» 
W W - " 
4h 
Switch 
control 
Comparator 
START-
EOC •< 
(end of conversion) 
Control logic 
vcomp 
JUL 
Clock 
Counter 
Assuming 
that 
t} = 0, f2 = 27/c, 
f3 = 12 + M/fc, we can write 
1 
CR 
vint 
rij, 
o 
1 
CR 
r 
i 
vreft 
rif, 
+ M/fc 
nu 
or 
V,n2" 
VlefM 
u 
V-. 
u 
2" 
This remarkable result is depen-
dent only on the reference voltage and 
two integers, 2" and M. The values of 
Cand R and the clock frequency,/c, do 
not appear in the equation. Implicit in 
the equation is the condition that/, is 
constant throughout the conversion 
process. Fortunately, this is a reason-
able assumption even for the simplest 
of clock generators. 
The dual-slope integrating A/D con-
verter is popular because of its very low 
cost and inherent simplicity. Moreover, it 
is exceedingly accurate and can provide 
12 or more bits of precision at a cost 
below that of 8-bit ADCs. Because this 
converter requires no absolute reference 
other than V,^ it is easy to fabricate the 
entire device in a single integrated circuit. 
The conversion time is variable and 
takes 2" + M clock periods in total. 
A 12-bit converter witha 1 p.s clock has 
a maximum 
conversion 
time of 
2 X 27/c seconds, because the maxi-
mum value of N is 2". Using these fig-
ures, the maximum conversion time is 
equal to 2 X 4096 X 1 u.s, or 8.192 ms, 
which is very much slower dian most 
forms of feedback A/D converter. 
Because the analog input is inte-
grated over a period of 27/c seconds, 
noise on the input is attenuated. 
Sinusoidal input signals, whose peri-
ods are submultiples of the integra-
tion period, do not affect the output 
of the integrator and hence the mea-
sured value of the input. Many high-
precision 
converters 
exploit 
this 
property to remove any noise at the 
power line frequency. Integrating 
converters are largely used in instru-
mentation such as digital voltmeters. 
Digital output 
Figure 11.73 The integrating A/D converter. 

486 
Chapter 11 Computer peripherals 
START 
Phase 2 
integrate input 
Phase 3 
j 
ntegrate reference : 
Figure 11.74 Timing diagram of an integrating A/D converter. 
Direction of C 
rotation 
C = centripetal force 
required to pull the 
counterweight 
inward. 
G =force of gravity 
pulling the counter-
ed 
weight down. 
Figure 11.75 The mechanical governor. 
Now that we've described how analog signals can be cap-
tured, processed, and then used to generate an analog output, 
we provide in insight into some of the things a computer can 
do with analog signals. 
11.7 Introduction to digital 
signal processing 
Digital signal processing (DSP) forms an entire branch of 
electronics covering electronic circuits, mathematics, and 
computer science. Here we explain why DSP is so important 
by looking at just two areas: control systems and audio signal 
processing. We set the scene by describing an early mechani-
cal analog control system before looking at the principles of 
digital control systems. The final part of this section describes 
DSP that is used in control systems and sound and video pro-
cessing systems. 
Control systems have been used for a long time; for exam-
ple, the governor used to keep the speed of stream engines 
constant during the nineteenth cen-
tury. Figure 11.75 shows the shaft of a 
steam engine driving a vertical spin-
dle. Two arms connected to the spin-
dle by pivots carry counterweights at 
the ends of the arms. The arms are 
pivoted and are free to swing outward 
as the spindle rotates. 
As the spindle rotates, the counter-
weights move outward. In everyday 
life people use the term centrifugal 
force to describe the tendency of a 
body following a curved path to fly 
outward. Centrifugal force doesn't 
exist. Any moving body tends to con-
tinue in a straight line. In order to 
force a body to follow a curved path 
(e.g. an orbit), a force is necessary to 
pull it toward the center. This force is 
called centripetal force. 
In Fig. 11.75 the force of gravity on the counterweights pro-
vides 
the centripetal 
force 
that 
pulls the 
counter-
weights inward. This situation is analogous to camber in a road 
bend—tilting the car inward provides the centripetal force 
required to pull the car round the bend without skidding. 
The position of the counterweights in Fig. 11.75 depends 
on the speed at which the spindle rotates. As the arms con-
nected to the counterweights move in and out, they control a 
valve that regulates the flow of stream to the engine. Below a 
certain speed, the valve is open and more steam is fed to the 
engine to cause it to speed up. As the spindle rotates faster, the 
counterweights fly further out until the valve begins to close 
and the flow of steam is reduced. Eventually equilibrium is 
reached and the spindle rotates at a constant speed. 
This control mechanism employs negative feedback, 
because an increase in the speed is used to decrease the flow of 
steam and hence the engine's speed. Similar mechanisms were 
used to provide aircraft with autopilots long before the age of 
the microprocessor. Today, the digital computer has replaced 
the governor. The speed of a spindle can be read with great 
precision and fed to a computer. The computer processes the 
speed according to a suitable algorithm and generates the con-
trol signals that determine the spindle's speed. 
Modern digital control systems are everywhere; for exam-
ple, an automobile measures the external air pressure, the 
manifold pressure, the external air temperature, the speed of 
the engine, and the position of the gas pedal to determines 
the optimum amount of fuel to inject into each cylinder. 
11.7.1 Control systems 
Analog-to-digital and digital-to-analog conversion tech-
niques are found in process control applications. Consider 
Phase 1 
auto-zero 
0 
Vint 
t 
I 
h 
1*2 
1*3 
I 
h 
ifi 

11.7 Introduction to digital signal processing 
487 
the automatic pilot of an aircraft. At any instant the location 
and altitude of an aircraft is measured, together with its per-
formance (heading, speed, rate of climb, rate of turn, and 
engine power). All these values are converted into digital 
form and fed into a computer that determines the best posi-
tion for the throttle, elevator, aileron, and rudder controls. 
The digital output from the computer is applied to digital-to-
analog converters, whose analog outputs operate actuators 
that directly move the appropriate control surfaces. 
Figure 11.76 describes a primitive control system. The 
input is an analog value that is digitized and processed by the 
computer. Real control systems are often much more sophis-
ticated than that of Fig. 11.76—consider the problem of over-
shoot. Suppose you apply a new demand input to a system 
such as banking an aircraft's wings. The aircraft rolls into the 
bank and attempts to attain the angle requested. However, the 
mechanical inertia of the aircraft might cause it to roll past 
(i.e. overshoot) the point it was aiming for. A practical con-
trol system should also take account of rapidly changing con-
ditions. 
Let's look at how control systems have evolved from the sim-
plest possible mechanisms. The crudest control mechanism is 
found in central heating systems where the desired tempera-
ture or setpoint is obtained from a control unit on the wall. The 
demand input is compared with the actual temperature 
measured by a sensor. If it is colder than the setpoint, the heater 
is turned on. Otherwise the heater is turned off. 
Compound _ 
input 
• Output 
Figure 11.76 The control system. 
Sensor 
Comparator 
Sensor 
Comparator 
Comparator 
Heater 
Comparator 
Heater 
Control 
Comparator 
Control 
Comparator 
Temperature 
Set point 
-•time 
Heater 
On 
Off 
-•time 
Figure 11.77 The on-off control system. 
Figure 11.77 demonstrates the operation of such a system. 
The temperature of the room rises and eventually the heater 
is turned off. Because of the heater's thermal inertia the 
room, the temperature will continue to rise after the current 
has been cut off. Eventually, the room begins to cool and the 
heater is turned on and the temperature starts rising again. 
This type of on-off control system is also called a bang-bang 
control system to indicate its crude approach—bang the sys-
tem goes on and bang it goes off. There is no intermediate 
point between on and off, and the room is never at the correct 
temperature because it's either slightly too hot or too cold. 
A better method of controlling the temperature of a room 
is to measure the difference between the desired temperature 
and the actual temperature and use this value to determine 
how much power is to be fed to the heater. The colder the 
room, the more power sent to the heater. If the room is close 
to its desired temperature, less power if fed to the heater. This 
is an example of a proportional control system. As the room 
temperature approaches its desired setpoint value, the power 
fed to the heater is progressively reduced; that is, the current 
supplied to the heater is JC(fsetpoint - froom). 
The proportional control system can be improved further 
by taking into account changes in the variable you are trying to 
control. Suppose you're designing a camera with an automatic 
focusing mechanism for use at sporting events. The camera 
measures the distance of the subject from the camera using the 
difference between the current point of focus and the desired 
point of focus to drive the motor that performs the focusing. 
Suppose the subject suddenly changes direction, speeds up, 
or slows down. A proportional control system can't deal with 
this situation well. If the subject is in focus and then begins 
accelerating away, a proportional control signal can't apply a 
large correction until the target is out of focus. What we need 
is a control signal that doesn't depend on the magnitude of the 
error but on the rate at which the error is changing. 
A differential control system uses the rate of change of the 
error as a control signal; for example, a camera with auto-
focusing can use any rapid change in the subject's position to 
control the focusing motor—even if the subject is approxi-
mately in focus and there's no proportional error. A differen-
tial control system must also incorporate proportional 
control because if the subject were out of focus but not mov-
ing there would be no differential feedback signal. 
If we call the error between the setpoint in a control system 
and its output e, the control input in a proportional plus deriv-
ative (i.e. differential) control system is given by 
y = Kte + K2deldt, 
where K, and K2 are the proportional and derivative control 
coefficients, respectively. 
Even this control algorithm isn't perfect. Suppose you 
design a radar-controlled docking system for two spacecraft. 
One craft can track the other by using both proportional 
Feedback path 
N System H 
Error 
\ signal 

FOR i = 1 TO k DO 
O u t p u t = Mi 
ENDFOR 
Figure 11.78 The derivative and integral control system. 
control and derivative control to minimize the difference 
between their trajectories. However, once their trajectories 
are closely (but not exactly) matched, there is neither a pro-
portional error signal nor a derivative error signal to force 
exact tracking. What we need is a mechanism that takes 
account of a persistent small error. 
An integral control system adds up the error signal over a 
period of time. The integral correction term is K3jedt Even 
the smallest error eventually generates a control signal to fur-
ther reduce the error. Integral control ensures that any drift 
over time is corrected. 
A high-performance controller might combine propor-
tional control, rate-of-change control, and integral control as 
Fig. 11.78 demonstrates. This system is called a PID (propor-
tional, integral, and derivative) controller. In Fig. 11.78 the 
box marked differentiator calculates the rate of change of the 
system output being controlled. 
The equation for a PID can be expressed in the form 
y = Kte + K2de/dt + KJedt 
The control signal y now depends and the size of the error 
between the desired and actual outputs from the controller, 
the rate at which the error is changing, and the accumulated 
error over a period. 
We can't go into control theory here but we should men-
tion several important points. Designing a PID system is not 
easy. You have to choose the amounts of proportional, deriv-
ative, and integral feedback as well as the time constant of the 
integrator. If the system is not correctly designed it can 
become unstable and oscillate. 
In the final part of this section we look at how digital sig-
nals are processed by the computer. 
11.7.2 Digital signal processing 
Let's begin with a simple example of signal processing. 
Suppose music from a microphone is quantized, converted 
into a sequence of digital values by an ADC, fed into a com-
puter, and stored in an array, M. We can read consecutive 
digital values from the array and use a DAC to convert them 
into an analog signal that is fed to a loudspeaker. Consider the 
following algorithm. 
The digitally stored music is reconverted 
into analog form by sending it to the output 
port connected to a DAC. This algorithm 
does nothing other than retrieve the stored 
music. In the next example, the samples 
from the array are amplified by a scalar fac-
tor A. By changing the value of A, the amplitude (i.e. the 
loudness) of the music can be altered. Now we have a digital 
volume control with no moving parts that can be pro-
grammed to change the sound level at any desired rate. 
FOR i = 1 TO k DO 
Output = A * M± 
ENDFOR 
We can average consecutive samples to calculate the loud-
ness of the signal and use it to choose a value for A. The fol-
lowing expression shows how we might average the loudness 
over a period of k samples. 
Loudness = 
Suppose we choose the scale factor A to make the average 
power of the signal approximately constant. When the music 
is soft, is the volume is increased, and when it is loud, the vol-
ume is decreased. This process is called compressing the music 
and is particularly useful for listeners with impaired hearing 
who cannot hear soft passages without turning the volume 
up so far that loud passages are distorted. 
In the next example, the signal fed to the loudspeaker is 
composed of two parts. M{ represents the current value, 
and B-Mi-j the value of the signal; samples earlier, scaled 
by a factor B. Normally the factor B is less than unity. 
Where do we get a signal plus a delayed, attenuated value? 
These features are found in an echo and are of interest to 
the makers of electronic music. By very simple processing, 
we are able to generate echoes entirely by digital tech-
niques. Analog signal processing requires complex and 
inflexible techniques. Synthesizing an echo by analog tech-
niques requires you to first convert the sound into vibra-
tion by a transducer. A spring is connected to the 
transducer and the acoustic signal travels down it to a 
microphone at the other end. The output of the micro-
phone represents a delayed version of the original signal— 
the echo. The length of the delay is increased by using a 
longer spring. In the digital version, simply modifying the 
value of; changes the delay. 
FOR i = i+1 TO k DO 
Output= Mi + B * M ^ 
ENDFOR 
488 
Chapter 11 Computer peripherals 
Feedback  
Command 
/ Z \ 
/CT\ 
J~. 
7Z~] J""~T~ 
: 
Output 
input 
* \ Z n 
* W ~ • j A m p l i f i e i f - - * 
Systemj-< •—»• 
| 
1 Feedback 
*—P Integrator —' 
Differentiator < 
' 
Feedback 
I 
1 f^~2 

11.7 Introduction to digital signal processing 
489 
The final example of signal processing represents the linear 
transversal equalizer that implements a general-purpose dig-
ital filter. In audio terms, a digital filter acts as tone controls or 
an equalizer. We are going to look at this topic in a little more 
detail next. 
FOR i = 1 TO k DO 
a = K4 
b = K3 
M 
'i-3 
c = K2 * Mi.2 
d = Kl * Mi.! 
e = KO 
Output 
M, 
+ b + c + d + 
ENDFOR 
The output is a fraction of the current sample plus 
weighted fractions of the previous four samples. Let's look at 
this operation in a little more detail. 
Digital filters 
An important application of digital signal processing is the 
digital filter. A digital filter behaves like an analog filter—it 
can pass or stop signals whose frequencies fall within certain 
ranges. Consider an analog signal, X, that has been digitized 
and its successive values are 
*0> x
x, *2> -^3> • • • > * i - l > Xp X,- + 1,.. . 
Now suppose we generate a new sequence of digital values, 
Y, whose values are y0, yx, y2>..., 
where, 
Y, = Or*. + C\-*>-\ 
An element in the output series, y„ is given by a fraction of 
the current element from the input series C0-x; plus a fraction 
of the previous element Q •*,-_, of the input series. 
Figure 11.79 illustrates this operation. The symbol Z~l is 
used to indicate a 1-unit delay (i.e. the time between two suc-
cessive samples of a signal). In other words the operation 
x,Z~' is equivalent to delaying signal x, by one time unit— 
similarly Z~2 delays x; by two time units. This notation 
belongs to a branch of mathematics called Z transforms. 
. Delay element 
Input sequence 
Xj 
o-Wj-l)—O 
• 
, . 
,X. M 
• Coefficient 
C
co) 
© 
-*\ 2 ) 
W Output sequence 
Summer 
Let's give see what happens when we give the filter coeffi-
cients C0 the value 0.6 and Cj the value 0.4, and make the 
input series X = 0, 0, 1, 1, 1,1,... 1, which corresponds to a 
step function. The output sequence is given by 
y0 = 0.6 • % + 0.4 • x_i = 0.6 • 0 + 0.4 • 0.0 = 0.0 
= 0.6 • 0 + 0.4 • 0.0 = 0.0 
= 0.6 • 1 + 0.4 • 0.0 = 0.6 
y, = 0.6 • xx + 0.4 • 
y2 = 0.6 • x2 + 0.4 • 
y3 = 0.6-x3 + 0.4-x2 =0.6-1 +0.4-1.0= 1.0 
y4 = 0.6-x4 + 0.4-x3 =0.6-1 +0.4-1.0= 1.0 
The output sequence is a rounded or smoothed step func-
tion (i.e. when the input goes from 0 to 1 in one step, the out-
put goes 0.0,0.6,1.0). This type of circuit is called a low-pass 
filter because sudden changes in the input sequence are 
diminished by averaging consecutive values. Real digital fil-
ters have many more delays and coefficients. Consider the 
output of a filter with four delay units given by 
y, = CQ-X, + Cj-X,_] + C2-X;„2 ' ^3'*i-3 "•" ^4"X,-4 
If we use this filter with coefficients 0.4, 0.3, 0.2, 0.1 and 
subject it to a step input, we get 
y0 = 0.4-xo + 0.3 •*_, + 0.2-x_2 + 0.1-x^3 
= 0.4 • 0 + 0.3 • 0 + 0.2 • 0 + 0.1 - 0 = 0.0 
yx = 0.4-X! + 0.3-Xo + 0.2 • x_x + 0.1-x_2 
= 0.4 • 1 + 0.3 • 0 + 0.2 • 0 + 0.1 • 0 = 0.4 
y2 = 0.4 • x2 + 0.3 • xx + 0.2 • x„ + 0.1 • x_, 
= 0.4 • 1 + 0.3 • 1 + 0.2 • 0 + 0.1 • 0 = 0.7 
y, = 0.4 • x3 + 0.3 • x2 + 0.2 • x, + 0.1 • x„ 
= 0.4 • 1 + 0.3 • 1 + 0.2 • 1 + 0.1 • 0 = 0.9 
y4 = 0.4 • x4 + 0.3 • x3 + 0.2 • x2 + 0.1 • x, 
= 0.4 • 1 + 0.3 • 1 + 0.2 • 1 + 0.1 • 1 = 1.0 
In this case, the output is even more rounded (i.e. 0.0,0.4, 
0.7,0.9,1.0). 
A more interesting type of filter is called a recursive filter 
because the output is expressed as a fraction of the current 
input and a fraction of the previous output. In this case, the 
output sequence for a recursive filter with a single delay unit 
is given by 
Yi = Q-Xi + Q-y,-.!. 
Figure 11.80 shows the structure of a recursive filter. 
Suppose we apply the same step function to this filter that we 
used in the previous examples. The output sequence is given by 
y0 = 0.6 • Xo + 0.4 -y_, 
y0 = 0.6 • 0 + 0.4 • 0 = 0.0 
Figure 11.79 The digital filter. 
y, = 0.6-x, + 0.4-y0 
y2 = 0.6-x2 + 0.4-y, 
y3 = 0.6-x3 + 0.4-y2 
y4 = 0.6 • x4 + 0.4 • y3 
y5 = 0.6-x5 + 0.4-y4 
y6 = 0.6-xt + 0A-yi 
y7 = 0.6-X7 + 0.4-y6 
yx = 0.6 • 0 + 0.4 • 0 = 0.0 
y2 = 0.6 • 1 + 0.4 • 0 = 0.6 
y3 = 0.6 • 1 + 0.4 • 0.6 = 0.84 
y4 = 0.6 • 1 + 0.4 • 0.84 = 0.936 
y5 = 0.6 • 1 + 0.4 • 0.936 = 0.9744 
y6 = 0.6 • 1 + 0.4 • 0.9744 = 0.98976 
y7 = 0.6 • 1 + 0.4 • 0.98976 = 0.995904 

490 
Chapter 11 Computer peripherals 
Input sequence 
*> 
* & 
Output sequence 
y< 
C0x,+ C\y-,_ 
Figure 11.80 The recursive digital filter. 
Figure 11.81 plots the input and output series for the 
recursive filter of Fig. 11.80. As you can see, the output series 
(i.e. they;) rises exponentially to 1. The effect of the operation 
C0 • x, + Q • /i _ j on a digital sequence is the same as that of 
a low-pass analog filter on a step signal. You can see that the 
recursive digital filter is more powerful than a linear digital 
filter. By changing the constants in the digital equation we can 
change the characteristics of the digital filter. Digital filters 
are used to process analog signals and to remove noise. 
The opposite of a low-pass filter is a high-pass filter, which 
passes rapid changes in the input sequence and rejects slow 
changes (or a constant level). Consider the recursive digital 
filter defined by 
y, = Co-Xi - c, •/,-_,. 
All we have done is change the sign of the constant Q and 
subtracted a fraction of the old output from a fraction of the 
new input. In this case, a constant or slowly changing signal is 
subtracted from the output. Consider the previous example 
with a step input and coefficients C0 = 0.6 and C, = 0.4: 
y0 = 0.6-x0-0.4-y_, 
y, = 0.6-xI-0.4-y0 
y1 = 0.6-x2-0.4-yi 
y3 = 0.6-x3-0.4-y2 
y4 = 0.6-x4~0.4-y3 
y5 = 0.6 • X5-O.4 • y4 
y6 = 0.6- ^-0.4-ys 
y7 = 0.6-x7-0.4-y6 
y0 = 0.6-0-0.4-0 = 0.0 
y, = 0.6-0-0.4-0 = 0.0 
y2 = 0.6-1 -0.4 • 0 = 0.60 
y3 = 0.6-1 -0.4 • 0.6 = 0.36 
y4 = 0.6-1-0.4-0.36 = 0.4176 
y5 = 0.6-1 -0.4 • 0.4176 = 0.43296 
y6 = 0.6-1 -0.4 • 0.43296 = 0.426816 
y7 = 0.6-1 -0.4 • 0.426816 = 0.42927 
1.0 
0.9 
0.8 
0.7 
0.6 
0.5 
0.4 
0.3 
0.2 
0.1 
In this case the step function dies away as Fig. 11.82 
demonstrates. 
Correlation 
One of the most important applications of digital signal 
processing is the recovery of very weak signals that have 
been corrupted by noise. Signals received from satellites and 
deep space vehicles are often so weak that there is 
considerably more noise than signal—anyone listening to 
such a signal on a loudspeaker would hear nothing 
more than the hiss of white noise. Modern signal processing 
techniques enable you to extract signals from noise when 
the signal level is thousands of times weaker than the noise. 
\ 
nput ,,«"" 
Output 
3 
4 
5 
time 
6 
7 
Figure 11.81 Response of the filter of Fig. 11.80 to a step input. 
Output 
1.0 A 
Figure 11.82 Response of a high-pass filter to a step input. 
The technique used to recover signals from noise is called 
correlation. We met correlation when we discussed the wave-
forms used to record data on disks—the more unalike the 
waveforms used to record Is and 0s, the better. Correlation is 
a measure of how similar two waveforms or binary sequences 
are. Correlation varies from - 1 to 0 to +1. If the correlation 
is + 1, the signals are identical. If the correlation is 0, the two 
signals are unrelated. If the correlation is - 1 , one signal is the 
inverse of the other. 
Two signals can be correlated by taking successive samples 
from each of the series, multiplying the pairs of samples, and 
then averaging the sum of the product. Consider now the cor-
relation function of two series X = x0, xv x2, x}, x4 and Y = 
y^yvy^y^y^ 
The correlation between X and Y is given by 1/5 (x„ • y0, + 
Xi-yi, + 
x2-y2,+xi-yi). 
An example of the use of correlation is the effect of rain-
fall in the mountains on crop growth in the plain. Simply 
correlating the sequence of rainfall measurements with 
crop growth doesn't help because there's a delay between 
rainfall and plant growth. We can generate several correla-
tion functions by correlating one sequence with a 
Input 
j 
Output 
1
2 
3 
4 
5 
6 
7 
time 
0.9 
0.8 
0.7 
0.6 
0.5 
0.4 
0.3 
0.2 
0.1 
0 
e 
si 
0 
1 
2 

11.7 Introduction to digital signal processing 491 
THE KALMAN FILTER 
The Kalman filter, which was introduced in the early 1960s, 
provides a spectacular application of digital filtering. This filter 
was proposed by Rudolf Emil Kalman who was born in 
Budapest but emigrated to the USA from Hungary during the 
Second World War. A Kalman filter takes a time-varying signal 
corrupted by noise and predicts the future value of the signal; 
that is, it can eliminate some of the effects of noise. Kalman 
filters have been applied to a wide range of systems from 
space vehicles to medical systems. 
The mathematics of Kalman filters belongs in advanced 
courses in control theory. All we can do here is mention some 
of the underlying concepts. A dynamic system that varies with 
time can be described by state variables. For example, you can 
write x,+ 1 = ax,, where x, + 1 represents the state of the 
system at time / + 1, x, represents the system at time /, and a 
characterizes the behavior of the system. In practice, the state 
equation is given by x, + 1 = ax,, + w,, where w, represents a 
random noise component. 
The Kalman filter lets you predict (i.e. make a best guess) 
the next state of the system when the system is affected by 
random noise and the measurements themselves are also 
affected by noise. Suppose you design an aircraft's autopilot 
to enable it to follow the ground from a very low level. The 
height of the aircraft about the ground is measured by radar 
techniques. However, the successive readings from the radar 
altimeter are corrupted by random noise, which means that 
any particular reading can't be relied on. Furthermore, succes-
sive altimeter readings can't just be averaged because the ter-
rain itself is undulating. 
If the /th estimate of the aircraft's height is A),, the Kalman 
filter evaluates: 
h, = a;/?,_, + (1 - ajx. 
The underscore under h; and /i, - 1 indicates that these 
are estimated values. The current value of h is obtained 
from the previous estimate, h,— 1, plus the new data, xh 
The coefficient of the filter, a, is a function of r, that is, the 
coefficient varies with time. The recursive nature of the 
Kalman filter means that trends in the input are taken into 
account. 
delayed version of the other sequence. Now we have a 
sequence of correlation functions that depend on the 
delay between the sequences, and we can express the kth 
correlation value as 
Q — Zixi /,+* 
Suppose that X= 1,2,3, -1,4,2,0,1 and Y = 0,0,1, - 1 , 
0,1,1,0,0,0: 
C0 =1-0 + 2- 0+3-1 -I—1 — 1 +4-0 + 2-1 +0-1 + 1-0 = 6 
C, = 1-0 + 2-1 + 3-1 + -1-0 + 4-1 + 2-1 +0-0+ 1-0 = 5 
C2 = 1-1 +2--1 +3-0+ 1-1 +4-1 + 2-0 + 0-0+ 1-0 = 4 
These results don't seem very interesting until we apply this 
technique to a real situation. Suppose a transmitter uses the 
sequence 0.25, -0.5, 1.0, -0.5, 0.25 to represent a logical 1; 
that is a 1 is transmitted as the sequence of values 0.25, —0.5, 
1.0, —0.5, 0.25. Suppose we receive this signal without noise 
and correlate it with the sequence representing a 1. That is, 
C0 = 0.25 • 0.25 + -0.5 • -0.50 +1-1.0 
+ -0.5 • -0.5 + 0.25 • 0.25 
= 1.625 
C, = 0.25-0.00+ -0.5-0.25 
+ 1 • -0.5 + -0.5 • 1.0 + 0.25 • -0.5 + 0.0 -0.25 
= -1.25 
C2 = 0.25 • 0.00 + -0.5 • 0.00 + 1 • 0.25 + -0.5 • -0.5 
+ 0.25 • 1.0 + 0.0 • -0.5 + 0.0 • 0.25 
= 0.75 
As you can see, the greatest correlation factor occurs 
when the sequence is correlated with itself. If the sequence 
is corrupted by. samples of random noise, the noise is not 
correlated with the sequence and the correlation function 
is low. Noisy data from, say a satellite, is correlated with the 
same sequence used to generate the data. This operation is 
performed by correlating the incoming data with the 
appropriate sequence and varying the delay k. A sequence 
of correlation values are recorded and compared with a 
threshold value. If the correlation is above the threshold, it 
is assumed that the sequence is present in the received 
signal. 
Here we have done little more than mention a few examples 
of digital signal processing. The techniques we have described 
can be used in both the audio and visual domains. Processing 
the signals that represent images allows us to, for example, 
sharpen blurred images or to remove noise from them, or to 
emphasize their edges. 
• 
SUMMARY 
In this chapter we've looked at some of the many 
peripherals that can be connected to a computer. We began 
with the two most important peripherals from the point of 
view of the average PC user, the input device (keyboard and 
mouse) and the output device (CRT, LCD, and plasma display). 
We have looked at the construction of input/output devices 
and have described how printers work. In particular, we have 
demonstrated how computers handle color displays. 
Some devices receive or generate analog signals. We have 
examined how analog signals from sensors are processed by the 
computer. We have provided a brief discussion of how analog 
signals can be converted into digital form and vice versa and the 
problems of sampling a time-varying signal. 

492 
Chapter 11 Computer peripherals 
We have also briefly introduced some of the devices that 
enable us to control the World around us: temperature, pressure, 
and even rotation sensors. 
PROBLEMS 
11.1 Why are mechanical switches unreliable? 
11.2 Imagine that keyboards did not exist (i.e. you are free from 
all conventional design and layout constraints) and you were 
asked to design a keyboard for today's computers. Describe the 
layout and functionality of your keyboard. 
11.3 Why are optical mice so much better than mechanical 
mice? 
11.4 A visual display has a resolution of 1600 X 1200 pixels. If 
the display is updated at 60 frames a second, what is the aver-
age rate at which a pixel must be read from memory? 
11.5 Most displays are two-dimensional. How do you think 
three-dimensional displays can be constructed? Use the 
Internet to carry out your research. 
11.6 How do dot printers (for example, the inkjet printer) 
increase the quality of an image without increasing the number 
of dots? 
11.7 What is the difference between additive and subtractive 
color models? 
11.8 Use the Internet or a current computer magazine to 
calculate the ratio of the cost of a 17-inch monitor to a basic 
color printer. What was the value of this ratio 12 months ago? 
11.9 Why does an interlaced CRT monitor perform so badly 
when used as a computer monitor? 
11.10 Describe, with the aid of diagrams, how an integrating 
analog-to-digital converter operates. Explain also why the 
accuracy of an integrating converter depends only on the 
reference voltage and the clock. 
11.11 What is a tree network (when applied to DACs) and what 
is its advantage over other types of DAC (e.g. the R-2R ladder). 
11.12 A triangular-wave generator produces a signal with a 
peak-to-peak amplitude of 5 V and a period of 200 (jis.This 
analog signal is applied to the input of a 14-bit A to D converter. 
(a) What is the signal-to-noise ratio that can be achieved by 
the converter? 
(b) What is the minimum input change that can be reliably 
resolved? 
(c) For this signal explain how you would go about calculating 
the minimum rate at which it must be sampled. 
11.13 What is a sample and hold circuit and how is it used in 
ADC systems? 
11.14 One of the most important applications of microproces-
sors in everyday systems is the controller. Describe the structure 
of a three-term PID (proportional, integral, derivative) control 
system and explain how, for example, it can be used in tracking 
systems. 
11.15 Find further examples of the use of digital signal 
processing. 
11.16 What is the meaning of XQ, XV X2, X3, Xp • • •. xu..., xn in 
the context of digital filter. 
11.17 A digital filter is defined by the equation yn = 0.2xn 
+ 0.1 • * „ _ ! +0.4-y„_ 1 + 0.3 • yn _2 where yn is the nth 
output and xn is the nth input. 
(a) What is the meaning of this equation in plain English? 
(b) What is the difference between yn and y„_-|? 
(c) How is the above equation represented diagrammatically? 
(d) Does the above equation represent a recursive filter? 
(e) Describe the circuit elements in the diagram. 
(f) If the input sequence XQ, XV XZ, X3, x4,... is 0.0,0.1,0.2,0.3, 
0.4,..., what is the output sequence? 
(g) What does this filter do? 
11.18 A recursive digital filter is described by the expression 
yn = c0-xn + Ci-x^i 
+ c2-y„_, 
where the output of the filter is the sequence yo,yi,y2/ • • •. 
yn - v Yn ar|dthe 'nPut: is the sequence XQ, XV ..., x„ _,, xn. The 
terms Q,, q, and c2 are filter coefficients with the values 
Co = 0.4, c, = 0.1, and c2 = 0.3. 
(a) What is the meaning of a recursive filter? 
(b) Draw a block diagram of the structure of this filter (note 
that the delay element is represented by Z~n). 
(c) Draw a graph of the output sequence from this filter 
corresponding to the input sequence given by the step 
f u n c t i o n e d , 1,1,1,... 1. 
(d) In plain English, what is the effect of this filter on the step 
input? 
11.19 Why is speech not used more widely as a form of 
computer input? 
11.20 Suppose that speech were used as a form of computer 
input. Do you think that all languages would have the same 
degree of accuracy (i.e. the number of errors in the input 
stream) or would some languages work better with speech 
recognition software than others? 

Computer memory 
12 Computer memory 
Having described the sTni' i ure 
ana riperati.in of the CPu. v.e now 
look ill how Lata i= v.ored. 
I'licnpal'on isrs'i storec! in a 
computer in just one type- ol 
iterate device: it's s'.o'pd<n nP.AM 
and • m disk. CD-ROM. DVD. and 
tape-. There is such.»largo r.nge ot 
srcrrigi- dev ces an? tch^oii^g-es 
because each storage- mtc'iHiiim 
•; ideal ior s:!me tasks but not 
orhci s. Here :\Q lock at hoth high 
spec" immediates.cess 
M'lii'ioncj'jctor technology ard 
the much slovvc-r m.ignetk -lricj 
opiiia1. sempCoiy Mi:age systems 
•used to archn-e data 
INTRODUCTION 
Memory systems are divided into two classes: immediate access memory and secondary 
storage. We begin with the high-speed immediate access main store based on semiconductor 
technology and demonstrate how memory components are interfaced to the CPU. Then we 
look at magnetic and optical secondary stores that hold data not currently being processed by 
the CPU. Secondary stores have gigantic capacities but are much slower than immediate access 
stores. 
Over the years, memory systems have been subject to three trends, a reduction in their cost, an 
increase in their capacity, and an increase in their speed. Figure 12.1 (from IBM) demonstrates just 
how remarkably memory costs have declined over a decade for both semiconductor and magnetic 
memory. Fifteen years has witnessed a reduction of costs by three orders of magnitude. 
12.1 Memory hierarchy 
A computer may have registers, cache memory, flash 
Computer memory systems are not homogeneous. The 
memory, a floppy disk, a hard disk, a CD ROM, and a DVD. 
various memory devices in a typical PC may use four or more 
Some computers even have tape storage systems. If all 
different technologies, each with its own properties. For 
thesedevicesstoredata,whydoweneedsomanyofthem?As 
example, internal registers within the CPU itself are built 
in every aspect of life, economics plays a dominant role in 
with semiconductor technology and have access times of the 
memory systems design. The characteristics a computer 
order of one nanosecond, whereas the hard disks that holds 
designer would like to see in a memory device are often 
programs and data use magnetic technology and are millions 
mutually exclusive. The ideal memory has the following 
of times slower. 
characteristics. 
•;..'APTER MAP 
10 Buses and 
• input/output 
i mechanisms 
): Chapter 10 deals With 
I input/output techniques and 
rshowshow information is 
i.'-transferred between a computer 
t and its peripherals. We look at 
i internal buses that link devices 
j vvithinthe computer and external 
f'fcuses that link remote devices 
such as printers with the 
.computer. 
11 Peripherals computer 
| The power of.a computer is as 
. much a function of its 
peripherals as of its data 
processing capabilities. 
I Chapter 11 introduces some of 
the peripherals found in a typical 
' PC such as the keyboard; display, 
printer, and mouse, as well as 
some of tiietnore unusual 
i peripherals that, forexample, can 
[ measure how fast a body is •'•.• 
rotating. 
3 The operating system 
Chapter 13 provides a brief 
overview of the software that 
controls the computer's interface 
and the way in which it accesses 
I memory—the operating system. 

4 9 4 
Chapter 12 Computer memory 
100 
0.01 
0.001 L 
0.0001 
DRAM/Flash 
HDD D DRAM O Flash 
J 
Figure 12.1 The downward 
trend in memory cost 
(from IBM). 
2010 
Capacity 
lowl 
slow 100 s 
Optical storage systems 
are smaller than magnetic 
storage (600 Mbytes to 14 Gbytes) 
1000 bytes 
512 kbytes 
1 Mbytes 
500 Gbytes 
t high t 1000 Gbytes 
(1 Tbyte) 
Figure 12.2 Memory hierarchy. 
High speed A memory's access time should be very low, 
preferably 0.1 ns, or less. 
Small size Memory should be physically small. Five hundred 
thousand megabytes (i.e. 500 Gbytes) per cubic centimeter 
would be nice. 
Low power The entire memory system should run off a watch 
battery for 100 years. 
Highly robust The memory should not be prone to errors; 
a logical one should never spontaneously turn into a logical 
zero or vice versa. It should also be able to work at tempera-
tures of -60°C to 200°C in dusty environments and tolerate 
high levels of vibration—the military are very keen on this 
aspect of systems design. 
Low cost Memory should cost nothing and, ideally, should be 
given away free with software (e.g. buy Windows 2015® and 
get the 500 Gbytes of RAM needed to run it free). 
Figure 12.2 illustrates the memory hierarchy found in 
many computers. Memory devices at the top of the hierarchy 
are expensive and fast and have small capacities. Devices at 
the bottom of the hierarchy are cheap and store vast amounts 
of data, but are abysmally slow. This diagram isn't exact 
because, for example, the CD-ROM has a capacity of 
_J 
1 
1 
L-: 
I 
! 
! 
! 
< 
' 
I 
I 
1995 
2000 
2005 
Year 
1" HDD 
I 
1 1 Mobile/server HDD 
Desktop HDD 
Price/Mbyte ($) 
Random access 
memory 
i 
Serial 
access 
memory 
Main store 
/RegistersK 
Cache 
^ 
Hard disk (magnetic) 
/ : 
CD-ROM and DVD (optical) 
Tape (magnetic) 
A 
1 ns 
5 ns 
50 ns 
10 ms 
On-chip 
In the computer 
4 
100 ms 5? 
.1/ 
Speed 
fast T 
1 ns 
10 
1 
j 
D.I 
"•§---£_£ 

12.1 Memory hierarchy 
495 
600 Mbytes and (from the standpoint of capacity) should 
appear above hard disks in this figure. 
Internal CPU memory lies at the tip of the memory hierar-
chy in Fig. 12.2. Registers have very low access times and are 
built with the same technology as the CPU. They are expen-
sive in terms of the silicon resources they take up, limiting the 
number of internal registers and scratchpad memory within 
the CPU itself. The number of registers that can be included 
on a chip has increased dramatically in recent years. 
Immediate access store holds programs and data during 
their execution and is relatively fast (10 ns to 50 ns). Main 
store is implemented as semiconductor static or dynamic 
memory. Up to the 1970s ferrite core stores and plated wire 
memories were found in main stores. Random access mag-
netic memory systems are now obsolete because they are 
slow, they are costly, they consume relatively high power, and 
they are physically bulky. Figure 12.2 shows the two types of 
random access memory, cache and main store. 
The magnetic disk stores large quantities of data in a small 
space and has a very low cost per bit. Accessing data at a par-
ticular point on the surface is a serial process and a disk's 
access time, although fast in human terms, is orders of mag-
nitude slower than immediate access store. A disk drive can 
store 400 Gbytes (i.e. >238 bytes) and has an access time of 
5 ms. In the late 1990s an explosive growth in disk technology 
took place and low-cost hard disks became available with 
greater storage capacities than CD-ROMs and tape systems. 
The CD-ROM was developed by the music industry to 
store sound on thin plastic disks called CDs (compact disks). 
CD-ROM technology uses a laser beam to read tiny dots 
embedded on a layer within the disk. Unlike hard disks, 
CD-ROMs use interchangeable media, are inexpensive, and 
store about 600 Mbytes, but have longer access times than 
conventional hard disks. The CD-ROM is used to distribute 
software and data. Writable CD drives and their media are 
more expensive and are used to back up data or to distribute 
data. The CD-ROM was developed into the higher capacity 
DVD in the 1990s. 
Magnetic tape provides an exceedingly cheap serial access 
medium that can store 1000 Gbytes on a tape costing a few 
dollars. The average access time of tape drives is very long in 
comparison with other storage technologies and they are 
used only for archival purposes. Writable CDs and DVDs 
have now replaced tapes in many applications. 
By combining all these types of memory in a single com-
puter system, the computer engineer can get the best of all 
worlds. You can construct a low-cost memory system with a 
performance only a few percent lower than that of a memory 
constructed entirely from expensive high-speed RAM. The 
key to computer memory design is having the right data in 
the right place at the right time. A large computer system may 
have thousands of programs and millions of data files. 
Fortunately, the CPU requires few programs and files at any 
one time. By designing an operating system that moves data 
from disk into the main store so that the CPU always (or 
nearly always) finds the data it wants in the main store, the 
system has the speed of a giant high-speed store at a tiny frac-
tion of the cost. Such an arrangement is called a virtual 
memory because the memory appears to the user as, say, a 
400 Gbyte main store, when in reality there may be a real 
main memory of only 512 Mbytes and 400 Gbytes of disk 
storage. Figure 12.3 summarizes the various types of memory 
currently available. 
Before we begin our discussion of storage devices proper, 
we define memory and introduce some of the terminology 
and underlying concepts associated with memory systems. 
Figure 12.3 Classes of memory. 
Memory 
Primary 
1 
j Secondary 
(random access) | 
(sequential access) 
Semiconductor 
Magnetic 
Magnetic 
Optica! 
~] Volatile | 
r~ Nonvolatile 
Nonvolatile 
Volatile Nonvolatile 
Volatile Nonvolatile 
None 
None 
Obsolete 
Obsolete 
»|Static RAM 
» 
EPROM 
•-• Ferrite core 
-*• 
Bubble 
•*• i.L;-a,::M 
H 
DRAM 
| 
*| 
EEPROM ~ | 
-JFloppy disk! 
^ K i - . v . r . - ^ V i ) 
*-» Flash EPROM 
•* Hard disk"] 
U.' 
i:-\T; 
•-• 
Tape 
iScv.'.-'--ji:l.t-r:VD 

496 
Chapter 12 Computer memory 
12.2 What is memory? 
12.3 Memory technology 
Everyone knows what memory is, but it's rather more 
difficult to say exactly what we mean by memory. We can 
define memory as the long- or short-term change in the phys-
ical properties of matter caused by an event. For example, ice 
forms on a pond during a spell of cold weather and remains 
for a short time after the weather gets warmer. The water has 
changed state from a liquid to a solid under the influence of a 
low temperature and now remembers that it has been cold, 
even though the temperature has risen above freezing point. 
To construct a computer memory, we have to choose a prop-
erty of matter that can be modified (i.e. written) and later 
detected (i.e. read). 
Without human memory we wouldn't be able to follow a 
movie, because anything that happened prior to the current 
point in time would have vanished. As we watch the movie, 
optical signals from the retina at the back of the eye cause 
changes within the brain—the event has passed but its effect 
remains. The movie itself is yet another memory device. The 
photons of light once generated by a scene alter the chemical 
structure of a thin coating of silver compounds on a film of 
plastic. 
The von Neumann stored program computer is based on 
the sequential execution of instructions. Clearly, the program 
must be stored if the individual instructions are to be carried 
out one after the other. Furthermore, as computation yields 
temporary values, memory is needed to hold them. Such 
memory is called immediate access memory because it must be 
able to access its contents at the same rate the CPU executes 
instructions. 
Programs not currently being executed have to be stored 
somewhere. Secondary storage devices hold vast amounts 
of information cheaply but cannot retrieve data at anything 
like the rate at which a computer executes instructions. 
Immediate access stores are approximately 1 000 000 times 
faster than secondary stores. 
Because digital logic devices operate on binary signals, it's 
reasonable to expect computer memories to behave in a 
similar fashion. Memory systems store information in binary 
form by exploiting a two-valued property of the storage 
medium. 
The most important requirements of a memory element 
are that it must be able to assume one of two stable states, and 
that an energy barrier must separate these states; that is, it 
must require energy to change the state of the memory. If 
there were no energy barrier separating the states, it would 
be possible for a stored binary value to change its state at 
the least provocation. In the case of a piece of string, it 
requires a considerable energy input either to tie a knot or to 
untie it. 
Memory systems employ a wider range of technologies than 
any other component of a digital computer. Each technology 
has advantages and disadvantages; for example, cost, speed, 
density (bits/mm3), power consumption, physical size, and 
reliability. We now provide a brief overview of some of these 
memory technologies. 
12.3.1 Structure modification 
We can store information by modifying the structure, shape, 
or dimensions of an object. Three decades ago this storage 
technology was found in punched cards and paper tape 
systems that use the there/not-there principle in which 
holes are made or not made in paper. The gramophone 
record is a structural memory that stores analog informa-
tion by deforming the sides of a spiral groove cut into the 
surface of a plastic disk. At any instant the analog information 
is a function of the depth of the cut in the side of the 
groove. The CD-ROM and DVD are structural memories 
because information is stored as a string of dots on a sheet of 
plastic. 
12.3.2 Delay lines 
Superman used a neat trick to view the past—he just zoomed 
away from Earth at a speed faster than light and then viewed 
past events from their light, which had been streaming away 
from the Earth at a constant speed of 300 000 km/s. A stream 
of photons moving through space represents the memory of 
the event. We can call this spatial memory because data is 
stored as a wave traveling through a medium. Early comput-
ers converted data into ultrasonic sound pulses traveling 
down tubes filled with mercury. When the train of pulses 
representing the stored binary sequence travels from one 
end of the tube to the other end, it is detected, amplified, 
and then recirculated. This type of memory is also called 
a delay-line memory and is no longer found in digital 
computers. 
12.3.3 Feedback 
Data can be stored in an electronic device by means of 
feedback like the flip-flop, which is held in a stable logical state 
by feeding its output back to its input. Modern semiconduc-
tor devices based on feedback have a very low access time and 
are found in cache memory systems. Semiconductor mem-
ory based on feedback can be designed to consume very little 
power. Such memory is used in portable systems. 

12.3 Memory technology 
497 
MEMORY TECHNOLOGY—SOME DEFINITIONS 
Memory cell A memory cell is the smallest unit of information 
storage and holds a single 0 or 1. Memory cells are grouped 
together to form words. The location of each cell in the 
memory is specified by its address. 
Capacity A memory's capacity is expressed as the quantity of 
data that it can hold. Sometimes the capacity of a memory 
device is quoted in bits and sometimes in bytes. We use the 
convention that 1K = 210 = 1024 and 1M = 2 2 0= 1048 576. 
Some manufacturers use the convention that 1K = 1000 
and 1M = 1 000 000. Note that 1C (gigabyte) = 230and 
1T (terabyte) = 240. 
Density The density of a memory system is a measure of how 
much data can be stored per unit area or per unit volume; that 
is density = capacity/size. 
Access time A memory component's most important parameter 
is its access time, which is the time taken to read data from a 
given memory location, measured from the start of a read cycle. 
Access time is the time taken to locate the required memory cell 
within the memory array plus the time taken for the data to 
become available from the memory cell. We should refer to read 
cycle access time and write cycle access time because some 
devices have quite different read and write access times. 
Random access When memory is organized so that the access 
time of any cell within it is constant and is independent of the 
actual location of the cell, the memory is said to be random 
access memory (RAM); that is, the access time doesn't depend 
where the data being accessed is located. This means that the 
CPU doesn't have to worry about the time taken to read a word 
from memory because all read cycles have the same duration. If 
a memory is random access for the purpose of read cycles, it is 
invariably random access for the purpose of write cycles. The 
term RAM is often employed to describe read/write memory 
where data may be read from the memory or written into it (as 
opposed to read-only memory). This usage is incorrect, because 
random access indicates only the property of constant access 
time.The dialed telephone system is a good example of random 
access system in everyday life.The time taken to access any 
subscriber is constant and independent of their physical location. 
Serial access In a serial or sequential access memory, the time 
taken to access data is dependent on the physical location of 
the data within the memory and can vary over a wide range 
for any given system. Examples of serial access memories are 
magnetic tape transports, disk drives, CD drives, and shift 
registers. If data is written on a magnetic tape, the time taken 
to read the data is the time taken for the section of tape 
containing the data to move to the read head. This data might 
be 1" or 2400 ft from the beginning of the tape. 
Bandwidth The bandwidth of a memory system indicates the 
speed at which data can be transferred between the memory 
and the host computer and is measured in bytes/second or 
bits/second. Bandwidth is determined by the access time of 
the memory, the type of data path between the memory and 
the CPU, and the interface between the memory and CPU. 
Latency Bandwidth indicates how fast you can transfer data once 
you have the data to transfer. Latency refers to the delay between 
beginning a memory access and the start of the transfer. 
Volatile memory Volatile memory loses its stored data 
when the source of power is removed. Most semiconductor 
memories are volatile, although devices such as EPROM and 
flash memory are non-volatile. Memories based on magnetism 
are non-volatile because their magnetic state doesn't depend 
on a continuous supply of power. 
Read-only memory The contents of a read-only memory 
(ROM) can be read but not modified under normal operating 
conditions. Read-only memories are non-volatile and are 
used to hold system software. Flash memory is a form of 
erasable ROM that can be written to. This is also called 
read-mostly memory and is used in compact flash cards in 
digital cameras. 
Static memory Once data has been written into a static 
memory cell, the data is stored until it is either altered by 
over-writing it with new data, or by removing the source of 
power if the memory is volatile. 
Dynamic memory Dynamic memories (DRAMs) store data as 
an electronic charge on the inter-electrode capacitance of a 
field effect transistor. The charge gradually leaks away and the 
data is lost after a few milliseconds. Dynamic memories have 
to restore the charge on the capacitors every 2 to 16 ms in an 
operation known as memory refreshing. DRAMs are much 
cheaper than static memories of the same capacity. 
12.3.4 Charge storage 
Dynamic memory devices store data as an electrical charge. 
If a conductor is electrically insulated from its surroundings, 
it can be given an electrical charge. Binary information 
is stored by regarding one state as electrically charged and 
the other state as not charged. The insulation prevents 
the charge from rapidly draining away. Such a memory 
element is known as a capacitor and was used in one of 
the World's first computers. (In 1946, Dr F. C. Williams 
used a cathode ray tube to store a bit as a charge on the 
screen. Tom Kilburn worked with Williams at Manchester 
University to extend the CRT store to 2048 bits in 1947.) 
The most popular form of immediate memory found in 
today's PCs and workstations, DRAM, employs charge 
storage. 

498 
Chapter 12 Computer memory 
12.3.5 Magnetism 
The 
most 
common 
low-cost, 
high-capacity 
storage 
mechanism uses magnetism. An atom consists of a nucleus 
around which electrons orbit. The electrons themselves have 
a spin that can take one of two values, called up and down. 
Electron spin generates a magnetic field and the atom can be 
in one of two possible magnetic states. Atoms themselves are 
continually vibrating due to thermal motion. In most sub-
stances, the spin axes of the electrons are randomly oriented, 
because of the stronger thermal vibrations of the atoms and 
there is no overall magnetic effect. A class of materials exhibit 
ferromagnetism, in which adjacent electrons align their spin 
axes in parallel. When all the atoms in the bulk material are 
oriented with their spins in the same direction, the material 
is magnetized. Because we can magnetize material with its 
electron spins in one of two states and then detect these 
states, magnetic materials are used to implement memory. 
Up to the 1960s, immediate access memories stored data in 
tiny ferromagnetic rings called ferrite cores (hence the term 
core stores). Ferrite core stores are virtually obsolete today 
and the most common magnetic storage device is the 
hard disk. 
12.3.6 Optical 
The oldest mechanism used to store data is optical 
technology. Printed text is an optical memory because 
ink modifies the optical (i.e. reflective) properties of the 
paper. The same mechanism stores digital information in 
barcodes. More recently, two technologies have been 
combined to create high-density optical storage devices. 
Figure 12.4 The 512K X 8 static RAM. 
The laser creates a tiny beam of light that illuminates a 
correspondingly tiny dot that has been produced by 
semiconductor fabrication techniques. These dots store 
information rather like the holes in punched cards and 
paper tape. 
12.4 Semiconductor memory 
Semiconductor random access memory is fabricated on 
silicon chips by the same process used to manufacture micro-
processors. Without the availability of low-cost semiconduc-
tor memory, the microprocessor revolution would have been 
delayed had microprocessors used the slow, bulky, and 
expensive ferrite core memory of 1960s and 1970s main-
frames. The principal features of semiconductor memory are 
its high density and ease of use. 
12.4.1 Static semiconductor 
memory 
Static semiconductor memory is created by fabricating an 
array of latches on a single silicon chip. It has a very low access 
time, but is about four times more expensive than dynamic 
memory because it requires four transistors per bit unlike the 
DRAM's cell, which uses one transistor. Static RAM is easy to 
use from the engineer's point of view and is found in small 
memories. Some memory systems use static memory devices 
because of their greater reliability than dynamic memory. 
Large memories are constructed with dynamic memory 
because of its lower cost. 
Figure 12.4 illustrates a 4M CMOS 
semiconductor memory. The acronym 
CMOS means complementary metal 
oxide semiconductor and indicates the 
semiconductor technology used to 
manufacture the chip. The 4M denotes 
the memory's capacity in bits; that is, 
222 bits. Power is fed to the memory via 
its Vss and Vcc pins. 
The chip in Fig. 12.4 is interfaced to 
the computer via its 32 pins, of which 
19 are the address inputs needed to 
select one of 219 = 524 288 (i.e. 512K) 
unique locations. Eight data lines 
transfer data from the memory during 
a read cycle and receive data from 
the processor during a write cycle. 
Electrical power is fed to the chip 
via two pins. The three control 
0 V + 5 V 
Address bus . 
1 
I 
.Data bus 
^A0VSS Vcc D0< 
• 
0 A 1 
D-i < 
• 
„ A 2 
D2 < 
• 
• A 3 
D 3« 
• 
* 
• A 4 
D4 < 
• 
\ . 
p A5 
D5 4 
• 
N. 
Data is fed into or 
> A6 
D6 ^ 
». 
\ received from the RAM 
19 lines select 
> ^ 
°7 " 
* 
° n >ts g- b i t d a t a b u s 
one of 2 1 9
 
fc 
A
8 
locations 
* f t 
CS .Chip select 
^ A i 
o l 
Output enable Memory control 
*"Al2 
R/W , Read/write
 
i n P
u t s 
>A 1 3 
« 
-
*^4 
Memory 
^
\ 
»A15 
__ 
\ 
* ^ 6 
CS. enables a read or write access 
~^ ^17 
GEjnables the data bus drivers 
*| I 8 
I 
R/W selects a read or a write cycle 

12.4 Semiconductor memory 
499 
MEMORY ORGANIZATION 
Memory components are organized as n words by m bits 
(the total capacity is defined a s m X n bits). Bit-organized 
memory components have a 1-bit width; for example, a 
bit-organized 256K chip is arranged as 256K locations each of 
one bit. Some devices are nibble organized; for example, a 
256K chip can be arranged as 64K locations, each containing 
4 bits. The device in Fig. 12.4 is byte organized as 512K words 
of 8 bits and is suited to small memories in microprocessor 
systems in which one or two chips may be sufficient for 
all the processor's read/write memory requirements. 
pins, CS, OE, and R/W determine the operation of the 
memory component as follows. 
Pin 
CS 
Name 
Chip select 
R/W 
Read/not write 
OE 
Output enable 
Function 
When low, CS selects the chip 
for a memory access 
When high R/W indicates a 
read cycle; when low it 
indicates a write cycle 
When low in a read cycle, OE 
allows data to be read from the 
chip and placed on the data bus 
In order for the chip to take part in a read or write 
operation, its CS pin must be in a low state. Whenever CS is 
inactive-high, the memory component ignores all signals at 
its other pins. Disabling the memory by turning off its inter-
nal tri-state bus drivers permits several memories to share the 
same data bus as long as only one device is enabled at a time. 
The R/W input determines whether the chip is storing the 
data at its eight data pins (R/W = 0), or is transferring data 
to these pins (R/W = 1). The output enable pin, OE, turns 
on the memory's tri-state bus drivers during a read cycle and 
off at all other times. Some chips combine OE with CS and 
R/W so that the output data buffers are automatically 
enabled when CS = 0 and R/W = 1. 
Address decoding and read/write electronics is located on 
the chip, simplifying the design of modern memory systems. 
Figure 12.5 demonstrates how this device can be connected 
to a CPU. Because the chip is 8 bits wide (i.e. it provides 8 bits 
at a time), two chips would be connected in parallel in a sys-
tem with a 16-bit data bus, and four chips in a system with a 
32-bit data bus. 
The CPU's data bus is connected to the memory's data 
pins and the CPU's address bus is connected to the mem-
ory's address pins. The memory's CS, R/W, and OE control 
inputs are connected to signals from the block labeled 
'Control logic'. This block takes control signals from the CPU 
and generates the signals required to control a read or a write 
cycle. 
Suppose the memory device is connected to a CPU with 
a 32-bit address bus that can access 232 locations. This 
RAM has 19 address inputs and provides only a fraction 
of the address space that the CPU can access (i.e. 512 kbytes 
out of 4 Gbytes). Extra logic is required to map this block of 
RAM onto an appropriate part of the processor's address 
space. The high-order address lines from the CPU (in this 
case, A]9 to A31) are connected to a control logic block that 
uses these address lines to perform the mapping operation. 
Essentially, there are 4G/512K = 232/219 = 213 slots into 
which the RAM can be mapped. We'll explain how this is 
done later. 
12.4.2 Accessing memory—timing 
diagrams 
The computer designer is interested in the relationship 
between the memory and the CPU. In particular, the memory 
must provide data when the CPU wants it, and the CPU must 
provide data when the memory wants it. The engineer's 
most important design tool is the timing diagram. A timing 
diagram is a cause-and-effect diagram that illustrates the 
sequence of actions taking place during a read or write cycle. 
The designer is concerned with the relationship between 
information on the address and data buses, and the memory's 
control inputs. Figure 12.6 shows the simplified timing 
diagram of a static RAM memory chip during a read cycle. 
The timing diagram illustrates the state of the signals 
involved in a memory access as a function of time. Each sig-
nal may be in a 0 or a 1 state and sloping edges indicate a 
change of signal level. The timing diagram of the address bus 
appears as two parallel lines crossing over at points A and B. 
The two parallel lines mean that some of the address lines 
may be high and some low; it's not the actual logical values of 
the address lines that interest us, but the time at which the 
contents of the address bus become stable for the duration of 
the current memory access cycle. We haven't drawn the R/W 
line because it must be in its electrically high state for the 
duration of the entire read cycle. 
Let's walk through this diagram from left to right. At point A 
in Figure 12.6, the contents of the address bus have changed 
from their previous value and are now stable; that is, the old 

500 
Chapter 12 Computer memory 
Address bu£ 
1Z. 
Address Data 
CPU 
R/W 
DS 
Data bus I X 
tow-orde 
address 
lines 
High-order 
address 
5*> 
\7 
\7 
Address Data 
Memory 
block 1 
CS R/W OE 
Address 
_ 
CS1 
DS 
R/W 
C S 2 
Control RAM 
logic 
— 
5 
V. 
0£ 
Select bl 
Select DM 
:k1 
±2 
Select read or v 
I X 
_SZ :iz 
Enable data output in read cycle 
Address Data 
Memory 
block 2 
CS y R/W„. OEy^_ 
Output enable switches 
1 
~"-on tri-state bus drivers in 
\ 
a write cycle 
' R/W determines whether the 
s 
current cycle is a read or a write 
\ 
cyae 
\ -
Chip select determines whether 
the chip takes part in a memory cycle 
The control logic uses address and control 
signals from the CPU to control access to 
individual memory chips 
Figure 12.5 Interfacing 
memory components 
to a CPU. 
-•time 
Address 
-\ -
from CPU o-
A 
Read cycle time > 50 ns 
)I 
CS r 
Data from 1 _ 
memory 0 
Address valid 
__New address 
valid at point A CS goes low to 
access the memory 
Bus floating 
Read access time < 50 ns 
Data valid 
Data becomes 
valid at point F 
Figure 12.6 Timing diagram of the read cycle of a static RAM. 
address from the CPU has been replaced by a new address. 
Because logic transitions from 0 to 1 or 1 to 0 are never 
instantaneous, changes of state are depicted by sloping lines. 
Some timing diagrams use the high-to-low transition of CS 
as the principal reference point. 
Between points A and B the address bus contains the 
address of the memory location currently being read from. 
During this time the address from the CPU must not change. 
The time between A and B is the minimum cycle time of the 
memory. If the minimum cycle time is quoted as 50 ns, 
another memory access cannot begin until at least 50 ns after 
the start of the current cycle. 
Consider the operation of the memory component in a 
read cycle. The CPU first puts out an address on its address 
bus. The higher-order address lines from the CPU cause 
r 
Data hold 
time 
a chip-select output of the address decoder 
in the control logic to be asserted and to 
select a memory component as described 
in Fig. 12.5. At point C in Figure 12.6 the 
memory's active-low chip select input, CS, 
goes low and turns on the three-state bus 
driver outputs connected to the data pins. 
Up to point E the contents of the data bus 
are represented by a single line mid-way 
between the two logic levels. This conven-
tion indicates that the data bus is floating 
and is disconnected from the data output 
circuits of the memory. 
When a low level on CS turns on the memory's three-state 
output circuits at point E, the data bus stops floating and data 
appears at the output terminals. Sufficient time has not yet 
elapsed for the addressed memory word to be located and its 
contents retrieved. Consequently, the contents of the data bus 
between points E and F are not valid and cannot be used. At 
point F the data is valid and the time between points A and F 
is called the chip's read access time. 
At the end of the read cycle at point B, the contents of the 
address bus change. Because of propagation delays in the 
chip, the data at its output pins does not change until a guar-
anteed minimum time has elapsed. This delay is called the 
data hold time and is the duration between points B and D. 
The write cycle 
A static RAM's write cycle is similar to its read cycle, except 
that R/W must be in a low state, and data placed on the chip's 
c 
B 
P 
F 

12.4 Semiconductor memory 
5 0 1 
-•time 
Write cycle time > 50 ns 
Address 
1 -
from CPU o-
CS 
R/W 
Address valid 
CS goes low to 
access the memory 
R/W goes low to write 
data to the memory 
Data from 1 Bus floating 
CPU 
0 
•c: 
Data valid 
Data setup time 
I Data hold 
time 
Figure 12.7 Timing diagram of the write cycle of a static RAM. 
RAS CAS 
W 
OE 
. • 
• 
f 
L 
| 
Timing and control 
X 
A0-
A1-
A11-
I 
Column-
address 
buffers 
Row-
address 
buffers 
12 
16< 
T2~ 
Column decode 
Sense amplifiers 
512K array 
S12K array 
512K array 
11 
S1ZK array 
512K array 
512K array 
S-16 
Figure 12.8 The internal organization of a 16M X 4 dynamic RAM. 
data input lines by the CPU. Figure 12.7 shows the write-cycle 
timing diagram of a static RAM. We haven't provided OE 
timing (we'll assume that output enable is inactive-high 
throughout the write cycle). 
During the write cycle described by Fig. 12.7, data from 
the CPU is presented to the memory at its data inputs, R/W 
is set low for a write access, and CS asserted. Data isjatched 
into the memory cell by the rising edge of the R/W input. 
The critical timing parameters in a write cycle are the 
duration of the write pulse width (i.e. the minimum time 
for which R/W must be low) andjhe data setup time 
with respect to the rising edge of R/W. Data setup time is 
the time for which data must be present at the input 
terminals of a memory before the data is latched into the 
memory. 
12.4.3 Dynamic memory 
The immediate access store of the typical PC 
is fabricated with dynamic random access 
read/write memory (DRAM), which stores 
1 bit of information in a single transistor 
memory cell. In 2000 a typical DRAM 
had a capacity of 64 Mbits organized as 
16M X 4 bits and by 2004 128-Mbit chips 
were becoming standard. 
Data in a dynamic memory cell is stored 
as an electrical charge on a terminal of a 
field-effect transistor. This charge generates 
an electrostatic field that modifies the flow of 
current between the transistor's other two 
terminals. A dynamic memory chip contains 
all the electronics needed to access a given 
cell, to write a one or a zero to 
it in a write cycle, and to read 
its contents in a read cycle. 
Figure 12.8 illustrates the 
internal arrangement of a 
16M X 4 dynamic memory 
chip (i.e. the chip has 16M 
locations each holding 4 bits). 
You might 
diink 
that 
a 
16M X 4-bit DRAM requires 
24 address lines, because it takes 
24 address lines to access 16M 
locations (i.e. 224 = 16M). In 
order to reduce the size of the 
DQ1-DQ4 
DRAM's package, its address 
bus is multiplexed; that is, an 
address is input in two halves. 
One half of the address is called the row address and the other 
half is called the column address. A DRAM requires two control 
signals to handle the address: a row address strobe (RAS), which 
captures the row address and a column address strobe (CAS), 
which captures the column address. 
Multiplexing the address bus increases the complexity of 
the interface between the DRAM and the CPU. As the 
16M X 4 memory component contains only 4 bits in each of 
its 16M addressable locations, 16 of these chips are required 
to construct a 64-bit wide memory module. 
The electrical charge on the transistor in a memory cell 
gradually leaks away and the cell loses its stored data. A typi-
cal dynamic memory cell is guaranteed to retain data for up 
to 16 ms after it has been written. In order to retain data for 
longer than 16 ms, data must be rewritten into every cell peri-
odically in an operation called refreshing. In practice, simply 
accessing a memory cell refreshes it (and all cells in the same 
row). The need to refresh dynamic memories periodically 
increases the complexity of the CPU-DRAM interface. 
i/o 
buffers 
Data 
in 
reg. 
Data 
out 
reg. 
A 
t* 
c 
I 
D 
B 
F 
C 
- 1 
0 — 
1 — 
0 
1 — 
0 
4 
DQ1-DQ4 
4 

502 
Chapter 12 Computer memory 
The DRAM read cycle 
We now describe the generic DRAM 
memory component. Although several 
variations on the standard DRAM have 
been introduced to improve access time, 
they all have the same basic memory cell— 
only the access mechanism and timing have 
changed. We provide a box at the end of this 
section that highlights some of the DRAM 
variations. 
Figure 12.9 provides the simplified 
timing diagram of a DRAM read cycle with 
224 addressable locations. Note that there 
are two address traces. One trace describes 
die address from the CPU and the other the 
address at the DRAM's address inputs. 
A read cycle starts with the row address of 
the current memory location being applied 
to the address inputs of the DRAM. An 
address multiplexer in the memory control 
system transmits bits Apo to A„ from the 
CPU to address inputs A„ to A,, at_the 
DRAM. The chip's row address strobe (RAS) 
is then asserted active-low to latch the row address into the 
chip (point A in Fig. 12.9). 
The next step is to switch over the address from row to 
column (point B) and apply the column address to the chip. 
In this case, address lines Al2 to A23 from the CPU are con-
nected to address lines Ao to A„ at the DRAM. The column 
address strobe (CAS) is asserted at point C to capture the col-
umn address. At this point, the entire 24-bit address has been 
captured by the DRAM and the address bus plays no further 
role in the read cycle. The data in the cell accessed by the 
address appears on the data-output line after a delay of typi-
cally 30 to 70 ns from the point at which RAS went activelow 
(point D). A read cycle ends when the first of either RAS or 
CAS returns inactive high. 
Figure 12.9 shows the role of the CPU's R/W signal, which 
must go high before CAS is asserted and remain high for the 
rest of the cycle. Note that many DRAM designers write W 
rather than R/W. 
The dynamic RAM's cycle time is longer than its access 
time because internal operations take place within the 
DRAM before another access can begin. A DRAM might have 
an access time of 30 ns and a cycle time of 60 ns. Cycle time is 
important because it places a limitation on the speed of the 
system. 
Figure 12.10 indicates the organization of dynamic 
memory control logic. The DRAM control must carry out 
the address multiplexing and generate the necessary RAS 
and CAS signals. You can obtain much of the logic 
Figure 12.9 Timing diagram of the read cycle of a dynamic RAM. 
needed to implement a dynamic memory controller on a 
single chip. 
The DRAM write cycle 
A DRAM write cycle, described in Fig^J2.11, is similar to a 
read cycle except that the DRAM's R/W input must go low 
before CAS goes low, and the data to be stored in the 
addressed cell must be applied to the data-in line.  
The timing of the DRAM's address and the RAS and CAS 
signals are the same in both read and write cycles. However, in 
a write cycle the data from the CPU must be available andthe 
R/W control signal must be low before the DRAM's CAS 
input is asserted (some DRAMs support other write modes). 
Refreshing DRAM 
A DRAM memory cell must be periodically rewritten if its 
data is not to be lost. DRAMs are designed so that you don't 
have to refresh cells individually. Accessing a row simultane-
ously refreshes all cells in that row. A typical refresh controller 
performs all row refresh cycles every 16 ms. 
All that needs be done to refresh a DRAM is to assert CAS 
while RAS is high. This mode is called CAS-before-RAS 
refresh to distinguish it from a normal read or write access 
when CAS goes low after RAS. The DRAM automatically 
generates row refresh addresses internally. The DRAM 
refresh logic stops the processor and carries out a burst of 
refresh cycles at a time. 
Address 
1 
V— 
Address valid 
¥ 
from CPU 0 
"-P- 
: 
— 
" 
at DRAM I 
^iRpv^address \ Column address ]( 
RAS o 
^b^Jw 
| 
. 
jF^ 
CAS Q 
r 
\i
 
s^~-*i—J 
F) 
i 
— 1 
1 
n \ 
b 
! 
7 r — / 
R/w 0 
| 
i/ \ j ( j 
(I 
/ 
D a t a f r o m ' 
, 
, 
— ^ 
Data valid 
f 
DRAM 
0 
j 
\ 
:—A 
A 
B 
C 
D 
E 
Capture 
Multiplex Capture 
Data 
End of_read cycle 
row 
address 
column 
becomes 
when RAS or CAS 
address 
from row address 
valid 
goes high 
to column 

12.4 Semiconductor memory 
503 
DRAM control signals 
derived from CPU 
control outputs 
The multiplexer feeds 
either the row address 
or the column address 
to the DRAM 
Figure 12.10 Controlling the dynamic memory. 
Address 
1 -
from CPU o-
Address 
1 -
at DRAM 0-
Address valid 
J(ROW address I Column address j 
RAS 
CAS 
n 
R/W Q ^ _ _ 
Data from 1 
CPU 
0 
1 
R/W must be low 
and data valid before 
CAS goes active-low 
Figure 12.11 The write-cycle timing diagram of a dynamic RAM. 
DRAM reliability 
Dynamic memory suffers from two 
peculiar weaknesses. When a memory 
cell is accessed and the inter-electrode 
capacitor charged the dynamic memory 
draws a very heavy current from the 
power supply causing a voltage drop 
along the power supply lines. This volt-
age drop can be reduced by careful lay-
out of the circuit of the memory system. 
Another weakness of the dynamic 
memory is its sensitivity to alpha parti-
cles. Semiconductor chips are encapsu-
lated in plastic or ceramic materials that 
contain tiny amounts of radioactive 
material. One of the products of 
radioactive decay is the alpha particle 
(helium nucleus), which is highly ioniz-
ing and corrupts data in cells through 
which it passes. 
DRAM ORGANIZATION 
.mile 
DRAM chips are fabricated by the same technology as CPUs and 
encapsulated in a ceramic or plastic material. These small packages and 
then soldered onto printed circuit boards that can be plugged into PCs. 
PCs once used SIMMs (single in-line memory modules) with 72 pins 
that supported 32-bit data buses. Today, the SIMM has been replaced by 
the 168-pin dual in-line (DIMM) module that supports modern 64-bit 
data buses. DIMMs are available as 1 Cbyte modules. 
— I 
I 
I 
l\ 
,, ••^LSwitch 
Address 
a d d r e s s 
* * - 
„ 
DRAM array 
a g g r e s s ^ 
\ 
DRAM * 
—— 
J* * 
address 
_ 
Column' 
Address 
PS R/W 
I address | 
multiplexer | 
| RAS CAS R/W 
~ j 
T 
TT 
« 
I 
X 
IT 
Control from 
^ \ 
1 
J-r 
CPU 
\ . 
I 
• 
MPLX 
\ 
' 
• 
control 
».RAS 
i 
r- 
I 
: 
MPLX 
1 
1 
I CAS 
I 
I 
' 
Clock 
, . . . _ , 
Timing and control 
Data valid 
J f 
1 — 
5 0 
• 
1 
— 
0— 
i 1 _ 
0 

504 
Chapter 12 Computer memory 
DRAM FAMILIES 
Over the years, the access time of DRAMs has declined, but 
their performance has improved less than that of the CPU. 
Manufacturers have attempted to hide the DRAM's 
relatively poor access time by introducing enhanced DRAM 
devices. 
Fast page mode DRAM (FPD) This variation lets you 
provide a row address and then access several data elements 
in the same row just by changing the column address. Access 
time is reduced for sequential addresses. 
Extended data out DRAM (EDO) An EDO provides a 
small improvement by starting the next memory access 
LIMITS TO MEMORY DENSITY 
Since their introduction in the 1960s, the density of 
semiconductor memories has steadily grown. Semiconductor 
memories have grown from a capacity of 16 bytes to 
256 Mbytes, a growth of Z24 in three decades. Unfortunately, 
such progress can't continue because there are limits to 
memory density. Consider the following factors. 
Feature size Semiconductor devices are manufactured by a 
process that involves a step called photolithography. Silicon is 
coated with a photosensitive material and an image projected 
onto the surface. This image is developed and used to create 
the transistors that make up the memory. If you can create a 
smaller image, you can make smaller transistors and therefore 
build memories with more cells on a silicon chip. The smallest 
feature that can be etched on the silicon is, of course, gov-
erned by the smallest line that can be projected. The minimum 
width of a line projected onto the silicon is determined by the 
wavelength of the light used by the projector, because a beam 
of light spreads out due to an effect called diffraction. Even 
blue light with its short wavelength cannot generate features 
small enough for modern chips.Today, electron beams are 
used to draw features on the silicon. 
Quantum mechanical effects One of the consequences 
of quantum mechanics is that an object can spontaneously 
When an alpha particle passes through a DRAM cell, a soft 
error occurs. An error is called soft if it is not repeatable (i.e. 
the cell fails on one occasion but has not been permanently 
damaged). The quantify of alpha particles can be reduced by 
careful quality control in selecting the encapsulating mater-
ial, but never reduced to zero. By the way, all semiconductor 
memory is prone to alpha-particle errors—it's just that DRAM 
cells have a low stored energy/bit and are more prone to these 
errors than other devices. 
early and thereby reducing the overall access time by 
about 15%. 
Synchronous DRAM (SDRAM) The SDRAM is operated in a 
burst mode and several consecutive locations are accessed 
sequentially; for example 5-1-1-1 SDRAM provides the first 
data element in five clock cycles but the next three elements 
are provided one clock cycle after each other. 
Double data rate synchronous DRAM (DDR DRAM) This is a 
version of SDRAM where the data is clocked out on both the 
rising and falling edges of its clock to provide twice the data 
transfer rate. 
penetrate a barrier without having the energy to go through it. 
The probability of penetrating a barrier depends on the 
barrier's width and the size of the object—the thinner the 
barrier the more likely the penetration. If the insulators that 
separate one transistor from another in a memory become too 
thin, electrons will be able to tunnel spontaneously through 
the insulators. 
Statistical nature of a current An electrical current is com-
posed of a flow of electrons. In normal systems the number of 
electrons flowing in a circuit is staggeringly large. If memories 
are made smaller and smaller, the number of electrons flowing 
will diminish. At some point, the random nature of a current 
flow will have to be taken into account. 
Energy It requires a finite amount of energy to change a 
memory cell from one state to another. However, the mini-
mum amount of energy that can be used in switching is lim-
ited by quantum mechanics (i.e. there is a fixed minimum level 
of energy that can be used to perform switching). 
Power It requires energy to operate a microcircuit. Energy 
consumption has two problems: source (important if the 
equipment is battery operated) and dissipation. Getting rid 
of heat is one of the factors limiting the complexity of 
silicon-based circuits. 
tft 
A random soft error that corrupts a bit once a year in a PC 
,e. 
is an irritation. In professional and safety-critical systems 
:ly 
the consequences of such errors might be more severe. The 
by 
practical solution to this problem lies in the type of error-
:r- 
correcting codes we met in Chapter 3. For example, five check 
or 
bits can be appended to a 16-bit data word to create a 21-bit 
M 
code word. If one of the bits in the code word read back from 
se 
the DRAM is in error, you can calculate which bit it was and 
correct the error. 

12.4 Semiconductor memory 
505 
12.4.4 Read-only semiconductor 
memory devices 
As much as any other component, the ROM (read-only 
memory) was responsible for the growth of low-cost PCs in 
the 1980s when secondary storage mechanisms such as disk 
drives were still very expensive. In those days a typical operat-
ing system and BASIC interpreter could fit into an 8- to 
64-kbyte ROM. Although PCs now have hard disks, ROMs 
are still found in diskless palm-top computers and personal 
organizers. All computers require read-only memory to store 
the so-called bootstrap program that loads the operating sys-
tem from disk when the computer is switched on (called the 
BIOS (basic input/output system)). 
ROMs are used in dedicated microprocessor-based con-
trollers. When a microcomputer is assigned to a specific task, 
such as the ignition control system in an automobile, the soft-
ware is fixed for the lifetime of the device. A ROM provides 
the most cost-effective way of storing this type of software. 
Semiconductor technology is well suited to the production 
of high-density, low-cost, read-only memories. We now 
describe the characteristics of some of the read-only memo-
ries in common use: mask-programmed ROM, PROM, 
EPROM, flash EPROM, and EEPROM. 
Mask-programmed ROM 
Mask-programmed ROM is so called because its contents 
(i.e. data) are permanently written during the manufacturing 
process. A mask (i.e. stencil) projects the pattern of connec-
tions required to define the data contents when the ROM is 
fabricated. A mask-programmed ROM cannot be altered 
because the data is built into its physical structure. It is the 
cheapest type of read-only semiconductor memory when 
manufactured in bulk. These devices are used only when 
large numbers of ROMs are required because the cost of 
setting up the mask is high. The other read-only memories 
we describe next are all user programmable and some are 
reprogrammable. 
PROM 
A PROM (programmable read-only mem-
ory) can be programmed once by the user 
in a special machine. A transistor is a 
switch that can pass or block the passage of 
the current through it. Each memory cell 
in a PROM contains a transistor that can 
be turned on or off to store a 1 or a 0. The 
transistor's state (on or off) is determined 
by the condition of a tiny metallic link that 
connects one of the transistor's inputs to a 
fixed voltage. When you buy a PROM, it is 
filled with all Is because each link forces 
the corresponding transistor into a 1 state. A PROM is pro-
grammed by passing a current pulse to melt it and change the 
state of the transistor from a 1 to a 0. For obvious reasons, 
these links are often referred to as fuses. A PROM cannot be 
reprogrammed because if you fuse a link it stays that way. The 
PROM has a low access time (5 to 50 ns) and is largely used as 
a logic element rather than as a means of storing programs. 
EPROM 
The EPROM is an erasable programmable read-only memory 
that is programmed in a special machine. Essentially, an 
EPROM is a dynamic memory with a refresh period of tens 
of years. Data is stored in an EPROM memory cell as an 
electrostatic charge on a highly insulated conductor. The 
charge can remain for periods in excess of 10 years without 
leaking away. 
We don't cover semiconductor technology, but it's worth 
looking at how EPROMs operate. All we need state is that 
semiconductors are constructed from pure silicon and that 
the addition of tiny amounts of impurities (called dopants) 
changes the electrical characteristics of silicon. Silicon doped 
with an impurity is called n-type or p-type silicon depending 
on how the impurity affects the electrical properties of the 
silicon. 
Figure 12.12 illustrates an EPROM memory cell consisting 
of a single NMOS field effect transistor. A current flows in the 
N+ channel between the transistor's positive and negative 
terminals, Vdd and Vss. By applying a negative charge to a gate 
electrode, the negatively charged electrons flowing through 
the channel are repelled and the current turned off. The tran-
sistor has two states: a state with no charge on the gate and a 
current flowing through the channel, and a state with a 
charge on the gate that cuts off the current in the channel. 
A special feature of the EPROM is the floating gate that 
is insulated from any conductor by means of a thin layer of 
silicon dioxide—an almost perfect insulator. By placing or not 
placing a charge on the floating gate, the transistor can by 
turned on or off to store a one or a zero in the memory cell. 
Silicon dioxide insulator 
Silicon select gate 
( 
Silicon floating gate ) 3 
n+ inplant 
p-type inplant 
n+ inplant 
p-tvps silicon v.ibitr'ati 
Figure 12.12 The structure of an EPROM memory cell. 
Vss 
-
vgg 
- 
v d d 

506 
Chapter 12 Computer memory 
If the floating gate is entirely insulated, how do we put a 
charge on it in order to program the EPROM? The solution is 
to place a second gate close to the floating gate but insulated 
from it. By applying typically 12 to 25 V to this second gate, 
some electrons cross the insulator and travel to the floating 
gate (in the same way that lightning crosses the normally 
non-conducting atmosphere). 
You can program an EPROM, erase it, and reprogram it 
many times. Illuminating the silicon chip with ultra-violet 
light erases the data stored in it. Photons of ultra-violet light 
hit the floating gate and cause the stored charge to drain away 
through the insulator. The silicon chip is located under a 
quartz window that is transparent to ultra-violet light. 
EPROMs are suitable for small-scale projects and for 
development work in laboratories because they can be pro-
grammed, erased, and reprogrammed by the user. The disad-
vantage of EPROMs is that they have to be removed from a 
computer, placed under a ultra-violet light to erase them, and 
then placed in a special-purpose programmer to reprogram 
them. Finally, they have to be re-inserted in the computer. 
EPROMs have largely been replaced by flash EPROMs. 
Flash EPROM 
The most popular read-only memory is the flash EPROM, 
which can be erased and reprogrammed electronically. Until 
recently, typical applications of the flash EPROM were the 
personal organizers and system software in personal comput-
ers (e.g. the BIOS in PCs). Today, the flash EPROM is used to 
store images in digital cameras and audio in MP3 players. 
When flash memories first appeared, typical capacities were 
8 Mbytes. By 2005 you could buy 12-Gbyte flash memories. 
The structure of an EPROM memory cell and a flash 
EPROM cell are very similar. The difference lies in the thick-
ness of the insulating layer (silicon oxynitride) between the 
floating gate and the surface of the transistor. The insulating 
layer of a conventional EPROM is about 300 A thick, whereas 
a flash EPROM's insulating layer is only 100 A thick. Note 
that 1 A = 1 X 10~10m (or 0.1 nanometers). 
When an EPROM is programmed, the charge is trans-
ferred to the floating gate by the avalanche effect. The voltage 
difference between the gate and the surface of the transistor 
causes electrons to burst through the oxynitride insulating 
layer in the same way that lightning bursts through the 
atmosphere. These electrons are called hot electrons because 
of their high levels of kinetic energy (i.e. speed). The charge 
on the floating gate is removed during exposure to ultra-
violet light which gives the electrons enough energy to cross 
the insulating layer. 
A flash EPROM is programmed in exactly the same way 
as an EPROM (i.e. by hot electrons crashing through the 
insulator). However, the insulating layer in a flash EPROM is 
so thin that a new mechanism is used to transport electrons 
across it when the chip is erased. This mechanism is known as 
Fowler-Nordheim tunnelling and is a quantum mechanical 
effect. When a voltage in the range 12 to 20 V is applied across 
the insulating layer, electrons on the floating gate are able to 
tunnel through the layer, even though they don't have enough 
energy to cross the barrier. 
A flash EPROM is divided into sectors with a capacity of 
typically 1024 bytes. Some devices let you erase a sector or the 
whole memory and others permit only a full chip erase. Flash 
EPROMs can't be programmed, erased, and reprogrammed 
without limit. Repeated write and erase cycles eventually 
damage the thin insulating layer. Some first-generation flash 
EEPROMs are guaranteed to perform only 100 erase/write 
cycles, although devices are now available with lifetimes of at 
least 10 000 cycles. 
EEPROM 
The electrically erasable and reprogrammable ROM (EEPROM 
or E2PROM) is similar to the flash EPROM and can be 
programmed and erased electrically. The difference between 
the EEPROM and the flash EPROM is that the flash EPROM 
uses Fowler-Nordheim tunneling to erase data and hot elec-
tron injection to write data, whereas pure EEPROMs use the 
tunneling mechanism to both write and erase data. Table 12.1 
illustrates the difference between the EPROM, flash EPROM, 
and EEPROM. 
EEPROMs are more expensive than flash EPROMs and gen-
erally have smaller capacities. The size of the largest state-of-the-
art flash memory is usually four times that of the corresponding 
EEPROM. Modern EEPROMs operate from single 5 V supplies 
and are rather more versatile than flash EPROMs. Like the flash 
memory, they are read-mostly devices, with a lifetime of 10 000 
erase/write cycles. EEPROMs have access times as low as 35 ns 
but still have long write cycle times (e.g. 5 ms). 
The differences between a read/write RAM and an EEPROM 
are subtle. The EEPROM is non-volatile, unlike the typical 
semiconductor RAM. Second, the EEPROM takes much 
longer to write data than to read it. Third, the EEPROM can 
be written to only a finite number of times. Successive erase 
and write operations put a strain on its internal structure and 
eventually destroy it. Finally, EEPROM is much more expen-
sive than semiconductor RAM. The EEPROM is found in 
special applications where data must be retained when the 
power is off. A typical application is in a radio receiver that 
can store a number of different frequencies and recall them 
when the power is re-applied. 
12.5 Interfacing memory to a CPU 
We now look at how the semiconductor 
memory 
components are interfaced to the microprocessor. Readers 
who are not interested in microprocessor systems design may 
skip this section. 

12.5 Interfacing memory to a CPU 
507 
Device 
EPROM 
Flash EPROM 
EEPROM 
Normalized cell size 
1.0 
1.0 to 1.2 
3.0 
Programming mechanism 
Hot electron injection 
Hot electron injection 
Tunneling 
Erase mechanism 
Ultra-violet light 
Tunneling 
Tunneling 
Erase time 
Z0 minutes 
1s 
5 ms 
Minimum erase 
Entire chip 
Entire chip (or sector) 
Byte 
Write time (per cell) 
<100jts 
<100p,s 
5 ms 
Read access time 
200 ns 
200 ns 
35 ns 
Table 12.1 Programmable EPROM differences. 
8-bit CPU 
(e.g. MC6805) 
16-bit CPU 
(e.g.68K) 
32-bit CPU 
(e.g. 68020) 
8-bit data path 
D0 to D7 
An 8-bit bus transfers 8 bits of 
data at a time (i.e. a byte) 
8-bit 
memory 
This memory must be able 
to supply 8 bits of data. 
16-bit data path 
Do t 0 Dis 
An 16-bit bus transfers 16 bits 
(i.e. 2 bytes) of data at a time 
data path 
D o t 0 D31 
An 32-bit bus transfers 32 bits 
(i.e. 4 bytes) of data at a time 
16-bit memory 
This memory must be able 
to supply 16 bits of data. 
32-bit memory 
This memory must be able 
to supply 32 bits of data. 
Figure 12.13 CPU, bus, and 
memory organization. 
12.5.1 Memory organization 
A microprocessor operates on a word of width w bits and 
communicates with memory over a bus of width b bits. 
Memory components of width m bits are connected to the 
microprocessor via this bus. In the best of all possible worlds, 
the values of w, b, and m are all the same. This was often true 
of 8-bit microprocessors, but is rarely true of today's high-
performance processors. Consider the 68K microprocessor, 
which has an internal 32-bit architecture and a 16-bit data 
bus interface. When you read a 32-bit value in memory, the 
processor automatically performs two 16-bit read cycles. 
The programmer doesn't have to worry about this, because 
the memory accesses are carried out automatically. Memory 
components are normally 1, 4, or 8 bits wide. If you use 
4-bit-wide memory devices in a 68K system, you have to 
arrange them in groups of four because a memory block 
must provide die bus with 16 bits of data. Figure 12.13shows 
the organization of 8-bit, 16-bit, and 32-bit systems. 
A memory system must be as wide as the data bus. That is, 
the memory system must be able to provide an 8-bit bus with 
8 bits of data, a 16-bit bus with 16 bits of data, and a 32-bit 
bus with 32 bits of data, etc. Consider the following examples. 
Example 1 An 8-bit computer with an 8-bit bus uses 
memory components that are 4 bits wide. Two of these 

508 
Chapter 12 Computer memory 
4K locations of 16 bits = 8 kbytes 
1M locations of 16 bits = 2 Mbytes 
64K tocations of 16 bits = 128 kbytes 
Figure 12.14 16-bit 
memory organization. 
devices are required to supply 8 bits of data; each chip 
supplies 4 bits. 
Example 2 The amount of data in a block of memory, in 
bytes, is equal to the width of the data bus (in bytes) multi-
plied by the number of locations in the block of memory. A 
16-bit computer with a 16-bit bus uses memory components 
that are 1 bit wide. Sixteen of these devices are required to 
supply 16 bits of data at a time. 
Example 3 An 8-bit computer uses memory components 
organized as 64K X 4 bits; that is, there are 64K = 216 differ-
ent addressable locations in the chip. Two of these chips are 
required to provide the CPU with 8 data bits. The total size of 
the memory is 64 kbytes. 
Example 4 A 16-bit computer uses memory components 
that are 64K X 4 bits. Four of these chips must be used to 
provide the CPU with 16 bits of data. Therefore, each of the 
64K locations provide 16 bits of data or 2 bytes (i.e. each of 
the 4 chips provides 4 of the 16 bits). The total size of the 
memory is 2 bytes X 64K = 128 kbytes. 
Example 5 A 16-bit computer uses 64K X 16-bit memory 
components, Only one of these chips is required to provide 
16 bits of data (2 bytes). Therefore, each chip provides 
2 X 64K= 128 kbytes. 
Figure 12.14 demonstrates memory organization by show-
ing how three 16-bit-wide blocks of memory can be con-
structed from 4-bit-wide, 8-bit-wide, and 16-bit-wide 
memory components. 
12.5.2 Address decoders 
If the memory in a microprocessor system were constructed 
from memory components with the same number of 
uniquely addressable locations as the processor, the problem 
of address decoding would not exist. For example, an 8-bit 
CPU with address lines, Aw to A31, would simply be con-
nected to the corresponding address input lines of the mem-
ory component. Microprocessor systems often have memory 
components that are smaller than the addressable memory 
space. Moreover, there are different types of memory: read/ 
write memory, read-only memory, and memory-mapped 
peripherals. We now look at some of the ways in which mem-
ory components are interfaced to a microprocessor. 
In order to simplify the design of address decoders we will 
assume an 8-bit microcontroller with a 16-bit address bus 
spanned by address lines A„ to A15. We are not going to use the 
68K because it has a 23-bit address bus, a 16-bit data bus, and 
special byte selection logic. These features of the 68K make it 
more powerful than earlier 8-bit processors, but they do get 
in the way of illustrating the basic principles. We provide 
several 68K-based examples later in this chapter. 
Consider the situation illustrated by Fig. 12.15, in which 
two IK X 8 memory components are connected to the 
address bus of an 8-bit microprocessor. This processor has 
16 address lines, A„ to A15. Ten address lines, Aj to A9, from the 
CPU are connected to the corresponding address inputs of 
the two memory components, Ml and M2. Whenever a loca-
tion (one of 2'° = IK) is addressed in Ml, the corresponding 
location is addressed in M2. The data outputs of Ml and M2 
are connected to the system data bus. Because the data out-
puts of both memory devices Ml and M2 are connected 
together, the data bus drivers in the memory components 
must have tri-state outputs. That is, only one of the memory 
components may put data onto the system data bus at a time. 
Both memory devices in Fig. 12.15 have a chip-select input 
(CS1 for block 1 and CS2 for block 2). Whenever the chip-
select input of a memory is active-low, that device takes part 
in a memory access and puts data on the data bus if R/W = 1. 
When CSl or CS2 is inactive (i.e. in a high state) the appro-
priate data bus drivers are turned off, and no data is put on 
the data bus by that chip. 
Let CSl be made a function of the address lines A10 to A15, 
so that CSl =/l(Ai5,A14,A13,A12,A)1,A10). Similarly, let 
CS2 be a function of the same address lines, so that 
CS2 =/2(A15,A14,A13,A,2,A11,A10). Suppose we choose 
functions/l and/2 subject to the constraint that there are no 
values of A15, A14, A13, An, A,„ A10 that cause both CSl and 
CS2 to be low simultaneously. Under these circumstances, the 
conflict between Ml and M2 is resolved, and the memory 
map of the system now contains two disjoint IK blocks of 
memory. There are several different strategies for decoding 
A10 to A16 (i.e. choosing functions/l and/2). These strategies 
may be divided into three groups: partial address decoding, 
full address decoding, and block address decoding. 
Partial address decoding 
Figure 12.16 demonstrates how two 1 kbyte blocks of mem-
ory are connected to the address bus in such a way that both 
blocks of memory are never accessed simultaneously. The 
conflict between Ml and M2 is resolved by connecting CSl 
I 4Kx4 | 1 4Kx4 | | 4Kx4 [ [ 4Kx4 \* —-—___ 
| 
1MX8 
"] | 
1Mx8 
|* 
______ 
| 
64KX16 
|* 
—. 

12.5 Interfacing memory to a CPU 
5 0 9 
A14-
AQA, Ag A 
Address 
CPU 
Data I 
AJJA, 
Address 
Memory 
block M1 
1 K x 8 
Data 
Uata ftus 
CS1 
A0A, 
A9 
Address 
Memory 
block M2 
1 K x 8 
Low to select M l 
Data 
7v 
16-bit address 
bus 
A0 to A9 select 
o n e o f 2 1 0 = 1024 
'possible locations 
in the memory 
CS2 
Low to select M2 
Figure 12.15 Connecting two 1 kbyte 
memories to a 16-bit address bus. 
EXAMPLE 1 
An 8-bit microprocessor 
with a 16-bit address bus 
accesses addresses in the range 
101xxxxxxxxxxxxx2 
(where bits A1S,A14,A13 
marked 101 are selected by the 
address decoder and the xs refer to locations within the 
memory 
block). 
What range of addresses does this block correspond to? 
How big is this block? 
The lowest address is 10100000000000002 and the highest 
address is 10111111111111112.This corresponds to the range 
A00016toBFFF16. 
Three address lines are decoded to divide the address space 
spanned by AQ to A1S into eight blocks. The size of one block is 
64K/8 = 8K. You could also calculate the size of the block 
because you know it is spanned by 13 address lines and 
213 = 8K. 
EXAMPLE 2 
An 8-bit microprocessor 
with a 16-bit address bus addresses a 
block of 32 kbytes of ROM. 
(a) How many memory components 
are required if the 
memory is composed of 8 kbyte 
chips? 
(b) What address tines from the processor select a location in 
the 32 kbyte 
ROM? 
(c) What address lines have to be decoded to select 
the ROM? 
{d} What is the range of memory locations provided by each 
of the chips (assuming that the memory blocks are 
mapped contiguously in the region of memory space 
starting at address 000016)? 
(a) The number of chips required is (memory block)/ 
(chip size) = 32K/8K = 4. 
(b) Each chip has 8K = 213 locations, which are 
accessed by the 13 address lines AQ to A12 from the 
processor. 
(c) Address lines AQ to A12 from the CPU select a location in 
the chip leaving A13 to A15 to be decoded. 
(d) The memory blocks are 
0000,6 to 1FFF16 
200016 to 3FFF16 
400016 to 5FFF16 
600016 to 7FFF16. 
directly to A15 of the system address bus and by connecting 
CS2 to AI5 via an inverter. Ml is selected whenever A15 = 0, 
and M2 is selected whenever A15 = 1. Although we have dis-
tinguished between Ml and M2 for the cost of a single 
inverter, a heavy price has been paid. Because A,5 = 0 selects 
Ml and A15 = 1 selects M2, it follows that either Ml or M2 
will always be selected. Although the system address bus 
can specify 216 = 64K unique addresses, only 2K different 
locations can be accessed. Address lines A10 to A14 take no 
part in the address-decoding process and consequently have 
no effect on the selection of a location within either Ml 
orM2. 
Figure 12.17 gives the memory map of the system corre-
sponding to Fig. 12.16. Memory block Ml is repeated 32 
times in the lower half of the memory space and M2 is 
repeated 32 times in the upper half of the memory space 
A 9 _ 
Ai 
-
A0 ^r 

510 
Chapter 12 Computer memory 
A1S-
A14-
A9 -
Address line Als is used to select block M1 or block M2 
AT 
-
Ao f 
A Q A , A9 A, 4A, 
Address 
CPU 
<% A1 
* 9 
Address 
Memory 
block M1 
AgA, 
A9 
Address 
Memory 
block M2 
CS2 
1Kx8 
3A10toA14 
not used 
^A0 to A9 
V7_ 
Low-order address 
lines select a 
location in the RAM 
M2 is selected when 
Am=1 
M1 is selected when 
:0 
Figure 12.16 Resolving 
contention by partial address 
decoding. 
1KJ 
0000 
03FF 
M1 
0400 
07FF 
M1 
0800 
0BFF 
M1 
ocoo 
0FFF 
Ml 
7800 
7BFF 
M1 
7C00 
7FFF 
M1 
80CC 
83FF 
M2 
84 00 
87FF 
M2 
F8C0 
FBFF 
M2 
FC0 0 
FFFF 
M2 
M1 is repeated 32 times 
in the 32K memory space 
0000 to 7FFF 
M2 is repeated 32 times 
in the 32K memory space 
8000 to FFFF 
Figure 12.17 The memory map corresponding to Fig. 12.16. 
because the five address lines A10 to A14 take no part in address 
decoding. In this section, all addresses will be given in hexa-
decimal form (we don't need to use a subscript). 
The penalty paid when a partial address-decoding scheme 
is employed is that it prevents full use of the microprocessor's 
address space and frequently makes it difficult to expand the 
memory system at a later date. 
Full address decoding 
A microprocessor system has full address decoding when 
each addressable location within a memory component is 
accessed by a single address on the system's address bus; that 
is, all the microprocessor's address lines are used to access 
each physical memory location, either by specifying a given 
memory device or by specifying an address within it. Full 
address decoding represents the ideal but is sometimes 
impractical because it may require an excessive quantity of 
hardware to implement it. We will design an address decoder 
for the pervious example of a system with two IK blocks of 
memory. Address lines A„ to A, select a location in one of the 
memory components, leaving A10 to A15 to be decoded. 
Suppose we select Ml when A15, A14, A13, A]2, A,,, A10 = 0,0, 
0,0,0,0andM2whenA15,A14,A13,A,2,A|1,A10 = 0,0,0,0,0, 
1. These address values correspond to the IK address blocks 
0000 to 03FF and 0400 to 07FF. Figure 12.18 demonstrates 
how we might perform the address decoding with random 
logic. 
Block address decoding 
Block address decoding is a compromise between partial 
address decoding and full address decoding. It avoids the 
inefficient memory usage of partial address decoding, by 
dividing the memory space into blocks or pages. Block 
address decoding is implemented by dividing the processor's 
address space into a number of equal-sized blocks. This 
operation is easy to perform because you can use commonly 
available logic devices to carry out this function. 
In a typical application of block address decoding, an 8-bit 
microprocessor's 64K memory space is divided into four 
blocks of 16K. A 2-line to 4-line decoder logic element 
converts the two high-order address lines, A15 and A14, into 
four lines. Each of these four lines is associated with one of 
the binary states of address lines A15 and A14. The four out-
puts of this address decoder are used as the chip-select inputs 
of memory components. The advantage of block address 
decoding is that no memory component can occupy a 
memory space larger than a single block. In practice, real 
1Kx8 
Data 
Data 

12.5 Interfacing memory to a CPU 
511 
*15. 
"14 
[ > ^ 
A 1 3 _ 
A i 2 _ 
A n _ 
A w _ 
"is. 
A u . 
A 1 3 . 
A12. 
A n . 
A10. 
> 
— CS1 
Asserted for addresses in 
the 1K range 0000 to 03FF 
> 
CS2 
Asserted for addresses in 
the IK ranee 0400 to 07FF 
Figure 12.18 A full address decoder for two 1K memory 
blocks of Fig. 12.16. 
64 kbytes 
of memory 
space 
0000 
3FFF 
4000 
7FFF 
8000 
BFFF 
C000 
Selected by 
A 1 S,AH = 00 
Selected by 
A15,A14 = 01 
Selected I;;, 
A15.AK= 1C-; 
Selected by 
A A,., = 11 
16 kbyte block 
YFFFF 
(a) Memory map. 
(b) Circuit of simple Mock address decoder. 
Figure 12.19 Dividing 64K memory space into 4 blocks. 
microprocessor systems often employ a combination of 
partial address decoding, Ml address decoding, and block 
address decoding. You can further decode these 16K blocks 
and divide the memory space between several peripheral 
devices. Figure 12.19 describes how this arrangement might 
be implemented. 
Decoding using m-line to n-line decoders 
The problems of address decoding can be greatly diminished 
by means of data decoders that decode an w-bit binary input 
into one of n outputs, where n = 2m. Table 12.2 gives the 
truth table of the 74138, a typical 3-line to 8-line decoder. 
This decoder has active-low outputs, making it especially 
suitable for address-decoding applications, because the 
majority of memory components have active-low chip-select 
inputs. Because the 74138 has three enable inputs (two 
active-low and one active-high) it is particularly useful when 
decoders are to be connected in series, or when the enable 
inputs are to be connected to address lines in order to reduce 
the size of the block of memory being decoded. 
Consider an example of address decoding in an 8-bit 
microprocessor system using the 74138 3-line to 8-line 
decoder. A microprocessor system is to be designed with 
16 kbytes of ROM in the range 0000 to 3FFF using 4-kbyte 
EPROMs and 8 kbytes of read/write memory in the range 
4000 to 5FFF using a single 8-kbyte chip. Provision must 
be made for at least eight memory-mapped peripherals in the 
256 byte range 6000 to 60FF. 
The first step is to work out what address lines have to be 
decoded to select each memory block. We can do this in two 
ways. First, if we know the size of the block we can calculate 
how many address lines are required to select it out of all pos-
sible blocks of the same size. Consider 
the 8K RAM block. The memory 
space is 64K, so there are 64K/8K = 8 
blocks. Because 8 = 23, the three 
high-order address lines have to be 
decoded. Alternatively, we can write 
down the first and last addresses 
in the block and note which address 
values are common to all locations; 
that is, 
4000 = 0100000000000000 
5FFF = 0101111111111111 
As you can see, only the three high-
order address lines are common to 
every location within this memory 
block. 
Table 12.3 gives the address table 
for this system and shows how one 
74LS138 divides the memory space 
into eight 4K blocks. A second 
decoder subdivides one of these blocks to provide memory 
space for the peripherals. Figure 12.20 gives a circuit diagram 
of the address decoder and Figure 12.21 illustrates its mem-
ory map. It is easy to see the advantages of block address 
decoding. First, RAM, ROM, or peripheral devices can be 
added without further alterations to the address-decoding 
circuitry by employing the unused outputs of the three 
decoders. Second, the system is flexible. By modifying the 
connections between the decoder circuits and the memory 
components they select, the effective address of those mem-
ory components may be altered. 
Note how we've selected the 8K block of RAM in 
Fig. 12.20. Because the RAM is selected if either of the two 4K 
blocks selected by Y4 or Y5 is selected, we can OR (in nega-
tive logic terms) Y4 and Y5 to select the RAM. Because the 
Select block 0 
^ , 4 = 0,0 
0000to3FFF 
Select block 1 
A15.A14 = 0,1 
4000 to 7FFF 
Select block 2 
' 
;,A14=1,0 
.000 to BFFF 
Select block 3 
A15,A14=1,1 
C000 to FFFF 

512 
Chapter 12 Computer memory 
Enable inputs 
Control inputs 
Outputs 
Y0 
Y, 
El 
E2 
E3 
C 
B 
A 
Outputs 
Y0 
Y, 
Y2 
Y3 
Y4 
Y5 
Y6 
Y7 
1 
1 
0 
X 
X 
X 
1 
1 
1 
X 
X 
X 
1 
0 
0 
X 
X 
X 
1 
0 
1 
X 
X 
X 
0 
1 
0 
X 
X 
X 
0 
1 
1 
X 
X 
X 
0 
0 
0 
X 
X 
X 
0 
0 
1 
0 
0 
0 
0 
1 
0 
0 
1 
0 
0 
1 
1 
0 
0 
0 
1 
0 
1 
0 
0 
0 
0 
1 
0 
1 
1 
0 
0 
0 
1 
1 
0 
0 
0 
0 
0 
1 
1 
0 
1 
0 
0 
0 
1 
1 
1 
0 
0 
0 
0 
1 
1 
1 
1 
1 
0 
Table 12.2 Truth table of a 74138 3-line to 8-line decoder. 
Device 
Size 
Address Range 
A15 
A14 
A,3 
A,2 
A„ 
"10 
A9 
A8 
A7 
A6 
A5 
A4 
A3 
Az 
A i 
Ao 
ROM1 
4K 
0000-OFFF 
0 
0 
0 
0 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
ROM2 
4K 
1000-1FFF 
0 
0 
0 
1 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
ROM3 
4K 
2000-2FFF 
0 
0 
1 
0 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
ROM4 
4K 
3000-3FFF 
0 
0 
1 
1 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
RAM 
8K 
4000-5FFF 
0 
1 
0 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
PI 
32 
6000-601F 
0 
1 
1 
0 
0 
0 
0 
0 
0 
0 
0 
X 
X 
X 
X 
X 
P2 
32 
6020-603F 
0 
1 
1 
0 
0 
0 
0 
0 
0 
0 
1 
X 
X 
X 
X 
X 
P8 
32 
60E0-60FF 
0 
1 
1 
0 
0 
0 
0 
0 
1 
1 
1 
X 
X 
X 
X 
X 
Table 12.3 Address table of a microprocessor system. 
peripherals don't occupy a 4K block, we have used address 
lines Ag to An to select a second 3-line to 8-line decoder that 
decodes the peripheral address space. 
Address decoding with the PROM 
Address decoding is the art of generating a memory compo-
nent's chip-select signal from the high-order address lines. 
An alternative to logic synthesis techniques is the program-
mable read-only memory (PROM) look-up table. Instead of 
calculating whether the current address selects this or that 
device, you just read the result from a table. The PROM was a 
popular address decoder because of its low access time and its 
ability to perform most of the address decoding with a single 
chip. The PROM address decoder saves valuable space on the 
microprocessor board and makes the debugging or modifica-
tion of the system easier. Because PROMs consume more 
power than modern devices, they've largely been replaced by 
CMOS programmable array logic devices. 
The PROM's n address inputs select one of 2" unique 
locations. When accessed, each of these locations puts a word 
on the PROM's m data outputs. This word is the value of the 
various chip-select signals themselves; that is, the processor's 
higher-order address lines directly look up a location in the 
PROM containing the values of the chip selects. 

12.5 Interfacing memory to a CPU 
5 1 3 
This decoder divides the 
lower 32 kbytes of memory 
into eight 4K blocks, 
0000 
A14-
A13-
A12. 
M5-
1-
A6 
l o h 
I? 
Y3 
Is 
Xe 
V7 
1000 
2000 
-A ROM1 
* ROMZ 
3000 
T 
4000 
5000 rx> 
*|ROM3 
ROM4 
6000 
7000 
> 
Address lines from 
the CPU used by the 
address decoder 
-*• B 
l o 
h 
1* 
1* 
600d 
6020!,, 
60401 
6060 
60801, 
60A0 
60C01 
RAM 
Select signals 
to memory 
and peripherals. 
Peripheral 1 
Peripheral 2 
Peripheral 3 
Peripheral 4 
Peripheral 5 
Peripheral 6 
• j Peripheral 7 
This decoder divides the 
region 6000 to 60FF into 
eight 256-byte blocks. 
§ 2 ^ 
Peripherals 
Figure 12.20 Circuit of an address decoder for Table 12.3. 
Let's look at a very simple example of a PROM-
based address decoder. Table 12.4 describes a 
16-location PROM that decodes address lines A12 
to A15 in an 8-bit microcomputer. Address lines 
A|2 to A15 are connected to the PROM's AQ to A3 
address inputs. Whenever the CPU accesses its 
64K memory space, the contents of one (and only 
one) of the locations in the PROM are read. 
Suppose that the processor reads the contents of 
memory location El24. The binary address of 
this location is 11100001001001002 whose four 
higher-order bits are 1110. Memory location 
1110 in the PROM is accessed and its contents 
applied to the PROM's data pins DO to D7 to give 
the values of the eight chip selects CS0 to CS7. In 
this case, the device connected to D5 (i.e. CS5) is 
selected. Figure 12.22 demonstrates how the 
PROM-based address decoder is used. This is a 
simplified diagram-in practice we would have to 
ensure that the PROM was enabled only during a 
valid memory access (for example, by using the 
processor's data strobe to enable the decoder). 
Table 12.4 divides the CPU's memory 
space into 16 equal-sized blocks. Because 
the processor has a 64 kbyte memory space, 
each of these blocks is 64K/16 = 4 kbytes. 
Consequently, this address decoder can 
select 4-kbyte devices. If we wanted to select 
devices as small as 1 kbyte, we would 
require a PROM with 64 locations (and six 
address inputs). If you examine the D4 
(CS4) output column, you find that there 
are two adjacent 0s in this column. If the 
processor accesses either the 4K range 6000 
to 6FFF or 7000 to 7FFF, CS4 goes low. 
We have selected an 8K block by putting a 0 
in two adjacent entries. Similarly, there are 
four 0s in the CS5 column to select a 
4 X 4 K = 16K block. 
As we have just observed, the PROM can 
select blocks of memory of differing size. 
In a system with a 16-bit address bus, a 
PROM with n address inputs (i.e. 2" bytes) 
can fully decode a block of memory with a 
minimum size of 2,6/2" = 216-" bytes. 
Larger blocks of memory can be decoded by 
increasing the number of active entries (in 
our case, 0s) in the data column of the 
PROM's address/ data table. The size of the 
block of memory decoded by a data output 
is equal to the minimum block size multi-
plied by the number of active entries in the 
appropriate data column. 
4 0000 I 
I 
* 
RONI1 
Selected by Y0 
_ 
v 
1000 
,. 
ROM2 
Selected by Y1 
2000 
" 
ROM3 
Selected by Y2 
v 
3000 
A 
ROM4 
Selected by Y3 
Address space 
„„„„ 
• 
for which A1S = 0 " 0 0 1 
f c l 
., 
_, 
'* 
Selected by Y4 
5ooo 
8 K R A M 
:: 
Selected by Y5 
_T 
" 
6 0 0 0 
„ 
• I 
! 
""--•< L 
Peripheral 
^eteeted by Y6 
space 
^^^-~~^ 
v 
^ 
7000 
\ 
eooo 
Peripheral 1 
U n u s e d 
\ 
6020 
Peripheral 2 
" 800° 
\ 
6Qi0 
Peripheral 3 
\ 
606° 
Peripheral 4 
Address space 
32 kbytes 
\ 
eoso 
Peripheral 5 
for which A 1 5 = l 
Nused) 
\ 
60R0 
p e r ip h e r 3 [ 6 
\ 
6oco 
Peripheral 7 
| 
, „ 
V O E O 
Peripheral 8 
ffiOO 
\ 
Unused 
6FFF\| 
Figure 12.21 Memory map for the system of Table 12.3 
and Fig. 12.20. 
A n — 
A10 
A Q 
— 
-4i 
- » E 
• c 
• 
B 
*• A 
» | 
•> E 
* E 

5 1 4 
Chapter 12 Computer memory 
Figure 12.22 Simplified 
circuit of a PROM-based 
decoder corresponding to 
Table 12.4. 
Table 12.4 Address decoding with a PROM. 
Today, the systems designer can also use programmable 
logic elements such as PALs and PLAs to implement address 
decoders. Moreover, modern microprocessors now include 
sufficient RAM, flash EPROM, and peripherals on-chip to 
make address decoding unnecessary. 
The structure of 68K-based memory systems 
To conclude this section on memory organization, we look at 
how memory components are connected to a 68K micro-
processor with its 16-Mbyte memory space and 16-bit data 
bus. Because the 68K has 16 data lines d^, to d,5, memory 
~ 
^ 
Address bus 
. „ 
Z3 
I 
Data bus 
. . 
. . 
I 
II Ji 
lur^nT 
Address Data 
Address Data 
Address Data 
CPU 
The PROM takes the high-order 
bits of the address bus and uses 
| 
Memory 
Memory 
them to index into a table. The 
device 1 
device 2 
contents of the table are the 
v 
chip-select signals. 
High-order 
V 
[CS 
| 
[CS 
address 
Address 
n 
f 
T 
lines 
°lfCS0 
_ 
n'il 
1= 'cs, 
D2 J J 
• CS2 
PROM 
D3_ J 
^ . - ^ 
4 
* C S4 Select signals to 
Ds 
*-CS5 other memory 
D6 4-1 
•'CS; devices. 
D 7 J 4 p ^ - — • c s - ; 
^ ^ T h e PROM's data 
outputs directly 
provide chip-select 
signals 
Inputs 
Outputs 
A15 
A14 
A13 
A,2 
CPU name 
CSO 
CsT 
CS2 
CS3 
CS4 
CS5 
CS6 
CS7 
A3 
A2 
A, 
AJ, 
PROM name 
D0 
D, 
D2 
D3 
D4 
D5 
D6 
D7 
Range 
0 
0 
0 
0 
OOOOtoOFFF 
0 
1
1
1
1
1
1
1 
0 
0 
0 
1 
1000to1FFF 
1
0 
1
1
1
1
1
1 
0 
0 
1
0 
2000to2FFF 
1
1
0 
1
1
1
1
1 
0 
0 
1
1 
3000to3FFF 
1
1
1
0 
1
1
1
1 
0 
1
0 
0 
4000to4FFF 
1
1
1
1
1
1
1
1 
0 
1
0 
1 
5000to5FFF 
1
1
1
1
1
1
1
1 
0 
1
1
0 
6000to6FFF 
1
1
1
1
0 
1
1
1 
0 
1
1
1 
7000to7FFF 
1
1
1
1
0 
1
1
1 
1 
0 
0 
0 
8000to8FFF 
1
1
1
1
1
1
1
1 
1 
0 
0 
1 
9000to9FFF 
1
1
1
1
1
1
1
1 
1
0 
1
0 
AOOOtoAFFF 
1
1
1
1
1
1
1
1 
1
0 
1
1 
BOOOtoBFFF 
1
1
1
1
1
1
1
1 
1 
1
0 
0 
COOOtoCFFF 
1
1
1
1
1
0 
1
1 
1 
1
0 
1 
DOOOtoDFFF 
1
1
1
1
1
0 
1
1 
1 
1
1
0 
EOOOtoEFFF 
1
1
1
1
1
0 
1
1 
1 
1
1
1 
FOOOtoFFFF 
1
1
1
1
1
0 
1
1 
0 
0 
0 
0 
0 
1 
0 
1 
0 
0 
1 
1 
1 
0 
0 
1 
0 
1 
1 
1 
0 
1 
1 
1 
0 
0 
0 
0 
0 
1 
0 
1 
0 
0 
1 
1 
1 
0 
0 
1 
0 
1 
1 
1 
0 
1 
1 
1 
0 
1 
1 
1 
0 
1 
1 
1 
0 
1 
1 
1 
0 
1 
1 
0 
1 
1 
0 
1 
1 
0 
1 
1 

12.6 Secondary storage 
515 
Figure 12.23 Dealing with 
byte and word accesses in a 
68K-based system. 
EXAMPLE 3 
Draw an address decoding table to satisfy the following 68K memory map 
RAMI 
00 0000 to 00 FFFF 
RAM2 01 0000 to 01 FFFF 
IZO_1 E0 0000toE0 001F 
IZO_2 EO 0020 to EO 003F 
Address lines 
Device Range 
23 22 21 20 
19 18 17 16 15 14 13 12 11 10 9 8 
7 6 5 4 3 2 1 0 
RAMI 
00 0000 to 00 FFFF 
0 0 
0 
0 
0 
0 
0 
0 
x 
x 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X X 
X 
X 
X 
RAM2 01 0000 to 01 FFFF 
0 0 
0 
0 
0 
0 
0 
1 
x 
x 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X X 
X 
X 
X 
I/O 7 E0 0000toE0 001F 
1
1
1
0 
0 
0 
0 
0 
0 
0 0 
0 
0 
0 
0 0 
0 0 0 
X X 
X 
X 
X 
\IO_2 
E0 0020toE0 003F 
1
1
1
0 
0 
0 
0 
0 
0 
0 0 
0 
0 
0 
0 0 
0 0 1 X X 
X 
X 
X 
blocks must be 16 bits wide in order to support both word 
and byte accesses. The address bus is not composed of 24 
address lines A23 to A ^ but 23 address lines A23 to A ^ These 
address lines select a 16-bit word (i.e. 2 bytes), rather than a 
single byte. Two control signals, UDS (upper data strobe) and 
LDS (lower data strobe), distinguish between the upper and 
lower bytes of a 16-bit word, respectively. 
Figure 12.23 shows the arrangement of a 68K-based 
system. If the 68K accesses a byte on data lines d00 to do7, it 
asserts data strobe LDS. If the 68K accesses a byte on data lines 
do8 to d15, it asserts data strobe UDS. If the68K accesses a word 
on do0 to d15, it asserts both LDS and UDS simultaneously. 
This mechanism provides a pseudo A^ (i.e. LDS asserted, 
UDS negated = Ao0 = 1, and LDS negated, UDS asserted = 
AQ,, = 0). By means of its two data strobes and 23-bit address 
bus, the 68K can address a word and then access either of 
the bytes at the word address or both bytes at this address. 
The byte on data lines do, to d^ is at the odd address and the 
byte on data lines d^ to dI5 is at the even address. 
12.6 Secondary storage 
Secondary storage describes storage with a relatively large 
access time that is used to hold large quantities of data. The 
term secondary store is synonymous with disk drives, tape 
transports, and optical storage. 
In the last few years, storage capacities have increased 
at up to 100% a year. Figure 12.24 illustrates the growth in 
disk capacity for three disk form factors (i.e. physical disk 
size). 
12.6.1 Magnetic surface recording 
We first examine the nature of the ferromagnetic materials 
used to store data. The origin of magnetism lies in the motion 
of electrons in their orbits—an electron orbiting a nucleus 
generates a magnetic field and the atom behaves like a tiny 
Address bus 
j 
I 
D a t a b u s ^ ' 
' ' 
^ ^ 
\ 
d08 t o d15 
dootodoj 
\ 
\ > 
\ > 
<> 
<> 
<> 
\ 
. A 
' 
, D„at3 
Address 
Data 
Address 
Data 
\ 
"o, 
Li 
aoo t o c )is 
The address bus uses 
A o i to A23 to select 
a word (2 bytes) 
ASK TPI i 
Memory 
Memory 
6 8 K C P U 
(upper byte) 
(lower byte) 
Asserted for 
' 
^ 
/ ' 
c . : t 0 " i s 
^ ~ ^ ~ ^ . Big endian data with 
TKc 
rjFTc"" 
r- 
— 
the high byte at the 
^ 
u u b 
I 
|CS 
I 
[CS 
J 
tow address 
/ 
r T ~ N 
~ ~ 
— - — - 
^ ^ " ^ - ^ T h e LDS and UDS data 
/ 
c 
N. 
~ 
'—•—• strobes select the lower, 
/ 
— 
P~c~i 
T 
i i 
upper, or both bytes of the 
/ 
CS 
1 
i _ y 
Select upper byte 
word currently accessed 
/ 
From 
/ 
address 
c 
>. 
Asserted for 
decoder 
h — . 
. 
. 
1 
d03tod15 
I 
q _ y Select low byte 

516 
Chapter 12 Computer memory 
EXAMPLE 4 
A 68K microprocessor system implements the following memory blocks: 
(a) 1 Mbyte of ROM using Z56K x 16-bit chips 
{b) 8 Mbytes of DRAM using 2M x 4-bit chips 
Construct a suitable address-decoding table and design an address decoder for this system. 
A 16-bit-wide chip provides 2 bytes of data per location. Therefore, a single 256K x 16-bit ROM provides 512 kbytes of data. 
We need two of these chips to provide 1-Mbyte. A 1-Mbyte block of data contains 220 bytes and is spanned by address lines A^, to A19. 
In a 68K-based system address lines A20 to A23 must be decoded to select this block. Assume that the block of ROM is located at 
address 00 0000 and that A23,A22, A21, A20 = 0,0,0,0.This 1-Mbyte block is composed of two 512-kbyte sub-blocks. Therefore one 
of these sub-blocks is selected when A19 = 0 and the other when A19 = 1. 
The 8 Mbytes of DRAM are spanned by AQQ to A22 (i.e. 223 bytes). This block of memory must be on an 8-Mbyte boundary 
(i.e. 00 0000 or 80 0000 in a 68K-based system). Because 00 0000 is occupied by ROM, we'll put the DRAM at 80 0000 for which 
A23 = I.This block is composed of 2M-location by 4-bit-wide devices. Four 4-bit-wide chips are required to provide 16 bits 
(2 bytes) of data. The amount of data provided by these four chips per location is2M locations X 2 bytes = 4M.We need two 
of these sub-blocks to get 8 Mbytes. The first sub-block is selected by A22 = 0 and the second by A22 = 1. 
00 0000 
OF FFFF 
80 0000 
FF FFFF 
1 Mbyte 
ROM 
i Mbytes 
DRAM 
16 bits 
2S6K x16 
256K x16 
512 Kbytes 
512 Kbytes 
1 Mbyte 
1 Mbyte ROM requires two 
512KX 16 chips. 
2Mx4 
2Mx4 
2MX4 
2Mx4 
2Mx4 
2Mx4 
2Mx4 
2Mx4 
4 Mbytes 
4 Mbytes 
8 Mbytes 
The next step is to construct an address-decoding table.A memory block must be on a boundary equal to its own size.The fol-
lowing address decoding table shows address lines A23 to A^. Although the 68K lacks an A^, line, it's easier to add an A^, line to 
the table so that we can operate in bytes. 
Device 
Range 
A23 
A22 
A21 
A2o 
A 
ROM1 
00 0000 to 07 FFFF 
0 
0 
0 
0 
0 
ROM 2 
08 0000 to OF FFFF 
0 
0 
0 
0 
1 
DRAM1 
80 0000 to BF FFFF 
1 
0 
x 
X 
X 
DRAM 2 
CO 0000 to FF FFFF 
1 
1 
x 
X 
X 
If you didn't treat the 8-Mbyte block of DRAM as a single block, but as two separate 4 Mbyte blocks, you could put each of these 
4-Mbvte sub-block on anv 4 Mbvte boundary. The following address decoding table is also a legal solution. 
" 0 0 
Device 
Range 
A23 
A22 
A21 
A 
ROM1 
00 000 to 07 FFFF 
0 
0 
0 
0 
ROM 2 
00 000 to OF FFFF 
0 
0 
0 
0 
DRAM1 
40 000 to 07 FFFF 
0 
1 
X 
X 
DRAM 2 
80 000 to BF FFFF 
1 
0 
X 
X 
0 
1 
x 
x 
Aoo 
x 
x 
x 
x 
"18 
X 
X 
X 
X 
" 0 0 
X 
X 
X 
X 
" 1 8 
X 
X 
X 
X 

12.6 Secondary storage 
517 
EXAMPLE 5 
Design an address decoder using a PROM to im plement the following 68K memory map. 
(a) 4 Mbytes of ROM at address 00 0000 using IM X 8-bit chips 
(b) 8 Mbytes of RAM at address 80 0000 using 4M X 4-bit chips 
(c) 7 Mbyte of RAM at address 60 0000 using 512KX 8-bit chips 
We begin by working out the sub-blocks of memory reqi ired from the size of the spec fied memory cc jmponents. 
(a) A pah of 1M X 8-bit chips gives 2 Mbytes. We need two sub-blocks to get 4 Mbyt es. 
(b) Four 4M X 4-bit chips gives 8 Mbytes. This provides all our needs. 
(c) A pair of512KX 8-bit chips gives 1 Mbyte This provides all our needs. 
Address decoding table 
Device 
Range 
A23 
A2z 
Azi 
Azo 
A1S 
Al8 
AQQ 
Size 
ROM1 
OOOOOOtolFFFFF 
0 
0 
0 
X 
X 
X 
X 
2 Mbytes 
ROM 2 
20 0000to3FFFFF 
0 
0 
1 
X 
X 
X 
X 
2 Mbytes 
RAMI 
80 0000toFFFFFF 
1 
X 
X 
X 
X 
X 
X 
8 Mbytes 
RAM 2 
60 0000 to 6F FFFF 
0 
1 
1 
0 
X 
X 
X 
1 Mbyte 
Each line n the PROM must select a block equal to the smallest block to be decoded; that is, 1 Mbyte. The PROM must 
decode Az3 to A20. In the following table, D0 from the PROM selects ROM^ D selects ROM2, D , selects RAM2, and D3 
selects RAMI. 
Device 
Range 
AZ3 
A22 
Azi 
Azo 
Do 
D, 
D2 
D3 
ROM1 
ROM2 
RAM2 
RAMI 
ROM1 
00 0000 to OF FFFF 
0 
0 
0 
0 
0 
1 
ROM1 
10 0000to1FFFFF 
0 
0 
0 
1 
0 
1 
ROM 2 
20 0000 to 2F FFFF 
0 
0 
1 
0 
0 
ROM 2 
30 0000 to 3F FFFF 
0 
0 
1 
1 
0 
unused 
40 0000 to 4F FFFF 
0 
1 
0 
0 
1 
unused 
50 0000 to 5F FFFF 
0 
1 
0 
1 
1 
RAM2 
60 0000 to 6F FFFF 
0 
1 
1 
0 
1 
0 
unused 
70 0000 to 7F FFFF 
0 
1 
1 
1 
1 
RAMI 
80 0000 to 8F FFFF 
0 
0 
0 
1 
0 
RAMI 
90 0000 to 9F FFFF 
0 
0 
1 
1 
0 
RAM 1 
A0 0000 to AF FFFF 
0 
1 
0 
1 
0 
RAMI 
B0 0000 to BF FFFF 
0 
1 
1 
1 
0 
RAMI 
CO 0000 to CF FFFF 
1 
0 
0 
1 
0 
RAM 1 
DO 0000 to DF FFFF 
1 
0 
1 
1 
0 
RAMI 
EO 0000 to EF FFFF 
1 
1 
0 
1 
0 
RAMI 
FO 0000 to FF FFFF 
1 
1 
1 
1 
0 
magnet. In most matter the magnetic effects of electron spin 
are overcome by the stronger force generated by the thermal 
vibration of the atoms that prevents magnetic interaction 
between adjacent atoms. 
In ferromagnetic materials such as iron there is a stronger 
interaction between electron spins, which results in the 
alignment of electrons over a region called a domain. Domains 
range from 1 u,m to several centimeters in size. Because the 
electron spins are aligned within a domain, the domain 
exhibits a strong spontaneous magnetization and behaves like 
a tiny magnet with a North Pole at one end and a South Pole at 
the other end. Within a large piece of ferromagnetic material, 
the magnetic axes of individual domains are arranged at 
random and there is no overall magnetic field in the bulk 
material. 
Suppose we thread a wire through a hole in a ring (called a 
toroid) of a ferromagnetic material and pass a current, i, 
through the wire. The current generates a vector magnetic 

518 
Chapter 12 Computer memory 
EXAMPLE 6 
A memory board in a 68K-based system with a 16-bit data bus has 7 Mbyte of RAM composed of 128K x 8 RAM chips located at 
address CO 0000 onward. The board also has a block of 256 kbytes of ROM composed of 128K X 8 chips located at address D8 
OOOO. Design an address decoder for this board. 
Two byte-wide RAM chips span the 16-bit data bus. The minimum block of memory is 2 x 128K = 256 kbytes accessed by 
address lines A17 to A^. We require 1 Mbyte of RAM, or four 256 kbyte blocks. Address tines A19 to A18 select a block and A23 to A20 
select a Mbyte block out of the 16 possible 1 Mbyte blocks (A23toA20 = 1100). The ROM is implemented as a single 256-kbyte 
block using two 128-kbyte chips. The following table can be used to construct a suitable decoder. 
Device 
A23 
A22 
A21 
A20 
A19 
Al8 
A 1 7. 
RAMI 
1 
0 
0 
0 
0 
X . . . 
RAM 2 
1 
0 
0 
0 
1 
X . . . 
RAM3 
1 
0 
0 
1 
0 
X . . . 
RAM4 
1 
0 
0 
1 
1 
X . . . 
ROM 
1 
0 
1 
1 
0 
X. .. 
Aoi 
Aoo 
Address range 
X 
X 
C0 0000toC3FFFF 
X 
X 
C4 0000toC7FFFF 
X 
X 
C8 0000toCBFFFF 
X 
X 
CCOOOOtoCFFFFF 
X 
X 
D8 0000 to DB FFFF 
EXAMPLE 7 
Design an address decoder that locates three block of memory in the following ranges: 00 0000 to 7F FFFF, AO 8000 toAO 8FFF, 
and F0 0000 to FF FFFF. 
Address range 
A23 to A20 
A19toA16 
A15toA12 
AntoA 8 
A7 to A4 
A3toAo 
Block size 
000000 to 7FFFFF 
First location 
0000 
0000 
0000 
0000 
0000 
0000 
8 Mbytes 
Last location 
0111 
1111 
1111 
1111 
1111 
1111 
spanned by 22 lines 
A08000toA08FFF 
First location 
1010 
0000 
1000 
0000 
0000 
0000 
4 kbytes 
Last location 
1010 
0000 
1000 
1111 
1111 
1111 
spanned by 12 lines 
F00000 to FFFFFF 
First location 
1111 
0000 
0000 
0000 
0000 
0000 
1 Mbyte 
Last location 
1111 
1111 
1111 
1111 
1111 
1111 
spanned by 20 lines 
From the table, you can see that the first block is selected by address line A23, the second block by address lines A23 to A12, and 
the third block by address lines A23 toA20. 
EXAMPLE 8 
The following address decoding PROM selects three blocks of memory in a 68K-based system. How targe is each block and what 
address range does it occupy? 
CPU address line 
A?, 
A22 
A2i 
CS2 
cs. 
C 
PROM address line 
A2 
A, 
Ao 
D2 
Di 
D 
0 
0 
0 
0 
1 
1 
0 
0 
1 
1 
1 
0 
1 
0 
0 
1 
0 
1 
1 
0 
1 
1 
0 
0 
1 
0 
1 
0 
1 
1 
0 
1 
1 
0 
1 
0 
1 
1 
1 
1 
0 
The PROM decodes the 68K's three highest-order address lines A23 to A21. These address lines partition the 68K's 16-Mbyte 
address space into eight 2-Mbyte blocks. CS2 selects the 2-Mbyte block for which A23, A22, A21 = 0,0,0. This is the address space 
00 0000 to 1F FFFF. CS, selects two 2-Mbyte blocks for which A23, A22 = 0, l.-This is the 4-Mbyte address space 40 0000 to 
7F FFFF. CS0 selects the four 2-Mbyte blocks for'which A23 = LThis is the 8-Mbyte address space 80 0000 to FF FFFF. 

12.6 Secondary storage 
5 1 9 
1000 
HDD roadfflfiap 
1 inch profile 10K RPM 
| 
146 Gbyte Server 
I 
180 Gbyte Desktop 7200 RPM, - * 
99 
2000 
01 
Availability year 
Figure 12.24 The increase in disk capacity (from Hitachi Global 
Storage Technologies, San Jose Research Centre). 
Magnetic field H 
due to current i 
Magnetic field B 
in the material 
06 
Wire carrying 
current; 
(a) Magnetic core. 
Figure 12.25 The hysteresis curve. 
field, H, in the surrounding space, where H is proportional to 
i. A magnetic field, B, is produced inside the ring by the 
combined effects of the external field, H and the internal 
magnetization of the core material. A graph of the relation-
ship between the internal magnetic field B and the external 
magnetic field H for a ferromagnetic material is given in 
Figure 12.25. This curve is called a hysteresis loop. 
Suppose that the external field round the wire is initially 
zero; that is, H = 0 because the current flowing through the 
wire, i, is zero. Figure 12.25 demonstrates that there are two 
possible values of B when H = 0: +B-, and -flr. These two 
states represent a logical one and a logical zero. The suffix r in 
Br stands for remnant and refers to the magnetism remaining 
in the ring when the external field is zero. Like the flip-flop, 
this magnetic material has two stable states 
and can remain in either of the states indefi-
nitely. Unlike the flip-flop, the ferromagnetic 
material is a non-volatile store and requires 
no power source to retain data. 
Assume that initially the ferromagnetic 
materia] is magnetized in a logical zero state 
and has an internal field -Br 
If a negative 
external field is applied (i.e. negative i, there-
fore negative H), the value of the internal 
magnetization B goes slightly more negative 
than —Bt and we move towards point P in 
Fig. 12.25. If H is now reduced to zero, the 
remnant magnetization returns to —B,. In 
other words, there is no net change in the 
state of the ferromagnetic material. 
Now consider applying a small positive 
internal field H. The internal magnetization 
is slightly increased from — BT and we move 
along the curve towards point Q. If the external magnetiza-
tion is reduced we move back to — Br However, if H is 
increased beyond the value 
+Hm, the magnetization of 
the material flips over at Q, 
and we end up at point R. 
When we reduce the external 
field H to zero, we return 
to +Bt and not to —Br. If 
the material is initially in a 
negative 
state, 
increasing 
the external magnetization 
beyond Hm causes it to 
assume a positive state. A 
magnetic field of less than 
Hm is insufficient to change 
the material's state. 
Similarly, if the ferromagnetic material is in a one state 
(+B,), a positive value of H has little effect, but a more nega-
tive value of H than -Hm will switch the material to a zero 
state (-Br). 
The switching of a ferromagnetic material from one state 
to another is done by applying a pulse with a magnitude 
greater than Im to the wire. A pulse of +Im always forces the 
material into a logical one state, and a pulse of —/m forces it 
into a logical zero state. 
The hysteresis curve can readily be explained in terms of the 
behavior of domains. Figure 12.26 shows a region of a ferro-
magnetic material at three stages. At stage (a) the magnetic 
material is said to be in its virgin state with the domains 
oriented at random and has no net magnetization. This corre-
sponds to the origin of the hysteresis curve, where H = 0 and 
B = 0. 
m External 
magnetic field 
(b) Hysteresis curve that relates internal field B to external field H. 
B Internal 
m magnetic field 
R 
'8m | 
..^^;::::;:^^ 
Magnetic material 
HDD Capacity (Gbytes) 
4*0.14 Gbytfc Microdrive 
1 Gbyte r-licrodra e 
4 Gbyte 
Microdrive 
f&hT 6fJGbyte 
\36 Gbyte Serve 
15KRPM 
:0Gby:e 
*5P 
te-Gbyie— 
^ 5 1 Gbytt 
.1 Gbyte 
11.2 Gbyte 
94 95 96 97 98 99 2000 01 02 03 04 05 
100 
10 
1 
~Hm 
h~~ 
l+Sr 
-Sm 
-B\ 
P 

520 
Chapter 12 Computer memory 
Figure 12.26 The behavior of domains. 
(a) No external field— 
magnetization of domains at random. 
_ 
(b) Weak external field applied— 
^~*C~^> magnetization of some domains 
rotate. 
cj Strong external field applied— 
magnetization of domains aligned 
in same direction. 
saSiSiiSSS^ 
\ 
Magnetic 
field 
Direction of 
movement 
• 
[ Magnetic coatmg 
Non-magnetic substrate 
Figure 12.27 Surface recording. 
Air gap 
The magnetic flux 
bulges out at the air 
gap in the head 
Magnetic flux 
in the head 
Figure 12.28 The air gap. 
At stage (b) an external magnetic field has been applied 
and some of the domains have rotated their magnetic axes to 
line up with the external field. As the external field is 
increased, more and more domains flip over, and there comes 
a point where the domains already aligned with the external 
field reinforce it, causing yet more domains to flip over. This 
process soon develops into an avalanche as the internal field 
rapidly builds up, and all domains are aligned with the exter-
nal field. At this point, stage (c), the bulk material is fully 
magnetized and is said to be saturated. 
The precise form of the hysteresis 
or B-H curve of Fig. 12.25 differs 
from 
one magnetic material to 
another. The best B-H curve for the 
purpose of storing data is square, 
so that the transition from one state 
to another (i.e. from —Bt to +Br) 
takes place for an infinitesimally 
small change in H. Such a magnetic 
material is said to have a square-loop 
B-H characteristic. Magnetic materi-
als displaying strong hysteresis effects 
are called hard, whereas those dis-
playing little are called soft. Now that 
we've described the basic principles of magnetization, we 
look at how it is applied in practice. 
Magnetizing a flat surface 
The operating principles of disk drives, tape transports, and 
VCRs are the same: the former records data on a flat platter 
coated with a magnetic material, whereas the latter records 
data on a thin band of flexible plastic coated with magnetic 
material. Figure 12.27 illustrates the generic recording 
process—the same model serves both disk and tape systems. 
The write head used to store data consists of a ring of high-
permeability soft magnetic material with a coil wound round 
it. High permeability means that the material offers a low 
resistance to a magnetic field. The material of the write head 
is magnetically soft and doesn't have a square-loop hysteresis; 
that is, it doesn't exhibit residual magnetization. 
The most important feature of the write head is a tiny air 
gap in the ring. When a current flows in the coil a magnetic 
flux is created within the ring. This flux flows round the core, 
but when it encounters the air gap, it spreads out into the sur-
rounding air as illustrated in Fig. 12.28; this external field is 
called a fringing field. 
Because the head is close to the recording medium, the 
magnetic field round the air gap passes through the magnetic 
material coating the backing. If this field is strong enough, it 
causes the domains within the coating to become aligned 
with the field from the head. Because the magnetic surface is 
moving, a continuous strip of surface is magnetized as it 
passes under the write head. If the direction of the current in 
the coil is changed the field reverses and the magnetic parti-
cles in the coating are magnetized in the opposite direction. 
Figure 12.29 shows how the domains in the surface material 
might be magnetized (nordi-south or south-north) after 
passing under die write head. We have also plotted the 
current in the write head on die same figure. 
As time has passed, engineers have produced greater and 
greater packing densities (about 10 Gbits per square inch in 
1995 and 100 Gbits per square inch by 2004). One of the 
improvements is due to the composition of the magnetic 
tcoil 
- G a p 

12.6 Secondary storage 
521 
(current in coil round write head) 
*-time 
Region magnetized S-N 
Region magnetized N-S 
Region magnetized S-N 
4 
: 
^.^ 
- 
p. 
Figure 12.29 The magnetized 
layer. 
medium used to store data. The size of the particles has been 
reduced and their magnetic properties improved. Some tapes 
employ a thin metallic film, rather than individual particles. 
Metal oxide coatings are about 800 u.m thick with oxide par-
ticles approximately 25 u,m by 600 u.m with an ellipsoidal 
shape. A thin film coating is typically only 100 u,m thick. 
Reading data 
A first-generation read head was essentially the same as a 
write head (sometimes a single head serves as a both a read 
and a write head). When the magnetized material moves past 
the gap in the read head, a magnetic flux is induced in the 
head. The flux, in turn, induces a voltage across the terminals 
of the coil that is proportional to the rate of change of the 
flux, rather than the absolute value of the magnetic flux itself. 
Figure 12.30 shows the waveforms associated with writing 
and reading data on a magnetic surface. The voltage from the 
read head is given by 
v(t) = iCd<J>/df 
K is a constant depending on the physical parameters of the 
system and $ is the flux produced by the moving magnetic 
medium. Because the differential of a constant is zero, only 
transitions of magnetic flux can be detected. The output from 
a region of the surface with a constant magnetization is zero, 
making it difficult to record digital data directly on tape or disk. 
12.6.2 Data encoding techniques 
Now that we've described the basic process by which informa-
tion is recorded on a magnetic medium, we are going to look 
at some of the ways in which digital data is encoded before it is 
recorded. Magnetic secondary stores record data serially, a bit 
at a time, along the path described by the motion of the mag-
netic medium under the write head. Tape transports have 
multiple parallel read/write heads and record several parallel 
tracks simultaneously across the width of the tape. 
Write 
current 
+/1 
-> time 
Recorded 
flux 
Br] 
k 
Br 
-•time 
Read 
voltage 
M 
A V A V A -+• time 
Figure 12.30 Read/write waveforms. 
You can't transmit the sequence of logical Is and 0s to be 
recorded directly to the write head. If you were to record a 
long string of 0s or Is by simply saturating the surface at — Bs 
or +Br, no signal would be received during playback. Why? 
Because only a change in flux creates an output signal. A 
process of encoding or modulation must first be used to trans-
form the data pattern into a suitable code so that the recorded 
data is always changing even if the source is all Is or 0s. 
Similarly, when the information is read back from the tape it 
must be decoded or demodulated to extract the original dig-
ital data. The actual encoding/decoding process chosen is a 
compromise between the desire to pack as many bits of data 
as possible into a given surface area while preserving the 
reliability of the system and keeping its complexity within 
reasonable bounds. 
Let's look as some of the possible recording codes (begin-
ning with a code that illustrates the problem of recording 
long strings of Is and 0s). However, before we can compare 
Substrate 

522 
Chapter 12 Computer memory 
ENCODING CRITERIA 
Efficiency A code's storage efficiency is defined as the 
number of stored bits per flux reversal and 
is expressed in 
percent. A 100% efficiency corresponds to 1 bit per flux 
reversal 
Self-clocking The encoded data must be separated into 
individual bits. A code that provides a method of splitting the 
bits off from one another is called self-clocking and is highly 
desirable. A non-self-clocking code provides no timing 
information and makes it difficult to separate the data stream 
into individual bits. 
Noise immunity An ideal code should have the largest 
immunity to noise and extraneous signals. Noise is caused by 
imperfections in the magnetic coating leading to drop-outs 
and drop-ins. A drop-out is a loss of signal caused'by missing 
magnetic material and a drop-in is a noise pulse. Another 
source of noise is cross-talk, which is the signal picked up by 
the head from adjacent tracks. Cross-talk is introduced because 
the read/write head might not be perfectly aligned with the 
track on the surface of the recording medium. Noise can also 
be caused by imperfect erasure. Suppose a track is recorded 
and later erased. If the erase head didn't pass exactly over the 
center of the track, it's possible that the edge of the track 
might not have been fully erased. When the track is rerecorded 
and later played back, a spurious signal from the unerased 
portion of the track will be added to the wanted signal. 
Serial data 
to be recorded 
Current in 
write head 
Voltage in read head 
during playback 
Data window during 
which state of signal 
from head is decoded 
n 
\ 
A 
\ 
AA 
Data window during 
nnnnnnnn
 
which***<*$&£ 
-I 
L_I 
L J 
l_J 
1_J 
L J 
L J 
L J 
L 
f r„_, u.„ j ;<- A~r„A?>A 
\
\ 
Serial data 
to be recorded 
Current in 
write head 
Voltage in read head 
during playback 
from head is decoded 
0 
A 
1 
X 
A 
0 
A 
1 
0 
0 
1 
1 
AA 
0 
A 
1 
X 
A 
0 
A 
1 
1 
AA 
Figure 12.31 Return-to-bias recording. 
Figure 12.32 Non-return to zero one recording (NRZ1). 
various encoding techniques we need to describe some of the 
parameters or properties of a code. In what follows the term 
flux reversal indicates a change of state in the recorded mag-
netic field in the coating of the tape or disk. Simply reversing 
the direction of the current in the write head causes a flux 
reversal. Some of the criteria by which a recording code may 
be judged are described in the box. 
Return-to-zero encoding 
Return-to-zero (RZ) recording requires that the surface be 
unmagnetized to store a zero and magnetized by a short pulse 
to store a 1. Because no signal is applied to the write head 
when recording a 0, any 1 s already written on the tape or disk 
are not erased or overwritten. Return-to-bias recording (RB) 
is a modification of RZ recording in which a 0 is recorded by 
saturating the magnetic coating in one direction and a 1 by 
saturating it in the opposite direction by a short pulse of the 
opposite polarity. 
Figure 12.31 illustrates the principles of return-to-bias 
recording and playback. A negative current in the write head 
saturates the surface to — Bv. A positive pulse saturates the 
surface to +Br to write a 1. The pulse width used depends on 
the characteristics of the head and the magnetic medium. A 
wide pulse reduces the maximum packing density of the 
recorded data and is wasteful of tape or disk surface but is 
easy to detect, whereas a very narrow pulse is harder to detect. 
Data is read from the disk/tape by first generating a data 
window, which is a time slot during which the signal from the 
read head is to be sampled. The signal from the read head is 
sampled at the center of this window. A sequence of 0s gener-
ates no output from the read head and there is no simple way 
of making sure that the data window falls exactly in the mid-
dle of a data cell. For this reason return-to-bias is said to be 
non-self-clocking. The worst-case efficiency of RB recording is 
50% 
(when the data is a string of Is) and its noise sensitivity 
is poor. RB recording is not used in magnetic recording. 
Non-return to zero encoding 
One of the first widely used data encoding techniques was 
modified non-return to zero or NRZ1. 
Each time a 1 is to be 
recorded, the current flowing in the head is reversed. When 
reading data each change in flux is interpreted as a 1. 
Figure 12.32 illustrates NRZ1 recording which requires a 
maximum of one flux transition per bit of stored data giving 
0 
1
0 
1
0 
0 
1
1 

12.6 Secondary storage 
5 2 3 
Serial data 
to be recorded 
Current in 
write head 
Voltage in read head 
during playback 
Data window during 
which state of signal 
from head is decoded 
0 
1
0 
1
0 
0 
1
1 
x_n_rLiJT" 
.A 1 is recorded 
as a positive transition 
A 0 is recorded 
as a negative transition 
Figure 12.33 Phase encoded 
recording (PE). 
it an optimum packing density of 100%. NRZ1 isn't 
self-clocking and it's impossible to reliably retrieve a long 
string of 0s. 
Phase encoding 
Several codes are based on phase or Manchester encoding, which 
was once widely used by magnetic tape transports. A flux tran-
sition is located at the center of each bit cell: a low-to-high tran-
sition indicates a 1 and a high-to-low transition a 0. Because 
there's always a flux transition at the center of each data cell, a 
clock signal can be derived from the recorded data and therefore 
this encoding technique is self-clocking. A stream of alternate Is 
and 0s requires one flux transition per bit, whereas a stream of 
1 s or 0s requires two flux changes per bit. 
Fig. 12.33 illustrates how the sequence 01010011 is phase 
encoded. Phase encoding has a low efficiency of 50% because 
up to two transitions per bit is required. Because up to two 
flux transitions are required per bit, the maximum recorded 
frequency is twice that of NRZ1 at an equivalent bit density. 
Phase encoding has a good immunity to noise. Phase encod-
ing is widely used in digital data transmission systems as well 
as magnetic recording systems. 
Modified frequency modulation 
Modified frequency modulation, (MFM) (also called Miller 
encoding and double density recording) became a standard for 
the recording of data on floppy disks. MFM is 100% efficient 
and needs only one flux transition per bit. 
Figure 12.34 demonstrates how a data stream may be 
divided conceptually into two separate signals: a timing sig-
nal consisting of a pulse at each cell boundary, and a data 
signal consisting of a pulse at the center of each data cell 
containing a 1. 
A data pulse is placed at the center of each cell containing a 
1. The clock pulses at the boundary of the cells are deleted, 
but with one exception. Whenever two 0s are to be recorded 
in succession, a clock pulse is placed between them (see 
Fig. 12.34). Because the maximum gap between flux transi-
tions is no more than 2 T, where T is the width of a data cell, 
MFM is self-clocking. 
Serial data 
to be recorded 
Clock pulses 
Data pulses 
Deleted clock pulses 
Combined clock 
and data pulses 
Current in write head 
during recording 
Voltage from read 
head during playback 
Data window during 
which state of signal 
from head is decoded 
^ ^ J J U U U U 
f 
-If 
it 
K ir 
4 
A dfeta puise 
esa 1„ 
lilt 
\ cioc 
>lacec 
oO: 
? v 
ft putie di t| 
between tw| 
clock 
a 0s ehsi 
gnat 
V 
1 
puis* is 
Pk 
Data window during 
HI 
HI 
HI 
HI 
HI 
HI 
HI 
I—I 
which state of signal 
J
U
U
U
U
U
U
L
J
L 
from head is decoded 
Figure 12.34 Modified frequency modulation (MFM). 
Croup codes and RLL codes 
An encoding technique found in both magnetic disk and tape 
stores is the group code, which gained popularity in the early 
1970s when IBM first adopted it for their tape systems. 
Simple coding schemes assign a particular waveform to each 
bit to be recorded, which proves incompatible with some of 
the requirements of an optimum code. A group code takes n 
bits to represent an m-bit source word, where n> m. 
Although there are 2" possible code words, only 2m of these 2" 
values are used to create 2m different waveforms for recording 
on the tape or disk. Waveforms with poor characteristics can 
be removed from the code words to be stored on the tape or 
disk; that is, only the best waveforms are used to store data. 
The 4/5 group code in Table 12,5 uses 5 bits to encode 4 bits 

524 
Chapter 12 Computer memory 
Input code 
Output code 
0000 
11001 
0001 
11011 
0010 
10010 
0011 
10011 
0100 
11101 
0101 
10101 
0110 
10110 
0111 
10111 
1000 
11010 
1001 
01001 
1010 
01010 
1011 
01011 
1100 
11110 
1101 
01101 
1110 
01110 
1111 
01111 
Table 12.5 ANSI X3.54 4/5 group 
code. 
of data. The algorithm that maps the 4 bits of data onto the 
5-bit group code to be recorded avoids the occurrence of 
more than two 0s in succession. This group code and a self-
clocking modification of NRZ1 guarantees at least one flux 
transition per three recorded bits. 
Another class of recording codes are the RLL or run-length 
limited codes. Instead of inserting clock pulses to provide 
timing information as in MFM recording, RLL codes limit 
the longest sequence of 0s that can be recorded in a burst. 
Because the maximum number of 0s in succession is fixed, 
timing circuits can be designed to reliably locate the center of 
each bit cell. A run-length limited code is expressed as Rm „, 
where m defines the minimum number of 0s and n the max-
imum number of 0s between two Is. 
A typical RLL code is RLL 2,7 which means that each 1 is 
separated from the next 1 by two to seven 0s. In RLL a maxi-
mum of four 0s may precede a 1 and three 0s may follow a 1. 
Because RLL records only certain bit patterns, the source data 
must be encoded before it can be passed to the RLL coder; for 
example, the source pattern 0011 would be converted to 
00001000. 
Figure 12.35 illustrates the RLL 2,7 encoding algorithm. 
You take the source code and use its bits to locate a terminal 
node on the tree. Suppose the source string is 0010110 . . . 
The first bit is zero and we move down the zero branch from 
Start. The second bit is 0 and we move down the 0 branch to 
the next junction. The third bit is 1 and we move to the next 
junction. The fourth bit is 0 and we move along the 0 branch. 
This is a terminal node with the value 00100100; that is, the 
encoded value of the input sequence 00100. 
Example 
Suppose the input code is 
1100101011 
This is re-arranged as 
110010 011 
The output is 
100000100100001000 
00100100 
) 0001000 
Figure 12.35 RLL 2,7 encoding algorithm. 
The next bit in the input sequence is 1 and we move from 
Start along the 1 branch. The second bit is 1 and that leads us to 
a terminal node whose output code is 1000. This process con-
tinues until we reach the end of the input code and each group 
of 2,3, or 4 input bits have been replaced by a terminal value. 
12.7 Disk drive principles 
We now look at the construction and characteristics of the 
disk drive. The hard disk stores data on the surface of a flat, 
circular, rigid platter of aluminum coated with a thin layer of 
magnetic material.1 Hard disks vary in size from 8 inches 
(obsolete) to VA and 5'A inches (PCs) to 1.3 to 2'A inches (lap-
tops and portable devices). The platter rotates continually 
about its central axis in much the same way as a black vinyl 
disk on the turntable of a gramophone (for readers old 
enough to remember the days before the CD). The rotational 
speed of disks in PCs was 3600 rpm, although 7200 rpm is 
now common and some disks rotate at 15 000 rpm. 
The read/write head is positioned at the end of an arm 
above the surface of the disk. As the disk rotates, the 
read/write head traces a circular path called a track around 
the disk. Digital information is stored along the concentric 
tracks (Fig. 12.36). Data is written in blocks called sectors 
along the track. Track spacing is of the order of 120 000 
tracks/inch. As time passes, track spacing will continue to 
improve, whereas the speed of rotation will not grow at any-
thing like the same rate. 
Figure 12.37 illustrates the structure of a disk drive. A sig-
nificant difference between the vinyl record and the magnetic 
1 Some modern platters are made of glass because of its superior 
mechanical properties such as a low coefficient of thermal expansion. 
1)1001 
^§)010C 
10010( 
^YSnmnnn 
g)000100 
y 
ft 
must be encoded before it can be passed to the RLL coder; for 

12.7 Disk drive principles 
525 
HISTORY OF DISK DRIVES 
The first high-speed magnetic storage devices were magnetic 
drums where data was recorded on the surface of a rotating 
cylinder. Magnetic drums were used in Manchester 
University's Mark 1 computer in 1948. In 1950 ERA built the 
world's first commercially produced computer for the USA 
navy, the ERA 1101, which used a magnetic drum to store 
over 1 million bits. 
In 1956 IBM introduced its 305 RAMAC (Random Access 
Method of Accounting and Control) computer, which 
incorporated the first disk drive. The RAMAC's disk drive stored 
5 million 7-bit characters on 50 24-inch rotating disks. 
In the 1960s IBM invented the first disk drive with 
removable storage media and in 1973 IBM shipped their first 
W/rtc/iesterdisk drive. The Winchester disk drive is the 
forerunner of all today's hard disk drives. 
In 1980 Seagate Technology introduced the ST506, the 
first disk drive for PCs.This was a 5V< inches disk drive with a 
capacity of 5 Mbytes. 
Track 
Sector 
Disk 
A sector is the smallest unit 
of data that can be written to 
or read from a disk 
Figure 12.36 Structure of a disk. 
Rotation 
Disk 
Track 
(path of the disk 
under the head) 
Read/write 
head 
Spindle 
(driven by 
a motor) 
The actuator moves 
the head assembly 
in or out to select a 
given track 
Figure 12.37 Principle of the disk drive. 
disk is that the groove on the audio disk is physically cut into 
its surface, whereas the tracks on a magnetic disk are simply 
the circular paths traced out by the motion of the disk under 
the read/write head. Passing a current through the head mag-
netizes the moving surface of the disk and writes data along 
the track. Similarly, when reading data, the head is moved to 
the required track and the motion of the magnetized surface 
induces a tiny voltage in the coil of the read head. 
A precision servomechanism called an actuator moves 
or steps the arm holding the head horizontally along a radius 
from track to track. An actuator is an electromechanical 
device that converts an electronic signal into mechanical 
motion. Remember the difference between the magnetic disk 
and the gramophone record. In the former the tracks are 
concentric and the head steps from track to track, whereas in 
the latter a continuous spiral groove is cut into the surface 
of the disk and the stylus gradually moves towards the center 
as the disk rotates. The actuator in Fig. 12.37 is a linear actua-
tor and is no longer used in hard disks. 
Modem disk drives use a rotary head positioner to move the 
read/write heads rather than the linear (in and out) position-
ers found on earlier hard disk drives. Figure 12.38 shows how 
a rotary head positioner called a voice coil actuator rotates an 
arm about a pivot, causing the head assembly to track over 
the surface of the disks. A voice coil is so called because it 
works like a loudspeaker. A current is passed through a coil 
£:—) Actuator) 

526 
Chapter 12 Computer memory 
Read/write head 
Voice coil 
Actuator shift 
Magnet 
When a current flows through 
the voice coil, it is either attracted 
to 
the 
magnet 
or 
repelled 
(depending on the direction of 
the current). One end of an arm 
is connected to the voice coil and 
the other end of the arm carries 
the read/write heads. The arm is 
pivoted on a shaft so that the 
heads move across the disk and 
the voice coil moves in or out. 
Actuator arm 
Figure 12.38 A head assembly 
positioning mechanism. 
Read/write 
head 
Disk drive 
housing 
Four platters 
• Actuator 
Control electronics 
Figure 12.39 Structure of a the disk drive. 
positioned within a strong magnetic field provided by a 
permanent magnet. The current in the coil generates a mag-
netic field, causing the coil to be attracted to, or repelled by, 
the fixed magnet, moving the pivoted arm. The multiple head 
arrangement of Fig. 12.38 means that the hard disk can access 
the same track on several surfaces simultaneously. These 
tracks form a cylinder. 
The characteristics of disk drives vary from manufac-
turer to manufacturer and are being improved on at an 
immense rate. A high-performance disk drive of the late 
1990s had a rotational speed of 5400 rpm (i.e. 90 revolutions 
per second), a capacity of 9 Gbytes (approximately 1036 bits), 
and an average seek time of 8 ms (seek time is the time 
taken to locate a given track) and could transfer data to the 
computer at over 10 Mbytes per second. A decade earlier, a 
typical hard disk in a PC had a capacity of 20 Mbytes and an 
access time of over 70 ms. During the 1990s, average disk 
storage densities were increasing at a phenomenal rate of 
about 70% per year compounded. The improvement in 
access time and data rate over the same period grew at a more 
modest 7% per year. By 2005 disk drives with a capacity of 
500 Gbytes were available and the standard drive speed was 
7200 rpm. 
Prior to the mid-1990s, disk drives were expensive items 
that often cost more than the CPU and main memory. Even 
today, a high-capacity, fast, state-of-the-art hard disk is one 
of the most expensive components in a computer. The cost of 
a disk drive lies in its complex and precise mechanical 
structure. Manufacturers have reduced the effective cost per 
megabyte of disk drives by stacking two or more disks on a 
common spindle and using multiple heads as described by 
Fig. 12.39. A drive might have three disks with six surfaces 
and six heads that move together when driven by the com-
mon actuator. The motion of the heads over the correspond-
ing tracks on each of the surfaces describes a cylinder. 
The parameters of a rigid disk are impressive. The mag-
netic layer is about 2000 atoms deep and the read/write head 
is positioned 0.01 u,m above the surface of the platter. On top 
of the magnetic layer is a lubricating layer of a fluorocarbon 
that is about one molecule thick. The structure of the heads 
themselves is quite complex. They must not only have the 
correct electrical and magnetic properties, but also the cor-
rect mechanical properties. If the head were actually in phys-
ical contact with the disk surface, the abrasive magnetic 
coating would soon wear it out because its velocity over the 
surface of the disk is of the order of 140 mph at 15 000 rpm. 
The head is mounted in a holder called a slipper positioned 
above the disk at about 0.01 (Jim from the surface. We cannot 
directly achieve such a level of precision with current engi-
neering technology. However, by exploiting the head's aero-
dynamic properties it can be made to fly in the moving layer 
of air just above the surface of the disk. 
When an object moves, the air near its surface, called the 
boundary layer, moves with it. At some distance above the sur-
face the air is still. Consequently a velocity gradient exists 
between the surface and the still air. At a certain point above the 
disk's surface, the velocity of the air flowing over the head gen-
erates enough lift to match the pressure of the spring pushing 
the head towards the disk. At this point, the head is in equilib-
rium and floats above the disk. Modern slippers fly below 
10 X 10~9m (i.e. 0.01 p,m) and have longitudinal grooves cut 
in them to dump some of the lift. The precision of a modern 
slipper is so great that the acid in a fingerprint caused by care-
less handling can destroy its aerodynamic contour. 

12.7 Disk drive principles 
527 
WINCHESTER DISK DRIVES 
Hard disk drives in the early 1980s found in compact, 
low-cost minicomputers and high-performance 
microprocessor systems were often called Winchester disks. 
The generic term Winchester describes a wide range of small 
disk drives and there appears to be no single feature that 
makes a drive a Winchester. The term is associated with IBM 
and some say it's related to the town of Winchester. Most say 
it's a reference to the 30-30 Winchester rifle because the 
original drive had two spindles, each holding 30 Mbytes. 
Winchester technology was originally applied to 14-inch disks 
and then extended to 8-51/4-3'/?-and the 2Vz-inch drives 
found in laptop computers. Although modem drives 
incorporate the features of the original Winchester drives, the 
term Winchester is seldom used today. 
As the recording density increased and inter-track spacing 
reduced, it became increasingly necessary to ensure that the 
head flies exactly over the track it is accessing.This led to 
increasingly complex head-positioning mechanisms and their 
associated electronics. Winchester technology solved the 
problem of head tracking by making the disks, read/write 
heads, and positioner, an integral unit. Earlier large hard drives 
had replaceable disk packs. Winchester disks cannot be 
changed so the problem of trying to follow a track on a disk 
written by another unit doesn't arise. Because the head disk 
assembly requires no head alignment, the track spacing can be 
reduced and the storage density increased. The Winchester 
disk drive is a sealed chassis that stops the entry of dirt and 
dust. Most drives have a small hole in the unit protected by an 
air filter to equalize internal and external air pressures. As the 
disk rotates in a clean environment, the flying height of the 
head can be reduced, and the recording density increased. 
Unlike earlier hard disk drives, it is not necessary to retract 
the heads beyond the outer rim of the disks when the unit is 
not in use. Because the heads fly only when the disks are 
rotating and aren't retracted when the disk is stationary, it's 
necessary to allocate a portion of the disk's surface as a 
landing area. That is, the heads are permitted to come into 
contact with (i.e. land on) a part of the disk where data is not 
stored. In order to make this possible it is necessary to 
lubricate the surface of the disk. Such disks must be brought 
up to speed (and stopped) as quickly as possible to reduce the 
time for which the heads are in contact with the disks. 
The height at which the head flies above the surface of the 
disk is related to the surface finish or roughness of the mag-
netic coating. If the magnetic material is polished, the surface 
to head gap can be reduced by 50% in comparison with an 
unpolished surface. 
Occasionally, the head hits the surface and is said to crash. 
A crash can damage part of the track and this track must be 
labeled bad and the lost data rewritten from a back-up copy 
of the file. 
The disk controller (i.e. the electronic system that controls 
the operation of a disk drive) specifies a track and sector and 
either reads its contents into a buffer (i.e. temporary store) or 
writes the contents of the buffer to the disk. Some call a disk 
drive a random access device because you can step to a given 
track without first having to read the contents of each track. 
Disk drives are sequential access devices because it is neces-
sary to wait until the desired sector moves under the head 
before it can be read. 
12.7.1 Disk drive operational 
parameters 
Disk drive users are interested in three parameters: the total 
capacity of the system, the rate at which data is written to or 
read from the disk, and its average access time. In the late 
1990s typical storage capacities were 14 Gbytes, data rates 
were several Mbytes/s and average access times from 8 ms to 
\2 ms. By the end of the century, data densities had reached 
10 Gbits/in2 and track widths of the order of 1 p,m. In 2004 
data densities had reached 100 Gbits/in2 and it was thought 
that densities would increase by a factor of 10 to yield 
1 Tbits/in2 within a decade. 
Access time 
A disk drive's average access time is composed of the time 
required to step to the desired track {seek time), the time taken 
for the disk to rotate so that the sector to be read is under the 
head (latency), the time for the head to stop vibrating when it 
reaches a track (settle time), and the time taken to read the data 
from a sector (read time). "We can represent access time as 
^access 
^seek ~*~ latency 
^settle 
*read 
The average time to step from track to track is difficult 
to determine because the modern voice coil actuated head 
doesn't move at constant velocity and considerations such as 
head settling time need to be taken into account. Each seek 
consists of four distinct phases: 
• acceleration (the arm is accelerated until it reaches 
approximately halfway to its destination track) 
• coasting (after acceleration on long seeks the arm moves at 
its maximum velocity) 
• deceleration (the head must slow down and stop at its 
destination) 
• settling (die head has to be exactly positioned over the 
desired track and any vibrations die out). 
Designing head-positioning mechanisms isn't easy. If you 
make the arm on which the head is mounted very light to 

528 
Chapter 12 Computer memory 
Link between 
sectors 
In a random access file, each 
sector in the file has a pointer to 
the next sector. This means that 
sectors can be read one-after-
another without having to read all 
sectors. However, it is necessary 
to perform a new seek between 
each read. 
Modern disk systems read sectors 
before they are needed and store 
them in a buffer. 
Figure 12.40 The arrangement 
of the sectors of a file. 
improve the head assembly's acceleration, the arm will be too 
flimsy and twist. If you make the arm stiffer and heavier, it 
will require more power to accelerate it. 
The average number of steps per access depends on the 
arrangement of the data on the disk and on what happens to 
the head between successive accesses. If the head is parked at the 
periphery of the disk, it must move further on average than if it 
is parked at the center of the tracks. Figure 12.40 shows a file 
composed of six sectors arranged at random over the surface of 
the disk. Consequently, the head must move from track to track 
at random when the file is read sector by sector. 
A crude estimate of the average stepping time is one-third 
the number of tracks multiplied by the time taken to step 
from one track to the adjacent track. This figure is based on 
the assumption that the head moves a random distance from 
its current track to its next track each time a seek operation 
is carried out. If the head were to be retracted to track 0 after 
each seek, the average access time would be half the total 
number of tracks multiplied by the track-to-track stepping 
time. If the head were to be parked in the middle of the tracks 
after each seek, the average access time would be 1/4 of the 
number of tracks multiplied by the track-to-track stepping 
time. These figures are valid only for older forms of actuators. 
Very short seeks (1 to 4 tracks) are dominated by head sett-
ling time. Seeks in the range 200 to 400 tracks are dominated 
by the constant acceleration phase and the seek time is pro-
portional to the square root of the number of tracks to step 
plus the settle time. Long seeks are dominated by the constant 
velocity or coasting phase and the seek time is proportional 
to the number of tracks. 
A hard disk manufacturer specifies seek times as minimum 
(e.g. 1.5 ms to step one track), average (8.2 ms averaged over 
all possible seeks), and maximum (17.7 ms for a full stroke 
end-to-end seek). These figures are for a 250 Gbyte Hitachi 
Deskstar. 
The access time of a disk is made up of its seek time and the 
time to access a given sector once a track has been reached 
(the latency). The latency is easy to calculate. If you assume 
that the head has just stepped to a given track, the minimum 
latency is zero (the sector is just arriving under the head). 
The worst case latency is one revolution (the head has just 
missed the sector and has to wait for it to go round). On aver-
age, the latency is 1/2 frev, where rrev is the time for a single rev-
olution of the platter. If a disk rotates at 7200 rpm, its latency 
is given by 
~ X 1/(7200^-60) = 0.00417 s = 4.17 ms 
An important parameter is the rate at which data is 
transferred to and from the disk. If a disk rotates at R revolu-
tions per minute and has s sectors per track, and each sector 
contains B bits, the capacity of a track is B • s bits. These 
B-s bits are read in 60/R seconds giving a data rate of 
B-sl(60IR) = B-sRI60 bits/s. This is, of course, the actual 
rate at which data is read from the disk. Buffering the data in 
the drive's electronics allows it to be transmitted to the host 
computer at a different rate. 
The length of a track close to the center of a disk is less than 
that of a track near to the outer edge of the disk. In order to 
maximize the storage capacity, some systems use zoning in 
which the outer tracks have more sectors than the inner 
tracks. 
Modern disk drives must be tolerant to shock (i.e. acceler-
ation caused by movement such as a knock or jolt). This 
requirement is particularly important for disk drives in 
portable equipment such as laptop computers. Two shock 
parameters are normally quoted. One refers to the tolerance 
to shock when the disk is inoperative and the other to shock 
while the disk is running. Shock can cause two problems. One 
is physical damage to the surface of the disk if the head 
crashes into it (this is called head slap). The other is damage 
to data structures if the head is moved to another track dur-
ing a write operation. Shock sensors can be incorporated in 
the disk drive to detect the beginning of a shock event and 
disable any write operation in progress. 
Sector 
•V-Track 

12.7 Disk drive principles 
529 
THE DISK ELECTRICAL INTERFACE 
A disk drive's electrical interface defines the way in which it 
communicates with the host computer. When hard disk drives 
first appeared, data was transferred between the computer 
and drive in the form it was to be written or read (i.e. the raw 
digital pulses). Disk drives were unintelligent and lacked 
sophisticated control systems. The signal processing 
electronics was located on an interface card in the computer. 
All signal processing is now carried out by circuits located 
inside the disk drive housing. 
Disk drives are described in terms of the interface between 
drive and the computer; for example, IDE (integrated drive 
electronics), serial ATA (AT attachment), or SCSI (small 
computer system interface). 
The first interface for disks in PC systems was the ST-506 
from Seagate technologies, which used two ribbon cables to 
connect an unintelligent disk to a controller card in the PC. 
This bus was replaced by the ESDI interface (an improved 
version of ST-506) in the mid-1980s. 
The IDE (integrated drive electronics) interface was the first 
high-performance PC disk interface introduced in 1986 to 
support disks up to 528 Mbytes at rates up to 3 Mbytes/s. In 
the late 1990s the E-IDE (enhanced IDE) interface was 
designed to handle disks up to 9.4 Cbytes and data rates up to 
20 Mbytes/s. Over the years, the IDE interface has been devel-
oped and some of its variants are called ATA, ATA/ATAPI, EIDE, 
ATA-2, Fast ATA, ATA-3, UltraATA, and Ultra DMA. 
IDE is little more than a parallel data highway that copies 
data between the PC's AT bus and the disk drive. The drive 
control electronics is now located where it belongs, in the 
disk drive. 
The general-purpose small computer systems interface 
(SCSI) connects up to eight independent disk drives or similar 
peripherals to a computer. SCSI interfaces are associated with 
high-performance computers and a special controller is 
required to operate the SCSI bus.The SCSI-1 standard, 
adopted in 1986, defines how data is transferred over the SCSI 
bus. As in the case of the IDE interface, the SCSI standard has 
been amended to provide a higher level of performance. The 
original SCSI interface had an 8-bit data path and operated 
at 2 Mbps (2 million bits/s). A new standard, SCSI-2, was 
introduced in 1990 to provide synchronous data transfers at 
10 Mbps. A SCSI-2 option called wide SCSI provides a 
16-bit data path and a maximum transfer rate of 20 Mbps. 
The SCSI-3 standard now supports data rates up to 
80 Mbps. 
In 2002 the Serial ATA interface was introduced into 
PCs. This interface is a serial version of the IDE interface 
which very much simplifies the integration of hard drives into 
a PC because a serial connector is less bulky than an IDE's 
bulky ribbon connector. In 2004 a second-generation 
serial interface with a data rate of 300 Mbps was 
introduced. 
AUDIO VISUAL DRIVES 
In the mid-1990s three things happened to PCs; their speed 
increased to the point at which they could process audio and 
video signals, the capacity of hard disks became sufficient to 
store over an hour of video, and computing entered an 
audio-visual age. Conventional drives suffer from data 
discontinuity when there is a short gap during a stream of 
data. Data processing applications require a low average 
access time and it doesn't matter if there are infrequent short 
gaps in the data stream. When data represents sound or 
moving images, the ear or the eye can detect even tiny 
interruptions. 
Because data elements are very small, tiny imperfections in 
the magnetic media cause errors in the data stream when 
data is read from a disk. Powerful error-correcting codes are 
used to protect the stored data. On readback, the data from 
the disk is processed and errors automatically corrected. A 
conventional disk might take 800 ms to recover from an error. 
Disk manufacturers created the so-called audio-visual (A/V) 
drive which employs the same storage technology as 
conventional drives, but uses high-speed error-correction 
hardware and algorithms. 
As the density of bits on platters has increased, the thermal 
characteristics (i.e. expansion or contraction with temperature 
changes) of the disk and read/write mechanism have become 
more important.Temperature changes affect the head's ability 
to follow a track. Some disk drives include thermal calibration 
that periodically compensates for temperature changes. This 
calibration takes place every few minutes and is invisible to 
the user. However, it does cause an interruption of about 0.1 s 
in the data flow. A/V disks perform thermal calibration intelli-
gently and delay calibration if a data request is pending. If 
thermal calibration is taking place and data is requested, the 
drive reschedules the recalibration process and immediately 
begins to access the data. 
An important parameter of the disk drive is its mean time 
between failure (MTBF), which is the average time between 
failures. The MTBF ranges from over 1 000 000 hours for 
large drives to 100 000 hours for smaller and older drives. A 
100 000-hour MTBF indicates that the drive can be expected 
to operate for about IVA years continually without failure—a 
value that is longer than the average working life of a PC. 
A disk with a MTBF of 1 000 000 hours can be expected to 
run for over 100 years. 
12.7.2 High-performance drives 
Several technologies have been used to dramatically increase 
the performance of disk drives. Here we discuss two of them: 

530 
Chapter 12 Computer memory 
PROGRESS 
In 1980 IBM introduced the world's first 1 Gbyte disk drive, 
the IBM 3380, which was the size of a refrigerator, weighed 
550 pounds, and cost $40 000. In 2000 IBM introduced a 
1-Gbyte microdrive, the world's smallest hard disk drive with a 
platter that's about the size of an American quarter. 
In 2002 IBM dropped out of the disk drive 
market and merged its disk drive division with 
Hitachi to create HGST (Hitachi Global Storage 
Technologies). 
Figure 12.41 Data density as a function of time (from HGST). 
magnetoresistive head technology and partial response 
maximum likelihood data demodulation (PRML).2 Figure 12.41 
shows the increase in areal densities for IBM disk drives since 
1980 and the recent growth rate made possible largely 
through the use of magnetoresistive heads. 
The magnetoresistive head 
The ultimate performance of a disk drive using the tradi-
tional read head we described earlier is limited because the 
recording head has to perform the conflicting tasks of writing 
data on the disk and retrieving previously written data. As the 
bit patterns recorded on the surface of disks have grown 
smaller, the amplitude of the signal from the read head has 
been reduced making it difficult for the drive's electronics to 
identify the recorded bit patterns. You can increases the read 
signal enough to determine the magnetic pattern recorded on 
the disk by adding turns around the magnetic core of the 
head because the read signal is proportional to the number of 
turns. However, increasing turns also increases the head's 
inductance—the resistance of a circuit to a change in the 
current flowing through it. A high 
inductance limits the frequency with 
which the current reversals can occur 
during write operations. 
Magnetoresistive head technology 
uses separate read and write heads. 
An inductive head, optimized for 
writing information, is integrated 
with a magnetoresistive structure 
optimized for reading. Each of the 
two elements can be optimized to 
perform its particular function— 
reading or writing data 
A magnetoresistive head operates 
in a different way to conventional 
read heads. In a conventional head, a 
change in magnetic flux from the 
disk induces a voltage in a coil. In a 
magnetoresistive 
head, 
the 
flux 
modifies the electrical resistance of a 
conductor (i.e. more current flows through the conductor 
when you apply a voltage across it). Lord Kelvin discovered 
this phenomenon, called anisotropic magnetoresistance, in 
1857. The read element of an MR head consists of a minute 
stripe of a permalloy material (a nickel-iron compound, 
NiFe) placed next to one of the write element's magnetic pole 
pieces. The electrical resistance of the permalloy changes by a 
few percent when it is placed in a magnetic field. This change 
in the material's resistance allows the MR head to detect the 
magnetic flux transitions associated with recorded bit pat-
terns. During a read operation, a small current is passed 
through the stripe of resistive material. As the MR stripe is 
exposed to the magnetic field from the disk, an amplifier 
measures the resulting voltage drop across the stripe. 
In the 1980s a phenomenon called the giant magnetoresis-
tive effect was discovered which provided a much greater 
sensitivity than the conventional magnetoresistivity. By 1991 
2 A description of PRML is beyond the scope of this book. PRML 
encoding places pulses so close together that the data from one pulse 
contains interference from adjacent pulses. Digital signal processing 
algorithms are used to reconstruct the original digital data. 
HGST areal density perspective 
Perpendicular 
1 06 
I r ^ e l s t . y _ g O g i ^ e c o r d i n g 
Future areal 
I 
Deskstar 180GXP 
\ . 
\ 
"'A 
density 
105 '. 
-EitsLAFfmpHp 
^
y 
J^gZ^L 
progress 
I 
SuperpAramagnetjc 
TcayfilsIar.SQ CM. r T ^ i ^ O - . 7 X 1 
A ' 
effect 
Microdrive II -—<§ra^*\i Mtra-i-ar i A « 7 i n 
104
 r 
-PJrstr6MR-head- 
-
^ 
— 
- - 
4 6 Z 1 ° 
\ 
Deskstar 16CP ^ , 
°
# 
100% C( IR 
I io3; 
0 - . - ^ % _ 
s 
E 
First MR head 
< s r V „ „ , ,-
g? i o 2 s 
Corsair ^
^
j
p
r 
t>u/i>iuK 
1^ 
: 
First thin film head 
_ ^ * " 
35 million X 
• f 
1 Q I 
3375 
- - ^ . ^ 
^
^ 
increase 
£ 
I 
_ ^ * " 
25%CGR 
| 
! " 
--jsr^- 
T 
: — 
, 
"^ 
; 
a ^ " ^ 
® HGST disk drive products 
-jO-1 ~ 
y& 
° Industry lab demos 
: 
A 
* HGST disk drives w/AFC 
2 " 
/_ 
& 
D e m o s w/AFC 
| ^ / I B M RAMAC (first hard disk drive) 
_ 
_ 
10~3 i i i i i I i i i i i i i i i I i i i i i i i i i I 
i i i i i i i i 
i i 
i 
60 
70 
80 
90 
2000 
10 
Production year 

12.7 Disk drive principles 
531 
SUPERPARAMAGNETISM 
Recording density increased by several orders of magnitude 
over a few years. However, such increases cannot continue 
because of the physical limitations of magnetic materials. 
Suppose we decide to scale magnetic media down and make 
the magnetic particles half their previous size. Halving the size 
of particles increases the areal density by 4 because you halve 
both length and width. Halving the size reduces the volume of 
a particle by 8; in turn this reduces the magnetic energy per 
particle by a factor of 8. 
In a magnetic material, particles are aligned with the 
internal field. However, thermal vibrations cause the magnetic 
orientation of the particles to oscillate and some particles can 
IBM was exploiting the effect in their drives. This technology 
became known as GMR and the GMR head is now the stan-
dard in high-performance disk drives. 
12.7.3 RAID systems 
RAID (redundant array of inexpensive disks) technology 
combines separate disk drives into a single system. In the 
1980s and 1990s low-capacity disks were relatively cheaper 
than their high-capacity counterparts—due to the low cost of 
mass-produced disk drives targeted at the PC market. It was 
more cost effective to create a large memory system by using 
several low-capacity drives than by using a single high-
capacity drive. 
In 1987 Garth Gibson, Randy H. Katz, and David A. 
Patterson at the University of California in Berkeley devised 
RAID technology using low-cost disk drives to create a large 
and reliable disk store. There are several variations of the 
RAID technologies called RAID 0, RAID 1, RAID 2,.. ., 
RAID 5. We discuss only RAID 1, RAID 3, and RAID 5 
systems here. 
RAID 0 systems provide performance enhancement rather 
than increasing fault tolerance. The data stream is split into 
blocks and distributed between the drives in a RAID array. 
RAID 1 systems connect two hard drives to a single disk 
controller to provide disk mirroring. Two copies are made of 
all files and each copy can be read independently of the other. 
Read access time is improved because the controller can 
obtain data from the first drive that has it ready. RAID 1 sys-
tems improve reliability because there are two copies of each 
file. If the probability of one drive failing isp(wherep < < 1), 
the probability of both drives failing is p2, which greatly 
enhances reliability. RAID 1 technology is inefficient because 
the amount of disk space required is doubled. 
RAID 3 technology transfers data in parallel to multiple 
disks. RAID 3 systems have n data disks and a separate parity 
disk (n is typically 4). Data in a RAID 3 system is said to be 
spontaneously reverse direction. Reducing a particle's size can 
dramatically increase its tendency to spontaneously change 
state.Accordingtoan IBM paper, halving the size of particles 
can change the average spontaneous flux reversal time form 
100 years to 100 ns! When particle sizes are so small that they 
spontaneously lose their magnetization almost instanta-
neously, the effect is called superparamagnetism.The limit 
imposed by superparamagnetism is of the order of 100 
Gbits/in2. Fortunately, techniques involving the use of com-
plex magnetic structures have been devised to delay the onset 
of superparamagnetism by at least an order of magnitude of 
areal density. 
Stripe 
Diski 
Disk 2 
Disk 3 
Disk 4 
Disk 5 
bit 1 
bit 2 
bit 3 
bit 4 
P 
1
0 
1
0 
0 
1 
2 
1 
1
0 
0 
0 
3 
0 
1
1 
1 
1 
4 
1 
0 
1 
0 
0 
Table 12.6 Principle of the RAID 3 array. 
striped so that a stripe is sent in parallel to each of the n drives. 
A parity byte (generated across the n stripes) is stored on the 
parity disk. If an error occurs in any of the stripes, the missing 
1 
data can be regenerated from the parity information. An 
; 
error in a data block of one of the disks can be detected by the 
z 
error-detecting code used whenever data is stored on disk. 
, 
Let's illustrate the RAID 3 system with a simple example, 
5 
where P represents a parity bit across bits (i.e. disks) 1 to 4. 
Table 12.6 shows four stripes across the five disks. The value 
r 
of stripe 1 is 0100 and its even parity bit is 1, which is stored 
3 
on the parity disk number 5. 
Suppose disk drive 3 in the array fails to give the situation 
{ 
in Table 12.7. The error-detecting codes on disk 3 indicate 
f 
that the data has been corrupted but cannot tell you what the 
data should have been. However, because we still have the 
l 
data on disks 1, 2, and 4 and the parity disk, we can recon-
struct the missing data. For example, stripe 2 is 11?00. In 
I 
order to maintain correct parity the missing bit must be 0 and 
, 
the corrected stripe is 11000. RAID 3 systems require that the 
Y 
heads of the disk be synchronized. 
s 
Another popular implementation of RAID technology is 
the RAID 5 array, which is similar to a RAID 3 array because 
: 
n drives are used to store stripes of data and one is used to 
y 
store a parity stripe. However, the stripes in a RAID 5 system 
e 
are sectors rather than bytes and the parity stripes are distributed 
the RAID 5 array, which is similar to a RAID 3 array because 
RAID 3 technology transfers data in parallel to multiple 
1 
1 
0 
1 
1 
0 

532 
Chapter 12 Computer memory 
Stripe 
Diskl 
bit 1 
Disk 2 
bit 2 
Disk 3 
bit 3 
Disk 4 
bit 4 
Disk 5 
P 
1 
0 
2 
1 
3 
0 
4 
1 
Table 12.7 Correcting an error in a RAID 3 array. 
across the array rather than stored on a specific drive. RAID 5 
systems are more suited to smaller blocks of data (e.g. in net-
work systems) and are simpler because they don't require the 
read/write heads of each of the drives to be synchronized. 
Both RAID 3 and RAID 5 systems can tolerate the 
complete failure of one of the disks in the array. When that 
happens, their error-correcting property vanishes although 
the array can operate (assuming no further errors) until a 
new drive is swapped in. The operator can pull out the failed 
drive and plug in a spare drive to keep the system running 
smoothly. 
By the way, the mean time between failures of an array of 
disks is less than that of a single disk. If you have five drives, 
it's five times more likely that one of them will fail over a given 
period than if you had just one drive. However, the use of 
redundancy (i.e. the ability to tolerate a single failure) in a 
RAID system more than compensates for the increased prob-
ability of a single disk failure because two disks have to fail to 
bring the system down. 
Parameters of a disk drive 
Table 12.8 describes the characteristics of a disk drive that 
represented the state of the art in 2004. We can be fairly 
confident that within 1 or 2 years its capacity will become 
commonplace and within 5 years its capacity will be well 
below that of newer drives. Equally, we can be confident that 
its access time and many of its other characteristics will not 
change much over a 5-year time span. 
12.7.4 The floppy disk drive 
The floppy disk is a removable secondary storage medium 
that can be transported between systems. Floppy disks have 
long access times and low capacities and are almost obsolete. 
Better removable magnetic media such as the Zip drive, 
writable optical storage, and the flash EPROM USB pen 
drive have replaced the floppy disk. We cover the floppy disk 
drive partially for historical reasons and partially because it 
demonstrates magnetic recording principles well. 
The floppy disk drive is an IBM invention dating back to 
the 1960s, when it was first used to load microcode into 
Configuration 
Interface 
Capacity 
Platters 
Heads 
Data buffer 
Rotational speed 
Maximum media transfer rate 
Interface data transfer rate 
Sustained data rate 
Average read seek time 
Non-recoverable hard error rate 
Operating shock (duration 2 ms) 
Parallel-ATA (Serial-ATA) 
UltraATA/133 (SATA 2 
3 Cbytes/s) 
500 Cbytes/s 
5 
10 
16 Mbytes (SATA 2 version) 
7200 rpm 
817Mbytes/s 
133Mbytes/s 
(3 CBytes/s SATA2) 
61.8 to 31 Mbytes/s 
(depends on zone) 
8.5 ms 
1 in 1014 
55 C 
Table 12.8 Parameters of the Deskstar 7K500. 
IBM's 370 computers and later to store information in the 
IBM 3740 Data Entry System. The original floppy disk was 
made of plastic coated with a magnetic material enclosed in 
an 8-inch square protective envelope. The 8-inch floppy disk 
was replaced by the 5K-inch minifloppy disk, which was 
replaced by the 3!^-inch floppy disk, which comes in a more 
robust rigid plastic case. The capacity of an 8-inch floppy 
disk was 300 kbytes and the capacity of a first-generation 
5!^-inch floppy disk was 80 Kbytes; 314-inch floppies store 
1.44 Mbytes (some have capacities of 2.88 Mbytes). Such 
tiny capacities mean that the floppy disk drive can be used 
only to copy small files such as text and emails or device 
drivers. Many modern PCs no longer provide floppy disk 
drives. 
Floppy disks rotate at 360 rpm, about 5% the speed of a 
hard disk drive, to reduce the frictional heating of the disk in 
its envelope. This gives a rotational latency of 166 ms. 
A 3/4-inch floppy disk's read/write head is moved to the 
desired track by a stepping mechanism. The head positioned 
over the disk accesses its surface through a sliding metal win-
dow in a 3/4-inch disk. Floppy drives use two heads to record 
data on both sides of the disk. The head in a floppy disk 
comes into contact with the surface. In order to prevent 
undue wear on the head and the disk's surface, the drive 
motor maybe stopped after a period of disk inactivity. 
The3/4-inch 1.44 Mbyte floppy disk has 80 0.115 mm wide 
tracks of 18 sectors spaced at 135 tracks per inch. Data is 
recorded at an average density of about 17000 bits/in. The 
capacity is expressed as formatted capacity and represents the 
data available to users. It does not include data that performs 
housekeeping tasks such as labeling the track and sector 
number of each sector stored on the disk. In hard disk terms 
this capacity is tiny indeed. 
1 
? 
0 
1 
1 
? 
0 
0 
1 
? 
1 
1 
0 
? 
0 
0 

12.7 Disk drive principles 
533 
Index address 
markv 
Record 1 
Record 2 
Record 1 
ID field 
Gap 3 Record 2 
ID field 
ID 
add'oss Track 
cid.:re=;s 
Side 
number 
:rcr 
ivjn;l«T 
Scctc 
•ngrr, 
CRC 
(2 ty: 
Figure 12.42 Structure of a track. 
Gap 2 Record 2 
data field 
address 
niurk 
2S6 hvtc. user ria'.a i'2 L-vtes' 
12.7.5 Organization of data on disks 
Having described the principles of magnetic recording sys-
tems we now briefly explain how data is can be arranged on a 
disk. This section provides an overview but doesn't describe a 
complete system in detail. Although there is an almost infi-
nite number of ways in which digital data may be organized 
or formatted on a disk, two systems developed by IBM have 
become standard: the IBM 3740-compatible single-density 
recording and the IBM System 34-compatible double-density 
recording. 
A disk must be formatted before it can be used by writing 
sectors along the tracks in order to let the controller know 
when to start reading or writing information. Formatting 
involves writing a series of sector headers followed by empty 
data fields that can later be filled with data as required. 
Figure 12.42 describes the structure of a track formatted 
according to the IBM 34 format double-density system. Gaps 
are required between data structures to allow for variations in 
the disk's speed and time to switch between read and write 
operations. The disk drive is a mechanical device and doesn't 
rotate at an exactly constant speed. Consequently, the exact 
size of a sector will be slightly different each time you write it. 
Second, the drive electronics needs a means of locating the 
beginning of each sector. 
A track consists of an index gap followed by a sequence of 
sectors. The number and size of sectors varies from operating 
system to operating system. Each sector includes an identity 
field (ID field) and a data field. The various information units 
on the disk are separated by gaps. A string of null bytes is 
written at the start of the track followed by an index address 
mark to denote the start of the current track. The address 
mark is a special byte, unlike any other. We've already seen 
that the MFM recording process uses a particular algorithm 
to encode data. That is, only certain recorded bit patterns are 
valid. By deliberately violating the recording algorithm and 
recording a bit pattern that does not conform to the set of 
valid patterns, uniquely identifiable bit patterns can be cre-
ated to act as special markers. Such special bit patterns are 
created by omitting certain clock pulses. 
The sectors following the index gap are made up of an ID 
(identification) address mark, an ID field, a gap, a data field, 
and a further gap. The ID field is 7 bytes long including 
the ID address mark. The other 6 bytes of the address field 
are the track number, the side number (0 or 1), the sector 
address, the sector length code, and a 2-byte cyclic redun-
dancy check (CRC) code. The 16-bit CRC provides a power-
ful method of detecting an error in the sector's ID field and 
is the 16-bit remainder obtained by dividing the polynomial 
representing the field to be protected by a standard generator 
polynomial. 
The beginning of the data field itself is denoted by one of 
two special markers: a data address mark or a deleted data 
address mark (these distinguish between data that is active 
and data that is no longer required). Following the data 
address mark comes a block of user data (typically 128 to 
1024 bytes) terminated by a 16-bit CRC to protect the data 
field from error. The data field is bracketed by two gaps to 
provide time for the write circuits in the disk to turn on to 
write a new data field and then turn off before the next sector 
is encountered. Gap 2 must have an exact size for correct 
operation with a floppy disk controller, whereas gaps 1,3, and 
4 are simply delimiters and must only be greater than some 
specified minimum. 
Disk data structures 
The large-scale structure of information on disks belongs 
to the realm of operating systems. However, now that 
we've come so far we should say something about files. 
Conceptually, we can imagine that a filing system might 
require three data structures: a list of sectors available to the 
filing system (i.e. the free sectors), a directory of files, and the 
files themselves. 
I Gap 4 
Cap 1 
r , „ , Record 1 
GaPZ|data field 

534 
Chapter 12 Computer memory 
PROBLEM 
A 3'A-inch floppy disk drive uses two-sided disks and records 
data on 80 tracks per side. A track has nine sectors and each 
holds 512 bytes of data. The disk rotates at 360 rpm, the 
seek time is 10 ms track to track, and the head settling time 
is 10 ms. From the above information calculate the 
following. 
(a) The total capacity of the floppy disk in bytes. 
(b) The a verage ro ta tional latency. 
(c) The average time to locate a given sector assuming that 
the head is initially parked at track 0. 
(d) The time taken to read a single sector once it has been 
located. 
(e) The average rate at which data is moved from the disk to 
the processor during the reading of a sector. This should 
be expressed in bits per second. 
(f) The packing density of the disk in terms of bits per inch 
around a track located at 3 inches from the center. 
(a) Total capacity = sides X tracks X sectors X 
bytes/sector = 2 X 80 X 9 X 512 = 737 280 bytes 
(called 720 Kbyts). 
(b) Average rotational latency = Vz period of revolution 
360 rpm corresponds to 360/60 = 6 revolutions per 
second one revolution = 1/6 second 
average latency is therefore 1/12 second = 83.3 ms. 
(c) Average time to locate sector = latency + head settling 
time + seek time = 83.3 ms + 10 ms + 80/2 X 10 ms 
(d) In one revolution (1/6 second), nine sectors pass under 
the head. Therefore, time to read one sector is 
1/6 X 1/9= 18.52 ms. 
(e) During the reading of a sector, 512 bytes are read in 
18.52 ms.The average data rate is the number of bits 
read divided by the time 
taken = (512 X 8)/0.01852 = 221 166 bits/s. 
(f) Packing density = total number of bits divided by track 
length = 9 X 512 X 8/(2 X 3.142 X 1.5) = 1955.4 bits/in. 
You can recover so-called deleted files as long 
as they haven't been overwritten since they 
were removed from the directory and their sec-
______^_^ 
^ r ^ u i 
t o r s returned to the pool of free sectors. 
•(T)1 1 0 0 0 1 1 A 1 0 0 0 0 0 0 0 0 / 
There's little point in storing data on a disk 
" 
• 
' 
unless it can be easily accessed. To achieve this 
objective, a data structure called a directory 
holds information about the nature of each file 
and where the file can be found. Information 
in directories varies from the file name plus the 
location of the first sector of the file to an 
Figure 12.43 Free sector list. 
The free sector list 
A simple method of dealing with the allocation of sectors to 
files is to provide a bit-map (usually in track 0, sector 1). Each 
bit in the bit-map represents one of the sectors on the disk 
and is clear to indicate a free sector and set to indicate an 
allocated sector. Free means that the sector can be given to a 
new file and allocated means that the sector already belongs to 
a file. If all bits of the bit-map are set, there are no more free 
sectors and the disk is full. Figure 12.43 illustrates the free 
sector list. 
Suppose the disk file manager creates a file. It first searches 
the bit-map for free sectors, and then allocates the appropri-
ate number of free sectors to the new file. When a file is 
deleted, the disk file manager returns the file's sectors to the 
pool of free sectors simply by clearing the corresponding bits 
in the bit-map. The sectors comprising the deleted file are not 
overwritten when the file is deleted by the operating system. 
extensive description of the file including attributes such as 
file ownership, access rights, date of creation, and date of last 
access. 
The sectors of a file can be arranged as a linked list in which 
each sector contains a pointer to the next sector in the list as 
Fig. 12.44 demonstrates. The final sector contains a null 
pointer because it has no next sector to point to. Two bytes are 
required for each pointer; one for the track number and one 
for the sector number. The advantage of a linked list is that 
the sectors can be randomly organized on the disk (random-
ization occurs because new files are continually being created 
and old files deleted). 
Linked lists create sequential access files rather than ran-
dom access files. The only way of accessing a particular sector 
in the file is by reading all sectors of the list until the desired 
sector is located. Such a sequential access is, of course, highly 
inefficient. Sequential access files are easy to set up and a 
sequential file system is much easier to design than one that 
caters for random access files. 
Free sector list 
in track 0 sector 1 
E a c h bjt i n the list determines 
whether the corresponding 
f 
sector is free or has been allocated 
I 
Sector 1 
\ 
Sector 2 
Sector 3  
\ 1 1 1 1 0 1 1 1 o ) \ + © 1 1 0 0 0 1 1 A 1 0 0 0 0 0 0 0 0 
vV^V 
Track 1 sector 2 
\ 
Na 
allocated 
Track 1 sector 6 
Three contiguous 
unallocated 
unallocated sectors 
Three contiguous 
unallocated sectors 
Track 1 sector 6 
unallocated 

12.7 Disk drive principles 
5 3 5 
Sector 1,1 
Sector 1,2 
1,3 
Sector 1,3 
2,3 
Each sector points 
to the next sector 
in the chain 
Sector 2,3 
Sector 2,4 
2,4 
Sector 2,7 
2.7 
o,c 
Figure 12.44 Linked list of 
sectors. 
(a) The file allocation table. 
Sector cluster 
3 
4 
5 
5 
6 
6 
8 
7 
9 
8 
7 
9 
FFFF 
A 
(b) Physical arrangement of clusters. 
(c) Logical arrangement of clusters. 
Cluster 4 
Cluster 5 
Cluster 6 
Cluster 7 
Cluster 8 
Cluster 9 
Figure 12.45 The file allocation table. 
As time passes and files are created, modified, and deleted, 
files on a disk may become very fragmented (i.e. the locations 
of their sectors are, effectively, random). Once the sectors of a 
file are located at almost entirely random points on the disk, 
disk accesses become very long because of the amount of 
head movement required. Defragmentation programs are 
used to clean up the disk by reorganizing files to make 
consecutive logical sectors have consecutive addresses. We 
now briefly describe the structure of the filing system used by 
MS-DOS. 
The MS-DOS file structure 
MS-DOS extends the simple bit-map of Fig. 12.43 to a linked 
list of clusters, where each cluster represents a group of sec-
tors. DOS associates each entry in a file allocation table (FAT), 
with a cluster of two to eight sectors (the size of the clusters is 
related to the size of the disk drive). Using a cluster-map 
rather than a bit-map reduces both the size of the map 
and the number of times that the operating system has to 
search the map for new sectors. However, the cluster-map 
increases the granularity of files because files are forced to 
grow in minimum increments of a whole cluster. If sectors 
hold 1024 bytes, four-sector clusters mean that the minimum 
increment for a file is 4 X 1024 bytes = 4 Kbytes. If the disk 
holds many files, the total wasted space can be quite large. 
Each entry in the FAT corresponds to an actual cluster of 
sectors on the disk. Figure 12.45 illustrates the structure of a 
FAT with entries 4 to 9 corresponding to clusters 4 to 9. 
Assume that a file starts with cluster number 4 and each clus-
ter points to the next cluster in a file. The FAT entry corres-
ponding to the first cluster contains the value 5, which 
indicates that the next cluster is 5. Note how entry 6 contains 
the value 8 indicating that the cluster after 6 is 8. Clusters 
aren't allocated sequentially, which leads to the fragmenta-
tion we described earlier. Figure 12.45(b) shows the physical 
sequence of clusters on the disk corresponding to this FAT. 
We have used lines with arrows to show how clusters are con-
nected to each other. Figure 12.45(c) shows how the operat-
ing system sees the file as a logical sequence of clusters. 
Cluster 9 in the FAT belonging to this file contains the 
value FFFF16, which indicates that this is the last cluster in a 
file. Another special code used by DOS, FFF716, indicates that 
the corresponding cluster is unavailable because it is 
damaged (i.e. the magnetic media is defective). The FAT is 
set up when the disk is formatted and defective sectors 
are noted. The first two entries in a file allocation table 
provide a media descriptor that describes the characteristics 
of the disk. 
When MS-DOS was designed, 12-bit FAT entries were suf-
ficient for disks up to 10 Mbytes. A 16-bit FAT, called FAT 16, 
, 
. , 
. 
. . £_ . 
\ 
Cluster 4 
Cluster 5 
Cluster 6 
Cluster 7 
Cluster 8 
Cluster 9 
r 
wk— 
T \ 
&,— 
r~. 
* ' 
i 
' 
End 

536 
Chapter 12 Computer memory 
can handle entries for disks above 10 Mbytes. The maximum 
size disk that can be supported is die number of clusters 
multiplied by the number of sectors per cluster multiplied 
by the number of bytes per sector. Because the FAT 16 system 
supports only 65 525 clusters, the maximum disk size is lim-
ited (assuming a limit on the size of sectors and clusters). 
These figures demonstrate how rapidly the face of computing 
changed in the 1990s—a 10 Mbyte hard disk was once 
considered large, whereas today it's difficult to find a disk 
less than about 80 Gbytes (apart from in some portable 
systems). The rapid growth in disk capacity forced Microsoft 
to adopt a 32-bit FAT with Windows NT and later releases of 
Windows 95. FAT32 allows the operating system to handle 
disks up to 2 terabytes. 
DOS storage media hold four types of data element. The 
first element is called the boot record and identifies the oper-
ating system, and structure of the disk (number of sectors 
and clusters, size of clusters) and can provide a boot program 
used when the system is first powered up). Following the boot 
sector are two FATs (one is a copy of the other provided for 
security). After the FATs a root directory provides the details 
of the files. These details include the file name, the file char-
acteristics (when created etc.) and the address of the file's first 
cluster in the FAT. The remainder of the disk is allocated to 
the files themselves. 
12.8 Optical memory technology 
Optical storage is the oldest method of storing information 
known to humanity. Early systems employed indentations in 
stone or pottery that were eventually rendered obsolete by 
flexible optical storage media such as papyrus and later 
paper. 
Up to the 1980s, general-purpose digital computers used 
magnetic devices to store information. Optical storage sys-
tems were not widely used until the 1990s, because it was dif-
ficult to perform all the actions required to store and to 
retrieve data economically until improvements had been 
made in a wide range of technologies. 
The optical disk or CD-ROM dramatically changed sec-
ondary storage technology and made it possible to store large 
quantities of information on a transportable medium at a 
low cost. A CD-ROM can store over 500 Mbytes of user data 
on one side of a single 120 mm (4.72 in) disk which is equiv-
alent to around 200 000 pages of text. The optical disk is a 
rigid plastic disk, (Fig. 12.46), whose surface contains a long 
spiral track. The track is laid down on a clear polycarbonate 
plastic substrate inside the disk and is covered with a trans-
parent plastic protective layer. Like the magnetic disk, informa-
tion is stored along a track in binary form. Unlike the 
THE RISE OF OPTICAL TECHNOLOGY 
• 1980—James T. Russell develops the principle of optical 
storage in 1980. 
• 1982—Philips and Sony release the first CD designed to 
store about 650 Mbytes of audio information (74 minutes). 
• 1988—The CD-R format that defines writable CDs is 
defined. 
1996—DVD emerges. DVD is almost identical to the CD but 
stores information more efficiently. 
1997—A consortium of manufacturers release CD-RW, 
a re-writable CD. 
2002—Release of high-speed DVD recording standards 
4 X DVD-R and 4 X DVD-RW. 
125 nml 
Top surface 
Polycarbonate disk 
I 
lAmim»mrefledJ~L___f~L 
Acrylic protection 
table 
1.2 mm 
Back of disk 
(a) View of CD. 
Figure 12.46 The optical disk system. 
(b) Cross-section of CD. 

12.8 Optical memory technology 
537 
magnetic disk, the track in an optical disk is 
a continuous spiral (like that of a gramo-
phone record). The spiral on a CD begins at 
the innermost track and spirals outward, 
whereas the track on a gramophone record 
begins at the edge and spirals inward. 
Although the principles of the optical 
disk are almost trivial, the details of its oper-
ation are very complex. The fundamental 
problems of optical storage are reliability, 
detecting the presence of tiny dents called 
pits on the surface of the disk, optically 
tracking the reflective elements, and 
encoding/decoding the data. 
12.8.1 Storing and reading 
information 
C2! 
Metalized surface 
Quarter-wave 
place 
Polarizing 
beam splitter 
Objective lens 
* Protective plastic 
Detector 
/ D 
-Collimatinglens 
Laser diode - >• 
Figure 12.47 Structure of CD optics. 
An optical disk stores information by means of dots along a 
track. Figure 12.47 shows how a beam of light produced by a 
semiconductor laser is focused onto the surface of the track 
and a photosensitive transistor detects the light reflected back 
from the surface. 
To give you an idea of the precision involved, the objective 
lens is positioned about 1 mm above the surface of the disk 
and the depth of focus of the spot is 2 jjim. The lens must be 
positioned to an accuracy of about one millionth of a meter 
above the disk. 
The amount of light reflected back from a laser depends on 
the height of the reflecting surface on the disk. The base of 
this reflecting surface is called the land and indentations in it 
are known as pits (when viewed from above the disk). 
Data is recorded as a series of variable length pits along a 
spiral track at a 1.6 (Jim constant pitch. A CD has 20 000 
tracks and a track is 30 times narrower than a single human 
hair. The pits and land are coated onto a substrate and cov-
ered with a protective transparent layer. The disk is produced 
by stamping from a master disk that is, itself, produced by the 
same type of technology employed to fabricate microproces-
sors. It's expensive to make one CD-ROM, but very cheap to 
make thousands. Indeed, by the late 1990s CD-ROMs were 
used to distribute advertising material and software from 
Internet service providers so freely that computer magazines 
were full of readers' letters asking what they should do witJi 
all these unwanted CDs. 
Light from the laser is focused first through an objective 
lens and then by the air-disk interface onto the pits and land 
as Fig. 12.48 demonstrates. This arrangement means that 
the spot of light on the surface is very much larger (by three 
orders of magnitude) than the pits. Consequently, surface 
imperfections and dust particles don't interfere with the 
read-back process. In other words, you can tolerate a speck of 
dust on the surface of a CD that's over 100 times larger than 
Protective layer 
A 
} \ i A"--
Polycarbonate 
t= 1.6 mm 
.zv :.r V e spot on t 
disk surface is far greate 
than the spot on the 
reflecting layer. 
Fig. 12.48 Focusing on the pits and land. 
the pit on which the beam is focused without getting a read 
error. By the way, many CD users put a CD down with the 
clear side up because they think the clear side must be 
protected. That's not so. The side witli the pits and lands is 
covered with a thin 0.02 mm protective coating and is more 
vulnerable to scratches than the clear side. 
In order to understand how data from a CD is read, you 
have to know something about the nature of laser light. The 
individual light waves in light from the sun or from a lamp 
are incoherent or random; that is, the light source is com-
posed of a very large number of random waves. Light from a 
laser is coherent and all the waves are synchronized—they go up 
and down together. If you take two laser beams with identical 
frequencies and shine them on the same spot the beams add 
up. If the beams are in phase (i.e. the light waves go up and 
down at the same time) the resulting spot will be four times 
Objective lens 

538 
Chapter 12 Computer memory 
Laser 
beam 
•^Output 
"' y> 
Laser 
beam 
Land 
/ \ N o output 
n phase 
Out of phase 
Polycarbonate 
1 
X/A 
T 
Figure 12.49 Light reflected from a CD. 
as bright (not twice as bright because the beam's energy is the 
square of its amplitude). However, if the beams are 180° out 
of phase with one wave going up as the other goes down, the 
waves will cancel and the spot will disappear. 
When light from the laser hits the land (i.e. the area 
between the pits), the light is reflected back and can be 
detected by a sensor. When the light from the laser hits a pit, 
about half falls on the pit and the other half on the land 
around the pit (see Fig. 12.49). The height of the pit is 
approximately 0.13 \x,m above the surrounding land so that 
light that hits the land has to travel an extra 2 X 0.13 u,m fur-
ther to get to the detector. However, 0.13 /Jim corresponds to 
1/4 of the wavelength of the light in the plastic medium and 
the light reflected back from around a pit travels 1/2 wave-
length further than the light reflected from the top of a pit. 
The light from the pit and light reflected from the surround-
ing land destructively interfere and the light waves cancel 
each other out. A change in the level of light intensity 
reflected from the surface of the disk represents a change 
from land to pit or from pit to land. Figure 12.50 shows the 
structure of the pits and land in more detail. 
The spot of laser light that follows a track should be as 
small as possible in order to pack as many pits and therefore 
data onto the disk as possible. The minimum size of the spot 
is determined by a number of practical engineering consider-
ations. The resolution (i.e. the smallest element that can be 
seen) of the optical system is determined by the wavelength of 
the laser light (780 nm) and the numerical aperture of the 
objective lens (0.45). Numerical aperture (NA) is defined as 
lens diameter divided by the focal length and is a measure of 
a lens's light-gathering power. The value of 0.45 is a compro-
mise between resolution and depth of focus. Increasing the 
resolution and hence storage capacity makes it harder to 
focus the beam on die disk. These values of wavelength and 
NA provide a minimum resolution of I |xm. Note that there 
is sometimes confusion about the wavelength of the laser 
light. The wavelength is 780 nm in air, but when the laser 
cu 
CZI~) 
CD 
0.5 microns 
, pit 
k 
/ 
czz>~ o 
CZD o 
* 
1.6 microns 
a 
a.o 
Land 
Figure 12.50 Organization of the track with land/pits. 
beam travels through the plastic material of the disk its 
wavelength is reduced to 500 nm. 
The sizes of the pits are such that half the energy of the spot 
falls on a pit and half falls onto the land. The reflected energy 
is ideally zero if the light from the pits and land interfere 
destructively. The optimum separation of the pits is deter-
mined by the wavelength of the light used by the laser. 
The data stored on the CD-ROM has to be encoded to 
achieve both maximum storage density and freedom 
from errors. Moreover, the encoding technique must be self-
clocking to simplify the data recovery circuits. Figure 12.51 
illustrates the basic encoding scheme chosen by the designers 
of the CD-ROM. The length of the pits themselves is modu-
lated and the transition of a pit to land (or from land to pit) 
represents a one bit. 
The source data is encoded so that each 8-bit byte is 
transformed into a 14-bit code. Although there are 2 H = 
16 384 possible 14-bit patterns, only 28 = 256 of these 
patterns are actually used. The encoding algorithm chooses 
14-bit code words that do not have two consecutive Is sepa-
rated by less than two 0s. Moreover, the longest permitted run 
of 0s is 10. These two restrictions mean that the 14-bit code 
has 267 legal values, of which 256 are actually used. The 
14-bit codes corresponding to the first 10 8-bit codes are 
given in Table 12.9. 
The groups of 14-bit code words are not simply joined end 
to end, but are separated by three so-called merging bits. The 
function of the merging bits is to ensure that the encoding 
rules are not violated when the end of one group is taken in 
conjunction with the start of the next. These merging bits 
carry no useful data and are simply separators. The following 
example demonstrates the need for merging bits. 
Source data: 
0010 
1000 
These two patterns generate the sequence ..00101000 . . . 
Note how the end of the first group and the start of the 
second group create the forbidden pattern 101 that has a 
1 separated from another 1 by less than two 0s. We can solve 
the problem by inserting the separator 000 between the 
groups to get 
Encoded data; 
00100001000 
In phase 

12.8 Optical memory technology 
539 
8-bit source data 
1111010001 
1111000101 
1101110101 
1111010111 
14-bit encoded data 
IQOOIQOIOOOOOTOlX1100100010000101 
I OOOIOOOOIOOTool lOQIOOOOQIOOOOTl 
~~~ Each source byte is encoded as a 14-bit value 
Data with merging bits 010 00010010000Q10@)10010001000010 00100010000100100 10000100000100001 
""-—- Merging bits ensure that coding rules are not violated at group ends 
Data stream 
0010000000000100100001001000010000100100010000100010001000010010010000100000100001000 
^~~~~—- 24-bit sync pattern 
NRZ encoding 
Disk surface 
( 
i 
CZD 
CD 
• Pits on the dis-
Figure 12.51 Encoding data on an optical disk. 
Source data bits 
Encoded bits 
0 
OOOOOOOO 
01001000100000 
1 
00000001 
10000100000000 
2 
00000010 
10010000100000 
3 
00000011 
10001000100000 
4 
00000100 
01000100000000 
5 
00000101 
00000100010000 
6 
00000110 
00010000100000 
7 
00000111 
00100100000000 
8 
00001000 
01001001000000 
9 
00001001 
10000001000000 
10 
00001010 
10010001000000 
Table 12.9 Converting 8-bit values to a 14-bit code. 
Three Os (i.e. merging bits) have been inserted between the 
two code words to eliminate the possibility of a forbidden 
sequence of bits. 
Another factor in the choice of the pattern of bits to be 
used as the merging bits is the need to keep the average 
lengths of the track and land along the surface of the tracks 
equal. This restriction is necessary because the CD drive's 
focusing and tracking mechanism uses the average energy 
reflected from the surface and, therefore, it is necessary to 
avoid changes in average energy due to data dependency. 
The channel clock derived from the signal recovered from 
the pits and land is 4.3218 MHz because this is the maximum 
rate of change of signal from the pits and land at the standard 
CD scanning speed of 1.3 m/s. The bit density is 1.66 bits//xm 
or 42 kbits/inch. At a track pitch of 1.6 |xm this corresponds 
to 6 X 10s bits/in2 or 106 bit/mm2. 
Because of the way in which pits are laid down to the very 
high mechanical precision required by the system, it's impos-
sible to avoid large numbers of errors. We therefore have to 
employ powerful error-correcting codes to nullify the effect 
of the errors. Due to the complexity of these codes, all we can 
do here is to describe their characteristics. Note that the 
encoding mechanism for data is different to that for audio 
information 
because data requires more 
protection. 
Consequently, a CD stores fewer bytes of used digital data 
than audio data. 
The Cross Interleaved Reed-Solomon code (CIRC) takes 
groups of 24 bytes of data and encodes them into groups of 
32 bytes. Information is interleaved (spread out over the sur-
face of a track) so that a burst of errors at one physical loca-
tion affects several code groups. The following hypothetical 
example should clarify this concept. Suppose data is recorded 
in groups of 4 bytes a!a2a3a4 b,b2b3b4 c,C2C3C4 d,d2d3d4 and 
that a group is not corrupted unless 2 bytes are lost in a group 
(because of some form of error-correcting mechanism). 
Because errors tend to occur in groups (because of, say, 
a scratch), large amounts of data will be lost. If we interleave 
the bytes, we might get a,b,c,di a2b2c2d2 a3b3C3d3 a4b4c4d4. In 
this case, if we lose 2 consecutive bytes, we will be able to cor-
rect the error because the bytes are from different groups. 
One of the differences between the CD used to store audio 
information and the CD-ROM used by computers is that the 
latter employs an extra layer of encoding to reduce further 
the undetected error rate to one in 1013 bits. Moreover, the 
sophisticated CIRC encoding makes it possible to correct an 
error burst of up to 450 bytes (which would take up 2 mm of 
track length). The capacity of a CD-ROM is 553 Mbytes of 
user data (an audio CD can store 640 Mbytes of sound). 
The spiral track of the CD-ROM is divided into individu-
ally addressable sectors. The address of a sector is expressed 
absolutely with respect to the start of the track and is in the 

540 
Chapter 12 Computer memory 
CD SPEED 
The speed at which a CD rotates was determined by the 
conflicting requirements of the technology at the time the 
CD was first manufactured and the desire to store as much 
data as possible. Because the CD was originally devised to 
store audio information, its duration was set as 74 minutes to 
allow von Karajan's Beethoven's Ninth Symphony to go on a 
single CD. 
First-generation CDs operated in real time; if it took 
74 minutes to play a symphony, the disk took 74 minutes to 
read. When CDs began to be used to store data, it was not 
convenient to wait up to about 74 minutes to load a program. 
form of minutes, seconds, and blocks from the start (this for-
mat is the same as that of the audio CD). A sector or block 
is composed of 12 synchronizing bytes (for clock recovery), 
a 4-byte header that identifies the sector, a block of 2048 bytes 
of user data and 288 auxiliary bytes largely made up of the 
error-correcting code. 
Because the size of the pits is constant and they are 
recorded along a spiral on a disk, the number of pits per rev-
olution must vary between the inner and outer tracks. 
Contrast this with the magnetic disk, in which the bit density 
changes between inner and outer tracks because the bits must 
be smaller on inner tracks if there are to be the same number 
as in outer tracks. 
A consequence of constant-size pits is that the speed of the 
disk depends on the location of the sector being read (i.e. the 
disk moves with a constant linear velocity, rather than a con-
stant angular velocity). If the pits have a constant length, 
there are more pits around an outer track and therefore the 
disk must rotate slowly to read them at a constant rate. As the 
read head moves in towards the center, the disk must speed 
up because there are fewer pits around the circumference. 
First-generation CD-ROMs (and audio CDs) spin at between 
about 200 and 500 rpm. As you might imagine, this arrange-
ment severely restricts the access time of the system. 
Moreover, the relatively heavy read head assembly also 
reduces the maximum track-to-track stepping time. These 
factors together limit the average access time of a CD-ROM 
to in the region of 100 to 200 ms (an order of magnitude 
worse than hard disks). We used the expression track-to-
track stepping, even through the track is really a continuous 
spiral. When in the seek mode, the head steps across the spi-
ral and reads an address block to determine whether it has 
reached the correct part of the spiral. As the technology used 
to manufacture CD drives improved through the 1990s, drive 
speeds were increased. Speeds went up from twice to 32 times 
the nominal CD rotation speed by the end of the 1990s and 
average access times dropped to 80 ms. 
Advances in technology allowed the disks to spin faster to 
read data in less time. A CD reader described as 8X can read 
data eight times faster than a standard drive. Modern CDs 
have read speeds of over 50 that of an audio disk, although 
such a sustained increase in speed is rarely achieved in 
practice. 
Write speeds and rewrite speeds have also increased; for 
example, in 2004 a writable and re-writable CD drive 
might be described as 52 X 32 X 52; that is, it has a 
52X read speed, a 52x write speed, and a 32x rewrite 
speed. 
12.8.2 Writable CDs 
k 
I 
When the CD drive first appeared, it was a read-only mecha-
s 
nism. Today, CD drives are available that can write data to a 
e 
CD once only (CD-R), or write to CDs that can be erased and 
rewritten (CD-RW). 
e 
Some CD write mechanisms simply ablate (i.e. blast 
away) the surface of a non-reflecting layer of material above 
5 
a reflecting background to create a pit. Others employ a 
y 
powerful laser to melt a region of a coating of tellurium to 
;t 
create a pit. Another writable disk uses an organic dye within 
,r 
a layer in the disk. When the dye is hit by a laser during the 
write operation, the dye's optical properties are modified. 
e 
The write laser has a power of 30 mW, which is about 
e 
six times more powerful than the laser used to read data from 
. 
a CD. 
, 
You can create true read/write optical storage systems 
e 
that write data onto the disk, read it, and then erase it in 
e 
order to write over it again. Clearly, any laser technology 
j 
that burns or ablates a surface cannot be used in an 
erasable system. Erasable CDs exploit two fundamental 
n 
properties of matter, its optical properties and its magnetic 
properties. 
t 
Figure 12.52 illustrates the principle of an erasable CD. 
0 
The CD substrate is prestamped with the track structure and 
e 
the track or groove coated with a number of layers (some are 
A 
for the protection of the active layer). The active layer uses a 
e 
material like terbium iron cobalt (TeFeCo), which changes the 
,. 
polarization of the reflected laser light. The magnetization of 
ls 
the TeFeCo film determines the direction of the reflected 
|. 
light's polarization. 
ls 
Initially the film in Fig. 12.52(a) is subjected to a uniform 
j 
magnetic field to align the TeFeCo molecules and therefore 
re 
provide a base direction for the polarization of the reflected 
,s 
light. This base can be thought of as a continuous stream of 
^ 
zero bits. During the write phase (Fig. 12.52(b)) a short 
pulse of laser light hits the surface and heats the film 

12.8 Optical memory technology 
541 
A A A A A A A 
A i. 
A A A A 
A i i 
(a) All molecules aligned. 
ft 
tt 
(b) Reverse field applied and spot heated by a laser beam. 
t t t t U i t t t t 
(c) Laser removed. Direction of magnetization of spot reversed. 
Laser beam 
Laser beam 
t t t t l i i t t t t 
(d) Magnetization changes optical properties of surface. 
Figure 12.52 Principle of the rewritable optical disk. 
Transfer rate 
Burst transfer rate 
3.0 Mbytes/s (async, max.) 
10.0 Mbytes/s (sync, max.) 
Sustained transfer 
rate 
6.14 Mbytes/s to 3.07 Mbytes/s 
(9.1 Gbyte/media) 
5.84 Mbytes/s to 2.87 Mbytes/s 
(8.6 Gbyte/media) 
Speed 
Access Time 
25 ms (avg.) 
Latency 
8.3 ms (avg.) 
Rotational speed 
3600 rpm 
Buffer memory 
8 Mbytes 
Reliability 
MTBF 
100 000POH 
MSBF 
750 000 Cycles 
MTTR 
30 minutes 
Bit error rate 
10~17bits 
Table 12.10 Characteristics of a magneto-optical disk drive. 
describes the characteristics of a typical magneto-optical 
disk drive. 
The greatest physical difference between optical disks is the 
reflectivity of the surface; CD-ROMs are 70% reflective, 
CD-R 65%, and CD-RW about 20%. 
changing its magnetic properties. When the surface is heated 
up to 300°C, the surface reaches its Curie point and loses the 
magnetization. By simultaneously activating an electromag-
net under the surface of the disk, the direction of the film's 
magnetization can be reversed with respect to the base 
direction when the laser is switched off and the material 
cools. This action creates a 1 state. As the spot cools down 
(Fig. 12.52(c)), the drop in temperature fixes the new 
direction of magnetization. 
The disk is read by focusing a weaker polarized beam on 
the disk and then detecting whether the reflected beam was 
rotated clockwise or counterclockwise (Fig. 12.52(d)). The 
same laser can be used for both reading and writing; the read 
power is much less than the write power. 
To erase a bit, the area that was written to is pulsed once 
again with the high power laser and the direction of 
the magnetic field from the electromagnet reversed to write 
a zero. 
High-capacity magneto-optical disks use a rugged 
polycarbonate disk platter mounted inside an enclosed car-
tridge that can store over 9 Gbytes of data. Table 12.10 
High-capacity optical storage and the DVD 
Not very long ago, the 600 Mbyte 5!4 inch CD-ROM was 
the state of the art. Progress in everything from laser tech-
nology to head-positioning to optical technology soon 
meant that the CD-ROM was no longer at the cutting edge 
of technology. Like all the other parts of the computer, the 
CD-ROM has evolved. In the late 1990s a new technology 
called the DVD-ROM (digital versatile disk) appeared. The 
DVD-ROM has a minimum capacity six times that of a 
CD-ROM and a potential capacity much more than that. 
Part of the driving force behind the development of the 
DVD-ROM was to put feature-length movies in digital 
video form on disk. 
The DVD-ROM looks like a conventional CR-ROM and 
the underlying technology is exacdy the same. Only the 
parameters have changed. Improvements in optical tracking 
have allowed the track spacing to be reduced and hence the 
length of the track to be considerably increased. DVD tracks 
are 0.74 u,m apart (conventional CD-ROMs use 1.6 u.m 
spacing). Lasers with shorter wavelengths (635 nm) have 
permitted the use of smaller pits. 
Applied field 
1 Laser beam I 
I 

542 
Chapter 12 Computer memory 
Feature 
DVD 
CD 
Disc diameter 
Disc thickness 
Track pitch 
Track density 
Bit density 
Data rate 
Data density 
Minimum pit length 
Laser wavelength 
Numerical aperture of 
Single layer data capacity 
Reference speed 
Reference user data rate 
Signal modulation method 
120 mm 
1.2 mm 
0.74 urn 
34 ktracks/in 
96 kbits/in 
11 Mbits/s 
3.28 Gbits/in2 
0.40 u.m 
635 to 650 nm (red) 
0.60 
4.7 G bytes 
4.0 m/s 
1108kbytes/s 
8/16 modulation 
120 mm 
1.2 mm 
1.6 u,m 
16 ktracks/in 
43 kbits/in 
1.2 to 4.8 Mbits/s 
0.68 Gbits/in2 
0.834 u.m 
780 nm (infrared) 
0.45 
0.68 Gbytes 
1.2 m/s 
Mode 1:153.6 kbytes/s 
8/14 modulation 
Table 12.11 Comparison of CD and DVD. 
Parameter 
CD 
DVD 
Improvement 
factor 
Bit length 
0.277 urn 
0.13 urn 
2.1 
Track pitch 
1.6 u.m 
0.74 u.m 
2.16 
Data area 
8605 mm2 
8759 mm2 
1.02 
Modulation efficiency 
17/8 
16/8 
1.06 
Error correction loss 
34% 
13% 
1.32 
Sector overhead 
8.2% 
2.6% 
1.07 
Total increase 
7 
Table 12.12 Improvements in the efficiency of DVD in terms of 
data density. 
The DVD-ROM can be double-sided, which instantly 
doubles its data capacity. Moreover, by using semitransparent 
layers, it is possible to have several optical layers within the 
disk. Focusing the laser on a particular layer accesses data in 
that layer. Other layers are out of focus. 
Just as writable CDs have been developed, it is possible to 
buy writable DVDs. Unfortunately, several mutually incom-
patible technologies were developed nearly simultaneously 
forcing consumers to select a particular system (a similar 
situation existed in the early days of the VCR until the VHS 
format swept away its competitors). By 2004 DVD manufac-
turers were selling drives that were compatible with most of 
the available formats. Table 12.11 describes the differences 
between CDs and DVDs and Table 12.12 highlights the 
improvements in DVD technology. 
Writable DVDs 
In principal, writable DVDs are implemented in the same 
way as writable CDs. You modify the pit/land structure by a 
laser beam that either modifies the disk's magnetic or optical 
properties. Unfortunately, the DVD industry has not settled 
on a single writable disk standard due to competing eco-
nomic, technical, and political (industrial) pressures. There 
are five basic standards for recordable DVDs: DVD-R, 
DVD-RW, DVD+R, DVD+RW, and DVD-RAM. 
The write-once standards are DVD-R and DVD+R. These 
use a laser to change the optical properties of a material by burn-
ing pits on a dye layer within the disk. The rewritable standards, 
DVD-RW and DVD+RW, use a laser to reversibly change the 
optical properties of a material by changing, for example, its 
state between crystalline (reflective) and amorphous (dull). 
The DVD-R and DVD+R formats compete against each 
other; the differences between fhem lie principally in the struc-
ture of the data on the disk. The most widely compatible for-
mat is DVD-R (in particular, it's compatible with the DVD 
players and with old DVD drives in PCs). In recent years, many 
DVD drives are able to handle a range of different formats by 
changing the laser properties to suit the actual media and the 
read/write software to suit the required data structures. 
Another rewritable DVD is DVD-RAM, which uses DVDs 
with a pattern pressed onto the surface. DVD-RAM was orig-
inally designed for random access data storage by computers. 

12.8 Optical memory technology 
543 
Sector address information is molded into the side of the 
track. DVD-RAM is the most incompatible of the DVD 
recordable formats (i.e. fewer drives read DVD-RAM than 
other recordable formats). 
All types of DVD reserve a control area at the inside 
edge of the track that contains the disk's identification. This 
mechanism allows the drive to identify the type of medium 
currently loaded. 
m SUMMARY 
The von Neumann machine needs memory to store programs 
and data. Lots of memory. As computer technology has 
advanced, the size of user programs and operating systems has 
more than kept up. In the early 1980s a PC with a 10 Mbyte 
hard disk was state of the art. Today, even a modest utility might 
require over 10 Mbytes and a high-resolution digital camera is 
quite happy to create 5 Mbyte files when operating in it RAW 
(uncompressed) mode. 
In this chapter we have looked at the computer's memory 
system. We began with a description of the characteristics 
of fast semiconductor memory and then moved on to 
characteristics of slower but much cheaper secondary storage. 
Today, there is a bewildering number of memory technologies. 
We have briefly covered some of them: from semiconductor 
dynamic memory to devices based on magnetism to optical 
storage technology. Memory technology is important because, 
to a great extent, it determines the way in which we use 
computers. Faster CPUs make it possible to process data rapidly, 
enabling us to tackle problems like high-speed, real-time graphics. 
Faster, denser, and cheaper memories make it possible to store 
and process large volumes of data. For example, the optical disk 
makes it possible to implement very large on-line databases. 
Low-cost high-capacity hard disks now enable people to carry 
more than 80 Cbytes of data in a portable computer or over 
400 Cbytes in a desktop machine. In just two decades the 
capacity of hard disks in personal PCs has increased by a factor 
of 40 000. 
M PROBLEMS 
12.1 Why is memory required in a computer system? 
12.2 Briefly define the meaning of the following terms 
associated with memory technology: 
(a) random access 
(b) non-volatile 
(c) dynamic memory 
(d) access time 
(e) EPROM 
(f) serial access 
12.3 What properties of matter are used to store data? 
12.4 A computer has a 64-bit data bus and 64-bit-wide 
memory blocks. If a memory access takes 10 ns, what is the 
bandwidth of the memory system? 
12.5 A computer has a 64-bit data bus and 64-bit-wide 
memory blocks.The memory devices have an access time of 
35 ns. A clock running at 200 MHz controls the computer and all 
operations take an integral (i.e. whole number) of clock cycles. 
What is the effective bandwidth of the memory system? 
12.6 What is the purpose of a semiconductor memory's CS 
(chip select) input? 
12.7 A dynamic RAM chip is organized as 64M X 4 bits. 
A memory composed of 1024 Mbytes is to be built with these 
chips. If each word of the memory is 64 bits wide, how many 
chips are required? 
12.8 What are the principal characteristics of random access 
and serial access memory? 
12.9 Why is all semiconductor ROM RAM but not all 
semiconductor RAM ROM? 
12.10 If content addressable memory (CAM) could be manufac-
tured as cheaply as current semiconductor memories, what impact 
do you think it would have on computers? We haven't covered 
CAM in this text—you'll have to look it up. 
12.11 What is flash memory and why is it widely used to store 
a PC's BIOS (basic input/output system)? 
12.12 Use a copy of a current magazine devoted to personal 
computing to work out the cost of memory today (price per 
megabyte for RAM, hard disk, flash memory, CD ROM, and 
DVD). 
12.13 Give the size (i.e. the number of addressable locations) of 
each of the following memory blocks as a power of 2. The blocks 
are measured in bytes. 
(a) 4K 
(b) 16K 
(c) 2M 
(d)64K 
(e) 16M 
(f)256K 
12.14 What address lines are required to span 
(i.e. address) each of the memory blocks in the previous prob-
lem? Assume that the processor is byte addressable and has 24 
address lines A^, to A23. What address lines must be decoded to 
select each of these blocks? 
12.15 What is an address decoder and what role does it carry 
out in a computer? 
12.16 A computer's memory can be constructed from memory 
components of various capacities (i.e. total number of bits) and 
organizations (i.e. locations x width of each location). For each 
of the following memory blocks, calculate how many of the 
specified memory chips are required to implement it. 
Memory block 
(a) 64 kbytes 
(b) 1 Mbyte 
(c) 16 Mbytes 
Chip organization 
8 K X 8 
32KX4 
256K X 8 
12.17 What is partial address decoding and what are its 
advantages and disadvantages over full address 
decoding? 

544 
Chapter 12 Computer memory 
12.18 An address decoder in an 8-bit microprocessor with 
16 address lines selects a memory device when address 
lines A,5IA14,A13,A11 = 1,1,0,1. What is the size of the 
memory block decoded and what range of addresses 
does it span (i.e. what are the first and last addresses in this 
block)? 
12.19 An address decoder in a 68K-based microprocessor 
selects a memory device when address lines 
A23,A22,A21,A20 = 1,1,0,1.What is the size of the memory 
block decoded and what range of addresses does 
it span (i.e. what are the first and last addresses in 
this block)? 
12.20 Design address decoders to implement each of the 
following 68K address maps. In each case, the blocks of memory 
are to start from address $00 0000. 
(a) 4 blocks of 64 kbytes using 32K x 8-bit chips 
(b) 8 blocks of 1 Mbyte using 512K X 8-bit chips 
(c) 4 blocks of 128 kbytes using 64K x 8-bit chips 
12.21 A memory system in a 68K-based computer includes 
blocks of ROM, static RAM, and DRAM. The sizes of these three 
blocks are 
ROM: 4 Mbytes 
SRAM: 2 Mbytes 
DRAM: 8 Mbytes 
These memory blocks are implemented with the following 
memory components: 
ROM:1MX 16-bit chips 
SRAM: 512KX 8-bit chips 
DRAM: 4M X 4-bit chips 
(a) Show how the blocks of memory are organized in 
terms of the memory devices used to implement 
them. 
(b) Draw a memory map for this system and indicate the start 
and end addresses of all blocks. 
(c) Draw an address decoding table for this 
arrangement. 
(d) Design an address decoder for this system using 
simple logic gates logic. 
(e) Construct an address decoder using a PROM for this system 
and design a decoding table to show its contents. 
12.22 A computer's memory system is invariably 
non-homogeneous. That is, it is made up of various types of 
storage mechanism, each with its own characteristics. 
Collectively, these storage mechanisms are said to form a 
memory hierarchy. Explain why such a memory hierarchy is 
necessary, and discuss the characteristics of the memory 
mechanisms that you would find in a modern 
high-performance PC. 
12.23 In the context of memory systems, what is the meaning 
of hysteresis? 
12.24 Can you think of any examples of the effects of 
hysteresis in everyday life? 
12.25 Why does data have to be encoded before it can be 
recorded on a magnetic medium? 
12.26 Explain how data is recorded using PE encoding and draw 
a graph of the current in the write head generated by the data 
stream 10101110. 
12.27 A disk is a serial (sequential) access device that can 
implement random access files. Explain this apparent contradic-
tion of terminology. 
12.28 How do the following elements of a track-seek time 
affect the optimum arrangement of data on a disk: acceleration, 
coasting, deceleration, and settling? 
12.29 What is an audio-visual drive and how does it differ from 
a conventional hard drive? 
12.30 What are the advantages of the SCSI interface over the 
IDE interface? 
12.31 What are the limits on ultimate performance of the 
following. 
(a) The hard disk. 
(b) The floppy disk. 
(c) The CD-ROM. 
12.32 What are the operational characteristics of the 
serial access devices found in a PC? Use one or more of the 
magazines devoted to the PC to answer this question. 
12.33 An image consists of 64 columns by 64 rows of pixels. 
Each pixel is a 4-bit 16-level gray-scale value. A sequence of 
these images is stored on a hard disk. This hard disk rotates at 
7200 rpm and has 64 1024-byte sectors per track. 
(a) Assuming that the images are stored sequentially, how fast 
can they be transferred from disk to screen? 
(b) If the images are stored randomly throughout the disk, what 
is the longest delay between two consecutive images if the 
disk has 1500 tracks and the head can step in or out at a 
rate of one track per millisecond. 
12.34 A hard disk drive has 10 disks and 18 surfaces 
available for recording. Each surface is composed of 200 con-
centric tracks and the disks rotate at 7200 rpm. Each track is 
divided into 8 blocks of 256 32-bit words. There is one 
read/write head per surface and it is possible to read the 18 
tracks of a given cylinder simultaneously. The time to step from 
track to track is 1 ms (10 3 s). Between data transfers the head 
is parked at the outermost track of the disk. 
Calculate the following. 
(a) The total capacity in bits of the disk drive. 
(b) The maximum data rate in bits/second. 
(c) The average access time in milliseconds. 
(d) The average transfer rate when reading 256 word blocks 
located randomly on the disk. 
(e) If the disk has a 3-inch diameter and the outermost track 
comes to 1 inch from the edge of the disk 
calculate the recording density (bits/in) of the 

12.8 Optical memory technology 
545 
innermost and the outermost tracks. The track 
density is 200 tracks/in. 
12.35 Derive an expression for the average distance moved by 
a head from one cylinder to another (in terms of the number of 
head movements). Movements are made at random and the 
disk has N concentric cylinders 
numbered from 0 to N-1 with the innermost cylinder num-
bered 0. Assume that when seeking the next 
cylinder, all cylinders have an equal probability of 
being selected. Show that the average movement approaches 
N/3 for large values of N. Hint: Consider the Kth cylinder and 
calculate the number of steps needed to move to the 7th cylin-
der where /varies from 0 to (N— 1). 
12.36 A floppy disk drive has the following parameters: 
sides: 2 
tracks: 80 
sectors/track: 9 
bytes/sector: 1024 
rotational speed: 360 rpm 
track-to-track step time: 1 ms 
Using the above data, calculate the following. 
(a) total capacity of the disk. 
(b) average time to locate a sector. 
(c) time to read a sector once it has been located. 
(d) data transfer rate during the reading of a 
sector. 
12.37 Why does a floppy disk have to be formatted before data 
can be written to it? How do you think that sector size affects 
the performance of a disk system? 
12.38 WhatisaCRC? 
12.39 Several books state that if you get the interleave factor 
of a disk wrong, the operating system's performance will be dra-
matically degraded. Why? 
12.40 What are the advantages of MS-DOS's file allocation 
table (FAT) over the free-sector bit-map and linked list 
of sectors. 
12.41 Interpret the meaning of the following extract from a FAT. 
1 
2 
2 
4 
3 
7 
4 
FFFF 
5 
6 
6 
8 
7 
5 
8 
FFFF 
9 
FFF7 
12.42 Why are gaps required when a data structure is set up on 
a floppy disk during formatting? 
12.43 Why are error-detecting systems so important in 
secondary storage systems (in comparison with primary storage 
systems)? 
12.44 What are the advantages of a magnetoresistive head 
over a thin-film head? 
12.45 Use the Internet to find the properties of today's large 
hard disk drives. 
12.46 SMART technology is used to predict the failure of a 
hard disk. To what extent can this technology be applied to 
other components and subsystems in a computer? 
12.47 The speed (access time) of semicouductor devices has 
increased dramatically over the past few decades. However, the 
access time of hard discs has failed to improve at the same rate. 
Why is this so? 
12.48 A magnetic tape has a packing density of 800 characters 
per inch and an interblock gap of '/> inch and is filled with 
records. Each contains 400 characters. Calculate the fraction of 
the tape containing useful data if the records are written as 
(a) single record blocks 
(b) blocks containing four records 
12.49 Data is recorded on magnetic tape at 9600 bpi along 
each track of nine-track tape. Information is organized as blocks 
of 20 000 bytes and an interblock gap of 0.75 in is left between 
blocks. No information is recorded in the interblock gaps. What 
is the efficiency of the storage system? 
12.50 An engineer proposes to use a video recorder (VCR) to 
store digital data. Assume that the useful portion of each line 
can be used to store 256 bits. What is the storage capacity of a 
1 -hour tape (in bits), and at what rate is data transferred? ATV 
picture is transmitted as 525 lines, repeated 30 times per sec-
ond in the USA and 625 lines, repeated 25 times per second in 
the UK. 
12.51 Do standards in memory technology help or hinder 
progress? 
12.52 Does magnetic tape have a future as a secondary storage 
medium? 
12.53 What are the relative advantages and disadvantages of 
magnetic and optical storage systems? 
12.54 Why is a laser needed to read the data on a CD-ROM? 
12.55 Why is it relatively harder to write data on a CD than to 
read it? 
12.56 Discuss the ethics of this argument: Copying software 
ultimately benefits the manufacturer of the copied 
software, because it creates a larger user base for the software 
and, in turn, creates new users that do pay for the software. 
12.57 Data is recorded along a continuous spiral on a CD-ROM. 
Data is read from a CD-ROM at a constant bit rate (i.e. the num-
ber of bits/s read from the CD-ROM is constant). What implica-
tions do you think that this statement has for both the designer 
and the user of a CD-ROM ? 
12.57 A disk platter has bit density of 1000 bits/mm2. Its 
innermost track is at a radius of 2 cm, its outermost 

546 
Chapter 12 Computer memory 
track at a radius of 5 cm. What is the total capacity of 
the disk if we assume a uniform bit density and no data over-
head? 
12.59 How fast does a hard disk have to rotate in order 
for its stored energy to be equivalent to its own weight in TNT? 
Assume a 3'/z-inch aluminum platter. Note: the energy density 
of TNT is 2175 J/g and the energy of rotation of a disk is Vila1 
and / = mi2 where m is the disk's mass, r its radius, and u> its 
rotational velocity in radians per second. 
12.60 When a ' 1 ' is recorded on a disk drive and the analog 
signal read back from the read head, the resulting sampled 
signal is 0.0,0.4,1.0,0.4,0.0, where the signal is sampled every T 
seconds. If a '0' is recorded, the sampled signal is 
0.0, -0.4, -1.0, -0.4,0.0. Suppose the binary string 011010 is 
written to the disk and each bit is transmitted at f-second 
intervals. What signal would be read back from the disk 
corresponding to 011010 if the signal were sampled every T 
seconds? 

T-N 
The operating system 
13 The operating system 
The operating system performs 
two functions. It provides a user 
interface and controls the 
processor's hardware. An 
operating system manages the 
memory system and allocates 
memory space to tasks. The 
operating system also controls 
I/O via the interrupt mechanism 
and performs multiprocessing 
(running more than one program 
at the same time).We briefly 
look at the operating system 
because it is the point at which 
hardware and software meet. 
INTRODUCTION 
We now look at one of the most important components of a modern computer, the operating 
system. Operating systems can be very large programs indeed (e.g. 100 Mbytes). Some might argue 
that a section on operating systems is out of place in an introductory course on computer hardware. 
We include this topic here for two reasons. First, the operating system is intimately connected with 
the hardware that it controls and allocates to user programs. Second, some students may not 
encounter the formal treatment of operating systems until later in their studies. 
We begin with an overview of operating systems and then concentrate on three areas in which 
hardware and software overlap: multitasking, exception handling, and memory management. 
Multitasking permits a computer to run several programs at the same time. Exception handling 
is concerned with the way in which the operating system communicates with user applications 
and external hardware. Memory management translates addresses from the computer into the 
actual addresses of data within the CPU's memory system. 
Before continuing, we need to make a comment about terminology. The terms program and job 
are used synonymously in texts on operating systems and mean the same thing. Similarly, the 
terms task and process are also equivalent. A process (i.e. task) is an instance of a program that 
includes the code, data, and volatile data values in registers. The ability of a computer to execute 
several processes concurrently is called multitasking or multiprogramming. However, the term 
multiprocessing describes a system with several processors (CPUs) that run parts of a process in 
parallel. 
13.1 The Operating System 
orchestra. The great conductor is a celebrity who gets invited to 
take part in talk shows on television and is showered with soci-
The relationship between an operating system and a computer 
ety's highest awards. And yet the conductor doesn't add a single 
is similar to the relationship between a conductor and an 
note to a concert. The importance of conductors is well 
CHAPTER MAP 
12 Memory 
Chapter 12 describes a. 
computer's memory system. 
Information isn't stored in a 
computer in just one type of 
storage device; it's stored in 
ORAM and on disk, CD-ROM. 
DVD, and tape. We look at both 
high-speed immediate access 
.semiconductor technology and 
. the much slower magnetic and' 
optical secondary storage . 
systems used to archive data. 
4 Computer 
communications 
Computers communicate with 
each ether to share resources 
such as printers and data or to 
-. increase performance by dividing 
a job between them, in Chapter 14 
we look at the background to 
computer communications, the 
way in which messages are sent 
from point to point, and the 
• protocols or rules that govern 
the exchange of data. 

548 
Chapter 13 The operating system 
known—they co-ordinate the players. A good conductor 
knows the individual strengths and weaknesses of players and 
can apply them in such a way as to optimize their collective 
performance. 
An operating system is the most important piece of software 
in a computer system. Its role is to co-ordinate the functional 
parts of the computer to maximize the efficiency of the system. 
We can define efficiency as the fraction of time for which the 
CPU is executing user programs. It would be more accurate if 
we were to say that the operating system is designed to remove 
inefficiency from the system. Suppose a program prints a docu-
ment. While the printer is busy printing the document, the 
CPU is idling with nothing to do. The operating system would 
normally intervene to give the CPU something else to do while 
it's waiting for the printer to finish. 
A second and equally important role of the operating system 
is to act as the interface between the user and the computer. 
Programmers communicated with first-generation operating 
systems via a job control language (JCL), which looked rather 
like any other conventional computer language. Today's oper-
ating systems such as Microsoft's Windows provide an inter-
face that makes use of a WIMP (windows, icons, mouse, and 
pointer) and GUI (graphical user interface) environment. 
From the user's point of view an operating system should 
behave like the perfect bureaucrat; it should be efficient, help-
ful, and, like all the best bureaucrats, should remain in the 
background. A poorly designed operating system, when 
asked to edit a file, might reply 'ERROR 53'. A really good 
operating system would have replied, 'Hi. Sorry, but my disk 
is full. I've noticed you've got a lot of backup copies, so if you 
delete a couple I think we'll be able to find room for your file. 
Have a nice day'. Raphael A. Finkel, in his book An Operating 
System's Vade Mecum (Prentice-Hall, 1988) calls this aspect 
of an operating system the beautification principle, which he 
sums up by'... an operating system is a collection of algo-
rithms that hides the details of the hardware and provides a 
more pleasant environment'. 
User applications 
Disk file manager 
Operating system 
Figure 13.1 Hierarchical model of an operating system. 
Figure 13.1 shows how the components of the operating 
system relate to each other and to the other programs that 
run under the operating system's supervision. The diagram is 
depicted as a series of concentric circles for a good reason— 
programs in the outer rings use facilities provided by pro-
grams in the inner rings. At the center of the circle lies the 
scheduler, which switches from one task to another in a multi-
tasking environment. The scheduler is smaller than pro-
grams in the outer ring such as database managers and word 
processors. A scheduler is often said to be tightly coded 
because it uses a small amount of code optimized for speed. 
Sophisticated operating systems employ hardware and 
software mechanisms to protect the important inner rings 
from accidental or illegal access by other components. If a 
user task corrupts part of the kernel, the operating system 
may crash and the system halts. 
Not all computers have an operating system. A computer 
used as a controller in, for example, a digital camera may not 
need an operating system (although complex embedded 
controllers do have operating systems). Whenever functions 
normally performed by an operating system are required, 
they are incorporated into the program itself. 
13.1.1 Types of operating system 
Operating systems can be divided into categories: single-
user, 
batch 
mode, 
demand 
mode, 
real-time, 
and 
client-server. The distinction between operating system 
classes can be vague and a real operating system may have 
attributes common to several classes. We now briefly 
describe the various types of operating system (although the 
modern high-performance PC and the local area network 
have rendered some of them obsolete). 
The single-user operating system (e.g. MS-DOS) allows 
only one user or process to access the system at a time. First-
generation mainframe operating systems worked in a batch-
mode. Jobs to be executed were fed into the computer, 
originally in the form of punched cards. Each user's program 
began with job control cards telling the operat-
ing system which of its facilities were required. 
The operating system scheduled the jobs accord-
ing to the resources they required and their pri-
ority, and eventually generated an output. 
Batch mode operation is analogous to a dry 
cleaning service. Clothes are handed in and are 
picked up when they've been cleaned. Batch-
mode operating systems accepted jobs on 
punched card (or magnetic tape). The disadvant-
interface 
aS e °f batch mode systems is their lengthy turn 
around time. It was frustrating in the 1970s to 
wait 5 hours for a printout only to discover that 
the job didn't run because of a simple mistake in 
one of the cards. 
J 
Scheduler 

13.1 The operating system 
549 
MODERN OPERATING SYSTEMS 
It is unfair to imply that an operating system only coordinates 
the hardware and provides a user interface. Modern operating 
systems include components that could be regarded as 
applications programs; for example, Windows incorporates a web 
browser called Internet Explorer. Web browsers can be obtained 
from more than one supplier and installed as user applications. 
By incorporating a function such as a web browser, an 
operating system like Windows can provide a consistent 
user interface across applications. Moreover, Windows 
achieves a constant look and feel across the operating 
system, Microsoft's web browser, and Microsoft's 
applications. 
MS-DOS 
When IBM was creating the PC in 1890, Bill Gates was 
approached to supply a simple operating system. Bill 
Gates came up with MS-DOS (Microsoft Disk Operating 
System), which was loosely based on a first-generation 
microprocessor operating system called CP/M. MS-DOS 
allowed you to create, list, delete, and manipulate files 
on disk. 
MS-DOS was released as MS-DOS 1.0 in 1981 and devel-
oped by stages to become MS-DOS 6.22 in 1994. Future 
developments were not necessary because the market for a 
command line operating system dried up when graphical 
operating systems like Windows became available. 
The final version of DOS, 7.0, was released in 1995 when it 
was incorporated in Windows 95. 
LINUXAND WINDOWS 
There are three major operating systems in the PC world. 
The vast majority of PCs use Microsoft's Windows or Linux 
(a public domain version of UNIX). The Apple Mac uses OS X 
(which is a variation of Linux). 
Microsoft's Windows has been developed since it first 
appeared in 1995. It is easy to use and has been responsible 
for bringing computing to the non-computer-specialist 
masses. Windows is relatively expensive, costing about 20% of 
the PC's hardware. 
Linux is an open-source, public domain operating system 
developed by Linus Tovalds.Tovalds created Linux by 
extending Andy Tannenbaum's mini operating system Minix. 
Modern versions of Linux now have a Windows-style 
front end. 
Demand mode operating systems allow you to access 
the computer from a terminal, which was a great improve-
ment over batch mode operation because you can complete 
each step before going on to the next one. Such an 
arrangement 
is also called interactive because the 
operating system and the user are engaged in a dialogue. 
Each time the user correctly completes an operation, they 
are informed of its success and invited to continue by 
some form of prompt message. If a particular command 
results in an error, the user is informed of this by the operat-
ing system and can therefore take the necessary corrective 
action. 
Real-time operating systems belong to the world of 
industrial process control. The primary characteristic of a 
real-time operating system is that it must respond to an 
event within a well-defined time. Consider a computer-
controlled petrochemical plant. The conditions at many 
parts of the plant are measured and reported to the computer 
on a regular basis. Control actions must be taken as condi-
tions in the plant change; for example, sudden build-up of 
pressure in a reaction vessel cannot be ignored. The com-
puter running the plant invariably has a real-time operating 
system that responds to interrupts generated by external 
events. 
Real-time operating systems are found wherever the 
response time of the computer must closely match that of 
the system it is controlling. Real-time operating systems are 
so called because the computer is synchronized with what 
people call clock time. Other operating systems operate in 
computer time. A job is submitted and its results delivered 
after some elapsed time. There is no particular relationship 
between the elapsed time and the time of day. The actual 
elapsed time is a function of the loading of the computer 
and the particular mix of jobs it is running. In a real-time 
system the response time of the computer to any stimulus is 
guaranteed. 
Modern multimedia systems using sound and video are 
also real-time systems—not least because a pause in a video 
clip while the computer is carrying out another process is 
most disconcerting. Real-time operating system technology 
has had a strong influence on the way in which processors 
have developed; for example, Intel's multimedia extensions 
(MMX) added special-purpose instructions to the Pentium's 
instruction set to handle video and sound applications. 

550 
Chapter 13 The operating system 
Some modern operating systems are called client-server 
and run on distributed systems. A client-server system may 
be found in a university where each user has their own com-
puter with a CPU, memory, and a hard disk drive, linked to a 
server by a local area network. Processes running on one of 
the terminals are called client processes and are able to make 
requests to the server. The operating system is distributed 
between the client and the server. A client on one host can use 
the resources of a server on another host. 
13.2 Multitasking 
Multitasking is the ability of a computer to give the impres-
sion that it can handle more than one job at once. A computer 
cannot really execute two or more programs simultaneously, 
but it can give the impression that it is running several pro-
grams concurrently. The following example demonstrates 
how such an illusion is possible. 
Consider a game of simultaneous chess where a first-class 
player is pitted against several weaker opponents by stepping 
from board to board making a move at a time. As the master 
player is so much better than their opponents, one of the 
master's moves takes but a fraction of the time they take. The 
HISTORY OF WINDOWS 
Microsoft's first version of Windows, version 1.0, was released 
in 198S.This was a graphical version of Microsoft's command-
line MS-DOS operating system. Version 2.0 appeared in 1987 
and provided better windows management facilities. 
Windows 3.0 was released in 1990 and was Microsoft's first 
really successful GUI-based operating system. This version 
made better use of the processor's memory management 
mechanisms. 
Windows 95 and 98 (released in 1995 and 1998 respect-
ively) continued the development of Microsoft's GUI techno-
logy. Changes were incremental rather than revolutionary; for 
example, Windows 95 provided support for long file names 
(rather than the old '6.3' DOS format, which restricted names 
to six characters). Windows 98 provided a better integration of 
operating system and Internet browser as well as the beginning 
of support for peripherals such as the universal serial bus, USB. 
Microsoft released Windows ME (Millennium Edition) in 2000. 
This was the end of the line for Microsoft's operating systems 
that began with Windows 3.0. ME provided further modest 
improvements to Windows 98 such as the incorporation of a 
media player and a system restore mechanism. ME was regarded 
as unstable and bug ridden and was not a significant success. 
Microsoft launched a separate range of graphical operating 
systems; first NT in 1993 (New Technology) and then 
Windows 2000. These were regarded as professional operating 
systems, targeted at corporate users. NT (and later Windows 
players share the illusion that they have a single opponent of 
their own. 
The organization of a game of simultaneous chess can 
readily be applied to the digital computer. All we need is a 
periodic signal to force the CPU to switch from one job to 
another and a mechanism to tell the computer where it was 
up to when it last executed a particular job. The jobs are 
referred to as tasks or processes and the concept of executing 
several processes together is called multiprogramming or 
multitasking. A process is a program together with its associated 
program counter, stack, registers, and any resources it's using. 
Before we look at how multitasking is implemented we dis-
cuss some of its advantages. If each process required only CPU 
time, multitasking would have little advantage over running 
processes consecutively (at least in terms of the efficient use of 
resources). If we re-examine simultaneous chess, we find that 
its success is based on the great speed of the master player 
when compared with that of their opponents. While each 
player is laboriously pondering their next move, the master 
player is busy making many moves. 
A similar situation exists in the case of computers. While 
one task is busy reading information from a disk drive and 
loading it into memory or is busy printing text on a printer, 
another task can take control of the CPU. A further advantage 
2000) used underlying 32-bit binary code rather than the 
16-bit code of DOS and earlier versions of Windows. Windows 
NT also introduced the /vTfSfile system that was far more 
sophisticated and reliable than the MTsystem used by 
MS-DOS and early versions of Windows. 
Windows XP was launched in 2001 and brought together 
Microsoft's previous two lines (Windows 98 aimed at the PC 
user and Windows 2000 aimed at the corporate user). Windows 
XP may be thought of as a mature version of Windows in the 
sense that it used true 32-bit code, supported the NTFS file 
management mechanism, and provided extensive support for 
multimedia applications, new high-speed interfaces, and local 
area networks. There are two versions of XP.The standard ver-
sion, XP home, is intended for the small-scale user and XP pro-
fessional is aimed at the corporate and high-performance user. 
XP professional supports remote processing (using the system 
via an Internet connection) and symmetrical multiprocessing 
(systems with more than one CPU). 
The Windows operating system became the target of many 
malware writers (malware includes computer viruses, worms, 
Trojan horses, and spyware). Microsoft has had to keep up 
with malware writers by continually releasing updates to close 
the loopholes exploited by, for example, virus writers. In late 
2004 Microsoft released its Service pack 2 to update XP.This 
service pack included a firewall to prevent illegal access from 
the Internet. 

13.2 Multitasking 
551 
of multiprogramming is that it enables several users to gain 
access to a computer at the same time. 
Consider two processes, A and B, each of which requires a 
several different activities to be performed during the course 
of its execution (e.g. video display controller, code execution, 
disk access, etc.). The sequence of activities carried out by 
each of these two processes as they are executed is given in 
Fig. 13.2. Note that VDT1 and VDT2 are two displays. 
If process A were allowed to run to completion before 
process B were started, valuable processing time would be 
wasted while activities not involving the CPU were carried 
out. Figure 13.3 shows how the processes may be scheduled to 
make more efficient use of resources. The boxes indicate the 
period of time for which a given resource is allocated to a par-
ticular process. For example, after process A has first used the 
CPU, it accesses the disk. While the disk is being accessed by 
process A, process B can use the processor. 
The fine details of multiprogramming operating systems 
are beyond the scope of an introductory book. However, the 
following principles are involved. 
1. The operating system schedules a process in the most 
efficient way and makes best use of the facilities available. 
The algorithm may adapt to the type of jobs that are run-
ning, or the operator may feed system parameters into the 
computer to maximize efficiency. 
2. Operating systems perform memory management. If 
several processes run concurrently, the operating system 
must allocate memory space to each of them. Moreover, 
the operating system should locate the processes in 
memory in such a way as to make best possible use of the 
memory. The operating system must also protect one task 
from unauthorized access to another. 
3. If the CPU is to be available to one process while another 
is accessing a disk or using a printer, these devices must be 
4 
Pre cess A 
4 
Process B 
• 
VDT1 CPU 
Disk 
CPU 
VDT1 
V33T2 CPU 
Disk VDT?" CPU 
-* time 
capable of autonomous operation. That is, they must 
either be able to take part in DMA (i.e. direct memory 
access) operations without the active intervention of the 
CPU, or they must be able to receive a chunk of high-
speed data from the CPU and process it at their leisure. 
One of the principal problems a complex multitasking 
operating system has to overcome is that of deadlock. Suppose 
process A and process B both require CPU time and a printer 
to complete their activity. If process A has been allocated the 
CPU and the printer by the operating system, all is well and 
process B can proceed once process A has been completed. 
Now imagine the situation that occurs when process A 
requests both CPU time and the printer but receives only the 
CPU, and process B makes a similar request and receives the 
printer but not the CPU. In this situation both processes have 
one resource and await the other. As neither process will give 
up its resource, the system is deadlocked and hangs up indef-
initely. Much work has been done on operating system 
resource allocation algorithms to deal with this problem. 
13.2.1 What is a process? 
A task or process is a piece of executable code. Each process 
runs in an environment made up of the contents of the pro-
cessor's registers, its program counter, its status register (SR), 
and the state of the memory allocated to this process. The 
environment defines the current state of the process and tells 
the computer where it's up to in the execution of a process. 
At any instant a process is in one of three states: running, 
runnable, or blocked. Figure 13.4 provides a state diagram for a 
process in a multitasking system. When a process is created, 
it is in a runnable state waiting its turn for execution. 
When the scheduler passes control to the process, it is running 
(i.e. being executed). If the process has to wait for a system 
resource such as a printer before it can continue, it enters the 
blocked state. The difference between runnable and blocked is 
simple—a runnable process can be executed when its turn 
comes; a blocked process cannot enter the runnable state until 
the resources it requires become free. 
Figure 13.2 Example of computing without multitasking. 
Powurce 
AaWny 
Slot I 
Siof i 
Slot 3 
5U)i A 
Situ r> 
S-oi f. 
i/DTl 
Process A 
Price;J A 
VDI,' 
Process B 
Process B 
Disk 
Process A Process B 
CPU 
Process A Process B Process A 
Process A 
-*time 
Figure 13.3 Applying multitasking to the system of Fig. 13.2. 
13.2.2 Switching processes 
We now outline the way in which process switching 
uses two mechanisms described earlier—the inter-
rupt and the stack. A clock connected to the CPU's 
interrupt request input generates a pulse, say, every 
0.01 seconds. At the moment the interrupt occurs, the 
information that defines the process is in the CPU 
(i.e. processor status word, program counter, and 
registers in use). This information is called the process's 
context or volatile portion. An interrupt saves the pro-
gram counter and machine status on the stack, and 
makes a jump to the interrupt handling routine. 

552 
Chapter 13 The operating system 
At the end of the interrupt handling routine an RTE (return 
from exception) instruction is executed and the program then 
continues from the point at which it was interrupted. 
The 68K's RTE instruction is similar to its RTS (return from 
subroutine) instruction. When a subroutine is called, the 
return address is pushed on the stack. When an exception 
(i.e. interrupt) is generated, both the return address and the 
Time allocation complete 
Resource becomes! 
available 
Waiting for a 
resource 
Figure 13.4 State diagram of a process in a multitasking system. 
current value of the processor status word (containing the 
CCR) are pushed on the stack. The RTE instruction restores 
both the program counter and the status word. Consequently, 
an exception doesn't affect the status of the processor. 
Suppose now that the interrupt handling routine modifies 
the stack pointer before die return from exception is executed. 
That is, the stack pointer is changed to point at another 
process's volatile portion. When the RTE is executed, the value 
of die program counter retrieved from the stack isn't that 
belonging to the program being executed just before the inter-
rupt. The value of the PC loaded by the return from exception 
belongs to a different process that was saved earlier when 
another program was interrupted—that process will now be 
executed. 
Figure 13.5 demonstrates the sequence of events taking 
place during process switching. Initially process A at the top 
of Fig. 13.5 is running. At time T, the program is interrupted 
by a real-time clock and control passed to the scheduler in the 
operating system. The arrow from the program to the sched-
uler shows the flow of control from process A to the operat-
ing system. The scheduler stores process A's registers, 
program counter, and status in memory. Process switching is 
also called context switching because it involves switching 
User tasks 
Process A 
interrupt 
Process 5 
interrupt 
Process B 
Interruot 
Operating system 
Scheduler 
Save registers of 
current process 
Select next process 
Restore registers 
of next process 
Scheduler 
Save registers of 
current process 
Select next process 
Restore registers 
of next process 
time = T 
Time 
time = T+ t 
The scheduler saves the current 
process's context (i.e. volatile 
portion) and invokes a new process. 
time = T+ Zt 
Figure 13.5 Switching 
processes. 
My turn to run 
I Runnable 
Kunir.ngj 
— f 
Blocked Y 

13.2 Multitasking 
553 
from the volatile portion of one process to the volatile 
portion of another process. The scheduler component of 
the operating system responsible for switching processes is 
called the first-level interrupt handler. 
In Fig. 13.5 an interrupt occurs at T, T + t, T + 2f,..., and 
every t seconds switching takes place between processes A and 
B. We have ignored the time required to process the interrupt. 
In some real-time systems, the process-switching overhead is 
very important. 
Figure 13.6 demonstrates how process switching works. Two 
processes, A and B, are located in memory. To keep things sim-
ple, we will assume that die regions of memory allocated to 
these processes do not change during die course of their execu-
tion. Each process has its own stack, and at any instant the stack 
pointer may be pointing to either A's stack or B's stack. 
In Fig. 13.6(a) process A is running and process A's stack 
pointer SPA is pointing at the top of the stack. In Fig. 13.6(b) 
a process-switching interrupt has occurred and the contents 
of the program counter and machine status have been 
pushed onto the stack (i.e. A's stack). For the sake of simpli-
city Fig. 13.6 assumes that all items on the stack occupy a 
single location. 
In Fig. 13.6(c) the operating system has changed the con-
tents of the system stack pointer so that it is now pointing at 
process B's stack (i.e. the stack pointer is SPB). Finally, in 
Fig. 13.6(d) the operating system executes an RTE and process 
B's program counter is loaded from its stack, which causes 
process B to be executed. Thus, at each interrupt, the operat-
ing system swaps the stack pointer before executing an RTE 
and a new process is run. 
A more realistic operating system maintains a table of 
processes to be executed. Each entry in the table is a task con-
trol block (TCB), which contains all the information the 
operating system needs to know about the process. The TCB 
•SP-. 
Task A's stack 
^ T } 
Task A 
Task A status word 
Task A return address 
Task A's stack 
Task A 
Task B status word 
Task B return address 
Task B's stack 
TaskB 
(a) Task A running. 
Task B status word 
Task B return address 
Task B's stack 
(b) Task A interrupted. 
TaskB 
CZTTi 
Task A status word 
Task A return address 
Task A's stack 
Task B status word 
Task B return address 
Task B's stack 
Sp.; 
Task A 
TaskB 
Task A status word 
Task A return address 
Task A's stack 
Task B's stack 
(C) Stack pointer modified by OS. 
(d) Task B running. 
Task A 
TaskB 
Figure 13.6 Process switching 
and the stack. 

554 
Chapter 13 The operating system 
includes details about the process's priority, its maximum 
run time, and whether or not it is currently runnable (as well 
as its registers). 
Figure 13.7 illustrates the structure of a possible task con-
trol block. In addition to the process's environment, the TCB 
contains a pointer to the next TCB in the chain of TCBs; that 
is, the TCBs are arranged as a linked list. A new process is 
created by inserting its TCB into the linked list. 
Some operating systems allow processes to be priorit-
ized so that a process with a high priority will always be 
executed in preference to a process with a lower priority. 
A runnable process is executed when its turn arrives 
PRE-EMPTIVE MULTITASKING 
There are two versions of multitasking. The simplest version 
is called non-pre-emptive multitasking or 
co-operative multitasking. When a task runs, it executes 
code until it decides to pass control to another task. 
Co-operative multitasking can lead to system crashes when 
a task does not relinquish control. 
In pre-emptive multitasking, the operating system forces 
a task to relinquish control after a given period. 
(subject to the limitations of priority). If the process 
is not runnable (i.e. blocked), it remains in the computer but 
is bypassed each time its turn comes. When the process is to 
be run, its run flag is set and it will be executed next time 
round. 
133 Operating system support 
from the CPU 
We now describe how a processor supports operating 
system functions. It's possible to design processors that are 
protected from certain types of error or that provide hard-
ware support for multitasking. First-generation 8-bit micro-
processors didn't provide the operating systems designer 
with any special help. Here, we concentrate on the 68K family 
because it provides particularly strong support to operating 
systems. 
At any instant a processor can be in one of several states or 
levels of privilege; for example, members of the 68K family 
provide two levels of privilege. One of the 68K's states is called 
the supervisor state and the other the user state. The operating 
system runs in the supervisor state and applications programs 
Process 1 
Process name 
Pointer to next process 
Process status 
Process priority 
Program counter 
Registers 
Memory requirement 
Process 2 
Process name 
Pointer to next process 
Process status 
Process priority 
Program counter 
Registers 
Memory requirement 
The task control blocks 
are arranged as a linked list 
Process 3 
Process name 
Pointer to next process 
Process status 
Process priority 
Program counter 
Registers 
Memory requirement 
: o ncs: process 
- TCB chain 
Fig. 13.7 The task control block. 

13.3 Operating system support from the CPU 
555 
running under the control of the operating system run in the 
user state. We will soon see that separating the operating sys-
tem from user applications makes the system very robust and 
difficult to crash. When an applications program crashes 
(e.g. due to a bug), the crash doesn't affect the operating sys-
tem running in its protected supervisor environment. 
13.3.1 Switching states 
Let's start from the assumption that the supervisor state used 
by the operating system confers first-class privileges on the 
operating system—we'll find out what these privileges are 
shortly. When the processor is running in its user state, any 
interrupt or exception forces it into its supervisor state. That 
is, an exception causes a transition to the supervisor state 
and, therefore, calls the operating system. 
Figure 13.8 illustrates two possible courses of action that 
may take place in a 68K system when an exception is generated. 
Both these diagrams are read from the top down. In each case, 
the left-hand side represents user or applications programs 
running in die user state and the right-hand side represents the 
operating system running in the supervisor state. 
In Fig. 13.8(a) a user program is running and an exception 
occurs (e.g. a disk drive may request a data transfer). A jump is 
made to the exception handler that forms part of the operating 
system. The exception handler deals with the request and a 
return is made to the user program. However, the exception 
might have been generated by a fatal error condition that arises 
during the execution of a program. Figure 13.8(b) shows the 
situation in which an exception caused by a fatal error occurs. 
In this case, the operating system terminates the faulted user 
program and then runs another user program. 
Figure 13.8(a) and (b) show user programs and the operat-
ing system existing in separate compartments or environ-
ments. We now explain why user programs and the operating 
system sometimes really do live in different universes. In sim-
ple 68K-based systems, the processor's supervisor and user 
state mechanisms aren't exploited, and all code is executed in 
the supervisor state. More sophisticated systems widi an oper-
ating system do make good use of the 68K's user and super-
visor state mechanisms. 
When power is first applied to the 68K, it automatically 
enters its supervisor state. This action makes sense, because 
you would expect the operating system to initially take con-
trol of the computer while it sets everything up and loads tiie 
user processes that it's going to run. 
The three questions we've now got to answer are the 
following. 
• How does the 68K know which state it's in? 
• How is a transition made from one state to another? 
• What does it matter anyway? 
The answer to the first question is easy—the 68K uses a flag 
bit, called an S-bit, in its status register to indicate what state 
it's currently operating in. If S = 1, the processor is in its 
supervisor state and if S = 0, the processor is in its user state. 
The S-bit is located in bit 13 of the 16-bit status register (SR). 
The lower-order byte of the status register is the condition 
code register (CCR). The upper byte of the status register 
containing the S-bit is called the system byte and defines the 
operating state of the processor. 
The second question we asked was 'How is a transition 
made from one state to another?' The state diagram in 
Fig. 13.9 describes the relationship between the 68K's user 
User state 
Exception 
.v.!| iorvivor —arc 
=vcop:icn 
hardier 
User state 
Exception 
(a) Program execution continues 
after an exception. 
(b) The operating system kills the current task 
and starts another. 
Figure 13.8 Taking 
action after an 
exception. 
iTask killed 
Si-
Exception 
handler 
» .rlurn 
S-jper/:>'.jE •••.-in-
New ' 
task 
I 

556 
Chapter 13 The operating system 
Any exception 
Any exception 
S<-0 
Figure 13.9 Switching between user and supervisor states. 
and supervisor states. Lines with arrows indicate transitions 
between states (text against a line explains the action that 
causes the transition). Figure 13.9 shows that a transition 
from the supervisor state to the user state is made by clearing 
the S-bit in the status register. Executing a MOVE #0, SR 
instruction clears the S-bit (and the other bits) of the status 
byte and puts the 68K in the user state. You could clear only 
the S-bit with the instruction ANDI #$DFFF, SR. 
When the operating system wishes to execute an applica-
tions program in the user state, it clears the S-bit and executes 
a jump to the appropriate program; that is, the operating sys-
tem invokes the less privileged user state by executing an 
instruction that clears the S-bit to 0. 
Figure 13.9 demonstrates that once the 68K is running in its 
user state, the only way in which a transition can be made to 
the supervisor state is by means of an exception—any excep-
tion. A return can't be made to the supervisor state by using an 
instruction to set the S-bit to 1. If you could do this, anyone 
would be able to access the supervisor state's privileged fea-
tures and the security mechanism it provides would be worth-
less. Let's say that again—a program running in the user state 
cannot deliberately invoke the supervisor state direcdy. 
Suppose a user program running in the user state tries 
to enter the privileged supervisor state by executing 
MOVE $2000, SR to set the S-bit. Any attempt by the user state 
programmer to modify the S-bit results in a privilege viola-
tion exception. This exception forces the 68K into its supervisor 
state, where the exception handler deals with the problem. 
We can now answer the third question—what's the benefit 
of the 68K's two-state mechanism? Some instructions such as 
STOP and RESET can be executed only in the supervisor state 
and are said to be privileged. The STOP instruction brings the 
processor to a halt and the RESET acts on external hardware 
such as disk drives. You might not want the applications pro-
grammer to employ these powerful instructions that may 
cause the entire system to crash if used inappropriately. Other 
privileged instructions are those that operate on the system 
byte (including the S-bit) in the status register. If the applica-
tions programmer were permitted to access the S-bit, they 
could change it from 0 (user state) to 1 (supervisor state) and 
bypass the processor's security mechanism. 
If the 68K's user/supervisor mode mechanism were lim-
ited to preventing the user-state programmer executing cer-
tain instructions, it would be a nice feature of the processor, 
but of no earth-shattering importance. The user/supervisor 
state mechanism has two important benefits; the provision of 
dual stack pointers and the support for memory protection. 
These two features protect the operating system's memory 
from either accidental or deliberate modification by a user 
application. We now describe how the 68K's supervisor state 
protects its most vital region of memory—the stack. 
13.3.2 The 68K's two stacks 
Most computers manage subroutine return addresses by 
means of a stack. The processor's stack pointer points to the 
top of the stack and the stack pointer is automatically updated 
as items are pushed onto the stack or are pulled off it. When a 
subroutine is called by an instruction like BSR XYZ, the address 
immediately after the subroutine call (i.e. the return address) is 
pushed on the stack. The final instruction of the subroutine, 
RTS (return from subroutine), pulls the return address off the 
stack and loads it in the program counter. 
If you corrupt the contents of the stack by overwriting the 
return address or if you corrupt the stack pointer itself, the 
RTS will load an undefined address into the program 
counter. Instead of making a return to a subroutine's calling 
point, the processor will jump to a random point in memory 
and start executing code at that point. The result might lead 
to an illegal instruction error or to an attempt to access non-
existent memory. Whatever happens, the program will crash. 
Consider the following fragment of very badly written 
code that contains a serious error. Don't worry about the fine 
details—it's 
the underlying 
principles 
that 
matter. 
Remember that the 68K's stack pointer is address register A7. 
MOVE.W D3,-(A7) Push the parameter in register D3 onto the stack 
BSR 
Sub_X 
Call a subroutine 
Return here 
Sub_X ADDA.L #4,A7 
Step over the return address on the top of the stack 
MOVE.Ii (A7)+,D0 Read the parameter from the stack 
SUBA.L #6,A7 
Restore the stack pointer 
The body of the subroutine goes here.... 
RTS 
Return from subroutine 
S.ipeiv.inrl 
. 
sUte 
I 
' 
User 
state 
k. 
i 

13.3 Operating system support from the CPU 
557 
(a) State of the stack 
after subroutine call. 
(b) State of the stack 
after adjusting the SP. 
Figure 13.10 The effect of an error on the stack. 
(c) State of the stack 
after incorrectly reading 
the parameters. 
(d) State of the stack 
after adjusting the 
stack pointer. 
The programmer first pushes the 16-bit parameter in data 
register D3 onto the stack by means of MOVE. w D3 , (A7), and 
then calls a subroutine at location Sub_x. Figure 13.10(a) 
illustrates the state of the stack at this point. As you can see, the 
stack contains the 16-bit parameter (one word) and the 32-bit 
return address (two words) on top of the buried parameter. 
When the subroutine is executed, the programmer attempts 
to retrieve the parameter from the stack by first stepping past 
the 4-byte return address on the top of the stack. The instruc-
tion ADDA.L #4,A7 adds 4 to the stack pointer to leave it 
pointing at the required parameter (Figure 13.10(b)). This is a 
terrible way of accessing the parameter because you should 
never move the stack pointer down the stack when mere are 
valid items on the stack above the stack pointer—do remember 
that we're providing an example of how not to do things. 
The programmer then reads the parameter from the stack by 
means of the operation MOVE. L (A7)+ , DO. This instruction 
pulls a longword off the stack and increments the stack pointer 
by the size of the operand (four for a longword) (Fig. 13.10(c)). 
Because the stack pointer has been moved down by first step-
ping past the return address and then pulling the parameter off 
the stack, it must be adjusted by six to point to the subroutine's 
return address once more (i.e. a 4-byte return address plus a 
2-byte parameter) (Fig. 13.10(d)). Finally, the return from 
subroutine instruction RTS) pulls the 32-bit return address off 
the stack and loads it in the program counter. 
This fragment of code fails because it contains a serious 
error. The parameter initially pushed on the stack was a 16-bit 
value, but the parameter read from the stack in the subroutine 
was a 32-bit value. The programmer really intended to 
write the instruction MOVE.W (A7)+,D0 rather than 
MOVE . L (A7) + , DO; the error in die code is just a single letter. 
The effect of this error is to leave the stack pointer pointing 
at the second word of the 32-bit return address, rather than 
the first word. The SUBA . L # 6, A7 instruction was intended to 
restore the stack pointer to its original value. However, because 
the stack pointer is pointing 2 bytes above the correct return 
address, the RTS instruction loads the program counter with 
Selected in user state 
when S = 0""^^. 
USP 
User stack pointer 
Selected in supervisor 
state when S = 1 
SSP 
Supervisor 
stack pointer 
User stack 
Supervisor stack 
Figure 13.11 The 68K's two stack pointers. 
an erroneous return address resulting in a jump to a random 
region of memory. We have demonstrated that this blunder 
not only gives the wrong result, but also generates a fatal error. 
We now demonstrate how the user/supervisor mechanism 
helps us to deal with such a situation. 
The 68K's user and supervisor stack pointers 
There's very little the computer designer can do to prevent pro-
gramming errors that corrupt either die stack or the stack 
pointer. What the computer designer can do is to limit the 
effects of possible errors. Members of the 68K family approach 
the problem of stack security by providing two identical stack 
pointers—each of which is called address register A7 (see 
Fig. 13.11). However, both stack pointers can't be active at the 
same time because either one or the other is in use (it's a bit like 
Clark Kent and Superman—you never see them together). 
One of the 68K's two stack pointers is called die supervisor 
stackpointer (SSP) and is active whenever the processor is in the 
supervisor state. The ouier stack pointer, the user stackpointer 
0 
Return «—[ SP | 
0 
a e U i m 
0 
Return 
„ 
[-7— 
0 
R , u ; r , 
2 
a d c l r eg 
2 
aadrejs 
2 
address "* 
L i L I 
2 
ackires. 
4 Parameter 
4 Parameter •<— SP 
4 Parameter 
4 Parameter 
6 
6 
6 
6 

558 
Chapter 13 The operating system 
(USP) is active when the processor is in the user state. Because 
the 68K is always in either the user state or the supervisor state, 
only one stack pointer is available at any instant. The supervisor 
stack pointer is invisible to the user programmer—there's no 
way in which the user programmer can access the supervisor 
stack pointer. However, the operating system in the supervisor 
state can use privileged instruction MOVE USP,Ai and MOVE 
Ai, USP to access the user stack pointer. 
Let's summarize what we've just said. When the 68K is 
operating in its supervisor state, its S-bit is 1 and the super-
visor stack pointer is active. The supervisor stack pointer points 
at the stack used by the operating system to handle its sub-
routine and exception return addresses. Because an exception 
sets the S-bit to 1, the return address is always pushed on the 
supervisor stack even if the 68K was running in the user 
mode at the time of the exception. When the 68K is operating 
in its user state, its S-bit is 0 and the user stack pointer is 
active. The user stack pointer points at the stack used by the 
current applications program to store subroutine return 
addresses. 
Consider the previous example of the faulty applications 
program running in the user state (see Fig. 13.11). When the 
return from subroutine instruction is executed, an incorrect 
return address is pulled off the stack and a jump to a random 
location made. An illegal instruction exception will eventu-
ally occur when the processor tries to execute a data pattern 
that doesn't correspond to a legal op-code. An illegal instruc-
tion exception forces a change of state from user to super-
visor mode. The illegal instruction exception handler runs in 
the supervisor state, whose own stack pointer has not been 
corrupted. That is, the applications programmer can corrupt 
their own stack pointer and crash their program, but the 
operating system's own stack pointer will not be affected by 
the error. When a user program crashes, the operating system 
mounts a rescue attempt. 
You may wonder what protects the supervisor stack 
pointer. Nothing. It is assumed that a well constructed and 
debugged operating system rarely corrupts its stack and 
crashes (at least in comparison with user programs and pro-
grams under development). 
The 68K's two-stack architecture doesn't directly prevent 
the user programmer from corrupting the contents of the 
operating system's stack. Instead, it separates the stack used 
by the operating system and all exception-processing soft-
ware from the stack used by the applications programmer by 
implementing two stack pointers. Whatever the user does in 
their own environment cannot prevent the supervisor step-
ping in and dealing with the problem. 
Use of two stacks in process switching 
Earlier in this chapter we described the notion of multitask-
ing. The 68K's two stack pointer mechanism is particularly 
useful in implementing multitasking. Each user program has 
its own private stack. When the process is running, it uses the 
USP to point to its stack. When the process is waiting or 
blocked, its own stack pointer is saved alongside the other ele-
ments of its volatile portion (i.e. environment) in its task 
control block. 
The supervisor stack pointer is used by the operating sys-
tem to manage process switching and other operating system 
functions. In this way, each application can have its own user 
stack pointer and the operating system's stack can be sep-
arated from the user processes. 
Suppose an applications program (i.e. process) is running 
and a process-switching interrupt occurs. A jump is made to 
the scheduler, the S-bit is set, the supervisor stack pointer 
becomes active, and the return address and status word are 
saved on the supervisor stack. 
The CPU's address and data registers plus its PC and status 
register hold information required by the interrupted 
process. These registers constitute the process's volatile por-
tion. The scheduler saves these registers on the supervisor 
stack. You can use MOVEM. L D0-D7/A0-A6, -(A7) to push 
registers DO to D7 and AO to A6 onto the stack pointed at by 
A7.1 We don't save A7 because that's the supervisor stack 
pointer. We do need to save the user stack pointer because 
that belongs to the process. We can access the USP by MOVE 
USP, AO and then save AO on the supervisor stack with the 
other 15 registers (PC and SR). 
Having saved the last process's volatile portion, the sched-
uler can go about its job of switching processes. The next step 
would be to copy these registers from the stack to the 
process's entry in its task control block. Typically, the sched-
uler might remove the process's volatile environment from 
the top of the supervisor stack and copy these registers to the 
process's task control block. 
The scheduler can now locate the next process to run 
according to an appropriate algorithm (e.g. first-come-first-
served, highest priority first, smallest process first, etc.). Once 
the next process has been located it can be restarted by copy-
ing the process's registers from the TCB to the supervisor 
stack and then pulling the registers off the stack immediately 
before executing an RTE instruction. Note that restoring a 
process's volatile environment is the mirror image of saving a 
process's volatile environment. 
The behavior of the task-switching mechanism can be 
expressed as pseudocode. 
1 The 68K instruction MOVEM.L A0-A3/D2-D4 -(A7) pushes regis-
ters AO to A3 and D2 to D4 on the stack pointed at by A7. The mnemonic 
MOVEM means move multiple and lets you copy a group of registers onto 
the stack in one operation, MOVEM.L (A7)+ , A0-A3/D2-D4 performs the 
inverse operation and pulls seven registers off the stack and restores them 
to AO to A3 and D2 to D4. 

13.3 Operating system support from the CPU 
559 
Module TaskSwitch 
Disable all further interrupts 
Push registers DO to D7 and AO to A6 on the supervisor stack 
Get the process's stack pointer from the USP and put it on the stack 
Transfer all registers, PC, and SR from the stack to TaskControlBlocki 
Locate ProcesSj the next process to run 
Copy registers of next process from TaskControlBlockj to the stack 
Copy process j's stack pointer from TCB to USP 
Pull registers DO to D7 and AO to A6 from the stack 
Enable interrupts 
Return from exception (i.e., restore SR, and PC) 
End module 
We represent this algorithm in the following 68K program. 
In order to test the task-switching mechanism, we've created a 
dummy environment with two processes. Process 1 prints the 
number 1 on the screen whenever it is executed. Process 2 prints 
the sequence 2,3,4,..., 9,2,3,... when it is called. If we allow 
each process to complete one print cycle before the next process 
is called, the output should be 12131415 . . . . 18191213 ... 
In a real system, a real-time clock might be used to period-
ically switch tasks. In our system we use a TRAP #0 instruction 
to call the task switcher. This instruction acts like a hardware 
interrupt that is generated internally by an instruction in the 
program (i.e. the program counter and status registers are 
pushed on the supervisor stack and a jump is made to the 
TRAP #0 exception handling routine whose address is in 
memory location $00 0080. 
The program is entered at $400 where the supervisor stack 
pointer is initialized and dummy values loaded into A6 and 
A0 for testing purposes (because much of the program 
involves transferring data between registers, the stack, and 
task control blocks, it's nice to have visible markers when you 
are debugging a program by single-stepping it). 
We have highlighted the body of the task switcher. The 
subroutine NEW selects the next process to run. In this case, 
there are only two processes and the code is as follows. 
IF task 1 running THEN next task = task 2 
ELSE next task = task 1 
TRAPO 
SAVE 
ORG 
DC.L 
$80 
TRAPO 
TRAP 
#0 vector 
Address of trap 0 handler 
ORG 
$400 
Entry point of program 
LEA 
$4000,A7 
Preset the SSP 
LEA 
$A6A6A6A6,A6 
Dummy value in A6 to help in tracing 
MOVEA.L #$12121212,A0 
Dummy value for USP 
MOVE.L 
A0,USP 
BRA 
TASK1 
Jump into task 1 
The task switcher is entered from TRAP #0 
MOVEM.L A0-A7/D0-D7,-(SP) 
Dump all registers on the stack 
MOVEA.L CURRENT,A0 
MOVE.W 
#34,DO 
MOVE.W 
(SP) + , (A0) + 
DBRA 
DO , SAVE 
MOVE.L 
USP.A1 
MOVE.L 
Al,-10(AO) 
Get pointer to current TCB 
Copy SR, PC, and registers to TCB 
This is 35 words (SR + PC + 16 registers) 
Get task's A7 = USP 
Save USP in A7 slot in TCB 
All the current task's environment is now saved in its TCB 
BSR 
NEW 
Switch tasks 
Now restore the new task 
MOVEA.L 
CURRENT,A0 
LEA 
70(A0),A0 
Get pointer to the new TCB 
Point to past end of TCB 
MOVE.W 
#34,DO 
Copy SR, PC, registers from TCB 

5 6 0 
Chapter 13 The operating system 
RESTORE MOVE.W 
-(AO),-(SP) 
This is 35 words (1 + 2 + 16 x 2) 
DBRA 
DO,RESTORE 
MOVEA.L 60(A0),A1 
Get USP from TCB 
MOVE.L 
A1,USP 
Restore USP 
MOVEM.L (SP)+,A0-A6/D0-D7 Restore registers from stack 
LEA 
4(SP),SP 
Skip past A7 on stack 
RTE 
Load SR and PC to return from exception 
NEW 
NOT1 
FINISH 
TASK1 
Switch tasks (simple routine goes 1,2,1,2,...) 
MOVEA.L CURRENT,A0 
Get current task pointer 
CMPA.L 
#TCB1,A0 
If it's 1 then make it 2 
BNE 
NOT1 
MOVE.L 
#TCB2 , CURRENT 
BRA 
FINISH 
MOVE.L 
#TCB1,CURRENT 
If it's 2 then make it 1 
RTS 
A dummy task that prints 1 and ends 
Call OS to print 
MOVE.B 
#'1',Dl 
MOVE.B 
#6, DO 
TRAP 
#15 
TRAP 
#0 
BRA 
TASK1 
Switch tasks 
Repeat 
TASK2 
ADD.B 
#1,D1 
MOVE.B 
# 6,DO 
TRAP 
#15 
TRAP 
#0 
CMP.B 
#'9',Dl 
BNE 
TASK2 
MOVE.B 
#'1',Dl 
BRA 
TASK2 
A dummy task that prints a number and ends 
Call OS to print 
Switch tasks 
Reset sequence 
Repeat 
ORG 
$1000 
DC.L 
TCB1 
ORG 
$2000 
DS.W 
35 
ORG 
$2080 
DC.L 
$D0D0D0D0 
DC.L 
$00000031 
DS.L 
6 
DC.L 
$A0A0AOAO 
DS.L 
6 
DC.L 
$77777770 
DC.W 
$00FF 
DC.L 
TASK2 
Pointer to current TCB 
Space for task 1 TCB 
Task 2 TCB (preset) 
Dummy DO 
Initial Dl (ASCII "1") 
Dummy A0 
Dummy A7 = USP 
Dummy SR 
Address of TASK2 for PC 
END 
$400 
Let's look at how task switching takes place. When an 
exception takes place (in this case a TRACE # 0 exception), the 
program counter (return address) and status register are 
pushed on the supervisor stack to give the situation of 
Fig. 13.12(a). 
The first instruction in the process switcher, MOVEM.L 
A0-A7/D0-D7, -(SP) pushes all the 68K's address and data 
registers on the supervisor stack. Together with the PC and 
SR, we now have the process's entire volatile environment on 
the stack. Well, not entirely. Remember that the 68K has a 
user stack pointer. So, we copy it into Al and then put it in the 
'A7' slot on the stack (thereby overwriting the copy of the 
supervisor stack pointer in that slot). 
SP 
Status 
PC 
SP 
(a) State of the stack 
after TRAP #0. 
DO 
"D7 
A0 
A7 
Status 
PC 
1 6 x 4 = 64 bytes 
2 + 4 = 6 bytes 
(b) State of the stack 
after pushing registers. 
Figure 13.12 Use of the stack during process switching. 
CURRENT 
TCB1 
TCB2 

13.4 Memory management 
561 
Task A 
Task A 
unused 
Task E 
Task A 
Task A 
unused 
!..-IL::,K: 
TaskB 
unused 
T^k i.) 
Tatk D 
TaskB 
unused 
unused 
Tas-.. E 
TaskC 
TaskC 
TaskC 
"dik f 
(a) Three tasks 
occupy memory. 
(b) TaskB deleted 
and its 
memory freed. 
Task E is split 
/ into two parts 
(c) Task A deleted 
(d) Task E in two 
and task D 
parts, 
started. 
Figure 13.13 Memory 
fragmentation in a multitasking 
environment. 
The next step is to copy all these registers to the task control 
block pointed at by CURRENT (the variable that points to the 
active TCB). This operation saves the current task's volatile 
portion. 
The task control block is changed by calling NEW to find the 
next task. The registers saved in the TCB are then copied to 
the stack and then restored from the stack to invoke the new 
process. 
Now that we've described the 68K's user and supervisor 
modes and the role of exceptions in process switching, we can 
introduce one of the most important aspects of an operating 
system, memory management. 
13.4 Memory management 
We've assumed that the computer's central processing unit 
generates the address of an instruction or data and that this 
address corresponds to the actual location of the data in 
memory. For example, if a computer executes MOVE $ 12 3 4, DO, 
the source operand is found in location number 123416 
in the computer's random access memory. Although this 
statement is true of simple microprocessor systems, it's not 
true of computers with operating systems such as UNIX 
and Windows. An address generated by the CPU doesn't 
necessarily correspond to the actual location of the data in 
memory. Why this is so is the subject of this section. 
Memory management is a general term that covers all the 
various techniques by which an address generated by a CPU 
is translated into the actual address of the data in memory. 
Memory management plays several roles in a computer sys-
tem. First, memory management permits computers with 
small main stores to execute programs that are far larger than 
the main store.2 Second, memory management is used in 
multitasking operating systems to make it look as if each 
process has sole control of the CPU. Third, memory manage-
ment can be employed to protect one process from being cor-
rupted by another process. Finally, memory management, in 
conjunction with the operating system, deals with the alloca-
tion of memory to variables. 
If all computers had an infinite amount of random access 
memory, life would be much easier for the operating system 
designer. When a new program is loaded from disk, you can 
place it immediately after the last program you loaded into 
memory. Moreover, with an infinitely large memory you never 
have to worry about loading programs that are too large for the 
available memory. In practice, real computers may have too 
little memory. In this section we are going to look at how 
the operating system manages the available memory. 
Figure 13.13(a) demonstrates multitasking where three 
processes, A, B, and C are initially in memory. This diagram 
shows the location of programs in the main store. In 
Fig. 13.13(b) process B has been executed to completion and 
deleted from memory to leave a hole in the memory. In 
Fig. 13.13(c) a new process, process D, is loaded in part of the 
unused memory and process A deleted. Finally, in Fig. 13.13(d) 
a new process, process E, is loaded in memory in two parts 
because it can't fit in any single free block of memory space. 
A multitasking system rapidly runs into the memory alloca-
tion and memory fragmentation problems described by 
Fig. 13.13. Operating systems use memory management to 
map the computer's programs onto the available memory 
space. Memory management is carried out by means of 
special-purpose hardware called a memory management unit 
(MMU) (see Fig. 13.14). Today's sophisticated micropro-
cessors like the Pentium include an MMU on the same chip as 
the CPU. Earlier microprocessors often used external MMUs. 
The CPU generates the address of an operand or an instruc-
tion and places it on its address bus. This address is called a log-
ical address—it's the address that the programmer sees. The 
MMU translates the logical address into the location or phys-
ical address of the operand in memory. Figure 13.14 shows how 
2 Running programs larger than the actual immediate access memory 
was once very important when memory cost a fortune and computers 
had tiny memory systems. 

562 
Chapter 13 The operating system 
the logical address 1234567816 address from the CPU gets 
mapped onto the physical address ABC67816. 
The logical address consists of two parts, a page address and 
a word address. In the previous example, page 1234516 gets 
translated into page ABC,6 and the word address 67816 remains 
unchanged. Figure 13.15 illustrates the relationship between 
word address and page address for a very simple computer sys-
tem with four pages of eight words (i.e. 4 X 8 = 32 locations). 
The logical address from the CPU in Fig. 13.15 consists of 
a 2-bit page address that selects one of 22 = 4 pages, and a 
3-bit word address that provides an offset (or index) into the 
currently selected page. A 3-bit offset can access 23 = 8 words 
within a page. If, for example, the CPU generates the address 
101102, location 6 on logical page 2 is accessed. 
In a system with memory management the 3-bit word 
address from the CPU goes directly to the memory, but the 
2-bit page address is sent to the memory management unit (see 
Fig. 13.16). The logical page address from the CPU selects an 
entry in a table of pages in the MMU as Fig. 13.16 demon-
strates. Suppose the processor accesses logical page 2 and the 
corresponding page table entry contains the value 3. This 
value (i.e. 3) corresponds to the physical page address of the 
location being accessed in memory; that is, the MMU has 
translated logical page 2 into physical page 3. The physical 
Physical 
rtddiv-ss 
Keinurv 
Figure 13.14 The memory management unit. 
Physical memory 
. 
"J 
2-bit page 
3-bit word 
2-bit page 
3-bit word 
address 
address 
q 
in m 
f 
2 
6 
2 
6 
| f \ \ 
/ / 
A 
* 
wordO 
/ 
\ 
* 
word 1 
The page number 
\ 
o f f s e t g 
selects page 2 
>^ 
* 
wordZ 
The page number 
\ 
o f f s e t g 
selects page 2 
>^ 
* 
wordd 
The page number 
\ 
o f f s e t g 
selects page 2 
>^ 
* 
word 4 
/ " • 
* 
word i> 
/ " • 
word 6 
word 7 
word 6 on the page 
word 6 on the page 
PageO 
Page 2 
Page 3 
5-bit logical address 
2-bit page 
3-bit word 
. address , .. address 
. 
< 
• - < 
>• 
Page 1 
Logical! 
page 2/ 
, The offset 6 selects 
\ word 6 on the page 
pLogical Physical 
page 
page 
0 
0 
1 
2 
~*2 
3 — 
3 
1 
Physical 
page 3 
word 1 
word Z 
avord 3 
word 4 
word 5 
PageO 
Pagel 
Page 2 
Page 3 
MMU 
Figure 13.15 The structure of paged memory. 
Figure 13.16 Mapping logical onto physical pages. 
Page number Address on page 
N. 
Word address (address on page) 
Logical 
f 
C 
~ 
f" 
\ 
address 
^ 
, 
^ 
. 
^ 
/ 
Lopii.al ^ __ cy> r x ^ 
CPU;'.,;;• 
^>erc'L 
. 
. 
*g^ 
Memory 
/-\ 
| 
management 
I 
I 
unit 
* 
/ 
^ 
• 
_J 
/ 
I 
I Physical page 
L 
' 
number 
The MMU translates a 
logical page address into 
a physical page address 
The MMU translates a 
logical page address into 
a physical page address 

13.4 Memory management 
563 
address corresponds to the location of the actual operand in 
memory. If you compare Figs 13.15and 13.16 you can see that 
the same logical address has been used to access two different 
physical addresses. 
Why should the operating system take an address from the 
processor and convert it into a new address to access physical 
memory? To answer this question we have to look at how pro-
grams are arranged in memory. Figure 13.17 shows the struc-
ture of both logical memory and physical memory during the 
execution of processes A, B, C, and D. As far as the processor 
is concerned, the processes all occupy single blocks of address 
space that are located consecutively in logical memory 
(Fig. 13.17(a)). 
If you examine the physical memory (Fig. 13.17(b)), the 
actual processes are distributed in real memory in an 
almost random fashion. Processes B and C are split into non-
consecutive regions and two regions of physical memory are 
unallocated. The logical address space seen by the processor is 
larger than the physical address space—process D is currently 
located on the hard disk and is not in the computer's RAM. 
This mechanism is called virtual memory. 
A processor's logical address space is composed of all the 
addresses that the processor can specify. If the processor has a 
32-bit address, its logical address space consists of 232 bytes. 
The physical address space is the memory and its size 
depends on how much memory the computer user can 
afford. We will soon see how the operating system deals with 
situations in which the processor wishes to run programs that 
are larger than the available physical address space. The func-
tion of the MMU is to map the addresses generated by the 
CPU onto the actual memory and to keep track of where data 
is stored as new processes are created and old ones removed. 
With an MMU, the CPU doesn't have to worry about where 
programs and data are actually located. 
CPU memory space 
Actual memory space 
Task A 
TaskB 
TaskC 
TaskD 
TaskC 
Task A 
unused 
TaskB 
unused 
TaskC 
TaskB 
Hard disk 
Task D is not in 
RAM—it's on disk 
(a) Logical address space, (b) Physical address space. 
Figure 13.17 Logical and physical address space. 
Consider a system with 4-kbyte logical and physical pages 
and suppose the processor generates the logical address 
88123416. This 24-bit address is made up of a 12-bit logical 
page address 88116 and a 12-bit word address 23416. The 12 
low-order bits (234,6) define the same relative location 
within both logical and physical address pages. The logical 
page address is sent to the MMU, which looks up the corre-
sponding physical page address in entry number 881 in the 
page table. The physical page address found in this location is 
passed to memory. 
Let's look at the way in which the MMU performs map-
ping. Figure 13.18 demonstrates how the pages or frames of 
logical address space are mapped onto die frames of physical 
address space. The corresponding address mapping table is 
described in Table 13.1. Notice that logical page 3 and logical 
page 8 are both mapped onto physical page 6. This situation 
might arise when two programs share a common resource 
(e.g. a compiler or an editor). Each program thinks that it has 
a unique copy of the resource, although both programs access 
a shared copy of the resource. 
13.4.1 Virtual memory 
We've already said that a computer can execute programs 
larger than its physical memory. In a virtual memory system 
the programmer sees a large array of physical memory (the 
virtual memory), which appears to be entirely composed of 
high-speed main store. In reality, the physical memory is 
composed of a relatively small high-speed RAM and a much 
larger but slower disk store. Virtual memory has two advant-
ages. It allows the execution of programs larger than the 
physical memory would normally permit and frees the pro-
grammer from worrying about choosing logical addresses 
falling within the range of available physical addresses. 
Programmers may choose any logical address they desire for 
their program and its variables. The actual addresses selected 
by a programmer don't matter, because the logical addresses 
are automatically mapped into the available physical memory 
space as the operating system sees fit. 
The means of accomplishing such an apparently impos-
sible task is called virtual memory and was first used in the Adas 
computer at the University of Manchester, England in 1960. 
Figure 13.19 illustrates a system with 10 logical address pages 
but only five physical address pages. Consequently, only 50% 
of the logical address space can be mapped onto physical 
address space at any instant. Table 13.2 provides a logical page 
to physical page mapping table for this situation. Each entry 
in the logical address page table has two entries: one is the 
present bit, which indicates whether the corresponding page 
is available in physical memory and the other is the logical 
page to physical page mapping. 
Part of a program that's not being used resides on disk. 
When this code is to be executed, it is copied from disk to the 

564 
Chapter 13 The operating system 
Logical address space 
Physical address space 
0 
—""--—-J 
\ 
8 
/* 
\ 
0 
—""--—-J 
\ 
8 
/ 
yJ 
\ 
0 
—""--—-J 
\ 
8 
\ > 
0 
—""--—-J 
\ 
8 
0 
—""--—-J 
\ 
8 
, ^ — 
—7 
0 
—""--—-J 
\ 
8 
/ 
/ 
0 
—""--—-J 
\ 
8 
.—v-
y 
0 
—""--—-J 
\ 
8 
0 
—""--—-J 
\ 
8 
X 
/ 
0 
—""--—-J 
\ 
8 
/ 
^ * 
Logical page 
Physical page • 
Figure 13,18 Mapping logical address 
space onto physical address space. 
Logical page 
Physical page 
Logical address space 
Table 13.1 Logical to physical address map-
ping table corresponding to Fig. 13.18. 
computer's immediate access memory. Sometimes it's 
impossible to fit all the program (and the data required by the 
program) in main memory. Consequently, only part of the 
program can be loaded into random access memory. The 
operating system divides the program into pages and loads 
some of these pages into its random access memory. As pages 
are loaded, the operating system updates the page table in the 
MMU so that each logical page can be mapped onto the cor-
responding physical page in RAM. 
Consider what happens when a program that resides par-
tially in memory and partially on disk is executed. When the 
processor generates a logical address, the memory manage-
ment unit reads the mapping table to look up the corres-
ponding physical page address. If the page is present in 
RAM, a logical to physical address translation takes place and 
Physical address space 
0 
1 
2 
3 
4 
0 
0 
1 
2 
^» 
3 
4 
—-*' 
\ 
5 
\ 
6 
s ^ ' 
7 
**^ 
Physical 
8 
Logical page 
y 
Figure 13.19 A system where physical address space < logical 
address space. 
the information is accessed. However, if the logical page is 
currently not in RAM, an address translation cannot take 
place. In this case, the MMU sends a special type of interrupt 
to the processor called a page fault. 
When the processor detects a page fault, the operating sys-
tem intervenes and copies a page of data from the disk to the 
random access memory. Finally, the operating system updates 
the page-mapping table in the MMU and reruns the faulted 
memory access. This arrangement is called virtual memory 
because the processor appears to have a physical memory as 
large as its logical address space. 
Virtual memory works effectively only if, for most of the 
time, the data being accessed is in physical memory. 
Fortunately, accesses to programs and their data are highly 
clustered. Operating systems designers speak of the 80:20 
ru[e—for 80% of the time the processor accesses only 20% of 
a program. Note that the principles governing the operation 
0 
2 
1 
5 
2 
8 
3 
6 
4 
3 
5 
4 
6 
0 
7 
1 
8 
6 
9 
9 
0 , 
1 
2 
3 
4 
5 
6 
7 
8 
9 

13.4 Memory management 
5 6 5 
of virtual memory are, essentially, the same as those 
governing the operation of cache memory (described later). 
When a page fault is detected, the operating system trans-
fers a new page from disk to physical memory and over-
writes a page in physical memory. If physical memory is full, 
it's necessary to discard an existing page. The most sensible 
way of selecting an old page for removal is to take the page 
that is not going to be required in the near future. 
Unfortunately, this scheme is impossible to implement. A 
simple page replacement algorithm is called the not-
recently-used algorithm, which is not optimum but it is very 
easy to implement. 
When a new page replaces an old page, any data in the old 
page frame that has been modified since it was created must be 
written back to disk. A typical virtual memory system clears a 
dirty bit in the page table when the page is first created. 
Whenever the processor performs a write operation to an 
operand on this page, the dirty bit is set. When this page is 
swapped out (i.e. overwritten by a new page), the operating 
Logical page 
Present bit 
Physical page 
0 
1 
0 
1 
1 
3 
2 
0 
3 
1 
1 
4 
0 
5 
0 
6 
1 
2 
7 
1 
4 
8 
0 
9 
0 
Table 13.2 Logical to physical address mapping table 
corresponding to Fig. 13.19. 
system looks at its dirty bit. If this bit is clear, nothing need be 
done; if it is set, the page must be copied to disk. 
Virtual memory allows the programmer to write programs 
without having to know anything about the characteristics of 
real memory and where the program is to be located. 
13.4.2 Virtual memory and the 
68K family 
Members of Motorola's 68K family are well suited to virtual 
memory technology. We've already stated that the 68K's archi-
tecture provides mechanisms to support operating systems. 
The 68K's protected state when S = 1 separates operating sys-
tem and application level programs (aided by the dual stack 
pointer mechanism). 68K processors have a function control 
output that tells an external system such as a memory manage-
ment unit whether the CPU is executing an instruction in the 
user or the supervisor state. 
Figure 13.20 illustrates the dialogue that takes place 
between the CPU, the memory management unit (MMU), 
and the memory system during a read or a write cycle. The 
MMU is configured by the operating system when the com-
puter is first powered up. The operating system sets up logical 
address to physical address translation tables and defines the 
type of access that each page may take part in (we'll see the 
reason for this shordy). 
At the start of a memory access the CPU generates a logical 
address and sends it to the MMU together with the control 
signals that define the type of the access (i.e. read or write, 
program or data, user or supervisor mode). If the location 
being accessed is not currently in the main store or is an ille-
gal access, the MMU sends an error message to the CPU to 
abort the current access and to begin exception processing 
and error recovery. An illegal access occurs when a process 
attempts to write to a page that has been designated read-
only, or when a user program is attempting to access a page 
assigned to supervisor space and the operating system. 
ere's an address 
MMU 
Address 
MEMORY 
(main store) 
Physical address 
Access memory 
The MMU translates 
a logical address into 
a physical address, if a 
translation is not possible, 
the MMU informs the CPU 
that the current access 
can't be completed. 
Figure 13.20 Dialogue between 
the CPU, MMU, and memory. 
f Start 
. 
± 
CPU 
Logical address 
Error! 
_ _Address/Xheck the\_; 
illegal 
\ ^ d J r e s i / o 

566 
Chapter 13 The operating system 
By dividing memory space into regions of different charac-
teristics, you can provide a considerable measure of security. A 
user program cannot access memory space belonging to the 
operating system, because an attempt to access this memory 
space would result in the MMU generating an interrupt. Not 
only does the processor protect the supervisor stack pointer 
from illegal access by a user program, the 68K and MMU com-
bination protects the supervisor stack (and any other address 
space allocated to the supervisor) from illegal access. 
Figure 13.21 illustrates the structure of a memory man-
agement system in a 68K-based computer that checks 
whether the address space currently being accessed is legal. 
Each entry in the MMU's page translation table contains the 
details about the page's access rights. Whenever the 68K 
performs a memory access, it indicates the type of access on 
its function 
code output pins (e.g. user/supervisor, 
code/data). For example, the 68020 may say 'I'm operating 
in the user state performing a read access to data with a log-
ical address 12345678,6'. The MMU compares the CPU's 
function code and the read/write signal with the informa-
tion in the currently accessed page in its mapping table. If 
the access is legal, a memory access takes place. If either the 
corresponding physical page is not in memory or the access 
is illegal, a page fault is generated and a signal returned to 
the 68K's bus error input. In terms of the previous example, 
the logical address 1234567816 might generate a page 
address 12345,6. If this page is in the MMU and it can be 
accessed by a user-mode write, a logical-to-physical page 
translation can take place. 
A bus error is a special type of exception and the 68K calls 
the appropriate handler in the operating system to deal with 
it. A missing physical page results in the operating system 
copying a page from disk to main store and then updating the 
MMU. An illegal access would probably result in the offend-
ing process being suspended. 
The 68K's user/supervisor modes, exception-handling 
facilities, and memory management make it a very robust 
processor. Errors in a user program that would otherwise 
bring the system to a halt force a switch to the 68K's super-
visor state and allow the operating system to either repair the 
damage or to terminate the faulty program. The memory 
management mechanism protects the operating system from 
illegal access by applications programs and even protects one 
user program from access by another. 
Memory management in real systems 
In reality, memory management is a very complex mechan-
ism, even though the underlying concepts are very simple. 
The picture we have just presented is very simplified because 
we've omitted the detail. 
A real memory management system does not normally 
have a single page table; it would be a too big. If we have a 
32-bit virtual address and an 8 kbyte page, the number of bits 
used to specify a logical page is 32 — 13 = 19. This arrange-
ment would require a page table with 219 entries. 
Figure 13.22 demonstrates how a real system solves the 
page table problem by means of a hierarchical table search. 
The 10 most-significant bits of the virtual address access a 
first-level table. The output of the first-level table is a pointer 
to a second-level table that is indexed into by 9 bits from the 
virtual address. This table provides that actual physical page 
number. A multilevel table scheme allows us to use the first-
level table to point to, for example, different processes, and 
the second-level table to point to the pages that make up each 
Figure 13.21 Memory space matching hardware. 
I 
1 . 
Data bus 
„. i 
1 
68K CPU 
? 
^ 
i „„-,,i 
Address bus 
. 
l°%iCal 
rs> 
Main store 
address 
1 l 
v 
Bus error 
Function code 
\ y 
Control signals 
K 
__ 
I 
j 
Memory 
' 
^ £ ^ _ ~ ~ 
^ ^ 
\ 
management 
/ 
\ 
\ 
unii(MMU) 
| 
| 
\ 
I Address from 
„ Space type 
CPU JCC«S type: 
I 
/ 
CPU selects 
» 
* * 
> r 
Read/write 
S 
memory space 
^ . - - ^ 
Program/data 
/ 
£ 
User/supervisor 
/Control signals 
A 
I 
1 
I from CPU select 
• ( = ) 
\ 
* v 
\space type 
J^ 
\ 
Error signal 
\ 
Error 
| 
\ 
(page fault) 
\ 
signal 
I 
1 
\ 
\ 
MMU mapping 
I 
v—-—_____ 
table 
^ 

13.4 Memory management 
567 
32-bit virtual address 
Page number 
Page offset 
10 bits 
9 bits 
13 bits 
Main memory 
Select 
2nd level 
table 
19 bits 
2nd level 
page table 
1st level 
page table 
Figure 13.22 Multiple levels of page tables. 
19 bits 
13 bits 
Physical address 
MALWARE 
Memory management is an important line of defense against 
errors that occur when an application accesses memory space 
not allocated to it. Some programs deliberately perform 
operations that are unintended by the computer user; these 
are collectively called malware and include viruses, worms, 
Trojan horses, and spyware. 
People who would never dream of going into the street and 
assaulting a passer-by will quite cheerfully release programs 
via the Internet that create havoc on people's computers; they 
will destroy a child's homework, modify patient records in a 
hospital, or delete a photographer's images. They use the 
excuse that they are testing people's computer security (even 
Bonnie and Clyde never claimed that they were testing bank 
security) or they say that they are leading an attack on 
Microsoft's evil empire. 
A virus is a program that has strong analogies with 
biological viruses because it replicates itself, spreads 
autonomously, mutates, and can damage its host. A virus is 
introduced into a host via the Internet or via an infected 
program on a floppy disk, flash memory, or CD/DVD. 
A virus must be an executable program in order for it to 
run and to replicate itself. In the PC world, a virus may have 
the extension .exe or .pif. However, one of the strengths of 
modern computer applications is the use of scripting 
languages and macros that allow a user program to respond 
to its environment. These facilities are employed by virus 
writers to embed viruses in data used by applications 
programs such as e-mails. 
Viruses can be injected by ingenious techniques such as 
buffer overflow. A buffer is a region of memory used to store 
data. Buffer overflow occurs when the data takes more space 
than that allocated by the buffer. By exploiting buffer overflow 
you can fill a region of memory with code (rather than data) 
and then transfer control to that code to activate the virus. 
Some processors now contain hardware mechanisms to 
prevent the execution of such code. 
Commercially available antivirus programs are 
widely available to scan memories for the signature of a 
virus (a signature is the binary sequence left behind when 
the code of a virus is compressed rather like a cyclic 
redundancy code). Some viruses are polymorphic and 
mutate as they spread, making it difficult to detect their 
signature. 
A Trojan horse is a program that appears harmless but 
which carries out a task unknown to the user. A worm is a 
program that exploits the Internet and spreads from computer 
to computer generating so much traffic that the Internet can 
be dramatically slowed. 
Spyware is a class of program that may spread like a virus or 
may be introduced as part of another program. Spyware 
monitors your surfing habits (or even accesses to personal 
data) and sends this information to a third party. 
» Target operand 
Page 
table — 
base 

568 
Chapter 13 The operating system 
process. Performing a logical-to-physical address translation 
to locate a physical page address is called a table walk. 
The arrangement of Fig. 13.22 requires 2ln level-one pages 
and 29 level-two pages; that is, 3 X 2 ' pages. A single-level 
page table would require 219 pages. 
The price paid for a memory management system (espe-
cially one with multilevel tables) is the time it takes to per-
form an address translation. Practical memory mapping is 
possible only because very few table accesses take place. Once 
a sequence of logical-to-physical address mappings have 
been performed the address translation is cached in a transla-
tion look aside buffer (TLB). The next time the same logical 
page address appears, the corresponding page address is read 
from the TLB to avoid a table walk. Because of the way data 
and programs are structured, address translations mainly 
take place using the TLB. 
•d SUMMARY 
The operating system is a unique topic in computer science 
because nowhere else does hardware and software so closely 
meet. Although most computers today see the operating system 
as the GUI and the file manager, there is another part of the 
operating system that lies hidden from the user. This is the 
kernel that performs process switching in a multitasking system 
and allocates logical address space to the available memory. 
In this chapter we have shown how the multitasking can be 
implemented by saving one process's volatile portion and then 
restoring another task by loading its volatile portion in the 
processor's registers. 
One of the most important functions carried out by 
operating systems is the management of the memory. We have 
shown how logical addresses in the program can be mapped 
onto locations in the immediate access memory. We have also 
looked at the 68K's user/supervisor mode facility and described 
how it can be used to create secure operating systems. 
m PROBLEMS 
13.1 What is an operating system? 
13.2 What is the difference between a modern operating 
system and a typical operating system from the 1970s? 
13.3 What is the difference between operating systems on 
large and small computers? 
13.4 WIMP-based operating systems have largely replaced JCL-
based operating systems on PCs. DoJCL-based operating 
systems such as Microsoft's DOS 6 and UNIX have any 
advantages over WIMP-based systems? 
13.5 Is it necessary for a CPU to support interrupts in order to 
construct an operating system? 
13.6 A process in a multitasking system can be in one of three 
states: running, runnable, or blocked. What does this statement 
mean and what are the differences between the three states? 
13.7 What is a process control block and what is the minimum 
amount of information that it must store? 
13.8 What are the 68K's user and supervisor states and why 
have they been implemented? 
13.9 Explain why the stack is such an important data structure 
and how stack errors can cause the system to crash. 
13.10 The 68K provides a greater degree of protection from 
user (applications) errors by implementing two stack pointers. 
Explain how this protection mechanism works. 
13.11 If two stack pointers are a good thing (i.e. the 68K's user 
and supervisor stack pointers), can you see advantages in having 
two PCs or two sets of data registers, and so on? 
13.12 What is the difference between a physical address and a 
logical address? 
13.13 When a new physical page is swapped into memory, one 
of the existing pages has to be rejected. How is the decision to 
reject an existing page made? 
13.14 What is the difference between virtual memory and 
cache memory? 
13.15 Write a program in 68K assembly language that 
periodically switches between two processes (assume these are 
fixed processes permanently stored in memory). 
13.16 What is the difference between pre-emptive and 
non-pre-emptive operating systems? Are the various Windows 
operating systems pre-emptive? 
13.17 What is malware? How has it developed over the last 
few years? 
13.18 What hardware facilities in a computer can be used to 
defeat the spread of malware? 

Computer communications 
INTRODUCTION 
Two of the greatest technologies of our age are telecommunications and computer engineering. 
Telecommunications is concerned with moving information from one point to another. We take 
the telecommunications industry for granted. If you were to ask someone what the greatest 
technological feat of 1969 was, they might reply, 'The first manned landing on the moon.'You 
could say that a more magnificent achievement was the ability of millions of people half a million 
kilometers away to watch events on the moon in their own homes. 
It's not surprising that telecommunications and computer engineering merged to allow 
computers to communicate and share resources. Until the 1990s developments in 
telecommunications didn't greatly affect the average person in the same way that computer 
technology had revolutionized every facet of life. Better communications meant lower telephone 
bills and the cell phone. 
Computer networks began as part of a trend towards distributed computing with 
multicomputer systems and distributed databases. From the 1970s onward computer networks 
were implemented to allow organizations such as the military, the business world, and the 
academic communities to share data. Easy access to the Internet and the invention of the browser 
created a revolution almost as big as the microprocessor revolution of the 1970s. The success of 
the Internet drove developments in communications equipment. 
This chapter examines the way in which computers communicate with each other, concentrating 
more on the hardware-related aspects of computer communication than the software. 
We begin with a short history of communications, concentrating on the development of long-
distance signaling systems. We then introduce the idea of protocols and standards, which play a 
vital role in any communications system. Simply moving data from one point to another isn't the 
whole story. Protocols are the mutually agreed rules or procedures enabling computers to 
exchange data in an orderly fashion. By implementing a suitable protocol we ensure that the data 
gets to its correct destination and deal with the problems of lost or corrupted data. 
The next step is to examine how digital data in serial form is physically moved from one point 
to another. We look at two types of data path, the telephone network and the RS232C interface 
that links together computers and peripherals. Two protocols for the transmission of serial data are 
CHAPTER HAP 
13 Operating systems 
'The Operating system controls 
'all the processor's hardware. An 
operating system is responsible 
.for actions ranging from 
'providing a user interface to 
"managing the memory 
'.'subsystem, controlling I/O via' 
the interrupt mechanism, and 
.'"supporting muttitasking.We took 
- at the operating system because 
it is the point at which hardware 
'..«••& software meet. 
14 Computer 
communication 
Computers communicate with 
each other to share resources 
such as printers and data. In this 
chapter we look at '.he way in 
which messages are sent from 
point topoinl.and the protocol* 
or rules that govern the exchange 
of data. 

570 
Chapter 14 Computer communications 
briefly examined—a character-oriented protocol that treats data as blocks of ASCII-encoded 
characters and a bit-oriented protocol that treats data as a continuous stream of bits. 
The next part of this chapter is devoted to local area networks and describes the features of 
some of the LANs in current use. An important aspect of LANs is the way in which the computers 
and peripherals are able to share the same network without apparent conflict. The final topic in 
this chapter is the wide area network (WAN) that connects computers together over distances 
longer than about a mile—WANs are used to implement the Internet. 
GROWTH IN HOUSEHOLDS WITH MULTIPLE PCs 
When the first edition of this text appeared, 
computer communications was very much a 
corporate affair. Only the rich communicated 
with each other. By the time the third edition 
appeared, the PC had become popular and 
the Internet and World Wide Web were used 
by people at home. Connections to the 
Internet were mainly via the public switched 
telephone network, although a lucky few had 
broadband connections via cable or the 
telephone using ASDL. 
Today, high-speed connections to the 
Internet are commonplace and many homes 
have several PCs. Each member of the 
household may have their own PC and some 
may have laptop PCs with wireless networks. 
This means that many home users now have 
their own private local area networks; by 
1999 25% of US households had more than 
one PC and this figure was expected to reach 
50% by 2005. 
The growth in PC ownership is driven by 
several factors. More and more people are 
moving from being computer literate to 
being computer experts capable of 
maintaining complex systems. 
The cost of computing has declined in real 
terms and the performance of hardware 
has continued to increase. The 
market has been driven by computer 
games and domestic entertainment 
such as home theatre and the rise 
of the DVD, the camcorder, and the 
digital camera. 
14.1 Background 
Tt's expensive to construct data links between computers sep-
arated by distances ranging from the other side of town to the 
other side of the World. There is, however, one network that 
has spanned the globe for over 50 years, the public switched 
telephone network (PSTN). Some even refer to the PSTN by 
the acronym POTS (plain old telephone system). The tele-
phone network doesn't provide an ideal solution to the link-
ing of computers, because it was not originally designed to 
handle high-speed digital data. 
During the 1980s a considerable change in the way com-
puters were used took place. The flood of low-cost microcom-
puters generated a corresponding increase in the number of 
peripherals capable of being controlled by a computer. It is 
now commonplace to connect together many different com-
puters and peripherals on one site (e.g. a factory), enabling 
data to be shared, control centralized, and efficiency 
improved. Such a network is called a local area network (LAN). 
When the PC became popular, low-cost hardware and 
software were used to link PCs to the global network, the 
Internet. By the late 1990s networks were no longer 
the province of the factory or university—any school child 
with a PC at home could access NASA's database to see pic-
tures of the latest space shots before they got on the evening 
news. Moreover, the child didn't need to know anything 
about computer science other than how to operate a mouse. 
Figure 14.1 illustrates the concept of a computer network 
with two interconnected local area networks. A network 
performs the same function as a telephone exchange and 
routes data from one computer to another. The LANs in 
Fig. 14.1 might be used to share data in, for example, a uni-
versity environment. The local area networks are themselves 
connected to the telephone system via hardware called a 
modem. Figure 14.1 also demonstrates that a single computer 
can be connected to the other networks via the PSTN. 
A LAN let's you communicate with a mainframe on a 
distant site or with one of the many microprocessors and 
peripherals on your own site. The local area network has 
made possible the paperless office in which people pass 
memos to each other via the network. 
Figure 14.2 describes the type of network that you might 
now see in a school, a small office, or a home. A wireless 
gateway is connected to a PC via a cable. The gateway has a 
connection to a cable modem that provides a high-speed link 
to the Internet via a cable network. The gateway uses wireless 

14.1 Background 
571 
LAN-
Figure 14.1 The network. 
Power 
supply unit 
K s# 
H 
Figure 14.2 The modern small-office network. 
technology to connect one or more laptop computers to the 
base machine and the Internet. 
14.1.1 Local area networks 
Local area networks have changed the face of modern com-
puting. The high performance and low cost of today's LAN 
makes it feasible for even the smallest organizations to link 
together all their computers and allied digital equipment. 
Some of the key features of a LAN are as follows. 
A LAN is local The term local implies a single site—even if 
the site is very large. The site may be a laboratory, a factory, 
or an entire complex of factories. The term MAN (metropoli-
tan area network) has been coined to indicate a network 
extending over a relatively large 
area such as a number of separate 
sites or even part of a city. 
A LAN is private A LAN belongs to 
the owner of the site on which it is 
operated and does not use public 
data 
transmission 
equipment 
such as the telephone network. 
Therefore, the owner of the LAN 
doesn't have to comply with the 
very complex legal restrictions and 
obligations associated with a pub-
lic network. A LAN on one site can 
be connected to a LAN on another 
site by means of the PSTN. The 
interface between the LAN and the 
PSTN is called a gateway. A gate-
way is an interconnection between 
two or more separate networks. 
A LAN offers a high data rate The 
rate at which information can be transmit-
ted across a physical channel depends on 
the length and the electrical properties of 
the transmission path. LANs have relatively 
short transmission paths and often use 
coaxial cable or a twisted pair, permitting 
data rates up to lOOMbits/s. This data 
rate is very much greater than the 
9600 to 56 kbits/s supported by telephone 
channels. 
A LAN is reliable Most LANs are relatively 
simple systems with a coaxial cable connect-
ing the various nodes of the network. There 
are no complex switching systems like those 
associated with telephone networks. LANs 
are reliable because they link systems over 
short distances and aren't subject to the 
types of interference that plague the long-
haul 
transmission 
paths of the telephone 
network. 
Furthermore, the LAN does not employ the fault- and noise-
prone mechanical or electronic message-switching techniques 
associated with the telephone system. A well-designed LAN 
should offer a very long MTBF (mean time between failure) 
and a short MTTR (mean time to repair) if it does fail. A repair 
may involve little more than replacing one of the nodes that 
has failed. LANs are designed so that the failure of a single 
node has no effect on the performance of the system. 
A LAN is cheap LANs have been devised to connect low-cost 
systems and therefore the use of expensive technology or 
transmission media can't be tolerated. LANs are not only 
cheap, but require little labor in their installation. One of the 
most clearly defined trends to emerge from the microprocessor 
Wireless gateway 
Modem 
Interne 
) 
Laptop 
( 
PSTN 
j 
LAN '•"• 
[Modern)*^ 
(psmS 
i 
* - — — i 
|Modem| 
L1H 
^ Modem] 
—H PC h 
••'if£K 
/El 
[ PC I nn 
m 
"PC| 
\K\ 

572 
Chapter 14 Computer communications 
COMMUNICATIONS HARDWARE 
A few years ago, most computer users employed only one 
piece of communications equipment; the modem, which links 
computers to the telephone network. The modem itself was 
invariably an external device that connected to a PC via its 
serial RSC32C interface. 
Today, modems are often internal devices that plug into a 
PC's motherboard. Indeed, many modem laptops come with 
an internal modem as standard. 
Today's PCs are designed to connect to local area networks. 
The computer uses a network interface card (NIC), to 
connect to a bus called an Ethernet. Each NIC has its own 
unique fixed internal address created at the time of 
the card's manufacture. This is a physical address that 
identifies the computer within the network, but it is 
not the address by which the computer is known 
externally. 
Some modern network interface cards use wireless 
communications, Wi-Fi, to allow computers, laptops, and even 
printers to operate over a range of between 10 m and 100 m. 
Large networks require devices to amplify signals on them. 
The repeaterIs a device that simply links two segments of a 
large network together. The router simply passes information 
unchanged from one segment to another. 
Some organizations might have multiple networks. A bridge 
is a device that links two different networks. If a computer on 
a network sends data to another device on the same network, 
the bridge takes no part in the communication. If, however, 
the message is intended for a device on another network, the 
bridge passes the message between the networks. The address 
examined by a bridge is the unique media access address 
given to each physical node in a network. The bridge operates 
at the data link layer level of a network. 
The router'is an even more sophisticated network device 
because it can link different types of network that may be 
separated by a communications path. The bridge simply 
detects information whose destination is on another network 
and passes it on, whereas a router has to be able to 
communicate with different types of network with different 
protocols. A router operates at the network level and can 
connect networks with different data link level protocols. 
Routers are able to reformat packets of information before 
transmitting then to another network. 
world is the tendency for the price of anything associated with 
microprocessors to fall dramatically as time passes. If low-cost 
microprocessor systems are to be linked, the local area network 
chosen to do this must be cost effective. 
A LAN is fair to the users A LAN should offer all its nodes full 
connectivity, which means that any given node should be able 
to communicate with any other node. Equally, each node 
should have the same access rights to the transmission 
medium, so that all nodes have the same probability that their 
message will be delivered across the network.1 
The nodes of a LAN should be equal When we say that all 
nodes should be equal we mean that they should have the 
same software and the same hardware. A corollary of this 
statement is that it should be possible to add a new node to an 
existing system without modifying the software at all the 
other nodes. 
14.1.2 LAN network topology 
The topology of a network describes the way in which the 
individual users of the network are linked together. There are 
four basic topologies suitable for use in a LAN: the uncon-
strained topology, the star network, the bus, and the ring. 
These topologies are the same topologies used to implement 
the multiprocessor systems we introduced in Chapter 8. 
The unconstrained network 
The most general topology is the unconstrained network of 
Fig. 14.3 where individual nodes are connected together in an 
Figure 14.3 The unconstrained topology. 
arbitrary fashion. Additional links can be provided to reduce 
botdenecks where heavy traffic passes between groups of 
nodes. Further nodes and links can readily be added without 
disturbing the existing hardware. The road network of most 
countries is an unconstrained topology, with new roads being 
added when and where necessary. 
The disadvantage of the unconstrained topology is that a 
decision must be made at each node on the best way to route 
a message to its destination. In terms of the analogy with the 
road system, the driver must have a road map to enable them 
to drive from one place to another. A message cannot just be 
transmitted from one node to each other node to which it is 
connected, as this would lead to the message being multiplied 
at each node and propagated round the network forever. 
1 The fairness criterion exists only at levels 1 and 2 of the ISO model 
for OSI. A higher level may limit the scope of a particular node's access 
rights. We discuss the ISO model for OSI later. 

14.1 Background 
573 
Instead, each node must have its own road map and make a 
decision on which link the message is to be transmitted on 
the way to its destination. 
Calculating the best route through the network for each 
message has the computational overhead of working out 
routing algorithms. Furthermore, whenever a new link or 
node is added to the network, the routing information must 
be changed at each node. Figure 14.4 shows how a message 
may be routed through an unconstrained topology. We will 
return to the topic of routing. 
The star network 
Figure 14.5 shows how the star network routes all messages 
from source to destination via one central node and elimin-
ates the need for nodes to make routing decisions. The star 
has a simple topology and has advantages when the network's 
physical topology matches its logical topology. Clearly, there 
are circumstances where the nodes are distributed in such a 
way that the links between some of the nodes and the central 
node are economically unviable. 
The star network has two obvious disadvantages. As all 
messages pass through the central node, the loss of the central 
node brings down the network. Other networks may offer 
degraded but useful service if part of the network fails. 
Furthermore, because all traffic passes through the central 
node, it must be capable of working at a sufficiently high 
speed to handle all nodes to which it is connected. 
The bus 
The bus topology is illustrated in Fig. 14.6. Both the bus and 
the ring are attempts to minimize the complexity of a net-
work by both removing a special-purpose central node and 
the need for individual nodes to make routing decisions. 
In a bus all nodes are connected to a common data highway. 
The bus may be a single path linking all nodes. A more general 
form of bus consists of several interlinked buses and is called an 
unrooted tree. When a message is put on the bus by a node, it 
flows outwards in all directions and eventually reaches every 
point in the network. The bus has one topological and one prac-
tical restriction. Only one path may exist between any two 
points, otherwise there would be nothing to stop a message 
flowing round a loop forever. The practical limitation is that the 
bus cannot exceed some maximum distance from end to end. 
The principal problem faced by the designers of a bus is how 
to deal with a number of nodes wanting to use the bus at the 
same time. This is called bus contention and is dealt with later. 
The ring 
Figure 14.7 illustrates the ring topology, in which the nodes 
are connected together in the form of a ring. Like the bus, this 
topology provides a decentralized structure, because no 
central node is needed to control the ring. Each node simply 
receives a message from one neighbor and passes it on to 
its other neighbor. Messages flow in one direction round 
the ring. 
The only routing requirement placed on each node is that 
it must be able to recognize a message intended for itself. The 
ring does not suffer from contention like the bus topology. 
Figure 14.4 Routing a message through an unconstrained topology. 
Figure 14.5 The star topology. 
? ? , ?? ,? ? 
Figure 14,6 The bus topology. 
Figure 14./ The ring topology. 
Destination 

574 
Chapter 14 Computer communications 
However, a node on the ring has the problem of how to inject 
a new message into the existing traffic flow. 
A ring is prone to failure because a broken link makes it 
impossible to pass messages all the way round the ring. Some 
networks employ a double ring structure with two links 
between each node. If one of the links is broken it is possible 
for the ring to reconfigure itself and bypass the failure. 
14.1.3 History of computer 
communications 
Before we describe computer networks, it's instructive to 
take a short look at the history of data transmission. Some 
think that electronics began in the 1960s or even later. 
Telecommunications predates the electronic digital com-
puter by over a century and its history is just as exciting as the 
space race of the 1960s. Key players were engineers every bit 
as great as Newton or Einstein. 
At the beginning of the nineteenth century, King 
Maximilian in Bavaria had seen how the French visual sema-
phore system helped Napoleon's military campaigns. In 1809 
Maximilian asked the Bavarian Academy of Sciences to look 
for a way to communicate over long distances. As a result, 
Samuil T. von Sommering designed a crude telegraph that 
used a conductor (one for each character) that required 35 
parallel wires. How was information transmitted in a 
pre-electronic age? If you pass electricity through water con-
taining a little acid the electric current breaks down the water 
into oxygen and hydrogen. Sommering's telegraph worked by 
detecting the bubbles that appeared in a glass tube containing 
acidified water when electricity was passed through it. 
Sommering's telegraph wasn't exactly suited to high-speed 
transmission—but it was a start. 
Hans C. Oersted made the greatest leap forward in elec-
trical engineering in 1819 when he discovered that an electric 
current creates a magnetic field round a conductor. 
Conversely, a moving magnetic field induces an electric 
current in a conductor. 
A major driving force behind early telecommunications 
systems was the growth of the rail network. A system was 
required to warn stations down the line that a train was arriv-
ing. Charles Wheatstone and Charles William Cooke 
invented a telegraph in 1828 that used the magnetic field 
round a wire to deflect a compass needle. By 1840 a 40-mile 
stretch between Slough and Paddington in London had been 
linked using the Wheatstone and Cooke telegraph. 
Figure 14.8 illustrates the operation of a different type of 
telegraph that produces a sound rather than the deflection of 
compass needles. When the key is depressed, a current flows 
in the circuit magnetizes the iron core inside the coil, and 
energizes the solenoid. The magnetized core attracts a small 
iron plate that produces an audible click as it strikes the core. 
Information is transmitted to this type of telegraph in the 
form of the Morse code. 
Samuel Morse constructed his code from four symbols: the 
dot, the dash (whose duration is equal to three dots), the 
space between dots and dashes, and the space between words. 
Unlike simple codes, the Morse code is a variable length code. 
The original Morse key didn't send a 'bleep'—a dot was the 
interval between two closely spaced clicks and a dash the 
interval between two more widely spaced clicks. In other 
words, the operator had to listen to the space between clicks. 
In 1843 Morse sent his assistant Alfred Vail to the printer's 
to count the relative frequencies of the letters they were using 
to set up their press. Morse gave frequently occurring letters 
short codes and infrequently occurring letters were given long 
symbols; for example, the code for E is • and Q is 
• —. 
It's interesting to note that the Morse code is relatively close to 
the optimum Huffman code for the English language. 
The very first long-distance telecommunications networks 
were designed to transmit digital information from point 
to point (i.e. on-off telegraph signals). Information was 
transmitted in binary form using two signal levels 
(current = mark, no current = space). The transmitter was 
the Morse key and the receiver was the Morse telegraph. 
The first long-distance data links 
We take wires and cables for granted. In the early nineteenth 
century, plastics hadn't been invented and the only materials 
available for insulation and waterproofing were things like 
asphaltum. In 1843 a form of rubber called gutta percha was 
discovered and was used to insulate the signal-carrying path 
in cables. The Atlantic Telegraph Company created an insu-
lated cable for underwater use containing a single copper 
Battery 
Morse key 
Telegraph line 
« 
Iron, 
;/////////////;////////;/////;/////////;////////////;//%;/// 
Iron core 
Figure 14.8 The telegraph. 
S Spring 
-p Coi! 

14.1 Background 
575 
conductor made of seven twisted strands, surrounded by 
gutta percha insulation. This cable was protected by 18 
surrounding iron wires coated with hemp and tar. 
Submarine cable telegraphy began with a cable crossing 
the English Channel to France in 1850. Alas the cable failed 
after only a few messages had been exchanged. A more 
successful attempt was made the following year. 
Transatlantic cable laying from Ireland began in 1857 but 
was abandoned when the strain of the cable descending to 
the ocean bottom caused it to snap under its own weight. The 
Atlantic Telegraph Company tried again in 1858. Again, the 
cable broke after only 3 miles but the two cable-laying ships 
managed to splice the two ends. After several more breaks 
and storm damage, the cable reached Newfoundland in 
August 1857. 
It soon became clear that this cable wasn't going to be a 
commercial success because the signal was too weak to detect 
reliably (the receiver used the magnetic field from current in 
the cable to deflect a magnetized needle). The original voltage 
used to drive a current down the cable was approximately 
600 V. So, they raised the voltage to about 2000 V to drive 
more current along the cable. Such a high voltage burned 
through the primitive insulation, shorted the cable, and 
destroyed the first transatlantic telegraph link after about 
700 messages had been transmitted in 3 months. 
In England, the Telegraph Construction and Maintenance 
Company developed a new 2300-mile-long cable weighing 
9000 tons, which was three times the diameter of the failed 
1858 cable. Laying this cable required the largest ship in the 
world. After a failed attempt in 1865 a transatlantic link was 
established in 1866. 
During the nineteenth century the length of cables 
increased as technology advanced. It soon became apparent 
that signals suffer distortion during transmission. The 1866 
transatlantic telegraph cable could transmit only eight words 
per minute. By the way, it cost $100 in gold to transmit 
20 words (including the address) across the first transatlantic 
cable. 
A sharply rising pulse at the transmitter end of a cable is 
received at the far end as a highly distorted pulse with long 
rise and fall times. The sponsors of the transatlantic cable 
project were worried by the effect of this distortion and the 
problem was eventually handed to William Thomson at the 
University of Glasgow. 
Thomson was one of the nineteenth century's greatest sci-
entists who published more than 600 papers. He developed 
the second law of thermodynamics and created the absolute 
temperature scale. The unit of temperature with absolute 
zero at 0 K is called the kelvin in his honor—Thomson later 
became Lord Kelvin. Thomson worked on the dynamical 
theory of heat and carried out fundamental work in hydro-
dynamics. His mathematical analysis of electricity and 
magnetism covered the basic ideas for the electromagnetic 
theory of light. I'm not certain what he did in his spare time. 
One of Thomson's most quoted statements that still applies 
today was: 
/ often say when you can measure what you are speaking about and 
express it in numbers, you know something about it, but when you can-
not measure it, when you cannot express it in numbers, your knowledge 
of it is of a meager and unsatisfactory kind. 
In 1855 Thomson presented a paper to the Royal Society 
analyzing the effect of pulse distortion, which became the 
cornerstone of what is now called transmission line theory. 
The cause of the problems investigated by Thomson lies in 
the physical properties of electrical conductors and insula-
tors. At its simplest, the effect of a transmission line is to 
reduce the speed at which signals can change state. 
Thomson's theories enabled engineers to construct data links 
with much lower levels of distortion. 
Origins of the telephone network 
In 1872 Alexander Graham Bell who had recently emigrated 
to the USA started work on a method of transmitting several 
signals simultaneously over a single line. Bell's project was 
called the harmonic telegraph. This project failed, but it did 
lead to the development of the telephone in 1876. Note that 
development of the telephone is a complex story and Bell is 
no longer recognized as the sole inventor of the telephone. 
A network designed to transmit intelligible speech 
(as opposed to hi-fi) must transmit analog signals in the 
frequency range 300 to about 3300 Hz (i.e. the so-called 
voice-band). Consequently, the telephone network now link-
ing millions of subscribers across the World can't be used 
to directly transmit digital data that requires a bandwidth 
extending to zero frequency (i.e. d.c). If the computer had 
been invented before the telephone, we wouldn't have had 
this problem. Transmission paths that transmit or pass sig-
nals with frequency components from d.c. to some upper 
limit are called baseband channels. Transmission paths that 
transmit frequencies between a lower and an upper 
frequency are called bandpass channels. 
Digital information from computers or peripherals must 
be converted into analog form before it is transmitted across 
a bandpass channel such as the PSTN. At the receiving end of 
the network, this analog signal is reconverted into digital 
form. The device that converts between digital and analog 
signals over a data link is called a modem 
(i.e. 
modulator-demodulator). Ironically enough, all the long-
haul links on modern telephone networks now transmit dig-
ital data, which means that the analog signal derived from the 
digital data must be converted to digital form before trans-
mission over these links. It is probable that the PSTN will 
become entirely digital and speech will be converted to digi-
tal form within the subscriber's own telephone. Indeed, the 
only analog link in many telephone systems is just the 

576 
Chapter 14 Computer communications 
BANDWIDTH AND COMMUNICATION 
If a signal conveys information, it must change over time. The 
minimum amount of information carried by a signal is the bit, 
which has two values, true and false or 1 and 0. When we 
change the state or level of the signal, we transmit a new 
value. Transmission speed is defined by both the number of 
changes in signal level we transmit per second and the number 
of different signal levels used to represent data. We can use 
signals with more that two valid levels. Consider the following 
example. 
1 0 1 1 1 0 1 0 0 0 
binary signal (two levels 0 and 1) 
A C B B A D A D D C 
four-level signal (four levels A = 00, B 
In the second case, each of the symbols A, B, C, D carries 
two bits of information because the symbol is one out of four, 
rather than one out of two. If symbols are transmitted at the 
same rate in each case, the four-level signal transmits 
information at twice the rate of a single level system. 
Data rate is defined as the number of times a signal can be 
switched per second multiplied by the number of bits needed to 
encode all the levels that the signal can assume. Suppose a signal 
can have 16 different values or levels and is transmitted as 2400 
symbols (values) per second, the data rate is 2400 X 4 = 9600 
bits per second. Note that the switching rate (2400 in this case) 
is called the Baud rate after the 
= 01, C = 10, and D = 11) 
French communications pioneer. 
connection between the subscriber and the local exchange. 
This link is sometimes called the last mile. 
Although the first telegraph systems operated from point 
to point, the introduction of the telephone led to the devel-
opment of switching centers, or telephone exchanges. The 
first-generation of switches employed a telephone operator 
who manually plugged a subscriber's line into a line con-
nected to the next switching center in the link. By the end of 
the nineteenth century, the infrastructure of die computer 
networks was already in place. 
In 1897 an undertaker called Almon Strowger invented the 
automatic telephone exchange that used electromechanical 
devices to route calls between exchanges. When a number 
was dialed, a series of pulses were sent down the line to a 
rotary switch. If you dialed, for example 5, the five pulses 
would move a switch five steps to connect you to line number 
five, which routed your call to the next switching center. 
Consequently, when you called someone the number you 
dialed depended on the route though the system. A system 
was developed where each user could be called with the same 
number from anywhere and the exchange would automati-
cally translate this number to the specific numbers required 
to perform the routing. Mechanical switching was gradually 
replaced by electronic switching and the pulse dialing that 
actually operated the switches gave way to the use of tones 
(i.e. messages to the switching computers). 
By the time the telegraph was well established, radio was 
being developed. lames Clerk Maxwell predicted radio waves 
in 1864 following his study of light and electromagnetic 
waves. Heinrich Hertz demonstrated the existence of radio 
waves in 1887 and Guglielmo Marconi is credited with being 
the first to use radio to span the Atlantic in 1901. 
In 1906 Lee deForest invented the vacuum tube amplifier. 
Without a vacuum tube (or transistor) to amplify weak 
signals, modern electronics would have been impossible 
(although primitive computers using electromechanical 
devices could have been built without electronics). 
The telegraph, telephone, and vacuum tube were all steps 
on the path to the development of computer networks. As 
each of these practical steps was taken, there was a corre-
sponding development in the accompanying theory (in the 
case of radio, the theory came before the discovery). 
Table 14.1 provides a list of some of the most significant dates 
in the early development of long-distance communications 
systems. 
Computer communications is a complex branch for com-
puting because it covers so many areas. A programmer drags 
an icon from one place to another on a screen. This action 
causes the applications program to send a message to the 
operating system that might begin a sequence of transactions 
resulting in data being retrieved from a computer half way 
around the World. Data sent from one place to another has to 
be encapsulated, given an address, and sent on its way. Its 
progress has to be monitored and its receipt acknowledged. It 
has to be formatted in the way appropriate to the transmis-
sion path. All these actions have to take place over many dif-
ferent communications channels (telephone, radio, satellite, 
and fiber optic cable). Moreover, all the hardware and soft-
ware components from different suppliers and constructed 
with different technologies have to communicate with each 
other. 
The only way we can get such complex systems to work is 
to create rules or protocols that define how the various 
components communicate with each other. In the next 
section we look at these rules and the bodies that define them. 
14.2 Protocols and computer 
communications 
Communication between two computers is possible 
provided that they employ standard hardware and software 
conforming to agreed standards. Much of computer 

14.2 Protocols and computer communications 
577 
1837 
Charles Wheatstone patents the electric telegraph. 
1844 
Samuel Morse demonstrates a Baltimore to Washington, DC, telegraph link. 
1847 
An inelastic latex called gutta percha is discovered. It serves as a reliable insulator in water. 
1850 
Morse patents his telegraph. 
1858 
First transatlantic telegraph. 
1861 
First USA transcontinental telegraph cable begins service. 
1864 
James C. Maxwell predicts electromagnetic radiation. 
1868 
First successful transatlantic telegraph cable completed between UK and Canada. 
1874 
Jean-Maurice-Emile Baudot invents a division multiplexing scheme for telegraphs. 
1875 
Typewriter invented. 
1876 
Alexander Graham Bell patents the telephone. 
1887 
Heinrich Hertz discovers radio waves and verifies Maxwell's theory. 
1906 
Lee deForest invents the vacuum tube triode (an amplifier). 
1915 
USA transcontinental telephone service begins between New York and San Francisco. 
1920s 
Catalina Island telephone service to mainland via radio system. 
1921 
Radio telephone calls between England and Norway implemented. 
1927 
First commercial transatlantic radio telephone service begins. 
1945 
Arthur C. Clarke proposes using Earth-orbiting satellite as a communications relay. 
1947 
The transistor invented at Bell laboratories. 
1948 
Claude Shannon publishes his work on information theory (related to channel capacity). 
1949 
High-performance submarine cable developed by AT&T using polyethylene and butyl rubber dielectric. 
1956 
First transatlantic telephone cables.A total of 102 repeater (vacuum tube) amplifiers were used. 
1957 
USSR launches first satellite, Sputnik 1. 
1962 
First television satellite launched, Telstar 1. 
1965 
First commercial communications satellite launched, Early Bird (INTELSAT 1). 
1966 
Fiber optics first proposed. 
1971 
First large-scale computer network, ARPANET, comes into service. 
1970s 
ALOHA local area network developed for the Hawaiian islands. 
1973 
Bob Metcalfe develops the Ethernet. 
1980 
OSI 7-layer reference model (for networks) adopted. 
1980 
Bell Systems develop fiber optic cables. 
1988 
First fiber optic transatlantic cable. 
1993 
IPv4 + (Internet protocol version 4) developed as the backbone of the Internet. 
1990s 
By the end of the 1990s, 56K bps modems were widely available for use over the PTSN. 
1997 
The introduction of WiFi and the IEEE 802.11 standard. 
Table 14.1 Key dates in the early developments in telecommunications. 
communications is concerned with how computers go about 
protocol as it is used in computer communications. When any 
exchanging data, rather than with just the mechanisms used 
two parties communicate with each other (be they people or 
to transmit data. Therefore, the standards used in computer 
machines), they must both agree to abide by a set of unam-
communications relate not only to the hardware parts of a 
biguous rules. For example, they must speak the same lan-
communication system (i.e. the plugs and sockets connecting 
guage and one may start speaking only when the other 
a computer to a transmission path, the transmission path 
indicates a readiness to listen. 
itself, the nature of the signals flowing along the transmission 
Suppose you have a bank overdraft and send a check to 
path), but to the procedures or protocols followed in trans- 
cover it. If after a few days you receive a threatening letter 
mitting the information. 
from the manager, what do you conclude? Was your check 
Most readers will have some idea of what is meant by a 
received after the manager's letter was sent? Has one of your 
standard, but they may not have come across the term 
debits reached your account and increased the overdraft? Was 

578 
Chapter 14 Computer communications 
the check lost in the post? This confusion demonstrates that 
the blind transmission of information can lead to unclear 
and ill-defined situations. It is necessary for both parties to 
know exactly what messages each has, and has not, received. 
We need a set of rules to govern the interchange of letters. 
Such a set of rules is called a protocol and, in the case of 
people, is learned as a child. When computers communicate 
with each other, the protocol must be laid down more for-
mally. If many different computers are to communicate with 
each other, it is necessary that they adhere to standard proto-
cols that have been promulgated by national and interna-
tional standards organizations, trade organizations, and 
other related bodies. 
In the 1970s and 1980s the number of computers and 
the volume of data to be exchanged between computers 
increased dramatically. Manufacturers were slow to agree on 
and to adopt standard protocols for the exchange of data, 
which led to incompatibility between computers. To add 
insult to injury, it was often difficult to transfer data between 
computers that were nominally similar. Computers frequently 
employed different dialects of the same high-level language 
and formatted data in different ways, encoded it in different 
ways, and transmitted it in different ways. Even the builders 
of the Tower of Babel had only to contend with different lan-
guages. The development of standard protocols has much 
improved the situation. 
The issue of standardization arises not only in the world of 
computer communications. Standardization is an important 
part of all aspects of information technology. For example, 
the lack of suitable standards or the non-compliance with 
existing standards has a dampening effect on the progress of 
information technology. Independent manufacturers do not 
wish to enter a chaotic market that demands a large number 
of versions of each product or service produced to cater for all 
the various non-standard implementations. Similarly, users 
do not want to buy non-standard equipment or services that 
do not integrate with their existing systems. 
14.2.1 Standards bodies 
If a computer user in Middlesbrough, England accesses a 
computer in Phoenix, Arizona the two computers must 
cooperate. The commands and data sent by one computer 
must be recognized and complied with by the other com-
puter. The protocols governing the communications process 
are formalized in a document called a standard. All aspects of 
the communications system must be standardized—from the 
communications protocol to the nature of the signals on the 
communications path to the plugs and sockets that connects 
the computer to the network. 
How do the components of a network get standardized? 
There are two types of standards. One is the de facto or indus-
trial standard that's imposed by a manufacturer. Microsoft's 
Windows operating system is an example of an industrial stan-
dard. The success of Windows has encouraged its adoption as a 
standard by most PC manufacturers and software houses. 
The other type of standard is a national or international 
standard that has been promulgated by a recognized body. 
There are international standards for the binary representa-
tion of numbers. When the decimal number nine is transmit-
ted over a network, it is represented by its universally agreed 
international standard, the binary pattern 00111001. 
The world of standards involves lots of different parties 
with vested interests at local, national, and international 
levels. A standard begins life in a working party in a profes-
sional organization such as the Institute of Electrical and 
Electronic Engineers (IEEE) or the Electronic Industries 
Association (EIA). The standard generated by a professional 
body is forwarded to the appropriate national standards body 
(e.g. the American National Standards Institute (ANSI) in the 
USA, the British Standards Institute (BSI) in the UK, or DIN 
in Germany). The standard may reach the International 
Standards Organization (ISO) made up of members from the 
World's national standards organizations. 
14.2.2 Open systems and standards 
The International Standards Organization (ISO) has con-
structed a framework for the identification and design of 
protocols for existing or for future communications systems. 
This framework enables engineers to identify and to relate 
together different areas of standardization. The OSI frame-
work doesn't imply any particular technology or method of 
implementing systems. 
This framework is called the Reference Model for Open 
Systems Interconnection (ISO model for OSI) and refers to an 
open system, which, in the ISO context, is defined as 
a set of one or more computers together with the software, peripherals, 
terminals, human operators, physical processes and means of data 
transfer that go with them, which make up a single information 
processing unit. 
The expression open system means a system that is open to 
communication with other open systems. A system is open 
only if it employs agreed protocols when it communicates 
with the outside world. It does not have to employ standard 
protocols for communications within the system itself. An 
analogy with an open system is a television receiver because it 
is open to the reception of sound and pictures from trans-
mitters using the agreed protocol (e.g. 525 lines/frame, 
60 fields/s, NTSC color in the USA or 625 lines/frame, 
50 fields/s, PAL color in the UK). A pocket calculator is a 
closed system because it is unable to receive inputs from 
other systems. 
The ISO reference mode isolates the specific functions per-
formed by the communications system from all other aspects 
of the system. Once these functions have been isolated, you 

14.2 Protocols and computer communications 
579 
can devise standards for them. In this way, any manufacturer 
can produce equipment or software that performs a particu-
lar function. If designers use hardware and software con-
forming to well-defined standards, they can create an 
information transmission system by putting together all the 
necessary parts. These parts may be obtained from more than 
one source. As long as their functions are clearly defined and 
the way in which they interact with other parts is explicitly 
stated, they can be used as the building blocks of a system. 
Figure 14.9 illustrates the structure of the ISO reference 
model for OSI, where two parties, A and B, are in communi-
cation with each other. The ISO model divides the task of 
communicating between two points between seven layers of 
protocol. Each layer carries out an action or service required 
by the layer above it. The actions performed by any given 
layer of the reference model are precisely defined by the ser-
vice for that layer and require an appropriate protocol for the 
layer between the two points that are communicating. This 
view conforms to current thinking about software and is 
strongly related to the concept of modularity. 
In everyday terms, consider an engineer in one factory who 
wishes to communicate with an engineer in another factory. 
Station A 
Virtual link 
Stati onB 
i.cvei 7 
Virtual link 
ApF-.i:V:i:r.nl.T,i>r 
Virtual link 
Virtual link 
' 
Level n 
Virtual link 
•..•:•.<:•[ 
6 
i:rese".::Jlio:". !rv?r 
Virtual link 
•..•:•.<:•[ 
6 
Virtual link 
' 
! n - ,••' 
^ 
Virtual link 
..c-.o! 5 
Sc-ss on i.iyo-
So?s:."-ii if^vO" 
Virtual link 
..c-.o! 5 
Sc-ss on i.iyo-
i . 
Virtual link 
' ' 
Level 4 
Transport layer 
Virtual link 
;e-.el-1 
T-arsx.- v>vr 
Level 4 
Transport layer 
Virtual link 
;e-.el-1 
T-arsx.- v>vr 
1 
r 
Virtual link 
' 
Level 3 
Network layer 
Virtual link 
,.Pv(-:l T 
Network :=>e' 
Level 3 
Network layer 
Virtual link 
,.Pv(-:l T 
Network :=>e' 
1' 
Virtual link 
' 
Level 2 
Data link layer 
Virtual link 
Level 2 
Data link layer 
Level 2 
Data link layer 
Virtual link 
Level 2 
Data link layer 
. 
Virtual link 
' 
L 
Virtual link 
6-
Level 1 
Physic at layer r 
Physic al layer 
[ 
Figure 14.9 The basic reference model for open systems 
interconnection. 
The engineer in the first factory describes to an assistant the 
nature of some work that is to be done. The assistant then 
dictates a letter to a secretary who, in turn, types the letter and 
hands it to a courier. Here, the original task (i.e. communi-
cating the needs of one engineer to another) is broken down 
into subtasks, each of which is performed by a different 
person. The engineer doesn't have to know about the actions 
carried out by other people involved in the exchange of data. 
Indeed, it does not matter to the engineer how the informa-
tion is conveyed to their counterpart. 
In the ISO model, communication between layers within a 
system takes place between a layer and the layers immediately 
above and below it. Layer X in System A communicates 
only with layers X + 1 andX - 1 in System A (see Fig. 14.9). 
Layer 1 is an exception, because there's no layer below it. 
Layer 1 communicates only with layer 2 in A and with the 
corresponding layer 1 in B at the other end of the communi-
cations link. In terms of the previous analogy, the secretary 
who types the letter communicates only with the assistant 
who dictates it and with the courier who transports it. 
Fig. 14.10 illustrates this example in terms of ISO layers, 
although this rather simple example doesn't correspond 
exactly to the ISO model. In particular, layers 3 to 6 are repre-
sented by the single layer called assistant. 
Another characteristic of the ISO model is the apparent or 
virtual link between corresponding layers at each end of the 
communication channel (this link is also called peer to peer). 
Two corresponding layers at two points in a network are 
called peer subsystems and communicate using layer proto-
cols. Therefore, a message sent by layer X at one end of the 
link is in the form required by the corresponding layer X at 
the other end. It appears that these two layers are in direct 
communication with each other, as they are using identical 
protocols. In fact, layer X at one end of the link is using the 
layers below it to transmit the message across the link. At the 
other end, layer 1 and higher layers process the message until 
it reaches layer X in the form it left layer X at the other end of 
the link. Returning to our analogy, the secretary at one 
factory appears to communicate directly with the secretary at 
the other factory, because the language used in the letter is 
appropriate to the task being performed by the two 
secretaries. 
We now look at the functions performed by die seven 
layers of the ISO reference model for open systems interconnec-
tion, starting with die uppermost layer, the application layer. 
The application layer 
The highest layer of the ISO reference model is the application 
layer, which is concerned with protocols for applications pro-
grams (e.g. file transfer, electronic mail). This layer represents 
the interface with the end user. Strictly speaking, the OSI ref-
erence model is concerned only with communications and 
does not represent the way in which the end user employs the 

580 
Chapter 14 Computer communications 
Level 7 
A 
:•• 
Engineer 
:.-:i-irwr 
. 
k 
I 
Level 3-6 
Assistant 
Level 3-6 
Assistant 
Level 3-6 
Assistant 
Level 3-6 
Assistant 
• ' 
Level 2 
Typist 
Level 2 
Typist 
Level 2 
Typist 
Real connection 
Level 2 
Typist 
' 
r 
Real connection 
Level 1 
Real connection 
Level 1 
Cou rier 
Courier 
Figure 14.10 Illustrating the concept of layered protocols. 
information. The protocol observed by the two users in 
the application layer is determined entirely by the nature of 
the application. Consider the communication between two 
lawyers when they are using the telephone. The protocol used 
by the lawyers is concerned with the semantics of legal jargon. 
Although one lawyer appears to be speaking directly to 
another, they are using another medium involving other pro-
tocols to transport the data. In other words, there is no real 
person-to-person connection but a virtual person-to-person 
connection built upon the telephone network. 
Another example of an application process is the operation 
of an automatic teller at a bank. The operator is in communi-
cation with the bank and is blissfully ignorant of all the tech-
nicalities involved in the transaction. The bank asks the user 
what transaction they wish to make and the user indicates the 
nature of the transaction by pushing the appropriate button. 
The bank may be 10 m or 1000 km away from the user. The 
details involved in the communication process are entirely 
hidden from the user; in the reference model the user is 
operating at the applications level. 
The presentation layer 
The application layer in one system passes information to the 
presentation layer below it and receives information back 
from this layer. Recall that a layer at one end of a network 
can't communicate directly with the corresponding layer at 
the other end. Each layer except one communicates with only 
the layer above it and with the layer below it. At one end of the 
communications system the presentation layer translates 
data between the local format required by the application 
layer above it and the format used for transfer. At the other 
end, the format for transfer is translated into the local format 
of data for the application layer. By format we mean the way 
in which the computer represents information such as char-
acters and numbers. 
Consider another analogy. A Russian diplomat can phone 
a Chinese diplomat at the UN, even though neither speaks 
the other's language. Suppose the Russian diplomat speaks to 
a Russian-to-English interpreter who speaks to an English-
to-Chinese interpreter at the other end of a telephone link, 
who, in turn, speaks to the Chinese diplomat. The diplomats 
represent the applications layer process and talk to each other 
about political problems. They don't speak to each other 
directly and use a presentation layer to format the data before 
it is transmitted between them. The Chinese-to-English and 
English-to-Russian translators represent the presentation 
layer. 
This analogy illustrates an important characteristic of the 
OSI reference model. The English-to-Chinese translator may 
be a human or a machine. Replacing one with the other has 
no effect on the application layer above it or on the informa-
tion transfer layers below it. All that is needed is a mechanism 
that translates English to Chinese, subject to specified perfor-
mance criteria. 
The presentation layer's principal function is the transla-
tion of data from one representation to another. This layer 
performs other important functions such as data encryption 
and text compression. 
The session layer 
Below the presentation layer sits the session layer. The session 
layer organizes the dialogue between two presentation layers. 
It establishes, manages, and synchronizes the channel 
between two application processes. This layer provides dia-
logue control of the type, 'Roger, over', in radio communica-
tions, and the mechanisms used to synchronize application 
communications (but synchronization actions must be initi-
ated at the application layer). The session layer resolves colli-
sions between synchronization requests. An example is 
'... did you follow that?...','... then I'll go over it again.' 
The transport layer 
The four layers below the session layer are responsible for car-
rying the message between the two parties in communica-
tion. The transport layer isolates the session and higher layers 
from the network itself. It may seem surprising that four lay-
ers are needed to perform such an apparently simple task as 
moving data from one point in a network to another point. 
We are talking about establishing and maintaining connec-
tions across interlinked LANs and wide area networks with, 
possibly, major differences in technology and performance— 
not just communications over a simple wire. The reference 
model covers both LANs and WANs that may involve com-
munication paths across continents and include several dif-
ferent communications systems. Figure 14.11 shows how the 
ISO model for OSI caters for communications systems with 
intermediate nodes. 

14.2 Protocols and computer communications 
581 
The transport layer is responsible for the reliable transmis-
sion of messages between two application nodes of a network 
and for ensuring that the messages are received in the order in 
which they were sent. The transport layer isolates higher lay-
ers from the characteristics of the real networks by providing 
the reliable economic transmission required by an applica-
tion independent of the characteristics of the underlying 
facilities (for example, error detection/correction, multiplex-
ing to reduce cost, splitting to improve throughput, and mes-
sage reordering). The transport layer doesn't have to know 
anything about how the network is organized. 
Packet switching networks divide information into units 
called packets and then send them across a complex network 
of circuits. Some packets take one route through the network 
and others take another. Consequently, it is possible for pack-
ets to arrive at their destination out of sequence. The trans-
port layer must assemble packets in the correct order, which 
involves storing the received out-of-sequence packets until 
the system is ready for them. 
The network layer 
The network layer serves the transport layer above it by con-
veying data between the local transport layer and the remote 
transport layer. The network layer is system dependent unlike 
the layers above it. Complex communications systems may 
have many paths between two points. The network layer 
chooses the optimum path for a message to cross the network 
or for the establishment of a virtual connection. As an anal-
ogy, consider the postal system. Mail sent to a nearby sorting 
office might be directed to a more distant sorting office if the 
local office is congested and cannot cope with the volume of 
traffic. Similarly, in a data transmission network, transmis-
sion paths are chosen to minimize the transit time of packets 
and the cost of transmission. 
The data link layer 
The data link layer establishes an error-free (to a given prob-
ability) connection between two adjacent points in a net-
work. Information may be transmitted from one end of a 
network to the other end directly or via intermediate nodes in 
a series of hops. The data link layer at one node receives a 
message from the network layer above it and sends it via 
the physical layer below it to the data link layer at the adja-
cent node. 
The data link layer also detects faulty messages and auto-
matically asks for their retransmission. Protocols for the data 
link layer and the physical layer below it were die first proto-
cols to be developed and are now widely adopted. Data link 
Station A 
Station B 
;_?•. el 7 
Virtual link 
H A •; 1 / 
Application liiJ'L1: 
Virtual link 
Apr,:lrnT .-r\ h > p r 
| 
* 
T 
Virtual link 
" 
: i".'f'l 6 
?r«i"nt;i*'o- layer 
Virtual link 
! evel 6 
: i".'f'l 6 
?r«i"nt;i*'o- layer 
Virtual link 
?rc-jo:'ii.ni;n layer 
Virtual link 
' • 
Level 5 
Session ,ayw 
Virtual link 
Level 5 
Session .JVV: 
Level 5 
Session ,ayw 
Virtual link 
Level 5 
Session .JVV: 
i 
Virtual link 
' • 
Lever. -1 
Transport layer 
Virtual link 
i.;-v-<X -'• 
" ransiwrt b w r 
Lever. -1 
Transport layer 
de 
i.;-v-<X -'• 
" ransiwrt b w r 
i . 
ntermediate node 
intermediate no de 
! 
. 
. 
«[ 
» 
! ' 
Level 3 
Network layer 
* 
|fc Nr-rv.crK layer »: 
{* 
I ey.*! ?• 
Nerv.wt \?.\-c 
«[ 
» 
Level 3 
Network layer 
. , 
1 
1 
• 
«[ 
» 
1 
' 
Level 2 
Data link layer 
« 
|» 
! i-v.'l ?. 
Av.n 1 nklavcr *i 
<» Ditalii* hyer «[ 
» 
Level 2 
Data link layer 
i L 
1 
.._: ' 
«[ 
» 
? • 
Level 1 
Level 1 
Physical layer i 
h 
Level 1 
«[ 
» 
Level 1 
Physic al layer 
Level 1 
Physical layer 
Physic a I layer 
Physica L layer 
Figure 14.11 Networks with 
intermediate nodes between 
end stations. 

582 
Chapter 14 Computer communications 
layer protocols cover many different technologies: LANs 
(for example Ethernet-type networks using CMSA/CD) and 
WANs (for example X.25). Systems often divide this layer 
into two parts, a higher level logical link control (LLC) and a 
lower level medium access control (MAC). 
The physical layer 
The lowest layer, the physical layer, is unique because it pro-
vides the only physical connection between any two points in 
a network. The physical layer is responsible for receiving the 
individual bits of a message from the data link layer and for 
transmitting them over some physical medium to the adja-
cent physical layer, which detects the bits and passes them to 
the data link layer above it. The physical layer ensures that bits 
are received in the order they are transmitted. 
The physical layer handles the physical medium (e.g. wire, 
radio, and optical fiber) and ensures that a stream of bits gets 
from one place to another. The physical layer also imple-
ments the connection strategy. There are three fundamental 
connection strategies. Circuit switching establishes a perma-
nent connection between two parties for the duration of the 
information transfer. Message switching stores a message tem-
porarily at each node and then sends it on its way across the 
network. Circuit switching uses a single route through the 
network, whereas in message switching different messages 
may travel via different routes. Packet switching divides a mes-
sage into units called packets and transmits them across the 
network. Packet switching doesn't maintain a permanent 
connection through the network and is similar to message 
switching. 
Packet switching comes in two forms, the datagram and 
the virtual circuit. A datagram service transmits packets 
MESSAGE ENCAPSULATION 
How do layered protocols deal with messages? In short, a 
higher layer wraps up a message from a lower layer in its own 
data structure. 
The figure demonstrates how information is transported 
across a network by means of a system using layered 
protocols. In (a) we have the application-level data that is to 
Data 
(a) Data at the applications layer 
Transport layer header 
Data 
Trailer 
(b) Data at the transport layer 
Network layer header Transport layer header 
Data 
Trailer 
(c) Data at the network layer 
independently and they have to be reassembled at their des-
tination (they may arrive out of order). A virtual circuit first 
establishes a route through the network and then sends all 
the packets, in order, via this route. The difference between 
circuit switching and a virtual circuit is that message 
switching requires a connection for the duration of the con-
nection, whereas the virtual circuit can be used by other 
messages. 
The service offered by the physical layer is a best effort ser-
vice because it doesn't guarantee reliable delivery of messages. 
Information sent on the physical medium might be lost or 
corrupted in transit because of electrical noise interfering 
with the transmitted data. On radio or telephone channels 
the error rate may be very high (1 bit lost in 103 transmitted 
bits), whereas on fiber optic links it may be very low (1 bit lost 
in 1012). Layers on top of the physical layer deal with imper-
fections in this layer. The physical communication path may 
be copper wires, optical fibers, microwave links, or satellite 
links. 
Remember that the ISO reference model permits modifi-
cations to one layer without changing the whole of a network. 
For example, the physical layer between two nodes can be 
switched from a coaxial cable to a fiber optic link without any 
alterations whatsoever taking place at any other level. After 
all, the data link layer is interested only in giving bits to, or 
receiving them from, the physical layer. It's not interested in 
how the physical layer goes about its work. 
Standards and the ISO reference model for OSI 
Figure 14.12 shows how actual standards for the layers of the 
reference model have grown. This figure is hourglass shaped. 
The bottom is broad to cater for the many low-level protocols 
be transmitted from one computer to another. For the sake of 
simplicity, we'll assume that there aren't any presentation or 
session layers. The applications layer passes the data to the 
transport layer, which puts a header in front of the data and a 
trailer after it.The data has now been encapsulated in the 
same way that we put a letter into an envelope. The header 
and trailer include the address of the sender and the 
receiver. 
The packet from the transport layer is handed to the 
network layer which, in turn, adds its own header and trailer. 
This process continues all the way down to the physical layer. 
Now look at the process in reverse. When a network later 
receives a packet from the data link layer below it, the network 
layer strips off the network layer header and trailer and uses 
them to check for errors in transmission and to decide how to 
handle this packet. The network layer then hands the packet to 
the transport later about it, and so on. 

14.2 Protocols and computer communications 
5 8 3 
System and network management 
Command languages. 
Job transfer 
File tansfer 
Business data exchange 
•deotex syntax 
CCITT message 
Virtual terminal/ 
Electronic funds transfer/ 
Application layer standards 
User-defined syntax. 
Encryption, 
ISO/CCITT session protocol 
ISO/CCITT transport protocol 
Connectionless mode 
Bridges\ 
x 2 5 
Gateway (public to private)N 
HDLC 
LapB 
CSMA/CD 
Token ring 
Tokenbus 
RS499 
Physical layer standards 
X24/X21 
Figure 14.12 Standards for 
the layers of the basic 
reference model. 
introduced to deal with diverse types of channel, technology, 
and network, whereas the middle is narrow because it's desir-
able to have as few protocols as possible to move information 
around a network. The top is wide because it reflects the great 
range of applications of LANs. 
The ISO reference model for OSI isn't quite as popular 
today as it was in the 1980s. It was anticipated that most stan-
dards for networks would fit within the OSI framework. That 
hasn't happened. Many of today's standards are propriety 
(ad hoc or industrial) and don't conform closely to the OSI 
model. Some of the current standards such as the Internet 
TCP/IP protocol are layered even if the layers don't corre-
spond exactly to the seven layers we've just described. 
Figure 14.13 shows the Internet protocol stack alongside the 
ISO reference model. 
Because this text is devoted to the hardware aspects of 
computers, we look more closely at the bottom two layers of die 
reference model—the physical layer and the data link layer. 
Level 7 
Application layer 
Level 6 
Presentation layer 
Level 5 
Session layer 
Level 4 
Transport layer 
Level 3 
Network layer 
Level 2 
Data link layer 
Level 1 
Physical layer 
Applir. rtion ;; jyer 
"rnn^'ni-i-.'i :i 
•:i •-.fro! 
in em-;-: 
p-1-lci.'c! 
Ni l.vurr. 
:;cc."i 
, " ! • : • : ; cc! 
(a) ISO protocol stack. 
Figure 14.13 ISO and Internet layers 
(b) Internet protocol stack. 
Connectionless mode 
/ 
SDLC 
BSC 
\ 
LLC 
V24 
/ RS232 

584 
Chapter 14 Computer communications 
14.3 The physical layer 
Figure 14.14 illustrates the physical links connecting together 
two stations, User A and User B. A station is a point in a 
network that communicates with another point in the net-
work. Alternative words for station are node, receiver, trans-
mitter, or host. Before we can consider the factors influencing 
the design of a physical channel, it's necessary to look at the 
function it performs. 
A physical channel is the actual transmission path connect-
ing two stations and may be a wire link, a radio link, or any 
other suitable medium. A logical channel is an apparent trans-
mission path linking two stations but which may not actually 
exist. Of course, a logical channel is made up of one or more 
physical channels operating in tandem. However, the charac-
teristics of a logical channel may be very different from those 
of the physical channels of which it is composed. 
We can describe a physical channel under three headings: 
• the signal path itself 
• the mechanical interface to the signal path 
• the functionality of the channel. 
The signal path is concerned with the way in which data is 
to be transmitted electronically over a channel and the nature 
of the signal flowing across the channel must be defined; for 
example, we must ask what signal levels constitute logical Is 
and logical Os. 
A second and less obvious consideration concerns the 
mechanical arrangement of the link. What type of plugs and 
sockets does it use to connect the node with the transmission 
path? Standard connectors are as vital as standard signal 
levels, if the equipment at the end of a link is to be readily 
interchangeable with equipment from several different 
manufacturers. 
The third aspect of a physical layer link of importance is its 
functionality. In other words, what does the channel do apart 
from transmit data? The telephone channel, for example, not 
only permits voice signals to be sent from one subscriber to 
Satellite 
Modem 
User A 
Logical channel 
Figure 14.14 The physical channel. 
another, but also transmits the dialing pulses or tones needed 
to set up the connection between the subscribers. In the same 
way, a serial data link must normally include provision for 
carrying supervisory signals or messages that take part in 
controlling the data link. 
Some describe a fourth component of the physical layer 
that they call the procedural aspect. The procedural aspect 
governs the sequence of events that take place when a channel 
is set up, maintained, and closed. We include the procedural 
element of a standard in the functional element. 
14.3.1 Serial data transmission 
Although we introduced serial data transmission when we 
covered computer interfaces, we have included a short section 
on serial transmission here because we are interested in other 
aspects. Ideally, information should be moved from one com-
puter to another a word at a time, with all the m bits of a word 
transmitted simultaneously. An m-bit parallel data highway 
requires m wires to carry the data, and two or three additional 
wires to control the flow of information. Parallel links are fea-
sible only for computers separated by up to several meters. 
Networks transmit data serially a bit at a time and require 
only two lines—one to carry the data and one to act as the 
ground return. Remember that a voltage has a meaning only 
when specified with respect to some reference point such as the 
ground. If a single path links two points, data can be moved in 
only one direction at a time. Fiber optic links require a single 
fiber, whereas radio links don't need a physical connection. 
Multiplexing signals 
A problem facing those who wish to transmit information 
over long distances is the cost of the physical transmission 
path. Whether it's the cost of constructing a line of telegraph 
poles from coast to coast in the nineteenth century or the cost 
of launching a satellite today, long-distance communications 
channels don't come cheap. Consequently, engineers have 
done everything they can to squeeze the last drop of capacity 
out of a communications channel. 
The information-carrying capacity 
of a channel is determined by two 
parameters—its bandwidth and the 
level of noise (i.e. unwanted signals) 
on the channel. If you have a channel 
that's transporting less data than its 
maximum capacity permits, you are 
not using it fully. 
The efficient use of a communica-
tions channel can be increased by a 
technique called multiplexing in which 
two or more streams of information 
share the same channel. Figure 14.15(a) 
UserB 
Physical channel 
Telephone 
link 
Radio X - 4 
Radio 
link,-' 
\ l i n k 
Telephone 
.link 
Modem I 

14.3 The physical layer 
585 
HALF- AND FULL-DUPLEX CHANNELS 
There are three types of transmission paths between stations.The 
most basic transmission path is called simplex and permits the 
transmission of information in one direction only; that is, there's a 
single transmitter at one end of the transmission path and a 
single receiver at the other end with no reverse flow of 
information. The other two arrangements are more interesting 
Single transmission 
path  
(a) Half-duplex transmission. 
Dual transmission 
paths 
| Full-duplex transmission. 
and are called halfduplex and full duplex, respectively and are 
illustrated below.A half-duplex data link transmits information 
in only one direction at a time (i.e. from A to B or from B to A). 
Two-way transmission is achieved by turning round the 
channel. 
The radio in a taxi represents a half-duplex system. Either 
the driver speaks to the base station or the base station speaks 
to the driver. They can't have a simultaneous two-way 
conversation. When the driver has finished speaking, they say 
'over' and switch the radio from transmit mode to receive 
mode. On hearing 'over', the base station is switched from 
receive mode to transmit mode. 
A full-duplex data link permits simultaneous transmission in 
both directions. The telephone channel is an example of a full-
duplex system, because you can both speak and listen at the 
same time. Some data transmission systems use the 
telephone network in a half-duplex mode. 
Transmitter 1 
Receiver 1 
Transmitter 1 
Receiver 1 
Transmitter 2 
» Receiver 2 
Transmitter 2 
*X 
/
' 
0 
Receiver 2 
Transmitter 3 
*X 
Communications 
channel 
/
' 
0 
Rori-.n-rj 
Transmitter 3 
Communications 
channel 
Rori-.n-rj 
Communications 
channel 
Transmitter 4 
Communications 
channel 
tr 
Transmitter 4 
Communications 
channel 
Hi^h-^peed v.vt* 
(multiplexer) 
(a) Time-division multiplexing. 
u 
(demultiplexer) 
Channel 1 
Channel 2 
Channel 3 
CIlciT-l-1 4 
Channel 1 
Channel 2 
Channel 3 
Channel 4 
-+• time 
(b) A time-division multiplexed signal consists of a sequence of time slots. 
Transmitter 11—•] Modulator 1 
Transmitter 2 
Modulator 2 
'ansmitter 3 
Modulator 3 
Transmitter 41—H Modulator 4 
Communications 
channel 
* | 
Demodulator 1 |—1>\ Receiver 1 
Demodulator 2 
Demodulator 3 
Receiver 2 
Receiver 3 
Demodulator 4 |—•( 
Receiver 4 
(c) Frequency-division multiplexing. 
Amplitude 
*. 
Frequency 
(d) A frequency-division multiplexed signal consists a series of frequency bands. 
Figure 14.15 Time- and frequency-division multiplexing. 
Channel I 
L'-sn-s: 
i'li.-rinol 
Cw-?-
1 II Ml"1 II 4 . 
A 
B 
B 

586 
Chapter 14 Computer communications 
demonstrates time division multiplexing (TDM) in which the 
output of several transmitters are fed to a communications 
channel sequentially. In this example, the channel carries a 
burst of data from transmitter 1 followed by a burst of data 
from transmitter 2, and so on. At the receiving end of the link, 
a switch routes the data to receiver 1, receiver 2 , . . . , in order. 
If the capacity of the channel is at least four times that of 
each of the transmitters, all four transmitters can share the 
same channel. All that's needed is a means of synchronizing 
the switches at both ends of the data link. 
A simple TDM system gives each transmitter (i.e. channel) the 
same amount of time whether it needs it or not. Such an 
arrangement leads to an inefficient use of the available band-
width. Statistical time division multiplexing allocates time slots 
only to those channels that have data to transmit. Each time slot 
requires a channel number to identify it, because channels aren't 
transmitted sequentially. Statistical multiplexing is very effective. 
Figure 14.15(c) demonstrates an alternative form of multi-
plexing called frequency division multiplexing, FDM. In this 
case the bandwidtJi of the channel is divided between the four 
transmitters. Unlike in TDM each transmitter has continuous 
Half- and Full-duplex Channels access to the channel but it has 
access to only one-quarter of the channel's bandwidth. 
SYNCHRONIZING SIGNALS 
Serial data transmission begs an obvious question. How is the 
stream of data divided up into individual bits and the bits 
divided into separate words? The division of the data stream 
into bits and words is handled in one of two ways: 
asynchronously and synchronously. 
We met asynchronous serial systems when we described 
the ACIA. In an asynchronous serial transmission system the 
clocks at the transmitter and receiver responsible for dividing 
the data stream into bits are not synchronized. When the 
transmitter wishes to transmit a word, it places the line in a 
space state for one bit period. When the receiver sees this 
start bit, it knows that a character is about to follow. The 
incoming data stream can then be divided into seven bit peri-
ods and the data sampled at the center of each bit. The 
receiver's clock is not synchronized with the transmitter's 
clock and the bits are not sampled exactly in the center. If the 
receiver's clock is within approximately 4% or so of the trans-
mitter's clock, the system works well. 
1
0 
1
0 
Data 
Phase-encoded 
-1 
We're already familiar with frequency division multiplex-
ing. All a radio station does is to change the frequency 
range of speech and music signals to a range that can be 
transmitted over the airwaves. A radio receiver filters out one 
range of frequencies from all the other frequencies and then 
converts them back to their original range. 
Suppose that the bandwidth of the data from each trans-
mitter extends from 0 to 20 kHz and the communications 
link has a bandwidth of 80 kHz. The output of the first trans-
mitter is mapped onto 0 to 20 kHz (no change), the output of 
the second transmitter is mapped onto 20 to 40 kHz, the out-
put of the third transmitter is mapped onto 60 to 60 kHz, and 
so on. A device that maps one range of frequencies onto 
another range of frequencies is called a modulator (we will 
have more to say about modulators when we introduce the 
modem later in this chapter). 
At the receiver end of the link, filters separate the incoming 
signal into four bands and the signals in each of these bands are 
converted back to their original ranges of 0 to 20 kHz. In practice 
it is necessary to leave gaps between the frequency bands because 
filters aren't perfect. Moreover, a bandpass channel doesn't usu-
ally start from a zero frequency. A typical FDM channel might be 
from, say, 600 MHz to 620 MHz in 400 slices of 50 kHz each. 
If the duration of a single bit is Tseconds, the length of a 
character is given by the start bit plus seven data bits plus the 
parity bit plus the stop bit = 101 Asynchronous transmission 
is clearly inefficient, because it requires 10 data bits to trans-
mit 7 bits of useful information. Several formats for asynchro-
nous data transmission are in common use; for example, eight 
data bits, no parity, one stop bit. 
Two problems face the designer of a synchronous serial sys-
tem. One is how to divide the incoming data stream into indi-
vidual bits and the other is how to divide the data bits into 
meaningful groups. We briefly look at the division of serial 
data into bits and return to the division of serial data into 
blocks when we introduce bit-oriented protocols. 
If the data stream is phase encoded, a separate clock can be 
derived from the received signal and the data extracted. The 
diagram shows a phase-encoded signal in which the data signal 
changes state in the center of each bit cell. A low-to-high tran-
sition signifies a 1 and a high-to-low transition signifies a 0. 
1
1
0 
0 
1 
nr^ju 
0 
1 
0 

14.4 The PSTN 
5 8 7 
14.4 The PSTN 
The most widely used transmission path for wide area 
digital data networks is the telephone system—often called 
the public switched telephone network (PSTN) to distinguish 
it from private networks. We first discuss some of the 
characteristics of the telephone network and then describe 
the modem used to interface digital equipment to the 
network. 
14.4.1 Channel characteristics 
One way of characterizing a telephone channel is to apply a 
sine wave (Fig. 14.16) to the transmitter end of a telephone 
link and then to measure its amplitude at the receiver. The 
gain of the telephone channel is expressed as a logarithm; that 
is, 101og10(Po/Pi), where P; is the transmitted power level and 
P0 the received power level. The unit of gain is the decibel 
(in honor of Bell) and is positive if the signal is amplified (i.e. 
P0 > P^ and negative if the signal is attenuated (i.e. P0 < PJ. 
In a system without amplifiers, the gain is always less than 1. 
By varying the frequency of the sine wave and recording 
the gain of the channel for each frequency, the relationship 
between the gain of the channel and the transmitted fre-
quency can be derived. Such a graph is called the ampli-
tude-frequency 
distortion characteristic of the channel 
Amplitude 
-»-Tirne 
One sine wave, period 7" 
Figure 14.16 The sine wave. 
Relative 
amplitude (dB) 
300 
3300 
Passband 
Figure 14.17 Characteristics of the telephone network. 
(see Fig. 14.17). The frequency axis is invariably plotted on 
a logarithmic scale. An ideal channel has a flat frequency 
response over all the frequencies of interest; that is, the gain 
should not vary with frequency. A similar type of graph is 
used to characterize hi-fi equipment. Figure 14.17 describes 
the frequency response of a hypothetical ideal telephone 
channel. The attenuation of the channel in its passband is 
referred to as a 0 dB level (i.e. a gain of unity) and attenuation 
at other frequencies is measured with respect to this value. 
Figure 14.17 demonstrates that some frequencies are 
transmitted with little attenuation but that frequencies below 
the lower cut-off point/, and above the upper cut-off point fu 
are severely attenuated. Most telephone channels are not as 
well behaved as the ideal channel of Fig. 14.17. The passband 
between jx and /„ is not usually flat and the passband may 
sometimes be very much less than 300 Hz to 3300 Hz. 
Although most of the energy in human speech is below 
3300 Hz, certain sounds have significant energy components 
above this frequency; for example, a cut-off point of 3300 Hz 
makes it very difficult to distinguish between the sibilant 
sounds 'f' and V. 
Figure 14.17 doesn't tell the whole story. Signals suffer not 
only from amplitude-frequency distortion but also from 
phase distortion. Any signal can be decomposed into a series 
of sine waves and cosine waves of different frequencies. Phase 
distortion is related to the time delay experienced by the var-
ious sine and cosine waves making up a particular digital 
sequence. When a pulse sequence travels along a cable, its 
component sine and cosine waves suffer different delays. 
These signals at the receiving end of the network add up to 
produce a waveform with a different shape to the one that 
was originally transmitted. The phase distortion introduced 
by a telephone channel distorts the shape of transmitted 
pulses, making it difficult to distinguish between signals rep-
resenting 0s and 1 s. A device called an equalizer can be used to 
overcome some of the effects of the amplitude and phase dis-
tortion introduced by a telephone channel. 
Figure 14.18 defines the limits of acceptance of attenua-
tion-frequency distortion for a telephone channel between a 
single transmitter and receiver. The shaded area represents 
the forbidden region of unacceptable atten-
uation. If a real telephone channel has an 
amplitude-frequency distortion character-
istic that falls outside the envelope of 
Fig. 14.18, the telephone company should 
try to correct the faulty line or equipment. 
You might think that any signal can be 
transmitted across a telephone channel, as 
long as its frequency components fall 
within the envelope described by Fig. 14.18. 
In practice, there are restrictions on the 
nature of a transmitted signal because die 
channel is used to carry more dian user 
-> Frequency (Hz) 
-3 
0 

588 
Chapter 14 Computer communications 
Relative 
attenuation (dB) 
+ 1.7 
+0.9 
0 
-0.9 
The two shaded regions represent 
the forbidden zone. Signals may 
not fall in these raaions. 
r 
250 300 
800 
2400 
3400 3600 
-+• Frequency (Hz) 
Figure 14.18 Limits of acceptance for attenuation-frequency distortion. 
Relative 
attenuation (dB) 
0 
-10 
-20 
-30 
-TO 
500 
1000 
1500 2000 
2500 3000 
•Frequency (Hz) 
Figure 14.19 Restriction on energy content of transmitted 
signals. 
Data 
Q 
Q 
Modulated signal 
Figure 14.20 Amplitude modulation. 
data. The analog channel provided by the PSTN is a linear 
channel in the sense that its output is the sum of all the inputs 
to the channel. This means that you can transmit two signals 
in different parts of the channel's bandwidth and then sepa-
rate them at the receiver. Digital systems don't have this prop-
erty—it's not generally possible to add two digital signals 
together at one end of a channel and then separate them at 
the other end. 
Because analog channels can transmit 
more than one signal simultaneously, the 
PTTs have allocated certain parts of the 
telephone channel's bandwidth to signal-
ing purposes. Human speech doesn't con-
tain appreciable energy within 
these 
signaling bands and a normal telephone 
conversation doesn't affect the switching 
and control equipment using these fre-
quencies. 
A consequence of the use of certain fre-
quencies for signaling purposes is that data 
transmission systems mustn't generate sig-
nals 
falling 
within 
specified 
bands. 
Figure 14.19 shows the internationally 
agreed restriction on signals transmitted by 
equipment connected to the PSTN. 
Any signals transmitted in the ranges 500 to 800 Hz and 
1800 to 2600 Hz must have levels 38 dB below the maximum 
in-band signal level. 
14.4.2 Modulation and data transmission 
We are now going to look at a topic called modulation, the 
means of modifying signals to make them suitable for trans-
mission over a particular channel. 
Signals and modulation 
A telephone channel can transmit signals within its pass band 
but can't transmit digital pulses that are 
composed of sine waves with an infinite 
range of frequencies. If a sequence of binary 
signals were presented to one end of a tele-
phone network, some of the sine waves 
making up the binary pulses would be atten-
uated. Because the telephone network does 
not attenuate each frequency component 
*_ 
p. ,ii,,e 
equally, the sine waves at the receiving end 
11 
of the network would not add up to produce 
» 
the same waveform that was presented to 
the transmitting end. The digital signals 
would be so severely distorted that they 
would be unrecognizable at the receiving 
end of the circuit. 
Because the telephone network can transmit voice-band sig-
nals in the range 300 to 3300 Hz, various ways of converting 
digital information into speech-like signals have been investi-
gated. Figure 14.20 shows how the digital data can be used to 
change, or modulate, the amplitude of a sine wave in sympathy 
with a digital signal. This technique is known as amplitude 
modulation or AM. The equipment needed to generate such a 
signal is called a modulator, and that required to extract the 
digital data from the resulting signal is called a demodulator. 
*Time 
1 
0 
1 
0 
1 

14.4 The PSTN 
5 8 9 
The interface between a computer and a 
telephone system is called a modem 
(modulator-demodulator). Because AM is 
more sensitive to noise than other modula-
tion techniques, it is not widely used in data 
transmission. 
Instead of modulating a sine wave by 
changing its amplitude, it's possible to 
change its frequency in sympathy with the 
digital data. In a binary system, one fre-
quency represents one binary value and a 
different frequency represents the other. 
Figure 14.21 shows a frequency modulated 
(FM) signal. FM is widely used because it 
has a better tolerance to noise than AM (i.e. 
it is less affected by various forms of inter-
ference). As two frequencies are used to 
represent the two binary states, frequency 
modulation is sometimes referred to as fre-
quency shift keying (FSK). 
Figure 14.22 illustrates phase modulation 
(PM), where the phase of the sine wave is 
changed in sympathy with the digital signal. 
PM is widely used and has fairly similar 
characteristics to FM. If the phase change 
corresponding to a logical 1 is 180°, and 0° 
(no change) corresponds to a logical 0,1 bit 
of information can be transmitted in each 
time slot (Fig. 14.22). If, however, the phase 
is shifted by multiples of 90°, 2 bits at a time 
can be transmitted (Fig. 14.23). 
High-speed m o d e m s 
Modems operate over a wide range of bit 
rates. Until the mid 1990s most modems 
operated between 300 bps to 9600 bps. Low 
bit rates were associated with the switched 
telephone network where some lines were 
very poor and signal impairments reduced 
the data rate to 2400 bps or below. The 
higher rates of 4800 bps and 9600 bps were 
generally found on privately leased lines 
where the telephone company offered a 
higher grade of service. 
The growth of the Internet provided a 
mass market for high-speed modems. 
Improved modulation techniques and bet-
ter signal-processing technology has had a 
massive impact on modem design. By die 
mid-1990s, low-cost modems operated at 
14.4 kbaud or 28.8 kbaud. By 1998, 
modems capable of operating at 56 kbaud 
over conventional telephone lines were 
Data 
Modulated signal 
*-Time 
Figure 14.21 Frequency modulation. 
Data 
Modulated signal 
• Time 
Figure 14.22 Phase modulation. 
11 
Digital 
1 0 
data 
01 
00 
-•Time 
Modulated 
signal 
• Time 
Next level 10 
advance 
phase by 180° 
Next level 00 
advance 
phase by 0' 
Next level 11 
advance 
phase by 270'' 
Next level 10 
advance 
phase by 180° 
Figure 14.23 Differential phase modulation. 
Reference 
sine wave 
0 
0 
1 
1 
0 
0 
0 
I 
1 
I 
0 
| 
1 
1 
I 0 

590 
Chapter 14 Computer communications 
NOISE 
Noise is the generic term for unwanted signals that are added 
to the received signal. One source of noise, called thermal 
noise, is caused by the random motion of electrons in matter. 
Thermal noise appears as the background hiss on telephone, 
radio, and TV circuits, and is called Gaussian noise because of 
its statistical properties. The amount of thermal noise depends 
on the temperature of the system and its bandwidth. Only by 
cooling the system or by reducing its bandwidth can we 
reduce the effects of thermal noise. Receivers that pick up the 
weak signals from distant space vehicles are cooled in liquid 
nitrogen to minimize the effects of thermal noise. In general, 
the contribution of thermal noise to all other forms of noise is 
not usually the limiting factor in terrestrial switched telephone 
networks. 
Another source of noise is cross-talk picked up from other 
circuits due to electrical, capacitive, or magnetic coupling. We 
can think of cross-talk as crossed lines. Careful shielding of 
cables and isolation of circuits can reduce cross-talk. Impulsive 
noise produces the clicks and crackles on telephone circuits 
and is caused by transients when heavy loads such as elevator 
motors are switched near telephone circuits, lightning, and 
dirty and intermittent electrical connections. Impulsive noise 
accounts for the majority of transmission errors in telephone 
networks. The diagram illustrates impulsive noise. 
Amplitude 
White noise 
(thermal noise) 
When the transmitted signal reaches the receiver, some of 
its energy is echoed back to the transmitter. Echo cancellers at 
the ends of a telephone channel remove this unwanted signal. 
If they are poorly adjusted, the receiver gets the transmitted 
signal plus a time-delay and distortion of the data. 
The signal-to-noise ratio of a channel is defined as 
10logl0(S//^, where 5 is the signal power and N the noise 
power. Because the signal-to-noise ratio is a logarithmic value, 
adding 10 dB means that the ratio increases by a factor of 10. 
Signal-to-noise ratio determines the error rate over the channel. 
These noises are additive because they are added to the 
received signal. Multiplicative noise is caused by multiplying the 
received signal by a noise signal.The most common 
multiplicative noise is phase jitter caused by random errors in the 
phase of the clock used to sample the received signal. All these 
sources of noise make it harder to distinguish between signal 
levels in a digital system. 
CHANNEL CAPACITY 
A channel has a finite bandwidth that limits its switching 
speed. The maximum data rate is given by 28 • log2£, where B 
is the channel's bandwidth and I is the number of signal levels. 
If the bandwidth is 3000 Hz and you are using a signal with 
1024 discrete signal levels, the maximum data rate is 
2 X 3000 X log21024 = 6000 X 10 = 60 kbps.This figure 
relates the capacity of a noiseless channel to its bandwidth. 
You can increase a channel's capacity by using more signal 
levels. Claude Shannon investigated the theoretical capacity 
of a noisy channel in the late 1940s and showed that its 
capacity is limited by both its bandwidth and the 
noise level. Shannon proved that the theoretical 
capacity of a communications channel is given by 
B • log2(1 + S/N), where B is the bandwidth, S is the 
signal power, and N is the noise power. A telephone 
line with a bandwidth of 3000 Hz and a signal-to-
noise ratio of 30 dB has a maximum capacity of 
3000 X log2(1 + 1000) = 29 900 bps. 
Shannon's theorem provides an absolute limit that 
can't be bettered. Modern modems can apparently do 
)lse 
better than theory suggests by compressing data 
before transmission. Moreover, the noise on 
telephone lines tends to be impulsive or bursty, whereas the 
theoretical calculations relating channel capacity to noise 
assume that the noise is white noise (e.g. thermal noise). By 
requesting the retransmission of data blocks containing errors 
due to noise bursts, you can increase the average data rate. 
••Time 
available for the price of a 1200 bps modem only a decade 
earlier. 
High-speed modems operate by simultaneously changing 
the amplitude and phase of a signal. This modulation tech-
nique is called quadrature amplitude modulation (QAM). A 
QAM signal can be represented mathematically by the expres-
sion S • sin(o>r) + C • (wf), where S and C are two constants. 
The term quadrature is used because a sine wave and a cosine 
wave of the same frequency and amplitude are almost identi-
cal. The only difference is that a sine wave and a cosine wave are 
90° out of phase (90° represents '/* of 360°—hence quadrature). 
Figure 14.24 demonstrates a 32-point QAM constellation in 
which each point represents one of 32 discrete signals. A signal 
element encodes a 5-bit value, which means a modem with a 
signaling speed of 2400 baud can transmitatl2000bps. 
Figure 14.25 demonstrates that the points in a QAM con-
stellation are spaced equally. Each circle includes the space 
that is closer to one of the signal elements than to any other 
element. When a signal element is received, the values of S and 
C are calculated and the value of the signal element 
^ 
Impulsive noise 

14.4 The PSTN 
5 9 1 
9 
«} 
O 
~9~ 
9 
9 
There are 32 possible 
signal values. Each 
point represents a 5-bit 
binary code. 
9 
O 
o 
m 
® 
- ® -
IS 
- + C 
m 
By changing the amplitude 
and phase of a sine wave 
and a cosine wave by fixed 
amounts, their sum 
generates 32 discrete points, 
each of which represents a 
5-bit binary value. 
9 
Figure 14.24 The 32-point QAM constellation. 
Figure 14.25 The packing of points in a QAM constellation. 
Amplitude 
' ' \ 
Amplitude f 
y'""'"'% 
difference 
!»••.-», 
•^Time 
Phase 
difference 
(a) Phase and amplitude difference. 
(b) Effect of phase ad amplitude 
errors on a QAM signal. 
Figure 14.26 Effect of errors on a QAM point. 
determined. If noise or other impair-
ments cause a point to be shifted (i.e. 
there are errors in the received values of 
constants S and C), an error doesn't 
occur unless the values of S and C move 
the received point outside a circle. 
Figure 14.26 shows how amplitude and 
phase errors modify the position of a 
point in the QAM constellation. 
14.4.3 High-speed 
transmission over the PSTN 
The backbone of the POTS (plain old 
telephone system) is anything but 
plain. Data can be transmitted across 
the World via satellite, terrestrial microwave links, and fiber 
optic links at very high rates. The limitation on the rate at 
which data can be transmitted is known as the last mile; that 
is, the connection between your phone and the global net-
work at your local switching center. 
ISDN 
A technology called integrated services digital network (ISDN) 
was developed in the 1980s to help overcome the bandwidth 
limitations imposed by the last mile. ISDN was intended for 
professional and business applications and is now available to 
anyone with a PC. There are two variants of ISDN—basic 
rate services and primary rate services. The basic rate service 
is intended for small businesses and provides three fully 
duplex channels. Two of these so-called B channels can carry 
voice or data and the third D channel is used to carry control 
information. B channels operate at 64 kbps and the D chan-
nel at 16 kbps. 
ISDN's early popularity was due to its relatively low cost 
and the high quality of service it offers over the telephone line. 
You can combine the two B channels to achieve a data rate of 
128 kbps. You can even use the D channel simultaneously to 
provides an auxiliary channel at 9.6 kbps. Note that ISDN can 
handle both voice and data transmission simultaneously. 
Several protocols have been designed to control ISDN 
systems. V. 110 and V. 120 are used to connect an ISDN commu-
nications devices to high-speed ISDN lines. ISDN took a long 
time from its first implementation to its adoption by many 
businesses. However, newer technologies plus cable networks 
have been devised to overcome the last mile problem and ISDN 
did not become as commonplace as some had anticipated. 
ADSL 
If there's one thing you can guarantee in the computing 
world, it's that yesterday's state-of-the-art technology will the 
current standard and a new state-of-the-art technology will 
emerge. Just as ISDN was becoming popular in the late 1990s, 
Amplitude 
difference — 
/ 

592 
Chapter 14 Computer communications 
MODEM STANDARDS 
In the USA, modem standards were dominated by the Bell 
System's de facto standards. Outside the USA, modem 
standards were determined by the International Consultative 
Committee on Telegraphy and Telephony (CCITT). Over time, 
CCITT standards became dominant when high-speed modems 
were introduced. 
Early modems operated at data rates of 75, 300,600,1200, 
2400,4800, and 9600 baud. Modern modem rates are 14 400, 
19 200,28 800,36 600, and 56 000 baud. Modem standards 
define the following. 
• Modulation method Low-and medium speed 
modems use frequency modulation. High-speed modems 
employ phase modulation and QAM (quadrature amplitude 
modulation). 
• Channel type Some modems operate in a half-duplex 
mode, permitting a communication path in only one 
direction at a time. Others support full-duplex operation 
with simultaneous, two-way communication. Some systems 
permit a high data rate in one direction and a low data rate 
in the other, or reverse, direction. 
• Originate/answer The originating modem is at the end of the 
channel that earned out the dialing and set up the channel.The 
answer modem is at the end of the channel that receives the 
call Many modems can both originate calls and answer calls, but 
some modems are answer-only and cannot originate a call. 
Originate and answer modems employ different frequencies to 
represent 1 s and 0 s. 
• Asynchronous/synchronous An asynchronous data 
transmission system transmits information as, typically, 8-
bit characters with periods of inactivity between characters. 
A synchronous system transmits a continuous stream of bits 
without pauses, even when the bits are carrying no user 
information. 
Examples of modem standards 
• CCITT V.32 2400 baud, 4800 or 9600 bps, QAM 
• CCITT V.33 2400 baud, 14 400 bps, QAM 
• CCITT V.34 2400 baud, 28 800 bps, QAM 
• CCITT V.90 56 000 bps (this standard uses analog transmis-
sion in one direction and digital in the other) 
a system called asymmetric digital subscriber line (ADSL) was 
being developed as a new high-speed last mile system. 
As we've said, telephone lines have a bandwidth of 3000 
Hz, which limits the maximum rate at which data can be 
transmitted. In fact, the twisted wire pair between your home 
and the telephone company has a much higher bandwidth. 
The bandwidth of a typical twisted pair less than about 3 
miles is over 1 MHz. 
Asymmetric digital subscriber line technology exploits the 
available bandwidth of the local connection. The bandwidth 
of the telephone link is divided into a number of 4 kHz slices 
as Fig. 14.27 demonstrates. The first slice from 0 to 4 kHz rep-
resents the conventional telephone bandwidth. Frequencies 
between 4 kHz and 24 kHz aren't used in order to provide a 
guard band to stop the higher frequencies interfering with 
conventional telephone equipment. 
The spectrum between 24 kHz and 1.1 MHz is divided 
into 249 separate 4 kHz channels in the same way as the FM 
band is divided into slots for broadcasting stations. A data 
signal is assigned one of these slices and its spectrum tailored 
to fit its allocated 4 kHz slot. At the other end of the link, the 
signal in that 4 kHz slot is converted back into the data signal. 
Until recently it was very difficult to perform these opera-
tions. The advent of low-cost digital signal processing has 
made it much easier to process signals (i.e. to shift their range 
of frequencies from one band to another). 
The characteristics of these slots vary with frequency; for 
example, there is a greater attenuation of signals in slots close 
to 1.1 MHz. The terminal equipment is able to use the better 
Amplitude 
1.1 MHz 
Frequency 
0 to 4 kHz 
(conventional 
PSTN bandwidth) 
Figure 14.27 Dividing a 1.1 MHz bandwidth into 4 kHz slots. 
channels to carry high data rates and to allocate the higher 
frequency channels to slower bit rates. 
14.5 Copper cable 
The majority of transmission paths are composed of twisted 
pairs, coaxial cable, radio links, or fiber optic links. A twisted 
pair is nothing more than two insulated wires that are twisted 
around each other. Why are the wires twisted? A wire acts as 
an antenna and picks up signals (i.e. interference). If two 
wires are intertwined, a signal induced in one wire is can-
celled by the signal induced in the other wire. Twisted pairs 
are used to transport signals over relatively short distances; 
for example, a twisted pair connects a telephone to its local 
exchange. Twisted pairs, using RJ-45 telephone-style connec-
tors, are used in some LANs. 

14.5 Copper cable 
5 9 3 
RS232C PHYSICAL LAYER PROTOCOL 
The first universal standard for the physical layer was published 
in 1969 by the Electronic Industry Association (EIA) and is 
known as RS232C (Recommended Standard 232 version C). 
This standard was intended for links between modems and 
computers but was adapted to suit devices such as printers. 
RS232 specifies the plug and socket at the modem and the 
digital equipment (i.e. their mechanics), the nature of the 
transmission path, and the signals required to control the 
operation of the modem (i.e. the functionality of the data link). 
In the standard, the modem is known as data 
communications equipment (DCE) and the digital equipment 
to be connected to the modem is known as data terminal 
equipment (DTE). 
DTE 
DCE 
Network 
DCE 
Terminal *-©-»• Modem 
RS232 / 
data link 
Modem 
Non-digital signals 
Because RS232 was intended for DTE to DCE links, its 
functions are very largely those needed to control a modem. 
The following control signals implement most of the 
important functions of an R232 DTE to DCE link. 
Request to send (RTS) is a signal from the DTE to the DCE. 
When asserted, RTS indicates to the DCE that the DTE wishes 
to transmit data to it. 
Clear to send (CTS) is a signal from the DCE to the DTE and, 
when asserted, indicates that the DCE is ready to receive data 
from the DTE. 
Data set ready (DSR) is a signal from the DCE to the DTE 
that indicates the readiness of the DCE. When this signal is 
asserted, the DCE is able to receive from the DTE. DSR 
indicates that the DCE (usually a modem) is switched on and is 
in its normal functioning mode (as opposed to its self-test 
mode). 
Data terminal ready (DTR) is a 
signal from the DTE to the DCE. When 
asserted, DTR indicates that the DTE is 
ready to accept data from the DCE. In 
systems with a modem, it maintains 
the connection and keeps the channel 
open. If DTR is negated, the 
communication path is broken. In everyday terms, negating 
DTR is the same as hanging up a phone. 
DTE 
Computer 
Cross-section 
•J coaxial cable. 
The diameters of the conductors (d and D) 
together with the dielectric constant of the 
insulator between the conductors determine 
the electrical characteristics of the cable. 
Coaxial cable consists of 
four tubes: 
1. inner conductor 
2. dielectric insulator 
3. outer conductor 
4. outer insulator (sheath) 
Dielectric 
Figure 14.28 Coaxial cable. 
Outer 
conductor 
\ 
Outer insulator 
(i.e. casing) 
transmit voice-band telephone signals 
(permitting up to 10000 channels per 
cable), cable television signals and digital 
signals in many local area networks. 
Transmission over distances greater than 
1 km is achieved by feeding the signal into 
an amplifier (called a repeater) and regen-
erating it before sending it on its way 
down the coaxial cable. 
14.5.1 Ethernet 
Coaxial cable consists of an inner conductor entirely sur-
rounded by an outer conductor and is the type of cable used 
to connect televisions to antennas. Between the two conduc-
tors lies an insulating material called a dielectric. Sometimes 
the outer conductor is braided or woven from fine copper 
wire and sometimes it's a solid conductor. Figure 14.28 illus-
trates the structure of coaxial cable (often abbreviated to co-
ax), whose thickness may vary between 5 and 25 mm. Coaxial 
cables can operate at high data rates over 100 Mbits/s and are 
used over short to medium distances. Coaxial cable can 
The Ethernet was proposed by Robert 
Metcalfe in the early 1970s. Metcalfe 
joined the Xerox Corporation and developed the Ethernet 
(the name comes from the ether, a medium that was once 
thought to fill all space). Xerox formed a consortium with 
DEC and Intel who approached the IEEE and proposed the 
Ethernet as a standard. 
The physical layer of the Ethernet was originally used a 
baseband coaxial cable with phase-encoded data at 
10 Mbits/s. All nodes are connected to the bus, subject to two 
conditions. There is a restriction on the length of the bus and 
a loop must not exist between any two points on the bus. 
Figure 14.29 describes the Ethernet's topology. Ethernet data 
is transmitted is discrete packets or bursts of data. 
Outer 
\ 
conductor 
Outer 
p 
insulator 
Inner 
S* 
conductor 
\ Insulator 
(dielectric) 
Inner 
conductor 

594 
Chapter 14 Computer communications 
STANDARDS AND CABLES 
During the late 1970s it became apparent that the introduction 
of a large number of ad hoc protocols for LANs would have a 
bad effect on the computer industry. In 1980 the IEEE 
established its Standards Project 802 to provide a framework 
for LAN standards. The 802 committee set itself 
the goal of designing a standard for the new LANs 
that would take account of existing and 
prospective technology, and the needs of LAN 
users.The 802 committee didn't intend to 
produce standards for all seven layers of the ISO 
reference model, but limited themselves to 
standards for the physical and data link layers. 
While the IEEE was organizing its 802 project, 
the Ethernet LAN was rapidly becoming a de facto 
standard for contention buses and therefore the 
IEEE had to incorporate it in their work. At the same time, 
engineers were involved in a vigorous debate about the relative 
merits of buses and rings as LAN topologies.The IEEE 802 
committee reflected the nature of the real world, so they 
devised a set of standards that took account of both bus and 
ring topologies. They wanted the greatest happiness for the 
greatest number of people.The IEEE 802 draft standard includes 
standards for an Ethernet bus, a token ring, and a token bus. The 
diagram illustrates the scope of the 802 standards. 
802.1 
802.2 Logical link control 
802.3 
Ethernet 
802.4 
Token 
bus 
802.5 
Token 
ring 
Data-link layer 
Logical link control 
(LLC) 
Medium access control 
(MAC) 
Physical layer 
The Ethernet's 10 Mbps data rate is low by today's 
standards. A new standard, IEEE 802.3u, operating at 100 Mbps 
was ratified in 1995, and work began on a Gigabit Ethernet in 
the late 1990s. In March 1996, the IEEE 802.3 committee 
approved the 802.3z Gigabit Ethernet Standardization project. 
Node 
o 
o 
£ o 
•—o 
Bus 
? ? ? 
All nodes (stations) are 
connected to the 
common bus. 
Figure 14.29 The bus. 
Such a bus network introduces the problem of contention. 
No two nodes can access the same channel simultaneously 
without their messages interfering destructively with each 
other. When two messages overlap in time, the event is called 
a collision and both messages are lost. 
The simplest solution to bus contention is for a node to go 
ahead and transmit a message when it wants. This approach 
assumes that the bus is only occasionally busy. If another 
node is transmitting at the same time or joins in before the 
message is finished, both messages are lost. If the node that 
sent the message doesn't receive an acknowledgement within 
a timeout period, it assumes that its message has been 
corrupted in transmission. 
When a message from a node collides with a message from 
another node, both messages are lost. If the duration of a mes-
sage is T seconds and two messages just collide, the total time 
lost is up to 2T seconds. Assuming that the 
probability of a station wanting to transmit 
a packet has a Poisson distribution, it can be 
shown that the maximum throughput of 
this system approaches 18% of the maxi-
mum channel capacity. A Poisson distribu-
tion is a statistical model of events such as 
the rate at which people use the telephone. 
The simplest form of contention control 
is to let the transmitters retransmit their 
messages. Unfortunately, such a scheme 
wouldn't work, because the competing 
nodes would keep retransmitting the 
messages and these would keep getting scrambled. The problem 
of collisions in a bus network is identical to that of two people 
approaching the same revolving door together—they can't both 
get in, they step back, and advance together causing a collision, 
so they step back again, advance together, collide,.... 
A better strategy on detecting a collision is to back off or wait 
a random time before trying to retransmit the frame. It is 
unlikely that the competing nodes would reschedule me trans-
missions for the same time. Networks operating under mis 
form of contention control are well suited to bursty traffic; that 
is, the arrangement works as long as the average traffic density 
is very low (much less than the maximum capacity of the bus). 
If the amount of traffic rises, there comes a point where colli-
sions generate repeat messages that generate further collisions 
and further repeats, and the system eventually collapses. 
The contention control mechanism can be improved by 
forcing a node to listen to the bus before trying to send its 

14.6 Fiber optic links 
595 
frame. Obviously, if one node is already in the process of 
sending a message, other nodes are not going to attempt to 
transmit. A collision will occur only if two nodes attempt to 
transmit at nearly the same instant. Once a node has started 
transmitting and its signal has propagated throughout the 
network, no other node can interrupt. For almost all systems 
this danger zone, the propagation time of a message from one 
end of the network to the other, is very small and is only a tiny 
fraction of the duration of a message. 
The contention mechanism adopted by Ethernet is called 
Carrier Sense Multiple 
Access with 
Collision Detect 
(CSMA/CD). When an Ethernet station wishes to transmit a 
packet, it listens to the state of the bus. If the bus is in use, it 
waits for the bus to become free. In Ethernet terminology this 
CABLE TERMINOLOGY 
The physical dimensions, the electrical or optical 
characteristics, and the connectors of the cables used to 
implement the physical medium of an Ethernet connection 
have been standardized. Some of the common standards 
are as follows. 
10Base2 
10 Mbps thin Ethernet cable (similar to TV 
antenna cable). 
10BaseT 
10 M bps switched Ethernet cable. Used with 
Ethernet routers and hubs.The cable is similar 
to telephone cable with the same RJ45 jack 
plug. 
100BaseT 
100 Mbps Ethernet cable using twisted pair 
cable (similar to 10BaseT). 
100BaseF 
100 Mbps fiber Ethernet cable. 
Medium n-| 
| 
Medium of refractive 
! 
J-"***" 
index n, 
x
^ 
Medium of refractive 
f\ 
index n2 
/ | 
) 
f& 
' 
Refracted ray 
(a) 92<9C Incident ray leaves the fiber. 
(c) Propagation of a ray along a fiber by repeated total internal reflection 
Figure 14.30 Total internal reflection. 
is called deference. Once a station has started transmitting it 
acquires the channel, and after a delay equal to the end-to-
end round trip propagation time of the network, a successful 
transmission without collision is guaranteed. 
14.6 Fiber optic links 
The very first signaling systems used optical technology—the 
signal fire, the smoke signal and later the semaphore. Such 
transmission systems were limited to line-of-sight operation 
and couldn't be used in fog. From the middle of the nine-
teenth century onward, electrical links have made it possible 
to communicate over long distances independently of 
weather conditions. 
Today, the confluence of different technologies has, once 
again, made it possible to use light to transmit messages. 
Semiconductor technology has given us the laser and light-
emitting diode (LED), which directly convert pulses of elec-
tricity into pulses of light in both the visible and infrared 
parts of the spectrum. Similarly, semiconductor electronics 
has created devices that can turn light directly into electricity 
so that we can detect the pulses of light from a laser or LED. 
The relatively new science of materials technology has given 
us the ability to create a fine thread of transparent material 
called an optical fiber. The optical fiber can pipe light from its 
source to its detector just as the coaxial cable pipes electronic 
signals from one point to another. 
Seemingly, light must be transmitted in a straight line and 
therefore can't be used for transmission over paths that turn 
corners or go round bends. Fortunately, one of the properties 
of matter (i.e. the speed of 
light in a given medium) 
makes it possible to transmit 
light down a long thin cylin-
der of material like an optical 
fiber. Figures 14.30(a) and (b) 
demonstrate the effect of a 
light beam striking the surface 
of an optically dense material 
in a less dense medium such as 
air. Light rays striking the sur-
face at nearly right angles to 
the surface pass from the 
material into the surrounding 
air after being bent or refracted 
as Fig. 14.30(a) demonstrates. 
The 
relationship 
between 
the angle of incidence G2 and 
the angle of refraction Qv is 
cos(92)/ cos(Q,) = index of 
refraction. 
(b) 62 > 6C Incident ray experiences 
total internal reflection. 
J Medium of refractive 
! 
j index n1  
Medium of refractive 
/ ' \ 
i index n2 
/ 
' 
\ 
\ 
Reflected ray 
] 
e?>6c 
Medium n^ 
Medium n2 
Medium n-! 
| 

596 
Chapter 14 Computer communications 
Light rays striking the surface at a shallow angle suffer total 
internal reflection and are reflected just as if the surface (i.e. the 
boundary between the optically dense material and the air) 
were a mirror. The critical angle, 6C, at which total internal 
reflection occurs, is a function of the refractive index of the 
material through which the light is propagated and the surface 
material at which the reflection occurs. The same phenome-
non takes place when a diver looks upward. Total internal 
reflection at the surface of the water makes the surface look 
like a mirror. Figure 14.30(c) demonstrates how light is prop-
agated along the fiber by internal reflections from the sides. 
By drawing out a single long thread of a transparent mater-
ial such as plastic or glass, we can create an optical fiber as illus-
trated in Fig. 14.31. The optical fiber consists of three parts: 
• the core itself that transmits the light 
• a cladding that has a different index of reflection to the core 
and hence causes total internal reflection at its interface with 
the core 
• a sheath that provides the optical fiber with protection and 
mechanical strength. 
The diameter of the optical fiber is very small indeed— 
often less than 100 u,m. Sometimes there is an abrupt junc-
tion between the core and cladding (a step-index fiber) and 
sometimes the refractive index of the material varies contin-
uously from the core to the cladding (a graded index fiber). 
Graded index fibers are difficult to produce and therefore 
more expensive than step-index fibers, but they offer lower 
attenuation and a higher bandwidth. 
Fiber optic links can be created from many materials but a 
fiber drawn from high-quality fused quartz has the least 
attenuation and the greatest bandwidth (e.g. the attenuation 
can be less than 1 db/km). The bandwidth of fiber optic links 
can range from 200 MHz to over 10 GHz (109Hz) which 
represents very high data rates indeed. 
,FM 
Cladding 
Figure 14.31 The optical fiber. 
dine 
-, / . 
Sheath 
There are several types of optical fiber, each with its own 
special properties (e.g. attenuation per km, bandwidth, and 
cost). Two generic classes of optical fiber are the multimode 
and single-mode fibers. Multimode fibers operate as 
described by bouncing the light from side to side as it travels 
down the fiber. Because a light beam can take many paths 
down the cable, the transit time of the beam is spread out and 
a single pulse of light is received as a considerably broadened 
pulse. Consequently, a multimode fiber cannot be used at 
very high pulse rates. 
A single-mode fiber has a diameter only a few times that of 
the wavelength of the light being transmitted (a typical diam-
eter is only 5 |xm). As a single-mode fiber does not support 
more than one optical path through the fiber, the transmitted 
pulse is not spread out in time and a very much greater band-
width can be achieved. 
The advantages of a fiber optic link (Fig. 14.32), over cop-
per cable and radio technologies are as follows. 
Bandwidth The bandwidth offered by the best fiber optic 
links is approximately 1000-fold greater than that offered by 
coaxial cable or microwave radio links. 
Attenuation High-quality optical fibers have a lower attenu-
ation than coaxial cables and therefore fewer repeaters are 
required over long links such as undersea cables. 
Mechanics The optical fiber itself is truly tiny and therefore 
lightweight. All that is needed is a suitable sheath to protect it 
from mechanical damage or corrosion. It is therefore cheaper 
to lay fiber optic links than coaxial links. 
Interference Fiber optic links are not affected by electromag-
netic interference and therefore they do not suffer the effect 
of noise induced by anything from nearby lightening strikes 
to cross-talk from adjacent cables. Furthermore, because they 
do not use electronic signals to convey information, there's 
no signal leakage from an optical fiber and therefore it's much 
harder for unauthorized persons to eavesdrop. 
14.7 Wireless links 
Wireless links transmit information through the ether and 
don't require a physical medium to be laid down between the 
transmitter and receiver. Wireless links are characterized by 
Source-to-fiber 
connector 
Fiber-to-detector 
connector 
Input 
Input 
signal 
* 
• 
Ji_nrir 
Transmitter 
module 
^
j
"
y 
O p t . c a l c a b t e J D ^ ^ : 
Fiber-to-fiber 
connector 
Output 
signal 
_n_nnr 
Receiver 
module 
Figure 14.32 The fiber optic 
link. 

14.7 Wireless links 
597 
the frequency of the radio signals used to transport data and 
whether or not they are terrestrial or satellite links. Table 14.2 
illustrates a portion of the electromagnetic spectrum used to 
transmit information. 
Signals in the frequency range 100 kHz to about 1000 MHz 
(i.e. 1 GHz) are used for terrestrial radio and television broad-
casting. Frequencies above 1 GHz are called microwaves and are 
used for applications ranging from radar to information trans-
mission to heating. Microwaves have two important properties: 
they travel in straight lines and they can carry high data rates. 
Because microwaves travel in straight lines, the Earth's cur-
vature limits direct links to about 100 km or so (depending 
on the terrain and the height of the transmitter and receiver 
dishes). Longer communication paths require repeaters— 
microwaves are picked up by an antenna on a tower, ampli-
fied, and transmitted to the next tower in the chain. Few 
industrial cities are without some tall landmark festooned 
with microwave dishes. 
Since the late 1960s satellite microwave links have become 
increasingly more important. A satellite placed in geostationary 
orbit 35 700 km above die equator takes 24 hours to orbit the 
Earth. Because the Earth itself rotates once every 24 hours, a 
satellite in a geostationary orbit appears to hang motionless in 
space and remain over the same spot. Such a satellite can be 
used to transmit messages from one point on the Earth's surface 
to another point up to approximately 12 000 km away, as 
illustrated in Fig. 14.33. 
Theoretically three satellites each separated by 120° could 
completely cover a band around the Earth. However, 
receivers at extreme limits of reception would have their 
dishes pointing along the ground at a tangent to the surface of 
the Earth. As the minimum practical angle of elevation is 
about 5°, satellites should not be more than about 110° apart 
for reliable operation. Data is transmitted up to the satellite 
on the uplink frequency, regenerated, and transmitted down 
again at the downlink frequency (the uplink frequency is 
higher than the downlink frequency). Table 14.3 describes 
some of the frequency bands used by satellites. Suitable 
microwave or coaxial links transmit data from a local source 
to and from the national satellite terminals. 
Satellites are used to transmit television signals, telephone 
traffic, and data signals. Data signals can be transmitted at 
rates greater than 50 Mbps, which is many times faster than 
that offered by the public switched telephone network but 
rather less than that offered by the fiber optic link (and much 
less than that offered by the super data highways). Satellite 
links can be replaced by fiber optic links. The advantage of the 
satellite is its ability to broadcast from one transmitter to 
many receivers. 
Satellite systems are very reliable. The sheer size of the 
investment in the satellite and its transport vehicle means that 
engineers have spent much time and energy in designing reli-
able satellites. Unfortunately, a satellite doesn't have an infinite 
life span. Its solar power panels gradually degrade due to the 
effects of the powerful radiation fields experienced in space, 
and it eventually runs out of the fuel required by its rocket jets 
to keep it pointing accurately at the surface of the Earth. 
Satellites operate mostly in the 1 to 10 GHz band. 
Frequencies below 1 GHz are subject to interference from 
terrestrial sources of noise and the atmosphere attenuates fre-
quencies above 10 GHz. Satellite users have to take account of 
a problem imposed by the length of the transmission path 
(about 70 000 km). Microwaves traveling at the speed of light 
(300 000 km/s) take approximately 250 ms to travel from the 
source to their destination. Consequently it is impossible to 
receive a reply from a transmission in under 0.5 s. Data trans-
mission modes using half duplex become difficult to operate 
due to the long transit delay and the large turnaround time. 
Satellite data links are better suited to full-duplex operation. 
High geosynchronous orbits are not the only option avail-
able. Figure 14.34 shows that satellites can be placed in one of 
three types of orbit. Satellites in low and medium Earth orbits 
Frequency band 
Name 
3^o 30 kHz 
Very low frequency 
(VLF) 
30 to 300 kHz 
Low frequency 
(LF) 
300 to 3000 kHz 
Medium frequency 
(MF) 
3 to 30 MHz 
High frequency 
(HF) 
Typical applications 
30 to 300 MHz 
0.3 to 3 GHz 
3 to 30 GHz 
30 to 300 GHz 
Very high frequency 
(VHF) 
Ultra-high frequency 
(UHF) 
Super-high frequency 
(SHF) 
Extra-high frequency 
(EHF) 
Long-range navigation, submarine communications 
Navigational aids and radio beacons 
Maritime radio, direction finding, commercial AM radio 
Short-wave broadcasting, transoceanic ship and aircraft 
communication, telegraph, facsimile 
FM radio, air traffic control, police, taxi, and utilities 
UHF television, navigational aids, cell phones 
Microwave links, radar, satellite communications 
Note: kHz = kilohertz = 103 Hz, MHz = megahertz = 106 Hz, GHz = gigahertz = 109Hz. 
Table 14.2 The radio frequency spectrum. 

598 
Chapter 14 Computer communications 
(a) Organization of a satellite link. 
/ - < Stateilite 
V 
Telephone 
Exchange 
UK 
Exchange 
(b) The geostationary orbit. 
35 700 km 
Diameter of the 
Earth (6400 km) 
Period 
of satellite 
orbit is 
24 hours 
(c) The time delay in sending a message between 
two ground stations. 
(d) The satellite's field of view is approximately one-third of the Earth's surface. 
Figure 14.33 The satellite link. 
Band 
Frequency range 
Characteristics 
L-band 
1.53 to 2.7 GHz 
Ku-band 
11.7 to 17.8 GHz 
Ka-band 
18 to 31 GHz 
Signals penetrate buildings and structures. Low power transmitters required 
Signals penetrate some structures. High data rates possible 
There is a lot of available unallocated spectrum and very high data rates are 
possible. The signals have little penetrating power and are attenuated by rain 
Table 14.3 Frequencies used in satellite communications. 
appear to move across the sky, which means that when your 
satellite drops below the horizon you have to switch the link 
to another satellite. Low Earth orbits require lots of satellites 
for reliable communications, but the latency is very low. 
Fewer satellites are required to cover the World from medium 
Earth orbits and the latency is about 0.05 to 0.14 s. 
14.7.1 Spread spectrum technology 
Although a radio signal is transmitted at a specific frequency, 
the signal does, in fact, occupy a range of frequencies because 
of the modulation process. For example, an AM signal trans-
mitted at frequency/c, occupies the frequency range fc — fm to 
/c + fm' w n e r e/m is the maximum modulating frequency. 
A problem with transmitting on a single frequency is the 
vulnerability of the radio link; the signal can be easily observed 
and it can be jammed. In the Second World War attempts were 
made to control torpedoes by radio links. It was clear that using 
a single frequency would not be a good idea because it could be 
jammed by transmitting another signal at the same frequency. 
A solution to the problem of jamming was suggested by Hedy 
Lamarr and George Antheil; they proposed changing the fre-
quency of the transmitter and receiver in synchrony to avoid 
transmitting on a single frequency. A clockwork-driven fre-
quency selector could be used in both the transmitter and 
receiver to change frequency every few seconds. 
Antheil and Lamar's proposal was not put into practice until 
the early 1960s when the US military implemented it to provide 
t \ 
Satellite in 
\ \ geostationary 
^S- orbit 
%.—i 
--*« 
• — 
Transmission delay (265 ms) 
(Earth) 
Transmission delay (265 ms) 

14.8 The data link layer 
5 9 9 
Orbit 500 to 1500 miles/ 
(800 km to 2400 k m ) . / 
Lateny 0.03 s 
/ 
Orbit 6250 to 13 000 miles 
(10000km to 20000km). 
., Latency 0.06 to 0.14 s 
Low Earth orbit 
<y 
Medium Earth orbit 
Geosynchronous " s . 
Earth orbit/ 
Qrbit 22 300 mites 
/ 
(35 700 km). 
_ / 
Latency 0.24 s 
Figure 14.34 Satellite orbits. 
secure radio links. The frequency changes (or hops) were made 
as a random sequence using electronics rather than mechanical 
switching. Because the frequency of the transmitted signal 
rapidly varies over a finite range within a band, the signal energy 
is distributed throughout the band. Consequendy, the system is 
often called spread spectrum technology. 
An advantage of spread spectrum technology is that the 
wireless link is less susceptible to interference. If an interfer-
ing signal is at a constant frequency, it will affect the received 
signal only when the interfering and data signal frequencies 
coincide. Moreover, if you have several spread spectrum 
frequencies occupying the same band at the same time, inter-
ference will take place only when the two or more frequencies 
are the same at the same time. 
The frequency 2.4 GHz has now been allocated to spread 
spectrum signals2 and IEEE standard 802.11 was developed 
to provide a short-range data commutations facility for lap-
tops and similar devices. The standard uses the same type of 
collision control mechanism as the Ethernet. 
Fourteen channels in the 2.4 GHz band are reserved for the 
802.11 systems. Each channel is separated by 5 MHz. However, 
these channels indicate only die center frequency used by a 
transmitter-receiver pair. An actual wireless link uses a band-
width of 30 MHz and, therefore, takes up five channels. 
14.8 The data link layer 
Now that we've looked at some of the ways in which bits are 
moved from one point to another by the physical layer, the 
next step is to show how the data link layer handles entire 
messages and overcomes imperfections in the physical layer. 
We are going to look at two popular protocols for the data 
link layer—a bit-oriented protocol and a protocol used by the 
Internet. 
14.8.1 Bit-oriented protocols 
A bit-oriented protocol handles pure binary data (i.e. strings 
of Is and 0s or arbitrary length). Binary data can be a core 
dump, a jpeg image, a program in binary form, a floating 
point number, and so on. When the data is stored in a pure 
binary form it's apparently impossible to choose any particu-
lar data sequence as a reserved marker or flag, because that 
sequence may also appear as valid data. We explain how the 
high-level data link control protocol (HDLC) delivers any 
pattern of bits between two nodes in a data link by means of 
a technique called bit stuffing. 
The key to understanding the HDLC protocol is the HDLC 
frame, the smallest unit of data that can be sent across a net-
work by the data link layer. Frames are indivisible in the sense 
that they cannot be subdivided into smaller frames, just as an 
atom can't be divided into other atoms. However, a frame is 
composed of several distinct parts just as an atom is made up 
of neutrons, protons and electrons. Figure 14.35 illustrates 
the HDLC format of a single frame. 
2 The 2.4 GHz band is shared by other users such a Bluetooth, baby 
monitors, and cordless phones. 
f Yii.: f!!er] 

6 0 0 
Chapter 14 Computer communications 
HISTORY OF WI-FI 
1997 IEEE Standard 802.11 specifies a wireless LAN using 
2.4 GHz with data rates of 1 and 2 MHz. Apple computer 
provides the first operating system to support Wi-Fi 
(called AirPort). 
1999 Standard 802.11b with a data rate of 11 Mbits/s is 
finalized.The maximum actual data rate is approximately 
5 Mbits/s. This was the first Wi-Fi standard to become widely 
accepted and it paved the way for low-cost wireless 
networks. 
1999 The 802.11a standard operates at 5 GHz and provides 
a maximum raw data rate of 54 MHz, corresponding to a prac-
tical user data rate of about 20 Mbits/s. Radio waves at 5 GHz 
are more readily absorbed than those at 2.4 GHz and 802.1 la-
based systems have not achieved the same success at 802.11b. 
2002 Intel's Centrino chipset had a remarkable effect on 
the wireless LAN market. Centrino consists of a low-power 
CPU, an interface chip, and an 802.11 b chip.This chipset was 
used in countless laptops to provide portability with low-
power consumption on Wi-Fi LAN connectivity. 
2003 Standard 802.11g combines the lower frequency 
advantage of 802.11b and the modulation rate of 802.11a to 
provide a raw bit rate of 54 Mbits/s in the 2.4 MHz band. 
Equally importantly, it is backward compatible with 802.1 lb. 
By the end of 2003, companies were producing tri-mode Wi-Fi 
adaptors capable of accessing 802.11a/b/g networks. 
The information field 
is optional 
Figyre 14.35 The HDLC frame format. 
Transmitter —•( + )—*f + \ 
Data link 
CD-"©-* 
Receiver 
Zero 
Flag 
insertion insertion 
Data to be transmitted 
Example 
Data to be transmitted 
Transmitted frame 
Flag removal 
Zero 
(detection of 
deletion 
start of frame) 
010111010111111111111111111110101 
01111110010111010111110111110111110111110010101111110 
•4 
• 
Closing 
flag 
Opening 
flag 
Inserted zeros 
Figure 14.36 Bit insertion and deletion. 
Each frame begins and ends with a unique 8-bit flag, 
01111110. Whenever a receiver detects the sequence 
01111110, it knows that it has located the start or the end of a 
frame. An error in transmission may generate a spurious flag 
by converting (say) the sequence 01101110 into 01111110. In 
such cases, the receiver will lose the current frame. Due to the 
unique nature of the flag, the receiver will automatically 
resynchronize when the next opening flag is detected. 
HDLC puts no restrictions whatsoever on the nature of the 
data carried across the link. Consequently, higher levels of the 
reference model can transmit any bit sequence they wish 
without affecting the operation of the data link layer. The 
only binary sequence that may not appear in a stream of 
HCLD data is the frame opening or closing flag 01111110. 
A simple scheme called zero insertion and deletion or bit stuff-
ing ensures that HDCL data is transparent. Figure 14.36 shows 
how bit stuffing operates. Data from the block marked trans-
mitter is passed to an encoder marked zero insertion that oper-
ates according to a simple algorithm. A bit at its input is passed 
unchanged to its output unless the five preceding bits have all 
been Is. In the latter case, two bits are 
passed to the output: a 0 followed by the 
input bit. As an example consider the 
sequence 010111111011 containing the 
forbidden flag sequence. If the first bit is 
the leftmost bit, the output of the encoder 
isOlOlllllOlOll. 
The bit-insertion mechanism guaran-
tees that any binary sequence can appear in 
the input data but a flag sequence can't 
occur in the output data stream because 
five Is are always terminated by 0. Flags 
intended as frame delimiters are appended 
to the data stream after the encoding block. 
At the receiving end of the link, open-
ing and closing flags are detected and 
removed from the data stream by the flag 
removal circuit. The data stream is then passed to the block 
marked zero deletion for decoding, which operates in the 
reverse way to zero insertion: if five Is are received in succes-
sion, the next bit (which must be a 0) is deleted. For example, 
the received sequence 0101111101011111000 is decoded as 
01011111101111100. 
Now that we've described how a data stream is divided into 
individual bits and the bits into frames, the next step is to 
look at the HDLC frame. Figure 14.35 demonstrates that the 
HDLC frame is divided into five logical fields: an address 
field, a control field, an optional information field, and a 
frame check sequence (FCS). 
Flag 
Flag 
01111110 
Address 
Control 
Information 
FCS 
01111110 
I 
1 
1 
111, 
1 
1 
1 

Slave 
Data link 
Slave 
Mailer 
Data link 
Mailer 
Slave 
Slave 
All communication is between a slave and the master. 
Direct stave-to-slave communication is not permitted. 
Figure 14.37 Master-slave transmission with HDLC. 
Address field 
The data link layer protocol may operate in one of several 
modes. Figure 14.37 illustrates the master-slave mode, where 
one station is the master station and all the other stations con-
nected to the master are called slaves. In the master-slave 
mode only the master may send messages when it wishes. A 
slave is not permitted to transmit until it is invited to do so by 
the master. 
The HDLC's address field provides the address of the slave. 
The master doesn't need an address because there's a unique 
master. When a master sends a frame, the address field 
identifies the slave for which the frame is intended. If the slave 
is transmitting a frame, the address is its own, identifying 
itself to the master. 
Any slave receiving a frame whose address doesn't match 
its own address ignores the message. Unlike humans, com-
puters don't listen to third-party traffic. 
The address field is 8 bits wide permitting 127 slaves to be 
directly identified. If the least-significant bit of the address 
field bit is a logical 0, the following byte is an extension of the 
address field. If the least-significant bit of the extension 
address is also a 0, the following byte is a further extension of 
the address. This arrangement permits an infinitely extend-
able address field. 
Two special-purpose addresses are defined. The address 
11111111 is a global address indicating that the frame is a 
broadcast intended for all stations on the network. The null 
address 00000000 causes the frame to be ignored by all 
stations! A null address is included for test purposes. 
Control field 
The 8-bit control field determines the type of the frame being 
transmitted and controls the flow of messages across the data 
link layer. Table 14.4 defines the three types of control field 
used by an HDLC frame. We have numbered the control field 
bits 1 to 8 (bit 1 is the least-significant bit) with the least-sig-
nificant bit on the left to conform to the HDLC standard. 
The two least-significant bits of a C-field define one 
of three types of frame: I-frame, S-frame or U-frame. 
14.8 The data link layer 
601 
THE MAC 
The MAC or media access control \s the address of a 
physical node in a computer network. In a PC the MAC is 
the address of the NIC (network interface card) or the 
wireless card.This address is unique and defines that 
particular card (no other card in the World has the 
same MAC). 
A MAC is a 12-digit hexadecimal number (i.e. 48 bits 
giving 248 possible unique addresses). MAC addresses are 
often printed on the back of a NIC, for example, 
00:50:BA:BD:12:C9 belongs to one of my interface cards. 
The first half of the MAC address defines the node's 
manufacturer, which is D-Link in this case. 
The MAC address is used by the data link layer to 
access nodes connected to the physical network. This is 
not to be confused with the IP (Internet protocol) address, 
which is a logical address identifying nodes across the entire 
Internet. 
Frame 
1
2 
3 
4 
5 
6 
7 
8 
type 
I frame 
0 
N(S) 
P/F 
N(R) 
S frame 
1 
0 
S 
S 
P/F 
N(R) 
U frame 
1 
1 
M 
M 
P/F 
M 
M 
M 
N(S) 
= send sequence number 
N(R) 
= receive sequence number 
P/F 
= poll/final bit 
SS 
= two supervisory bits 
MMMMM = five modifier bits 
Table 14.4 The format of the HDLC control field. 
An I-frame or information frame contains an information 
field and is used to transport data from a higher level layer 
than the data link layer. 
The S-frame or supervisory frame controls the flow of 
information on the link. Typical functions include acknowl-
edging I-frames or requesting the retransmission of frames 
lost during transmission. There are four types of S-frame, the 
type is indicated by the two bits labeled'S' in Table 14.4. We 
shall look more closely at the S-frame later. 
The unnumbered frame (U-frame) provides control func-
tions not available with the I- or C-frames. U-frames perform 
functions like setting up or changing the operating mode 
of the data link layer and connecting or disconnecting 
two stations. 
All three types of control field have a dual-purpose 
poll/final (P/F) bit. When transmitted by a master station, this 
is called a poll bit (P-bit) and indicates that the master is ask-
ing the secondary station for a response. Recall that in the 
master-slave mode, the secondary station cannot transmit 

602 
Chapter 14 Computer communications 
until it is invited to do so by the master. A control field with 
P/F = 1 sent by the master indicates such an invitation. 
When a control field is sent by a secondary station, the P/F 
bit is defined as a final bit and, when set, indicates that the 
current field is the last frame of the series. In other words, a 
slave sets P/F to 1 when it has no more frames to send. 
The state variables N(S) and N(R) in the control field are 
3-bit numbers in the range 0 to 7 that define the state of the 
system at any instant. N(S) is called the send sequence number 
and N(R) is called the receive sequence number. 
Only I-frames contain a send sequence number to label 
the current information frame; for example, if N(S) =101 
the frame is numbered 5. When this frame is received the 
value of N(S) is examined and compared with the previous 
value. If the previous value was 4, the message is received 
in sequence. But if the value was not 4, an error has 
occurred. The sequence count is modulo 8, so that it goes 
67012345670 
Consequently, if eight messages are lost, 
the next value of N(S) will apparently be correct. 
The receive sequence number, N(R), is available in both S 
and I control fields. N(R) indicates the number of the next I-
frame that the receiver expects to see; that is, N(R) acknowl-
edges I-frames up to and including N(R) — 1. Suppose station 
A is sending an I frame to B with N(S) = 3 and N(R) = 6. 
This means that frame A is sending frame number 3 and has 
safely received frames up to 5 from B. A expects to see an infor-
mation frame from B with the value of N(S) equal to 6. 
By means of the N(R) and N(S) state variables, it's impos-
sible to lose a frame without noticing it, as long as there are 
not more than seven outstanding I-frames that have not been 
acknowledged. If eight or more frames are sent, it is impossi-
ble to tell whether a value of N(R) = i refers to frame i or to 
frame i — 8. It is up to the system designer to ensure that this 
situation never happens. We will soon look at how N(S) and 
N(R) are used in more detail. 
FCS field 
Recall that the data link layer is built on top of an imperfect 
physical layer. Bits transmitted across a physical medium may 
become corrupted by noise with a 1 being transformed to a 0 
or vice versa. The error rate over point-to-point links in a local 
area network may be of the order of 1 bit lost in every 1012 bits. 
Error rates over other channels may be much worse than this. 
HDLC provides a powerful error-detection mechanism. 
At the receiver, the bits of the address field, control field, and 
I-field are treated as the coefficients of a long polynomial, 
which is divided by a polynomial called a generator. The 
HDLC 
protocol 
uses 
the 
CCITT 
generator 
10001000000100001 or*16 + xn + x5 + I. The result of the 
division yields a quotient (which is thrown away) and a 16-bit 
remainder, which is the 16-bit FCS appended to the frame. 
At the receiver, the message bits forming the A-, C-, and 
I-fields are also divided by the generator polynomial to yield 
a locally calculated remainder. The calculated remainder is 
compared with the received remainder in the FCS field. If 
they match, the frame is assumed to be valid. Otherwise the 
frame is rejected. 
You may wonder how the FCS is detected, because the I-
field may be of any length and no information is sent to indi-
cate its length directly. In fact, the FCS field cannot be 
detected. The receiver assembles data until the closing flag 
has been located and then works backward to identify the 
FCS and the I-field. 
HDLC message exchange 
The HDLC protocol supports several configurations. Here we 
consider only the unbalanced master-slave mode (NRM) 
where a slave may initiate transmission only as a result of 
receiving explicit permission from the master. 
Before we continue, it's necessary to define the four mes-
sages associated with a supervisory frame. Table 14.5 shows 
how the four S-frames are encoded. 
The RR (receiver ready) frame indicates that the station 
sending it is ready to receive information frames and is equiv-
alent to saying, 'I'm ready.' The REJ (reject) frame indicates an 
error condition and usually implies that one or more frames 
have been lost in transmission. The RE] frame rejects all 
frames, starting with the frame numbered N(R). Whenever a 
station receives an REJ frame, it must go back and retransmit 
all messages after N(R) — 1. Sending all these messages is 
sometimes inefficient, because not all frames in a sequence 
may have been lost. 
The RNR (receiver not ready) frame indicates that the 
station is temporarily unable to receive information frames. 
RNR is normally used to indicate a busy condition (e.g. the 
receiver's buffers may all be full). The busy condition is 
cleared by the transmission of an RR, REJ, or SREJ frame. 
An I-frame sent with the P/F bit set also clears the busy 
condition. 
The selective reject (SREJ) frame rejects the single frame 
numbered N(R) and is equivalent to 'Please retransmit frame 
number N(R)*. The use of SREJ is more efficient than REJ, 
because the latter requests the retransmission of all frames 
after N(R) as well as N(R). 
Control bit 
1 
Z 
3 
4 
5 
6 
7 
8 
S-frametype 
1 
0 
0 
0 
P/F 
<r- N(R) -> 
RR 
receiver ready 
1 
0 
0 
1 
P/F 
<- N(R) -» 
REJ 
reject 
1 
0 
1 
0 
P/F 
<— N(R) -> 
RNR receiver not ready 
1 0 
1 1 
P/F 
<— N(R) -> 
SREJ selective reject 
Table 14.5 The format of the S-frame. 

14.8 The data link layer 
603 
Figure 14.38 demonstrates a sequence of HDLC frame 
exchanges between A (the master) and B (the slave) in a half-
duplex mode. Each frame is denoted by type, N(S),N(R), 
P/F, where type is I, RR, REJ, RNR, or SREJ. Typical HDLC 
frames are 
Type 
N(S),N(R)P/F 
I, 5,0 
I-frame, N(S) = 5, N(R) = 0, 
I,5,0,P I-frame, N(S) = 5,N(R) = 0, poll bit set by master 
REJ„ 4, F S-frame, N(S) = 4 , reject, final bit set by slave 
Note that a double comma indicates the absence of an 
N(S) field. 
Initially in Fig. 14.38, the master station sends three 
I-frames. The poll bit in the third frame is set to force a 
response from the slave. The slave replies by sending two 
I-frames that are terminated by setting the F bit of the C-field. 
If the slave had no I-frames to send, it would have responded 
with RR„3,F. The values of N(S) and N(R) are determined by 
the sender of the frame. 
The master sends two more I-frames, terminated by a poll 
bit. The first frame (1,3,2) is corrupted by noise and rejected 
by the receiver. When the slave responds to the poll from the 
master, it sends a supervisory frame, R£J„3,F, rejecting the I-
frame numbered 3 and all succeeding frames. This causes the 
master station to repeat the two frames numbered N(S) = 3 
andN(S) = 4. 
When the master station sends an I-frame numbered 
I,5,2,P, it also is corrupted in transmission and rejected by the 
receiver. The secondary station cannot respond to this polled 
Computer A 
Computer B 
request. When the master sends a message with P = 1, it 
starts a timer. If a response is not received within a certain 
period, the timeout, the master station takes action. In this 
case, it sends a supervisory frame (RR„2,P) to force a 
response. The secondary station replies with another super-
visory frame (REJ„5,F) and the master then repeats the lost 
message. 
A selective reject frame, SREJ„N(R), rejects only the mes-
sage whose send sequence count is N(R). Therefore, 
SREJ„N(R) is equivalent to 'Please repeat your message with 
N(S) = N(R).' If a sequence of messages are lost, it is better to 
use REJ„N(R) and have N(R) and all messages following 
N(R) repeated. 
Figure 14.39 shows the operation of an HDLC system 
operating in full-duplex mode, permitting the simultaneous 
exchange of messages in both directions. 
We have explained only part of the HDLC data link layer 
protocol. Unnumbered fields are used to perform operations 
related to the setting up or establishing of the data link layer 
channel and the eventually clearing down of the channel. 
14.8.2 The Ethernet data link layer 
Figure 14.40 describe an Ethernet packet that consists of six 
fields. The 8-byte preamble is a synchronizing pattern used to 
detect the start of a frame and to derive a clock signal from it. 
The preamble consists of 7 bytes of alternating Is and 0s 
followed by the pattern 10101011. Two address fields are 
provided, one for the source and one for die destination. 
A 6-byte (48-bit) address allows sufficient address space for 
each Ethernet node to have a unique address. 
A message is denoted by 
type,N(S),N(R),P/F. For example, l,3,0,P 
indicates an information frame numbered 3, 
with an N(R) count of 0, and the poll bit set 
indicating that a response is required. Note 
that message 3 from A (i.e. 1.3,2) is lost. 
Therefore, when A sends the message 
1,4,2,P with the poll bit set, B responds with 
REJ„3,F. This indicates that B is rejecting 
all messages from A numbered 3 and 
above. The F bit is set denoting that B has no 
more messages to send to A.  
Figure 14.38 An example of 
an HDLC message exchange 
sequence. 
* 
' 
Ngisi 
Timeout! 
'—-
———S££__ 
^ 
* 
~~~-ifl£-_____ 
* 
" 
' 
Noise 
~ ~~—S££__ 

6 0 4 
Chapter 14 Computer communications 
Computer B 
(slave) 
A sends a frame 1,0,0 (information frame 
numbered 0, A is expecting a frame from B 
numbered 0). 
A sends frame 1,1,0 (information frame numbered 
1, A is still expecting a frame from B numbered 0). 
A sends frame 1,2,0. This frame is corrupted by 
noise and is not correctly received by B. 
B sends frame 1,0,2 (information frame numbered 
0, B is expecting a frame from A numbered (2). 
Note that because A's frame 1,2,0 has been lost, 
B is still expecting to see a frame from B labeled 
with N(S) = 2. 
A sends l,3,0,P (information frame numbered 3, 
A is expecting a frame numbered 0 from B). A is 
also polling B for a response. At this point A does 
not know that its previous message has been 
lost, and A has not yet received B's last 
message. 
B sends a reply to A's poll. This is REJ„2,F 
indicating that all A's messages numbered 2 and 
above have been rejected. The final bit, F, is set 
indicating that B has nothing more to send at the 
moment. 
A now sends 1,2,1 (information frame 2, and A is 
expecting to see a frame from B numbered 1). 
This frame is a repeat of A's information frame 
numbered 2, which was lost earlier. 
Figure 14.39 HDLC full-duplex transmission. 
Preamble 
(8 bytes) 
Destination 
address (6 bytes) 
Source 
address (6 bytes) 
Type 
(2 bytes) 
Figyre 14.40 Ethernet packet format. 
CRC 
(4 bytes) 
Preamble 
(7 bytes) 
STD 
(1 byte) 
Destination 
address (6 bytes) 
Source 
address {6 bytes) 
Type 
(2 bytes) 
The type field is reserved for use by higher level layers to 
specify the protocol. The data field has a variable length, 
although the size of an Ethernet packet must be at least 64 
bytes. The data field must be between 46 and 1500 bytes. The 
final field is a 4-byte cyclic redundancy checksum (CRC) that 
provides a very powerful error-detecting mechanism. 
Figure 14.41 describes the format of a packet conforming to 
the IEEE's 802.3 standard, which is very similar to the 
original Ethernet packet. The preamble and start-of-frame 
delimiter are identical to the corresponding Ethernet 
preamble. The principle difference is that the 802.3 packet has 
a field that indicates die length of the data portion of the frame. 
The 802.3 protocol covers layer 1 of the OSI reference 
model (the physical layer) and part of the data link layer 
Figure 14.41 IEEE 802.3 
packet format. 
called the medium access control (MAC). The IEEE 802 stan-
dards divide the data link layer into a medium access layer 
and a logical link control (LLC). 
14.9 Routing techniques 
How does a message get from one point in a network to its 
destination? Routing in a network is analogous to routing in 
everyday life. The analogy between network and computer 
routing is close in at least one sense—the shortest route isn't 
always the best. Drivers avoid highly congested highways. 
Similarly, a network strives to avoid sending packets along a 
link that is either congested or costly. 
' 
iiLQ-____^Noise 
•— 
— - i 2 d £ L _ _ _ 
• 
— - i i § _ _ _ 
I.6-1 
^—^Noise 
-—— 
^ " " ^ ^ 
Computer A 
(master) 
Data 
(variable) 
CRC 
(4 bytes) 

14.9 Routing techniques 
605 
CHARACTER-ORIENTED PROTOCOLS 
Character-oriented protocols belong to the early days of data 
communication. They transmit data as ASCII characters using 
special 7-bit characters for formatting and flow control. For 
example, the string 'Alan' is sent as a sequence of four 7-bit 
characters 100000100110111000011011101 LThis string of 
bits is read from left to right, with the leftmost bit 
representing the least-significant bit of the 'A'. We need a 
method of identifying the beginning of a message. Once this 
has been done, the bits can be divided into groups of seven (or 
eight if a parity bit is used) for the duration of the message. 
The ASCII synchronous idle character SYN (0010110;,) denotes 
the beginning of a message. The receiver reads the incoming 
bits and ignores them until it sees a SYN character. The 
following demonstrates the use of the SYN character. 
Character-oriented protocols provide point-to-point 
communication between two stations. Like all data link layer 
protocols, they both control the flow of information (message 
sequencing and error recovery) and they set up and maintain 
the transmission path. 
A consequence of reserving special characters for control 
functions is that the transmitted data stream must not 
contain certain combinations of bits, as these will be 
interpreted as control characters. Fortunately, there are ways 
of getting round this problem by using an escape character 
that modifies the meaning of following characters. 
The diagram below shows the format of a BiSync frame, a 
protocol originally devised by IBM.The SOH, STX, and ETC 
characters denote start of header, start of text, and end of 
text, respectively. 
Character sequence 
Casel 
010110000101100100111 
Case 2 
0100010 11011010100111 
Case 3 
0101100 0010110 0010110 
Bit sequence 
010110000101100100111 
010001011011010100111 
010110000101100010110 
On the left we have provided three consecutive 
characters with spaces between successive charac-
ters. On the right we've removed spaces to show the 
bit stream. Case 1 shows how the SYN is detected. 
This simple scheme is flawed because the end of 
one character plus the start of the next may look like 
a SYN character. Case 2 shows how a spurious SYN 
might be detected. To avoid this problem, two SYN 
characters are transmitted sequentially. If the receiver 
does detect a SYN, it reads the next character. If this 
is also a SYN the start of a message is assumed to have been 
located, otherwise a false synchronization is assumed and the 
search for a valid SYN character continued (case 3). 
A BiSync frame header keeps track of the data by giving it a 
sequence number and providing a means of sequencing and 
acknowledging frames. 
Figure 14.42 describes a hypothetical network consisting 
of six nodes A to F and 10 data links. Suppose you wish to 
route a message from F to C. Some of the available routings 
are as follows. 
F-A-C 
F-A-B-C 
F-A-D-C 
F-E-C 
F-E-D-C 
F-E-D-A-C 
F-E-D-A-B-C 
F-E-B-C 
F-E-B-A-C 
F-E-B-A-D-C 
One of the simplest and least efficient ways of implement-
ing routing involves a crude technique called flooding. When 
a node wishes to send a message, it sends the message on each 
of its links to adjacent nodes. Each node receiving the 
POINT-TO-POINT PROTOCOL (PPP) 
The HDLC link layer level protocol was created long before 
WANs and the Internet became popular.The PPP protocol 
has superseded HDLC in some applications. A PPP frame is 
like an HDLC frame. However, a PPP frame includes a 2-byte 
field that defines the protocol of the data it is transporting. 
The variable-length data field contains the datagram in the 
protocol defined by the protocol field. 
message copies the message to all its outgoing links (apart 
from the link to the node on which the message was received). 
You can now see where the term flooding came from—a 
message is replicated at each junction and it soon becomes a 
flood or avalanche. Because messages multiply at each node, 
you have to provide a means of stopping the process. 
Messages are stamped with a best-before-date and deleted by 
nodes if they exceed it. Although flooding is the simplest 
possible routing strategy it is inefficient because it wastes 
SYN SYN SOH Header 
STX 
Text 
ETX BCC 
50H Address 
Block sequence number 
Control 
Acknowledgement 
STX 
Text 
ETX 

606 
Chapter 14 Computer communications 
FLOW CONTROL MECHANISMS IN RINGS 
A ring network connects all nodes to each other in the form of 
a continuous loop. Unlike the nodes of a bus network that 
listen passively to data on the bus unless it is meant for them, 
the nodes of the ring take on an active part in all data 
transfers. When receiving incoming data, a node must test the 
packet and decide whether to keep it for itself or to pass it on 
to its next neighbor. 
Token rings pass a special bit pattern (the token) round the 
ring from station to station. The station currently holding the 
token can transmit data if it so wishes. If it does not wish to 
take the opportunity to send data itself, it passes the token on 
round the ring. For example, suppose the token has the special 
pattern 11111111, with zero stuffing used to keep the pattern 
unique. A station on the ring wishing to transmit monitors its 
incoming traffic. When it has detected seven 1s it inverts the 
last bit of the token and passes it on. Thus, a pattern called a 
connector (11111110) passes on down the ring.The connector 
Three octets 
Start delimiter 
Access control 
End delimiter 
(a) Token format. 
is created to avoid sending the eighth '1', thereby passing on 
the token.The station holding the token may now transmit its 
data. After it has transmitted its data, it sends a new token 
down the ring. As there is only one token, contention cannot 
arise on the ring unless, of course, a station becomes antisocial 
and sends out a second token. In practice, a practical system is 
rather more complex, because arrangements must be included 
for dealing with lost tokens. 
The IEEE has created a standard for the token ring LAN 
called 802.5. Two types of frame are supported—a three-octet 
frame and a variable-length frame. Each frame begins and ends 
with a starting and ending delimiter, which mark the frame's 
boundaries. The second octet provides access control (i.e. a 
token bit, a monitor bit, and priority bits).The short three-
octet frame format is used to pass the control token round the 
ring from one node to the next.The IEEE 802.5 standard pro-
vides for prioritization. When a station wishes to transmit data 
it waits for a free token whose priority is less than its own. 
1 octet 
1 octet 
1 octet 
1 octet 
2 or 6 octets 2 or 6 octets 0 or 5000 octets 
4 octets 
1 octet 
1 octet 
•1 
••< 
>-4 
•-< 
••< 
••« 
•-« 
X 
*~4 
• - « 
• 
Start 
delimiter 
Access 
control 
End 
delimiter 
Frame 
control 
Destination 
address 
Source 
address 
Data 
CCS 
End 
delimiter 
Frame 
status 
Start of frame 
(b) Frame format. 
- • * -
Body of frame covered by FCS 
-*-«-
End of frame 
Figure 14.42 Cost of routing in a network. 
bandwidth. Flooding is not normally used by today's 
networks. 
Suppose now we apply a cost to each of the routings. This 
cost is a figure-of-merit that might be determined by the 
reliability of a link, its latency (i.e. delay), or its actual cost (it 
might be rented). We have provided a number against each 
link in Fig. 14.42 to indicate its cost. If we now apply these 
costs to the routines, we get the figure shown in Table 14.6. 
Route 
Cost per segment 
Total cost 
F-A-C 
4 + 8 
12 
F-A-B-C 
4 + 9 + 4 
17 
F-A-D-C 
4 + 2 + 3 
9 
F-E-C 
1 + 7 
8 
F-E-D-C 
1 + 4 + 3 
8 
F-E-D-A--C 
1 + 4 + 2 + 8 
15 
F-E-D-A--B-C 
1 + 4 + 2 + 9 + 4 
20 
F-E-B-C 
1 + 2 + 4 
7 
F-E-B-A-C 
1 + 2 + 9 + 8 
20 
F-E-B-A-D-C 
1 + 2 + 9 + 2 + 3 
17 
Table 14.6 The cost of routing a message from node F to C in 
Fig. 14.42. 
Table 14.6 indicates that the cheapest route is F to E to B to C, 
which is slighdy cheaper than the more direct route F to E to C. 
How do you find the cheapest route though the network 
and what happens if the cost of a link changes (if every node 

attempts to use the same link its performance will fall 
and increase its cost)? Much research has been carried 
out into the routing of messages around complex networks. 
Here we can only mention some of the basic concepts 
of routing. 
14.9.1 Centralized routing 
A network with centralized routing used a master station with 
a knowledge of the whole network and the best routes 
between all nodes. The master station broadcasts the routing 
information to the other nodes. Let's see how this applies to 
the example of Fig. 14.42. Table 14.7 provides routing tables 
for nodes A to F. In each case we have calculated the cheapest 
route and the next node. Table 14.8 summarizes the informa-
tion in Table 14.7 and gives the next node for any destination. 
Consider the routing of a message from node A to B. At 
node A the router looks up the next destination for a message 
consigned to B and sends the packet to node F. Node F 
receives this packet and looks up the next node for a packet 
bound for B, which is E. At node E the packet is sent on the 
best route to D which is direct to B. The packet reaches B 
having followed the optimum route A to F to E to B. 
14.9.2 Distributed routing 
Getting a complete knowledge of a complex network is not 
easy. Another way of dealing with routing is to allow each 
node to build up its own database for the rest of the network. 
Initially each node knows only about its immediate neigh-
bors and the cost to reach them. After a time, a node can 
request information from its immediate neighbors about 
their neighbors, and so on. Eventually, a complete picture of 
the network can be constructed. 
The optimum route between any two points in a network 
isn't necessarily constant because the network itself is con-
stantly changing. Nodes are added and removed. Links can be 
broken or become hopelessly congested. Maintaining a fixed 
table of optimum routes (called static routing) is less efficient 
than constantly updating routing information to cope best with 
the current conditions. This strategy is called adaptive routing. 
14.9.3 IP (Internet protocol) 
Although networks were originally developed for highly spe-
cialized applications such as reliable military communications 
systems and academic research tools, it's the Internet that's 
caught people's attention because of its impact on everyday 
life. The Internet began as a development of the US Defense 
Advanced Research Projects Agency, (DARPA) in the 1960s. 
This project created and developed a small experimental net-
work using packet switching called ARPANET (the 'D' for 
defense has been dropped from the acronym). Research into 
the ARPANET was carried out at many universities and this 
network gradually evolved into what we now call the Internet. 
14.9 Routing techniques 
607 
Node A 
NodeB 
Destination Next node Cost 
Destination Next node Cost 
B 
F 
7 
A 
E 
7 
C 
D 
5 
C 
c 
4 
D 
D 
2 
D 
E 
6 
E 
F 
5 
E 
E 
2 
F 
F 
4 
F 
E 
3 
NodeC 
NodeD 
Destination Next node Cost 
Destination Next node Cost 
D 
8 
A 
B 
4 
B 
D 
3 
C 
B 
6 
E 
B 
7 
F 
Node E 
Node F 
Destination Next node Cost 
Destination Next node Cost 
F 
5 
A 
B 
2 
B 
B 
6 
C 
D 
4 
D 
F 
1 
E 
Table 14.7 Routing tables for nodes A to F. 
Source node 
Destination node 
A 
B 
C 
D 
E 
F 
A 
F 
D 
D 
F 
F 
B 
E 
C 
E 
E 
C 
C 
D 
B 
D 
B 
B 
D 
A 
E 
C 
E 
E 
E 
F 
B 
B 
D 
F 
F 
A 
E 
E 
E 
E 
Table 14.8 Routing matrix (next node table). 
The protocol used for ARPANET'S transport layer forms the 
basis of Internet's transmission control protocol (TCP). 
The Internet links together millions of networks and indi-
vidual users. In order to access the Internet, a node must use 
the TCP/IP protocol {transmission control protocol/Internet 
protocol), which corresponds to layers 4 and 3 of the OSI 
reference model, respectively. Some of the higher level 
protocols that make use of TCP/IP are TELNET (a remote 
login service that allows you to access a computer across the 
Internet), FTP (file transfer protocol), which allows you to 
A 
2 
E 
6 
C 
3 
E 
4 
E 
5 
A 
4 
E 
3 
E 
7 
E 
5 
E 
1 
A 
B 
C 
D 
F 
A 
B 
D 
E 
F 

608 
Chapter 14 Computer communications 
Version 
| 
Header length 
| 
Service type 
Datagram length 
Identification 
Flags 
| Fragment offset 
Time to live 
| 
Protocol 
Header checksum 
Source IP address 
Destination iP address 
Options 
| 
Padding 
Data (up to 64K octets total in IP packet) 
Figure 14.43 Structure of the 
IP layer packet. 
exchange files across the Internet, and SMTM (simple mail 
transfer protocol), which provides electronic mail facilities. 
Here we provide only an overview of the TCP/IP layers. 
Internet's network layer protocol, IP, routes a packet 
between nodes in a network. The packets used by the IP are 
datagrams and are handled by appropriate data link layer 
protocols—typically Ethernet protocols on LANs and X.25 
protocols across public data networks (i.e. the telephone sys-
tem). Figure 14.43 describes the format of an IP packet (or 
frame) that is received from the data link layer below it and 
passed to the TCP transport layer above it. 
IP's version field defines the version of the Internet proto-
col that created the current packet. This facility allows room 
for growth because improvements can be added as the state of 
the art improves while still permitting older systems to access 
the network. The IP version widely used in the late 1990s was 
IPv4, and IPv6 was developed to deal with some of the prob-
lems created by the Internet's increasing size and to provide 
for time-critical services such as real-time video and speech. 
The header length defines the size of the header in multi-
ples of 32-bit words (i.e. all fields preceding the data). The 
minimum length is five. Because the header must be a multi-
ple of 32 bits, IP's padding field is used to supply 0 to 3 octets 
to force the header to fit a 32-bit boundary. The datagram 
length is a 16-bit value that specifies the length of the entire IP 
packet, which limits the maximum size of a packet to 64K 
octets. In practice, typical IP packets are below 1 kbyte. 
The service type field tells the transport layer how the 
packet is to be handled; that is, priority, delay, throughput, 
and reliability. The service request allows the transport layer 
to choose between, for example, a link with a low delay or a 
link that is known to be highly reliable. 
The flags and fragment offset fields are used to deal with 
fragmentation. Suppose a higher level layer uses larger packets 
than the IP layer. A packet has to be split up (i.e. fragmented) 
and transmitted in chunks by the IP. The fragmentation flags 
indicate that an IP packet is part of a larger unit that has to be 
re-assembled and the fragment offset indicates where the 
current fragment fits (remember that IP packets can be 
received out of order). 
The time-to-live field corresponds to the packet's best-
before date and is used to specify the longest time that the 
packet can remain on the Internet. When a packet is created, 
it is given a finite life. Each time the packet passes a node, the 
time-to-live count is decremented. If the count reaches zero, 
the packet is discarded. This facility prevents packets circulat-
ing round the Internet endlessly. 
The protocol field specifies the higher level protocol that is 
using the current packet; for example, the TCP protocol has 
the value 6. This facility enables the destination node to pass 
the IP packet to the appropriate service. 
The header checksum detects errors in the header. Error 
checking in the data is performed by a higher level protocol. The 
checksum is the one's complement of the sum of all 16-bit inte-
gers in the header. When a packet is received the checksum is 
calculated and compared with the transmitted value. A check-
sum is a very crude means of providing error protection (it's not 
in the same league as the FCS) but it is very fast to compute. 
The source and destination IP address fields provide the 
address of where the packet is coming from and where it's 
going. We will return to IP addressing later. The options field 
is optional and allows the packet to request certain facilities. 
For example, you can request that the packet's route through 
the Internet be recorded or you can request a particular route 
though the network. Finally, the data field contains the infor-
mation required by the next highest protocol. 
IP routing 
Both the IP source and destination addresses are 32 bits in 
version 4 of the Internet protocol. Version 6 will provide 128-
bit addresses (that's probably enough to give each of the 
Earth's molecules its own Internet address). 
An IPv4 address is unique and permits 232 (over 4000 mil-
lion) different addresses. When specifying an Internet 
address it's usual to divide the 32 bits into four 8-bit fields and 
convert each 8-bit field into a decimal number delimited by a 
period; for example, the IP address 11000111 10000000 
01100000 00000000 corresponds to 199.128.96.0. 
Although an IP address provides 232 unique values, it 
doesn't allow up to 4000 million nodes (or users) to exist on 
the Internet, because not all addresses are available. An IP 
address is a hierarchical structure designed to facilitate the 
routing of a packet through the Internet and is divided into 
four categories as Fig. 14.44 demonstrates. 
Internet addresses have two fields—a network address and 
a node address. Class A Internet protocol addresses use a 7-bit 
network identifier and then divide each network into 224 
different nodes. Class B addresses can access one of 214 = 16K 
networks each with 64K nodes, and class C addresses select 
one of 212 = 4096 networks with 254 nodes. 

14.9 Routing techniques 
609 
0 
8 
31 
[Class A | 0 | Net ID 
| 
Host ID 
| 
0 
1 
16 
| Class B | 1 |0 | 
Net ID 
| 
Host ID 
| 
0 
1 
2 
24 
| Class C11 | 1 10 | 
Net ID 
| 
Host ID 
| 
0 
1 
2 
3 
Class D| 1 | 1 | 1 |o | 
Multicast address 
| 
0 
1 2 
3 
4 
Class E | 1 | 1 | 1 h 10 | Reserved for future use 
| 
Figure 14.44 Structure of an 
IP address. 
Source port 
| 
Destination port 
Sequence number 
Acknowledgement number 
Data offset 
Reserved 
I 
Flags 
Window 
Checksum 
Urgent pointer 
Options 
| 
Padding 
Data 
You can easily see how inefficient this arrangement is. 
Although only 128 networks can use a class A address, each 
network gets 16 million node addresses whether they are 
needed or not. Class A and B addresses have long since been 
allocated (removing large numbers of unique addresses from 
the pool). This leaves only a rapidly diminishing pool of class C 
addresses (until the IPv6 protocol becomes more widely used). 
The end user doesn't directly make use of a numeric 
Internet address. Logical Internet addresses are written in the 
form user@host.department.institution.domain. The way in 
which these logical addresses are mapped onto physical 
addresses is beyond the scope of this chapter. 
Transmission control protocol 
TCP performs a level-4 transport layer function by interfacing 
to the user and host's applications processes at each end of the 
net. The TCP is rather like an operating system because it carries 
out functions such as opening, maintaining, and closing the 
channel. The TCP takes data from the user at one end of the net 
and hands it to the IP layer below for transmission. At the other 
end of the net, the TCP takes data from the IP layer and passes it 
to the user. Figure 14.45 describes the transport header. 
The source and destination port addresses provide 
application addresses. Each node (host) might have several 
application programs running on it and each application is 
associated with a port. This means you can run several appli-
cations, each using the Internet, on a computer at any instant. 
The sequence number ensures that messages can be assem-
bled in sequence because it contains the byte number of 
the first byte in the data. The acknowledgement number 
indicates the byte sequence number the receiving TCP node 
expects to receive and, therefore, acknowledges the receipt of 
all previous bytes. This arrangement is analogous to the 
HDLC protocol used by layer two protocols. 
The offset defines the size of the TCP header and, there-
fore, the start of the data field. The flags field contains 6 bits 
Figure 14.45 Structure of TCP 
header. 
that control the operation of the TCP; for example, by indi-
cating the last data segment or by breaking the link. The win-
dow field tells the receiving node how many data bytes the 
sending node can accept in return. The checksum provides 
basic error correction for the transport layer. The options 
field defines TCP options. The padding field ensures that the 
header fits into a 32-bit boundary. 
The urgent pointer field is used in conjunction with the 
URG flag bit. If the URG bit is set, the urgent pointer provides 
a 16-bit offset from the sequence number in the current TCP 
header. This provides the sequence number of the last byte in 
urgent data (a facility used to provide a sort of interrupt facil-
ity across the Internet). The host receiving a message with its 
URG bit set should pass it to the higher layers ahead of any 
currently buffered data. 
Although the TCP protocol forms the backbone of the 
Internet, it is rather old and has its origin in the days of the 
ARPANET. In particular, the TCP's error-detecting checksum 
is almost worthless because it isn't as powerful as the data link 
layer's FCS error-detecting mechanism. TCP plus IP headers 
are 40 bytes or more and these add a significant overhead to 
short data segments. 
• 
SUMMARY 
In this chapter we have provided an overview of some of the 
aspects of interest to those involved with computer communica-
tions networks. Computer networks is a subject that is advancing 
as rapidly as any other branch of computer science, because it 
increases the power of computer systems and exploits many of 
today's growing technologies. It is all too easy to think of com-
puter communications as a hardware-oriented discipline cen-
tered almost exclusively on the transmission of signals from 
point A to point B. Modern computer communications networks 
have software components that far outweigh their hardware 
components in terms of complexity and sometimes even cost. In 
this chapter we have introduced the ideas behind the seven 

610 
Chapter 14 Computer communications 
layers of the ISO basic reference model for open systems intercon-
nection and have described protocols for the bottom two layers. 
m PROBLEMS 
14.1 If the cost of a computer and all its peripherals is so low 
today, why is the field of computer communications expanding 
so rapidly? 
14.2 What is the meaning of a protocol and why are protocols 
so important in the world of communications? 
14.3 What is the difference between a WAN and a LAN? 
14.4 What is an open system? 
14.5 Why has the ISO model for OSI proved so important in 
the development of computer communications? 
14.6 What are the differences between the transport and net-
work layers of the ISO reference model? 
14.7 Why is the physical layer of the OSI model different from 
all the other layers? 
14.8 What is a virtual connection? 
14.9 What are the differences between half-duplex and full-
duplex transmission modes? How is it possible to make a half-
duplex system look like a full-duplex system? 
14.10 What is the difference between phase and frequency 
modulation? 
14.11 What are the types of noise that affect a data link? Which 
types of noise are artificial and which are natural? If you were 
comparing a satellite link and a telephone link, what do you think 
are the effect, type, and consequences of noise on each link? 
14.12 What determines the maximum rate at which informa-
tion can be transmitted over a data link? 
14.13 Why cannot users transmit any type of signal they wish 
(i.e. amplitude, frequency characteristics) over the PSTN? 
14.14 What is the difference between DTE and DCE? 
14.15 What are the advantages and disadvantages of the fol-
lowing communications media: fibre optic link, twisted pair, and 
satellite link? 
14.16 Why is a SYN character required by character-oriented 
data link, and why is a SYN character not required by a 
bit-oriented data link? 
14.17 What is bit stuffing and how is it used to ensure 
transparency? 
14.18 What are the advantages and disadvantages of LANs 
based on the ring and bus topologies? 
14.19 What is the meaning of CSMA/CS in the context of a 
mechanism for handling collisions on a LAN? 
14.20 The maximum range of a line-of-sight microwave link, d, 
is given by the formula cf = 2rh+ tf, where ris the radius of 
the Earth and h is the height of the antenna above the Earth's 
surface.This formula assumes that one antenna is at surface 
level and the other at height h. Show that this formula is correct 
Hint: it's a simple matter of trigonometry. 
14.21 For each of the following bit rates determine the period 
of 1 bit in the units stated. 
Figure 14.46 Routing in a network. 
Bit rate 
Unit 
(a) 100 bps 
ms 
(b) 1 kbps 
ms 
(c) 56 kbps 
p.s 
(d) 100 Mbps 
ns 
14.22 Each of the following time values represents 1 bit. For each 
value give the corresponding bit rate expressed in the units stated. 
Duration 
Unit of bit rate 
(a) 1 s 
bps 
(b) 10p.s 
kbps 
(c) 10p.s 
Mbps 
(d) 15 ns 
Cbps 
14.23 For each of the following systems calculate the bit rate. 
(a) 300 baud 
2-level signal 
(b) 600 baud 
4-level signal 
(c) 9600 baud 
256-level signal 
14.24 The ISO reference model has seven layers. Is that too 
many, too few, or just right? 
14.25 Define an open system and provide three examples of 
open systems? 
14.26 What are the relative advantages and disadvantages of 
satellite links in comparison with fiber optic cables? 
14.17 If a signal has a signal-to-noise ratio of 50 dB and the 
power of the signal is 1 mW, what is the power of the noise 
component? 
14.28 For the network of Fig. 14.46 calculate the lowest cost 
route between any pairs of nodes. 
14.29 Suppose the network of Fig. 14.46 used flooding to route 
its packets. Show what would happen if a packet were to be sent 
from node F to node C. 
14.30 A network has a bandwidth of 3400 Hz and a signal-to-
noise ratio of 40 dB.What is the maximum theoretical data rate 
that the channel can support? 
14.31 Shannon's work on the capacity of a channel relates to 
so-called white Gaussian noise (e.g. thermal noise). Many tele-
phone channels suffer from impulse noise (switching transients 
that appear as clicks). Do you think that (for the same noise 
power) such a channel would have a better information-carrying 
capacity than predicted by Shannon? 
14.32 Why is a checksum error detector so much worse than a 
cyclic redundancy code? 
9 
B 
•A 
4 
5 
6 
5 
T. 
7 
2j 
3 
8' 
4 
D 
E 
'O 

APPENDIX: THE 68000 
INSTRUCTION SET 
This appendix provides details of the 68000's most important 
instructions (we have omitted some of the instructions that 
are not relevant to this book). 
In each case, we have given the definition and assembly 
language format of the instruction. We have also provided its 
size (byte, word, or longword) and the addressing modes it 
takes for both source and destination operands. 
Finally, we have included the effect of the instruction on 
the 68000's condition code register. Each instruction either 
sets/clears a flag bit, leaves it unchanged, or has an 'undefined' 
effect, which is indicated by the symbols *, -, and U, respec-
tively. A 0 in the CCR indicates that the corresponding bit is 
always cleared. 
ADD 
Operation: 
Syntax: 
Attributes: 
Description: 
Add binary 
[destination] <— [source] + [destination] 
ADD <ea>, Dn 
ADD Dn, <ea> 
Size=byte, word, longword 
Add the source operand to the destination operand and store the result in the destination 
location. 
Condition codes: 
Source operand addressing modes 
X N Z V C 
* + * * * 
Destination operand addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) 
(iM) (dJkS) ABS.W 
ABU m (trcjb) imm 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
modes 
Dn 
An 
(An) 
(An)+ -(An) 
(4An) [iMJH A6S.W 
ABS.L m (d,rUn) imm 
• 
• 
• 
• 
• 
• 
• 
• 
ADDA 
Operation: 
Syntax: 
Attributes: 
Description: 
Add address 
[destination] <—[source] + [destination] 
ADDA <ea>,An 
Size=word, longword 
Add the source operand to the destination address register and store the result in the destination 
address register. The source is sign-extended before it is added to the destination; e.g. if we 
execute ADDA.w D3, A4 where A4=0000010016 and D3.W=80021<;, the contents of D3 are first 

612 
Appendix: The 68000 instruction set 
Application: 
Condition codes: 
sign-extended to FFFF800216 and added to 00000100,6 to give FFFF8102]6> which is 
stored in A4. 
To add to the contents of an address register and without updating the CCR. Note that 
ADDA.W DO, A0 is the same as LEA (AO.DO.W) ,AO. 
X N Z V C 
An ADDA operation does not affect the state of the CCR. 
Source operand addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) m (d-AnJU) ABS.W 
ABS.L 
(<!.*) 
(d,PUtn) 
imm 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
ADDI 
Operation: 
Syntax: 
Attributes: 
Description: 
Condition codes: 
Add immediate 
[destination] <—<literal> + [destination] 
ADDI # < d a t a > , < e a > 
Size=byte, word, longword 
Add immediate data to the destination operand. Store the result in the destination operand. 
ADDI can be used to add a literal directly to a memory location. For example, 
ADDi.w #$123 4, $2000 has the effect [200016] <-[2000I6] + 123416. 
X N Z V C 
* * * * * 
Destination operand addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) 
(o\An) (iMJH) ABS.W 
ABS.L 
(d.PQ (d.PC,Xn) imm 
• 
• 
• 
• 
• 
• 
• 
• 
ADDQ 
Operation: 
Syntax: 
Sample syntax: 
Attributes: 
Description: 
Application: 
Condition codes: 
Add quick 
[destination] <—<literal>+ [destination] 
ADDQ #<data>,<ea> 
ADDQ #6,D3 
Size=byte, word, longword 
Add the immediate data to the contents of the destination operand. The immediate data must be 
in the range 1 to 8. Word and longword operations on address registers do not affect condition 
codes and a word operation on an address register affects all bits of the register. 
ADDQ is used to add a small constant to die operand at the effective address. Some assemblers per-
mit you to write ADD and then choose ADDQ automatically if the constant is in the range 1 to 8. 
X N Z V C 
* * * * 
The CCR is not updated if the destination operand is an address register. 
Destination operand addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) 
(d» (AMJb) ABS.W 
ABS.L m (d.PUn) 
imm 
• 
• 
• 
• 
• 
• 
• 
• 
• 

Appendix: The 68000 instruction set 
613 
ADDX 
Add extended 
Operation: 
Syntax: 
Attributes: 
Description: 
Application: 
LOOP 
[destination] <— [source] + [destination] + [X] 
ADDX Dy,Dx 
ADDX -(Ay) ,-(Ax) 
Size=byte, word, longword 
Add the source operand to the destination operand along with the extend bit, and store the result 
in the destination location. The only legal addressing modes are data register direct and memory 
to memory with address register indirect using predecrementing. 
The ADDX instruction is used in chain arithmetic to add together strings of bytes (words or 
longwords). Consider the addition of two 128-bit numbers, each of which is stored as four 
consecutive longwords. 
AO points at the first number 
Al points at the second number 
Four longwords to add 
Clear the X-bit and Z-bit of the CCR 
Add a p a i r of numbers 
Repeat u n t i l a l l added 
Condition codes: 
LEA 
N u m b e r l , A O 
LEA 
N u m b e r 2 , A l 
MOVE 
# 3 , DO 
MOVE 
# $ 0 0 , C C R 
ADDX 
- ( A 0 ) , - ( A 1 ) 
DBRA 
DO,LOOP 
X N Z V C 
The Z-bit is cleared if the result is non-zero, and left unchanged otherwise. The Z-bit can be used 
to test for zero after a chain of multiple precision operations. 
AND 
Operation: 
Syntax: 
Attributes: 
Description: 
Application: 
Condition codes: 
AND logical 
[destination] <—[source] • [destination] 
AND <ea>,Dn 
AND Dn,<ea> 
Size=byte, word, longword 
AND the source operand to the destination operand and store the result in the destination 
location. 
AND is used to mask bits. If you wish to clear bits 3 to 6 of data register D7, you can execute 
AND 
#%10000111,D7. Unfortunately, the AND operation cannot be used with an address 
register as either a source or a destination operand. If you wish to perform a logical operation 
on an address register, you have to copy the address to a data register and then perform the 
operation there. 
X N Z V C 
- * * 0 0 
Source operand addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) 
(M (dMK) ABS.W 
ABS.L (WQ (UCJta) imm 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
id addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) m (dJWi) ABS.W 
ABU m (dLPCJh) imm 
• 
• 
• 
• 
• 
• 
• 
• 
Destination operand addressing modes 

614 
Appendix: The 68000 instruction set 
ANDI 
AND immediate 
Operation: 
[destination]*— < l i t e r a l > - [destination] 
Syntax: 
ANDI # < d a t a > , < e a > 
Attributes: 
Size=byte, word, longword 
Description: 
AND the immediate data to the destination operand. The ANDI instruction permits a literal 
operand to be ANDed with a destination other than a data register. For example, 
ANDI #$FE00, $1234 or ANDI. B #$F0,(A2)+. 
Condition codes: 
x N z v c 
- * * 0 0 
Destination operand addressing modes 
On 
An 
(An) 
(An)+ 
-(An) 
(<Un) (dAiJti) ABS.W 
ABU m (d,PCXn) imm 
• 
• 
• 
• 
• 
• 
• 
• 
AN Dl to CCR 
AND immediate to CCR 
Operation: 
[CCR]<-<data>- [CCR] 
Syntax: 
ANDI #<data>,ccR 
Attributes: 
Size=byte 
Description: 
AND the immediate data to the condition code register (i.e. the least-significant byte of the 
status register). 
Application: 
ANDI is used to clear selected bits of the CCR. For example, ANDI 
#$FA,CCR clears the 
Z- and C-bits, i.e. XNZVC=X NOVO. 
Condition codes: 
x N z v c 
* * * * * 
X: cleared if bit 4 of data is zero 
N: cleared if bit 3 of data is zero 
Z: cleared if bit 2 of data is zero 
V: cleared if bit 1 of data is zero 
C: cleared if bit 0 of data is zero 
ANDI to SR 
AND immediate to status register 
Operation: 
I F [ s ] = l 
THEN 
[ S R ] < - < l i t e r a l > - [SR] 
ELSE TRAP 
Syntax: 
ANDI #<data>,SR 
Attributes: 
Size=word 
Description: 
AND the immediate data to the status register and store the result in the status register. All bits 
of the SR are affected. 
Application: 
This instruction is used to clear the interrupt mask, the S-bit, and the T-bit of the SR. 
ANDI #<data>,SR affects both the status byte of the SR and the CCR. For example, 
ANDI # $ 7 F F F , S R clears thetrace bit ofthe status register, whereasANDi # $ 7 F F E , SR clears the 
trace bit and also clears the carry bit ofthe CCR. 
Condition codes: 
x N z v c 
* * * * * 

Appendix: The 68000 instruction set 
615 
ASL.ASR 
Operation: 
Syntax: 
Attributes: 
Description: 
Arithmetic shift left/right 
[destination] <— [destination] shifted by < c o u n t > 
ASL Dx,Dy 
ASR Dx,I)y 
ASL #<data>,Dy 
ASR #<data>,Dy 
ASL < e a > 
ASR < e a > 
Size=byte, word, longword 
Arithmetically shift the bits of the operand in the specified direction (i.e. left or right). The shift 
count may be specified in one of three ways. The count may be a literal, the contents of a data 
register, or the value 1. An immediate (i.e. literal) count permits a shift of 1 to 8 places. If the 
count is in a register, the value is modulo 64 (i.e. 0 to 63). If no count is specified, one shift 
is made (i.e. ASL < e a > shifts the contents of the word at the effective address one place left). 
An arithmetic shift left shifts a zero into the least-significant bit position and shifts the most-
significant bit out into both the X- and the C-bits of the CCR. The overflow bit of the CCR is set if 
a sign change occurs during shifting (i.e. if the most-significant bit changes value during shifting). 
The effect of an arithmetic shift right is to shift the least-significant bit into both the X- and 
C-bits of the CCR. The most-significant bit (i.e. the sign bit) is replicated to preserve the sign 
of the number. 
ASL 
c 
S 
r 
Operand 
^ 
c* 
c 
K. 
•• 
" 
i 
Operand 
^ 
^ 
X 
S 
X 
*» 
ASR 
^» MSB Operand 
l > 
C 
s MSB Operand 
l > 
> 
C 
>» X 
s 
X 
Application: 
Condition codes: 
ASL multiplies a two's complement number by 2. ASL is almost identical to the corresponding 
logical shift, LSR. The only difference between ASL and LSL is that ASL sets the V-bit of the CCR 
if overflow occurs, whereas LSL clears the V-bit to zero. An ASR divides a two's complement 
number by 2. When applied to the contents of a memory location, all 68000 shift operations 
operate on a word. 
X N Z V C 
The X-bit and the C-bit are set according to the last bit shifted out of the operand. If the shift 
count is zero, the C-bit is cleared. The V-bit is set if the most-significant bit is changed at any 
time during the shift operation and cleared otherwise. 
Destination operand addressing modes 
On 
An 
(An) 
(An)+ -<*") 
(Un) (<UMi) ABS.W 
ABU (WQ (d.POn) 
iimn 
• 
• 
• 
• 
• 
• 
• 
• 

616 
Appendix: The 68000 instruction set 
Bcc 
Branch on condition cc 
Operation: 
Syntax: 
Sample syntax: 
Attributes: 
Description: 
Condition codes: 
If cc = l THEN [PC]<-[PC]+d 
Bcc 
< l a b e l > 
BEQ Loop_4 
BVC * + 8 
BEQ takes an 8-bit or a 16-bit offset (i.e. displacement). 
If the specified logical condition is met, program execution continues at location [PC] + dis-
placement, d. The displacement is a two's complement value. The value in the PC corresponds 
to the current location plus two. The range of the branch is —126 to +128 bytes with an 8-bit 
offset, and —32 kbyte to +32 kbyte with a 16-bit offset. A short branch to the next instruction is 
impossible, since the branch code 0 indicates a long branch with a 16-bit offset. 
BCC 
branch on carry clear 
C 
BCS 
branch on carry set 
C 
BEQ 
branch on equal 
Z 
BGE 
branch on greater than or equal 
N • V+ N • V 
BGT 
branch on greater than 
N-V-Z+N-V-Z 
BHI 
branch on higher than 
C-Z 
BLE 
branch on less than or equal 
Z+N-V+N-V 
BLS 
branch on lower than or same 
C+Z 
BLT 
branch on less than 
N-V+N-V 
BMI 
branch on minus (i.e. negative) 
N 
BNE 
branch on not equal 
Z 
BPL 
branch on plus (i.e. positive) 
N 
BVC 
branch on overflow clear 
V 
BVS 
branch on overflow set 
V 
There are two types of conditional branch instruction: those that branch on an unsigned 
condition and those that branch on a signed condition; e.g. $FF is greater than $10 when the 
numbers are unsigned (i.e. 255 is greater than 16). However, if the numbers are signed, $FF is 
less than $10 (i.e. — 1 is less than 16). 
The signed comparisons are: 
BGE 
branch on greater than or equal 
BGT 
branch on greater than 
BLE 
branch on lower than or equal 
BLT 
branch on less than 
The unsigned comparisons are: 
BHS 
BCC 
branch on higher than or same 
BHI 
branch on higher than 
BLS 
branch on lower than or same 
BLO 
BCS 
branch on less than 
The official mnemonics BCC (branch on carry clear) and BCS (branch on carry set) can be 
renamed as BHS (branch on higher than or same) and BLO (branch on less than), respectively. 
Many 68000 assemblers support these alternative mnemonics. 
X N Z V C 
BCLR 
Operation: 
Syntax: 
Test a bit and clear 
[Z]<— < bit number> OF [destination] 
<bit number> OF [destination]<—0 
BCLR Dn, <ea> 
BCLR # < d a t a > , < e a > 

Appendix: The 68000 instruction set 
617 
Attributes: 
Description: 
Application: 
Condition codes: 
Size=byte, longword 
A bit in the destination operand is tested and the state of the specified bit is reflected in the condi-
tion of the Z-bit in the condition code. After the test, the state of the specified bit is cleared in the 
destination. If a data register is the destination, the bit numbering is modulo 32, allowing bit manip-
ulation of all bits in a data register. If a memory location is the destination, a byte is read from that 
location, the bit operation performed using the bit number modulo 8, and the byte written back to 
the location. Bit zero refers to the least-significant bit. The bit number for this operation may be 
specified either by an immediate value or dynamically by the contents of a data register. 
Suppose that the contents of memory location $1234 are 111110102, and the operation 
BCLR #4 , $1234 is carried out. This instruction tests bit 4. It is a 1 and therefore the Z-bit of 
the CCR is set to 0. Bit 4 of the destination operand is cleared and the new contents of $ 1234 are: 
111010102. 
X N Z V C 
Z: set i f the bit tested is zero, cleared otherwise. 
Destination operand addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) 
(ita) (dJlnJU) ABS.W 
ABU m (d.PCXn) 
itnm 
• 
• 
• 
• 
• 
• 
• 
• 
Data register direct addressing, Dn, uses a longword operand. Other modes use a byte operand. 
BRA 
Operation: 
Syntax: 
Attributes: 
Description: 
Application: 
Condition codes: 
Branch always 
[PC]<-[PC] +d 
BRA < l a b e l > 
BRA < l i t e r a l > 
Size=byte, word 
Program execution continues at location [PC] +d. The displacement, d, is a two's complement 
value (8 bits for a short branch and 16 bits for a long branch). The value in the PC corresponds 
to the current location plus two. A short branch to the next instruction is impossible, since the 
branch code 0 is used to indicate a long branch with a 16-bit offset. 
A BRA is an unconditional relative jump (or goto). You use a BRA instruction to write position 
independent code, because the destination address (branch target address) is specified with 
respect to the current value of the PC. A JMP instruction does not produce position-independent 
code. 
X N Z V C 
BSET 
Operation: 
Syntax: 
Attributes: 
Description: 
Test a bit and set 
[Z]<—<bit number> OF 
[ d e s t i n a t i o n ] 
< b i t number> OF [ d e s t i n a t i o n ] < — 0 
BSET Dn, < e a > 
BSET # < d a t a > , < e a > 
Size=byte, longword 
A bit in the destination operand is tested and the state of the specified bit is reflected in the con-
dition of the Z-bit of the condition code. After the test, the specified bit is set in the destination. 
If a data register is the destination then the bit numbering is modulo 32, allowing bit manipula-
tion of all bits in a data register. If a memory location is the destination, a byte is read from that 
location, the bit operation performed using bit number modulo 8, and the byte written back to 
the location. Bit zero refers to the least-significant bit. The bit number for this operation may be 
specified either by an immediate value or dynamically by the contents of a data register. 

618 
Appendix: The 68000 instruction set 
Condition codes: 
X N Z V C 
Z: set if the bit tested is zero, cleared otherwise. 
Destination operand addressing mode for BSET Dn, < e a > form 
Dn 
An 
(An) 
(An)+ 
-(An) 
(<Un) (dM») ABS.W 
ABU 
(<).*) 
(d.PC.Xn) imm 
• 
• 
• 
• 
• 
• 
• 
• 
BSR 
Operation: 
Syntax: 
Attributes: 
Description: 
Applicaton: 
Condition codes: 
Branch to subroutine 
[ S P ] < - [ S P ] - 4 ; 
[ [SP]]<-[PC] ; 
[PC]<-[PC]+d 
BSR < l a b e l > 
BSR < l i t e r a l > 
Size=byte, word 
The longword address of the instruction immediately following the BSR instruction is pushed 
on to the system stack pointed at by A7. Program execution then continues at location [PC] + 
displacement, d. The displacement is an 8-bit two's complement value for a short branch, or a 
16-bit two's complement value for a long branch. The value in the PC corresponds to the cur-
rent location plus two. Note that a short branch to the next instruction is impossible, since the 
branch code 0 is used to indicate a long branch with a 16-bit offset. 
BSR is used to call a procedure or a subroutine. It provides relative addressing (and therefore 
position-independent code) and its use is preferable to JSR. 
X N Z V C 
BTST 
Operation: 
Syntax: 
Attributes: 
Description: 
Test a bit 
Condition codes: 
[ Z ] < - < b i t number> OF 
[ d e s t i n a t i o n ] 
BTST D n , < e a > 
BTST # < d a t a > , < e a > 
Size=byte, longword 
A bit in the destination operand is tested and the state of the specified bit is reflected in the con-
dition of the Z-bit in the CCR. The destination is not modified by a BTST instruction. If a data 
register is the destination, then the bit numbering is modulo 32, allowing bit manipulation of all 
bits in a data register. If a memory location is the destination, a byte is read from that location 
and the bit operation performed. Bit 0 refers to the least-significant bit. The bit number for this 
operation may be specified either statically by an immediate value or dynamically by the con-
tents of a data register. 
X N Z V C 
Z: set if the bit tested is zero, cleared otherwise. 
Destination operand addressing modes for BTST D n , < e a > 
form 
Dn 
An 
(An) 
(An)+ 
-(An) m (<UWi) ABS.W 
ABU m (d.PCXn) 
imm 
• 
• 
• 
• 
• 
• 
• 
• 

Appendix: The 68000 instruction set 
619 
CLR 
Clear an operand 
Operation: 
[ d e s t i n a t i o n ] <-0 
Syntax: 
CLR < e a > 
Sample syntax: 
CLR (A4) + 
Attributes: 
Size=byte, word, longword 
Description: 
The destination is cleared by loading with all zeros. The CLR instruction can't be used to clear 
an address register. You can use SUBA. L A0 , A0 to clear A0. 
Condition codes: 
X N z V c 
- 0 1 0 0 
Source operand addressing modes 
Dn 
An 
(An) 
(An)+ "(An) m (dJWi) AK.W 
AK.I m (Win) imm 
• 
• • 
• 
• 
• 
• 
• 
CMP 
Compare 
Operation: 
[ d e s t i n a t i o n ] - [ s o u r c e ] 
Syntax: 
CMP < e a > , D n 
Sample syntax: 
CMP ( T e s t , A6,D3 .W) ,D2 
Attributes: 
Size=byte, word, longword 
Description: 
Subtract the source operand from the destination operand and set the condition codes accord-
ingly. The destination must be a data register. The destination is not modified by this 
instruction. 
Condition codes: 
X N z V C 
* * * * 
Source operand addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) 
(**•) (<UkXi) ABS.W 
ABU 
(<".K) (d.PCXn) 
imm 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
C M PA 
Compare address 
Operation: 
[ d e s t i n a t i o n ] - [ s o u r c e ] 
Syntax: 
CMPA 
< e a > , An 
Sample syntax: 
CMPA.L 
#$1000, A4 
CMPA.W 
(A2)+,A6 
CMPA.L 
D5,A2 
Attributes: 
Size=word, longword 
Description: 
Subtract the source operand from the destination address register and set the condition codes 
accordingly. The address register is not modified. The size of the operation may be specified as 
word or longword. Word length operands are sign-extended to 32 bits before the comparison is 
carried out. 
Condition codes: 
X N z V C 
- 
* 
+ 
* 
* 
Source operand addressing modes 
Dn 
An 
(An) 
(to)+ 
-(An) 
(Ita) 
(<UWG) ABS.W 
ABU 
(4K) (d.PCXn) imm 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 

620 
Appendix: The 68000 instruction set 
CMPI 
Operation: 
Syntax: 
Attributes: 
Description: 
Condition codes: 
Compare immediate 
[ d e s t i n a t i o n ] - < i m m e d i a t e 
d a t a > 
CMPI # < d a t a > , < e a > 
Size=byte, word, longword 
Subtract the immediate data from the destination operand and set the condition codes accord-
ingly—the destination is not modified. CMPI permits the comparison of a literal with memory. 
X N Z V C 
Destination operand addressing modes 
Dn 
An 
(An) 
(*.)+ -(An) 
(i««) (Wi) ABS.W 
A6S.L 
(1.PC) (d.PUn) 
imm 
• 
• 
• 
• 
• 
• 
• 
• 
CMPM 
Operation: 
Syntax: 
Attributes: 
Sample syntax: 
Description: 
Application: 
Compare memory with memory 
[ d e s t i n a t i o n ] - [ s o u r c e ] 
CMPM (Ay) + , (Ax) + 
Size=byte, word, longword 
CMPM.B (A3)+, (A4) + 
Subtract the source operand from the destination operand and set the condition codes accord-
ingly. The destination is not modified by this instruction. The only permitted addressing mode is 
address register indirect with postincrementing for both source and destination operands. 
Used to compare the contents of two blocks of memory. For example: 
Compare two blocks of memory for equality 
AO points to source block 
Al points to destination block 
Compare Count words 
Compare pair of words 
Repeat until all done 
LEA 
Source,AO 
LEA 
D e s t i n a t i o n , A l 
MOVE.W 
# C o u n t - l , D 0 
RPT 
CMPM.W 
(A0)+, (Al) + 
DBNE 
DO,RPT 
Condition codes: 
X N Z V C 
DBcc 
Operation: 
Syntax: 
Attributes: 
Description: 
Test condition, decrement, and branch 
I F ( c o n d i t i o n 
f a l s e ) 
THEN [ D n ] < - [ D n ] - l 
{decrement loop c o u n t e r } 
IF [ D n ] = - 1 THEN [PC] <-[PC]+2 { f a l l through t o n e x t 
i n s t r u c t i o n } 
ELSE [PC] <- [PC]+d { t a k e b r a n c h } 
ELSE [PC]<-[PC]+2 
{ f a l l t h r o u g h t o n e x t 
i n s t r u c t i o n } 
DBcc D n , < l a b e l > 
Size=word 
The DBcc instruction provides an automatic looping facility. The DBcc instruction requires 
three parameters: a branch condition (specified by 'cc), a data register that serves as the loop 
down-counter, and a label that indicates the start of the loop. The DBcc first tests the condition 
'cc', and if'cc' is true the loop is terminated and the branch back to < l a b e l > not taken. The 14 
branch conditions supported by Bcc are also supported by DBcc, as well as DBF and DBT 

Appendix: The 68000 instruction set 
621 
LOOP 
(F—false, and T=true). Many assemblers permit the mnemonic DBF to be expressed as DBRA (i.e. 
decrement and branch back). 
The condition tested by the DBcc instruction works in the opposite sense to a Bcc. For exam-
ple, BCC means branch on carry clear, whereas DBcc means continue (i.e. exit the loop) on carry 
clear. That is, the DBc c condition is a loop terminator. If the termination condition is not true, the 
low-order 16 bits of the specified data register are decremented. If the result is - 1 , the loop is not 
taken and the next instruction is executed. If the result is not — 1, a branch is made to 'label'. The 
label is a 16-bit signed value, permitting a branch range of - 32 to +32 kbyte. The loop may be exe-
cuted up to 64K times. 
We can use the instruction DBEQ, decrement and branch on zero, to mechanize the high-level 
language construct REPEAT . . .UNTIL. 
REPEAT 
[DO] : = [D0]-1 
DBEQ 
DO,REPEAT 
UNTIL [DO]=-l OR [Z]=l 
Application: 
Suppose we wish to input a block of 512 bytes of data (the data is returned in register Dl). If the 
input routine returns a value zero in Dl, an error has occurred and the loop must be exited. 
Set up a pointer to the data destination 
512 bytes to be input 
Get a data value in Dl 
Store it 
REPEAT until D1=0 OR 512 times 
LEA 
D e s t , A 0 
MOVE.W 
#511,DO 
AGAIN 
BSR 
INPUT 
MOVE.B 
D l , (A0) + 
DBEQ 
DO,AGAIN 
lition codes: 
X N Z V C 
Not affected. 
DIVS, DIVU 
Operation: 
Syntax: 
Attributes: 
Description: 
Application: 
Condition codes: 
Signed divide, unsigned divide 
[ d e s t i n a t i o n ] <— [ d e s t i n a t i o n ] / [ s o u r c e ] 
DIVS < e a > , D n 
DIVU < e a > , D n 
Size=a longword is divided by a word to give a longword result quotient and remainder. 
Divide the destination operand by the source operand and store the result in the destination. 
The destination is a longword and the source is a 16-bit value. The result (i.e. destination regis-
ter) is a 32-bit value arranged so that the quotient is the lower-order word and the remainder is 
the upper-order word. DIVU performs division on unsigned values and DIVS performs divi-
sion on two's complement values. An attempt to divide by zero causes an exception. For DIVS, 
the sign of the remainder is always the same as the sign of the dividend (unless the remainder is 
zero). 
Attempting to divide a number by zero results in a divide-by-zero exception. If overflow is 
detected during division, the operands are unaffected. Overflow is checked for at the start of 
the operation and occurs if the quotient is larger than a 16-bit signed integer. If the upper 
word of the dividend is greater than or equal to the divisor, the V-bit is set and the instruction 
terminated. 
The division of DO by Dl is carried out by DIVU Dl, DO and results in: 
[DO ( 0 : 1 5 1]<-[DO ( 0 ! 3 1 )]/[Dl, 0 l l 5,] 
[DO(i6:3i) ]<—remainder 
X N Z V C 
- * * * 0 
Condi 

6 2 2 
Appendix: The 68000 instruction set 
The X-bit is not affected by a division. The N-bit is set if the quotient is negative. The Z-bit is set 
if the quotient is zero. The V-bit is set if division overflow occurs (in which case the Z- and 
N-bits are undefined). The C-bit is always cleared. 
Source operand addressing modes 
• 
• 
• 
• 
• 
• 
" 
' 
• 
' 
" 
EOR 
Exclusive OR logical 
Operation: 
[ d e s t i n a t i o n ] ^ [ s o u r c e ] + [ d e s t i n a t i o n ] 
Syntax: 
EOR Dn, < e a > 
Sample syntax: 
EOR D3 , - (A3) 
Attributes: 
Size=byte, word, longword 
Description: 
EOR (exclusive or) the source operand with the destination operand and store the result in the 
destination location. The source operand must be a data register and the operation 
EOR < e a > , D n is not permitted. 
Application: 
The EOR instruction is used to toggle (i.e. change the state of) selected bits in the operand. For 
example, if [DO] =00001111, and [Dl] = 10101010, the operation EOR. B DO , Dl toggles bits 
0 to 3 ofDl and results in [Dl] = 10100101. 
Condition codes: 
X N Z V C 
- * * 0 0 
Destination operand addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) 
(d,An) (d.An,Xi) ABS.W 
ABS.L 
(d.pq 
(d.PC.Xn) 
imm 
• 
• 
• 
• 
• 
• 
• 
• 
EORI 
EOR immediate 
Operation: 
[ d e s t i n a t i o n ] < - < l i t e r a l > + [ d e s t i n a t i o n ] 
Syntax: 
EORI # < d a t a > , < e a > 
Attributes: 
Size=byte, word, longword 
Description: 
EOR the immediate data with the contents of the destination operand. Store the result in the 
destination operand. 
Condition codes: 
X N Z V C 
- * * 0 0 
Destination operand addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) 
(d.An) (d.An,Xi) ABS.W 
ABS.L 
(<W) 
(d.PC,Xn) 
imm 
• 
• 
• 
• 
• 
• 
• 
• 
EXG 
Exchange registers 
Operation: 
[Rx] <- [Ry]; [Ry] <- [Rx] 
Syntax: 
EXG Rx,Ry 
Sample syntax: 
EXG D3 , D4 
EXG D2,A0 
EXG A7,D5 
Attributes: 
Size=longword 
imm 
(d,PC,Xn) 
m 
ABS.L 
ABS.W 
(AM.fi 
AM 
n 
[An' 
An 
An 
Dn 

Appendix: The 68000 instruction set 
623 
Description: 
Application: 
Condition codes: 
Exchange the contents of two registers. This is a longword operation because the entire 32-bit 
contents of two registers are exchanged. The instruction permits the exchange of address regis-
ters, data registers, and address and data registers. 
One application of EXG is to load an address into a data register and then process it using 
instructions that act on data registers. Then the reverse operation can be used to return the 
result to the address register. Using EXG preserves the original contents of the data register. 
X N Z V C 
EXT 
Operation: 
Syntax: 
Attributes: 
Description: 
Application: 
Condition codes: 
Sign-extend a data register 
[ d e s t i n a t i o n ] <— s i g n - e x t e n d e d [ d e s t i n a t i o n ] 
EXT.W Dn 
EXT.L Dn 
Size=word, longword 
Extend the least-significant byte in a data register to a word, or extend the least-significant word 
in a data register to a longword. If the operation is word sized, bit 7 of the designated data regis-
ter is copied to bits (8:15). If the operation is longword sized, bit 15 is copied to bits (16:31). 
If [D0]=$12345678, EXT.W DO results in 12340078,<j. 
If [DO] =$12345678, EXT.L DO results in 00005678]6. 
X N Z V C 
- * * 0 0 
ILLEGAL 
Operation: 
Syntax: 
Attributes: 
Description: 
Application: 
Condition codes: 
Illegal instruction 
[SSP] <~ [SSP] - 4; [ [SSP] ] <- [PC] ; 
[SSP] <- [SSP] ~ 2; [ [SSP] ] <- [SR] ; 
[PC] <•— Illegal instruction vector 
ILLEGAL 
None 
The bit pattern of the illegal instruction, 4AFC16, causes the illegal instruction trap to be taken. 
As in all exceptions, the contents of the program counter and the processor status word are 
pushed on to the supervisor stack at the start of exception processing. 
Any unknown pattern of bits read by the 68000 during an instruction read phase would cause 
an illegal instruction trap. The ILLEGAL instruction can be thought of as an official illegal 
instruction. It can be used to test the illegal instruction trap and will always be an illegal instruc-
tion in any future enhancement of the 68000. 
X N Z V C 
JMP 
Operation: 
Syntax: 
Attributes: 
Description: 
Application: 
Jump (unconditionally) 
[PC] <— d e s t i n a t i o n 
JMP < e a > 
Unsized 
Program execution continues at the effective address specified by the instruction. 
Apart from a simple unconditional jump to an address fixed at compile time (i.e. JMP l a b e l ) , 
the JMP instruction is useful for the calculation of dynamic or computed jumps. For example, 
the instruction JMP (AO, DO . L) jumps to the location pointed at by the contents of address 

624 
Appendix: The 68000 instruction set 
Condition codes: 
Source operand addressing modes 
register AO, offset by the contents of data register DO. Note that JMP provides several addressing 
modes, while BRA provides a single addressing mode (i.e. PC relative). 
X N Z V C 
Dn 
An 
(An) 
H+ 
-(An) 
(dLfci) (4An.Xi) ABS.W ABU m (4PCJ(n) imm 
• 
• 
• 
• 
• 
• 
• 
JSR 
Operation: 
Syntax: 
Attributes: 
Description: 
Application: 
Condition codes: 
Jump to subroutine 
[SP] <- [ S P ] - 4 ; 
[ [ S P ] ] < - [ P C ] 
[PC] <— d e s t i n a t i o n 
j S R < e a > 
Unsized 
JSR pushes the longword address of the instruction immediately following the JSR onto the sys-
tem stack. Program execution then continues at the address specified in the instruction. 
JSR (Ai) calls the procedure pointed at by address register Ai. The instruction JSR (Ai,Dj) 
calls the procedure at the location [Ai] + [Dj], which permits dynamically computed addresses. 
X N Z V C 
Source operand addressing modes 
Dn 
An 
(An) 
(ta)+ 
-(An) 
(4An) 
(dXXi) ABS.W 
ABU (4PQ (4PCJfo) imm 
• 
• 
• 
• 
• 
• 
• 
LEA 
Operation: 
Syntax: 
Sample syntax: 
Attributes: 
Description: 
Application: 
Load effective address 
[An] < - < e a > 
LEA < e a > , A n 
LEA 
Table,AO 
LEA 
( T a b l e , P C ) , A 0 
LEA ( - 6 , A 0 , D 0 . L ) ,A6 
LEA 
(Table,PC,DO),A6 
Size=longword 
The 
effective 
address 
is computed 
and loaded 
into 
an address 
register. 
LEA 
(—6 , AO , DO . W) , Al calculates the sum of address register AO plus data register DO.W 
sign-extended to 32 bits minus 6, and deposits this result in address register Al. The difference 
between the LEA and PEA instructions is that LEA calculates an effective address and puts it in 
an address register, whereas PEA calculates an effective address in the same way but pushes it on 
the stack. 
LEA is a very powerful instruction used to calculate an effective address. In particular, the use of 
LEA facilitates the writing of position-independent code. For example, LEA (TABLE, PC) , AO 
calculates the effective address of'TABLE' with respect to the PC and deposits it in AO. 
LEA 
(Table, PC) ,A0 
Compute a d d r e s s of T a b l e w i t h r e s p e c t t o t h e pc 
MOVE 
(A0),D1 
Pick up the first item in the table 
Do something with this item 

Appendix: The 68000 instruction set 
625 
MOVE 
Dl,(AO) 
Put i t back i n t h e 
t a b l e 
Table DS.W 100 
Condition codes: 
X N z V c 
Source operand addressing modes 
Dn 
An 
(An) 
(ta)+ 
-(An) 
(4An) (<WnJ(i) ABS.W 
ABS.L (m (d,PUn) •mm 
• 
• 
• 
• 
• 
• 
• 
LINK 
Link and allocate 
Operation: 
[SP] < - [ S P ] - 4 ; [[SP]] < - [ A n ] ; 
[An] <r- [SP] ; [SP] <- [SP] + d 
Syntax: 
LINK An, # < d i s p l a c e m e n t > 
Sample syntax: 
LINK A6, #-12 
Attributes: 
Size=word 
Description: 
The contents of the specified address register are first pushed onto the stack. Then, this address 
register is loaded with the updated stack pointer. Finally, the 16-bit sign-extended displacement, 
d, is added to the stack pointer. The contents of the address register occupy two words on the 
stack. A negative displacement must be used to allocate stack area to a procedure. At the end of 
a LINK instruction, the old value of address register An has been pushed on the stack and the 
new An is pointing at the base of the stack frame. The stack pointer itself has been moved up by 
d bytes and is pointing at the top of the stack frame. Address register An is called the frame 
pointer because it is used to reference data on the stack frame. By convention, programmers 
often use A6 as a frame pointer. 
Application: 
The L INK and UNLK instructions are used to create local workspace on the top of a procedure's 
stack. Consider the code: 
S u b r t n LINK A6,#-12 
C r e a t e a 1 2 - b y t e w o r k s p a c e 
MOVE D3,(—8,A6) 
Access t h e s t a c k frame v i a A6 
Condition codes: 
UNLK A6 
RTS 
X N Z V C 
Collapse the workspace 
Return from subroutine 
The LINK instruction does not affect the CCR. 
LSL, 
LSR 
Operation: 
Syntax: 
Attributes: 
Description: 
Logical shift left/right 
[ d e s t i n a t i o n ] <- [ d e s t i n a t i o n ] 
s h i f t e d 
b y < c o u n t > 
LSL Dx,Dy 
LSR Dx,Dy 
LSL # < d a t a > , Dy 
LSR # < d a t a > , Dy 
L S L < e a > 
L S R < e a > 
Size=byte, word, longword 
Logically shift the bits of the operand in the specified direction (i.e. left or right). A zero 
is shifted into the input position and the bit shifted out is copied into both the C- and the 

6 2 6 
Appendix: The 68000 instruction set 
X-bits of the CCR. The shift count may be specified in one of three ways. The count may be a 
literal, the contents of a data register, or the value 1. An immediate count permits a shift of 1 
to 8 places. If the count is in a register, the value is modulo 64—from 0 to 63. If no count is 
specified, one shift is made (e.g. LSL < e a > shifts the word at the effective address one posi-
tion left). 
LSL 
c 
£ 
X 
£ 
Operand 
LSR 
Operand 
> 
X 
Application: 
Condition codes: 
If [D3.W] = 11001100101011102,LSL.W #5, D3 produces the result 10010101110000002. After 
the shift, the X-and C-bits of the CCR are set to 1 (since the last bit shifted out was a 1). 
X N Z V C 
* 
* 
* Q * 
The X-bit is set to the last bit shifted out of the operand and is equal to the C-bit. However, a zero 
shift count leaves the X-bit unaffected and the C-bit cleared. 
Destination operand addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) 
(din) 
(d,An,Xi) ABS.W 
ABS.L 
(4K) (d,PC.Xn) imm 
• 
• 
• 
• 
• 
• 
• 
• 
MOVE 
Operation: 
[dest 
Syntax: 
MOVE 
Sample syntax: 
MOVE 
MOVE 
MOVE 
MOVE 
Attributes: 
Size= 
Description: 
Move 
Condition codes: 
Copy data from source to destination 
[destination] <— [source] 
<ea>, <ea> 
(A5),-<A2) 
-(A5). (A2) + 
#$123,(A6) + 
Templ,Temp2 
Size=byte, word, longword 
Move the contents of the source to the destination location. The data is examined as it is moved 
and the condition codes set accordingly. Note that this is actually a copy command because the 
source is not affected by the move. The move instruction has the widest range of addressing 
modes of all the 68000's instructions. 
X N Z V C 
- * * 0 0 
Source operand addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) 
(AMI 
((UtiOi) ABS.W 
ABS.L m (d.PCXn) imm 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 

Appendix: The 68000 instruction set 
6 2 7 
Destination operand addressing modes 
Dn 
An 
(An) 
(A«)
+ 
-(An) 
(4*1) (Utfi) ABS.W 
ABS.L m (d.PC» imm 
• 
• 
• 
• 
• 
• 
• 
• 
MOVEA 
Operation: 
Syntax: 
Attributes: 
Description: 
Application: 
Condition codes: 
Move address 
[An] <— [ s o u r c e ] 
MOVEA < e a > , An 
Size=word, longword 
Move the contents of the source to the destination location, which is an address register. The 
source must be a word or longword. If it is a word, it is sign-extended to a longword. The condi-
tion codes are not affected. 
The MOVEA instruction is used to load an address register (some assemblers simply employ the 
MOVE mnemonic for both MOVE and MOVEA). The instruction LEA can often be used to per-
form the same operation (e.g. MOVEA. L # $ 12 3 4 , A0 is the same as LEA $ 12 3 4 , A0). Take 
care because the MOVEA.W 
#$8000,AO instruction sign-extends the source operand to 
$FFFF8000 before loading it into AO, whereas LEA $ 8 0 0 0 , AO loads AO with $00008000. You 
should appreciate that the MOVEA and LEA instructions are not interchangeable. The operation 
MOVEA (Ai) , An cannot be implemented by an LEA instruction, since MOVEA (Ai), An per-
forms a memory access to obtain the source operand, as the following RTL demonstrates. 
LEA 
(Ai),An 
= [ A n ] « - [ A i ] 
MOVEA (Ai),An 
= [An] <- [M( [Ai] ) ] 
X N Z V C 
Source operand addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) 
(AM (dJkXi) ABS.W 
ABS.L 
(4K) (d.PWn) imm 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
MOVE to CCR 
Operation: 
Syntax: 
Attributes: 
Description: 
Application: 
Condition codes: 
Copy data to CCR from source 
[CCR] <- [ s o u r c e ] 
MOVE < e a > , C C R 
Size=word 
Move the contents of the source operand to the condition code register. The source operand is a 
word, but only the low-order byte contains the condition codes. The upper byte is neglected. 
Note that MOVE < e a > , CCR is a word operation, but ANDI, ORI and EORI to CCR are all 
byte operations. 
The move to CCR instruction permits the programmer to preset the CCR. For example, 
MOVE #0,CCR clears all the CCR's bits. 
X N Z V C 
Source operand addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) m (dJWi) ABS.W 
ABS.L m (d.K,Xn) imm 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 

6 2 8 
Appendix: The 68000 instruction set 
MOVE from SR 
Copy data from SR to destination 
Operation: 
[ d e s t i n a t i o n ] <— [SR] 
Syntax: 
MOVE S R , < e a > 
Attributes: 
Size=word 
Description: 
Move the contents of the status register to the destination location. The source operand, the sta-
tus register, is a word. This instruction is not privileged in the 68000, but is privileged in the 
68010,68020, and 68030. Executing a MOVE SR, < e a > while in the user mode on these proces-
sors results in a privilege violation trap. 
Condition codes: 
X N Z V C 
Destination operand addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) 
(4An) (d^njti) ABS.W 
ABS.L m (d.PC,Xn) imm 
• 
• 
• 
• 
• 
• 
• 
• 
MOVE to SR 
Copy data to SR from source 
Operation: 
IF [S] =1 
THEN [SR] <- [ s o u r c e ] 
ELSE TRAP 
Syntax: 
MOVE < e a > , S R 
Attributes: 
Size=word 
Description: 
Move the contents of the source operand to the status register. The source operand is a word and 
all bits of the status register are affected. 
Application: 
The MOVE to SR instruction allows the programmer to preset the contents of the status register. 
This instruction permits the trace mode, interrupt mask, and status bits to be modified. For 
example, MOVE # $ 2 7 0 0 , SR moves 00100111 00000000 to the status register, which clears all 
bits of the CCR, sets the S-bit, clears the T-bit, and sets the interrupt mask level to 7. 
Condition codes: 
X N z V C 
* * * * * 
Source operand addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) (m (dXXi) ABS.W 
ABS.L 
(4K) (d.PCJ(n) imm 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
MOVE USP 
Copy data to or from USP 
Operation 1: 
I F [ S ] = 1 
{MOVE USP,An form} 
THEN [USP] ir- [An] 
ELSE TRAP 
Operation 2: 
I F [ S ] = 1 
{MOVE An,USP form} 
THEN [An] <r- [USP] 
ELSE TRAP 
Syntax 1: 
MOVE USP,An 
Syntax2: 
MOVE An,USP 
Attributes: 
Size=longword 

Appendix: The 68000 instruction set 
629 
Description: 
Condition codes: 
Move the contents of the user stack pointer to an address register or vice versa. This is a privileged 
instruction and allows the operating system running in the supervisor state either to read the con-
tents of the user stack pointer or to set up the user stack pointer. 
X N Z V C 
MOVEM 
Operation 1: 
Operation 2: 
Syntax 1: 
Syntax 2: 
Sample syntax: 
Attributes: 
Description: 
Move multiple registers 
REPEAT 
[destination_register]<— [source] 
UNTIL all registers in list moved 
REPEAT 
[destination] <— [source^register] 
UNTIL all registers in list moved 
MOVEM 
MOVEM 
MOVEM.L 
MOVEM.L 
MOVEM.W 
MOVEM.W 
Application: 
<ea>, < r e g i s t e r l i s t > 
< r e g i s t e r 
l i s t > , < e a > 
D0-D7/A0-A6,$1234 
(A5),DO-D2/D5-D7/A0-A3/A6 
(A7) +.D0-D5/D7/A0-A6 
D0-D5/D7/A0-A6,-(A7) 
Size=word, longword 
The group of registers specified by < r e g i s t e r 
l i s t > is copied to or from consecutive 
memory locations. The starting location is provided by the effective address. Any combination 
of the 68000's sixteen address and data registers can be copied by a single MOVEM instruction. 
Note that either a word or a longword can be moved, and that a word is sign-extended to a long-
word when it is moved (even if the destination is a data register). 
When a group of registers is transferred to or from memory (using an addressing mode other 
than predecrementing or postincrementing), the registers are transferred starting at the speci-
fied address and up through higher addresses. The order of transfer of registers is data register 
DO to D7, followed by address register A0 to A7. 
MOVEM. L 
D0-D2/D4/A5/A6,$1234 copies registers DO , Dl, D2 , D4 , A5 , A6 to 
memory, starting at location $1234 (where DO is stored) and moving to locations $1238, 
$ 123C, 
The address counter is incremented by 2 or 4 after each move according to whether 
the operation is moving words or longwords, respectively. 
If the effective address is in the predecrement mode (i.e. —(An)), only a register to memory 
operation is permitted. The registers are stored starting at the specified address minus two (or 
four for longword operands) and down through lower addresses. The order of storing is from 
address register A7 to address register A0, then from data register D7 to data register DO. The 
decremented address register is updated to contain the address of the last word stored. 
If the effective address is in the postincrement mode (i.e. (An)+), only a memory to register 
transfer is permitted. The registers are loaded starting at the specified address and up through 
higher addresses. The order of loading is the inverse of that used by the predecrement mode and 
is DO to D7 followed by A0 to A7. The incremented address register is updated to contain the 
address of the last word plus two (or four for longword operands). 
Note that the MOVEM instruction has a side effect. An extra bus cycle occurs for memory 
operands, and an operand at one address higher than the last register in the list is accessed. This 
extra access is an 'overshoot' and has no effect as far as the programmer is concerned. However, it 
could cause a problem if the overshoot extended beyond the bounds of physical memory. Once 
again, remember that MOVEM. W sign-extends words when they are moved to data registers. 
This instruction is used to save working registers on entry to a subroutine and to restore them at 
the end of a subroutine. 
BSR 
Example 

630 
Appendix: The 68000 instruction set 
Condition codes: 
Example MOVEM.L D0-D5/A0-A3,-(SP) Save registers 
Body of subroutine 
MOVEM.L (SP) + ,D0-D5/A0-A3 Restore registers 
RTS 
Return 
X N Z V C 
Source operand addressing modes (memory t o register) 
Dn 
An 
(An) 
(An)+ 
-(An) 
(<Un) (d.An.Xi) ABS.W 
ABU 
(4K) (d.PC,Xn) imm 
• 
• 
• 
• 
• 
• 
• 
• 
Destinaton operand addressing modes (register to memory) 
Dn 
An 
(An) 
(An)+ -<An) 
(4An) (dJVnJCi) ABS.W 
ABU 
(d.K) 
(d,PC,Xn) imm 
• 
• 
• 
• 
• 
• 
MOVEQ 
Operation: 
Syntax: 
Attributes: 
Description: 
Application: 
Condition codes: 
Move quick (copy a small literal to a destination) 
[ d e s t i n a t i o n ] <— < l i t e r a l > 
MOVEQ # < d a t a > , D n 
Size = longword 
Move the specified literal to a data register. The literal is an eight-bit field within the MOVEQ 
op-code and specifies a signed value in the range —128 to +127. When the source operand is 
transferred, it is sign-extended to 32 bits. Consequently, although only 8 bits are moved, the 
MOVEQ instruction is a longword operation. 
MOVEQ is used to load small integers into a data register. Beware of its sign-extension. The two 
operations MOVE . B #12 , DO and MOVEQ #12 , DO are not equivalent. The former has the 
effect [D0(0:7)]<—12, whereas the latter has the effect [D0(0:31)] <— 12 (with sign-extension). 
X N Z V C 
- * * 0 0 
MULS, MULU 
Operation: 
Syntax: 
Attributes: 
Description: 
Application: 
Signed multiply, unsigned multiply 
[destination] <— [destination] * [source] 
MULS <ea>,Dn 
MULU <ea>,Dn 
Size=word (the product is a longword) 
Multiply the 16-bit destination operand by the 16-bit source operand and store the result in the 
destination. Both the source and destination are 16-bit word values and the destination result is 
a 32-bit longword. The product is dierefore a correct 32-bit product and is not truncated. MULU 
performs multiplication with unsigned values and MULS performs multiplication witfi two's 
complement values. 
MULU Dl, D2 multiplies the low-order words of data registers Dl and D2 and puts the 32-bit 
result in D2. MULU # $ 12 3 4 , D3 multiplies the low-order word of D3 by the 16-bit literal $1234 
and puts the 32-bit result in D3. 

Appendix: The 68000 instruction set 
631 
Condition codes: 
X N z v c 
- * * 0 0 
Source operand addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) 
(AM) 
(UnJG) ABS.W 
ABSL m (d.PCJ(n) imm 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
NEG 
Negate 
Operation: 
[ d e s t i n a t i o n ] <—0- [ d e s t i n a t i o n ] 
Syntax: 
NEG <ea> 
Attributes: 
Size=byte, word, longword 
Description: 
Subtract the destination operand from 0 and store the result in the destination location. The dif-
ference between NOT and NEG is that NOT performs a bit-by-bit logical complementation, 
whereas NEG performs a 2's complement arithmetic subtraction. All bits of the condition code 
register are modified by a NEG operation; e.g. if D3.B = 111001112, the logical operation 
NEG. B D3 results in D3 = 000110012 (XNZVC=10001) andNOT.B D3 results in D3 = 
000110002 (XNZVC= -0000). 
Condition codes: 
X N Z V C 
* * * * * 
Note that the X-bit is set to the value of the C-bit. 
Destination operand addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) 
(AM) 
(d.An,Xi) ABS.W 
ABSL 
(d.K) 
(d,PCXn) imm 
• 
• 
• 
• 
• 
• 
• 
• 
NEGX 
Negate with extend 
Operation: 
[ d e s t i n a t i o n ] <— 0 - [ d e s t i n a t i o n ] - [X] 
Syntax: 
NEGX <ea> 
Attributes: 
Size=byte, word, longword 
Description: 
The operand addressed as the destination and the extend bit are subtracted from zero. NEGX is 
the same as NEG except that the X-bit is also subtracted from zero. 
Condition codes: 
X N z V C 
* * * * * 
The Z-bit is cleared if the result is non-zero and is unchanged otherwise. The X-bit is set to the 
same value as the C-bit. 
Destination operand addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) m (d.An.Xi) ABS.W 
ABS.L m (d.PUn) imm 
• 
• 
• 
• 
• 
• 
• 
• 
NOP 
No operation 
Operation: 
None 
Syntax: 
NOP 
Attributes: 
Unsized 
Description: 
The no operation instruction NOP performs no computation. Execution continues with the 
instruction following the NOP instruction. The processor's state is not modified by an NOP. 

632 
Appendix: The 68000 instruction set 
Application: 
Condition codes: 
NOPs can be used to introduce a delay in code. Some programmers use them to provide space 
for patches—two or more NOPs can later be replaced by branch or jump instructions to fix a 
bug. This use of the NOP is seriously frowned upon, as errors should be corrected by reassem-
bling the code rather than by patching it. 
X N Z V C 
NOT 
Operation: 
Syntax: 
Attributes: 
Description: 
Condition codes: 
Logical complement 
[destination] <— [destination] 
NOT <ea> 
Size = byte, word, longword 
Calculate the logical complement of the destination and store the result in the destination. The 
difference between NOT and NEG is that NOT performs a bit-by-bit logical complementation, 
whereas a NEG performs a two's complement arithmetic subtraction. Moreover, NEG updates all 
bits of the CCR, while NOT clears the V- and C-bits, updates the N- and Z-bits, and doesn't affect 
the X-bit. 
X N Z V C 
- * * 0 0 
Source operand addressing modes 
On 
An 
(An) 
(An)+ 
-(An) 
(Un) (cwiai) ABS.W 
ABU 
(4PQ 
(d.PC,Xn) 
imm 
• 
• 
• 
• 
• 
• 
• 
• 
OR 
OR logical 
[ d e s t i n a t i o n ] <— [ s o u r c e ] + [ d e s t i n a t i o n ] 
OR <ea>,Dn 
OR Dn,<ea> 
Size = byte, word, longword 
OR the source operand to the destination operand and store the result in the destination 
location. 
The OR instruction is used to set selected bits of the operand. For example, we can set the four 
most-significant bits of a longword operand in DO by executing: 
OR.L #$F00000O0,DO 
X N Z V C 
- * * 0 0 
Source operand addressing modes 
Operation: 
Syntax: 
Attributes: 
Description: 
Application: 
Condition codes: 
Destination operand addressing modes 
Dn 
An 
(An) 
(An)+ -(An) 
(<Un) (<WnJ(i) ABS.W 
ABS.L 
(d.PQ (d.PUn) 
imm 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
l modes 
On 
An 
(An) 
(An)+ -(An) 
(<Wn) (4W0 ABS.W 
ABS.L m (d,PC.Xn) 
imm 
• 
• 
• 
• 
• 
• 
• 
• 

Appendix: The 68000 instruction set 633 
ORI 
OR immediate 
Operation: 
[ d e s t i n a t i o n ] < — < l i t e r a l > + [ d e s t i n a t i o n ] 
Syntax: 
ORI #<data>, <ea> 
Attributes: 
Size=byte, word, longword 
Description: 
OR the immediate data with the destination operand. Store the result in the destination 
operand. 
Condition codes: 
X N Z V C 
- * * 0 0 
Application: 
ORI forms the logical OR of the immediate source with the effective address, which may be a 
memory location. For example, 
ORI.B #%00000011,(A0)+ 
Destination operand addressing modes 
Dn 
An 
(An) 
(An)+ -(An) m (<Wn,Xi) ABS.W 
ABS.L 
(<i.K) (d,PC,Xn) imm 
• 
• 
• 
• 
• 
• 
• 
• 
ORI t o CCR 
Inclusive OR immediate to CCR 
Operation: 
[CCR] < - < l i t e r a l > + [CCR] 
Syntax: 
ORI #<data> , CCR 
Attributes: 
Size=byte 
Description: 
OR the immediate data with the condition code register (i.e. the least-significant byte of the 
status register). The Z flag of the CCR can be set by ORI #$04 , CCR. 
Condition codes: 
x N Z V C 
* * * * * 
X is set if bit 4 of data= 1; unchanged otherwise 
N is set if bit 3 of data= 1; unchanged otherwise 
Z is set if bit 2 of data= 1; unchanged otherwise 
V is set if bit 1 of data= 1; unchanged otherwise 
C is set if bit 0 of data= 1; unchanged otherwise 
ORI to SR 
Inclusive OR immediate to status register 
Operation: 
IF 
[ S ] = l 
THEN 
[SR] < - < l i t e r a l > + [SR] 
ELSE TRAP 
Syntax: 
ORI # < d a t a > , SR 
Attributes: 
Size=word 
Description: 
OR the immediate data to the status register and store the result in the status register. All bits of 
the status register are affected. 
Application: 
Used to set bits in the SR (i.e. the S, T, and interrupt mask bits). For example, ORI#$8000,SR 
sets bit 15 of the SR (i.e. the trace bit). 
Condition codes: 
X N Z V C 
* * * * * 
X is set if bit 4 of data= 1; unchanged otherwise 
N is set if bit 3 of data= 1; unchanged otherwise 
Z is set if bit 2 of data= 1; unchanged otherwise 
V is set if bit 1 of data= 1; unchanged otherwise 
C is set if bit 0 of data = 1; unchanged otherwise 

634 
Appendix: The 68000 instruction set 
PEA 
Push effective address 
Operation: 
Syntax: 
Attributes: 
Description: 
Application: 
Subroutine: 
MOVEA.L 
( 4 , S P ) , A 0 
MOVE.W 
( A 0 ) , D 2 
RTS 
Condition codes: 
X N Z V C 
[SP] <- [ S P ] - 4 ; [ [SP] ] <--<ea> 
PEA <ea> 
Size = longword 
The longword effective address specified by the instruction is computed and pushed onto the 
stack. For example, PEA XYZ would push the address of'XYZ' on to the stack. The difference 
between PEA and LEA is that LEA calculates an effective address and puts it in an address regis-
ter, whereas PEA calculates an effective address in the same way but pushes it on the stack. 
PEA calculates an effective address to be used later in address register indirect addressing. In par-
ticular, it facilitates the writing of position-independent code. For example, PEA (TABLE, PC) 
calculates the address of TABLE with respect to the PC and pushes it on the stack. This address can 
be read by a procedure and then used to access the data to which it points. Consider the example: 
PEA 
Wednesday 
Push t h e p a r a m e t e r a d d r e s s on t h e 
s t a c k 
BSR 
S u b r o u t i n e 
C a l l t h e 
p r o c e d u r e 
LEA 
( 4 , S P ) , S P 
Remove s p a c e o c c u p i e d by t h e 
p a r a m e t e r 
A0 points to parameter under return address 
Access the actual parameter - Wednesday 
Source operand addressing modes 
Dn 
An 
(An) 
(*")+ 
-(An) 
(*An) (dJWB) ABS.W 
ABS.L 
(4K) 
(d.PCXn) imm 
• 
• 
• 
• 
• 
• 
• 
ROL, 
ROR 
Operation: 
Syntax: 
Attributes: 
Description: 
Rotate left/right (without extend) 
[destination] <— [destination] rotated by<count> 
ROL Dx,Dy 
ROR Dx, Dy 
ROL #<data>,Dy 
ROR #<data>,Dy 
ROL <ea> 
ROR <ea> 
Size=byte, word, longword 
Rotate the bits of the operand in the direction indicated. The extend bit, X, is not included in the 
operation. A rotate operation is circular in the sense that the bit shifted out at one end is shifted 
into the other end. That is, no bit is lost or destroyed by a rotate operation. The bit shifted out is 
also copied into the C-bit of the CCR, but not into the X-bit. The shift count may be specified in 
one of three ways: the count may be a literal, the contents of a data register, or the value 1. An 
immediate count permits a shift of 1 to 8 places. If the count is in a register, the value is modulo 
64, allowing a range of 0 to 63. If no count is specified, the word at the effective address is rotated 
by one place (e.g. ROL <ea>). 
ROL 
E> 
Operand 
ROR 
Operand 
5» 
C 

Appendix: The 68000 instruction set 
635 
Condition codes: 
X N Z V C 
- * * 0 * 
The X-bit is not affected and the C-bit is set to the last bit rotated out of the operand (C is set to 
zero if the shift count is 0). 
Destination operand addressing modes 
On 
An 
(An) 
(*•)+ 
-(An) m (MRJG) ABS.W 
ABS.L m (iPCXn) imm 
• 
• 
• 
• 
• 
• 
• 
• 
ROXL, ROXR 
Operation: 
Syntax: 
Attributes: 
Description: 
Rotate left/right with extend 
[ d e s t i n a t i o n ] <— [ d e s t i n a t i o n ] r o t a t e d 
b y < c o u n t > 
ROXL 
Dx,Dy 
ROXR 
Dx,Dy 
ROXL 
#<data>,Dy 
ROXR 
#<data>,Dy 
ROXL 
<ea> 
ROXR 
<ea> 
Size=byte, word, longword 
Rotate the bits of the operand in the direction indicated. The extend bit of the CCR is included 
in the rotation. A rotate operation is circular in the sense that the bit shifted out at one end 
is shifted into the other end. That is, no bit is lost or destroyed by a rotate operation. Since the 
X-bit is included in the rotate, the rotation is performed over 9 bits (. B), 17 bits ( . W), or 33 bits 
(. L). The bit shifted out is also copied into the C-bit of the CCR as well as the X-bit. The shift 
count may be specified in one of three ways: the count may be a literal, the contents of a data reg-
ister, or the value 1. An immediate count permits a shift of 1 to 8 places. If the count is in a regis-
ter, the value is modulo 64 and the range is from 0 to 63. If no count is specified, the word at the 
specified effective address is rotated by one place (i.e. ROXL <ea>). 
Condition codes: 
c 
c 
ROXL 
Operand 
<E\ 
ROXR 
X N Z V C 
The X- and the C-bit are set to the last bit rotated out of the operand. If the rotate count is zero, 
the X-bit is unaffected and the C-bit is set to the X-bit. 
Destination operand addressing modes 
On 
h 
(An) 
(An)+ -w m 
(dJ^Xi) AK.W 
ABS.L m 
(d.PC.X») imm 
• 
• 
• 
• 
• 
• 
• 
• 
Operand 
X 
c 

636 
Appendix: The 68000 instruction set 
RTE 
Operation: 
Syntax: 
Attributes: 
Description: 
[SP] <- [SPJ+2 
[SP] <-[SP] + 4 
Condition codes: 
Return from exception 
IF 
[S]=1THEN 
[SR] <- [ [ S P ] ] 
[PC] <H- [[SP]] 
ELSE TRAP 
RTE 
Unsized 
The status register and program counter are pulled from the stack. The previous values of the SR 
and PC are lost. The RTE is used to terminate an exception handler. Note that the behavior of the 
RTE instruction depends on the nature of both the exception and processor type. The 68010 and 
later models push more information on the stack following an exception than the 68000. The 
processor determines how much to remove from the stack. 
X N Z V C 
The CCR is restored to its pre-exception state. 
RTS 
Operation: 
Syntax: 
Attributes: 
Description: 
Condition codes: 
Return from subroutine 
[PC] <- l [SP] ] ; [SP] <- [SP] +4 
RTS 
Unsized 
The program counter is pulled from the stack and the previous value of the PC is lost. RTS is used to 
terminate a subroutine. 
X N Z V C 
STOP 
Operation: 
Syntax: 
Sample syntax: 
Attributes: 
Description: 
Condition codes: 
Load status register and stop 
IF [S]=1 THEN 
[SR] <-<data> 
STOP 
ELSE TRAP 
STOP #<data> 
STOP #$2700 
STOP #SetUp 
Unsized 
The immediate operand is copied into the entire status register (i.e. both status byte and CCR are 
modified), and the program counter advanced to point to the next instruction to be executed. 
The processor then suspends all further processing and halts. That is, the privileged STOP 
instruction stops the 68000. 
The execution of instructions resumes when a trace, an interrupt, or a reset exception occurs. 
A trace exception will occur if the trace bit is set when the STOP instruction is encountered. If an 
interrupt request arrives whose priority is higher than the current processor priority, an interrupt 
exception occurs, otherwise the interrupt request has no effect. If the bit of the immediate data 
corresponding to the S-bit is clear (i.e. user mode selected), execution of the STOP instruction 
will cause a privilege violation. An external reset will always initiate reset exception processing. 
X N Z V C 
Set according to the literal. 

Appendix: The 68000 instruction set 
6 3 7 
SUB 
Subtract binary 
Operation: 
[ d e s t i n a t i o n ] <- [ d e s t i n a t i o n ] — [ s o u r c e ] 
Syntax: 
SUB <ea>,Dn 
SUB Dn,<ea> 
Attributes: 
Size=byte, word, longword 
Description: 
Subtract the source operand from the destination operand and store the result in the destination 
location. 
Condition codes: 
X N z V c 
* * * * * 
Source operand addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) 
(<LAn) (dMXi) ABS.W 
ABU m (d,PCJ(n) imm 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
Destination operand addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) 
(iM) (iAnJU) ABS.W 
ABS.L m (d.PCJ(n) imm 
• 
• 
• 
• 
• 
• 
• 
• 
SUBA 
Subtract address 
Operation: 
[ d e s t i n a t i o n ] <— [ d e s t i n a t i o n ] — [ s o u r c e ] 
Syntax: 
SUBA <ea>,An 
Attributes: 
Size=word, longword 
Description: 
Subtract the source operand from the destination operand and store the result in the destination 
address register. Word operations are sign-extended to 32 bits prior to subtraction. 
Condition codes: 
x N z v c 
Source operand addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) 
(4An) 
((Unjti) ABS.W 
ABS.L 
(«W) (d,PO) 
imm 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
SUBI 
Subtract immediate 
Operation: 
[ d e s t i n a t i o n ] <— [ d e s t i n a t i o n ] — [ s o u r c e ] 
Syntax: 
SUBI # < d a t a > , < e a > 
Attributes: 
Size=byte, word, longword 
Description: 
Subtract the immediate data from the destination operand. Store the result in the destination 
operand. 
Condition codes: 
X N z V C 
* * * * * 
Destination operand addressing modes 
Dn 
An 
(An) 
(An)+ 
-(An) 
(iM) (d.An.Xi) ABS.W 
ABS.L 
(d.Pq (d,PC,Xn) imm 
• 
• 
• 
• 
• 
• 
• 
• 

638 
Appendix: The 68000 instruction set 
SUBQ 
Operation: 
Syntax: 
Attributes: 
Description: 
Condition codes: 
Subtract quick 
[destination] <— [destination] — [source] 
SUBQ #<data>,<ea> 
Size=byte, word, longword 
Subtract the immediate data from the destination operand. The immediate data must be in the 
range 1 to 8. Word and longword operations on address registers do not affect condition codes. 
A word operation on an address register affects the entire 32-bit address. 
X N Z V C 
Destination operand addressing modes 
On 
An 
(An) 
(An)+ -(An) 
(iWn) (dMXi) ABS.W 
ABU (tfQ (d,PC.Xn) imm 
• 
• 
• 
• 
• 
• 
• 
• 
• 
SUBX 
Operation: 
Syntax: 
Attributes: 
Description: 
Condition codes: 
Subtract extended 
[destination] <— [destination] — [source] — [x] 
SUBX Dx,Dy 
SUBX - (Ax) ,-(Ay) 
Size=byte, word, longword 
Subtract the source operand from the destination operand along with the extend bit, and store 
the result in the destination location. The only legal addressing modes are data register direct and 
memory to memory with address register indirect using auto-decrementing. 
X N Z V C 
Z: Cleared if the result is non-zero, unchanged otherwise. The Z-bit can be used to test for zero 
after a chain of multiple precision operations. 
SWAP 
Operation: 
Syntax: 
Attributes: 
Description: 
Application: 
Condition codes: 
Swap register halves 
[ R e g i s t e r ( 1 6 : 3 1 ) ] <- [ R e g i s t e r ( 0 ; 1 5 ) ] ; 
[ R e g i s t e r { 0 : 1 5 ) ] <- [ R e g i s t e r ( 1 6 : 3 1 1 ] 
SWAP Dn 
Size=word 
Exchange the upper and lower 16-bit words of a data register. 
The SWAP Dn instruction enables the higher-order word in a register to take part in word 
operations by moving it into the lower-order position. SWAP Dn is effectively equivalent to 
ROR. L Di, Dn, where [Di] = 16. However, SWAP clears the C-bit of the CCR, whereas ROR sets 
it according to the last bit to be shifted into the carry bit, 
X N Z V C 
- 
* * 0 0 
The N-bit is set if most-significant bit of the 32-bit result is set and cleared otherwise. The Z-bit 
is set if 32-bit result is zero and cleared otherwise. 
TRAP 
Trap 
Operation: 
S < - 1 ; 
[SSP] f- [SSP]-4 
[SSP] <^ [SSP]-2 
[PC] 
^ - v e c t o r 
Syntax: 
Trap #<vector> 
[ [ S S P ] ] < - [ P C ] ; 
[ [ S S P ] ] < - [ S R ] ; 

Appendix: The 68000 instruction set 
639 
Attributes: 
Description: 
Application: 
Condition codes: 
Unsized 
This instruction forces the processor to initiate exception processing. The vector number used by 
the TRAP instruction is in the range 0 to 15 and, therefore, supports 16 traps (i.e. TRAP #0 to 
TRAP #15). 
The TRAP instruction is used to perform operating system calls and is system independent. That 
is, the effect of the call depends on the particular operating environment. For example, the 
University of Teesside 68000 simulator uses TRAP # 15 to perform I/O. The ASCII character in 
D1 .B is displayed by the following sequence. 
MOVE.B #6,DO Set up to display a character parameter in DO 
TRAP 
#15 
Now call the operating system 
X N Z V C 
TST 
Operation: 
Syntax: 
Attributes: 
Description: 
Condition codes: 
Test an operand 
[CCR] <-tested ( [operand]) 
i.e. [operand]—0; update CCR 
TST <ea> 
Size=byte, word, longword 
The operand is compared with zero. No results is saved, but the contents of the CCR are set 
according to the results. The effect of TST <ea> is the same as CMPI #0 , <ea> except that 
the CMPI instruction also sets/clears the V- and C-bits of the CCR. 
X N Z V C 
- * * 0 0 
Source operand addressing modes 
Dn 
An 
(An) 
(«•)+ 
-(An) m («UnJCi) ABS.W 
ABS.L 
(4K) 
(d.PUn) imm 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
UNLK 
Operation: 
Syntax: 
Attributes: 
Description: 
Application: 
Condition codes: 
Unlink 
[SP] <- [An] ; [An] <r- [ [SP] ] ; [SP] <- [SP] +4 
UNLK An 
Unsized 
The stack pointer is loaded from the specified address register and the old contents of the pointer 
are lost (this has the effect of collapsing the stack frame). The address register is then loaded with 
the longword pulled off the stack. 
The UNLK instruction is used in conjuction with the LINK instruction. The LINK creates a 
stack frame at the start of a procedure, and the UNLK collapses the stack frame prior to a return 
from the procedure. 
X N Z V C 


BIBLIOGRAPHY 
Logic, Computers Architecture, Computer 
Organization 
Buchanan, William & Wilson, Austin. (2001) Advanced PC Architecture 
Addison-Wesley. 
Carpinelli, John, D. (2001) Computer Systems Organization & 
Architecture. Addison-Wesley. 
Clements, Alan. (1997). Microprocessor Systems Design (3rd edition). 
International Thomson Publishing. 
Dowd, Kevin. (1993). High Performance Computing. 
O'Reilly & Associates Inc., Sebastopol, CA. 
Furber, Steve. (1996). ARM System Architecture. Addison-Wesley, 
Harlow. 
Hamacher, Carl V., Vranesic, Zvonko G., & Zaky, Safwat G. (2002). 
Computer Organization (5'h edition). McGraw-Hill. 
Hayes, John P. (1998). Computer Architecture and Organization 
(3rd edition). McGraw-Hill. 
Heuring, Vincent P., Jordan, Harry E. (2004). Computer Systems Design 
and Architecture (Second edition). Prentice Hall. 
Karp Alan H. & Flatt, Horace P. (1990). Measuring Parallel Processor 
Performance. Communications oftheACM,Vol. 33, No. 1, May 1990, 
pp539-543. 
Mano, Morris M. & Kime, Charles R. (2000). Logic and Computer 
Design Fundamentals. Prentice Hall. 
Null, Linda & Lobur, Julia. (2003). The Essentials of Computer 
Organization and Architecture. Jones and Bartlett Computer Science. 
Patterson, David A. & Hennessy, John L. (2005). Computer Organization 
& Design (3"d edition). Morgan Kaufmann Publishers, San Francisco. 
Roth, Charles H. (1992). Fundamentals of Logic Design (4'H edition). 
West Publishing Company. 
Sima, Dezso, Fountain, Terence & Kacsuk, Peter. (1997). Advanced 
Computer Architectures—A Design Space approach. 
Addison-Wesley. 
Skahill, Kevin. (1996). VHDL for Programmable Logic. 
Addison-Wesley. 
Sloss, Andrew N., Symes, Dominic & Wright, Chris. (2004). ARM 
System Developer's Guide. Elsevier. 
Stallings, William. (2006). Computer Organization and Architecture 
(Seventh edition). Prentice Hall. 
Tanenbaum, Andrew S. (2006). Structured Computer Organization 
(Fifth edition). Prentice-Hall International. 
Wakerly, John F. (2000). Digital Design {3"> edition). Prentice Hall 
International Inc. 
Warford, Stanley J. (1999). Computer Systems. Jones and Bartlett 
Publishers, Sudbury, MA. 
Wilkinson, Barry. (1996). Computer Architecture (2nd edition). Prentice 
Hall Europe. 
Yarbrough, John M. (1997). Digital Logic Applications and Design. West 
Publishing Company. 
Operating Systems 
Bryant, Randal E. & O'Hallaron, David. (2003). Computer Systems—a 
programmer's perspective. Prentice Hall. 
Cooling, Jim E. (1997). Real-time Software Systems. International 
Thomson Publishing. 
Flynn, Ida M. & Mclver McHoes, Ann, (1997). Understanding Operating 
Systems (2nd edition). International Thomson Publishing. 
Rajkumar, Davis. (2001). Operating Systems—a systematic view. 
Addison-Wesley. 
Stallings, William. (2002). Operating Systems—Internals and Design 
Principles (4th edition). Prentice-Hall International. 
Williams, Rob. (2001). Computer Systems Architecture—a networking 
approach. Addison-Wesley. 
Wolf, Wayne. (2001) Computers as Components. Morgan Kaufmann. 
Memory Systems. 
Bell, Alan E. (1996). Next-Generation Compact Discs. Scientific 
American, July 1996,pp28-37. 
Burger, Doug & Goodman, James, R. (1997). Billion-Transistor 
Architectures. Computer, September 1997, pp46-48. 
Gemmell, James D. et al. (1994). Delay-Sensitive Multimedia on Disks. 
IEEE Multimedia, Fall, 1994, pp56-66. 
Hewlett-Packard. (1997). Digital Modulation in Communications 
Systems—an Introduction. Application Note 1298, Hewlett-Packard 
Company. 
Hill, Mark D. (1988). A Case for Direct-Mapped Caches. Computer, 
December 1988, pp25-39. 
Lubell, Peter D. (1995). The Gathering Storm in High-Density Compact 
Disks. IEEE Spectrum, August 1995, pp32-37. 
Pohlmann, Ken C. (1992). The Compact Disc Handbook. Oxford 
University Press, Cambridge. 
Prince, Betty (1999). High Performance Memories (Revised edition). 
John Wiley 8< Sons, Ltd., Chichester. 
Smith, Alan Jay (1983). Cache Memories. Computing Surveys, Vol. 14, 
No. 3, September 1982, pp473-530. 
Williams, E. W (1996). The CD-ROM and Optical Disc Recording 
Systems. Oxford University Press, Oxford. 
I/O techniques. Peripherals. 
Johnson, Barry W. (1987). A Course on the Design of Reliable Digital 
Systems. IEEE Transactions on Education, Vol. E-30, No. 1, February 
1987,pp27-36. 
Kuc, Roman (1999). The Information Age. Thomson learning. 
Marven, Craig & Ewers, Gillian. (1996). A Simple Approach to Digital 
Signal Processing. John Wiley 8c Sons, Ltd., Chichester. 

642 
Bibliography 
Morrison, T. P. (1997). The Art of Computerized Measurement. Oxford 
University Press, Oxford. 
Schultz, Jerome S. (1991). Biosensors. Scientific American, August 1991, 
pp64-69. 
Communications 
Comer, Douglas E. (2003). Computer Networks and Internets 
(4"' Edition). Prentice Hall. 
Halsall, Fred (1995). Data Communications, Computer Networks and 
Open Systems (4'h edition). Addison-Wesley. 
Shay, William A. (1995). Understanding Data Communications Systems. 
International Thomson Publishing. 
Stallings, William (2003). Data and Computer Communications, 
(Seventh Edition). Prentice Hall. 
Tanenbaum, Andrew S. (2002). Computer Networks (Fourth edition). 
Prentice Hall. 

Index 
# symbol 249,300 
$ symbol 228 
% symbol 228 
360 series, IBM 204 
68K address decoding 516 
68K branches 224 
68K family 210 
68K interrupt structure 418 
68K registers 234 
68K stack 263 
68K, two stacks 556 
74138 86,512 
74LS139 113 
74LS373 112 
74LS74 110 
74LS95 125 
8080 13 
ABC computer 11 
Absolute address 215,260 
Accelerating performance 325 
Access time 497,499 
Access time, disk 527 
Accumulator 296,369 
Accuracy 182 
ACIA 426 586 
ACIA format control 430 
ACIA organization 429 
ACIA status register 431 
Acquisition time 479 
Active matrix LCD 459 
Actuator 525 
Ada Gordon 8 
ADC 218,468 
ADC, error function 470 
ADC, integrating 484 
ADC, parallel 479 
ADC, performance 469 
ADC, potentiometric network 475 
ADC, ramp feedback 481 
ADC, successive 
approximation 482 
ADC, tracking 482 
Add with carry 218 
ADD 235 
ADDA 236 
ADDEQ 380 
Adder, full 171 
Adder, half 170 
Adder, parallel 173 
Adder, serial 173 
Addition, extended 219 
Addition, words 173 
Additive colors 460 
Address decoder 508 
Address decoder, 68K 516, 
517,518 
Address decoder, PROM 512 
Address field, HDLC 601 
Address mapper 319 
Address mapping table 564, 565 
Address path 294 
Address register indirect 215, 
216,250 
Address register indirect, 
applications 252 
Address register indirect, ARM 383 
Address register indirect, 
overview 251 
Address register 215,236 
Address, absolute 260 
Addressing, indexed 251,258 
Addressing, predecrementing 254 
Addressing, relative 259 
Addressing, strings 255 
Addressing modes 377 
Addressing modes, M68HC12 370 
Addressing modes, overview 215 
Adjacent, Karnaugh maps 69 
ADSL 591 
Aiken, Howard 10,12 
Air gap 520 
Algebra, Boolean 35 
Aliasing effect 472 
ALOHAnet 15 
Alpha particle 504 
Alt key 437 
Altair 13 
ALU 296,309 
Amdahl's law 351 
Amplitude modulation 588 
Amplitude uncertainty, 
ADC 472 
Amplitude-frequency distortion 
587, 588 
Analog interface 466 
Analog signal 466 
Analog systems, definition 26 
Analog to digital conversion 477 
Analytical engine 8 
AND 221 
AND gate 28 
AND gate representation 30 
Anemometer 462 
Angular velocity, CD 540 
Antialiasing 452 
Aperture time 472,479 
Apple 13 
Application layer, OSI 579 
Arbiter circuit 119 
Arbitration 403 
Arbitration, bus 118,400 
Architecture, Harvard 10 
Arithmetic, complementary 176 
Arithmetic, computer 145 
Arithmetic, floating point 186 
Arithmetic, rules 150 
Arithmetic and logic unit 296 
Arithmetic instruction 218 
Arithmetic operations 241 
Arithmetic shift 125 
ARM assembler 385 
ARM branch 380 
ARM data processing 
instructions 378 
ARM development system 386 
ARM instructions 377 
ARM processor 375 
ARM shift instructions 378 
ARM, memory reference 384 
ARPA 14 
ARPANET 607 
ASCII code 437 
ASCII 147,246,222 
ASR 222 
Assembler 204 
Assembler directives 229 
Assembler, ARM 385 
Assembler, cross 232 
Assembly language 
programming 228 
Assembly language syntax 
228,229 
Associative law 57 
Associative memory 21 
Associative-mapped cache 348 
Asynchronous counter 128,129 
Asynchronous system 114 
Asynchronous transmission 428 
ATA 529 

644 
Index 
Atanasoff, John 11 
ATN (attention) 408 
Attenuation 587 
Audio visual drive 529 
Autohandshaking, PIA 427 
Auto-indexing 384 
Automatic control 17 
Avalanche effect, memory 506 
Axioms, Boolean algebra 56, 57 
Babbage, Charles 7,10 
Backplane 403 
Band-gap device 463 
Bandwidth 497,576 
Base, number 148 
Batch mode OS 548 
Baudrate 576 
Benchmark 326 
BEQ 298 
Berkeley RISC 330 
Berners-Lee.Tim 15 
Best effort service 582 
BCE 246 
B-H characteristic 529 
Biased exponent 183,184 
Bidirectional, data path 21 
BigEndian 235 
Binary arithmetic 169 
Binary tables 170 
Binary to decimal conversion 151 
Binary to hexadecimal 
conversion 152 
Binary tree 360 
BIOS 505 
Biosensor 465 
Bit 146 
Bit-mapped image 451 
Bit-slice 320,321 
Bit insertion 600 
Bit instruction 221 
Bit stuffing 411,599 
Bits required to represent a 
number 151 
Black body 463 
Block address decoding 510 
Block parity code 160 
BNE 224 
Boole, George 56 
Boolean algebra 35 
Boolean algebra, axioms 56, 57 
Boolean algebra, introduction 56 
Boolean algebra, simplifying 
equations 60 
Booth's algorithm 191 
Bottleneck, von Neumann 210 
Boundary layer 526 
BRA 223 
Branch, ARM 380 
Branch, conditional 224 
Branch, relative address 260 
Branch, unconditional 223 
Branch, 68K 224 
Branch instruction 223 
Branch penalty 339 
Branch prediction 340 
Branch with link 382 
Bresenham's algorithm 451 
Bridging fault 97 
BSET 221 
BSR 225,266 
BTST 221 
Bubble 337 
Buffer 88,403 
Burst mode DMA 422 
Bus 88 
Bus, IEEE 488 408 
Bus, network 573 
Bus, network 594 
Bus, PC 404 
Bus arbitration 118 
Bus architecture 400 
Bus contention 404, 573 
Bus driver 404 
Bus error 556 
Bus management, IEEE bus 407 
Bus master 404 
Bus topology 356 
Buses 399 
Buses, multiple 401 
Byte addressable 234 
Byte order 235 
Byte-wide memory 235 
Cable 8 
Cable, coaxial 593 
Cable, copper 592 
Cable terminology 595 
Cache, associative-mapped 348 
Cache, design considerations 350 
Cache, direct-mapped 346 
Cache, level2 349 
Cache, set associative-mapped 349 
Cache, speedup ratio 345 
Cache, tag 347 
Cache, writeback 350 
Cache memory 344 
Cache coherency 350 
Cache organization 346 
Call, subroutine 225 
Canonical expression 35 
Capacitive switch 439 
Capacity, channel 590 
Capacity, disk 526 
Carrier sense multiple access 595 
CAS-before-RAS refreshing 502 
Cathode 444 
C-bit 299 
CCITT generator, FCS field 602 
CCR register 218,379,220,248, 
298,300, 305 
CD 536 
CD, land 537 
CD, pit 537 
CD, track organization 538 
CD, writable 540 
CD speed 540 
Cell, plasma 450 
Centralized routine 607 
Channel capacity 590 
Channel characteristics 587 
Chappe, Claude 8 
Character 147 
Characteristic equation 105 
Characteristic equation, 
JK flip-flop 121 
Character-oriented protocols 605 
Charge storage, memory 497 
Chip select 499 
Chord keyboard 436 
Circuit, cross-coupled 103 
Circuit, sequential 102 
Circuit conventions 29 
Circuit switching 583 
Circuit symbols 31 
Circular buffer 282 
Circular shift 125,222 
CISC 210,214,327 
Clear, flip-flop 104 
Clear, logic element 110 
Clear to send 429,431 
Clock skew 116 
Clock, level sensitive 112,115 
Clocked flip-flops 108,113,115 
Closed-loop transfer 400 
CLR 220,235 
Cluster topology 360 
CMOS 498 
CMP 220,246,248 
CMPA 236 
CMPEQ 381 
CMY 457,460 
Coaxial cable 593 
Code, block parity 160 
Code, data compressing 161 
Code, error correcting 157,158 
Code, error detecting 156 
Code, Hadamard 160 
Code, Hamming 160 
Code, Huffman 164 
Code, variable length 164 
Codes, special purpose 153 
Codes, unweighted 154 
Coherent light 537 
Collision, networks 594 

Index 
645 
Color CRT 458 
Color displays 457 
Color laser 461 
Color LCD 459 
Color look-up table 458 
Color printer 460 
Color space 457 
Color theory 457 
Column address strobe 501 
Combinational logic 25 
Communications hardware 572 
Communications protocols 576 
Commutative law 57 
Compare instruction 220 
Comparing circuits 39 
Comparing computers 326 
Complementary arithmetic 176 
Compression ratio 164 
Computer, electromechanical 10 
Computer, embedded 17 
Computer, mechanical 6 
Computer, stored program 19 
Computer arithmetic 145 
Computer communications 
history 574 
Computer communications 569 
Computer history 6 
Computer memory 493 
Computer peripherals 435 
Condition code flags 218 
Condition code register 245,298 
Condition code, RISC 330 
Condition codes, ARM 380 
Conditional branch 224,244, 
300,340 
Conditional execution 380 
Conditional instruction 298, 
298,299 
Conditional operation 319 
Constellation, QAM 591 
Contention, networks 594 
Context switching 552 
Continuous, signals 26 
Contrast, LCD and plasma 450 
Control field, HDLC 601 
Control key 437 
Control register, PIA 424 
Control signals, control unit 309 
Control store 320 
Control structures 246 
Control systems 486 
Control unit, microprogrammed 315 
Control unit, random logic 308 
Conversion, fraction 152 
Copper cable 592 
Coriolis force 464 
Correlation 490 
Counter, asynchronous 128 
Counter, decimal 129 
Counter, ripple 129 
Counter, synchronous 132 
CPI 326 
CPU, introduction 206 
CPU simulator 300 
CPU structure 209,293 
CPU structure, multiple registers 302 
CPU 19 
CRC 533,604 
Cross assembler 232 
Crossbar network 359 
Cross-coupled circuit 103 
CRT 444,458 
CSMA/CD 595 
CTS 593 
Current processor status 
register 376 
Curriculum, hardware 2, 3 
Cursor 437 
Cycle stealing DMA 422 
Cycle time, memory 500 
Cycles per instruction 326 
Cyclic redundancy code 533 
Cylinder, disk 526 
D flip-flop 109 
D flip-flop, use in registers 110 
D flip-flop circuit 110 
DAC 473 
DAC, basic principles 473 
DAC.R-2R 475 
DAC errors 476 
Daisy chaining 421 
Data carried detect 429,431 
Data compressing code 161 
Data density, disk 530 
Data dependency, pipeline 338 
Data direction register, PIA 425 
Data encoding, recording 521 
Data link layer 581,599 
Data link layer, Ethernet 603 
Data movement instructions 218 
Datapath 209 
Data processor, computer 15,16 
Data registers 235 
Data setup time 501 
Data structures 4 
Data transfer, closed-loop 401 
Data transfer, IEEE 488 bus 409 
Data transmission 584 
Datagram 583 
DAV, IEEE bus 409 
DBRA 252 
DC 229,231 
DCE 593 
DDR DRAM 504 
de Morgan's theorem 59,63 
Dead zone 441,443 
Deadlock, multiprocessor 352 
Debounced switch 438 
decibel 587 
Decimal counter 129 
Decimal to binary conversion 150 
Decimal to hexadecimal 
conversion 151 
Decision tree ADC 483 
Defect 97 
Define constant 231 
Define storage 231 
Delay line memory 496 
Delayed jump 338 
Demand mode OS 549 
Demultiplexer 84 
Device-dependent data 488 
Difference engine 7 
Differential control 487 
Differential phase modulation 589 
Differential transmission 411 
Digital circuits, testing 96 
Digital computer, definition 14 
Digital Equipment Corporation 12 
Digital filter 489 
Digital signal processing 486 
Digital signal processing 
applications 488 
Digital to analog converter see DAC 
Digital Works, 172 
Digital Works, binary up counter 130 
Digital Works, clock speed 47 
Digital Works, connecting 
gates 43 
Digital Works, creating a circuit 41 
Digital Works, creating a register 111 
Digital Works, embedded circuits 50 
Digital Works, introduction 40 
Digital Works, logic history 47 
Digital Works, macro 50,52 
Digital Works, pulse generator 131 
Digital Works, recording outputs 46 
Digital Works, running 45,46 
Digital Works, sequence generator 48 
DigitalWorks.tri-stategate 90 
Diode bridge 478 
Direct memory access 422 
Directive, assembler 229 
Direct-mapped cache 346 
Disc capacity 519,526 
Discrete signal 26 
Disk, data density 530 
Disk, head assembler 526 
Disk,Winchester 527 
Disk data structures 533 
Disk drive history 525 
Disk drive principles 524 
Disk drive progress 530 

646 
Index 
Disk interface 529 
Disk mirroring 531 
Disk shock 528 
Displacement, addressing 252 
Displacement, relative 259 
Display, color 457 
Display, organic 451 
Display, raster-scan 445 
Display, TV 446 
Display controller 446 
Distortion, telegraph 9 
Distributed routine 607 
Distributive law 57 
Dithering 460 
Division 194 
Division, 68K 219 
Division, by multiplication 195 
Division, non-restoring 195 
Division, restoring 194 
DIVU 219,241,243 
DMA 413 
Domain, magnetic 517,520 
Don't care condition 77 
Dot matrix printer 454 
Double precision 185 
DRAM control 502 
DRAM families 504 
DRAM organization 501,503 
DRAM read cycle 502 
DRAM reliability 503 
DRAM write cycle 502 
Drawing lines 450 
Drive, audio-visual 529 
Drop-on-demand printing 454 
DS 229 
DTE 593 
Dual-bus multiprocessor 359 
Dual-ported RAM 119 
Dual-slope integrator 484 
DVD 541 
Dvorak keyboard 436 
Dyadic operation 220, 262, 296 
Dye 461 
Dye sublimation printer 461 
Dynamic branch prediction 341 
Dynamic memory 497,501 
Dynamic range 469 
Dynamic shift 221,380 
EASy68K 228,231,266 
Edge sensitive flip-flop 115,116 
Edge-triggered flip-flop, circuit 118 
EDVAC 11 
EEPROM 506 
Effective address 277 
Efficiency, multiprocessor 350 
Elastomeric switch 439 
Electromechanical computer 10 
Electromechanical relay 31 
Electron spin 498 
Electronics 9 
Embedded computer 17 
Emissivity 464 
Encoding criteria 522 
Encoding data, CD 539 
END 231,234 
End around carry 180 
ENIAC 11 
EOI (end or identify) 409 
EOR 221 
EORgate 30,35,36,156 
EPROM 505 
EPROM operation 506 
EQU 231 
Equality tester 37 
Equalizer 587 
Erasable programmable ROM 505 
Error-correcting code 157 
Error-detecting code 156 
Error diffusion, printing 460 
Error function, ADC 470 
Errors, assembly language 234 
Errors in DACs 476 
Ethernet 593 
Ethernet, data link layer 603 
Exception 415,555,556 
Excitation table 132 
Execution, conditional 380 
EXC 218,237 
Exponent 183 
Exponent, biased 183 
Extended addition 219 
Extended data out DRAM 504 
Faggin, Federico 13 
Fast page mode DRAM 504 
FAT 535 
Fault 97 
Fault, OS 555 
Fault, undetectable 97 
FCS field 602 
Feedback ADC 480 
Feedback memory 496 
Ferritecore 11 
Ferromagnetic material 517 
Ferromagnetism 498 
Fetch cycle 21 
Fetch-execute cycle 296,301,320 
Fetch-execute, flip-flop 314 
Fiber optic links 595 
Field (display) 445 
FIFO 262,403 
File allocation table 535 
Finite differences 7 
Firmware 206 
First-level interrupt handler 553 
Flags, condition code 218 
Flags, status 217 
Flash ADC 479,480 
Flash EPROM 506 
Flat panel display 449 
Flip-flop, applications 122 
Flip-flop, fetch-execute 314 
Flip-flop, JK 120 
Flip-flop, T 121 
Flip-flops, summary 121 
Floating, logic state 90 
Floating point addition, 
flowchart 187 
Floating point arithmetic 186 
Floating point examples 188 
Floating point number 181 
Floating point, normalization 183 
Flooding 605 
Floppy disk drive 532 
Flow control in rings 606 
Flowmeter 463 
Flyback, display 445 
Force feedback, joystick 443 
Formatting, disk 533 
Forwarding, internal 339 
Fowler-Nordheim tunneling 506 
Fractional arithmetic 181 
Fractional mantissa 185 
Fractions 152 
Fragmentation, packets 608 
Frame (display) 445 
Frame format, HDLC 600 
Frame format, token ring 606 
Frame pointer 332 
Framing error 432 
Free sector list 534 
Frequency folding 470,471 
Frequency-division multiplexing 585 
Front side bus 405 
Full adder 171 
Full adder, circuit 172 
Full address decoder 510 
Full-duplex 585 
Fully interlocked handshaking 402 
Function code, 68K 420 
Function, parameters 332 
Fusible link 93 
Cain error 477 
Gate, applications 34 
Gate, definition 28 
Gate, transmission element 33 
Gates 25 
Gates, fundamental 28 
Geosynchronous orbit 597 
Global space, windows 331 
Glucose sensor 466 
Glyph 437 

Index 
647 
Gordon, Ada 8 
GPS 18 
Graded index fiber 596 
Gray code 155 
Gray code conversion 156 
Group codes 522 
Guaranteed noise immunity 27 
Gyroscopic mouse 442 
Hadamard code 160 
Half adder 170 
Half adder, circuit 171 
Half-duplex 585 
Hamming code 160 
Hamming distance 154,159 
Handshake, software 414 
Handshaking 402 
Handshaking, PIA 426 
Hangup 403 
Hardware, curriculum 2,3 
Hardware, definition 1 
Hardware, teaching 2 
Harvard, architecture 10 
Harvard, Mark 1 10 
Hazards, pipeline 336 
HDLC 599 
HDLC frame format 600 
HDLC full-duplex 604 
HDLC message exchange 602 
Head assembly, disk 526 
Head, magnetoresistive 530 
Hexadecimal to binary 
conversion 152 
Hierarchical model, operating 
system 548 
Hierarchical table search 566 
High performance drive 529 
High permeability 520 
High-speed multiplication 191 
High-speed transmission 591 
Highpass filter 490 
History, computer 
communications 574 
History, computer 6 
Hit, cache 344 
Hit ratio 345 
Holerith, Herman 10 
Hue 457 
Huffman code 164,574 
Hybrid topology 359 
Hypercube topology 357 
Hysteresis loop 519 
I/O, interrupt-driven 415 
I/O, memory-mapped 413 
I/O fundamentals 412 
I/O programmed 412 
IACK 420 
IAS 11 
IBM, history 12 
IBM 360 series 204 
IC, invention 12 
ID field, disk 533 
IDE 529 
IEEE 488 bus 407 
IEEE 488 bus, configure 410 
IEEE 802.3 packet format 604 
IEEE floating point 183,184 
Image smoothing, multiprocessor 
354 
Immediate access memory 496 
Immediate addressing 215,249 
Immediate instructions 250 
Immediate mode, Berkeley RISC 330 
Immediate operand, ARM 381 
Impulsive noise 590 
Indeterminate state 105 
Index register 369 
Indexed addressing 251,258,370 
Indirect addressing 215,250 
Indivisible instruction 416 
Inkjet, color 460 
Inkjet paper 455 
Inkjet printer 453 
Inner product 353 
Inner product, ARM 385 
Instruction 210 
Instruction, arithmetic 218 
Instruction, bit 221 
Instruction, branch 223 
Instruction, data movement 218 
Instruction, indivisible 416 
Instruction, logical 220 
Instruction, reading 295 
Instruction, representation 146 
Instruction, shift 221 
Instruction, variable length 214 
Instruction decode 335 
Instruction fetch 335 
I nstruction format 211,212 
Instruction format, 68K 213 
Instruction format, ARM 381,384 
Instruction format, MIPS 331 
Instruction formats 366 
Instruction interpretation 308 
Instruction overlap 336 
Instruction privileged 556 
Instruction register 295,299 
Instruction set architecture 203, 365 
Instruction set architecture 
See also ISA 
Instruction set, Berkeley RISC 334 
Instruction types 366 
Instruction usage 328 
Insulating layer, EPROM 506 
Insulation, cables 574 
Integral control 488 
Integrating ADC 484 
Interchangeable media 495 
Interlace 446 
Interface, disk 529 
Interface, memory 499 
Interlocked handshaking 403 
Internal forwarding 339 
International Standards 
Organization 578 
Internet protocol 607 
Internet protocol stack 583 
Internet revolution 14 
Internet 607 
Interpret, instruction 308,310,312 
Interrupt 273,415 
Interrupt, level 7 419 
Interrupt, non-maskable 418 
Interrupt, prioritized 416 
Interrupt-driven I/O 415 
Interrupt acknowledge 420 
Interrupt handler 416,417 
Interrupt priority level 418 
Interrupt request 415 
Interrupt vector 418,420 
Interrupt vector table 421 
IP routing 608 
Irrational number 148 
ISA 2,203 
ISA, definition 204 
ISA bus 405 
ISDN 591 
ISO 7-bit code 147 
Jacquard, Joseph 7 
JK flip-flop 120,132 
JK flip-flop, circuit 122 
J K flip-flop, state diagram 128 
Job control language 548 
Joystick 440,442 
JSR 266 
Jump, delayed 338 
Kalman filter 491 
Karnaugh map, Boolean 
simplification 70 
Karnaugh map, don't care 
condition 77 
Karnaugh maps 67 
Kelvin, Lord 9,530 
Keyboard 436 
Keyboard encoder 439 
Kilbyjack St, Clair 12 
LAN 570 
LAN characteristics 571 
Land, CD 537 
Laser 537 
Instruction set architecture 
See also ISA 
Instruction set, Berkeley RISC 334 
Instruction types 366 
Instruction usage 328 
Insulating layer, EPROM 506 
Insulation, cables 574 

648 
Index 
Laser, color 461 
Laser printer 455 
Latency 404,497 
LCD, color 459 
LCD, transmissive mode 449 
LDC, reflective mode 449 
LCD cell 449 
LEA 237, 252,257, 260,262, 
267, 272 
Leading bit 185 
LED display 444 
Left shift register 124 
Legal addressing modes 254 
Level 2 cache 349 
Level 7 interrupt 419 
Level sensitive clock 115 
Levels of abstraction 204 
Light, coherent 537 
Light, properties 448 
Light, theory 457 
Light pen 443 
Line, cache 346 
Line, scan 445 
Line printer 454 
Linear velocity, CD 540 
Lines, drawing 450 
Link register 376 
Liquid crystal display 447 
Liquid crystal shutter 457 
Liquid crystal, twisted 448 
Listener, IEEE 488 bus 407 
Listing file 233 
Literal operand 300 
Literal operand, data paths 301 
Literal, numeric 249 
Little Endian 235 
Load, RISC 342 
Load control field 319 
Load effective address 237 
Local space, windows 331 
Local storage 329 
Locality of reference 344 
Logic, combinational 25 
Logic, majority 173 
Logic, programmable 91 
Logic operations 248 
Logic values 27 
Logical address 551 
Logical address space 563 
Logical instruction 220 
Logical operations 244 
Logical shift 125 
Long branch 260 
Longword 214 
Look-up table 93,191 
Loom, weaving 7 
Lord Kelvin 575 
LSL 222 
LSR 222,223 
Luminance 457 
M68HC12 368 
MAC 601,604 
Machine, von Neumann 20 
Machine level 206 
Macroinstruction 315 
Magnetic core 519 
Magnetic disk 495 
Magnetic surface recording 515 
Magnetic tape 495 
Magnetism 498 
Magneto-optical disk 541 
Magnetoresistive head 530 
Mainframe 10 
Majority logic 34,173 
Malware 567 
Manchester encoding 523 
Mantissa 183 
Mantissa, fractional 185 
MAR 295 
Mask, definition 30 
Mask-programmed ROM 505 
Master-slave flip-flop 115,117 
Master-slave flip-flop, circuit 
118,122 
Master-slave transmission 601 
Mauchly.John 11 
MBR 295 
Mealy machine 134 
Measurement, position 461 
Measuring light 464 
Measuring performance 326 
Measuring pressure 464 
Mechanical computer 6 
Media access control 601 
Membrane switch 438 
Memory, associative 21 
Memory, cache 344 
Memory, definition 496 
Memory, introduction 21 
Memory, timing diagram 499 
Memory-mapped I/O 413 
Memory access, ARM 383 
Memory address register 295 
Memory and registers 207 
Memory cell 497 
Memory class 495 
Memory control logic 499 
Memory density 497 
Memory density, limits 504 
Memory hierarchy 493,494 
Memory interface 506 
Memory interfacing 499 
Memory management 561 
Memory map 510,513 
Memory organization 507 
Memory performance, cache 345 
Memory price trends 494 
Memory refresh 501 
Memory space matching 556 
Memory technology 496 
Memory to memory 294 
Memory width 235 
Merging bits 538 
Message encapsulation 583 
Message exchange, HDLC 602 
Metastable state 120 
MFM 522 
Microcomputer, invention 12,13 
Microcontroller 377 
Microcontroller families 367 
Microinstruction 298,312 
Microprogram 206,298,316 
Microprogram sequence control 319 
Microprogrammed control unit 315,318 
Microprogramming 328 
Microwave link 597 
MIMD architecture 355 
Minterm 35,68 
MIPS 330 
MIPS instruction format 331 
MISD architecture 355 
Miss ratio 345 
Missing code error 477 
MITS 13 
Mixed logic 32,33,35 
m-line to n-line decoder 511 
MMU 561,65 
Mnemonics, for address 
registers 236 
Modem standards 592 
Modem 429,575 
Modem, high speed 589 
Modified frequency modulation 522 
Modulation 521 
Modulation, QAM 590 
Modulo-2 arithmetic 169 
Monitor, resolution 459 
Moore machine 134 
Morse code 26, 574 
Motherboard 22 
Mouse 441 
MOVEA 236 
MOVEM 265,266,58,560 
MS-DOS 549 
MS-DOS files 535 
MTBF.disk 529 
Multilevel page table 567 
Multiline message, IEEE bus 410 
Multiple buses 401 
Multiplexer 84,111,468 
Multiplexer, application 36 
Multiplexer, Digital Works 50,53 
Multiplexer circuit 34 
Long branch 260 
Longword 214 
Look-up table 93,191 
Loom, weaving 7 
Lord Kelvin 575 
LSL 222 
Memory management 561 
Memory map 510,513 
Memory organization 507 
Logic, majority 173 
Logic, programmable 91 
Logic operations 248 
Logic values 27 
Logical address 551 
Logical address space 563 
Logical instruction 220 
Logical operations 244 
Logical shift 125 
Listing file 233 
Literal operand 300 
Literal operand, data paths 301 
Literal, numeric 249 
Little Endian 235 
Load, RISC 342 
Load control field 319 
Load effective address 237 
Local space, windows 331 
Local storage 329 
Locality of reference 344 
Logic, combinational 25 

Index 
649 
Multiplexing signals 584 
Multiplication 189 
Multiplication, 68K 219 
Multiplication, high speed 191 
Multiplication, negative numbers 191 
Multiplier circuit 62 
Multiply and accumulate 377 
Multiprocessor applications 352 
Multiprocessor categories 356 
Multiprocessor 350 
Multiprocessor, symmetric 357 
Multiprocessor organization 353 
Multitasking 550 
Multitasking, pre-emptive 554 
MULU 219,237,241 
MVN 382 
My listen address 410 
My talk address 410 
NaN 185 
NAND gate 31 
NAND logic 65 
Navigation 6 
N-bit 299 
NDAC, IEEE bus 409 
Negative logic 32 
Negative number 175 
Nematic liquid crystal 448,449 
Nested subroutines 225 
Network interface card 572 
Network layer 581 
Noise 584,590 
Noise, quantization 469 
Noise immunity 27,522 
Non-linear error 477 
Non-maskable interrupt 418 
Non-restoring division 195 
Non-return to zero encoding 522 
NOR gate 31 
NOR logic 65 
Normalization 183 
Normalize 186 
Not a number 185 
NOT gate 31 
Noyce, Robler 12 
NRDF, IEEE bus 409 
Null byte 255 
Number, floatingpoint 181 
Number, natural 148 
Number, signed 175 
Number base 148 
Number base, conversion 150 
Numeric processor 16 
Numerical aperture 537 
Nyquistrate 470 
Octal register 113 
Offset error 477 
Offset, signed 252 
OLED 451 
One address machine 213 
One's complement representation 180 
ON-off control 487 
Op-code interpretation 312 
Open Systems Interconnection 578 
Open-ended transfer 400 
Operand, literal 300 
Operand fetch 335 
Operand field 295 
Operand store 335 
Operating system process 551 
Operating system 4,547 
Operation, dyadic 220 
Operation, subword 214 
Operational amplifier 473,474 
Optical encoder 155 
Optical fiber 595,596 
Optical memory 498,536 
Optical mouse 441 
OR 221 
OR array 94 
OR gate 30 
Orbit, satellite 599 
ORG 230 
Organic display 451 
Organization, multiprocessor 353 
Organization and architecture 205 
Originate/answer 592 
OSI 578 
Output enable 499 
Overflow 179 
Packet switching 581, 583 
Page fault 564 
Page table 564 
Paged memory 552 
Palindrome 387 
Parallel ADC 479 
Parallel adder 173 
Parallel poll, IEEE bus 410 
Parallel to serial converter 124 
Parameter passing 271 
Parameter space, windows 331,332 
Parity codes 158 
Parity error, ACIA 432 
Partial address decoder 508 
Pascal, Bliase 6 
Passband 587 
Passing parameters 271 
Passing parameters by reference 276 
Passive matrix 459 
Pattern sensitivity 96 
PC 295 
PC, introduction 22 
PC bus 404 
PC display 447 
PCI bus 405,406 
PCI chipset 406 
PCI express 405 
PEA 277,279 
Peer subsystem 579 
Penalty, branch 339 
Performance, measuring 326 
Peripheral data register, PIA 424 
Peripheral interface adaptor 423 
Phase change printer 461 
Phase distortion 587 
Phase encoding 523 
Phase modulation 589 
Phosphor 444 
Photocopier 455 
Photodiode 464 
Photoresistor 464 
Physical address 551 
Physical channel 584 
Physical layer 582 
Physical layer, networks 584 
PIA 424 
PIA, handshaking 426 
Picture element 148 
PID 488 
Pigment 461 
Pipeline, data dependency 338 
Pipeline, implementing 341 
Pipeline bubble 337 
Pipeline efficiency 337 
Pipeline hazards 336 
Pipeline stall 337,339 
Pipelining speedup 335 
Pipelining 114,117,327,335 
Pit, CD 537 
Pixel 458 
Plasma cell 450 
Plasma display 444,449 
Plasma display, contrast 450 
Platter, magnetic 524 
Plug and play 411 
Pointer 215,250 
Pointing devices 440 
Polarization, writable CD 540 
Polarizing filter 448 
Polarizing material 448 
Poll/final bit 601 
Polling 419 
Polling loop 415 
Pop 262 
Port 413 
Port, status 419 
Position independent code 259,266 
Position measurement 461 
Position transducer 462 
Positional notation 148 
Positive logic 32 
Postincrementing 253 

650 
Index 
Potentiometric network, DAC 475 
PPP protocol 605 
Precision 182 
Predecrementing 254 
Prediction, branch 340 
Pre-emptive multitasking 554 
Presentation layer 580 
Preset, logic element 110 
Pressure measurement 464 
Principle of duality 59 
Printer 452 
Printer, color 460 
Printer, dot matrix 454 
Printer, inkjet paper 455 
Printer, inkjet 453 
Printer, thermal wax 461 
Printer, thermal 453 
Prioritized interrupt 416 
Prioritizer circuit 37 
Priority encoder 417,480 
Privileged instruction 556 
Process, in operating system 551 
Process, switching 551 
Processor status byte 418 
Product term 94 
Product-of-sums 35 
Program counter 295 
Program counter, ARM 382 
Program counter relative 259 
Program counter relative 
addressing 304 
Program modification 
instructions 328 
Programmable array logic 94 
Programmable logic array 93 
Programmable logic 91 
Programmed I/O 412 
Programs status register 376 
PROM 505 
PROM, address decoder 512 
Propagation delay 39,68 
Proportional control 487 
Propriety standards 583 
Protocol, communications 576 
Protocols, character oriented 605 
Pseudocode 21 
PSTN 570,587 
Public switched telephone 
network 570 
Pull 262 
Pulse generator 107,131 
Punched card 10 
Push 264 
QAM 590 
Quad precision 185 
Quadtree 167 
Quantization 468 
Quantization noise 469 
QWERTY 10,436 
R-2R ladder 475 
Radar 17 
Radio frequency spectrum 597 
Radix point 149 
RAID systems 531 
RAM, cache tag 347 
Ramp feedback ADC 481 
Random access 497 
Random logic control unit 308 
Range 182 
Raster-scan 445 
Rational number 148 
Read access time 500 
Read cycle 499 
Read cycle, DRAM 502 
Reading data, magnetic 
recording 521 
Read-only memory 91,497 
Read-only semiconductor 
memory 505 
Real-rime OS 549 
Receive sequence number, 
HDLC 602 
Recording density 520 
Recursive filter 489 
Redundancy 98 
Redundant bits 157,163 
Reed relay 436 
Reference model for OSI 578 
Refreshing RAM 502 
Register 207 
Register, address 236 
Register, CCR 218 
Register, index 369 
Register, link 376 
Register, shadow 376 
Register, using D flip-flops 110 
Register selection, PIA 425 
Register set, 68K 211,217 
Register sets 365 
Register to register architecture 
213,294 
Register transfer language 208 
Register window 330 
Register windows, parameters 332 
Registers, 68K 234 
Registers, ARM 375 
Registers, windowed 333 
Relative addressing 259 
Relative branch 260 
Relay 31 
Relay, reed 436 
Remnant magnetism 519 
Request to send 429 
Reset, logic element 110 
Resolution, monitor 459 
Restoring division 194 
Return to zero encoding 522 
Return from exception 558 
Return to bias recording 522 
Return, subroutine 225 
Reverse subtract instruction 377 
RGB 457,460 
Ring, network 573 
Ring topology 357 
Rings, flow control 606 
Ripple counter 129 
Ripple-carry adder 175 
RISC 210,212,327 
RISC, Berkeley instruction set 334 
RISC, invention 14 
RISC characteristics 329 
RLLcodes 522 
ROL 222 
ROR 222 
Rotary head positioner 525 
Rotate through carry 222 
Rotation sensor 464 
Rounding 188 
Router 572 
Routine, network 606 
Routing techniques 604 
Routing, IP 608 
Row address strobe 501 
ROXL 223 
ROXR 223 
RR (receiver ready) 602 
RS flip-flop 103 
RS flip-flop, clocked 108 
RS flip-flop, NAND gates 106 
RS flip-flops, application 107 
RS flip-flop truth table 105 
RS232C 593 
RTE 558,208,295,298, 225, 556 
RTS 593 
Run length limited encoding 523 
Sample and hold 468,473 
Sample and hold circuit 478 
Sample and hold timing 479 
Sampling signals 469 
Sampling theorem 470 
Satellite 597 
Saturation 457 
Sawtooth waveform 445 
SBC 22,219 
S-bit,68K 555 
Scalar product 253 
Scheduler 558 
Scheduler, OS 553 
Scheduler program 559 
Schickard, William 7 
Schockley, William 12 

Index 
651 
Scientific notation 182 
SCSI 529 
Secondary storage 515 
Sectors, disk 524,533 
Selenium drum 455 
Self-clocking 522 
Semiconductor memory 498 
Send sequence number, HDLC 602 
Sensitive path test 97 
Sequence control, ARM 381 
Sequence control, microprogram 319 
Sequence detector 136 
Sequencer 312 
Sequential circuit operation 104 
Sequential logic 101 
Sequential logic, definition 102 
Serial access 497 
Serial adder 173 
Serial data transmission 584 
Serial interface 428 
Serial poll, IEEE bus 410 
Serial to parallel converter 124 
Servomechanism 525 
Session layer 580 
Set, cache 346 
Set, flip-flop 104 
Set associative-mapped cache 349 
Settling time 528 
Seven-segment decoder 79 
Shadow mask 458 
Shadow register 376 
Shannon, Claude 56 
Shift, arithmetic 125 
Shift,ARM 378 
Shift, circular 125,222 
Shift, dynamic 221 
Shift, logical 125 
Shift instruction 221 
Shift key 436 
Shift operations 244 
Shift register 122 
Shift register, designing 125 
Shift register, JK 123 
Shift register, left shift 124 
Shock, disk 528 
Sholes, Christopher 10 
Sign and magnitude, mantissa 183 
Sign and magnitude 
representation 176 
Sign extension, 68K 214 
Signal acquisition 467 
Signal to noise ratio 469 
Signals and modulation 586 
Signed multiplication 190,219 
Signed numbers 175 
Signed offset 252 
SIMD architecture 353 
Simulating a CPU 300 
Simulator 239 
Simulator, 16 bit 304 
Simulator, 8-bit 301 
Sine wave 587 
Single bus CPU 308 
Single precision 185 
Single-mode fiber 596 
SISD architecture 353 
Skew, clock 116 
Slew rate 479 
Soft error 504 
Soft magnetic material 520 
Software, definition 1 
SPARC 330,333 
SPEC 326 
Special purpose codes 153 
Special purpose logic 83 
Speedup, multiprocessor 350,351 
Speedup, pipelining 335 
Speedup ratio, cache 345 
Spread spectrum technology 598 
Sprite 148 
Stack 254,262,272 
Stack, 68K supervisor state 556 
Stack, 68K 263 
Stack, context switching 552 
Stack, subroutine 266 
Stack machine 263,366 
Stack pointer 264 
Stall, pipeline 337,339 
Standards 577 
Standards, cables 594 
Standards, modem 592 
Standards and the OSI model 582 
Standards bodies 578 
Star network 573 
Star topology 357 
State diagram 128,136 
State diagram, 68K 556 
State diagram, multitasking 
system 552 
State machines 134,136 
State table 138 
Static branch prediction 341 
Static memory 496,497 
Statistical encoding 165 
Status byte 414 
Status byte, 68K 418 
Status flags 217 
Status register 229 
Status register, ACIA 431 
Steering gates 109 
STOP 229 
Storage, local 329 
Store, RISC 342 
Stored computer, overview 19 
Strain gauge 462 
Strings 255 
Strings, comparing 256 
Strings, removing spaces 257 
Strowger 9 
Structure modification, memory 496 
Stuck-at fault 97 
SUBA 236 
Subroutine 225,266 
Subroutine, ARM 382 
Subroutine nesting 331 
Subtractive colors 460 
Subword operation 214 
Successive approximation ADC 482 
Sum-of-products 35 
Sum-of-products, Karnaugh map 71 
Superparamagnetism 531 
Superscalar 326 
Supervisor stack 238,264 
Supervisor state 555, 556 
Surface recording 520 
SVGA 448 
SWAP 220,237 
Switch 437 
Switch, debounced 438 
Switch, elastomeric 439 
Switch, electromechanical 31 
Switching processes 551 
Switching states, 68K 555 
SXCA 448 
Symbolic name 296 
Symmetric multiprocessor 357 
SYN character 605 
Synchronizing signals 586 
Synchronous counter 132 
Synchronous DRAM 504 
Synchronous system 114 
System byte, 68K 555 
System stack 264 
System/360 12 
T flip-flop 121 
Table search 556 
Tachometer 462 
Talker, IEEEE 488 bus 407 
Task control block 553 
TCP/IP 607 
Telegraph 8 
Telegraph distortion 9 
Telephone 9 
Telephone network, origins 575 
Templates, control structures 246 
Ten's complement arithmetic 176 
Test equivalence instruction 378 
Testing digital circuits 96 
The last mile 591 
Theorems, of Boolean algebra 56 
Thermal printer 453 
Thermal wax printer 461 
Thermistor 463 

652 
Index 
Thermocouple 463 
Thermoelectric effect 463 
Thermoelectric junction 463 
Thin film transistor 460 
Three address instruction 211 
Three-wire handshake 408 
Time-division multiplexing 585 
Time-to-live, routing 608 
Timing delay 113 
Timing diagram, memory 499 
Timing diagram, static Ram 500 
Timing pulse generator 313 
Token rings 606 
Toner 455 
Topology, bus 356 
Topology, cluster 360 
Topology, hybrid 359 
Topology, hypercube 357 
Topology, ring 357 
Topology, star 357 
Topology, unconstrained 356 
TOS 262 
Total internal reflection 595 
Track structure 533 
Track 524 
Trackball 441,443 
Tracking ADC 482 
Tracking converter 482 
Transaction, protocol diagram 401 
Transatlantic cable 574 
Transducer 467 
Transducer, position 462 
Transistor, invention 12 
Transmission control protocol 609 
Transmission delay, satellite 598 
Transport layer 580 
TRAP 559 
Tree, binary 360 
Trellis 165 
Trigonometry 6 
Triple modular redundancy 17 
Tri-stage logic 87,113 
Truncation 188 
Truth table, definition 29 
TST 256 
TV display 446 
Twisted liquid crystal 448 
Twisted pair 592 
Two address machine 213 
Two's complement, alternative 
view 179 
Typewriter 10 
Unbiased error 188 
Unconditional branch 223 
Unconstrained technology 356,572 
Undetectable fault 97 
Unicode 148 
Uniline message, IEEE bus 410 
Unit distance code 154 
Unlisten 410 
Unnumbered frame, SDLC 601 
Unpacking 186 
Unsigned multiplication 190 
Unweighted code 154 
Up-counter 128 
USB serial bus 411 
User microprogrammed 320 
User stack 238,264 
User stack pointer 558 
UXVCA 448 
Variable length code 164 
Variables, local 329 
V-bit 299 
Vector, interrupt 418,420 
VGA 448 
VHDL 96 
Video DAC 447 
Video display card 447 
Virtual circuit 583 
Virtual memory 563 
Visible register 208 
Voice coil actuator 525 
Volatile memory 497 
von Neumann, John 11,20 
von Neumann bottleneck 210 
von Neumann machine 209 
Voting network 17,34 
VRAM 447 
WAN 580 
White noise 590 
Wi-Fi 600 
Winchester disk 527 
Window, register 330,333 
Windows, history 550 
Wordlengths, typical 146 
Workstation 15 
Writable CD 540 
Write cycle 500 
Xerography 455 
XGA 448 
Z transform 489 
Z-bit 299 
Zero address machine 213,366 
Zilog 13 
Zoning 528 
Zuse, Konrad 10 

The CD 653 
The Software Contained on the CD 
The enclosed CD contains four major items of software, all of 
which run on IBM PCs and their clones. I have tested the 
software on several PCs under Windows 98 for the third edition 
and under Windows XP for this fourth edition. One item runs 
only under DOS. 
• A 68000 processor DOS-based cross-assembler and simulator 
• A 68000 processor Windows-based editor, cross-assembler 
and simulator 
• A digital logic simulator 
• A simulator for the ARM microprocessor 
• Documentation for the 68000 processor family 
These items are in separate directories and have appropriate 
readme files. You also need Adobe Acrobat Reader to view some 
of the information such as the Motorola and ARM's user 
manuals. The CD also contains copies of the Adobe Acrobat 
Reader that you can install if you do not already have it. 
IT IS IMPORTANT THAT YOU APPRECIATE THAT NONE 
OF THE SOFTWARE IS OWNED BY OXFORD UNIVERSITY 
PRESS. ALL THE SOFTWARE WAS KINDLY SUPPLIED BY 
THIRD PARTIES FOR USE BY THE READERS OF THIS 
BOOK. 
THIS SOFTWARE IS SUBJECT TO THE INDIVIDUAL 
CONDITIONS 
STATED 
BY 
THE 
APPROPRIATE 
COPYRIGHT HOLDERS. 
THE SOFTWARE HAS BEEN SUPPLIED TO OUP ON THE 
CONDITION THAT IT IS NOT SUPPORTED. 
ONE ITEM OF SOFTWARE ON THE CD, WINZIP, IS 
SUPPLIED AS A DEMONSTRATION COPY AND MAY NOT 
BE USED FOR MORE THAN 21 DAYS WITHOUT PAYMENT. 
This software is required only if you cannot unzip the ARM 
development software. 
The four directories on the CD containing the above items are 
• 68Ksim 
• Digital 
• ArmSim 
• 68Kdocs 
• Easy68K_4ed 
I suggest that you copy 68Kdocs and 68Ksim to your hard disk. 
The DOS-based 68K simulator software simply has to be copied 
to your system and does not require any installation procedure. 
You simply run the appropriate X68K.EXE or E68K.EXE file 
from your DOS prompt. The Windows-based 68K simulator 
has to be installed. 
NOTE When I tested the DOS-based 68K simulator I found 
that some of the demonstration files had become "read-only" in 
the transfer to the CD. This means that you will get an error 
message when you try and assemble them or run them. You can 
solve their problem by changing the attribute from read-only to 
read/write, this problem affected only the demonstration/test 
files. 
NOTE The ARM software also includes a substantial amount 
of documentation including the ARM Reference Manual in 
the subdirectory PDF. Note also that I have already unzipped the 
ARM software on the CD and you will be able to find the 
documentation in ArmSim\ARM202U\PDF. The documentation 
goes well beyond the level of this text and has been included to 
allow readers to delve more deeply into the ARM's architecture. 
The digital logic simulator, Digital Works, must be installed on 
your system. Similarly, you must unzip the ARM logic simulator 
files and install them on your hard disk. 
The following is the testing schedule that was used to test this 
CD. Further information about the packages can be found in 
the CD's files and in the body of the text. 
OUP have set up an online resource centre to support this 
book. Its URL is: www.oxfordtextbooks.co.uk/orc/clements4e 
I can be contacted by email at a.clements@tees.ac.uk 
CD Testing Schedule 
This "testing schedule" has been devised to allow my "pre-
release testers" to examine the software on this disk before it is 
released with Principles of Computer Hardware. It should also 
help other readers to get the software going. This software 
contains third-party utilities, simulators, and documentation 
(in Adobe's Portable Document Format). 
1. Read the Readme.txt file in the root directory. 
2. Install Adobe Acrobat 
Reader. The 
CD 
contains 
AdbeRdr70_enu_full.exe that will install Version 7 on a PC 
with Windows XP. 
You can also install Version 4 Adobe Acrobat Reader (for 
compatibility with the 3rd edition of the book) of using 
one of the two files ar40eng.exe or rs40eng.exe. The former 
is a Windows 95 version and the latter a Windows 98 
version. 
3. If you have Adobe acrobat Reader already installed or have 
just installed it, open the 68Kdocs directory and click on the 
68Kprm.pdf file. This should enable you to read Motorola's 
definitive document on the 68000 family. 
4. Test the 68K simulator. Open directory 68Ksim and click on 
the pdf document sim.pdf. This will open the guide to the 
use of the simulator software in Adobe Acrobat Reader. 
5. Examine the other .txt files in directory 68Ksim. 
6. Use an ASCII text editor to create a file, for example, 
TEST.X68 that contains a minimal 68K assembly language 
program. (You can use one of the 'demo' files provided on 
the CD.) Go into the DOS command-line mode on your PC 
and assemble the program with the command line X68K 
TEST-L. 
Note that you MUST not provide the extension .X68 or the 
assembly with fail. The extension, -L, is used to generate a 
listing file. That is, X68K TEST -L will generate TEST.BIN 
(if assembly is successful) and TEST.LIS. 

654 
The CD 
If assembly succeeds (i.e. there are no errors in your source 
code), invoke the simulator from the DOS command line 
with the command E68K TEST. 
You can test the simulator (if you have read the 
documentation) and then exit by using the Q (quit) 
command. This takes you back to the DOS command level. 
If you run a program that puts you in an infinite loop, you 
can get out by hitting the escape key. 
NOTE that this directory contains several test files (i.e. 
Demol.bin and Demo2.x68). You can assemble Demol.x68 
with the command X68K DEMOl -L. You can then run 
the binary file with E68K DEOM1. To execute a program in 
the simulator type GO followed by a carriage return (i.e., the 
"enter" key). 
7. Test Digital Works. Open the directory Digital and double 
click on dw20_95.exe to install Digital Works into the 
directory of your choice. If you change to the directory where 
Digital Works is located, double clicking on Digital.exe will 
run Digital Works. Note that Digital Works also puts a 
command on the Windows 98 Start/Programs menu. 
The simplest way of testing Digital Works is to select a gate 
my moving the cursor to it and then clicking on that gate's 
icon. Then move the cursor to the work area and then click 
again. A copy of the gate should be moved to the work area. 
8. Test the ARM simulator. This is the most complex software 
on the CD and, for the purpose of The Principles of 
Computer Hardware you will be using only a fraction of its 
capabilities. Note that the package includes considerable 
documentation in Adobe's PDF format. 
You must first install the ARM software. I have provided 
202u_w32_vl.zip which is the package I downloaded from 
ARM's university web site. The directory ARM202U was 
created by unzipping 202u_w32_vl .zip. 
When I tested this package, I first unzipped the files to C:\ 
which created the directory 'C:\ARM202U' containing the 
unzipped files and subdirectories. I then changed the name 
of the directory to 'C:\ARM200' to suit the software's initial 
default paths to its \B1N and \L1B directories. 
The following provides an introduction to testing this 
software: 
a. Put the file clements.s (the test program written by 
me 
and 
located 
in 
directory 
ARMsim) 
in 
C:\ARM200\BIN. 
b. Run the simulator package from Windows by clicking on 
Apm.exe in the \BIN directory. 
c. Use the Project pull-down menu and select 'New Project' 
d. Give the project a name and save the project in the 
C:\ARM200\BIN directory. This will create an 'Edit 
Project' window that asks for files to include. 
e. If you have created a source file with the extension .S 
(e.g. CLEMENTS.S) add it to the project and click OK. 
I have created CLEMENTS.S for you to test. You should 
have copied this to the \BIN directory. 
f. Note that the system needs to know where the compiler, 
etc., is. Click on 'Options' and select 'Directories'. You 
will probably have to give the path of the compiler, etc. 
on your own system if you have not used the path 
C:\ARM200\BIN. 
g. From the Project pull-down menu select 'Build 
name.APJ', where 'name' is the name of the project. You 
should get a 'Build complete' message if your source 
code had no errors. 
h. From the Project pull-down menu select 'Debug 
name.APJ' to enter the debugger/simulator mode. 
i. In the debugger you can use the 'View' pull-down menu 
to see registers, etc. Select the 'User registers' menu. This 
system loads the program at 8080 hexadecimal. Change 
the PC to 8080 by clicking on it. 
j. You can now run the code line-by-line with the step into 
command (one of the icons on the debugger toolbar). 
k. Note—from the "Project" pull-down menu you can edit 
your source code. 
Test the 68K Windows-based simulator. This is a system 
created by a team led by Chuck Kelly. The software is available 
in the public domain and I would suggest that you obtain the 
latest version from the Internet at www.monroeccc.edu/ 
ckelly/easy68k.htm. The version in this CD has been included 
to ensure that all readers have a copy of this software. 
a. Click on SetupEASy68K/exe to install EASy68K. 
Installation puts the software in a sub-directory 
EASy68k (we've created this directory on the CD). 
b. The sub-directory EASy68k contains several files 
including EDIT68k.exe and SIM68k.exe. If you double-
click on EDIT68k.exe, you will invoke a text editor that 
uses a template for a 68K assembly language program. 
You can type your 68K assembly language into this 
template and save it. The EDIT68k program is intuitive 
to use and has a 'Help' function. 
c. You can assemble a program from within the editor. 
Select the 'Project' tab in the editor window to get the 
'Assemble source' option. Left-click on this and your 
program will be assembled. If you make any errors, you 
will have to re-edit the source. If there are no errors, you 
can select the 'Close' button and exit, or the 'Execute' 
button to enter the simulator. 
d. If you select the 'Execute' button, the 68K simulator is 
invoked. Now you can run the code to its completion or 
execute it line-by-line. The simulator displays the 68K's 
register and you can also open memory or stack 
windows. The F7 function key can be used to execute 
code an instruction at a time. 
Last modified on 14 July 2005 
9. 
Lasi 

The CD 
655 
CD-ROM conditions of use and copyrights 
Please read these terms before proceeding with the CD installa-
tion. By installing the CD you agree to be bound by these terms, 
including the terms applicable to the software described below. 
The enclosed CD contains four major items of software, all of 
which run on IBM PCs and their clones. One item runs only 
under DOS. 
• A 68000 cross-assembler and simulator 
• A digital logic simulator 
• A simulator the ARM microprocessor 
• Documentation for the 68000 family 
These items are in separate directories and have appropriate 
"readme" files. You also need Adobe Acrobat Reader to view 
some of the information such as Motorola and ARM's user man-
uals. The CD also contains a copy of the Adobe Acrobat Reader 
that you can install if you do not already have it. 
The materials contained on this CD-ROM have been supplied by 
the author of the book. Whilst every effort has been made to 
check the software routines and the text, there is always the pos-
sibility of error and users are advised to confirm the information 
in this product through independent sources. 
Alan Clements and/or his licensors grant you a non-exclusive 
licence to use this CD to search, view and display the contents of 
this CD on a single computer at a single location and to print off 
multiple screens from the CD for your own private use or study. 
All rights not expressly granted to you are reserved to Alan 
Clements and/or his licensors, and you shall not adapt, modify, 
translate, reverse engineer, decompile or disassemble any part of 
the software on this CD, except to the extent permitted by law. 
These terms shall be subject to English laws and the English 
courts shall have jurisdiction. 
THIS CD-ROM IS PROVIDED AS IS' WITHOUT WARRANTY 
OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT 
LIMITED TO IMPLIED WARRANTIES OF SATISFACTORY 
QUALITY OR FITNESS FOR A PARTICULAR PURPOSE. IN 
NO EVENT SHALL ANYONE ASSOCIATED WITH THIS 
PRODUCT BE LIABLE FOR ANY DIRECT, INDIRECT, 
SPECIAL, CONSEQUENTIAL, OR INCIDENTAL DAMAGES 
RESULTING FROM ITS USE. 
THIS SOFTWARE IS SUBJECT TO THE INDIVIDUAL 
CONDITIONS STATED BY THE APPROPRIATE COPYRIGHT 
HOLDERS WHICH ARE GIVEN BELOW AND ON THE 
CD WALLET COVER. 
THE SOFTWARE IS NOT SUPPORTED. 
ONE ITEM OF SOFTWARE ON THE CD, WINZIP, IS 
SUPPLIED AS A DEMONSTRATION COPY AND MAY NOT 
BE USED FOR MORE THAN 21 DAYS WITHOUT PAYMENT. 
This software is required only if you cannot unzip the ARM 
development software. 
DIGITAL WORKS 95 VERSION 2.04 is © John Barker 2000. 
TERMS OF USE: Digital Works 95 version 2.04 (The Product) 
shall only be used by the individual who purchased this book. 
The Product may not be used for profit or commercial gain. The 
Product shall only be installed on a single machine at any one 
time. No part of the Product shall be made available over a 
Wide Area Network or the internet. The title and copyright in all 
parts of the Product remain the property of David John Baker. 
The Product and elements of the Product may not be reverse 
engineered, sold, lent, displayed, hired out or copied. It shall only 
be installed on a single machine at any one time. 
M6800PM/AD—MOTOROLA M68000 FAMILY PROGRAMMERS 
REFERENCE MANUAL Copyright of Motorola. Used by 
permission. 

656 TheCd 
Schedule 2 
Shrinkwrap Agreement 
End User Licence Agreement for the ARM Software Development Toolkit 
2.02u Version 2 
IMPORTANT READ CAREFULLY PRIOR TO ANY INSTALLATION OR 
USE OF THE SOFTWARE 
You are in possession of certain software ("Software") identified in the 
attached Schedule 1. The Software is owned by ARM Limited ("ARM") or its 
licensors and is protected by copyright laws and international copyright 
treaties as well as other intellectual property laws and treaties. The Software is 
licensed not sold. You were advised, at the time that the Software was provided 
to you, that any use, by you, of the Software will be regulated by the terms and 
conditions of this Agreement ("Agreement"). 
ACCEPTANCE 
If you agree with and accept the terms and conditions of this Agreement it 
shall become a legally binding agreement between you and ARM Limited and 
you may proceed to install, copy and use the Software in accordance with the 
terms and conditions of the Agreement. 
REJECTION AND RIGHT TO A REFUND 
If you do not agree with or do not wish to be bound by the terms and conditions 
of this Agreement you may NOT install, copy or use the Software. 
TERMS AND CONDITIONS 
1. Software Licence Grant 
ARM hereby grants to you. subject to the terms and conditions of this 
Agreement, a non-exclusive, non-transferable, worldwide licence, solely for 
non-commercial purposes, to; 
• use and copy the Software identified in Schedule 1 Part A and Schedule 1 
PartB; 
• incorporate into software application programs that you develop, the 
Software identified in Schedule 1 Part B; and 
• use the documentation identified in Schedule 1 Part C. 
2. Restrictions on Use of the Software 
Except for the making of one additional copy of the Software for backup pur-
poses only, copying of the Software by you is limited to the extent necessary 
for; (a) use of the Software on a single computer; and (b) incorporation into 
software application programs developed by you as permitted under the 
terms of this Agreement. 
Except to the extent that such activity is permitted by applicable law you shall 
not reverse engineer, decompile or disassemble any of the Software identified 
in Schedule I Part A. If the Software was provided to you in Europe you shall 
not reverse engineer, decompile or disassemble any of the Software identified 
in Schedule 1 Part A for the purposes of error correction. 
You shall only use the Software on a single computer connected to a single 
monitor at any one time except that you may use the Software from a common 
disc running on a server and shared by multiple computers provided that 
one authorised copy of the Software has been licensed for each computer 
concurrently using the Software. 
You shall not make copies of the documentation identified in Schedule 1 Part B. 
You acquire no rights to the Software other than as expressly provided by this 
Agreement. 
You shall not remove from the Software any copyright notice or other notice 
and shall ensure that any such notice is reproduced in any copies of the whole 
or any part of the Software made by you. 
3. No Support 
For the avoidance of doubt, this license to use the Software does not provide you 
with any right to receive any support and maintenance in respect of the Software. 
4. Restrictions on Transfer of Licensed Rights 
The rights granted to you under this agreement may not be assigned, sublicensed 
or otherwise transferred by you to any third party without the prior written 
consent of ARM. You shall not rent or lease the Software. 
5. Limitation of Liability 
THE SOFTWARE IS LICENSED "AS IS". ARM EXPRESSLY DISCLAIMS ALL 
REPRESENTATIONS, WARRANTIES, CONDITIONS OR OTHER TERMS, 
EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION THE 
IMPLIED WARRANTIES OF NON-INFRINGEMENT, SATISFACTORY 
QUALITY AND FITNESS FOR A PARTICULAR PURPOSE. 
TO THE MAXIMUM EXTENT PERMITTED BY APPLICABLE LAW, IN 
NO EVENT SHALL ARM BE LIABLE FOR ANY INDIRECT, SPECIAL, 
INCIDENTAL OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF 
PROFITS) ARISING OUT OF THE USE OR INABILITY TO USE THE 
SOFTWARE WHETHER BASED ON A CLAIM UNDER CONTACT, TORT 
OR OTHER LEGAL THEORY, EVEN IF ARM WAS ADVISED OF THE 
POSSIBILITY OF SUCH DAMAGES. ARM does not seek to limit or exclude 
liability for death or personal injury arising from ARM's negligence and 
because some jurisdictions do not permit the exclusion or limitation of 
liability for consequential or incidental damages the above limitation relating 
to liability for consequential damages may not apply to you. 
6. Term and Termination 
This Agreement shall remain in force until terminated by you or by ARM. 
Without prejudice to any of its other rights if you are in breach of any of the 
terms and conditions of this Agreement then ARM may terminate the 
Agreement immediately upon giving written notice to you. 
You may terminate this Agreement at any time. 
Upon termination of this Agreement by you or by ARM you shall stop using 
the Software and destroy all copies of the Software in your possession together 
with all documentation and related materials. 
The provisions of Clauses 5,6 and 7 shall survive termination of the Agreement 
7. General 
This Agreement is governed by English Law. 
This is the only agreement between you and ARM relating to the Software and 
it may only be modified by written agreement between you and ARM. This 
Agreement may not be modified by purchase orders, advertising or other 
representation by any person. 
If any Clause in this Agreement is held by a court of law to be illegal or 
unenforceable the remaining provisions of the Agreement shall not be 
affected thereby. 
The failure by ARM to enforce any of the provisions of this Agreement, unless 
waived in writing, shall not constitute a waiver of ARM'S rights to enforce 
such provision or any other provision of the Agreement in the future. 
Use, copying or disclosure by the US Government is subject to the restrictions 
set out in subparagraph (c)(1)(H) of the Rights in Technical Data and 
Computer Software clause at DFARS 252.227 7013 or subparagraphs (c)(1) 
and (2) of the Commercial Computer Software - Restricted Rights at 48 CFR 
52.227-19, as applicable. 
You agree that you will not export or re-export the Software to any country, 
person or entity or end user subject to U.S.A. export restrictions. Restricted 
countries currently include, but are not necessarily limited to, Cuba, Iran, 
Iraq, Libya, North Korea, Syria and the Federal Republic of Yugoslavia (Serbia 
and Montenegro, U.N. Protected Areas and areas of Bosnia and Herzegovina 
under the control of Bosnian Serb forces). 


Alan Clements is a Professor in the 
School of Computing at the University 
of Teesside, Middlesborough, UK, and is 
a member of the IEEE Computer Society 
Board of Governors. In 2002 he was 
B
arded a National Teaching Fellowship 
his contribution to education, and 
; also received the 2002 
dergraduate Teaching Award from 
the IEEE Computer Society. 
t
^Hl 
online resource centre 
www.oxfordtextbooks.co.uk/orc/ 
clements4e/ 
The new Online Resource Centre is free 
to adopters and contains: 
• Figures from the book available to 
download, to facilitate lecture 
preparation 
• Solutions to problems featured in 
the book, to aid formative learning 
• Multiple choice question test bank 
for several chapters, to facilitate 
assessment of students' learning 
Cover images: Joe Tree / Alamy; Medioimages / 
Alamy; Inmagine / Alamy; Thinkstock / Alamy; 
Photodisc 
PRINCIPLES OF 
COMPUTER HARDWARE 
Computer technology pervades almost every aspect of our life: from the 
cars that we drive, to the mobile phones that we use to communicate; from 
the digital cameras that capture images of the world around us, to the laser 
printers that turn image into picture. Yet these enabling technologies could 
never have been developed without an understanding of fundamental 
components and systems, which lie at their heart. 
Principles of Computer Hardware explores the fundamentals of computer 
structure, architecture, and programming that underpin the array of 
computerized technologies around which our lives are now built. 
Always putting educational value first, Principles of Computer Hardware 
uses the 68K processor as a powerful teaching and learning tool, placing 
educational value at the book's core. With the clarity of explanation and 
engaging style for which Alan Clements is renowned, the book draws the 
student in to the core of the subject, to foster an in-depth understanding 
from which more specialized study can then extend. 
New to this edition: 
• A revised structure leads the student through the heart of the subject in 
a more progressive manner, to help them master the subject more easily 
• Enriched pedagogy, including boxes, additional examples, and two colour 
artwork, makes learning more stimulating, enjoyable, and effective 
• The new two colour text design enlivens the book's presentation to 
fully engage students, and enhances the educational value of the 
illustrations presented 
• New boxes throughout augment the content of the main text, and 
relate key concepts to familiar contexts, such as the PC 
• Advanced topics from chapter 12 of the previous edition now integrated 
throughout the book, offering deeper insights where the topics are 
first encountered 
The learning experience is enriched still further with a free CD-ROM, 
providing resources to enable students to engage with the subject in a 
hands-ron manner: 
• A Windows-based simulator for the student to explore the design of 
digital circuits 
• Windows and DOS-based 68K simulators for students to investigate 
the operation of the 68K processor 
• An ARM simulator for students to write programs for a RISC processor, 
and run them on a PC 
OXFORD 
UNIVERSITY PRESS 
v. vt.oup.com 


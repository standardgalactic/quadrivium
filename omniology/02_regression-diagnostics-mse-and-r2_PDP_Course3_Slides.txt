Python Data Products
Course 3: Making Meaningful Predictions from Data
Lecture: Course Introduction

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Course introduction: Week 1
In this course we will cover...
• Evaluation metrics for regression and 
classification algorithms
• How to select the right evaluation metrics under different 
conditions
e.g. which of these bags contains a weapon?

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Course introduction: Week 2
In this course we will cover...
• Predictive pipelines: how do we build models that will 
generalize well to new data?
Training data
Validation data
Testing data
Used to select
Used to select
Only used once, to 
evaluate the model

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Course introduction: Week 3
In this course we will cover...
• Implementation of predictive pipelines in Python
• Rules and guidelines for model selection using 
training/validation/test sets

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Learning outcomes
By the end of this course you should be able to:
• Implement robust predictive pipelines in Python, and deal 
with issues surrounding evaluation metrics, model 
complexity, and generalization

Week 1
Regression and 
Classification Diagnostics

Python Data Products
Course 3: Making Meaningful Predictions from Data
Lecture: Introduction

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Learning objectives
In this lecture we will...
• Introduce the problem of evaluating machine 
learning systems
• Outline the content of this course

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Challenges in model evaluation
• How can we evaluate regression and classification 
models?
• What does it mean for a model to be "good"?
• Does the meaning of "good performance" change 
in different contexts?
• How can we compare the performance of 
different models?
• If a model works well on our training data, how 
can we ensure that it will still work well on 
unseen data?

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Challenges in model evaluation
• How can we evaluate regression and classification 
models?
• E.g. should we consider the average error? The average squared 
error? The accuracy? The number of false positives? etc.
• What is the motivation behind (and the consequences of) these 
different choices?

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Challenges in model evaluation
• What does it mean for a model to be "good"?
• Does the meaning of "good performance" change 
in different contexts?
• E.g. consider running a classifier that recognizes fingerprints, versus 
one that detects pedestrians, versus one that detects weapons in 
luggage
• How can we design a classifier that targets one of these situations?

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Challenges in model evaluation
• How can we compare the performance of 
different models?
• Given two classifiers (or regressors) how can we decide which of 
them is better?
• Is it enough to select whichever classifier has the highest accuracy?
• How can we make other decisions, e.g. if we were using a one-hot 
encoding to represent time, should we do so at the granularity of 
weeks, or months?

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Challenges in model evaluation
• If a model works well on our training data, how 
can we ensure that it will still work well on 
unseen data?
• E.g. a higher level of granularity (e.g. days rather than weeks) for 
our one-hot encoding might always lead to better performance on 
our training data, but does that mean it is the better model?
• If not, how can we develop a training regime that corrects for this 
issue?

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Summary of concepts
• Introduced the main problems and challenges in 
evaluating regression and classification models
• Outlined the remainder of this course

Python Data Products
Course 3: Making Meaningful Predictions from Data
Lecture: Recap on mathematical notation

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Learning objectives
In this lecture we will...
• Revise the mathematical notation necessary to cover 
the basics of evaluation and regularization

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
: mean (average) of a vector
: Variance of a vector
: vector norm
(in general                             )
: regularization tradeoff parameter
Notation for evaluation and regularization

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Summary of concepts
• Covered basic notation for evaluation and regularization

Python Data Products
Course 3: Making Meaningful Predictions from Data
Lecture: Motivation behind the MSE

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Learning objectives
In this lecture we will...
• Present the most common evaluation measures used 
for regression models (the Mean Squared Error)
• Motivate the choice of this particular error measure 
using statistics and probability

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Regression diagnostics
Q: How should we evaluate our 
regression model?

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Regression diagnostics
Height
Weight
40kg
120kg
130cm
200cm
Q: Can we find a line that (approximately) fits the data)?
Error between correct 
value and prediction

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Concept: Mean Squared Error
Mean-squared error (MSE)

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Regression diagnostics
Q: Why MSE (and not mean-absolute-
error or something else)

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Regression diagnostics
label = prediction + error

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Regression diagnostics

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Summary of concepts
• Understand the motivation behind the MSE in terms 
of probability
• Understand the notion of "error distributions"
• (at a high level) understand the relationship between 
likelihood (probability) and error (prediction)
On your own...
•
Compute MSE and related statistics 
(like Mean Absolute Error) and 
compare cases where the errors are 
high and low

Python Data Products
Course 3: Making Meaningful Predictions from Data
Lecture: Regression Diagnostics: MSE and R^2

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Learning objectives
In this lecture we will...
• Present the R^2 statistic
• Explain the relationship between MSE and 
mean+variance

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Regression diagnostics: Coefficient of determination
Q: How low does the MSE have to be 
before it’s “low enough”?
A: It depends! The MSE is proportional 
to the variance of the data

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Coefficient of determination (R^2 statistic)
Mean:
Variance:
MSE:

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Coefficient of determination (R^2 statistic)
Mean:
Variance:
MSE:

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Coefficient of determination (R^2 statistic)
FVU(f) = 1
Trivial predictor
FVU(f) = 0
Perfect predictor
(FVU = fraction of variance unexplained)

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Coefficient of determination (R^2 statistic)
R^2    = 0
Trivial predictor
R^2    = 1
Perfect predictor

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Coefficient of determination (R^2 statistic)
Q: Is it possible to have a negative R^2?
A: Yes! A "trivial" predictor has an R^2 of zero, but 
it's possible to have a predictor that's worse than a 
trivial predictor
e.g. predict star ratings as the mean R^2 = 0
Predict all star ratings as 0 R^2 < 0

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Summary of concepts
• Introduced the R^2 statistic
• Explained the relationship between 
MSE, mean+variance, and R^2

Python Data Products
Course 3: Making Meaningful Predictions from Data
Lecture: Over and underfitting

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Learning objectives
In this lecture we will...
• Introduce the concept of overfitting
• expand our previous discussion training and test sets

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Example
Q: What model is the best fit to this data?
A line?

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Example
A high-degree polynomial might be the best fit to 
the data (in terms of the mean-squared error), but 
intuition tells us this is not a good solution

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Overfitting
Q: But can’t we get an R^2 of 1 (MSE of 0) just by 
throwing in enough random features?
A: Yes! This is why MSE and R^2 (or other statistics) 
should be evaluated on data that wasn’t used to train 
the model
A good model is one that 
generalizes to new data

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Training and test sets
Training data
Testing data
=
=
train
test

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Training and test sets
•
We first split our data into a training and a test 
set
•
The training set is used to tune the model 
parameters (i.e.,
)
•
The test set is used to evaluate the model's 
performance on unseen data

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Training and test sets
How should the training and test sets be selected?
•
The training and test sets should be non-
overlapping samples of the data
•
They should each be a random sample of the 
data

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Training and test sets
The size of the training and test sets should be 
chosen to balance various considerations:
•
The training set should be large enough (compared to the 
model complexity) so that we don't overfit too badly
•
The test set should be large enough so that it is representative 
of the variance in the data
•
We might also be constrained by running time, etc.

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Summary of concepts
• Explained the difference between training 
performance versus generalization ability
• Showed how a training and test set can be used to 
measure generalization ability

Python Data Products
Course 3: Making Meaningful Predictions from Data
Lecture: Classification Diagnostics:
Accuracy & Error

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Learning objectives
In this lecture we will...
• Introduce a variety of diagnostics for evaluating 
classifiers
• Describe different situations where different 
performance measures may be preferable

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Which of these classifiers is best?
a
b

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Which of these classifiers is best?
The solution which minimizes the 
#errors may not be the best one

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Which of these classifiers is best?
1. When data are highly imbalanced
If there are far fewer positive examples than negative 
examples we may want to assign additional weight to 
negative instances (or vice versa)
e.g. will I purchase a 
product? If I 
purchase 0.00001% 
of products, then a 
classifier which just 
predicts “no” 
everywhere is 
99.99999% accurate, 
but not very useful

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Which of these classifiers is best?
2. When mistakes are more costly in 
one direction
False positives are nuisances but false negatives are 
disastrous (or vice versa)
e.g. which of these bags contains a weapon?

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Which of these classifiers is best?
3. When we only care about the 
“most confident” predictions
e.g. does a relevant 
result appear 
among the first 
page of results?

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Evaluating classifiers
decision boundary
positive
negative

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Evaluating classifiers
TP (true positive): Labeled as positive, predicted as positive
decision boundary
positive
negative

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Evaluating classifiers
decision boundary
positive
negative
TN (true negative): Labeled as negative, predicted as negative

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Evaluating classifiers
decision boundary
positive
negative
FP (false positive): Labeled as negative, predicted as positive

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Evaluating classifiers
decision boundary
positive
negative
FN (false negative): Labeled as positive, predicted as negative

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Evaluating classifiers
Label
true
false
Prediction
true
false
true 
positive
false 
positive
false 
negative
true 
negative
Classification accuracy
= correct predictions / #predictions
=
Error rate
= incorrect predictions / #predictions
=

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Evaluating classifiers
Label
true
false
Prediction
true
false
true 
positive
false 
positive
false 
negative
true 
negative
True positive rate (TPR) = true positives / #labeled positive
=
True negative rate (TNR) = true negatives / #labeled negative
=

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Evaluating classifiers
Label
true
false
Prediction
true
false
true 
positive
false 
positive
false 
negative
true 
negative
Balanced Error Rate (BER) = ½ (FPR + FNR)
= ½ for a random/naïve classifier, 0 for a perfect classifier

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Summary of concepts
• Introduced several diagnostics for evaluating 
classification algorithms
• Described situations where each diagnostic technique 
might be useful
On your own...
•
Using one of our previous classification 
examples, try evaluating each of these 
measures, e.g. TPR, TNR, BER, etc.

Python Data Products
Course 3: Making Meaningful Predictions from Data
Lecture: Classification Diagnostics:
Precision & Recall

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Learning objectives
In this lecture we will...
• Introduce the relationship between classification and 
ranking
• Describe techniques for evaluating classifiers when 
our goal is to use them for ranking
• Introduce the concepts of precision and recall

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Evaluating classifiers – ranking
The classifiers we’ve seen can 
associate scores with each prediction
decision boundary
positive
negative
furthest from decision 
boundary in negative direction 
= lowest score/least confident
furthest from decision 
boundary in positive direction 
= highest score/most confident

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Evaluating classifiers – ranking
The classifiers we’ve seen can 
associate scores with each prediction
•
In ranking settings, the actual labels 
assigned to the points (i.e., which side of 
the decision boundary they lie on) don’t 
matter
•
All that matters is that positively labeled 
points tend to be at higher ranks than 
negative ones​

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Evaluating classifiers – ranking
The classifiers we’ve seen can 
associate scores with each prediction
•
For naïve Bayes, the “score” is the ratio between an 
item having a positive or negative class
•
For logistic regression, the “score” is just the 
probability associated with the label being 1
•
For Support Vector Machines, the score is the 
distance of the item from the decision boundary 
(together with the sign indicating what side it’s on)

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Evaluating classifiers – ranking
The classifiers we’ve seen can 
associate scores with each prediction
Sort both according to confidence
e.g.
y = [  1,  -1,   1,   1,  1, -1,  1,  1,  -1,  1]
Confidence = [1.3,-0.2,-0.1,-0.4,1.4,0.1,0.8,0.6,-0.8,1.0]

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Evaluating classifiers
e.g.
y = [  1,  -1,   1,   1,  1, -1,  1,  1,  -1,  1]
Confidence = [1.3,-0.2,-0.1,-0.4,1.4,0.1,0.8,0.6,-0.8,1.0]

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Evaluating classifiers
e.g.
y = [  1,  -1,   1,   1,  1, -1,  1,  1,  -1,  1]
Confidence = [1.3,-0.2,-0.1,-0.4,1.4,0.1,0.8,0.6,-0.8,1.0]
y = [  1,  1,  1,  1,  1, -1,   1,  -1,   1,  -1]
Confidence = [1.4,1.3,1.0,0.8,0.6,0.1,-0.1,-0.2,-0.4,-0.8]

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Evaluating classifiers – ranking
The classifiers we’ve seen can 
associate scores with each prediction
[1, 1, 1, 1, 1, -1, 1, -1, 1, -1]
Labels sorted by confidence:
Suppose we have a fixed budget (say, six) of items that we can return
(e.g. we have space for six results in an interface)
•
Total number of relevant items = 
•
Number of items we returned = 
•
Number of relevant items we returned = 

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Evaluating classifiers – ranking
The classifiers we’ve seen can 
associate scores with each prediction
“fraction of retrieved documents that are relevant”
“fraction of relevant documents that were retrieved”

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Evaluating classifiers – ranking
The classifiers we’ve seen can 
associate scores with each prediction
= precision when we have a budget 
of k retrieved documents
e.g.
•
Total number of relevant items = 7
•
Number of items we returned = 6
•
Number of relevant items we returned = 5
precision@6 = 

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Evaluating classifiers – ranking
The classifiers we’ve seen can 
associate scores with each prediction
(harmonic mean of precision and recall)
(weighted, in case precision is more important 
(low beta), or recall is more important (high beta))

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Precision/recall curves
How does our classifier behave as we 
“increase the budget” of the number 
retrieved items?
•
For budgets of size 1 to N, compute the precision and recall
•
Plot the precision against the recall
recall
precision

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Summary of concepts
• Discussed the relationship between ranking and 
classification
• Introduced additional diagnostic techniques for 
evaluating classifiers for ranking-based settings
On your own...
•
Using one of our previous classification 
examples, try evaluating it in terms of 
ranking performance, e.g. precision, recall, 
F1-score

Week 2
Training and testing 
pipelines

Python Data Products
Course 3: Making Meaningful Predictions from Data
Lecture: Setting up a codebase for 
evaluation and validation

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Learning objectives
In this lecture we will...
• Setup a simple codebase to be used in future lectures 
for the purpose of evaluating regressors and 
classifiers, and for implementing 
training/validation/testing pipelines

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: sentiment analysis
In this lecture, we'll build a model that implements 
sentiment analysis, i.e., our goal is to predict star 
ratings based on the text in a review
We choose this problem mainly because it includes 
complex, high-dimensional features, such that 
model tuning and evaluation becomes important

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: sentiment analysis
Importing libraries and reading data:

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: sentiment analysis
Importing libraries and reading data:

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: sentiment analysis
Our goal is going to be to build a classifier which 
estimates sentiment (e.g. a star-rating) based on 
the occurrence of words in a document:
i.e., Let’s build a predictor of the form:
using a model based on linear regression:

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: sentiment analysis
Our first challenge is to build a (relatively) small 
dictionary of words to use in our model (since it 
would be impractical to include every word)

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: sentiment analysis
Counting unique words:

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: sentiment analysis
Removing capitalization and punctuation

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Concept: Stemming
drinks drink
drinking drink
drinker drink
argue argu
arguing argu
argues argu
arguing argu
argus argu
Stemming is a process that maps different 
instances of words to a unique word "stem":
E.g.

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: sentiment analysis
Stemming
We use a stemmer from 
the Python Natural 
Language Toolkit (NLTK) 
called the Porter Stemmer

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: sentiment analysis
Even after removing punctuation, capitalization, 
and stemming, we still have a dictionary that is too 
large to deal with practically
So, let's just take the subset of the most popular 
words to build our dictionary

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: sentiment analysis
Extracting the most popular words:
Just removing capitalization 
and punctuation for the 
purposes of this example
Sort words by 
popularity and keep the 
top 1000 most popular
Utility data structures to map 
each word to a unique ID

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: sentiment analysis
Extracting features from the most popular words
Increment the counter of the 
corresponding word each time 
we see that word in the text
Append the offset 
feature to the end

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: sentiment analysis
Finally, having extract a (fixed-length) feature vector 
for each document (review), we can train our 
sentiment analysis model:

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: sentiment analysis
For the moment, we fit our model much as we have 
done in previous lectures (though will change this 
in later lectures)

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: sentiment analysis
Finally, we can examine which words have the most 
positive/negative sentiment by looking at their 
corresponding coefficients:

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Summary of concepts
• Developed a new codebase for a sentiment analysis 
problems
• Showed some of the challenges in modeling features 
from text
On your own...
•
Modify the code to experiment with 
alternative feature representations (e.g. 
removing or keeping capitalization, 
punctuation, changing the dictionary size, 
etc.) to determine their impact on model 
accuracy

Python Data Products
Course 3: Making Meaningful Predictions from Data
Lecture: Model Complexity and 
regularization

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Learning objectives
In this lecture we will...
• Introduce different notions of model complexity, 
and show how we can encourage our model to favor 
simpler (over more complex) solutions
• Further discuss the concept of overfitting

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Concept: Overfitting
When a model performs well on 
training data but doesn’t 
generalize, we are said to be 
overfitting

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Concept: Overfitting
When a model performs well on 
training data but doesn’t 
generalize, we are said to be 
overfitting
Q: What can be done to avoid 
overfitting?

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
How to avoid overfitting
Q: What can be done to avoid overfitting?
A: Introduce a cost function that penalizes model 
complexity. Then, we can train a model that balances 
two goals:
(1) The model should have a high 
accuracy (e.g. low Mean Squared Error)
(2) The model should have a low 
complexity

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
How to measure complexity?
Q: How do we measure whether a model is simple or 
complex?
E.g. can we come up with a measurement that states 
that a line is a "simpler" model than a high-degree 
polynomial?

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
How to measure complexity?
Consider the case of a linear model to describe a 
polynomial:

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
How to measure complexity?
Then a high-degree polynomial might look like:
(i.e., most coefficients are non-zero)

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
How to measure complexity?
Whereas a line function would look like:
(i.e., only the intercept and slope 
terms are non-zero)

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
How to measure complexity?
So, one definition might be to count the number of 
non-zero parameters in theta.
>
(more 
complex 
than)

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
How to measure complexity?
Another commonly used definition is to count the 
amount of variance in the parameters, i.e., a "simple" 
model has mostly equal parameters
>
(more 
complex 
than)

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
How to measure complexity?
Complex:
Simple:
How can we measure (and encourage) 
these two notions of simplicity?

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
How to measure complexity?
Complex:
Simple:
"Theorem": These notions of simplicity will be 
encouraged by minimizing the l1 and l2 norms

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
How to measure complexity?
Rough proof...
• Consider a linear model with two highly 
correlated features:

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
How to measure complexity?
Rough proof...
• So, if we penalized the l1-norm, we would 
select model 2
• If we penalized the l2-norm, we would 
select model 1

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
How to measure complexity?
So (again roughly speaking)
• If we penalize the l1-norm, we will tend to get a model 
with sparse features, with few non-zero terms
• If we penalize the l2-norm, we will tend to get a model 
with mostly uniform features, with few large outliers
• We’ll see in the following lecture how to incorporate 
these penalties

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Summary of concepts
• Introduced and compared complexity measures for 
machine learning models
• Explained the consequences behind different choices 
of complexity measure
On your own...
•
Considering one or more of the models 
you’ve trained so far, evaluate its 
complexity in terms of the measures 
covered in this lecture (l1 and l2 
complexity)

Python Data Products
Course 3: Making Meaningful Predictions from Data
Lecture: Adding a regularizer to our model, and 
evaluating the regularized model

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Learning objectives
In this lecture we will...
• Extend our sentiment analysis codebase to 
incorporate a regularizer
• Demonstrate some of the model performance 
measures we covered previously

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Incorporating a regularizer into our model
The first thing we want to do is to improve our previous 
model (for sentiment analysis) to include a regularizer:

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: Regularization
This can be done using the "Ridge" model in sklearn
Regularization strength (i.e., lambda)

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: Regularization
We can now fit the model much as before:
Regularization strength (i.e., lambda)

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: Regularization
Again we can extract parameters etc. from the model, 
which may be slightly different than they were before:

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Model evaluation
Next, let's try to evaluate our model using some of the 
measures introduced previously

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: MSE and R^2
Calculating the MSE and R^2 statistic:
List of squared 
differences between 
labels and predictions
MSE = average of 
squared differences
•
FVU = Fraction of 
Variance Unexplained
•
R^2 = 1 - FVU

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Classifier evaluation
To look at some of the classifier evaluation measures 
we previously introduced, we can set the problem up as 
a classification problem
To do so, rather than estimating the ratings (a 
regression problem), we'll estimate whether the rating 
is greater than 3 (a classification problem)

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: Setting up a classification problem
Convert the problem to a classification problem, and 
solve using logistic regression:

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: Accuracy
First we can calculate the accuracy of our classifier:
List of True/False values 
indicating which 
predictions were correct
Fraction of predictions 
that were correct

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: True Positives, True Negatives, etc.
Next, using our lists of predictions and labels, we can 
calculate true positives, true negatives, etc.
Note: should add 
up to the total size 
of the dataset

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: True Positives, True Negatives, etc.
Using these counts (TP/FP/TN/FN), we can now 
compute related statistics like the accuracy:
The True Positive Rate, and True Negative Rate (etc.):
Or the Balanced Error Rate:

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Summary of concepts
• Showed how to adapt our regression code to 
incorporate a regularizer
• Computed simple statistics (such as the MSE and 
R^2) on regression data
• Computed several accuracy measures on 
classification data
On your own...
•
Adapt the code to compute other 
evaluation measures, like the Mean 
Absolute Error, or the Precision and Recall

Python Data Products
Course 3: Making Meaningful Predictions from Data
Lecture: Evaluating classifiers for ranking

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Learning objectives
In this lecture we will...
• Extend our classifier from the previous lecture in 
order to evaluate its ranking performance
• Demonstrate the precision, recall, and F1 ranking 
measures

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: Precision and Recall
Let’s start where we left off in the previous lecture. 
Previously, we had computed values for the number of 
True Positives (TP), False Positives, True Negatives, 
and False Negatives:

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: Precision and Recall
First, we can use these values to compute the precision 
and recall:

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: Precision and Recall
And the F1-score:

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: Sorting scores by confidence
Next we want to sort our predictions by confidence. 
First we obtain the confidences from the model:
Note: confidence 
scores are equivalent 
to           .

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: Sorting scores by confidence
Then we sort them along with the labels:

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: Sorting scores by confidence
At this point we can discard the confidences:

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: Precision@K and Recall@K
Now we can compute Precision@K and Recall@K
values:

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code example: Precision@K and Recall@K
Now we can compute Precision@K and Recall@K
values:

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Summary of concepts
• Showed how to compute the precision, recall, and F1-
score on our sentiment classification example
On your own...
•
Adapt the code to compute a precision-
recall curve, i.e., plot precision@k and 
recall@k values for each value of k

Week 3
Implementing the 
Predictive Pipeline

Python Data Products
Course 1: Basics
Lecture: Validation

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Learning objectives
In this lecture we will...
• Introduce the concept of the validation set
• Explain the relationship between model parameters 
and hyperparameters
• Introduce the training validation test pipeline

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Recap...
In the last few lectures we saw...
•
How a training set can be used to evaluate 
model performance on seen data
•
How a test set can be used to estimate 
generalization performance
•
How we can use a regularizer to mitigate 
overfitting

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Recap...
In particular, our regularizer "trades-off" between 
model accuracy and model complexity
•
We want a value of our regularization parameter 
that balances model accuracy (low MSE) with 
complexity (low sum of squared parameters)

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Recap...
In particular, our regularizer "trades-off" between 
model accuracy and model complexity
•
If we only cared about training error, we’d 
always select the smallest possible value of 
lambda (i.e., lambda = 0)
•
We could tune against our test set, but that 
would mean looking at the test set many times 
(which would be cheating!)

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Recap...
In particular, our regularizer "trades-off" between 
model accuracy and model complexity
•
So, we need a third partition of our data, which is 
similar to the test set, but which can be used to 
select hyperparameters like lambda
•
This set is called the validation set

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Training and test sets
Training data
Validation data
=
=
train
validation
Testing data
test

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Training and test sets
Training data
Validation data
Testing data
Used to select
Used to select
Only used once, to 
evaluate the model

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Summary of concepts
• We showed how a validation set can be used to tune 
parameters (or “hyperparameters”) that cannot be 
selected using the training set (or the test set)
• In the following lecture, we’ll explore more how this 
set can be used to optimize model performance

Python Data Products
Course 3: Making Meaningful Predictions from Data
Lecture: "Theorems" about Training, Testing, 
and Validation

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Learning objectives
In this lecture we will...
• Explain the relationship between training, validation, 
and test performance
• Introduce several "theorems" characterizing these 
relationships
• Briefly describe how these theorems can be used to 
guide model selection

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Recap...
Previously we saw the relationship between 
training, validation, and test sets, and described how 
the validation set should be used to select 
hyperparameters
In practice, how do training, validation, and test 
errors relate to each other, and how do we select the 
best model?

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Recap...
Our basic setup consists of
•
A series of hyperparameters (e.g. values of 
lambda) to experiment with
•
One model for each hyperparameter combination
•
A training, validation, and test set on which to 
evaluate performance

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Theorems about training, testing, and validation
Model complexity (i.e.,    )
Error (e.g. MSE)
more complex
less complex
1. Error should increase as lambda increases

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Theorems about training, testing, and validation
Error (e.g. MSE)
more complex
less complex
1(b). For large lambda, the model will generally behave like a 
"trivial" model
MSE = variance (trivial model)
Model complexity (i.e.,    )

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Theorems about training, testing, and validation
Error (e.g. MSE)
more complex
less complex
2. Validation and test error will be larger than training error
MSE = variance (trivial model)
training error
validation/test error
Model complexity (i.e.,    )

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Theorems about training, testing, and validation
Error (e.g. MSE)
more complex
less complex
2(b). The validation error can be used to identify
underfitting and overfitting
MSE = variance (trivial model)
Underfitting
Overfitting
validation/test error
training error
Model complexity (i.e.,    )

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Theorems about training, testing, and validation
Error (e.g. MSE)
more complex
less complex
3. There will usually be a "sweet spot" between under- and over-
fitting; this is the model we ultimately select
MSE = variance (trivial model)
Model complexity (i.e.,    )
Underfitting
Overfitting
validation/test error
training error

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Theorems about training, testing, and validation
1. Error should increase as lambda increases
2. Validation and test error will be larger than training error
3. There will usually be a "sweet spot" between under- and over-
fitting

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Theorems about training, testing, and validation
Note about these "theorems“:
• Due to the randomness of real datasets, these theorems may 
not always hold precisely
• However they are good guidelines – if these theorems are badly 
violated it could be a sign of a bug
• They should hold assuming we have large datasets, and 
assuming that our training, validation, and test sets are 
randomly sampled
• Note that if we were maximizing accuracy (rather than 
minimizing error), this plot would be inverted

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Validation pipeline
So, the validation pipeline looks roughly as follows
(to be described in more detail in later lectures):
• Select a range of values for lambda (I usually use powers of 10)
• For each value of lambda train a model, and compute its 
validation error
• Select the model with the lowest validation error (or highest 
accuracy), and compute its performance on the test set

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Summary of concepts
• Introduced several theorems characterizing 
the relationship between training, validation, and test 
performance
• Showed how these concepts might be used within a 
training/validation/testing pipeline
On your own...
•
Try computing training/validation/test 
error on one of our previous examples (e.g. 
our example on bankruptcy data), and 
experimentally confirm the relationships 
described in this lecture

Python Data Products
Course 3: Making Meaningful Predictions from Data
Lecture: Implementing a regularization 
pipeline in Python

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Learning objectives
In this lecture we will...
• Show how to implement the 
training/validation/testing pipeline in Python
• Show how to select regularization parameters, and 
evaluate model performance using a validation set

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Validation pipeline
To summarize our validation pipeline so far, our goal is to:
•
Split the data into train/validation/test fractions
•
Consider several different values of our hyperparameters (e.g. 
lambda)
•
For each of these values, train a model on the training set
•
Evaluate each model's performance on the validation set
•
For the model that performs best on the validation set, 
evaluate its performance on the test set

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code: data and problem setup
First, let's set up our prediction problem (which is mostly 
code we've seen before):
Read the data, and 
convert numerical 
values to integers

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code: data and problem setup
Next we extract features (again, much as we did in 
previous examples):
Counting instances 
of words in each 
review

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code: splitting the data
Now, our first task is to split the data into training, 
validation, and test samples:
Remember to shuffle the dataset, 
so that our train/valid/test sets 
are i.i.d. samples
This example uses 50%/25%/25% 
(non-overlapping) splits, though 
other ratios would also be 
possible

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code: regression model
Again, we'll use the "Ridge" model from sklearn, which 
allows us to implement regression with a regularizer
Regularization parameter

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code: regression model
Set up a quick utility function to calculate the MSE for a 
particular model:

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code: regression model
Train the model for a range of regularization parameters:
Keep track of which model 
worked the best
Fit a model for each 
lambda value
Report the training and validation 
error (but not the test error yet!)

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Code: regression model
Finally, report the test error for the model that had the 
best performance on the validation set

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Summary of concepts
• Showed a full pipeline of model selection and 
evaluation on a real dataset
On your own...
•
Reproduce this pipeline for a different task, 
e.g. for a classification experiment

Python Data Products
Course 3: Making Meaningful Predictions from Data
Lecture: Guidelines on the implementation of 
predictive pipelines

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Learning objectives
In this lecture we will...
• Suggest practical guidelines for model selection
• Show how our “theorems” about model selection can 
be applied in practice
• Demonstrate other cases where the validation set can 
be used, besides selection regularization parameters

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Choosing among several different models
1. As well as selecting model hyperparameters (like 
lambda) the validation set can also be used to select 
among model alternatives. E.g.:
•
Should I use an SVM or a logistic regressor?
•
How deep should my decision tree be?
•
How many layers should my neural network have?

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Choosing among several different models
2. When using iterative models (like gradient 
descent/ascent), it is not necessary (or desirable) to train 
the model to convergence
Rather, we should periodically compute the validation 
error, and stop once the validation error is no longer 
improving

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Choosing among several different models
3. The validation set can also be used to guide feature 
engineering choices. E.g.:
•
How many words should we include in our dictionary? 
In our example we used 1000 but we could make a 
better choice using our validation set
•
Should we remove punctuation, capitalization, etc.?

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Choosing among several different models
4. What values of lambda should we choose
•
Our validation "theorems" can guide us to good 
choices of values
•
E.g. given two values of lambda (a and b > a), if the 
validation error is higher for a, then we should try 
larger values than b; if the validation error is higher for 
b, we should try smaller values than a

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Choosing among several different models
4. What values of lambda should we choose
Error (e.g. MSE)
more complex
less complex
MSE = variance (trivial model)
Model complexity (i.e.,    )
Underfitting
Overfitting
validation/test error
training error

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Summary of concepts
• Gave practical advice as to how to select 
regularization parameters
• Showed how the same concepts can be used to 
guide other modeling decisions (e.g. different feature 
representation options)
On your own...
•
Use these guidelines to optimize various 
model parameters, e.g. the dictionary size, 
text-processing options, etc., as well as the 
hyperparameter lambda

Python Data Products
Course 3: Making Meaningful Predictions from Data
Lecture: Course Summary

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Course summary: Week 1
In this course we covered...
• Evaluation metrics for regression algorithms: MSE, R^2, and 
the motivation behind them
• Evaluation metrics for classification algorithms: Accuracy, 
Error, Precision, Recall, etc.
• How to select the right evaluation metrics under different 
conditions

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Course summary: Week 2
In this course we covered...
• Predictive pipelines: how do we build models that will 
generalize well to new data?
• Model complexity and overfitting
• Regularization
• How can we train models that will not be too complex? 
I.e., how can we trade-off accuracy versus complexity?

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Course summary: Week 3
In this course we covered...
• Implementation of the predictive pipeline in Python
• Rules and guidelines for model selection using 
training/validation/test sets

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Course summary
By now you should be able to...
• Implement robust predictive pipelines in Python. In the 
previous course, we saw the same basic pipelines, but by 
now you should be able to:
a) Select appropriate evaluation metrics based on sound 
principles
b) Ensure that your models are not too complex, by using a 
regularizer
c) Ensure that your models will generalize well to new data

Data to Products Specialization: Course 3: Making Meaningful Predictions from Data
Course summary
In the next course we will...
• Introduce recommender systems, to be used for our 
capstone project
• See how to implement simple, working recommender 
systems on real datasets, and deal with the various 
implementation issues involved with doing so
• Combine the knowledge that we have developed so far to 
deal with various issues around evaluation, model fitting, 
implementation, and deployment


Lecture Notes in Computer Science
2502
Edited by G. Goos, J. Hartmanis, and J. van Leeuwen

3
Berlin
Heidelberg
New York
Barcelona
Hong Kong
London
Milan
Paris
Tokyo

Dieter Gollmann Günter Karjoth
Michael Waidner (Eds.)
Computer Security –
ESORICS 2002
7th European Symposium on Research in Computer Security
Zurich, Switzerland, October 14-16, 2002
Proceedings
1 3

Series Editors
Gerhard Goos, Karlsruhe University, Germany
Juris Hartmanis, Cornell University, NY, USA
Jan van Leeuwen, Utrecht University, The Netherlands
Volume Editors
Dieter Gollmann
Microsoft Research
7 J J Thomson Avenue, Cambridge CB3 0FB, UK
E-mail: diego@microsoft.com
Günther Karjoth
Michael Waidner
IBM Zurich Research Lab
Säumerstr. 4, 8803 Rüschlikon, Switzerland
E-mail: {gka/wmi}@zurich.ibm.com
Cataloging-in-Publication Data applied for
Bibliograhpic information published by Die Deutsche Bibliothek
Die Deutsche Bibliothek lists this publication in the Deutsche Nationalbibliograﬁe;
detailed bibliographic data is available in the Internet at http://dnb.ddb.de
CR Subject Classiﬁcation (1998): D.4.5, E.3, C.2.0, H.2.0, K.6.5, K.4.4
ISSN 0302-9743
ISBN 3-540-44345-2 Springer-Verlag Berlin Heidelberg New York
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is
concerned, speciﬁcally the rights of translation, reprinting, re-use of illustrations, recitation, broadcasting,
reproduction on microﬁlms or in any other way, and storage in data banks. Duplication of this publication
or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965,
in its current version, and permission for use must always be obtained from Springer-Verlag. Violations are
liable for prosecution under the German Copyright Law.
Springer-Verlag Berlin Heidelberg New York
a member of BertelsmannSpringer Science+Business Media GmbH
http://www.springer.de
© Springer-Verlag Berlin Heidelberg 2002
Printed in Germany
Typesetting: Camera-ready by author, data conversion by DA-TeX Gerd Blumenstein
Printed on acid-free paper
SPIN: 10870813
06/3142
5 4 3 2 1 0

Preface
ESORICS, the European Symposium on Research in Computer Security, is the
leading research-oriented conference on the theory and practice of computer
security in Europe. It takes place every two years, at various locations throughout
Europe, and is coordinated by an independent Steering Committee.
ESORICS 2002 was jointly organized by the Swiss Federal Institute of Tech-
nology (ETH) and the IBM Zurich Research Laboratory, and took place in
Zurich, Switzerland, October 14-16, 2002.
The program committee received 83 submissions, originating from 22 coun-
tries. For fans of statistics: 55 submissions came from countries in Europe, the
Middle East, or Africa, 16 came from Asia, and 12 from North America. The
leading countries were USA (11 submissions), Germany (9), France (7), Italy
(7), Japan (6), and UK (6). Each submission was reviewed by at least three pro-
gram committee members or other experts. Each submission coauthored by a
program committee member received two additional reviews. The program com-
mittee chair and cochair were not allowed to submit papers. The ﬁnal selection
of papers was made at a program committee meeting and resulted in 16 accepted
papers. In comparison, ESORICS 2000 received 75 submissions and accepted 19
of them.
The program reﬂects the full range of security research: we accepted papers on
access control, authentication, cryptography, database security, formal methods,
intrusion detection, mobile code security, privacy, secure hardware, and secure
protocols.
We gratefully acknowledge all authors who submitted papers for their eﬀorts
in maintaining the standards of this conference.
It is also my pleasure to thank the members of the program committee, the
additional reviewers, and the members of the organization committee for their
work and support.
Zurich, October 2002
Michael Waidner

Program Committee
Mart´ın Abadi
University of California at Santa Cruz, USA
Ross Anderson
University of Cambridge, UK
Tuomas Aura
Microsoft Research, UK
Joachim Biskup
University of Dortmund, Germany
Fr´ed´eric Cuppens
ONERA, France
Marc Dacier
Eurecom, France
Herv´e Debar
France Telecom R&D, France
Yves Deswarte
LAAS-CNRS, France
Simone Fischer-H¨ubner
Karlstad University, Sweden
Simon Foley
University College Cork, Ireland
Dieter Gollmann
Microsoft Research, UK
Martin Hirt
ETH Zurich, Switzerland
Trent Jaeger
IBM Research, USA
Socratis Katsikas
University of the Aegean, Greece
Kaoru Kurosawa
Ibaraki University, Japan
Heiko Mantel
DFKI, Germany
John McHugh
CERT, USA
David Naccache
Gemplus, France
Birgit Pﬁtzmann
IBM Research, Switzerland
Avi Rubin
AT&T Labs–Research, USA
Peter Ryan
University of Newcastle, UK
Pierangela Samarati
University of Milan, Italy
Tomas Sander
Intertrust, USA
Einar Snekkenes,
Program Co-chair
Norwegian Computer Center, Norway
Michael Steiner
Saarland University, Germany
Gene Tsudik
University of California, Irvine, USA
Dennis Volpano
Naval Postgraduate School, USA
Michael Waidner,
Program Chair
IBM Research, Switzerland
Additional Reviewers
Ammar Alkassar, Michael Backes, S´ebastien Canard, Jan Camenisch, Nora
Cuppens, Xuhua Ding, Thomas D¨ubendorfer, Alberto Escudero-Pascual, Serge
Fehr, Felix G¨artner, Stuart Haber, Thomas Holenstein, Yongdae Kim, Fabien
Laguillaumie, Jean-Fran¸cois Misarsky, Maithili Narasimha, Benny Pinkas,
Andrei Sabelfeld, Ahmad-Reza Sadeghi, Axel Schairer, Simon Skaria, Jacques
Traor´e, J¨urg Wullschleger

Organization
VII
Organization Committee
Endre Bangerter (IBM Research), Dieter Gollmann (Microsoft Research, UK;
Publication Chair) G¨unter Karjoth (IBM Research, Switzerland; General Chair),
J¨urg Nievergelt (ETH Zurich, Switzerland; General Co-chair), Floris Tschurr
(ETH Zurich)
Steering Committee
Joachim Biskup (University of Dortmund, Germany), Fr´ed´eric Cuppens (ON-
ERA, France), Yves Deswarte (LAAS-CNRS, France), Gerard Eizenberg
(CERT, France), Simon Foley (University College Cork, Ireland), Dieter Goll-
mann (Microsoft Research, UK), Franz-Peter Heider (debis IT Security Services,
Germany), Jeremy Jacob (University of York, UK), Socratis Katsikas (Univer-
sity of the Aegean, Greece), Helmut Kurth (atsec, Germany), Peter Landrock
(Cryptomathic, UK), Emilio Montolivo (FUB, Italy), Roger Needham (Microsoft
Research, UK), Jean-Jacques Quisquater (UCL, Belgium), Peter Ryan (Univer-
sity of Newcastle, UK: Steering Committee Chair), Pierangela Samarati (Univer-
sity of Milan, Italy), Einar Snekkenes (Norwegian Computer Center, Norway),
Michael Waidner (IBM Research, Switzerland).

Table of Contents
Computational Probabilistic Non-interference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
Michael Backes and Birgit Pﬁtzmann
Bit-Slice Auction Circuit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .24
Kaoru Kurosawa and Wakaha Ogata
Conﬁdentiality Policies and Their Enforcement
for Controlled Query Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
Joachim Biskup and Piero Bonatti
Cardinality-Based Inference Control in Sum-Only Data Cubes . . . . . . . . . . . . . 55
Lingyu Wang, Duminda Wijesekera, and Sushil Jajodia
Outbound Authentication for Programmable Secure Coprocessors . . . . . . . . . . 72
Sean W. Smith
Hamming Weight Attacks on Cryptographic Hardware –
Breaking Masking Defense . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
Marcin Gomu$lkiewicz and Miros$law Kuty$lowski
A Fully Compliant Research Implementation of the P3P Standard
for Privacy Protection: Experiences and Recommendations . . . . . . . . . . . . . . . .104
Giles Hogben, Tom Jackson, and Marc Wilikens
Authentication for Distributed Web Caches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
James Giles, Reiner Sailer, Dinesh Verma, and Suresh Chari
Analysing a Stream Authentication Protocol Using Model Checking . . . . . . .146
Philippa Broadfoot and Gavin Lowe
Equal To The Task? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
James Heather and Steve Schneider
TINMAN: A Resource Bound Security Checking System
for Mobile Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
Aloysius K. Mok and Weijiang Yu
Conﬁdentiality-Preserving Reﬁnement is Compositional – Sometimes . . . . . .194
Thomas Santen, Maritta Heisel, and Andreas Pﬁtzmann
Formal Security Analysis with Interacting State Machines . . . . . . . . . . . . . . . . .212
David von Oheimb and Volkmar Lotz
Decidability of Safety in Graph-Based Models for Access Control . . . . . . . . . 229
Manuel Koch, Luigi V. Mancini, and Francesco Parisi-Presicce
Inter-Packet Delay Based Correlation
for Tracing Encrypted Connections through Stepping Stones . . . . . . . . . . . . . . 244
Xinyuan Wang, Douglas S. Reeves, and S. Felix Wu

X
Table of Contents
Learning Fingerprints for a Database Intrusion Detection System . . . . . . . . . 264
Sin Yeung Lee, Wai Lup Low, and Pei Yuen Wong
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .281

Computational Probabilistic Non-interference
Michael Backes1 and Birgit Pﬁtzmann2
1 Saarland University, Saarbr¨ucken, Germany
mbackes@cs.uni-sb.de
2 IBM Zurich Research Laboratory, R¨uschlikon, Switzerland
bpf@zurich.ibm.com
Abstract. In recent times information ﬂow and non-interference have
become very popular concepts for expressing both integrity and pri-
vacy properties. We present the ﬁrst general deﬁnition of probabilis-
tic non-interference in reactive systems which includes a computational
case. This case is essential to cope with real cryptography since non-
interference properties can usually only be guaranteed if the underlying
cryptographic primitives have not been broken. This might happen, but
only with negligible probability. Furthermore, our deﬁnition links non-
interference with the common approach of simulatability that modern
cryptography often uses. We show that our deﬁnition is maintained un-
der simulatability, which allows secure composition of systems, and we
present a general strategy how cryptographic primitives can be included
in information ﬂow proofs. As an example we present an abstract speciﬁ-
cation and a possible implementation of a cryptographic ﬁrewall guarding
two honest users from their environment.
1
Introduction
Nowadays, information ﬂow and non-interference are known as powerful possi-
bilities for expressing privacy and integrity requirements a program or a crypto-
graphic protocol should fulﬁll. The ﬁrst models for information ﬂow have been
considered for secure operating systems by Bell and LaPadula [3], and Den-
ning [5]. After that, various models have been proposed that rigorously de-
ﬁne when information ﬂow is considered to occur. The ﬁrst one, named non-
interference, was introduced by Goguen and Meseguer [6, 7] in order to analyze
the security of computer systems, but their work was limited to determinis-
tic systems. Nevertheless, subsequent work was and still is based on their idea
of deﬁning information ﬂow. After that, research focused on non-deterministic
systems, mainly distinguishing between probabilistic and possibilistic behav-
iors. Beginning with Sutherland [19] the possibilistic case has been dealt with
in [15, 20, 16, 22, 14], while the probabilistic and information-theoretic cases have
been analyzed by Gray [9, 10] and McLean [17]. Clark et. al. have shown in [4]
that possibilistic information ﬂow analysis can be used to check for probabilistic
interference.
Gray’s deﬁnition of “Probabilistic Non-Interference” of reactive systems
stands out. It is closely related to the perfect case of our deﬁnition, but it does
D. Gollmann et al. (Eds.): ESORICS 2002, LNCS 2502, pp. 1–23, 2002.
c
⃝Springer-Verlag Berlin Heidelberg 2002

2
Michael Backes and Birgit Pﬁtzmann
not cover computational aspects which are essential for reasoning about systems
using real cryptographic primitives. Thus, if we want to consider real cryptog-
raphy we cannot restrict ourselves to perfect non-interference as captured by
the deﬁnition of Gray (nor to any other deﬁnition mentioned before, because
they are non-probabilistic and hence not suited to cope with real cryptography)
because it will not be suﬃcient for most cryptographic purposes. As an exam-
ple, consider an arbitrary public key encryption scheme. Obviously, an adversary
with unlimited computing power can always break the scheme by computing all
possible encryptions of every plaintext and comparing the results with the given
ciphertext. Moreover, even polynomially bounded adversaries may have a very
small, so-called negligible probability of success. Thus, cryptographic deﬁnitions
usually state that every polynomially bounded adversary can only achieve its
goal with a negligible probability. Adopting this notion we present the ﬁrst gen-
eral deﬁnition of non-interference for this so-called computational case. Besides
our work, the paper of Laud [12] contains the only deﬁnition of non-interference
including such a computational case. However, only encryption is covered so far,
i.e., other important concepts like authentication, pseudo-number generators,
etc. are not considered. Moreover, the deﬁnition is non-reactive, i.e., it does not
comprise continuous interaction between the user, the adversary, and the sys-
tem, which is a severe restriction to the set of considered cryptographic systems.
Our deﬁnition is reactive and comprises arbitrary cryptographic primitives.
In contrast to other deﬁnitions, we will not abstract from cryptographic de-
tails and probabilism, e.g., by using the common Dolev-Yao abstraction or spe-
cial type systems, but we immediately include the computational variant in our
deﬁnition. This enables sound reduction proofs with respect to the security def-
initions of the included cryptographic primitives (e.g., reduction proofs against
the security of an underlying public key encryption scheme), i.e., a possibility
to break the non-interference properties of the system can be used to break the
underlying cryptography. Moreover, we show that our deﬁnition behaves well
under the concept of simulatability that modern cryptography often uses, i.e.,
non-interference properties proved for an abstract speciﬁcation automatically
carry over to the concrete implementation. This theorem is essential since it
enables modular proofs in large systems, i.e., proofs done for ideal systems not
containing any probabilism simply carry over to their corresponding real cryp-
tographic counterparts. Moreover, properties of these ideal systems could quite
easily be proved machine-aided, so our theorem additionally provides a link
between cryptography and formal proof tools for non-interference. Thus, non-
interference properties can be expressed for reactive systems containing arbitrary
cryptographic primitives, which is of great importance for extensible systems like
applets, kernel extensions, mobile agents, virtual private networks, etc.
Outline of the paper. In Section 2 we brieﬂy review the underlying model of
asynchronous reactive system introduced in [18]. The original contributions are
presented in Sections 3, 4 and 5. In Section 3 we extend the underlying model
to multiple users and we refer to as multi-party conﬁgurations; built on this
deﬁnition we construct our deﬁnition of non-interference. In Section 4 we show

Computational Probabilistic Non-interference
3
that our deﬁnition behaves well under simulatability, hence reﬁnement does not
change the non-interference properties. In Section 5 we present an abstract spec-
iﬁcation and a possible implementation of a cryptographic ﬁrewall guarding two
honest users from their environment, and we prove that they fulﬁll our deﬁnition
of non-interference.
2
General System Model for Reactive Systems
In this section we brieﬂy recapitulate the model for probabilistic reactive sys-
tems as introduced in [18]. All details of the model which are not necessary for
understanding are omitted; they can be looked up in the original paper.
Systems mainly are compositions of diﬀerent machines. Usually we consider
real systems consisting of a set ˆM of machines {M1, . . . , Mn} and ideal systems
built by one machine {TH}, called trusted host. The machine model is proba-
bilistic state-transition machines, similar to I/O automata as introduced in [13].
For complexity we consider every automata to be implemented as a probabilis-
tic Turing machine; complexity is measured in the length of its initial state,
i.e. the initial worktape content (often a security parameter k, given in unary
representation).
Communication between diﬀerent machines is done via ports. Inspired by the
CSP-Notation [11], we write output and input ports as p! and p?, respectively.
The ports of a machine M will be denoted by ports(M). Connections are deﬁned
implicitly by naming convention, i.e., port p! sends messages to p?. To achieve
asynchronous timing, a message is not directly sent to its recipient, but it is
ﬁrst stored in a special machine p called a buﬀer and waits to be scheduled. If
a machine wants to schedule the i-th message of buﬀer p (this machine must
have the unique clock-out port p◁!) it simply sends i at p◁!. The i-th message
is then scheduled by the buﬀer and removed from its internal list. In our case,
most buﬀers are either scheduled by a speciﬁc master scheduler or the adversary,
i.e., one of those has the corresponding clock-out port.
A collection C of machines is a ﬁnite set of machines with pairwise diﬀerent
machine names and disjoint sets of ports. The completion [C] of a collection C is
the union of all machines of C and the buﬀers needed for every channel.
A structure is a pair ( ˆM , S), where ˆM is a collection of machines and S ⊆
free([ ˆM ]), the so called speciﬁed ports, are a subset of the free1 ports of [ ˆM ].
Roughly speaking the ports of S guarantee special services to the users. We
will always describe speciﬁed ports by their complements S c, i.e., the ports
honest users should have. A structure can be completed to a conﬁguration by
adding special machines H and A, modeling honest users and the adversary. The
machine H is restricted to the speciﬁed ports S, A connects to the remaining
free ports of the structure and both machines can interact. If we now consider
sets of structures we obtain a system Sys.
Scheduling of machines is done sequentially, so we have exactly one active
machine M at any time. If this machine has clock-out ports, it is allowed to select
1 A port is called free if its corresponding port is not in the system. These port will
be connected to the users and to the adversary.

4
Michael Backes and Birgit Pﬁtzmann
the next message to be scheduled as explained above. If that message exists, it
is delivered by the buﬀer and the unique receiving machine is the next active
machine. If M tries to schedule multiple messages, only one is taken, and if it
schedules none or the message does not exist, the special master scheduler is
scheduled.
Altogether we obtain a runnable system which we refer to as a conﬁguration
and a probability space over all possible executions (also denoted as runs or
traces) of the system. If we restrict runs to certain sets ˆM of machines, we
obtain the view of ˆM . Moreover we can restrict a run r to a set S of ports which
is denoted by r⌈S.
For a conﬁguration conf , we furthermore obtain random variables over this
probability space which are denoted by runconf ,k and view conf ,k, respectively.
3
Expressing Non-interference and Multi-party
Conﬁgurations
In this section we deﬁne non-interference for reactive systems as introduced
in Section 2. At ﬁrst we look at the more general topic of information ﬂow.
Information ﬂow properties consist of two components: a ﬂow policy and a deﬁ-
nition of information ﬂow. Flow policies are built by graphs with two diﬀerent
classes of edges. The ﬁrst class symbolizes that information may ﬂow between
two users, the second class symbolizes that it may not. If we now want to de-
ﬁne non-interference, we have to provide a semantics for the second class of
edges.2 Intuitively, we want to express that there is no information ﬂow from
a user HH to a user HL iﬀthe view of HL does not change for every possible
behaviour of HH, i.e., HL should not be able to distinguish arbitrary two fam-
ilies of views induced by two behaviours of HH. As we have seen in Section 2,
we did not regard diﬀerent honest users as diﬀerent machines so far, we just
combined them into one machine H. Obviously, this distinction is essential for
expressing non-interference, so we ﬁrst have to deﬁne multi-party conﬁgurations
for the underlying model. Multi-party conﬁgurations are deﬁned identically to
usual conﬁgurations except that we have a set U of users instead of a one-user
machine H.
3.1
Multi-party Conﬁgurations
Deﬁnition 1. (Multi-Party Conﬁgurations) A multi-party conﬁguration conf mp
of a system Sys is a tuple ( ˆM , S, U , A) where ( ˆM , S) ∈Sys is a structure,
U is a set of machines called users without forbidden ports, i.e., ports(U ) ∩
forb( ˆM , S) = ∅must hold and the completion ˆC := [ ˆM ∪U ∪{A}] is a closed
collection. The set of these conﬁgurations will be denoted by Confmp(Sys), those
with polynomial-time users and a polynomial-time adversary by Confmp
poly(Sys).
We will omit the indices mp and poly if they are clear from the context.
✸
2 We will not present a semantics for the ﬁrst class of edges here, because we only
focus on absence of information ﬂow in this paper.

Computational Probabilistic Non-interference
5


Fig. 1. A typical ﬂow policy graph consisting of high and low users only
It is important to note that runs and views are also deﬁned for multi-party
conﬁgurations because we demanded the completion ˆC to be closed.
3.2
Flow Policies
We start by deﬁning the ﬂow policy graph.
Deﬁnition 2. (General Flow Policy) A general ﬂow policy is a pair G = (S, E)
with E ⊆S × S × {❀, ̸❀}. Thus, we can speak of a graph G with two diﬀerent
kind of edges: ❀, ̸❀⊆S ×S. Furthermore we demand (s1, s1) ∈❀for all s1 ∈S,
and every pair (s1, s2) of nodes should be linked by exactly one edge, so ❀and
̸❀form a partition of S × S.
✸
Remark 1. The set S often consists of only two elements S = {L, H} which are
referred to as low- and high-level users. A typical ﬂow policy would then be given
by L ❀L, L ❀H, H ❀H, and ﬁnally H ̸❀L, cf. Figure 1, so there should
not be any information ﬂow from high- to low-level users.
This deﬁnition is quite general since it uses an arbitrary set S. If we want to
use it for our purpose, we have to reﬁne it so that it can be applied to a system
Sys of our considered model. The intuition is to deﬁne a graph on the possible
participants of the protocol, i.e., users and the adversary. However, this deﬁnition
would depend on certain details of the users and the adversary, e.g., their port
names, so we specify users by their corresponding speciﬁed ports of Sys, and the
adversary by the remaining free ports of the system to achieve independency.
After that, our ﬂow policy only depends on the ports of Sys.
Deﬁnition 3. (Flow Policy) Let a structure ( ˆM , S) be given, and let Γ( ˆ
M,S) =
{Si | i ∈I} denote a partition of S for a ﬁnite index set I, so ∆( ˆ
M,S) :=
Γ( ˆ
M,S) ∪{¯S} is a partition of free([ ˆM ]). A ﬂow policy G( ˆ
M ,S) of the structure
( ˆM , S) is now deﬁned as a general ﬂow policy G( ˆ
M,S) = (∆( ˆ
M ,S), E( ˆ
M,S)).
The set of all ﬂow policies for a structure ( ˆM , S) and a partition ∆( ˆ
M,S) of
free([ ˆM ]) will be denoted by POL ˆ
M,S,∆( ˆ
M ,S). Finally, a ﬂow policy for a system
Sys is a mapping
φSys :
Sys →POL ˆ
M,S,∆( ˆ
M ,S)
( ˆM , S) →G( ˆ
M,S)
that assigns each structure ( ˆM , S) a ﬂow policy G( ˆ
M ,S) which is deﬁned on
( ˆM , S).
✸

6
Michael Backes and Birgit Pﬁtzmann



 	

 	





Fig. 2. Sketch of our non-interference deﬁnition
We will simply write G, ∆, and E instead of G( ˆ
M ,S), ∆( ˆ
M ,S), and E( ˆ
M,S) if the
underlying structure is clear from the context. Additionally, we usually con-
sider graphs with the following property: for blocks of ports SH, SL ∈∆with
(SH, SL) ∉❀there should not be a path from SH to SL consisting of “❀”-
edges only. We will refer to this property as transitivity property and speak of
a transitive ﬂow policy.
The relation ̸❀is the non-interference relation of G, so for two arbitrary
blocks SH, SL ∈∆, (SH, SL) ∉❀means that no information ﬂow must occur
directed from the machine connected to SH to the machine connected to SL. The
notion of a transitive ﬂow policy is motivated by our intuition that if a user HH
should not be allowed to directly send any information to user HL, he should
also not be able to send information to HL by involving additional users, similar
for the adversary.
3.3
Deﬁnition of Non-interference
We still have to deﬁne the semantics of our non-interference relation ̸❀. Usually,
expressing this semantics is the most diﬃcult part of the whole deﬁnition. In
our underlying model, it is a little bit easier because we already have deﬁnitions
for runs, views, and indistinguishability that can be used to express the desired
semantics.
Figure 2 contains a sketch of our deﬁnition of non-interference between two
users HH and HL (one of them might also be the adversary). Mainly, we deﬁne
a speciﬁc machine BITH, that simply chooses a bit b ∈{0, 1} at random and
outputs it to HH. Non-interference now means that HH should not be able to
change the view of HL, so it should be impossible for HL to output the bit b
at p∗
L bit! with a probability better than 1
2 in case of perfect non-interference.
Statistical and computational non-interference now means that the advantage
of HL for a correct guess of b should be a function of a class SMALL or neg-
ligible, respectively, measured in the given security parameter k. The approach
of “guessing a bit”, i.e., including the machines BITH and OUTL in our case, is
essential to extend the notation of probabilistic non-interference to error prob-
abilities and complexity-theoretic assumptions. Moreover, it is a fundamental
concept in cryptography, so our deﬁnition additionally serves as a link between

Computational Probabilistic Non-interference
7
prior work in non-interference and real cryptographic primitives along with their
security deﬁnitions.
These speciﬁc conﬁgurations including the special BITH- and OUTL-
machines will be called non-interference conﬁgurations. Before we turn our at-
tention to the formal deﬁnition of these conﬁgurations we brieﬂy describe which
machines have to be included, how they behave, and which ports are essential
for these sort of conﬁgurations, cf. Figure 3.
First of all, we have special machines BITH, OUTL and Xn in. As described
above, BITH will uniformly choose a bit at the start of the run and output it to
the user HH, the second machine simply catches the messages received at port
p∗
L bit?.
The machine Xn in is the master scheduler of the conﬁguration. Its function is
to provide liveness properties and to avoid denial of service attacks, so it ensures
that every machine will be able to send messages if it wants to. Before we turn
our attention to this machine, we brieﬂy describe the ports of the users. In order
to improve readability we encourage the reader to compare these descriptions
with Figure 3.
At ﬁrst, we demand that every user must not have any clock-out ports;
instead they have additional output ports ps! connected to the master scheduler
for every “usual” output port p!. The master scheduler will use these ports to
schedule outputs from ports p!, so a user can tell the master scheduler which
port it wants to be scheduled. This is essential for achieving liveness properties,
because otherwise, there might be cycles inside of the system, so that neither the
master scheduler nor some users will ever be scheduled. Therefore, we explicitly
give the control to the master scheduler and deﬁne a suitable scheduling strategy
in the formal deﬁnition.
We now focus on the ports of the master scheduler. First of all, it has the
corresponding input ports ps? for receiving scheduling demands from the users,
and the corresponding clock-out ports ps◁!. Furthermore, it has clock-out ports
p◁! to schedule every buﬀer p that delivers messages from the users to the actual
system. Finally, it has special ports masteri! to schedule (or to give control to)
the user Hi. The actual scheduling process, i.e., how and in what order users are
scheduled will be described in the formal deﬁnition.
Deﬁnition 4. (Non-Interference Conﬁguration) Let a ﬁnite index set I with
A ̸∈I and H, L ∈I ∪{A}, H ̸= L be given. Furthermore, let a multi-party
conﬁguration conf mp
H,L = ( ˆM , S, U ∪{BITH, OUTL, Xn in}, A) of a system Sys
with U = {Hi | i ∈I} and a partition ∆= {Si | i ∈I} ∪{¯S} of free([ ˆM ])
be given. For naming convention we set HA := A and SA := ¯S. We call this
conﬁguration a non-interference conﬁguration of Sys if the following holds:
a) The ports of BITH are given by {masterBITH ?, pH bit!, pH bit◁!}.
The speciﬁc machine OUTL has only one input port p∗
L bit? connected to HL.
The machine Xn in is the master scheduler of the conﬁguration. Its ports are
given by
• {clk◁?}: The master clock-in port.

8
Michael Backes and Birgit Pﬁtzmann




 	

 	





 
 



 







Fig. 3. Main Parts of a non-interference conﬁguration. Additionally, the ports of
the master scheduler Xn in and the two emphasized users HH and HL are sketched
• {p◁! | p◁! ∈S c
i , i ∈I}: The ports for scheduling buﬀers between users
and the actual system.
• {ps?, ps◁! | p◁! ∈S c
i , i ∈I}: The ports connected to the users for receiving
scheduling demands.
• {ps∗
L bit?, ps∗
L bit
◁!, p∗
L bit
◁!}: The ports for scheduling demands and out-
puts to machine OUTL.
• {masteri!, masteri◁! | i ∈I ∪{A} ∪{BITH}}: The ports for scheduling
(that is giving control to) the users, the adversary and BITH.
b) For i ∈I the ports of Hi must include {ps! | p◁! ∈S c
i } ∪{masteri?}, the
ports of the adversary must include masterA?. Additionally, HH must have
an input port pH bit?, HL must have output ports p∗
L bit! and ps∗
L bit!.
Furthermore, we demand that the remaining ports of every user and of the
adversary connect exactly to their intended subset of speciﬁed ports. For-
mally,
ports(HH)\({pH bit?}∪{ps! | p◁! ∈S c
H}∪{masterH?}) = S c
H\{p◁! | p◁! ∈S c
H},
ports(HL) \ ({p∗
L bit!, ps∗
L bit!} ∪{ps! | p◁! ∈S c
L} ∪{masterL?})
= S c
L \ {p◁! | p◁! ∈S c
L}
must hold, respectively.
For the remaining users Hi with i ∈I ∪{A}, i ̸∈{H, L} we simply have to
leave out the special bit-ports, i.e., the equation
ports(Hi) \ ({ps! | p◁! ∈S c
i } ∪{masteri?}) = S c
i \ {p◁! | p◁! ∈S c
i }

Computational Probabilistic Non-interference
9
must hold. Essentially this means that an arbitrary user Hi must not have
any clock-out ports and its “usual” simple ports are connected exactly to
the simple ports of Si. The special ports pH bit?, p∗
L bit!, ps∗
L bit! and ports of
the form masteri?, ps! are excluded because they must be connected to the
master scheduler and to the machines BITH and OUTL.
If the adversary is one of the two emphasized users, i.e., A ∈{H , L}, we
have to leave out the term {ps! | p◁! ∈S c
H}, or {ps! | p◁! ∈S c
L}, respectively.
Alternatively, we can wlog. restrict our attention to those adversaries that
do not have such port names which can easily be achieved by consistent port
renaming.
c) The behaviour of the machine BITH is deﬁned as follows. If BITH receives
an arbitrary input at masterBITH ?, it chooses a bit b ∈{0, 1} at random,
outputs it at pH bit!, and schedules it.
The machine OUTL simply does nothing on inputs at p∗
L bit?. It just “catches”
the inputs to close the collection. This ensures that runs and views of the
conﬁguration are still deﬁned without making any changes.
d) The behaviour of the machine Xn in is deﬁned as follows. Internally, it main-
tains two ﬂags start and ﬂover {0, 1}, both initialized with 0, and a counter
cnt over the ﬁnite index set I ∪{A}. Without loss of generality we assume
I := {1, . . ., n}, so the counter is deﬁned over {0, . . . , n}, initialized with 0
(identifying the number 0 with A). Additionally, it has a counter MaSc poly
if the machine is demanded to be polynomial time, furthermore, a polyno-
mial P must be given in this case that bounds the steps of Xn in. If Xn in is
scheduled, it behaves as follows:
Case 1: Start of the run. If start = 0: Set start := 1 and output 1 at
masterBITH !, 1 at masterBITH
◁!.
Case 2: Schedule users. If ﬂ= 0 and start = 1: If Xn in has to be polynomial
time, it ﬁrst checks cnt = n, increasing MaSc poly in this case and checking
whether MaSc poly < P(k) holds for the security parameter k, stopping at
failure. Now, it sets cnt := cnt +1 mod (n+1), and outputs 1 at mastercnt!, 1
at mastercnt ◁!. If cnt ̸= 0, i.e., the clocked machine is an honest user, it
additionally sets ﬂ:= 1 to handle the scheduling demands of this user at its
next clocking.
Case 3: Handling scheduling demands. If ﬂ= 1 and start = 1: In this case it
outputs 1 at every port ps◁! with p◁! ∈S c
cnt (for cnt = L it also outputs 1 at
ps∗
L bit
◁!) and tests whether it gets a non-empty input at exactly one input
port.3 If this does not hold, it sets ﬂ:= 0 and does nothing. Otherwise, let
ps? denote this port with non-empty input i. Xn in then outputs i at p◁! and
sets ﬂ:= 0. This case corresponds to a valid scheduling demand of the user,
so the corresponding buﬀer is in fact scheduled.
3 More formally, it enumerate the ports and sends 1 at the ﬁrst one. The buﬀer either
schedules a message to Xn in or it does nothing. In both cases Xn in is scheduled again,
so it can send 1 at the second clock-out port and so on. Every received message is
stored in an internal array so the test can easily be applied.

10
Michael Backes and Birgit Pﬁtzmann
We obtain a “rotating” clocking scheme on the set I ∪{A}, so every user
and the adversary will be clocked equally often with respect to the special
master-ports.
Non-interference conﬁgurations are denoted by conf n in
H,L,I = ( ˆM , S, U n in, A)n in
I
with U n in := U ∪{BITH, OUTL, Xn in} but we will usually omit the index I.
conf n in
H,L is called polynomial-time if its underlying multi-party conﬁguration
conf mp
H,L is polynomial-time. The set of all non-interference conﬁgurations of
a system Sys for ﬁxed H, L, and I will be denoted by Confn in
H,L,I(Sys), and the
set of all polynomial-time non-interference conﬁgurations by Confn in
H,L,I,poly(Sys).
✸
Deﬁnition 5. (Non-Interference) Let a ﬂow policy G = (∆, E) for a struc-
ture ( ˆM , S) be given. Given two arbitrary elements H, L ∈I ∪{A}, H ̸= L
with (SH, SL) ∉❀, we say that ( ˆM , S) fulﬁlls the non-interference requirement
NIReqH,L,G
a) perfectly (written ( ˆM , S) |=perf NIReqH,L,G) iﬀfor any non-interference
conﬁguration conf n in
H,L ∈Confn in
H,L,I(Sys) of this structure the inequality
P(b = b∗| r ←runconf n in
H,L ,k; b := r⌈pH bit!; b∗:= r⌈p∗
L bit?) ≤1
2
holds.
b) statistically for a class SMALL (( ˆM , S) |=SMALL NIReqH,L,G) iﬀfor any
non-interference conﬁguration conf n in
H,L ∈Confn in
H,L,I(Sys) of this structure
there is a function s ∈SMALL such that that the inequality
P(b = b∗| r ←runconf n in
H,L ,k; b := r⌈pH bit!; b∗:= r⌈p∗
L bit?) ≤1
2 + s(k)
holds. SMALL must be closed under addition and with a function g also
contain every function g′ ≤g.
c) computationally (( ˆM , S) |=poly NIReqH,L,G) iﬀfor any polynomial-time
non-interference conﬁguration conf n in
H,L ∈Confn in
H,L,I,poly(Sys) the inequality
P(b = b∗| r ←runconf n in
H,L ,k; b := r⌈pH bit!; b∗:= r⌈p∗
L bit?) ≤1
2 +
1
poly(k)
holds.
We write ”|=” if we want to treat all cases together.
If a structure fulﬁlls all non-interference requirements NIReqH,L,G with
(SH, SL) ∉❀, we say it fulﬁlls the (global) requirement NIReqG (( ˆM , S)
|= NIReqG). A system Sys fulﬁlls a ﬂow policy φSys if every structure
( ˆM , S) ∈Sys fulﬁlls its requirement NIReqφSys( ˆ
M ,S), and we consequently
write Sys |= NIReqφSys( ˆ
M,S), or Sys |= φSys for short.
✸

Computational Probabilistic Non-interference
11
4
Preservation of Non-interference under Simulatability
Simulatability essentially means that whatever might happen to an honest user H
in a real system Sysreal can also happen to the same honest user in an ideal sys-
tem Sysid. Formally speaking, for every conﬁguration conf 1 of Sysreal there is
a conﬁguration conf 2 of Sysid with the same users yielding indistinguishable
views of H in both systems [21]. We abbreviate this by Sysreal ≥sec Sysid (Sysreal
is “at least as secure as” Sysid), indistinguishability of the views of H is de-
noted by view conf 1(H) ≈view conf 2(H). Usually only certain “corresponding”
structures ( ˆM1, S1) of Sysreal and ( ˆM2, S2) of Sysid are compared. Structures are
called corresponding or validly mapped if their set of speciﬁed ports are equal. In
this section we show that our deﬁnition of non-interference behaves well under
simulatability. More precisely, we will show that the relation “at least as secure
as” will not change the non-interference relation between two arbitrary users
(one of them might also be the adversary). Usually, deﬁning a cryptographic
system starts with an abstract speciﬁcation of what the system actually should
do, possible implementations have to be proven to be at least as secure as this
speciﬁcation. Such a speciﬁcation usually consists of a monolithic idealized ma-
chine that neither contains any cryptographic details nor any probabilism. Thus,
it can be validated quite easily by formal proof systems, e.g., formal theorem
proving or even automatic model checking, at least if it is not to complex. Hence,
it would be of great use to also verify integrity and privacy properties for this
idealized machine that will automatically carry over to the real system.
Our theorem states that non-interference properties are in fact preserved un-
der the relation “at least as secure as”. In the proof of the preservation theorem,
the following lemma will be needed.
Lemma 1. The statistical distance ∆(φ(vark), φ(var′
k)) for function φ of random
variables is at most ∆(vark, var′
k).
✷
This is a well-known fact; a proof can be found in, e.g., [8].
Theorem 1. (Preservation of Non-Interference Properties) Let a ﬂow policy
φSys2 for a system Sys2 be given, so that Sys2 |= φSys2 holds. Furthermore, let
a system Sys1 be given with Sys1 ≥f Sys2 for a mapping f with S1 = S2 whenever
( ˆM2, S2) ∈f( ˆM1, S1). Then Sys1 |= φSys1 for all φSys1 with φSys1( ˆM1, S1) :=
φSys2( ˆM2, S2) for an arbitrary structure ( ˆM2, S2) ∈f( ˆM1, S1). This holds for the
perfect, statistical, and the computational case.
✷
Proof. We ﬁrst show that φSys1 is a well-deﬁned ﬂow policy for Sys1 un-
der our preconditions. Let an arbitrary structure ( ˆM1, S1) ∈Sys1 be given.
Simulatability implies that for every structure ( ˆM1, S1) ∈Sys1, there exists
( ˆM2, S2) ∈f( ˆM1, S1).
φSys2 is a ﬂow policy for Sys2, so we have a ﬂow policy G( ˆ
M2,S2) = (∆, E) for
( ˆM2, S2). Furthermore, we have S1 = S2 by precondition, so we can indeed build
the same set Γ of blocks on the speciﬁed ports and therefore the same partition

12
Michael Backes and Birgit Pﬁtzmann
∆of the free ports of [ ˆM1].4 Hence, φSys1( ˆM1, S1) is deﬁned on ( ˆM1, S1), so φSys1
is a well-deﬁned ﬂow policy for Sys1.
We now have show that Sys1 fulﬁlls φSys1. Let a structure ( ˆM1, S1) ∈Sys1
and two elements H, L ∈I ∪{A}, H ̸= L with (SH, SL) ∉❀(with respect to
the ﬂow policy φSys1( ˆM1, S1)) be given. We have to show that ( ˆM1, S1) fulﬁlls
the non-interference requirement NIReqH,L,G.
Let
now
a
non-interference
conﬁguration
conf n in
H,L,1
=
( ˆM1, S1,
U n in, A)n in ∈Confn in
H,L,I(Sys1) be given. Because of Sys1 ≥f Sys2 there
is a conﬁguration conf H,L,2
=
( ˆM2, S2, U n in, A′)
∈
Confn in
H,L,I(Sys2) for
( ˆM2, S2) ∈f( ˆM1, S1) with view conf n in
H,L,1(U n in) ≈view conf H,L,2(U n in). More-
over, the honest users U n in are unchanged by simulatability, so conf H,L,2 is
again a non-interference conﬁguration; hence, we write conf n in
H,L,2 in the following
instead of conf H,L,2. As usual we distinguish between the perfect, statistical,
and the computational case. In the computational case, both conﬁgurations
must be polynomial-time.
In the perfect case, we have view conf n in
H,L,1(U n in) = view conf n in
H,L,2(U n in) be-
cause of Sys1 ≥f
perf Sys2. Now, both b := r⌈pH bit! and b∗:= r⌈p∗
L bit? are part
of the view of U n in because BITH and OUTL are elements of U n in, so we
obtain the same probabilities in both conﬁgurations. Our precondition ( ˆM2, S2)
|=perf NIReqH,L,G and our arbitrary choice of conf n in
H,L,1 implies that ( ˆM1, S1)
also fulﬁlls NIReqH,L,G.
We will treat the statistical and the computational case together. In
the statistical (computational) case we have view conf n in
H,L,1(U n in) ≈SMALL
view conf n in
H,L,2(U n in) (view conf n in
H,L,1(U n in) ≈poly view conf n in
H,L,2(U n in)). We as-
sume for contradiction that ( ˆM1, S1) does not fulﬁll the non-interference re-
quirement NIReqH,L,G, so the probability p(k) of a correct guess for b = b∗is
not smaller than 1
2 +s(k) for any s ∈SMALL in the statistical case (or p(k)−1
2 is
not negligible in the computational case). Thus, the advantage ϵ(k) := p(k) −1
2
of the adversary is not contained in SMALL (or ϵ(k) is not negligible). ( ˆM2, S2)
fulﬁlls the non-interference requirement, so in this conﬁguration, the advantage
ϵ′(k) for a correct guess is a function of SMALL in the statistical or negligible
in the computational case.
We can then deﬁne a distinguisher D as follows. Given the views of U n in
in both conﬁgurations it explicitly knows the views of BITH and OUTL. Now D
outputs 1 if b = b∗and 0 otherwise. Its advantage in distinguishing is
|P(D(1k, view conf n in
H,L,1,k(U n in)) = 1) −P(D(1k, view conf n in
H,L,2,k(U n in)) = 1)|
= |1
2 + ϵ(k) −(1
2 + ϵ′(k))| = ϵ(k) −ϵ′(k).
4 More, precisely the block ¯S1 is identiﬁed with ¯S2. The ports of both sets may be
diﬀerent, but this does not matter because our deﬁnition of ﬂow policies only uses
whole blocks, so the diﬀerent ports do not cause any trouble.

Computational Probabilistic Non-interference
13
For the polynomial case, this immediately contradicts our assumption Sys1 ≥f
poly
Sys2 because ϵ(k) −ϵ′(k) is not negligible. For the statistical case, the distin-
guisher D can be seen as a function on the random variables, so Lemma 1 implies
∆(view conf n in
H,L,1,k(U n in), view conf n in
H,L,2,k(U n in))
≥|P(D(1k, view conf n in
H,L,1,k(U n in)) = 1) −P(D(1k, view conf n in
H,L,2,k(U n in)) = 1)|
= ϵ(k) −ϵ′(k).
But ϵ(k) −ϵ′(k) ̸∈SMALL must hold, because ϵ′(k) ∈SMALL and SMALL is
closed under addition. Thus, ∆(view conf n in
H,L,1,k(U n in), view conf n in
H,L,2,k(U n in)) ̸∈
SMALL because SMALL is closed under making functions smaller which yields
the desired contradiction. (SH, SL) ∉❀and ( ˆM1, S1) have been chosen arbitrary
so ( ˆM1, S1) |= NIReqG and ﬁnally Sys1 |= φSys1 which ﬁnishes the proof.
5
A Cryptographic Firewall
In the following we present an example of a system that allows authenticated
communications between two users and furthermore ensures that these two users
cannot be aﬀected by their environment. This yields a ﬂow policy our system
has to (and indeed will) fulﬁll.
5.1
Some Preliminaries
We start with a brief review on standard cryptographic systems and composition,
cf. [18] for more details. In cryptographic protocols every user u usually has
exactly one machine Mu and its machine is correct if and only if the user is
honest.
The machine Mu of user u has special ports inu? and outu! for connecting
to user u. A standard cryptographic system Sys can now be derived by a trust
model. The trust model consists of an access structure ACC and a channel model
χ. ACC is a set of subsets H of {1, . . . , n} and denotes the possible sets of
correct machines. For each set H there will be exactly one structure built by the
machines belonging to the set H. The channel model classiﬁes every connection as
either secure (private and authentic), authenticated or insecure. In the considered
model these changes can easily be achieved via port renaming (see [18]).
For a ﬁxed set H and a ﬁxed channel model we obtain modiﬁed machines
for every machine Mu which we refer to as Mu,H. We denote their combina-
tion by ˆMH (i.e., ˆMH := {Mu,H | u ∈H}), so real systems are given by
Sysreal = {( ˆMH, SH) | H ∈ACC}. Ideal systems typically are of the form
Sysid = {({THH}, SH) | H ∈ACC} with the same sets SH as in the corre-
sponding real system Sysreal.
We now brieﬂy review what has already been proven about composition of
reactive systems. What we actually want is the relation “at least as secure as”
to be consistent with the composition of systems. Assume that we have already

14
Michael Backes and Birgit Pﬁtzmann
≥
	

	

	

	

	
≥
≥
≥
	

	
Fig. 4. Composition of systems
proven that a system Sys0 is at least as secure as another system Sys′
0. Typically,
Sys0 is a real system whereas Sys′
0 is an ideal speciﬁcation of the real system. If
we now consider larger protocols that use Sys′
0 as an ideal primitive we would
like to be able to securely replace it with Sys0. In practice this means that we
replace the speciﬁcation of a system with its implementation.
Usually, replacing means we have another system Sys1 using Sys′
0; we call this
combination Sys∗. We now want to replace Sys′
0 with Sys0 inside of Sys∗which
gives a combination Sys#. Typically, Sys# is a completely real system whereas
Sys∗is at least partly ideal. This fact is illustrated in the left and middle part of
Figure 4. The composition theorem now states that this replacement maintains
security, i.e., Sys# is at least as secure as Sys∗(see [18] for further details).
After this brief review we can turn our attention to the cryptographic ﬁre-
wall. The construction of both our ideal and our real system can be explained
using Figure 4. Our ideal speciﬁcation is based on an ideal speciﬁcation for se-
cure message transmission with ordered channels introduced in [2] which we will
slightly modify to ﬁt our requests. Mainly, we have to model reliable channels
to avoid denial of service attacks. We will denote this modiﬁed ideal system by
Sys′
0 following the notation of Figure 4. Furthermore, a possible implementa-
tion has also been presented in [2] which we have to modify similar to the ideal
speciﬁcation to maintain the at least as secure as relation.
Our cryptographic ﬁrewall will then be derived by deﬁning a new system Sys1
so that combination with Sys′
0 yields the ideal system, replacing Sys′
0 with Sys0
ﬁnally yields a possible implementation. Sys1 will be designed to ﬁlter messages
sent by “wrong” senders that should not be allowed to inﬂuence the special users
according to the ﬂow policy shown in Figure 5. According to Figure 4, we denote
our ideal system as Sys∗and our real implementation as Sys#.
We start with a brief review of the ideal system for secure message trans-
mission with ordered channels and present our modiﬁcations of the system af-
terwards which will be essential for achieving non-interference. After that, we
introduce our system Sys1, and brieﬂy sketch the concrete implementation.
We then prove that our ideal system Sys∗fulﬁlls its non-interference require-
ments. Finally, we apply our preservation theorem 1 and conclude that these
non-interference requirements carry over to the concrete implementation, which
successfully ﬁnishes our attempt to design a real example that ﬁts our non-
interference deﬁnition.

Computational Probabilistic Non-interference
15

	


Fig. 5. Sketch of the ﬂow policy of our system. Only one non-emphasized user
S1 is considered and some edges of the graph are omitted. Missing edges are of
the form “❀”
5.2
The Ideal System
Let n denote the number of participants, I := {1, . . . , n} the set of indices of the
considered participants, and IA := I ∪{A} the set of participants including the
adversary. In the following we will identify these indices with their corresponding
user. Intuitively, we want a system that fulﬁlls the ﬂow policy shown in Figure 5.
We consider two distinguished users a and b with {a, b} ∈I. We now have
two blocks of speciﬁed ports Sa and Sb, so that information must not ﬂow to
one of these ports from the outside. More precisely, we have non-interference
requirements NIReqi1,i2,G for every pair (i1, i2) with i1 ∈IA \ {a, b}, i2 ∈{a, b}.
We start with a brief description of the ideal speciﬁcation for secure mes-
sage transmission with ordered channels. The speciﬁcation is of the typical form
Sys′
0 = {({THH}, SH)|H ⊆M}, i.e., there is one structure for every subset of
the machines, denoting the honest users.
The ideal machine THH models initialization, sending and receiving of mes-
sages. A user u can initialize communications with other users by inputting
a command of the form (snd init) to the port inu? of THH. In real systems, initial-
ization corresponds to key generation and authenticated key exchange. Sending
of messages to a user v is triggered by a command (send, m, v). If v is honest, the
message is stored in an internal array deliver spec
u,v of THH together with a counter
indicating the number of the message. After that, a command (send blindly, i, l, v)
is output to the adversary, l and i denote the length of the message m and its po-
sition in the array, respectively. This models that the adversary will notice in the
real world that a message has been sent and he might also be able to know the
length of that message. We speak of tolerable imperfections that are explicitly
given to the adversary. Because of the underlying asynchronous timing model,
THH has to wait for a special term (receive blindly, v, i) or (rec init, u) sent by
the adversary, signaling, that the message stored at the ith position of deliver spec
u,v
should be delivered to v, or that a connection between u and v should be ini-
tialized. In the ﬁrst case, THH reads (m, j) := deliver spec
u,v [i] and checks whether
msg outspec
u,v ≤j holds for a message counter msg outspec
u,v . If the test is successful
the message is delivered and the counter is set to j + 1, otherwise THH outputs
nothing. The condition msg outspec
u,v ≤j ensures that messages can only be de-
livered in the order they have been received by THH, i.e., neither replay attacks

16
Michael Backes and Birgit Pﬁtzmann
	




	



	
 !
 !	

"

"	
 !"
 !"	
Fig. 6. Sketch of system Sys1
nor reordering messages is possible for the adversary; cf. [2] for details. The user
will receive inputs of the form (receive, u, m) and (rec init, u), respectively. If v is
dishonest, THH will simply output (send, m, v) to the adversary. Finally, the ad-
versary can send a message m to a user u by sending a command (receive, v, m)
to the port from advu? of THH for a corrupted user v, and he can also stop the
machine of any user by sending a command (stop) to a corresponding port of
THH, which corresponds to exceeding the machine’s runtime bound in the real
world.
Necessary modiﬁcations of the abstract scheme for secure ordered channels. We
want our system to fulﬁll our ﬂow policy shown in Figure 5, so especially the
non-interference requirement NIReqA,a,G must hold. If we explicitly allow the
adversary to schedule the communication between Ha and Hb he can obviously
achieve two distinguishable behaviours by denial of service attacks as follows.
On the one hand, he directly schedules every message sent from Hb to Ha in one
behaviour, on the other hand he does not schedule any message sent from Hb to
Ha. This problem cannot be solved by the ﬁltering system Sys1 if scheduling of
the communication channel is done by the adversary.5 In practice, this means
that two persons will not be able to communicate without being interfered from
outside if the channel they use can be cut oﬀby the adversary. A possible solu-
tion for the problem is to deﬁne reliable, non-authenticated channels between a
and b, so messages sent between two participants are not only output to the
adversary but also output to the recipient and directly scheduled. Obviously,
channels which are reliable and authenticated could be used as well for sending
of messages, but in this case, we would no longer need the underlying cryp-
tography (e.g., authentication). Therefore, we only consider these authenticated
channels for key exchange as usual, but sending of messages is still performed
over non-authenticated channels. The modiﬁcations carry over to the trusted
host THH as follows:
– If THH receives an input (snd init) from Ha, it implicitly initializes a com-
munication with Hb and outputs (snd init) to the adversary, (rec init, a) to
Hb and schedules the second output.
5 Sys1 can only sort out messages from “wrong” senders, the messages mentioned are
sent by the “valid” user Hb, so they have to be delivered because Sys1 has no internal
clock to check for denial of service attacks.

Computational Probabilistic Non-interference
17

###


	

###





###



	
$
$
$
Fig. 7. Ideal system for non-interfered communication
– If THH receives an input (send, m, b) from Ha, it outputs (send blindly, i, l, b)
to the adversary, (receive, m, a) to Hb scheduling the second output.
These modiﬁcations are also done for switched variables a and b. We will omit
a more detailed description of the machine THH due to lack of space and refer the
reader to the long version of this paper. After this brief review and modiﬁcation
of Sys′
0 we can turn our attention to the system Sys1. The system Sys1 is built
by additional machines Mn in
u
for u ∈{a, b}. These machines will be inserted
between the users and the trusted host THH, see Figure 7. Formally, we obtain
the following scheme:
Scheme 1 (Sys1) Let n ∈N and polynomials L, s, s′ ∈N[x] be given. Here n
denotes the number of intended participants, L(k) bounds the message length
and s(k), s′(k) bound the number of messages each user can send and receive
respectively for a security parameter k. Let I := {1, . . . , n} denote the set of
possible users again and a, b ∈I the special users that should not be inﬂuenced
from outside. Then
Sys1 := {( ˆM , S)}
with
ˆM
=
{Mn in
a
, Mn in
b
} and S c
:=
{outu?, inu!, inu
◁! | u
∈
{a, b}} ∪
{in′
u?, out′
u!, out′
u
◁! | u ∈{a, b}}. Without loss of generality we just describe
the ports and the behaviour of machine Mn in
a
. The machine Mn in
b
is deﬁned
analogously by exchanging the variables a and b. The ports of machine Mn in
a
are
{ina?, outa!, outa ◁!} ∪{out′
a?, in′
a!, in′
a
◁!} ∪{pMb?, pMa!, pMa
◁!}, cf. Figure 6.
Internally, Mn in
a
maintains a counter sa
∈{0, . . . , s(k)} and an array
(s′
a,u)u∈I over {0, . . ., s′(k)} bounding the number of messages Ha can send
and receive, respectively, and a variable stoppeda ∈{0, 1} all initialized with 0
everywhere. The state-transition function of Mn in
a
is deﬁned by the following
rules, written in a simple pseudo-code language.

18
Michael Backes and Birgit Pﬁtzmann
Initialization.
– Send initialization: On input (snd init) at ina?: If sa < s(k) it sets sa :=
sa + 1, otherwise it stops. If stoppeda = 0 it outputs (snd init) at in′
a!, 1 at
in′
a
◁!, otherwise it outputs (snd init) at pMa!, 1 at pMa
◁!.
– Receive initialization: On input (rec init, u) at out′
a?: It ﬁrst checks
whether s′
a,u < s′(k) hold. In this case it sets s′
a,u = s′
a,u + 1, otherwise it
stops. If stoppeda = 0 it checks u = b. If this also holds it outputs (rec init, b)
at outa! and 1 at outa ◁!. On input (rec init, b) at pMb?, it outputs (rec init, b)
at outa! and 1 at outa ◁!.
Sending and receiving messages.
– Send: On input (send, m, v) at ina? with m ∈Σ+ and len(m) ≤L(k) it
checks whether sa < s(k). If this holds it sets sa := sa+1, otherwise it stops.
If stoppeda = 0 holds, it outputs (send, m, v) at in′
a!, 1 at in′
a
◁!. Otherwise
it ﬁrst checks v = b. After a successful test it outputs (receive, a, m) at pMa!
and 1 at pMa
◁!.
– Receive: On input (receive, u, m) at out′
a? it ﬁrst checks whether s′
a,u <
s′(k). If this holds it sets s′
a,u := s′
a,u + 1, otherwise it stops. If u = b holds
it outputs (receive, b, m) at outa! and 1 at outa ◁!. On input (receive, b, m) at
pMb? it outputs (receive, b, m) at outa! and 1 at outa◁!.
– Stop: On input (stop) at out′
a? or pMb?: If stoppeda = 0, it sets stoppeda = 1
and outputs (stop) at pMa! and 1 at pMa
◁!.
The special communication ports pMa and pMb are just included to prevent denial
of service attacks. We already brieﬂy stated in our review of Sys′
0 that a mighty
attacker could simply overpower the machine of an honest user by sending too
many messages, i.e., to exceed its runtime bound in the real world. In the ideal
system this is modeled by letting the adversary stop arbitrary machines any
time he likes. If we now consider an adversary that stops the machine of user a
at the very start of the run and another one that never stops this machine, we
would certainly obtain diﬀerent views for this user. This problem cannot really
be avoided if we do not provide additional channels for communication that
guarantee availability. In practice this would correspond to a connection that
contains trash all the time sent by the adversary, so the users (their machines
in our case) would certainly look for a new way to communicate. Furthermore,
this problem is much weaker in practice than in theory because it ought to be
impossible (or at least very diﬃcult) for an adversary to overpower a machine
(the machine would surely be able to take countermeasures). If we did not con-
sider these sorts of attacks the ports pMa and pMb could as well be omitted.
Finally, a stopped machine Mn in
a
would want the machine Mn in
b
also to use the
special communication ports, so it will stop the machine as soon it has been
stopped itself. Before we now build the combination of both systems to obtain
our complete system Sys∗, we rename the ports inu?, outu! and outu◁! of Sys′
0
into in′
u?, out′
u! and out′
u
◁!, respectively, for u ∈{a, b} such that Sys′
0 and Sys1
are connected in the desired way. Furthermore, we restrict the structures of

Computational Probabilistic Non-interference
19
Sys′
0 to all sets H with {a, b} ⊆H. Combination now means that we combine
every structure of Sys′
0 with the (only) structure of Sys1. The resulting system
Sys∗= {( ˆMH, SH) | {a, b} ⊆H ⊆I} is shown in Figure 7.
Remark 2. It is quite obvious how to modify the system Sys1 to an arbitrary
set of users (instead of {a, b}) that have to be guarded by the ﬁrewall. Moreover
we can easily consider multiple disjoint sets of users so that a user can com-
municate with other users of its own set without being interfered from outside.
This corresponds to multiple ﬁrewalls and can easily be achieved by modifying
the ﬁltering system Sys1, so our speciﬁcation carries over to arbitrary transitive
ﬂow policies.
5.3
The Real System
The real system Sys# is derived by replacing the ideal system Sys′
0 with its
concrete implementation Sys0. For our purpose, it is suﬃcient to give an informal
review of the system Sys0. The system is a standard cryptographic system of
the form Sys0 = {( ˆMH, SH) | H ⊆M}, i.e., any subset of participants may be
dishonest. It uses asymmetric encryption and digital signatures as cryptographic
primitives. A user u can let his machine create signature and encryption keys
that are sent to other users over authenticated channels autu,v. Furthermore,
messages sent from user u to user v will be signed and encrypted by Mu and
sent to Mv over an insecure channel netu,v, representing the net in the real
world. Similar to THH each machine maintains internal counters which are used
for discarded messages that are out of order. The adversary is able to schedule
the communication between the users, and he can furthermore send arbitrary
messages m to arbitrary users u for a dishonest sender v.
We now have to implement the modiﬁcation of the ideal system in our
concrete implementation. This can simply be achieved by changing the chan-
nel type of the (formerly insecure) channels between a and b to reliable, non-
authenticated. This channel type is not comprised by the original model of [18],
but it can be deﬁned quite easily, cf. [1]. Moreover, by inspection of the proof
of [2] and [18], it is easy to see that the “at least as secure as” relation still holds
for these modiﬁed systems Sys0 and Sys1 with only slight changes in the proof.
Therefore, and due to lack of space, we omit the proof here and refer the reader
to the long version again.
5.4
Non-interference Proof
In the following we will show that our abstract system Sys∗(cf. Figure 7) fulﬁlls
its non-interference requirements given by the following ﬂow policy. For two
given elements i1, i2 ∈I ∪{A}, we deﬁne (Si1, Si2) ∉❀iﬀi1 ∈(H \ {a, b}) ∪{A}
and i2 ∈{a, b}. The ﬂow policy is sketched in Figure 5.
Theorem 2. (Non-Interference Properties of Sys∗) Let an arbitrary structure
({THH, Mn in
a
, Mn in
b
}, SH) ∈Sys∗be given. For the sake of readability, we set
ˆMH := {THH, Mn in
a
, Mn in
b
} in the following. Let a function φSys∗be given

20
Michael Backes and Birgit Pﬁtzmann
that maps the structures ( ˆMH, SH) of Sys∗to the ﬂow policy G( ˆ
MH,SH) =
(∆( ˆ
MH,SH), E( ˆ
MH,SH)) as deﬁned above. The partition ∆( ˆ
MH,SH) of SH is de-
ﬁned by ∆( ˆ
MH,SH) := {Si | i ∈H} ∪{¯S} with S c
i := {outi?, ini!, ini
◁!} for i ∈H
and SA := ¯S = free([ ˆMH]) \ (
i∈H Si). Then Sys∗fulﬁlls φSys∗perfectly.
✷
Before we turn our attention to the proof of Theorem 2, we present the following
lemma.
Lemma 2. By deﬁnition of the system, the following invariants hold for all
possible runs of the conﬁguration.
1. The collection {Mn in
a
, Mn in
b
}, i.e., the system Sys1, is polynomial-time.
2. If Ha receives an input at outa?, it is of the form (rec init, b) or (receive, b, m)
for an arbitrary m ∈Σ+. If Ha receives an input at mastera?, it is sent by
the master scheduler and is of the form 1.
3. No output of Xn in at mastera! depends on inputs from other machines. Each
machine is clocked equally often using a rotating clocking scheme. Further-
more, each output at a port p◁! for p◁! ∈S c
a and the scheduled message does
only depend on prior outputs of Ha at port ps! and p!.
4. If Ha receives a term of the form (rec init, b) at outa?, it is a direct conse-
quence of the input (snd init) sent by Hb (i.e., the scheduling sequence must
have been Hb, Xn in, Mn in
b
, THH, Mn in
a
, Ha or Hb, Xn in, Mn in
b
, Mn in
a
, Ha). This
also implies that initializing a communication between Ha and Hb is not
possible for the adversary, so there cannot be any replay attacks with initial-
ization commands because they will be sorted out by THH.
5. If Ha receives a term of the form (receive, b, m) at outa?, it is a direct
consequence (in the sense of Point 4) of the message (send, a, m) sent by
Hb, so the scheduling sequence has been Hb, Xn in, Mn in
b
, THH, Mn in
a
, Ha or
Hb, Xn in, Mn in
b
, Mn in
a
, Ha. Thus, it is not possible for the adversary to pretend
to be user Hb and furthermore the number of received messages of this form
equals the number of messages sent by Hb to Ha. Therefore, the adversary
can neither replay these messages nor throw them away.
The invariants also hold if we exchange the variables a and b.
✷
The proof is omitted due to lack of space. It will be contained in the long version.
Proof (Theorem 2). We have to show that Sys∗fulﬁlls the non-interference re-
quirement φSys∗. Let an arbitrary structure ( ˆMH, SH) ∈Sys∗be given so we
have a ﬂow policy G( ˆ
MH,SH) = (∆( ˆ
MH,SH), E( ˆ
MH,SH)) for this structure. Let now
two arbitrary blocks Si1, Si2 ∈∆( ˆ
MH,SH) with i1 ∈(H \ {a, b}) ∪{A}, i2 ∈{a, b}
be given, so (Si1, Si2) ∉❀must hold. By deﬁnition of non-interference showing
( ˆMH, SH) |=perf NIReqi1,i2,G is suﬃcient for proving Sys∗|=perf φSys ∗.
Let a non-interference conﬁguration conf n in
i1,i2 = ( ˆMH, SH, U n in, A)n in for
this structure be given. Without loss of generality we can assume i2 = a because
of the symmetry of the ﬂow policy. Depending on the choice of the bit b we denote
the two families of views of Ha by view conf n in
i1,a ,0(Ha) and view conf n in
i1,a ,1(Ha).

Computational Probabilistic Non-interference
21
Assume for contradiction that the probability of a correct guess b = b∗is greater
than 1
2, which implies view conf n in
i1,a ,0({Ha, Hb}) ̸= view conf n in
i1,a ,1({Ha, Hb}). First
of all, we can exclude denial of service attacks applying Part 3 of the above
lemma, so there has to be a ﬁrst input at {Ha, Hb} with diﬀerent probability in
both cases because Part 3 ensures that scheduling of messages sent by a user
only depends on its own prior behaviour. We will now use the previous lemma
to show that this cannot happen.
By Part 2 of Lemma 2 this input can only be of the form (rec init, u),
(receive, u, m) at outu?, or 1 at masteru? for u ∈{a, b}. We will in the fol-
lowing write ¯u for the other emphasized user (i.e., ¯u ∈{a, b} \ {u}). Assume
this input to be of the ﬁrst form. Now Part 4 implies that this input is a direct
consequence of an input (snd init) sent by the other emphasized user H¯u. Hence,
there had to be an input of H¯u with diﬀerent probability in both cases which
contradicts our assumption of the ﬁrst diﬀerent input, so there cannot be any
inﬂuence from outside Sys1. Thus, we obtain identical probability distributions
for possible inputs of Ha in both cases yielding the desired contradiction.
Now assume this input to be of the form (receive, u, m). By Part 5 the corre-
sponding input (send, ¯u, m) must have been sent directly by Hu with exactly the
same message m. Furthermore, the underlying system for secure ordered chan-
nels ensures that the message has been sent exactly that often as Ha receives
this input, so there cannot be any inﬂuence from outside because of the same
reason as in the ﬁrst case.
Finally, assume this input to be at port masteru?. This input does not depend
on arbitrary behaviours of other machines by Part 3 so we obtain identical
probability distributions again. Therefore, the views must in fact be identical in
both cases so the probability of a correct guess b = b∗is exactly 1
2. Thus, we
have Sys∗|=perf φSys∗.
After proving the non-interference property for the ideal speciﬁcation, we now
concentrate on the concrete implementation.
Theorem 3. (Non-Interference Properties of Sys#) The real system Sys# ful-
ﬁlls the non-interference property φSys# computationally, with ϕSys# given as in
theorem 1. In formulas, Sys# |=poly φSys#.
✷
Proof. Putting it all together, we know that the original and also the modiﬁed
real implementation of secure message transmission with ordered channels is
computationally at least as secure as its (modiﬁed) speciﬁcation. Using Part 1
of Lemma 2 we know that the system Sys1 is polynomial-time, which is an es-
sential precondition for applying the composition theorem in the computational
case, so we have Sys# ≥poly Sys∗. Since perfect fulﬁllment of non-interference
requirements implies computational fulﬁllment, we have Sys# |=poly φSys# using
theorem 1.

22
Michael Backes and Birgit Pﬁtzmann
6
Conclusion
We have presented the ﬁrst general deﬁnition of probabilistic non-interference
in reactive systems which includes a computational case (Section 3). Our ap-
proach is mainly motivated by the concept of simulatability which is funda-
mental for modern cryptography, and it might help to build a bridge between
prior research in the ﬁeld of information ﬂow and designing systems involv-
ing real cryptographic primitives. We have shown that our deﬁnition behaves
well under simulatability (Section 4), which enables modular proofs and step-
wise reﬁnement without destroying the non-interference properties. This is not
only important for the development process of cryptographic protocols but also
because non-interference properties of ideal systems not containing any crypto-
graphic details can often easily be validated by formal proof tools whereas real
systems are usually much more diﬃcult to validate. As an example, we have pre-
sented an abstract speciﬁcation of a cryptographic ﬁrewall guarding two honest
users from their environment (Section 5). Moreover, we have presented a con-
crete implementation that also ﬁts our deﬁnition, which we have shown using
our preservation theorem.
Acknowledgments
We thank Heiko Mantel, Matthias Schunter, and Michael Waidner for interesting
discussions.
References
[1] M. Backes.
Cryptographically sound analysis of security protocols.
Ph.D
thesis, Computer Science Department, Saarland University, 2002. Available at
http://www-krypt.cs.uni-sb.de/∼mbackes/diss.ps.
19
[2] M. Backes, C. Jacobi, and B. Pﬁtzmann.
Deriving cryptographically sound
implementations using composition and formally veriﬁed bisimulation. In Pro-
ceedings of Formal Methods Europe 2002 (FME’02), Copenhagen, 2002. 14, 16,
19
[3] D. Bell and L. LaPadula. Secure computer systems: Uniﬁed exposition and mul-
tics interpretation. Technical Report ESD-TR-75-306, The Mitre Corporation,
Bedford MA, USA, March 1976.
1
[4] D. Clark, C. Hankin, S. Hunt, and R. Nagarajan. Possibilistic information ﬂow
is safe for probabilistic non-interference. Workshop on Issues in the Theory of
Security (WITS’00), available at www.doc.ic.ac.uk/ clh/Papers/witscnh.ps.gz.
1
[5] D. E. Denning. A lattice model of secure information ﬂow. Communications of
the ACM 19/5 (1976) 236-243.
1
[6] J. A. Goguen and J. Meseguer.
Security policies and security models.
IEEE
Symposium on Security and Privacy, IEEE Computer Society Press, Washington
1982, 11-20.
1
[7] J. A. Goguen and J. Meseguer. Unwinding and inference control. IEEE Sym-
posium on Security and Privacy, IEEE Computer Society Press, Oakland 1984,
75-86.
1

Computational Probabilistic Non-interference
23
[8] O. Goldreich. Foundations of cryptography: Basic tools. Cambridge University
Press, 2001.
11
[9] J. W. Gray III. Probabilistic interference. IEEE Symposium on Security and
Privacy, IEEE Computer Society Press, Los Alamitos 1990, 170-179.
1
[10] J. W. Gray III. Toward a mathematical foundation for information ﬂow security.
Journal of Computer Security, Vol.1, no.3,4, 1992, 255-295.
1
[11] C. A. R. Hoare.
Communicating sequential processes.
International Series in
Computer Science, Prentice Hall, Hemel Hempstead 1985.
3
[12] P. Laud. Semantics and program analysis of computationally secure information
ﬂow. 10th European Symposium On Programming (ESOP 2001), LNCS 2028,
Springer-Verlag, Berlin 2001, 77-91.
2
[13] N. Lynch. Distributed algorithms. Morgan Kaufmann Publishers, San Francisco
1996.
3
[14] H. Mantel. Unwinding possibilistic security properties. 6th European Sympo-
sium on Research in Computer Security (ESORICS’00), Toulouse 2000, 238-254.
1
[15] D. McCullough. Speciﬁcations for multi-level security and a hook-up property.
IEEE Symposium on Security and Privacy, IEEE Computer Society Press, Oak-
land 1987, 161-166.
1
[16] J. McLean. Security models. in: John Marciniak (ed.): Encyclopedia of Software
Engineering; Wiley Press, 1994.
1
[17] J. McLean. Security models and information ﬂow. IEEE Symposium on Security
and Privacy, IEEE Computer Society Press, Los Alamitos 1990, 180-187.
1
[18] B. Pﬁtzmann and M. Waidner. A model for asynchronous reactive systems and
its application to secure message transmission. IEEE Symposium on Security
and Privacy, Oakland, May 2001, 184-201.
2, 3, 13, 14, 19
[19] D. Sutherland. A model of information. 9th National Computer Security Con-
ference; National Bureau of Standards, National Computer Security Center,
September 1986, 175-183.
1
[20] J. T. Wittbold and D. M. Johnson. Information ﬂow in nondeterministic systems.
IEEE Symposium on Security and Privacy, IEEE Computer Society Press, Los
Alamitos 1990, 144-161.
1
[21] A. C. Yao. Protocols for secure computations. 23rd Symposium on Foundations
of Computer Science (FOCS) 1982, IEEE Computer Society, 1982, 160-164. 11
[22] A. Zakinthinos and E. S. Lee. A general theory of security properties. IEEE
Symposium on Security and Privacy, IEEE Computer Society Press, Washington
1997, 94-102.
1

Bit-Slice Auction Circuit
Kaoru Kurosawa1 and Wakaha Ogata2
1 Ibaraki University
4-12-1 Nakanarusawa, Hitachi, Ibaraki, 316-8511, Japan
kurosawa@cis.ibaraki.ac.jp
2 Tokyo Institute of Technology
2–12–1 O-okayama, Meguro-ku, Tokyo 152–8552, Japan
wakaha@ss.titech.ac.jp
Abstract. In this paper, we introduce a bit-slice approach for auctions
and present a more eﬃcient circuit than the normal approach for the
highest-price auction. Our circuit can be combined with any auction
protocol based on general circuit evaluation. Especially, if we combine
with the mix and match technique, then we can obtain a highest-price
auction protocol which is at least seven times faster. A second-price auc-
tion protocol is also easily constructed from our circuit.
Keywords: auction, multiparty protocol, bit-slice, circuit, mix and
match
1
Introduction
1.1
Sealed-Bid Auction
Auctions are getting very popular in the Internet. They are now a major area
in the web electric commerce.
A sealed-bid auction consists of two phases, the bidding phase and the open-
ing phase. In the bidding phase, all the bidders submit their bidding prices to the
auctioneer. In the opening phase (of the highest-price auction), the auctioneer
announces the highest price and the identities of the winners.
In the second-price auction (also known as Vickrey auction), the highest
bidder wins, and the clearing price, the price that the winner has to pay, is
equal to the second highest bid. (It is closer to the real life auction than the
highest-price auction.)
Throughout the paper, we assume that:
– There are n servers.
– There are m bidders, denoted by A1, A2, . . . , Am.
– Each bidder Ai has a k-bit bid Bi = (b(k−1)
i
, . . . , b(0)
i )2.
In general, X = (x(k−1), . . . , x(0))2 denotes a k-bit integer, where x(k−1)
denotes the most signiﬁcant bit and x(0) denotes the least signiﬁcant bit.
D. Gollmann et al. (Eds.): ESORICS 2002, LNCS 2502, pp. 24–38, 2002.
c
⃝Springer-Verlag Berlin Heidelberg 2002

Bit-Slice Auction Circuit
25
1.2
Auction Protocols Based on Circuit Evaluation
In the trivial scheme which assumes a single trusted auctioneer, the auctioneer
knows all the bidding prices. He may also tell a lie about the highest price and
the winners.
Hence we need a cryptographically secure auction protocol which satisﬁes
privacy and correctness. The correctness means that the announced highest price
and the identities of the winners are guaranteed to be correct. The privacy means
that no adversary can compute any other information.
In principle, it is known that any function can be computed securely by
using general multiparty protocols [25, 16, 5, 9, 17]. (They are also known as
general function evaluation protocols.) A problem is, however, that the cost of
each bidder is very large in their general forms. Therefore, several schemes have
been proposed to achieve eﬃcient and secure auctions.
In the auction protocol of Naor, Pinkas and Sumner [22] which involves two
servers, a proxy oblivious transfer protocol was introduced and it was combined
with Yao’s garbled circuit technique [25]. Jakobsson and Juels pointed out a ﬂaw
and it was ﬁxed by Juels and Szydlo [21]. The ﬁxed protocol is secure if the two
servers do not collude. The cost of each server is O(mkt), where t is a security
parameter.
Jakobsson and Juels introduced a new general multiparty protocol called
mix and match and showed its application to auctions [20]. The mix and match
technique avoids the use of veriﬁable secret sharing schemes (VSS) which is
intensively used in the other general multiparty protocols. In their auction pro-
tocol (JJ auction protocol), therefore, each bidder Ai has only to submit her
encrypted bidding price without executing a VSS. The cost of each server is
O(mnk) exponentiations.
Cramer, Damg˚ard and Nielsen introduced another general multiparty proto-
col which avoids VSS [10]. It can also be used for auctions. While the mix and
match protocol is based on the DDH assumption alone, it is not known if this
is possible for this protocol. On the other hand, the round complexity is O(d)
while it is O(n + d) in the mix and match protocol, where d is the depth of the
circuit of a given function. The message complexities are the same.
Baudron and Stern showed an auction protocol which assumes a semi-trusted
auctioneer T [4]. In this protocol, T blindly and noninteractively evaluates a cir-
cuit whose output tells if Ai is a winner or not for each bidder Ai. Ai knows if
he is a winner by decrypting the output ciphertext of this circuit. T learns no
information if he does not collude with any bidder. The cost of T is Θ(mkm).
(This protocol is not based on general circuit evaluation. It uses some special
predicates and makes use of its special properties.)
1.3
Our Contribution
In general, a multiparty protocol for computing a function f(x1, . . . , xn) is de-
signed as follows. First we draw a Boolean circuit Cf which computes f. We

26
Kaoru Kurosawa and Wakaha Ogata
next apply a general gate evaluation technique to each gate of Cf. Therefore,
the smaller the circuit size is, the more eﬃcient the protocol is.
The normal approach for circuit design for auctions is to compare the bidding
prices one by one (in other words, to run a millionaire’s problem protocol in
order). To the authors’ knowledge, the smallest size circuit in this approach for
the highest-price auction requires 7mk logical gates and m Selectk gates, where
a logical gate has 2 input and 1 output bits, and a Selectk gate has 2k + 1 input
and k output bits. (We present such a circuit in Sec. 2.)
In this paper, we introduce a bit-slice approach for auctions and present
a more eﬃcient circuit than the normal approach for the highest-price auction.
The proposed circuit requires only 2mk logical gates while the normal approach
circuit requires 7mk logical gates and m Selectk gates as shown above.
Suppose that Bmax
= (b(k−1)
max , . . . , b(0)
max)2 is the highest bidding price,
where b(k−1)
max
denotes the most signiﬁcant bit and b(0)
max denotes the least sig-
niﬁcant bit. Then the proposed approach ﬁrst determines b(k−1)
max
by looking at
the most signiﬁcant bits of all the bids. It next determines b(k−2)
max
by looking at
the second most signiﬁcant bits of all the bids, and so on.
Our circuit can be combined with any auction protocol based on general
circuit evaluation. Especially, if we combine our circuit with the mix and match
technique, then we can further reduce the number of gates just to mk by using
the homomorphic property of the encryption function. Hence we can obtain
a protocol which is at least seven times faster than JJ auction protocol.
We also show that a second-price auction protocol (which is closer to the
real-life auction than the highest-price auction) can be easily obtained from our
bit-slice circuit for the highest-price auction.
1.4
Other Related Works
There are many auction protocols which do not use circuit evaluation. However,
they have problems such as follows.
The ﬁrst cryptographic auction scheme was proposed by Franklin and Re-
iter [14]. This scheme is not fully private, in the sense that it only ensures the
conﬁdentiality of bids until the end of the protocol.
In the scheme of Cachin [6] which involves two servers, a partial order of bids
is leaked to one of the two servers. The cost of each bidder is O(mkt) and the
cost of each server is O(m2kt), where t is a security parameter.
In the scheme of Di Crescenzo [13] which involves a single server, a honest
but curious server does not learn any information under the quadratic residu-
osity assumption. However, nothing is known about the security if the server
is malicious. The cost of each bidder is O(m2k2) and the cost of server is also
O(m2k2).
Some other works require O(mn2k) cost for each server [18, 23]. In the scheme
of [3], the cost of a server is O(m2k) and the cost of each bidder is O(2k). Note
that these costs are much larger than the cost of auction protocols based on
circuit design such as [22, 20].

Bit-Slice Auction Circuit
27
1.5
Organization of the Paper
In Sec. 2, we present the normal approach for circuit design for auctions. In
Sec. 3, we propose a bit-slice circuit for the highest-price auction. In Sec. 4, we
brieﬂy describe the mix and match technique. In Sec. 5, we show a new highest-
price auction protocol which is obtained by combining the bit slice circuit and
the mix and match technique. In Sec. 6, we present our second-price auction
protocol.
2
Normal Approach for Auction Circuit Design
The normal approach for circuit design for auctions is to compare the bidding
prices one by one (in other words, to run a millionaire’s problem protocol in
order). In this section, we present such a circuit which seems to be the most
eﬃcient.
2.1
Primitive Gate
For two bits x and y, deﬁne Bigger1 and EQ1 by
Bigger1(x, y) =

1 if x > y
0 otherwise
EQ1(x, y) =
1 if x = y
0 otherwise
We next deﬁne a gate Selectκ which has 2κ + 1 input bits and κ output bits
as follows.
Selectκ(b, x(κ−1), . . . , x(0), y(κ−1), . . . , y(0)) =
(x(κ−1), . . . , x(0)) if b = 1
(y(κ−1), . . . , y(0)) if b = 0
2.2
Boolean Circuit for the Millionaire’s Problem
For X = (x(k−1), . . . , x(0))2 and Y = (y(k−1), . . . , y(0))2, deﬁne
Biggerk(X, Y ) =
1 if X > Y
0 otherwise
Maxk(X, Y ) =

X if X > Y
Y
otherwise
EQk(X, Y ) =
1 if X = Y
0 otherwise
We ﬁrst show two circuits for the millionaire’s problem, Biggerk and Maxk, which
seem to be the most eﬃcient.
Let ak = 1. For i = k −1 to 1, do
ai = ai+1 ∧EQ1(x(i), y(i)).

28
Kaoru Kurosawa and Wakaha Ogata
Then
Biggerk(X, Y ) = Bigger1(x(k−1), y(k−1))
∨(Bigger1(x(k−2), y(k−2)) ∧ak−1)
...
∨(Bigger1(x(0), y(0)) ∧a1).
Maxk(X, Y ) = Selectk(Biggerk(X, Y ), X, Y ).
2.3
Boolean Circuit for Auction
We next present two circuits for the highest-price auction, Highest and Winner.
Highest outputs the highest bidding price of m bids, B1, . . . , Bm. It is ob-
tained by implementing the following algorithm by a circuit. Let Bmax = 0. For
i = 1, . . . , m, do
Bmax := Maxk(Bmax, Bi).
It is clear that the ﬁnal Bmax is the highest bidding price.
Winner is a circuit which outputs the winners. That is,
Winner(B1, . . . , Bm) = (w1, . . . , wm),
where
wi =

1 if Bi = Bmax
0 otherwise
Each wi is obtained as follows.
wi = EQk(Bi, Bmax).
The sizes of these circuits will be given in Sec. 3.3.
3
Bit-Slice Approach
In this section, we present a more eﬃcient circuit than the normal approach by
using a bit-slice approach for the highest-price auction. Suppose that Bmax =
(b(k−1)
max , . . . , b(0)
max)2 is the highest bidding price. Then the proposed circuit ﬁrst
determines b(k−1)
max
by looking at the most signiﬁcant bits of all the bids. It next
determines b(k−2)
max
by looking at the second most signiﬁcant bits of all the bids,
and so on.
For two m-dimensional binary vectors X=(x1, . . . , xm) and Y=(y1, . . . , ym),
deﬁne
X ∧Y = (x1 ∧y1, . . . , xm ∧ym).

Bit-Slice Auction Circuit
29
3.1
Proposed Circuit for Auction
Our idea is as follows. Let Dj be the highest price when considering the upper j
bits of the bids. That is,
D1 = (b(k−1)
max , 0 · · · , 0)2,
D2 = (b(k−1)
max , b(k−2)
max , 0 · · · , 0)2,
etc,
Dk = (b(k−1)
max , . . . , b(0)
max)2 = Bmax.
In the ﬁrst round, we ﬁnd b(k−1)
max
and then eliminate all the bidders Ai such
that Bi < D1. In the second round, we ﬁnd b(k−2)
max
and then eliminate all the
bidders Ai such that Bi < D2, and so on. At the end, the remained bidders are
the winners. For that purpose, we update W = (w1, . . . , wm) such that
wi =
1 if Bi ≥Dj
0 otherwise
for j = 1 to k.
Our circuit is obtained by implementing the following algorithm. For given m
bids, B1, . . . , Bm, deﬁne Vj as
Vj = (b(j)
1 , . . . , b(j)
m )
for j = 0, . . . , k −1. That is, Vj is the vector consisting of the j + 1th lowest bit
of each bid.
Let W = (1, . . . , 1). For j = k −1 to 0, do;
(Step 1) For W = (w1, . . . , wm), let
Sj = W ∧Vj
= (w1 ∧b(j)
1 , . . . , wm ∧b(j)
m ),
(1)
b(j)
max = (w1 ∧b(j)
1 ) ∨· · · ∨(wm ∧b(j)
m ).
(2)
(Step 2) If b(j)
max = 1, then let W = Sj.
Then the highest price is obtained as Bmax = (b(k−1)
max , . . . , b(0)
max)2. Let the ﬁnal
W be (w1, . . . , wm). Then Ai is a winner if and only if wi = 1.
We record this as the following theorem.
Theorem 1. In the above algorithm,
– Bmax is the highest bidding price.
– For the ﬁnal W = (w1, . . . , wm), Ai is a winner if and only if wi = 1.
The size of our circuit will be given in Sec. 3.3.

30
Kaoru Kurosawa and Wakaha Ogata
3.2
Example
Suppose that m = 4, k = 5 and each bid is
B1 = 20 = (1, 0, 1, 0, 0)2,
B2 = 17 = (0, 1, 1, 1, 1)2,
B3 = 18 = (1, 0, 0, 1, 0)2,
B4 = 29 = (1, 1, 1, 0, 1)2.
Then V4 = (1, 0, 1, 1), V3 = (0, 1, 0, 1) and etc. Let W = (1, 1, 1, 1). Now
1. S4 = W ∧V4 = (1, 0, 1, 1), b(4)
max = 1 and W := S4 = (1, 0, 1, 1).
2. S3 = W ∧V3 = (0, 0, 0, 1), b(3)
max = 1 and W := S3 = (0, 0, 0, 1).
3. S2 = W ∧V2 = (0, 0, 0, 1), b(2)
max = 1 and W := S2 = (0, 0, 0, 1).
4. S1 = W ∧V1 = (0, 0, 0, 0), b(1)
max = 0.
5. S0 = W ∧V0 = (0, 0, 0, 1), b(0)
max = 1 and W := S0 = (0, 0, 0, 1).
Therefore, we obtain that the highest bidding price is (b(4)
max, . . . , b(0)
max)2 =
(1, 1, 1, 0, 1)2 = 29 and A4 is the winner.
3.3
Comparison of Circuit Size
In this subsection, we compare the size of the normal circuit shown in Sec. 2 and
that of our bit-slice circuit for the highest-price auction. See Table 1.
First the size of the normal circuit is given as follows. The circuit Biggerk
requires k Bigger1 gates, 2(k −1) AND gates, k −1 OR gates and k −1 EQ1
gates. The circuit Maxk requires one Selectk gate and one Biggerk circuit. The
circuit Highest is obtained by implementing m Maxk circuits. In addition, the
circuit Winner requires m EQk gates, where an EQk gate is implemented by k
EQ1 gates and (k −1) AND gates.
Therefore, the normal circuits for the highest-price auction, Highest and
Winner, require mk Bigger1 gates, 3m(k −1) AND gates, m(k −1) OR gates,
m(2k −1) EQ1 gates and m Selectk gates in total.
Next our bit slice circuit is given by implementing Eq.(1) and Eq.(2) k times.
Therefore, it requires mk AND gates and (m −1)k OR gates.
Hence roughly speaking, the proposed circuit requires only 2mk logical gates
while the normal circuit requires 7mk logical gates and m Selectk gates.
Table 1. Comparison of circuit sizes
AND
OR
Bigger1
EQ1
Selectk
Normal circuit 3m(k −1) m(k −1)
mk
m(2k −1)
m
Bit-slice circuit
mk
(m −1)k
0
0
0

Bit-Slice Auction Circuit
31
4
MIX and Match Protocol
4.1
Overview
In this section, we brieﬂy describe the mix and match technique introduced by
Jakobsson and Juels [20]. It is a general multiparty protocol which does not use
VSS. Instead, it uses a homomorphic encryption scheme (for example, ElGamal)
and a MIX net [8, 1, 2].
This model involves n players, denoted by P1, P2, . . . , Pn and assumes that
there exists a public board. We consider an adversary who may corrupt up to t
players, where n ≥2t + 1.
The players agree in advance on a representation of the target function f as
a circuit Cf. Suppose that Cf consists of N gates, G1, . . . , GN. Let the input
of Pi be a k-bit integer Bi. The aim of the protocol is for players to compute
f(B1, . . . , Bn) without revealing any additional information. It goes as follows.
Input stage: Each Pi computes ciphertexts of the bits of Bi and broadcasts
them. She proves that each ciphertext represents 0 or 1 in zero-knowledge
by using the technique of [11].
Mix and Match stage: The players blindly evaluates each gate Gj in order.
Output stage: After evaluating the last gate GN, the players obtain oN, a ci-
phertext encrypting f(B1, . . . , Bn). They jointly decrypt this ciphertext
value to reveal the output of the function f.
This protocol meets the security requirements formalized by Canetti [7] for
secure multiparty protocols [20, page 171]. The cost of each player is O(nN)
exponentiations and the overall message complexity is also O(nN) (see [20, page
172]).
The details of the protocol are described in the following subsections.
4.2
Requirements for the Encryption Function
Let E be a public-key probabilistic encryption function. We denote by E(m)
the set of encryptions for a plaintext m and by e ∈E(m) a particular encryp-
tion of m. We say that e is a standard encryption if it is encrypted with no
randomness.
E must satisfy the following properties.
– homomorphic property
There exists a polynomial time computable operations, −1 and ⊗, as follows
for a large prime q.
1. If e ∈E(m), then e−1 ∈E(−m mod q).
2. If e1 ∈E(m1) and e2 ∈E(m2), then e1 ⊗e2 ∈E(m1 + m2 mod q).
For a positive integer a, deﬁne
a · e = e ⊗e ⊗· · · ⊗e



a
.

32
Kaoru Kurosawa and Wakaha Ogata
– random re-encryptability
Given e ∈E(m), there is a probabilistic re-encryption algorithm that outputs
e′ ∈E(m), where e′ is uniformly distributed over E(m).
– threshold decryption
For a given ciphertext e ∈E(m), any t out of n players can decrypt e along
with a zero-knowledge proof of the correctness. However, any t −1 out of n
players cannot decrypt e.
Such E(·) can be obtained by slightly modifying ElGamal encryption scheme
over a group G of order |G| = q, where q is a large prime. G can be constructed
as a subgroup of Z∗
p, where p is a prime such that q | p −1. It can also be
obtained from elliptic curves.
Let g be a generator of G, i.e. G = ⟨g⟩. The secret key x is randomly chosen
from Zq and the public key is y = gx. An encryption of m is given by
(gr, gmyr) ∈E(m),
where r ∈Zq is a random element. For ciphertexts, deﬁne −1 and ⊗as
(u, v)−1 = (u−1, v−1).
(u1, v1) ⊗(u2, v2) = (u1u2, v1v2).
Then it is easy to see that the homomorphic property is satisﬁed. A re-encryption
of (u, v) ∈E(m) is given by (u′, v′) = (gr′u, yr′v) for a random element r′ ∈Zq.
For threshold decryption, each player obtains a private share xi of x in
Shamir’s (t + 1, n)-threshold secret-sharing scheme [24]. Each gxi is published.
For details, see [12, 15]. Each player needs to broadcast O(1) messages and
compute O(n) exponentiations in threshold decryption.
4.3
MIX Protocol
A MIX protocol takes a list of ciphertexts (ξ1, . . . , ξL) and outputs a permuted
and re-encrypted list of the ciphertexts (ξ′
1, . . . , ξ′
L) without revealing the re-
lationship between (ξ1, . . . , ξL) and (ξ′
1, . . . , ξ′
L), where ξi or ξ′
i can be a single
ciphertext e, or a list of l ciphertexts, (e1, . . . , el), for some l > 1. We further
require that anybody (even an outsider) can verify the validity of (ξ′
1, . . . , ξ′
L)
(public veriﬁability).
For small L, a MIX protocol is eﬃciently implemented as shown in [1, 2, 19].
In this protocol, each player needs to compute O(nlL log L) exponentiations and
broadcast O(nlL log L) messages.
4.4
Plaintext Equality Test
Given two ciphertexts e1 ∈E(m1) and e2 ∈E(m2), this protocol checks if m1 =
m2. Let e0 = e1 ⊗e−1
2 .

Bit-Slice Auction Circuit
33
Table 2. Logical Table of AND
x1
x2
x1 ∧x2
a1 ∈E(0)
b1 ∈E(0)
c1 ∈E(0)
a2 ∈E(0)
b2 ∈E(1)
c2 ∈E(0)
a3 ∈E(1)
b3 ∈E(0)
c3 ∈E(0)
a4 ∈E(1)
b4 ∈E(1)
c4 ∈E(1)
(Step 1) For each player Pi (where i = 1, . . . , n):
Pi chooses a random element ai ∈Zq and computes zi = ai · e0. He broad-
casts zi and proves the validity of zi in zero-knowledge.
(Step 2) Let z = z1 ⊗z2 ⊗· · · ⊗zn. The players jointly decrypt z by threshold
veriﬁable decryption and obtain the plaintext m.
Then it holds that
m =
0
if m1 = m2,
random otherwise
(3)
This protocol is minimal knowledge under the DDH assumption (see [20,
page 169]). The cost of each player is O(n) exponentiations.
4.5
Mix and Match Stage
For each logical gate G(x1, x2) of a given circuit, n players jointly computes
E(G(x1, x2)) from e1 ∈E(x1) and e2 ∈E(x2) keeping x1 and x2 secret. For
simplicity, we show a mix and match stage for AND.
1. n players ﬁrst consider the standard encryption of each entry of Table 2.
2. By applying a MIX protocol to the four rows of Table 2, n players jointly
compute blinded and permuted rows of Table 2. Let the ith row be (a′
i, b′
i, c′
i)
for i = 1, . . . , 4.
3. n players next jointly ﬁnd the row i such that the plaintext of e1 is equal to
that of a′
i and the plaintext of e2 is equal to that of b′
i by using the plaintext
equality test protocol.
4. For this i, it holds that c′
i ∈E(x1 ∧x2).
5
Bit-Slice Circuit and Mix-Match Protocol
5.1
Overview
We can obtain a highest-price auction protocol by combining the proposed circuit
of Sec. 3.1 with any general multiparty protocol. Then a more eﬃcient protocol
is obtained because the bit-slice circuit is more eﬃcient than the normal circuit
as shown in Table 1.

34
Kaoru Kurosawa and Wakaha Ogata
In this section, we show a combination with the mix and match technique, as
an example. In this case, we can further improve the eﬃciency. That is, we can
remove all the OR computations of Eq.(2) by using the homomorphic property
of the encryption function as follows. Let
hj = (x1 ∧b(j)
1 ) + · · · + (xm ∧b(j)
m ).
Then it is easy to see that hj = 0 if and only if b(j)
max = 0. Therefore, n servers
have only to execute a plaintext equality test protocol for checking if hj = 0
to decide if b(j)
max = 0. Note that each server can compute summation locally by
using the homomorphic property of the encryption scheme. Hence we can replace
(m −1)k OR computations with only k plaintext equality tests.
5.2
Bidding Phase
Each bidder Ai computes a ciphertext of her bidding price Bi as
ENCi = (ei,k−1, . . . , ei,0),
where ei,j ∈E(b(j)
i ), and submits ENCi. She also proves in zero-knowledge
that b(j)
i
= 0 or 1 by using the technique of [11]. (This submission may be also
digitally signed by Ai.)
5.3
Opening Phase
Suppose that
e1 ∈E(bi) and e2 ∈E(b2), where b1 and b2 are binary. Let
Mul(e1, e2) denote a protocol which outputs
e ∈E(b1 ∧b2)
by applying a mix and match protocol for AND.
Let 
W = ( ˜w1, . . . , ˜wm), where each ˜wj ∈E(1) is the standard encryption.
(Step 1) For j = k −1 to 0, do:
(Step 1-a) For 
W = ( ˜w1, . . . , ˜wm), n servers jointly compute
Sj = (Mul( ˜w1, e1,j), . . . , Mul( ˜wm, em,j)).
(Step 1-b) Each server locally computes
hj = Mul( ˜w1, e1,j) ⊗· · · ⊗Mul( ˜wm, em,j).
(Step 1-c) n servers jointly check if hj ∈E(0) by using a plaintext equality
test. Let
b(j)
max =
0 if hj ∈E(0)
1 otherwise
(Step 1-d) If b(j)
max = 1, then let 
W = Sj.
(Step 2) For the ﬁnal 
W = ( ˜w1, . . . , ˜wm), n servers jointly decrypt each ˜wi by
threshold veriﬁable decryption and obtain the plaintext wi.
The highest price is obtained as Bmax = (b(k−1)
max , . . . , b(0)
max)2. Ai is a winner
if and only if wi = 1.

Bit-Slice Auction Circuit
35
Table 3. Comparison of the highest-price auction protocols
AND
OR
Bigger1
EQ1
Selectk
JJ [20]
3m(k −1) m(k −1)
mk
m(2k −1)
m
Proposed
mk
0
0
0
0
5.4
Comparison
If we combine the normal circuit with the mix and match technique, then JJ
auction protocol is obtained [20]. Table 3 shows the comparison between JJ
auction protocol and proposed protocol.
In our protocol, we can replace (m −1)k OR computations of Table 1 with
only k plaintext equality tests as shown in Sec. 5.1. Therefore, our protocol
requires mk AND computations and k plaintext equality tests.
We omit the cost of k plaintext equality tests in this table because it is much
smaller than the cost for each logical gate. For example, an AND computation
requires 8 plaintext equality tests and a MIX protocol of four items if we im-
plement the protocol of Sec. 4.5. Hence mk AND computations requires 8mk
plaintext equality tests and mk MIX protocols of four items. This is much larger
than the cost of k plaintext equality tests.
Now roughly speaking, our protocol requires mk logical gates while JJ auction
protocol requires 7mk logical gates and m Selectk circuit. Therefore, our protocol
is at least seven times faster than JJ auction protocol.
5.5
Discussion
We can slightly improve the opening phase by letting the initial value of 
W be

W = (e1,k−1, · · · , em,k−1).
The eﬃciency is further improved if each bid is only k′ < k bits long. (Of course,
this fact is not known in advance.) The smaller k′ is, the faster the modiﬁed
version is. For example, if Bi = (0, . . . , 0, b(0)
i ) for all i, then no mix and match
for AND is required. If Bi = (0, . . . , 0, b(1)
i , b(0)
i ) for all i, then the mix and match
for AND is executed only once, and so on.
Such speed-up is impossible in JJ auction protocol.
6
Second-Price Auction
In the second-price auction (also known as Vickrey auction), the highest bidder
wins, and the clearing price, the price that the winner has to pay, is equal to
the second highest bid. Therefore, Vickrey auctions are much closer to the real
life auction than the highest-price auctions. A cryptographically secure second-
price auction scheme should reveal only the identities of the winners (the highest
bidders) and the second-highest bid.

36
Kaoru Kurosawa and Wakaha Ogata
In this section, we show that a second-price auction protocol is easily obtained
from our bit-slice circuit for the highest-price auction.
6.1
Normal Approach Circuit
If we use the normal approach, we can obtain a second-price auction circuit by
keeping track of the two highest bids found so far, Bﬁrst and Bsecond, as follows.
Let Bﬁrst = Bsecond = 0. For i = 1, . . . , m, do
X := Selectk(Biggerk(Bsecond, Bi), Bsecond, Bi).
Bsecond := Selectk(Biggerk(Bﬁrst, X), X, Bﬁrst).
Bﬁrst := Selectk(Biggerk(Bﬁrst, X), Bﬁrst, X).
It is easy to see that the ﬁnal Bﬁrst is the highest bidding price and Bsecond is
the second-highest price. The identities of the winners are obtained similarly to
Sec. 2.3.
6.2
Bit-Slice-Type Second-Price Auction
We deﬁne two types of highest-price auction schemes, a winner-only scheme and
a price-only scheme. A winner-only scheme reveals only the identities of the
winners, but not the highest price. A price-only scheme reveals only the highest
bid, but not the identities of the winners.
Now suppose that there is a winner-only scheme Q1 and a price-only
scheme Q2. Then we can obtain a second-price auction scheme as follows:
Step 1. Run Q1.
Step 2. Delete the winners of Q1 from the set of bidders.
Step 3. Run Q2 for the rest of the bidders.
Indeed, the above scheme reveals only the identities of the winners of Q1,
and the highest bidding price of Q2 which is the second highest price among the
bidders.
Such protocols Q1 and Q2 are easily obtained from our bit-slice circuit for
the highest-price auction as follows.
– Apply any multiparty protocol to the circuit of Sec. 3.1 and decrypt
only b(k−1)
max , . . . , b(0)
max (but not W) in the output stage. Then we obtain
a price-only scheme.
– In the circuit of Sec. 3.1, replace Step 2 with
W = Selectm(b(j)
max, Sj, W).
Then apply any multiparty protocol to the above circuit and decrypt only W
(but not b(k−1)
max , . . . , b(0)
max) in the output stage. We now obtain a winner-only
scheme.

Bit-Slice Auction Circuit
37
Table 4. Comparison of the second-price auction protocols
AND
OR
Bigger1
EQ1
Selectk Selectm
Normal 5m(k −1) 2m(k −1)
2mk
m(2k −1)
3m
0
Bit slice (2m −1)k (m −1)k
0
0
0
k
6.3
Comparison
Suppose that we use the mix and match technique as a general multiparty proto-
col. In this case, the price-only scheme is obtained by deleting Step 2 of Sec. 5.3.
Then roughly speaking, our second-price auction protocol requires 3mk log-
ical gates and k Selectm gates. On the other hand, the second-price auction
protocol obtained by combining the normal circuit and the mix and match tech-
nique requires 11mk logical gates and 3m Selectk gates.
We show a comparison in Table 4.
Acknowledgement
The authors thank Martin Hirt for providing many useful comments.
References
[1] M. Abe, “Mix-Networks on permutation networks,” Proc. of Asiacrypt ’99,
LNCS Vol. 1716, pp. 258–273 (1999).
31, 32
[2] M. Abe and F. Hoshino, “Remarks on Mix-Network Based on Permutation Net-
works,” Proc. of PKC 2001, pp. 317–324 (2001).
31, 32
[3] M. Abe and K. Suzuki, “M1-st Price Auction Using Homomorphic Encryption,”
Proc. of PKC2002, pp. 115–124 (2002).
26
[4] O. Baudron and J. Stern, “Non-Interactive Private Auctions,” Proc. of Financial
Cryptography 2001 (2001).
25
[5] M. Ben-Or, S. Goldwasser, and A. Wigderson. “Completeness theorems for non-
cryptographic fault-tolerant distributed computation,” Proceedings of the twen-
tieth annual ACM Symp. Theory of Computing, STOC, pp. 1–10, May 2–4
(1988).
25
[6] C. Cachin, “Eﬃcient private bidding and auctions with an oblivious third party,”
ACM CSS ’99, pp. 120–127, ACM (1999).
26
[7] R. Canetti, “Security and composition of multiparty cryptographic protocols,”
Journal of Cryptology, Vol. 13, No. 1, pp. 143–202 (2000).
31
[8] D.
Chaum,
“Untraceable
electronic
mail,
return
addresses,
and
digital
pseudonyms,” Communications of the ACM, Vol. 24, pp. 84–88 (1981).
31
[9] D. Chaum, C. Cr´epeau, and I. Damg˚ard. “Multiparty unconditionally secure
protocols,” Proc. of the twentieth annual ACM Symp. Theory of Computing,
STOC, pp. 11–19, May 2–4 (1988).
25
[10] R. Cramer, I. Damg˚ard, and J. B. Nielsen, “Multiparty Computation from
Threshold Homomorphic Encryption,” Proc. of Eurocrypt’01, LNCS Vol. 2045,
pp. 280–300 (2001).
25

38
Kaoru Kurosawa and Wakaha Ogata
[11] R. Cramer, I. Damg˚ard, and B. Schoenmakers, “Proofs of partial knowledge
and simpliﬁed design of witness hiding protocols,” Proc. of CRYPTO ’94. LNCS
Vol. 839, pp. 174–187 (1994).
31, 34
[12] Y. Desmedt and Y. Frankel, “Threshold cryptosystems,” In Proc. of Crypto ’89,
pp. 307–315.
32
[13] G.Di Crescenzo, “Private selective payment protocols,” In Proc. of Financial
Cryptography ’00, LNCS Vol. 1962, pp. 72–89 (2000).
26
[14] M. Franklin and M. Reiter, “The Design and Implementation of a Secure Auction
Service,” IEEE Trans. on Software Engineering, Vol. 22, No. 5 (1996).
26
[15] R. Gennaro, S. Jarecki, H. Krawczyk, and T. Rabin, “Robust threshold DSS
signatures,” In Proc of Eurocrypt ’96, LNCS Vol. 1070, pp. 354–371 (1996). 32
[16] O. Goldreich, S. Micali, and A. Wigderson. “How to play any mental game,” In
Proceedings of the nineteenth annual ACM Symp. Theory of Computing, STOC,
pp. 218–229, May 25–27 (1987).
25
[17] M. Hirt, U. Maurer, and B. Przydatek, “Eﬃcient secure multiparty computa-
tion,” Proc. of Asiacrypt’2000, LNCS Vol. 1976, pp. 143–161 (2000).
25
[18] M. Harkavy, J. D. Tygar, and H. Kikuchi, “Electronic auction with private bids,”
In Third USENIX Workshop on Electronic Commerce Proceedings, pp. 61–74
(1998).
26
[19] M. Jakobsson and A. Juels, “Millimix: Mixing in small batches,” DIMACS Tech-
nical report 99-33 (June 1999).
32
[20] M. Jakobsson and A. Juels, “Mix and Match: Secure Function Evaluation via
Ciphertexts,” In Proc. of Asiacrypt 2000, LNCS Vol. 1976, pp. 162–177 (2000).
25, 26, 31, 33, 35
[21] A. Juels and M. Szydlo, “An Two-Server Auction Protocol,” In Proc. of Financial
Cryptography 2002, (2002).
25
[22] M. Naor, B. Pinkas, and R. Sumner, “Privacy preserving auctions and mech-
anism design,” In 1st ACM Conference on Electronic Commerce, pp. 129–140,
ACM (1999).
25, 26
[23] K. Sako, “An auction protocol which hides bids of losers,” Proc. of PKC’2000,
LNCS Vol. 1751, pp. 422–432 (2000).
26
[24] A. Shamir, “How to Share a Secret,” Communications of the ACM, Vol. 22,
pp. 612-613 (1979).
32
[25] A. Yao, “Protocols for secure computations (extended abstract),” Proc. of
FOCS’82, pp. 160–164, IEEE Computer Society (1982).
25

Conﬁdentiality Policies and Their Enforcement
for Controlled Query Evaluation
Joachim Biskup1 and Piero Bonatti2
1 Fachbereich Informatik, Universit¨at Dortmund
D-44221 Dortmund, Germany
biskup@ls6.informatik.uni-dortmund.de
2 Dipartimento di Tecnologie dell’Informazione, Universit`a di Milano
I-26013 Crema, Italy
bonatti@dti.unimi.it
Abstract. An important goal of security in information systems is con-
ﬁdentiality. A conﬁdentiality policy speciﬁes which users should be for-
bidden to acquire what kind of information, and a controlled query eval-
uation should enforce such a policy even if users are able to reason about
a priori knowledge and the answers to previous queries. We put the
following aspects into a unifying and comprehensive framework: formal
models of conﬁdentiality policies based on potential secrets or secrecies,
user awareness of the policy instance, and enforcement methods apply-
ing either lying or refusal, or a combination of lying and refusal. Two
new evaluation methods are introduced. Diﬀerent approaches are sys-
tematically compared and evaluated.
Keywords: Inference control; Controlled query evaluation; Conﬁdential-
ity; Policy; Potential secret; Secrecy; Refusal; Lying; Combined refusal
and lying.
1
Introduction
An important goal of security in information systems is conﬁdentiality, i.e., the
ability to hide speciﬁc information from certain users according to some conﬁ-
dentiality policy. Roughly speaking, such a conﬁdentiality policy speciﬁes which
users should be forbidden to acquire what kind of information. Typically, a con-
ﬁdentiality policy is expressed by the owner or a designated administrator of the
information to be hidden. Using a logic-oriented model of information systems,
including the relational model and Datalog, a controlled query evaluation has to
enforce such a policy, even if users are able to infer more information than what
is explicitly returned as an answer to their queries.
In this paper, we deal with the theoretical foundations of controlled query
evaluation. Our work is based on a simple but powerful logic-oriented data model
which considers an instance of an information system as a structure (interpre-
tation) in the sense of logic, a query as a sentence, and the ordinary answer as
the truth value of the query w.r.t. the instance.
D. Gollmann et al. (Eds.): ESORICS 2002, LNCS 2502, pp. 39–55, 2002.
c
⃝Springer-Verlag Berlin Heidelberg 2002

40
Joachim Biskup and Piero Bonatti
This theoretical model is nonetheless relevant for practical and static ap-
proaches to information ﬂow control and inference control [6, 5], including discre-
tionary and mandatory access control, and data customization such as statistical
perturbation and polyinstantiation.
Previous work on controlled query evaluation [1, 2, 3, 4, 9] has identiﬁed
three important aspects:
1. Formal models of conﬁdentiality policies: potential secrets versus secrecies.
Under the model of secrecies, a conﬁdentiality policy requires that, given
a set of sentences that constitute the policy instance, a user should not
be able to infer the truth value of those sentences in the current instance
of the information system. Whereas under the model of potential secrets,
a conﬁdentiality policy requires that, for a given set of sentences, if any of
these sentences is true in the current instance of the information system then
the user should not be able to infer that fact. However, users are allowed to
believe the opposite truth value. Thus under the former model, query answers
should not suggest any truth value at all for the given set of sentences, while
under the latter model, a predetermined alternative of the truth values can
be suggested.
Clearly, the goal of conﬁdentiality has to be complemented by availability. In
this paper, like in previous work, we deal with availability only by observing
the following informal heuristic. A controlled query evaluation should be as
cooperative to the user as possible, i.e., it should return a distorted answer
(i.e., a refusal or a lie) only if necessary for achieving conﬁdentiality.
2. User awareness of the policy instance: unknown versus known.
A user who knows the policy instance might use such knowledge to infer
hidden information, say, by the following kind of reasoning: “I received a dis-
torted answer. The system did so only because otherwise the correct answer
together with previously observed data would logically imply a speciﬁc sen-
tence Ψ contained in the policy instance. So, observing the distortion, I con-
clude that Ψ is valid indeed.” Even if the actual policy instance is unknown
to him, the user might base a similar reasoning solely on his awareness of
the model of the policy. Surely, any enforcement method should make such
arguments impossible.
3. Enforcement method: lying versus refusal, and combined lying and refusal.
(Uniform) refusal returns a mum, i.e., it refuses to return the truth value of
the query, if the correct answer lets the user infer (either directly or indi-
rectly) one of the sentences of the policy instance. Under similar conditions,
(uniform) lying returns a lie, i.e., the negation of the correct answer. Com-
bined methods are allowed to protect the sentences of the policy instance by
both kinds of answer distortion.
Sicherman/de Jonge/van de Riet [9] introduce the topic of controlled query
evaluation and discuss refusal for unknown and known secrecies. Bonatti/Kraus/
Subrahmanian [4] pioneer lying for known potential secrets. Biskup [1] deﬁnes
a common framework for studying and comparing enforcement methods dealing

Conﬁdentiality Policies and Their Enforcement
41
Table 1. Aspects for controlled query evaluation and contributions of previous
work
conﬁdentiality policy
potential secrets
secrecies
en
forcemen
t
lying
[4]
[2, section 4]
“not applicable” [1, section 5]
refusal
[2, section 3]
[9]
[1, Section 4.2]
[9]
[1, section 4.1]
combined
[3, section 3]
[3, section 4]
unknown
known
unknown
user awareness
with lying and refusal for unknown and known secrecies. A main conclusion is
that “for unknown secrecies refusal is better than lying”. Biskup/Bonatti [2]
adapt that framework in order to treat lying and refusal for known potential
secrets. They show that in this context lying and refusal are incomparable in
general, but functionally equivalent under some natural restriction (i.e., the pol-
icy instance should be closed under disjunction). Finally, Biskup/Bonatti [3]
suggest a combination of refusal and lying for known policies, including both
potential secrets and secrecies. The combined methods avoid the drawbacks of
the uniform methods (the mandatory protection of disjunctions for lying, and,
for refusal, the independence from the actual instance of the information system).
Table 1 gives a structured overview of all these works, highlighting the com-
binations of the three major aspects that have not yet been investigated. In
this paper, we try to ﬁll in the table’s gaps, and provide a unifying view of
all these approaches, including a general analysis of their mutual relationships.
More precisely, the structure and the contributions of the paper are as follows.
– In Section 2 we introduce a uniform reformulation of the existing approaches,
including a uniﬁed notion of conﬁdentiality, parametric w.r.t the three clas-
siﬁcation dimensions.
Furthermore, this section points out the mutual relationships between the
existing approaches for known policies and their counterparts for unknown
policies, and the relationships between the methods based on secrecies and
their counterparts for potential secrets. In the latter case, we observe that
each method based on a policy instance presented as a set of secrecies
secr := {{Ψ1, ¬Ψ1}, . . . , {Ψk, ¬Ψk}}
(1)
is equivalent to the corresponding method based on potential secrets, applied
to the policy instance
pot sec(secr) := {Ψ1, ¬Ψ1, . . . , Ψk, ¬Ψk} .
(2)
– In Section 3, we ﬁll in the two table slots corresponding to lies and refusal for
unknown potential secrets. The new methods are derived from their coun-

42
Joachim Biskup and Piero Bonatti
terparts for unknown secrecies by analogy with the reduction of secrecies to
potential secrets illustrated in (2).
Then, in the same section, we try to derive a combined method for unknown
potential secrets from the corresponding method for known potential secrets,
by analogy with the relationships observed in Section 2. Surprisingly, all the
natural attempts along this direction fail to preserve conﬁdentiality.
– In Section 4 we prove some general results that explain the relationships
empirically observed in Section 2. As a byproduct, we identify some gen-
eral properties of the controlled query evaluation methods under which the
reduction of secrecies to potential secrets based on (2) preserves conﬁdential-
ity. These results have a potential practical impact on the design of uniform
access control mechanisms that support simultaneously both of the policy
models (and combinations thereof).
The paper is closed by a section with a discussion of the results and some con-
clusions. Proofs will be omitted due to space limitations, with the exception of
the proof of Theorem 4.
2
A Uniﬁed Formal Model for Conﬁdentiality Policies
2.1
Ordinary Query Evaluation
An information system maintains two kinds of data: A schema DS captures the
universe of discourse for the intended application and is formally deﬁned as the
set of all allowed instances. An instance db is a structure which interprets the
symbols of some logic, i.e. of the universe of discourse (see e.g. [8, 7]). We only
consider the most elementary kind of query, namely a sentence in the language of
the logic. Given a structure db (stored as an instance) and a sentence Φ (issued
as a query), Φ is either true (valid) or false in db, or in other words, the structure
is either a model of the sentence or not. When a user issues a query Φ against the
schema DS, the (ordinary) query evaluation eval(Φ) determines the pertinent
case for the current instance db. Thus we formally deﬁne
eval(Φ) : DS →{true, false} with eval(Φ)(db) := db model of Φ ,
(3)
where the boolean operator model of is assumed to be appropriately speciﬁed
for the logic under consideration.1 We also use an equivalent formalization where
either the queried sentence or its negation is returned:
eval ∗(Φ) : DS →{Φ, ¬Φ} with
eval ∗(Φ)(db) := if db model of Φ then Φ else ¬Φ .
(4)
1 This abstract formulation makes our approach compatible with a variety of database
models. To get a more concrete understanding of our framework, the reader may
instantiate model of with standard ﬁrst-order satisfaction.

Conﬁdentiality Policies and Their Enforcement
43
The symbols of the logic comprise both the negation symbol (as implicitely
assumed above) and disjunction. We assume that both connectives have their
classical semantics . Our deﬁnition trivially implies that for all instances db and
for all queries Φ we have db model of eval ∗(Φ)(db).
We also deﬁne the semantic relationship |= for logical implication in a stan-
dard way: Φ |= Ψ iﬀfor every structure db such that db model of Φ we also have
db model of Ψ. The complementary relationship is denoted with ̸|=.
2.2
Controlled Query Evaluation
Controlled query evaluation consists of two steps. First, the correct answer is
judged by some censor and then, depending on the output of the censor, some
modiﬁcator is applied. In order to assist the censor, the system maintains a user
log, denoted by log, which represents the explicit part of the user’s assumed
knowledge. Formally, log is declared to be a set of sentences. The log is meant to
contain all the sentences that the user is assumed to hold true in the instance,
in particular publicly known semantic constraints. Additionally, the log records
the sentences returned as answers to previous queries.
Formally we will describe an approach to controlled query evaluation by
a family of (possibly) partial functions control eval( Q, log0), each of which has
two parameters: a (possibly inﬁnite) query sequence Q = ⟨Φ1, Φ2, . . . , Φi, . . .⟩,
and an initial user log log0. The inputs to any such function are “admissible”
pairs (db, policy) where db is an instance of the information system, and policy
is an instance of a suitably formalized conﬁdentiality policy. The admissibil-
ity of an argument pair (db, policy) is determined by some formal precondition
associated with the function. Throughout the paper we suppose that for each
function, the policies in its admissible pairs all belong to exactly one of the policy
models. The function returns an answer sequence to the user, and updates the
user log as a side eﬀect. For any speciﬁc function, we indicate the underlying
choices w.r.t. the three aspects—model of policy, user awareness, enforcement
method—by a superscript p, a, e with p ∈{sec, ps}, a ∈{unknown, known}, and
e ∈{L(ying), R(efusal), C(ombined)}. In symbols,
control eval p,a,e(Q, log0)(db, policy) =
⟨(ans1, log1), (ans2, log2), . . . , (ansi, logi), . . .⟩,
where the side eﬀect on the user log is described by
logi := if ansi = mum then logi−1 else logi−1 ∪{ansi} ,
and a censorp,a,e is formally described by a truth function (or two truth
functions for combined methods) with arguments of the form (Ψ, log, db, policy).
The censor returns true iﬀthe modiﬁcation e is required.
2.3
Conﬁdentiality Requirements
The syntactical appearance of an instance of a conﬁdentiality policy depends on
the model: Either a policy instance is given by a ﬁnite set secr of complementary

44
Joachim Biskup and Piero Bonatti
pairs of sentences, secr = {{Ψ1, ¬Ψ1}, . . . , {Ψk, ¬Ψk}}, where each pair is called
a secrecy. Or a policy instance is given by a ﬁnite set pot sec of sentences,
pot sec = {Ψ1, . . . , Ψk}, where each sentence is called a potential secret.
The original motivations for the two models of conﬁdentiality policies are
quite diﬀerent (see [1, 2] for a detailed exposition).
– A secrecy {Ψ, ¬Ψ} speciﬁes the following: A user, seeing only the apriori
knowledge log0 and the returned answer sequence, should not be able to
distinguish whether Ψ or ¬Ψ is true in the actual instance of the information
system, or speaking otherwise, both cases should be possible for him. More
formally, the controlled query evaluation must be “nowhere injective with
respect to the secrecy {Ψ, ¬Ψ}”.
– A potential secret Ψ speciﬁes the following: Such a user should not be able
to exclude that ¬Ψ is true in the actual instance of the information system,
or speaking otherwise, this case should be possible for him. More formally,
the controlled query evaluation must have a “surjective restriction on the
false potential secret, i.e. on ¬Ψ”.
The intended semantics is formalized as follows.
Deﬁnition 1. Let control eval p,a,e(Q, log0) describe a speciﬁc controlled query
evaluation with precond as associated precondition for “admissible” arguments,
and policy1 be a policy instance.
1. control eval p,a,e(Q, log0) is deﬁned to preserve conﬁdentiality2 with respect
to policy1 iﬀ
for all ﬁnite preﬁxes Q′ of Q,
for all instances db1 of the information system such that (db1, policy1) sat-
isﬁes precond,
and for all Θ ∈policy1,
there exists db2 and policy2 such that (db2, policy2) also satisﬁes precond
and such that the following properties hold:
(a) [same answers]
control eval p,a,e(Q′, log0)(db1, policy1)
=
control eval p,a,e(Q′, log0)(db2,
policy2) ;
(b) [diﬀerent secrets/false potential secrets]
if p = sec , i.e., Θ={Ψ, ¬Ψ} is a secrecy: {eval ∗(Ψ)(db1), eval ∗(Ψ)(db2)}=
{Ψ, ¬Ψ} ,
if p = ps , i.e., Θ = Ψ is a potential secret: eval ∗(Ψ)(db2) = ¬Ψ ;
(c) [awareness] if a = known : policy1 = policy2 .
2. More generally, control eval(Q, log0)p,a,e is deﬁned to preserve conﬁdential-
ity iﬀit preserves conﬁdentiality with respect to all “admissible” policy in-
stances.
2 Or to be secure, as a shorthand used in previous work

Conﬁdentiality Policies and Their Enforcement
45
The above deﬁnition generalizes and encompasses all the notions of con-
ﬁdentiality introduced in previous works on controlled query evaluation. Some
assumptions are implicit in the deﬁnition: (i) the user may know the algorithm of
the controlled evaluation function, (ii) the user is rational, i.e., he derives nothing
besides what is implied by his knowledge and the behavior of the database.
2.4
Existing Methods for Known Policies
Table 2 sketches the censors of the enforcement methods for known policies
reported previously (i.e., columns 2 and 3 of Table 1) together with the pertinent
preconditions in the sense of the new Deﬁnition 1. The original deﬁnitions are
reformulated using the function pot sec(.) (cf. (2)).
Proposition 1 (conﬁdentiality). The (suitably formalized versions of) previ-
ously reported enforcement methods for known policies preserve conﬁdentiality
in the sense of Deﬁnition 1.
A careful comparison of the diﬀerent methods summarized in Table 2 shows
an interesting relationship between the treatment of potential secrets and secre-
cies. Each method for secrecies is equivalent to the corresponding method for
potential secrets applied to the policy pot sec(secr) (where secr is the original
policy). This is not immediately obvious for refusal, but it can be proved easily.
Proposition 2 (correspondence). Each (suitably formalized versions of a)
previously reported enforcement method for known secrecies using a policy in-
stance secr is equivalent to the corresponding method for known potential secrets
using the policy instance pot sec(secr).
2.5
Existing Methods for Unknown Policies
For unknown policies, only methods based on secrecies have been introduced
so far. The existing methods for unknown secrecies (i.e., column 4 of Table 1)
are summarized in the rightmost column of Table 3. Extending Proposition 1,
we note that they also satisfy the generalized notion of conﬁdentiality (Deﬁni-
tion 1). The left column of Table 3 provides equivalent reformulations of the
corresponding methods for known secrecies.
A crucial diﬀerence between a method for known policies and the correspond-
ing method for unknown policies is the following: in the former case the pertinent
censor takes care about all sentences in pot sec or pot sec(secr), respectively,
whereas in the latter case only sentences that are true in the current instance of
the information system are considered. This property makes the censors instance
dependent, whereas the censors for known policies depend only on the query, the
log and the policy.

46
Joachim Biskup and Piero Bonatti
Table 2. Correspondences between enforcement methods for known policies: for
each method we indicate the pertinent censor as given in previous work and the
precondition as requested by Deﬁnition 1. For combined methods, we make the
simplifying assumption that policy ̸= ∅in order to avoid explicit consistency
checks for the log
potential secrets
secrecies
pot sec = {Ψ1, . . . , Ψk}
secr = {{Ψ1, ¬Ψ1}, . . . , {Ψk, ¬Ψk}}
pot sec(secr) = {Ψ1, ¬Ψ1, . . . , Ψk, ¬Ψk}
lying [4], [2, section 4]:
censorps,known,L:
log ∪{eval ∗(Φ)(db)} |= pot sec disj ,
where
pot sec disj := 
Ψ∈pot sec Ψ ;
preconditionps,known,L:
log0 ̸|= pot sec disj
lying [1, section 5]:
censorsec,known,L: “not applicable”
preconditionsec,known,L:
log0 ̸|= 
Ψ∈pot sec(secr) Ψ
refusal [2, section 3]:
censorps,known,R:
(exists Ψ ∈pot sec)
[ log ∪{eval ∗(Φ)(db)} |= Ψ or
log ∪{¬eval ∗(Φ)(db)} |= Ψ ] ;
preconditionps,known,R:
db model of log0 and
(for all Ψ ∈pot sec) [ log0 ̸|= Ψ ]
refusal [9], [1, section 4.2]:
censorsec,known,R:
(exists Ψ ∈pot sec(secr) )
[ db model of Ψ and
[ log ∪{eval ∗(Φ)(db)} |= Ψ or
log ∪{¬eval ∗(Φ)(db)} |= Ψ or
log ∪{¬eval ∗(Φ)(db)} |= ¬Ψ ] ] ;
preconditionsec,known,R:
db model of log0 and
(for all Ψ ∈pot sec(secr) )[ log0 ̸|= Ψ ]
combined [3, section 3]:
refusalcensorps,known,C:
(exists Ψ1 ∈pot sec)
[ log ∪{eval ∗(Φ)(db)} |= Ψ1 ]
and (exists Ψ2 ∈pot sec)
[ log ∪{¬eval ∗(Φ)(db)} |= Ψ2 ] ;
lyingcensorps,known,C:
(exists Ψ1 ∈pot sec)
[ log ∪{eval ∗(Φ)(db)} |= Ψ1 ]
and (for all Ψ2 ∈pot sec)
[ log ∪{¬eval ∗(Φ)(db)} ̸|= Ψ2 ] ;
preconditionps,known,C:
(for all Ψ ∈pot sec) [ log0 ̸|= Ψ ]
combined [3, section 4]:
refusalcensorsec,known,C:
(exists Ψ1 ∈pot sec(secr) )
[ log ∪{eval ∗(Φ)(db)} |= Ψ1 ]
and (exists Ψ2 ∈pot sec(secr) )
[ log ∪{¬eval ∗(Φ)(db)} |= Ψ2 ] ;
lyingcensorsec,known,C:
(exists Ψ1 ∈pot sec(secr) )
[ log ∪{eval ∗(Φ)(db)} |= Ψ1 ]
and (for all Ψ2 ∈pot sec(secr) )
[ log ∪{¬eval ∗(Φ)(db)} ̸|= Ψ2 ] ;
preconditionsec,known,C:
(for all Ψ ∈pot sec(secr) ) [ log0 ̸|= Ψ ]

Conﬁdentiality Policies and Their Enforcement
47
Table 3. Correspondences between enforcement methods depending on the user
awareness of the policy instance: For each method we indicate the suitably re-
formulated pertinent censor and the precondition as requested by Deﬁnition 1
known
unknown
lying under secrecies [1, section 5]:
“not applicable” censorsec,known,L:
log ∪{eval ∗(Φ)(db)} |= 
Ψ∈pot sec(secr) Ψ
preconditionsec,known,L:
log0 ̸|= 
Ψ∈pot sec(secr) Ψ
lying under secrecies [1, section 5]:
censorsec,unknown,L:
log ∪{eval ∗(Φ)(db)} |=

Ψ∈pot sec(secr) and db model of Ψ Ψ
preconditionsec,unknown,L:
log0 ̸|= 
Ψ∈pot sec(secr) and db model of Ψ Ψ
refusal under secrecies [9], [1, sect. 4.2]:
censorsec,known,R:
(exists Ψ ∈pot sec(secr) )
[ log ∪{eval ∗(Φ)(db)} |= Ψ or
log ∪{¬eval ∗(Φ)(db)} |= Ψ ] ;
preconditionsec,known,R:
db model of log0 and
(for all Ψ ∈pot sec(secr) )[ log0 ̸|= Ψ ]
refusal under secrecies [9], [1, sect. 4.1]:
censorsec,unknown,R:
(exists Ψ ∈pot sec(secr) )
[ db model of Ψ and
log ∪{eval ∗(Φ)(db)} |= Ψ] ;
preconditionsec,unknown,R:
db model of log0 and
(for all Ψ ∈pot sec(secr) )
[ if db model of Ψ
then log0 ̸|= Ψ ]
3
Filling in the Gaps
In this section, we shall try to ﬁll in the gaps of Table 1 by analogy with the rela-
tionships between the existing methods observed in Section 2.4 and Section 2.5.
In the next subsection, we shall derive secure controlled query evaluations for
unknown potential secrets from their counterparts for unknown secrecies (Ta-
ble 3), by analogy with the relationships observed in Table 2. In the second
subsection, we shall try to adapt to unknown policies the combined methods
for known policies, by analogy with the observations on Table 3. Unfortunately,
it will turn out that all the obvious attempts in this direction fail to preserve
conﬁdentiality.
3.1
Lies and Refusal for Unknown Potential Secrets
In the case of known potential secrets, the controlled query evaluation based on
lies uses a censor that protects the disjunction of all potential secrets. In the case
of unknown potential secrets, we can take a less restrictive disjunction. Now it is
suﬃcient to take care of only those potential secrets that are valid in the current
instance of the information system. Note that in this case the censor actually
depends on the current instance, in contrast to the case for known policies.

48
Joachim Biskup and Piero Bonatti
Formally
we
deﬁne
the
uniform
lying
method
as
follows.
The
censorps,unknown,L is given by
log ∪{eval ∗(Φ)(db)} |= true pot sec disj
(5)
with true pot sec disj := 
Ψ∈pot sec and db model of Ψ Ψ , and the precondition
preconditionps,unknown,L is given by
log0 ̸|= true pot sec disj .
(6)
Theorem 1 (conﬁdentiality). Uniform lying for unknown potential secrets
preserves conﬁdentiality in the sense of Deﬁnition 1.
Next, we introduce a uniform refusal method for unknown potential secrets
by following the same ideas adopted above. Formally, the method is deﬁned as
follows. The censorps,unknown,R is given by
(exists Ψ ∈pot sec)[ db model of Ψ and log ∪{eval ∗(Φ)(db)} |= Ψ ] ,
(7)
and the preconditionps,unknown,R by
db model of log0 and (for all Ψ ∈pot sec)[ if db model of Ψ then log0 ̸|= Ψ ] .
(8)
Theorem 2 (conﬁdentiality). Uniform refusal for unknown potential secrets
preserves conﬁdentiality in the sense of Deﬁnition 1.
3.2
Combined Methods for Unknown Policies
We try to adapt the combined method for known potential secrets to unknown
such policies, by restricting the censor tests to true secrets only. Due to this
restriction, we have to explicitly ensure log consistency. The returned answers
are:
ansi :=
if [ logi−1 ∪{eval ∗(Φi)(db)} is inconsistent or
(exists Ψ1 ∈pot sec)[ db model of Ψ1 and logi−1 ∪{eval ∗(Φi)(db)} |= Ψ1] ]
then
if [ logi−1 ∪{¬eval ∗(Φi)(db)} is inconsistent or
(exists Ψ2 ∈pot sec)[ db model of Ψ2 and logi−1∪{¬eval ∗(Φi)(db)}|=Ψ2] ]
then mum
else ¬eval ∗(Φi)(db)
else eval ∗(Φi)(db)
Thus the censor for refusals, refusalcensorps,unknown,C, looks like
[ log ∪{eval ∗(Φ)(db)} is inconsistent or
(exists Ψ1 ∈pot sec)[ db model of Ψ1 and log ∪{eval ∗(Φ)(db)} |= Ψ1 ] ]
and
[ log ∪{¬eval ∗(Φ)(db)} is inconsistent or
(exists Ψ2 ∈pot sec)[ db model of Ψ2 and log ∪{¬eval ∗(Φ)(db)} |= Ψ2 ] ] ;
(9)

Conﬁdentiality Policies and Their Enforcement
49
and the censor for lies, lyingcensorps,unknown,C, looks like
[ log ∪{eval ∗(Φ)(db)} is inconsistent or
(exists Ψ1 ∈pot sec)[ db model of Ψ1 and log ∪{eval ∗(Φ)(db)} |= Ψ1 ] ]
and
[log ∪{¬eval ∗(Φ)(db)} is consistent and
(for all Ψ2 ∈pot sec)[ if db model of Ψ2 then log ∪{¬eval ∗(Φ)(db)} ̸|= Ψ2 ] ] .
(10)
Furthermore, the preconditionps,unknown,C(., log0)(db, pot sec) is speciﬁed by
log0 is consistent and
(for all Ψ ∈pot sec) [ if db model of Ψ then log0 ̸|= Ψ ] .
(11)
Unfortunately, the controlled evaluation method for combined lying and re-
fusal for unknown potential secrets do not preserve conﬁdentiality in the sense
of Deﬁnition 1.
Example 1. Let Q = ⟨p ∨q⟩, log0 = ∅, pot sec = {p ∨q, ¬q} and db = {p}. Then
ans1 = mum. Now consider any pair (db′, pot sec′) satisfying the precondition (11)
and returning the same answers as (db, pot sec). By deﬁnition, since ans1 = mum,
p∨q entails a potential secret Ψ such that db′ model of Ψ. Since the vocabulary is
{p, q} and three out of the four possible interpretations satisfy p∨q, there are two
possibilities: either Ψ ≡p ∨q or Ψ is a tautology. The latter case is not possible
because it violates the precondition. It follows that for all the allowed pairs
(db′, pot sec′) that return ans1 = mum, db′ model of p ∨q, and hence conditions
(a) and (b) of Deﬁnition 1 cannot be simultaneously satisﬁed. This proves that
the above combined method does not preserve conﬁdentiality.
There are at least two obvious alternative deﬁnitions for ansi, where only
Ψ1 or Ψ2 (respectively) must be satisﬁed by db. It can be shown that these two
alternatives do not preserve conﬁdentiality, either.
In the light of these negative results, we can assess diﬀerent methods for
unknown policies w.r.t. the given state of the art. First, for unknown policies—
be they secrecies (see [1]) or potential secrets (see Section 3.1)—the censor for
uniform refusal is weaker than the censor for uniform lying. Consequently, we
can state a “longest honeymoon lemma” in the sense of [3]. Informally speaking,
the lemma says that the method based on uniform refusal does never modify
a correct answer before the corresponding method based on uniform lies. Of
course, this result holds for input pairs that are admissible for both methods. In
general, the two preconditions are incomparable.
Second, the censors for uniform refusal should not be further weakened: If we
allowed log |= Ψ, for some Ψ occurring in the policy such that db model of Ψ,
then we could not prevent users from getting a view (given by log) that entails
an actual secret. Surely we do not want such a situation. Hence uniform re-
fusal cannot be improved unless some refusals are replaced by lies. However, as
shown above, the current attempts at designing combined methods for unknown
potential secrets fail to preserve conﬁdentiality.

50
Joachim Biskup and Piero Bonatti
We conclude that in the framework of unknown policies, the methods based
on uniform refusals are currently the best choice from the point of view of the
longest honeymoon lemma, and that there are diﬃculties in improving them.
4
A Systematic View of Controlled Query Evaluation
In this section we develop a more general analysis of the relationships between
diﬀerent policy models and diﬀerent awareness assumptions. The results of this
section justify the recurrent patterns observed in the previous sections.
4.1
Known Policies vs. Unknown Policies
We start with a simple and intuitive result, stating that unknown policies are
easier to handle than known policies.
Proposition 3. If control eval preserves conﬁdentiality w.r.t. known policies,
then control eval preserves conﬁdentiality w.r.t. unknown policies. The converse
is not true.
The converse of the ﬁrst part of Proposition 3 holds if the censor of
control eval is instance independent, that is, for all db and db′,
censor(Ψ, log, db, policy) = censor(Ψ, log, db′, policy) .
Theorem 3. If control eval has an instance independent censor and preserves
conﬁdentiality w.r.t. unknown policies, then control eval preserves conﬁdential-
ity w.r.t. known policies.
In other words, if the censor is instance independent, then control eval pre-
serves conﬁdentiality w.r.t. known policies iﬀit preserves conﬁdentiality w.r.t.
unknown policies. Intuitively, this means that in order to take really advantage
of the extra degrees of freedom permitted by unknown policies (as suggested by
Proposition 3), the censor must be instance dependent.
On one hand, this result justiﬁes the diﬀerence between the existing censors
for known policies and those for unknown policies. On the other hand, this re-
sult tells us that even if the instance dependent attempts at deﬁning a secure
combined method for unknown policies failed (Section 3.2), nonetheless any sig-
niﬁcant future approach should keep on searching for an instance dependent
censor.
4.2
Potential Secrets vs. Secrecies
In this section we study some general conditions under which the transforma-
tion of secrecies into potential secrets (function pot sec(.)) yields a secure query
evaluation method. First, the transformation is formalized.

Conﬁdentiality Policies and Their Enforcement
51
Deﬁnition 2. Let control eval ps,a,e be any controlled query evaluation based on
potential secrets. The naive reduction of secrecies to control eval ps,a,e is the
function naive red(control eval ps,a,e) :=
λQ, log0, db, secr. control eval ps,a,e(Q, log0)(db, pot sec(secr)) .
Let the precondition of the naive reduction be satisﬁed by (Q, log0, db, secr) iﬀ
the precondition of control eval ps,a,e is satisﬁed by (Q, log0, db, pot sec(secr)).
The main theorem needs the following deﬁnition.
Deﬁnition 3. Let censor ps,a,e and precondps,a,e denote the censor and the pre-
condition (respectively) of control eval ps,a,e. We say that control eval ps,a,e is
insensitive to false potential secrets iﬀfor all Q, log0, db, pot sec, and for all
sets of sentences S whose members are all false in db,
censor ps,a,e(Q, log0, db, pot sec) = censor ps,a,e(Q, log0, db, pot sec ∪S)
precondps,a,e(Q, log0, db, pot sec) = precondps,a,e(Q, log0, db, pot sec ∪S) .
Note that all the methods for unknown policies introduced so far are insensitive
to false potential secrets (this property causes instance dependence, discussed in
the previous subsection). Now we are ready to state the main theorem of this
subsection.
Theorem 4. naive red(control eval ps,a,e)
preserves
conﬁdentiality
(w.r.t.
sec, a and e, cf. Deﬁnition 1) whenever control eval ps,a,e satisﬁes any of the
following conditions:
1. a = known and control eval ps,a,e preserves conﬁdentiality (w.r.t. ps, a
and e).
2. e = L (lies), the logs are always consistent and entail no potential secrets,
all answers are true in the absence of secrets, and all pairs with an empty
policy are admissible.
3. a = unknown, control eval ps,a,e preserves conﬁdentiality and is insensitive
to false potential secrets.
Proof. Part 1 Let precondps denote the precondition of control eval ps,a,e. Con-
sider arbitrary Q, log0, db1 and secr 1, satisfying the corresponding precondition
of naive red(control eval ps,a,e). Let Q′ be any ﬁnite preﬁx of Q, and consider
Θ = {Ψ, ¬Ψ} ∈secr 1.
Then, by the deﬁnitions, (db1, pot sec(secr 1)) satisﬁes precondps and
Ψ ∗= eval ∗(Ψ)(db1) ∈pot sec(secr 1).
(12)
Since, by assumption, conﬁdentiality is preserved for known potential secrets,
there exists db2 such that the following holds:
– (db2, pot sec(secr 1)) satisﬁes precondps, and thus also (db2, secr 1) satisﬁes
precondsec.

52
Joachim Biskup and Piero Bonatti
– (db1, pot sec(secr 1)) and (db2, pot sec(secr 1)) produce the same answers,
and hence so do (db1, secr 1) and (db2, secr 1).
– eval ∗(Ψ ∗)(db2) = ¬Ψ ∗, and thus also
eval ∗(Ψ)(db2) = ¬Ψ ∗.
(13)
From (12) and (13) we conclude that
{eval ∗(Ψ)(db1), eval ∗(Ψ)(db2)} = {Ψ ∗, ¬Ψ ∗} = {Ψ, ¬Ψ}.
Hence conﬁdentiality is preserved for known secrecies. This concludes Part 1.
Part 2 Given Q, log0, db1 and secr 1, let Ψ ∈pot sec(secr 1) and Q′ be any
ﬁnite preﬁx of Q. By assumption, there exists always an instance db2 that satisﬁes
the last log of control eval ps,a,e(Q′, log0)(db1, pot sec(secr 1)) and falsiﬁes Ψ. It
can be veriﬁed by a straightforward induction that this evaluation sequence
equals
control eval ps,a,e(Q′, log0)(db2, pot sec(∅))
(because all answers must be true by assumption and db2 is a model of the orig-
inal answers by construction). Moreover, the pair (db2, pot sec(∅)) is admissible
by assumption. It follows that the naive reduction preserves conﬁdentiality. This
completes Part 2.
Part 3 (Sketch) It suﬃces to show that for all ﬁnite sequences of queries Q′,
for all log0, for all admissible pairs (db1, pot sec(secr 1)) for control eval ps,a,e, and
for all Ψ ∈pot sec(secr 1), there exists an admissible pair (db2, pot sec(secr 2))
satisfying conditions (a) and (b) of Deﬁnition 1, with policy1 = pot sec(secr 1)),
policy2 = pot sec(secr 2)), and p = ps.
By assumption, control eval ps,a,e preserves conﬁdentiality, therefore there ex-
ists an admissible pair (db2, pot sec2) satisfying conditions (a) and (b) of Deﬁ-
nition 1. Since control eval ps,a,e is insensitive to false potential secrets, we can
assume, without loss of generality, that all the sentences in pot sec2 are satisﬁed
by db2.
Now let secr 2 = {{Ψ, ¬Ψ} | Ψ ∈pot sec2}. By construction, the sentences
in pot sec(secr 2) \ pot sec2 are all false in db2, and hence (by assumption)
control eval(Q′, log0)(db2, pot sec2) = control eval(Q′, log0)(db2, pot sec(secr 2))
and (db2, pot sec(secr 2)) is admissible. Then (db2, pot sec(secr 2)) is an admissi-
ble pair that satisﬁes (a) and (b) of Deﬁnition 1, as required.
⊓⊔
It is interesting to note that all the existing methods fall into the three cases
of the above theorem.
From Theorem 3 and Theorem 4.(1) we immediately get the following result.
Corollary 1. If the censor of control eval ps,a,e is instance independent and
control eval preserves conﬁdentiality w.r.t. ps, a and e, then the naive reduction
naive red(control eval ps,a,e) preserves conﬁdentiality w.r.t. both sec,known,e,
and sec,unknown,e.

Conﬁdentiality Policies and Their Enforcement
53
Remark 1. The above results subsume neither Theorem 1 nor Theorem 2. In
fact, in this section we show how methods for potential secrets can be adapted
to secrecies (not viceversa), while the gaps of Table 1 concerned only potential
secrets.
The naive reduction does not always preserve conﬁdentiality, even if the
underlying control eval ps,a,e does.
Example 2. Suppose that the vocabulary is {p, q}, and control eval ps,a,e returns
always mum for all possible combinations of Q, log0, db and pot sec, with two
exceptions, where log0 = ∅, and either
– pot sec = {p ∨¬q, ¬(p ∨¬q)}, db = {q}, or
– pot sec = {¬(p ∨q)}, db = ∅.
In these two cases, let ansi := eval ∗(Ψi)(db) if (i) logi−1 ∪ansi does not entail
any potential secret, and (ii) both {q} and ∅are models of logi−1 ∪ansi. If these
conditions are not simultaneously satisﬁed, let ansi := mum.
Finally, the only admissible instances (in all cases) are those that satisfy log0.
It can be veriﬁed that this method preserves conﬁdentiality (hint: the two
special cases return the same answers for all query sequences; the other cases
are obviously indistinguishable). Now, given Q = ⟨¬p, q⟩, the answers returned
for the admissible pair ({q}, {p ∨¬q, ¬(p ∨¬q)}) are ⟨¬p, mum⟩. The only other
instance that returns these answers is ∅under the policy {¬(p∨q)}, which is not
closed under negation, i.e., it does not correspond to any set of secrecies. Then
the naive reduction does not preserve conﬁdentiality w.r.t. p = sec.
⊓⊔
Note that the above example makes use of a controlled evaluation where (i)
the logs are always satisﬁed by the current instance, and (ii) the answers depend
on the false potential secret p ∨¬q. Then the critical assumptions in Theo-
rem 4.(2) (uniform lies) and Theorem 4.(3) (independence from false potential
secrets) cannot be simply dropped, because this example would then become
a counterexample.
5
Summary and Conclusions
The many works on controlled query evaluation have been given a uniform view
in Table 2 and Table 3, and the diﬀerent notions of conﬁdentiality have been
uniﬁed by Deﬁnition 1. Moreover, we have extended the picture of Table 1 by
introducing two new controlled evaluation methods based on lies and refusals
for unknown potential secrets (Section 3.1).
Surprisingly, all the attempts at designing secure combined methods for un-
known policies failed. The attempts were based on instance dependent censors,
by analogy with the existing methods for unknown policies. The general results
of Section 4.1 tell us that this is the right direction if the extra degree of freedom
allowed by unknown policies is to be exploited eﬀectively.

54
Joachim Biskup and Piero Bonatti
We have introduced a so-called naive reduction of secrecies to potential se-
crets. The naive reduction does not always preserve conﬁdentiality, but we proved
in Theorem 4 that conﬁdentiality is guaranteed under fairly natural assumptions.
All the known controlled evaluation methods fall into the three points of this
theorem, so Table 1 could be entirely reconstructed from the methods based
on potential secrets (including the new methods). This fact provides a formal
justiﬁcation of the relationships between diﬀerent methods empirically observed
in Section 2. Thus we achieve a deeper and systematic view of the many forms
of controlled query evaluation proposed so far. Moreover, our results show how
administrators may freely choose the preferred policy model (secrecies or po-
tential secrets) if the adopted evaluation method is based on potential secrets
and ﬁts into one of the cases of Theorem 4. Finer-grained conﬁdentiality require-
ments can be obtained by mixing secrecies and potential secrets. For each pair of
complementary sentences the administrator may decide whether both sentences
should be protected, or only one of them should be protected, or none of them
needs protection.
We are planning to relate our theoretical studies to the existing approaches to
conﬁdentiality, (e.g., polyinstantiation and statistical database perturbation), in
order to investigate the practical consequences of our theoretical results. Further
future work includes availability policies. Thereby, we hope to get a deeper un-
derstanding about the minimality of censors (see for instance [4] or [2], Prop. 4)
and the various roles of the user log.
Acknowledgements
We are grateful to the anonymous referees for their careful and cooperative
comments and suggestions.
References
[1] Biskup, J.: For unknown secrecies refusal is better than lying, Data and Knowl-
edge Engineering, 33 (2000), pp. 1-23.
40, 41, 44, 46, 47, 49
[2] Biskup, J., Bonatti, P. A. : Lying versus refusal for known potential secrets, Data
and Knowledge Engineering, 38 (2001), pp. 199-222.
40, 41, 44, 46, 54
[3] Biskup, J., Bonatti, P. A. : Controlled query evaluation for known policies by
combining lying and refusal, Proceedings 2nd Int. Symp. on th Foundations of
Information and Knowledge Systems, FoIKS 02, Lecture Notes in Computer
Science 2284, Springer, Berlin etc., 2002, pp. 49-66.
40, 41, 46, 49
[4] Bonatti, P. A., Kraus, S., Subrahmanian, V. S.: Foundations of secure deductive
databases, IEEE Transactions on Knowledge and Data Engineering 7,3 (1995),
pp. 406-422.
40, 41, 46, 54
[5] S. Castano, M. Fugini, G. Martella, P. Samarati: Database Security, Addison-
Wesley, 1994.
40
[6] D. E. Denning: Cryptography and Data Security, Addison-Wesley, 1982.
40
[7] Lloyd, J. W.: Foundations of Logic Programming, Springer, 1987.
42
[8] Shoenﬁeld, J. R.: Mathematical Logic, Addison-Wesley, Reading etc., 1967.
42

Conﬁdentiality Policies and Their Enforcement
55
[9] Sicherman, G. L., de Jonge, W., van de Riet, R. P.: Answering queries without
revealing secrets, ACM Transactions on Database Systems 8,1 (1983), pp. 41-59.
40, 41, 46, 47

Cardinality-Based Inference Control in
Sum-Only Data Cubes⋆
Lingyu Wang, Duminda Wijesekera, and Sushil Jajodia
Center for Secure Information Systems, George Mason University
Fairfax, VA 22030-4444, USA
{lwang3,dwijesek,jajodia}@gmu.edu
Abstract. This paper addresses the inference problems in data ware-
houses and decision support systems such as on-line analytical processing
(OLAP) systems. Even though OLAP systems restrict user accesses to
predeﬁned aggregations, inappropriate disclosure of sensitive attribute
values may still occur. Based on a deﬁnition of non-compromiseability
to mean that any member of a set of variables satisfying a given set
of their aggregations can have more than one value, we derive suﬃcient
conditions for non-compromiseability in sum-only data cubes. Under this
deﬁnition, (1) the non-compromiseability of multi-dimensional aggrega-
tions can be reduced to that of one dimensional aggregations, (2) full or
dense core cuboids are non-compromiseable, and (3) there is a tight lower
bound for the cardinality of a core cuboid to remain non-compromiseable.
Based on these results, taken together with a three-tier model for con-
trolling inferences, we provide a divide-and-conquer algorithm that uni-
formly divides data sets into chunks and builds a data cube on each such
chunk. The union of these data cubes are then used to provide users with
inference-free OLAP queries.
1
Introduction
Decision support systems such as On-line Analytical Processing (OLAP) are
becoming increasingly important in industry. These systems are designed to
answer queries involving large amounts of data and their statistical averages
in near real time. It is well known that access control alone is insuﬃcient in
eliminating all forms of disclosures, as information not released directly may be
inferred indirectly from answers to legitimate queries. This is known as inference
problem. An OLAP query typically consists of multiple aggregations, and hence
vulnerable to unwanted inferences. Providing inference-free answers to sum-only
data cube style OLAP queries while not adversely impacting performance or
restricting availability in an OLAP system is the subject matter of this paper.
The inference problem has been investigated since 70’s and many inference
control methods have been proposed for statistical databases. However, most of
⋆This work was partially supported by the National Science Foundation under grant
CCR-0113515.
D. Gollmann et al. (Eds.): ESORICS 2002, LNCS 2502, pp. 55–71, 2002.
c
⃝Springer-Verlag Berlin Heidelberg 2002

56
Lingyu Wang et al.
these methods become computationally infeasible if directly applied to OLAP
systems. OLAP applications demand short response times although queries usu-
ally aggregate large amounts of data [20]. Because most existing inference control
algorithms have run times proportional to the size of queries or aggregated data
sets, their impact upon performance renders them impractical for OLAP sys-
tems.
While arbitrary queries are common in statistical databases, OLAP queries
usually comprise of well-structured operations such as group-by, cross-tab and
sub-totals. These operations can conveniently be integrated with various data
cube operations, such as slicing-dicing, roll up and drill down [19]. We will show
that the limited formats and predictable structures of OLAP queries as well
as the multi-dimensional hierarchical data model of OLAP datasets can be ex-
ploited to simplify inference control.
Table 1 shows a small two-dimensional data set consisting of monthly em-
ployee salaries. Individual salaries should be hidden from users, and hence have
been replaced with the symbol ?. The symbol N/a denotes null value for inappli-
cable combinations of months and employees that are known to users.1 Assume
subtotals are allowed to be released to users through OLAP queries. Inference
problem occurs if any of the values represented by symbol ? can be derived from
the subtotals. No value in the ﬁrst two quarters can be inferred, because inﬁnitely
many diﬀerent values may satisfy each ? symbol with the subtotals satisﬁed. For
the third quarter, Mary’s salary in September can be inferred from the subto-
tals of September salaries because she is the only employee with a valid salary
for September. For the fourth quarter, by subtracting Bob’s and Jim’s fourth
quarter salaries ($4300 and $3000 respectively) from the subtotals in October
and November ($7100 and $4100 respectively) Alice’s salary for October can be
computed to be $3900.
Based on a deﬁnition of non-compromiseability to mean that any member of
a set of variables satisfying a given set of their aggregations can have more than
one value 2, we derive suﬃcient conditions for non-compromiseability in sum-only
data cubes: (1) the non-compromiseability of multi-dimensional aggregations can
be reduced to that of one dimensional aggregations, (2) full or dense core cuboids
are non-compromiseable, and (3) there is a tight lower bound on the cardinality
of a core cuboid for it to remain non-compromiseable. Based on these results,
and a three-tier model for controlling inferences, we provide a divide-and-conquer
algorithm that uniformly divides data sets into chunks and builds a data cube
on each such chunk. The union of these data cubes are then used to provide
users with inference-free OLAP queries.
The rest of the paper is organized as follows. Section 2 reviews existing
inference control methods proposed in statistical databases and OLAP systems.
Section 3 formalizes sum-only data cube and proves suﬃcient conditions for its
1 In general, data values are known through various kinds of external knowledge
(knowledge obtained through channels other than queries.)
2 In the settings of this paper, each variable can have either one value or inﬁnitely
many values.

Cardinality-Based Inference Control in Sum-Only Data Cubes
57
non-compromiseability. On the basis of a three-tier model these conditions are
integrated into an inference control algorithm in Section 4. Section 5 concludes
the paper.
2
Related Work
Inference control has been extensively studied in statistical databases as surveyed
in [14, 1, 15]. Inference control methods proposed in statistical databases are
usually classiﬁed into two main categories; restriction based techniques and per-
turbation based techniques. Restriction based techniques [18] include restricting
the size of a query set (i.e., the entities that satisfy a single query), overlap con-
trol [16] in query sets, auditing all queries in order to determine when inferences
are possible [11, 8, 22, 24], suppressing sensitive data in released tables contain-
ing statistical data [12], partitioning data into mutually exclusive sets [9, 10], and
restricting each query to range over at most one partition. Perturbation based
technique includes adding noise to source data [29], outputs [5, 25], database
structure [27], or size of query sets (by sampling data to answer queries) [13].
Some variations of the inference problem have been studied lately, such as the
inferences caused by arithmetic constraints [7, 6], inferring approximate values
instead of exact values [24] and inferring intervals enclosing exact values [23].
The inference control methods proposed for statistical databases generally
sacriﬁce eﬃciency for the control of inferences caused by arbitrary queries, which
Table 1. An example data cube
Quarter
Month
Alice Bob Jim Mary Sub Total
1
January
?
?
?
?
5500
February
?
?
?
?
5500
March
?
?
?
?
5500
Sub Total
3000 3000 4500 6000
2
April
?
?
?
?
6100
May
?
N/a
?
?
6100
June
?
?
?
?
4100
Sub Total
4500 3300 4500 4000
3
July
?
?
?
?
6100
August
?
?
?
?
6100
September
N/a N/a N/a
?
2000
Sub Total
3500 2200 2500 6000
4
October
?
?
?
N/a
7100
November
N/a
?
?
N/a
4100
December
?
N/a N/a
?
4100
*
Bonus
?
N/a N/a
?
6000
Sub Total
7000 4300 3000 7000

58
Lingyu Wang et al.
is essential for general purpose databases. However, this sacriﬁce is not desirable
for OLAP systems, because in OLAP systems near real time response takes pri-
ority over the generality of answerable queries. Hence most of these methods are
computationally infeasible in OLAP systems. As an example, Audit Expert [11]
models inference problem with a linear system Ax = b and detects the occurrence
of inference by transforming the m by n matrix A (corresponding to m queries
on n attribute values) to its reduced row echelon form. The transformation has
a well-known complexity of O(m2n), which is prohibitive in the context of data
warehouses and OLAP systems since m and n can be as large as a million.
Our work shares similar motivation with that of [16], i.e., to eﬃciently control
inference with the cardinality of data and queries, which can be easily obtained,
stored and maintained. Dobkin et al. gave suﬃcient conditions for the non-
compromiseability of arbitrary sum only queries [16]. The conditions are based
on the smallest number of queries that suﬃces to compromise the individual
data. Our work addresses multi-dimensional data cube queries. The fact that
data cube queries are a special case of arbitrary queries implies better results.
To the best of our knowledge, inference control for OLAP systems and data
warehouses are limited to [3, 2, 17, 26]. They all attempt to perturb sensi-
tive values while preserving the data distribution model, such that classiﬁcation
(or association rules) results obtained before and after the perturbation do not
change. These approaches are application-speciﬁc, that is, the sole purpose of
data analysis is limited to classiﬁcation (or association rule mining). We do not
have this restriction. Moreover, we do not use perturbation in this paper.
3
Cardinality-Based Non-compromiseability Criteria for
Data Cubes
This section deﬁnes our model for sum-only data cubes and formalizes com-
promiseability. We then derive cardinality-based suﬃcient conditions for non-
compromiseability based on the model and deﬁnitions.
3.1
Problem Formulation
In our model, a k-dimensional data cube consists of one core cuboid and several
aggregation cuboids. In addition, we use an aggregation matrix to abstract the
relationship between them. Each dimension is modeled as a closed integer inter-
val. The Cartesian product of the k dimensions is referred to as full core cuboid.
Each integer vector in the full core cuboid is a tuple. A core cuboid is a subset
of the full core cuboid, which contains at least one tuple for every value chosen
from every dimension. This condition allows us to uniquely identify the size of
each dimension for a given core cuboid. Deﬁnition 1 formalizes these concepts.
Deﬁnition 1 (Core Cuboids and Slices).
Given a set of k integers Di satisfying Di > 1 for all 1 ≤i ≤k. A
k-dimensional core cuboid is any subset S of Πk
i=1[1, Di] satisfying the prop-
erty that for any xi ∈[1, Di] there exist (k −1) integers xj ∈[1, Dj] for all

Cardinality-Based Inference Control in Sum-Only Data Cubes
59
1 ≤j ≤k and j ̸= i, such that (x1, . . . xi−1, xi, xi+1, . . . xk) ∈S. Cc denotes
a core cuboid. Each vector t ∈Cc is referred to as a tuple. Further, the ith ele-
ment of vector t ∈Cc, denoted by t[i], is referred to as the ith dimension of t.
We say that Πk
i=1[1, Di] is the full core cuboid denoted by Cf. We say a tuple t
is missing from the core cuboid Cc if t ∈Cf \ Cc. The subset of Cc deﬁned by
{t |t ∈Cc, t[i] = j} for each j ∈[1, Di] is said to be the jth slice of Cc on the ith
dimension, denoted by Pi(Cc, j). If Pi(Cc, j) = {t |t ∈Cf, t[i] = j, j ∈[1, Di]},
we say that Pi(Cc, j) is a full slice.
For example, the fourth quarter salaries in the sample data cube of Table 1 is
depicted in Table 2. It has two dimensions: month (dimension 1) and employee
name (dimension 2). Both dimensions have four diﬀerent values that are mapped
to the integer interval [1, 4]. Therefore, the full core cuboid Cf is [1, 4] × [1, 4].
The core cuboid Cc contains nine tuples and seven missing tuples are shown as
N/a.
To deﬁne aggregations of a data cube, we follow [19] to augment each dimen-
sion with a special value ALL, for which we use the symbol *. Each aggregation
vector is similar to a tuple except that it is formed with the augmented dimen-
sions. An aggregation vector selects a set of tuples in core cuboids with its *
values, which form its aggregation set. All aggregation vectors having * value in
the same dimensions form an aggregation cuboid. The concepts of aggregation
vector, aggregation cuboid and aggregation set are formalized in Deﬁnition 2.
Deﬁnition 2 (j-* Aggregation Vectors, Cuboids and Data Cubes).
A j-* aggregation vector t is a k dimensional vector satisfying t
∈
Πk
i=1([1, Di] ∪{∗}) and | {i : t[i] = ∗
for
1 ≤i ≤k} |= j. If t[i] = ∗,
then we say that the ith element is a
*-elements, and others are called
non
*-elements. A j-* aggregation cuboid is a set of aggregation vectors C such that
for any t, t′ ∈C, {i : t[i] = ∗} = {i : t′[i] = ∗} and | {i : t[i] = ∗} |= j. The
aggregation set of an j-* aggregation vector t is deﬁned as {t′
: t′ ∈Cc such
that t′[i] = t[i], ∀i t[i] ̸= ∗}. We use the notation Qset(t) for the aggregation set
of t. The aggregation set of a set of aggregation vectors S is deﬁned as the union
of Qset(t) for all t ∈S. We use notation Qset(S) for the aggregation set of S.
A data cube is deﬁned as a pair < Cc, Sall >, where Cc is a core cuboid, and Sall
is the set of all j-* aggregation cuboids, for all 1 ≤j ≤k.
For example, subtotals of the fourth quarter data in the data cube of Table 1
is depicted in Table 2. Each subtotal is represented as an aggregation vector
with * value. For example, (1, ∗) represents the subtotal in October. The aggre-
gation set of (1, ∗) is {(1,1), (1,2), (1,3)}. The set of four aggregation vectors
{(1, ∗), (2, ∗), (3, ∗), (4, ∗)} form an aggregation cuboid because all of them have
* value in the second dimension.
To abstract the relationship between a core cuboid and its aggregation
cuboids in a given data cube, we deﬁne a binary matrix referred to as aggre-
gation matrix. Each element of an aggregation matrix is associated with a tuple
and an aggregation vector. An element 1 means the tuple is contained in the ag-
gregation set of the aggregation vector, 0 otherwise. We assign the tuples in Cf

60
Lingyu Wang et al.
Table 2. Illustration of data cube
1 (Al) 2 (Bob) 3 (Jim) 4 (Ma) 5 (SubT)
1 (Oct)
(1,1)
(1,2)
(1,3)
N/a
(1,*)
2 (Nov)
N/a
(2,2)
(2,3)
N/a
(2,*)
3 (Dec)
(3,1)
N/a
N/a
(3,4)
(3,*)
4 (Bonus)
(4,1)
N/a
N/a
(4,4)
(4,*)
5 (SubT)
(*,1)
(*,2)
(*,3)
(*,4)
(*,*)
and any C in dictionary order, the aggregation cuboids in Sall in ascending
order on the number of *-elements and descending order on the index of the
*-element. This assignment enables us to refer to the ith tuple in Cf as Cf[i]
(similarly for Cc, Sall or their subsets). We use M[i, j] for the (i, j)th element of
matrix M. Aggregation matrix is formalized in Deﬁnition 3.
Deﬁnition 3 (Aggregation Matrix).
The aggregation matrix of the aggregation cuboid C on the core cuboid Cc is
deﬁned as the following (m × n) matrix MCc,C ( or simply M when Cc and C
are clear from context).
MCc,C[i, j] =

1, if Cf[j] ∈Qset(C[i]);
0, otherwise.
We deﬁne the aggregation matrix of S on Cc as the row block matrix with
the ith row block as the aggregation matrix of the ith aggregation cuboid in S.
We use S1 to represent the set of all 1-* aggregation cuboids for a given Cc,
and M1 the aggregation matrix of S1 on Cc (that is MCc,S1 ), referred to as the
1-* aggregation matrix.
Aggregation matrix and compromiseability are illustrated in Table 3. By rep-
resenting individual salaries with variables xi, we get a linear system MCc,S1·−→
X =
−→
B. It has at least one solution, since −→
B is caculated from the “real” salaries,
which must satisfy the stated linear system. From linear algebra [21], each xi
can have either a unique value or inﬁnitely many diﬀerent values among all the
solutions to MCc,S1 · −→
X = −→
B. This depends on MCc,S1 but not on −→
B (this is not
valid if additional knowledge about −→
X is known to users, for example, salaries
are non-negative [23, 24, 22]). If an xi has a unique value among all the solutions,
then clearly the sensitive value represented by xi was compromised. In this ex-
ample, x1 has the value of 3900 in any solution, so Alice’s salary for October is
compromised. In Deﬁnition 4, we formalize the deﬁnition of compromiseability.
We distinguish between two cases, that is, the trivial case illustrated by the third
quarter data of Table 1, and the complementary case illustrated by the fourth
quarter data.

Cardinality-Based Inference Control in Sum-Only Data Cubes
61
Table 3. Equations formulating the disclosure of the core cuboid given in Table 2
1 (Alice) 2 (Bob) 3 (Jim) 4 (Mary) 5 (Sub Total)
1 (Oct)
x1
x2
x3
N/a
7100
2 (Nov)
N/a
x4
x5
N/a
4100
3 (Dec)
x6
N/a
N/a
x7
4100
4 (Bonus)
x8
N/a
N/a
x9
6000
5 (Sub Total)
7000
4300
3000
7000
-
0
B
B
B
B
B
B
B
B
B
B
@
1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1
1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0
0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0
0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1
1
C
C
C
C
C
C
C
C
C
C
A
×
0
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
@
x1
x2
x3
0
0
x4
x5
0
x6
0
0
x7
x8
0
0
x9
1
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
A
=
0
B
B
B
B
B
B
B
B
B
B
@
7100
4100
4100
6000
7000
4300
3000
7000
1
C
C
C
C
C
C
C
C
C
C
A
Deﬁnition 4 (Compromiseability).
Given a data cube < Cc, Sall > and a set of aggregation cuboids S ⊆Sall, B
is arbitrarily chosen such that MCc,S.−→
X = −→
B has at least one solution. S
is said to compromise Cc, if at least one component xi of −→
X has the same
value among all the solutions to MCc,S.−→
X = −→
B.
1. Cc is trivially compromised by S if there is an integer i ∈[1, m] such that
the ith row of MCc,S is ej. Here 1 ≤j ≤n.
2. Cc is non-trivially compromised by S if Cc is not trivially compromised by S.
It is well-known that Cc is compromised by S if and only if there exists at
least one unit row vector ei ( where ei[i] = 1 and ei[j] = 0 for j ̸= i) in any
reduced row echelon form of MCc,S [21]. This yields an alternative deﬁnition of
compromiseability which we shall use in the rest of this paper.
3.2
Trivial Compromiseability
In this section, we derive cardinality-based criteria of non-compromiseability in
the trivial case. We have two results. Firstly, full core cuboids cannot be trivially
compromised. The second is an upper bound on the cardinality of the core cuboid
such that it is trivially compromised by the set of all 1-* aggregation cuboids.
They are stated and proved in Theorem 1.

62
Lingyu Wang et al.
Theorem 1.
1. A full core cuboid Cf cannot be trivially compromised by any
set of aggregation cuboids S.
2. Cc is trivially compromised by S1 if |Cc| < 2k−1 · max(D1, D2, . . . , Dk) for
k ≥2
Proof: Given in [30] (ommitted here due to space limitation).
Theorem 1 provides cardinality-based criteria for the two extreme cases, i.e.,
the core cuboid is either full or sparse. However, cardinality-based criteria are
ineﬀective for cases in between. As an example, consider the third quarter data
in Table 1, which is trivially compromised. Without changing the cardinality,
evenly distributing the three “N/a” in three months makes the core cuboid free of
trivial compromise. This invalidates any cardinality based criteria because trivial
compromiseability varies for core cuboids with exactly the same cardinality.
3.3
Non-trivial Compromiseability
In this section, we derive cardinality-based criteria for determining compro-
miseability in the non-trivial case. We have two results. The ﬁrst is that full
core cuboids cannot be non-trivially compromised. The second is a lower bound
on the cardinality of the core cuboid such that it remains safe from non-trivial
compromise. First we have Lemma 1.
Lemma 1.
1. Cc can not be non-trivially compromised by any single cuboid.
2. If Cc cannot be compromised by S1, then it cannot be compromised by Sall.
3. For any integers k and D1, D2, . . . , Dk that satisfy Di ≥4 for 1 ≤i ≤k,
there is a k-dimensional data cube < Cc, Sall >, with integer boundaries Di,
such that Cc is non-trivially compromised by S1.
Proof: Given in [30] (ommitted here due to space limitation).
Because of the second claim of Lemma 1, it is suﬃcient to safeguard the
core cuboid from the collection of 1-* aggregation cuboids. The last condition in
Lemma 1 shows that it is impossible to obtain a criteria for preventing non-trivial
compromiseability by only looking at the dimension cardinalities.
Theorem 2 (Non-trivial Compromiseability).
1. Cf cannot be non-trivially compromised by S1.
2. For any integers k and Di ≥4, there exists a k-dimensional data cube <
Cc, Sall > satisfying |Cf −Cc| = 2Dl +2Dm −9 such that Cc is non-trivially
compromised by S1, where Dl and Dm are the least two among Di.
3. If |Cf −Cc| < 2Dl + 2Dm −9, then Cc cannot be non-trivially compromised.
Proof: See the Appendix.
The ﬁrst claim in Theorem 2 guarantees the non-trivial compromiseability
of full core cuboid. Second and third claims give a tight lower bound on the
cardinality of a core cuboid for it to remain free of non-trivial compromises. The
second claim also implies that no cardinality based criteria can be derived for
core cuboids with a cardinality below the computed lower bound.

Cardinality-Based Inference Control in Sum-Only Data Cubes
63
Corollary 1 (Non-trivial Compromiseability).
If for any i ∈[1, k], there exists j ∈[1, Di] such that |Pi(Cf, j) −Pi(Cc, j)| =
0, Cc cannot be non-trivially compromised.
Proof: Follows from the proof of Theorem 2.
✷
By Corollary 1, full slice on every dimension guarantees non-compromiseabil-
ity in the non-trivial case.
4
A Cardinality-Based Inference Control Algorithm for
Data Cubes
This section describes an algorithm to control inferences in data cube style OLAP
queries using the results on compromiseability developed in Section 3. Our al-
gorithm is based on a three-tier model consisting of core data, pre-computed
aggregations and answerable queries.
4.1
Three-Tier Inference Control Model
Our three-tier model consists of three basic components and two abstract re-
lations in between as given below and illustrated in Figure 1. In addition we
enforce three properties on the model.
1. Three Tiers:
(a) A set of data items D.
(b) A set of aggregations A.
(c) A set of queries Q.
2. Relations Between Tiers:
(a) RAD ⊆A × D.
(b) RQA ⊆Q × A.
3. Properties:
(a) |A| << |Q|.
(b) There are partitions PD on D and PA on A, such that for any (a, d) ∈
RAD and (a′, d′) ∈RAD, d and d′ are in the same chunk of PD if and
only if a and a′ are in the same chunk of PA.
(c) D is not compromised by A.
Three-tier inference control model simpliﬁes inference control problem in sev-
eral ways. Firstly, because all queries in Q are derived from aggregations in A,
it suﬃces to ensure the non-compromiseability A instead of Q. This reduces the
complexity of inference control due to the ﬁrst characteristic of A. Secondly,
the second characteristic of A allows us to adopt a divide-and-conquer approach
to further reduce the complexity of inference control. Thirdly, inference control
is embedded in the oﬀ-line design of A and RAD, so the overhead of on-line
inference control is eliminated or reduced. Although the restriction of Q to be
derived from A reduces the total number of answerable queries, A can be de-
signed in such a way that it contains most semantics required by the application.
Hence the restricted queries are mostly arbitrary and meaningless with respect
to application requirements.

64
Lingyu Wang et al.
Data Set
(D)
Pre-defined Aggregations
(A)
User Queries
(Q)
RDA
RAQ
Inference Control
Fig. 1. Three-tier model for controlling inferences
Fig. 2. Inference control algorithm for data cubes
4.2
Inference Control Algorithm
The inference control algorithm shown in Figure 2 applies the results given in
Section 3 using our three-tier model. The algorithm ﬁrst partitions the core
cuboid into disjoint chunks, each of which is then passed to the subroutine
Ctrl Inf Chunk. The subroutine checks the non-compromiseability of the sub-

Cardinality-Based Inference Control in Sum-Only Data Cubes
65
data cube deﬁned on this chunk of data, using the cardinality based criteria. If it
is compromised the subroutine returns an empty set, indicating no aggregation
is allowed on the data. Otherwise, the subroutine returns the set of all aggrega-
tion cuboids of the sub-data cube. The ﬁnal outcome is the union of all partial
results returned by the subroutine (this set of aggregations can then be used to
answer data cube style OLAP queries without inference problem).
Correctness
The
correctness
of
the
algorithm,
that
is,
the
non-
compromiseability of the ﬁnal result is straight-forward. The subroutine
Ctrl Inf Chunk guarantees the non-compromiseability of each sub-data cube
respectively. In addition, the sub-data cubes are disjoint, making the non-
compromiseability of each of them independent of others.
Runtime Analysis: The main routine of the algorithm partitions Cc by eval-
uating the k dimensions of each tuple. Let n = |Cc|, so the runtime of the main
routine is O(nk)=O(n) (suppose k is ﬁxed with respect to n). The subroutine
Ctrl Inf Chunk is called for each of the N = k
i=1 mi chunks (mi are deﬁned in
the algorithm). It evaluates the cardinality of each input chunk C′
c, which has
the same complexity as establishing its 1-* aggregation matrix M ′
1.
Let n′ = k
i=1 D′
i be the number of columns in M ′
1 ( D′
i are deﬁned in the
algorithm), then m′ = n′ k
i=1
1
D′
i is the number of rows. Let Dmax
i
be the max-
imum value among D′
i. Out of the (m′n′) elements, O(m′ ·Dmax
i
) elements must
be considered to compute M ′
1. Suppose (k
i=1
1
D′
i )Dmax
i
= O(k). Then the run-
time of the subroutine is O(k·k
i=1 D′
i). It is called N times so the total runtime
is O(k·k
i=1 mi ·k
i=1 D′
i) = O(k·k
i=1 mi ·k
i=1
Di
mi ), which is O(k·k
i=1 Di) =
O(n). We note that by deﬁnition, determining non-compromiseability has a com-
plexity of O(n3) and the maximum non-compromiseable subset of aggregations
cannot be found in polynomial time [11].
Enhancing the Algorithm: The algorithm demonstrates a simple application
of the cardinality based criteria in Section 3, which can be improved in many
aspects. The dimension hierarchies inherent to most OLAP datasets can be
exploited to increase the semantics included in the ﬁnal output of the algorithm.
For example, assume time dimension has a hierarchy comprised of day, week,
month and year. Instead of partitioning the dataset arbitrarily, each week can be
used for a chunk. Queries about weeks, months and years can then be answered
with only the aggregations in algorithm output.
Notice that the key to cardinality-based non-compromiseability is that each
chunk in the partition of a core cube must be either empty or dense (full). The
row shuﬄing [4] technique proposed by Barbara et al. increases the subspace
density of data sets by shuﬄing tuples along those categorical, unordered di-
mensions. Row shuﬄing can be integrated into the inference control algorithm
as a pre-processing step prior to partitioning.

66
Lingyu Wang et al.
Data Cube Operations: We brieﬂy discuss how our algorithm may address
common data cube operations such as slicing, dicing, rolling up, drilling down
and range query. Slicing, dicing and range query require aggregations to be
deﬁned on a subspace formed by intervals in dimension domains. Our algorithm
also partitions the data set into small chunks. Therefore, in order to enhance our
algorithm to address these operations, the subspace required by these data cube
operations should be formed as the union of multiple chunks. Rolling up and
drilling down require aggregations to be deﬁned at diﬀerent granularities than
those in the original data cube. Rolling up does not directly create an inference
threat because with coarser granulated queries include less information about
individual data. Our ongoing work is addressing these details.
Although update operations are uncommon in decision support systems, data
stored in data warehouses need to be updated over time. Our algorithm is suit-
able for update operations in two aspects. Firstly, changing values has no eﬀect
on cardinality, which determines the non-compromiseability in our algorithm.
Secondly, because we have localized protection by partitioning data set into
small disjoint chunks, the eﬀect of an insertion or deletion is restricted to only
the chunks containing updated tuples.
5
Conclusions
Based on a deﬁnition of non-compromiseability to mean that each unknown
sensitive variable has more than one choices of value to ﬁt a given set of their
aggregations, we have derived suﬃcient conditions for non-compromiseability in
sum-only data cubes. We have proved that full core cuboids can not be com-
promised, and that there is a tight lower bound on the cardinality of a non-
compromiseable core cuboid. To apply our results to the inference control of
data cube style OLAP queries, we have shown a divide and conquer algorithm
based on a three-tier model. Future work includes enhancing our results and al-
gorithm to include data cube operations and consider other variations of OLAP
queries.
References
[1] N. R. Adam and J. C. Wortmann.
Security-control methods for statistical
databases: a comparative study. ACM Computing Surveys, 21(4):515–556, 1989.
57
[2] D. Agrawal and C. C. Aggarwal. On the design and quantiﬁcation of privacy pre-
serving data mining algorithms. In Proceedings of the Twentieth ACM SIGACT-
SIGMOD-SIGART Symposium on Principles of Database Systems, pages 247–
255, 2001.
58
[3] R. Agrawal and R. Srikant. Privacy-preserving data mining. In Proceedings of
the 2000 IEEE Symposium on Security and Privacy, pages 439–450, 2000.
58
[4] D. Barbar´a and X. Wu. Using approximations to scale exploratory data analysis
in datacubes. In Proceedings of the Fifth ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining, pages 382–386, 1999.
65

Cardinality-Based Inference Control in Sum-Only Data Cubes
67
[5] L. L. Beck.
A security mechanism for statistical databases.
ACM Trans. on
Database Systems, 5(3):316–338, 1980.
57
[6] A. Brodsky, C. Farkas, and S. Jajodia.
Secure databases: Constraints, infer-
ence channels, and monitoring disclosures. IEEE Trans. Knowledge and Data
Engineering, 12(6):900–919, 2000.
57
[7] A. Brodsky, C. Farkas, D. Wijesekera, and X. S. Wang. Constraints, inference
channels and secure databases. In the 6th International Conference on Principles
and Practice of Constraint Programming, pages 98–113, 2000.
57
[8] F. Y. Chin, P. Kossowski, and S. C. Loh. Eﬃcient inference control for range
sum queries. Theoretical Computer Science, 32:77–86, 1984.
57
[9] F. Y. Chin and G. ¨Ozsoyoglu.
Security in partitioned dynamic statistical
databases. In Proc. of IEEE COMPSAC, pages 594–601, 1979.
57
[10] F. Y. Chin and G. ¨Ozsoyoglu.
Statistical database design.
ACM Trans. on
Database Systems, 6(1):113–139, 1981.
57
[11] F. Y. Chin and G. ¨Ozsoyoglu.
Auditing and inference control in statistical
databases.
IEEE Trans. on Software Engineering, 8(6):574–582, 1982.
57,
58, 65
[12] L. H. Cox. Suppression methodology and statistical disclosure control. Journal
of American Statistic Association, 75(370):377–385, 1980.
57
[13] D. E. Denning. Secure statistical databases with random sample queries. ACM
Trans. on Database Systems, 5(3):291–315, 1980.
57
[14] D. E. Denning and P. J. Denning.
Data security.
ACM computing surveys,
11(3):227–249, 1979.
57
[15] D. E. Denning and J. Schl¨orer. Inference controls for statistical databases. IEEE
Computer, 16(7):69–82, 1983.
57
[16] D. Dobkin, A. K. Jones, and R. J. Lipton. Secure databases: protection against
user inﬂuence. ACM Trans. on Database Systems, 4(1):97–106, 1979.
57, 58
[17] A. Evﬁmievski, R. Srikant, , R. Agrawal, and J. Gehrke. Privacy preserving
mining of association rules. In Proceedings of the 8th Conference on Knowledge
Discovery and Data Mining (KDD’02), 2002.
58
[18] L. P. Fellegi. On the qestion of statistical conﬁdentiality. Journal of American
Statistic Association, 67(337):7–18, 1972.
57
[19] J. Gray, A. Bosworth, A. Layman, and H. Pirahesh. Data cube: A relational
operator generalizing group-by, crosstab and sub-totals. In Proceedings of the
12th International Conference on Data Engineering, pages 152–159, 1996.
56,
59
[20] V. Harinarayan, A. Rajaraman, and J. D. Ullman.
Implementing data cubes
eﬃciently. In Proceedings of the 1996 ACM SIGMOD international conference
on Management of data, pages 205–227, 1996.
56
[21] K. Hoﬀman. Linear Algebra. Prentice-Hall, 1961.
60, 61
[22] J. Kleinberg, C. Papadimitriou, and P. Raghavan. Auditing boolean attributes.
In Proc. of the 9th ACM SIGMOD-SIGACT-SIGART Symposium on Principles
of Database Systems, pages 86–91, 2000.
57, 60
[23] Y. Li, L. Wang, X. S. Wang, and S. Jajodia. Auditing interval-based inference.
In Proceedings of the 14th Conference on Advanced Information Systems Engi-
neering (CAiSE’02), 2001.
57, 60
[24] F. M. Malvestuto and M. Moscarini. Computational issues connected with the
protection of sensetive statistics by auditing sum-queries.
In Proc. of IEEE
Scientiﬁc and Statistical Database Management, pages 134–144, 1998.
57, 60

68
Lingyu Wang et al.
[25] J. M. Mateo-Sanz and J. Domingo-Ferrer. A method for data-oriented multi-
variate microaggregation. In Proceedings of the Conference on Statistical Data
Protection’98, pages 89–99, 1998.
57
[26] S. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining.
In Proceedings of the 28th Conference on Very Large Data Base (VLDB’02),
2002.
58
[27] J. Schl¨orer. Security of statistical databases: multidimensional transformation.
ACM Trans. on Database Systems, 6(1):95–112, 1981.
57
[28] R. P. Tewarson. Sparse Matrices. Academic Press, 1973.
69
[29] J. F. Traub, Y. Yemini, and H. Wo´zniakowski. The statistical security of a sta-
tistical database. ACM Trans. on Database Systems, 9(4):672–679, 1984.
57
[30] L. Wang, D. Wijesekera, and J. Sushil. Cardinality-based inference control in
sum-only data cubes (extended version). Technical Report, 2002. Available at
http://ise.gmu.edu/techrep/.
62
Appendix
Proof(Theorem 2):
1. Without loss of generality, we show that t0 = (1, 1, . . . , 1) cannot be nontriv-
ially compromised by S1. Let C′
f = {t : ∀i ∈[1, k], t[i] = 1 ∨t[i] = 2}. Since
(D1, D2, . . . , Dk) > 1, we have that C′
f ⊆Cf and |C′
f| = 2k. It suﬃces to
prove that t0 cannot be compromised by S1 in the data cube < C′
f, Sall >.
Let M ′
1 be the 1-* aggregation matrix of S1 on C′
f. According to Deﬁni-
tion 3, there are 2k non-zero column vectors in M ′
1, corresponding to the 2k
tuples in C′
f. In the rest of the proof we formally show that each of the 2k
non-zero column vectors can be represented by the linear combination of the
left 2k −1 column vectors. Then, it follows from linear algebra that t0 cannot
be compromised by S1 in data cube < C′
f, Sall > (and hence in < Cf, Sall >.
In order to prove our informally stated claim, we deﬁne the sign assignment
vector as an n dimensional column vector tsign where n is |Cf|, as follows:
– tsign[1] = 1
– tsign[2i + j] = −tsign[j] for all 0 ≤i ≤k −1 and 1 ≤j ≤2i
– tsign[j] = 0 for all j > 2k
Claim: M ′
1 · tsign = 0, where 0 is the n dimensional zero column vector.
Justiﬁcation:
Let t = S1[i], t[l] = ∗for l ∈[1, k].
Let v be M ′
1[i, −].
Suppose t[j] = 1 or t[j] = 2 for all j ̸= l.
Then |Qset(t)| = 2,
and as a consequence we get Qset(t) = {t1, t2}
where t1, t2 ∈Cf, t1[l] = 1,t1[l] = 2
and t1[j] = t2[j] = t[j] for all j ̸= l
Hence, there are two integers j1, j2 ∈[1, n] satisfying
v[j1] = v[j2] = 1 and v[j] = 0 for any j ̸= j1, j2.
By Deﬁnition 3, M ′
1[−, j1] (the jth
1
column of M ′
1) and M ′
1[−, j2]

Cardinality-Based Inference Control in Sum-Only Data Cubes
69
correspond to t1 and t2 respectively.
Because C′
f is formed in dictionary order, we get j2 = j1 + 2l−1.
Hence, we have v · tsign = 0.
Otherwise, |Qset(t)| = 0; and hence Qset(t) = φ.
Hence, v = 0, and hence, 0 · tsign = 0.
This justiﬁes our claim. Hence, as stated earlier, the justiﬁcation concludes
the main proof.
2. Without loss of generality we assume D1, D2 are the least two among Di’s.
For an arbitrary but ﬁxed value of D2, we show by induction on D1 that Cc
as constructed in the proof of Lemma 1 satisﬁes |Cf −Cc| = 2D1 + 2D2 −9.
Inductive Hypothesis: Cc as constructed in the proof of Lemma 1 satisﬁes:
– |Cf −Cc| = 2j + 2D2 −9 for any j ≥4.
– | P1(Cf, j) −P1(Cc, j) |= 2 for any j ∈[1, D1].
Base Case: In the base case of the proof of Lemma 1, the core cuboid Cc
satisﬁes |Cf −Cc| = 2D1 + 2D2 −9. Notice that the core cuboid, D1 = 4,
and | P1(Cf, j)−P1(Cc, j) |= 2. This validates the base case of our inductive
hypothesis.
Inductive Case: Suppose for D1 = j we have |Cf −Cc| = 2j+2D2−9 and |
P1(Cf, j)−P1(Cc, j) |= 2. Let C′
f be the full core cuboid corresponding to C′
c
for D1 = j+1. By the deﬁnition of C in the proof of Lemma 1, we have |C| =
|P1(Cc, j)| and as a consequence |C′
f −C′
c| = |Cf −Cc|+2 = 2(j+1)+2D2−9.
Since P1(C′
c, j + 1) = C. Hence, | P1(C′
f, j) −P1(C′
c, j) |= 2. This validates
the inductive case of our inductive argument and consequently concludes our
proof of the tightness of the cardinality lower bound for avoiding nontrivial
compromiseability.
3. Lower Bound: We show that if Cc is nontrivially compromised then we
have |Cf −Cc| ≥2D1 + 2D2 −9. First we make following assumptions.
(a) The tuple t = (1, 1, . . . , 1) ∈Cc is nontrivially compromised by S1
(b) No tuple in Cc is trivially compromised
(c) There exists S ⊆S1 such that S non-trivially compromises t, but for
any C ∈S, S \ C does not non-trivially compromise t
(d) For any t′ ∈Cf \Cc, t cannot be nontrivially compromised by S1 in data
cube < Cc ∪{t′}, Sall >.
Assumption (b) holds by Deﬁnition 4. Assumption (c) is reasonable consid-
ering the case S contains only a single aggregation cuboid. Assumption (d)
is reasonable considering the case |Cf \ Cc| = 1.
Claim: Suppose Assumption (a),(b),(c), and (d) hold. Furthermore assume
that there is a C ∈S where t ∈C satisﬁes t[i] = ∗. Then |Pi(Cf, 1) −
Pi(Cc, 1)| ≥1, and |Pi(Cf, j) −Pi(Cc, j)| ≥2 holds for any j ∈[2, Di].
Justiﬁcation: The proof is by contradiction. Without loss of generality, we
only justify the claim for i = k and j = 2. That is, given a C ∈S satisfying
t[k] = ∗for any t ∈C, we prove that |Pk(Cf, 2) −Pk(Cc, 2)| ≥2.
First we transform the aggregation matrix of S on Cc by row permutation
into a singly bordered block diagonal form (SBBDF) [28], denoted by Mm×n.
The ith diagonal block of M corresponds to Pk(Cc, i) and {t : t ∈S\C, t[k] =

70
Lingyu Wang et al.
i} , and the border of M denotes the aggregation cuboid C. We call the
columns of M corresponding to the ith diagonal block as the ith slice of M.
Due to Assumption (a), there exists a row vector a satisfying a · M = e1.
Let ri be M[i, −] then we get e1 = m
i=1 a[i] · ri. Suppose each diagonal
block of M has size m′ × n′. Use rj
i , for 1 ≤j ≤Dk to represent the row
vector composed of the elements of ri that falls into the jth slice of M. Notice
that there are n′ elements in rj
i . We also use e′
1 and 0′ to represent the n′
dimensional unit row vector and n′ dimensional zero row vector, respectively.
Then the following are true:
i. e′
1 = m′
i=1 a[i]r1
i + m
i=m−m′+1 a[i]r1
i
ii. 0′ = 2m′
i=m′+1 a[i]r2
i + m
i=m−m′+1 a[i]r2
i
First we suppose |Pk(Cf, 2) −Pk(Cc, 2)| = 0, that is, the second slice of M
contains no zero column. We then derive contradictions to our assumptions.
Since |Pk(Cf, 2) −Pk(Cc, 2)| = 0 the ﬁrst slice of M contains no more non-
zero columns than the second slice of M does. Intuitively if the latter is
transformed into a zero vector then applying the same transformation on
the former leads to a zero vector, too. This is formally represented as:
iii. 0′ = m′
i=1 a[m′ + i]r1
i + m
i=m−m′+1 a[i]r1
i .
Subtracting (iii) from (i) gives e′
1 = m′
i=1(a[i] −a[m′ + i])r1
i . That im-
plies Cc is nontrivially compromised by S \ {Ck}, contradicting Assumption
(c). Thus Pk(Cf, 2) −Pk(Cc, 2)| ̸= 0.
Next we assume |Pk(Cf, 2) −Pk(Cc, 2)| = 1 and derive a contradiction to
our assumptions.
First the row vector r3
i satisﬁes the following condition:
iv. 0′ = 3m′
i=2m′+1 a[i]r3
i + m
i=m−m′+1 a[i]r3
i .
Let t′ ∈Pk(Cf, 2) \ Pk(Cc, 2). Notice that (i), (ii) still hold. Suppose t′
corresponds to M[−, y] = 0. Now assume we add t′ to Pk(Cc, 2), consequently
we have M[−, y] ̸= 0. Due to Assumption (d), we have that the left side of
(ii) becomes e′
1, that is, a · M[−, y] = 1. There is also an extra 1-element
M[x, y] in the border of M.
Now let t′′ be the tuple corresponding to M[−, y+n′] in the third slice of M.
Suppose t′′ ∈Pk(Cc, 3) and consequently M[−, y + n′] ̸= 0. We have that
M[−, y + n′] = M[−, y] and consequently a · M[−, y + n′] = 1.
By removing t′ from Pk(Cc, 2) we return to the original state that all our
assumption hold. Now we show by contradiction that t′′ ∈Pk(Cc, 3) cannot
hold any longer. Intuitively, since t′ is the only missing tuple in the second
slice of M, the third slice of M contains no more non-zero vectors than the
second slice of M does, except t′′. Because a · M[−, y + n′] = 1, elements
of a transform the second slice of M to a zero vector, as shown by (ii), also
transform the third slice of M to a unit vector. This is formally represented
in (v):
v. e′′ = 3m′
i=2m′+1 a[i −m′]r3
i + m
i=m−m′+1 a[i]r3
i
Subtracting (iv) from (v) we get that e′′ = 3m′
i=2m′+1(a[i −m′] −a[i])r3
i ;
implying Cc is compromised by S \ {Ci}. Hence, Assumption (c) is false.
Consequently, t′′ /∈Cc.

Cardinality-Based Inference Control in Sum-Only Data Cubes
71
Similar proof exists for the ith slice of Cc, where i = 4, 5, . . ., Dk. However,
M[x, −] ̸= 0 because if so, we can let ax be zero and then decrease the number
of missing tuples in Cc, contradicting Assumption (d). Hence M[x, −] is
a unit vector with the 1-element in the ﬁrst slice of M. However, this further
contradicts Assumption (b), that no trivial compromise is possible. Hence
we have that |Pk(Cf, 2) −Pk(Cc, 2)| = 1 is false.
Now consider |Pk(Cf, 1)−Pk(Cc, 1)|. Suppose all the assumptions hold, and
|Pk(Cf, 1) −Pk(Cc, 1)| = 0. Let t1, t2 ∈Pk(Cf, 2) \ Pk(Cc, 2). Now deﬁne
C′
c = Cc \ {t} ∪{t1} and M ′ be the aggregation matrix of S on C′
c. From
a·M = e1, and Assumption (d) we get a·M ′ = ei, where M[−, i] corresponds
to t1. This implies the nontrivially compromise of t1 in < C′
c, Sall >, with
|Pk(Cf, 1) −Pk(C′
c, 1)| = 1, which contradicts what we have already proved.
Hence, we get |Pk(Cf, 1)−Pk(Cc, 1)| ≥1. This concludes the justiﬁcation of
our claim.
The claim implies that the number of missing tuples in Cc increases mono-
tonically with the following:
– The number of aggregation cuboids in S.
– Di, provided there is C ∈S satisfying t[i] = ∗for any t ∈C.
Hence |Cf −Cc| reaches its lower bound when S = {C1, C2}, which is equal
to 2D1 +2D2 −9, as shown in the ﬁrst part of the current proof - concluding
the proof of Theorem 2.
✷

Outbound Authentication for
Programmable Secure Coprocessors
Sean W. Smith⋆
Department of Computer Science, Dartmouth College
Hanover, New Hampshire USA
http://www.cs.dartmouth.edu/~sws/
Abstract. A programmable secure coprocessor platform can help solve
many security problems in distributed computing. However, these solu-
tions usually require that coprocessor applications be able to participate
as full-ﬂedged parties in distributed cryptographic protocols. Thus, to
fully enable these solutions, a generic platform must not only provide
programmability, maintenance, and conﬁguration in the hostile ﬁeld—it
must also provide outbound authentication for the entities that result.
A particular application on a particular untampered device must be able
to prove who it is to a party on the other side of the Internet.
This paper oﬀers our experiences in solving this problem for a high-end
secure coprocessor product. This work required synthesis of a number
of techniques, so that parties with diﬀerent and dynamic views of trust
can draw consistent and complete conclusions about remote coproces-
sor applications. These issues may be relevant to the industry’s growing
interest in rights management for general desktop machines.
1
Introduction
How does one secure computation that takes place remotely—particularly when
someone with direct access to that remote machine may beneﬁt from compromis-
ing that computation? This issue lies at the heart of many current e-commerce,
rights management, and PKI issues.
To address this problem, research (e.g., [17, 24, 25]) has long explored the
potential of secure coprocessors: high-assurance hardware devices that can be
trusted to carry out computation unmolested by an adversary with direct phys-
ical access. For example, such an adversary could subvert rights management on
a complex dataset by receiving the dataset and then not following the policy;
secure coprocessors enable solutions by receiving the dataset encapsulated with
the policy, and only revealing data items in accordance with the policy. [13] For
⋆A preliminary version of this paper appeared as Dartmouth College Technical Report
TR2001-401. This paper reports work the author did while a research staﬀmember
at IBM Watson. Subsequent work was supported in part by the Mellon Foundation,
AT&T/Internet2, and the U.S. Department of Justice (contract 2000-DT-CX-K001).
The views and conclusions do not necessarily reﬂect the sponsors.
D. Gollmann et al. (Eds.): ESORICS 2002, LNCS 2502, pp. 72–89, 2002.
c
⃝Springer-Verlag Berlin Heidelberg 2002

Outbound Authentication for Programmable Secure Coprocessors
73
another example, an adversary could subvert decentralized e-cash simply by in-
creasing a register. However, secure coprocessors enable solutions: the register
lives inside a trusted box, which modiﬁes the value only as part of a transaction
with another trusted box. [24] Many other applications—including private infor-
mation retrieval [3, 19], e-commerce co-servers [14], and mobile agents [26]—can
also beneﬁt from the high-assurance neutral environment that secure coproces-
sors could provide.
As the literature (e.g.,[5, 8]) discusses, achieving this potential requires sev-
eral factors, including establishing and maintaining physical security, enabling
the device to authenticate code-loads and other commands that come from the
outside world, and building applications whose design does not negate the secu-
rity advantages of the underlying platform.
However, using secure coprocessors to secure distributed computation also
requires outbound authentication (OA): the ability of coprocessor applications
be able to authenticate themselves to remote parties. (Code-downloading loses
much of its eﬀect if one cannot easily authenticate the entity that results!) Merely
conﬁguring the coprocessor platform as the appropriate entity—a rights box,
a wallet, an auction marketplace—does not suﬃce in general. A signed state-
ment about the conﬁguration also does not suﬃce. For maximal eﬀectiveness,
the platform should enable the entity itself to have authenticated key pairs and
engage in protocols with any party on the Internet: so that only that particu-
lar trusted auction marketplace, following the trusted rules, is able to receive
the encrypted strategy from a remote client; so that only that particular trusted
rights box, following the trusted rules, is able to receive the object and the rights
policy it should enforce.
The Research Project. The software architecture for a programmable se-
cure coprocessor platform must address the complexities of shipping, upgrades,
maintenance, and hostile code, for a generic platform that can be conﬁgured
and maintained in the hostile ﬁeld. [18] Our team spent several years working
on developing a such a device; other reports [7, 8, 20] present our experiences in
bringing such a device into existence as a COTS product, the IBM 4758.
Although our initial security architecture [20] sketched a design for outbound
authentication, we did not fully implement it—nor fully grasp the nature of the
problem—until the Model 2 device released in 2000. As is common in product
development, we had to concurrently undertake tasks one might prefer to tackle
sequentially: identify fundamental problems; reason about solutions; design, code
and test; and ensure that it satisﬁed legacy application concerns.
The Basic Problem. A relying party needs to conclude that a particular key
pair really belongs to a particular software entity within a particular untam-
pered coprocessor. Design and production constraints led to a non-trivial set of
software entities in a coprocessor at any one time, and in any one coprocessor
over time. Relying parties trust some of these entities and not others; further-
more, we needed to accommodate a multiplicity of trust sets (diﬀerent parties

74
Sean W. Smith
have diﬀerent views), as well as the dynamic nature of any one party’s trust set
over time.
This background sets the stage for the basic problem: how should the device
generate, certify, change, store, and delete private keys, so that relying parties
can draw those conclusions consistent with their trust set, and only those con-
clusions?
This Paper. This paper is a post-facto report expanding on this research and
development experience, which may have relevance both to other secure co-
processor technology (e.g., [23]) as well as to the growing industry interest in
remotely authenticating what’s executing on a desktop (e.g., [9, 10, 11, 21]).
Section 2 discusses the evolution of the problem in the context of the un-
derlying technology. Section 3 presents the theoretical foundations. Section 4
presents the design. Section 5 suggests some directions for future study.
2
Evolution of Problem
2.1
Underlying Technology
We start with a brief overview of the software structure of the 4758. The de-
vice is tamper-responding: with high assurance, on-board circuits detect tamper
attempts and destroy the contents of volatile RAM and non-volatile battery-
backed RAM (BBRAM) before an adversary can see them. The device also is
a general purpose computing device; internal software is divided into layers, with
layer boundaries corresponding to divisions in function, storage region, and ex-
ternal control. The current family of devices has four layers: Layer 0 in ROM,
and Layer 1 through Layer 3 in rewritable FLASH.
The layer sequence also corresponds to the sequence of execution phases after
device boot: initially Layer 0 runs, then invokes Layer 1, and so on. (Because our
device is an enclosed controlled system, we can avoid the diﬃculties of secure
boot that arise in exposed desktop systems; we know execution starts in Layer 0
ROM in a known state, and higher-level ﬁrmware is changed only when the device
itself permits it.) In the current family, Layer 2 is intended to be an internal
operating system, leading to the constraint that it must execute at maximum
CPU privilege; it invokes the single application (Layer 3) but continues to run,
depending on its use of the CPU privilege levels to protect itself.
We intended the device to be a generic platform for secure coprocessor ap-
plications. The research team insisted on the goal that third parties (diﬀerent
from IBM, and from each other) be able to develop and install code for the OS
layer and the application layer. Business forces pressured us to have only one
shippable version of the device, and to ensure that an untampered device with
no hardware damage can always be revived. We converged on a design where
Layer 1 contains the security conﬁguration software which establishes owners
and public keys for the higher layers, and validates code installation and update
commands for those layers from those owners. This design decision stemmed

Outbound Authentication for Programmable Secure Coprocessors
75
from our vision that application code and OS code may come from diﬀerent
entities who may not necessarily trust each other’s updates; centralization of
loading made it easier to enforce the appropriate security policies.
Layer 1 is updatable, in case we want to upgrade algorithms, ﬁx bugs, or
change the public key of the party authorized to update the layer. However,
we mirror this layer—updates are made to the inactive copy, which is then
atomically made active for the next boot—so that failures during update will
not leave us with a non-functioning code-loading layer.
Authentication Approach. Another business constraint we had was that
the only guaranteed contact we would have with a card was at manufacture
time. In particular, we could assume no audits, nor database of card-speciﬁc
data (secret or otherwise), nor provide any on-line services to cards once they
left. This constraint naturally suggested the use of public-key cryptography for
authentication, both inbound and outbound.
Because of the last-touch-at-manufacturing constraint, we (the manufac-
turer) can last do something cryptographic at the factory.
2.2
User and Developer Scenarios
Discussions about potential relying parties led to additional requirements.
Developers were not necessarily going to trust each other. For example, al-
though an application developer must trust the contents of the lower layers when
his application is actually installed, he should be free to require that his secrets
be destroyed should a lower layer be updated in a way he did not trust. As
a consequence, we allowed each code-load to include a policy specifying the con-
ditions under which that layer’s secrets should be preserved across changes to
lower layers. Any other scenario destroys secrets.
However, even full-preservation developers reserved the right to, post-facto,
decide that certain versions of code—even their own—were untrusted, and be
able to verify whether an untrusted version had been installed during the lifetime
of their secrets.
In theory, the OS layer should resist penetration by a malicious application;
in practice, operating systems have a bad history here, so we only allow one
application above it (and intend the OS layer solely to assist the application
developer). Furthermore, we need to allow that some relying parties will believe
that the OS in general (or speciﬁc version) may indeed be penetrable by malicious
applications.
Small developers may be unable to assure the public of the integrity and
correctness of their applications (e.g., through code inspection, formal modeling,
etc). Where possible, we should maximize the credibility our architecture can
endow on such applications.

76
Sean W. Smith
Epoch 1
Epoch 2
Conﬁg 1, in Epoch 1
Conﬁg 2, in Epoch 1
Conﬁg 1, in Epoch 2
Secret-destroying code-load
Secret-destroying code-load
Secret-preserving code-load
Fig. 1. An epoch starts with code-load action that clears a layer’s secrets; with
an epoch, each secret-preserving code-load starts a new conﬁguration
2.3
On-Card Entities
One of the ﬁrst things we need to deal with is the notion of what an on-card
entity is. Let’s start with a simple case: suppose the coprocessor had exactly one
place to hold software and that it zeroized all state with each code-load. In this
scenario, the notion of entity is pretty clear: a particular code-load C1 executing
inside an untampered device D1. The same code C1 inside another device D2
would constitute a diﬀerent entity; as would a re-installation of C1 inside D1.
However, this simple case raises a challenges. If a reload replaces C1 with C2,
and reloads clear all tamper-protected memory, how does the resulting entity—
C2 on D1—authenticate itself to a party on the other side of the net? The card
itself would have no secrets left, since the only data storage hidden from physical
attack was cleared. Consequently, any authentication secrets would have to come
with C2, and we would start down a path of shared secrets and personalized code-
loads. Should an application entity “include” the OS underneath it? Should it
include the conﬁguration control layers that ran earlier in this boot sequence,
but are no longer around?
Since we built the 4758 to support real applications, we gravitated toward
a practical deﬁnition: an entity is an installation of the application software in
a trusted place, identiﬁed by all underlying software and hardware.
Secret Retention. As noted, developers demanded that we sometimes permit
secret retention across reload. With a secret-preserving load; the entity may stay
the same, but the code may change. The conﬂicting concepts that developers had
about what exactly happens to their on-card entity when code update occurs
lead us to think more closely about entity lifetimes. We introduce some language
to formalize that. Figure 1 sketches these concepts.
Deﬁnition 1 (Conﬁguration, Epoch). A Layer N conﬁguration is the max-
imal period in which that Layer is runnable, with an unchanging software envi-
ronment. A Layer N epoch is the maximal period in which the Layer can run
and accumulate state. If E is an on-card entity in Layer N,
– E is an epoch-entity if its lifetime extends for a Layer n epoch.

Outbound Authentication for Programmable Secure Coprocessors
77
– E is a conﬁguration-entity if its lifetime extends for a Layer n conﬁguration.
An Layer n epoch-entity consists of a sequence of Layer n conﬁguration-
entities. This sequence may be unbounded—since any particular epoch might
persist indeﬁnitely, across arbitrarily many conﬁguration changes.
2.4
Authentication Scenarios
This design left us with on-card software entities made up of several compo-
nents with diﬀering owners, lifetimes, and state. A natural way to do outbound
authentication to give the card a certiﬁed key pair, whose private key lives in
tamper-protected memory. However, the complexity of the entity structure cre-
ates numerous problems.
Application Code. Suppose entity C is the code C1 residing in the application
Layer 3 in a particular device. C may change: two possible changes include
a simple code update taking the current code C1 to C2, or a complete re-install
of a diﬀerent application from a diﬀerent owner, taking C1 to C3.
If a relying party P trusts C1, C2, and C3 to be free of ﬂaws, vulnerabilities,
and malice, then the natural approach might work. However, if P distrusts some
of this code, then problems arise.
– If P does not trust C1, then how can P distinguish between an entity with
the C2 patch, and an entity with a corrupt C1 pretending to have the C2
patch?
– If P does does not trust C2, then then how can P distinguish between the
an entity with the honest C1, and an entity with the corrupt C2 pretending
to be the honest C1? (The mere existence of a signed update command
compromises all the cards.)
– If P does not trust C3, then how can P distinguish between the honest C1
and a malicious C3 that pretends to be C1?
Code-Loading Code. Even more serious problems arise if a corrupted version
of the conﬁguration software in Layer 1 exists. If an evil version existed that al-
lowed arbitrary behavior, then (without further countermeasures) a party P can-
not distinguish between any on-card entity E1, and an E2 consisting of a rogue
Layer 1 carrying out some elaborate impersonation.
OS Code. Problems can also arise because the OS code changes. Debugging an
application requires an operating system with debug hooks; in ﬁnal development
stages, a reasonable scenario is to be able to “update” back-and-forth between
a version of the OS with debug hooks and a version without.
With no additional countermeasures, a party P cannot distinguish between
the application running securely with the real OS, the application with debug
hooks underneath it, and the application with the real OS but with a policy that
permits hot-update to the debug version. The private key would be the same in
all cases.

78
Sean W. Smith
Internal Certiﬁcation. The above scenarios suggest that perhaps a single
key pair (for all entities in a card for the lifetime of the card) may not suﬃce.
However, extending to schemes where one on-card entity generates and certiﬁes
key pairs for other on-card entities also creates challenges.
For example, suppose Layer 1 generates and certiﬁes key pairs for the Layer 2
entity. If a reload replaces corrupt OS B1 with an honest B2, then party P should
be able to distinguish between the certiﬁed key pair for B2 and that for B1.
However, without further countermeasures, if supervisor-level code can see all
data on the card, then B1 can forge messages from B2—since it could have seen
the Layer 1 private key.
A similar penetrated-barrier issue arises if we expect an OS in Layer 2 to
maintain a private key separate from an application Layer 3, or if we enter-
tained alternative schemes where mutually suspicious applications executed con-
currently. If a hostile application might in theory penetrate the OS protections,
then an external party cannot distinguish between messages from the OS, mes-
sages from the honest application, and messages from rogue applications.
This line of thinking led us to the more general observation that, if the
certiﬁer outlives the certiﬁed, then the integrity of what the certiﬁed does with
their key pair depends on the future behavior of the certiﬁer. In the case of
the coprocessor, this observation has subtle and dangerous implications—for
example, one of the reasons we centralized conﬁguration control in Layer 1 was
to enable the application developer to distrust the OS developer and request that
the application (and its secrets) be destroyed, if the underlying OS undergoes
an update the application developer does not trust. What if the untrusted OS
has access to a private key used in certifying the original application?
We revisit these issues in Section 4.3.
3
Theory
The construction of the card suggests that we use certiﬁed key pairs for outbound
authentication. However, as we just sketched, the straightforward approach of
just sending the card out with a certiﬁed key pair permits trouble.
In this section, we try to formalize the principles that emerged while consid-
ering this problem.
A card leaves the factory and undergoes some sequence of code loads and
other conﬁguration changes. A relying party interacts with an entity allegedly
running inside this card. The card’s OA scheme enables this application to wield
a private key and to oﬀer a collection of certiﬁcates purporting to authenticate
its keyholder.
It would be simplest if the party could use a straightforward validation algo-
rithm on this collection. As Maurer [15, 16] formalized, a relying party’s valida-
tion algorithm needs to consider which entities that party trusts. Our experience
showed that parties have a wide variety of trust views that change dynamically.
Furthermore, we saw the existence of two spaces: the conclusions that a party will
draw, given an entity’s collection of certiﬁcates and the party’s trust view; and

Outbound Authentication for Programmable Secure Coprocessors
79
the conclusions that a party should draw, given the history of those keyholders
and the party’s trust view.
We needed to design a scheme that permits these sets of conclusions to match,
for parties with a wide variety of trust views.
3.1
What the Entity Says
Relying party P wants to authenticate interaction with a particular entity E.
Many scenarios could exist here; for simplicity, our analysis reduces these to the
scenario of E needing to prove to P that own(E, K): that E has exclusive use of
the private element of key pair K.
We need to be able to talk about what happens to a particular coprocessor.
Deﬁnition 2 (History, Run, ≺). Let a history be a ﬁnite sequence of com-
putation for a particular device. Let a run be some unbounded sequence of com-
putation for a particular device. We write H ≺R when history H is a preﬁx of
run R.
In the context of OA for coprocessors that cannot be opened or otherwise
examined, and that disappear once they leave the factory, it seemed reasonable to
impose the restriction that on-card entities carry their certiﬁcates. For simplicity,
we also imposed the restriction that they present the same ﬁxed set no matter
who asks.
Deﬁnition 3. When entity E wishes to prove it owns K after history H, let
Chain(E, K, H) denote the set of certiﬁcates that it presents.
3.2
Validation
Will a relying party P believe that E owns K?
First, we need some notion of trust. A party P usually has some ideas of
which on-card applications it might trust to behave “correctly” regarding keys
and signed statements, and of which ones it is unsure.
Deﬁnition 4. For a party P, let TrustSet(P) denote the set of entities whose
statements about certiﬁcates P trusts. Let root be the factory CA: the trust root
for the card. A legitimate trust set is one that contains root.
In the context of OA for coprocessors, it was reasonable to impose the re-
striction that the external party decides validity based on an entity’s chain and
the party’s own list of trusted entities. (The commercial restriction that we can
never count on accessing cards after they leave made revocation infeasible.) We
formalize that:
Deﬁnition 5 (Trust-set scheme). A trust-set certiﬁcation scheme is one
where the relying party’s Validate algorithm is deterministic on the variables
Chain(E, K, H) and TrustSet(P).

80
Sean W. Smith
We thus needed to design a trust-set certiﬁcation scheme that accommodates
any legitimate trust set, since discussion with developers (and experiences doing
security consulting) suggested that relying parties would have a wide divergence
of opinions about which versions of which software they trust.
3.3
Dependency
The problem scenarios in Section 2.4 arose because one entity E2 had an un-
expected avenue to use the private key that belonged to another entity E1. We
need language to express these situations, where the integrity of E1’s key actions
depends on the correct behavior of E2.
Deﬁnition 6 (Dependency Function). Let E be the set of entities. A depen-
dency function is a function D : E −→2E such that, for all E1, E2:
–
E1 ∈D(E1)
– if E2 ∈D(E1) then D(E2) ⊂D(E1)
When a dependency function depends on the run R, we write DR.
What dependency function shall we use for analysis? In our specialized hard-
ware, code runs in a single-sandbox controlled environment which (if the physi-
cal security works as intended) is free from outside observation or interference.
Hence, in our analysis, dependence follows read and write:
Deﬁnition 7. For entities E1 and E2 in run R, we write E2
data
−→R E1 when E1
has read/write access to the secrets of E2 (E2
data
−→R E2 trivially) and E2
code
−→R E1
when E1 has write access to the code of E2. Let −→R be the transitive closure
of the union of these two relations. For an entity E in a run R, deﬁne DepR(E)
to be {F : E −→R F}.
In terms of the coprocessor, if C1 follows B1 in the post-boot sequence, then
we have C1
data
−→R B1 (since B1 could have manipulated data before passing con-
trol). If C2 is a secret-preserving replacement of C1, then C1
data
−→RC2 (because C2
still can touch the secrets C1 left). If A can reburn the FLASH segment where B
lives, then B
code
−→R A (because A can insert malicious code into B, that would
have access to B’s private keys).
3.4
Consistency and Completeness
Should the relying party draw the conclusions it actually will? In our analysis,
security dependence depends on the run; entity and trust do not. This leads to
a potential conundrum. Suppose, in run R, C −→R B and C ∈TrustSet(P), but
B ̸∈TrustSet(P). Then a relying party P cannot reasonably accept any signed
statement from C, because B may have forged it.
To capture this notion, we deﬁne consistency for OA. The intention of consis-
tency is that if the party concludes that an message came from an entity, then it

Outbound Authentication for Programmable Secure Coprocessors
81
really did come from that entity—modulo the relying party’s trust view. That is,
in any H ≺R where P concludes own(E, K) from Chain(E, K, H), if the entities
in TrustSet(P) behave themselves, then E really does own K. We formalize this
notion:
Deﬁnition 8. An OA scheme is consistent for a dependency function D when,
for any entity E, a relying party P with any legitimate trust set, and history and
run H ≺R:
Validate(P, Chain(E, K, H)) =⇒DR(E) ⊆TrustSet(P)
One might also ask if the relying party will draw the conclusions it actually
should. We consider this question with the term completeness. If in any run
where E produces Chain(E, K, H) and DR(E) is trusted by P—so in P’s view,
no one who had a chance to subvert E would have—then P should conclude
that E owns K.
Deﬁnition 9. An OA scheme is complete for a dependency function D when,
for any entity E claiming key K, relying party P with any legitimate trust set,
and history and run H ≺R:
DR(E) ⊆TrustSet(P) =⇒Validate(P, Chain(E, K, H))
These deﬁnitions equip us to formalize a fundamental observation:
Theorem 1. Suppose a trust-set OA scheme is both consistent and complete for
a given dependency function D. Suppose entity E claims K in histories H1 ≺R1
and H2 ≺R2. Then:
DR1(E) ̸= DR2(E) =⇒Chain(E, K, H1) ̸= Chain(E, K, H2)
Proof. Suppose DR1(E) ̸= DR2(E) but Chain(E, K, H1) = Chain(E, K, H2). We
cannot have both DR1(E) ⊆DR2(E) and DR2(E) ⊆DR1(E), so, without loss
of generality, let us assume DR2(E) ̸⊆DR1(E). There thus exists a set S with
DR1(E) ⊆S but DR2(E) ̸⊆S.
Since the scheme is consistent and complete, it must work for any legitimate
trust set, including S. Let party P have S = TrustSet(P). Since this is a trust-set
certiﬁcation scheme and E produces the same chains in both histories, party P
must either validate these chains in both scenarios, or reject them in both sce-
narios. If party P accepts in run R2, then the scheme cannot be consistent for
D, since E depends on an entity that P did not trust. But if party P rejects
in run R1, then the scheme cannot be complete for D, since party P trusts all
entities on which E depends.
3.5
Design Implications
We consider the implications of Theorem 1 for speciﬁc ways of constructing
chains and drawing conclusions, for speciﬁc notions of dependency. For exam-
ple, we can express the standard approach—P makes its conclusion by recur-
sively verifying signatures and applying a basic inference rule—in a Maurer-
style calculus [15]. Suppose C is a set of certiﬁcates: statements of the form

82
Sean W. Smith
K1 says own(E2, K2). Suppose S be a set of entities trusted to speak the truth
about certiﬁcate ownership: {E1 : trust(E1, own(E2, K2))}. A relying party may
start by believing C ∪{own(root, Kroot)}.
To describe what a party will conclude, we can deﬁne Viewwill(C, S) to be the
set of statements derivable from this set by applying the rule
own(E1, K1),
E1 ∈S,
K1 says own(E2, K2) ⊢own(E2, K2)
The Validate algorithm for party P then reduces to the decision of whether
own(E, K) is in this set.
We can also express what a party should conclude about an entity, in terms
of the chain the entity presents, and the views that the party has regarding trust
and dependency. If D is a dependency function, we can deﬁne Viewshould(C, S, D)
to be the set of statements derivable by applying the alternate rule:
own(E1, K1),
D(E1) ⊆S,
K1 says own(E2, K2) ⊢own(E2, K2)
In terms of this calculus, we obtain consistency be ensuring that for any
chain and legitimate trust set, and H ≺R, the set Viewwill(Chain(E, K, H), S) is
contained in the set Viewshould(Chain(E, K, H), S, DR). The relying party should
only use a certiﬁcate to reach a conclusion when the entire dependency set of
the signer is in TrustSet(P).
4
Design
For simplicity of veriﬁcation, we would like Chain(E, K, H) to be a literal chain:
a linear sequence of certiﬁcates going back to root. To ensure consistency and
completeness, we need to make sure that, at each step in the chain, the partial
set of certiﬁers equals the dependency set of that node (for the dependency
function we see relying parties using). To achieve this goal, the elements we
can manipulate include generation of this chain, as well as how dependency is
established in the device.
4.1
Layer Separation
Because of the post-boot execution sequence, code that executes earlier can
subvert code that executes later.1 If B, C are Layer i, Layer i + 1 respectively,
then C −→R B unavoidably.
However, the other direction should be avoidable, and we used hardware to
avoid it. To provide high-assurance separation, we developed ratchet locks: an
independent microcontroller tracks a counter, reset to zero at boot time. The
microcontroller will advance the ratchet at the main coprocessor CPU’s request,
but never roll it back. Before B invokes the next layer, it requests an advance.
1 With only one chance to get the hardware right, we did not feel comfortable with
attempting to restore the system to a more trusted state, short of reboot.

Outbound Authentication for Programmable Secure Coprocessors
83
To ensure B
data
̸−→R C, we reserved a portion of BBRAM for B, and used
the ratchet hardware to enforce access control. To ensure B
code
̸−→R C, we write-
protect the FLASH region where B is stored. The ratchet hardware restricts
write privileges only to the designated preﬁx of this execution sequence.
To keep conﬁguration entities from needlessly depending on the epoch en-
tities, in our Model 2 device, we subdivided the higher BBRAM to get four
regions, one each for epoch and conﬁguration lifetimes, for Layer 2 and Layer 3.
The initial clean-up code in Layer 1 (already in the dependency set) zeroizes
the appropriate regions on the appropriate transition. (For transitions to a new
Layer 1, the clean-up is enforced by the old Layer 1 and the permanent Layer 0—
to avoid incurring dependency on the new code.)
4.2
The Code-Loading Code
As discussed elsewhere, we felt that centralizing code-loading and policy de-
cisions in one place enabled cleaner solutions to the trust issues arising when
diﬀerent parties control diﬀerent layers of code. But this centralization creates
some issues for OA. Suppose the code-loading Layer 1 entity A1 is reloaded
with A2. Business constraints dictated that A1 do the reloading, because the
ROM code had no public-key support. It’s unavoidable that A2
code
−→R A1 (be-
cause A1 could have cheated, and not installed the correct code). However, to
avoid A1
data
−→R A2, we take these steps as an atomic part of the reload:
A1
generates a key pair for its successor A2; A1 uses its current key pair to sign
a transition certiﬁcate attesting to this change of versions and key pairs; and A1
destroys its current private key.
This technique—which we implemented and shipped with the Model 1 devices
in 1997—diﬀers from the standard concept of forward security [1] in that we
change keys with each new version of software, and ensure that the name of the
new version is spoken by the old version. As a consequence, a single malicious
version cannot hide its presence in the trust chain; for a coalition of malicious
versions, the trust chain will name at least one. (Section 5 considers forward-
secure signatures further.)
4.3
The OA Manager
Since we do not know a priori what device applications will be doing, we felt
that application key pairs needed to be created and used at the application’s
discretion. Within our software architecture, Layer 2 should do this work—since
it’s easier to provide these services at run-time instead of reboot, and the Layer 1
protected memory is locked away before Layer 2 and Layer 3 run.
This OA Manager component in Layer 2 will wield a key pair generated and
certiﬁed by Layer 1, and will then generate and certify key pairs at the request of
Layer 3. These certiﬁcates indicate that said key pair belongs to an application,
and also include a ﬁeld chosen by the application. (A fuller treatment of our

84
Sean W. Smith
(a)
BCA
C1
B1
B2
updated to
(b)
BCA
C1
C2
replaced by
penetrates
Fig. 2. Having the OS certify key pairs for the application creates interesting
lifetime issues. In (a), if the OS is updated while preserving its key pair, then
the application depends on both versions; in (b), if the OS outlives the appli-
cation but is potentially penetrable, then the application may depend on future
applications
trust calculus would thus distinguish between owning and trusting a key pair for
certiﬁcation purposes, and owning and trusting a key pair for the application-
speciﬁed purpose—the last link.)
To keep the chain linear, we decided to have Layer 1 generate and destroy the
OA Manager key pair (e.g., instead of adding a second horizontal path between
successive versions of the OA Manager key pairs). The question then arises of
when the OA Manager key pair should be created and destroyed.
We discuss some false starts. If the OA Manager outlived the Layer 2 con-
ﬁguration, then our certiﬁcation scheme cannot be both consistent and com-
plete. For a counterexample (see Figure 2, (a)) suppose that application C1 is
a conﬁguration-entity on top of OS B1; that OS B1 changes code to OS B2 but
both are part of the same entity BCA; and that party P trusts C1, B1 but not B2.
For the scheme to be complete, P should accept certiﬁcate chains from C1—but
that means accepting a chain from BCA, and BCA −→R B2 ̸∈TrustSet(P).
This counterexample fails because the application entity has a CA whose
dependency set is larger than the application’s. Limiting the CA to the current
Layer 2 conﬁguration eliminates this issue, but still fails to address penetration
risk. Parties who come to believe that a particular OS can be penetrated by an
application can end up with the current BCA depending on future application
loads. (See Figure 2, (b).)
Our ﬁnal design avoided these problems by having the Layer 2 OA Manager
live exactly as long as the Layer 3 conﬁguration. Using the protected BBRAM
regions, we ensure that upon any change to the Layer 3 conﬁguration, Layer 1
destroys the old OA Manager private key, generates a new key pair, and certiﬁes
it to belong to the new OA Manager for the new Layer 3 conﬁguration. This
approach ensures that the trust chain names the dependency set for Layer 3
conﬁgurations—even if dependency is extended to include penetration of the
OS/application barrier.
Since we need to accommodate the notion of both epoch and conﬁguration life-
times (as well as to allow parties who choose the former to change their minds),
we identify entities both by their current epoch, as well as the current conﬁgura-

Outbound Authentication for Programmable Secure Coprocessors
85
tion within that epoch. When it requests a key pair, the Layer 3 application can
specify which lifetime it desires; the certiﬁcate includes this information. Private
keys for a conﬁguration lifetime are kept in the Layer 3 conﬁguration region in
BBRAM; private keys for an epoch lifetime are kept in the epoch region.
Note that a Layer 3 epoch certiﬁcate (say, for epoch E) still names the conﬁg-
uration (say, C1) in which it began existence. If, in some later conﬁguration Ck
within that same epoch, the relying party decides that it wants to examine the
individual conﬁgurations to determine the whether an untrusted version was
present, it can do that by examining the trust chain for Ck and the sequence of
OA Manager certiﬁcates from C1 to Ck. An untrusted Layer 1 will be revealed in
the Layer 1 part of the chain; otherwise, the sequence of OA Manager certiﬁcates
will have correct information, revealing the presence of any untrusted Layer 2
or Layer 3 version.
4.4
Summary
As noted earlier, the trust chain for the current Layer 1 version starts with the
certiﬁcate the factory root signed for the ﬁrst version of Layer 1 in the card,
followed by the sequence of transition certiﬁcates for each subsequent version
of Layer 1 installed. The trust chain for the OA Manager appends the OA
Manager certiﬁcate, signed by the version of Layer 1 active when that Layer 3
conﬁguration began, and providing full identiﬁcation for the current Layer 2
and Layer 3 conﬁgurations and epochs. The trust chain for a Layer 3 key pair
appends the certiﬁcate from the OA Manager who created it.
Our design thus constitutes a trust-set scheme that is consistent and complete
for the dependency function we felt was appropriate, for any legitimate trust set.
4.5
Implementation
Full support for OA shipped with all Model 2 family devices and the CP/Q++
embedded operating system. (The announced Linux port [12] still has the Layer 1
OA hooks; extending Linux to handle that is an area of future work.)
Implementation required some additional design decisions. To accommodate
small developers (Section 2.2), we decided to have the OA Manager retain all
Layer 3 private keys and wield them on the application’s behalf; consequently,
a party who trusts the penetration-resistance of a particular Layer 2 can thus
trust that the key was at least used within that application on an untampered
device. Another design decision resulted from the insistence of an experienced
application architect that users and developers will not pay attention to details
of certiﬁcate paths; to mitigate this risk, we do not provide a “verify this chain”
service—applications must explicitly walk the chain—and we gave diﬀerent fam-
ilies of cards diﬀerent factory roots.
A few aspects of the implementation proved surprising. One aspect was the
fact that the design required two APIs: one between Layer 1 and Layer 2, and
another between Layer 2 and the application. Another aspect was ﬁnding places
to store keys. We extended the limited area in BBRAM by storing a MAC key

86
Sean W. Smith
and a TDES encryption key in each protected region, and storing the ciphertext
for new material wherever we could: during a code-change, that region’s FLASH
segment; during application run-time, in the Layer 2-provided PPD data storage
service. Another interesting aspect was the multiplicity of keys and identities
added when extending the Layer 1 transition engine to perform the appropriate
generations and certiﬁcations. For example, if Layer 1 decides to accept a new
Layer 1 load, we now also need to generate a new OA Manager key pair, and
certify it with the new Layer 1 key pair as additional elements of this atomic
change. Our code thus needed two passes before commitment: one to determine
everyone’s names should the change succeed, and another to then use these
names in the construction of new certiﬁcates.
As has been noted elsewhere [8], we regret the design decisions to use our
own certiﬁcate format, and the fact that the device has no form of secure time
(e.g., Layer 3 can always change the clock). Naming the conﬁguration and epoch
entities was challenging, particularly since the initial architecture was designed
in terms of parameters such as code version and owner, and a precise notion of
“entity” only emerged later.
5
Conclusions
One might characterize the entire OA architecture process “tracing each depen-
dency, and securing it.” Our experience here, like other aspects of this work,
balanced the goals of enabling secure coprocessing applications while also living
within product deadlines. OA enables Alice to design and release an applica-
tion; Bob to download it into his coprocessor; and Charlie to then authenticate
remotely that he’s working with this application in an untampered device.
Outbound authentication allows third-party developers to ﬁnally deploy
coprocessor applications, such as Web servers [14] and rights management
boxes [13], that can by authenticated by anyone in the Internet, and partici-
pate in PKI-based protocols.
We quickly enumerate some avenues for future research and reﬂection.
Alternative Software Structure Our OA design follows the 4758 architecture’s
sequence of increasingly less-trusted entities after boot. Some current research
explores architectures that dispense with this limiting assumption, and also dis-
pensing with the 4758 assumptions of one central loader/policy engine, and of
a Layer 2 that exists only to serve a one-application Layer 3. It would be inter-
esting to explore OA in these realms.
Similarly, the analysis and design presented in this paper assumes that an
authority makes a statement about an entity at the time a key pair is created.
Long-lived entities with the potential for run-time corruption suggest ongoing
integrity-checking techniques. It would be interesting to examine OA in light of
such techniques.

Outbound Authentication for Programmable Secure Coprocessors
87
Alternate Platforms
Since our work, the Trusted Computing Platform Al-
liance [21] has published ideas on how remote parties might gain assurance about
the software conﬁguration of remote desktop machines; Microsoft is considering
similar ideas [9, 10, 11]. It would be interesting to explore the interaction of our
OA work with this increasingly timely topic, as well as the longer history of work
in securely booting desktops [2, 6, 22].
Alternate Cryptography
We developed our transition certiﬁcate scheme for
Layer 1 to ensure that not all corrupt entities could hide their presence in a chain.
Entities aside, this scheme is essentially a basic forward-secure signature scheme
(e.g., [4], Sec. 3.3). It would be interesting how the broader space of forward-
secure signature schemes might be used in these settings.
Alternate Dependency Our dependency function—entity E1 can subvert E2
when it can read or write secrets, or write code, at any time—emerged for the
special case of our device. A more careful incorporation of time would be inter-
esting, as would an examination of the other avenues of manipulation in more
complex settings (e.g., the opportunities for covert channels in common desktop
operating systems, or if the coprocessor cryptopages to the host ﬁle system).
Formalizing Trust Sets One linchpin of our design was the divergence and dy-
namic nature of what relying parties tend to trust. (Consequently, our analysis
assumed that parties may have “any legitimate trust set.”) It would be inter-
esting to measure and formally characterize the trust behavior that occurs (or
should occur) with real-world relying parties and software entities.
Formalizing Penetration Recovery Much complicated reasoning arose from sce-
narios such as “what if, in six months, trusted software component X turns out
be ﬂawed?” Further exploration of the design and implementation of authenti-
cation schemes that explicitly handle such scenarios would be interesting.
Acknowledgments
The author gratefully acknowledges the comments and advice of the greater IBM
4758 team; particular thanks go to Mark Lindemann, who co-coded the Layer 2
OA Manager, and Jonathan Edwards, who tested the API and transformed
design notes into manuals and customer training material. The author is also
grateful for the comments and advice of the Internet2 PKI Labs on new research
issues here. Finally, gratitude is due the anonymous referees, who have made
this a stronger and clearer paper.
References
[1] R.
Anderson.
Invited
lecture,
ACM
CCS,
1997.
The
deﬁnitive
writ-
ten
record
appears
to
be
Two
Remarks
on
Public
Key
Cryptology,
http://www.ftp.cl.cam.ac.uk/ftp/users/rja14/forwardsecure.pdf. 83

88
Sean W. Smith
[2] W. A. Arbaugh, D. J. Farber, J. M. Smith. “A Secure and Reliable Bootstrap Ar-
chitecture.” IEEE Computer Society Conference on Security and Privacy. 1997.
87
[3] D. Asonov and J. Freytag. “Almost Optimal Private Information Retrieval.”
Privacy Enhancing Technology 2002. Springer-Verlag LNCS.
73
[4] M. Bellare and S. Miner. “A Forward-Secure Digital Signature Scheme.” Ex-
tended abstract appears in Crypto 99, Springer-Verlag LNCS. Full version at
http://www-cse.ucsd.edu/~mihir/papers/fsig.html. 87
[5] M. Bond, R. Anderson. “API-Level Attacks on Embedded Systems.” IEEE Com-
puter. 34:67-75. October 2001.
73
[6] L. Clark and L. J. Hoﬀmann. “BITS: A Smartcard Protected Operating System.”
Communications of the ACM. 37: 66-70. 1994.
87
[7] J. Dyer, R. Perez, S. W. Smith, M. Lindemann. “Application Support Architec-
ture for a High-Performance, Programmable Secure Coprocessor.” 22nd National
Information Systems Security Conference. October 1999.
73
[8] J. Dyer, M. Lindemann, R. Perez, R. Sailer, S. W. Smith, L. van Doorn, S. Wein-
gart. “Building the IBM 4758 Secure Coprocessor.” IEEE Computer, 34:57-66.
October 2001.
73, 86
[9] P. England, J. DeTreville and B. Lampson. Digital Rights Management Operating
System. United States Patent 6,330,670. December 2001.
74, 87
[10] P. England, J. DeTreville and B. Lampson. Loading and Identifying a Digital
Rights Management Operating System. United States Patent 6,327,652. Decem-
ber 2001.
74, 87
[11] P. England, M. Peinado. “Authenticated Operation of Open Computing De-
vices.” 7th Australasian Conference on Information Security and Privacy.
Springer-Verlag LNCS2384. June 2002.
74, 87
[12] “IBM Research Demonstrates Linux Running on Secure Cryptographic Copro-
cessor.” Press release, August 28, 2001.
85
[13] A. Iliev, S. W. Smith. “Prototyping an Armored Data Vault: Rights Management
for Big Brother’s Computer.” Privacy Enhancing Technology 2002. Springer-
Verlag LNCS.
72, 86
[14] S. Jiang, S. W. Smith, K. Minami. “Securing Web Servers against Insider At-
tack.” ACSA/ACM Annual Computer Security Applications Conference. Decem-
ber 2001.
73, 86
[15] R. Kohlas and U. Maurer. “Reasoning About Public-Key Certiﬁcation: On Bind-
ings Between Entities and Public Keys.” Journal on Selected Areas in Com-
munications. 18: 551-560. 2000. (A preliminary version appeared in Financial
Cryptography 99, Springer-Verlag.). 78, 81
[16] U. Maurer. “Modelling a Public-Key Infrastructure.” ESORICS 1996. Springer-
Verlag LNCS.
78
[17] S. W. Smith. Secure Coprocessing Applications and Research Issues. Los Alamos
Unclassiﬁed Release LA-UR-96-2805, Los Alamos National Laboratory. August
1996.
72
[18] S. W. Smith, E. R. Palmer, S. H. Weingart. “Using a High-Performance, Pro-
grammable Secure Coprocessor.” Proceedings, Second International Conference
on Financial Cryptography. Springer-Verlag LNCS, 1998.
73
[19] S. W. Smith, D. Saﬀord. “Practical Server Privacy Using Secure Coprocessors.”
IBM Systems Journal (special issue on End-to-End Security) 40: 683-695. 2001.
73

Outbound Authentication for Programmable Secure Coprocessors
89
[20] S. W. Smith, S. H. Weingart. “Building a High-Performance, Programmable Se-
cure Coprocessor.” Computer Networks (Special Issue on Computer Network
Security). 31: 831-860. April 1999.
73
[21] Trusted Computing Platform Alliance. TCPA Design Philosophies and Con-
cepts, Version 1.0. January, 2001.
74, 87
[22] J. D. Tygar and B. S. Yee. “Dyad: A System for Using Physically Secure Co-
processors.” Proceedings of the Joint Harvard-MIT Workshop on Technological
Strategies for the Protection of Intellectual Property in the Network Multimedia
Environment. April 1993.
87
[23] N. van Someren. “Access Control for Secure Execution.” RSA Conference 2001.
74
[24] B. S. Yee. Using Secure Coprocessors. Ph.D. thesis. Computer Science Technical
Report CMU-CS-94-149, Carnegie Mellon University. May 1994.
72, 73
[25] B. S. Yee and J. D. Tygar. “Secure Coprocessors in Electronic Commerce Appli-
cations.” 1st USENIX Electronic Commerce Workshop. 1996.
72
[26] B. S. Yee. A Sanctuary For Mobile Agents. Computer Science Technical Report
CS97-537, UCSD, April 1997. (An earlier version of this paper appeared at the
DARPA Workshop on Foundations for Secure Mobile Code.).
73

Hamming Weight Attacks on Cryptographic
Hardware – Breaking Masking Defense⋆
Marcin Gomulkiewicz1 and Miroslaw Kutylowski1,2
1 Cryptology Centre, Pozna´n University
2 Institute of Mathematics, Wroclaw University of Technology
ul. Wybrze˙ze Wyspia´nskiego 27, 50-370 Wroclaw, Poland
Abstract. It is believed that masking is an eﬀective countermeasure
against power analysis attacks: before a certain operation involving a key
is performed in a cryptographic chip, the input to this operation is com-
bined with a random value. This has to prevent leaking information since
the input to the operation is random.
We show that this belief might be wrong. We present a Hamming weight
attack on an addition operation. It works with random inputs to the
addition circuit, hence masking even helps in the case when we cannot
control the plaintext. It can be applied to any round of the encryption.
Even with moderate accuracy of measuring power consumption it de-
termines explicitly subkey bits. The attack combines the classical power
analysis (over Hamming weight) with the strategy of the saturation at-
tack performed using a random sample.
We conclude that implementing addition in cryptographic devices must
be done very carefully as it might leak secret keys used for encryption. In
particular, the simple key schedule of certain algorithms (such as IDEA
and Twoﬁsh) combined with the usage of addition might be a serious
danger.
Keywords: cryptographic hardware, side channel cryptanalysis, Ham-
ming weight, power analysis
1
Introduction
Symmetric encryption algorithms are often used to protect data stored in inse-
cure locations such as hard disks, archive copies, and so on. The key used for this
purpose is stored inside a cryptographic device and should not leave the device
in an unencrypted form during its lifetime. The device must oﬀer reasonable
protection against retrieving the key, since security cannot be based on phys-
ical security of the device. For this reason, investigating cryptanalytic attacks
against such devices has signiﬁcant importance.
The threats for devices considered are not conﬁned to such classical “math-
ematical” methods as diﬀerential or linear cryptanalysis, not less important are
other methods taking into account not only plaintexts and ciphertexts, but also
⋆This research was initiated when the second author visited University of Mannheim
D. Gollmann et al. (Eds.): ESORICS 2002, LNCS 2502, pp. 90–103, 2002.
c
⃝Springer-Verlag Berlin Heidelberg 2002

Hamming Weight Attacks on Cryptographic Hardware
91
any sources of information due to physical nature of computation. Particularly
dangerous might be information sources that are overlooked during designing of
the algorithm or those that emerge at the time an algorithm is implemented.
Cryptanalysis of cryptographic algorithms based on such side channel infor-
mation was initiated a few years ago. In the meantime it became a recognized
and extremely dangerous line of attacks.
Timing attack. Execution time of encryption may leak information on the data
involved. It has been ﬁrst observed for asymmetric ciphers [11] – the operations
such as modular exponentiation are optimized in order to reduce the computa-
tion time (which is a problem in this case). However, computation time becomes
strongly dependent on data involved and in this way may leak information on
the key. In the case of a symmetric ciphers timing attack may also be relevant,
as shown in the example of non careful implementation of IDEA [10].
Power analysis attack. The second side channel source of information on
computations inside cryptographic device is power consumption [12]. It depends
very much on the operations performed and the data manipulated inside the
device.
For technical reasons, we cannot measure consumption occurring at a chosen
place in the circuit (although it is possible to do similar measurements for elec-
tromagnetic radiation [8]). We can measure only global consumption and only
up to some extend. However, the power consumption can be sampled over the
time providing information on diﬀerent stages of computation separately.
Simple power analysis (SPA) takes the data on the global power consumption
of the device during diﬀerent moments of the computation. A more sophisticated
method is diﬀerential power analysis (DPA) [12]. In that case, one considers
the power consumption data and some other data computed, for instance, for
all candidate subkeys that may occur at some point of the computation. The
idea is that for the right subkey, power consumption and the data computed for
the subkey should be somehow statistically related. On the other hand, for the
wrong key relation between these data should be purely random. In this way,
one hopes to retrieve information on the local keys despite the fact that only
global information on power consumption is available.
Hamming attack. Some authors [10] indicate that for certain hardware tech-
nologies there is a strong correlation between the number of bits set to one
and power consumption. Based on this phenomenon, they propose a Hamming-
weight attack on DES-chips (and similar ciphers) with the aim to derive a secret
key stored inside the device. The attack is potentially quite dangerous for block
ciphers such that small portions of a key act independently on small portions of
data – just like in the case of DES and its S-boxes.
It is required for the attack to determine the total Hamming weights of
the output of the Feistel function of DES during the last round in a series of
encryptions. This weight consists of weight of the output of an S-Box S we
are attacking and the output of the remaining S-boxes. For ﬁnding the key one
guesses the subkey bits which go into the XOR gate immediately before S. Then
it is possible to compute the (hypothetical) outputs of S for the ciphertexts

92
Marcin Gomulkiewicz and Miroslaw Kutylowski
at hand. If the subkey bits were guessed correctly, then there is a statistical
correlation between the weights of the computed outputs of S and the Hamming
weights measured. For a wrong guess, there is no such a correlation. This is
a way to determine which guess is correct and thereby to derive the key bits.
This attack cannot be applied as it is for non-Feistel ciphers. Also operations
performed on larger data blocks increase the complexity of an attack signiﬁcantly.
Saturation attack. Somewhat related to this paper is saturation attack,
which was applied to such ciphers as Square [5], Crypton [17], Rijndael [6] and
Twoﬁsh [13]. It can be used if a part, say U, of an encryption circuit performs
a (key dependent) permutation on blocks of bits of a ﬁxed length, say m. The
general idea is that if we input each possible bit string of length m into U exactly
once, then each bit string of length m occurs as an output exactly once. The
order in which these outputs appear depends on an unknown key, but the set of
values is known.
The idea of a saturation attack is that “global” behaviour of encryption
algorithm of some part is key independent, even if “local” behaviour (that is,
for a single input) is key dependent. This is used to “get through” U. However,
a little bit diﬀerent strategy can be applied: since “global” behaviour does not
depend on particular input, it may depend on the key only. In this way it may
leak some information on the key involved.
Masking. Some of the attacks mentioned rely on speciﬁc properties of certain
operations. The methods such as timing attack, SPA, DPA and Hamming attack
explore the property that certain parameters of an operation depend heavily on
input/output data. Having found a speciﬁc situation we may conclude on data
involved in the operation. A part of this data might be the key bits.
In order to prevent such attacks two techniques have been introduced. The
ﬁrst one proposes to “split” or “duplicate” all intermediate results [9, 2, 3]. How-
ever, this method has large memory or time overhead. The second much more
eﬃcient method, called masking, “blinds” input data to arithmetic and boolean
operations [14, 4, 7]. The idea is to combine the input with a random value, then
to perform an operation with the key, and ﬁnally extract the random factor. For
instance, adding subkey k to an intermediate result a can be implemented by
the following operations:
choose r at random
z = a + r
z = z + k
z = z −r
Note that in the above procedure subkey k is added to a random number.
This prevents the attacks mentioned. Similarly, one may mask bitwise XOR
and multiplication operations. It is also possible to mask together addition and
XOR [7], even if algebraically these operations are remote to each other.

Hamming Weight Attacks on Cryptographic Hardware
93
1.1
New Hamming Weight Attack
We present a Hamming weight attack on addition operation where one operand
is a subkey. In this way we derive eﬃciently most signiﬁcant bits of the subkey.
The attack has the following features:
– complexity of the attack depends mainly on the size of the words added, for
the ciphers with 16-bit operations and simple key schedule (like IDEA) the
attack reveals the whole main key; when key schedule is not reversible, then
the attack reduces key space signiﬁcantly,
– accuracy of measuring Hamming weight need not to be high, even poor
preciseness suﬃces,
– it is irrelevant when the addition operation is performed, the attack does not
use input and output data for the analysis,
– the data entering addition with the subkey may be masked, it does not
prevent the analysis; moreover, it even helps to since the data to be added
to the subkey should be fairly random: so in the situation when it is not
possible to encrypt a random input, masking helps to get useful side-channel
information,
– the attack computes directly subkey bits.
The side-channel information used for the attack is the Hamming weight of
the sequence of carry bits. Please note: apart from Hamming weight of inputs and
outputs, we also consider Hamming weight of internal data. If that weight can be
measured in some way (even with a quite large error), then we are done. It does
not disturb, if this weight is measured together with some other data like weight
of input and output of addition, of the subkey, or any of these quantities. For the
sake of clarity of exposition we describe the simplest case, in which addition is
done with the school method. However, it follows from our considerations that
the Hamming weight of a small number of operands existing in the addition
circuit may be treated as an error of measurement and thereby our attack works
in the same way.
Even if Hamming weight attacks seem at the moment to be more theoretical
than practical, one has to be very careful when implementing addition in cryp-
tographic devices. For instance, resetting registers holding the carry bits to zero
before addition might help to use the standard power analysis devices: in this
case power consumption due to putting the correct values of carry bits is closely
related to the number of bit values changed, that is to the Hamming weight of
the sequence of carry bits.
On top of that, although our attack is aimed to addition, it is probably not
impossible to mount similar attack on e.g. (modular) multiplication. Multiplica-
tion is of course much more complicated operation, and ﬁnding and proving the
closed formula similar to the one we prove for addiction seems very hard at the
moment. Nevertheless, it is another possible threat for the hardware implemen-
tation of several block ciphers.

94
Marcin Gomulkiewicz and Miroslaw Kutylowski
2
Properties of Addition
We consider addition of n-bits numbers modulo 2n with the carry bit for posi-
tion n thrown away. One of the operands will be the subkey to be derived, while
the other one a random n-bit number.
We analyse the Hamming weight of numbers that occur during addition. We
assume that addition is performed according to the school method. The eﬀects
speciﬁc to particular architectures of addition circuits are postponed to further
research.
2.1
Expected Value of Hamming Weight
We adopt the following notation convention: random variables are denoted with
capital letters, and their realisations with small letters. We make an exception
for K - an unknown but ﬁxed key.
First let us consider the total number of the ones that occur when we add
a ﬁxed key K to (chosen uniformly from {0, 1, . . ., 2n −1}) number a. Note that
we do not care about the value of a: we only want it to be chosen uniformly, and
we need not know either input a or output K + a. If internal data are masked
(as protection against power analysis), then uniform distribution is guaranteed
by masking. If the device does not use masking we should use randomly chosen
inputs (e.g. perform a chosen-plaintext attack).
Let |x| denote the Hamming weight of number x. We consider the ones that
occur while a is being added to (ﬁxed) K. These ones fall into four categories: the
ones occurring in K itself, the ones occurring in a, the ones occurring in K + a
and in the carry bits. Let c = c(K, a) denote the number of carry-bits with the
value 1. So the Hamming weight corresponding to adding a and K equals:
w = |K| + |a| + |K + a| + c(K, a) .
In terms of random variables we have:
W = |K| + |A| + |K + A| + C(K) ,
where C = C(K) is a random variable whose distribution depends on K alone:
C realises as c = c(K, a) which is a function of two arguments, one (K) ﬁxed, and
the other chosen at random (from a known distribution). Let E [ X ] denote the
expected value of X. Since a is chosen uniformly from the set of all n-bit strings,
we expect its Hamming weight to be close to
n
2 and E [ |A| ] =
n
2 . Similarly,
Hamming weight of K + a is supposed to be close to n
2 and E [ |K + A| ] = n
2 .
Since expected value of a sum is a sum of expected values (even for dependent
variables), we have:
E [ W ] = |K| + n
2 + n
2 + E [ C(K) ] = |K| + E [ C(K) ] + n .
E [ W ] depends on the unknown key K only, so we shall denote it as a function
of K: ϕ(K). Basically, an attack could look as follows:

Hamming Weight Attacks on Cryptographic Hardware
95
1. Collect a large pool of Hamming weight data corresponding to addition to
the unknown key K. Compute average value ϕ(K).
2. For any K′ compute the theoretical values of ϕ(K′) and compare with ϕ(K).
If |ϕ(K′) −ϕ(K)| is large, reject K′.
Of course, such an attack would be not very practical. The attack we present
is much simpler. We do not have to execute the second step: we derive certain bits
of K directly from ϕ(K). Obviously, the key point is to determine the relation
between C(K) and the (sub)key used.
2.2
Formula for Expected Hamming Weight of Carry Bits
In this subsection we prove the following key lemma.
Lemma 1. Let K = (kn−1, . . . , k0) be a n-bit string. Let C = C(K) denote the
number of carry bits set to 1 that occur during computation of K + a mod 2n
with the school method, where a is chosen uniformly at random from the set of
all n-bit strings. Then
E [ C(K) ] =
n−1

i=0
ki −21−n · K .
(1)
Proof. Let A = (an−1, . . . , a0). Let ci denote the ith carry bit generated while
adding K to A. Obviously, c0 = 0 (there is no carry at the beginning), and c1
depends only on k0 and a0 while for i > 0 the bit ci depends on ai−1, ki−1
and ci−1. This dependence complicates computation of E [ C(K) ].
In order to count the number of carry bits set to 1 we consider an−1, . . . , a0
as independent random variables with values 0, 1 and uniform distribution. Let
P [ X ] denote probability of an event X. For i = 1 we have:
P [ c1 = 0 ]
=
P [ k0 + a0 ≤1 ]
=
1 −1
2 · k0 ,
P [ c1 = 1 ]
=
P [ k0 + a0 > 1 ]
=
1
2 · k0 .
For i > 1 we may easily derive that
P [ ci = 0 | ci−1 = 0 ] =
P [ ci−1 + ki−1 + ai−1 ≤1 | ci−1 = 0 ]
=
P [ ki−1 + ai−1 ≤1 ] =
1 −1
2 · ki−1 ,
P [ ci = 0 | ci−1 = 1 ] =
P [ ci−1 + ki−1 + ai−1 ≤1 | ci−1 = 1 ]
=
P [ ki−1 + ai−1 ≤0 ] =
1
2 −1
2 · ki−1 ,
P [ ci = 1 | ci−1 = 0 ] =
P [ ci−1 + ki−1 + ai−1 > 1 | ci−1 = 0 ]
=
P [ ki−1 + ai−1 > 1 ] =
1
2 · ki−1 ,
P [ ci = 1 | ci−1 = 1 ] =
P [ ci−1 + ki−1 + ai−1 > 1 | ci−1 = 1 ]
=
P [ ki−1 + ai−1 > 0 ] =
1
2 + 1
2 · ki−1 .

96
Marcin Gomulkiewicz and Miroslaw Kutylowski
One may treat the random variables c1, c2, . . . as a non-homogeneous Markov
chain where transition probabilities depend on the values of k0, k1, . . . , kn−1.
For each i we consider vector Pi = [1 −pi, pi] where pi stands for P [ ci ] = 1.
Then
P1 = [1 −1
2 · k0 ,
1
2 · k0] ,
and for i > 1
Pi = Pi−1 ·
1 −1
2 · ki−1
1
2 · ki−1
1
2 −1
2 · ki−1
1
2 + 1
2 · ki−1

.
We get
1 −pi = (1 −pi−1) · (1 −1
2 · ki−1) + pi−1 · ( 1
2 −1
2 · ki−1)
= 1 −1
2 · (pi−1 + ki−1)
and so
pi = 1
2 · (pi−1 + ki−1) .
(2)
By equality (2) we get easily
pi = 1
2(ki−1 + 1
2 · (ki−2 + 1
2 · (. . . (k1 + 1
2k0) . . .))) .
(3)
Since ci is a random variable with values 0, 1, we have E [ ci ] = pi. Then, by
linearity of expectation,
E
 n−1
i=1 ci

=
n−1

i=1
pi
= 1
2 · k0 + 1
2 · (k1 + 1
2 · k0) + 1
2 · (k2 + 1
2 · (k1 + 1
2 · k0)) + . . .
+ 1
2 · (kn−2 + 1
2 · (. . .))
= k0
 1
2 + 1
4 + . . . +
1
2n−1
	
+ k1
 1
2 + 1
4 + . . . +
1
2n−2
	
+ . . .
+kn−3
 1
2 + 1
4
	
+ kn−2
 1
2
	
= k0

1 −
1
2n−1
	
+ . . . + kn−2

1 −1
2
	
+ kn−1 (1 −1) .
So we see that
E
 n−1

i=1
cj

=
n−1

i=0
ki −21−n · K .
This concludes the proof of Lemma 1.
⊓⊔
We note that the expected value E [ C(K) ] depends on K in a very nice way
– the bits of K inﬂuence separate bites of E [ C(K) ] (except the most signiﬁcant
part of E [ C(K) ]).

Hamming Weight Attacks on Cryptographic Hardware
97
2.3
Deviations from Expected Value on Random Inputs
The expected value ϕ(K) may be experimentally determined as a mean value
of a series of random experiments. Now it is crucial to determine how close this
approximation is.
Lemma 2. With probability at least 1 −n × 6.33 × 10−5, the observed value C
does not deviate from its expectation in N experiments more than 2n ·
√
N.
Proof. Now we consider N stochastically independent experiments in which
a random number is added modulo 2n to the same (sub)key K. In each ex-
periment some carry bits are set, e.g. like this (n = 16):
c15 c14 c13 c12 c11 c10 c9 c8 c7 c6 c5 c4 c3 c2 c1 c0
Exp. 1
1
0
0
0
1
0
0
0
0
1
1
1
0
1
0
0
Exp. 2
0
0
0
1
1
1
1
1
0
1
1
0
0
1
1
0
Exp. 3
1
1
0
0
0
1
0
1
0
1
0
0
1
0
0
0
. . .
Exp. N
0
0
0
1
1
1
0
0
0
0
1
1
0
0
1
0
(c0 = 0)
We shall think of columns separately, e.g. column i represents outcomes of
independent experiments which yield result 1 with (some) probability pi and 0
with probability 1 −pi. Variation of one such experiment is pi · (1 −pi). We
sum up the elements in the column, let a random variable Xi denote this sum.
Clearly, E [ Xi ] = N · pi. We shall estimate the probability of large deviations
from this expected value.
If pi ∈{0, 1}, then we always get the expected value. If pi /∈{0, 1}, by Central
Limit Theorem, Xi ∼N(µ, σ2), where µ = Npi and σ =

Npi(1 −pi), and
N(µ, σ2) denotes normal distribution with expectation µ and variation σ2. Since
the maximum of function p →p (1 −p) is in p = 1
2 and equals 1
4, we see that
σ ≤

N
4 . For a normal distribution the probability that observation deviates
from expectation by more than 4σ is about 6.33 · 10−5. Therefore with the
probability of at least 1 −6.33 · 10−5 :
E [ Xi ] −2
√
N ≤Xi ≤E [ Xi ] + 2
√
N
for every i ∈{1, 2, . . . , n −1}. By summing up n these inequalities we get
E [ C ] −2n
√
N ≤C ≤E [ C ] + 2n
√
N .
(since c0 = 0, we could sum (n −1) inequalities only; we sum n to simplify
further calculations a bit)
This event occurs with probability at least 1−n×6.33×10−5 (more precisely:
at least 1 −(n −1) × 6.33 × 10−5). This concludes the proof of Lemma 2.
⊓⊔
For n = 16 mentioned probability equals about 0.99898, for n = 32 about
0.99797.

98
Marcin Gomulkiewicz and Miroslaw Kutylowski
3
Deriving Subkeys
3.1
Ideal Case
As we have shown above, the expected value of Hamming weight in N experi-
ments equals
N · n + N · n
i=1 ki + N · (n
i=1 ki −21−nK)
= N · n + 2N · (n
i=1 ki −2−n · K) .
(4)
The values N and n are known, K = (kn−1 . . . k1k0) is sought. For a moment let
us assume that N = 2n, the Hamming weights are measured exactly, and each
input number occurs exactly once (just like in saturation attack). Let x be the
total Hamming weight measured. Then:
x = 2n · n + 2 · (2n
n

i=1
ki −K).
Since K < 2n,
n

i=1
ki =
x −2n · n
2n+1

(5)
and
K =

−x −2n · n
2

mod 2n .
(6)
As we can see, it would be possible to compute directly key K from x using
information gathered in our experiment.
3.2
Taking Deviations into Account
Of course, the likelihood of an ideal situation considered above is rather small.
We will get N · |K| ones (belonging to K), but we will not get exactly N · n/2
ones before and after addition, nor exactly N ·(n
i=1 ki −21−nK) carry bits. On
top of all, we have to consider measurement errors. Nevertheless, we shall see
that it is still possible to gather signiﬁcant information about the key.
Let us assume we want to ﬁnd some information about n-bit key and can
make N = 2m additions. We expect about 2m · n/2 ones in the input (and the
same amount of ones in the output) of the adding circuit. We shall think of
input bits as of 2m · n independent random variables (say, Xi) with uniform
distribution on {0, 1}. Then E [ Xi ] = 1
2 and Var[Xi] = 1
4. By Central Limit
Theorem we may say that:
n·2m
i=1 Xi ∼N(µ, σ2)
where µ = 2m · n
2 and σ2 =

1
4 · 2m · n
2
= 2m−2 · n.

Hamming Weight Attacks on Cryptographic Hardware
99
It is known that with probability close to 1 (that is at least 1−6.33·10−5) the
distance between the actual value and expectation of a normal random variable
is not greater than 4σ. In our case, 4σ = 4 ·
√
2m−2 · n

= 2m/2+1 · √n. The
same calculations are valid for the output ones.
In the previous subsection we have shown that the total number C of carry
bits falls into the interval

E [ C ] −2n · 2m/2 , E [ C ] + 2n · 2m/2
with the probability of at least 1 −n · 6.33 · 10−5.
Finally, there are errors due to measurement inaccuracies. We assume that
error εi in experiment i is a random variable with the values in the interval
[−u, u] uniformly distributed. Then E [ εi ] = 0 and Var [εi] = u2
3 . The values
of u depend on the equipment available, but for n = 16 it is sound to assume
for instance that u ≤2 (error of magnitude 12.5%). By Central Limit Theorem
−4 · 2m/2 · u
√
3 ≤
2m

i=1
εi ≤4 · 2m/2 · u
√
3
with the probability of at least 1 −6.33 · 10−5.
Together, with the probability of at least 1 −(n + 2) · 6.33 · 10−5 all kinds of
deviations from the expected value sum up to a value not exceeding
2 · 2m/2+1 · √n + 2n · 2m/2 + 4 · 2m/2 · u
√
3 = 2m/2+1

2√n + n + 2 · u
√
3

We have to compare the above value with the expected value of the Hamming
weight (4):
2m · n + 2m+1 · (
n

i=1
ki −2−n · K) .
In the last expression we are interested in bit positions m −1, m −2, . . . , m −n
representing the key K . On the other hand, the deviations from the expected
value and measurement errors inﬂuence the positions up to m/2+1+log(2√n+
n + 2 ·
u
√
3). For a very large m we can obviously read all key bits. However, we
have to balance the number of experiments N and gaining key bits. In the next
subsection we discuss some practically relevant settings of parameters.
3.3
Examples
Below we put concrete parameters for N, n and u and check how many key bits
we may gain from the experiment.
We describe separately three kinds of deviations from the expected value:
error 1– deviations from the expected Hamming weight of carry bits, error 2–

100
Marcin Gomulkiewicz and Miroslaw Kutylowski
deviations from the expected value of the total Hamming weight of the inputs and
outputs, error 3– deviations due to measurement errors. Total error denotes the
estimated value of the largest deviations that may occur. Signal level 2i means
that i is the ﬁrst bit position corresponding to the key K in the expression for the
expected value of the total Hamming weight (we mean that the least signiﬁcant
bit of a binary number has position 0).
The following table describes the situation for n = 16 (the subkey length is
like for IDEA) and u ≈22.5:
N
error 1
error 2
error 3
total
signal
key bits
level
level
level
error
level
found
216
∼213
∼212
< 29.2 · u
< 214
21
3
218
∼214
∼213
< 210.2 · u < 215
23
4
220
∼215
∼214
< 211.2 · u < 216
25
5
222
∼216
∼215
< 212.2 · u < 217
27
6
224
∼217
∼216
< 213.2 · u < 218
29
7
226
∼218
∼217
< 214.2 · u < 219
211
8
For n = 32 and u ≈23.5 we get:
N
error 1
error 2
error 3
total
signal
key bits
level
level
level
error
level
found
216
∼214
∼212.5
< 29.2 · u
< 215
2−15
2
220
∼216
∼214.5
< 211.2 · u < 217
2−11
4
224
∼218
∼216.5
< 213.2 · u < 219
2−7
6
228
∼220
∼218.5
< 215.2 · u < 221
2−3
8
232
∼222
∼220.5
< 217.2 · u < 223
21
10
236
∼224
∼222.5
< 219.2 · u < 225
25
12
240
∼226
∼224.5
< 221.2 · u < 227
29
14
244
∼228
∼226.5
< 223.2 · u < 229
213
16
It is quite astonishing that measurement accuracy u might be quite poor,
but we are still able to ﬁnd a signiﬁcant number of key bits.
4
Vulnerability of Popular Algorithms
4.1
IDEA
IDEA encryption algorithm uses 18 subkeys of length 16 for addition. The key
schedule is so simple that the subkey bits are directly the bits of the main key.
If our aim is to break completely the main key, we should ﬁnd at least 4
(preferably 5) bits per subkey – that would yield 72 or 90 bits of the main key.
Given that we could ﬁnd the remaining bits of the main key by an exhaustive
search (at most 256 or 238 trials). By inspecting the tables in the previous section
we see that we need about 220 experiments per subkey, and the error in measuring
Hamming weight should not exceed some 12%. Note that subkey bits are exactly
the bits of the main key.

Hamming Weight Attacks on Cryptographic Hardware
101
4.2
Twoﬁsh
Twoﬁsh uses two 32-bit subkeys in each of its 16 rounds for addition. The key
schedule, although not as simple as in IDEA, is still reversible. It can be easily
shown that to ﬁnd the main key it suﬃces:
128-bit key: 4 subkeys used for addition during the ﬁrst two rounds and about
8 · 28 very simple operations,
192-bit key: 6 subkeys used for addition during the ﬁrst three rounds and
about 8 · 216 very simple operations,
256-bit key: 8 subkeys used for addition during the ﬁrst four rounds and about
8 · 224 very simple operations.
As we know, ﬁnding more than 16 bit per 32-bit long subkey is rather in-
feasible; however, for breaking 128-bit main key we do not require more. We
ﬁnd 16 most important bits of 4 keys, and guess the rest. Then we reverse the
key schedule, and test the keys. Of course, we should begin our guessing from
value suggested by the procedure: if we have measured xi, 1 ≤i ≤4 ones dur-
ing 2m additions, then we should start with Ki =

−xi−2m·32
2

mod 232 (see
equation 5), and then increase and decrease guessed keys by 1; also note, that
expression

xi−2m·32
2m+1

(see equation 6) gives us an approximation of Ki’s Ham-
ming weight; keys with Hamming weights ”much” diﬀerent than those may be
checked later. In worst case we will have to check 264 possibilities. Since an ex-
haustive search of 264 queries is possible, we are able to recover the whole main
key. The total number of operations required is in the range of 264. This is not
few, but compared with the claimed 128-bit security it is very attractive.
Complete breaking 192 and 256-bit keys are not practically possible: on aver-
age it would require 295 or 2127 attempts (note, that reversing of the key schedule
is also more tedious). However, it is still a great improvement over the exhaustive
search in the set of 2192 or 2256 keys.
4.3
MARS
MARS encryption algorithm [1] uses several 32-bit long subkeys for addition:
in fact key addition is the ﬁrst, and key subtraction (which may be also imple-
mented as addition) is the last operation performed on data. Apart from that,
another subkey addition takes place inside the E function used in each of 16
rounds of the keyed transformation. Summarily we have 4 + 16 to 4 + 16 + 4
subkeys used for addition. As shown above, at a cost of about 244 encryptions
we may ﬁnd half of each subkey, i.e. 320 to 384 bits of total 1280 bits of an
expanded key. Due to its complex structure, the key-schedule does not seem to
be reversible, so an attack would require to execute key expansion scheme and
falsify wrong key guesses. Although it does not seem very practical, it is still an
improvement over the exhaustive search.

102
Marcin Gomulkiewicz and Miroslaw Kutylowski
4.4
RC5 and RC6
RC5 and RC6 use addition as one of their building blocks. RC5 uses 2 + 2r sub-
keys for addition: two at the beginning and two more per each round. RC6 works
similarly, on top of that it uses another two subkeys at the end of computation.
Case of RC5 and RC6 is in a way similar to MARS: since the key expansion
scheme seems to be irreversible, our attack is rather a potential than an actual
threat. It is worth noting, though, that there are no other subkeys than those
used for addition. This property would theoretically allow us to break the ci-
pher by ﬁnding all the subkeys. Let us assume that we have equipment of great
(preferably: indeﬁnite) accuracy, so we can measure (almost) exact Hamming
weights (as in section Ideal case). Therefore we start with a saturated set of
plaintexts and ﬁnd the keys one by one from the ﬁrst round to the last (we
peel oﬀconsecutive rounds starting from the beginning; since we need to recover
2 + 2r keys, we want to minimize all the errors as much as possible). In that
case, even though we still do not know the main key, we are able to duplicate
the encrypting device or decrypt encrypted messages. Of course, such a theo-
retical attack would not be possible against Twoﬁsh which uses key-dependent
permutation, or MARS which uses another subkeys for operations diﬀerent from
addition.
5
Conclusions
Our attack reveals that addition should be used with special care in hardware
implementations of symmetric block ciphers. Potentially, a side channel attack
may retrieve completely the secret key stored in a protected device, or at least
provide signiﬁcant amount of information on this key.
It also indicates that masking with random values the inputs of the arithmetic
operations is not a universal technique that may be used as a defense against side
channel attack on protected cryptographic hardware storing secret symmetric
keys. Unexpectedly, masking may even help to perform the attack.
The attack presented is one more argument that the subkey schedules of
symmetric ciphers should evolve into irreversible schemes.
References
[1] Burwick C., Coppersmith D., D’Avignon E., Gennaro R., Halevi S., Jutla C.,
Matyas S., O’Connor L., Peyravian M., Saﬀord D., Zunic N., MARS — A Can-
didate Cipher for AES, http://www.research.ibm.com/security/mars.html.
101
[2] Chari S., Jutla Ch., Rao J. R., Rohatgi P., A Cautionary Note Regarding Evalua-
tion of AES Candidates on Smart-Cards, Second Advanced Encryption Standard
(AES) Candidate Conference.
92
[3] Chari S., Jutla Ch., Rao J. R., Rohatgi P., Towards sound approaches to coun-
teract power-analysis attacks, CRYPTO’99, Lecture Notes in Computer Science
1666. Springer-Verlag, 398-412.
92

Hamming Weight Attacks on Cryptographic Hardware
103
[4] Coron J. S., On Boolean and arithmetic masking against diﬀerential power analy-
sis, CHES’2000, Lecture Notes in Computer Science 1965. Springer-Verlag, 231–
237.
92
[5] Daemen J., Knudsen L., Rijmen V., The block cipher Square, Fast Software
Encryption’97, Lecture Notes in Computer Science 1267. Springer-Verlag, 149–
165.
92
[6] Daemen J., Rijmen V., The block cipher Rijndael,
http://www.esat.kuleuven.ac.be/~rijmen/rijndael.
92
[7] Goubin L., A sound method for switching between Boolean and arithmetic mask-
ing, CHES’2001, Lecture Notes in Computer Science 2162. Springer-Verlag, 3–15.
92
[8] GandolﬁK., Mourtel Ch., Olivier F., Electromagnetic Analysis: Concrete Re-
sults, CHES’2001, Lecture Notes in Computer Science 2162. Springer-Verlag,
251–261.
91
[9] Goubin L., Patarin J., DES and Diﬀerential Power Analysis (The ”Duplication”
Method), CHES’99, Lecture Notes in Computer Science 1717. Springer-Verlag,
158-172.
92
[10] Kesley J., Schneier B., Wagner D., Hall Ch., Side channel cryptanalysis of prod-
uct ciphers, Journal on Computer Security 8 (2000), 141–158.
91
[11] Kocher P., Timing Attacks on Implementations of Diﬃe-Hellman, RSA, DSS,
and Other Systems. CRYPTO’96, Lecture Notes in Computer Science 1109.
Springer-Verlag, 104-113.
91
[12] Kocher
P.,
Jaﬀe
J.,
Jun
B.,
Diﬀerential
power
analysis,
CRYPTO’99,
Lecture
Notes
in
Computer
Science
1666.
Springer-Verlag,
388–397,
also:
Introduction
to
diﬀerential
power
analysis
and
related
attacks,
http://www.cryptography.com/dpa/technical.
91
[13] Lucks S., The saturation attack - a bait for Twoﬁsh,
http://eprint.iacr.org/2000/046/.
92
[14] Messerges Th., Securing AES ﬁnalists against power analysis attack, FSE’2000,
Lecture Notes in Computer Science 1978. Springer-Verlag, 150–164.
92
[15] Rivest R., Robshaw M., Sidney R., The RC6 Block Cipher,
http://theory.lcs.mit.edu/~rivest/rc6.ps.
[16] Schneier B., Kesley J., Whiting D., Wagner D., Ch. Hall, N.Ferguson, The
Twoﬁsh Encryption Algorithm: a 128-Bit Block Cipher, Wiley, 1999, ISBN 0-
471-35381-7.
[17] AES Development Eﬀort, NIST, http://www.nist.gov/aes.
92

A Fully Compliant Research Implementation
of the P3P Standard for Privacy Protection:
Experiences and Recommendations
Giles Hogben, Tom Jackson, and Marc Wilikens
Institute for the Security and Protection of the Citizen
Joint Research Centre of the EC⋆⋆
Marc.Wilikens@jrc.it
Abstract. This paper describes experiences gained from development
of a fully compliant implementation of the W3C’s XML based P3P stan-
dard. P3P aims to make privacy policies of web sites transparent for
automated agents, and thereby to improve transactions of personal data
on the Internet. We look at some of the most important issues that have
arisen from our development work, including problems with the privacy
preference standard, APPEL, before concentrating on issues related to
end user assurance. We look at P3P usage scenarios to show that the
current P3P standard has weaknesses in this area. The paper then con-
siders possible extensions to P3P, which could provide greater assurance
to end users and facilitate dispute resolution. In particular, we present an
overview of a way for increasing assurance of a privacy policy’s validity
using signed XML.
Keywords: privacy enhancing technologies, P3P, XML Digital Signa-
tures, secure electronic commerce, transaction management, security ver-
iﬁcation
1
Introduction
There have been several optimistic projections of rapid growth in e-commerce
however the business-to-consumer (B2C) sector has failed to realise its expected
growth potential. Surveys show that lack of information about privacy practices,
and lack of trust in third party management of personal data are some of the
most signiﬁcant obstacles to the growth of e-commerce on the Internet. A survey
performed by Consumers International [8] revealed that a third of investigated
web sites made no mention of their privacy practices and the quality of actual
practices where they existed had not been audited. A Harris poll [1] showed that
when asked speciﬁcally about their online privacy concerns, respondents said
they were most worried about web sites providing their personal data to others
⋆⋆The views expressed in this paper are the authors’ own, and may not be taken in
any way as representative of those of the European Commission.
D. Gollmann et al. (Eds.): ESORICS 2002, LNCS 2502, pp. 104–125, 2002.
c
⃝Springer-Verlag Berlin Heidelberg 2002

A Fully Compliant Research Implementation of the P3P Standard
105
without their knowledge (64 percent) and web sites collecting information about
them without their knowledge (59 percent).
The P3P standard has been proposed and developed by the World Wide Web
Consortium (W3C) in response to the growing technological need to manage the
privacy concerns of consumers engaging in on-line commerce. This paper will de-
scribe the experience gained in the development of a P3P implementation that
is fully compliant with the standard, identify some of the outstanding problems,
and outline recommendations for addressing them. It should be stressed that
P3P oﬀers only one (albeit important) technical solution in a wider spectrum of
privacy enhancing technologies and security technologies that can be applied by
IT systems for managing on-line privacy issues. In particular it provides a frame-
work, which allows users and, importantly, automated agents on behalf of users
to assess the terms and conditions of a web site’s Privacy Policy via unam-
biguous machine readable statements which explicitly declare a the web site’s
data handling practices for personal data. These statements may be processed
automatically, but visibly, before any personal data is sent to a remote web site.
After a brief introduction to P3P, we describe the JRC P3P reference imple-
mentation in section 2. Section 3 reviews some of the ﬁndings deriving form the
implementation. Section 4 describes how some important questions of end user
trust remain inadequately addressed by the current P3P speciﬁcation. For exam-
ple, the privacy statement of a web site might state that they will not contravene
certain privacy principles (such as disclosure to third parties) but currently P3P
does not contribute to the process of making this an agreement that can be
enforced by providing non-repudiability. We claim that the P3P process has the
potential to deliver far more in terms of a legally binding statement.
1.1
Information Privacy
An important concept in the context of online privacy protection and P3P is in-
formational self-determination; i.e. empowering the user to assert his/her rights
in the use of personal data [11]. According to EU data protection legislation, data
are personal data when they deal with any information relating to an identiﬁable
natural person. To determine whether a person is identiﬁable is subject to inter-
pretation and therefore the technical deﬁnition of personal data is in continuous
ﬂux, not least because in the emerging Information Society an increasingly wide
range of human characteristics (demographics, behaviour, biological features,
etc.) will ﬁnd themselves reﬂected in some kind of electronic equivalent. For the
purpose of web site interactions, personal data can be broadly categorised in
terms of the following categories: Communications traﬃc data indicating the
source and destination of a communication (e.g. http headers, clickstream data,
cookies, user password, etc); demographic data (e.g. name, address, etc.); con-
textual data, i.e. additional data needed for processing a particular transaction
(e.g. credit card, income, health status, location, etc.).

106
Giles Hogben et al.
1.2
Platform for Privacy Preferences – Brief Introduction
The W3C speciﬁcation document gives the following description of P3P [3]: “The
Platform for Privacy Preferences Project (P3P) enables Web sites to express
their privacy practices in a standard format that can be retrieved automatically
and interpreted easily by user agents. P3P user agents will allow users to be
informed of site practices (in both machine- and human-readable formats) and
to automate decision-making based on these practices when appropriate. Thus
users need not read the privacy policies at every site they visit.” The aims of
P3P can be summarised as follows:
• Transparency: To increase trust in data transactions on the Internet by pro-
viding statements of Data Protection policies.
• Automation: To facilitate data ﬂow by making statements machine readable.
• Dispute resolution: To facilitate this by providing unambiguous statements.
A schematic overview of P3P is given in ﬁgure 1. In words:
• A web service/site (1.) (e.g. e-commerce shop) deﬁnes its Privacy Policy
that is then translated into a P3P XML version often using an oﬀ-the-shelf
tool. The privacy policy speciﬁes a web site’s data processing practices of
the personal data of its customers. Typical issues speciﬁed are length of
data retention, disclosure to unrelated third parties and user access to their
personal data.
• The user deﬁnes their Privacy user preferences (4.) which are translated into
a P3P XML version possibly using a graphical tool such as the JRC ruleset
editor1. This typically speciﬁes a set of data processing characteristics to
look for in P3P policies and behaviors to execute if they are matched. The
characteristics that the rules look for can be any of those possible within
P3P e.g. user access rights, data retention, etc.
• For any http request by the user, before data is transmitted, the agent fetches
the P3P policy (3.) and evaluates it against the user’s preference set.
• Depending on the evaluation result, data such as http header info or demo-
graphic data (6.) may be released to the web site’s data store and/or the
client may be given information on evaluation results. Note that current op-
tions in P3P only specify that requests SHOULD be blocked, executed or
limited. There is no interaction with a user data store or granular information
ﬁltering.
2
Description of the P3P Reference Implementation
2.1
Objectives
The JRC has developed a complete implementation1 of the P3P standard for the
purposes of developing research extensions to P3P, as an educational platform
1 See the JRC P3P resource center http://p3p.jrc.it

A Fully Compliant Research Implementation of the P3P Standard
107
Fig. 1. Schematic overview of the P3P Process
and as a way of investigating the level of the standard’s compliance with EU
Personal Data Protection legislation. This implementation was the ﬁrst to be
fully compliant to the standard, and was adopted by the W3C2 as a reference
model to demonstrate the maturity of the standard. A demonstration site and
evaluation environment are available to provide a broad range of assessment and
familiarisation functions.
2.2
Architecture
The implementation is written in Java 1.2 with a proxy architecture, ensuring
compatibility with most browsers. A modular architecture allows easy exper-
imentation with diﬀerent components. For example the rule based evaluation
system is completely decoupled from the proxy handler.
1. APPEL preference matching component: The intelligent agent which per-
forms matches between the web site P3P privacy policy and the user’s AP-
PEL [4] privacy preferences. APPEL allows a user to express his/her privacy
preferences in a set of XML rules (called a ruleset), which can then be used
by a user agent to make automated or semi-automated decisions regard-
ing the acceptability of machine-readable privacy policies from P3P enabled
web sites. Only one rule is permitted to ﬁre in any set, thus strict ordering
is crucial. This is a drawback in our view because although there may be
many reasons for a policy to be rejected within one ruleset, only one is given
to the user. It does however make for faster performance and obviates the
possibility of conﬂicting behaviors.
2 See the W3C web site: http://www.w3c.org/p3p

108
Giles Hogben et al.
2. Preference creation component (ruleset creation): A tool allowing users to
translate their privacy preferences into XML format.
3. Syntax validation component: Performs numerous syntax checks on the Pri-
vacy Policy and the privacy preference ruleset.
4. Proxy component: Mediates between client and server, ﬁltering content and
providing feedback to the user, according to P3P. Also handles the diﬀerent
users accessing the machine simultaneously so that content is sent back only
to the user who requested it. Manages user proﬁles, each with a username,
password, preferences and history. This is thus a simple identity management
system.
5. Policy discovery component. Looks for Policy Reference Files (PRF’s) (ﬁles
specifying which policies apply to which resources) and decides which P3P
policy applies to which resource.
The set up also contains a mock-up of a typical P3P enabled commercial web
site to demonstrate a realistic process and end user interactions.
3
Main Findings Deriving from the P3P Implementation
3.1
User Interface and Performance Issues
Perhaps the most interesting part of the development eﬀort was in trying to
build an interface through which users can easily set up their privacy prefer-
ences. P3P can express various aspects of human readable privacy policies in an
unambiguous machine readable XML format. Building on this, APPEL is able to
create rules, which will match any aspect of P3P’s granular structure. Therefore
in setting up an interface for creating APPEL rules, we were forced to present
a very large number of possibilities to the user in order to allow them to match
any aspect of a P3P policy. In terms of ECA (Event, Condition, Action) rule
theory the problem is that the rule condition – is very complex.
Fig. 2. Architecture components of the JRC P3P

A Fully Compliant Research Implementation of the P3P Standard
109
Fig. 3. Simple and Advanced rule creation interfaces
User Interface Issues with APPEL A quick survey shows that existing
commercial implementations have not implemented full versions of APPEL. One
of the possible reasons for this is that the possibility space provided by APPEL
is too large for any application to be able to oﬀer an easy to use interface for
creating privacy rules. For example it is possible to state for any one of over 100
data types whether their collection is optional or not. To give the ordinary user
the ability to control the optionality of every one of the data types would present
him with an impossibly complex array of choices. However for experts having
to use APPEL in a more professional capacity such as regulators, the ability to
control such choices is vital.
Any user interface therefore needs to be ﬁrmly divided between the expert
and the general user. After creating our Rule creation GUI, we were conﬁdent
that we had an interface of use to informed experts. We also found that it was
possible to create an interface for general users based on selecting from a set
of predeﬁned rules (e.g. “block all cookies which link to personal information”).
The main obstacle we needed to overcome was making sure that the complexity
of full rule creation was not presented to ordinary users, whilst still allowing it as
a possibility for advanced users. User testing showed that it was very important
to separate the condition and action parts of a rule for it to make sense to users.
Figure 3 shows the two sections of the GUI, one based on choosing between
a simple selection of predeﬁned rules (aimed at the general user), the other
oﬀering complete control over all aspects of a privacy rule (aimed at expert
users). The second interface also separates the choices clearly into those to do
with the condition and those to do with the action.
Complexity of Algorithm Another possible reason for the lack of full imple-
mentations of APPEL is that in order to provide a completely general matching

110
Giles Hogben et al.
algorithm capable of dealing with any possible privacy preference within the
semantic space provided by P3P, a complex sub-tree matching algorithm is re-
quired. This is costly to implement in terms of development time and diﬃcult
to make eﬃcient.
At ﬁrst sight, the APPEL algorithm appears to be very heavy because it is
highly recursive. This may have put oﬀmany implementers. However, because
P3P policies never reach more than 5 levels in depth and generally only extend
to 4, this recursivity does not produce excessive load. We found that with a full
policy validation in our Java implementation, the APPEL evaluation process on
a standard PC can take of the order of 1 second for a real world ruleset and
policy. However, timing tests revealed that this was more due to ineﬃciencies
in document parsing than in excessive load created by the algorithm. The ac-
tual matching process only took typically 20 milliseconds. We would suggest
therefore that implementers should not be put oﬀusing APPEL because of its
apparent complexity. Already some implementers have been successfully using
our evaluator which can be used as a stand alone component.
Performance in P3P User Agents The fact that most implementations of
P3P user agents (most notably Microsoft’s IE 6) have chosen to implement a cut
down version of P3P, and in general policies are processed after resources have
been loaded, highlights performance problems faced by implementers of P3P.
The actions of a P3P user agent for a typical page can involve 6 extra http calls
for PRF’s, policies for embedded resources etc.
W3C’s proposed solution to this has been the P3P compact policy. This is
an abbreviated version of the P3P policy contained in the http header, which
allows a much quicker evaluation time. However these only apply to cookies and
are therefore by no means a complete solution.
One solution is to capitalize on the fact that many companies are tending
to use standardized (“catch-all¨o) privacy policies for reasons of legal security
and to save work in creating policies. For example, on Yahoo.com, the policy,
which covers the majority of pages within their domain states that information
collected may be used for opt out telemarketing. But on most pages within
Yahoo, no personal data is actually collected. This reﬂects the fact that it is
easier for companies to create a “lowest common denominator” policy, which
will cover anything that might possibly happen on their site, than to create
policies tailored to individual pages.
If this trend develops, it may be that companies will use standardized privacy
policies kept by third parties at URI’s, which may be well known to a user agent,
in the same way as data documents can use standard XML schemas. This would
allow a major performance enhancement because clients could then store a list of
trusted policy URI’s and simply match against these rather than having to fetch
a policy and do complex sub-tree matching. A related solution to the problem
would be possible if the use of trusted seals became widespread. P3P, in its
disputes resolution tag, allows policies to specify a trusted seal. Were these to

A Fully Compliant Research Implementation of the P3P Standard
111
be widely adopted, as seems likely, it would again reduce the policy matching
load to a simple string match.
Other possible avenues for performance enhancement might include the ex-
tension of compact policies to cover all resources, and the use of binary XML.
3.2
APPEL Speciﬁcation Issues
Ambiguity of Logic Inference System At present it is possible to write two
semantically equivalent P3P policies where one will be blocked by a given rule
and the other will not. This is clearly unacceptable. For example, the following
rule looks for any information which is not the user’s IP address or user agent
string and blocks resources which ask for it.
<appel:RULE behavior="block">
<p3p:POLICY>
<p3p:STATEMENT><p3p:DATA-GROUP appel:connective="non-and">
<p3p:DATA ref="#dynamic.clickstream.clientip.fullip"/>
<p3p:DATA ref="#dynamic.http.useragent"/>
</p3p:DATA-GROUP></p3p:STATEMENT>
</p3p:POLICY>
</appel:RULE>
This RULE will cause a block behavior for the following web site policy (only
relevant parts quoted),
<POLICY>
<STATEMENT><DATA-GROUP appel:connective="and">
<DATA ref="#dynamic.clickstream.clientip.fullip"/>
<DATA ref="#dynamic.http.useragent"/>
</DATA-GROUP></STATEMENT>
</POLICY>
but not for this one
<POLICY>
<STATEMENT><DATA-GROUP>
<DATA ref="#dynamic.clickstream.clientip.fullip"/>
</DATA-GROUP></STATEMENT>
<STATEMENT><DATA-GROUP>
<DATA ref="#dynamic.http.useragent"/>
</DATA-GROUP></STATEMENT>
</POLICY>
Note the presence of the “non-and” connective, which means “only if not all
sub-elements in the rule are present in the sub-elements of the matched element
in the policy”. This is true for the ﬁrst policy snippet but not the second, which
given that they have the same meaning is clearly unacceptable. We will look at
solutions which address this problem below.

112
Giles Hogben et al.
Expressive Inadequacy of Logical Connectives The problem of ambiguity
outlined above is just one example of the inadequacy of the logical system con-
tained within APPEL. One of our lines of research will shortly be in bi-lateral
negotiation systems. This will require a much more sophisticated capability in
terms of logic and inference.
For instance we would like to be able to express the following: If the privacy
policy says it will retain my information “indeﬁnitely”, try changing this to “for
the stated purpose only” and send the policy back to see if the server’s logic engine
will agree to this in exchange for the same services. At present, APPEL contains
only the ability to match arbitrary sub-trees beginning at the POLICY’s root
node, and is not well suited for such inferences.
Proposed Solutions to Problems with APPEL Any rule system able to
match the full range of possibilities within P3P will by always be as complex as
P3P. So we do not believe simplifying APPEL is an answer. Instead, we suggest
the following lines of research:
1. As a quick ﬁx, use a standard query language for the condition matching
part of APPEL. Instead of using the somewhat quirky APPEL connective system
and recursive matching algorithm the rule condition could be speciﬁed by an
XPATH [12] query. These query languages are designed to match arbitrary node
sets with high eﬃciency. They have the advantage that developers are familiar
with them and eﬃcient algorithms exist to execute the queries. As it has become
very clear that APPEL is not a language that will be written by anyone other
than developers or ordinary users using a GUI, this is clearly the best approach.
E.g. a rule in this format, which would solve the above ambiguity problem
would be:
<appel:RULE behavior="block" prompt="yes" promptmsg="Rule found policy
using your home info beyond current purpose ">
<appel:MATCHQUERY query=
"//DATA[not(substring(@ref,’ dynamic.clickstream.clientip.fullip’)
or substring(@ref,’ dynamic.http.useragent’))]"
querylanguage="XPATH">
</appel:RULE>
2. Explore moving APPEL into an Resource Description Framework
(RDF) [2] based system, which would relate rules to a Darpa Agent Markup
Language (DAML) [7] ontology (a speciﬁc privacy ontology would have to be
developed in this case.). We cite the following reasons for this:
• Semantic Web technology, as its name suggests, supports query and matching
systems based on an underlying semantics. In other words queries designed
to match underlying meaning and not syntactical patterns. It is therefore
unlikely that the ambiguity problems outlined above would occur.
• An RDF schema already exists for P3P and several members of the P3P
working group have suggested that P3P 2.0 should use RDF/Semantic Web
syntax.

A Fully Compliant Research Implementation of the P3P Standard
113
• Because it is based on subject, predicate object structures, RDF is well suited
to expressing natural language statements such as “site x will retain your
information indeﬁnitely” and using them in intelligent inference engines like
rule matchers.
• The RDF community is already rich with Rule matching systems like
ruleML [9] which have a much wider development community than APPEL.
3.3
P3P and EU Data Protection Legal Compliance Issues
One of the most pressing questions for P3P in the context of the on-line business
sector is the degree to which the P3P standard is able to support or comply with
the diﬀerent data protection legislation and regimes. P3P should be considered
as a toolbox that can be conﬁgured to speciﬁc data protection regimes and as
such the coding of the level of protection is up to the implementation. From
a EU perspective, there are three important issues in this regard:
1. Is P3P able to provide support for e-commerce providers who wish to im-
plement personal data handling practices in an EU compliant way?
2. Can P3P permit end users to express their privacy preferences as a statement
of their rights under EU legislation?
3. How will P3P operate in trans-border environments where the privacy legis-
lation may change to reﬂect member state individual interpretations of the
EU directives or across global borders outside of the jurisdiction of the EU
directives?
The issue of EU compliance for e-commerce providers is complex. The EU
Directives set out principles of transparency for data collection, which include
concepts such as purpose speciﬁcation, collection minimization and purpose lim-
itation. The EU directives do not make any speciﬁc reference to the role of
Privacy Policies, which of course is the focus of the P3P standard. Clearly, Pri-
vacy Policies can contribute to the task of informing users of the data processing
practices of a web site, and of deﬁning the purposes of data collection and sub-
sequent usage. To this extent, by automating the exchange of privacy policies,
P3P can support a web site in specifying its practices to be EU compliant. How-
ever, there are some key pragmatic issues that are not yet clear in regard to
P3P’s compliance. The directives state that information as to purpose of data
collection and usage should be declared at the point at which data is collected,
so that end users are explicitly informed at the moment of data submission.
P3P does not, by default, operate in this mode. There is no explicit statement,
rather it is implicit in the agreement reached by the APPEL parser during the
exchange of the privacy policy and a user’s ruleset. As such, privacy policies do
not go far enough in meeting the requirements of the EU directives, unless they
are expressed and exhibited explicitly at the points of data collection. There is
nothing in principle to stop P3P implementations doing this, but it is not the
currently envisaged model for its deployment.
The second issue is the expression of a user’s preferences in a way that re-
ﬂects their rights granted under EU data protection legislation. In this respect,

114
Giles Hogben et al.
P3P can play a useful role, and in the course of our development work, we have
produced a prototype Ruleset that reﬂects most of the principles of the EU di-
rectives. The APPEL language used to create the Rulesets is suﬃciently ﬂexible
and expressive to facilitate this. Although it would be beyond the capabilities of
most consumer users to deﬁne such a Ruleset, there is nothing to prevent them
downloading a default EU Ruleset from a local agency, such as the member state
Data Protection Agencies. Our EU ruleset was produced and edited using the
APPEL Ruleset editor tool developed within the project. With the EU Ruleset
conﬁgured for the P3P proxy server, we visited a large number of the websites
that are currently advertised as P3P compliant. We observed that almost every
site failed to comply with the preferences expressed in the Ruleset. The principle
reason for this was that they did not allow user access to all data collected, as
stipulated in the directive. Whilst this may initially be seen as a very negative
result, it does have some positive implications. What it shows is that by virtue
of its unambiguous transparency, P3P will bring such issues much more out into
the open. In order to compete in an internet environment where P3P is used,
companies will be forced to state unambiguously what their practices are, and
end-users, whether consumers or Data Protection Auditors will be able to see
almost instantly those whose privacy policies are not conforming to the EU Data
Protection Legislation and why.
The third issue of global cross border browsing is not so much a technology
issue as a policy issue. If users wish to express their privacy preference in terms
of the EU directives it is to be expected that they will not ﬁnd sites that are
compliant outside of the EU. End users need to be aware of this fact, and should
be clearly educated that their privacy rights may vary signiﬁcantly if they wish
to browse or participate in e-commerce outside of the EU. However, government
bodies such as the EC may wish to specify minimum levels to which a particular
P3P implementation should be conﬁgured. Finally, it is worth noting that P3P
can in no way oﬀer any enforcement of privacy statements. Whether or not
statements actually reﬂect practice is not an issue the protocol can hope to
address. Vice versa, if a resource has no P3P policy, it also does not mean that
it does not comply with European law. So in the respect of law enforcement,
P3P can never be more than an aid to transparency.
4
Current Limitations and Outlook Concerning End User
Assurance
With the increasing prominence of the P3P standard, the number of P3P poli-
cies is increasing to the point where they may become a standard feature of
commercial web sites. However, there are still outstanding problems related to
assuring the integrity and non-repudiation of these policies.
4.1
P3P Examples of Risk Scenarios
In order to clarify these problems, we provide three scenarios to describe how the
P3P process might be abused to the detriment of both end users and businesses:

A Fully Compliant Research Implementation of the P3P Standard
115
Scenario 1. I receive a large amount of unsolicited email from various sources
unknown to me, despite the fact that I only ever give my email address away
to sites whose P3P policies promise only to use my email address to carry out
the stated purpose of the transaction. I seek to take legal action using the P3P
policy, which was available on one of the oﬀending sites at the time I made the
transaction, as evidence. The corporation I am suing denies that it published this
policy at the time I sent my details. I have no way of proving what Privacy Policy
was published at the time and as a consequence my legal action is not successful.
Furthermore, if I visit such a site, I am put oﬀusing the service because I know
that I cannot ever prove that the company made the statements on its P3P policy.
This example highlights up two speciﬁc repudiation issues in regard to the
deployment of P3P:
1. If a user cannot prove that a company published a speciﬁc privacy statement,
then a privacy policy is of limited beneﬁt to him/her in pursuing a legal
action;
2. If a user cannot prove the source of a loss of privacy then any number of
assurances are of limited beneﬁt to him/her.
3. Even if legal action is not actually taken, the fact that any action would be
unlikely to stand up in court is enough to put many people oﬀusing a web
service.
Scenario 2. I operate a web site and collect sensitive personal information which
I use for marketing purposes. In order to comply with the European Directive, I
ensure that the user gives his/her consent for this. After some years, I receive
a court summons from a user saying that I used their information without con-
sent. As I have no signed agreement from the user, I have no way of proving
that this was not the case.
This example shows
1. The importance of the issue of legal consent in this arena. As consent is
such an important component of compliance to the EU directives, it seems
important to have some means of proving that it was given.
2. That non repudiatable assurances may not only be useful to end users but
also to commercial organisations. If one considers that P3P policies are sub-
ject to security problems and that court cases may be brought on their basis,
it also seems likely that there might be demand from the corporate side for
P3P policies, which can provide watertight evidence.
Scenario 3. A possible extension to P3P is to include bi-lateral negotiation,
which allows the end user to negotiate data processing practices (and changes
to the P3P policy) for services or goods. This is also mentioned in the current
W3C speciﬁcation [4] as a possible route for extending the protocol. A possible
speciﬁcation for this type of extension is treated in detail by R.H. Thibadeau [6].
One scenario, which arises from this inclusion of bi-lateral negotiation is the
following:

116
Giles Hogben et al.
Private individual x’s computer negotiates with server Entity y, according to
the proposed P3P negotiation protocol. Private individual x agrees to give away
his email address if and only if server y assures him that it will not be given
to third parties. Later Private individual x discovers that Server Entity y has in
fact given his information away to a third party. Server Entity y claims that he
did not make such an agreement and an unresolvable dispute follows.
This example shows that policies which need to be assured may not always
be ﬁxed documents, but may be produced dynamically as part of a bilateral
negotiation process, hence requiring more care in the consideration of loading
on servers.
4.2
Extensions to the P3P Speciﬁcation to Deal with Assurance
Issues
In the following sections we discuss extensions to the P3P standard, which could
provide it with more enforcement rigour, and consequently provide greater as-
surance.
It should be noted however, that any modiﬁcations to the P3P standard need
to take into account the constraints imposed by the computing environment of
end-users. This often includes a low bandwidth network environment and low-to-
mid range PC platforms, particularly in the European context. They also need
to be accommodate the fact that although end-users are keen to protect their
privacy, they are also unwilling to accept computational overheads that might
interrupt the ﬂow of their browsing experience.
Non-repudiable Privacy Policies via P3P A principle problem for P3P is
that if a company’s practices contravene its stated privacy policy, there is little
technical framework to prove that a company made the statements which may
have existed on its server at a given time. I.e. it is too easy for a company to
repudiate its policy.
While P3P does increase the level of trust felt by consumers by providing
more transparent and unambiguous information, it does not however provide
any assurance as to the authenticity and integrity of this information.
XML signatures oﬀer an ideal solution to the problem of making a policy
at a given URI non-repudiatable. XML signatures provide the opportunity to
introduce assertions such as “X assures the content of this document” into the
semantics of signed material. Also since P3P is entirely expressed in XML, it is
pragmatic to use the XML version of asymmetric digital signatures to provide
this assurance. The following section deﬁnes in detail how this might be achieved.
We examine and build upon the proposals of Reagle [5] for the inclusion of
XML digitally signed [10] policies within P3P. As Reagle has already set out most
of the mechanisms for achieving this, we make only three minor additions to the
technical speciﬁcation. Our main aim is to look at possible technical problems
with the use of the XML signature extension, and their solutions.

A Fully Compliant Research Implementation of the P3P Standard
117
XML Digitally Signed Policies P3P enabled servers could have the possibil-
ity of providing an XML digital signature as part of their policy, or as a separate
document referenced within the policy. This is easily accomplished provided that
the correct syntax is incorporated into the P3P speciﬁcation, as shown by Rea-
gle. We provide an example, see Annex 1, adapted from Reagle, which we have
extended in the following ways:
a) We have added X.509 certiﬁcate bag to provide full non-repudiatability.
b) We have included a time stamp to comply with EU regulations.
c) By requiring an additional signature over the PRF, which details which
resources the policy applies to. Any signature that does not assure this in-
formation loses much of its legal signiﬁcance. Note also that this signature
cannot be bundled in with the policy signature because several PRF’s may
refer to the same policy. Furthermore, the person responsible for produc-
ing policy signatures may not even know the location of PRF’s referring
to the policy (in the case of a standard issue policy used by third parties.)
We suggest the addition of a “DISPUTES” element to the PRF identical to
the DISPUTES element in the P3P policy which allows the speciﬁcation of
a signature URI using the validation attribute.
The P3P process has 2 main components on the server; an XML policy and
an XML PRF, which binds policies to resources. Semantically therefore, a P3P
statement is a combination of the policy and the PRF, which binds a policy to
a resource at a given time. The PRF, like the policy has a validity timestamp.
However, Reagle’s P3P extension does not include any binding signature for
the P3P PRF. This is an oversight, because omission of a signature binding the
policy to the resource it applies to negates the non-repudiability of the statements
being made. The importance of the PRF cannot be ignored. We therefore suggest
that a signature also be provided for any PRF’s used. We show, however, in the
example signature (Annex 1) the necessary extensions for a signature to be
bound to a PRF.
Computational Overhead in Processing Signed Policies The P3P process
involves the exchange and processing of policy ﬁles. So any increase in complex-
ity for policies, such as addition of digital signatures, implies an increase in
computational overhead for processing the data. This applies to both server and
client.
On the server, the additional processing can be considered negligible. There
is no dynamic load on the server from the encryption processes involved. This is
because the Privacy Policy statement is usually static (that is, a site’s privacy
policy does not change regularly) hence the signing process only needs to be
carried out once, when a new policy is created or edited. Once signed, the policy
signature can be posted on a web site with no further load than is the case with
a standard XML policy. For the client, there is a more signiﬁcant computational
overhead, as it must verify the policies received from a remote server. In tests, we
observed that signing and veriﬁcation operations for a client, using a typical PC,

118
Giles Hogben et al.
are order 100ms. Which, in isolated cases will have minimal impact on a user’s
browsing experience.
However, there are certain scenarios, which could theoretically lead to the
processor loading becoming signiﬁcant for both server and client. For example:
• If the client had to handle multiple policies, assigned to diﬀerent resources
within a web site, it could make the browsing process unacceptably slow.
For example a page might require the client to verify signatures from several
diﬀerent sources to handle a page’s embedded resources.
• If the client is in a mobile phone or other device with low processing capa-
bility.
• Conversely, for servers with high traﬃc loads, if there is an additional http
request to a signature resource with every http resource requested, then the
additional load becomes signiﬁcant.
• In the case of bi-lateral negotiated policies, some form of ﬁltering is an essen-
tial component in the process. In this case, P3P policies could be dynamically
altered to suit the user’s preferences, leading, in extreme scenarios, to a dif-
ferent policy being created for every request. The resultant load on the server
would be a processing bottleneck.
Fortunately, there are a number of pragmatic ways in which the above issues
can be constrained, to limit the number of times such operations would have to
be carried out (both on the server, and the client). The ﬁrst is by a proposed
simple extension using the rule system. This applies initially to APPEL [4] but
can easily be generalized to any subsequent rule system.
In the basic version, a P3P policy is retrieved and matched against an XML
preference set according to the APPEL protocol. Once this matching process
has been carried out, there are only 3 basic outcomes: page block(6.), page
request (7.) and page request with http header limitation (8.). Each of these can
be optionally authorized by a user prompt. An example rule is given above in
section on logical ambiguity of APPEL rules.
Reducing Signed Policy Processor Loading in P3P Using Rule Action
It would be easy to extend this syntax to solve the problem of loading and
redundancy for signature veriﬁcation in P3P. In the extended version (dotted
lines, ﬁgure 4), an extra but OPTIONAL decision step has been added (4.),
which may lead either to a page block (5.) in the case of an invalid or non-
existent signature, or to the continuation to the standard behaviors (6,7,8). This
again may be automated or may involve input from the user in the form of
a prompt (for example “this page does not have a valid signed policy – do you
wish to proceed”).
We suggest that the addition of a signaturerequired attribute to the rule ele-
ment, rules could limit the necessity of signature veriﬁcation as in this example:
<appel:RULE prompt="no" behavior="request" signaturerequired="yes">
<p3p:POLICY appel:connective="and"><p3p:ACCESS/></ p3p:POLICY>
</appel:RULE>

A Fully Compliant Research Implementation of the P3P Standard
119
Fig. 4. APPEL matching process. Dashed lines show our proposed extension to
process
Such a rule would then state in plaintext: “If the agent ﬁnds a statement of how
l access my data after it has been submitted., require that there is a valid XML
policy and prf signature, retrivt and store it.”
This change reduces processor loading and network traﬃc. It also makes it
easier to retrieve signatures when needed because the user stores only those sig-
natures deemed necessary. A suggestion for solving problems with limited device
capabilities in scenarios such as consent proof where client signatures might be re-
quired, would be to use CC/PP technology (Composite Capabilities/Preferences
Proﬁle) [13] which allows servers to make demands on devices according to their
capabilities.
5
Conclusion
P3P has been a great boost to raising awareness of privacy on the Internet. By
giving web users a tool for controlling the disclosure of personal data, it provides
one component in a spectrum of PET’s. The JRC reference implementation of
the P3P standard and a neutral platform for its evaluation allows the privacy
community to assess P3P. So far, we have identiﬁed a number of challenges on
the road to full maturity of the standard:
• Creation of an easy to use interface for creating XML privacy preference
rulesets, which will be useful to both the ordinary and the expert user.
• Creation of a language for the storage of privacy preferences which solves the
problems inherent in the current APPEL speciﬁcation, namely inadequate
logical expressivity and ambiguity.
• Solution of performance problems faced by an implementation, which is faith-
ful to the standard.
• Continuous evaluation of the use of P3P for enhancing compliance of privacy
practices with EU privacy regulation and other regulatory initiatives.
Data ﬂow and consumer conﬁdence on the Internet is severely hampered by
privacy related factors, which can easily be addressed. The lack of trust in privacy

120
Giles Hogben et al.
policies and established mechanisms for legal recourse based on the policies, are
issues, which can be addressed by extensions to P3P. One practical extension,
which has been discussed in detail, is the application of XML digital signatures.
We suggest XML digital signatures within the framework of P3P in combination
with mechanisms for automatic selection of which resources might require such
evidence.
References
[1] Harris Poll carried out for National Consumer League of America October 2000
– see http://www.nclnet.org/pressessentials.htm
104
[2] RDF Speciﬁcation. Resource Description Framework (RDF) Model and Syntax
Speciﬁcation http://www.w3.org/TR/1999/REC-rdf-syntax-19990222/
112
[3] The Platform for Privacy Preferences 1.0 (P3P1.0) Speciﬁcation W3C Working
Draft 28 September 2001, http://www.w3.org/TR/2001/WD-P3P-20010928/
106
[4] A P3P Preference Exchange Language 1.0 (APPEL1.0), W3C Working Draft
26 February 2001, http://www.w3.org/TR/2001/WD-P3P-preferences-20010226
107, 115, 118
[5] Joseph Reagle, A P3P Assurance Signature Proﬁle, W3C Note 2, February 2001,
http://www.w3.org/TR/xmldsig-p3p-profile/
116
[6] Thibadeau, Robert. 2001 “Privacy Science”, Paper presented at the Eu-
ropean Commission Workshop on Privacy and Identity in the Information
Society: Emerging technological challenges, October 4-5, Brussels, Belgium,
http://yuan.ecom.cmu.edu/psp/privacy2001.ppt.
115
[7] Darpa Agent Markup Language – W3C speciﬁcation – soon to become OWL
(Ontology Web Language) http://www.w3.org/TR/daml+oil-reference
112
[8] Scribbins,
K.,
“Should
I
Buy?
Shopping
online
2001:
An
international
comparative
study
of
electronic
commerce”,
Consumers
International,
http://www.consumersinternational.org/CI Should I buy.pdf,
ISBN 1902391365, 2001.
104
[9] International XML rule language initiative, home page
http://www.dfki.uni-kl.de/ruleml/
113
[10] XML Digital Signatures – W3C Recommendation
http://www.w3.org/TR/ xmldsig-core/
116
[11] Berthol, O. and Marit K¨ohntopp, M. “Identity Management Based On P3P
http://www.koehntopp.de/marit/publikationen/idmanage/
BeKoe 00IdmanageBasedOnP3P.pdf, presented at the Workshop on Design Issues
in Anonymity and Unobservability. Available from http://www.w3.org/P3P/ 105
[12] W3C Xpath Speciﬁcation http://www.w3.org/TR/xpath
112
[13] Composite Capabilities/Preference Proﬁle: W3C standard under development
http://www.w3.org/Mobile/CCPP/
119

A Fully Compliant Research Implementation of the P3P Standard
121
Annexe 1: Sample XML signature of P3P policy. Note that a signature for the
PRF would be identical except that the node marked with ***’s would refer to
a policy reference ﬁle.
<Signature Id="Signature1" xmlns="http://www.w3.org/2000/09/xmldsig#">
<SignedInfo>
<CanonicalizationMethod
Algorithm="http://www.w3.org/TR/2000/WD-xml-c14n-20000907"/>
<SignatureMethod
Algorithm="http://www.w3.org/2000/09/xmldsig#dsa-sha1"/>
<Reference URI="http://www.example.org/p3p.xml">
<Transforms>
<Transform Algorithm="http://www.w3.org/TR/2000/
WD-xml-c14n-20000907"/>
</Transforms>
<DigestMethod Algorithm="http://www.w3.org/2000/09/xmldsig#sha1"/>
<DigestValue>j6lwx3rvEPO0vKtMup4NbeVu8nk=</DigestValue>
</Reference>
<Reference URI="#Assurance1"
Type="http://www.w3.org/2000/09/xmldsig#SignatureProperties">
<DigestMethod Algorithm="http://www.w3.org/2000/09/xmldsig#sha1"/>
<DigestValue>1342=-0KKAASIC!=123Adxdf</DigestValue>
</Reference>
<!-- Reference over signature policy *** or policy reference file
if for prf ***-->
<Reference URI="http://www.example.org/signaturePolicy.xml">
<DigestMethod Algorithm="http://www.w3.org/2000/09/xmldsig#sha1"/>
<DigestValue>1234x3rvEPO0vKtMup4NbeVu8nk=</DigestValue>
</Reference>
<!-- Reference over Time Stamp to comply with EU directive -->
<Reference URI="#TimeStamp1"
Type="http://www.w3.org/2000/09/xmldsig#SignatureProperties">
<DigestMethod Algorithm="http://www.w3.org/2000/09/xmldsig#sha1"/>
<DigestValue>k3453rvEPO0vKtMup4NbeVu8nk=</DigestValue>
</Reference>
</SignedInfo>
<SignatureValue>MC0CFFrVLtRlk=...</SignatureValue>
<KeyInfo>
<X509Data>
<X509IssuerSerial>
<X509IssuerName>CN=Smith John, OU=TRL, O=IBM, L=Example,
ST=Example, C=</X509IssuerName>
<X509SerialNumber> 12345678 </X509SerialNumber>
</X509IssuerSerial>
<X509SKI> 31d97bd7 </X509SKI>
</X509Data>
<X509Data><!-- single pointer to certificate-B -->
<X509SubjectName>Subject of Certificate B</X509SubjectName>
</X509Data>
<X509Data> <!-- certificate chain -->

122
Giles Hogben et al.
<X509Certificate>MIICXTCCA...</X509Certificate>
<X509Certificate>MIICPzCCA...</X509Certificate>
<X509Certificate>MIICSTCCA...</X509Certificate>
</X509Data></KeyInfo>
<Object>
<SignatureProperties>
<SignatureProperty Id="Assurance1" Target="#Signature1"
xmlns="http://www.w3.org/2000/09/xmldsig#">
<Assures Policy="<http://www.example.org/p3p.xml>"
xmlns="http://www.w3.org/2001/02/xmldsig-p3p-profile"/>
</SignatureProperty>
<SignatureProperty Id="TimeStamp1" Target="#MySecondSignature">
<timestamp xmlns="http://www.ietf.org/rfcXXXX.txt">
<date> 19990908 </date>
<time> 14:34:34:34 </time>
</timestamp>
</SignatureProperty>
</SignatureProperties></Object></Signature>
Annexe 2: APPEL Ruleset for an example user privacy preference consistent
with EU data protection directive
<?xml version="1.0" encoding="UTF-8"?>
<appel:RULESET xmlns:appel="http://www.w3.org/2001/02/appelv1"
xmlns:p3p="http://www.w3.org/2000/12/p3pv1">
<appel:RULE behavior="block" description="Any marketing must be opt-in
with prompt|Data-Type|Any" prompt="yes" promptmsg="Your privacy
agent has detected a~site which will use your data for
marketing if you agree to it - do you want to go to this page">
<p3p:POLICY>
<p3p:STATEMENT>
<p3p:PURPOSE appel:connective="or">
<p3p:telemarketing required="opt-in"/>
<p3p:contact required="opt-in"/>
</p3p:PURPOSE>
</p3p:STATEMENT>
</p3p:POLICY>
</appel:RULE>
<appel:RULE behavior="block" description="No compulsary marketing"
prompt="no">
<p3p:POLICY>
<p3p:STATEMENT>
<p3p:PURPOSE appel:connective="or">
<p3p:telemarketing required="always"/>
<p3p:contact required="always"/>
</p3p:PURPOSE></p3p:STATEMENT></p3p:POLICY></appel:RULE>
<appel:RULE behavior="block" description="Blocked because site will use
your information for marketing purposes on an opt-out basis."

A Fully Compliant Research Implementation of the P3P Standard
123
prompt="no">
<p3p:POLICY>
<p3p:STATEMENT>
<p3p:PURPOSE appel:connective="or">
<p3p:telemarketing required="opt-out"/>
<p3p:contact required="opt-out"/>
</p3p:PURPOSE></p3p:STATEMENT></p3p:POLICY></appel:RULE>
<appel:RULE behavior="block" description="Blocked because you cannot
access all your data after submitting it" prompt="no">
<p3p:POLICY>
<p3p:ACCESS appel:connective="non-and">
<p3p:all/><p3p:nonident/>
</p3p:ACCESS>
</p3p:POLICY>
</appel:RULE>
<appel:RULE behavior="block" description="Site will retain information
collected by this resource beyond what is necessary to carry out
the stated purpose" prompt="no">
<p3p:POLICY>
<p3p:STATEMENT>
<p3p:RETENTION appel:connective="non-and">
<p3p:stated-purpose/>
</p3p:RETENTION></p3p:STATEMENT></p3p:POLICY></appel:RULE>
<appel:RULE behavior="block" description="Require identity and physical
address of controller" prompt="no">
<p3p:POLICY appel:connective="non-or">
<p3p:ENTITY>
<p3p:DATA-GROUP>
<p3p:DATA ref="#business.contact-info.postal.street"/>
<p3p:DATA ref="#business.contact-info.postal.city"/>
<p3p:DATA ref="#business.contact-info.postal.stateprov"/>
<p3p:DATA ref="#business.contact-info.postal.postalcode"/>
<p3p:DATA ref="#business.contact-info.postal.country"/>
</p3p:DATA-GROUP>
</p3p:ENTITY>
</p3p:POLICY>
</appel:RULE>
<appel:RULE behavior="request" description="Passed all rules and final
check on disclosure to countries outside EU which don’t follow
the same practices and if they do, Access rights must be
stated." prompt="no">
<p3p:POLICY>
<p3p:STATEMENT>
<p3p:RECIPIENT appel:connective="or-exact">
<p3p:ours/>
<p3p:delivery/>
<p3p:same/>
</p3p:RECIPIENT>
</p3p:STATEMENT>
</p3p:POLICY>

124
Giles Hogben et al.
</appel:RULE>
<appel:RULE behavior="block" description="Default Rule fired">
<appel:OTHERWISE/></appel:RULE></appel:RULESET>
Annexe 3: Example of a Web site privacy policy complying with the principles
of the EU data protection directive
<?xml version="1.0"?>
<POLICIES xmlns="http://www.w3.org/2002/01/P3Pv1">
<EXPIRY max-age="604800"/>
<POLICY
discuri="http://p3p.jrc.it/modelsite/htmlpolicies/
officialpolicy.html"
opturi="http://p3pproxy.jrc.it:1080"
name="mainpolicy">
<!-- Description of the entity making this policy statement. -->
<ENTITY>
<DATA-GROUP>
<DATA ref="#business.name">Joint Research Center</DATA>
<DATA ref="#business.contact-info.postal.street">Cybersecurity TP 361
Via Enrico Fermi 1</DATA>
<DATA ref="#business.contact-info.postal.city">Ispra</DATA>
<DATA ref="#business.contact-info.postal.stateprov">Lombardia</DATA>
<DATA ref="#business.contact-info.postal.postalcode">21020</DATA>
<DATA ref="#business.contact-info.postal.country">Italy</DATA>
<DATA ref="#business.contact-info.postal.organization">
Giles Hogben</DATA>
<DATA ref="#business.contact-info.online.email">
giles.hogben@jrc.it</DATA>
<DATA ref="#business.contact-info.online.uri">http://p3p.jrc.it</DATA>
</DATA-GROUP>
</ENTITY>
<!-- Disclosure -->
<ACCESS><all/></ACCESS>
<!-- No dispute information -->
<!-- Statement for group "General clickstream data" -->
<STATEMENT>
<!-- No consequence specified -->
<!-- Data in this statement is marked as being non-identifiable -->
<NON-IDENTIFIABLE/>
<!-- Use (purpose) -->
<PURPOSE><current/></PURPOSE>
<!-- Recipients -->
<RECIPIENT><ours/></RECIPIENT>
<!-- Retention -->
<RETENTION><stated-purpose/></RETENTION>

A Fully Compliant Research Implementation of the P3P Standard
125
<!-- Base dataschema elements. -->
<DATA-GROUP>
<DATA ref="#dynamic.clickstream"/>
<DATA ref="#dynamic.http"/>
</DATA-GROUP>
</STATEMENT>
<!-- End of policy -->
</POLICY>
</POLICIES>

Authentication for Distributed Web Caches
James Giles, Reiner Sailer, Dinesh Verma, and Suresh Chari
IBM, T. J. Watson Research Center
P.O. Box 704, Yorktown Heights, New York, USA
{gilesjam,sailer,dverma,schari}@watson.ibm.com
Abstract. We
consider
the
problem
of
oﬄoading
secure
access-
controlled content from central origin servers to distributed caches so
clients can access a proximal cache rather than the origin servers. Our
security architecture enforces the access-control policies of the origin
server without replicating the access-control databases to each of the
caches. We describe the security mechanisms to aﬀect such a system
and perform an extensive security analysis of our implementation. Our
system is an example of how less trustworthy systems can be integrated
into a distributed system architecture; it provides mechanisms to pre-
serve the whole distributed system security even in case less trustworthy
subsystems are compromised. An application of our system is the cached
distribution of access-controlled contents such as subscription-based
electronic libraries.
Keywords:
Security,
Authentication,
Cookies,
Distributed
Appli-
cations, CDN.
1
Introduction
It is well known that caching content in a distributed fashion throughout the
network can provide substantial beneﬁts such as shorter response time, smaller
overall bandwidth requirements, and balanced load among servers. In content
distribution networks (CDN), a content provider usually deploys central origin
servers that feed multiple content cache servers which are strategically located
near client machines as in Figure 1. Clients address their requests to the origin
server, but are redirected to one of the content cache servers by a request router
in the network. The content cache server requests content from the origin server
to satisfy the client requests if the content is not already cached. In the Internet,
there is an extensive infrastructure in place (see Akamai [1] and Speedera [2])
to do limited content distribution. However, most of the content caching infras-
tructure does not provide access control and typically assumes a complete trust
relationship between origin servers and the content caches.
This paper addresses the important, practically motivated problem of provid-
ing access control for content which we wish to serve from distributed caches. We
seek to integrate several proven theoretical solutions into a practical, deployable
architecture keeping the following security goals in mind: i) data should be pro-
tected until it reaches the client system, ii) from the time that we learn a cache is
D. Gollmann et al. (Eds.): ESORICS 2002, LNCS 2502, pp. 126–146, 2002.
c
⃝Springer-Verlag Berlin Heidelberg 2002

Authentication for Distributed Web Caches
127
Client
Cache
Cache
Origin
Server
Fig. 1. Content distribution system including origin servers and caches
compromised, the cache should not be able to disclose any additional data that
it does not have cached, iii) caches should not be able to masquerade as clients,
and iv) the system should be self-healing when caches are known to be compro-
mised. We distinguish trusted caches and known-compromised caches. Trusted
caches become known-compromised if we detect security breaches, e.g., through
deploying intrusion detection systems (IDS [3], [4]). Since there is a possibility
that information stored by caches could be disclosed if a cache is compromised,
we do not trust caches with sensitive long-term secrets such as user identiﬁers
and passwords or any other user information that would enable a cache to mas-
querade as a user. This trust model is motivated from the practical constraint
that there are a number of caches residing close to clients, making it expen-
sive to provide administration and physical protection at each location. Our
assumptions are that i) the origin server is well protected, ii) the architectures
we build on such as SSL and PKI are secure and the implementation is correct,
and iii) good intrusion detection systems with low false negative rates are avail-
able (infrequent false positives pose less of a problem than false negatives). Our
architecture is as strong as the intrusion detection; we beneﬁt from future im-
provements of IDS. In Section 7, we describe extensions to our architecture that
rely less on the IDS systems and protect all information, including information
stored in the caches, from disclosure by compromised caches.
An application for this architecture is the USENIX online periodical deliv-
ery system [5]. Response time for this system could be improved by distributing
content to caches throughout the network using a content distribution service
provider, e.g., Akamai [1]. Since most of the content is only available by sub-
scription, the content caches must provide access control mechanisms. However,
USENIX is unlikely to trust the content caches with sensitive information such
as user passwords, account information, and credit card information.
The mechanisms we have designed to address this problem enable a graceful
transition of caches from trusted to distrusted while balancing security against
availability of content and access control information revealed to caches. The
features of our system include:
– Long-term sensitive content is not revealed unprotected to caches.

128
James Giles et al.
– Access control information such as userid and password are not revealed
directly to caches. Rather, we use access control tokens with limited validity.
– Our architecture incorporates cryptographic mechanisms to protect against
replay attacks with access control tokens.
In general, the weakest subsystem determines the overall security of a sys-
tem. To maintain a desired level of security for the overall content distribution
system, our architecture combines observation technology [4], which monitors
the security level of the subsystems, with mechanisms that neutralize subsys-
tems whose security level falls below the desired level. The self-healing mecha-
nisms ensure that known-compromised subsystems cannot be misused to further
compromise other subsystems in the future. Because there is a risk that the
security sensors will not detect compromised subsystems, we deny the less se-
cure subsystems access to the most sensitive information such as user passwords
and long-term protected data. The architecture we have implemented provides
a practical solution which balances the need for security with the need for cost
eﬀective distribution of content. Our architecture is a compromise between func-
tionality, security, and usability and we explain how these three factors relate to
each other in the conclusion.
We perform an extensive security exposure analysis of our architecture and
highlight which attacks the mechanisms in our architecture can mitigate. The
client setting we have chosen for our solution is the standard web browser and
its associated state maintenance mechanisms. We discuss how stronger security
guarantees can be obtained by extension of these mechanisms.
The paper is organized as follows. Section 2 discusses related work. Section 3
describes the distributed content cache setting in detail. In Section 4, we describe
the basic security architecture which emphasizes compatibility with the existing
client infrastructure. In Section 5, we describe the implementation of our system.
In Section 6, we show the coverage of a generic threat model by our architecture.
Section 7 introduces extensions that resolve remaining exposures at the cost of
extending client browsers. Finally, in Section 8, we evaluate the architecture and
describe the beneﬁts it provides for content distribution systems.
2
Related Work
There are several related authentication schemes that are used for web-based
communication for distributed systems. Since extensions to client browsers are
usually infeasible, most of the authentication schemes rely on functions normally
available to browsers.
Fu [6] describes generic procedures for implementing secure cookie-based
client authentication on the web. Our work can be considered an extension that
uses the HTTP cookie mechanism [7] to receive and present user credentials to
servers within the same domain. Fu’s scheme includes a timestamp with the cre-
dentials to limit their usefulness and a MAC over the credential ﬁelds to protect
the credential integrity. The conﬁdentiality of the credentials is assumed since
the cookies are sent only over SSL to servers that are assumed to be secure.

Authentication for Distributed Web Caches
129
Since we use the credentials to access servers in multiple administrative domains
(e.g., origin server and caches) and do not have the same level of security at the
origin server and caches, we oﬀer protection against cookie replay attacks. Ad-
ditionally, we provide mechanisms by which cache servers can be excluded from
participation in the authentication process if they become known-compromised.
Kerberos [8] makes use of authentication tokens that are granted by an au-
thority to access services on other machines. However, Kerberos depends on
distribution and maintenance of secret keys shared by each client and authority
pair, which has proven intractable so far in the web environment; our scheme
has no such limitations. Inter-realm authentication between diﬀerent domains
is quite expensive using Kerberos. Additionally, Kerberos is currently not sup-
ported by common browsers. The Kerberized Credential Translator [9] is an at-
tempt to allow browsers to access Kerberized services over SSL without browser
modiﬁcation, but this scheme also relies on mechanisms for distribution and
maintenance of client certiﬁcates used to authenticate the client under SSL.
Microsoft’s Passport system [10] oﬀers authentication across multiple do-
mains through a centralized authentication server and token-based credentials.
Our system diﬀers from Passport in that we use the same token to access multi-
ple caches rather than returning to the origin server to obtain a new token each
time a new cache is accessed. Caches in our architecture have freedom to extend
the lifetime of the token, but a known-compromised cache can be eliminated
from participation in the authentication scheme.
3
Distributed Secure Content Caching System
We propose a self-healing distributed security architecture for content distribu-
tion systems, which allows the origin server to maintain control over authenti-
cation, while distributing content through partially trusted content caches.
In our architecture the origin server trusts content caches for certain func-
tions, such as distributing content to clients, only after verifying their autho-
rization at the origin site. However, our architecture assumes that distributed
caches can be compromised. The failure model we assume is that up to a certain
time a cache can be trusted to execute its functions and once compromised it
can no longer be trusted to execute its functions. Our architecture also assumes
that the compromise of caches can be eﬀectively detected by the use of appro-
priate security sensor technology (see for example [4]). Thus, our architecture
proposes mechanisms by which content caches distribute sensitive information
as long as they are trusted by origin servers with the safeguard that a content
cache known by origin servers to be compromised can be eﬀectively neutral-
ized from the rest of the system without further risk. The compromised cache
is neutralized by disabling its access to the content on the origin server and by
distributing new cryptographic material to the remaining caches allowing their
continued participation in the system.
The security architecture we propose is a token-based access-control system,
which we describe in the context of web servers, browsers, and web caches. There

130
James Giles et al.
is an origin server, one or more content caches, and one or more clients as in
Figure 1. Content distribution systems fall into two broad categories: those in
which the client primarily contacts the origin server for content, with the origin
server directing the client to retrieve certain objects from a convenient content
cache as in [1], and those for which the client is automatically directed to retrieve
most content from a convenient content cache without frequently returning to the
origin server as in [2]. We focus on content caching systems of the second type,
although the architecture works equally well with both content cache types. In
addition, the architecture that we propose is limited to HTTP [11, 12] traﬃc, but
our approach can be generalized to other protocols and distributed applications.
We implemented our security architecture using standard security techniques
like Secure Sockets Layer (SSL) [13, 14], IPSEC [15], Password Authentication
Protocol, public key signatures, symmetric encryption, and standard web tech-
nology like browsers and Apache web servers including SSL and servlet modules.
The state maintenance mechanism we use is the HTTP cookie mechanism [7]
which we employ to convey authentication tokens. We take safeguards against
the well-known exposures in HTTP cookies, such as cookie theft. The signiﬁcant
contributions of our architecture are that it:
– protects against a large number of attacks arising from the distributed cache
and server architecture.
– provides single sign-on to all the content caches for a particular origin site.
– maintains user privacy.
– enables self-healing and containment when a content cache is determined to
be known-compromised.
By self-healing and containment, we mean that an origin server can disable
a compromised content cache in such a way that it cannot be used to further
jeopardize the security, even if it is completely controlled by an adversary.
We perform an extensive security analysis of our architecture by enumer-
ating exhaustively the diﬀerent security exposures of our system. We discuss
in detail those exposures which are unique to our architecture as opposed to
those common to any client-server application and highlight the features of our
architecture that mitigate each of these exposures.
4
Security Architecture and Mechanisms
Our security architecture imposes two primary requirements. First, the origin
servers’ and clients’ long-term secrets and integrity-sensitive contents are not
made accessible to caches without protecting the content. Second, compromised
caches must be excluded seamlessly from further participation in the content
distribution network (self-healing).
4.1
Authentication and Access Control
Figure 2 illustrates the information and control ﬂows for the basic architecture.
In its simplest form, clients initially authenticate with a trusted origin server

Authentication for Distributed Web Caches
131
Client
Cache
Origin
Server
User
Registry
2. Authentication request
3. Authentication response
1. Request protected content
5. Set token and redirect client
4. Validate user
and create token
7. Validate 
6. Request protected content
8. Reset token, serve protected content
token
Fig. 2. Requesting protected data in our architecture
using userid and password (cf. 1-4 in Figure 2) and receive an authentication
token from the origin server (cf. 5 in Figure 2) containing information about the
content that the client should be entitled to retrieve. The origin server redirects
the client to the content caches with the speciﬁc content cache being resolved
from the redirection URL with DNS-based request routing ([16, 17]). The client
(or browser) presents this authentication token to the cache along with a request
for content (cf. 6 in Figure 2). After receiving an initial request and a token,
the cache then validates the token and grants access to the client if the access
control information in the token indicates that the client is authorized to view the
requested content (cf. 7-8 in Figure 2). After successful validation of a token, the
cache stores the SSL session identiﬁer together with the authentication token in a
session identiﬁer cache so that subsequent requests within the same SSL session
and within a short time period ∆t do not require the server to evaluate the token
again. Assuming that user and client do not change within a SSL session, we
do not lose security by relying on the SSL session identiﬁer for authentication
rather than evaluating the token again. A small ∆t ensures that a token still
expires near to the timeouts, i.e., that a token is evaluated at least every ∆t
seconds. At the end of an SSL session, the cache entry of this SSL identiﬁer and
the related token are deleted on the server. When the client logs-oﬀ, the cache
or the origin server deletes the cookie in the client’s browser.
Communication of authentication credentials, tokens, and sensitive informa-
tion between origin servers and clients, and caches and clients is protected by
SSL. Content and keys are exchanged by the origin servers and caches via mutu-
ally authenticated SSL or IPSEC connections. For cases when the origin servers
and the caches are part of the same name-space domain, the authentication to-
ken is passed from the origin servers to the clients and from the clients to the
caches through the use of HTTP cookies [7]. More elaborate forms of the system,
discussed in Section 7, incorporate techniques for cross-domain cookies in cases
where the cache and origin server are in diﬀerent domains, as well as techniques
for protecting highly sensitive content from disclosure if a cache is compromised.

132
James Giles et al.
4.2
Authentication Token
The authentication token implements a one-way authentication whereby the
client sends a token to the cache and the cache identiﬁes the client through
the content of the token. The authentication is usually one of possession, i.e.,
a client possessing the token is assumed to be authenticated for the identity or
capabilities claimed in the token. Therefore, unauthorized replay of authentica-
tion tokens – especially when implemented as cookies – is a major threat. To
thwart unauthorized replay, we include information in the token which enables
the cache to validate the sender of the token. Such information should be:
– unique to sender (sender IP address hash, Browser Process and User IDs)
– easily veriﬁable by cache (apparent client IP address, HTTP header)
– diﬃcult to guess for attackers
– ﬂexible with client privacy and security
First, we describe the structure of the cookie that carries the authentication
token between clients and servers. Afterwards, we describe the authentication
token in detail. Finally, we discuss the protection mechanisms for the cookie and
the authentication token.
Cookie Structure. Our architecture uses cookies to transport authentication
tokens (AUTH TOKEN) from origin servers to client browsers and between
client browsers and caches. Below, we show an exemplary cookie that includes
an authentication token; it will be sent over SSL only to servers in the .ibm.com
domain (see “secure” and “domain” ﬁelds [7]). The expires-ﬁeld of the cookie is
not used, i.e., the cookie is deleted when the browser closes.
Cookie: authtoken=AUTH_TOKEN; path=/; domain=.ibm.com; secure;
The cookie as illustrated above ensures the following properties:
– Cookies containing authentication tokens are sent only over SSL connec-
tions: First, cookies are sent only to sites that authenticated via SSL with
a certiﬁcate issued for a machine in the stated domain. Second, servers will
receive cookies and included authentication tokens only if the request comes
over SSL; non-SSL traﬃc will not impose cookie handling on the server.
– The cookie and included authentication token will be destroyed once the web
browser is closed.
– The cookie transports the authentication token transparently (the authen-
tication token is relatively small).
Any information used for access control is stored securely in the authenti-
cation token. We do not protect the integrity or conﬁdentiality of the cookie
because any sensitive information is protected inside the authentication token.
The contents of the authentication token are illustrated in Table 1.
The origin domain ID contains the domain name of the origin server. A gener-
ation timestamp indicates the time the token is generated in seconds since Jan 1,
1970. Inactivity and global timeouts are oﬀsets from this generation timestamp

Authentication for Distributed Web Caches
133
Table 1. Authentication token contents: signed (S), hashed (H), and en-
crypted (E)
Origin domain id
S, H, E
Generation timestamp
S, H, E
Global timeout
S, H, E
Access control information
S, H, E
Client environment hash: client IP address and HTTP header ﬁelds S, H, E
Origin server signature
H, E
Inactivity timeout
H, E
Token hash MD5/SHA-1
E
and invalidate the cookie if either expires. The global timeout is set by the origin
server and cannot be changed (e.g., 2 hours). The global timeout sets an upper
limit on the validity period of authentication tokens in seconds starting from
the generation time. The inactivity timeout (e.g., 180 seconds) invalidates the
authentication token if there is no client request within this time period. It is
advanced by servers when handling client requests after the cookie is validated.
Subscription service and access rights are stored in the access control infor-
mation ﬁeld (e.g., user/group ID). Additional information about the client such
as the apparent IP address of the client or header ﬁelds is hashed and included in
the Client environment hash ﬁeld to further authenticate the sender of a cookie.
The Origin server signature is built by signing all authentication token ﬁelds
except the Token hash, Inactivity timeout, and Origin server signature ﬁelds us-
ing an asymmetric algorithm (e.g., 1024bit-RSA). The Token hash ﬁeld includes
a one-way hash (e.g., MD5 [18]) over the whole authentication token exclud-
ing the Token hash ﬁeld. Finally, the authentication token is encrypted using
a symmetric encryption algorithm (e.g., 2key TripleDES-CBC).
Authentication Token Tamper and Disclosure Protection. The authentication
token includes three levels of protection. A ﬁrst level (static integrity protec-
tion) is achieved by signing the static part of the cookie with the secret key
of the origin server. This signature, stored in the Origin server signature ﬁeld,
protects against unnoticed tampering of the signed static authentication token
ﬁelds. Only the origin server can create an authentication token. The second
level (privacy protection) consists of encrypting the whole authentication token
with a symmetric token encryption key shared only by the origin server and the
trusted cache servers. A third level (dynamic integrity protection) is added by
the encrypted hash value computed over the authentication token and stored
in the Token hash ﬁeld; this level is needed because the cache servers need to
adjust the dynamic Inactivity timeout ﬁeld. After updating the authentication
token, the cache servers recompute the hash and encrypt the whole token with
the known symmetric encryption key. We compute the token hash value before
encrypting the authentication token and use the symmetric key both for pri-
vacy protection and for implementing the hash signature. An attacker trying to

134
James Giles et al.
change the inactivity timeout (blindly because encrypted) would have to adapt
the hash value in order to obtain a valid cookie; encrypting the hash thwarts
this attack. Changing any other ﬁeld would both invalidate the token hash and
the origin server signature.
Considering all three levels, the authentication token is protected against
unauthorized disclosure to any party other than the origin and caches servers.
The dynamic ﬁelds (inactivity timeout and hash) are protected against unau-
thorized change by any party other than the origin and trusted cache servers.
The other ﬁelds (static ﬁelds) are protected against unauthorized change by any
party other than the origin server. No third party can disclose the content from
the authentication token or change its contents unnoticed by other means than
breaking the cryptography (brute force attacks on the keys or the algorithms).
Re-keying. We need to change the token encryption keys from time to time:
– to protect against successful known plain-text or brute force attacks on the
token encryption key (normal re-keying mode)
– to exclude known-compromised caches from updating or re-using tokens
(compromised re-keying mode)
Re-keying occurs about once every few hours but depends on the amount of token
data encrypted under the same encryption key. We consider caches well-protected
and assume that known compromises occur infrequently and are handled in an
exception mode described in Section 4.3. Therefore we implemented re-keying by
distributing the new key from the origin server to all trusted caches via SSL. If
re-keying occurs often or the number of caches is very large, then we propose to
apply fast group re-keying schemes as known from multicast architectures [19].
The public key certiﬁcates of the origin server, used by caches to verify the origin
server signature, are supposed to be valid for a very long time (e.g., half a year).
If the origin server private key changes, re-distributing certiﬁcates to caches can
be done manually.
Authentication Token Replay Protection. As with other capability based sys-
tems, we usually want to verify the sender of valid credentials [20], [21]. One
solution is to assume attackers cannot gain access to valid authentication to-
kens. Obviously, following the many security problems in todays web browsers,
this would not be a valid assumption in our application environment - even
considering non-persistent, secure cookies.
If the client-side public key infrastructure were feasible, demanding client
side SSL certiﬁcate authentication would be an obvious choice; the cookies that
are received via an SSL session would be authenticated by the contents of the
client certiﬁcate. Besides client certiﬁcates being not very popular and PKI being
not broadly trusted, this approach presumes a working revocation mechanism
for client certiﬁcates. As the respective secret signature keys would reside on
the client machine, exposure will happen regularly and revocation lists would
become quite long and diﬃcult to manage, so we do not take this approach.
Therefore, our authentication tokens – generated by the origin server – have
several “built-in” protections against replay attacks. The timeouts restrict the
replay window for clients to the inactivity timeout. The global timeout restricts

Authentication for Distributed Web Caches
135
replay of tokens after a client is authenticated (e.g., at latest after a global
timeout, changes in the user registry – such as changing group membership or
deleting users – will go into eﬀect). Additionally, the client environment hash over
the client IP address and HTTP header ﬁelds restricts replay of tokens to clients
that share the same IP address (or Proxy) and the same HTTP header ﬁelds. If
the client environment changes, as observed by the caches or origin server, then
the client environment hash computed by the servers will change, too. Hence
the cookie will be invalid when submitted from the new environment because
the computed client environment hash diﬀers from the Client environment hash
stored in the token. Note, that dynamic IP addresses obtained over the DHCP
protocol will usually not adversely eﬀect the Client environment hash because
the leases are most often quite long compared to typical secure web sessions and
in any case clients are often re-assigned the same IP address when leases expire.
For token replay by known-compromised caches see Section 4.3.
More elaborate techniques using diﬃcult to guess environment information
are presented in Section 7 as they presume enhancements in the client envi-
ronment (browser extensions or applets). In environments, where the IP address
changes often or multiple proxy servers are used, we propose to use the extended
version described in Section 7.
Authentication Token Creation and Distribution Protection. The user is au-
thenticated against the origin before before an authentication token is created.
A variety of authentication methods, such as userid/password or one-time pass-
words, can be used over SSL to authenticate the user. Before submitting authen-
tication information, the user should carefully verify the certiﬁcate obtained
during the SSL server authentication to thwart server masquerading attacks.
A weakness of todays web service infrastructure is that major web browsers do
not automatically show information about the authenticated server to the user
if the certiﬁcate is valid.
The origin server transfers the created authentication token over SSL directly
to the client using HTTP cookies, or one of the other techniques described in
Section 7. Clients present their tokens to caches with each information request
over SSL. SSL helps in this case to protect cookies from disclosure to attackers
and subsequent replay attacks.
Authentication Token Validation. Servers validate authentication tokens re-
ceived in cookies in a multi-stage process as shown in Figure 3. Recall, that the
cache does not need to validate every token, see Section 4.1. The authentication
token is validated successfully only if it passes several checks. First, we discard
authentication tokens whose length is not a multiple of the block size of our
encryption algorithm (usually 8-byte aligned). Second, we discard authentica-
tion tokens that were stored previously in an invalid authentication token cache.
Third, the authentication token is decrypted, and the decrypted token is hashed
and matched against the decrypted Token hash ﬁeld; this veriﬁes both that the
authentication token was encrypted using the valid shared key and that it was
not tampered with. Next, the decrypted timestamps are checked to see if the
authentication token has timed-out. Then, the client environment hash in the

136
James Giles et al.
Check Token
Length
Check Token
Timeouts
Check Origin
Signature
Check
Client Environment
Hash 
Token in
Decrypt Token
Invalid Token
Cache?
Valid 
Invalid 
Token
OK 
No
OK
OK
OK
Yes 
Invalid
Authentication
Token Cache
NOK
NOK
NOK
(discard)
Token
Fig. 3. Validating authentication tokens
cookie is veriﬁed by checking sender information (e.g., IP address) as observed by
the server. Finally, the origin server signature is checked. The validation process
stops as soon as a check fails and the request is denied. A rejected authentica-
tion token is added to a cache of invalid authentication tokens and the client is
redirected to the origin server for re-authentication. We classify authentication
tokens for which the client environment hash check fails as invalid to prevent
attackers from simulating diﬀerent client environments against the same cache.
Authentication Token Update. Inactivity timeouts must be updated regularly
to ensure that tokens do not expire throughout their use. Cache servers can
update tokens whenever a request is granted after validating a token. When
updating a token throughout validation, the cache server must calculate the new
inactivity timeout (current local time - generation time + inactivity oﬀset), re-
calculate the token hash, and encrypt the new authentication token. Throughout
bursts, the server would have a signiﬁcant overhead by updating tokens for the
same client in a single SSL session.
For server performance reasons, tokens are validated and their inactivity
timeout is advanced not for each client request but rather only if a time ∆t has
gone by since the last update of the inactivity timeout or if the SSL session
identiﬁer changes. The time stamp of the last update is stored together with
the token in the session identiﬁer cache. Hence, a token must be re-validated
if (local time - last update ≥∆t). The value of ∆t must be relatively small
in relation to the inactivity timeout, e.g., 5 seconds. This ensures that a server
does not need to update an authentication token for each request within a burst.
Depending on a valid SSL session identiﬁer, this mechanism also ensures that
the ﬁrst authentication token received via a SSL session is validated.

Authentication for Distributed Web Caches
137
4.3
Self-Healing in Case of Compromised Caches
Neutralizing a known-compromised cache includes (i) excluding the cache from
accessing and updating authentication tokens created or updated in the future,
(ii) excluding the cache from accessing the origin server or other CDN infrastruc-
ture, and (iii) ensuring that clients do not connect to distrusted cache servers.
To exclude a known-compromised cache from accessing and updating authen-
tication tokens, a new shared cookie encryption key is distributed by the origin
server to the remaining trusted caches via SSL. In doing so, the compromised
cache cannot disclose or update any new authentication tokens and old tokens
are no longer accepted by the other caches. To ensure immediate impact, any
cache that receives a signed re-keying notiﬁcation with the compromised-ﬂag set,
will not accept any tokens encrypted by the old token encryption key and will
use only the new token encryption key included in the notiﬁcation. Clients sub-
mitting such old tokens are redirected to the origin server for re-authentication
and for creating a new token. As these cases should be rare, it seems acceptable
to re-authenticate users.
To exclude a known-compromised cache from further participation in the
CDN infrastructure, the origin server disconnects the SSL connection to the
cache so that the cache no longer has any access to the content. The cache’s
certiﬁcate is invalidated so that the cache cannot set-up new SSL connections to
the CDN infrastructure. To prevent clients from accessing compromised caches,
the CDN request router will no longer redirect clients to known-compromised
caches. If the CDN router uses DNS-based redirection [17, 16], we can ensure
that clients are directed to remaining trusted caches even if they bookmarked
pages from the caches – as long as the bookmarks contain server names and not
IP addresses. The DNS resolves machine names to trusted caches only.
If a cache server is compromised, any content at the cache will be disclosed to
the attacker. The basic system does not prevent such disclosure, but rather does
not replicate long-term sensitive content to caches (e.g. replicate articles, but no
passwords). An extension of this system stores only protected content on caches
(encrypted and signed content with encryption and signature keys unknown to
the cache); if such a cache is compromised, the content cannot be disclosed or un-
noticeably changed. Such an extension needs a client browser enhancement (e.g.,
plug-in) that handles protected (encrypted and integrity protected) content.
5
Implementation
For our prototype implementation, we used standard client systems running
Microsoft Windows or Redhat Linux operating systems with either Internet Ex-
plorer or Netscape Navigator browsers. The caches and origin servers were PC
systems running Redhat Linux and the Apache [22] web server with the Tomcat
servlet extensions. The initial authentication at the origin server was imple-
mented with Java servlets and the authentication cookie was set directly in the
client browser or stored in an OpenLDAP [23] directory which was accessible to

138
James Giles et al.
Table 2. Cryptographic performance
1024bit RSA Signature
9300 us
1024bit RSA Veriﬁcation
520 us
2-key TDES CBC
5.29 MByte/s
MD5
93.5 MByte/s
SHA-1
47.51 MByte/s
the caches as part of the cross-domain cookie extension discussed in Section 7.
The access control at caches was implemented with a Java servlet, and servlets
were used to manage the updates for the keys shared between the origin server
and the trusted caches. We used Apache SSL to protect communication between
the client and both the origin and cache servers. We also used Apache SSL for
all communication between the caches, the origin servers, and the LDAP direc-
tory, e.g., protecting the distribution of token encryption keys. For performance
reasons, we plan to replace the servlets with Apache module extensions.
Performance. Our authentication token ﬁeld lengths are as follows: Origin do-
main id (256 bytes), Generation timestamp (4 bytes), Global timeout (4 bytes),
Access control information (userid 4 bytes, groupid 4 bytes), client environment
MD5 hash (16 bytes), Origin server signature (128 bytes), Inactivity timeout
(4 bytes), MD5 Token hash (16 bytes). The total length of our authentication
token is 436 bytes, the cookie length will therefore around 500 bytes. We mea-
sured the following performance with OpenSSL on a Pentium III, 700 MHz,
compiled with MSVC 5.0 as shown in Table 2. To create authentication tokens,
origin servers need a signature (9.3ms), a 420 byte MD5 hash (4.5us), and a 436
byte 2-key TripleDES CBC encryption (82us) to create a token (total: 9.386 ms).
To validate a token, a cache needs to decrypt the authentication token (82us)
and to compute the token hash (4.5 us), hence 86.5 us in total. Updating a token
means to reset the inactivity timeout, recalculate the token hash and encrypt
the token (total 86.5 us), whereby a token is validated and updated at most
every ∆t seconds per client within an SSL session. For more information about
proper key sizes, see [24].
6
Security Analysis
For an attack model, content outsourcing is partitioned into three components
which are to be secured: static data objects, which we refer to as D, authenti-
cation functions, which we refer to as A, and other functions such as programs
that build pages dynamically, which we refer to as F. Parties involved in the
content cache system are clients (U), caches (C), origin servers (O), and the
network (N). Table 3 summarizes the features addressed in this paper.
Our security architecture addresses features of authentication (A) at the
client, data (D), and other functions (F) at the cache, and A at the origin. The

Authentication for Distributed Web Caches
139
Table 3. Addressed (x), no control (†), assumed (‡), SSL (△)
Components
Players
Data
Auth
Functions
User
†
x
†
Cache
x
x
x
Origin
‡
x
‡
Network
△
architecture makes use of SSL for all communications, allowing our protocol to
be independent of F (other than availability) and A and D on the network. We
do not address D and F on the client and D and F on the origin server. On
the cache, we address security breaches insofar as we assume to recognize when
a cache is compromised. System security, addressing F and D, can be imple-
mented independently and complements our architecture. Availability attacks
are not explicitly addressed, although there are aspects of our architecture that
protect against availability attacks. The threat tree [25] in Figure 4 models pos-
sible threats to the security of our scheme. The abbreviations D, I, and D.O.S.
in the ﬁgure correspond to disclosure attacks, integrity attacks, and denial of ser-
vice attacks, respectively. Our system is primarily concerned with protecting the
authentication functions and with neutralizing caches that are known to be com-
promised. We will not discuss attacks such as network disclosure and integrity,
because our architecture uses established techniques, such as SSL or IPSEC, to
protect against them. Protections against denial of service attacks – other than
token ﬂooding – are not addressed by our scheme, but can be implemented inde-
pendently. We rely on clients to carefully protect their authentication credentials
such as passwords, and to cautiously verify SSL certiﬁcates to ensure that they
are communicating with the correct entities. In the remainder of this section, we
describe the attacks that our system speciﬁcally addresses.
Internal
Attack
Origin
Client
Cache
Network
Threat Tree for Distributed Web Cache Architecture
External
Attack
Internal
Attack
External
Attack
Not-known-
compromised
Known-
compromised
SSL
outside
the scope
outside
the scope
Masq
Cache
Masq
Origin
Token
Flooding
Masq
Client
Masq
Client
Masq
Cache
D
I
D.O.S.
D
I
D.O.S.
D
I
D.O.S.
D
I
D.O.S.
D
I
D.O.S.
Masq
Client
Masq
Origin
Masq
Origin
Masq
Client
Masq
Cache
Masq
Origin
Data
Disclosure
Data
Spoofing
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
(11)
(12)
(13)
(14)
Threat #:
Fig. 4. Threat tree: disclosure (D), integrity (I), and denial of service (D.O.S.)

140
James Giles et al.
Since we assume clients and origin servers are secured against internal at-
tacks, we focus on external attacks against these systems. In particular, the
origin server is expected to be highly secured with intrusion detection and virus
protection software, ﬁrewall protection, and log monitoring. However, we explore
the case that a cache is known to be compromised and must be excluded from
the architecture and consider what damage such a cache compromise can cause
to overall system security in the future.
6.1
Masquerading Clients
Threat. Origin or cache servers may disclose sensitive content to a masquerading
client. Additionally, masquerading clients may manipulate the data at an origin
server or cache violating the data integrity. This threat occurs when a mas-
querading client has restricted access to put data into the origin server or cache
(service proﬁles, passwords management, order history). See attacks 1, 3, 4, and
7 in Figure 4.
Attacks. Certain exploits of this threat, such as an attacker obtaining or
guessing and using identities and passwords of a careless user, are common to all
authentication schemes and are not considered here. We focus on attacks where
an attacker tries to masquerade as a legitimate client by submitting a valid
authentication token to the origin server or cache. If the origin server is tricked
into accepting an authentication token, then the attacker can access content and
possibly be granted future access to the system.
Implementing Attacks. An attacker can try to masquerade either against the
origin server or against the cache – the result is the same. To masquerade, the
attacker may attempt to obtain a valid authentication token by:
– generating a valid authentication token from scratch.
– modifying an existing token to gain or extend access rights.
– stealing a valid authentication token from a client, cache, or origin server.
Once the attacker has obtained a valid authentication token, he can try to mas-
querade as a client by replaying the stolen token.
Defenses. The encrypted token hash acts as a signature on the token contents.
To create or modify a valid token, the attacker needs to create a new token hash
in encrypted form. Only knowing the current token encryption key could this
be accomplished. Hence, besides it being diﬃcult to steal a valid authentication
token, to successfully replay such a token, the attacker must stay within the
validity period of the token and satisfy the server’s client environment check.
For this, the attacker must guess and simulate a legitimate client’s environment,
see Sections 4 and 7.4.
6.2
Masquerading Caches
Threat. The origin server might disclose information to a masquerading cache.
This threat is especially dangerous because a cache has the authority to obtain

Authentication for Distributed Web Caches
141
content from the origin server on behalf of any users. A successfully masquerad-
ing cache could simply request any desired content for which it is authorized
from the origin server.
Defenses. The caches and the origin server use mutually authenticated SSL
to protect the communication stream and to protect against a masquerading
cache by verifying SSL certiﬁcates. This ensures the identity of the cache. Each
cache is trusted as long the the IDS connected to the cache does not report
otherwise. This protection is as secure as SSL and the public key infrastructure,
and as good as the IDS deployed.
Threat. Clients might disclose sensitive information to a masquerading cache
such as user passwords or authentication tokens. These passwords could be used
to change user proﬁles and even deny access to legitimate users, e.g. by changing
access passwords. Authentication tokens could be used to launch other mas-
querading client attacks. Masquerading caches may also serve manipulated in-
formation to clients. See attacks 2, 12, and 14 in Figure 4.
Implementing Attacks. To masquerade as a trusted cache, an attacker can
either compromise and take control of an existing cache or trick the victims
into believing that another machine is a legitimate cache. To trick the victim,
the attacker would need to route clients to a masquerading cache (e.g., by DNS
spooﬁng).
Defenses. Caches are observed by intrusion detection systems. Once a cache
is taken over, the IDS will report the cache as compromised to the origin server.
The origin server will initiate a re-keying in compromise-mode, hence any old
tokens will be invalid. This protection is as good as the IDS deployed. An attacker
masquerading as a cache must authenticate throughout the SSL session setup
against the client. As the cache does not have the private key of any trusted
cache, the client will not send the cookie including the authentication token to
such an attacker. For further protection against masquerading caches supplying
forged information to clients or against disclosure of cache contents in case of
cache compromise see Section 7.
6.3
Masquerading Origin Servers
An attacker masquerading as an origin server could attempt to capture infor-
mation from clients and caches, and serve manipulated information to clients
and caches. Since we deploy SSL authentication between caches or clients and
the origin server, the caches or clients can use the contents of the origin server
certiﬁcate after authentication to verify the identity of the origin server. See
attacks 5, 6, 11, and 13 in Figure 4.
6.4
D.O.S. Attacks Against Caches
All publicly accessible server systems are subject to denial of service attacks, e.g.
by ﬂooding. The multi-stage validation of authentication tokens identiﬁes invalid
tokens as early as possible to prevent more expensive authorization operations
such as decryption and signature veriﬁcation. Furthermore, denial of service

142
James Giles et al.
attacks from authenticated users do not result in authentication token validation
overhead since SSL session identiﬁers are used for validating the identiﬁcation
of authenticated users rather than authentication tokens. Additional denial of
service attack prevention should be provided by the content distribution network
infrastructure. See attack 8 in Figure 4.
6.5
Compromised Caches
Threat. A compromised cache could disclose data including content, crypto-
graphic keys, and authentication tokens. Until the cache is known-compromised,
it can also retrieve content from the origin server and send manipulated data to
the origin server. See attacks 9 and 10 in Figure 4.
Implementing Attacks. Remote attacks include exploiting known vulnerabil-
ities of the services running on the caches. Local attacks can occur since caches
are not located within the physical protections of the origin domain.
Defenses. We assume that each cache is monitored by the origin server with
intrusion detection systems (e.g., by IDS running in physically secured copro-
cessors located at the cache [4]). When a compromise is detected, the origin
server neutralizes the compromised cache as described in Section 4. To protect
long-term secrets and integrity of sensitive data, we propose in Section 7 ex-
tensions that achieve end-to-end security between origin servers and clients and
that allows clients to verify the validity of cache certiﬁcates more thoroughly.
7
Extensions to the Architecture
We have implemented our base architecture, making use of servlets on the origin
servers and caches to manage the authentication and update authentication to-
kens stored in cookies. Our analysis of the system allowed us to develop several
enhancements that help improve protection. In this section we describe several
extensions to our base system.
7.1
Cross Domain Cookies
The origin server sets an authentication token stored in a cookie on the client’s
browser using the HTTP protocol, and then the client’s browser presents the
cookie to the cache using the HTTP protocol. This technique only works if the
cache and the origin server are in the same domain (see [12]).
It is desirable to have caches in a diﬀerent domain than the origin server
because this allows users to diﬀerentiate connections to the origin servers from
those to caches, helping to protect against attacks 12 and 14. One technique to
enable cross domain cookies is for the origin server to store a cookie in a shared
directory (we use an LDAP directory) that is accessible to the caches, in addi-
tion to setting the cookie on the client. Then, when the origin server redirects
a client to use a particular cache in a diﬀerent domain, the origin server includes
additional ﬁelds in the redirection URL that indicate the record number of the

Authentication for Distributed Web Caches
143
cookie in the shared directory. The record number is a diﬃcult to guess, large
random number because an attacker guessing this number within the inactiv-
ity timeout period is equivalent to an attacker possessing a valid authentication
token (see Section 4.2 for replay protection). The cache extracts the cookie iden-
tiﬁer, fetches the cookie from the directory, and then sets the updated cookie
(with new inactivity timeout) on the client so that the directory need not be
consulted on future visits. In this way, authentication tokens stored in cookies
can be shared among domains and the caches do not need write access to the
directory. The cookies in the directory lose their validity as soon as the inactivity
timeout expires and should be removed from the directory periodically.
7.2
Secure Content
The basic architecture may not be appropriate for more sensitive data as used
in e-commerce with the requirement for conﬁdentiality or stock quotes with the
need for integrity since the clients cannot detect when a cache has been compro-
mised. To serve highly sensitive content, the origin server only delivers encrypted
content to the cache and provides the client with a means to decrypt this con-
tent when the client authenticates with the origin server. This extension protects
contents in case of cache compromise. However, it requires extra software on the
client in the form of an applet or browser plug-in for encrypting and decrypting
or checking and protecting the integrity of content.
7.3
Cache Veriﬁcation
Any clients interacting with a cache when it is compromised will probably not be
aware of this until the global lifetime of the authentication token expires. To raise
security for clients that are SSL connected to a cache that becomes compromised,
an applet or browser plug-in could be used to validate the cache certiﬁcate at
the origin server before sensitive operations. The extra security oﬀered by this
extension needs to be balanced against the communication overhead.
7.4
Stronger Client Identiﬁcation
To enhance the protection against authentication token replay attacks, we pro-
pose either a client-side program that computes a hash of additional client-
speciﬁc information or a browser enhancement that computes a unique identiﬁ-
cation that is recomputed each time the browser is started. A client environment
hash including this additional information is both included in the authentica-
tion token and available on request to the caches for validating the sender of
an authentication token. Both approaches increase the cost of an attacker to
masquerade as a legitimate client. The attacker must simulate or guess these
parameters to successfully replay an authentication token.

144
James Giles et al.
8
Conclusion
We have described a self-healing, distributed security architecture, which allows
origin servers to maintain control over authentication, while distributing content
through partially trusted content caches. The strongest feature of this architec-
ture is the self-healing feature that allows corrupted caches to be disqualiﬁed
from the content distribution network simply by changing an encryption key,
with no future harm to the security of the system. In addition, the fact that the
user authentication credentials are never seen by the less secure caches reduces
the possibility that the user will be inconvenienced beyond re-authentication
when a cache is known to be compromised. There are a number of tradeoﬀs
of the various aspects of our architecture such as functionality, security, and
usability:
– Less functionality on caches makes them less vulnerable to attacks because
the system is able to keep a smaller amount of sensitive information on the
caches (which must be assumed to become compromised).
– More client speciﬁc information kept in the authentication token implies less
privacy and protection against caches. For example, the client environment
hash requires the cache to obtain user speciﬁc information to validate that
hash, but protects against replay attacks. Authorization information allows
ﬁner-grained access control, but reveals more information to the caches.
– The shorter the time-outs, the smaller the replay-windows but the more
often users need to authenticate interactively; hence less usability and less
beneﬁt from caching.
The presented architecture was prototyped and proved easy to implement. The
overhead for using the architecture is relatively small, especially if clients already
communicate with the origin server before being directed to the content caches.
The cookies, which are transmitted on each request, are limited in size (depend-
ing on the signature system used in the authentication token). Since the ﬁelds
of the authentication token are just encrypted with a shared key mechanism, it
is relatively easy for the content caches to decode and encode the authentication
tokens stored in the cookie when necessary. Additionally, the signature by the
origin server is done only once. No state needs to be kept by the content caches
or the origin server, although state can be kept to improve performance.
Content distribution networks have proved to be viable for distributing con-
tents to improve the user’s experience. However, businesses are now moving from
distributing public content to distributing subscription-based services to increase
their proﬁtability. The shared caching infrastructure and protected contents im-
ply the need for a higher security level which can be oﬀered by our architecture.
References
[1] Akamai Technologies, Inc. Freeﬂow content distribution service.
http://www.akamai.com.
126, 127, 130

Authentication for Distributed Web Caches
145
[2] Speedera. SpeedCharge for Site Delivery. http://www.speedera.com.
126, 130
[3] T. H. Ptacek and T. N. Newsham. Insertion, evasion, and denial of service: Elud-
ing network intrusion detection, 1998. http://secinf.net/info/ids/idspaper/
idspaper.html.
127
[4] J. Dyer, R. Perez, R. Sailer, and L. van Doorn. Personal ﬁrewalls and intrusion
detection systems. In 2nd Australian Information Warfare & Security Conference
(IWAR), November 2001.
127, 128, 129, 142
[5] USENIX. USENIX online library and index. http://www.usenix.org/
publications/library/index.html.
127
[6] K. Fu, E. Sit, K. Smith, and N. Feamster. Dos and don’ts of client authentication
on the web. In The 10th USENIX Security Symposium. USENIX, August 2001.
128
[7] D. Kristol and L. Montulli. HTTP state management mechanism, February 1997.
Request for Comment 2109, Network Working Group.
128, 130, 131, 132
[8] J. Kohl and C. Neuman. Kerberos network authentication service (V5), September
1993. Request for Comment 1510, Network Working Group. 129
[9] O. Kornievskaia, P. Honeyman, B. Doster, and K. Coﬀman. Kerberized credential
translation: A solution to web access control.
In The 10th USENIX Security
Symposium. USENIX, August 2001.
129
[10] Microsoft Corporation. .NET Passport 2.0 Technical Overview.
http://www.microsoft.com/myservices/passport/technical.doc,
October
2001.
129
[11] T. Berners-Lee, R. Fielding, and H. Frystyk.
Hypertext transfer protocol –
HTTP/1.0, May 1996.
Request for Comment 1945, Network Working Group.
130
[12] R. Fielding, J. Gettys, J. Mogul, H. Frystyk, L. Masinter, P. Leach, and
T. Berners-Lee. Hypertext transfer protocol –HTTP/1.1, June 1999.
Request
for Comment 2616, Network Working Group. 130, 142
[13] A. Freier, P. Karlton, and P. Kocher. The SSL protocol version 3.0, November
1996. http://home.netscape.com/eng/ssl3/draft302.txt.
130
[14] T. Dierks and C. Allen. The TLS protocol version 1.0, January 1999. Request for
Comment 2246, Network Working Group. 130
[15] S. Kent and R. Atkinson. Security architecture for the internet protocol, November
1998. Request for Comment 2401, Network Working Group. 130
[16] A. Gulbrandsen, T. Technologies, P. Vixie, and L. Esibov. A DNS RR for speci-
fying the location of services (DNS SRV), February 2000. Request for Comment
2782, Network Working Group. 131, 137
[17] T. Brisco. DNS support for load balancing, April 1995. Request for Comment
1794, Network Working Group. 131, 137
[18] R. Rivest. The MD5 message-digest algorithm, April 1992. Request for Comment
1321, Network Working Group. 133
[19] D. Wallner, E. Harder, and R. Agee. Key management for multicast: Issues and
architectures, June 1999. Request for Comment 2627, Network Working Group.
134
[20] J. Saltzer.
Protection and the Control of Information Sharing in MULTICS.
Communications of the ACM, 17:388–402, 1974.
134
[21] A. Tanenbaum, S. Mullender, and R. Renesse. Using sparse capabilities in a dis-
tributed operating system. In The 6th IEEE Conference on Distributed Computing
Systems. IEEE, June 1986.
134
[22] Apache project. http://www.apache.org.
137

146
James Giles et al.
[23] OpenLDAP project. http://www.openldap.org.
137
[24] E. R. Verheul A. K. Lenstra. Selecting cryptographic key sizes.
http://www.cryptosavvy.com/Joc.pdf.
138
[25] E. Amoroso. Fundamentals of Computer Security Technology. Prentice Hall, 1994.
139

Analysing a Stream Authentication Protocol
Using Model Checking
Philippa Broadfoot and Gavin Lowe
∗
Oxford University Computing Laboratory
Wolfson Building, Parks Road, Oxford, OX1 3QD, UK
{philippa.broadfoot,gavin.lowe}@comlab.ox.ac.uk
Abstract. In this paper, we consider how one can analyse a stream au-
thentication protocol using model checking techniques. In particular, we
focus on the Timed Eﬃcient Stream Loss-tolerant Authentication Pro-
tocol, TESLA. This protocol diﬀers from the standard class of authen-
tication protocols previously analysed using model checking techniques
in the following interesting way: an unbounded stream of messages is
broadcast by a sender, making use of an unbounded stream of keys; the
authentication of the n-th message in the stream is achieved on receipt
of the n + 1-th message. We show that, despite the inﬁnite nature of the
protocol, it is possible to build a ﬁnite model that correctly captures its
behaviour.
1
Introduction
In this paper, we consider how one can capture and analyse a stream authenti-
cation protocol using model checking techniques. In particular, we focus on the
Timed Eﬃcient Stream Loss-tolerant Authentication Protocol, TESLA, devel-
oped by Perrig et al. [11]. This protocol is designed to enable authentication of
a continuous stream of packets, broadcast over an unreliable medium to a group
of receivers; an example application would be to provide authenticated streamed
audio or video over the Internet.
This protocol diﬀers from the standard class of authentication protocols pre-
viously analysed using model checking techniques in the following interesting
way: a continuous stream of messages is broadcast by a sender; the authentica-
tion of the n-th message in the stream is achieved on the receipt of the n + 1-th
message. Thus, receivers use information in later packets to authenticate earlier
packets. We give a complete description of the protocol below. A particular chal-
lenge when modelling and analysing this protocol is that it uses an unbounded
stream of cryptographic keys.
In [2], Myla Archer analyses TESLA using the theorem prover TAME. She
states:
∗This work was partially funded by the UK Engineering and Physical Sciences Re-
search Council and Particle Physics and Astronomy Research Council as part of the
e-science program.
D. Gollmann et al. (Eds.): ESORICS 2002, LNCS 2502, pp. 146–162, 2002.
c
⃝Springer-Verlag Berlin Heidelberg 2002

Analysing a Stream Authentication Protocol Using Model Checking
147
Model checking this kind of protocol is not feasible because an inﬁnite
state system is required to represent the inductive relationship between
an arbitrary n-th packet and the initial packet.
We took this claim as a challenge, and the current paper is the result. In partic-
ular, we construct a ﬁnite CSP model [5, 12], which can be analysed using the
model checker FDR [4], that correctly captures the unbounded stream of keys.
To simulate the unbounded stream of keys in the system, we make use of the
data independence techniques developed by Roscoe [13].
The rest of this paper is organised as follows. In Section 2 we describe the
TESLA protocol in more detail. In particular, we describe the ﬁrst two schemes
(out of ﬁve) presented in [11]. In Section 3 we present a natural but inﬁnite state
model of the basic protocol (Scheme I) within our CSP/FDR framework, so as to
introduce many of the basic techniques. We reduce this system to an equivalent
ﬁnite version in Section 4 by applying the data independence techniques to the
generation of keys; we also present a number of techniques for reducing the
size of the model’s state space, so as to make the analysis more eﬃcient. In
Section 5 we describe how we adapt our model to handle the second version of
the protocol (Scheme II), where each key is formed as the hash of the following
key. We conclude and discuss future work in Section 6. A brief introduction to
CSP is included in Appendix A.
The main contributions of this paper are:
– An application of data independence techniques to the analysis of a stream
protocol, demonstrating the feasibility of applying model checking techniques
to protocols of this class;
– An extension of existing techniques so as to deal with hash-chaining;
– The presentation of a number of state-space reduction techniques.
2
The TESLA Protocol
The TESLA protocol is designed to enable authentication of a continuous stream
of packets over an unreliable medium. One important factor is eﬃciency of
throughput. Therefore, apart from the initial message, the protocol does not
make use of expensive cryptographic primitives such as public key signatures;
instead it makes use of message authentication codes (MACs) and commitments
using cryptographic hashes.
Perrig et al. [11] introduce 5 schemes for the TESLA protocol, each addressing
additional requirements to the previous one. We will consider only the ﬁrst two
of these schemes in this paper.
Informally, Scheme 1 of TESLA works as follows. An initial authentication
is achieved using a public key signature; subsequent messages are authenticated
using MACs, linked back to the initial signature.
In message n −1, the sender S generates a key kn, and transmits f (kn),
where f is a suitable cryptographic hash function, to the receivers, as a com-
mitment to that key; in message n, S sends a data packet mn, authenticated

148
Philippa Broadfoot and Gavin Lowe
using a MAC with key kn; the key itself is revealed in message n + 1. Each
receiver checks that the received kn corresponds to the commitment received
in message n −1, veriﬁes the MAC in message n, and then accepts the data
packet mn as authentic. Message n also contains a commitment to the next
key kn+1, authenticated by the MAC, thus allowing a chain of authentications.
More formally, the initial messages are as follows:1
Msg 0a.
R →S : nR
Msg 0b.
S →R : {f (k1), nR}SK(S)
Msg 1.
S →R : D1, MAC(k1, D1) where D1 = ⟨m1, f (k2)⟩
Msg 2.
S →R : D2, MAC(k2, D2) where D2 = ⟨m2, f (k3), k1⟩.
nR is a nonce generated by the receiver to ensure freshness. The signature on
f (k1) in message 0b acts as an authenticated commitment to k1. When R re-
ceives k1 in message 2, he can be sure that it really was sent by S. This allows R
to verify the MAC in message 1, and so be assured of the authenticity of the
data m1. Further, R is assured of the authenticity of f (k2), the commitment to
the next key.
For n > 1, the n-th message is:2
Msg n.
S →R : Dn, MAC(kn, Dn) where Dn = ⟨mn, f (kn+1), kn−1⟩.
An authenticated commitment to kn−1 will have been received in message n −2;
the receipt of this key assures R of the authenticity of the data received in
message n −1, and also the commitment to kn in that message. Later, when kn
is received in message n +1, R will be assured of the authenticity of the data mn
and the commitment f (kn+1) in message n.
The protocol requires an important time synchronisation assumption, the
security condition: the receiver will not accept message n if it arrives after the
sender might have sent message n + 1 (otherwise an intruder can capture mes-
sage n + 1, and use the key kn from within it to fake a message n). Thus the
agents’ clocks need to be loosely synchronised; this is achieved within an initial
exchange.
Scheme II diﬀers from Scheme I by not generating a stream of fresh keys, but
instead generating a single key km, and calculating each key kn, for n < m, by
hashing km m −n times: kn = f m−n(km). Thus each key acts as a commitment
to the next: when the receiver obtains kn he can verify that f (kn) = kn−1. The
explicit commitment can be dropped and the protocol simpliﬁed to:
Msg 0a.
R →S : nR
Msg 0b.
S →R : {k0, nR}SK(S)
Msg 1.
S →R : m1, MAC(k1, m1).
1 The message numbering scheme is a bit clumsy, but has the advantage that data
packet mn is received in message n and authenticated using key kn.
2 In the original version [11], the n-th MAC is authenticated using f ′(kn), instead
of kn, where f ′ is a second hash function; we can omit the use of f ′ for modelling
purposes.

Analysing a Stream Authentication Protocol Using Model Checking
149
And for n > 1:
Msg n.
S →R : Dn, MAC(kn, Dn) where Dn = ⟨mn, kn−1⟩.
One aim of this second version is to be able to tolerate an arbitrary number of
packet losses, and to drop unauthenticated packets, yet continue to authenticate
later packets.
3
Modelling the Basic Protocol
In this section, we present a natural, but inﬁnite state model of the basic protocol
(Scheme I), explaining some of the techniques we use for modelling and analysing
security protocols within the CSP/FDR framework. In the next section we re-
duce this system to an equivalent ﬁnite version, by applying data independence
techniques.
The basic idea is to build CSP models of the sender and receiver, following
the protocol deﬁnition. We also model the most general intruder who can interact
with the protocol, overhearing, intercepting and faking messages; however, as is
standard, we assume strong cryptography, so we do not allow the intruder to
encrypt or decrypt messages unless he has the appropriate key. These processes
synchronise upon appropriate events, capturing the assumption that the intruder
has control over the network, and so can decide what the honest agents receive.
We then capture the security requirements, and use FDR to discover if they are
satisﬁed. See [9, 10, 16] for more details and examples of the technique.
We will actually consider a slightly simpliﬁed version of the protocol: we omit
the old key kn−1 from the MAC, since it seems to be redundant:
Msg n.
S →R : mn, f (kn+1), kn−1, MAC(kn, ⟨mn, f (kn+1)⟩).
This simpliﬁcation is fault-preserving in the sense of Hui and Lowe [7]: if there is
an attack upon the original protocol, there is also an attack upon this simpliﬁed
protocol; hence if we can verify the simpliﬁed protocol, we will have veriﬁed the
original protocol.
3.1
Sender and Receiver Processes
The sender and receiver nodes are modelled as standard CSP processes that
can perform send and receive events according to the protocol description. We
assume that the intruder has complete control over the communications medium;
hence all communication goes through the intruder, as in Figure 1.
As discussed in Section 2, the TESLA protocol relies upon a time synchroni-
sation between the sender and receiver processes (known as the Security Property
in [11]). We capture this requirement in our models by introducing the special
event tock that represents the passage of one time unit. The processes represent-
ing the sender S and receiver R synchronise upon tock, modelling the fact that

150
Philippa Broadfoot and Gavin Lowe
Sender
Key Manager
Receiver
send
receive
pickKey
getData
Intruder
send
receive
putData
Fig. 1. Overview of the network
the two agents’ clocks are loosely synchronised. This allows R to tell whether it
has received message n before S might have sent message n + 1.
We begin by presenting a process to represent the sender. It would be nat-
ural to parameterise this process by the inﬁnite sequence of keys that are used
to authenticate messages. However, in order to ease the transition to the next
model, we instead arrange for the keys to be provided by an external process,
KeyManager, and for the sender to obtain keys on a (private) channel pickKey
(see Figure 1).
The sender is initially willing to receive an authentication request (mes-
sage 0a) containing a nonce nR; it responds by obtaining a key (k1) from the
key manager, and sending back the initial authentication message (message 0b);
it then waits until the next time unit before becoming ready to send the next
message; if no initial authentication request is received, it simply stays in the
same state:
Sender0(S) =
✷R : Agent, nR : Nonce •
receive . R . S . (Msg0a, ⟨nR⟩) →pickKey?k1 →
send . S . R . (Msg0b, ⟨{f (k1), nR}SK(S)⟩) →tock →Sender1(S, R, k1)
✷
tock →Sender0(S).
The sender obtains the next key (k2), gets the ﬁrst piece of data to be sent (mcurr)
from some channel getData, and then sends the data and the commitment to
the next key, including a MAC using the current key (message 1); it then waits
until the next time unit before becoming ready to send the next message:
Sender1(S, R, k1) =
pickKey?k2 →getData . S?mcurr →
send . S . R . (Msg1, ⟨mcurr, f (k2), MAC(k1, ⟨mcurr, f (k2)⟩)⟩) →
tock →Sendern(S, R, k1, k2).
Subsequent behaviour is very similar to the previous step, except the previous
key (kprev) is included in each message. This is the last time the sender uses this
key, so he can now forget it, modelled by the event forget . kprev; we include this

Analysing a Stream Authentication Protocol Using Model Checking
151
forgetting as an explicit event to ease the transition to the next model:
Sendern(S, R, kprev, kcurr) =
pickKey?knext →getData . S?mcurr →
send . S . R . (Msgn, ⟨mcurr, f (knext), kprev,
MAC(kcurr, ⟨mcurr, f (knext)⟩)⟩) →
forget . S . kprev →tock →Sendern(S, R, kcurr, knext).
It is straightforward to model the process KeyManager that issues keys; it is
parameterised by the sequence of keys that are issued:
KeyManager(xs) = pickKey . head(xs) →KeyManager(tail(xs)).
It is precisely this mechanism of generating fresh keys that needs to be adapted
in Section 4 in order to reduce the model to an equivalent ﬁnite version.
It is convenient to deﬁne the sets AllCommits and AllMacs of commitments
and MACs that the receiver might receive. The receiver is unable to tell imme-
diately whether it has received a valid commitment or MAC, so should also be
willing to receive an arbitrary bit-string, modelled using a special value Garbage:
AllCommits = {f (k) | k ∈Key} ∪{Garbage},
AllMacs = {MAC(k, ⟨m, f ⟩) | k ∈Key, m ∈Packet, f ∈AllCommits}
∪{Garbage}.
The receiver begins by sending a nonce to the sender as an authentication
request (message 0a). It then becomes willing to receive an appropriate initial
authentication message (message 0b); it becomes ready to receive the next mes-
sage in the next time unit. If the initial authentication message is not received
before the end of the current time unit, the receiver should abort.
Receiver0(R, nR) =
⊓S : Agent • send . R . S . (Msg0a, nR) →Receiver ′
0(R, S, nR),
Receiver ′
0(R, S, nR) =
✷fnext : AllCommits •
receive . S . R . (Msg0b, {fnext, nR}SK(S)) →
tock →Receiver1(R, S, fnext)
✷
tock →Abort(R).
The receiver is then willing to receive an appropriate message 1; note that it
should be willing to accept an arbitrary key commitment and MAC, because it
is not yet able to verify either. If the message is not received before the end of
the time unit, the receiver should abort.
Receiver1(R, S, fcurr) =
✷mcurr : Packet, fnext : AllCommits, maccurr : AllMacs •
receive . S . R . (Msg1, ⟨mcurr, fnext, maccurr⟩) →
tock →Receivern(R, S, fcurr, fnext, mcurr, maccurr)
✷
tock →Abort(R).

152
Philippa Broadfoot and Gavin Lowe
The receiver has now reached the phase where it can receive a message each time
unit. For each message received, the authenticity of the previous message is veri-
ﬁed by checking that (i) f (kprev) is equal to the key commitment in the previous
message, fprev; and (ii) the MAC is authentic, i.e. MAC(kprev, ⟨mprev, fcurr⟩) is
equal to the MAC sent in the previous message, macprev. If these conditions
are satisﬁed, then the receiver outputs the data on a channel putData; he then
forgets the old key and moves to the next time unit. If either of these checks
fails, or no message arrives, then the protocol run is aborted.
Receivern(R, S, fprev, fcurr, mprev, macprev) =
✷mcurr : Packet, fnext : AllCommits, kprev : Key, maccurr : AllMacs •
receive . S . R . (Msgn, ⟨mcurr, fnext, kprev, maccurr⟩) →
if f (kprev) = fprev ∧MAC(kprev, ⟨mprev, fcurr⟩) = macprev then
putData . R . S . mprev →forget . R . kprev →
tock →Receivern(R, S, fcurr, fnext, mcurr, maccurr)
else Abort(R)
✷
tock →Abort(R).
If the receiver aborts a protocol run, it simply allows time units to pass.
Abort(R) = tock →Abort(R).
3.2
Intruder Process
The intruder model follows the standard Dolev-Yao [3] model. He is able to act
as other agents, which might or might not behave in a trustworthy way; overhear
all network messages; prevent messages from reaching their intended recipients;
and ﬁnally, fake messages to any agent, purporting to be from any other.
The formal model is built around a set Deductions representing the ways in
which the intruder can deduce new messages from messages he already knows,
for example by encrypting and decrypting with known keys: each element (m, S)
of Deductions represents that from the set of messages S, he can deduce the
message m. The intruder process is parameterised by the set IK of messages
that he currently knows. He can intercept messages (on the channel send) and
add them to his knowledge; send messages that he knows to honest agents (on
the channel receive); and deduce new messages from messages he already knows.
Intruder(IK) =
send?A . B . m →Intruder(IK ∪{m})
⊓
⊓m : IK, A, B : Agent • receive . A . B . m →Intruder(IK)
⊓
⊓(m, S) : Deductions, S ⊆IK • infer . (m, S) →Intruder(IK ∪{m}).
For reasons of eﬃciency, the actual intruder implementation used is built as
a highly parallel composition of many two-state processes, one process for each

Analysing a Stream Authentication Protocol Using Model Checking
153
fact that the intruder could learn. This model was developed by Roscoe and
Goldsmith [15] and is equivalent to the process above.
The intruder’s knowledge is initialised to a set IIK containing values that the
intruder could be expected to know, including all agents’ identities, all public
keys, all data values, his own secret key, a nonce and key diﬀerent from those
used by the honest agents, and the value Garbage that represents an arbitrary
bit-string.
3.3
Overall System and Speciﬁcation
The above processes are combined together in parallel, as illustrated in Figure 1,
and all events except for the getData, putData and tock events are hidden (made
internal):
Agents = Sender0(S)
∥
{|pickKey|}
KeyManager(Ks)) ||| Receiver0(R, N1),
System0 = Agents
∥
{|send,receive|}
Intruder(IIK),
System = System0 \ {| send, receive, pickKey, forget |}.
We now verify that the system meets a suitable speciﬁcation. According
to Perrig et al. [11], TESLA guarantees that the receiver does not accept as
authentic any message mn unless mn was sent by the sender; in fact, mn must
have been sent in the previous time interval.
We capture this by the following CSP speciﬁcation. The speciﬁcation captures
our security requirement by deﬁning the order and timing by which messages
can be sent by the sender and get accepted by the receiver as authentic. We can
use FDR to verify that all traces (i.e. sequences of external events) of the system
are traces allowed by the speciﬁcation.
In the initial state, the speciﬁcation allows the sender S to input a
packet mcurr, before evolving after one time unit into the state Spec′(mcurr).
Alternatively, time is allowed to pass without changing the state of the speciﬁ-
cation.
Spec = getData . S?mcurr →tock →Spec′(mcurr)
⊓
tock →Spec.
Subsequently, the receiver can, but might not, output on putData the value mprev
received in the previous time unit. The sender can input a new value mcurr. These
events can happen in either order, giving the following speciﬁcation.
Spec′(mprev) =
getData . S?mcurr →tock →Spec′(mcurr)
⊓
getData . S?mcurr →putData . R . S . mprev →tock →Spec′(mcurr)
⊓
putData . R . S . mprev →getData . S?mcurr →tock →Spec′(mcurr).

154
Philippa Broadfoot and Gavin Lowe
The model checker FDR can be used to verify that the system meets this spec-
iﬁcation when the key manager is initialised with a ﬁnite sequence of keys; that is,
we verify Spec ⊑System, which is equivalent to traces(System) ⊆traces(Spec).
The following section describes how to adapt the model to deal with an
inﬁnite sequence of keys.
4
A Finite Model of an Inﬁnite System
In this section we show how to transform the previous model of TESLA to an
equivalent ﬁnite model, which can be analysed using FDR. We make use of the
data independence techniques developed by Roscoe [13].
The data independence techniques allow us to simulate a system where agents
can call upon an unbounded supply of fresh keys even though the actual type
remains ﬁnite. In turn this enables us to construct models of protocols where
agents can perform unbounded sequential runs and verify security properties for
them within a ﬁnite check.
The basic idea is as follows: once a key is no longer held by any honest par-
ticipant (i.e., each agent who held the key has performed a corresponding forget
event), we recycle the key: that is, we allow the same key to be subsequently
re-issued to the sender; hence, because there is a ﬁnite bound on the number of
keys held by honest participants, we can make do with a ﬁnite number of keys.
However, at the point where we recycle a key k, the intruder’s memory will
include messages using k, since the intruder overhears all messages that pass on
the network; in order to prevent FDR from discovering bogus attacks, based upon
reusing the same key, we need to transform the intruder’s memory appropriately.
The transformation replaces k with a special key Kb, known as the background
key; more precisely, every message containing k in the intruder’s memory is
replaced by a corresponding message containing Kb. The eﬀect of this is that
anything the intruder could have done using k before recycling, he can now do
using Kb. Thus we collapse all old keys down to the single background key.
Note that this key recycling is a feature of the model rather than of the
protocol. We argue below that it is safe in the sense that it does not lose attacks.
Further, it turns out not to introduce any false attacks.
The technique is implemented within the CSP protocol model by adapting
the key manager process (which issues keys to the sender), so that it keeps
track of which honest agents currently hold which keys. It synchronises on the
messages where agents acquire new keys (pickKey events in the case of the
sender, and receipt of all messages in the case of the receiver) and release keys
(forget events); when a key has been released by all agents, the manager triggers
the recycling mechanism, remapping that key within the intruder’s memory, as
described above. (We need to adapt the model of the receiver slightly so that it
releases all keys when it aborts.) That key can then be re-issued.
We write System′ for the resulting system and System′
0 for the system without
the top level of hiding (analogous to System0). Since System′ is a ﬁnite model,
we can check that it reﬁnes Spec automatically using the model checker FDR.

Analysing a Stream Authentication Protocol Using Model Checking
155
In [14], Roscoe proves that this transformation is sound for protocol models
that satisfy the special condition Positive Conjunction of Equality Tests [8];
informally, a process P satisﬁes this property when the result of an equality test
proving false will never result in P performing a trace that it could not have
performed were the test to prove true. The processes in our model satisfy this
condition.
The critical property that justiﬁes this transformation is
System′ ⊑System.
(1)
If we use FDR to verify Spec ⊑System′, then we can deduce that Spec ⊑System,
by the transitivity of reﬁnement.
Let φ be the function over traces that replaces each key from the inﬁnite
model with the corresponding key in the ﬁnite model, in particular replacing
keys that have been forgotten by all honest agents with the background key Kb.
Equation (1) holds because of the following relationship between System0 and
System′
0, noting that all the keys are hidden at the top level:
traces(System′
0) ⊇{φ(tr) | tr ∈traces(System0)}.
(2)
In turn, property (2) holds because the corresponding property holds for each
individual agent in the system3:
traces(Agents′) = {φ(tr) | tr ∈traces(Agents)},
traces(Intruder ′(IIK)) ⊇{φ(tr) | tr ∈traces(Intruder(IIK))}.
The ﬁrst property holds by construction of the new key manager, and because
the sender and receiver processes are data independent in the type of keys. The
latter holds because of the following property of the deduction system:
(m, S) ∈Deductions ⇒(φ(m), φ(S)) ∈Deductions.
which says that for any deduction the intruder can make in the original system,
he can make the corresponding deduction in the reduced system.
Modelling the protocol directly as above is ﬁne in theory, but in practice leads
to an infeasibly large state space. Below we discuss a number of implementation
strategies that help overcome this problem. For ease of presentation, we discuss
the optimisations independently, although in practice we combined them.
Number of data packets One interesting question concerns the number of distinct
data packets required in our model. It turns out that two distinct packets M0
and M1 suﬃce: if the protocol fails in a setting where more than two packets are
used, then it will also fail in the model with just two packets. To see why this is
the case informally, suppose there were an error in the former case represented by
trace tr; this error must be because the receiver receives a particular packet 
M
3 We write “Agents′” and “Intruder ′” for the new models of the honest agents and
intruder, respectively.

156
Philippa Broadfoot and Gavin Lowe
incorrectly; consider the eﬀect of replacing in tr all occurrences of 
M by M1,
and all occurrences of other packets by M0; it is easy to see that this would be
a trace of the model with just two packets, and would be an error because the
receiver would accept M1 incorrectly. This argument can be formalised, making
use of a general theorem by Lazi´c [8] [12, Theorem 15.2.2].
Splitting protocol messages Large protocol messages comprising many ﬁelds are
expensive to analyse using FDR, because they lead to a large message space
and a large degree of branching in the corresponding processes. Splitting such
messages into several consecutive ones is a simple, yet eﬀective reduction tech-
nique that we frequently use to handle this situation. In the case of TESLA, we
split the main message n into two messages with contents ⟨mn, f (kn+1), kn−1⟩
and ⟨MAC(kn, ⟨mn, f (kn+1)⟩)⟩. This strategy speeds up checking by an order of
a magnitude. Hui and Lowe prove in [7, Theorem 11] that this transformation
is sound in the sense that a subsequent veriﬁcation of the transformed protocol
implies the correctness of the original protocol.
Associating incorrect MACs The model of the receiver allows him to receive an
arbitrary MAC in each message:
Receivern(R, S, fprev, fcurr, mprev, macprev) =
✷. . . maccurr : AllMacs •
receive . S . R . (Msgn, ⟨mcurr, fnext, kprev, maccurr⟩) →. . . ,
where AllMacs contains all valid MACs and also the special value Garbage that
models a bit-string not representing a valid MAC. However, if the MAC does not
correspond to mcurr and fnext then it will be rejected at the next step. Therefore
the above model allows the receiver to receive many diﬀerent incorrect MACs,
all of which are treated in the same way. We can reduce the state space by an
order of magnitude by collapsing all of these incorrect MACs to Garbage, noting
that there is no essential diﬀerence between a behaviour using this value and
a behaviour using a diﬀerent incorrect MAC. Thus we rewrite the receiver to:
Receivern(R, S, fprev, fcurr, mprev, macprev) =
✷. . . maccurr : {MAC(k, ⟨mcurr, fnext⟩) | k ∈Key} ∪{Garbage} •
receive . S . R . (Msgn, ⟨mcurr, fnext, kprev, maccurr⟩) →. . . .
This transformation can be justiﬁed in a similar way to the transformation
that reduced the key stream to a ﬁnite stream. Equation (2) holds when we use
the reduction function φ that maps all incorrect MACs onto Garbage.
Placement of tests One can also obtain a large speed up by careful placement of
the test that veriﬁes the previous MAC. In the earlier model, the receiver was
willing to receive any key in message n, and then checked the previous MAC.
It is more eﬃcient to simply refuse to input any key that does not allow the

Analysing a Stream Authentication Protocol Using Model Checking
157
MAC to be veriﬁed, as follows:
Receivern(R, S, fprev, fcurr, mprev, macprev) =
✷kprev : Key, f (kprev) = fprev ∧MAC(kprev, ⟨mprev, fcurr⟩) = macprev •
✷mcurr : Packet, fnext : AllCommits, maccurr : AllMacs •
receive . S . R . (Msgn, ⟨mcurr, fnext, kprev, maccurr⟩) →. . . .
This reduction can be justiﬁed in a similar way to previous reductions.
Placement of forget actions Recall that the sender and receiver perform forget
events to indicate that they have ﬁnished using a key. It turns out that placing
this forget event as early as possible—i.e. immediately after the last event using
that key—leads to a large reduction in the size of the state space.
Combining events In our previous model, we included diﬀerent events within the
sender for obtaining the key from the key manager and for sending the message
using that key. It is more eﬃcient to combine these events into a single event,
arranging for the sender to be able to use an arbitrary key, and arranging for the
key manager to allow an arbitrary send event using the key it wants to supply
next, and synchronising these two processes appropriately. It is also possible
to combine the getData event with the send event, meaning that the sender’s
environment chooses the value sent. Once the whole system has been combined,
a renaming can be applied for compatibility with the speciﬁcation. It is also
possible to combine the forget events with appropriate messages, although we
have not done this.
Combining these state space reduction techniques with the key recycling
mechanism, we have been able to construct a reduced model of TESLA that
simulates the key chain mechanism. The agent processes are able to perform
an unbounded number of runs; the use of the recycling mechanism gives the
appearance of an inﬁnite supply of fresh keys within a ﬁnite state model.
5
Modelling Key Chaining and Re-Authentication
In this section we show how to adapt the protocol so as to deal with Scheme II,
where explicit key commitments are omitted, but instead each key is formed as
the hash of the following key. Most of the adaptation is straightforward, except
for the key chaining, which we describe below.
5.1
Modelling Key Chaining
Recall that the n-th key kn is calculated by hashing some ﬁxed key km m −n
times. The receiver should accept a key kcurr only after checking that it hashes to
the previously accepted key kprev. Clearly we cannot model this hash-chaining
through explicit modelling of the hashes, for there is no ﬁxed bound on the
number of hashes, and so the state space would be inﬁnite.

158
Philippa Broadfoot and Gavin Lowe
Our model of the hash-chaining instead makes use of the following observa-
tion: kcurr hashes to kprev precisely when kprev and kcurr are keys that were issued
consecutively by the key manager. We therefore introduce a key checker process
that keeps track of the order in which keys were issued, and allows the event
check . k1 . k2 if k1 and k2 were consecutively issued; we then model the checking
of hashes by having the receiver process attempt the event check . kprev . kcurr;
synchronising the two processes upon check events ensures that only appropriate
checks succeed. In more detail:
– The key checker allows an event check . k1 . k2 for fresh keys k1 and k2 pre-
cisely when those keys were consecutively issued.
Hence the receiver will accept a foreground key precisely when it was issued
immediately after the previously accepted key, which corresponds to the case
that it hashes to the previous key. The key checker should allow additional
check events in the situation where the intruder has managed to get the receiver
to accept a key that has already been recycled:
– The key checker allows the event check . Kb . k for fresh or background keys k.
The key checker should at least allow check . Kb . k if k is the key issued immedi-
ately after a fresh key that could correspond to this Kb (because a corresponding
check event would have been possible if we were not using the recycling mecha-
nism). We allow more such check events, which is sound, because it only gives
the intruder more scope to launch attacks; this turns out not to introduce any
false attacks, essentially because if the intruder had managed to get the receiver
to accept Kb, then he would have already broken the protocol anyway.
One might have expected the key checker to also allow events of the form
check . k1 . Kb for fresh keys k1 when the key issued after k1 has been recycled.
However, it turns out that allowing such checks introduces false attacks into the
model: if the key following k1 has been recycled, the intruder can fake a MAC
using the background key, even though there is no corresponding behaviour in the
inﬁnite state model. We avoid this problem by enforcing that keys are recycled
in the same order in which they are issued.
We can then model the receiver process as follows:
Receivern(R, S, kprev, mprev, macprev) =
✷kcurr : Key, MAC(kcurr, mprev) = macprev •
check.kprev.kcurr →forget.R.kprev →
✷mcurr : Packet, maccurr : AllMacs •
receive . S . R . (Msgn, ⟨mcurr, kcurr, maccurr⟩) →. . . .
The reduction, from a model that uses explicit hashes to form the keys to the
ﬁnite model described above, can be justiﬁed as before. Equation (2) holds for the
function φ over traces that replaces keys and inserts check events appropriately.
In particular
traces((Receiver ′ ∥
X
KeyChecker) \ Y ) ⊇{φ(tr) | tr ∈traces(Receiver)},

Analysing a Stream Authentication Protocol Using Model Checking
159
where X = {| check |} and Y = {| getKey |}.
We can also adapt the model of the protocol to allow the receiver to attempt
new initial re-authentications. However, the receiver should use a diﬀerent nonce
for each re-authentication, and so we need to model an inﬁnite supply of nonces.
Doing so is simple: we simply recycle nonces in the same way that we recycled
keys, using a nonce manager that issues fresh nonces to the receiver, and recycles
nonces that are no longer stored by any agent.
6
Conclusions
In this paper, we described how to model and analyse the Timed Eﬃcient Stream
Loss-tolerant Authentication Protocol presented in [11], using model checking
techniques within the CSP/FDR framework. This was initially motivated by
a challenging statement by Archer [2] claiming that this class of protocols is
infeasible to analyse using model checking techniques.
We started by presenting a CSP model for Scheme I of the protocol using
an unbounded supply of fresh cryptographic keys. We then showed how one
can apply the data independence techniques presented in [13, 14] to reduce this
system to an equivalent ﬁnite version. This was achieved by implementing the
recycling mechanism upon a ﬁnite set of fresh keys, creating the necessary illusion
of having an unbounded stream of them. A number of reduction strategies were
developed to keep the state space within a feasible range.
This protocol model was then extended to capture Scheme II. In particular,
we had to develop a technique for modelling unbounded hash-chains. We also
had to adapt the model to allow recycling of nonces, so as to allow an unbounded
number of repeat authentications.
Our analysis showed that the protocol is correct, at least when restricted to
a system comprising a single sender and a single receiver. It is interesting to ask
whether we can extend this result to larger systems:
– It is easy to extend this result to multiple receivers: the intruder would
not learn anything from interactions with additional receivers except more
nonces. These would not help him, because he could only use them in the
same way that he could use his own nonces; further, any failure of the pro-
tocol when executed by multiple receivers would correspond to a failure for
one of those receivers, which we would have discovered in the above analysis.
– It is also easy to extend the result to multiple senders with diﬀerent identities:
if the intruder were able to replay messages from a server S2 and have them
accepted as coming from another server S1, then in the model of this paper
he would have been able to use his own messages in the same way as the
messages of S2 so as to have them accepted as coming from S1, which again
we would have discovered in the above analysis.
– Finally, it is not possible to extend the result to the case where a single
server runs the protocol multiple times simultaneously, issuing diﬀerent data
streams, but using the same key for the initial authentications; indeed in this

160
Philippa Broadfoot and Gavin Lowe
circumstance it is possible for the intruder to cause confusion between these
data streams.
Our model of Scheme II allows the receiver to recover from the loss or corruption
of packets only by performing a repeat authentication. TESLA itself allows the
receiver to recover from d successive lost or corrupted packets, by checking that
the next key received, when hashed d + 1 times, gives the previously accepted
key. It would be interesting to extend our model to consider this case, at least
for a limited range of values of d. However, we believe that it is not possible
to consider this extension for arbitrary values of d, because the system seems
inherently inﬁnite state. Further, even for small values of d, one would run
into problems with a state space explosion; however, a parallel version of FDR,
designed for execution on a network of computers, is now available, which could
be employed on this problem.
We would also like to investigate an alternative method for recycling the
keys: use a ﬁxed set of keys, K1, . . . , Kn for a suitable value of n (in the setting
of Section 5, we need n = 5) and arrange that at each stage the new key issued
is K1; in order for this to work, we would need to arrange a re-mapping of
keys on each tock event, both within the states of the honest agents and the
intruder: we replace K1 with K2, replace K2 with K3, . . . , and replace Kn with
the background key Kb. Thus each key Ki represents the key introduced i −1
time units ago. The eﬀect of this is that each message sent by the sender would
introduce K1, have the MAC authenticated using K2, and disclose K3. This
increased regularity should cause a decrease in the size of the state space.
TESLA is based upon the Guy Fawkes Protocol [1]; it would seem straight-
forward to adapt the techniques of this paper to that protocol. A further protocol
of interest is the second one presented in [11], the Eﬃcient Multi-chained Stream
Signature Protocol (EMSS). TESLA does not provide non-repudiation; EMSS is
designed to fulﬁll this requirement. Non-repudiation is a property that is easily
captured within the CSP/FDR framework; see [6].
References
[1] R. Anderson, F. Bergadano, B. Crispo, J.-H. Lee, C. Manifavas, and R. Needham.
A new family of authentication protocols. Operating Systems Review, 32(4):9–20,
1998.
160
[2] M. Archer. Proving correctness of the basic TESLA multicast stream authenti-
cation protocol with TAME. In Workshop on Issues in the Theory of Security,
2002.
146, 159
[3] D. Dolev and A. C. Yao. On the security of public-key protocols. Communications
of the ACM, 29(8):198–208, August 1983.
152
[4] Formal Systems (Europe) Ltd.
Failures-Divergence Reﬁnement—FDR2 User
Manual, 2000. At http://www.fsel.com/fdr2 manual.html.
147
[5] C. A. R. Hoare. Communicating Sequential Processes. Prentice Hall, 1985.
147
[6] M. L. Hui. A CSP Approach to the Analysis of Security Protocols. PhD thesis,
University of Leicester, 2001.
160

Analysing a Stream Authentication Protocol Using Model Checking
161
[7] M. L. Hui and G. Lowe. Fault-preserving simplifying transformations for security
protocols. Journal of Computer Science, 9(1, 2):3–46, 2001.
149, 156
[8] R. Lazi´c. Theorems for mechanical veriﬁcation of data-independent CSP. D.Phil,
Oxford University, 1999.
155, 156
[9] G. Lowe. Breaking and ﬁxing the Needham-Schroeder public-key protocol using
FDR.
In Proceedings of TACAS, volume 1055 of Lecture Notes in Computer
Science, pages 147–166. Springer Verlag, 1996. Also in Software—Concepts and
Tools, 17:93–102, 1996.
149
[10] G. Lowe and B. Roscoe. Using CSP to detect errors in the TMN protocol. IEEE
Transactions on Software Engineering, 23(10):659–669, 1997.
149
[11] A. Perrig, R. Canetti, J. D. Tygar, and D. X. Song. Eﬃcient authentication and
signing of multicast streams over lossy channels. In IEEE Symposium on Security
and Privacy, pages 56–73, May 2000.
146, 147, 148, 149, 153, 159, 160
[12] A. W. Roscoe. The Theory and Practice of Concurrency. Prentice Hall, 1997.
147, 156
[13] A. W. Roscoe. Proving security protocols with model checkers by data indepen-
dence techniques. In 11th IEEE Computer Security Foundations Workshop, pages
84–95, 1998.
147, 154, 159
[14] A. W. Roscoe and P. J. Broadfoot. Proving security protocols with model checkers
by data independence techniques. Journal of Computer Security, 7(2, 3):147–190,
1999.
155, 159
[15] A. W. Roscoe and M. H. Goldsmith. The perfect ‘spy’ for model-checking crypto-
protocols. In Proceedings of DIMACS workshop on the design and formal veriﬁ-
cation of cryptographic protocols, 1997.
153
[16] P. Ryan, S. Schneider, M. Goldsmith, G. Lowe, and B. Roscoe. Modelling and
Analysis of Security Protocols. Pearson Education, 2001.
149
A
CSP Notation
An event represents an atomic communication; this might either be between
two processes or between a process and the environment. Channels carry sets of
events; for example, c.5 is an event of channel c. The event tock represents the
passage of one unit of time.
The process a →P can perform the event a, and then act like P. The process
c?x →Px inputs a value x from channel c and then acts like Px.
The process P ✷Q represents an external choice between P and Q; the
initial events of both processes are oﬀered to the environment; when an event is
performed, that resolves the choice. P ⊓Q represents an internal or nondeter-
ministic choice between P and Q; the process can act like either P or Q, with
the choice being made according to some criteria that we do not model.
The external and internal choice operators can also be distributed over a set
of processes. ✷i : I • Pi and ⊓i : I • Pi represent replicated external and
nondeterministic choices, indexed over set I . The range of indexing can be re-
stricted using a predicate; for example, ✷i : I , p(i) • Pi restricts the indexing
to those values i such that p(i) is true.
P ∥
A Q represents the parallel composition of P and Q, synchronising on
events in A. P ||| Q represents the parallel composition of P and Q without

162
Philippa Broadfoot and Gavin Lowe
any synchronisation. P \ A represents P with the events in A hidden, i.e. made
internal.

Equal To The Task?
James Heather1 and Steve Schneider2
1 Department of Computing, University of Surrey
Guildford, Surrey GU2 7XH, UK
j.heather@eim.surrey.ac.uk
2 Department of Computer Science, Royal Holloway, University of London
Egham, Surrey TW20 0EX, UK
steve@cs.rhul.ac.uk
Abstract. Many methods of analysing security protocols have been pro-
posed, but most such methods rely on analysing a protocol running only
a ﬁnite network. Some, however—notably, data independence, the strand
spaces model, and the rank functions model—can be used to prove cor-
rectness of a protocol running on an unbounded network.
Roscoe and Broadfoot in [17] show how data independence techniques
may be used to verify a security protocol running on an unbounded net-
work. They also consider a weakness inherent in the RSA algorithm,
discovered by Franklin and Reiter [3], and show that their data indepen-
dence approach cannot deal with an intruder endowed with the ability
to exploit this weakness.
In this paper, we show that neither can the use of honest ideals in the
strand spaces model or the use of rank functions in the CSP model be
easily adapted to cover such an intruder. In each case, the inequality tests
required to model the new intruder cause problems when attempting to
extend analysis of a ﬁnite network to cover an unbounded network. The
results suggest that more work is needed on adapting the intruder model
to allow for cryptographic attacks.
Key words: cryptographic protocols; rank functions; strand spaces; data
independence; inequality tests; RSA; formal methods in security; security
models; security veriﬁcation.
1
Introduction
In [3], Franklin and Reiter present an attack on the RSA algorithm [15], allowing
decryption of a pair of distinct encrypted messages, in the case where the two
ciphertexts stand in a known linear relation and are encrypted under the same
RSA public key with a public-key exponent of 3.
Suppose that we have two encrypted messages
c1 = {m}k
c2 = {am + b}k
and that we know a and b (with a ̸= 0, and not both a = 1 and b = 0), and the
public key k, but we do not know m. Suppose further that the exponent of k is
D. Gollmann et al. (Eds.): ESORICS 2002, LNCS 2502, pp. 162–177, 2002.
c
⃝Springer-Verlag Berlin Heidelberg 2002

Equal To The Task?
163
3, and that the modulus is n. Then we have
c1 = m3 mod n
c2 = (am + b)3 mod n
We can recover m by observing that
m mod n = 3a3bm3 + 3a2b2m2 + 3ab3m
3a3bm2 + 3a2b2m + 3ab3
= b(c2 + 2a3c1 −b3)
a(c2 −a3c1 + 2b3)
Coppersmith et al. generalise this attack in [1] to deal with an arbitrary
public-key exponent e, but the complexity of the attack is O(e log2 e). They
claim that the attack should be practical for exponents of up to 232. In particular,
it is eﬃcient with an exponent of 216 +1, a popular choice in many applications.
This paper explores the consequences of this attack for security protocol
analysis. Speciﬁcally, it investigates the feasibility of tailoring existing protocol
veriﬁcation methods to allow an intruder to take advantage of the RSA attack
described above. For one of these methods—data independence—it has already
been shown by Roscoe and Broadfoot that the attack cannot be incorporated
into the model without destroying the foundation on which the model is built.
We demonstrate in this paper that two other current methods—rank functions
for CSP models, and honest ideals for strand spaces—have surprisingly similar
problems in allowing for this strengthened intruder.
The structure of the paper is as follows. In the next section, we describe
how an intruder might use the RSA attack to break a security protocol, and
give a brief overview of how one might attempt to model this capability when
analysing a protocol. In Section 3, we summarise Roscoe and Broadfoot’s ﬁndings
with regard to the RSA attack and the data independence model. Section 4
describes the rank functions model, and investigates the possibility of including
the RSA attack among the rank functions intruder’s weapons. Section 5 deals
with the same question for the strand spaces model. Finally, in Section 6, we
sum up and draw conclusions.
2
Exploiting the Attack to Break Security Protocols
If concatenations of smaller protocol messages are formed by simple concatena-
tions of the bit strings representing those messages, then the concept of a linear
relation between messages is clear: forming the compound message x.y will in-
volve multiplying the value of x by 2ℓ(y) and adding the value of y (where ‘ℓ(y)’
denotes the length of the bit string representing y). The most natural formu-
lation of this is when m1 = a.X .b and m2 = c.X .d, with the intruder already
knowing a, b, c and d (any of which could be null, but not both a = c and
b = d). For then
m2 = 2ℓ(d)−ℓ(b)(m1 −b −2ℓ(X.b)a) + d + 2ℓ(X.d)c

164
James Heather and Steve Schneider
An intruder who knows {m1}k and {m2}k for some known RSA key k with a low
encrypting exponent can make use of the attack given above to deduce the value
of X .
2.1
Modelling the New Intruder
In each of the three models under consideration, the intruder has complete con-
trol over the entire network, in the style of the Dolev-Yao model [2]. The intruder
may
– allow messages to be sent normally from agent to agent (but take note of
the contents of the message in the process);
– intercept messages and fail to deliver them;
– construct and deliver spurious messages purporting to come from anyone he
pleases.
In this last case, he may send any message that he has already seen in the network
or that he can produce using only messages that he has seen. For instance, if he
has observed NA and NB as separate messages, then he may construct NA.NB
from them and deliver this concatenation. (This concatenation operator is de-
ﬁned to be associative.)
To allow for the extra intruder capability of exploiting the RSA weakness
when conducting protocol analysis, one has to introduce a new deduction rule,
or type of penetrator strand, or the analogue in the model in question. With
a rank functions approach, this will be an extra ⊢rule (explained more fully in
Section 4):
{{a.X .b}k, {c.X .d}k, a, b, c, d} ⊢X
(a ̸= c or b ̸= d)
In the strand spaces model we would extend the deﬁnition of a penetrator strand
to include a new type L (see Section 5):
L Low-exp RSA ⟨−{a.X .b}k, −{c.X .d}k,
−k, −a, −b, −c, −d, +X ⟩for a, b, c, d, X ∈A; a ̸= c or b ̸= d; and k ∈K.
The data independence CSP model would contain a new set of deductions along
the following lines, describing how the intruder, already in possession of mes-
sages from a set Z, can extend his knowledge by utilising the RSA attack to
deduce a new message f . The set deductions(Z) is a set of ordered pairs of the
form ⟨S, m⟩, corresponding to S ⊢m in the rank functions world (Section 3):
deductions(Z) = {⟨{a, b, c, d, k, {a.f .b}k, {c.f .d}k}, f ⟩|
{a.f .b}k ∈Z, {b.f ′.d}k ′ ∈Z, f = f ′, k = k ′, a ̸= c ∨b ̸= d}
This deﬁnition diﬀers from the one given in [17] in three respects, none of great
moment:
1. The name given in [17] is ‘deductions5’.

Equal To The Task?
165
2. The key k has been erroneously omitted in [17] from the set of messages
required in order to make the deduction; it is here included.
3. Roscoe and Broadfoot give the deduction from {a.f }k and {b.f }k, and leave
the more general case (with c and d included) for another (implied) deduc-
tion rule. Here, we give the full case with {a.f .b}k and {c.f .d}k, and assume
that the model will be constructed so that a message of the form a.f can
be parsed as a.f .b with b = ⟨⟩(the null message—and we naturally further
assume that an intruder can produce the null message whenever required).
Note the inequality tests in each case. Without them, the rule would degen-
erate: in the case where a = b = c = d = ⟨⟩, we would have rank functions
deductions of the form
{{X }k} ⊢X
and penetrator strands of the form
⟨−{X }k, +X ⟩
and a subset of deductions(Z) as
D(Z) = {⟨{{f }k, k}, f ⟩| {f }k ∈Z}
In other words, the intruder would be able to break every encryption.
The remainder of this paper deals with the issues involved in attempting to
include this new deduction rule or strand type in the analysis, and in particu-
lar with the eﬀect on proving correctness of a security protocol running on an
unbounded network.
3
Data Independence
Roscoe and Broadfoot [17] have demonstrated how one may analyse a security
protocol running on a ﬁnite network, and then use Lazi´c’s work on data inde-
pendence [11, 12] to extend the results to cover unbounded networks.
Broadly speaking, a CSP process P, parameterised by a data type T, is said
to be data independent with respect to T if the operation of P does not depend
in any way on T. The results that Lazi´c presents give conditions under which
the abstract data type T may be replaced with a small, concrete data type T ′
without aﬀecting the whether P has particular properties.
Roscoe and Broadfoot put forward the concept of a positive deductive system.
A deductive system is positive relative to a type T essentially if it treats all
members of T equivalently, and never requires inequality between two members
of type T in the set from which the deduction is made. They then show that if
the intruder model is built over a positive deductive system, it satisﬁes Lazi´c’s
Positive Conjunctions of Equality Tests (PosConjEqT) property (or a slight
variant), which requires that a process should not perform explicit or implicit
equality tests except for equality tests after which the process halts whenever the

166
James Heather and Steve Schneider
test gives a negative result. For data independence techniques to apply to a model
of a network running a security protocol, we need the process representing the
intruder, and also every process representing an honest agent, to satisfy this
property.
The authors use this result to replace inﬁnite sets of nonces and keys with
ﬁnite sets while ensuring that no attacks will be defeated by the replacement.
Model checking can then be used to show that there are no attacks on a small
system, and the data independence theorems provide the guarantee that there
will consequently be no attacks on an unbounded system.
As Roscoe and Broadfoot make clear, however, giving the intruder the power
to exploit the weakness in low-exponent RSA, by allowing him to make use of
the set deductions(Z) deﬁned earlier, results in an intruder process that does
not satisfy this condition. The intruder must check that he holds two distinct
encryptions in order to perform the attack; and this check (the ‘a ̸= c ∨b ̸= d’
in the deﬁnition of deductions(Z)) violates the PosConjEqT condition.
The necessity of the inequality check in specifying the RSA attack makes
it impossible to work this new intruder capability into the data independence
model without invalidating the results.
4
Rank Functions
In this section, we describe the rank functions approach to security protocol
analysis, as set forth in [18, 9] and discuss the consequences of attempting to
strengthen the rank functions intruder model to endow him with the ability to
use the attack on low-exponent RSA.
Some knowledge of CSP is assumed. For an introduction to CSP, see [10, 16,
19].
4.1
The Network
The network considered in [9] consists of a (usually inﬁnite) number of honest
agents, and one dishonest enemy. The behaviour of an agent J is described as
two CSP process U I
J and U R
J , controlling respectively J’s actions as initiator and
responder. These user processes will vary according to the protocol under con-
sideration; they will consist of communication along channels trans, representing
transmission of a message, and rec, representing reception.
We shall write ‘U’ for the set of user identities on the network (including
identities of any dishonest agents), and ‘N’ for the set of nonces that can be
used in protocol runs.
We deﬁne a ‘generates’ relation ⊢, writing ‘S ⊢m’ to denote that the enemy
may construct message m if he possesses every message in the set S. If m and n
are messages, k is a key, and k −1 is the inverse of k, then ⊢is the smallest

Equal To The Task?
167
relation that satisﬁes
{m, n} ⊢m.n
{m.n} ⊢m
{m.n} ⊢n
{m, k} ⊢{m}k
{{m}k, k −1} ⊢m
and also satisﬁes the closure conditions
m ∈S ⇒S ⊢m
(∀v ∈T • S ⊢v) ∧T ⊢m ⇒S ⊢m
The enemy (already in possession of a set of messages S) is then described by
the recursive deﬁnition:
ENEMY (S) =
trans?i?j?m →ENEMY (S ∪{m}) ✷
✷
i,j,S⊢m
rec!i!j!m →ENEMY (S)
Here the enemy can either receive any message m transmitted by any agent i to
any other agent j along a trans channel, and then act as the enemy with that
additional message; or pass any message m that it can generate from S to any
agent i along its rec channel, remaining with the same information S.
The whole network is then
NET = (|||
J∈U
(U I
J ||| U R
J )) ∥ENEMY
For any given protocol, there will be a (possibly inﬁnite) set of all atoms
that could ever appear in a message of the protocol. This set will encompass all
the user identities, nonces and keys, and any other types of atom used in the
protocol (for instance, timestamps). From this set we can construct the message
space, usually denoted by ‘M’, which is the space of all messages that can be
generated from these atoms.
We use ‘INIT’ to denote the set of atoms known to the enemy right from the
start. Some users will be under the control of the enemy, and hence their secret
keys and all nonces that they might produce will be in INIT; other users will be
free of enemy control, and so their secret keys and nonces will not be in INIT.
4.2
Authentication
For an authentication protocol to be correct, we usually require that a user B
should not ﬁnish running the protocol believing that he has been running with
a user A unless A also believes that he has been running the protocol with B.
(For a discussion of diﬀerent forms of authentication, see [18].) Conditions such

168
James Heather and Steve Schneider
as this can easily be expressed as trace speciﬁcations on NET, requiring that no
event from a set T has occurred unless another event from a set R has previously
occurred.
Deﬁnition 1. For sets R, T ∈M, we deﬁne the trace speciﬁcation R precedes
T as
P sat R precedes T ⇔∀tr ∈traces(P) • (tr ↾R = ⟨⟩⇒tr ↾T = ⟨⟩)
and note that, since all processes are preﬁx-closed, this guarantees that any oc-
currence of t ∈T in a trace will be preceded by an occurrence of some r ∈R.
4.3
Rank Functions
Deﬁnition 2. A rank function, as deﬁned in [7], is a function
ρ : M →{0, 1}
from the message space to the binary-valued set {0, 1}. In addition, we deﬁne
Mρ−= {m ∈M • ρ(m) = 0}
Mρ+ = {m ∈M • ρ(m) = 1}
The point of a rank function is to partition the message space into those
messages that the enemy might be able to get hold of, and those messages that
will certainly remain out of his grasp. Anything with a rank of one will be
something that the enemy might get his hands on; anything of zero rank will be
unavailable to him.
4.4
The Central Theorem from [9]
For a process P to maintain the rank with respect to a rank function ρ, we
mean that it will never transmit any message m with ρ(m) = 0 unless it has
previously received a message m′ with ρ(m′) = 0. Essentially, this means that
the process will never give out anything secret unless it has already received
a secret message.
Deﬁnition 3. We say that P maintains ρ if
P sat rec.U.U.Mρ−precedes trans.U.U.Mρ−
Theorem 1. If, for sets R and T, there exists a rank function ρ : M →{0, 1}
satisfying
1. ∀m ∈INIT • ρ(m) = 1
2. ∀S ⊆M, m ∈M • ((∀m′ ∈S • ρ(m′) = 1) ∧S ⊢m) ⇒ρ(m) = 1
3. ∀t ∈T • ρ(t) = 0
4. ∀J ∈U • USERJ |[ R ]| Stop maintains ρ
then NET sat R precedes T.
The proof is omitted; the interested reader is advised to consult [18, 9]. For
a demonstration of how Theorem 1 can be used to verify security protocols,
see [9].

Equal To The Task?
169
4.5
Strengthening the Intruder Model
The following protocol is based on Lowe’s ﬁxed version [13] of the Needham-
Schroeder Public Key Protocol [14]:
Msg 1.
a →b
: {a.na}PK(b)
Msg 2.
b →a
: {na.nb.b}PK(a)
Msg 3.
a →b
: {nb}PK(b)
Msg 4.
b →a
: nb′
Msg 5.
a →b
: {SK(a).na′.nb′}PK(a)
The ﬁrst three messages of this protocol are exactly the same as those of the
ﬁxed Needham-Schroeder Public Key Protocol. In Message 4, agent b sends
a new nonce nb′ to the initiator; in the ﬁnal message, agent a packages up his
own secret key, a new nonce of his choice, and nb′, and sends all three to b
encrypted under his own public key.
If the RSA weakness is not exploitable, this protocol is secure. Nothing of
moment occurs in the last two messages, except for the sending of a long-term
secret key; and this secret key is never sent except encrypted in such a way that
this key itself is required in order to perform the decryption. (There may be type
ﬂaw attacks on this protocol; but such attacks are easily and cheaply avoided by
following the implementation scheme recommended in [8].)
Even in the presence of an intruder who can exploit the RSA weakness, the
protocol remains secure (see [6] for analysis of this protocol on a small network,
in the presence of such an intruder). No matter how many times the intruder
learns an encryption from a Message 5, he cannot ﬁnd two with a known linear
relation between them, because the value of na′ will be diﬀerent (and unknown)
in each case.
However, it is not possible to ﬁnd a rank function to prove that the protocol
is secure in the presence of such an intruder. Consider the following protocol
run, in which agent A initiates a run with the intruder, agent C:
Msg 1.
A →C : {A.NA}PK(C)
Msg 2.
C →A : {NA.NC.C}PK(A)
Msg 3.
A →C : {NC}PK(C)
Msg 4.
C →A : NC
′
Msg 5.
A →C : {SK(A).NA
′.NC
′}PK(A)
In the fourth message, the intruder chooses some nonce NC
′ to send to A, and
this is reﬂected in the message {SK(A).NA
′.NC
′}PK(A) that C receives as a
result: A chooses a nonce NA
′, and sends it to C along with C’s nonce and A’s
secret key, all encrypted under A’s public key (the inverse of which C does not
hold).
Now, it is clear from the above sequence of messages that we must have
ρ({SK(A).NA
′.NC
′}PK(A)) = 1
for A is willing to send this message out at the end of the run: if the process
representing A is to maintain the rank, then the ﬁnal message that it sends out

170
James Heather and Steve Schneider
during this protocol run must have a rank of one. But what if the intruder had
chosen a diﬀerent nonce, in place of NC
′? What if he had instead chosen NC
′′?
Then A would have sent {SK(A).NA
′.NC
′′}PK(A) instead. We must therefore
also have
ρ({SK(A).NA
′.NC
′′}PK(A)) = 1
—despite the fact that, during the operation of the network, at most one of these
messages can be sent out. Agent A will choose a diﬀerent value for na′ in each
run, and will never use the same value twice; but because C can send either NC
′
or NC
′′, each of the above encryptions could be (independently) available to the
intruder, and so each of them must be given a rank of one. But now, of course,
our new deduction rule tells us that

{SK(A).NA
′.NC
′}PK(A), {SK(A).NA
′.NC
′′}PK(A), NC
′, NC
′′, PK(A)

⊢SK(A)
and so A’s secret key must have a rank of one—as if the intruder could learn
the secret key. He cannot in fact learn it, because he cannot get hold of both of
these messages in order to exploit the weakness. If the intruder really could get
hold of SK(A), then of course he could masquerade as A as often as he pleased,
and the protocol would be broken. The rank functions model cannot, it seems,
be extended to deal with the attack on low-exponent RSA because its inability
to distinguish mutually exclusive possibles from compossibles wreaks havoc with
this type of deduction rule.
The root cause of this is the inequality test in the deduction rule. The failure
to distinguish a case in which either of two (but not both) messages can be sent
out from a case in which both can be sent out seems not to cause a problem when
inequality tests are not permitted in the actions of the agents or the intruder;
but with deduction rules such as the RSA rule, where it is advantageous to the
intruder to possess two distinct messages of the same type, this failure gives the
rank functions model trouble with verifying some correct protocols.
5
Strand Spaces
We next turn to the strand spaces model, as described in [22, 24, 21, 23], and
conduct an analogous line of enquiry: can we ﬁt our strengthened intruder into
the strand spaces model without encountering similar diﬃculties?
5.1
Basic Deﬁnitions
A strand models the actions of an agent on the network, or an atomic action
performed by the penetrator. (The penetrator in the strand spaces model cor-
responds to the intruder in the data independence and rank functions models.)
A regular strand represents a run of the protocol considered from the point of
view of one of the agents involved. A penetrator strand models an atomic ac-
tion performed by the penetrator; for instance, concatenating two messages that

Equal To The Task?
171
he knows, or sending a message out over the network. The techniques available
to the penetrator in the strand spaces model are essentially the same as those
available to the intruder in the rank functions and data independence models.
Deﬁnition 4. We write ‘A’ to denote the space of messages that are communi-
cable across the network. An element t ∈A is called a term.
Deﬁnition 5. The set of cryptographic keys is denoted by ‘K’; this set is a subset
of A.
Deﬁnition 6. The atomic messages that are not keys form the set T. This set
is a subset of A. The sets K and T are disjoint.
Deﬁnition 7. A strand is a sequence of signed terms. We write ‘+t’ to repre-
sent transmission of a term t, with reception written as ‘−t’. A general strand
is denoted by ‘⟨±t1, . . . , ±tn⟩’.
A strand representing an honest agent models the transmissions and receptions
involving that agent in a single run of the protocol.
Deﬁnition 8. The signed terms of a strand are called its nodes. The ith node
of a strand s is denoted by ‘⟨s, i⟩’.
5.2
Strand Spaces and Bundles
A collection of strands may be considered as a graph, with two edge types, ⇒
and →, representing respectively consecutive terms on the same strand, and
communication between two strands.
Deﬁnition 9. If ni+1 immediately follows ni on the same strand, then we write
‘ni ⇒ni+1’.
Deﬁnition 10. If n1 = +a and n2 = −a for some term a ∈A then we write
‘n1 →n2’.
Deﬁnition 11. A strand space is then a collection of strands considered as
a graph ordered by ⇒∪→.
A bundle in the strand space model corresponds to a trace of the whole
network in the rank functions approach. It is a ﬁnite set of strands, ordered by
⇒∪→, on which certain conditions are imposed to ensure that
– reception events never occur unless the corresponding transmission event has
occurred;
– whenever an agent starts a protocol run, he starts from the beginning of the
protocol;
– there is no backwards causation; that is, there are no loops in the graph.
Deﬁnition 12. If C ⊆(→∪⇒) is a ﬁnite set of edges, and N is the set of
nodes that appear on edges in C, then C will be called a bundle if
– whenever n2 ∈N and n2 is a negative node, then there exists a unique n1 ∈
N with n1 →n2 ∈C;
– whenever we have n2 ∈N and n1 ⇒n2, then n1 ⇒n2 ∈C;
– C is acyclic.

172
James Heather and Steve Schneider
5.3
Penetrator Strands
The analogue of the ⊢relation in the rank functions world is a type of strand
known as a penetrator strand. A penetrator strand represents a deduction that
the penetrator may make under ⊢, with diﬀerent types of penetrator strand
corresponding to diﬀerent types of deduction. In addition, since the penetrator
in the strand spaces model has no local state, there will be a type of penetrator
strand to represent duplicating a term in order to be able to use it twice; and
since every network message that is transmitted is always received by another
strand, we introduce one ﬁnal type of penetrator strand to model hearing and
disregarding a message.
Just as the rank functions intruder starts by knowing everything in INIT, so
the penetrator will have some initial knowledge. However, in the strand spaces
model, only the keys known to the penetrator are stated: these keys form the
set KP.
Deﬁnition 13. A penetrator strand is one of the following:
M Text message ⟨+t⟩for t ∈T.
F Flushing ⟨−x⟩for x ∈A.
T Tee ⟨−x, +x, +x⟩for x ∈A.
C Concatenation ⟨−x, −y, +xy⟩for x, y ∈A.
S Separation ⟨−xy, +x, +y⟩for x, y ∈A.
K Key ⟨+k⟩for k ∈KP.
E Encryption ⟨−k, −x, +{x}k⟩for x ∈A, k ∈K.
D Decryption ⟨−{x}k, −k −1, +x⟩for x ∈A, k ∈K.
This deﬁnition is parameterised by the set T.
Recall from Deﬁnition 12 that for each negative node n2 in a bundle there
must be exactly one positive node n1 such that n1 →n2; that is, every reception of
a message must be matched to exactly one transmission. The purpose of strands
of type F is to allow the penetrator to ‘absorb’ transmissions of messages that
he does not wish to use; strands of type T perform the corresponding rˆole of
replicating messages that the penetrator wants to transmit more than once.
Recent work on the strand spaces model in [4, 5] has dropped this require-
ment of matched transmissions and receptions, allowing communications to be
sent to zero nodes, one node or many nodes. Consequently, the need for pen-
etrator strands of types F and T has been abrogated. This slightly simpliﬁes
the notation, but does not alter the expressive power of the language. Here, we
follow the notation of [21] and keep these types of penetrator strand; but this
decision does not aﬀect the analysis.
5.4
Regular Strands
A regular strand corresponds to a run of the protocol by an honest agent, or
the actions of a trusted server. It will usually be, as is the case in our model,
a speciﬁc instantiation of a strand template containing free variables. The formal
deﬁnition of a regular strand is very simple:

Equal To The Task?
173
Deﬁnition 14. A regular strand is any strand that is not a penetrator strand.
5.5
Subterms and Ideals
We sometimes wish to talk about the subcomponents of a compound message.
The concept in the strand spaces model that allows us to do so is that of a sub-
term. The notation ‘t1 ⊏t2’ will imply, informally speaking, that t1 can be found
somewhere in the makeup of t2); but note that we shall not have k ⊏{m}k unless
k ⊏m.
The ⊏relation is deﬁned in terms of ideals. Ideals in the strand spaces model
allow us to talk about all messages containing a particular submessage, when
encryption is restricted to a particular set of keys.
Deﬁnition 15. Let k ⊆K be a set of keys, and I ⊆A a set of terms. Then we
say that I is a k-ideal of A if
1. hg ∈I and gh ∈I whenever h ∈I and g ∈A;
2. {h}k ∈I whenever h ∈I and k ∈k.
We write ‘Ik[h]’ for the smallest k-ideal containing h. Similarly, if S is a set of
terms, then ‘Ik[S]’ denotes the smallest k-ideal that includes S.
Deﬁnition 16. We say that h ⊏k m if m ∈Ik[h]. When h ⊏K m, we drop the
subscript and write simply ‘h ⊏m’, and say that h is a subterm of m.
5.6
Honesty
A node is an entry point to a set if it transmits a term without having already
transmitted or received any term in the set.
Deﬁnition 17. If, for a node n of a strand s, and a set I ⊆A,
– n is a positive node;
– the term of n is in I ;
– no previous node on s has its term in I
then we say that n is an entry point to I .
The idea of an honest set is an important one. A set is honest relative to
a given bundle if the penetrator can never break into the set except by pure
ﬂuke: either he guesses the right nonce (on an M strand), or he guesses the right
key (on a K strand).
Deﬁnition 18. A set I ⊆A is honest relative to a bundle C if whenever a node
of a penetrator strand s is an entry point to I , then s is a strand of type M or
type K.
Although honesty is deﬁned for sets in general, it is when the concepts of an
honest set and an ideal are conjoined that they are most powerful. The main
theorem from [21] gives conditions under which an ideal is honest.

174
James Heather and Steve Schneider
Theorem 2. Suppose
1. C is a bundle over A;
2. S ⊆T ∪K;
3. k ⊆K;
4. K ⊆S ∪k−1.
Then Ik[S] is honest.
Proof. The proof is omitted; it may be found in [21].
5.7
Strengthening the Intruder
Theorem 2 is the lynchpin of a cutting-edge strand space analysis (see [21] for
examples of the theorem in action); and, in fact, strand spaces proofs prior to the
introduction of this theorem were also conducted along essentially these lines.
The proof of the theorem is by a case-by-case analysis of the possible types
of penetrator strand, demonstrating that for each type, with the exceptions of
types M and K, no strand can provide an entry point to Ik[S].
Extending the strand spaces model to cover the RSA attack will involve
introducing a new type of penetrator strand
L Low-exp RSA ⟨−{a.X .b}k, −{c.X .d}k,
−k, −a, −b, −c, −d, +X ⟩for a, b, c, d, X ∈A; a ̸= c or b ̸= d; and k ∈K
modelling the penetrator’s new-found ability to exploit the weakness. Before we
can use this extended model to prove correctness of security protocols even in
the presence of our new penetrator, we are required to show that the standard
strand spaces results still hold good. In particular, this means that we need to
extend the proof of Theorem 2 to include penetrator strands of type L.
This, sadly, cannot be done. Let C be a bundle over A, and let S = {X , K −1},
with K ∈K, k = K \ {K} and X ∈T. Now let a, b, c, d ∈T \ S. The conditions
of Theorem 2 are now satisﬁed, and so we should have that Ik[S] is honest.
However, suppose that C contains the strand
⟨−{a.X .b}K , −{c.X .d}K, −K, −a, −b, −c, −d, +X ⟩
representing the penetrator’s deduction of X by making use of the RSA attack.
Now the ﬁrst two terms lie outside the ideal, because K /∈k; the next ﬁve are
not in the ideal because they are not contained in S. But the last term, +X ,
is positive, and X ∈Ik[S]. So this strand provides an entry point for Ik[S],
contradicting the deﬁnition of honesty.
It appears, then, that there is no simple way to extend the strand spaces
model to cover this RSA attack without falsifying the central result used to
verify protocols. It is, again, the implicit inequality test that lies at the root.
For the concept of an honest ideal captures the notion of the set of messages
such that it is undesirable to allow the penetrator to learn any of the messages
in the set: if the penetrator can pick up a single message from inside Ik[S], for

Equal To The Task?
175
some suitable set k, he will be able to learn the values of some members of S.
Allowing for the RSA weakness would require specifying a set H such that the
penetrator should be able to pick up no more than one message from H —or,
in other words, if he can learn two messages c1 and c2 from H , and we have
that c1 ̸= c2, then he can make use of the attack. Incorporating this into the
strand spaces model would require substantial reworking of the basic tools of
the trade; and, in any case, it is not at all clear how it could be done.
6
Conclusion
The data independence model cannot be easily adapted to allow the intruder to
take advantage of the weakness in low-exponent RSA. We have shown in this
paper that both the rank functions method for CSP models, and the use of
honest ideals in the strand spaces model, suﬀer from the same inﬂexibility, and
that it is the implicit inequality checks that are the cause in each case.
The fact that all three models fail to allow for this attack, and all for the
same reason, suggests that the mechanisms by which these three approaches to
security protocol analysis manage to deal with unbounded networks have more
similarities than meet the eye.
Each approach involves identifying an inﬁnite set of messages that can all be
treated in the same way. The data independence model requires a data type T
such that replacing one member of T for another throughout the system should
not defeat any attacks. The rank functions model identiﬁes sets of messages
such that the intruder gains no more information from learning all members
of the set than he does from learning just one, and so simply keeps track of
whether the intruder knows the whole set or none of it. The strand spaces model
operates in much the same way: an honest ideal Ik[S] is the set of messages any
of which can be used by the penetrator to learn something from the set S; and
so the requirement is always to show that none of the messages in the ideal
can be picked up by the penetrator. What none of the models can allow for
is the idea that allowing the intruder to learn one of the messages might be
acceptable, whereas to reveal two (or more) will result in a breach of security—
or, more appropriately, that it is dangerous to allow the intruder to learn two
such messages c1 and c2 if and only if c1 ̸= c2. We simply cannot deal with the
inequality test.
We suspect that Roscoe and Broadfoot’s notion of a positive deductive system
will accurately limit what can be analysed not just with the data independence
model, but with each of the three models considered in this paper. Formalising
this idea within the rank functions and strand spaces models, and proving that
positive deductive systems do indeed deﬁne the scope of what can be veriﬁed
using these models, is the subject of ongoing research.
We are interested to note that Stoller’s approach [20] to identifying bounds on
the number of protocol runs required for an attack can be extended to deal with
the inequality tests required for the attack on RSA considered here, under certain
mild conditions on the nature of the protocol under consideration. This implies

176
James Heather and Steve Schneider
that Stoller’s method is diﬀerent at a deeper level from the three approaches
considered in this paper, and gives hope that alternative techniques for CSP
models and strand spaces might be developed that can handle inequality tests.
Acknowledgement
Thanks are due to Bill Roscoe for his helpful comments on a draft of this paper.
References
[1] D. Coppersmith, M. Franklin, J. Patarin, and M. Reiter. Low-exponent RSA with
related messages. Lecture Notes in Computer Science, 1070, 1996.
163
[2] Danny Dolev and Andrew C. Yao. On the security of public key protocols. IEEE
Transactions on Information Theory, 29(2), 1983.
164
[3] M. Franklin and M. Reiter. A linear protocol failure for RSA with exponent three.
1995. Presented at the Rump Session of Crypto ’95, Santa Barbara, CA.
162
[4] Joshua D. Guttman and F. Javier Thayer F´abrega. Authentication Tests. In Pro-
ceedings of the 2000 IEEE Symposium on Security and Privacy. IEEE Computer
Security Press, May 2000.
172
[5] Joshua D. Guttman and F. Javier Thayer F´abrega.
Protocol Independence
through Disjoint Encryption. Proceedings of 13th IEEE Computer Security Foun-
dations Workshop, pages 24–34, June 2000.
172
[6] James A. Heather. Exploiting a weakness of RSA. Master’s thesis, Oxford Uni-
versity Computing Laboratory, September 1997.
169
[7] James A. Heather.
‘Oh! . . . Is it really you?’—Using rank functions to verify
authentication protocols. Department of Computer Science, Royal Holloway, Uni-
versity of London, December 2000.
168
[8] James A. Heather, Gavin Lowe, and Steve A. Schneider.
How to avoid type
ﬂaw attacks on security protocols. Proceedings of 13th IEEE Computer Security
Foundations Workshop, June 2000.
169
[9] James A. Heather and Steve A. Schneider.
Towards automatic veriﬁcation of
authentication protocols on an unbounded network. Proceedings of 13th IEEE
Computer Security Foundations Workshop, June 2000.
166, 168
[10] C. A. R. Hoare. Communicating Sequential Processes. Prentice-Hall International,
1985.
166
[11] Ranko S. Lazi´c. A semantic study of data-independence with applications to the
mechanical veriﬁcation of concurrent systems. PhD thesis, University of Oxford,
1997.
165
[12] Ranko S. Lazi´c. Theorems for Mechanical Veriﬁcation of Data-Independent CSP.
Technical report, Oxford University Computing Laboratory, 1997.
165
[13] Gavin Lowe. Breaking and Fixing the Needham-Schroeder Public-Key Protocol
using FDR. In Proceedings of TACAS, volume 1055 of Lecture Notes in Computer
Science, pages 147–166. Springer-Verlag, 1996.
169
[14] Roger M. Needham and Michael D. Schroeder. Using encryption for authentica-
tion in large networks of computers. Communications of the ACM, 21(12):993–
999, December 1978.
169
[15] Ron L. Rivest, Adi Shamir, and Leonard Adleman.
A method for obtaining
digital signatures and public-key cryptosystems. Communications of the ACM,
21(2):120–126, 1978.
162

Equal To The Task?
177
[16] A. W. Roscoe. The Theory and Practice of Concurrency. Prentice-Hall Interna-
tional, 1998.
166
[17] A. W. Roscoe and Philippa J. Broadfoot. Proving security protocols with model
checkers by data independence techniques. Journal of Computer Security, 1999.
162, 164, 165
[18] Steve A. Schneider. Verifying authentication protocols in CSP. IEEE TSE, 24(9),
September 1998.
166, 167, 168
[19] Steve A. Schneider. Concurrent and real-time systems: the CSP approach. John
Wiley & Sons, 1999.
166
[20] Scott D. Stoller. A bound on attacks on authentication protocols. In Proceedings of
the 2nd IFIP International Conference on Theoretical Computer Science (TCS),
Kluwer, 2002.
175
[21] F. Javier Thayer F´abrega, Jonathan C. Herzog, and Joshua D. Guttman. Honest
ideals on strand spaces. Proceedings of 11th IEEE Computer Security Foundations
Workshop, June 1998.
170, 172, 173, 174
[22] F. Javier Thayer F´abrega, Jonathan C. Herzog, and Joshua D. Guttman. Strand
spaces: Why is a security protocol correct? Proceedings of the 1998 IEEE Sym-
posium on Security and Privacy, May 1998.
170
[23] F. Javier Thayer F´abrega, Jonathan C. Herzog, and Joshua D. Guttman. Mixed
strand spaces. Proceedings of 12th IEEE Computer Security Foundations Work-
shop, pages 72–82, June 1999.
170
[24] F. Javier Thayer F´abrega, Jonathan C. Herzog, and Joshua D. Guttman.
Strand spaces: Proving security protocols correct. Journal of Computer Security,
7(2,3):191–230, 1999.
170

TINMAN: A Resource Bound Security Checking
System for Mobile Code⋆
Aloysius K. Mok and Weijiang Yu
Department of Computer Sciences, University of Texas at Austin
Austin, Texas 78712 USA
{mok,wjyu}@cs.utexas.edu
Abstract. Resource security pertains to the prevention of unauthorized
usage of system resources that may not directly cause corruption or leak-
age of information. A common breach of resource security is the class of
attacks called DoS (Denial of Service) attacks. This paper proposes an
architecture called TINMAN whose goal is to eﬃciently and eﬀectively
safeguard resource security for mobile source code written in C. We cou-
ple resource usage checks at the programming language level and at the
run-time system level. This is achieved by the generation of a resource
skeleton from source code. This resource skeleton abstracts the resource
consumption behavior of the program which is validated by means of
a resource usage certiﬁcate that is derived from proof generation. TIN-
MAN uses resource-usage checking tools to generate proof obligations
required of the resource usage certiﬁcate and provides full coverage by
monitoring any essential property not guaranteed by the certiﬁcates. We
shall describe the architecture of TINMAN and give some experimental
results of the preliminary TINMAN implementation.
1
Introduction
Mobile codes are becoming widely deployed to make wide-area networks exten-
sible and ﬂexible by adding functionality and programmability into the nodes
of the network. Mobile code which is written on one computer and executed on
another, can be transmitted in the form of either binary or source code. The
former includes Java Applet, ActiveX, or executables for speciﬁc platforms. The
latter includes applications written in high-level languages like C, scripts such as
shell ﬁles or JavaScript embedded in HTML ﬁles and speciﬁc languages designed
for active network [1] such as PLAN [2] and Smart Packets [3].
Not surprisingly, there is increasing demand on host systems to provide se-
curity mechanisms for shielding users from the damages caused by executing
untrustworthy mobile code. One of the serious concerns of mobile code secu-
rity is resource bound security. Resource bound security pertains to resource
consumption limits and the prevention of unauthorized use or access to system
⋆This work is supported in part by a grant from the US Oﬃce of Naval Research
under grant number N00014-99-1-0402 and N00014-98-1-0704.
D. Gollmann et al. (Eds.): ESORICS 2002, LNCS 2502, pp. 178–193, 2002.
c
⃝Springer-Verlag Berlin Heidelberg 2002

TINMAN: A Resource Bound Security Checking System for Mobile Code
179
resources that may not directly cause corruption or leakage of protected infor-
mation. Failure to properly delimit resource consumption by untrusted mobile
code may deny legitimate users access to system resources, as is in the case of
Denial Of Service (DoS) attacks.
In this paper, we propose TINMAN, a platform-independent architecture
whose goal is to eﬃciently and eﬀectively perform resource bound security checks
on mobile code. Although the TINMAN architecture is not restricted to mobile
code developed in a particular language, we analyze mobile C code in our pro-
totype system since many mobile programs are written in C and lack security
checks. For example, a user might try to extend his system software with an
external software component, or download and install open source software and
applications that are often in the form of C source code, from both trusted and
untrusted sources. Thus, we want to validate the TINMAN architecture ﬁrst for
C code; we believe that the TINMAN architecture should be applicable directly
to the mobile code in scripts and byte-code as well.
Most of the work on mobile code security has not addressed resource bound
security or is not eﬀective when it is applied to a general-purpose languages
such as C. For example, Java Sandbox [5], Smart Packets [3] and PLAN for
PLANet strive to prevent abuse by mobile code at the programming language
level by limiting a user’s ability to gain access to system resources through
language design. This usually comes at the price of reduced expressiveness of
the programming language and may turn oﬀprogrammers who are unwilling
to adopt a restrictive language just for their mobile applications. Also, these
approaches cannot guarantee the resource bound consumption which is essential
to the hosting systems; for instance, they cannot detect buggy or malicious code
that may fall into an inﬁnite loop and use all the CPU cycles and possibly all
system memory. An alternative approach to attaining resource bound security is
to enforce certain security policies that restrict the resource utilization of mobile
code execution at the run-time system level. Run-time checking of resource usage
alleviates the drawback of strict reliance on language-level mechanisms but it is
costly, and certain methods of access control do not apply since mobile code is
not stored on the computer it is executed on. In addition, not all these systems
provide resource utilization prediction and thus resource-usage security policies
are diﬃcult to enforce.
We believe that in order to ensure resource bound security for mobile source
code, every hosting system should be endowed with a capability to accept us-
age speciﬁcation (via policy-based resource bounds) and to monitor (via policy
enforcement) the execution of untrusted mobile programs. This policy speciﬁ-
cation to enforcement linkage should be established in a way that cannot be
spoofed, or else the system should not be trusted to transport mobile code. Our
approach is to endow mobile source code with a veriﬁable certiﬁcate that spec-
iﬁes its resource consumption behavior. The key idea underlying the TINMAN
architecture, however, is the recognition that exact resource usage cannot in gen-
eral be derived a priori by compile-time static analysis (unless, say, the halting
problem is decidable, which it is not) and that a combination of compile-time

180
Aloysius K. Mok and Weijiang Yu
(oﬀ-line) and run-time (on-line) techniques is needed to link policy speciﬁcation
to enforcement. The real hard issue is to determine the relative roles of oﬀ-line
vs. on-line analysis. To answer this question, we adopt the following principle:
“Check the veriﬁable. Monitor the unveriﬁable.”
TINMAN applies the above principle. Resource bound is analyzed and veriﬁed
oﬀ-line from user-supplied information and security policy, to the extent that it is
practical to do so, and security enforcement is performed by coupling language-
level and run-time system mechanisms to monitor what cannot be established by
oﬀ-line analysis. The oﬀ-line and run-time mechanisms together provide complete
coverage and guarantee resource bound security. TINMAN advances the state of
the art by pushing the limit in the automation of ﬁne-grain resource consumption
security checks, by deploying a suite of tools for resource utilization prediction,
certiﬁcate generation and validation, and run-time event matching.
In this paper, we focus on the system design and implementation of the
TINMAN architecture. The next section describes the methodology of system
design. The architecture of TINMAN is given in Section 3, followed by some
details of the oﬀ-line checker and on-line checker. Some experimental results are
given in Section 7. Section 8 compares TINMAN with related work. Concluding
remarks and future work are in Section 9.
2
Methodology
As noted in the previous section, there are two aspects to the resource security
problem: policy and enforcement. Resource security policy establishes resource
usage constraints that mobile programs must satisfy. In TINMAN, the security
policy to be satisﬁed by mobile program is given by a speciﬁcation in three
parts: (1) resource usage for each service that may be called by a program and
provided by a hosting system; (2) resource limit for each program; (3) a proof
system consisting of axioms and inference rules for the interpretation of the
speciﬁcation.
Resource security enforcement prevents a mobile program from violating the
resource security policy. It pertains to the authorization of resource usage and the
limitation of actual resource usage by a program. Enforcement may be performed
dynamically, by monitoring the real-time resource utilization of a mobile program
at run time. Complete reliance on dynamic enforcement may be too expensive
in general as to be practical. An alternative way to perform enforcement is to
check the certiﬁcation of resource bounds for a certiﬁed program, similar to Proof
Carrying Code (PCC) [6] and Typed Assembly Language (TAL) [7].
Ideally, if certiﬁcation is trustworthy, the code is considered resource usage
safe. The system may then grant resources to the program to complete its ex-
ecution, and there is no run-time checking (as in PCC). This is in general not
possible for resource bound security since exact resource usage is usually de-
termined dynamically and cannot in general be derived by compile-time static
analysis. Our approach is to a combine oﬀ-line veriﬁcation and on-line monitor-
ing.

TINMAN: A Resource Bound Security Checking System for Mobile Code
181
TINMAN deploys an oﬀ-line analysis tool that attempts to derive tight re-
source usage bounds for user-supplied codes that may be loaded and run on
hosting systems. In an ideal world, the oﬀ-line analyzer would be able to de-
rive an exact resource usage bound and output a correctness proof of the bound
which constitutes the usage certiﬁcate. Short of getting an exact bound, a tight
bound may be obtained interactively with the programmer giving help in the
form of assertions about the program’s behavior.
An on-line analysis tool resides on the code recipient where the code is run.
We note that if veriﬁcation is possible at compile-time, all the on-line analyzer
needs to do is to check the usage certiﬁcate, and dispense with monitoring. Since
checking a proof or validating a usage certiﬁcate via, say, a trusted certiﬁcation
authority is in general much easier than deriving the proof from scratch (just as it
may take exponential time to solve an NP-complete problem but only polynomial
time to check the correctness of a solution), the run-time overhead is relatively
small. In the case not all veriﬁcation conditions can be veriﬁed at compile-time
or if programmer input is required, the oﬀ-line analyzer also outputs all the
assertions, and the on-line checker will automatically monitor the validity of the
assertions at run time. In this way, a network node determines with certainty
that a piece of mobile code is compliant with the resource security policy.
3
System Architecture of TINMAN
Figure 1 shows the TINMAN architecture and the dependencies among the sys-
tem’s components. We now give a high-level description of each component and
explain how they ﬁt together to perform resource security check for mobile code.
3.1
Oﬀ-Line Checker
The goals of the oﬀ-line checker are to provide programmers with a tool for
resource bound prediction and usage certiﬁcate generation for their mobile pro-
gram. As shown in Figure 2, resource usage prediction involves timing analysis
On−line Checker
Network Traffic Analyzer
Off−line Checker
Source Code
Run−time Events
Install and Run
Traffic Data
Resource Security Policy
Skeleton & Usage Certifcate
Source Code with Resource
Mobile Code Generator
Mobile Code Recipient
Fig. 1. TINMAN Architecture

182
Aloysius K. Mok and Weijiang Yu
Flow Information
Source Code
User Bounds and
Arguments Requests
Resource Security Policy
Resource Skeleton
Usage Certificate
Parser
Timing Analysis 
Ceritificate Generation
Resource Bounds Prediction
Pre−Compiling
Proof Generation
Resource Specification
Generation
& Live Memory Demand Analysis
Fig. 2. Overview of Static Resource Bound Prediction and Veriﬁcation
and live-memory demand analysis. The output is a set of annotations and as-
sertions we call a resource skeleton. The resource skeleton is parameterized such
that the exact values of the bounds can be determined at the code recipient
site. A resource speciﬁcation generation component automatically translates the
resource skeleton into a speciﬁcation consisting of a group of predicates that will
be veriﬁed by a proof generation module to produce a usage certiﬁcate.
Once mobile code is analyzed by the oﬀ-line checker, it is ready to be sent to
the recipient site.
3.2
On-Line Checker
The on-line checker validates the annotations inserted by the oﬀ-line checker,
and detects any violation against the security policy by the imported mobile
code before or during its execution. A typical on-line checker session involves
resource usage bound calculation, resource skeleton veriﬁcation, usage certiﬁ-
cate validation and run-time assertion monitoring. The on-line checker will be
discussed further in Section 6.
3.3
Network Traﬃc Analyzer
Unlike the CPU and memory, network resource abuse in general may aﬀect
entities outside of a host executing the mobile code and as such cannot be read-
ily detected by checking resource usage limits on the system where the mobile
programs run. It is diﬃcult to statically check exact network bandwidth usage
at the language level because of the dependency on protocol implementation.
TINMAN instead prevents such attacks at the sender by performing run-time
network traﬃc analysis in addition to static checks on network-related service
routines. Run-time network resource checking is not the subject of focus in this
paper, and interested readers are referred to [8] for more details.
4
Resource Bound Prediction
Resource bound prediction is done at the source code level. The oﬀ-line checker
modiﬁes the Broadway Compiler [9], a source-to-source translator for ANSI C,

TINMAN: A Resource Bound Security Checking System for Mobile Code
183
to parse the source code and output ﬂow information to resource analysis mod-
ules. To strengthen the resource prediction results, the oﬀ-line checker performs
type-checking to assure type safety. We make the following assumptions about
the mobile code discussed throughout this paper: No cast, pointer arithmetic,
address operation and recursive calls are used. All variables must be declared
with types. Variables including structure ﬁelds must be initialized before being
used. In addition, all array subscripting operations are checked for bound viola-
tion. The type-checking is done statically. A failed type-check will result in the
termination of the oﬀ-line checker in which case no resource skeleton and usage
certiﬁcate will be generated. We shall formalize the semantics of this subset of
ANSI C in section 5.
4.1
Timing Analysis
Timing analysis attempts to determine the worst-case execution time (WCET)
of a program. Prediction of WCETs is an extensive research area, and substantial
progress has been made over the last decade. Most practical WCET techniques,
however, cannot be directly applied here since they are tied to particular target
architectures and do not take mobile code into consideration. The oﬀ-line checker
addresses this issue by combining a source level timing schema approach with
a policy-based, parameterized WCET computation method.
The basic idea of the timing schema approach [10] is that the execution time
is determined by basic blocks and control structures of a program. For example,
the execution time of the assignment statement A : a = b −c; can be computed
as T (A) = T (b)+T (−)+T (c)+T (a)+T (=), where T(X) denotes the execution
time of the item X, referred to as an atomic block, i.e., basic component in the
abstract syntax tree of a program. The execution time for the control statements
is computed similarly,
T(if(exp0) S1; else S2 ) = T(exp0) + T(if) + max(T(S1), T(S2)).
T( for(i = 0; i < N; i + +) stmt;) = T(i) + T(=) + (N + 1) · (T(i) + T(<)
+ N · (T(stmt) + T(for) + T(i) + T(++)).
assuming i is not changed in the loop’s stmt. The timing of compound statement
is calculated recursively based on the simple statements.
We note that the above approach fails when a program contains loops whose
iteration bounds cannot be directly obtained, or when system service calls are
invoked; the latter is common for a mobile application. The oﬀ-line checker uses
a number of techniques to alleviate these problems.
Loops
For loops, a pre-compiling analysis of the source code discovers constant loop
bounds or handles dependencies between loop iteration variables of the nested
loops. For example, consider the loop
while(m < n) {S1; n = n + I0; S2}
If both m and n are constants at the entry point, the increment/decrement of n

184
Aloysius K. Mok and Weijiang Yu
(i.e. I0) is a constant and dominates the next loop iteration, and the number of
loop iterations can be statically determined. Therefore, the accurate loop bound
can be calculated using constant propagation and the dominance relationship1
analysis. For a nested loop where the number of iterations in the inner loop
depends on the value of the index in the outer loop, we rely on techniques
similar to [12] to give a tight prediction on loop iterations.
For loops where loop bounds are diﬃcult to deduce automatically (or even
do not exist as in an inﬁnite loop) but are straightforward for a programmer
to deduce, our tool will ask the programmer to input the asserted loop bound
which will be monitored.
System Service Calls
The execution time of system service calls in a program is not known without
information of the remote system where the program runs. Our approach is
to specify the resource usage of a service call in a policy rule in the form of
pre and post conditions deﬁned in a formal logic (to be described in detail in
section 5). The resource consumed by a service is parameterized given the ranges
of its arguments. Exact values are determined at the remote site. This approach
is ﬂexible since the policy is conﬁgurable for a speciﬁc platform and run-time
environment. As a result, only parameter information and the system call itself
are need in the resource skeleton by the oﬀ-line checker.
4.2
Live Memory Demand Analysis
Accurate memory allocation prediction is a non-trivial task, and usually re-
quires considerable cost [13]. The memory used by a mobile code consists of
three spaces: stack space, dynamically-loaded code, and heap space. Stack space
consumption is largely due to recursive calls which are currently prohibited in the
current prototype of TINMAN. On the other hand, the size of a piece of mobile
code is generally small and it is relatively trivial to compute the heap memory
size of the mobile code. The heap space, however, is exploited by dangling point-
ers and may be allocated by malicious programmers. TINMAN analyzes the
heap space allocation, referred to as the live memory demand at the language
level. Considering the possible actions of malicious codes on memory, our eﬀorts
are focused on memory allocation requests in a program.
Consider for example, the memory demand of an assignment statement S:
head = (Node∗)malloc(sizeof(Node) ∗exp);
is M(S) = sizeof(Node) * Value(exp). The sizeof(Node) can be determined at the
entry point of S, while exp may not. In addition, the memory demand computa-
tion may be complicated if the malloc() statement is in a loop, and/or the paired
free statement is not properly given. Taking all these into account, we adopt
a general approach for live memory demand prediction in the following steps:
(1) Analysis of memory allocation statements: All memory allocation statements
1 A dominates B iﬀall paths from the start node to B intersects A [11].

TINMAN: A Resource Bound Security Checking System for Mobile Code
185
are identiﬁed such as malloc(), calloc(), memalign(). The size (in bytes) of max-
imum memory request is either calculated automatically using ﬂow analysis or
provided by the programmer if it cannot be statically bounded, for example, the
value of exp in S depends on program input data. (2) Path analysis of mem-
ory freed: The live memory demand may be overestimated if we simply sum all
memory allocation requests. We check the dominance relationship between pairs
of memory allocation and free statements, and within the same basic blocks or
compound statements to tighten the live memory bound (3) Call chain analysis:
The demanded memory size is increased throughout the call chain. Therefore,
we compute recursively the memory demand at the points of interest, such as
loops and procedure calls.
4.3
Resource Skeleton
The timing and memory bound information obtained by either automatic anal-
ysis or from programmer input need to be maintained for further use. We use
a resource skeleton to annotate the information to help bound the resources.
Basically, a resource skeleton can be viewed as an abstraction of a program in
regard to its resource consumption. The resource skeleton of a program also
makes it possible for the on-line checker which is invoked by the code recipient
to detect violations against the resource security policy.
The Resource skeleton is created along with the construction of the ﬂow graph
of a program derived from its syntax tree. A node in a ﬂow graph is called a
task. Task types include basic blocks, conditional statements, loops, user-deﬁned
procedure calls and system service calls. The tasks of basic blocks and service
calls are primitive tasks. The ﬂow graph is hierarchical which means a program
is a sequence of tasks, and each task (except primitive tasks) can be expended
into a ﬂow graph. To reduce unnecessary annotations and proof generation based
on these annotations, a sequence of primitive tasks are combined in the same
resource skeleton. We illustrate the construction of resource skeleton with an
example, shown in Figure 3.
          n = getrecord(group, sender);
          if (m−>nodes != NULL){
             for (i = 0; i < m−>length; i++)
          }
          else {
          m−>time = n.time − 10;
          } 
          m = &n;
 C2:   /* must find it to continue */
 L3:      /* send a copy every way */
 B4:          routefornode(&n, m−>nodes[i]);
             /* or deliver to application */
              delivertoapp(&n, dpt);
 B5:       m−>node = (Node*)malloc(sizeof(Node)*10);
 B1:   /* look up forwarding record */
B1
C2
L3
B4
B5
 
/*@ Exit: T[T[C2] M[MC2] */
/*@ Entry: T[t0] M[m0] */
/*@ B1:T[T[Entry]+Tgetrecord+13]   M[M[Entry]+Mgetrecord]*/
M[M[B1]+L3_li*Mroutefornode] */
M[M[B1]+50+Mdellivertoapp] */
/*@ C2: T[Max[T[L3], T[B5]]   M[Max[M[L3], M[B5]] */
/*@ B4:T[T[B1]+6+L3_li*(7+Troutefornode)]
/*@ B5: T[T[B1]+4+Tmalloc+Tdelivertoapp]
M[M[B1]+64*Mroutefornode]*/
/*@ L3: L3_lb = 64 T[T[B1]+472+64*Troutefornode]
Fig. 3. Example of Resource Skeleton

186
Aloysius K. Mok and Weijiang Yu
The task in the ﬂow graph is labeled with its type (e.g. L = Loop task)
followed by a global counter value. The solid line marks the execution sequence
of related tasks while a broken line means a task expansion. Resource annotation
for each task is enclosed in “/∗@ . . . ∗/” which can be identiﬁed by the on-line
checker and be ignored by a standard compiler. T[exp] and M[exp] represent the
time and memory-demand bounds of the corresponding task.
For example, the execution time of C2 is the maximum execution time of
T[L3] and T[B5], where T[L3] and T[B5] can be recursively computed from
the resource annotations for L3 and B5. The loop bound for L3 is provided
by the programmer. We note that, for example, the CPU and memory usage
bound for B1 are given only with the name of the service call (T getrecord). The
actual bound value calculation is performed at the remote site with instantiated
resource security policies.
5
Generating Usage Certiﬁcate
The mobile program is transported with a usage certiﬁcate since a code recipient
cannot trust the resource skeleton which may be corrupted. Our basic strategy
to establish the correctness of certiﬁcates is to use an assertional programming
logic such as Hoare Logic. Our approach is to translate the resource skeleton
into a resource safety speciﬁcation in a formal logic, and generate a certiﬁcate
by checking for compliance with security policy. In this section, we ﬁrst intro-
duce the resource speciﬁcation, using extended Hoare triples. Next, we present
a proof system by formulating the formal semantics of a subset of the C lan-
guage (mentioned in section 4) in terms of a set of axioms and inference rules.
The program correctness on satisfying a speciﬁcation can be proved within the
proof system.
5.1
Resource Speciﬁcation
For a task T, a resource speciﬁcation for T is an extended Hoare triple {P} T
{Q}, where assertion P is the precondition, and assertion Q is the postcondition
which holds if P is true and T terminates. An assertion is a ﬁrst-order logic
formula which is typically a conjunction of predicates over program variables,
time and memory usage and terminational judgment. In order to translate a re-
source skeleton annotation for task T into an assertion, we ﬁrst deﬁne the type
T ime to be the nonnegative reals and Memory to be the nonnegative integers
as respectively domains of time and memory size values. Three special variables:
now of type T ime, mem of type Memory, and terminate of type boolean are
introduced. These variables denote the moment, the allocated memory and the
termination before, if they are in P, or after, if in Q, the execution of T, respec-
tively. The initial value of now is t0, and mem m0. With this terminology, the
resource skeleton can be represented as logical speciﬁcations. For example, the
speciﬁcation for task B1 in Figure 3 is

TINMAN: A Resource Bound Security Checking System for Mobile Code
187
{now = t0 ∧mem = m0 ∧terminate}
B1
{now <= t0 + Tgetrecord + 13∧
mem <= m0 + Mgetrecord ∧terminate}
And the speciﬁcation for loop task L3 is as follows. Note that L3 lb is the user-
provided loop bound.
{L3 lb = 64∧now <= t0+Tgetrecord+17∧mem <= m0+Mgetrecord∧terminate}
L3
{now <= t0 + Tgetrecord + 472 + 64 · Troutefornode∧
mem <= m0 + Mgetrecord + 64 · Mroutefornode ∧terminate}
It should be pointed out that the speciﬁcation for a service call contains a pre-
condition, if any, over the ranges of its arguments and expressions of its running
time and memory usage. It is published as part of the security policy.
5.2
Proof System
Similar to Jozef Hooman’s framework [14] for sequential programs, we construct
a proof system for resource speciﬁcation by formalizing programming constructs,
or tasks in our cases. The tasks are axiomatized by inference rules and axioms.
The rules in the framework are proved independently and published as part of
security resource policies as well.
Clearly, the formalization of resource speciﬁcation and the proof system re-
quires mechanical support. TINMAN uses the Prototype Veriﬁcation System
(PVS) [15] to implement its logical framework. In order to formulate resource
speciﬁcation into the PVS speciﬁcation language, our approach is to identify
programs with their semantics, i.e., the relations on states. A state contains
a mapping of program variables to values, current time, allocated memory, and
termination indicator. We developed an extended and modiﬁed version of con-
struction rules deﬁned in [14]. All tasks are deﬁned with regard to their resource
speciﬁcations. For example, the task B1 in the previous example is deﬁned in
PVS as follows,
P0 : [State->bool] =
((LAMBDA s : state) : now(s) = t0
AND mem(s) = m0 AND terminate(s) )
srvc1: program = SRVC(Tgetrecord, Mgetrecord)
bb1
: program = BB(13);
B1
: program = seq(srvc1, bb1);
Q0 : [State->bool] = (LAMBDA s :
now(s) = t0 + Tgetrecord + 13 AND
mem(s) = m0 + Mgetrecord AND
terminate(s))
{P0}B1{Q0} : THEOREM
((FORALL s0, s1 : state):
P0(s0) AND B1(s0, s1) IMPLIES Q0(s1))
The complete set of axioms and inferences rules and program constructs deﬁni-
tion can be found in [8].

188
Aloysius K. Mok and Weijiang Yu
5.3
Proof Generation
A PVS speciﬁcation for resource skeleton is obtained by applying the program
construction rules. To prove the speciﬁcations, one would like to construct proofs
interactively using the PVS prover system. For a mobile program, however, we
aim at generating a proof or certiﬁcate as automatically as possible. PVS pro-
vides a mechanism for automatic theorem proving by composing proof steps into
proof strategies. We note that the speciﬁcation for a diﬀerent type of program
constructs may require diﬀerent proof strategies.
For primitive constructs like basic block tasks and service call tasks, we
have deﬁned a strategy that performs a sequence of built-in proof steps. In
this strategy, the theory deﬁnition including the task deﬁnitions and assertions
is expended by automatic rewrite rules, and then Skolemization and Decision
procedure are invoked repeatedly until the theory is proved. For example, the
theorem {P0}B1{Q0} in the previous example is proved using this strategy by
ﬁrst auto-rewriting P0, Q0, B1, seq, srvc1, and bb1, and then repeatedly invoking
prover commands ASSERT and SKOSIMP* until the theorem is proved.
The proof strategy for proving a choice task speciﬁcation, say {P}if b then T1
else T2 {Q}, is more complicated than the primitive tasks. The Rule 2 for choice
tasks in the proof system illustrates the general steps of the strategy. Brieﬂy,
we need ﬁrst to prove the two corollaries, {P ∧b} COND(tb, mb); T1 {Q}, and
{P∧¬b} COND(tb, mb); T2 {Q} by applying some other strategies and then invoke
the prover command LEMMA Rule 2 and the quantiﬁer rule. The construction of
those strategies and other proof steps in the strategy for choice tasks are non-
trivial, and will not be discussed any further here.
The strategy for sequential tasks and loop tasks are constructed similarly
in consideration of the corresponding rules in proof system. The strategy for
loop task, however, involves a loop invariant and an assertion that holds if the
loop terminates. In order to generate them automatically, we simplify a loop
invariant by introducing an auxiliary loop index, say li for a loop L, where li ∈
[0, loopboundL] and is increased by 1 in each loop iteration. The loop invariant
is constructed with li and the speciﬁcation (precondition and postcondition) of
task L, since we only need to assure the correctness of the resource bound, not
what the program actually does.
Using these proof strategies, the resource speciﬁcation for an annotated pro-
gram is proved automatically. PVS outputs the proof onto a text ﬁle which
constitutes the usage certiﬁcate for the program. However, due to the large size
of the proofs written in the built-in PVS speciﬁcation logic, we further shorten
the detailed usage certiﬁcate by only keeping the strategies and related param-
eters required to produce the proof. We refer the shortened usage certiﬁcate as
a certiﬁcate skeleton. Finally, the annotated mobile program only requires the
certiﬁcate skeleton to be transferred to the remote site.

TINMAN: A Resource Bound Security Checking System for Mobile Code
189
6
On-Line Checker
The on-line checker performs limited static veriﬁcation by validating the sup-
plied resource skeleton and usage certiﬁcate, and it detects any violation against
security policy on resource utilization limits.
Figure 4 shows the major steps of static on-line veriﬁcation. We note that the
calculation of actual resource bound, given instantiated policies on service calls
is performed before the validation of the resource skeleton. It enables detection
of any violation against a resource usage limit at an early stage since if there is
a violation, the mobile program will not be trusted, and no further veriﬁcation
is warranted.
The purpose of the resource skeleton validation is to check for consistency
between the source code and the resource skeleton. Annotations are inserted at
the appropriate points. Speciﬁcally, the tasks, programmer-provided information
if necessary for loop bound, memory allocations and service call arguments are
annotated, and there are no further manually inserted annotations.
The resource speciﬁcation generation is the same as that in the oﬀ-line
checker. It outputs a speciﬁcation consisting of predicates on the resource skele-
ton. The full usage certiﬁcate is restored from the certiﬁcate skeleton. The proof
checker veriﬁes the supplied usage certiﬁcate to conform with the speciﬁcations
within the PVS system. In our implementation, the proof checking is as simple
as a validation run of PVS in a batch mode which automatically reruns all proofs
in the usage certiﬁcate. An invalid usage certiﬁcate will generate errors which
can be caught by the proof checker by examining the run log ﬁle.
After the veriﬁcation of the resource skeleton and validation of usage certiﬁ-
cate, the only untrusted part are the user-provided assertions on loop bounds
and the ranges of function arguments. The on-line checker needs to monitor
these values at run-time. In order to do this, the on-line checker translates them
into related assertions for checking the range of the values of untrusted data.
A run-time exception will be raised if any violation of policy is detected. Dy-
namic resource utilization monitoring is a tried concept used by much previous
Resource Bound Calculation
Resource Skeleton Validation
Handling
Exception
Run−time Events Insertion
Resource
Security
Policy
Resource Skeleton
Mobile Code w/
& Usage Certificate Skeleton
violation?
Certificate Restoration
Certificate Verification
Resource Specification Generation
instantiation
violation?
invalid?
Fig. 4. Structure of Static On-line Veriﬁcation

190
Aloysius K. Mok and Weijiang Yu
work. Our approach avoids much of the dynamic resource utilization monitoring
since the resource bound safety is guaranteed by static veriﬁcation, and run-time
checking is required only on a few programmer-provided annotations.
7
Experimental Results
In this section, we present some experimental results obtained by using our tools.
We have translated the following two mobile programs used by other research
groups [1, 4] into C. The multicast subscribe installs forwarding pointers in each
router (member of a group) it traverses so it can receive messages sent to the
group. It has six services calls, one loop and six tasks. The multicast data simply
routes itself along a distribution tree, and it has ﬁve services calls, two loops and
eight tasks.
We ﬁrst measured the code size augments due to insertion of annotations
and the usage certiﬁcate. The results are shown in Table 1. We note that the
increased size depends on the number of tasks in a program, but is not directly
related to the size of the program. This is because, for example, multicast data
has more complicated control structure and more tasks. It also explains the
reason that multicast data has a larger usage certiﬁcate. We also observed that
the certiﬁcate size is signiﬁcantly decreased by up to 94.3% by using a certiﬁcate
skeleton.
Table 2 shows the cost of oﬀ-line certiﬁcate generation and on-line annotation
veriﬁcation and certiﬁcate checking. The certiﬁcates of both example programs
are generated completely automatically. However, oﬀ-line certiﬁcate validation
and on-line certiﬁcate check result in an order of magnitude slower than cer-
Table 1. Code Size with Annotations (in bytes)
Program
multicast
subscribe
multicast
data
Original Size
1508
1113
Resource Skeleton
316
462
Certiﬁcate Skeleton
569
1087
Full Certiﬁcate
9963
14595
Increase(%)
58.7
139.2
Table 2. Cost of oﬀ-line checker and on-line checker
Program
multicast subscribe multicast data
Speciﬁcation Generation
36.5ms
58.5ms
Oﬀ-line Cert. Construction + Validation
0.83s + 11.06s
1.45s + 23.85s
On-line Anno. Veriﬁcation
62.9ms
51.1ms
On-line Cert. Check
11.09s
22.72s

TINMAN: A Resource Bound Security Checking System for Mobile Code
191
Table 3. Resource Utilization Measurement
Program
Predicted
WCET
Observed
WCET
(w/o ann.)
Observed
WCET
(w/ ann.)
Pess. Predicted
Mem(B)
Actual
Mem(B) Pess.
multicast subscribe 74.08ms
57.56ms
81.73ms
29.8%
932
804
15.9%
multicast data
1456ms
1363ms
1503ms
6.8%
12800
8192
56.3%
tiﬁcate construction. The overhead is two-fold. First, a certiﬁcate consists of
PVS rules that are interpreted by the PVS prover interactively. Second, current
proof strategy for loops and compound choice tasks involves catch-all prover
commands like GRIND which is timing consuming. Table 2 also indicates that an-
notation veriﬁcation time (e.g resource usage calculation and annotation check)
is quite small and negligible compared with certiﬁcate check time.
Table 3 shows the timing and memory analysis results of the programs. The
loop bounds of multicast data are automatically calculated while the bound of
multicast subscribe are given by a programmer. The pessimism of multicast data
is only 6.8% because we have obtained a tight loop bound. On the contrary, the
actual loop counts in multicast subscribe depend on the conditions of inner break
statements. Therefore, it is very conservative and is oﬀby 29.8%. The overhead
of running a mobile program with annotations comes from run-time monitoring
of programmer-provided information and the communication between the anno-
tated program and the on-line checker during the execution. All of them have
small monitoring overheads.
The pessimism of memory analysis is caused by the actual execution path
that aﬀects memory allocation statements. For example, in multicast data, the
malloc() inside a loop is only executed upon satisfaction of some condition, and
its total execution time is also decided by the loop bound. The experiments
show, however, the actual memory allocated does not exceed the predicted live
memory demand that is essential to our goal of resource security.
8
Related Work
Previous resource usage safety eﬀorts for mobile code usually enforce the security
policy in the run-time system to limit the resource utilization of mobile code.
Smart Packets [3] checks the CPU and memory usage of active packets written
in Sprocket and enforces limits on the number of instructions executed, amount
of memory used, and access to MIB variables. The KeyNote in PLANet has
a similar mechanism [16]. These active network systems do not provide tools
to do source-code-level checking concerning resource usage, and have signiﬁcant
restrictions on languages features and thus limit the expressiveness of mobile
code. Some researchers have developed extensions for more expressive security
policies. For instance, Naccio [17] speciﬁes security policies for Java and Win32

192
Aloysius K. Mok and Weijiang Yu
using a speciﬁcation language. Java programs are transformed to call wrapper
functions instead of the original library code in order to enforce safety policies.
In PCC, a code producer creates a formal safety proof to prove adherence to
the safety rules that guarantee safe behavior of programs. A remote host depends
on proof validation techniques to check that the proof is valid so that the foreign
code is safe to execute. The diﬃculty of generating proofs for large programs
and more interesting policies are the diﬃculties in the application of PCC. In
practice, PCC has been used to verify low-level safety properties, and it does
not address the resource bound security problems in terms of resource behavior
prediction and program termination. Unlike PCC, TINMAN concentrates on
resource security assurance in high-level to prevent DoS-like attacks and buggy
or malicious codes with inﬁnite loops or improper arguments to services calls
that are not addressed by previous work. The resource security policy is ﬂexible
and conﬁgurable at code recipient site. In addition, the proof system constructed
using the PVS system makes it easier for proof construction and validation for
more complicated mobile applications.
9
Conclusions
In this paper we have presented TINMAN, a resource bound security checking
system for mobile code. The system detects malicious mobile source code that,
once installed and executed, may consume inordinate amounts of resources such
as CPU, memory and network bandwidth, as is common in DoS (Denial-Of-
Service) attacks.
TINMAN provides multiple levels of protection on resource security at both
compile time and run time, at both the source-code level as well as run-time
system level. It has been implemented by a set of tools that support resource
bound prediction and certiﬁcate generation and validation. TINMAN exploits
programmer input but does not depend on it for ensuring resource security; in-
correct programmer input about resource bounds will be checked and detected
against the resource skeleton associated with the mobile code, and the usage cer-
tiﬁcate is validated against given resource security policies. An on-line checker
tool is used to detect malicious modiﬁcation of resource bound annotations and
certiﬁcate validation. This enables any violation of resource utilization with re-
spect to security policy to be detected as early as possible before the execution
of the mobile code. Together, the oﬀ-line and on-line checkers provide complete
coverage, following the guideline of proving what can be veriﬁed and monitoring
what cannot be veriﬁed.
In this paper, we consider mobile programs written in C with active network
benchmarks. However, our framework is extensible and applicable, in an even
simpler style, for other programming languages for mobile applications such as
Javascript of which source codes are usually embedded in a HTML ﬁle. By
gaining experience in TINMAN, we shall hopefully be able to customize the
framework for some version of byte-code for C. Our plan is to use the script

TINMAN: A Resource Bound Security Checking System for Mobile Code
193
from the veriﬁer session as the usage certiﬁcate so that the proof can be checked
on-line eﬃciently.
References
[1] Wetherall, D., Guttag, J., Tennenhouse, D.: ANTS: A Toolkit for Building and
Dynamically Deploying Network Protocols. IEEE OPENARCH (1998) 117–129
178, 190
[2] Hicks, M. W., Kakkar, P., Moore, J. T., Gunter, C. A., Nettles, S.: PLAN: A Packet
Language for Active Networks. International Conference on Functional Program-
ming (1998) 86–93
178
[3] Schwartz, B., Jackson, A. W., Strayer, W. T., Zhou W., Rockwell, D., Partridge,
C.: Smart Packets: Applying Active Networks to Network Management. ACM
Transactions on Computer Systems 18:1 (2000) 67–88
178, 179, 191
[4] Kornblum, J., Raz, D., Shavitt, Y.: The Active Process Interaction with its En-
vironment. Lucent Technology (1999)
190
[5] Fritzinger, J. S., Mueller, M.: Java Security. Sun Microsystems white paper (1996)
179
[6] Necula, G. C.: Proof-Carrying Code. POPL’97 (1997) 106–119
180
[7] Morrisett, G., Walker, D., Crary, K., Glew, N.: From System F to Typed Assembly
Language. IEEE Symposium on Principles of Programming Languages (1998) 180
[8] TINMAN Project. http://www.cs.utexas.edu/wjyu/tinman
182, 187
[9] Guyer, S. Z., Jim´enez, D. A., Lin, C.: Using C-Breeze. Department of Computer
Sciences, The University of Texas, (2002).
http://www.cs.utexas.edu/users/lin/cbz
182
[10] Park, C. Y., Shaw, A. C.: Experiments with a program timing tool based on source-
level timing schema. Computer J 25:5 (1991) 48–57
183
[11] Aho, A. V., Sethi, R., Ullman, J. D.: Compilers: Principles, Techniques, and Tools.
Addison Wesley (1986)
184
[12] Healy, C., Sjdin, M., Rustagi, V., Whalley, D.: Bounding Loop Iterations for
Timing Analysis. IEEE Real-Time Applications Symposium (RTAS’98) (1998)
12–21
184
[13] Unnikrishnan, L., Stoller, S. D., Liu, Y. A.: Automatic accurate stack space and
heap space analysis for high-level languages. Computer Science Department, In-
diana University TR 538 (2000)
184
[14] Hooman, J.: Correctness of Real Time Systems by Construction. FTRTFTS: For-
mal Techniques in Real-Time and Fault-Tolerant Systems LNCS 863, Springer-
Verlag, (1994) 19–40
187
[15] Owre, S., Rushby, J., Shankar, N.: PVS: A prototype veriﬁcation system. 11th
International Conference on Automated Deduction, Lecture Notes in Artiﬁcial
Intelligence, Springer Verlag (1992) 748–752
187
[16] Alexander, D. S., Anagnostakis, K. G., Arbaugh, W. A., Keromytis, A. D., Smith,
J. M.: The Price of Safety in an Active Network. University of Pennsylvania MS-
CIS-99-02 (1999)
191
[17] Evans, D., Twyman, A.: Flexible Policy-directed Code Safety. The IEEE Sympo-
sium on Research in Security and Privacy, Research in Security and Privacy IEEE
Computer Society Press (1999) 32–45
191

Conﬁdentiality-Preserving Reﬁnement is
Compositional – Sometimes
Thomas Santen1, Maritta Heisel2, and Andreas Pﬁtzmann3
1 Institut f¨ur Softwaretechnik und Theoretische Informatik
Technische Universit¨at Berlin, Germany
santen@acm.org
2 Institut f¨ur Praktische Informatik und Medieninformatik
Technische Universit¨at Ilmenau, Germany
maritta.heisel@prakinf.tu-ilmenau.de
3 Fakult¨at Informatik, Technische Universit¨at Dresden, Germany
pfitza@inf.tu-dresden.de
Abstract. Conﬁdentiality-preserving reﬁnement describes a relation be-
tween a speciﬁcation and an implementation that ensures that all con-
ﬁdentiality properties required in the speciﬁcation are preserved by the
implementation in a probabilistic setting. The present paper investigates
the condition under which that notion of reﬁnement is compositional, i.e.
the condition under which reﬁning a subsystem of a larger system yields
a conﬁdentiality-preserving reﬁnement of the larger system. It turns out
that the reﬁnement relation is not composition in general, but the con-
dition for compositionality can be stated in a way that builds on the
analysis of subsystems thus aiding system designers in analyzing a com-
position.
1
Introduction
In systems and software engineering, the consent is growing that secure systems
cannot be built by adding security features ex post to an existing implementa-
tion but that “security-aware” engineering of systems and software must take
security concerns into account, starting from requirements engineering through
architectural and detailed design to coding, testing, and deployment.
It is obvious that only some kind of divide-and-conquer approach makes
building non-trivial systems feasible. Such an approach must support decom-
posing a system into subsystems, implementing those subsystems largely inde-
pendently of each other, and ﬁnally composing the implementations of those
subsystems to make up an implementation of the entire system. More speciﬁ-
cally, such an approach decomposes a system speciﬁcation into speciﬁcations of
subsystems, and it composes (correct) implementations of subsystem speciﬁca-
tions to yield a (correct) implementation of the system speciﬁcation.
In this setting, the question arises whether correctness of the subsystem im-
plementations with respect to their speciﬁcations is suﬃcient to guarantee that
composing the subsystem implementations yields a correct implementation of
D. Gollmann et al. (Eds.): ESORICS 2002, LNCS 2502, pp. 194–211, 2002.
c
⃝Springer-Verlag Berlin Heidelberg 2002

Conﬁdentiality-Preserving Reﬁnement is Compositional – Sometimes
195
the original speciﬁcation. If this is true, then the implementation relation relat-
ing a speciﬁcation to the set of its correct implementations is called composi-
tional. Adapting this deﬁnition to security, the question arises whether security-
property-preserving implementations of the subsystems with respect to their
speciﬁcations are suﬃcient to guarantee that composing the subsystem imple-
mentations yields a security-property-preserving implementation of the original
speciﬁcation of the whole system.
When formal techniques are used for system development, both the spec-
iﬁcation and the implementation are often described in the same formalism.
Then, the former is called the abstract speciﬁcation, and the latter is called the
concrete speciﬁcation. The relation describing the correct implementations of an
abstract speciﬁcation is called a reﬁnement relation, and a concrete speciﬁcation
implementing an abstract one is called a reﬁnement of the abstract speciﬁcation.
For a notion of reﬁnement, the properties of transitivity and compositionality are
very important. Without these properties, the practical application of reﬁnement
is hardly possible.
In
earlier
work
[4],
we
have
motivated
a
probabilistic
notion
of
a conﬁdentiality-preserving reﬁnement and sketched its formalization using an
extension of CSP with probabilistic choice. Classical formal techniques, which
are possibilistic, either impose suﬃcient conditions that are too strong or impose
only necessary conditions that are too weak to realize required conﬁdentiality
properties [16].
In the present paper, we further investigate properties of conﬁdentiality-pre-
serving reﬁnement, with the goal of enhancing its potential for practical appli-
cability. For these investigations, we represent our systems using a probabilistic
variant of CSP, and slightly rephrase our deﬁnition to better capture the intuition
motivated in [4]. We prove that the resulting reﬁnement relation is transitive.
By way of a counterexample, we show that conﬁdentiality-preserving reﬁnement,
in general, is not compositional. The main contribution of the paper is a nec-
essary and suﬃcient condition for compositionality of conﬁdentiality-preserving
reﬁnement, called non-disclosure.
A technical report [14] contains more explanatory prose and the complete
proofs of all theorems and lemmas mentioned in the paper.
2
Probabilistic CSP and Behavioral Reﬁnement
We use a probabilistic extension of the process algebra of “Communicating Se-
quential Processes” (CSP) to formally describe the systems we reason about.
Roscoe [12] comprehensively treats classical CSP. In this section, we brieﬂy in-
troduce the notation and the notion of behavioral reﬁnement on which we build
conﬁdentiality-preserving reﬁnement in Section 3.
2.1
CSP Notation
A process P produces sequences of events, called traces. An event c.d consists of a
channel name c and a data item d. Two processes can synchronize on a channel c

196
Thomas Santen et al.
by transmitting the same data d over c. If one process generates an event c.d and
the other generates an event c.x, where x is a variable, both processes exchange
data when synchronizing on channel c: the value of x becomes d.
In the following, we describe the CSP notation used in this paper. In the
following, P and Q are processes, e ∈Σ is an event, X ⊆Σ is a set of events,
S ∈Σ ↔Σ is a relation on events, and R ∈D ↔D is a relation on data.
The process e →P ﬁrst generates event e, and behaves like P afterwards.
The process P |[X ]| Q is a parallel composition of P and Q: if P or Q generate
events on channels not in X , then those events appear in an arbitrary order; if a
process generates an event on a channel in X , it waits until the other process also
generates an event on the same channel; if the data transmitted by both processes
are equal (or can be made equal because an event contains a variable), then the
parallel composition generates that event, otherwise the parallel composition
deadlocks.
In the notion of reﬁnement we use, we are interested in changing data repre-
sentations (data reﬁnement), because many eﬀects compromising conﬁdentiality
can be described by distinguishing data representations in an implementation
that represent the same abstract data item (e.g., diﬀerent representations of the
same natural number). For a relation R on D, the process P[[R]]D is the process
P where each data item a in events of P is replaced by a data item b that is in
relation with a, i.e. a R b holds.
The process P \ X is distinguished from P by hiding the channels in X ⊆
α P, where α P is the set of channels used by P. The traces of P \ X are the
traces of P where all events over channels in X are removed. The external choice
P ✷Q is the process that behaves as either P or Q, depending on the event
that the environment oﬀers.
For a family of processes P(x), the process ⊓P(x) nondeterministically be-
haves like one of the P(x). As an extension to classical CSP, we also need a
probabilistic choice P
x P(x): this process chooses x – and thus P(x) – accord-
ing to a probability distribution P.
For behavioral reﬁnements, we disregard distributions on choices and treat
all probabilistic choices as nondeterministic ones: the possibilistic version P of a
process P is deﬁned by replacing each occurrence of the probabilistic choice 
by a corresponding nondeterministic choice ⊓.
2.2
Reﬁnement of Behavior and Data
There are several notions of reﬁnement for CSP: trace reﬁnement, failure reﬁne-
ment, and failure-divergence reﬁnement. The latter two imply trace reﬁnement.
If P is reﬁned by Q, denoted P ⊑Q, then – regardless of the reﬁnement relation
used – traces(Q) ⊆traces(P).
We wish to cover changes of data representations in our reﬁnement relation.
Therefore, we extend the usual CSP reﬁnement with a retrieve relation mapping
concrete to abstract data, and deﬁne behavioral reﬁnement as a combination of
CSP reﬁnement and data renaming according to the retrieve relation.

Conﬁdentiality-Preserving Reﬁnement is Compositional – Sometimes
197
Deﬁnition 1 (Retrieve Relation). Let P and Q be processes over Σ. A rela-
tion R ∈D ↔D between concrete and abstract data is called a retrieve relation
from Q to P, if dom R ⊆data Q and ran R ⊆data P, where dom R and ran R
denote the domain and range, respectively, of relation R, and data P is the set
of data occuring in events of P.
At some places we need the set R−1(r) of all possible data reﬁned versions
of a trace r. Applying R−1 to a trace r means applying the inverse of R to the
data in each event of the trace, and R−1(r) denotes the set of all such traces:
R−1(r)={t | dom t =dom r ∧
∀i ∈dom r; c ∈Ch; d ∈D •∃d′ ∈R(d) • t(i)=c.d ⇒r(i)=c.d′}
Deﬁnition 2 (Behavioral Reﬁnement). Let P and Q be processes over Σ.
Let R be a retrieve relation from Q to P. Then Q reﬁnes P via R (written
P ⊑R Q), if P ⊑Q[[R]]D, where ⊑is the usual reﬁnement of CSP.
Behavioral reﬁnement is transitive and monotonic [14].
We will need to consider a restriction of a “concrete” process Q to a behavior
implementing a given “abstract” trace r.
Deﬁnition 3. Let Q be a process, R be a retrieve relation abstracting the data
in Q, and let r be a trace over the range of R. Then Q|R
r is a process that chooses
a behavior of Q whose starting sequence is compatible with r.
Q|R
r := Pr(r)[[R−1]]D |[α Q]| Q
The process Pr(r) produces the trace r and behaves arbitrarily afterwards:
Pr(⟨⟩) = Pr(⟨✓⟩) = RUN
Pr(⟨e⟩⌢s) = e →Pr(s)
The event ✓at the end of a trace signiﬁes termination of the process. The
process RUN engages in any communication the environment proposes.
Behavioral reﬁnement is deﬁned in terms of the possibilistic versions of the
involved processes. Morgan et.al. [11] deﬁne a reﬁnement relation for probabilis-
tic processes, which turns out to be a very delicate task when allowing both,
nondeterministic and probabilistic choices.1 Because conﬁdentiality-preserving
reﬁnement as deﬁned in Section 3 imposes a condition on processes that, in par-
ticular, does not require the probabilistic behavior of a process to be preserved
in a reﬁnement, we do not use Morgan, et.al.’s deﬁnition2 of reﬁnement.
1 It is not easy to avoid either one when deﬁning processes.
2 An investigation of the relation between the two is nevertheless theoretically inter-
esting.

198
Thomas Santen et al.
2
1
l
l
inp
w
ANet
AReceiver
out
ASender
ASystem
==a
w
==c
w
==c
w
abstract
concrete
R
0.6
0.6
r
0.4
s
u
t’
t
v
w
0.4
y
Fig. 1. System model (left), and concretization vs. indistinguishability (right)
2.3
Probability Distributions on Processes
We conclude the brief discourse on probabilistic CSP with some properties of
probability distributions, which we will need later. Given a process P, which may
contain external, nondeterministic, and probabilistic choices, the set of processes
Prob(P) contains all processes that are obtained by replacing each external
choice and each nondeterministic choice in P by a probabilistic choice for some
(arbitrary) distribution.
When we will consider probabilistic properties of processes later, we will
argue about all members Prob(P) for a given P, because thus we consider all
possible probabilistic behavior of the environment (external choices) and all pos-
sible probabilistic behavior of an implemented system for which the process (as
a speciﬁcation) does not determine the distribution (nondeterministic choices).
For a process Q that contains only probabilistic choices, we deﬁne a family
of probability distributions Pn(Q, t) that is indexed by the maximal length n
of traces it considers: Pn(Q, t) is a distribution on the set of traces with length
n or that terminate (the last event is ✓) and have a length less than n. For a
given t, we write PQ(t) for P#t(Q, t), where #t is the length of t.
The probability of Q producing a trace in a set M , which describes a certain
property of Q, is given by
PQ(t ∈M ) =

t∈pfree(M )
PQ(t)
(1)
The set pfree(M ) ⊆M is the maximal subset of M that does not contain any
t ∈M for which there is a preﬁx t′ of t in M .
Finally, we deﬁne PQ(s) = 0 for traces s /∈traces(Q).
3
Conﬁdentiality-Preserving Reﬁnement (CPR)
In this section, we present our deﬁnition of conﬁdentiality-preserving reﬁnement,
which we have extensively motivated in an earlier publication [4]. To specify

Conﬁdentiality-Preserving Reﬁnement is Compositional – Sometimes
199
conﬁdentiality properties we use a system model illustrated on the left-hand
side of Fig. 1 for the example of a communication system between a sender
and a receiver communicating over an untrusted network. We specify a system
for which conﬁdentiality is a relevant requirement by a pair of a process and
a window channel.
Deﬁnition 4 (System, Window). A system speciﬁcation A = (Q, w) is
a pair of a process deﬁnition Q and a distinguished channel w ∈α Q, called
the window of A.
Specifying the system ASystem of Fig. 1, we deﬁne three processes ASender,
ANet, and AReceiver, where ASender and ANet communicate via the internal
channel l1, and ANet and AReceiver communicate over the internal channel l2.
Then, the process ASystem is the parallel composition of those three processes.
The window w is a distinguished channel of ASystem.
ASystem = (QA, w)
QA = ((ASender |[l1]| ANet) |[l2]| AReceiver) \ {l1, l2}
α QA = {inp, out, w}
The channel w models the ﬂow of data from the system to an adversary.
Observing the channel w, the adversary gains information about the system.
Any distinction the adversary can make about the internal state of the system
based on the observations on w is information that the system does not keep
conﬁdential. Conversely, the system keeps conﬁdential any aspect of its behavior
that an adversary cannot distinguish by observing w. We formally capture that
conﬁdentiality property by deﬁning equivalences over system traces.
Deﬁnition 5 (Indistinguishability). Let A = (Q, w) be a system speciﬁca-
tion. Two traces s, t ∈traces(Q) are indistinguishable by w (denoted s ≡w t)
iﬀtheir projections to w are equal: s ≡w t ⇔s ↾{w} = t ↾{w}
In the transition from an abstract to a concrete system speciﬁcation, the
interpretation of a window changes. The window of an abstract system speciﬁes
what information is allowed to be visible to an adversary. The window of a con-
crete system speciﬁes what information cannot be hidden from the adversary.
Here, a purely logical argument is insuﬃcient because it is not enough to
ask whether a distinction in the concrete system deﬁnitely allows an observer to
distinguish conﬁdential data, but we must describe whether such a distinction
provides more information about the conﬁdential data than the abstract window
reveals. Therefore, we consider the respective probabilities of internal data that
may cause a particular observable behavior on a window. The right-hand side of
Fig. 1 illustrates our approach to formalizing that probabilistic argument:
Consider an abstract and a concrete system that behaviorally reﬁnes the
abstract one with retrieve relation R. Let r and s be two abstract traces that
are indistinguishable with respect to the window w, i.e. r ≡a
w s. According to the
retrieve relation R, trace r can be represented by the concrete traces u and w,

200
Thomas Santen et al.
and trace s can be represented by the concrete traces v and y, where u and y
as well as v and w are indistinguishable by observing the concrete window, i.e.
u ≡c
w y and w ≡c
w v. For keeping r and s indistinguishable in the concrete
system, we must require that the probability that r is represented by u be the
same as the probability that s is represented by y. If this were not the case, an
adversary might be able to gain information whether r or s happened on the
abstract layer: if the probability that r is represented by u is greater than the
probability that s is represented by y, for an adversary, the observation of some
element t ≡c
w y increases the probability of r with respect to s.
Deﬁnition 6 reﬂects this argument: A conﬁdentiality-preserving reﬁnement is
one that (1) is the behavioral reﬁnement of the processes describing a system (c.f.
Deﬁnition 2), and that (2) (probabilistically) preserves the indistinguishability
of system traces. In the latter condition, we consider the behavior of the concrete
system implementing an abstract trace r, i.e. the process Q|R
r . This process may
contain external choices stemming from the diﬀerent implementation choices
that R−1 assigns to the data in r. Because there is no way of knowing with what
probability those implementation choices are resolved, we need to consider all
possible distributions that make Q|R
r a probabilistic process, i.e. all members of
Prob(Q|R
r ).
Remark 1. To keep the language simple, we will talk about a probabilistic prop-
erty E of a process Q, when we mean that all members Qp ∈Prob(Q) satisfy E.
Deﬁnition 6 (Conﬁdentiality-Preserving Reﬁnement, CPR). Let A =
(P, w) and C = (Q, w) be two system speciﬁcations. Let ≡a
w be the indistin-
guishability in A (wrt. w), and let ≡c
w be the indistinguishability in C (wrt. w).
The system C is a conﬁdentiality-preserving reﬁnement (CPR) of the system A
via the retrieve relation R from Q to P (A ⊑cpr
R
C) iﬀ:
1. P \ {w} ⊑R Q \ {w}, and
behavioral reﬁnement,BR
2. ∀r, s ∈traces(P); t ∈traces(Q);
Qr ∈Prob(Q|R
r ); Qs ∈Prob(Q|R
s ) •
r ≡a
w s ⇒PQr(u ≡c
w t) = PQs(v ≡c
w t)
indistinguishability preservation,IP
We write P ⊑cpr
R,w Q for (P, w) ⊑cpr
R
(Q, w), which is useful when analyzing
systems with respect to diﬀerent windows. Although the windows of the two sys-
tems have the same name w, they may carry diﬀerent data because the processes
of the systems determine the data that is transmitted on a channel.
Hiding w from P and Q in Condition BR, Deﬁnition 6 does not require Q to
reﬁne the window w: At the implementation level, an adversary may have means
of observation that are in no way related to the means of observation given at
the speciﬁcation level. Requiring Q to reﬁne the window w therefore would –
inadequately – allow the speciﬁcation to impose restrictions on the power of an
adversary at the level of implementation. Condition IP captures the important
restriction on conﬁdentiality-preserving implementations that adversaries must
not be able to infer more information by observing the implementation than the

Conﬁdentiality-Preserving Reﬁnement is Compositional – Sometimes
201
window at the speciﬁcation level allows them to. It states that, given two indis-
tinguishable abstract traces r and s, and a concrete trace t, the probability of
choosing a concrete trace u which is indistinguishable of t as an implementation
of r must be the same as choosing a concrete trace v which is indistinguishable
of t as an implementation of s, see Fig. 1.
To determine the probability PQr(u ≡c
w t), we consider a subset of the set
T = {u | u ∈traces(Qr) ∧u ≡c
w t} of all traces that are indistinguishable
from t. Because T need not be preﬁx-free, we must consider the set pfree(T) for
calculating probabilities. Then, it holds:
PQr (u ≡c
w t) = 
u∈pfree(T) PQr(u)
To support stepwise reﬁnement and the independent reﬁnement of subsys-
tems, a reﬁnement relation must have two properties: it must be transitive and
compositional. The following Theorem 1 establishes the transitivity of CPR.
Section 4 extensively discusses compositionality.
Theorem 1 (Transitivity of CPR). Let A = (Pa, w), B = (Pb, w), and
C = (Pc, w) be system speciﬁcations. Let Rba and Rcb be retrieve relations
from Pb to Pa, and from Pc to Pb, respectively. Then
A ⊑cpr
Rba B ∧B ⊑cpr
Rcb
C ⇒A ⊑cpr
Rcbo
9Rba C, where o
9 is the forward composition of relations.
4
Compositionality of CPR
Compositionality of security properties and compositionality of reﬁnement are
two diﬀerent notions. Section 4.1 contrasts the two. Indistinguishability is not
a compositional property, and CPR is, in general, not compositional. Section 4.2
illustrates this fact by way of a counterexample. For a composed system for which
reﬁnements of subsystems are known, we can ﬁnd a condition that characterizes
the circumstances under which a CPR is compositional. This condition is more
intuitive than the Condition IP of Deﬁnition 6, and it allows one to build on the
analyses made for verifying the CPR of subsystems. Section 4.3 establishes this
result in Theorem 2.
4.1
Compositionality of Security Properties vs. Compositionality
of Reﬁnement
Compositionality of a security property, as it is often considered in the context
of secure systems, means the preservation of that property under composition
of systems: if certain systems satisfy a certain security property, some variant of
non-interference, say, then the composition of those systems satisﬁes the same
property. Mantel [10] investigates the relation between compositionality results
for many known information ﬂow properties.
Compositionality of a reﬁnement relation, in which we are interested in this
paper, addresses the interplay of decomposing a system speciﬁcation, and com-
posing the implementations of the subsystems to yield an implementation of

202
Thomas Santen et al.
w   wx
C
w   wx
inp
w = cipher(inp,k)
CSys
Con
wx = k
out
inp
A
w = length(inp)
ASys
Con
wx
out
⊗
⊗
com = inp
aux = k
k
com = inp
aux = 0
Fig. 2. Conﬁdentiality-preserving reﬁnement is not compositional
the original system speciﬁcation. Thus it is a preservation property that relates
diﬀerent levels of abstraction (speciﬁcation – implementation), whereas the com-
positionality of security properties is concerned with one level of abstraction only.
As we will see in the following section, the security property indistinguishability
(c.f. Deﬁnition 5) is not compositional. Since this is true for the abstract as well as
the concrete level of a reﬁnement, the non-compositionality of indistinguishabil-
ity, in principle, weakens the requirements for a reﬁnement to be compositional.
As a consequence, embedding a system into a context but reﬁning it in isolation
leads to two questions to be answered:
1. Does the composed system still fulﬁll the desired security requirements?
2. Does the replacement of the abstract by the concrete system in the given
context compromise security?
Question 1 means that the composition has to be considered and possibly be
rejected, independently of later reﬁnements. The present paper does not address
this problem. Since the usual notion of correctness of reﬁnement does provide
for preservation of integrity and hopefully availability, but not at all for conﬁ-
dentiality, we narrow Question 2 to: Does the replacement of the abstract by the
concrete system in the given context compromise conﬁdentiality? This amounts
to showing that the reﬁnement is compositional for the given context. In the rest
of the paper, we will show how to answer this question.
4.2
A Counterexample
The two systems shown in Fig. 2 illustrate that, in general, CPR is not com-
positional. The left-hand side of the ﬁgure shows an abstract system ASys with
two communication channels inp and out, and a window w ⊗wx that is a com-
bination of the windows w and wx of the two subsystems A and Con: all data
observable on w or wx are also observable on w ⊗wx. The subsystem A spec-
iﬁes a secure communication service. It allows its environment to observe the
length of messages transmitted from channel inp to channel com, but no other
information about the content of messages. The subsystem Con speciﬁes the
context in which A operates. The context Con copies the data that A produces
on com to its output channel out. The systems A and Con also communicate

Conﬁdentiality-Preserving Reﬁnement is Compositional – Sometimes
203
via the channel aux: Con receives data from A that do not contain any relevant
information (represented by a constant 0). The window wx of Con allows data
received on aux to be observed by the environment.
The right-hand side of Fig. 2 shows an implementation CSys of ASys. The
subsystem C is an implementation of the communication service that A speciﬁes
in the presence of an untrusted network. The implementation C probabilistically
chooses keys k with equal probabilities and uses a (suitable) encryption function
cipher to conceal the transmitted data from an observer who can intercept com-
munication on the network: the window w of C allows an adversary to observe
the ciphertext cipher(inp, k).
Because observing that ciphertext will reveal the length of the message but
nothing else about its content, the system C is a CPR of the system A, as shown
in [4]. If CPR was unconditionally compositional, we would expect CSys, which
is obtained from ASys by substituting C for A, to be a CPR of ASys. This,
however, is not true: C not only implements A correctly3, but it also transmits
the selected key k over the channel aux. This does not compromise conﬁdentiality
if C is considered in isolation, because w does not make information about the
data on aux available to the adversary. Composing C with Con as in CSys,
however, allows the adversary to observe the key k on the new window wx!
Thus, the combination of the information gained by observing w and wx reveals
the original input message to an adversary, which is not revealed on the abstract
level.
In this example, the non-compositionality of CPR is a direct consequence
of the non-compositionality of indistinguishability: composing the windows w
and wx in CSys makes more observations possible (the key becomes observable)
and thus an observer can distinguish more behavior of the subsystems than by
observing their respective windows alone.
The same argument, however, is also true for the abstract level: the indistin-
guishability requirement on the implementation will, in general, become weaker
by combining windows, thus strengthening the premise of Condition IP in Deﬁ-
nition 6, and allowing for more conﬁdentiality-preserving reﬁnements. Addition-
ally, much more subtle eﬀects relating to the probabilistic nature of Deﬁnition 6
may compromise the compositionality of CPR.
4.3
A Condition for Compositionality
After the somewhat discouraging result of the previous section, we will now in-
vestigate the conditions under which CPR is nevertheless compositional. We will
make precise the intuition gained from analyzing the counterexample, and come
up with a condition for compositionality that reduces the question of compo-
sitionality to the question what additional distinctions a new window on the
subsystem allows an adversary to make.
3 The retrieve relation maps each key transmitted on channel aux to the constant 0,
which is an admissible data reﬁnement.

204
Thomas Santen et al.
Formally, we consider two systems A = (P, w) and C = (Q, w) with A ⊑cpr
R
C for some retrieve relation R from Q to P. The context Con = (Cx, wx) is
another system that can communicate with A and C via a set of channels K.
The context window is diﬀerent4 from the window on A and C: wx ̸= w.
Combining
the
system
A
with
the
context Con
yields
the
system
(P |[K]| Cx, w ⊗wx). We assume here that the processes P and Cx work on
the same set of abstract data. Therefore, to combine C with Con, we must
“concretize” the data in Cx by a data renaming consistent with the retrieve
relation from Q to P: (Q |[K]| Cx[[R−1]]D, w ⊗wx). Viewed abstractly, the pro-
cess Cx[[R−1]]D has the same behavior as Cx, but for each data item a that Cx
transmits, the process Cx[[R−1]]D transmits a data item b that implements a, i.e.
for which b R a holds. In this setting, compositionality means that C combined
with Con reﬁnes A combined with Con:
(P |[K]| Cx, w ⊗wx) ⊑cpr
R
(Q |[K]| Cx[[R−1]]D, w ⊗wx)
(2)
If Condition IP of Deﬁnition 6 does not hold for (2), then the additional
observations of the concrete system that the window wx permits via the context
must allow an adversary to distinguish more behavior of P than the window w
permits. Composing the context with the processes P or Q, respectively, forces
them to synchronize with the context and thus reduces their possible behavior.
This may change the probabilities of traces of P and Q, which might also aﬀect
Condition IP.
This analysis motivates the three essential tasks to solve for stating our com-
positionality theorem:
1. to reduce the combination of a system with a context, which itself has an
additional window, to adding a window to the system;
2. to come up with a condition describing the circumstances under which a CPR
between two systems is preserved under addition of a window to both sys-
tems;
3. to show that reducing behavior by adding a context preserves CPR.
To solve the ﬁrst task, we wish to consider the context as a means of an
adversary to observe the behavior of P or Q at the channels K. Technically, we
can achieve this by hiding all channels of Cx but wx and but the ones in K from
the composed systems. This leads to the notion of a system in context:
Deﬁnition 7 (System in Context). Let A = (P, w) and Con = (Cx, wx) be
system speciﬁcations. Let K ⊆(α P ∩α Cx) −{w, wx} be a set of channels over
which A and Con can communicate, and let X := α Cx −(α P ∪{wx}). Then A
in context Con, written A
K∢Con, is the system (P |[K]| (Cx \ X ), w ⊗wx).
The system A
K∢Con is the system A with an additional window wx and the
reduced behavior that is a consequence of synchronizing with Con. Similarly,
4 Although the channel names are diﬀerent, the same data can, of course, be trans-
mitted over those channels.

Conﬁdentiality-Preserving Reﬁnement is Compositional – Sometimes
205
                                                                                                                                                                                                                                                                    









==w
==wx
==wx
==w
s
w
y
u
R
abstract
concrete
x
r
Fig. 3. Non-Disclosure
at the concrete level, the system C
K∢Con[[R−1]]D is the system C with an
additional window and reduced behavior. Lemma 1 shows that it is suﬃcient to
prove a CPR between those systems in context to establish (2).
Lemma 1. Let A = (P, w), C = (Q, w), and Con = (Cx, wx) be system
speciﬁcations. Let w ∈(α P ∩α Q) −α Cx, wx ∈α Cx, K ⊆(α P ∩α Q ∩
α Cx) −{w, wx}, and X := α Cx −(α P∪α Q∪{wx}). Let R be a retrieve relation
from Q to P. Let (P, w ⊗wx) := A
K∢Con and ( Q, w ⊗wx) := C
K∢Con[[R−1]]D.
Then
P ⊑cpr
R,w Q ∧P ⊑cpr
R,w⊗wx Q ⇒P |[K]| Cx ⊑cpr
R,w⊗wx Q |[K]| Cx[[R−1]]D
To solve the second task, consider the eﬀect of adding a window wx to the
abstract and concrete systems. By the following Equivalence (3), going from w
to w ⊗wx makes the equivalence classes of the indistinguishability ﬁner.
s ≡w⊗wx t ⇔s ≡w t ∧s ≡wx t
(3)
As Fig. 3 illustrates, the equivalence classes at both, the abstract and the
concrete level, become ﬁner. Thus, we need to consider fewer pairs (r, s) of
abstract traces in Condition IP of Deﬁnition 6, which weakens the condition, but
at the same time, we need to show a stronger property for the pairs (r, s) that we
still must consider: For all traces t of the concrete system, the probability of the
concrete system to choose a behavior u implementing r that is indistinguishable
from t by both w and wx must be the same as the probability of the system to
choose an implementation v of s that is indistinguishable from t by both w and
wx. Formally, we need to show:

u ∈pfree

[t]≡cw ∩[t]≡cwx
 PQr (u) = 
v ∈pfree

[t]≡cw ∩[t]≡cwx
 PQs(v)
(4)

206
Thomas Santen et al.
We already know [t]≡cw from proving A ⊑cpr
R
C. Proving Equation (4) as
it stands would mean to analyze which behavior of C that is indistinguishable
by w remains indistinguishable when adding wx. From a practical point of view,
however, it is more suitable to analyze which observations made using wx allow
an adversary to distinguish behavior that is indistinguishable by w, and to com-
pare probabilities for that behavior. This means to let the sums range over the
set diﬀerence of the respective equivalence classes. If the resulting equation of
probabilities holds, we call wx non-disclosing on the system with respect to r
and s.
Deﬁnition 8 (Non-Disclosure). Let S = (Q, w) be a system speciﬁcation,
and let wx ∈α Q −{w} be a channel of Q that is distinct from w. Let R be
a retrieve relation from Q to the data in two traces r and s. We call wx non-
disclosing on S wrt. R, r and s, written Q|R
r,s ⊢w ⊜wx, iﬀthe following
condition holds:
∀t : traces(Q); Qr ∈Prob(Q|R
r ); Qs ∈Prob(Q|R
s ) •

u∈

T(Qr,t) −[t]≡wx
 PQr (u) = 
v∈

T(Qs,t) −[t]≡wx
 PQs(v)
The set of traces T(Q, t) is given by T(Q, t) := T0(Q, t)∪pfree(T1(Q, t)), where
T0(Q, t) = {u ∈traces(Q) | u ≡w t ∧#(u ↾{wx}) = #(t ↾{wx}) ∧
¬ (∃u′ ∈traces(Q) • u′ preﬁx u ∧
u′ ≡w t ∧#(u′ ↾{wx}) = #(t ↾{wx}))}
(5)
T1(Q, t) = {u ∈traces(Q) | u ≡w t ∧u /∈T0(Q, t) ∧
¬ (∃u′ ∈T0(Q, t) • u preﬁx u′)}
(6)
The preﬁx-free set T(Q, t) is an alternative for pfree([t]≡w ) when comput-
ing the probability that Q produces a trace u with u ≡w t. Additionally, the
(rather technical) construction ensures that T(Q, t) contains a maximal number
of traces that have as many observations over the other window wx as t has.
Lemma 2 states that – given P ⊑cpr
R,w Q – non-disclosure characterizes the
circumstances under which CPR is preserved when a new window is added.
Lemma 2. Let P and Q be processes, w, wx ∈α P ∩α Q be channels common
to P and Q, and let R be a retrieve relation from Q to P. If P ⊑cpr
R,w Q then
P ⊑cpr
R,w⊗wx Q ⇔

∀r, s ∈traces(P) • r ≡a
w⊗wx s ⇒Q|R
r,s ⊢w ⊜wx

To solve the third task, we need Lemma 3 that relates a given CPR to a CPR
of the same systems in a context that does not add a new window. When we
apply this lemma to prove compositionality, we will regard wx not as a window
but as an ordinary channel of Cx.
Lemma 3. Let A = (P, w), C = (Q, w), and Con = (Cx, wx) be system spec-
iﬁcations with w ̸= wx. Let K ⊆(α P ∩α Q ∩α Cx) −{w, wx} be a set of

Conﬁdentiality-Preserving Reﬁnement is Compositional – Sometimes
207
channels over which A and Con, and C and Con, respectively, can communi-
cate. Let (P, w ⊗wx) := A
K∢Con and ( Q, w ⊗wx) := C
K∢Con[[R−1]]D. Then
P ⊑cpr
R,w Q ⇒P ⊑cpr
R,w Q.
Lemmas 1, 2, and 3 allow us to prove the main result of this paper: CPR is
compositional if the context window wx is non-disclosing on the reﬁned subsys-
tem.
Theorem 2 (Compositionality of CPR). Let A = (P, w), C = (Q, w), and
Con = (Cx, wx) be system speciﬁcations with w ̸= wx. Let K ⊆(α P ∩α Q ∩
α Cx) −{w, wx} be a set of channels over which A and Con or C and Con,
respectively, can communicate. Let R be a retrieve relation from Q to P. Let P
be the process of A
K∢Con, and let Q be the process of C
K∢Con[[R−1]]D. If
1. A ⊑cpr
R
C, and
2. ∀r,s : traces(P) • r ≡a
w⊗wx s ⇒Q|R
er,
es ⊢w ⊜wx
then (P |[K]| Cx, w ⊗wx) ⊑cpr
R
(Q |[K]| Cx[[R−1]]D, w ⊗wx).
Proof. Assume P ⊑cpr
R,w Q. With Lemma 3, we get P ⊑cpr
R,w
Q, which im-
plies P ⊑cpr
R,w⊗wx Q by Assumption 2 and Lemma 2. From Assumption 1 and
Lemma 1, we conclude P |[K]| Cx ⊑cpr
R,w⊗wx Q |[K]| Cx[[R−1]]D.
⊓⊔
5
Related Work
Because indistinguishability and its preservation by reﬁnement (CPR) is con-
cerned with hiding certain information about events occurring in a system from
an adversary, our work is related to research on non-interference.
Non-interference, ﬁrst introduced by Goguen and Meseguer [1], is a security
property that has extensively been studied. Much work on non-interference is
possibilistic, i.e. it disregards probabilistic arguments. Ryan and Schneider [13]
recast many known deﬁnitions of possibilistic non-interference in (classical) CSP
and show that the diﬀerent ways of deﬁning non-interference are closely related
to the diﬀerent notions of process equivalence.
The windows in our setting can be viewed as a channel from the considered
system to an outside adversary, i.e. from the “high” system to the “low” outside
world. In contrast to non-interference, we do not require no information to ﬂow
through that channel. Our deﬁnition of CPR ensures that possible observations
which adversaries may make of the implemented system do not oﬀer them ad-
ditional ways of inferring information about the system than the speciﬁcation
allows them to.
Ryan and Schneider [13] discuss an approach of generalizing non-interference
that has a similar motivation: requiring total absence of information ﬂow often
is too strong in practice. Their generalization is parameterized by an equivalence
on traces, an equivalence on processes, and a way of abstracting High’s behavior

208
Thomas Santen et al.
from a process. Depending on the instantiation of these parameters one obtains
weak versions of non-interference that allow Low to determine High’s behavior
up to the equivalence on traces. It may be interesting to recast our deﬁnition of
indistinguishability into that framework.
Gray [3] deﬁnes probabilistic non-interference (PNI), and J¨urjens [6] proves
a compositionality theorem for a variant of that deﬁnition. The condition for
PNI basically states that the probability of a high user producing a particular
observation of a low user is the same for all behaviors of the high user. Because
it formalizes the fact that certain behaviors cannot be distinguished probabilis-
tically, this condition of PNI is similar to our reﬁnement condition IP. The
diﬀerence is that IP requires equal probabilities of (indistinguishable) concrete
behavior only for implementations of indistinguishable abstract behavior.
Lowe [8] recently investigated how to quantify information ﬂow from high
to low while staying in a possibilistic setting. Using a discretely timed version
of CSP, he can analyze timing channels, which we currently ignore. The aim of
Lowe’s work is similar to ours in that he does not per se require no information
to ﬂow from high to low: he puts bounds on the capacity of channels whereas
(by the abstract window) we restrict the ways in which information may ﬂow
from high to low.
Graham-Cumming
and
Sanders
[2]
discuss
the
preservation of
non-
interference under data reﬁnement. They specify systems using the speciﬁcation
language Z [15] and deﬁne security as indistinguishability on system traces with
respect to a given user. They give conditions under which a reﬁnement of the
internal data of the system preserves indistinguishability. Their approach is pos-
sibilistic, and, in contrast to our setting, they consider only reﬁnements of the
internal state of a system but not of the input and output data. We emphasize
reﬁning the inputs and outputs, because an implementation must be designed in
such a way that choosing particular representations of inputs and outputs does
not allow adversaries to infer more information about the system than they are
allowed to.
Mantel [9] considers the preservation of information ﬂow properties under
reﬁnement. It is well-known that CSP-style reﬁnement does not preserve infor-
mation ﬂow properties in general [5]. Mantel shows how reﬁnement operators tai-
lored for speciﬁc information ﬂow properties can modify an intended reﬁnement
such that the resulting reﬁnement preserves the given ﬂow property. Working
top-down from the speciﬁcation to an implementation, the reﬁnement operators
may lead to concrete speciﬁcations that are practically hard to implement, be-
cause the changes in the reﬁnement they induce are hard to predict and may
not be easy to realize in an implementation.
J¨urjens [7] uses stream processing functions to model systems, and he deﬁnes
a possibilistic notion of secrecy in that setting. He identiﬁes conditions under
which certain reﬁnement operators on stream processing functions preserve his
notion of secrecy.

Conﬁdentiality-Preserving Reﬁnement is Compositional – Sometimes
209
6
Conclusions
Security-aware engineering of systems and software needs a notion of reﬁne-
ment which comprises not only integrity and availability, but conﬁdentiality
as well. To contribute to providing a ﬁrm basis for security-aware engineering,
we developed a precise notion of conﬁdentiality-preserving reﬁnement (CPR).
CPR inherits all properties of behavioral reﬁnement and additionally introduces
indistinguishability preservation, which is the probabilistic characterization of
conﬁdentiality-preservation.
At the end of Section 4.1, we have identiﬁed two questions concerning the
composition and reﬁnement of secure systems: (i) Does the composed system still
fulﬁll the desired security properties? (ii) Does the replacement of the abstract
by the concrete system in a given context compromise conﬁdentiality? Question
(i) should be investigated in more detail, taking into account our deﬁnition
of CPR. For this investigation, one can build on the work of Mantel [10] and
Ryan and Schneider [13]. Concerning the investigation of compositionality of
reﬁnement in a probabilistic setting, i.e., question (ii), we know of no work
prior to ours, as Graham-Cumming and Sanders [2], Mantel [9], and J¨urjens [7]
consider possibilistic reﬁnement only.
The present paper shows that conﬁdentiality-preserving reﬁnement is tran-
sitive, but not compositional in general. An analysis of the situation shows that
this result is not surprising. It is even inevitable, because conﬁdentiality prop-
erties are of a fundamentally diﬀerent nature than integrity (and – to a certain
extent – availability) properties, which correspond to the notion of correctness
as considered in classical reﬁnement. Reﬁning a subsystem that is embedded
in a context yields a reﬁnement of the composed system, because the reﬁned
subsystem always behaves in a way that is consistent with the behavior of the
abstract subsystem. A corresponding property does not hold for conﬁdentiality.
As Section 4.2 shows, it is possible to reﬁne a subsystem in such a way that
additional ways of obtaining information about the subsystem become possible
on the concrete level as compared to the abstract level. This is due to the facts
that, ﬁrst, the context adds an additional window to the system, and second,
“non-conﬁdential” data may be reﬁned to data that permits an adversary to
gain additional information as compared to the abstract system.
In such a situation, the only possible remedy is to investigate the conditions
that must hold in addition to the conﬁdentiality-preserving reﬁnement of the
subsystem. If proving these conditions is easier than proving the CPR for the
composed systems from scratch, then the notion of conﬁdentiality-preserving
reﬁnement is still useful for stepwise development using a divide-and-conquer
approach.
With the notion of non-disclosure, we capture the additional condition that
must hold to guarantee the compositionality of CPR. This condition corresponds
well to the intuition that (i) the “information leaks” introduced by the context
can be represented by the additional data visible in the context’s window, and
(ii) that adding a new window must not change the relative probabilities of
indistinguishable traces. Non-disclosure does not only give insight into our notion

210
Thomas Santen et al.
of CPR, but also into the relationship between reﬁnement and compositionality
in general.
We can therefore conclude that the notion of CPR may be useful for security-
aware engineering of systems and software, even if it cannot be compositional in
general. The work presented in this paper lays the foundations for an engineer-
ing approach to CPR: identifying architectures that guarantee non-disclosure by
construction will facilitate the task of proving non-disclosure. Further research
must apply our deﬁnitions and theorems to examples of larger scale, further in-
vestigate the relation of indistinguishability of traces to other security properties,
as well as start the development of tools supporting our approach.
Acknowledgement
We thank Peter Ryan for extensive and constructive feedback on a previous
version of the paper. The anonymous reviewers also provided useful comments.
The program committee allowed us to considerably rework the paper during the
acceptance process. Thanks also go to Sandra Steinbrecher and Elke Franz for
asking good questions and ﬁnding some typos.
References
[1] J. A. Goguen and J. Meseguer. Security policies and security models. In IEEE
Symposium on Security and Privacy, pages 11–20. IEEE Computer Society Press,
1982.
207
[2] J. Graham-Cumming and J. W. Sanders. On the reﬁnement of non-interference.
In 9th IEEE Computer Security Foundations Workshop, pages 35–42. IEEE Com-
puter Society Press, 1991.
208, 209
[3] J. W. Gray.
Toward a mathematical foundation for information ﬂow security.
Journal of Computer Security, 1992.
208
[4] M. Heisel, A. Pﬁtzmann, and T. Santen. Conﬁdentiality-preserving reﬁnement.
In 14th IEEE Computer Security Foundations Workshop, pages 295–305. IEEE
Computer Society Press, 2001.
195, 198, 203
[5] J. Jacob. On the derivation of secure components. In IEEE Symposium on Security
and Privacy, pages 242–247. IEEE Press, 1989.
208
[6] J. J¨urjens. Secure information ﬂow for concurrent processes. In CONCUR 2000,
LNCS 1877. Springer-Verlag, 2000.
208
[7] J. J¨urjens. Secrecy-preserving reﬁnement. In J. N. Oliveira and P. Zave, editors,
FME 2001: Formal Methods for Increasing Software Productivity, LNCS 2021,
pages 135–152. Springer-Verlag, 2001.
208, 209
[8] G. Lowe. Quantifying information ﬂow. In 15th IEEE Computer Security Foun-
dations Workshop, pages 18–31. IEEE Computer Society, 2002.
208
[9] H. Mantel. Preserving information ﬂow properties under reﬁnement. In IEEE
Symposium on Security and Privacy, pages 78–91. IEEE Computer Society Press,
2001.
208, 209
[10] H. Mantel. On the composition of secure systems. In IEEE Symposium on Security
and Privacy. IEEE Computer Society Press, 2002. to appear.
201, 209

Conﬁdentiality-Preserving Reﬁnement is Compositional – Sometimes
211
[11] C. Morgan, A. McIver, K. Seidel, and J. W. Sanders. Reﬁnement-oriented prob-
ability for CSP. Formal Aspects of Computing, 8(6):617–647, 1996.
197
[12] A. W. Roscoe. The Theory and Practice of Concurrency. Prentice Hall, 1998. 195
[13] P. Y. A. Ryan and S. A. Schneider. Process algebra and non-interference. In 12th
IEEE Computer Security Foundations Workshop, pages 214–227. IEEE Computer
Society, 1999.
207, 209
[14] T. Santen, M. Heisel, and A. Pﬁtzmann.
Compositionality of conﬁdentiality-
preserving reﬁnement. Technical Report 10/2002, Technische Universit¨at Berlin,
2002.
195, 197
[15] J. M. Spivey. The Z Notation – A Reference Manual. Prentice Hall, 2nd edition,
1992.
208
[16] J. T. Wittbold and D. M. Johnson. Information ﬂow in nondeterministic systems.
In IEEE Symposium on Security and Privacy, pages 144–161. IEEE, 1990.
195

Formal Security Analysis
with Interacting State Machines
David von Oheimb and Volkmar Lotz
Siemens AG, Corporate Technology, D-81730 Munich
{David.von.Oheimb,Volkmar.Lotz}@siemens.com
Abstract. We introduce the ISM approach, a framework for modeling
and verifying reactive systems in a formal, even machine-checked, way.
The framework has been developed for applications in security analy-
sis. It is based on the notion of Interacting State Machines (ISMs), sort
of high-level Input/Output Automata. System models can be deﬁned
and presented graphically using the AutoFocus tool. They may be type-
checked and translated to a representation within the theorem prover
Isabelle or deﬁned directly as Isabelle theories. The theorem prover may
be used to perform any kind of syntactic and semantic checks, in par-
ticular semi-automatic veriﬁcation. We demonstrate that the framework
can be fruitfully applied for formal system analysis by two classical ap-
plication examples: the LKW model of the Inﬁneon SLE66 SmartCard
chip and Lowe’s ﬁx of the Needham-Schroeder Public-Key Protocol.
1
Introduction
1.1
Motivation
In industrial environments, there is an increased demand for rigorous analysis
of security properties of systems. Due to restrictions imposed by the application
domain, the system environment, and business needs, new security mechanisms
and architectures have to be invented frequently, with time-to-market pressure
and intellectual property considerations obstructing the chance to gain conﬁ-
dence by exposing a proposed solution to the security community (which has
been shown to be appropriate for cryptographic algorithm assessment). Formal
analysis of suitable abstractions of systems has instead turned out to be ex-
tremely helpful in reasoning about a system’s security, since the mathematical
precision of the arguments allows for maximal conﬁdence in the results obtained
and, thus, in the security of the system being modeled.
The importance of formal analysis – on top of open review – in security as-
sessment is, for instance, reﬂected by the requirements stated for high assurance
levels of criteria like ITSEC [ITS91] and CC [CC99], which include formal se-
curity modeling and formal system development steps, and the achievements of
the security protocol veriﬁcation community, which discovered ﬂaws in protocols
that failed to be detected by informal approaches.
D. Gollmann et al. (Eds.): ESORICS 2002, LNCS 2502, pp. 212–229, 2002.
c
⃝Springer-Verlag Berlin Heidelberg 2002

Formal Security Analysis with Interacting State Machines
213
However, even in a formal setting it is easy to make – minor and some-
times even major – mistakes: undeﬁned expressions, type mismatches, inconsis-
tent speciﬁcations, missing evidence in proofs, false conclusions etc. Therefore,
pure pen-and-paper formalizations cannot be considered fully reliable. Machine-
checking of formal objects and structures has to be employed in order to sig-
niﬁcantly reduce the occurrence of such mistakes. Machine support additionally
gives the opportunity to represent and deal with formal objects – both spec-
iﬁcations and proofs – in an easy-to-comprehend way, which is a prerequisite
for introducing formal approaches in an industrial environment characterized by
time and cost restrictions.
1.2
Goals
A framework for formal security analysis particularly suited for industrial use
should enjoy the following properties:
Expressiveness. It should be possible to describe any typical security sensitive
computation, storage, and communication systems in an abstract way. This
requires in particular the notions of state transformation, concurrency and
message passing.
Flexibility. Since IT systems and their security threats evolve quickly, the mod-
els produced within the framework should be easily adaptable and extendable
as necessary to reﬂect the changes.
Simplicity. Modeling a system, stating its properties and proving them should
require as little expertise and time as possible while maintaining the rigor of
a fully formal approach.
Graphical capabilities. System models should be representable as diagrams
that give a good overview of the system structure and a quick intuition about
its behavior.
Maturity of the semantics. The semantic foundation of the framework
should be well-developed, supporting in particular modular reﬁnement.
Availability of tools. The framework should be built from existing widely
available (open-source) software like editors and proof tools and require at
most minor modiﬁcations or extensions to them.
1.3
Related Work
The IOA Language and Toolset [GL98, Kay01] is a framework for analyzing com-
putational processes with aims very similar to ours. It consists of a speciﬁcation
language and tool support for simulation, theorem proving, model checking, and
code generation, where by now the simulation aspect is developed most and the-
orem proving support is limited to PVS. Its semantic foundation is the notion of
I/O Automata (IOAs) [LT89] modeling asynchronous distributed computation
with synchronous communication. Since the notion is based on transition sys-
tems augmented by communication primitives (rather than e.g. a process algebra
augmented by local computation primitives), it is fairly easy to understand. It

214
David von Oheimb and Volkmar Lotz
is equipped with a well-developed meta theory supporting reﬁnement and com-
positional reasoning. System properties, both safety and lifeness ones, may be
described using temporal logics and proved by model checking and interactive
theorem proving.
The only — but severe — drawback of IOAs from our perspective, in partic-
ular when modeling system security in an abstract way, is that their interaction
scheme is rather low-level: buﬀered communication has to be modeled explicitly,
and transitions involving several related input, internal processing and output
activities cannot be expressed atomically. Instead, each high-level transition has
to be split into multiple low-level transitions, and between these, any number of
further input events may take place due to the input-enabledness of IOAs. The
solution to this problem is to add extra structure, essentially by interpreting
parts of the local state of an automaton as input/output buﬀers. Our notion of
ISMs, introduced in [Ohe02a], provides for that.
A further related work that provided inspiration for our framework is Auto-
Focus [HSSS96] – see also §2.2. Even though developed primarily for modeling
and verifying functional properties of embedded systems, it is used also for the
securiy analysis of general distributed systems [WW01, JW01].
Other related approaches combine state-oriented and message-oriented de-
scription methods, for example translating CSP to B [But99] or Z to CSP [Fis00].
The drawback of such hybrids is that the user has to deal with two diﬀerent non-
trivial formalisms. Moreover, theorem proving support respecting the structure
of the mixed-style speciﬁcations seems not to be available.
2
Preliminaries
In this section, we brieﬂy introduce the two software tools we rely on and com-
ment on their suitability for the ISM approach.
2.1
Isabelle/HOL
Isabelle [Pau94] is a generic theorem prover that has been instantiated to
many logics, in particular the very practical Higher-Order Logic (HOL).
Isabelle/HOL [PNW+] is a predicate logic based on the simply-typed λ-calculus
and thus in a sense combines logical and functional programming. Being quite
expressive and supporting automatic type inference, it is the most important
and best supported logic of Isabelle. The lack of dependent types introduces
a minor nuisance for applications like ours: for each system modeled there is
a single type of message contents into which all message data has to be injected,
and analogously for the local states of automata.
Proofs are conducted primarily in an interactive fashion where automatic and
semi-automatic methods are available to tackle the routine parts. The Isabelle
system is well-documented and well-supported, is freely available (including
sources) and comes with the excellent user interface ProofGeneral [AGKS99].
We consider it the most ﬂexible and mature veriﬁcation environment available.

Formal Security Analysis with Interacting State Machines
215
Using Isabelle/HOL, security properties can be expressed easily and adequately
and veriﬁed with powerful proof methods.
2.2
AutoFocus
AutoFocus [HSSS96] is a freely available speciﬁcation and simulation tool for
distributed systems. Components and their behavior are speciﬁed by a combina-
tion of system structure diagrams (SSDs), state transition diagrams (STDs) and
auxiliary data type deﬁnitions (DTDs). Their execution can be visualized using
extended event traces (EETs). Various back-ends including code generators and
interfaces to model checkers may be acquired by purchase from Validas [S+].
We employ AutoFocus for its strengths concerning graphical design and pre-
sentation, which is important when setting up models in collaboration with
clients (where strong familiarity with formal notations cannot be assumed), when
documenting our work and publishing its results. For abstract security modeling,
there are currently two problems. First, expressiveness is limited concerning the
type system and the handling of underspeciﬁcation. These weaknesses are going
to be removed in the near future. Second, due to the original emphasis of Auto-
Focus on embedded systems, the underlying semantics is still clock-synchronous.
In contrast, for the most of our applications, in particular communication pro-
tocols, an asynchronous (buﬀered) semantics is more adequate, which is under
consideration also for future versions of AutoFocus. Using an alternative seman-
tics implies that we cannot make use of the simulation, code generation and
model checking capabilites of current AutoFocus and its back-ends. Yet this is
not a real obstacle for us since we are interested mainly in its graphic capabilities
and the oﬀered speciﬁcation syntax is general enough to cover also our deviating
semantics.
3
The ISM Approach
We ﬁrst introduce the core of our modeling and veriﬁcation framework, viz. ISMs,
and then give some general comments how they are handled by AutoFocus and
Isabelle.
3.1
Interacting State Machines
An Interacting State Machine (ISM) is an automaton whose state transitions
may involve multiple input and output simultaneously on any number of ports.
As the name suggests, the key concepts of ISMs are states (and in particular
the transitions between them) and interaction. By interaction we mean explicit
buﬀered communication via named ports, where on each of them one receiver
listens to possibly multiple senders. A system consists of the parallel composi-
tion of any number of ISM components where the state of the whole system is
essentially the Cartesian product of the states of its components.

216
David von Oheimb and Volkmar Lotz
In
Local State:
Input Buffers:
Trans
Out
Control State
Data State
Fig. 1. ISM structure
The state of an ISM consists of the local state and its input buﬀers. The
local state may have arbitrary structure but typically is the Cartesian product of
a control state which is of ﬁnite type, and a data state that typically is a record
of named ﬁelds. Transitions between states may be nondeterministic and can
speciﬁed in any relational style. Thus the user has the choice to deﬁne them in
an operational (i.e., executable) or axiomatic (i.e., property-oriented) fashion or
a mixture of the two.
Each ISM declares two sets of port names, one for input and the other for
output. The local input buﬀers are a family of unbounded FIFOs indexed by
port names. Input of individual messages is triggered by any ISM and cannot
be blocked, i.e. may occur at any time, appending the received value to the
corresponding FIFO. Values stored in the input buﬀers may be processed by
the ISM when it is ready to do so. This is done in a transition speciﬁed as
follows: under a given precondition, the ISM consumes as much input from the
buﬀers as appropriate, makes a transition of the local state, and produces output
values at its discretion. These values are forwarded to the input buﬀers of all
ISMs listening to the respective port, which may include feedback to the current
component.
Each ISM has a single1 initial state with empty input buﬀers. The execu-
tion of a system is a ﬁnite (but unbounded) sequence of nondeterministically
interleaved steps of any of its components. Finiteness implies that we can han-
dle safety, but not liveness properties. Transitions of diﬀerent ISMs are related
only by the causality wrt. the messages interchanged. Execution gets stuck when
there is no component that can perform any step. As typical for reactive systems,
there is no built-in notion of ﬁnal or “accepting” states.
The representation of ISMs and their semantics consists of several layers. Its
details, as well as a translation to IOAs, may be found in [Ohe02a].
1 If a non-singleton set of initial states is required, these may be simulated by spon-
taneous nondeterministic transitions originating from a single dummy initial state.

Formal Security Analysis with Interacting State Machines
217
3.2
AutoFocus Representation
By design, ISMs have almost the same structure as the automata deﬁnable with
AutoFocus, and thus we can use AutoFocus as a graphical front-end to our
Isabelle implementation.
In a typical application of our framework, ISMs are ﬁrst “painted” using
AutoFocus, saved in the so-called Quest ﬁle format, and then translated into
suitable Isabelle theory ﬁles by a tool program.
3.3
Isabelle Representation
ISMs can be deﬁned in special sections of Isabelle theories. This abstract (and
almost semantics-independent) representation has essentially a one-to-one cor-
respondence to the AutoFocus representation. As the cases studies below show,
the structure of this section is almost self-explanatory. The formal deﬁnition of
both the syntax and the semantics of ISMs in Isabelle/HOL is given in [Ohe02a].
4
LKW Model for the Inﬁneon SLE66
We give an improvement of the LKW model for the Inﬁneon SLE66 SmartCard
processor. We demonstrate that, with the ISM approach, transition systems can
be adequately modeled and their security properties stated and proven.
The LKW model [LKW00] is one of the ﬁrst formal models for security prop-
erties of hardware chips. It has been used successfully within the security evalua-
tion process for the SLE66 on ITSEC level E4 and the corresponding Evaluation
Assurance Level 5 (semiformally designed and tested, which includes a formal
security model) [CC99]. Recently, a slight extension was introduced [OLW02] in
order to reﬂect additional application-oriented security objectives as deﬁned in
the SmartCard IC Platform Protection Proﬁle [AETS01].
The LKW model gives an abstract system model for the SLE66 based on an
ad-hoc automaton formalism, formalizes the security requirements in terms of
properties of automaton runs and proves that the system meets the given re-
quirements. All this is done as a pen-and-paper work, i.e. without tool assistance.
Thus it is inevitable that the model contains many (mostly minor) syntactical,
typographical and semantical slips as well as type errors, but also omissions
like missing assumptions and incomplete proofs. Therefore it was desirable to
formalize the model in a machine-checked way, applying a well-developed meta
theory. Using ISMs, the LKW model can be represented adequately, including
some improvements.
4.1
AutoFocus Diagrams
On the abstract level of the LKW model, the system architecture of the SLE66
is rather trivial: there is one component with one input port named In and
one output port named Out, as depicted by Figure 2. The data state of the

218
David von Oheimb and Volkmar Lotz
SLE66
In:message
Out:message
Local Variables:
  map(fn,value) valF
  map(dn,value) valD
Fig. 2. SLE66 System Structure Diagram
component consists of two stores mapping names of functions and data objects
to corresponding values.
Much more involved is the structure of the state transitions. There are four
control states corresponding to the phases of the SLE66 life cycle:
Phase 0: construction of the chip
Phase 1: upload of Smartcard Embedded Software, personalization
Phase 2: normal usage
Phase Error: locked mode from which there is no escape
In order to keep the state transition diagram clear, Figure 3 contains all
control states and transitions, but instead of showing the preconditions, inputs,
outputs, and changes to the data state, we just label the transitions with the
names of the corresponding transition rules. Part of the rules are described in
P0
P1
P2
Error
R0.0
R1.1
R5.2
R0.2
R0.1
R5.2
R1.2
R5.2
R0.0
R1.1
R5.2
R0.2
R0.1
R5.1
R0.4
R2.2
R1.3
R1.4
R2.1
R5.3
R3.2
R4.1
R4.2
R4.2
R4.2
R3.1
R0.3
R5.1
R5.1
R5.2‘
R5.2‘
R5.2‘
R5.2
R1.2
R5.2
R4.2
Fig. 3. SLE66 State Transition Diagram

Formal Security Analysis with Interacting State Machines
219
detail in the next subsection. Here we give an informal general description of the
transitions.
R0.0 thru R0.4 describe the execution of functions in the initial phase 0. Only
the processor manufacturer is allowed to invoke functions in this phase and
the required function must be enabled.
R0.0 states that if the function belongs to class FTest0 and the corresponding
test succeeds, phase 1 will be entered, and the test functions of that class
are disabled.
R0.1 describes a shortcut leaving out phase 1: if the function belongs to class
FTest1 and the test succeeds, phase 2 will be entered, and all test functions
are disabled.
R0.2 states that if a test fails, the system will enter the error state.
R0.3 models the successful execution of any other (enabled) function, in which
case the function may change the chip state and yield a value.
R0.4 states that in all remaining cases of function execution the chip responds
with No and its state is unchanged.
R1.1 thru R1.4 describe the execution of functions in the upload phase 1 anal-
ogously to R0.1 thru R0.4.
R2.1 and R2.2 describe the execution of functions in the usage phase 2 anal-
ogously to R0.3 and R0.4.
R3.1 and R3.2 describe the execution of functions in the error phase analo-
gously to R0.3 and R0.4, except that the only function allowed to be executed
in this phase is chip identiﬁcation.
R4.1 and R4.2 describe the eﬀects of a speciﬁc operation used for uploading
new (operating system and application) functionality on the chip. This must
be done by subjects trusted by the processor manufacturer and is allowed
only in phase 1.
R4.1 describes the admissible situations, and
R4.2 describes all other cases.
R5.1 thru R5.3 describe the eﬀects of attacks. Any attempts to tamper with
the chip and to read security-relevant objects via physical probing on side
channels (by mechanical, electrical, optical, and/or chemical means), for ex-
ample diﬀerential power analysis or inspecting the silicon with a microscope,
are modeled as a special “spy” input. Note that modeling physical attacks
in more detail is not feasible because this would require a model of physi-
cal hardware. In particular, the conditions (and related mechanisms) under
which the processor detects a physical attack is beyond the scope of the
model.
R5.1 describes the innocent case of reading non-security-relevant objects in any
regular phase, which actually reveals the requested information.
R5.2 describes the attempt to reading security-relevant objects in any regular
phase. The chip has to detect this and enters the error phase, while the
requested object may be revealed or not. This concept is called “destructive
reading”: one cannot rule out that attacks may reveal information even about
security-relevant objects, but after the ﬁrst of any such attacks, the processor
hardware will be “destroyed”, i.e. cannot be used regularly.
R5.3 states that in the error phase no (further) information is revealed.

220
David von Oheimb and Volkmar Lotz
4.2
Isabelle Theory
Next we give the Isabelle/HOL representation of the SLE66 model. For lack of
space, of course we can show only the most essential deﬁnitions and a very small
selection of the transition rules. Yet we do describe the slight extension men-
tioned above. For a detailed formal description of the LKW model see [LKW00].
The full Isabelle deﬁnitions are contained in [Ohe02b].
Objects stored on the chip may be either functions or data and are referred
to by object names:
datatype on = F fn | D dn
Objects are classiﬁed as security-relevant (demanding secrecy and integrity)
by including their names in the sets F_Sec or D_Sec, whose union is called Sec. In
order to meet the additional requirements of [AETS01], the domain of security
relevant functions F_Sec of the original LKW model has been reﬁned to the
disjoint union of F_PSec and F_ASec, which control the protection of the processor
and application functionality, respectively.
The four control states of the SLE66 are deﬁned as
datatype ph = P0 | P1 | P2 | Error
The data state consists of two ﬁelds, a function store and a data store:
record data =
valF :: "fn ❀val"
valD :: "dn ❀val"
The initial data state is declared but not actually deﬁned. This is a typical
example of underspeciﬁcation, an important modeling technique.
const s0 :: data
We need only two port names, one for input and one for output:
datatype interface = In | Out
Possible input to the chip consists of either the two kinds of regular input
messages (modeling function execution and load commands to the SLE66), or the
Spy operation. The chip may respond with a value or a status message indicating
success or failure.
datatype message =
Exec sb fn | Load sb fn val | Spy on
| Val val | Ok | No
The subjects sb performing regular operations identify themselves to the chip
via physical means. The actual authentication mechanism, as well as many other
implementation details, is beyond the scope of this article.
Having deﬁned its various parameters, we can now give the new ism theory
element that speciﬁes the SLE66 model:
ism SLE66 =
ports interface

Formal Security Analysis with Interacting State Machines
221
inputs
"{In}"
outputs
"{Out}"
messages
message
state
control P0 :: ph
data
s0 :: data
transitions
R0.0: P0 -> P1
pre "f ∈fct s∩FTest0", "test f s"
in
In
"[Exec Pmf f]"
out
Out "[Ok]"
eﬀ
"valF := valF s⌊-FTest0"
Rule R0.0 speciﬁes execution of a test function f by the processor manufacturer
Pmf from the initial phase P0: if the test is successful then the SLE66 enters the
next phase P1, disables the test functions FTest0, and answers with Ok. This rule
is typical for interactions with the SLE66 in the sense that a single input triggers
a single output. Note that the direct relation of input and output is expressed
easily using ISMs, whereas using IOAs, two transitions would be required whose
relation would be cumbersome to express and to use during veriﬁcation.
R5.2: ph -> Error
pre "ph ̸= Error", "oname ∈Sec",
"v ∈{[],[Val (the (val s oname))]}"
in
In
"[Spy oname]"
out
Out "v"
eﬀ
"valF := fs, valD := ds"
Rule R5.2 speciﬁes the typical reaction of the SLE66 upon attacks trying to
read (the representation of) a secret object: The desired value may be output
or not, but in any case the Error phase is reached. Note that R5.2 is a generic
transition from any regualar phase to the Error phase. Furthermore, two sorts of
nondeterminism are involved: v denotes either the empty output or the singleton
output giving the desired value, and the attack may corrupt the function and
data stores arbitrarily.
In contrast to the original LKW model, the Load operation may upload not
only non-security-relevant functions but also functions of the application security
domain, as long as they are not overwritten:
R4.1: P1 -> P1
pre "f ∈F_NSec ∪(F_ASec - fct s)"
in
In
"[Load Pmf f v]"
out
Out "[Ok]"
eﬀ
"valF := valF s(f→v)"
All remaining transition rules and further details on the system model may
be found in [LKW00] and [Ohe02b].

222
David von Oheimb and Volkmar Lotz
4.3
Properties
The original security objectives for the SLE66 were stated as follows.
SO1. “The hardware must be protected against unauthorised disclosure of se-
curity enforcing functionality.”
SO2. “The hardware must be protected against unauthorised modiﬁcation of
security enforcing functions.”
SO3. “The information stored in the processor’s memory components must be
protected against unauthorised access.”
SO4. “The information stored in the processor’s memory components must be
protected against unauthorised modiﬁcation.”
SO5. “It may not occur that test functions are executed in an unauthorised
way.”
Later, an additional requirement concerning the conﬁdentialiy and integrity of
Smartcard Embedded Software, which is not part of the security enforcing func-
tionality, has been added [AETS01, §4.1].
Having deﬁned the SLE66 system model, these informal statements can now
be expressed formally as predicates on the system behavior, describing unam-
biguously and in detail which states may be reached under which circumstances,
which data may be modiﬁed, and which output may appear on the output chan-
nel.
After formalizing the security objectives, it is natural to ask if the chip be-
havior, as speciﬁed in the system model, actually fulﬁlls these requirements. The
corresponding proofs have been conducted ﬁrst using pen and paper, as reported
in [LKW00]. Within the ISM framework, we meanwhile have veriﬁed these prop-
erties even mechanically (and thus with maximal reliability) using Isabelle.
Due to the abstract speciﬁcation style where the semantics of parts of the
chip functionality is not fully speciﬁed, it turns out that in order to prove the
properties, a few general axioms are required. These assert for example that
security-relevant functions do not modify security-relevant functions:
Axiom1: "f∈fct s∩F_Sec =⇒valF (change f s)⌊F_Sec = valF s⌊F_Sec"
In comparison to the version of this axiom in the original model, the scope of
functions f has been extended from “initially available” to “security-relevant”,
reﬂecting the changes to rule R4.1. Part of the lemmas as well as the formalized
security objective FSO2.1 change accordingly:
FSO21: "[[((ib,(ph,s)),p,(ib’,(ph’,s’))) ∈Trans; ph’ ̸= Error;
g ∈fct s∩fct s’∩F_Sec]] =⇒valF s’ g = valF s g"
The proof of this property is — as usual — by induction on the construction of
all runs the SLE66 ISM can perform, which boils down to a case distinction over
all possible transitions. Most cases are trivial except for those where function
execution may change the stored objects, which are described by the rules R0.3,
R1.3, and R2.1. Here an argumentation about the invariance of security-relevant
functions g is needed, which follows easily from Axiom1 and Axiom2 stating the
analogous property for non-security-relevant functions f.

Formal Security Analysis with Interacting State Machines
223
The third (and last) axiom introduced in the LKW model states that in
phase 2, a non-security-relevant function may not “guess” or (accidentally) reveal
security-relevant information.
When machine-checking the proofs contained in [LKW00] with Isabelle, we
noticed that a fourth axiom was missing that makes an implicit but important
assumption explicit: if a function object may be referenced in two (diﬀerent) ways
and one of them declares the function to be security-relevant, the other has to
do the same. Such experience demonstrates how important machine support is
when conducting formal analysis.
Another omission was that in the proof of the security objective FSO5 an
argumentation about the accessibility of certain functions was not given in a rig-
orous way. We ﬁx this by introducing an auxiliary property (where, as typical
with invariants, ﬁnding the appropriate one is the main challenge) and proving
it to be an invariant of the ISM:
no_FTest_invariant :: "state ⇒bool"
"no_FTest_invariant ≡λ(ph,s).
∀f ∈fct s. (ph = P1 −→f /∈FTest0) ∧(ph = P2 −→f /∈FTest)"
Exploiting the invariant, we can prove the desired property easily:
FSO5: "[[((ib,(_,s)),(p,_,(_,s’))) ∈Trans; ib In = Exec sb f#r;
f ∈FTest]] =⇒sb = Pmf ∨p Out = [No] ∧s’ = s"
The Isabelle proofs of all six theorems formalizing the security objectives and
the two lemmas required are well supported by Isabelle: each of them takes just
a few steps, about half of which are automatic.
The formalization of the remaining security objectives as well as their proofs
wrt. the system model may be found in [LKW00] and [Ohe02b].
5
Needham-Schroeder Public-Key Protocol
As an example of an interaction-oriented system modeled with ISMs, we take
Lowe’s ﬁx of the Needham-Schroeder public-key authentication protocol [Low96],
which we call NSL. The emphasis here is not to provide new insights to the
protocol, but to take a well-known (and thus easy to compare) benchmark system
in order to show that, using the ISM approach, not only high-level requirements
analysis but also low-level analysis of distributed systems can be done in a both
rigorous and elegant way.
5.1
AutoFocus Diagrams
The system consists of an agent called Alice aiming to establish an authen-
ticated session with another agent called Bob in the presence of an Intruder
according to the Dolev-Yao attacker model [DY83]. As will be motivated in
§5.2, we introduce a server NGen generating nonces. The corresponding system
structure diagram in Figure 4 shows the four components with their data state
(reﬂecting the expectations of the two agents, the set of messages the intruder

224
David von Oheimb and Volkmar Lotz
Bob
Alice
Intruder
NGen
IB:msg
AI:msg
IA:msg
BI:msg
NA:msg
NB:msg
Local Variables:
  agent Bpeer 
  nonce BnA 
  nonce BnB 
Local Variables:
  set(msg) known = initState Intruder
Local Variables:
  agent Apeer 
  nonce AnA 
Local Variables:
  set(nonce) used = N0
Fig. 4. NSL System Structure Diagram
knows of, and the set of already used nonces, respectively) and the named con-
nections between them. Even if sometimes neglected, agents involved in com-
munication protocols do have state: their current expectations and knowledge.
This is made explicit in a convenient way by describing their interaction be-
havior with state transition diagrams. Figure 5 shows the three states of the
agent Alice and the transitions between them, which have the general format
guard : inputs : outputs : assignments.
In the initial state, Alice decides which agent she wants to talk to and sends
the corresponding request. In the next state she awaits the response from the
prospective peer before sending an acknowledgment. The third state represents
(hopefully) successful session establishment. From the example of Alice’s tran-
Init
Wait
Fine
 : NA ? Nonce(nA) : AI ! Crypt (pubK(B), MPair (Nonce(nA), Agent(Alice)))
 : Apeer := B, AnA := nA
nA = AnA & B = Apeer : IA ? Crypt (pubK(Alice), MPair (Nonce(nA), MPair (Nonce(nB), Agent(B))))
 : AI ! Crypt (pubK(B), Nonce(nB)) :
Fig. 5. NSL State Transition Diagram: Alice

Formal Security Analysis with Interacting State Machines
225
sitions we realize that control state information is the natural way to ﬁx the
order of protocol steps.
If the analysis needs to include the possibility that an agent takes part in
more than one protocol run simultaneously, this can be modeled by multiple
instantiation of the respective agent — under the assumption that from that
agent’s perspective the protocol runs are independent of each other.
5.2
Isabelle Theory
We base our ISM model on the formalization by Paulson [Pau98]. His so-called
“inductive approach” is tailored to semi-automated veriﬁcation of cryptographic
protocols. Its great advantage is a high degree of automation, due to abstraction
to the core semantics of the protocols: event traces. On the other hand, this
makes both the models and the properties at least cumbersome to express: state
information is implicit, yet often it has to be referred to, which is done by
repeating suitable parts of the event history and sometimes even by introducing
auxiliary events.
For lack of space, we do not show the deﬁnitions of the various state and
message components since they are straightforward and analogous to the SLE66
model. Moreover, we show only the ISM deﬁnition of agent Bob as a typical
representative.
ism Bob =
ports channel
inputs
"{NB,IB}"
outputs
"{BI}"
messages
msg
state
control Idle :: B_control
data
B0
:: B_data
transitions
Resp: Idle -> Resp
in
NB "[Nonce nB]",
IB "[Crypt (pubK Bob) {|Nonce nA, Agent A|}]"
out
BI "[Crypt (pubK A) {|Nonce nA, Nonce nB, Agent Bob|}]"
eﬀ
"Bpeer := A, BnA := nA, BnB := nB"
Ack’: Resp -> Conn
pre
"nB’ = BnB s"
in
IB "[Crypt (pubK Bob) (Nonce nB’)]"
Note that Bob’s ﬁrst transition Resp takes two inputs, from the nonce generator
and the intruder, and produces one output. If we modeled this transition using
IOAs, we would have needed three transitions with intermediate states. The
precondition of transition Ack’ could have been made implicit by moving the
comparison as a pattern to the in part, yet we make it explicit in order to
emphasize its importance. The local variable BnB serves to remember the value

226
David von Oheimb and Volkmar Lotz
of the nonce expected, while the other two variables express Bob’s view to whom
he is connected in which session. In Paulson’s approach, this state information
is implicit in the event trace.
Modeling the freshness of nonces is intricate. In Paulson’s model [Pau98],
nonces are generated under the side condition that they do not already appear in
the current event history. This criterion refers to the semantic and system-global
notion of event traces — something not available from the (local) perspective
of an ISM. We solve the problem by introducing a component called NGen that
performs the generation of nonces for all agents in a centralized fashion. In
this way we can ensure global freshness with a local criterion. Note that this
component is just a modeling aid and thus its correct interplay with the agents
does not need to be analyzed. We could alternatively express global freshness
by adding an axiom restricting system runs in the desired way, yet we prefer
the more constructive approach and derive the required freshness property as
a lemma.
5.3
Properties
Properties of protocols speciﬁed with ISMs may be expressed with reference
to both the state of agents and the messages exchanged. In the case of NSL,
the most interesting property is authentication of Alice to Bob (actually, even
session agreement [Low97] from Bob’s view), which we formulate as
[[Alice /∈bad;
Bob /∈bad;
(b,s)#cs ∈Runs;
Bob_state s = (Conn, (|Bpeer = Alice, BnA = nA, BnB = _|))]] =⇒
∃(_,s’) ∈set cs.
Alice_state s’ = (Wait, (|Apeer = Bob, AnA = nA|))
This can be quite intuitively read as: if in the current state s of the system Bob
believes to be connected to Alice within a session characterized by the nonce
nA then there is an earlier state s’ where Alice was in the waiting state after
initiating a connection to Bob using the same nonce nA.
It is interesting to compare the above formulation with the one given by
Paulson:2
[[A /∈bad;
B /∈bad;
evs ∈ns_public;
Crypt (pubK B) (Nonce NB) ∈parts (spies evs);
Says B A (Crypt (pubK A) {|Nonce NA,Nonce NB,Agent B|}) ∈set evs
]] =⇒
Says A B (Crypt (pubK B) {|Nonce NA,Agent A|}) ∈set evs
This statement is necessarily more indirect since the beliefs of the agents have
to be coded by elements of the event history. At least in this case, all messages
of the protocol run have to be referred to. Note that this formulation makes
stronger assumptions than ours because the value of the nonce NB is involved.
On the other hand, due to the extra detail concerning agent state and the
input buﬀers (which are not actually required here), the inductive proofs within
2 http://isabelle.in.tum.de/library/HOL/Auth/NS_Public.html

Formal Security Analysis with Interacting State Machines
227
the ISM approach are more painful and require more lemmas on intermediate
states of protocol runs than Paulson’s inductive proofs.
6
Conclusion
The Interacting State Machines approach turns out to oﬀer good support for
formal security analysis in the way required within an industrial environment.
ISMs are designed as high-level I/O automata, with additional structure and
communication facilities. Like IOAs, ISMs are suitable for describing typical
state-based communication systems relevant for security analysis, where ISM
provide increased simplicity wrt. specifying component interaction via buﬀered
communication and means to relate input and output actions directly.
The ISM approach oﬀers graphical representation by means of AutoFocus
System Structure Diagrams and State Transitions Diagrams. The graphical views
are closely related to the formal system speciﬁcation and veriﬁcation via a tool
translating the AutoFocus representation to an Isabelle theory.
We have shown that the ISM approach is equally applicable to a variety of
security analysis tasks, ranging from high-level security modeling and require-
ments analysis, typically showing less system structure but increased complexity
of state transitions, to security analysis of distributed systems including crypto-
graphic protocols, likely to exhibit advanced system structuring. The examples
explicate the importance of a fully formalized strategy, in particular, the LKW
model has been signiﬁcantly improved by identifying hidden assumptions and
completing sloppy argumentation.
Further work on ISMs includes the extension of the proof support in the ISM
level and the provision of a speciﬁcation language based on temporal logic. Addi-
tional AutoFocus capabilities may be made available, including further systems
views like event traces and simulation, as well as test case generation.
Acknowledgments We thank Guido Wimmel, Thomas Kuhn and some anony-
mous referees for their comments on earlier versions of this article.
References
[AETS01]
Atmel, Hitachi Europe, Inﬁneon Technologies, and Philips Semiconductors.
Smartcard IC Platform Protection Proﬁle, Version 1.0, July 2001.
217,
220, 222
[AGKS99]
David Aspinall, Healfdene Goguen, Thomas Kleymann, and Dilip Sequeira.
Proof General, 1999.
214
[But99]
Michael Butler. csp2B : A practical approach to combining CSP and B. In
Proceedings of FM’99: World Congress on Formal Methods, pages 490–508,
1999. http://www.dsse.ecs.soton.ac.uk/techreports/99-2.html.
214
[CC99]
Common Criteria for Information Technology Security Evaluation (CC),
Version 2.1, 1999. ISO/IEC 15408.
212, 217
[DY83]
Danny Dolev and Andrew C. Yao.
On the security of public key pro-
tocols.
IEEE Transactions on Information Theory, IT-29(12):198–208,
March 1983.
223

228
David von Oheimb and Volkmar Lotz
[Fis00]
Clemens Fischer. Combination and implementation of processes and data:
from CSP-OZ to Java. PhD thesis, University of Oldenburg, 2000.
214
[GL98]
Stephen J. Garland and Nancy A. Lynch. The IOA language and toolset:
Support for designing, analyzing, and building distributed systems. Tech-
nical Report MIT/LCS/TR-762, Laboratory for Computer Science, MIT,
August 1998.
213
[HSSS96]
Franz Huber, Bernhard Sch¨atz, Alexander Schmidt, and Katharina Spies.
Autofocus - a tool for distributed systems speciﬁcation.
In Proceedings
FTRTFT’96 - Formal Techniques in Real-Time and Fault-Tolerant Sys-
tems, volume 1135 of LNCS, pages 467–470. Springer-Verlag, 1996. See
also http://autofocus.in.tum.de/index-e.html.
214, 215
[ITS91]
Information Technology Security Evaluation Criteria (ITSEC), June 1991.
212
[JW01]
Jan J¨urjens and Guido Wimmel. Formally testing fail-safety of electronic
purse protocols. In Automated Software Engineering. IEEE Computer So-
ciety, 2001.
214
[Kay01]
Dilsun Kirli Kaynar. IOA language and toolset, 2001.
http://theory.lcs.mit.edu/tds/ioa.html.
213
[LKW00]
Volkmar Lotz, Volker Kessler, and Georg Walter.
A Formal Se-
curity Model for Microprocessor Hardware.
In IEEE Transactions
on Software Engineering, volume 26, pages 702–712,
August 2000.
http://www.computer.org/tse/ts2000/e8toc.htm.
217, 220, 221, 222,
223
[Low96]
Gavin Lowe. Breaking and ﬁxing the Needham-Schroeder public-key pro-
tocol using FDR. In Proceedings of TACAS, volume 1055, pages 147–166.
Springer-Verlag, 1996.
223
[Low97]
Gavin Lowe.
A hierarchy of authentication speciﬁcations.
In PCSFW:
Proceedings of The 10th Computer Security Foundations Workshop. IEEE
Computer Society Press, 1997.
226
[LT89]
Nancy Lynch and Mark Tuttle. An introduction to input/output automata.
CWI Quarterly, 2(3):219–246, 1989.
http://theory.lcs.mit.edu/tds/
papers/Lynch/CWI89.html.
213
[Ohe02a]
David von Oheimb. Interacting State Machines, 2002. Submitted for pub-
lication.
214, 216, 217
[Ohe02b]
David von Oheimb. The Isabelle/HOL implementation of Interacting State
Machines, 2002. Technical documentation, available on request. 220, 221,
223
[OLW02]
David von Oheimb, Volkmar Lotz, and Gerog Walter. An interpretation
of the LKW model according to the SLE66CX322P security target. Un-
published, January 2002.
217
[Pau94]
Lawrence C. Paulson. Isabelle: A Generic Theorem Prover, volume 828
of LNCS.
Springer-Verlag, 1994.
For an up-to-date description, see
http://isabelle.in.tum.de/.
214
[Pau98]
Lawrence C. Paulson. The inductive approach to verifying cryptographic
protocols. Journal of Computer Security, 6:85–128, 1998.
225, 226
[PNW+]
Lawrence C. Paulson, Tobias Nipkow, Markus Wenzel, et al.
The
Isabelle/HOL library. http://isabelle.in.tum.de/library/HOL/.
214
[S+]
Oscar Slotosch et al. Validas Model Validation AG.
http://www.validas.de/.
215

Formal Security Analysis with Interacting State Machines
229
[WW01]
Guido Wimmel and Alexander Wisspeintner. Extended description tech-
niques for security engineering. In M. Dupuy and P. Paradinas, editors, In-
ternational Conference on Information Security (IFIP/SEC). Kluwer Aca-
demic Publishers, 2001.
214

Decidability of Safety in Graph-Based Models
for Access Control
Manuel Koch1, Luigi V. Mancini2, and Francesco Parisi-Presicce2,3
1 Freie Universit¨at Berlin, Berlin (DE)
mkoch@inf.fu-berlin.de
2 Univ. di Roma La Sapienza, Rome (IT)
{lv.mancini,parisi}@dsi.uniroma1.it
3 George Mason University, Fairfax VA (USA)
fparisi@ise.gmu.edu
Abstract. Models of Access Control Policies speciﬁed with graphs and
graph transformation rules combine an intuitive visual representation
with solid semantical foundations. While the expressive power of graph
transformations leads in general to undecidable models, we prove that
it is possible, with reasonable restrictions on the form of the rules, to
obtain access control models where safety is decidable. The restrictions
introduced are minimal in that no deletion and addition of a graph struc-
ture are allowed in the same modiﬁcation step. We then illustrate our
result with two examples: a graph based DAC model and a simpliﬁed
decentralized RBAC model.
1
Introduction
Safety analysis determines, for a given set of policy rules and an initial state,
whether or not it is possible to reach a state in which a particular access right
is acquired by a subject that did not previously possess it. A system state is
said to be safe with respect to an access right r if no sequence of commands can
transform the state into a state that leaks r. Safety analysis was ﬁrst formalized
by Harrison, Ruzzo and Ullman [HRU76] in a model commonly known as HRU
model. The HRU model captures security policies in which access rights may
change and subjects as well as objects may be created and deleted [HRU76]. In
general, safety of a system state with respect to an access right r is an unde-
cidable problem, but decidability can be gained by restricting the models. For
example, an authorization system is decidable if one considers mono-operational
commands only [HRU76] or if the number of subjects is ﬁnite [LS78]. Sandhu and
Suri have shown in [SS92] that their model of non-monotonic transformations
(i.e., no deletion of access rights from the system), is subsumed by the model
of [LS78] if object creation is possible but no subject creation. The approach of
deﬁning limited access control models with suﬃcient expressive power for prac-
tical use has not been followed recently because of complexity (both conceptual
and algorithmic).
D. Gollmann et al. (Eds.): ESORICS 2002, LNCS 2502, pp. 229–244, 2002.
c
⃝Springer-Verlag Berlin Heidelberg 2002

230
Manuel Koch et al.
More recent research has focused on the alternative of deﬁning constraint
languages for the treatment of safety requirements. An example of a limited
logical language is RSL99 [AS00]. Unfortunately, the languages proposed so far
seem to lack either in simplicity or in expressive power.
We proposed a graph-based security framework to specify access control mod-
els [KMPP00, KMPP01a, KMPP01b] and we investigate in the present paper
the safety issue of this graph-based framework. Compared to the HRU model,
safety in our framework is decidable if each graph rule either deletes or adds
graph structure but does not do both.
Graph rules do not have to be mono-operational and object creation is pos-
sible. Subject nodes chosen from a predeﬁned ﬁnite set can be added to the
system graph using the graph rules. The graph transformations do not have to
be non-monotonical in the sense of [SS92].
There are other approaches [NO99, JT01] to the graphical representation of
constraints in access control. The ﬁrst deﬁnes a graphical model to combine role
inheritance and separation of duty constraints. The second one uses a graphical
model to express constraints based on set identiﬁcation and set comparison.
Neither paper deals with decidability of safety. In the context of the Take-Grant
model, decidability is discussed in [Sny77]. The model presented is graph based,
in the sense that the state is represented by a graph and state changes by graph
transformations as rewriting rules. Safety is described as the (in)ability to derive,
using the rewrite rules, a graph containing a speciﬁed edge.
In this paper, we illustrate our result with two simple examples: a graph
based DAC model and a decentralized RBAC model. These examples are over-
simpliﬁed in order to focus on the methodology to carry on the safety analysis.
Note that the graph-based security framework proposed can specify also more
expressive (and realistic) access control models for which the safety property
remains decidable.
The result can be seen also as a new mechanism to compute safety in the
presence of propositional constraints.
In section 2 of this paper, we review the basic notion of graph transforma-
tions; section 3 presents the decidability results and section 4 and section 5 show
examples of decidability in a graph-based DAC model and a decentralized RBAC
model, respectively. Section 6 contains a summary and points to future work.
2
Graph-Based Models
This section recalls some basic deﬁnitions and notation for graph transfor-
mations [Roz97]. A graph G = (GV , GE(e)e∈ET yp, tG
V , lG) consists of a set of
nodes GV , a relation GE(e) ⊆GV × GV for each edge type e in the set of
edge types ET yp, a typing mapping tG
V
: GV
→V T yp and a label map-
ping lG : GV →V Label for nodes. Node labels are elements of a disjoint union
V Label = X ∪C, where X is a set of variables and C is a set of constants.
A reﬂexive binary relation <⊆V Label × V Label is deﬁned as α < β if and only
if α ∈X or α = β.

Decidability of Safety in Graph-Based Models for Access Control
231
Figure 1 on the left-hand side depicts a graph with three nodes and three
edges. The node types are U (for user) and O (for objects). There is an edge type
for edges without any label and an edge type for edges with label r (for a read
access right). The labels for nodes are the user names Jackie and Thomas, the
object label is the object name newProject.pdf. We omit the explicit presentation
of the typing and labeling morphisms in the ﬁgures and attach types and labels
at the nodes and edges.
A graph morphism f = (fV , fE(e)e∈ET yp) : G →H between graphs G and H
is given by partial mappings fV : GV →HV between nodes and fE(e) : GE(e) →
HE(e) for e ∈ET yp between edges so that
– f preserves the graph structure, i.e., fE(e)((v, v′)) = (fV (v), fV (v′)) for all
(v, v′) ∈dom(fE(e)) and e ∈ET yp,
– f preserves types, i.e., tG
V (v) = tH
V (fV (v)) for each v ∈dom(fV ) and
– f respects the label order, i.e. lG(v) < lH(fV (v)) for each v ∈dom(fV ).
A graph morphism f is total (injective) if all underlying mappings fV and fE(e)
(e ∈ET yp) are total (injective). Nodes in the domain of a graph morphism need
not have variable labels. But if they do, they can be replaced by other variables
or constants; if they are not variables, the labels must be preserved.
A graph rule p(V ) : (r, A(p)) consists of a rule name p, a tuple of variables
V = (x1, ..., xn), xi ∈X, a label preserving injective graph morphism r : L →
R and a set A(p) of negative application conditions. The graph L, left-hand
side, describes the elements a graph must contain for p to be applicable. The
morphism r is undeﬁned on nodes/edges that are intended to be deleted, deﬁned
on nodes/edges that are intended to be preserved. Nodes and edges of R, right-
hand side, without a pre-image are newly created. The actual deletions/additions
are performed on the graphs to which the rule is applied. A Negative Application
Condition (NAC) consists of a set A(p) of pairs (L, N), where the graph L is
a subgraph of N. The part N \ L represents a structure that must not occur in
a graph G for the rule to be applicable. In the ﬁgures, we depict (L, N) by the
graph N, where the subgraph L is drawn with solid and N \ L with dashed lines
U Ux
Richard
U
Graph Rules
Graph Rule Instances
U
U
O
delete 
object
U
U
Thomas
Jackie
Jackie
newProject.pdf
Jackie
U
Ux
Ux
U
Ox
O
delete 
new user
object
new user
O
r
newProject.pdf
r
Fig. 1. An example of a graph (left), graph rules and graph rule instances (right)

232
Manuel Koch et al.
(e.g., rule remove user in ﬁgure 4). A rule p with NAC A(p) is applicable to G
if L occurs in G and it is not possible to extend L to N for each (L, N) in A(p).
A rule instance p(c1, ..., cn) : (rI, A(p)) with ci ∈C for a rule p(x1, ..., xn) :
(r, A(p)) instantiates the rule variables xi by constants ci (i = 1...n), so that the
constants in the left-hand side and the right-hand side are unique.
Figure 1 on the right-hand side shows examples for rules and rule instances.
The rule new user creates a new user, the rule delete object deletes an object
which belongs to a user. The rules contain the variables Ux and Ox which can
be instantiated by constants. The bottom of the ﬁgure shows two possible rule
instances where, on the one hand, Ux is instantiated by Richard (rule new user),
on the other hand Ux is instantiated by Jackie and Ox by newProject.pdf.
A match for a rule instance p(C) : (r, A(p)) in a graph G is a label preserving
total graph morphism m : L →G. The application of a rule instance p(C) :
(r, A(p)) to a graph G is given by a match for p in G which satisﬁes the NAC
A(p). The direct derivation G
p,m
⇒H is given by the pushout of r and m in the
category of graphs [Roz97]. The pushout is constructed by deleting all elements
m(L\dom(r)) and adding afterwards all elements R\r(L) to the part m(dom(r)).
A derivation sequence ρ = (G0
p0,m0
⇒
G1
p1,m2
⇒
G2
p2,m2
⇒
...) is a sequence of direct
derivations Gi
pi,mi
⇒
Gi+1. The length of a derivation sequence is the number
of direct derivations it consists of. We denote a derivation sequence (possibly of
length 0) from G0 to Gn using rule instances of R by G0
R
⇒∗Gn.
Figure 2 depicts an example of a derivation sequence of length 2. First, a rule
instance for the rule delete object is applied. The rule delete object speciﬁes the
deletion of an object which belongs to a user, in this rule instance, the deletion
of the object newProject.pdf of user Jackie. We can ﬁnd these graphical elements
into graph G and delete the newProject.pdf node. Since no dangling edges must
delete 
object
U
Richard
U
U
U
Richard
U
O
Jackie
newProject.pdf
U
Jackie
m
m*
new user
m’
U
U
Thomas
Jackie
G
m’*
G’
H
r
O
newProject.pdf
r
U
Thomas
Jackie
U
Thomas
Jackie
Fig. 2. A derivation sequence

Decidability of Safety in Graph-Based Models for Access Control
233
occur in graphs, dangling edges are removed as well. In the second derivation
step, a rule instance for the rule new user is applied to the result graph of the
ﬁrst derivation step. The rule new user adds a new user, in this instance the user
Richard.
Graph transformations are the basic component for security policy frame-
works used to model access control policies (a role-based model in [KMPP00],
a lattice-based model and a discretionary model in [KMPP01a]).
3
Decidability in Graph Transformations
Deﬁnition 1 (safety). Let Q0 and A be graphs and RI be a set of graph rule
instances. The graph Q0 is safe w.r.t. RI and A if and only if there does not
exist a graph Q so that A ⊆Q and Q0
RI
⇒∗Q.
In other words, an initial system state represented by Q0 is safe, with re-
spect to the access right represented by the graph A and a set of policy rule
instances represented by RI, if and only if no derivation sequence exists from Q0
to a graph Q such that A ⊆Q, that is no sequence of graph rule instances can
transform Q0 in a state Q that leaks A.
Proposition 1. Safety for graph transformations in general is undecidable.
This is not surprising since graph transformation rules can model the rules
of a type 0 grammar, but safety is decidable if each rule is either expanding or
deleting. Deleting rules delete nodes or edges, but add nothing (i.e., r(L) = R)
and expanding rules may add nodes and edges, but do not delete anything (i.e.,
dom(r) = L ⊂R) and do not have a negative application condition.
Proposition 2 (upper bound). Let Q0 and A be graphs, R+
I be a ﬁnite set
of expanding graph rule instances and Der = {Q0
R+
I
⇒∗Q|A ⊆Q} be the set of
all derivation sequences starting at Q0 that use the rule instances in R+
I and
ending in a graph Q which contains A. Then, the minimal derivation ρmin in
Der (i.e., for all ρ ∈Der, the length of ρ is greater or equal the length of ρmin)
has an upper bound that depends only on R+
I , Q0 and A.
Proof. Let R0 be the set of rule instances in R+
I that are applicable to Q0 and
M(p) = {m : L →Q0|m total} the set of all matches of rule instance p ∈R0
in Q0. Furthermore, let Rf be the set of rule instances in R+
I that construct
parts of A, i.e., (p : L
r→R) ∈Rf if and only if there is a nonempty injective
partial graph morphism h : R →A so that h(R \ r(L)) ∩A ̸= ∅. The set Rf(p)
contains all these partial morphisms h in A for the rule instance p ∈Rf.
If all rule instances in Rf are applicable to a graph Qf, the number of rule
applications to construct graph A is at most 
p∈Rf card(Rf(p))1. Then, all
1 card(A) of a set A denotes the number of elements in A.

234
Manuel Koch et al.
elements of A are constructed that can be constructed by the rule instances in
R+
I and one can decide whether A occurs or not.
We show now, that there is an upper bound for the number of rule ap-
plications to construct a graph Qf (i.e., a graph to which all rule instances
are applicable) from Q0 by the rule instances in R+
I . We consider all paths
σ = (p0
m0
→p1 →p2 →...pn−2 →pn−1), where p0 ∈R0, m0 ∈M(p0), pi ∈R+
I
for each 0 ≤i < n, so that each pi occurs at most once. The paths have a length
not greater than card(R+
I ). Let P be the set that contains all the paths σ.
A derivation ρ = (Q0
p0,m0
⇒
Q1
p1,m1
⇒
...
pn−1,mn−1
⇒
Qn) is a transformation of
a path σ = (p0
m0
→p1 →p2... →pn−1), if it applies the rule instances in the
order given in the path, starting at match m0. Let Ωbe the set of derivations
transformed from the paths in P. All derivations ρ ∈Ωhave the maximal length
of maxDerivation = card(R+
I ).
r*0
r*1
m*
0
m*
1
r*2
m*
2
1 1
p  :r 
2 2
p  :r 
L
R
Q
L
Q
R
m
Q
p  :r 
0 0
0
0
1
1
2
1
0
0
1
m
L 2
R 2
m2
Q3
We claim now: if a graph Qf can be constructed by the rule instances in R+
I ,
there is a derivation ρ = (Q0
p0,m0
⇒
Q1
p1,m1
⇒
... ⇒Qf) in Ω. If we assume the
opposite, there is a derivation ρ′ = (Q0
p0,m0
⇒
Q1
p1,m1
⇒
...
pj,mj
⇒
Q′
f) ̸∈Ω, which
constructs a graph Q′
f to which all rule instances are applicable. Then, the path
σ′ = (p0
m0
→p1 →p2... →pj) can not be in P, i.e., there must be at least one rule
instance pi that occurs more than once in the path σ′. In the diagram below, we
assumed that the rule instance pi is applied at position i and k.
1 1
p  :r 
r*0
m*
k
r*k
m*
0
r*i
m*
i
L
R
mi
q
q
i
p  :r i
Q
Qi+1
i
i
i
i
i+1
Q
Q
r
i
i+1
i
m*
j
r*j
L
R
L
R
m
p  :r 
0 0
0
0
1
1
0
1
m
q0
q1
Q
Q1
0
L
R
mk
m
q
q
p  :r 
i
i
i
i
k
Qk
k+1
p     :r k+1
Lk+1
R k+1
Qk+1
k+1
k+1
Q
Q1
0
r 0
Q
Q
r
i
i+1
i
Qk
~
Qk+2
Q
Q1
0
r 0
Q
Q
r
k
k+1
k
Qk+2
Qk+2
~
~
~
~
~
~
id
u1
ui+1
uk
L
m
q
q
p     :r 
Q
Q’f
Q
Q’
r
j
j
R j
f
j
j
j
j
j−1
f
j
Q~
f
uf
uk+2
m1
~
mk+1
~
In the derivation ρ′, we identify in each graph Qi (0 ≤i ≤j) all nodes with
the same label and get total surjective morphisms qi : Qi →¯Qi. The sequence
( ¯Q0 →¯Q1... →¯Q′
f) is generally not a derivation sequence, since the diagrams ¯ri◦

Decidability of Safety in Graph-Based Models for Access Control
235
qi ◦mi = qi+1 ◦m∗
i ◦ri are generally not pushout diagrams. But the identiﬁcation
ensures that qi+1 ◦m∗
i (Ri) = qk+1 ◦m∗
k(Ri), so that in particular qi+1 ◦m∗
i (Ri) =
qk+1 ◦m∗
k(Ri) ⊆¯Qi+1 ⊆¯Qk ⊆¯Qk+1.
We construct now the derivation sequence ˜ρ
=
( ˜Q0
p0, ˜m0
⇒
˜Q1
p1, ˜m1
⇒
... ˜Qk−1
pk−1, ˜mk−1
⇒
˜Qk
pk+1
⇒
˜Qk+2 ⇒.... ⇒˜Qf), where ˜Q0 = Q0, ˜m0 = q0 ◦m0
and ˜mi = ui ◦mi where ui is the universal pushout morphism w.r.t. the
pushout diagram m∗
i ◦ri = r∗
i ◦mi (0 < i < j, i ̸= k + 1). We deﬁne
˜mk+1 = uk ◦q−1
k
◦qk+1 ◦mk+1, what is possible since qk is surjective and ¯rk is
the identity on Qk, since qi+1 ◦m∗
i (Ri) = qk+1 ◦m∗
k(Ri).
By assumption, there is a match mi : Li →Qf for each rule instance pi :
ri ∈R+
I . Then, we have also a match uf ◦mi : Li →˜Qf, i.e., all rule instances
in R+
I are applicable to ˜Qf. In such a way, we can remove each repeated rule
instance application from ρ′ and get a path ˜σ ∈Ω, what is a contradiction to
our assumption.
To conclude, the upper bound for constructing a graph Qf from Q0 us-
ing the rule instances in R+
I in which all rule instances of Rf are applicable
is card(R+
I ). The necessary rule applications for constructing A from Qf are
at most 
p∈Rf card(Rf(p)). Together, we get an upper bound of card(R+
I ) +

p∈Rf card(Rf(p)).
Theorem 1 (safety). Safety of a graph Q0 with respect to a graph A and a ﬁ-
nite set of graph rule instances RI is decidable if RI contains only expanding
and deleting graph rules.
Proof. We show ﬁrst that the minimal length derivation has only expanding
rules. Suppose Q0
p1
⇒Q1
p2
⇒...
pn
⇒Q is a minimal length derivation reaching
graph Q that contains A. Then, each rule instance pi for 1 ≤i ≤n is an
expanding rule. If we assume not, let pj be a deleting rule. The removal of
the rule pj from the derivation does not aﬀect the construction of the graph A
in Q: If we remove pj, we get the derivation sequence Q0
p1
⇒Q1
p2
⇒...
pj−1
⇒
Qj−1
pj+1
⇒Q′
j+1...
pn
⇒Q′. Since the graph Q could be constructed also without
the structure Qj−1 \ Qj and expanding rules do not have application conditions
forbidding some structure in a graph, the rule pj+1 can be applied to Qj−1 as
well. Analog, the rules pk for k > j can be applied constructing a graph Q′
containing A. Hence, we got a shorter derivation what is a contradiction to the
assumption of a minimal length derivation sequence.
Thus, the minimal sequence for constructing a graph containing A starting
at Q0 contains only expanding rules. Proposition 2 shows that the minimal
sequence to construct A has an upper bound m. A decision procedure would try
all derivation sequences of expanding rules of length up to m.
Proposition 3. Let A be a graph and pI(C) : LI
rI→RI be an instance rule of
rule p(V ) : L
r→R. Furthermore, let
– Rf(p) be the set of nonempty partial injective morphisms g : R →A so that
g(R \ r(L)) ∩A ̸= ∅and

236
Manuel Koch et al.
– Rf(pI) the set of nonempty partial injective morphisms g : RI →A so that
g(RI \ rI(LI)) ∩A ̸= ∅.
Then, card(Rf(pI)) ≤card(Rf (p)).
Proof. Bases on the fact that the variables in rules can be mapped to any appro-
priate constant, but constants in rule instances can be mapped only to the same
constants: For each morphism in Rf(pI) there is a morphism in Rf(p). A mor-
phism in Rf(p), however, does not need to have a counterpart on the instance
level, since nodes labeled with a constant can be mapped only to nodes labeled
with the same constant.
Corollary 1 (upper bound). Let Q0 and A be graphs, R+ a set of expanding
rules, R+
I (p) a ﬁnite set of rule instances for p ∈R+ and Der = {Q0
R+I
⇒∗
Q|A ⊆Q} be the set of all derivation sequences starting at Q0 and ending in
a graph Q which contains A. Then,
UB = card(R+
I ) + RF
is
an upper
bound for
the
minimal
derivation
in
Der,
where
RF
=

p∈R+ card(R+
f (p)).
Proof. In the proof of proposition 2, the upper bound was given by
card(R+
I ) +

pI∈Rf
card(Rf (pI)).
By proposition 3, we have

pI∈Rf
card(Rf(pI)) ≤

pI∈R+
I
card(Rf(pI)) ≤

p∈R+
card(Rf(p)).
Figure
3
depicts
the
decidability
algorithm.The
method
setOfOver-
laps(rule,graph) returns the set of partial injective mappings from the right-
hand side of the rule to the graph, where the domain of the mapping con-
tains at least an element constructed by rule (i.e. in R \ r(L)). The method
setOfDerivationSequences(ruleSet,startGraph, length) constructs all derivation
sequences up to length starting from startGraph using rule instances of ruleSet.
The method card(set) returns the number of elements in set. The method sub-
graph(graph1,graph2) returns true if graph1 is a subgraph of graph2, otherwise
it returns false.
The decidability algorithm presented above can probably be optimized. The
apparent complexity is exponential because it requires generating a set of over-
laps and checking the subgraph property. The eﬀective complexity of the algo-
rithm is probably much lower than the computed worst case since labels (or
unique names) on nodes reduce considerably the number of possible matchings.
Graph transformation tools [EEKR99] can be used to automate the process
of generating all the mappings and all the derivation sequences to be checked
against the unwanted conﬁguration.

Decidability of Safety in Graph-Based Models for Access Control
237
input: sets RI(p) (p ∈R) of expanding rule instances of rules in R, graph Q0 and
graph A
output: true, if there is a derivation sequence (Q0 ⇒... ⇒Q) where A ⊆Q, false
otherwise
begin
//get the rules which construct parts of A
RF = 0;
for each p ∈R {
Rf(p) = setOfOverlaps(p, A);
RF = RF + card(Rf(p));
}
//construct the upper bound for derivation sequences
length = card(RI) + RF ;
Der = setOfDerivationSequences(RI, Q0, length);
for each (Q0 ⇒... ⇒Q) ∈Der
if subgraph(A,Q) then return true;
return false;
end
Fig. 3. The decidability algorithm
4
Example: Decidability in a Graph-Based Model for
Discretionary Access Control
We describe a graph-based DAC model for which the safety property is decidable.
This simpliﬁed DAC model provides subject and object creation/removal and
the granting of rights to access objects from one subject to another subject. The
terms subject and user are synonymies in this section. For simplicity, the access
rights considered are read and write only.
4.1
Graph Model for the DAC Policy
Users are represented by nodes of type U, objects to which users may have access
by nodes of type O. Labels attached to user nodes specify the user name, labels
attached to objects the object name. The variables used in rules are denoted by
Ux, Uy, Ox, Oy, ....
The rule new user in ﬁgure 4 introduces a user Ux. The creation of new
objects (e.g., ﬁles and directories) on behalf of a user Ux is speciﬁed in the rule
new object. Every object has an owner, namely the user who has created the
object. The ownership of an object to a user is modeled by an edge from the
user node to the object node. The owner of the new object has read and write
access to the object. A permission on an object o for the owner is modeled by an
edge from o to the owner with a label specifying the permitted access right (r
for read and w for write). The owner of an object can simply remove the object
(modeled in rule delete object). Users can be removed with rule remove user.

238
Manuel Koch et al.
U
Ox
O
U
U
U
Ux
Ox
O
Ux
U
delete 
object
remove user
U
O
Ux
Ox
U Ux
new user
U
Ux’
U
Ox
O
Ux
r
U
Ux’
U
Ox
O
Ux
r
Ox
O
U
Ux
r
Ox
O
U
Ux
r
new object
Ux
w
r
U
Ux
revoke read
 right
Ux’
U
Ox
O
Ux
Ux’
U
Ox
O
Ux
r
grant read
 right
r
copy object
O
Ox’
r
w
Fig. 4. The graph rules for the DAC model
To prevent objects without owner, the deletion of users is possible only if they
do not own objects (speciﬁed in the NAC of rule remove user which forbids the
application of remove user to a user who is connected to an object).
A user may grant its right on an object to other users. The rule grant read
right speciﬁes how a user Ux grants a read right on an object Ox (the right
is modeled by an r-labeled edge from Ox to Ux) to another user Ux′. The r-
labeled edge from Ox to Ux′ speciﬁes the read access right for Ux′ on Ox. The
rule for granting the write permission is analogously. Whereas an access right on
an object can be granted by anybody who has the right, the revocation of a right
on an object can be only executed by the object’s owner. This is speciﬁed in rule
revoke read right. The revocation of the write right is similar to rule revoke read
right.
If a user Ux has read access to an object (i.e., there exists an edge with label r
from the object to the user), and Ux copies the object, then Ux becomes the
owner of the copy with read and write permission. The rule copy object speciﬁes
the copying of objects.
4.2
Safety of the DAC Model
This section carries on a safety analysis of the initial system state Q0 shown
in ﬁgure 5 with users Richard, Jackie and Thomas. Jackie is the owner of the
object newProject.pdf. Thomas has read access to the object newProject.pdf and
Richard has no right to access the object at all. Jackie has read and write access
to the object because she is the object owner.

Decidability of Safety in Graph-Based Models for Access Control
239
newProject.pdf
r
A
U
O
Richard
Richard
U
0
Q
U
U
O
r
w
newProject.pdf
r
Jackie
Thomas
Fig. 5. An example for safety
Suppose that we would like to know now if it is possible for Richard to read
newProject.pdf unbeknown to Jackie. That means, is it possible to construct
a graph Q starting from Q0 using the rules in ﬁgure 4 so that graph A of ﬁgure 5
is a subgraph of Q? Theorem 1 and corollary 1 state that it is suﬃcient to check
all the derivation sequences of rule applications using rule instances of expanding
rules up to a length of the upper bound UB = card(R+
I ) + RF . The expanding
rules in ﬁgure 4 are: new user, new object, grant read right and copy object.
The number of rule instances card(R+
I ) depends on the (ﬁnite) set of lables
for user names, where UN denotes the number of elements, and of the (ﬁnite)
set of labels for object names, where we denote by ON the number of elements.
For example, the number of rule instances for new user is equal to UN since new
user has one variable for a user. The number of rule instances for rule new object
is UN · ON since both a user and an object variable must be instantiated. The
number of rule instances for rule grant read right is UN ·(UN −1)·ON, since two
user and one object node must be instantiated. Since node labels for nodes of
the same type must be unique in the left-hand side of the rule (as well as in the
right-hand side), we have UN · (UN −1) possibilities to instantiate the two user
nodes with diﬀerent labels. For the rule copy object, we get UN · ON · (ON −1)
possible rule instances. Table 1 shows the number of possible instance rules.
The last column of table 1 depicts the number of partial nonempty injective
mappings from the right-hand side R of rule p into the graph A, so that at least
some newly created elements in R are mapped to A. By the information of this
last column, the value of RF can be calculated (cf. corollary 1).
Table 1.
number of instances card(Rf(p))
new user(Ux)
UN
1
new object(Ux,Ox)
UN · ON
3
grant read right(Ux,Ox) UN · (UN −1) · ON
1
copy object(Ux,Ox)
UN · ON · (ON −1)
3

240
Manuel Koch et al.
In this example, the upper bound is: UN + UN · ON + UN · (UN −1) ·
ON + UN · ON · (ON −1) + 8. After having calculated the upper bound, only
a ﬁnite number of derivation sequences with length less or equal than the upper
bound are checked to decide the safety of the model. In particular, in this DAC
example the derivation sequence which applies ﬁrst the rule instance copy ob-
ject(Thomas,newProject.pdf) to the initial state Q0 and then the rule instance
grant read right(Thomas,Richard,newProject.pdf) to the resulting graph con-
structs a graph which contains A. Therefore, the algorithm in ﬁgure 3 returns
true, meaning that Q0 leaks A.
5
Example: Decidability in a Graph-Based Model for
Role-Based Access Control
The example in this section considers a simple graph model for decentralized
RBAC (i.e., with more than one administrator for roles). In this model, where
the safety property is decidable, a user is assigned to at most one role at a time
and the assignment of a user to a role is static in the sense that it can only be
changed by deleting the assignment and inserting a new one. The deletion of user-
role assignment implies the loss of authorization for roles that were granted by
the deleted assignment edge. This simpliﬁed RBAC graph model was originally
presented in [KMPP00], and is reported in ﬁgure 6.
Users are created by the rule add user and are deleted by the rule remove
user. A user can be assigned to a role if (s)he is not yet a member of a role.
Membership is indicated by the color of the u node. A white u node indicates
that the user is not yet in a role; a black one indicates that (s)he is a member
of a role. The rule add to role turns a white user node to black when an admin-
istrator sets the assignment edge from the user to the role node. Since the roles
authorization for each user is known, namely all junior roles reachable from the
unique assignment edge, the deletion of a user-role assignment is simpler than in
the general decentralized RBAC model. In particular, the deletion of the unique
assignment edge ensures that the user does not have authorization for any role.
In this case, all sessions are deactivated since all active roles for a session are
authorized by the removed assignment edge, and the user node is changed to
white.
A session is graphically presented by a node of type s and has always a con-
nection to one user. The rules for the creation and deletion of sessions are new
session and remove session. A session can be deleted at any time regardless of
the presence of active roles of the session. The session is deleted by deleting the
session node. This implies that all session-to-role assignments are deleted as well.
A user may activate any role r for which she/he is authorized. A user is
authorized for r if there is a path starting from an assignment edge and ending
in r. The corresponding graph rule is activate role. Rule deactivate role speciﬁes
the deactivation of a role from a session by deleting the edge between the session
and the role node.

Decidability of Safety in Graph-Based Models for Access Control
241
u
s
u
u
r
ar
r
r
u
s
r
ar
r
ar
s
r
r
s
r
s
s
r
r
ar
u
u
u
u
u
u
u
u
Ax
Ux
Rx
Rx
role
Ux
Sx
Sx
Rx
Ux
Ax
Rx1
Ux
Rx2
Sx
Rx2
Sx
Rx1
Ux
Ux
Ux
Ax
Sxn
Sx1,..
Rx
Ux
Ux
Ux
Ux
Sx
Rx
Ax
Ux
Rx
activate role
*
*
deactivate
add to
add user
remove user
new session
remove session
remove from
role
role
Fig. 6. Graph rules for the RBAC model
5.1
Safety of the RBAC Model
Suppose that we want to know if the initial state Q0 in ﬁgure 7 can leak A, that
is we would like to know whether Elena may play the role President. Table 2
shows the number of possible rule instances for the expanding rules of the RBAC
example and the number of mappings from the right-hand side of the rules into
the graph A. The number of rule instances depends on the ﬁnite label sets for
user, session, role and administrator role variables. The label set for user includes
the label Elena, the set of role labels is {President, ChiefManager, Manager},
the set of administrator role labels is {Bart, Anna}. We denote by UN the
number of elements in the user label set, by SN the number of elements in the
session label set, by RN = 3 the number of elements in the role label set and by
AN = 2 the number of elments in the administrator role set.
U
U
Elena
Q
Manager
Manager
Chief
President
Bart
0
Elena
President
A
R
Anna
R
R
Ar
Ar
R
Fig. 7. A safety example for the RBAC model

242
Manuel Koch et al.
Table 2.
number of instances
card(Rf(p))
add user(Ux)
UN
1
new session(Ux,Sx)
UN · SN
0
activate role(Ux,Rx1,Rx2,Sx) UN · SN · RN · (RN −1)
0
add to role
UN · RN · AN
1
The last column of table 2 depicts the number of nonempty partial injective
mappings from the right-hand side R of the rule into graph A so that at least
some newly created elements of R are mapped to A. Here, only the rules add user
and add to role construct parts of A. After having calculated the upper bound, all
derivation sequences with the given rule instances are checked up to the length
given by the upper bound. In this example, the decidability algorithm returns
true since the application of the rule instance add to role(Elena, President, Anna)
constructs graph A.
Note that if we remove Anna from the administrator set AN (e.g., because
we have conﬁdence in Anna, but Bart must be checked explicitly), this RBAC
example turns to be safe. Indeed, in the modiﬁed example, it will result that A
cannot be constructed from Q0. We will not list here all derivation sequences up
to the upper bound, this can be automated with the help of a graph transforma-
tion tool [EEKR99]. However, we would like to informally convince the reader,
that none of these derivation sequences constructs the graph A. The reason is
that the rule add to role(Elena,President,Anna) cannot be instantiated, since the
administrator label Anna is not in the label set AN for administrators. Indeed,
only Anna can assign Elena to the president role, and Elena will never become
president. Therefore, the decidability algorithm returns false.
6
Conclusion
The safety problem (can a state be reached in which a subject is granted a per-
mission not previously possessed) in an arbitrary access control model based on
graph transformations is in general undecidable.
This is not surprising since graph transformation rules can easily model the
rules of a type 0 grammar. By imposing reasonable restrictions on the form of
the rules, we have shown that the problem becomes decidable. The restrictions
imposed still allow for an expressive framework as illustrated by the two examples
of a graph-based DAC model and a graph-based single assignment decentralized
RBAC. Moreover, it is possible in our framework to specify the Take-Grant
model so that the rewrite rules take, grant, create, remove in [Sny77] satisfy the
restrictions of our Theorem 1. Our notion of safety is more general than the one
in [Sny77], since we test the presence of a general subgraph instead of a simple
edge.

Decidability of Safety in Graph-Based Models for Access Control
243
The safety problem “there exists a subject with a permission not previ-
ously possessed” can be handled, with a ﬁnite number of subjects, by deﬁning
a graph A for each speciﬁc subject. Alternatively, the problem can be handled
by allowing the nodes in the graph A to be labelled with variables and checking
if there exist an instantiation that is a subgraph of a state Q generated by the
graph rules (similar to resolution in logic programming, shown in [CRPP91]).
The eﬀective complexity of the decidability algorithm is probably much lower
than the computed worst case since labels (or unique names) on nodes reduce
considerably the number of possible matchings.
References
[AS00]
G. Ahn and R. Sandhu. Role-based Authorization Constraint Speciﬁca-
tion. ACM Trans. of Info. and System Security, 3(4), 2000.
230
[CRPP91]
A. Corradini, F. Rossi, and F. Parisi-Presicce. Logic programming as
hypergraph rewriting. In Proc. of CAAP91, volume 493 of LNCS, pages
275–295. Springer, 1991.
243
[EEKR99]
H. Ehrig, G. Engels, H.-J. Kreowski, and G. Rozenberg, editors. Hand-
book of Graph Grammars and Computing by Graph Transformations.
Vol. II: Applications, Languages, and Tools. World Scientiﬁc, 1999. 236,
242
[HRU76]
M. A. Harrison, M. L. Ruzzo, and J. D. Ullman. Protection in operating
systems. Communications of the ACM, 19(8):461–471, 1976.
229
[JT01]
Trent Jaeger and Jonathan E. Tidswell. Practical safety in ﬂexible access
control models. ACM Trans. of Info. and System Security, 4(2), 2001.
230
[KMPP00]
M. Koch, L. V. Mancini, and F. Parisi-Presicce.
A Formal Model for
Role-Based Access Control using Graph Transformation. In F.Cuppens,
Y.Deswarte, D.Gollmann, and M.Waidner, editors, Proc. of the 6th Eu-
ropean Symposium on Research in Computer Security (ESORICS 2000),
number 1895 in Lect. Notes in Comp. Sci., pages 122–139. Springer, 2000.
230, 233, 240
[KMPP01a]
M. Koch, L. V. Mancini, and F. Parisi-Presicce.
On the Speciﬁcation
and Evolution of Access Control Policies. In S. Osborne, editor, Proc.
6th ACM Symp. on Access Control Models and Technologies, pages 121–
130. ACM, May 2001.
230, 233
[KMPP01b]
M. Koch, L. V. Mancini, and F. Parisi-Presicce. Foundations for a graph-
based approach to the Speciﬁcation of Access Control Policies.
In
F.Honsell and M.Miculan, editors, Proc. of Foundations of Software Sci-
ence and Computation Structures (FoSSaCS 2001), Lect. Notes in Comp.
Sci. Springer, March 2001.
230
[LS78]
R. J. Lipton and L. Snyder. On synchronization and security. In Demillo
et al., editor, Foundations of Secure Computation. Academic Press, 1978.
229
[NO99]
M. Nyanchama and S. L. Osborne. The Role Graph Model and Conﬂict
of Interest. ACM Trans. of Info. and System Security, 1(2):3–33, 1999.
230
[Roz97]
G. Rozenberg, editor.
Handbook of Graph Grammars and Computing
by Graph Transformations. Vol. I: Foundations. World Scientiﬁc, 1997.
230, 232

244
Manuel Koch et al.
[Sny77]
L. Snyder. On the Synthesis and Analysis of Protection Systems. In
Proc. of 6th Symposium on Operating System Principles, volume 11 of
Operating System Review, pages 141–150. ACM, 1977.
230, 242
[SS92]
Ravi S. Sandhu and Gurpreet S. Suri. Non-Monotonic Transformation
of Access Rights. In Proc. IEEE Symposium on Research and Privacy,
pages 148–161, 1992.
229, 230

Inter-Packet Delay Based Correlation
for Tracing Encrypted Connections
through Stepping Stones
Xinyuan Wang1, Douglas S. Reeves1,2, and S. Felix Wu3 ⋆
1 Department of Computer Science
2 Department of Electrical and Computer Engineering
North Carolina State University
{xwang5,reeves}@eos.ncsu.edu
3 Department of Computer Science, University of California at Davis
wu@cs.ucdavis.edu
Abstract. Network based intrusions have become a serious threat to the
users of the Internet. Intruders who wish to attack computers attached
to the Internet frequently conceal their identity by staging their attacks
through intermediate “stepping stones”. This makes tracing the source
of the attack substantially more diﬃcult, particularly if the attack traﬃc
is encrypted. In this paper, we address the problem of tracing encrypted
connections through stepping stones. The incoming and outgoing connec-
tions through a stepping stone must be correlated to accomplish this. We
propose a novel correlation scheme based on inter-packet timing charac-
teristics of both encrypted and unencrypted connections. We show that
(after some ﬁltering) inter-packet delays (IPDs) of both encrypted and
unencrypted, interactive connections are preserved across many router
hops and stepping stones. The eﬀectiveness of this method for corre-
lation purposes also requires that timing characteristics be distinctive
enough to identify connections. We have found that normal interactive
connections such as telnet, SSH and rlogin are almost always distinctive
enough to provide correct correlation across stepping stones. The num-
ber of packets needed to correctly correlate two connections is also an
important metric, and is shown to be quite modest for this method.
1
Introduction
Network-based intrusions have become a serious threat to users of the Internet.
Today, perpetrators can attack networked information systems from virtually
anywhere in the world.
One major problem in apprehending and stopping network-based intrusion
is that the intruders can easily hide their identity and point of origin through
⋆This work was supported by AFOSR contract F49620-99-1-0264 and by DARPA
contract F30602-99-1-0540. The views and conclusions contained herein are those of
the authors.
D. Gollmann et al. (Eds.): ESORICS 2002, LNCS 2502, pp. 244–263, 2002.
c
⃝Springer-Verlag Berlin Heidelberg 2002

Inter-Packet Delay Based Correlation
245
readily available means. One of the techniques most commonly used by intruders
is to hide their origin by connecting across multiple stepping stones [4, 9, 13]
before attacking the ﬁnal targets. For example, an attacker logged into host
A may telnet into host B, and from there launch an attack on host C. An
analysis of the traﬃc at C will only reveal it is being attacked from B, but will
not identify the actual source of the attack. A careful inspection of the contents
of the traﬃc coming into and going out of B may reveal that A is the source of
the attack. However, if the traﬃc arriving at B is encrypted (using SSH [6, 11]
or IPSEC [3]) before being transmitted to C, it will not be possible to use the
traﬃc contents for correlation purposes. Network-based intruders thus have an
easy way to launch attacks without revealing their identity. Without a means of
eﬀectively and quickly tracing the source of an attack back to its source, it will
not be possible to stop further attacks or punish those who are responsible.
In this paper, we address the problem of correlating the incoming and outgo-
ing connections of a stepping stone. The goal is to identify which connections are
part of an attack path, so that the attack can be traced back to its source. We as-
sume that attack traﬃc may be encrypted at any stepping stone in an attempt to
interfere with correlation. We propose a novel scheme based on the inter-packet
timing characteristics of both encrypted and unencrypted connections. While, as
with most intrusion tracing and detection systems, out correlation scheme could
be evaded by highly sophisticated intruders, it is our goal to make it diﬃcult to
do so and thus deter network-based intrusions.
The remainder of the paper is organized as follows. In section 2, we give
a summary of related works. In section 3, we formulate the correlation problem
of our focus and give our correlation problem solution model. In section 4, we
discuss IPD (Inter-Packet Delay) based correlation in detail. In section 5, we
evaluate correlation eﬀectiveness of our proposed correlation metrics through
experiments. In section 6, we conclude with summary of our ﬁndings.
2
Related Work
Most of the existing work on correlating connections across stepping stones as-
sumes the traﬃc is unencrypted. In general, attack tracing approaches can be cat-
egorized as either being host-based or network-based. In a host-based approach,
the stepping stone itself participates in the tracing, while in the network-based
approaches, the stepping stones are not used for tracing purposes. Based on how
the traﬃc is traced, tracing approaches can further be classiﬁed as either active
or passive. Passive approaches monitor and compare all traﬃc, allowing any traf-
ﬁc to be traced at any time. On the other hand, active approaches dynamically
control when, where, what and how the traﬃc is to be correlated, through cus-
tomized packet processing. They only trace the traﬃc of interest when needed.
Table 1 provides a classiﬁcation of existing tracing approaches, as well as our
proposed tracing mechanism.
The Distributed Intrusion Detection System (DIDS) [8] developed at UC
Davis is a host-based tracing mechanism that attempts to keep track of all the

246
Xinyuan Wang et al.
Table 1. Classiﬁcation of Correlation and Tracing Approaches
Passive
Active
Host-Based
DIDS [8]
CIS [2]
Network-Based
Thumb printing [9]
IDIP [7]
ON/OFF-Based [13]
SWT [11]
Deviation-Based [12]
IPD-Based
(proposed method)
users in the network and report all activities to network-wide intrusion detection
systems. The Caller Identiﬁcation System (CIS) [2] is another host-based trac-
ing mechanism. It eliminates centralized control by utilizing a truly distributed
model. Each host along the connection chain keeps a record about its view of
the connection chain so far.
A fundamental problem with the host-based tracing approach is its trust
model. Host-based tracing places its trust upon the monitored hosts themselves.
In speciﬁc, it depends on the correlation of connections at every host in the con-
nection chain. If one host is compromised and is providing misleading correlation
information, the whole tracing system is fooled. Because host-based tracing re-
quires the participation and trust of every host involved in the network-based
intrusion, it is very diﬃcult to be applied in the context of the public Internet.
Network-based tracing is the other category of tracing approaches. It does
not require the participation of monitored hosts, nor does it place its trust on
the monitored hosts. Rather, it is based on the assumption that one or more
properties of a connection chain is maintained throughout the chain. In particu-
lar, the thumbprint [9] is a pioneering correlation technique that utilizes a small
quantity of information to summarize connections. Ideally it can uniquely distin-
guish a connection from unrelated connections and correlate successfully those
connections in the same connection chain.
Sleepy Watermark Tracing (SWT) [11] applies principles of steganogra-
phy and active networking in tracing and correlating unencrypted connections
through stepping stones. By injecting watermarks in the traﬃc echoed back to
the attacker, SWT is able to trace and correlate even a single keystroke by the
intruder. By actively generating tracing traﬃc, it can trace and correlate even
when an intrusion connection is idle.
IDIP (the Intrusion Identiﬁcation and Isolation Protocol) [7] is a part of
Boeing’s Dynamic Cooperating Boundary Controllers Program that uses an ac-
tive approach to trace the incoming path and source of the intrusion. In this
method, boundary controllers collaboratively locate and block the intruder by
exchanging intrusion detection information, namely, attack descriptions. The
ON/OFF-based scheme [13] by Zhang and Paxson is the ﬁrst correlation in-
tended to correlate traﬃc across stepping stones even if the traﬃc is encrypted

Inter-Packet Delay Based Correlation
247
by the stepping stone. The method is based on correlation of the ends of OFF
periods (or equivalently the beginnings of ON periods) of interactive traﬃc,
rather than the connection contents. While it is robust against payload padding,
ON/OFF-based correlation requires that the packets of connections have pre-
cise, synchronized timestamps in order to be able to correlate them. This makes
correlations of measurements taken at diﬀerent points in the network diﬃcult or
impractical.
The deviation-based approach [12] by Yoda and Etoh is another network-
based correlation scheme. It deﬁnes the minimum average delay gap between
the packet streams of two TCP connections as the deviation. The deviation-
based approach considers both the packet timing characteristics and their TCP
sequence numbers. It does not require clock synchronization and is able to cor-
relate connections observed at diﬀerent points of network. However, it can only
correlate TCP connections that have one-to-one correspondences in their TCP
sequence numbers, and thus is not able to correlate connections where padding
is added to the payload, e.g., when certain types of encryption are used.
Both ON/OFF-based and deviation-based approaches deﬁne their correlation
metrics over the entire duration of the connections to be correlated. This makes
the correlation applicable to post-attack traces only.
The IPD-based approach we discuss in the remainder of the paper deﬁnes its
correlation metric over the sliding window of packets of the connections to be
correlated. This enables it to correlate both live traﬃc (at real-time) and col-
lected traces (post-attack). It supports distributed correlation of traﬃc measure
at diﬀerent points of network and is robust against payload padding.
3
Problem Formulation
A connection ci (also called a ﬂow) is a single connection from computer host Hi
(the source) to host Hi+1 (the destination). A user may log into a sequence
of hosts H1, H2, . . . Hn+1 through a connection chain c1, c2, . . . cn, where
connection ci is a remote login from host Hi to host Hi+1.1 The tracing problem
is, given connection cn, to determine the other connections c1, c2, . . . cn−1 in
the chain, and only those connections. From these connections the identities of
all the hosts in the chain, including H1, may be directly determined.2
3.1
Correlation Problem Solution Model
Let ˆC represent the set of all connections being examined. We can deﬁne an
ideal correlation function CF : ˆC × ˆC →{0, 1} such that CF(ci, cj) = 1 iﬀci
1 The same host may appear in a connection chain more than once, in which case the
chain contains a loop. Due to space limitations we do not consider this case here.
2 If IP spooﬁng is used, of course, the packets of a connection will incorrectly identify
the source of the connection. We consider this problem to be orthogonal to our
problem and do not address it here. It is, in any event, unlikely that IP spooﬁng will
be used for interactive traﬃc, where the response to the interactive traﬃc must be
correctly echoed back to the source.

248
Xinyuan Wang et al.
and cj are in the same connection chain, otherwise CF(ci, cj) = 0. To solve the
tracing problem we must ﬁnd such a function CF.
In practice, connection correlation is based on the characteristics of the con-
nections, which may include packet contents, header information (such as packet
size) and packet arrival and/or departure times. The connection characteristics
can be modeled by a metric function of the connection
M : ˆC × P →Z
(1)
where ˆC is the set of connections to be correlated, P is some domain of param-
eters and Z is the correlation metric domain. Based on the connection metric,
a correlation value function (CVF) can be deﬁned as
CVF : Z × Z →[0, 1]
(2)
where the result of CVF is a real number between 0 and 1. To approximate
CF through CVF, we introduce a threshold 0 ≤δ ≤1 such that ci and cj are
considered correlated iﬀ
CVF(M(ci, p), M(cj, p)) ≥δ
(3)
Therefore the tracing problem is now replaced by the following: ﬁnd or con-
struct M, p, CVF and δ such that
∀ci, cj ∈ˆC, CVF(M(ci, p), M(cj, p)) ≥δ
iﬀci and cj are in the same connection chain
(4)
In ﬁnding M, p, CVF and δ, the key is to identify those unique character-
istics of connections that are invariant across routers and stepping-stones. If
those identiﬁed invariant characteristics of connections are distinctive enough to
exclude other unrelated connections, reliable correlation of connections can be
constructed from these metrics.
4
IPD Based Correlation of Encrypted Connections
In principle, correlation of connections is based on inherent characteristics of
connections. To correlate potentially encrypted connections, the key is to iden-
tify a correlation metric from the connection characteristics that is: 1) invariant
across routers and stepping stones; 2) not aﬀected by encryption and decryp-
tion; 3) unique to each connection chain. Potential candidates for the correlation
metric of a ﬂow of packets include header information, packet size, inter-packet
timing etc. In particular, inter-packet timing should not be aﬀected by encryp-
tion and decryption. We now present an original correlation method based on
inter-packet delays or IPDs.

Inter-Packet Delay Based Correlation
249
4.1
General IPD Based Correlation Model
The overall IPD correlation of two connections is a two-step process. First, the
two connections to be correlated are processed to generate a number of corre-
lation points between the two connections. Second, these generated correlation
points are evaluated to obtain the correlation value of the two connections.
The rationale behind this two-step process is to support the true real-time
correlation, which is the capability to correlate “live” traﬃc when they come
and go. This means that the approach must be able to correlate connections
before their ends are reached. Therefore, the correlation metric for true real-time
correlation cannot be deﬁned over the entire duration of a connection; we choose
instead to compute it over a window of packets in the connection. A correlation
point generated from IPDs within the window reﬂects some localized similarity
between the two ﬂows; the correlation value obtained from all the correlation
points will indicate the overall similarity of the two ﬂows.
Basic IPD Correlation Concepts and Deﬁnitions Given a bi-directional
connection, we can split it into two unidirectional ﬂows. We deﬁne our correlation
metric over the unidirectional ﬂow of connections.
Given a unidirectional ﬂow of n > 1 packets, we use ti to represent the
timestamp of the ith packet observed at some point of the network. We assume
all the ti’s of a ﬂow are measured at the same observation point with the same
clock. We deﬁne the ith inter-packet delay (IPD) as
di = ti+1 −ti
(5)
Therefore, for any ﬂow consisting of n > 1 packets, we can measure the inter-
packet delay (IPD) vector ⟨d1, . . . , dn−1⟩. Ideally, the IPD vector would uniquely
identify each ﬂow and we could construct our correlation metric from the IPD
vectors. To support real-time correlation based on the IPD vector, we deﬁne the
IPD correlation window Wj,s on ⟨d1, . . . , dn⟩as
Wj,s(⟨d1, . . . , dn⟩) = ⟨dj, . . . , dj+s−1⟩
(6)
where 1 ≤j ≤n represents the starting point of the window, and 1 ≤s ≤n−j+1
is the size of the window.
Given any two ﬂows X and Y , whose IPD vectors are ⟨x1, . . . , xm⟩and
⟨y1, . . . , yn⟩respectively, we deﬁne a Correlation Point Function CPF over IPD
correlation windows of X : Wj,s(X) and of Y : Wj+k,s(Y ) as
CPF(X, Y, j, k, s) = φ(Wj,s(X), Wj+k,s(Y ))
(7)
where φ is a function of two vectors: Rs ×Rs →[0, 1], 1 ≤j ≤min(m−s+1, n−
k −s + 1) is the start of the IPD correlation window, −j + 1 ≤k ≤n −j −s + 1
is the oﬀset between the two IPD correlation windows, and 1 ≤s ≤min(m, n)
is the size of the two IPD correlation windows. CPF(X, Y, j, k, s) quantitatively

250
Xinyuan Wang et al.
x1, . . . , xj, . . . , xj+s−1, . . . , xm
y1, . . . , yj+k, . . . , yj+k+s−1, . . . , yn
Flow X:
Flow Y:
[!t]
Fig. 1. CPF in PD Correlation Windows Wj,s(X) and Wj+k,s(Y )
expresses the correlation between Wj,s(⟨x1, . . . , xm⟩) and Wj+k,s(⟨y1, . . . , yn⟩)
as shown in Figure 1.
Because the value of CPF(X, Y, j, k, s) changes as j and k changes, we can
think of CPF(X, Y, j, k, s) as a function of j and k. Given any particular value
of j, CPF(X, Y, j, k, s) may have a diﬀerent value for each diﬀerent value of k.
We are interested in the maximum value of CPF(X, Y, j, k, s) for any particular
value of j. We deﬁne (j, j + k) as a correlation point if
max
−j+1≤k≤n−j−s+1 CPF(X, Y, j, k, s) ≥δcp
(8)
where δcp is the correlation point threshold with value between 0 and 1. The δcp
here is for detecting correlation point and is diﬀerent from δ in inequality (3).
We further deﬁne k for this correlation point (j, j+k) as the correlation-oﬀset
of CPF(X, Y, j, k, s) and the correlation point.
Given ﬂow X, Y , correlation window size s and threshold δcp, by applying for-
mula (8), we can obtain a series of correlation points: (x1, y1), (x2, y2), . . . (xn, yn)
where n ≥0.
Assuming one packet of ﬂow X corresponds to one packet of ﬂow Y 3, if
ﬂow X and Y are really part of the same connection chain, the IPDs of ﬂow X
should have a one-to-one correspondence with the IPDs of ﬂow Y . In this case, all
the correlation points should have same correlation-oﬀset. This can be formally
represented with CPF as
∃k′∀j

CPF(X, Y, j, k′, s) =
max
−j+1≤k≤n−j−s+1 CPF(X, Y, j, k, s)

(9)
That is there exists an oﬀset k′ such that CPF(X, Y, j, k′, s) is closest to 1 for
all possible j. In this case, all the correlation points (x, y) will be covered by
a linear function y = x + k′.
After obtaining n > 0 correlation points: (j1, j1+k1), (j2, j2+k2), . . . , (jn, jn+
kn), we represent those n correlation points with two n-dimensional vectors Cx =
⟨j1, . . . , jn⟩and Cy = ⟨j1 + k1, . . . , jn + kn⟩. The Correlation Value Function
formula (3) is now transformed to
CVF(Cx, Cy) ≥δ
(10)
In summary, the metric function M in formula (1) is now mapped to CPF, the
parameter domain P in formula (1) is mapped to s and δcp, and Z in formula (1)
is mapped to n-dimensional vectors Cx = ⟨j1, . . . jn⟩and Cy = ⟨j1 +k1, . . . , jn +
kn⟩.
3 We have found this is true for most packets in correlated ﬂows.

Inter-Packet Delay Based Correlation
251
Correlation Method Assessment Criteria A critical issue in this method is
the choice of the function φ for computing the correlation point function CPF.
Criteria by which the method may be judged include:
• Uniqueness of perfect correlation: for a ﬂow X, no ﬂow Y not in the same
connection chain as X should have CPF(X, Y, j, k, s) = 1.
• Correlation Point (CP) true positives (hits): this is the number of packets
that should be correlated, and are found to be correlated according to equa-
tion 8. The true positive rate is the number of hits divided by the number
of packets that should be correlated.
• Correlation Point (CP) false positives (misses): this is the number of pack-
ets which are not in fact correlated, but which are found to be correlated
according to equation 8.
Ideally, we would expect a perfect correlation method 1) has unique perfect
correlation; 2) has a 100% CP true positive rate; and 3) has 0 misses or false
positives.
4.2
Correlation Point Functions
We now propose four correlation point functions, each of which enjoys certain
advantages or applicability, as discussed below.
Min/Max Sum Ratio (MMS) One simple metric to quantitatively express
the ˆosimilarity¨o between two vectors is the ratio between the summation of the
minimum elements and the summation of the maximum elements.
CPF(X, Y, j, k, s)MMS =
j+s+1
i=j
min(xi, yi+k)
j+s+1
i=j
max(xi, yi+k)
(11)
CPF(X, Y, j, k, s)MMS has the range [0, 1] and takes the value 1 only when xi =
yi+k for i = j, . . . , j +k −1. Therefore, CPF(X, Y, j, k, s)MMS is likely to exhibit
unique perfect correlation.
Statistical Correlation (STAT) Based on the concept of the coeﬃcient of
correlation from statistics [1], we can deﬁne
CPF(X, Y, j, k, s)Stat =
ρ(X, Y, j, k, s) , ρ(X, Y, j, k, s) ≥0
0
, ρ(X, Y, j, k, s) < 0
(12)
where
ρ(X, Y, j, k, s) =
j+s+1
i=j
(xi −E(X)) × (yi+k −E(Y ))
j+s+1
i=j
(xi −E(X))2

×
j+s+1
i=j
(yi+k −E(Y ))2


252
Xinyuan Wang et al.
The
range
of
CPF(X, Y, j, k, s)Stat
is
also
[0, 1].
Unlike
CPF(X, Y, j, k, s)MMS,
for
a
given
Wj,s(X),
there
may
be
more
than
one
value
of
Wj+k,s(Y )
for
which
CPF(X, Y, j, k, s)Stat
has
the
value
1.
For
example,
for
a
particular Wj,s(X),
any
linear
transform
of Wj,s(X) : Wj+k,s(Y ) = a × Wj,s(X) + b will result in CPF(X, Y, j, k, s)Stat
being equal to 1 (a > 0) or -1 (a < 0). CPF(X, Y, j, k, s)Stat is therefore less
likely to exhibit unique perfect correlation, and is more likely to result in false
positives.
Normalized Dot Product 1 (NDP1) In digital signal processing, linear
correlation (or matched ﬁltering) of two discrete signals will reach a maximum at
the point where the signals have the most similarity. It is well known that linear
correlation is optimal in detecting the similarity between a discrete signal and
the corresponding signal distorted by additive, white Gaussian noise. However
the range of linear correlation is not necessarily between 0 and 1.
If the discrete signals are replaced by two vectors, the corresponding oper-
ation to linear correlation of signals is the inner-product or dot-product of two
vectors in n-dimensional space. From linear algebra, the inner-product of two n-
dimensional vectors is equal to the cosine of the angle between the two vectors,
multiplied by the lengths of the two vectors. That is:
W(X) • W(Y ) = cos(θ) × |W(X)| × |W(Y )|
(13)
where θ is the angle between vector W(X) and W(Y ), and |W(X)| and |W(Y )|
are the lengths of vector W(X) and W(Y ) respectively4.
cos(θ) in (13) can be used as a correlation point function. The range of
cos(θ) is [−1, 1] and it provides a measure of the similarity of two vectors. For
any vector Wj,s(X), cos(θ) will be 1 for any vector Wj+k,s(Y ) = a×Wj,s(X)+b.
To make the correlation point function more likely to exhibit unique perfect
correlation, we can deﬁne it as follows:
CPF(X, Y, j, k, s)NDP 1 = min(|W(X)|, |W(Y )|
max(|W(X)|, |W(Y )| × cos(θ)
= min(|W(X)|, |W(Y )|)
max(|W(X)|, |W(Y )| ×
j+s−1
i=j
xi × yi+k
|W(X)| × |W(Y )|
=
j+s−1
i=j
xi × yi+k
[max(|W(X)|, |W(Y )|)]2
=
j+s−1
i=j
xi × yi+k
max
j+s−1
i=j
x2
i , j+s−1
i=j
y2
i+k
	
(14)
As xi and yi are non negative, the range of CPF(X, Y, j, k, s)NDP 1 is [0, 1]. It can
be shown that CPF(X, Y, j, k, s)NDP 1 will be 1 only when Wj,s(X) = Wj+k,s(Y ).
Therefore, CPF(X, Y, j, k, s)NDP 1 is likely to exhibit unique perfect correlation.
4 We have dropped the subscripts of W(X) and W(Y) for clarity purposes in this
section.

Inter-Packet Delay Based Correlation
253
Normalized Dot Product 2 (NDP2) Another way to normalize the dot-
product of two vectors is
CPF(X, Y, j, k, s)NDP 2 =
j+s−1
i=j
xi × yi+k
j+s−1
i=j
[max(xi, yi+k)]2
(15)
Because xi and yi are non negative, the range of CPF(X, Y, j, k, s)NDP 2 is
[0, 1]. It is obvious that CPF(X, Y, j, k, s)NDP 2 equals 1 only when Wj,s(X) =
Wj+k,s(Y ).
Among these four proposed correlation point functions, Mini/MaxSum Ratio
(MMS) is likely to be the most sensitive to local details of the IPD vectors to be
correlated. This is because it does not average any diﬀerences, and it accumulates
all the IPD diﬀerences. As a result, MMS may potentially have a lower true
positive rate due to its emphasis on local details. While the STAT CPF is much
more robust to noise, we expect it to have substantially more false positives. The
normalized dot product functions (NDP1 and NDP2) are likely to be in between
MMS and STAT in terms of sensitivity to local detail and robustness to noise.
4.3
Correlation Value Function
Given ﬂows X, Y , correlation window size s and threshold δ, by applying formula
(8), we can potentially obtain a set of correlation points: (j1, j1 + k1), (j2, j2 +
k2), . . . , (jn, jn + kn). We represent this sequence of correlation points through
two n-dimensional vectors Cx = ⟨j1, . . . jn⟩and Cy = ⟨j1 + k1, . . . , jn + kn⟩.
We deﬁne the overall Correlation Value Function CVF of ﬂows X and Y
from this sequence of correlation points, as follows:
CVF(Cx, Cy) =



0
n = 0
ρ(Cx, Cy) n = 1
1
n = 1
(16)
where
ρ(Cx, Cy) =
n
i=1(ji −E(Cx)) × (ji + ki −E(Cy))

[n
i=1(ji −E(Cx))2] × [n
i=1(ji + ki −E(Cy))2]
CVF(Cx, Cy) quantitatively expresses the overall correlation between ﬂows X
and Y , and its value range is [−1, 1]. When there exists more than one correlation
point and all the correlation points have same correlation oﬀset (i.e., k1 = k2 =
. . . kn), CVF(Cx, Cy) = 1. When ﬂow X and Y has only one correlation point,
CVF(Cx, Cy) = 1. When ﬂow X and Y have no correlation point, CVF(Cx, Cy)
is deﬁned to be 0.
4.4
Limitations and Countermeasures
The eﬀectiveness of IPD-based correlation relies on the uniqueness of IPDs of
connections. It may be ineﬀective at diﬀerentiating uncorrelated connections

254
Xinyuan Wang et al.
that exhibit very similar IPD patterns, such as ﬁle transfers accomplished via
FTP. Interactive traﬃc, as we show later in the experiments section, usually has
IPD patterns that are distinctive enough for our purposes.
Intruders could thwart IPD-based correlation by deliberately changing the
inter-packet delays of a connection in a chain. Such delays may be designed to
reduce the true positive rate, or increase the number of false positives. There
are relatively simple means of accomplishing such traﬃc shaping, although they
may require kernel-level manipulations. The amount of delay that can be added
by the intruder is limited by the maximum delay that is tolerable for interactive
traﬃc. Another countermeasure against IPD-based correlation is to set some
connection into line mode while keeping other connection in character mode.
This could potentially merge several packets into one bigger packet. However,
the server side shell could always dynamically turn oﬀthe line mode [10]. Other
countermeasures include:
• Injecting “padding” packets that can be removed by the application
• Segmenting one ﬂow into multiple ﬂows and reassembling them, again at the
application level
It is an area of future work to address such countermeasures.
5
Experiments
The goal of the experiments is to answer the following questions about IPD
based correlation:
1. Are inter-packet delays preserved through routers and stepping stone, and
to what extent?
2. Are inter-packet delays preserved across encryption/decryption and various
network applications (such as telnet/rlogin/SSH)?
3. How eﬀective is the IPD-based correlation metric in detecting connections
which are part of the same chain?
4. How well does the IPD-based correlation metric diﬀerentiate a connection
from connections that are not part of the same chain?
5.1
Correlation Point Experiment
To answer the ﬁrst two questions, we have conducted the following experiment.
We ﬁrst telnet’ed from a Windows 2000 PC behind a cable modem connected to
an ISP in North Carolina to a Sun workstation via VPN. From the workstation,
we used SSH to login to another workstation at N. C. State University. We then
telnet’ed to a PC running Linux at UC Davis, and from there we SSH’ed back
to a PC running FreeBSD at NC State. As shown in Figure 2, the connection
chain has a total of 3 stepping-stones and 59 hops and covers a distance on the
order of 10,000 km. The connection chain consists of links of diﬀerent speeds –
including residential Internet access, typical campus LAN and public Internet

Inter-Packet Delay Based Correlation
255
Windows 2000
nc.rr.com
24.25.x.x
telnet (5 hops)
VPN
ssh (18 hops)
Sun OS
64.102.x.x
Sun OS
ncsu
152.14.x.x
telnet (18 hops)
Linux
ucdavis
169.237.x.x
ssh (18 hops)
FreeBSD
ncsu
152.14.x.x
flow X
flow Y
Fig. 2. Correlation Experiment on telnet and ssh
backbone. We have captured the packet traces at the Windows 2000 node and
the FreeBSD node; both traces have a timestamp resolution of 1 microsecond.
We label the telnet return path5 ﬂow from the Sun workstation to the Windows
2000 PC ﬂow X, and the SSH backwards ﬂow from the FreeBSD PC to the Linux
PC ﬂow Y . Therefore, ﬂow X consists of telnet packets and ﬂow Y consists of
SSH packets.
We have calculated the IPD vectors after ﬁltering out the following sources
of errors:
• Duplicated packets
• Retransmitted packets
• ACK only packets
We then calculated correlation points (j, k) by applying (8) using each of the
four correlation point functions, with diﬀerent correlation window sizes s and
correlation point thresholds δcp.
Figure 3 shows the correlation points between ﬂow X and Y obtained by the
MMS CPF with diﬀerent correlation window sizes s and thresholds δcp. In these
plots, a point at position (j, k) indicates inequality (8) was true for that value of
j, k, s and δcp. True positives are points located along the major diagonal. False
positives are points located oﬀthe major diagonal.
With correlation window size of 5 and δcp threshold of 0.70, there are a large
number of falsely detected correlation points (false positives) in addition to the
true correlation points (true positives). The overall CVF value (equation (4.13))
for this case is 0.1707. With larger correlation window size, or a higher threshold
δcp, MMS results in fewer false positives and has a higher CVF value, as would
be expected. At correlation window size 15, and a threshold δcp of 0.70, MMS
detects most of the true correlation points between ﬂow X and Y , ﬁnds no false
positives, and has an overall CVF value of 0.9999. When the threshold δcp is
increased to 0.95 with the same correlation window size of 15, MMS detects sub-
stantially fewer correlation points between ﬂow X and Y , with no false positives,
and has an overall CVF value of 1.0. This suggests that with correlation window
5 The “return path” is the echoed traﬃc generated on the destination host and sent
to the origination host.

256
Xinyuan Wang et al.
MMS Correlation Points with Window
Size 5, Threshold 0.95
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
Packet # of Flow X
Packet # of Flow Y
max CPF(X, Y, 5)>0.95
CVF()=0.5830
MMS Correlation Points with Window
Size 10, Threshold 0.95
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
Packet # of Flow X
Packet # of Flow Y
max CPF(X, Y, 10)>0.95
CVF()=0.9999
MMS Correlation Points with Window
Size 15, Threshold 0.95
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
Packet # of Flow X
Packet # of Flow Y
max CPF(X, Y, 15)>0.95
CVF()=1.0
MMS Correlation Points with Window
Size 20, Threshold 0.95
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
Packet # of Flow X
Packet # of Flow Y
max CPF(X, Y, 20)>0.95
CVF()=1.0
MMS Correlation Points with Window
Size 25, Threshold 0.95
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
Packet # of Flow X
Packet # of Flow Y
max CPF(X, Y, 25)>0.95
CVF()=1.0
MMS Correlation Points with Window
Size 5, Threshold 0.80
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
Packet # of Flow X
Packet # of Flow Y
max CPF(X, Y, 5)>0.80
CVF()=0.2771
MMS Correlation Points with Window
Size 10, Threshold 0.80
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
Packet # of Flow X
Packet # of Flow Y
max CPF(X, Y, 10)>0.80
CVF()=07879
MMS Correlation Points with Window
Size 15, Threshold 0.80
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
Packet # of Flow X
Packet # of Flow Y
max(X, Y, 15)>0.80
CVF()=0.9999
MMS Correlation Points with Window
Size 20, Threshold 0.80
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
Packet # of Flow X
Packet # of Flow Y
max CPF(X, Y, 20)>0.80
CVF()=1.0
MMS Correlation Points with Window
Size 25, Threshold 0.80
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
Packet # of Flow X
Packet # of Flow Y
max CPF(X, Y, 25)>0.80
CVF()=1.0
MMS Correlation Points with Window
Size 5, Threshold 0.70
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
Packet # of Flow X
Packet # of Flow Y
max CPF(X, Y, 5)>0.70
CVF()=0.1707
MMS Correlation Points with Window
Size 10, Threshold 0.70
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
Packet # of Flow X
Packet # of Flow Y
max CPF(X, Y, 10)>0.70
CVF()=0.4637
MMS Correlation Points with Window
Size 15, Threshold 0.70
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
Packet # of Flow X
Packet # of Flow Y
max CPF(X, Y, 15)>0.70
CVF()=0.9999
MMS Correlation Points with Window
Size 20, Threshold 0.70
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
Packet # of Flow X
Packet # of Flow Y
max CPF(X, Y, 20)>0.70
CVF()=1.0
MMS Correlation Points with Window
Size 25, Threshold 0.70
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
Packet # of Flow X
Packet # of Flow Y
max CPF(X, Y, 25)>0.70
CVF()=1.0
Fig. 3. Correlation Point between Two Correlated Flows Detected by MMS with
Diﬀerent Correlation Window Sizes and Thresholds

Inter-Packet Delay Based Correlation
257
Table 2. Traces of Flows Used in Correlation Experiments
Flow Set
Date
Flow Type Flow # Packet #
FS1
03/02/02
SSH
16
12372
FS2
03/02/02
Telnet
15
5111
FS3
02/20/01 Telnet/SSH
144
34344
FS4
02/26/01 Telnet/SSH
135
38196
FS5
05/xx/02
SSH
400
364158
size 15, the threshold δcp of 0.95 is probably too high for MMS. The correlation
points missed by correlation windows size 15 and threshold δcp of 0.95 are ac-
tually due to correlation-oﬀset shifts. The correlation-oﬀset between our sample
ﬂows X and Y has shifted 3 times between the start and ﬁnish. This indicates
that a telnet packet may trigger more than one SSH packet, or vice versa. For-
tunately, such correlation-oﬀset shifts are infrequent between correlated ﬂows.
Generally, a larger correlation window size is very eﬀective in ﬁltering out false
positives, and a higher threshold δcp tends to ﬁlter out both true positive and
false positive correlation points. An excessively large correlation window size
with a high threshold δcp tends to have a low true positive rate, due to both
correlation-oﬀset shifts and IPD variations introduced by the network.
Figure 4 compares the detected correlation points between ﬂow X and Y
by diﬀerent CPFs: MMS, STAT, NDP1 and NDP2, with identical correlation
window sizes of 10 and threshold δcp of 0.80. As expected, the statistical CPF
results in substantially more false positives than the other three CPFs. While
NDP2 has slightly fewer false positives than NDP1, they both have somewhat
more false positives than MMS. Generally, MMS is very sensitive to localized
details of IPDs and is able to accurately correlate the ﬂows using a smaller
correlation window (i.e. 5). NDP1 and NDP2 are less eﬀective with a small
correlation window, but they are able to correlate eﬀectively with a moderate
window size (15 or 20). The statistical CPF appears to fail to consider enough
localized details to correlate accurately
5.2
Aggregated Flow Correlation Experiment
To evaluate more generally the performance of the diﬀerent correlation point
functions, we have used ﬁve sets of ﬂows (Table 2). FS1 and FS2 were collected
at two ends of connection chains similar to the scenario shown in Figure 2. FS1
and FS2 contain 16 SSH ﬂows and 15 Telnet ﬂows, respectively; for each ﬂow
in FS2, there is one ﬂow in FS1 which was in the same connection chain. FS3
and FS4 are derived from 5 million packet headers and 12 million packet headers
of the Auckland-IV traces of NLANR [5]. FS5 is derived from over 49 million
packet headers of the Bell Lab-I traces of NLANR.
We have conducted four sets of aggregated correlation experiments. For all
of these experiments, two ﬂows were regarded as being correlated if the CVF

258
Xinyuan Wang et al.
MMS Correlation Points with Window
Size 10, Threshold 0.80
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
Packet # of Flow X
Packet # of Flow Y
max CPF(X, Y, 10)>0.80
CVF()=07879
Statistical Correlation Points with
Window Size 10, Threshold 0.80
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
Packet # of Flow X
Packet # of Flow Y
max CPF(X, Y, 10)>0.80
CVF()=0.0979
NDP1 Correlation Points with Window
Size 10, Threshold 0.80
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
Packet # of Flow X
Packet # of Flow Y
max CPF(X, Y, 10)>0.80
CVF()=0.4713
NDP2 Correlation Points with Window
Size 10, Threshold 0.80
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
Packet # of Flow X
Packet # of Flow Y
max CPF(X, Y, 10)>0.80
CVF()=0.3784
Fig. 4. Correlation Points Detected by MMS, Stat, NDP1 and NDP2 with Same
Window Size and Threshold
of their correlation points (equation (16)) was greater than δ = 0.6. The ﬁrst
set tests how well diﬀerent correlation metrics (CPF) detect correlation between
sets FS1 and FS2. Figure 5 shows both the number of true positives (out of
a total of 15) and the number of false positives (out of 15×15=225) of ﬂow cor-
relation detection with diﬀerent correlation window sizes and correlation point
thresholds δcp.
With a δcp threshold of 0.70, MMS reaches its TP peak of 93% at a correlation
window size of 20, and NDP2 reaches its TP peak of 80% with a correlation
window size of 20 or 25. However NDP2 has a signiﬁcantly higher number of
false positives at the window size corresponding to its peak true positive rate
than does than MMS. Both STAT and NDP1 have very low (<7%) TP rates with
all correlation window size. This indicates that STAT and NDP1 are ineﬀective
with a low δcp threshold.
For all δcp threshold values, MMS attains its peak TP rate with 0 false pos-
itives. NDP1 and NDP2 show a similar success rate, with a somewhat higher
failure (false positive) rate. STAT is generally not successful at correlating the
ﬂows in the same chain. The best results are obtained for the highest δcp thresh-
old setting. MMS is able to achieve 100% TP rate with 0 false positives with
correlation window size 15, δcp threshold 0.90 and window size 10, δcp threshold
0.95. NDP2 is also able to have 100% TP rate with 0 FP at correlation win-

Inter-Packet Delay Based Correlation
259
dow size 15, δcp threshold 0.95. NDP1’s overall TP peak is 93% with 7% FP at
correlation window size 20, δcp threshold 0.90.
The second set of experiments shows the correlation detection eﬀectiveness
by diﬀerent correlation metrics. We use combined ﬂow set of FS3 and FS4 (279
ﬂows) and ﬂow set FS5 (400 ﬂows) to correlate themselves respectively. Figure 6
shows the true positive rate of diﬀerent correlation metric with diﬀerent correla-
tion window size and δcp threshold. Again the STAT correlation point function
consistently performs poorly. MMS and NDP1 almost have identical correlation
detection rates across all the correlation window size and δcp threshold combi-
nations in both data sets, where NDP1 has little lower detection rate. For ﬂow
set FS5, the detection rates of both MMS and NDP2 reach 92% and higher with
correlation window size 25 or bigger. At correlation window size 35, MMS’s and
NDP2’s detection rate achieve over 97%. For the combined ﬂow set FS3 and
FS4, at a correlation window size of 15, for δcp threshold 0.95, MMS, NDP1
and NDP2 all have the highest correlation detection rate of 76.7%. This lower
detection rate is due to the nature of the ﬂows in FS3 and FS4. We have found
a number of SSH ﬂows in FS3 and FS4 show very similar periodicity, with con-
stant very short IPDs. We suspect they are bulk data transfers within the SSH
connections. This shows a potential limitation of the use of IPD-based tracing.
The third experiment is intended to evaluate the ability of the diﬀerent cor-
relation point functions to successfully discriminate ﬂows not part of the same
chain. Figure 7 shows the number of false positives (out of 16×279=4464) when
correlating FS1 and the combined ﬂow set of FS3 and FS4. Because no ﬂow from
FS1 correlates with any ﬂow from FS3 and FS4, any detected correlation by the
correlation metric is a false positive. MMS consistently has 0 false positives;
and NDP1 and NDP2 false positives decrease as the correlation window size
increases. The STAT correlation point function reports an increasing number of
FPs with larger correlation sizes.
The fourth experiment similarly investigated the false positive rate, this time
between sets FS3 and FS4. Figure 8 shows the results. The number of false pos-
itives (out of 144×133 = 19152) for MMS, NDP1 and NDP2 decreases dramat-
ically when the correlation window size increases; that of MMS decreases faster
than NDP1 and NDP2. Again, the statistical correlation metric has a consis-
tently higher FP rate with increasing correlation window size. For the MMS
method, a window size of 20 0r 25 packets is suﬃcient to reduce the false posi-
tive rate to a fraction of a percent.
In summary, we have found that MMS is very eﬀective in both detecting inter-
active, correlated ﬂows and diﬀerentiating uncorrelated ﬂows with even relatively
small correlation window sizes (10, 15). NDP1 and NDP2 are not as sensitive
as MMS with small correlation windows; however, they both perform well with
larger correlation windows. We have conﬁrmed that the statistical correlation
metric is not eﬀective in detecting correlation and diﬀerentiating uncorrelated
ﬂows.

260
Xinyuan Wang et al.
True Positive of 15 Correlated Flow Pairs
with Threshold 0.70
0
2
4
6
8
10
12
14
16
0
5
10
15
20
25
Correlation Window Size
True Positive
MMS
Stat
NDP1
NDP2
True Positive of 15 Correlated Flow Pairs
with Threshold 0.90
0
2
4
6
8
10
12
14
16
0
5
10
15
20
25
Correlation Window Size
True Positive
MMS
Stat
NDP1
NDP2
True Positive of 15 Correlated Flow Pairs
with Threshold 0.95
0
2
4
6
8
10
12
14
16
0
5
10
15
20
25
Correlation Window Size
True Positive
MMS
Stat
NDP1
NDP2
False Positive of 15 Correlated Flow Pairs
with Threshold 0.70
0
2
4
6
8
10
12
14
16
0
5
10
15
20
25
Correlation Window Size
False Positive
MMS
Stat
NDP1
NDP2
True Positive of 15 Correlated Flow Pairs
with Threshold 0.80
0
2
4
6
8
10
12
14
16
0
5
10
15
20
25
Correlation Window Size
True Positive
MMS
Stat
NDP1
NDP2
False Positive of 15 Correlated Flow Pairs
with Threshold 0.80
0
2
4
6
8
10
12
14
16
0
5
10
15
20
25
Correlation Window Size
False Positive
MMS
Stat
NDP1
NDP2
False Positive of 15 Correlated Flow Pairs
with Threshold 0.90
0
2
4
6
8
10
12
14
16
0
5
10
15
20
25
Correlation Window Size
False Positive
MMS
Stat
NDP1
NDP2
False Positive of 15 Correlated Flow Pairs
with Threshold 0.95
0
2
4
6
8
10
12
14
16
0
5
10
15
20
25
Correlation Window Size
False Positive
MMS
Stat
NDP1
NDP2
Fig. 5. True Positive and False Positive of Correlation between 16 and 15 Cor-
related Flows
5.3
Correlation Performance
We have measured the number of calculations of correlation points per second
achieved by our unoptimized code. Table 3 shows the average number of millions
of correlation point calculation per second of various correlation point functions
under diﬀerent load. Despite dynamic overheads of disk operation, the overall
throughput remains largely constant at various loads.

Inter-Packet Delay Based Correlation
261
True Positive Rate of 279 Correlated Flow
Pairs with Threshold 0.90
0
10
20
30
40
50
60
70
80
90
100
10
15
20
25
30
35
Correlation Window Size
True Positive %
MMS
Stat
NDP1
NDP2
True Positive Rate of 279 Correlated Flow
Pairs with Threshold 0.95
0
10
20
30
40
50
60
70
80
90
100
10
15
20
25
30
35
Correlation Window Size
True Positive %
MMS
Stat
NDP1
NDP2
True Positive Rate of 400 Correlated Flow
Pairs with Threshold 0.90
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0
10
15
20
25
30
35
Correlation Window Size
True Positive %
MMS
Stat
NDP1
NDP2
True Positive Rate of 400 Correlated Flow
Pairs with Threshold 0.95
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0
10
15
20
25
30
35
Correlation Window Size
True Positive %
MMS
Stat
NDP1
NDP2
Fig. 6. True Positive Rate of Correlation between 279 and 279, 400 and 400
Correlated Flows
False Positive of 16 and 279 Uncorrelated
Flows with Threshold 0.90
0
2
4
6
8
10
12
10
15
20
25
Correlation Window Size
False Positive
MMS
Stat
NDP1
NDP2
False Positive of 16 and 279 Uncorrelated
Flows with Threshold 0.95
0
2
4
6
8
10
12
10
15
20
25
Correlation Window Size
False Positive
MMS
Stat
NDP1
NDP2
Fig. 7. False Positive of Correlation between 16 and 279 Uncorrelated Flows
Table 3. Throughput (Millions Per Second) of Correlation Point Calculation
with Correlation Window Size 15
40k 379k 937k 2309k 5704k 54210k
MMS 2.00 1.65 1.74
1.75
1.43
1.35
STAT 0.90 0.76 0.69
1.14
1.22
1.83
NDP1 3.99 3.16 2.23
3.25
2.29
3.24
NDP2 1.33 1.31 1.16
1.37
1.17
1.13

262
Xinyuan Wang et al.
False Positive of 144 and 135 Uncorrelated
Flows with Threshold 0.90
0
100
200
300
400
500
600
700
800
10
15
20
25
30
35
40
45
Correlation Window Size
False Positive
MMS
Stat
NDP1
NDP2
False Positive of 144 and 135 Uncorrelated
Flows with Threshold 0.95
0
100
200
300
400
500
600
700
800
10
15
20
25
30
35
40
45
Correlation Window Size
False Positive
MMS
Stat
NDP1
NDP2
Fig. 8. False Positive of Correlation between 144 and 135 Uncorrelated Flows
6
Conclusions
Tracing intrusion connections through stepping-stones at real-time is a challeng-
ing problem, and encryption of some of the connections within the connection
chain makes tracing even harder. We have addressed the tracing problem of
encrypted connections based on the inter-packet delays of the connections. We
proposed and investigated four correlation point functions. Our correlation met-
ric does not require clock synchronization, and allows correlation of measure-
ments taken at widely scattered points. Our method also requires only small
packet sequences (on the order of a few dozen packets) for correlation. We have
found that after some ﬁltering, IPDs (Inter-Packet Delay) of both encrypted
and unencrypted, interactive connections are largely preserved across many hops
stepping-stones. We have demonstrated that both encrypted and unencrypted,
interactive connections can be eﬀectively correlated and diﬀerentiated based on
IPD characteristics.
Our experiments also indicate that correlation detection is signiﬁcantly de-
pendent on the uniqueness of ﬂows. We have found that normal interactive con-
nections such as telnet, SSH and rlogin are almost always unique enough to be
diﬀerentiated from connections not in the same chain. While bulk data transfer
with SSH connection introduces an additional challenge in correlation detection,
its impact on correlation diﬀerentiation may simply be oﬀset by larger correla-
tion windows and higher correlation point thresholds.
A natural area of future work is to extend the correlation to non-interactive
traﬃc. How to address countermeasures with “bogus packets” and packet split-
ting and merging remains an open problem.
References
[1] M. H. DeGroot. Probability and Statistics. Addison-Wesley, 1989.
251
[2] H. Jung, et al. Caller Identiﬁcation System in the Internet Environment. In Pro-
ceedings of 4th USENIX Security Symposium, 1993.
246
[3] S. Kent, R. Atkinson. Security Architecture for the Internet Protocol. IETF RFC
2401, September 1998.
245

Inter-Packet Delay Based Correlation
263
[4] S. C. Lee and C. Shields. Tracing the Source of Network Attack: A Technical, Legal
and Social Problem. In Proceedings of the 2001 IEEE Workshop on Information
Assurance and Security, June 2000.
245
[5] NLANR Trace Archive. http://pma.nlanr.net/Traces/long/.
257
[6] OpenSSH. http://www.openssh.com.
245
[7] D. Schnackenberg. Dynamic Cooperating Boundary Controllers.
http://www.darpa.mil/ito/Summaries97/E295 0.html,
Boeing
Defense
and
Space Group, March 1998.
246
[8] S. Snapp, et all. DIDS (Distributed Intrusion Detection System) – Motivation,
Architecture and Early Prototype. In Proceedings of 14th National Computer Se-
curity Conference, 1991.
245, 246
[9] S. Staniford-Chen, L. T. Heberlein. Holding Intruders Accountable on the Inter-
net. In Proceedings of IEEE Symposium on Security and Privacy, 1995.
245,
246
[10] W. R. Stevens. TCP/IP Illustrated, Volume 1: The Protocol. Addison-Wesley,
1994.
254
[11] X. Y. Wang, D. S. Reeves, S. F. Wu and J. Yuill. Sleepy Watermark Tracing: An
Active Network-Based Intrusion Response Framework. In Proceedings of 16th In-
ternational Conference on Information Security (IFIP/Sec’01), June, 2001. 245,
246
[12] K. Yoda and H. Etoh. Finding a Connection Chain for Tracing Intruders. In
F. Guppens, Y. Deswarte, D. Gollmann and M. Waidner, editors, 6th Euro-
pean Symposium on Research in Computer Security - ESORICS 2000 LNCS-1895,
Toulouse, France, October 2000.
246, 247
[13] Y. Zhang and V. Paxson. Detecting Stepping Stones. In Proceedings of 9th
USENIX Security Symposium, 2000.
245, 246

Learning Fingerprints for a
Database Intrusion Detection System
Sin Yeung Lee, Wai Lup Low, and Pei Yuen Wong
Computer Security Laboratory, DSO National Laboratories, Singapore
{lsinyeun,lwailup,wpeiyuen}@dso.org.sg
Abstract. There is a growing security concern on the increasing num-
ber of databases that are accessible through the Internet. Such databases
may contain sensitive information like credit card numbers and per-
sonal medical histories. Many e-service providers are reported to be
leaking customers’ information through their websites. The hackers ex-
ploited poorly coded programs that interface with backend databases us-
ing SQL injection techniques. We developed an architectural framework,
DIDAFIT (Detecting Intrusions in DAtabases through FIngerprinting
Transactions) [1], that can eﬃciently detect illegitimate database ac-
cesses. The system works by matching SQL statements against a known
set of legitimate database transaction ﬁngerprints. In this paper, we ex-
plore the various issues that arise in the collation, representation and
summarization of this potentially huge set of legitimate transaction ﬁn-
gerprints. We describe an algorithm that summarizes the raw transac-
tional SQL queries into compact regular expressions. This representation
can be used to match against incoming database transactions eﬃciently.
A set of heuristics is used during the summarization process to ensure
that the level of false negatives remains low. This algorithm also takes
into consideration incomplete logs and heuristically identiﬁes “high risk”
transactions.
1
Introduction
Nowadays, most e-commerce sites oﬀering some kind of online services have
a database at its backend. Applications accessing these databases support a large
variety of activities. Data found in these databases ranges from personal infor-
mation and banking transactions to medical records and commercial secrets.
Any breach of security to these databases can result in tarnished reputation for
the organization, loss of customers’ conﬁdence and might even result in lawsuits.
Unfortunately, recent reports indicate that there is a large increase in the num-
ber of security breaches, which resulted in theft of transaction information and
ﬁnancial fraud [2, 3, 4]. Clearly, it is important that the data in these databases
be protected from unauthorized access and modiﬁcation.
One mechanism to safeguard the information in these databases is to use
intrusion detection systems (IDS). These systems aim to detect intrusions as
early as possible, so that any damage caused by the intrusions is minimized.
D. Gollmann et al. (Eds.): ESORICS 2002, LNCS 2502, pp. 264–279, 2002.
c
⃝Springer-Verlag Berlin Heidelberg 2002

Learning Fingerprints for a Database Intrusion Detection System
265
They function as sentinels and ensure that any compromise on the integrity and
conﬁdentiality of the data is detected and reported as soon as possible. Intrusion
detection research is not new and has been on going for many years. However,
previous eﬀorts were focused on network-based intrusion detection and host-based
intrusion detection. Network-based intrusion detection typically works by mon-
itoring network traﬃc and host-based intrusion detection works by monitoring
log ﬁles in the hosts. Both network- and host-based intrusion detection systems
look for attack signatures, which are speciﬁc patterns that usually indicate ma-
licious or suspicious intent, to identify intrusions. There are numerous commer-
cial network- and host-based intrusion detection systems in the market today,
and the market leaders include RealSecure [5], NFR [6], Dragon [7], Cisco [8]
and Symantec [9]. There are also IDS that are free and highly acclaimed (e.g.
Snort [10]).
However, these intrusion detection systems do not work at the applica-
tion layer, which can potentially oﬀer more accurate and precise detection for
the targeted application. The distinctive characteristics of database manage-
ment systems (DBMSes), together with their widespread use and the invaluable
data they hold, make it vital to detect any intrusion attempts made at the
databases. Therefore, intrusion detection models and techniques specially de-
signed for databases are becoming imperative needs. There are recent reports [4]
in which SQL injection techniques, which refer to the use of carefully crafted
and malicious SQL statements, were used by the intruders to pilfer sensitive
information. SQL injection will be further discussed in a later section. This re-
inforces the point that intrusion detection systems should not only be employed
at the network and hosts, but also at the database systems where the critical
information assets lie.
DIDAFIT (Detecting Intrusions in DAtabases through FIngerprinting Trans-
actions) is a system developed to perform database intrusion detection at the ap-
plication level. It works by ﬁngerprinting access patterns of legitimate database
transactions, and using them to identify database intrusions. The framework
for DIDAFIT has been described in [1]. This paper describes how the ﬁnger-
prints for database transactions can be represented and presents an algorithm
to learn and summarize SQL statements into ﬁngerprints. The main contribution
of this work is a technique to eﬃciently summarize SQL statements queries into
compact and eﬀective regular expression ﬁngerprints. The technique can han-
dle incomplete training data sets and uses heuristics to maintain false negatives
(missed attacks) at low levels.
The rest of the paper is as follows. Section 2 discusses two basic concepts
necessary for the understanding of subsequent sections. A brief introduction to
the DIDAFIT framework is given in Section 3. The issues that arise during
ﬁngerprint learning and derivation are discussed in Section 4. In Section 5, our
algorithm for ﬁngerprint learning is detailed together with a full example. We
survey the related works in Section 6, and conclude in Section 7.

266
Sin Yeung Lee et al.
2
Preliminaries
In this section, we introduce SQL injection and SQL ﬁngerprints which are
necessary for understanding the rest of the paper.
2.1
SQL Injection
Application users do not usually communicate with the database server directly,
but through the application server. Although there is no direct interaction with
the database server, it is possible that unauthorized users can access the database
in ways unintended by the developer. This is made possible by carelessly designed
applications, database server holes, as well as application server exploits. One
technique of exploiting carelessly written database applications is SQL injection
[11, 12, 13]. SQL injection refers to crafting SQL statements using “string build-
ing” techniques to trick the application server into executing the intruder’s (often
malicious) code. Possible results of actions by the injected code include infor-
mation disclosure, unauthorised data modiﬁcation, deletion of database or even
escalation of the intruder’s database privileges to that of the administrator’s.
As an illustration, consider the following Perl script:
...
my $passwd = $cgi->param(’passwd’);
my $name= $cgi->param(’name’);
$sql = "select * from cust where name=’$name’".
" and passwd=’$passwd’";
$sth = $dbh->prepare($sql);
$sth->execute;
if (!($sth->fetch)) {
report_illegal_user();
} ...
The script shown is a typical procedure for login checking. It is supposed to
verify if the user has supplied the user name and his/her password correctly.
However, this script is vulnerable to SQL injection attacks. A malicious user can
enter the following text into the password ﬁeld of the submitted form:
x’ OR ’x’=’x
Assuming the user name is “alice” and the password is as above, the prepared
SQL statement becomes
select * from customer
where name=’alice’ and passwd=’x’
OR ’x’=’x’
The where clause of this statement will always be true since the intruder
has carefully injected a “ OR ’x’=’x’ ” clause into it. This makes the result
set of the query non-empty no matter what password is supplied. The malicious

Learning Fingerprints for a Database Intrusion Detection System
267
user can now log in as the user “alice” without knowing the password. The
main reason for this vulnerability is the carelessly written procedure. DIDAFIT
can detect such attacks on the database and other anomalous data accesses.
Conventional solutions to this vulnerability include the use of stored procedures
and checking user parameters before using them (e.g. dangerous characters such
as ’ should not be allowed in the input). While stored procedures tend to be
non-portable and require longer development time, methods to enforce good
programming practices are beyond the scope of this work.
2.2
SQL Fingerprints
For most applications with database services, the SQL statements submitted to
the database are typically generated by some server-side scripts/programs. These
statements are generated in a predictable manner and this regularity gives rise
to opportunities to characterize valid transactions with some sort of signatures.
For example, assume we have a delete order transaction that deletes from
the order table. Further assume that an order can only be deleted by specifying
its orderID. A typical valid SQL statement can be
delete from order where orderID=’12573’;
Now, suppose the database server receives the following SQL statement:
delete from order where custid=’eric’;
This SQL statement does not conform to the pattern of the SQL statements
for delete order transaction (i.e. it uses custid as the criteria for ﬁltering
instead of orderID). This statement could be the result of an intruder.
We have presented the idea behind ﬁngerprinting database transactions.
DIDAFIT uses a ﬁngerprinting technique that derives signatures (which we call
ﬁngerprints) to help identify illegitimate SQL statements. In DIDAFIT, each ﬁn-
gerprint characterizes one set of SQL statements. Our ﬁngerprints (as described
in [1]) uses regular expressions. A single ﬁngerprint can precisely represent var-
ious variants of SQL statements that can result from a transaction.
For example, we have a transaction that can query the order table given
a customer ID (custid) and a range of values for the order amount (amt). The
code below shows how this service may be coded in Perl.
$sqlstm = "SELECT orderID, amt from order where ";
$sqlstm .= "custid=’$custid’ and ";
$sqlstm .= "amt>$min_amt and amt<$max_amt";
We list below some possible SQL statements generated from this code:
SELECT orderID, amt from order where custid=’3822’ and amt>20 and amt<100
SELECT orderID, amt from order where custid=’7312’ and amt>10 and amt<200
All variants of queries generated by the code above can be represented using
this regular expression:

268
Sin Yeung Lee et al.
^SELECT orderID, amt from order where custid=’[^’]*’
and amt>[[:digit:]]+ and amt<[[:digit:]]+$
Notably,
1. The literal containing the value of custid has been replaced with the regular
expression ’[^’]*’, which represents a quoted string.
2. The numerical value of amt is represented by the regular expression
[[:digit:]]+, which represents an integer.
3. Finally, the whole expression begins with a ^ and ends with a $ to prevent
any additional clauses from being injected into the statement (such as using
the union clause to append another query).
3
DIDAFIT: An Overview
DIDAFIT is designed as a misuse detection system. It can be broadly classiﬁed
as a signature-based IDS with enhanced capabilities for learning and deducing
new signatures. The overall architecture for DIDAFIT is shown in Figure 1. The
meaning of the numbered ﬂows in Figure 1 are as follows:
1. The application user issues a service request to the application server. This
transaction may or may not be legitimate.
2. The application server formulates the necessary SQL statements and issues
them to the database server through the database user.
3. The database user logs into the database. The database session is traced
and the SQL statements received from the application are channeled to the
misuse detection module.
4. In the misuse detection module, the received SQL statements are matched
with the set of ﬁngerprints of legitimate database transactions.
5. Anomalies or intrusions are then channeled to the reaction modules for the
appropriate action to be taken. Actions that can be taken include alerting
the administrators, sounding the alarm on the console and paging the duty
personnel.
6. Output is returned to the application user (if applicable).
Fingerprint
Monitor and
Anomaly Detection
Module
Signature
Database
Database Server
3
4
Actions to
be taken
Database
User
6
2
5
Application User
6
Transaction
1
Application
Server
Fig. 1. Architecture for DIDAFIT

Learning Fingerprints for a Database Intrusion Detection System
269
DIDAFIT involves three components:
1. An utility to log all the SQL statements submitted to the database server.
We need to capture the submitted SQL statements for the database trans-
actions in order to compare them with those in the “legitimate” ﬁngerprint
database. Diﬀerent database systems oﬀer diﬀerent ways to capture the SQL
statements. For example, Oracle provides the sql trace [14] utility that can
be used to trace all database operations in a database session of an user. The
trace results are logged to a ﬁle, but we can channel the data to a monitor-
ing and misuse detection module. Note that the sql_trace utility is not
designed for this purpose. Rather, its intended use was to aid in the process
of optimizing database performance. However, we make use of its capability
to log SQL statements executed by the database engine. A concern with us-
ing the trace facility of database systems is its impact on the performance of
the databases. Our experiments in [1] show that the impact on performance
is extremely small.
2. A process to derive the ﬁngerprints for SQL statements of legitimate
database transactions.
We propose the use of regular expressions to represent the derived ﬁnger-
prints as described in Section 2.2.
3. A database of “legitimate” ﬁngerprints to be used for database intrusion
detection.
To make use of the ﬁngerprints, every submitted SQL statement is matched
with the set of legitimate ﬁngerprints. If the SQL statement cannot match
any of the ﬁngerprints, then an intrusion may have occurred.
As an illustration, consider the example of order table queries (Section 2.2).
Suppose our database server receives the following SQL statement.
select orderID, amt from order;
This statement does not match our legitimate ﬁngerprint shown previously.
This is caused by the missing custid input, which is mandatory by our
signature. Hence, this anomalous statement is detected.
There are many software packages and utilities that can process regular
expressions. The standard Unix tool egrep is one such software that can
handle the matching of our ﬁngerprints eﬃciently.
4
Issues in Automated Fingerprints Learning and
Derivation
One obvious method of generating the complete set of ﬁngerprints for all
database transactions is to do a code-walkthrough. However, this may not be
feasible for several reasons. The code may contain sensitive business logic and
not available for the walkthrough for some applications. The code base for many
applications are also large and changes frequently. Maintaining the consistency
and keeping the set of ﬁnerprints updated will require a lot of eﬀort. Hence, it
is important that the ﬁngerprints be automatically learnt and deduced as much
as possible.

270
Sin Yeung Lee et al.
In this section, we ﬁrst discuss the problems and challenges that arise when
we automate the learning and derivation of the ﬁngerprints for the database
transactions. We propose possible solutions after that.
4.1
Problems and Challenges
An obvious method to “learn” ﬁngerprints is to use each legitimate SQL state-
ment in the training log as a ﬁngerprint. However, it suﬀers from the following
problems:
1. The set of SQL statements collected is a very large set. If each unprocessed
SQL statement forms a ﬁngerprint, the size of the ﬁngerprint database be-
comes unmanageable. We need a way to automatically summarize the SQL
statements. Consider the following SQL statements:
delete from order where orderID=’13245’;
delete from order where orderID=’45718’;
Although the orderIDs are diﬀerent, they serve the same function, which
is to remove a particular order based on the orderID and hence, can be
summarized to a single ﬁngerprint:
delete from order where orderID=#$ORDERID;
where #$ORDERID is a meta-constant to denote a string literal in the domain
of orderID. However, we need to avoid over-summarization. For example,
delete from order where orderType=’UNDELIVERABLE’;
delete from order where orderType=’NORMAL’;
should not be summarized to a single ﬁngerprint. The ﬁrst SQL statement
may be a typical statement used during maintenance. However, it is pos-
sible that the second statement originated from a malicious attack (why
should anyone delete all “normal” orders?). Over-summarization may cause
DIDAFIT to miss such illegitimate statements. Thus, our learning algorithm
must be able to group similar SQL statements into ﬁngerprints, but yet does
not increase the probability of missing attacks.
2. The SQL transactions in the training trace log may contain illegitimate state-
ments from past intrusion activities. We do not assume that all the SQL
statements in the training log to be legitimate. Our learning algorithm must
be able to detect potentially invalid SQL statements, even within the training
set of SQL statements.
3. The trace log may not be complete. Some legitimate transactions may not be
executed during the period of monitoring. With incomplete input for train-
ing, DIDAFIT may treat unseen, but legitimate statements as illegitimate.
This may give rise to a large number of false positives. Thus, our learning
algorithm should be able to deduce a set of possibly legitimate ﬁngerprints
with an associated level of conﬁdence, that are missing from the training
data. The legitimacy of these deduced ﬁngerprints can be ascertained by the
DBA.

Learning Fingerprints for a Database Intrusion Detection System
271
L1
select orderID, Amt from order where custID=’2317’;
L2
select orderID, Amt from order where custID=’1920’;
L3
select orderID from order where custID=’3571’ and amt>100 and amt<300;
L4
select prod_desc from product where prodID=’X3175’;
L5
select orderID from order where custID=’’ or TRUE or custID=’’;
L6
select comment from prod_comment where prodID=’X3175’ and conf=’PUBLIC’;
L7
select orderID, Amt from order where custID=’8256’;
L8
select orderID from order where custID=’1354’ and odate=’1-Jan-1999’ and amt>500;
L9
select comment from prod_comment where prodID=’X0507’ and conf=’PRIVATE’;
L10
select prod_desc from product where prodID=’X1754’;
L11
select orderID from order where custID=’1028’ and odate=’1-Jan-1999’ and amt<1000;
L12
select orderID, Amt from order where custID=’1754’;
L13
select comment from prod_comment where prodID=’X1754’ and conf=’PUBLIC’;
L14
select comment from prod_comment where prodID=’X0507’ and conf=’PUBLIC’;
L15
select comment from prod_comment where prodID=’X3075’ and conf=’PUBLIC’;
L16
select prod_desc from product where prodID=’X0675’;
L17
select orderID, Amt from order where custID=’8317’;
Fig. 2. A snapshot of the SQL trace log for an E-mall
Before we present our solutions to these three challenges, we consider an
online E-mall application with the following business rules:
– Users can create and delete their own orders.
– Users can perform searches on their own orders and limit the search by
specifying the order ID (orderID), product ID (prodID), the range of the
order’s value (amt), and/or the date of order (odate).
– Users can see the description of the products (prod desc).
– Users can only see the public comments on the product (conf=’PUBLIC’),
but not internal comments, which are reserved only for the staﬀof the E-
mall.
A snapshot of the SQL trace log is shown in Figure 2. This example will be
used in the examples for the rest of this work.
4.2
Selective Summarization of Literals
Observe from the logs in Figure 2 that L1 diﬀers from L2 only by the value of the
custID parameter. They query the same attributes for a given customer ID. Our
learning algorithm can group these SQL statements under the same ﬁngerprint.
This can be done by ﬁrst replacing each literal by a meta-constant. In this case,
each string literal that represents the value of a custID attribute is replaced by
the token #$CUSTID. The summarized ﬁngerprint for L1 and L2 is
F1
select orderID, Amt from order where custID=#$CUSTID;
Likewise, the next three log records (L3,L4,L5) give the following three ﬁn-
gerprints:

272
Sin Yeung Lee et al.
F2
select orderID from order where custID=#$CUSTID and amt>#%AMT and amt<#%AMT;
F3
select prod_desc from product where prodID=#$PRODID;
F4
select orderID from order where custID=#$CUSTID or TRUE or custID=#$CUSTID;
However, as mentioned previously, replacing all literals to wild-card tokens
indiscriminately may cause some malicious SQL statements to go undetected.
Consider L6, L9, L13, L14 and L15 , the ﬁrst attribute in the WHERE-clause
(product ID prodID), can take any value, but the second attribute, (conﬁdential-
ity conf), takes only the value “PUBLIC”. This corresponds well to the “business
rule” that customers can see only the public comments but not otherwise. Thus,
we should only replace the literals of the prodID with the wild-card token and
let the literals of the conf attribute remain unchanged, i.e.
select comment from prod_comment
where prodID=#$PRODID and conf=’PUBLIC’;
The diﬀerence between the two treatments lies on the fact that prodID is not
from a small set of pre-speciﬁed values. This value carries no other implication,
except to identify a product. On the other hand, the valid values of conf is
restricted to a small list of pre-determined values. Values such as “PUBLIC”,
“PRIVATE” and “SECRET” are not only used to identify tuples in the database,
but also carry implications for the operations and sensitivity of the tuples. Hence,
it is not advisable to summarize these literals into one token.
In view of this, our algorithm will replace a literal with a token only when the
literal corresponds to a domain that carries no implicit meaning for operations
and data sensitivity. Such domains can be determined by consulting the DBA.
However, in the event that this information is unavailable, we can assume that if
the range of values in the domain is very large (or unbounded), then it is unlikely
that such value has an implicit meaning. To test if the domain is unbounded,
we test the growth rate of the number of unique values for the attribute as the
size of the sample increases. If the domain is unbounded, the growth rate will
be constant. That is to say:
Number of distinct literals found ∝s
where s is the number of the samples in the observation. On the other hand,
if the domain contains only a few ﬁxed constants, then the number of distinct
values will be similar to the standard diminishing growth curve,
Number of distinct literals = N0(1 −e−λs)
This behavior can be easily tested with many statistical methods such as the
non-parametric one-sample Komoglov-Smirnov’s D Statistics test (KS-test). For
example, consider the set of trace L6, L9, L13, L14, L15 from Figure 2. The
prodID attribute employs four literals: ’X3175’, ’X0507’, ’X1754’ and ’X3075’
in the ﬁrst conjunction in the WHERE-clause.
Using the one-sample KS-test, the D-statistic for testing the linear growth
can be computed as follows:
D =
√
5 max{|1
4 −1
5|, |2
4 −2
5|, |3
4 −3
5|, |3
4 −4
5|} = 0.15

Learning Fingerprints for a Database Intrusion Detection System
273
Table 1. Distribution of the prodID Literal
Number of statements
1
2
3
4
5
sampled
(L6) (L6,L9) (L6,L9,L13) (L6,L9,L13,L14) (L6,L9,L13,L14,L15)
Total no. of unique
1
2
3
3
4
literals observed
Cumulative distribution
1/4
2/4
3/4
3/4
4/4
Expected uniform distribution 1/5
2/5
3/5
4/5
5/5
At α=90% signiﬁcance level, this value should be at most 0.563. Thus, we accept
the hypothesis that the domain of prodID is unbounded.
On the other hand, the conf attribute has 2 unique literals in the 5-statement
sample: ’PUBLIC’ and ’PRIVATE’. The D-statistic is computed likewise as:
D =
√
5 max{|1
2 −1
5|, |2
2 −2
5|, |2
2 −3
5|, |2
2 −4
5|} = 0.6
It exceeds 0.563. Thus, we reject the hypothesis that the domain of conf is
unbounded. Hence, we should retain all the constants related with conf when
generating the ﬁngerprints.
4.3
Detection of High-Risk Transactions
It is obvious that L5 is an instance of SQL injection. It is caused by injecting
the string “’ or TRUE or custID=’” into the custID parameter. We consider
this ﬁngerprint to be rare. A ﬁngerprint is rare if its frequency is statistically
below a small threshold. The occurrence of a rare ﬁngerprint warns of a possible
malicious SQL statement.
In our algorithm, we tabulate the frequency of each ﬁngerprint learnt. If
a ﬁngerprint occurs frequently, then it is safe to include it in the ﬁngerprint
database. For instance, consider the four ﬁngerprints, F1, F2, F3 and F4 (in
Section 4.2).
Fingerprint F1 matches 5 statements in the trace of size 17 (Figure 2). A z-
test can conclude that the frequency is not close to 0. Similarly, ﬁngerprint F3
can be considered safe based on its frequency. On the other hand, ﬁngerprints F2
and F4 matches only one statement each in the trace. Statistically, at 95% con-
ﬁdence level, we cannot deny that their frequency is 0. In other words, they may
correspond to potentially malicious statements and require further investigation.
To further reﬁne our decision to ascertain the legitimacy of F2, we observe
that F2 diﬀers from F1 as follows:
1. F2 has a more restrictive WHERE-clause compared to F1. It speciﬁes more
conditions using the “AND” operator in the WHERE-clause than F1.
2. F2 selects fewer attributes than F1.
Thus, we can consider F2 to be safe because for any SQL statement that
matches F2, the set of returned tuples is a subset of the tuples returned by

274
Sin Yeung Lee et al.
some SQL statement matching F1 (which is a legitimate ﬁngerprint). Thus,
even if some SQL statements that match F2 is malicious, it cannot reveal more
information than some legitimate query that is allowed by F1.
On the other hand, F4 cannot be considered safe. F4’s WHERE-conditions
are joined using the “OR” operator. Thus, it is possible that a query that matches
F4 can reveal more tuples than what is allowed by the legitimate ﬁngerprints.
Thus, our learning algorithm decides that an rare ﬁngerprint F is legitimate
if there exists another legitimate ﬁngerprint F′, such that F diﬀers from F′ only
by
1. any extra conditions in the WHERE-clause of F that is missing from F′ is
joined with the “AND” operator; and
2. F selects an equal number of or fewer columns than F′.
4.4
Deduction of Missing Fingerprints.
As mentioned previously, some legitimate ﬁngerprints might not appear in the
trace log due to incomplete training data. This incompleteness can cause many
false alarms during the actual operation. To reduce this problem, we propose an
algorithm to deduce a set of possible missing ﬁngerprints. This set of missing
ﬁngerprints will then be presented to the DBA for conﬁrmation to be included
in the legitimate ﬁngerprint set.
Consider the transaction set L3, L8, L11 of the log in Figure 2. These logs
can be captured by the following three ﬁngerprints:
P1
select orderID from order where custID=#$CUSTID and amt>#%AMT and amt<#%AMT;
P2
select orderID from order where custID=#$CUSTID and odate=#$ODATE and amt>#%AMT;
P3
select orderID from order where custID=#$CUSTID and odate=#$ODATE and amt<#%AMT;
The three ﬁngerprints share the following properties:
1. Except for the conditions at the WHERE-clause, they are identical.
2. The WHERE-clause of each pattern uses three out of the following four
conjuncts:
“custID=#$CUSTID”, “odate=#$ODATE”, “amt>#%AMT” and “amt<#%AMT”.
It is possible that there is another ﬁngerprint, that uses all four conjuncts:
P4
select orderID from order where custID=#$CUSTID and odate=$#ODATE
and amt>$%AMT and amt<$%AMT;
P4 compacts all the nine conjuncts used in P1, P2 and P3 into only four
conjuncts. Indeed, this scenario can be generated by the following Perl script:
$sqlstm = "select orderID from order where custID=’$custID’";
if !($odate eq "") {
$sqlstm .= " and odate=’$odate’";
}
if ($min_amt > 0) {
$sqlstm .= " and amt>$min_amt";

Learning Fingerprints for a Database Intrusion Detection System
275
}
if ($max_amt > 0) {
$sqlstm .= " and amt<$max_amt";
}
In general, a ﬁngerprint F is a derived ﬁngerprint
from ﬁngerprints
F1, . . . , Fm with compactness η if
1. Each pair of ﬁngerprints Fi, Fj (1 ≤i, j ≤m), as well as F, Fi (1 ≤i ≤m),
diﬀers only by some missing conjuncts in their WHERE-clauses.
2. The condition in the WHERE-clause of F subsumes every condition in the
WHERE-clause of each Fj (1 ≤j ≤m).
3. Each conjunct in the WHERE-clause of F appears in at least the WHERE-
clause of at least one Fj (1 ≤j ≤m).
4. Furthermore, η (η ≥1) is the ratio of the total number of possibly identical
conjuncts in F1, . . . , Fm over the number of conjuncts in F.
In our derived ﬁngerprint P4, the number of conjuncts used in the WHERE-
clause is 4. The total number of conjuncts used in the three ﬁngerprints
(P1,P2,P3) is 9. Hence, P4’s compactness is 9
4. High compactness gives higher
conﬁdence that the ﬁngerprints F1, . . . , Fm are closely related. For example,
consider the following case,
P1’
select orderID from order where custID=#$CUSTID;
P2’
select orderID from order where odate=#$ODATE;
P3’
select orderID from order where itemid=#$ITEMID;
These three ﬁngerprints can derive the following:
P4’
select orderID from order where custID=#$CUSTID and odate=#$ODATE and itemid=#$ITEMID;
of compactness 1. The three ﬁngerprints (P1’,P2’,P3’) are likely generated
by diﬀerent functions. If this is the case, the derived ﬁngerprint (P4’) will never
appear in the trace log.
To search for derived ﬁngerprints with a high degree of compactness, we
perform the following steps,
1. Group all the legitimate ﬁngerprints into equivalent classes such that each
pair of ﬁngerprint Fi, Fj in the same class diﬀers only by some missing con-
juncts in their WHERE-clauses.
2. For each equivalent class, conjunct all the conditions in the WHERE-clauses
of all ﬁngerprints, removing duplicate conjuncts if necessary, into a condi-
tion C. Construct F0 such that it is identical as F1 except that the WHERE-
clause is replaced with C.
Note that the legitimacy of this derived ﬁngerprint should be conﬁrmed with
the DBA, before including it in the set of legitimate ﬁngerprints.

276
Sin Yeung Lee et al.
5
Algorithm for Fingerprint Learning and Derivation
In this section, we summarize the discussion in the previous sections by present-
ing the algorithm to automatically generate the set of ﬁngerprints. We illustrate
the algorithm with a complete example.
The algorithm is shown in Algorithm 1. After that is a detailed walkthrough
of the algorithm using the trace log shown in Figure 2.
Algorithm 1: Algorithm for Fingerprint Learning and Derivation
Given trace log LOG, and a set of attributes A that carries some implicit mean-
ing optionally speciﬁed by the DBA, we perform the following steps:
1. For each attribute in the WHERE-clauses of LOG that is not found in A,
scan LOG to compute the Kolmogorov-Smirnov’s D statistic to test for linear
growth of the number of unique literals.
2. If the number of unique literals of any attribute does not statistically support
a linear growth, put it into the set A′.
3. Generate a set of unique ﬁngerprints by replacing all the literals of attributes
in the WHERE-clause that are neither in A nor A′, with meta-constants.
(a) replace all the string literals of the expression attr=’literal’ in the
WHERE-clause with attr=#$attr.
(b) replace all the numeric literals of the expression attr=numeric_literal
in the WHERE-clause with attr=#%attr.
Let F1, . . . , Fm be the ﬁngerprints generated.
4. Count the number of occurrences for each ﬁngerprint. Label the ﬁngerprint
“safe” if the number of occurrences is not statistically 0.
5. For each unclassiﬁed ﬁngerprint F, label it “safe” if there exists another
“safe” ﬁngerprint F′, such that F diﬀers from F′ by only
(a) any extra condition in the WHERE-clause of F that is missing from F′
is joined with the “AND” operator; and
(b) F selects an equal number of or fewer columns than F′.
The remaining ﬁngerprints are labelled “unsafe”.
6. Compute the set of derived ﬁngerprints from the set of “safe” ﬁngerprints.
Mark these derived ﬁngerprints as “derived”.
7. Present all the “unsafe” and “derived” ﬁngerprints to the DBA. Each “un-
safe” or “derived” ﬁngerprint that is approved by the DBA will be marked
as “safe”. If there remains any “unsafe” ﬁngerprints, then an intrusion has
occurred in LOG.
8. For each “safe” ﬁngerprint, replace the meta-constant by the regular ex-
pressions. This legitimate ﬁngerprint database can now be used to detect
intrusions for the database application.
1. Assuming A is an empty set, we ﬁrst scan the log to compute the
Kolmogorov-Smirnov’s D statistic for discrete data against linear growth

Learning Fingerprints for a Database Intrusion Detection System
277
Table 2. Occurrence count of the ﬁngerprints
Fingerprint F1 F2 F3 F4 F5 F6 F7 F8
Frequency
5
1
3
1
4
1
1
1
of the number of unique values for each attribute in the WHERE-clause (i.e.
custid, amt, prodid, conf, odate). For example, (as calculated in the pre-
vious section) the prodid has a D statistic of 0.15, while the conf has a D
statistic of 0.6.
2. After step 2, we have A′ = {conf}.
3. The next step is to generate the initial ﬁngerprint set by replacing the literals
of each attribute not in A nor A′ with its corresponding meta-constant. We
obtain the following ﬁngerprints:
F1
select orderID, Amt from order where custid=#$CUSTID;
F2
select orderID from order where custid=#$CUSTID and amt>#%AMT and amt<#%AMT;
F3
select prod_desc from product where prodid=#$PRODID;
F4
select orderID from order where custid=#$CUSTID or TRUE or custid=#$CUSTID;
F5
select comment from prod_comment where prodid=#$PRODID and conf=’PUBLIC’;
F6
select comment from prod_comment where prodid=#$PRODID and conf=’PRIVATE’;
F7
select orderID from order where custid=#$CUSTID and odate=#$ODATE and amt>#%AMT;
F8
select orderID from order where custid=#$CUSTID and odate=#$ODATE and amt<#%AMT;
4. We count the number of occurrences for each ﬁngerprint next. The results
are tabulated in Table 2.
F1, F3 and F5 occurs frequently enough for us to infer that they are “safe”.
On the other hand, F2, F4, F6, F7 and F8, occurs only once each in the
trace log. These ﬁngerprints need to be looked into.
5. Upon inspection, F2, F7 and F8 can be classiﬁed as “safe” as they can be
subsumed by F1. F4 and F6, however, are marked “unsafe”. They should be
highlighted to the DBA.
6. The next step involves ﬁnding derived ﬁngerprints of high compactness. In
this example, we set the degree of compactness to be at least 2. The following
new ﬁngerprint marked “derived” is found,
F9
select orderID from order where custid=#$CUSTID and odate=#$ODATE
and amt>#%AMT and amt<#%AMT;
7. Assume the DBA approves the “derived” ﬁngerprint F9, but does not ap-
prove all the other “unsafe” ﬁngerprints. The legitimate ﬁngerprints are F1,
F2, F3, F5, F7, F8 and F9. The meta-constants are replaced by the appro-
priate regular expressions. The ﬁnal ﬁngerprints set of legitimate ﬁngerprints
for this database application consists of:
F1
select orderID, Amt from order where custid=’[^’]*’;
F2
select orderID from order where custid=’[^’]*’ and amt>[0-9]+ and amt<[0-9]+;
F3
select prod_desc from product where prodid=’[^’]*’;
F5
select comment from prod_comment where prodid=’[^’]*’ and conf=’PUBLIC’;
F7
select orderID from order where custid=’[^’]*’ and odate=’[^’]*’ and amt>[0-9]+;
F8
select orderID from order where custid=’[^’]*’ and odate=’[^’]*’ and amt<[0-9]+;
F9
select orderID from order where custid=’[^’]*’ and odate=’[^’]*’ and amt>[0-9]+
and amt<[0-9]+;

278
Sin Yeung Lee et al.
6
Related Work
As far as the authors know, this is the only work using SQL transaction ﬁn-
gerprints or signatures to detect database intrusions. Closest to our work is [15]
which proﬁles users and roles in a relational database system. It generates the
proﬁles from the “working scopes” of the users which are deﬁned as the sets
of attributes that are usually referenced together with some values. This pro-
ﬁle describes typical user behaviour and is used to detect misuse. This method
assumes that the legitimate users show some level of consistency in using the
database system. If this assumption does not hold, or if the threshold for incon-
sistency is not set properly, the result will be a high level of false positives. This
method also faces the attribute selection problem when it chooses the “features”
to consider when building the working scopes.
Classiﬁcation algorithms (e.g. C4.5 [16]) seem to be applicable to our problem
of identifying the instances of SQL statements that constitute an intrusion. How-
ever, this will lead to the “feature selection” problem in which we have to decide
on the feature set of the SQL statements to be used in the classiﬁer. The entire
training set will also have to be tagged as either legitimate or illegitimate which
may be infeasible for large training sets. Text summarization techniques [17, 18]
cannot be applied to our problem of SQL transaction summarization as unlike
text collections, our logs consist of largely independent SQL transactions and
lack the dependency (style, theme) among linguistic units (phrase, clause, etc)
in text collections.
7
Conclusion
DIDAFIT is a database intrusion detection system that identiﬁes anomalous
database accesses by matching database transactions with a set of legitimate
transaction ﬁngerprints. This work addresses the problem of learning the set
of legitimate ﬁngerprints from the database trace logs that contain the SQL
statements. We developed an algorithm that can:
1. selectively and eﬀectively summarize SQL statements into ﬁngerprints.
2. detect “high-risk” (possibly malicious) SQL statements even in the training
set of SQL statements.
3. derive possibly legitimate ﬁngerprints that are missing from the SQL state-
ments in the training set.
Currently, the algorithm works mostly at the syntactic-level. We are look-
ing into how domain knowledge can contribute to form a semantically-richer
intrusion detection mechanism. Further research is also done to study how the
algorithm can be extended to support incremental learning.
Acknowledgment
The authors would like to thank the anonymous reviewers for their useful com-
ments.

Learning Fingerprints for a Database Intrusion Detection System
279
References
[1] Low, W. L., Lee, S. Y., Teoh, P.: DIDAFIT: Detecting Intrusions in Databases
Through Fingerprinting Transactions. In: Proceedings of the 4th International
Conference on Enterprise Information Systems (ICEIS). (2002)
264, 265, 267,
269
[2] Atanasov, M.: The truth about internet fraud. In: ZiﬀDavis Smart Business,
Available at URL http://techupdate.zdnet.com/techupdate/stories/main/
0,14179,2688776-11,00.html (2001)
264
[3] Hatcher, T.: Survey: Costs of computer security breaches soar. In: CNN.com,
Available at URL
http://www.cnn.com/2001/TECH/internet/03/12/ csi.fbi.hacking.report/
(2001)
264
[4] Poulsen, K.: Guesswork Plagues Web Hole Reporting. In: SecurityFocus, Available
at URL http://online.securityfocus.com/news/346 (2002)
264, 265
[5] Internet Security Systems: RealSecure Intrusion Detection Solution, Available at
URL http://www.iss.net (2001)
265
[6] NFR Security: NFR network intrusion detection, Available at URL
http://www.nfr.com/products/NID/ (2001)
265
[7] Enterasys Networks, Inc.: The Dragon IDS, Available at URL
http://www.enterasys.com/ids/dragonids.html (2001)
265
[8] Cisco Systems, Inc.: Cisco Intrusion Detection, Available at URL
http://www.cisco.com/warp/public/cc/pd/sqsw/sqidsz/ (2001)
265
[9] Symantec Corporation: Enterprise Solutions, Available at URL
http://enterprisesecurity.symantec.com/ (2001)
265
[10] Roesch, M.: Snort: Lighweight intrusion detection for networks. In: Proceedings of
the 13th Conference on Systems Administration (LISA-99), USENIX Association
(1999) 229–238
265
[11] Andrews, C.: SQL injection FAQ, Available at URL
http://www.sqlsecurity.com (2001)
266
[12] Anley, C.: Advanced SQL Injection In SQL Server Applications, Next Generation
Security Software Ltd, Available at URL http://www.nextgenss.com/papers/
advanced sql injection.pdf (2002)
266
[13] Anley, C.: (more) Advanced SQL Injection, Next Generation Security Software
Ltd, Available at URL http://www.nextgenss.com/papers/
more advanced sql injection.pdf (2002)
266
[14] Oracle: Oracle, 2001, Available at URL http://www.oracle.com (2001)
269
[15] Chung, C. Y., Gertz, M., Levitt, K.: Misuse detection in database systems through
user proﬁling. In: Web Proceedings of the 2nd International Workshop on the
Recent Advances in Intrusion Detection (RAID). (1999)
278
[16] Quinlan, J. R.: Induction of decision trees. In Shavlik, J. W., Dietterich, T. G.,
eds.: Readings in Machine Learning. Morgan Kaufmann (1990) Originally pub-
lished in Machine Learning 1:81–106, 1986.
278
[17] Hovy, E., Lin, C. Y.: Automated Text Summarization in SUMMARIST. In: Pro-
ceedings of ACL/EACL Workshop on Intelligent Scalable Text Summarization.
(1997) Madrid, Spain.
278
[18] Boguraev, B., Bellamy, R.: Dynamic Presentation of Phrasally-Based Document
Abstractions. In: Proceedings of Thirty-second Annual Hawaii International Con-
ference on System Sciences (HICSS). (1998)
278

Author Index
Backes, Michael . . . . . . . . . . . . . . . . . .1
Biskup, Joachim . . . . . . . . . . . . . . . . 39
Bonatti, Piero . . . . . . . . . . . . . . . . . . 39
Broadfoot, Philippa . . . . . . . . . . . .146
Chari, Suresh . . . . . . . . . . . . . . . . . . 126
Giles, James . . . . . . . . . . . . . . . . . . . 126
GomuPlkiewicz, Marcin . . . . . . . . . . 90
Heather, James . . . . . . . . . . . . . . . . 162
Heisel, Maritta . . . . . . . . . . . . . . . . 194
Hogben, Giles . . . . . . . . . . . . . . . . . 104
Jackson, Tom . . . . . . . . . . . . . . . . . .104
Jajodia, Sushil . . . . . . . . . . . . . . . . . .55
Koch, Manuel . . . . . . . . . . . . . . . . . 229
Kurosawa, Kaoru . . . . . . . . . . . . . . . 24
KutyPlowski, MirosPlaw . . . . . . . . . . . 90
Lee, Sin Yeung . . . . . . . . . . . . . . . . 264
Lotz, Volkmar . . . . . . . . . . . . . . . . . 212
Low, Wai Lup . . . . . . . . . . . . . . . . . 264
Lowe, Gavin . . . . . . . . . . . . . . . . . . . 146
Mancini, Luigi V. . . . . . . . . . . . . . .229
Mok, Aloysius K. . . . . . . . . . . . . . . 178
Ogata, Wakaha . . . . . . . . . . . . . . . . . 24
Oheimb, David von . . . . . . . . . . . . 212
Parisi-Presicce, Francesco . . . . . . 229
Pﬁtzmann, Andreas . . . . . . . . . . . .194
Pﬁtzmann, Birgit . . . . . . . . . . . . . . . . 1
Reeves, Douglas S. . . . . . . . . . . . . .244
Sailer, Reiner . . . . . . . . . . . . . . . . . . 126
Santen, Thomas . . . . . . . . . . . . . . . 194
Schneider, Steve . . . . . . . . . . . . . . . 162
Smith, Sean W. . . . . . . . . . . . . . . . . .72
Verma, Dinesh . . . . . . . . . . . . . . . . .126
Wang, Lingyu . . . . . . . . . . . . . . . . . . .55
Wang, Xinyuan . . . . . . . . . . . . . . . . 244
Wijesekera, Duminda . . . . . . . . . . . 55
Wilikens, Marc . . . . . . . . . . . . . . . . 104
Wong, Pei Yuen . . . . . . . . . . . . . . . 264
Wu, S. Felix . . . . . . . . . . . . . . . . . . . 244
Yu, Weijiang . . . . . . . . . . . . . . . . . . .178


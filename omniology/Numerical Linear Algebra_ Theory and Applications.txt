Larisa Beilina
Evgenii Karchevskii
Mikhail Karchevskii
Numerical Linear 
Algebra: Theory 
and Applications

Numerical Linear Algebra: Theory and Applications

Larisa Beilina
• Evgenii Karchevskii
Mikhail Karchevskii
Numerical Linear Algebra:
Theory and Applications
123

Larisa Beilina
Department of Mathematical Sciences
Chalmers University of Technology and
University of Gothenburg
Gothenburg
Sweden
Evgenii Karchevskii
Department of Applied Mathematics
Kazan Federal University
Kazan, Tatarstan Republic
Russia
Mikhail Karchevskii
Department of Computational Mathematics
Kazan Federal University
Kazan, Tatarstan Republic
Russia
ISBN 978-3-319-57302-1
ISBN 978-3-319-57304-5
(eBook)
DOI 10.1007/978-3-319-57304-5
Library of Congress Control Number: 2017938311
© Springer International Publishing AG 2017
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
Problems of numerical linear algebra arise in all ﬁelds of modern science. Important
examples are computational ﬂuid dynamics, solid mechanics, electrical networks,
signal analysis, and optimization. In our book we present an extended basic theory of
linear algebra including such topics as matrix algebra, theory of linear systems of
equations, spectral theory, vector and matrix norms, combined with the most impor-
tant direct and iterative numerical methods for solution of linear systems of equations,
least squares problems, and eigenproblems. In this book we wanted to combine a solid
theoretical background in linear algebra with practical algorithms for numerical
solution of linear algebra problems. Most of the presented numerical algorithms
are illustrated by computer programs written in MATLAB®, which are given in the
electronic supplementary material that can be found on SpringerLink. These programs
allow the reader to obtain experience in implementation and evaluation of numerical
algorithms for the problems described in the book and give possibility to apply them to
the solution of the computer exercises presented in this book. They can also give the
reader a better understanding of professional numerical software for the solution of
real-life problems in numerical linear algebra.
This book is suitable for use as course material in a one- or two-semester course on
numerical linear algebra, matrix computations, or large sparse matrices at the
advanced undergraduate or graduate level. We recommend using the material of
Chapters 1–7 for courses in the theoretical aspects of linear algebra, or as the ﬁrst part
for a course in numerical linear algebra. In addition to traditional content for courses
in linear algebra for students specializing in the physical and mathematical sciences,
we include in these chapters some sections that can be useful as course material for
special courses on various applications of linear algebra. We hope that this material
will also be of interest to scientists. We recommend Chapters 8–12 for courses related
to numerical linear algebra, or as the second part of a course in numerical linear
algebra. The material of Chapters 8–12 follows the book of Demmel [23]. Compared
with [23], we present the numerical material of Chapters 8–12 in a more concise form,
which is appropriate to a one-semester course in numerical linear algebra at the
undergraduate level. We also enrich our Chapters 8–12 with numerical examples,
v

which can be tested by the MATLAB® and PETSc programs are available in the
electronic supplementary material that can be found on SpringerLink.
In the ﬁrst four chapters we introduce readers to the topic of linear algebra and
give the main deﬁnitions of complex numbers and polynomials, systems of linear
equations, matrices, determinants, vector and inner product spaces, subspaces,
linear operators, and eigenvalues and eigenvectors of a linear operator. In Chapter 5
we present canonical forms and factorizations: the singular value decomposition,
the Jordan canonical form, matrix pencils and Weierstrass canonical form, the
Kronecker canonical form, and their applications in the theory of ordinary differ-
ential equations. Chapter 6 discusses vector and matrix norms and Chapter 7 pre-
sents the main elements of perturbation theory for basic problems of linear algebra.
Chapters 8–11 deal with numerical solution of systems of linear equations, linear
least squares problems, and the solution of eigenvalue problems. In Chapter 12 we
give a brief introduction to the main iterative methods for the solution of linear
systems: Jacobi, Gauss–Seidel, and Successive overrelaxation. We also discuss
Krylov subspace methods, the conjugate gradient algorithm, and the preconditioned
conjugate gradient algorithm. Compared with other books on the same subject, this
book presents a combination of extended material on the rigorous theory of linear
algebra together with numerical aspects and implementation of algorithms of linear
algebra in MATLAB®. The material of this book was developed from a number of
courses which the authors taught repeatedly for a long period at the master’s
program in engineering mathematics and computational science at Chalmers
University of Technology and University of Gothenburg, Sweden, and at Institute
of Computational Mathematics and Information Technologies of Kazan Federal
University, Russia. Chapters 1–7 were written by Mikhail and Evgenii Karchevskii.
Larisa Beilina wrote Chapters 8–12 and the electronic supplementary material that
can be found on SpringerLink.
The authors want to thank the following colleagues and students for corrections,
proofreading, and contributions to the material of this book: Yu.A. Al’pin,
V.B. Andreev, A. Bergqvist, I.D. Beilin, E.V. Chizhonkov, R.Z. Dautov,
H. Eklund, N. Ericsson, M. Hoppe, J. Jagers, J. Jansson, B. Galimullin,
A.V.
Goolin,
R.N.
Gumerov,
A.S.
Ilinskii,
A.
Mutygullin,
A.
Repina,
R.R. Shagidullin, Yu.G. Smirnov, E.L. Stolov, S.I. Soloviov, M.R. Timerbaev,
A. Vasilyeva, O. Wickius.
Gothenburg, Sweden
Larisa Beilina
Kazan, Russia
Evgenii Karchevskii
Kazan, Russia
Mikhail Karchevskii
February 2017
vi
Preface

Contents
1
Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Complex Numbers and Polynomials . . . . . . . . . . . . . . . . . . . . .
1
1.1.1
Complex Numbers: Basic Operations and Formulas . . .
1
1.1.2
Algebraic Operations with Polynomials . . . . . . . . . . . .
8
1.1.3
Roots of Polynomials and Their Properties. . . . . . . . . .
11
1.1.4
Viète’s Formulas. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
1.1.5
Polynomials with Real Coefﬁcients. . . . . . . . . . . . . . . .
15
1.2
Systems of Linear Equations, Matrices, Determinants . . . . . . . .
17
1.2.1
Permutations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
1.2.2
Determinants and Their Basic Properties . . . . . . . . . . .
19
1.2.3
Cramer’s Rule. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
1.2.4
Matrices: Basic Operations and Transformations . . . . .
32
1.2.5
Gaussian Elimination . . . . . . . . . . . . . . . . . . . . . . . . . .
43
1.2.6
The Determinant of the Product of Matrices. . . . . . . . .
49
1.2.7
Basic Matrix Types. . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
1.2.8
Block Matrices and Basic Operations with Block
Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
2
Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
2.1
The Vector Spaces Rn and Cn . . . . . . . . . . . . . . . . . . . . . . . . . .
55
2.1.1
The Vector Space Rn . . . . . . . . . . . . . . . . . . . . . . . . . .
55
2.1.2
The Vector Space Cn . . . . . . . . . . . . . . . . . . . . . . . . . .
57
2.2
Abstract Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
2.2.1
Deﬁnitions and Examples . . . . . . . . . . . . . . . . . . . . . . .
57
2.2.2
Linearly Dependent Vectors . . . . . . . . . . . . . . . . . . . . .
59
2.2.3
Linearly Independent Sets of Vectors . . . . . . . . . . . . . .
62
2.2.4
The Rank of a Set of Vectors . . . . . . . . . . . . . . . . . . . .
63
2.3
Finite-Dimensional Vector Spaces. Bases. . . . . . . . . . . . . . . . . .
64
2.3.1
Bases in the Space Cn . . . . . . . . . . . . . . . . . . . . . . . . .
64
2.3.2
Finite-Dimensional Spaces. Examples. . . . . . . . . . . . . .
65
2.3.3
Change of Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
vii

3
Inner Product Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
3.1
Inner Products on Rn and Cn . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
3.2
Abstract Inner Product Spaces . . . . . . . . . . . . . . . . . . . . . . . . . .
71
3.2.1
Deﬁnitions and Examples . . . . . . . . . . . . . . . . . . . . . . .
71
3.2.2
The Cauchy–Schwarz Inequality. . . . . . . . . . . . . . . . . .
72
3.2.3
The Gram Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
3.2.4
Orthogonal Sets of Vectors. Gram–Schmidt
Orthogonalization Process. . . . . . . . . . . . . . . . . . . . . . .
75
3.2.5
The Expansion of a Vector with Respect to a Basis
in an Inner Product Space. . . . . . . . . . . . . . . . . . . . . . .
79
3.2.6
The Calculation of an Inner Product . . . . . . . . . . . . . . .
80
3.2.7
Reciprocal Basis Vectors . . . . . . . . . . . . . . . . . . . . . . .
80
3.2.8
Examples of Orthogonal Bases . . . . . . . . . . . . . . . . . . .
81
3.3
Subspaces. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
3.3.1
The Sum and Intersection of Subspaces . . . . . . . . . . . .
84
3.3.2
The Dimension of the Sum of Subspaces . . . . . . . . . . .
86
3.3.3
The Orthogonal Projection of a Vector onto
a Subspace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
3.3.4
The Orthogonal Decomposition of an Inner
Product Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
4
Linear Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
4.1
Linear Operators and Their Basic Properties . . . . . . . . . . . . . . .
93
4.1.1
Basic Deﬁnitions. Operations with Operators . . . . . . . .
93
4.1.2
The Inverse Operator . . . . . . . . . . . . . . . . . . . . . . . . . .
95
4.1.3
The Coordinate Representation Operator. . . . . . . . . . . .
96
4.1.4
Isomorphism of Finite-Dimensional Linear Spaces . . . .
97
4.1.5
The Matrix of a Linear Operator. . . . . . . . . . . . . . . . . .
98
4.1.6
The Matrix of the Inverse Operator . . . . . . . . . . . . . . .
101
4.1.7
Linear Spaces of Linear Operators . . . . . . . . . . . . . . . .
102
4.1.8
The Image and the Kernel of a Linear Operator . . . . . .
102
4.1.9
The Rank of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . .
103
4.1.10
Calculating the Rank of a Matrix Using
Determinants. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
106
4.1.11
The General Solution of a Linear Equation. . . . . . . . . .
107
4.1.12
Systems of Linear Algebraic Equations. Solvability
Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108
4.1.13
The General Solution of a System of Linear
Algebraic Equations . . . . . . . . . . . . . . . . . . . . . . . . . . .
110
4.2
Eigenvalues and Eigenvectors of a Linear Operator . . . . . . . . . .
113
4.2.1
Invariant Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . .
113
4.2.2
Basic Properties of Eigenvalues and Eigenvectors . . . .
115
4.2.3
Diagonalizable Operators . . . . . . . . . . . . . . . . . . . . . . .
121
4.2.4
Invariants of an Operator . . . . . . . . . . . . . . . . . . . . . . .
122
viii
Contents

4.2.5
Invariant Subspaces of an Operator in a Real Space. . .
125
4.2.6
Nilpotent Operators. . . . . . . . . . . . . . . . . . . . . . . . . . . .
126
4.2.7
The Triangular Form of the Matrix of an Operator. . . .
127
4.2.8
The Real Schur Form . . . . . . . . . . . . . . . . . . . . . . . . . .
131
4.3
Operators on Unitary Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . .
132
4.3.1
Linear Functionals . . . . . . . . . . . . . . . . . . . . . . . . . . . .
132
4.3.2
The Adjoint Operator . . . . . . . . . . . . . . . . . . . . . . . . . .
133
4.3.3
Linear Equations in Unitary Spaces . . . . . . . . . . . . . . .
135
4.3.4
Pseudosolutions. The Tikhonov Regularization
Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137
4.3.5
Self-adjoint and Skew-Hermitian Operators . . . . . . . . .
139
4.3.6
Positive Deﬁnite and Positive Semideﬁnite
Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
4.3.7
Unitary Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
4.3.8
Normal Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
4.3.9
The Root of a Positive Semideﬁnite
Self-adjoint Operator. . . . . . . . . . . . . . . . . . . . . . . . . . .
145
4.3.10
Congruent Hermitian Operators . . . . . . . . . . . . . . . . . .
146
4.3.11
Variational Properties of Eigenvalues
of Self-adjoint Operators. . . . . . . . . . . . . . . . . . . . . . . .
148
4.3.12
Examples of Application of Variational Properties
of Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
150
4.4
Operators on Euclidean Spaces . . . . . . . . . . . . . . . . . . . . . . . . .
155
4.4.1
Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
155
4.4.2
The Structure of Normal Operators. . . . . . . . . . . . . . . .
156
4.4.3
The Structure of Orthogonal Operators . . . . . . . . . . . . .
158
4.4.4
Givens Rotations and Householder Transformations. . .
160
5
Canonical Forms and Factorizations. . . . . . . . . . . . . . . . . . . . . . . . .
163
5.1
The Singular Value Decomposition . . . . . . . . . . . . . . . . . . . . . .
163
5.1.1
Singular Values and Singular Vectors
of an Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
163
5.1.2
The Polar Decomposition . . . . . . . . . . . . . . . . . . . . . . .
167
5.1.3
Basic Properties of the Pseudoinverse Operator . . . . . .
169
5.1.4
Elements of the Theory of Majorization . . . . . . . . . . . .
170
5.1.5
Some Estimates of Eigenvalues and Singular
Values. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
174
5.2
The Jordan Canonical Form . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179
5.2.1
Existence and Uniqueness of the Jordan Canonical
Form. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179
5.2.2
Root and Cyclic Subspaces. . . . . . . . . . . . . . . . . . . . . .
185
5.2.3
The Real Jordan Canonical Form . . . . . . . . . . . . . . . . .
186
5.2.4
Power Series of Matrices . . . . . . . . . . . . . . . . . . . . . . .
188
Contents
ix

5.3
Matrix Pencils . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
191
5.3.1
Deﬁnitions and Basic Properties . . . . . . . . . . . . . . . . . .
191
5.3.2
The Quasidiagonal Form of a Regular Pencil . . . . . . . .
195
5.3.3
Weierstrass Canonical Form . . . . . . . . . . . . . . . . . . . . .
196
5.3.4
Hermitian and Deﬁnite Pencils . . . . . . . . . . . . . . . . . . .
197
5.3.5
Singular Pencils. The Theorem on Reduction . . . . . . . .
198
5.3.6
Kronecker Canonical Form . . . . . . . . . . . . . . . . . . . . . .
203
5.3.7
Applications to Systems of Linear Differential
Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
204
6
Vector and Matrix Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
209
6.1
Basic Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
209
6.2
Norms on the Space Cn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
211
6.3
The Hahn–Banach Theorem. Dual Norms . . . . . . . . . . . . . . . . .
214
6.4
Norms on the Space of Matrices . . . . . . . . . . . . . . . . . . . . . . . .
218
6.5
The Gap Between Two Subspaces of Cn . . . . . . . . . . . . . . . . . .
226
7
Elements of Perturbation Theory . . . . . . . . . . . . . . . . . . . . . . . . . . .
231
7.1
Perturbations in the Symmetric Eigenvalue Problem . . . . . . . . .
231
7.2
Perturbations of Singular Values and Singular Vectors . . . . . . .
236
7.3
Perturbations of Characteristic Values of Arbitrary Matrices . . .
237
7.4
Perturbations and the Invertibility of a Matrix . . . . . . . . . . . . . .
241
7.5
The Stability of Systems of Linear Equations . . . . . . . . . . . . . .
243
7.6
Perturbations in the Linear Least Squares Problem . . . . . . . . . .
246
8
Solving Systems of Linear Equations . . . . . . . . . . . . . . . . . . . . . . . .
249
8.1
Algorithms for Gaussian Elimination . . . . . . . . . . . . . . . . . . . . .
249
8.1.1
LU Factorization with Pivoting. . . . . . . . . . . . . . . . . . .
249
8.1.2
The Need for Pivoting . . . . . . . . . . . . . . . . . . . . . . . . .
252
8.1.3
A Numerical Example . . . . . . . . . . . . . . . . . . . . . . . . .
255
8.2
Error Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
258
8.2.1
Rounding Analysis in Polynomial Evaluation. . . . . . . .
259
8.2.2
Error Analysis in Gaussian Elimination . . . . . . . . . . . .
265
8.2.3
Estimating the Condition Number. . . . . . . . . . . . . . . . .
268
8.2.4
Estimating the Relative Condition Number. . . . . . . . . .
271
8.2.5
Practical Error Bounds . . . . . . . . . . . . . . . . . . . . . . . . .
271
8.3
Algorithms for Improving the Accuracy of the Solution . . . . . .
272
8.4
Special Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
275
8.4.1
Real Symmetric Positive Deﬁnite Matrices . . . . . . . . . .
275
8.4.2
Symmetric Indeﬁnite Matrices . . . . . . . . . . . . . . . . . . .
279
8.4.3
Band Matrices. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
279
8.4.4
A Numerical Example . . . . . . . . . . . . . . . . . . . . . . . . .
284
Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
284
x
Contents

9
Numerical Solution of Linear Least Squares Problems . . . . . . . . . .
291
9.1
Linear Least Squares Problems . . . . . . . . . . . . . . . . . . . . . . . . .
292
9.2
Nonlinear Least Squares Problems . . . . . . . . . . . . . . . . . . . . . . .
297
9.3
Method of Normal Equations. . . . . . . . . . . . . . . . . . . . . . . . . . .
303
9.4
QR Decomposition. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
305
9.5
Orthogonalization Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . .
307
9.5.1
Householder Transformations . . . . . . . . . . . . . . . . . . . .
307
9.5.2
Givens Rotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
319
9.5.3
Gram–Schmidt Orthogonalization . . . . . . . . . . . . . . . . .
323
9.6
Singular Value Decomposition. . . . . . . . . . . . . . . . . . . . . . . . . .
326
9.6.1
Rank-Deﬁcient Least Squares Problems . . . . . . . . . . . .
333
9.6.2
How to Solve Rank-Deﬁcient Least Squares
Problems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
336
9.7
Software for the Solution of Linear Least Squares Problems . . .
338
Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
339
10
Algorithms for the Nonsymmetric Eigenvalue Problem . . . . . . . . . .
345
10.1
Power Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
345
10.2
Inverse Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
348
10.3
Orthogonal Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
351
10.4
QR Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
355
10.5
QR Iteration with Shifts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
356
10.6
Hessenberg Reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
358
10.7
Tridiagonal and Bidiagonal Reduction . . . . . . . . . . . . . . . . . . . .
362
10.7.1
Tridiagonal Reduction Using Householder
Transformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
365
10.7.2
Tridiagonal Reduction Using Givens Rotation . . . . . . .
367
10.8
QR Iteration with Implicit Shifts . . . . . . . . . . . . . . . . . . . . . . . .
370
Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
373
11
Algorithms for Solution of Symmetric Eigenvalue Problems. . . . . .
375
11.1
Tridiagonal QR Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
376
11.2
Rayleigh Quotient Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . .
377
11.3
Divide and Conquer. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
380
11.4
Bisection and Inverse Iteration. . . . . . . . . . . . . . . . . . . . . . . . . .
385
11.5
Jacobi’s Method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
388
11.6
Algorithms for the Singular Value Decomposition. . . . . . . . . . .
393
11.7
Different Versions of QR Iteration for the Bidiagonal SVD. . . .
396
11.8
Jacobi’s Method for the SVD. . . . . . . . . . . . . . . . . . . . . . . . . . .
400
Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
403
12
Introduction to Iterative Methods for the Solution
of Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
407
12.1
Basic Iterative Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
407
12.2
Jacobi Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
409
12.3
Gauss–Seidel Method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
411
Contents
xi

12.4
Successive Overrelaxation SOR(x) Method. . . . . . . . . . . . . . . .
413
12.5
Symmetric Successive Overrelaxation SSOR(x) Method. . . . . .
414
12.6
Convergence of Main Iterative Methods . . . . . . . . . . . . . . . . . .
418
12.7
Krylov Subspace Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
424
12.8
Conjugate Gradient Method . . . . . . . . . . . . . . . . . . . . . . . . . . . .
429
12.9
Preconditioning for Linear Systems . . . . . . . . . . . . . . . . . . . . . .
433
Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
436
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
441
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
445
xii
Contents

Chapter 1
Preliminaries
In this chapter we provide the necessary background from the theory of complex
numbers. Then we describe the basic properties of polynomials and their roots. We
introduce the concept of determinants, establish their properties, and present the
basic theory of systems of linear algebraic equations with nonsingular matrices. The
main types of rectangular matrices are described.
1.1
Complex Numbers and Polynomials
1.1.1
Complex Numbers: Basic Operations and Formulas
It is well known that not every quadratic equation has a real solution. For example,
a simple equation like
x2 + 1 = 0
(1.1)
has no real solution, since the square of a real number is never negative. The situation
ischangedifweintroduceanewnumber(moreprecisely,anewsymbol).Thisnumber
is called the imaginary unit and is denoted by i. By deﬁnition, we have
i2 = −1.
Then α1 = i is a root of equation (1.1). It is natural that
(−i)2 = (−1)2i2 = −1.
Electronic supplementary material The online version of this chapter
(doi:10.1007/978-3-319-57304-5_1) contains supplementary material, which is available to
authorized users.
© Springer International Publishing AG 2017
L. Beilina et al., Numerical Linear Algebra: Theory and Applications,
DOI 10.1007/978-3-319-57304-5_1
1

2
1
Preliminaries
Thenα2 = −i isthesecondrootofequation (1.1),i.e.,thisequationhastwosolutions,
similarly to the equation
x2 −1 = 0.
Consider the quadratic equation
x2 + q = 0,
where q > 0. It is natural to decide that this equation has two roots:
α1 = i√q
and α2 = −i√q.
The numbers of the form ib, where b is real, are called imaginary numbers.
Let us now consider a general quadratic equation. For convenience, we write it in
the reduced form
x2 −2px + q = 0.
(1.2)
Using elementary calculations, we get
(x −p)2 + q −p2 = 0.
Suppose that q −p2 > 0, i.e., the discriminant of equation (1.2) is negative. It is
natural to decide that Eq.(1.2) has two roots too:
α1 = p + i

q −p2,
α2 = p −i

q −p2.
(1.3)
The numbers α1, α2 in (1.3) have the new form a + ib, where a and b are real.
They are called complex numbers. In the particular case that b = 0, the complex
number a + ib will be the same as the real number a. If a = 0, then the complex
number will be the same as the imaginary number ib.
Usually we denote a complex number by the letter z:
z = x + iy.
The real number x is called the real part of z and is denoted by Re z. The real
number y is called the imaginary part of z and is denoted by Im z. Therefore, we
can write
z = Re z + i Im z.
By deﬁnition, two complex numbers z1 = x1 + iy1 and z2 = x2 + iy2 are equal
if and only if x1 = x2 and y1 = y2.
Let us verify that the numbers α1, α2 deﬁned by (1.3) actually satisfy the quadratic
equation (1.2). To do this we have to introduce the algebraic operations with complex
numbers.

1.1 Complex Numbers and Polynomials
3
The sum of the complex numbers z1 = x1 + iy1 and z2 = x2 + iy2 is the complex
number z = x + iy, where x = x1 + x2, y = y1 + y2, i.e.,
Re (z1 + z2) = Re z1 + Re z2,
Im (z1 + z2) = Im z1 + Im z2.
The difference of the complex numbers z1 and z2 is the complex number
z = (x1 −x2) + i(y1 −y2).
Clearly, if z is the difference of complex numbers z1 and z2, then z2 + z = z1.
For example, the sum of the complex numbers z1 = 1 + i2 and z2 = 3 + i4 is
equal to z = 4 + i6, and their difference is z = −2 −i2.
The complex number 0 + i 0 is called zero and is denoted by the symbol 0.
Multiplication of complex numbers is carried out in the same way as the multi-
plication of usual binomials using the relation i2 = −1. Therefore, we have
z1z2 = (x1 + iy1)(x2 + iy2) = x1x2 −y1y2 + i(x1y2 + x2y1),
i.e., by deﬁnition,
Re (z1z2) = Re z1Re z2 −Im z1Im z2,
(1.4)
Im (z1z2) = Re z1Im z2 + Re z2Im z1.
(1.5)
For example, the product of the complex numbers z1 = 1 + i2, z2 = 3 + i4 is
calculated as follows:
z1z2 = (1 + i2)(3 + i4) = (1 × 3 −2 × 4) + i(1 × 4 + 3 × 2) = −5 + i10.
For every complex number z we have z × 0 = 0 × z = 0.
The reader can easily prove that the operations of addition and multiplication
of complex numbers deﬁned above have the same properties as the corresponding
operations with real numbers.
1. Commutativity: z1 + z2 = z2 + z1, z1z2 = z2z1.
2. Associativity: (z1 + z2) + z3 = z1 + (z2 + z3), (z1z2)z3 = z1(z2z3).
3. Distributivity: (z1 + z2)z3 = z1z3 + z2z3.
Now using direct substitution, the reader can check that the numbers α1 and α2
deﬁned by (1.3) satisfy the quadratic equation (1.2).
Division of complex numbers z1 and z2 is deﬁned by the following relationship:
zz2 = z1.
(1.6)

4
1
Preliminaries
Here the complex number z is the quotient of z1 divided by z2.
If z2 ̸= 0, then there exists a unique solution z of Eq.(1.6). Indeed, using (1.4),
(1.5), we can write (1.6) in more detail:
xx2 −yy2 + i(xy2 + x2y) = x1 + iy1.
(1.7)
Equating the real and imaginary parts, we get
xx2 −yy2 = x1,
(1.8)
xy2 + yx2 = y1.
(1.9)
The system of equations (1.8), (1.9) has the unique solution
x = x1x2 + y1y2
x2
2 + y2
2
,
(1.10)
y = x2y1 −x1y2
x2
2 + y2
2
.
(1.11)
Formulas (1.10) and (1.11) deﬁne the rule of division of complex numbers.
For example, let us divide the complex number z1 = 1 + i2 by z2 = 3 + i4:
z1
z2
= 1 + i2
3 + i4 = 1 × 3 + 2 × 4
32 + 42
+ i 3 × 2 −1 × 4
32 + 42
= 11
25 + i 2
25.
For every natural number n, by deﬁnition, put zn = zz · · · z, where the factor is
repeated n times, z0 = 1, z−n = (1/z)n.
It is important to note that if the imaginary parts of complex operands are equal
to zero, then the operations with complex numbers deﬁned above are identical to the
operations with real numbers (check this!). Therefore, we can accept that the ﬁeld
of all complex numbers is an extension of the ﬁeld of all real numbers.
For a complex number z = x + iy, the number z = x −iy is called the complex
conjugate of z. Evidently,
z = z,
z1 + z2 = z1 + z2,
z1z2 = z1z2.
(1.12)
Note also that
z + z = 2x,
z −z = i2y,
zz = x2 + y2.
The real nonnegative number |z| =
√
zz =

x2 + y2 is called the absolute value
(or modulus) of z = x + iy. Obviously,
if |z| = 0, then x = 0, y = 0, i.e., z = 0.
(1.13)
For all complex numbers z1, z2, by elementary calculations we get

1.1 Complex Numbers and Polynomials
5
|z1z2| = |z1||z2|.
(1.14)
For all real numbers x, y, the following inequality is well known:
2|xy| ≤(x2 + y2).
Using it, the reader can easily prove that for all complex numbers z1, z2, the next
inequality holds:
|z1 + z2| ≤|z1| + |z2|.
(1.15)
Relationships (1.13), (1.14), (1.15) show that we can use the absolute values of
complex numbers in the same way as the absolute values of real numbers.
Note that |z1| = |z1 −z2 +z2| ≤|z1 −z2|+|z2|; therefore, |z1|−|z2| ≤|z1 −z2|.
Similarly, |z2| −|z1| ≤|z1 −z2|. Thus,
||z2| −|z1|| ≤|z1 −z2|.
(1.16)
Recall that every real number x is assumed to correspond to a point on the number
line. Analogously, the set of complex numbers may be thought of as a Cartesian plane
with “real axis” x and “imaginary axis” y. Thus, z = x + iy may be identiﬁed with
the point (x, y) in the complex plane. Then complex conjugation is reﬂection across
the real axis, and |z| is the Euclidean distance of z from the origin in the complex
plane. (Check this by drawing!)
Recall that the sum of two vectors (x1, y1) and (x2, y2) is the vector with coordi-
nates x1 + x2 and y1 + y2. Therefore, the sum of two complex numbers z1 = x1 +iy1
and z2 = x2 + iy2 corresponds to the sum of two vectors (make a drawing!). Hence
inequalities (1.15), (1.16) can be interpreted as the well-known triangle inequalities
for vectors.
In the last two paragraphs we have described the complex plane in terms of
rectangular coordinates. The complex plane may also be represented usefully in
terms of polar coordinates, in which the position of z in the plane is described in
terms of the modulus of z and the angle ϕ, measured in the counterclockwise direction
from the positive real axis. The angle ϕ belongs to the interval [0, 2π) and is called
the argument of z. The following notation is often used:
ϕ = arg z,
ρ = |z|.
(1.17)
Let us obtain an explicit representation of z using |z| and arg z. We have
z = |z|
 x
|z| + i y
|z|

.
Evidently (make a drawing!),

6
1
Preliminaries
x
|z| = cos ϕ,
y
|z| = sin ϕ,
(1.18)
and thus
z = ρ(cos ϕ + i sin ϕ).
(1.19)
Relationships (1.17)–(1.19) give the so-called trigonometric form of a complex num-
ber. This form enables us to take a fresh look at algebraic operations with complex
numbers and to obtain several useful formulas.
If z1 = ρ1(cos ϕ1 + i sin ϕ1), z2 = ρ2(cos ϕ2 + i sin ϕ2), then using well-known
trigonometric formulas, we have
z1z2 = ρ1ρ2 (cos(ϕ1 + ϕ2) + i sin(ϕ1 + ϕ2)) ,
(1.20)
i.e., to multiply two complex numbers, we multiply their absolute values and add
their arguments.
For example, the product of the complex numbers z1 = 3 (cos(π/2) + i sin(π/2))
and z2 = 2 (cos(π/4) + i sin(π/4)) is z1z2 = 6 (cos(3π/4) + i sin(3π/4)) .
Note that using formula (1.20), we can obtain ϕ1 + ϕ2 ≥2π. Naturally, because
of the periodicity of trigonometric functions, the argument of a complex number is
determined modulo 2π. Hence equalities like (1.20) involving arguments will always
be interpreted as congruences modulo 2π; i.e., we shall neglect differences of integer
multiples of 2π.
Using the trigonometric form of complex numbers and formula (1.20), we can
write Eq.(1.6) as
ρρ2(cos(ϕ + ϕ2) + i sin(ϕ + ϕ2)) = ρ1(cos ϕ1 + i sin ϕ1),
(1.21)
whence
z = z1
z2
= ρ1
ρ2
(cos(ϕ1 −ϕ2) + i sin(ϕ1 −ϕ2)),
(1.22)
i.e., to divide two complex numbers, we divide their absolute values and subtract
their arguments.
For example, using formula (1.22), for the numbers z1 = 3(cos(π/2) +
i sin(π/2)) and z2 = 2 (cos(π/4) + i sin(π/4)), we get z1/z2 = (3/2)(cos(π/4) +
i sin(π/4)).
Let us obtain a formula expressing the powers of a complex number. If we put z1 =
z2 = z in (1.20), then we get
z2 = zz = ρ2(cos 2ϕ + i sin 2ϕ),
and generally,
zn = ρn(cos nϕ + i sin nϕ)
(1.23)

1.1 Complex Numbers and Polynomials
7
for every integer n (including zero and negative integers). Formula (1.23) is called
de Moivre’s formula.1
Let us now turn to the problem of calculation of an nth root of a complex num-
ber z = ρ(cos ϕ+i sin ϕ). Here n ≥1 is an integer. This is the problem of calculation
of a number ˜z = ˜ρ(cos ˜ϕ + i sin ˜ϕ) such that
˜zn = ˜ρn(cos n ˜ϕ + i sin n ˜ϕ) = ρ(cos ϕ + i sin ϕ).
(1.24)
Evidently, Eq.(1.24) has the following solutions:
˜ρ =
n√ρ,
˜ϕ = ϕ
n + 2πk
n ,
k = 0, 1, . . . .
Here
n√ρ is the unique real nth root of the real nonnegative number ρ. Hence we see
that the n complex numbers
zk =
n√ρ (cos ϕk + i sin ϕk) ,
ϕk = ϕ
n + 2πk
n ,
k = 0, 1, . . . , n −1,
(1.25)
are the nth roots of the complex number z. For k > n −1 the numbers zk repeat
periodically because of the periodicity of the trigonometric functions.
For example, the four fourth roots of the complex number
z = 3

cos π
2 + i sin π
2

are calculated by the following formulas:
zk =
4√
3(cos ϕk + i sin ϕk),
ϕk = π
8 + k π
2 ,
k = 0, 1, 2, 3.
Thus each complex number (except zero) has n distinct nth roots. All these roots
lie on the circle of radius
n√ρ centered at the origin. They divide it into n equal parts.
The following question naturally arises: is it possible to ﬁnd other roots of a
complex numberz? The answer is negative. To verify this, the reader can use the
results of Section1.1.3, p. 10, interpreting Eq.(1.24) as a problem of calculation of
roots of a polynomial of degree n.
Formula (1.25) is often written in a slightly different form. Let
qk = cos 2πk
n
+ i sin 2πk
n ,
k = 0, 1, 2, . . . , n −1.
Obviously, qn
k = 1 for k = 0, 1, 2, . . . , n −1, i.e., the qk are nth roots of unity. It is
easy to see that
zk = z0qk,
k = 0, 1, 2, . . . , n −1.
1Abraham de Moivre (1667–1754) was a French mathematician.

8
1
Preliminaries
Therefore, if we calculate the ﬁrst root
z0 =
n√ρ (cos ϕ/n + i sin ϕ/n) ,
then we obtain all other roots by successive shifts through the angle 2π/n on the unit
circle.
1.1.2
Algebraic Operations with Polynomials
A polynomial is a function of the form
Pn(z) = a0 + a1z + a2z2 + · · · + anzn.
(1.26)
Here a0, a1, . . . , an are ﬁxed complex numbers. They are called the coefﬁcients of the
polynomial. If an ̸= 0, then the integer n ≥0 is called the degree of the polynomial,
and an is called the leading coefﬁcient of the polynomial. The variable z can take
any complex value.
Two polynomials Pn(z) and Qn(z) are equal if they have exactly the same coef-
ﬁcients.
If all coefﬁcients of a polynomial are equal to zero, then the polynomial is equal
to zero for all z. This polynomial is called the zero polynomial and is denoted by 0.
The degree of the zero polynomial is undeﬁned.
The sum of two polynomials Pn(z) + Qm(z) is a polynomial, and its degree is
less than or equal to the maximum of n and m, or it is the zero polynomial.
The product of two polynomials Pn(z)Qm(z) is a polynomial of degree n + m.
The addition of the zero polynomial to any polynomial does not change that
polynomial. The product of two polynomials is equal to the zero polynomial if and
only if one of the factors is the zero polynomial (prove it!).
Let us introduce and investigate the operation of division of polynomials.
Theorem 1.1 For polynomials P(z)and Q(z)there exist polynomials q(z) and r(z)
such that
P(z) = Q(z)q(z) + r(z),
(1.27)
and either r = 0 or the degree of r(z) is less than the degree of Q(z). More-
over, the polynomials q(z) and r(z) are uniquely determined by the polynomi-
als P(z)and Q(z).
Proof First we suppose that either P = 0 or the degree of P(z) is less than the degree
of Q(z). Then the unique solution of equation (1.27) is q = 0 and r(z) = P(z).
Assume now that P(z) is a polynomial of degree n, Q(z) has degree m, and n ≥m.
To simplify the notation we suppose that the leading coefﬁcient of the polyno-
mial Q(z) is equal to one. If we suppose that the leading coefﬁcient is an arbitrary

1.1 Complex Numbers and Polynomials
9
nonzero number, then the formulas written below should be modiﬁed in an obvious
way. Thus we take
P(z) = anzn + an−1zn−1 + · · · + a0,
Q(z) = zm + bm−1zm−1 + · · · + b0,
q(z) = cn−mzn−m + cn−m−1zn−m−1 + · · · + c0,
r(z) = dm−1zm−1 + dm−2zm−2 + · · · + d0.
The coefﬁcients of polynomials P(z) and Q(z) are known. Let us calculate the
coefﬁcients of q(z) and r(z). Collecting all coefﬁcients with the same power of z on
the right-hand side of (1.27) and equating them with the corresponding coefﬁcients
of the polynomial P(z), we get
an = cn−m,
an−1 = cn−m−1 + cn−mbm−1,
an−2 = cn−m−2 + cn−m−1bm−1 + cn−mbm−2,
. . . . . . . . .
am = c0 + c1bm−1 + c2bm−2 + · · · + cmb0,
am−1 = dm−1 + c0bm−1 + c1bm−2 + · · · + cm−1b0,
. . . . . . . . .
a0 = d0 + c0b0.
(1.28)
The obtained relationships form a system of linear equations for the coefﬁcients of
the polynomials q(z) and r(z). This system is easily solved and uniquely deﬁnes the
coefﬁcients of these polynomials. First the coefﬁcients c j are calculated in the order
of descending indices:
cn−m = an,
cn−m−1 = an−1 −cn−mbm−1,
cn−m−2 = an−2 −cn−m−1bm−1 −cn−mbm−2,
. . . . . . . . .
c0 = am −c1bm−1 −c2bm−2 −· · · −cmb0.
(1.29)
Then, using the calculated values of c j, the coefﬁcients d j are calculated by the
following formulas:
dm−1 = am−1 −c0bm−1 −c1bm−2 −· · · −cm−1b0,
dm−2 = am−2 −c0bm−2 −c1bm−3 −· · · −cm−2b0,
. . . . . . . . .
d0 = a0 −c0b0.
(1.30)
Note that cn−m ̸= 0, since an ̸= 0, but the coefﬁcients of r(z) generally speaking
can be equal to zero.

10
1
Preliminaries
We suppose here that 2m ≤n. If 2m > n, then formulas (1.28)–(1.30) should be
modiﬁed in an obvious way.
□2
The method of calculation of the coefﬁcients of polynomials q(z) and r(z), which
is described in the proof, is called Horner’s rule.3 It is commonly used for algebraic
calculations.
Formula (1.27) deﬁnes the operation of division of a polynomial P(z) by a poly-
nomial Q(z). The polynomials q(z) and r(z) are called respectively the quotient and
remainder of the division. If r = 0, then we say that Q(z) divides P(z), or Q(z) is a
factor of P(z).
Remark 1.1 It follows from (1.29) and (1.30) that if P(z) and Q(z) are polynomials
with real coefﬁcients, then the coefﬁcients of the polynomials q(z) and r(z) are real
numbers.
As an example of application of Horner’s rule, let us divide
P4(z) = 2z4 −3z3 + 4z2 −5z + 6 by
Q2(z) = z2 −3z + 1,
i.e., let us calculate polynomials
q2(z) = c2z2 + c1z + c0
and r(z) = d1z + d0
such that
P4(z) = Q2(z)q2(z) + r(z).
In this example we have n = 4 and m = 2. First, using (1.29), we calculate the
coefﬁcients c2, c1, and c0:
c2 = a4 = 2,
c1 = a3 −c2b1 = −3 −2(−3) = 3,
c0 = a2 −c1b1 −c2b0 = 4 −3(−3) −2 × 1 = 11.
Then, using (1.30), we calculate the coefﬁcients d1 and d0:
d1 = a1 −c0b1 −c1b0 = −5 −11(−3) −3 × 1 = 25,
d0 = a0 −c0b0 = 6 −11 × 1 = −5.
Thus,
q2(z) = 2z2 + 3z + 11,
r(z) = 25z −5.
The question naturally arises whether the corresponding coefﬁcients of the poly-
nomials Pn(z) and Qn(z) are equal if the values of those polynomials are equal for all
2Here and below, the symbol □indicates the end of a proof.
3William George Horner (1786–1837) was a British mathematician.

1.1 Complex Numbers and Polynomials
11
z. In other words, are all coefﬁcients of a polynomial equal to zero if that polynomial
is equal identically to zero? The answer is yes, but we will prove it somewhat later.
Oddly enough, a simple proof is based on some results of the theory of systems of
linear algebraic equations (see Section1.2.3, p. 27).
1.1.3
Roots of Polynomials and Their Properties
A root of a polynomial Pn(z) is a complex number α such that Pn(α) = 0.
Theorem 1.2 (Bézout’s4 theorem). Let α be a complex number, n ≥1. Then z −α
divides the polynomial Pn(z) −Pn(α).
Proof Using Theorem 1.1, p. 10, we obtain Pn(z) −Pn(α) = qn−1(z)(z −α) + r,
where r is either a number (a polynomial of degree zero) or the zero polynomial. If in
the last equality we take z = α, thenr = 0, i.e., Pn(z)−Pn(α) = qn−1(z)(z−α).
□
The next corollary immediately follows from Bézout’s theorem.
Corollary 1.1 A complex number α is a root of a polynomial Pn(z) if and only
if z −α divides Pn(z).
A complex number α is called a root of multiplicity k ≥1 of a polynomial Pn(z)
if (z −α)k divides Pn(z):
Pn(z) = (z −α)kqn−k(z),
but z −α does not divide qn−k(z), i.e., the number α is not a root of the polyno-
mial qn−k(z).
A root of multiplicity one is called simple.
A polynomial is called normalized if the original polynomial has been divided by
the leading coefﬁcient. Evidently, each root of the original polynomial is a root of
the normalized polynomial, and conversely, each root of the normalized polynomial
is a root of the original polynomial. To simplify the notation, properties of roots of
polynomials are usually investigated for polynomials in normalized form.
Theorem 1.3 (The fundamental theorem of algebra). Each polynomial
Pn(z) = zn + an−1zn−1 + · · · + a0,
n ≥1,
has at least one root.
Proof As usual, we denote by x1, x2 the Cartesian coordinates of points in the
plane R2. Let x = (x1, x2) ∈R2 and let z = x1 + ix2 be the corresponding complex
4Étienne Bézout (1730–1783) was a French mathematician.

12
1
Preliminaries
number. The equality f (x) = |Pn(z)| determines the function f of two real variables.
This function is nonnegative for all x ∈R2. If there exists a point y = (y1, y2) such
that f (y) = 0, then the complex number α = y1 +iy2 is a root of the polynomial Pn.
To prove the existence of such a point y, ﬁrst of all, we check that the function f is
continuous on R2.
From (1.16), p. 5, it follows that
| f (˜x) −f (x)| = ||Pn(˜z)| −|Pn(z)|| ≤|Pn(˜z) −Pn(z)|
for any two points x and ˜x. Here ˜z = ˜x1 + i˜x2. Put h = ˜z −z. Then
Pn(˜z) = Pn(z + h) = (z + h)n + an−1(z + h)n−1 + · · · + a1(z + h) + a0. (1.31)
Using the binomial formula, we see that
(z + h)k = zk + C1
k zk−1h + · · · + Ck−1
k
zhk−1 + hk
for each integer k ≥1. Further, we group like terms on the right-hand side of (1.31)
and get
Pn(z + h) = Pn(z) + c1h + c2h2 + · · · + cn−1hn−1 + hn,
(1.32)
where the coefﬁcients c1, . . . , cn−1 depend only on z and on the coefﬁcients of the
polynomial Pn. Using (1.14), (1.15), pp. 4, 5, it is easy to verify that
| f (˜x) −f (x)| ≤|Pn(z + h) −Pn(z)| ≤L(|h| + |h|2 + · · · + |h|n),
(1.33)
where L depends only on |z| and on the modulus of the coefﬁcients of the polyno-
mial Pn. The right-hand side of inequality (1.33) is less than any preassigned positive
number if the distance |h| between points ˜x and x is small enough. This means that
the function f is continuous.
Suppose that f (0) = |a0| > 0. Otherwise, zero is a root of the polynomial.
Let BR be the open disk of radius R centered at the origin. Denote by SR the circle
that constitutes the boundary of BR. Take x ∈SR. Write f (x) in the form
f (x) = |zn −(−an−1zn−1 −· · · −a0)|.
Using (1.16), p.5, we get
f (x) ≥|z|n −|an−1||z|n−1 −· · · −|a0| = Rn −|an−1|Rn−1 −· · · −|a0|
= Rn(1 −|an−1|R−1 −· · · −|a0|R−n).
The right-hand side of the last inequality tends to inﬁnity as R →∞. Therefore, if
R is big enough, then
f (x) ≥2 f (0) for all
x ∈SR.
(1.34)

1.1 Complex Numbers and Polynomials
13
As we have seen, the function f is continuous on the whole plane. Hence, by the
extreme value theorem, in the closure of BR there exists a point y where the function f
attains its minimum. Evidently, f (y) ≤f (0), and using (1.34), we see that y /∈SR,
i.e., y is an interior point of BR. We assume that f (y) > 0. Otherwise, α = y1 + iy2
is a root of the polynomial Pn.
Let h = h1 + ih2. If |h| is small enough, then ˜y = (y1 + h1, y2 + h2) ∈BR. By
deﬁnition, f ( ˜y) = |Pn(α + h)|. Using (1.32), we get
Pn(α + h) = Pn(α) + c1h + c2h2 + · · · + hn,
where the coefﬁcients c1, . . . , cn−1 depend only on α and on the coefﬁcients of the
polynomial Pn. Since by assumption Pn(α) ̸= 0, we can write
Pn(α + h)
Pn(α)
= 1 + d1h + · · · + dnhn.
Among the numbers d1, . . . , dn, at least the number dn is not equal to zero. Suppose
that dk ̸= 0 and d j = 0, j = 1, . . . , k −1. Then for every c ̸= 0 we have
Pn(α + h)
Pn(α)
= 1 + dk
ck (ch)k + dk+1
ck+1 (ch)k+1 + · · · + dn
cn (ch)n.
(1.35)
Choose the number c such that ck = −dk (see p.6) and put v = ch. Then
f ( ˜y)
f (y) = |Pn(α + h)|
|Pn(α)|
= |1 −vk + vkb(v)|,
where
b(v) = dk+1
ck+1 v + · · · + dn
cn vn−k.
Choose now the number h such that 0 < v < 1 and |b(v)| ≤1/2. For this v, evidently
f ( ˜y)
f (y) ≤1 −vk
2 < 1,
but that is impossible, since in the closure of BR the function f attains its minimum
value at the point y. Thus we have a contradiction. Therefore, f (y) = 0, i.e., the
number α = y1 + iy2 is a root of the polynomial Pn.
□
Theorem 1.4 Each polynomial of degree n ≥1 has n roots (counted with their
multiplicities).

14
1
Preliminaries
Proof Let Pn(z) = zn + an−1zn−1 + · · · + a0, n ≥1. By the fundamental theorem
of algebra, the polynomial Pn has a root. Denote this root by α1, and suppose that its
multiplicity is equal to k1 ≥1. Then
Pn(z) = (z −α1)k1qn−k1(z).
If k1 = n, then evidently, qn−k1 = 1. Otherwise, the polynomial qn−k1(z) has a
root. Denote it by α2. Clearly, the number α2 is a root of the polynomial Pn, and by
construction, α2 ̸= α1. Suppose that the multiplicity of α2 as a root of the polyno-
mial qn−k1 is equal to k2. Then
qn−k1(z) = (z −α2)k2qn−k1−k2(z),
and hence
Pn(z) = (z −α1)k1(z −α2)k2qn−k1−k2(z).
Obviously, the number k2 is the multiplicity of α2 as the root of the polynomial Pn.
Continuing this process, we get
Pn(z) = (z −α1)k1(z −α2)k2 · · · (z −αm)km,
(1.36)
where the integers k1, k2, . . . , km are greater than or equal to one, and
k1 + k2 + · · · + km = n.
□
Theorem 1.5 No polynomial Pn of degree n ≥1 can have more than n roots.
Proof Indeed, let Pn(α) = 0, and suppose that the root α is not equal to any roots
α1, α2, . . . , αm,whichweredeﬁnedintheproofoftheprevioustheorem.Bythecorol-
lary of Bézout’s theorem, we have Pn(z) = (z −α)qn−1(z). Therefore, using (1.36),
we get
(z −α1)k1(z −α2)k2 · · · (z −αm)km = (z −α)qn−1(z).
For z = α, the right-hand side of the last equality is equal to zero, but the left-hand
side is not equal to zero. This contradiction means that the polynomial Pn has only
the roots α1, α2, . . . , αm.
□
1.1.4
Viète’s Formulas
Let Pn(z) = a0 + a1z + a2z2 + · · · + anzn be a polynomial of degree n ≥1. Sup-
pose that the polynomial Pn has roots α1, α2, . . . , αm of multiplicities k1, k2, . . . , km,

1.1 Complex Numbers and Polynomials
15
respectively, and k1 + k2 + · · · + km = n. Using the results of Section1.1.3, we can
write the polynomial Pn in the form
Pn(z) = A(z −α1)k1(z −α2)k2 · · · (z −αm)km,
where A is a constant.
Let now Pn be a normalized polynomial of degree n ≥1. Let us numerate the
roots of Pn by integers 1, 2, …, n, repeating each root according to its multiplicity,
and write (1.36) in the form
Pn(z) = (z −α1)(z −α2) · · · (z −αn).
Removing parentheses, collecting all coefﬁcients with the same power of z on the
right-hand side of the last equality, and equating them with the corresponding coef-
ﬁcients of the polynomial Pn, we get the following formulas, which relate the coef-
ﬁcients of Pn to sums and products of its roots:
an−1 = −(α1 + α2 + · · · + αn),
an−2 = α1α2 + α1α3 + · · · + αn−1αn,
. . . . . . . . . . . . . . . . . . . . .
a0 = (−1)nα1α2 · · · αn.
The rule of construction of these formulas is obvious: the number of factors increases
by one per line, and then all the possible products of different factors are added up
in each line. They are called Viète’s formulas.5
1.1.5
Polynomials with Real Coefﬁcients
Suppose that all coefﬁcients of a polynomial Pn(z) = zn + an−1zn−1 + · · · + a0 of
degree n ≥1 are real, and α is a root of Pn. Then the complex conjugate α is also a
root of Pn. Indeed, if Pn(α)=0, then Pn(α)=0. Further, using (1.12), p. 4, we see
that Pn(α) = αn + an−1αn−1 + · · · + a0 = Pn(α).
Assume now that α1, α2, …, αs are all the real roots of the polynomial Pn. Denote
by k1, k2, …, ks their multiplicities and put
r = k1 + k2 + · · · + ks,
Qr(z) = (z −α1)k1(z −α2)k2 · · · (z −αs)ks.
5François Viète (1540–1603) was a French mathematician.

16
1
Preliminaries
Then
Pn(z) = Qr(z)Rn−r(z).
(1.37)
Evidently, all coefﬁcients of the polynomial Qr are real, and therefore all coefﬁcients
of the polynomial Rn−r are also real (see the remark on p. 10). By construction, the
polynomial Rn−r can have only complex roots. Note that for all z and α we have
(z −α)(z −α) = z2 + pz + q,
where p = −α −α = −2 Re α, q = αα = |α|2 are real numbers. Hence if α is a
complex root of the polynomial Pn, and therefore is a root of Rn−r, then using (1.37),
we obtain
Pn(z) = Qr(z)(z2 + pz + q)Rn−r−2(z).
Since the numbers p and q are real, the polynomial Rn−r−2 has only real coefﬁcients.
Continuing this process, we see that
Pn(z) = (z−α1)k1(z−α2)k2 · · · (z−αs)ks(z2+ p1z+q1) · · · (z2+ ptz+qt). (1.38)
Here s is the number of distinct real roots of the polynomial Pn, and t is the number
of all pairs of complex conjugate roots of Pn.
From (1.38) it follows immediately that every polynomial with real coefﬁcients
of odd degree has at least one real root.
Assuming that z in equality (1.38) is real, we see that every polynomial with real
coefﬁcients can be written in the form of a product of real linear and quadratic factors.
For example, it is easy to see that the number α = −3 is a root of the polynomial
P3(z) = a3z3 + a2z2 + a1z + a0 = z3 −6z + 9.
Let us divide P3(z) by Q1(z) = z + b0 = z + 3, i.e., let us calculate a polynomial
q2(z) = c2z2 + c1z + c0
such that P3(z) = Q1(z)q2(z). We perform calculations by Horner’s rule and see
that q2(z) = z2 −3z + 3. The remainder is equal to zero, since z + 3 divides the
polynomial P3(z):
P3(z) = (z + 3)

z2 −3z + 3

.
Clearly, the number α = −3 is not a root of the polynomial q2(z). Hence α is a
simple root of the polynomial P3(z). To ﬁnd the other two roots we have to solve the
quadratic equation z2 −3z +3 = 0. The discriminant of this equation is equal to −3,
and therefore, it does not have real roots. Thus we have written the polynomial P3(z)
of degree three with real coefﬁcients in the form of a product of its real linear and
quadratic factors.

1.2 Systems of Linear Equations, Matrices, Determinants
17
1.2
Systems of Linear Equations, Matrices, Determinants
1.2.1
Permutations
Let us consider the set of n integers Mn = {1, 2, 3, . . . , n}. We can arrange these
integers in different orders. Each arrangement of Mn in some deﬁnite order is called
a permutation of Mn. For example, the following permutations exist:
1, 2, 3, . . . , n
(1.39)
and
2, 1, 3, . . . , n.
(1.40)
In general, a permutation of Mn can be written in the form
n1, n2, . . . , nn,
(1.41)
where n1, n2, . . . , nn are the integers 1, 2, 3, . . . , n in some order.
The number of distinct permutations of the set Mn is usually denoted by Pn. Let us
prove by induction that Pn is equal to the product of the ﬁrst n integers: 1×2×3 · · · n,
which is written n! and referred to as “n factorial.” We obviously have P1 = 1! and
P2 = 2!. Suppose that Pn−1 = (n −1)!. Take a permutation of Mn−1 and unite it
with the element n. We can put n in the ﬁrst place, in the second place, and so on.
The last possible place for the element n is the nth. Hence, using each permutation
of Mn−1, we can construct n permutations of Mn. Since by the induction hypothesis
we have Pn−1 = (n −1)!, we see that
Pn = n!.
(1.42)
We say that two elements ni and n j in the permutation (1.41) form an inversion if
i < j but ni > n j. For example, the permutation (1.39) has no inversions, while the
permutation (1.40) involves a single inversion, formed by the elements n1 and n2.
The number of inversions in a given permutation is called the signature of the
permutation and is denoted by σ(n1, n2, . . . , nn).
A permutation is called even or odd according to whether the signature of the
permutation is even or odd. For example, the permutation (1.39) is even, and the
permutation (1.40) is odd.
If any two elements in a permutation are interchanged, we say that a transposition
has been made in the permutation. Every transposition is uniquely determined by
the serial numbers of the two interchanged elements. For example, the permutation
(1.40) is transformed from the permutation (1.39) by the transposition (1, 2).

18
1
Preliminaries
Theorem 1.6 A transposition changes the parity of a permutation.
Proof It is enough to check that the numbers of inversions in the permutations
n1, n2, . . . , ni−1, ni, ni+1, . . . , n j−1, n j, n j+1, . . . , nn,
(1.43)
n1, n2, . . . , ni−1, n j, ni+1, . . . , n j−1, ni, n j+1, . . . , nn
(1.44)
differ by an odd number. Let us introduce the sets
B1 = {n1, n2, . . . , ni−1}, B2 = {ni+1, . . . , n j−1}, B3 = {n j+1, . . . , nn},
and denote by B+
ks (B−
ks) the number of elements in the set Bk that are greater (less)
than ns, s = i, j. Clearly, B+
ks + B−
ks = card(Bk) for all k = 1, 2, 3 and s = i, j.
Here card(Bk) is the number of elements in the set Bk. The transposition (i, j)
described in (1.43), (1.44) changes only the pairs that include ni or n j. Hence, it
is enough to calculate the numbers of inversions in the permutations (1.43), (1.44)
corresponding to the pairs that include ni or n j. Evidently, the number of such
inversions in the permutation (1.43) is equal to
BI = B+
1i + B−
2i + B−
3i + B+
1 j + B+
2 j + B−
3 j + I (ni, n j),
where I (ni, n j) is the number of inversions in the pair ni, n j, and for the permuta-
tion (1.44), this number is equal to
BI I = B+
1 j + B−
2 j + B−
3 j + B+
1i + B+
2i + B−
3i + I (n j, ni).
Obviously,
BI −BI I = B−
2i −B+
2i + B+
2 j −B−
2 j + I (ni, n j) −I (n j, ni)
= B−
2i −B+
2i + B+
2 j −B−
2 j ± 1 = B−
2i −B+
2i + B+
2 j −B−
2 j ± 2(B+
2i + B−
2 j) ± 1
= B−
2i + B+
2i + B+
2 j + B−
2 j −2(B+
2i + B−
2 j) ± 1
= 2card(B2) −2(B+
2i + B−
2 j) ± 1.
Thus the number BI −BI I is odd.
□
Theorem 1.7 For every natural number n, the number of all even permutations of
the set Mn = {1, 2, 3, . . . , n} is equal to the number of all odd permutations of Mn.
Proof It follows from Theorem 1.6 that by the operation of transposition, each even
permutation is transformed to an odd permutation. The converse is also true. There-
fore, there is a one-to-one correspondence between the set of all even permutations
and the set of all odd permutations of Mn. These two sets are ﬁnite, and thus the
numbers of even and odd permutations are equal.
□

1.2 Systems of Linear Equations, Matrices, Determinants
19
1.2.2
Determinants and Their Basic Properties
A square matrix of order n is a square array consisting of n rows and n columns:
A =
⎛
⎜⎜⎝
a11 a12 . . . a1n
a21 a22 . . . a2n
. . . . . .
an1 an2 . . . ann
⎞
⎟⎟⎠.
(1.45)
Here ai j, i, j = 1, 2, . . . , n, are (generally speaking) complex numbers.
The determinant of the matrix A is the number
|A| =

n1n2...nn
(−1)σ(n1,n2,...,nn)a1n1a2n2 · · · annn.
(1.46)
We also use the following notation:
|A| = det(A) =  =

a11 a12 . . . a1n
a21 a22 . . . a2n
. . . . . .
an1 an2 . . . ann

.
(1.47)
Observe that the determinant of a matrix A of order n is the sum of n! summands.
The summands are all possible products of n elements of the matrix A such that
each term in (1.46) contains one element from each row and one element from each
column of the matrix A. Each product enters into the determinant with a + sign if
the permutation n1, n2, . . . , nn is even and with a −sign if it is odd.
Using Theorem 1.7 we see that the number of terms in (1.46) with a + sign is
equal to the number of terms with a −sign.
We say that the elements a1n1, a2n2, . . . , annn form the diagonal of the matrix A.
The diagonal is called even if the permutation n1, n2, . . . , nn is even and odd if this
permutation is odd.
It is useful to note that

a11 a12
a21 a22
 = a11a22 −a12a21,
(1.48)

a11 a12 a13
a21 a22 a23
a31 a32 a33

= a11a22a33 + a12a23a31+a13a21a32
−a13a22a31 −a12a21a33 −a11a23a32.
The reader can prove the following equality using only the deﬁnition of the deter-
minant:

20
1
Preliminaries

1
0 . . . 0
a21 a22 . . . a2n
. . . . . .
an1 an2 . . . ann

=

a22 a23 . . . a2n
a32 a33 . . . a3n
. . . . . .
an2 an3 . . . ann

.
(1.49)
Note that the left-hand side of (1.49) contains a determinant of a matrix of order n,
and the right-hand side contains a determinant of a matrix of order n −1.
Denote by ak the kth row of the matrix A, i.e., ak = (ak1, ak2, . . . , akn). The sum
of two rows f = ( f1, f2, . . . , fn) and g = (g1, g2, . . . , gn), by deﬁnition, is the row
f + g = ( f1 + g1, f2 + g2, . . . , fn + gn).
The product of a number α and a row f is the row
αf = (αf1, αf2, . . . , αfn).
If all elements of a row are equal to zero, then we say that this row is zero and
write 0 = (0, 0, . . . , 0).
Often it is useful to consider the determinant to be a function of its rows, i.e., we
may write  = (a1, a2, . . . , an). Similarly, we can consider the determinant to be
a function of its columns.
Let us formulate and prove the basic properties of determinants.
1. If all elements of a row (or a column) of a determinant |A| are equal to zero,
then the determinant is equal to zero. The proof immediately follows from the fact
that in this case, each diagonal of the matrix A contains a zero element.
2. The determinant is a linear function of each row (or column) separately with
the others ﬁxed, namely,
(a1, a2, . . . , ak + bk, . . . , an)
= (a1, a2, . . . , ak, . . . , an) + (a1, a2, . . . , bk, . . . , an),
(a1, a2, . . . , αak, . . . , an) = α(a1, a2, . . . , ak, . . . , an),
where α is a number. This statement immediately follows from formula (1.46).
3. If two rows (or two columns) of a determinant |A| are identical, then the
determinant vanishes. Suppose that row ai is equal to row a j, i < j. We can represent
the set of all diagonals of the matrix A as the union of all pairs of the form
a1n1, a2n2, . . . , aini , . . . , a jn j , . . . , annn,
(1.50)
a1n1, a2n2, . . . , ain j , . . . , a jni , . . . , annn.
(1.51)

1.2 Systems of Linear Equations, Matrices, Determinants
21
The diagonals (1.50), (1.51) have opposite parity, since the corresponding permuta-
tions have the form (1.43), (1.44), and each of them is transformed into the other by
the transposition (i, j). The product of all elements in the diagonal (1.50) is equal
to the product for the diagonal (1.51), since by assumption, aini = a jni, ain j = a jn j.
Therefore, in (1.46), the sum of each pair of terms corresponding to diagonals (1.50)
and (1.51) is equal to zero. Thus |A| = 0. In the same way we can prove that if two
columns of a determinant are identical, then the determinant is equal to zero.
4. If two rows (or columns) of a determinant  are interchanged, then the resulting
determinant has the value −. To simplify the notation, we prove the statement for
the rows a1 and a2. The proof for any other rows is analogous. By Property 3, we
have (a1 + a2, a1 + a2, a3, . . . , an) = 0. On the other hand, using Properties 2
and 3, we see that
0 = (a1 + a2, a1 + a2, a3, . . . , an)
= (a1, a1 + a2, a3, . . . , an) + (a2, a1 + a2, a3, . . . , an)
= (a1, a1, a3, . . . , an) + (a1, a2, a3, . . . , an)
+ (a2, a1, a3, . . . , an) + (a2, a2, a3, . . . , an)
= (a1, a2, a3, . . . , an) + (a2, a1, a3, . . . , an),
i.e., (a1, a2, a3, . . . , an) = −(a2, a1, a3, . . . , an).
5. The value of a determinant remains unchanged if to any row is added the product
of any number and another row. The same is true for columns. Let us prove as above
the statement for the ﬁrst two rows. Then by Properties 2 and 3, we have
(a1 + αa2, a2, a3, . . . , an)
= (a1, a2, a3, . . . , an) + α(a2, a2, a3, . . . , an)
= (a1, a2, a3, . . . , an).
6. Now we prove the formulas of expansion of determinants in terms of rows and
columns, which is often used in calculations. Let us introduce the unit row:
ik = (0, . . . , 0
  
k−1
, 1, 0, . . . , 0
  
n−k
).
Using Property 2, we easily get
(a1, a2, a3, . . . , an) =
n

k=1
a1k(ik, a2, a3, . . . , an).
(1.52)

22
1
Preliminaries
The determinant (ik, a2, a3, . . . , an) is called the cofactor of the element a1k in
the determinant (a1, a2, a3, . . . , an) and is denoted by A1k. Using this notation, we
write (1.52) in the form
(a1, a2, a3, . . . , an) =
n

k=1
a1k A1k.
For the same reason,
(a1, a2, a3, . . . , an) =
n

k=1
aik Aik,
i = 1, 2, . . . , n.
(1.53)
Here Aik is the cofactor of aik, i.e., the determinant obtained when the row ai in
the determinant (a1, a2, a3, . . . , an) is replaced by the unit row ik. Formula (1.53)
is known as the expansion of the determinant (a1, a2, a3, . . . , an) in terms of the
row ai.
Note that
n
k=1
aik Alk = 0 for l ̸= i. Indeed, the left-hand side of the last equality is
the expansion of the determinant with ai = al, but we know that such a determinant
is equal to zero. Combining this equality with (1.53), we get
n

k=1
aik Alk = |A|δil,
i, l = 1, 2, . . . , n,
(1.54)
where
δil =
 0, i ̸= l,
1, i = l,
(1.55)
is the Kronecker delta.6
Often it is more convenient to write the determinant Alk in another form. The
reader can easily prove that Alk = (−1)k+l Mlk, where Alk is the cofactor and Mlk is
the determinant obtained from the determinant |A| by deleting the lth row and kth
column (hint: rearranging rows and columns, write Alk in the form of the determinant
on the left-hand side of Eq.(1.49)). The determinant Mlk is called the minor of the
element alk.
Note the following formula for the expansion of the determinant in terms of its
columns:
n

k=1
aki Akl = |A|δil,
i, l = 1, 2, . . . , n.
(1.56)
6Leopold Kronecker (1823–1891) was a German mathematician.

1.2 Systems of Linear Equations, Matrices, Determinants
23
As an example, let us calculate the ﬁfth-order determinant
 =

−2
5
0 −1
3
1
0
3
7 −2
3 −1
0
5 −5
2
6 −4
1
2
0 −3 −1
2
3

.
First we zero out the third column except for the last entry. To do this, we multiply
the last row by three and add the result to the second row. After that, we multiply the
last row by four and subtract the result from the fourth row. As a result, we get
 =

−2
5
0 −1
3
1 −9
0 13
7
3 −1
0
5
−5
2 18
0 −7 −10
0 −3 −1
2
3

.
Expanding this determinant in terms of the third column, we obtain
 = (−1)3+5(−1)

−2
5 −1
3
1 −9 13
7
3 −1
5 −5
2 18 −7 −10

.
Now we zero out the ﬁrst column except for the second element. To do this, we
multiply the second row by two and add the result to the ﬁrst row. After that, we
multiply the second row by three and subtract the result from the third row. And
ﬁnally, we multiply the second row by two and subtract the result from the last row.
As a result, we get
 = −

0 −13
25
17
1 −9
13
7
0
26 −34 −26
0
36 −33 −24

.
Expanding this determinant in terms of the ﬁrst column, we obtain
 = −(−1)2+1

−13
25
17
26 −34 −26
36 −33 −24

=

−13
25
17
26 −34 −26
36 −33 −24

.
Let us calculate this third-order determinant, expanding it in terms of the third row:

24
1
Preliminaries
 = 36

25
17
−34 −26
 −(−33)

−13
17
26 −26
 + (−24)

−13
25
26 −34

= 36(−72) −(−33)(−104) + (−24)(−208) = −1032.
7. The matrix
AT =
⎛
⎜⎜⎝
a11 a21 . . . an1
a12 a22 . . . an2
. . . . . .
a1n a2n . . . ann
⎞
⎟⎟⎠
(1.57)
is called the transpose of the matrix A. Observe that in (1.57) we write the columns
of A as the rows of AT .
The determinants of matrices A and AT are equal.
Let us prove this statement by induction on the order of the determinant. For a
second-order determinant the statement is evidently true. We shall suppose that this
equality holds for every determinant of order n −1 and prove it for an arbitrary
determinant |A| of order n. Expanding |A| in terms of the ﬁrst row, we get
|A| = a11M11 −a12M12 + · · · + (−1)n+1a1nM1n.
(1.58)
Expanding the determinant |AT | along the ﬁrst column, we obtain
|AT | = a11MT
11 −a12MT
21 + · · · + (−1)n+1a1nMT
n1.
(1.59)
Here MT
i j is the minor of the element in the position i, j in the determinant |AT |. By
the inductive assumption, MT
i j = M ji. Thus |AT | = |A|.
8. We say that the rows of a matrix A are linearly dependent if there exist scalars
α1, α2, . . . , αn, not all zero, such that
α1a1 + α2a2 + · · · + αnan = 0.
(1.60)
In the contrary case, i.e., when (1.60) implies α1 = · · · = αn = 0, the rows are called
linearly independent. Evidently, if the rows of a matrix are linearly independent, then
none of them are equal to zero. The concept of linear dependence of the columns of
a matrix is introduced similarly.
The determinant of a matrix A is equal to zero if and only if the rows of the
matrix A are linearly dependent. Let us prove this statement.
Suppose that the rows of A are linearly dependent, and for deﬁniteness suppose
that equality (1.60) holds for α1 ̸= 0. Using the property of linearity of the determi-
nant in the ﬁrst row, and then using Properties 5 and 1, we get
α1(a1, a2, . . . , an) = (α1a1,a2, . . . , an)
= (α1a1 + α2a2 + · · · + αnan, a2, . . . , an) = 0.

1.2 Systems of Linear Equations, Matrices, Determinants
25
Thus (a1, a2, . . . , an) = 0, since α1 ̸= 0.
Let us prove the converse statement, namely, that if the rows of a matrix are
linearly independent, then the determinant of the matrix is not equal to zero. Suppose
that |A| = 0. Let us consider all determinants of order n −1 obtained from the
determinant |A| by deleting a row and a column. If each of them is equal to zero,
then we consider the determinants of order n−2, and so on. Finally, either all elements
of the matrix A are equal to zero (then the assertion is obviously true) or there is a
nonzero determinant of order k ≥1 obtained from the determinant |A| by deleting
some n−k rows and some n−k columns, and all determinants of order greater than k
are equal to zero. Denote this determinant by dk. Since after interchanging rows and
columns of a determinant only its sign is changed, we can assume without loss of
generality that dk consists of elements of the ﬁrst k rows and the ﬁrst k columns of
the matrix A.
Consider the determinant dk+1, which consists of the elements of the ﬁrst k + 1
rows and the ﬁrst k+1 columns of the matrix A. By assumption, dk+1 = 0. Expanding
dk+1 in terms of the last column, we get
α1a1k+1 + α2a2k+1 + · · · + αkakk+1 + dkak+1k+1 = 0.
(1.61)
We emphasize that dk ̸= 0, and the numbers α1, α2, …, αk are cofactors of corre-
sponding elements of the last column of the determinant dk+1.
By interchanging columns in the determinant |A| we can construct the last column
of the determinant dk+1 using the following elements:
a1 j, a2 j, . . . , akj, ak+1 j,
j = k + 2, k + 3, . . . , n.
By assumption, each of the constructed determinants dk+1 is equal to zero. Expanding
each of them in terms of its last column, we get
α1a1 j + α2a2 j + · · · + αkakj + dkak+1 j = 0,
j = k + 2, k + 3, . . . , n.
Finally, if we replace the (k +1)th column of the determinant |A| with its column
number j ≤k, then we obtain the zero determinant (since the two columns of this
determinant are equal). For the same reason we have dk+1 = 0. Expanding each dk+1
thus constructed in terms of its last column, we get
α1a1 j + α2a2 j + · · · + αkakj + dkak+1 j = 0,
j = 1, 2, . . . , k.
Thus
α1a1 j + α2a2 j + · · · + αkakj + dkak+1 j + 0 × ak+2 j + · · · + 0 × anj = 0,
where j = 1, 2, . . . , n; dk ̸= 0, i.e., the rows of the matrix A are linearly dependent.

26
1
Preliminaries
Remark 1.2 Since |AT | = |A|, it is clear that the determinant of a matrix A is equal
to zero if and only if the columns of the matrix A are linearly dependent.
Below are two examples of calculation of determinants that are often used in
applications.
1. The determinant of a triangular matrix. A matrix A is called upper triangular
if ai j = 0 for i > j. A matrix A is called lower triangular if ai j = 0 for i < j.
If a matrix A is triangular, then
|A| = a11a22 · · · ann.
(1.62)
Let us prove this statement for an upper triangular matrix. Then (1.62) holds also
for each lower triangular matrix A, since |A| = |AT |, and AT is an upper triangular
matrix.
For determinants of order one and two, equality (1.62) is evidently true. Let us
check (1.62) by induction on the order of the determinant. Suppose that (1.62) holds
for each determinant of order n −1 and consider the following determinant:
|A| =

a11
0
0
. . .
0
a12
a22
0
. . .
0
a13
a23
a33
. . .
0
. . .
. . .
. . .
. . .
. . .
a1n
a2n
a3n
. . .
ann

.
Expanding |A| in terms of the ﬁrst column, we get
|A| = a11

a22
0
. . .
0
a23
a33
. . .
0
. . .
. . .
. . .
. . .
a2n
a3n
. . .
ann

.
The determinant on the right-hand side in the last equality has order n −1, and by
the inductive assumption it is equal to a22a33 · · · ann. Thus (1.62) holds.
2.The Vandermonde7 determinant of order n is the determinant deﬁned as follows:
d =

1
1
1
. . .
1
a1
a2
a3
. . .
an
a2
1
a2
2
a2
3
. . .
a2
n
. . .
. . .
. . .
. . .
. . .
an−1
1
an−1
2
an−1
3
. . .
an−1
n

.
Let us prove that for every n ≥2, the Vandermonde determinant is equal to the
product of all possible differences ai −a j with 1 ≤j < i ≤n, namely,
7Alexandre-Théophile Vandermonde (1735–1796) was a French musician and mathematician.

1.2 Systems of Linear Equations, Matrices, Determinants
27
d =

1≤j<i≤n
(ai −a j).
The assertion is obviously true for n = 2. Let us use the method of mathematical
induction. Suppose that the assertion is true for the Vandermonde determinant of
order n −1, i.e.,

1
1
. . .
1
a2
a3
. . . an
. . .
. . . . . . . . .
an−2
2
an−2
3
. . . an−2
n

=

2≤j<i≤n
(ai −a j).
Consider the determinant d. Multiply the penultimate row by a1 and subtract from
the last row; then multiply the (n −2)th row by a1 and subtract from the (n −1)th
row, and so on. As a result, we get
⎛
⎜⎜⎜⎜⎝
1
1
1
. . .
1
0
a2 −a1
a3 −a1
. . .
an −a1
0
a2
2 −a1a2
a2
3 −a1a3
. . .
a2
n −a1an
. . .
. . .
. . .
. . .
. . .
0 an−1
2
−a1an−2
2
an−1
3
−a1an−2
3
. . . an−1
n
−a1an−2
n
⎞
⎟⎟⎟⎟⎠
.
Expanding d in terms of the ﬁrst column, we obtain the following determinant of
order n −1:
d =

a2 −a1
a3 −a1
. . .
an −a1
a2
2 −a1a2
a2
3 −a1a3
. . .
a2
n −a1an
. . .
. . .
. . .
. . .
an−1
2
−a1an−2
2
an−1
3
−a1an−2
3
. . . an−1
n
−a1an−2
n

.
Note that a2 −a1 is a common multiple of all elements of the ﬁrst column, a3 −a1
is a common multiple of all elements of the second column, and so on. Therefore,
d = (a2 −a1) (a3 −a1) . . . (an −a1)

1
1
. . .
1
a2
a3
. . . an
. . .
. . . . . . . . .
an−2
2
an−2
3
. . . an−2
n

,
where the last multiplier is the Vandermonde determinant of order n −1. Thus,
d = (a2 −a1) (a3 −a1) . . . (an −a1)

2≤j<i≤n
(ai −a j) =

1≤j<i≤n
(ai −a j).

28
1
Preliminaries
1.2.3
Cramer’s Rule
In this section we consider systems of linear algebraic equations in which the number
of unknowns is equal to the number of equations:
a11x1 + a12x2 + · · · + a1nxn = b1,
a21x1 + a22x2 + · · · + a2nxn = b2,
....................
an1x1 + an2x2 + · · · + annxn = bn.
(1.63)
The square matrix
A =
⎛
⎜⎜⎝
a11 a12 . . . a1n
a21 a22 . . . a2n
. . . . . . . . . . . .
an1 an2 . . . ann
⎞
⎟⎟⎠,
(1.64)
which consists of the coefﬁcients of these equations, is called the matrix of sys-
tem (1.63). In this section we assume that |A| ̸= 0. In this case, the matrix A is
called nonsingular (if |A| = 0, then the matrix A is called singular). The array of
numbers b1, b2, . . . , bn is called the column of the right-hand side of system (1.63).
If the right-hand side of the system is zero, i.e., bi = 0 for all i = 1, 2, . . . , n, then
system (1.63) is called homogeneous. A homogeneous system of equations always
has a solution. For example, we can take x1, x2, …, xn = 0. This solution is called
trivial.
Theorem 1.8 Every homogeneous system of linear algebraic equations with a
square nonsingular matrix has only the trivial solution.
Proof Assume the contrary. Then there exist scalars x1, x2, …, xn, not all zero, such
that
a11x1 + a12x2 + · · · + a1nxn = 0,
a21x1 + a22x2 + · · · + a2nxn = 0,
...................
an1x1 + an2x2 + · · · + annxn = 0.
(1.65)
This means that the columns of the matrix A are linearly dependent; hence we
get |A| = 0, which is impossible for the nonsingular matrix A. Therefore, our
assumption of the existence of a nontrivial solution of system (1.65) with a nonsin-
gular matrix is incorrect.
□
Remark 1.3 System (1.65) has a nontrivial solution if and only if |A| = 0. This
statement immediately follows from Remark 1.2.
Theorem 1.9 Foragivenright-handside,system (1.63)withanonsingularmatrix A
cannot have two different solutions.

1.2 Systems of Linear Equations, Matrices, Determinants
29
Proof Assume the contrary, and let x1, x2, . . . , xn and y1, y2, . . . , yn be two different
solutions of system (1.63), i.e.,
a11x1 + a12x2 + · · · + a1nxn = b1,
a21x1 + a22x2 + · · · + a2nxn = b2,
....................
an1x1 + an2x2 + · · · + annxn = bn,
(1.66)
and
a11y1 + a12y2 + · · · + a1nyn = b1,
a21y1 + a22y2 + · · · + a2nyn = b2,
....................
an1y1 + an2y2 + · · · + annyn = bn.
(1.67)
Put z1 = x1 −y1, z2 = x2 −y2, …, zn = xn −yn. Subtracting term by term
the corresponding equations in (1.66) and (1.67), we see that z1, z2, …, zn is the
solution of the homogeneous system (1.65). Then by Theorem 1.8, it follows that
z1 = z2 = · · · = zn = 0, i.e., the assumption of the existence of two different
solutions of system (1.63) is incorrect.
□
Theorem 1.10 For every right-hand side, system (1.63) with a nonsingular matrix A
has a solution.
Proof Let us construct the solution of system (1.63) in the form
xi = ci1b1 + ci2b2 + · · · + cinbn,
i = 1, 2, . . . , n.
(1.68)
Here cik, i, k = 1, 2, . . . , n, are unknown coefﬁcients. Substituting (1.68) into the
equations of system (1.63) and collecting on the left-hand side of these equations all
coefﬁcients of the same bi, we get
b1(ai1c11 + ai2c21 + · · · + aincn1)
+b2(ai1c12 + ai2c22 + · · · + aincn2)
+ · · · + bi(ai1c1i + ai2c2i + · · · + aincni)
+ · · · + bn(ai1c1n + ai2c2n + · · · + aincnn) = bi, (1.69)
where i = 1, 2, . . . , n. Clearly, if we can ﬁnd coefﬁcients cik such that the following
conditions hold, then formulas (1.68) will give us a solution of system (1.63):
ai1c1k + ai2c2k + · · · + aincnk = δik,
i, k = 1, 2, . . . , n,
(1.70)
where δik is the Kronecker delta. Comparing (1.70) and (1.54), p. 21, we see that
Eq.(1.70) holds if we put
cik = Aki
|A| ,
i, k = 1, 2, . . . , n.
(1.71)

30
1
Preliminaries
Substituting (1.71) into (1.68), we get the following formulas for the solution of
system (1.63):
xi = (A1ib1 + A2ib2 + · · · + Anibn)/|A|,
i = 1, 2, . . . , n.
(1.72)
Using the expansion of a determinant in terms of a column, we can write (1.72) in
more compact form:
xi = i
 ,
i = 1, 2, . . . , n.
(1.73)
Here  = |A|, i is the determinant obtained when the ith column of |A| is replaced
by the right-hand side of system (1.63).
□
Formulas (1.73) are called Cramer’s8 formulas (or Cramer’s rule). As an example,
we now solve the following system of equations using Cramer’s rule:
x1 + x2 + x3 = −2,
x1 + 3x2 −2x4 = −4,
2x1 + x3 −x4 = −1,
2x2 −x3 −3x4 = −3.
Let us calculate the corresponding determinants:
 =

1 1
1
0
1 3
0 −2
2 0
1 −1
0 2 −1 −3

= 4,
1 =

−2 1
1
0
−4 3
0 −2
−1 0
1 −1
−3 2 −1 −3

= 4,
2 =

1 −2
1
0
1 −4
0 −2
2 −1
1 −1
0 −3 −1 −3

= −4,
3 =

1 1 −2
0
1 3 −4 −2
2 0 −1 −1
0 2 −3 −3

= −8,
4 =

1 1
1 −2
1 3
0 −4
2 0
1 −1
0 2 −1 −3

= 4.
By Cramer’s rule, we get x1 = 1/ = 1, x2 = 2/ = −1, x3 = 3/ = −2,
and x4 = 4/ = 1.
For numerical computations, Cramer’s formulas are used very rarely. Systems
of linear equations are usually solved numerically by different variants of Gaussian
elimination or by iterative methods (see p. 41 in this chapter and Chapters 8, 12).
As an example of application of Cramer’s rule, let us construct the so-called
Lagrange9 interpolation formula.
Theorem 1.11 Let z0, z1, . . . , zn be distinct numbers, and let h0, h1, . . . , hn be arbi-
trary numbers. Then there exists one and only one polynomial
8Gabriel Cramer (1704–1752) was a Swiss mathematician.
9Joseph-Louis Lagrange (1736–1813) was a French mathematician.

1.2 Systems of Linear Equations, Matrices, Determinants
31
Pn(z) = a0 + a1z + a2z2 + · · · + anzn
such that
Pn(z j) = h j, j = 0, 1, . . . , n.
(1.74)
Proof Thesetofconditions(1.74)isthesystemoflinearequationsforthecoefﬁcients
ofthepolynomial Pn.ThedeterminantofthissystemistheVandermondedeterminant
(seep.25).Obviously,itisnotequaltozero;hencesystem (1.74)hasauniquesolution
for every right-hand side.
□
It is clear now that if a polynomial of order n is equal to zero at least n + 1 points,
then all its coefﬁcients are equal to zero.
It is not hard to construct the polynomial satisfying conditions (1.74) in an explicit
form. Namely, the Lagrange interpolation formula gives the solution of this problem:
Pn(z) = h0Φ0(z) + h1Φ1(z) + · · · + hnΦn(z),
(1.75)
where Φ j is a polynomial of order n satisfying the following conditions:
Φ j(zk) = 0,
k = 0, 1, . . . , j −1, j + 1, . . . , n,
(1.76)
Φ j(z j) = 1,
(1.77)
for j = 0, 1, 2 . . . , n.
As we have seen in Section1.1.3, p. 10, every polynomial is uniquely determined
up to a constant factor by all its roots. Therefore,
Φ j(z) = A j(z −z0)(z −z1) · · · (z −z j−1)(z −z j+1) · · · (z −zn).
Using (1.77), we see that
A j =
1
(z j −z0)(z j −z1) · · · (z j −z j−1)(z j −z j+1) · · · (z j −zn),
i.e.,
Φ j(z) =
(z −z0)(z −z1) · · · (z −z j−1)(z −z j+1) · · · (z −zn)
(z j −z0)(z j −z1) · · · (z j −z j−1)(z j −z j+1) · · · (z j −zn),
where j = 0, 1, 2, . . . , n.

32
1
Preliminaries
1.2.4
Matrices: Basic Operations and Transformations
We have introduced the concept of a square matrix (see p. 18). A rectangular m × n
matrix is a rectangular array consisting of m rows and n columns:
A =
⎛
⎜⎜⎝
a11 a12 . . . a1n
a21 a22 . . . a2n
. . . . . . .
am1 am2 . . . amn
⎞
⎟⎟⎠.
(1.78)
Here ai j, i = 1, 2, . . . , m, j = 1, 2, . . . , n, are (generally speaking) complex num-
bers. Sometimes, the dimensions of the matrix A are indicated explicitly, and we use
the following notation: A(m, n) or Am×n.
In the particular case m = n, we have a square matrix of order n. We denote
the set of all rectangular m × n matrices by Mm,n. We denote the set of all square
matrices of order n by Mn.
There are two more special cases. If m = 1 and n is arbitrary, we have the row
matrix
x = (x1, x2, . . . , xn).
(1.79)
A row matrix is often called a row, and we say that this row has length n. If n = 1
and m is arbitrary, we have the column matrix
x =
⎛
⎜⎜⎜⎝
x1
x2
...
xm
⎞
⎟⎟⎟⎠.
(1.80)
This matrix is also called a column of length m. Note that the second subscript in the
notation for elements of rows and columns is usually not used. Rows and columns
are often called vectors.
Let us describe some special types of square matrices.
The elements a11, a22, . . . , ann of a square matrix A of order n constitute its main
diagonal. A diagonal matrix is a square matrix all of whose elements outside the
main diagonal are equal to zero:
D =
⎛
⎜⎜⎝
d11 0 . . . 0
0 d22 . . . 0
. . . . . .
0
0 . . . dnn
⎞
⎟⎟⎠.
(1.81)
We also denote this matrix by
D = diag(d11, d22, . . . , dnn).

1.2 Systems of Linear Equations, Matrices, Determinants
33
A diagonal matrix in which dii = 1 for i = 1, 2, . . . , n is called the identity
matrix, and is denoted by I:
I =
⎛
⎜⎜⎝
1
0
. . .
0
0
1
. . .
0
. . . . . . .
0
0
. . .
1
⎞
⎟⎟⎠.
(1.82)
A permutation matrix is a matrix obtained from the identity matrix by interchang-
ing the ith and the kth columns, it is denoted by Pik. For example, the following three
matrices are the permutation matrices of order 3:
P12 =
⎛
⎝
0
1
0
1
0
0
0
0
1
⎞
⎠,
P13 =
⎛
⎝
0
0
1
0
1
0
1
0
0
⎞
⎠,
P23 =
⎛
⎝
1
0
0
0
0
1
0
1
0
⎞
⎠.
Let us recall that a lower triangular matrix L is a square matrix all of whose
elements above the main diagonal are equal to zero,
L =
⎛
⎜⎜⎝
l11 0 . . . 0
l21 l22 . . . 0
. . . . . .
ln1 ln2 . . . lnn
⎞
⎟⎟⎠,
(1.83)
and an upper triangular matrix U is a square matrix all of whose elements below
the main diagonal are equal to zero,
U =
⎛
⎜⎜⎝
u11 u12 . . . u1n
0 u22 . . . u2n
. . . . . . .
0
0 . . . unn
⎞
⎟⎟⎠.
(1.84)
A triangular matrix of the form
Lk =
⎛
⎜⎜⎜⎜⎜⎜⎝
1 · · ·
0
0 · · · 0
. . . . . . . .
0 · · · lk,k
0 · · · 0
0 · · · lk+1,k 1 · · · 0
. . . . . . . .
0 · · · ln,k
0 · · · 1
⎞
⎟⎟⎟⎟⎟⎟⎠
(1.85)
is called an elementary lower triangular matrix. Note that this matrix differs from
the identity matrix only in elements of the kth column.
Let us introduce the operations of matrix addition and multiplication of matrices
by scalars.

34
1
Preliminaries
The product of an m × n matrix A by a scalar α is the matrix
αA =
⎛
⎜⎜⎝
αa11 αa12 . . . αa1n
αa21 αa22 . . . αa2n
. . . . . . . . .
αam1 αam2 . . . αamn
⎞
⎟⎟⎠.
Each element of the matrix αA is the product of the corresponding element of the
matrix A by the number α.
The sum of two m × n matrices A and B is the m × n matrix C with the ele-
ments ci j = ai j + bi j. In matrix notation we write this as C = A + B.
The zero matrix is the matrix all of whose elements are equal to zero. It is denoted
by 0.
The reader can easily verify the following properties of the two operations that
we have just introduced:
1. A + 0 = A,
2. (A + B) + C = A + (B + C),
3. A + B = B + A,
4. (α + β)A = αA + β A.
Note that the sum of two upper (lower) triangular matrices is an upper (lower)
triangular matrix.
By deﬁnition, the product of a row x and a column y of the same length n is the
number
(x1, x2, . . . , xn)
⎛
⎜⎜⎜⎝
y1
y2
...
yn
⎞
⎟⎟⎟⎠=
n

k=1
xk yk.
(1.86)
In other words, this product is obtained by multiplying together corresponding ele-
ments in the row x and the column y and then adding the products.
For example,
5 −1 3 1
⎛
⎜⎜⎝
−1
−2
3
4
⎞
⎟⎟⎠= 5 × (−1) + (−1) × (−2) + 3 × 3 + 1 × 4 = 10.
The product Ax of an m × n matrix A by a column x of length n is the column y
of length m with the elements
yi =
n

j=1
ai jx j,
i = 1, 2, . . . , m.

1.2 Systems of Linear Equations, Matrices, Determinants
35
In matrix notation, it is written as follows:
y = Ax.
We can write the same in detail:
⎛
⎜⎜⎜⎝
y1
y2
...
ym
⎞
⎟⎟⎟⎠=
⎛
⎜⎜⎝
a11 a12 . . . a1n
a21 a22 . . . a2n
. . . . . . .
am1 am2 . . . amn
⎞
⎟⎟⎠
⎛
⎜⎜⎜⎝
x1
x2
...
xn
⎞
⎟⎟⎟⎠.
Observe that the ith element of the column y is the product of the ith row of the
matrix A and the column x.
For example,
⎛
⎝
0 −3
1
2
1
5
−4
0 −2
⎞
⎠
⎛
⎝
3
−2
2
⎞
⎠=
⎛
⎝
8
14
−16
⎞
⎠.
It immediately follows from the deﬁnition that for all scalars α, β and vectors x,
y (of suitable length), we have
A(αx + βy) = αAx + β Ay.
(1.87)
Therefore, we say that the operation of matrix-vector multiplication is linear.
The product of a row x of length m by an m × n matrix A is the row y of length n
with the elements
y j =
m

i=1
ai jxi,
j = 1, 2, . . . , n.
In matrix notation, it is written as follows:
y = x A.
We can write the same in detail:
y1, y2, . . . , yn

=
x1, x2, . . . , xm

⎛
⎜⎜⎝
a11 a12 . . . a1n
a21 a22 . . . a2n
. . . . . . .
am1 am2 . . . amn
⎞
⎟⎟⎠.
The jth element of the row y is the product of the row x and the jth column of the
matrix A.

36
1
Preliminaries
For example,
5 1 0 −3
⎛
⎜⎜⎝
2
0
1 −4
3
1
0 −1
⎞
⎟⎟⎠=
11 −1
.
It immediately follows from the deﬁnition that for all scalars α, β and rows x, y
(of suitable length), we have
(αx + βy)A = αx A + βy A.
(1.88)
This means that the operation of multiplication of a row by a matrix is linear.
Using the operations that we have just introduced, we can write the system (1.63)
of n linear equations with n unknowns either in the form
Ax = b,
(1.89)
where A is a given square matrix, b is a given vector, and x is an unknown vector, or
in the form
x AT = b,
(1.90)
where b is a given row and x is an unknown row. The form (1.89) is used more often.
Suppose that A is an m × n matrix and B is an n × p matrix. Then the m × p
matrix C with the elements
ci j =
n

q=1
aiqbqj,
i = 1, 2, . . . , m, j = 1, 2, . . . , p,
is called the product of the matrices A and B, which is written as C = AB, or in
detail as follows:
⎛
⎜⎜⎝
c11 c12 . . . c1p
c21 c22 . . . c2p
. . . . . . .
cm1 cm2 . . . cmp
⎞
⎟⎟⎠=
⎛
⎜⎜⎝
a11 a12 . . . a1n
a21 a22 . . . a2n
. . . . . . .
am1 am2 . . . amn
⎞
⎟⎟⎠
⎛
⎜⎜⎝
b11 b12 . . . b1p
b21 b22 . . . b2p
. . . . . .
bn1 bn2 . . . bnp
⎞
⎟⎟⎠.
It is useful to observe that each column in the matrix C is calculated as the product
of the matrix A by the corresponding column of the matrix B. Similarly, each row in
the matrix C is calculated as the product of the corresponding row of the matrix A
by the matrix B. Note also that each element ci j is the product of the ith row of the
matrix A and the jth column of the matrix B.

1.2 Systems of Linear Equations, Matrices, Determinants
37
For instance,
5 −1
3 1
2
0 −1 4

⎛
⎜⎜⎝
−1 3
0
−2 1
1
3 0 −2
4 1
2
⎞
⎟⎟⎠=
10 15 −5
11 10 10

.
The matrix product depends on the order of the factors. For example,
1 2
3 2
 1 2
1 1

=
3 4
5 8

,
1 2
1 1
 1 2
3 2

=
7 6
4 4

.
Two matrices A, B commute (with each other), or are commuting matrices, if
AB = B A.
Commuting matrices exist. For example,
 7 −12
−4
7
 26 45
15 26

=
26 45
15 26
  7 −12
−4
7

=
2 3
1 2

.
For every square matrix A, we have
AI = I A = A.
Let us verify the following properties of the operation of matrix multiplication:
1. (A + B)C = AC + BC,
2. C(A + B) = C A + C B,
3. A(BC) = (AB)C.
Clearly, the dimensions of these matrices are matched such that all operations here
make sense.
It is easy to see that Properties 1 and 2 follow from (1.88), (1.87), respectively.
To prove Property 3, note that each element of the matrix D = A(BC) is a number
of the form di j = ai(Bc j), where ai is the ith row of the matrix A, and c j is
the jth column of the matrix C. The elements of the matrix F = (AB)C are the
numbers fi j = (ai B)c j. Therefore, it is enough to prove that x(By) = (x B)y
for each row x and column y (obviously, their lengths have to correspond to the
dimensions of the matrix B). Suppose that the matrix B has m rows and n columns.
By elementary calculations, we get

38
1
Preliminaries
x(By) =
m

i=1
xi
n

j=1
bi j y j =
m

i=1
n

j=1
bi jxiy j,
(1.91)
and similarly,
(x B)y =
n

j=1
y j
m

i=1
bi jxi =
n

j=1
m

i=1
bi jxiy j.
(1.92)
The sums (1.91), (1.92) differ only in the order of their summands, and hence are
equal.
The proof of the following statements is left to the reader.
1. Let Pik be a permutation matrix. Then the vector Pikx is obtained from the vector x
by interchanging the elements xi and xk.
2. The matrix Pik A is obtained from the matrix A by interchanging the ith and kth
rows. Hint: this is a consequence of the previous statement.
3. If matrices L, M are lower triangular, then the matrix LM is lower triangular,
and the same is true for upper triangular matrices.
4. Every lower triangular matrix L can be represented in the form of the product of
the elementary lower triangular matrices Lk, namely,
L = L1L2 · · · Ln−1Ln.
(1.93)
Hint: make calculations according to the following placement of parentheses:
L = L1(L2 · · · (Ln−2(Ln−1Ln) · · · ),
i.e., ﬁrst premultiply Ln by Ln−1, then premultiply the product by Ln−2, and so on.
5. For every square matrix A, the following equalities hold:
det(Pik A) = det Pik det A = −det A.
(1.94)
Let us prove that for each square matrix A and elementary lower triangular matrix
Lk, the following equality holds:
det(Lk A) = lkk det A.
(1.95)

1.2 Systems of Linear Equations, Matrices, Determinants
39
Indeed, let a = (a1, a2, . . . , an) be a vector. By elementary calculations we get
Lka =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
a1
a2
...
ak−1
lk,kak
lk+1,kak + ak+1
lk+2,kak + ak+2
...
ln,kak + an
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
Each column of the matrix Lk A has this form. Therefore, we can factor out from the
kth row of det(Lk A) the common factor lkk. Then we can see that the determinant
thus obtained is equal to det A if we multiply its kth row by l jk and subtract the result
from the jth row for j = k + 1, k + 2, . . . , n. As a result, we get (1.95).
Now using (1.95), (1.93), and (1.62), the reader can easily verify the equality
det(L A) = det L det A
(1.96)
for every square matrix A and lower triangular matrix L, and also the analogous
equality for every upper triangular matrix R:
det(RA) = det R det A.
(1.97)
Let us discuss the concept of the transpose of a rectangular matrix, whose deﬁni-
tion is analogous to the deﬁnition of transpose of a square matrix (see p. 23).
Clearly, the transpose of an m × n matrix is an n × m matrix. For instance, the
transpose of a row matrix is a column matrix, and conversely.
The basic properties of the operation of transposition of rectangular matrices are
the following.
1. For every matrix A we have (AT )T = A.
2. For all numbers α, β and matrices A, B of the same dimensions, we have (αA +
βB)T = αAT + βBT . This means that the operation of transposition is linear.
3. If the product AB is deﬁned, then (a) the product BT AT is also deﬁned, and (b)
the following equality is true:
(AB)T = BT AT .
All properties except 3(b) follow immediately from the deﬁnition (prove them!).
Let us prove Property 3(b). The (i, j)th element of the matrix (AB)T is the
product of the jth row of the matrix A and the ith column of the matrix B. The
(i, j)th element of the matrix BT AT is the product of the ith row of the matrix BT

40
1
Preliminaries
and the jth column of the matrix AT . The ith row of the matrix BT is equal to the
ith column of the matrix B, and the jth column of the matrix AT is equal to the jth
row of the matrix A. This completes the proof of Property 3(b).
We shall next investigate the operation of inversion of square matrices using
results of Section1.2.3, p. 27. Let us recall that if |A| ̸= 0, then the matrix A is
called nonsingular, and if |A| = 0, then the matrix A is called singular. We prove the
following two statements using Remark 1.3.
If the square matrices A, B are both nonsingular, then the matrix C = AB is
nonsingular. To prove this, it is enough to show that the homogeneous system of
linear equations
ABx = 0
(1.98)
has only the trivial solution. Indeed, Bx = 0, since A is nonsingular, and hence
x = 0, since B is nonsingular.
If one of the square matrices A, B is singular, then the matrix C = AB is also
singular. Indeed, in this case it is enough to show that system (1.98) has a nontrivial
solution. Suppose that the matrix B is singular. Then there exists a vector x ̸= 0 such
that Bx = 0; therefore, ABx = 0.
Now let the matrix A be singular and the matrix B nonsingular. Then there exists
a vector y ̸= 0 such that Ay = 0. Since B is nonsingular, the system Bx = y has
a unique solution x. The vector x is not equal to zero, since y ̸= 0. Again we get
ABx = 0 for x ̸= 0.
A matrix X is called a right inverse of the matrix A if
AX = I.
(1.99)
A matrix Y is called a left inverse of the matrix A if
Y A = I.
(1.100)
If the matrix A is singular, then it does not have a right inverse. Indeed, if there
existed a right inverse X, then we would have
det(AX) = det(I) = 1.
On the other hand, det(AX) = 0, since A is singular. It is proved similarly that a
singular matrix does not have a left inverse.
If det(A) ̸= 0, then there exists one and only one right inverse of the matrix A.
Indeed, let us denote by xk the kth column of the matrix X, and by ik the kth column
of the matrix I. Using (1.99), we get the following systems of linear equations:
Axk = ik,
k = 1, 2, . . . , n.
(1.101)
Each of these n systems has a unique solution xk, since the matrix A is nonsingular.
The proof of the existence and uniqueness of a left inverse of the nonsingular matrix A
is similar.

1.2 Systems of Linear Equations, Matrices, Determinants
41
In fact, the left inverse and the right inverse of a nonsingular matrix A are equal
to each other. Indeed, if YA = I, then YAX = X, but AX = I, and hence Y = X.
Thus if (1.99) holds, then the matrix X is called the inverse of A. The inverse
matrix of a matrix A is denoted by A−1, and by deﬁnition,
AA−1 = I.
Let us write the inverse matrix in an explicit form. To do this, we introduce the
adjugate matrix. The adjugate matrix ˜A of A is the transpose of the matrix of cofactors
of the elements of the matrix A. Namely,
˜A =
⎛
⎜⎜⎝
A11 A21 . . . An1
A12 A22 . . . An2
. . . . . . .
A1n A2n . . . Ann
⎞
⎟⎟⎠,
where Ai j denotes the cofactor of the element ai j in A.
Now we can write formulas (1.54), p. 21, in matrix form:
A ˜A = |A|I.
(1.102)
Therefore, if |A| ̸= 0, then the matrix
A−1 = |A|−1 ˜A
(1.103)
is the inverse of the matrix A.
For example, let us calculate the inverse of the matrix
A =
⎛
⎝
3 −1 0
−2
1 1
2 −1 4
⎞
⎠.
Expanding the determinant of A in terms of the ﬁrst row, we get |A| = 5. The
cofactors of the elements of the matrix A are calculated in the following way:
A11 =

1 1
−1 4
 =
5, A12 = −

−2 1
2 4
 = 10,
A13 =

−2
1
2 −1
 = 0,
A21 = −

−1 0
−1 4
 =
4, A22 =

3 0
2 4
 = 12,
A23 = −

3 −1
2 −1
 = 1,
A31 =

−1 0
1 1
 = −1, A32 = −

3 0
−2 1
 = −3, A33 =

3 −1
−2
1
 = 1.

42
1
Preliminaries
Using (1.103), we obtain
A−1 = 1
|A|
⎛
⎝
A11 A21 A31
A12 A22 A32
A13 A23 A33
⎞
⎠=
⎛
⎝
1
4/5 −1/5
2 12/5 −3/5
0
1/5
1/5
⎞
⎠.
Below are some properties of the operation of inversion of matrices.
1. The matrix A−1 is nonsingular and (A−1)−1 = A. This statement is an evident
consequence of the equality AA−1 = I.
2. If the matrices A, B are nonsingular, then (AB)−1 = B−1A−1. Indeed, the fol-
lowing equalities hold: AB(B−1 A−1) = A(BB−1)A−1 = AA−1 = I.
3. If the matrix A is nonsingular, then AT is nonsingular and (AT )−1 = (A−1)T .
The matrix AT is nonsingular, since the equality |AT | = |A| holds. Using Prop-
erty 3 (b), p. 38, we see that (AT )(A−1)T = (A−1 A)T = I T = I, i.e., the
matrix (A−1)T is the inverse of AT .
The proof of the following statements is left to the reader.
1. If matrices A1, A2,…, Ap are nonsingular, then
(A1A2 · · · Ap)−1 = A−1
p A−1
p−1 · · · A−1
1 .
(1.104)
2. If Pik is a permutation matrix, then
P−1
ik
= Pik.
(1.105)
3. If Lk is an elementary lower triangular matrix such that lkk ̸= 0, then
L−1
k
=
⎛
⎜⎜⎜⎜⎜⎜⎝
1 . . .
0
0 . . . 0
. . . . . . . . . . .
0 . . .
1/lk,k
0 . . . 0
0 . . . −lk+1,k/lk,k 1 . . . 0
. . . . . . . . . . .
0 . . . −ln,k/lk,k
0 . . . 1
⎞
⎟⎟⎟⎟⎟⎟⎠
.
(1.106)
4. If L is a lower triangular matrix such that all elements of its main diagonal
are nonzero, then the inverse L−1 exists and is a lower triangular matrix. The
analogous statement is true for upper triangular matrices.

1.2 Systems of Linear Equations, Matrices, Determinants
43
1.2.5
Gaussian Elimination
In this section we consider an algorithm for solving the system of linear equations
Ax = b
(1.107)
with a square nonsingular matrix A. This algorithm is called Gaussian elimination.10
Like many other methods, it is based on the following statement.
Let B be a given nonsingular matrix. Then the system of linear equations
B Ax = Bb
(1.108)
is equivalent to system (1.107), namely, each solution of system (1.108) is a solution
of (1.107), and conversely, each solution of system (1.107) is a solution of (1.108).
Indeed, if x is a solution of system (1.108), then
B(Ax −b) = 0,
but the matrix B is nonsingular; hence, Ax −b = 0. The converse is obvious.
Usually the matrix B is chosen such that the matrix B A is “simpler” than A and the
solution of system (1.108) is easier to calculate than the solution of system (1.107). In
Gaussian elimination, the matrix B is the product of special lower triangular matrices
such that the matrix B A is upper triangular. In this case, problem (1.108) is trivial.
Let us describe the Gaussian elimination algorithm. In the ﬁrst step we take in
the ﬁrst column of the matrix A the element with the largest absolute value. Suppose
that this element is ai1. It is not equal to zero. Indeed, if ai1 = 0, then all elements in
the ﬁrst column of A are equal to zero, and |A| = 0, but we have assumed that the
matrix A is nonsingular.
Then we multiply both sides of system (1.107) by the permutation matrix Pi1. We
denote this matrix by P1 (note that P1 = I if in the ﬁrst column of A, the element
with the largest absolute value is a11) and get
A1x = b1,
(1.109)
where A1 = P1A, b1 = P1b. Observe that the matrix A1 is obtained from the
matrix A by interchanging the ﬁrst and the ith rows, and the column b1 is obtained
from the column b by interchanging the ﬁrst and the ith elements. We denote by a(1)
kl
the elements of the matrix A1, and by b(1)
k
the elements of the column b1. By con-
struction, a(1)
11 ̸= 0.
After that, we multiply both sides of system (1.109) by the elementary lower
triangular matrix
10Johann Carl Friedrich Gauss (1777–1855) was a German mathematician.

44
1
Preliminaries
L1 =
⎛
⎜⎜⎜⎜⎝
l1,1
0 0 . . . 0 0
l2,1
1 0 . . . 0 0
. . . . . . .
ln−1,1 0 0 . . . 1 0
ln,1 0 0 . . . 0 1
⎞
⎟⎟⎟⎟⎠
,
(1.110)
where l11 = 1/a(1)
11 , l21 = −a(1)
21 /a(1)
11 , . . . , ln1 = −a(1)
n1 /a(1)
11 , and get
A2x = b2,
(1.111)
where b2 = L1b1,
A2 = L1A1 =
⎛
⎜⎜⎜⎝
1 a(2)
12 a(2)
13 . . . a(2)
1n
0 a(2)
22 a(2)
23 . . . a(2)
2n
. . . . . . . .
0 a(2)
n2 a(2)
n3 . . . a(2)
nn
⎞
⎟⎟⎟⎠.
(1.112)
Multiplication of the matrices L1 and A1 is equivalent to the following transforma-
tion of the matrix A1: all elements of the ﬁrst row of the matrix A1 are divided by a(1)
11 ;
after that, for all i = 2, . . . , n, the ﬁrst row is multiplied by a(1)
i1 and is subtracted
from the ith row of A1. Similarly, the elements of the column b2 are calculated by
the following formulas: b(2)
1
= b(1)
1 /a(1)
11 , b(2)
i
= b(1)
i
−b(2)
1 a(1)
i1 , where i = 2, . . . , n.
Note that all elements of the ﬁrst column of the matrix A2 except the ﬁrst element
are equal to zero. Now we take the element with the largest absolute value among
the elements a(2)
22 , a(2)
32 , …, a(2)
n2 . Suppose that this element is a(2)
i2 . It is not equal to
zero. Indeed, if a(2)
i2 = 0, then all the numbers a(2)
22 , a(2)
32 , …, a(2)
n2 would be equal to
zero, and expanding the determinant of A2 in terms of the ﬁrst column, we would
get det A2 = 0. On the other hand, we see that
det A2 = l11 det(P1A) = det(P1A)/a(1)
11 = ± det(A)/a(1)
11 ̸= 0,
since L1 is an elementary lower triangular matrix and P1 is either the identity matrix
or a permutation matrix.
Then we multiply both sides of system (1.111) by the permutation matrix P2 = P2i
(in other words, we interchange the second row and the ith row of the matrix A2)
and get
˜A2x = P2L1P1b,
(1.113)
where
˜A2 = P2 A2 =
⎛
⎜⎜⎜⎝
1 a(2)
12 a(2)
13 . . . a(2)
1n
0 ˜a(2)
22 ˜a(2)
23 . . . ˜a(2)
2n
. . . . . . . .
0 ˜a(2)
n2 ˜a(2)
n3 . . . ˜a(2)
nn
⎞
⎟⎟⎟⎠.

1.2 Systems of Linear Equations, Matrices, Determinants
45
Multiplying both sides of (1.113) by the elementary lower triangular matrix
L2 =
⎛
⎜⎜⎜⎜⎜⎜⎝
1
0
0 0 . . . 0 0
0 l2,2
0 0 . . . 0 0
0 l3,2
1 0 . . . 0 0
. . . . . . . .
0 ln−1,2 0 0 . . . 1 0
0 ln,2
0 0 . . . 0 1
⎞
⎟⎟⎟⎟⎟⎟⎠
,
where l22 = 1/˜a(2)
22 , l32 = −˜a(2)
32 /˜a(2)
22 , . . . , ln2 = −˜a(2)
n2 /˜a(2)
22 , we get
A3x = L2P2L1P1b,
where A3 = L2 ˜A2 = L2P2L1P1A. It is easy to see that
A3 =
⎛
⎜⎜⎜⎜⎜⎝
1 a(2)
12 a(2)
13 . . . a(2)
1n
0 1 a(3)
23 . . . a(3)
2n
0 0 a(3)
33 . . . a(3)
3n
. . . . . . . .
0 0 a(3)
n3 . . . a(3)
nn
⎞
⎟⎟⎟⎟⎟⎠
.
It is important to note that all elements of the second column of the matrix A3 except
the ﬁrst two elements are equal to zero.
Continuing this process, we ﬁnally get the system of linear equations
Ux = f
(1.114)
(which obviously is equivalent to the original system), where
U = Ln PnLn−1Pn−1 · · · L1P1 A,
(1.115)
f = Ln PnLn−1Pn−1 · · · L1P1b,
and, what is important,
U =
⎛
⎜⎜⎜⎜⎜⎜⎜⎝
1 a(2)
12 a(2)
13 . . . a(2)
1n−1
a(2)
1n
0 1 a(3)
23 . . . a(3)
2n−1
a(3)
2n
0 0
1 . . . a(4)
3n−1
a(4)
3n
. . . . . . . . . . . .
0 0
0 . . .
1
a(n)
n−1,n
0 0
0 . . .
0
1
⎞
⎟⎟⎟⎟⎟⎟⎟⎠
(1.116)
is a triangular matrix with ones on the main diagonal.

46
1
Preliminaries
Problem (1.114) is not difﬁcult. From the last equation of system (1.114) we see
that xn = fn. Using the penultimate equation, we get
xn−1 = fn−1 −a(n)
n−1,nxn,
(1.117)
and so on. Finally, using the ﬁrst equation, we obtain
x1 = f1 −a(2)
1,2x2 −a(2)
1,3x3 −· · · −a(2)
1,nxn.
(1.118)
Thus the Gaussian elimination algorithm can be divided into two parts. In the
ﬁrst part (sometimes called forward elimination), we reduce a given system to a
system with a triangular matrix U. In the second part (sometimes called backward
substitution), we solve this system with the triangular matrix obtained in the ﬁrst
part.
Remark 1.4 We choose the elements with the largest absolute values for forward
elimination to improve numerical stability. These elements are called pivot elements,
and the corresponding interchanging of rows is called pivoting (see Section8.1,
p. 249, for a detailed description). If we do not worry about rounding errors of
numerical computations, then we can use as a pivot element any nonzero element of
the column in each step of forward elimination.
Gaussian elimination also makes it possible to compute the determinant of a
square matrix. Using (1.115), (1.104), and (1.105), we get
A = P1L−1
1 P2L−1
2 · · · PnL−1
n U.
(1.119)
From (1.119), (1.94), and (1.96) it follows that
det A = det(P1L−1
1 P2L−1
2 · · · PnL−1
n U) =
n
i=1
det Pi
n
i=1
det L−1
i
= ±
n
i=1
det L−1
i .
(1.120)
Here we have taken into account that det U = 1. Using (1.106), it is easy to see that
det L−1
i
= ˜a(i)
ii ;
hence
det A = ±a(1)
11 ˜a(2)
22 · · · ˜a(n)
nn .
(1.121)
Thus the determinant of the matrix A is equal to the product of all the pivot elements
up to sign. The sign is determined by the number of interchanges of rows in forward

1.2 Systems of Linear Equations, Matrices, Determinants
47
elimination. If this number is even, then the sign is plus; if this number is odd, then
the sign is minus.
Let us estimate the number of arithmetic operations required to solve a system of
linear equations by Gaussian elimination. In the ﬁrst step of forward elimination, the
matrix L1 is constructed. This requires n operations. Then the matrix L1 is multiplied
by the matrix A1. It is easy to verify that the multiplication of the matrix L1 by a
column requires 2(n −1) + 1 = 2n −1 operations. The total number of columns
is n. Therefore, the multiplication of the matrix L1 by the matrix A1 requires 2n2 −n
operations. After that, the matrix L1 is multiplied by the column P1b. Thus the ﬁrst
step of forward elimination requires 2n2 + n −1 operations.
It is easy to see that in the second step of forward elimination, the product L2 ˜A2
is calculated by the multiplication of matrices of order n −1. Hence the second
step requires 2(n −1)2 + (n −2) operations, and all steps of forward elimination
require 2(12 + 22 + · · · + n2) + (1 + 2 + · · · + (n −1)) operations. It is well known
that 1 + 2 + · · · + n −1 = n(n −1)/2, 12 + 22 + · · · + n2 = n(n + 1)(2n + 1)/6.
Thus forward elimination requires
n(n + 1)(2n + 1)/3 + n(n −1)/2 ≈2n3/3
arithmetic operations. Note that we neglect the terms of order n2, on the assumption
that n is big enough. It is easy to see that the calculations by formulas (1.117), (1.118)
require 2(n−1)+2(n−2)+· · ·+2 = 2(1+· · ·+n−1) = n(n −1) ≈n2 operations,
and ﬁnally, we can conclude that to solve a system of n equations in n unknowns,
the Gaussian elimination algorithm requires approximately 2n3/3 operations.
Note that Cramer’s formulas require, as is easy to calculate, n2n! arithmetic oper-
ations, which is much bigger. For example, if n = 20, then n2n! ≈9.7 × 1020
and 2n3/3 ≈5.3 × 103.
For example, let us solve the following system of linear equations by Gaussian
elimination:
3x1 + 6x2 + 15x3 = 60,
3x1 + 2x2 + 9x3 = 34,
9x1 + 6x2 −3x3 = 12.
First of all, we write down the matrix and the column of the right-hand side of the
system:
A =
⎛
⎝
3 6 15
3 2
9
9 6 −3
⎞
⎠,
b =
⎛
⎝
60
34
12
⎞
⎠.
The element with the largest absolute value in the ﬁrst column of the matrix A
is a31 = 9. In accordance with the algorithm described above, the matrix A1 and the
column b1 look as follows:

48
1
Preliminaries
A1 =
⎛
⎝
9 6 −3
3 2
9
3 6 15
⎞
⎠,
b1 =
⎛
⎝
12
34
60
⎞
⎠
(we have interchanged the ﬁrst row of the matrix A with its third row and the ﬁrst
element of the column b with its third element).
Now we divide the ﬁrst row of the matrix A1 by 9, multiply the result by 3,
and subtract that result from the second and the third rows; also we divide the ﬁrst
element of the column b1 by 9, multiply the result by 3, and subtract that result from
the second and the third elements of b1. As a result, we get
A2 =
⎛
⎝
1 2/3 −1/3
0 0
10
0 4
16
⎞
⎠,
b2 =
⎛
⎝
4/3
30
56
⎞
⎠.
The element with the largest absolute value among the elements a(2)
22 , a(2)
32 is a(2)
32 .
Therefore, we interchange the second and third rows of the matrix A2, and also the
second and third elements of the column b2, and obtain
˜A2 =
⎛
⎝
1 2/3 −1/3
0 4
16
0 0
10
⎞
⎠,
˜b2 =
⎛
⎝
4/3
56
30
⎞
⎠.
We divide the second row of the matrix ˜A2 and the second element of the column ˜b2
by 4, and get
˜˜A2 =
⎛
⎝
1 2/3 −1/3
0 1
4
0 0
10
⎞
⎠,
˜˜b2 =
⎛
⎝
4/3
14
30
⎞
⎠.
Finally, we divide the last row of ˜˜A2 and the last element of ˜˜b2 by 10, and get
A3 =
⎛
⎝
1 2/3 −1/3
0 1
4
0 0
1
⎞
⎠,
b3 =
⎛
⎝
4/3
14
3
⎞
⎠.
The forward elimination is done.
Using the back substitution algorithm, we calculate x3 = 3, after that x2 =
14 −3 × 4 = 2, and ﬁnally, x1 = 4/3 −(2/3) × 2 + (1/3) × 3 = 1.
As we have seen above, the determinant of the matrix A is equal to the product
of all the pivot elements up to sign. In this example, the pivot elements are 9, 4, and
10. The number of interchanges of rows in forward elimination was two. Therefore,
the determinant is equal to the product of the pivot elements: det(A) = 360.

1.2 Systems of Linear Equations, Matrices, Determinants
49
1.2.6
The Determinant of the Product of Matrices
Theorem 1.12 The determinant of the product of arbitrary square matrices A and B
(of the same dimension) is equal to the product of their determinants:
det(AB) = det A det B.
(1.122)
Proof If the matrix A is singular, then the matrix AB is also singular (see p. 39), and
in this case, equality (1.122) obviously holds.
If the matrix A is nonsingular, then, using (1.119), we get
AB = P1L−1
1 P2L−1
2 · · · PnL−1
n U B.
In this product, each factor except B is either a permutation matrix or a triangular
matrix; hence
det(AB) =
n
i=1
det Pi
n
i=1
det L−1
i
det(U) det B =
n
i=1
det Pi
n
i=1
det L−1
i
det B,
but we have (see (1.120))
n
i=1
det Pi
n
i=1
det L−1
i
= det A.
Thus equality (1.122) holds.
□
From (1.122) it follows immediately that if the matrix A is nonsingular, then
det(A−1) = 1/ det A.
1.2.7
Basic Matrix Types
In this section we describe some types of matrices that are often used in different
problems of linear algebra. Here we consider some basic properties of these matrices.
A more detailed study will be done in the next chapters.
Let A be a rectangular matrix. The Hermitian11 adjoint A∗of A is deﬁned by A∗=
( ¯A)T , where ¯A is the componentwise conjugate. Another name for the Hermitian
adjoint of a matrix is the conjugate transpose. It is easy to see that (A∗)∗= A,
(αA)∗= ¯αA∗, (A + B)∗= A∗+ B∗, (AB)∗= B∗A∗.
A square matrix A is called Hermitian if A = A∗. It is called skew-Hermitian
if A = −A∗. The determinant of a Hermitian matrix is a real number. Indeed,
11Charles Hermite (1822–1901) was a French mathematician.

50
1
Preliminaries
since det(A∗) = det((A)T ) = det(A) = det(A), we see that det(A) = det(A) if the
matrix A is Hermitian.
Every square matrix A can be represented in the form
A = H1 + iH2,
(1.123)
where H1, H2 are Hermitian matrices and i is the imaginary unit. The matrices H1, H2
are uniquely determined by the matrix A. Indeed, representation (1.123) follows from
the obvious identity
A = 1
2(A + A∗) + i 1
2i(A −A∗)
and the following easily veriﬁable equalities:
(A + A∗)∗= A + A∗,
1
i (A −A∗)
∗
= 1
i (A −A∗).
If we assume that in addition to (1.123) there exists an additional representation
A = 
H1 + i 
H2
with Hermitian matrices 
H1, 
H2, then
(H1 −
H1) + i(H2 −
H2) = 0.
(1.124)
The Hermitian adjoint of the left part of equality (1.124) is also equal to zero:
(H1 −
H1) −i(H2 −
H2) = 0.
(1.125)
Adding together the corresponding terms in (1.124) and (1.125), we get H1 = 
H1;
hence H2 = 
H2. Thus representation (1.123) is unique.
A real matrix is a matrix all of whose elements are real numbers.
A real Hermitian matrix is called symmetric. For every symmetric matrix we
have A = AT . A real square matrix is called skew-symmetric if A = −AT .
For every real square matrix, the following representation holds:
A = A1 + A2,
(1.126)
where A1 is a symmetric matrix and A2 is skew-symmetric. Arguing as above, we
see that this representation is unique and
A1 = 1
2(A + AT ),
A2 = 1
2(A −AT ).

1.2 Systems of Linear Equations, Matrices, Determinants
51
A square matrix A is called unitary if AA∗= I, in other words, if A−1 = A∗.
It follows from the deﬁnition that the absolute value of the determinant of every
unitary matrix is equal to one. The product of two unitary matrices is a unitary
matrix (prove it!).
An important example of a unitary matrix is a diagonal matrix with diagonal
elements q1, q2, …, qn such that the absolute value of each element is equal to one.
The reader can easily prove that this matrix is unitary.
Let A be a unitary matrix of order n. Sometimes we say that the rectangular
matrix B that consists of m, m < n, columns of the square unitary matrix A is
unitary. Clearly, B∗B = Im, where Im is the identity matrix of order m.
A real unitary matrix is called orthogonal. The determinant of every orthogonal
matrix is either plus one or minus one. Two examples of orthogonal matrices are the
permutation matrix Pkl and the second-order matrix
Q2(ϕ) =
cos ϕ −sin ϕ
sin ϕ
cos ϕ

,
where ϕ is a real number.
A square matrix A is called normal if AA∗= A∗A, that is, if A commutes with
its Hermitian adjoint. It is easy to see that Hermitian, skew-Hermitian, and unitary
matrices are normal.
For example, the matrix A =
1 −1
1
1

is normal, but it belongs to none of above-
mentioned matrix types.
1.2.8
Block Matrices and Basic Operations with Block
Matrices
It is useful in many cases to interpret a matrix as having been broken into sections
called blocks or submatrices, that is, to represent it in the form
A =
⎛
⎜⎜⎝
A11 A12 . . . A1n
A21 A22 . . . A2n
. . . . . . . .
Am1 Am2 . . . Amn
⎞
⎟⎟⎠,
(1.127)
where the elements Ai j are themselves matrices. Note that all blocks in (1.127)
belonging to one row have the same number of rows, and all blocks in one column
have the same number of columns. Every matrix may be interpreted as a block matrix
in different ways, with each interpretation deﬁned by how its rows and columns are
partitioned. For example,

52
1
Preliminaries
⎛
⎝
1
8
7
6
3
5
0
2
1
4
9
3
⎞
⎠,
⎛
⎝
1
8
7
6
3
5
0
2
1
4
9
3
⎞
⎠,
⎛
⎝
1
8
7
6
3
5
0
2
1
4
9
3
⎞
⎠.
It is easy to see that with block matrices we can operate by the same formal rules
as with ordinary matrices. If in addition to matrix (1.127) we introduce the matrix
B =
⎛
⎜⎜⎝
B11 B12 . . . B1n
B21 B22 . . . B2n
. . . . . . . .
Bm1 Bm2 . . . Bmn
⎞
⎟⎟⎠
(1.128)
such that for each pair of indices i, j the dimensions of blocks Ai j, Bi j coincide,
then the matrix C = A + B can be represented as the block matrix with blocks
Ci j = Ai j + Bi j, i = 1, . . . , m, j = 1, . . . , n. Suppose that
B =
⎛
⎜⎜⎝
B11 B12 . . . B1p
B21 B22 . . . B2p
. . . . . . .
Bn1 Bn2 . . . Bnp
⎞
⎟⎟⎠.
(1.129)
Then the matrix C = AB can be represented as the block matrix with blocks
Ci j =
n

q=1
Aiq Bqj,
i = 1, 2, . . . , m, j = 1, 2, . . . , p.
(1.130)
This, of course, requires that each product Aiq Bqj exist, i.e., that the horizontal
dimension of each block Aiq coincide with the vertical dimension of the correspond-
ing block Bqj.
Let us obtain some useful formulas for calculation of determinants of block matri-
ces. We begin with the simplest case. Suppose that
A =
I A12
0 A22

(1.131)
is a 2 × 2 block matrix, I is the identity matrix, A22 is a square matrix, and A12 is a
(generally speaking) rectangular matrix. Then we get
|A| = |A22|.
(1.132)
Similarly, if
A =
A11 A12
0
I

,
(1.133)

1.2 Systems of Linear Equations, Matrices, Determinants
53
where A11 is a square matrix, then
|A| = |A11|.
(1.134)
Theorem 1.13 Suppose that
A =
A11 A12
0
A22

,
(1.135)
where A11, A22 are square matrices. Then
|A| = |A11||A22|.
(1.136)
Proof First we prove that if the matrix A11 is singular, then |A| = 0. Denote by n1
the order of the matrix A11, and by n2 the order of A22. If |A11| = 0, then there
exists a vector x1 ̸= 0 of length n1 such that A11x1 = 0. Then for the nonzero vector
x = (x1, 0, . . . , 0) of length n1+n2, we obviously have Ax = 0. Therefore, |A| = 0.
Thus we have proved that if |A11| = 0, then equality (1.136) trivially holds. Now
let |A11| ̸= 0. It is easy to see that
A11 A12
0
A22

=
A11 0
0
I
 I A−1
11 A12
0
A22

.
(1.137)
Hence
|A| =

A11 0
0
I


I A−1
11 A12
0
A22
 .
Combining the last equality, (1.132), and (1.134), we ﬁnally obtain (1.136).
□
The proof of the following statements is left to the reader.
1. Suppose that
A =
⎛
⎜⎜⎜⎜⎝
A11
0
0
. . .
0
A12
A22
0
. . .
0
A13
A23
A33
. . .
0
. . .
. . .
. . .
. . .
. . .
A1n
A2n
A3n
. . .
Ann
⎞
⎟⎟⎟⎟⎠
(1.138)
is a block triangular matrix, where Aii, i = 1, 2, . . . , n, are arbitrary square
matrices. Then |A| = |A11||A22| · · · |Ann|.
2. Suppose that
A =
A11 A12
A21 A22

is a block matrix, A11, A22 are square matrices, and |A11| ̸= 0. Then
|A| = |A11||A22 −A21A−1
11 A12|.
(1.139)

54
1
Preliminaries
Hint: calculate the product
A11 A12
A21 A22
 
I −A−1
11 A12
0
I

.
Relationship (1.139) can be considered a generalization of formula (1.48) for
calculating a second-order determinant.
A matrix A of the form (1.138) is called block upper triangular. A block lower
triangular matrix can be deﬁned similarly. A matrix A of the form
A =
⎛
⎜⎜⎜⎜⎝
A11
0
0
. . .
0
0
A22
0
. . .
0
0
0
A33
. . .
0
. . .
. . .
. . .
. . .
. . .
0
0
0
. . .
Ann
⎞
⎟⎟⎟⎟⎠
,
where Aii, i = 1, 2, . . . , n, are arbitrary square matrices, is called block diagonal.
In this case, we also use the following notation: A = diag(A11, A22, . . . , Ann).

Chapter 2
Vector Spaces
In courses on analytic geometry, basic operations with vectors in three-dimensional
Euclidean space are studied. If some basis in the space is ﬁxed, then a one-to-one
correspondence between geometric vectors and ordered triples of real numbers (the
coordinatesofvectorsinthebasis)isdetermined,andalgebraicoperationswithvector
coordinates can be substituted for geometric operations with the vectors themselves.
Similar situations arise in many other areas of mathematics and its applications
when objects under investigation are described by tuples (ﬁnite ordered lists) of real
(or complex) numbers. Then the concept of a multidimensional coordinate space as
the set of all tuples with algebraic operations on those tuples naturally arises.
In this chapter we will systematically construct and investigate such spaces. First
of all, we will introduce the space Rn of all n-tuples of real numbers and the space
Cn of all n-tuples of complex numbers. We will begin with deﬁnitions and basic
properties of these spaces, since later we will introduce and study more general
vector spaces. All results that we will obtain for general spaces hold for the vector
spaces Rn and Cn. We will provide also a variety of useful examples of speciﬁc bases
in ﬁnite-dimensional spaces.
2.1
The Vector Spaces Rn and Cn
2.1.1
The Vector Space Rn
The vector space Rn is the set of all n-tuples x = (x1, x2, . . . , xn) of real numbers,
where n ≥1 is a given integer. Elements of the space Rn are called vectors or points;
the numbers xk, k = 1, 2, …, n, are called the components of the vector x.
© Springer International Publishing AG 2017
L. Beilina et al., Numerical Linear Algebra: Theory and Applications,
DOI 10.1007/978-3-319-57304-5_2
55

56
2
Vector Spaces
Two vectors x, y ∈Rn are equal if and only if xk = yk for all k = 1, 2, . . . , n.
The vector all of whose components are zero is called the zero vector and is denoted
by 0. A vector of the form
ik = (0, . . . , 0
  
k−1
, 1, 0, . . . , 0
  
n−k
),
where the kth component is equal to one and all other components are zero, is
called a standard unit vector. In the space Rn there are exactly n standard unit
vectors: i1, i2, . . . , in.
The following linear operations are introduced on the space Rn: scalar multipli-
cation (the multiplication of a vector by a scalar) and vector addition. Namely, for
every real number α and x, y ∈Rn, by deﬁnition, we put
αx = (αx1, αx2, . . . , αxn),
x + y = (x1 + y1, x2 + y2, . . . , xn + yn).
These linear operations satisfy the following properties. In the list below, x, y, z
are arbitrary vectors in Rn, and α, β are arbitrary real numbers.
1. Commutativity of vector addition: x + y = y + x.
2. Associativity of vector addition: (x + y) + z = x + (y + z).
3. The zero vector is the identity element of vector addition: x + 0 = x.
4. For every vector x there exists a unique inverse element such that x + (−x) = 0,
where by deﬁnition, −x = (−1)x.
5. Distributivity of scalar multiplication with respect to vector addition:
α(x + y) = αx + αy.
6. Distributivity of scalar multiplication with respect to scalar addition:
(α + β)x = αx + βx.
7. Associativity of scalar multiplication: (αβ)x = α(βx).
8. The number 1 is the identity element of scalar multiplication: 1x = x.
Properties 1–8 are called the vector space axioms. They follow immediately from
the deﬁnition of linear operations with elements of the space Rn. It is easy to see that
Axioms 1–8 correspond exactly to the properties of linear operations with vectors in
three-dimensional Euclidean space.
It is important to note that R1 is a vector space, but at the same time it is the set
of all real numbers. As usual, we denote R1 by R.

2.1 The Vector Spaces Rn and Cn
57
2.1.2
The Vector Space Cn
The vector space Cn is the set of all n-tuples x = (x1, x2, . . . , xn) of complex num-
bers, where n ≥1 is a given integer. The elements of the space Cn are called vectors
or points; the numbers xk, k = 1, 2, …, n, are called the components of the vector x.
Two vectors x, y ∈Cn are equal if and only if xk = yk for all k = 1, 2, . . . , n.
The vector all of whose components are zero is called the zero vector and is denoted
by 0. The vector ik whose kth component is equal to one and all other components
are equal to zero is called a standard unit vector. In the space Cn there are exactly n
standard unit vectors: i1, i2, …, in.
The linear operations of scalar multiplication and vector addition are introduced
on the space Cn in the usual way: for every complex number α and all x, y ∈Cn,
by deﬁnition, we put
αx = (αx1, αx2, . . . , αxn),
x + y = (x1 + y1, x2 + y2, . . . , xn + yn).
Note that we have actually already met this linear space. We can interpret the
set of all m × n matrices with operations of matrix addition and multiplication of
matrices by scalars (see p. 32) as the space Cmn of all vectors of length mn. The
vectors were written in the form of rectangular arrays, but from the point of view of
linear operations with vectors, this fact does not matter.
Properties 1–8 above hold also for the linear operations on the space Cn.
Note that C1 is a vector space, but at the same time it is the set of all complex
numbers. As usual, we denote C1 by C.
2.2
Abstract Vector Spaces
2.2.1
Deﬁnitions and Examples
Two concepts more general than the spaces Rn and Cn are widely used in many areas
of mathematics. These generalizations are abstract vector spaces: real and complex.
A real vector space X is a set that is closed under the operations of vector addition
and scalar multiplication that satisfy the axioms listed below. Elements of X are
called vectors. The operation of vector addition takes any two elements x, y ∈X
and assigns to them a third element z = x + y ∈X, which is called the sum of the
vectors x and y. The operation of scalar multiplication takes any real number α and
any element x ∈X and gives an element αx ∈X, which is called the product of α
and x.
In order for X to be a real vector space, the following vector space axioms, which
are analogous to Properties 1–8 of the space Rn (see p. 56), must hold for all elements
x, y, z ∈X and real numbers α, β.

58
2
Vector Spaces
1. Commutativity of vector addition: x + y = y + x.
2. Associativity of vector addition: (x + y) + z = x + (y + z).
3. There exists a unique element 0 ∈X, called the zero element of the space X, such
that x + 0 = x for all x ∈X.
4. For every element x ∈X there exists a unique element x′ ∈X, called the additive
inverse of x, such that x + x′ = 0.1
5. Distributivity of scalar multiplication with respect to vector addition:
α(x + y) = αx + αy.
6. Distributivity of scalar multiplication with respect to scalar addition:
(α + β)x = αx + βx.
7. Associativity of scalar multiplication: (αβ)x = α(βx).
8. The number 1 is the identity element of scalar multiplication: 1x = x.
If in the deﬁnition of the space X multiplication by complex numbers is allowed,
then X is called a complex vector space. It is assumed that Axioms 1–8, where
α, β ∈C, hold.
The proof of the following statements is left to the reader (here X is an arbitrary
vector space):
1. −0 = 0 (here 0 is the zero element of X);
2. α0 = 0 for every scalar α;
3. 0x = 0 for every vector x ∈X;
4. if αx = 0, x ∈X, then at least one of the factors is zero;
5. −x = (−1)x for every x ∈X;
6. y + (x −y) = x for every x, y ∈X, where, by deﬁnition, x −y = x + (−y).
In the remainder of the book we denote vector spaces by the capital letters X,
Y, Z. Unless otherwise stated, vector spaces are complex. The deﬁnitions and the
results are mostly true for real spaces, too. Cases in which distinctions must be made
in the interpretation of results for real spaces are specially considered.
The proof that the following sets are vector spaces is left to the reader.
1. The set V3 of all geometric vectors of three-dimensional Euclidean space with
the usual deﬁnitions of operations of multiplication of a vector by a real scalar
and vector addition is a real vector space.
2. The set of all real-valued functions of a real variable is a real vector space if the
sum of two functions and the product of a function and a real number are deﬁned
as usual.
3. The set of all real-valued functions that are deﬁned and continuous on the closed
segment [a, b] of the real axis is a real vector space. This space is denoted by
1The vector x′ is usually denoted by −x.

2.2 Abstract Vector Spaces
59
C[a, b]. Hint: recall that the sum of two continuous functions is a continuous func-
tion, and the product of a continuous function and a real number is a continuous
function.
4. The set of all functions in the space C[a, b] that are equal to zero at a ﬁxed point
c ∈[a, b] is a real vector space.
5. The set of all polynomials with complex coefﬁcients with the usual deﬁnitions
of the sum of two polynomials and the product of a polynomial and a complex
number is a complex vector space.
6. The set Qn of all polynomials of order no more than n, where n ≥0 is a given
integer, joined with the zero polynomial, is a complex vector space. Hint: as we
have seen in Section1.1.2, p. 8, the sum of two polynomials is either a polyno-
mial of degree no more than the maximum degree of the summands or the zero
polynomial.
The reader can answer the next two questions.
1. Consider the set of all positive functions deﬁned on the real axis and introduce
on this set the operation of vector addition as the multiplication of two func-
tions f · g and the operation of scalar multiplication as the calculation of the
power function f α. Is this set a vector space?
2. Consider the set of all even functions deﬁned on the segment [−1, 1] and introduce
on this set the operation of vector addition as the multiplication of two functions
and the operation of scalar multiplication as usual multiplication of a function by
a scalar. Is this set a vector space?
2.2.2
Linearly Dependent Vectors
Two vectors a and b in a vector space X are said to be linearly dependent (propor-
tional) if there exist numbers α and β, not both zero, such that
αa + βb = 0.
Clearly, in this case we have either a = γ b or b = δa, where γ , δ are some numbers.
For example, if k ̸= l, then the standard unit vectors ik, il ∈Cn, are nonpropor-
tional (prove it!).
Vectors x1 = (1 + i, 3, 2 −i, 5), x2 = (2, 3 −3i, 1 −3i, 5 −5i) ∈C4 are pro-
portional, since 2/(1 + i) = (3 −3i)/3 = (1 −3i)/(2 −i) = (5 −5i)/5 = 1 −i.
Let us generalize the concept of linear dependence of two vectors. A set of vec-
tors {ai}m
i=1 = {a1, a2, …,am}, m ≥1, in a vector space X is said to be linearly
dependent if there exist numbers x1, x2,…, xm, not all zero, such that
x1a1 + x2a2 + · · · + xmam = 0.
(2.1)

60
2
Vector Spaces
For instance, the set of vectors
a1 =
⎛
⎝
5
2
1
⎞
⎠,
a2 =
⎛
⎝
−1
3
3
⎞
⎠,
a3 =
⎛
⎝
9
7
5
⎞
⎠,
a4 =
⎛
⎝
3
8
7
⎞
⎠
in the space R3 is linearly dependent, since for x1 = 4, x2 = −1, x3 = −3, x4 = 2
we have
x1a1+x2a2+x3a3 + x4a4 = 4
⎛
⎝
5
2
1
⎞
⎠−
⎛
⎝
−1
3
3
⎞
⎠−3
⎛
⎝
9
7
5
⎞
⎠+ 2
⎛
⎝
3
8
7
⎞
⎠=
⎛
⎝
0
0
0
⎞
⎠= 0.
It is useful to note that there are many other sets of coefﬁcients x1, x2, x3, x4 such
that the linear combination x1a1 + x2a2 + x3a3 + x4a4 is equal to zero. For example,
2a1 + a2 −a3 = 2
⎛
⎝
5
2
1
⎞
⎠+
⎛
⎝
−1
3
3
⎞
⎠−
⎛
⎝
9
7
5
⎞
⎠= 0,
3a2 + a3 −2a4 = 3
⎛
⎝
−1
3
3
⎞
⎠+
⎛
⎝
9
7
5
⎞
⎠−2
⎛
⎝
3
8
7
⎞
⎠= 0.
It is useful to write the deﬁnition of linear dependence of vectors in matrix form.
We use the following notation. Let Am = {a1, a2, . . . , am} be a ﬁnite ordered list of
vectors in the space X. For x ∈Cm, by deﬁnition, we put
Amx = x1a1 + x2a2 + · · · + xmam.
Then we can say that the vectors a1, a2,…, am are linearly dependent if there exists
a nonzero vector x ∈Cm such that
Amx = 0.
A vector a ∈X is a linear combination of vectors b1, b2, …, bp, p ≥1, if there
exists a vector x ∈Cp such that
a = x1b1 + x2b2 + · · · + x pbp.
(2.2)
We can write this in matrix form:
a = Bpx.

2.2 Abstract Vector Spaces
61
A linear combination of vectors is called nontrivial if at least one of the numbers x1,
x2, …, x p in (2.2) is not equal to zero.
The proof of the following two theorems is left to the reader.
Theorem 2.1 A set of vectors is linearly dependent if it contains a linearly dependent
subset, in particular if it contains the zero vector.
Theorem 2.2 A set of vectors {ai}m
i=1 is linearly dependent if and only if it contains a
vector ak that can be represented as a linear combination of other vectors of the set
{ai}m
i=1.
Suppose that each vector of the set {ai}m
i=1 is a linear combination of the vec-
tors {bi}p
i=1, i.e.,
ak =
p

j=1
x jkb j,
k = 1, 2, . . . , m.
(2.3)
We can write (2.3) in matrix form:
Am = BpX(p, m),
(2.4)
where the kth column of the matrix X consists of the coefﬁcients x jk of the kth linear
combination in (2.3).
The following property of transitivity holds. If each vector of the set {ai}m
i=1
is a linear combination of the vectors {bi}p
i=1, and each vector of {bi}p
i=1 is a linear
combination of the vectors {ci}q
i=1, then each vector of {ai}m
i=1 is a linear combination
of {ci}q
i=1. Indeed, using matrix notation, we can write
Am = BpX(p, m),
Bp = CqY(q, p).
Substituting CqY(q, p) for Bp in the ﬁrst equality, we get
Am = Cq Z(q, m),
where
Z(q, m) = Y(q, p)X(p, m).
We say that the two sets of vectors {ai}m
i=1 and {bi}p
i=1 are equivalent if there exist
matrices X(p, m) and Y(m, p) such that
Am = BpX(p, m),
Bp = AmY(m, p),
(2.5)
i.e., each vector of the set Am is a linear combination of the vectors of the set Bp and
conversely.
Using the property of transitivity, the reader can easily prove the next statement.
Suppose that the sets {ai}m
i=1 and {bi}p
i=1 are equivalent and the vector x ∈X is a

62
2
Vector Spaces
linear combination of the vectors {ai}m
i=1. Then x can be represented as a linear
combination of the vectors {bi}p
i=1.
2.2.3
Linearly Independent Sets of Vectors
A set of vectors Am = {ai}m
i=1 in a vector space X is said to be linearly independent
if Amx = 0 implies x = 0.
Linearly independent sets exist. Let us give some simple examples.
1. Each vector a ̸= 0 forms a linearly independent set, which consists of one vector.
2. Ifm ≤n,thenthestandardunitvectorsi1,i2,…,im ∈Cn arelinearlyindependent.
Indeed, for every x ∈Cm, the vector
x1i1 + x2i2 + · · · + xmim ∈Cn
has the form (x1, x2, . . . , xm, 0, . . . , 0) and is equal to zero if and only if x = 0.
3. The set of vectors ϕ0(z) ≡1, ϕ1(z) = z, …, ϕk(z) = zk, where z is a complex
number and k ≥0 is a given integer, is linearly independent in the vector space of
polynomials (see p. 59). This statement immediately follows from the fact that if
a polynomial is equal to zero, then all its coefﬁcients are equal to zero (see p. 31).
The next theorem is an evident consequence of Theorem 2.1.
Theorem 2.3 Every subset of a linearly independent set {ai}m
i=1 is linearly indepen-
dent.
Theorem 2.4 Every set {a1, a2, ..., an, b} of n + 1 vectors in the space Cn is linearly
dependent.
Proof Suppose that the set of vectors {ai}n
i=1 is linearly independent. Denote by A
the matrix whose columns are the vectors ai, i = 1, 2, . . . , n. Clearly, det A ̸= 0,
and the system of linear equations Ax = b has a solution x. Therefore,
x1a1 + · · · + xnan = b,
i.e., the set of vectors {a1, a2, ..., an, b} is linearly dependent.
□
It follows immediately from Theorem 2.4 that every set {ai}m
i=1 ∈Cn, m > n, is
linearly dependent.
Theorem 2.5 Suppose that the set of vectors Am = {ai}m
i=1 in the space X is linearly
independent and each vector of the set Am is a linear combination of the vectors Bp =
{bi}p
i=1. Then m ≤p.

2.2 Abstract Vector Spaces
63
Proof Assume the contrary, i.e., let m > p. By deﬁnition, there exists a p × m matrix
X such that Am = BpX. Therefore, for every y ∈Cm we have Am y = BpXy. The
columns of the matrix X form a set of vectors in the space Cp. The number of
vectors in this set is m > p, and hence it is linearly dependent. Thus there exists a
vector y ∈Cm that is not equal to zero such that Xy = 0, but then Am y = 0, which
means, contrary to the assumption, that the set of vectors a1, a2, . . . , am is linearly
dependent.
□
Corollary 2.1 Every two linearly independent equivalent sets of vectors have the
same number of vectors.
The reader is invited to prove the next theorem (hint: use the reasoning of the
proof of Theorem 2.5).
Theorem 2.6 Suppose that the set {ak}m
k=1 is linearly independent and each vector
of the set {bk}m
k=1 is a linear combination of the vectors {ak}m
k=1, i.e., there exists
a square matrix X of order m such that Bm = Am X. The set {bk}m
k=1 is linearly
independent if and only if the matrix X is nonsingular.
It is important to note that in Theorem 2.6 the matrix X is uniquely determined
by the sets Am and Bm. Indeed, if we assume that there exists another matrix X such
that Bm = Am X, then Am(X −X) = 0, and X = X, since the set Am is linearly
independent.
2.2.4
The Rank of a Set of Vectors
Let {ai}m
i=1 be a given set of vectors in the space X. Suppose that not all vectors {ai}m
i=1
are equal to zero. Then this set necessarily contains a linearly independent subset of
vectors. In particular, the set {ai}m
i=1 itself can be linearly independent.
A linearly independent subset {aik}r
k=1 ⊂{ai}m
i=1 is called maximal if including
any other vector of the set {ai}m
i=1 would make it linearly dependent.
For example, let us consider the following set of vectors:
a1 =
⎛
⎝
2
−2
−4
⎞
⎠,
a2 =
⎛
⎝
1
9
3
⎞
⎠,
a3 =
⎛
⎝
−2
−4
1
⎞
⎠,
a4 =
⎛
⎝
3
7
−1
⎞
⎠
(2.6)
in the space R3. Evidently, the vectors a1, a2 are linearly independent and form a
maximal linearly independent subset, since the determinants

2
1 −2
−2
9 −4
−4
3
1

,

2
1
3
−2
9
7
−4
3 −1

,

64
2
Vector Spaces
which consist of the components of the vectors a1, a2, a3 and a1, a2, a4, respectively,
are equal to zero. Therefore, the sets of vectors a1, a2, a3 and a1, a2, a4 are linearly
dependent.
Generally speaking, the set {ai}m
i=1 can contain several maximal linearly indepen-
dent subsets, but the following result is true.
Theorem 2.7 Every two maximal linearly independent subsets of the set {ai}m
i=1
contain the same number of vectors.
Proof It follows from the deﬁnition of a maximal linearly independent subset that
each vector of the set {ai}m
i=1 is a linear combination of vectors of a maximal linearly
independent subset {aik}r
k=1. Obviously,
aik = aik +
m

i=1,i̸=ik
0ai;
hence the converse is also true. Therefore, the set {ai}m
i=1 and each of its maximal
linearly independent subsets are equivalent. Thus, using Corollary 2.1, we claim that
every two maximal linearly independent subsets of the set {ai}m
i=1 contain the same
number of vectors.
□
This result allows us to introduce the following concept. The rank of a set of
vectors in the space X is the number of vectors in each of its maximal linearly
independent subsets.
For example, the rank of the set of vectors (2.6) is equal to two.
The number of linearly independent vectors in the space Cn is no more than n.
Therefore, the rank of every set of vectors in Cn is less than or equal to n.
Clearly, a set of vectors {ai}m
i=1 in a vector space X is linearly independent if and
only if its rank is equal to m.
2.3
Finite-Dimensional Vector Spaces. Bases
2.3.1
Bases in the Space Cn
A linearly independent set {ek}n
k=1 (which consists of n vectors) is called a basis in
the space Cn. The standard unit vectors {ik}n
k=1 form the standard (or the natural)
basis in the space Cn.
It follows from Property 8, p.23, of determinants that a set {ek}n
k=1 ⊂Cn is a
basis if and only if the matrix En, the columns of which are formed by the vec-
tors e1, e2, . . . , en, is nonsingular.
In the proof of Theorem 2.4, p.62, we established that if {ek}n
k=1 is a basis in the
space Cn, then each vector x ∈Cn can be represented as a linear combination

2.3 Finite-Dimensional Vector Spaces. Bases
65
x = ξ1e1 + ξ2e2 + · · · + ξnen.
(2.7)
The coefﬁcients in the linear combination (2.7) are uniquely determined by the vector
x and satisfy the following system of linear algebraic equations with the nonsingular
matrix En:
Enξ = x.
(2.8)
Here ξ = (ξ1, ξ2, . . . , ξn) is the column of coefﬁcients of the expansion of x with
respect to the basis {ek}n
k=1.
2.3.2
Finite-Dimensional Spaces. Examples
A vector space X is called ﬁnite-dimensional if there exist vectors
En = {e1, e2, . . . , en}
that form a linearly independent set in the space X and such that each vector x ∈X
can be represented as a linear combination
x =
n

k=1
ξkek = Enξ,
ξ ∈Cn.
(2.9)
The set of vectors {ek}n
k=1 is called a basis of the space X. The number n is called
the dimension of X, and we denote by Xn this n-dimensional vector space. The
coefﬁcients ξ1, ξ2, …, ξn in the expansion (2.9) are called the coordinates of x with
respect to the basis {ek}n
k=1.
The coordinates of each vector x ∈Xn are uniquely determined by the basis
{ek}n
k=1. Indeed, suppose that in addition to (2.9) there exists an expansion x = En ˜ξ.
Then En(ξ −˜ξ) = 0. Therefore, ξ = ˜ξ, since the set of vectors {ek}n
k=1 is linearly
independent.
Theorem 2.8 In an n-dimensional vector space Xn, every system ˜En = {˜ek}n
k=1 con-
sisting of n linearly independent vectors is a basis.
Proof It is enough to show that each vector x ∈Xn can be represented as a linear
combination
x = ˜En ˜ξ.
(2.10)
By the deﬁnition of an n-dimensional vector space, a basis En exists in Xn. Therefore,
each vector of the set ˜En can be represented as a linear combination of the vectors
of En. In other words, there exists a square matrix T of order n such that ˜En =
EnT . The matrix T is nonsingular (see p. 63). Since En is a basis, there exists a
vector ξ ∈Cn such that x = Enξ. Since the matrix T is nonsingular, there exists

66
2
Vector Spaces
a vector ˜ξ ∈Cn such that ξ = T ˜ξ. Thus we get the relationship x = EnT ˜ξ = ˜En ˜ξ of
the form (2.10).
□
If a vector space is not ﬁnite-dimensional, then the space is called inﬁnite-
dimensional.
Let us give some examples of ﬁnite-dimensional and inﬁnite-dimensional vector
spaces.
1. Three arbitrary non-coplanar vectors form a basis in the space V3. The space V3
is three-dimensional.
2. Evidently, the spaces Cn, Rn are n-dimensional.
3. The set Qn of all polynomials of order no more than n is ﬁnite-dimensional. Its
dimension is equal to n + 1. For example, the set of vectors {1, z, . . . , zn}, where z
is a complex variable, is a basis in Qn.
4. The vector space of all polynomials is inﬁnite-dimensional. Indeed, for an arbi-
trarily large integer k, the set of vectors {1, z, . . . , zk} is linearly independent in
this space.
5. The space C[a, b] is inﬁnite-dimensional, since it contains polynomials with real
coefﬁcients of arbitrary order.
2.3.3
Change of Basis
Let En = {ek}n
k=1, ˜En = {˜ek}n
k=1 be bases in a vector space Xn. As we have shown,
the sets En and ˜En are equivalent, and there exist square matrices T and T of order n
such that
En = EnT ,
En = EnT.
(2.11)
The matrix T is called the change of basis matrix from En to En. The matrices T and T
are mutually inverse. Indeed, substituting EnT for En in the ﬁrst equality in (2.11),
we obtain En = EnT T . Thus we get
T T = I,
(2.12)
since the vectors of the basis En are linearly independent (see the remark after
Theorem 2.6, p. 63).
Suppose that we know the vector ξ of the coordinates of an element x ∈Xn with
respect to the basis En, and we also know the change of basis matrix T from En to
the basis En. Let us construct a formula for calculating the vector ˜ξ of coordinates of
the same element x with respect to the basis En. Using (2.9), we see that x = Enξ,
but En = EnT = EnT −1 (see (2.11), (2.12)). Therefore, x = EnT −1ξ, which means
that
˜ξ = T −1ξ.
(2.13)

2.3 Finite-Dimensional Vector Spaces. Bases
67
For example, suppose that vectors e1, e2, e3 form a basis in a three-dimensional
space X3. Let us consider the vectors
˜e1 =
5e1 −e2 −2e3,
˜e2 =
2e1 + 3e2,
˜e3 = −2e1 + e2 + e3.
Writing these equalities in matrix form, we get ˜E = ET , where
˜E = {˜e1, ˜e2, ˜e3},
E = {e1, e2, e3},
T =
⎛
⎝
5
2 −2
−1
3
1
−2
0
1
⎞
⎠.
It is easy to see that det T = 1; hence the matrix T is nonsingular. Therefore, the
vectors ˜e1, ˜e2, ˜e3 alsoformabasis inthespaceX3. Let us consider thevectora = e1 +
4e2 −e3. The coordinates of the vector a with respect to the basis E are the numbers
ξ1 = 1, ξ2 = 4, ξ3 = −1, i.e., a = Eξ, where ξ = (ξ1, ξ2, ξ3). Now we calculate the
coordinates of the same vector, but with respect to the basis ˜E. Calculating the matrix
T −1, we get
T −1 =
⎛
⎝
3 −2
8
−1
1 −3
6 −4 17
⎞
⎠,
and therefore,
˜ξ = T −1ξ =
⎛
⎝
3 −2
8
−1
1 −3
6 −4 17
⎞
⎠
⎛
⎝
1
4
−1
⎞
⎠=
⎛
⎝
−13
6
−27
⎞
⎠,
i.e., a = −13˜e1 + 6˜e2 −27˜e3. Thus we have calculated the coordinate representation
of the vector a with respect to the basis ˜E.
Note that inﬁnitely many bases exist in the space Xn. Indeed, if En is a basis, then
the set of vectors ˜En = EnT , where T is an arbitrary nonsingular matrix, also is a
basis (see Theorem 2.6, p. 63).
Below are some examples of bases in the space of polynomials of order no more
than n with complex coefﬁcients, which are often used in applications.
1. The natural basis for this space is the set of vectors {1, z, . . . , zn}, where z is a
complex variable.
2. The polynomials
Φ j(z) =
(z −z0)(z −z1) · · · (z −z j−1)(z −z j+1) · · · (z −zn)
(z j −z0)(z j −z1) · · · (z j −z j−1)(z j −z j+1) · · · (z j −zn),

68
2
Vector Spaces
j = 0, 1, 2, . . . , n, where z0, z1, …, zn are arbitrary distinct complex numbers,
also form a basis in the space of polynomials (see p. 29). Such a basis is called a
Lagrange basis.
3. Let us prove that the polynomials
ϕ0(z) ≡1, ϕ1(z) = (z −z0), ϕ2(z) = (z −z0)(z −z1), . . . ,
ϕn(z) = (z −z0)(z −z1) · · · (z −zn−1),
(2.14)
where z0, z1, …, zn−1 are arbitrary distinct complex numbers, form a basis. As in
the case of a Lagrange basis, it is enough to check that for the numbers z0, z1, …,
zn−1, and a number zn that does not coincide with any of the numbers z0, z1, …,
zn−1, the system of equations
c0ϕ0(z j) + c1ϕ1(z j) + · · · + cnϕn(z j) = h j,
j = 0, 1, 2, . . . , n,
(2.15)
has a unique solution for every choice of h0, h1, …hn. This fact is evident, since
system (2.15) is triangular,
c0 = h0,
c0 + c1(z1 −z0) = h1,
c0 + c1(z2 −z0) + c2(z2 −z0)(z2 −z1) = h2,
(2.16)
· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·
c0 + c1(zn −z0) + · · · + cn(zn −z0)(zn −z1) · · · (zn −zn−1) = hn,
and all diagonal coefﬁcients are different from zero. The basis deﬁned in (2.14)
is called a Newton basis.2
2Sir Isaac Newton (1642–1727) was an English physicist and mathematician.

Chapter 3
Inner Product Spaces
Asdiscussedinthepreviouschapter,vectorspacesareanalogoustothree-dimensional
Euclidean space V3 of geometric vectors (directed line segments). However, such
important concepts as the length of a vector and the angle between two vectors were
not introduced for abstract spaces. In three-dimensional Euclidean space, using the
lengths of two vectors and the angle between them, we can calculate the inner prod-
uct (the dot product) of these vectors. Many geometric problems in the space V3 are
solved with help of the dot product.
The concept of an inner product on an abstract space will be introduced axiomat-
ically in this chapter. After that, the concepts of the length of a vector and the angle
between two vectors will be introduced based on the concept of the inner product.
Then we will investigate the concept of orthogonal bases. Some important exam-
ples of orthogonal bases in ﬁnite-dimensional spaces, particularly in polynomial
spaces, will be constructed. The basic properties of subspaces of unitary spaces will
be described. We begin our considerations with inner products on the spaces Rn
and Cn.
3.1
Inner Products on Rn and Cn
An inner product on the space Rn is a function that assigns to each pair of vec-
tors x, y ∈Rn a real number (x, y) and satisﬁes the following axioms (which corre-
spond to the properties of the inner product of vectors in three-dimensional Euclidean
space):
1. (x, x) ≥0 for all x ∈Rn; (x, x) = 0 if and only if x = 0;
2. (x, y) = (y, x) for all x, y ∈Rn;
3. (αx + βy, z) = α(x, z) + β(y, z) for all x, y, z ∈Rn and for all α, β ∈R.
Clearly, the next property follows from Axioms 2 and 3:
4. (x, αy + βz) = α(x, y) + β(x, z) for all x, y, z ∈Rn and for all α, β ∈R.
© Springer International Publishing AG 2017
L. Beilina et al., Numerical Linear Algebra: Theory and Applications,
DOI 10.1007/978-3-319-57304-5_3
69

70
3
Inner Product Spaces
An inner product on the space Rn can be speciﬁed in inﬁnitely many ways. For
example, we can put
(x, y) =
n

k=1
xk yk.
This inner product on the space Rn is called standard. We can construct a variety of
inner products if we put
(x, y) =
n

k=1
ρkxk yk,
(3.1)
where ρ1, ρ2, . . . , ρn are positive numbers. Varying these numbers, we get different
inner products. The veriﬁcation of Axioms 1–3 is trivial for both examples.
By the length of a vector x ∈Rn we mean the nonnegative number |x| = √(x, x).
It can be shown that the length of vectors in Rn satisﬁes the following properties1
(which correspond to the properties of the length of vectors in three-dimensional
Euclidean space):
1. |x| ≥0 for all x ∈Rn; |x| = 0 if and only if x = 0;
2. |αx| = |α||x| for all x ∈Rn and for all α ∈R;
3. |x + y| ≤|x| + |y| for all x, y ∈Rn.
Inequality 3 is called the triangle inequality (or Minkowski’s2 inequality).
The vector space Rn, together with a speciﬁed inner product on it, is often called
the Euclidean space Rn. It is important to note that by specifying the inner product
on the space Rn in different ways, we get different Euclidean spaces. The space Rn
together with the standard inner product is called the real coordinate space. This
space plays an important role in many areas of mathematics and its applications. For
instance, it is systematically used in calculus for the study of functions of several
real variables.
An inner product on the space Cn is a function that assigns to each pair of vec-
tors x, y ∈Cn a (generally speaking) complex number (x, y) and satisﬁes the fol-
lowing axioms:
1. (x, x) ≥0 for all x ∈Cn; (x, x) = 0 if and only if x = 0;
2. (x, y) = (y, x) for all x, y ∈Cn; recall that the overline means the complex
conjugate and note that the inner product on the complex vector space Cn is not
commutative, unlike the inner product on the real space Rn
3. (αx + βy, z) = α(x, z) + β(y, z) for all x, y, z ∈Cn and for all α, β ∈C.
Clearly, the next property follows from Axioms 2 and 3:
4. (x, αy + βz) = α(x, y) + β(x, z) for all x, y, z ∈Cn and for all α, β ∈C.
1The veriﬁcation of inequality 3 will be carried out in Section3.2.2, p. 72.
2Hermann Minkowski (1864–1909) was a German mathematician.

3.1 Inner Products on Rn and Cn
71
The vector space Cn, together with a speciﬁed inner product on it, is often called
the unitary space Cn.
An inner product on the space Cn can be speciﬁed in inﬁnitely many ways. For
example, we can put
(x, y) =
n

k=1
xk yk.
This inner product on the space Cn is called standard. The veriﬁcation of Axioms
1–3 is trivial. Inner products on Cn can also be speciﬁed similarly to (3.1).
The length of a vector x ∈Cn is deﬁned by the relationship |x| = √(x, x).
Properties of the form 1–3, p. 70, hold.
3.2
Abstract Inner Product Spaces
3.2.1
Deﬁnitions and Examples
An inner product on an abstract real vector space X is a function that assigns to each
pair of vectors x, y ∈X a real number (x, y) and satisﬁes axioms of the form 1–3,
p. 69 (which are called the inner product axioms for a real vector space). An inner
product space is a vector space together with a speciﬁed inner product on that space.
A real inner product space is often called a Euclidean space.
An inner product on the complex vector space X is a function that assigns to
each pair of vectors x, y ∈X a (generally speaking) complex number (x, y) and
satisﬁes axioms of the form 1–3, p. 70 (which are called the inner product axioms
for a complex vector space). A complex inner product space is often referred to as a
unitary space.
The reader can verify the inner product axioms for the following examples.
1. The space V3 together with the usual inner product (the dot product) is a real
inner product space.
2. Let p be an integrable and positive function on an interval (a, b) of the real axis.
Let us specify an inner product on the space C[a, b] by the formula
( f, g) =
b
a
p(x) f (x)g(x)dx,
f, g ∈C[a, b].
(3.2)
The space C[a, b] together with the inner product (3.2) is a Euclidean space.
3. Let us specify an inner product on the space Qn. We assign to each pair of elements
Pn(z) = a0 + a1z + · · · + anzn,
Qn(z) = b0 + b1z + · · · + bnzn
of the space Qn the complex number

72
3
Inner Product Spaces
(Pn, Qn) =
n

j=0
ρ ja jb j,
where ρ0, ρ1, …, ρn are given positive numbers. Together with this inner product,
the space Qn is a unitary space.
An inner product can be speciﬁed on every ﬁnite-dimensional vector space Xn
of dimension n ≥1. Indeed, let {ek}n
k=1 be a basis in Xn and let x =
n
k=1
ξkek,
y =
n
k=1
ηkek be elements of the space Xn. We can take as an inner product on Xn
the function
(x, y) =
n

k=1
ξk ¯ηk,
x, y ∈Xn.
(3.3)
It is easy to see that the function (3.3) satisﬁes the inner product axioms.
3.2.2
The Cauchy–Schwarz Inequality
Suppose that a and b are vectors in the three-dimensional Euclidean space V3, and
the vectors a−b and b are orthogonal, i.e., (a−b, b) = 0.3 Then by the Pythagorean4
theorem,
|a|2 = |a −b|2 + |b|2.
(3.4)
Now suppose that a and b are vectors in an abstract inner product space X such
that (a−b, b) = 0. If we put |v| = √(v, v) for all vectors v ∈X, then the Pythagorean
identity of the form (3.4) holds for vectors in X. Indeed, using elementary calcula-
tions, we get
|a|2 = (a, a) = (a −b + b, a −b+b)
= (a −b, a −b)+(b, b) + (a −b, b) + (b, a −b)
= (a −b, a −b)+(b, b) + (a −b, b) + (a −b, b)
= (a −b, a −b)+(b, b) = |a −b|2 + |b|2.
Theorem 3.1 (Cauchy–Schwarz5 inequality). Let X be an inner product space.
For all vectors x, y ∈X, the following inequality holds:
3We can say that the vector b is the projection of the vector a on the line that is parallel to the
vector b.
4Pythagoras of Samos (570–495 B.C.) was an Ionian Greek philosopher and mathematician.
5Augustin-Louis Cauchy (1789–1857) was a French mathematician, Karl Hermann Amandus
Schwarz (1843–1921) was a German mathematician.

3.2 Abstract Inner Product Spaces
73
|(x, y)|2 ≤(x, x)(y, y).
(3.5)
The two sides in (3.5) are equal if and only if x and y are proportional.
Proof If y = 0, then inequality (3.5) transforms to a trivial equality, and for each
vector x ∈X, the vectors x and y are proportional, since 0x + y = 0. For this reason,
we suppose that y ̸= 0, and put e = |y|−1y. Clearly, (e, e) = 1 and
(x −(x, e)e, (x, e)e) = 0.
Hence in the identity (3.4) we can take a = x, b = (x, e)e and get
|x|2 = |x −(x, e)e|2 + |(x, e)|2.
Therefore, |x|2 ≥|(x, e)|2. The last inequality is equivalent to (3.5). Now we suppose
that |x|2 = |(x, e)|2, i.e., the two sides in (3.5) are equal. Then |x −(x, e)e|2 = 0,
and therefore, x = (x, e)e, i.e., x = ((x, y)/|y|2)y, and thus the vectors x and y are
proportional. Conversely, if the vectors x and y are proportional, then it is easy to
see that the two sides in (3.5) are equal.
□
Thenumber|x| = √(x, x) iscalledthelength ofthevector x ∈X.Inequality (3.5)
often is written in the form
|(x, y)| ≤|x||y| for all
x, y ∈X.
(3.6)
The length of vectors in an abstract inner product space satisﬁes properties that are
analogous to the properties of the length of vectors in three-dimensional Euclidean
space, namely:
1. |x| ≥0 for all x ∈X; |x| = 0 if and only if x = 0;
2. |αx| = |α||x| for all x ∈X and for all α ∈C;
3. |x + y| ≤|x| + |y| for all x, y ∈X.
Inequality 3 is called the triangle inequality (or Minkowski’s inequality).
It is evident that Properties 1 and 2 hold. Let us prove that the triangle inequality
follows from the Cauchy–Schwarz inequality. Indeed,
|x + y|2 = (x + y, x + y) = |x|2 + 2Re(x, y) + |y|2.
Using (3.6), we see that |Re(x, y)| ≤|x||y|, and therefore,
|x + y|2 ≤|x|2 + 2|x||y| + |y|2 = (|x| + |y|)2.
The last inequality is equivalent to inequality 3.
By analogy with three-dimensional Euclidean space V3, we say that two vectors x,
y ∈X are orthogonal if (x, y) = 0.

74
3
Inner Product Spaces
For example, if k ̸= l, then the vectors ik and il ∈Cn are orthogonal with respect
to the standard inner product.
It follows from inequality (3.6) that if X is a real inner product space, then
(x, y)/|x||y| ∈[−1, 1]
for all nonzero vectors x, y ∈X. This fact leads us to introduce the concept of the
angle between two vectors in X. Namely, we assume that the cosine of the angle
between x, y ∈X is equal to (x, y)/|x||y|.
3.2.3
The Gram Matrix
Let {ai}m
i=1 be a set of vectors in an inner product space X. The Gram6 matrix of the
set {ai}m
i=1 is the square matrix of order m of the form
G =
⎛
⎜⎜⎝
(a1, a1) (a2, a1) . . . (am, a1)
(a1, a2) (a2, a2) . . . (am, a2)
. . . . . . . . . . . . .
(a1, am) (a2, am) . . . (am, am)
⎞
⎟⎟⎠.
(3.7)
Note that since (ak, al) = (al, ak), the Gram matrix of any set of vectors is Hermitian
(see p. 48).
Theorem 3.2 A set of vectors {ai}m
i=1 is linearly independent if and only if its Gram
matrix is nonsingular.
Proof Suppose that the Gram matrix G of a set of vectors {ai}m
i=1 is nonsingular.
Then the set {ai}m
i=1 is linearly independent. Indeed, if
x1a1 + x2a2 + · · · + xmam = 0,
then
(x1a1 + x2a2 + · · · + xmam, ak) = 0,
k = 1, 2, . . . , m.
Hence,
x1(a1, ak) + x2(a2, ak) + · · · + xm(am, ak) = 0,
k = 1, 2, . . . , m.
(3.8)
System (3.8) is a homogeneous system of linear algebraic equations for the
unknowns x1, x2, …, xm with the matrix G. Since the Gram matrix G is nonsingular,
system (3.8) has the trivial solution only. Thus, x1 = · · · = xm = 0. Conversely,
suppose that a set of vectors {ai}m
i=1 is linearly independent. Let us construct a linear
6Jørgen Pedersen Gram (1850–1916) was a Danish mathematician.

3.2 Abstract Inner Product Spaces
75
combination of the columns of the matrix G with some coefﬁcients x1, x2, …, xm.
Equating this linear combination to zero, we get
x1(a1, ak) + x2(a2, ak) + · · · + xm(am, ak) = 0,
k = 1, . . . , m.
(3.9)
Multiplying both sides of the kth equality in (3.9) by xk and then adding term by
term all obtained equalities, we get
 m

k=1
xkak,
m

k=1
xkak

= 0.
Therefore,
x1a1 + x2a2 + · · · + xmam = 0.
(3.10)
Sincethesetofvectors{ai}m
i=1 islinearlyindependent,itfollowsfrom (3.10)that x1 =
· · · = xm = 0. Thus we see that if a linear combination of the columns of the matrix
G is equal to zero, then all the coefﬁcients in this linear combination are equal to
zero. This means that the columns of the matrix G are linearly independent, i.e., the
matrix G is nonsingular.
□
Let us examine for linear dependence the vectors
x1 = (1, 3, 3, 1, −2),
x2 = (3, 3, 1, −3, 2),
x3 = (1, 3, −1, 1, 3)
in the space R5. For this purpose we introduce the standard inner product on R5 and
calculate the third-order Gram matrix G = {(xi, x j)}3
i, j=1. By elementary calcula-
tions we get
G =
⎛
⎝
24 8
2
8 32 14
2 14 21
⎞
⎠,
det(G) = 24 650,
i.e., the vectors x1, x2, x3 are linearly independent.
3.2.4
Orthogonal Sets of Vectors. Gram–Schmidt
Orthogonalization Process
A set of vectors {ai}m
i=1 is called orthogonal if all the vectors ai, i = 1, 2, . . . , m,
are nonzero and (ai, ak) = 0 for i ̸= k. The Gram matrix of every orthogonal set is
diagonal and nonsingular. Evidently, every orthogonal set is linearly independent. A
set of vectors {ai}m
i=1 is called orthonormal if (ai, ak) = δik for i, k = 1, 2, . . . , m.
The Gram matrix of every orthonormal set is the identity matrix. The length of each
vector in an orthonormal set is equal to one.

76
3
Inner Product Spaces
The change of basis matrix from one orthonormal basis {ek}n
k=1 to another ortho-
normal basis {˜ek}n
k=1 in an inner product space is unitary. Indeed, writing the equality
En = EnT
(3.11)
in detail, we get ˜ek =
n
j=1
t jke j, k = 1, 2, . . . , n. Therefore,
⎛
⎝
n

j=1
t jke j,
n

j=1
t jle j
⎞
⎠= (˜ek, ˜el) = δkl,
k,l = 1, 2, . . . , n,
since the basis ˜En is orthonormal. Now we transform the left-hand side of the last
equality using the orthonormality of the set En and obtain
n

j=1
t jkt jl = δkl,
k,l = 1, 2, . . . , n.
This means that the matrix T is unitary (see p. 50).
It is important to note that the inverse statement is also true, which can be shown
by arguments similar to those given above. Namely, if the basis En is orthonormal
and the matrix T is unitary, then the basis En = EnT is also orthonormal.
Theorem 3.3 (Gram–Schmidt7 orthogonalization). Every linearly independent
set {ai}m
i=1 is equivalent to an orthonormal set {bi}m
i=1, and the vector b1 may be
chosen proportional to the vector a1.
Proof Put h1 = a1 and h2 = x2,1h1 + a2. The vector h1 is not equal to zero,
since the vector a1 is not equal to zero, as an element of a linearly independent set.
For every coefﬁcient x2,1, the vector h2 is not equal to zero, since h2 is a linear
combination of linearly independent vectors, and one of the coefﬁcients in this linear
combination is not equal to zero (it is equal to one). Now we deﬁne the number x2,1
such that the vector h2 is orthogonal to the vector h1. Writing this condition, we get
0 = x2,1(h1, h1) + (a2, h1), and hence x2,1 = −(a2, h1)/(h1, h1). Thus we have
constructed the vectors h1 and h2 such that (h1, h2) = 0 and h1, h2 ̸= 0. Suppose
that we have constructed the vectors h1, h2, …, hk such that h1, h2, …, hk ̸= 0
and (hi, h j) = 0 for i ̸= j, i, j = 1, . . . , k. We are looking for a vector hk+1 of the
form
hk+1 = xk+1,1h1 + xk+1,2h2 + · · · + xk+1,khk + ak+1.
(3.12)
The vector hk+1 is not equal to zero for all coefﬁcients xk+1,1, …, xk+1,k. Indeed, by
construction, each vector h1, h2, …, hk is a linear combination of the vectors {ai}m
i=1,
and the linear combination h j consists of the vectors of the set {ai}m
i=1 whose indices
7Erhard Schmidt (1876–1959) was a German mathematician.

3.2 Abstract Inner Product Spaces
77
i are less than or equal to j. Therefore, the vector hk+1 is a linear combination of the
linearly independent vectors a1, a2, …, ak+1, and the vector ak+1 is included in this
linear combination with a coefﬁcient that is equal to one.
We deﬁne the numbers xk+1,1, xk+1,2, …, xk+1,k such that the vector hk+1 is
orthogonal to the vectors h1, h2, …, hk. Consistently fulﬁlling these conditions, we
get
xk+1,1 = −(ak+1, h1)/(h1, h1),
xk+1,2 = −(ak+1, h2)/(h2, h2), . . . ,
xk+1,k = −(ak+1, hk)/(hk, hk).
Continuing this process, we construct an orthogonal set of nonzero vectors {hi}m
i=1.
If we take
bi = (|hi|)−1hi,
i = 1, . . . , m,
(3.13)
then we get the orthonormal set of vectors {bi}m
i=1.
As we have established, each vector of the set {hi}m
i=1 is a linear combination of
the vectors {ai}m
i=1. Formula (3.12) shows that each vector of the set {ai}m
i=1 is a linear
combination of the vectors {hi}m
i=1. Formula (3.13) shows that the sets {bi}m
i=1 and
{hi}m
i=1 are equivalent. Thus all three considered sets are equivalent.
Finally, we note that the vectors a1 and b1 are proportional, since by construction,
b1 = (|a1|)−1a1.
□
Remark 3.1 The proof of Theorem 3.3 is constructive. It includes a description of
the algorithm for construction of an orthonormal set of vectors that is equivalent
to a given linearly independent set of vectors. This algorithm is called the Gram–
Schmidt orthogonalization process. Note that for numerical realizations, the Gram–
Schmidt orthogonalization process is used very rarely, since it is strongly inﬂuenced
by rounding errors.
Let us assume, for example, that the polynomials Q0(x) ≡1, Q1(x) = x,
Q2(x) = x2 of a real variable x are given. Using the Gram–Schmidt orthogonaliza-
tion process, we construct polynomials P0, P1, P2 of respective degrees zero, one,
and two that are orthonormal with respect to the inner product deﬁned by the formula
( f, g) =
1
−1
f (x)g(x)dx.
Calculating according to the Gram–Schmidt orthogonalization process, we get
˜P0 = Q0 ≡1,

78
3
Inner Product Spaces
˜P1(x) = Q1(x) −˜P0(x)
1
−1
Q1(x) ˜P0(x)dx
⎛
⎝
1
−1
˜P2
0 (x)dx
⎞
⎠
−1
= x,
˜P2(x) = Q2(x) −˜P0(x)
1
−1
Q2(x) ˜P0(x)dx
⎛
⎝
1
−1
˜P2
0 (x)dx
⎞
⎠
−1
−˜P1(x)
1
−1
Q2(x) ˜P1(x)dx
⎛
⎝
1
−1
˜P2
1 (x)dx
⎞
⎠
−1
= x2 −1/3,
P0(x) = ˜P0(x)
 1
−1
˜P2
0 (x)dx
−1/2
= 1/
√
2,
P1(x) = ˜P1(x)
 1
−1
˜P2
1 (x)dx
−1/2
= x

3/2,
P2(x) = ˜P2(x)
⎛
⎝
1
−1
˜P2
2 (x)dx
⎞
⎠
−1/2
= 1
2

5
2(3x2 −1).
In the same way, we can construct the polynomials P3(x), …, Pn(x) of degree
greater than two, applying the Gram–Schmidt orthogonalization process to the poly-
nomials 1, x, x2, . . . , xn for a given positive integer n. The polynomials
P0(x), P1(x), . . . , Pn(x), . . .
are called the Legendre8 polynomials. The following so-called Rodrigues’s formula9
is valid:
Pk(x) =

2k + 1
2
1
k!2k
dk
dxk (x2 −1)k,
k = 0, 1, . . . .
(3.14)
Using Rodrigues’s formula and the formula for integration by parts, the reader can
prove that
1
−1
Pk(x)Pl(x)dx = 0
k ̸= l, k,l = 0, 1, 2, . . . .
(3.15)
8Adrien-Marie Legendre (1752–1833) was a French mathematician.
9Benjamin Olinde Rodrigues (1794–1851) was a French mathematician.

3.2 Abstract Inner Product Spaces
79
Remark 3.2 Let f1 be a given nonzero vector in an inner product space Xn, n > 1.
Clearly, there exists a vector f2 that is not proportional to f1. If n > 2, we may
take a vector f3 such that the vectors f1, f2, f3 are linearly independent. Continuing
this process, if n > 3, we get a basis in the space Xn that includes the vector f1.
Applying after that the Gram–Schmidt orthogonalization process, we can construct
an orthonormal basis that includes a vector that is proportional to the vector f1.
3.2.5
The Expansion of a Vector with Respect to a Basis
in an Inner Product Space
Let Xn be an inner product space and {ek}n
k=1 a basis of Xn. The coefﬁcients of the
expansion of a vector x ∈Xn with respect to the basis {ek}n
k=1 can be computed as
the solution of a system of linear equations with a Hermitian nonsingular matrix.
Indeed, successively calculating the inner product of both sides of the equality
ξ1e1 + ξ2e2 + · · · + ξnen = x
with the vectors e1, e2, …, en, we get the following system of linear equations:
(e1, e1)ξ1 + (e2, e1)ξ2 + · · · + (en, e1)ξn = (x, e1),
(e1, e2)ξ1 + (e2, e2)ξ2 + · · · + (en, e2)ξn = (x, e2),
· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·
(e1, en)ξ1 + (e2, en)ξ2 + · · · + (en, en)ξn = (x, en).
The matrix of this system is the Gram matrix of the basis {ek}n
k=1. If the basis is
orthogonal, then the matrix is diagonal, and the solution of the system can be easily
calculated:
ξk = (x, ek)
(ek, ek),
k = 1, 2, . . . , n.
(3.16)
Thecoefﬁcients(3.16)arecalledtheFourier10 coefﬁcients ofthevector x withrespect
to the orthogonal set of vectors {ek}n
k=1. Note that if the basis {ek}n
k=1 is orthonormal,
then for every vector x ∈Xn we have the following expansion:
x =
n

k=1
(x, ek)ek.
(3.17)
10Jean Baptiste Joseph Fourier (1768–1830) was a French mathematician and physicist.

80
3
Inner Product Spaces
3.2.6
The Calculation of an Inner Product
Let x and y be vectors in an inner product space Xn. Suppose that we know the
vectors ξ, η ∈Cn of coefﬁcients of the expansions of x and y with respect to a
basis En, i.e., x = Enξ and y = Enη. Then
(x, y) =

n

k=1
ξkek,
n

k=1
ηkek

=
n

k,l=1
ξkηl(ek, el) = (Gξ, η),
(3.18)
where G is the Gram matrix of the basis En, and the parentheses on the right-hand
side of equality (3.18) denote the standard inner product on the space Cn. Therefore,
for the calculation of the inner product (x, y) it is enough to know the coefﬁcients
of the expansions of the vectors x and y with respect to a basis and the Gram matrix
of that basis.
If the basis is orthonormal, then
(x, y) =
n

k=1
ξkηk.
(3.19)
Thus the inner product of vectors can be computed as the standard inner product of
the coefﬁcients of the expansions of these vectors with respect to any orthonormal
basis.
3.2.7
Reciprocal Basis Vectors
Let En={ek}n
k=1 be a basis in an inner product space Xn. It is easy to see that the
equations
(ei, e j) = δi j,
i, j = 1, 2, . . . , n,
(3.20)
uniquelydeﬁnethelinearlyindependentvectorse1,e2,…,en.ThebasisEn={ek}n
k=1 is
reciprocal to the original one. Clearly, the original and the reciprocal bases coincide
if and only if the basis En is orthonormal. Let G be the Gram matrix of the basis En,
and let 
G be the Gram matrix of the basis En. Using elementary calculations, we get
En = EnG, En = En 
G. Therefore, 
G = G−1. The coefﬁcients of the expansions x =
Enξ, y = Enη are the following: ξ k = (x, ek), ηk = (y, ek), k = 1, 2, . . . , n, and
(x, y) =
n

k=1
ξ k ¯ηk.

3.2 Abstract Inner Product Spaces
81
The numbers ξ 1, ξ 2, …, ξ n are called the contravariant components of the vector x,
and the numbers η1, η2, …, ηn are called the covariant components of the vector y.
3.2.8
Examples of Orthogonal Bases
Let us begin with examples of orthogonal bases in the space Cn.
1. The standard basis {ik}n
k=1 is orthonormal with respect to the standard inner
product on Cn (prove it!).
2. The Fourier basis. It is convenient now to number the basis vectors and their
components from 0 to n −1. Recall that the complex numbers
qk = cos 2πk
n
+ i sin 2πk
n ,
k = 0, 1, . . . , n −1,
are the nth roots of unity (see p. 7). As usual, i denotes the imaginary unit. Let
us introduce the set of vectors {ϕk}n−1
k=0 whose components are calculated by the
following formula:
(ϕk) j = q j
k ,
j, k = 0, 1, . . . , n −1.
(3.21)
The set of vectors {ϕk}n−1
k=0 is orthogonal with respect to the standard inner product
on the space Cn. Indeed, ﬁrst of all, we note that qk = qk
1, qk = q−k
1 . Therefore,
calculating the inner product (ϕk, ϕl), we get
(ϕk, ϕl) =
n−1

j=0
q(k−l) j
1
= 1 + (q p
1 ) + (q p
1 )2 + · · · + (q p
1 )n−1,
(3.22)
where p = k −l. For k = l, i.e., for p = 0, we have (ϕk, ϕk) = n. If p ̸= 0, then the
sum on the right-hand side of (3.22) is a geometric progression with ratio q p
1 , and
since |p| = |k −l| < n, we see that q p
1 ̸= 1. Using the formula for the sum of the
ﬁrst n terms of a geometric progression, we obtain
n−1

j=0
(q p
1 ) j = (q p
1 )n −1
q p
1 −1 ,
(3.23)
but (qn
1 )p = q pn
1
= 1, whence (ϕk, ϕl) = 0 for k ̸= l.
Using (3.16), we see that the Fourier coefﬁcients ξ of every vector x ∈Cn with
respect to the basis (3.21) are calculated by the formulas
ξk = (x, ϕk)
(ϕk, ϕk) = 1
n
n−1

j=0
x jq−j
k ,
k = 0, 1, . . . , n −1.
(3.24)

82
3
Inner Product Spaces
The components of the vector x are calculated as follows:
x j =
n−1

k=0
ξkq j
k ,
j = 0, 1, . . . , n −1.
(3.25)
The basis {ϕk}n−1
k=0 is usually called the Fourier basis. It is widely used in digital
(audio and video) signal processing.
In real-life applications, the number n (the length of the processed signal) is very
large, and therefore, special algorithms for calculation of sums of the form (3.25)
and (3.24) are used. They are called the fast Fourier transform (FFT).
Below are examples of orthogonal bases in the space Pn of polynomials with real
coefﬁcients. Let us consider the set of all polynomials of the form
Pn(x) = anxn + an−1xn−1 + · · · + a0,
where the coefﬁcients a0,a1,…,an are real numbers, x is a real variable, n ≥0 is
a given integer. Evidently, this set joined with the zero polynomial is a real vector
space with the usual deﬁnitions of the operations of addition of two polynomials and
multiplication of a polynomial by a real number.
1. The Legendre polynomials. If we specify the inner product on the space Pn by
the formula
( f, g) =
1
−1
f (x)g(x) dx
for all
f, g ∈Pn,
(3.26)
then the Legendre polynomials P0, P1, …, Pn (see (3.14), (3.15), p.78) form an
orthogonal basis in the space Pn.
2. The Chebyshev11 polynomials. Now we specify the inner product on the
space Pn using the relationship
( f, g) =
1
−1
f (x)g(x)
1
√
1 −x2 dx
for all
f, g ∈Pn.
(3.27)
The Chebyshev polynomials are deﬁned by the recurrence relation
T0(x) ≡1, T1(x) = x,
(3.28)
Tk+1(x) = 2xTk(x) −Tk−1(x), k = 1, 2, . . . .
(3.29)
Here k is the degree of the polynomial Tk(x).
11Pafnuty Lvovich Chebyshev (1821–1894) was a Russian mathematician.

3.2 Abstract Inner Product Spaces
83
Let us construct an explicit formula for the Chebyshev polynomials. We look for
the value of the polynomial Tk(x) in the form Tk(x) = λk. Substituting λk for Tk(x)
in the recurrence relation (3.29), we get
λk+1 = 2xλk −λk−1.
Therefore, if λ ̸= 0, then λ satisﬁes the quadratic equation
λ2 −2xλ + 1 = 0.
This equation has the following roots: λ1,2 = x ±
√
x2 −1. Hence the functions
T (1)
k
(x) = (x +

x2 −1)k,
T (2)
k
(x) = (x −

x2 −1)k,
and as a consequence, the functions
Tk(x) = c1T (1)
k
(x) + c2T (2)
k
(x),
k = 0, 1, . . . ,
satisfy (3.29). Here c1 and c2 are arbitrary real numbers. The numbers c1 and c2 are
deﬁned by conditions (3.28):
c1 + c2 = 1,
(c1 + c2)x + (c1 −c2)

x2 −1 = x.
Therefore, c1 = c2 = 1/2, i.e., the polynomials
Tk(x) = 1
2

x +

x2 −1
k
+ 1
2

x −

x2 −1
k
,
k = 0, 1, 2, . . . ,
satisfy (3.29) and (3.28). For |x| ≤1, the Chebyshev polynomials can be written in
a more compact form. In this case, we can put x = cos ϕ. Then
Tk(x) = 1
2 (cos ϕ + i sin ϕ)k + 1
2 (cos ϕ −i sin ϕ)k ,
and using de Moivre’s formula (see (1.23), p.6), we get Tk(x) = cos kϕ, and hence
Tk(x) = cos(k arccos x).
(3.30)
The Chebyshev polynomials are orthogonal with respect to the inner prod-
uct (3.27). Indeed, using (3.30), we can write
(Tk, Tl) =
1
−1
cos(k arccos x) cos(l arccos x)
√
1 −x2
dx.

84
3
Inner Product Spaces
If we put x = cos ϕ, then using elementary calculations, we get
(Tk, Tl) =
π
0
cos kϕ coslϕ dϕ = 0,
k ̸= l.
Thus the Chebyshev polynomials T0, T1, …, Tn form an orthogonal basis with respect
to the inner product (3.27) on the space Pn of polynomials with real coefﬁcients.
3.3
Subspaces
3.3.1
The Sum and Intersection of Subspaces
A set L of elements in a vector space X is a subspace of X if αx + βy ∈L for all
x, y ∈L and for all complex numbers α, β. Trivial examples of subspaces are the
following: the space X itself is a subspace; the set consisting only of the zero vector
is a subspace. Every subspace L includes the zero vector, since by deﬁnition, for
every x ∈L the vector 0x belongs to L.
The proof of the two following theorems is left to the reader.
Theorem 3.4 Let a1, a2, …, am, m ≥1, be given vectors in a vector space X. The
set of all linear combinations x1a1 + x2a2 + · · · + xmam is a subspace of X. This
subspace is called the span of a1, a2, …, am and is denoted by span{a1, a2, . . . , am}.
Theorem 3.5 Let a1, a2 be given vectors in a vector space X, and a2 ̸= 0. The set L
of all vectors of the form a1 + αa2, where α ∈C, is called the line passing through
the point a1 and parallel to the vector a2. The set L is a subspace if and only if the
vectors a1, a2 are linearly dependent.
Let L1, L2 be subspaces of a vector space X. The set L of all vectors of the
form a1 + a2, where a1 ∈L1, a2 ∈L2, is called the sum of the subspaces L1 and L2
and is denoted by L = L1 + L2. The set L is a subspace. Indeed, let x, y ∈L.
This means that there exist vectors a1, b1 ∈L1,a2, b2 ∈L2 such that x = a1 + a2
and y = b1 + b2. Let α, β be arbitrary complex numbers. Then
αx + βy = α(a1 + a2) + β(b1 + b2) = (αa1 + βb1) + (αa2 + βb2).
Since L1 is a subspace, the vector αa1+βb1 belongs to L1. Similarly, the vector αa2+
βb2 belongs to L2. Therefore, the vector αx + βy belongs to L.
The intersection of the subspaces L1 and L2, i.e., the set L1 ∩L2 of all vectors
that are elements of both L1 and L2, is also a subspace of X. Indeed, let there be
given vectors x, y ∈L1 ∩L2. For every complex number α, the vector αx belongs to
both L1 and L2, i.e., αx ∈L1 ∩L2. Similarly, for every β, the vector βy belongs to

3.3 Subspaces
85
L1∩L2. Hence evidently, αx +βy ∈L1∩L2. A set of vectors {ek}m
k=1 ⊂L is a basis
of a subspace L if those vectors are linearly independent and every vector x ∈L
can be represented as a linear combination of the vectors {ek}m
k=1. The number m is
called the dimension of the subspace L and is denoted by dim L.
The subspace consisting only of the zero vector is called the zero subspace and
is denoted by {0}. As usual, we assume that dim{0} = 0.
The reader is now invited to describe all possible subspaces of the space V3.
A subspace L of a ﬁnite-dimensional space Xn coincides with Xn if and only if
dim L = n. This statement immediately follows from the fact that every set of n
linearly independent vectors in the space Xn forms a basis in this space (see Theo-
rem 2.8, p. 65).
Evidently, a given basis {ek}m
k=1 of a subspace L ⊂Xn can be joined with some
vectors to complete a basis {ek}n
k=1 of the space Xn. Similarly, if L1 and L2 are
subspaces, and L1 ⊂L2, then dim L1 ≤dim L2, and every basis of L1 can be joined
with some elements of L2 to complete the basis in the subspace L2.
The sum of the subspaces L1 and L2 is called direct if the components x1 ∈L1
and x2 ∈L2 of each vector x = x1 + x2 ∈(L1 + L2) are uniquely determined. The
direct sum of subspaces L1 and L2 is denoted by L1 ⊕L2.
Theorem 3.6 The sum of two subspaces L1 and L2 is direct if and only if it follows
from the equality
x1 + x2 = 0,
x1 ∈L1, x2 ∈L2,
(3.31)
that x1 = 0, x2 = 0.
Proof Suppose that x1 = 0, x2 = 0 follow from (3.31). Let us prove that the
components x1 ∈L1 and x2 ∈L2 of each vector x = x1 + x2 ∈(L1 + L2) are
uniquely determined. Suppose that there exists one more expansion of the vector x,
i.e., we have x = ˜x1+ ˜x2, ˜x1 ∈L1, ˜x2 ∈L2. Then evidently, (x1−˜x1)+(x2−˜x2) = 0.
Since x1 −˜x1 ∈L1, x2 −˜x2 ∈L2, we see that x1 −˜x1 = 0, x2 −˜x2 = 0, and
therefore, x1 = ˜x1, x2 = ˜x2. Conversely, suppose that the components x1 ∈L1
and x2 ∈L2 of each vector x = x1 + x2 ∈(L1 + L2) are uniquely determined,
and let x1 + x2 = 0 for some x1 ∈L1, x2 ∈L2. Since 0 + 0 = 0, we have
x1 = x2 = 0.
□
Theorem 3.7 The sum of two subspaces L1 and L2 is direct if and only if
L1 ∩L2 = {0}.
Proof Let L1 ∩L2 = {0}, x1 + x2 = 0, x1 ∈L1, x2 ∈L2. Since x1 = −x2, we
have x1 ∈L2. Hence x1 ∈L1 ∩L2. Therefore, x1 = 0, and evidently, x2 = 0.
Conversely, let x ∈L1 ∩L2. Then x ∈L1, x ∈L2, and moreover, it is obvious that
x + (−x) = 0. Since the sum of L1 and L2 is direct, using Theorem 3.6, we get
x = 0, and thus L1 ∩L2 = {0}.
□
The reader may supply a proof of the next theorem.

86
3
Inner Product Spaces
Theorem 3.8 Let L be a subspace of a ﬁnite-dimensional vector space Xn. Then
there exists a subspace M ⊂Xn such that Xn = L ⊕M.
Let L1 and L2 be subspaces of an inner product space. If (x, y) = 0 for all
x ∈L1 and y ∈L2, then we say that the subspaces L1 and L2 are orthogonal and
write L1⊥L2. The sum of orthogonal subspaces is called an orthogonal sum.
Every orthogonal sum is direct. Indeed, let L1⊥L2, x1 ∈L1, x2 ∈L2, x1+x2 = 0.
Since the vectors x1 and x2 are orthogonal, using the Pythagorean identity, we see
that |x1 + x2|2 = |x1|2 + |x2|2. Hence, |x1|2 + |x2|2 = 0, and x1 = x2 = 0.
The concepts of the direct sum and the orthogonal sum are applied in a nat-
ural way to the case of any ﬁnite number of subspaces. Namely, the sum of sub-
spaces L1, L2, . . . , Lk in an inner product space is called orthogonal if Li⊥L j
for i ̸= j, i, j = 1, 2, . . . , k. Theorem 3.6 is easily generalized to the case of any
ﬁnite number of subspaces.
The reader can prove that every orthogonal sum of a ﬁnite number of subspaces
is direct, i.e., the components x j ∈L j, j = 1, 2, . . . , k, of every vector x in the sum
are uniquely determined.
The reader can also answer the next question. Is it true that the sum of subspaces
L1 + L2 + · · · + Lk, k > 2, is direct if their intersection is the zero subspace?
3.3.2
The Dimension of the Sum of Subspaces
Theorem 3.9 If L = L1 ⊕L2 ⊕· · · ⊕Lk is the direct sum of ﬁnite-dimensional
subspaces L1, L2,…, Lk of a vector space X, then
dim L = dim L1 + dim L2 + · · · + dim Lk.
(3.32)
Proof Let us prove the theorem for the case k = 2. For an arbitrary k, the proof is
analogous. Let
f1, f2, . . . , f p;
g1, g2, . . . , gq
(3.33)
be bases of the subspaces L1 and L2, respectively. Then the union of these two sets is
a basis of the subspace L1 ⊕L2. Indeed, for every x ∈L1 ⊕L2, we have x = x1 +x2,
where
x1 = α1 f1 + α2 f2 + · · · + αp f p ∈L1,
x2 = β1g1 + β2g2 + · · · + βqgq ∈L2,
and if x = 0, then x1 = x2 = 0, since the sum L1 ⊕L2 is direct. Hence all the
numbers α1, α2, . . . , αp, β1, β2, . . . , βq are equal to zero, since { fk}p
k=1, {gk}q
k=1 are
bases. Thus the set of vectors (3.33) is linearly independent. It is clear now that
dim(L1 ⊕L2) = p + q.
□

3.3 Subspaces
87
Theorem 3.10 If L1 and L2 are arbitrary ﬁnite-dimensional subspaces of a vector
space X, then
dim(L1 + L2) = dim L1 + dim L2 −dim(L1 ∩L2).
(3.34)
Proof Obviously, the space G = L1 ∩L2 is ﬁnite-dimensional. Suppose that a
set Gl = {gi}l
i=1 is a basis of G, the union of Gl and vectors Fk = { fi}k
i=1 is a basis of
the subspace L1, and the union of Gl and the vectors Hm = {hi}m
i=1 is a basis of the
subspace L2. Let F be the span of Fk and let H be the span of Hm. We shall prove
that
L1 + L2 = F + G + H.
(3.35)
Indeed, if x ∈L1 + L2, then x = x1 + x2, where x1 ∈L1, x2 ∈L2. Clearly,
x1 = f + g−, x2 = h + g+, where f ∈F, h ∈H, g+, g−∈G. Therefore,
x = f + g + h, where g = g+ + g−∈G. Thus, x ∈F + G + H. It is easier to
prove that if x ∈F + G + H, then x ∈L1 + L2. The sum on the right-hand side
of (3.35) is direct. In fact, suppose that f + g + h = 0, where f ∈F, g ∈G,
h ∈H. Let us show that f, g, h = 0. We have f + g = −h. Clearly, −h ∈L2,
and f + g ∈L1, and therefore, f + g ∈G, h ∈G. If we put h + g = g, then
f + g = 0 and g ∈G. Since the set of vectors Fk ∪Gl is linearly independent,
we obtain f = 0, g = 0. Similarly, h = 0, g = 0. Using Theorem 3.9, we get
dim(L1 + L2) = dim(F ⊕G ⊕H) = k + l + m, but dim L1 = k + l, dim L2 =
l + m, and dim(L1 ∩L2) = l. Finally, let us note that k + l + m = (k + l) +
(l + m) −l.
□
Corollary 3.1 Suppose that L1, L2 are subspaces of an n-dimensional space Xn,
and dim L1 + dim L2 > n. Then L1 ∩L2 ̸= {0}.
Proof Since L1+L2 is a subspace of Xn, we get dim(L1+L2) ≤n, and using (3.34),
we see that dim(L1 ∩L2) = dim L1 + dim L2 −dim(L1 + L2) ≥1.
□
3.3.3
The Orthogonal Projection of a Vector onto a Subspace
Let L be a subspace of an
inner product space X and let x be a vector in X.
A vector y ∈L is the best approximation of x if
|x −y| ≤|x −z| for all z ∈L.
(3.36)
Theorem 3.11 Let L be a ﬁnite-dimensional subspace of X. Then for every x ∈X,
there exists a unique best approximation of x in L.
Proof If L = {0}, then the unique best approximation of x is the zero vector. There-
fore we assume that L ̸= {0}. Let y, z ∈L. If we write z in the form z = y + h,
where h ∈L, then

88
3
Inner Product Spaces
(x −z, x −z) = (x −y −h, x −y −h)
= (x −y, x −y) −(x −y, h) −(h, x −y) + (h, h).
Hence if (x −y, h) = 0 for all h ∈L, then condition (3.36) holds. Conversely,
if (3.36) holds, then
−(x −y, h) −(h, x −y) + (h, h) ≥0 for all h ∈L.
Substituting h1 = ((x −y, h)/|h|2)h for h, we get −|(x −y, h)|2/|h|2 ≥0, there-
fore, (x −y, h) = 0. Thus, y ∈L is the best approximation of x ∈X if and
only if
(x −y, h) = 0 for all h ∈L.
(3.37)
In other words, the vector x −y is orthogonal to the subspace L. Geometrically,
this conclusion is quite obvious (make a drawing!). If a vector y satisfying condi-
tion (3.37) exists, then it is uniquely determined by the vector x. Indeed, let there exist
an additional vector ˜y ∈L such that (x−˜y, h) = 0 for all h ∈L. Then (y−˜y, h) = 0
for all h ∈L. If we take h = y −˜y, then we get y = ˜y.
We shall prove now that a vector y ∈L satisfying condition (3.37) exists.
Let {ek}m
k=1 be a basis of the subspace L. Condition (3.37) is equivalent to the fol-
lowing:
(x −y, ek) = 0,
k = 1, 2, . . . , m.
(3.38)
We seek the vector y in the form y =
m
i=1
ηiei. It follows from (3.38) that

m

i=1
ηiei, ek

= (x, ek),
k = 1, 2, . . . , m.
The last condition gives a system of linear equations with unknowns η1, η2, . . . , ηm:
m

i=1
ηi(ei, ek) = (x, ek),
k = 1, 2, . . . , m.
(3.39)
The matrix of this system is the Gram matrix of the basis {ek}m
k=1. This matrix is
nonsingular (see Theorem 3.2, p. 74). Therefore, system (3.39) has a unique solution
for each x ∈X, i.e., condition (3.37) uniquely determines the vector y.
□
Remark 3.3 If the basis {ek}m
k=1 of the subspace L is orthonormal, then the vector y
can be easily calculated, namely, in this case we get y =
m
k=1
(x, ek)ek.
It is natural that the vector y satisfying condition (3.37) is called the orthogonal
projection of the vector x onto the subspace L and that the vector z = x −y is called
the perpendicular dropped from the point x to the subspace L.

3.3 Subspaces
89
Note that (x −y, y) = 0, since y ∈L. Therefore, the Pythagorean identity (see
Section3.2.2, p.72) holds:
|x|2 = |x −y|2 + |y|2.
(3.40)
It follows from (3.40) that |y|2 ≤|x|2. This is the so-called Bessel’s12 inequality,
which shows that the length of the projection of a vector is less than or equal to the
length of the vector.
If the set of vectors {ek}m
k=1 is orthonormal, then Bessel’s inequality has the form
m

k=1
|(x, ek)|2 ≤|x|2
for all
x ∈X.
(3.41)
The two sides in (3.41) are equal if and only if x ∈L, i.e., if x =
m
k=1
(x, ek)ek.
Note that the Cauchy–Schwarz inequality (3.5), p.73, can be interpreted as a
special case of Bessel’s inequality (3.41) in which the orthonormal set of vectors
consists of only one vector e1 = |y|−1y, y ̸= 0.
For example, let L be the subspace of the space R4 spanned by the vectors
a1 = (−3, 0, 7, 6), a2 = (1, 4, 3, 2), and a3 = (2, 2, −2, −2). Let us calculate
the orthogonal projection of the vector x = (14, −3, −6, −7) onto the subspace L
and the perpendicular dropped from the point x to the subspace L.
The vectors a1 and a2 are linearly independent, and the vector a3 is the linear
combination of a1 and a2, namely, a3 = (−1/2)a1 + (1/2)a2. Hence the vectors a1,
a2 form a basis of the subspace L. The components η1, η2 of the vector y (which is
the projection of x onto L) with respect to the basis a1, a2 can be computed as the
solution of the system of equations
η1(a1, a1) + η2(a2, a1) = (x, a1),
(3.42)
η1(a1, a2) + η2(a2, a2) = (x, a2).
(3.43)
Computing the inner products, we get (a1, a1) = 9 + 49 + 36 = 94, (a2, a1) = 30,
(a2, a2) = 30, (x, a1) = −126, (x, a2) = −30. Solving system (3.42), (3.43), we
obtain η1 = −3/2, η2 = 1/2, i.e., y = (−3/2)a1 + (1/2)a2 = (5, 2, −9, −8) is
the orthogonal projection of the vector x onto the subspace L, and z = x −y =
(9, −5, 3, 1) is the perpendicular dropped from the point x to the subspace L.
A bad choice of basis in the subspace L can cause great computational difﬁculties
in the practical calculation of the element of best approximation. Here is a relevant
example.LetusspecifytheinnerproductinthespaceC[0, 1]ofcontinuousfunctions,
using the formula
12Friedrich Wilhelm Bessel (1784–1846) was a German mathematician and astronomer.

90
3
Inner Product Spaces
Fig. 3.1 Example of an almost linearly dependent basis. The plot of the function ϕ is indicated
by the solid line; the plots of the approximating polynomial are indicated by the symbols “+” (for
ε = 5 · 10−4) and “∗” (for ε = 2 · 10−4)
( f, g) =
1

0
f (x)g(x)dx,
f, g ∈C[0, 1].
(3.44)
We shall consider the ﬁve-dimensional subspace of C[0, 1] spanned by the basis that
consists of the functions ϕ0(x) ≡1, ϕ1(x) = x, ϕ2(x) = x2, ϕ3(x) = x3, ϕ4(x) = x4
and calculate the best approximation of the function ϕ(x) = x5.
The Gram matrix in this case is easily calculated:
1

0
ϕk(x)ϕl(x)dx = 1/(k + l + 1),
k, l = 0, 1, . . . , 4.
(3.45)
Evidently, the right-hand-side column of the system of linear equations (3.39) is
equal to (1/6, 1/7, 1/8, 1/9, 1/10)T . We assume that the last element of the right-
hand-side column was calculated with a computational error, and substitute the num-
ber (1/10) + ε for 1/10.
Figure3.1 shows a plot of the function ϕ(x) and plots of the approximating poly-
nomial P4(x) = η0 +η1x +η2x2 +η3x3 +η4x4 for different values of ε. We see that
signiﬁcant errors in the approximation of the function ϕ correspond to small errors of
the computation of the right-hand side (which are inevitable in practice). The reason
for this effect is that the selected basis is almost linearly dependent. To verify this,
just look at the plots of the functions x p, p = 1, 2, . . . , on the interval [0, 1]. These
plots are similar even if the numbers p are not very large. Therefore, the matrix of
system (3.39) is almost singular (it is also said to be ill-conditioned).

3.3 Subspaces
91
The matrix with elements (3.45), i.e., the matrix of the form
Hn =

1
i + j −1
n
i, j=1
(3.46)
is called a Hilbert13 matrix. It is often applied in various areas of mathematics. Even
for n > 10, this matrix is so ill conditioned that the corresponding system is not
practically solvable by a computer.
Remark 3.4 Usually, orthogonal bases (for example, the Legendre polynomials or
the Chebyshev polynomials; see pp. 79, 82) are used for approximations of functions
by polynomials. In this case, system (3.39) is diagonal.
3.3.4
The Orthogonal Decomposition of an Inner
Product Space
Let L be a subspace of an inner product space X. The set of all vectors in X that
are orthogonal to L is called the orthogonal complement of the subspace L and is
denoted by L⊥. The reader can easily prove that L⊥is a subspace of the space X.
Theorem 3.12 (orthogonal decomposition). Let L be a ﬁnite-dimensional sub-
space of an inner product space X and let L⊥be the orthogonal complement of the
subspace L. Then the space X is the orthogonal sum of the subspaces L and L⊥, i.e.,
X = L ⊕L⊥.
(3.47)
Proof Using Theorem 3.11, we see that for each x ∈X there exists y ∈L such
that (x −y, h) = 0 for all h ∈L. Therefore, z = x −y ∈L⊥and x = y + z, which
means (see Section3.3.1, p.84) that decomposition (3.47) is valid.
□
Let e ∈X, e ̸= 0. Denote by πe the set of all vectors in the space X that are
orthogonal to e. It is easy to see that πe is a subspace of X. This subspace is called
the hyperplane orthogonal to the vector e.
Theorem 3.13 Let x be an arbitrary vector and e a nonzero vector in an inner
product space Xn. Then there exist a vector y ∈πe and a number μ such that
x = μe + y.
(3.48)
13David Hilbert (1862–1943) was a German mathematician.

92
3
Inner Product Spaces
The number μ and the vector y are uniquely determined by the vector x. Moreover,
|x −y| ≤|x −z| for all z ∈πe,
(3.49)
i.e., y is the element of best approximation of x in the subspace πe.
The reader is invited to prove Theorem 3.13 (hint: use the idea of the proof of
Theorem 3.12).

Chapter 4
Linear Operators
In this chapter we introduce the concept of a linear operator deﬁned on a linear
space. We study basic properties of linear operators acting on ﬁnite-dimensional
linear spaces. We give a detailed investigation of their spectral properties. Special
attention is paid to the study of the structure of the main classes of linear operators
in ﬁnite-dimensional Euclidean and unitary spaces.
4.1
Linear Operators and Their Basic Properties
4.1.1
Basic Deﬁnitions. Operations with Operators
Let X,Y be linear spaces. We say that ϕ is a map from X to Y and write ϕ : X →Y if
a unique vector ϕ(x) ∈Y is deﬁned for every x ∈X. We also say in this case that the
function ϕ with values in the space Y is deﬁned on the space X and write x →ϕ(x).
We note that at the same time, not every vector in Y is necessarily the result of the
mapping ϕ of some vector x ∈X.
We say that a map ϕ is linear if for every x, y ∈X and for scalars α, β, we have
ϕ(αx + βy) = αϕ(x) + βϕ(y).
(4.1)
In linear algebra, almost all mappings are linear, and they are called linear operators,
or in some contexts simply operators. Usually operators are denoted by capital letters.
For example, the relationship (4.1) for a linear operator A will be written as
A(αx + βy) = αAx + βAy.
Using the deﬁnition of a linear mapping, we see that A0 = 0 for every operator A.
© Springer International Publishing AG 2017
L. Beilina et al., Numerical Linear Algebra: Theory and Applications,
DOI 10.1007/978-3-319-57304-5_4
93

94
4
Linear Operators
If an operator maps a space X into the same space X, then we say that it acts on
the space X or that the operator is a transformation of the space X.
If some basis {e j}n
j=1 is ﬁxed in a ﬁnite-dimensional space Xn, then to deﬁne a
linear operator A on Xn, it is enough to describe the action of the operator on all the
basis vectors, since for every vector x =
n
j=1
ξ je j, we have Ax =
n
j=1
ξ jAe j.
Operations with operators.
We deﬁne a linear combination of two operators A : X →Y and B : X →Y as
a mapping αA + βB : X →Y given by
(αA + βB)x = α(Ax) + β(Bx) for all
x ∈X,
(4.2)
where α and β are scalars. We deﬁne the product of two operators A : X →Y
and B : Y →Z as the mapping BA : X →Z given by
BAx = B(Ax) for all
x ∈X.
(4.3)
The reader can easily prove that αA + βB and BA are linear operators.
The product of a ﬁnite number of operators is deﬁned in the same way. The reader
will have no difﬁculty in showing that if the product of operators C, B, A is deﬁned,
then CBA = C(BA) = (CB)A.
Examples of linear operators.
1. The null operator 0 : X →Y is deﬁned by 0x = 0 for all x ∈X. This operator
transforms every vector of the space X into the zero vector of the space Y.
2. The identity operator I : X →X is deﬁned by I x = x for all x ∈X. This
operator transforms every vector of the space X into itself.
3. The projection operator. Let the linear space X be a direct sum of subspaces L
and M.Theneveryvector x ∈Xcanbewrittenintheform x = x1+x2,where x1 ∈L,
x2 ∈M, and the vectors x1, x2 are uniquely deﬁned by the vector x. Let us deﬁne
the operator P : X →L such that Px = x1. The operator P is called the projection
operator onto the subspace L (in parallel with the subspace M). If X is an inner
product space and can be represented as an orthogonal sum of subspaces L and M,
then the operator P is called the operator of the orthogonal projection.
Let us prove that the operator P is linear. Suppose x = Px +x2 and y = Py + y2,
where x, y ∈X, x2, y2 ∈M. Then for all scalars α, β we have
αx + βy = αPx + βPy + αx2 + βy2.
Since L and M are subspaces, we have αPx + βPy ∈L, αx2 + βy2 ∈M, and
thus, P(αx + βy) = αPx + βPy.

4.1 Linear Operators and Their Basic Properties
95
InthesamemannerwecanintroducethelinearoperatorQthatprojectsthespaceX
onto the subspace M. We can easily obtain the following equalities: P + Q = I,
PQ = 0, QP = 0, P2 = P, Q2 = Q. Generally, if the space X is a direct sum of
several subspaces,
X = L1 ⊕L2 ⊕· · · ⊕Lk,
and Pi is the projection operator onto the subspace Li, i = 1, 2,…, k, then
P1 + P2 + · · · + Pk = I,
P2
i = Pi,
PiP j = 0 for i ̸= j,
(4.4)
where i, j = 1, 2,…, k.
4. Matrix–vector multiplication. Let A(m, n) be a rectangular matrix. Deﬁne the
map A : Cn →Cm by the rule
y = Ax.
(4.5)
Matrix–vector multiplication is a linear operation (see Section1.2.4, p. 34). There-
fore, the operator A : Cn →Cm deﬁned by (4.5) is linear.
4.1.2
The Inverse Operator
A linear operator A:X→Y is called invertible if there exists a map B: Y→X such
that
BAx = x
for all
x ∈X,
(4.6)
ABy = y
for all
y ∈Y.
(4.7)
A map B : Y →X satisfying (4.6) and (4.7) is called an inverse of the map A.
Let us check that if an inverse B of the map A exists, then B is a linear operator.
Let y1, y2 ∈Y, α, β ∈C. Take x1 = By1, x2 = By2. Then Ax1 = ABy1 = y1
and Ax2 = ABy2 = y2. Therefore,
B(αy1 + βy2) = B(αAx1+βAx2)
= BA(αx1 + βx2) = αx1 + βx2 = αBy1 + βBy2.
We claim that if a linear operator A : X →Y is invertible, then that operator
is a bijective map acting from the space X to the space Y. In fact, ﬁrst let x1, x2 be
two vectors in X such that x1 ̸= x2. Then Ax1 ̸= Ax2. Indeed, if we assume that
Ax1 = Ax2, then BAx1 = BAx2, and hence x1 = x2. Second, if y ∈Y, then for
x = By we have Ax = ABy = y, i.e., every vector y ∈Y is the result of the
mapping A of some vector x ∈X.

96
4
Linear Operators
It is easy to see that if a linear operator A is invertible, then it has a unique inverse
(check it!). The inverse operator to the operator A is denoted by A−1. By deﬁnition, if
the operator A−1 exists, then (A−1)−1 = A.
Examples.
1. The identity operator is invertible with I −1 = I.
2. Obviously, the null operator acting from Xn into Ym, where m ≥1, is a nonin-
vertible operator.
3. If L ̸= X, then the projection operator P : X →L is a noninvertible operator.
4. Every square matrix A of order n deﬁnes a linear operator acting on the space Cn.
If the matrix A is nonsingular, then the operator is invertible. The inverse operator
is deﬁned by the inverse matrix A−1 (see p.40).
Let A : X →Y, B : Y →Z be invertible operators. The reader can easily show
that the operator BA is invertible and (BA)−1 = A−1B−1.
4.1.3
The Coordinate Representation Operator
Let Xn be an n-dimensional linear space, and let En = {ek}n
k=1 be a basis for Xn.
Deﬁne an operator that maps the space Cn onto the space Xn by the rule
x = Enξ,
ξ ∈Cn.
(4.8)
Evidently, this operator is linear, and we denote it by E.
If x ∈Xn is a given vector, then there exists a unique representation x =
n
k=1
ξkek,
because En is a basis. The scalars ξk, k = 1, 2, . . . , n, are the coordinates of x with
respect to the basis En, and the vector ξ ∈Cn is the unique coordinate representation
of x. Given the basis En, the following linear mapping from Xn to Cn is well deﬁned:
x →ξ,
where x = Enξ.
We call this map the coordinate representation operator and denote it by E−1.
Using the deﬁnitions of the operators E and E−1, we get
E−1Eξ = ξ
for all ξ ∈Cn,
EE−1x = x
for all
x ∈Xn,
i.e., the operators E and E−1 are mutually inverse.
Usually, to calculate a coordinate representation of x, it is necessary to solve a v
system of linear algebraic equations with a square nonsingular matrix (see pp. 64,
67, 79). If Xn is an inner product space and En is an orthonormal basis, then the
coordinate representation of x can be calculated more easily (see (3.17), p. 80, and
the examples on pp. 81, 82).

4.1 Linear Operators and Their Basic Properties
97
4.1.4
Isomorphism of Finite-Dimensional Linear Spaces
We say that two linear spaces X, Y are isomorphic if there exists an invertible linear
operator such that A : X →Y. In other words, linear spaces X and Y are isomorphic
if there exists a linear bijective correspondence between X and Y. We also say in this
case that the mapping A of X onto Y is an isomorphism between X and Y.
Obviously, isomorphisms between linear spaces have the property of transitivity,
i.e., if X, Z are isomorphic and Y, Z are isomorphic, then X, Y are also isomorphic.
Theorem 4.1 All ﬁnite-dimensional complex linear spaces of the same dimension
are isomorphic to each other.
Proof By transitivity, it is sufﬁcient to prove that every n-dimensional complex
linear space Xn is isomorphic to the space Cn. Let En be a basis of Xn. Then the
coordinate representation operator E−1 realizes an isomorphism between Xn and Cn
(see Section4.1.3).
□
For the same reason, all n-dimensional real linear spaces are isomorphic to the
space Rn.
Theorem 4.2 If two ﬁnite-dimensional linear spaces X, Y are isomorphic, then they
have the same dimension.
Proof Let {ek}n
k=1 be a basis of X. Suppose that a linear operator A is a bijective map
from the space X to the space Y. Writing
n
k=1
αkAek = 0, we obtain A
n
k=1
αkek = 0.
Acting on both sides of the last equality by the operator A−1, we get
n
k=1
αkek = 0.
Therefore, α1, α2, . . . , αn = 0, i.e., the vectors {Aek}n
k=1 are linearly independent
elements of the space Y. Hence the dimension of the space X is greater than or equal
to n. Exchanging the roles of the spaces X and Y, we get that they have the same
dimension.
□
Consequently, we have the following result.
Theorem 4.3 Two ﬁnite-dimensional complex (or real) linear spaces are isomor-
phic if and only if they have the same dimension.
If linear spaces X, Y are isomorphic, then there exists a bijective correspondence
between the linear operations with elements in X and the linear operations with
elements in Y. Particularly, if a complex (or real) linear space X is ﬁnite-dimensional,
then by introducing a basis for X, the linear operations with elements in X can be
replaced by the linear operations with vectors in the space Cn (or Rn).

98
4
Linear Operators
4.1.5
The Matrix of a Linear Operator
Let A : Xn →Ym be a linear operator. Suppose that En = {ek}n
k=1 is a basis of Xn
and Qm = {qk}m
k=1 is a basis of Ym. For each i = 1, 2, . . . , n, the vector Aei can be
uniquely expanded in terms of the basis Qm:
Aei =
m

j=1
a(eq)
ji q j,
i = 1, 2, . . . , n.
(4.9)
Consider the matrix
Aeq =
⎛
⎜⎜⎜⎝
a(eq)
11
a(eq)
12
. . . a(eq)
1n
a(eq)
21
a(eq)
22
. . . a(eq)
2n
. . . . . . . . . . . . . . . . .
a(eq)
m1 a(eq)
m2 . . . a(eq)
mn
⎞
⎟⎟⎟⎠
(4.10)
(theith column of Aeq consists of the coordinates of Aei with respect to the basis Qm).
The matrix Aeq is called the matrix of the operator A. This matrix is uniquely
determined by the operator A and by the bases En, Qm. We denote an operator and
the corresponding matrix by the same letter in different typefaces. Subscripts in the
notation of the matrix of an operator indicate which bases were used to construct the
matrix.
Note that we can write relations (4.9) more concisely:
AEn = Qm Aeq.
(4.11)
Suppose that x = Enξ ∈Xn, ξ ∈Cn. We can expand the vector Ax in terms of
the basis Qm: Ax = Qmη, η ∈Cm. Then, using (4.11), we get
Qmη = Ax = AEnξ = Qm Aeqξ,
and therefore,
η = Aeqξ.
(4.12)
Relationship (4.12) shows the dependence between the coordinates of the vectors x
and Ax with respect to the bases of the linear spaces Xn and Ym, respectively.
It follows from (4.12) that if the matrix Aeq of the operator A is known, then we
can construct the vector Ax ∈Ym corresponding to the vector x ∈Xn in the following
way.
1. Calculate the coordinates ξ ∈Cn of x with respect to the basis En. Using the
coordinate representation operator E−1, we can write ξ = E−1x (see Section4.1.3).
2. Using (4.12), calculate the coordinates η ∈Cm of y = Ax ∈Ym with respect
to the basis Qm.

4.1 Linear Operators and Their Basic Properties
99
3. Calculate the vector y by the formula y = Qη. Here Q is the operator deﬁned
by the rule analogous to (4.8).
The above implies that using the operators E and Q constructed by the bases En
and Qm, we can write (4.11) in the following equivalent forms:
Aeq = Q−1AE
or A = QAeqE−1.
(4.13)
To be precise, equalities (4.13) mean that
Aeqξ = Q−1AEξ
for all ξ ∈Cn,
Ax = QAeqE−1x
for all
x ∈Xn.
(4.14)
Equalities (4.13), (4.14) are illustrated by the following diagrams:
Xn
A
−−−−→Ym
E

⏐⏐
⏐⏐Q−1
Cn −−−−→
Aeq
Cm
Xn
A
−−−−→Ym
E−1
⏐⏐

⏐⏐Q
Cn −−−−→
Aeq
Cm
Therefore, if some bases En and Qm were chosen for the spaces Xn and Ym,
then to each linear operator A : Xn →Ym there uniquely corresponds the linear
operator Aeq : Cn →Cm. This is the matrix–vector multiplication operator deﬁned
by the rule (4.12), where Aeq is the matrix of the operator A with respect to the
bases En and Qm. Conversely, the linear operator A : Xn →Ym that is deﬁned by
the equality A = QAE−1 uniquely corresponds to each m × n matrix A.
If A : Xn →Xn, then
AEn = En Ae,
(4.15)
or
Ae = E−1AE,
(4.16)
where Ae is the matrix of the operator A with respect to the basis En.
We note that there are two obvious cases in which the matrix of the linear opera-
tor A : Xn →Ym does not depend on the choice of the bases for Xn and Ym.
1. The matrix of the null operator for any choice of bases for Xn and Ym is the
zero matrix.
2. The matrix of the identity operator with respect to any basis of the space Xn is
the identity matrix.
By deﬁnition of the matrix of a linear operator we have
(αA + βB)eq = αAeq + βBeq
(4.17)

100
4
Linear Operators
for all linear operators A, B : Xn →Ym and α, β ∈C, i.e., the linear operations
with their matrices correspond to the linear operations with their operators.
A similar statement under certain conditions is true for the product of two oper-
ators. Let A : Xn →Ym, B : Ym →Zp be linear operators. Suppose that {ek}n
k=1,
{qk}m
k=1, and {rk}p
k=1 are the bases for the spaces Xn, Ym, and Zp, respectively. Let
Aeq be the matrix of the operator A, Bqr the matrix of the operator B, (B A)er the
matrix of the operator BA : Xn →Zp. Let us show that
(B A)er = Bqr Aeq,
(4.18)
i.e., the matrix of the product of two operators is equal to the product of the matrices
of those operators. Indeed, using (4.13), we get
(B A)er = R−1BAE = R−1RBqrQ−1QAeqE−1E = Bqr Aeq.
It is important to note that the same basis {qk}m
k=1 ⊂Ym was used for the deﬁnition
of the matrices of the operators A and B. Usually we assume that the matching
condition for these bases is satisﬁed.
Let us consider two examples.
1. We deﬁne the linear operator A : C4 →C4 by the rule Ax = (x2, x1, x3 +
x4, x4), where x = (x1, x2, x3, x4) ∈C4. Our goal is to calculate the matrix of the
operator A with respect to the natural basis (see p. 64) of the space C4. It is easy to
see that Ai1 = (0, 1, 0, 0) = i2, Ai2 = (1, 0, 0, 0) = i1, Ai3 = (0, 0, 1, 0) = i3,
and Ai4 = (0, 0, 1, 1) = i3 + i4. Hence the matrix of the operator A is
⎛
⎜⎜⎝
0 1 0 0
1 0 0 0
0 0 1 1
0 0 0 1
⎞
⎟⎟⎠.
2.LetusdenotebyQ2 thelinearspaceofallpolynomialswithcomplexcoefﬁcients
and with degree at most 2. Let us deﬁne the linear operator T : Q2 →Q2 by the rule
T q2(z) = q2(z + h), where q2 ∈Q2. Here h is a ﬁxed complex number (a shift).
Our goal is to calculate the matrix of the operator T with respect to the basis of the
space Q2 that consists of the polynomials ϕ0(z) ≡1, ϕ1(z) = z, and ϕ2(z) = z2. We
see that T ϕ0 = ϕ0, T ϕ1 = hϕ0 + ϕ1, T ϕ2 = h2ϕ0 + 2hϕ1 + ϕ2. Hence the matrix
of the operator T is equal to
⎛
⎝
1 h h2
0 1 2h
0 0 1
⎞
⎠.
Therefore, if q2(z) = a0 + a1z + a2z2, then T q2(z) = b0 + b1z + b2z2, where

4.1 Linear Operators and Their Basic Properties
101
⎛
⎝
b0
b1
b2
⎞
⎠=
⎛
⎝
1 h h2
0 1 2h
0 0 1
⎞
⎠
⎛
⎝
a0
a1
a2
⎞
⎠=
⎛
⎝
a0 + ha1 + h2a2
a1 + 2ha2
a2
⎞
⎠.
The matrix Aeq of the linear operator A : Xn →Ym is determined by the
bases {ek}n
k=1 and {qk}m
k=1 of the spaces Xn and Ym. Suppose now that we take new
bases {˜ek}n
k=1 and {˜qk}m
k=1 in Xn and Ym. The linear operator A will be represented by
a new matrix with respect to these bases. Let us check what relations exist between
different matrices representing the same operator. Denote by A˜e ˜q the matrix of the
operator A with respect to the bases {˜ek}n
k=1 and {˜qk}m
k=1. Suppose that we know the
matrices of the bases change (see Section2.3.3, p. 66), i.e.,
˜En = EnT,
˜Qm = Qm R.
(4.19)
Using (4.13), we obtain A = QAeqE−1, A˜e ˜q = 
Q−1AE. Therefore,
A˜e ˜q = 
Q−1QAeqE−1E.
Taking into account (4.19), we get ˜Enξ = EnT ξ for every ξ ∈Cn. Hence ˜E = ET .
This implies that E−1E = T . Likewise, 
Q−1Q = R−1. Consequently,
A˜e ˜q = R−1AeqT.
(4.20)
Two matrices A and B are called equivalent if A = C BD for some nonsingular
matrices C and D.
Consider an important special case. If the linear operator A maps the space Xn
into the same space Xn, then
A˜e = T −1AeT.
(4.21)
Square matrices B and C are called similar matrices if there exists a nonsingular
matrix D such that
B = D−1C D.
(4.22)
We also say that the matrix C is transformed into the matrix B by a similarity
transformation. Relation (4.21) shows that the matrices of the same operator A :
Xn →Xn are similar with respect to the different bases.
4.1.6
The Matrix of the Inverse Operator
For every nonsingular matrix D we have det(D−1) = 1/ det(D). Hence similar
matrices have the same determinant. Because of that, we say that the determinant of
the matrix of a linear operator A : Xn →Xn is the determinant of this operator and
write det(A). The determinant is an invariant of the linear operator, i.e., it is the
same for every basis in Xn.

102
4
Linear Operators
We say that a linear operator A : Xn →Xn is nonsingular if det(A) ̸= 0. Every
nonsingular operator A is invertible. Indeed, let {ek}n
k=1 be a basis in Xn. Deﬁne an
operator B by the relationship
B = E A−1
e E−1.
Since A = E AeE−1, we have BA = AB = E IE−1 = I. Therefore, the operator B is
the inverse operator to the operator A.
The above implies that for every basis of the space Xn, the matrix of the inverse
operator A−1 is the inverse matrix to the matrix of the operator A.
Theorem 4.4 If a linear operator A : Xn →Xn is invertible, then it is nonsingular.
Theorem 4.5 A linear operator A : Xn →Xn is invertible if and only if the equa-
tion Ax = 0 has only the trivial solution x = 0.
The proof of Theorems 4.4 and 4.5 is left to the reader.
4.1.7
Linear Spaces of Linear Operators
Consider the set of all linear operators from Xn to Ym. The deﬁnitions of addition of
linear operators and their multiplication by scalars were introduced in Section4.1.1.
It is easy to prove that these operations satisfy the linear space axioms.Thus the set
of all linear operators fromXn toYm isa linear space.
Using results of Section4.1.5, we can conclude that this linear space is isomor-
phic to the linear space of all m × n matrices. The isomorphism can be deﬁned by
relationship (4.11). The dimension of the linear space of all linear operators from Xn
to Ym is equal to mn.
We obtain a real linear space of operators if the linear spaces Xn andYm are real
and linear operators can be multiplied only by real scalars.
4.1.8
The Image and the Kernel of a Linear Operator
Let A be a linear operator acting from a linear space X into a linear spaceY. The
image of A, denoted by Im(A), is the subset of Y consisting of all vectors that can
be represented in the form y = Ax for some x ∈X. The kernel of A, denoted by
Ker(A), is the subset of X consisting of all vectors x such that Ax = 0.
Theorem 4.6 The set Im(A) is a linear subspace of the space Y.
Proof If y1, y2 ∈Im(A), then there exist vectors x1, x2 ∈X such that y1 = Ax1
and y2 = Ax2. Therefore, for all α, β ∈C we have αy1 + βy2 = αAx1 + βAx2.

4.1 Linear Operators and Their Basic Properties
103
Since the operator A is linear, we have αy1 + βy2 = A(αx1 + βx2). This means that
αy1 + βy2 ∈Im(A).
□
The proof of the following theorem is left to the reader.
Theorem 4.7 The set Ker(A) is a linear subspace of the space X.
The dimension of the subspace Im(A) ⊂Ym is called the rank of the operator A
and is denoted by rank(A). The dimension of the kernel of A is called the defect of
the operator A and is denoted by def(A).
Theorem 4.8 For every linear operator A : Xn →Ym,thefollowingequalityholds:
rank(A) + def(A) = n.
(4.23)
Proof Denote by M the subspace of Xn such that Xn = Ker(A) ⊕M (see
Theorem 3.8, p. 86). Using Theorem 3.9, p. 86, we get n = def(A) + dim(M).
Taking into account Theorem 4.3, p. 97, it is enough to prove that the spaces M
and Im(A) are isomorphic. Let us check for this purpose that the operator A is a
bijective map acting from M to Im(A). In fact, every x ∈Xn can be written in the
form x = x0 + x1, where x0 ∈Ker(A), x1 ∈M. Hence, Ax = Ax1. Therefore,
every element of Im(A) is the image of some element of M. It remains to prove that
if Ax1 = Ax2 for x1, x2 ∈M, then x1 = x2. Equality A(x1 −x2) = 0 means that
x1 −x2 ∈Ker(A). On the other hand, M is a linear subspace, and thus x1 −x2 ∈M.
By Theorem 3.7, p. 85, this implies that x1 −x2 = 0.
□
4.1.9
The Rank of a Matrix
Let A(m, n) be an m × n matrix. Let us interpret the set of the matrix columns as a
subset of the space Cm. We say that the rank of this set (see Section2.2.4, p. 63) is
the rank of the matrix A(m, n) and denote it by rank(A).
Theorem 4.9 Suppose A : Xn →Ym is a linear operator, En is a basis for Xn, Qm
is a basis for Ym, Aeq is the matrix of the operator A with respect to these bases.
Then rank(Aeq) = rank(A).
Proof Let x = Enξ ∈Xn. Then Ax = Qmη, where η = Aeqξ (see Section4.1.5).
Obviously, the vector η belongs to the span of the set of the matrix Aeq columns. The
rank of the span of the set of the matrix Aeq columns is equal to rank(Aeq). Since
the linear operator Q is invertible, this span is isomorphic to Im(A). Therefore, by
Theorem 4.3, p. 97, the dimension of Im(A) is equal to rank(Aeq).
□
Consequently, the rank of the matrix of the linear operator A : Xn →Ym is an
invariant of this operator, i.e., it is the same for all bases in Xn and Yn. Hence we
could equivalently deﬁne the rank of a linear operator as the rank of its matrix.
We can interpret the set of rows of the matrix A(m, n) as a subset of the space Cn.
Denote the rank of this set by rs. The following result is unexpected at ﬁrst glance.

104
4
Linear Operators
Theorem 4.10 For every matrix A(m, n), one has the equality rs = rank(A(m, n)).
Proof We can assume without loss of generality that the ﬁrst rs rows of the matrix
A(m, n) are linearly independent and that each of the other rows is a linear combi-
nation of those ﬁrst rows. Denote by A(rs, n) the matrix that consists of the ﬁrst rs
rows of the matrix A(m, n). Let us transform the matrix A(rs, n) by an algorithm
that is in fact equivalent to Gaussian elimination.
Take a nonzero entry in the ﬁrst row of the matrix A(rs, n), which is possible
because none of the rows of the matrix A(rs, n) can be equal to zero. Interchange the
columns of the matrix A(rs, n) such that the column that contains this nonzero entry
takes the ﬁrst place. Denote this transformed matrix in the same way. Multiply the
ﬁrst row by −a21/a11 and add the result to the second row. Then do the analogous
transformations of all other rows of the matrix A(rs, n). As a result, we obtain a
matrix with zeros in the ﬁrst column below a11 ̸= 0.
The second row of the transformed matrix is a nontrivial linear combination of
the ﬁrst two rows. Therefore, it is not equal to zero. Interchanging the second column
of the transformed matrix with one of the following columns as needed, we obtain
a matrix that has the entry a22 ̸= 0. Multiply the second row by −a32/a22 and add
the result to the third row. Do the analogous transformations of all following rows
of the matrix A(rs, n). Continuing these transformations, we ﬁnally get a matrix of
the block form
( ˜A(rs,rs), B(rs, n −rs)),
(4.24)
where ˜A(rs,rs) is an upper triangular matrix that has nonzero entries along the main
diagonal.
At each step of the described transformation process we get a row that is a nontriv-
ial linear combination of the previous rows of the matrix A(rs, n). Therefore, this row
is not equal to zero, and the transformations are valid. Clearly, we can assume without
loss of generality that the original matrix A(rs, n) has such ﬁrst rs columns that by
performing the described transformations without interchanging any columns, we
get a matrix of the form (4.24).
Evidently, det( ˜A(rs,rs)) ̸= 0, and hence the ﬁrst rs columns of the origi-
nal matrix A(rs, n) are linearly independent. Thus the ﬁrst rs columns of the
matrix A(m, n) are linearly independent too. Let us check that by uniting this set of
columns with any other column of the matrix A(m, n), we get a linearly dependent
set.
Let rs be a leading principal minor1 of degree rs of the matrix A(m, n). By the
previous argumentation, rs ̸= 0. Therefore, the system of linear equations
rs

j=1
ai jx j = aik,
i = 1, 2, . . . ,rs,
(4.25)
1The leading principal minor of degree r is the determinant of the submatrix lying in the same set
of the ﬁrst r rows and columns.

4.1 Linear Operators and Their Basic Properties
105
has a solution for all k = 1, 2, . . . , n. Since each row of the matrix A(m, n) with
a number greater than rs is a linear combination of the ﬁrst rs rows of this matrix,
we see that if a vector (x1, x2, . . . , xrs) is the solution of linear system (4.25), then it
satisﬁes the following relations:
rs

j=1
ai jx j = aik,
i = rs + 1, . . . , m.
Consequently, each column of the matrix A(m, n) is a linear combination of the
ﬁrst rs columns; hence rank(A(m, n)) = rs.
□
It follows immediately from the deﬁnition that rank(A) ≤min(m, n) for every
matrix A(m, n). A matrix A(m, n) is said to have full rank if rank(A) = min(m, n).
An n × n matrix is nonsingular if and only if its rank is equal to n. Interchanging
any of the matrix’s rows or columns evidently does not change the rank of the matrix.
Moreover, we have the following result.
Theorem 4.11 Let A(m, n) be an m ×n matrix. Let B(m, m) and C(n, n) be square
nonsingular matrices. Then
rank(A) = rank(B A),
(4.26)
rank(A) = rank(AC).
(4.27)
Proof For the justiﬁcation of equality (4.26), it is enough to check the following
statement. If the matrix B is nonsingular, then a necessary and sufﬁcient condition
for the linear independence of the columns Ba1, …, Ba p is the linear independence
of the columns a1, …, a p (check it!). If equality (4.26) holds, then equality (4.27) is
proved by taking the matrix transpose.
□
The reader can easily prove that for all matrices A and B that permit matrix
multiplication, the following inequality holds: rank(AB) ≤min{rank(A), rank(B)}.
Let us consider two examples.
1. The matrix
A =
⎡
⎣
3
4 1
−2 −3 1
5
7 0
⎤
⎦
has rank 2. Indeed, since the ﬁrst two rows are linearly independent, the rank is at
least 2. However, all three rows are linearly dependent, since the ﬁrst is equal to the
sum of the second and third. Thus the rank must be less than 3.
2. The matrix
A =
 3
3 0 2
−3 −3 0 −2

has rank 1: there are nonzero columns, so the rank is positive, but each pair of columns
are linearly dependent. Similarly, the transpose of the matrix A,

106
4
Linear Operators
AT =
⎡
⎢⎢⎣
3 −3
3 −3
0 0
2 −2
⎤
⎥⎥⎦,
has rank 1. As we have proved above, the rank of a matrix is equal to the rank of its
transpose, i.e., rank(A) = rank(AT ).
4.1.10
Calculating the Rank of a Matrix Using Determinants
It follows from the proof of Theorem 4.10 that if rank(A) = r, then we can inter-
change the rows and the columns of the matrix A such that the leading principal
minor r of the transformed matrix will not vanish. This minor is called basic.
Let us formulate and prove the converse statement. Namely, let A be a rectangular
matrix. The leading principal minor r of order r < min(m, n) of the matrix A is
bordered by the leading principal minor r+1. We can construct different minors
bordering r by interchanging the rows and the columns of the matrix A whose
numbers are greater than r.
Lemma 4.1 If the leading principal minor r is nonzero and all minors of order
r + 1 that border r are equal to zero, then rank(A) = r.
Proof Since r ̸= 0, the ﬁrst r columns of the matrix A are linearly independent. Let
us show that each column of A whose number is greater than r is a linear combination
of its ﬁrst r columns. This means that rank(A) = r. Assume the contrary. Then there
exists a column of A such that the rank of the matrix that consists of this column
and the ﬁrst r columns of A is equal to r + 1. Therefore, this matrix has r + 1
linearly independent rows. The ﬁrst r rows of this matrix are linearly independent,
since r ̸= 0. Hence there exists a row whose number is greater than r that is not
a linear combination of the ﬁrst r rows. If we turn this row into the (r + 1)th row
of the matrix A, then we get r+1 ̸= 0, but this contradicts the assumption of the
lemma.
□
Lemma 4.1 gives the following method of calculating the rank of a matrix A.2
1. We check all elements of A. If all elements are zero, then rank(A) = 0.
2. If an element of A is not equal to zero, then we interchange the rows and the
columns of the matrix A to put this element in the place of a11.
3. We calculate all minors of order two that border 1 = |a11|. If all these minors
are equal to zero, then rank(A) = 1.
4. If a minor of order two is not equal to zero, then we interchange the rows and
the columns to put this minor in the place of 2, i.e., to put it in the top left corner
of the matrix A.
2Usually, the algorithm, which is described in Section5.1.1, p. 163, is used for numerical realiza-
tions.

4.1 Linear Operators and Their Basic Properties
107
5. We calculate all minors of order three that border 2 until we ﬁnd a nonzero
minor, and so on. If at a step of this algorithm we see that the leading principal minor
r is nonzero and all minors of order r + 1 that border r are equal to zero, then
rank(A) = r.
Clearly, it is not necessary to interchange the rows and the columns of the matrix
at each step of this algorithm. It is enough to calculate all minors of order r + 1 that
border an arbitrary nonzero minor of order r.
For example, let us calculate the rank of the matrix
A =
⎛
⎜⎜⎝
2 −4 3
1 0
1 −2 1 −4 2
0 1 −1 3 1
4 −7 4 −4 5
⎞
⎟⎟⎠.
Note that A includes the nonzero minor
d =

−4 3
−2 1
 .
The minor of order three
d ′ =

2 −4 3
1 −2 1
0 1 −1

borders d and is not equal to zero, but both of the fourth-order minors

2 −4 3
1
1 −2 1 −4
0 1 −1 3
4 −7 4 −4

,

2 −4 3 0
1 −2 1 2
0 1 −1 1
4 −7 4 5

bordering d ′ evidently vanish. Thus rank(A) = 3.
4.1.11
The General Solution of a Linear Equation
Let A be a linear operator mapping a linear space Xn into a linear space Ym. Consider
the linear equation
Ax = y,
(4.28)
where y is a given element of Ym and x ∈Xn is an unknown. In this section we
suppose that Eq.(4.28) is solvable and describe the form of all its possible solutions.
In other words, we describe the form of the general solution of equation (4.28).

108
4
Linear Operators
Suppose x1 and x2 are two solutions of equation (4.28) for the same right-hand
side y. Then evidently, A(x1−x2) = 0, i.e., x1−x2 ∈Ker(A). This yields that if sys-
tem (4.28) possesses a solution x0 (it is called a particular solution of the inhomoge-
neous equation), then every other solution of equation (4.28) has the form x = x0+ ˜x,
where ˜x ∈Ker(A).
Let ϕ1, ϕ2, …, ϕp be a basis of Ker(A). Then
x = x0 +
p

k=1
ckϕk.
(4.29)
Therefore, the general solution of equation (4.28) has the form (4.29). We can obtain
every solution of equation (4.28) by changing the coefﬁcients in (4.29). The vec-
tors ϕ1, ϕ2, …, ϕp are called the fundamental set of solutions of the homogeneous
equation
Ax = 0.
(4.30)
The vector
˜x =
p

k=1
ckϕk
is called the general solution of the homogeneous equation. Thus the general solution
of equation (4.28) is the sum of a particular solution of (4.28) and the general solution
of homogeneous equation (4.30).
4.1.12
Systems of Linear Algebraic Equations. Solvability
Conditions
For practical construction of the solution of linear equation (4.28) it is necessary to
introduce some bases En = {ek}n
k=1, Qm = {qk}m
k=1 for the spaces Xn, Ym and to
reduce Eq.(4.28) to a system of linear algebraic equations
Aeqξ = η.
(4.31)
The unknown vector ξ ∈Cn is the coordinate representation of x with respect to the
basis En. The vector η ∈Cm is the coordinate representation of y with respect to the
basis Qm. The matrix Aeq is the matrix of the linear operator A (see Section4.1.5).
Let us write system (4.31) in the components of ξ and η:
n

j=1
a(eq)
i j
ξ j = ηi,
i = 1, 2, . . . , m.
(4.32)

4.1 Linear Operators and Their Basic Properties
109
Here a(eq)
i j
(the entries of the matrix Aeq of the linear operator A) and ηi are given
numbers, and ξ j are unknowns.
In contrast to the systems of linear algebraic equations that were discussed in
Section1.2.3, p. 27, system (4.32) has, generally speaking, different numbers of
equations and unknowns.
Problems (4.28) and (4.31) are equivalent to each other in the sense that if ξ is a
solution of (4.31), then x = Enξ is the solution of equation (4.28) with y = Qmη;
conversely, if x is a solution of equation (4.28), then the coordinate representations of
x and y with respect to the corresponding bases are connected by relationship (4.31).
Let us obtain necessary and sufﬁcient conditions for solvability of the system of
linear algebraic equations
Ax = b,
(4.33)
where A is a given m × n matrix with complex (generally speaking) entries, and b ∈
Cm is a given vector. Let us attach the column b to the matrix A and denote this
resulting m × (n + 1) matrix by (A, b). The matrix (A, b) is called the augmented
matrix of system (4.33).
Theorem 4.12 (Kronecker–Capelli3 theorem). A system of linear algebraic equa-
tions has a solution if and only if the matrix A has the same rank as the matrix (A, b).
Proof Evidently, the rank of the augmented matrix (A, b) is greater than or equal
to the rank of the matrix A, and rank(A) = rank(A, b) if and only if b is a linear
combination of the columns of the matrix A.Thelast condition is equivalent to the
statement that there exists a vector x ∈Cn that is a solution of (4.33).
□
Theorem 4.13 (Fredholm’s4 matrix theorem). A system of linear algebraic equa-
tions has a solution if and only if for every solution of the homogeneous system of
equations zA = 0, the equality zb = 0 holds.
Note here that b is a column and z is a row.
Proof Sufﬁciency. Let r = rank(A). We can assume without loss of generality that
the ﬁrst r rows of the matrix A are linearly independent. Clearly, this implies that the
ﬁrst r rows of the matrix (A, b) are linearly independent too. If the kth row of the
matrix A is a linear combination of the ﬁrst r rows of A, then there exists a nonzero
vector z such that zA = 0. Under the hypothesis of the theorem, zb = 0. This implies
that the kth row of the matrix (A, b) is a linear combination of the ﬁrst r rows of
(A, b). Thus, rank(A) = rank(A, b), and by the Kronecker–Capelli theorem, system
(4.33) has a solution.
Necessity. Suppose that system (4.33) has a solution, i.e., that there exists x ∈Cn
such that Ax = b. Then for every z ∈Cm, we have the equality zAx = zb. Clearly,
if zA = 0, then zb = 0.
□
3Alfredo Capelli (1858–1916) was an Italian mathematician.
4Erik Ivar Fredholm (1866–1927) was a Swedish mathematician.

110
4
Linear Operators
Let us give an example of how to apply Fredholm’s matrix theorem. Consider the
symmetric n × n matrix
A =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
1 −1 0 · · · · · · · · · 0
−1 2 −1 0 · · · · · · 0
· · · · · · · · · · · · · · · · · · · · ·
0 · · · −1 2 −1 · · · 0
· · · · · · · · · · · · · · · · · · · · ·
0 · · · · · · 0 −1 2 −1
0
0 · · · · · · 0 −1 1
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
We have to calculate rank(A) and describe necessary and sufﬁcient solvability con-
ditions for the system of linear algebraic equations
Ax = b.
(4.34)
Let us interpret the matrix A as a linear operator acting on the space Rn and describe
its kernel. Consider the homogeneous system of linear algebraic equations
Ax = 0.
(4.35)
The ith equation of this system for i = 2, 3, . . . , n−1 can be written in the following
form: −xi−1 + 2xi −xi+1 = 0, or xi −xi−1 = xi+1 −xi. Therefore, if x is a solution
of system (4.35), then
x1 = x2 = · · · = xn,
i.e., the kernel of the operator A is a one-dimensional subspace of the space Rn; each
vector x of this subspace has the form x = c(1, . . . , 1), where c is a real number.
Hence, using Theorem 4.8, p. 103, we see that rank(A) = n −1.
Further, since the matrix A is symmetric, using Fredholm’s matrix theorem, we
see that a necessary and sufﬁcient solvability condition for system (4.34) is zT b = 0,
where z is any solution of equation (4.35). Consequently, a solution of system (4.34)
exists if and only if b1 + b2 + · · · + bn = 0.
4.1.13
The General Solution of a System of Linear Algebraic
Equations
Let us describe an elementary method of calculating the general solution of the
system of linear algebraic equations5
5Usually, methods based on the singular value decomposition (see Section5.1.1, p. 163) are used
for numerical approximations of general solutions of systems of linear algebraic equations.

4.1 Linear Operators and Their Basic Properties
111
Ax = b.
(4.36)
Our consideration is based on the results of Section4.1.11. Now we suppose that
system (4.36) has a solution, and denote by r the rank of the augmented matrix of
system (4.36).
Let us begin with the calculation of a particular solution of system (4.36). Using
the method of calculation of the rank of a matrix described in Section4.1.10, we
transform the matrix (A, b) such that the leading principal minor of order r of the
transformed matrix is not equal to zero and each row starting from the (r + 1)th is a
linear combination of the ﬁrst r rows.
Clearly, the transformed system of linear equations is equivalent to the original
one, i.e., every solution of system (4.36) is a solution of the transformed system, and
conversely, every solution of the transformed system is the solution of (4.36).
Since the last m −r equations of the transformed system follow from the ﬁrst r
equations of this system, we delete the last m−r equations. In the ﬁrst r equations we
move to the right-hand side all summands with the variables starting from the (r+1)th
variable. These variables are called free.
After that, we assign some values to the free variables (usually there is no rea-
son why we cannot take xr+1 = · · · = xn = 0). As a result, we get a system
of r linear equations in r unknowns. The matrix of this system is nonsingular,
and we ﬁnd the values of the variables x1, x2, …, xr as the unique solution of
this system. Thus we have the solution of the original system (4.36) of the form
x = (x1, x2, . . . , xr, xr+1, . . . , xn).
For example, let us calculate a particular solution of the following system:
x1 −x2 + x3 −x4 = 4,
(4.37)
x1 + x2 + 2x3 + 3x4 = 8,
(4.38)
2x1 + 4x2 + 5x3 + 10x4 = 20.
(4.39)
The determinant
2 =

1 −1
1 1
 ,
which is located in the top left corner of the matrix A of this system, is nonzero. All
determinants bordering 2 are equal to zero:

1 −1 1
1 1 2
2 4 5

= 0.
Hence rank(A) = 2, and the rank of the augmented matrix is also two. System
(4.37)–(4.39) is solvable, and the last equation follows from the ﬁrst two equations
of this system. Thus, to ﬁnd a particular solution of system (4.37)–(4.39), it is enough
to solve the system of two equations (4.37),(4.38), equating x3, x4 to some numbers.

112
4
Linear Operators
If we take x3 = x4 = 0 in (4.37),(4.38), then we get x1 = 6, x2 = 2. Therefore, the
vector x = (6, 2, 0, 0) is a particular solution of (4.37)–(4.39).
Now we construct the fundamental set of solutions of the homogeneous system
of linear equations
Ax = 0
(4.40)
with an m×n matrix. Let rank(A) = r. Using Theorem 4.8, we see that it is enough to
construct n −r arbitrary linearly independent solutions of system (4.40). Of course,
we assume that n > r.
Arguing as in the ﬁrst part of this subsection, we transform system (4.40) to the
following form:
A(r,r)x(r, 1) + B(r, n −r)y(n −r, 1) = 0.
(4.41)
Here A(r,r) is a nonsingular matrix, and the column y((n −r), 1) corresponds to
the free variables. We take the vectors
y1((n −r), 1), y2((n −r), 1), . . . , yn−r((n −r), 1)
(4.42)
such that they are linearly independent (the simplest way is to equate them to the
vectors of the standard basis in the space Cn−r). Using vectors (4.42), we calculate
vectors
x1(r, 1), x2(r, 1), . . . , xn−r(r, 1)
as the solutions of the following systems:
A(r,r)xk(r, 1) + B(r, (n −r))yk((n −r), 1) = 0,
k = 1, 2, . . . , n −r.
Writing together the components of the vectors xk(r, 1) and yk((n −r), 1), we con-
struct the following vectors:
zk(n, 1) = (xk(r, 1), yk((n −r), 1)),
k = 1, 2, . . . , n −r.
By construction, Azk = 0 for k = 1, . . . , n −r. Clearly, the vectors zk, k =
1, . . . , n −r, are linearly independent, since the vectors (4.42) are linearly indepen-
dent. Thus the vectors zk, k = 1, 2, . . . , n −r, form a fundamental set of solutions
of the homogeneous system of linear equations (4.40).
For example, let us calculate the fundamental set of solutions of the system of
linear equations
x1 −x2 + x3 −x4 = 0,
(4.43)
x1 + x2 + 2x3 + 3x4 = 0,
(4.44)
2x1 + 4x2 + 5x3 + 10x4 = 0,
(4.45)

4.1 Linear Operators and Their Basic Properties
113
which corresponds to system (4.37)–(4.39). As we know from the previous example,
the rank of the matrix of this system is equal to two. Therefore, we have to calculate
two linearly independent solutions of system (4.43)–(4.45). As we have seen, the
last equation in this system follows from the ﬁrst two equations. If we put x3 = 1
and x4 = 0 Eqs. (4.43), (4.44), we get
x1 −x2 + 1 = 0,
(4.46)
x1 + x2 + 2 = 0.
(4.47)
Hence x1 = −3/2, x2 = −1/2. If we put x3 = 0, x4 = 1 in (4.43), (4.44),
then we obtain x1 = −1, x2 = −2. Hence x1 = (−3/2, −1/2, 1, 0) and x2 =
(−1, −2, 0, 1) form a fundamental set of solutions of the homogeneous system of
linear equations (4.43)–(4.45). Each vector of the form
x = c1(−3/2, −1/2, 1, 0) + c2(−1, −2, 0, 1),
(4.48)
where c1, c2 are arbitrary numbers, is a solution of system (4.43)–(4.45), and con-
versely, each solution of system (4.43)–(4.45) can be represented in the form (4.48)
for some c1, c2. Thus the general solution of system (4.37)–(4.39) can be represented
in the form x = (6, 2, 0, 0)+c1(−3/2, −1/2, 1, 0)+c2(−1, −2, 0, 1), where c1, c2
are arbitrary numbers.
4.2
Eigenvalues and Eigenvectors of a Linear Operator
4.2.1
Invariant Subspaces
Let A : X →X be a linear operator. A subspace L ⊂X is said to be invariant
under A if Ax ∈L for all x ∈L. The subspaces L = {0} and L = X are invariant
for every A : X →X. We refer to these as trivial invariant subspaces.
Let the linear space X be a direct sum of subspaces L and M. Let P : X →L
be the projection operator onto the subspace L in parallel with the subspace M.
Then Px = x for every x ∈L and Px = 0 for every x ∈M, i.e., the subspaces L
and M are invariant under P.
Let us give an example of an operator that has no nontrivial invariant subspaces.
Let X2 be a two-dimensional real Euclidean space. It is easy to see that if L ⊂X2
is a nontrivial subspace, then L is the set of all vectors having the form x = αe,
where e ̸= 0 is a given vector, α ∈R. In other words, L is a straight line in the
plane containing the origin. Denote by e1, e2 an orthonormal basis of the space X2.
Let Q : X2 →X2 be the linear operator that maps each vector x = ξ1e1 + ξ2e2 into
the vector y = −ξ2e1 + ξ1e2. The vectors x and y are orthogonal. Hence if L is a
nontrivial subspace of X2, then Qx ∈L⊥for every x ∈L. Therefore, Qx /∈L for
every x ̸= 0, i.e., the operator Q has no nontrivial invariant subspaces.

114
4
Linear Operators
If a basis of an invariant subspace is known, then the form of the matrix of the linear
operatorbecomessimpler.Namely,let{ek}n
k=1 beabasisofthelinearspaceXn,andlet
L ⊂Xn be an m-dimensional invariant subspace under the operator A : Xn →Xn.
Suppose that the vectors {ek}m
k=1 belong to L. Then {ek}m
k=1 is a basis for L (prove
it!) and
Aek =
m

j=1
a(e)
jk e j, k = 1, . . . , m,
Aek =
n

j=1
a(e)
jk e j, k = m + 1, . . . , n.
These equalities show that the entries of the matrix Ae that are located in the inter-
section of the ﬁrst m columns and the last (n −m) rows are equal to zero. Therefore,
the matrix Ae can be written as the 2 × 2 block triangular matrix
Ae =
A11 A12
0
A22

,
(4.49)
where A11 is a square m × m matrix, A22 is a square (n −m) × (n −m) matrix, 0 is
the zero (n −m) × m matrix, and A12 is an m × (n −m) matrix.
We get a simpler matrix Ae if the space Xn is decomposed into a direct sum of
invariant subspaces L and M of the operator A, i.e., Xn = L ⊕M and the basis
{ek}n
k=1 of the space Xn is chosen in such way that the vectors {ek}m
k=1 form a basis
of the subspace L. Then, as is easy to see, the matrix A12 in (4.49) is the zero matrix,
i.e., the matrix Ae has block diagonal form
Ae =
A11
0
0
A22

.
(4.50)
Obviously, the converse statement is also true. Namely, if the matrix of an operator
with respect to a basis {ek}n
k=1 has the form (4.50), then the space Xn is the direct
sum of two subspaces. The vectors of the set {ek}n
k=1 whose numbers are equal to the
numbers of the rows of the corresponding blocks form the bases of these subspaces.
If the subspaces L and M are decomposed into direct sums of invariant subspaces
of smaller dimensions, then the number of diagonal blocks of the matrix Ae increase
and their dimensions decrease.
The simplest case is that in which the space Xn can be represented as a direct sum
of n one-dimensional invariant subspaces of the operator A. Then the matrix Ae is
diagonal. However, that is possible only for some special classes of operators.
Lemma 4.2 Let A : Xn →Xn be a nonsingular operator. Let L ⊂Xn be an
invariant subspace of the operator A. Then for every x ∈L there exists a unique
vector y ∈L such that Ay = x.6
6Therefore, we can say that a nonsingular operator generates a one-to-one mapping of each of its
invariant subspaces onto this subspace.

4.2 Eigenvalues and Eigenvectors of a Linear Operator
115
Proof The subspace L is invariant under the operator A. So we can introduce the
operator AL : L →
L, assuming ALx = Ax for x ∈L. The operator AL is
nonsingular, because if ALx = Ax = 0, then x = 0, since A is nonsingular (see
Theorem 4.5, p. 102). This implies that the equation AL y = x has a unique solution
y ∈L for every vector x ∈L.
□
The operator AL deﬁned in the proof of Lemma 4.2 is called the restriction of the
operator A to its invariant subspace L.
4.2.2
Basic Properties of Eigenvalues and Eigenvectors
A special role of one-dimensional invariant subspaces of operators was shown in
the previous section. The concept of one-dimensional invariant subspaces is closely
related to the concept of eigenvectors of operators.
A vector x ∈X is called an eigenvector of the operator A : X →X if x is
nonzero and there exists a number λ such that
Ax = λx.
(4.51)
The number λ is called the eigenvalue of the operator A. We say that the vector x
corresponds to (is associated with) the eigenvalue λ. The pair of the eigenvalue and
the eigenvector associated with it is also called the eigenpair of the operator A.
Let (x, λ) be an eigenpair of the operator A. Then Aαx = λαx for each α ∈C,
i.e., the one-dimensional subspace of the space X spanned by a single eigenvector
of the operator A is invariant under A.
Let λ be an eigenvalue of the operator A. The kernel of the operator A −λI is
called the eigenspace of A corresponding to the eigenvalue λ and is denoted by Lλ. It
is clear that Lλ ̸= {0}. Every nonzero element of Lλ is an eigenvector of the operator
A corresponding to the eigenvalue λ.
Let us give some examples of operators that have eigenvectors.
1. Every nonzero element of the space Xn is an eigenvector corresponding to the
eigenvalue λ = 0 of the null operator.
2. Consider the operator αI, where α ∈C. Every nonzero element of Xn is an
eigenvector of this operator corresponding to the eigenvalue λ = α.
3. Let the space X be a direct sum of subspaces L and M. As usual, we denote
by P : X →L the projection operator onto the subspace L in parallel with the
subspace M. Then the following relations hold: Px = x for every x ∈L and
Px = 0 for every x ∈M, i.e., all nonzero elements of L are eigenvectors of the
operator P corresponding to the eigenvalue λ = 1, and all nonzero elements of
M are eigenvectors of P corresponding to the eigenvalue λ = 0.
If the linear space Xn is real, then there exist linear operators A : Xn →Xn that
do not have any eigenvectors. For example, the linear operator Q (see Section4.2.1)

116
4
Linear Operators
does not have any eigenvectors in the real space X2. This follows immediately from
the fact that the operator Q does not have any nontrivial invariant subspaces.
Theorem 4.14 Every operator A acting in the complex space Xn has eigenvectors.
Proof It is enough to prove that there exists a complex number λ such that the linear
equation
(A −λI)x = 0
(4.52)
has a nontrivial solution. Let Ae be the matrix of the operator A with respect to a
basis En in the space Xn. Consider the equation
det(Ae −λI) = 0.
(4.53)
Itiseasytoseethatdet(Ae−λI)isapolynomialinλofdegreen.Thusequation (4.53)
has n roots. Every root λ of Eq. (4.53) is an eigenvalue of the operator A. Indeed,
(Ae −λI)ξ = 0
(4.54)
is a homogeneous system of linear equations with a singular matrix. Hence this
system has a nontrivial solution ξ. Then the vector x = Enξ is evidently nonzero,
and it is a solution of equation (4.52).
□
The proof of the next corollary is left to the reader.
Corollary 4.1 Suppose that a linear operator A acts on the complex space Xn,
and L ̸= {0} is an invariant subspace of A. Then the operator A has an eigenvec-
tor x ∈L.
The linear operators A and B acting on the linear space X are called permutable
if AB = BA.
Lemma 4.3 Suppose that A, B are permutable transformations of the linear
space X, and Lλ ⊂X is an eigenspace of the operator A. Then Lλ is an invari-
ant subspace of the operator B.
Proof Suppose that x ∈Lλ. Then Ax = λx. Hence BAx = λBx. By assumption,
we have BA = AB. Therefore, ABx = λBx, which means that Bx ∈Lλ.
□
The polynomial det(A −λI) is called the characteristic polynomial of the
matrix A. The equation det(A −λI) = 0 is called the characteristic equation
of the matrix A. The roots of the characteristic polynomial are called the charac-
teristic values (eigenvalues) of the matrix A. The set of all characteristic values of
the matrix A is called the spectrum of A and is denoted by σ(A).
As established in the proof of Theorem 4.14, for every λ ∈σ(A) there exists a
nonzero vector x ∈Cn such that
Ax = λx.

4.2 Eigenvalues and Eigenvectors of a Linear Operator
117
This vector x is called the eigenvector of the matrix A corresponding to the charac-
teristic value λ of the matrix.
Theorem 4.15 Similar matrices have the same characteristic polynomials and
therefore the same characteristic values.
Proof Let T be a nonsingular matrix. By deﬁnition of matrix similarity, the matrix
B = T −1 AT is similar to the matrix A. Then for every λ ∈C we have
B −λI = T −1 AT −λI = T −1(A −λI)T.
Since det(T −1) = 1/ det(T ), we see that det(B −λI) = det(A −λI).
□
The matrices of the same operator A : Xn →Xn are similar with respect to
different bases (see Section4.1.5, p. 101); hence the characteristic polynomial and its
roots do not depend on the choice of basis in Xn. Thus the characteristic polynomial of
the matrix of a linear operator is called the characteristic polynomial of the operator,
and equation (4.53) is called the characteristic equation of the operator A.
The characteristic values of the matrix of an operator are called the characteristic
values of the operator. Therefore, they are invariants of the operator. The set of all
characteristic values of the operator A is called the spectrum of the operator and is
denoted by σ(A).
As follows from the proof of Theorem 4.14, for every operator acting on the com-
plex space Xn, the concepts of characteristic values and eigenvalues are equivalent,
and for such operators, the corresponding terms are used as synonyms.
EveryoperatoractinginthespaceXn hasnomorethan n distinct eigenvalues.
Theorem 4.16 (Cayley–Hamilton7 theorem). Let
Pn(λ) = λn + an−1λn−1 + · · · + a0
(4.55)
be the characteristic polynomial of an operator A acting on the space Xn. Then
Pn(A) = An + an−1An−1 + · · · + a0I = 0.
(4.56)
Proof Let A be the matrix of the operator A with respect to a basis. Using formula
(1.104), p.40, we get (A −λI) 
(A −λI) = Pn(λ)I for all λ ∈C. Obviously, each
element of the matrix

(A −λI) is a polynomial of λ of degree no greater than n −1.
Therefore, we can write

(A −λI) = λn−1Cn−1 + λn−2Cn−2 + · · · + C0,
where C0, C1, …, Cn−1 are some square matrices of order n, i.e.,
7Arthur Cayley (1821–1895) was a British mathematician. William Rowan Hamilton (1805–1865)
was an Irish physicist and mathematician.

118
4
Linear Operators
Pn(λ)I = (A −λI)(λn−1Cn−1 + λn−2Cn−2 + · · · + C0) for all λ ∈C.
(4.57)
Equating all coefﬁcients with the same power of λ on both sides of equality (4.57),
we obtain
AC0 = a0I,
AC1 −C0 = a1I,
AC2 −C1 = a2I,
· · · · · · · · · · · · · · · · · · · · · · · ·
ACn−1 −Cn−2 = an−1I,
−Cn−1 =
I.
(4.58)
Now we multiply the ﬁrst equality in (4.58) by I, premultiply the second equality by
A, premultiply the third equality by A2, and so on. We premultiply the last equality
in (4.58) by An. After that, we add together all obtained equalities and get Pn(A) = 0,
which is equivalent to Pn(A) = 0.
□
The next corollary follows from the Cayley–Hamilton theorem. This corollary
plays an important role in applications, for instance in mechanics.
Corollary 4.2 If the operator A : Xn →Xn is invertible, then there exists a poly-
nomial Qn−1 of degree no greater than n −1 such that A−1 = Qn−1(A).
The proof of Corollary 4.2 is left to the reader.
Theorem 4.17 Let λ1, λ2, …, λp be distinct eigenvalues of the operator A :
Xn
→
Xn. Denote by x1, x2, …, x p the eigenvectors of the operator A such
that Axk = λkxk, k = 1, 2, . . . , p. Then the vectors x1, x2, . . . , x p are linearly
independent.
Proof Suppose contrary to the assertion of the theorem that the set of vectors x1,
x2, . . . , x p is linearly dependent. Without loss of generality, it can be assumed that
the vectors x1, x2, …, xr, r < p, form a maximal linearly independent subset of this
set. Denote by Lr the subspace of the linear space Xn spanned by x1, x2, . . . , xr.
The subspace Lr is invariant under A and has dimension r. Let ALr be the restriction
of the operator A to Lr. Then λ1, λ2,…, λr are the eigenvalues of the operator ALr,
and they are distinct. The nonzero vector xr+1 belongs to Lr, and we see also that
ALr xr+1 = Axr+1 = λr+1xr+1, i.e., λr+1 is an eigenvalue of the operator ALr , but
the operator ALr acts on a space of dimension r. Therefore, it cannot have more than
r distinct eigenvalues.
□
It follows from the above that if all eigenvalues of the operator A are distinct, then
the corresponding eigenvectors xk, k = 1, 2, …, n, form a basis of the space Xn. We
have
Axk = λkxk,
k = 1, 2, . . . , n,
and hence the matrix of the operator A with respect to the basis {xk}n
k=1 is diagonal.
The eigenvalues λk, k = 1, 2, …, n, form the main diagonal of this matrix.

4.2 Eigenvalues and Eigenvectors of a Linear Operator
119
For example, let us calculate all the eigenvalues and all the eigenvectors of the
matrix
A =
⎛
⎝
4 −5 7
1 −4 9
−4
0 5
⎞
⎠.
The characteristic equation has the form

4 −λ
−5
7
1
−4 −λ
9
−4
0
5 −λ

= 0.
Calculating this determinant, we get
λ3 −5λ2 + 17λ −13 = 0.
(4.59)
Evidently, λ = 1 is a root of Eq.(4.59). It is easy to see that
λ3 −5λ2 + 17λ −13 = (λ −1)(λ2 −4λ + 13).
The equation λ2 −4λ + 13 = 0 has two roots: λ = 2 ± 3i. Therefore,
λ1 = 1,
λ2 = 2 + 3i,
λ3 = 2 −3i
are all the eigenvalues of the matrix A. The coordinates of the eigenvector corre-
sponding to λ1 are the solution of the homogeneous system of linear equations
3x1 −5x2 + 7x3 = 0,
(4.60)
x1 −5x2 + 9x3 = 0,
(4.61)
−4x1 + 4x3 = 0.
(4.62)
We have

3 −5
1 −5
 ̸= 0. Hence the rank of the matrix of system (4.60)–(4.62) is equal
to two, and this system has only one linearly independent solution. Take x3 = 1 and
obtain x1, x2 as a solution of system (4.60), (4.61). We get x1 = 1, x2 = 2. Thus the
vector (1, 2, 1) is a solution of the system of equations (4.60)–(4.62). Therefore, the
set of all eigenvectors corresponding to the eigenvalue λ1 = 1 is the set of vectors
having the form c(1, 2, 1), where c is an arbitrary nonzero complex number.
The coordinates of the eigenvector corresponding to λ2 are the solution of the
homogeneous system of linear equations
(2 −3i)x1 −5x2 + 7x3 = 0,
(4.63)
x1 −(6 + 3i)x2 + 9x3 = 0,
(4.64)
−4x1 + (3 −3i)x3 = 0.
(4.65)

120
4
Linear Operators
We have

2 −3i
−5
1
−(6 + 3i)
 ̸= 0. Hence the coordinates of an eigenvector are the
solution of system (4.63), (4.64) for x3 = 1. We get x1 = (3−3i)/4, x2 = (5−3i)/4.
Therefore, the set of all eigenvectors corresponding to the eigenvalue λ2 is the set of
vectors having the form c(3−3i, 5−3i, 4), where c is an arbitrary nonzero complex
number. Analogous calculations show that the set of all eigenvectors corresponding
to the eigenvalue λ3 is the set of vectors having the form c(3+3i, 5+3i, 4), where c
is an arbitrary nonzero complex number.
In this example all the eigenvalues are distinct and the corresponding eigenvectors
form a basis of the space C3. This can be seen also from the fact that the determinant

1
2
1
3 −3i 5 −3i 4
3 + 3i 5 + 3i 4

,
which is composed of the coordinates of the eigenvectors, is not equal to zero.
If the characteristic polynomial of the operator A has multiple roots, then the
number of corresponding linearly independent eigenvectors can be less than n, and
these eigenvectors are not a basis of the space Xn.
Now let us calculate all eigenvalues and all eigenvectors of the matrix
A =
⎛
⎝
2 −1
2
5 −3
3
−1
0 −2
⎞
⎠.
The characteristic equation has the form λ3 + 3λ2 + 3λ + 1 = 0, and the numbers
λ1 = λ2 = λ3 = −1 are the roots of this equation. Therefore, we have the following
system for the calculation of the coordinates of eigenvectors:
3x1 −x2 + 2x3 = 0,
(4.66)
5x1 −2x2 + 3x3 = 0,
(4.67)
−x1 −x3 = 0.
(4.68)
The determinant

3 −1
5 −2
 is not equal to zero. Hence the rank of the matrix of this
system is equal to two, and the linear space of all solutions of system (4.66)–(4.68)
is one-dimensional. It is easy to see that the vector x = (1, 1, −1) is a solution
of system (4.66)–(4.68). Thus the set of all eigenvectors of the matrix is the set of
vectors having the form c(1, 1, −1), where c is an arbitrary nonzero complex number.
Clearly, in this example the eigenvectors of the matrix are not a basis of the space C3.
The dimension of the eigenspace of the operator A corresponding to the eigen-
value λ of this operator is called the geometric multiplicity of the eigenvalue λ.
The multiplicity of λ as a root of the characteristic polynomial of the operator A

4.2 Eigenvalues and Eigenvectors of a Linear Operator
121
is called the algebraic multiplicity of the eigenvalue λ. In general, these two con-
cepts are different. If the term multiplicity is used without qualiﬁcation in reference
to an eigenvalue, it usually means the algebraic multiplicity. We shall follow this
convention.
Theorem 4.18 For every operator A acting on the ﬁnite-dimensional space Xn
the geometric multiplicity of each eigenvalue is less than or equal to the algebraic
multiplicity of that eigenvalue.
Proof Let Lλ0 be the eigenspace of the operator A corresponding to an eigenvalue
λ0 of that operator and dim(Lλ0) = m. Denote by f1, f2, . . . , fm a basis of the
eigenspace Lλ0.Extend this basis to a basis of the space Xn bysome additional vectors
gm+1, gm+2, . . . , gn. Since A fk = λ0 fk, k = 1, 2, . . . , m, it follows that the matrix
of the operator A with respect to this basis of Xn can be written as a block matrix
(see Section4.2.1):
Λ0 A12
0 A22

,
(4.69)
where Λ0 is a diagonal m × m matrix all of whose diagonal entries are equal to λ0.
Hence the characteristic polynomial of the operator A can be written as
det(A −λI) = (λ −λ0)m Qn−m(λ),
where Qn−m(λ) is a polynomial of degree n −m. Evidently, m cannot be greater
than the multiplicity of the root λ0 of the polynomial det(A −λI).
□
4.2.3
Diagonalizable Operators
We say that a linear operator A : Xn →Xn is diagonalizable if there is a basis En of
the space Xn consisting entirely of the eigenvectors of A. The matrix of the operator A
with respect to the basis En can be written in the form
Ae = diag(λ1, . . . , λ1, λ2, . . . , λ2, . . . , λk, . . . , λk),
where each eigenvalue of the operator A is repeated according to its geometric
multiplicity.
If A : Xn →Xn is a diagonalizable operator, λ1, λ2, …, λk, k ≤n, are distinct
eigenvalues of this operator, and Lλi, i = 1, 2, . . . , k, are corresponding eigenspaces
of A, then
Xn = Lλ1 ⊕Lλ2 ⊕· · · ⊕Lλk.
For i = 1, 2, …, k, denote by Pi the operator that projects the space Xn onto the
subspace Lλi. Then it is easy to see that

122
4
Linear Operators
Ax = λ1P1x + λ2P2x + · · · + λkPkx
for all
x ∈Xn,
i.e.,
A = λ1P1 + λ2P2 + · · · + λkPk.
(4.70)
Equality (4.70) is referred to as a spectral resolution of the operator A .
Using (4.70) and (4.4), pp. 95, 122, we get A j = λ j
1P1 + λ j
2P2 + · · · + λ j
kPk for
every integer j ≥0. Therefore, if Qm is a polynomial of degree m ≥0, then
Qm(A) = Qm(λ1)P1 + Qm(λ2)P2 + · · · + Qm(λk)Pk.
(4.71)
Since the numbers λ1, λ2, …, λk are distinct, we can deﬁne Lagrange basis func-
tions (see p. 29)
Φ j(λ) =
(λ −λ1)(λ −λ2) · · · (λ −λ j−1)(λ −λ j+1) · · · (λ −λk)
(λ j −λ1)(λ j −λ2) · · · (λ j −λ j−1)(λ j −λ j+1) · · · (λ j −λk), j = 1, 2, . . . , k.
Then, taking into account (4.71), we obtain
P j = Φ j(A),
j = 1, 2, . . . , k.
(4.72)
Equation (4.72) is called
Sylvester’s formula.8 This shows that for each j =
1, 2, . . . , k, the projection operator P j is a polynomial of degree k −1, and the
coefﬁcients of this polynomial depend only on the eigenvalues of the operator A.
Theorem 4.19 A linear operator A is diagonalizable if and only if the geometric
multiplicity of each eigenvalue λ of the operator A is equal to the algebraic multi-
plicity of λ.
The proof of Theorem 4.19 is left to the reader.
Suppose that operators A, B acting on the ﬁnite-dimensional space Xn are diago-
nalizable and have the same characteristic polynomial. The reader can easily prove
that there exists a nonsingular operator Q : Xn →Xn such that B = QAQ−1.
4.2.4
Invariants of an Operator
In this section we use in an essential way the following lemma.
Lemma 4.4 For every x ∈C, the following expansion holds:
8James Joseph Sylvester (1814–1897) was an English mathematician.

4.2 Eigenvalues and Eigenvectors of a Linear Operator
123
d(x) =

a11 + x
a12
. . .
a1n
a21
a22 + x . . .
a2n
. . .
. . .
. . .
. . .
an1
an2
. . . ann + x

= xn + c1xn−1 + c2xn−2 + · · · + cn−1x + cn,
(4.73)
where
ck =

1≤p1<p2<···<pk≤n

ap1,p1 ap1,p2 . . . ap1,pk
ap2,p1 ap2,p2 . . . ap2,pk
. . .
. . .
. . .
. . .
apk,p1 apk,p2 . . . apk,pk

,
k = 1, 2, . . . , n.
(4.74)
For each k, the right-hand side of (4.74) is the sum of all Ck
n determinants of the
indicated form. These determinants are called the principal minors of order k of the
matrix
A =
⎛
⎜⎜⎝
a11 a12 . . . a1n
a21 a22 . . . a2n
. . . . . . . . . . . . .
an1 an2 . . . ann
⎞
⎟⎟⎠.
Note that c1 = a11 + a22 + · · · + ann, cn = det A.
Proof of Lemma 4.4. Denote by a1, a2, …, an the columns of the matrix A. Let us
interpret the determinant of the matrix A as a function of its columns, i.e.,
det A = (a1, a2, . . . , an).
Then the function d(x) in (4.73) can be represented in the form
d(x) = (a1 + xi1, a2 + xi2, . . . , an + xin),
where by i1, i2, . . . , in we denote as usual the standard unit vectors in the space Cn.
Since the determinant is a linear function of its columns, it follows easily that
d(x) = (a1, a2, . . . , an)
+ x((i1, a2, . . . , an) + (a1, i2, . . . , an) + · · · + (a1, a2, . . . , an−1, in))
+ x2((i1, i2, a3 . . . , an) + · · · + (a1, a2, . . . , an−2, in−1, in))
+ · · · + xn(i1, i2, . . . , in).
(4.75)
The multiplier of each xk in (4.75) is the sum of Ck
n determinants, each of which is
obtained from the determinant (a1, a2, . . . , an) by replacing k columns of  with

124
4
Linear Operators
the corresponding standard unit vectors. To complete the proof, it is enough to note
that (i1, i2, . . . , in) = 1 and that each principal minor of order n−k of the matrix A
is obtained from (a1, a2, . . . , an) by replacing k corresponding columns with the
standard unit vectors with the same numbers.
□
The characteristic polynomial of the matrix Ae of the linear operator A is equal
to det(λI −Ae) up to sign. Let us expand this determinant as the polynomial in λ:
det(λI −Ae) = Pn(λ) = λn −I1λn−1 + I2λn−2 + · · · + (−1)nIn.
(4.76)
As we have noted in Section4.2.2, the coefﬁcients of the polynomial Pn are invariants
of the operator A. All of them are functions of entries of the matrix Ae, but they are
invariant under every transformation of the basis. In this connection the following
notation is used: Ik = Ik(A), k = 1, 2, . . . , n. Using (4.73) and (4.74), we get the
following representations for the invariants Ik(A) of the operator A by entries of the
matrix Ae:
Ik(A) =

1≤i1<i2<···<ik≤n

ae
i1,i1 ae
i1,i2 . . . ae
i1,ik
ae
i2,i1 ae
i2,i2 . . . ae
i2,ik
. . .
. . . . . . . . .
ae
ik,i1 ae
ik,i2 . . . ae
ik,ik

,
k = 1, 2, . . . , n.
(4.77)
In particular,
I1(A) = ae
11 + ae
22 + · · · + ae
nn,
In(A) = det Ae.
(4.78)
Using Viète’s formulas (see Section1.1.4, p. 14), we have
ae
11 + ae
22 + · · · + ae
nn = λ1 + λ2 + · · · + λn,
det Ae = λ1λ2 · · · λn,
(4.79)
where λ1, λ2, . . . , λn are the characteristic values of the operator A. Generally, Ik(A)
is the sum of all products of k distinct characteristic values of the operator A.
Every square matrix A = {ai j}n
i, j=1 generates a linear operator in Cn deﬁned
by the matrix–vector multiplication rule (4.5), p. 95. Hence it is possible to assign
the numbers Ik(A), k = 1, 2, …, n (calculated by formulas (4.77), where ae
i j are
replacedwithai j),toeachsquarematrix.Evidently,thesenumbersareinvariantunder
similarity transformations, and therefore, they are called invariants of the matrix A.
Theorem 4.20 Let A be an operator acting on a ﬁnite-dimensional space Xn. Then
thereexistsapositivenumberε0 suchthatif|ε| < ε0 andε ̸= 0,thentheoperatorA+
εI is invertible.
The proof of Theorem 4.20 is left to the reader.
The number I1(A) = ae
11 + ae
22 + · · · + ae
nn = λ1 + λ2 + · · · + λn is called the
trace of the operator A and is denoted by tr(A).

4.2 Eigenvalues and Eigenvectors of a Linear Operator
125
The following equality holds:
tr(αA + βB) = αtr(A) + βtr(B).
(4.80)
Here A, B are linear operators in a ﬁnite-dimensional linear space, and α, β are
complex numbers. Let A : Xn →Xm, B : Xm →Xn. Then
tr(AB) = tr(BA).
(4.81)
Equality (4.80) follows immediately from the deﬁnition of the trace of a linear oper-
ator. Equality (4.81) is veriﬁed by direct calculation of the sums of diagonal elements
of thematrices of operators deﬁnedontheleft-handsideandright-handsideof (4.81).
Let A, B be arbitrary linear operators acting on a ﬁnite-dimensional vector space.
The reader can prove that the characteristic polynomials of the operators AB and BA
coincide. Hint: if the operator A is nonsingular, the result follows from the similarity
of the matrices of the operators AB and BA. In the general case, it is useful to apply
Theorem 4.20.
4.2.5
Invariant Subspaces of an Operator in a Real Space
Let A be a linear operator acting on the real space Xn. Then the matrix Ae of the
operator A is real with respect to every basis En. The characteristic equation (4.53)
for the matrix Ae is a polynomial equation with real coefﬁcients. This equation has,
generally speaking, both real and complex roots.
If λ is a real root of Eq.(4.53), then the system of equations
(Ae −λI)ξ = 0
(4.82)
has a nontrivial real solution ξ, and for x = Enξ, the equality Ax = λx holds, i.e.,
x is an eigenvector of the operator A. Therefore, all real characteristic values of the
matrix Ae are eigenvalues of the operator A.
If equation (4.53) has no real root, then the system of equations (4.82) has no
nontrivial real solutions. Hence if all roots of equation (4.53) are complex, then the
operator A does not have eigenvectors. Therefore, there can exist linear operators
acting on a real space that have no one-dimensional invariant subspaces.
There is a two-dimensional invariant subspace of the operator A corresponding
to every complex characteristic value of the matrix Ae. In fact, if λ = α + iβ is
a complex characteristic value of the matrix Ae, then det(Ae −λI) = 0, and the
system of equations
(Ae −λI)ξ = 0
(4.83)

126
4
Linear Operators
has a nontrivial complex solution ξ = ζ +iη. Here ζ and η belong to Rn. The matrix
Ae is real, and thus writing system (4.83) in terms of the complex solution, we get
Aeζ + iAeη = (α + iβ)(ζ + iη) = αζ −βη + i(βζ + αη).
Equating the real and imaginary parts of the last equation, we obtain
Aeζ = αζ −βη,
Aeη = βζ + αη.
If x = Enζ and y = Enη, then
Ax = αx −βy,
(4.84)
Ay = βx + αy.
(4.85)
Denote by L the subspace of the linear space Xn spanned by x and y. Suppose
that z ∈L, which means that z = γ x +δy for some γ, δ ∈R. Then Az ∈L. Indeed,
Az = γ Ax + δAy = γ (αx −βy) + δ(βx + αy) =
=(αγ + βδ)x + (αδ −βγ )y ∈L.
Therefore, L is an invariant subspace of the operator A.
To complete the proof, the reader can show that the vectors x and y that satisfy
relationships (4.84), (4.85) are linearly independent, i.e., the subspace L is two-
dimensional.
The reader can easily prove that if a linear operator A acts on the real space Xn
and a subspace Lm ⊂Xn is invariant under A and has dimension m ≥2, then the
operator A has either a one-dimensional or a two-dimensional invariant subspace of
the subspace Lm.
4.2.6
Nilpotent Operators
AlinearoperatorAactingonaﬁnite-dimensionalspaceXn iscallednilpotent ifAq =
0 for some integer q ≥1. The smallest such q is called the index of nilpotency of the
operator A. The deﬁnition of a square nilpotent matrix is similar.
Using (4.16), p.99, we see that Aq
e = E−1AqE. Therefore, if an operator A is
nilpotent, then its matrix with respect to every basis is nilpotent of the same index.
Conversely, if the matrix of an operator is nilpotent, then the operator is nilpotent of
the same index.

4.2 Eigenvalues and Eigenvectors of a Linear Operator
127
Theorem 4.21 An operator A is nilpotent if and only if all its eigenvalues are equal
to zero.
Proof Let A be a nilpotent operator of index q and let (λ, x) be an eigenpair of the
operator A. Then Ax = λx. Therefore, Aqx = λqx. We have assumed that Aq = 0,
and hence λqx = 0, but x ̸= 0. Thus λ = 0. Conversely, suppose that all eigenvalues
of the operator A are equal to zero. Then the characteristic equation of the operator A
has the form λn = 0, and by Theorem 4.16, p. 117, we get An = 0.
□
The following corollary is obvious.
Corollary 4.3 The index of nilpotency of a nilpotent operator acting on an n-
dimensional space is less than or equal to n.
Let A : Xn →Xn be a nilpotent operator of index q. Then evidently, there
exists a vector x0 ∈Xn such that Aq−1x0 ̸= 0. The reader can easily prove that the
vectors x0, Ax0, …, Aq−1x0 are linearly independent.
4.2.7
The Triangular Form of the Matrix of an Operator
Theorem 4.22 For each operator A acting on the complex space Xn, there exists a
basis such that the matrix of A with respect to this basis is triangular, the eigenvalues
of A forming the main diagonal of this matrix.
The proof of Theorem 4.22 is based on the following result.
Theorem 4.23 (Schur’s9 theorem). Let A be an n × n matrix. Let λ1, λ2, …, λn
be its characteristic values numbered in arbitrary order. Then there exists a unitary
matrix U such that
U ∗A U = T,
(4.86)
where T is an upper triangular matrix of the form
T =
⎛
⎜⎜⎝
λ1 t12 . . .
t1n
0 λ2 . . .
t2n
. . . . . . . . . tn−1,n
0
0 . . .
λn
⎞
⎟⎟⎠.
(4.87)
Proof Denote by u1 an eigenvector of the matrix A corresponding to the eigen-
value λ1. Every eigenvector is deﬁned up to a scalar multiplier. Hence we can assume
that |u1| = 1.10 Let us construct an orthonormal basis {uk}n
k=1 of the space Cn con-
taining u1 (see Section3.2, p. 71). Denote by U1 the matrix with columns consist-
ing of the elements of the vectors {uk}n
k=1. Taking into account that Au1 = λ1u1
and (uk, u1) = 0 for k = 2, 3, . . . , n, we get
9Issai Schur (1875–1941) was a German mathematician.
10In this subsection we use only the standard inner product on Cn.

128
4
Linear Operators
U ∗
1 A U1 =
λ1 ∗
0 A1

.
(4.88)
The right-hand side of this equality is a block 2 × 2 matrix. The ﬁrst diagonal block
of this matrix consists of the number λ1 only. The second diagonal block is a square
matrix of order n −1. The block in location (2, 1) is a zero (n −1)-dimensional
column. The block in location (1, 2) is an (n −1)-dimensional row with nonzero,
generally speaking, elements. Analogous notations will be used in the proof below.
The matrix U ∗
1 A U1 is similar to A; hence (see Theorem 4.15, p. 117)
σ(U ∗
1 A U1) = σ(A).
Using (4.88) and expanding the determinant det(λI −U ∗
1 A U1) in terms of the ﬁrst
column, we get σ(U ∗
1 A U1) = λ1 ∪σ(A1). Therefore,
σ(A1) = {λ2, . . . , λn}.
By analogy with U1, we can construct a unitary matrix U2 such that
U ∗
2 A1U2 =
λ2 ∗
0 A2

.
(4.89)
Let
V2 =
1 0
0 U2

.
Then V2 is a unitary matrix of order n. By elementary calculations,
V ∗
2 U ∗
1 A U1V2 =
⎛
⎝
λ1 ∗
∗
0 λ2 ∗
0 0 A2
⎞
⎠.
Continuing this process, we can construct unitary matrices V3, …, Vn−1 such that the
matrix
V ∗
n−1 · · · V ∗
2 U ∗
1 A U1V2 · · · Vn−1
is an upper triangular matrix with the numbers λ1, λ2, …, λn on the main diagonal. Let
U = U1V2 · · · Vn−1. Then U is a unitary matrix, since it is represented as a product
of unitary matrices (see Section1.2.7, p. 48), and U ∗= V ∗
n−1 · · · V ∗
2 U ∗
1 . Therefore,
the matrix T = U ∗A U has the form (4.87).
□
Arguing as above, we see that there exists a unitary matrix V such that
V ∗A V = L,

4.2 Eigenvalues and Eigenvectors of a Linear Operator
129
where L is a lower triangular matrix, and all characteristic values of A form the main
diagonal of L.
Remark 4.1 From the proof of Schur’s theorem, we see that if a matrix A is real and
all its characteristic values (and hence all eigenvectors) are real, then the matrix U
in (4.86) can be chosen as a real unitary matrix, in other words, as an orthogonal
matrix.
Proof of Theorem 4.22. Let A be a linear operator acting on the space Xn, and let
Fn = { fk}n
k=1 be an arbitrarily chosen basis in Xn. Then AFn = Fn A f , where A f is
the matrix of the operator A with respect to this basis (see (4.11), p. 99). Using Schur’s
theorem, we see that there exists a unitary matrix U such that A f = UTU ∗, where
T is a matrix of the form (4.87), λ1, λ2, . . . , λn are characteristic values of A f (i.e.,
eigenvalues of A). Hence, AFn = FnUTU ∗. Therefore, we get AFnU = FnUT .
Let En = FnU. Then AEn = EnT . Thus T is the matrix of the operator A with
respect to the basis En.
□
Remark 4.2 If the space Xn is unitary and the basis Fn is orthonormal, then the
basis En is also orthonormal.
The matrix T that appears in Theorem 4.22 is usually called the Schur form of
the matrix of the operator. The following simpliﬁcation of the matrix of an operator
is often useful.
Theorem 4.24 Let A be a square matrix of order n, let λ1, λ2, …, λk be distinct
characteristic values of A having multiplicities n1, n2, …, nk, respectively, where n1+
n2 + · · · + nk = n. Then there exists a nonsingular matrix S such that
S−1 AS =
⎛
⎜⎜⎜⎝
T1
0
T2
...
0
Tk
⎞
⎟⎟⎟⎠
(4.90)
is a block diagonal matrix, and each diagonal block Ti is an upper triangular matrix
of order ni, i = 1, 2, . . . , k. All diagonal elements of each block Ti are identical and
equal to λi.
Proof At the ﬁrst stage, using Schur’s theorem, we transform the matrix A to an
upper triangular matrix T by a unitary similarity transformation. We can order the
characteristic values on the diagonal of the triangular matrix T according to the
statement of the theorem, i.e., the ﬁrst n1 numbers of the diagonal are equal to λ1,
the next n2 numbers are equal to λ2, and so on. To complete the proof, it is enough to
transform the matrix T to the form (4.90) by a similarity transformation. We construct
this transformation as a result of a sequence of elementary similarity transformations.
Let us write the above-mentioned upper triangular matrix T in the block form

130
4
Linear Operators
T =
T11 T12
0 T22

.
Here T11 is an upper triangular matrix of order n1 each of whose diagonal elements
are equal to λ1, and T22 is an upper triangular matrix of order n −n1 none of whose
diagonal elements is equal to λ1. We consider the upper triangular matrices of the
form
In1
P
0 In−n1

,
In1 −P
0 In−n1

,
(4.91)
where In1, In−n1 are the identity matrices of orders n1, n −n1, respectively. Using
elementary calculations, we see that matrices (4.91) are mutually inverse. Now we
ﬁnd the matrix P such that the following equality holds:
In1
P
0 In−n1
 T11 T12
0 T22
 In1 −P
0 In−n1

=
T11 0
0 T22

.
(4.92)
Clearly, the equality (4.92) is true if the matrix P is a solution of the following
equation 11:
PT22 −T11P = −T12.
(4.93)
Equation (4.93) is a system of linear algebraic equations for the elements of the
matrix P. Let us check that the corresponding homogeneous system
PT22 −T11P = 0
(4.94)
has the trivial solution only. Indeed, we can rewrite Eq.(4.94) in the equivalent
form P(T22 −λ1In−n1) = (T11 −λ1In1)P. Obviously, the matrix T22 −λ1In−n1 is
nonsingular. Hence, P = (T11 −λ1In1)P(T22 −λ1In−n1)−1. Therefore,
P = (T11 −λ1In1)q P((T22 −λ1In−n1)−1)q
for all integers q ≥1. By construction, the matrix T11 −λ1In1 is nilpotent, and
there exists an integer q ≥1 such that (T11 −λ1In1)q = 0. Hence P = 0. Thus
a transformation of the form (4.92) exists. In the following steps of the proof we
construct analogous transformations, which successively reduce the orders of the
blocks of the transformed matrix. Arguing as in the proof of Theorem 4.23, as a
result we get relationship (4.90).
□
Using Theorem 4.24, the reader can easily prove the following result.
Theorem 4.25 For each operator A acting on the space Xn there exist invariant
subspaces M and N such that Xn = M ⊕N, the restriction of the operator A to
the subspace M is a nilpotent operator, and the restriction of the operator A to the
subspace N is an invertible operator.
11Equation (4.93) is a Sylvester equation; see, for example, [23, p. 170], [59, 66].

4.2 Eigenvalues and Eigenvectors of a Linear Operator
131
Below is a useful example of applications of Schur’s theorem.
Theorem 4.26 Let A = {ai j}n
i, j=1 be an arbitrary square matrix. For every ε > 0,
there exists an invertible diagonalizable matrix Am = {a(m)
i j }n
i, j=1 such that
max
1≤i, j≤n |ai j −a(m)
i j | ≤ε.
(4.95)
Proof Using Theorem 4.23, we represent the matrix A in the form A = UTU ∗,
where U is a unitary matrix and T is an upper triangular matrix. Without loss of
generality we can assume that the diagonal elements of the matrix T are ordered in
the following way:
λ1, λ1, . . . , λ1, λ2, λ2, . . . , λ2, . . . , λk, λk, . . . , λk.
Here each characteristic value of the matrix A is repeated according to its multiplicity.
Denote by Tm the upper triangular matrix that differs from the matrix T only by the
diagonal elements, which are equal to the following numbers:
λ1 + 1/m, λ1 + 1/2m, . . . , λ1 + 1/n1m,
λ2 + 1/m, λ2 + 1/2m, . . . , λ2 + 1/n2m, . . . ,
λk + 1/m, λk + 1/2m, . . . , λk + 1/nkm,
where ni is the multiplicity of λi, i = 1, 2, . . . , k, m ≥1. Let Am = UTmU ∗. It is
easy to see that all diagonal elements of the matrix Tm for large enough m are nonzero
and distinct. Therefore, all characteristic values of the matrix Am are nonzero and
distinct. This means that for all large enough m, the matrices Am are invertible and
diagonalizable. Further, A −Am = U(T −Tm)U ∗. Hence
max
1≤i, j≤n |ai j −a(m)
i j | ≤c/m,
where c is a constant depending only on n and on the elements of the matrix U. Thus
for every ε > 0, we get (4.95) for large enough m.
□
We can say that the sequence of the matrices {Am}∞
m=1 converges to the matrix A.
4.2.8
The Real Schur Form
Theorem 4.27 Let A be a real square matrix of order n ≥1. There exists an
orthogonal matrix Q such that A = QT T Q, where T is a block upper triangular
matrix. The diagonal blocks of the matrix T are square matrices of order one or two.
The set of all characteristic values of the second-order blocks coincides with the set
of all complex characteristic values of the matrix A.

132
4
Linear Operators
Proof If all characteristic values of the matrix A are real, then this theorem immedi-
ately follows from Theorem 4.23 (see Remark 4.1). Therefore, we assume that among
the characteristic values of the matrix A there exists a complex number λ = α + iβ.
As we have seen in Section4.2.5, p.126, a two-dimensional invariant subspace of
the matrix A in the space Rn corresponds to this number. Let q1, q2 be a basis of this
subspace. Suppose that this basis is orthonormal with respect to the standard inner
product on the space Rn. Then
Aq1 = α11q1 + α21q2,
Aq2 = α12q1 + α22q2.
(4.96)
The matrices
 α β
−β α

and T11 =
α11 α12
α21 α22

are similar to the matrices of the same operator with respect to the different bases.
Therefore,theyhavethesamecharacteristicvaluesλand ¯λ.Wecanjointhevectorsq1,
q2 to some vectors to complete a basis {qk}n
k=1 of the space Rn. Denote by Q the
matrix whose columns are the vectors of this basis. Using equalities (4.96) and the
orthonormality of the vectors q1 and q2, we get
QT AQ =
T11 T12
0 T22

.
The completion of the proof is similar to the corresponding argumentation in the
proof of Schur’s theorem.
□
4.3
Operators on Unitary Spaces
4.3.1
Linear Functionals
LetXbe a complex linear space.A linear map l from X into the one-dimensional
space Y = C is called a linear functional (linear form) on X. We point out that a
complex number l(x) uniquely corresponds to each vector x ∈X.
Theorem 4.28 (Riesz12). Let Xn be a ﬁnite-dimensional unitary space, andl a linear
functional on Xn. Then there exists a unique vector u ∈Xn such that
l(x) = (x, u) for all x ∈Xn.
(4.97)
Proof First, we make sure that exactly one vector u is determined by the linear
functional l. Suppose that there is an additional vector u1 ∈Xn such that
12Frigyes Riesz (1880–1956) was a Hungarian mathematician.

4.3 Operators on Unitary Spaces
133
l(x) = (x, u1) for all
x ∈Xn.
(4.98)
Then subtracting term by term (4.97) from (4.98), we get (x, u1 −u) = 0 for all
x ∈Xn. If we put x = u1 −u in the last equality, then (u1 −u, u1 −u) = 0,
i.e., u1 = u. Let us prove the existence of a vector u deﬁned by (4.97). Let {ek}n
k=1
be an orthonormal basis for Xn, and let x =
n
k=1
ξkek. Since the functional l is linear,
we see that
l(x) =
n

k=1
ξkl(ek).
(4.99)
Put u =
n
k=1
l(ek)ek. Using (3.19), p. 80, we get l(x)=(x, u) for each x ∈Xn.
□
4.3.2
The Adjoint Operator
Let Xn, Ym be unitary spaces, and let A : Xn →Ym be a linear operator. A linear
operator A∗: Ym →Xn is called the adjoint of A if
(Ax, y) = (x, A∗y) for all x ∈Xn and for all y ∈Ym,
(4.100)
where on the left-hand side of (4.100) we use an inner product on the space Ym, and
on the right-hand side of (4.100) we use an inner product on Xn.
For every linear operator A : Xn →Ym there exists an adjoint of A. Indeed, for
each ﬁxed y ∈Ym, the inner product (Ax, y) is a functional on Xn. This functional
is linear, since the operator A is linear and the inner product in the ﬁrst argument is
linear too. Using Riesz’s theorem, we see that there exists a unique vector g ∈Xn
such that
(Ax, y) = (x, g) for all
x ∈Xn.
Therefore a vector g ∈Xn uniquely corresponds to vector y ∈Ym, and thus the map
from Ym to Xn is deﬁned. Denote this map by A∗. Then we can write
(Ax, y) = (x, A∗y) for all x ∈Xn and for all y ∈Ym.
(4.101)
We assert that the map A∗is linear. In fact, if y1, y2 ∈Ym, α, β ∈C, then
(Ax, αy1 + βy2) = ¯α(Ax, y1) + ¯β(Ax, y2)
= ¯α(x, A∗y1) + ¯β(x, A∗y2) = (x, αA∗y1 + βA∗y2).
(4.102)

134
4
Linear Operators
On the other hand, by the deﬁnition of A∗we have
(Ax, αy1 + βy2) = (x, A∗(αy1 + βy2)).
(4.103)
In (4.102) and (4.103) the vector x ∈Xn is arbitrary. Therefore, comparing (4.102)
and (4.103), we see that
A∗(αy1 + βy2) = αA∗y1 + βA∗y2.
The reader can easily prove that for each linear operator A there exists exactly
one adjoint of A.
By the deﬁnition of the adjoint operator we obviously have
(A∗)∗= A.
It is easy to see that
(AB)∗= B∗A∗
(4.104)
and
(αA + βB)∗= ¯αA∗+ ¯βB∗
(4.105)
for all operators A, B and α, β ∈C, and also that if an operator A is invertible, then
the adjoint A∗is invertible too, and
(A∗)−1 = (A−1)∗.
(4.106)
If the space Ym is unitary, then there exists a useful formula for calculating the
matrix of an operator A : Xn →Ym. Let En be a basis of Xn, Qm a basis of Ym, and
Gq = {(q j, qi)}m
i, j=1 the Gram matrix corresponding to the basis Qm. Consider the
matrix
GA =
⎛
⎜⎜⎝
(Ae1, q1) (Ae2, q1) . . . (Aen, q1)
(Ae1, q2) (Ae2, q2) . . . (Aen, q2)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
(Ae1, qm) (Ae2, qm) . . . (Aen, qm)
⎞
⎟⎟⎠.
Then
GA = Gq Aeq.
(4.107)
Indeed, calculating the inner products of both sides of equation (4.9), p. 98, with ql,
we get
(Aei, ql) =
m

j=1
a(eq)
ji (q j, ql),
i = 1, 2, . . . , n, l = 1, 2, . . . , m.
(4.108)

4.3 Operators on Unitary Spaces
135
Formula (4.107) is the matrix form of equality (4.108). The Gram matrix Gq is
nonsingular, since Qm is a basis. Therefore,
Aeq = G−1
q GA.
(4.109)
If the basis Qm is orthonormal, then Gq = I and
Aeq = GA.
(4.110)
If both spaces Ym and Xn are unitary and A∗: Ym →Xn is the adjoint of the
operator A, then as above,
GA∗= Ge A∗
qe,
(4.111)
where Ge is the Gram matrix of the basis En, A∗
qe is the matrix of the operator A∗
with respect to the bases Qm, En, and
GA∗=
⎛
⎜⎜⎝
(A∗q1, e1) (A∗q2, e1) . . . (A∗qm, e1)
(A∗q1, e2) (A∗q2, e2) . . . (A∗qm, e2)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
(A∗q1, en) (A∗q2, en) . . . (A∗qm, en)
⎞
⎟⎟⎠.
Since (A∗qi, e j) = (qi, Ae j) = (Ae j, qi), we see that the matrices GA and GA∗
are mutually adjoint. Hence, using (4.107), we get GA∗= (Aeq)∗Gq, and because
of (4.111), the following equality holds:
A∗
qe = G−1
e (Aeq)∗Gq.
(4.112)
Formula (4.112) shows the relationship between the matrices of operators A and A∗.
In particular, if the bases En and Qm are orthonormal, then the matrices of the oper-
ators A and A∗are mutually adjoint.
4.3.3
Linear Equations in Unitary Spaces
Theorem 4.29 Let Xn,Ym be unitary spaces. Every linear operator A : Xn →Ym
determines an orthogonal decomposition
Ym = Ker(A∗) ⊕Im(A)
(4.113)
of the space Ym.
Proof Suppose that y ∈Im(A), y1 ∈Ker(A∗). Then there exists x ∈Xn such
that y = Ax; hence

136
4
Linear Operators
(y, y1) = (Ax, y1) = (x, A∗y1) = 0,
i.e., y is orthogonal to Ker(A∗). If the vector y ∈Ym is orthogonal to Im(A),
then (y, Ax) = 0 for every x ∈Xn, and (A∗y, x) = 0 for every x ∈Xn. There-
fore, A∗y = 0, i.e., y ∈Ker(A∗). These arguments show that Im(A) is the orthog-
onal complement of Ker(A∗). Thus, using Theorem 3.12, p. 91, we see that equal-
ity (4.113) holds.
□
Obviously, the following decomposition holds too:
Xn = Ker(A) ⊕Im(A∗).
(4.114)
Theorem 4.30 Suppose that a linear operator A maps a ﬁnite-dimensional unitary
space Xn into a ﬁnite-dimensional unitary space Ym. Then
rank(A) = rank(A∗).
(4.115)
Proof The operator A realizes an isomorphism between Im(A∗) and Im(A). Indeed,
using (4.114), for every x ∈Xn, we get Ax = Ax1, where x1 ∈Im(A∗), i.e., each
element of Im(A) is the image of an element of Im(A∗). Suppose that Ax′ = Ax′′ for
distinct elements x′, x′′ of Im(A∗). Then A(x′ −x′′) = 0, and (x′ −x′′) ∈Ker(A).
Since Im(A∗) is a linear subspace, we see that (x′ −x′′) ∈Im(A∗). Using (4.114)
again, we have x′ −x′′ = 0. Hence the ﬁnite-dimensional spaces Im(A) and Im(A∗)
are isomorphic. Thus (see Theorem 4.3, p. 97) they have the same dimension.
□
An immediate consequence of Theorem 4.29 is the following.
Theorem 4.31 (Fredholm’stheorem).LetXn,Ym beunitaryspacesandA : Xn →
Ym a linear operator. A linear equation
Ax = y
(4.116)
has a solution if and only if the vector y is orthogonal to each solution z of the
homogeneous equation A∗z = 0.
Note here that Theorems 4.12, p. 109, and 4.13, p. 109, can be proved on the basis
of Fredholm’s theorem.
Using the decomposition (4.114) and arguing as in the proof of Theorem 4.30,
the reader can prove that if the linear equation (4.116) is solvable, then the set of
all its solutions contains a unique element x0 with minimal length. The element x0
is called the normal solution of equation (4.116). It is easy to see that the vector x0
belongs to Im(A∗).

4.3 Operators on Unitary Spaces
137
4.3.4
Pseudosolutions. The Tikhonov Regularization Method
Suppose that a linear operator A maps a unitary space Xn into a unitary space Ym. Let
y be a ﬁxed element of Ym and x an arbitrary element of Xn. Then the vector Ax −y
is called the residual corresponding to the Eq. (4.116). The real-valued function
F(x) = |Ax −y|2
deﬁned on the space Xn is called the residual functional. If Ax ̸= y, i.e., the vector x
is not a solution of equation (4.116), then F(x) > 0. It is important to ﬁnd a vector x
that minimizes the residual functional.
A vector x ∈Xn minimizing the residual functional is called a pseudosolution
of equation (4.116).13 If equation (4.116) is solvable, then each of its solutions is a
pseudosolution.
A pseudosolution of equation (4.116) exists for every y ∈Ym. Indeed, using
(4.113), we can write y = y1 + y0, where y1 ∈Im(A), y0 ∈Ker(A∗). Then for
every x ∈Xn, the vector Ax −y1 belongs to Im(A). Hence
F(x) = |Ax −y1|2 + |y0|2.
Evidently, the minimum of the function F is equal to |y0|2 and is achieved at the
vector x that is a solution of the equation
Ax = y1.
(4.117)
Equation (4.117) is solvable, since y1 ∈Im(A). The normal solution x0 of (4.117) is
called the normal pseudosolution of equation (4.116).
We can write Ax0 = Py, where P is the operator of the orthogonal projection of
Ym ontoIm(A).AswehaveseenintheproofofTheorem4.30,theoperatorArealizes
an isomorphism between Im(A∗) and Im(A). Therefore, there exists a linear operator
A+ : Ym →Xn such that x0 = A+y, where x0 is the normal pseudosolution of the
equation Ax = y for a given y ∈Ym. The operator A+ is called the pseudoinverse
of A. It is easy to see that if the operator A is invertible, then A+ = A−1.
For every y ∈Ym, the equation
A∗Ax = A∗y
(4.118)
is solvable, and every solution of (4.118) is a pseudosolution of equation (4.116).
Indeed, since A∗y0 =0, we see that equation(4.118)isequivalenttotheequation
A∗(Ax −y1) = 0.
(4.119)
13The problem of calculating the pseudosolution is often called the linear least squares problem.

138
4
Linear Operators
Equation (4.119) is solvable because each solution of (4.117) is a solution of equa-
tion (4.119). Conversely, if x is a solution of equation (4.119), then the vector Ax−y1
belongs to Ker(A∗), and by (4.113), it is orthogonal to Im(A). On the other hand,
Ax −y1 ∈Im(A). Thus Ax −y1 = 0, i.e., x is a solution of equation (4.117).
We say that the original equation (4.116) is reduced to Eq. (4.118) by the Gauss
transformation. The Gauss transformation of a linear equation leads to a solvable
equation.
The Tikhonov14 regularizationmethodcanbeusedforthepractical construction of
the normal pseudosolution of equation (4.116). Along with the residual functional,
consider the so-called regularizing functional (the Tikhonov functional):
Fα(x) = F(x) + α|x|2 = |Ax −y|2 + α|x|2.
(4.120)
Here α is a positive number called the regularization parameter.
Theorem 4.32 For every positive α, there exists a unique vector xα minimizing the
functional Fα on the space Xn. The limit of xα as α →0 exists and is equal to the
normal pseudosolution x0 of Eq.(4.116).
Proof Consider the following equation:
A∗Ax + αx = A∗y.
(4.121)
Equation (4.121) has a unique solution xα ∈Xn for every y ∈Ym. Indeed, if x is
a solution of the homogeneous equation corresponding to (4.121), then calculating
the inner products of both sides of the corresponding homogeneous equation with x
yields |Ax|2 + α|x|2 = 0. Hence x=0, since α > 0. Using the equality A∗y =
A∗Axα + αxα, by elementary calculations we obtain
Fα(x) = (Bα(x −xα), x −xα) + (y, y) −(Bαxα, xα),
where Bα = A∗A + αI. Since (Bα(x −xα), x −xα) > 0 for every x ̸= xα, we see
that xα is the unique minimum point of the functional Fα. Therefore,
Fα(xα) = |Axα−y1|2+|y0|2+α|xα|2 ≤|Ax−y1|2+|y0|2+α|x|2
for all
x ∈Xn.
If we take here x = x0, then
|Axα −y1|2 + α|xα|2 ≤α|x0|2.
(4.122)
This implies that |xα| ≤|x0|, and hence by the Bolzano–Weierstrass theorem (see
a calculus textbook), we can ﬁnd a sequence αk →0 and a vector x∗∈Xn such
14Andrei Nikolaevich Tikhonov (1906–1993) was a Soviet and Russian mathematician.

4.3 Operators on Unitary Spaces
139
that xαk →x∗as αk →0. From (4.122), it follows that Ax∗= y1. The normal
pseudosolution is unique. Therefore, x∗= x0. Using the uniqueness of the normal
pseudosolution again, we see that xα →x0 as α tends to zero in any manner.
□
4.3.5
Self-adjoint and Skew-Hermitian Operators
A linear operator A : Xn →Xn is called self-adjoint (Hermitian) if A∗= A, in
other words, if
(Ax, y) = (x, Ay) for all
x, y ∈Xn.
(4.123)
A linear operator A : Xn →Xn is called skew-Hermitian if A∗= −A, i.e.,
(Ax, y) = −(x, Ay) for all
x, y ∈Xn.
(4.124)
The reader can easily prove that if an operator A is self-adjoint, then the inner
product (Ax, x) is real for all x ∈Xn; if an operator A is skew-Hermitian, then the
inner product (Ax, x) is imaginary for all x ∈Xn.
Sincethematrices of operators A andA∗withrespect toanyorthonormal bases are
mutually adjoint (see Section4.3.2), we see that the matrix of a self-adjoint operator
with respect to an orthonormal basis is Hermitian, and the matrix of a skew-Hermitian
operator is skew-Hermitian.
Theorem 4.33 If the matrix of an operator A with respect to an orthonormal basis
is Hermitian, then the operator A is self-adjoint; if the matrix of an operator A
with respect to an orthonormal basis is skew-Hermitian, then the operator A is
skew-Hermitian.
The proof of Theorem 4.33 is left to the reader.
Theorem 4.34 Each operator of the orthogonal projection15 is self-adjoint.
Proof Let P be an operator of the orthogonal projection of a unitary space X onto
a subspace L ⊂X, and let x and y be arbitrary elements of the space X. By def-
inition, x = Px + x2, y = Py + y2, where the vectors x2 and y2 are orthogonal
to L. Hence, (Px, y) = (Px, Py). Similarly, we have (x, Py) = (Px, Py). There-
fore, (Px, y) = (y, Px).
□
Theorem 4.35 If an operator A is self-adjoint and A2 = A, then the operator A is
the operator of the orthogonal projection.
The proof of Theorem 4.35 is left to the reader.
Arguing exactly as in Section1.2.7, p. 48, it is easy to verify that every operator
can be uniquely represented in the form
15See the deﬁnition on p. 94.

140
4
Linear Operators
A = H1 + iH2,
(4.125)
where i is the imaginary unit, and
H1 = 1
2(A + A∗),
H2 = 1
2i(A −A∗)
are self-adjoint operators.
Theorem 4.36 Let A be a linear operator acting on the unitary space Xn. If
(Ax, x) = 0 for all x ∈Xn,
(4.126)
then A = 0.
Proof Assume ﬁrst that A is self-adjoint. Then for all x, y ∈Xn, the following
equality holds: (A(x + y), x + y) = (Ax, x) + (Ay, y) + 2Re(Ax, y). Combining
this with (4.126), we get Re(Ax, y) = 0. The last equality holds for every y ∈Xn.
Hence we can replace y by iy, but Re(Ax, iy) = Im(Ax, y). Therefore, (Ax, y) = 0
for all x, y ∈Xn. If we put y = Ax, we obtain |Ax| = 0 for every x ∈Xn, i.e.,
A = 0. So the theorem is true for self-adjoint operators. Let now A be an arbitrary
operator. If (Ax, x) = 0, then using (4.125) and considering the self-adjointness of
H1, H2, we get (H1x, x) = 0, (H2x, x) = 0 for every x ∈Xn. Hence, using the
self-adjointness of the operators H1 and H2 again, we see that H1, H2 = 0.
□
Lemma 4.5 Let A be a linear operator acting on the unitary space Xn. If the inner
product (Ax, x) is real for all x ∈Xn, then A is self-adjoint.
Proof If (Ax, x) is real, then (A∗x, x) = (x, Ax) = (Ax, x), and ((A∗−A)x, x) =
0 for every x ∈Xn. Therefore, using Theorem 4.36, we see that A∗−A = 0.
□
The following lemma is proved similarly.
Lemma 4.6 Let A be a linear operator acting on the unitary space Xn. If the inner
product (Ax, x) is imaginary for all x ∈Xn, then A is skew-Hermitian.
Thus we have the following theorem.
Theorem 4.37 Let A be a linear operator acting on the unitary space Xn. The
operator A is self-adjoint if and only if the inner product (Ax, x) is real for all
vectors x ∈Xn; A is skew-Hermitian if and only if the inner product (Ax, x) is
imaginary for all x ∈Xn.
It follows from Theorem 4.37 that all eigenvalues of each self-adjoint operator
are real and all eigenvalues of each skew-Hermitian operator are imaginary. Indeed,
if (x, λ) is an eigenpair of the operator A, then (Ax, x) = λ(x, x).
The reader can easily prove now that the determinant of every self-adjoint operator
is real.

4.3 Operators on Unitary Spaces
141
4.3.6
Positive Deﬁnite and Positive Semideﬁnite Operators
A self-adjoint operator A : Xn →Xn is called positive semideﬁnite if
(Ax, x) ≥0 for all x ∈Xn.
(4.127)
A self-adjoint operator A : Xn →Xn is called positive deﬁnite if
(Ax, x) > 0 for all nonzero x ∈Xn.
(4.128)
A Hermitian matrix A of order n is called positive semideﬁnite if
(Ax, x) =
n

i, j=1
ai jx j ¯xi ≥0 for all x ∈Cn.
(4.129)
A Hermitian matrix A of order n is called positive deﬁnite if
(Ax, x) =
n

i, j=1
ai jx j ¯xi > 0 for all nonzero x ∈Cn.
(4.130)
In the rest of this subsection we give without proof some useful properties of
positive deﬁnite operators and matrices. The proof of the following properties is left
to the reader.
1. The equality (x, y)A = (Ax, y) deﬁnes an inner product on the space Xn for
every positive deﬁnite operator A : Xn →Xn.
2. For every operator A : Xn →Xn, the operator A∗A is self-adjoint and positive
semideﬁnite. If A is invertible, then A∗A is positive deﬁnite.
3. Let A be a linear operator acting on the unitary space Xn. If the operator A + A∗
is positive deﬁnite, then the operator A is nonsingular.
4. The matrix of a positive deﬁnite operator with respect to an orthonormal basis is
positive deﬁnite.
5. All elements of the main diagonal of a positive deﬁnite matrix are positive.
6. The Gram matrix of a set of vectors in a unitary space is positive semideﬁnite.
7. The Gram matrix of a set of vectors is positive deﬁnite if and only if the set of
vectors is linearly independent.
4.3.7
Unitary Operators
An operator A : Xn →Xn is called unitary if
AA∗= A∗A = I.
(4.131)

142
4
Linear Operators
The proof of the following properties of unitary operators is left to the reader.
1. An operator A : Xn →Xn is unitary if and only if its matrix with respect to every
orthonormal basis of Xn is unitary (see p. 49).
2. The modulus of the determinant of a unitary operator is equal to one.
3. The product of two unitary operators is a unitary operator.
If an operator A is unitary, then we have (Ax, Ay) = (x, A∗Ay) = (x, y) for
all x, y ∈Xn, i.e., a unitary operator does not change the inner product of vectors.
Hence it does not change the length of vectors.
Conversely, if a linear operator does not change the inner product of vectors in Xn,
then this operator is unitary. Indeed, taking into account the equality (Ax, Ay) =
(x, y), we obtain (x, A∗Ay) = (x, y). Since the last equality holds for all x, y ∈Xn,
we see that
A∗A = I.
(4.132)
We prove that the equality AA∗= I holds too. From (4.132) it follows that the oper-
ator A is invertible. Then, using left multiplication of both sides of equality (4.132),
by A and then right multiplication by A−1, we obtain AA∗= I.
Now the reader can easily prove that if |Ax| = |x| for all x ∈Xn, then the
operator A is unitary.
Thus a linear operator A : Xn →Xn is unitary if and only if it does not change
the length of every vector in the space Xn.
The modulus of every eigenvalue of a unitary operator is equal to one. Indeed, if
Ax =λx, x ̸= 0, then since |Ax| = |x| for every unitary operator (see Section4.3.7),
we get |λ||x| = |Ax| = |x|, i.e., |λ| = 1.
Let us point out the following useful corollary. Its proof is obvious.
Corollary 4.4 All eigenvalues of a Hermitian matrix are real; all eigenvalues of a
skew-Hermitian matrix are imaginary; the modulus of every eigenvalue of a unitary
matrix is equal to one.
4.3.8
Normal Operators
A linear operator A acting on a unitary space Xn is called normal if
AA∗= A∗A.
Evidently, self-adjoint operators, skew-Hermitian operators, and unitary operators
are normal. An operator is normal if and only if its matrix with respect to every
orthonormal basis of the space Xn is normal (see the deﬁnition of a normal matrix
on p. 50).
Theorem 4.38 Let A:Xn →Xn be a normal operator. Then Ker(A)=Ker(A∗).

4.3 Operators on Unitary Spaces
143
Proof Suppose that Ax = 0. Then
0 = (Ax, Ax) = (A∗Ax, x) = (AA∗x, x) = (A∗x, A∗x),
and hence A∗x = 0. The same calculations show that if A∗x = 0, then Ax=0.
□
Theorems4.29 and 4.38,p.136,immediately imply the following corollary.
Corollary 4.5 Let A : Xn →Xn be a normal operator. Then
Xn = Ker(A) ⊕Im(A) = Ker(A∗) ⊕Im(A∗),
Im(A) = Im(A∗).
Theorem 4.39 Let A : Xn →Xn be a normal operator and (x, λ) an eigenpair
of A, i.e., Ax = λx. Then (x, ¯λ) is an eigenpair of the operator A∗.
Proof It is obvious that if an operator A is normal, then for each λ ∈C, the opera-
tor A −λI is normal too, and (A −λI)∗= A∗−¯λI. If we combine this equality
with Theorem 4.38, we get Ker(A −λI) = Ker(A∗−¯λI).
□
Theorem 4.40 Eigenvectors of a normal operator with distinct eigenvalues are
mutually orthogonal.
Proof Let A be a normal operator, and let Ax = λx, Ay = μy, where λ ̸= μ.
Then λ(x, y) = (Ax, y) = (x, A∗y). By Theorem 4.39, it follows that A∗y = ¯μy;
hence(x, A∗y)=μ(x, y). Thus, λ(x, y)=μ(x, y), and (x, y) = 0, since λ̸=μ. □
Theorem 4.41 Let A be a linear operator acting on the space Xn. There exists an
orthonormal basis {ek}n
k=1 ⊂Xn such that Aek = λkek, k = 1, 2, . . . , n, if and only
if the operator A is normal.
Proof Necessity The matrices of mutually adjoint operators with respect to an ortho-
normal basis are mutually adjoint (see Section4.3.2, p. 133). Hence if
Ae = diag(λ1, λ2, . . . , λn)
is the matrix of the operator A with respect to the orthonormal basis {ek}n
k=1, then
A∗
e = diag(¯λ1, ¯λ2, . . . , ¯λn)
is the matrix of the operator A∗with respect to the same basis. The matrix of the
product of two operators is equal to the product of the matrices of those operators
(see Section4.1.5, p. 98), and diagonal matrices are permutable. Therefore,
(A∗A)e = A∗
e Ae = Ae A∗
e = (AA∗)e,
and thus A∗A = AA∗, i.e., the operator A is normal.

144
4
Linear Operators
Sufﬁciency. Let (e1, λ1) be an eigenpair of A. Suppose that |e1| = 1. By
Theorem 4.39, it follows that (e1, ¯λ1) is an eigenpair of the operator A∗. Denote
by Ln−1 the subspace of all vectors in Xn that are orthogonal to e1. The subspace Ln−1
is invariant under the operator A. Indeed, if x ∈Ln−1, i.e., (x, e1) = 0, then we
get (Ax, e1) = (x, A∗e1) = λ1(x, e1) = 0. Therefore, using Corollary 4.1, p. 116,
we see that there exist a normalized16 vector e2 ∈Ln−1 and a number λ2 such
that Ae2 = λ2e2. Let now Ln−2 be the subspace of all vectors in Xn that are orthogo-
nal to both vectors e1 and e2. Arguing as above, we prove that there exist a normalized
vector e3 ∈Ln−2 and a number λ3 such that Ae3 = λ3e3. Continuing this process,
we construct an orthonormal set of vectors {ek}n
k=1 ⊂Xn such that Aek = λkek,
where k = 1, 2, . . . , n.
□
Remark 4.3 Theorem 4.41 states that for every normal operator A there exists an
orthonormal basis such that the matrix of A with respect to this basis is diagonal,
and all eigenvalues of A form the main diagonal of this matrix. Thus every normal
operator is diagonalizable (see Section4.2.3, p. 121).
Remark 4.4 The following equivalent formulation of the last result is often useful.
Let A be a normal operator acting on the space Xn. Denote by λ1, λ2, …, λk, k ≤n, all
distinct eigenvalues of A and by Lλi, i = 1, 2, . . . , k, all corresponding eigenspaces.
Then
Xn = Lλ1 ⊕Lλ2 ⊕· · · ⊕Lλk,
(4.133)
A = λ1P1 + λ2P2 + · · · + λkPk,
(4.134)
where the sums in (4.133) are orthogonal and Pi is the operator of the orthogonal
projection of the space Xn onto the subspace Lλi for i = 1, 2, . . . , k.
The proof of the following corollary of Theorem 4.41 is left to the reader.
Corollary 4.6 Let A be a real square matrix of order n such that AT A = AAT .
Then there exist an orthonormal17 set of vectors {ξk}n
k=1 ⊂Cn and numbers λ1, …,
λn such that Aξk = λkξk, k = 1, 2, . . . , n. Moreover, if the number λk is real, then
we can choose the corresponding real vector ξk.
The proof of the following three propositions is left to the reader.
Proposition 4.1 If all eigenvalues of a normal operator are real, then the operator is
self-adjoint. If all eigenvalues of a normal operator are imaginary, then the operator
is skew-Hermitian. If the modulus of each eigenvalue of a normal operator is equal
to one, then the operator is unitary.
Proposition 4.2 Let A and B be normal operators whose characteristic polynomials
are equal to each other. Then there exists a unitary operator Q such that B=QAQ∗.
16As usual, a vector x is called normalized if |x| = 1.
17With respect to the standard inner product on the space Cn.

4.3 Operators on Unitary Spaces
145
Proposition 4.3 LetAbeanormaloperator,Qaunitaryoperator.Thentheoperator
˜A = QAQ∗is normal, and the following resolution holds:
˜A = λ1 
P1 + λ2 
P2 + · · · + λk 
Pk.
(4.135)
Here λ1, λ2, . . . , λk are all distinct eigenvalues of A, and 
Pi = QPiQ∗is the
operator of the orthogonal projection of the space Xn onto the subspace QLλi,
where i = 1, 2, . . . , k.
Theorem 4.42 Normal operators A and B are permutable if and only if they have a
common orthonormal basis that consists of their eigenvectors.
Proof Sufﬁciency. Let {e j}n
k=1 be a common orthonormal basis that consists of the
eigenvectors of the operators A and B, i.e., Aek = λkek and Bek = μkek, where k =
1, 2, . . . , n. Then BAek=λkμkek, ABek =λkμkek for k = 1, 2, . . . , n, i.e., for each
vector of the basis, the values of the operators AB and BA coincide. Thus, AB = BA.
Necessity. Let us use representation (4.133) of the space Xn in the form of an
orthogonal sum of the eigenspaces of the operator A corresponding to distinct eigen-
values of A. It follows from Lemma 4.3, p. 116, that every subspace Lλi is invariant
under B. Since the operator B is normal, we see that in each Lλi there exists an
orthonormal basis that consists of eigenvectors of the operator B. Clearly, the union
of all such bases is a basis of the space Xn, and by construction, all vectors of this
basis are eigenvectors of the operator A.
□
4.3.9
The Root of a Positive Semideﬁnite Self-adjoint
Operator
Theorem 4.43 Let A be a positive semideﬁnite self-adjoint operator acting on a
ﬁnite-dimensional unitary space Xn and let k ≥2 be a given integer. Then there
exists a unique positive semideﬁnite self-adjoint operator T such that T k = A.
The operator T is called the kth root of the operator A and is denoted by A1/k or
by
k√
A.
Proof Since the operator A is self-adjoint, there exists an orthonormal basis {ei}n
i=1
consisting entirely of the eigenvectors of A. Let us denote the corresponding eigen-
values by λ1, λ2, …, λn and deﬁne the operator T by the action of this operator on
the basis vectors:
T ei =
k
λi ei,
i = 1, 2, . . . , n.
All eigenvalues of a positive semideﬁnite operator are nonnegative, and hence we
can assume that all numbers
k√λi, i = 1, 2, . . . , n, are nonnegative. Obviously, the
operator T is self-adjoint and positive semideﬁnite. Moreover, we see that T k = A,

146
4
Linear Operators
i.e., T = A1/k. To complete the proof, we shall show that the kth root of the opera-
tor A is unique. For this purpose, we ﬁrst establish that there exists a polynomial Pm
of degree m ≤n −1 such that T = Pm(A). Indeed, let λ1, λ2, . . . , λr, r ≤n, be
all the distinct eigenvalues of the operator A. Then there exists a polynomial Pr−1
of degree r −1 such that Pr−1(λi) =
k√λi, i = 1, 2, . . . ,r.18 Hence
Pr−1(A)ei = Pr−1(λi)ei =
k
λi ei,
i = 1, 2, . . . , n,
i.e., Pr−1(A) = T . Let U be an arbitrary positive semideﬁnite self-adjoint operator
such that Uk = A. Then
T U = Pr−1(A)U = Pr−1(Uk)U = U Pr−1(Uk) = UT ,
i.e., the operators T and U are permutable. Therefore, by Theorem 4.42, these oper-
ators have a common orthonormal basis that consists of their eigenvectors. We also
denote this basis by e1, e2, . . . , en and write
T ei = μiei,
Uei = μiei,
μi, μi ≥0,
i = 1, 2, . . . , n.
Hence
T kei = μk
i ei,
Ukei = μk
i ei,
i = 1, 2, . . . , n,
but T k=Uk. Therefore, μk
i = μk
i , and μi = μi, i = 1, . . . , n. Thus U = T .
□
4.3.10
Congruent Hermitian Operators
Hermitian operators A, B : Xn →Xn are said to be congruent if there exists a
nonsingular operator X such that B = X ∗AX. Let n+(A) be the number of positive
characteristic values of A, n−(A) the number of negative characteristic values of A,
and n0(A) the number of zero characteristic values of A, all counting multiplicity.
Since all characteristic values of a Hermitian operator are real, we see that n+(A) +
n−(A) + n0(A) = n. The inertia of A is the triple (n+(A), n−(A), n0(A)).
Theorem 4.44 (Sylvester’s law of inertia). Hermitian operators A, B are congru-
ent if and only if they have the same inertia.
Proof Sufﬁciency. Let (n+, n−, n0) be the inertia of the operator A and
Aek = λk(A)ek,
k = 1, 2, . . . , n,
(4.136)
18The polynomial Pr−1 can be written in an explicit form, for example using the Lagrange
interpolation formula (see p. 29).

4.3 Operators on Unitary Spaces
147
where ek, k = 1, 2, . . . , n, is the orthonormal set of all eigenvectors of A. We assume
that all eigenvalues of the operator A are ordered by increasing values such that the
ﬁrst n−eigenvalues are negative, the next n0 eigenvalues are zero, and ﬁnally, the
last n+ eigenvalues are positive. Let us deﬁne the Hermitian operator D by the action
of this operator on the basis vectors En = {ek}n
k=1:
Dek =

|λk(A)|−1/2ek, λk(A) ̸= 0,
ek,
λk(A) = 0,
where k = 1, 2, . . . , n. Then we can write equality (4.136) in the form
DADE = ETA,
(4.137)
where TA is a diagonal matrix. The ﬁrst n−elements of its diagonal are equal to
−1, the next n0 elements are zero, and the last n+ elements are equal to one. Let
Qn = {qk}n
k=1 be an orthonormal basis in Xn. We deﬁne the operator M by the
following equality:
MQ = QTA.
(4.138)
The bases En and Qn are orthonormal; hence there exists a unitary operator U such
that E = UQ (see p. 76), and we can write (4.137) in the form
U∗DADUQ = QTA.
(4.139)
Comparing the left-hand sides of (4.138) and (4.139), we see that the operators A
and M are congruent. Thus all operators having the same inertia (n+, n−, n0) are
congruent to the operator M. Therefore, all of them are mutually congruent.
Necessity. We denote by L+, L−, L0 the subspaces of the space Xn spanned by
the eigenvectors of the operator A corresponding to the positive, negative, and zero
eigenvalues of the operator A, respectively. Let us decompose the space Xn into the
orthogonal sum Xn = L+ ⊕L−⊕L0 (see Remark 2, p. 144). Then we see that
dim(L+) + dim(L−) + dim(L0) = n. Denote by M+ the subspace of Xn spanned
by all eigenvectors of the operator B corresponding to all of its positive eigenval-
ues. For each x ∈M+, x ̸= 0, we have (Bx, x) = (AX x, X x) = (Ay, y) > 0,
where y = X x. This means that (Ay, y) > 0 for each y belonging to the sub-
space 
M+ = X M+. Since X is invertible, dim(M+) = dim( 
M+). Obviously,

M+∩(L−⊕L0) = {0}; hence dim(M+)+dim(L−)+dim(L0) ≤n, and dim(M+) ≤
dim(L+). Arguing similarly, we get the opposite inequality, whence dim(M+) =
dim(L+), or n+(A) = n+(B). For the same reason, we get n−(A) = n−(B),
n0(A) = n0(B).
□

148
4
Linear Operators
4.3.11
Variational Properties of Eigenvalues of Self-adjoint
Operators
Recall that a linear operator A : Xn →Xn is self-adjoint if
(Ax, y) = (x, Ay) for all
x, y ∈Xn.
(4.140)
Recall also that all eigenvalues of a self-adjoint operator are real and that there exists
an orthonormal basis of the space Xn consisting of the eigenvectors of the operator A.
Let A : Xn →Xn be a self-adjoint operator, λ1, λ2, …, λn the eigenvalues of A,
and {ek}n
k=1 the orthonormal basis of the corresponding eigenvectors. We assume that
the eigenvalues are ordered by increasing values:
λ1 ≤λ2 · · · ≤λn.
(4.141)
Let us point out that we consider the characteristic values of the matrix of the
operator A to be the eigenvalues of A, i.e., each multiple eigenvalue is repeated
according to its multiplicity. Therefore, generally speaking, the inequalities in (4.141)
are not strict.
Let p, q be integers such that 1 ≤p ≤q ≤n. Denote by L pq the subspace of the
space Xn spanned by the vectors {ek}q
k=p . Clearly, L1n = Xn.
Lemma 4.7 For every x ∈L pq, the following inequalities hold:
λp(x, x) ≤(Ax, x) ≤λq(x, x).
(4.142)
Moreover,
λp =
min
x∈L pq, x̸=0
(Ax, x)
(x, x) ,
λq =
max
x∈L pq, x̸=0
(Ax, x)
(x, x) .
(4.143)
Proof For every x ∈L pq, we have
(Ax, x) =

A
q

k=p
ξkek,
q

k=p
ξkek

=

q

k=p
λkξkek,
q

k=p
ξkek

=
q

k=p
λk|ξk|2.
(4.144)
Evidently,
λp
q

k=p
|ξk|2 ≤
q

k=p
λk|ξk|2 ≤λq
q

k=p
|ξk|2,
q

k=p
|ξk|2 = (x, x),

4.3 Operators on Unitary Spaces
149
and hence (4.142) holds, and for every x ̸= 0 that belongs to L pq, the following
inequalities hold:
λp ≤(Ax, x)
(x, x) ≤λq.
We have
(Aep, ep)
(ep, ep) = λp,
(Aeq, eq)
(eq, eq) = λq.
Thus equalities (4.143) hold also.
□
Obviously, the next theorem follows from Lemma 4.7.
Theorem 4.45 For each k = 1, 2, . . . , n, the following equalities hold:
λk =
min
x∈Lkn, x̸=0
(Ax, x)
(x, x) ,
λk =
max
x∈L1k, x̸=0
(Ax, x)
(x, x) .
(4.145)
Note that Lkn = L⊥
1,k−1, L1k = L⊥
k+1,n. Therefore, for calculating the kth eigen-
value, we need to know all eigenvectors e j for j = 1, 2, . . . , k −1 or for j =
k + 1, . . . , n. Thus formulas (4.145) are inconvenient. The next two theorems give
descriptions of each eigenvalue of the self-adjoint operator A without reference to
the preceding or succeeding eigenvectors.
Theorem 4.46 For each k = 1, 2, . . . , n, the following equality holds:
λk = max
Rn−k+1
min
x∈Rn−k+1, x̸=0
(Ax, x)
(x, x) .
(4.146)
Here Rn−k+1 is an (n −k +1)-dimensional subspace of the space Xn. The maximum
is taken over all subspaces Rn−k+1 ⊂Xn of dimension n −k + 1.
Proof Clearly,dim(Rn−k+1) + dim(L1k) = n + 1. Hence (seeCorollary3.1, p.87)
there exists a vector x ̸= 0 belonging to Rn−k+1 ∩L1k. Therefore, using (4.145),
we see that for each subspace Rn−k+1, there exists a vector x ∈Rn−k+1 such that
(Ax, x)/(x, x) ≤λk. Thus for each subspace Rn−k+1, we get
min
x∈Rn−k+1, x̸=0
(Ax, x)
(x, x) ≤λk.
If we choose now a subspace Rn−k+1 for which
min
x∈Rn−k+1, x̸=0
(Ax, x)
(x, x) = λk,
then we have proved equality (4.146). It follows from Theorem 4.45 that the desired
subspace Rn−k+1 is Lkn.
□

150
4
Linear Operators
Theorem 4.47 For each k = 1, 2, . . . , n, the following equality holds:
λk = min
Rk
max
x∈Rk, x̸=0
(Ax, x)
(x, x) .
(4.147)
Here Rk is a k-dimensional subspace of the space Xn. The minimum is taken over
all subspaces Rk ⊂Xn of dimension k.
Proof Clearly, dim(Rk) + dim(Lkn) = n + 1 for each subspace Rk, and therefore,
we see that Rk ∩Lkn ̸= {0}. By Theorem 4.45, we have
min
x∈Lkn, x̸=0
(Ax, x)
(x, x) = λk,
and hence for each subspace Rk, we get
max
x∈Rk, x̸=0
(Ax, x)
(x, x) ≥λk.
To conclude the proof, it remains to choose a k-dimensional subspace Rk for which
max
x∈Rk, x̸=0
(Ax, x)
(x, x) = λk.
Using Theorem 4.45, we see that the desired subspace is L1k.
□
It follows immediately from (4.142) that a self-adjoint operator A is positive
semideﬁnite (see (4.127), p. 141) if and only if all eigenvalues of A are nonnegative;
a self-adjoint operator A is positive deﬁnite (see (4.128), p. 141) if and only if all
eigenvalues of A are positive. Using the last statement, the reader can easily prove
the following proposition.
Proposition 4.4 If an operator A is positive deﬁnite, then det(A) > 0.
NowthereadercaneasilyprovetheCauchy–Schwarzinequality(seeTheorem3.1,
p. 72) using the Gram matrix (see (3.7), p. 74) for a set of two vectors x, y in a unitary
space.
4.3.12
Examples of Application of Variational Properties
of Eigenvalues
Theorem 4.48 Let A, B, C : Xn →Xn be self-adjoint operators, and let
λ1(A) ≤λ2(A) ≤· · · ≤λn(A),

4.3 Operators on Unitary Spaces
151
λ1(B) ≤λ2(B) ≤· · · ≤λn(B),
λ1(C) ≤λ2(C) ≤· · · ≤λn(C)
be eigenvalues of A, B, and C, respectively. Suppose that A = B + C. Then
λ1(C) ≤λk(A) −λk(B) ≤λn(C),
k = 1, 2, . . . , n.
(4.148)
Proof To prove this statement it is enough to note that for each arbitrarily ﬁxed
subspace Rk of the space Xn, we have
(Ax, x)
(x, x) = (Bx, x)
(x, x) + (Cx, x)
(x, x)
for all
x ∈Rk, x ̸= 0.
Since (4.142), we see that
(Cx, x)
(x, x) ≤λn(C) for all
x ∈Xn, x ̸= 0.
Hence
max
x∈Rk, x̸=0
(Ax, x)
(x, x) ≤
max
x∈Rk, x̸=0
(Bx, x)
(x, x) + λn(C),
and thus
min
Rk
max
x∈Rk, x̸=0
(Ax, x)
(x, x) ≤min
Rk
max
x∈Rk, x̸=0
(Bx, x)
(x, x) + λn(C).
By Theorem 4.47, the last inequality is equivalent to the following:
λk(A) −λk(B) ≤λn(C).
(4.149)
Note that B = A + (−C). The eigenvalues of the operator −C are equal to −λk(C),
k = 1, 2, . . . , n, and the maximal eigenvalue of −C is equal to −λ1(C). Therefore,
arguing as above, we get
λk(B) −λk(A) ≤−λ1(C).
(4.150)
Combining (4.149) and (4.150), we obtain (4.148).
□
The estimates (4.148) are useful, because they show how the eigenvalues of a self-
adjoint operator B can change if we add to B a self-adjoint operator C. It is evident
that if the eigenvalues of the operator C are small, then changes of the eigenvalues
of B are small too.
Theorem 4.49 Let An+1 = {ai j}n+1
i, j=1 be an arbitrary Hermitian matrix of order
n + 1, and let An = {ai j}n
i, j=1 be the matrix corresponding to its leading principal

152
4
Linear Operators
minor of order n. Let ˆλ1 ≤ˆλ2 ≤· · · ≤ˆλn+1 be the eigenvalues of the matrix An+1,
and let λ1 ≤λ2 ≤· · · ≤λn be the eigenvalues of An. Then
ˆλ1 ≤λ1 ≤ˆλ2 ≤λ2 ≤· · · ≤λn ≤ˆλn+1,
(4.151)
i.e., the eigenvalues of An+1 are interlaced with the eigenvalues of An.
Proof In this proof we use the standard inner product on Cn. Let 1 ≤k ≤n. By
Theorem 4.47,
ˆλk+1 = min
Rk+1
max
x∈Rk+1, x̸=0
(An+1x, x)
(x, x)
.
(4.152)
The minimum here is taken over all subspaces Rk+1 of dimension k + 1 of the
space Cn+1. Denote by Rk ⊂Cn the set of all vectors in Rk+1 such that the (n + 1)th
coordinate with respect to the natural basis is zero. Then
max
x∈Rk+1, x̸=0
(An+1x, x)
(x, x)
≥
max
x∈Rk, x̸=0
(Anx, x)
(x, x) .
To justify this inequality, it is enough to note that on the left-hand side, the maxi-
mum is taken over a broader set of vectors than on the right-hand side. Therefore,
using (4.152), we get
ˆλk+1 = min
Rk+1
max
x∈Rk+1, x̸=0
(An+1x, x)
(x, x)
≥min
Rk
max
x∈Rk, x̸=0
(Anx, x)
(x, x) ,
but by Theorem 4.47, the right-hand side of this inequality is equal to λk. Thus,
ˆλk+1 ≥λk for all k = 1, 2, . . . , n.
Let us now use Theorem 4.46. By this theorem,
ˆλk = max
Rn+2−k
min
x∈Rn+2−k, x̸=0
(An+1x, x)
(x, x)
.
(4.153)
The maximum here is taken over all subspaces Rn+2−k of dimension n +2−k of the
space Cn+1. If we narrow the set of vectors over which the minimum is taken, then
this minimum cannot decrease. Therefore, analogously to the previous case, we can
write
ˆλk = max
Rn+2−k
min
x∈Rn+2−k, x̸=0
(An+1x, x)
(x, x)
≤max
Rn+1−k
min
x∈Rn+1−k, x̸=0
(Anx, x)
(x, x)
= λk.
(4.154)
Thus inequalities (4.151) hold.
□
In the same way we can prove the following more general result.

4.3 Operators on Unitary Spaces
153
Theorem 4.50 Let A be a Hermitian matrix of order n and Am be the Hermitian
matrix of order m < n corresponding to a principal minor of order m of the matrix A
(see Section4.2.4, p. 121). Let λ1(A) ≤λ2(A) ≤· · · ≤λn(A) be the eigenvalues of
the matrix A, and let λ1(Am) ≤λ2(Am) ≤· · · ≤λm(Am) be the eigenvalues of the
matrix Am. Then
λk(A) ≤λk(Am) ≤λk+n−m(A),
k = 1, 2, . . . , m.
(4.155)
Remark 4.5 Clearly, Theorem 4.49 is the particular case of Theorem 4.50 when
m = n −1 and An−1 corresponds to the leading principal minor of the matrix A of
order n −1. Sometimes it is convenient to order the eigenvalues by nonincreasing
values. Then, obviously, the estimate (4.155) has the form
λk+n−m(A) ≤λk(Am) ≤λk(A),
k = 1, 2, . . . , m.
(4.156)
Theorem 4.51 (Sylvester’s criterion). A Hermitian matrix A is positive deﬁnite if
and only if all the leading principal minors of A are positive.
Proof Necessity. Take an integer k, 1 ≤k ≤n. If in condition (4.130), p. 141, we
put x = (x1, . . . , xk, 0, . . . , 0) = (y, 0, . . . , 0), where y is an arbitrary vector in Ck,
then(Ax, x) = (Ak y, y).Here Ak isthematrixcorrespondingtotheleadingprincipal
minor of order k of the matrix A.19 Evidently, it follows now from condition (4.130)
that (Ak y, y) > 0 for every nonzero vector y ∈Ck, i.e., the matrix Ak is positive
deﬁnite. Therefore, its determinant (the leading principal minor of order k of the
matrix A) is positive (see Proposition 4.4, p. 150).
Sufﬁciency. Now we prove that if all leading principal minors of the matrix A are
positive, then all its eigenvalues are positive. The last condition means that the matrix
A is positive deﬁnite. Actually, we prove more, namely, that all eigenvalues of all
leadingprincipalminorsofthematrix A arepositive.Obviously,fortheminoroforder
one, i.e., for a11, the assertion is true. Let us assume that all eigenvalues λ1 ≤· · · ≤λk
of the matrix Ak corresponding to the leading principal minor of order k are positive,
and prove that all eigenvalues ˆλ1 ≤· · · ≤ˆλk+1 of the matrix Ak+1 are positive too.
Using Theorem 4.49, we see that the following inequalities hold:
ˆλ1 ≤λ1 ≤ˆλ2 ≤λ2 ≤· · · ≤λk ≤ˆλk+1.
Therefore, ˆλ2, . . . , ˆλk+1 > 0. Since by hypothesis, det(Ak+1) > 0, and by equality
(4.79), p. 124, det(Ak+1) = ˆλ1ˆλ2 · · · ˆλk+1, we get ˆλ1 > 0.
□
Now we introduce two concepts which will be used below. Let x, y ∈Rn. Addi-
tionally, we assume that x1 ≥x2 ≥· · · ≥xn, y1 ≥y2 ≥· · · ≥yn. We write x ≺w y
and say that x is weakly majorized by y if
19The matrix Ak is usually called the leading principal submatrix of order k of the matrix A.

154
4
Linear Operators
k

i=1
xi ≤
k

i=1
yi,
k = 1, 2, . . . , n.
We write x ≺y and say that x is majorized by y if x ≺w y and
n

i=1
xi =
n

i=1
yi.
(4.157)
Theorem 4.52 (Schur). Let A be a Hermitian matrix of order n. Let λ(A) ∈Rn be
the vector consisting of all the eigenvalues of the matrix A ordered by nonincreasing
values, and let d(A) ∈Rn be the vector consisting of all the diagonal entries of the
matrix A ordered by nonincreasing values. Then
d(A) ≺λ(A).
(4.158)
Proof Since for every permutation matrix P, the eigenvalues of the matrices A
and P AP are equal, without loss of generality we can assume that the matrix A
is such that all its diagonal entries are ordered by nonincreasing values, i.e., a11 ≥
a22 ≥· · · ≥ann. Let Ak be the leading principal submatrix of A of order k. Using
equality (4.79), p. 124, and the estimate (4.156), we get
k

i=1
aii =
k

i=1
λi(Ak) ≤
k

i=1
λi(A).
(4.159)
Now using (4.79), p. 124, with respect to the matrix A, we see that for k = n,
inequality (4.159) transforms to an equality.
□
The next corollary is obvious.
Corollary 4.7 Let A be a Hermitian matrix and U a unitary matrix. Then
d(U ∗A U) ≺λ(A).
Theorem 4.53 Let A be a Hermitian matrix of order n. Assume that all the eigen-
values of A are ordered by nonincreasing values. Then
k

i=1
λi(A) = max
V
tr(V ∗A V ),
k = 1, 2, . . . , n.
The maximum here is taken over all rectangular unitary matrices V ∈Mn,k.20
20See the deﬁnition on p. 50.

4.3 Operators on Unitary Spaces
155
Proof Let V be an arbitrary rectangular n × k unitary matrix. Let U = (V, W)
be a square unitary matrix of order n. For every matrix W, the diagonal elements
of the matrix V ∗A V equal the ﬁrst k diagonal elements of the matrix U ∗A U. By
Corollary 4.7, their sum is no more than the number
k
i=1
λi(U ∗A U), which is equal
to
k
i=1
λi(A). If the columns of the matrix V are the eigenvectors of the matrix A
corresponding to λ1(A), λ2(A), …, λk(A) and are orthonormal with respect to the
standard inner product on the space Cn, then tr(V ∗A V ) =
k
i=1
λi(A).
□
Theorem 4.54 (Fan21). Let A, B be Hermitian matrices of the same order. Then
λ(A + B) ≺(λ(A) + λ(B)).
This theorem follows immediately from Theorem 4.53 and the fact that the trace
of the sum of matrices is equal to the sum of their traces (see (4.80), p. 124).
4.4
Operators on Euclidean Spaces
4.4.1
Overview
Let Xn be a Euclidean space (i.e., an n-dimensional real inner product space). In this
section we consider linear operators A acting on the Euclidean space Xn and note
some features related to the assumption that Xn is real.
The matrices of operators A and A∗with respect to an orthonormal basis of the
space Xn are transposes of each other.
A linear operator is self-adjoint if and only if the matrix of this operator with
respect to every orthonormal basis of the space Xn is symmetric.
Skew-Hermitian operators acting on a Euclidean space are usually called skew-
symmetric. A linear operator is skew-symmetric if and only if the matrix of this
operator with respect to every orthonormal basis of the space Xn is skew-symmetric.
Every linear operator A : Xn →Xn can be uniquely represented in the form
A = A1 + A2,
where A1 is a self-adjoint operator, A2 is a skew-symmetric operator, and
A1 = 1
2(A + A∗),
A2 = 1
2(A −A∗).
21Ky Fan (1914–2010) was an American mathematician.

156
4
Linear Operators
Similar arguments for matrices can be found on pp. 48, 49.
Theorem 4.5522 A linear operator A acting on the Euclidean space Xn is skew-
symmetric if and only if
(Ax, x) = 0 for all x ∈Xn.
(4.160)
Proof If A = −A∗, then
(Ax, x) = (x, A∗x) = −(x, Ax),
i.e., (Ax, x) = 0. The sufﬁciency of condition (4.160) follows from the obvious
identity (A(x + y), x + y) = (Ax, x) + (Ay, y) + (Ax + A∗x, y).
□
Unitary operators (i.e., operators satisfying the condition AA∗= I) acting on a
Euclidean space are called orthogonal. A linear operator is orthogonal if and only
if the matrix of this operator with respect to every orthonormal basis of the space Xn
is orthogonal (see Section1.2.7, p. 48).
An orthogonal operator does not change the lengths of vectors and the angles
between vectors, which follows immediately from the deﬁnition. The determinant
of an orthogonal operator is equal to plus or minus one. Every eigenvalue of an
orthogonal operator is equal to plus or minus one.
Recall that a linear operator A is normal if AA∗= A∗A. Self-adjoint operators,
skew-symmetric operators, and orthogonal operators are normal.
If an operator A : Xn →Xn is normal, then the matrix Ae of the operator A with
respect to every orthonormal basis of the space Xn is normal, i.e., Ae satisﬁes the
following condition:
Ae AT
e = AT
e Ae.
(4.161)
The converse is also true: if there exists an orthonormal basis En of the space Xn such
that the matrix of the operator A with respect to this basis satisﬁes condition (4.161),
then the operator A is normal.
4.4.2
The Structure of Normal Operators
In this subsection we consider linear operators acting on the Euclidean space Xn.
Theorem 4.56 Let A be a linear operator acting on the Euclidean space Xn. The
operator A is normal if and only if there exists an orthonormal basis En of the
space Xn such that the matrix of the operator A with respect to this basis is block
diagonal:
22Compare with Theorem 4.36 p. 140.

4.4 Operators on Euclidean Spaces
157
Ae =
⎛
⎜⎜⎜⎝
A1
A2
...
Ak
⎞
⎟⎟⎟⎠.
(4.162)
Every diagonal block here is an 1 × 1 matrix or a 2 × 2 matrix. Each 1 × 1 block is
a real number, while each 2 × 2 block is a matrix of the form
Ap =
αp −βp
βp αp

,
(4.163)
where αp, βp are real numbers.
Proof Sufﬁciency. By direct calculations we can easily verify that the matrix Ae as
described in the statement of the theorem satisﬁes condition (4.161).
Necessity. Let Ae be the matrix of the normal operator A with respect to an
arbitrarily chosen orthonormal basis En. Then Ae satisﬁes condition (4.161). Using
Corollary 4.6, p. 144, we see that for the matrix Ae, there exists an orthonormal
basis Fn = { fk}n
k=1 of the space Cn such that
Ae fk = λk fk,
k = 1, 2, . . . , n,
(4.164)
where λ1, λ2, . . . , λn are the characteristic values of the matrix Ae, and if λk isreal,
then the corresponding vector fk is real. Let us enumerate the characteristic values
of the matrix Ae in the following order: λ1 = α1, λ2 = α2, . . . , λm = αm, where
0 ≤m ≤n are real, and λm+ j = αm+ j + iβm+ j, ¯λm+ j = αm+ j −iβm+ j, for j =
1, 2, . . . , p, where p = (n −m)/2, are complex. Then the eigenvectors fk for
k = 1, 2, . . . , m are real, and the other corresponding eigenvectors are complex, i.e.,
fk = gk + ihk, where gk, hk ∈Rn, k > m. The matrix Ae is real, and therefore,
if λk is a complex characteristic value of Ae and Ae fk = λk fk, then Ae ¯fk = ¯λk ¯fk.
By Theorem 4.40, p. 143, we see that the eigenvectors of the normal operator A
satisfying distinct eigenvalues are orthogonal to each other. Hence ( fk, ¯fk) = 0,
and (gk, gk) = (hk, hk), (gk, hk) = 0. Moreover, we have ( fk, fk) = 1. This easily
yields that (gk, gk) = (hk, hk) = 1/2. Let now fk, fl ∈Fn, k ̸= l, be complex
vectors such that fk ̸= ¯fl. Then we have ( fk, fl) = 0 and ( fk, ¯fl) = 0, whence
by elementary calculations we obtain (gk, gl), (hk, hl), (gk, hl), (hk, gl) = 0. Recall
that (see Section4.2.5, p. 125) if Ae fk = λk fk, where λk = αk + iβk, fk = gk + ihk,
then Aegk = αkgk −βkhk, Aehk = αkgk + βkhk. We now associate with each real
eigenvalue λk of the matrix Ae the real eigenvector fk ∈Fn; we associate the pair
of real eigenvectors ˜gk =
√
2 gk, ˜hk =
√
2 hk with each pair of complex-conjugate
characteristic values λk, ¯λk of the matrix Ae. As a result, we obtain the following set
of n vectors in the space Rn:

Fn = { f1, f2, . . . , fm, ˜g1, ˜h1, ˜g2, ˜h2, . . . , ˜gp, ˜h p}.

158
4
Linear Operators
We have proved that this set is orthonormal. For the vectors of the set 
Fn we get
Ae fk = αk fk,
k = 1, 2, . . . , m,
(4.165)
Ae ˜g j = α j ˜g j −β j ˜h j,
Ae ˜h j = β j ˜g j + α j ˜h j,
(4.166)
where j = 1, 2, . . . , p. Using (4.165) and (4.166), we see that the matrix of the
operator A with respect to the orthonormal basis En = E 
Fn of the space Xn has the
form (4.162). The blocks of this matrix consist of the corresponding elements of the
matrix Ae.
□
Let us discuss two important special cases using Corollary 4.4, p. 142.
1. Self-adjoint operators. The matrix of a self-adjoint operator A with respect to
an orthonormal basis is symmetric. By Corollary 4.4, p. 142, all the characteristic
values of this matrix are real. Therefore, all the numbers β j, j = 1, 2, . . . , p, in
equalities (4.166) are equal to zero. Thus there exists an orthonormal basis of the
space Xn such that the matrix of the operator A with respect to this basis is diagonal.
2. Skew-symmetric operators. The matrix of a skew-symmetric operator A with
respect to an orthonormal basis is skew-symmetric. By Corollary 4.4, p. 142, all
the characteristic values of this matrix are imaginary. Therefore all numbers α j in
equalities (4.165), (4.166) are equal to zero. Thus there exists an orthonormal basis
of the space Xn such that the matrix of the operator A with respect to this basis has
the form (4.162), where all the diagonal blocks of order one are equal to zero, and
all the blocks of order two are skew-symmetric:
A j =
 0 −β j
β j
0

,
where j = 1, 2, . . . , p.
4.4.3
The Structure of Orthogonal Operators
The matrix of an orthogonal operator with respect to an orthonormal basis is orthog-
onal. By Corollary 4.4, p. 142, the modulus of each characteristic value of this
matrix is equal to one. Therefore, all the numbers αk, k = 1, 2, . . . , m, in equali-
ties (4.165) are equal to plus or minus one; the numbers α j, β j for j = 1, 2, . . . , p
in (4.166) satisfy the conditions α2
j + β2
j = 1; hence there exist angles ϕ j ∈[0, 2π)
such that α j = cos ϕ j, β j = sin ϕ j. Thus there exists an orthonormal basis of the
space Xn such that the matrix of the orthogonal operator with respect to this basis
has the form (4.162), where each diagonal block of order one is a number equal to
plus or minus one, and each diagonal block of order two has the following form:

4.4 Operators on Euclidean Spaces
159
cos ϕ j −sin ϕ j
sin ϕ j
cos ϕ j

.
Now we can give a clear geometric interpretation of each orthogonal transforma-
tion of the Euclidean space Xn.
Let us begin with the two-dimensional case. As follows from the above, for each
orthogonal transformation A of the Euclidean space X2 there exists an orthonormal
basis e1, e2 such that the matrix of the transformation with respect to this basis has
either the form
Ae =
−1 0
0 1

or the form
Ae =
cos ϕ −sin ϕ
sin ϕ
cos ϕ

.
In the ﬁrst case, the operator A transforms each vector x = ξ1e1 + ξ2e2 ∈X2 into
the vector Ax = −ξ1e1 + ξ2e2, i.e., the operator A performs a specular reﬂection
with respect to the coordinate axis ξ2.
In the second case, (Ax, x) = |x||Ax| cos ϕ, i.e., the operator A performs a
rotation of each vector x ∈X2 through an angle ϕ. For ϕ > 0, the direction of the
rotation coincides with the direction of the shortest rotation from e1 to e2.
In the three-dimensional case, every orthogonal operator A has at least one eigen-
value, since the corresponding characteristic equation is an polynomial equation of
degree three with real coefﬁcients. Therefore, the matrix Ae of the operator A with
respect to the orthonormal basis e1, e2, e3 ∈X3 (renumbered if necessary) has one
of the following forms:
Ae =
⎛
⎝
1
0
0
0 cos ϕ −sin ϕ
0 sin ϕ
cos ϕ
⎞
⎠,
(4.167)
Ae =
⎛
⎝
−1
0
0
0 cos ϕ −sin ϕ
0 sin ϕ
cos ϕ
⎞
⎠.
(4.168)
Observe that if the operator A has exactly one eigenvalue, then these representations
follow immediately from Theorem 4.56. If the operator A has three eigenvalues, then
we obtain representation (4.167) or (4.168) by choosing a special angle ϕ.
Arguing by analogy with the two-dimensional case, it is easy to verify that an
operator A that has the matrix (4.167) performs a rotation through an angle ϕ about
the coordinate axis ξ1, while an operator A that has the matrix (4.168) performs
ﬁrst a rotation through an angle ϕ about the coordinate axis ξ1 and then a specular
reﬂection with respect to the ξ2ξ3 coordinate plane. In the ﬁrst case, the determinant
of the operator A is equal to one, while in the second case, it is equal to minus one.

160
4
Linear Operators
As we know, the determinant of a linear operator does not depend on the choice
of the basis of the space. Therefore, we can divide all orthogonal transformations of
a three-dimensional space into two classes: proper rotations and improper rotations.
A proper rotation is a transformation with a positive determinant. It performs a
rotation of the space about an axis. An improper rotation is a transformation with a
negative determinant. It is a combination of a rotation about an axis and a reﬂection
in the plane that is orthogonal to that axis.
Using Theorem 4.56, we can represent the Euclidean space Xn of arbitrary dimen-
sion n as an orthogonal sum of some one-dimensional invariant subspaces of the
orthogonal operator A : Xn →Xn and some two-dimensional invariant subspaces
of A. In each two-dimensional invariant subspace, the operator A performs a rotation
through an angle. Generally speaking, these angles can differ for different subspaces.
In each one-dimensional invariant subspace, only the direction of a coordinate axis
can be transformed.
The proof of the following proposition is left to the reader.
Proposition 4.5 Everyrealsymmetricmatrix A isorthogonallysimilartoadiagonal
matrix, i.e., QT AQ = Λ, where Λ is a diagonal matrix and Q is an orthogonal
matrix. The columns of the matrix Q are the eigenvectors of A. The diagonal elements
of the matrix Λ are the eigenvalues of A.
4.4.4
Givens Rotations and Householder Transformations
In this subsection we consider two important types of orthogonal matrices that are
often used in applications.
1. Givens23 rotations. A real matrix Qst(ϕ) = {qi j(ϕ)}n
i, j=1, 1 ≤s < t ≤n,
is called a Givens rotation if qss(ϕ) = qtt(ϕ) = cos ϕ, qii(ϕ) = 1 for i ̸= s, t,
qst(ϕ) = −sin ϕ, qts(ϕ) = sin ϕ, and all other elements of the matrix Qst(ϕ) are
equal to zero.
It is easy to see that the matrix Q = Qst(ϕ) is orthogonal. This matrix deﬁnes an
orthogonal transformation of the Euclidean space Rn with the standard inner product,
and it performs a rotation trough an angle ϕ in the two-dimensional space (in the
plane) spanned by the vectors is, it of the natural basis in the space Rn. The matrix
QT is the inverse of Q and performs the opposite rotation in the same plane.
Let x be an arbitrary vector in the space Rn. Obviously, (Qx)i = xi for i ̸= s, t,
(Qx)s = xs cos ϕ −xt sin ϕ, (Qx)t = xs sin ϕ + xt cos ϕ. Take ρ = (x2
s + x2
t )1/2.
Suppose that ϕ = 0 if ρ = 0 and cos ϕ = xs/ρ, sin ϕ = −xt/ρ if ρ > 0. Then we
get (Qx)s = ρ, (Qx)t = 0.
Now it is perfectly clear that if x is an arbitrary nonzero vector in Rn, then sequen-
tially choosing the angles ϕn, ϕn−1, …, ϕ2, we can construct the Givens rotations
Q1,n(ϕn), Q1,n−1(ϕn−1), …, Q1,2(ϕ2) such that Qx = |x| i1, where
23Wallace Givens (1910–1993) was an American mathematician.

4.4 Operators on Euclidean Spaces
161
Q = Q1,2(ϕ2) · · · Q1,n−1(ϕn−1)Q1,n(ϕn).
Thus, using an orthogonal matrix, we can transform any nonzero vector into a vector
whose direction coincides with the direction of the vector i1 of the natural basis.
Let x, y be two arbitrary nonzero vectors in Rn. As we have just shown, there
exist orthogonal matrices Qx and Qy such that Qxx = |x|i1, Qyy = |y|i1. There-
fore, Qx = (|x|/|y|)y, where Q = QT
y Qx, i.e., for every pair of nonzero vectors
there exists an orthogonal matrix that transforms the ﬁrst vector into a vector whose
direction coincides with the direction of the second vector.
2. Householder24 transformations. Let w = {wi}n
i=1 be an arbitrarily chosen vector
in Rn with |w| = 1. A matrix
R = I −2wwT
is called a Householder transformation (or reﬂection). We explain that the vector w
is treated here as a column vector. Hence R = {δi j −2wiw j}n
i, j=1.
The matrix R is symmetric. Let us show that this matrix is orthogonal. Indeed,
RT R = R2 = I −4wwT + 4wwT wwT = I,
because wT w = |w|2 = 1. Note further that
Rw = w −2wwT w = −w,
Rz = z −2wwT z = z,
(4.169)
if wT z = (w, z) = 0, i.e., the vectors w and z are orthogonal.25
Now let x be an arbitrary vector. By Theorem 3.1.3, p. 91, it can be uniquely
represented in the form x = αw + z, where α is a real number and z is a vector
orthogonal to w. Using equalities (4.169), we see that Rx = −αw + z. We can say
therefore that the matrix R performs a specular reﬂection of the vector x with respect
to the (n −1)-dimensional hyperplane orthogonal to the vector w. This property of
the matrix R allows us to call it a Householder reﬂection.
Consider the following problem. A nonzero vector a and an unit vector e are
given. It is necessary to construct a Householder reﬂection R such that Ra = μe,
where μ is a number (clearly, the equality |μ| = |a| holds, since R is orthogonal).
It is easy to see (make a drawing!) that the solution of this problem is the House-
holder reﬂection deﬁned by the vector
w = a −|a|e
|a −|a|e|
(4.170)
or by the vector w = (a + |a|e)/|a + |a|e|. For minimization of effects of rounding
errors in numerical calculations, we should take the vector w that has the larger
denominator.
24Alston Scott Householder (1904–1993) was an American mathematician.
25With respect to the standard inner product on the space Rn.

162
4
Linear Operators
It is useful to note that if a is an arbitrary nonzero vector, then a Householder
transformation R can be constructed such that for every vector x ∈Rn, the following
condition holds:
(a, Rx) = |a|xk,
(4.171)
where k is a given integer lying in the range from 1 to n, and xk is the kth component
of the vector x. Evidently, to do this we need to choose e = ik in formula (4.170).

Chapter 5
Canonical Forms and Factorizations
In this chapter we explore in detail the problem of reducing the matrix of an oper-
ator to a simple form using special bases in ﬁnite-dimensional spaces. The singular
value decomposition of an operator is constructed. The Jordan canonical form of the
matrix of a ﬁnite-dimensional operator is obtained. A special section is devoted to
studying matrix pencils. We obtain their canonical forms and describe applications
to investigate the structure of solutions of systems of ordinary linear differential
equations.
5.1
The Singular Value Decomposition
5.1.1
Singular Values and Singular Vectors of an Operator
In this section we show that for every linear operator A mapping a ﬁnite-dimensional
unitary space Xn into a ﬁnite-dimensional unitary space Ym, there exist orthonormal
bases {ek}n
k=1 ⊂Xn and {qk}m
k=1 ⊂Ym such that
Aek =

σkqk, k ≤r,
0
, k > r,
(5.1)
where σk > 0, k = 1, 2, . . . ,r. The numbers σ1, σ2, . . . , σr are called the singular
values of the operator A. Sometimes it is convenient to include min(m, n) −r zeros
in the set of singular values.
Relationships (5.1) show that the numbers σ1, σ2, . . . , σr form the main diagonal
of the leading principal (basic) minor of the matrix Aeq of the operator A with respect
to the bases {ek}n
k=1, {qk}m
k=1, and all other elements of the matrix Aeq are equal to
zero.
© Springer International Publishing AG 2017
L. Beilina et al., Numerical Linear Algebra: Theory and Applications,
DOI 10.1007/978-3-319-57304-5_5
163

164
5
Canonical Forms and Factorizations
The vectors {ek}n
k=1, {qk}m
k=1 are called respectively the left and the right singular
vectors of the operator A. Let us construct them. The operator A∗A is self-adjoint and
positive semideﬁnite (see Property 2, p.140). Therefore (see Theorem 4.41, p.142,
and Section4.3.11, p.150), there exist the orthonormal eigenvectors {ek}n
k=1 of the
operator A∗A, and all its eigenvalues are nonnegative. Thus,
A∗Aek = σ 2
k ek,
k = 1, 2, . . . , n.
(5.2)
Here σ 2
k ≥0 are the eigenvalues of the operator A∗A. Let us enumerate them
as follows: σ1 ≥σ2 ≥· · · ≥σr > 0, σr+1 = · · · = σn = 0. Put zk = Aek for k =
1, . . . ,r and note that (z p, zq) = (Aep, Aeq) = (A∗Aep, eq) = σ 2
p(ep, eq). Hence
(z p, zq) =

0,
p ̸= q,
σ 2
p,
p = q,
(5.3)
and the vectors
qk = σ −1
k Aek,
k = 1, 2, . . . ,r,
(5.4)
formanorthonormalsetinthespaceYm.Ifr < m,thenwejointhissetwithsomevec-
tors qk, k = r + 1, r + 2, . . . , m, to complete an orthonormal basis of the space Ym.
Relationships (5.1) now follow immediately from the deﬁnition of the vectors {ek}n
k=1
and {qk}m
k=1.
Using (5.1), we see that the vectors {qk}r
k=1 form a basis of Im(A). Hence it follows
from Theorem 4.29, p.136, that the vectors {qk}m
k=r+1 form a basis of Ker(A∗).
Therefore,
A∗qk = 0 for k = r + 1,r + 2, . . . , m.
(5.5)
For k = 1, 2, . . . ,r, using (5.4), (5.2), we get
A∗qk = σ −1
k A∗Aek = σkek.
(5.6)
Combining (5.6), (5.4), and (5.5), we obtain
AA∗qk = σ 2
k qk, k = 1, 2, . . . ,r,
AA∗qk = 0, k = r + 1,r + 2, . . . , m. (5.7)
It follows from (5.2) and (5.7) that all the nonzero eigenvalues of the operators A∗A
and AA∗coincide, i.e., the spectra of these operators can differ only by the multi-
plicity of the zero eigenvalue.
Moreover, the next equalities follow from the previous arguments:
rank(A) = rank(A∗A) = rank(AA∗),
def(A∗A) = n −rank(A),
def(AA∗) = m −rank(A).

5.1 The Singular Value Decomposition
165
Clearly, the rank r of the operator A is equal to the number of all nonzero sin-
gular values of the operator A. This remark gives us a real opportunity to compute
the rank of the operator A: we have to solve the eigenvalue problem for the posi-
tive semideﬁnite self-adjoint operator A∗A and calculate the number of all nonzero
eigenvalues. Precisely this method is typically used in practical computations of the
rank. Evidently, the eigenvectors {ei}n
i=r+1 of the operator A∗A form an orthonormal
basis of the kernel of the operator A.
If the singular values and the singular vectors of the operator A are known, then
the pseudosolution (see Section4.3.4, p.136) of the equation
Ax = y
(5.8)
can be easily constructed. Indeed, in Section4.3.4 we proved that every solution of
the equation
A∗Ax = A∗y
(5.9)
is a pseudosolution of (5.8). Substituting in (5.9) the expansions x =
n

k=1
ξkek
and y =
m

k=1
ηkqk with respect to the bases {ek}n
k=1 ⊂Xn and {qk}m
k=1 ⊂Ym for x
and y and using after that (5.2), (5.5), (5.6), we get
r

k=1
(σ 2
k ξk −σkηk)ek = 0.
(5.10)
Therefore, ξk = ηk/σk for k = 1, 2, . . . ,r. Thus every vector
x =
r

k=1
ηk
σk
ek +
n

k=r+1
ξkek,
(5.11)
where ξr+1, …, ξn are arbitrary numbers, is a pseudosolution of Eq. (5.8).
If y ∈Im(A), i.e., Eq.(5.8) is solvable, then formula (5.11) gives the general
solution (see Section4.1.11, p.107) of Eq. (5.8). Indeed, in this case, the vector x0 =
r
k=1
(ηk/σk)ek is a particular solution of Eq.(5.8), and
n
k=r+1
ξkek is the general solution
of the corresponding homogeneous equation.
For each pseudosolution x of Eq.(5.8), we have
|x|2 =
r

k=1
|ηk|2
σ 2
k
+
n

k=r+1
|ξk|2.

166
5
Canonical Forms and Factorizations
If we take ξr+1, …, ξn = 0, then we get the pseudosolution of minimal length. This
pseudosolution is normal. Obviously, it is orthogonal to the kernel of the operator A.
The proof of the following four propositions is left to the reader.
Proposition 5.1 The absolute value of the determinant of every operator acting on
a ﬁnite-dimensional space is equal to the product of all its singular values.
Proposition 5.2 Let A ∈Mm,n be an arbitrary rectangular matrix of rank r. Then
there exist unitary matrices U and V (of order m and n, respectively) such that
A = UΣV,
(5.12)
where
Σ =
 S
O1,2
O2,1 O2,2

is a block 2 × 2 matrix, S = diag(σ1, σ2, . . . , σr), all elements of the diagonal S
are positive, and all elements of the matrices O1,2, O2,1, O2,2 are equal to zero.
Formula (5.12) determines the so-called singular value decomposition of a rectan-
gular matrix.
Proposition 5.3 Let A ∈Mm,n be an arbitrary matrix, and U, V arbitrary unitary
matrices of orders m and n, respectively. The singular values of the matrices A
andU AV coincide(therefore,wesaythatthesingularvaluesofamatrixareinvariant
under unitary transformations).
Proposition 5.4 Let A ∈Mm,n be an arbitrary matrix, σ1, σ2, . . . , σr its singular
values. Then
max
1≤k≤r σk ≤
⎛
⎝
m,n

i, j=1
|ai j|2
⎞
⎠
1/2
.
(5.13)
The singular values of an operator characterize the sensitivity of the solution of a
linear equation with respect to changes in its right-hand side. Let A be a nonsingular
operator acting on a ﬁnite-dimensional unitary space Xn. Consider two equations:
Ax = y
(5.14)
and
Ax = ˜y.
(5.15)
Since the operator A is nonsingular, these equations are uniquely solvable. Denote
by x the solution of Eq.(5.14) and by
˜x the solution of Eq.(5.15). The

5.1 The Singular Value Decomposition
167
number δx = |x −˜x|/|x| is the relative change in the solution with respect to the
change in the right-hand side. Let us clarify the dependence of δx on the relative
change in the right-hand side δy = |y −˜y|/|y|. If we represent the vectors y and
˜y in the form of expansions y =
n

k=1
ηkqk and ˜y =
n

k=1
˜ηkqk, then using (5.1), we
obtain
x = A−1y =
n

k=1
ηk
σk
ek,
˜x = A−1 ˜y =
n

k=1
˜ηk
σk
ek.
Here,asusual,σk,k = 1, 2, . . . , n,arethesingularvalues,and{ek}n
k=1 and{qk}m
k=1 are
the singular vectors of A. Therefore, using the inequalities σ1 ≥σ2 ≥· · · ≥σn > 0,
we get
δ2
x =
n

k=1
|ηk −˜ηk|2
σ 2
k
n

k=1
|ηk|2
σ 2
k
≤σ 2
1
σ 2n
n

k=1
|ηk −˜ηk|2
n
k=1
|ηk|2
= σ 2
1
σ 2n
δ2
y.
(5.16)
Thus,
δx ≤σ1
σn
δy.
(5.17)
The number σ1/σn, which characterizes the stability of the solution of Eq.(5.14)
with respect to changes in its right-hand side, is called the condition number of
the operator A and is denoted by cond (A). Evidently, cond (A) ≥1 for every
operator A.
The reader can easily check the following properties of the condition number.
1. There exist vectors y and ˜y such that the two sides in (5.17) are equal. It this
sense, estimate (5.17) cannot be improved.
2. There exist operators whose condition numbers are equal to one (give some
examples!).
5.1.2
The Polar Decomposition
Theorem 5.1 Let A : Xn →Ym be an arbitrary operator. There exist operators
U : Xn →Ym, S : Xn →Xn, and T : Ym →Ym such that
U∗U = I if n ≤m, UU∗= I if n ≥m.
(5.18)

168
5
Canonical Forms and Factorizations
The operators S, T are self-adjoint and positive semideﬁnite, and
A = US = T U.
(5.19)
Proof Let {ek}n
k=1, {qk}m
k=1 be the singular vectors of the operator A (which form
orthonormal bases in the spaces Xn, Ym; see Section5.1.1). Let σ1, σ2, …, σr be the
singular values of A. If n ≤m, we deﬁne the operator U by the relationships
Uek = qk,
k = 1, 2, . . . , n.
(5.20)
If n ≥m, we put
Uek = qk, k = 1, 2, . . . , m,
Uek = 0, k = m + 1, m + 2, . . . , n.
(5.21)
We deﬁne the operators S, T by the following equalities:
Sek = σkek, k = 1, 2, . . . ,r,
Sek = 0, k = r + 1,r + 2, . . . , n,
T qk = σkqk, k = 1, 2, . . . ,r,
T qk = 0, k = r + 1,r + 2, . . . , m.
The operators T and S are self-adjoint and positive semideﬁnite, since, as is easy to
see, the numbers (Sx, x), (T y, y) are nonnegative for all x ∈Xn and for all y ∈Ym.
Obviously,
USek = Aek,
T Uek = Aek, k = 1, 2, . . . , n,
i.e., the relationships (5.19) are true. By direct calculations we verify that the opera-
tor U∗can be deﬁned by the relationships
U∗qk = ek, k = 1, 2, . . . , m, if m ≤n,
(5.22)
U∗qk = ek, k = 1, 2, . . . , n, U∗qk = 0, k = n + 1, n + 2, . . . , m, if m ≥n.
(5.23)
Clearly, equalities (5.18) follow from (5.20)–(5.23).
□
Formulas (5.19) deﬁne the polar decomposition of the operator A.
Now let us dwell on the case in which an operator A acts on a space Xn. The
relationships (5.18) show that in this case, the operator U is unitary. It follows from
equalities (5.19) that every linear transformation of a ﬁnite-dimensional space Xn
is the result of the sequential execution of a unitary transformation, which does not
change the length of vectors, and a positive semideﬁnite self-adjoint transformation,
which stretches the space Xn in n mutually orthogonal directions.
It immediately follows from (5.19) that A∗A = S2, AA∗= T 2. Since the oper-
ators S and T are self-adjoint and positive semideﬁnite, the last two equalities show

5.1 The Singular Value Decomposition
169
that S and T are uniquely determined by the operator A, namely (see Theorem 4.43,
p.145),
S =
√
A∗A,
T =
√
AA∗.
(5.24)
If the operator A is nonsingular, then the operator A∗A is nonsingular. Hence the
operator S is also nonsingular. Therefore, in this case, the operator U = AS−1 is
also uniquely determined.
The next theorem readily follows from formulas (5.19), (5.24).
Theorem 5.2 An operator A is normal if and only if the operators T and S in the
factorization (5.19) coincide, in other words, if and only if the operators U and S
commute.
Note that if the space Xn is real, then there is a valid polar decomposition, but
the operator U in (5.19) is orthogonal, and the operators T and S are symmetric and
positive semideﬁnite.
5.1.3
Basic Properties of the Pseudoinverse Operator
Let Xn, Ym be ﬁnite-dimensional unitary spaces, A : Xn →Ym a linear opera-
tor, {ek}n
k=1 ⊂Xn, {qk}m
k=1 ⊂Ym its singular vectors, σ1, σ1, …, σr the singular
values of A, where r = rank(A), r ≤min(m, n).
As we have seen in Section5.1.1, the formula
x0 =
r

k=1
ηk
σk
ek,
where ηk = (qk, y), k = 1, 2, . . . , m, are the coordinates of the vector y with respect
to the basis {qk}m
k=1, deﬁnes the normal pseudosolution x0 of the equation Ax = y.
Thus the pseudoinverse of the operator A (see Section4.3.4, p. 109) can be repre-
sented in the form
A+y =
r

k=1
(qk, y)
σk
ek.
(5.25)
Here are the basic properties of the pseudoinverse operator:
1. (A∗)+ = (A+)∗,
2. (A+)+ = A,
3. (AA+)∗= AA+, (AA+)2 = AA+,
4. (A+A)∗= A+A, (A+A)2 = A+A,
5. AA+A = A,

170
5
Canonical Forms and Factorizations
6. A+AA+ = A+,
7. if rank A = n, then A+A = I.
We prove only the ﬁrst and third equalities. The reader can easily prove all the
other properties.
1. Let x =
n
k=1
ξkek, y =
m
k=1
ηkqk. Solving the equation AA∗y = Ax in the same
way as (5.9), we see that (A∗)+x =
r
k=1
(ξk/σk)qk. By elementary calculations, we
get ((A∗)+x, y) =
r
k=1
ξk ¯ηk/σk. Using (5.25), we obtain (x, A+y) =
r
k=1
ξk ¯ηk/σk.
This means that (A∗)+ = (A+)∗.
3. It follows from (5.25) that A+qk = σ −1
k ek for k = 1, 2, . . . ,r and A+qk = 0
for
k = r + 1,r + 2, . . . , m.
Hence,
AA+qk = qk
for
k = 1, 2, . . . ,r
and
AA+qk = 0 for k = r + 1,r + 2, . . . , m. Therefore, (AA+)2 =AA+. By elemen-
tary calculations, we get (AA+y, y) =
r
k=1
|ηk|2 ≥0 for each y ∈Ym. Thus we
obtain (AA+)∗= AA+ (see Lemma 4.5, p. 140).
It follows from Properties 3 and 4 that the operators AA+ and A+A are ortho-
gonal projection operators (see Theorem 4.35, p. 140).
5.1.4
Elements of the Theory of Majorization
A real-valued function f of a real variable is called convex on an interval (a, b) if
for all points x1, x2 in this interval and for every t ∈[0, 1], the following inequality
holds:
f (tx1 + (1 −t)x2) ≤t f (x1) + (1 −t) f (x2).
(5.26)
Geometrically, this means that every point on the graph of the function f on the
closed interval [x1, x2] lies below or on the chord subtending the points (x1, f (x1))
and (x2, f (x2)).
Theorem 5.3 (Jensen’s1 inequality). If a function f is convex on an interval (a, b),
then for all points x1, x2, …, xm that belong to (a, b) and for all nonnegative numbers
α1, α2, …, αm such that α1 + α2 + · · · + αm = 1, the following inequality is valid:
f
 m

i=1
αixi

≤
m

i=1
αi f (xi).
(5.27)
1Johan Ludvig William Valdemar Jensen (1859–1925) was a Danish mathematician and engineer.

5.1 The Singular Value Decomposition
171
Proof We easily get (5.27) by induction on m using the obvious identities
m

i=1
αixi = αmxm + (1 −αm)
m−1

i=1
αi
(1 −αm)xi,
m−1

i=1
αi
(1 −αm) = 1.
□
Theorem 5.4 Suppose that a function f is differentiable on an interval (a, b) and
the derivative of f is nondecreasing on (a, b). Then the function f is convex on the
interval (a, b).
Proof It is enough to prove that for all x1, x2 ∈(a, b), x1 < x2, the function
ϕ(t) = f ((1 −t)x1 + tx2) −(1 −t) f (x1) −t f (x2)
is nonpositive for all t ∈[0, 1]. It is easy to see that ϕ(0) = 0, ϕ(1) = 0, and ϕ′(t) is
nondecreasing on the segment [0, 1]. Using the Lagrange ﬁnite-increments formula,
we see that ϕ(t) = ϕ(t) −ϕ(0) = tϕ′(t1), where t1 is a point in the interval (0, t).
Similarly, ϕ(t) = (t −1)ϕ′(t2), where t2 is a point in the interval (t, 1). Hence it is
evident that ϕ(t) = t(t −1)(ϕ′(t2) −ϕ′(t1)) ≤0.
□
Below we will use the following deﬁnitions. A real matrix is called nonnegative
if all its elements are nonnegative. A nonnegative square matrix is called stochastic
if it is nonnegative and the sum of all the elements of each of its rows is equal to one.
A stochastic matrix is called doubly stochastic if the sum of all the elements of each
of its columns is also equal to one.
Theorem 5.5 Let x, y ∈Rn, x1 ≥x2 ≥· · · ≥xn, y1 ≥y2 ≥· · · ≥yn, and x ≺y.2
Then there exists a doubly stochastic matrix S such that x = Sy.
Proof We prove the theorem by induction on n. For n = 1, the theorem is trivial.
Now we assume that the assertion is true for all vectors of length n −1 and prove
that it holds for all vectors of length n. We easily check that if vectors x and y
satisfy all conditions of the theorem, then x1 ≥yn.3 Therefore, there exist an integer
k, 1 ≤k ≤n −1, and a real number τ ∈[0, 1] such that
x1 = τyk + (1 −τ)yk+1.
(5.28)
Let us consider the two following vectors of length n −1:
˜x = (x2, x3, . . . , xn) and ˜y = (y1, y2, . . . , yk−1, yk + yk+1 −x1, yk+2, . . . , yn).
2We use the notation deﬁned on p. 153.
3Otherwise, equality 4.157, p. 153, is impossible.

172
5
Canonical Forms and Factorizations
It is easy to see that the components of these vectors are monotonically nonincreasing
and ˜x ≺˜y. Therefore, by the induction hypothesis, there exists a doubly stochastic
matrix ˜S of order n −1 such that
˜x = ˜S ˜y.
(5.29)
Writing (5.28), (5.29) in the form of a single matrix equality, we get x = Sy, where S
is a doubly stochastic matrix.
□
Theorem 5.6 Let x, y ∈Rn, x1 ≥x2 ≥· · · ≥xn, y1 ≥y2 ≥· · · ≥yn, and x ≺w y.
Let f be a function that is nondecreasing and convex on the whole real axis. Then
n

i=1
f (xi) ≤
n

i=1
f (yi).
(5.30)
Proof By assumption, α =
n
i=1
yi −
n
i=1
xi ≥0. We take numbers xn+1, yn+1 such that
the following conditions hold: xn+1 ≤xn, yn+1 ≤yn, and xn+1 −yn+1 = α. Then for
the vectors (x1, x2, . . . , xn+1) and (y1, y2, . . . , yn+1), all conditions of Theorem 5.5
hold. Hence there exists a doubly stochastic matrix S = {si j}n+1
i, j=1 such that
xi =
n+1

j=1
si j y j,
i = 1, 2, . . . , n + 1,
whence, using Jensen’s inequality, we get
f (xi) ≤
n+1

j=1
si j f (y j),
i = 1, 2, . . . , n + 1.
(5.31)
Summing all inequalities (5.31), we see that
n+1

i=1
f (xi) ≤
n+1

j=1
n+1

i=1
si j f (y j) =
n+1

i=1
f (yi).
(5.32)
By construction, xn+1 ≥yn+1. By assumption, the function f is nondecreasing.
Therefore, f (xn+1) ≥f (yn+1). Thus (5.30) follows from (5.32).
□
Remark 5.1 Obviously, if all conditions of Theorem 5.6 hold and additionally x ≺
y, then the proof is simpler. In this case, we can omit the condition that f is a
nondecreasing function.
Corollary 5.1 Suppose that x, y ∈Rn, x1 ≥x2, ≥· · · ≥xn, y1 ≥y2, ≥· · · ≥yn,
and x ≺w y. Additionally, we assume that all components of the vectors x and y are
nonnegative. Then for every p > 1, the following inequality holds:

5.1 The Singular Value Decomposition
173
n

i=1
x p
i ≤
n

i=1
y p
i .
(5.33)
Proof It follows immediately from Theorem 5.4 that for every p > 1, the func-
tion f (t) = t p is convex on the positive semiaxis. Extending f (t) to the whole real
axis such that f (t) = 0 if t < 0, we get (5.33).
□
Corollary 5.2 Suppose that
x1 ≥x2 ≥· · · ≥xn ≥0,
y1 ≥y2 ≥· · · ≥yn ≥0,
and
k
i=1
xi ≤
k
i=1
yi,
k = 1, 2, . . . , n.
(5.34)
Then
x ≺w y.
(5.35)
Proof If x = 0, then the assertion is obvious. Let x p+1 = x p+2 = · · · = xn = 0,
where p ≥1, and let all other components of the vector x be positive. Then, using
conditions (5.34), we see that the ﬁrst q, p ≤q ≤n, components of the vector y are
also positive (and all other components are zero). In this case, conditions (5.34) have
the form
k
i=1
xi ≤
k
i=1
yi,
k = 1, 2, . . . , q.
(5.36)
It is easy to see that there exists a positive δ such that for all ε ∈(0, δ) and for
˜xi =

xi, i = 1, 2, . . . , p,
ε,
i = p + 1, p + 2, . . . , q,
as a consequence of (5.36), we have
k
i=1
˜xi ≤
k
i=1
yi,
k = 1, 2, . . . , q.
(5.37)

174
5
Canonical Forms and Factorizations
Taking the logarithm of all inequalities (5.37), we obtain
k

i=1
log ˜xi ≤
k

i=1
log yi,
k = 1, 2, . . . , q.
(5.38)
If we now put f (t) = et, t ∈R, then using Theorem 5.6, we get
k

i=1
˜xi ≤
k

i=1
yi,
k = 1, 2, . . . , q.
(5.39)
Taking the limit as δ →0 in these inequalities, we obtain (5.35).
□
5.1.5
Some Estimates of Eigenvalues and Singular Values
It readily follows from the deﬁnition that the singular values of a matrix A are
calculated by the formulas σk = √λk(A∗A), where λk(A∗A), k = 1, 2, . . . ,r, are
the nonzero eigenvalues of the matrix A∗A. The next lemma gives another (some-
times more useful) representation of the singular values using the eigenvalues of a
Hermitian matrix.
Lemma 5.1 Let A ∈Mm,n be an arbitrary matrix, σ1, σ2, …, σr its singular values,
and {ek}n
k=1 ⊂Cn, {qk}m
k=1 ⊂Cm its singular vectors (see Section5.1.1, p. 163). Let
˜A be the Hermitian matrix of order m + n of the form
˜A =
 0 A
A∗0

.
Then the vectors uk = (qk, ek) ∈Cm+n, k = 1, 2, . . . ,r; ur+k = (qk, −ek), k =
1, 2, . . . ,r; u2r+k = (qk, 0), k = r + 1,r + 2, . . . , m; ur+m+k = (0, ek), k = r +
1,r + 2, . . . , n, form a complete orthogonal set of eigenvectors of ˜A. The corre-
sponding eigenvalues are ±σ1, ±σ2, …, ±σr, and m + n −2r zeros.
The reader can prove this lemma by multiplying the vectors uk by the matrix ˜A
for k = 1, 2, . . . , m + n.
Using Lemma 5.1, we can easily prove, for example, a theorem that is analogous
to Theorem 4.54, p. 154. Let us introduce the necessary notation. Let A ∈Mm,n be an
arbitrary matrix, and σ1, σ2, …, σr its singular values. We denote by σ(A) the vector
of length min(m, n) that consists of the singular values of the matrix A completed
by zeros if r < min(m, n) and ordered by nonincreasing values of all its elements.

5.1 The Singular Value Decomposition
175
Theorem 5.7 Let A and B be arbitrary m × n matrices. Then
σ(A + B) ≺w (σ(A) + σ(B)).
The next theorem is useful for the estimation of the singular values of the product
of matrices.
Theorem 5.8 Let A and B be arbitrary square matrices of order n. Then
σi(AB) ≤σ1(A)σi(B),
i = 1, 2, . . . , n.
(5.40)
Proof Let M = σ 2
1 (A)I −A∗A. Obviously, the matrix M is Hermitian and posi-
tivesemideﬁnite,andσ 2
1 (A)B∗B = B∗(A∗A + M)B = (AB)∗(AB) + B∗M B.The
matrix B∗M B is also Hermitian and positive semideﬁnite. Therefore, inequali-
ties (5.40) are valid.
□
The next corollary is obvious but useful.
Corollary 5.3 For all k = 1, 2, . . . , n and p > 1, the following inequalities hold:
 k

i=1
σ p
i (AB)
1/p
≤
 k

i=1
σ p
i (A)
1/p  k

i=1
σ p
i (B)
1/p
.
(5.41)
Theorem 5.9 Let A ∈Mm,n be given, and let Ar denote a submatrix of A obtained
by deleting a total of r columns and (or) rows from A. Then
σk+r(A) ≤σk(Ar) ≤σk(A),
k = 1, 2, . . . , min(m, n),
(5.42)
where for X ∈Mp,q, we set σ j(X) = 0 if j > min(p, q).
Proof It sufﬁces to note that if columns from the matrix A are deleted (replaced by
zeros), then the nonzero rows and columns of the matrix A∗
r Ar form a submatrix
corresponding to the principal minor of the matrix A∗A of the corresponding order.4
After that, to conclude the proof, we use Theorem 4.50, p. 152.
□
Lemma 5.2 Let A ∈Mm,n, Vk ∈Mm,k, Wk ∈Mn,k, k ≤min(m, n). We assume that
the columns of the matrices Vk, Wk are orthonormal with respect to the standard inner
products on the spaces Cm, Cn, respectively. Then
σi(V ∗
k A Wk) ≤σi(A),
i = 1, 2, . . . , k.
(5.43)
Proof Let matrices V = (Vk, Vm−k) ∈Mm, W = (Wk, Wn−k) ∈Mn be unitary. It is
easy to see that the matrix V ∗
k A Wk is the leading principal submatrix of order k of the
matrix V ∗A W. Therefore, using Theorem 5.9 and the fact that the singular values
4Similarly, if rows from the matrix A are deleted, then we get a submatrix of AA∗of the same order.

176
5
Canonical Forms and Factorizations
of every matrix are invariant under its unitary transformations, we get the following
estimates:
σi(V ∗
k A Wk) ≤σi(V ∗A W) = σi(A),
i = 1, 2, . . . , k.
□
Theorem 5.10 (Weyl5). Let A ∈Mn have singular values σ1(A) ≥. . . ≥σn(A) ≥
0 and eigenvalues λ1(A), . . . , λn(A) ordered so that |λ1(A)| ≥. . . ≥|λn(A)|. Then
|λ1(A)λ2(A) · · · λk(A)| ≤σ1(A)σ2(A) · · · σk(A),
k = 1, 2, . . . , n,
(5.44)
with equality for k = n.
Proof BySchur’s theorem, Theorem4.23, p. 127, thereexists aunitarymatrixU such
that U ∗AU = T , where T is upper triangular and the numbers λ1(A), λ2(A), . . . ,
λn(A) form the main diagonal of T . Let Uk ∈Mn,k be the matrix that consists of the
ﬁrst k columns of U. By elementary calculations, we get
U ∗AU = (Uk,Un−k)∗A(Uk,Un−k) =
U ∗
k AUk T12
T21
T22

= T,
where U ∗
k AUk is an upper triangular matrix, and λ1(A), λ2(A), . . . , λk(A) form its
main diagonal. Evidently,
|λ1(A)λ2(A) · · · λk(A)| = | det(U∗
k AUk)|
= σ1(U∗
k AUk)σ2(U∗
k AUk) · · · σk(U∗
k AUk).
Hence (5.44) follows from Lemma 5.2. Equality in (5.44) for k = n holds, since,
as we know, for every square matrix A of order n the following equalities hold:
det(A) = λ1(A)λ2(A) · · · λn(A), | det(A)| = σ1(A)σ2(A) · · · σn(A).
□
Now it follows from Corollary 5.2, p. 172, that for every matrix A ∈Mn we have
k

i=1
|λi(A)| ≤
k

i=1
σi(A),
k = 1, 2 . . . , n.
(5.45)
Theorem 5.11 Let A ∈Mm,p, B ∈Mp,n be arbitrary matrices, q = min(m, n, p).
Then
k
i=1
σi(AB) ≤
k
i=1
σi(A)σi(B),
k = 1, 2, . . . , q.
(5.46)
If m = n = p, then equality holds in (5.46) for k = n.
5Hermann Klaus Hugo Weyl (1885–1955) was a German mathematician.

5.1 The Singular Value Decomposition
177
Proof Let AB = U DV be the singular value decomposition of the product AB.
Then D = U ∗ABV ∗. Denote by Uk, V ∗
k the matrices consisting of the ﬁrst k columns
of the matrices U, V ∗, respectively. Then U ∗
k ABV ∗
k = diag(σ1(AB), σ2(AB), . . . ,
σk(AB)), because it is the leading principal submatrix of order k of the matrix D. By
assumption, p ≥k. Therefore, by Theorem 5.1, there exists a polar decomposition
BV ∗
k = Xk Qk, where X∗
k Xk = Ik, and Qk ∈Mk is a non-negative semideﬁnite Her-
mitian matrix, and Ik is the identity matrix of order k. By elementary calculations,
we get the equality Q2
k = (BV ∗
k )∗BV ∗
k . Therefore, by Lemma 5.2, we obtain
det(Q2
k) = det(Vk B∗BV ∗
k ) ≤σ1(B∗B)σ2(B∗B) · · · σk(B∗B) = σ 2
1 (B)σ 2
2 (B) · · · σ 2
k (B).
Using Lemma 5.2 one more time, we see that
σ1(AB)σ2(AB) · · · σk(AB) = | det(U ∗
k ABV ∗
k )|
= | det(U ∗
k AXk Qk)| = | det(U ∗
k AXk) det(Qk)|
≤σ1(A)σ2(A) · · · σk(A)σ1(B)σ2(B) · · · σk(B).
Finally, if m = n = p, then
σ1(AB)σ2(AB) · · · σn(AB) = | det(AB)|
= | det(A)|| det(B)| = σ1(A)σ2(A) · · · σn(A)σ1(B)σ2(B) · · · σn(B).
□
Using Corollary 5.2, p. 172, we see that under the assumptions of Theorem 5.11,
the following inequalities hold:
k

i=1
σi(AB) ≤
k

i=1
σi(A)σi(B),
k = 1, 2, . . . , n.
(5.47)
Sums of singular values of a matrix have useful variational characterizations. Let
us introduce the following concept, which we will use below in the formulations of
corresponding results. A matrix C ∈Mm,n is called a partial isometry of rank k if it
has rank k and all its (nonzero) singular values are equal to one. If m = n = k, we
get the set of all unitary matrices of order n.
Theorem 5.12 Let A ∈Mm,n, q = min(m, n). Then for each k = 1, 2, . . . , q, the
following equalities hold:
k

i=1
σi(A) = max
X,Y |trX∗AY|,
(5.48)

178
5
Canonical Forms and Factorizations
k

i=1
σi(A) = max
C
|trAC|.
(5.49)
In the ﬁrst case, the maximum is taken over all matrices
X ∈Mm,k, Y ∈Mn,k such that X∗X = I, Y ∗Y = I.
(5.50)
In the second case, the maximum is taken over all matrices C ∈Mn,m that are partial
isometries of rank k.
Proof First we show that formulations (5.48) and (5.49) are equivalent. Using for-
mula (4.81), p. 125, we get trX∗AY = trAYX∗= trAC, where C = Y X∗∈Mn,m.
Therefore, C∗C = X X∗. As we have seen on p. 164, all the nonzero eigenvalues
of the self-adjoint matrices X X∗and X∗X coincide. Hence all their singular val-
ues coincide too, but X∗X is the identity matrix of order k. Thus the matrix C has
exactly k singular values, and all of them are equal to one, i.e., C is a partial isometry
of rank k. Conversely, if C ∈Mn,m is a partial isometry of rank k, then by deﬁnition,
the following singular value decomposition of C holds:
C = (Yk, Yn−k)
Ik 0
0 0
 
X∗
k
X∗
m−k

,
(5.51)
where Ik is the identity matrix of order k, and the matrices
Y = (Yk, Yn−k) ∈Mn,
X∗=

X∗
k
X∗
m−k

∈Mm
are unitary. Using equality (5.51), by elementary calculations, we see that C = Yk X∗
k,
and the matrices Yk, X∗
k satisfy conditions (5.50). Thus the equivalence of formula-
tions (5.48) and (5.49) is established.
Now successively using (5.45) and (5.47), we can write that if C is an arbitrary
partial isometry of rank k, then
|tr(AC)| ≤
m

i=1
|λi(AC)| ≤
m

i=1
σi(AC) ≤
q

i=1
σi(A)σi(C) =
k

i=1
σi(A).
(5.52)
To conclude the proof, it is enough to ﬁnd a partial isometry C of rank k such that
inequality (5.52) transforms to an equality. Let A = U DV be the singular value
decomposition of the matrix A. Put C = V ∗PU ∗, where
P =
Ik 0
0 0

∈Mn,m.

5.1 The Singular Value Decomposition
179
By construction, C ∈Mn,m, and it is a partial isometry of rank k. Moreover,
AC = U DV V ∗PU ∗= U DPU ∗,
and therefore, tr(AC) = tr(DP), and using elementary calculations, we obtain the
equality tr(DP) =
k
i=1
σi(A).
□
5.2
The Jordan Canonical Form
In this section we show that for every linear operator acting on a complex ﬁnite-
dimensional space Xn, there exists a basis such that the matrix of the operator with
respect to this basis has a very simple form. It is bidiagonal. The elements of the main
diagonal of this matrix form the set of all eigenvalues of the operator. Each element
of the diagonal above the main diagonal is either one or zero. A matrix of this form
is called a Jordan matrix.6 To obtain the Jordan canonical form of the operator it
is necessary to take its matrix in an arbitrarily chosen basis and after that to reduce
this matrix to the Jordan canonical form by a similarity transformation. This plan is
realized in this section.
The following question naturally arises: is it possible to reduce every matrix
to diagonal form by a similarity transformation? Simple examples show that it is
impossible. For instance, if we require that the matrix SAS−1 be diagonal, where
A =
0 1
0 0

and S is a nonsingular matrix, then we get contradictory equalities.
5.2.1
Existence and Uniqueness of the Jordan
Canonical Form
We begin with a deﬁnition. A Jordan block Jk(λ) is a k × k upper triangular matrix
of the form
6Marie Ennemond Camille Jordan (1838–1922) was a French mathematician.

180
5
Canonical Forms and Factorizations
Jk(λ) =
⎛
⎜⎜⎜⎜⎜⎝
λ
1
0
λ
1
... ...
λ
1
0
λ
⎞
⎟⎟⎟⎟⎟⎠
.
(5.53)
We explain that all k elements of the main diagonal of the matrix Jk(λ) are equal
to λ, all k −1 elements of the diagonal above the main diagonal are equal to one,
and all other entries of this matrix are zero.
It is useful to note that if the matrix of the operator A: Xk →Xk with respect to
a basis {ei}k
i=1 is the Jordan block Jk(0), then, evidently, the vectors of this basis are
connected to each other by the following relationships:
Ae1 = 0,
Ae2 = e1, . . . , Aek = ek−1.
If we denote the vector ek by f , then we see that the basis {ei}k
i=1 consists of the
vectors f , A f , A2 f , …, Ak−1 f ,7 and moreover, Ak f = 0.
Let us formulate now the main result of this section.
Theorem 5.13 Let A be a given complex matrix of order n. There is a nonsingular
matrix S such that
S−1AS = J,
(5.54)
where
J =
⎛
⎜⎜⎜⎝
Jn1(λ1)
0
Jn2(λ2)
...
0
Jnk(λk)
⎞
⎟⎟⎟⎠
(5.55)
and n1 + n2 + · · · + nk = n. The numbers λi, i = 1, 2, . . . , k (which are not neces-
sarily distinct), form the set of all characteristic values of the matrix A (according
to their multiplicities).
The matrix (5.55) is called the Jordan canonical form of the matrix A. Obviously,
Theorem 5.13 is equivalent to the following statement. For every operator A acting
on a ﬁnite-dimensional complex space, there exists a basis En such that the matrix
of the operator A with respect to this basis has the form (5.55), i.e.,
AEn = En J.
(5.56)
The basis En is called a Jordan basis.
7Which are listed in reverse order.

5.2 The Jordan Canonical Form
181
The easiest proof of the existence of a Jordan basis is for a nilpotent operator.
Using Theorem 4.21, p.126, and Schur’s theorem, p.127, we see that an operator is
nilpotent if and only if there exists a basis such that the matrix of the operator with
respect to this basis is upper triangular and all elements of the main diagonal of this
matrix are zero.
Theorem 5.14 Let A : Xn →Xn be a nilpotent operator acting on a complex vector
space Xn. Then there exists a basis of the space Xn such that the matrix of the operator
A with respect to this basis has the following Jordan canonical form:
⎛
⎜⎜⎜⎝
Jn1(0)
0
Jn2(0)
...
0
Jnm(0)
⎞
⎟⎟⎟⎠.
(5.57)
Here n1 + n2 + · · · + nm = n.
Proof Taking into account the remark in the last paragraph before Theorem 5.13, it
is easy to see that the assertion is equivalent to the following one: for every nilpotent
operator A : Xn →Xn, there exist vectors f1, f2, … fm such that the vectors
f1, A f1, A2 f1, . . . , An1−1 f1, f2, A f2, A2 f2, . . . , An2−1 f2, . . . ,
fm, A fm, A2 fm, . . . , Anm−1 fm
(5.58)
form a basis in the space Xn, and
An1 f1 = An2 f2 = · · · = Anm fm = 0.
(5.59)
We prove the existence of the required basis by induction on the dimension of
the space. In the case of a nilpotent operator acting on a one-dimensional space, the
assertion is obviously true. Now we suppose that the assertion is true for every space
whose dimension is less than n, and prove that then the statement is true for every
n-dimensional space.
The operator A is nilpotent. Therefore, def(A) ≥1, and hence rank(A) < n (see
equality (4.23), p.102). Evidently, the subspace Im(A) is invariant under A, whence
by the induction hypothesis, we conclude that there exist vectors u1, u2, …, uk such
that the vectors
u1, Au1, A2u1, . . . , Ap1−1u1, u2, Au2, A2u2, . . . , Ap2−1u2, . . . ,
uk, Auk, A2uk, . . . , Apk−1uk
(5.60)

182
5
Canonical Forms and Factorizations
form a basis of the subspace Im(A), and
Ap1u1 = Ap2u2 = · · · = Apkuk = 0.
(5.61)
Fori = 1, 2, . . . , k, the vectors ui belong to Im(A). Hence there exist vectors vi ∈Xn
such that
ui = Avi.
(5.62)
The vectors
Api−1ui, i = 1, 2, . . . , k,
(5.63)
belong to the basis (5.60). Hence they are linearly independent. Relationships (5.61)
show that these vectors belong to Ker(A). Thus we can join vectors (5.63) with some
vectors w1, w2, …, wl to complete the basis of the subspace Ker(A).
If we prove now that the vectors
v1, Av1, . . . , Ap1v1, v2, Av2, . . . , Ap2v2, . . . , vk, Avk, . . . , Apkvk,
w1, w2, . . . , wl
(5.64)
form a basis of the space Xn, then obviously, this basis is the required Jordan basis
of the operator A. The set (5.64) consists of n vectors. Indeed, this set consists of
p1 + · · · + pk + k + l elements, and moreover, p1 + · · · + pk = rank(A), k + l =
def(A), but rank(A) + def(A) = n for every operator A. Further, put
α1,0v1 + α1,1Av1 + · · · + α1,p1Ap1v1 + α2,0v2 + α2,1Av2 + · · · + α2,p2Ap2v2
+ · · · + αk,0vk + αk,1Avk + · · · + αk,pkApkvk
+ β1w1 + β2w2 + · · · + βlwl = 0.
(5.65)
Acting on both sides of the last equality by the operator A, using relationships (5.61),
(5.62), and also using the fact that w1, w2, …, wl ∈Ker(A), we get
α1,0u1 + α1,1Au1 + · · · + α1,p1−1Ap1−1u1
+ α2,0u2 + α2,1Au2 + · · · + α2,p2−1Ap2−1u2
+ · · · + αk,0uk + αk,1Auk + · · · + αk,pk−1Apk−1uk = 0. (5.66)
The vectors (5.60) are linearly independent. Therefore, all the coefﬁcients in the
linear combination on the left-hand side of (5.66) are zero, and (5.65) has the form
α1,p1Ap1v1 + α2,p2Ap2v2 + · · · + αk,pkApkvk
+ β1w1 + β2w2 + · · · + βlwl = 0.
(5.67)

5.2 The Jordan Canonical Form
183
The left-hand side of (5.67) is a linear combination of the vectors of the basis of
the subspace Ker(A). Therefore, all the coefﬁcients in this linear combination are
equal to zero. Thus we have proved that all the coefﬁcients in the linear combination
on the left-hand side of equality (5.65) must all be zero, i.e., the set of vectors
(5.64) is linearly independent and consists of n vectors, and hence it is a basis of the
space Xn.
□
The next theorem is an immediate generalization of Theorem 5.14.
Theorem 5.15 Suppose that an operator A acting on a complex space Xn has the
form A = A0 + λI, where A0 is a nilpotent operator, and λ is an arbitrary number.
Then the matrix of the operator A with respect to the Jordan basis of the operator A0
has the following Jordan canonical form:
⎛
⎜⎜⎜⎝
Jn1(λ)
0
Jn2(λ)
...
0
Jnm(λ)
⎞
⎟⎟⎟⎠.
(5.68)
This statement follows immediately from the facts that linear operations with their
matrices correspond to linear operations with operators and that the matrix of the
identity operator with respect to every basis is the identity matrix.
Proof of Theorem 5.13. Representation (5.54) is the result of the sequential real-
ization of the following steps.
1. Using Schur’s theorem, p.127, we construct an upper triangular matrix T that
is unitarily similar to the matrix A.
2. Using Theorem 4.24, p.129, we reduce the matrix T to block diagonal form.
Each block here is an upper triangular matrix. All diagonal elements of this matrix
are equal and coincide with a characteristic value of the matrix A.
3.UsingTheorems5.14and5.15,weindependentlyreduceeachblockconstructed
in the second step to the form (5.68).
□
The next lemma is useful for the investigation of the uniqueness of the Jordan
canonical form.
Lemma 5.3 The following relationships hold for a Jordan block Jk(0):
(Jk(0))k = 0,
(5.69)
(Jk(0)) j ̸= 0,
j = 1, 2, . . . , k −1.
(5.70)
Proof The relationship (5.69) follows immediately from Theorem 4.21, p.126, and
Corollary 4.3, p.126. The relationships (5.70) are easily veriﬁed by direct calculation.
Itisimportanttonotethatwhiletheordersofthematrices Jk(0)increasessequentially
with k, the nonzero columns of Jk(0) are displaced to the right.
□

184
5
Canonical Forms and Factorizations
Theorem 5.16 The Jordan matrix (5.55) is uniquely determined by the matrix A
(up to permutation of the diagonal Jordan blocks).
Proof Two possible Jordan canonical forms of the matrix A are similar to the
matrix A. Therefore, they have the same set of characteristic values (according to
their multiplicities). Hence to complete the proof, it is enough to show the coinci-
dence of the orders of the Jordan blocks that correspond to a given characteristic
value of the matrix A.
This problem can be formulated as follows: prove the coincidence of the orders
of the Jordan blocks of two possible Jordan canonical forms of a matrix that has a
unique characteristic value. Moreover, arguing as in the proof of Theorem 5.15, it is
easy to see that it is sufﬁcient to consider a matrix A0 with a unique zero characteristic
value.
Thus, let
J(0) =
⎛
⎜⎜⎜⎝
Jn1(0)
0
...
0
Jnk(0)
⎞
⎟⎟⎟⎠,
˜J(0) =
⎛
⎜⎜⎜⎝
Jm1(0)
0
...
0
Jmr (0)
⎞
⎟⎟⎟⎠
be two possible Jordan canonical forms of the matrix A0. We assume that the Jordan
blocks are sorted according to nondecreasing values of their orders (this may be
achieved by the corresponding numeration of the Jordan bases) such that
n1 ≥n2 ≥· · · ≥nk,
n1 + n2 + · · · + nk = n,
m1 ≥m2 ≥· · · ≥mr,
m1 + m2 + · · · + mr = n,
where n is the order of the matrix A0. Suppose that the ﬁrstl −1,l ≥1, Jordan blocks
of the matrices J(0) and ˜J(0) coincide. By assumption, there exists a nonsingular
matrix S such that
J(0) = S ˜J(0)S−1.
(5.71)
As a result of the assumption of the coincidence of the ﬁrst Jordan blocks, the matrix S
has the following form:
S =
Ip
0
0 Sn−p

,
where Ip is the identity matrix of order p = n1 + · · · + nl−1. This gives us the oppor-
tunity to consider only the matrices J(0) and ˜J(0) such that their ﬁrst blocks do not
coincide, i.e., Jn1(0) ̸= Jm1(0). If we prove that this is impossible, then we will have
concluded the proof of the theorem. To be deﬁnite, assume that n1 > m1. Raising
both sides of equality (5.71) to the power m1, we get

5.2 The Jordan Canonical Form
185
(J(0))m1 = S( ˜J(0))m1 S−1.
(5.72)
Using Lemma 5.3, we see that ( ˜J(0))m1 = 0 and also that (J(0))m1 ̸= 0. This con-
tradiction concludes the proof of the theorem.
□
5.2.2
Root and Cyclic Subspaces
A Jordan matrix is block diagonal. Hence a space Xn can be represented as a direct
sumofinvariantsubspacesoftheoperatorAcorrespondingtotheblocksoftheJordan
matrix (see Section4.2.1, p.114). The subspace corresponding to the block Jn j(λ j)
in representation (5.54) is called a cyclic subspace. The direct sum of all cyclic
subspaces corresponding to the same eigenvalue λ of the operator A is called the
root subspace.
Let us investigate the structure of cyclic and root subspaces. Suppose that an m-
dimensional cyclic subspace corresponds to the eigenvalue λ of the operator A. For
the sake of being deﬁnite, assume that the vectors {ek}m
k=1 of the basis En belong to
this subspace. Using (5.56), we see that
Ae1 = λe1,
Ae2 = λe2 + e1, . . . , Aem = λem + em−1.
(5.73)
This immediately implies that e1 is an eigenvector of the operator A. Clearly, the
vectors e1, e2, …, em−1 are nonzero, and therefore no other vectors e2, e3, …, em are
eigenvectors of the operator A.
Every cyclic subspace includes exactly one eigenvector of the operator A. Indeed,
if we assume that x = ξ1e1 + ξ2e2 + · · · + ξmem is an eigenvector of the opera-
tor A, then Jm(λ)ξ = λξ, where ξ = (ξ1, ξ2, . . . , ξm)T . The last equality is equiva-
lent to Jm(0)ξ = 0. The rank of the matrix Jm(0) is m −1. Indeed, det Jm(0) = 0,
and the minor obtained from the determinant det Jm(0) by deleting the ﬁrst column
and the last row is equal to one. Thus the dimension of the kernel of the matrix Jm(0)
is one.
Clearly, if the root subspace corresponding to an eigenvalue λ of the operator A
is the direct sum of k cyclic subspaces, then it contains exactly k linearly indepen-
dent eigenvectors of the operator A corresponding to the eigenvalue λ. Therefore,
the number of cyclic subspaces of a given root subspace is equal to the geometric
multiplicity of the corresponding eigenvalue λ.
The sum of dimensions of all cyclic subspaces corresponding to the eigenvalue λ
is equal the multiplicity of λ as the root of the characteristic equation, i.e., it is equal
to the algebraic multiplicity of the eigenvalue λ.
It follows immediately from (5.73) that
(A −λI) je j = 0,
j = 1, 2, . . . , m.
(5.74)

186
5
Canonical Forms and Factorizations
Also it is easy to see that (A −λI)pe j ̸= 0 for p < j. For this reason, the integer j
is called the height of the cyclic vector e j. Particularly, an eigenvector is a cyclic
vector of height one.
It is easy to guess that if l is the dimension of the root subspace corresponding
to the eigenvalue λ of the operator A, then for every vector x of this subspace, the
following equality holds:
(A −λI)lx = 0.
(5.75)
Remark 5.2 Obviously, a Jordan basis is not uniquely determined by the operator A.
Moreover, if we have a Jordan basis, then using it, we can easily construct another
Jordan basis. For example, if in the basis En we replace the vector e2 by the vector ˜e2 =
e2 + αe1, where α is an arbitrary number, then for this new basis, the equalities (5.73)
hold, i.e., it is also a Jordan basis of the operator A. However, since the Jordan matrix
is uniquely determined by the operator A (up to permutation of the diagonal Jordan
blocks), all Jordan bases have the structure described above.
5.2.3
The Real Jordan Canonical Form
Theorem 5.17 Let A be a real square matrix of order n ≥1. There exists a nonsin-
gular real matrix S such that S−1 AS = J, where
J = diag(Jm1(λ1), Jm2(λ2), . . . , Jml(λl))
and λ1, λ2,…, λl are the characteristic values of the matrix A. If λk is real, then the
Jordan block Jmk(λk) that corresponds to λk is of exactly the same form as (5.55) in
Theorem 5.13. The diagonal block of the form
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
α
β
1
0
0
0 . . . 0
0
−β α
0
1
0
0 . . . 0
0
0
0
α
β
1
0 . . . 0
0
0
0 −β α
0
1 . . . 0
0
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . 0
1
0
0
0
0
0
0 . . . α
β
0
0
0
0
0
0 . . . −β α
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
(5.76)
in the matrix J corresponds to each pair of the complex conjugate characteristic
values λ = α + iβ, ¯λ = α −iβ of the matrix A. The dimension of block (5.76) is
twice the dimension of the corresponding Jordan block of the eigenvalue λ in the
matrix A in representation (5.54), p. 180.

5.2 The Jordan Canonical Form
187
Proof Let {ek}n
k=1 be the Jordan basis of the matrix A constructed in the proof
of Theorem 5.13. Suppose that the vectors {ek}m
k=1, m ≤n, correspond to the real
characteristic values of the matrix A. Taking into account the relationships of the
form (5.73), it is easy to see that these vectors can be real. Suppose that the vec-
tors {ek}m+p
k=m+1 correspond to the Jordan block Jp(λ) of the complex characteristic
value λ = α + iβ of the matrix A. Then we can assume that the vectors {¯ek}m+p
k=m+1
of the same Jordan basis correspond to the block Jp(¯λ). Put xk = Re ek, yk = Im ek,
where k = m + 1, m + 2, . . . , m + p. It is easy to see that the vectors
e1, e2, . . . , em, xm+1, ym+1, . . . , xm+p, ym+p, em+2p+1, . . . , en
(5.77)
are linearly independent by writing the change of basis matrix from the Jordan
basis {ek}n
k=1 to (5.77). Now let us move from the Jordan basis to the basis (5.77).
Note that by the deﬁnition of the Jordan basis,
Aem+1 = λem+1, Aem+2 = λem+2 + em+1, . . . , Aem+p = λem+p + em+p−1.
Equating the real and imaginary parts of these equalities,8 we see that the block of
the form (5.76) corresponds to the vectors xm+1, ym+1, . . . , xm+p, ym+p in the basis
(5.77). To conclude the proof, we apply this process to all other pairs of the complex
conjugate characteristic values of the matrix A.
□
Let us give some examples of applications of the Jordan canonical form.
Theorem 5.18 A square matrix A is similar to its transpose AT .
Proof If we represent A in Jordan canonical form and write A = SJ S−1, then AT =
(S−1)T J T ST . If there exists a nonsingular matrix P such that
J T = P−1J P,
(5.78)
then the assertion is true. Indeed, in this case the matrix AT is similar to the matrix J,
and J is similar to A. Obviously, it is enough to check the equality of the form (5.78)
only for an arbitrary Jordan block. Moreover, since every Jordan block is equal
to λI + J(0), it is sufﬁcient to specify a matrix P such that (J(0))T = P−1J(0)P.
By elementary calculations, we see that the permutation matrix
P =
⎛
⎜⎜⎝
0
1
...
1
1
0
⎞
⎟⎟⎠
(5.79)
satisﬁes the required condition.
□
8Analogous calculations were done in Section4.2.5, p.125.

188
5
Canonical Forms and Factorizations
Theorem 5.19 Every real square matrix A can be represented as the product of two
real symmetric matrices, one of which can be nonsingular.
Proof By Theorem 5.17, the matrix A is represented in the real Jordan canonical
form: A = SJ S−1. Using elementary calculations, we see that the matrix J P, where
the matrix P is deﬁned by equality (5.79), is symmetric (it is convenient to perform
calculations for each Jordan block of the matrix J separately). Now we write the evi-
dent equalities A = SJ P PS−1 = SJ PST (S−1)T PS−1. Clearly, the matrix SJ PST
is symmetric, and the matrix (S−1)T PS−1 is symmetric and nonsingular.
□
5.2.4
Power Series of Matrices
Let us recall that by Mn we denote the set of all square matrices of order n with
complex, generally speaking, elements. An inﬁnite series of the form
a0I + a1 A + a2 A2 + · · · + ak Ak + · · · ,
(5.80)
where a0, a1, …, are complex numbers, A ∈Mn, n ≥1, is called a power series of
matrices. We say that series (5.80) converges if there is a matrix B such that
lim
m→∞
m

k=0
ak Ak = B.
We connect power series of matrices (5.80) with the following power series:
∞

k=0
akλk,
λ ∈C.
(5.81)
Let us recall some results from calculus. The series (5.81) is connected with the
series of nonnegative terms:
∞

k=0
|ak|tk, t ≥0. The set of all numbers t ≥0 such
that
∞

k=0
|ak|tk < ∞forms an interval on the positive semiaxis. This interval includes
the point t = 0, can be open or closed on the right, ﬁnite or inﬁnite. The length of this
interval (denote it by r) is called the radius of convergence of the power series (5.81).
The series (5.81) converges absolutely for all |λ| < r. For |λ| > r, the series (5.81)
diverges.
Now we clarify the conditions of convergence of the series (5.80).
Let λ1, λ2, …, λn be all the eigenvalues of the matrix A. The spectral radius of A
is the nonnegative number
ρ(A) = max
1≤j≤n |λ j|.
(5.82)

5.2 The Jordan Canonical Form
189
This is just the radius of the smallest closed disk centered at the origin in the complex
plane that includes all eigenvalues of A.
Let λ be an eigenvalue of the matrix A. Denote by nλ the maximal order of the
Jordan blocks corresponding to λ.
Theorem 5.20 (1) If ρ(A) < r, then the series (5.80) converges. (2) If ρ(A) > r,
then the series (5.80) diverges. (3) If ρ(A) = r, then the series (5.80) converges if
and only if for each characteristic value λ of the matrix A such that |λ| = ρ(A), the
following series (which are obtained by differentiation of the series (5.81)) converge:
∞

k=1
akk(k −1) · · · (k −j + 1)λk−j, j = 0, 1, . . . , nλ −1.
(5.83)
Proof Suppose that the matrix S reduces the matrix A to Jordan canonical form, i.e.,
A = SJ S−1, where the matrix J is deﬁned by equality (5.55), p.180. Then for each
m ≥0, we have
m

k=0
ak Ak = S
 m

k=0
ak J k

S−1.
Therefore, the series (5.80) converges if and only if the series
∞

k=0
ak J k
(5.84)
converges. The series (5.84) converges if and only if each of the following series
converges:
∞

k=0
ak J k
l (λ), where λ is the characteristic value of the matrix A, Jl(λ)
is the Jordan block corresponding to λ, and l is the order of the matrix Jl(λ). By
deﬁnition, 1 ≤l ≤nλ. Now we note that Jl(λ) = λIl + Jl(0). By Lemma 5.3, p.183,
it follows that (Jl(0))l = 0, (Jl(0)) j ̸= 0, j = 1, 2, . . . ,l −1. Therefore, for each
k > l −1, we get
(Jl(λ))k = λk I + Ck
1λk−1Jl(0) + · · · + Ck
l−1λk−l+1(Jl(0))l−1.
(5.85)
Thus the investigation of convergence of the series (5.84) is reduced to the investi-
gation of convergence of l power series of the form (5.83). If ρ(A) > r, then there
exists a characteristic value λ of the matrix A such that |λ| > r. In this case, the
series
∞

k=l
akλk corresponding to j = 0 diverges, and hence the condition ρ(A) ≤r
is necessary for the convergence of the series (5.80). Let now ρ(A) < r. Then for

190
5
Canonical Forms and Factorizations
each characteristic value λ of the matrix A, the following inequality holds: |λ| < r.9
Therefore, for each j = 1, 2, . . . ,l −1 and for all large enough k, we get
|akk(k −1) · · · (k −j + 1)λk−j|
= |ak|k(k −1) · · · (k −j + 1)
|λ|
r
k−j
rk−j ≤r j|ak|rk,
Thus all series (5.83) converge, and the series (5.80) also converges. Finally,
if ρ(A) = r, and for each characteristic value λ of the matrix A such that |λ| = ρ(A)
all series (5.83) converge, then as follows from the previous arguments, the series
(5.80) also converges; if at least one of them diverges, then the series (5.80) also
diverges.10
□
Let us give some examples of power series of matrices that arise in different
applications.
1. A Neumann series (or geometric progression) is a power series of matrices of the
form
I + A + A2 + · · · + Ak + · · · .
(5.86)
The power series
∞

k=1
λk, which diverges only for |λ| < 1, corresponds to the
series (5.86). Therefore, the series (5.86) converges if and only if ρ(A) < 1. If this
condition holds, then as a consequence, we get
Ak →0 as k →∞.
(5.87)
A matrix that satisﬁes condition (5.87) is called convergent. Using (5.85), we see
that if ρ(A) ≥1, then condition (5.87) does not hold. Thus a matrix A is convergent
if and only if ρ(A) < 1.
Theorem 5.21 Let A be a convergent matrix. Then the inverse matrix of I −A exists
and can be expressed as a power series:
(I −A)−1 = I + A + A2 + · · ·
(5.88)
Proof Clearly, if λ is an eigenvalue of the matrix I −A, then 1 −λ is an eigenvalue
of A. Since ρ(A) < 1, none of the eigenvalues of A are equal to one; hence none
of the eigenvalues of the matrix I −A are equal to zero. For each integer k ≥1, we
obviously have (I −A)(I + A + · · · + Ak) = I −Ak+1. Therefore,
9We can assume that r > 0, since otherwise, the matrix A is nilpotent and the series (5.80) consists
of a ﬁnite number of terms.
10Here we take into account the structure of the powers of the matrix Jl(0)); see the proof of
Lemma 5.3, p.183.

5.2 The Jordan Canonical Form
191
k

i=0
Ai = (I −A)−1 −(I −A)−1Ak+1.
Since A is convergent, the limit as k →∞on the right-hand side of the last equality
exists and is equal to (I −A)−1. Hence the limit on the left-hand side of the last
equality exists too, and the relationship (5.88) holds.
□
2. A matrix exponential is a power series of matrices of the following form:
eA = I + A + 1
2! A2 + · · · + 1
k! Ak + · · · .
(5.89)
The power series
∞

k=1
1
k!λk corresponding to (5.89) has inﬁnite radius of convergence.
Therefore, the series (5.89) converges for every A ∈Mn, n ≥1.
In the rest of this section we give without proof some useful properties of the
matrix exponential. The proof of these properties is left to the reader.
1. For all A ∈Mn, the following equality holds:
eA = lim
k→∞

I + 1
k A
k
.
Hint: note that for each m ≥1, we have
eA −

I + 1
m A
m
=
∞

k=1
 1
k! −Ck
m
mk

Ak,
and the coefﬁcients of Ak are nonnegative.
2. If A, B ∈Mn commute, then eA+B = eAeB. Hint: use the well-known equal-
ity ex+y = exey, which holds for all x, y ∈C.
3. For all A ∈Mn, t ∈R, the following equality holds:
d et A
d t
= Aet A.
(5.90)
5.3
Matrix Pencils
5.3.1
Deﬁnitions and Basic Properties
In this section, all vectors are elements of the space Cn, all matrices are, generally
speaking, complex, and we use only the standard inner product on Cn.

192
5
Canonical Forms and Factorizations
Let A, B be rectangular m × n matrices. The function that assigns to each λ ∈C
the matrix A + λB is called a matrix pencil. Since the pencil is uniquely deﬁned by
the ordered pair of matrices A, B, we usually denote the pencil by (A, B). If m = n,
i.e., A and B are square matrices of order n, then the polynomial det(A −λB) is
called the characteristic polynomial of the pencil (A, B). Arguing as in the derivation
of formula (4.75), p. 123, we get
det(A −λB) = Δ(a1, a2, . . . , an)
−λ (Δ(b1, a2, . . . , an) + Δ(a1, b2, . . . , an) + · · · + Δ(a1, a2, . . . , bn))
+ λ2(Δ(b1, b2, . . . , an) + · · · + Δ(a1, a2, . . . , bn−1, bn)) −· · ·
± λnΔ(b1, b2, . . . , bn).
(5.91)
Here a1, a2, . . . , an are the columns of the matrix A; b1, b2, . . . , bn are the columns
of the matrix B. All other symbols are the same as in formula (4.75), p.123.
It immediately follows from (5.91) that the degree of the characteristic polynomial
of the pencil (A, B) is less than or equal to rank(B).
If det(A −λB) = 0 for all λ ∈C, then the pencil (A, B) is called singular. If m ̸=
n, then the pencil is also called singular. In all other cases, the pencil is called regular.
Thus a regular pencil is a pencil of the square matrices A and B such that there exists
λ ∈C for which det(A −λB) ̸= 0.
Theorem 5.22 If Ker(A) ∩Ker(B) ̸= {0}, then the pencil (A, B) of square matrices
is singular.
Proof Indeed, if x ̸= 0 and Ax = 0, Bx = 0, then Ax −λBx = 0 for all λ ∈C.
Therefore, det(A −λB) = 0 for all λ ∈C.
□
Corollary 5.4 If the pencil (A, B) is regular, then Ker(A) ∩Ker(B) = {0}.
Let us give some examples.
1. A = I3 =
⎛
⎝
1 0 0
0 1 0
0 0 1
⎞
⎠, B =
⎛
⎝
1 1 1
0 1 1
0 0 1
⎞
⎠, rank(A) = rank(B) = 3,
p(λ) = det(A −λB) = (1 −λ)3, deg(p) = 3.11
2. A = I3, B =
⎛
⎝
1 1 1
0 1 1
0 0 0
⎞
⎠, rank(B) = 2,
p(λ) = det(A −λB) = (1 −λ)2, deg(p) = 2.
11We denote degree of the polynomial p by deg(p).

5.3 Matrix Pencils
193
3. A = I3, B =
⎛
⎝
0 1 1
0 0 1
0 0 1
⎞
⎠, rank(B) = 2,
p(λ) = det(A −λB) = (1 −λ), deg(p) = 1.
4. A = I3, B =
⎛
⎝
0 1 1
0 0 1
0 0 0
⎞
⎠, rank(B) = 2,
p(λ) = det(A −λB) = 1, deg(p) = 0.
5. A =
⎛
⎝
1 0 0
0 1 0
0 0 0
⎞
⎠, B =
⎛
⎝
0 1 1
0 0 1
0 0 0
⎞
⎠, rank(A) = rank(B) = 2,
Ker(A) ∩Ker(B) = {0}, p(λ) = det(A −λB) ≡0; the pencil (A, B) is singular.
A number λ ∈C is called a characteristic value of a regular pencil (A, B) if
det(A −λB) = 0. Let λ be a characteristic value of the pencil (A, B). Then a vec-
tor x ̸= 0 is called an eigenvector corresponding to λ if Ax = λBx.
Two pencils (A, B) and (A1, B1) are called equivalent if there exist nonsingular
matrices U, V such that A1 = U AV , B1 = U BV .12 It is useful to note that the
conversion to an equivalent pencil is in fact a change of bases in the spaces Cn, Cm
(see Section4.1.5, p. 97).
Theorem 5.23 The characteristic polynomials of two equivalent pencils coincide
up to a constant nonzero factor.
Proof Indeed, det(U AV −λU BV ) = det(U) det(A −λB) det(V ).
□
We also note that if x is an eigenvector of the pencil (A, B) corresponding to a
characteristic value λ, then the vector y = V −1x is an eigenvector of the equivalent
pencil (U AV,U BV ) corresponding to the same characteristic value. Indeed, the
vector y is nonzero, and if Ax = λBx, then AV y = λBV y. Therefore, U AV y =
λU BV y.
Theorem 5.24 (Generalized Schur’s theorem). If the pencil (A, B) is regular, then
there exist unitary matrices U, V such that the matrices A1 = U AV and B1 = U BV
are upper triangular.
Proof First let us prove that there exist unitary matrices U1 and V1 such that all
entries of the ﬁrst columns of the matrices ˜A = U1AV1 and B = U1BV1 that are
located below the main diagonal are zero. By assumption, the pencil (A, B) is regular.
Therefore, two cases are possible: (1) the characteristic polynomial of the pencil has
a root (denote it by λ1); (2) the characteristic polynomial of the pencil is identically
equal to det(A) ̸= 0 (see (5.91)).
12The matrices A, B can be rectangular.

194
5
Canonical Forms and Factorizations
Let us consider the ﬁrst case. Let v1 be a normalized eigenvector of the pencil
corresponding to λ1, i.e.,
Av1 = λ1Bv1.
(5.92)
We join the vector v1 with some vectors to complete the orthonormal basis {vk}n
k=1
of the space Cn. Let V be a unitary matrix whose columns are formed by the vec-
tors {vk}n
k=1. Now we note that the vector Bv1 is nonzero, since otherwise, using
(5.92), we see that Av1 would also equal zero, but that contradicts the assumption
on the regularity of the pencil (see Corollary 5.4). We take u1 = Bv1/|Bv1|, join
the vector u1 with some vectors to complete the orthonormal basis {uk}n
k=1 of the
space Cn, and construct the matrix U1 whose rows are ¯u1, ¯u2, …, ¯un. Using ele-
mentary calculations, we see that the elements of the ﬁrst column of the matrix
B = U1BV1 are computed by the formulas ˜b j,1 = |Bv1|(u1, u j), j = 1, 2, . . . , n.
Therefore, ˜b1,1 = |Bv1| > 0, ˜b j,1 = 0 for j = 2, 3, . . . , n. The elements of the ﬁrst
column of the matrix ˜A = U1AV1 are analogously determined. As a result, we
get ˜a11 = (Av1, u1) = λ1|Bv1|, ˜a1 j = (Av1, u j) = 0 for j = 2, 3, . . . , n. Now let
us turn to the case that the characteristic polynomial of the pencil (A, B) has no
roots. Then det(B) = 0, but det(A) ̸= 0, and hence there exists a normalized vec-
tor v1 such that Bv1 = 0 and Av1 ̸= 0. As in the ﬁrst case, we construct orthonormal
bases {vk}n
k=1 and u1 = v1/|Av1|, u2, …, un of the space Cn, and using them, we
form the unitary matrices U, V . Using elementary calculations, we see that the ﬁrst
column of the matrix B = U1BV1 is zero, the diagonal element of the ﬁrst column of
the matrix ˜A = U1AV1 is equal to |Av1| > 0, and all other elements of this column
are zero. The further arguments are based on decreasing the order of the considered
matrices and are completely analogous to the corresponding arguments in the proof
of Theorem 4.23, p. 127.
□
It is useful to note that if the triangular matrices A1 and B1 that appear in
Theorem 5.24 are constructed, then the characteristic equation of the pencil (A, B)
can be written in the form
n
i=1
(a(1)
ii −λb(1)
ii ) = 0,
where a(1)
ii
and b(1)
ii , i = 1, 2, …, n, are the diagonal elements of the matrices A1
and B1, respectively. Therefore, if the characteristic polynomial of a pencil has
degree k, then the characteristic values of the pencil are calculated by the formu-
las λi = a(1)
ii /b(1)
ii
for i = 1, 2, . . . , k. Obviously, b(1)
ii = 0 for i > k. Hence if the
polynomial of a pencil has degree k < n, then we say that the pencil has an inﬁnite
characteristic value of multiplicity n −k.

5.3 Matrix Pencils
195
5.3.2
The Quasidiagonal Form of a Regular Pencil
Theorem 5.25 Every regular pencil (A, B) can be reduced to an equivalent quasi-
diagonal form, namely, to the pencil (A1, B1), where
A1 =
A11
0
0
In−k

,
B1 =
Ik
0
0 B22

,
(5.93)
the matrix A11 is upper triangular, all characteristic values λ1, λ2, …, λk of the pencil
(A, B) form the main diagonal of this matrix; B22 is an upper triangular matrix with
a zero main diagonal; Ik, In−k are identity matrices of the corresponding orders.
Proof As we proved in Theorem 5.24, the pencil (A, B) is equivalent to the pencil
A11 A12
0
A22

,
B11 B12
0
B22

.
(5.94)
Here A11 is an upper triangular matrix of order k whose main diagonal is formed
by the numbers b11λ1, b22λ2, …, bkkλk; B11 is an upper triangular matrix of order
k whose main diagonal is formed by the numbers b11, b22, …, bkk, all of which are
nonzero; A22 is an upper triangular matrix of order n −k whose diagonal elements
ak+1,k+1, ak+2,k+2, …, ann are nonzero; B22 is an upper triangular matrix with zero
main diagonal. Multiplying both matrices of the pencil (5.94) by the block diagonal
matrix diag(B−1
11 , A−1
22 ), we move to the equivalent pencil
 ˜A11 ˜A12
0
In−k

,
Ik ˜B12
0 ˜B22

.
Here ˜A11 is an upper triangular matrix whose main diagonal is formed by the numbers
λ1, λ2, …, λk; the main diagonal of the upper triangular matrix ˜B22 is zero. We shall
complete the proof if we can construct k × (n −k) rectangular matrices P and Q
such that
Ik
Q
0 In−k
  ˜A11 ˜A12
0
In−k
 Ik
P
0 In−k

=
 ˜A11
0
0
In−k

,
Ik
Q
0 In−k
 Ik ˜B12
0 ˜B22
 Ik
P
0 In−k

=
Ik
0
0 ˜B22

.
By elementary calculations we get the following equations for the determination of
the matrices P and Q:
˜A11P + Q = −˜A12,
P + Q ˜B22 = −˜B12.
(5.95)

196
5
Canonical Forms and Factorizations
We can consider the system of Eq.(5.95) as a system of linear algebraic equations
for the elements of the matrices P and Q. To prove its solvability for every ˜A12 and
˜B12, it is enough to check that the corresponding homogeneous system
˜A11P + Q = 0,
P + Q ˜B22 = 0
(5.96)
hasthetrivialsolutiononly.Ifthematrices P and Q satisfy(5.96),then P = ˜A11P ˜B22
and Q = ˜A11Q ˜B22. The matrix ˜B22 is nilpotent. Therefore, arguing as in the proof
of Theorem 4.24, p.157, we see that P = 0 and Q = 0.
□
5.3.3
Weierstrass Canonical Form
In this subsection we show that every regular matrix pencil is equivalent to a pencil
of bidiagonal matrices that is analogous to the matrix Jordan canonical form.
Theorem 5.26 (Weierstrass13). Let (A,B) be a regular pencil of matrices of order n,
and let λ1, λ2, …, λk, k ≤n, be all its characteristic values. Then there exist nonsin-
gular matrices U and V such that U AV = diag(J, In−k) and U BV = diag(Ik, H).
Here In−k and Ik are the identity matrices of order n −k and k, respectively; J is a
Jordan matrix whose main diagonal is formed by the numbers λ1, λ2, . . . , λk; and
H is a nilpotent Jordan matrix.
Proof Let Sk and Sn−k be nonsingular matrices that reduce the matrices A11 and B22
of the equivalent quasidiagonal form (5.93) of the pencil (A, B) to Jordan canonical
form. Then we reduce the pencil (A1, B1) to the required form by the following
similarity transformation:
diag(Sk, Sn−k)A1diag(S−1
k , S−1
n−k),
diag(Sk, Sn−k)B1diag(S−1
k , S−1
n−k).
□
The pencil of matrices (diag(J, In−k), diag(Ik, H)) that appears in Theorem 5.26
is called the Weierstrass canonical form.
Theorem 5.27 The Jordan matrices J and H in the Weierstrass canonical form are
uniquely determined by the matrices of the original pencil (A, B) up to permutation
of the diagonal Jordan blocks.
Proof Let(diag(J, In−k), diag(Ik, H))and(diag(J1, In−k), diag(Ik, H1))betwodif-
ferent Weierstrass canonical forms of the same pencil of matrices (A, B). Then there
exist nonsingular matrices U and V such that
U11 U12
U21 U22
 J 0
0 I

=
J1 0
0 I
 V11 V12
V21 V22

,
(5.97)
13Karl Theodor Wilhelm Weierstrass (1815–1897) was a German mathematician.

5.3 Matrix Pencils
197
U11 U12
U21 U22
 I 0
0 H

=
I 0
0 H1
 V11 V12
V21 V22

.
(5.98)
In equalities (5.97), (5.98), we use the block representations of the matrices U and V
that correspond to the orders of the blocks J and H. Here we do not write the indices
that indicate the orders of the identity blocks. Using elementary calculations, as a
consequence of (5.97), (5.98) we get U11 = V11 and U22 = V22,
U11J = J1U11,
U22H = H1U22,
(5.99)
V21 = U21J,
U21 = H1V21,
(5.100)
U12 = J1V12,
V12 = U12H.
(5.101)
It follows from (5.100) and (5.101) that U21 = H1U21J and U12 = J1U12H. The
matrices H and H1 are nilpotent, and hence (see the ﬁnal part of the proof of Theorem
5.93) U12 = 0 and U21 = 0, and therefore, V12 = 0 and V21 = 0. The matrix U is
nonsingular, whence the matrices U11 and U22 are nonsingular. Thus (see (5.99)) the
matrix J is similar to J1, the matrix H is similar to H1, and the assertion follows
now from Theorem 5.16, p. 184.
□
5.3.4
Hermitian and Deﬁnite Pencils
If det(B) ̸= 0, then the matrix pencil (A, B) is equivalent to the pencils (B−1A, I)
and (AB−1, I). The set of all characteristic values of the pencil (A, B) coincides
with the spectrum of the matrix B−1A. Obviously, the eigenvectors of the matrix
B−1A are also connected with the eigenvectors of the pencil (A, B). These facts
are useful for theoretical investigations, but spectral problems for matrix pencils
with det(B) ̸= 0 are not solved numerically as eigenvalue problems for the matrix
B−1A, since usually, such important properties of the matrices A and B as symmetry,
sparseness, and so on, are lost.
The pencil (A, B) is called Hermitian if A = A∗, B = B∗. All characteristic
values of a Hermitian pencil are real. Indeed, if x ̸= 0, Ax = λBx, then (Ax, x) =
λ(Bx, x). The numbers (Ax, x) and (Bx, x) are real (see Theorem 4.37, p. 140).
It is important to note that as a consequence of Theorem 5.19, p. 187, we see that
the problem of calculating eigenvalues and eigenvectors of an arbitrary real matrix
is equivalent to the problem of characteristic values and eigenvectors of a pencil of
symmetric real matrices with det(B) ̸= 0. Thus the spectral problem for a pencil of
symmetric real matrices with det(B) ̸= 0 in the general case is as difﬁcult as the
spectral problem for an arbitrary real matrix. The situation improves if we narrow
the class of the allowable matrices B.

198
5
Canonical Forms and Factorizations
A pencil (A, B) is called deﬁnite if it is Hermitian and the matrix B is positive
deﬁnite. The next theorem shows that every deﬁnite pencil can be reduced to diagonal
form by a similarity transformation.
Theorem 5.28 If a pencil (A, B) is deﬁnite, then there exists a nonsingular matrixU
such that U ∗BU = I and U ∗AU = , where  = diag(λ1, λ2, . . . , λn).14
Proof Let us deﬁne a new inner product on Cn by the formula (x, y)B = (Bx, y).
The operator C = B−1A : Cn →Cn is self-adjoint with respect to this inner product,
since for all x, y ∈Cn, we have
(Cx, y)B = (BB−1 Ax, y) = (x, Ay) = (Bx, B−1 Ay) = (x, Cy)B.
Therefore, by Theorem 4.41, p. 143, there exist vectors e1, e2, …, en and numbers
λ1, λ2, …, λn such that
Cek = λkek, k = 1, 2, . . . , n,
(ek, el)B = δkl, k,l = 1, 2, . . . , n.
(5.102)
Let us construct the matrix U whose columns are the vectors e1, e2, …, en. It is easy
to see that the matrix U is nonsingular. We can write relationships (5.102) in the
form B−1AU = U and U ∗BU = I, where  = diag(λ1, λ2, . . . , λn). Evidently,
we have the equality U ∗AU = .
□
5.3.5
Singular Pencils. The Theorem on Reduction
In this subsection we show that every singular pencil is equivalent to a quasidiagonal
pencil of 2 × 2 block matrices of a special form. Let us denote by r the maximal
order of the minors of the matrix A + λB that are not identically zero as functions
of λ ∈C. The number r is called the rank of the pencil (A, B). We assume that the
pencil (A, B) is singular; hence we have one of the following inequalities: either
r < m or r < n. Here, as usual, m is the number of rows of the pencil, and n is
the number of columns of the pencil. To be deﬁnite, assume that r < n. Since the
pencil (A, B) is singular, for every λ ∈C, there exists a nonzero vector x(λ) ∈Cn
such that
(A + λB)x(λ) = 0 for all λ ∈C.
(5.103)
Solving the homogeneous system (5.103) by the method described in
Section 4.1.13, p. 110, we see that the components xk(λ) of the vector x(λ) are
calculated by the formulas Plk(λ)/Qmk(λ), where Plk and Qmk are some polyno-
mials, k = 1, 2, . . . , n. Multiplying the vector x(λ) by an arbitrary function of the
14Clearly, λ1, λ2, . . . , λn are the characteristic values of the pencil (A, B).

5.3 Matrix Pencils
199
variable λ, we also get a solution of system (5.103). Therefore, we can assume that
the vector x(λ) is a polynomial of degree ε:
x(λ) = x0 + λx1 + · · · + λεxε,
ε ≥0.
(5.104)
We choose the polynomial of minimal degree among all polynomials satisfy-
ing (5.103). The corresponding integer ε is called the minimal index of the singular
pencil (A, B).
Lemma 5.4 The minimal indices of equivalent pencils are equal.
Proof If x(λ) is a polynomial of the minimal degree satisfying (5.103), then for all
nonsingular matrices U and V , we have
U(A −λB)V V −1x(λ) = 0 for all λ ∈Cn.
Obviously, the degrees of the polynomials V −1x(λ) and x(λ) are equal.
□
Substituting the polynomial (5.104) into Eq. (5.103), collecting all coefﬁcients
with the same power of λ, and equating them to zero, we get a homogeneous system
of linear equations for vectors x0, x1, …, xε, which is equivalent to (5.103):
Ax0 = 0, Bx0 + Ax1 = 0, . . . , Bxε−1 + Axε = 0, Bxε = 0.
(5.105)
(Clearly, the matrix of system (5.105) is block bidiagonal.) If ε > 0 is the minimal
index of the pencil (A, B), then the system
Ax0 = 0, Bx0 + Ax1 = 0, . . . , Bxk−1 + Axk = 0, Bxk = 0
(5.106)
has only the trivial solution for each k < ε. Thus we have the following lemma.
Lemma 5.5 If ε>0 is the minimal index of the pencil (A, B), then the columns of
the matrix of system (5.106) are linearly independent for each k <ε.
Lemma 5.6 Let ε be the minimal index of the pencil (A, B) and let x0, x1, …, xε
be the solutions of system (5.105). Then these vectors are linearly independent. The
vectors Ax1, Ax2, …, Axε are also linearly independent.
Proof First let us prove that none of the vectors {xi}ε
i=0 are equal to zero. Indeed,
if x0 = 0, then the polynomial λ−1x(λ) has degree ε −1 and satisﬁes relation-
ship (5.103), which contradicts the assumption of the minimality of the index ε.
If x j = 0 for every j ≥1, then Ax j = 0. Not all vectors x0, x1, …, x j−1 are zero. It
follows from (5.105) that these vectors satisfy system (5.106) for k = j −1, which
contradicts the assumption of the minimality of the index ε. In the same way, we
prove that none of the vectors {Axi}ε
i=1 are equal to zero. Let us prove that the vec-
tors {Axi}ε
i=1 are linearly independent. If we assume the contrary, then there exist an
integer h ∈[1, ε] and numbers α1, α2, …, αh−1, not all zero, such that

200
5
Canonical Forms and Factorizations
Axh = α1Axh−1 + α2 Axh−2 + · · · + αh−1Ax1.
(5.107)
Let
y0 = x0,
y1 = x1 −α1x0,
y2 = x2 −α1x1 −α2x0, . . . ,
yh−1 = xh−1 −α1xh−2 −· · · −αh−1x0.
Using Eqs. (5.105) and (5.107), it is easy to see that
Ay0 = Ax0 = 0,
Ay1 = Ax1 −α1Ax0 = −Bx0 = −By0,
Ay2 = Ax2 −α1Ax1 −α2 Ax0 = −Bx1 + α1Bx0 = −B(x1 −α1x0) = −By1, . . . ,
Ayh−1 = −Byh−2,
Byh−1 = 0.
Therefore, the polynomial y(λ) = y0 + λy1 + · · · + λh−1yh−1 has degree h −1<ε
and satisﬁes a relationship of the form (5.103), but this fact contradicts the assumption
of the minimality of the index ε. It remains to prove that the vectors {xi}ε
i=0 are
linearly independent. Now we assume that α0x0 + α1x1 + · · · + αεxε = 0 for some
numbers α0, α1, …, αε. Then α1Ax1 + α2 Ax2 + · · · + αε Axε = 0. As a consequence
of the linear independence of the vectors {Axi}ε
i=1, we get α1 = α2 = · · · = αε = 0.
Hence α0x0 = 0, but x0 ̸= 0, and therefore, α0 = 0.
□
Lemma 5.7 If a pencil (A, B) has minimal index ε > 0, then it is equivalent to the
quasitriangular pencil (A1, B1), where
A1 =
L(0)
ε
D
0
ˆA

,
B1 =
L(1)
ε
F
0
ˆB

,
(5.108)
L(0)
ε
= (0, Iε),
L(1)
ε
= (Iε, 0),
(5.109)
and the minimal index of the pencil ( ˆA, ˆB) is greater than or equal to ε.
Proof Using Lemma 5.6, we see that we can introduce a basis in the space Cn
such that the ﬁrst ε + 1 vectors of this basis are {(−1)ixi}ε
i=0. Analogously, we can
introduce a basis in the space Cm whose ﬁrst vectors are {(−1)i Axi}ε
i=1.15 Equali-
ties (5.105) show that conversion to the speciﬁed bases leads to a pencil of matri-
ces (5.108). Let us prove the second part of the theorem. Note that there is no
polynomial y(λ) of degree less than ε that satisﬁes the identity
(L(0)
ε
+ λL(1)
ε )y(λ) = 0 for all λ ∈C.
(5.110)
15The alternation of signs in these bases will be convenient for some formulas below.

5.3 Matrix Pencils
201
Indeed, in this case, the polynomial x(λ) = (y(λ), 0) of degree less than ε satisﬁes
the identity
(A1 + B1)x(λ) = 0 for all λ ∈C,
(5.111)
but that is impossible, since the minimal indices of the equivalent pencils (A, B)
and (A1, B1) must be equal. Now we suppose that contrary to the assertion of the
theorem, there exists a polynomial z(λ) of degree less than ε that satisﬁes the identity
( ˆA + ˆB)z(λ) = 0 for all λ ∈C.
(5.112)
If we construct a polynomial v(λ) such that
(L(0)
ε
+ λL(1)
ε )v(λ) + (D + λF)z(λ) = 0 for all λ ∈C,
(5.113)
then x(λ) = (v(λ), z(λ)) ∈Cn
satisﬁes (5.111). Let z(λ) = z0 + λz1 + · · · +
λε−1zε−1, and we let v(λ) = v0 + λv1 + · · · + λε−1vε−1. Substituting these repre-
sentations into (5.113), collecting all coefﬁcients with the same power of λ, and
equating them to zero, we get the system of equations
L(0)
ε v0 = −g0, L(1)
ε v0 + L(0)
ε v1 = −g1, . . . , L(1)
ε vε−2 + L(0)
ε vε−1 = −gε−1,
L(1)
ε vε−1 = −gε,
(5.114)
where g0 = Dz0, g1 = Fz0 + Dz1,…, gε−1 = Fzε−2 + Dzε−1, gε = Fzε−1. It has
been proved that the minimal index of the pencil (L(0)
ε , L(1)
ε ) is equal to ε. Therefore,
by Lemma 5.5, it follows that the rank of the matrix of the system (5.114) is equal
to ε(ε + 1) (the number of its columns). It is easy to compute that the number of equa-
tions in the system (5.114) is also equal to ε(ε + 1). Therefore, the system (5.114) is
uniquely solvable for every right-hand side. Thus the polynomial x(λ) = (v(λ), z(λ))
has degree ε −1 and satisﬁes identity (5.111). This contradiction concludes the proof
of the lemma.
□
Theorem 5.29 (on reduction). If the minimal index of the pencil (A, B) is equal
to ε > 0 and the rank of the pencil is less than n, then the pencil (A, B) is equivalent
to the pencil
L(0)
ε
0
0
ˆA

,
L(1)
ε
0
0
ˆB

,
(5.115)
where as above, L(0)
ε
= (0, Iε), L(1)
ε
= (Iε, 0), and the minimal index of the
pencil ( ˆA, ˆB) is greater than or equal to ε.
Proof Let (A1, B1) be the pencil that was constructed in the proof of Lemma 5.7
and is equivalent to the pencil (A, B). Using elementary calculations, we see that

202
5
Canonical Forms and Factorizations
Iε
Q
0 Im−ε

A1
Iε+1
−P
0
In−ε−1

=
L(0)
ε
R
0
ˆA

,
Iε
Q
0 Im−ε

B1
Iε+1
−P
0
In−ε−1

=
L(1)
ε
S
0
ˆB

.
Here P and Q are some yet unknown rectangular matrices of corresponding orders,
R = D + Q ˆA −L(0)
ε P,
S = F + Q ˆB −L(1)
ε P.
We shall complete the proof of the theorem if we show that the matrices P and Q
can be chosen such that R = 0 and S = 0. Let p1, p2, . . . , pε+1 be the rows of the
matrix P. It is easy to see that the matrix L(0)
ε P consists of the rows p2, p3, . . . , pε+1,
and the matrix L(1)
ε P consists of the rows p1, p2, . . . , pε. Therefore, if R = 0 and
S = 0, then
q j ˆA + q j+1 ˆB + f j+1 + d j = 0,
j = 1, 2, . . . , ε −1.
(5.116)
Here the lowercase letters with indices denote the rows of the corresponding matrices.
Let us consider (5.116) as a system of equations for the elements of the rows (−1) jq j,
j = 1, 2, . . . , ε. Obviously, the matrix of this system has the same form as the matrix
of the system (5.106) for k = ε −2. Since by Lemma 5.7, the minimal index of
the pencil ( ˆA, ˆB) is greater than or equal to ε, the rank of this matrix is equal to
(ε −1)(n −ε −1), i.e., it is equal to the number of its columns. It is easy to calculate
that the number of equalities in system (5.116) is also equal to (ε −1)(n −ε −1).
Therefore, system (5.116) is solvable for every D and F. If we ﬁnd the matrix
Q, then the matrix P is easily found as the solution of the system of equations
L(0)
ε P = D + Q ˆA and L(1)
ε P = F + Q ˆB.
□
Remark 5.3 Let us recall that we have assumed above that the rank of the pen-
cil (A, B) is less than n. If the rank of the pencil (A, B) is less than m, then it is easy
to establish by reduction to the pencil (AT , BT ) that (A, B) is equal to the pencil
(L(0)
η )T 0
0
A

,
(L(1)
η )T 0
0
B

,
where η is the minimal index of the pencil (AT , BT ), and the minimal index of the
pencil (AT , BT ) is greater than or equal to η. The number ε is called the right minimal
index of the pencil (A, B), and the number η is called the left minimal index.

5.3 Matrix Pencils
203
5.3.6
Kronecker Canonical Form
In this subsection we show that every singular pencil (A, B) is equivalent to a qua-
sidiagonal pencil such that each of the diagonal blocks is a bidiagonal matrix.
First we assume that the right minimal index of the pencil (A, B) is equal to zero.
This means that there is a nonzero vector x ∈Cn such that Ax = 0 and Bx = 0. In
other words, the defect of the 2 × 1 block matrix
M =
A
B

(5.117)
is positive. Denote it by hr. Evidently, choosing the vectors of a basis of Ker(M) as
the ﬁrst hr vectors for the basis in the space Cn, we reduce the pencil (A, B) to the
pencil ((0(m, hr), A0), (0(m, hr), B0)), where 0(m, hr) is the m × hr zero matrix,
and the right minimal index of the pencil (A0, B0) is positive.
Now we assume that the left minimal index of the pencil (A0, B0) is equal to zero.
Then, arguing as above, we reduce the original pencil (A, B) to the quasidiagonal
pencil (diag(0(hl, hr), A1), diag(0(hl, hr), B1)), where hl is the dimension of the
kernel of the 1 × 2 block matrix M0 = (A0, B0). Clearly, in this case the left and
right minimal indices of the pencil (A1, B1) are positive. To be deﬁnite, assume
that the rank of the pencil (A1, B1) is less than the number of its columns. Then
by Theorem 5.29, it follows that the pencil (A1, B1) is equivalent to the pencil
(diag(L(0)
ε1 , ˆA1), diag(L(1)
ε1 , ˆB1)), where ε1 > 0, and the right minimal index of the
pencil ( ˆA1, ˆB1) is greater than or equal to ε1. Continuing this process, we get the
pencil
(diag(0(hl, hr), L(0)
ε1 , L(0)
ε2 . . . , L(0)
εp , ˆAp), diag(0(hl, hr)), L(1)
ε1 , L(1)
ε2 . . . , L(1)
εp , ˆBq),
where 0 < ε1 ≤ε1 · · · ≤εp, and the rank of the pencil ( ˆAp, ˆBp) is equal to the
number of its columns.
Suppose that the number of rows of the pencil ( ˆAp, ˆBp) is more than its rank (in
the contrary case, this pencil is regular). It is easy to see that since the left minimal
index of the pencil (A1, B1) is positive, the left minimal index of the pencil ( ˆAp, ˆBp)
is also positive. Consistently applying Theorem 5.29 again (see also Remark 5.3),
we reduce the pencil ( ˆAp, ˆBp) to the pencil
((L(0)
η1 )T , (L(0)
η2 )T . . . , (L(0)
ηq )T , ˆAq), (L(1)
η1 )T , (L(1)
η2 )T . . . , (L(1)
ηq )T , ˆBq),
where 0 < η1 ≤η1 · · · ≤ηq. Here the pencil ( ˆAq, ˆBq) is regular, and therefore it can
be reduced to Weierstrass canonical form (see Theorem 5.26, p. 196).
Thus we have proved that every arbitrary singular pencil can be reduced to an
equivalent pencil

204
5
Canonical Forms and Factorizations
(diag(0(hl, hr), L(0)
ε1 , L(0)
ε2 . . . , L(0)
εp , (L(0)
η1 )T , (L(0)
η2 )T . . . , (L(0)
ηq )T , J, In−k)),
diag(0(hl, hr), L(1)
ε1 , L(1)
ε2 . . . , L(1)
εp , (L(1)
η1 )T , (L(1)
η2 )T . . . , (L(1)
ηq )T , Ik, H)).
(5.118)
Here n is the order of the pencil ( ˆAq, ˆBq), k is the number of its characteristic
values, J is the corresponding Jordan matrix, and H is a nilpotent Jordan matrix
(details see in Theorem 5.26, p. 191).
The pair of matrices (5.118) is so called the Kronecker canonical form of the
singular pencil in the most general case. Clearly, in speciﬁc particular situations both
numbers hr and hl or one of them can be equal to zero, and therefore, every group
of diagonal blocks of the pencil (5.118) can be omitted.
5.3.7
Applications to Systems of Linear Differential
Equations
The Jordan, Weierstrass, and Kronecker canonical forms have many applications to
the study of systems of linear differential equations.
1. Let us begin with the Cauchy problem for a system of ordinary linear differential
equations with constant coefﬁcients:
˙x(t) = Ax(t) + f (t),
(5.119)
x(0) = x0.
(5.120)
Here A is a given square matrix of order n, f is a given continuous vector-valued
function of the variable t ∈R with values in the space Cn, the vector x0 ∈Cn is
given, and x is an unknown vector-valued function.
Let us recall (see (5.90), p.191) that
det A
d t
= Aet A,
(5.121)
et A|t=0 = I.
(5.122)
Using relationships (5.121), (5.122), it is easy to check by direct substitution that the
solution of problem (5.119), (5.120) is given by the formula
x(t) = et Ax0 +
t
0
e(t−τ)A f (τ)dτ.
(5.123)
In the simplest case of n = 1, i.e., if A = λ ∈C and x0, f (t) ∈C, the solution of
problem (5.119), (5.120) is calculated as follows:

5.3 Matrix Pencils
205
x(t) = x0eλt +
t
0
f (τ)eλ(t−τ)dτ.
(5.124)
Let S be a nonsingular matrix that reduces the matrix A to Jordan canonical
form (i.e., A = SJ S−1, J = diag(Jn1(λ1), Jn2(λ2), . . . , Jnk(λk)); see Section5.2.1,
p. 180). Then problem (5.119), (5.120) is reduced to the problem
˙y(t) = Jy(t) + g(t),
(5.125)
y(0) = y0,
(5.126)
where y = S−1x, g = S−1 f , y0 = S−1y(0). Clearly, problem (5.125), (5.126) splits
into the following system of independent equations:
˙yni(t) = Jni(λni)yni(t) + gni(t),
(5.127)
yni(0) = y0,ni ,
(5.128)
where i = 1, 2, . . . , k. Thus it is enough to solve the problem
˙y(t) = J(λ)y(t) + g(t),
(5.129)
y(0) = y0,
(5.130)
where J(λ) is the Jordan block whose order we denote by m. Let us write the
homogeneous system corresponding to (5.129) in detail:
˙y1(t) = λy1(t) + y2(t),
˙y2(t) = λy2(t) + y3(t),
. . . . . . . . . . . . . . . . . . . . . . . .
˙ym(t) = λym(t).
It follows from the last equation that
ym(t) = ym(0)eλt.
Therefore,
˙ym−1(t) = λym−1(t) + ym(0)eλt,
whence by formula (5.124), we get
ym−1(t) = (ym(0)t + ym−1(0))eλt.

206
5
Canonical Forms and Factorizations
Analogously,
ym−2(t) =

ym(0)t2
2 + ym−1(0)t + ym−2(0)

eλt, . . . .
Finally,
y1(t) =

ym(0)
tm−1
(m −1)! + ym−1(0)
tm−2
(m −2)! + · · · + y2(0)t + y1(0)

eλt.
Writing the obtained relationships in matrix form, we see that
y(t) = E J(λ)(t)y(0),
where
E J(λ)(t) = eλt
⎛
⎜⎜⎜⎜⎝
1
t t2/2 . . . tm−1/(m −1)!
0
1
t
. . . tm−2/(m −2)!
0
0
1
. . . tm−3/(m −3)!
. . . . . . . . . . . .
. . .
0
0
. . . . . .
1
⎞
⎟⎟⎟⎟⎠
.
It is easy to see that relationships (5.121), (5.122), where A = J(λ), hold for the
matrix-valued function E J(λ)(t). Thus,
E J(λ)(t) = et J(λ),
(5.131)
and the solution of problem (5.125), (5.126) can be calculated by the formula
y(t) = E J(λ)(t)y0 +
t
0
E J(λ)(t −τ)g(τ)dτ,
which is often used in the theory of differential equations for the analysis of systems
of the form (5.119).
Note that the reader can easily prove formula (5.131) using Property 2, p. 191.
Hint: use the equality J(λ) = λI + J(0).
2. Now we consider a system of linear differential equations of the form:
B ˙x(t) = Ax(t) + f (t).
(5.132)
First we assume that A, B ∈Mn and that the pencil (A, B) is regular. For all non-
singular matrices U, V ∈Mn, we have
U BV V −1 ˙x(t) = U AV V −1x(t) + U f (t).
(5.133)

5.3 Matrix Pencils
207
Suppose that the matrices U, V reduce the pencil (A, B) to Weierstrass canonical
form. Put y = V −1x, g = U f (t). Then (see Theorem 5.26, p. 196)
diag(Ik, H) ˙y = diag(J, In−k)y + g.
(5.134)
System (5.134) splits into independent subsystems of the form
˙y = J(λ)y + g,
(5.135)
where J(λ) is the Jordan block corresponding to a characteristic value λ of the pencil
(A, B), and
J(0) ˙y = y + g,
(5.136)
where J(0) is a nilpotent Jordan block. We denote its order by m and write sys-
tem (5.136) in detail as follows:
˙y2 = y1 + g1,
˙y3 = y2 + g2,
. . .. . .. . . . . . . . . . . .
˙ym = ym−1 + gm−1,
ym + gm = 0.
Starting from the last equation, we get
ym = −gm,
ym−1 = −gm−1 −˙gm, . . . .
Generally,
yi = −
m

k=i
dk−i
d tk−i gk,
i = 1, 2, . . . , m.
(5.137)
The method of solving the system (5.135) has been described above.
Thus, if the pencil (A, B) is regular, then the Cauchy problem for system (5.132) is
solvable. It is important to note that as follows from relationship (5.137), the solution
of (5.132) can include the derivatives of the function f . Therefore, if the function f
is insufﬁciently smooth, then the solution of (5.132) can be a discontinuous function.
If the pencil (A, B) is singular, then the system (5.132), as we will show below,
is solvable only if some conditions on the vector-valued function f (t) hold. When
using some matrices U, V (see (5.133)), we reduce the pencil (A, B) to Kronecker

208
5
Canonical Forms and Factorizations
canonical form (5.118), we get a system of differential equations that splits into
independent subsystems of the investigated forms and the following forms:
0(hl, hr) ˙y(t) = 0(hl, hr)y + g,
(5.138)
L(0)
ε ˙y(t) = L(1)
ε y + g,
(5.139)
(L(0)
η )T ˙y(t) = (L(1)
η )T y + g.
(5.140)
First of all, using (5.138), we see that for solvability of the system (5.132) it is
necessary that the ﬁrst hl components of the vector U f be equal to zero. Note that
the number of equations in system (5.139) is less than the number of unknowns,
i.e., this system is underdetermined. System (5.140) is overdetermined: the number
of equations in (5.140) is greater than the number of unknowns. Writing the system
(5.139) in detail, we get (see (5.109), p. 200)
˙y2 = y1 + g1, ˙y3 = y2 + g2, . . . , ˙yε+1 = yε + gε.
(5.141)
Obviously, we can equate the function y1 with any function that is integrable on
every ﬁnite interval. After that, using (5.141), we can sequentially deﬁne all other
components of the vector-valued function y.
The system (5.140) in detail looks as follows:
0 = y1 + g1, ˙y1 = y2 + g2, . . . , ˙yε−1 = yε + gε, ˙yε = gε+1.
Hence, y1 = −g1, y2 = −g2 −˙g1, . . . , yε = −gε −˙gε−1 −· · · −dε−1
d tε−1 g1,
gε+1 = −d
d t

gε + ˙gε−1 + · · · + dε−1
d tε−1 g1

.
(5.142)
Equality (5.142) is a necessary condition of solvability of the system (5.140). Obvi-
ously, this equality deﬁnes some additional conditions for the components of the
vector-valued function f (t) that are necessary for the solvability of the original
system (5.132).

Chapter 6
Vector and Matrix Norms
In this chapter the concept of a norm on the vector space Cn is introduced. We
investigate relationships between different norms. We give the deﬁnition of a norm
on the space of complex rectangular matrices and study its properties in detail,
particularly with regard to estimates of eigenvalues and singular values of operators.
6.1
Basic Inequalities
Using Theorem 5.4, p. 171, it is easy to see that the function −ln(x) is convex on
the interval (0, ∞). Therefore, for arbitrary positive numbers a, b and p, q > 1 such
that 1/p + 1/q = 1, we have ln(a p/p + bq/q) ≥ln(a p)/p + ln(bq)/q = ln(ab);
hence we get ab ≤a p/p + bq/q. Clearly, the last inequality holds also for ab = 0.
Further, since |ab| = |a||b|, we get
|ab| ≤|a|p
p
+ |b|q
q
(6.1)
for all, generally speaking, complex numbers a, b and for all p, q > 1 such that 1/p+
1/q = 1. Inequality (6.1) is called Young’s inequality.1
Theorem 6.1 (Hölder’s2 inequality). Let x, y ∈Cn, p > 1, 1/p + 1/q = 1. Then

n

k=1
xk yk
 ≤
 n

k=1
|xk|p
1/p  n

k=1
|yk|q
1/q
.
(6.2)
1William Henry Young (1863–1942) was an English mathematician.
2Otto Ludwig Hölder (1859–1937) was a German mathematician.
© Springer International Publishing AG 2017
L. Beilina et al., Numerical Linear Algebra: Theory and Applications,
DOI 10.1007/978-3-319-57304-5_6
209

210
6
Vector and Matrix Norms
Proof If at least one of the vectors x, y is equal to zero, then inequality (6.2) holds.
Assume that the vectors x, y are nonzero. Then using Young’s inequality, we get
|xl|
 n
k=1
|xk|p
1/p
|yl|
 n
k=1
|yk|q
1/q ≤
|xl|p
p
n
k=1
|xk|p
+
|yl|q
q
n
k=1
|yk|q
,
l = 1, 2, . . . , n.
Summing these inequalities over all l, we obtain
n

k=1
|xk||yk| ≤
 n

k=1
|xk|p
1/p  n

k=1
|yk|q
1/q
.
Hence inequality (6.2) holds.
□
In the special case of p = 2, inequality (6.2) is called the Cauchy–Schwarz
inequality.
Theorem 6.2 (Minkowski’s inequality). Let x, y ∈Cn, p > 1. Then
 n

k=1
|xk + yk|p
1/p
≤
 n

k=1
|xk|p
1/p
+
 n

k=1
|yk|p
1/p
.
(6.3)
Proof If the left-hand side of (6.3) is equal to zero, then inequality (6.3) evidently
holds. We therefore assume that the left-hand side of (6.3) is positive. Clearly,
n

k=1
|xk + yk|p =
n

k=1
|xk + yk|p−1|xk + yk|
≤
n

k=1
|xk + yk|p−1|xk| +
n

k=1
|xk + yk|p−1|yk|.
(6.4)
Let us estimate the sums on the right-hand side of the last inequality using Hölder’s
inequality:
n

k=1
|xk + yk|p−1|xk| ≤
 n

k=1
|xk + yk|(p−1)q
1/q  n

k=1
|xk|p
1/p
,
(6.5)
n

k=1
|xk + yk|p−1|yk| ≤
 n

k=1
|xk + yk|(p−1)q
1/q  n

k=1
|yk|p
1/p
,
(6.6)

6.1 Basic Inequalities
211
where 1/p + 1/q = 1 and (p −1)q = p. Thus, combining (6.4)–(6.6), we obtain
n

k=1
|xk + yk|p ≤
 n

k=1
|xk + yk|p
1/q ⎛
⎝
 n

k=1
|xk|p
1/p
+
 n

k=1
|yk|p
1/p⎞
⎠,
and taking into account that 1 −1/q = 1/p, we get (6.3).
□
6.2
Norms on the Space Cn
In this section we consider the concept of a vector norm on the space Cn. This concept
generalizes the notion of the length of a vector x ∈Cn and in many cases is more
convenient.
We say that a norm is introduced on the space Cn if for each x ∈Cn, there exists
a unique real number ∥x∥(the norm of x) such that the following conditions (the
axioms of a norm) are satisﬁed:
1. ∥x∥≥0 for all x ∈Cn; ∥x∥=0 if and only if x =0;
2. ∥αx∥= |α|∥x∥for all x ∈Cn and for all α ∈C;
3. ∥x + y∥≤∥x∥+ ∥y∥for all x, y ∈Cn.
Condition 3 is usually called thetriangle inequality. Also, the following inequality
holds:
4.
∥x∥−∥y∥
 ≤∥x −y∥for all x, y ∈Cn.
Inequality 4 follows from Axiom 3. Indeed,
∥x∥= ∥x −y + y∥≤∥x −y∥+ ∥y∥.
Similarly,
∥y∥≤∥x −y∥+ ∥x∥.
Combining the last two inequalities, we obtain inequality 4.
Examples of norms on the space Cn.
1. Let p ≥1. The equality ∥x∥p =

n
k=1
|xk|p1/p
deﬁnes a norm. Indeed,
Axioms 1 and 2 evidently hold; inequality 3 for p = 1 immediately follows from
the properties of the modulus of a complex number, and for p > 1, it coincides with
Minkowski’s inequality. Note that ∥x∥2 = |x| = (x, x)1/2 for all x ∈Cn. Here and
further in this section, by (·, ·) we denote the standard inner product on the space Cn.
2. Let ∥x∥∞= max
1≤k≤n |xk|. It is easy to verify that this equality deﬁnes a norm.
3. The function ∥x∥A = (Ax, x)1/2 is a norm on the space Cn if A is a Hermitian
positive deﬁnite matrix. To substantiate this fact it is enough to recall that the

212
6
Vector and Matrix Norms
relationship (x, y)A = (Ax, y) deﬁnes an inner product on the space Cn (see Prop-
erty 1, p. 141, and also Section3.2.2, p. 73).
A vector norm is continuous on the entire space Cn. Indeed, let x and y be
arbitrary vectors in Cn. Expanding these vectors into the ﬁnite series x =
n
k=1
xkik
and y =
n
k=1
ykik with respect to the natural basis of Cn and using the triangle
inequality, we obtain ∥x −y∥≤
n
k=1
∥ik∥|xk −yk|. Obviously, this inequality implies
that if x tends to y, then ∥x −y∥tends to zero.
We say that a sequence {xk} ⊂Cn converges to a vector x ∈Cn in norm
if lim
k→∞∥x −xk∥= 0. As we have seen in the previous paragraph, the conver-
gence of a sequence of vectors in any norm introduced on the space Cn follows from
componentwise convergence. Below we will prove that the converse statement is also
true.
We say that two norms ∥·∥(1) and ∥·∥(2) are equivalent if there exist two positive
constants c1 and c2 such that
c1∥x∥(1) ≤∥x∥(2) ≤c2∥x∥(1)
for all
x ∈Cn.
(6.7)
Theorem 6.3 Any two norms on the space Cn are equivalent.
Proof Clearly, the relation of the equivalence of norms is transitive. Therefore, it is
sufﬁcient to prove that every norm ∥· ∥is equivalent to the norm ∥· ∥2 = | · |, i.e.,
that there exist two positive constants c1 and c2 such that
c1|x| ≤∥x∥≤c2|x|
for all
x ∈Cn.
(6.8)
Let S1(0) be the set of all vectors in the space Cn such that |x| = 1 (S1(0) is the
unit sphere centered at the origin). This set is closed and bounded in the space Cn.
The function ϕ(x1, x2 . . . , xn) = ∥x∥is continuous on the whole space Cn. There-
fore, by Weierstrass’s theorem (see a calculus textbook), it follows that there exist
points x1, x2 ∈S1(0) such that ∥x1∥=
min
x∈S1(0) ∥x∥and ∥x2∥= max
x∈S1(0) ∥x∥. Let
c1 = ∥x1∥, c2 = ∥x2∥. Clearly, 0 ≤c1 ≤c2, and we see also that c1 is not
equal to zero, since otherwise, x1 = 0, but x1 ∈S1(0), whence |x1| = 1 and
x1 ̸= 0. Thus, 0 < c1 ≤∥x∥≤c2 for all x ∈S1(0). Let now x be an arbitrary
nonzero vector in the space Cn. Then evidently, the vector (1/|x|)x belongs to S1(0),
and therefore, c1 ≤∥(1/|x|)x∥≤c2, whence it follows that the vector x satisﬁes
inequalities (6.8). Obviously, if x is equal to zero, then inequalities (6.8) hold.
□
It follows from Theorem 6.3 that every norm on the space Cn is equivalent to the
norm ∥· ∥∞. Therefore, the componentwise convergence of a sequence of vectors
follows from the convergence of this sequence in any norm.

6.2 Norms on the Space Cn
213
It is important to note that generally, the constants c1, c2 depend on n, i.e., they
depend on the dimension of the space Cn. For example, the following estimates hold:
∥x∥∞≤∥x∥p
for all x ∈Cn and for all p ≥1;
(6.9)
∥x∥p ≤∥x∥q
for all x ∈Cn if p ≥q ≥1;
(6.10)
∥x∥p ≤n1/p−1/q∥x∥q
for all x ∈Cn if q > p ≥1;
(6.11)
∥x∥p ≤n1/p∥x∥∞
for all x ∈Cn and for all p ≥1.
(6.12)
Before proving these estimates, we note that they are the best possible, i.e., for
each of them there exists a nonzero vector x such that the inequality becomes an
equality. In particular, if x = (1, 0, . . . , 0), then the ﬁrst two inequalities become
equalities; if x = (1, 1, . . . , 1), then the last two inequalities become equalities.
Now we present appropriate proofs.
1. Let ∥x∥∞= max
1≤k≤n |xk| = |xi|. Evidently,
|xi| = (|xi|p)1/p ≤
 n

k=1
|xk|p
1/p
= ∥x∥p.
2. Doing the obvious calculations, we obtain
∥x∥p =
 n

k=1
|xk|q|xk|p−q
1/p
≤∥x∥(p−q)/p
∞
∥x∥q/p
q
;
hence using (6.9), we get (6.10).
3. Writing |xk|p in the form |xk|p · 1 and then using Hölder’s inequality with t =
q/p > 1, r = t/(t −1) = q/(q −p) for the estimation of ∥x∥p, we get
∥x∥p =
 n

k=1
|xk|p
1/p
≤
 n

k=1
|xk|q
1/q  n

k=1
1
(q−p)/(pq)
= n1/p−1/q∥x∥q.
The reader can easily prove inequality (6.12). Then it can be proved that
∥x∥∞= lim
p→∞∥x∥p
for all
x ∈Cn.
The proof is also left to the reader.
A vector norm is called absolute if it depends only on the absolute values of the
components of the vector. For example, the norm ∥· ∥p for any p ≥1 is absolute;
the norm ∥x∥= (|x1|2 + |x2|2 −Re(x1x2))1/2 on the space C2 is not absolute.

214
6
Vector and Matrix Norms
Let D = diag(d1, d2, . . . , dn), 0 ≤di ≤1, i = 1, 2, . . . , n, x ∈Cn. Then for any
absolute norm we get ∥Dx∥≤∥x∥. Evidently, it is sufﬁcient to verify this inequality
for D = diag(1, . . . , 1, dk, 1, . . . , 1), dk ∈[0, 1]. We have
Dx = 1
2(1 −dk)(x1, x2, . . . , −xk, . . . , xn) + 1
2(1 −dk)x + dkx;
therefore, ∥Dx∥≤1
2(1 −dk)∥x∥+ 1
2(1 −dk)∥x∥+ dk∥x∥= ∥x∥.
A vector norm on Cn is called monotone if the inequality ∥x∥≤∥y∥follows
from the inequalities |xk| ≤|yk|, k = 1, 2, . . . , n. Every monotone norm is absolute.
Indeed, if a norm is monotone, then for each vector x, the following inequalities
hold:
∥(|x1|, |x2|, . . . , |xn|)∥≤∥(x1, x2, . . . , xn)∥≤∥(|x1|, |x2|, . . . , |xn|)∥.
Conversely, every absolute norm is monotone. Indeed, if for vectors x and y we
have |xk| ≤|yk|, k = 1, 2, . . . , n, then there exists a matrix3
D = diag(d1eiϕ1, d2eiϕ2, . . . , dneiϕn),
0 ≤dk ≤1,
k = 1, 2, . . . , n,
such that x = Dy. Using now the deﬁnition of an absolute norm and the inequal-
ity ∥Dy∥≤∥y∥, we get ∥x∥≤∥y∥.
6.3
The Hahn–Banach Theorem. Dual Norms
Let us recall that a linear functional f is deﬁned on the space Cn if a complex
number f (x) uniquely corresponds to each vector x ∈Cn and this map is linear, i.e.,
f (αx + βy) = αf (x) + β f (y)
for all
x, y ∈Cn and for all α, β ∈C. (6.13)
We say that a real linear functional f is deﬁned on the space Cn if a real number f (x)
uniquely corresponds to each vector x ∈Cn and
f (αx + βy) = αf (x) + β f (y)
for all
x, y ∈Cn and for all α, β ∈R. (6.14)
If a norm ∥· ∥is deﬁned on the space Cn, then for every linear functional (real or
complex), we can deﬁne its norm by the formula
∥f ∥=
sup
x∈Cn, x̸=0
| f (x)|
∥x∥
=
sup
x∈Cn, ∥x∥=1
| f (x)|.
(6.15)
3Let us recall that by deﬁnition, eiϕ = cos ϕ + i sin ϕ.

6.3 The Hahn–Banach Theorem. Dual Norms
215
For every linear functional we have
∥f ∥< ∞.
(6.16)
Let us prove inequality (6.16) for real functionals. For complex functionals the proof
is analogous and easier. Let z = (z1, z2, . . . , zn) ∈Cn, ∥z∥= 1. If we assume now
that zk = xk + iyk, xk, yk ∈R, k = 1, 2, . . . , n, then we get
f (z) = f
 n

k=1
(xk + iyk)ik

=
n

k=1
(xk f (ik) + yk f (iik)) .
Hence | f (z)| ≤max( max
1≤k≤n | f (ik)|, max
1≤k≤n | f (iik)|)
n
k=1
|zk|. Since all norms on Cn are
equivalent, using the last inequality, we conclude that | f (z)| ≤c∥z∥= c, where c
is a constant that depends only on n. This means that (6.16) is valid.
Theorem 6.4 (Hahn–Banach4). Let L be a subspace of the space Cn and f a linear
functional deﬁned on L,
∥f ∥=
sup
x∈L, ∥x∥=1
| f (x)|.
(6.17)
Then there exists a linear functional F deﬁned on Cn such that F(x) = f (x) for
all x ∈L and5
∥F∥=
sup
x∈Cn, ∥x∥=1
|F(x)| = ∥f ∥.
(6.18)
Proof First we assume that f is a real linear functional. Naturally, we suppose
that f is not identically zero. Therefore, without loss of generality, we can assume
that ∥f ∥= 1. We do not consider the trivial case L = Cn. Let u /∈L and let L1 ⊃L
be the set of all vectors of the form x + tu, where x ∈L, t ∈R. Using the triangle
inequality, we see that
f (x) −f (y) ≤∥x −y∥≤∥x + u∥+ ∥y + u∥
for all x, y ∈L. Hence, f (x) −∥x + u∥≤f (y) + ∥y + u∥. Therefore, there exists
a number a such that
sup
x∈L
( f (x) −∥x + u∥) ≤a ≤inf
x∈L( f (x) + ∥x + u∥).
(6.19)
Let us deﬁne a functional f1 on L1 by the formula f1(x + tu) = f (x) −at (check
that f1 is a real linear functional!). It follows from inequalities (6.19) that
4Hans Hahn (1879–1934) was an Austrian mathematician. Stefan Banach (1892–1945) was a Polish
mathematician.
5One says, F is the norm-preserving extension of the functional f onto the entire space Cn.

216
6
Vector and Matrix Norms
| f (x) −a| ≤∥x + u∥
for all
x ∈L,
and | f1(x+u)| ≤∥x+u∥for all x ∈L. For t ̸= 0 we get f1(x+tu) = t f1(t−1x+u);
hence
| f1(x + tu)| = |t|| f1(t−1x + u)| ≤|t|∥t−1x + u)∥= ∥x + tu∥,
or | f1(x)| ≤∥x∥for all x ∈L1. Arguing as above, we construct a real linear
functional f2 deﬁned on the set L2 ⊃L1 of all vectors of the form x + t(iu), where
x ∈L1, t ∈R, such that | f2(x)| ≤∥x∥for all x ∈L2. It is easy to see that the set L2
coincides with the subspace of the space Cn spanned by a basis of the subspace L and
the vector u. Thus we have constructed an extension of the real linear functional f
deﬁned on L onto the wider subspace. Increasing sequentially the dimension of the
subspaces, we can construct a real linear functional F deﬁned on the entire space Cn
such that F(x) = f (x) for all x ∈L and |F(x)| ≤∥x∥for all x ∈Cn. It follows
from the last estimate and (6.17) that ∥F∥= ∥f ∥.
Let now f be a (complex) linear functional deﬁned on L. We represent it in the
form f (x) = g(x) + ih(x) for all x ∈L, where g and h are real linear functionals
on L. Since the functional f is linear, we get f (ix) = g(ix) + ih(ix) = i f (x) =
ig(x) −h(x). Hence h(x) = −g(ix), and f (x) = g(x) −ig(ix). By assumption,
∥f ∥= 1, and consequently, we have ∥g∥≤1. Using the construction described in
the previous part of the proof, we construct a real linear functional G(x) deﬁned on
the entire space Cn such that
G(x) = g(x)
for all
x ∈L, and |G(x)| ≤∥x∥
for all
x ∈Cn.
Further, let F(x) = G(x) −iG(ix) for all x ∈Cn. Clearly, F(x) = f (x) for all
x ∈L. Now we prove that the functional F is linear. For this purpose, it is enough to
show additionally that F(ix) = iF(x) for all x ∈Cn. This fact follows directly from
the deﬁnition. Indeed, F(ix) = G(ix)+iG(x) = i(G(x)−iG(ix)). To complete the
proof, we check equality (6.18). Let x ∈Cn be a given vector. Take a real number θ
such that F(x)eiθ is nonnegative. Then
|F(x)| = F(eiθx) = G(eiθx) ≤∥eiθx∥= ∥x∥.
Combining (6.17) with the last inequality, we get (6.18).
□
Corollary 6.1 Let x0 ∈Cn be a given vector. There exists a linear functional F
deﬁned on Cn such that F(x0) = ∥x0∥and ∥F∥= 1.
Proof Let us consider the subspace L ⊂Cn of all vectors of the form αx0, where α ∈
C, and let us deﬁne on this subspace a linear functional f by the formula f (αx0) =
α∥x0∥. Then obviously, f (x0) = ∥x0∥and ∥f ∥= 1. To conclude the proof, using the
Hahn–Banach theorem, we construct a norm-preserving extension of the functional
f onto the entire space Cn.
□

6.3 The Hahn–Banach Theorem. Dual Norms
217
We can consider the space Cn to be a unitary space if we deﬁne an inner product
(for example, standard) on it. Using Riesz’s theorem (see p. 132), we see that for
each linear functional f on Cn there exists one and only one vector y ∈Cn such
that f (x) = (x, y) for all x ∈Cn, and conversely, every vector y ∈Cn generates a
linear functional: f (x) = (x, y) for all x ∈Cn. Let ∥· ∥be a norm on the space Cn.
For each vector y ∈Cn, we put
∥y∥∗= ∥f ∥=
sup
x∈Cn, x̸=0
|(x, y)|
∥x∥
=
sup
x∈Cn, ∥x∥=1
|(x, y)|.
(6.20)
Thereadercaneasilyprovethattherelationship(6.20)deﬁnesanormonthespaceCn.
Thisnormiscalleddual totheoriginalnorm.Thenexttheoremshowsthattheconcept
of the duality of norms is reciprocal.
Theorem 6.5 Let ∥· ∥be a norm on the space Cn and let ∥· ∥∗be its dual norm.
Then
∥x∥=
sup
y∈Cn, ∥y∥∗=1
|(x, y)|.
(6.21)
Proof It follows immediately from the deﬁnition of the dual norm that for every
nonzero y ∈Cn, the following inequality holds: ∥x∥≥|(x, y)|/∥y∥∗. Using Corol-
lary 6.1, we see that there exists a vector y such that ∥x∥= |(x, y)|/∥y∥∗. These
arguments show that equality (6.21) is valid.
□
In the proof of Theorem 6.5 we have established the following result.
Corollary 6.2 For all x, y ∈Cn, the following inequality holds:
|(x, y)| ≤∥x∥∥y∥∗.
(6.22)
Inequality (6.22) is called the generalized Cauchy–Schwarz inequality.
For example, the norms ∥·∥p, ∥·∥q for p > 1, 1/p+1/q = 1 are dual to each other
withrespecttothestandardinnerproductonCn.Indeed,forall x, y ∈Cn,byHölder’s
inequality (see (6.2)), we have |(x, y)| ≤∥x∥p∥y∥q. Let xk = ρkeiϕk, where k =
1, 2, . . . , n. Put yk = ρ p−1
k
eiϕk, k = 1, 2, . . . , n. By elementary calculations, we see
that |(x, y)| = ∥x∥p∥y∥q. Therefore,
∥x∥p =
sup
y∈Cn, y̸=0
|(x, y)|
∥y∥q
.
Now the reader can easily prove that the norms ∥· ∥1 and ∥· ∥∞are dual to each
other with respect to the standard inner product on the space Cn.

218
6
Vector and Matrix Norms
6.4
Norms on the Space of Matrices
As above, we denote by Mm,n the set of all rectangular matrices with m rows, n
columns, and (generally speaking) complex elements. If m = n, we write Mn. If
we deﬁne on the set Mm,n the operations of matrix addition and multiplication of a
matrix by a scalar in the usual way, then this set becomes a complex linear space
of dimension mn. On this linear space we introduce a norm, i.e., we associate with
each A ∈Mm,n a number ∥A∥such that the following axioms hold:
1. ∥A∥≥0 for all A ∈Mm,n; ∥A∥= 0 if and only if A = 0;
2. ∥αA∥= |α|∥A∥for all A ∈Mm,n and for all α ∈C;
3. ∥A + B∥≤∥A∥+ ∥B∥for all A, B ∈Mm,n.
We say in this case that a vector norm has been introduced on the space Mm,n. Clearly,
this norm has all the properties that were investigated in the last section for the norms
of vectors.
So-called consistent norms are used often on spaces of matrices. For consistent
norms, in addition to axioms 1–3, the following axiom must be satisﬁed:
4. ∥AB∥(mp) ≤∥A∥(mn)∥B∥(np) for all matrices A ∈Mm,n, B ∈Mn,p.
Here subscripts indicate norms on the corresponding spaces of matrices.
Not all vector norms on spaces of matrices are consistent. For example, we may
put
∥A∥= max
1≤i, j≤n |ai j|
(6.23)
for A ∈Mn. Obviously, this is a vector norm, but it is not a consistent norm on Mn.
Indeed, if
A =
1 1
1 1

, then AA =
2 2
2 2

,
and ∥A∥= 1, ∥AA∥= 2, then the inequality ∥AA∥≤∥A∥∥A∥is not satisﬁed.
Let ∥· ∥be a consistent norm on Mn and let S ∈Mn be an arbitrary nonsingular
matrix. Then, as the reader can easily prove, the formula
∥A∥(s) = ∥SAS−1∥
for all
A ∈Mn
deﬁnes a consistent norm on Mn.
Here are important examples of consistent matrix norms.
1. Let ∥A∥l1 =
n
i, j=1
|ai j| for A ∈Mn. Evidently, the ﬁrst three axioms hold. Let
us verify Axiom 4. By deﬁnition, for A, B ∈Mn we have
∥AB∥l1 =
n

i, j=1

n

k=1
aikbkj
 .

6.4 Norms on the Space of Matrices
219
Therefore,
∥AB∥l1 ≤
n

i, j,k=1
|aik||bkj|.
Adding nonnegative items to the right-hand side of the last inequality, we get
∥AB∥l1 ≤
n

i, j,k,m=1
|aik||bmj|.
It remains to note that
n

i, j,k,m=1
|aik||bmj| =
n

i,k
|aik|
n

j,m=1
|bmj| = ∥A∥l1∥B∥l1.
2. Let ∥A∥E =

m,n

i, j=1
|ai j|2
1/2
for A ∈Mm,n. This norm is generated by the
standardinner product on the space Cmn. Hence the ﬁrst three axioms for this norm
hold. Usually, the norm ∥A∥E is called the Euclidean norm or the Frobenius6 norm.
Using the Cauchy–Schwarz inequality (see p. 210), we verify Axiom 4. Let A ∈Mm,n
and B ∈Mn,p. Then
∥AB∥2
E =
m,p

i, j=1

n

k=1
aikbkj

2
≤
m,p

i, j=1
n

k=1
|aik|2
n

k=1
|bkj|2
=
m,n

i,k=1
|aik|2
n,p

k, j=1
|bkj|2 = ∥A∥2
E∥B∥2
E.
3. The reader can easily prove that the norm ∥A∥= n max
1≤i, j≤n |ai j| is consistent on
the space Mm,n.
Let A ∈Mm,n and let ∥· ∥(m), ∥· ∥(n) be some norms on the spaces Cm, Cn,
respectively. Then there exists a nonnegative number NA such that
∥Ax∥(m) ≤NA∥x∥(n)
for all
x ∈Cn.
(6.24)
Indeed, since every norm on Cn is equivalent to the norm ∥· ∥∞, i.e.,
c1∥x∥∞≤∥x∥(n)
for all
x ∈Cn,
∥x∥(m) ≤c2∥x∥∞
for all
x ∈Cm,
6Ferdinand Georg Frobenius (1849–1917) was a German mathematician.

220
6
Vector and Matrix Norms
where c1, c2 are positive constants independent of x, we see that the following chain
of inequalities holds:
∥Ax∥(m) ≤c2∥Ax∥∞= c2 max
1≤i≤m

n

j=1
ai jx j

≤c2∥x∥∞max
1≤i≤m
n

j=1
|ai j|
≤c2
c1
max
1≤i≤m
n

j=1
|ai j|∥x∥(n).
Denote by ν(A) the inﬁmum of the set of all numbers NA that satisfy (6.24).
Evidently, we can deﬁne the function ν on the space Mm,n in the following equivalent
way:
ν(A) =
sup
x∈Cn, x̸=0
∥Ax∥(m)
∥x∥(n)
=
sup
x∈Cn, ∥x∥(n)=1
∥Ax∥(m).
(6.25)
Clearly,
∥Ax∥(m) ≤ν(A)∥x∥(n)
for all
x ∈Cn.
The reader can easily prove that all axioms of a consistent matrix norm hold for the
function ν. The matrix norm (6.25) is called subordinate or induced or an operator
norm.
For each deﬁnition of a norm on the spaces Cm, Cn, there exists a vector x0 ∈Cn
such that ∥x0∥(n) = 1 and
∥Ax0∥(m) =
sup
x∈Cn, ∥x∥(n)=1
∥Ax∥(m),
i.e.,wecanreplace“sup”by“max”inthedeﬁnition (6.25).Theproofofthisstatement
is left to the reader.
It is easy to see that for each deﬁnition of a norm on Cn the subordinate norm of
the identity matrix (of order n) is equal to one.
Not every norm deﬁned on Mn is induced by a vector norm. For example, the
Frobenius norm is not induced by any vector norm, since ∥I∥E = √n. The norm
(6.23) is also not an operator norm, since this norm is not consistent on Mn.
Examples of calculations of subordinate matrix norms.
1. Suppose that a norm on the space Cn is deﬁned by the following equality (see
the ﬁrst example on p. 211 for p = 1): ∥x∥1 =
n
k=1
|xk|. Then the induced matrix
norm is
∥A∥1 =
max
x∈Cn, ∥x∥1=1 ∥Ax∥1.
It is easy to see that for every x ∈Cn, ∥x∥1 = 1, we have

6.4 Norms on the Space of Matrices
221
∥Ax∥1 =
n

i=1

n

j=1
ai jx j

≤
n

i=1
n

j=1
|ai j||x j| =
n

j=1
|x j|
n

i=1
|ai j|
≤max
1≤j≤n
n

i=1
|ai j|
n

j=1
|x j| = max
1≤j≤n
n

i=1
|ai j|.
Supposethat max
1≤j≤n
n
i=1
|ai j| =
n
i=1
|aik|.Let ˜x beavectorinthespaceCn suchthat ˜xk =
1 and all other coordinates of the vector ˜x are equal to zero. Then clearly, ∥˜x∥1 = 1
and ∥A ˜x∥1 =
n
i=1
|aik|. Therefore,
∥A∥1 =
max
x∈Cn, ∥x∥1=1 ∥Ax∥1 = max
1≤j≤n
n

i=1
|ai j|,
(6.26)
and so ∥A∥1 is called the maximum absolute column sum norm of the matrix A.
2. If a norm on the space Cn is deﬁned by the equality ∥x∥∞= max
k≤1≤n |xk|, then
for every x ∈Cn such that ∥x∥∞= 1, we have
∥Ax∥∞= max
1≤i≤n

n

j=1
ai jx j

≤max
1≤i≤n
n

j=1
|ai j||x j|
≤max
1≤j≤n |x j| max
1≤i≤n
n

j=1
|ai j| = max
1≤i≤n
n

j=1
|ai j|.
Suppose that max
1≤i≤1
n
j=1
|ai j| =
n
j=1
|akj|. Let ˜x ∈Cn be the vector with components
˜x j =

¯akj/|akj|, akj ̸= 0,
1,
akj = 0,
where j = 1, 2, . . . , n, and as usual, the overline is the symbol of complex con-
jugation. Clearly, ∥˜x∥∞= 1, and by elementary calculations, we see that for
all i = 1, 2, . . . , n, we have the inequalities

n

j=1
ai j ˜x j

≤
n

j=1
|ai j| ≤
n

j=1
|akj|,
and for i = k, we have

222
6
Vector and Matrix Norms

n

j=1
ai j ˜x j

=
n

j=1
|akj|,
i.e., ∥A ˜x∥∞= max
1≤i≤1
n
j=1
|ai j|. Therefore,
∥A∥∞=
max
x∈Cn, ∥x∥∞=1 ∥Ax∥∞= max
1≤i≤n
n

j=1
|ai j|,
and so ∥A∥∞is called the maximum absolute row sum norm of the matrix A.
3. Now we introduce norms on the spaces Cm and Cn induced by the standard
inner product, i.e., we set ∥x∥2 = |x|. For every x ∈Cn, we have
∥Ax∥2
2 = (Ax, Ax) = (A∗Ax, x).
The matrix A∗A is Hermitian and positive semideﬁnite. Hence there exists an ortho-
normal basis {ek}n
k=1 such that A∗Aek = σ 2
k ek, where σ 2
k , k = 1, 2, . . . , n, are the
eigenvalues of the matrix A∗A. They all are nonnegative. Expanding x into a ﬁnite
series x =
n
k=1
ξkek with respect to the basis {ek}n
k=1 and assuming that ∥x∥2 = 1,
we get
n
k=1
|ξk|2 = 1, ∥Ax∥2
2 =
n
k=1
σ 2
k |ξk|2 ≤max
1≤k≤n σ 2
k . Put now σ j = max
1≤k≤n σk and
˜x = e j. Then ∥A ˜x∥2
2 = σ 2
j . Thus we see that
max
x∈Cn, ∥x∥2=1 ∥Ax∥2 = max
1≤k≤n σk, i.e.,
∥A∥2 = max
1≤k≤n σk.
(6.27)
The next special case is of interest in many applications. Let A ∈Mn be a
Hermitian matrix, i.e., A = A∗. Then evidently, σk = |λk(A)|, where k = 1, 2, . . . , n,
and λk(A) is the eigenvalue of the matrix A. Therefore, for every Hermitian matrix,
we get
∥A∥2 = max
1≤k≤n |λk(A)| =
max
x∈Cn, x̸=0
|(Ax, x)|
(x, x)
= ρ(A),
where ρ(A) is the spectral radius of A (see p. 188). In this connection, the norm ∥A∥2
is usually called spectral.
The proof of the following propositions is left to the reader.
Proposition 6.1 For every matrix A we have (see p. 192)
∥A+∥2 = 1
σr
.
(6.28)
Here σr is the minimal singular value of the matrix A.

6.4 Norms on the Space of Matrices
223
Proposition 6.2 If the matrix A is invertible, then (see p. 167)
cond (A) = ∥A∥2∥A−1∥2.
Therefore, the notation cond (A) = cond 2(A) is often used.
The calculation of eigenvalues of a matrix, generally speaking, is a complicated
problem. Hence it is useful to estimate the norm∥A∥2 using an explicit function of
elements of A. Let us prove that for every matrix A ∈Mmn, we have the estimate
∥A∥2 ≤∥A∥E. Indeed, by elementary calculations, we get7 tr(A∗A) =
m,n

i, j=1
|ai j|2.
On the other hand, tr(A∗A) =
n
k=1
σ 2
k ≥max
1≤k≤n σ 2
k . Hence
∥A∥2 = max
1≤k≤n σk ≤
⎛
⎝
m,n

i, j=1
|ai j|2
⎞
⎠
1/2
= ∥A∥E.
(6.29)
The reader can easily check the following properties.
1. ∥A∥2 = ∥U AV ∥2 and ∥A∥E = ∥U AV ∥E for every matrix A ∈Mn and all
unitary matrices U and V .
2. ∥A∥2 = ∥A∗∥2 for every matrix A ∈Mn.
The value of a consistent matrix norm is useful particularly for estimating the spec-
tral radius of a matrix. Namely, for every square matrix A, the following inequality
holds:
ρ(A) ≤∥A∥,
(6.30)
where ∥A∥is any consistent norm of the matrix A. Indeed, let λ, x be an eigenpair
of the matrix A, and let X be a square matrix such that all columns of X are equal to
each other and equal to the vector x. Then AX = λX. Hence,
|λ|∥X∥= ∥AX∥≤∥A∥∥X∥
for every consistent matrix norm. We also see that ∥X∦= 0, since x is an eigenvector
that is not equal to zero by deﬁnition. Thus for each eigenvalue λ of the matrix A, the
following inequality holds: |λ| ≤∥A∥. This last inequality is equivalent to (6.30).
Clearly, the next corollary follows from estimate (6.30).
Corollary 6.3 If a consistent matrix norm of the matrix A ∈Mn is less than one,
then A is a convergent matrix.
7Here the trace of the matrix A∗A is calculated as the sum of all elements on the leading diagonal;
see p. 124.

224
6
Vector and Matrix Norms
Theorem 6.6 For every consistent matrix norm introduced on the space Mn and for
every matrix A ∈Mn, the following equality holds:
ρ(A) = lim
k→∞∥Ak∥1/k.
(6.31)
Proof If λ is an eigenvalue of the matrix A, then for every integer k >0, the numberλk
isaneigenvalueofthematrix Ak.Therefore,usinginequality(6.30),weget(ρ(A))k =
ρ(Ak)≤∥Ak∥,andρ(A)≤∥Ak∥1/k for every integer k >0. Further, let ε be a positive
number. Then the matrix (ρ(A) + ε)−1A is convergent, since the modulus of each
eigenvalue of this matrix is less than one. Therefore, (ρ(A) + ε)−k Ak →0 as
k →∞. Since every norm on Mn is a continuous function (see pp. 211, 217), we get
∥(ρ(A)+ε)−k Ak∥→0 as k →∞. Hence there exists N > 0 such that for all k ≥N,
the following inequalities hold: ∥(ρ(A) + ε)−k Ak∥≤1 and ∥Ak∥1/k ≤ρ(A) + ε.
Thus for every ε > 0 and sufﬁciently large k, we have the following estimates:
ρ(A) ≤∥Ak∥1/k ≤ρ(A) + ε. This statement is equivalent to (6.31).
□
Using (6.31), the reader can easily prove that
ρ(A + B) ≤ρ(A) + ρ(B),
ρ(AB) ≤ρ(A)ρ(B)
for all permutable matrices A and B.
Theorem 6.7 For every matrix A ∈Mn, we have
ρ(A) =
inf
S∈Mn, det(S)̸=0 ∥SAS−1∥1 =
inf
S∈Mn, det(S)̸=0 ∥SAS−1∥∞.
(6.32)
Proof Let us prove the theorem for the norm ∥· ∥1. For the norm ∥· ∥∞, all the
arguments are repeated verbatim. Matrices A and SAS−1 are similar. Therefore,
they have the same spectrum, and ρ(A) = ρ(SAS−1). Using this and (6.30), we get
ρ(A) ≤∥SAS−1∥1
for all
S ∈Mn, det(S) ̸= 0.
(6.33)
By Schur’s theorem, there is a unitary matrix U such that
U ∗AU = T,
(6.34)
where T is an upper triangular matrix, and all eigenvalues λ1, λ2, . . . , λn of the
matrix A form the main diagonal of T . Let D = diag(d, d2, . . . , dn), where d is a
positive number. Set
Q = DT D−1
(6.35)

6.4 Norms on the Space of Matrices
225
and compute
Q =
⎛
⎜⎜⎜⎜⎜⎜⎝
λ1 d−1t12 d−2t13 . . . d−(n−2)t1,n−1 d−(n−1)t1,n
0
λ2
d−1t23 . . . d−(n−3)t2,n−1 d−(n−2)t2,n
0
0
λ3
. . . d−(n−4)t3,n−1 d−(n−3)t3,n
. . . . . . . . . . . . . . . . . . . .
0
0
0
. . .
λn−1
d−1tn−1,n
0
0
0
. . .
0
λn
⎞
⎟⎟⎟⎟⎟⎟⎠
.
(6.36)
Let now ε be a given positive number. Then for d large enough, we can be certain
that the sum of all the absolute values of the entries of each column of the matrix Q
is less than or equal to ρ(A) + ε. Using (6.34), (6.35), we get SAS−1 = Q, where
S = DU −1, and ∥SAS−1∥1 = ∥Q∥1 ≤ρ(A) + ε. Since the last inequality can
be achieved by choosing the number d for any positive ε, this together with (6.33)
provides the ﬁrst equality in (6.32).
□
Using Theorem 6.7, the reader can easily prove that for every matrix A ∈Mn, the
following equality holds:
ρ(A) = inf
∥·∥∥A∥,
(6.37)
where the inﬁmum is taken over all consistent matrix norms on Mn, but in general,
“inf” cannot be replaced by “min” in (6.37).
At the end of this section we consider one important kind of norm on the space
of matrices. These are the so-called Ky Fan norms.
Theorem 6.8 Let A ∈Mm,n, σ1(A) ≥σ2(A) ≥· · · ≥σq(A) ≥0, q = min(m, n),
be the singular values of the matrix A (zeros are also included). Then
∥A∥k,p =
⎛
⎝
k

j=1
σ p
j (A)
⎞
⎠
1/p
,
where 1 ≤k ≤q, p ≥1 are given numbers, is a norm on the space Mm,n. For
m = n, the norm ∥A∥k,p is consistent.
Proof Obviously, in the considered case, Axioms 1 and 2, p. 218, are satisﬁed.
Therefore, we check only the following inequalities:
∥A + B∥k,p ≤∥A∥k,p + ∥B∥k,p for all A, B ∈Mm,n, and 1 ≤k ≤q, p ≥1,
(6.38)
∥AB∥k,p ≤∥A∥k,p∥B∥k,p for all A, B ∈Mn, and 1 ≤k ≤n, p ≥1.
(6.39)
Using Theorem 5.7, p. 174, Corollary 5.1, p. 173, and Minkowski’s inequality, p. 210,
we get (6.38). Inequality (6.39) follows immediately from Corollary 5.3, p. 175. □
The norm ∥A∥k,p is called the Ky Fan norm.

226
6
Vector and Matrix Norms
Remark 6.1 Since for every matrix A and all unitary matrices U, V of corresponding
dimensions, all the singular values of the matrices A and U A V coincide, we can
say that the norm ∥A∥k,p is unitarily invariant. If k = 1, we get the spectral norm.
If k = q and p = 2, we get the Frobenius norm. For every k > 1, the Ky Fan norm
of the identity matrix is greater than one. Therefore, it is not an operator norm. By
Theorem 5.12, p. 177, the norms ∥A∥k,1 =
k
j=1
σ j(A), 1 ≤k ≤q, can be calculated
using formulas (5.48) and (5.49), p. 177.
6.5
The Gap Between Two Subspaces of Cn
In this section, a norm on the space Cn is induced by an inner product. The norm on
Mn is the corresponding subordinate matrix norm.
Let L, M be subspaces of the space Cn and let PL, PM be the operators of the
orthogonal projection of Cn onto L and M, respectively (see p. 94). The number
ϑ(L, M) = ∥PL −PM∥
is called the gap between the subspaces L and M. It follows immediately from the
deﬁnition that the function ϑ satisﬁes the properties of a distance (a metric):
1. ϑ(L, M) ≥0, ϑ(L, M) = 0 if and only if L = M,
2. ϑ(L, M) = ϑ(M, L),
3. ϑ(L, M) ≤ϑ(L, N) + ϑ(N, M) for all subspaces L, M, N of Cn.
The following numbers will be useful in our future considerations:
dL,M =
max
x∈L,∥x∥=1 ∥x −PMx∥,
σL,M =
min
x∈L,∥x∥=1 ∥PMx∥.
These are connected by the equality
d2
L,M = 1 −σ 2
L,M.
(6.40)
Indeed, using the relationships PM = P∗
M, PM = P2
M (see pp. 94, 139), we can write
∥x −PM x∥2 = (x, x) + (PM x, PM x) −(x, PM x) −(PM x, x)
= (x, x) + (PM x, PM x) −2(PM x, PM x) = ∥x∥2 −∥PM x∥2.
Therefore,
max
x∈L, ∥x∥=1 ∥x −PMx∥2 = 1 −
min
x∈L, ∥x∥=1 ∥PMx∥2.
The number dL,M can be calculated by the formula dL,M = ∥(I −PM)PL∥.
Indeed, by deﬁnition,

6.5 The Gap Between Two Subspaces of Cn
227
∥(I −PM)PL∥=
sup
x∈Cn, x̸=0
∥(I −PM)PLx∥
∥x∥
.
Evidently,
∥(I −PM)PLx∥
∥x∥
≤∥(I −PM)PLx∥
∥PLx∥
if PLx ̸= 0. Therefore,
∥(I −PM)PL∥≤
sup
y∈L, y̸=0
∥(I −PM)y∥
∥y∥
= dL,M.
On the other hand, for x ∈L, x ̸= 0, we have
∥(I −PM)x∥
∥x∥
= ∥(I −PM)PLx∥
∥x∥
.
Thus,
dL,M =
sup
x∈L, x̸=0
∥(I −PM)PLx∥
∥x∥
≤
sup
x∈Cn, x̸=0
∥(I −PM)PLx∥
∥x∥
= ∥(I −PM)PL∥.
Theorem 6.9 For subspaces L, M of the space Cn, we have the following equality:
∥PL −PM∥= max(dL,M, dM,L).
Proof Obviously, PL −PM = PL(I −PM) −(I −PL)PM. Hence, using the equal-
ity PL(I −PL) = 0 (see p. 94), for all x ∈Cn we get
∥(PL −PM)x∥2 = ∥PL(I −PM)x∥2 + ∥(I −PL)PM x∥2
= ∥PL(I −PM)(I −PM)x∥2 + ∥(I −PL)PM PM x∥2
≤∥PL(I −PM)∥2∥(I −PM)x∥2 + ∥(I −PL)PM∥2∥PM x∥2
≤max(∥PL(I −PM)∥2, ∥(I −PL)PM∥2)(∥(I −PM)x∥2 + ∥PM x∥2)
= max(∥PL(I −PM)∥2, ∥(I −PL)PM∥2)∥x∥2.
Note that ∥PL(I −PM)∥= ∥(PL(I −PM))∗∥= ∥(I −PM)PL∥(see Property 2,
p. 167). Therefore, ∥PL −PM∥≤max(dL,M, dM,L). The converse inequality also
holds. Indeed,
(I −PL)PM = PM −PL PM = P2
M −PL PM = (PM −PL)PM,
and ∥(I −PL)PM∥≤∥PL −PM∥. Analogously, ∥(I −PM)PL∥≤∥PL −PM∥.
□
Clearly, we have the following corollary.

228
6
Vector and Matrix Norms
Corollary 6.4 For all subspaces L, M ∈Cn, we have
0 ≤ϑ(L, M) ≤1.
Theorem 6.10 If ϑ(L, M) < 1, then dim L = dim M.
Proof Using Theorem 4.3, p. 97, we see that it is enough to show that PL is a
bijective map from M to L. By assumption, ∥PL −PM∥< 1. Therefore, the operator
I −(PL −PM) is invertible (see Theorem 5.21, p. 191, and Corollary 6.3, p. 223),
and Im(I −(PL −PM)) = Cn. In other words, (I −(PL −PM))Cn = Cn. Acting
on both sides of the last equality by the operator PL, we get PL PMCn = L. Hence,
PL M = L, i.e., the operator PL maps M onto the entire subspace L. To complete the
proof, we show that this map is bijective. If we assume the contrary, then there exists a
nonzero vector x0 ∈M such that PLx0 = 0. Then x0 = PMx0 −PLx0, and therefore,
∥x0∥≤∥PM −PL∥∥x0∥< ∥x0∥, but this inequality cannot be satisﬁed.
□
Theorem 6.11 If dim L = dim M, then dL,M = dM,L.
Proof First we assume that σL,M = 0, i.e., that there exists x ∈L, ∥x∥= 1, such that
PMx = 0 (in other words, x ∈M⊥). Now we show that σM,L = 0, i.e., that there
exists a vector y ∈M, ∥y∥= 1, such that PL y = 0. Denote by L⊥
x the orthogonal
complement in L of the one-dimensional subspace spanned by x. Clearly, dim L⊥
x =
dim L −1. Let (L⊥
x )⊥be the orthogonal complement in the space Cn of the subspace
L⊥
x . Then dim(L⊥
x )⊥= n −dim L + 1, and hence dim(L⊥
x )⊥+ dim M = n + 1.
Therefore,thereexistsavector y,∥y∥= 1,thatbelongsto(L⊥
x )⊥∩M.Since x ∈M⊥,
we see that y is orthogonal to x, i.e., y ∈L⊥. Hence PL y = 0. Now we note that
if σL,M = σM,L = 0, then dL,M = dM,L = 1 (see (6.40)). Thus we can assume
that σL,M > 0. By the deﬁnition of σL,M, we see that there exists a vector x ∈L,
∥x∥= 1, such that ∥PMx∥2 = σ 2
L,M. Let us show that PMx −σ 2
L,Mx ∈L⊥. To do
this, using the deﬁnition of σL,M, we write
(PM(x + v), x + v) ≥σ 2
L,M(x + v, x + v)
for all
v ∈L.
By elementary calculations, we therefore obtain
(1 −σ 2
L,M)(v, v) + (PMx −σ 2
L,Mx, v) + (v, PMx −σ 2
L,Mx) ≥0
for all
v ∈L.
If we replace here v by tv, t ∈R, we get
t2(1−σ 2
L,M)(v, v)+t(PMx −σ 2
L,Mx, v)+t(v, PMx −σ 2
L,Mx) ≥0
for all
t ∈R.
(6.41)
It follows from (6.41) that Re(PMx −σ 2
L,Mx, v) = 0. Replacing v by iv in (6.41), we
get Im(PMx −σ 2
L,Mx, v) = 0. Therefore, (PMx −σ 2
L,Mx, v) = 0 for all v ∈L. In
other words, PMx−σ 2
L,Mx ∈L⊥. Hence, PL PMx−σ 2
L,M PLx = PL PMx−σ 2
L,Mx =0.
Now let y = σ −1
L,M PMx. Then y ∈M, ∥y∥= 1, PL y = σL,Mx, ∥PL y∥= σL,M, and
hence σM,L ≤σL,M. Analogously, σL,M ≤σM,L, i.e., σL,M = σM,L.
□

6.5 The Gap Between Two Subspaces of Cn
229
Corollary 6.5 If dim L = dim M, then
ϑ(L, M) = ∥PL −PM∥= ∥(I −PM)PL∥= ∥(I −PL)PM∥.
As a conclusion, the reader is invited to give a geometric interpretation of the
function ϑ and all statements of this section for subspaces of three-dimensional
Euclidean space V3.

Chapter 7
Elements of Perturbation Theory
In this chapter we study the inﬂuence of perturbations of matrices on the solutions of
such basic problems of linear algebra as calculating eigenvalues and singular values
of operators, constructing the inverse matrix, solving systems of linear algebraic
equations, and solving the linear least squares problem.
7.1
Perturbations in the Symmetric Eigenvalue Problem
Let A and B be Hermitian matrices of order n. Writing the obvious equality
A = B + (A −B),
using inequalities (4.148), p. 150, and inequality (6.30), p. 223, we see that
max
1≤k≤n |λk(A) −λk(B)| ≤max
1≤k≤n |λk(A −B)|,
(7.1)
max
1≤k≤n |λk(A) −λk(B)| ≤∥A −B∥,
(7.2)
where ∥· ∥is any consistent matrix norm. Using, for example, the Frobenius norm
(see p. 218), we get
max
1≤k≤n |λk(A) −λk(B)| ≤
⎛
⎝
n

i, j=1
|ai j −bi j|2
⎞
⎠
1/2
.
(7.3)
Inequalities (7.1)–(7.3) are usually called Weyl’s inequalities.
© Springer International Publishing AG 2017
L. Beilina et al., Numerical Linear Algebra: Theory and Applications,
DOI 10.1007/978-3-319-57304-5_7
231

232
7
Elements of Perturbation Theory
If we put |ai j −bi j| ≤ε, then max1≤k≤n |λk(A) −λk(B)| ≤nε. It is easy to see
that if A = I and all the elements of the matrix E are equal to ε > 0, then
max
1≤k≤n |λk(A) −λk(A + E)| = nε,
i.e., the estimate (7.3) is the best possible for the set of all Hermitian matrices.
A special perturbation of a Hermitian matrix is considered in the next theorem.
Theorem 7.1 (“relative” Weyl’s theorem). Let λ1 ≥λ2 ≥· · · ≥λn be the eigen-
values of a Hermitian matrix A ∈Mn, and let ˜λ1 ≥˜λ2 ≥· · · ≥˜λn be the eigenvalues
of the matrix X∗AX, where X is an arbitrary nonsingular matrix. Then
|˜λi −λi| ≤λi∥I −X∗X∥,
i = 1, 2, . . . , n,
(7.4)
where ∥· ∥is any consistent matrix norm.
Proof Let us take an integer i ∈[1, n] and write the obvious equality
X∗(A −λi I)X = H + F,
where H = X∗AX −λi I, F = λi(I −X∗X). It is easy to see that the ith eigen-
value of the matrix A −λi I is zero. Using Sylvester’s law of inertia, p. 146, we can
easily check that the ith eigenvalue of the matrix X∗(A−λi I)X is also zero. The ith
eigenvalue of the matrix H is ˜λi −λi. Hence using inequality (7.2), we get (7.4).
□
Theorem 7.1 shows that if we replace the matrix A by X∗AX, where X is a
nonsingular matrix, then all zero eigenvalues are preserved, and for all nonzero
eigenvalues, we have the relative error estimate
|˜λi −λi|
|λi|
≤∥I −X∗X∥,
i = 1, 2, . . . , n.
In the rest of this section we assume that the standard inner product is speciﬁed
on the space Cn. The following theorem describes how perturbations of a Hermitian
matrix inﬂuence its eigenspaces.
Theorem 7.2 Let A, B be Hermitian matrices of order n, and let
λ1(A) ≥λ2(A) ≥· · · ≥λn(A),
λ1(B) ≥λ2(B) ≥· · · ≥λn(B)
be their eigenvalues. Let k ∈[1, n] be a given integer and let λk(A) have multiplicityr
such that λk−1(A) > λk(A) > λk+r(A).1 Let Lk be the eigenspace (of dimension r)
of the matrix A corresponding to λk(A), and let Mk be the subspace of dimension r in
the space Cn spanned by the orthogonal eigenvectors of the matrix B corresponding
to its eigenvalues λk(B), λk+1(B), …, λk+r−1(B). Let
1For k = 1 and k = n, these inequalities are modiﬁed in the obvious way.

7.1 Perturbations in the Symmetric Eigenvalue Problem
233
gapk(A) = min(λk−1(A) −λk(A), λk(A) −λk+r(A)),
∥A −B∥2 < gapk(A)
2
.
(7.5)
Then
ϑ(Lk, Mk) ≤
∥A −B∥2
gapk(A) −∥A −B∥2
< 1.
(7.6)
Proof Let x ∈Lk, ∥x∥2 = 1. If we write x in the form of the orthogonal decompo-
sition x = PMk x + y, where y ∈M⊥
k , then ∥x −PMk x∥2 = ∥y∥2. Evidently,
|((A −B)x, y)| ≤∥A −B∥2∥y∥2.
(7.7)
On the other hand, by the deﬁnition of the vector x, we have
((A −B)x, y) = λk(A)(x, y) −(Bx, y).
Note that (x, y) = (y, y), (Bx, y) = (x, By) = (y, By). We have used that B = B∗
and that M⊥
k is an invariant subspace of the operator B. Therefore,
((A −B)x, y) = λk(A)(y, y) −(By, y).
(7.8)
For y ̸= 0, we get
λk(A)(y, y) −(By, y) =

λk(A) −(By, y)
(y, y)

∥y∥2
2.
(7.9)
By the deﬁnition of the subspace M⊥
k ,
(By, y)
(y, y) ≥λk−1(B) or (By, y)
(y, y) ≤λk+r(B)
(see Lemma 4.7, p. 148). Now using inequality (7.2) and condition (7.5), we obtain
				λk(A) −(By, y)
(y, y)
				 ≥gapk(A) −∥A −B∥2.
(7.10)
Obviously, it follows from (7.5), (7.7)–(7.10) that
∥y∥2 ≤
∥A −B∥2
gapk(A) −∥A −B∥2
< 1.
(7.11)
Thus (see also Theorem 6.11, p. 217) we have the inequalities (7.6).
□
Sometimes the next corollary gives a more useful estimate.

234
7
Elements of Perturbation Theory
Corollary 7.1 Let all the conditions of Theorem 7.2 hold, and let the multiplicity of
the eigenvalue λk(A) be one. Then
ϑ(Lk, Mk)

1 −ϑ2(Lk, Mk) ≤∥A −B∥2
gapk(A) .
(7.12)
Proof Let x ∈Mk, ∥x∥2 = 1. Writing x in the form of the orthogonal decomposi-
tion x = ˜x + y, where ˜x = PLk x, y ∈L⊥
k , and using estimate (7.11), we get
∥˜x∥2 =

1 −∥y∥2
2 > 0.
(7.13)
If we put B = A + E, then we can write
(A + E)(˜x + y) = λk(B)(˜x + y).
(7.14)
Obviously, A ˜x = λk(A)˜x. Subtracting these equalities term by term, by elementary
calculations we obtain
(A −λk(A)I)y = (ηI −E)x,
(7.15)
where η = λk(B) −λk(A). Now we calculate the inner product of both sides of
equality (7.14) with ˜x. We note that (A ˜x, ˜x) = λk(A)(˜x, ˜x) and also that (y, ˜x) = 0
and (Ay, ˜x) = 0, since y, Ay ∈L⊥
k . As a result we get
η = (Ex, ˜x)
∥˜x∥2
2
.
(7.16)
Computing the inner product of both sides of equality (7.15) with y and using (7.16),
by elementary calculations we get
((A −λk(A)I)y, y) =

Ex, (y, y)
∥˜x∥2
2
˜x −y

.
(7.17)
As we have seen in the proof of Theorem 7.2,
|((A −λk(A)I)y, y)| ≥gapk(A)∥y∥2
2.
(7.18)
The vectors ˜x and y are orthogonal; hence

(y, y)
∥˜x∥2
2
˜x −y

2
=
|(y, y)|2
∥˜x∥2
2
+ ∥y∥2
2
1/2
=
∥y∥2
2
∥˜x∥2
2
+ 1
1/2
∥y∥2 = ∥y∥2
∥˜x∥2
.
(7.19)
Combining (7.17)–(7.19) and (7.13), we ﬁnally get (7.12).
□

7.1 Perturbations in the Symmetric Eigenvalue Problem
235
Remark 7.1 For one-dimensional subspaces Lk and Mk of a real space, we obtain
ϑ(Lk, Mk) = sin α, where α is the angle between Lk and Mk (prove it!). Therefore,
estimate (7.12) is usually written as follows:
1
2 sin 2α ≤∥A −B∥2
gapk(A) .
We may consider all estimates obtained in this section as a priori estimates.
Using them, we can estimate the perturbations of eigenvalues and eigenvectors by
known perturbations of the original matrix. In some situations, so-called a posteriori
estimates are useful. They give information about errors using results of calcula-
tions already performed. Let now a normalized vector x be an approximation of an
eigenvector of the matrix A, and let α be an approximation of the corresponding
eigenvalue. Then the accuracy of calculations can be characterized by the residual
vector r(x, α) = Ax −αx. If the matrix A is Hermitian, then it is easy to see that
min
α∈R ∥r(x, α)∥2 = ∥Ax −ρ(x)x∥2,
where ρ(x) = (Ax, x).2 This means that the number (Ax, x) is in a certain sense
the best approximation of the eigenvalue of the Hermitian matrix A if we know the
approximation x of the corresponding eigenvector.
The next theorem shows that the residual r(x) = Ax −ρ(x)x can actually be
used to estimate the accuracy of solutions of spectral problems.
Theorem 7.3 Let A ∈Mn be a Hermitian matrix, λ = λi a simple eigenvalue
of the matrix A, and u the corresponding normalized eigenvector. Let x ∈Cn,
∥x∥2 = 1, ρ = (Ax, x) ̸= λ, |ρ −λ| < gapk(A), r = Ax −ρx. Let L and M be
the one-dimensional subspaces in Cn spanned by u and x, respectively,
γ =
min
μ∈σ(A), μ̸=λ |ρ −μ|.
Then
ϑ(L, M) ≤∥r∥2
γ
,
|λ −ρ| ≤∥r∥2
2
γ
.
(7.20)
Proof Write the vector x in the form of an orthogonal decomposition x = ˜x + y,
where ˜x ∈L, y ∈L⊥. Then r = (λ −ρ)˜x + Ay −ρy. Since the vector Ay belongs
to the subspace L⊥, we have
∥r∥2
2 = (λ −ρ)2∥˜x∥2
2 + ∥Ay −ρy∥2
2.
(7.21)
It is easy to see that
∥Ay −ρy∥2
2 ≥γ 2∥y∥2
2.
(7.22)
2Hint: for a given x, write ∥r(x, α)∥2
2 as a quadratic trinomial in α.

236
7
Elements of Perturbation Theory
It follows immediately from (7.21), (7.22) that the ﬁrst estimate in (7.20) is valid.
Further, using the deﬁnition of r, we obtain (r, x) = 0, or in detail,
(λ −ρ)∥˜x∥2
2 + ((A −ρI)w, w)∥y∥2
2 = 0,
(7.23)
where w = ∥y∥−1
2 y, ∥w∥2 = 1. Using the equality ∥˜x∥2
2 = 1 −∥y∥2
2 and (7.23), by
elementary calculations we get
∥y∥2
2 =
ρ −λ
((A −λ)w, w).
(7.24)
It follows from (7.23) that ∥˜x∥2
2 = ((A −ρI)w, w)∥y∥2
2/(ρ −λ). Combining this
with (7.21), after elementary calculations we see that
∥r∥2
2 = ((A −ρI)w, (A −λI)w)∥y∥2
2.
(7.25)
Equalities (7.25) and (7.24) show that
∥r∥2
2 = |ρ −λ||((A −ρI)w, (A −λI)w)|
|((A −λI)w, w)|
.
(7.26)
If we represent here w in the form of an expansion with respect to the orthonormal
set of eigenvectors of the matrix A, then we obtain
∥r∥2
2 = |ρ −λ|
		 
j̸=i
(λ j −ρ)(λ j −λi)|c j|2		
		 
j̸=i
(λ j −λi)|c j|2		
,
(7.27)
where c j, j = 1, 2, . . . , n, j ̸= i, are the coefﬁcients of the expansion. Obviously, it
follows from the assumptions of the theorem that for j ̸= i, all the numbers (λ j −
ρ)(λ j −λi) are positive. Therefore,
			

j̸=i
(λ j −ρ)(λ j −λi)|c j|2			 ≥γ
			

j̸=i
(λ j −λi)|c j|2			,
i.e., the second estimate in (7.20) is also valid.
□
7.2
Perturbations of Singular Values and Singular Vectors
The next theorem follows immediately from Lemma 5.1, p. 174, estimate (7.1),
p. 231, and inequality (5.13), p. 166.

7.2 Perturbations of Singular Values and Singular Vectors
237
Theorem 7.4 Let A, B ∈Mm,n bearbitrarymatrices,q = min(m, n),andletσ1(A),
σ2(A), …, σq(A), σ1(B), σ2(B), …, σq(B) be their singular values (here we also
include zeros for uniformity of notation). Then
max
1≤k≤q |σk(A) −σk(B)| ≤max
1≤k≤q σk(A −B),
(7.28)
max
1≤k≤n |σk(A) −σk(B)| ≤
⎛
⎝
m,n

i, j=1
|ai j −bi j|2
⎞
⎠
1/2
.
(7.29)
The following theorem is analogous to Theorem 7.2, p. 232.
Theorem 7.5 Let A, B ∈Mm,n bearbitrarymatrices,q = min(m, n),andletσ1(A),
σ2(A), …, σq(A), σ1(B), σ2(B), …, σq(B) be their singular values ordered by non-
increasing values. Let σk(A) be a positive singular value of the matrix A of multi-
plicity r. Let Lu,k be the subspace of Cn spanned by the right singular vectors of
the matrix A corresponding to σk(A) and let Lv,k be the subspace spanned by the
left singular vectors of A corresponding to σk(A). Denote by Mu,k the subspace
spanned by the right singular vectors of the matrix B corresponding to the singular
values σk(B), σk+1(B), …, σk+r−1(B), and by Mv,k the subspace spanned by the left
singular vectors of B corresponding to the same singular values.3 Let
gapk(A) = min(σk−1(A) −σk(A), σk(A) −σk+r(A)),
∥A −B∥2 < gapk(A)
2
.
Then
max(ϑ(Lu,k, Mu,k), ϑ(Lv,k, Mv,k)) ≤
∥A −B∥2
gapk(A) −∥A −B∥2
< 1.
The proof of this theorem is left to the reader.
7.3
Perturbations of Characteristic Values of Arbitrary
Matrices
Let A = {ai j}n
i, j=1 be a square matrix, and let
Ri(A) =

1≤j≤n, j̸=i
|ai j| for all i = 1, 2, . . . , n,
3See the footnote on p. 232.

238
7
Elements of Perturbation Theory
C j(A) =

1≤i≤n, i̸= j
|ai j| for all
j = 1, 2, . . . , n.
Theorem 7.6 (Gershgorin4). Let A be an arbitrary matrix of order n. Then all the
characteristic values of A are located in the union of n disks
G R
i = {z ∈C : |z −aii| ≤Ri(A)},
i = 1, 2, . . . , n.
(7.30)
Proof Let (λ, x) be an eigenpair of the matrix A, and let xi be the element of x that
has the largest absolute value. Evidently, xi ̸= 0. Using the deﬁnition of an eigenpair,
we get
(aii −λ)xi = −

1≤j≤n, j̸=i
ai jx j.
Therefore,|aii −λ||xi|≤Ri(A)|xi|, and|aii −λ|≤Ri(A). Thus every characteristic
value of the matrix A belongs to one of the disks G R
i ,i =1, 2, . . . , n.
□
This theorem is often called the Gershgorin disk theorem. Since A and AT have
the same eigenvalues, they all are located in the union of n disks
GC
i = {z ∈C : |z −aii| ≤Ci(A)},
i = 1, 2, . . . , n.
(7.31)
This is the so-called the column sum version of the Gershgorin disk theorem.
Theorem 7.6 can be interpreted as a theorem on perturbations of a diagonal matrix
D = diag(a11, a22, . . . , ann). It shows that if the nondiagonal elements of the matrix
A are small, then its characteristic values are not very different from the characteristic
values of the matrix D.
The next two theorems are called the Bauer–Fike theorems.5
Theorem 7.7 Suppose that for the square matrix A = {ai j}n
i, j=1 there exists a non-
singular matrix V such that
V −1AV = 
 = diag(λ1, λ2, . . . , λn).
(7.32)
Let B = {bi j}n
i, j=1 be an arbitrary square matrix. Then all characteristic values of
the matrix A + B are located in the union of n disks
Gi = {z ∈C : |z −λi| ≤∥B∥∥V ∥∥V −1∥},
i = 1, 2, . . . , n.
(7.33)
Here ∥· ∥is a matrix norm that is induced by any absolute vector norm.
4Semyon Aronovich Gershgorin (1901–1933) was a Soviet mathematician.
5Friedrich Ludwig Bauer (1924–2015) was a German mathematician. Charles Theodore Fike (born
1933) is an American mathematician.

7.3 Perturbations of Characteristic Values of Arbitrary Matrices
239
Proof If (λ, x) is an eigenpair of A + B, then (λI −
)V −1x = V −1BV V −1x,
whence (see p. 213) we get min 1≤i≤n|λ −λi|∥V −1x∥≤∥B∥∥V −1∥∥V ∥∥V −1x∥,
but we have V −1x ̸= 0. Therefore, min 1≤i≤n|λ −λi| ≤∥B∥∥V −1∥∥V ∥. Thus,
λ ∈ n
i=1Gi.
□
Theorem 7.8 Suppose that the conditions of Theorem 7.7 hold. Then all the char-
acteristic values of the matrix A + B are located in the union of n disks
Gi = {z ∈C : |z −λi| ≤nsi∥B∥2},
i = 1, 2, . . . , n,
(7.34)
where si = ∥ui∥2∥vi∥2/|(ui, vi)|, vi is the ith column of the matrix V , ui is the ith
column of the matrix U = (V −1)∗, and the inner product (·, ·) is the standard inner
product on the space Cn.
Remark 7.2 It is obvious that (λi, vi), i = 1, 2, . . . , n, are the eigenpairs of the
matrix A, and (¯λi, ui), i = 1, 2, . . . , n, are the eigenpairs of the matrix A∗. Each
number si for i = 1, 2, . . . , n is greater than or equal to one. The number si is called
the coefﬁcient of skewness of the corresponding eigenvector vi of the matrix A.
If the algebraic multiplicity of the characteristic value λi of the matrix A is equal
to one, then evidently, the algebraic multiplicity of the characteristic value ¯λi of the
matrix A∗is equal to one, too. The eigenspaces corresponding to these eigenval-
ues are one-dimensional, and hence the corresponding coefﬁcient of skewness si is
uniquely determined.
Proof of Theorem 7.8. The matrices A + B and 
 + V −1BV = 
 + B, where we
set B = U ∗BV , have the same characteristic values. Using the column sum ver-
sion of the Gershgorin disk theorem, we see that all the characteristic values of the
matrix 
 + B are located in the union of n disks
G′
i = {z ∈C : |z −λi −˜bii| ≤Ci(B)},
i = 1, 2, . . . , n.
Note that |z −λi −˜bii| ≥|z −λi| −|˜bii|, Ci(B) + |˜bii| = ∥˜bi∥1, where as usual,
by ˜bi we denote the ith column of the matrix B. Therefore, all the characteristic
values of the matrix A + B are located in the union of n disks
G′′
k = {z ∈C : |z −λk| ≤∥˜bk∥1},
k = 1, 2, . . . , n.
Let us estimate ∥˜bk∥1. Consider the vectors tk ∈Cn with the following components6:
t jk =

˜b jk/|˜b jk|, ˜b jk ̸= 0,
0, ˜b jk = 0.
Trivially, ∥˜bk∥1 = (Bik, tk), where ik is a column of the identity matrix. Using the
Cauchy–Schwarz inequality, we then obtain
6By ˜b jk we denote the jth element of the column ˜bk.

240
7
Elements of Perturbation Theory
∥˜bk∥1 = (BVik,Utk) ≤∥B∥2∥U∥2∥vk∥2∥tk∥2.
(7.35)
It is easy to check that ∥tk∥2 ≤√n. Further, using estimate (6.29), p. 223, we
obtain ∥U∥2 ≤
 n

k=1
∥uk∥2
2
1/2
. Obviously, each column of the matrix U is uniquely
determined up to a nonzero scalar factor. Therefore, we can normalize them to
get ∥uk∥2 = 1 for all k = 1, 2, . . . , n. Then evidently, the columns of the matrix V
must be normalized so that (vk, uk) = 1 for all k = 1, 2, . . . , n. In this case,
we see that ∥vk∥2 = ∥vk∥2∥uk∥2/|(uk, vk)| = sk. Thus, using (7.35), we ﬁnally
obtain ∥˜bk∥1 ≤nsk∥B∥2.
□
The next theorem helps to compare estimate (7.33) with (7.34).
Theorem 7.9 For every normalization of the columns of the matrix V , the following
inequality holds:
∥V ∥2∥V −1∥2 ≥max
1≤k≤n sk.
(7.36)
The columns of the matrix V can be normalized so that
∥V ∥2∥V −1∥2 ≤
n

k=1
sk.
(7.37)
Proof Clearly, we have Vik =vk, k =1, 2, . . . , n, and ∥V ∥2 =sup ∥x∥2=1∥V x∥2 ≥
∥vk∥2. Similarly, we see that ∥V −1∥2 = ∥U∥2 ≥∥uk∥2. Therefore, inequality (7.36)
holds. Now we normalize the columns of the matrix V so that ∥vk∥2 = s1/2
k
. Then,
using the equality (vk, uk) = 1, we get ∥uk∥2 = s1/2
k
, k = 1, 2, . . . , n. Obviously,
this implies that ∥V −1∥E = ∥V ∥E =
 n

k=1
sk
1/2
. Using inequality (6.29), p. 223,
we obtain esstimate (7.37).
□
Remark 7.3 The matrix V that has columns that form a basis of the space Cn
that consists of the eigenvectors of the matrix A is not uniquely determined. For
every matrix V , we have ∥V ∥∥V −1∥≥1. This inequality becomes an equality if,
for example, the matrix V is unitary and the norm of V is spectral. By Theo-
rem 4.41, p. 142, it follows that a matrix is unitarily similar to a diagonal matrix
if and only if that matrix is normal. Thus, if A is a normal matrix, λi, i = 1, 2, . . . , n,
are all the characteristic values of A, and B is an arbitrary square matrix, then
all the characteristic values of the matrix A + B are located in the union of n
disks Gi = {z ∈C : |z −λi| ≤∥B∥2}, i = 1, 2, . . . , n.

7.4 Perturbations and the Invertibility of a Matrix
241
7.4
Perturbations and the Invertibility of a Matrix
Let A ∈Mn be an invertible matrix, i.e., det A ̸= 0, and let B ∈Mn. The following
question arises: what are sufﬁcient conditions on the matrix B so that the matrix
A + B has an inverse? Since A + B = A(I + A−1B), we see that the matrix A + B
is invertible if and only if the spectrum of the matrix A−1B does not include −1.
Therefore, we have the following practically important sufﬁcient conditions of the
invertibility of the matrix A + B.
1. The matrix A + B is invertible if A−1B is convergent, i.e., ρ(A−1B) < 1.
2. The matrix A + B is invertible if ∥A−1B∥< 1.
3. The matrix A + B is invertible if ∥A−1∥∥B∥< 1.
Here and below in this section, the norm of a matrix is any consistent norm. The
third condition usually is written in the form
cond(A)∥B∥
∥A∥< 1,
(7.38)
where cond(A) = ∥A−1∥∥A∥. This number is called the condition number of the
matrix A (compare it with the deﬁnition in Section5.1.1, p. 167). We can interpret
condition (7.38) in the following way: the matrix A + B is invertible if the relative
perturbation of the matrix A, i.e., ∥B∥/∥A∥, is small compared with its condition
number.
Example 7.1 Let A = {ai j}n
i, j=1 be a square matrix. The matrix A is said to be row
diagonally dominant if7
|aii| > Ri(A) for all i = 1, 2, . . . , n.
(7.39)
It is said to be column diagonally dominant if
|aii| > Ci(A) for all i = 1, 2, . . . , n.
(7.40)
Let us prove that if a matrix A is row diagonally dominant, then it is nonsingular.
Put D = diag(a11, a22, . . . , ann). Using condition (7.39), we see that the matrix D
is nonsingular. Writing A in the form A = D + (A −D) and using condition (7.39)
one more time, we get ∥D−1(A −D)∥∞< 1. Therefore, condition 2 holds, and
the matrix A is nonsingular. Since det(A) = det(AT ), we see that every column
diagonally dominant matrix is nonsingular as well.
The reader can easily prove that if condition (7.39) or (7.40) holds, then all the
leading principal minors of the matrix A are nonzero.
7See the notation in Section7.3.

242
7
Elements of Perturbation Theory
Using Example 7.1, the reader can easily prove the Gershgorin disk theorem, and
conversely, using the Gershgorin disk theorem, the reader can prove that if a matrix A
is row diagonally dominant, then it is nonsingular.
Theorem 7.10 Let matrices A and ˜A = A + B be invertible. Then
∥A−1 −˜A−1∥
∥˜A−1∥
≤∥A−1B∥.
(7.41)
If ∥A−1B∥< 1, then
∥˜A−1∥≤
∥A−1∥
1 −∥A−1B∥,
(7.42)
∥A−1 −˜A−1∥
∥A−1∥
≤
∥A−1B∥
1 −∥A−1B∥.
(7.43)
Proof By assumption, I = (A + B) ˜A−1. Therefore, A−1 = (I + A−1B) ˜A−1. This
implies that A−1 −˜A−1 = A−1B ˜A−1, whence obviously, we get (7.41). Further, we
have ˜A−1 = A−1 −A−1B ˜A−1, and ∥˜A−1∥≤∥A−1∥+ ∥A−1B∥∥˜A−1∥. Hence we
have estimate (7.42). Finally, estimate (7.43) is an obvious consequence of esti-
mates (7.41), (7.42).
□
The next corollary follows immediately from Theorem 7.10.
Corollary 7.2 Let matrices A and ˜A = A + B be invertible. Then
∥A−1 −˜A−1∥
∥˜A−1∥
≤cond(A)∥B∥
∥A∥.
(7.44)
If cond(A)(∥B∥/∥A∥) < 1, then
∥˜A−1∥≤
∥A−1∥
1 −cond(A)(∥B∥/∥A∥),
(7.45)
∥A−1 −˜A−1∥
∥A−1∥
≤
cond(A)(∥B∥/∥A∥)
1 −cond(A)(∥B∥/∥A∥).
(7.46)
The following theorem shows that the “distance” between a nonsingular matrix A
and the “nearest” singular matrix is characterized by the number 1/cond(A).
Theorem 7.11 Let A be an invertible matrix and A + B a singular matrix. Then
∥B∥/∥A∥≥1/cond(A).
(7.47)
If the matrix norm is induced by a vector norm, then we can ﬁnd a matrix B such
that

7.4 Perturbations and the Invertibility of a Matrix
243
∥B∥/∥A∥= 1/cond(A)
(7.48)
and the matrix A + B is singular.
Proof As we have seen, if a matrix A is invertible and a matrix A + B is singular, then
the spectrum of the matrix A−1B contains the number −1. Therefore, ρ(A−1B) ≥
1, but ρ(A−1B) ≤∥A−1B∥≤∥A−1∥∥B∥, i.e., we have ∥B∥≥1/∥A−1∥. The last
inequality is equivalent to (7.47). Now we prove the second part of the theorem. It
follows from the deﬁnition of the induced matrix norm that there exists a vector x
such that ∥x∥= 1, ∥A−1x∥= ∥A−1∥. Put y = ∥A−1∥−1A−1x. Then ∥y∥= 1, Ay =
∥A−1∥−1x. By Corollary 6.1, p. 216, there exists a linear functional f on the space
Cn such that ∥f ∥= sup v∈Cn, ∥v∥=1| f (v)| = 1. We deﬁne the matrix B by the action
of this matrix on vectors, using the following relationship: Bv = −( f (v)/∥A−1∥)x
for all v ∈Cn. Clearly, By = −∥A−1∥−1x, whence (A + B)y = 0, and therefore,
det(A + B) = 0. Moreover,
∥B∥=
sup
v∈Cn, ∥v∥=1
∥Bv∥= ∥A−1∥−1
sup
v∈Cn, ∥v∥=1
| f (v)| = ∥A−1∥−1.
The last equality is equivalent to (7.48).
□
7.5
The Stability of Systems of Linear Equations
In this section we assume that matrix norms are consistent with vector norms. The
next theorem establishes the connection between the relative perturbations of the
matrix and the right-hand side of the system with the relative perturbations of its
solution. The main role in the estimates obtained below is played by the condition
number of the matrix of the system.
Theorem 7.12 Let A be an invertible matrix and let B be a matrix such that
∥A−1B∥< 1. Let x be the solution of the system of equations
Ax = y,
(7.49)
and let ˜x be the solution of the system of equations
˜A ˜x = y + b,
˜A = A + B.
(7.50)
Then
∥x −˜x∥
∥x∥
≤
cond(A)
1 −∥A−1B∥
∥b∥
∥y∥+ ∥B∥
∥A∥

.
(7.51)
If we assume additionally that ∥A−1∥∥B∥< 1, then

244
7
Elements of Perturbation Theory
∥x −˜x∥
∥x∥
≤
cond(A)
1 −cond(A)(∥B∥/∥A∥)
∥b∥
∥y∥+ ∥B∥
∥A∥

.
(7.52)
Proof By assumption, the inverse matrices A−1 and ˜A−1 exist. Therefore, x = A−1y
and ˜x = ˜A−1(y + b). Hence ˜x −x = ˜A−1b + ( ˜A−1 −A−1)y, and
∥x −˜x∥≤∥˜A−1∥∥b∥+ ∥˜A−1 −A−1∥∥y∥.
Therefore, using (7.42), (7.43), and the inequality ∥y∥≤∥A∥∥x∥, by elementary
calculations we get (7.51). We note that estimate (7.52) is an obvious consequence
of (7.51).
□
In many situations, an error estimate based on the residual of the approxi-
mate solution is especially useful. Now we introduce a number that we shall use
for this estimate. Let A be an invertible matrix and let x ̸= 0, Ax = y. Put η =
∥A∥∥x∥/∥y∥. Obviously, η ≥1, and since ∥x∥≤∥A−1∥∥y∥, we see that η ≤
∥A∥∥A−1∥= cond(A). For a vector ˜x ∈Cn, we put r = A ˜x −y. Then ˜x −x =
A−1r, and
∥x −˜x∥≤∥A−1∥∥r∥.
(7.53)
Therefore,
∥x −˜x∥
∥x∥
≤cond(A)
η
∥r∥
∥y∥,
(7.54)
and as a consequence we get
∥x −˜x∥
∥x∥
≤cond(A) ∥r∥
∥y∥.
Estimate (7.54) shows that the relative error is estimated better by the relative
residual of the approximate solution as the number η better approximates cond(A).
Let ˜x be an approximate solution of the system of equations Ax = y. In some
cases, for example in backward error analysis, it is useful to represent the vector ˜x
in the form of the exact solution of the system with a perturbed matrix:
(A + B)˜x = y.
(7.55)
It is natural to seek a matrix B with minimal norm (induced by a norm of vectors).
The possibility of such a choice of the matrix B is justiﬁed by the next theorem.
Theorem 7.13 (Rigal–Gaches). Let ˜x ∈Cn, ˜x ̸= 0, r = A ˜x −y. There exists a
matrix B such that Eq.(7.55) holds and ∥B∥= ∥r∥/∥˜x∥. If we assume additionally
that there exists a matrix D ̸= B such that (A + D)˜x = y, then ∥D∥≥∥B∥.
Proof To justify the last assertion it is enough to note that D ˜x =−r, and therefore,
∥D ˜x∥= ∥r∥; hence ∥D∥≥∥r∥/∥˜x∥. Let us deﬁne the matrix B by the relationship

7.5 The Stability of Systems of Linear Equations
245
Bv =−( f (v)/∥˜x∥)r for all v ∈Cn, where f is a linear functional on the space Cn
that satisﬁes the following conditions: f (˜x) = ∥˜x∥and ∥f ∥= 1.8 Then we see that
Eq.(7.55) holds, and ∥B∥= ∥f ∥∥r∥/∥˜x∥= ∥r∥/∥˜x∥.
□
In all previous estimates we assumed that the values of perturbations of the matrix
and the right-hand side of the system were known in the sense of some norms.
However, often it is more natural to deﬁne componentwise perturbations. Namely,
now we assume that for a given ε > 0, we have
|B| ≤ε|A|,
|b| ≤ε|y|.
(7.56)
Here and below in this section the symbol | · | indicates a matrix or a vector that
consists of the absolute values of its components. Inequalities (7.56) are to be under-
stood componentwise. Thus estimates (7.56) mean that the relative perturbation of
each element of the matrix and of the right-hand side of the system is less than or
equal to ε.
Theorem 7.14 (Bauer–Skeel9). Let x be the solution of system (7.49) and let ˜x be
the solution of system (7.50). We assume that the matrix A is nonsingular, condi-
tions (7.56) hold, and
ε∥|A−1||A|∥< 1.
(7.57)
Then
∥x −˜x∥≤ε∥|A−1|(|A||x| + |y|)∥
1 −ε∥|A−1||A|∥
.
(7.58)
Here the vector norm is any monotone norm, and the matrix norm is consistent with
the vector norm.
Proof Using Eqs.(7.49), (7.50), we get ˜x −x = A−1(Bx + b + B(˜x −x)). There-
fore, |˜x −x| ≤|A−1|(|B||x| + |b| + |B||˜x −x|), hence, using (7.56) and taking into
account the assumed agreements for the vector and matrix norms, we obtain
∥˜x −x∥≤ε∥∥|A−1|(|A||x| + |y|)∥+ ε∥|A−1||A|∥∥˜x −x∥.
Combining the last inequality with (7.57), we get (7.58).
□
It is useful to note that if the right-hand side of the system is known exactly,
i.e., b = 0, then instead of (7.58), we get
∥˜x −x∥
∥x∥
≤
ε∥|A−1||A|∥
1 −ε∥|A−1||A|∥.
This estimate shows the dependence of the relative error of the solution on the relative
perturbation of the matrix of the system. For this reason, the number
8See the proof of Theorem 7.11.
9Robert D. Skeel (born 1947) is an American mathematician.

246
7
Elements of Perturbation Theory
κBS(A) = ∥|A−1||A|∥
is called the relative condition number of the matrix A or the Bauer–Skeel condition
number.
It is easy to see that under the assumed agreements for vector and matrix
norms, κBS(A) ≥1 for every matrix A. For every diagonal matrix, κBS = 1. Thus
diagonal systems of equations are ideally conditioned with respect to the perturba-
tions of the matrix.
7.6
Perturbations in the Linear Least Squares Problem
In this section we investigate the stability of a pseudosolution with respect to per-
turbations of a matrix and the right-hand side of the system. The vector norm in
this section is Euclidean. The matrix norm is induced by this norm, i.e., we use the
spectral matrix norm.
Lemma 7.1 Let A, B ∈Mm,n,rank(A) = rank(B) = r,η = ∥A+∥∥A −B∥< 1.10
Then
∥B+∥≤
1
1 −η∥A+∥.
(7.59)
Proof It follows from (7.28), p. 236, and (6.27), p. 221, that
σr(B) −σr(A) ≥−∥A −B∥.
Using (6.28), p. 223, we can write the last inequality in the form
1
∥B+∥−
1
∥A+∥≥−∥A −B∥,
whence, by elementary calculations, we get (7.59).
□
Lemma 7.2 Let A, B ∈Mm,n, rank(A) = rank(B), PA = AA+, PB = BB+. Then
∥PA(I −PB)∥= ∥PB(I −PA)∥≤∥A −B∥min(∥A+∥, ∥B+∥).
(7.60)
Proof By construction, PA, PB are the orthogonal projectors deﬁned on Cm (see
Section5.1.3, p. 169). Therefore, the equality ∥PA(I −PB)∥= ∥PB(I −PA)∥fol-
lows from the results of Section6.5, p. 226. In Section6.5, we have seen also that
the following equality holds: ∥PB(I −PA)∥= ∥(I −PA)PB∥. Now we note that
(I −PA)PB = (I −AA+)PB = (I −AA+)(A + B −A)B+,
10Let us recall that A+ is the pseudoinverse of A (see Section4.3.4, p. 137, and Section5.1.3,
p. 168).

7.6 Perturbations in the Linear Least Squares Problem
247
but (I −AA+)A = 0 (see Property 5 on p. 169). Hence
∥PB(I −PA)∥= ∥(I −PA)PB∥= ∥(I −PA)(B −A)B+∥
≤∥(B −A)B+∥≤∥(B −A)∥∥B+∥.
(7.61)
Analogously, ∥PA(I −PB)∥≤∥(B −A)∥∥A+∥.
□
Theorem 7.15 (Wedin11). Let A, ˜A ∈Mm,n, m ≥n, be matrices of full rank. Let x
be the normal pseudosolution of the system of equations (7.49), and let ˜x be the
normal pseudosolution of system (7.50). Let r = y −Ax and let ˜r = ˜y −˜A ˜x be the
corresponding residuals. Suppose that ∥A+∥∥B∥< 1. Then
∥x −˜x∥
∥x∥
≤
κ2(A)
1 −κ2(A)(∥B∥/∥A∥)
×
∥B∥
∥A∥

1 + κ2(A)
∥r∥
∥A∥∥x∥

+ ∥b∥
∥y∥

1 +
∥r∥
∥A∥∥x∥

,
(7.62)
∥r −˜r∥
∥y∥
≤
∥b∥
∥y∥+ 2κ2(A)∥B∥
∥A∥

,
(7.63)
where κ2(A) = ∥A+∥∥A∥.
Proof By the deﬁnition of the pseudoinverse operator, we have
˜x −x = ˜A+(y + b) −x = ˜A+(r + Ax + b) −x = ˜A+(r + ˜Ax + b −Bx) −x.
Since by hypothesis, rank( ˜A) = n, we get ˜A+ ˜A = I (see Property 7 of the pseudoin-
verse operator, p. 169). Therefore,
˜x −x = ˜A+(r + b −Bx).
(7.64)
Now we note that by Property 6, p. 169, we have ˜A+r = ˜A+ ˜A ˜A+r = ˜A+P ˜Ar. We
note also that
PAr = AA+(y −Ax) = A(A+y) −AA+Ax = Ax −Ax = 0,
(7.65)
i.e., ˜A+r = ˜A+P ˜A(I −PA)r, whence by Lemmas 7.1 and 7.2, we see that
∥˜A+r∥≤
∥A+∥2
1 −∥A+∥∥B∥∥B∥∥r∥=
∥A+∥2∥A∥2
1 −∥A+∥∥A∥(∥B∥/∥A∥)
∥B∥∥r∥
∥A∥2∥x∥∥x∥.
(7.66)
11Per-Åke Wedin (born 1938) is a Swedish mathematician.

248
7
Elements of Perturbation Theory
Analogously, using the evident inequality ∥y∥≤∥r∥+ ∥A∥∥x∥, we get
∥˜A+(b −Bx)∥≤∥˜A+∥(∥b∥+ ∥B∥∥x∥) ≤
∥A+∥
1 −∥A+∥∥B∥
∥b∥
∥x∥+ ∥B∥

∥x∥
≤
∥A+∥∥A∥
1 −∥A+∥∥A∥(∥B∥/∥A∥)

∥b∥∥r∥
∥y∥∥A∥∥x∥+ ∥b∥
∥y∥+ ∥B∥
∥A∥

∥x∥.
(7.67)
Combining (7.66), (7.67), and (7.64), we obtain (7.62). Let us estimate r −˜r. Using
the deﬁnitions of r and ˜r, we get
˜r −r = y + b −(A + B)˜x −y + Ax = b + ˜A(x −˜x) −Bx.
Moreover, we have the inequality ˜A(x −˜x) = −˜A ˜A+(r −Bx + b). Indeed,
˜A ˜A+(r −Bx + b) = ˜A ˜A+(y + b −˜Ax) = ˜A ˜A+ ˜y −˜A ˜A+ ˜Ax = ˜A ˜x −˜Ax.
Therefore, ˜r −r = (I −˜A ˜A+)(b −Bx) −˜A ˜A+r. Since I −˜A ˜A+ is a projector,
we get ∥r −˜r∥≤∥b −Bx∥+ ∥˜A ˜A+r∥. Recall that r = r −PAr. Hence, we have
∥˜A ˜A+r∥≤∥P ˜A(I −PA)∥∥r∥, whence, using Lemma 7.2, we obtain the estimate
∥˜A ˜A+r∥≤∥A+∥∥B∥∥r∥. Thus, ∥r −˜r∥≤∥b∥+ ∥B∥∥x∥+ ∥A+∥∥B∥∥r∥. Now
we note that x = A+y, ∥r∥= min v∈Cn∥y −Av∥≤∥y∥. Finally, using elementary
calculations, we get (7.63).
□
Remark 7.4 Inequalities (7.62), (7.63) show that in the estimates of the perturbations
of the linear least squares problem, the number κ2(A) plays an important role. It is
called the condition number of the linear least squares problem. Note also that if
the system of equations (7.49) is solvable, then r = 0, and the estimate (7.62) is
transformed to an estimate of the form (7.52). Clearly, if A is a square nonsingular
matrix, then we get κ2(A) = cond2(A).

Chapter 8
Solving Systems of Linear Equations
In this chapter we present algorithms and error analysis of numerical methods for
solving linear systems Ax = b with nonsingular square matrices. Here we present
only direct methods. They are called direct because in the absence of rounding errors
theywouldgivetheexactsolutionof Ax = b afteraﬁnitenumberofsteps.Section8.1
presents Gaussian elimination algorithms. Section8.2 analyzes their rounding errors
and presents practical error bounds. Section8.3 shows how to improve the accu-
racy of the computed solution of Ax = b through an iterative reﬁnement procedure.
In Section8.4 we discuss the basic special systems of linear equations (with sym-
metric positive deﬁnite matrices, symmetric indeﬁnite matrices, band matrices) and
numerical methods of their solution.
8.1
Algorithms for Gaussian Elimination
In this section we consider numerical algorithms for solving systems of linear equa-
tions with nonsingular matrices based on Gaussian elimination. Gaussian elimination
was already discussed in Section1.2.5, p. 42.
8.1.1
LU Factorization with Pivoting
As we saw in Section1.2.5, p. 42, for every nonsingular matrix A of order n, by
Gaussian elimination with pivoting we can construct unit1 elementary lower triangu-
lar matrices Lk, k = 1, 2, . . . , n, permutation matrices Pk, k = 1, 2, . . . , n, and an
upper triangular matrix U such that
1All the diagonal entries of a unit triangular matrix are equal to one.
© Springer International Publishing AG 2017
L. Beilina et al., Numerical Linear Algebra: Theory and Applications,
DOI 10.1007/978-3-319-57304-5_8
249

250
8
Solving Systems of Linear Equations
A = P1L−1
1 P2L−1
2 · · · PnL−1
n U.
(8.1)
If we have obtained representation (8.1), then we can solve the system of linear
equations
Ax = b
(8.2)
for any right-hand side b by computing the vector
f = Ln Pn · · · L1P1b
(8.3)
and solving the system
Ux = f
(8.4)
with the triangular matrix U. The cost of computing the vector f and solving sys-
tem (8.4) is approximately 2n2 arithmetic operations, which is much cheaper than
constructing representation (8.1) (see Section1.2.5, p. 42).
Calculating the vector f can be performed by solving a system of linear equa-
tions with a triangular nonsingular matrix. To show this, let us analyze the matrix
P1L−1
1 P2L−1
2 · · · PnL−1
n . The matrix P2 differs from the identity matrix by a permuta-
tion of the second and ith columns, i ≥2 (see the description of Gaussian elimination
in Section1.2.5, p. 42). Therefore, the matrix L−1
1 P2 differs from L−1
1
by a permuta-
tion of the second and ith columns. Hence L−1
1 P2 = P2 ˆL−1
1 , where the matrix ˆL−1
1
differs from L−1
1
by a permutation of the second element and the ith element in the
ﬁrst column. Thus, we obviously get
P1L−1
1 P2L−1
2 · · · PnL−1
n
= P1P2 · · · Pn ˜L−1
1
˜L−1
2 · · · ˜L−1
n ,
where each matrix ˜L−1
i , i = 1, 2, . . . , n, can differ from the matrix L−1
i
only by
permutations of elements in column i.
Now we can write A = PLU, where P = P1P2 · · · Pn, and L = ˜L−1
1
˜L−1
2 · · · ˜L−1
n
is a unit lower triangular matrix. This factorization of A is called an LU factorization
with pivoting. If the matrices P, L, and U have been constructed, then we can solve
system (8.2) in the following way:
1. Permute elements of b to get ˜b = P−1b = Pn Pn−1 · · · P1b.
2. Solve Ly = ˜b with a lower triangular matrix.
3. Solve Ux = y with an upper triangular matrix.
A method for constructing the matrices P, L, U was actually described in
Section1.2.5, p. 42. It can be realized by the following algorithm.
Algorithm 8.1 LU factorization with pivoting: calculating a permutation matrix P,
a unit lower triangular matrix L, and a nonsingular upper triangular matrix U such
that LU = P A for a given nonsingular A.
let P = I, L = I, U = A
for i = 1 to n −1

8.1 Algorithms for Gaussian Elimination
251
ﬁnd m such that |U(m, i)| is the largest entry in |U(i : n, i)|
if m ̸= i
swap rows m and i in P
swap rows m and i in U
if i ≥2 swap elements L(m, 1 : i −1) and L(i, 1 : i −1)
end if
L(i + 1 : n, i) = U(i + 1 : n, i)/U(i, i)
U(i + 1 : n, i + 1 : n)=U(i + 1 : n, i + 1 : n) −L(i + 1 : n, i) U(i, i + 1 : n)
U(i + 1 : n, i) = 0
end for
Obviously, this algorithm can be improved. For example, in Algorithm 8.1 we
observe that when column i of A is used to compute the elements of column i of L,
then that column is not used again. Also, when row i of A is used to compute row i
of U, that row is not used again. This observation allows us to organize the storage
arrangement in Algorithm 8.1, overwriting L and U on A (see Question 8.2, p. 284).
Further, to save information on permutations, we can use only one vector with the
numbers mi, i = 1, 2, . . . , n, and so on.
The lu function in MATLAB® expresses a matrix A as the product of two
triangular matrices, one of them a permutation of a lower triangular matrix and the
other an upper triangular matrix. The function [L,U] = lu(A) returns an upper
triangular matrix inU and a permuted lower triangular matrix in L such that A = LU.
The return value L is a product of lower triangular and permutation matrices. The
function [L,U,P] = lu(A) returns an upper triangular matrix in U, a lower
triangular matrix L with unit main diagonal, and a permutation matrix P such that
P A = LU.
The next algorithm is called forward substitution. We use it to easily solve a given
system Lx = b with a unit lower triangular matrix L.
Algorithm 8.2 Forward substitution: solving Lx = b with a unit lower triangular
matrix L.
x(1) = b(1)
for i = 2 to n
x(i) = b(i) −L(i, 1 : (i −1)) x(1 : (i −1))
end for
The last algorithm is called backward substitution.2 Using this algorithm, we can
easily solve a given system Ux = b with an upper triangular matrix U.
Algorithm 8.3 Backward substitution: solving Ux = b with a nonsingular upper
triangular matrix U.
x(n) = b(n)/U(n, n)
for i = n −1 to 1
x(i) = (b(i) −U(i, (i + 1) : n) x((i + 1) : n))/U(i, i)
end for
2See also (1.119), (1.120), p. 45.

252
8
Solving Systems of Linear Equations
Note that in Algorithm 8.1 we apply permutations on the rows of the matrix A.
This process is called Gaussian elimination with partial pivoting (GEPP): swap rows
with numbers mi and i of the matrix A such that |A(mi, i)| will be the largest entry
in |A(i : n, i)|. In the case of Gaussian elimination with complete pivoting (GECP),
we swap rows mi and i as well as columns ki and i in the matrix A such that
|A(mi, ki)| will be the largest entry in |A(i : n, i : n)|. GEPP is the most common
way to implement Gaussian elimination in practice. GECP is more expensive. It is
almost never used in practice.
8.1.2
The Need for Pivoting
First of all, let us describe a class of matrices for which LU factorization can be done
without pivoting. In other words, in this case all the matrices Pi, i = 1, 2, . . . , n, can
be equal to the identity matrix.
Theorem 8.1 Let A be a given square matrix of order n. There exist a unique unit
lower triangular matrix L and a unique nonsingular upper triangular matrix U such
that A = LU if and only if the all leading principal submatrices of A are nonsingular.
Proof Necessity. The decomposition A = LU may also be written in terms of block
matrices as
 A11 A12
A21 A22

=
 L11
0
L21 L22
 U11 U12
0 U22

=
 L11U11
L11U12
L21U11 L21U12 + L22U22

,
where A11 is a leading principal submatrix of order j, 1 ≤j ≤n, as are L11 and U11.
Therefore, det A11 = det(L11U11) = det L11 det U11 =  j
k=1(U11)kk ̸= 0, since L is
unit triangular and U is nonsingular.
Sufﬁciency. The proof is by induction on the order n of the matrix A. For all
matrices of order one, we have the obvious decomposition a11 = l11u11 = 1a11. To
prove that there exists a decomposition for the matrix ˜A of order n, we need to ﬁnd
unique triangular matrices L and U of order (n −1), unique (n −1) × 1 vectors l
and u, and a unique nonzero number η such that the following decomposition holds:
˜A =
 A b
cT δ

=
 L 0
lT 1
 U u
0 η

=
 LU
Lu
lTU lT u + η

.
(8.5)
By the induction hypothesis, there exist unique matrices L and U of order (n −1)
such that A = LU. Comparing the left- and right-hand sides of (8.5), we get
u = L−1b,
lT = cTU −1,
η = δ −lT u.
(8.6)

8.1 Algorithms for Gaussian Elimination
253
It follows from (8.6) that u,l, η are unique. By the induction hypothesis, the diagonal
entries of the matrix U are nonzero, since U is nonsingular. Using (8.5), we get
0 ̸= det ˜A = det
 L 0
lT 1

det
U u
0 η

= η det(U).
Thus, η ̸= 0.
□
From Theorem 8.1 we conclude that there are classes of matrices important for
applications for which pivoting is not necessary. For example, all the leading principal
minors of the following matrices are nonzero:
1. Positive deﬁnite Hermitian matrices (see Sylvester’s criterion, p. 153).
2. Row
diagonally
dominant
and
column
diagonally
dominant
matrices
(see p. 242).
Theorem 8.1 says also that LU decomposition of a matrix A without pivoting can
fail even on well-conditioned nonsingular matrices A. This is because j × j leading
principal minors of these matrices can be singular. For example, the permutation
matrix P =
 0 1
1 0

is orthogonal, but the ﬁrst element in the ﬁrst column of this
matrix is zero. Thus, LU decomposition without pivoting will fail on this matrix.
Now we consider an example showing that pivoting in Gaussian elimination can
signiﬁcantly reduce the inﬂuence of rounding errors. At the same time, the result of
declining to pivot can be catastrophic.
Example 8.1 Let us consider a system of two equations in two unknowns
Ax = b,
(8.7)
where
A =
α 1
1 1

,
α is a given positive small real number, b = (1, 2)T . The solution of this system is
x1 = 1/(1 −α) ≈1,
x2 = (1 −2α)/(1 −α) ≈1.
(8.8)
By elementary calculations, we get
A−1 =
−1/(1 −α),
1/(1 −α)
1/(1 −α), −α/(1 −α)

.
Therefore, it is easy to see that for a small α, we have the inequality
cond∞(A) = ∥A∥∞∥A−1∥∞≈4,

254
8
Solving Systems of Linear Equations
i.e., the matrix A is very well conditioned, and the impact of rounding errors in
storage of its elements and of the right-hand side on the solution of system (8.7)
must be insigniﬁcant.
First, we solve system (8.7) by Gaussian elimination without pivoting. We obtain
L =
 1
0
1/α
1

,
U =
α
1
0
1 −1/α

.
The solution of the system Ly = b is y1 = b1 = 1, y2 = b2 −l2,1y1 = 2 −1/α. The
solution of Ux = y is x2 = y2/u2,2, x1 = (y1 −y2u1,2)/u1,1.
Let us put α = 10−14 and calculate the matrices L, U and the vectors y, x in
MATLAB®, using double precision, by the above formulas:
L =
1.000000000000000e + 000
0
1.000000000000000e + 014
1.000000000000000e + 000

,
U =
1.000000000000000e −014 1.000000000000000e + 000
0
−9.999999999999900e + 013

,
y1 = 1,
y2 = −9.999999999999800e + 013,
x2 = 9.999999999999900e −001
x1 = 9.992007221626409e −001,
(8.9)
LU =
1.000000000000000e −014
1.000000000000000e + 000
1.000000000000000e + 000
1.000000000000000e + 000

.
If we calculate the solution of system (8.7) directly by formulas (8.8), we obtain
x2 = 9.999999999999900e −001,
x1 = 1.000000000000001e + 000.
(8.10)
Comparing (8.9) and (8.10), we observe that the impact of the rounding errors on
the solution is signiﬁcant. If we set α = 10−16, then we get
y1 = 1,
y2 = −9.999999999999998e + 015,
x2 = 9.999999999999998e −001,
x1 = 2.220446049250313e + 0,
LU =
1.000000000000000e −016
1.000000000000000e + 000
1.000000000000000e + 000
0

,
i.e., the inﬂuence of the rounding errors is catastrophic. In the considered example,
this fact is explained in the following way. When we calculate y2 = 2 −1/α for a
small α, the impact of the ﬁrst term is lost because of the rounding errors.

8.1 Algorithms for Gaussian Elimination
255
Now we use Gaussian elimination with pivoting. For system (8.7), this means that
we have to permute the equations in (8.7) and write
A =
1 1
α 1

,
b = (2, 1)T . Then
L =
1
0
α
1

,
U =
1
1
0
1 −α

.
In this case, for α = 10−14 we get
x2 = 9.999999999999900e −001,
x1 = 1.000000000000010e + 000,
LU =
1.000000000000000e + 000 1.000000000000000e + 000
1.000000000000000e −014 1.000000000000000e + 000

.
For α = 10−16, we obtain
x2 = 9.999999999999999e −001,
x1 = 1,
LU =
1.000000000000000e + 000 1.000000000000000e + 000
1.000000000000000e −016 1.000000000000000e + 000

,
i.e., the impact of the rounding errors is practically absent.
8.1.3
A Numerical Example
Now we illustrate the performance of Gaussian elimination algorithms by solving
the Dirichlet problem for Poisson’s equation in two dimensions. Clearly, after a
discretization of the problem using ﬁnite elements or ﬁnite differences we obtain a
system of linear equations that can be solved by a number of different methods. In
this section we present ﬁrst the ﬁnite difference discretization of the problem and
show how to construct a system of linear equations from this discretization. Next,
our numerical example illustrates how Gaussian elimination algorithms can be used
to solve this system.
The model problem is the following Dirichlet3 problem for Poisson’s4 equation:
−△u(x) = f (x) in Ω,
u = 0 on ∂Ω.
(8.11)
3Johann Peter Gustav Lejeune Dirichlet (1805–1859) was a German mathematician.
4Siméon Denis Poisson (1781–1840) was a French mathematician.

256
8
Solving Systems of Linear Equations
Here f (x) is a given function, u(x) is an unknown function, and the domain Ω
is the unit square Ω = {(x1, x2) ∈(0, 1) × (0, 1)}. To solve (8.11) numerically, we
ﬁrst discretize the domain Ω with x1i = ih1 and x2 j = jh2, where h1 = 1/(ni −1)
and h2 = 1/(n j −1) are the mesh sizes in the directions x1, x2, respectively, and ni
and n j are the numbers of discretization points in the directions x1, x2, respectively.
In computations we usually have the same mesh size h = h1 = h2. In this example,
we choose ni = n j = n with n = N + 2, where N is the number of inner mesh nodes
in the directions x1, x2, respectively.
Indices (i, j) are such that 0 ≤i, j < n, and they are associated with every global
node nglob of the ﬁnite difference mesh. Global nodes numbers nglob in the two-
dimensional case can be computed using the following formula:
nglob = j + ni(i −1).
(8.12)
We use the standard ﬁnite difference discretization of the Laplace operator Δu in
two dimensions and obtain the discrete Laplacian Δui, j:
Δui, j = ui+1, j −2ui, j + ui−1, j
h2
+ ui, j+1 −2ui, j + ui, j−1
h2
,
(8.13)
where ui, j is the solution at the discrete point (i, j). Using (8.13), we obtain the
following scheme for solution of the problem (8.11):
−
ui+1, j −2ui, j + ui−1, j
h2
+ ui, j+1 −2ui, j + ui, j−1
h2

= fi, j,
(8.14)
where fi, j is the value of the function f at the discrete point (i, j). We observe
that (8.14) can be rewritten as
−

ui+1, j −2ui, j + ui−1, j + ui, j+1 −2ui, j + ui, j−1

= h2 fi, j,
(8.15)
or in a more convenient form as
−ui+1, j + 4ui, j −ui−1, j −ui, j+1 −ui, j−1 = h2 fi, j.
(8.16)
System (8.16) can be written in the form Au = b. The vector b has the compo-
nents bi, j = h2 fi, j. The explicit elements of the matrix A are given by the following
block matrix:
A =
⎛
⎜⎜⎜⎜⎝
AN −IN
−IN
...
...
...
... −IN
−IN
AN
⎞
⎟⎟⎟⎟⎠
,

8.1 Algorithms for Gaussian Elimination
257
with blocks AN of order N given by
AN =
⎛
⎜⎜⎜⎜⎝
4 −1
0
0 · · ·
0
−1
4 −1
0 · · ·
0
0 −1
4
0 · · ·
0
· · · · · · · · · · · · · · · · · ·
0 · · · · · ·
0 −1
4
⎞
⎟⎟⎟⎟⎠
,
which are located on the main diagonal of the matrix A, and blocks with identity
matrices −IN of order N on its off-diagonals. The matrix A is symmetric and positive
deﬁnite (see Question 8.16, p. 289). Therefore, we can use the LU factorization
algorithm without pivoting.
Suppose that we have discretized the two-dimensional domain Ω as described
above, and the number of inner points in both directions is N = 3. We present
the schematic discretization for the inner nodes of this domain and corresponding
numbering for the global nodes using (8.12) in the following scheme:
⎛
⎝
a1,1 a1,2 a1,3
a2,1 a2,2 a2,3
a3,1 a3,2 a3,3
⎞
⎠=⇒
⎛
⎝
n1 n2 n3
n4 n5 n6
n7 n8 n9
⎞
⎠=⇒
⎛
⎝
1 2 3
4 5 6
7 8 9
⎞
⎠.
(8.17)
Then the explicit form of the block matrix A will be
A =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
4 −1
0 −1
0
0
0
0
0
−1
4 −1
0 −1
0
0
0
0
0 −1
4
0
0 −1
0
0
0
−1
0
0
4 −1
0 −1
0
0
0 −1
0 −1
4 −1
0 −1
0
0
0 −1
0 −1
4
0
0 −1
0
0
0 −1
0
0
4 −1
0
0
0
0
0 −1
0 −1
4 −1
0
0
0
0
0 −1
0 −1
4
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
Example 8.2 In this example we present the numerical solution of problem (8.11).
We deﬁne the right-hand side f (x) of (8.11) as
f (x1, x2) = A f exp

−(x1 −c1)2
2s2
1
−(x2 −c2)2
2s2
2

1
a(x1, x2).
(8.18)
The coefﬁcient a(x1, x2) in (8.18) is given by the following Gaussian function:
a(x1, x2) = 1 + A exp

−(x1 −c1)2
2s2
1
−(x2 −c2)2
2s2
2

.
(8.19)

258
8
Solving Systems of Linear Equations
1
x 1
0.5
u(x 1,x2) with A = 12, n = 20
0
0
0.2
0.4
0.6
x 2
0.8
1
0
6
5
4
3
2
1
10 -3
u(x 1,x 2)
10 -3
0
1
2
3
4
5
x 1
0
0.2
0.4
0.6
0.8
1
x 2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
u(x 1,x2) with A = 12,  n = 20
10 -3
0
1
2
3
4
5
1
x 1
0.5
u(x 1,x2) with A = 12, N = 40
0
0
0.2
0.4
0.6
x 2
0.8
6
0
1
2
3
4
5
1
10 -3
u(x 1,x 2)
10 -3
0
1
2
3
4
5
x 1
0
0.2
0.4
0.6
0.8
1
x 2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
u(x 1,x2) with A = 12,  N = 40
10 -3
0
1
2
3
4
5
Fig. 8.1 Solution of problem (8.11) in the example of Section8.1.3 on the unit square.
Here A, A f are the amplitudes of these functions, c1, c2 are constants that show the
location of the center of the Gaussian functions, and s1, s2 are constants that show
the spreading of the functions in the x1 and x2 directions.
We produce a mesh with the points (x1i, x2 j) such that x1i = ih, x2 j = jh
with h = 1/(N + 1), where N is the number of inner points in the x1 and x2 direc-
tions. We take the same number of points in the x1 and x2 directions: ni = n j =
N + 2. The linear system of equations Au = f is solved then via the LU factor-
ization of the matrix A without pivoting. Figure 8.1 shows the results of numerical
simulations for different discretizations of the unit square with the number of inner
points N = 20, 40 and for A = 12, A f = 1, c1 = c2 = 0.5, s1 = s2 = 1 in (8.81)
and (8.18). The MATLAB® programs of Section 1.1 are available for running this
test.5
8.2
Error Analysis
One of the main techniques for computing the error in the computed solution is check-
ing its stability. This means that we need to check how much the computed solution
5The
MATLAB®
programs
can
be
found
on
the
online
version
of
Chapter
1
(doi:10.1007/978-3-319-57304-5_1).

8.2 Error Analysis
259
is changed depending on changes in input data. We will begin with a derivation of
perturbation theory in polynomial evaluation.
8.2.1
Rounding Analysis in Polynomial Evaluation
In this section we discuss the stability of polynomial evaluation by Horner’s rule.
Let there be given a polynomial
p(x) =
d

i=0
cixi,
where ci are the coefﬁcients of the polynomial, and d is its degree. For polynomial
evaluation we use Horner’s rule, noting that the polynomial can be written in an
alternative form as
p(x) = c0 + x(c1 + c2x + ...cdxd−1)
= c0 + x(c1 + c2(x + ...cdxd−2))
= c0 + x(c1 + x(....(cd−1 + cdx)...)).
(8.20)
Using (8.20), this rule can be programmed as the following iterative algorithm for
every mesh point x j ∈[xlef t, xright], j ∈1, 2, ...N, where N is the total number of
discretization points.
Algorithm 8.4 Horner’s rule for polynomial evaluation at the point x j ∈[xlef t,
xright], j ∈1, 2, ...N.
0. Initialize p = cd. Set counter i = d −1.
1. Compute p = x jp + ci.
2. Set i = i −1 and go to step 1. Stop if i = 0.
To compute error bounds in the polynomial evaluation we insert a rounding
term 1 + (δ1,2)i for every ﬂoating-point operation in Algorithm 8.4 to obtain the
following algorithm.
Algorithm 8.5 Error bounds in polynomial evaluation at the point x j ∈[xlef t,
xright], j ∈1, 2, ...N.
0. Set counter i = d −1 and initialize p = cd.
1. Compute p =

x jp(1 + (δ1)i) + ci

(1 + (δ2)i), where |(δ1)i|, |(δ2)i| ≤ε.
2. Set i := i −1 and go to step 1. Stop if i = 0.
In Algorithm 8.5 the number ε is the machine epsilon, and we deﬁne it as the
maximum relative representation error 0.5b1−p, which is measured in ﬂoating-point
arithmetic with base b and precision p > 0. Table8.1 presents the values of the
machine epsilon in standard ﬂoating-point formats.

260
8
Solving Systems of Linear Equations
Table 8.1 The values of machine epsilon in standard ﬂoating-point formats. The notation ∗means that one bit is implicit in precision p
EEE 754 - 2008
Description
Base, b
Precision, p
ε1 = 0.5b−(p−1)
ε2 = b−(p−1)
binary16
Half precision
2
11∗
2−11 = 4.88e −04
2−10 = 9.77e −04
binary32
Single precision
2
24∗
2−24 = 5.96e −08
2−23 = 1.19e −07
binary64
Double precision
2
53∗
2−53 = 1.11e −16
2−52 = 2.22e −16
binary80
Extended precision
2
64
2−64 = 5.42e −20
2−63 = 1.08e −19
binary128
Quad(ruple) precision
2
113∗
2−113 = 9.63e −35
2−112 = 1.93e −34
decimal32
Single-precision decimal
10
7
5 × 10−7
10−6
decimal64
Double-precision decimal
10
16
5 × 10−16
10−15
decimal128
Quad(ruple)-precision decimal
10
34
5 × 10−34
10−33

8.2 Error Analysis
261
Expanding the expression for pi in Algorithm 8.5, we get the ﬁnal value of p0:
p0 =
d−1

i=0

(1 + (δ2)i)
i−1

k=0
(1 + (δ1)k)(1 + (δ2)k)

cixi
+
d−1

k=0
(1 + (δ1)k)(1 + (δ2)k)

cdxd.
(8.21)
Next, we write the upper and lower bounds for the products of δ = δ1,2, provided
that kε < 1:
(1 + δ1) · · · (1 + δk) ≤(1 + ε)k ≤1 + kε + O(ε2),
(1 + δ1) · · · (1 + δk) ≥(1 −ε)k ≥1 −kε.
(8.22)
Applying the above estimates, we get the following inequality:
1 −kε ≤(1 + δ1) · ... · (1 + δk) ≤1 + kε.
(8.23)
Using estimate (8.23), we can rewrite (8.21) as
p0 ≈
d

i=0
(1 + ˜δi)cixi =
d

i=0
˜cixi
(8.24)
with approximate coefﬁcients ˜ci = (1 + ˜δi)ci such that |˜δi| ≤2kε ≤2dε. Now we
can write the formula for the computing the error ep in the polynomial:
ep := |p0 −p(x)| =

d

i=0
(1 + ˜δi)cixi −
d

i=0
cixi
 =

d

i=0
˜δicixi

≤2
d

i=0
dε|cixi| ≤2dε
d

i=0
|cixi| = Δ(x),
(8.25)
so the true value of the polynomial is in the interval (p −Δ, p + Δ).
If we choose ˜δi = ε sign(cixi), then the error bound above can be attained within
the factor 2d. In this case, we can take
cond(p) :=
d
i=0
|cixi|

d
i=0
cixi

(8.26)
as the relative condition number for the case of polynomial evaluation.

262
8
Solving Systems of Linear Equations
In the following algorithm, we use (8.25) to compute the lower bound in polyno-
mial evaluation.
Algorithm 8.6 Computation of the error Δ(x j) in the polynomial evaluation at the
point x j ∈[xlef t, xright], j ∈1, 2, ...N.
0. Set counter i = d −1 and initialize p = cd, Δ = |cd|.
1. Compute p = x jp + ci, Δ = |x j|Δ + |ci|.
2. Set i = i −1 and go to step 1. Stop if i = 0.
3. Set Δ(x j) = 2dεΔ as the error bound at the point x j.
Example 8.3 Figure8.2-(a) shows the behavior of the computed solution using
Horner’s rule (Algorithm 8.4) for the evaluation of the polynomial
p(x) = (x −9)9 = x9 −81x8 + 2916x7 −61236x6 + 826686x5 −7440174x4
+ 44641044x3 −172186884x2 + 387420489x1 −387420489.
(8.27)
Figure8.2-(b) shows the upper and lower bounds computed for the polynomial
p(x) = (x −9)9 using Algorithm 8.6. We have performed all our computations tak-
ing ε as the machine epsilon in Algorithm 8.6. Using these ﬁgures, we observe that
changing the argument x slightly can change computed values drastically. Figure8.3
presents comparison of the theoretically estimated relative error eln = −ln
 Δ
p

with the computed relative error ecomp = −ln
 p(x)−˜p(x)
p(x)
 for two polynomials: for
the polynomial p(x) = (x −1)2(x −2)(x −3)(x −4)(x −5) (see Fig.8.3a) and
for the polynomial p(x) = (x −1)2(x −2)(x −3)(x −4)(x −5)(x −7)(x −9)
(x −11)(x −15)(x −17) (See Fig.8.3b). We note that ˜p(x) is computed polyno-
mial using the Horner’s rule (Algorithm 8.4) and p(x) is the exact polynomial.
Indeed, we encounter difﬁculties when we want to compute p(x) with a high
relative accuracy if p(x) is close to zero. This is because small changes in ε result
in an inﬁnite relative error given by ε/p(x) = ε/0, which means that our relative
conditionnumber (8.26)isinﬁnite;seealsoanillustrationofthisstatementinFig.8.2-
(c), (d). There is a simple geometric interpretation of this condition number: it tells
us how far p(x) is from a polynomial whose condition number at x is inﬁnite. Now
we introduce a necessary concept and prove the corresponding theorem.
Let p(x) =
d
i=0
aixi and q(x) =
d
i=0
bixi be two polynomials. Then the relative
distance dist(p, q) from p(x) to q(x) is deﬁned as the smallest value such that
|ai −bi| ≤dist(p, q)|ai|, i ≤1 ≤d.

8.2 Error Analysis
263
input interval for x
8.7
8.8
8.9
9
9.1
9.2
9.3
9.4
input interval for x
8.7
8.8
8.9
9
9.1
9.2
9.3
9.4
input interval for x
8.7
8.8
8.9
9
9.1
9.2
9.3
9.4
input interval for x
8.7
8.8
8.9
9
9.1
9.2
9.3
9.4
10-5
-3
-2
-1
0
1
2
3
Horners rule (8000 points)
exact p(x)=(x - 9)
9
10 -4
-2.5
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
2.5
exact p(x)=(x - 9)
9
upper bound
lower bound
a)
b)
0
500
1000
1500
2000
2500
3000
3500
error = abs(bp/P)
-10
-8
-6
-4
-2
0
2
4
6
8
10
log error = -log(abs(bp/P))
  -log(abs((P -(x-9).
9)/P))
c)
d)
Fig. 8.2
(a) Evaluation of the polynomial p(x) = (x −9)9 by Horner’s rule (Algorithm 8.4)
compared with the exact polynomial. (b) Computed upper and lower bounds for the polynomial
p(x) = (x −9)9 using Algorithm 8.6. (c) Plot of the graph of the estimated relative error e =
 Δ
p
. (d) Plot of the graph of the estimated relative error eln = −ln
 Δ
p
 (presented in blue color)
compared with the computed relative error ecomp = −ln
 p(x)−(x−9)9
p(x)
 (presented in red color).
Here, p(x) is computed by Horner’s rule (Algorithm 8.4) and the exact polynomial (x −9)9 is
computed in MATLAB®. Input interval for x in this example is x ∈[8.7, 9.3].
If ai ̸= 0, i ≤1 ≤d, the condition above can be rewritten as
max
0≤i≤d
|ai −bi|
|ai|
= dist(p, q), i ≤1 ≤d.
Theorem 8.2 Let a polynomial p(x) =
d
i=0
cixi be not identically zero, and let q be
another polynomial whose condition number at x is inﬁnite, i.e., q(x) = 0. Then

264
8
Solving Systems of Linear Equations
input interval for x
-1
0
1
2
3
4
5
6
7
10
15
20
25
30
35
40
  estimated bound
  computed bound
input interval for x
-5
0
5
10
15
20
10
15
20
25
30
35
40
  estimated bound
  computed bound
a)
b)
Fig. 8.3 Plot of the graph of the estimated relative error eln = −ln
 Δ
p
 (shown in blue) compared
with the computed relative error ecomp = −ln
 p(x)−˜p(x)
p(x)
 (here ˜p(x) is computed by Horner’s
rule and p(x) is the exact polynomial, which we compute in MATLAB®): (a) for the polyno-
mial p(x) = (x −1)2(x −2)(x −3)(x −4)(x −5) and (b) for the polynomial p(x) = (x −1)2
(x −2)(x −3)(x −4)(x −5)(x −7)(x −9)(x −11)(x −15)(x −17).
min {dist(p, q) : q(x) = 0} =

d
i=0
cixi

d
i=0
|cixi|
.
(8.28)
Proof To prove this theorem, let us write q(x) =
d
i=0
bixi =
d
i=0
(1 + εi)cixi such
that dist(p, q) = max
0≤i≤d |εi|. Then q(x) = 0 implies that
|p(x)| = |q(x) −p(x)| = |
d

i=0
εicixi| ≤
d

i=0
|εicixi| ≤max
0≤i≤d |εi|
d

i=0
|cixi|.
Thus,
dist(p, q) = max
0≤i≤d |εi| ≥
|p(x)|
d
i=0
|cixi|
.
There is a q that is close to p, for example the polynomial q with
εi = −p(x)
d
i=0
|cixi|
sign(cixi).
□

8.2 Error Analysis
265
8.2.2
Error Analysis in Gaussian Elimination
In this section we derive an error analysis for LU decomposition and Gaussian elim-
ination that is similar to the error analysis of polynomial evaluation of Section8.2.1.
We assume that the matrix A has already been pivoted. We will simplify the error
analysis only for two equations of Algorithm 8.1, one for a jk, j ≤k, and one for
j > k. Let us ﬁrst analyze what this algorithm is doing with element a jk when j ≤k.
We observe that this element is repeatedly updated as
u jk = a jk −
j−1

i=1
l jiuik.
If j > k, then we have
l jk =
a jk −
k−1

i=1
l jiuik
ukk
.
To do rounding error analysis of these two formulas, we use the following expression
for ﬂoating-point approximations:
ﬂ
 d

i=1
xi yi

=
d

i=1
xi yi(1 + δi),
|δi| ≤dε,
(8.29)
where ε is the machine epsilon or the relative representation error. The maximum
of the relative representation error in a ﬂoating-point arithmetic with p digits and
base b is 0.5b1−p; see also Table8.1 for ε in standard ﬂoating-point formats.
We apply (8.29) to the formula for u jk:
u jk =

a jk −
j−1

i=1
l jiuik(1 + δi)

(1 + δ′)
with |δi| ≤( j −1)ε and |δ′| ≤ε. Expressing a jk, we get
a jk =
1
1 + δ′ u jkl j j +
j−1

i=1
l jiuik(1 + δi)
≤
j

i=1
l jiuik +
j

i=1
l jiuikδi =
j

i=1
l jiuik + E jk,
(8.30)
where we have used the fact that l j j = 1 and the assumptions

266
8
Solving Systems of Linear Equations
|δi| ≤( j −1)ε,
1 + δ j :=
1
1 + δ′ .
In the expression above we can bound E jk by
|E jk| =

j

i=1
l jiuikδi
 ≤
j

i=1
|l ji||uik|nε = nε(|L||U|) jk.
Thus, we get the following estimate for a jk:
a jk ≤
j

i=1
l jiuik + E jk.
We perform the same analysis for the formula for l jk to get
l jk = (1 + δ′′)
⎛
⎜⎜⎜⎝
(1 + δ′)(a jk −
k−1

i=1
l jiuik(1 + δi))
ukk
⎞
⎟⎟⎟⎠,
where |δi| ≤(k −1)ε, |δ′| ≤ε, and |δ′′| ≤ε. Expressing a jk, we get
a jk =
1
(1 + δ′)(1 + δ′′)ukkl jk +
k−1

i=1
l jiuik(1 + δi).
Deﬁning 1 + δk :=
1
(1+δ′)(1+δ′′), we can rewrite the previous expression as
a jk ≤
k

i=1
l jiuik +
k

i=1
l jiuikδi ≡
k

i=1
l jiuik + E jk
with |δi| ≤nε and |E jk| ≤nε(|L||U|) jk as before. We summarize this error analysis
with the simple formula
A = LU + E,
where
|E| ≤nε|L||U|.
Taking norms, we get
∥E∥≤nε∥|L| ∥∥|U| ∥.

8.2 Error Analysis
267
If the norm does not depend on the sign of the entries of the matrix (this is valid for
Frobenius, inﬁnity, one-norms, but not for two-norms), we can simplify the expres-
sion above as
∥E∥≤nε∥L∥∥U∥.
(8.31)
Thus, in formula (8.31) we have obtained an error estimate for LU decomposition.
The next step is to obtain errors in forward and backward substitutions. We solve
LUx = b via Ly = b and Ux = y. Solving Ly = b gives as a computed solution ˆy
such that (L + δL) ˆy = b, where |δL| ≤nε|L|. The same is true for (U + δU)ˆx = ˆy
with |δU| ≤nε|U|. Combining both estimates, we get
b = (L + δL) ˆy = (L + δL)(U + δU)ˆx
= (LU + LδU + δLU + δLδU)ˆx
= (A −E + LδU + δLU + δLδU)ˆx
= (A + δA)ˆx,
(8.32)
where δA = −E + LδU + δLU + δLδU. Now we combine all bounds for
E, δU, δL and use the triangle inequality to get
|δA| = | −E + LδU + δLU + δLδU|
≤|E| + |L||δU| + |δL||U| + |δL||δU|
≤nε|L||U| + nε|L||U| + nε|L||U| + n2ε2|L||U|
≈3nε|L||U|.
(8.33)
Assuming that ∥|X| ∥= ∥X∥is true (as before, this is valid for Frobenius, inﬁnity
and one-norms but not for two-norms), we obtain
∥δA∥≤3nε∥L∥∥U∥.
(8.34)
Thus, Gaussian elimination is backward stable if (recall that in this analysis we have
used δb = 0) the following condition holds:
3nε∥L∥∥U∥= O(ε)∥A∥.
We note that GEPP allows us to estimate every entry of the matrix L by an entry
in absolute value, so we need consider only ∥U∥. The pivot growth factor for GEPP
is the number
g = ∥U∥max
∥A∥max
,
(8.35)
where ∥A∥max = max
1≤i, j≤n |ai j|. In other words, stability is equivalent to g being small
or growing slowly as a function of n.

268
8
Solving Systems of Linear Equations
Let us prove that the bound of the pivot growth factor for GEPP is
g ≤2n−1.
(8.36)
Indeed, at the ﬁrst step in GEPP, we perform an update for elements
˜a jk = a jk −l jiuik
(8.37)
with |l ji| ≤1, |uik| = |aik| ≤max
r,s |ars|. Substituting these estimates in (8.37), we
obtain that
|˜a jk| ≤2 max
r,s |ars|.
(8.38)
Using estimate (8.38), we conclude that at every (n −1) step of the Algorithm8.1
we can double the size of the remaining matrix entries. Estimate (8.36) follows from
this observation.
There are practical examples showing that the bound in (8.36) is attainable [23].
Now using the facts that ∥L∥∞≤n and ∥U∥∞≤ng∥A∥∞and substituting these
estimates together with estimate (8.36) for g in (8.34), we obtain
∥δA∥∞≤3nε∥L∥∞∥U∥∞≤3 gn3ε∥A∥∞.
(8.39)
For example, if ε = 10−7 and the order of the matrix A is n > 120. Then using
equality in (8.36), we can compute that 3gn3ε > 1. We observe that with such an
estimate we lose all precision.
8.2.3
Estimating the Condition Number
Let us recall that in Theorem 7.12, p. 243, we established a connection between the
relative perturbations of a matrix and the right-hand side of the system Ax = b with
the relative perturbations of its solution. An important role in the estimates obtained
is played by the condition number cond(A) = ∥A∥∥A−1∥of A. To compute cond(A)
we need to estimate ∥A−1∥, since ∥A∥is easy to compute.
If we compute A−1 explicitly and then compute its norm, that would cost 2n3 oper-
ations, but the number of operations in Gaussian elimination is approximately 2n3/3
(see p. 46). Therefore, we will seek a cheaper algorithm for an estimate of ∥A−1∥.
Below we present this algorithm and will call it a condition estimator or Hager’s algo-
rithm. The algorithm was developed in [45, 50, 51] and has the following properties:

8.2 Error Analysis
269
1. This estimator is guaranteed to produce only a lower bound of ∥A−1∥, not an
upper bound.
2. The cost is O(n2) operations. This is negligible compared to the 2n3/3 cost of
Gaussian elimination if the order n of the matrix A is large.
3. It provides an estimate that is almost always within a factor of 10 of ∥A−1∥and
usually within of factor 2 to 3.
The algorithm estimates the one-norm ∥B∥1 of a matrix B, under the condition
that we can compute the products Bx and BT y for arbitrary vectors x and y. Recall
that (see (6.26), p. 220) the norm ∥B∥1 is deﬁned by
∥B∥1 =
max
x∈Rn, ∥x∥1=1 ∥Bx∥1 = max
1≤j≤n
n

i=1
|bi j|,
(8.40)
or as the maximum absolute column sum. As we saw on p. 220, the maximum
over x is attained at the vector x = i j0 = (0, . . . , 0, 1, 0, . . . , 0) with component j0
as the single nonzero entry, where max j

i |bi j| occurs at j = j0. However, direct
searching over all such vectors i j, j = 1, . . . , n, is too expensive, because this means
computing all columns of B = A−1. Since ∥Bx∥1 = max∥x∥1≤1 ∥Bx∥1, we can use
the gradient method to ﬁnd the maximum of f (x) = ∥Bx∥1 inside the set ∥x∥1 ≤1.
Here ∥x∥1 ≤1 is a convex set of vectors, and f (x) is a convex function. Indeed, if
0 ≤α ≤1, then
f (αx + (1 −α)y) = ∥αBx + (1 −α)By∥1
≤α∥Bx∥1 + (1 −α)∥By∥1 = αf (x) + (1 −α) f (y).
(8.41)
If the gradient ∇f (x) of the function f (x) exists then the convexity property of f (x)
means that f (y) ≥f (x) + ∇f (x)(y −x). To compute gradient ∇f (x) we assume
that all n
j bi jx j ̸= 0 (since B = A−1) in f (x) = ∥Bx∥1 = n
i

n
j bi jx j
. Let
ζi = sign(n
j bi jx j) such that ζi = ±1 and f (x) = n
i
n
j ζibi jx j. Then ∂f /∂xk =
n
i ζibik and ∇f = ζ T B = (BT ζ)T .
Algorithm 8.7 Hager’s condition estimator returns a lower bound ∥w∥1 for ∥B∥1.
choose any x such that ∥x∥1 = 1
/∗e.g., xi = 1/n ∗/
repeat
w = Bx, ζ = sign(w), z = BT ζ,
/∗zT = ∇f ∗/
if ∥z∥∞≤zT x, then
return ∥w∥1
else
x = i j where |z j| = ∥z∥∞
/∗here 1 in ij should be placed at index j where
|z j| = ∥z∥∞∗/
end if
end repeat

270
8
Solving Systems of Linear Equations
Theorem 8.3 The next two statements are valid for Algorithm 8.7.
1. If ∥z∥∞≤zT x and ∥w∥1 is returned, then ∥w∥1 = ∥Bx∥1 is a local maximum
of ∥Bx∥1.
2. Otherwise, the algorithm has made progress in maximizing f (x).
Proof 1. We have that ∥z∥∞≤zT x. Close to x,
f (x) = ∥Bx∥1 =
n

i
n

j
ζibi jx j
is linear in x, and we can use a Taylor series to get
f (y) ≈f (x) + ∇f (x)(y −x) = f (x) + zT (y −x),
where zT = ∇f (x). To show that x is a local maximum, we need to prove the
inequality f (y) ≤f (x) or to show that zT (y −x) ≤0 with ∥y∥1 = 1. We get
zT (y −x) = zT y −zT x =
n

i
zi yi −zT x ≤
n

i
|zi||yi| −zT x
≤∥z∥∞∥y∥1 −zT x = ∥z∥∞−zT x ≤0.
(8.42)
2. In this case, ∥z∥∞> zT x. We choose x = i jsign(z j), where the number j is
such that |z j| = ∥z∥∞. Then
f (x) = f (x) + ∇f × (x −x) = f (x) + zT (x −x)
= f (x) + zTx −zT x = f (x) + |z j| −zT x > f (x).
(8.43)
The last inequality holds, since ∥z∥∞> zT x.
□
In the example presented below, we will test the Algorithm 8.7 using the
MATLAB® program of Section1.3.6
Example 8.4 We test computation of Hager’s condition estimator for the matrix
A =
⎛
⎜⎜⎜⎜⎝
16.5488 14.6149 4.3738
7.0853 2.3420
14.6149 3.4266 29.5580 23.7673 6.8848
4.3738 29.5580 0.1620
3.9291
6.7942
7.0853 23.7673 3.9291
6.5877 25.1377
2.3420
6.8848
6.7942 25.1377 1.4003
⎞
⎟⎟⎟⎟⎠
(8.44)
6The
MATLAB®
programs
can
be
found
on
the
online
version
of
Chapter
1
(doi:10.1007/978-3-319-57304-5_1).

8.2 Error Analysis
271
using Algorithm 8.7. We run the MATLAB® program of Section1.3 and get the
following results: the computed one-norm of the matrix (8.44) by Algorithm 8.7
is 78.2517 which is the same as the computed one-norm using the MATLAB®
command norm(A,1).7
Remark 8.1 In [51, 52], a slightly improved version of this algorithm was tested on
many random matrices A of orders 10, 25, and 50. These matrices had condition
numbers cond(A) = 10, 103, 106, 109. Then using Hager’s algorithm, in the worst
case the computed cond(A) underestimated the true cond(A) by a factor of 0.44,
which says something about the efﬁciency of Hager’s algorithm for computation of
cond(A).
8.2.4
Estimating the Relative Condition Number
We also can apply Hager’s algorithm to estimate the relative (or Bauer–Skeel) con-
dition number κBS(A) = ∥|A−1||A|∥∞presented in Theorem 7.14, p. 245. To do
so, we will estimate ∥|A−1|g∥∞, where g is a vector with nonnegative entries. We
explain this now. Let e = (1, ..., 1) be the vector of all ones. Using properties of the
inﬁnity norm, we see that ∥X∥∞= ∥Xe∥∞if the matrix X has nonnegative entries.
Then
κBS(A) = ∥|A−1||A|∥∞= ∥|A−1||A|e∥∞= ∥|A−1|g∥∞,
where g = |A|e.
Now we can estimate ∥|A−1|g∥∞. Let G = diag(g1, . . . , gn) be the diagonal
matrix with entries gi on its diagonal. Then g = Ge and thus
∥|A−1|g∥∞= ∥|A−1|Ge∥∞= ∥|A−1|G∥∞= ∥|A−1G|∥∞= ∥A−1G∥∞. (8.45)
The last equality holds because ∥Y∥∞= ∥|Y|∥∞for every matrix Y.
Now we will show how to estimate the inﬁnity norm of the matrix A−1G by
Hager’s algorithm. Applying this algorithm to the matrix (A−1G)T , we can estimate
∥(A−1G)T ∥1 = ∥A−1G∥∞. Thus, we will apply Hager’s algorithm to ∥(A−1G)T ∥1
and in this way compute ∥A−1G∥∞.
8.2.5
Practical Error Bounds
Below, we present two practical error bounds for an approximate solution x of the
equation Ax = b. In the ﬁrst bound, we use inequality (7.53), p. 244, to get
error = ∥x −x∥∞
∥x∥∞
≤∥A−1∥∞
∥r∥∞
∥x∥∞
,
(8.46)
7The
MATLAB®
programs
can
be
found
on
the
online
version
of
Chapter
1
(doi:10.1007/978-3-319-57304-5_1).

272
8
Solving Systems of Linear Equations
where r = Ax −b is the residual. To compute ∥A−1∥∞, we can apply Hager’s
algorithm for B = (A−1)T , noting that ∥B∥1 = ∥(A−1)T ∥1 = ∥A−1∥∞. The second
error bound follows from the inequality
∥x −x∥∞= ∥A−1r∥∞≤∥|A−1||r|∥∞,
where we have used the triangle inequality. This practical error bound has the form
error = ∥x −x∥∞
∥x∥∞
≤∥|A−1||r|∥∞
∥x∥∞
.
(8.47)
The estimate of ∥|A−1||r|∥∞can be obtained using Hager’s algorithm, taking into
account (8.45).
Remark 8.2
1. Error bounds (8.46) and (8.47) cannot be guaranteed to provide
bounds in all cases.
2. The estimate of ∥A−1∥1 from Algorithm 8.7 provides only a lower bound.
3. There is a small probability that the rounding in the evaluation of the residual
r = Ax −b can make ∥r∥artiﬁcially small. To take this into account, we add a
small number to |r| and replace |r| with |r| + (n + 1)ε(|A||x| + |b|) in the bound
(8.47) or ∥r∥with ∥r∥+ (n + 1)ε(∥A∥∥x∥+ ∥b∥) in the bound (8.46). This is
done by noting that the rounding error in evaluating r is bounded by
|(Ax −b) −ﬂ(Ax −b)| ≤(n + 1)ε(|A||x| + |b|).
(8.48)
4. The rounding in performing Gaussian elimination on very ill conditioned matrices
A can get such inaccurate L and U that the bound (8.47) can be too low.
8.3
Algorithms for Improving the Accuracy of the Solution
If the error in the computed solution ˜x = x + δx is as large as cond(A)ε, we can try
to use Newton’s method to improve the solution. This means that to solve an equa-
tion f (x) = 0, we can construct an iterative procedure xi+1 = xi −f (xi)/ f ′(xi)
and obtain the improved computed solution xi+1. Applying the idea of this method
to f (x) = Ax −b yields the following algorithm:
Algorithm 8.8 Newton’s algorithm.
repeat
r = Axi −b
solve Ad = r to compute d
xi+1 = xi −d
end repeat

8.3 Algorithms for Improving the Accuracy of the Solution
273
If we could compute the residual r = Axi −b in this algorithm exactly and solve
the equation Ad = r exactly, then we could ﬁnish the algorithm in one step. We
expect such a solution from Newton’s method applied to a linear problem. However,
rounding errors prevent this immediate convergence. Algorithm 8.8 is useful when
A is so ill conditioned that solving Ad = r (and Ax0 = b) is rather inaccurate.
Theorem 8.4 Suppose that r = Axi −b in Algorithm 8.8 is computed in double
precision and cond(A)ε < c = 1/(3n3g + 1) < 1, where n is the dimension of A
and g is the pivot growth factor given by (8.35). Then Algorithm 8.8 converges as
∥xi −A−1b∥∞
∥A−1b∥∞
= O(ε)
and has the following relaxation property:
∥xi+1 −x∥∞≤cond(A)ε
c
∥xi −x∥∞= ζ∥xi −x∥∞,
(8.49)
with the relaxation parameter ζ = cond(A)ε/c < 1.
Proof Let us denote here ∥· ∥∞by ∥· ∥. Our goal is to show that (8.49) holds. By
assumption, ζ < 1, so this inequality implies that the error ∥xi+1 −x∥decreases
monotonically to zero. We begin the proof by estimating the error in the computed
residual r. Using estimate (8.29), we can write
r = ﬂ(Axi −b) = Axi −b + f,
(8.50)
where
| f | ≤nε2(|A||xi| + |b|) + ε|Axi −b| ≈ε|Axi −b|.
(8.51)
Here, as usual, ε is the relative representation error. The ε2 term comes from the
double-precision computation of r, and the ε term comes from rounding the double-
precision result back to single precision. Since ε2 ≪ε, we will neglect the ε2 term
in the bound on | f |. Next, from Newton’s method we have
(A + δA)d = r.
(8.52)
From the bound (8.39) we know that ∥δA∥≤γ ε∥A∥, where γ = 3n3g, although
this is usually too large in reality. We assume that computations of xi+1 = xi −d in
Algorithm 8.8 are performed exactly. Using (8.52) and substituting here (8.50) and
ignoring all ε2 terms, we get

274
8
Solving Systems of Linear Equations
d = (A + δA)−1r = (I + A−1δA)−1 A−1r
= (I + A−1δA)−1 A−1(Axi −b + f ) = (I + A−1δA)−1(xi −x + A−1 f )
≈(I −A−1δA)(xi −x + A−1 f ) ≈xi −x −A−1δA(xi −x) + A−1 f.
(8.53)
Next, subtracting x from both sides of xi+1 = xi −d and using then (8.53), we get
xi+1 −x = xi −d −x = A−1δA(xi −x) −A−1 f.
(8.54)
Taking then norms from (8.54), using (8.51) for estimation of f and the bound (8.39)
for estimation of ∥δA∥, we see that
∥xi+1 −x∥≤∥A−1δA(xi −x)∥+ ∥A−1 f ∥
≤∥A−1∥∥δA∥∥xi −x∥+ ∥A−1∥ε∥Axi −b∥
≤∥A−1∥∥δA∥∥xi −x∥+ ∥A−1∥ε∥A(xi −x)∥
≤∥A−1∥γ ε∥A∥∥xi −x∥+ ∥A−1∥∥A∥ε∥xi −x∥
= ∥A−1∥∥A∥ε(γ + 1)∥xi −x∥.
(8.55)
Let
ζ = ∥A−1∥∥A∥ε(γ + 1) = cond(A)ε/c < 1.
Then (8.49) is satisﬁed, and Newton’s Algorithm 8.8 converges.
□
Note that the condition number in Theorem 8.4 does not appear in the ﬁnal error
bound. This means that we compute the answer accurately independent of the condi-
tion number, provided that cond(A)ε is sufﬁciently less than 1. Usually, c is too con-
servativeanupperbound,andthealgorithmoftensucceedsevenwhencond(A)ε > c.
Sometimes we cannot run Algorithm 8.8 with double precision, and only compu-
tations with single precision are available. In this case, we still can run Algorithm 8.8
and compute the residual r in single precision. However, Theorem 8.4 is not valid
for this case, and we have the following theorem.
Theorem 8.5 Suppose that the residual r = Axi −b in Algorithm 8.8 is computed
in single precision and
∥A−1∥∞∥A∥∞
maxi(|A||x|)i
mini(|A||x|)i
ε < 1.
Then one step of iterative reﬁnement yields x1 such that (A + δA)x1 = b + δb
with |δai j| = O(ε)|ai j| and |δbi| = O(ε)|bi|.
For a proof, see [53] as well as [2], [104–106] for details. Theorem 8.5 says that
the componentwise relative backward error is as small as possible. For example, this

8.3 Algorithms for Improving the Accuracy of the Solution
275
means that if A and b are sparse, then δA and δb have the same sparsity structures
as A and b, respectively.
Now we present one more common technique for improving the error in solving
a linear system: equilibration, which yields the following algorithm.
Algorithm 8.9 Choose an appropriate diagonal matrix D to solve DAx = Db
instead of Ax = b. The matrix D is chosen to try to make the condition number
of DA smaller than that of A.
For example, if we choose dii to be the reciprocal of the two-norm of row i of A,
that would make DA nearly equal to the identity matrix for very ill conditioned matri-
ces. In [112] it was shown that choosing D in this way reduces the condition number
of DA to within a factor of √n of its smallest possible value for any diagonal D.
In computations we may also choose two diagonal matrices Drow and Dcol and
solve (Drow ADcol)¯x = Drowb, x = Dcol ¯x, and thus Drow Ax = Drowb.
8.4
Special Linear Systems
It is important to exploit any special structure of the matrix to increase the speed of
algorithms for linear systems Ax = b and decrease storage of intermediate matrices
and vectors. In this section we will discuss only real matrices, since extension to
complex matrices is straightforward. Matrices A with the following structures will
be considered:
• Symmetric positive deﬁnite matrices (s.p.d. matrices),
• Symmetric indeﬁnite matrices,
• Band matrices.
8.4.1
Real Symmetric Positive Deﬁnite Matrices
Recall that a real matrix A is called s.p.d. if A = AT and xT Ax > 0 for all x ̸= 0.8
In this subsection we show how to solve Ax = b in half the time and half the space
of Gaussian elimination when A is s.p.d.
Asweknow(seep.150),if A = AT ,then A iss.p.d.ifandonlyifallitseigenvalues
are positive. Below we prove some other useful properties of s.p.d. matrices.
Proposition 8.1 If X is nonsingular, then A is s.p.d. if and only if X T AX is s.p.d.
Proof If X is nonsingular, then Xx ̸= 0 for all x ̸= 0, and thus xT X T AXx > 0 for
all x ̸= 0. Since is A s.p.d., this implies that X T AX is s.p.d. Use X−1 to deduce that
if X T AX is s.p.d., then A is s.p.d.
□
8See (4.130), p. 150.

276
8
Solving Systems of Linear Equations
Proposition 8.2 If A is s.p.d. and H = A( j : k, j : k) is any principal submatrix
of A, then H is s.p.d.
Proof Suppose ﬁrst that H = A(1 : m, 1 : m) is a leading principal submatrix
of A. Then for any given vector y of size m, the n-vector x = (y, 0) satisﬁes the
equality yT Hy = xT Ax. Since A is s.p.d., xT Ax > 0 for all vectors x ̸= 0, then
yT Hy > 0 for all vectors y ̸= 0, and thus H is s.p.d. If H is not a leading principal
submatrix of A, let P be a permutation matrix such that H lies in the upper left corner
of PT AP. Then apply Proposition 8.1 to PT AP.
□
Proposition 8.3 If A is s.p.d., then all aii > 0, and maxi j |ai j| = maxi aii > 0.
Proof The ﬁrst assertion is easy to check (see Property 5, p. 141). Let us prove the
second assertion. Let as usual ik be the kth standard unit vector. If |akl| = maxi j |ai j|
but k ̸= l (this means that we assume that maxi j |ai j| ̸= maxi aii), we choose the
vector x = ek −sign(akl)el. Then xT Ax = akk + all −2|akl| ≤0. But this is a con-
tradiction to the positive deﬁniteness of the matrix A, and thus maxi j |ai j| = maxi aii.
Proposition 8.4 A matrix A is s.p.d. if and only if there is a unique lower triangular
nonsingular matrix L, with positive diagonal entries, such that A = LLT . A matrix
A = LLT is called the Cholesky9 factorization of A, and L is called the Cholesky
factor of A.
Proof Sufﬁciency. Assume that there exists a factorization A = LLT with L nonsin-
gular. Then xT Ax = (xT L)(LT x) = ∥LT x∥2
2 > 0 for all x ̸= 0, so A is s.p.d.
Necessity. If A is s.p.d., we show that L exists by induction on the dimension n. If
we choose each lii > 0, our construction will determine L uniquely. If n = 1, choose
l11 = √a11, which exists since a11 > 0. Let us write
A =
 a11 A12
AT
12 A22

=
√a11
0
y
˜L22
  √a11 yT
0
˜LT
22

=

a11
√a11yT
√a11y yyT + ˜L22 ˜LT
22

=

a11
A12
AT
12 ˜A22 + AT
12 A12
a11

=
√a11 0
AT
12
√a11 I
  1 0
0 ˜A22
 √a11
A12
√a11
0
I

.
(8.56)
We observe that the (n −1)-by-(n −1) matrix
˜A22 = A22 + AT
12 A12/a11 is a
symmetric matrix. Using Proposition 8.1 and expression (8.56), we conclude that
 1 0
0 ˜A22

is s.p.d. By Proposition 8.2, the matrix ˜A22 is also s.p.d. Thus, by induc-
tion, there is an ˜L such that ˜A22 = ˜L ˜LT and
9André-Louis Cholesky (1875–1918) was a French military ofﬁcer and mathematician.

8.4 Special Linear Systems
277
A =
√a11 0
AT
12
√a11 I
  1
0
0 ˜L ˜LT
 √a11
A12
√a11
0
I

=
√a11 0
AT
12
√a11
˜L
 √a11
A12
√a11
0
˜LT

= LLT .
(8.57)
□
We rewrite Proposition 8.4 as the following algorithm.
Algorithm 8.10 Cholesky algorithm.
for j = 1 to n
l j j = (a j j − j−1
k=1 l2
jk)1/2
for i = j + 1 to n
li j = (ai j − j−1
k=1 likl jk)/l j j
end for
end for
Using this algorithm, we observe that if A is not positive deﬁnite, then the algo-
rithm fails by attempting to compute the square root of a negative number in line
l j j = (a j j − j−1
k=1 l2
jk)1/2 or by dividing by zero in lineli j = (ai j − j−1
k=1 likl jk)/l j j.
We conclude that running this algorithm is the cheapest way to test whether a sym-
metric matrix is positive deﬁnite.
In the Cholesky algorithm, L can overwrite the lower half of A. Only the lower
half of A is referred to in the algorithm, so in fact, only n(n + l)/2 storage is needed
instead of n2. The number of FLOPS in the Cholesky algorithm is (see Question 8.8,
p. 286)
n

j=1
⎛
⎝2 j +
n

i= j+1
2 j
⎞
⎠= 1
3n3 + O(n2).
(8.58)
We see that the Cholesky algorithm requires just half the FLOPS of Gaussian elim-
ination (see p. 46).
Pivoting is not necessary for Cholesky to be numerically stable. We show this as
follows. The same analysis as for Gaussian elimination in Section8.2.2 reveals that
we will have a similar formula for the error E in Cholesky decomposition as in LU
decomposition:
A = LLT + E,
where the error in Cholesky decomposition will be bounded as
|E| ≤nε|L||LT |.
Taking norms, we get
∥E∥≤nε∥|L| ∥∥|LT | ∥.

278
8
Solving Systems of Linear Equations
We can rewrite the expression above as
∥E∥≤nε∥L∥∥LT ∥.
(8.59)
Thus in formula (8.59), we have obtained an error estimate in the decomposition
A = LLT . The next step is to obtain the error in the Cholesky algorithm. We again
solve LLT x = b via Ly = b and LT x = y. Solving Ly = b gives as a computed
solution ˆy such that (L + δL) ˆy = b, where |δL| ≤nε|L|. The same is true for (LT +
δLT )ˆx = ˆy with |δLT | ≤nε|LT |. Combining both estimates, we get
b = (L + δL) ˆy = (L + δL)(LT + δLT )ˆx
= (LLT + LδLT + δLLT + δLδLT )ˆx
= (A −E + LδLT + δLLT + δLδLT )ˆx
= (A + δA)ˆx,
(8.60)
where δA = −E + LδLT + δLLT + δLδLT . Now we combine all bounds for E,
δLT , and δL and use the triangle inequality to get
|δA| ≤| −E + LδLT + δLLT + δLδLT |
≤|E| + |L||δLT | + |δL||LT | + |δL||δLT |
≤nε|L||LT | + nε|L||LT | + nε|L||LT | + n2ε2|L||LT |
≈3nε|L||LT |.
(8.61)
Assuming that ∥|X|∥= ∥X∥is true (as before, this is valid for Frobenius, inﬁnity
and one-norms but not for two-norms), we obtain
∥δA∥≤3nε∥L∥∥LT ∥.
(8.62)
Thus, it follows from (8.62) that the computed solution ˜x satisﬁes (A + δA)˜x = b
with |δA| ≤3nε|L||LT |. But by the Cauchy–Schwartz inequality and Proposi-
tion 8.3, we see that for every entry (i, j) of |L||LT |, we can write the estimate
(|L||LT |)i j =
n

k
|lik||l jk| ≤




n

k
l2
ik




n

k
l2
jk = √aii√a j j ≤max
i j
|ai j|.
Then applying this estimate to all n entries of |L||LT |, we have
∥|L||LT |∥∞≤n∥A∥∞.
(8.63)

8.4 Special Linear Systems
279
Substituting (8.63) into (8.62), we get the following estimate:
∥δA∥∞≤3n2ε∥A∥∞,
(8.64)
which says that ∥δA∥∞has an upper bound depending on ∥A∥∞, but not on ∥L∥∞.
This estimate is also valid for Frobenius and one-norms.
8.4.2
Symmetric Indeﬁnite Matrices
Let us consider now indeﬁnite matrices that are neither positive deﬁnite nor negative
deﬁnite. The question is whether there exists an algorithm that can solve a symmetric
indeﬁnite linear system of equations and save half the time and half the space. It turns
out that this is possible with a more complicated pivoting scheme and factorization.
If A is nonsingular, one can show that there exist a permutation matrix P, a unit
lower triangular matrix L, and a block diagonal matrix D with 1 × 1 and 2 × 2 blocks
such that PAPT = L DLT . This algorithm is described in [16].
8.4.3
Band Matrices
A matrix A is called a band matrix with lower bandwidth bL and upper bandwidth bU
if ai j = 0 whenever i > j + bL or i < j −bU:
A =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
a11
· · ·
a1,bU +1
0
...
a2,bU +2
abL+1,1
...
abL+2,2
an−bU ,n
...
...
0
an,n−bL · · ·
an,n
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
Band matrices often arise in practice and are useful to recognize because their L
and U factors are also “essentially banded.” This makes them cheaper to compute
and store.
Let the matrix A be banded with lower bandwidth bL and upper bandwidth bU.
Then after Gaussian elimination with partial pivoting, U is banded with upper band-
width at most bL + bU, and L is “essentially banded” with lower bandwidth bL. This
means that L has at most bL + 1 nonzero entries in each column and so can be stored
in the same space as a band matrix with lower bandwidth bL.
For the case L = U = 1, the band matrix A becomes a tridiagonal matrix. There
exists a special tridiagonal matrix algorithm known as Thomas’s10 algorithm, which
10Llewellyn Hilleth Thomas (1903—1992) was a British physicist and applied mathematician.

280
8
Solving Systems of Linear Equations
solves such linear system of equations. This method is a simpliﬁed form of Gaussian
elimination and was proposed by Gelfand11 and Lokucievsky12 in 1952 and then
modiﬁed by different researchers. In general, this algorithm is stable only for s.p.d.
or for diagonally dominant matrices; see details in [53].
The method is derived as follows. The tridiagonal system of linear equations
Ax = b can be written in the form
ai−1,ixi−1 + ai,ixi + ai+1,ixi+1 = bi,
i = 1, ..., n −1.
(8.65)
The tridiagonal matrix algorithm is based on the assumption that the solution can be
obtained as
xi = αi+1xi+1 + βi+1,
i = n −1, ..., 1.
(8.66)
Writing (8.66) for i −1, we get
xi−1 = αixi + βi
= αi(αi+1xi+1 + βi+1) + βi = αiαi+1xi+1 + αiβi+1 + βi.
(8.67)
Substituting (8.66) and (8.67) into (8.65), we obtain
ai−1,i(αiαi+1xi+1 + αiβi+1 + βi)
+ ai,i(αi+1xi+1 + βi+1) + ai+1,ixi+1 = bi,
i = 1, ..., n −1.
(8.68)
The equation above can be rewritten in the form
(ai−1,iαiαi+1 + ai,iαi+1 + ai+1,i)xi+1
+ ai−1,iαiβi+1 + ai−1,iβi + ai,iβi+1 −bi = 0,
i = 1, ..., n −1.
(8.69)
The Eq.(8.69) will be satisﬁed if we require that
ai−1,iαiαi+1 + ai,iαi+1 + ai+1,i = 0,
ai−1,iαiβi+1 + ai−1,iβi + ai,iβi+1 −bi = 0.
(8.70)
From (8.70), it follows that
αi+1 = −
ai+1,i
ai−1,iαi + ai,i
,
βi+1 = bi −ai−1,iβi
ai−1,iαi + ai,i
.
(8.71)
11Israel Moiseevich Gelfand (1913–2009) was a Russian mathematician.
12Oleg Vyacheslavovich Lokucievsky (1922—1990) was a Russian mathematician.

8.4 Special Linear Systems
281
Starting from i = 1, we can ﬁnd
α2 = −a2,1
a1,1
,
β2 = b1
a1,1
.
(8.72)
Then from (8.71), we can obtain all other coefﬁcients αi+1, βi+1, i = 1, ..., n −1,
recursively. Knowing all coefﬁcients αi+1, βi+1, i = 1, ..., n −1, we can obtain the
solution of the tridiagonal system of linear equations via (8.66).
Band matrices often arise from discretizing different physical problems in which
mathematical models are usually described by ordinary differential equation (ODE)
or by partial differential equations (PDE).
Example 8.5 Consider the following boundary value problem for the ODE
y′′(x) −p(x)y′(x) −q(x)y(x) = r(x), x ∈[a, b],
(8.73)
y(a) = α, y(b) = β.
(8.74)
We assume q(x) ≥qm > 0. Equation (8.73) models the heat ﬂow in a long pipe,
for example. To solve it numerically, we discretize it by seeking its solution only
at mesh points xi = a + ih, i = 0, . . . , N + 1, where h = (b −a)/(N + 1) is the
mesh size. Deﬁne pi = p(xi), ri = r(xi), and qi = q(xi).
To derive discretized equations for yi ≈y(xi) with boundary conditions y0 =
α, yN+1 = β we approximate the derivative y′(xi) by the following ﬁnite difference
scheme called central difference:
y′(xi) ≈yi+1 −yi−1
2h
.
(8.75)
As the mesh size h gets smaller, (8.75) approximates y′(xi) more and more accurately.
We can similarly approximate the second derivative by
y′′(xi) ≈yi+1 −2yi + yi−1
h2
.
(8.76)
Inserting approximations (8.75), (8.76) into (8.73) yields
yi+1 −2yi + yi−1
h2
−pi
yi+1 −yi−1
2h
−qi yi = ri,
1 ≤i ≤N.
(8.77)
Multiplying by h2/2, we can rewrite (8.77) in the form of a linear system Ay = b
with

282
8
Solving Systems of Linear Equations
y =
⎛
⎜⎜⎜⎜⎜⎝
y1
...
yN
⎞
⎟⎟⎟⎟⎟⎠
,
b = −h2
2
⎛
⎜⎜⎜⎜⎜⎝
r1
...
rN
⎞
⎟⎟⎟⎟⎟⎠
+
⎛
⎜⎜⎜⎜⎜⎝
 1
2 + h
4 p1

α
0
...
0
 1
2 −h
4 pN

β
⎞
⎟⎟⎟⎟⎟⎠
,
(8.78)
and
A =
⎛
⎜⎜⎜⎜⎝
a1 −c1
−b2
...
...
...
... −cN−1
−bN
aN
⎞
⎟⎟⎟⎟⎠
,
ai = 1 + h2
2 qi,
bi = 1
2

1 + h
2 pi

,
ci = 1
2

1 −h
2 pi

.
(8.79)
Since by our assumption we have qi > 0, it follows from (11.36) that ai > 0. For
sufﬁciently small h < 1, also bi > 0 and ci > 0.
System Ay = b is an asymmetric tridiagonal system. We will show how to change
it to a symmetric positive deﬁnite tridiagonal system so that we may use Cholesky
decomposition to solve it. Choose the diagonal matrix
D = diag

1,
 c1
b2
,
 c1c2
b2b3
, . . . ,
c1c2 · · · cN−1
b2b3 · · · bN

.
Then we may change Ay = b to (DAD−1)(Dy) = Db or ˜A ˜y = ˜b, where
˜A =
⎛
⎜⎜⎜⎜⎜⎜⎝
a1
−√c1b2
−√c1b2
a2
−√c2b3
−√c2b3
...
...
...
−√cN−1bN
−√cN−1bN
aN
⎞
⎟⎟⎟⎟⎟⎟⎠
.
We observe that ˜A is symmetric, and it has the same eigenvalues as A, because
A and ˜A = DAD−1 are similar. Let us prove that ˜A is positive deﬁnite. By the
Gershgorin disk theorem (see Theorem 7.6, p. 238), all the eigenvalues λ of an
arbitrary matrix B are located in the union of the n disks
|λ −bkk| ≤

j̸=k
|bkj|.
(8.80)
We can take the mesh size h so small that for all i, we have |h/2pi| < 1 in all
nondiagonal elements of the matrix A. Then
|bi| + |ci| = 1
2

1 + h
2 pi

+ 1
2

1 −h
2 pi

= 1 < 1 + h2
2 qm ≤1 + h2
2 qi = ai.

8.4 Special Linear Systems
283
1
x 1
0.5
u(x 1,x2) with N = 20
0
0
0.2
0.4
0.6
x 2
0.8
0.14
0
0.02
0.04
0.06
0.08
0.1
0.12
1
u(x 1,x 2)
0
0.02
0.04
0.06
0.08
0.1
0.12
x 1
0
0.2
0.4
0.6
0.8
1
x 2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
u(x 1,x2) with N = 20
0
0.02
0.04
0.06
0.08
0.1
0.12
1
x 1
0.5
u(x 1,x2) with N = 40
0
0
0.2
0.4
0.6
x 2
0.8
0.1
0.14
0.12
0.08
0.06
0.04
0.02
0
1
u(x 1,x 2)
0
0.02
0.04
0.06
0.08
0.1
0.12
x 1
0
0.2
0.4
0.6
0.8
1
x 2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
u(x 1,x2) with N = 40
0
0.02
0.04
0.06
0.08
0.1
0.12
1
x 1
0.5
u(x 1,x2) with N = 60
0
0
0.2
0.4
0.6
x 2
0.8
0
0.14
0.12
0.1
0.08
0.06
0.04
0.02
1
u(x 1,x 2)
0
0.02
0.04
0.06
0.08
0.1
0.12
x 1
0
0.2
0.4
0.6
0.8
1
x 2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
u(x 1,x2) with N = 60
0
0.02
0.04
0.06
0.08
0.1
0.12
Fig. 8.4 Solution of Poisson’s equation (8.11) using Cholesky decomposition in the example of
Section8.4.4.
Thus using (8.80), we see that all the eigenvalues of A lie inside the disks Di centered
at ai = 1 + h2qi/2 ≥1 + h2qm/2 with radius Ri = 1. They must all have positive
real parts.
Since ˜A is symmetric, its eigenvalues are real and positive. Then the matrix ˜A is
positive deﬁnite. Its smallest eigenvalue is bounded below by qmh2/2. Thus, it can
be solved by Cholesky decomposition.

284
8
Solving Systems of Linear Equations
8.4.4
A Numerical Example
In this section we illustrate performance of the Cholesky algorithm on the solu-
tion of Poisson’s equation (8.11) on the unit square {(x1, x2) : 0 ≤x1, x2 ≤1} with
boundary conditions u = 0 on the boundary of this square. We take now in (8.11)
the following function f (x1, x2) given by two choices of Gaussians:
f (x1, x2) = 1 + A1 exp

−(x1 −0.25)2
0.02
−(x2 −0.25)2
0.02

+ A2 exp

−(x1 −0.75)2
0.02
−(x2 −0.75)2
0.02

,
(8.81)
where A1, A2 are the amplitudes of these two Gaussians.
As in the example of Section8.1.3, we construct a mesh with the points (x1i, x2 j)
such that x1i = ih, x2 j = jh with h = 1/(N + 1), where N is the number of inner
points in the x1 and x2 directions, and we take the same number of points in the
x1 and x2 directions: ni = n j = N + 2. The linear system of equations Au = f
is solved then via Cholesky factorization (Algorithm 8.10). We can use Cholesky
decomposition, since the matrix A is symmetric positive deﬁnite (see Question 8.16,
p. 289).
Figure8.4 shows results of numerical simulations for different discretizations of
the unit square with number of inner points N = 20, 40, 60 and for A1 = A2 = 10
in (8.81). The MATLAB® program of Section 1.2 are available for running this
test.13
Questions
8.1 (Programming)
Solve in MATLAB® Poisson’s equation (8.11) on the unit square {(x1, x2) : 0 ≤
x1, x2 ≤1} with boundary conditions u = 0 on the boundary of this square and with
function f (x1, x2) given by (8.81) using the lu function and programs of Section1.1.
Compare the obtained results with those of Section8.4.4. Optional: extend these
results to three-dimensional case.
8.2 (Programming)
Improve Algorithm 8.1, overwriting L and U on A. Write your own MATLAB®
program and test it on your own examples.
8.3 (Programming)
Apply the bisection algorithm (see Algorithm 8.11 below) to ﬁnd the roots of the
polynomial p(x) = (x −2)9 and of some of your own polynomials, where p(x) is
13All MATLAB® programs referenced on this page can be found on the online version of Chapter 1
(doi:10.1007/978-3-319-57304-5_1).

Questions
285
evaluated using Horner’s rule (Algorithm 8.4). Write your own MATLAB® program.
Conﬁrm that changing the input interval for x = [xlef t, xright] slightly changes the
computed root drastically. Modify the algorithm to use the relative condition number
for polynomial evaluation (8.26) to stop bisecting when the rounding error in the
computed value of p(x) gets so large that its sign cannot be determined. Present
your results similarly to the results of Fig.8.2.
Hint: use the MATLAB® function coeffs to compute the coefﬁcients of a poly-
nomial p(x).
Here we present the bisection algorithm to ﬁnd roots of a polynomial p(x).
Suppose that the input interval for x where we want to ﬁnd roots of p(x) = 0 is
x ∈[xlef t, xright]. At every iteration, this algorithm divides the input interval in two
by computing the midpoint xmiddle = (xlef t + xright)/2 of the input interval as well
as the value of the polynomial p(xmiddle) at that point. We will compute the value
of the polynomial p(xmiddle) using Horner’s rule (Algorithm 8.4). Then if plef t and
pmid have opposite signs, the bisection algorithm sets xmiddle as the new value for
xright, and if pright and pmid have opposite signs, then the method sets xmiddle as the
new xlef t. If p(xmiddle) = 0, then xmiddle may be taken as the root of the polynomial
and the algorithm stops.
Algorithm 8.11 Bisection algorithm to ﬁnd zeros of the polynomial p(x).
0. Initialization: set the left xlef t and the right xright bounds for the input interval for
x ∈[xlef t, xright] where we will seek roots of the polynomial. Set the computa-
tional tolerance tol.
1. Evaluate the polynomial p(x) at the points xlef t and xright to get plef t = p(xlef t)
and pright = p(xright) using Algorithm 8.4.
Perform steps 2–3 while xright −xlef t > 2tol
2. Compute point xmid = (xlef t + xright)/2 and then pmid = p(xmid), using Algo-
rithm 8.4.
3. Check:
If plef t pmid < 0, then we have a root in the interval [xlef t, xmid]. Assign xright =
xmid and pright = pmid.
Else if pright pmid < 0, then we have a root in the interval [xmid, xright]. Assign
xlef t = xmid and plef t = pmid.
Else we have found a root at xmid and assign xlef t = xmid, xright = xmid.
4. Compute the root as (xlef t + xright)/2.
8.4 Write Algorithm 8.1 for the case n = 3. Using Algorithm 8.1, perform LU
factorization of the matrix
A =
⎛
⎝
4
1
1
1
8
1
1
1
16
⎞
⎠.
8.5 Using Cholesky’s algorithm (Algorithm 8.10), perform the factorization A =
LLT of the matrix A in Question 8.4.

286
8
Solving Systems of Linear Equations
8.6 (Programming)
Implement Hager’s algorithm, Algorithm 8.7, in MATLAB®. Test it on different
matrices. Take, for example, A = hilb(N) or A = rand(N, N) for different N.
8.7 Let us consider the solution of the linear system AX = B, where A is an n × n
matrix, B is an n × m matrix, and X is an unknown n × m matrix. We have two
methods to solve it:
1. Factorization of A = PLU and then using the algorithms of forward and back-
ward substitution, Algorithms 8.2 and 8.3 to ﬁnd every column of X.
2. Computation of A−1 by Gaussian elimination and then ﬁnding of X = A−1B.
Count the number of FLOPS required for every algorithm. Show that the ﬁrst algo-
rithm requires fewer FLOPS than the second one.
Hint: use material of Section8.2.2.
8.8 Derive formula (8.58) for the operation count in Cholesky decomposition. Hint:
use the formula
n

i=1
i2 = (n + 1)n(2n + 1)
6
and the formula for the sum of an arithmetic progression,
n

k=1
ai = n(a1 + an)
2
.
8.9 Let A be an s.p.d. matrix. Show that |ai j| < √aiia j j.
8.10 Suppose A is an invertible nonsingular square matrix of order n and that u, v are
vectors. Suppose furthermore that 1 + vT A−1u ̸= 0. Prove the Sherman–Morrison
formula
(A + uvT )−1 = A−1 −A−1uvT A−1
1 + vT A−1u .
Here, uvT is the outer product of two vectors u and v.
8.11 Suppose A is an invertible square matrix of order n and that U, V are n × k
rectangular matrices with k ≤n. Prove the Sherman–Morrison–Woodburg formula,
which states that T = I + V T A−1U is nonsingular if and only if A + UV T is non-
singular and
(A + UV T )−1 = A−1 −A−1UT −1V T A−1.
8.12 (Programming)
Similarly, with MATLAB® programs of Sections 1.1, 1.2, solve the three-
dimensional problem14
14The
MATLAB®
programs
can
be
found
on
the
online
version
of
Chapter
1
(doi:10.1007/978-3-319-57304-5_1).

Questions
287
−△u(x) = f (x) in Ω,
u = 0 on ∂Ω
(8.82)
on the unit cube Ω = [0, 1] × [0, 1] × [0, 1]. Choose an appropriate function f (x).
Hint. We discretize the unit cube Ω with x1i = ih1, x2 j = jh2, x3k = kh3, where
h1 =
1
ni −1,
h2 =
1
n j −1,
h3 =
1
nk −1
are the steps of the discrete ﬁnite difference mesh and ni, n j, nk are the numbers
of discretization points in the directions x1, x2, x3, respectively. The indices (i, j, k)
are such that 0 ≤i < ni, 0 ≤j < n j, 0 ≤j < nk. The global node numbers nglob
in the three-dimensional case can be computed as
nglob = j + n j ((i −1) + ni(k −1)) .
(8.83)
We take ni = n j = nk = n = N + 2, h1 = h2 = h3 = 1/(n −1) = 1/(N + 1) and
obtain the following scheme for the solution of Poisson’s equation (8.11) in three
dimensions:
−ui+1, j,k −2ui, j,k + ui−1, j,k
h2
1
−ui, j+1,k −2ui, j,k + ui, j−1,k
h2
2
−ui, j,k+1 −2ui, j,k + ui, j,k−1
h2
3
= fi, j,k
ai, j,k
,
(8.84)
where ui, j,k, fi, j,k, ai, j,k are values of u, f, a, respectively, at the discrete point nglob
with indices (i, j, k). We rewrite Eq.(8.84) with h = h1 = h2 = h3 as
6ui, j,k −ui+1, j,k −ui−1, j,k −ui, j+1,k −ui, j−1,k −ui, j,k+1 −ui, j,k−1 = h2 fi, j,k
ai, j,k
.
(8.85)
Again, we recognize that the scheme (8.85) is a system of linear equations Au = b.
The matrix A is of size (ni −2)(n j −2)(nk −2) = N 3, and on the unit cube it is
given by the block matrix
A =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
AN −IN ON −IN
...
−IN
AN −IN
...
...
...
...
...
...
...
−IN
... −IN AN −IN
... −IN ON −IN
AN
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠

288
8
Solving Systems of Linear Equations
with zero blocks ON of order N. The blocks AN of size N × N on the main diagonal
of this matrix are given by
AN =
⎛
⎜⎜⎜⎜⎝
6 −1
0 · · · · · ·
0
−1
6 −1
0 · · ·
0
0 −1
6
0 · · ·
0
· · · · · · · · · · · · · · · · · ·
0 · · · · · ·
0 −1
6
⎞
⎟⎟⎟⎟⎠
.
8.13 (Programming)
Use the MATLAB® programs of Sections 1.1, 1.2 and solve the problem in
Example 8.2 on an L-shaped 2D domain.15
8.14 (Programming)
Use formula (8.80) to estimate the eigenvalues of the matrix
A =
⎛
⎜⎜⎝
10 −1 0
1
0.2 8 0.2 0.2
1
1
2
1
−1 −1 −1 −11
⎞
⎟⎟⎠.
(8.86)
Write your own MATLAB® program to present results similar to those in Fig.8.5,
which shows Gershgorin disks together with their centers and the computed eigen-
values λi. Hint: use the MATLAB® function eigs(A) to compute eigenvalues.
8.15 (Programming)
Use formula (8.80) to estimate the eigenvalues of the matrix
A =
⎛
⎜⎜⎝
7 5 2 1
2 8 3 2
1 1 5 1
1 1 1 6
⎞
⎟⎟⎠.
(8.87)
−15
−10
−5
0
5
10
15
−3
−2
−1
0
1
2
3
 D(10,2)
 D(8,0.6)
 D(2,3)
D(−11,3)
eigenvalues
centers
Fig. 8.5 Eigenvalues in Question 8.14 computed and estimated by the Gershgorin disk theorem.
The computed eigenvalues of A are λ1 = 9.8218, λ2 = 8.1478, λ3 = 1.8995, λ4 = −10.86.
15The
MATLAB®
programs
can
be
found
on
the
online
version
of
Chapter
1
(doi:10.1007/978-3-319-57304-5_1).

Questions
289
−2
0
2
4
6
8
10
12
14
16
−8
−6
−4
−2
0
2
4
6
8
 D(7,8)
 D(8,7)
 D(5,3)
D(6,3)
eigenvalues
centers
Fig. 8.6 Eigenvalues in Question 8.15 computed and estimated by the Gershgorin disk theorem.
The computed eigenvalues are λ1 = 12.2249 + 0.0000i, λ2 = 4.4977 + 0.6132i, λ3 = 4.4977 −
0.6132i, λ4 = 4.7797 + 0.0000i.
Write your own MATLAB® program to present results similar to those in Fig.8.6,
which shows Gershgorin disks together with their centers and the computed eigen-
values λi. Hint: use the MATLAB® function eigs(A) to compute eigenvalues.
8.16 Prove that the matrix A in the numerical examples of Sections8.1.3 and 8.4.4
is s.p.d. Hint: prove that for all u, v ∈R(N+2)2 that equal zero on the boundary of the
mesh domain, we can write
(Au, v) =
N−1

i, j=1
(ui+1, j −ui, j)(vi+1, j −vi, j) +
N−1

i, j=1
(ui, j+1 −ui, j)(vi, j+1 −vi, j).
8.17 (Programming)
Write your own MATLAB® program and solve numerically the problem of
Example 8.5 via the solution of the system of linear equations Ay = b with b, A
given in (8.78), (8.79), respectively. Use the recurrence formulas (8.66), (8.71) to
obtain the solution of this system.

Chapter 9
Numerical Solution of Linear Least
Squares Problems
In this chapter we present methods for numerical solution of linear least squares
problems. These problems arise in many real-life applications such that curve ﬁtting,
statistical modelling and different inverse problems, when some model function
should be ﬁtted to the measured data.
Various matrix factorizations are usually applied to solve linear least squares
problems (LLSP). In this chapter we will present several methods for the solution of
LLSP:
1. Method of normal equations,
2. QR decomposition,
3. SVD decomposition.
The method of normal equations is widely used, since it is the fastest compared
with all other methods for the solution of LLSP. However, this method is the least
accurateandcanbeusedonlywhentheconditionnumberofthematrix A issmall.The
method of factorization of a matrix A into two matrices Q and R such that A = QR,
where Q is orthogonal and R is upper triangular, is called QR decomposition. This
method is more accurate than the method of normal equations and is standard for the
solution of LLSP. A drawback of this method is that it costs twice as much as the
method of normal equations. When a matrix A is very ill conditioned, for example
when A does not have full rank, then matrix factorization called SVD decomposition
is commonly used for the solution of LLSP. However, like QR decomposition, this
methodisseveraltimesmoreexpensivethanthemethodofnormalequations.Another
method for the solution of very ill conditioned LLSP is iterative reﬁnement, which
iteratively improves the solution of a linear system of equations. This method can be
adapted to deal efﬁciently with sparse matrices; see [14] for details.
In Section9.1 we present the topic of LLSP and some typical examples of their
application. In Section9.2 we brieﬂy present the main methods that also solve nonlin-
ear least squares problem. Different methods for the solution of LLSP are described
© Springer International Publishing AG 2017
L. Beilina et al., Numerical Linear Algebra: Theory and Applications,
DOI 10.1007/978-3-319-57304-5_9
291

292
9
Numerical Solution of Linear Least Squares Problems
in the following sections: the method of normal equations is presented in Section9.3,
QR decomposition is outlined in Section9.4, orthogonalization methods to perform
QR decomposition are described in Section9.5, SVD decomposition is presented in
Section9.6. Rank-deﬁcient least squares problems and how to solve them are dis-
cussed in Sections9.6.1 and 9.6.2, respectively. Available software for the solution
of LLSP is outlined in Section9.7.
9.1
Linear Least Squares Problems
Suppose that we have a matrix A of size m × n and a vector b of size m × 1. The
linear least square problem is to ﬁnd a vector x of size n × 1 that will minimize
∥Ax −b∥2. In the case that m = n and the matrix A is nonsingular, we can obtain a
solution to this problem as x = A−1b. However, when m > n (more equations than
unknowns), the problem is called overdetermined. Conversely, when m < n (more
unknowns than equations), the problem is called underdetermined.
Inreal-lifeapplications,engineersmorecommonlydealwithoverdeterminedleast
squares problems in which the number of equations is much larger than the number
of unknowns. This occurs because engineers usually take many more measurements
than necessary to smooth out measurement error and remove noise from data. We
will restrict our considerations to linear least squares problems. We refer to [14] and
to the next section for the solution of nonlinear least squares problems.
Further, we assume that we are dealing with overdetermined problems, in which
we have more equations than unknowns. This means that we will be interested in the
solution of a linear system of equations
Ax = b,
(9.1)
where A is of size m × n with m > n, b is a vector of length m, and x is a vector of
length n.
In a general case we are not able to get vector b of the size m as a linear combination
of the n columns of the matrix A and n components of the vector x, from what follows
that there is no solution to (9.1). In this chapter we will consider methods that can
minimize the residual r = b −Ax as a function of x in principle in any norm, but we
will use only the 2-norm for theoretical and computational convenience (relationships
between the 2-norm and the inner product and orthogonality, smoothness, and strict
convexity properties). The 2-norm method is also called the method of least squares.
Wecanwritealeastsquaresproblemasaproblemofminimizingthesquaredresiduals
∥r∥2
2 =
m

i=1
r2
i =
m

i=1
(Axi −b)2.
(9.2)

9.1 Linear Least Squares Problems
293
In other words, our goal is to ﬁnd the minimum of this residual using least squares:
min
x
∥r∥2
2 = min
x
m

i=1
r2
i = min
x
m

i=1
(Axi −b)2.
(9.3)
Example 9.1 Data ﬁtting.
In this example we present a typical application of least squares called data ﬁt-
ting or curve ﬁtting. This problem appear in statistical modeling and experimental
engineering when data are generated by laboratory or other measurements.
Suppose that we have data points (xi, yi), i = 1, ..., m, and our goal is to ﬁnd the
vector of parameters c of size n that will ﬁt best the data yi of the model function
f (xi, c), where f : Rn+1 →R, in the least squares sense:
min
c
m

i=1
(yi −f (xi, c))2.
(9.4)
If the function f (x, c) is linear, then we can solve the problem (9.4) using the
least squares method. The function f (x, c) is linear if we can write it as a linear
combination of the functions φ j(x), j = 1, ..., n as
f (x, c) = c1φ1(x) + c2φ2(x) + ... + cnφn(x).
(9.5)
The functions φ j(x), j = 1, ..., n are called basis functions.
Let now the matrix A have entries ai j = φ j(xi), i = 1, ..., m; j = 1, ..., n, and the
vector b will be such that bi = yi, i = 1, ..., m. Then a linear data-ﬁtting problem
takes the form of (9.1) with x = c:
Ac ≈b.
(9.6)
The elements of the matrix A are created by the basis functions φ j(x), j = 1, ..., n.
We will now consider different examples of choosing the basis functions φ j(x), j =
1, ..., n.
Example 9.2 Problem of the ﬁtting to a polynomial.
In the problem of ﬁtting a polynomial
f (x, c) =
d

i=1
cixi−1
(9.7)
of degree d −1 to data points (xi, yi), i = 1, ..., m, the basis functions φ j(x), j =
1, ..., n can be chosen as φ j(x) = x j−1, j = 1, ..., n. The matrix A constructed by
these basis functions in a polynomial-ﬁtting problem is a Vandermonde matrix:

294
9
Numerical Solution of Linear Least Squares Problems
A =
⎛
⎜⎜⎜⎜⎜⎝
1 x1 x2
1 . . . xd−1
1
1 x2 x2
2 . . . xd−1
2
1 x3 x2
3 . . . xd−1
3
...
... ... ...
...
1 xm x2
m . . . xd−1
m
⎞
⎟⎟⎟⎟⎟⎠
.
(9.8)
Here xi, i = 1, ...., m, are discrete points on the interval x = [xlef t, xright]. Sup-
pose, that we choose d = 4 in (9.7). Then we can write the polynomial as f (x, c) =
	4
i=1 cixi−1 = c1 + c2x + c3x2 + c4x3, and our data-ﬁtting problem (9.6) for this
polynomial takes the form
⎛
⎜⎜⎜⎜⎜⎝
1 x1 x2
1 x3
1
1 x2 x2
2 x3
2
1 x3 x2
3 x3
3
...
... ... ...
1 xm x2
m x3
m
⎞
⎟⎟⎟⎟⎟⎠
·
⎛
⎜⎜⎝
c1
c2
c3
c4
⎞
⎟⎟⎠=
⎛
⎜⎜⎜⎜⎝
b0
b1
b2
...
bm
⎞
⎟⎟⎟⎟⎠
.
(9.9)
Theright-handsideoftheabovesystemrepresentsmeasurementsofafunctionthat
we want to ﬁt. Our goal is to ﬁnd coefﬁcients c = {c1, c2, c3, c4} that will minimize
the residualri = f (xi, c) −bi, i = 1..., m. Since we want to minimize the squared 2-
norm of the residual, or ∥r∥2
2 = 	m
i=1 r2
i , we will solve a linear least squares problem.
Let us consider an example in which the right-hand side bi, i = 1, ...m is taken
as a smooth function b = sin(πx/5) + x/5. Figure9.1 shows ﬁtting a polynomial
to the function b = sin(πx/5) + x/5 for different values of d in (9.7) in the inter-
val x ∈[−10, 10]. We observe in this ﬁgure that with increasing degree d −1 of
the polynomial, we have a better ﬁt to the exact function b = sin(πx/5) + x/5.
However, for a degree of the polynomial greater than 18, we get an erratic ﬁt to the
function. Check this using the MATLAB® programs of Section1.4.1 This happens
because the matrix A becomes more and more ill-conditioned with increasing degree
d of the polynomial. And this, in turn, is because of the linear dependence of the
columns in the Vandermonde matrix A.
Example 9.3 Approximation using linear splines.
When we want to solve the problem (9.4) of approximation to the data vector
yi, i = 1, ..., m, with linear splines, we use the basis functions φ j(x), j = 1, ..., n,
in (9.5), which are also called hat functions:
φ j(x) =
⎧
⎨
⎩
x−Tj−1
Tj−Tj−1 , Tj−1 ≤x ≤Tj,
Tj+1−x
Tj+1−Tj , Tj ≤x ≤Tj+1.
(9.10)
Here, the column j in the matrix A is constructed by the given values of φ j(x) at
points Tj, j = 1, .., n, which are called conjunction points and are chosen by the
1The
MATLAB®
programs
can
be
found
on
the
online
version
of
Chapter
1
(doi:10.1007/978-3-319-57304-5_1).

9.1 Linear Least Squares Problems
295
x
-10
-5
0
5
10
-2.5
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
2.5
exact function
 computed
x
-10
-5
0
5
10
-2.5
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
2.5
exact function
 computed
a) d=10
b) d=10
x
-10
-5
0
5
10
-3
-2
-1
0
1
2
3
exact function
 computed
x
-10
-5
0
5
10
-2.5
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
2.5
exact function
 computed
c) d=5
d) d=5
x
-10
-5
0
5
10
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
exact function
 computed
x
-10
-5
0
5
10
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
exact function
 computed
e) d=3
f) d=3
Fig. 9.1 Polynomial ﬁtting for different degrees d in (9.7) to the function b = sin(πx/5) + x/5
on the interval x ∈[−10, 10] using the method of normal equations. Left-hand ﬁgures: ﬁt to 100
points xi, i = 1, ..., 100; Right-hand ﬁgures: ﬁt to 10 points xi, i = 1, ..., 10. Lines with blue stars
represent the computed function, those with red circles, the exact function.
user. Using (9.10), we may conclude that the ﬁrst basis function is φ1(x) = T2−x
T2−T1
and the last one is φn(x) =
x−Tn−1
Tn−Tn−1 .

296
9
Numerical Solution of Linear Least Squares Problems
-10
-5
0
5
10
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
function
approx
-10
-5
0
5
10
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
function
approx
a) n=10
b) n=5
Fig. 9.2 Example9.3. Polynomial ﬁtting to the function b = sin(πx/5) + x/5 on the interval
x ∈[−10, 10] using linear splines with different numbers n of conjunction points Tj, j = 1, ..., n,
in (9.10). Blue stars represent the computed function, and red circles, the exact function.
Figure9.2 shows an approximation of the function b = sin(πx/5) + x/5 on the
interval x ∈[−10, 10] using linear splines with different numbers n of conjunction
points Tj, j = 1, ..., n. A MATLAB® program is available for running this test in
Section1.7.2
Example 9.4 Approximation using bellsplines.
To solve the problem (9.4) using bellsplines, the number of bellsplines that can
be constructed is n + 2, and the function f (x, c) in (9.4) is written as
f (x, c) = c1φ1(x) + c2φ2(x) + ... + cn+2φn+2(x).
(9.11)
We deﬁne
φ0
j(x) =
1, Tj ≤x ≤Tj+1,
0, otherwise.
(9.12)
Then all other basis functions, or bellsplines, φk
j(x), j = 1, ..., n + 2; k = 1, 2, 3,
are deﬁned as follows:
φk
j(x) = (x −Tk)
φk−1
j
(x)
Tj+k −Tj
+ (Tj+k+1 −x)
φk−1
j+1(x)
Tj+k+1 −Tj+1
.
(9.13)
Here, the column j in the matrix A is constructed by the given values of φ j(x) at con-
junction points Tj, j = 1, .., n, which are chosen by the user. If in (9.13) we obtain the
ratio 0/0, then we assign φk
j(x) = 0. We deﬁne three additional points T−2, T−1, T0 at
the left side of the input interval as T−2 = T−1 = T0 = T1, and correspondingly three
points Tn+1, Tn+2, Tn+3 on the right side of the interval as Tn = Tn+1 = Tn+2 = Tn+3.
Altogether, we have n + 6 conjunction points Tj, j = 1, ..., n + 6. The number of
bellsplines that can be constructed is n + 2.
2The
MATLAB®
programs
can
be
found
on
the
online
version
of
Chapter
1
(doi:10.1007/978-3-319-57304-5_1).

9.1 Linear Least Squares Problems
297
If the conjunction points Tj are distributed uniformly, then we can introduce the
mesh size h = Tk+1 −Tk, and the bellsplines can be written explicitly as
φ j(x) =
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
1
6t3
if Tj−2 ≤x ≤Tj−1, t = 1
h (x −Tj−2),
1
6 + 1
2(t + t2 −t3) if Tj−1 ≤x ≤Tj, t = 1
h (x −Tj−1),
1
6 + 1
2(t + t2 −t3) if Tj ≤x ≤Tj+1, t = 1
h (Tj+1 −x),
1
6t3
if Tj+1 ≤x ≤Tj+2, t = 1
h (Tj+2 −x).
(9.14)
In the case of uniformly distributed bellsplines, we place additional points at
the left side of the input interval as T0 = T1 −h, T−1 = T1 −2h, T−2T1 −3h, and
correspondingly on the right side of the interval as Tn+1 = Tn + h, Tn+2 = Tn +
2h, Tn+3 = Tn + 3h. Then the function f (x, c) in (9.4) will be the following linear
combination of n + 2 functions φ j(x) for indices j = 0, 1, ..., n + 1:
f (x, c) = c1φ0(x) + c2φ1(x) + ... + cn+2φn+1(x).
(9.15)
Figure9.3 shows an approximation of the function b = sin(πx/5) + x/5 on the
interval x ∈[−10, 10] using bellsplines. A MATLAB® program is available in
Section1.8 for running this test.3
9.2
Nonlinear Least Squares Problems
Suppose that for our data points (xi, yi), i = 1, ..., m, we want to ﬁnd the vector of
parameters c = (c1, ..., cn) that will best ﬁt the data yi, i = 1, ..., m, of the model
function f (xi, c), i = 1, ..., m. We now consider the case that the model function
f : Rn+1 →R is nonlinear. Our goal is to ﬁnd the minimum of the residual r =
y −f (x, c) in the least squares sense:
min
c
m

i=1
(yi −f (xi, c))2.
(9.16)
To solve problem (9.16) we can still use the linear least squares method if we can
transform the nonlinear function f (x, c) to a linear one. This can be done if the
function f (x, c) can be represented in the form f (x, c) = Aecx, A = const. Then
taking the logarithm of f (x, c), we get ln f = ln A + cx, which is already a linear
function. Then the linear least squares problem after this transformation can be
written as
min
c
m

i=1
(ln yi −ln f (xi, c))2.
(9.17)
3The
MATLAB®
programs
can
be
found
on
the
online
version
of
Chapter
1
(doi:10.1007/978-3-319-57304-5_1).

298
9
Numerical Solution of Linear Least Squares Problems
0
2
4
6
8
10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 number of bellsplines=5
-10
-5
0
5
10
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
exact
B-spline degree 3, QR
0
2
4
6
8
10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 number of bellsplines=7
-10
-5
0
5
10
-2.5
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
2.5
exact
B-spline degree 3, QR
-10
-5
0
5
10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 number of bellsplines=9
-10
-5
0
5
10
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
exact
B-spline degree 3, QR
Fig. 9.3 Example9.4. Polynomial ﬁtting to the function b = sin(πx/5) + x/5 on the interval
x ∈[−10, 10] with different numbers of bellsplines. Blue stars represent the computed function,
and red circles, the exact function.
Another possibility for dealing with nonlinearity is to consider the least squares
problem as an optimization problem. Let us deﬁne the residual r : Rn →Rm as
ri(c) = yi −f (xi, c), i = 1, ..., m.
(9.18)

9.2 Nonlinear Least Squares Problems
299
Our goal is now to minimize the function
F(c) = 1
2r(c)Tr(c) = 1
2∥r(c)∥2
2.
(9.19)
To ﬁnd the minimum of (9.19), we should have
∇F(c) = ∂F(c)
∂ci
= 0, i = 1, ..., m.
(9.20)
Direct computations show that the gradient vector ∇F(c) is
∇F(c) = dF
dc = J T (c)r(c),
(9.21)
where J T is the transposed Jacobian matrix of the residual r(c). For a sufﬁciently
smooth function F(c), we can write its Taylor expansion as
F(c) = F(c0) + ∇F(c0)(c −c0) + O(h2),
(9.22)
with |h| = ∥c −c0∥. Since our goal is to ﬁnd the minimum of F(c), at a minimum
point c∗we should have ∇F(c∗) = 0. Taking the derivative with respect to c from
(9.22), we obtain
H(F(c0))(c −c0) + ∇F(c0) = 0,
(9.23)
where H denotes the Hessian matrix of the function F(c0). Using (9.21) in (9.23),
we obtain
H(F(c0))(c −c0) + J T (c0)r(c0) = 0,
(9.24)
and from this expression we observe that we have obtained a system of linear equa-
tions
H(F(c0))(c −c0) = −J T (c0)r(c0),
(9.25)
which can be solved again using the linear least squares method. The Hessian matrix
H(F(c0)) can be obtained from (9.21) as
H(F(c0)) = J T (c0)J(c0) +
m

i=1
ri(c0)H(ri),
(9.26)
where H(ri) denotes the Hessian matrix of the residual function ri(c). These m
matrices H(ri) are inconvenient to compute, but since they are multiplied by the
small residuals ri(c0), the second term in (9.26) is often very small in the solution c0
and can be omitted. Then the system (9.25) is transformed to the linear system
J T (c0)J(c0)(c −c0) ≈−J T (c0)r(c0),
(9.27)

300
9
Numerical Solution of Linear Least Squares Problems
which actually is a system of normal equations for the m × n linear least squares
problem
J(c0)(c −c0) ≈−r(c0).
(9.28)
Using (9.27) we can write the iterative formula for the solution of the least squares
problem
ck+1 = ck −[J T (ck)J(ck)]−1J T (ck)r(ck),
(9.29)
where k is the number of the iteration. Method which uses (9.29) in an iterative
update is called the Gauss-Newton method.
An alternative to the Gauss–Newton method is the Levenberg4–Marquardt5
method, which is used for ill-conditioned and rank-deﬁcient problems. This method
is similar to the problem of ﬁnding the minimum of the regularized function
F(c) = 1
2r(c)Tr(c) + 1
2γ (c −c0)T (c −c0) = 1
2∥r(c)∥2
2 + 1
2γ ∥c −c0∥2
2, (9.30)
where c0 is a good initial guess for c and γ is a small regularization parameter. To
employ the Levenberg–Marquardt method, we repeat all the steps that we performed
for the Gauss–Newton method, see (9.21)–(9.26), but for the functional (9.30).
Finally, in the Levenberg–Marquardt method, the linear system that should be
solved at every iteration k is
(J T (ck)J(ck) + γk I)(ck+1 −ck) ≈−J T (ck)r(ck),
(9.31)
and the corresponding linear least squares problem is
J(ck)
√γk I

· (ck+1 −ck) ≈
−r(ck)
0

.
(9.32)
In (9.31), (9.32), the number γk is an iteratively chosen regularization parameter,
which can be computed as in [7] or using a trust region approach [13]. We refer
to [31, 110] for various techniques for choosing the regularization parameter γ in
(9.30), and to [74] for implementation and convergence analysis of this method.
Example 9.5 Let us consider the nonlinear model equation
AeE/T−T0 = y.
(9.33)
Our goal is to determine parameters A, E, and T0 in this equation from knowledge
of y and T . We rewrite (9.33) as a nonlinear least squares problem in the form
4Kenneth Levenberg (1919–1973) was an American statistician.
5Donald W. Marquardt (1929–1997) was an American statistician.

9.2 Nonlinear Least Squares Problems
301
min
A,E,T0
m

i=1
(yi −AeE/Ti−T0)2.
(9.34)
We will show how to obtain a linear problem from the nonlinear problem (9.34). We
take the logarithm of (9.33) to get
ln A +
E
T −T0
= ln y.
(9.35)
Now multiply both sides of (9.35) by T −T0 to obtain
ln A(T −T0) + E = ln y(T −T0).
(9.36)
Rewrite the above equation as
T ln A −T0 ln A + E + T0 ln y = T ln y.
(9.37)
Let us now deﬁne the vector of parameters c = (c1, c2, c3) with c1 = T0, c2 =
ln A, c3 = E −T0 ln A. Now the problem (9.37) can be written as
c1 ln y + c2T + c3 = T ln y,
(9.38)
which is already a linear problem. We can rewrite (9.38), writing f (c, y, T ) =
c1 ln y + c2T + c3 as a linear least squares problem in the form
min
c
m

i=1
(Ti ln yi −f (c, yi, Ti))2.
(9.39)
The system of linear equations that we need to solve is
⎛
⎜⎜⎜⎝
ln y1 T1 1
ln y2 T2 1
...
...
...
ln ym Tm 1
⎞
⎟⎟⎟⎠·
⎛
⎝
c1
c2
c3
⎞
⎠=
⎛
⎜⎜⎜⎝
T1 ln y1
T2 ln y2
...
Tm ln ym
⎞
⎟⎟⎟⎠.
(9.40)
Example 9.6 Suppose that a nonlinear model function is given as
f (x, c) = Aec1x + Bec2x, A, B = const > 0,
(9.41)
and our goal is to ﬁt this function using the Gauss–Newton method. In other words,
we will use the iterative formula (9.28) for the iterative update of c = (c1, c2). The
residual function will be

302
9
Numerical Solution of Linear Least Squares Problems
r(c) = y −f (x, c),
(9.42)
where y = yi, i = 1, ..., m, are data points. First, we compute the Jacobian matrix
J(c), where two columns in this matrix will be given by
J(c)i,1 = ∂ri
∂c1
= −xi Aec1xi, i = 1, ..., m,
J(c)i,2 = ∂ri
∂c2
= −xi Bec2xi, i = 1, ..., m.
(9.43)
If we make an initial guess for the parameters c0 = (c0
1, c0
2) = (1, 0), then we have
to solve the following problem at iteration k = 1:
J(c0)(c1 −c0) = −r(c0).
(9.44)
The next update for the parameters c1 = (c1
1, c1
2) in the Gauss–Newton method can
be computed as
c1 = c0 −[J T (c0)J(c0)]−1J T (c0)r(c0).
(9.45)
Here r(c0) and J(c0) can be computed explicitly as follows:
r(c0) = yi −f (xi, c0) = yi −(Ae1·xi + Be0·xi) = yi −Aexi −B, i = 1, ..., m,
(9.46)
where we note that c0 = (c0
1, c0
2) = (1, 0) and two columns in the Jacobian matrix
J(c0) will be
J(c0)i,1 = −xi Ae1·xi = −xiAexi, i = 1, ..., m,
J(c0)i,2 = −xi Be0·xi = −xiB, i = 1, ..., m.
(9.47)
Substituting (9.46), (9.47) into (9.44) yields the following linear system of equations:
⎛
⎜⎜⎜⎝
−x1 Aex1 −x1B
−x1 Aex1 −x2B
...
...
−xm Aexm −xm B
⎞
⎟⎟⎟⎠·
c1
1 −c0
1
c1
2 −c0
2

=
⎛
⎜⎜⎜⎝
−y1 −Aex1 −B
−y2 −Aex2 −B
...
−ym −Aexm −B
⎞
⎟⎟⎟⎠,
(9.48)
which is solved for c1 −c0 using the method of normal equations as

9.2 Nonlinear Least Squares Problems
303
⎛
⎜⎜⎜⎝
−x1 Aex1 −x1B
−x1 Aex1 −x2B
...
...
−xm Aexm −xm B
⎞
⎟⎟⎟⎠
T
·
⎛
⎜⎜⎜⎝
−x1 Aex1 −x1B
−x1 Aex1 −x2B
...
...
−xm Aexm −xm B
⎞
⎟⎟⎟⎠·
c1
1 −c0
1
c1
2 −c0
2

=
⎛
⎜⎜⎜⎝
−x1 Aex1 −x1B
−x1 Aex1 −x2B
...
...
−xm Aexm −xm B
⎞
⎟⎟⎟⎠
T
·
⎛
⎜⎜⎜⎝
−y1 −Aex1 −B
−y2 −Aex2 −B
...
−ym −Aexm −B
⎞
⎟⎟⎟⎠.
(9.49)
This system can be solved for c1 −c0, and the next values for c1 are obtained using
(9.45) as
c1
1
c1
2

=
c0
1
c0
2

+
⎛
⎜⎜⎜⎝
⎛
⎜⎜⎜⎝
−x1 Aex1 −x1B
−x1 Aex1 −x2B
...
...
−xm Aexm −xm B
⎞
⎟⎟⎟⎠
T
·
⎛
⎜⎜⎜⎝
−x1 Aex1 −x1B
−x1 Aex1 −x2B
...
...
−xm Aexm −xm B
⎞
⎟⎟⎟⎠
⎞
⎟⎟⎟⎠
−1
·
⎛
⎜⎜⎜⎝
−x1Aex1 −x1B
−x1Aex1 −x2B
...
...
−xm Aexm −xm B
⎞
⎟⎟⎟⎠
T
·
⎛
⎜⎜⎜⎝
−y1 −Aex1 −B
−y2 −Aex2 −B
...
−ym −Aexm −B
⎞
⎟⎟⎟⎠.
(9.50)
9.3
Method of Normal Equations
The ﬁrst method that we will consider for the solution of (9.2) will be the method of
normal equations. This method is the fastest but is not as accurate as the QR or SVD
decompositions. We can apply this method for the solution of linear least squares
problems if the condition number of the matrix A is small.
Our goal is to minimize the function F(x) = ∥r(x)∥2
2 = ∥Ax −b∥2
2. To ﬁnd min-
imum of this function and derive the normal equations, we look for the values of x
for which the gradient of
F(x) = ∥r(x)∥2
2 = ∥Ax −b∥2
2 = (Ax −b)T (Ax −b)
(9.51)
vanishes, or where ∇F(x) = 0. Considering the standard deﬁnition of the Fréchet
derivative we get

304
9
Numerical Solution of Linear Least Squares Problems
0 = lim
h→0
(r(x + h))Tr(x + h) −(r(x))Tr(x)
∥h∥2
= lim
h→0
(A(x + h) −b)T (A(x + h) −b) −(Ax −b)T (Ax −b)
∥h∥2
= lim
h→0
2hT (AT Ax −AT b) + hT AT Ah
∥h∥2
.
(9.52)
We observe that the second term |hT AT Ah|
∥h∥2
≤∥A∥2
2∥h∥2
2
∥h∥2
= ∥A∥2
2∥h∥2 goes to zero as
h →0. This means that the term AT Ax −AT b must also be zero, and thus we must
have
0 = ∇F = 2AT Ax −2AT b.
(9.53)
We conclude that
AT Ax = AT b,
(9.54)
which is a symmetric linear system of n × n equations, commonly called a system
of normal equations.
Using (9.53), we can compute the Hessian matrix H = 2AT A. If the Hessian
matrix H = 2AT A ispositivedeﬁnite,then x isindeedaminimum.Thisisasufﬁcient
condition for x to be a minimum of (9.51). We can show that the matrix AT A is
positive deﬁnite if and only if the columns of A are linearly independent, or when
r(A) = n.
If the matrix A has a full rank (r(A) = n), the system (9.54) is of size n × n and is
a symmetric positive deﬁnite system of normal equations. It has the same solution x
as the least squares problem minx ∥Ax −b∥2
2 of size m × n. To solve system (9.54)
one can use the Cholesky decomposition
AT A = LLT
(9.55)
with L a lower triangular matrix. Then the solution of (9.54) will be given by the
solution of the triangular system
Ly = AT b,
LT x = y.
(9.56)
However, in practice, the method of normal equations can be inaccurate for two
reasons.
• The condition number of AT A is the square of the condition number of the original
matrix A:
cond(AT A) = cond(A)2.
(9.57)
Thus, the method of normal equations can give a squared condition number even
when the ﬁt to data is good and the residual is small. This makes the computed
solution more sensitive. In this sense, the method of normal equations is unstable.

9.3 Method of Normal Equations
305
• Information can be lost during computation of the product AT A. For example,
take
A =
⎛
⎝
1 1
δ 0
0 δ
⎞
⎠
(9.58)
with 0 < δ < √ε in a given ﬂoating-point system. In ﬂoating-point arithmetic we
can compute AT A:
AT A =
1 δ 0
1 0 δ

·
⎛
⎝
1 1
δ 0
0 δ
⎞
⎠=
1 + δ2
1
1
1 + δ2

=
1 1
1 1

,
(9.59)
which is a singular matrix in the working precision.
These inconveniences do not make the method of normal equations useless, but
they provide motivation to seek more robust methods for linear least squares prob-
lems.
9.4
QR Decomposition
In this section we consider QR decomposition of a matrix A. QR decomposition of a
matrix A can be computed, for example, using the Gram–Schmidt orthogonalization
process [47]; see Section9.5.3. QR decomposition of a matrix A means that a matrix
A of size m × n with m ≥n can be factorized as the product of a unitary matrix Q
of size m × m and an upper triangular matrix R of size m × n:
A = QR = Q
R1
0

=
Q1, Q2
 R1
0

= Q1R1,
(9.60)
where R1 is an upper triangular matrix of size n × n, 0 is the zero matrix of size
(m −n) × n, Q1 is a matrix of size m × n with orthogonal columns, Q2 is of size
m × (m −n) with orthogonal columns. We note that (m −n) rows of the upper
triangular matrix R consist of zeros.
We can consider also an alternative deﬁnition of QR decomposition:
Theorem 9.1 QR decomposition. Let A be an m × n matrix with m ≥n. Suppose
that A has full column rank. Then there exist a unique m × n orthogonal matrix
Q(QT Q = In) and a unique n × n upper triangular matrix R with positive diagonals
rii > 0 such that A = QR.
Proof TheprooffollowsfromtheGram–Schmidtorthogonalizationprocess[47];see
Section9.5.3. Another proof follows from the QR factorization using Householder
reﬂection.
⊓⊔

306
9
Numerical Solution of Linear Least Squares Problems
We will now show how to obtain a formula for the value of x that minimizes
∥Ax −b∥2 using three different decompositions the matrix A into matrices Q and R.
• In the ﬁrst method, we choose m −n more orthonormal vectors ˜Q such that (Q, ˜Q)
isasquareorthogonalmatrix,andtherefore, ˜QT Q = 0.Onewayinwhichtodothis
is to choose any m −n independent vectors ˜X and then apply the Gram–Schmidt
orthogonalization Algorithm9.4 to the n × n nonsingular matrix (Q, ˜X)). Using
the property
∥Q AZ∥2 = ∥A∥2
(9.61)
of norms and matrices for any orthogonal or unitary matrices Q, Z and applying
it to ∥Ax −b∥2
2, we can write
∥r(x)∥2
2 = ∥Ax −b∥2
2 = ∥(Q, ˜Q)T (Ax −b)∥2
2
=

 QT
˜QT

(QRx −b)

2
2
=


I n×n
O(m−n)×n

Rx −
 QT b
˜QT b

2
2
=

 Rx −QT b
−˜QT b

2
2
=
Rx −QT b
2
2 + ∥˜QT b∥2
2
≥∥˜QT b∥2
2.
We can solve the triangular linear system Rx −QT b = 0 for x, since A and R
have the same rank, n, and so R is nonsingular and QT b is a vector of length n.
Then x = R−1QT b, and the minimum residual norm of ∥Ax −b∥2 is given by
∥˜QT b∥2.
• The second method is slightly different and does not use the matrix ˜Q. This method
uses adding and subtracting the same term QQT to the expression for the residual
r(x) = Ax −b:
r(x) = Ax −b = QRx −b = QRx −(QQT + I −QQT )b
= Q(Rx −QT b) −(I −QQT )b.
Note that the vectors Q(Rx −QT b) and (I −QQT )b are orthogonal, because
(Q(Rx −QT b))T ((I −QQT )b) = (Rx −QT b)T (QT (I −QQT ))b
= (Rx −QT b)T (0)b = 0.
(9.62)
Thus, we can use the Pythagorean theorem,
∥Ax −b∥2
2 = ∥Q(Rx −QT b)∥2
2 + ∥(I −QQT )b∥2
2
= ∥Rx −QT b∥2
2 + ∥(I −QQT )b∥2
2,

9.4 QR Decomposition
307
where we have used the property of the norm ∥Qy∥2
2 = ∥y∥2
2. This sum of squares
is minimized when the ﬁrst term is zero, i.e., x = R−1QT b.
• A third derivation uses the normal equations solution and then QR decomposition
inside this solution:
x = (AT A)−1AT b
= (RT QT QR)−1RT QT b = (RT R)−1RT QT b
= R−1R−T RT QT b = R−1QT b.
9.5
Orthogonalization Methods
In this section we will present the main orthogonalization methods for computing
the QR factorization of a matrix A, which include:
• Householder transformation (called also reﬂection),
• Givens transformation (called rotation),
• Gram–Schmidt orthogonalization.
9.5.1
Householder Transformations
A Householder transformation, which is also called reﬂection, is a matrix of the form
P = I −2uuT ,
where ∥u∥2 = 1. We can see that P = PT and
P · PT = (I −2uuT )(I −2uuT ) = I −4uuT + 4uuT uuT = I.
Using the above equations, we conclude that P is a symmetric orthogonal matrix.
This matrix is called a reﬂection, because Px is the reﬂection of the vector x in the
plane that passes through 0 and is perpendicular to the vector u.
For a given vector x, we can ﬁnd a Householder reﬂection P = I −2uuT that
will zero out all except the ﬁrst component of x and leave ﬁrst entry of x as the
nonzero value
Px = (c, 0, . . . , 0)T = c · e1
with e1 = (1, 0, ..., 0). We do this using the following procedure. First we apply P
to x to get
Px = (I −2uuT )x = x −2u(uT x) = c · e1.
From the equation above, we get

308
9
Numerical Solution of Linear Least Squares Problems
u =
1
2(uT x)(x −ce1),
(9.63)
i.e., u is a linear combination of x and e1.
Since P is an orthogonal matrix, we can use the following property of the 2-norm:
∥x∥2 = ∥Px∥2 = |c|. Then the vector u in (9.63) must be parallel to the vector
˜u = x ± ∥x∥2e1,
(9.64)
and thus the vector u can be computed as
u = ˜u/∥˜u∥2.
One can verify that as long as ˜u ̸= 0, the choice of sign in (9.64) yields a vector u
satisfying Px = ce1. We will determine the vector ˜u as
˜u = x + sign(x1)e1,
and this means that there is no cancellation in computing the ﬁrst component of u.
Here, x1 is the ﬁrst coordinate in the vector x, after which all other entries of x in the
matrix A are 0. Finally, the vector ˜u will have the following form:
˜u =
⎛
⎜⎜⎜⎝
x1 + sign(x1) · ∥x∥2
x2
...
xn
⎞
⎟⎟⎟⎠where u =
˜u
∥˜u∥2
.
We denote the procedure for obtaining the vector u by u = House(x) and use it
in the Algorithm9.2. In computations, it is more efﬁcient to store ˜u instead of u to
save the work of computing u, and use the formula P = I −(2/∥˜u∥2
2)˜u ˜uT instead
of P = I −2uuT .
Example 9.7 In this example, we present a general procedure for carrying out the
QR decomposition of a matrix A of size 5 × 4 using Householder transformations. In
all the matrices below, Pi denotes an orthogonal matrix, x denotes a generic nonzero
entry, and 0 denotes a zero entry. Thus, for the decomposition A = QR, we need to
perform the following steps:
• Choose the matrix P1 such that
A1 ≡P1A =
⎛
⎜⎜⎜⎜⎝
x x x x
0 x x x
0 x x x
0 x x x
0 x x x
⎞
⎟⎟⎟⎟⎠
.

9.5 Orthogonalization Methods
309
• Choose the matrix P2 such that
P2 =
⎛
⎜⎜⎜⎜⎝
1 0 0 0
0
0
P′
2
0
0
⎞
⎟⎟⎟⎟⎠
and
A2 ≡P2 A1 =
⎛
⎜⎜⎜⎜⎝
x x x x
0 x x x
0 0 x x
0 0 x x
0 0 x x
⎞
⎟⎟⎟⎟⎠
.
• Choose
P3 =
⎛
⎜⎜⎝
1
0
1
0
P′
3
⎞
⎟⎟⎠
such that
A3 ≡P3 A2 =
⎛
⎜⎜⎜⎜⎝
x x x x
0 x x x
0 0 x x
0 0 0 x
0 0 0 x
⎞
⎟⎟⎟⎟⎠
.
• Choose
P4 =
⎛
⎜⎜⎝
1
1
0
1
0
P′
4
⎞
⎟⎟⎠
such that
˜R := A4 ≡P4 A3 =
⎛
⎜⎜⎜⎜⎝
x x x x
0 x x x
0 0 x x
0 0 0 x
0 0 0 0
⎞
⎟⎟⎟⎟⎠
.
In this example, we have chosen a Householder matrix P′
i , i = 2, 3, 4, to zero
out the subdiagonal entries in column i. We note that this does not disturb the zeros
introduced in previous columns.
We observe that we have obtained the decomposition

310
9
Numerical Solution of Linear Least Squares Problems
A4 = P4P3P2P1A.
(9.65)
Let us denote the ﬁnal triangular matrix A4 by ˜R ≡A4. Then using (9.65), we
observe that the matrix A is obtained via the decomposition
A = PT
1 PT
2 PT
3 PT
4 ˜R = QR,
(9.66)
which is our desired QR decomposition. Here, the matrix Q comprises the ﬁrst four
columns of PT
1 PT
2 PT
3 PT
4 = P1P2P3P4 (since all Pi are symmetric), and R comprises
the ﬁrst four rows of ˜R.
QR factorization for a matrix A of order m × n is summarized in Algorithm9.1. In
this algorithm, ak denotes the kth column of the matrix A. For simplicity, a rescaling
procedure is omitted in this algorithm.
Algorithm 9.1 QR factorization using Householder reﬂections.
for k = 1 to min(m −1, n) /* loop over all columns */
αk = −sign(akk)

a2
kk + ... + a2
mk
uk = (0, ...0 akk....amk)T −αkek
βk = uT
k uk
if βk = 0 then /* skip column k since it is already 0 */
go to the next k
for j = k to n
γ j = uT
k a j /* a j are elements of column j of A */
a j = a j −2γ j
βk uk
end
end
Below we present an algorithm for obtaining the QR decomposition using House-
holder transformations in a more general form.
Algorithm 9.2 QR factorization using Householder reﬂections.
for i = 1 to min(m −1, n)
ui = House(A(i : m, i))
P′
i = I −2uiuT
i
A(i : m, i : n) = P′
i A(i : m, i : n)
end for
We can discuss some implementation issues of this algorithm. We note that we
never form the matrix Pi explicitly but instead use the efﬁcient multiplication
(I −2uiuT
i )A(i : m, i : n) = A(i : m, i : n) −2ui(uT
i A(i : m, i : n)).

9.5 Orthogonalization Methods
311
To store Pi, we need only ui, or ˜ui and ∥˜ui∥. These values can be stored in column
i of A, which means that the QR decomposition can be “overwritten” on A, where
Q is stored in factored form P1, . . . , Pn−1, and Pi is stored as ˜ui below the diagonal
in column i of A.
Householder reﬂections can be applied for the solution of the following least
squares problem:
Find x subject to min
x
∥Ax −b∥2
2 .
(9.67)
To solve (9.67) using QR decomposition of the matrix A, we need to compute
the vector QT b; see details in Section9.4. We can do this computation in a fol-
lowing way: compute QT b using Householder matrices Pi, i = 1, ..., n, as QT b =
Pn Pn−1 · · · P1b, so we need only keep multiplying b by P1, P2, . . . , Pn. We summa-
rize this discussion in the following algorithm:
Algorithm 9.3 Computation of QT b = Pn Pn−1 · · · P1b
for i = 1 to n
γ = −2 · uT
i b(i : m)
b(i : m) = b(i : m) + γ ui
end for
In MATLAB®, the command A \ b solves the least squares problem if the matrix
A isofsizem × n withm > n.Itisalsopossibletousethecommand(Q, R) = qr(A)
in MATLAB® to perform a QR decomposition of the matrix A.
Let us explain now in greater detail how to perform the Householder transforma-
tion u = House(x) in Algorithm9.2. First we introduce some notions:
• Let x be an arbitrary real m-dimensional column vector of A such that ∥x∥= |α|
for a scalar α.
• If the Householder algorithm is implemented using ﬂoating-point arithmetic, then
α should get the opposite sign to that of the kth coordinate of x, where xk is the
pivot element after which all entries in the ﬁnal upper triangular form of the matrix
A are 0.
Then to compute the Householder matrix P, set
v = x + αe1,
α = −sign(x1)∥x∥,
u =
v
∥v∥,
P = I −2uuT ,
(9.68)
where e1 is the vector (1, 0, ..., 0)T , ∥· ∥is the Euclidean norm, I is the m × m
identity matrix, and x1 is the ﬁrst component of the vector x. The matrix P thus
obtained is an m × m Householder matrix such that

312
9
Numerical Solution of Linear Least Squares Problems
Px = (α, 0, · · · , 0)T .
To transform an m × n matrix A gradually to upper triangular form, we ﬁrst left
multiply A by the ﬁrst Householder matrix P1. This results in a matrix P1A with
zeros in the left column (except for the ﬁrst row):
P1A =
⎛
⎜⎜⎜⎝
α1 ⋆. . . ⋆
0
...
A′
0
⎞
⎟⎟⎟⎠.
This can be repeated for the matrix A′ that is obtained from P1A by deleting the ﬁrst
row and ﬁrst column, resulting in a Householder matrix P′
2. Note that P′
2 is smaller
than P1. Since we want P′
2 to operate on P1A instead of A′, we need to expand it on
the upper left, or in general,
Pk =
Ik−1 0
0
P′
k

.
After k iterations of this process, k = min(m −1, n), we obtain an upper triangular
matrix
R = Pk · · · P2P1 A .
Now choosing
Q = PT
1 PT
2 · · · PT
k ,
we obtain the QR decomposition of A.
Example 9.8 Let us calculate the QR decomposition of the matrix
A =
⎛
⎝
12 −51
4
6
167 −68
−4 24 −41
⎞
⎠
using Householder reﬂection. First, we need to ﬁnd a reﬂection that transforms
the ﬁrst column of the matrix A, the vector x = a1 = (12, 6, −4)T , to ∥x∥e1 =
∥a1∥e1 = (14, 0, 0)T .
Now using (9.68), we construct the vector
v = x + αe1,
where
α = −sign(x1)∥x∥
and
u =
v
∥v∥.

9.5 Orthogonalization Methods
313
We observe that in our example, ∥x∥= ∥x∥2 =

122 + 62 + (−4)2 = 14,
α = −sign(12)∥x∥= −14, and the vector αe1 will be αe1 = (14, 0, 0)T .
Therefore,
v = x + αe1 = (12, 6, −4)T + (−14, 0, 0)T = (−2, 6, −4)T = 2(−1, 3, −2)T ,
and thus u =
v
∥v∥=
1
√
14(−1, 3, −2)T . Then the ﬁrst Householder matrix will be
P1 = I −
2
√
14
√
14
⎛
⎝
−1
3
−2
⎞
⎠−1 3 −2
= I −1
7
⎛
⎝
1 −3 2
−3 9 −6
2 −6 4
⎞
⎠
=
⎛
⎝
6/7
3/7 −2/7
3/7 −2/7 6/7
−2/7 6/7
3/7
⎞
⎠.
We computer the product P1A to get the matrix
A1 = P1A =
⎛
⎝
14 21 −14
0 −49 −14
0 168 −77
⎞
⎠,
(9.69)
which is almost a triangular matrix. We have only to zero the (3, 2) entry.
Take the (1, 1) minor of (9.69) and then apply the same process again to the matrix
A′ = M11 =
−49 −14
168 −77

.
By the same method as above, we ﬁrst need to ﬁnd a reﬂection that transforms the ﬁrst
column of the matrix A′, namely the vector x = (−49, 168)T , to ∥x∥e1 = (175, 0)T .
Here ∥x∥=

(−49)2 + 1682 = 175,
α = −sign(−49)∥x∥= 175 and αe1 = (175, 0)T .
Therefore,
v = x + αe1 = (−49, 168)T + (175, 0)T = (126, 168)T ,
∥v∥=

1262 + 1682 =
√
44100 = 210,
u =
v
∥v∥= (126/210, 168/210)T = (3/5, 4/5)T .

314
9
Numerical Solution of Linear Least Squares Problems
Then
P′
2 = I −2
3/5
4/5
 3/5 4/5
or
P′
2 = I −2
 9/25 12/25
12/25 16/25

=
 7/25
−24/25
−24/25 −7/25

.
Finally, we obtain the matrix of the Householder transformation P2 such that
P2 =
1 0
0 P′
2

with P′
2 given above and thus,
P2 =
⎛
⎝
1
0
0
0
7/25
−24/25
0 −24/25 −7/25
⎞
⎠.
Now we obtain
Q = P = PT
1 PT
2 =
⎛
⎝
6/7
69/175
−58/175
3/7 −158/175
6/175
−2/7
−6/35
−33/35
⎞
⎠.
Thus, we have performed the QR decomposition of the matrix A with matrices Q
and R given by
Q = PT
1 PT
2 =
⎛
⎝
0.8571
0.3943 −0.3314
0.4286 −0.9029 0.0343
−0.2857 −0.1714 −0.9429
⎞
⎠,
R = P2 A1 = P2P1A = QT A =
⎛
⎝
14
21
−14
0 −175 70
0
0
35
⎞
⎠.
We observe that the matrix Q is orthogonal and R is upper triangular, so A = QR
is the required QR decomposition. To obtain the matrices Q and R above, we have
used the facts that
P2 A1 = P2P1A = R,
PT
1 PT
2 P2P1A = PT
1 PT
2 R,
A = PT
1 PT
2 R = QR,
with Q = PT
1 PT
2 .
We can also perform tridiagonalization of the matrix A using Householder reﬂec-
tion matrices. We follow [12] in the description of this procedure. In the ﬁrst step
of the tridiagonalization procedure, to form a Householder matrix at every step we

9.5 Orthogonalization Methods
315
need to determine constants α and r, which are given by the formulas
α = −sgn(a21)




n

j=2
a2
j1,
r =

1
2(α2 −a21α).
(9.70)
Knowing α and r, we can construct the vector v such that
v(1) =
⎛
⎜⎜⎝
v1
v2
...
vn
⎞
⎟⎟⎠,
(9.71)
where v1 = 0, v2 = a21−α
2r
and
vk = ak1
2r
for each k = 3, 4, ..., n.
Then we can compute the ﬁrst Householder reﬂection matrix as
P(1) = I −2v(1)(v(1))T
and obtain the matrix A(1) as
A(1) = P(1)AP(1).
Using P(1) and A(1) thus obtained, the process of tridiagonalization is repeated
for k = 2, 3, ..., n as follows:
α = −sgn(ak+1,k)




n

j=k+1
a2
jk,
r =

1
2(α2 −ak+1,kα),
v(k)
1
= v(k)
2
= ... = v(k)
k
= 0,
v(k)
k+1 = ak+1,k −α
2r
,
v(k)
j
= a jk
2r
for
j = k + 2; k + 3, ..., n,
P(k) = I −2v(k)(v(k))T ,
A(k+1) = P(k)A(k)P(k).
(9.72)

316
9
Numerical Solution of Linear Least Squares Problems
In (9.72), the elements ak+1,k, a jk are entries of the matrix A(k).
Example 9.9 In this example, the given matrix
A =
⎛
⎝
5 1 0
1 6 3
0 3 7
⎞
⎠
is transformed into a similar tridiagonal matrix A1 using Householder’s method.
We perform tridiagonalization by the following steps:
• First compute α via (9.70) as
α = −sgn(a21)




n

j=2
a2
j1 = −

(a2
21 + a2
31) = −

(12 + 02) = −1.
• Using α, we obtain r via (9.70) as
r =

1
2(α2 −a21α) =

1
2((−1)2 −1 · (−1)) = 1.
• Using the known values of α and r, we construct the vector v(1) as in (9.71). Using
(9.71), we compute
v1 = 0,
v2 = a21 −α
2r
= 1 −(−1)
2 · 1
= 1,
v3 = a31
2r = 0.
Now we have obtained the vector
v(1) =
⎛
⎝
0
1
0
⎞
⎠.
We compute the ﬁrst Householder matrix P1 as
P(1) = I −2v(1)(v(1))T
and get
P(1) =
⎛
⎝
1 0 0
0 −1 0
0 0 1
⎞
⎠.
The tridiagonal matrix A(1) is obtained as

9.5 Orthogonalization Methods
317
A(1) = P(1) AP(1) =
⎛
⎝
5 −1 0
−1 6 −3
0 −3 7
⎞
⎠.
Example 9.10 In this example, the given 4 × 4 matrix
A =
⎛
⎜⎜⎝
4 1 −2 2
1 2 0
1
−2 0 3 −2
2 1 −2 −1
⎞
⎟⎟⎠
is transformed into a similar tridiagonal matrix A2 using Householder reﬂections.
Similarly with the example above we perform the following steps:
• First compute α via (9.70) as
α = −sgn(a21)




n

j=2
a2
j1 = (−1) ·

(a2
21 + a2
31 + a2
41)
= −1 · (12 + (−2)2 + 22) = (−1) ·
√
1 + 4 + 4 = −
√
9 = −3.
• Using α, we obtain r as
r =

1
2(α2 −a21α) =

1
2((−3)2 −1 · (−3)) =
√
6.
• From α and r, we construct the vector v(1). Using (9.71), we compute
v1 = 0,
v2 = a21 −α
2r
= 1 −(−3)
2 ·
√
6
=
2
√
6
,
v3 = a31
2r =
−2
2 ·
√
6
= −1
√
6
,
v4 = a41
2r =
2
2 ·
√
6
=
1
√
6
.
Thus, we have obtained
v(1) =
⎛
⎜⎜⎜⎝
0
2
√
6
−1
√
6
1
√
6
⎞
⎟⎟⎟⎠.
Now we can compute the ﬁrst Householder matrix P(1),

318
9
Numerical Solution of Linear Least Squares Problems
P(1) = I −2v(1)(v(1))T = I −2 ·
⎛
⎜⎜⎜⎝
0
2
√
6
−1
√
6
1
√
6
⎞
⎟⎟⎟⎠·

0
2
√
6
−1
√
6
1
√
6

,
and obtain
P(1) =
⎛
⎜⎜⎝
1
0
0
0
0 −1/3 2/3 −2/3
0 2/3 2/3 1/3
0 −2/3 1/3 2/3
⎞
⎟⎟⎠.
After that, we compute the matrix A(1) as
A(1) = P(1)AP(1)
to get
A(1) = P(1)AP(1) =
⎛
⎜⎜⎝
4
−3
0
0
−3 10/3
1
4/3
0
1
5/3 −4/3
0
4/3 −4/3 −1
⎞
⎟⎟⎠.
Next, having found A(1), we need to construct A(2) and P(2). Using formulas (9.72)
for k = 2, we get
α = −sgn(a3,2)




4

j=3
a2
j,2 = −sgn(1)

a2
3,2 + a2
4,2 = −

1 + 16
9 = −5
3;
r =

1
2(α2 −a3,2 · α) =

20
9 ;
v(2)
1
= v(2)
2
= 0,
v(2)
3
= a3,2 −α
2r
=
2
√
5
,
v(2)
4
= a4,2
2r =
1
√
5
,
and thus new vector v will be v(2) = (0, 0,
2
√
5,
1
√
5)T , and the new Householder
matrix P(2) will be

9.5 Orthogonalization Methods
319
P(2) = I −2v(2)(v(2))T = I −2
⎛
⎜⎜⎝
0 0 0
0
0 0 0
0
0 0 4/5 2/5
0 0 2/5 1/5
⎞
⎟⎟⎠=
⎛
⎜⎜⎝
1 0
0
0
0 1
0
0
0 0 −3/5 −4/5
0 0 −4/5 3/5
⎞
⎟⎟⎠.
Finally, we obtain the tridiagonal matrix A(2) as
A(2) = P(2)A(1)P(2) =
⎛
⎜⎜⎝
4
−3
0
0
−3 10/3
−5/3
0
0 −5/3 −33/25 68/75
0
0
68/75 149/75
⎞
⎟⎟⎠.
We observe that we have performed the process of tridiagonalization in two steps.
The ﬁnal result is a tridiagonal symmetric matrix A(2) that is similar to the original
one A.
9.5.2
Givens Rotation
In the previous section we described the Householder transformation, which intro-
duces many zeros in a column of matrix at once. But in some situations we need to
introduce zeros one at a time, and in such cases, we should use Givens rotations.
A Givens rotation is represented by a matrix of the form
G(i, j, θ) =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1 · · · 0 · · · 0 · · · 0
... ... ...
...
...
0 · · · c · · · −s · · · 0
...
... ...
...
...
0 · · · s · · · c · · · 0
...
...
...
... ...
0 · · · 0 · · · 0 · · · 1
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
where c = cos Θ and s = sin Θ appear at the intersections of the ith and jth rows
and columns. Here Θ is the angle of rotation. The nonzero elements of the Givens
matrix are given by
gk k = 1
for k ̸= i, j,
gi i = c,
g j j = c,
g j i = −s,
gi j = s
for i > j.

320
9
Numerical Solution of Linear Least Squares Problems
We note that the sign of the sine function switches for j > i. The orthogonality of
matrix G implies that c2 + s2 = 1. This is true for the cosine and sine of an arbitrary
angle. The product G(i, j, θ)x represents a counterclockwise rotation of the vector
x in the (i, j)-plane through θ radians.
When a Givens rotation matrix G is multiplied by another matrix A from the right
to form the product G A, only rows i and j of A are affected. Thus we restrict ourself
to the solution of the following problem: Given a and b, ﬁnd c = cos θ and s = sin θ
such that
c −s
s c
 a
b

=
r
0

.
Explicit calculation of θ is rarely necessary or desirable. Instead, we directly seek c,
s, and r. An obvious solution is
r =

a2 + b2,
c = a/r,
s = −b/r.
(9.73)
If |a| > |b|, then we work with the tangent of the rotation angle
t = s/c = b/a,
(9.74)
so that we have the following alternative formulas for computating c, s:
c = 1/

1 + t2,
s = −ct.
(9.75)
If |b| > |a|, then we can use the cotangent τ of the rotation angle,
τ = s/c = a/b,
(9.76)
and obtain
c = 1/

1 + τ 2,
s = −cτ.
(9.77)
Example 9.11 Given the 3 × 3 matrix
A =
⎛
⎝
6 5 0
5 1 4
0 4 3
⎞
⎠,
we perform two iterations of the Givens rotation to bring the matrix into upper
triangular form.

9.5 Orthogonalization Methods
321
We must zero entries (2, 1) and (3, 2) of the matrix A. We ﬁrst make element
(2, 1) equal to zero and construct a rotation matrix G1:
G1 =
⎛
⎝
c −s 0
s c 0
0 0 1
⎞
⎠.
We have to perform the matrix multiplication
A1 = G1 · A =
⎛
⎝
c −s 0
s c 0
0 0 1
⎞
⎠
⎛
⎝
6 5 0
5 1 4
0 4 3
⎞
⎠
(9.78)
such that
c −s
s c
 6
5

=
r
0

.
(9.79)
Now we compute parameters c, s, and r in (9.79) using explicit formulas (9.73):
r =

62 + 52 = 7.8102,
c = 6/r = 0.7682,
s = −5/r = −0.6402.
Plugging these values for c, s in (9.78) and performing matrix multiplication yields
a new matrix A1:
A1 =
⎛
⎝
7.8102 4.4813 2.5607
0
−2.4327 3.0729
0
4
3
⎞
⎠.
The next step will be to zero out the element (3, 2). Using the same idea as before,
we construct a rotation matrix G2:
G2 =
⎛
⎝
1 0 0
0 c −s
0 s c
⎞
⎠.
We have to perform the matrix multiplication
A2 = G2 · A1 =
⎛
⎝
1 0 0
0 c −s
0 s c
⎞
⎠
⎛
⎝
7.8102 4.4813 2.5607
0
−2.4327 3.0729
0
4
3
⎞
⎠
such that
c −s
s c
 −2.4327
4

=
r
0

.
(9.80)

322
9
Numerical Solution of Linear Least Squares Problems
The parameters c, s, and r in (9.80) are computed using the explicit formulas
(9.73):
r =

(−2.4327)2 + 42 = 4.6817,
c = −2.4327/r = −0.5196,
s = −4/r = −0.8544.
Plugging in these values for c and s and performing matrix multiplication gives us a
new matrix A2 that is also the upper triangular matrix R:
R = A2 =
⎛
⎝
7.8102 4.4813 2.5607
0
4.6817 0.9664
0
0
−4.1843
⎞
⎠.
This new matrix R is the upper triangular matrix that is needed to perform an
iteration of the QR decomposition. The matrix Q is now formed using the transpose
of the rotation matrices as follows:
Q = GT
1 GT
2 .
We note that we have performed the following computations:
G2G1A = R,
GT
1 GT
2 G2G1A = GT
1 GT
2 R,
and thus
A = GT
1 GT
2 R = QR
with
Q = GT
1 GT
2 .
Performingthismatrixmultiplicationyieldsthefollowingmatrix Q intheQRdecom-
position:
Q =
⎛
⎝
0.7682 0.3327
0.5470
0.6402 −0.3992 −0.6564
0
0.8544 −0.5196
⎞
⎠.
The Givens rotation can also be applied to zero out any desired component of an
m-vector. We illustrate how to zero out the element (4, 4) of a 5 × 5 matrix A. We
construct the Givens matrix

9.5 Orthogonalization Methods
323
G(2, 4, θ) =
⎛
⎜⎜⎜⎜⎝
1 0 0 0 0
0 c 0 −s 0
0 0 1 0 0
0 s 0 c 0
0 0 0 0 1
⎞
⎟⎟⎟⎟⎠
and compute the parameters r, c, s from
⎛
⎜⎜⎜⎜⎝
1 0 0 0 0
0 c 0 −s 0
0 0 1 0 0
0 s 0 c 0
0 0 0 0 1
⎞
⎟⎟⎟⎟⎠
·
⎛
⎜⎜⎜⎜⎝
a1
a2
a3
a4
a5
⎞
⎟⎟⎟⎟⎠
=
⎛
⎜⎜⎜⎜⎝
a1
r
a3
0
a5
⎞
⎟⎟⎟⎟⎠
.
Using a sequence of a such Givens rotations, we can zero out individual entries
of the matrix A and reduce it to upper triangualar form. In so doing, we should
avoid reintroducing nonzero entries into the matrix entries that have already been
zeroed out. This can be done by a number of different reorderings. The product of
all rotations will be an orthogonal matrix Q in the QR factorization of the matrix A.
Implementation of a Givens rotation for solving linear least square problems is
about 50 percent more expensive than doing Householder transformations. Givens
rotations also require more disk space to store c, s,r. Therefore, Givens rotations are
used in cases in which the matrix A is sparse.
9.5.3
Gram–Schmidt Orthogonalization
Gram–Schmidt orthogonalization is one more method for computing QR factoriza-
tions. If we apply Gram–Schmidt to the columns ai of A = (a1, a2, . . . , an) from left
to right, we get a sequence of orthonormal vectors q1 through qn spanning the same
space. These orthogonal vectors are the columns of the matrix Q. The Gram–Schmidt
orthogonalization process also computes the coefﬁcients r ji = qT
j ai expressing each
column ai as a linear combination of q1 through qi: ai = 	i
j=1 r jiq j. The r ji are just
the entries of the upper triangular matrix R.
More precisely, in the Gram–Schmidt orthogonalization process, for a given pair
of linearly independent vectors a1 and a2 of length m, we want to determine two
orthonormal vectors q1 and q2 of length m that span the same subspace as the vectors
a1 and a2. To do so, we ﬁrst normalize a1 and obtain q1 = a1/∥a1∥2. Then we subtract
from a2 the values (qT
1 a2)q1. This is the same as the following m × 1 least squares
problem:
q1γ ≈a2.
(9.81)
The solution of this problem is given by the method of normal equations as

324
9
Numerical Solution of Linear Least Squares Problems
γ ≈(qT
1 q1)−1qT
1 a2 = qT
1 a2.
(9.82)
Then the desired vector q2 is obtained by normalizing the residual vector
r = a2 −(qT
1 a2)q1.
This process, called the classical Gram–Schmidt (CGS) orthogonalization procedure,
can be extended to any number of vectors a1, ..., ak, 1 ≤k ≤m; see Algorithm9.4
for its implementation.
The classical Gram–Schmidt (CGS) procedure is unsatisfactory when imple-
mented in ﬁnite-precision arithmetic. This is because the orthogonality among the
computed vectors qk will be lost due to rounding errors. CGS also requires sepa-
rate storage for A, Q1, and R, since the element ak is used in the inner loop, and
thus qk cannot overwrite it (because qk is used in the inner loop). The modiﬁed
Gram–Schmidt (MGS) procedure overcomes these difﬁculties; see Algorithm9.5.
The classical Gram–Schmidt (CGS) and modiﬁed Gram–Schmidt (MGS) algo-
rithms for factoring A = QR are the following:
Algorithm 9.4 The classical Gram–Schmidt (CGS) orthogonalization algorithm.
for k = 1 to n /* loop over columns 1...n */
qk = ak
for j = 1 to k −1
r jk = qT
j ak
qk = qk −r jkq j
end
rkk = ∥qk∥2
if rkk = 0 then stop /* stop if linearly dependent */
qk = qk/rkk
end
Algorithm 9.5 Modiﬁed Gram–Schmidt (MGS) orthogonalization algorithm.
for k = 1 to n /* loop over columns 1...n */
rkk = ∥ak∥2
if rkk = 0 then stop /* stop if linearly dependent */
qk = ak/rkk /* normalize current column */
for j = k + 1 to n
rkj = qT
k a j
a j = a j −rkjqk
end
end
If A has full column rank, then rkk will not be zero. Although MGS is more stable
than CGS, we still can have a matrix Q that is far from orthogonal. This is because
∥QT Q −I∥can be larger than ε when A is ill conditioned, though the loss is much

9.5 Orthogonalization Methods
325
less than with CGS. To avoid this difﬁculty, in solving the linear system of equations
Ax ≈b with MGS we should not compute the right-hand side c1 as c1 = QT
1 b. Much
better is to treat the vector b as a column of length n + 1 and use MGS to compute the
reduced QR factorization for the following augmented matrix of order m × n + 1:
Ab
=
Q1qn+1
 R c1
0 ρ

.
(9.83)
Then the solution of the least squares problem can be found as the solution to the
n × n triangular linear system Rx = c1.
The orthogonality of the resulting matrix Q1 can also be enhanced by a reorthogo-
nalization process. This means that we need to repeat the orthogonalization procedure
for Q1 which can be considered as a form of iterative reﬁnement. We refer to [14,
53] for further reading on this subject.
Example 9.12 Classical Gram–Schmidt (CGS) orthogonalization algorithm for the
solution of least squares problems
We illustrate the CGS Algorithm9.4 on the solution of the following least squares
problem: ﬁnd x = (x1, x2, x3) subject to minx ∥Ax −y∥2
2 when the matrix A is
given by
A =
⎛
⎜⎜⎜⎜⎜⎜⎝
1
0 0
0
1 0
0
0 1
−1 1 0
−1 0 1
0 −1 1
⎞
⎟⎟⎟⎟⎟⎟⎠
and the elements of the vector y are y = (1237, 1941, 2417, 711, 1177, 475)T . We
have implemented Algorithm9.4 and applied it to the solution of this linear least
square problem. Our QR decomposition of the matrix A is
Q =
⎛
⎜⎜⎜⎜⎜⎜⎝
0.577350269189626
0.204124145231932
0.353553390593274
0
0.612372435695794
0.353553390593274
0
0
0.707106781186548
−0.577350269189626 0.408248290463863 −0.000000000000000
−0.577350269189626 −0.204124145231932 0.353553390593274
0
−0.612372435695794 0.353553390593274
⎞
⎟⎟⎟⎟⎟⎟⎠
,
R =
⎛
⎝
1.732050807568877 −0.577350269189626 −0.577350269189626
0
1.632993161855452 −0.816496580927726
0
0
1.414213562373095
⎞
⎠.
After performing the QR decomposition of A, we solved the linear least squares
problem transformed to the solution of the equation Rx = QT y with upper triangular

326
9
Numerical Solution of Linear Least Squares Problems
matrix R, by backward substitution. We have obtained the following solution of the
least squares problem: x = (1236, 1943, 2416)T .
The MATLAB® program of Section1.4 is available for running this test.6
9.6
Singular Value Decomposition
In this section we will show how the singular value decomposition (SVD) of a matrix
A allows us to reduce a linear least squares problem to a diagonal linear least squares
problem that is easer to solve.
Let us recall (see Section5.1.1) that the singular value decomposition of a matrix
A of order m × n has the form
A = UΣV T ,
(9.84)
where U is an m × m orthogonal matrix U TU = I, V is an n × n orthogonal matrix
such that V T V = I, and Σ is an m × n diagonal matrix with elements σi j on its
diagonal such that
σi j =
0
for i ̸= j,
σi ≥0 for i = j.
(9.85)
The elements σi are called singular values of A. They are ordered such that σ1 ≥
· · · ≥σn ≥0. The columns u1, . . . , um of U are called left singular vectors. The
columns v1, . . . , vn of V are called right singular vectors. We note that if m < n,
then the SVD is deﬁned for AT .
An alternative deﬁnition of SVD decomposition is formulated in the following
theorem.
Theorem 9.2 Let A be an arbitrary m × n matrix with m ≥n. Then the SVD
decomposition has the form A = UΣV T , where U is of order m × n and satisﬁes
U TU = I, V is of order n × n and satisﬁes V T V = I, and Σ = diag(σ1, . . . , σn),
where σ1 ≥· · · ≥σn ≥0. The columns u1, . . . , un of U are called left singular vec-
tors. The columns v1, . . . , vn of V are called right singular vectors. The numbers σi
are called singular values.
Proof The proof of this theorem is done by induction on m and n. This means that we
assume that the SVD decomposition exists for matrices of order (m −1) × (n −1),
and our goal is to prove that the SVD decomposition exists also for matrices of order
m × n. In this proof we assume A ̸= 0. If A = 0, we can take Σ = 0 and let U and
V be arbitrary orthogonal matrices.
Since m ≥n, let us consider the case n = 1 and write the SVD decomposition as
A = UΣV T with U = A/∥A∥2, Σ = ∥A∥2, and V = 1.
To apply the induction step, we choose the vector υ such that ∥υ∥2 = 1 and
∥A∥2 = ∥Aυ∥2 > 0. Such a vector υ exists by the deﬁnition of the two-norm of the
6The
MATLAB®
programs
can
be
found
on
the
online
version
of
Chapter
1
(doi:10.1007/978-3-319-57304-5_1).

9.6 Singular Value Decomposition
327
matrix A: ∥A∥2 = max∥υ∥2=1 ∥Aυ∥2. Let us deﬁne u =
Aυ
∥Aυ∥2 , which is a unit vector.
We choose now two matrices ˜U and ˜V such that U = (u, ˜U) is an m × m orthogonal
matrix, and V = (υ, ˜V ) is an n × n orthogonal matrix. We now multiply the matrix
A from the left by the matrix U T and from the right by the matrix V to get
U T AV =
 uT
˜U T

· A · (υ ˜V ) =
 uT Aυ uT A ˜V
˜U T Aυ ˜U T A ˜V

.
Since the vector u is chosen as u =
Aυ
∥Aυ∥2 , we observe that
uT Aυ = (Aυ)T (Aυ)
∥Aυ∥2
= ∥Aυ∥2
2
∥Aυ∥2
= ∥Aυ∥2 = ∥A∥2 ≡σ.
Next, we also observe that the following block is zero: ˜U T Aυ = ˜U T u∥Aυ∥2 = 0.
We want to prove that the block uT A ˜V will be zero also: uT A ˜V = 0. To do so, we
consider ∥(σ|uT A ˜V )∥2. We observe that ∥(σ|uT A ˜V )∥2 > σ and ∥(1, 0, . . . , 0) =
U T AV ∥2 = ∥(σ|uT A ˜V )∥2. Then using the properties of the two-norm, we can
write σ = ∥A∥2 = ∥U T AV ∥2 ≥∥(1, 0, . . . , 0)U T AV ∥2 = ∥(σ|uT A ˜V )∥2 > σ, a
contradiction.
Collecting our observations above for blocks uT Aυ, uT A ˜V and ˜U T Aυ, we can
rewrite the expression for U T AV as
U T AV =
σ
0
0 ˜U T A ˜V

=
σ 0
0 ˜A

.
(9.86)
Now we use the induction hypothesis for ˜A to obtain the matrix ˜A = U1Σ1V T
1 ,
where U1 is a matrix of order (m −1) × (n −1), Σ1 is a matrix of order (n −1) ×
(n −1), and V1 is a matrix of order (n −1) × (n −1). Thus we can rewrite (9.86)
as
U T AV =
σ
0
0 U1Σ1V T
1

=
 1 0
0 U1
 σ 0
0 Σ1
  1 0
0 V1
T
.
Multiplying the above equation by the matrix U from the left and by the matrix V T
from the right, we obtain the desired SVD decomposition of the matrix A:
A =

U
 1 0
0 U1

·
σ 0
0 Σ1

·

V
 1 0
0 V1
T
.
⊓⊔
The following theorems present some properties of the SVD decomposition that
are frequently very important in computations. We note that analogous results also
hold for the matrix A when m < n, only that we consider AT instead of A.

328
9
Numerical Solution of Linear Least Squares Problems
Theorem 9.3 Let A = UΣV T be the SVD of the m × n matrix A, where m ≥
n. Suppose that A is symmetric, with eigenvalues λi and orthonormal eigen-
vectors ui. This means that A = UU T is an eigendecomposition of A, with
 = diag(λ1, . . . , λn), U = (u1, . . . , un), and UU T = I. Then an SVD of A is
A = UΣV T , where σi = |λi| and υi = sign(λi)ui, where sign(0) = 1.
Proof Since A = UU T is an eigendecomposition of A, then by the deﬁnition an
SVD of A is A = UΣV T , where σi = |λi| and vi = sign(λi)ui with sign(0) = 1.
⊓⊔
Theorem 9.4 Let A = UΣV T be an SVD of an m × n matrix A, where m ≥n. Then
the eigenvalues of the symmetric matrix AT A are σ 2
i . The right singular vectors υi
are corresponding orthonormal eigenvectors.
Proof Let us consider the SVD decomposition of A = UΣV T and write it for AT A:
AT A = V ΣU TUΣV T = V Σ2V T .
(9.87)
We observe that by deﬁnition of the eigendecomposition, the above decomposition
is an eigendecomposition of AT A. In this decomposition, the columns of V are the
eigenvectors, and the diagonal entries of Σ2 are the eigenvalues.
⊓⊔
Theorem 9.5 Let A = UΣV T be an SVD of an m × n matrix A, where m ≥n.
Then the eigenvalues of the symmetric matrix AAT are σ 2
i and m −n zeros. The
left singular vectors ui are corresponding orthonormal eigenvectors for the eigen-
values σ 2
i . We can take any m −n other orthogonal vectors as eigenvectors for the
eigenvalue 0.
Proof Choose an m × (m −n) matrix ˜U such that (U, ˜U) is square and orthogonal.
Then we can write
AAT = UΣV T V ΣU T = UΣ2U T =

U, ˜U

·
Σ2 0
0 0

·

U, ˜U
T
.
We observe that the above decomposition is an eigendecomposition of AAT .
⊓⊔
Theorem 9.6 Let A = UΣV T be an SVD of an m × n matrix A, where m ≥n.
Let the matrix H be constructed such that H =
0 AT
A 0

, where A is square and
A = UΣV T is the SVD of A. Let Σ = diag(σ1, . . . , σn), U = (u1, . . . , un), and
V = (υ1, . . . , υn). Then the 2n eigenvalues of H are ±σi, with corresponding unit
eigenvectors
1
√
2
 υi
±ui

.
Proof The proof of this theorem is exercise given in Question9.5.
⊓⊔

9.6 Singular Value Decomposition
329
Theorem 9.7 Let A = UΣV T be the SVD of the m × n matrix A, where m ≥n. If
A has a full rank, then the solution of the linear least squares problem
min
x
∥r(x)∥2
2 = min
x
∥Ax −b∥2
2
is x = V Σ−1U T b.
Proof Let us consider the two-norm of the residual ∥r(x)∥2
2 = ∥Ax −b∥2
2 =
∥UΣV T x −b∥2
2. Since A has full rank, Σ also has full rank, and thus Σ is invertible.
Now let us construct the matrix (U, ˜U), which will be square and orthogonal. We
can write
∥UΣV T x −b∥2
2 =

U T
˜U T

(UΣV T x −b)

2
2
=

ΣV T x −U T b
−˜U T b

2
2
= ∥ΣV T x −U T b∥2
2 + ∥˜U T b∥2
2.
We observe that by making the ﬁrst term zero, ΣV T x = U T b, we will ﬁnd the
minimum of the least squares problem given by x = V Σ−1U T b.
⊓⊔
Theorem 9.8 Let A = UΣV T be the SVD of the m × n matrix A, where m ≥
n, ∥A∥2 = σ1. If A is square and nonsingular, then ∥A−1∥−1
2
= σn and ∥A∥2 ·
∥A−1∥2 = σ1
σn .
This assertion was proved in Section6.4, p. 142.
Theorem 9.9 Let A = UΣV T be the SVD of the m × n matrix A, where m ≥n.
Assume that σ1 ≥· · · ≥σr > σr+1 = · · · = σn = 0. Then the rank of A is r. The null
space of A, i.e., the subspace of vectors υ such that Aυ = 0, is the space spanned
by columns r + 1 through r of V : span(υr+1, . . . , υn). The range space of A, the
subspace of vectors of the form Aw for all w, is the space spanned by columns 1
through r of U : span(u1, . . . , ur).
Proof Let us choose an m × (m −n) matrix ˜U such that the m × m matrix ˆU =
(U, ˜U) is orthogonal. Since ˆU and V are nonsingular, A and
ˆU T AV =
 Σn×n
0(m−n)×n

≡ˆΣ
(9.88)
have the same rank r. We may claim this using our assumption about entries of the
matrix Σ.
Values of υ are in the null space of A if and only if V T υ is in the null space of
ˆU T AV = ˆΣ. This is true because Aυ = 0 if and only if ˆU T AV (V T υ) = 0.
But the null space of ˆΣ is spanned by columns r + 1 through n of the n × n
identity matrix In. This means that the null space of A is spanned by V times these
columns, i.e., υr+1 through υn.

330
9
Numerical Solution of Linear Least Squares Problems
Similarly, we can show that the range space of A is the same as ˆU times the range
space of ˆU T AV = ˆΣ, i.e., ˆU times the ﬁrst r columns of Im, or u1 through ur.
⊓⊔
Theorem 9.10 Let A = UΣV T be the SVD of the m × n matrix A, where m ≥n.
Let Bn−1 be the unit sphere in Rn: Bn−1 = {x ∈Rn : ∥x∥2 = 1}. Let A · Bn−1 be the
image of Bn−1 under A: A · Bn−1 = {Ax : x ∈Rn and ∥x∥2 = 1}. Then A · Bn−1 is
an ellipsoid centered at the origin of Rm, with principal axes σiui.
Proof We construct the set A · Bn−1 by multiplying A = UΣV T by Bn−1 step by
step. Let us assume for simplicity that A is square and nonsingular. Since V is
orthogonal and thus maps unit vectors to other unit vectors, we can write
V T · Bn−1 = Bn−1.
(9.89)
Now consider the product Σ Bn−1. Since υ ∈Bn−1 if and only if ∥υ∥2 = 1, it follows
that w ∈Σ Bn−1 if and only if ∥Σ−1w∥2 = 1 or
n

i=1
(wi/σi)2 = 1.
(9.90)
The equation above deﬁnes an ellipsoid with principal axes σiei, where ei is the ith
column of the identity matrix. Finally, multiplying each w = Συ by U just rotates
the ellipse so that each ei becomes ui, that is, the ith column of U.
⊓⊔
Theorem 9.11 Let A = UΣV T be the SVD of the m × n matrix A, where m ≥n.
Write
V = (υ1, υ2, . . . , υn)
and
U = (u1, u2, . . . , un),
so
A = UΣV T =
	n
i=1 σiuiυT
i (a sum of rank-1 matrices). Then a matrix of rank k < n closest to
A (measured with ∥· ∥2) is Ak = 	k
i=1 σiuiυT
i
and ∥A −Ak∥2 = σk+1. We may
also write Ak = UΣkV T , where Σk = diag(σ1, . . . , σk, 0, . . . , 0).
Proof The matrix Ak has rank k by construction. We can write the difference ∥A −
Ak∥2 as
∥A −Ak∥2 =

n

i=k+1
σiuiυT
i
 =

U
⎛
⎜⎜⎜⎝
0
σk+1
...
σn
⎞
⎟⎟⎟⎠V T

2
= σk+1.
(9.91)
We need to show now that there is no other matrix closer to the matrix A than
the matrix Ak. Let B be any matrix of rank k such that its null space has dimension
n −k. The space spanned by {υ1, ..., υk+1} has dimension k + 1. Since the sum of
their dimensions is (n −k) + (k + 1) > n, these two spaces must overlap. Let h be
a unit vector in their intersection. Then

9.6 Singular Value Decomposition
331
∥A −B∥2
2 ≥∥(A −B)h∥2
2 = ∥Ah∥2
2 =
UΣV T h
2
2
=
Σ(V T h)
2
2
≥σ 2
k+1
V T h
2
2
= σ 2
k+1.
□
Comparing the obtained expression ∥A −B∥2
2 ≥σ 2
k+1 with (9.91), we observe
that ∥A −Ak∥2
2 = σ 2
k+1, and thus Ak is a best approximation to A.
⊓⊔
Example 9.13 Image compression using SVD.
In this example we will demonstrate how to perform image compression using
the standard demo-library in MATLAB® with pictures. For example, we can load
the image of clown from this directory from the ﬁle clown.mat:
load clown.mat
The resulting image will be read into the array X. The size of this array will be
(in pixels):
> Size(X) = m × n = 320 × 200
Now we can simply use the svd command to perform SVD decomposition of the
matrix X:
> [U,S,V] = svd(X);
> colormap(map);
For example, to see rank k = 20 of this image, we write:
> k = 20;
> image(U(:,1:k)*S(1:k,1:k)*V(:,1:k)’);
Example 9.14 Image compression using SVD.
This is another example of how to compress an image in JPEG format that you
can produce using any digital camera and then download in MATLAB®. To read
any digital image from data ﬁle named File.jpg, we can write in the command
line in MATLAB®:
> A = imread(’File.jpg’);
Then if you write
> size(A)
you will get the size of the obtained matrix A from your image. For the image
presented in Fig.9.4a we have obtained
> size(A)
> ans
= 218 171 3
Thus, the matrix of the original image of Fig.9.4a is of size m × n with m =
218, n = 171. We obtained a three-dimensional array A since the image presented
in Fig.9.4a is color, and the MATLAB®-command imread returned the value for A
as an m × n × 3 array. We are not able simply to use the svd command in MATLAB®
for such a matrix. If we tried to apply svd, we would get the following error message:
> [U3,S3,V3] = svd(A(:,:,3));
Undeﬁned function ’svd’ for input arguments of type ‘uint8’.

332
9
Numerical Solution of Linear Least Squares Problems
20
40
60
80
100 120 140 160
20
40
60
80
100 120 140 160
20
40
60
80
100 120 140 160
20
40
60
80
100 120 140 160
20
40
60
80
100 120 140 160
20
40
60
80
100 120 140 160
20
40
60
80
100
120
140
160
180
200
20
40
60
80
100
120
140
160
180
200
a) Original image
b) Rank k=15 approximation
20
40
60
80
100
120
140
160
180
200
20
40
60
80
100
120
140
160
180
200
c) Rank k=10 approximation
d) Rank k=6 approximation
20
40
60
80
100
120
140
160
180
200
20
40
60
80
100
120
140
160
180
200
e) Rank k=5 approximation
f) Rank k=4 approximation
Fig. 9.4 Example9.14. Image compression using SVD decomposition for different rank-k approx-
imations.

9.6 Singular Value Decomposition
333
To avoid this error message, we need convert A from ’uint8’ format to the
double format using the following command:
> DDA = im2double(A);
Then the size of the matrix DDA will be m × n × 3. At the next step, we perform
SVD decomposition for every three entries of DDA:
> [U1,S1,V1] = svd(DDA(:,:,1));
> [U2,S2,V2] = svd(DDA(:,:,2));
> [U3,S3,V3] = svd(DDA(:,:,3));
Finally, we can perform image compression for different rank-k approximations.
For example, let us choose rank k = 15. Then using the following commands, we
can compute new approximation matrices svd1, svd2, svd3:
> svd1 = U1(:,1:k)* S1(1:k,1:k)* V1(:,1:k)’;
> svd2 = U2(:,1:k)* S2(1:k,1:k)* V2(:,1:k)’;
> svd3 = U3(:,1:k)* S3(1:k,1:k)* V3(:,1:k)’;
To obtain different compressed images that are similar to the images of Fig.9.4,
we write:
> DDAnew = zeros(size(DDA));
> DDAnew(:,:,1) = svd1;
> DDAnew(:,:,2) = svd2;
> DDAnew(:,:,3) = svd3;
Then to see the approximated image we use the following command:
> image(DDAnew);
9.6.1
Rank-Deﬁcient Least Squares Problems
In all our considerations above, we have assumed that our matrix A has full column
rank, or r(A) = n. if A has linearly dependent columns, so that r(A) < n, then it
is still possible to perform QR factorization, but the matrix R will be singular. This
means that many vectors can give the minimal norm ∥Ax −b∥2, and the least squares
solution is not unique. This can happen in the case of insufﬁcient data collection,
digital image restoration, computing the inverse Laplace transform, in other words,
in ill-posed problems [110].
The next proposition says that in the case of a nearly rank-deﬁcient matrix A, the
least squares solution is not unique.
Proposition 9.1 Let A be an m × n matrix with m ≥n and rank A = r < n. Then
there is an (n −r)-dimensional set of vectors that minimizes ∥Ax −b∥2.
Proof Let Az = 0. Then if x minimizes ∥Ax −b∥2, x + z also minimizes ∥A(x +
z) −b∥2.
This means that the least squares solution is not unique.
⊓⊔

334
9
Numerical Solution of Linear Least Squares Problems
Below, we deﬁne the Moore–Penrose7 pseudoinverse A+ for a full-rank matrix
A. The pseudoinverse allows us to write the solution of the full-rank overdetermined
least squares problem minx ∥Ax −b∥2 simply as x = A+b.
Suppose that A is m × n with m > n and has full rank, with A = QR = UΣV T
representing QR and SVD decompositions of A, respectively. Then
A+ ≡(AT A)−1AT = R−1QT = V Σ−1U T
is called the Moore–Penrose pseudoinverse of A. If m < n, then A+ ≡AT (AAT )−1.
If A is square and has full rank, then the solution of the full-rank overdetermined
least squares problem minx ∥Ax −b∥2 reduces to x = A−1b. The matrix A+ is com-
puted by the function pinv(A) in MATLAB®.
In the case of a rank-deﬁcient matrix A, we have the following deﬁnition of the
Moore–Penrose pseudoinverse A+.
Suppose that A is m × n with m > n and is rank-deﬁcient with rank r < n. Let
A = UΣV T = U1Σ1V T
1 be SVD decompositions of A such that
A = (U1,U2)
Σ1 0
0 0

(V1, V2)T = U1Σ1V T
1 .
Here, (Σ1) is of size r × r and is nonsingular; U1 and V1 have r columns. Then
A+ ≡V1Σ−1
1 U T
1
is called the Moore–Penrose pseudoinverse for the rank-deﬁcient matrix A.
The solution of the least-squares problem is always x = A+b. The next proposi-
tion states that if A is nearly rank-deﬁcient, then the solution x of Ax = b will be ill
conditioned and very large.
Proposition 9.2 Let σmin > 0 be the smallest singular value of the nearly rank-
deﬁcient matrix A. Then
• If x minimizes ∥Ax −b∥2, then ∥x∥2 ≥|uT
n b|
σmin , where un is the last column of U in
the SVD decomposition of A = UΣV T .
• Changing b to b + δb can change x to x + δx, where ∥δx∥2 can be estimated as
∥δb∥2
σmin , or the solution is very ill conditioned.
Proof
1. By Theorem9.7, we have that for the case of a full-rank matrix A, the
solution of Ax = b is given by x = (UΣV T )−1b = V Σ−1U T b. The matrix
A+ = V Σ−1U T is the Moore–Penrose pseudoinverse of A. Thus, we can write
this solution also as
x = V Σ−1U T b = A+b.
7Eliakim Hastings Moore (1862–1932) was an American mathematician. Sir Roger Penrose (born
1931) is an English mathematical physicist, mathematician, and philosopher of science.

9.6 Singular Value Decomposition
335
Then taking norms of both sides of the above expression yields
∥x∥2 = ∥Σ−1U T b∥2 ≥|(Σ−1U T b)n| = |uT
n b|
σmin
,
(9.92)
where |(Σ−1U T b)n| is the nth column of this product.
2. We now apply (9.92) for ∥x + δx∥instead of ∥x∥to get
∥x + δx∥2 = ∥Σ−1U T (b + δb)∥2 ≥|(Σ−1U T (b + δb))n|
= |uT
n (b + δb)|
σmin
= |uT
n b + uT
n δb|
σmin
.
(9.93)
We observe that
|uT
n b|
σmin
+ |uT
n δb|
σmin
≤∥x + δx∥2 ≤∥x∥2 + ∥δx∥2.
Choosing δb parallel to un and applying again (9.92) for estimating ∥x∥2, we
have
∥δx∥2 ≥∥δb∥2
σmin
.
(9.94)
⊓⊔
In the next proposition we prove that the minimum-norm solution x is unique and
may be well conditioned if the smallest nonzero singular value in Σ is not too small.
Proposition 9.3 Let a matrix A be singular and suppose that x minimizes
∥Ax −b∥2. Let A = UΣV T have rank r < n. Write the SVD decomposition of
A as
A = (U1,U2)
Σ1 0
0
0

(V1, V2)T = U1Σ1V T
1 .
Here, (Σ1) is of size r × r and is nonsingular; U1 and V1 have r columns. Let
σ = σmin(Σ1). Then:
• All solutions x can be written as x = V1Σ−1
1 U T
1 + V3z.
• The solution x has minimal norm ∥x∥2 when z = 0. Then x = V1Σ−1
1 U T
1 and
∥x∥2 ≤∥b∥2
σ .
• Changing b to b + δb can change x to ∥δb∥2
σ
.
Proof We choose the matrix ˜U such that (U, ˜U) = (U1,U2, ˜U) is an m × m orthog-
onal matrix. Then using the property of the norm, we can write

336
9
Numerical Solution of Linear Least Squares Problems
∥Ax −b∥2
2 = ∥(U1,U2, ˜U)T (Ax −b)∥2
2
=


⎛
⎝
U T
1
U T
2
˜U T
⎞
⎠(U1Σ1V T
1 x −b)


2
2
=


⎛
⎝
I r×r
Om×(n−r)
0m×m−n
⎞
⎠(Σ1V T
1 x −(U1,U2, ˜U)T · b)


2
2
=


⎛
⎝
Σ1V T
1 x −U T
1 b
−U T
2 b
−˜U T b
⎞
⎠


2
2
= ∥Σ1V T
1 x −U T
1 b∥2
2 + ∥U T
2 b∥2
2 + ∥˜U T b∥2
2.
To prove part 1, we observe that ∥Ax −b∥2 is minimized when Σ1V T
1 x −U T
1
b = 0. Using Proposition3.1, we can also write that the vector x = (Σ1V T
1 )−1U T
1 b +
V3z or x = V1Σ−1
1 U T
1 b + V3z is also a solution of this minimization problem, where
V3z = V T
1 V2z = 0, since the columns of V1 and V2 are orthogonal.
To prove part 2, we note that since the columns of V1 and V2 are orthogonal, it
follows by the Pythagorean theorem that we have
∥x∥2
2 = ∥V1Σ−1
1 U T
1 b∥2 + ∥V3z∥2,
(9.95)
which is minimized for z = 0.
For a proof of part 3, we change b to δb in (9.95) to get
∥V1Σ−1
1 U T
1 δb∥2 ≤∥V1Σ−1
1 U T
1 ∥2 · ∥δb∥2 = ∥Σ−1
1 ∥2 · ∥δb∥2 = ∥δb∥2
σ
,
(9.96)
where σ is the smallest nonzero singular value of A. In this proof we used the property
of the norm that ∥Q AZ∥2 = ∥A∥2 if Q, A are orthogonal.
⊓⊔
9.6.2
How to Solve Rank-Deﬁcient Least Squares Problems
In this section we discuss how to solve rank-deﬁcient least squares problems using
QR decomposition with pivoting. QR decomposition with pivoting is cheaper but
can be less accurate than the SVD technique for the solution of rank-deﬁcient least
squares problems. If A has rank r < n with r independent columns, then the QR
decomposition can look like this:
A = QR = Q ·
⎛
⎝
R11 R12
0
0
0
0
⎞
⎠
(9.97)

9.6 Singular Value Decomposition
337
with nonsingular R11 of order r × r and R12 of order r × (n −r). We can try to
obtain a matrix
R =
⎛
⎝
R11 R12
0
R22
0
0
⎞
⎠,
(9.98)
in which the elements of R22 are very small and are of order ε∥A∥2. If we set R22 = 0
and choose (Q, ˜Q), which is square and orthogonal, then we will minimize
∥Ax −b∥2
2 =

QT
˜QT

(Ax −b)

2
2
=

QT
˜QT

(QRx −b)

2
2
=

Rx −QT b
−˜QT b

2
2
= ∥Rx −QT b∥2
2 + ∥˜QT b∥2
2.
(9.99)
Here we again used the property of the norm that ∥Q AZ∥2 = ∥A∥2 if Q, A are
orthogonal.
Let us now decompose Q = (Q1, Q2) with x = (x1, x2)T and
R =
R11 R12
0
0

(9.100)
such that Eq.(9.99) becomes
∥Ax −b∥2
2 =

R11 R12
0
0

·
x1
x2

−
QT
1 b
QT
2 b

2
2
+ ∥˜QT b∥2
2
= ∥R11x1 + R12x2 −QT
1 b∥2
2 + ∥QT
2 b∥2
2 + ∥˜QT b∥2
2.
(9.101)
We now take the derivative with respect to x to get (∥Ax −b∥2
2)′
x = 0. We see
that the minimum is achieved when
x =

R−1
11 (QT
1 b −R12x2)
x2

(9.102)
for every vector x2. If R11 is well conditioned and R−1
11 R12 is small, then the choice
x2 = 0 will be a good one.
The method just described is not reliable for all rank-deﬁcient least squares prob-
lems. This is because R can be nearly rank-deﬁcient in case when we cannot construct
R22 with all small elements. In this case, QR decomposition with column pivoting

338
9
Numerical Solution of Linear Least Squares Problems
can help: we factorize AP = QR with a permutation matrix P. To compute this
permutation, we proceed as follows:
• In all columns from 1 to n, at step i we select from the unﬁnished decomposition
of part A in columns i to n and rows i to m the column with largest norm, and
exchange it with the ith column.
• Then we compute the usual Householder transformation to zero out column i in
entries i + 1 to m.
Much research has been devoted to more advanced algorithms called rank-
revealing QR algorithms, which detect the rank faster and more efﬁciently; see [15,
18] for details.
9.7
Software for the Solution of Linear Least Squares
Problems
We list available packages and routines that solve linear least squares problems
in Table9.1. Usually, we use MATLAB® for implementation of the solution of
least squares problems. Here, the backslash \ is used for the solution of square and
rectangular linear systems of the form Ax = b. The solution is given as x = A\b.
The QR decomposition of a matrix A in MATLAB® is computed as the function
[Q, R] = qr(A), and the SVD decomposition is given as the function [U, S, V ] =
svd(A).
Many statistical packages such as BMDP, Minitab, Omnitab, S, S-plus, SAS,
SPSS, as well as the statistical toolbox in MATLAB®, have extensive software for
solving least squares problems.
TheprogramsofSections1.4–1.8solvelinearleastsquaresproblemofpolynomial
ﬁtting.8 see Questions 1 through 4 for details.
Table 9.1 Software for linear least squares problems (LLSP)
Package
Factorization
Solution of LLSP
Rank-deﬁcient LLSP
MATLAB®
qr
\
svd
FMM [58]
svd
svd
IMSL
lqrrr
lqrsl
lsqrr
KMN [61]
sqrls
sqrls
ssvdc
LAPACK [3]
sqeqrf
sormqr/strtrs
sgeqpf/stzrqf
Lawson and Hanson [68]
hft
hs1
hfti
LINPACK [69]
sqrdc
sqrsl
sqrst
NAPACK [88]
qr
over
sing/rsolve
NUMAL [89]
lsqortdec
lsqsol
solovr
SOL [114]
hredl
qrvslv
mnlnls
8The
MATLAB®
programs
can
be
found
on
the
online
version
of
Chapter
1
(doi:10.1007/978-3-319-57304-5_1).

Questions
339
Questions
9.1 (Programming)
Solve the least squares problem minc ∥Ac −y∥2
2 of Example9.2 by the method
of normal equations and QR decomposition (either Algorithm9.4 or Algorithm9.5).
The matrix A in this least squares problem is a Vandermonde matrix (9.8). This
means that the columns of the matrix A are powers of the vector x such that y(x) =
	d
i=0 cixi−1, d is the degree of the polynomial, and (xi, yi), i = 1, ..., m.
Use your own code or the programs of Section1.4.9 Show that we get an erratic
ﬁt to the function for polynomials of degree greater than 18. Compare both methods
for different values of d by computing the relative error
e = ∥y −y∗∥2
∥y∗∥2
.
(9.103)
Here, the y∗
i , i = 1, . . . , m are the exact values of the function y∗, and the yi, i =
1, . . . , m are the computed values of generated data y. Report your results in a table
for different discretizations of the interval for x and different values of d.
9.2 (Programming)
Solve the least squares problem
min
c
m

i=1
(yi −f (xi, c))2
(9.104)
of Example9.2 by approximating the function f (xi, c) by linear splines; see Exam-
ple9.3.
9.3 (Programming)
Solve the least squares problem
min
c
m

i=1
(yi −f (xi, c))2
(9.105)
of Example9.2 by approximating the function
f (xi, c) by bellsplines; see
Example9.4.
9.4 (Programming)
Solve the problem of ﬁtting a polynomial p(x) = 	d
i=0 cixi−1 of degree d to data
points (xi, yi), i = 1, ..., m, in the plane by the method of normal equations and QR
decomposition (either Algorithm9.4, 9.5). Choose the degree of the polynomial to
be d = 5 and then d = 14, choose the interval x ∈[0, 1], discretize it using N points,
9The
MATLAB®
programs
can
be
found
on
the
online
version
of
Chapter
1
(doi:10.1007/978-3-319-57304-5_1).

340
9
Numerical Solution of Linear Least Squares Problems
and compute discrete values of y(x) as yi = y(xi) = p(xi). Our goal is to recover
the coefﬁcients ci of the polynomial p(x) = 	d
i=0 cixi−1 by solving the system
Ac = y
(9.106)
using the method of normal equations and QR decomposition (Algorithm9.4 or
Algorithm9.5). Here, the columns of the matrix A are powers of the vector x that
create the Vandermonde matrix (9.8). Compare both methods for d = 5 and then for
d = 14 by computing the relative error
e = ∥c −c∗∥2
∥c∗∥2
.
(9.107)
Here, the c∗
i are the exact values of the computed coefﬁcients ci.
Hints:
• Compute ﬁrst the values of the right-hand side of (9.106), the vector yi, at the points
xi, i = 1, ...., m, with known values of the coefﬁcients ci. Take exact values ci = 1.
• The matrix A is a Vandermonde matrix:
A =
⎛
⎜⎜⎜⎜⎜⎝
1 x1 x2
1 . . . xd
1
1 x2 x2
2 . . . xd
2
1 x3 x2
3 . . . xd
3
...
...
... ... ...
1 xm x2
m . . . xd
m
⎞
⎟⎟⎟⎟⎟⎠
.
(9.108)
Here, xi, i = 1, ...., m, are points in the interval x ∈[0, 1], and d is the degree of
the polynomial.
• Use the method of normal equations and QR decomposition to solve the resulting
system Ax = y. Compare your results in a table by computing the relative error
(12.24) for both methods for different discretizations of the interval x ∈[0, 1].
9.5 Prove Theorem9.6:
Let H =
0 AT
A 0

, where A is square and A = UΣV T is the SVD of A.
Let Σ = diag(σ1, . . . , σn), U = (u1, . . . , un), and V = (υ1, . . . , υn). Then the 2n
eigenvalues of H are ±σi, with corresponding unit eigenvectors
1
√
2
 υi
±ui

.
9.6 (Programming)
We deﬁne the covariance matrix of the m × n least squares problem minx ∥Ax −
b∥2
2 by
δ2(AT A)−1,
(9.109)
where
δ2 = ∥b −Ax∥2
2
m −n
(9.110)

Questions
341
at the least squares solution x.
The inverse of the covariance matrix cannot be computed explicitly. Instead, for
computing AT A we use
(AT A)−1 = (RT R)−1,
(9.111)
where R is the upper triangular matrix in the QR decomposition of A.
Implement the computation of the covariance matrix (9.109) using only the com-
puted matrix R and then (9.111). Test your implementation on some examples to
show that it gives the same result as computing (AT A)−1.
9.7 Let the matrix A be deﬁned as
A =
⎛
⎝
5 4 3
4 6 1
3 1 7
⎞
⎠.
(9.112)
1. Transformthematrix A totridiagonalformusingHouseholderreﬂection.Describe
all steps of this transformation.
2. Transform the matrix A to tridiagonal form using Givens rotation. Describe this
procedure step by step.
9.8 Let us consider a weighted least squares problem. When some entries of Ax −b
are more important than other components, we can use scale factors di to weight
them. Then instead of the solution minx ∥Ax −b∥2, we are interested in the solution
minx ∥D(Ax −b)∥2. Here D is a diagonal matrix with entries di on its diagonal.
Derive the method of normal equations for this problem.
9.9 Let A be of order m × n with SVD A = UΣV T . Compute the SVDs of the
following matrices in terms of U, Σ, and V :
1. (AT A)−1,
2. (AT A)−1AT ,
3. A(AT A)−1,
4. A(AT A)−1AT .
9.10 Assume that we have three data points (xi, yi) = (0, 1), (1, 2), (3, 3) and we
want ﬁt them by a polynomial y = c0 + c1x.
1. Formulate an overdetermined linear system for the least squares problem.
2. Write corresponding normal equations.
3. Compute the least squares solution using Cholesky factorization.

342
9
Numerical Solution of Linear Least Squares Problems
9.11 Let A be of order n × n. Prove that:
1. AT = A,
2. AT A = I,
3. A2 = I.
What is a nontrivial class of matrices that have all these properties called? Give an
example of a 3 × 3 matrix that has all three properties (other than I or a permutation
of it).
9.12 Show that if a vector u is nonzero, then the matrix
P = I −2uuT
uT u
is orthogonal and symmetric.
9.13 Let a be any nonzero vector such that u = a −αe1 with α = ±∥a∥2 and
P = I −2uuT
uT u .
Show that Pa = αe1.
9.14 Prove that the pseudoinverse A+ of a matrix A of order m × n satisﬁes the
following Moore–Penrose conditions:
1. AA+A = A,
2. A+AA+ = A+,
3. (AA+)T = AA+,
4. (A+A)T = A+A.
9.15 Let A+ be the pseudoinverse of a matrix A of order m × n. Prove:
1. If m = n and A is nonsingular, then A+ = A−1.
2. If m > n and A has rank n, then A+ = (AT A)−1AT .
3. If m < n and A has rank m, then A+ = AT (AAT )−1.
9.16 (Programming)
Consider the nonlinear Vogel–Fulcher–Tammans equation of some chemical reac-
tion:
y(T ) = A · exp−
E
T −T0 .
Determine parameters A, E, T0, which are positive constants, in terms of T (which
is temperature in the model equation (in kelvins)) and output data y(T ).
Hint: Transform ﬁrst the nonlinear function y(T ) to a linear one and then solve a
linear least squares problem. Discretize T by N points and compute discrete values
of y(T ) as yi = y(Ti) for the known values of the parameters A, E, T0. Then forget
about these parameters (we will call them exact parameters A∗, E∗, T ∗
0 ) and solve

Questions
343
the linear least squares problem using the method of normal equations (optionally
QR decomposition) in order to recover these exact parameters.
Tryaddingrandomnoiseδ todata y(T )usingtheformula yσ(T ) = y(T )(1 + δα),
where α ∈(−1, 1) is a randomly distributed number and δ is the noise level (if noise
in the data is 5%, then δ = 0.05).
Analyze your results by computing the relative errors eA, eE, eT0 in the computed
parameters as
eA = ||A −A∗||2
||A∗||2
,
eE = ||E −E∗||2
||E∗||2
,
eT0 = ||T0 −T ∗
0 ||2
||T ∗
0 ||2
.
(9.113)
Here, A∗, E∗, T ∗
0 are exact values, and A, E, T0 are computed values.
9.17 (Programming)
Suppose that a nonlinear model function is given as
f (x, c) = Aec1x + Bec2x, A, B = const > 0,
(9.114)
and our goal is to ﬁt this function using the Gauss–Newton method. In other words,
we want to use following formula for iteratively updating of c = (c1, c2):
ck+1 = ck −[J T (ck)J(ck)]−1J T (ck)r(ck),
(9.115)
where k is the iteration number and J(ck) is the Jacobian matrix of the residual r(ck).
We deﬁne the residual function
r(c) = y −f (x, c),
(9.116)
where y = yi, i = 1, ..., m, are known data points (use information in Question9.16
to generate data y = yi, i = 1, ..., m).
Add random noise δ to data y = yi, i = 1, ..., m, using the formula yσ(x, c) =
f (x, c)(1 + δα), where α ∈(−1, 1) is a randomly distributed number and δ is the
noise level (if the noise in the data is 5%, then δ = 0.05).
Analyze the results obtained by computing the relative errors ec in the computed
parameters c = (c1, c2) as
ec = ||c −c∗||2
||c∗||2
.
(9.117)
Here c∗= (c∗
1, c∗
2) are exact values.

Chapter 10
Algorithms for the Nonsymmetric Eigenvalue
Problem
In this chapter, we will present the main algorithms for solving the nonsymmet-
ric eigenvalue problem using direct methods. Direct methods are usually applied
to dense matrices, and iterative methods such as the Rayleigh–Ritz method and
Lanczos’s algorithm are applied to sparse matrices. Iterative methods usually can
compute not all of the eigenvalues and eigenvectors, but only some subset, and their
convergence depends on the structure of the matrix. We will begin with an analysis
of the simplest direct method, called the power method, and then continue to con-
sider more complicated methods such as inverse iteration, orthogonal iteration, QR
iteration, QR iteration with shifts, and Hessenberg reduction. To simplify our pre-
sentation in this chapter, we will assume that the matrix A is real. We will illustrate
the performance of every method by MATLAB® programs which are available in
the online version of Chapter 1 (doi:10.1007/978-3-319-57304-5_1). We will test
these programs by computing eigenvalues and eigenvectors for different kinds of
matrices, such as matrices with real and complex eigenvalues, and matrices with
different multiplicities of eigenvalues.
10.1
Power Method
This method can ﬁnd only the largest absolute eigenvalue and corresponding eigen-
vector for a matrix A.
Algorithm 10.1 The power method.
0. Set i = 0 and initialize x0.
1. Compute yi+1 = Axi.
2. Compute the approximate normalized eigenvector as xi+1 = yi+1/∥yi+1∥.1
1In this chapter, the norm of a vector is ∥· ∥2, as deﬁned on p. 211.
© Springer International Publishing AG 2017
L. Beilina et al., Numerical Linear Algebra: Theory and Applications,
DOI 10.1007/978-3-319-57304-5_10
345

346
10
Algorithms for the Nonsymmetric Eigenvalue Problem
3. Compute the approximate eigenvalue ˜λi+1 = xT
i+1Axi+1.
4. Stop updating the approximate eigenvalue and set ˜λM = ˜λi+1, M = i + 1, if either
|˜λi+1 −˜λi| ≤θ or the absolute values of the differences |˜λi+1 −˜λi| stabilize. Here
θ is a tolerance number. Otherwise, set i = i + 1 and go to step 1.
Theorem 10.1 Let A be a diagonalizable matrix,2 i.e., A = SΛS−1, where the matrix
Λ is diag(λ1, λ2, . . . , λn) and the eigenvalues satisfy the inequalities
|λ1| > |λ2| ≥· · · ≥|λn|.
Write the matrix S = (s1, s2, . . . , sn), where the columns si are the corresponding
eigenvectors that also satisfy ∥si∥= 1. Then the approximate eigenvector computed
in step 2 of Algorithm 10.1 converges to the eigenvector s1, which corresponds to λ1,
and the approximate eigenvalue converges to λ1.
Proof First we will prove the theorem for the case that A is a diagonal matrix. Let
A = diag(λ1, ..., λn), with |λ1| > |λ2| ≥· · · ≥|λn|. In this case, the eigenvectors
xi of A are columns i1, i2,…, in of the identity matrix I. We note that the factors
1/∥yi+1∥in step 2 of Algorithm 10.1 scale xi+1 to be a unit vector and do not change
its direction. Then xi can also be written as xi = Aix0/∥Aix0∥. Let us write the vector
x0 in the form x0 = S(S−1x0) = S((x(0)
1 , . . . , x(0)
n )T), where the matrix S is the identity
and x0 = (x(0)
1 , . . . , x(0)
n )T. Assuming that x(0)
1
̸= 0, we get
Aix0 ≡Ai
⎛
⎜⎜⎜⎝
x(0)
1
x(0)
2...
x(0)
n
⎞
⎟⎟⎟⎠=
⎛
⎜⎜⎜⎝
x(0)
1 λi
1
x(0)
2 λi
2
...
x(0)
n λi
n
⎞
⎟⎟⎟⎠= x(0)
1 λi
1
⎛
⎜⎜⎜⎜⎜⎝
1

x(0)
2 /x(0)
1
	
(λ2/λ1)i
...

x(0)
n /x(0)
1
	
(λn/λ1)i
⎞
⎟⎟⎟⎟⎟⎠
.
(10.1)
Using (10.1), we observe that all the fractions |λj/λ1|, j = 2, . . . , n, are less than
1. At every ith iteration, Aix0 becomes more nearly parallel to i1, so that xi =
Aix0/∥Aix0∥will be closer to ±i1, which is an eigenvector corresponding to the
largest eigenvalue λ1. Since xi converges to ±i1, the computed eigenvalue ˜λi = xT
i Axi
converges to the largest eigenvalue λ1.
Consider now the general case in which the matrix A = SΛS−1 is diagonalizable.
We write again the vector x0 as x0 = S(S−1x0) = S((x(0)
1 , . . . , x(0)
n )T) to get
Ai = (SΛS−1) · · · (SΛS−1)



i times
= SΛiS−1.
Here we have used the fact that all S−1S pairs cancel. Because of that, we can write
2See Section4.2.3, p. 120.

10.1 Power Method
347
Aix0 = (SΛiS−1)S
⎛
⎜⎜⎜⎝
x(0)
1
x(0)
2...
x(0)
n
⎞
⎟⎟⎟⎠= S
⎛
⎜⎜⎜⎝
x(0)
1 λi
1
x(0)
2 λi
2
...
x(0)
n λi
n
⎞
⎟⎟⎟⎠= x(0)
1 λi
1S
⎛
⎜⎜⎜⎜⎜⎝
1

x(0)
2 /x(0)
1
	
(λ2/λ1)i
...

x(0)
n /x(0)
1
	
(λn/λ1)i
⎞
⎟⎟⎟⎟⎟⎠
.
Similarly to the ﬁrst case, the vector in parentheses converges to i1 = (1, 0, ..., 0), so
Aix0 gets closer and closer to a multiple of Si1 = s1, the eigenvector corresponding
to λ1. Therefore, ˜λi = xT
i Axi converges to sT
1 As1 = sT
1 λ1s1 = λ1.
□
Remark 10.1
1. One of the drawbacks of the power method is the assumption that the element
x(0)
1
is nonzero, i.e., that x0 is not from the invariant subspace span{s2, . . . , sn}.3
If x0 is chosen as a random vector, this is true with very high probability.
2. A major drawback is that the power method converges only to the eigenvalue
of the largest absolute magnitude. Theorem 10.1 states that the power method
converges under the assumption that all other eigenvalues of the matrix are smaller
in absolute value.
3. The rate of convergence depends on the ratios |λ2/λ1| ≥· · · ≥|λn/λ1|. If the
ratios |λ2/λ1| ≥· · · ≥|λn/λ1| are much smaller than 1, then we will get faster
convergence. Otherwise, convergence will be slower.
Below we present some examples that illustrate convergence of the power method
for different kinds of matrices A. As an initial guess for the approximate eigenvector
x0, we take normalized randomly distributed numbers in the interval (0, 1). The
MATLAB® programs of Section1.9 are available for running all tests of this section.4
Example 10.1 In this example, we test the matrix
A =
⎛
⎝
5 0 0
0 2 0
0 0 −5
⎞
⎠
with the exact eigenvalues 5, 2, and −5. The MATLAB® program of Section1.9 is
available for running of this test. The power method can converge to the exact ﬁrst
eigenvalue 5, as well as to a completely erroneous eigenvalue. This is because two
eigenvalues of this matrix, 5 and −5, have the same absolute values, |5| = | −5|, as
well as because the initial guess x(0)
1
in the MATLAB® program is chosen randomly.
Thus, assumptions 1, 2 of Remark 10.1 about the convergence of the power method
are not fulﬁlled.
3See Theorem 3.4, p. 83.
4All MATLAB® programs referenced on this page can be found on the online version of Chapter 1
(doi:10.1007/978-3-319-57304-5_1).

348
10
Algorithms for the Nonsymmetric Eigenvalue Problem
Example 10.2 In this example, the matrix A is given by
A =
⎛
⎜⎜⎝
3 7 8 9
5 −7 4 −7
1 −1 1 −1
9 3 2 5
⎞
⎟⎟⎠.
This matrix has four different real reference eigenvalues5 λ = (λ1, ..., λ4) given by
λ = (12.3246, −11.1644, −0.3246, 1.1644).Thus,allassumptionsofTheorem10.1
about the matrix A are fulﬁlled, and we run the MATLAB® program of Section1.9
to get the reference eigenvalue 12.3246.6
Example 10.3 Now we take the matrix
A =
⎛
⎝
0 −5
2
6 0 −12
1 3
0
⎞
⎠
with the following one real and two complex eigenvalues (with the largest absolute
value):
λ = (1.4522, −0.7261 + 8.0982i, −0.7261 −8.0982i).
We
run
the
MATLAB® program of Section1.9 and observe that the power method does not
converge in this case. Clearly, starting from a real initial approximation x0 ∈R3 in
Algorithm 10.1, we cannot compute an approximate complex eigenvalue.
Example 10.4 In this example, the matrix A has order 5. The elements of this matrix
are uniformly distributed pseudorandom numbers in the open interval (0, 1). We run
the MATLAB® program of Section1.9 and observe that sometimes we can obtain
a good approximation to the eigenvalue 2.9. In a second round of computations, we
can get a completely different erroneous eigenvalue. This is because we randomly
generate elements of the matrix A as well as because the initial guess x(0)
1
in the
MATLAB® program is chosen randomly. Thus, assumptions 1, 2 of Remark 10.1
about the convergence of the power method cannot be fulﬁlled. This example is
similar to Example 10.1, where convergence was not achieved.
10.2
Inverse Iteration
The method of inverse iteration can ﬁnd all eigenvalues and eigenvectors obtained by
applying the power method for (A −σI)−1 for some shift σ. This means that we will
apply the power method to the matrix (A −σI)−1 instead of A, which will converge
to the eigenvalue closest to σ, rather than just λ1. This method is called the method
of inverse iteration or the inverse power method.
5We get the reference eigenvalues in all examples using the command eig(A) in MATLAB®.
These eigenvalues are computed to high precision.
6All MATLAB® programs referenced on this page can be found on the online version of Chapter 1
(doi:10.1007/978-3-319-57304-5_1).

10.2 Inverse Iteration
349
Algorithm 10.2 The method of inverse iteration.
0. Set i = 0 and initialize x0. Choose a shift σ.
1. Compute yi+1 = (A −σI)−1xi.
2. Compute the approximate normalized eigenvector as xi+1 = yi+1/∥yi+1∥.
3. Compute the approximate eigenvalue ˜λi+1 = xT
i+1Axi+1.
4. Stop updating the approximate eigenvalue and set ˜λM = ˜λi+1, M = i + 1, if either
|˜λi+1 −˜λi| ≤θ or the absolute values of the differences |˜λi+1 −˜λi| have stabi-
lized. Here θ is a tolerance number. Otherwise, set i = i + 1 and go to step 1.
Theorem 10.2 Let A be a diagonalizable matrix, i.e., A = SΛS−1, where the matrix
Λ is diag(λ1, λ2, . . . , λn). Assume that for a given shift σ, the following inequalities
hold:
|λk −σ| < |λi −σ| ∀i ̸= k.
(10.2)
Write S = (s1, s2, . . . , sn), where the columns si are the corresponding eigenvectors
and they also satisfy ∥si∥= 1. Then the approximate eigenvector computed at step
2 of Algorithm 10.2 converges to the eigenvector sk, which corresponds to λk, and
the approximate eigenvalue converges to λk.
Proof We begin the proof by noting that the matrix A = SΛS−1 is diagonalizable.
ThenA −σI = S(Λ −σI)S−1,andhence(A −σI)−1 = S(Λ −σI)−1S−1.Thusthe
matrix (A −σI)−1 has the same eigenvectors si as A with the corresponding eigen-
values ((Λ −σI)−1)jj = (λj −σ)−1. By assumption, |λk −σ| is smaller than all the
other |λi −σ|. This means that (λk −σ)−1 is the largest eigenvalue in absolute value.
As in the proof of Theorem 10.1, we write x0 = S(S−1x0) = S((x(0)
1 , . . . , x(0)
n )T) and
assume xk(0) ̸= 0. Then we get
(A −σI)i = (S(Λ −σI)S−1) · · · (S(Λ −σI)S−1)



i times
= S(Λ −σI)iS−1,
where all S−1S pairs cancel. This means that
(A −σI)−ix0 = (S(Λ −σI)−iS−1)S
⎛
⎜⎜⎜⎝
x(0)
1
x(0)
2...
x(0)
n
⎞
⎟⎟⎟⎠
=S
⎛
⎜⎝
x(0)
1 (λ1 −σ)−i
...
x(0)
n (λn −σ)−i
⎞
⎟⎠= xk
(0)(λk −σ)−iS
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝

x(0)
1 /x(0)
k
	 
λk −σ)i/(λ1 −σ
i
...
1
...

x(0)
n /x(0)
k
	 
λk −σ)i/(λn −σ
i
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
,

350
10
Algorithms for the Nonsymmetric Eigenvalue Problem
where we put 1 in the kth entry. Since by assumption (10.2), all the fractions
|(λk −σ)/(λi −σ)| are less than 1, and the vector in parentheses will approximate
ik such that ∥(A −σI)−ix0∥will be closer to a multiple of Sik = sk, which is the
eigenvector corresponding to λk. As in Theorem 10.1, we see that ˜λi = xT
i Axi also
converges to λk.
□
Remark 10.2
1. The advantage of inverse iteration over the power method is the ability to converge
to any desired eigenvalue (the one nearest the shift σ).
2. By choosing σ very close to a desired eigenvalue, we can achieve rapid conver-
gence and are thus not as limited by the proximity of nearby eigenvalues as in
the original power method.
3. The method is very effective when we have a good approximation to an eigenvalue
and want only its corresponding eigenvector.
To test the performance of the inverse iteration method, we run the MATLAB®
program of Section1.10 with different shifts σ.7 We tested the same matrices as in
the power method of Section10.1, except Example 10.1.
Example 10.5 In this example, we tested the matrix
A =
0 10
0 0

,
which has the exact eigenvalues λ = (0, 0) with multiplicity m = 2. Note that in this
example and in all other examples of this section, we made the additional transfor-
mation of the original matrix A as ˜A = QTAQ, where Q was an orthogonal matrix
that was generated in MATLAB® as Q = orth(rand(n, n)), where n is the size of
the matrix A. Running the MATLAB® program of Section1.10, we observe that the
inverse iteration method could converge to the reference eigenvalues for both shifts
σ = 2 and σ = 10. We also note that by applying the power method to this matrix,
we could get only NaN as a result.
Example 10.6 We tested the matrix of Example 10.2. Let us recall that the refer-
ence eigenvalues in this example are λ = (12.3246, −11.1644, −0.3246, 1.1644).
Running the MATLAB® program of Section1.10, we observe nice convergence.
For σ = 2 we were able to obtain the eigenvalue 1.1644, which is the same as the
last reference eigenvalue. This is because the shift σ = 2 is closer to this eigenvalue
than to all others. For the shift σ = 10, the algorithm converged to the ﬁrst reference
eigenvalue 12.3246, as expected.
This test conﬁrms that the inverse iteration method converges to the eigenvalue
that is closest to the shift σ.
Example 10.7 We tested the matrix of Example 10.3. Running the MATLAB® pro-
gram of Section1.10 allowed us to obtain nice convergence in this case, too, for both
shifts σ. Recall that the power method does not converge at all in Example 10.3.
7All MATLAB® programs referenced on this page can be found on the online version of Chapter 1
(doi:10.1007/978-3-319-57304-5_1).

10.2 Inverse Iteration
351
Example 10.8 WetestedthematrixofExample10.4.Again,runningtheMATLAB®
program of Section1.10, we observe nice convergence to the ﬁrst eigenvalue of the
matrix A for both shifts σ = 2, 10.8
10.3
Orthogonal Iteration
In this section we will consider the method of orthogonal iteration, which converges
to a p-dimensional invariant subspace (with p > 1) rather than to one eigenvector as
in the two previous methods. The method of orthogonal iteration is sometimes called
the method of subspace iteration or simultaneous iteration.
Let Q0 be an n × p orthogonal matrix with p ≤n. Our goal is to compute eigen-
values and eigenvectors of the square matrix A of order n. To do so, we perform the
following iterative algorithm.
Algorithm 10.3 Orthogonal iteration.
0. Set i = 0 and initialize a matrix Q0.
1. Compute Yi+1 = AQi.
2. Factorize Yi+1 using QR decomposition (see Section9.4) to obtain the matrices
Qi+1 and Ri+1. The matrix Qi+1 spans an approximate invariant subspace.
3. Compute Ti+1 = QT
i+1A Qi+1.
4. Compute the vector of approximate eigenvalues ˜λi+1 = (˜λ(i+1)
1
, ..., ˜λ(i+1)
p
) from
the real Schur block (see Theorem 4.27, p. 131) of the matrix Ti+1. The approxi-
mate eigenvectors will be the columns of Qi+1.
5. Stop updating the eigenvalues and set ˜λM = ˜λi+1, M = i + 1, if either the norm
∥˜λi+1 −˜λi∥≤θ or the differences ∥˜λi+1 −˜λi∥are stabilized or the subdiagonal
entries of Ti are small enough (smaller than the rounding errors of size O (ε∥Ti∥)).
Here θ is a tolerance number, and ε is machine epsilon.9 Otherwise, set i = i + 1
and go to step 1.
Theorem 10.3 Assume that A = SΛS−1 is diagonalizable, Λ = diag(λ1, λ2,
. . . , λn), the eigenvalues sorted so that |λ1| ≥|λ2| ≥· · · ≥|λn| and |λn| > 0 or
|λp| > |λp+1|, where p ∈[1, n −1] is an integer. Write S = (s1, s2, . . . , sn), where
the columns si are the corresponding eigenvectors, and they also satisfy ∥si∥= 1. Let
Qi+1Ri+1 be the QR decomposition of the matrix Yi+1 = AQi on step 2 and iteration
i in Algorithm 10.3. Then span(Qi) converges to span(Sp), the invariant subspace
spanned by the ﬁrst p eigenvectors, 1 ≤p ≤n.
Proof We assume that |λp| > |λp+1|. If we set p = 1, then the method of orthogonal
iteration and its analysis are identical to the power method.
8The
MATLAB®
programs
can
be
found
on
the
online
version
of
Chapter
1
(doi:10.1007/978-3-319-57304-5_1).
9The machine epsilon ε represents the upper bound on the relative error due to rounding in ﬂoating-
point arithmetic.

352
10
Algorithms for the Nonsymmetric Eigenvalue Problem
Consider now the case p > 1. Using step 1 of Algorithm 10.3, we can write
span(Qi+1) = span(Yi+1) = span(AQi). Thus, we conclude that the following
equalities hold: span(Qi) = span(AiQ0) = span(SΛiS−1Q0). We also note that
SΛiS−1Q0 = S diag(λi
1, . . . , λi
n)S−1Q0
= λi
pS
⎛
⎜⎜⎜⎜⎜⎜⎝
(λ1/λp)i
...
1
...
(λn/λp)i
⎞
⎟⎟⎟⎟⎟⎟⎠
S−1Q0.
(10.3)
By the assumption that |λp| > |λp+1|, we have |λj/λp| ≥1 for j ≤p and |λj/λp| < 1
if j > p. Then for the entries of the matrix Λ, we get
⎛
⎜⎝
(λ1/λp)i
...
(λn/λp)i
⎞
⎟⎠S−1Q0 =

V p×p
i
W (n−p)×p
i

= Xi,
where the elements of the submatrix Wi tend to zero like (λp+1/λp)i, and the elements
of the submatrix Vi do not converge to zero. This is true, since if V0 has full rank
(by assumption, we have λp ̸= 0), then the Vi have full rank, too. Now we write the
matrix of eigenvectors S = (s1, ..., sn) as

Sn×p
p
, ˆSn×(n−p)
p
	
or Sp = (s1, ..., sp). Then
we get
SΛiS−1Q0 = λi
pS

V p×p
i
W (n−p)×p
i

= λi
p

Sn×p
p
V p×p
i
+ ˆSn×(n−p)
p
W (n−p)×p
i
	
.
Thus,
span (Qi) = span

SΛiS−1Q0

= span

Sn×p
p
V p×p
i
+ ˆSn×(n−p)
p
W (n−p)×p
i
	
= span(SpXi)
(10.4)
converges to span

SpVi

= span

Sp

, the invariant subspace spanned by the ﬁrst p
eigenvectors, as stated in the theorem.
□
The next theorem states that under certain assumptions, by the method of orthog-
onal iteration we can compute eigenvalues of A from the Schur form of A.
Theorem 10.4 Let us consider Algorithm 10.3 applied to the matrix A with p = n
and Q0 = I. If all the eigenvalues of A have distinct absolute values and if all
the principal submatrices S(1 : j, 1 : j) are nonsingular, then the sequence of the
matrices Ti = QT
i AQi converges to the Schur form of A, i.e., an upper triangular

10.3 Orthogonal Iteration
353
matrix with the eigenvalues on the diagonal. The eigenvalues will appear in decreas-
ing order of absolute value.
Proof Using the assumption of the nonsingularity of S(1 : j, 1 : j) for all j, we have
that X0 in the proof of Theorem 10.3 is nonsingularity. This means that no vector
in the invariant subspace span{s1, . . . , sj} is orthogonal to span{i1, . . . , ij}, which is
the space spanned by the ﬁrst j columns of Q0 = I. First note that Qi is a square
orthogonal matrix, so A and Ti = QT
i AQi are similar. We can decompose the matrix
Qi into two submatrices as Qi = (Q1i, Q2i), where Q1i has j columns and Q2i has
n −j columns such that
Ti = QT
i AQi =
 QT
1iAQ1i QT
1iAQ2i
QT
2iAQ1i QT
2iAQ2i

.
(10.5)
Since span(Q1i) converges to an invariant subspace of A, span(AQ1i) converges to
the same subspace. Next, QT
2iAQ1i converges to QT
2iQ1i = 0. This is because we have
QT
i = (Q1i, Q2i)T and QT
i Qi = I, which means that
I = QT
i Qi = (Q1i, Q2i)T(Q1i, Q2i) =
 QT
1iQ1i QT
1iQ2i
QT
2iQ1i QT
2iQ2i

=
 I 0
0 I

.
Since QT
2iAQ1i converges to zero in (10.5) for all j < n, every subdiagonal entry
of Ti converges to zero, and thus Ti converges to upper triangular form, i.e., Schur
form. We see that the submatrix QT
2iAQ1i = Ti(j + 1 : n, 1 : j) should converge to
zero like |λj+1/λj|i. Thus, λj should appear as the (j, j) entry of Ti and converge like
max(|λj+1/λj|i, |λj/λj−1|i).
□
Remark 10.3
1. The use of the QR decomposition keeps the vectors spanning span(AiQ0) of full
rank despite rounding.
2. The method of orthogonal iteration is effectively running the algorithm for all
˜p = 1, 2, . . . , p at the same time. If all the eigenvalues have distinct absolute
values, the same convergence analysis as in Theorem 10.3 implies that the ﬁrst
˜p ≤p columns of Qi converge to span{s1, . . . , s˜p} for all ˜p ≤p.
3. If all assumptions of Theorem 10.4 hold, then we can set p = n and Q0 = I in
Algorithm 10.3 in order to obtain all eigenvalues and corresponding eigenvectors
of the matrix A.
WetesttheperformanceofthemethodoforthogonaliterationusingtheMATLAB®
program of Section1.11.10 In this program we compute the eigenvalues and corre-
sponding eigenvectors in six different cases which are described below.
Example 10.9 In this example, we tested the Hilbert matrix (3.46), p. 90, of order
10. Let us recall that the elements of this matrix are given by 1/(i + j −1), where
i, j = 1, 2, . . . , n. From Fig.10.1, Example 1, we observe that we have obtained all
10The
MATLAB®
programs
can
be
found
on
the
online
version
of
Chapter
1
(doi:10.1007/978-3-319-57304-5_1).

354
10
Algorithms for the Nonsymmetric Eigenvalue Problem
1
2
3
4
5
6
7
8
9
10
0
0.5
1
1.5
2
Real part of eigenvalues
Imag. part of eigenvalues
Example 1.  Nr. of it. in method of Orthogonal iteration:7
Exact eigenvalues
Computed eigenvalues
0
2
4
6
8
10
12
14
16
18
20
0
0.5
1
1.5
2
Real part of eigenvalues
Imag. part of eigenvalues
Example 2.  Nr. of it. in method of Orthogonal iteration:8
Exact eigenvalues
Computed eigenvalues
−1
−0.5
0
0.5
1
1.5
−10
−5
0
5
10
Real part of eigenvalues
Imag. part of eigenvalues
Example 3.  Nr. of it. in method of Orthogonal iteration:12
Exact eigenvalues
Computed eigenvalues
1
1.5
2
2.5
3
3.5
4
−15
−10
−5
0
5
10
15
Real part of eigenvalues
Imag. part of eigenvalues
Example 4.  Nr. of it. in method of Orthogonal iteration:15
Exact eigenvalues
Computed eigenvalues
−10
−5
0
5
10
15
20
−3
−2
−1
0
1
2
3
Real part of eigenvalues
Imag. part of eigenvalues
Example 5.  Nr. of it. in method of Orthogonal iteration:24
Exact eigenvalues
Computed eigenvalues
−1
0
1
2
3
4
5
6
−1
−0.5
0
0.5
1
Real part of eigenvalues
Imag. part of eigenvalues
Example 6.  Nr. of it. in method of Orthogonal iteration:101
Exact eigenvalues
Computed eigenvalues
Fig. 10.1 Performance of the method of orthogonal iteration
computed eigenvalues of this matrix, which coincide with the reference eigenvalues
from the seventh iteration.
Example 10.10 Here we tested the Hilbert matrix of order 20. Again we computed
almost exact eigenvalues of this matrix, this time at the eighth iteration; see Fig.10.1,
Example 2.
Example 10.11 ThisisthesameasExample10.3withthepowermethod.Figure10.1,
Example 3 shows the nice convergence to the one real and two complex eigenvalues
of the matrix A at the twelfth iteration.
Example 10.12 ThisisthesameasExample10.2withthepowermethod.Figure10.1,
Example 4 shows convergence to the four real eigenvalues of the matrix A at the ﬁf-
teenth iteration.
Example 10.13 Here we tested the matrix
A =
⎛
⎜⎜⎜⎜⎝
3 7
8
9
12
5 −7 4 −7 8
1 1 −1 1 −1
4 3
2
1
7
9 3
2
5
4
⎞
⎟⎟⎟⎟⎠
,
which has three real and two complex reference eigenvalues:

10.3 Orthogonal Iteration
355
λ = (19.9655, −8.2137 + 2.3623i, −8.2137 −2.3623i, −3.4043, −0.1337).
From Fig.10.1, Example 5 we observe the convergence of all the computed eigen-
values to the reference eigenvalues at the 24th iteration.
Example 10.14 Here we choose a square matrix of order 10 whose elements are uni-
formlydistributedpseudorandomnumbersintheopeninterval(0, 1).UsingFig.10.1,
Example 6 we observe the convergence of the computed eigenvalues to the reference
ones at the 101th iteration.
10.4
QR Iteration
Now we will consider an improvement of the method of orthogonal iteration, namely
the method of QR iteration. This method reorganizes the method of orthogonal
iteration and is more efﬁcient, since for the variant with shifts (see the next section)
it does not requires the assumption of distinct absolute eigenvalues of A, in contrast
to Theorem 10.4.
Let A0 be an n × n matrix, and our goal is to compute eigenvalues and eigenvectors
of this matrix. To do so, we perform the following iterative algorithm.
Algorithm 10.4 The method of QR iteration.
0. Set i = 0 and initialize a matrix A0.
1. Compute the QR decomposition of Ai such that Ai = QiRi.
2. Compute Ai+1 = RiQi.
3. Compute the vector of the approximate eigenvalues ˜λi+1 = (˜λ(i+1)
1
, ..., ˜λ(i+1)
p
)
from the real Schur block of the matrix Ai+1. The approximate eigenvectors will
be the columns of Qi.
4. Stop updating the eigenvalues and set ˜λM = ˜λi+1, M = i + 1, if either the norm
∥˜λi+1 −˜λi∥≤θ or the differences ∥˜λi+1 −˜λi∥are stabilized or the subdiago-
nal elements of Ai+1 are small enough (smaller than the rounding errors of size
O (ε∥Ai+1∥). Here θ is a tolerance number, and ε is the machine epsilon. Other-
wise, set i = i + 1 and go to step 1.
By step 2 of Algorithm 10.4, we have Ai+1 = RiQi. Using step 1, we can also write
that RiQi = QT
i (QiRi)Qi = QT
i AiQi. From both equalities, we see that the matrices
Ai+1 and Ai are orthogonally similar. The next theorem states that the matrix Ai
computed by QR iteration is identical to the matrix QT
i AQi implicitly computed by
the method of orthogonal iteration.
Theorem 10.5 Let Ai be the matrix computed by Algorithm 10.4. Then Ai = QT
i AQi,
where Qi is the matrix computed from the method of orthogonal iteration (Algorithm
10.3) starting with Q0 = I. Thus, Ai converges to the Schur form if all the eigenvalues
have different absolute values.

356
10
Algorithms for the Nonsymmetric Eigenvalue Problem
Proof We use induction. Assume Ai = QT
i AQi. Using step 2 of Algorithm 10.3, we
can write AQi = Qi+1Ri+1, where Qi+1 is orthogonal and Ri+1 is upper triangular.
Hence,
Ai = QT
i AQi = QT
i (Qi+1Ri+1) = (QT
i Qi+1)Ri+1 = QR.
This is the product of an orthogonal matrix Q = QT
i Qi+1 and an upper triangular
matrix R = Ri+1 = QT
i+1AQi (this is because AQi = Qi+1Ri+1, and thus multiplying
both sides of this equality by QT
i+1, we get R = Ri+1 = QT
i+1AQi). Since the QR
decomposition is unique (except for possibly multiplying each column of Q and row
of R by −1), this is the QR decomposition Ai = QR. Then
QT
i+1AQi+1 = QT
i+1(AQiQT
i )Qi+1 = (QT
i+1AQi)(QT
i Qi+1) = Ri+1(QT
i Qi+1) = RQ.
This is precisely how the QR iteration maps Ai to Ai+1, and therefore, Ai+1 =
QT
i+1AQi+1, as desired. Thus, the convergence of the method of QR iteration follows
from the convergence of the method of orthogonal iteration. If all the eigenvalues of
A have different absolute values, the proof is similar to the proof of Theorem 10.4
applied to the matrix Ai+1 = QT
i+1AQi+1.
□
Example 10.15 We test the performance of the method of QR iteration for six dif-
ferent matrices that are chosen the same as for the method of orthogonal iteration.
Running the MATLAB® program of Section1.12, we observe that in the method of
QR iteration we obtain the same rate of convergence as in the method of orthogonal
iteration.11
10.5
QR Iteration with Shifts
From previous sections we know that the convergence rate depends on the ratios of
eigenvalues. In order to speed convergence of the method of QR iteration, we can
use shifts. Let A0 be an n × n matrix, and our goal is to compute eigenvalues and
eigenvectors of this matrix. To do so, we perform the following iterative algorithm.
Algorithm 10.5 The method of QR iteration with shifts.
0. Set i = 0 and initialize a matrix A0. Choose an initial shift σ0.
1. Compute the QR decomposition of Ai −σiI such that Ai −σiI = QiRi.
2. Compute Ai+1 = RiQi + σiI.
3. Compute the vector of the approximate eigenvalues ˜λi+1 = (˜λ(i+1)
1
, ..., ˜λ(i+1)
p
)
from the real Schur block of the matrix Ai+1. The approximate eigenvectors will
be the columns of Qi.
4. Stop updating the eigenvalues and set ˜λM = ˜λi+1, M = i + 1, if either the norm
∥˜λi+1 −˜λi∥≤θ or the differences ∥˜λi+1 −˜λi∥have stabilized or the subdiag-
onal elements of Ai+1 are small enough (smaller than the rounding errors of
size O (ε∥Ai+1∥)). Here θ is a tolerance number, and ε is the machine epsilon.
Otherwise, set i = i + 1, choose a shift σi, and go to step 1.
11The
MATLAB®
programs
can
be
found
on
the
online
version
of
Chapter
1
(doi:10.1007/978-3-319-57304-5_1).

10.5 QR Iteration with Shifts
357
Lemma 10.1 The matrices Ai and Ai+1 in Algorithm 10.5 are orthogonally similar.
Proof From step 2 of Algorithm 10.5 we see that
Ai+1 = RiQi + σiI = QT
i QiRiQi + σiQT
i Qi = QT
i (QiRi + σiI)Qi.
Using step 1 of Algorithm 10.5, we observe that QT
i (QiRi + σiI)Qi = QT
i AiQi, and
thus Ai+1 = QT
i AiQi.
□
If Ri is nonsingular, we can also get
Ai+1 = RiQi + σiI = RiQiRiR−1
i
+ σiRiR−1
i
= Ri(QiRi + σiI)R−1
i
= RiAiR−1
i
.
Remark 10.4
1. If σi is an exact eigenvalue of Ai, then the method of QR iteration with the shift
σi converges in one step. This is because if σi is an eigenvalue, then Ai −σiI is
singular and Ri is singular, which means that some diagonal entry of Ri must
be zero. Assume that the (n, n)th element of the matrix Ri is zero, Ri(n, n) = 0.
Then the last row of RiQi is zero, and the last row of the matrix Ai+1 = RiQi + σiI
equals σiiT
n , where in is the nth column of Ai.
We can also say that the last row of Ai+1 is zero except for the eigenvalue σi
appearing in the (n, n)th entry. This means that Algorithm 10.5 has converged,
because we have obtained that Ai+1 is a block upper triangular matrix, with a
1 × 1 block σi: Ai+1 =
A′ a
0 σi

. In this matrix, the leading (n −1) × (n −1)
block A′ is a new matrix, where QR iteration can be used again without changing
σi.
2. When σi is not an exact eigenvalue, we will have convergence to the matrix
Ai+1(n, n) when the lower left block Ai+1(n, 1 : n −1) is small enough. Recall
that in the convergence of the method of inverse iteration (see Theorem 10.1), we
expect that Ai+1(n, 1 : n −1) will shrink by a factor |λk −σi|/ minj̸=k |λj −σi|,
where |λk −σi| = minj |λj −σi|. This means that if σi is a very good approxi-
mation to the eigenvalue λk, then we will have fast convergence.
Now we will concentrate on how to choose shifts σi in Algorithm 10.5 in order
to get accurate approximate eigenvalues. When we want to get good convergence to
the n-eigenvalue of the matrix A, then the choice σi = Ai(n, n) for a shift is a good
one. Such a choice of shift means local quadratic convergence to a real eigenvalue in
Algorithm 10.5. This means that the number of correct digits doubles at every step i
of Algorithm 10.5. However, it is difﬁcult to get global convergence with this shift,
and there exist examples for which the algorithm of QR iteration with this shift does
not converge [94].
Another choice of shift is the Francis12 shift, in which double shifts σ, ¯σ are
chosen as eigenvalues of the 2 × 2 corner of the matrix Ai:
12John G.F. Francis (born 1934) is an English computer scientist.

358
10
Algorithms for the Nonsymmetric Eigenvalue Problem
an−1,n−1 an−1,n
an,n−1
an,n

.
Such a choice of shifts allows convergence to either two real eigenvalues in the bottom
2 × 2 corner of the matrix Ai or the single 2 × 2 block with complex conjugate
eigenvalues. Such a choice leads to quadratic convergence asymptotically, which
means that if the values of an−1,n−2 are small enough, its amplitude will rapidly
decrease to zero. However, the method of QR iteration with Francis shift can often
fail to converge; see [8, 22].
There is another option for choosing a shift that is called Wilkinson’s13 shift: the
shift σi is chosen as an eigenvalue of the matrix
an−1,n−1 an−1,n
an,n−1
an,n

that is closest to the value an,n of the matrix Ai.
Theorem 10.6 The method of QR iteration with Wilkinson’s shift is globally and
at least linearly convergent. It is asymptotically cubically convergent for almost all
matrices.
A proof of this theorem can be found in [94].
Example 10.16 We test the performance of the method of QR iteration with shift for
the same matrices as in the method of orthogonal iteration. Running the MATLAB®
program of Section1.13, we observe good convergence to the reference eigenvalues
for the shift chosen as σ = Ann as well as for the Wilkinson’s shift.14
10.6
Hessenberg Reduction
All QR algorithms are computationally expensive: one iteration of the QR decom-
position costs O(n3) FLOPS. Assume that we can do only one iteration to ﬁnd one
eigenvalue. Then in this case, the cost will be O(n4). The goal of this section is to
present one more technique for reducing computations. It turns out that if we ﬁrst
reduce the original matrix A to upper Hessenberg form and then apply the method
of QR iteration without computing Q, we dramatically reduce computations, and
instead of O(n4) FLOPS, we perform our computations in O(n3) FLOPS.
A Hessenberg matrix is a special kind of square matrix, one that is “almost”
triangular. More precisely, an upper Hessenberg matrix has zero entries below the
ﬁrst subdiagonal, and a lower Hessenberg matrix has zero entries above the ﬁrst
superdiagonal. They are named after Karl Hessenberg.15 For example:
13James Hardy Wilkinson (1919–1986) was an English mathematician.
14The
MATLAB®
programs
can
be
found
on
the
online
version
of
Chapter
1
(doi:10.1007/978-3-319-57304-5_1).
15Karl Adolf Hessenberg (1904–1959) was a German mathematician.

10.6 Hessenberg Reduction
359
⎛
⎜⎜⎝
5 7 2 3
2 5 1 7
0 2 3 4
0 0 1 3
⎞
⎟⎟⎠
is upper Hessenberg and
⎛
⎜⎜⎝
1 2 0 0
5 2 3 0
3 4 3 7
5 6 1 1
⎞
⎟⎟⎠
is lower Hessenberg.
When the matrix A is upper Hessenberg, the setting to zero of the element ap+1,p
of this matrix will bring A into a block upper triangular matrix of the form
A =

Ap×p
11
Ap×(n−p)
12
0(n−p)×(p−1) A(n−p)×(n−p+1)
22

with the upper Hessenberg matrices A11 and A22. This decomposition of A means that
we can independently ﬁnd eigenvalues of A11 and A22. If in the process of Hessenberg
reduction any subdiagonal or superdiagonal entry of the matrix Ai is smaller than
the rounding errors of size O(ε∥A∥), then we set this value to zero. We stop our
computations when all these diagonal blocks are of size 1 × 1 or 2 × 2, and our
algorithm for ﬁnding the eigenvalues of A is ﬁnished.
Below we present an algorithm for the reduction of a matrix A of order n to an
upper Hessenberg matrix. Given a real matrix A, we seek an orthogonal matrix Q
such that the matrix QAQT is an upper Hessenberg matrix.
Algorithm 10.6 Reduction to an upper Hessenberg matrix.
0. Initialize the matrix Q = I and perform steps 1–7 in a loop from i = 1 to n −2.
1. Take the elements of the vector ui = A(i + 1 : n, i).
2. Obtain the ﬁrst element of the vector ui as ui(1) = ui(1) + sign(ui(1))∥ui∥.
3. Compute the elements of the vector ui = ui/∥ui∥.
4. Compute the elements of the matrix Pi = I(n−i)×(n−i) −2uiuT
i .
5. Compute the elements of the matrix A(i + 1 : n, i : n) = PiA(i + 1 : n, i : n).
6. Compute the elements of the matrix A(1 : n, i + 1 : n) = A(1 : n, i + 1 : n)Pi.
7. Compute
the
elements
of
the
matrix
Q
as
Q(i + 1 : n, i : n) = Pi
Q(i + 1 : n, i : n).
Proposition 10.1 Hessenberg form is preserved by QR iteration.
Proof If Ai is upper Hessenberg, then Ai −σI is also upper Hessenberg. Let us
consider Algorithm 10.5 and perform QR decomposition of the matrix Ai −σI.
Since the jth column of Q is a linear combination of the leading j columns of the
matrix Ai −σI, the QR decomposition yields an upper Hessenberg matrix Q. Then

360
10
Algorithms for the Nonsymmetric Eigenvalue Problem
1
2
3
4
5
6
7
8
9
10
0
0.5
1
1.5
2
Real part of eigenvalues
Imag. part of eigenvalues
Example 1.  Nr. of it. in method of QR iteration:2
Exact eigenvalues
Computed eigenvalues
0
2
4
6
8
10
12
14
16
18
20
0
0.5
1
1.5
2
Real part of eigenvalues
Imag. part of eigenvalues
Example 2.  Nr. of it. in method of QR iteration:2
Exact eigenvalues
Computed eigenvalues
−1
−0.5
0
0.5
1
1.5
−10
−5
0
5
10
Real part of eigenvalues
Imag. part of eigenvalues
Example 3.  Nr. of it. in method of QR iteration:2
Exact eigenvalues
Computed eigenvalues
1
1.5
2
2.5
3
3.5
4
−15
−10
−5
0
5
10
15
Real part of eigenvalues
Imag. part of eigenvalues
Example 4.  Nr. of it. in method of QR iteration:2
Exact eigenvalues
Computed eigenvalues
−10
−5
0
5
10
15
20
−3
−2
−1
0
1
2
3
Real part of eigenvalues
Imag. part of eigenvalues
Example 5.  Nr. of it. in method of QR iteration:2
Exact eigenvalues
Computed eigenvalues
−1
0
1
2
3
4
5
6
−0.5
0
0.5
Real part of eigenvalues
Imag. part of eigenvalues
Example 6.  Nr. of it. in method of QR iteration:2
Exact eigenvalues
Computed eigenvalues
Fig. 10.2 Performance of the method of QR iteration: ﬁrst we reduce the original matrix to upper
Hessenberg form and then we apply the method of QR iteration
RQ is also upper Hessenberg, as well as RQ + σI. This means that Hessenberg form
is preserved by QR iteration.
□
Clearly, the convergence analysis of Hessenberg reduction follows from the con-
vergence of the method of QR iteration.
Example 10.17 We test performance of the method of QR iteration via reducing ﬁrst
the original matrix to an upper Hessenberg matrix, using the MATLAB® program
of Section1.15.16 We again test the same matrices as in the method of orthogonal
iteration. In Fig.10.2 we observe the rapid convergence for all examples.
Now we illustrate the general pattern of Hessenberg reduction with a matrix A of
order 4. Every matrix Qi below is a Householder reﬂection matrix17 of order 4.
1. Choose Q1 such that
Q1A =
⎛
⎜⎜⎝
x x x x
x x x x
0 x x x
0 x x x
⎞
⎟⎟⎠such that A1 = Q1AQT
1 =
⎛
⎜⎜⎝
x x x x
x x x x
0 x x x
0 x x x
⎞
⎟⎟⎠.
16The
MATLAB®
programs
can
be
found
on
the
online
version
of
Chapter
1
(doi:10.1007/978-3-319-57304-5_1).
17See p. 161.

10.6 Hessenberg Reduction
361
The matrix Q1 leaves the ﬁrst row of Q1A unchanged, and QT
1 leaves the ﬁrst column
of Q1AQT
1 unchanged, including the zeros.
2. Choose Q2 such that
Q2A1 =
⎛
⎜⎜⎝
x x x x
x x x x
0 x x x
0 0 x x
⎞
⎟⎟⎠and obtain A2 = Q2A1QT
2 =
⎛
⎜⎜⎝
x x x x
x x x x
0 x x x
0 0 x x
⎞
⎟⎟⎠.
The matrix Q2 changes only the last two rows of A1, and QT
2 leaves the ﬁrst two
columns of Q2A1QT
2 unchanged, including the zeros. The matrix A2 is upper Hes-
senberg. Combining steps 1 and 2, we get A2 = (Q2Q1)A(Q2Q1)T = QAQT.
Let us consider an example of obtaining an upper Hessenberg matrix using House-
holder reﬂection.
Example 10.18 In this example we will use Householder reﬂection to get an upper
Hessenberg matrix from the matrix
A =
⎛
⎝
12 −51
4
6
167 −68
−4 24 −41
⎞
⎠.
To do so, we need zero out the value of entry (3, 1) of this matrix. First, we need
to ﬁnd the Householder reﬂection that transforms the ﬁrst column of the matrix A,
i.e., the vector x = (6, −4)T, to the form ∥x∥i1 = (

62 + (−4)2, 0)T = (2
√
13, 0)T.
Now,
u = x + αi1,
and
v =
u
∥u∥.
Here, α = −sign(x1)∥x∥= −2
√
13 and x = (6, −4)T. Therefore,
u = (6 −2
√
13, −4)T ≈(−1.21, −4)T
and v = u/∥u∥≈(−0.29, −0.96)T, and then
Q1 = I −2
−0.29
−0.96
 −0.29 −0.96
= I −
0.1682 0.5568
0.5568
1.84

=
 0.8318 −0.5568
−0.5568
−0.84

.
Now observe that Q1A preserves the ﬁrst row of the matrix A,

362
10
Algorithms for the Nonsymmetric Eigenvalue Problem
Q1A =
⎛
⎝
1
0
0
0 0.8318 −0.5568
0 −0.5568
−0.84
⎞
⎠
⎛
⎝
12 −51
4
6
167 −68
−4 24 −41
⎞
⎠
=
⎛
⎝
12
−51
4
7.2180 125.5474 −33.7336
0.0192 −113.1456 72.3024
⎞
⎠,
and the matrix Q1AQT
1 preserves the ﬁrst column of the matrix Q1A,
A1 = Q1AQT
1 =
⎛
⎝
12
−44.6490
25.0368
7.2180 123.2132 −41.5686
0.0192 −134.3725
2.2655
⎞
⎠,
which is an upper Hessenberg matrix.
10.7
Tridiagonal and Bidiagonal Reduction
Suppose now that the matrix A is symmetric. Then Hessenberg reduction leaves the
matrix A symmetric at every step in such a way that zero elements will be created in
symmetric positions. This will reduce the number of operations to (4/3)n3 + O(n2)
or (8/3)n3 + O(n2) to form matrices Qn−1, . . . , Q1 (see [23]). This procedure is
called tridiagonal reduction. We recall that the eigenvalues of the symmetric matrix
ATA are the squares of the singular values of A.18 The SVD algorithms that we
consider in Section11.6 use this fact, and the goal of this section is to ﬁnd a form for
A that implies that ATA is tridiagonal. Our goal is to compute orthogonal matrices Q
and V such that QAV is an upper bidiagonal matrix, or nonzero only on the diagonal
and the ﬁrst superdiagonal. This algorithm is called bidiagonal reduction.
Below we present the general procedure of bidiagonal reduction, which is illus-
trated on a matrix A of size 5 × 5.
1. Choose Q1 and V1 such that
Q1A =
⎛
⎜⎜⎜⎜⎝
x x x x x
0 x x x x
0 x x x x
0 x x x x
0 x x x x
⎞
⎟⎟⎟⎟⎠
and A1 = Q1AV1 =
⎛
⎜⎜⎜⎜⎝
x x 0 0 0
0 x x x x
0 x x x x
0 x x x x
0 x x x x
⎞
⎟⎟⎟⎟⎠
.
Here Q1 is the matrix obtained after Householder reﬂection, and V1 is another matrix
obtained after Householder reﬂection that leaves the ﬁrst column of Q1A unchanged.
18See Section5.1.1, p. 163.

10.7 Tridiagonal and Bidiagonal Reduction
363
2. Choose Q2 and V2 such that
Q2A1 =
⎛
⎜⎜⎜⎜⎝
x x 0 0 0
0 x x x x
0 0 x x x
0 0 x x x
0 0 x x x
⎞
⎟⎟⎟⎟⎠
and A2 = Q2A1V2 =
⎛
⎜⎜⎜⎜⎝
x x 0 0 0
0 x x 0 0
0 0 x x x
0 0 x x x
0 0 x x x
⎞
⎟⎟⎟⎟⎠
.
Here Q2 is the matrix obtained after Householder reﬂection that leaves the ﬁrst row
of A1 unchanged. The matrix V2 is a Householder reﬂection that leaves the ﬁrst two
columns of Q2A1 unchanged.
3. Choose Q3 and V3 such that
Q3A2 =
⎛
⎜⎜⎜⎜⎝
x x 0 0 0
0 x x 0 0
0 0 x x x
0 0 0 x x
0 0 0 x x
⎞
⎟⎟⎟⎟⎠
andA3 = Q3A2V3 =
⎛
⎜⎜⎜⎜⎝
x x 0 0 0
0 x x 0 0
0 0 x x 0
0 0 0 x x
0 0 0 x x
⎞
⎟⎟⎟⎟⎠
.
Here Q3 is a Householder reﬂection that leaves the ﬁrst two rows of A2 unchanged.
The matrix V3 is a Householder reﬂection that leaves the ﬁrst three columns of Q3A2
unchanged.
4. Choose Q4 such that
Q4A3 =
⎛
⎜⎜⎜⎜⎝
x x 0 0 0
0 x x 0 0
0 0 x x 0
0 0 0 x x
0 0 0 0 x
⎞
⎟⎟⎟⎟⎠
and V4 = I so A4 = Q4A3.
Here Q4 is a Householder reﬂection that leaves the ﬁrst three rows of A3 unchanged.
Then we obtain the tridiagonal matrix as
AT
4 A4 =
⎛
⎜⎜⎜⎜⎝
x x 0 0 0
x x x 0 0
0 x x x 0
0 0 x x x
0 0 0 x x
⎞
⎟⎟⎟⎟⎠
.
In the general case, the matrix A has order n. Then applying the above procedure to
this matrix, we get orthogonal matrices Q = Qn−1 · · · Q1 and V = V1 · · · Vn−2 such
that QAV = A′ is upper bidiagonal. Note that A′TA′ = V TATQTQAV = V TATAV ,
so A′TA′ has the same eigenvalues as ATA; i.e., A′ has the same singular values as A.
The cost of the algorithm of bidiagonal reduction is (8/3)n3 + 4n3 + O(n2), where
4n3 + O(n2) counts for computations of Q and V .

364
10
Algorithms for the Nonsymmetric Eigenvalue Problem
Example 10.19 In this example we apply the above procedure to the bidiagonal
reduction of the matrix
A =
⎛
⎝
4 4 3
3 6 1
0 1 7
⎞
⎠
using Householder transformation. We proceed in following steps.
First, we need to zero out the second entry in the ﬁrst column of the matrix A, the
vector x = (4, 3, 0)T. We compute ﬁrst α = −sign(x1)∥x∥= −5, and then the vec-
tors u = x + αi1 = (−1, 3, 0)T and v = u/∥u∥= (−1, 3, 0)T/
√
10. Compute the
Householder matrix P1 as
P1 = I −2vvT =
⎛
⎝
0.8 0.6 0
0.6 −0.8 0
0
0
1
⎞
⎠.
Compute P1A to zero out the two entries below 5 in the ﬁrst column:
P1A =
⎛
⎝
5 6.8 3
0 −2.4 1
0
1
7
⎞
⎠.
(10.6)
Now we want to zero out the (1, 3) entry of the matrix (10.6). To do so, we take
the minor
M =
 6.8 3
−2.4 1

and compute again for x = (6.8, −2.4)T the number α = −sign(x1)∥x∥= −7.4324
and then the vectors u = x + αi1 = (−0.6324, 3)T, v = u/∥u∥= (−0.2063,
0.9785)T. Compute the matrix
V ′
1 = I −2vvT =
0.9149 0.4037
0.4037 −0.9149

.
Construct V1 such that
V1 =
⎛
⎝
1 0 0
0 V ′
1
0
⎞
⎠=
⎛
⎝
1
0
0
0 0.9149 0.4037
0 0.4037 −0.9149
⎞
⎠.
Compute P1AV1 to zero out the (1, 3) entry:
P1AV1 =
⎛
⎝
5 7.4324
0.0005
0 −1.7921 −1.8838
0 3.7408 −6.0006
⎞
⎠.
(10.7)

10.7 Tridiagonal and Bidiagonal Reduction
365
It remains only to zero out the (3, 2) entry of the matrix in (10.7). We take the
minor
M =
 −1.7921 −1.8838
3.7408 −6.0006

and compute for x = (−1.7921, 3.7408)T the number α = −sign(x1)∥x∥= 4.1479
and the vectors u = x + αi1 = (2.3558, 3.7408)T, v = u/∥u∥= (0.5329, 0.8462)T.
Compute the 2 × 2 matrix P′
2:
P′
2 = I −2vvT =
 0.4320 −0.9019
−0.9019 −0.4321

.
Construct P2 such that the matrix P′
2 is inserted into the 3 × 3 identity matrix:
P2 =
⎛
⎝
1 0 0
0 P′
2
0
⎞
⎠=
⎛
⎝
1.0000
0
0
0
0.4320 −0.9019
0
−0.9019 −0.4321
⎞
⎠.
Finally, multiply the matrix P2 by the matrix P1AV1 obtained in (10.7) to get the
bidiagonal matrix
P2P1AV1 =
⎛
⎝
5.0000
7.4324 0.0005
−0.0000 −4.1480 4.5981
0.0000 −0.0001 4.2918
⎞
⎠.
(10.8)
10.7.1
Tridiagonal Reduction Using Householder
Transformation
In this section we present an alternative procedure that can be used for tridiagonal
reduction using Householder transformation. This procedure is taken from [12]. To
form the Householder matrix in this procedure, in each step we need to determine α
and r, which are given by
α = −sign(a21)




n

j=2
a2
j1,
r =

1
2(α2 −a21α).
From α and r, we construct the vector v:

366
10
Algorithms for the Nonsymmetric Eigenvalue Problem
v(1) =
⎛
⎜⎜⎜⎝
v1
v2
...
vn
⎞
⎟⎟⎟⎠,
where v1 = 0, v2 = (a21 −α)/(2r), and
vk = ak1
2r
for k = 3, 4, . . . n. Then we compute the matrix
P1 = I −2v(1)(v(1))T
and obtain the matrix A(1) as
A(1) = P1AP1.
Having found P1 and computed A(1), we repeat the process for k = 2, 3, ..., n as
follows:
α = −sign(ak+1,k)




n

j=k+1
a2
jk,
r =

1
2(α2 −ak+1,kα),
vk
1 = vk
2 = .. = vk
k = 0, vk
k+1 =
ak
k+1,k −α
2r
, vk
j =
ak
jk
2r for j = k + 2, k + 3, ..., n,
Pk = I −2v(k)(v(k))T,
A(k+1) = PkA(k)Pk.
Example 10.20 In this example we apply the above algorithm to perform tridiagonal
reduction of the matrix
A =
⎛
⎝
5 4 3
4 6 1
3 1 7
⎞
⎠,
(10.9)
using Householder transformation. To do so, we proceed in the following steps.
First we compute α as
α = −sign(a21)




n

j=2
a2
j1 = −

(a2
21 + a2
31) = −

42 + 32 = −5.
Using α, we ﬁnd r as
r =

1
2(α2 −a21α) =

1
2((−5)2 −4 · (−5)) = 3
√
5
√
2
.

10.7 Tridiagonal and Bidiagonal Reduction
367
Then we compute the components of the vector v:
v1 = 0,
v2 = a21 −α
2r
= 3
√
2
2
√
5
,
v3 = a31
2r =
√
2
2
√
5
,
and we get
v(1) =

0, 3
√
2
2
√
5
,
√
2
2
√
5
T
.
Now we compute the ﬁrst Householder matrix P1 = I −2v(1)(v(1))T to get
P1 =
⎛
⎝
1
0
0
0 −4/5 −3/5
0 −3/5 4/5
⎞
⎠.
Finally, we obtain the tridiagonal matrix A(1) as
A(1) = P1AP1 =
⎛
⎝
5
−5
0
−5 7.32 −0.76
0 −0.76 5.68
⎞
⎠.
10.7.2
Tridiagonal Reduction Using Givens Rotation
To make the tridiagonal matrix from the matrix A using Givens rotation,19 we ﬁrst
recall that a Givens rotation is represented by a matrix of the form
G(i, j, θ) =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1 · · · 0 · · · 0 · · · 0
... ... ...
...
...
0 · · · c · · · −s · · · 0
...
... ...
...
...
0 · · · s · · · c · · · 0
...
...
...
... ...
0 · · · 0 · · · 0 · · · 1
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
19See p. 160.

368
10
Algorithms for the Nonsymmetric Eigenvalue Problem
where c = cos(Θ) and s = sin(Θ) appear at the intersections of the ith and jth rows
and columns. The elements in the Givens matrix are
gk k = 1
for k ̸= i, j,
gi i = c,
gj j = c,
gj i = −s,
gi j = s
for i > j.
We note that the sign of elements gj i, gj i switches for j > i. For given a and b, our
goal is to ﬁnd c = cos θ and s = sin θ such that
c −s
s c
 a
b

=
r
0

,
where
r =

a2 + b2,
c = a/r,
s = −b/r.
Example 10.21 To obtain a tridiagonal matrix from the matrix (10.9) using Givens
rotation we have to zero out the (3, 1) and (1, 3) elements of the matrix A.
Thus, we use the above expressions to construct the Givens rotation matrix G1 of
the form
G1 =
⎛
⎝
1 0 0
0 c −s
0 s c
⎞
⎠.
We compute then the product G1A to get
G1A =
⎛
⎝
1 0 0
0 c −s
0 s c
⎞
⎠
⎛
⎝
5 4 3
4 6 1
3 1 7
⎞
⎠=
⎛
⎝
5
4
3
4c −3s 6c −s c −7s
4s + 3c 6s + c s + 7c
⎞
⎠.
The element (3, 1) of the matrix will be zero if 4s + 3c = 0. This is true when
c = 4/5 and s = −3/5. To compute c, s we have used the formulas
r =

a2 + b2 =

42 + 32 = 5,
c = a
r = 4
5,
s = −b
r = −3
5.
Next, to get a tridiagonal matrix, we have to compute G1AG1T:

10.7 Tridiagonal and Bidiagonal Reduction
369
A1=G1AG1
T =
⎛
⎝
1
0
0
0 4/5 3/5
0 −3/5 4/5
⎞
⎠
⎛
⎝
5 4 3
4 6 1
3 1 7
⎞
⎠
⎛
⎝
1 0
0
0 4/5 −3/5
0 3/5 4/5
⎞
⎠=
⎛
⎝
5
5
0
5 7.32 0.76
0 0.76 5.68
⎞
⎠.
(10.10)
Example 10.22 As another example, let us now make an upper triangular matrix
from the matrix (10.10), using Givens rotation. To do so, we need to zero out the
elements (2, 1) and (3, 2) of (10.10). To zero out the element (2, 1), we compute the
numbers c and s from the known a = 5 and b = 5 as
c −s
s c
  a
b

=
 r
0

to get
r =

a2 + b2 =

52 + 52 = 5
√
2 ≈7.0711,
c = a
r ≈0.7071,
s = −b
r
≈−0.7071.
The Givens matrix will be
G2 =
⎛
⎝
c −s 0
s c 0
0 0 1
⎞
⎠=
⎛
⎝
0.7071 0.7071 0
−0.7071 0.7071 0
0
0
1
⎞
⎠.
Finally, we obtain the matrix
A2 = G2A1 =
⎛
⎝
0.7071 0.7071 0
−0.7071 0.7071 0
0
0
1
⎞
⎠
⎛
⎝
5
5
0
5 7.32 0.76
0 0.76 5.68
⎞
⎠
=
⎛
⎝
7.7071 8.7116 0.5374
0
1.6405 0.5374
0
0.7600 5.6800
⎞
⎠. (10.11)
Now to zero out the element (3, 2), we compute c, s from the known a = 1.6405 and
b = 0.76 to get
r =

a2 + b2 =

1.64052 + 0.762 = 1.8080,
c = a
r ≈0.9074,
s = −b
r
≈−0.4204.
The last Givens matrix will be

370
10
Algorithms for the Nonsymmetric Eigenvalue Problem
G3 =
⎛
⎝
1 0 0
0 c −s
0 s c
⎞
⎠=
⎛
⎝
1
0
0
0 0.9074 0.4204
0 −0.4204 0.9074
⎞
⎠.
Finally, we obtain the upper triangular matrix
A3 = G3A2 =
⎛
⎝
7.0711 8.7116 0.5374
0
1.8080 2.8752
0
0.0000 4.9279
⎞
⎠.
(10.12)
10.8
QR Iteration with Implicit Shifts
In this section we will ﬁrst reduce the matrix A to an upper Hessenberg matrix and
then compute its QR factorization implicitly. This means that QR factorization will
be computed by construction of the matrix Q using the implicit Q theorem. This
theorem improves the efﬁciency of Hessenberg’s QR iteration algorithm. Next, we
will present how to choose a single shift to accelerate convergence of the method of
QR iteration.
We say that an upper Hessenberg matrix H is unreduced if all elements on its
subdiagonal are nonzero.
Theorem 10.7 Let H and G be unreduced upper Hessenberg matrices of order n
such that H = QTAQ and G = V TAV . Here, Q and V are orthogonal matrices of
order n, where the ﬁrst columns are the same, or Qi1 = V i1 with i1 = (1, 0, ..., 0).
Let X(:, i) denote the ith column of the matrix X. Then the columns of Q and V are
the same up to the sign, or Q(:, i) = ±V (:, i) for i = 2, 3, ..., n.
Proof The assertion regarding the ﬁrst columns of the matrices Q and V is obvious.
Our goal is to prove that Q(:, i) = ±V (:, i) for i > 1. This is equivalent to proving
that the matrix W satisﬁes W = V TQ = diag(±1, ..., ±1). Since by assumption,
W = V TQ, we can write GW = GV TQ = V TAV V TQ = V TAQ = V TQQTAQ =
V TQH = WH.
Since
GW = WH,
we
have
GW(:, i) = (GW)
(:, i) = (WH)(:, i) = i+1
j=1 HjiW(:, j), and thus, Hi+1,iW(:, i + 1) = GW(:, i) −
i
j=1 HjiW(:, j). The ﬁrst column of W is W(:, 1) = (1, 0, ..., 0)T (this is because Q
and V are orthogonal and Q(:, 1) = V (:, 1)), and G is an upper Hessenberg matrix.
We can use induction on the index of column i to show that Wi is nonzero only for
entries from 1 to i. Thus, W is an upper triangular matrix. But because W is also an
orthogonal matrix, it must be diagonal, or W = diag(±1, ..., ±1).
□
Algorithm 10.7 The single shift QR algorithm.
Theorem 10.7 implies that to compute Ai+1 = QT
i AiQi from Ai in the QR algo-
rithm, we will need to perform only the following two steps.

10.8 QR Iteration with Implicit Shifts
371
1. Compute the ﬁrst column of the matrix Qi. This column is parallel to the ﬁrst
column of Ai −σiI and thus can be obtained just by normalizing this column
vector.
2. Choose other columns of Qi such that Qi is an orthogonal matrix and Ai+1 =
QT
i AiQi is an unreduced upper Hessenberg matrix.
Using the above theorem, we can conclude that the matrix Ai+1 in the algorithm
is computed correctly because the matrix Qi is computed uniquely up to signs. The
choice of sign does not matter. This is true because if we change signs in the columns
of Qi, then the signs in Ai −σiI = QiRi also will be changed: Ai −σiI = QiRi =
QiDiDiRi, where Di = diag(±1, ..., ±1). Then we can write
Ai+1 = QT
i AiQi = QT
i (QiRi + σiI)Qi = QT
i (QiDiDiRi + σiI)Qi
= DiRiQiDi + σiI = Di(RiQi + σiI)Di,
(10.13)
and this is an orthogonal similarity that changes only the signs in the columns and
rows of Ai+1.
As an example, we will now illustrate how the above algorithm works for the
computation of Ai+1 = QT
i AiQi for i = 4. In all matrices QT
i below, the values ci
and si can be computed using the Givens rotation algorithm. The symbols ∗in the
matrices Ai should be removed during the iterations on index i in order to restore
Hessenberg form.
1. Choose Q1 such that
QT
1 =
⎛
⎜⎜⎜⎜⎝
c1 s1 0 0 0
−s1 c1 0 0 0
0
0 1 0 0
0
0 0 1 0
0
0 0 0 1
⎞
⎟⎟⎟⎟⎠
to get A1 = QT
1 A0Q1 =
⎛
⎜⎜⎜⎜⎝
x x x x x
x x x x x
∗x x x x
0 0 x x x
0 0 0 x x
⎞
⎟⎟⎟⎟⎠
.
2. Choose Q2 such that
QT
2 =
⎛
⎜⎜⎜⎜⎝
1
0
0 0 0
0 c2 s2 0 0
0 −s2 c2 0 0
0
0
0 1 0
0
0
0 0 1
⎞
⎟⎟⎟⎟⎠
to get A2 = QT
2 A1Q2 =
⎛
⎜⎜⎜⎜⎝
x x x x x
x x x x x
0 x x x x
0 ∗x x x
0 0 0 x x
⎞
⎟⎟⎟⎟⎠
.
We note that
QT
2 A1 =
⎛
⎜⎜⎜⎜⎝
x x x x x
x x x x x
0 x x x x
0 0 x x x
0 0 0 x x
⎞
⎟⎟⎟⎟⎠
.

372
10
Algorithms for the Nonsymmetric Eigenvalue Problem
3. Choose Q3 such that
QT
3 =
⎛
⎜⎜⎜⎜⎝
1 0
0
0 0
0 1
0
0 0
0 0 c3 s3 0
0 0 −s3 c3 0
0 0
0
0 1
⎞
⎟⎟⎟⎟⎠
to get A3 = QT
3 A2Q3 =
⎛
⎜⎜⎜⎜⎝
x x x x x
x x x x x
0 x x x x
0 0 x x x
0 0 ∗x x
⎞
⎟⎟⎟⎟⎠
.
We note that
QT
3 A2 =
⎛
⎜⎜⎜⎜⎝
x x x x x
x x x x x
0 x x x x
0 0 x x x
0 0 0 x x
⎞
⎟⎟⎟⎟⎠
.
4. Choose Q4 such that
QT
4 =
⎛
⎜⎜⎜⎜⎝
1 0 0
0
0
0 1 0
0
0
0 0 1
0
0
0 0 0 c4 s4
0 0 0 −s4 c4
⎞
⎟⎟⎟⎟⎠
to get A4 = QT
4 A3Q4 =
⎛
⎜⎜⎜⎜⎝
x x x x x
x x x x x
0 x x x x
0 0 x x x
0 0 0 x x
⎞
⎟⎟⎟⎟⎠
.
We note that
QT
4 A3 =
⎛
⎜⎜⎜⎜⎝
x x x x x
x x x x x
0 x x x x
0 0 x x x
0 0 0 x x
⎞
⎟⎟⎟⎟⎠
.
At step 4 we have obtained the upper Hessenberg matrix A4. We observe that com-
bining steps 1–4, we get A4 = QT
4 QT
3 QT
2 QT
1 AQ1Q2Q3Q4 = QTAQ, where the matrix
Q = Q1Q2Q3Q4 is such that
Q =
⎛
⎜⎜⎜⎜⎝
c1 x x x x
s1 x x x x
0 s2 x x x
0 0 s3 x x
0 0 0 s4 x
⎞
⎟⎟⎟⎟⎠
.
If we now choose the ﬁrst column of Q, which is Q(:, 1) = (c1, s1, 0, 0, 0), propor-
tional to the ﬁrst column of the matrix A −σI, which is
(A −σI)(:, 1) = (a11 −σ, a21, 0, 0, 0)T,
then this matrix Q will be the same as in the QR decomposition of A −σI.

10.8 QR Iteration with Implicit Shifts
373
We can choose the single shift σ as σ = an,n for the matrix Ai. This will result in
asymptotic quadratic convergence to a real eigenvalue; see [23] for details on how
to choose shifts.
Questions
Do the following exercises, explaining the results that you have obtained.
10.1 (Programming)
Use the MATLAB® program PowerM.m of Section1.9 to test the power method
and to compute the largest eigenvalue of the matrix A.20 Try the following examples
when the matrix A and the tolerance Θ in Algorithm 10.1, p. 345, are deﬁned as
follows:
1. A = randn(5) and tolerance Θ = {1e −5, 1e −4, 1e −3, 1e −2, 0.1}.
2. A = diag(ones(2n, 1)) + diag(ones(2n −1, 1), 1) + diag(ones(2n −1, 1), −1)
for each number n = 3, 4, 5 and tolerance Θ={1e−12, 1e −10, 1e −8, 1e −7}.
3.
A =
⎛
⎜⎜⎝
1 1e6 0
0
0 2
1
0
1 2
3 10
0 0 −1 4
⎞
⎟⎟⎠
(10.14)
and tolerance Θ = {1e −12, 1e −10, 1e −8}.
4.
A =
⎛
⎜⎜⎜⎜⎜⎜⎝
1 0 0 0 0
0
0 2 1 0 0
0
0 0 2 0 0
0
0 0 0 3 1e2 1e4
0 0 0 0 3 1e2
0 0 0 0 0
3
⎞
⎟⎟⎟⎟⎟⎟⎠
(10.15)
and tolerance Θ = {1e −10, 1e −8, 1e −6, 1e −4, 1e −3}.
10.2 (Programming)
Use the MATLAB® program InverseIteration.m of Section1.10 to test
the inverse iteration method for the computation of the eigenvalue of the matrix A that
is closest to the shift σ. Try all examples of matrix A and tolerance Θ as in Question
10.1. Choose also different shifts σ. For a stopping criterion using tolerance Θ, we
refer to Algorithm 10.2, p. 348.
10.3 (Programming)
Use the MATLAB® program MethodOrtIter.m of Section1.11 to test
the method of orthogonal iteration for the computation of the eigenvalues
20All MATLAB® programs referenced on this page can be found on the online version of Chapter 1
(doi:10.1007/978-3-319-57304-5_1).

374
10
Algorithms for the Nonsymmetric Eigenvalue Problem
of the matrix A.21 Try all examples of matrix A and tolerance Θ deﬁned in Question
10.1. For a stopping criterion using tolerance Θ, we refer to Algorithm 10.3, p. 351.
10.4 (Programming)
Use the MATLAB® program MethodQR_iter.m of Section1.12 to test the
method of QR iteration for the computation of the eigenvalues of the matrix A. Try
all examples of matrix A and tolerance Θ deﬁned in Question 10.1. For a stopping
criterion using tolerance Θ, we refer to Algorithm 10.4, p. 355.
10.5 (Programming)
Use the MATLAB® program MethodQR_shift.m of Section1.13 to test the
method of QR iteration with the shift σ = A(n, n) for the computation of the eigen-
values of the matrix A. Try all examples of matrix A and tolerance Θ deﬁned in
Question 10.1. For a stopping criterion using tolerance Θ, we refer to Algorithm
10.5, p. 356.
10.6 (Programming)
Use the MATLAB® program MethodQR_Wshift.m of Section1.14 to test the
method of QR iteration with Wilkinson’s shift for the computation of the eigenvalues
of the matrix A. Try all examples of matrix A and tolerance Θ deﬁned in Question
10.1. For a stopping criterion using tolerance Θ, we refer to Algorithm 10.5, p. 356.
10.7 (Programming)
Use the MATLAB® program HessenbergQR.m of Section1.15 to test the
reduction of the matrix A to an upper Hessenberg matrix. Try the following examples
when the matrix A and the tolerance Θ are deﬁned thus:
1. A = randn(5) and the tolerance is Θ = {1e −7, 1e −5, 1e −4, 1e −3, 1e −
2, 0.1}.
2. A = diag(ones(2n, 1)) + diag(ones(2n −1, 1), 1) + diag(ones(2n −1, 1), −1)
for
each
number
n = 3, 4, 5
and
tolerance
Θ = {1e −12, 1e −10,
1e −8, 1e −7}.
3.
A =
⎛
⎜⎜⎝
1 1e6
0
0
0 2 1e −3 0
0 0
3
10
0 0
−1
4
⎞
⎟⎟⎠
(10.16)
and tolerance Θ = {1e −12, 1e −10, 1e −8}.
4. Each tolerance from the set Θ = {1e −10, 1e −8, 1e −6, 1e −4, 1e −3} and
the matrix A = (1, 0, 0, 0, 0, 0; 0, 2, 1, 0, 0, 0; 0, 0, 2, 0, 0, 0; 0, 0, 0, 1e4, 1, 1;
0, 0, 0, 0, 1e2, 1; 0, 0, 0, 0, 0, 1e4) + diag(ones(5, 1), −1).
21All MATLAB® programs referenced on this page can be found on the online version of Chapter 1
(doi:10.1007/978-3-319-57304-5_1).

Chapter 11
Algorithms for Solution of Symmetric
Eigenvalue Problems
In this chapter we will discuss algorithms which can solve only symmetric eigenvalue
problems using direct noniterative methods. Recall that in the previous chapter, the
algorithms that can ﬁnd all eigenvalues and eigenvectors for an nonsymmetric eigen-
value problem were based only on the method of QR iteration. However, there exist
many algorithms for the solution of symmetric eigenvalue problems that are more
efﬁcient than the algorithms for the solution of nonsymmetric eigenvalue problems.
We list here the main algorithms and main advantages of every algorithm that we
will consider in this chapter:
1. Tridiagonal QR iteration. This algorithm can be used to ﬁnd all the eigenvalues,
and if needed, all the eigenvectors, of a symmetric tridiagonal matrix. This method
isthefastestnumericalmethodthatcancomputealltheeigenvaluesofasymmetric
tridiagonal matrix. If we want to apply this algorithm to ﬁnd all the eigenvectors
as well, then it will be efﬁcient only for small matrices of dimension up to 25.
We note that this algorithm is used in the Matlab® command eig.
2. Rayleigh quotient iteration. This algorithm is similar to the algorithm of QR
iteration, and we present it here to analyze its extremely rapid cubic convergence.
3. Divide and conquer. This is the fastest method for ﬁnding all the eigenvalues
and eigenvectors of symmetric tridiagonal matrices that have dimensions larger
than 25.
4. Bisection and inverse iteration. Bisection may be used to ﬁnd a subset of the
eigenvalues of a symmetric tridiagonal matrix on some subinterval of the interval
on which the eigenvalues are located. The algorithm of Inverse iteration described
in the previous chapter can then be used to ﬁnd the corresponding eigenvectors.
In [29, 42, 90–92, 96, 97, 115], the inverse iteration method is developed further
to ﬁnd close eigenvalues and eigenvectors as fast as possible.
© Springer International Publishing AG 2017
L. Beilina et al., Numerical Linear Algebra: Theory and Applications,
DOI 10.1007/978-3-319-57304-5_11
375

376
11
Algorithms for Solution of Symmetric Eigenvalue Problems
5. Jacobi’s method. This is the ﬁrst known method for the solution of eigenvalue
problems. It was developed in 1846 by Jacobi. Compared with all previous meth-
ods, Jacobi’s method is much slower. However, it is very accurate and can ﬁnd tiny
eigenvalues much more precisely than the previous methods; see details in [25].
11.1
Tridiagonal QR Iteration
The tridiagonal QR iteration algorithm is very similar to the usual algorithm of QR
iteration for the solution of nonsymmetric eigenproblems that we considered in the
previous chapter. That algorithm consists of two stages:
• First we constructed an orthogonal matrix Q via Algorithm 10.6 (reduction to an
upper Hessenberg matrix) such that Q AQT = H is an upper Hessenberg matrix.
• Then we applied QR iteration to the resulting matrix H and obtained a sequence
of upper Hessenberg matrices H0, H1, H2, ..., that converged to real Schur form.
The algorithm of QR iteration for a symmetric tridiagonal matrix is very similar
to the procedure described above and consists of the following steps:
• Use a modiﬁed Algorithm10.6 (reduction to upper Hessenberg form) to ﬁnd an
orthogonal Q such that Q AQT = T is tridiagonal.
• Apply the algorithm of QR iteration on the resulting matrix T to obtain a sequence
of tridiagonal matrices T0, T1, T2, . . . that will converge to diagonal form.
We note that the algorithm of QR iteration keeps all matrices Ti tridiagonal. This
is because the matrix Q AQT = T is symmetric and upper Hessenberg, and thus also
lower Hessenberg, or tridiagonal.
We now describe how to choose the shifts at every QR iteration. Let us denote by
Ti the tridiagonal matrix obtained at iteration i in the algorithm of QR iteration:
Ti =
⎛
⎜⎜⎜⎜⎝
a1 b1
b1
... ...
... ... bn−1
bn−1 an
⎞
⎟⎟⎟⎟⎠
.
We can choose as a shift the single shift σi = an in the algorithm of QR iteration;
see Section10.8. Then the method is cubically convergent for almost all matrices.
However, in some special cases, the method of QR iteration does not converge; see
p. 76 in [94]. Thus, to get global convergence of the method, one needs to compute a
shift in a more complicated manner. Let the shift σi, called Wilkinson’s shift, be the
eigenvalue of
an−1 bn−1
bn−1 an
	
closest to an.

11.1 Tridiagonal QR Iteration
377
Theorem 11.1 (Wilkinson).
The algorithm of QR iteration with Wilkinson’s shift is globally and at least
linearly convergent. This algorithm is asymptotically cubically convergent for almost
all matrices.
We refer to [94] for a proof of this theorem. Efﬁcient implementation of this
algorithm is studied in [43, 95].
11.2
Rayleigh Quotient Iteration
The Rayleigh1 quotient of a symmetric matrix A and a nonzero real vector u is
ρ(u, A) ≡uT Au
uT u .
Evident properties of the Rayleigh quotient ρ(u, A) are (see Section4.3.11 for
variational properties of eigenvalues and self-adjoint operators):
• ρ(γ u, A) = ρ(u, A) for every nonzero scalar γ .
• If Aqi = ˜λiqi, then ρ(qi, A) = ˜λi.
Algorithm 11.1 Rayleigh quotient iteration.
0. Initialization: set i = 0, stopping tolerance θ and choose a shift σ. Initialize x0
such that ∥x0∥= 1, and set ρ0 = xT
0 Ax0
xT
0 x0 .
1. Compute yi+1 = (A −ρi I)−1xi.
2. Compute xi+1 = yi+1/∥yi+1∥2.
3. Compute the approximate Rayleigh quotient ρi+1 = ρ(xi+1, A).
4. Stop updating the Rayleigh quotient and set ρM = ρi+1, M = i + 1, if
∥Axi+1 −ρi+1xi+1∥2 < θ. Otherwise, set i = i + 1 and go to step 1.
From results of Section7.1, p. 217, it follows that when the stopping criterion
∥Axi −ρixi∥2 < θ in the above algorithm is satisﬁed, then the computed Rayleigh
quotient ρi is within tolerance θ of an eigenvalue of A.
If we will take the shift σi = ann in Algorithm10.5 and then run Algorithm11.1
with x0 = (0, ..., 0, 1)T , then σi = ρi.
Theorem 11.2 Rayleigh quotient iteration is locally cubically convergent.
Proof We will analyze only the case that A is diagonal. This is enough, since writing
QT AQ = Λ, where Q is an orthogonal matrix whose columns are eigenvectors, and
Λ = diag(˜λ1, . . . , ˜λn) is a diagonal matrix of eigenvalues, we can write the Rayleigh
quotient ρi computed at iteration i as
ρi = ρ(xi, A) = xT
i Axi
xT
i xi
= ˆxT
i QT AQ ˆxi
ˆxT
i QT Q ˆxi
= ˆxT
i Λˆxi
ˆxT
i ˆxi
= ρ(ˆxi, Λ),
1John William Strutt, 3rd Baron Rayleigh (1842–1919), was an English physicist.

378
11
Algorithms for Solution of Symmetric Eigenvalue Problems
where ˆxi ≡QT xi and ˆyi ≡QT yi. We observe also that Q ˆyi+1 = (A−ρi I)−1Q ˆxi, so
ˆyi+1 = QT (A −ρi I)−1Q ˆxi = (QT AQ −ρi I)−1 ˆxi = (Λ −ρi I)−1 ˆxi.
We see that running Algorithm 11.1 for Rayleigh quotient iteration with A and x0 is
the same as running Rayleigh quotient iteration with Λ and ˆx0. Thus we will assume
that A = Λ is already diagonal and that the eigenvectors of A are ei, or the columns
of the identity matrix I.
Assume that xi converges to e1, so we can write xi = e1 + di, where ∥di∥2 ≡
ε ≪1. To prove cubic convergence, we need to show that xi+1 = e1 + di+1 with
∥di+1∥2 = O(ε3). We ﬁrst note that
1 = xT
i xi = (e1 + di)T (e1 + di) = eT
1 e1 + 2eT
1 di + dT
i di = 1 + 2di1 + ε2,
so that di1 = −ε2/2. Therefore,
ρi = xT
i Λxi = (e1 + di)T Λ(e1 + di) = eT
1 Λe1 + 2eT
1 Λdi + dT
i Λdi
= ˜λ1 −(−2eT
1 Λdi −dT
i Λdi) = ˜λ1 −η = ˜λ1 −˜λ1ε2 −dT
i Λdi,
(11.1)
and since −2eT
1 di = ε2, we have that η ≡−2eT
1 Λdi −dT
i Λdi = ˜λ1ε2 −dT
i Λdi. We
see that
|η| ≤|˜λ1|ε2 + ∥Λ∥2∥di∥2
2 ≤|˜λ1|ε2 + ∥Λ∥2ε2 ≤2∥Λ∥2ε2,
so ρi = ˜λ1 −η = ˜λ1 + O(ε2) is a very good approximation to the eigenvalue ˜λ1.
Using Algorithm 11.1, we see that yi+1 = (A −ρi I)−1xi. Thus, using the remark
above, we can write
yi+1 = (Λ −ρi I)−1xi = diag
1
˜λ j −ρi
xi
=

xi1
˜λ1 −ρi
,
xi2
˜λ2 −ρi
, . . . ,
xin
˜λn −ρi
	T
.
Since xi = e1 + di we get
yi+1 =
 1 + di1
˜λ1 −ρi
,
di2
˜λ2 −ρi
, . . . ,
din
˜λn −ρi
	T
.
Next, because ρi = ˜λ1 −η and di1 = −ε2/2 we obtain
yi+1 =
1 −ε2/2
η
,
di2
˜λ2 −˜λ1 + η
, . . . ,
din
˜λn −˜λ1 + η
	T
= 1 −ε2/2
η

1,
di2η
(1 −ε2/2)(˜λ2 −˜λ1 + η)
, . . . ,
dinη
(1 −ε2/2)(˜λn −˜λ1 + η)
	T
≡1 −ε2/2
η
(e1 + ˆdi+1).

11.2 Rayleigh Quotient Iteration
379
To bound ∥ˆdi+1∥2, we will bound every denominator in the above expression using
the inequality (see Section7.1, p. 217, for the deﬁnition of gap)
|˜λ j −˜λ1 + η| ≥gap(1, Λ) −|η|,
as well as the following estimate for |η| in every numerator,
|η| ≤|˜λ1|ε2 + ∥Λ∥2∥di∥2
2 ≤2∥Λ∥2ε2,
to get
∥ˆdi+1∥2 ≤
∥di+1∥2|η|
(1 −ε2/2)(gap(1, Λ) −|η|)
≤
2∥Λ∥2ε3
(1 −ε2/2)(gap(1, Λ) −2∥Λ∥2ε2).
(11.2)
In other words, inequality (11.2) means that ∥ˆdi+1∥2 = O(ε3). Finally, by Algorithm
11.1, we have that xi = yi/∥yi∥2, and thus
xi+1 = e1 + di+1 = yi+1/∥yi+1∥2,
(11.3)
or
xi+1 =
yi+1
∥yi+1∥2
=
1 −ε2/2
η
(e1 + ˆdi+1)
	




1 −ε2/2
η
(e1 + ˆdi+1)




2
= (e1 + ˆdi+1)/∥e1 + ˆdi+1∥2.
Comparing the above expression with (11.2), we see that ∥ˆdi+1∥2 = O(ε3).
□
Below we present an example of Algorithm 11.1 used for the computation of the
Rayleigh quotient of some predeﬁned symmetric tridiagonal matrix A.
Example 11.1 We compute the Rayleigh quotient of the matrix
A =
⎛
⎜⎜⎝
1 5 0 0
5 7 1 0
0 1 3 4
0 0 4 2
⎞
⎟⎟⎠
using Algorithm 11.1. This matrix has four different eigenvalues,
λ = (−2.0607, −1.3469, 6.4239, 9.9837),
which we obtained using the command eig(A) in MATLAB®. The computed
Rayleigh quotient is ρ = −1.3469, which is one of the eigenvalues of A. The
MATLAB® program RayleighQuotient.m of Section1.16 is available for run-
ning this test.2
2The
MATLAB®
programs
can
be
found
on
the
online
version
of
Chapter
1
(doi:10.1007/978-3-319-57304-5_1).

380
11
Algorithms for Solution of Symmetric Eigenvalue Problems
11.3
Divide and Conquer
The main advantage of the method of divide and conquer is that it is the fastest among
existing methods to compute all eigenvalues and eigenvectors of a tridiagonal matrix
of size n greater than about n = 25. The divide and conquer method is not easy to
implement efﬁciently in a stable way. It was introduced ﬁrst in [21], and the ﬁrst
efﬁcient and stable implementation of it was presented in [30, 38].
The structure of the algorithm is the following. Let T be a tridiagonal symmetric
matrix,
T =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
a1 b1
0
...
...
0
b1 a2
b2
0
...
0
... ...
...
...
...
...
... am−1 bm−1 ...
...
0
0 bm−1 am
bm
0
0
0 0
bm
am+1 bm+1 0
0 0
0
bm+1 ...
...
... ...
...
...
...
...
0 0
0
...
...
bn−1
0 0
0
0
bn−1 an
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
such that we can decompose it in the following way:
T =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
a1 b1
0
...
...
0
b1 a2
b2
0
...
0
... ...
...
...
...
...
... am−1 bm−1
...
...
0
0 bm−1 am −bm 0
0
0
0 0
0
am+1 −bm bm+1 0
0 0
0
bm+1
...
...
... ...
...
...
...
...
0 0
0
...
...
bn−1
0 0
0
0
bn−1 an
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
+
⎛
⎜⎜⎝
...
...
...bm bm...
...bm bm...
...
...
⎞
⎟⎟⎠
=
 T1 0
0 T2
	
+ bm
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
0
...
0
1
1
0
...
0
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
0...0 1 1 0....0 
=
 T1 0
0 T2
	
+ bmvvT .
(11.4)

11.3 Divide and Conquer
381
Assume that we have an eigendecomposition of T1, T2 such that T1 = Q1Λ1QT
1
and T2 = Q2Λ2QT
2 . Then we can write that
T =
 T1 0
0 T2
	
+ bmvvT =
 Q1Λ1QT
1
0
0
Q2Λ2QT
2
	
+ bmvvT
=
 Q1 0
0 Q2
	  Λ1 0
0 Λ2
	
+ bmuuT
	  QT
1
0
0
QT
2
	
.
(11.5)
Let us deﬁne the diagonal matrix
D =
Λ1 0
0 Λ2
	
and rewrite (11.5) as
T =
 Q1 0
0 Q2
	 
D + bmuuT   QT
1
0
0
QT
2
	
.
(11.6)
We observe that the eigenvalues of T are the same as those of
D + bmuuT = D + ρuuT
(11.7)
with scalar ρ = bm. Thus, our goal now is to ﬁnd the eigenvalues of (11.7). To do
so, we proceed in the following steps:
Step 1. We assume that the diagonal elements of D are sorted such that d1 ≥
... ≥dn and D + λI is nonsingular.
Step 2. To ﬁnd the eigenvalues of D + ρuuT, we compute the characteristic
polynomial
det(D + ρuuT −λI) = 0,
noting that
det(D + ρuuT −λI) = det((D −λI)(I + ρ(D −λI)−1uuT )).
(11.8)
Step 3. By assumption, in Step 1 we have
det(D −λI) ̸= 0,
and thus in (11.8) we should have
det(I + ρ(D −λI)−1uuT ) = 0.
Finally, the eigenvalues of D + ρuuT are found from the above expression.
Lemma 11.1 If x and y are vectors, then det(I + xyT ) = 1 + yT x.

382
11
Algorithms for Solution of Symmetric Eigenvalue Problems
The proof of this lemma is left as Exercise 11.9. Thus, using this lemma we can get
that
det(I + ρ(D −λI)−1uuT ) = 1 + uT ρ(D −λI)−1u = 1 + ρ
n

i=1
u2
i
di −λ = f (λ).
(11.9)
We see that the eigenvalues of T are the roots of the so-called secular equation
f (λ) = 0.
The secular equation can be solved using Newton’s method
λk+1 = λk −f (λk)
f ′(λk),
(11.10)
where
f ′(λk) = ρ
n

i=1
u2
i
(di −λk)2 ,
(11.11)
and k is the number of the iteration in Newton’s method. We observe that f ′(λk) is
positive and increasing when ρ = bm > 0 except at the point λ = di. Thus, the roots
of f (λ) are seperated by the di where λ = di be a vertical asymptotes, and f (λ) is
monotonic and smooth on every interval (di, di+1). Then Newton’s method (11.10)
will converge for a starting point λ0 ∈(di, di+1).
To obtain the eigenvectors x for the eigenvalues λ that we have found via solution
of the secular equation (11.9), we need the following lemma.
Lemma 11.2 If ˜λ is an eigenvalue of D + ρuuT , then
x = (D −˜λI)−1u
(11.12)
is its eigenvector.
Proof If ˜λ is an eigenvalue of D +ρuuT and (11.12) is its eigenvector, then we must
have
(D + ρuuT )((D −˜λI)−1u) = ((D −˜λI) + ˜λI + ρuuT )((D −˜λI)−1u)
= u + ˜λ(D −˜λI)−1u + u(ρuT (D −˜λI)−1u).
(11.13)
Now we use the expression for the secular equation, or
ρuT (D −˜λI)−1u + 1 = f (˜λ) = 0,
and thus
ρuT (D −˜λI)−1u = −1,

11.3 Divide and Conquer
383
or
(D + ρuuT )((D −˜λI)−1u) = u + ˜λ(D −˜λI)−1u −u
= ˜λ(D −˜λI)−1u = ˜λx.
(11.14)
□
Algorithm 11.2 Finding the eigenvalues and eigenvectors of a symmetric tridiago-
nal matrix using divide and conquer.
1. If T is 1 × 1
return Q = 1, Λ = T
else
form T =
 T1 0
0 T2
	
+ bmυυT
2. Compute output matrices Q1 and Λ1 by eigendecomposition of T1.
3. Compute output matrices Q2 and Λ2 by eigendecomposition of T2.
4. Form D + ρuuT from Λ1, Λ2, Q1, Q2.
5. Find eigenvalues Λ and eigenvectors Q′ of D + ρuuT as roots of the secular
equation (11.9) using Newton’s method (11.10).
6. Form Q =
 Q1 0
0 Q2
	
Q′ = eigenvectors of T .
6. Return Q, Λ and stop.
Remark 11.1
• Eigenvectors can be computed by formula (11.12).
• This formula is not stable when two eigenvalues (˜λi, ˜λi+1) are located close to
each other. This means that (D −˜λi)−1u and (D −˜λi+1)−1u are inaccurate and
far from orthogonal.
• Löwner’s theorem is used to compute eigenvectors for two eigenvalues (˜λi, ˜λi+1)
that are close to each other.
Theorem 11.3 (Löwner)3.
Let D = diag(d1, . . . , dn) be diagonal with dn < . . . < d1. Let ˜λn < . . . < ˜λ1 be
given, satisfying the alternating property
dn < ˜λn < · · · < di+1 < ˜λi+1 < di < ˜λi < · · · < d1 < ˜λ1.
Then there is a vector ˆu such that the ˜λi are the exact eigenvalues of ˆD ≡D+ ˆu ˆuT .
The entries of ˆu are
|ˆui| =

n
j=1(˜λ j −di)
n
j=1, j̸=i(d j −di)
1/2
.
(11.15)
3Charles Löwner (1893–1968) was an American mathematician.

384
11
Algorithms for Solution of Symmetric Eigenvalue Problems
Proof The characteristic polynomial of ˆD can be written in two ways, as
det( ˆD −λI) =
n
j=1
(˜λ j −λ)
(11.16)
and using
det( ˆD −λI) = det(D + ˆu ˆuT −λI) = det(D(I + D−1 ˆu ˆuT ) −λI)
= det((D −λI)(I + (D −λI)−1 ˆu ˆuT ))
as
det( ˆD −λI) =
⎛
⎝
n

j=1
(d j −λ)
⎞
⎠
⎛
⎝1 +
n

j=1
ˆu2
j
d j −λ
⎞
⎠
=
⎛
⎝
n

j=1
(d j −λ)
⎞
⎠
⎛
⎜⎜⎜⎜⎜⎝
1 +
n

j = 1
j ̸= i
ˆu2
j
d j −λ +
ˆu2
i
di −λ
⎞
⎟⎟⎟⎟⎟⎠
=
⎛
⎝
n

j=1
(d j −λ)
⎞
⎠
⎛
⎜⎜⎜⎜⎜⎝
1 +
n

j = 1
j ̸= i
ˆu2
j
d j −λ
⎞
⎟⎟⎟⎟⎟⎠
+
⎛
⎜⎝
n

j = 1
(d j −λ)
⎞
⎟⎠
ˆu2
i
di −λ
=
⎛
⎝
n

j=1
(d j −λ)
⎞
⎠
⎛
⎜⎜⎜⎜⎜⎝
1 +
n

j = 1
j ̸= i
ˆu2
j
d j −λ
⎞
⎟⎟⎟⎟⎟⎠
+
⎛
⎜⎜⎜⎜⎜⎝
n

j = 1
j ̸= i
(d j −λ)
⎞
⎟⎟⎟⎟⎟⎠
ˆu2
i .
(11.17)
Now we choose λ = di in (11.16) and in (11.17) for det( ˆD −λI) to get
n
j=1
(˜λ j −di) = ˆu2
i
n
j = 1
j ̸= i
(d j −di),
or
ˆu2
i =
n
j=1(˜λ j −di)
n
j=1, j̸=i(d j −di).

11.3 Divide and Conquer
385
Using the alternating property, we can show that the right-hand side in the above
expression is positive, and thus we get (11.15).
□
Below we give a stable algorithm for computing the eigenvalues and eigenvectors
where we have assumed that ρ = 1.
Algorithm 11.3 Compute the eigenvalues and eigenvectors of D + uuT .
1. Solve the secular equation 1+
n

i=1
u2
i
di−˜λ = 0 to get the eigenvalues ˜λi of D+uuT .
2. Use Löwner’s theorem to compute ˆu so that the ˜λi are “exact” eigenvalues of
D + ˆu ˆuT .
3. Use formula (11.12) in Lemma 11.2 to compute the eigenvectors of ˆD = D +
ˆu ˆuT .
Below we present an example using Algorithm11.2 on the computation of
eigenvalues and eigenvectors of a predeﬁned symmetric tridiagonal matrix A. The
MATLAB® program of Section1.17 is available for running this test.4
Example 11.2 We compute the eigenvalues and eigenvectors of the matrix
A =
⎛
⎜⎜⎜⎜⎝
10.8901 9.5557
0
0
0
9.5557 10.6813 2.6985
0
0
0
2.6985 2.2341 4.0888
0
0
0
4.0888 13.5730 14.8553
0
0
0
14.8553 3.7942
⎞
⎟⎟⎟⎟⎠
using Algorithm11.2. This matrix has ﬁve different eigenvalues,
λ = (−7.5981, −0.1710, 3.5923, 20.5154, 24.8341),
obtained using the command eig(A) in MATLAB®. We apply the MATLAB®
program of Section1.17 and compute the eigenvalues and eigenvectors of the matrix
A above. It turns out that the computed eigenvalues and eigenvectors of the above
matrix using the MATLAB® program of Section1.17 are the same as those obtained
by the command eig(A) in MATLAB®.
11.4
Bisection and Inverse Iteration
The bisection algorithm uses Sylvester’s inertia theorem, Theorem 4.44, to ﬁnd only
the k eigenvalues that we want.
Recall that Inertia(A) = (ν, ζ, π), where ν, ζ, and π are the numbers of negative,
zero, and positive eigenvalues of A, respectively. Suppose that X is nonsingular.
Using Sylvester’s inertia theorem, we have that Inertia(A) = Inertia(X T AX).
4All MATLAB® programs referenced on this page can be found on the online version of Chapter 1
(doi:10.1007/978-3-319-57304-5_1).

386
11
Algorithms for Solution of Symmetric Eigenvalue Problems
Now suppose that we use Gaussian elimination to factorize A −zI = L DLT ,
where L is nonsingular and D is diagonal. Then Inertia(A −zI) = Inertia(D). The
inertia of D is very easily computable, since D is diagonal.
Further in our considerations of this section we use the notation
#dii < 0,
which means “the number of values of dii less than zero.” Then
Inertia(A −zI) = (#dii < 0, #dii = 0, #dii > 0)
= (# negative eigenvalues of A −zI,
# zero eigenvalues of A −zI,
# positive eigenvalues of A −zI)
= (# eigenvalues of A < z,
# eigenvalues of A = z,
# eigenvalues of A > z).
Let us deﬁne
NrofEig(A, z) = # eigenvalues of A < z.
Suppose z1 < z2, and we compute Inertia(A −z1I) and Inertia(A −z2I). Then
the number of eigenvalues N[z1,z2) in the interval [z1, z2) equals (# eigenvalues of
A < z2) – (# eigenvalues of A < z1), or
N[z1,z2) = NrofEig(A, z2) −NrofEig(A, z1).
Algorithm 11.4 Bisection: ﬁnd all eigenvalues of A inside [a, b) to a given error
tolerance θ.
na = NrofEig(A, a)
nb = NrofEig(A, b)
if na = nb, quit (because there are no eigenvalues in [a, b))
put [a, na, b, nb] onto WorkingArray
/∗WorkingArray contains all subintervals of [a, b) containing
eigenvalues from n −na through n −nb + 1, which the algorithm
will update until they are less than tolerance θ. ∗/
while WorkingArray is not empty
remove [low, nlow, up, nup] from WorkingArray
if up −low < θ then
print ”there are nup −nlow eigenvalues in [low, up)”
else
mid = (low + up)/2
nmid = NrofEig(A, mid)
if nmid > nlow then print ”there are eigenvalues in [low, mid)”
put [low, nlow, mid, nmid] onto WorkingArray
end if

11.4 Bisection and Inverse Iteration
387
if nup > nmid then print “there are eigenvalues in [mid, up)”
put [mid, nmid, up, nup] onto WorkingArray
end if
end if
end while
From NrofEig(A, z) it is easy to use Gaussian elimination, provided that
A −zI =
⎛
⎜⎜⎜⎜⎝
a1 −z
b1
...
...
b1
a2 −z
...
...
...
...
...
...
...
bn−2 an−1 −z bn−1
...
...
bn−1
an −z
⎞
⎟⎟⎟⎟⎠
= L DLT =
⎛
⎜⎜⎝
1
... ...
l1
1
...
... ... ...
... ln−1 1
⎞
⎟⎟⎠
⎛
⎜⎜⎝
d1 ... ...
... d2 ...
... ... ...
... .. dn
⎞
⎟⎟⎠
⎛
⎜⎜⎝
1 l1... ...
..
1
...
... ... ln−1
... ...
1
⎞
⎟⎟⎠.
(11.18)
Using (11.18), we observe that
a1 −z = d1,
d1l1 = b1,
l2
i−1di−1 + di = ai −z,
dili = bi.
(11.19)
Substitute li = bi/di into l2
i−1di−1 + di = ai −z to obtain the recurrence formula
di = (ai −z) −b2
i−1
di−1
,
(11.20)
from which is easy to compute the values di of the matrix D from previously known
values di−1 and known values ai, bi. In [26, 27], it was shown that since A −zI is a
tridiagonal matrix, the formula (11.20) is stable.
Below we present an example using Algorithm11.4. The MATLAB® program of
Section1.18 is available for running this test.5
Example 11.3 We compute the eigenvalues of the matrix
A =
⎛
⎜⎜⎜⎜⎝
16.1984 2.8029
0
0
0
2.8029 9.0301 23.0317
0
0
0
23.0317 12.5310 24.2558
0
0
0
24.2558 10.5238 17.5216
0
0
0
17.5216 10.4891
⎞
⎟⎟⎟⎟⎠
5The
MATLAB®
programs
can
be
found
on
the
online
version
of
Chapter
1
(doi:10.1007/978-3-319-57304-5_1).

388
11
Algorithms for Solution of Symmetric Eigenvalue Problems
using the bisection algorithm, Algorithm11.4. This matrix has ﬁve different eigen-
values λ = (−25.0154, −1.2034, 15.9244, 21.8223, 47.2444), which we obtained
using the command eig(A) in MATLAB®.
We compute now the eigenvalues of the matrix A using the MATLAB® program
of Section1.18.6 Since in the MATLAB® program of Section1.18 we require that the
left- and right-hand sides of the input interval in Algorithm11.4 have difference no
more than the given error tol, it follows that this interval will contain one eigenvalue,
and the left- or right-hand side of this interval can be taken as our desired eigenvalue.
The output information obtained by the MATLAB® program of Section1.18 for the
matrix A deﬁned above is the following:
There is 1 eigenvalue in the interval [-25.0154,-25.0154)
There is 1 eigenvalue in the interval [-1.2034,-1.2034)
There is 1 eigenvalue in the interval [15.9244,15.9244)
There is 1 eigenvalue in the interval [21.8223,21.8223)
There is 1 eigenvalue in the interval [47.2444,47.2444)
Comparing the above results with the exact ones, we observe that the computed
eigenvalues using the MATLAB® program of Section1.18 are the same as those
produced by the command eig(A).
11.5
Jacobi’s Method
We will not reduce the original matrix A to a tridiagonal matrix T as in all previous
methods, but will work on the original A. Jacobi’s7 method produces a sequence
Ai, i = 0, ..., m, of orthogonally similar matrices for a given matrix A = A0, which
will converge to a diagonal matrix with the eigenvalues on the diagonal. The next
matrix Ai+1 is obtained from the previous one Ai by the recurrence formula
Ai+1 = J T
i Ai Ji,
where Ji is an orthogonal matrix called a Jacobi rotation. Thus
Am = J T
m−1 Am−1Jm−1
= J T
m−1J T
m−2 Am−2Jm−2Jm−1 = · · ·
= J T
m−1 · · · J T
0 A0J0 · · · Jm−1
= J T AJ.
If we choose every Ji in some special way, then Am will converge to a diagonal
matrix Λ for large m. Thus we can write
Λ ≈J T AJ,
6All MATLAB® programs referenced on this page can be found on the online version of Chapter 1
(doi:10.1007/978-3-319-57304-5_1).
7Carl Gustav Jacob Jacobi (1804–1851) was a German mathematician.

11.5 Jacobi’s Method
389
or
JΛJ T ≈A,
from which we see that the columns of J are approximate eigenvectors.
To make J T AJ nearly diagonal, we will construct the Ji iteratively to make one
pair of off-diagonal entries of Ai+1 = J T
i Ai Ji zero at a time. We will do this by
taking Ji to be a Givens rotation, or
j
k
Ji = R( j, k, θ) ≡
j
k
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1
1
...
cos θ
−sin θ
...
sin θ
cos θ
...
1
1
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
(11.21)
where θ is chosen such that the ( j, k) and (k, j) entries of Ai+1 will be zero. To
determine θ (or actually cos θ and sin θ), let us consider

a(i+1)
j j
a(i+1)
jk
a(i+1)
kj
a(i+1)
kk

=

cos θ −sin θ
sin θ
cos θ
T 
a(i)
j j a(i)
jk
a(i)
kj a(i)
kk
 
cos θ −sin θ
sin θ
cos θ

=
c −s
s c
	T 
a(i)
j j a(i)
jk
a(i)
kj a(i)
kk
  c −s
s c
	
=

λ1 0
0 λ2

,
(11.22)
where λ1 and λ2 are the eigenvalues of

a(i)
j j a(i)
jk
a(i)
kj a(i)
kk

.
It is easy to compute c = cos θ and s = sin θ from (11.22) on every iteration i:
 λ1 0
0 λ2
	
=

a j jc2 + akks2 + 2sca jk
sc(akk −a j j) + a jk(c2 −s2)
sc(akk −a j j) + a jk(c2 −s2)
a j js2 + akkc2 −2sca jk

.
Setting the off-diagonals to zero and solving for θ, we get
0 = sc(akk −a j j) + a jk(c2 −s2),

390
11
Algorithms for Solution of Symmetric Eigenvalue Problems
or
a j j −akk
2a jk
= c2 −s2
2sc
= cos 2θ
sin 2θ = cot 2θ ≡τ.
We now introduce the notation t = s
c = tan θ, noting that t2 + 2τt −1 = 0. Solving
this quadratic equation, we get
t =
sign(τ)
|τ| +
√
1 + τ 2 ,
c =
1
√
1 + τ 2 ,
s = tc.
(11.23)
Algorithm 11.5 Compute and apply a Jacobi rotation to A for indices ( j, k).
function Jacobi-Rotation(A, j, k)
if |a jk| is not too small
τ = (a j j −akk)/(2a jk)
t = sign(τ)/(|τ| +
√
1 + τ 2)
c = 1/
√
1 + τ 2
s = tc
A = RT ( j, k, θ)AR( j, k, θ)
/* here, c = cos θ and s = sin θ */
J = J R( j, k, θ) /* if eigenvectors are desired */
end if
end if
The general Jacobi algorithm is given below.
Algorithm 11.6 Jacobi’s method to ﬁnd the eigenvalues of a symmetric matrix.
Perform the following steps in a loop:
1. Choose ( j, k).
2. Call the function Jacobi-Rotation(A, j, k) until A is sufﬁciently diagonal.
There are different ways to choose the pairs ( j, k). To measure the progress of
convergence, we deﬁne
off(A) ≡


1≤j<k≤n
a2
jk.
Thus, off(A) is the root-sum-of-squares of the (upper) off-diagonal entries of A, so
A is diagonal if and only if off(A) = 0. We want to make off(A) = 0 as quickly as
possible.
The next lemma shows that off(A) decreases monotonically with every iteration
of the Jacobi rotation.

11.5 Jacobi’s Method
391
Lemma 11.3 Let A′ be the matrix obtained after calling the procedure Jacobi-
Rotation (A, j, k) for j ̸= k. Then
off2(A′) = off2(A) −a2
jk.
The proof of this lemma can be found in [23].
The next algorithm is the original version of the Jacobi algorithm developed in
1846. However, in practical computations, this algorithm is too slow.
Algorithm 11.7 Classical Jacobi’s algorithm.
0. Set i = 0 and tolerance θ.
1. Choose ( j, k) such that a jk is the largest off-diagonal entry in magnitude.
2. Call Jacobi-Rotation(A, j, k).
3. Compute offi(A).
4. Stop and set offM(A) = offi(A), M = i, if offi(A) < θ. Otherwise, set i = i + 1
and go to step 1.
Theorem 11.4 Let A′ be the matrix obtained after calling Jacobi-Rotation(A, j, k)
for j ̸= k. After one step of calling the Jacobi-Rotation procedure in the classical
Jacobi’s algorithm, Algorithm11.7, we have
off(A′) ≤

1 −1
N off(A),
where N =
n(n−1)
2
is the number of superdiagonal entries of A. After k steps of
calling the Jacobi-Rotation procedure, we have
off(A′) ≤

1 −1
N
	k/2
off(A).
Proof By Lemma 11.3, after one step of Jacobi rotation, we have
off2(A′) = off2(A) −a2
jk,
where a jk is the largest off-diagonal entry. Thus,
off2(A) ≤n(n −1)
2
a2
jk,
or
a2
jk ≥
1
n(n −1)/2off2(A),

392
11
Algorithms for Solution of Symmetric Eigenvalue Problems
so that
off2(A) −a2
jk ≤

1 −1
N
	
off2(A),
from which follows the statements of Theorem 11.4.
□
Summarizing, we have that the classical Jacobi’s algorithm converges at least
linearly with the error decreasing by a factor of at least

1 −1
N at each step.
Theorem 11.5 Jacobi’s method is locally quadratically convergent after N steps.
This means that for a large i,
off(Ai+N) = O(off2(Ai)).
The proof of this theorem is given in [113]. In practice, we do not use the classical
Jacobi’s algorithm, because searching for the largest entry is too slow. We use the
following simple method to choose j and k.
Algorithm 11.8 Cyclic-by-row-Jacobi: run through the off-diagonals of A rowwise.
Loop
for j = 1 to n −1
for k = j + 1 to n
call Jacobi-Rotation(A, j, k)
end for
end for
until A is sufﬁciently diagonal.
The matrix A no longer changes when Jacobi-Rotation(A, j, k) chooses only
c = 1 and s = 0 for an entire pass through the inner loop. As was shown in [113],
the cyclic Jacobi’s algorithm is also asymptotically quadratically convergent, like the
classical Jacobi’s algorithm.
The cost of one loop in Algorithm 11.8 is around half the cost of the tridiagonal
reduction and the computation of eigenvalues and eigenvectors via QR iteration, as
well as more than the cost using divide and conquer. For convergence of Jacobi’s
method, one needs to perform from ﬁve to ten loops in Algorithm 11.8, and thus this
method is much slower than other methods.
Belowwepresent anexampleusingtheclassical Algorithm 11.7. TheMATLAB®
program of Section1.19 is available for running this test.8
Example 11.4 We compute the eigenvalues of the matrix
A =
⎛
⎜⎜⎜⎜⎝
14.7776 4.9443
0
0
0
4.9443 18.2496 28.3358
0
0
0
28.3358 10.8790 2.5361
0
0
0
2.5361 11.0092 18.9852
0
0
0
18.9852 15.0048
⎞
⎟⎟⎟⎟⎠
(11.24)
8The
MATLAB®
programs
can
be
found
on
the
online
version
of
Chapter
1
(doi:10.1007/978-3-319-57304-5_1).

11.5 Jacobi’s Method
393
using the classical Jacobi algorithm, Algorithm 11.7. This matrix has ﬁve different
eigenvalues,
λ = (−14.6416, −5.8888, 14.6644, 32.0314, 43.7547),
whichweobtainedusingthecommand eig(A)inMATLAB®.Werun the MATLAB®
program of Section1.19 until the matrix A is sufﬁciently diagonal, i.e., until
of f (A) <
tol for tol = 0.005.9 The computed ﬁnal matrix A obtained after
all Jacobi rotations is the following:
A =
⎛
⎜⎜⎜⎜⎝
14.6644
0.0000
−0.0000
0.0029
0.0001
0.0000
43.7547 −0.0000
0.0000 −0.0008
−0.0000 −0.0000 −14.6416 0.0000 −0.0000
0.0029
0.0000
0.0000
−5.8888 0.0000
0.0001 −0.0008 −0.0000
0.0000
32.0314
⎞
⎟⎟⎟⎟⎠
.
We observe that the values lying on the diagonal of the above matrix A are the
eigenvalues of the initial matrix A given in (11.24).
Comparing these values with the exact ones, we observe that the computed eigen-
values using the MATLAB® program of Section1.19 are almost the same (depending
on the input tolerance tol) as produced by the command eig(A).
11.6
Algorithms for the Singular Value Decomposition
Algorithms for the solution of the symmetric eigenvalue problem can be transformed
to the algorithms for the SVD of a symmetric matrix A. Eigendecomposition of a
symmetric matrix A, except for Jacobi’s method, can be performed in the following
steps:
1. Reduce A to tridiagonal form T with an orthogonal matrix Q1:
A = Q1T QT
1 .
2. Find the eigendecomposition of T :
T = Q2ΛQT
2 ,
where Λ is the diagonal matrix of eigenvalues and Q2 is the orthogonal matrix
whose columns will be eigenvectors.
3. Combine these decompositions to get
A = (Q1Q2)Λ(Q1Q2)T .
9The MATLAB® programs referenced on this page can be found on the online version of Chapter 1
(doi:10.1007/978-3-319-57304-5_1).

394
11
Algorithms for Solution of Symmetric Eigenvalue Problems
The columns of Q = Q1Q2 will be the eigenvectors of A.
All the algorithms for the SVD of a general matrix G, except Jacobi’s method,
have an analogous structure, which is the following:
1. Reduce G to bidiagonal form B, which has nonzero elements only on the main
diagonal and ﬁrst superdiagonal, with orthogonal matrices U1 and V1 such that
G = U1BV T
1 .
2. Find the SVD of B:
B = U2ΣV T
2 ,
where Σ is the diagonal matrix of singular values, and U2 and V2 are orthogonal
matrices whose columns are left and right singular vectors, respectively.
3. Combine these decompositions to get
G = (U1U2)Σ(V1V2)T .
The columns of U = U1U2 and V = V1V2 are left and right singular vectors of
G, respectively.
Lemma 11.4 Let B be an n × n bidiagonal matrix such that
B =
⎛
⎜⎜⎜⎜⎝
a1 b1
...
...
... a2
...
...
... ...
...
...
... ... an−1 bn−1
... ...
...
an
⎞
⎟⎟⎟⎟⎠
.
(11.25)
There are the following possibilities for converting the problem of ﬁnding the SVD
of B to that of ﬁnding the eigenvalues and eigenvectors of a symmetric tridiagonal
matrix.
1. Let the matrix A be such that
A =
 0 BT
B 0
	
.
Let P be the permutation matrix
P = (e1, en+1, e2, en+2, . . . , en, e2n).
Here ei denote the ith column of the 2 × 2 identity matrix. Then the matrix
TPT AP ≡PT AP

11.6 Algorithms for the Singular Value Decomposition
395
is symmetric tridiagonal such that
TPT AP =
⎛
⎜⎜⎜⎜⎝
0
a1
... ...
a1
0
... ...
...
...
... ...
... bn−1 0 an
...
... an 0
⎞
⎟⎟⎟⎟⎠
.
The matrix TPT AP has all zeros on its main diagonal, and its superdiagonal and
subdiagonal are a1, b1, a2, b2, . . . , bn−1, an.
If (˜λi, xi) is an eigenpair for TPT AP, with xi a unit vector such that
TPT APxi = ˜λixi,
then ˜λi = ±σi, where σi is a singular value of B, and Pxi =
1
√
2
 υi
±ui
	
. Here,
ui and υi are left and right singular vectors of B, respectively.
2. Let TBBT ≡BBT . Then TBBT is symmetric tridiagonal,
TBBT =
⎛
⎜⎜⎜⎜⎝
a2
1 + b2
1
a2b1
...
...
a2b1
a2
2 + b2
2
...
...
...
...
...
...
...
...
a2
n−1 + b2
n−1 anbn−1
...
...
anbn−1
a2
n
⎞
⎟⎟⎟⎟⎠
,
with diagonal a2
1 +b2
1, a2
2 +b2
2, . . . , a2
n−1 +b2
n−1, a2
n, and superdiagonal and sub-
diagonal a2b1, a3b2, . . . , anbn−1. The singular values of B are the square roots
of the eigenvalues of TBBT , and the left singular vectors of B are the eigenvectors
of TBBT .
3. Let TBT B ≡BT B. Then TBT B is symmetric tridiagonal,
TBT B =
⎛
⎜⎜⎜⎜⎝
a2
1
a1b1
...
...
a1b1 a2
2 + b2
1
...
...
...
...
...
...
...
...
a2
n−1 + b2
n−2 an−1bn−1
...
...
an−1bn−1
a2
n + b2
n−1
⎞
⎟⎟⎟⎟⎠
,
with diagonal a2
1, a2
2 + b2
1, a2
3 + b2
2, . . . , a2
n + b2
n−1 and superdiagonal and subdi-
agonal a1b1, a2b2, . . . , an−1bn−1. The singular values of B are the square roots of
the eigenvalues of TBT B, and the right singular vectors of B are the eigenvectors
of TBT B.
The proof of the ﬁrst statement of this lemma follows from Theorem 9.6, and the
proof of the second and third statements follows from Theorem 9.4.

396
11
Algorithms for Solution of Symmetric Eigenvalue Problems
However, direct application of Lemma 11.4 for computing the SVD of a sym-
metric tridiagonal matrix using the algorithms of QR iteration, divide and conquer,
or bisection is inefﬁcient, since by Lemma 11.4, in the case of a matrix TPT AP, we
need to compute not all eigenvalues, but only positive ones, and in addition, there are
difﬁculties in computing singular vectors for tiny singular values: numerical com-
puting of entries for TBBT , TBT B is unstable because of rounding in ﬂoating-point
arithmetic.
There exist, however, the following stable algorithms for computing the SVD
numerically:
1. Different versions of QR iteration. This is the fastest algorithm for small matrices
up to size 25 to ﬁnd all the singular values of a bidiagonal matrix.
2. Divide and conquer. This is the fastest method for ﬁnding all singular values and
singular vectors for matrices larger than 25 × 25.
3. Bisection and inverse iteration. In this algorithm, the ﬁrst part of Lemma 11.4 is
applied for TPT AP = PT AP to ﬁnd only the singular values in a desired interval.
Singular values are computed with high accuracy, but singular vectors can lose
orthogonality.
4. Jacobi’s method. The SVD of a dense matrix G is computed implicitly by applying
Jacobi’s method, Algorithm11.5, to GGT or GT G.
11.7
Different Versions of QR Iteration
for the Bidiagonal SVD
In this section we will present the so-called algorithm dqds (differential quotient–
difference algorithm with shifts [99]), which was originally derived in [28] and later
updated in [32] for the case of computing only singular values. We refer to [95] for
a survey of different versions of QR Iteration for the SVD.
To derive the dqds algorithm, we will begin with the algorithm of LR iteration,
which can be applied to symmetric positive deﬁnite (s.p.d.) matrices. Let T0 be any
symmetric positive deﬁnite matrix. The following algorithm produces a sequence of
similar symmetric positive deﬁnite matrices Ti:
Algorithm 11.9 LR iteration.
0. Set i = 0 and initialize s.p.d. T0. Perform steps 1–4 in a loop:
1. Compute a shift τ 2
i such that it is smaller than the smallest eigenvalue of Ti.
2. Compute the Cholesky factorization of Ti −τ 2
i I = BT
i Bi, where Bi is an upper
triangular matrix with all positive elements on the main diagonal.
3. Update Ti+1 = Bi BT
i + τ 2
i I.
4. Stop updating Ti and set TM = Ti+1, M = i + 1, if ∥Ti+1 −Ti∥2 ≤θ. Here, θ is
a tolerance number. Otherwise, set i = i + 1 and go to step 1.

11.7 Different Versions of QR Iteration for the Bidiagonal SVD
397
The algorithm of LR iteration is very similar to that of QR iteration: we compute
a factorization and multiply the factors in reverse order to get the next iterate Ti+1.
It is easy to see that Ti+1 and Ti are similar:
Ti+1 = Bi BT
i + τ 2
i I = B−T
i
BT
i Bi BT
i + τ 2
i B−T
i
BT
i
= B−T
i
(BT
i Bi + τ 2
i )BT
i = B−T
i
Ti BT
i .
(11.26)
The following lemma states that when we choose the shift τ 2
i = 0, then two steps
of LR iteration produce the same T2 as one step of QR iteration.
Lemma 11.5 Let T2 be the matrix produced by two steps of Algorithm 11.9 with
τ 2
i = 0. Let T ′ be the matrix produced by one step of QR iteration such that QR = T0,
T ′ = RQ. Then T2 = T ′.
Proof Using the property that T0 is symmetric, we factorize T 2
0 in two ways:
1. The ﬁrst factorization is
T 2
0 = T T
0 T0 = (QR)T QR = RT R,
where RT is a lower triangular matrix. We assume that Rii > 0. By uniqueness
of the Cholesky factorization, it is unique.
2. The second factorization is
T 2
0 = BT
0 B0BT
0 B0.
Using Algorithm11.9, we have
T1 = B0BT
0 = BT
1 B1.
Using the second factorization and then the above expression, we can rewrite
T 2
0 = BT
0 B0BT
0 B0 = BT
0 (BT
1 B1)B0 = (B1B0)T B1B0,
where (B1B0)T is a lower triangular matrix. This must be the Cholesky factorization,
since T0 is s.p.d. By uniqueness of the Cholesky factorization, we conclude that
R = B1B0 and that two steps of LR iteration equal one step of QR iteration. We can
prove this also in the following way: since T0 = QR and T0 = QR, we have
T ′ = RQ = RQ(RR−1) = R(QR)R−1 = RT0R−1.
Substituting R = B1B0 and T0 = BT
0 B0 into the right-hand side of the above
equation, we get
T ′ = (B1B0)(BT
0 B0)(B1B0)−1 = B1B0BT
0 B0B−1
0 B−1
1
= B1(B0BT
0 )B−1
1 .

398
11
Algorithms for Solution of Symmetric Eigenvalue Problems
Using the fact B0BT
0 = T1 = BT
1 B1, we ﬁnally obtain
T ′ = B1(BT
1 B1)B−1
1
= B1BT
1 = T2.
□
Remark 11.2
• We observe that Algorithm11.9 and Lemma 11.5 depend on the s.p.d. T0, which
should not be tridiagonal.
• Because of the similarity of LR iteration and QR iteration stated in Lemma 11.5,
the analysis of LR iteration follows from the analysis of QR iteration.
• We observe that the matrices Ti+1 = Bi BT
i +τ 2
i I in Algorithm11.9 are constructed
explicitly, and that can be an unstable procedure because of rounding errors in
ﬂoating-point arithmetic.
The next dqds algorithm is mathematically the same as the algorithm of LR
iteration. However, in the dqds algorithm, the matrices Bi+1 are computed directly
from Bi without constructing Ti+1 = Bi BT
i + τ 2
i I.
Let Bi have diagonal a1, . . . , an and superdiagonal b1, . . . , bn−1, and let Bi+1
have diagonal ˆa1, . . . , ˆan and superdiagonal ˆb1, . . . , ˆbn−1. We assume that b0 =
ˆb0 = bn = ˆbn = 0. Using Algorithm11.9, we have that
BT
i+1Bi+1 + τ 2
i+1I = Ti+1 = Bi BT
i + τ 2
i I.
(11.27)
Writing (11.27) for the ( j, j) entries for j < n, we get
ˆa2
j + ˆb2
j−1 + τ 2
i+1 = a2
j + b2
j + τ 2
i
and expressing ˆa2
j from this, we have
ˆa2
j = a2
j + b2
j −ˆb2
j−1 −δ,
(11.28)
where δ = τ 2
i+1 −τ 2
i . The shift τ 2
i should be chosen in a special way; see step 1 in
Algorithm11.9. Writing (11.27) for the squares of ( j, j + 1), we have
ˆa2
j ˆb2
j = a2
j+1b2
j,
and expressing ˆb2
j from this, we obtain
ˆb2
j = a2
j+1b2
j/ˆa2
j.
(11.29)

11.7 Different Versions of QR Iteration for the Bidiagonal SVD
399
Combining the two Eqs.(11.28) and (11.29), we get the intermediate algorithm.
Algorithm 11.10 Intermediate algorithm.
for j = 1 to n −1
ˆa2
j = a2
j + b2
j −ˆb2
j−1 −δ
ˆb2
j = b2
j(a2
j+1/ˆa2
j)
end for
ˆa2
n = a2
n −ˆb2
n−1 −δ
We observe that Algorithm11.10 maps the squares of the entries of Bi directly to
the squares of the entries of Bi+1. Thus, the square roots are taken only at the end of
the algorithm.
In the next algorithm, we rewrite Algorithm11.10 in the classical notation of [99]
using the change of variables
q j = a2
j, e j = b2
j.
Algorithm 11.11 One step of the qds algorithm.
for j = 1 to n −1
ˆq j = q j + e j −ˆe j−1 −δ
ˆe j = e j(q j+1/ˆq j)
end for
ˆqn = qn −ˆen−1 −δ
The ﬁnal dqds algorithm is the same as qds except that it will be more accurate.
To derive the dqds algorithm, we take the part q j −ˆe j−1 −δ from ˆq j of Algorithm
11.11 and use ﬁrst (11.29) to express ˆe j−1 and then (11.28) to express ˆq j−1 to obtain
d j ≡q j −ˆe j−1 −δ = q j −q je j−1
ˆq j−1
−δ
= q j
 ˆq j−1 −e j−1
ˆq j−1
	
−δ = q j
q j−1 −ˆe j−2 −δ
ˆq j−1
	
−δ
=
q j
ˆq j−1
d j−1 −δ.
(11.30)
Using (11.30), we can rewrite the inner loop of Algorithm 11.11 as
ˆq j = d j + e j,
ˆe j = e j(q j+1/ˆq j),
d j+1 = d j(q j+1/ˆq j) −δ.
(11.31)

400
11
Algorithms for Solution of Symmetric Eigenvalue Problems
To get the ﬁnal algorithm, we note that d j+1 can overwrite d j:
Algorithm 11.12 One step of the dqds algorithm.
d = q1 −δ
for j = 1 to n −1
ˆq j = d + e j
t = (q j+1/ˆq j)
ˆe j = e j t
d = d t −δ
end for
ˆqn = d
The dqds algorithm, Algorithm11.12, has the same number of ﬂoating-point
operations in its inner loop as the qds algorithm, Algorithm 11.11. How to choose
a shift τi in δ = τ 2
i+1 −τ 2
i and an analysis of the convergence of these algorithms are
presented in [32].
11.8
Jacobi’s Method for the SVD
In this section, we will present algorithms that can determine the SVD of a dense
matrix. These algorithms will use Jacobi’s algorithm, Algorithm 11.8, for a symmet-
ric matrix
A = GT G.
Like Algorithm 11.8, the algorithms of this section are very slow in computing
the SVD compared with other methods that we have considered. However, Jacobi’s
method can compute the singular values and singular vectors much more accurately
than those other algorithms.
The ﬁrst Jacobi algorithm computes a Jacobi rotation matrix J at every iteration
step and updates GT G to J T GT G J. Since we compute only G J instead of GT G or
J T GT G J, this algorithm is called one-sided Jacobi rotation.
Algorithm 11.13 One-sided Jacobi rotation of G.
function One-Sided-Jacobi-Rotation (G, j, k)
Compute a j j = (GT G) j j, a jk = (GT G) jk, and akk = (GT G)kk
if |a jk| > ε√a j jakk
τ = (a j j −akk)/(2a jk)
t = sign(τ)/(|τ| +
√
1 + τ 2)
c = 1/
√
1 + t2
s = c t
G = G R( j, k, θ) /* here c = cos θ and s = sin θ */
/* if right singular vectors are desired */
J = J R( j, k, θ)
end if
end if

11.8 Jacobi’s Method for the SVD
401
We note that the entries a j j, a jk, and akk of A = GT G are computed by Algorithm
11.13, where the Jacobi rotation R( j, k, θ) is computed using Algorithm 11.5.
In the next algorithm we assume that G is of order n ×n. We compute the singular
values σi, the left singular vector matrix U, and the right singular vector matrix V
such that G = UΣV T , where Σ = diag(σi).
Algorithm 11.14 One-sided Jacobi.
Loop
for j = 1 to n −1
for k = j + 1 to n
call One-Sided-Jacobi-Rotation (G, j, k)
end for
end for
until GT G is diagonal enough
Set σi = ∥G(:, i)∥2 (the 2-norm of column i of G)
Set U = [u1, . . . , un], where ui = G(:, i)/σi
Set V = J (product of Jacobi rotations)
Thefollowingtheoremshowsthattheone-sidedJacobialgorithm,Algorithm11.14,
can compute the SVD with high accuracy.
Theorem 11.6 Let G = DX be an n × n matrix, where D is diagonal and non-
singular, and X is nonsingular. Let ˆG be the matrix after calling One-Sided-Jacobi-
Rotation (G, j, k) m times in ﬂoating-point arithmetic. Let σ1 ≥. . . ≥σn be the
singular values of G, and let ˆσ1 ≥. . . ≥ˆσn be the singular values of ˆG. Then
|σi −ˆσi|
σi
≤O(mε)κ(X),
where κ(X) = ∥X∥∥X−1∥is the condition number of X.
A proof can be found in [23].
In the example presented below we will illustrate the performance of the one-sided
Jacobi algorithm, Algorithm 11.14, using the MATLAB® program of Section1.20.10
Example 11.5 We compute the SVD decomposition of the matrix
A =
⎛
⎜⎜⎜⎜⎝
3.8373 16.5466
0
0
0
16.5466 17.7476 5.5205
00
0
5.5205 11.4120 7.1830
0
0
0
7.1830 11.4657 8.7969
0
0
0
8.7969 18.5031
⎞
⎟⎟⎟⎟⎠
(11.32)
using the one-sided Jacobi algorithm, Algorithm 11.14. We run the MATLAB®
program of Section1.20 until the matrix AT A is sufﬁciently diagonal, i.e., until
of f (AT A) < tol for tol = 0.005. The computed SVD decomposition of the matrix
10All MATLAB® programs referenced on this page can be found on the online version of Chapter 1
(doi:10.1007/978-3-319-57304-5_1).

402
11
Algorithms for Solution of Symmetric Eigenvalue Problems
A = UΣV T obtained by applying the MATLAB® program of Section1.20 is the
following:11
U =
⎛
⎜⎜⎜⎜⎝
−0.8000 0.4934 −0.2008 −0.2157 −0.1723
0.5608 0.7867 0.0260 −0.1254 −0.2241
−0.1934 0.2983 0.5262
0.7440
0.2077
0.0853 0.1764 −0.7317 0.3083
0.5754
−0.0286 0.1324 0.3830 −0.5377 0.7388
⎞
⎟⎟⎟⎟⎠
,
Σ =
⎛
⎜⎜⎜⎜⎝
7.7615
0
0
0
0
0
30.2188
0
0
0
0
0
1.6960
0
0
0
0
0
13.4582
0
0
0
0
0
25.3541
⎞
⎟⎟⎟⎟⎠
,
V =
⎛
⎜⎜⎜⎜⎝
0.8000 0.4934 −0.2008 −0.2157 −0.1723
−0.5608 0.7867 0.0260 −0.1254 −0.2241
0.1934 0.2983 0.5262
0.7440
0.2077
−0.0853 0.1764 −0.7317 0.3083
0.5754
0.0286 0.1324 0.3830 −0.5377 0.7388
⎞
⎟⎟⎟⎟⎠
.
For comparison, the computed SVD decomposition of the matrix A = UΣV T given
in (11.32) using the svd command in MATLAB® gives the following result:
U =
⎛
⎜⎜⎜⎜⎝
−0.4934 −0.1723 −0.2157 −0.8000 −0.2008
−0.7867 −0.2241 −0.1254 0.5608
0.0260
−0.2983 0.2077
0.7440 −0.1934 0.5262
−0.1764 0.5754
0.3083
0.0853 −0.7317
−0.1324 0.7388 −0.5377 −0.0286 0.3830
⎞
⎟⎟⎟⎟⎠
,
Σ =
⎛
⎜⎜⎜⎜⎝
30.2188
0
0
0
0
0
25.3541
0
0
0
0
0
13.4582
0
0
0
0
0
7.7615
0
0
0
0
0
1.6960
⎞
⎟⎟⎟⎟⎠
,
V =
⎛
⎜⎜⎜⎜⎝
−0.4934 −0.1723 −0.2157 0.8000 −0.2008
−0.7867 −0.2241 −0.1254 −0.5608 0.0260
−0.2983 0.2077
0.7440
0.1934
0.5262
−0.1764 0.5754
0.3083 −0.0853 −0.7317
−0.1324 0.7388 −0.5377 0.0286
0.3830
⎞
⎟⎟⎟⎟⎠
.
11The
MATLAB®
programs
can
be
found
on
the
online
version
of
Chapter
1
(doi:10.1007/978-3-319-57304-5_1).

Questions
403
Questions
11.1 Prove that 5 is an eigenvalue of the matrix
A =
⎛
⎜⎜⎝
6 3 3 1
0 7 4 5
0 0 5 4
0 0 0 8
⎞
⎟⎟⎠.
(11.33)
Compute an eigenvector of A that corresponds to the eigenvalue 5.
11.2 Compute the eigenvalues and corresponding eigenvectors of the matrix
A =
⎛
⎝
1 2 −4
0 2 1
0 0 3
⎞
⎠.
(11.34)
11.3 Compute the eigenvalues and corresponding eigenvectors of the matrix
A =
1 4
1 1
	
.
(11.35)
(a) Compute an estimate to an eigenvalue of A by the Rayleigh quotient with
vector x = (1, 1)T .
(b) If we apply the method of inverse iteration to A, to which one eigenvector of
A will this method converge?
(c) If we apply the method of inverse iteration with a shift σ = 2, what eigenvalue
of A will be obtained?
(d) If we apply the method of QR iteration to A, to what form will this matrix
converge: diagonal or triangular? Why?
11.4 Assume that GT G converges to a diagonal matrix. Prove that Algorithm 11.14
implements the SVD decomposition of the matrix G.
11.5 Let x be a unit vector and y a vector orthogonal to x. Prove that ∥(x + y)xT −
I ∥2 = ∥x + y∥2.
11.6 Let A = D + ρuuT , where D is the diagonal matrix D = diag(d1, ..., dn) and
u is the vector u = (u1, ..., un)T .
(a) Prove that di is an eigenvalue of A if di = di+1 or ui = 0.
(b) Prove that an eigenvector corresponding to di is ei (the ith column of I) if
ui = 0.
11.7 Show how to compute scalars c and ˜c in the function f (λ) = ˜c +
c
d−λ if we
know that at λ = ξ we have f (ξ) = ψ and f ′(ξ) = ψ′. Here ψ, ψ′ are known
scalars.

404
11
Algorithms for Solution of Symmetric Eigenvalue Problems
11.8 Let A = GT G in Algorithm 11.14. Here A and G are of order n × n. Assume
that |a jk| ≤ε√a j jakk for all j ̸= k. Let σn ≤σn−1 ≤... ≤σ1 be the singular values
of G, and λ2
n ≤... ≤λ2
1 the sorted diagonal entries of A. Prove that |σi −λi| ≤nε|λi|,
where the λi are the singular values computed with high relative accuracy.
11.9 Prove Lemma 11.1.
11.10 Let A be a symmetric matrix and consider Algorithm 10.5 with a Rayleigh
quotient shift σi = ann. Consider also the algorithm of Rayleigh quotient iteration,
Algorithm11.1,startingwith x0 = (0, ..., 0, 1)T ,whichcomputesRayleighquotients
ρi. Show that the sequences σi = ρi are the same for alli. Hint: to prove this statement
we can use the same arguments as those used to prove the connection between the
algorithm of QR iteration and the algorithm of inverse iteration.
11.11 Prove part 1 of Lemma 11.4.
11.12 Prove parts 2 and 3 of Lemma 11.4.
11.13 Let the matrix A be deﬁned as
A =
 I
B
¯BT I
	
,
(11.36)
where B is a Hermitian matrix with ∥B∥2 < 1. Prove that
κ(A) = ∥A−1∥2∥A∥2 = 1 + ∥B∥2
1 −∥B∥2
.
11.14 (Programming)
Use the MATLAB® program RayleighQuotient.m of Section1.16 to test
the Rayleigh quotient iteration algorithm, Algorithm 11.1. Try your own examples
of a symmetric matrix A and different tolerances tol.
11.15 (Programming)
Use the MATLAB® program DivideandConq.m of Section1.17 to test the
divide and conquer algorithm, Algorithm11.3.12 Try your own examples of a sym-
metric matrix A and different tolerances in Newton’s method for the solution of the
secular equation.
11.16 (Programming)
Use the MATLAB® programs of Section1.18 to test the inverse iteration algo-
rithm, Algorithm11.4. Try your own examples of a symmetric matrix A and different
tolerances tol.
12All MATLAB® programs referenced on this page can be found on the online version of Chapter 1
(doi:10.1007/978-3-319-57304-5_1).

Questions
405
11.17 (Programming)
Use the MATLAB® programs of Section1.19 to test the classical Jacobi algo-
rithm, Algorithm11.4.13 Try your own examples of a symmetric matrix A and dif-
ferent tolerances tol.
11.18 (Programming)
Use the MATLAB® programs of Section1.20 to test the SVD decomposition of
a symmetric matrix A using the one-sided Jacobi algorithm, Algorithm 11.14. Try
your own examples of a matrix A and different tolerances tol.
13All MATLAB® programs referenced on this page can be found on the online version of Chapter 1
(doi:10.1007/978-3-319-57304-5_1).

Chapter 12
Introduction to Iterative Methods
for the Solution of Linear Systems
In this chapter we will discuss iterative algorithms for the solution of linear systems
of equations (LSE) Ax = b. These algorithms are used when direct methods take a
lot of time and computer space to solve this system, or in other words, when they are
not efﬁcient.
Most of the methods presented in this chapter are described in greater detail in
[9, 23]. We also refer to the books on the iterative methods [4, 49, 100]. Parallel
implementation of many of the iterative methods discussed here is presented in [98].
The goal of this chapter is to introduce the reader to the topic of iterative algorithms.
In Sections 12.1–12.6, we will discuss basic iterative methods such as Jacobi,
Gauss–Seidel, and successive overrelaxation, and in Section12.7, we introduce
Krylov subspace methods. Further, the conjugate gradient method (CG) and pre-
conditioned conjugate gradient method (PCG) are presented in Sections 12.8, 12.9,
respectively. We refer to [4, 44, 49, 100] for a survey of Krylov subspace methods
and different preconditioning techniques.
12.1
Basic Iterative Methods
The basic iterative methods for the solution of a system of linear equations Ax = b
are:
1. Jacobi.
2. Gauss–Seidel.
3. Successive overrelaxation (SOR).
These methods produce a sequence of iterative solutions xm of a linear system
Ax = b that converge to the solution x = A−1b, provided that there exists an initial
guess x0. To use iterative methods we will introduce a splitting:
© Springer International Publishing AG 2017
L. Beilina et al., Numerical Linear Algebra: Theory and Applications,
DOI 10.1007/978-3-319-57304-5_12
407

408
12
Introduction to Iterative Methods for the Solution …
A = M −K,
where det M ̸= 0. Applying this splitting to Ax = b, we get
Ax = Mx −K x = b.
From the equation above, we have
Mx = b + K x,
and thus
x = M−1(b + K x) = M−1b + M−1K x.
Let us deﬁne
Rx = M−1K x, c = M−1b.
The iterative update for xm can be written as
xm+1 = Rxm + c,
(12.1)
where m is the number of the iteration.
Lemma 12.1 Let ∥R∥= maxx̸=0
∥Rx∥
∥x∥. If ∥R∥< 1 then the iterations (12.1) will
converge for all initial guesses x0.
Proof For exact x, we have
x = Rx + c.
(12.2)
Subtracting (12.2) from (12.1), we get
xm+1 −x = R(xm −x).
(12.3)
Taking norms, we have
∥xm+1 −x∥= ∥R(xm −x)∥≤∥R∥∥xm −x∥≤∥Rm+1∥∥x0 −x∥.
(12.4)
This inequality will converge to zero, since ∥R∥< 1.
□
Another convergence criterion requires the introduction of the deﬁnition of spec-
tral radius for R. Let (λ1, ..., λn) be the (real or complex) eigenvalues of a matrix R.
The spectral radius of R is ρ(R) ≡maxλi,i=1,...,n |λi|.
Lemma 12.2 For all operator norms, ρ(R) ≤∥R∥. Then ∀R and ∀ε > 0, there
exists an operator norm ∥· ∥(R,ε) such that ∥R∥(R,ε) ≤ρ(R) + ε.
A proof of this lemma can be found in [23].

12.1 Basic Iterative Methods
409
Theorem 12.1 Let ∥R∥= maxx̸=0
∥Rx∥
∥x∥. If ρ(R) < 1 then the iterations (12.1) will
converge for all initial guesses x0.
Proof Using (12.4), we have
∥xm+1 −x∥= ∥R(xm −x)∥≤∥R∥∥xm −x∥≤∥Rm+1∥∥x0 −x∥.
(12.5)
Since ρ(R) < 1, using Lemma 12.2 we may choose an operator norm such that
∥R∥(R,ε) < 1. Then by Lemma 12.1, the iterations (12.1) will converge for all initial
guesses x0.
□
The rate of convergencer(R) of the iterative procedure xm+1 = Rxm+c is deﬁned
as
r(R) = −log10 ρ(R).
In the iterative methods considered below, we want to have a splitting A = M −K
as efﬁcient as possible. Let us introduce the following notation. If a matrix A has no
zeros on its diagonal, we will write the splitting as
A = D −˜L −˜U = D(I −L −U),
(12.6)
where D is a diagonal matrix, −˜L is the strictly lower triangular part of A such that
DL = ˜L,
and −˜U is the strictly upper triangular part of A such that
DU = ˜U.
12.2
Jacobi Method
The splitting for the Jacobi method is
A = D −( ˜L + ˜U).
(12.7)
Applying it to the solution of Ax = b, we have
Ax = Dx −( ˜Lx + ˜Ux) = b.
From the equation above, we obtain
Dx = b + ˜Lx + ˜Ux,

410
12
Introduction to Iterative Methods for the Solution …
and thus
x = D−1(b + ˜Lx + ˜Ux) = D−1b + D−1 ˜Lx + D−1 ˜Ux.
Let us deﬁne
RJ ≡D−1( ˜L + ˜U) = L + U,
cJ ≡D−1b.
(12.8)
Then the iterative update in the Jacobi method can be written as
xm+1 = RJ xm + cR.
(12.9)
Formula (12.9) can be also written as
Dxm+1 = b + ˜Lxm + ˜Uxm,
(12.10)
or using the deﬁnition of the matrix D at the element level, the same formula can be
represented as
a j, jxm+1, j = b j −
n

k=1
k̸= j
a j,kxm,k.
(12.11)
Algorithm 12.1 One step in the Jacobi method.
for j = 1 to n
xm+1, j =
b j −n
k=1
k̸= j a j,kxm,k
a j, j
end
In the case of the model problem for the Poisson’s equation of Section 8.1.3
implemented on a square, we will have the following Jacobi method:
Algorithm 12.2 One step in the Jacobi method for the two-dimensional Poisson’s
equation.
for i = 1 to N
for j = 1 to N
um+1,i, j = um,i−1, j + um,i+1, j + um,i, j−1 + um,i, j+1 + h2 fi, j
4
end
end
Example 12.1 In this example we present the numerical solution of the Dirichlet
problem for the Poisson’s equation (8.11) in two dimensions using the iterative

12.2 Jacobi Method
411
Jacobi method. We deﬁne the right-hand side f (x) and the coefﬁcient a(x1, x2) in
(8.11) to be the same as in Example 8.2 of Chapter8. We produce the same mesh as in
this example and then solve the linear system of equations Au = f . The MATLAB®
program of Section 1.21 is available for running this test.1 We have implemented
three different versions of the Jacobi method in this program: the ﬁrst version uses
the formula (12.9), the second version employs Algorithm 12.1, and the third version
employs Algorithm 12.2. For all three algorithms we have used the stopping criterion
∥um+1 −um∥2 < tol, where the chosen tolerance was tol = 10−9.
The results of our numerical simulations are the same as those presented in Fig.8.1
for thenumber of inner points N = 20 andfor thetolerance tol = 10−9 intheiterative
update (check it by running the MATLAB® program of Section 1.21).
12.3
Gauss–Seidel Method
To get the Gauss–Seidel2 method, we use the same splitting (12.7) as for the Jacobi
method. Applying it to the solution of Ax = b, we have
Ax = Dx −( ˜Lx + ˜Ux) = b.
Next, we rearrange terms on the right-hand side of the above equation to get
Dx −˜Lx = b + ˜Ux,
(12.12)
and thus the solution of (12.12) is computed as
x = (D −˜L)−1(b + ˜Ux) = (D −˜L)−1b + (D −˜L)−1 ˜Ux.
We can rewrite the above equation using the notation DL = ˜L and DU = ˜U to
get
x = (D −˜L)−1b + (D −˜L)−1 ˜Ux
= (D −DL)−1b + (D −DL)−1 ˜Ux
= (I −L)−1D−1b + (I −L)−1D−1 ˜Ux
= (I −L)−1D−1b + (I −L)−1Ux.
(12.13)
Let us deﬁne
RGS ≡(I −L)−1U,
cGS ≡(I −L)−1D−1b.
(12.14)
Then iterative update in the Gauss–Seidel method can be written as
xm+1 = RGSxm + cGS.
(12.15)
1All MATLAB® programs referenced on this page can be found on the online version of Chapter 1
(doi:10.1007/978-3-319-57304-5_1).
2Philipp Ludwig von Seidel (1821–1896) was a German mathematician.

412
12
Introduction to Iterative Methods for the Solution …
We can also write the formula (12.13) via an iterative update as
(I −L)Dxm+1 = b + DUxm,
(12.16)
or using the deﬁnition of matrices D, L,U at the element level as
a j, jxm+1, j = b j −
j−1

k=1
a j,kxm+1,k −
n

k= j+1
a j,kxm,k.
(12.17)
Here  j−1
k=1 a j,kxm+1,k represents already updated terms with values of xm+1, and
terms n
k= j+1 a j,kxm,k are those with older values of xm that we have updated on
iteration m.
Algorithm 12.3 One step in the Gauss–Seidel method.
for j = 1 to n
xm+1, j =
b j − j−1
k=1 a j,kxm+1,k −n
k= j+1 a j,kxm,k
a j, j
end
If we want apply the Gauss–Seidel method to the solution of the model problem
for the Poisson’s equation of Section 8.13, we need to organize the ordering for the
new m + 1 variables and old already computed values m. We will use a so-called
red–black ordering based on a chessboard-like coloring. Let B nodes correspond to
the black squares on a chessboard, and R nodes correspond to the weight squares.
The Gauss–Seidel method for the solution of the two-dimensional Poisson’s equation
on a square becomes the following.
Algorithm 12.4 One step in the Gauss–Seidel method for the two-dimensional
Poisson’s equation.
for all R red nodes i, j
um+1,i, j = um,i−1, j + um,i+1, j + um,i, j−1 + um,i, j+1 + h2 fi, j
4
end
for all B black nodes i, j
um+1,i, j = um+1,i−1, j + um+1,i+1, j + um+1,i, j−1 + um+1,i, j+1 + h2 fi, j
4
end

12.3 Gauss–Seidel Method
413
Example 12.2 Here we present the numerical solution of the Dirichlet problem for
the Poisson’s equation (8.11) in two dimensions using the iterative Gauss–Seidel
method. The setup for our numerical experiments is the same as in Example 8.2
of Chapter8. The MATLAB® programs of Sections 1.22 and 1.23 are available for
running this test.3 The MATLAB® program of Section 1.22 implements Algorithm
12.3, while the MATLAB® program of Section 1.23 implements Algorithm 12.4,
the Gauss–Seidel method with red–black ordering. In both cases, we have used the
computation of the residual in the stopping criterion ∥Aum+1 −b∥2 < tol, where
the chosen tolerance was tol = 10−9.
The results of our numerical simulations are the same as in Fig.8.1 for the number
of inner points N = 20 and tolerance tol = 10−9 in the iterative update (check it by
running the MATLAB® programs of Sections 1.22, 1.23). However, the convergence
of the Gauss–Seidel method (665 iterations in the usual Gauss–Seidel method and
634 iterations in Gauss–Seidel with red–black ordering) is much faster than in the
usual Jacobi method, which converged after 1204 iterations.
12.4
Successive Overrelaxation SOR(ω) Method
The method of successive overrelaxation improves the Gauss–Seidel method in the
following way: it takes the weighted average of values xm+1 and xm such that
xm+1, j = (1 −ω)xm, j + ωxm+1, j,
(12.18)
where ω is a weight, also called a relaxation parameter. When ω = 1, then we get
the usual Gauss–Seidel method; when ω < 1, we get the underrelaxation method;
and when ω > 1, we have the overrelaxation method. We will investigate all three
cases in Section12.6.
To get the SOR(ω) method in matrix form, we again apply the splitting (12.7) and
obtain an equation similar to (12.12), but only in the iterative form
(D −˜L)xm+1 = b + ˜Uxm.
(12.19)
Applying now the weighted average (12.18) to this equation, we have
(D −ω ˜L)xm+1 = ωb + ((1 −ω)D + ω ˜U)xm.
(12.20)
Using the notation DL = ˜L and DU = ˜U, Eq. (12.20) can be rewritten as
xm+1 = (D −ω ˜L)−1ωb + (D −ω ˜L)−1((1 −ω)D + ω ˜U)xm
= (I −ωL)−1D−1ωb + (I −ωL)−1((1 −ω)I + ωU)xm.
(12.21)
Now deﬁning
3All MATLAB® programs referenced on this page can be found on the online version of Chapter 1
(doi:10.1007/978-3-319-57304-5_1).

414
12
Introduction to Iterative Methods for the Solution …
RSO R = (I −ωL)−1((1 −ω)I + ωU),
cSO R = (I −ωL)−1D−1ωb,
(12.22)
we can rewrite (12.21) in the form
xm+1 = RSO Rxm + cSO R.
(12.23)
To get SOR(ω) for implementation, we take xm+1, j on the right-hand side of
(12.18) from the Gauss–Seidel algorithm, Algorithm 12.3, and obtain the following
algorithm:
Algorithm 12.5 One step in the SOR(ω) method.
for j = 1 to n
xm+1, j = (1 −ω)xm, j + ω

b j − j−1
k=1 a j,kxm+1,k −n
k= j+1 a j,kxm,k
a j, j

end
To apply the SOR(ω) method for the solution of the model problem for the
Poisson’s equation of Section8.1.3, we will use the red–black ordering as in the
Gauss–Seidel method. The SOR(ω) method will be the following.
Algorithm 12.6 One step in the SOR(ω) method for the two-dimensional Poisson’s
equation.
for all R red nodes i, j
um+1,i, j = (1 −ω)um,i, j + ω(um,i−1, j + um,i+1, j + um,i, j−1 + um,i, j+1 + h2 fi, j)
4
end
for all B black nodes i, j
um+1,i, j = (1 −ω)um,i, j + ω(um+1,i−1, j + um+1,i+1, j + um+1,i, j−1 + um+1,i, j+1 + h2 fi, j)
4
end
12.5
Symmetric Successive Overrelaxation
SSOR(ω) Method
The main scheme of all iterative methods that we studied before was to construct an
iterative procedure xi+1 = Rxi +c such that all xi will converge to the exact solution
x of the system of linear equations Ax = b under the condition that ρ(R) < 1.

12.5 Symmetric Successive Overrelaxation SSOR(ω) Method
415
The method of symmetric successive overrelaxation constructs such a sequence of
solutions of Ax = b that improves the approximations xi already obtained. In other
words,weareinterestedintheanswertothefollowingquestion:foralreadycomputed
approximations xi, can we construct a linear combination ˆxn = n
i=0 αi,nxi whose
coefﬁcients αi,n satisfy n
i=0 αi,n = 1 and that will be a better approximation of the
exact solution x?
Suppose that we have a positive answer to this question. Then the error e = x −ˆxn
in the new computed ˆxn can be computed as
e = ˆxn −x =
n

i=0
αi,nxi −x =
n

i=0
αi,n(xi −x) =
n

i=0
αi,n Ri(x0 −x) = Pn(R)(x0 −x), (12.24)
where Pn(R) = n
i=0 αi,n Ri is a polynomial of degree n such that Pn(1) =
n
i=0 αi,n = 1. The classical Chebyshev polynomials usually satisfy this condition.
The Chebyshev polynomials of the ﬁrst kind are deﬁned by the recurrence formula
T0(x) = 1,
T1(x) = x,
Tn+1(x) = 2xTn(x) −Tn−1(x).
(12.25)
Properties of the Chebyshev polynomials are listed in [109]. We are going to construct
a polynomial of the form
Pn(x) = Tn(x/ρ)
Tn(1/ρ)
(12.26)
such that the spectral radius of Pn(R) is as small as possible. The polynomial Pn(x)
in (12.26) has the following properties:
Pn(1) = 1,
max
−ρ(R)<x<ρ(R) |Pn(x)| < ε
(12.27)
for small tolerances ε > 0. By the spectral mapping theorem (since Pn(R) =
n
i=0 αi,n Ri is a polynomial of degree n), we have that the eigenvalues of Pn(R)
are Pn(λ(R)), where λ are the eigenvalues of R. Combining this observation with
(12.27), we can conclude that the spectral radius of R will be small, and thus poly-
nomials of the form (12.26) are suitable for our purposes.
Let us deﬁne now
μn :=
1
Tn(1/ρ).
(12.28)
Then (12.26) can be written for x = R in terms of μn as
Pn(R) = μnTn(R/ρ).
(12.29)

416
12
Introduction to Iterative Methods for the Solution …
Writing (12.25) for x = 1/ρ and using (12.28), we get
1
μn
= Tn(1/ρ) = 2/ρTn−1(1/ρ) −Tn−2(1/ρ).
(12.30)
Writing (12.25) for x = R/ρ, we have
Tn(R/ρ) = 2 R
ρ Tn−1(R/ρ) −Tn−2(R/ρ).
(12.31)
We now substitute Eqs.(12.29), (12.30) into the error Eq.(12.24) to obtain
e = ˆxn −x = Pn(R)(x0 −x) = μnTn
 R
ρ

(x0 −x)
= μn
2 R
ρ Tn−1(R/ρ)(x0 −x) −Tn−2(R/ρ)(x0 −x)

.
(12.32)
Writing (12.28) for x = R and different indices n, we get
Pn−1(R) = μn−1Tn−1(R/ρ),
Pn−2(R) = μn−2Tn−2(R/ρ).
(12.33)
We use Eq.(12.33) in the last row of (12.32) to obtain
e = ˆxn −x = μn
2 R
ρ
Pn−1(R/ρ)(x0 −x)
μn−1
−Pn−2(R/ρ)(x0 −x)
μn−2

.
(12.34)
Using (12.24), we can write the error for different indices n as
e = ˆxn−1 −x = Pn−1(R)(x0 −x),
e = ˆxn−2 −x = Pn−2(R)(x0 −x).
(12.35)
Substituting (12.35) in (12.34), we have
e = ˆxn −x = μn
2 R
ρ
ˆxn−1 −x
μn−1
−ˆxn−2 −x
μn−2

= μn
2 R
ρ
ˆxn−1
μn−1
−μn
2 R
ρ
x
μn−1
−μn
ˆxn−2
μn−2
+ μn
x
μn−2
= μn
2 R
ρ
ˆxn−1
μn−1
−μn
ˆxn−2
μn−2
+ C1(x),
(12.36)

12.5 Symmetric Successive Overrelaxation SSOR(ω) Method
417
where
C1(x) = μn
x
μn−2
−μn
2 R
ρ
x
μn−1
.
(12.37)
Adding x to both parts of (12.36), we get
ˆxn = μn
2 R
ρ
ˆxn−1
μn−1
−μn
ˆxn−2
μn−2
+ C2(x),
(12.38)
where
C2(x) = x + μn
x
μn−2
−μn
2 R
ρ
x
μn−1
.
(12.39)
Further, since for the exact value of x we have x = Rx+c and thus R = (x−c)/x,
the function C2(x) can be written as
C2(x) = x + μn
x
μn−2
−2 (x −c)
ρ
μn
μn−1
= xμn
 1
μn
+
1
μn−2
−
2
ρμn−1

+ 2μnc
ρμn−1
.
(12.40)
Since by (12.28) and (12.25) we have
1
μn
=
2
ρμn−1
−
1
μn−2
,
(12.41)
it follows that (12.40) can be simpliﬁed to
C2(x) = 2μnc
ρμn−1
.
(12.42)
Combining(12.38),(12.41),and (12.42),wecanformulatethefollowingaccelerating
algorithm for iterations xi+1 = Rxi + c.
Algorithm 12.7 Chebyshev acceleration algorithm.
Step 0: Initialization:
set N, ε, μ0 = 1, μ1 = ρ(R), ˆx0 = x0; ˆx1 = Rx1 + c.
Step 1: for n = 1 to N
μn =
1
2
ρ(R)μn−1 −
1
μn−2
ˆxn = μn
2 R
ρ(R)μn−1 ˆxn−1 −
μn
μn−2 ˆxn−2 +
2μnc
ρ(R)μn−1
if ∥ˆxn −ˆxn−1∥< ε quit
else set n := n + 1 and go to Step 1.
end
Algorithm 12.7 requires that the matrix R has only real eigenvalues. Thus, this
algorithm cannot be applied to SO R(ω), since the matrix RSO R deﬁned in (12.22)

418
12
Introduction to Iterative Methods for the Solution …
can have complex eigenvalues. However, if we write the iterations in SO R(ω) as
xi+1 = ˆRxi + c with a matrix ˆR that has real eigenvalues, then Algorithm 12.7 can
be used.
Assume that we have a symmetric matrix A such that A = D(I −L −U) with
U = LT . Recall now the iterations in (12.21) in SO R(ω) and let us write them in
two steps:
• Step 1:
xnew
i
= (I −ωL)−1D−1ωb + (I −ωL)−1((1 −ω)I + ωU)xi
= (I −ωL)−1((1 −ω)I + ωU)xi + const. := ˆLxi + const.
(12.43)
• Step 2:
xi+1 = (I −ωU)−1((1 −ω)I + ωL)xnew
i
+ const. := ˆUxnew
i
+ const.
(12.44)
Substituting (12.43) into (12.44), we get iterations xi+1 = ˆRxi + c, where the
matrix ˆR := ˆL ˆU has only real eigenvalues, since the similar symmetric matrix
(I −ωU) ˆR(I −ωU)−1 has real eigenvalues:
(I −ωU) ˆR(I −ωU)−1 = I + (2 −ω)2(I −ωL)−1(I −ωU)−1
+ (ω −2)(I −ωU)−1 + (ω −2)(I −ωL)−1
= I + (2 −ω)2(I −ωL)−1(I −ωLT )−1
+ (ω −2)(I −ωLT )−1 + (ω −2)(I −ωL)−1.
(12.45)
Thus, we can formulate the following symmetric successive overrelaxation
SSOR(ω) algorithm for iterations xi+1 = ˆRxi + c.
Algorithm 12.8 SSOR(ω) algorithm for xi+1 = ˜Rxi + c
Step 1.
Apply one step of the SOR(ω) algorithm, Algorithm 12.5, for j = 1
to n to compute xi,1, ..., xi,n.
Step 2.
Apply one step of Algorithm 12.5 backward for j = n to 1 to compute
xi,n, ..., xi,1.
12.6
Convergence of Main Iterative Methods
Theorem 12.2 If the matrix A is strictly row diagonally dominant (i.e., such that
|aii| > 
i̸= j |ai j|), then the Jacobi and Gauss–Seidel methods converge such that
∥RGS∥∞< ∥RJ∥∞< 1,
(12.46)

12.6 Convergence of Main Iterative Methods
419
where RGS and RJ are deﬁned in (12.14), (12.8), respectively.
Proof We can rewrite (12.46) with e = (1, ..., 1)T as
∥RGS∥∞= ∥|RGS| e∥∞< ∥RJ∥∞= ∥|RJ| e∥∞< 1.
(12.47)
Using deﬁnitions (12.14) and (12.8), we can get from the above inequality,
∥|RGS| e∥∞= ∥|(I −L)−1U| e∥∞< ∥|RJ| e∥∞= ∥|L +U| e∥∞< 1.
(12.48)
Further, the triangle inequality along with the facts that Ln = 0 and (I −|L|)−1 ≈
n−1
i=0 |L|i imply that
|(I −L)−1U| e ≤|(I −L)−1| |U| e ≈|
n−1

i=0
Li| |U| e
≤
n−1

i=0
|L|i |U| e ≈(I −|L|)−1|U| e.
(12.49)
Using the assumption
∥RJ∥∞= ρ < 1
(12.50)
together with the fact that all entries of (I −|L|)−1 ≈n−1
i=0 |L|i are positive, we
have that
0 ≤(I −|L| −|U|) e.
(12.51)
By assumption (12.50), we have
|RJ| e = (|L| + |U|) e ≤e.
Next, multiplying (12.51) by |L|, we obtain
0 ≤|L|(I −|L| −|U|) e = (|L| −|L|2 −|L||U|) e.
Then adding |U| e to both sides of the above inequality, we get
|U| e ≤(I −|L|)(|L| + |U|) e = (|L| −|L|2 −|L||U| + |U|) e,
from which follows (12.47) and thus (12.46).
□
Theorem 12.3 Let the spectral radius of RSO R be such that ρ(RSO R) ≥|ω −1|.
Then 0 < ω < 2 is required for convergence of SOR(ω).

420
12
Introduction to Iterative Methods for the Solution …
Proof We write the characteristic polynomial for RSO R as
ϕ(λ) = det(λI −RSO R) = det((I −ωL)(λI −RSO R))
= det((λ + ω −1)I −ωλL −ωU).
(12.52)
From the equation above, we have
ϕ(0) = ±

λi(RSO R) = ± det((ω −1)I) = ±(ω −1)n,
and thus
max
i
|λi(RSO R)| ≥|ω −1|,
from which it follows that ρ(RSO R) ≥|ω −1|.
□
Theorem 12.4 If A is an s.p.d. matrix, then ρ(RSO R) < 1 for all 0 < ω < 2, and
thus SOR converges for all 0 < ω < 2. If we choose ω = 1, then we obtain the usual
Gauss–Seidel method, which also converges.
A proof of this theorem can be found in [23].
Assume that for every matrix M that can be written as
M = D −˜L −˜U,
RJ(α) is the matrix that is deﬁned as
RJ(α) = αD−1 ˜L + 1
α D−1 ˜U
for every scalar α > 0. The matrix M is said to be consistently ordered if the
eigenvalues of RJ(α) are independent of α.
Theorem 12.5 Assume that A is consistently ordered and ω ̸= 0. Then the following
statements are true:
1. The eigenvalues of RJ appear in pairs with positive and negative signs.
2. Assume that ˜λ is an eigenvalue of R j and the following equation is true:
(λ + ω −1)2 = λω2˜λ2.
(12.53)
Then λ is an eigenvalue of RSO R.
3. If λ ̸= 0 is an eigenvalue of RSO R, then ˜λ in (12.53) is an eigenvalue of R j.
Proof 1. Since A is consistently ordered, RJ(1) = RJ and RJ(−1) = −RJ have
the same eigenvalues and thus appear in ± pairs.
2. Assume that λ = 0. Then from (12.53), we have

12.6 Convergence of Main Iterative Methods
421
(ω −1)2 = 0.
(12.54)
We see that ω = 1 and RSO R(1) = RGS = (I −L)−1U. Thus, λ = 0 is an
eigenvalue of RSO R. Otherwise, we can write
0 = det(λI −RSO R) = det((I −ωL)(λI −RSO R))
= det((λ + ω −1)I −ωλL −ωU)
= det
	√
λω
		λ + ω −1
√
λω

I −
√
λL −1
√
λ
U


= det
		λ + ω −1
√
λω

I −L −U

(
√
λω)n.
(12.55)
Deﬁning
˜λ = λ + ω −1
√
λω
,
(12.56)
we see that it is an eigenvalue of L +U = RJ, and thus equation (12.53) is valid.
3. If λ ̸= 0, then we use the previous proof in the opposite direction.
□
From Theorem 12.5 it follows that if A is consistently ordered, then ρ(RGS) =
ρ(RJ)2. We can see this from (12.53), since for ω = 1, we have the Gauss–Seidel
method, and thus (12.53) can be written as
λ2 = λ˜λ2,
(12.57)
or
λ = ˜λ2.
(12.58)
Thus, ρ(RGS) = ρ(RJ)2, and the Gauss–Seidel method is twice as fast as the
Jacobi method.
Theorem 12.6 Assume that A is consistently ordered and that RJ has real eigen-
values such that ˜λ = ρ(RJ) < 1. Then following statements are true:
• The optimal relaxation parameter ωopt in SOR(ω) can be computed as
ωopt =
2
1 +

1 −˜λ2 .
(12.59)
• The spectral radius ρ(RSO R(ωopt)) for ωopt deﬁned by (12.59) can be computed as
ρ(RSO R(ωopt)) = ωopt −1 =
˜λ2
(1 +

1 −˜λ2)2 .
(12.60)

422
12
Introduction to Iterative Methods for the Solution …
• The spectral radius ρ(RSO R(ω)) for 0 < ω < 2 can be computed as
ρ(RSO R(ω)) =

1 −ω + 1
2ω2˜λ2 + ω˜λ

1 −ω + 0.25ω2˜λ2, if ω ∈(0, ωopt),
ω −1,
if ω ∈[ωopt, 2).
(12.61)
Proof The proof follows from the solution of equation (12.53) for λ.
□
Example 12.3 The matrix A in the model problem for the Poisson’s equation of
Section8.1.3 is s.p.d. Thus by Theorem 12.4, the values of SOR(ω) for this problem
will converge for all 0 < ω < 2.
We present the numerical solution of the Dirichlet problem for the Poisson’s equa-
tion (8.11) in two dimensions using the iterative SOR(ω) method. The setup for our
numerical simulations is the same as in Example 8.2 of Chapter8. The MATLAB®
program of Section 1.24 is available for running this test.4 This program implements
two algorithms: the ﬁrst is Algorithm 12.5, and the second is Algorithm 12.6, or
the SOR(ω) method with red–black ordering. We have used the stopping criterion
∥um+1 −um∥2 < tol, with tolerance tol = 10−9.
The results of our numerical simulations are the same as in Fig.8.1 for the number
of inner points N = 20 and for the tolerance tol = 10−9 in the iterative update (check
0
0.2
0.4
0.6
0.8
1
x 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x 2
solution u(x 1,x2) in SOR  method ,  N = 60, iter. = 282
0
1
2
3
4
5
× 10 -3
0
1
2
1
× 10 -3
u(x 1,x 2)
4
solution u(x 1,x2) in SOR method,  N = 60, iter. = 282
x 2
0.5
x 1
6
0.5
0
0
0
1
2
3
4
5
× 10 -3
10
1
11
1
a(x 1,x 2)
12
coefficient a(x
1,x2) with A = 12
x 2
0.5
x 1
13
0.5
0
0
0.75
1
0.8
0.85
1
f(x 1,x 2)
0.9
f(x 1,x2) with A f = 1
x 2
0.95
0.5
x 1
1
0.5
0
0
Fig. 12.1 Solution of problem (8.11) in the example of Section8.1.3 on the unit square with mesh
60 × 60 points.
4The
MATLAB®
programs
can
be
found
on
the
online
version
of
Chapter
1
(doi:10.1007/978-3-319-57304-5_1).

12.6 Convergence of Main Iterative Methods
423
1
1.2
1.4
1.6
1.8
2
Relaxation parameter 
ω
0
100
200
300
400
500
600
Number of iterations in SOR
Mesh:  20 by 20 points
SOR(ω)
Computed optimal 
ω
1
1.2
1.4
1.6
1.8
2
Relaxation parameter 
ω
0.75
0.8
0.85
0.9
0.95
1
 Spectral radius ρ(RSOR(ω))
Mesh:  20 by 20 points
ρ(RSOR(ω))
1
1.2
1.4
1.6
1.8
2
Relaxation parameter 
ω
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
Number of iterations in SOR
Mesh:  60 by 60 points
SOR(ω)
Computed optimal 
ω
1
1.2
1.4
1.6
1.8
2
Relaxation parameter 
ω
0.91
0.92
0.93
0.94
0.95
0.96
0.97
0.98
0.99
1
 Spectral radius ρ(RSOR(ω))
Mesh:  60 by 60 points
ρ(RSOR(ω))
Fig. 12.2 Convergence of SOR(ω) computed for the model problem (8.11) in the example of
Section8.1.3 on the unit square for different discretizations. In the left-hand ﬁgures, we plot the
number of iterations in SOR depending on ω (blue line) and corresponding optimal relaxation
parameter ωopt (red line). The computed spectral radius ρ(RSO R(ω)) is presented in the right-hand
ﬁgures.
it by running the MATLAB® programs of Section 1.24).5 Figure12.1 presents the
numerical solution for the number of inner points N = 60, relaxation parameter
ω = 1.95, and tolerance tol = 10−9.
We perform computations for different relaxation parameters ω ∈(0, 2). We
also compute the optimal relaxation parameter ωopt, as well as the spectral radius
ρ(RSO R(ω)) using formulas (12.59), (12.61), respectively. Figure12.2 presents the
results of these computations. The left-hand pictures of Fig.12.2 show the conver-
gence of SOR(ω) for different ω ∈(0, 2) (blue line) and corresponding optimal
relaxation parameter ωopt computed by (12.59) (red line). The right-hand pictures of
Fig.12.2 show the spectral radius ρ(RSO R(ω)) computed by (12.61) with ωopt imple-
mented by (12.59) and ˜λ given by (12.56). From this ﬁgure, we also conclude that
the convergence of SOR(ω) is much faster than that of the Gauss–Seidel method and
the Jacobi method.
5The
MATLAB®
programs
can
be
found
on
the
online
version
of
Chapter
1
(doi:10.1007/978-3-319-57304-5_1).

424
12
Introduction to Iterative Methods for the Solution …
12.7
Krylov Subspace Methods
Krylov6 subspace methods are used for the solution of large systems of linear equa-
tions Ax = b andforﬁndingeigenvaluesof A avoidingmatrix–matrixmultiplication.
Instead, these methods use matrix–vector multiplication.
The Krylov subspace generated by the n × n matrix A and vector b of length n is
the linear subspace spanned by powers of A multiplied by b:
Kr(A, b) = span{b, Ab, A2b, ..., Ar−1b}.
(12.62)
For a symmetric matrix A, we can write the decomposition QT AQ = H, where
Q is an orthogonal transformation and H is an upper Hessenberg matrix, which
also will be a lower Hessenberg matrix and thus a tridiagonal matrix. Writing Q as
Q = {q1, ..., qn} and using AQ = QH, we have
Aq j =
j+1

i=1
hi, jqi.
(12.63)
Multiplying both sides of the above expression by orthonormal vectors qT
m and
using the fact that the qi are orthonormal, we obtain
qT
m Aq j =
j+1

i=1
hi, jqT
mqi = hm, j, 1 ≤m ≤j.
(12.64)
We can rewrite (12.63) as
h j+1, jq j+1 = Aq j −
j

i=1
hi, jqi.
(12.65)
The two formulas (12.64) and (12.65) are used in the Arnoldi7 algorithm for the
reduction of a matrix A to upper Hessenberg form. Let r be the number of columns
in the matrices Q and H that we need to compute. We now formulate the Arnoldi
algorithm, which performs partial reduction of the matrix A to upper Hessenberg
form. The vectors q j computed in this algorithm are called Arnoldi vectors.
Algorithm 12.9 Arnoldi algorithm.
Initialization: q1 =
b
∥b∥2
for j = 1 to r
z = Aq j
for i = 1 to j
6Aleksey Krylov (1863–1945) was a Russian naval engineer, applied mathematician, and memoirist.
7Walter Edwin Arnoldi (1917–1995) was an American engineer.

12.7 Krylov Subspace Methods
425
hi, j = qT
i z
z = z −hi, jqi
end
h j+1, j = ∥z∥2
if h j+1, j = 0 quit
qi+1 =
z
h j+1, j
end
Let us deﬁne Q = (Qr, Qu) with Qr = (q1, ..., qr) and Qu = (qr+1, ..., qn). We
have the following structure of the matrix H after r steps of the Arnoldi algorithm,
Algorithm 12.9:
H = QT AQ = (Qr, Qu)T A (Qr, Qu)
=
 QT
r AQr QT
r AQu
QT
u AQr QT
r AQr

=
 Hr Hur
Hru Hu

,
(12.66)
where Hr is an upper Hessenberg matrix. We know only Hr and Hru, with Hur and
Hu still unknown.
For the case of a symmetric matrix A, the Arnoldi algorithm can be simpliﬁed,
since the matrix H is symmetric and tridiagonal, which means that
H =
⎛
⎜⎜⎜⎜⎝
α1 β1
...
...
β1 α2
...
...
... ...
...
...
... ... αn−1 βn−1
... ... βn−1 αn
⎞
⎟⎟⎟⎟⎠
.
(12.67)
Rewriting (12.63) for the case of the symmetric and tridiagonal H given by (12.67),
we have
Aq j = β j−1q j−1 + α jq j + β jq j+1.
(12.68)
We note that the columns of Q are orthonormal. Thus, multiplying (12.68) by qT
j ,
we get
qT
j Aq j = qT
j (β j−1q j−1 + α jq j + β jq j+1) = α j.
(12.69)
From (12.68), we can obtain an expression for computing q j+1,
q j+1 = (Aq j −β j−1q j−1 −α jq j)/β j,
(12.70)
which is used in the Lanczos8 algorithm. Combining (12.69) and (12.70), we get
the Lanczos algorithm for partial reduction of a symmetric matrix A to symmetric
tridiagonal form.
8Cornelius Lanczos (1893–1974) was a Hungarian mathematician and physicist.

426
12
Introduction to Iterative Methods for the Solution …
Algorithm 12.10 Lanczos algorithm.
Initialization: q1 =
b
∥b∥2 , β0 = 0, q0 = 0
for j = 1 to r
z = Aq j
α j = qT
j z
z = z −α jq j −β j−1q j−1
/* no reorthogonalization */
β j = ∥z∥2
if β j = 0 quit
qi+1 =
z
β j
end
The vectors q j computed by Algorithm 12.10 are called Lanczos vectors. The
vectors qr computed in the Lanczos or Arnoldi algorithm create an orthonormal
basis of the Krylov subspace Kr deﬁned in (12.62). The matrix Hr = QT
r AQr in
both algorithms is called the projection of A onto the Krylov subspace Kr.
Taking into account (12.66), we can write the following structure of the matrix T
after r steps of the Lanczos algorithm, Algorithm 12.10:
T = QT AQ = (Qr, Qu)T A (Qr, Qu)
=
 QT
r AQr QT
r AQu
QT
u AQr QT
r AQr

=
 Tr Tur
Tru Tu

=
 Tr T T
ru
Tru Tu

.
(12.71)
We can compute the elements of Tr and Tru = T T
ur, since the matrix A is symmetric.
However, the elements of Tu are not known.
Our goal now is to use r steps in the Lanczos or Arnoldi algorithm to solve the
linear system Ax = b. To do so, we seek the best approximation xr to the exact
solution x = A−1b given by
xr =
r

j=1
z jq j = Qrz,
(12.72)
where z = (z1, ..., zr)T . Let us deﬁne the residual as Rr = b −Axr. For the case of
an s.p.d. matrix A, we can deﬁne the norm ∥R∥A−1 := (RT A−1R)1/2. We note that
∥R∥A−1 = ∥xr −x∥A. Thus, the best computed solution xr will minimize ∥R∥A−1.
An algorithm that can compute such a vector xr is called the conjugate gradient
algorithm (CG).
Theorem 12.7 Let A be a symmetric matrix, Hr = QT
r AQr, and let the residuals
be deﬁned as Rr = b −Axr∀xr ∈Kr. When Hr is nonsingular, we can deﬁne
xr = Qr H −1
r
e1∥b∥2,
(12.73)
where e1 = (1, 0, ..., 0)T . Then QT
r Rr = 0.

12.7 Krylov Subspace Methods
427
Let A be also a positive deﬁnite matrix. Then Hr must be nonsingular, and xr
deﬁned as in (12.73) minimizes ∥Rr∥A−1 for all xr ∈Kr, where Rr = ±∥Rr∥2qr+1.
A proof of this theorem can be found in [23]. Numerical exploration of the con-
vergence of the Lanczos algorithm, Algorithm 12.10, is provided in [23]. We note
that rounding errors destroy the orthogonalization property of the Lanczos algorithm
12.10: the vectors qi can lose orthogonality and become linearly dependent. Below,
we present a more expensive algorithm with full orthogonalization for ﬁnding eigen-
values and eigenvectors of a symmetric matrix A. We say that the algorithm carries
out full orthogonalization because we perform the Gram–Schmidt orthogonaliza-
tion process twice to be sure that we make z orthogonal to all q1, ..., q j−1; see the
discussion in [94].
Algorithm 12.11 Lanczos algorithm with orthogonalization.
Initialization: q1 =
b
∥b∥2 , β0 = 0, q0 = 0
for j = 1 to r
z = Aq j
α j = qT
j z
z = z − j−1
i=1 (zT qi)qi
/* carry out reorthogonalization twice */
z = z − j−1
i=1 (zT qi)qi
β j = ∥z∥2
if β j = 0 quit
qi+1 =
z
β j
end
We can see that the r steps of the Lanczos algorithm with full orthogonaliza-
tion, Algorithm 12.11, takes O(r2n) FLOPS compared with O(rn) FLOPS for the
Lanczos algorithm, Algorithm 12.10. The selective reorthogonalization process takes
advantages of both algorithms and makes Lanczos vectors nearly orthogonal sufﬁ-
ciently cheaply; see details in [23]. To formulate the Lanczos algorithm with selec-
tive reorthogonalization, we deﬁne the Rayleigh–Ritz9 procedure. In this procedure,
the eigenvalues of A are approximated by the eigenvalues of Tr = QT
r ΛQr with
Tr deﬁned in (12.71), and are called Ritz values. Let us deﬁne Tr = V ΛV T , the
eigendecomposition of Tr. The Ritz vectors are the columns of QrV , which are also
eigenvector approximations of the eigenvectors of Tr.
The next theorem provides a criterion for selective orthogonalization of Lanczos
vectors.
Theorem 12.8 (Paige10). Let the jth step of the Lanczos algorithm 12.10 be written
as
β jq j+1 + f j = Aq j −α jq j −β j−1q j−1,
(12.74)
9Walther Ritz (1878–1909) was a Swiss theoretical physicist.
10Constantin Marie Le Paige (1852–1929) was a Belgian mathematician.

428
12
Introduction to Iterative Methods for the Solution …
where f j is the rounding error and ∥f ∥2 ≤O(ε∥A∥) with ε represents the machine
epsilon. Let Tr = V ΛV T be the eigendecomposition of Tr with orthogonal V =
(v1, ..., vr), and Λ = diag(Θ1, ..., Θr). Let the columns yi,r = Qrvi of QrV with
orthogonal Q = (q1, ..., qr) be the Ritz vectors, with the Θi the Ritz values. Then
yT
i,rqr+1 = O(ε∥A∥)
βr|vi(r)| ,
where vi(r) is the rth entry of vi, i = 1, ...,r, and βr = ∥Tru∥2 with Tru deﬁned in
(12.71).
A proof of this theorem is given in [23].
Using Paige’s theorem, Theorem 12.8, we can design the simplest version of
the Lanczos algorithm with selective orthogonalization. In this algorithm we check
valuesofβr|vi(r)|ateverystepofthealgorithm,andthenforsmallvaluesofβr|vi(r)|,
we orthogonalize the values of the vector z.
Algorithm 12.12 Lanczos algorithm with selective orthogonalization.
Initialization: q1 =
b
∥b∥2 , β0 = 0, q0 = 0
for j = 1 to r
z = Aq j
α j = qT
j z
z = z −α jq j −β j−1q j−1
/* no reorthogonalization */
for i ≤r
if βr|vi(r)| ≤√ε∥Tr∥
yi,r = Qrvi
/* Ritz vectors */
z = z −(yT
i,rz)yi,r
/* selective reorthogonalization */
end if
end for
β j = ∥z∥2
if β j = 0 quit
qi+1 =
z
β j
end
Algorithm 12.12 can be improved using a recurrence formula given in [107],
since it is not necessary that the condition βr|vi(r)| ≤√ε∥Tr∥be checked at every
iteration. Thus, many steps in selective reorthogonalization can be eliminated. See
also [40], where a shifted block Lanczos algorithm with standard implementation is
presented.
However, when the matrix A is asymmetric, then all the Lanczos algorithms
considered above are no longer valid. This is because the eigenvalues of A can be
complex or badly conditioned. We refer to [5, 6, 20, 100, 102] for the theory and
implementation of the Lanczos algorithm for asymmetric matrices.

12.8 Conjugate Gradient Method
429
12.8
Conjugate Gradient Method
In this section we will present the main steps in the derivation of the conjugate
gradient (CG) algorithm. We will begin by using the Lanczos algorithm, Algorithm
12.10, and then combine it with formula (12.73). Then using Theorem 12.7, we will
conclude that the residuals Rr = b −Axr are parallel to the Lanczos vectors qr+1.
Let us introduce conjugate gradient vectors pr. They are called gradients because
in a single step of the CG algorithm we compute the approximate solution as xr =
xr−1 + νpr with some scalars ν (see Algorithm 12.13), and this solution minimizes
the residual norm ∥Rr∥A−1 = (RT
r A−1Rr)1/2. The vectors pr are called conjugate,
or more precisely A-conjugate, because pT
r Ap j = 0 if j ̸= r.
Since A is symmetric positive deﬁnite, the matrix Hr = QT
r AQr is also symmetric
positive deﬁnite. Thus, we can use Cholesky decomposition on Hr to get
Hr = ˆLr ˆLT
r = Lr Dr LT
r ,
(12.75)
where Lr is unit lower bidiagonal and Dr is diagonal. Then using the formula (12.73),
we get
xr = Qr H −1
r
e1||b||2
= Qr(L−T
r
D−1
r L−1
r )e1||b||2
= (Qr L−T
r
)(D−1
r L−1
r e1||b||2)
≡( ˜Pr)(yr),
where ˜Pr ≡Qr L−T
r
and yr ≡D−1
r L−1
r e1||b||2. Let ˜Pr = ( ˜p1, . . . , ˜pr). The conju-
gate gradients pi will be parallel to the columns ˜pi of ˜Pr.
Lemma 12.3 The columns pi of the matrix ˜Pr are A-conjugate. This means that
˜PT
r A ˜Pr is diagonal.
Proof We can write
˜PT
r A ˜Pr = (Qr L−T
r
)A(Qr L−T
r
) = L−1
r (QT
r AQr)L−T
r
= L−1
r (Hr)L−T
r
= L−1
r (Lr Dr LT
r )L−T
r
= Dr.
□
Let us deﬁne the following iterative update for xr:
xr = ˜Pr yr = ( ˜Pr−1, ˜pr)
 yr−1
ηr

= ˜Pr−1yr−1 + ˜prηr = xr−1 + ˜prηr.
(12.76)
To use this formula, we need to compute the scalars ηr. Since Hr−1 is the leading
(r −1)×(r −1) submatrix of Hr, Lr−1 and Dr−1 are also the leading (r −1)×(r −1)
submatrices of Lr and Dr, respectively. Thus, we can write

430
12
Introduction to Iterative Methods for the Solution …
Hr =
⎛
⎜⎜⎜⎜⎝
α1 β1
β1
... ...
... ... βr−1
βr−1 αr
⎞
⎟⎟⎟⎟⎠
= Lr Dr LT
r =
⎛
⎜⎜⎜⎜⎝
1
l1
...
... ...
lr−1 1
⎞
⎟⎟⎟⎟⎠
⎛
⎜⎜⎜⎝
d1
...
dr−1
dr
⎞
⎟⎟⎟⎠
⎛
⎜⎜⎜⎜⎝
1
l1
...
... ...
lr−1 1
⎞
⎟⎟⎟⎟⎠
T
=

Lr−1
lr−1 ˜eT
r−1 1

diag(Dr−1, dr)

Lr−1
lr−1 ˆeT
r−1 1
T
,
where ˆeT
r−1 = (0, . . . , 0, 1) and dim ˆeT
r−1 = r −1.
We see also that D−1
r−1 and L−1
r−1 are the leading (r −l) × (r −1) submatrices of
the matrices D−1
r
= diag(D−1
r−1, d−1
r ) and
L−1
r
=

L−1
r−1 ...
...
1

,
respectively. Below we show that the vector deﬁned by yr−1 = D−1
r−1L−1
r−1 ˆe1||b||2,
where dim ˆe1 = r −1, is the same as the leading r −1 components of the vector yr.
We have
yr = D−1
r L−1
r e1||b||2 =

D−1
r−1
d−1
r
 
L−1
r−1 ...
...
1

e1||b||2
=

D−1
r−1L−1
r−1 ˆe1||b||2
ηr

=
 yr−1
ηr

.
To use formula (12.76) for computing xr, we need to derive iterative formulas for
the columns of ˜Pr = ( ˜p1, . . . , ˜pr) and ηr.
We begin ﬁrst to derive ˜Pr = ( ˜p1, . . . , ˜pr). We observe that LT
r−1 is upper tri-
angular, and thus the matrix L−T
r−1 is also upper triangular, and it forms the leading
(r −l) × (r −1) submatrix of L−T
r
. Therefore, ˜Pr−1 is identical to the leading r −1
columns of ˜Pr:
˜Pr = Qr L−T
r
= (Qr−1, qr)

L−T
r−1 ...
0
1

= (Qr−1L−T
r−1, ˜pr) = ( ˜Pr−1, ˜pr).
From ˜Pr = Qr L−T
r
, we get

12.8 Conjugate Gradient Method
431
˜Pr LT
r = Qr.
Now equating the rth columns on both sides of the above equation, we obtain an
iterative formula for updating ˜pr:
˜pr = qr −lr−1 ˜pr−1.
(12.77)
The iterative formulas (12.77) for updating ˜pr, qr (from the Lanczos algorithm)
and ﬁnally (12.76) for computing xr provide the main steps in the CG algorithm. We
will additionally simplify these formulas to obtain the ultimate CG algorithm. By
Theorem 12.7, the residuals Rr and vectors qr+1 are parallel. Thus, we can replace
the Lanczos vectors qr+1 with the residuals Rr = b −Axr. We now multiply both
sides of xr = xr−1 + ηr ˜pr by A and subtract from b to obtain
Rr = b −Axr = b −A(xr−1 + ηr ˜pr) = Rr−1 −ηr A ˜pr.
(12.78)
The above formula yields the following iterative updates:
Rr = Rr−1 −ηr A ˜pr;
(12.79)
from (12.76) we get
xr = xr−1 + ηr ˜pr;
(12.80)
from (12.77) we get
˜pr = qr −lr−1 ˜pr−1.
(12.81)
The next step is to eliminate qr. To do this, we substitute Rr−1 = qr||Rr−1||2 and
pr ≡||Rr−1||2 ˜pr into (12.79)–(12.81) to get
Rr = Rr−1 −
ηr
||Rr−1||2
Apr ≡Rr−1 −νr Apr,
(12.82)
xr = xr−1 +
ηr
||Rr−1||2
pr ≡xr−1 + νr pr,
(12.83)
pr = Rr−1 −||Rr−1||2lr−1
||Rr−2||2
pr−1 ≡Rr−1 + μr pr−1.
(12.84)
In analyzing (12.82)–(12.84), we observe that we need formulas for the scalars
νr and μr. For derivation of νr, we multiply both sides of (12.84) on the left by pT
r A
and use Lemma 12.3 to get
pT
r Apr = pT
r ARr−1 + 0 = RT
r−1 Apr.
(12.85)

432
12
Introduction to Iterative Methods for the Solution …
Multiplying both sides of (12.82) on the left by RT
r−1 and using the equality RT
r−1
Rr = 0 (since the Ri are parallel to the columns of the orthogonal matrix Q) and
then (12.85), we obtain
νr = RT
r−1Rr−1
RT
r−1 Apr
= RT
r−1Rr−1
pTr Apr
.
(12.86)
Finally, we derive a formula for μr. Multiplying both sides of (12.84) on the left
by pT
r−1 A and using Lemma 12.3 (by this lemma, pr and pr−1 are A-conjugate), we
obtain
μr = −pT
r−1 ARr−1
pT
r−1Apr−1
.
(12.87)
We can derive alternative formulas for νr and μr. Multiplying both sides of (12.82)
on the left by RT
r , using that RT
r−1Rr = 0, and solving for νr, we get
νr = −RT
r Rr
RTr Apr
.
(12.88)
Equating (12.86) and (12.88) for νr−1 and comparing with Eq.(12.87) yields a
different formula for computing μr:
μr = −pT
r−1 ARr−1
pT
r−1Apr−1
= RT
r−1Rr−1
RT
r−2Rr−2
.
(12.89)
Combining (12.82), (12.83), (12.84), (12.86), and (12.89) yields the conjugate
gradient algorithm.
Algorithm 12.13 Conjugate gradient algorithm.
Initialization: r = 0;
x0 = 0;
R0 = b;
p1 = b;
repeat
r = r + 1
z = A pr
νr = (RT
r−1Rr−1)/(pT
r z)
xr = xr−1 + νr pr
Rr = Rr−1 −νrz
μr+1 = (RT
r Rr)/(RT
r−1Rr−1)
pr+1 = Rr + μr+1 pr
until ||Rr||2 is small enough
A convergence analysis of this algorithm is presented in [23]. From this analysis
it follows that
∥Rr∥A−1
∥R0∥A−1 ≤
1
Hr(1 +
2
k−1),
(12.90)

12.8 Conjugate Gradient Method
433
where k = λmax
λmin is the condition number of A. The estimate (12.90) tells us that when
the condition number k is close to 1, the term on the right-hand side of (12.90) is
small, and we have rapid convergence in the CG algorithm, Algorithm 12.13. If the
condition number k is large, then the estimate (12.90) can be rewritten as
∥Rr∥A−1
∥R0∥A−1 ≤
1
Hr(1 +
2
k−1) ≤
1
(1 +
2
√k−1),
(12.91)
and convergence is slower.
We note that when the matrix A is simply symmetric, the norm ∥Rr∥2 is mini-
mized using the minimum residual algorithm MINRES [93]. When the matrix A is
asymmetric, the generalized minimum residual algorithm GMRES for minimization
of ∥Rr∥2 is used [101].
Example 12.4 We present a numerical solution of the Dirichlet problem for the
Poisson’s equation (8.11) in two dimensions using the conjugate gradient method.
The setup for our numerical experiments is the same as in Example 8.2 of Chapter8.
The MATLAB® program of Section 1.25, which implements Algorithm 12.13, is
available for running this test.11 We used computation of the residual Rr in the
stopping criterion ∥Rr∥2 < tol, where the chosen tolerance was tol = 10−9.
The conjugate gradient method converged in 33 iterations for number of inner
points in the computational mesh N = 20. Our computed solution is the same as that
presented in Fig.8.1 (check it by running the MATLAB® programs of Section 1.25).
12.9
Preconditioning for Linear Systems
A preconditioning technique is used for the reduction of the condition number of a
problem. For the solution of a linear system of equations Ax = b, the preconditioner
matrix P of a matrix A is a matrix P−1A such that P−1A has a smaller condition
number than the original matrix A. This means that instead of the solution of a system
Ax = b, we will consider the solution of the system
P−1 Ax = P−1b.
(12.92)
The matrix P should have the following properties:
• P is an s.p.d. matrix;
• P−1 A is well conditioned;
• the system Px = b should be easily solvable.
The preconditioned conjugate gradient method is derived as follows. First we
multiply both sides of (12.92) by P1/2 to get
11All MATLAB® programs referenced on this page can be found on the online version of Chapter 1
(doi:10.1007/978-3-319-57304-5_1).

434
12
Introduction to Iterative Methods for the Solution …
(P−1/2 AP−1/2)(P1/2x) = P−1/2b.
(12.93)
We note that the system (12.93) is s.p.d., since we have chosen the matrix P such
that P = QΛQT , which is the eigendecomposition of P. Then the matrix P1/2 will
be s.p.d. if it is deﬁned as P1/2 = QΛ1/2QT . Deﬁning
˜A := P−1/2 AP−1/2,
˜x := P1/2x,
˜b := P−1/2b,
(12.94)
we can rewrite (12.93) as the system ˜A ˜x = ˜b. The matrices ˜A and P−1 A are sim-
ilar, since P−1 A = P−1/2 ˜AP1/2. Thus, ˜A and P−1 A have the same eigenvalues.
Thus, instead of the solution of P−1 Ax = P−1b, we will present the preconditioned
conjugate gradient algorithm (PCG) for the solution of ˜A ˜x = ˜b.
Algorithm 12.14 Preconditioned conjugate gradient algorithm.
Initialization: r = 0;
x0 = 0;
R0 = b;
p1 = P−1b; y0 = P−1R0
repeat
r = r + 1
z = A pr
νr = (yT
r−1Rr−1)/(pT
r z)
xr = xr−1 + νr pr
Rr = Rr−1 −νrz
yr = P−1Rr
μr+1 = (yT
r Rr)/(yT
r−1Rr−1)
pr+1 = yr + μr+1 pr
until ||Rr||2 is small enough
Common preconditioner matrices P are as follows:
• Jacobi preconditioner P = diag(a11, ..., ann). In [108] it was shown that such a
choice of preconditioner reduces the condition number of P−1 A by about a factor
of n of its minimal value.
• Block Jacobi preconditioner
P =
⎛
⎝
P1,1 ... 0
... ... ...
0
... Pr,r
⎞
⎠
(12.95)
with Pi,i = Ai,i, i = 1, ...,r, for a block matrix A given by
A =
⎛
⎝
A1,1 ... A1,r
... ... ...
Ar,1 ... Ar,r
⎞
⎠
(12.96)

12.9 Preconditioning for Linear Systems
435
with square blocks Ai,i, i = 1, ...,r. In [24] it was shown that the choice of pre-
conditioner P given by (12.95) minimizes the condition number of P−1/2 AP−1/2
within a factor of r.
• The method of SSOR can be used as a block preconditioner as well; see details in
[100].
• Incomplete Cholesky factorization [62, 100, 111] with A = LLT is often used for
the PCG algorithm, Algorithm 12.14. In this case, a sparse lower triangular matrix
˜L is chosen to be close to L. Then the preconditioner is deﬁned as P = ˜L ˜LT .
• Incomplete LU preconditioner [100].
• Domain decomposition methods [23].
Some of these preconditioners are implemented in the software package PETSc
[98]. An example using PETSc for the solution of the Dirichlet problem for the
Poisson’s equation (8.11) in two dimensions is presented below.
Example 12.5 In this example we demonstrate how PETSc [98] can be used for the
solution of the Dirichlet problem for the Poisson’s equation (8.11). The setup for our
problem in this and the next example is the same as in Example 8.2 of Chapter 8.
The PETSc programs of Section 1.27 are available for running this example.12 We
executed these programs by running the main program Main.cpp using PETSc
version petsc −3.7.4 on a 64-bit Red Hat Linux Workstation. Here is an example
of the Makeﬁle used for compilation of the PETSc programs of Section 1.27:
PETSC_ARCH=/sup64/petsc-3.7.4
include ${PETSC_ARCH}/lib/petsc/conf/variables
include ${PETSC_ARCH}/lib/petsc/conf/rules
MPI_INCLUDE = ${PETSC_ARCH}/include/mpiuni
CXX=g++
CXXFLAGS = -Wall -Wextra -g -O0 -c -Iinclude \
-I${PETSC_ARCH}/include -I${MPI_INCLUDE}
LD=g++
OBJECTS=Main.o CG.o Create.o DiscretePoisson2D.o \
GaussSeidel.o Jacobi.o PCG.o Solver.o SOR.o
Run=Main
all: $(Run)
$(CXX) $(CXXFLAGS) -o $@ $<
$(Run): $(OBJECTS)
$(LD) $(OBJECTS) $(PETSC_LIB) -o $@
Thedifferentiterativemethodsareencodedbynumbers1–7inthefollowingorder:
(1) Jacobi’s method, (2) Gauss–Seidel method, (3) successive overrelaxation method
(SOR), (4) Conjugate gradient method, (5) conjugate gradient method (Algorithm
12.13), (6) preconditioned conjugate gradient method, (7) preconditioned conjugate
gradient method (Algorithm 12.14). Methods 1–5 use inbuilt PETSc functions, and
12All MATLAB® programs referenced on this page can be found on the online version of Chapter 1
(doi:10.1007/978-3-319-57304-5_1).

436
12
Introduction to Iterative Methods for the Solution …
methods 6 and 7 implement Algorithms 12.13, 12.14, respectively. For example, to
run Main.cpp with the SOR method, one should run this program, for example, as
follows:
> hohup Main 3 > result.m
The results will be printed in the ﬁle result.m, and they can be viewed in
MATLAB® using the command surf(result) (note that before loading the ﬁle
result.m in MATLAB®, one must remove the ﬁrst two rows in the output ﬁle
containinginformationaboutthemethodchosenandthenumberofiterationsatwhich
convergence was achieved). An additional ﬁle with results called solution.m will
also be created. By doing so, we simply illustrate different possibilities for the output
of results in PETSc. Using the command surf(solution) in MATLAB®, the
computed solution of the Dirichlet problem for the Poisson’s equation (8.11) can be
observed.
Example 12.6 In this example we present the numerical solution of the Dirichlet
problem for the Poisson’s equation (8.11) in two dimensions using the precondi-
tioned conjugate gradient method (Algorithm 12.14) implemented in MATLAB®.
The MATLAB® program of Section 1.26 is available for running this test.13 In this
program we can choose among three preconditioners as the preconditioner matrix P
in Algorithm 12.14: Jacobi preconditioner, block Jacobi preconditioner, and incom-
plete Cholesky factorization [62, 100, 111] with A = LLT . We also use the com-
putation of the residual Rr in the stopping criterion ∥Rr∥2 < tol, where the chosen
tolerance was tol = 10−9.
The preconditioned conjugate gradient method converged in 17 iterations for
the preconditioner matrix P constructed using incomplete Cholesky factorization
for N = 20 inner points and for tolerance tol = 10−9. We note that by choosing
Jacobi or block Jacobi preconditioners, we have the same convergence as in the usual
conjugate gradient method. Our ﬁnal solution is the same as that in Fig.8.1 (check
it by running the MATLAB® programs of Section 1.26).
Questions
12.1 Find values of the real parameter α such that the matrix
⎛
⎝
1 0 α
4 2 0
6 5 3
⎞
⎠
(12.97)
(a) has all real values;
(b) has all complex eigenvalues with positive imaginary parts.
13All MATLAB® programs referenced on this page can be found on the online version of Chapter 1
(doi:10.1007/978-3-319-57304-5_1).

Questions
437
12.2 Let A be an n × n Hermitian matrix and let λ and μ be such that λ ̸= μ are
eigenvaluesof A withcorrespondingeigenvectors x and y.Provethattheeigenvectors
corresponding to different eigenvalues are orthogonal, i.e., ¯yT x = 0.
12.3 Let the n × n matrix A have block triangular form
A =
 A11 A12
0
A22

,
(12.98)
where the block A11 isr×r and the blocks A12, A22 are (n−r)×(n−r). Prove that λ is
an eigenvalue of A if λ is an eigenvalue of A11 and x is the corresponding eigenvector.
Hint: construct a vector y of length n −r such that (x, y)T is an eigenvector of A
for the eigenvalue λ.
12.4 Let the dim A = n × n matrix A have spectral radius ρ(A) < 1.
(a) Show that the matrix I −A is nonsingular.
(b) Show that
(I −A)−1 =
n

i=0
Ai.
12.5 Let the n × n complex Hermitian matrix C be represented as C = A + i B,
where the matrices A and B are the real and imaginary parts of C, respectively. Let
us deﬁne the real 2n × 2n matrix ˜C as
˜C =
 A −B
B
A

.
(12.99)
(a) Show that ˜C is a symmetric matrix.
(b) Let λ be an eigenvalue of C with corresponding eigenvector x + iy. Show
that λ is an eigenvalue of ˜C, where both (x, y)T and (−y, x)T are corresponding
eigenvectors.
12.6 Find the eigenvalues of the following matrix:
2i 1
1 0

.
(12.100)
How many linearly independent eigenvectors does it have?
12.7 Let λ be an eigenvalue of the orthogonal matrix Q. Show that |λ| = 1. What
are the singular values of an orthogonal matrix?
12.8 Let the matrix A be a real symmetric tridiagonal matrix that has no zero
elements on its subdiagonal. Show that the matrix A must have distinct eigenvalues.
12.9 (Programming)
Implement the Arnoldi and Lanczos algorithms, Algorithms 12.9 and 12.10. First,
run both algorithms on A with starting vector for q as in Algorithms 12.9, 12.10.

438
12
Introduction to Iterative Methods for the Solution …
Then run both algorithms on QT AQ with the starting vector for q given by QTq.
Conﬁrm that you obtain identical upper Hessenberg matrices Hr in Algorithm 12.9
or tridiagonal matrices Tr in Algorithm 12.10 in both cases.
12.10 (Programming)
Implement different versions of the Lanczos algorithm, Algorithms 12.10, 12.11,
12.12.Presentthefollowingresultsgraphicallydependingonthestepi oftheLanczos
algorithms:
1. some largest and smallest computed eigenvalues λi(A) of A;
2. the global errors in the computed eigenvalues λi(Tr) of item 1 given by
|λi(Tr) −λi(A)|
|λi(A)|
;
3. the local errors of item 1 given by
min
j
|λi(Tr) −λ j(A)|
|λi(A)|
,
where λ j(A) is the eigenvalue of A nearest to λi(Tr); sometimes these errors are
smaller than the global errors;
4. the error bounds of item 1
|βrvi(r)|
|λi(A)| .
12.11 Prove that the conjugate vectors pr in Algorithm 12.13 are orthogonal with
respect to the inner product deﬁned by A.
12.12 Prove that if A, of order dim A = n × n, is symmetric positive deﬁnite, then
Hr = QT
r AQr, where dim Q = n × r with full column rank, is also symmetric
positive deﬁnite. Here the matrix Q has full column rank and is not orthogonal.
12.13 (Programming)
Modify the MATLAB® program of Section 1.21, which implements the Jacobi
method of Algorithm 12.2, and using this program, solve the model problem for the
Poisson’s equation of Section8.1.3 in three dimensions on the unit cube.14
12.14 (Programming)
Modify the MATLAB® program of Section 1.22, which implements the Gauss–
Seidel method of the algorithm 12.4, and using this program solve the model problem
for the Poisson’s equation of Section8.1.3 in three dimensions on the unit cube.
14All MATLAB® programs referenced on this page can be found on the online version of Chapter 1
(doi:10.1007/978-3-319-57304-5_1).

Questions
439
12.15 (Programming)
Modify the MATLAB® program of Section 1.24, which implements the SOR(ω)
method of Algorithm 12.6 for different ω, and using this program, solve the model
problem for the Poisson’s equation of Section8.1.3 in three dimensions on the unit
cube.15 For which ω does the SOR(ω) method converge? Why?
12.16 (Programming)
Write a MATLAB® program to implement the SSOR(ω) algorithm, Algo-
rithm 12.8, with Chebyshev acceleration of xi+1 =
ˆRxi + c for different ω. In
[46], it was shown that the choice ω =
2
1+√2(1−ρ) is a good one. In that case,
ρ( ˆR) ≈1 −
π
2N , where N is the number of points in the mesh. Compare the perfor-
mance of SSOR(ωopt) and SOR(ωopt) with optimal ωopt given by (12.59).
12.17 (Programming)
Write a program to implement the Arnoldi algorithm, Algorithm 12.9. Test the
program on a real symmetric n × n matrix A with eigenvalues 1, 2, ..., n. Hint: to
generate such a matrix A, ﬁrst generate an n ×n matrix B with randomly distributed
entries in the interval [0, 1) and compute its QR factorization B = QR. Then con-
struct the matrix A as A = QDQT with diagonal matrix D = diag(1, ..., n). Run
the Arnoldi algorithm, Algorithm 12.9, for n iterations.
12.18 (Programming)
Write a program to implement the Lanczos algorithm, Algorithm 12.10. Test the
program on a real symmetric matrix A as in Question 12.4. Run Algorithm 12.10 for
n iterations.
12.19 (Programming)
Modify the MATLAB® program of Section1.25, which implements the conju-
gate gradient algorithm, Algorithm 12.13, and using this program, solve the model
problem for the Poisson’s equation of Section8.1.3 in three dimensions on the unit
cube.
12.20 (Programming)
Modify the MATLAB® program of Section1.25, which implements the precondi-
tioned conjugate gradient algorithm, Algorithm 12.13, and using this program, solve
the model problem for the Poisson’s equation of Section8.1.3 in three dimensions
on the unit cube.
12.21 (Programming)
Modify the PETSc programs of Section1.27, which solve the Dirichlet prob-
lem for the Poisson’s equation (8.11) and apply them to solve this problem in
three dimensions on the unit cube. See details on running PETSc programs in
Example 12.5.
15All MATLAB® programs referenced on this page can be found on the online version of Chapter 1
(doi:10.1007/978-3-319-57304-5_1).

References
1. Allen, G.D.: Lectures on Linear Algebra and Matrices. Texas A&M University. http://www.
math.tamu.edu/~dallen/m640_03c/readings.htm
2. Arioli, M., Demmel, J., Duff, I.S.: Solving sparse linear systems with sparse backward error.
SIAM J. Matrix Anal. Appl. 10, 165–190 (1989)
3. Anderson, E., Baiu, Z., Bischof, C., Blackford, S., Demmel, J., Dongarra, J., Du Croz, J.,
Greenbaum, A., Hammarling, S., McKenney, A., Sorensen, D.: LAPACK Users’ Guide Third
Edition (2012)
4. Axelsson, O.: Iterative Solution Methods. Cambridge University Press, Cambridge (1996)
5. Bai, Z.: Error analysis of the Lanczoz algorithm for the nonsymmetric eigenvalue problem.
Math. Comput. 62, 209–226 (1994)
6. Bai, Z.: Progress in the numerical solution of the nonsymmetric eigenvalue problem. J. Numer.
Linear Algebra Appl. 2, 210–234 (1995)
7. Bakushinsky, A., Kokurin, M.Y., Smirnova, A.: Iterative Methods for Ill-posed Problems.
Inverse and Ill-Posed Problems Series, vol. 54. De Gruyter (2011)
8. Batterson, S.: Convergence of the shifted QR algorithm on 3 by 3 normal matrices. Numer.
Math. 58, 341–352 (1990)
9. Barrett, R., Berry, M., Chan, T.F., Demmel, J., Donato, J.M., Dongarra, J., Eijkhout, V., Pozo,
R., Romine, C., Van der Vorst, H.: Templates for the Solution of Linear Systems: Building
Blocks for Iterative Methods. SIAM, Philadelphia (1994)
10. Belitskii, G.R., Lyubich, Y.I.: Matrix Norms and their Applications. Birkhäuser Verlag, Basel
(1988)
11. Bellman, R.: Introduction to Matrix Analysis. SIAM (1997)
12. Burden, R.L., Douglas Faire, J.: Numerical Analysis, 8th edn. (2004). ISBN-10: 0534392008
13. Byrd, R.H., Schnabel, R.B., Shultz, G.A.: A trust region algorithm for nonlinearly constrained
optimization. SIAM J. Numer. Anal. 24(5), 11521170 (1987). doi:10.1137/0724076
14. Björck, Å.: Numerical Methods for Least Squares Problems. SIAM (1996)
15. Bischof, C.: Incremental condition estimation. SIAM J. Matrix Anal. Appl. 11, 312–322
(1990)
16. Bunch, J., Kaufman, L.: Some stable methods for calculating inertia and solving symmetric
linear systems. Math. Comput. 31, 163–179 (1977)
17. Calinger, R.: A Contextual History of Mathematics. Prentice Hall (1999). ISBN 978-0-02-
318285-3
18. Chan, T.: Rank revealing QR factorizations. Linear Algebra Appl. 88/89, 67–82 (1987)
19. Ciarlet, P.: Introduction to Numerical Linear Algebra and Optimisation. Cambridge University
Press (1989)
© Springer International Publishing AG 2017
L. Beilina et al., Numerical Linear Algebra: Theory and Applications,
DOI 10.1007/978-3-319-57304-5
441

442
References
20. Cullum, J., Kerner, W., Willoughby, R.: A generalized nonsymmetric Lanczos procedure.
Comput. Phys. Commun. 53, 19–48 (1989)
21. Cuppen, J.J.M.: A divide and conquer method for the symmetric tridiagonal eigenproblem.
Numer. Math. 36, 177–195 (1981)
22. Day, D.: How the QR algorithm fails to converge and how to ﬁx it, Technical report 96–0913J.
Sandia National Laboratory, NM (1996)
23. Demmel, J.W.: Applied Numerical Linear Algebra. SIAM (1997)
24. Demmel, J.W.: The condition number of equivalence transformations that block diagonal-
ize matrix pencils. SIAM J. Numer. Anal. 20, 599–610 (1983). Applied Numerical Linear
Algebra. SIAM (1997)
25. Demmel, J., Veselic, K.: Jacobi’s method is more accurate than QR. SIAM J. Matrix Anal.
Appl. 13, 1204–1246 (1992)
26. Demmel, J., Dillon, Ren, H.: On the correctness on some bisection-like parallel eigenvalue
algorithms in ﬂoating point arithmetic. Electronic Trans. Numer. Anal. 3, 116–140 (1995)
27. Demmel, J., Gragg, W.: On computing accurate singular values and eigenvalues of matrices
with acyclic graphs. In: Linear Algebra and Its Applications, pp. 203–217 (1993)
28. Demmel, J., Kahan, W.: Accurate singular values of bidiagonal matrices. SIAM J. Sci. Stat.
Comput. 11, 873–912 (1990)
29. Dhillon, I.S.: A new O(n2) algorithm for the symmetric tridiagonal eigenvalue/eigenvector
problem, Ph.D. thesis, Computer Science Division, University of California, Berkeley (1997)
30. Eisenstat, S.: A stable algorithm for the rank-1 modiﬁcation of the symmetric eigenproblem.
Computer Science Department. Report YALEU/DCS/RR-916, Yale University, September
1992
31. Engl, H.W., Hanke, M., Neubauer, A.: Regularization of Inverse Problems. Kluwer Academic
Publishers, Boston (2000)
32. Fernando, K., Parlett, B.: Accurate singular values and differential qd algorithms. Numer.
Math. 67, 191–229 (1994)
33. Gantmaher, F.R.: The Theory of Matrices. AMS Chelsea Publishing, vol. 1, vol. 2 (2000)
34. Gantmacher, F.R.: Applications of the Theory of Matrices. Dover Publications (2005)
35. Gelfand, I.M.: Lectures on Linear Algebra, Dover Books on Mathematics (1989)
36. Glazman, I.M., Ljubich, Y.I.: Finite-Dimensional Linear Analysis: A Systematic Presentation
in Problem Form. Dover Publications (2006)
37. Godunov, S.K.: Modern Aspects of Linear Algebra. American Mathematical Society (1998)
38. Gu, M., Eisenstat, S.C.: A divide-and-conquer algorithm for the symmetric tridiagonal eigen-
problem. SIAM J. Matrix Anal. Appl. 16, 172–191 (1995)
39. Grcar, J.F.: Mathematicians of Gaussian elimination. Not. Am. Math. Soc. 58(6), 782–792
(2011)
40. Grimes, R., Lewis, J., Simon, H.: A shifted block Lanczoz algorithm for solving sparce
symmetric generalized problems. SIAM J. Matrix Anal. Appl. 15, 228–272 (1994)
41. Faddeev, D.K.: Lectures in Algebra. Nauka (1984) (in Russian)
42. Fernando, V., Parlett, B., Dhillon, I.: A way to ﬁnd the most redundant equation in a tridiagonal
system. Preprint at California University, Berkeley Center (1995)
43. Fernando, K., Parlett, B.: Accurate singular values and differential qd algorithms. Numer.
Math. 67, 191–229 (1994)
44. Freund, R., Golub, G.H., Nachtigal, N.: Iterative solution of linear systems. Acta Numerica,
97–100 (1992)
45. Hager, W.W.: Condition estimators. SIAM J. Sci. Stat. Comput. 5, 311–316 (1984)
46. Hageman, L.A., Young, D.M.: Applied Iterative Methods. Academic Press, New York (1981)
47. Halmos, P.: Finite Dimensional Vector Spaces. Van Nostrand, New York (1958)
48. Halmos, P.R.: Linear Algebra Problem Book. The Mathematical Association of America
(1995)
49. Hackbusch, W.: Iterative Solution of Large Sparse Systems of Equations. Springer, Berlin
(1994)

References
443
50. Higham, N.J.: A survey of condition number estimation for triangular matrices. SIAM Rev.
29, 575–596 (1987)
51. Higham, N.J.: Experience with a matrix norm estimator. SIAM J. Sci. Stat. Comput. 11,
804–809 (1990)
52. Higham, N.J.: FORTRAN codes for estimating the one-norm of a real or complex matrix.
SIAM Rev. 29, 575–596 (1987)
53. Higham, N.: Accuracy and Stability of Numerical Algorithms, 2nd edn. SIAM (2002)
54. Higham, N.J.: Functions of Matrices. Theory and Computation. Society for Industrial and
Applied Mathematics (2008)
55. Horn, R.A., Johnson, C.R.: Topics in Matrix Analysis. Cambridge University Press (1999)
56. Horn, R.A., Johnson, C.R.: Matrix Analysis. Cambridge University Press (2012)
57. Fine, B., Rosenberger, G.: The Fundamental Theorem of Algebra. Springer, Berlin (1997)
58. Forsythe, G.E., Malcolm, M.A., Moler, C.B.: Computer Methods for Mathematical Compu-
tations. Prentice Hall, Englewood Cliffs (1977)
59. Ikramov, KhD: Numerical Solution of Matrix Equations. Nauka, Moscow (1984). (in Russian)
60. Ikramov, KhD: Matrix pencils-theory, applications, numerical methods. J. Soviet Math. 64(2),
783–853 (1993)
61. Kahaner, D., Moler, C., Nash, S.: Numerical Methods and Software. Prentice Hall, Englewood
Cliffs (1989)
62. Kelley, C.T.: Frontiers in Applied Mathematics: Iterative Methods for Linear and Nonlinear
Equations. SIAM, Philadelphia (1995)
63. Krasnosel’skii, M.A., Vainikko, G.M., Zabreiko, P.P., Rutitskii, Y.B., Stetsenko, V.Y.: Approx-
imate Solution of Operator Equations. Wolters-Noordhoff Publishing, Groningen (1972)
64. Kurosh, A.: Higher Algebra. Mir Publishers (1980)
65. Kuttler, K.: Elementary Linear Algebra. Brigham Young University (2009)
66. Lancaster, P., Tismenetsky, M.: The Theory of Matrices. Academic Press (1985)
67. Lau,D.:ÜbungsbuchzurLinearenAlgebraundanalytischenGeometrie.AufgabenmitLösun-
gen. Springer, Berlin (2011)
68. Lawson, C.L., Hanson, R.J.: Solving Least Squares Problems. SIAM, Philadelphia (1995)
69. Dongarra, J.J., Bunch, J.R., Moler, C.B., Stewart, G.W.: Linpack Users Guide, 2nd edn. SIAM,
Philadelphia (1979)
70. Mal’cev, A.I.: Foundations of Linear Algebra. W.H Freeman and Company (1963)
71. Marcus, M., Min, H.: A Survey of Matrix Theory and Matrix Inequalities. Dover Publications
(2010)
72. Meyer, C.D.: Matrix Analysis and Applied Linear Algebra. With Solutions to Problems. SIAM
(2000)
73. Mirsky, L.: An Introduction to Linear Algebra. Dover Publications (2011)
74. Moré, : The Levenberg–Marquardt algorithm: implementation and theory. In: Watson, (ed.)
Numerical Analysis, Proceedings of the Biennial Conference Held at Dundee, pp. 105–116
(1977)
75. Prasolov, V.V.: Problems and Theorems in Linear Algebra. American Mathematical Society
(1994)
76. Shilov, G.E.: Linear Algebra. Dover Publications (1977)
77. Stewart, F.M.: Introduction to Linear Algebra. D. Van Nostrand Company Inc. (1963)
78. Stewart, G.W., Sun, J.: Matrix Perturbation Theory. Academic Press, Inc. (1990)
79. Strang, G.: Linear Algebra and Its Applications. Cengage Learning (2006)
80. Tyrtyshnikov, E.E.: Matrix Analysis and Linear Algebra. FIZMATLIT, Moscow (2007). (in
Russian)
81. Varga, R.S.: Matrix Iterative Analysis. Springer, Berlin (2010)
82. Vinberg, E.B.: A Course in Algebra. American Mathematical Society (2003)
83. Voevodin, V.V.: Linear Algebra. Mir Publishers (1983)
84. Voevodin, V.V., Kuznecov, Y.A.: Matrices and Calculations. Nauka, Moscow (1984). (in
Russian)

444
References
85. Wildon, M.: A short proof of the existence of Jordan normal form. http://www.ma.rhul.ac.
uk/~uvah099/Maths/JNFﬁnal.pdf
86. Zhan, X.: Matrix Inequalities. Springer, Berlin (2002)
87. Zhang, F.: Matrix Theory. Basic Results and Techniques. Springer, New York (1999)
88. Hager, W.: Applied Numerical Linear Algebra. Prentice Hall, Englewood Cliffs (1988)
89. Lau, H.T.: A Numerical Library in C for Scientists and Engineers. CRC Press, Boca Raton
(1995)
90. Li, T.-Y., Zhang, H., Sun, X.-H.: Parallel homotopy algorithm for symmetric tridiagonal
eigenvalue problem. SIAM J. Sci. Stat. Comput. 12, 469–487 (1991)
91. Li, T.-Y., Zeng, Z.: Homotopy-determinant algorithm for solving nonsymmetric eigenvalue
problems. Math. Comput. 59, 483–502 (1992)
92. Li, T.-Y., Zeng, Z., Cong, L.: Solving eigenvalue problems of nonsymmetric matrices with
real homotopies. SIAM J. Numer. Anal. 29, 229–248 (1992)
93. Paige, C.C., Saunders, M.A.: Solution of sparce indeﬁnite systems of linear equations. SIAM
J. Numer. Anal. 12, 617–629 (1975)
94. Parlett, B.: The Symmetric Eigenvalue Problem. Prentice Hall, Englewood Cliffs (1980)
95. Parlett, B.: The new qd algorithms. In: Acta Numerica, pp. 459–491. Cambridge University
Press, Cambridge (1995)
96. Parlett, B.N., Dhillon, I.S.: Fernando’s solution to Wilkinson’s problem: an application of
double factorization. Linear Algebr. Appl. (1997)
97. Parlett,B.:Theconstructionoforthogonaleigenvectorsfortightclustersbyuseofsubmatrices,
CenterforPureandAppliedMathematicsPAM-664,UniversityofCalifornia,Berkeley(1996)
98. PETSc, Portable, Extensible Toolkit for Scientiﬁc Computation. https://www.mcs.anl.gov/
petsc/
99. Rutishauser, H.: Lectures on Numerical Mathematics. Birkhauser, Basel (1990)
100. Saad, Y.: Iterative Methods for Sparse Linear Systems. PWS Publishing Co., Boston (1996)
101. Saad, Y., Schulz, M.H.: GMRES: a generalized minimal residual algorithm for solving non-
symmetric linear systems. SIAM J. Sci. Stat. Comput. 7, 856–869 (1986)
102. Saad, Y.: Numerical solution of large nonsymmetric eigenvalue problems. Comput. Phys.
Comput. 37, 105–126 (1981)
103. Scilab documentation - number properties - determine ﬂoating-point parameters (2013)
104. Skeel, R.D.: Scaling for numerical stability in Gaussian elimination. J. ACM 26, 494–526
(1979)
105. Skeel, R.D.: Iterative reﬁnement implies numerical stability for Gaussian elimination. Math.
Comput. 35, 817–832 (1980)
106. Skeel, R.D.: Effect of equilibration on residual size for partial pivoting. SIAM J. Numer. Anal.
18, 449–454 (1981)
107. Simon, H.: The Lanczoz algorithm with partial reorthogonalization. Math. Comput. 42, 115–
142 (1984)
108. Van Der Sluis, : Condition numbers and equilibration of matrices. Numer. Math. 14, 14–23
(1969)
109. Szegö, G.: Orthogonal Polynomials. AMS, Providence (1967)
110. Tikhonov, A.N., Goncharsky, A.V., Stepanov, V.V., Yagola, A.G.: Numerical Methods for the
Solution of Ill-Posed Problems. Kluwer, London (1995)
111. Trefethen, L.N., Bau III, D.: Numerical Linear Algebra. SIAM, Philadelphia (1997)
112. Van Der Sluis, A.: Condition numbers and equilibration of matrices. Numer. Math. 14, 14–23
(1969)
113. Wilkinson, J.H.: The Algebraic Eigenvalue Problem. Oxford University Press, Oxford (1965)
114. Wright, M.H., Glassman, S.: Fortran subroutines to solve linear least squares problems and
compute the complete orthogonal factorization. Technical report, Stanford University, Stan-
ford, CA (1978)
115. Zeng, Z.: Homotopy-Determinant Algorithm for Solving Matrix Eigenvalue Problems and Its
Parallelizations, Ph.D. thesis, Michigan State University, East Lansing, MI (1991)

Index
A
Algorithms for Gaussian Elimination
algorithm
bisection, 285
Cholesky, 277
equilibration, 275
backward substitution, 251
condition number, 268
error analysis, 265
forward substitution, 251
Hager’s estimator, 269, 271
LU factorization with pivoting, 250
relative condition number, 271
Approximation
the best, 87, 92
Arnoldi
algorithm, 424–426, 439
vectors, 424
Axioms
inner product
for a complex vector space, 70
for a real vector space, 69
matrix norm, 218
norm on the space Cn, 211
vector space, 57
B
Basis
in the space Cn
Fourier, 81
standard, 64
in the space of polynomials
Lagrange, 68
natural, 67
Newton, 68
of a ﬁnite-dimensional vector space, 65
reciprocal, 80
Bauer–Fike theorems, 238
Bauer–Skeel theorem, 245
Bessel’s inequality, 89
Bézout’s theorem, 11
C
Cauchy–Schwarz inequality, 73, 210
generalized, 217
Cauchy–Schwarz inequality theorem, 72
Cayley–Hamilton theorem, 117
Characteristic equation
of a matrix, 116
of an operator, 117
Characteristic polynomial
of a matrix, 116
of a matrix pencil, 192
of an operator, 117
Characteristic values
of a matrix, 116
of a regular matrix pencil, 193
of an operator, 117
Cofactor, 22
Column matrix, 32
Complement
orthogonal, 91
Complex number
absolute value, 4
argument, 5
conjugate, 4
difference, 3
division, 3
imaginary part, 2
© Springer International Publishing AG 2017
L. Beilina et al., Numerical Linear Algebra: Theory and Applications,
DOI 10.1007/978-3-319-57304-5
445

446
Index
multiplication, 3
nth roots, 7
real part, 2
sum, 3
trigonometric form, 6
zero, 3
Condition number, 167, 241
of the linear least squares problem, 248
relative or Bauer–Skeel, 246
Conjugate gradient
algorithm, 407, 426, 432
method, 433, 436
vectors, 429
Conjugate gradients, 429
Convergence
componentwise, 212
Gauss–Seidel method, 418, 420
in norm, 212
Jacobi method, 418, 420
successive
overrelaxation
SOR(ω)
method, 420, 422
Convergent
power series of matrices, 188
sequence of matrices, 131
Convex function, 170
Cramer’s formulas, 30
D
Defect of an operator, 103
De Moivre’s formula, 7
Determinant, 19
of an operator, 101
Vandermonde, 26
E
Eigenpair of an operator, 115
Eigenspace of an operator, 115
Eigenvalue of an operator, 115
Eigenvector
of a matrix, 117
of a regular matrix pencil, 193
of an operator, 115
F
Fan’s theorem, 155
Fourier coefﬁcients, 79
Fredholm’s theorem, 136
matrix, 109
Fundamental set of solutions of a homoge-
neous equation, 108
Fundamental theorem of algebra, 11
G
Gap between subspaces, 226
Gaussian elimination, 43
Gauss transformation, 138
General solution
of a homogeneous equation, 108
of a linear equation, 107
Generalized Schur’s theorem, 193
Gershgorin theorem, 238
column sum version, 238
Givens rotation, 160
Gram–Schmidt
orthogonalization process, 77
orthogonalization theorem, 76
H
Hahn–Banach theorem, 215
Hessenberg
matrix, 358
lower, 359
unreduced, 370, 371
upper, 359–362, 370, 372
reduction, 358–360
Hölder’s inequality, 209
Horner’s rule, 10, 259, 261
Householder transformation, 161
Hyperplane, 91
I
Image of an operator, 102
Imaginary unit, 1
Invariant subspace of an operator, 113
trivial, 113
Invariants
of a matrix, 124
of an operator, 124
Inverse map, 95
Inversion, 17
Isomorphic spaces, 97
Isomorphism, 97
Iterative
Gauss–Seidel method, 411–414
Jacobi method, 409
methods, 407
successive
overrelaxation
SOR(ω)
method, 413
symmetric
successive
overrelaxation
SSOR(ω) method, 415
Iterative algorithms, 407
Ivoting, 249, 250

Index
447
J
Jensen’s inequality, 170
Jordan
basis, 180
block, 179
canonical form, 179
real, 186
K
Kernel of an operator, 102
Kronecker canonical form, 204
Kronecker–Capelli theorem, 109
Kronecker delta, 22
Krylov subspace, 424, 426
methods, 407, 424
L
Lagrange interpolation formula, 31
Lanczos
algorithm, 425, 426, 429, 431, 439
vectors, 426, 429
Leading principal submatrix, 153
Linear functional, 132
on Cn, 214
real, 214
Linear least squares problems, 291–293
data ﬁtting, 293, 294, 296
normal equations, 303
QR decomposition, 305, 310
rank-deﬁcient, 333, 336
SVD decomposition, 326–328, 330, 331
Linear space of linear operators, 102
real, 102
Löwner’s theorem, 383, 385
M
Majorization, 154
weak, 153
Matrices
commuting, 37
equivalent, 101
similar, 101
sum, 34
Matrix
adjugate, 41
block, 51
diagonal, 54
lower triangular, 54
upper triangular, 54
change of basis, 66
convergent, 190
diagonal, 32
diagonally dominant
column, 241
row, 241
exponential, 191
full rank, 105
Gram, 74
Hermitian, 49
Hermitian adjoint, 49
identity, 33
inverse, 41
left, 40
right, 40
Jordan, 179
nilpotent, 126
nonnegative, 171
nonsingular, 28
normal, 51
of an operator, 98
orthogonal, 51
partial isometry, 177
permutation, 33
positive deﬁnite, 141
positive semideﬁnite, 141
product
by a matrix, 36
by a scalar, 34
by a vector, 34
real, 50
rectangular, 32
singular, 28
skew-Hermitian, 49
skew-symmetric, 50
square, 19
stochastic, 171
doubly, 171
symmetric, 50
transpose, 24, 39
triangular
lower, 26, 33
lower elementary, 33
upper, 26, 33
unitary, 51
zero, 34
Minkowski inequality, 210
Minkowski’s inequality, 70, 73
Minor, 22
basic, 106
principal, 123
leading, 104
Moore–Penrose pseudoinverse, 334
Multiplicity of an eigenvalue
algebraic, 121
geometric, 120

448
Index
N
Neumann series, 190
Newton’s method, 272, 382
Nonlinear least squares problems, 297
Nonsymmetric eigenvalue problems, 345
algorithm
bidiagonal reduction, 362
Hessenberg reduction, 359, 360, 362
inverse iteration, 348
orthogonal iteration, 351
power method, 345
QR iteration, 355
QR iteration with shifts, 356
single shift QR algorithm, 370
tridiagonal reduction, 362, 365–367
Norm
dual, 217
equivalent, 212
on the space Cn, 211
∥· ∥A, 211
∥· ∥∞, 211
∥· ∥p, 211
absolute, 213
monotone, 214
on the space of matrices, 218
∥· ∥l1, 218
absolute column sum, 221
absolute row sum, 222
consistent, 218
Euclidean, 219
Frobenius, 219
induced, 220
Ky Fan, 225
operator, 220
spectral, 222
vector, 218
Normal solution, 136
Number
complex, 2
imaginary, 2
O
Operator
adjoint, 133
congruent, 146
diagonalizable, 121
identity, 94
index of nilpotency, 126
inertia, 146
inverse, 96
invertible, 95
kth root, 145
linear, 93
linear combination, 94
nilpotent, 126
nonnegative semideﬁnite, 141
nonsingular, 102
normal, 142
null, 94
orthogonal, 156
permutable, 116
positive deﬁnite, 141
product, 94
projection, 94
orthogonal, 94
pseudoinverse, 137
self-adjoint (Hermitian), 139
skew-Hermitian, 139
skew-symmetric, 155
unitary, 141
Orthogonal decomposition theorem, 91
Orthogonalization methods
classical Gram–Schmidt, 325
Givens rotation, 322
Gram–Schmidt orthogonalization, 323,
427
Householder transformations, 307, 308
Orthogonal projection, 88
P
Paige’s theorem, 427
Pencil of matrices, 192
deﬁnite, 198
Hermitian, 197
rank, 198
regular, 192
quasidiagonal form, 195
singular, 192
minimal index, 199
minimal index left, 202
minimal index right, 202
Pencils of matrices
equivalent, 193
Permutation, 17
even, 17
odd, 17
signature, 17
transposition, 17
Pivoting, 250, 252–255, 257, 258, 277, 279
complete, 252
partial, 252
Polar decomposition, 168
Polynomial, 8
Chebyshev, 82

Index
449
coefﬁcients, 8
degree, 8
division, 10
leading coefﬁcient, 8
Legendre, 82
normalized, 11
root, 11
multiplicity, 11
simple, 11
zero, 8
Power series of matrices, 188
Preconditioned conjugate gradient
algorithm, 434
method, 436
Pseudosolution, 137
normal, 137, 166
R
Rank
of a matrix, 103
of an operator, 103
Rayleigh quotient, 377, 379
Rayleigh–Ritz procedure, 427
Residual, 137, 272–274
Residual functional, 137
Restriction of an operator, 115
Riesz’s theorem, 132
Rigal–Gaches theorem, 244
Ritz
values, 427
vectors, 427
Rodrigues’s formula, 78
Root of an operator, 145
Rotation
improper, 160
proper, 160
Rounding
analysis, 259
error, 265, 272, 273
Row, 32
product
by a column, 34
by a matrix, 35
S
Scalar multiplication (multiplication of a
vector by a scalar), 56
Schur form of a matrix, 129
real, 131
Schur’s theorem, 127, 154
Shift, 348–351, 356, 357, 370, 373, 377,
397–399, 401
Francis shift, 357, 358
Wilkinson’s shift, 358, 376
Singular
value decomposition, 166
values of an operator, 163
vectors of an operator, 164
Span, 84
Spectral radius of a matrix, 188
Spectral resolution of an operator, 122
Spectrum
of a matrix, 116
of an operator, 117
Subspace, 84
basis, 85
cyclic, 185
dimension, 85
intersection, 84
orthogonal, 86
root, 185
sum, 84
direct, 85, 86
orthogonal, 86
zero, 85
Sylvester equation, 130
Sylvester’s criterion, 153
Sylvester’s formula, 122
Sylvester’s inertia theorem, 385, 386
Sylvester’s law of inertia, 146
Symmetric eigenvalue problems
algorithm, 375
bisection and inverse iteration, 387
classical Jacobi’s algorithm, 392
cyclic-by-row-Jacobi, 393
divide and conquer, 383
dqds, 400, 401
Jacobi rotation, 391
LR iteration, 397
one-sided Jacobi rotation, 401, 402
qds, 400
Rayleigh quotient iteration, 377
tridiagonal QR iteration, 376
System of linear algebraic equations, 28, 108
homogeneous, 28
matrix, 28
right-hand side, 28
trivial solution, 28
T
Tikhonov regularization method, 138
Trace of an operator, 124
Transformation similarity, 101
Triangle inequality, 211

450
Index
Triangle inequality (Minkowski’s
inequality), 70
Tuple, 55
V
Vector, 55, 57
addition, 56
components, 55
contravariant, 81
covariant, 81
coordinates, 65, 96
cyclic, 186
height, 186
equivalent sets, 61
Euclidean length, 70
inner product, 71
standard on the space Cn, 71
standard on the space Rn, 70
length, 73
linear combination, 60
maximal linearly independent subset, 63
normalized, 144
orthogonal, 73
proportional, 59
rank of a set, 64
set
linearly dependent, 59
linearly independent, 62
orthogonal, 75
orthonormal, 75
standard unit, 56
zero, 56
Vector space
C[a, b], 59
Cn, 57
Rn, 55
Qn, 59
V3, 58
complex, 58
dimension, 65
Euclidean, 71
ﬁnite-dimensional, 65
inner product, 71
real, 57
unitary, 71
Vi‘ete’s formulas, 15
W
Wedin’s theorem, 247
Weierstrass canonical form, 196
Weierstrass’s theorem, 196
Weyl’s inequalities, 231
Weyl’s theorem, 176
“relative”, 232
Y
Young’s inequality, 209


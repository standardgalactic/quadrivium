
Springer Texts in Statistics
Advisors:
George Casella, Stephen Fienberg, Ingram Olkin

Springer Texts in Statistics
Athreya/Lahiri: Measure Theory and Probability Theory
Bilodeau/Brenner: Theory of Multivariate Statistics
Brockwell/Davis: An Introduction to Time Series and Forecasting
Carmona: Statistical Analysis of Financial Data in S-PLUS
Chow/Teicher: Probability Theory: Independence, Interchangeability,
Martingales, 3rd ed.
Christensen: Advanced Linear Modeling: Multivariate, Time Series, and Spatial
Data; Nonparametric Regression and Response Surface Maximization, 2nd ed.
Christensen: Log-Linear Models and Logistic Regression, 2nd ed.
Christensen: Plane Answers to Complex Questions: The Theory of Linear
Models, 2nded.
Davis: Statistical Methods for the Analysis of Repeated Measurements
Dean/Voss: Design and Analysis of Experiments
Dekking/Kraaikamp/Lopuha¨a/Meester:A Modern Introduction to Probability and
Statistics
Durrett: Essential of Stochastic Processes
Edwards: Introduction to Graphical Modeling, 2nd ed.
Everitt: An R and S-PLUS Companion to Multivariate Analysis
Ghosh/Delampady/Samanta: An Introduction to Bayesian Analysis
Gut: Probability: A Graduate Course
Heiberger/Holland: Statistical Analysis and Data Display; An Intermediate Course
with Examples in S-PLUS, R, and SAS
Jobson: Applied Multivariate Data Analysis, Volume I: Regression and
Experimental Design
Jobson: Applied Multivariate Data Analysis, Volume II Categorical and Multi-
variate
Methods
Karr: Probability
Kulkarni: Modeling, Analysis, Design, and Control of Stochastic Systems
Lange: Applied Probability
Lange: Optimization
Lehmann: Elements of Large Sample Theory
Lehmann/Romano: Testing Statistical Hypotheses, 3rd ed.
Lehmann/Casella: Theory of Point Estimation, 2nd ed.
Marin/Robert: Bayesian Core: A Practical Approach to Computational Bayesian
Statistics
Nolan/Speed: Stat Labs: Mathematical Statistics Through Applications
Pitman: Probability
Rawlings/Pantula/Dickey: Applied Regression Analysis
Robert: The Bayesian Choice: From Decision-Theoretic Foundations to
Computational Implementation, 2nd ed.
Robert/Casella: Monte Carlo Statistical Methods, 2nd ed.
Rose/Smith: Mathematical Statistics with Mathematica
Ruppert: Statistics and Finance: An Introduction
(continued after index)

Christian P. Robert
The Bayesian Choice
From Decision-Theoretic Foundations
to Computational Implementation
Second Edition

Christian P. Robert
CEREMADE
Universite Paris Dauphine
Place du Mar´echal de Lattre de Tassigny
75775 Paris cedex 16
France
xian@ceremade.dauphine.fr
Library of Congress Control Number: 2007926596
ISBN 978-0-387-71598-8
e-ISBN 978-0-387-71599-5
Printed on acid-free paper.
c⃝2007 Springer Science+Business Media, LLC
All rights reserved. This work may not be translated or copied in whole or in part without the written
permission of the publisher (Springer Science+Business Media, LLC, 233 Spring Street, New York,
NY 10013, USA), except for brief excerpts in connection with reviews or scholarly analysis. Use
in connection with any form of information storage and retrieval, electronic adaptation, computer
software, or by similar or dissimilar methodology now known or hereafter developed is forbidden.
The use in this publication of trade names, trademarks, service marks, and similar terms, even if they
are not identiﬁed as such, is not to be taken as an expression of opinion as to whether or not they are
subject to proprietary rights.
9 8 7 6 5 4 3 2 1
springer.com

To my reference prior,
Brigitte,
and to two major updates,
Joachim and Rachel.

Preface to the Paperback Edition
What could not be changed must be endured.
Robert Jordan, New Spring, Prequel to The Wheel of Time
Thanks
While this paperback edition is almost identical to the second edition
of The Bayesian Choice, published in 2001, and thus does not require a
speciﬁc introduction, it oﬀers me the opportunity to thank several groups
of people for their contributions that made this edition possible.
First, the changes, when compared with the second edition, are only
made of corrections of typographical and conceptual errors (whose updated
list can be found on my Webpage1). Almost all errors have been pointed
out to me by friends, colleagues, unknown lecturers or anonymous readers
who (always kindly and sometimes apologetically) sent me emails asking
me to clarify a speciﬁc paragraph, a formula or a problem that did not
make sense to them. Needless to say, I am very grateful to those numerous
contributors for making the book more accurate and I obviously encourage
all contributors who think there could be an error in the current edition to
contact me because they cannot be wrong! Either there indeed is a mistake
that needs to be set right or there is no mistake but the context is am-
biguous at best and the corresponding text needs to be rewritten. Thanks,
then, to Guido Consonni, Estelle Dauchy, Arnaud Doucet, Pierre Druihlet,
Ed Green, Feng Liang, Jean-Michel Marin, M.R.L.N. Panchanana, Fabrice
Pautot, and Judith Rousseau.
Second, working with my colleague Jean-Michel Marin on the design of
a course for teaching Bayesian Statistics from a practical and computa-
tional perspective (a venture now published as Bayesian Core by Springer
in early 2007) was a very important moment in that I realized that the
material in this very book, The Bayesian Choice, was essential in commu-
nicating the essential relevance and coherence of the Bayesian approach
1 http://www.ceremade.dauphine.fr/∼xian/books.html

viii
Preface to the Paperback Edition
through its decision-theoretic foundations, while the message contained
in the other book and transmitted only through processing datasets is
that the Bayesian methodology is a universal and multifaceted tool for
data analysis. While introducing wider and less mathematical audiences to
the elegance and simplicity of the Bayesian methodology in a shorter and
therefore more focussed volume was also necessary, if only because some
learn better from examples than from theory, I came to the conclusion that
there was no paradox in insisting on those foundations in another book! I
am therefore immensely thankful to Jean-Michel Marin for initiating this
epiphany (if I may rightly borrow this expression from Joyce!), as well as
for several years of intense collaboration. Similarly, the DeGroot Prize com-
mittee of the ISBA—International Society for Bayesian Analysis—World
meeting of 2004 in Valparaiso, Chile, greatly honored me by attributing
to The Bayesian Choice this prestigious prize. In doing so, this committee
highlighted the relevance of both foundations and implementation for the
present and future of Bayesian Statistics, when it stated that the “book sets
a new standard for modern textbooks dealing with Bayesian methods, espe-
cially those using MCMC techniques, and that it is a worthy successor to
DeGroot’s and Berger’s earlier texts”. I am quite indebted to the members
of the committee for this wonderful recognition.
Third, it has been more than 18 years since I started working with John
Kimmel from Springer New York (on a basic Probability textbook with
Arup Bose that never materialized), and I always appreciated the support
he provided over the various editions of the books. So, when he presented me
with the possibility to publish this paperback edition, I ﬁrst got some mixed
feelings, because he made me feel like a classics author! This caused my kids
poking endless fun at me and, in the end, I am quite grateful to John for the
opportunity to teach from this book to a wider audience and thus hopefully
exposing them to the beauty of Bayesian theory and methodology. Short
of embarking upon a translation of The Bayesian Choice into Chinese or
Arabic, I do not think there is much more he could do to support the book!
In Memoriam
This is a sheer consequence of time moving on, unfortunately, but I lost
another dear friend since the last publication of The Bayesian Choice. Jos´e
Sam Lazaro passed away last Spring: a mathematician, a professor and a
colleague at the Universit´e de Rouen, a music addict and a movie aﬁcionado
that made me discover Der Tod und das M¨adchen as well as The Night of
The Hunter, an intense piano player, a memorable tale teller, he was above
all a philosopher and a friend. Although he would have made a joke out of
it, I would like to dedicate this edition to his memory and wish him well
to play this ﬁnal and endless sonata...
Valencia and Paris
Christian P. Robert
February 2007

Preface to the Second Edition
“You can never know everything,” Lan said quietly, “and part of what you
know is always wrong. Perhaps even the most important part. A portion of
wisdom lies in knowing that. A portion of courage lies in going on anyway.”
Robert Jordan, Winter’s Heart, Book IX of the Wheel of Time.
Overview of Changes
Why a second edition? When thinking about it, this is more like a third
edition, since the previous edition of The Bayesian Choice was the transla-
tion of the French version, and already included updates and corrections.
The reasons for a new edition of the book are severalfold. The Bayesian
community has grown at an incredible pace since 1994. The previous version
not only overlooks important areas in the ﬁeld but misses the signiﬁcant
advances that have taken place in the last seven years.
Firstly, the MCMC2 revolution has fueled considerable advances in Bayesian
modeling, with applications ranging from medical Statistics, to signal pro-
cessing, to ﬁnance. While present in the 1994 edition, these methods were
not emphasized enough: for instance, MCMC methods were not presented
until the penultimate chapter.
Another signiﬁcant advance that needed attention is the development
of new testing approaches and, more generally, of model choice tools in
connection with, and as a result of, MCMC techniques such as reversible
jump. Other important expansions include hierarchical and dynamic mod-
els, whose processing only began to emerge in the early 1990s.
This second edition is not revolutionary, compared with the 1994 edition.
It includes, however, important advances that have taken place since then.
The only new chapter deals with model choice (Chapter 7) and is isolated
from general testing theory (Chapter 5), since model choice is indeed a
diﬀerent problem and also because it calls for new, mostly computational,
2 MCMC stands for Markov chain Monte Carlo, a simulation methodology which was
(re)discovered in the early 1990s by the Bayesian community.

x
Preface to the Second Edition
tools. For this reason, and also to emphasize the increasing importance
of computational techniques, Chapter 6—previously Chapter 9—has been
placed earlier in the book, after the presentation of the fundamentals of
Bayesian Statistics. Chapter 6 could almost be called a new chapter in
that its presentation has been deeply renovated in the light of ten years
of MCMC practice. In Chapter 3, the material on noninformative priors
has been expanded and includes, in particular, matching priors, since the
research activity has been quite intense in this area in the past few years.
Chapter 4 still deals with general estimation problems, but I have incorpo-
rated a new section on dynamic models, since those are quite central to the
development of Bayesian Statistics in applied ﬁelds such as signal process-
ing, ﬁnance and econometrics. Despite Delampady’s criticisms of Chapter
11 in The Mathematical Reviews, I have decided to leave this chapter in:
it does not hurt, when one is ﬁnished reading a book, to take an overall
and more philosophical view of the topic because the reader has very likely
acquired enough perspective to understand such arguments. (In a strictly
textbook implementation, this chapter can be suggested as an additional
reading, comparable with the Notes.)
Another noteworthy change from the previous edition is the decreased
emphasis on decision-theoretic principles. Although I still believe that sta-
tistical procedures must be grounded on such principles, the developments
in the previous decade have mainly focused on methodology, including com-
putational methodology, rather than attacking broader and more ambitious
decision problems (once again, including computational methodology). The
second part of the book (starting with Chapter 6) is therefore less decision-
theoretic and, in contrast to others, chapters such as Chapters 8 and 9 have
hardly been changed.
At a more typographical level, subsections and separations have been
introduced in many sections to improve visibility and reading, and more
advanced or more sketchy parts have been relegated to a Notes section at
the end of each chapter, following the approach adopted in Monte Carlo
Statistical Methods, written with George Casella. The end of an example is
associated with the ∥symbol, while the end of a proof is indicated by the
22 symbol.
Several books on Bayesian Statistics have appeared in the interim, among
them Bernardo and Smith (1994), Carlin and Louis (1996, 2000a), Gelman
et al. (1996), O’Hagan (1994), and Schervish (1995). However, these books
either emphasize deeper theoretical aspects at a higher mathematical level
(Bernardo and Smith (1994), O’Hagan (1996), or Schervish (1996)) and are
thus aimed at a more mature audience than this book, or they highlight
a diﬀerent vision of the practice of Bayesian Statistics (Carlin and Louis
(2000a) or Gelman et al. (1996)), missing for instance the connection with
Decision Theory developed in this book.

Preface to the Second Edition
xi
Course Schedules
My advice about running a course based on this book has hardly changed.
In a ﬁrst course on Bayesian analysis, the basic chapters (Chapters 1–6)
should be covered almost entirely, with the exception of the Notes and Sec-
tions 4.5 and 5.4, while a course focusing more on Decision Theory could
skip parts of Chapters 1 and 3, and Chapter 4 altogether, to cover Chap-
ters 7–9. For a more advanced curriculum for students already exposed
to Bayesian Statistics, my suggestion is ﬁrst to cover the impropriety is-
sue in Section 1.5, the noninformative priors in Section 3.5, the dynamic
models in Section 4.5 and Notes 4.7.3 and 4.7.4. I would also spend time
on the testing issues of Chapter 5 (with the possible exception of Sections
5.3 and 5.4). Then, after a thorough coverage of simulation methods via
Chapter 6, I would move to the more controversial topic of model choice in
Chapter 7, to recent admissibility results as in Section 8.2.5 and Note 8.7.1,
and to the hierarchical and empirical modelings of Chapter 10. In this later
scenario, the Notes should be most helpful for setting out reading seminars.
Thanks
I have always been of two minds about including a thank-you section in
a book: on the one hand, it does not mean anything to most readers, ex-
cept maybe to bring to light some of the author’s idiosyncrasies that might
better remain hidden! It may also antagonize some of those concerned be-
cause they are not mentioned, or because they are not mentioned according
to their expectations, or even because they are mentioned! On the other
hand, the core of ethical requirements for intellectual works is that sources
should be acknowledged. This extends to suggestions that contributed to
making the work better, clearer or simply diﬀerent. And it is a small token
of gratitude to the following people for the time spent on the successive
drafts of this edition that their eﬀorts should be acknowledged in print for
all to behold!
Although this is “only” a revision, the time spent on this edition was
mostly stolen from evenings, early mornings and week-ends, that is from
Brigitte, Joachim and Rachel’s time! I am thus most grateful to them
for reading and playing (almost) quietly while I was typing furiously and
searching desperately through piles of material for this or that reference,
and for listening to Bartoli and Gudj´onsson, rather than to Manau or Di-
ana Krall! I cannot swear this book-writing experience will never happen
again but, in the meanwhile, I promise there will be more time available for
reading Mister Bear to the Rescue, and for besieging the Playmobil castle
in full scale, for playing chess and for biking on Sunday afternoons!
I am thankful to several people for the improvements in the current
edition! First, I got a steady stream of feedback and suggestions from
those who taught from the book. This group includes Ed Green, Tatsuya
Kubokawa, and Marty Wells. In particular, Judith Rousseau, radical biker

xii
Preface to the Second Edition
and Jordanite as well as Bayesian, deﬁnitely was instrumental in the re-
organization of Chapter 3. I also got many helpful comments from many
people, including the two “Cambridge Frenchies” Christophe Andrieu and
Arnaud Doucet (plus a memorable welcome for a retreat week in Cam-
bridge to ﬁnish Chapter 6), Jim Berger (for his support in general, and for
providing preprints on model choice in particular), Olivier Capp´e (who also
installed Linux on my laptop, and consequently brought immense freedom
for working on the book anywhere, from the sand-box to the subway, and,
lately, to CREST, where Unix is now banned!), Maria DeIorio, Jean-Louis
Fouley, Malay Ghosh (through his very supportive review in JASA), Jim
Hobert (who helped in clarifying Chapters 6 and 10), Ana Justel, Stephen
Lauritzen (for pointing out mistakes with Wishart distributions), Anne
Philippe, Walter Racugno (who gave me the opportunity to teach an ad-
vanced class in model choice in Ca’liari last fall, thus providing the core
of Chapter 7), Adrian Raftery, Anne Sullivan Rosen (about the style of
this preface), and Jean-Michel Zakoian (for his advice on the new parts on
dynamic models). I also take the opportunity to thank other friends and
colleagues such as George Casella, J´erˆome Dupuis, Merrilee Hurn, Kerrie
Mengersen, Eric Moulines, Alain Monfort, and Mike Titterington, since
working with them gave me a broader vision of the ﬁeld, which is hope-
fully incorporated in this version. In particular, the experience of writing
Monte Carlo Statistical Methods with George Casella in the past years left
its mark on this book, not only through the style ﬁle and the inclusion of
Notes, but also as a sharper focus on essentials. Manuela Delbois helped
very obligingly with the transformation from TEX to LATEX, and with the
subsequent additions and indexings. And, last but not least, John Kimmel
and Jenny Wolkowicki, from Springer-Verlag, have been very eﬃcient and
helpful in pushing me to write this new edition for the former, in keeping
the whole schedule under control and in getting the book published on
time for the latter. Needless to say, the usual proviso applies: all remaining
typos, errors, confusions and obscure statements are mine and only mine!
In Memoriam
A most personal word about two people whose absence has marked this
new edition: in the summer 1997, I lost my friend Costas Goutis in a diving
accident in Seattle. By no means am I the only one to feel keenly his
absence, but, beyond any doubt, this book would have beneﬁted from his
vision, had he been around. Two summers later, in 1999, Bernhard K.
Flury died in a mountain accident in the Alps. While the criticisms of our
respective books always focussed on the cover colors, even to the extent of
sending one another pirated versions in the “right” color, the disappearance
of his unique humor has taken a measure of fun out of the world.
Paris, France
Christian P. Robert
March 2001

Preface to the First Edition
From where we stand, the rain seems random.
If we could stand somewhere else, we would see the order in it.
— T. Hillerman (1990) Coyote Waits.
This book stemmed from a translation of a French version that was written
to supplement the gap in the French statistical literature about Bayesian
Analysis and Decision Theory. As a result, its scope is wide enough to cover
the two years of the French graduate Statistics curriculum and, more gener-
ally, most graduate programs. This book builds on very little prerequisites
in Statistics and only requires basic skills in calculus, measure theory, and
probability. Intended as a preparation of doctoral candidates, this book
goes far enough to cover advanced topics and modern developments of
Bayesian Statistics (complete class theorems, the Stein eﬀect, hierarchical
and empirical modelings, Gibbs sampling, etc.). As usual, what started as
a translation eventually ended up as a deeper revision because of the com-
ments of French readers, adjustments to the diﬀerent needs of American
programs, and because my perception of things has changed slightly in the
meantime. As a result, this new version is quite adequate for a general
graduate audience of an American university.
In terms of level and existing literature, this book starts at a level similar
to those of the introductory books of Lee (1989) and Press (1989), but it
also goes further and keeps up with most of the recent advances in Bayesian
Statistics, while justifying the theoretical appeal of the Bayesian approach
on decision-theoretic grounds. Nonetheless, this book diﬀers from the refer-
ence book of Berger (1985a) by including the more recent developments of
the Bayesian ﬁeld (the Stein eﬀect for spherically symmetric distributions,
multiple shrinkage, loss estimation, decision theory for testing and conﬁ-
dence regions, hierarchical developments, Bayesian computation, mixture
estimation, etc.). Moreover, the style is closer to that of a textbook in the
sense that the progression is intended to be linear. In fact, the exposition of

xiv
Preface to the First Edition
the advantages of a Bayesian approach and of the existing links with other
axiomatic systems (ﬁducial theory, maximum likelihood, frequentist theory,
invariance, etc.) does not prevent an overall unity in the discourse. This
should make the book easier to read by students; through the years and on
both sides of the blackboard(!), I found most Statistics courses disturbing
because a wide scope of methods was presented simultaneously with very
little emphasis on ways of discriminating between competing approaches.
In particular, students with a strong mathematical background are quite
puzzled by this multiplicity of theories since they have not been exposed
previously to conﬂicting systems of axioms. A unitarian presentation that
includes other approaches as limiting cases is thus more likely to reassure
the students, while giving a broad enough view of Decision Theory and
even of parametric Statistics.
The plan3 of the book is as follows: Chapter 1 is an introduction to statis-
tical models, including the Bayesian model and some connections with the
Likelihood Principle. The book then proceeds with Chapter 2 on Decision
Theory, considered from a classical point of view, this approach being jus-
tiﬁed through the axioms of rationality and the need to compare decision
rules in a coherent way. It also includes a presentation of usual losses and
a discussion of the Stein eﬀect. Chapter 3 gives the corresponding analysis
for prior distributions and deals in detail with conjugate priors, mixtures of
conjugate priors, and noninformative priors, including a concluding section
on prior robustness. Classical statistical models are studied in Chapter 4,
paying particular attention to normal models and their relations with linear
regression. This chapter also contains a section on sampling models that
allows us to include the pedagogical example of capture-recapture models.
Tests and conﬁdence regions are considered separately in Chapter 5, since
we present the usual construction through 0 −1 losses, but also include
recent advances in the alternative decision-theoretic evaluations of testing
problems. The second part of the book dwells on more advanced topics
and can be considered as providing a basis for a more advanced graduate
course. Chapter 8 covers complete class results and suﬃcient/necessary ad-
missibility conditions. Chapter 9 introduces the notion of invariance and
its relations with Bayesian Statistics, including a heuristic section on the
Hunt–Stein theorem. Hierarchical and empirical extensions of the Bayesian
approach, including some developments on the Stein eﬀect, are treated in
Chapter 10. Chapter 6 is quite appealing, considering the available litera-
ture, as it incorporates in a graduate textbook an introduction to state-of-
the-art computational methods (Laplace, Monte Carlo and, mainly, Gibbs
sampling). In connection with this chapter, a short appendix provides the
usual pseudo-random generators. Chapter 11 is a more personal conclusion
on the advantages of Bayesian theory, also mentioning the most common
criticisms of the Bayesian approach. French readers may appreciate that
3 The chapter and section numbers have been adapted to the current edition.

Preface to the First Edition
xv
a lot of eﬀort has been put into the exercises of each chapter in terms of
volume and diﬃculty. They now range from very easy to diﬃcult, instead
of being uniformly diﬃcult! The most diﬃcult exercises are indexed by
asterisks and are usually derived from research papers (covering subjects
such as spherically symmetric distributions (1.1), the Pitman nearness criti-
cism (2.57–2.62), marginalization paradoxes (3.44–3.50), multiple shrinkage
(10.38), etc.). They should beneﬁt most readers by pointing out new direc-
tions of Bayesian research and providing additional perspectives.
A standard one-semester course should cover the ﬁrst ﬁve chapters (with
the possible omission of Note 2.8.2, §2.5.4, §2.6, §3.4, Note 4.7.1, §4.3.3, and
§5.4). More advanced (or longer) courses can explore the material presented
in Chapters 8, 9, and 10, bearing in mind that a detailed and rigorous treat-
ment of these topics requires additional reading of the literature mentioned
in those chapters. In any case, I would advise against entirely forgoing
Chapter 6. Even a cursory reading of this chapter may be beneﬁcial to
most students, by illustrating the practical diﬃculties related to the com-
putation of Bayesian procedures and the corresponding answers brought
by simulation methods.
This book took many excruciatingly small steps and exacted a heavy toll
on evenings, weekends, and vacations. . . It is thus only a small indication of
my gratitude that this book be dedicated to Brigitte (although she might
take this as a propitiatory attempt for future books!!!). Many persons are
to be thanked for the present version of this book. First and foremost, Jim
Berger’s “responsibility” can be traced back to 1987 when he invited me
to Purdue University for a year and, as a result, considerably broadened
my vision of Statistics; he emphasized his case by insisting very vigorously
that I translate the French version and urging me along the whole time.
My gratitude to Jim goes very deep when I consider his strong inﬂuence
in my “coming-of-age” as a statistician. Mary-Ellen Bock, Anirban Das
Gupta, Edward George, Gene (formerly Jiunn) Hwang, and Marty Wells
were also very instrumental in my progression towards the Bayesian choice,
although they do not necessarily support this choice. In this regard, George
Casella must be singled out for his strong inﬂuence through these years of
intense collaboration and friendship, even during his most severe (and “un-
bearable”) criticisms of the Bayesian paradigm! I am also quite grateful to
Jean-Fran¸cois Angers, Dean Foster, and Giovanni Parmigiani for taking the
risk of using a preliminary version of these notes in their courses, as well as
for their subsequent comments. Thanks to Teena Seele for guiding my ﬁrst
steps in TEX, as well as some delicate points in this book—never use \def as
an abbreviation of deﬁnition! I am also grateful to Elsevier North-Holland
for granting me permission to use Diaconis and Ylvisaker’s (1985) ﬁgures
in §3.3. Last, and deﬁnitely not least, Kerrie Mengersen and Costas Goutis
put a lot of time and eﬀort reading through a preliminary version and pro-
vided many helpful comments on content, style, and clarity, while adding a

xvi
Preface to the First Edition
touch of Ausso-Greek accent to the tone. (In addition, Costas Goutis saved
the subject index from utter destruction!) They are thus partly responsi-
ble for the improvements over previous versions (but obviously not for the
remaining defects!), and I am most grateful to them for their essential help.
Paris, France
Christian P. Robert
May 1994

Contents
Preface to the Paperback Edition
vii
Preface to the Second Edition
ix
Preface to the First Edition
xiii
List of Tables
xxiii
List of Figures
xxv
1
Introduction
1
1.1
Statistical problems and statistical models
1
1.2
The Bayesian paradigm as a duality principle
8
1.3
Likelihood Principle and Suﬃciency Principle
13
1.3.1
Suﬃciency
13
1.3.2
The Likelihood Principle
15
1.3.3
Derivation of the Likelihood Principle
18
1.3.4
Implementation of the Likelihood Principle
19
1.3.5
Maximum likelihood estimation
20
1.4
Prior and posterior distributions
22
1.5
Improper prior distributions
26
1.6
The Bayesian choice
31
1.7
Exercises
31
1.8
Notes
45
2
Decision-Theoretic Foundations
51
2.1
Evaluating estimators
51
2.2
Existence of a utility function
54
2.3
Utility and loss
60
2.4
Two optimalities: minimaxity and admissibility
65
2.4.1
Randomized estimators
65
2.4.2
Minimaxity
66
2.4.3
Existence of minimax rules and maximin strategy
69
2.4.4
Admissibility
74
2.5
Usual loss functions
77
2.5.1
The quadratic loss
77
2.5.2
The absolute error loss
79

xviii
Contents
2.5.3
The 0 −1 loss
80
2.5.4
Intrinsic losses
81
2.6
Criticisms and alternatives
83
2.7
Exercises
85
2.8
Notes
96
3
From Prior Information to Prior Distributions
105
3.1
The diﬃculty in selecting a prior distribution
105
3.2
Subjective determination and approximations
106
3.2.1
Existence
106
3.2.2
Approximations to the prior distribution
108
3.2.3
Maximum entropy priors
109
3.2.4
Parametric approximations
111
3.2.5
Other techniques
113
3.3
Conjugate priors
113
3.3.1
Introduction
113
3.3.2
Justiﬁcations
114
3.3.3
Exponential families
115
3.3.4
Conjugate distributions for exponential families
120
3.4
Criticisms and extensions
123
3.5
Noninformative prior distributions
127
3.5.1
Laplace’s prior
127
3.5.2
Invariant priors
128
3.5.3
The Jeﬀreys prior
129
3.5.4
Reference priors
133
3.5.5
Matching priors
137
3.5.6
Other approaches
140
3.6
Posterior validation and robustness
141
3.7
Exercises
144
3.8
Notes
158
4
Bayesian Point Estimation
165
4.1
Bayesian inference
165
4.1.1
Introduction
165
4.1.2
MAP estimator
166
4.1.3
Likelihood Principle
167
4.1.4
Restricted parameter space
168
4.1.5
Precision of the Bayes estimators
170
4.1.6
Prediction
171
4.1.7
Back to Decision Theory
173
4.2
Bayesian Decision Theory
173
4.2.1
Bayes estimators
173
4.2.2
Conjugate priors
175
4.2.3
Loss estimation
178
4.3
Sampling models
180
4.3.1
Laplace succession rule
180

Contents
xix
4.3.2
The tramcar problem
181
4.3.3
Capture-recapture models
182
4.4
The particular case of the normal model
186
4.4.1
Introduction
186
4.4.2
Estimation of variance
187
4.4.3
Linear models and G–priors
190
4.5
Dynamic models
193
4.5.1
Introduction
193
4.5.2
The AR model
196
4.5.3
The MA model
198
4.5.4
The ARMA model
201
4.6
Exercises
201
4.7
Notes
216
5
Tests and Conﬁdence Regions
223
5.1
Introduction
223
5.2
A ﬁrst approach to testing theory
224
5.2.1
Decision-theoretic testing
224
5.2.2
The Bayes factor
227
5.2.3
Modiﬁcation of the prior
229
5.2.4
Point-null hypotheses
230
5.2.5
Improper priors
232
5.2.6
Pseudo-Bayes factors
236
5.3
Comparisons with the classical approach
242
5.3.1
UMP and UMPU tests
242
5.3.2
Least favorable prior distributions
245
5.3.3
Criticisms
247
5.3.4
The p-values
249
5.3.5
Least favorable Bayesian answers
250
5.3.6
The one-sided case
254
5.4
A second decision-theoretic approach
256
5.5
Conﬁdence regions
259
5.5.1
Credible intervals
260
5.5.2
Classical conﬁdence intervals
263
5.5.3
Decision-theoretic evaluation of conﬁdence sets
264
5.6
Exercises
267
5.7
Notes
279
6
Bayesian Calculations
285
6.1
Implementation diﬃculties
285
6.2
Classical approximation methods
293
6.2.1
Numerical integration
293
6.2.2
Monte Carlo methods
294
6.2.3
Laplace analytic approximation
298
6.3
Markov chain Monte Carlo methods
301
6.3.1
MCMC in practice
302

xx
Contents
6.3.2
Metropolis–Hastings algorithms
303
6.3.3
The Gibbs sampler
307
6.3.4
Rao–Blackwellization
309
6.3.5
The general Gibbs sampler
311
6.3.6
The slice sampler
315
6.3.7
The impact on Bayesian Statistics
317
6.4
An application to mixture estimation
318
6.5
Exercises
321
6.6
Notes
334
7
Model Choice
343
7.1
Introduction
343
7.1.1
Choice between models
344
7.1.2
Model choice: motives and uses
347
7.2
Standard framework
348
7.2.1
Prior modeling for model choice
348
7.2.2
Bayes factors
350
7.2.3
Schwartz’s criterion
352
7.2.4
Bayesian deviance
354
7.3
Monte Carlo and MCMC approximations
356
7.3.1
Importance sampling
356
7.3.2
Bridge sampling
358
7.3.3
MCMC methods
359
7.3.4
Reversible jump MCMC
363
7.4
Model averaging
366
7.5
Model projections
369
7.6
Goodness-of-ﬁt
374
7.7
Exercises
377
7.8
Notes
386
8
Admissibility and Complete Classes
391
8.1
Introduction
391
8.2
Admissibility of Bayes estimators
391
8.2.1
General characterizations
391
8.2.2
Boundary conditions
393
8.2.3
Inadmissible generalized Bayes estimators
395
8.2.4
Diﬀerential representations
396
8.2.5
Recurrence conditions
398
8.3
Necessary and suﬃcient admissibility conditions
400
8.3.1
Continuous risks
401
8.3.2
Blyth’s suﬃcient condition
402
8.3.3
Stein’s necessary and suﬃcient condition
407
8.3.4
Another limit theorem
407
8.4
Complete classes
409
8.5
Necessary admissibility conditions
412
8.6
Exercises
416

Contents
xxi
8.7
Notes
425
9
Invariance, Haar Measures, and Equivariant Estimators
427
9.1
Invariance principles
427
9.2
The particular case of location parameters
429
9.3
Invariant decision problems
431
9.4
Best equivariant noninformative distributions
436
9.5
The Hunt–Stein theorem
441
9.6
The role of invariance in Bayesian Statistics
445
9.7
Exercises
446
9.8
Notes
454
10 Hierarchical and Empirical Bayes Extensions
457
10.1 Incompletely Speciﬁed Priors
457
10.2 Hierarchical Bayes analysis
460
10.2.1 Hierarchical models
460
10.2.2 Justiﬁcations
462
10.2.3 Conditional decompositions
465
10.2.4 Computational issues
468
10.2.5 Hierarchical extensions for the normal model
470
10.3 Optimality of hierarchical Bayes estimators
474
10.4 The empirical Bayes alternative
478
10.4.1 Nonparametric empirical Bayes
479
10.4.2 Parametric empirical Bayes
481
10.5 Empirical Bayes justiﬁcations of the Stein eﬀect
484
10.5.1 Point estimation
485
10.5.2 Variance evaluation
487
10.5.3 Conﬁdence regions
488
10.5.4 Comments
490
10.6 Exercises
490
10.7 Notes
502
11 A Defense of the Bayesian Choice
507
A Probability Distributions
519
A.1 Normal distribution, Np(θ, Σ)
519
A.2 Gamma distribution, G(α, β)
519
A.3 Beta distribution, Be(α, β)
519
A.4 Student’s t-distribution, Tp(ν, θ, Σ)
520
A.5 Fisher’s F-distribution, F(ν, ϱ)
520
A.6 Inverse gamma distribution, IG(α, β)
520
A.7 Noncentral chi-squared distribution, χ2
ν(λ)
520
A.8 Dirichlet distribution, Dk(α1, . . . , αk)
521
A.9 Pareto distribution, Pa(α, x0)
521
A.10 Binomial distribution, B(n, p).
521
A.11 Multinomial distribution, Mk(n; p1, . . . , pk)
521

xxii
Contents
A.12 Poisson distribution, P(λ)
521
A.13 Negative Binomial distribution, Neg(n, p)
522
A.14 Hypergeometric distribution, Hyp(N; n; p)
522
B Usual Pseudo-random Generators
523
B.1 Normal distribution, N(0, 1)
523
B.2 Exponential distribution, Exp(λ)
523
B.3 Student’s t-distribution, T (ν, 0, 1)
524
B.4 Gamma distribution, G(α, 1)
524
B.5 Binomial distribution, B(n, p)
525
B.6 Poisson distribution, P(λ)
525
C Notations
527
C.1 Mathematical
527
C.2 Probabilistic
528
C.3 Distributional
528
C.4 Decisional
529
C.5 Statistical
529
C.6 Markov chains
530
References
531
Author Index
579
Subject Index
587

List of Tables
2.4.1 Utility function
69
3.2.1 Capture and survival information
107
3.2.2 Capture and survival priors
108
3.2.3 Ranges of posterior moments
112
3.3.1 Natural conjugate priors
121
3.5.1 Matching reference priors
140
3.8.1 Approximations by conjugate mixtures
161
4.2.1 Bayes estimators for exponential families
176
4.3.1 Probabilities of capture
182
4.3.2 Population capture division
183
4.3.3 Posterior deer distribution
185
4.3.4 Posterior mean deer population
185
4.3.5 Estimated deer population
186
5.2.1 Posterior probabilities of p = 1/2
232
5.2.2 Posterior probabilities of θ = 0
232
5.2.3 Posterior probabilities of θ = 0
233
5.2.4 Posterior probabilities of |θ| < 1.
233
5.2.5 Posterior probabilities of θ = 0
234
5.2.6 Posterior probabilities of θ = 0
235
5.3.1 Comparison between p-values and Bayesian answers
252
5.3.2 Comparison between p-values and Bayesian answers
253
5.3.3 Bayes factors and posterior probabilities
254
5.3.4 Comparison between p-values and Bayesian posterior
probabilities
255
5.5.1 Conﬁdence intervals for the binomial distribution.
261
6.1.1 Parameters of a lung radiograph mode
290
6.5.1 Frequencies of car passages
333
7.1.1 Orange tree circumferences
346
7.3.1 Adequacy for the four orange models
361
7.5.1 Parameters for Kullback–Leibler divergences
371
7.5.2 Submodels for the breast-cancer dataset
373
7.7.1 Number of women in a queue
385

xxiv
List of Tables
10.2.1 Posterior probabilities and 95% conﬁdence intervals
470
10.6.1 Car-buying intentions of households
495
10.6.2 Car acquisitions and intentions
496

List of Figures
1.1.1 Monthly unemployment vs. accidents
4
1.1.2 Histogram of a chest radiograph
5
2.4.1 Comparison of risks
68
2.4.2 Bernoulli Risk set
72
3.3.1 Densities of IN(α, μ, τ)
119
3.4.1 Three priors for a spinning coin
124
3.4.2 Posterior distributions for 10 coins observations
125
3.4.3 Posterior distributions for 50 coin observations
125
4.1.1 Bayesian and frequentist error evaluations
172
4.5.1 Averaged IBM stock prices
196
4.7.1 Two priors on ϱ
218
4.7.2 Sample from the stochastic volatility model
219
4.7.3 Allocations for the stochastic volatility model
220
5.2.1 Intrinsic prior for exponential testing
239
6.2.1 Variation of Monte Carlo approximations
297
6.3.1 Path of a Markov chain for the repulsive normal model
306
6.3.2 Histograms from the beta-binomial distribution
310
7.1.1 Histogram of the galaxy dataset
345
7.3.1 Sequence of simulated numbers of components
366
8.4.1 Risk set and admissible estimators
410
10.1.1 DAG for the HIV model
459
10.2.1 Convergence assessment for the rat experiment
469
10.2.2 Gibbs samples for the rat experiment
470

CHAPTER 1
Introduction
“Sometimes the Pattern has a randomness to it—to our eyes, at least—but
what chance that you should meet a man who could guide you in this thing,
and he one who could follow the guiding?”
Robert Jordan, The Eye of the World, Book I of the Wheel of Time.
1.1 Statistical problems and statistical models
The main purpose of statistical theory is to derive from observations of a
random phenomenon an inference about the probability distribution under-
lying this phenomenon. That is, it provides either an analysis (description)
of a past phenomenon, or some predictions about a future phenomenon of
a similar nature. In this book, we insist on the decision-oriented aspects of
statistical inference because, ﬁrst, these analysis and predictions are usu-
ally motivated by an objective purpose (whether a company should launch
a new product, a racing boat should modify its route, a new drug should be
put on the market, an individual should sell shares, etc.) having measurable
consequences (monetary results, position at the end of the race, recovery
rate of patients, beneﬁts, etc.). Second, to propose inferential procedures
implies that one should stand by them, i.e., that the statistician thinks they
are preferable to alternative procedures. Therefore, there is a need for an
evaluative tool that allows for the comparison of diﬀerent procedures; this is
the purpose of Decision Theory. As with most formal deﬁnitions, this view
of Statistics ignores some additional aspects of statistical practice such as
those related to data collection (surveys, design of experiments, etc.). This
book does, as well, although we do not want to diminish the importance of
these omitted topics.
We also insist on the fact that Statistics should be considered an interpre-
tation of natural phenomena, rather than an explanation. In fact, statistical
inference is based on a probabilistic modeling of the observed phenomenon
and implies a necessarily reductive formalization step since without this
probabilistic support it cannot provide any useful conclusion (or decision).

2
Introduction
1
Example 1.1.1 Consider the problem of forest ﬁres. They usually appear
at random, but some ecological and meteorological factors inﬂuence their
eruption. Determining the probability p of ﬁre as a function of these factors
should help in the prevention of forest ﬁres, even though such modeling is
obviously unable to lead to the eradication of forest ﬁres and cannot pos-
sibly encompass all the factors involved. A more reductive approach is to
assume a parametrized shape for the function p, including physical con-
straints on the inﬂuential factors. For instance, denoting by h the humidity
rate, t the average temperature, x the degree of management of the forest,
a logistic model
p = exp(α1h + α2t + α3x)/ [1 + exp(α1h + α2t + α3x)]
could be proposed, the statistical step dealing with the evaluation of the
parameters α1, α2, α3.
∥
To impose a probability modeling on unexplained phenomena seems to
be overly reductive in some cases because a given phenomenon can be
entirely deterministic, although the regulating function of the process is
unknown and cannot be recovered from the observations. This is, for in-
stance, the case with chaotic phenomena, where a deterministic sequence of
observations cannot be distinguished from a sequence of random variables,
in the sense of a statistical test (see Berg´e, Pommeau, and Vidal (1984)
and Gleick (1987) for introductions to chaos theory). Pseudo-random gen-
erators are actually based on this indeterminacy. While they use iterative
deterministic algorithms such as
at+1 = f(at),
they imitate (or simulate) rather well the behavior of a sequence of random
variables (see Devroye (1985), Gentle (1998), Robert and Casella (2004),
and Appendix B for a list of the most common generators).
Although valid on philosophical grounds, this criticism does not hold if
we consider the probabilistic modeling from the interpretation perspective
mentioned above. This modeling simultaneously incorporates the available
information about the phenomenon (inﬂuential factors, frequency, ampli-
tude, etc.) and the uncertainty pertaining to this information. It thus autho-
rizes a quantitative discourse on the problem by providing via probability
theory a genuine calculus of uncertainty going beyond the mere description
of deterministic modelings. This is why a probabilistic interpretation is
necessary for statistical inference; it provides a framework replacing a sin-
gular phenomenon in the globality of a model and thus allows for analysis
and generalizations. Far from being a misappropriation of the inferential
purposes, the imposition of a probabilistic structure that is only an ap-
proximation of reality is essential for the subsequent statistical modeling
to induce a deeper and more adequate understanding of the considered
phenomenon.

1.1
Statistical problems and statistical models
3
Obviously, probabilistic modeling can only be defended if it provides
an adequate representation of the observed phenomenon. A more down-
to-earth criticism of probabilistic modeling is therefore that, even when
a modeling is appropriate, it is diﬃcult to know exactly the probability
distribution underlying the generation of the observations, e.g., to know
that it is normal, exponential, binomial, etc., except in special cases.
Example 1.1.2
Consider a radioactive material with unknown half-life
H. For a given atom of this material, the time before disintegration fol-
lows exactly an exponential distribution1 with parameter log(2)/H. The
observation of several of these particles can then lead to an inference
about H.
∥
Example 1.1.3 In order to determine the number N of buses in a town, a
possible inferential strategy goes as follows: observe buses during an entire
day and keep track of their identifying number. Then repeat the experiment
the next day by paying attention to the number of buses been already
observed on the previous day, n. If 20 buses have been observed the ﬁrst
day and 30 the second day, n is distributed as a hypergeometric1 random
variable, H(30, N, 20/N), and the knowledge of this distribution leads, for
instance, to the approximation of N by 20(30/n). This method, called
capture-recapture, has induced numerous and less anecdotal developments
in ecology and population dynamics (see Chapter 4).
∥
We could create many other examples where the distribution of the ob-
servations is exactly known, its derivation based upon physical, economical,
etc., considerations. In the vast majority of cases, however, statistical mod-
eling is reductive in the sense that it only approximates the reality, losing
part of its richness but gaining in eﬃciency.
Example 1.1.4
Price and salary variations are closely related. One way
to represent this dependence is to assume the linear relation
ΔP = a + b ΔS + ϵ,
where ΔP and ΔS are the price and salary variations, a and b are unknown
coeﬃcients and ϵ is the error factor. A further, but drastic, reduction can
be obtained by assuming that ϵ is normally distributed because, while ϵ
is indeed a random variable, there are many factors playing a role in the
determination of prices and salaries and it is usually impossible to deter-
mine the distribution of ϵ. Nonetheless, besides a justiﬁcation through the
Central Limit Theorem (i.e., the additional inﬂuence of many small factors
of similar magnitude), this advanced modeling also allows for a more thor-
ough statistical analysis, which remains valid even if the distribution of ϵ
is not exactly normal. (See also Exercise 1.3.)
∥
1 See Appendix A for a survey of the most common distributions.

4
Introduction
1
8
10
12
14
16
15
20
25
30
35
40
45
Figure 1.1.1. Plot of monthly unemployment rate versus number of accidents (in
thousands) in Michigan, from 1978 to 1987. (Source: Lenk (1999).)
Example 1.1.5 Consider the dataset depicted by Figure 1.1, which plots
the monthly unemployment rate against the monthly number of accidents
(in thousands) in Michigan, from 1978 to 1987. Lenk (1999) argues in favor
of a connection between these two variates, in that higher unemployment
rates lead to less traﬃc on the roads and thus fewer accidents. A major
step towards reduction is then to postulate a parametric structure in the
dependence, such as the Poisson regression model
N|ϱ ∼P(exp{β0 + β1 log(ϱ)}) ,
(1.1.1)
where N denotes the number of accidents and ϱ the corresponding unem-
ployment rate. Figure 1.1 also depicts the estimated expectation IE[N|ϱ],
which tends to conﬁrm the decreasing impact of unemployment upon ac-
cidents. But the validity of the modeling (1.1.1) ﬁrst needs to be assessed,
using goodness-of-ﬁt and other model choice techniques (see Chapter 7). ∥
In some cases, the reductive eﬀect is deliberately sought as a positive
smoothing eﬀect which partly removes unimportant perturbations of the
phenomenon and often improves its analysis by highlighting the major fac-
tors, as in the following example.
Example 1.1.6 Radiographs can be represented as a 1000 × 1200 grid of
elementary points, called pixels, which are grey levels represented by num-
bers between 0 and 256. For instance, Figure 1.1.6 provides the histogram
of the grey levels for a typical chest radiograph. If we consider a pixel to be
a discrete random variable taking values in {0, 1, . . ., 256}, the histogram
gives an approximation of the distribution of this random variable. As
shown by the ﬁgure, this distribution is rather complex, but approximately
bimodal. This particularity is observed on most radiographs and suggests

1.1
Statistical problems and statistical models
5
0
50
100
150
200
0.000
0.005
0.010
Grey levels
Figure 1.1.2. Histogram of the grey levels of a chest radiograph and its modeling
by a two-component mixture. (Source: Plessis (1989).)
a modeling of the distribution through a continuous approximation by a
mixture of two normal distributions, with density
f(x) =
p
√
2πσ1
exp

−(x −μ1)2
2σ2
1

+ 1 −p
√
2πσ2
exp

−(x −μ2)2
2σ2
2

.
(1.1.2)
Obviously, this modeling considerably smoothes the histogram (see Figure
1.1.6), but also allows a description of the image through ﬁve parameters,
with no substantial loss of information. The two important modes of the
true distribution have actually been shown to correspond to two regions of
the chest, the lungs and the mediastinum. This smoothing technique is used
in an image-processing algorithm called Parametric Histogram Speciﬁcation
(see Plessis (1989)). We will consider the Bayesian estimation of mixture
distributions in detail in Section 6.4.
∥
Given this imperative of reduction of the complexity of the observed
phenomenon, two statistical approaches contend. A ﬁrst approach assumes
that statistical inference must incorporate as much as possible of this com-
plexity, and thus aims at estimating the distribution underlying the phe-
nomenon under minimal assumptions, generally using functional estimation
(density, regression function, etc.). This approach is called nonparametric.
Conversely, the parametric approach represents the distribution of the ob-
servations through a density function f(x|θ), where only the parameter θ
(of ﬁnite dimension) is unknown.
We consider that this second approach is more pragmatic, since it takes
into account that a ﬁnite number of observations can eﬃciently estimate
only a ﬁnite number of parameters. Moreover, a parametric modeling au-
thorizes an evaluation of the inferential tools for ﬁnite sample sizes, con-
trary to the more involved nonparametric methods, which are usually

6
Introduction
1
justiﬁed only asymptotically, therefore strictly only apply when the sample
size becomes inﬁnite (see, however, Field and Ronchetti (1990), who study
the applicability of asymptotic results for ﬁnite sample sizes). Of course,
some nonparametric approaches, like rank tests (Hajek and Sid`ak (1967)),
completely evacuate the estimation aspect by devising distribution-free
statistics, but their applicability is limited to testing settings.
Both approaches have their interest and we shall not try to justify any
further the parametric choice. Quite naturally, there is also an extensive
literature on model construction. See Cox (1990) and Lehmann (1990) for
references, as well as reﬂections on the very notion of a statistical model.
We will see in Chapter 7 some approaches to the comparison of models,
which can be used in the modeling stage, that is, when a model is sought
in order to ﬁt the data and several potential models contend.
In this book, we only consider parametric modeling. We assume that
the observations supporting the statistical analysis, x1, . . . , xn, have been
generated from a parametrized probability distribution, i.e., xi (1 ≤i ≤n)
has a distribution with density fi(xi|θi, x1, . . . , xi−1) on IRp, such that the
parameter θi is unknown and the function fi is known (see Exercise 1.2
about the formal ambiguity of this deﬁnition and Note 1.8.2 for indications
about the Bayesian approach to nonparametrics). This model can then be
represented more simply by
x ∼f(x|θ),
where x is the vector of the observations and θ is the set of the parameters,
θ1, . . . , θn, which may all be equal. This approach is unifying, in the sense
that it represents similarly an isolated observation, dependent observations,
and repeated independent and identically distributed (i.i.d.) observations
x1, . . . , xn from a common distribution, f(x1|θ). In the latter case, x =
(x1, . . . , xn) and
f(x|θ) =
n

i=1
f(xi|θ).
Notice that densities of discrete and continuous random variables will
be denoted identically in this book, the reference measure being generally
provided by the setting. Moreover, we use the notation “x is distributed
according to f” or “x ∼f” instead of “x is an observation from the distri-
bution with density f” for the sake of conciseness2. Most of the time, the
sample is reduced to a single observation, for simpliﬁcation reasons, but
2 This book does not follow the usual probabilistic convention that random variables
are represented by capital letters, X say, and their realization, that is, their observed
value, by the corresponding lower case letter, x, as in P (X ≤x). This is because
from a Bayesian point of view, we always condition on the realized value x and, be-
sides, consider the parameter, θ say, as a random variable: the use of capital Greek
letters then gets confusing in the extreme since Θ is rather, by convention, the pa-
rameter space. This also facilitates the use of conditional expressions, which abound
in Bayesian computations. In cases potentially provoking confusion, we will revert to
the capital-lower case convention.

1.1
Statistical problems and statistical models
7
also because we are usually dealing with distributions where the sample
size does not matter, since they allow for suﬃcient statistics of constant
dimension (see Section 1.3 and Chapter 3).
Deﬁnition 1.1.7
A parametric statistical model consists of the observa-
tion of a random variable x, distributed according to f(x|θ), where only the
parameter θ is unknown and belongs to a vector space Θ of ﬁnite dimension.
Once the statistical model is deﬁned, the main purpose of the statistical
analysis is to lead to an inference on the parameter θ. This means that
we use the observation x to improve our knowledge on the parameter θ,
so that one can take a decision related with this parameter, i.e., either
estimate a function of θ or a future event which distribution depends on θ.
The inference can deal with some components of θ, precisely (“What is the
value of θ1?”) or not (“Is θ2 larger than θ3?”). A distinction is often made
between estimation and testing problems, depending on whether the exact
value of the parameters (or of some functions of the parameters), or just
a hypothesis about these parameters, is of interest. For instance, the two
reference books of classical Statistics, Lehmann (1986) and Lehmann and
Casella (1998), deal, respectively, with each of these themes. Other authors
have proposed a more subtle distinction between estimation and evaluation
of estimation procedures (see, for instance, Casella and Berger (1990)).
More generally, inference covers the random phenomenon directed by θ
and thus includes prediction, that is, the evaluation of the distribution of a
future observation y depending on θ (and possibly the current observation
x), y ∼g(y|θ, x). As shown later, these divisions are somehow artiﬁcial,
since all inferential problems can be expressed as estimation problems when
considered from a decision-theoretic perspective.
The choice of the parametric approach made in this book can be crit-
icized, since we cannot always assume that the distribution of the obser-
vations is known up to a (ﬁnite dimensional) parameter, but we maintain
that this reduction allows for deeper developments in the inferential pro-
cess, even though this may seem a paradoxical statement. Criticisms on the
reductive aspects of the statistical approach and, a fortiori, on the para-
metric choice, are actually seconded by other criticisms about the choice of
the evaluation criteria and the whole purpose of Decision Theory, as we will
see in Chapter 2. However, we stand by these choices on the ground that
these increasingly reductive steps are minimal requirements for a statisti-
cal approach to be coherent (that is, self-consistent). Indeed, the ultimate
goal of statistical analysis is, in the overwhelming majority of cases, to
support a decision as being optimal (or at least reasonable). It is thus nec-
essary to be able to compare the diﬀerent inferential procedures at hand.
The next section presents the foundations of Bayesian statistical analysis,
which seems to us to be the most appropriate approach for this determi-
nation of optimal procedures, while also being the most coherent method3,
3 As reported in Robins and Wasserman (2000), there are several formal deﬁnitions of

8
Introduction
1
since it builds up these procedures by starting from required properties,
instead of the reverse, namely, verifying the good behavior of procedures
selected in an ad-hoc manner. The Bayesian choice, as presented in this
book, may appear as an unnecessary reduction of the inferential scope,
and it has indeed been criticized by many as being so. But we will see in
the following chapters that this reduction is both necessary and beneﬁcial.
Chapter 11 summarizes various points of a defense of the Bayesian choice,
and can be read in connection with the previous arguments4.
Notice that there also exists a Bayesian approach to nonparametric
Statistics. It usually involves prior distributions on functional spaces, such
as Dirichlet processes. See Ferguson (1973, 1974) and Escobar (1989), Es-
cobar and West (1994), Dey et al. (1998), M¨uller et al. (1999) and Note
1.8.2 for references in this area. Example 1.4.3 provides an illustration of
the interest of the Bayesian approach in this setting.
1.2 The Bayesian paradigm as a duality principle
Compared5 with probabilistic modeling, the purpose of a statistical anal-
ysis is fundamentally an inversion purpose, since it aims at retrieving the
causes—reduced to the parameters of the probabilistic generating mecha-
nism—from the eﬀects—summarized by the observations6. In other words,
when observing a random phenomenon directed by a parameter θ, statis-
tical methods allow to deduce from these observations an inference (that
is, a summary, a characterization) about θ, while probabilistic modeling
characterizes the behavior of the future observations conditional on θ. This
inverting aspect of Statistics is obvious in the notion of the likelihood func-
tion, since, formally, it is just the sample density rewritten in the proper
order,
ℓ(θ|x) = f(x|θ),
(1.2.1)
i.e., as a function of θ, which is unknown, depending on the observed value
x. Historically, the ﬁducial approach of Fisher (1956) also relies on this
inversion (see Note 1.8.1).
A general description of the inversion of probabilities is given by Bayes’s
Theorem: If A and E are events such that P(E) ̸= 0, P(A|E) and P(E|A)
are related by
P(A|E)
=
P(E|A)P(A)
P(E|A)P(A) + P(E|Ac)P(Ac)
coherence, from Savage (1954) to Heath and Sudderth (1989), which all lead to the
conclusion that a procedure is coherent if, and only if, it is Bayesian.
4 This chapter and Chapter 11 are worth re-reading once the more technical points of
the inferential process and the issues at hand have been fully understood.
5 The word paradigm, which is a grammatical term, is used here as an equivalent for
model or principles.
6 At the time of Bayes and Laplace, i.e., at the end of the eighteenth century, Statistics
was often called Inverse Probability because of this perspective. See Stigler (1986,
Chapter 3).

1.2
The Bayesian paradigm as a duality principle
9
=
P(E|A)P(A)
P(E)
.
In particular,
P(A|E)
P(B|E) = P(E|A)
P(E|B) ,
(1.2.2)
when P(B) = P(A). To derive this result through the machinery of modern
axiomatized probability theory is trivial. However, it appears as a major
conceptual step in the history of Statistics, being the ﬁrst inversion of
probabilities. Equation (1.2.2) expresses the fundamental fact that, for two
equiprobable causes, the ratio of their probabilities given a particular ef-
fect is the same as the ratio of the probabilities of this eﬀect given the two
causes. This theorem also is an actualization principle since it describes
the updating of the likelihood of A from P(A) to P(A|E) once E has been
observed. Thomas Bayes (1764) actually proved a continuous version of
this result, namely, that given two random variables x and y, with condi-
tional distribution7 f(x|y) and marginal distribution g(y), the conditional
distribution of y given x is
g(y|x) =
f(x|y)g(y)

f(x|y)g(y) dy .
While this inversion theorem is quite natural from a probabilistic point of
view, Bayes and Laplace went further and considered that the uncertainty
on the parameters θ of a model could be modeled through a probability
distribution π on Θ, called prior distribution. The inference is then based on
the distribution of θ conditional on x, π(θ|x), called posterior distribution
and deﬁned by
π(θ|x) =
f(x|θ)π(θ)

f(x|θ)π(θ) dθ .
(1.2.3)
Notice that π(θ|x) is actually proportional to the distribution of x condi-
tional upon θ, i.e., the likelihood, multiplied by the prior distribution of
θ. (It seems that the full generality of (1.2.3) was not perceived by Bayes,
but by Laplace, who developed it to a greater extent.) The main addition
brought by a Bayesian statistical model is thus to consider a probability
distribution on the parameters.
Deﬁnition 1.2.1
A Bayesian statistical model is made of a parametric
statistical model, f(x|θ), and a prior distribution on the parameters, π(θ).
In statistical terms, Bayes’s Theorem thus actualizes the information on
θ by extracting the information on θ contained in the observation x. Its
impact is based on the daring move that puts causes (observations) and
7 We will often replace distribution with density, assuming that the later is well deﬁned
with respect to a natural dominating measure, like the Lebesgue measure. It is only
in advanced settings, such as the Haar measure in Chapter 9, that a ﬁner level of
measure theory will be needed.

10
Introduction
1
eﬀects (parameters) on the same conceptual level, since both of them have
probability distributions. From a statistical modeling viewpoint, there is
thus little diﬀerence between observations and parameters, since condi-
tional manipulations allow for an interplay of their respective roles. Notice
that, historically, this perspective that parameters directing random phe-
nomena can also be perceived as random variables goes against the atheistic
determinism of Laplace8 as well as the clerical position of Bayes, who was
a nonconformist minister. By imposing this fundamental modiﬁcation to
the perception of random phenomena, these two mathematicians created
modern statistical analysis and, in particular, Bayesian analysis.
Indeed, the recourse to the prior distribution π on the parameters of a
model is truly revolutionary. There is in fact a major step from the notion
of an unknown parameter to the notion of a random parameter, and many
statisticians place an absolute boundary between the two concepts, al-
though they accept the probabilistic modeling on the observation(s). They
defend this point of view on the ground that, even though in some par-
ticular settings the parameter is produced under the simultaneous action
of many factors and can thus appear as (partly) random, as for instance,
in quantum physics, the parameter to be estimated cannot be perceived
as resulting from a random experiment in most cases. A typical setting
occurs when estimating physical quantities like the speed of light, c. An
answer in this particular setting is that the limited accuracy of the mea-
surement instruments implies that the true value of c will never be known,
and thus that it is justiﬁed to consider c as being uniformly distributed on
[c0 −ϵ, c0 + ϵ], if ϵ is the maximal precision of the measuring instruments
and c0 the obtained value.
We will consider in Chapter 3 some approaches to the delicate problem
of prior distribution determination. However, more fundamentally, we want
to stress here that the importance of the prior distribution in a Bayesian
statistical analysis is not at all that the parameter of interest θ can (or can-
not) be perceived as generated from π or even as a random variable, but
rather that the use of a prior distribution is the best way to summarize the
available information (or even the lack of information) about this param-
eter, as well as the residual uncertainty, thus allowing for incorporation of
this imperfect information in the decision process. (Similar reasoning led
Laplace to develop statistical models, despite his determinism.) A more
technical point is that the only way to construct a mathematically justi-
ﬁed approach operating conditional upon the observations is to introduce a
corresponding distribution on the parameters. See also Lindley (1990, §3)
for a detailed axiomatic justiﬁcation of the use of prior distributions.
Let us conclude this section with the historical examples of Bayes and
Laplace.
Example 1.2.2 (Bayes (1764))
A billiard ball W is rolled on a line of
8 “We must envision the present state of the Universe as the eﬀect of its anterior state
and as the cause of the following state” – Laplace (1795).

1.2
The Bayesian paradigm as a duality principle
11
length one, with a uniform probability of stopping anywhere. It stops at p.
A second ball O is then rolled n times under the same assumptions and X
denotes the number of times the ball O stopped on the left of W. Given
X, what inference can we make on p?
In modern terminology, the problem is then to derive the posterior dis-
tribution of p given X, when the prior distribution on p is uniform on [0, 1]
and X ∼B(n, p), the binomial distribution (see Appendix A). Since
P(X = x|p)
=
n
x

px(1 −p)n−x,
P(a < p < b and X = x)
=
 b
a
n
x

px(1 −p)n−xdp
and
P(X = x) =
 1
0
n
x

px(1 −p)n−x dp,
we derive that
P(a < p < b|X = x)
=
 b
a
	n
x

px(1 −p)n−x dp
 1
0
	n
x

px(1 −p)n−x dp
=
 b
a px(1 −p)n−x dp
B(x + 1, n −x + 1) ,
i.e., that the distribution of p conditional upon X = x is a beta distribution,
Be(x + 1, n −x + 1) (see Appendix A).
∥
In the same spirit, Laplace introduced a probabilistic modeling of the
parameter space. But his examples are more advanced than Bayes’s, in the
sense that the prior distributions Laplace considers are based on abstract
reasoning, instead of the physical basis of Bayes’s prior distribution.9
Example 1.2.3 (Laplace (1773)) An urn contains a number n of black
and white cards. If the ﬁrst card drawn out of the urn is white, what is the
probability that the proportion p of white cards is p0? In his resolution of
the problem, Laplace assumes that all numbers from 2 to n −1 are equally
likely values for pn, i.e., that p is uniformly distributed on {2/n, . . ., (n −
1)/n}. The posterior distribution of p can then be derived using Bayes’s
Theorem and
P(p = p0| data )
=
p0 × 1/(n −2)
(n−1)/n
p=2/n
p × 1/(n −2)
=
n p0
n(n −1)/2 −1.
∥
9 It is also possible to picture a more Machiavellian Bayes who picked up this particular
example in order to circumvent potential criticisms of the choice of the prior. But it
seems that this was not the case, i.e., that Bayes was actually studying this example
for its own sake. See Stigler (1986) for more details.

12
Introduction
1
The above choice of the prior distribution can obviously be attacked as
being partly arbitrary. However, in Laplace’s view of probability theory,
most events can be decomposed into elementary equiprobable events and,
therefore, in this particular case, it seems reasonable to consider the events
{p = i/n} (2 ≤i ≤n−1) as elementary events. A similar reasoning justiﬁes
the following example.
Example 1.2.4 (Laplace (1786)) Considering male and female births in
Paris, Laplace wants to test whether the probability x of a male birth is
above 1/2. For 251, 527 male and 241, 945 female births, assuming that x
has a uniform prior distribution on [0, 1], Laplace obtains
P(x ≤1/2|(251, 527; 241, 945)) = 1.15 × 10−42.
(see Stigler (1986, p. 134) and Exercise 1.6). He then deduces that this
probability x is more than likely to be above 50%. Still assuming a uniform
prior distribution on this probability, he also compares the male births
in London and Paris and deduces that the probability of a male birth is
signiﬁcantly higher in England.
∥
The following example worked out by Laplace is even more interesting
because, from a practical point of view, it provides a method of deriving
optimal procedures and, from a theoretical point of view, it is the ﬁrst
formal derivation of a Bayes estimator.
Example 1.2.5 In astronomy, one frequently gets several observations of
a quantity ξ. These measurements are independently distributed according
to a distribution that is supposed to be unimodal and symmetric around
ξ. If we put a uniform distribution on the parameter ξ, it should be a
“uniform distribution on (−∞, +∞)”, which is not deﬁned as a probability
distribution. However, if we agree on this formal extension (see Section 1.5
for a justiﬁcation), we can work with the Lebesgue measure on (−∞, +∞)
instead.
Using this generalized distribution, Laplace (1773) establishes that the
posterior median of ξ, i.e. the median for the distribution of ξ conditional
on the observations, is an optimal estimator in the sense that it minimizes
the average absolute error
IEξ[ |ξ −δ| ]
(1.2.4)
in δ, where IEξ[·] denotes the expectation under the distribution of ξ (see
Appendix C for a list of usual notations). This result justiﬁes the use of
the posterior median as an estimator of ξ, whatever the distribution of
the observation. Although established more than two centuries ago, it is
strikingly modern (generality of the distribution, choice of a loss function
to evaluate the estimators) and Laplace extended it in 1810 by establishing
a similar result for squared error.
Surprisingly, though, Laplace was rather unsatisﬁed with this result be-
cause he still needed the distribution of the observation error to be able to
calculate the resulting estimator. He ﬁrst considered, in 1774, the double

1.3
Likelihood Principle and Suﬃciency Principle
13
exponential distribution
ϕξ(x) = ξ
2e−ξ|x|,
x ∈IR, ξ > 0,
(1.2.5)
also called the Laplace distribution, which supposedly involved the resolu-
tion of a ﬁfteenth degree equation for three observations. (Actually, Laplace
made a mistake and the correct equation is cubic, as shown by Stigler
(1986).) Then, in 1777, he looked at the even less tractable alternative
ϕξ(x) = 1
2ξ log (ξ/|x|) II|x|≤ξ,
ξ > 0,
where II denotes the indicator function. It was only in 1810 when Legendre
and Gauss independently exposed the importance of the normal distribu-
tion, that Laplace was able to compute his (Bayes) estimators explicitly,
since he then thought this was the ideal error distribution.
∥
We will consider again this example, along with other optimality results,
in Chapter 2, when we study diﬀerent loss functions to evaluate estima-
tion procedures and the associated Bayes estimators. Let us stress here
that the main consequence of the Bayes and Laplace works has been to
introduce the conditional perspective in Statistics, i.e., to realize that pa-
rameters and observations are fundamentally identical objects, albeit dif-
ferently perceived.10 To construct in parallel a probability distribution on
the parameter space completes this equivalence and, through Bayes’s The-
orem, allows a quantitative discourse on the causes, i.e., in our parametric
framework an inference on the parameters. As mentioned above, the choice
of the prior distribution is delicate, but its determination should be incor-
porated into the statistical process in parallel with the determination of
the distribution of the observation. Indeed, a prior distribution is the best
way to include residual information into the model. In addition, Bayesian
statistical analysis provides natural tools to incorporate the uncertainty
associated with this information in the prior distribution (possibly through
a hierarchical modeling, see Chapter 10). Lastly, as pointed out in Lindley
(1971), the Bayesian paradigm is intrinsically logical: given a set of required
properties represented by the loss function and the prior distribution, the
Bayesian approach provides estimators satisfying these requirements, while
other approaches evaluate the properties of estimators derived for reasons
external to the inferential framework.
1.3 Likelihood Principle and Suﬃciency Principle
1.3.1 Suﬃciency
Classical Statistics can be envisaged as being directed by principles often
justiﬁed by “common sense” or additional axioms. On the contrary, the
10 Again, this is why this book indistinctly writes random variables, observations and
parameters in lower case.

14
Introduction
1
Bayesian approach naturally incorporates most of these principles with no
restraint on the procedures to be considered, and also deﬁnitely rejects
other principles, such as unbiasedness. This notion once was a cornerstone
of Classical Statistics and restricted the choice of estimators to those who
are on average correct (see Lehmann and Casella (1998)). While intuitively
acceptable, it imposes too stringent conditions on the choice of the pro-
cedures and often leads to ineﬃciency in their performances. (See, e.g.,
the Stein eﬀect in Note 2.8.2.) More importantly, the number of problems
which allow for unbiased solutions is a negligible percentage of all estima-
tion problems (Exercise 1.12). Despite these drawbacks, a recent statistical
technique like the bootstrap (Efron (1982), Hall (1992)) was introduced as
a way to (asymptotically) reduce the bias.
Two fundamental principles are followed by the Bayesian paradigm,
namely the Likelihood Principle and the Suﬃciency Principle.
Deﬁnition 1.3.1
When x ∼f(x|θ), a function T of x (also called a
statistic) is said to be suﬃcient if the distribution of x conditional upon
T (x) does not depend on θ.
A suﬃcient statistic T (x) contains the whole information brought by x
about θ. According to the factorization theorem, under some measure theo-
retic regularity conditions (see Lehmann and Casella (1998)), the density
of x can then be written as
f(x|θ) = g(T (x)|θ)h(x|T (x)),
if g is the density of T (x). We will see in Chapter 2 that, when an estima-
tor is evaluated under a convex loss, the optimal procedures only depend
on suﬃcient statistics (this is the Rao–Blackwell Theorem). In particular,
when the model allows for a minimal suﬃcient statistic, i.e., for a suﬃcient
statistic that is a function of all the other suﬃcient statistics, we only have
to consider the procedures depending on this statistic or equivalently the
restricted statistical model associated with this statistic. The concept of
suﬃciency has been developed by Fisher and is associated with the follow-
ing principle.
Suﬃciency Principle Two observations x and y factorizing through the
same value of a suﬃcient statistic T , that is, such that T (x) = T (y), must
lead to the same inference on θ.
Example 1.3.2 Consider x1, . . . , xn independent observations from a nor-
mal distribution N(μ, σ2) (see Appendix A). The factorization theorem
then implies that the pair T (x) = (¯x, s2), where
¯x = 1
n
n

i=1
xi
and
s2 =
n

i=1
(xi −¯x)2,

1.3
Likelihood Principle and Suﬃciency Principle
15
is a suﬃcient statistic for the parameter (μ, σ), with density
g(T (x)|θ) =

n
2πσ2 e−(¯x−θ)2n/2σ2 (s2)(n−3)/2e−s2/2σ2
σnΓ(n −1/2)2n−1/2.
Therefore, according to the Suﬃciency Principle, inference on μ should only
depend on this two-dimensional vector, whatever the sample size n is. We
will see in Chapter 3 that the existence of a suﬃcient statistic of constant
dimension is in a sense characteristic of exponential families11.
∥
Example 1.3.3 Consider x1 ∼B(n1, p), x2 ∼B(n2, p), and x3 ∼B(n3, p),
three binomial independent observations when the sample sizes n1, n2, and
n3 are known. The likelihood function is then
f(x1, x2, x3|p) =
n1
x1
n2
x2
n3
x3

px1+x2+x3(1 −p)n1+n2+n3−x1−x2−x3
and the statistics
T1(x1, x2, x3) = x1 + x2 + x3
or
T2(x1, x2, x3) = x1 + x2 + x3
n1 + n2 + n3
are suﬃcient, on the contrary of x1/n1 + x2/n2 + x3/n3.
∥
The Suﬃciency Principle is generally accepted by most statisticians, in
particular because of the Rao–Blackwell Theorem, which rules out esti-
mators which do not depend only on suﬃcient statistics. In model choice
settings, it is sometimes criticized as being too drastically reductive, but
note that the Suﬃciency Principle is only legitimate when the statistical
model is actually the one underlying the generation of the observations.
Any uncertainty about the distribution of the observations should be in-
corporated into the model, a modiﬁcation which almost certainly leads to
a change of suﬃcient statistics. A similar cautionary remark applies to the
Likelihood Principle.
1.3.2 The Likelihood Principle
This second principle is partly a consequence of the Suﬃciency Principle. It
can be attributed to Fisher (1959) or even to Barnard (1949), but was for-
malized by Birnbaum (1962). It is strongly defended by Berger and Wolpert
(1988) who provide an extended study of the topic. In the following deﬁni-
tion, the notion of information is to be considered in the general sense of
the collection of all possible inferences on θ, and not in the mathematical
sense of Fisher information, deﬁned in Chapter 3.
Likelihood Principle The information brought by an observation x about
θ is entirely contained in the likelihood function ℓ(θ|x). Moreover, if x1 and
11 For other distributions, suﬃciency is not a very interesting concept, since the dimen-
sion of a suﬃcient statistic is then of the order of the dimension of the observation x
(or of the corresponding sample), as detailed in Chapter 3.

16
Introduction
1
x2 are two observations depending on the same parameter θ, such that there
exists a constant c satisfying
ℓ1(θ|x1) = cℓ2(θ|x2)
for every θ, they then bring the same information about θ and must lead to
identical inferences.
Notice that the Likelihood Principle is only valid when
(i) inference is about the same parameter θ; and
(ii) θ includes every unknown factor of the model.
The following example provides a (now classical) illustration of this prin-
ciple.
Example 1.3.4
While working on the audience share of a TV series,
0 ≤θ ≤1 representing the part of the TV audience, an investigator found
nine viewers and three nonviewers. If no additional information is available
on the experiment, two probability models at least can be proposed:
(1) the investigator questioned 12 persons, thus observed x ∼B(12, θ) with
x = 9;
(2) the investigator questioned N persons until she obtained 3 nonviewers,
with N ∼Neg(3, 1 −θ) and N = 12.
In other words, the random quantity in the experiment can be either 9 or
12. (Notice that it could also be both.) The important point is that, for
both models, the likelihood is proportional to
θ3(1 −θ)9.
Therefore, the Likelihood Principle implies that the inference on θ should
be identical for both models. As shown in Exercise 1.23, this is not the case
for the classical approach.
∥
Since the Bayesian approach is entirely based on the posterior distribu-
tion
π(θ|x) =
ℓ(θ|x)π(θ)

ℓ(θ|x)π(θ)dθ
(see (1.2.3) and Section 1.4), which depends on x only through ℓ(θ|x), the
Likelihood Principle is automatically satisﬁed in a Bayesian setting.
On the contrary, the classical or frequentist approach12 focuses on the
average behavior properties of procedures and thus justiﬁes the use of an
estimator for reasons that can contradict the Likelihood Principle. This
perspective is particularly striking in testing theory. For instance, if x ∼
12 The theory built up by Wald, Neyman and Pearson in the 1950s is called frequentist,
because it evaluates statistical procedures according to their long-run performances,
that is, on the average (or in frequency) rather than focusing on the performance of
a procedure for the obtained observation, as a conditional approach would do. The
frequentist approach will be considered in detail in Chapters 2 and 5.

1.3
Likelihood Principle and Suﬃciency Principle
17
N(θ, 1) and if the hypothesis to be tested is H0 : θ = 0, the classical
Neyman–Pearson test procedure at level 5% is to reject the hypothesis if
x = 1.96, on the basis that P(|x −θ| ≥1.96) = 0.05, thus conditioning
on the event |x| > 1.96 rather than x = 1.96 (which is impossible for the
frequentist theory). The frequency argument associated with this procedure
is then that, in 5% of the cases when H0 is true, it rejects wrongly the null
hypothesis. Such arguments come to contradict the Likelihood Principle
because tail behaviors may vary for similar likelihoods (see Exercises 1.17
and 1.23). The opposition between the frequentist and Bayesian paradigms
is stronger in testing theory than in point estimation, where the frequentist
approach usually appears as a limiting case of the Bayesian approach (see
Chapter 5).
Example 1.3.5
Consider x1, x2 i.i.d. N(θ, 1). The likelihood function is
then
ℓ(θ|x1, x2) ∝exp{−(¯x −θ)2}
with ¯x = (x1 + x2)/2. Now, consider the alternative distribution
g(x1, x2|θ) = π−3/2 e−(x1+x2−2θ)2/4
1 + (x1 −x2)2 .
This distribution gives a likelihood function proportional to ℓ(θ|x1, x2) and
therefore should lead to the same inference about θ. However, the distri-
bution g is quite diﬀerent from f(x1, x2|θ); for instance, the expectation of
(x1 −x2) is not deﬁned. Therefore, the estimators of θ will have diﬀerent
frequentist properties if they do not depend only on ¯x. In particular, the
conﬁdence regions on θ may diﬀer signiﬁcantly because of the heavier tails
of g.
∥
Example 1.3.6
Another implication of the Likelihood Principle is the
Stopping Rule Principle in sequential analysis. A stopping rule τ can be
deﬁned as follows. If the experiments Ei lead to observations xi ∈Xi, with
xi ∼f(xi|θ), consider a corresponding sequence Ai ⊂X1×. . .×Xi such that
the criterion τ takes the value n if (x1, . . . , xn) ∈An, i.e., the experiment
stops after the nth observation only if the ﬁrst n observations are in An.
The likelihood of (x1, . . . , xn) is then
ℓ(θ|x1, . . . , xn)
=
f(x1|θ)f(x2|x1, θ)
. . . f(xn|x1, . . . , xn−1, θ)IIAn(x1, . . . , xn),
thus depends only on τ through the sample x1, . . . , xn. This implies the
following principle.
Stopping Rule Principle If a sequence of experiments, E1, E2, . . ., is di-
rected by a stopping rule, τ, which indicates when the experiments should
stop, inference about θ must depend on τ only through the resulting sample.

18
Introduction
1
Example 1.3.4 illustrates the case where two diﬀerent stopping rules lead
to the same sample: either the sample size is ﬁxed to be 12, or the expe-
riment is stopped when 9 viewers have been interviewed. Another striking
(although artiﬁcial) example of a stopping rule is to observe xi ∼N(θ, 1)
and to take τ as the ﬁrst integer n such that
|¯xn| =

n

i=1
xi/n
 > 1.96/√n.
In this case, the stopping rule is obviously incompatible with frequentist
modeling since the resulting sample always leads to the rejection of the null
hypothesis H0 : θ = 0 at the level 5% (see Chapter 5). On the contrary,
a Bayesian approach avoids this diﬃculty (see Raiﬀa and Schlaifer (1961)
and Berger and Wolpert (1988, p. 81)).
∥
1.3.3 Derivation of the Likelihood Principle
A justiﬁcation of the Likelihood Principle has been provided by Birnbaum
(1962) who established that it is implied by the Suﬃciency Principle, con-
ditional upon the acceptance of a second principle.
Conditionality Principle
If two experiments on the parameter θ, E1
and E2, are available and if one of these two experiments is selected with
probability p, the resulting inference on θ should only depend on the selected
experiment.
This principle seems diﬃcult to reject when the selected experiment is
known, as shown by the following example.
Example 1.3.7 (Cox (1958)) In a research laboratory, a physical quantity
θ can be measured by a precise but often busy machine, which provides
a measurement, x1 ∼N(θ, 0.1), with probability p = 0.5, or through a
less precise but always available machine, which gives x2 ∼N(θ, 10). The
machine being selected at random, depending on the availability of the more
precise machine, the inference on θ when it has been selected should not
depend on the fact that the alternative machine could have been selected.
In fact, a classical conﬁdence interval at level 5% taking into account this
selection, i.e., averaging over all the possible experiments, is of half-length
5.19, while the interval associated with E1 is of half-length 0.62 (Exercise
1.20).
∥
The equivalence result of Birnbaum (1962) is then as follows.
Theorem 1.3.8 The Likelihood Principle is equivalent to the conjunction
of the Suﬃciency and the Conditionality Principles.
Proof.
We deﬁne ﬁrst the evidence associated with an experiment E,
Ev(E, x), as the collection of the possible inferences on the parameter θ
directing this experiment. Let E∗denote the mixed experiment starting

1.3
Likelihood Principle and Suﬃciency Principle
19
with the choice of Ei with probability 0.5 (i = 1, 2), thus with result (i, xi).
Under these notations, the Conditionality Principle can be written as
Ev(E∗, (j, xj)) = Ev(Ej, xj).
(1.3.1)
Consider x0
1 and x0
2 such that
ℓ(·|x0
1) = cℓ(·|x0
2).
(1.3.2)
The Likelihood Principle then implies
Ev(E1, x0
1) = Ev(E2, x0
2).
(1.3.3)
Let us assume that (1.3.2) is satisﬁed. For the mixed experiment E∗derived
from the two initial experiments, consider the statistic
T (j, xj) =

(1, x0
1)
if j = 2, x2 = x0
2,
(j, xj)
otherwise,
which takes the same value for (1, x0
1) and for (2, x0
2). Then, this statistic
is suﬃcient, since, if t ̸= (1, x0
1),
Pθ(X∗= (j, xj)|T = t) = IIt(j, xj)
and
Pθ(X∗= (1, x0
1)|T = (1, x0
1)) =
c
1 + c,
due to the proportionality of the likelihood functions. The Suﬃciency Prin-
ciple then implies that
Ev(E∗, (1, x1)) = Ev(E∗, (2, x2))
(1.3.4)
and, combined with (1.3.1), leads to (1.3.3).
The reciprocal of this theorem can be derived for the Conditionality
Principle from the fact that the likelihood functions of (j, xj) and xj are
proportional, and for the Suﬃciency Principle from the factorization theo-
rem.
22
Evans, Fraser and Monette (1986) have shown that the Likelihood Prin-
ciple can also be derived as a consequence of a stronger version of the
Conditionality Principle.
1.3.4 Implementation of the Likelihood Principle
It thus seems quite justiﬁed to follow the Likelihood Principle because this
principle can be derived from the unassailable Suﬃciency and Condition-
ality Principles. However, this principle is altogether too vague, since it
does not lead to the selection of a particular procedure when faced with
a given inferential problem. It has been argued that the role of the statis-
tician should stop with the determination of the likelihood function (Box
and Tiao (1973)) since it is suﬃcient for clients to draw their inference,
but this extreme view only stands in the most simple cases (or from a
Bayesian decisional point of view, if the decision-maker also provides a

20
Introduction
1
prior distribution and a loss function). In large (parameter) dimensions,
the likelihood function is also diﬃcult to manipulate because of the lack of
proper representations tools.
The vagueness of the Likelihood Principle calls for a reinforcement of the
axiomatic bases of the inferential process, i.e., for additional structures in
the construction of statistical procedures. For instance, an eﬀective imple-
mentation of the Likelihood Principle is the maximum likelihood estima-
tion method, as brieﬂy described in Section 1.3.5. Similarly, the Bayesian
paradigm allows for implementation of the Likelihood Principle in prac-
tice, with the additional advantage of including the decision-related re-
quirements of the inferential problem, and even getting optimal procedures
from a frequentist point of view (see below).
If we keep in mind the inversion aspect of Statistics presented in Section
1.2, it is tempting to consider the likelihood as a generalized density in
θ, whose mode would then be the maximum likelihood estimator, and to
work with this density as with a regular distribution. This approach seems
to have been advocated by Laplace when he suggested using the uniform
prior distribution when no information was available on θ (see Examples
1.2.3–1.2.5). Similarly, Fisher introduced the ﬁducial approach (see Note
1.8.1) to try to circumvent the determination of a prior distribution while
putting into practice the Likelihood Principle, the choice of his distribution
being objective (since depending only on the distribution of the observa-
tions). However, this approach is at its most defensible when θ is a location
parameter (see also Example 1.5.1), since it leads in general to paradoxes
and contradictions, the most immediate being that ℓ(θ|x) is not necessarily
integrable as a function of θ (Exercise 1.26). The derivation of objective
posterior distributions actually calls for a more advanced theory of non-
informative distributions (see Chapter 3), which shows that the likelihood
function cannot always be considered the most natural posterior distribu-
tion.
Many approaches have been suggested to implement the Likelihood Prin-
ciple like, for instance, penalized likelihood theory (Akaike (1978, 1983)) or
stochastic complexity theory (Rissanen (1983, 1990)). See also Bjørnstad
(1990) for a survey of non-Bayesian methods derived from the Likelihood
Principle in the prediction area. The overall conclusion of this section is
nonetheless that, apart from the fact that many of these theories have
a Bayesian ﬂavor, a truly Bayesian approach is the most appropriate to
take advantage of the Likelihood Principle (see Berger and Wolpert (1988,
Chapter 5) for an extensive discussion of this point).
1.3.5 Maximum likelihood estimation
The Likelihood Principle is altogether distinct from the maximum likeli-
hood estimation approach, which is only one of several ways to implement
the Likelihood Principle. Because we encounter this technique quite often
in the next chapters, and also because it can be situated at the fringe of

1.3
Likelihood Principle and Suﬃciency Principle
21
the Bayesian paradigm, we recall brieﬂy some basic facts about the max-
imum likelihood approach. Extended coverage can be found in Lehmann
and Casella (1998).
When x ∼f(x|θ) is observed, the maximum likelihood approach consid-
ers the following estimator of θ
ˆθ = arg sup
θ
ℓ(θ|x),
(1.3.5)
i.e., the value of θ that maximizes the density at x, f(x|θ), or, informally,
the probability of observing the given value of x. The maximization (1.3.5)
is not always possible (see, e.g., the case of a mixture of two normal distri-
butions, which is detailed in Chapter 6) or can lead to several (equivalent)
global maxima (see, e.g., the case of a Cauchy distribution, C(0, 1), with
two well-separated observations). Nevertheless, the maximum likelihood es-
timator method is widely used, partly because of this intuitive motivation
of maximizing the probability of occurrence and partly because of strong
asymptotic properties (consistency and eﬃciency). An interesting feature
of maximum likelihood estimators is also that they are parameterization-
invariant. That is to say, for any function h(θ), the maximum likelihood
estimator of h(θ) is h(ˆθ) (even when h is not one-to-one). This property is
not enjoyed by any other statistical approach (except by Bayes estimators
in the special case of intrinsic losses. See Section 2.5.4).
The maximum likelihood method also has drawbacks. First, the practical
maximization of ℓ(θ|x) can be quite complex, especially in multidimensional
and constrained settings. Consider, for instance, the examples of a mixture
of normal distributions, of a truncated Weibull distribution
ℓ(θ1, θ2|x1, . . . , xn) = (θ1θ2)n(x1 . . . xn)θ1 exp

−θ2
n

i=1
xθ1
i

(see Exercise 1.29), or of a 10 × 10 table where xij ∼N(θij, 1) when θij is
increasing in i and j (see Robert and Hwang (1996) and Exercises 1.30 and
1.31). Some numerical procedures, such as the EM algorithm of Dempster
et al. (1977) for missing data models or the algorithm of Robertson et
al. (1988) for order-restricted parameter spaces, have been tailored to this
approach, but unsolved diﬃculties remain.
Second, a maximization technique is bound to give estimators that lack
smoothness, as opposed to integration for instance. This is particularly true
when the parameter space is restricted. For example, Saxena and Alam
(1982) show that, if x ∼χ2
p(λ), that is, a noncentral chi-squared distribu-
tion with p degrees of freedom13, the maximum likelihood estimator of λ
is equal to 0 for x < p. Similarly, maximum likelihood estimators can be
quite unstable, i.e., vary widely for small variations of the observations, at
least for reduced sample sizes (see Exercise 1.32).
13 This example also exhibits a limitation of the invariance mentioned above: when
y ∼Np(θ, Ip), the maximum likelihood estimator of λ = ||θ||2 is ||y||2 = x ∼χ2
p(λ),
which diﬀers from the maximum likelihood estimator based on x (see Exercise 3.55).

22
Introduction
1
A last but important defect of the maximum likelihood approach is that
it lacks decision-theoretic and probabilistic supports. In fact, it does not
incorporate the requirements of a decision-theoretic analysis and also fails
to provide evaluation tools for the estimators it proposes. For instance,
tests are not possible in a purely maximum likelihood context: it is nec-
essary to call for frequentist justiﬁcations, even if they are based upon a
likelihood ratio (see Section 5.3). Similarly, conﬁdence regions of the form
C = {θ; ℓ(θ)/ℓ(ˆθ) ≥c}, which are asymptotically shortest, will not depend
solely on the likelihood function if the bound c is to be chosen to achieve
coverage at a given level α.
1.4 Prior and posterior distributions
Let us assume at this point that, in addition to the sample distribution,
f(x|θ), a prior distribution on θ, π(θ), is available, that is, that we deal with
a complete Bayesian model. Chapter 3 considers the preliminary problem
of deriving this distribution from the prior information. Given these two
distributions, we can construct several distributions, namely:
(a) the joint distribution of (θ, x),
ϕ(θ, x) = f(x|θ)π(θ) ;
(b) the marginal distribution of x,
m(x)
=

ϕ(θ, x) dθ
=

f(x|θ)π(θ) dθ ;
(c) the posterior distribution of θ, obtained by Bayes’s formula,
π(θ|x)
=
f(x|θ)π(θ)

f(x|θ)π(θ) dθ
=
f(x|θ)π(θ)
m(x)
;
(d) the predictive distribution of y, when y ∼g(y|θ, x), obtained by
g(y|x) =

g(y|θ, x)π(θ|x)dθ .
Example 1.4.1 (Example 1.2.2 continued)
If x ∼B(n, p) and p ∼
Be(α, β) (with α = β = 1 in the particular case of Bayes),
f(x|p)
=
n
x

px(1 −p)n−x,
x = 0, 1, ..., n,
π(p)
=
1
B(α, β)pα−1(1 −p)β−1,
0 ≤p ≤1.

1.4
Prior and posterior distributions
23
The joint distribution of (x, p) is then
ϕ(x, p) =
	n
x

B(α, β)pα+x−1(1 −p)n−x+β−1
and the marginal distribution of x is
m(x)
=
	n
x

B(α, β)B(α + x, n −x + β)
=
n
x
 Γ(α + β)
Γ(α)Γ(β)
Γ(α + x)Γ(n −x + β)
Γ(α + β + n)
,
since the posterior distribution of p is
π(p|x) = pα+x−1(1 −p)β+n−x−1
B(α + x, β + n −x) ,
i.e., a beta distribution Be(α + x, β + n −x).
∥
Among these distributions, the central concept of the Bayesian paradigm
is the posterior distribution. In fact, this distribution operates conditional
upon the observations, thus operates automatically the inversion of prob-
abilities deﬁned in Section 1.2, while incorporating the requirement of the
Likelihood Principle. It thus avoids averaging over the unobserved values
of x, which is the essence of the frequentist approach. Indeed, the posterior
distribution is the updating of the information available on θ, owing to
the information contained in ℓ(θ|x), while π(θ) represents the information
available a priori, that is, before observing x.
Notice that the Bayesian approach enjoys a speciﬁc kind of coherence
(we will meet others in the following chapters) in that the order in which
i.i.d. observations are collected does not matter (this is a consequence of
the Likelihood Principle), but also that updating the prior one observation
at a time, or all observations together, does not matter. In other words,
π(θ|x1, . . . , xn)
=
f(xn|θ)π(θ|x1, . . . , xn−1)
 f(xn|θ)π(θ|x1, . . . , xn−1)dθ
=
f(xn|θ)f(xn−1|θ)π(θ|x1, . . . , xn−2)

f(xn|θ)f(xn−1|θ)π(θ|x1, . . . , xn−2)dθ
=
. . .
(1.4.1)
=
f(xn|θ)f(xn−1|θ) . . . f(x1|θ)π(θ)

f(xn|θ)f(xn−1|θ) . . . f(x1|θ)π(θ)dθ .
It may happen that the observations do not modify the distribution of
some parameters. This is obviously the case when the distribution of x does
not depend on these parameters, as in some nonindentiﬁable settings.
Example 1.4.2
Consider one observation x from a normal
N
θ1 + θ2
2
, 1


24
Introduction
1
distribution, with a prior π on (θ1, θ2) such that π(θ1, θ2) = π1(θ1 +
θ2)π2(θ1 −θ2). If we operate the change of variables
ξ1 = θ1 + θ2
2
,
ξ2 = θ1 −θ2
2
,
the posterior distribution of ξ2 is then
π(ξ2)
∝

IR
exp

−(x −ξ1)2/2

2π1(2ξ1)2π2(2ξ2)dξ1
∝
π2(2ξ2)

IR
exp

−(x −ξ1)2/2

π1(2ξ1)dξ1
∝
π2(2ξ2)
for every observation x. The observation thus brings no information
on ξ2.
∥
We must warn the reader that not every nonidentiﬁable setting leads to
this simple conclusion: depending on the choice of the prior distribution
and on the reparameterization of the parameter θ in (θ1, θ2), where the
distribution of x only depends on θ1, the marginal posterior distribution of
θ2 may or may not depend on x (Exercise 1.45). An important aspect of
the Bayesian paradigm in nonidentiﬁable settings is, however, that the prior
distribution can be used as a tool to identify the parts of the parameter
that are not covered by the likelihood, even though the choice of prior may
have a bearing on the identiﬁable part.
This invariance from prior distribution to posterior distribution may also
occur for some parameters when the number of parameters becomes too
large compared with the sample size (Exercise 1.39).
Example 1.4.3
A general setting where this situation occurs is found
when the number of parameters is inﬁnite, for instance, when the inference
encompasses a whole distribution. Studden (1990) considers n observations
x1, . . . , xn from a mixture of geometric distributions,
x ∼
 1
0
θx(1 −θ) dG(θ),
x taking its values in IN and the probability distribution G being unknown.
In this setting, G can be represented by the sequence of its noncentral mo-
ments c1, c2, . . .. The likelihood function is then derived from P(X = k) =
ck−ck+1. Studden (1990) shows that, although the ci are constrained by an
inﬁnite number of inequalities (starting with c1 > c2 > c2
1), it is possible to
derive (algebraically) independent functions of the ci’s, p1, p2, . . ., taking
values in [0, 1] and such that ci only depends on p1, . . . , pi (see Exercise
1.46 for details). Therefore, if the prior distribution of p1, p2, . . . is
π(p1, p2, . . .) =
+∞

i=1
πi(pi),

1.4
Prior and posterior distributions
25
and if the largest observation in the sample is k, the posterior distribution
of pk+2, pk+3, . . . does not depend on the observations:
π(pk+2, . . . |x1, . . . , xn) = π(pk+2, . . .) =
+∞

i=k+2
πi(pi).
∥
Conversely, the marginal distribution does not involve the parameter
of interest θ. It is therefore rarely of direct use, except in the empirical
Bayesian approach (see Chapter 10), since the posterior distribution is
much more adapted to inferential purposes. The marginal distribution can,
however, be used in the derivation of the prior distribution if the available
information has been gathered from diﬀerent experiments, that is, dealing
with diﬀerent θ’s as in meta-analysis (see Mosteller and Chalmers (1992),
Mengersen and Tweedie (1993), and Givens et al. (1997)).
Given a probability distribution π on θ, the Bayesian inferential scope
is much larger than the classical perspective. For instance, not only the
mean, mode, or median of π(θ|x) can be computed, but also evaluations of
the performances of these estimators (through their variance and higher-
order moments) are available. Moreover, the knowledge of the posterior
distribution also allows for the derivation of conﬁdence regions through
highest posterior density (HPD) regions, that is, regions of the form
{θ; π(θ|x) ≥k},
in both unidimensional and multidimensional cases. Similarly, it is possible
to derive quite naturally the probability of a hypothesis H0, by condition-
ing on the observations, i.e., P π(θ ∈H0|x). Let us stress that the Bayesian
approach is the only one justifying such an expression because the expres-
sion P(θ = θ0) = 0.95 is meaningless unless θ is a random variable. From
a Bayesian point of view, this expression signiﬁes that we are ready to bet
that θ is equal to θ0 with a 95/5 odds ratio, or, in other words, that the
uncertainty about the value of θ is reduced to a 5% zone. Chapters 4 and
5 are devoted to the study of estimation techniques that incorporate the
decisional requirements. We just illustrate the simplicity of this derivation
by constructing a conﬁdence interval in the following example.
Example 1.4.4 Consider x ∼N(θ, 1) and θ ∼N(0, 10). Therefore, for a
given14 x,
π(θ|x)
∝
f(x|θ)π(θ) ∝exp

−(x −θ)2
2
−θ2
20

14 The proportionality symbol ∝is to be taken for functions of θ (not of x). While being
entirely rigorous, computations using proportionality signs lead to greater eﬃciency
in the derivation of posterior distributions. In fact, probability densities are uniquely
determined by their functional form, and the normalizing constant can be recovered,
when necessary, at the end of the computation. This technique will therefore be used
extensively in this book. Obviously, it is not always appropriate, for instance when
the proportionality constant is 0 or inﬁnity, as seen in Section 1.5.

26
Introduction
1
∝
exp

−11θ2
20
+ θx

∝
exp

−11
20 {θ −(10x/11)}2

and θ|x ∼N( 10
11x, 10
11). A natural conﬁdence region is then
C
=
{θ; π(θ|x) > k}
=

θ;
θ −10
11x
 > k′

.
We can also associate a conﬁdence level α with this region in the sense that,
if zα/2 is the α/2 quantile of N(0, 1),
Cα =

10
11x −zα/2

10
11, 10
11x + zα/2

10
11

has a posterior probability (1 −α) of containing θ.
∥
We will see in Chapter 10 that a posterior distribution can sometimes
be decomposed into several levels according to a hierarchical structure,
the parameters of the ﬁrst levels being treated as random variables with
additional prior distributions. But this decomposition is instrumental and
does not modify the fundamental structure of the Bayesian model.
A problem we did not mention above is that, although all posterior quan-
tities are automatically deﬁned from a conceptual point of view as integrals
with respect to the posterior distribution, it may be quite diﬃcult to pro-
vide a numerical value in practice and, in particular, an explicit form of the
posterior distribution cannot always be derived. In fact, the complexity of
the posterior distributions increases when the parameters are continuous
and when the dimension of Θ is large.
These computational diﬃculties are studied in Chapter 6, where we pro-
vide some general solutions. Still, they should not be considered a major
drawback of the Bayesian approach. Indeed, Computational Statistics is
currently undergoing such a rapid development that we can clearly reject
the notion of a prior distribution chosen for its computational tractability,
even though we may still rely on these particular distributions to present
simpler and clearer examples in this book. On the contrary, it is stimulating
to see that we are getting closer to the goal of providing more powerful and
eﬃcient statistical tools because of these new computational techniques, as
they allow for the use of more complex prior distributions, which are in
turn more representative of the available prior information.
1.5 Improper prior distributions
When the parameter θ can be treated as a random variable with known
probability distribution π, we saw in the previous section that Bayes’s

1.5
Improper prior distributions
27
Theorem is the basis of Bayesian inference, since it leads to the posterior
distribution. In many cases, however, the prior distribution is determined
on a subjective or theoretical basis that provides a σ-ﬁnite measure on the
parameter space Θ instead of a probability measure, that is, a measure π
such that

Θ
π(θ) dθ = +∞.
In such cases, the prior distribution is said to be improper (or generalized).
(An alternative deﬁnition of generalized Bayes estimators is considered in
Chapter 2.)
When this distribution stems from subjective reasons, that is, when the
decision-maker is evaluating the relative likelihoods of diﬀerent parts of
the parameter space Θ (see Chapter 3), it really makes sense that, for large
parameter spaces, for instance when Θ is noncountable, the sum of these
weights, that is, the measure of Θ, should be inﬁnite.
Example 1.5.1
Consider a distribution f(x −θ) where the location pa-
rameter θ is in IR with no restriction. If no prior information is available on
the parameter θ, it is quite acceptable to consider that the likelihood of an
interval [a, b] is proportional to its length b −a, therefore that the prior is
proportional to the Lebesgue measure on IR. This was also the distribution
selected by Laplace (see Example 1.2.5).
∥
When such improper prior distributions are derived by automatic me-
thods from the density f(x|θ) (see Chapter 3), they seem more open to
criticism, but let us point out the following points.
(1) These automatic approaches are usually the only way to derive prior
distributions in noninformative settings, that is, in cases where the only
available (or retained) information is the knowledge of the sample dis-
tribution, f(x|θ). This generalization of the usual Bayesian paradigm
thus makes possible a further extension of the scope of Bayesian tech-
niques.
(2) The performances of the estimators derived from these generalized dis-
tributions are usually good enough to justify these distributions. More-
over, they often permit recovery of usual estimators like maximum like-
lihood estimators, thus guaranteeing a closure of the inferential ﬁeld
by presenting alternative approaches at the boundary of the Bayesian
paradigm.
(3) The generalized prior distributions often occur as limits of proper dis-
tributions (according to various topologies). They can thus be inter-
preted as extreme cases where the reliability of the prior information
has completely disappeared and seem to provide a more robust (or more
objective) answer in terms of a possible misspeciﬁcation of the prior dis-
tribution (i.e., a wrong interpretation of the sparse prior information).
(4) Such distributions are generally more acceptable to non-Bayesians,
partly for reasons (2) and (3), but also because they may have fre-
quentist justiﬁcations, such as:

28
Introduction
1
(i) minimaxity, which is related to the usually improper “least favorable
distributions”, deﬁned in Chapter 2);
(ii) admissibility, as proper and some improper distributions lead to ad-
missible estimators, while admissible estimators sometimes only cor-
respond to Bayes estimators (see Chapter 8); and
(iii) invariance, as the best equivariant estimator is a Bayes estimator for
the generally improper Haar measure associated with the transforma-
tion group (see Chapter 9).
(5) A recent perspective (see, e.g., Berger (2000)) is that improper pri-
ors should be preferred to vague proper priors such as, a N(0, 1002)
distribution say, because the later gives a false sense of safety owing
to properness, while lacking robustness in terms of inﬂuence on the
resulting inference.
These reasons do not convince all Bayesians (see, e.g., Lindley (1965)), but
the inclusion of improper distributions in the Bayesian paradigm allows for
a closure of the inferential scope (ﬁguratively as well as topologically).
From a more practical perspective, the fact that the prior distribution is
improper weakens the above symmetry between the observations and the
parameters, but as long as the posterior distribution is deﬁned, Bayesian
methods apply as well. In fact, the notion of conditional measures is not
clearly deﬁned in measure theory, although Hartigan (1983) advocates such
an extension, but the convention is to take the posterior distribution π(θ|x)
associated with an improper prior π as given by Bayes’s formula
π(θ|x) =
f(x|θ)π(θ)

Θ f(x|θ)π(θ) dθ ,
when the pseudo marginal distribution

Θ f(x|θ)π(θ) dθ is well deﬁned.
This is an imperative condition for using improper priors, which (almost)
always hold for proper priors (Exercise 1.47).
Example 1.5.2 (Example 1.5.1 continued) If f(x −θ) is the density
of the normal distribution N(θ, 1) and π(θ) = ϖ, an arbitrary constant,
the pseudo marginal distribution is the measure
m(x) = ϖ
 +∞
−∞
1
√
2π exp

−(x −θ)2/2

dθ = ϖ
and, by Bayes’s formula, the posterior distribution of θ is
π(θ | x) =
1
√
2π exp

−(x −θ)2
2

,
i.e., corresponds to N(x, 1). Notice that the constant ϖ does not play a role
in the posterior distribution, and that the posterior distribution is actually
the likelihood function. Therefore, even though improper priors cannot be
normalized, it does not matter because the constant is of no interest for
the statistical inference (but see Chapter 5 for an important exception). ∥

1.5
Improper prior distributions
29
According to the Bayesian version of the Likelihood Principle, only pos-
terior distributions are of importance. Therefore, the generalization from
proper to improper prior distributions should not cause problems, in the
sense that the posterior distribution corresponding to an improper prior can
be used similarly to a regular posterior distributions, when it is deﬁned.
(Obviously, the interpretation of the prior distribution is more delicate.)
For instance, in Example 1.5.1, the relative prior weight of any interval is
null, but this does not mean that this interval is unlikely a priori. Actu-
ally, a misinterpretation of improper priors as regular prior distributions
may lead to diﬃculties like marginalization paradoxes (see Chapter 3) be-
cause the usual calculus of conditional probability does not apply in this
setting. As expressed by Lindley (1990), the mistake is to think of them
[non-informative priors] as representing ignorance.
It may happen that, for some observations x, the posterior distribution
is not deﬁned (Exercises 1.49–1.52). The usual solution is to determine the
improper answer as a limit for a sequence of proper distributions (while
also checking the justiﬁcations of the improper distribution).
Example 1.5.3
Consider a binomial observation, x ∼B(n, p), as in the
original example of Bayes. Some authors (see Novick and Hall (1965) and
Villegas (1977)) reject Laplace’s choice of the uniform distribution on [0, 1]
as automatic prior distribution because it seems to be biased against the
extreme values, 0 and 1. They propose to consider instead Haldane’s (1931)
prior
π∗(p) ∝[p(1 −p)]−1.
In this case, the marginal distribution,
m(x)
=
 1
0
[p(1 −p)]−1
n
x

px(1 −p)n−xdp
=
B(x, n −x),
is only deﬁned for x ̸= 0, n . Therefore, π(p|x) does not exist for these
two extreme values of x, since the product π∗(p)px(1 −p)n−x cannot be
normalized for these two values. For the other values, the posterior distri-
bution is Be(x, n−x), with posterior mean x/n, which is also the maximum
likelihood estimator.
The diﬃculty in 0 and n can be overcome as follows. The prior measure
π∗appears as a limit of unnormalized beta distributions,
πα,β(p) = pα−1(1 −p)β−1,
when α and β go to 0. These distributions πα,β lead to beta posterior dis-
tributions, Be(α+x, β+n−x), notwithstanding the lack of the normalizing
factor, since the choice of the constant in the prior distribution is irrelevant.
The posterior distribution πα,β(p|x) has the expectation
δπ
α,β(x) =
x + α
α + β + n,

30
Introduction
1
which goes to x/n when α and β go to 0. If the posterior mean is the
quantity of interest, we can then extend the inferential procedure to the
cases x = 0 and x = n by taking also x/n as a formal Bayes estimator.
∥
Example 1.5.4 Consider x ∼N(0, σ2). It follows from invariance consid-
erations that an interesting prior distribution on σ is the measure π(σ) =
1/σ (see Chapter 9). It gives the posterior distribution
π(σ2|x) ∝e−x2/2σ2
σ2
,
which is not deﬁned for x = 0. However, owing to the continuity of the
random variable x, this diﬃculty is of little importance compared with
Example 1.5.3.
∥
Obviously, these limiting arguments are ad-hoc expedients which are not
always justiﬁed, in particular because the resulting estimator may depend
on the choice of the converging sequence. An example of this phenomenon
is provided by Richard (1973) (see also Bauwens (1991)) in the case of a
normal distribution N(θ, σ2), when π(θ) is the Lebesgue measure and σ−2
is distributed according to a gamma distribution G(α, s2
0), i.e., when
π(θ, σ2) ∝
1
σ2(α+1) e−s2
0/2σ2 ;
the estimator of θ then depends on the behavior of the ratio s2
0/(α −1)
when both numerator and denominator go to 0.
Moreover, when estimating a discontinuous function of θ, the estimator
for the limiting distribution may diﬀer from the limit of the estimators.
This is, for instance, the case in testing theory with the Jeﬀreys–Lindley
paradox (see Chapter 5). Finally, there may be settings such that improper
prior distributions cannot be used easily, like in mixture estimation (see
Exercise 1.57 and Chapter 6) or in testing theory when testing two-sided
hypotheses (see Exercises 1.61–1.63 and Chapter 5).
It is thus important to exercise additional caution when dealing with
improper distributions in order to avoid ill-deﬁned distributions. In this
book, improper distributions will always be used under the implicit as-
sumption that the corresponding posterior distributions are deﬁned, even
though there are settings where this condition could be relaxed (see Note
1.8.3).
The practical diﬃculty is in checking the propriety (or properness) con-
dition

f(x|θ)π(θ) dθ < ∞
in complex settings like hierarchical models (see Exercise 1.67 and Chapter
10), where the use of improper priors on the upper level of the hierarchy
is quite common. The problem is even more crucial because new computa-
tional tools like MCMC algorithms (Chapter 6) do not require in practice

1.7
Exercises
31
this checking of properness (see Note 1.8.3, and Hobert and Casella (1996,
1998)).
Let us stress again that the main justiﬁcation for using improper prior
distributions is to provide a completion of the Bayesian inferential ﬁeld for
subjective, axiomatic (in relation with complete class results, see Chapter
8), and practical reasons. This extension does not modify the complex-
ity of the inference, however, because the posterior distribution is truly a
probability distribution.
1.6 The Bayesian choice
To close this introduction, let us impress upon the reader that there is such
a thing as a Bayesian choice. Thus, it is always possible to adhere to this
choice, or to opt for other options. While we are resolutely advocating for
this choice, there is no reason to become overly strident. Most statistical
theories, such as those presented in Lehmann and Casella (1998), have a
reasonable level of coherence and most often agree when the number of
observations gets large compared with the number of parameters (see Note
1.8.4).
If we do not present these options here, it is both for philosophical and
practical reasons (exposed in Chapter 11), and also for the purpose of pre-
senting an uniﬁed discourse on Statistics, where all procedures logically
follow from a given set of axioms. This is indeed for us the compelling rea-
son for adhering to the Bayesian choice, namely, the ultimate coherence of
the axioms of Bayesian statistical inference. By modeling the unknown pa-
rameters of the sampling distribution through a probability structure, i.e.,
by probabilizing uncertainty, the Bayesian approach authorizes a quanti-
tative discourse on these parameters. It also allows incorporation in the
inferential procedure of the prior information and of the imprecision of this
information. Besides, apart from subjective and axiomatic arguments in
favor of the Bayesian approach, which is the only system allowing for con-
ditioning on the observations (and thus for an eﬀective implementation of
the Likelihood Principle), Bayes estimators are also quintessential for the
frequentist optimality notions of Decision Theory. In fact, they can provide
essential tools even to those statisticians who reject prior elicitation and
the Bayesian interpretation of reality.
1.7 Exercises
Section15 1.1
1.1
⋆(Kelker (1970))
A vector x ∈IRp is distributed according to a spheri-
cally symmetric distribution if e.x has the same distribution than x for every
15 The exercises with stars are more advanced, but oﬀer a broader view of the topics
treated in each chapter. They can be be treated as useful complements, or as a guided
lecture of relevant papers by most readers.

32
Introduction
1
orthogonal transform e.
a. Show that, when a spherically symmetric distribution has a density, it is a
function of xtx only.
b. Show that, if the density of x is ϕ(xtx), the density of r = ||x|| is propor-
tional to
rp−1 ϕ(r2),
and give the proportionality coeﬃcient.
c. Show that, if x = (x′
1, x′
2)′ with x1 ∈IRq and x2 ∈IRp−q, and ||x||2 =
||x1||2 + ||x2||2, the density of (r1, r2) = (||x1||, ||x2||) is proportional to
rq−1
1
rp−q−1
2
ϕ 	
r2
1 + r2
2

.
d. Deduce that
U =
||x1||2
||x1||2 + ||x2||2
is distributed according to a beta distribution Be(q/2, (p −q)/2).
e. Conclude that
p −q
q
||x1||2
||x2||2
is distributed according to the F-distribution Fp−q,q independently of the
spherically symmetric distribution of x. Deduce that the F-ratio is a ro-
bust quantity in the sense that its distribution is constant on a range of
spherically symmetric distributions.
1.2
⋆(Gouri´eroux and Monfort (1996)) This exercise points out that the bound-
ary between parametric and nonparametric models is quite diﬃcult to deter-
mine. However, in the second case, the parameter cannot be identiﬁed.
a. Show that a c.d.f. is characterized by the values it takes at the rational
numbers.
b. Deduce that the collection of the c.d.f.’s on IR has the power of continuum
(i.e., the cardinal of the set of the parts of IN, the set of natural integers)
and thus that all probability distributions on IR can be indexed by a real
parameter.
1.3 Show that, if x1, . . . , xn are known explanatory variables and y1, . . . , yn are
distributed as IE[yi] = bxi, the least-squares estimator of b, solution of
min
b
n

i=1
(yi −bxi)2,
is also a maximum likelihood estimator under a normality assumption.
1.4 In Example 1.1.3, give the expectation of n. Does that mean that 20 × 30/n
is an unbiased estimator of N?
1.5 In Example 1.1.6, show that the moments of x ∼f(x) can be written as
IE[xk] = pIE[xk
1]+(1−p)IE[xk
2]. Deduce a moment estimator of (p, μ1, μ2, σ2
1, σ2
2).
[Note: Historically, this is the estimate of Pearson (1894).]

1.7
Exercises
33
Section 1.2
1.6 Derive the probabilities of Example 1.2.4 from the approximation
Φ(−x) ≃
1
√
2πx
e−x2/2,
which is valid when x is large.
1.7 An examination has 15 questions, each with 3 possible answers. Assume that
70% of the students taking the examination are prepared and answer correctly
each question with probability 0.8; the remaining 30% answer at random.
a. Characterize the distribution of S, score of a student if one point is at-
tributed to each correct answer.
b. Eight correct answers are necessary to pass the examination. Given that a
student has passed the examination, what is the probability that she was
prepared?
1.8 Prove the discrete and continuous versions of Bayes’s Theorem.
1.9
⋆(Romano and Siegel (1986)) The Simpson paradox provides an illustration
of the need for a conditional approach in Statistics. Consider two medical
treatments, T1 and T2, T1 being applied to 50 patients and T2 to 50 others.
The result of the experiment gives the following survival percentages: 40% for
treatment T1, 32% for treatment T2. Therefore, treatment T1 seems better be-
cause it leads to an higher survival rate. However, if age is taken into account,
dividing the subjects between juniors (50) and seniors (50), the success rates
are described in the following table:
T1
T2
junior
40
50
senior
10
35
and T1 is worse than T2 in both cases. Explain the paradox in terms of Bayes’s
Theorem.
1.10 Show that the quantity δ that minimizes (1.2.4) is the median of the dis-
tribution of ξ. Give the quantity δ that minimizes the average squared error
IEξ[(ξ −δ)2].
1.11 Find the median of the posterior distribution associated with the sampling
distribution (1.2.5) and a ﬂat prior π(ξ) = 1 on ξ. [Note: See Stigler (1986)
for a resolution.]
Section 1.3
1.12 Show that, for a sample from a normal N(θ, σ2) distribution, there does
not exist an unbiased estimator of σ but only of powers of σ2.
1.13 Consider x ∼P(λ). Show that δ(x) = II0(x) is an unbiased estimator of
e−λ which is null with probability 1 −e−λ.
1.14
∗A statistic S is said to be ancillary if its distribution does not depend on
the parameter θ and it is said to be complete if IEθ[g(S)] = 0 for every θ implies
g(s) ≡0. Show that, if S is complete and minimal suﬃcient, it is independent
of every ancillary statistic. [Note: This result is called Basu’s Theorem. The
reverse is false.]
1.15 Consider a sample x1, . . . , xn of i.i.d. variables with c.d.f. F.
a. Give the density of the order statistic.

34
Introduction
1
b. Show that O = (X(1), ..., X(n)) is suﬃcient. What is the conditional distri-
bution of (X1, ..., Xn) given O?
c. Consider X1, ..., Xn i.i.d. with totally unknown density. Show that O is
then complete.
1.16
Show that a statistic T is suﬃcient if and only if
ℓ(θ|x) ∝ℓ(θ|T(x)).
1.17 (Berger and Wolpert (1988, p. 21)) Consider x with support {1, 2, 3} and
distribution f(· | 0) or f(· | 1), where
x
1
2
3
f(x|0)
0.9
0.05
0.05
f(x|1)
0.1
0.05
0.85
Show that the procedure that rejects the hypothesis H0 : θ = 0 (to accept
H1 : θ = 1) when x = 2, 3 has a probability 0.9 to be correct (under H0
as well as under the alternative). What is the implication of the Likelihood
Principle when x = 2?
1.18 Show that the Stopping Rule Principle given in Example 1.3.6 is a conse-
quence of the Likelihood Principle for the discrete case. [Note: See Berger and
Wolpert (1988) for the extension to the continuous case.]
1.19 For Example 1.3.6, show that the stopping rule τ is ﬁnite with probability
1. (Hint: Use the law of the iterated logarithm. See Billingsley (1986).)
1.20 Show that the conﬁdence intervals of Example 1.3.7 are correct: under the
mixed experiment, x ∼0.5N(θ, 0.1) + 0.5N(θ, 10) and P(θ ∈[x −5.19, x +
5.19]) = 0.95, while, under experiment E1, x ∼N(θ, 0.1) and P(θ ∈[x −
0.62, x + 0.62) = 0.95.
1.21 (Raiﬀa and Schlaifer (1961)) Show that, if z ∼f(z|θ) and if x = t(z), x is
a suﬃcient statistic if and only if for every prior π on θ, π(θ|x) = π(θ|z).
1.22 Consider x1, . . . , xn distributed according to Exp(λ). The data is censored in
the sense that there exist n random variables y1, . . . , yn distributed according
to f(y), independent of λ, and z1 = x1 ∧y1, . . . , zn = xn ∧yn are the actual
observations.
a. Show that, according to the Likelihood Principle, the inference on λ should
not depend on f.
b. Extend this independence to other types of censoring.
1.23 Compare the lengths of the conﬁdence intervals at level 10% in the setting
of Example 1.3.7.
1.24 (Berger (1985a)) In the setting of Example 1.3.4, show that, for the UMPU
test of H0 : p = 1/2, the null hypothesis will be accepted or rejected at level
5%, depending on the distribution considered. Deduce that the frequentist
theory of tests is not compatible with the Likelihood Principle. (Hint: See
Chapter 5 for deﬁnitions.)
1.25 This exercise aims at generalizing Examples 1.3.4 and 1.3.5 in the con-
tinuous case by showing that there can also be incompatibility between the
frequentist approach and the Likelihood Principle in continuous settings.

1.7
Exercises
35
a. If f(x|θ) is a density such that x is a complete statistic, show that there is no
other density g(x|θ) such that the two likelihood functions ℓf(θ|x) = f(x|θ)
and ℓg(θ|x) = g(x|θ) are proportional (in θ) for every x.
b. Consider now a sample x1, . . . , xn from f(x|θ). We assume that there exists
a complete suﬃcient statistic T(x1, . . . , xn) of dimension 1 and an ancillary
statistic S(x1, . . . , xn) such that the couple (T, S) is a one-to-one function
of (x1, . . . , xn). Show that, if there exists another density g(x1, . . . , xn|θ)
such that the two likelihood functions are proportional,
ℓg(θ|x1, . . . , xn) = ω(x1, . . . , xn)ℓf(θ|x1, . . . , xn),
the proportionality factor ω only depends on S(x1, . . . , xn).
c. In the particular case when f(x|θ) is the exponential density, f(x|θ) =
θe−θx, give an example of a density g(x1, . . . , xn|θ) such that the two like-
lihood functions are proportional. (Hint: Find an ancillary statistic S and
derive a function h(x1, . . . , xn) depending only on S(x1, . . . , xn) such that
IEθ[h(x1, . . . , xn)] = 1.)
The following exercises (1.27 to 1.36) present some additional aspects of
maximum likelihood estimation.
1.26 Show that, if the likelihood function ℓ(θ|x) is used as a density on θ, the
resulting inference does not obey the Likelihood Principle (Hint: Show that
the posterior distribution of h(θ), when h is a one-to-one transform, is not the
transform of ℓ(θ|x) by the Jacobian rule.)
1.27 Consider a Bernoulli random variable y ∼B([1 + eθ]−1).
a. If y = 1, show that there is no maximum likelihood estimator of θ.
b. Show that the same problem occurs when y1, y2 ∼B([1 + eθ]−1) and y1 =
y2 = 0 or y1 = y2 = 1. Give the maximum likelihood estimator in the other
cases.
1.28 Consider x1, x2 two independent observations from C(θ, 1). Show that, when
|x1 −x2| > 2, the likelihood function is bimodal. Find examples of x1, x2, x3
i.i.d. C(θ, 1) for which the likelihood function has three modes.
1.29 The Weibull distribution We(α, c) is widely used in engineering and relia-
bility. Its density is given by
f(x|α, c) = cα−1(x/α)c−1e−(x/α)c.
a. Show that, when c is known, this model is equivalent to a gamma model.
b. Give the likelihood equations in α and c and show that they do not allow
for explicit solutions.
c. Consider an i.i.d. x1, . . . , xn sample from We(α, c) censored from the right
in y0. Give the corresponding likelihood function when α and c are unknown
and show that there is no explicit maximum likelihood estimators in this
case either.
1.30
∗(Robertson et al. (1988)) For a sample x1, . . . , xn, and a function f on X ,
the isotonic regression of f with weights ωi is the solution of the minimization
in g of
n

i=1
ωi(g(xi) −f(xi))2,
under the constraint g(x1) ≤. . . ≤g(xn).

36
Introduction
1
a. Show that a solution to this problem is obtained by the pool-adjacent-
violators algorithm: if f is not isotonic, ﬁnd i such that f(xi−1) > f(xi),
replace f(xi−1) and f(xi) by
f ∗(xi) = f ∗(xi−1) = ωif(xi) + ωi−1f(xi−1)
ωi + ωi−1
,
and repeat until the constraint is satisﬁed. Take g = f ∗.
b. Apply to the case n = 4, f(x1) = 23, f(x2) = 27, f(x3) = 25, f(x4) = 28,
when the weights are all equal.
1.31
∗(Exercise 1.30 cont.) The simple tree-ordering is obtained when one
compares some treatment eﬀects with a control state. The isotonic regression
is then obtained under the constraint g(xi) ≥g(x1) for i = 2, . . . , n.
a. Show that the following algorithm provides the isotonic regression g∗: if f is
not isotonic, assume w.l.o.g. that the f(xi) are in increasing order (i ≥2).
Find the smallest j such that
Aj = ω1f(x1) + . . . + ωjf(xj)
ω1 + . . . ωj
< f(xj+1)
and take g∗(x1) = Aj = g∗(x2) = . . . = g∗(xj), g∗(xj+1) = f(xj+1), . . ..
b. Apply to the case where n = 5, f(x1) = 18, f(x2) = 17, f(x3) = 12,
f(x4) = 21 and f(x5) = 16, with ω1 = ω2 = ω5 = 1 and ω3 = ω4 = 3.
1.32 (Olkin et al. (1981)) Consider n observations x1, . . . , xn from B(k, p) where
both k and p are unknown.
a. Show that the maximum likelihood estimator of k, ˆk, is such that
(ˆk(1 −ˆp))n ≥
n

i=1
(ˆk −xi)
and
((ˆk + 1)(1 −ˆp))n <
n

i=1
(ˆk + 1 −xi),
where ˆp is the maximum likelihood estimator of p.
b. If the sample is 16, 18, 22, 25, 27, show that ˆk = 99.
c. If the sample is 16, 18, 22, 25, 28, show that ˆk = 190 and conclude on the
stability of the maximum likelihood estimator.
1.33 Give the maximum likelihood estimator of p for Example 1.1.6 if the other
parameters are known and if there are two observations. Compare with the
mean of the posterior distribution if p ∼U[0,1].
1.34 (Basu (1988)) An urn contains 1000 tickets; 20 are tagged θ and 980 are
tagged 10θ. A ticket is drawn at random with tag x.
a. Give the maximum likelihood estimator of θ, δ(x), and show that P(δ(x) =
θ) = 0.98.
b. Suppose now there are 20 tickets tagged θ and 980 tagged aiθ (i ≤980),
such that ai ∈[10, 10.1] and ai ̸= aj (i ̸= j). Give the new maximum
likelihood estimator, δ′, and show that P(δ′(x) < 10θ) = 0.02. Conclude
about the appeal of maximum likelihood estimation in this case.
1.35 (Romano and Siegel (1986)) Given
f(x) =
1
x exp

−50
 1
x −1
2
(x > 0),

1.7
Exercises
37
show that f is integrable and that there exist a, b > 0 such that
 b
0
af(x)dx = 1
and
 b
1
af(x)dx = 0.99.
For the distribution with density
p(y|θ) = aθ−1f(yθ−1)II[0,bθ](y),
give the maximum likelihood estimator, δ(y), and show that P(δ(y) > 10θ) =
0.99.
1.36 (Romano and Siegel (1986)) Consider x1, x2, x3 i.i.d. N(θ, σ2).
a. Give the maximum likelihood estimator of σ2 if (x1, x2, x3) = (9, 10, 11) or
if (x1, x2, x3) = (29, 30, 31).
b. Given three additional observations x4, x5, x6, give the maximum likelihood
estimator if (x1, . . . , x6) = (9, 10, 11, 29, 30, 31). Does this result contradict
the Likelihood Principle?
Section 1.4
1.37 If x ∼N(θ, σ2), y ∼N(ϱx, σ2), as in an autoregressive model, with ϱ
known, and π(θ, σ2) = 1/σ2, give the predictive distribution of y given x.
1.38 If y ∼B(n, θ), x ∼B(m, θ), and θ ∼Be(α, β), give the predictive distribu-
tion of y given x.
1.39 Given a proper distribution π(θ) and a sampling distribution f(x|θ), show
that the only case such that π(θ|x) and π(θ) are identical occurs when f(x|θ)
does not depend on θ.
1.40 Consider a prior distribution π positive on Θ and x ∼f(x|θ). Assume that
the likelihood ℓ(θ|x) is bounded, continuous, and has a unique maximum ˆθ(x).
a. Show that, when considering a virtual sample xn = (x, . . . , x) made of n
replications of the original observation x, the posterior distribution π(θ|xn)
converges to a Dirac mass in ˆθ(x).
b. Derive a Bayesian algorithm for computing maximum likelihood estimators.
1.41
⋆Given a couple (x, y) of random variables, the marginal distributions f(x)
and f(y) are not suﬃcient to characterize the joint distribution of (x, y).
a. Give an example of two diﬀerent bivariate distributions with the same
marginals. (Hint: Take these marginals to be uniform U([0, 1]) and ﬁnd a
function from [0, 1]2 to [0, 1]2 which is increasing in both its coeﬃcients).
b. Show that, on the contrary, if the two conditional distributions f(x|y) and
f(y|x) are known, the distribution of the couple (x, y) is also uniquely
deﬁned.
c. Extend b. to a vector (x1, . . . , xn) such that the full conditionals fi(xi|xj, j ̸=
i) are known. [Note: This result is called the Hammersley–Cliﬀord Theorem,
see Robert and Casella (2004).]
d. Show that property b. does not necessarily hold if f(x|y) and f(x) are
known, i.e., that several distributions f(y) can relate f(x) and f(x|y).
(Hint: Exhibit a counter-example.)
e. Give some suﬃcient conditions on f(x|y) for the above property to be true.
(Hint: Relate this problem to the theory of complete statistics.)

38
Introduction
1
1.42 Consider x1, . . . , xn i.i.d. P(λ). Show that n
i=1 xi is a suﬃcient statistic
and give a conﬁdence region as in Example 1.4.4 when π(λ) is a G(α, β) dis-
tribution. For a given α level, compare its length with an equal tail conﬁdence
region.
1.43 Give the posterior and the marginal distributions in the following cases:
(i)
x|σ ∼N(0, σ2),
1/σ2 ∼G(1, 2);
(ii)
x|λ ∼P(λ),
λ ∼G(2, 1);
(iii)
x|p ∼Neg(10, p),
p ∼Be(1/2, 1/2).
1.44 Show that, for a sample x1, . . . , xn from a distribution with conditional
density f(xi|θ, xi−1), the actualizing decomposition (1.4.1) also applies. [Note:
The sequence xi is then a Markov chain.]
1.45 Show that, in the setting of Example 1.4.2, the marginal posterior distri-
bution on ξ2 is diﬀerent from the marginal prior distribution if π(ξ1, ξ2) does
not factorize in π1(ξ1)π2(ξ2).
1.46
⋆(Studden (1990)) In the setting of Example 1.4.3, we deﬁne the canonical
moments of a distribution and show that they can be used as a representation
of this distribution.
a. Show that the ﬁrst two moments c1 and c2 are related by the two following
inequalities:
c2
1 ≤c2 ≤c1
and that the sequence (ck) is monotonically decreasing to 0.
b. Consider a kth degree polynomial
Pk(x) =
k

i=0
aixi.
Deduce from
 1
0
P 2
k (x)g(x)dx ≥0
(1.7.1)
that
atCka ≥0,
∀a ∈IRk+1,
(1.7.2)
where
Ck =
⎛
⎜
⎝
1
c1
c2
. . .
ck
c1
c2
c3
. . .
ck+1
. . .
. . .
. . .
. . .
. . .
ck
ck+1
. . .
c2k
⎞
⎟
⎠
and at = (a0, a1, . . . , ak).
c. Show that for every distribution g, the moments ck satisfy

1
c1
c2
. . .
ck
c1
c2
c3
. . .
ck+1
. . .
. . .
. . .
. . .
. . .
ck
ck+1
. . .
c2k

> 0.
(1.7.3)
(Hint: Interpret (1.7.2) as a property of Ck.)

1.7
Exercises
39
d. Using inequalities similar to (1.7.1) for the polynomials t(1−t)P 2
k (t), tP 2
k (t),
and (1 −t)P 2
k (t), derive the following inequalities on the moments of g:

c1 −c2
c2 −c3
. . .
ck−1 −ck
c2 −c3
c3 −c4
. . .
ck −ck+1
. . .
. . .
. . .
. . .
ck−1 −ck
. . .
. . .
c2k−1 −c2k

> 0,
(1.7.4)

c1
c2
. . .
ck
c2
c3
. . .
ck+1
. . .
. . .
. . .
. . .
ck
ck+1
. . .
c2k−1

> 0,
(1.7.5)

1 −c1
c1 −c2
. . .
ck−1 −ck
c1 −c2
c2 −c3
. . .
ck −ck+1
. . .
. . .
. . .
. . .
ck−1 −ck
. . .
. . .
c2k−2 −c2k−1

> 0.
(1.7.6)
e. Show that (1.7.3) (resp. (1.7.4)) induces a lower (resp. upper) bound c2k
(resp. ¯c2k) on c2k and that (1.7.5) (resp. (1.7.6)) induces a lower (resp.
upper) bound c2k−1 (resp. ¯c2k−1) on c2k−1.
f. Deﬁning pk as
pk = ck −ck
¯ck −ck
,
show that the relation between (p1, ..., pn) and (c1, ..., cn) is one-to-one for
every n and that the pi are independent.
g. Show that the inverse transform is given by the following recursive formulas.
Let us deﬁne
qi = 1 −pi,
ζ1 = p1,
ζi = piqi−1
(i ≥2).
Then
 S1,k = ζ1 + . . . + ζk
(k ≥1),
Sj,k = k−j+1
i=1
ζiSj−1,i+j−1
(j ≥2),
cn = Sn,n.
Section 1.5
1.47 The problem with improper priors that the integral 
Θ f(x|θ)π(θ) dθ may
not exist does not appear with proper priors.
a. Recall Fubini’s theorem and apply to the couple of functions (f(x|θ), π(θ)).
b. Deduce that, if π is a ﬁnite positive measure,

Θ
f(x|θ)π(θ) dθ < ∞
(1.7.7)
almost everywhere.
c. Show that, if π is improper and f(x|θ) has a ﬁnite support, then π(θ|x) is
deﬁned if, and only if, (1.7.7) is ﬁnite for every x in the support of f(x|θ).
1.48 Show that, if π is a positive measure on Θ, the integral (1.7.7) is positive
almost everywhere.
1.49 (Fernandez and Steel (1999)) Consider n i.i.d. observations x1, . . . , xn from
the mixture
pN(μ0, σ2
0) + (1 −p)N(μ0, σ2
1) ,

40
Introduction
1
where p, μ0 and σ0 are known. The prior on σ1 is a beta Be(α, β) distribution.
Show that, if r ≥1 observations are equal to μ0, the posterior distribution is
only deﬁned when α > r. [Note: From a measure theoretical point of view, the
set of xi’s equal to μ0 is of measure zero. If one (or more) observation is exactly
equal to μ0, it means that the continuous mixture model is not appropriate.]
1.50 (Exercise 1.49 cont.) Consider an observation x from a normal distri-
bution N(0, σ2).
a. If the prior distribution on σ is an exponential distribution Exp(λ), show
that the posterior distribution is not deﬁned for x = 0.
b. If the prior distribution on σ is the improper prior π(σ) = σ−1 exp(−ασ−2),
with α > 0, show that the posterior distribution is always deﬁned.
1.51 (Exercise 1.50 cont.) Consider an observation y with y = x −λ, where
x is distributed from Laplace’s distribution,
f(x|θ) = θ−1 exp(−|x|/θ) ,
and λ is distributed from
π(λ) = |λ|−1/2II[−1/2,1/2](λ) .
If θ is distributed from a gamma G(1/2, a) (a > 0), show that, if y = 0, the
posterior distribution is not deﬁned.
1.52 (Musio and Racugno (1999)) Consider the Poisson P(θ) model
Pθ(X = x) = θx
x! e−θ,
x = 0, 1, . . . ,
θ > 0,
and the prior distribution π(θ) = 1/θ. Show that for x = 0, the posterior
distribution is not deﬁned.
1.53 (Raiﬀa and Schlaifer (1961))
Consider a Be(αm, (1 −m)α) prior on p ∈
[0, 1]. Show that, if m is held ﬁxed and α approaches 0, the prior distribution
converges to a two-point mass distribution with weight m on p = 1 and (1−m)
on p = 0. Discuss the drawbacks of such a setting.
1.54 (Bauwens (1991)) Consider x1, . . . , xn i.i.d. N(θ, σ2) and
π(θ, σ2) = σ−2(α+1) exp(−s2
0/2σ2).
a. Compute the posterior distribution π(θ, σ2|x1, . . . , xn) and show that it
only depends on ¯x and s2 = n
i=1(xi −¯x)2.
b. Derive the posterior expectation IEπ[θ|x1, . . . , xn] and show that its behav-
ior when α and s0 both converge to 0 depends on the limit of the ratio
s2
0/α −1.
1.55 Show that if the prior π(θ) is improper and the sample space X is ﬁnite,
the posterior distribution π(θ|x) is not deﬁned for some values of x.
1.56 Consider x1, . . . , xn distributed according to N(θj, 1), with θj ∼N(μ, σ2)
(1 ≤j ≤n) and π(μ, σ2) = σ−2. Show that the posterior distribution
π(μ, σ2|x1, . . . , xn) is not deﬁned.
1.57 In the setting of Example 1.1.6, that is, for a mixture of two normal distri-
butions,
a. Show that the maximum likelihood estimator is not deﬁned when all the
parameters are unknown.

1.7
Exercises
41
b. Similarly, show that it is not possible to use an improper prior of the form
π1(μ1, σ1)π2(μ2, σ2)π3(p)
to estimate these parameters. (Hint: Write the likelihood as a sum of n +
1 terms, depending on the number of observations allocated to the ﬁrst
component.)
[Note: Mengersen and Robert (1996) show that it is possible to use some
improper priors by introducing prior dependence between the components.]
1.58
∗(Exercise 1.57 cont.) For a mixture of two normal distributions (1.1.2),
if the prior distribution on the parameters is of the form
π1(μ1, σ1)π1(μ2, σ2)π3(p)
and π3(p) = π3(1−p), show that the marginal posterior distribution of (μ1, σ1)
is the same as the marginal posterior distribution of (μ2, σ2), whatever the
sample. Deduce that the posterior mean of (μ1, σ1) is equal to the posterior
mean of (μ2, σ2) and therefore that it is not a pertinent estimator. [Note: This
problem is a consequence of the nonidentiﬁability of the component labels
in a mixture. Solutions involve imposing identifying constraints such as the
ordering μ1 ≤μ2, or using loss functions that are invariant under permutation
of the component labels.]
1.59 Construct a limiting argument as in Example 1.5.3 to solve the indetermi-
nacy of Example 1.5.4. Derive the posterior mean.
1.60 Show that, if the prior distribution is improper, the pseudo marginal dis-
tribution is also improper.
1.61
∗(Hobert and Casella (1998)) Consider a random-eﬀect model,
yij = β + ui + εij,
i = 1, . . . , I, j = 1, . . . , J,
where ui ∼N(0, σ2) and εij ∼N(0, τ 2). Under the prior
π(β, σ2, τ 2) =
1
σ2τ 2 ,
the posterior does not exist.
a. By integrating out the (unobservable) random-eﬀects ui, show that the full
posterior distribution of (β, σ2, τ 2) is
π(β, σ2, τ 2|y) ∝
σ−2−I τ −2−IJ exp

−
1
2τ2

i,j (yij −¯yi)2 
× exp

−
J 
i(¯yi−β)2
2(τ2+Jσ2)

(Jτ −2 + σ−2)−I/2 .
b. Integrate out β to get the marginal posterior density
π(σ2, τ 2|y)
∝
σ−2−I τ −2−IJ
(Jτ −2 + σ−2)I/2 (τ 2 + Jσ2)1/2
×
exp

−1
2τ 2

i,j
(yij −¯yi)2 −
J
2(τ 2 + Jσ2)

i
(¯yi −¯y)2

.
c. Show that the full posterior is not integrable. (Hint: For τ ̸= 0, π(σ2, τ 2|y)
behaves like σ−2 in a neighborhood of 0.)

42
Introduction
1
d. Show that the conditional distributions
Ui|y, β, σ2, τ 2
∼
N

J(¯yi −β)
J + τ 2σ−2 , (Jτ −2 + σ−2)−1

,
β|u, y, σ2, τ 2
∼
N(¯y −¯u, τ 2/JI) ,
σ2|u, β, y, τ 2
∼
IG
!
I/2, (1/2)

i
u2
i
"
,
τ 2|u, β, y, σ2
∼
IG
!
IJ/2, (1/2)

i,j
(yij −ui −β)2
"
,
are well deﬁned. [Note: The consequences of this deﬁnition of the full pos-
terior will be clariﬁed in Chapter 6.]
1.62
∗Consider a dichotomous probit model, where (1 ≤i ≤n)
P(di = 1) = 1 −P(di = 0) = P(zi ≥0) ,
(1.7.8)
with zi ∼N(riβ, σ2), β ∈IR, ri being a covariate. (Note that the zi’s are not
observed.)
a. Show that the parameter (β, σ) is not identiﬁable.
b. For the prior distribution π(β, σ) = 1/σ, show that the posterior distribu-
tion is not deﬁned.
c. For the prior distribution
σ−2 ∼Ga(1.5, 1.5) ,
β|σ ∼N(0, 102) ,
show that the posterior distribution is well deﬁned.
d. An identifying constraint is to impose σ = 1 on the model. Give suﬃcient
conditions on the observations (di, ri) for the posterior distribution on β to
be deﬁned if π(β) = 1.
e. Same question as d. when the normal distribution on the zi’s is replaced
with the logistic function, that is,
P(di = 1) = 1 −P(di = 0) =
exp(riβ)
1 + exp(riβ) ,
which gives the dichotomous logit model.
1.63
∗(Kubokawa and Robert (1994)) In linear calibration models, the interest is
in determining values of the regressor x from observed responses y, as opposed
to standard linear regression. A simpliﬁed version of this problem can be put
into the framework of observing the independent random variables
y ∼Np(β, σ2Ip), z ∼Np(x0β, σ2Ip), s ∼σ2χ2
q ,
(1.7.9)
with x0 ∈IR, β ∈IRp. The parameter of interest is x0.
a. A reference prior on (x0, β, σ) yields the joint posterior distribution
π(x0, β, σ2|y, z, s)
∝
σ−(3p+q)−1
2 exp{−(s + ∥y −β∥2
+∥z −x0β∥2)/2σ2} (1 + x2
0)−1/2 .
Show that this posterior is compatible with the sampling distribution (1.7.9).

1.7
Exercises
43
b. Show that the marginal posterior distribution of x0 is
π(x0|y, z, s) ∝
(1 + x2
0)(p+q−1)/2

x0 −
ytz
s + ∥y∥2
2
+ ∥z∥2 + s
∥y∥2 + s −
(ytz)2
(s + ∥y∥2)2
(2p+q)/2 .
c. Deduce that the posterior distribution of x0 is well deﬁned.
[Note: See Osborne (1991) for an introduction and review of calibration prob-
lems. The model (1.7.9) is also equivalent to Fieller’s (1954) problem. See,
e.g., Lehmann and Casella (1998).]
Note 1.8.2
1.64
∗(Diaconis and Kemperman (1996)) Show that the deﬁnition of the Dirich-
let process D(F0, α) given in 1.8.2 is compatible with the following one: given
an i.i.d. sequence xi from F0 and a sequence of weights ωi such that
ω1 ∼Be(1, α),
ω1 + ω2 ∼Be(1, α)II[ω1,1], . . .
the random distribution
F =
∞

i=1
ωiδxi
is distributed from D(F0, α).
1.65
∗(Exercise 1.64 cont.) If F ∼D(F0, α), the quantity X = 
xF(dx) is
a random variable.
a. If α = 1 and F0 is a Cauchy distribution, show that X is also distributed as a
Cauchy random variable. [Note: This relates to the characterizing property
of Cauchy distributions that the average of Cauchy random variables is a
Cauchy variable with the same parameters.]
b. If α = 1 and F0 = ϱδ0 + (1 −ϱ)δ1, show that X is distributed as a beta
Be(ϱ, 1 −ϱ) random variable.
c. Show that, if α = 1 and F0 is U[0,1], X has the density
e
π
sin(πy)
(1 −y)(1−y)yy .
[Note: See Diaconis and Kemperman (1996) for the general formula relating
F0 and the density of X.]
1.66
∗(Diaconis and Kemperman (1996)) The Dirichlet process prior D(F0, α)
can also be described via the so-called Chinese restaurant process. Consider a
restaurant with many large tables and label each table j with a realization yj
from F0. Then seat arrivals as follows: the ﬁrst person to arrive sits at the ﬁrst
table. The (n+ 1) th person sits at an empty table with probability α/(α+ n)
and to the right of a seated person with probability n/(α + n).
a. If xi denotes the label zj of the table where the i th person sits, show that
the sequence x1, x2, . . . is exchangeable (that is, that the distribution is
invariant under any permutation of the indices).
b. Show that this sequence can be seen as i.i.d. replications from F, with F
distributed from D(F0, α), using the conditional distribution given in Note
1.8.2.

44
Introduction
1
c. Show that this deﬁnition is also compatible with the deﬁnition of Exercise
1.64.
Note 1.8.3
1.67
∗(Hadjicostas and Berry (1999))
Consider independent observations xi
(i = 1, . . . , n) from Poisson distributions Poi(λiti), where the durations ti
are known. The prior on the λi’s is a gamma distribution G(α, β) with an
independence assumption. The model is hierarchical because the parameters
(α, β) are supposed to be distributed from a prior distribution π(α, β) such
that
π(α, β) ∝αk1(α + s1)k2βk3(β + s2)k4 ,
(1.7.10)
where the values ki and sj > 0 are known (i = 1, . . . , 4, j = 1, 2).
a. Show that the prior distribution (1.7.10) is proper if, and only if,
k1 + k2 + 1 < 0,
k1 + 1 > 0,
k3 + k4 + 1 < 0,
k3 + 1 > 0.
b. By integrating out the λi’s from the joint distribution of the λi’s and of
(α, β), derive the posterior (marginal) distribution of (α, β).
c. Show that the posterior (marginal) distribution of (α, β) is deﬁned (proper)
if, and only if,
k1 + y + 1 > 0,
k3 + r + 1 > 0,
k3 > k1 + k2
and either k3 + k4 + 1 < 0 or k3 + k4 + 1 = 0 and k1 + y > 0, where
y =
n

i=1
II0(xi),
r =
n

i=1
xi .
d. Verify that the conditions of a. imply the conditions of b. (as they should).
e. Show that the conditions of b. are satisﬁed when (k1, . . . , k4) = (−8, 0, −5, 0)
and (y, r) = (10, 337), while the conditions of a. are not satisﬁed.
f. Show that the conditions of b. are not satisﬁed when (k1, . . . , k4) = (−12, 0,
1, 1) and (y, r) = (10, 337).
Note 1.8.4
1.68
∗(Robins and Ritov (1997)) Consider i.i.d. observations (xi, yi) in (0, 1)k ×
IR from the following model: x ∼f(x), y|x ∼N(θ(x), 1), with the mean
function θ uniformly bounded on (0, 1)k and f in the set of densities such that
c < f(x) < 1/c uniformly on (0, 1)k, where c < 1 is a ﬁxed constant. Assume
that the quantity of interest is
ϕ =

(0,1)k
θ(x)dx .
a. Show that the space Θ of mean functions θ is inﬁnite dimensional.
b. Give the likelihood ℓ(θ, f) and show that it factorizes in a function of f
times a function of θ.
c. When f is known, show that (x1, . . . , xn) is ancillary.
d. When f is unknown, show that (x1, . . . , xn) is θ-ancillary, in the sense that
the conditional likelihood given (x1, . . . , xn) is a function of θ only, the
marginal distribution of (x1, . . . , xn) is a function of f only, and the pa-
rameter space is a product space. (See Cox and Hinkley (1974) and Robins
and Wasserman (2000) for details about this notion.)

1.8
Notes
45
e. When f is known, show that
1
n
n

i=1
yi
f(xi)
is a consistent estimator of ϕ. (In fact, it is √n uniformly consistent.)
f. When f is unknown, Robins and Ritov (1997) have shown that there is no
uniformly consistent estimator of ϕ. Deduce that, if the prior distribution
on (θ, f) factorizes as π1(θ)π2(f), the Bayesian inference on θ (and thus ϕ)
is the same whatever the value of f.
g. On the contrary, if the prior distribution on (θ, f) makes θ and f dependent,
and if f is known to be equal to f0, the posterior distribution will depend
on f0. Deduce that this dependence violates the Likelihood Principle.
[Note: The previous simpliﬁed description of Robins and Ritov (1997) follows
from Robins and Wasserman (2000).]
1.8 Notes
1.8.1 A brief history of Bayesian Statistics
Books have been written on the history of Bayesian Statistics, including Stigler
(1986), Dale (1991), Lad (1996) and Hald (1998), and we only point out here
a few highlights in the development of Bayesian Statistics in the last two
centuries.
As detailed in this chapter, the ﬁrst occurrence of Bayes’s formula took place
in 1761, in the setting of the binomial example of Section 1.2, exposed by
the Reverent Thomas Bayes before the Royal Society, and published posthu-
mously by his friend R. Price in 1763. Pierre Simon Laplace then rediscovered
this formula in a greater generality in 1773, apparently ignoring Bayes’s previ-
ous work. The use of the Bayesian principle then became common in the 19th
century, as reported in Stigler (1986), but criticisms started to arise by the
end of the 19th century, as for instance in Venn (1886) or Bertrand (1889), fo-
cusing on the choice of the uniform prior and the resulting reparameterization
paradoxes, as reported by Zabell (1989).
Then, despite further formalizations of the Bayesian paradigm by Edgeworth
and Karl Pearson at the turn of the century and Keynes (1921) later, came
ﬁrst Kolmogorov, whose axiomatization of the theory of probabilities in the
1920s seemed to go against the Bayesian paradigm and the notion of subjective
probabilities, and second Fisher, who moved away from the Bayesian approach
(Fisher (1912)) to the deﬁnition of the likelihood function (Fisher (1922)), then
to ﬁducial Statistics (Fisher (1930)), but never revised his opinion on Bayesian
Statistics. This is slightly paradoxical, since ﬁducial Statistics was, in a sense,
an attempt to overcome the diﬃculty of selecting the prior distribution by
deriving it from the likelihood function (Seidenfeld (1992)), in the spirit of the
noninformative approaches of Jeﬀreys (1939) and Bernardo (1979).
For instance, considering the relation O = P + ϵ where ϵ is an error term,
ﬁducial Statistics argues that, if P (the cause) is known, O (the eﬀect) is
distributed according to the above relation. Conversely, if O is known, P = O−
ϵ is distributed according to the symmetric distribution. In this perspective,
observations and parameters play a symmetric role, depending on the way the

46
Introduction
1
model is analyzed, i.e., depending on what is known and what is unknown.
More generally, the ﬁducial approach consists of renormalizing the likelihood
(1.2.1) so that it becomes a density in θ when

Θ
ℓ(θ|x) dθ < +∞,
thus truly inverting the roles of x and θ. As can be seen in the above example,
the argument underlying the causal inversion is totally conditional: conditional
upon P, O = P + ϵ while, conditional upon O, P = O −ϵ. Obviously, this
argument does not hold from a probabilistic point of view: if O is a random
variable and P is a (constant) parameter, to write P = O −ϵ does not imply
that P becomes a random variable. Moreover, the transformation of ℓ(θ|x)
into a density is not always possible. The ﬁducial approach was progressively
abandoned after the exposure of fundamental paradoxes (see Stein (1959),
Wilkinson (1977) and the references in Zabell (1992)).
Jeﬀreys’s (1939) book appears as the ﬁrst modern treatise on Bayesian Statis-
tics: it contains, besides the idea of a noninformative prior, those of predictive
distribution, Bayes factors and improper priors. But it came out at the time of
Fisher’s development of likelihood Statistics and Neyman’s (1934) conﬁdence
intervals and did not meet with the same success. Alternatives to Bayesian
Statistics then became the standard in the 1930s with the introduction of
maximum likelihood estimators and the development of a formalized theory
of Mathematical Statistics, where prior distributions only appeared as a way
of constructing formally optimal estimators as in Wald (1950) or Ibragimov
and Has’minskii (1981) (see Chapter 8). Attempts to formalize further the
Bayesian approach to Statistics by Gini or de Finetti from the 1930s to the
1970s did not bring it more popularity against the then-dominant Neyman–
Pearson paradigm, even though the Bayesian community was growing and
produced treatises such as those of Savage (1954) and Lindley (1965, 1971).
It can be argued that it is only recently that Bayesian Statistics got a new
impetus, thanks to the development of new computational tools—which have
always been central to the Bayesian paradigm—and the rapidly growing in-
terest of practitioners in this approach to statistical modeling, as stressed in
Berger’s (2000) view of the present and future states of Bayesian Statistics.
The vitality of current Bayesian Statistics can be seen through the percentage
of Bayesian papers in Statistics journals as well as in other ﬁelds. It thus looks
as though practitioners in this century will be taking better heed of Bayesian
Statistics than their twentieth-century counterparts.
1.8.2 Bayesian nonparametric Statistics
While this book sticks to the parametric approach to Statistics, there is a large
literature on Bayesian nonparametric Statistics. First, optimality notions such
as minimaxity are central to functional estimation; similar to the parametric
case (see Chapter 3), Bayes estimators can be used to determine minimaxity
bounds and minimax estimators.
A second and much less formal aspect is to envision Bayesian prior modeling in
a inﬁnite dimensional space. This is obviously harder, for mathematical as well
as prior construction, reasons. But a ﬁrst solution is to stand in the grey area
between parametric and non-parametric Statistics as in Example 1.4.3: the

1.8
Notes
47
number of parameters is ﬁnite but grows to inﬁnity with the number of obser-
vations. This is, for instance, the case with kernel estimation, where a density
is approximated by a mixture
1
nσ
n

i=1
K
x −xi
σ

,
K being a density function, and where σ can be estimated in a Bayesian way,
with Hermite expansions (Hjort (1996)), or with wavelets bases (M¨uller and
Vidakovic (1999, Chapter 1)), where a function f is decomposed in a functional
basis,
f(x) =

i

j
ωijΨ

x −μi
σj

,
where Ψ denotes a special function called the mother wavelet, like the Haar
wavelet
Ψ(x) = II[0,1/2) −II[1/2,1) ,
where the scale and location parameters μi and σj are ﬁxed and known, and
where the coeﬃcients ωij can be associated with a prior distribution like
(Abramovich et al. (1998))
ωij ∼ϱiN(0, τ 2
i ) + (1 −ϱi)δ0 ,
where δ0 denotes the Dirac mass at 0.
A second solution, when estimating a c.d.f. F, is to put a prior distribution on
F. The most common choice is to use a Dirichlet distribution D(F0, α) on F,
F0 being the prior mean and α the precision, as introduced in Ferguson (1974).
This prior distribution enjoys the coherency property that, if F ∼D(F0, α),
the vector (F(A1), . . . , F(Ap)) is distributed as a Dirichlet variable in the usual
sense Dp(αF0(A1), . . . , αF0(Ap)) for every partition (A1, . . . , Ap). But it also
leads to posterior distributions which are partly discrete: if x1, . . . , xn are
distributed from F and F ∼D(F0, α), the marginal conditional distribution
of x1 given (x2, . . . , xn) is
α
α + n −1F0 +
1
α + n −1
n

i=2
δxi .
(See also Exercises 1.64 and 1.66 for other characterizations.) The approxima-
tion of the posterior distribution requires advanced computational tools that
will be developed in Chapter 6. (See Note 6.6.7 for more details.) Other propo-
sals have thus appeared in the literature such as the generalized Dirichlet dis-
tribution (Hjort (1996)), P´olya tree priors (Fabius (1964), Lavine (1992)), beta
process prior (Hjort (1990)), L´evy process priors (Phillips and Smith (1996)).
As a concluding note, let us mention that a recent trend in Bayesian statistics
has been to study models with varying dimensions, such as mixtures, hidden
Markov models and other dynamic models, as well as neural networks, thanks
to new computational tools developed by Grenander and Miller (1994), Green
(1995), Phillips and Smith (1996), or Stephens (1997) (see Chapter 6). This
is, for instance, the case with mixtures
k

i=1
pikϕ(x|θik)

48
Introduction
1
where ϕ(·|θ) is a parametrized density, the sum of the weights pik sum up to
1, and the number of components k is unknown. While this is a well-deﬁned
parametric problem, it is closer to nonparametric imperatives than to stan-
dard parametric estimation (see Richardson and Green (1997) or Stephens
(2000)).
1.8.3 Proper posteriors
It must by now be clear from Section 1.5 that an improper prior π can only
be used for inference purposes if (1.7.7) holds for the observation x at hand.
If this condition is not satisﬁed, posterior quantities like the posterior mean
or posterior median have no meaning, since, for instance, the ratio

Θ f(x|θ)π(θ) dθ

Θ θf(x|θ)π(θ) dθ
is not deﬁned. To verify that (1.7.7) is satisﬁed in complex models can be quite
diﬃcult (see Exercises 1.61 and 1.62) or even simply impossible. Unfortunately,
because of computational innovations such as the Gibbs sampler (see Chapter
6), it is possible to work directly from the relation π(θ|x) ∝f(x|θ)π(θ) to
simulate values from the posterior π(θ|x), but the simulation output does not
always signal that the posterior does not exist (see Hobert and Casella (1998)).
There are therefore examples in the literature where data have been analyzed
with such undeﬁned posteriors, this lack of deﬁnition of the posterior been
only discovered years later.
We will see in Note 6.6.4, however, that there are good reasons to work with
improper posteriors on extended spaces, that is, through a completion of θ in
(α, θ), as long as the properness of the posterior π(θ|x) is satisﬁed.
1.8.4 Asymptotic properties of Bayes estimators
We do not develop the asymptotic point of view in this book for two main
reasons, the ﬁrst of which being that the Bayesian point of view is intrinsically
conditional. When conditioning on the observation x, which may be a sample
(x1, . . . , xn), there is no reason to wonder what might happen if n goes to
inﬁnity since n is ﬁxed by the sample size. Theorizing on future values of the
observations thus leads to a frequentist analysis, opposite to the imperatives
of the Bayesian perspective. The second point is that, even though it does
not integrate asymptotic requirements, Bayesian procedures perform well in
a vast majority of cases under asymptotic criteria. It is not so paradoxical
that, most often, the Bayesian perspective, and in particular the choice of
the prior distribution, cease to be relevant when the number of observations
gets inﬁnitely large compared with the number of parameters. (There are
well-known exceptions to this ideal setting, as in the Neyman–Scott problem
of Example 3.5.10, in Diaconis and Freedman (1986), where the number of
parameters increases with the number of observations and leads to inconsistent
Bayes estimators, or in Robins and Ritov (1997), as detailed in Exercise 1.68.)
In a general context, Ibragimov and Has’minskii (1981, Chapter 1) show that
Bayes estimators are consistent, that is, that they almost surely converge
to the true value of the parameter when the number of observations goes
to inﬁnity. This is, for instance, the case with estimators δα (α ≥1) that
minimize the posterior loss (see Chapter 2) associated with the loss function

1.8
Notes
49
L(δ, θ) = |θ−δ|α, under fairly weak constraints on the prior distribution π and
the sampling density f(x|θ). Ibragimov and Has’minskii (1981, Chapter 3) also
establish (under more stringent conditions) the asymptotic eﬃciency of some
Bayes estimates, that is, that the posterior distribution converges towards the
true value at the rate n−1/2. (See Schervish (1995) for more details.)
Barron et al. (1999) also give general conditions for consistency of a posterior
distribution in the following sense: the posterior probability of every Hellinger
neighborhood of the true distribution tends to 1 almost surely when the sample
size goes to inﬁnity. (The Hellinger distance between two densities f1 and f2
(or the corresponding distributions) is deﬁned as
d(f1, f2) =
 	
f1(x)1/2 −f2(x)1/2
2 dx .
We will use it for decision-theoretic purposes in Chapter 2.) The basic as-
sumption on the prior distribution π is that it gives positive mass to every
Kullback–Leibler neighborhood of the true distribution. (We will also use the
Kullback–Leibler pseudo-distance in Chapter 2.)
We will come back to asymptotics, nonetheless, in Chapter 3 with the deﬁni-
tion of noninformative priors via the asymptotic approximation of tail beha-
viors, and in Chapter 6 with the Laplace approximation to posterior integrals.


CHAPTER 2
Decision-Theoretic Foundations
Today would run out according to the Pattern. But over and over he mulled
over the decisions he had made since he ﬁrst entered the Waste. Could he have
done something diﬀerent, something that would have avoided this day, this
place? Next time, perhaps.
Robert Jordan, The Fires of Heaven, Book V of the Wheel of Time.
2.1 Evaluating estimators
Considering that the overall purpose of most inferential studies is to provide
the statistician (or a client) with a decision, it seems reasonable to ask for an
evaluation criterion of decision procedures that assesses the consequences
of each decision and depends on the parameters of the model, i.e., the
true state of the world (or of Nature). These decisions can be of various
kinds, ranging from buying equities depending on their future returns θ,
to stopping an agricultural experiment on the productivity θ of a new
crop species, to estimating the underground economy contribution θ to
the U.S. GNP, to deciding whether the number θ of homeless people has
increased since the last census. They also include assessing whether a new
scientiﬁc theory is compatible with the experimental evidence at hand. If
no evaluation criterion is available, it is impossible to compare diﬀerent
decision procedures and absurd solutions, such as proposing ˆθ = 3 for
any real estimation problem or even more dramatically the answer one
wants to impose, can only be eliminated by ad-hoc reasoning. To avoid such
reasoning implies a reinforced axiomatization of the statistical inferential
framework, called Decision Theory. This augmented theoretical structure
is necessary for Statistics to reach a coherence otherwise unattainable1.
Although almost everybody agrees on the need for such an evaluation
criterion, there is an important controversy running about the choice of this
1 The Bayesian approach is, from our point of view, the ultimate step in this quest for
coherence.

52
Decision-Theoretic Foundations
2
evaluation criterion, since the consequences on the decision are not innocu-
ous. This diﬃculty even led some statisticians to totally reject Decision
Theory, on the basis that a practical determination of the decision-maker
evaluation criterion is utterly impossible in most cases.
This criterion is usually called loss and is deﬁned as follows, where D
denotes the set of possible decisions. D is called the decision space and
most theoretical examples focus on the case D = Θ, which represents the
standard estimation setting.
Deﬁnition 2.1.1 A loss function is any function L from Θ×D in [0, +∞).
This loss function is supposed to evaluate the penalty (or error) L(θ, d)
associated with the decision d when the parameter takes the value θ. In a
traditional setting of parameter estimation, when D is Θ or h(Θ), the loss
function L(θ, δ) measures the error made in evaluating h(θ) by δ. Section 2.2
introduces a set of so-called rationality axioms that ensures the existence
of such a function in a decision setting.
The actual determination of the loss function is often awkward in prac-
tice, in particular because the determination of the consequences of each
action for each value of θ is usually impossible when D or Θ are large sets,
for instance when they have an inﬁnite number of elements. Moreover, in
qualitative models, it may be delicate to quantify the consequences of each
decision. We will see through paradoxes like the Saint Petersburg paradox
that, even when the loss function seems obvious, for instance when errors
can be expressed as monetary losses, the actual loss function can be quite
diﬀerent from its intuitive and linear approximation.
The complexity of determining the subjective loss function of the decision-
maker often prompts the statistician to use classical (or canonical) losses,
selected because of their simplicity and mathematical tractability. Such
losses are also necessary for a theoretical treatment of the derivation of
optimal procedures, when there is no practical motivation for the choice of
a particular loss function. The term classical is related to their long history,
dating back to Laplace (1773) for the absolute error loss (2.5.3) and Gauss
(1810) for the quadratic loss (2.5.1), when errors in terms of performance of
estimators or consequences of decisions were confused with errors in terms
of the irreducible variability of random variables (variance). But this at-
tribute should not be taken as a value statement, since an extensive use of
these losses does not legitimize them any further. In fact, the recourse to
such automatic (or generic) losses, although often justiﬁed in practice—it
is still better to take a decision in a ﬁnite time using an approximate crite-
rion rather that spending an inﬁnite time to determine exactly the proper
loss function—has generated many of the criticisms addressed to Decision
Theory.
A fundamental basis of Bayesian Decision Theory is that statistical in-
ference should start with the rigorous determination of three factors:

2.1
Evaluating estimators
53
(1) the distribution family for the observations, f(x|θ);
(2) the prior distribution for the parameters, π(θ);
(3) the loss associated with the decisions, L(θ, δ);
the prior and the loss, and even sometimes the sampling distribution,
being derived from partly subjective considerations. Classical decision-
theoreticians omit the second point. The frequentist criticisms of the Bay-
esian paradigm often fail to take into account the problem of the construc-
tion of the loss function, even though this may be at least as complicated as
the derivation of the prior distribution. In addition, to presuppose the exis-
tence of a loss function implies that some information about the problem at
hand is available. This information could therefore be used more eﬃciently
by building up a prior distribution. Actually, Lindley (1985) states that loss
and prior are diﬃcult to separate and should be analyzed simultaneously.
We will see in Section 2.4 an example of the duality existing between these
two factors. We also mention in Section 2.5.4 how classical losses could be
replaced by more intrinsic losses (similar to the noninformative priors intro-
duced in Chapter 3), when no information at all is available on the penalty
associated with erroneous decisions or even with the parameterization of
interest.
In some cases, it is possible to reduce the class of acceptable loss functions
by invariance considerations, for example when the model is invariant under
the action of a group of transformations. Such considerations apply as well
to the choice of the prior distribution, as we will see in Chapter 9. It is
also interesting to note that these invariance motivations are often used in
other decision-theoretic approaches, where a drastic reduction of the class
of inferential procedures is necessary to select a best solution.
Example 2.1.2 Consider the problem of estimating the mean θ of a nor-
mal vector, x ∼Nn(θ, Σ), where Σ is a known diagonal matrix with diag-
onal elements σ2
i (1 ≤i ≤n). In this case, D = Θ = IRp, and δ stands for
an evaluation of θ. If no additional information is available on the model,
it seems logical to choose the loss function so that it weights equally the
estimation of each component, i.e., to use a loss of the form
n

i=1
L
δi −θi
σi

,
where L takes its minimum at 0. Indeed, for such losses, the components
with larger variances do not strongly bias the selection of the resulting
estimator. In other words, the components with a larger variance are not
overly weighted when the estimation errors (δi −θi) are normalized by
σi. The usual choice of L is the quadratic loss L(t) = t2, i.e., the global
estimation error is the sum of the squared componentwise errors.
∥

54
Decision-Theoretic Foundations
2
2.2 Existence of a utility function
The notion of utility (deﬁned as the opposite of loss) is used not only in
Statistics, but also in Economics and in other ﬁelds like Game Theory where
it is necessary to order consequences of actions or decisions. Consequences
(or rewards) are generic notions which summarize the set of outcomes re-
sulting from the decision-maker’s action. In the simplest cases, it may be
the monetary proﬁt or loss resulting from the decision. In an estimation set-
ting, it may be a measure of distance between the evaluation and the true
value of the parameter, as in Example 2.1.2. The axiomatic foundations of
utility are due to Von Neumann and Morgenstern (1947) and led to numer-
ous extensions, in particular in Game Theory. This approach is considered
in a statistical framework by Wald (1950) and Ferguson (1967). Extensions
and additional comments can be found in DeGroot (1970, Chapter 7) and
recent references on utility theory are Fishburn (1988) and Machina (1982,
1987). See also Chamberlain (2000) for a connection with econometrics.
The general framework behind utility theory considers R, space of re-
wards, which is assumed to be completely known. For instance, R = IR. We
also suppose that it is possible to order the rewards, i.e., that there exists
a total ordering, denoted ⪯, on R such that, if r1 and r2 are in R,
(1) r1 ⪯r2 or r2 ⪯r1; and
(2) if r1 ⪯r2 and r2 ⪯r3, then r1 ⪯r3.
These two properties seem to be minimal requirements in a decision-making
setting. In particular, transitivity (2) is absolutely necessary to allow a
comparison of decision procedures. Otherwise, we may end up with cycles
such as r1 ⪯r2 ⪯r3 ⪯r1 and be at a loss about selecting the best reward
among the three. Section 2.6 presents a criterion which is intransitive (and
thus does not pertain to Decision Theory). We denote by ≺and ∼the strict
order and equivalence relations derived from ⪯respectively. Therefore, one
and only one of the three following relations is satisﬁed by any pair (r1, r2)
in R2
r1 ≺r2,
r2 ≺r1,
r1 ∼r2.
To proceed further in the construction of the utility function, it is nec-
essary to extend the reward space from R to P, the space of probability
distributions on R. This also allows the decision-maker to take into ac-
count partly randomized decisions; moreover, the extended reward space is
convex.
Example 2.2.1
In most real-life settings, the rewards associated with
an action are not exactly known when the decision is taken or, equiva-
lently, some decisions involve a gambling step. For instance, in ﬁnance, the
monetary revenue r ∈R = IR derived from stock market shares is not guar-
anteed when the shareholder has to decide from which company she should
buy shares. In this case, D = {d1, . . . , dn}, where dk represents the action
“buy the share from company k.” At the time of the decision, the rewards

2.2
Existence of a utility function
55
associated with the diﬀerent shares are random dividends, only known by
the end of the year.
∥
The order relation ⪯is also assumed to be available on P. For instance,
when the rewards are monetary, the order relation on P can be derived by
comparing the average yields associated with the distributions P. There-
fore, it is possible to compare two distributions of probability on R, P1 and
P2. We thus assume that ⪯satisﬁes the extensions of the two hypotheses
(1) and (2) to P:
(A1)
P1 ⪯P2 or P2 ⪯P1; and
(A2)
if P1 ⪯P2 and P2 ⪯P3, then P1 ⪯P3.
The order relation on R then appears as a special case of the order on P
by considering the Dirac masses δr (r ∈R).
The existence of the order ⪯on P relies on the assumption that there
exists an optimal reward, therefore, that there exists at least a partial
ordering on the consequences, even when they are random. This is obviously
the case when there exists a function U on R associated with ⪯, such that
P1 ⪯P2 is equivalent to
IEP1[U(r)] ≤IEP2[U(r)],
as in the above monetary example. This function U is called the utility func-
tion. We now present an axiomatic system on ⪯that ensures the existence
of the utility function.
For simplicity’s sake, we only consider here the set of bounded distribu-
tions, PB, corresponding to the distributions with bounded support, for
which there exist r1 and r2 such that
[r1, r2] = {r : r1 ⪯r ⪯r2}
and
P([r1, r2]) = 1.
For P1, P2 in PB, we deﬁne the mixture P = αP1 + (1 −α)P2 as the distri-
bution that generates a reward from P1 with probability α and a reward
from P2 with probability (1−α). For instance, αr1 +(1−α)r2 is the distri-
bution that gives the reward r1 with probability α and the reward r2 with
probability (1 −α). Two additional assumptions (or axioms) are necessary
to derive the existence of a utility function on R. First, there must be con-
servation of the ordering under indiﬀerent alternatives:
(A3)
if P1 ⪯P2, αP1 + (1 −α)P ⪯αP2 + (1 −α)P for every P ∈P.
For example, if the share buyers of Example 2.2.1 can compare two com-
panies with dividend distributions P1 and P2, they should be able to keep
a ranking of the two companies if there is a chance (1 −α) that both divi-
dends are replaced by state bounds with dividend distribution P. The order
relation must also be connected (or closed):
(A4)
if P1 ⪯P2 ⪯P3, there exist α and β ∈(0, 1) such that
αP1 + (1 −α)P3 ⪯P2 ⪯βP1 + (1 −β)P3.

56
Decision-Theoretic Foundations
2
The last assumption then implies the following result.
Lemma 2.2.2
If r1, r2, and r are rewards in R with r1 ≺r2 and r1 ⪯
r ⪯r2, there exists a unique v (0 ≤v ≤1) such that r ∼vr1 + (1 −v)r2.
Lemma 2.2.2 is actually the key to the derivation of the utility function,
U, on R. Indeed, given r1 and r2, two arbitrary rewards such that r2 ≺r1,
we can deﬁne U in the following way. For every r ∈R, consider
(i) U(r) = v if r2 ⪯r ⪯r1 and r ∼vr1 + (1 −v)r2;
(ii) U(r) =
−v
1−v
if r ⪯r2 and r2 ∼vr1 + (1 −v)r; and
(iii) U(r) = 1
v
if r1 ⪯r and r1 ∼vr + (1 −v)r2.
In particular, U(r1) = 1 and U(r2) = 0. Moreover, this function U preserves
the order relation on R (see DeGroot (1970, p. 105) for a proof).
Lemma 2.2.3
If r1, r2, and r3 are three rewards in R such that r2 ∼
αr1 + (1 −α)r3
U(r2) = αU(r1) + (1 −α)U(r3).
Actually, the axioms (A3) and (A4) can be further reduced while Lemma
2.2.3 still holds. It is indeed suﬃcient that they are satisﬁed on R only.
The extension of the deﬁnition of the utility function to PB calls for an
additional assumption. Given P such that P([r1, r2]) = 1, deﬁne
α(r) = U(r) −U(r1)
U(r2) −U(r1)
and
β =

[r1,r2]
α(r) dP(r).
Then the additional axiom
(A5)
P ∼βδr2 + (1 −β)δr1
implies that, if r is equivalent to α(r)r1 +(1 −α(r))r2 for every r ∈[r1, r2],
this equivalence must hold on average. In fact, notice that β is derived from
the expected utility,
β = IEP [U(r)] −U(r1)
U(r2) −U(r1)
,
and this assumption provides a deﬁnition of U on PB. As in Lemma 2.2.3
where U is restricted to R, and as shown by the following result, axiom (A5)
indicates that U provides a linearization (or a linear parameterization) of
the order relation ⪯on PB. Although slightly tautological—since it involves
in its formulation the utility function we are trying to derive—, (A5) indeed
leads to the following extension of Lemma 2.2.3 to PB.
Theorem 2.2.4
Consider P1 and P2 in PB. Then,
P1 ⪯P2

2.2
Existence of a utility function
57
if and only if
IEP1[U(r)] ≤IEP2[U(r)].
Moreover, if U ∗is another utility function satisfying the above equivalence
relation, there exist a > 0 and b such that
U ∗(r) = aU(r) + b.
Proof.
Consider r1 and r2 such that
P1([r1, r2]) = P2([r1, r2]) = 1
(with r1 ≺r2). Since
P1 ∼IEP1[U(r)] −U(r1)
U(r2) −U(r1)
r2 + U(r2) −IEP1[U(r)]
U(r2) −U(r1)
r1
and
P2 ∼IEP2[U(r)] −U(r1)
U(r2) −U(r1)
r2 + U(r2) −IEP2[U(r)]
U(r2) −U(r1)
r1,
P1 ⪯P2 is truly equivalent to
IEP1[U(r)] −U(r1)
U(r2) −U(r1)
≤IEP2[U(r)] −U(r1)
U(r2) −U(r1)
,
i.e., IEP1[U(r)] ≤IEP2[U(r)]. Moreover, for any other utility function U ∗,
there exist a and b such that U ∗(r1) = aU(r1) + b, U ∗(r2) = aU(r2) + b.
The extension of this relation to every r ∈R follows from Lemma 2.2.3.
22
Notice that the above derivation does not involve any restriction on the
function U. Therefore, it does not need to be bounded, although this condi-
tion is often mentioned in textbooks. It can be argued that this generality is
artiﬁcial and formal, since subjective utility functions are always bounded.
For instance, when considering monetary rewards, there is a psychologi-
cal threshold, say $100,000,000, above which (most) individuals have an
almost constant utility function.
However, this upper bound varies from individual to individual, and even
more so from individuals to companies or states. It is also important to in-
corporate unacceptable rewards, although the assumption (A4) prevents
rewards with utility equal to −∞. (This restriction implies that the death
of a patient in a pharmaceutical study or a major accident in a nuclear plant
have a ﬁnite utility.) Moreover, most theoretical losses are not bounded. A
counterpart of this generality is that the above results have only been estab-
lished for PB. Actually, they can be extended to PE, the set of distributions
P in P such that IEP [U(r)] is ﬁnite, under the assumption that (A1)–(A5)
and two additional hypotheses are satisﬁed for PE (see Exercise 2.3).
Theorem 2.2.5
Consider P and Q, two distributions in PE. Then, P ⪯
Q if and only if
IEP [U(r)] ≤IEQ[U(r)].

58
Decision-Theoretic Foundations
2
Of course, Theorem 2.2.5 fails to deal with inﬁnite utility distributions. If
such distributions exist, they must be compared between themselves and a
separate utility function constructed on this restricted class, since they are
in a sense the only distributions of interest. However, the loss functions con-
sidered in the sequel are bounded from below, usually by 0. Therefore, the
corresponding utility functions, opposites of the loss functions, are always
bounded from above and inﬁnite reward paradoxes can be avoided. (Rubin
(1984) and Fishburn (1987) provide reduced axiomatic systems ensuring
the existence of a utility function.)
Many criticisms have been addressed on theoretical or psychological
grounds against the notion of rationality of decision-makers and the associ-
ated axioms (A1)–(A4). First, it seems illusory to assume that individuals
can compare all rewards, that is, that they can provide a total ordering
of P (or even of R) because their discriminating abilities are necessarily
limited, especially about contiguous or extreme alternatives. The transi-
tivity assumption is also too strong, since examples in sports or politics
show that real-life orderings of preferences often lead to nontransitivity,
as illustrated by Condorcet and Simpson paradoxes (see Casella and Wells
(1993) and Exercises 1.9 and 2.2). More fundamentally, the assumption
that the ordering can be extended from R to P has been strongly attacked
because it implies that a social ordering can be derived from a set of in-
dividual orderings and this is not possible in general (see Arrow (1951)
or Blyth (1993)). However, while recognizing this fact, Rubin (1987) notes
that this impossibility just implies that utility and prior are not separable,
not that an optimal (Bayesian) decision cannot be obtained, and he gives a
restricted set of axioms pertaining to this purpose. In general, the criticisms
above are obviously valuable, but cannot stand against the absolute need
of an axiomatic framework validating decision-making under uncertainty.
As already mentioned in Chapter 1, statistical modeling is and must be re-
ductive; although necessarily missing part of the complexity of the world,
the simpliﬁed representation it gives of this world allows statisticians and
others to reach decisions. Decision Theory thus describes an idealized set-
ting, under an ultimate rationality real decision-makers fail to attain, but
aim at nonetheless2.
From a more practical point of view, the above derivation of the utility
function can be criticized as being unrealistic. Berger (1985a) provides a few
examples based on DeGroot (1970), deriving the utility function from suc-
cessive partitions of the reward space (see also Raiﬀa and Schlaifer (1961)).
However, if R is large (e.g., noncountable), U cannot be evaluated for each
reward r, even though the linearity exhibited by Lemma 2.2.3 allows for
approximations when R ⊂IR. In a multidimensional setting, linear ap-
proximations are no longer possible unless one uses a linear combination of
2 To borrow from Smith (1984), to criticize the idealized structures of Decision Theory
because of human limitations is somehow akin to attacking integration because some
integrals can only be solved numerically.

2.2
Existence of a utility function
59
componentwise utilities, i.e.,
U(r1, r2, . . . , rn) =
n

i=1
αiUi(ri)
(see Raiﬀa (1968), Keeney and Raifa (1976) and Smith (1988) for a discus-
sion). In general, practical utility functions will thus only approximate the
true utility functions.
Even when the reward is purely monetary there is a necessity of rigorous
determination of the utility function because U may be far from linear,
especially for large rewards. This means that a gain of $3000 with proba-
bility 1/2 may not be equivalent to earning $1500 with certainty. To solve
this paradox, Laplace (1795) introduced the notion of moral expectation,
derived from the relative value of an increase of wealth, “absolute value di-
vided by the total wealth of the involved person.” Laplace deduces that the
moral expectation “coincides with the mathematical expectation when the
wealth becomes inﬁnite compared with the variations due to uncertainty,”
meaning the utility is indeed linear only around 0. Otherwise, risk aver-
sion attitudes slow down the utility curve, which is typically concave for
large values of rewards and bounded above. (Persons with a convex utility
function are called risk lovers because they prefer a random gain to the ex-
pectation of this gain. Notice that this attitude is quite understandable in
a neighborhood of 0.) To construct the money utility function is obviously
more cumbersome than to use a linear utility, but this derivation gives a
more accurate representation of reality and can even prevent paradoxes
such as the following one.
Example 2.2.6 (Saint Petersburg Paradox)
Consider a game where a
coin is thrown until a head appears. When this event occurs at the nth
throw, the player gain is 3n, leading to an average gain of
+∞

n=1
3n 1
2n = +∞.
Every player should then be ready to pay an arbitrarily high entrance fee
to play this game, even though there is less than a 0.05 chance to go beyond
the ﬁfth throw! This modeling does not take into account that the fortune
of a player is necessarily bounded and that he or she can only play a limited
number of games. A solution to this paradox is to substitute for the linear
utility function a bounded utility function, such as
U(r) =
r
δ + r
(δ > 0, r > −δ),
and U(r) = −∞otherwise. This construction is quite similar to Laplace’s
moral expectation. An acceptable entrance fee e will then be such that the
expected utility of the game is larger than the utility of doing nothing, i.e.,
IE[U(r −e)] ≥U(0) = 0.

60
Decision-Theoretic Foundations
2
Consider now a modiﬁcation to the game such that the player can leave the
game at any time n and take the gain 3n if a head has not yet appeared.
The average gain at time n is then
3n
δ + 3n 2−n,
which can provide an optimal leaving time n0, depending on the utility
parameter δ, which in its turn somehow characterizes the risk aversion of
the player (see Smith (1988) for a more thorough description). For instance,
δ may represent the fortune of the player, since U(τ) goes to −∞when
τ goes to −δ. The particular choice of U can obviously be criticized, but
a more accurate representation of the utility function requires a detailed
analysis of the motivations of the player (see also Exercise 2.9).
∥
See also Bernardo and Smith (1994) for a detailed analysis of the founda-
tions of Utility Theory, with in particular a description of decision trees.
2.3 Utility and loss
Let us switch back to a purely statistical setting. From a decision-theoretic
point of view, the statistical model now involves three spaces: X, observa-
tion space, Θ, parameter space, and D, decision space (or action space).
Statistical inference then consists of taking a decision d ∈D related to the
parameter θ ∈Θ based on the observation x ∈X, x and θ being related
by the distribution f(x|θ). In most cases, the decision d will be to evalu-
ate (or estimate) a function of θ, h(θ), as accurately as possible. Decision
Theory assumes in addition that each action d can be evaluated (i.e., that
its accuracy can be quantiﬁed) and leads to a reward r, with utility U(r)
(which exists under the assumption of rationality of the decision-makers).
From now on, this utility is written as U(θ, d) to stress that it only depends
on these two factors. When other random factors r are involved in U, we
take U(θ, d) = IEθ,d[U(r)]. Therefore, U(θ, d) can be seen as a measure of
proximity between the proposed estimate d and the true value h(θ).
Once the utility function has been constructed (or approximated), we
derive the corresponding loss function
L(θ, d) = −U(θ, d).
In general, the loss function is supposed to be nonnegative, which implies
that U(θ, d) ≤0, and therefore that there is no decision with inﬁnite utility.
The existence of a lower bound on L can be criticized as being too stringent,
but it does avoid paradoxes such as those mentioned above. It can also be
argued that, from a statistical point of view, the loss function L indeed
represents the loss (or error) owing to a bad evaluation of the function of θ
of interest, and therefore that even the best evaluation of this function, i.e.,
when θ is known, can induce at best a null loss. Otherwise, there would be

2.3
Utility and loss
61
a lack of continuity of the loss function in d = θ which could even prevent
the choice of a decision procedure.
Obviously, except for the most trivial settings, it is generally impossible
to uniformly minimize (in d) the loss function L(θ, d) when θ is unknown.
In order to derive an eﬀective comparison criterion from the loss function,
the frequentist approach proposes to consider instead the average loss (or
frequentist risk)
R(θ, δ)
=
IEθ[L(θ, δ(x))]
=

X
L(θ, δ(x))f(x|θ) dx,
where δ(x) is the decision rule, i.e., the allocation of a decision to each
outcome x ∼f(x|θ) from the random experiment. The function δ, from X
in D, is usually called estimator (while the value δ(x) is called estimate of
θ). When there is no risk of confusion, we also denote the set of estimators
by D.
The frequentist paradigm relies on this criterion to compare estimators
and, if possible, to select the best estimator, the reasoning being that esti-
mators are evaluated on their long-run performance for all possible values
of the parameter θ. Notice, however, that there are several diﬃculties as-
sociated with this approach.
(1) The error (loss) is averaged over the diﬀerent values of x proportionally
to the density f(x|θ). Therefore, it seems that the observation x is not
taken into account any further. The risk criterion evaluates procedures
on their long-run performance and not directly for the given observa-
tion, x. Such an evaluation may be satisfactory for the statistician, but
it is not so appealing for a client, who wants optimal results for her
data x, not that of another’s!
(2) The frequentist analysis of the decision problem implicitly assumes that
this problem will be met again and again, for the frequency evaluation
to make sense. Indeed, R(θ, δ) is approximately the average loss over
i.i.d. repetitions of the same experiment, according to the Law of Large
Numbers. However, on both philosophical and practical grounds, there
is a lot of controversy over the very notion of repeatability of experi-
ments (see Jeﬀreys (1961)). For one thing, if new observations come to
the statistician, she should make use of them, and this could modify
the way the experiment is conducted, as in, for instance, medical trials.
(3) For a procedure δ, the risk R(θ, δ) is a function of the parameter θ.
Therefore, the frequentist approach does not induce a total ordering
on the set of procedures. It is generally impossible to compare decision
procedures with this criterion, since two crossing risk functions prevent
comparison between the corresponding estimators. At best, one may
hope for a procedure δ0 that uniformly minimizes R(θ, δ), but such
cases rarely occur unless the space of decision procedures is restricted.

62
Decision-Theoretic Foundations
2
Best procedures can only be obtained by restricting rather artiﬁcially
the set of authorized procedures.
Example 2.3.1
Consider x1 and x2, two observations from
Pθ(x = θ −1) = Pθ(x = θ + 1) = 0.5,
θ ∈IR.
The parameter of interest is θ (i.e., D = Θ) and it is estimated by estimators
δ under the loss
L(θ, δ) = 1 −IIθ(δ),
often called 0 −1 loss, which penalizes errors of estimation, whatever their
magnitude, by 1. Considering the particular estimator
δ0(x1, x2) = x1 + x2
2
,
its risk function is
R(θ, δ0)
=
1 −Pθ(δ0(x1, x2) = θ)
=
1 −Pθ(x1 ̸= x2) = 0.5.
This computation shows that the estimator δ0 is correct half of the time.
Actually, this estimator is always correct when x1 ̸= x2, and always wrong
otherwise. Now, the estimator δ1(x1, x2) = x1 + 1 also has a risk function
equal to 0.5, as does δ2(x1, x2) = x2 −1. Therefore, δ0, δ1 and δ2 cannot
be ranked under the 0 −1 loss.
∥
On the contrary, the Bayesian approach to Decision Theory integrates
on the space Θ since θ is unknown, instead of integrating on the space X
as x is known. It relies on the posterior expected loss
ϱ(π, d|x)
=
IEπ[L(θ, d)|x]
=

Θ
L(θ, d)π(θ|x) dθ,
which averages the error (i.e., the loss) according to the posterior distri-
bution of the parameter θ, conditionally on the observed value x. Given
x, the average error resulting from decision d is actually ϱ(π, d|x). The
posterior expected loss is thus a function of x but this dependence is not
troublesome, as opposed to the frequentist dependence of the risk on the
parameter because x, contrary to θ, is known.
Given a prior distribution π, it is also possible to deﬁne the integrated
risk, which is the frequentist risk averaged over the values of θ according
to their prior distribution
r(π, δ)
=
IEπ[R(θ, δ)]
=

Θ

X
L(θ, δ(x)) f(x|θ) dx π(θ) dθ.
One particular interest of this second concept is that it associates a real
number with every estimator, not a function of θ. It therefore induces a

2.3
Utility and loss
63
total ordering on the set of estimators, i.e., allows for the direct comparison
of estimators. This implies that, while taking into account the prior infor-
mation through the prior distribution, the Bayesian approach is suﬃciently
reductive (in a positive sense) to reach an eﬀective decision. Moreover, the
above two notions are equivalent in that they lead to the same decision.
Theorem 2.3.2
An estimator minimizing the integrated risk r(π, δ) can
be obtained by selecting, for every x ∈X, the value δ(x) which minimizes
the posterior expected loss, ϱ(π, δ|x), since
r(π, δ) =

X
ϱ(π, δ(x)|x)m(x) dx.
(2.3.1)
Proof.
Equality (2.3.1) follows directly from Fubini’s Theorem since, as
L(θ, δ) ≥0,
r(π, δ)
=

Θ

X
L(θ, δ(x))f(x|θ) dx π(θ) dθ
=

X

Θ
L(θ, δ(x))f(x|θ)π(θ) dθ dx
=

X

Θ
L(θ, δ(x))π(θ|x) dθ m(x) dx .
22
This result leads to the following deﬁnition of a Bayes estimator.
Deﬁnition 2.3.3
A Bayes estimator associated with a prior distribution
π and a loss function L is any estimator δπ which minimizes r(π, δ). For
every x ∈X, it is given by δπ(x), argument of mind ϱ(π, d|x). The value
r(π) = r(π, δπ) is then called the Bayes risk.
Theorem 2.3.2 thus provides a constructive tool for the determination of
the Bayes estimators. Notice that, from a strictly Bayesian point of view,
only the posterior expected loss ϱ(π, δ|x) is important, as the Bayesian
paradigm is based on the conditional approach. To average over all possi-
ble values of x, when we know the observed value of x, seems to be a waste
of information. Nonetheless, the equivalence exhibited in Theorem 2.3.2 is
important because, on one hand, it shows that the conditional approach
is not necessarily as dangerous as frequentists may depict it. This is so
because, while the Bayesian approach works conditional upon the actual
observation x, it also incorporates the probabilistic properties of the dis-
tribution of the observation, f(x|θ). On the other hand, this equivalence
provides a connection between the classical results of Game Theory (see
Section 2.4) and the axiomatic Bayesian approach, based on the posterior
distribution. It also explains why Bayes estimators play an important role
in frequentist optimality criteria.
The result above is valid for proper and improper priors, as long as the
Bayes risk r(π) is ﬁnite. Otherwise, the notion of a (decision-theoretic)

64
Decision-Theoretic Foundations
2
Bayes estimator is weakened: we then deﬁne a generalized Bayes estimator
as the minimizer, for every x, of the posterior expected loss. In terms of
frequentist optimality, we will see that the division between proper and
improper priors is much less important than the division between regular
and generalized Bayes estimators, since the formers are admissible. Notice
that, for strictly convex losses, the Bayes estimators are unique.
We conclude this section with an example of construction of a loss func-
tion in an expert calibration framework. References on this topic are DeG-
root and Fienberg (1983), Murphy and Winkler (1984), Bayarri and DeG-
root (1988) and Schervish (1989). Smith (1988) also shows how forecaster
evaluation can help improve the assessment of prior probabilities. See also
Note 2.8.1 for an illustration in imaging.
Example 2.3.4
Meteorological forecasts are often given as probability
statements such as “the probability of rain for tomorrow is 0.4.” Such
forecasts being quantiﬁed, it is of interest to evaluate weather forecasters
through a loss function (for their employers as well as users).
For a given forecaster, let N be the number of diﬀerent percentages
predicted at least once in a year and let pi (1 ≤i ≤N) be the corresponding
percentages. For instance, we may have N = 5 and
p1 = 0,
p2 = 0.45,
p3 = 0.7,
p4 = 0.9,
and p5 = 0.95.
In this case, the parameters θi are actually observed, i.e.,
θi = number of rainy days when pi is forecasted
number of days when pi is forecasted
(more exactly, this ratio is a good approximation of θi).
If qi denotes the proportion of days where pi is forecasted, a possible loss
function for the forecasters is
L(θ, p) =
N

i=1
qi(pi −θi)2 +
N

i=1
qi log(qi).
For a given set of θi’s (1 ≤i ≤N), the best forecaster is the perfectly
calibrated forecaster, i.e., the one who satisﬁes pi = θi (1 ≤i ≤N).
Moreover, among these perfect forecasters, the best one is the most well
balanced, satisfying qi = 1/N (1 ≤i ≤N), i.e., the more daring forecaster,
as opposed to a forecaster which would always give the same forecast, pi0,
because of the entropy term, 
i qi log(qi). However, the distance (pi −θi)2
could be replaced by any other function taking its minimum at pi = θi
(see Exercises 2.12 and 2.14). The weight qi in the ﬁrst sum is also used to
calibrate more properly forecasters, in order to prevent overpenalization of
rare forecasts.
This loss has been constructed with a bias in favor of forecasters with
large N, since the entropy log(N) increases with N. However, a better

2.4
Two optimalities: minimaxity and admissibility
65
performance for a larger N requires that pi is (almost) equal to θi and qi
is close to 1/N.
∥
2.4 Two optimalities: minimaxity and admissibility
This section deals with the two fundamental notions of frequentist Decision
Theory, introduced by Wald (1950) and Neyman and Pearson (1933a,b).
As mentioned above, and contrary to the Bayesian approach, the frequen-
tist paradigm is not reductive enough to lead to a single optimal estimator.
While we are mainly concerned in this book with the Bayesian aspects
of Decision Theory, it is still necessary to study these frequentist notions
in detail because they show that Bayes estimators are often optimal for
the frequentist concepts of optimality, therefore should still be considered
even when prior information is ignored. In other words, one can reject the
Bayesian paradigm and ignore the meaning of the prior distribution and
still obtain good estimators from a frequentist point of view when using
this prior distribution. Therefore, in this technical sense, frequentists should
also take into account the Bayesian approach, since it provides a tool for
the derivation of optimal estimators (see Brown (1971, 2000), Strawder-
man (1974), Berger (1985a), or Berger and Robert (1990) for examples).
Moreover, these properties can be helpful in the selection of a prior dis-
tribution, when prior information is not precise enough to lead to a single
prior distribution (see Chapter 3).
2.4.1 Randomized estimators
Similar to the study of the utility function, where we extended the reward
space from R to P, we need to extend the decision space to the set of
randomized estimators, taking values in D∗, space of the probability dis-
tributions on D. To use a randomized estimator δ∗means that the action
is generated according to the distribution with probability density δ∗(x, .),
once the observation x has been collected. The loss of a randomized esti-
mator δ∗is then deﬁned as the average loss
L(θ, δ∗(x)) =

D
L(θ, a)δ∗(x, a) da.
This extension is necessary to deal with minimaxity and admissibility. Ob-
viously, such estimators are not to be used, if only because they contradict
the Likelihood Principle, giving several possible answers for the same value
of x (and thus of ℓ(θ|x)). Moreover, it seems quite paradoxical to add noise
to a phenomenon in order to take a decision under uncertainty!
Example 2.4.1 (Example 2.3.1 continued) Consider the randomized
estimator
δ∗(x1, x2)(t) =
 II(x1+x2)/2(t)
if x1 ̸= x2,
[II(x1−1)(t) + II(x1+1)(t)]/2
otherwise,

66
Decision-Theoretic Foundations
2
where IIv denotes the Dirac mass at v. Actually, if x1 = x2, the two values
θ1 = x1 −1 and θ2 = x1 + 1 have the same likelihood. Compared with δ0
which never estimates θ correctly if x1 = x2, δ∗is exact with probability
1/2. However, when δ∗misses θ, it is farther away from θ than δ0. The
choice of the estimator then depends on the loss function, i.e., the way the
distance between the estimator and θ (or the error) is measured.
∥
Randomized estimators are nonetheless necessary from a frequentist point
of view, for instance, for the frequentist theory of tests, as they provide ac-
cess to conﬁdence levels otherwise unattainable (see Chapter 5). The set
D∗thus appears as a completion of D. However, this modiﬁcation of the
decision space does not modify the Bayesian answers, as shown by the fol-
lowing result (where D∗also denotes the set of functions taking values in
D∗).
Theorem 2.4.2
For every prior distribution π on Θ, the Bayes risk on
the set of randomized estimators is the same as the Bayes risk on the set
of nonrandomized estimators, i.e.,
inf
δ∈D r(π, δ) =
inf
δ∗∈D∗r(π, δ∗) = r(π).
Proof.
For every x ∈X and every δ∗∈D∗, we have

Θ

D
L(θ, a)δ∗(x, a)da π(θ|x)dθ
=

D

Θ
L(θ, a)π(θ|x)dθ δ∗(x, a)da
≥

D
inf
a

Θ
L(θ, a)π(θ|x)dθ

δ∗(x, a)da
= ϱ(π, δπ|x).
22
This result thus holds even when the Bayes risk r(π) is inﬁnite. The proof
relies on the fact that a randomized procedure averages the risks of nonran-
domized estimators and thus cannot improve on them. However, the fact
that randomized procedures are not relevant does not hold for the frequen-
tist risk unless some conditions, such as convexity, are imposed on the loss
function.
2.4.2 Minimaxity
The minimax criterion we introduce now appears as an insurance against
the worst case because it aims at minimizing the expected loss in the least
favorable case. It also represents a frequentist eﬀort to skip the Bayesian
paradigm while producing a (weak) total ordering on D∗.

2.4
Two optimalities: minimaxity and admissibility
67
Deﬁnition 2.4.3
The minimax risk associated with a loss function L is
the value
¯R = inf
δ∈D∗sup
θ
R(θ, δ) = inf
δ∈D∗sup
θ
IEθ[L(θ, δ(x))],
and a minimax estimator is any (possibly randomized) estimator δ0 such
that
sup
θ
R(θ, δ0) = ¯R.
This notion is validated by Game Theory, where two adversaries (here,
“the statistician” and “Nature”) are competing. Once the statistician has
selected a procedure, Nature selects the state of nature (i.e., the param-
eter) that maximizes the loss of the statistician. (We will see below that
this choice is usually equivalent to the choice of a prior distribution π.
Therefore, the Bayesian approach does not really ﬁt in that conﬂicting
framework, since the prior distribution is also supposed to be known.) In
general, it seems unfortunate to resort to such an antagonistic perspective
in a statistical analysis. Indeed, to perceive Nature (or reality) as an enemy
involves a bias toward the worst cases and prevents the statistician from us-
ing the available information (for an analysis and a defense of minimaxity,
see Brown (1993) and Strawderman (2000)).
The notion of minimaxity provides a good illustration of the conservative
aspects of the frequentist paradigm. Since this approach refuses to make
any assumption on the parameter θ, it has to consider the worst cases as
equally likely, and thus needs to focus on the maximal risk. In fact, from a
Bayesian point of view, it is often equivalent to take a prior concentrated
on these worst cases (see Section 2.4.3). In most settings, this point of view
is thus too conservative because some values of the parameter are less likely
than others.
Example 2.4.4 The ﬁrst oil-drilling platforms in the North Sea were de-
signed according to a minimax principle. In fact, they were supposed to
resist the conjugate action of the worst gale and the worst storm ever ob-
served, at the minimal record temperature. This strategy obviously gives
a comfortable margin of safety, but is quite costly. For more recent plat-
forms, engineers have taken into account the distribution of these weather
phenomena in order to reduce the production cost.
∥
Example 2.4.5 A waiting queue at a red light is usually correctly repre-
sented by a Poisson distribution. The number of cars arriving during the
observation time, N, is thus distributed according to P(λ), with the mean
parameter λ to be estimated. Obviously, the values of λ above a given limit
are quite unlikely. For instance, if λ0 is the number of cars in the whole
city, the average number of cars waiting at a given traﬃc light will not
exceed λ0. However, it may happen that some estimators are not minimax
because their risk are above ¯R for the largest values of λ.
∥

68
Decision-Theoretic Foundations
2
0
2
4
6
8
10
0
2
4
6
8
10
theta
Figure 2.4.1. Comparison of the risks of the estimators δ1 and δ2.
The example above does not directly criticize the minimax principle but
rather argues for the fact that some residual information is attached to
most problems and that it should be used, even marginally. In a similar
manner, Example 2.4.6 exhibits two estimators, δ1 and δ2, such that δ1
has a constant minimax risk ¯R and δ2 has a risk which can be as low as
¯R/10 but goes slightly above ¯R for the largest values of the parameter (see
Figure 2.4.2). Therefore, according to the minimax principle, δ1 should be
preferred to δ2, although the values of θ for which δ1 dominates δ2 are the
most unlikely (see Exercise 2.28 for another striking example).
Example 2.4.6
For reasons explained in Note 2.8.2, we consider the fol-
lowing estimator
δ2(x) =
⎧
⎨
⎩

1 −2p −1
||x||2

x
if ||x||2 ≥2p −1,
0
otherwise,
to estimate θ when x ∼Np(θ, Ip). This estimator, called the positive-part
James–Stein estimator, is evaluated under quadratic loss,
L(θ, d) = ||θ −d||2.
Figure 2.4.2 gives a comparison of the respective risks of δ2 and δ1(x) = x,
maximum likelihood estimator, for p = 10. This ﬁgure shows that δ2 cannot
be minimax, since the maximum risk of δ2 is above the (constant) risk of
δ1, that is, R(θ, δ2) = IEθ[||θ −δ2(x)||2] = p. (We show in Section 2.4.3
that δ1 is actually minimax in this case.) But the estimator δ2 is deﬁnitely
superior on the most interesting part of the parameter space, the additional
loss being in perspective quite negligible.
∥

2.4
Two optimalities: minimaxity and admissibility
69
Table 2.4.1. Utility function U(θi, aj).
a1
a2
θ1
−4
−10
θ2
8
30
The opposition between minimax and Bayesian analyses is illustrated by
the following example, which borrows from Game Theory (since there is no
observation or statistical model).
Example 2.4.7
Two persons, A and B, suspected of being accomplices
in a robbery, have been apprehended and placed in separate cells. Both
suspects are questioned and enticed to confess the burglary. Although they
cannot be convicted unless one of them talks, the incentive is that the
ﬁrst person to cooperate will get a reduced sentence. Table 2.4.7 provides
the rewards as perceived by A (in years of freedom), where a1 (resp. θ1)
represents the fact that A (resp. B) talks. The two suspects have an optimal
gain if they both remain silent. However, from A’s point of view, the optimal
strategy is to be the ﬁrst one to talk, i.e., a1, since maxθ R(a1, θ) = 4 and
maxθ R(a2, θ) = 10. Therefore, both burglars will end up in jail!
On the contrary, if π is the (subjective) probability assigned by A to the
event “B talks,” i.e., to θ1, the Bayes risk of a1 is
r(π, a1) = IEπ[−U(θ, a1)] = 4π −8(1 −π) = 12π −8
and, for a2,
r(π, a2) = IEπ[−U(θ, a2)] = 10π −30(1 −π) = 40π −30 .
It is straightforward to check that, for π ≤11/14, r(π, a2) is smaller than
r(π, a1). Therefore, unless A is convinced that B will talk, it is better for
A to keep silent.
∥
2.4.3 Existence of minimax rules and maximin strategy
An important diﬃculty related with minimaxity is that a minimax estima-
tor does not necessarily exist. Ferguson (1967) and Berger (1985a, Chapter
5) give suﬃcient conditions. In particular, there exists a minimax strategy
when Θ is ﬁnite and the loss function is continuous. More generally, Brown
(1976) (see also Le Cam (1986) and Strasser (1985)) considers the decision
space D as embedded in another space so that the set of risk functions on
D is compact in this larger space. From this perspective and under addi-
tional assumptions, it is then possible to derive minimax estimators when
the loss is continuous. However, these extensions involve topological tech-
niques too advanced to be considered in this book. Therefore, we only give
the following result (see Blackwell and Girshick (1954) for a proof).

70
Decision-Theoretic Foundations
2
Theorem 2.4.8
If D ⊂IRk is a convex compact set and if L(θ, d) is
continuous and convex as a function of d for every θ ∈Θ, there exists a
nonrandomized minimax estimator.
The restriction to nonrandomized estimators when the loss is convex
follows from Jensen’s inequality, since
L(θ, δ∗) = IEδ∗[L(θ, δ)] ≥L(θ, IEδ∗(δ)).
This result is a special case of the Rao–Blackwell Theorem (see Lehmann
and Casella (1998, p. 47)).
Example 2.4.9 (Example 2.4.1 continued) The randomized estima-
tor δ∗is uniformly dominated for every convex loss by the nonrandomized
estimator IEδ∗[δ∗(x1, x2)], i.e.,
˜δ(x1, x2) =
 1
2(x1 + x2)
if x1 ̸= x2,
1
2(x1 −1) + 1
2(x1 + 1) = x1
otherwise,
which is actually identical to the estimator δ0 considered originally. Notice
that this is not true for the 0 −1 loss where δ∗dominates ˜δ.
∥
The following result points out the connection between the Bayesian
approach and the minimax principle. (The proof is straightforward and
thus omitted.)
Lemma 2.4.10
The Bayes risks are always smaller than the minimax
risk, i.e.,
R = sup
π r(π) = sup
π
inf
δ∈D r(π, δ) ≤¯R = inf
δ∈D∗sup
θ
R(θ, δ).
The ﬁrst value is called maximin risk and a distribution π∗such that
r(π∗) = R is called a least favorable distribution, when such distributions
exist. In general, the upper bound r(π∗) is rather attained by an improper
distribution, which can be expressed as a limit of proper prior distributions
πn, but this phenomenon does not necessarily deter from the derivation of
minimax estimators (see Lemma 2.4.15). When they exist, least favorable
distributions are those with the largest Bayes risk, thus the less interesting
distributions in terms of loss performances if they are not suggested by the
available prior information. The above result is quite logical, in the sense
that prior information can only improve the estimation error, even in the
worst case.
A particularly interesting case corresponds to the following deﬁnition.
Deﬁnition 2.4.11
The estimation problem is said to have a value when
R = ¯R, i.e., when
sup
π
inf
δ∈D r(π, δ) = inf
δ∈D∗sup
θ
R(θ, δ).

2.4
Two optimalities: minimaxity and admissibility
71
When the problem has a value, some minimax estimators are the Bayes
estimators for the least favorable distributions. However, they may be ran-
domized, as illustrated by the following example. Therefore, the minimax
principle does not always lead to acceptable estimators.
Example 2.4.12
Consider3 a Bernoulli observation, x ∼Be(θ) with θ ∈
{0.1, 0.5}. Four nonrandomized estimators are available,
δ1(x)
=
0.1,
δ2(x) = 0.5,
δ3(x)
=
0.1 IIx=0 + 0.5 IIx=1,
δ4(x) = 0.5 IIx=0 + 0.1 IIx=1.
We assume in addition that the penalty for a wrong answer is 2 when
θ = 0.1 and 1 when θ = 0.5. The risk vectors (R(0.1, δ), R(0.5, δ)) of the
four estimators are then, respectively, (0, 1), (2, 0), (0.2, 0.5), and (1.8, 0.5).
It is straightforward to see that the risk vector of any randomized estimator
is a convex combination of these four vectors or, equivalently, that the risk
set, R, is the convex hull of the above four vectors, as represented by Figure
2.4.3.
In this case, the minimax estimator is obtained at the intersection of the
diagonal of IR2 with the lower boundary of R. As shown by Figure 2.4.3,
this estimator δ∗is randomized and takes the value δ3(x) with probability
α = 0.87 and δ2(x) with probability 1−α. The weight α is actually derived
from the equation
0.2α + 2(1 −α) = 0.5α.
This estimator δ∗is also a (randomized) Bayes estimator with respect to
the prior
π(θ) = 0.22 II0.1(θ) + 0.78 II0.5(θ);
the prior probability π1 = 0.22 corresponds to the slope between (0.2, 0.5)
and (2, 0), i.e.,
π1
1 −π1
=
0.5
2 −0.2.
Notice that every randomized estimator that is a combination of δ2 and
of δ3 is a Bayes estimator for this distribution, but that δ∗only is also a
minimax estimator.
∥
Similar to minimax estimators, a least favorable distribution does not
necessarily exist since its existence depends on a separating hyperplane the-
orem that does not always apply (see Pierce (1973), Brown (1976), Berger
(1985a), and Chapter 8). In addition, Strawderman (1973) shows that, in
the special case when x ∼Np(θ, Ip), there is no minimax proper Bayes
estimator if p ≤4. From a more practical point of view, Lemma 2.4.10
provides suﬃcient conditions of minimaxity.
Lemma 2.4.13 If δ0 is a Bayes estimator with respect to π0 and if R(θ, δ0)
≤r(π0) for every θ in the support of π0, δ0 is minimax and π0 is the least
favorable distribution.
3 The computations in this example are quite simple. See Chapter 8 for details.

72
Decision-Theoretic Foundations
2
0.0
0.5
1.0
1.5
2.0
0.0
0.2
0.4
0.6
0.8
1.0
δ2
δ4
δ1
δ3
δ*
Figure 2.4.2. Risk set for the estimation of the Bernoulli parameter.
Example 2.4.14 (Berger (1985a)) Consider x ∼B(n, θ) when θ is to be
estimated under the quadratic loss,
L(θ, δ) = (δ −θ)2.
Bayes estimators are then given by posterior expectations (see Section 2.5)
and, when θ ∼Be
√n
2 ,
√n
2

, the posterior mean is
δ∗(x) = x + √n/2
n + √n .
Moreover, this estimator has constant risk, R(θ, δ∗) = 1/4(1+√n)2. There-
fore, integrating out θ, r(π) = R(θ, δ∗) and δ∗is minimax according to
Lemma 2.4.13. Notice the diﬀerence with the maximum likelihood estima-
tor, δ0(x) = x/n, for the small values of n, and the unrealistic concentration
of the prior around 0.5 for larger values of n.
∥
Since minimax estimators usually correspond to generalized Bayes esti-
mators, it is often necessary to use a limiting argument to establish mini-
maxity, rather than computing directly the Bayes risk as in Lemma 2.4.13.
Lemma 2.4.15
If there exists a sequence (πn) of proper prior distribu-
tions such that the generalized Bayes estimator δ0 satisﬁes
R(θ, δ0) ≤lim
n→∞r(πn) < +∞
for every θ ∈Θ, then δ0 is minimax.
Example 2.4.16
When x ∼N(θ, 1), the maximum likelihood estimator
δ0(x) = x is a generalized Bayes estimator associated with the Lebesgue
measure on IR and the quadratic loss. Since R(δ0, θ) = IEθ(x−θ)2 = 1, this
risk is the limit of the Bayes risks r(πn) when πn is equal to N(0, n), as
r(πn) =
n
n+1. Therefore, the maximum likelihood estimator δ0 is minimax.
Note that this argument can be extended directly to the case x ∼Np(θ, Ip)
to establish that δ0 is minimax for every p.
∥

2.4
Two optimalities: minimaxity and admissibility
73
When the space Θ is compact, minimax Bayes rules (or estimators) can
be exactly described, owing to the separated zeros principle in complex
calculus: if R(θ, δπ) is not constant and is analytic, the set of θ’s where
R(θ, δπ) is maximal is separated and, in the case of a compact set Θ, is
necessarily ﬁnite.
Theorem 2.4.17
Consider a statistical problem that simultaneously has
a value, a least favorable distribution π0, and a minimax estimator δπ0.
Then, if Θ ⊂IR is compact and if R(θ, δπ0) is an analytic function of θ,
then either π0 has a ﬁnite support or R(θ, δπ0) is constant.
Example 2.4.18
Consider x ∼N(θ, 1), with |θ| ≤m, namely, θ ∈
[−m, m]. Then, according to Theorem 2.4.17, least favorable distributions
have necessarily a ﬁnite support, {±θi, 1 ≤i ≤ω}, with cardinal 2ω and
supporting points θi depending on m. In fact, the only estimator with con-
stant risk is δ0(x) = x, which is not minimax in this case. In general, the
exact determination of n and of the points θi can only be done numerically.
For instance, when m ≤1.06, the prior distribution with weights 1/2 at
±m is the unique least favorable distribution. Then, for 1.06 ≤m ≤2, the
support of π contains −m, 0, and m. See Casella and Strawderman (1981)
and Bickel (1981) for details, and Johnstone and MacGibbon (1992) for a
similar treatment of the Poisson model.
∥
The above examples show why, while being closely related to the Bayesian
paradigm, the minimax principle is not necessarily appealing from a Bay-
esian point of view. Indeed, apart from the fact that minimax estimators are
sometimes randomized, as in Example 2.4.12, Examples 2.4.14 and 2.4.18
show that the least favorable prior is often unrealistic because it induces
a strong prior bias towards a few points of the sample space. For Exam-
ple 2.4.18, Gatsonis et al. (1987) have shown that uniform priors are good
substitutes to the point mass priors, although they are not minimax.
Extensions of Theorem 2.4.17 to the noncompact case are given in Kemp-
thorne (1988). In multidimensional settings, when the problem is invariant
under rotation, the least favorable distributions are uniform on a sequence
of embedded spheres (see Robert et al. (1990)). The practical problem of
determining the points of the support is considered in Kempthorne (1987)
and Eichenauer and Lehn (1989).
In settings where the problem has a value, it is often diﬃcult to derive the
least favorable distribution and alternative methods are then necessary to
produce a minimax estimator. Chapter 9 shows how the exhibition of some
invariance structures of the model may lead to identify the best equivariant
estimator and a minimax estimator (Hunt–Stein Theorem). Unfortunately,
the conditions under which this theorem applies are diﬃcult to check and
often do not hold.
Lastly, when a minimax estimator has been derived, its optimality is still
to be assessed: there may exist several minimax estimators and some may

74
Decision-Theoretic Foundations
2
perform uniformly better than others. It is then necessary to introduce
a second (and more local) criterion to compare minimax estimators, i.e.,
estimators that perform well globally.
2.4.4 Admissibility
This second frequentist criterion induces a partial ordering on D∗by com-
paring the frequentist risks of the estimators, R(θ, δ).
Deﬁnition 2.4.19
An estimator δ0 is inadmissible if there exists an es-
timator δ1 which dominates δ0, that is, such that, for every θ,
R(θ, δ0) ≥R(θ, δ1)
and, for at least one value θ0 of the parameter,
R(θ0, δ0) > R(θ0, δ1).
Otherwise, δ0 is said to be admissible.
This criterion is particularly interesting for its reductive action. Indeed,
at least in theory, it seems logical to advocate that inadmissible estimators
should not be considered at all since they can be uniformly improved. For
instance, the Rao–Blackwell Theorem then implies that, for convex losses,
randomized estimators are inadmissible. However, admissibility alone is
not enough to validate the use of an estimator. For instance, constant
estimators δ(x) = θ0 are usually admissible because they produce the exact
value at θ = θ0. From a frequentist point of view, it is then important to
look for estimators satisfying both optimalities, that is, minimaxity and
admissibility. In this regard, two results can be mentioned.
Proposition 2.4.20
If there exists a unique minimax estimator, this es-
timator is admissible.
Proof.
If δ∗is the only minimax estimator, for any estimator ˜δ ̸= δ∗,
sup
θ
R(θ, ˜δ) > sup
θ
R(θ, δ∗).
Therefore, ˜δ cannot dominate δ∗.
22
Notice that the converse to this result is false, since there can exist several
minimax admissible estimators. For instance, in the Np(θ, Ip) case, there
exist proper Bayes minimax estimators when p ≥5 (Strawderman (1973)
and Fourdrinier and Strawderman (1999)). When the loss function L is
strictly convex (in d), it also allows for the following characterization.
Proposition 2.4.21 If δ0 is admissible with constant risk, δ0 is the unique
minimax estimator.
Proof.
For any θ0 ∈Θ, supθ R(θ, δ0) = R(θ0, δ0). Therefore, if there exists
δ1 such that ¯R ≤supθ R(θ, δ1) < R(θ0, δ0), δ0 cannot be admissible. Simi-
larly, if ¯R = supθ R(θ, δ1) = R(θ0, δ0) and if θ1 is such that R(θ1, δ1) < ¯R,

2.4
Two optimalities: minimaxity and admissibility
75
δ1 dominates δ0. Therefore, when δ0 is admissible, the only possible case is
that there exists δ1 such that R(θ, δ1) = R(θ, δ0) for every θ ∈Θ. And this
is also impossible when δ0 is admissible (see Exercise 2.36).
22
Again, notice that the converse of this result is false. There may be mini-
max estimators with constant risk that are inadmissible: actually, they are
certainly inadmissible if there are other minimax estimators. For instance,
this is the case for δ0(x) = x when x ∼Np(θ, Ip) and p ≥3 (see Note
2.8.2). There also are cases when there is no minimax admissible estimator
(this requires that there is no minimal complete class, see Chapter 8).
The previous section showed that minimaxity can sometimes be consid-
ered from a Bayesian perspective as the choice by Nature of a maximin
strategy (least favorable distribution), π, therefore that some minimax es-
timators are Bayes. Admissibility is even more strongly related to the Bayes
paradigm in the sense that, in most statistical problems, the Bayes estima-
tors are “spanning” the class of admissible estimators, i.e., the latter can
be expressed as Bayes estimators or generalized Bayes estimators or lim-
its of Bayes estimators. Chapter 8 deals in more detail with the relations
between Bayes estimators and admissibility. We only give here two major
results.
Proposition 2.4.22 If a prior distribution π is strictly positive on Θ, with
ﬁnite Bayes risk and the risk function, R(θ, δ), is a continuous function of
θ for every δ, the Bayes estimator δπ is admissible.
Proof.
Suppose δπ is inadmissible and consider δ′ which uniformly dom-
inates δπ. Then, for every θ, R(θ, δ′) ≤R(θ, δπ) and, in an open set C of
Θ, R(θ, δ′) < R(θ, δπ). Integrating out this inequality, we derive that
r(π, δ′) < r(π, δπ) =

Θ
R(θ, δπ)π(θ) dθ,
which is impossible.
22
Proposition 2.4.23
If the Bayes estimator associated with a prior π is
unique, it is admissible.
The proof of this result is similar to the proof of Proposition 2.4.20. Even
if the Bayes estimator is not unique, it is still possible to exhibit at least
one admissible Bayes estimator. When the loss function is strictly convex,
the Bayes estimator is necessarily unique and thus admissible, according
to the above proposition.
Example 2.4.24 (Example 2.4.14 continued) The estimator δ∗is a
(proper) Bayes estimator, therefore admissible, and it has constant risk.
Therefore, it is the unique minimax estimator under squared error loss. ∥
Notice that Proposition 2.4.22 contains the assumption that the Bayes
risk is ﬁnite. Otherwise, every estimator is, in a way, a Bayes estimator (see

76
Decision-Theoretic Foundations
2
Exercise 2.43). On the other hand, some admissibility results can be estab-
lished for improper priors. This is why we prefer to call generalized Bayes
estimators the estimators associated with an inﬁnite Bayes risk, rather
those corresponding to an improper prior. This choice implies that the
Bayes estimators of diﬀerent quantities associated with the same prior dis-
tribution can be simultaneously regular Bayes estimators and generalized
Bayes estimators, depending on what they estimate. This also guarantees
that regular Bayes estimators will always be admissible, as shown by the
following result.
Proposition 2.4.25
If a Bayes estimator, δπ, associated with a (proper
or improper) prior π and a strictly convex loss function, is such that the
Bayes risk,
r(π) =

Θ
R(θ, δπ)π(θ) dθ,
is ﬁnite, δπ is admissible.
Example 2.4.26 Consider x ∼N(θ, 1) and the null hypothesis H0 : θ ≤
0 is tested against the alternative hypothesis H1 : θ > 0. This testing prob-
lem is an estimation problem if we consider the estimation of the indicator
function IIH0(θ). Under the quadratic loss
(IIH0(θ) −δ(x))2 ,
we can propose the following estimator
p(x)
=
P0(X > x)
(X ∼N(0, 1))
=
1 −Φ(x),
called the p-value, which is considered as a good frequentist answer to the
testing problem (see Kiefer (1977) and Casella and Berger (1987)). Using
Example 1.5.1, it is easy to show that p is a generalized Bayes estimator
under Lebesgue measure and quadratic loss, since π(θ|x) is the N(x, 1)
distribution and
p(x)
=
IEπ[IIH0(θ)|x] = P π(θ < 0|x)
=
P π(θ −x < −x|x) = 1 −Φ(x).
Moreover, the Bayes risk of p is ﬁnite (Exercise 2.34). Therefore, the p-
value, when taken as an estimator of IIH0, is admissible. (See Section 5.4
for an extended analysis of the properties of the p-value.)
∥
Example 2.4.27 In the setting of the previous example, if θ is the param-
eter of interest, δ0(x) = x is a generalized Bayes estimator under quadratic
loss, but
r(π, δ0)
=
 +∞
−∞
R(θ, δ0) dθ
=
 +∞
−∞
1 dθ = +∞.

2.5
Usual loss functions
77
Therefore, Proposition 2.4.23 is useless in this case to assess the admis-
sibility of δ0. While δ0 is actually admissible, its admissibility must be
established through a sequence of proper priors, as shown in Chapter 8. ∥
Example 2.4.28
Consider x ∼Np(θ, Ip). If the parameter of interest
is ||θ||2 and the prior distribution is the Lebesgue measure on IRp, since
IEπ[||θ||2|x] = IE[||y||2], with y ∼Np(x, Ip), the Bayes estimator under
quadratic loss is
δπ(x) = ||x||2 + p.
This generalized Bayes estimator is not admissible because it is dominated
by δ0(x) = ||x||2 −p (Exercise 2.35). Since the classical risk is R(θ, δπ) =
var(∥x∥2) + 4p2, the Bayes risk is inﬁnite. This phenomenon shows that
the Lebesgue measure is not necessarily the best noninformative choice
for a prior measure when the parameter of interest is a subvector of the
parameter (see Chapter 3).
∥
2.5 Usual loss functions
When the setting of an experiment is such that the utility function cannot
be determined (lack of time, limited information, etc.), a customary alter-
native is to resort to classical losses, which are mathematically tractable
and well documented. Of course, this approach is an approximation of the
underlying statistical model and should only be adopted when the utility
function is missing. We conclude this section with a note on more intrinsic
loss functions, although these are rarely used in practice. (See also Note
2.8.1 for a description of losses used in imaging.)
2.5.1 The quadratic loss
Proposed by Legendre (1805) and Gauss (1810), this loss is undoubtedly the
most common evaluation criterion. Founding its validity on the ambiguity
of the notion of error in statistical settings (i.e., measurement error versus
random variation), it also gave rise to many criticisms, commonly dealing
with the fact that the squared error loss
L(θ, d) = (θ −d)2
(2.5.1)
penalizes large deviations too heavily.
However, convex loss functions like (2.5.1) have the incomparable ad-
vantage of avoiding the paradox of risk lovers and to exclude randomized
estimators. Another usual justiﬁcation for the quadratic loss is that it pro-
vides a Taylor expansion approximation to more complex symmetric losses
(see Exercise 4.14 for a counterexample). In his 1810 paper, Gauss already
acknowledged the arbitrariness of the quadratic loss and was defending it

78
Decision-Theoretic Foundations
2
on grounds of simplicity. Although the criticisms over a systematic use of
the quadratic loss are quite valid, this loss is nonetheless extensively used
because it gives intuitively sound Bayesian solutions, i.e., those one would
naturally suggest as estimators for a non-decision-theoretic inference based
on the posterior distribution. In fact, the Bayes estimators associated with
the quadratic loss are the posterior means. However, note that the quadratic
loss is not the only loss enjoying this property. Losses leading to posterior
means as the Bayes estimators are called proper losses and characterized in
Lindley (1985), Schervish (1989), van der Meulen (1992), and Hwang and
Pemantle (1994). (See also Exercise 2.15.)
Proposition 2.5.1 The Bayes estimator δπ associated with the prior dis-
tribution π and with the quadratic loss (2.5.1), is the posterior expectation
δπ(x) = IEπ[θ|x] =

Θ θf(x|θ)π(θ) dθ

Θ f(x|θ)π(θ) dθ .
Proof.
Since
IEπ[(θ −δ)2|x] = IEπ[θ2|x] −2δIEπ[θ|x] + δ2,
the posterior loss actually attains its minimum at δπ(x) = IEπ[θ | x].
22
The following corollaries are straightforward to derive.
Corollary 2.5.2
The Bayes estimator δπ associated with π and with the
weighted quadratic loss
L(θ, δ) = ω(θ)(θ −δ)2,
(2.5.2)
where ω(θ) is a nonnegative function, is
δπ(x) = IEπ[ω(θ)θ|x]
IEπ[ω(θ)|x] .
Corollary 2.5.3
When Θ ∈IRp, the Bayes estimator δπ associated with
π and with the quadratic loss,
L(θ, δ) = (θ −δ)tQ(θ −δ),
is the posterior mean, δπ(x) = IEπ[θ|x], for every positive-deﬁnite symmet-
ric p × p matrix Q.
Corollary 2.5.2 exhibits a (weak) duality between loss and prior distri-
bution, in the sense that it is equivalent to estimate θ under (2.5.2) with
the prior π, or under (2.5.1) with the prior πω(θ) ∝π(θ)ω(θ). Moreover,
while admissibility is independent of the weight factor, the Bayes estima-
tor strongly depends on the function ω. For instance, δπ may not exist
if ω increases too fast to +∞. On the other hand, Corollary 2.5.3 shows
that the Bayes estimators are robust with respect to the quadratic form
Q. (Shinozaki (1975) has also proved that admissibility does not depend
on Q.)

2.5
Usual loss functions
79
The quadratic loss is particularly interesting in the setting of bounded
parameter spaces when the choice of a more subjective loss is impossible.
In fact, this loss is quite tractable and the approximation error is usually
negligible. Indeterminacy about the loss function (and thus its replace-
ment by a quadratic approximation) often occurs in accuracy evaluation,
including for instance loss estimation (see Rukhin (1988a,b), Lu and Berger
(1989a,b), Hwang, Casella et al. (1992), Robert and Casella (1993, 1994),
and Fourdrinier and Wells (1994)).
Example 2.5.4 (Example 2.4.9 continued)
We are looking for an
evaluation of the performances of the estimator
δ(x1, x2) =
 x1 + x2
2
if x1 ̸= x2,
x1 + 1
otherwise,
by α(x1, x2) under the quadratic criterion
[IIθ(δ(x1, x2)) −α(x1, x2)]2 ,
where IIθ(v) is 1 if v = θ, 0 otherwise; the function α somehow evaluates
the probability that δ takes the true value θ. (This is a special case of loss
estimation, when the loss function is 1 −IIθ(δ).) Two estimators can be
proposed:
(i)
α0(x1, x2) = 0.75, which is the expectation of IIθ(δ(x1, x2)); and
(ii)
α1(x1, x2) =

1
if x1 ̸= x2,
0.50
if x1 = x2.
The risks of the two evaluators are then
R(θ, α0)
=
IEθ (IIθ(δ(x1, x2)) −0.75)2
=
0.75 −(0.75)2 = 0.1875 ;
and
R(θ, α1)
=
IEθ (IIθ(δ(x1, x2)) −α1(x1, x2))2
=
(0.5)2 1
2 = 0.125 .
Therefore, α1 is a better estimator of the performances of δ than α0. As
mentioned in Berger and Wolpert (1988), this domination result is quite
logical and it suggests that a conditional evaluation of estimators is more
appropriate.
∥
2.5.2 The absolute error loss
An alternative solution to the quadratic loss in dimension one is to use the
absolute error loss,
L(θ, d) =| θ −d |,
(2.5.3)

80
Decision-Theoretic Foundations
2
already considered by Laplace (1773) or, more generally, a multilinear func-
tion
Lk1,k2(θ, d) =

k2(θ −d)
if θ > d ,
k1(d −θ)
otherwise.
(2.5.4)
Such functions increase more slowly than the quadratic loss. Therefore,
while remaining convex, they do not overpenalize large but unlikely errors.
Huber (1964) also proposed a mixture of the absolute error loss and the
quadratic loss, in order to keep a quadratic penalization around 0,
˜L(θ, d) =

(d −θ)2
if | d −θ |< k,
2k | d −θ | −k2
otherwise.
Although a convex4 loss, the mixed loss slows down the progression of the
quadratic loss for large errors and has a robustifying eﬀect. Unfortunately,
there usually is no explicit derivation of Bayes estimators under this loss ˜L.
Proposition 2.5.5
A Bayes estimator associated with the prior distribu-
tion π and the multilinear loss (2.5.4) is a (k2/(k1 + k2)) fractile of π(θ|x).
Proof.
The following classical equality
IEπ[Lk1,k2(θ, d)|x]
=
k1
 d
−∞
(d −θ)π(θ|x) dθ + k2
 +∞
d
(θ −d)π(θ|x) dθ
=
k1
 d
−∞
P π(θ < y|x) dy + k2
 +∞
d
P π(θ > y|x) dy,
is obtained by an integration by parts. Taking the derivative in d, we get
k1P π(θ < d|x) −k2P π(θ > d|x) = 0,
i.e.,
P π(θ < d|x) =
k2
k1 + k2
.
22
In particular, if k1 = k2, i.e., in the case of the absolute error loss, the
Bayes estimator is the posterior median, which is the estimator obtained
by Laplace (see Example 1.2.4). Notice that, when π has a nonconnected
support, Proposition 2.5.5 provides examples of multiple Bayes estimators
for some values of x (see Exercise 2.40).
2.5.3 The 0 −1 loss
This loss is mainly used in the classical approach to hypothesis testing, as
formalized by Neyman and Pearson (see Section 5.3). More generally, this
4 Again, if we insist so much on convexity, it is because it ensures that randomized
estimators are suboptimal from a frequentist point of view. Therefore, a statistical
decision-theoretic approach that would agree as much as possible with the Likelihood
Principle necessarily calls for convex losses. This requirement obviously eliminates
bounded losses.

2.5
Usual loss functions
81
is a typical example of a nonquantitative loss. In fact, for this loss, the
penalty associated with an estimate δ is 0 if the answer is correct and 1
otherwise.
Example 2.5.6
Consider the test of H0 : θ ∈Θ0 versus H1 : θ ̸∈Θ0.
Then D = {0, 1}, where 1 stands for acceptance of H0 and 0 for rejection
(in other words, the function of θ to be estimated is IIΘ0(θ)). For the 0 −1
loss, i.e.,
L(θ, d) =

1 −d
if θ ∈Θ0
d
otherwise,
(2.5.5)
the associated risk is
R(θ, δ)
=
IEθ[L(θ, δ(x))]
=

Pθ(δ(x) = 0)
if θ ∈Θ0 ,
Pθ(δ(x) = 1)
otherwise,
which are exactly the type–one and type–two errors underlying the Neyman–
Pearson theory.
∥
This loss is not very interesting because of its nonquantitative aspect,
and we will consider in Chapter 5 some alternative theories for testing hy-
potheses. The associated Bayes estimators also reﬂect the primitive aspect
of such a loss (see also Exercise 2.41).
Proposition 2.5.7
The Bayes estimator associated with π and with the
loss (2.5.5) is
δπ(x) =

1
if P(θ ∈Θ0|x) > P(θ ̸∈Θ0|x),
0
otherwise,
i.e., δπ(x) is equal to 1 if and only if P(θ ∈Θ0|x) > 1/2.
2.5.4 Intrinsic losses
It may occur that some settings are so noninformative that not only the
loss function is unknown, but there is not even a natural parameterization.
Such cases happen when the distribution f(x|θ) itself is of interest, for
instance, in prediction settings.
However, as we mentioned in the previous section, the choice of the pa-
rameterization is important because, contrary to the maximum likelihood
estimation approach, if g is a one-to-one transformation of θ, the Bayes
estimator of g(θ) is usually diﬀerent from the transformation by g of the
Bayes estimator of θ under the same loss (see Exercise 2.36). This lack of in-
variance, although often troubling to beginners, is not usually a concern for
decision-makers because it shows how the Bayesian paradigm can adapt to
the estimation problem at hand and the selected loss function, while max-
imum likelihood estimation is totally loss-blind. But the few cases where

82
Decision-Theoretic Foundations
2
loss function and natural parameterization are completely unavailable may
call for this kind of ultimate invariance. (See Wallace and Boulton (1975)
for another approach.)
In such noninformative settings, it seems natural to use losses that com-
pare directly the distributions f(·|θ) and f(·|δ) associated with the true
parameter θ and the estimate δ. Such loss functions,
L(θ, δ) = d(f(·|θ), f(·|δ)),
are indeed parameterization-free. Two usual distribution distances are
(1) the entropy distance
Le(θ, δ) = IEθ

log
f(x|θ)
f(x|δ)

,
(2.5.6)
which is also called the Kullback–Leibler divergence and which is not
a distance in the mathematical sense because of its asymmetry; and
(2) the Hellinger distance
LH(θ, δ) = 1
2IEθ
⎡
⎣
!(
f(x|δ)
f(x|θ) −1
"2⎤
⎦.
(2.5.7)
Example 2.5.8
Consider x ∼N(θ, 1). Then we have
Le(θ, δ)
=
1
2IEθ[−(x −θ)2 + (x −δ)2] = 1
2(δ −θ)2,
LH(θ, δ)
=
1 −exp{−(δ −θ)2/8}.
Considering the normal case when π(θ|x) is a N(μ(x), σ2) distribution, it
is straightforward to show that the Bayes estimator is δπ(x) = μ(x) in both
cases.
∥
The Hellinger loss is undoubtedly more intrinsic than the entropy loss,
if only because it always exists (note that (2.5.7) is bounded above by
1). Unfortunately, while leading to explicit expressions of LH(θ, δ) for the
usual distribution families, it does not allow for an explicit derivation of the
Bayes estimators, except in the special case treated above. On the contrary,
in exponential families, the entropy loss provides explicit estimators which
are the posterior expectations for the estimation of the natural parameter
(see Chapter 3). Moreover, although quite diﬀerent from the Hellinger loss,
the entropy loss provides similar answers for the usual distribution families
(see Robert (1996b)). There are also various theoretical reasons to defend
the use of the Kullback-Leibler distance, ranging from information theory
(Exercise 2.48) to the relevance of logarithmic scoring rule and the location-
scale invariance of the distance, as detailed in Bernardo and Smith (1994).

2.6
Criticisms and alternatives
83
2.6 Criticisms and alternatives
Some criticisms about the frequentist notions of minimaxity and admis-
sibility have been mentioned in the previous sections. These concepts are
actually of secondary interest from a purely Bayesian point of view, since,
on one hand, admissibility is automatically satisﬁed by most Bayes esti-
mators. On the other hand, minimaxity is somehow incompatible with the
Bayesian paradigm, since, under a prior distribution, each value of the pa-
rameter cannot be equally weighted. However, minimaxity may be relevant
from a robustness point of view, that is, when the prior information is not
precise enough to determine the prior distribution.
It may happen that the decision-maker cannot deﬁne a loss function
exactly. For instance, when the decision-maker is a committee comprising
several experts, it is often the case that they diﬀer about the relevant
loss function (and sometimes even about the prior distribution). Starting
with Arrow (1951), the literature on these extensions of classical Decision
Theory is quite extensive (see Genest and Zidek (1986), Rubin (1987), and
Van Eeden and Zidek (1993) for details and references).
When the loss function has not been completely determined, it might be
assumed to belong to a parametrized class of loss functions, the decision
maker selecting the most accurate parameter. Apart from Lp losses, two
other possible classes are
L1(θ, δ) = log(α||θ −δ||2 + 1),
L2(θ, δ) = 1 −exp{−c||θ −δ||2}.
An alternative approach more in tune with the Bayesian paradigm is to
consider that, since the loss is partly unknown, this uncertainty can be
represented by using a random loss L(θ, δ). The evaluation of estimators is
then done by integrating out with respect to this additional variable: If F
is the distribution of the loss, the objective function to minimize (in δ) is

Θ

Ω
L(θ, δ, ω)dF(ω) dπ(θ|x),
(2.6.1)
where F possibly depends on θ or even on x. This case is actually the only
interesting extension because, otherwise, to minimize (2.6.1) is equivalent
to using the average loss
¯L(θ, δ) =

Ω
L(θ, δ, ω) dF(ω).
Another approach to the lack of precision on the loss function consists
of considering simultaneously a set of losses and look for estimators per-
forming well for all these losses. Obviously, this multidimensional criterion
only induces a partial ordering on estimators.
Example 2.6.1
Consider x ∼Np(θ, Ip). The parameter θ is estimated
under quadratic loss. If the loss matrix Q is not exactly determined, a
robust alternative is to include the losses associated with the matrices Q
such that Q1 ⪯Q ⪯Q1 (where A ⪯B means that the matrix B −A is

84
Decision-Theoretic Foundations
2
nonnegative deﬁnite). Notice that, according to Corollary 2.5.3, the Bayes
estimator is the same for all Q’s.
∥
Example 2.6.2 In the setting of the above example, Brown (1975) shows
that a shrinkage estimator of the form (1 −h(x))x dominates δ0(x) = x for
a class of quadratic losses, i.e., a class of matrices Q if and only if
tr(Q) −2λmax(Q) > 0
(2.6.2)
for every matrix in the class (where λmax denotes the largest eigenvalue).
Notice that this condition excludes the case p ≤2, where δ0 is actually
admissible. The constant tr(Q)−2λmax(Q) also appears in the majorization
constant of ||x||2h(||x||2) (see Theorem 2.8.1). Therefore, (2.6.2) is both a
necessary and suﬃcient condition for the Stein eﬀect to occur.
∥
The ultimate criterion in loss robustness is called universal domination
and was introduced in Hwang (1985). It actually takes into account the set
of all losses ℓ(||δ −θ||Q), for a given norm ||x||Q = xtQx and all nonde-
creasing functions ℓ. An estimator δ1 will be said to universally dominate
another estimator δ2 if, for every ℓ,
IEθ[ℓ(||δ1(x) −θ||Q)] ≤IEθ[ℓ(||δ2(x) −θ||Q)].
A second criterion is called stochastic domination: δ1 stochastically domi-
nates δ2 if, for every c > 0,
Pθ(||δ1(x) −θ||Q ≤c) ≥Pθ(||δ2(x) −θ||Q ≤c).
Although this criterion seems more intrinsic and less related to Decision
Theory than universal domination, Hwang (1985) has shown that the two
criteria are actually equivalent.
Theorem 2.6.3
An estimator δ1 universally dominates an estimator δ2
if and only if δ1 stochastically dominates δ2 .
Proof.
The estimator δ1 stochastically dominates δ2 if, for every c > 0,
Pθ(||δ1(x) −θ||Q ≤c) ≥Pθ(||δ2(x) −θ||Q ≤c).
This can be rewritten as
IEθ
+
II[c,+∞[(||δ1(x) −θ||Q)
,
≤IEθ
+
II[c,+∞[(||δ2(x) −θ||Q)
,
.
Since ℓ(t) = II[c,+∞[(t) is a nondecreasing function of t, universal domina-
tion implies stochastic domination. The converse follows from the fact that
the ﬁrst moments of two stochastically ordered random variables are also
ordered.
22
Moreover, these two criteria are not empty since Hwang (1985) has es-
tablished the following domination result: If x ∼Tα(μ, σ2), Student’s t-
distribution with α degrees of freedom, some shrinkage estimators uni-
versally dominate δ0(x) = x. If the dimension is not too small (usually,

2.7
Exercises
85
p = 4 is suﬃcient), Brown and Hwang (1989) virtually showed that, if
x ∼Np(θ, Σ), the estimator δ0(x) is admissible for universal domination if
and only if Q = Σ. For other choices of the matrix Q and p large enough,
δ0 is stochastically dominated. Therefore, even though this criterion is less
discriminating than usual losses, it allows for comparison, and even for a
Stein eﬀect, since classical estimators are not necessarily optimal.
The study of multiple losses is not very developed from a Bayesian point
of view, since Bayes estimators usually vary with a change in the loss func-
tion. However, in a very special case, Rukhin (1978) has shown that the
Bayes estimators were independent of the loss function. Under some regu-
larity assumptions, this case corresponds to the equation
log f(x|θ) + log π(θ) = A1(x)eαθ + A2(x)e−αθ + A3(x),
where π is the prior distribution. Therefore, for this exponential family (see
Section 3.3.3),
f(x|θ) = B(x)
π(θ) exp{A1(x)eαθ + A2(x)e−αθ},
(2.6.3)
the Bayes estimators are universal, because they do not depend on the loss.
The next chapter covers in detail the case of exponential families, which
are classes of distributions on IRk with densities
f(x|θ) = c(θ)h(x) exp[R(θ) · T (x)],
where R(θ), T (x) ∈IRp. However, notice that (2.6.3) is a rather special
exponential family.
2.7 Exercises
Section 2.2
2.1 Show that, if the utility function U is convex, every P ∈PE satisﬁes
IEP [r] =

R
r dP(r) ⪯P.
Conclude that a concave loss is not realistic.
2.2 Consider four dice with respective numbers on their faces (4, 4, 4, 4, 0, 0),
(3, 3, 3, 3, 3, 3), (6, 6, 2, 2, 2, 2), (1, 1, 1, 5, 5, 5). Two players roll one die each
and compare their outcome. Show that the relation die [i] beats die [j] is
intransitive, i.e., that for every choice of the ﬁrst player the second player can
choose a die so that the probability of winning is greater than 0.5. Relate this
example to the Pitman closeness setting of Note 2.8.3.
2.3 Show that PB ⊂PE, i.e., that bounded reward distributions have a ﬁnite
expected utility.
2.4 Show Lemmas 2.2.2 and 2.2.3.
2.5 *(DeGroot (1970))
In order to show the extension of Theorem 2.2.4 from
PB to PE, consider a sequence sm decreasing (for ⪯) in R such that, for every

86
Decision-Theoretic Foundations
2
r ∈R, there exists m with sm ⪯r. If P ∈PE and if P({sm ⪯r}) > 0, denote
by Pm the conditional distribution
Pm(A) = P(A ∩{sm ⪯r})
P({sm ⪯r})
.
Similarly, if tn is an increasing sequence in R such that, for every r ∈R, there
exists n with r ⪯tn, we deﬁne P n as
P n(A) = P(A ∩{r ⪯tn})
P({r ⪯tn})
,
when P({r ⪯tn}) > 0. We assume that such sequences can be exhibited in R.
a. Show that P n and Pm are included in PB.
We introduce the additional hypothesis:
(A6) For every P, Q ∈PE, such that there exists r0 ∈R satisfying P({r ⪯
r0}) = Q({r0 ⪯r}) = 1, the ordering P ⪯Q is necessarily satisﬁed.
b. Show that (A6) is actually satisﬁed in PB.
c. Show that, for every P ∈PE,
IEP [U(r)] =
lim
m→+∞IEPm[U(r)] =
lim
n→+∞IEP n[U(r)].
d. Consider P ∈PE and m < m1, n < n1 such that P({sm ⪯r}) > 0 and
P({r ⪯tn}) > 0. Show that
P n ⪯P n1 ⪯P ⪯Pm1 ⪯Pm.
The second additional hypothesis:
(A7) Consider P and Q in PE. If there exists m0 such that Pm ⪰Q when
m ≥m0, then P ⪰Q. Moreover, if there exists n0 such that P n ⪯Q
when n ≥n0, then P ⪯Q,
is assumed to hold below.
e. Consider P and Q in PE with r1, r2 in R such that
P({r1 ⪯r}) = Q({r2 ⪯r}) = 1.
Show that P ⪯Q if and only if IEP [U(r)] ≤IEQ[U(r)]. (Hint: Consider the
sequences P n, Pm, and am = IEPm[U(r)], bn = IEP n[U(r)]. Use hypothesis
(A4) and questions c. and d.)
f. Deduce from the above question that, if P, Q ∈PE, P ⪯Q if and only if
IEP [U(r)] ≤IEQ[U(r)].
2.6 In the setup of Example 2.2.6 on the Saint Petersburg paradox, give the
average utility of a player for Saint Petersburg paradox, give the average utility
of a player for δ = 1 and δ = 10. Compute the average number of games a
player is ready to play in the modiﬁed game.
2.7 *(Smith (1988)) An expert has a preference ordering such that the rewards
αII(x+h) + (1 −α)II(x−h) and x are equivalent, with α independent of x. Show
that the utility function of this expert is either linear (when α = 1/2), of the
form ecx (c > 0) (α < 1/2), or of the form 1 −e−cx (α > 1/2).

2.7
Exercises
87
2.8 (Raiﬀa (1968)) In a ﬁrst setup, a person has to choose between a sure gain
of $10,000 (a1) and a gain of $50,000 with probability 0.89 (a2). The second
setup is such that a gain of $50,000 with probability 0.1 (a3) is opposed to a
gain of $10,000 with probability 0.11 (a4). Show that, even if it seems natural
to prefer a1 to a2 and a3 to a4, there is no utility function preserving the order
a1 ⪯a2 and a3 ⪯a4.
2.9 In the setup of the Saint Petersburg paradox deﬁned in Example 2.2.6, con-
sider the following three classes of utility functions:
(i) U(r) = log(δ + r);
(ii) U(r) = (δ + r)ϱ (0 < ϱ < 1); and
(iii) U(r) = 1 −eδ+r.
For each class, determine the maximum entrance fee and the optimal number
of games.
Section 2.3
2.10 (Casella (1990)) Show that, if the function r, from IR+ in IR+, is concave,
then r(t) is nondecreasing and r(t)/t is nonincreasing.
2.11 Considering the loss proposed in Example 2.3.4, show that a perfect expert
for N = 2 dominates a perfect expert for N = 1. Does the same phenomenon
occur for N = 3?
2.12 (Smith (1988))
Using the notations of Example 2.3.4, the Brier score is
deﬁned as the loss function
L(θ, p) =
N

i=1
qi(pi −θi)2 + ¯q(1 −¯q) −
N

i=1
qi(pi −¯q)2,
with ¯q = N
i=1 qiθi, the proportion of rainy days. Show that a perfect expert
P1 is better than a perfect expert P2 if its (so-called) resolution
R =
N

i=1
qi(θi −¯q)2
is larger. Comment on the form of the loss.
2.13 Show that, for a loss function L(θ, d) strictly increasing in |d −θ| such
that L(θ, θ) = 0, there is no uniformly optimal statistical procedure. Give a
counterexample when
L(θ, ϕ) = θ(IIIR∗(θ) −ϕ)2.
2.14 In relation to Example 2.3.4, the scoring rule of a weather forecaster is the
sum, over the year, of the errors (IIAij −pi)2 for all the days for which the
probability pi was announced and for which Aij is the event that it actually
rained. If ni is the number of days that pi was forecasted, show that the scoring
rule can be decomposed as
N

i=1
ni

j=1
(IIAij −θi)2 +
N

i=1
ni(θi −pi)2.

88
Decision-Theoretic Foundations
2
2.15 *(Schervish (1989)) Consider an inferential problem where the probability
p of an event E is to be forecasted. The answer δ ∈[0, 1] of a forecaster is
evaluated through a scoring rule L(E, δ), which takes the value gi(δ) ≥0 if
IIE = i (i = 0, 1). The scoring rule is said to be proper if the average error
m(δ) = pg1(δ) + (1 −p)g0(δ)
is minimized for δ = p.
a. Show that, for a proper scoring rule, g0 is nondecreasing and g1 is nonin-
creasing.
b. Show that, if the gi are diﬀerentiable, the scoring rule is proper if and only
if
−pg′
1(p) = (1 −p)g′
0(1 −p)
for every p in [0, 1].
c. Deduce that, when the scoring rule is proper, there exists a nonnegative
function h, integrable on [0, 1] such that
g0(r) =

[0,r]
h(t) dt
and
g1(r) =

[1−r,1]
t
1 −th(t) dt.
2.16 Show through discrete and continuous examples that a Bayes estimator can
correspond to several prior distributions for the same loss function.
2.17 Two experts must provide an estimate of p ∈[0, 1] under the loss (δ −p)2.
They have the respective prior distributions π1 and π2, equal to Be(1, 2) and
Be(2, 3).
a. Give both estimates δ1 and δ2 when the experts answer separately (with
no observation).
b. Expert 1 knows δ2. We assume that the quantity p is observed afterward
and that the best expert is ﬁned (δi −p)2 while the worst expert is ﬁned a
ﬁxed amount A. Show that the loss function for expert 1 is
(δ1 −p)2II|δ1 −p| ≤|δ2 −p| + AII|δ1 −p| > |δ2 −p|.
Deduce that, if A is large enough, the optimal answer for expert 1 is δ1 = δ2.
c. Modify the above loss function in order to force expert 1 to give an honest
answer, i.e., the original δ1.
2.18 (Raiﬀa and Schlaifer (1961)) Given a loss function L(θ, d), deﬁne the op-
timal decision as the decision dθ that minimizes L(θ, d) for a given θ. The
opportunity loss is then deﬁned as L∗(θ, d) = L(θ, d) −L(θ, dθ).
a. Show that this is equivalent to assume that infθ L(θ, d) = 0 for every θ.
b. Show that the set of classical (frequentist) optimal procedures (admissible,
minimax) is the same for L and L∗.
c. Show that the Bayes procedures are the same for L and L∗.
2.19 (Raiﬀa and Schlaifer (1961))
Given a loss function L(θ, d) and a prior
distribution π, the optimal prior decision is dπ which minimizes IEπ[L(θ, d)].
a. Consider D = {d1, d2} and L(θ, d1) = 0.5 + θ, L(θ, d2) = 2 −θ. Give the
optimal prior decisions when π is Be(1, 1) and Be(2, 2).

2.7
Exercises
89
b. The value of sample information x is deﬁned as
ν(x) = IEπ[L(θ, dπ)|x] −IEπ[L(θ, δπ(x))|x],
where δπ(x) is the regular Bayesian estimator of θ. Indicate why ν(x) ≥0
and give the value of sample information when x ∼B(n, θ) for the above
loss function and priors.
c. When Θ = D = IR, x ∼N(θ, 1), and θ ∼N(θ0, 102), show that the optimal
prior decision under squared error loss is dπ = θ0 and that the value of
sample information is (θ0 −x)2. Conclude by discussing the coherence of
this notion.
2.20 An investment strategy can be implemented according to two diﬀerent
strategies, d1 and d2. The beneﬁt (or utility) of the investment depends on a
rentability parameter θ ∈IR and is U(θ, di) = ki + Kiθ.
a. Given a prior distribution π on θ, what is the optimal prior decision?
b. Let x ∼N(θ, 1) and let θ ∼N(0, 10). Give the optimal prior and posterior
strategies. Give the improvement brought about by the observation of x in
terms of posterior utility and expected utility.
c. If there is a cost cs for the observation of x, determine the maximum cost
cs when the advantage of observing x disappears.
2.21 (Raiﬀa and Schlaifer (1961))
In a setting similar to the above exercise,
the decision space is D = {d1, d2} and the parameter θ ∈[0, 1]. The utility
function is L(θ, di) = ki + Kiθ.
a. Deﬁning ϕ = (k1 −k2)/(K1 −K2), show that ϕ ̸∈(0, 1) implies that one of
the two decisions is always optimal. In the following questions, we assume
ϕ ∈(0, 1).
b. Let x|θ ∼B(n, θ) and let θ ∼Be(r, n′ −r). Compute the optimal prior
and posterior decisions and the expected improvement (in utility) gained
by using the observation x.
c. Given an observation cost of K for each Bernoulli random variable, deter-
mine the optimal sample size n for the expected utility.
Section 2.4.1
2.22 Prove Theorem 2.4.2 when r(π) is ﬁnite.
2.23 Compare δ0 and δ∗of Example 2.3.1 under 0 −1 loss. Does this result
contradict the Rao–Blackwell Theorem (Theorem 2.4.8)?
Section 2.4.2
2.24 Produce an example similar to Example 2.4.7, but where A would be forced
to confess from a Bayesian point of view.
2.25 Consider the case when Θ = {θ1, θ2} and D = {d1, d2, d3}, for the following
loss structure
d1
d2
d3
θ1
2
0
0.5
θ2
0
2
1

90
Decision-Theoretic Foundations
2
a. Determine the minimax procedures.
b. Identify the least favorable prior distribution. (Hint: Represent the risk
space associated with the three actions as in Example 2.4.12.)
2.26 Consider the following risk function for Θ = {θ1, θ2} and D = {d1, d2, d3}
d1
d2
d3
θ1
1
2
1.75
θ2
2
1
1.75
a. Draw the risk diagram as in Example 2.4.12 and deduce the minimax esti-
mators.
b. Deduce from this example that minimaxity is not coherent in the follow-
ing sense: d1, d2, d3 may be such that maxθ R(θ, d1) ≥maxθ R(θ, d3) and
maxθ R(θ, d2) ≥maxθ R(θ, d3), while the minimax estimator is of the form
αd1 + (1 −α)d2.
Section 2.4.3
2.27 Prove Lemma 2.4.10.
2.28 Consider x ∼B(n, θ), with n known.
a. If π(θ) is the beta distribution Be(√n/2, √n/2), give the associated poste-
rior distribution π(θ|x) and the posterior expectation, δπ(x).
b. Show that, when L(δ, θ) = (θ −δ)2, the risk of δπ is constant. Conclude
that δπ is minimax.
c. Compare the risk for δπ with the risk function of δ0(x) = x/n for n = 10, 50,
and 100. Conclude about the appeal of δπ.
2.29 Prove Lemmas 2.4.13 and 2.4.15.
2.30 Consider x ∼N(θ, 1) and θ ∼N(0, n). Show that the Bayes risk is equal
to n/(n + 1). Conclude about the minimaxity of δ0(x) = x.
2.31 *Give the density of the uniform distribution on the sphere of radius c
and derive the marginal distribution of x ∼Np(θ, Ip), when θ is uniformly
distributed on this sphere. Compute the posterior expectation δπ and study
its properties.
2.32 Show the equivalent of Example 2.4.16 when x ∼P(λ), i.e., that δ0(x) = x
is minimax. (Hint: Notice that δ0 is a generalized Bayes estimator for π(λ) =
1/λ and use a sequence of G(α, β) priors.)
2.33 Establish Propositions 2.4.20, 2.4.23, and 2.4.25.
Section 2.4.4
2.34
In the setting of Example 2.4.26, we want to show that the Bayes risk of
p(x) is ﬁnite.
a. Show that
τ(π) =

IR2

Φ2(x) −2Φ(x)IIθ≤0 + IIθ≤0
 e−(x−θ)2/2
√
2π
dθdx
when π(θ) = 1.

2.7
Exercises
91
b. Deduce that
τ(π)
=
 +∞
−∞
Φ(x)Φ(−x)dx
=
2
 +∞
0
Φ(x)Φ(−x)dx
by integrating ﬁrst in θ.
c. Show that
 +∞
0
Φ(−x)dx =
 +∞
0
y e−y2
√
2π
dy.
d. Conclude about the ﬁniteness of τ(π).
2.35 Consider x ∼Np(θ, Ip). A class of estimators of ||θ||2 is given by
δc(x) = ||x||2 + c,
c ∈IR.
a. Show that, under quadratic loss, δ−p minimizes the risk for every θ among
the estimators δc. Does this estimation problem have a value?
b. How can we choose ω(θ) so that the risk of δ−p is uniformly bounded for
the quadratic loss weighted by ω(θ)? Conclude about the minimaxity of
δ−p.
c. Show that δ−p is not admissible, and propose an estimator that dominates
δ−p uniformly.
2.36 Show that, under squared error loss, if two real estimators δ1 and δ2 are
distinct and satisfy
R(θ, δ1) = (θ −δ1(x))2 = R(θ, δ2) = (θ −δ2(x))2,
the estimator δ1 is not admissible. (Hint: Consider δ3 = (δ1 + δ2)/2 or δ4 =
δα
1 δ1−α
2
.) Extend this result to all strictly convex losses and construct a counter-
example when the loss function is not convex.
2.37 Let Θ = {θ1, θ2} and consider
the case when the risk set is R = {(r1, r2); (r1 −2)2 + (r2 −2)2 < 2, r1 ≤
2, r2 ≤2}.
a. Draw R and deduce whether there exists a minimax point.
b. Exhibit the two admissible rules for this problem.
c. What can be said about the existence of Bayes procedures?
2.38 Two experts have diﬀerent loss functions described in the following table
for D = {d1, d2, d3} and Θ = {θ1, θ2}
L1/L2
d1
d2
d3
θ1
1/1
2.5/1.5
2/2.5
θ2
1.5/4
2/3.5
3/3
a. Plot the risk sets for both experts and identify minimax and admissible
procedures in each case.
b. There are several ways to combine the expert opinions, i.e., to derive a
single loss function. For each of the following choices, derive the risk set
and the optimal procedures:
(i) L = (L1 + L2)/2
(ii) L = sup(L1, L2)
(iii) L =
√
L1L2.

92
Decision-Theoretic Foundations
2
c. For what choice of L above are the admissible rules admissible for one of
the two original losses? When is the risk set convex?
Section 2.5
2.39 *Establish Propositions 2.5.1, 2.5.2, and 2.5.3. Show Shinozaki’s lemma
(1975): if δ is admissible for the usual quadratic loss, it is admissible for every
quadratic loss.
2.40 Consider π(θ) = (1/3)(U[0,1](θ) + U[2,3](θ) + U[4,5](θ)) and f(x|θ) = θe−θx.
Show that, under the loss (2.5.4), for every x, there exist values of k1 and k2
such that the Bayes estimator is not unique.
2.41 Establish Proposition 2.5.7 and show that the loss L considered in Example
2.5.8 is equivalent to estimate IIH0(θ) under the absolute error loss,
L(θ, δ) = |θ −δ|.
Derive the Bayes estimator associated with the quadratic loss.
2.42 *(Zellner (1986a)) Consider the LINEX loss in IR, deﬁned by
L(θ, d) = ec(θ−d) −c(θ −d) −1.
a. Show that L(θ, d) > 0 and plot this loss as a function of (θ −d) when
c = 0.1, 0.5, 1, 2.
b. Give the expression of a Bayes estimator under this loss.
c. If x1, . . . , xn ∼N(θ, 1) and π(θ) = 1, give the associated Bayes estimator.
2.43 (Berger (1985a)) Consider x ∼N(θ, 1), θ ∼N(0, 1) and the loss
L(θ, δ) = e3θ2/2(θ −δ)2.
a. Show that δπ(x) = 2x.
b. Show that δπ is uniformly dominated by δ0(x) = x and that r(π) = +∞.
2.44 Determine the Bayes estimator associated with the absolute error loss in
IRk,
L(θ, δ) = ||θ −δ||.
2.45 Consider the following questions for the entropic and the Hellinger intrinsic
losses.
a. Show that Le (resp. LH) is nonnegative, is equal to 0 when d = θ, and
determine under which condition d = θ is the unique solution of Le(θ, d) = 0
(resp. of LH(θ, d) = 0).
b. Give the expressions of both losses when x ∼N(0, θ) and x ∼Be(n, θ).
c. Show that, if x ∼G(α, θ) and θ ∼G(ν, x0), the Bayes estimator of θ under
the Hellinger loss is of the form k/(x0 + x).
2.46 *(Wells (1992)) As mentioned in Section 2.5.4, the Bayes estimators are not
invariant under arbitrary reparameterization. In the normal case, x ∼N(θ, 1),
examine whether the only transformations of θ for which the Bayes estimators
are invariant under squared error loss are the aﬃne transformations, η = aθ+b.
[Note: The answer is no.]

2.7
Exercises
93
2.47 *(Efron (1992)) Derive the Bayes estimators of θ when θ|x ∼N(μ(x), 1)
and when the loss function is the asymmetric squared error loss,
L(θ, δ) =

ω(θ −δ)2
if δ < θ,
(1 −ω)(θ −δ)2
otherwise.
2.48 (Robert (1996b)) Show that both the entropic and the Hellinger losses are
locally equivalent to the quadratic loss associated with the Fisher information,
I(θ) = IEθ

∂log f(x|θ)
∂log

∂log f(x|θ)
∂log
t
,
that is,
Le(θ, δ) = Le(θ −δ)tI(θ)−1(θ −δ) + O(∥θ −δ∥2)
and
LH(θ, δ) = cH(θ −δ)tI(θ)−1(θ −δ) + O(∥θ −δ∥2),
where ce and cH are constants.
2.49 Consider y = x+ϵ with ϵ and x independent random variables and IE[ϵ] = 0.
a. Show that IE[y|x] = x.
b. Show that the reverse does not necessarily hold, i.e., that IE[x|y] is not al-
ways equal to y. (Hint: Consider, for instance, the case when x ∼pN(θ1, 1)+
(1 −p)N(θ2, 1) and ϵ ∼N(0, 1).)
Section 2.6
2.50 Show that, for the universal distributions of Rukhin (1978), the Bayes es-
timators are indeed independent of the loss function. In the particular case
when x ∼G(ν, 1/ν) identify θ, A1(x), A2(x), and the universal prior π(θ).
Note 2.8.1
2.51 Show that that the Bayes estimator associated with the L0 loss function is
the MAP estimator.
2.52 Show that that the Bayes estimator associated with the L1 loss function is
the componentwise MAP estimator.
2.53 If D is a subset of {1, . . . , N}, e denotes a misclassiﬁcation vector and mD
the corresponding number of misclassiﬁcations.
a. Show that p(mD) can also be written as
p(mD) = 1 −

i∈D
(1 −ei) .
b. Let q(mD) to be 1 if and only if mD = |D|. Show that
q(mD) =

i∈D
ei .
c. Show that
p(mD) =
|D|

k=1
(−1)k+1

ω∈Pk(D)
q(mω) .

94
Decision-Theoretic Foundations
2
Note 2.8.2
2.54 Show that the Stein paradox cannot occur when δ0 is a proper Bayes es-
timator, whatever the dimension p is. [Note: Brown (1971) shows that there
are also generalized Bayes estimators that enjoy this property.]
2.55 Show that the majorizing constant in Theorem 2.8.1 can be replaced by
c = 2
q −2α
p −q + 4β .
(Hint: Bound h2(t, u) by c(u/t)h(t, u) ﬁrst.) Compare the two bounds.
2.56 *(Stein (1973)) Establish Stein’s lemma: If x ∼N(θ, 1) and f is continuous
and a.e. diﬀerentiable, then
IEθ[(x −θ)f(x)] = IEθ[f ′(x)].
Deduce that, if x ∼Np(θ, Σ), δ(x) = x+Σγ(x), and L(θ, δ) = (δ−θ)tQ(δ−θ),
with γ diﬀerentiable, then
R(θ, δ) = IEθ
+
tr(QΣ) + 2 tr(Jγ(x)Q∗) + γ(x)tQ∗γ(x),
,
where tr(A) is the trace of A, Q∗= ΣQΣ and Jγ(x) is the matrix with generic
element
∂
∂xi γj(x). [Note: This representation of the risk leads to the technique
of unbiased estimation of the risk, which is quite inﬂuential in the derivation
of suﬃcient conditions of domination of usual estimators. See Berger (1985a)
and Johnstone (1988).]
Note 2.8.3
The following exercises (2.57–2.62) consider the Pitman closeness criterion.
An estimator δ1 of θ is said to Pitman-dominate an estimator δ2, denoted
δ1
P≻δ2, if, for every θ ∈Θ,
Pθ(|δ1(X) −θ| < |δ2(X) −θ|) > 0.5.
The notion of Pitman-admissibility follows directly.
2.57 *Consider a median unbiased estimator δM, i.e., δM such that
∀θ,
Pθ(δM(x) ≤θ) = 0.5.
a. Show that δM is the best estimator (for the Pitman criterion) among the
linear estimators δM(x) + K, K ∈IR.
b. If θ > 0 and δM > 0, show that δM is also the best estimator (for the
Pitman criterion) among the estimators KδM, K > 0.
2.58 *Consider X = θU, θ > 0, U ∼U(−0.9, 1.1). Show that
X
P≻0.9|X|
P≻3.2|X|
P≻X .
2.59 *(Robert et al. (1993)) Consider X ∼f(x −θ), with
 0
−∞
f(u) du = 1/2
and f(0) > 0. If F is the c.d.f. of X for θ = 0, the function ϵ(θ) is deﬁned by
F(−θ) =

P0(0 < X < ϵ(θ))
if θ > 0,
1 −P0(0 > X > −ϵ(θ))
if θ < 0,

2.7
Exercises
95
and ϵ(0) = 0. Consider
θ1 = Arg{min
θ>0 |θ + ϵ(θ)|},
θ2 = Arg{min
θ<0 |θ −ϵ(θ)|}.
The truncated version of ϵ is deﬁned by
ϵ∗(θ) =
 ϵ(θ)
if θ > θ1 or θ < θ2
θ1 + ϵ(θ1) −θ
if 0 < θ < θ1
θ + ϵ(θ2) −θ2
if 0 > θ > θ2.
The set A satisﬁes
(x, θ) ∈A
if and only if
θ < x ≤θ + ϵ∗(θ)
for θ > 0, and
(x, θ) ∈A
if and only if
θ −ϵ∗(θ) ≤x < θ.
for θ < 0.
a. Justify the truncation of ϵ in terms of A and represent A in a special case
when the derivation of ϵ∗is manageable.
b. Show that, if δ(x) is a nondecreasing function such that (x, δ(x)) ∈A, then
δ
P≻δ0(x) = x.
c. Show that, if F(c) −F(−c) = 1/2, every estimator δ such that
δ(x) = 0
when
|x| < c
(2.7.1)
is Pitman-admissible.
d. When δ is monotone, satisﬁes (2.7.1), and belongs to A, show that δ is
Pitman-admissible and Pitman-dominates δ0. Show that
c < θ1 + ϵ(θ1)
and
−c > θ2 −ϵ(θ2)
and conclude about the existence of such estimators.
2.60 Consider a couple of random variables (x, y) with joint c.d.f.
Fα(x, y) =
xy
1 + α(1 −x)(1 −y)II[0,1]2(x, y).
a. Show that Fα is indeed a c.d.f. and deduce the density fα(x, y).
b. Give the marginal distribution of x and y.
c. Suppose two estimators δ1 and δ2 are distributed according to θ−2fα(δ1/θ,
δ2/θ). What can be said about the Pitman closeness to θ? (Hint: Compute
P(|δ1 −θ| < |δ2 −θ|).)
2.61 *Show that, if X1, X2 ∼f(x|θ), X
P≻X1. Apply this result to the case of
the Cauchy distribution. Show that, for every real η, X is Pitman-closer to η
than X1, even if η is arbitrary. [Note: This property is not speciﬁc to Pitman
closeness, since it is also satisﬁed by the quadratic loss.]
2.62 *(Robert et al. (1993))
Show (or use the result) that, if χ2
α(p, λ) is the
α-quantile of a noncentral chi-squared distribution, χ2
p(λ), it satisﬁes
p −1 + λ ≤χ2
0.5(p, λ) ≤χ2
0.5(p, 0) + λ.

96
Decision-Theoretic Foundations
2
a. Deduce from this inequality that the James–Stein estimators
δh(x) =

1 −h(x)
||x||2

x
Pitman-dominate δ0 when x ∼N(θ, Ip) and
0 < h(x) ≤2(p −1).
b. Show that this condition is also necessary when h is constant.
2.8 Notes
2.8.1 Loss functions for imaging
An image, as represented on a computer screen, is a two dimensional struc-
ture x made of pixels of diﬀerent colors (or grey levels, for black and white
images). Images are often observed with some noise, which can be resulting
from imperfections in the imaging device, like a camera out of focus, to per-
turbations in the transmission process, like parasites on a telephone line, or
to defects in the image itself, like clouds for a satellite image. Among other
things, Bayesian imaging aims at reconstructing the true image.
The observed image, x, can also be written as a vector (x1 . . . , xN), each xi
taking values in {0, 1, . . . , C −1}, which is the set of colors. The true image is
denoted θ and x is distributed as x ∼f(x|θ).
The most rudimentary loss function in this setting is the “0–1” dichotomous
loss function, L0(θ, δ) = 0 if θ = δ and L0(θ, δ) = 1 otherwise. Given a prior
π(θ), the Bayes estimator δπ associated with the 0–1 loss function is the image
that maximizes the posterior distribution π(θ|x), the so-called MAP estimator.
As pointed out by Rue (1995), since the loss function is extremely sensitive to
misclassiﬁcations, this loss leads to oversmoothing, omitting small structures
which are important in settings like pattern recognition.
The other standard loss function is the misclassiﬁcation rate, that is, the
number of misclassiﬁcations, based on the misclassiﬁcation vector e, which is
deﬁned, for an estimator δ and a true image θ, as ei = IIδi̸=θi (i = 1, . . . , N).
The number of misclassiﬁcations is thus
L1(θ, δ) =
N

i=1
ei .
Given the additive structure of this loss, the posterior loss is the sum of the site
losses IE[ei|x] and the Bayes estimator is therefore the vector of the marginal
MAP estimators. The drawback of this loss function is thus the opposite of the
0–1 loss function: the estimation is too local and does not take into account
interactions between neighboring sites.
Rue (1995) introduces a family of new loss functions for the construction of
estimators in Bayesian imaging, in order to account for diﬀerent features of
the image. If D is a subset of {1, . . . , N}, mD denotes the number of misclas-
siﬁcations in D,
mD =

i∈D
ei ,
p(mD) is 0 if mD = 0 and 1 otherwise, RφD is the set D rotated by an angle
φ ∈{0, ±π/2, π} and TsD is the set D translated by s (in its two-dimensional

2.8
Notes
97
representation). If Pj(D) denotes the set of subsets of D of size j, the loss
functions are constructed by selecting (i) a set of basic subsets of {1, . . . , N},
B1, . . . , Bn, and (ii) penalty coeﬃcients tij, such that the penalty associated
with a region Bi is
Pi(mBi) =
|Bi|

i=1
ti,j

ω∈Pj(Bi)
p(mω) .
The loss function is then
L(θ, δ) =
n

i=1

s,φ
Pi(mTsRφBi) ,
(2.8.1)
where the inner sum is restricted to the couples (s, φ) such that TsRφBi is a
subset of {1, . . . , N}, that is, is inside the original image.
The motivation for using such a combination becomes clearer when, as in Rue
(1995), n = 1, B1 is the 2×2 region made of the four neighbors of an arbitrary
point. In this particular case, Rue (1995) propose to take t1,1 = 1 to penalize
misclassiﬁcations at one site and to choose a t1,2 > 0 to penalize further cases
when two neighbor sites simultaneously misclassify, while t1,3 = t1,4 = 0. The
resulting loss is then the number of misclassiﬁed sites in the image, plus t1,2
times the number of couples of simultaneously misclassiﬁed neighbors.
As detailed in Rue (1995), other examples include minimal resolution prob-
lems, pattern recognition and Ising models. For instance, the basic subsets Bi
may include particular shapes of interest, like cars for traﬃc control vision
or tumors in radiological image-processing. Obviously, the computation of the
Bayes estimate associated with (2.8.1) is not so straightforward as with L0 and
L1, and Rue (1995) proposes an iterative method based on a Markov chain
(Chapter 6).
2.8.2 The Stein eﬀect
If there is a unique minimax estimator, this estimator is admissible, according
to Proposition 2.4.20. Conversely, if a minimax estimator δ0 is inadmissible,
there are other minimax estimators that improve upon δ0 (under some minor
regularity conditions, see Brown (1976)). In particular, if the constant risk
minimax estimator is inadmissible, this is the worst minimax estimator in
the sense that every other minimax estimator has a uniformly smaller risk.
Until 1955, it was assumed that the least-squares estimator, δ0(x) = x, when
x ∼Np(θ, Ip), was admissible and, since its risk is constant, that it was the
unique minimax estimator. Stein (1955a) showed that this result only holds
for p = 1, 2 and hence discovered “the Stein eﬀect” phenomenon, that is, the
exhibition of apparently paradoxical domination results for usual estimators.
Formally, the Stein paradox is as follows. If a standard estimator δ∗(x) =
(δ0(x1), . . . , δ0(xp)) is evaluated under weighted quadratic loss
p

i=1
ωi(δi −θi)2,
(2.8.2)
with ωi > 0 (i = 1, . . . , p), there exists p0 such that δ∗is not admissible for
p ≥p0, although the components δ0(xi) are separately admissible to estimate

98
Decision-Theoretic Foundations
2
the θi’s. The Stein eﬀect can be explained through the use of the joint loss
(2.8.2), that allows the dominating estimator to borrow strength from the
other components, even when they are independent and deal with totally
diﬀerent estimation problems. The literature on the Stein eﬀect and related
phenomena is now too extensive for us to give here a comprehensive covering
of the results in this ﬁeld. We refer the reader to Judge and Bock (1978),
Lehmann (1983), and Berger (1985a) for a more detailed bibliography and we
develop in Chapter 10 a Bayesian analysis of the Stein eﬀect. This note brieﬂy
presents the main results about the Stein eﬀect from a frequentist point of
view.
First, while Stein’s (1955a) proof of inadmissibility was nonconstructive, James
and Stein (1961) exhibited an estimator that uniformly dominates δ0(x) = x
under quadratic loss for p ≥3 in the normal case, i.e., such that, for every θ,
p = IEθ[||δ0(x) −θ||2] > IEθ[||δJS(x) −θ||2].
This estimator,
δJS(x) =

1 −p −2
||x||2

x,
(2.8.3)
is now called the James–Stein estimator. Note the strange behavior of δJS
when x gets near 0. The factor 1 −
p−2
||x||2 becomes negative and even goes
to −∞as ||x|| goes to 0. However, δJS still dominates δ0 for all θ’s. (This
is a consequence of Theorem 2.8.1 below.) Baranchick (1970) corrected this
paradoxical behavior by showing that the truncated estimators (p −2 ≤c ≤
2(p −2))
δ+
c (x)
=

1 −
c
||x||2
+
x
=

(1 −
c
||x||2 )x
if ||x||2 > c,
0
otherwise,
(2.8.4)
uniformly dominate their nontruncated counterparts and, in particular, that
δ+
p−2 was improving on δJS. They are, moreover, noncomparable (as c varies).
This class of estimators is important because, although made of inadmissible
estimators (see Chapter 8), estimators that dominate the truncated James–
Stein estimator are quite diﬃcult to derive and do not bring signiﬁcant im-
provement in terms of risk (see Shao and Strawderman (1996)). On the con-
trary, the truncated (or positive-part) James–Stein estimators improve quite
signiﬁcantly on the least-squares estimator, as illustrated in Figure 2.4.2 for
p = 10 and c = 2p −1.
Following James and Stein (1961), more general classes of estimators dom-
inating δ0 have been proposed by Alam (1973), Berger and Bock (1976),
Judge and Bock (1978), Stein (1981), George (1986a,b), and Brandwein et al.
(1992). These estimators are called shrinkage estimators because, as (2.8.3)
and (2.8.4), they shrink x toward 0. Stein eﬀects have also been exhibited
for distributions other than the normal distribution and losses other than
the quadratic loss by Berger (1975), Brandwein and Strawderman (1980),
Hwang (1982a), Ghosh et al. (1983), Bock (1985), Haﬀand Johnston (1987),
Srivastava and Bilodeau (1988), Brandwein and Strawderman (1990). Some
restrictions on the classes of shrinkage estimators have been proposed, in or-
der to integrate the admissibility requirement (Brown (1971), Alam (1973),

2.8
Notes
99
Strawderman (1974), Brown (1975), Berger and Srinivasan (1978), Brown and
Hwang (1982), Das Gupta and Sinha (1986), Brown (1988), and Fraisse et al.
(1998)). Bondar (1987) has shown that the improvement (in terms of risk)
brought by the shrinkage estimators is only signiﬁcant on a limited part of
the parameter space, but George (1986a,b) introduced the concept of multiple
shrinkage estimators to extend the region where the improvement occurs (see
Exercise 10.38).
The Stein eﬀect is also robust in the sense that it depends mainly on the
loss function, rather than on the exact distribution of the observations, as
shown by Brown (1975), Shinozaki (1980, 1984), Berger (1980a,b), Das Gupta
(1984), Bilodeau (1988), Cellier, Fourdrinier and Robert (1989), Brandwein
and Strawderman (1990) or Kubokawa, Robert and Saleh (1991, 1992, 1993).
It is not limited to point estimation, but also occurs for conﬁdence regions
(Stein (1962a), Hwang and Casella (1982, 1984), Casella and Hwang (1983,
1987), Robert and Casella (1990), Hwang and Ullah (1994)) and in accu-
racy (or loss) estimation (Johnstone (1988), Rukhin (1988a,b), Lu and Berger
(1989a,b), Robert and Casella (1993), Fourdrinier and Wells (1994)), George
and Casella (1994). However, Gutmann (1982) established that the Stein ef-
fect cannot occur in ﬁnite parameter spaces. Brown (1971) (see also Srinivasan
(1981), Johnstone (1984), and Eaton (1992)) showed that admissibility is re-
lated to the recurrence of a stochastic process associated with the estimator
and Brown (1980) shows the surprising result (called Berger’s phenomenon,
from Berger (1980b)) that there always exists a loss function such that the
boundary between admissibility and inadmissibility for the usual estimator is
an arbitrary dimension p0.
This overview does not do justice to the richness of the work on the Stein eﬀect.
The advances realized in this ﬁeld in the last thirty years have greatly beneﬁt-
ted Decision Theory in general, and Bayesian Decision Theory in particular.
In fact, one of the major impacts of the Stein paradox has been to signify the
end of a Golden Age for classical Statistics, since it shows that the quest for the
best estimator, that is, the unique minimax admissible estimator, is hopeless,
unless one restricts the class of estimators to be considered, or incorporates
some prior information. The works on the Stein eﬀect have thus led to the
progressive abandonment of unbiasedness, to a deeper understanding of mini-
maxity and admissibility, and to the improvement of frequentist techniques of
risk computation (with Stein’s (1973) idea of unbiased estimator of the risk).
However, its main consequence has been to reinforce the Bayesian-frequentist
interface,5 by inducing frequentists to call for Bayesian techniques (see, for
instance, Bock’s (1988) idea of pseudo-Bayes estimators) and Bayesians to
robustify their estimators in terms of frequentist performances and prior un-
certainty (Berger (1980b, 1982a, 1984a), George (1986a,b), Lu and Berger
(1989a,b), Berger and Robert (1990)). We refer the reader to the books men-
tioned above as well as to Brandwein and Strawderman (1990) and Lehmann
and Casella (1998) for additional references.
We conclude this note with the proof of the inadmissibility of δ0(x) = x in the
estimation of θ for spherically symmetric distributions, i.e., distributions with
densities f(||x −θ||) in IRp (p ≥3). References on these distributions which
5 A typical example is provided by the development of the empirical Bayes techniques
(see Chapter 10).

100
Decision-Theoretic Foundations
2
generalize the normal distribution in linear regression models are given in
Kelker (1970), Eaton (1986) and Fang and Anderson (1990) (see also Exercise
1.1). This result was ﬁrst established in Cellier et al. (1989).
Theorem 2.8.1
Consider z = (xt, yt)t ∈IRp, with distribution
z ∼f(||x −θ||2 + ||y||2),
(2.8.5)
and x ∈IRq, y ∈IRp−q. An estimator
δh(z) = (1 −h(||x||2, ||y||2))x
dominates δ0 under quadratic loss if there exist α, β > 0 such that:
(1) tαh(t, u) is a nondecreasing function of t for every u;
(2) u−βh(t, u) is a nonincreasing function of u for every t; and
(3) 0 ≤(t/u)h(t, u) ≤
2(q −2)α
p −q −2 + 4β .
The above conditions on h are thus independent of f in (2.8.5), which does
not need to be known, and, moreover, they are identical to those obtained in
the normal case (see Brown (1975)). The occurrence of the Stein eﬀect is then
robust in the class of spherically symmetric distributions with ﬁnite quadratic
risk.
Proof.
Conditions (1) and (2) imply

t ∂
∂th(t, u) ≥−αh(t, u),
u ∂
∂uh(t, u) ≤βh(t, u).
The risk of δh can be developed as follows:
R(θ, δh)
=
IEθ

q

i=1

xi −θi −h(||x||2, ||y||2)xi
2

=
IEθ

q

i=1
(xi −θi)2

−2IEθ

q

i=1
h(||x||2, ||y||2)xi(xi −θi)

+IEθ
+
h2(||x||2, ||y||2)||x||2,
.
An integration by parts shows that
 +∞
−∞
h(||x||2, ||y||2)xi(xi −θi)f(||x −θ||2 + ||y||2) dxi
=
 +∞
−∞
∂
∂xi
+
h(||x||2, ||y||2)xi
, ¯F(||x −θ||2 + ||y||2) dxi,
with
¯F(t) =
 +∞
t
f(u)du.
Therefore,
IEθ

q

i=1
h(||x||2, ||y||2)xi(xi −θi)

=

IRp
+
qh(||x||2, ||y||2) + 2h′
1(||x||2, ||y||2)||x||2, ¯F(||x −θ||2 + ||y||2) dz ,

2.8
Notes
101
where h′
1(t, u) =
∂
∂th(t, u). Similarly,
IEθ[h2(||x||2, ||y||2)||x||2] = IEθ

||x||2
||y||2 h2(||x||2, ||y||2)||y||2

=

IRp
||x||2
p−q

j=1
∂
∂yj

h2(||x||2, ||y||2) yj
||y||2

¯F(||x −θ||2 + ||y||2) dz
=

IRp
||x||2

4h(||x||2, ||y||2)h′
2(||x||2, ||y||2)||x||2
= + (p −q −2)h2(||x||2, ||y||2)
1
||y||2

¯F(||x −θ||2 + ||y||2) dz,
where h′
2(t, u) =
∂
∂uh(t, u). The diﬀerence of the risks is then
R(θ, δ0) −R(θ, δh) =

IRp

2

qh(||x||2, ||y||2) + 2h′
1(||x||2, ||y||2)||x||2

||x||2h(||x||2, ||y||2)

4h′
2(||x||2, ||y||2) −(p −q −2)h(||x||2, ||y||2)
1
||y||2

× ¯F(||x −θ||2 + ||y||2) dz
≥

IRp
h(||x||2, ||y||2)

−h(||x||2, ||y||2)||x||2
||y||2 (p −q −2 + 4β)
+2(q −2α)

¯F(||x −θ||2 + ||y||2) dz > 0,
which concludes the proof.
22
Notice that this domination result includes as a particular case the estimation
of a normal mean vector when the variance is known up to a multiplicative
factor, i.e., the problem originally considered in James and Stein (1961). When
h(t, u) = au/t, the bound on a is 2(q −2)/(p −q + 2), as obtained in James
and Stein (1961).
2.8.3 Pitman closeness
An alternative to standard Decision Theory has been proposed by Pitman
(1937). In order to compare two estimators of θ, δ1 and δ2, he indeed suggested
the comparison of the distribution of their distance (or closeness) to θ, i.e.,
Pθ (||δ1(x) −θ|| ≤||δ2(x) −θ||) .
If this probability is uniformly larger than 0.5, δ1 is said to dominate δ2 in
Pitman’s sense, with the implicit message that δ1 should be preferred to δ2 in
this case. Even though formally close to stochastic domination, this criterion,
called Pitman closeness, exhibits important ﬂaws and we suggest it not be
used as a comparison criterion. Nonetheless, the literature on the subject is
quite important (see, e.g., Blyth (1972), Rao (1980, 1981), Blyth and Pathak
(1985), Rao et al. (1986), Keating and Mason (1985), Peddada and Khattree
(1986), Sen et al. (1989), Ghosh and Sen (1989)). These papers study the
properties of Pitman closeness and stress its intrinsic aspect, since it involves

102
Decision-Theoretic Foundations
2
the whole distribution of ||δ1(x) −θ|| (as opposed to the reductive evaluation
through a loss, like the quadratic loss). On the other hand, Robert et al.
(1993) expose the fundamental drawbacks of this criterion. We present here
two characteristic points (see Exercises 2.57–2.62 for other illustrations).
A ﬁrst major criticism of Pitman closeness deals with its nontransitivity. In-
deed, it does not provide a mean of selecting an optimal estimator, or even
to order estimators. Pitman (1937) already pointed out this defect, but some
proponents of the criterion (see, e.g., Blyth (1993)) paradoxically assert that
the property is an additional advantage of the criterion, since it better reﬂects
the complexity of the world. As discussed above, it may indeed happen that
realistic orderings of preferences are not always transitive. But, apart from the
utmost necessity for reducing this complexity, note that the Pitman closeness
criterion is advocated as a comparison criterion, an alternative to regular loss
functions: when nontransitivity occurs, the ordering derived from the criterion
is not absolute since, as the following example shows, there is always a chance
for a preference cycle. In such cases, the criterion cannot provide a selection
of a best estimator.
Example 2.8.2
Consider U ∼U[−0.9,1.1] and x = θU. Then, it can be shown
that, under the Pitman closeness criterion, δ0(x) = x dominates δ1(x) = 0.9|x|,
δ1 dominates δ2(x) = 3.2|x|, and δ2 dominates δ0. If one of the three estimators
δ0, δ1, and δ2 has to be selected, the criterion is of no help.
∥
Obviously, nontransitivity prevents the Pitman criterion from being equivalent
to a loss; therefore, it does not pertain to Decision Theory. For the same reason,
it cannot be equivalent to stochastic domination. Actually, Blyth and Pathak
(1985) provide an example where the two criteria give opposed ordering. It is
also impossible to deﬁne a (decision-theoretic) Bayes estimator for the Pitman
criterion (although a posterior Pitman estimator may exist. See Bose (1991)
and Ghosh et al. (1993).)
A second major defect of Pitman closeness is that it may exclude some classi-
cal estimators although these are admissible under most losses. For instance,
Efron (1975) noticed that it is possible to dominate δ0(x) = x for the Pitman
closeness in the normal case, x ∼N(θ, 1). Robert et al. (1993) show that there
is a Stein eﬀect for Np(θ, Ip) (p ≥2), and that the dominating condition only
involves an upper bound on the shrinkage function h (see also Sen et al. (1989)
and Exercise 2.62). The following result extends Efron (1975) to the general
case when x ∼f(x −θ) and θ is the median of the distribution (see Exercise
2.59 for a proof).
Proposition 2.8.3
Under the above conditions, the estimator δ0(x) = x is
inadmissible for the Pitman criterion.
Moreover, the dominating estimators may exhibit strange behaviors, such as
being 0 on large parts of the sample space (see Exercise 2.59).
These multiple drawbacks seem to indicate clearly that the Pitman closeness
is not a viable alternative to Decision Theory. On the other hand, the failure
of this substitution reinforces our belief that Decision Theory is the proper
setting to take decisions under uncertainty.

2.8
Notes
103
As stressed in the Introduction, determination of the loss is an important
step in the derivation of the model. This requirement is too often bypassed
by resorting to classical losses, and it would be interesting to consider loss
robustness analyses, similar to those conducted about the inﬂuence of the
prior distribution (see Section 3.5). However, the diﬃculty of the task at hand
is not enough to justify the abandonment of the coherence inherent to Decision
Theory for exotic criteria like the Pitman closeness.


CHAPTER 3
From Prior Information
to Prior Distributions
In the meantime, there was so much information to gather, so many puzzles
to solve. Their house was the perfect place for Moraine to ﬁnd the information
she needed. Except that it was not there.
Robert Jordan, The Great Hunt, Book II of the Wheel of Time.
3.1 The diﬃculty in selecting a prior distribution
Undoubtedly, the most critical and most criticized point of Bayesian ana-
lysis deals with the choice of the prior distribution, since, once this prior
distribution is known, inference can be led in an almost mechanic way by
minimizing posterior losses, computing higher posterior density regions, or
integrating out parameters to ﬁnd the predictive distribution. The prior
distribution is the key to Bayesian inference and its determination is there-
fore the most important step in drawing this inference. To some extent, it is
also the most diﬃcult. Indeed, in practice, it seldom occurs that the avail-
able prior information is precise enough to lead to an exact determination
of the prior distribution, in the sense that many probability distributions
are compatible with this information. There are many reasons for this: the
decision-maker, the client or the statistician do not necessarily have the
time or resources (nor often the willingness) to hunt for an exact prior
(which, anyway, may not exist given the information at hand) and they
have to complete the partial information they gathered with a subjective
input to build a prior distribution.
Most often, it is then necessary to make a (partly) arbitrary choice of
the prior distribution, which can drastically alter the subsequent inference.
In particular, the systematic use of parametrized distributions (like the
normal, gamma, beta, etc., distributions) and the further reduction to con-
jugate distributions (deﬁned below in Section 3.3) cannot be justiﬁed at all
times, since they trade an improvement in the analytical treatment of the

106
From Prior Information to Prior Distributions
3
problem for the subjective determination of the prior distribution and may
therefore ignore part of the prior information. Some settings nonetheless
call for a partly automated determination of the prior distribution as, for
instance, when prior information is totally lacking. We will consider two
common techniques, the conjugate prior approach (Section 3.3), which re-
quires a limited amount of information, and the noninformative approach
(Section 3.5), which can be directly derived from the sampling distribution.
Historically, critics of the Bayesian paradigm have focused their criti-
cisms on the choice of the prior distribution, starting with Laplace’s as,
while Bayes was able to justify his prior modeling on the billiard balls by
a physical reasoning (see Section 1.2), the abstract modelings of Laplace
on the distribution of white balls in an urn (Example 1.2.3), or on the
proportion of boys (Example 1.2.4), both based on a principle of insuﬃ-
cient reason, were more apt to give an opening for criticisms which indeed
appeared soon after (see Boole (1854), Venn (1866), Bertrand (1889), and
Chrystal (1891)).
These attacks against the Bayesian approach had some validity in the
sense that they pointed out that there is no unique way of choosing a prior
distribution, and that the choice of the prior distribution has an inﬂuence
on the resulting inference. This inﬂuence can be negligible, moderate or
enormous: it is always possible to choose a prior distribution that gives the
answer one wishes. The main point here is that, ﬁrst, ungrounded prior dis-
tributions produce unjustiﬁed posterior inference and, second, that there is
no such thing as the prior distribution, except for very special settings. Af-
ter years of criticism, the work of Jeﬀreys (1946) on noninformative priors
thus came as a blessing for the Bayesian community because it describes a
method to derive the prior distribution directly from the sampling distri-
bution, although some Bayesians disagree with the use of such automated
methods (see, e.g., Lindley (1971, 1990)). More recently, theoretical de-
velopments on robustness and sensitivity analysis also provided a sounder
basis for Bayesian analysis when it is faced with incomplete prior informa-
tion, while the introduction of hierarchical modeling (Chapter 10) allowed
to push the prior selection to higher levels, with an observed decrease in
the inﬂuence on the resulting inference.
3.2 Subjective determination and approximations
3.2.1 Existence
Unless the decision-maker (or statistician) is informed about the (physical,
economical, biological, etc.) mechanism underlying the generation of the
parameter θ, it is generally quite diﬃcult to propose an exact, or even a
parametrized, form for the prior distribution on θ. Indeed, in most cases,
θ does not have an (intrinsic) existence of its own, but rather corresponds
to an indexing of the distributions describing the random phenomenon of

3.2
Subjective determination and approximations
107
Table 3.2.1. Capture and survival parameter prior information for diﬀerent cap-
ture times and capture sites. (Source: Dupuis (1995).)
Time
2
3
4
5
6
Mean
0.3
0.4
0.5
0.2
0.2
95% cred. int.
[0.1,0.5]
[0.2,0.6]
[0.3,0.7]
[0.05,0.4]
[0.05,0.4]
Site
A
B
Time
t = 1, 3, 5
t = 2, 4
t = 1, 3, 5
t = 2, 4
Mean
0.7
0.65
0.7
0.7
95% cred. int.
[0.4,0.95]
[0.35,0.9]
[0.4,0.95]
[0.4,0.95]
interest. The prior π is then a tool summarizing the available information
on this phenomenon, as well as the uncertainty related with this infor-
mation. Such settings obviously imply approximations of the true prior
distribution—if there is such a thing! In fact, as discussed earlier in Chap-
ter 1, statistical models are most often reductive representations of these
random phenomena and, since there is no true model—but only the model
closest to the phenomenon for some appropriate distance— it is concep-
tually diﬃcult to speak of the true value of θ and, a fortiori, of a true
prior.
Example 3.2.1 (Dupuis (1995)) In a capture-recapture experiment (see
Section 4.3.3) on lizards, biologists are interested in the migrations of lizards
between zones of their habitat (located in a mountainous area in the south
of France). After discussions with the biologists, the information avail-
able on capture and survival probabilities, pt and qit respectively, where
t and i are indices for the various times and zones respectively, is repre-
sented in Table 3.2.1, as prior mean and prior 95% conﬁdence intervals on
these probabilities. Many prior distributions are compatible with this prior
information, obviously, but, since the beta distribution Be(α, β) can be
characterized by its mean and a conﬁdence interval (see Exercise 3.1), the
statistician chose to use the beta prior distributions given in Table 3.2.1. ∥
Example 3.2.2
A decision-maker wants to model the distributions on
both the observations and the parameter as normal distributions: x1, . . . , xn
∼N(θ, 1) and θ ∼N(μ, τ). Since the posterior mean of θ is
δπ(x1, . . . , xn) = ¯xτ + μ/n
τ + 1/n ,
the hyperparameter τ−1 behaves like the sample size, n, while μ behaves
like the sample average, ¯x. Therefore, these hyperparameters can be ap-
proximated by deriving a sample-equivalent of the amount of information
brought through (μ, τ), for instance, by considering that the (known) mean
μ is the average of a virtual sample of size 1/τ.
∥

108
From Prior Information to Prior Distributions
3
Table 3.2.2. Capture and survival prior modeling corresponding to the information
of Table 3.2.1 (Source: Dupuis (1995).)
Time
2
3
4
5
6
Dist.
Be(6, 14)
Be(8, 12)
Be(12, 12)
Be(3.5, 14)
Be(3.5, 14)
Site
A
B
Time
t=1,3,5
t=2,4
t=1,3,5
t=2,4
Dist.
Be(6.0, 2.5)
Be(6.5, 3.5)
Be(6.0, 2.5)
Be(6.0, 2.5)
From a formal point of view, it is possible to build up a prior distribu-
tion the way utility functions were constructed in the previous chapter, by
determining a scale of the respective likelihoods of the values of the param-
eter θ. When the scaling is coherent, i.e., respects axioms given below, the
existence of a prior distribution can be deduced. The existence of subjective
prior distributions as a consequence of an ordering of relative likelihoods
is very important, since it allows us to escape the restrictive framework
of frequentist justiﬁcations that are not always applicable. We describe in
Note 3.8.1 the axioms underlying this derivation of the existence of a prior
distribution from the ordering on the likelihoods, and refer the reader to
DeGroot (1970, Chapter 6) for a more thorough treatment (see also Jeﬀreys
(1961) and Bernardo and Smith (1994)).
It often occurs that the subjective determination of the prior distribution
leads to incoherences in the likelihood ordering, for psychological reasons,
but also because the ability of individuals to identify small probabilities
is quite limited. On this topic, as well as on the practical construction of
probability distributions and the assessment of forecasters, see DeGroot
and Fienberg (1983), Dawid (1984), Lindley (1985) and Smith (1988).
Example 3.2.3 A study in the New England Journal of Medicine showed
that 44% of the questioned individuals were ready to undertake a treat-
ment against lung cancer when told that the survival probability was 68%.
However, only 18% were still willing to undertake it when told that the
probability of failure (death) was 32%.
∥
3.2.2 Approximations to the prior distribution
When the parameter space, Θ, is ﬁnite, one can often obtain a subjective
evaluation of the probabilities of the diﬀerent values of θ. Sometimes, but
not always, it is possible to use past experiments of the same type. Think,
for instance, of constructing the probability of a nuclear war! (See Press
(1989) for such a construction.) More fundamentally, this frequentist ap-
proach leads to the conceptual question of the repeatability of experiments

3.2
Subjective determination and approximations
109
(Are experimental settings always the same? Can a previous experiment be
without eﬀect on the following one?); Jeﬀreys (1961) provides an extended
criticism of this assumption.
When the parameter space Θ is noncountable, for instance, equal to
an interval, the subjective determination of the prior π is obviously much
more complicated. A ﬁrst approximation of π is usually obtained through a
partition of Θ in sets (e.g., intervals) and determination of the probability of
each set; π(θ) is thus approached by an histogram. An alternative approach
is to select signiﬁcant elements of Θ, to evaluate their respective likelihoods
and to deduce a likelihood curve proportional to π. In both cases, a major
diﬃculty occurs when Θ is not bounded, due to the construction of the tails
of the distribution, since it is quite complicated to evaluate subjectively
the probabilities of the extreme regions of the parameter space, while the
shape and properties of the resulting estimators deeply depend on them
(see Example 3.2.6).
When no direct information is available on θ, an alternative approach is
to use the marginal distribution of x,
m(x) =

Θ
f(x|θ)π(θ) dθ
to derive information on π. Several techniques have been proposed in the
literature (see Berger (1985a, §3.6)); apart from the moment method, we
can mention the maximum entropy and the ML-II methods (Good (1983)).
The basis for this derivation is that it may occur that the observed random
phenomenon can be incorporated into a larger class (or a meta-model)
about which information is available. For instance, if θ is the average daily
milk production of a given dairy cow, information about θ can be gathered
from the production of the herd it belongs to, although these observations
originate from the marginal distribution. This perspective is at the core of
hierarchical models (Chapter 10).
3.2.3 Maximum entropy priors
If some characteristics of the prior distribution (moments, quantiles, etc.)
are known, assuming that they can be written as prior expectations,
IEπ[gk(θ)] = ωk
(3.2.1)
(k = 1, . . . , K), a way to select a prior π satisfying these constraints is the
maximum entropy method, developed in Jaynes (1980, 1983).
In a ﬁnite setting, the entropy is deﬁned as
E(π) = −

i
π(θi) log(π(θi)) .
This quantity has been introduced by Shannon (1948) as a measure of
uncertainty in information theory and signal processing. The prior π ma-
ximizing the entropy is, in this information-theoretic sense, minimizing the

110
From Prior Information to Prior Distributions
3
prior information brought through π about θ. The maximum entropy distri-
bution, under the moment constraints (3.2.1), is associated with the density
π∗(θi) =
exp
K
1 λkgk(θi)
 

j exp
K
1 λkgk(θj)
 ,
the numbers λk being derived from (3.2.1) as Lagrange multipliers. For
instance, if there is no constraint on π, the maximum entropy distribution
is the uniform distribution on Θ. (This is a problem because it means the
maximum entropy priors are not invariant under reparameterization; see
Section 3.5.1).
The extension to the continuous case is quite delicate, as it involves the
choice of a reference measure, π0, which can be seen as the completely
noninformative distribution, i.e., the maximum entropy prior when no con-
straint is imposed. This reference measure can be selected in many ways, as
explained in Section 3.5, while the maximum entropy distribution depends
on this choice. When a group structure is available (and accepted as part of
the prior information) for the problem of interest, it is usually agreed that
the right-invariant Haar measure associated with this group is the natural
choice for π0. (Justiﬁcations for such a choice are given in Chapter 9.) The
reference measure π0 being selected, the entropy of π is deﬁned as
E(π)
=
IEπ0

log
 π(θ)
π0(θ)

=

log
 π(θ)
π0(θ)

π0(dθ),
which is also the Kullback–Leibler distance between π and π0. In this case,
the maximum entropy distribution under (3.2.1) is given by the density
π∗(θ) =
exp
K
1 λkgk(θ)
 
π0(θ)

exp
K
1 λkgk(η)
 
π0(dη)
,
(3.2.2)
thus showing the importance of π0. Notice that the above distributions π∗
are necessarily in an exponential family (see Section 3.3.3).
In addition to the dependence on π0 exhibited in (3.2.2), another draw-
back of the maximum entropy method is that the constraints (3.2.1) are
not always suﬃcient to derive a distribution on θ. Notice that this is often
the case when characteristics (3.2.1) are related with the quantiles, since
the functions gk(θ) are of the form II(−∞,ak](θ) or II(bk,∞](θ).
Example 3.2.4 Consider θ, a real parameter such that IEπ[θ] = μ. If the
reference measure π0 is the Lebesgue measure on IR, the maximum entropy
prior satisﬁes π∗(θ) ∝eλθ and cannot be normalized into a probability
distribution. On the contrary, if in addition it is known that var(θ) = σ2,

3.2
Subjective determination and approximations
111
the corresponding maximum entropy prior is
π∗(θ) ∝exp{θλ1 + θ2λ2},
i.e., the normal distribution N(μ, σ2).
∥
See Seidenfeld (1987) and Kass and Wasserman (1996) for criticisms of
the maximum entropy approach (Exercise 3.2).
3.2.4 Parametric approximations
A frequently used alternative when building up a continuous prior con-
sists of arbitrarily restricting the choice of π to a parametrized type of
density and determining the corresponding parameters, either through the
moments, or through the quantiles, since the latter are more robust. For
instance, subjective evaluations of the median and the 75% quantile are
enough to identify the two parameters of a normal distribution. (See also
Example 3.2.1.)
Example 3.2.5
Let Xi ∼B(ni, pi) be the number of passing students
in a freshman calculus course of ni students. Over the previous years, the
average of the pi is 0.70, with variance 0.1. If we assume that the pi’s
are all generated according to the same beta distribution, Be(α, β), the
parameters α and β can be estimated through
α
α + β = 0.7,
αβ
(α + β)2(α + β + 1) = 0.1,
i.e., α = 0.77, β = 0.33, leading to the prior distribution
p ∼Be(0.77, 0.33).
The choice of a beta distribution is motivated in this setting by conjugate
prior arguments (see Section 3.3).
∥
The moment method is often impractical and sometimes produces impos-
sible values of the parameters; for instance, it can give negative variances.
However, a deeper drawback of most parametric approaches is that the
selection of the parametrized family is based on ease in the mathematical
treatment, not on a subjective basis such as a preliminary histogram ap-
proximating π. These approaches may even lead to a partial rejection of
the available information, on the grounds that it is not compatible with
the parametric distribution. For instance, in Examples 3.2.1 and 3.2.5, the
additional prior knowledge of the median may prevent the use of a beta
distribution. Actually, the derivation of a distribution from a histogram
may also be misleading since diﬀerent families may ﬁt the histogram and
still lead to quite diﬀerent inferences. (Nonetheless, we will study in the
next section a particular parametrized prior determination in detail, since
limited information settings call for parametrized prior distributions.)

112
From Prior Information to Prior Distributions
3
Table 3.2.3. Ranges of the posterior moments for ﬁxed prior moments μ1 = 0 and
μ2. (Source: Goutis (1990).)
Minimum
Maximum
Maximum
μ2
x
mean
mean
variance
3
0
-1.05
1.05
3.00
3
1
-0.70
1.69
3.63
3
2
-0.50
2.85
5.78
1.5
0
-0.59
0.59
1.50
1.5
1
-0.37
1.05
1.97
1.5
2
-0.27
2.08
3.80
Example 3.2.6 (Berger (1985a)) Let x ∼N(θ, 1). Assume that the prior
median of θ is 0, the ﬁrst quartile is −1, and the third quartile is +1.
Then, if the prior distribution on θ is of the form N(μ, τ), we must have
θ ∼N(0, 2.19). On the contrary, the choice of a Cauchy distribution implies
θ ∼C(0, 1). Under a quadratic loss, the Bayes estimator will be in the ﬁrst
case
δπ
1 (x) = x −
x
3.19
and, for |x| ≥4,
δπ
2 (x) ≈x −
x
1 + x2
in the second case (see Berger and Srinivasan (1978)). Therefore, for x = 4,
which is an observation quite compatible with the prior information in both
cases, the two estimations would be δπ
1 (4) = 2.75 and δπ
2 (4) = 3.76.
∥
These posterior discrepancies call for some tests on the validity (or ro-
bustness) of the selected priors, depending on the observation, in order
to evaluate how a slight change in the prior distribution is perceived in
the inference about the parameter of interest. (Section 3.5 deals with this
evaluation.) The example below illustrates again the fact that an item of
information that is too vague can produce very diﬀerent conclusions, de-
pending on the way it is interpreted.
Example 3.2.7 (Goutis (1990, 1994)) Let x ∼f(x|θ), with θ ∈IR, and
assume that the prior mean of θ, μ1, is known. Too many prior distributions
agree with this piece of information, since
inf
π IEπ[θ|x] = −∞
and
sup
π IEπ[θ|x] = +∞
and no useful inference can be derived from this single fact; notice that, in
this setting, it is not possible to construct a maximum entropy distribution
either (see Example 3.2.4). If, in addition, the prior variance μ2, is ﬁxed,

3.3
Conjugate priors
113
the variability of the posterior answers is more restricted since
−∞< inf
π IEπ[θ|x] ≤sup
π IEπ[θ|x] < +∞,
(3.2.3)
as long as f(x|θ) is positive in a neighborhood of μ1 and bounded when
|θ −μ1| is large. Under the same set of assumptions, we have, in addition,
0 = inf
π Varπ[θ|x] ≤sup
π Varπ[θ|x] < +∞.
(3.2.4)
Table 3.2.4 gives the exact range of the bounds (3.2.3) and (3.2.4) for a
normal distribution N(θ, 1) and μ1 = 0.
∥
3.2.5 Other techniques
Empirical and hierarchical Bayes techniques are two rather antagonistic
approaches that naturally incorporate uncertainty about the prior distri-
bution and will be detailed in Chapter 9 (see also Carlin and Louis (2000a)).
The empirical Bayes technique relies on the observations (and the marginal
distribution) to estimate the parameters of the prior distribution; it is used
by frequentists more often than by Bayesians because it does not belong
to the Bayesian paradigm. Formally, it seems paradoxical to choose a prior
distribution a posteriori! More fundamentally, the choice of π depending on
x, the derived estimators do not enjoy the optimality properties of the true
Bayes estimators. A last criticism is that too many choices are possible for
the estimation techniques used in the construction of the prior distribution,
thus leading to an important arbitrariness in the selection of the prior.
The hierarchical Bayes approach models the lack of information on the
parameters of the prior distribution according to the Bayesian paradigm,
i.e., through another prior distribution on these parameters (the parameters
of this distribution are then called hyperparameters and this new prior
a hyperprior). Although this choice may seem too abstract conceptually,
Bayesians usually prefer this approach over the empirical Bayes alternative
because it generally provides better estimators in practical and theoretical
senses. (Chapter 10 presents and compares these two techniques.)
3.3 Conjugate priors
3.3.1 Introduction
When prior information about the model is too vague or unreliable, a full
subjective derivation of the prior distribution is obviously impossible. Other
reasons (time delays, cost prohibitions, lack of communication between the
statistician and the decision maker, etc.) may explain the absence of a well-
deﬁned prior distribution. Moreover, objectivity requirements may force the
statistician to provide an answer with as little subjective input as possible,
in order to base the inference on the sampling model alone. These settings

114
From Prior Information to Prior Distributions
3
may seem to call for a non-Bayesian solution (maximum likelihood estima-
tor, best unbiased estimator, etc.). However, keeping in mind the Bayesian
foundations of the frequentist optimality criteria (see Chapters 2, 8 and
9), it appears preferable to follow a Bayesian approach, using an objective
prior derived from the model as a technical tool. When no prior information
at all is available, these priors are called noninformative and are considered
in Section 3.5.
First, we study in this section a classical parametric approach which in-
volves a subjective input as limited as possible, and which underlies both
the hierarchical and empirical Bayes techniques of Chapter 10. Besides
requiring a minimal subjective input, conjugate priors can indeed be con-
sidered a starting point to build up prior distributions based on limited
prior input, whose imprecision can be assessed through additional prior
distributions. The reader should be aware, however, from the beginning
that the common impression that conjugate priors are noninformative pri-
ors is false: the choice of conjugate priors, while arguable as shown below,
is still a choice and, therefore, inﬂuences the resulting inference to some
extent. Moreover, it may force to ignore part of the prior information if the
later is not entirely compatible with the conjugate prior format and also
there are other priors with the same limited input, but with a more limited
inﬂuence on the resulting inference (see Section 3.6).
Deﬁnition 3.3.1
A family F of probability distributions on Θ is said to
be conjugate (or closed under sampling) for a likelihood function f(x|θ) if,
for every π ∈F, the posterior distribution π(θ|x) also belongs to F.
A trivial example of a conjugate family is the set F0 made of all distribu-
tions on Θ, which is, of course, useless for the choice of a prior distribution.
The main interest of conjugacy becomes more apparent when F is as small
as possible and parametrized. Indeed, when F is parametrized, switching
from prior to posterior distribution is reduced to an updating of the corre-
sponding parameters. This property alone can explain why conjugate priors
are so popular, since the posterior distributions are always computable (at
least to a certain extent). On the contrary, such a justiﬁcation is rather
weak from a subjective point of view and any other parametrized family
would be as convenient. Notice that the goal to obtain the minimal con-
jugate family as the intersection of all conjugate families is unfortunately
doomed to failure, since this intersection is empty (Exercise 3.13).
3.3.2 Justiﬁcations
The conjugate prior approach, which originated in Raiﬀa and Schlaifer
(1961), can be partly justiﬁed through an invariance reasoning. Actually,
when the observation of x ∼f(x|θ) modiﬁes π(θ) into π(θ|x), the infor-
mation conveyed by x about θ is obviously limited; therefore, it should

3.3
Conjugate priors
115
not lead to a modiﬁcation of the whole structure of π(θ), but simply of
its parameters. In other words, the modiﬁcation resulting from the obser-
vation of x should be of ﬁnite dimension. A more radical change of π is
thus unacceptable and the choice of the prior distributions should always
be made among conjugate distributions, whatever the prior information is.
De Finetti (1974) somehow held similar views because he considered that
the prior information could be translated into virtual past observations, as
in Example 3.2.2, leading necessarily to conjugate priors for exponential
families (see below). This requirement unfortunately becomes paradoxical
in the extreme case when the whole prior distribution is already available!
But conjugate priors are mainly used in limited information environments,
since they only call for the determination of a few parameters. Another jus-
tiﬁcation for using conjugate priors is that some Bayes estimators are then
linear, as shown by Diaconis and Ylvisaker (1979) (see Proposition 3.3.13,
below). Nonetheless, we must acknowledge that the main motivation for
using conjugate priors remains their tractability.
This particular modeling through a parametrized family of priors is in-
deed attractive, since it allows for an explicit treatment of posterior dis-
tributions. These conjugate priors are sometimes called objective because
the sampling model, f(x|θ), entirely determines the class of priors, but any
method deriving priors automatically from sampling distributions would be
similarly objective. A contrario, the use of conjugate priors is strongly sus-
picious for subjective Bayesians because it is mainly justiﬁed on technical
grounds, rather than for its proper ﬁtting of the available prior informa-
tion. The role of conjugate priors is thus to provide a ﬁrst approximation to
the adequate prior distribution, which should be followed by a robustness
analysis (see Section 3.5). We will see in Section 3.4 that they are more
justiﬁed if treated as a basis (in the functional sense) for prior information
modeling.
3.3.3 Exponential families
Conjugate prior distributions are usually associated with a particular type
of sampling distribution that always allows for their derivation, and is even
characteristic of conjugate priors, as we will see below. These distributions
constitute what is called exponential families, studied in detail in Brown
(1986).
Deﬁnition 3.3.2
Let μ be a σ-ﬁnite measure on X, and let Θ be the
parameter space. Let C and h be functions, respectively, from X and Θ to
IR+, and let R and T be functions from Θ and X to IRk. The family of
distributions with densities (w.r.t. μ)
f(x|θ) = C(θ)h(x) exp{R(θ) · T (x)}
(3.3.1)
is called an exponential family of dimension k. In the particular case when

116
From Prior Information to Prior Distributions
3
Θ ⊂IRk, X ⊂IRk and
f(x|θ) = C(θ)h(x) exp{θ · x},
(3.3.2)
the family is said to be natural.
Notice that a change of variables from x to z = T (x) and a reparameteri-
zation from θ to η = R(θ) authorizes us to consider mainly the natural form
(3.3.2), although the spaces T (X) and R(Θ) may be diﬃcult to describe
and work with.
From an analytic point of view, exponential families have many interest-
ing properties (see Brown (1986)). In particular, they are such that, for any
sample from (3.3.1), there exists a suﬃcient statistic of constant dimension.
Indeed, if x1, . . . , xn ∼f(x|θ), with f satisfying (3.3.2),
¯x = 1
n
n

i=1
xi ∈IRk
is suﬃcient for every n. The converse of this result has also been established
by Koopman (1936) and Pitman (1936) (see also Jeﬀreys (1961, §3.7.1) for
a proof).
Theorem 3.3.3 (Pitman–Koopman Lemma) If a family of distribu-
tions f(·|θ) is such that, for a sample size large enough, there exists a
suﬃcient statistic of constant dimension, the family is exponential if the
support of f(·|θ) does not depend on θ.
The restriction on the support of f(x|θ) is necessary for the lemma to
hold because the uniform U([−θ, θ]) and the Pareto P(α, θ) distributions
also satisfy this property (see Example 3.3.10). These distributions could
actually be called quasi-exponential families because they partake of many
interesting properties of exponential families, including the existence of
constant-dimension suﬃcient statistics and of conjugate priors (Exercise
3.15).
Many common continuous and discrete distributions belong to exponen-
tial families.
Example 3.3.4
If S is the simplex of IRk,
S =

ω = (ω1, . . . , ωk);
k

i=1
ωi = 1, ωi > 0

,
the Dirichlet distribution on S, Dk(α1, . . . , αk), is an extension of the beta
distribution, which is deﬁned by
f(p|α) = Γ(α1 + · · · + αk)
Γ(α1) · · · Γ(αk)
k

i=1
pαi−1
i
IIS(p),

3.3
Conjugate priors
117
where p = (p1, . . . , pk). Since
f(p|α) = C(α)h(p) exp
	
k

i=1
αi log(pi)

,
the Dirichlet distributions constitute a natural exponential family for T (p) =
(log(p1), . . . , log(pk)).
∥
Example 3.3.5
Let x ∼Np(θ, σ2Ip). Then
f(x|θ)
=
1
σp
1
(2π)p/2 exp
	
−
p

i=1
(xi −θi)2/2σ2
=
C(θ, σ)h(x) exp{x.(θ/σ2) + ||x||2(−1/2σ2)}
and the normal distribution belongs to an exponential family with natural
parameters θ/σ2 and −1/2σ2. Similarly, if x1, . . . , xn ∼Np(θ, σ2Ip), the
joint distribution satisﬁes
f(x1, . . . , xn)
=
C′(θ, σ)h′(x1, . . . , xn)
×
exp

n¯x · (θ/σ2) +
n

i=1
||xi −¯x||2(−1/2σ2)

and the statistic (¯x, 
i ||xi −¯x||2) is suﬃcient for all n ≥2.
∥
In the previous example, notice that the parameter space is of dimension
p + 1, while the dimension of the observable, x, is p. While the dimension
of an exponential family is not ﬁxed, since it is always possible to add
convex combinations of the original parameters as additional (and useless)
parameters, there is an intrinsic minimal dimension associated with this
family.
Deﬁnition 3.3.6 Let f(x|θ) = C(θ)h(x) exp(θ.x) be a natural exponential
family. The natural parameter space is
N =

θ;

X
eθ·xh(x) dμ(x) < +∞

.
The family is said to be regular if N is an open set and minimal if dim(N) =
dim(K) = k, where K is the closure of the convex envelope of the support
of μ.
It is always possible to reduce an exponential family to a standard and
minimal form of dimension m, and this dimension m does not depend on
the chosen parameterization (Brown (1986, pp. 13–16)). (See Exercise 3.23
for the example of a non-regular exponential family.)
Natural exponential families can also be rewritten under the form
f(x|θ) = h(x)eθ.x−ψ(θ)
(3.3.3)

118
From Prior Information to Prior Distributions
3
and ψ(θ) is called the cumulant generating function for the following reason,
whose proof is left to the reader.
Lemma 3.3.7
If θ ∈
◦
N, the interior set of N, the cumulant generating
function ψ is C∞and
IEθ[x] = ∇ψ(θ),
cov(xi, xj) =
∂2ψ
∂θi∂θj
(θ),
where ∇denotes the gradient operator.
Example 3.3.8
Let x ∼P(λ). Then
f(x|λ) = e−λ λx
x! = 1
x! eθ.x−eθ
and ψ(θ) = exp(θ) for the natural parameter θ = log λ. Therefore, IEλ[x] =
eθ = λ and var(x) = λ.
∥
The regular structures of exponential families allow for numerous sta-
tistical developments, as shown by the extensive literature on the subject.
(See, for instance, the classiﬁcation of exponential families according to the
type of variance function: Morris (1982), Letac and Mora (1990), and Ex-
ercises 3.24 and 10.33.) We show in Section 3.3.4 that they also allow for a
straightforward derivation of conjugate priors.
Example 3.3.9
If x ∼N(θ, θ2) in a multiplicative model, the conjugate
prior is not the normal distribution. The likelihood is proportional to
1
| θ | exp
x
θ −x2
2θ2

and the distribution induces an exponential family of dimension 2. There-
fore, the generalized inverse normal distributions IN(α, μ, τ), with density
π(θ) ∝|θ|−α exp

−
1
θ −μ
2 -
2τ 2

constitute a conjugate family in this model. This family of distributions,
which belongs to the exponential family, generalizes the distribution of the
inverse of a normal observation (which corresponds to the case α = 2). (See
Exercise 3.33 for details.)
∥
Obviously, most distributions do not belong to an exponential family!
For instance, Student’s t-distribution, Tp(ν, θ, σ2), cannot be written under
the form (3.3.1). Deﬁnition 3.3.2 also excludes all distributions with non-
constant support, while some do allow for conjugate priors with a ﬁnite
number of parameters (or, more exactly, hyperparameters).
Example 3.3.10
Pareto distributions, P(α, θ), with density
f(x|α, θ) = α θα
xα+1 II[θ,+∞[(x)
(θ > 0),

3.3
Conjugate priors
119
-2
-1
0
1
2
0.0
0.5
1.0
1.5
2.0
 0
1
2
Figure 3.3.1. Densities of IN(α, μ, τ) for α = 2, τ = 1, and μ = 0, 1, 2.
are such distributions since, although outside the exponential family frame-
work because their support depends on θ, they allow for simple conjugate
distributions on θ, namely, Pareto distributions for 1/θ.
∥
Other examples of families where conjugate priors are available are U[−θ,θ]
and U[0,θ]
distributions; these distributions are also quasi-exponen-
tial, since they allow for suﬃcient statistics of constant dimension. For
instance, if x1, . . . , xn ∼U[−θ,θ], a suﬃcient statistic is the order statistic
(x(1), x(n)), where x(1) denotes the smallest value in the sample and x(n)
the largest.
Note that, in Example 3.3.9, the conjugate prior on θ depends on three
hyperparameters, α, μ, and τ2; therefore, its use induces a greater complex-
ity than the sampling distribution. This phenomenon, that is, the fact that
the structure of the model requires a larger number of hyperparameters, is
often encountered for curved exponential families, for instance when a nat-
ural reparameterization by η = R(θ) is not useful because of the constraints
on the natural parameters. It is obviously a drawback, since the values of
these hyperparameters have to be determined to derive an inference on θ
using conjugate priors.
When a distribution does not allow for conjugate families, except the
trivial F0, it is sometimes possible to express the distribution as a mixture
of distributions from exponential families; f is then said to be a hidden
mixture, since this mixture representation is of no importance for the in-
ferential problem, but is useful for the practical computation of posterior
distributions and Bayes estimators, as shown in Chapter 6.
Example 3.3.11 (Dickey (1968)) For Student’s t-distribution, there ex-
ists a hidden mixture representation through the normal distribution, since
f(x|θ) is the mixture of a normal distribution and an inverse gamma

120
From Prior Information to Prior Distributions
3
distribution: If x ∼T1(p, θ, σ2),
x|z ∼N(θ, zσ2),
z−1 ∼G(p/2, p/2) .
A technically interesting prior on θ is then N(μ, τ2) and most of the compu-
tations can be done conditional upon z. This decomposition is more useful
when x is multidimensional because some integrals can then be expressed
as being one-dimensional.
∥
Example 3.3.12
Several noncentral distributions can be written as the
(hidden) mixture of the corresponding central distribution by a Poisson dis-
tribution, due to an inﬁnite divisibility property (see Feller (1971, Chapter
9)). For instance, this is the case for the noncentral chi-squared distribu-
tion, χ2
p(λ): When x ∼χ2
p(λ), the generation of x can also be decomposed
as
x|z ∼χ2
p+2z,
z ∼P(λ/2).
This decomposition is used in James and Stein (1961) to express the risk of
the James–Stein estimator and derive a suﬃcient condition of domination
of the maximum likelihood estimator (see Note 2.8.2).
∥
3.3.4 Conjugate distributions for exponential families
Consider f(x|θ) = h(x)eθ.x−ψ(θ), a generic distribution from an exponential
family. It then allows for a conjugate family, as shown by the following
result (whose proof is straightforward).
Proposition 3.3.13
A conjugate family for f(x|θ) is given by
π(θ|μ, λ) = K(μ, λ) eθ.μ−λψ(θ),
(3.3.4)
where K(μ, λ) is the normalizing constant of the density. The corresponding
posterior distribution is π(θ|μ + x, λ + 1).
The measure deﬁned by (3.3.4) is σ-ﬁnite; it induces a probability dis-
tribution on Θ if and only if
λ > 0
and
μ
λ ∈
◦
N
(3.3.5)
(Exercise 3.35). It is only when (3.3.5) holds that K(μ, λ) is well deﬁned.
Therefore, there is an automated way to deduce a conjugate distribution
from f(x|θ); this is why (3.3.4) is often called the natural conjugate distribu-
tion of f. Table 3.3.4 presents the conjugate distributions for the usual dis-
tributions belonging to an exponential family.1 Obviously, Bayesian infer-
ence cannot be conducted unless the hyperparameters μ and λ are known.
1 Since the conjugate distributions are also from an exponential family, Bar-Lev et al.
(1994) have studied a reciprocal problem, namely the determination of the distribu-
tions π(θ) for which an exponential family has π(θ) as conjugate distribution.

3.3
Conjugate priors
121
Table 3.3.1. Natural conjugate priors for some common exponential families
f(x|θ)
π(θ)
π(θ|x)
Normal
Normal
N(θ, σ2)
N(μ, τ2)
N(ϱ(σ2μ + τ 2x), ϱσ2τ 2)
ϱ−1 = σ2 + τ 2
Poisson
Gamma
P(θ)
G(α, β)
G(α + x, β + 1)
Gamma
Gamma
G(ν, θ)
G(α, β)
G(α + ν, β + x)
Binomial
Beta
B(n, θ)
Be(α, β)
Be(α + x, β + n −x)
Negative Binomial
Beta
Neg(m, θ)
Be(α, β)
Be(α + m, β + x)
Multinomial
Dirichlet
Mk(θ1, . . . , θk)
D(α1, . . . , αk)
D(α1 + x1, . . . , αk + xk)
Normal
Gamma
N(μ, 1/θ)
Ga(α, β)
G(α + 0.5, β + (μ −x)2/2)
The automatic aspect of conjugate priors is thus misleading because they
still require a subjective input through the determination of these values.
Notice also that (3.3.4) requires an additional parameter, compared with
f(x|θ).
For natural exponential families, conjugate priors have an additional ap-
peal, as shown by Diaconis and Ylvisaker (1979): If ξ(θ) is the expectation
of x ∼f(x|θ), the posterior mean of ξ(θ) is linear in x for a conjugate prior
distribution.
Proposition 3.3.14
If Θ is an open set in IRk and θ has the prior dis-
tribution
πλ,x0(θ) ∝eθ·x0−λψ(θ)
with x0 ∈X, then
IEπ[ξ(θ)] = IEπ[∇ψ(θ)] = x0
λ .
Therefore, if x1, . . . , xn are i.i.d. f(x|θ),
IEπ[ξ(θ)|x1, . . . , xn] = x0 + n¯x
λ + n .
(3.3.6)
This result is well known for the normal distributions and can be genera-
lized for all exponential families. (The proof is straightforward.) Equation
(3.3.6) shows that the parameter λ is similar to the sample size n. Its

122
From Prior Information to Prior Distributions
3
determination can therefore be achieved, if necessary, by considering that
the prior information on x0 originated from a virtual sample of size λ.
Brown (1986) shows that Proposition 3.3.14 can be extended to the case
where πλ,x0 is improper, for instance when λ = 0 and x0 = 0. In this
case, the posterior expectation is ¯x, which is also the maximum likelihood
estimator of ξ(θ).
Diaconis and Ylvisaker (1979) have shown, in addition, a reciprocal to
this proposition, namely, that, under speciﬁc regularity conditions, if the
dominating measure is continuous with respect to the Lebesgue measure,
linearity of IEπ[ξ(θ)|x] as in (3.3.6) implies that the prior distribution is of
the form (3.3.2). Discrete-case extensions are more delicate.
While exponential families usually allow for an easy processing and, in
particular, for the convenient call to conjugate prior distributions and the
analytical derivation of posterior means, as in Proposition 3.3.14, this is
not always the case. For instance, when x ∼Be(α, θ) with known α, the
distribution belongs to an exponential family because
f(x|θ) ∝Γ(α + θ)(1 −x)θ
Γ(θ)
,
but the conjugate distributions are not easily manageable, as
π(θ|x0, λ) ∝
Γ(α + θ)
Γ(θ)
λ
(1 −x0)θ
involves the Gamma function Γ(θ), which does not have a closed-form ex-
pression.
Example 3.3.15
Logistic regression is used to describe qualitative mod-
els as in Example 1.1.1. Given an indicator variable y, which only takes
values in {0, 1}, and explanatory variables x ∈IRk, the distribution of y
conditional on x is
Pα(y = 1) = 1 −Pα(y = 0) =
exp(αtx)
1 + exp(αtx) .
(3.3.7)
This model allows for the extension of the quite useful linear regression
model to more qualitative settings. For a sample (y1, x1), . . . , (yn, xn) from
(3.3.7), the model is indeed exponential conditional upon the xi’s, as
f(y1, . . . , yn|x1, . . . , xn, α) = exp
!
αt
n

i=1
yixi
" n

i=1
(1 + eαtxi)−1,
which only depends on the suﬃcient statistic n
i=1 yixi. In practice, the
conjugate priors are rather diﬃcult to handle because they are of the form
π(α|y0, λ) ∝eαty0
n

i=1
(1 + eαtxi)−λ.
The normalizing constant for π(α|y0, λ) is unknown and approximations of
posterior quantities such as the posterior mean and posterior median can

3.4
Criticisms and extensions
123
only be achieved through simulation techniques presented in Chapter 6. ∥
3.4 Criticisms and extensions
As seen above, the automated aspect of conjugate distributions is simulta-
neously an advantage and a nuisance. In addition to invariance and linearity
arguments, it has been argued that this is an objective approach, where the
subjective input is reduced to the choice of the hyperparameters. Barring
the fact that objectivity is a concept diﬃcult to deﬁne, it can be countered
that any other prior distribution with the same number of hyperparameters
would seem equally objective. Moreover, the conjugate priors are not nec-
essarily the most robust prior distributions (see Section 3.5) and, from this
point of view, alternative distributions could be preferred, if the imperative
is to minimize the inﬂuence of the prior input on the inferential output.
The following example shows how the choice of the prior can modify the
posterior distribution for small sample sizes.
Example 3.4.1 (Diaconis and Ylvisaker (1985)) When a coin is spun on
its edge, instead of being thrown in the air, the proportion of heads is rarely
close to 1/2, but is rather 1/3 or 2/3 because of irregularities in the edge
that cause the game to favor one side or the other. When spinning, n times,
a given coin on its edge, we observe the number of heads, x ∼B(n, p). The
prior distribution on p is then likely to be bimodal; this cannot be modeled
through a conjugate prior π1 like Be(1, 1). A mixture prior distribution π2
such as
1
2 [Be(10, 20) + Be(20, 10)]
is indeed more appropriate. It may also be the case that previous experi-
ments with the same coin have already hinted at a bias toward head and
that they lead to the following alternative, π3:
0.5 Be(10, 20) + 0.2 Be(15, 15) + 0.3 Be(20, 10).
Figure 3.4.1 provides the graphs of the two prior densities above, along
with the neutral Be(1, 1) prior, the diﬀerences between the three prior
modelings being quite important. If, for n = 10, we observe x = 3, the
corresponding posterior distributions are
(i) Be(1 + x, 1 + n −x), i.e., Be(4, 8);
(ii) 0.84 Be(13, 27) + 0.16 Be(23, 17); and
(iii) 0.77 Be(13, 27) + 0.16 Be(18, 22) + 0.07 Be(23, 17).
In (ii), the posterior probability weights are obtained as being propor-
tional to
1
2
B(13, 27)
B(10, 20)
and
1
2
B(23, 17)
B(20, 10)

124
From Prior Information to Prior Distributions
3
p
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.5
1.0
1.5
2.0
2.5
3.0
1 comp.
2 comp.
3 comp.
Figure 3.4.1. Three prior distributions for a spinning-coin experiment.
and, for (iii),
0.5 B(13, 27)
B(10, 20),
0.2 B(18, 22)
B(15, 15),
and
0.3 B(23, 17)
B(20, 10),
where
B(a, b) = Γ(a)Γ(b)
Γ(a + b)
is the inverse of the normalizing term in the beta density (deﬁned in Ap-
pendix A.3), which must be approximated using numerical methods.
Therefore, for this sample, the three posterior means, 1/3, 0.365, and
0.362 respectively, are fairly close but the shapes of the posterior distribu-
tions still diﬀer (see Figure 3.4). Consider now a sample of size n = 50 with
x = 36. The posterior distributions are
(i) Be(15, 37);
(ii) 0.997 Be(24, 56) + 0.003 Be(34, 46); and
(iii) 0.95 Be(24, 56) + 0.047 Be(29, 51) + 0.003 Be(34, 46).
They are then much closer than for n = 10, as shown by Figure 3.4.
∥
Two general remarks logically stem from this example. First, it shows
that prior modeling is indeed important for small sample sizes, but also
that it becomes less important as the sample size increases. When the
sample size goes to inﬁnity, most priors will lead to the same inference and
this inference will be equivalent to the one based only on the likelihood
function, as pointed out in Note 1.8.4. Moreover, this example shows that
mixtures of conjugate priors are as easy to manipulate as regular conjugate
distributions, while leading to a greater freedom in the modeling of the
prior information. In fact, mixtures of natural conjugate distributions also
make conjugate families.

3.4
Criticisms and extensions
125
p
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
1 comp.
2 comp.
3 comp.
Figure 3.4.2. Posterior distributions for the spinning model for 10 observations.
p
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
1 comp.
2 comp.
3 comp.
Figure 3.4.3. Posterior distributions for 50 observations.
Lemma 3.4.2
Let F be the natural conjugate family of an exponential
family (3.3.2). Then the set of mixtures of N conjugate distributions,
˜FN =
 N

i=1
ωiπ(θ|λi, μi);
N

i=1
ωi = 1, ωi > 0

,
is also a conjugate family. Moreover, if
π(θ) =
N

i=1
ωiπ(θ|λi, μi),
the posterior distribution is a mixture
π(θ|x) =
N

i=1
ω′
i(x)π(θ|λi + 1, μi + x),

126
From Prior Information to Prior Distributions
3
with
ω′
i(x) =
ωiK(μi, λi)/K(μi + x, λi + 1)
N
j=1 ωjK(μj, λj)/K(μj + x, λj + 1)
.
Mixtures can then be used as a basis to approximate any prior distribu-
tion, in the sense that the Prohorov distance between a distribution and
a mixture representation can be made arbitrarily small. The Prohorov dis-
tance between two measures π and ˜π, dP (π, ˜π) is deﬁned as
dP (π, ˜π) = inf
A {ϵ ; π(A) ≤˜π(Aϵ) + ϵ} ,
where the inﬁmum is taken on the Borel sets A, and the set Aϵ denotes the
set of points at distance at most ϵ of A (Le Cam (1986)).
Theorem 3.4.3
If Θ is the natural parameter space for the exponential
family f(x|θ) and π is a prior distribution on Θ, then, for any ϵ > 0, there
exist N and ˜π ∈˜FN such that dP (π, ˜π) < ϵ.
The proof of this theorem can be related to the fact that ﬁnite mixtures
of Dirac measures constitute a dense set for the Prohorov topology, and
that Dirac masses can be approximated by mixtures of conjugate priors.
(For more details, see Brown (1986, pp. 254–267).) This result justiﬁes the
use of conjugate priors much more strongly than the invariance, linearity,
or simplicity arguments presented in the previous section. Whatever prior
information is available, it could be modeled through a mixture from ˜FN
with N as small as possible. However, this approximation result is also
incomplete, since it does not show how the approximation is transferred to
posterior quantities, while Bayesian inference only considers these posterior
quantities. Berger (1985b) illustrates this diﬀerence through the following
example.
Example 3.4.4
Consider x ∼N(θ, 1) when the associated prior π0 is a
Cauchy distribution, C(0, 1). The natural conjugate priors being N(μ, A),
π0 can be approximated by
˜π =
N

i=1
λiπi,
where πi is N(μi, Ai), according to Theorem 3.4.3. Actually, when x goes
to +∞, π0(θ|x) goes to N(x, 1) while ˜π(θ|x) is roughly N(μ(x), ϱ), with
ϱ =
A∗
1 + A∗,
μ(x) = ϱx + (1 −ϱ)μ∗,
A∗= max
i {Ai},
μ∗= max
Ai=A∗μi.
Therefore, π0(θ|x) and ˜π(θ|x) will signiﬁcantly diﬀer for large values of x.
One could argue that such values are not compatible with the prior infor-
mation anyway and should lead to a modiﬁcation of the prior modeling. But
these diﬀerences still show that the prior approximation is not uniformly
valid a posteriori.
∥

3.5
Noninformative prior distributions
127
Example 3.4.4 illustrates rather forcefully the following point: distribu-
tions with heavy tails will not be properly approximated by lighter tail
distributions. This diﬃculty, and more generally, the problem of approxi-
mating the posterior distributions somehow disappears in the generaliza-
tion of Dalal and Hall (1983), who consider continuous mixtures (in the
continuous case). We brieﬂy describe their approach in Note 3.8.3, but
want to point out that their approximation by continuous mixtures misses
the appeal of the previous approximation, since the latter most often calls
for a numerical or Monte Carlo resolution.
3.5 Noninformative prior distributions
The previous section has shown that conjugate priors were useful as ap-
proximations of the true prior distributions. However, when no prior infor-
mation is available, their sole justiﬁcation is analytical, as they can lead to
closed-form expressions for some posterior quantities. In such settings, it
is impossible to justify the choice of prior distributions on a subjective ba-
sis and the hyperparameters of the conjugate prior cannot be determined.
Instead of turning back to classical alternatives, like maximum likelihood
estimation, or to use the data to approximate these hyperparameters, as in
empirical Bayes analysis, it still is preferable to use Bayesian techniques,
if only because they underlie classical optimality criteria (see Chapters 2,
8, and 9). Then, these particular prior distributions must be derived from
the sample distribution, since this is the only available information. For
obvious reasons, they are called noninformative priors. We describe below
some of the most important techniques in the derivation of noninformative
priors, referring the reader to Kass and Wasserman (1996) for an exten-
sive treatment of these notions, and an annotated bibliography. Their main
point is worth reproducing here, before we embark upon this description:
noninformative priors cannot be expected to represent exactly total igno-
rance about the problem at hand, but should rather be taken as reference
or default priors, upon which everyone could fall back when the prior infor-
mation is missing. In this light, some noninformative priors may be more
useful or more eﬃcient than others, but they cannot be said to be less
informative than others.
3.5.1 Laplace’s prior
Historically, Laplace was the ﬁrst to use noninformative techniques, since,
although he had no information about the number of white balls in the
urn or the proportion of male births (Examples 1.2.3 and 1.2.4), he put a
prior distribution on these parameters that took into account his ignorance
and gave the same likelihood to each value of the parameter, i.e., he used
a uniform prior. His reasoning, later called the Principle of Insuﬃcient
Reason, was based on the equiprobability of elementary events and therefore

128
From Prior Information to Prior Distributions
3
appeared to be sound enough.
Three criticisms were later addressed to this choice. First, the resulting
distributions are improper when the parameter space is not compact and
some statisticians object to the use of improper priors, arguing that they
lead to diﬃculties like the marginalization paradoxes (see Exercises 3.44–
3.50). Such misgivings are not really justiﬁed, since it is actually possible
to work with improper priors, as seen in Section 1.5, as long as we do not
try to interpret them as probability distributions (see also Stone (1976)).
As mentioned in Section 3.2, it may be argued that, on the contrary, a sub-
jective determination of the prior distribution should lead to an improper
prior.
Second, the Laplace principle of equiprobable events is not coherent un-
der partitioning: if Θ = {θ1, θ2}, Laplace’s rule gives π(θ1) = π(θ2) = 1/2
but, if the deﬁnition of Θ is reﬁned as Θ = {θ1, ω1, ω2}, Laplace’s rule leads
to π(θ1) = 1/3, which is obviously not coherent with the ﬁrst statement.
As argued in Kass and Wasserman (1996), this incoherence is not such a
serious problem: it can be evacuated by arguing that the level of partition-
ing must be ﬁxed at some point in the analysis and that introducing a ﬁner
degree in the partitioning does not necessarily make sense.
The third criticism is more fundamental, since it deals with the problem
of invariance under reparameterization. If we switch from θ ∈Θ to η = g(θ)
by a one-to-one transformation g, prior information is still totally missing
and should not be modiﬁed. However, if π(θ) = 1, the corresponding prior
distribution on η is π∗(η) =
 d
dηg−1(η)
 by the Jacobian formula. Therefore,
π(η) is usually not constant.
Example 3.5.1
If p, the proportion of male births, has a uniform prior
distribution on [0,1], the odds parameter ϱ =
p
1−p has a prior distribution
with density 1/(1 + ϱ)2 which is therefore not constant.
∥
Of course, it can sometimes be argued that there is a natural parameter of
interest, and therefore that the choice of a uniform prior on the parameter
of interest does not need to be invariant under reparameterization. But this
argument does not hold if more than one inference about θ is necessary;
for instance, we may need to derive the ﬁrst two posterior moments of θ,
but the latter is also the expectation of θ2. Or, in Example 3.5.1, both the
probability θ and the odds ratio ϱ may be of interest. Therefore, it seems
that a more intrinsic and more acceptable notion of noninformative priors
should satisfy invariance under reparameterization.
3.5.2 Invariant priors
A ﬁrst solution is to take advantage of the invariance characteristics of the
problem, that is, to use the groups G acting on X that induce groups G∗
acting on Θ (that is, only the parameters the distribution of x change under

3.5
Noninformative prior distributions
129
transformations of x by elements of G). Chapter 9 details the links between
invariance structures and the Bayesian approach, since these structures
allow for derivation of a noninformative prior that is compatible with the
invariance requirement, namely, the right Haar measure on G∗. See Kass
and Wasserman (1996) for various arguments in favor of the right Haar
measure.
Two introductory examples are presented below.
Example 3.5.2
The family f(x −θ) is translation invariant, i.e., y =
x −x0 has a distribution in the same family for every x0, f(y −(θ −x0));
θ is then said to be a location parameter and an invariance requirement is
that the prior distribution should be translation invariant, i.e., satisfy
π(θ) = π(θ −θ0)
for every θ0. The solution is π(θ) = c, the uniform distribution on Θ.
∥
Example 3.5.3
If the distribution family is parametrized by a scale pa-
rameter, i.e., is of the form 1/σf(x/σ) (σ > 0), it is scale-invariant, i.e.,
y = x/σ ∼f(y). A scale invariant prior π satisﬁes π(A) = π(A/c) for every
measurable set A in (0, +∞) and c > 0, i.e.
π(σ) = 1
cπ(σ
c ).
This implies π(σ) = α/σ, where α is a constant. Therefore, the invariant
measure is not constant anymore.
∥
The invariance approach is only partly satisfactory because it implies the
reference to an invariance structure, which may be chosen in several ways,
may not exist (see Chapter 9), or may be of no interest to the decision
maker.
3.5.3 The Jeﬀreys prior
Jeﬀreys (1946, 1961) proposes an intrinsic approach that indeed obviates
the need to take a potential invariance structure into account, while often
being compatible with it when it exists. The Jeﬀreys noninformative prior
distributions are based on Fisher information, given by
I(θ) = IEθ
∂log f(X | θ)
∂θ
2
in the one-dimensional case. Under some regularity assumptions, this in-
formation can also be written as
I(θ) = −IEθ
∂2 log f(X | θ)
∂θ2

.
(3.5.1)

130
From Prior Information to Prior Distributions
3
The Jeﬀreys prior distribution is
π∗(θ) ∝I1/2(θ),
deﬁned up to a normalization coeﬃcient when π∗is proper. It actually
satisﬁes the invariant reparameterization requirement, since, given a one-
to-one transform h, we have the (Jacobian) transformation
I(θ) = I(h(θ))(h′(θ))2
(which explains the exponent 1/2). Moreover, it also provides the invariant
distributions obtained in Examples 3.5.2 and 3.5.3. More fundamentally,
the choice of a prior depending on Fisher information is justiﬁed by the
fact that I(θ) is widely accepted as an indicator of the amount of informa-
tion brought by the model (or the observation) about θ (Fisher (1956)).
Therefore, at least at a qualitative level, it seems intuitively justiﬁed that
the values of θ for which I(θ) is larger should be more likely for the prior
distribution. In other words, I(θ) can evaluate the ability of the model to
discriminate between θ and θ+dθ through the expected slope of log f(x|θ).
To favor the values of θ for which I(θ) is large is equivalent to minimizing
the inﬂuence of the prior distribution and is therefore as noninformative
as possible. In fact, the Jeﬀreys prior is usually improper but the deve-
lopments of Section 1.5 show how to enhance a Bayesian analysis in this
case.
Example 3.5.4 (Example 3.5.1 continued) If x ∼B(n, p),
f(x|p)
=
n
x

px(1 −p)n−x,
∂2 log f(x|p)
∂p2
=
x
p2 +
n −x
(1 −p)2 ,
and
I(p)
=
n
1
p +
1
1 −p

=
n
p(1 −p).
Therefore, the Jeﬀreys prior for this model is
π∗(p) ∝[p(1 −p)]−1/2
and is thus proper, since it is a Be(1/2, 1/2) distribution.
∥
When θ is a multidimensional parameter, the Fisher information matrix
is deﬁned as a generalization of (3.5.1). For θ ∈IRk, I(θ) has the following
elements,
Iij(θ) = −IEθ

∂2
∂θi∂θj
log f(x|θ)

(i, j = 1, . . . , k),
and the Jeﬀreys noninformative prior is then deﬁned by
π∗(θ) ∝[det(I(θ))]1/2.

3.5
Noninformative prior distributions
131
It is still reparameterization-invariant. Note that, if f(x|θ) belongs to an
exponential family,
f(x|θ) = h(x) exp(θ · x −ψ(θ)),
the Fisher information matrix is given by I(θ) = ∇∇tψ(θ) and
π∗(θ) ∝
! k

i=1
ψ′′
ii(θ)
"1/2
,
(3.5.2)
where ψ′′
ii(θ) =
∂2
∂θ2
i ψ(θ).
In the multidimensional case, the Jeﬀreys noninformative approach may
lead to incoherences or even paradoxes (see Examples 3.5.6 and 3.5.9) and
we must stress that Jeﬀreys (1961) was mainly emphasizing the use of
these distributions in the one-dimensional case (see Berger and Bernardo
(1992b)). However, his method provides one of the best automated tech-
niques to derive noninformative prior distributions. Moreover, it is often
related to some classical estimators.
Example 3.5.5 Consider x ∼N(θ, Ip). Because this is a location family,
the Jeﬀreys prior is constant. The corresponding generalized Bayes estima-
tor is given by
δπ∗(x) =

IRp θ exp(−||x −θ||2/2) dθ

IRp exp(−||x −θ||2/2) dθ = x.
It is minimax for every p and admissible for p ≤2. Notice that this estima-
tor is, in addition, the best equivariant estimator for location parameters
(see Chapter 9).
∥
Example 3.5.6 Consider x ∼N(μ, σ2) with θ = (μ, σ) unknown. In this
case,
I(θ)
=
IEθ

1/σ2
2(x −μ)/σ3
2(x −μ)/σ3
3(μ −x)2/σ4 −1/σ2

=

1/σ2
0
0
2/σ2

and the corresponding noninformative prior distribution is π(θ) ∝1/σ2. If,
however, μ and σ are assumed to be a priori independent, the corresponding
noninformative prior would be π(μ, σ) = σ−1, which is also the invariant
Haar measure for this location-scale model (see Example 3.5.3 and Chapter
9).
∥
This approach is criticized by some Bayesians as being merely a tool
without subjective justiﬁcations in terms of prior information. However,
the only alternative to an automated approach is to require that prior
information be always available, a requirement that cannot hold in ev-
ery setting. Another criticism of the Jeﬀreys method is that, although it

132
From Prior Information to Prior Distributions
3
meets the reparameterization-invariance requirement, it does not satisfy
the Likelihood Principle. In fact, the Fisher information can diﬀer for two
experiments providing proportional likelihoods, as shown by the following
example.
Example 3.5.7
We saw in Example 1.3.4 that binomial and negative
binomial modelings could lead to the same likelihood. However, if x ∼
B(n, θ), the noninformative prior π1(θ) is Be(1/2, 1/2) (Example 3.5.1) and,
if n ∼Neg(x, θ), the Jeﬀreys prior is
π2(θ)
=
−IEθ
 ∂2
∂θ2 log f(x|θ)

=
IEθ
 x
θ2 +
n −x
(1 −θ)2

=
x
θ2(1 −θ),
i.e., π2(θ) ∝θ−1(1 −θ)−1/2, which is improper and, more importantly,
diﬀers from π1.
∥
As shown by the following example, it often occurs that Jeﬀreys’ nonin-
formative distribution is a limit of conjugate priors.
Example 3.5.8
If x ∼U([0, θ]), a conjugate prior is the Pareto distribu-
tion, Pa(θ0, α),
π(θ) = α θα
0 θ−α−1II[θ0,+∞[(θ),
leading to the posterior distribution Pa(max(θ0, x), α + 1). Under the in-
variant loss
L(θ, δ) = (θ −δ)2
θ2
,
the Bayes estimator is, if θ0 ∨x = max(θ0, x),
δπ(x) =
 +∞
θ0∨x θ−1(α + 1) θα+1
0
θ−α−2dθ
 +∞
θ0∨x θ−2(α + 1) θα+1
0
θ−α−2dθ
= α + 3
α + 2 (θ ∨x),
converging to the minimax estimator, δ0(x) = (3/2)x, when α and θ0 go to
0. Because θ is a scale parameter, the noninformative distribution can be
taken to be π(θ) = 1/θ, which is also the Jeﬀreys prior for this model. This
distribution corresponds to θ0 = 0 and α = 0 for an unnormalized Pareto
distribution (that is, without the scaling factor αθα
0 ). This representation
can also be used to show that δ0 is admissible, using Stein’s suﬃcient
admissibility condition (see Chapter 8).
∥
A more important drawback of Jeﬀreys’ noninformative prior distribu-
tions is that they do not necessarily perform satisfactorily for all inferential
purposes, in particular when considering subvectors of interest. The follow-
ing problem was pointed out by Stein (1959) (see also Tibshirani (1989)).
Example 3.5.9
If x ∼Np(θ, Ip), the noninformative prior is π(θ) = 1.
The resulting estimator of θ, x, is quite acceptable, as seen in Example

3.5
Noninformative prior distributions
133
3.5.5. Now, as θ|x ∼Np(x, Ip), the posterior distribution of η = ||θ||2 is
χ2
p(||x||2), the noncentral chi-squared distribution. When η is the parameter
of interest, the posterior mean of η is δπ(x) = IEπ[η|x] = ||x||2+p. However,
the best estimator among the estimators of the form ||x||2+c (for quadratic
loss) is ||x||2 −p, which uniformly dominates the generalized Bayes esti-
mator, δπ (see Exercise 2.35). Therefore, the marginal distribution on η
deduced from the Jeﬀreys noninformative prior on θ is deﬁnitely subopti-
mal. Moreover, the Jeﬀreys noninformative distribution derived from the
reduced observation z = ||x||2 is quite diﬀerent from χ2
p(||x||2) and leads
to an estimator of η with much more acceptable performance (see Exercise
3.52).
∥
Exercise 4.47 also illustrates the possible inconsistency of the Jeﬀreys
prior in a linear calibration setting, with a resolution of the inconsistency
by the reference prior method.
3.5.4 Reference priors
The type of problem mentioned at the end of the previous section was
taken into account by Bernardo (1979), who proposed a modiﬁcation of the
Jeﬀreys approach called the reference prior approach. A major diﬀerence is
that this method distinguishes between parameters of interest and nuisance
parameters (for instance, ||θ||2 and θ/||θ|| in Example 3.5.9). Therefore, the
resulting prior distribution depends not only on the sample distribution,
but also on the inferential problem at hand. The remainder of this section
brieﬂy presents the derivation of reference priors. For a detailed study,
see Berger and Bernardo (1989, 1992a, 1992b) and Kass and Wasserman
(1996).
When x ∼f(x|θ) and θ = (θ1, θ2), where θ1 is the parameter of interest,
the reference prior is obtained by ﬁrst deﬁning π(θ2|θ1) as the Jeﬀreys
prior associated with f(x|θ) when θ1 is ﬁxed, then deriving the marginal
distribution
˜f(x|θ1) =

f(x|θ1, θ2)π(θ2|θ1)dθ2 ,
(3.5.3)
and computing the Jeﬀreys prior π(θ1) associated with ˜f(x|θ1). The prin-
ciple behind the reference prior is therefore to eliminate the nuisance pa-
rameter by using a Jeﬀreys prior where the parameter of interest remains
ﬁxed. (Notice that the integral in (3.5.3) is not necessarily deﬁned, a prob-
lem that may require integrating ﬁrst on a sequence of compact sets and
then taking the limit.)
Example 3.5.10
The Neyman–Scott (1948) problem is related to the
observation of xij’s distributed from N(μi, σ2), i = 1, . . . , n, j = 1, 2. The
usual Jeﬀreys prior for this model is π(μ1, . . . , μn, σ) = σ−n−1 and an

134
From Prior Information to Prior Distributions
3
inconsistency arises because IE[σ2|x11, . . . , xn2] = s2/(2n −2), where
s2 =
n

i=1
(xi1 −xi2)2
2
,
and this posterior expectation converges to σ2/2 with n. (Notice that this is
a case where the number of parameters increases with the number of obser-
vations.) The reference prior associated with θ1 = σ and θ2 = (μ1, . . . , μn)
gives a ﬂat prior for π(θ2|θ1), since θ2 is a location parameter. Then
˜f(x|θ1) =
n

i=1
e−(xi1−xi2)2/4σ2
1
√
2π2σ
is a scale family and π(σ) = 1/σ. Therefore, IE[σ2|x11, . . . , xn2] = s2/(n −
2), which is consistent.
∥
The general derivation of the reference prior is as follows: Consider x ∼
f(x|θ), with θ ∈Θ ⊂IRk. Assume that the Fisher information matrix
I(θ) exists and is of full rank. We denote S = I−1(θ). Parameters are
now separated in m groups that correspond to their respective levels of
importance,
θ(1) = (θ1, . . . , θn1),
. . .
θ(m) = (θNm−1+1, . . . , θk),
(3.5.4)
with Ni = i
j=1 nj (after a possible reindexing of the components of θ).
The reference prior method derives a prior distribution on (θ(1), . . . , θ(m))
that takes into account this division, i.e., which truly separates between
nuisance parameters and parameters of interest, even allowing a ﬁner level
of separation between the respective levels of importance of these parame-
ters. We introduce the following notation: for j = 1, . . . , m,
θ[j] = (θ(1), . . . , θ(j))
and
θ[∼j] = (θ(j+1), . . . , θ(m)).
The matrix S is decomposed according to the partition (3.5.4),
S =
⎛
⎜
⎝
A11
At
21
. . .
At
m1
A21
A22
At
m2
. . .
Am1
Amm
⎞
⎟
⎠
and Sj is the upper left (Nj, Nj) corner of S. (For instance, S1 = A11.) We
denote Hj = S−1
j
and hj is the lower right (nj, nj) corner of Hj. (In partic-
ular, h1 = A−1
11 .) The reference prior construction then proceeds as follows.
– Initialization:
πm(θ(m)|θ[m−1]) =
|hm(θ)|1/2
 |hm(θ)|1/2 dθ(m)
.

3.5
Noninformative prior distributions
135
– Iteration:
For j = m −1, . . . , 1,
πj(θ[∼j−1]|θ[j−1]) = πj+1(θ[∼j]|θ[j]) exp{ 1
2IEj[log(|hj(θ)|)|θ[j]]}

exp{ 1
2IEj[log(|hj(θ)|)|θ[j]]} dθ(j)
,
where
IEj[g(θ)|θ[j]] =

g(θ)πj+1(θ[∼j]|θ[j]) dθ[∼j].
– Conclusion:
The reference prior is π(θ) = π1(θ[∼0]|θ[0]).
Often, some of the integrals appearing in this algorithm are not deﬁned.
Berger and Bernardo (1989) then propose to derive the reference prior for
compact subsets Θn of Θ and to consider the limit of the corresponding
reference priors πn as n goes to inﬁnity and Θn goes to Θ. In general, the
resulting limit does not depend on the choice of the sequence of compact
sets.
Example 3.5.11 (Example 3.5.9 continued) Since η = ||θ||2 is the pa-
rameter of interest, θ can be written in polar coordinates (η, ϕ1, . . . , ϕp−1),
with
θ1
=
√η cos(ϕ1),
θ2
=
√η sin(ϕ1) cos(ϕ2),
. . .
θp−1
=
√η sin(ϕ1) · · · cos(ϕp−1),
θp
=
√η sin(ϕ1) · · · sin(ϕp−1).
The Fisher information matrix for (η, ϕ1, . . . , ϕp−1) is then H = JJt, where
J is the Jacobian matrix
D(θ1,...,θp)
D(η,ϕ1,...,ϕp−1). It can be shown that J is of the
form
J =

At/√η
√ηB

,
with A ∈IRp and B (p −1) × p matrix. Thus, for the partition of θ in
θ(1) = η, θ(2) = (ϕ1, . . . , ϕp−1), we get
π2(ϕ1, . . . , ϕp−1|η) ∝|H22|1/2,
which does not depend on η. The marginal distribution of η is
π1(η) ∝exp

IE

log
1
2
|H|
|H22|
 η

and
|H|
|H22| ∝(1/η). Therefore,
π1(η) = 1/√η,
which leads to an estimator of ||θ||2 more interesting than ||x||2 + p (see
Exercise 3.52).
Actually, the same marginalization problem appears for maximum likeli-
hood estimation. In fact, the maximum likelihood estimator of η based on

136
From Prior Information to Prior Distributions
3
the sample is ||x||2, which is also dominated by ||x||2 −p. On the contrary,
the maximum likelihood estimator derived from
z = ||x||2 ∼χ2
p(||θ||2)
behaves similarly to (||x||2 −p)+ (see Saxena and Alam (1982), Chow
(1987), and Chow and Hwang (1990)).
∥
This algorithm is justiﬁed as providing the prior distribution that maxi-
mizes the posterior information (see Bernardo (1979) and Berger and Ber-
nardo (1992b)). More precisely, if x1:n denotes the sample (x1, . . . , xn) and
if Kn(π) is the Kullback–Leibler divergence between the prior π and the
corresponding posterior,
Kn(π) =

π(θ|x1:n) log (π(θ|x1:n)/π(θ)) dθ,
Bernardo’s (1979) idea is to use IE[Kn(π)], where the expectation is taken
under the marginal distribution of x1:n, as a measure of missing informa-
tion, and he deﬁnes the reference prior as π maximizing
K∗(π) = lim
n→∞IE[Kn(π)] .
Barring the technical diﬃculties associated with possibly inﬁnite integrals,
the resulting prior is the Jeﬀreys prior for continuous parameter spaces
and the uniform prior for ﬁnite spaces. See Ghosh and Mukerjee (1992a),
Clarke and Wasserman (1993) and Kass and Wasserman (1996) for more
motivations in terms of asymptotic optimality.
Reference prior distributions also depend on the way parameters are or-
dered (see Exercise 3.59), an advantage compared with the Jeﬀreys method
because nuisance parameters are considered in a diﬀerent way. Paradoxes
like those of Example 3.5.9 are then avoided. It may seem unreasonable to
modify the prior distributions according to the problem of interest, but one
has to realize that, apart from the sample distribution f(x|θ), these inferen-
tial problems are the unique available information.2 Notice that invariance
under reparameterization is preserved only if the changes are bijective and
within each of the groups in (3.5.4). However, the invariance requirement
is of less importance in this setting because the ordering (3.5.4) somehow
prohibits a reparameterization between the classes, as the diﬀerent groups
are not of the same type. When no such ordering can be proposed, Berger
and Bernardo (1992a) suggest that one consider as a noninformative prior
the reference prior for which each component of θ is considered separately.
(On the contrary, the Jeﬀreys noninformative prior treats θ as a single
group.)
2 If a loss function L is available, it also contains some information about θ and one
could use the duality between loss and prior distribution to derive a prior distribution
adapted to this loss. (See Chapter 2 and Rubin (1987).) But very little work has been
done about the derivation of a prior distribution from a loss function.

3.5
Noninformative prior distributions
137
Example 3.5.12 (Berger and Bernardo (1992a)) Consider an analysis of
variance model
xij = μ + αi + ϵij,
i = 1, . . . , p,
j = 1, . . . , n,
with αi ∼N(0, τ2), ϵij ∼N(0, σ2). For diﬀerent orderings of the parame-
ters, μ, τ2, σ2, we get the following reference priors:
π1((μ, σ2, τ2))
∝
σ−2(nτ 2 + σ2)−3/2
π2(μ, σ2, τ2)
∝
τ−Cnσ2 +
(n −1) + (1 + nτ 2/σ2)−2,1/2
π3(μ, (σ2, τ2))
∝
σ−2(nτ 2 + σ2)−1
π4((μ, σ2), τ2)
∝
σ−5/2(nτ 2 + σ2)−1
with Cn = {1 −√n −1(√n + √n −1)−3}.
∥
3.5.5 Matching priors
A peculiar, not to say paradoxical, approach to noninformative priors is
to look for good frequentist properties, that is, properties that hold on the
average (in x), rather than conditional on x. First, as discussed in Chapters
2 and 8, there are priors that lead to optimal estimators in terms of frequen-
tist optimality properties such as minimaxity or admissibility, and one may
want to restrict the choice of a prior to these optimal distributions. It is,
however, rarely the case that one speciﬁc prior emerges from this require-
ment. Either there is no such prior, as in small dimensions for estimation
under the quadratic loss (Note 2.8.2), or there is an inﬁnity of priors associ-
ated with, say, admissible minimax estimators (Fourdrinier, Strawderman
and Wells (1998)). (The exception here is when invariance structures are
present, in which case the right Haar measure is the appropriate choice, as
discussed in Section 3.5.2.)
A more common approach is to require that some posterior probabili-
ties coincide, to a certain degree of approximation, with the corresponding
frequentist coverage. Hence the denomination of matching priors, which is
often restricted in the literature to one-sided conﬁdence intervals. Given a
posterior conﬁdence set on g(θ), Cx,
π(g(θ) ∈Cx|x) = 1 −α ,
which may be one- or two-sided, this set deﬁnes a conﬁdence set in the
frequentist sense, with a frequentist coverage
Pθ(Cx ∋g(θ)) =

IICx(g(θ)) f(x|θ) dx ,
which is usually diﬀerent from 1 −α. When there are pivotal quantities, as
in the normal N(θ, 1/n) case, the 1 −α HPD region (Chapter 5) is given
by
Cx = [¯xn −n−1/2qα/2, ¯xn + n−1/2qα/2] ,

138
From Prior Information to Prior Distributions
3
where qα/2 is the normal 1 −α/2 quantile and the frequentist coverage of
Cx is also 1 −α. (Lindley (1958) generalizes this result to other location
families and shows that it only holds for location families.) In a general
(unidimensional) framework, Welch and Peers (1963, 1965) have shown
that, when Cx = (−∞, kα(x)],
Pθ(θ ≤kα(x)) = 1 −α + O(n−1/2) ,
and that, for the Jeﬀreys prior,
Pθ(θ ≤kα(x)) = 1 −α + O(n−1) ,
which thus improves the agreement by a factor of 1/2.
In the case there are nuisance parameters in the model, that is, when
inference concentrates on a unidimensional component θ1 of the parame-
ter, things get more complicated. References to works in this area include
Sweeting (1985), Severini (1991), Ghosh and Mukerjee (1992a,b, 1993),
Mukerjee and Dey (1993), DiCiccio and Stern (1993, 1994), Liseo (1993),
and Datta and Ghosh (1995a,b). We focus here on some results obtained
in Rousseau (1997, 2000, 2002, 2005).
The Edgeworth expansion (see Bhattacharya and Rao (1986), Bickel and
Ghosh (1990), and DiCiccio and Stern (1994)) of the frequentist coverage
probability is given by
Pθ(θ1 < kn(α)) = 1 −α +
ϕ(Φ−1(1 −α))
√n
I′(θ)∇log π(θ)
I′′(θ)1/2
−∇t
I′(θ)
I′′(θ)1/2

+ O(n−1) ,
in the one-sided case, where ϕ and Φ are the density and c.d.f. of the normal
distribution, respectively, and I(θ), I′(θ), and I′′(θ) are Fisher information
and its ﬁrst and second derivatives, respectively. For the two-sided case of
an HPD region at level 1 −α, CHP D
x
(α), on θ ∈IR, the corresponding
expansion is
Pθ(θ ∈CHP D
x
) = 1 −α + n−1q(α)b(π, θ) + O(n−3/2) ,
where q is related to the χ2 density and
b(π, θ) = μ′
3 −μ′′
2
I(θ)2
+2μ′
2(μ3 −μ′
2)
I(θ)3
+ π′(θ)
π(θ)
μ3 −μ′
2
I(θ)2
−
π′′(θ)
π(θ)I(θ) −μ′
2π′(θ)
π(θ)I(θ)2 ,
where the μj’s are deﬁned as (j = 2, 3)
μj = IEθ
∂j log f(x|θ)
∂θj

.
The choice of a matching prior is then dictated by the cancellation of the
ﬁrst order term, as in Welch and Peers’ (1963) diﬀerential equation:
[I′′(θ)]−1/2I′(θ)∇log π(θ) + ∇t{I′(θ)[I′′(θ)]−1/2} = 0 .
This diﬀerential equation may or may not have a solution. Moreover, as
shown in the generalization of Rousseau (2002) to HPD regions, the solution,

3.5
Noninformative prior distributions
139
when it exists, will depend on the parameter of interest for those
regions and most often does not produce Jeﬀreys’ prior, although there
always is a parameterization leading to Jeﬀreys’ prior.
Example 3.5.13 (Rousseau (2000))
Consider the G(k, θ) distribution.
Then, if θ itself is the parameter of interest, the priors canceling the second
order term for HPD regions are of the form
π(θ) = c1 + c2θ
θ
,
c1, c2 > 0,
and thus include the Jeﬀreys prior as a particular case. If η = c1θ5/3 +
c2 log(θ) is the quantity of interest, corresponding to the χ2 parameteriza-
tion, the prior with the maximum matching property is
π(η) = I(η)−1 ,
instead of the Jeﬀreys prior, I(η)1/2. Lastly, consider the mean parameter-
ization, μ = k/θ. The matching priors are then of the form
π(μ) = c1μ2 + c2/μ ,
c1, c2 > 0,
and, again, do not include the Jeﬀreys prior.
∥
See also Rousseau (1997) for an extension to discrete settings where
matching the coverage probabilities is not possible for orders higher than
n1/2 and randomization is necessary to achieve such orders.
Example 3.5.14 (Ghosh, Carlin and Srivastava (1995)) A simple version
of the linear calibration model is to consider (i = 1, . . . , n, j = 1, . . . , k),
yi = α + βxi + εi,
y0j = α + βx0 + ε0j ,
(3.5.5)
where x0 is unknown and is the quantity of interest (see Exercise 4.47
for more details on this model). For one-sided conﬁdence intervals, the
diﬀerential equation associated with (3.5.5) is then
|β|−1s−1/2 ∂
∂x0
{e(x0)π(θ)} −e−1/2(x0)sgn(β)n−1s1/2 ∂π(θ)
∂x0
−e−1/2(x0)(x0 −¯x)s−1/2 ∂
∂β {sgn(β)π(θ)} = 0
with θ = (x0, α, β, σ2) and
s = Σ(xi −¯x)2, e(x0) = [(n + k)s + nk(x0 −¯x)2]/nk .
The solutions to this diﬀerential equation are then of the form
π(x0, α, β, σ2) ∝e(x0)(d−1)/2|β|dg(σ2) ,
(3.5.6)
where g is arbitrary. For instance, if g(σ2) = (σ2)−a/2, the corresponding
posterior distribution is proper for (n+k+a−2d−5) > 0. In this case, the
reference priors are also matching priors (3.5.6), as shown by Table 3.5.5
for four diﬀerent orderings of the parameters.
∥

140
From Prior Information to Prior Distributions
3
Table 3.5.1. Matching reference priors associated with diﬀerent orderings for the
linear calibration model (3.5.5).
Partition
Prior
(x0, α, β, σ2)
|β|(σ2)−5/2
x0, α, β, σ2
e(x0)−1/2(σ2)−1
x0, α, (σ2, β)
e(x0)−1/2(σ2)−3/2
x0, (α, β), σ2
e(x0)−1/2(σ2)−1
x0, (α, β, σ2)
e(x0)−1/2(σ2)−2
In general, (reverse) reference priors are matching priors when the pa-
rameter of interest, λ, and the nuisance parameter, ω, are orthogonal in
the sense of Fisher information
I(λ, η) =
 I11
0
0
I22

,
as detailed in Tibshirani (1989), and also when the reverse order (ω, λ)
is used for the construction of the reference prior as discussed in Berger
(1992) and Berger et al. (1998).
Besides the technical diﬃculty one faces in handling matching priors,
there is also a conceptual diﬃculty in asking for frequentist coverage when
constructing a prior distribution, whose goal is to condition on the obser-
vation rather than to rely on frequentist long-term properties. Agreement
between both approaches should not be systematically rejected, as shown
in Chapter 5, but this shift of paradigm is fairly disturbing, as shown for
instance in Rousseau (1997) by the need to call for randomization, in vio-
lation of the Likelihood Principle. Therefore, we do not recommend it.
3.5.6 Other approaches
Alternatives to noninformative Bayesian analysis are described in Berger
(1985a, Chapter 3) and Kass and Wasserman (1996). For instance, we can
mention Rissanen (1983, 1990), who uses transmission information theory
as in Shannon (1948). Considering the transmission of a binary message by
a physical device, the noninformative prior distribution for a model f(x|θ)
is the minimum length of message necessary to describe this model. In the
simplest case, these distributions are similar to the Jeﬀreys priors and this
similarity should also hold in general because of the connections existing
between statistical information and information theory. A recent survey
of this theory of stochastic complexity is given in Dawid (1992). See also
Hansen and Yu (2001).
Notice also that testing settings require special prior distributions, as
pointed out by Jeﬀreys (1961) and Kass and Wasserman (1996). We will

3.6
Posterior validation and robustness
141
discuss this speciﬁc problem in Chapter 5.
3.6 Posterior validation and robustness
Even when prior information is available, it rarely occurs that this in-
formation leads to an exact determination of the prior distribution, π(θ),
if only because the discriminating power of individuals is restricted and
practical determination of the tails of a distribution is almost impossible.
In most cases, there is therefore an uncertainty about the selected prior
distribution used for Bayesian inference. Obviously, if the information is
precise, the prior will be better deﬁned than in a noninformative setting.
However, it always matters that the inﬂuence of this indeterminacy in the
prior distribution on posterior quantities is clearly assessed and that the
arbitrary part of the prior distribution does not predominate. The assess-
ment of the inﬂuence of the prior is called sensitivity analysis (or robustness
analysis). The concern about robustness and the derivation of appropriate
tools to deal with this problem appear in the works of Good (1983) and
Berger (1982b, 1984a, 1985a, 1990a). Other references on this problem are
Berger and Berliner (1986), Berger and Sellke (1987), Berger and Delam-
pady (1987), O’Hagan and Berger (1988), Sivaganesan and Berger (1989),
Walley (1991), and Wasserman (1992).
Following Berger’s (1990a) classiﬁcation, we consider that uncertainty
about the prior distribution π can be represented by the assumption that
the (unknown) prior belongs to a class of distributions, Γ. These classes
can be determined from a subjective or practical perspective. The major
types of robustness classes considered in the literature follow.
(i) Conjugate prior classes. Such classes are typically chosen for practical
reasons, since they generally provide explicit bounds on the quantity
of interest. For instance, Das Gupta and Studden (1988) consider the
case when x ∼Np(θ, Ip) and θ ∼Np(0, Σ), where Σ1 ⪯Σ ⪯Σ2, the
order relation ⪯being that the diﬀerence of the two matrices is posi-
tive semideﬁnite. The above criticisms on conjugate prior distributions
obviously apply in this case and even more strongly, since the resulting
class is only made of convenient distributions, but does not lead to a
wide set of prior distributions compatible with the prior information.
(ii) Determined moment classes. If we assume that the (limited) prior in-
formation is only providing bounds on some moments of π, the corre-
sponding class is
ΓM = {π; ai ≤IEπ[θi] ≤bi, i = 1, . . . , k}.
However, ΓM is not really more satisfactory than the previous classes
in (i), because it imposes strong conditions on the tails of the prior and
contains unrealistic prior distributions, including ﬁnite support distri-
butions.More speciﬁcally, in most cases, the bounds on the posterior

142
From Prior Information to Prior Distributions
3
quantities will be attained for ﬁnite support distributions, owing to
convexity reasons.
(iii) Neighborhood classes. Following its introduction by Huber (1972) for
outliers detection, a rather popular class in robustness studies is the
ϵ-contamination class around a given prior distribution π0,
Γϵ,Q = {π = (1 −ϵ)π0 + ϵq; q ∈Q},
where Q is a class of distributions, to be chosen according to the
precision of the prior information. Berger and Berliner (1986) and
Sivaganesan and Berger (1989) provide examples where such classes
can be used. The main drawback in using Γϵ,Q is that ϵ and Q both
need to be determined and it is usually diﬃcult to derive them from
the degree of uncertainty about π0. But mixture estimation techniques
may be instrumental in this setting when the prior information is de-
rived from a sample of (possibly virtual) previous observations (see
Section 6.4). A related relation would be to consider a true neigh-
borhood associated with a given functional distance like Hellinger or
Kullback–Leibler distances (see Section 2.5.4 and Zucchini (1999)).
The diﬃculty is then in the scaling of these neighborhoods.
(iv) Underspeciﬁed classes. Such classes result from a construction of the
prior on a sub-σ-algebra, that is, on a coarser set of events than the
one of interest. This approach is strongly related to the axiomatic de-
velopments of Note 3.8.1, since the ordering on the relative likelihoods
does not necessarily lead to a prior distribution on the Borel σ-ﬁeld.
For instance, it may be the case that the prior distributions have some
determined quantiles,
ΓQ = {π; ℓi ≤

Ii
π(θ) dθ ≤ui, i = 1, . . . , m}
where I1, . . . , Im is a partition of Θ. These classes are preferable to (ii),
but it may still be necessary to eliminate unrealistic prior distributions
from ΓQ as in O’Hagan and Berger (1988). However, it seems that
this approach is the most realistic because, for instance, fractiles are
usually easier to determine than moments, and the most apt to give
rise to practical implementation among the classes we consider here.
(v) Ratio of densities classes. Considering as for (iv) a subjective deriva-
tion of the prior distribution, this may also be done by a histogram
representation. In such a case, the uncertainty about the prior infor-
mation can be represented by upper and lower bounds on the density
π and leads to the class
ΓR = {π; L(θ) ≤π(θ) ≤U(θ)},
where L and U are speciﬁed. The choice of these functions is diﬃcult
but is quite inﬂuential because, if they are similar, all the distributions
in ΓR will have the same behavior in the tails. See DeRobertis and
Hartigan (1981) for a related class.

3.6
Posterior validation and robustness
143
Berger (1990a) and Wasserman (1992) also provide computational tools
to derive bounds on posterior quantities for the above classes. In fact, the
robustness point of view substitutes for the current estimator ϱ(π) the range
of possible values of the estimator when the prior π varies in the class Γ,
ϱL = inf
π∈Γ ϱ(π),
ϱU = sup
π∈Γ
ϱ(π).
Goutis (1990, 1994) (see Example 3.2.7) illustrates this approach for class
(ii). Chapter 5 presents such a study in order to derive conservative bounds
on posterior probabilities of a null hypothesis.
A more conservative approach to robustness requirements is to look for
robust prior distributions, i.e., for parametrized distributions as insensitive
as possible to small variations in the prior information. For instance, it can
be shown that Student’s t-distributions are preferable to normal priors in
the normal case, although the latter distributions are conjugate and so are
maximum entropy priors in some cases (see Zellner (1971), Angers (1987),
and Angers and MacGibbon (1990)). Similarly, poly-t distributions, derived
from the product of several Student’s t densities, are used in the econo-
metric analysis of simultaneous equations for the same reason (see Dr`eze
and Morales (1976), Richard and Tompa (1980), and Bauwens (1984)). In
general, and unlike conjugate distributions, these robust priors will have
heavy tails.
Another approach robustiﬁes the conjugate distributions by hierarchi-
cal modeling. The hierarchical Bayes approach is presented in Chapter 10,
but it seems quite intuitive at this stage that an additional level in the
prior modeling should increase the robustness of the prior distribution.
Consider a conjugate prior for f(x|θ), π1(θ|λ). As mentioned above, classes
like (i) are not very eﬃcient in terms of robustness and, moreover, require
the speciﬁcation of bounds on the hyperparameters λ. Because these hy-
perparameters are (partly or totally) unknown, a natural extension (in a
Bayesian framework) is to introduce a noninformative prior on λ, π2 (or
a hyperprior compatible with the available information). This modeling
induces the following hierarchical structure:
λ
∼
π2(λ),
θ|λ
∼
π1(θ|λ),
x|θ
∼
f(x|θ).
The prior distribution on θ is then the marginal distribution derived from
π1(θ|λ)π2(λ), i.e., by integrating λ out,
π(θ) =

π1(θ|λ)π2(λ)dλ.
(3.6.1)
This prior distribution is not conjugate in general, but the main purpose of
the hierarchical extension was actually to avoid the restrictive framework
of conjugate priors. By integrating out the hyperparameters λ, we derive a
distribution (3.6.1) that usually enjoys heavier tails than conjugate priors.

144
From Prior Information to Prior Distributions
3
For instance, the Student’s t-distribution can be written as (3.6.1) with π2
an inverse gamma distribution (see Example 3.3.11). Hierarchical settings
are also quite interesting from a computational point of view, as shown in
Chapter 6.
Other perspectives incorporate the loss function in the robustness study
in order to select an estimator that is conservative with respect to all pos-
sible priors π ∈Γ. For instance, δ∗can be the solution of
inf
δ sup
π∈Γ
r(π, δ)
or
inf
δ sup
π∈Γ
[r(π, δ) −r(π, δπ)],
the ﬁrst quantity being the Γ-minimax risk and the second quantity the
Γ-minimax regret, as developed in Robbins (1951) and Good (1952). See
Berger and Berliner (1986), Berger (1985a), and Kempthorne (1988) for
additional references.
The literature on Bayesian robustness has increased considerably in the
last few years and we refer the reader to the papers mentioned above for
additional references. To conclude this chapter, let us point out that the
choice of the prior distribution determines the resulting Bayesian inference,
that this choice may sometimes be trivial and sometimes quite delicate, but
that it should be justiﬁed in term of the available prior information in all
cases and, moreover, that a robustness analysis should be conducted in
order to assess the amount of posterior modiﬁcation a change in the prior
distribution induces. Obviously, this assessment will also depend on the
evaluation measures considered for the quantities of interest, as for instance
on the losses used in the estimation process. This leads to the possibility
of using the knowledge of the loss function to determine noninformative
prior distributions, but little work has been done in this direction, even
though many Bayesians point out that loss and prior are indistinguishable
(see, e.g., Lindley (1985) and Exercise 3.57.) A ﬁnal warning to the reader
is that the prior inﬂuence is often underestimated by users, while it may
have unexpected inﬂuence on the resulting inference. Therefore, whenever
possible, other values of the hyperparameters, but also other types of dis-
tributions, should be used to assess the real eﬀect of the prior choice on
the resulting inference.
3.7 Exercises
Section 3.1
3.1 (Dupuis (1995)) Recall that the beta Be(α, β) distribution has a density
given by
π(θ) = Γ(α + β)
Γ(α)Γ(β)θα−1(1 −θ)β,
0 ≤θ ≤1 .
a. Give the mean of the Be(α, β) distribution.
b. Show that there a one-to-one correspondence between (α, β) and the triplet
(μ, θ0, θ1), where π(θ ∈[θ0, θ1]) = p and μ is the mean of the distribution.
c. What are the conditions on (μ, θ0, θ1) for (α, β) to exist?

3.7
Exercises
145
Section 3.2.3
3.2 (Seidenfeld (1987)) Consider a six-sided die, with θ the random variable
corresponding to the upper face of the die.
a. If π is the distribution of θ, give the maximum entropy prior associated
with the information IE[θ] = 3.5.
b. Show that, if A is the event “θ is odd”, the updated distribution π(·|A) is
(1/3, 0, 1/3, 0, 1/3, 0).
c. Show that the maximum entropy prior associated with the constraints
IE[θ] = 3.5 and IE[IIA] = 1 is (.22, 0, .32, 0, .47).
[Note: Seidenfeld (1987) and Kass and Wasserman (1996) use this example to
show that the maximum entropy approach is not always compatible with the
Bayesian updating principle given in (1.4.1).]
3.3 Show that, if the constraints (3.2.1) are all associated with functions gk of
the form gk(θ) = II(−∞,ak](θ), there is no maximum entropy prior distribution
for Θ = IR and π0 the Lebesgue measure on IR.
3.4 Consider θ ∈IR and a prior π such that varπ(θ) = 1, π(θ < −1) = 0.1,
and π(θ > 1) = 0.1. Derive the maximum entropy prior associated with the
Lebesgue measure on IR if this is possible.
3.5 Let π0 be a reference measure for the maximum entropy method and π′
0 a
measure which is absolutely continuous with respect to π0.
a. Give examples where the maximum entropy priors associated with π0 and
π′
0 coincide.
b. Apply this to the case when π0 is the Lebesgue measure on IR, π′
0 is the
N(0, 1) distribution, and the constraints (3.2.1) are IEπ[θ] = 0, varπ(θ) =
σ2, depending on the value of σ.
3.6 Consider θ ∈IR+. Determine whether there exists a maximum entropy prior
under the constraint IEπ[θ] = μ for π0(θ) = 1 and π0(θ) = 1/θ.
3.7 Let x ∼P(λ).
a. Find the maximum entropy prior associated with π0(θ) = 1/
√
θ and
IEπ[θ] = 2.
b. Determine the hyperparameters of the prior distribution π if π is
(i) Exp(μ);
(ii) G(2, ϱ).
c. Derive the three corresponding posterior distributions when x = 3 and
compare the Bayes estimators of θ under the loss L(θ, δ) = θ(θ −δ)2.
Section 3.2.4
3.8 Determine the prior distributions in Example 3.2.6, when the ﬁrst and third
quartiles are 2 and −2 and the median is 0.
3.9 Let x ∼B(n, θ) and θ ∼Be(α, β). Determine whether there exist values of
α, β such that π(θ|x) is the uniform prior on [0, 1], even for a single value of x.

146
From Prior Information to Prior Distributions
3
3.10 Let x ∼Pa(α, θ), a Pareto distribution, and θ ∼Be(μ, ν). Show that, if
α < 1 and x > 1, a particular choice of μ and ν gives π(θ|x) as the uniform
prior on [0, 1].
Section 3.3.1
3.11 If π is a ﬁnite mixture of conjugate distributions, give the form of π(θ|x). In
particular, derive the posterior weights. Deduce the results of Example 3.4.1.
3.12 Determine symmetric distributions, i.e., distributions such that conjugate
distributions and sampling distributions belong to the same parametrized fam-
ily.
3.13 This exercise shows why the notion of a minimal conjugate family is usually
vacuous.
a. Using the notations of Proposition 3.3.13, show that the set of λ’s in
π(θ|μ, λ) can be restricted to vary in λ0 + IN for any λ0 > 0.
b. Deduce that, if λ0 −λ′
0 ̸∈ZZ, the conjugate families associated with λ0 + IN
and λ′
0 + IN are disjoint.
c. Conclude that the intersection of all conjugate families is empty.
3.14 Consider a population divided into k categories (or cells) with probability
pi for an individual to belong to the ith cell (1 ≤i ≤n). A sequence (πk)
of prior distributions on pk = (p1, . . . , pk), k ∈IN, is called coherent if any
grouping of cells into m categories leads to the prior πm for the transformed
probabilities.
a. Determine coherence conditions on the sequence (πk).
b. In the particular case when πk is a Dirichlet distribution Dk(α1, . . . , αk),
express these conditions in terms of the αk’s.
c. Does the Jeﬀreys prior induce a coherent sequence?
d. What about πk(pk) ∝.
i p−1/k
i
, proposed by Perk (1947)?
Section 3.3.3
3.15 Show that every distribution from an exponential family can be generalized
into a pseudo-exponential family by adding parametrized constraints on the
support of x. Elaborate on the modiﬁcation in the suﬃcient statistics.
3.16 Show that, if the support of f(x|θ) does not depend on θ and if there exists a
parametrized conjugate prior family F = {π(θ|λ), λ ∈Λ} with dim(Λ) < +∞,
f(x|λ) is necessarily from an exponential family. (Hint: This is a consequence
of the Pitman–Koopman lemma.)
3.17 Give a suﬃcient statistic associated with a sample x1, . . . , xn from a Pareto
Pa(α, θ) distribution.
3.18 Give a suﬃcient statistic associated with a sample x1, . . . , xn from a trun-
cated normal distribution
f(x|θ) ∝e−(x−θ)2/2II[θ−c,θ+c](x),
when c is known.
3.19 *(Brown (1986)) Show that, for every exponential family, there exists a
reparameterization which gives a natural exponential family. Show also that
the dimension of a natural reparameterization does not depend on the choice
of the reparameterization.

3.7
Exercises
147
3.20 *(Dynkin (1951))
Show that the normal distributions and distributions
of the form c log(y), when y ∼G(α, β), are the only ones which can belong
simultaneously to an exponential family and a location family. Deduce that
the normal distribution is the only distribution from an exponential family
that is also spherically symmetric (see Exercise 1.1).
3.21 *(Lauritzen (1996)) Consider X = (xij) and Σ = (σij) symmetric positive-
deﬁnite m × m matrices. The Wishart distribution, Wm(α, Σ), is deﬁned by
the density
pα,Σ(X) = |X|
α−(m+1)
2
exp(−tr(Σ−1X)/2)
Γm(α)|Σ|α/2
,
with tr(A) the trace of A and
Γm(α) = 2αm/2πm(m−1)/4
m

i=1
Γ
α −i + 1
2

.
a. Show that this distribution belongs to an exponential family. Give its nat-
ural representation and derive the mean of Wm(α, Σ).
b. Show that, if z1, . . . , zn ∼Nm(0, Σ),
n

i=1
ziz′
i ∼Wm(n, Σ) .
c. Show that the moments are given by
IE[X|α, Σ] = αΣ,
Cov(X) = 2αΣ ⊗Σ .
d. Show that the mean of the inverse X−1 is given by
IE[X−1|α, Σ] =
1
α −p −1Σ,
α > p + 1 .
3.22 *(Pitman (1936)) Show the Pitman–Koopman lemma: If, for n ≥n0, there
exists Tn from IRn in IRk such that Tn(x1, . . . , xn) is suﬃcient when x1, . . . , xn
are i.i.d. f(x|θ), the distribution f necessarily belongs to an exponential family
if the support of f does not depend on θ. Study the case when the support of
f depends on θ.
3.23 *(Brown (1986)) A natural exponential family f(x|θ) = exp(θ · x −ψ(θ))
is said to be steep if, for every θ0 ∈
◦
N, the interior set of N, θ1 ∈N/
◦
N, and
θϱ = ϱθ1 + (1 −ϱ)θ0, it satisﬁes
lim
ϱ→1
∂ψ
∂ϱ (θϱ) = +∞.
a. Show that the family is steep if and only if
IEθ[||x||] = +∞
for every θ ∈N/
◦
N.
b. Show that the Inverse Gaussian distribution, with density
(π)−1/2z−3/2 exp{θ1z + θ2(1/z) −(2θ1θ2)1/2 + (1/2) log(−2θ2)}
where z ∈IR+ and θ1, θ2 ∈IR−, is exponential and steep but not regular.

148
From Prior Information to Prior Distributions
3
c. Show that a minimal and steep exponential family can be re parametrized
by ξ(θ) = IEθ[x] = ∇ψ(θ) and that this function deﬁnes a one-to-one
transformation from
◦
N to
◦
K.
d. Show that, for minimal and steep exponential families, the maximum like-
lihood estimator of θ, ˆθ(x), satisﬁes
ξ(ˆθ(x)) = x.
3.24 *(Morris (1982))
A restricted natural exponential family on IR is deﬁned
by
Pθ(x ∈A) =

A
exp{θx −ψ(θ)} dF(x),
θ ∈Θ.
(3.7.1)
a. Show that, if 0 ∈Θ, F is necessarily a cumulative distribution function.
Otherwise, show that the transformation of F into
dF0(x) = exp{θ0x −ψ(θ)} dF(x),
for an arbitrary θ0 ∈Θ and the replacement of θ by θ −θ0, provides this
case.
b. Show that, in this restricted sense, Be(mμ, m(1 −μ)) and the lognormal
distribution LogN(α, σ2) do not belong to an exponential family.
c. If μ = ψ′(θ) is the mean of the distribution (3.7.1), the variance function
of the distribution is deﬁned by V (μ) = ψ′′(θ) = varθ(x). Show that V is
indeed a function of μ and, moreover, that if the variation space of μ, Ω, is
known, the couple (V, Ω) completely characterizes the family (3.7.1) by
ψ
 μ
μ0
dm
V (m)

=
 μ
μ0
m dm
V (m).
(Notice that θ =  μ
μ0 dm/V (m).) Show that V (μ) = μ2 deﬁnes two families,
depending on whether Ω = IR−or Ω = IR+.
d. Show that V (μ) = μ(1 −μ)/(m + 1) corresponds simultaneously to the
binomial distribution B(m, μ) and to Be(mμ, m(1 −μ)). Deduce that the
characterization by V is only valid for natural exponential families.
e. Show that exponential families with quadratic variance functions, i.e.,
V (μ) = v0 + v1μ + v2μ2,
(3.7.2)
include the following distributions: normal, N(μ, σ2), Poisson, P(μ), gamma,
G(r, μ/r), binomial, B(m, mμ), and negative binomial, Neg(r, p), deﬁned in
terms of the number of successes before the rth failure, with μ = rp/(1−p).
f. Show that the normal distribution (respectively, the Poisson distribution)
is the unique natural exponential distribution with a constant (respectively,
of degree one) variance function.
g. Assume v2 ̸= 0 in (3.7.2) and deﬁne d = v2
1 −4v0v2, discriminant of (3.7.2),
a = 1 if d = 0 and a = √dv2 otherwise. Show that x∗= aV ′(x) is a linear
transformation of x with the variance function
V ∗(μ∗) = s + v2(μ∗)2,
(3.7.3)
where μ∗= aV ′(μ) and s = −sign(dv2). Show that it is suﬃcient to con-
sider V ∗to characterize natural exponential families with a quadratic vari-
ance function, in the sense that other families are obtained by inverting the
linear transform.

3.7
Exercises
149
h. Show that (3.7.3) corresponds to six possible cases depending on the sign
of v2 and the value of s (−1, 0, 1). Eliminate the two impossible cases and
identify the families given in e, above. Show that the remaining case is v2 >
0, s = 1. For v2 = 1, show that this case corresponds to the distribution of
x = log{y/(1 −y)}/π, where
y ∼Be
1
2 + θ
π , 1
2 −θ
π

,
|θ| < π
2 ,
and
f(x|θ) = exp[θx + log(cos(θ))]
2 cosh(πx/2)
.
(3.7.4)
(The reﬂection formula B(0.5 + t, 0.5 −t) = π/ cos(πt) can be of use.) The
distributions spanned by the linear transformations of (3.7.4) are called
GHS(r, λ) (meaning generalized hyperbolic secant), with λ = tan(θ), r =
1/v2, and μ = rλ. Show that the density of GHS(r, λ) can be written
fr,λ(x) = 	
1 + λ2
−r/2 exp{x arctan(λ)}fr,0(x)
(do not try to derive an explicit expression for fr,0).
[Note: Exercise 10.33 exhibits additional properties of the quadratic variance
exponential families in terms of conjugate families and Bayes estimators. Exer-
cise 6.2.6 shows how orthogonal polynomials can be related to each distribution
in the quadratic variance exponential families.]
3.25 Compare usual exponential families with the distributions (2.6.1) obtained
in Chapter 2 and check whether they give universal estimators.
3.26 Show that, for every exponential family, the natural space N is convex.
3.27 Show the decomposition of Example 3.3.11
(i) directly; and
(ii) through a usual representation of Student’s t-distribution.
3.28 An alternative to the logistic regression introduced in Example 3.3.15 is
the probit model, where
Pα(yi = 1) = 1 −Pα(yi = 0) = Φ(αtxi),
i = 1, . . . , n,
and Φ is the c.d.f. of the standard normal distribution.
a. Show that this alternative does not belong to an exponential family, even
conditional upon the xi’s.
b. The observation yi can be considered as the indicator function IIzi≤αtxi
where zi is an unobserved N(0, 1) random variable. Show that, if the zi’s are
known, the Lebesgue measure provides an explicit posterior distribution.
[Note: The interesting aspect of this remark will be made clearer in Chapter
6, since the missing data z1, . . . , zn can be simulated.]
Section 3.3.4
3.29 For an arbitrary exponential family distribution, determine the constraints
such that a maximum entropy prior is also a conjugate prior.
3.30 A classical linear regression can be written as y ∼Np(Xβ, σ2Ip) with X a
p×q matrix and β ∈IRq. When X is known, give the natural parameterization
of this exponential family and derive the conjugate priors on (β, σ2). Generalize
to Np(Xβ, Σ) with Σ known.

150
From Prior Information to Prior Distributions
3
3.31 Consider x ∼N(θ, θ) with θ > 0.
a. Determine the Jeﬀreys prior πJ(θ).
b. Say whether the distribution of x belongs to an exponential family and
derive the conjugate priors on θ.
c. Use Proposition 3.3.14 to relate the hyperparameters of the conjugate priors
with the mean of θ.
3.32 Show that, if x ∼Be(θ1, θ2), there exist conjugate priors on θ = (θ1, θ2)
but that they do not lead to tractable posterior quantities, except for the
computation of IEπ[θ1/(θ1 + θ2)|x], according to Proposition 3.3.14.
3.33 *(Robert (1991)) The generalized inverse normal distribution IN (α, μ, τ)
has the density
K(α, μ, τ)|θ|−α exp

−(1
θ −μ)2/2τ 2 
,
with α > 0, μ ∈IR, and τ > 0.
a. Show that this density is well deﬁned and that the normalizing factor is
K(α, μ, τ)−1 = τ α−1e−μ2/2τ22(α−1)/2 Γ(α −1
2
) 1F1

α −1
2
; 1/2; μ2
2τ 2

,
where 1F1 is the conﬂuent hypergeometric function (see Abramowitz and
Stegun (1964)).
b. Show that this distribution generalizes the distribution of y = 1/x when
x ∼N(μ, τ 2). Check that the above normalizing constant is correct in this
particular case.
c. Deduce that the mean of IN(α, μ, τ) is deﬁned for α > 2 and is
IEα,μ,τ[θ] = μ
τ 2
1F1( α−1
2 ; 3/2; μ2/2τ 2)
1F1( α−1
2 ; 1/2; μ2/2τ 2).
d. Show that these distributions IN (α, μ, τ) constitute a conjugate family for
the multiplicative model N(θ, θ2).
3.34 Show that a Student’s t-distribution Tp(ν, θ, τ 2) does not allow for a con-
jugate family, apart from the trivial family F0.
3.35 Proposition 3.3.13 exhibits a conjugate family for every exponential family,
of the form (3.3.4),
π(θ|λ, μ) = exp{θ · μ −λψ(θ)}K(μ, λ).
a. Show that the distribution (3.3.4) is actually well deﬁned when λ > 0 and
(μ/λ) ∈
◦
N.
b. Give the constant K for normal, gamma, and negative binomial distribu-
tions.
c. Deduce that the likelihood function ℓ(θ|x) is a particular prior distribution
for exponential families (by mean of a reparameterization) and give the
corresponding prior for the above families.
d. Is this property characterizing exponential families? Give a counter-example.
3.36 *Show Proposition 3.3.14 and its reciprocal in the continuous case. Apply
to the distributions in Table 3.3.4.

3.7
Exercises
151
3.37 Show that the distributions in Table 3.3.4 are actually conjugate
(i) directly; and
(ii) through Proposition 3.3.14.
3.38 Consider x ∼G(θ, β), i.e., fβ(x|θ) =
βθ
Γ(θ)xθ−1e−βx.
a. Can you derive a conjugate family for this distribution?
b. Consider the case where θ ∈IN.
c. Same question for x ∼Be(1, θ).
3.39 Show that, for exponential families, a multiplication of the number of hi-
erarchical levels does not modify the conjugate nature of the resulting prior if
conjugate distributions with ﬁxed scale parameters are used at every level of
the hierarchy. (Consider, for instance, the normal case.)
3.40 *(Robert (1993a)) Consider f(x|θ) from an exponential family,
f(x|θ) = eθ·x−ψ(θ)h(x),
x ∈IRk,
and π0(θ|x0, λ) a conjugate prior distribution,
π0(θ|x0, λ) = eθ·x0−λψ(θ).
We are looking for a so-called objective estimation of ∇ψ(θ), based on an arbi-
trary prior distribution π0(θ|x0, λ). To this eﬀect, we replace the distribution
π0 by π1(θ|x1, λ) deﬁned by the relation
IEπ1[∇ψ(θ)] = IEπ0[∇ψ(θ)|x],
(3.7.5)
in order to reduce the inﬂuence of x0.
a. Deduce the relation between x1 and x0.
b. We iterate the updating process (3.7.5) in order to eliminate, as much
as possible, the inﬂuence of x0 and we construct in this way a sequence
πn(θ|xn, λ) of conjugate priors. Give the relation between xn and xn−1 and
deduce the limit of the sequence (xn).
c. Give the corresponding limit of the Bayes estimators of ∇ψ(θ). How do you
characterize the resulting estimator? Is it still a Bayes estimator?
d. In the particular case when x ∼N(θ, 1), the parameter of interest is h(θ) =
e−θ. Give the estimator h(θ) obtained this way using the iterative updating
IEπn[h(θ)] = IEπn−1[h(θ)|x].
e. Consider the case x ∼G(α, θ) and h(θ) = θk to show that this iterative
method, called Prior Feedback, does not always converge to the maximum
likelihood estimator.
f. Show that the limit of the Prior Feedback estimate when λ goes to +∞is
the maximum likelihood estimator of h(θ), for an arbitrary function h and
every exponential family.
Section 3.4
3.41 In the setting of Example 3.4.1, build up a prior distribution by observing
a few coins and imposing a mixture of beta distributions as in Diaconis and
Ylvisaker (1985). Select one of these coins and derive a posterior distribution
on θ, probability of heads, after 10 trials and 50 trials.

152
From Prior Information to Prior Distributions
3
3.42 Consider x ∼N(0, 1) and θ ∼T1(5, 0, 1).
a. Devise a method to approximate this prior distribution by a mixture of: (i)
two normal distributions; and (ii) ﬁve normal distributions.
b. In each case, give the approximating posterior expectation of θ when x = 1
and compare it with the exact value.
Section 3.5.1
3.43 Consider x1, . . . , xn ∼N(μ + ν, σ2), with π(μ, ν, σ) ∝1/σ.
a. Show that the posterior distribution is not deﬁned for every n.
b. Extend this result to overparametrized models with improper priors.
The following exercises (3.44–3.50) present the marginalization paradox through
several examples and show that it can only occur for improper priors. Dawid
et al. (1973), Stone (1976), and Jaynes (1980) give some partial resolutions
of these paradoxes. Notice that a fundamental explanation is that the improper
distribution π(dη, dθ) = π(η) dη dθ does not induce the pseudo-marginal dis-
tribution π(dη) = π(η) dη.
3.44 *(Dawid et al. (1973)) Consider n random variables x1, . . . , xn, such that
the ﬁrst ξ of these variables has an Exp(η) distribution and the n−ξ other have
a Exp(cη) distribution, where c is known and ξ takes its values in {1, 2, . . . ,
n −1}.
a. Give the shape of the posterior distribution of ξ when π(ξ, η) = π(ξ) and
show that it only depends on z = (z2, . . . , zn), with zi = xi/x1.
b. Show that the distribution of z, f(z|ξ), only depends on ξ.
c. Show that the posterior distribution π(ξ|x) cannot be written as a posterior
distribution for z ∼f(z|ξ), whatever π(ξ), although it only depends on z.
How do you explain this?
d. Show that the paradox does not occur when π(ξ, η) = π(ξ)η−1.
3.45 *(Dawid et al. (1973)) Consider u1, u2, s2 such that
u1 ∼N(μ1, σ2),
u2 ∼N(μ2, σ2),
s2 ∼σ2χ2
ν/ν,
and ζ = (μ1 −μ2)/(σ
√
2) is the parameter of interest. The prior distribution is
π(μ1, μ2, σ) = 1
σ .
a. Show that the posterior distribution π(ζ|x) only depends on
z = u1 −u2
s
√
2
.
b. Show that the distribution of z only depends on ζ, but that a paradox
occurs; it is still impossible to derive π(ζ|x) from f(z|ζ), even though π(ζ|x)
only depends on z.
c. Show that the paradox does not occur when
π(μ1, μ2, σ) = 1
σ2 .

3.7
Exercises
153
3.46 *(Dawid et al. (1973)) Consider
x11, . . . , x1n
∼
N(μ1, σ2),
x21, . . . , x2n
∼
N(μ2, σ2),
2n independent random variables.
a. The parameter of interest is ξ = (ξ1, ξ2) = (μ1/σ, μ2/σ) and the prior
distribution is
π(μ1, μ2, σ) = σ−p.
Show that π(ξ|x) only depends on z = (z1, z2) = (¯x1/s, ¯x2/s) and that the
distribution of z only depends on ξ. Derive the value of p that avoids the
paradox.
b. The parameter of interest is now ζ = ξ1. Show that π(ζ|x) only depends
on z1 and that f(z1|ξ) only depends on ζ. Give the value of p that avoids
the paradox.
c. Consider the previous questions when σ ∼Pa(α, σ0).
3.47 *(Dawid et al. (1973)) Consider (x1, x2) with the following distribution:
f(x1, x2|θ) ∝
 +∞
0
t2n−1 exp
/
−1
2

t2 + n(x1t −ζ)2 + n(x2t −ξ)20
dt,
with θ = (ζ, ξ). Justify this distribution by considering the setting of Exercise
3.46. The prior distribution on θ is π(θ) = 1.
a. Show that π(ζ|x) only depends on x1 and that f(x1|θ) only depends on ζ,
but that π(ζ|x) cannot be obtained from x1 ∼f(x1|ζ).
b. Show that, for any distribution π(θ) such that π(ζ|x) only depends on x1,
π(ζ|x) cannot be proportional to π(ζ)f(x1|ζ).
3.48 *(Jaynes (1980)) In the setting of Exercise 3.44, consider π(ξ, η) = π(ξ)π(η).
a. Show that
π(ξ|x) ∝π(ξ)c−ξ
 +∞
0
η−n exp(−ηx1Q)π(η)dη,
with
Q =
ξ

i=1
zi + c
n

ξ+1
zi.
b. Examine whether the paradox occurs for π(η) = η−k (k > −n −1).
c. Same question for η ∼Pa(α, η0).
3.49 *(Jaynes (1980)) Consider
f(y, z|η, ζ) ∝ζzηy(1 −η)z−y
y!(z −y)!
(0 ≤y ≤z),
with 0 < η < 1.
a. Show that f(z|η, ζ) only depends on ζ and derive the distribution f(y,
z|η, ζ) from f(y|z, η, ζ).
b. Show that, for every π(η), the paradox does not occur.

154
From Prior Information to Prior Distributions
3
3.50 *(Dawid et al. (1973))
Consider x = (y, z) with distribution f(x|θ) and
θ = (η, ξ). Assume that π(ξ|x) only depends on z and that f(z|θ) only depends
on ξ.
a. Show that the paradox does not occur if π(θ) is proper.
b. Generalize to the case where 
π(η, ξ) dη = π(ξ) and examine whether the
paradox is eliminated.
Section 3.5.3
3.51 In relation to Example 3.5.7, if x ∼B(n, p), ﬁnd a prior distribution on n
such that π(n|x) is Neg(x, p).
3.52 *In relation to Example 3.5.9,
a. Show that the Bayes estimator of η = ||θ||2 under quadratic loss for π(η) =
1/√η and x ∼N(θ, Ip) can be written as
δπ(x) =
1F1(3/2; p/2; ||x||2/2)
1F1(1/2; p/2; ||x||2/2),
where
1F1 is the conﬂuent hypergeometric function.
b. Deduce from the series development of
1F1 the asymptotic development
of δπ (for ||x||2 →+∞).
c. Compare δπ with δ0(x) = ||x||2 −p.
d. Study the behavior of these estimators under the weighted quadratic loss
L(δ, θ) = (||θ||2 −δ)2
2||θ||2 + p
and conclude.
3.53 Find a transform of θ, η = g(θ), such that the Fisher information I(η) is
constant for:
(i) the Poisson distribution, P(θ);
(ii) the gamma distribution, G(α, θ), with α = 1, 2, 3; and
(iii) the binomial distribution, B(n, θ).
3.54 Assuming that π(θ) = 1 is an acceptable prior for real parameters, show
that this generalized prior leads to π(σ) = 1/σ if σ ∈IR+ and to π(ϱ) =
1/ϱ(1 −ϱ) if ϱ ∈[0, 1] by considering the natural transformations θ = log(σ)
and θ = log(ϱ/(1 −ϱ)).
3.55 *(Saxena and Alam (1982)) In a setting identical to that in Exercise 3.52:
a. Give the maximum likelihood estimator of ||θ||2 when x ∼N(θ, Ip).
b. Show that the maximum likelihood estimator derived from z = ||x||2 satis-
ﬁes the implicit equation
1 =
z
√
λz
Ip/2(
√
λz)
I(p−1)/2(
√
λz)
(z > p),
where Iν is the modiﬁed Bessel function (see Abramowitz and Stegun (1964)
or Exercise 4.35).
c. Use the series expansion of Iν to show that the maximum likelihood esti-
mator ˆλ satisﬁes
ˆλ(z) = z −p + 0.5 + O(1/z).

3.7
Exercises
155
d. Show that ˆλ is dominated by (z −p)+ under quadratic loss.
3.56 The Fisher information is not deﬁned when the support of f(x|θ) depends
on θ. Consider the following cases:
(i) x ∼U[−θ,θ];
(ii) x ∼Pa(α, θ);
(iii) f(x|θ) ∝e−(x−θ)2/2II[0,θ](x).
3.57 Show that a second-order approximation of both the entropy and Hellinger
losses introduced in Section 2.5.4 is (θ−δ)2I(θ). Does this result give additional
support to the use of the Jeﬀreys prior?
3.58 Consider x ∼P(θ).
a. Determine the Jeﬀreys prior πJ and discuss whether the scale-invariant
prior π0(θ) = 1/θ is preferable.
b. Find the maximum entropy prior for the reference measure πJ
0 and the
constraints IEπ[θ] = 1, varπ(θ) = 1. What about using π0 instead?
c. Actually, x is the number of cars crossing a railroad in a period T. Show
that x is distributed according to a Poisson distribution P(θ) if the interval
between two arrivals is distributed according to Exp(λ). Note that θ = λT.
d. Use the above derivation of the Poisson distribution to justify the use of
π0.
Section 3.5.4
3.59 If x ∼N(θ, σ2), give the reference prior distributions for {θ, σ} and {σ, θ}.
3.60 Consider θ ∈[a, b] and π(θ) ∝1/θ.
a. Determine the normalizing factor of π.
b. Compute pi = π(i ≤θ < i + 1) for a ≤i ≤b −1.
c. Deduce the limit of pi as a goes to 0 or b goes to ∞. [Note: This exercise is
related to the table entry problem, namely, that in many numerical tables
the frequency of the ﬁrst signiﬁcant digit is log10(1 + i−1) (1 ≤i ≤9). See
Berger (1985a, p. 86) for a detailed account.]
3.61 *(Kass and Wasserman (1996)) Show that the reference prior derived from
the Jeﬀreys prior for θ1 ﬁxed, π(θ2|θ1), and the Jeﬀreys prior on the marginal
(3.5.3) can also be written as
π(θ1, θ2) ∝π(θ2|θ1) exp

π(θ2|θ1) log
1
|I|/|I22|dθ2

,
where I is the Fisher information and I22 is the part of I associated with θ2.
Section 3.6
3.62 *(Berger (1990a)) Consider the class Γϵ,Q deﬁned in §3.6 (iii), with
Q = { unimodal distributions symmetric around θ0}.
When π varies in Q, the marginal
m(π) =

f(x|θ)π(θ) dθ
varies between upper and lower bounds, mU and mL.

156
From Prior Information to Prior Distributions
3
a. Show that every unimodal distribution symmetric around θ0 can be written
as a mixture of uniform distributions symmetric around θ0, U[θ0−a,θ0+a].
b. Deduce that
mU =
sup
π∈Γϵ,Q
m(π) = (1 −ϵ)m(π0) + ϵ sup
z>0
 θ0+z
θ0−z
f(x|θ)
2z
dθ.
c. If the quantity of interest is the Bayes factor,
B(π) =
f(x|θ0)

θ̸=θ0 f(x|θ)π1(θ) dθ ,
where π1 is π conditioned by θ ̸= θ0 and π0 is the Dirac mass at θ0, show
that
BL =
inf
π∈Γϵ,Q
B(π) =
f(x|θ)
ϵ supz
 θ0+z
θ0−z (f(x|θ)/2z) dθ
.
3.63 Consider the class of prior distributions
Γ = {N(μ, τ 2), 0 ≤μ ≤2, 2 ≤τ 2 ≤4}
when x ∼N(θ, 1).
a. Study the variations of IEπ[θ|x] and varπ(θ|x) when π ∈Γ.
b. Study ϱ(π, δπ′) when π, π′ ∈Γ and δπ(x) = IEπ[θ|x], L(θ, δ) = (θ −δ)2 in
order to determine the Γ-minimax estimator.
3.64 *(Walley (1991)) Suppose that, instead of deﬁning a prior distribution π
on the σ-algebra of Θ, one deﬁnes some upper and lower bound measures on π,
π and π. For any event A, π(A) represents the largest amount one is willing to
bet to receive one unit if A occurs. Similarly, 1 −π(A) is the smallest amount
one is ready to bet against the event A.
a. Show that, if a prior distribution π is available, π = π = π.
b. Show that π(A) + π(Ac) ≤1 ≤π(A) + π(Ac) for every A if sure loss is to
be avoided.
c. If π(A ∪B) is the maximum bet one is ready to place on A ∪B, show that
π(A ∪B) ≥π(A) + π(B) and, similarly, that π(A ∪B) ≤π(A) + π(B).
3.65 *(Exercise 3.64 cont.) If, instead, one considers gambles, i.e., bounded
real-valued functions X on a measured space Ω corresponding to variable
rewards depending on the uncertainty state ω ∈Ω, it is also possible to deﬁne
upper and lower previsions, P and P, where P(X) denotes the maximum price
acceptable to get the reward X and P(X) the maximum selling price.
a. A gamble is desirable if there is a chance one gambles for it. Justify the
following axioms:
(A) If supω X(ω) < 0, then X is not desirable;
(B) If infω X(ω) > 0, then X is desirable;
(C) If X is desirable and λ > 0, then λX is desirable; and
(D) If X and Y are both desirable, then X + Y is desirable.

3.7
Exercises
157
b. Justify the following coherence axioms on P and show that they correspond
to (B), (C) and (D) above:
(P1) P(X) ≥infω X(ω);
(P2) P(λX) = λP(X); and
(P3) P(X + Y ) ≥P(X) + P(Y ).
c. Given a lower prevision P, the conjugate upper prevision is deﬁned by
P(X) = −P(−X). Show that, if P is coherent and P is the conjugate of
P, they satisfy
inf
ω X(ω) ≤P(X) ≤P(X) ≤sup
ω
X(ω) ,
and deduce that P is a convex function.
d. Show that, when P is self-conjugate, then P(X) = P(X) and it satisﬁes
the linearity requirements
P(X + Y ) = P(X) + P)Y )
and
P(λX) = λP(X),
λ ∈IR.
3.66 *(Exercise 3.65 cont.) A lower prevision P is said to avoid sure loss if,
for every n ≥1 and every set of gambles X1, . . . , Xn,
sup
ω
n

i=1
Xi −P(Xi) ≥0.
a. Show that P avoids sure loss if and only if
sup
ω
n

i=1
λi(Xi −P(Xi)) ≥0
for every n ≥1, every set of gambles X1, . . . , Xn and every λi ≥0.
b. Assuming that P avoids sure loss, show that, for every λ ≥0,
P(λX) ≤λP(X),
P(λX) ≥λP(X),
P(λX + (1 −λ)Y ) ≤λP(X) + (1 −λ)P(Y ).
when P is the conjugate upper prevision.
c. A lower prevision is coherent if
sup
ω

n

i=1
(Xi −P(Xi)) −m(X0 −P(X0))

≥0
for every m, n and every set of gambles X0, . . . , Xn. Show that P is coherent
if and only if it satisﬁes axioms (P1), (P2), and (P3).
d. Show that linearity is equivalent to coherence plus self-conjugacy, if linearity
is deﬁned by
sup
ω

n

i=1
Xi(ω) −
m

j=1
Yi(ω)

≥
n

i=1
P(Xi) −
m

j=1
P(Yi)
for every n, m and every set of gambles X1, . . . , Xn, Y1, . . . , Ym.
e. Show that P is a linear prevision if and only if P(X + Y ) = P(X) + P(Y )
and P(X) ≥infω X(ω). Deduce that P is a linear prevision if and only if
it satisﬁes linearity, (P2), and
(P4) if X ≥0, then P(X) ≥0; and
(P5) P(1) = 1.

158
From Prior Information to Prior Distributions
3
Note 3.8.3
3.67 Apply Dalal and Hall’s (1983) decomposition to the following cases:
(i) x ∼N(θ, Ip), θ ∼Tp(m, 0, τ 2); and
(ii) x ∼Neg(N, p), p/(1 −p) ∼G(1/2, 1/2).
3.68 *Find the natural measures νm of Dalal and Hall (1983) for the distributions
of Table 3.8.3.
3.8 Notes
3.8.1 Axiomatic derivation of prior distributions
To derive the existence of a prior distribution, we need, as in the utility case
(see Section 2.2), to start from an ordering on events rather than rewards.
Consider, thus, that the decision-maker, the client or the statistician is able to
determine an order relation on a σ-algebra B(Θ). This order relation, denoted
by ⪯, is such that B ≺A means that A is more likely than B, B ⪯A, that A is
at least as likely as B, and B ∼A, that A and B are equally likely. Obviously,
if there exists a probability distribution on (Θ, B(Θ)), P, P automatically
induces an order relation on B(Θ). We consider below under what hypotheses
the reciprocal can be established. A ﬁrst assumption is that the order relation
is total:
(A1) For all measurable sets A and B, one and only one of the following relations
is satisﬁed:
A ≺B,
B ≺A
or
A ∼B.
Another assumption is:
(A2) If A1, A2, B1, B2 are four measurable sets satisfying A1 ∩A2 = B1 ∩B2 = ∅
and Ai ⪯Bi (i = 1, 2), then A1 ∪A2 ⪯B1 ∪B2. Moreover, if A1 ≺B1,
A1 ∪A2 ≺B1 ∪B2.
This natural hypothesis implies transitivity for the order relation. The fol-
lowing assumption ensures that there is no measurable set with a negative
likelihood (i.e., less likely than the empty set):
(A3) For every event A, ∅⪯A and ∅≺Θ.
The additional condition ∅≺Θ avoids the trivial case where all events are
equivalent. It is also necessary to allow for the comparison of an inﬁnite se-
quence of events.
(A4) If A1 ⊃A2 ⊃· · · is a decreasing sequence of measurable sets and B is a
given event such that B ⪯Ai for every i, then
B ⪯
+∞
2
i=1
Ai.
This assumption somehow ensures the continuity of the preference ordering
and is related to the σ-additivity property of probability measures. However,
these axioms (A1)–(A4) are still not suﬃcient to derive the existence of a pro-
bability distribution from the likelihood ordering. In fact, a last assumption is
also necessary to move from a qualitative comparison scaling to a quantitative
comparison.

3.8
Notes
159
(A5) There exists a random variable X on (Θ, B(Θ)) with uniform distribution
on [0, 1], i.e., such that, for all I1, I2, intervals on [0, 1], it satisﬁes
{X ∈I1} ⪯{X ∈I2}
if and only if
λ(I1) ≤λ(I2),
where λ is the Lebesgue measure.
This additional hypothesis is then suﬃcient to establish the following existence
result (see DeGroot (1970) for a proof).
Theorem 3.8.1
Under the axioms (A1)–(A5), there exists a distribution P
such that P(A) ≤P(B) if and only if A ⪯B.
Compared with the utility function derivation in Chapter 2, the previous de-
velopments on the axiomatic foundations of the prior distribution are more
limited. A ﬁrst reason for this brevity is that the above hypotheses and the
surrounding setting are more diﬃcult to justify. Indeed, when a statistician
is able to talk about the likelihood of an event, it implies that she has, con-
sciously or not, built up an underlying probabilistic model and, therefore, the
previous construction is rather tautological. Assumption (A5) is particularly
demanding and can seldom be veriﬁed in practice. Notice, however, that to
some extent a similar criticism could be addressed to the derivation of the
utility function.
A second reason for this limitation takes place at a more pragmatic level; in
fact, according to Theorem 3.8.1, the decision-maker can recover a prior dis-
tribution from her likelihood ordering. However, it is most likely, especially
when Θ is not ﬁnite, that this ordering will be coarse, i.e., that the derived
σ-algebra B(Θ) will not correspond to the usual Borelian σ-algebra on Θ, thus
preventing the use of classical distributions on θ. Nevertheless, it is comforting
to be able to justify the use of a prior distribution on an alternative basis than
on the frequentist model implying repeatability of experiments, even though
it is of limited use in practice.
3.8.2 Exchangeability and conjugate priors
Bernardo and Smith (1994, Section 4.3) justify to some extent the existence
of prior distributions by the notion of exchangeability:
Deﬁnition 3.8.2 A sequence (x1, . . . , xn) of random variables is ﬁnitely ex-
changeable if the joint distribution p(x1, . . . , xn) is invariant under any per-
mutation of the indices of the random variables, that is,
p(x1, . . . , xn) = p(x(1), . . . , x(n)) ,
An inﬁnite sequence (xn)n is inﬁnitely exchangeable if any ﬁnite subsequence
is ﬁnitely exchangeable.
While the assumption of exchangeability is not always reasonable (see Bernardo
and Smith (1994, Section 4.2.2) for examples), there are also many settings
where the order in which the data is obtained is indeed unimportant. The con-
sequences of this assumption of inﬁnite exchangeability are, moreover, quite
interesting.

160
From Prior Information to Prior Distributions
3
For instance, if (xn)n is an inﬁnite sequence of random variables taking values
on {0, 1}, de Finetti (1930) has shown that there exists a probability measure
π(θ) such that, for every n, the joint distribution of (x1, . . . , xn) writes down as
p(x1, . . . , xn) =
 1
0
n

i=1
θxi(1 −θ)1−xidπ(θ),
that is, conditional on θ, the xi’s are i.i.d. Bernoulli B(θ) random variables.
As shown by Bernardo and Smith (1994, Section 4.3.2), this property extends
to random variables taking value in a ﬁnite set, {1, 2, . . . , k} say, as being
multinomial, conditional on a vector θ = (θ1, . . . , θk).
In the general case where the xi’s are real valued, and inﬁnitely exchangeable,
there also exists an interesting representation, under the form
p(x1, . . . , xn) =

n

i=1
F(xi)dπ(F) ,
where F is a c.d.f. and π a probability measure on the space of distribu-
tion functions (see Chow and Teicher (1988) for a more precise formulation
of this result, whose measure-theoretic subtleties are beyond the level of this
book). This representation is intrinsically non-parametric (see Note 1.8.2),
but Bernardo and Smith (1994, Section 4.6) elaborate on other notions of ex-
changeability to come back to the parametric setting.
3.8.3 Approximation by continuous mixtures of conjugate priors
Consider a density from an exponential family written in the form
f(x|θ) = exp{x · τ(θ) −γ(θ)},
with IE[x] = θ. (This parameterization is called mean parameterization; see
Brown (1986, Chapter 3).) A sequence of natural conjugate distributions is
given by (m ∈IN)
hm(θ|s) = exp{s · τ(θ) −mγ(θ)}cm(s),
(3.8.1)
where cm(s) is the normalizing factor. Recall that the prior distribution (3.8.1)
corresponds to an update of a ﬂat prior on θ for m ﬁctious (or virtual) obser-
vations ˜x1, . . . , ˜xm from f(x|θ), such that s = m
i=1 ˜xi.
Now, (3.8.1) can also be considered as the density of s for a measure, dνm,
called the natural measure. If Sm is the space in which s varies, and dQm is a
probability measure on Sm,
vm(θ) =

Sm
hm(θ|s) dQm(s)
(3.8.2)
constitutes a (continuous) mixture of conjugate priors. For a prior distribution
π on Θ, deﬁning
dQm(s) =
π(s/m) dνm(s)

Sm π(t/m) dνm(t),
we get an approximation of π, as shown by the following lemma.
Theorem 3.8.3
If νm is absolutely continuous with respect to the Lebesgue
measure or if νm is absolutely continuous with respect to the counting measure

3.8
Notes
161
Table 3.8.1. Approximation of prior distributions by mixtures of conjugate priors.
(Source: Dalal and Hall (1983).)
Distrib.
τ(θ), γ(θ)
cm(s)
hm(θ|s) prior
f(x, θ)
distribution
Normal
N(θ, 1)
θ, θ2/2
√mϕ(s/√m)
θ ∼N 	 s
m, 1
m

Gamma
G 	 β
θ , θ
−β
θ , −β log 	 β
θ

smβ−1
β Γ(mβ−1)
1
θ ∼G(sβ, mβ −1)
Poisson
P(θ)
log θ, θ
ms+1/Γ(s + 1)
θ ∼G(m, s + 1)
Bernoulli
B(1, θ)
log
θ
1−θ , log
1
1−θ
(m+1)!
s!(m−s)!
θ ∼Be(s + 1, m −s + 1)
Neg. bin.
Neg (r,
log
θ
r+θ,
rmr(mr+s−1)!
rs!(mr−2)!
r
r+θ ∼Be(mr −1, s + 1)
r
r+θ

r log(r + θ)
on Sm with density fm(s), and if fm(s) uniformly converges to 1 on Sm when
m goes to +∞, then
vm(θ) −→π(θ)
pointwise and for the L1 convergence.
Moreover, the approximation remains valid a posteriori, using the total varia-
tion distance as a measure of discrepancy. The total variation norm is deﬁned
as
||π −˜π||T V = sup
A
|π(A) −˜π(A)| ,
and is therefore always bounded by 1. It is thus somehow a weaker approxi-
mation result, compared with the L1 norm of Theorem 3.8.3.
Theorem 3.8.4
If pm, marginal distribution of x under hm, is ﬁnite and if
π(θ) and π(θ|x) are proper, vm(θ|x) converges to π(θ|x), pointwise and for the
total variation norm.
The approximative posterior distribution is, for n observations and t = n
i=1 xi,
vm(θ|n, t) =

Sm hm+n(θ|s + t)
cm(s)
cm+n(s+t)π(s/m) dνm(s)

Sm
cm(s′)
cm+n(s′+t)π(s′/m) dνm(s′)
and Table 3.8.3 provides the values of τ, γ and cm for some usual distributions.
Compared with the results of Diaconis and Ylvisaker (1985), Theorems 3.8.3
and 3.8.4 are indeed more general and ensure, in addition, convergence of the
posterior distributions. The drawback, however, is that this approach does not
preserve the advantage of conjugate priors, namely their simplicity. Simula-
tion methods such as those presented in Chapter 6 are thus necessary to derive
these Bayes estimators.

162
From Prior Information to Prior Distributions
3
3.8.4 Bartlett corrections
In standard asymptotic theory, the likelihood ratio statistic
ϖn = 2

n

i=1
f(xi|ˆθ) −
n

i=1
f(xi|ˆθ0)

,
is approximately distributed from a χ2
k distribution, where ˆθ and ˆθ0 denote the
maximum likelihood estimator and the constrained maximum likelihood esti-
mator, and k is the number of constraints (Gouri´eroux and Monfort (1996)).
Bartlett (1937) noticed that the ﬁt to a χ2
k distribution was improved when
ϖn was replaced with kϖn/IEθ[ϖn], in the sense that (Lawley (1956))
Pθ
kϖn
ˆE
≤t

= χ2
k(t) + O 	
n−2
,
where ˆE is an appropriate estimate of IEθ[ϖn] and χ2
k(t) denotes the χ2
k c.d.f.
The Bartlett correction thus leads to an improvement from O 	
n−1
to O 	
n−2
in the χ2
k approximation.
As noted in DiCiccio and Stern (1994), if θ = (ψ, ϕ) and the constraint on
θ is that ψ is ﬁxed, the likelihood ratio depends on ψ, ϖn = ϖn(ψ). Bickel
and Ghosh (1990) have established that the Bartlett correction extends to
the posterior distribution of ϖn(ψ), that is, that there exists a correction to
ϖn(ψ) such that
P

ϖn(ψ) ×

1 −AB
k

≤t|x, . . . , xn

= χ2
k(t) + O 	
n−2
,
where AB is deduced from an expansion of the posterior expectation
IE[ϖn(ψ)|x1, . . . , xn] = k + AB + O 	
n−3/2
,
and is of order O 	
n−1
. DiCiccio and Stern (1994) also established that this
second order χ2
k approximation holds for adjusted likelihood-ratio statistics,
ϖn(ψ)+ ωn(ψ), where ωn(ψ) is of order O(1). For instance, Kass et al. (1989)
use
ωn(ψ) = −1
2 log

det ℓϕϕ(ˆθ(ψ))
det ℓϕϕ(ˆθ)

+ log

π(ˆθ(ψ))
π(ˆθ)

,
where ˆθ and ˆθ(ψ) are the maximum likelihood estimator and the constrained
maximum likelihood estimator, and ℓϕϕ is the matrix of the second derivatives
of the log-likelihood for the nuisance parameter ϕ. DiCiccio and Stern (1994)
provide the corresponding correction AB, while DiCiccio and Stern (1993) give
the correction factor for the posterior ratio statistic
κπ = 2 
log π( ˆψ|x) −log π(ψ|x)
.
where ˆψ is the marginal MAP estimator of ψ.
Example 3.8.5 (DiCiccio and Stern (1993)) Consider the normal regression
yi ∼N
!
k

j=1
uijβj, σ2
"
,
i = 1, . . . , n,

3.8
Notes
163
associated with a ﬂat improper prior on (β, η), for η = log σ. If the parameter
of interest is η (or σ2), then
κπ = (n −k + 2)
/
nϱ
n −k + 2 −log
nϱ
n −k + 2 −1
0
,
where ϱ = ˆσ2/σ2 and the correcting term ℵπ, such that (1 + ℵπ)−1κπ is χ2
p
up to the order O(n−2), is ℵπ(η) = n−1/3. When ξ = (β1, . . . , βp) is the
parameter of interest, ℵπ(ξ) = (1 + p/2)n−1.
∥


CHAPTER 4
Bayesian Point Estimation
“There is always something new from you,” Perrin growled. “Can’t you tell
us what to expect once in a while, instead of explaining after it happens?”
Uno looked as though he was trying to think of a reason to leave.
Moraine gave Perrin a ﬂat look. “You want me to share a lifetime of knowledge
with you in a single afternoon? Or even a single year?”
Robert Jordan, The Dragon Reborn, Book III of the Wheel of Time.
4.1 Bayesian inference
4.1.1 Introduction
When the prior distribution π(θ) is available, the posterior distribution
π(θ|x) can be formally derived from the observation x with distribution
f(x|θ). This updated distribution is then the extensive summary of the
information available on the parameter θ, integrating simultaneously prior
information and information brought by the observation x. (The same ob-
viously holds when a sample x1, . . . , xn is available, but it can usually be
reduced to the above situation through a suﬃcient statistic.) The Bayesian
version of the Likelihood Principle thus implies that the inference on θ
should rely entirely on the posterior distribution π(θ|x). Even though θ is
not necessarily a random variable, the distribution π(θ|x) can be used as a
regular probability distribution to describe the properties of θ. Summariz-
ing indices for π(θ|x) such as the posterior mean, the posterior mode, the
posterior variance, and the posterior median, can be used. For instance,
when the quantity of interest is h(θ), a possible estimator of h(θ) is the
posterior mean IEπ[h(θ)|x]. (As mentioned in Section 3.5, when the distri-
bution π is a noninformative prior, some marginalization diﬃculties may
occur, and it is sometimes necessary to derive a new reference prior for the
parameter of interest, h(θ).)

166
Bayesian Point Estimation
4
4.1.2 MAP estimator
If a choice must be made among the above summaries, there is no way
of selecting a best estimator, short of using a loss criterion. Nonetheless, a
possible estimator of θ based on π(θ|x) is the maximum a posteriori (MAP)
estimator, deﬁned as the posterior mode. Notice that the MAP estimator
also maximizes ℓ(θ|x)π(θ), thus bypassing the computation of the marginal
distribution.
It is associated with the 0−1 loss, as seen in Section 2.5.3 for the special
case θ ∈{0, 1}. In continuous settings, since, for every δ ∈Θ,

Θ
IIδ̸=θπ(θ|x)dθ = 1 ,
the 0−1 loss must be replaced by a sequence of losses, Lε(d, θ) = II||θ−d||>ε,
and the MAP estimate is then the limit of the Bayes estimates associated
with Lε, when ε goes to 0.
This natural estimator can be expressed as a penalized maximum like-
lihood estimator in the classical sense (see Akaike (1978, 1983)). Notice
that the asymptotic optimality properties of the regular maximum likeli-
hood estimator (consistency, eﬃciency) are preserved for these Bayesian
extensions, under a few regularity conditions on f and π (see Note 1.8.4
and Ibragimov and Has’minskii (1981)). This extension of the asymptotic
properties of the maximum likelihood estimator is intuitively sound since,
as the sample size grows to inﬁnity, the information contained in this sample
becomes predominant compared with the ﬁxed information brought by the
prior distribution π. Therefore, MAP estimators are asymptotically equiva-
lent to the classical maximum likelihood estimators but, furthermore, have
the advantage of being available for ﬁnite sample sizes; the latter are mainly
justiﬁed on asymptotic grounds.
Example 4.1.1
Consider x ∼B(n, p). We saw in the previous chapter
that the Jeﬀreys prior in this setting is the beta distribution Be(1/2, 1/2),
i.e.,
π∗(p) =
1
B(1/2, 1/2)p−1/2(1 −p)−1/2 ,
omitting the indicator function II[0,1](p) to simplify the notations. Two
other noninformative distributions have been proposed in the literature
by Laplace and Haldane (1931) (see also Exercise 4.4),
π1(p) = 1
and
π2(p) = p−1(1 −p)−1 .
The corresponding MAP estimators are then, for n > 2,
δ∗(x)
=
max
x −1/2
n −1 , 0

,
δ1(x)
=
x
n,

4.1
Bayesian inference
167
δ2(x)
=
max
x −1
n −2, 0

.
When n = 1, δ∗and δ2 are equal to δ1. For n = 2 and x = 1, the estimator
δ2 is also equal to δ1, which is the regular maximum likelihood estimator.
Notice that, when n is large, the three estimators are indeed equivalent. ∥
Example 4.1.2
Consider x ∼C(θ, 1), i.e.,
f(x|θ) = 1
π
+
1 + (x −θ)2,−1 ,
and π(θ) =
1
2e−|θ|. The MAP estimator of θ is then δ∗(x) = 0, as the
maximum of exp(−|θ|)[1 + (x −θ)2]−1 is attained for θ = 0, whatever the
value of x. This behavior may be explained by the ﬂatness of the likelihood
function, which is not informative enough, compared with the sharp prior
distribution. However, from a practical point of view, this estimator is
useless (see also Exercise 4.6).
∥
4.1.3 Likelihood Principle
Bayesian inference appears as a way to eﬃciently implement the Likeli-
hood Principle, since it provides an estimator, by selecting for instance,
as in Example 4.1.3 below, one of several maxima of the likelihood func-
tion. As stressed by Savage (1954) and Berger and Wolpert (1988), there
are many philosophical and practical considerations linking the Likelihood
Principle to a robust Bayesian implementation. In particular, it allows for
the elimination of some classical paradoxes, such as those of Stein (1962b),
Stone (1976), Fraser, Monette and Ng (1984), and Le Cam (1990). The
following example illustrates the resolution of the paradox of Fraser et al.
(1984). (See also Joshi (1990) for an extended analysis of the phenomenon.)
Example 4.1.3 (Berger and Wolpert (1988)) Consider X = Θ = IN∗and
f(x|θ) = 1
3 for
⎧
⎨
⎩
θ/2, 2θ, 2θ + 1
if θ is even,
(θ −1)/2, 2θ, 2θ + 1
if θ is odd and θ ̸= 1,
1, 2, 3
if θ = 1.
(4.1.1)
The likelihood function is then
ℓ(θ|x) = 1
3 for
⎧
⎨
⎩
x/2, 2x, 2x + 1
if x is even,
(x −1)/2, 2x, 2x + 1
if x is odd and x ̸= 1,
1, 2, 3
if x = 1,
and the three values of θ where ℓ(θ|x) ̸= 0 are equally weighted by the
likelihood function. Consider the three following estimators:
δ1(x) =
⎧
⎨
⎩
x/2
if x is even,
(x −1)/2
if x is odd and x ̸= 1,
1
if x = 1,

168
Bayesian Point Estimation
4
and
δ2(x) = 2x,
δ3(x) = 2x + 1.
They are equivalent from the Likelihood Principle perspective, since the
likelihood function is ﬂat on its support, but δ2 and δ3 are quite suboptimal
estimators because
P(δ2(x) = θ) = P(x = θ/2) =

1/3
if θ is even,
0
otherwise,
P(δ3(x) = θ) = P(x = (θ −1)/2) =

1/3
if θ ̸= 1 is odd,
0
otherwise,
while
P(δ1(x) = θ) =
 1
if θ = 1,
2/3
otherwise.
The estimator δ1 is therefore preferable under losses such as the 0 −1 loss.
When the information available on the model is reduced to the likelihood
function (4.1.1), a possible noninformative distribution on θ is π(θ) = 1/θ,
since θ can approximately be considered as a scale parameter. In this case,
π(θ|x) ∝1
3θ
+
IIδ1(x)(θ) + IIδ2(x)(θ) + IIδ3(x)(θ)
,
and this posterior distribution gives δ1(x) as being four times more likely
than δ2 or δ3. It can also be shown that P π(θ = δ1(x)|x) ≃2/3 for large x’s.
This gives a good justiﬁcation to the choice of δ1. A more informative prior
modeling would lead to a similar conclusion (since a proper distribution
π(θ) must be decreasing for θ large enough).
∥
Berger and Wolpert (1988) provide similar resolutions of the paradoxes
exhibited by Stein (1962b) and Stone (1976). An immediate advantage of
the Bayes approach over other implementations of the Likelihood Principle
is that it takes care of the nuisance parameters in the likelihood function
by integrating them out. In fact, if ℓ(θ, τ|x) also depends on a nuisance
parameter τ, a natural derivation of an estimate ˆθ of θ is to consider the
maximum of the integrated likelihood

ℓ(θ, τ|x)π(θ, τ) dτ
instead of the more classical “proﬁle” likelihood,
max
τ
ℓ(θ, τ|x)π(θ, τ).
See also Basu (1988) for an extensive analysis of the treatment of the
nuisance parameters.
4.1.4 Restricted parameter space
Berger (1985a) points out the interest of a noninformative Bayesian ap-
proach for restricted parameter spaces, since the prior distribution is just

4.1
Bayesian inference
169
the truncation of the nonrestricted noninformative distribution. From a
classical point of view, the derivation of restricted maximum likelihood es-
timators is often complicated, especially when the constraints are nonlinear
(see Robertson et al. (1988)). On the contrary, the implementation of the
Bayesian approach through a Monte Carlo simulation scheme (see Chapter
6) allows for an easy derivation of the Bayes estimators. (This advantage
can even be used to compute restricted maximum likelihood estimators
via Bayesian techniques. See Geyer and Thompson (1992) and Robert and
Casella (2004, Chapter 5)).
Example 4.1.4
Consider the estimation of the linear regression model
y = b1X1 + b2X2 + ϵ,
(4.1.2)
which relates direct incomes (X1), saving incomes (X2), and savings (y). A
careful estimation of the saving rates b1 and b2 helps enable the government
to determine its ﬁscal and interest rates policies. The saving rates are obvi-
ously constrained by 0 ≤b1, b2 ≤1. Consider a sample (y1, X11, X21), . . . ,
(yn, X1n, X2n) from (4.1.2) and assume that the errors ϵi are independent
and distributed according to N(0, 1), i.e., that yi ∼N(b1X1i + b2X2i, 1).
The corresponding noninformative distribution is then the proper distribu-
tion
π(b1, b2) = II[0,1](b1)II[0,1](b2)
and the posterior mean is given by (i = 1, 2)
IEπ[bi|y1, . . . , yn] =
 1
0
 1
0 bi
.n
j=1 ϕ(yj −b1X1j −b2X2j) db1 db2
 1
0
 1
0
.n
j=1 ϕ(yj −b1X1j −b2X2j) db1 db2
,
where ϕ is the density of the standard normal distribution. If we denote by
(ˆb1,ˆb2) the unconstrained least-squares estimator of (b1, b2), which is also
the regular maximum likelihood estimator of (b1, b2), the unconstrained
posterior distribution on (b1, b2) is

b1
b2

∼N2
ˆb1
ˆb2

,
	
XtX

−1

,
(4.1.3)
with
X =
⎛
⎝
X11
X21
...
...
X1n
X2n
⎞
⎠.
Therefore, the restricted Bayes estimator is given by (i = 1, 2)
δπ
i (y1, . . . , yn) = IEπ +
biII[0,1]2(b1, b2)|y1, . . . , yn
,
P π ((b1, b2) ∈[0, 1]2|y1, . . . , yn),
where the right-hand term is computed under the distribution (4.1.3). If
we denote
Σ = (XtX)−1 =

σ2
11
σ12
σ12
σ2
22

,

170
Bayesian Point Estimation
4
the conditional distribution of b1 is
b1|b2 ∼N

ˆb1 + σ12(b2 −ˆb2)/σ2
22, σ2
11 −σ2
12σ−2
22

.
Then
P π ((b1, b2) ∈[0, 1]2|y1, . . . , yn

=
 1
0
⎧
⎨
⎩Φ
⎛
⎝1 −ˆb1 −σ12(b2 −ˆb2)/σ2
22
3
σ2
11 −σ2
12σ−2
22
⎞
⎠
−Φ
⎛
⎝−ˆb1 −σ12(b2 −ˆb2)/σ2
22
3
σ2
11 −σ2
12σ−2
22
⎞
⎠
⎫
⎬
⎭σ−1
22 ϕ
!
b2 −ˆb2
σ22
"
db2
and
IEπ[biII[0,1]2(b1, b2)|y1, . . . , yn] =
 1
0

ˆb1 + σ12
σ2
22
(b2 −ˆb2)
+(σ2
11 −σ2
12σ−2
22 )1/2
⎧
⎨
⎩ϕ
⎛
⎝1 −ˆb1 −σ12(b2 −ˆb2)/σ2
22
3
σ2
11 −σ2
12σ−2
22
⎞
⎠
−ϕ
⎛
⎝−ˆb1 −σ12(b2 −ˆb2)/σ2
22
3
σ2
11 −σ2
12σ−2
22
⎞
⎠
⎫
⎬
⎭
⎤
⎦σ−1
22 ϕ
!
b2 −ˆb2
σ22
"
db2.
Notice that this second integral can be obtained in closed form using the
standard normal c.d.f. Φ, but that the denominator cannot be computed
analytically. It is therefore more eﬃcient to compute both integrals by a
Monte Carlo simulation (see Chapter 6).
If b1 and b2 are independent a posteriori, i.e., when σ12 = 0, the Bayes
estimators are explicit and given by (i = 1, 2)
IEπ[bi|y1, . . . , yn] = ˆbi −σii
exp{−(1 −ˆbi)2/2σ2
ii} −exp{−ˆb2
i /2σ2
ii}
√
2π{Φ((1 −ˆbi)/σii) −Φ(−ˆb1/σii)}
.
∥
Notice that a Bayesian modeling is also quite appropriate to incorporate
vague information, i.e., cases where a restriction on the parameter space is
likely but not certain. Chapter 10 demonstrates that a typical way to do
this is to use a hierarchical or empirical Bayes modeling.
4.1.5 Precision of the Bayes estimators
Since the whole posterior distribution π(θ|x) is available, it is possible to
associate to an estimator δπ(x) of h(θ) an evaluation of the precision of the
estimation, through, for instance, the posterior squared error,
IEπ[(δπ(x) −h(θ))2|x],

4.1
Bayesian inference
171
equal to varπ(h(θ)|x) when δπ(x) = IEπ[h(θ)|x]. Similarly, in a multidi-
mensional setting, the covariance matrix characterizes the performances
of estimators. These additional indications provided by the posterior dis-
tribution illustrate the operational advantage of the Bayesian approach,
since the classical approach usually has diﬃculties motivating the choice
of these evaluations. Moreover, Bayesian evaluation measures are always
conditional1, while the frequentist approach usually relies on upper bounds
through the minimax principle, since the parameter θ is unknown (see
Berger and Robert (1990) for a comparison of both approaches).
Example 4.1.5 (Example 4.1.1 continued)
Consider the maximum
likelihood estimator of p, δ1(x) = x/n. Then
IEπ[(δ1(x) −p)2|x]
=
IEπ[(p −x/n)2|x]
=
x + 1/2
n + 1
−x
n
2
+ (x + 1/2)(n −x + 1/2)
(n + 1)2(n + 2)
=
(x −n/2)2
(n + 1)2n2 + (x + 1/2)(n −x + 1/2)
(n + 1)2(n + 2)
,
(4.1.4)
since π(p|x) is the beta distribution Be(x + 1/2, n −x + 1/2). From a
frequentist viewpoint, the risk of the maximum likelihood estimator is
IEp[(δ1(x) −p)2] = var(x/n) = p(1 −p)
n
and
sup
p p(1 −p)/n = 1/4n.
Developing (4.1.4), it is easy to verify that the maximum of (4.1.4) is
1/[4(n + 2)],
always smaller than 1/4n. The major advantage of the quantity (4.1.4),
though, is still to provide an adjustable answer for the evaluation of δ1,
since (4.1.4) varies between 1/[4(n+2)] and 3/[4(n+1)(n+2)]. Obviously,
a frequentist approximation of p(1 −p)/n can also be proposed, namely,
(x/n)(1 −x/n)/n. This evaluation has then the opposite drawback of 1/4n
since it varies too widely, as shown by Figure 4.1.5. It can even take the
value 0 when x = 0, n. A similar behavior is discussed in Berger (1990b) in
a general framework.
∥
4.1.6 Prediction
Furthermore, Bayesian inference can operate as well in prediction problems.
If x ∼f(x|θ) and z ∼g(z|x, θ), where z does not necessarily depend on x,
1 In fact, there are Bayesian counterparts to the Cram´er-Rao inequalities used in the
evaluation of unbiased estimators. These are the Van Trees bounds (Gill and Levit
(1995)), whose use can be found in signal processing and other domains, as shown in
Bergman et al. (2001).

172
Bayesian Point Estimation
4
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.01
0.02
0.03
0.04
0.05
0.06
p
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.01
0.02
0.03
0.04
0.05
0.06
Bayes
frequentist
Figure 4.1.1. Comparison of Bayesian and frequentist evaluations of the estima-
tion error in the binomial case (n = 3).
the predictive distribution of z after the observation of x is given by
gπ(z|x) =

Θ
g(z|x, θ)π(θ|x) dθ.
(4.1.5)
The distribution of z is thus quite logically averaged over the values of θ
according to the posterior distribution, which is also the actualized distri-
bution of θ. It is possible to use (4.1.5) to derive the predictive mean and
variance of the random variable z. In Section 4.3.1, we consider a particu-
lar example of determination of a discrete predictive distribution (see also
Exercise 4.40).
Example 4.1.6 A particular case of AR(1) model, where AR stands for
autoregressive, deﬁnes the distribution of a stochastic process (xt)1≤t≤T by
a linear dynamic representation conditional on the previous variable xt−1,
as
xt = ϱxt−1 + ϵt ,
where the ϵt’s are i.i.d. N(0, σ2). (This model will be considered in detail
in Section 4.5.) Given a sequence of observations till time T −1, x1:(T −1) =
(x1, . . . , x(T −1)), the predictive distribution of xT is then given by
xT |x1:(T −1) ∼

1
√
2πσ−1 exp{−(xT −ϱxT −1)2/2σ2}π(ϱ, σ|x1:(T −1))dϱdσ ,
where π(ϱ, σ|x1:(T −1)) can be expressed in closed form (Exercise 4.13).
∥
Notice that the decision-theoretic approach developed in the following
sections is also applicable in a prediction setting, even though we do not
mention it later. Indeed, if a prediction loss L(z, δ) is available, a predictor
δ(x) can be chosen in order to minimize the expected prediction error (the

4.2
Bayesian Decision Theory
173
expectation being with respect to the predictive distribution (4.1.5)). (See
Exercise 4.45.)
4.1.7 Back to Decision Theory
Given the scope of possible uses of the posterior distribution, some Bayesians
consider that clients should be provided with the posterior distribution so
that they might use it at their convenience. Although the communication of
π(θ|x) is indeed of interest for small dimensions, the information provided
by π(θ|x) gets blurred by its own complexity for large dimensions. The pos-
terior distribution is obviously essential in the decision-making process, but
it is part of the statistician’s role to assist the decision-maker further, in
order to extract the features of interest from π(θ|x). We thus confront again
the major problem of selecting among estimators and we saw in Chapter
2 that this selection is eﬃcient and coherent only when based on a loss
criterion. The following sections set forth Bayesian Decision Theory, with
particular attention to the normal and sampling cases. Testing perspectives
and conﬁdence-set estimation are treated separately in Chapter 5.
4.2 Bayesian Decision Theory
4.2.1 Bayes estimators
Let us recall here that, given a loss function L(θ, δ) and a prior distribution
(or a measure) π, the Bayes rule δπ(x) is solution of
min
δ
IEπ[L(θ, δ)|x].
Depending on the complexity of the loss L and the posterior distribution
π(θ|x), the estimator δπ will be determined analytically or numerically,
similar to maximum likelihood estimation.
As shown in Chapter 2, the solutions associated with classical losses
are formally known and correspond to the natural indicators associated
with a distribution (mean, median, mode, quantiles, etc.). For instance, the
Bayes estimator associated with the quadratic loss is the posterior mean
(Proposition 2.5.1 and Corollary 2.5.2). Of course, this formal derivation of
classical Bayes estimators does not always avoid a numerical approximation
of the estimators, especially in multidimensional settings.
Example 4.2.1
Consider x ∼Np(θ, Ip). As mentioned in Section 3.6,
Student’s t-distribution provides a robust alternative to the conjugate nor-
mal prior for estimating θ. Consider thus θ ∼Tp(α, 0, τ2Ip), i.e.,
π(θ|α, τ) =
Γ((α + p)/2)
(ατπ)p/2Γ(α/2)

1 + ||θ||2
ατ2
−(α+p)/2
.

174
Bayesian Point Estimation
4
Therefore,
π(θ|x) ∝

1 + ||θ||2
ατ2
−(α+p)/2
e−||x−θ||2/2,
which does not lead to a closed-form expression for the posterior distribu-
tion. However, it is still possible to reduce the computational problem to a
single integration, for every value of p, as shown by Dickey (1968). In fact,
if θ ∼Tp(α, 0, τ2Ip), the posterior distribution of θ can be expressed as a
hidden mixture (see Example 3.3.11),
θ|z
∼
Np(0, τ2zIp),
z−1
∼
G(α/2, α/2),
where z is an auxiliary random variable. Conditional upon z, the posterior
distribution of θ is
θ|x, z ∼Np

x
1 + τ2z ,
τ 2z
1 + τ2z Ip

and, since
π(z|x) ∝(1 + τ 2z)−p/2e−||x||2/2(1+τ 2z)π(z),
we derive the Bayes estimator as
δπ(x)
=
 +∞
0
IEπ[θ|x, z]π(z|x) dz
=
x
 +∞
0
(1 + τ 2z)−(p+2)/2e−||x||2/2(1+τ 2z)z−(α+2)/2e−α/2z dz
 +∞
0
(1 + τ 2z)−p/2e−||x||2/2(1+τ 2z)z−(α+2)/2e−α/2z dz
.
This estimator can therefore be expressed through a single integral for every
value of p.
∥
However, decomposition subtleties as in the above example are not al-
ways available, and the computation of a Bayes estimator then calls for
general approximation methods such as those described in Chapter 6. An
interesting result is that, when the marginal distribution m(x) is available,
the posterior expectation of the natural parameter can easily be derived
for exponential families.
Lemma 4.2.2
Consider f(x|θ) = h(x)eθ·x−ψ(θ), a distribution from an
exponential family. For every prior distribution π, the posterior mean of θ
is given by
δπ(x) = ∇log mπ(x) −∇log h(x),
(4.2.1)
where ∇denotes the gradient operator and mπ is the marginal distribution
associated with π.
Proof.
The posterior expectation is given by
IEπ[θi|x]
=

Θ θih(x)eθ·x−ψ(θ)π(θ) dθ
mπ(x)

4.2
Bayesian Decision Theory
175
=
 ∂
∂xi

Θ
h(x)eθ·x−ψ(θ)π(θ) dθ

1
mπ(x) −
 ∂
∂xi
h(x)

1
h(x)
=
∂
∂xi
[log mπ(x) −log h(x)] .
22
Notice that this lemma is satisﬁed for every π; it appears as the dual
result of the derivation of the moments of f(x|θ) from the derivative of ψ
in exponential families (see Lemma 3.3.7). Its practical interest is rather
limited since the derivation of the marginal distribution is usually quite
delicate and to know mπ(x) explicitly is equivalent to knowing π(θ|x) ex-
plicitly. From a theoretical viewpoint, a consequence of this lemma is to
show that Bayes estimators are analytic (or holomorphic) functions for ex-
ponential families such that the function h spanning the exponential family
is holomorphic (since mπ/h is the Laplace transform of e−ψ(θ)π(θ)). Chap-
ter 8 derives an inadmissibility criterion from this property.
Example 4.2.3
We introduced in Note 2.8.2 the truncated James–Stein
estimator,
δJS(x) =

1 −p −2
||x||2
+
x
when x ∼Np(θ, Ip). In the normal case, (4.2.1) reduces to
δπ(x) = x + ∇log mπ(x).
Although there exists a function m such that δJS can be written as above
(see Bock (1988)), m is not a marginal distribution and this estimator
cannot be a Bayes estimator: it is equal to 0 on the open set {||x||2 < p−2}
and should be null everywhere if it were analytic.
∥
The representation (4.2.1) of the Bayes estimators is also useful in tech-
niques underlying the Stein eﬀect, either to establish domination condi-
tions as in Stein (1981), George (1986a), Berger and Robert (1990), and
Brandwein and Strawderman (1990), or to point out the inadmissibility of
some estimators as in Bock (1988) and Brown (1988). (See Exercise 4.43.)
4.2.2 Conjugate priors
In the particular case of conjugate distributions, the posterior expectations
of the natural parameters can obviously be expressed analytically and this
is practically the only case where closed-form expressions are available in
such generality. Table 4.2.1 presents the Bayes estimators associated with
the usual distributions and their conjugate priors. Notice that when several
observations from f(x|θ) are available the conjugate distributions are the
same, and that only the parameters in the estimator are modiﬁed by virtue
of the suﬃciency properties of exponential families (see Section 3.3.3).

176
Bayesian Point Estimation
4
Table 4.2.1. The Bayes estimators of the parameter θ under quadratic loss for
conjugate distributions in the usual exponential families.
Distribution
Conjugate prior
Posterior mean
Normal
Normal
N(θ, σ2)
N(μ, τ2)
μσ2 + τ 2x
σ2 + τ 2
Poisson
Gamma
P(θ)
G(α, β)
α + x
β + 1
Gamma
Gamma
G(ν, θ)
G(α, β)
α + ν
β + x
Binomial
Beta
B(n, θ)
Be(α, β)
α + x
α + β + n
Negative binomial
Beta
Neg(n, θ)
Be(α, β)
α + n
α + β + x + n
Multinomial
Dirichlet
Mk(n; θ1, . . . , θk)
D(α1, . . . , αk)
αi + xi

j αj

+ n
Normal
Gamma
N(μ, 1/θ)
G(α/2, β/2)
α + 1
β + (μ −x)2
Example 4.2.4 If x1, ..., xn are independent observations from Neg(m, θ)
and if θ ∼Be(α, β), the posterior distribution of θ is the beta distribution
Be (α + mn, n
i=1 xi + β) and
δπ(x1, ..., xn) =
α + mn
α + β + mn + n
i=1 xi
.
This result is a straightforward consequence of n
i=1 xi ∼Neg(mn, θ).
∥
Example 4.2.5
Consider n observations x1, ..., xn from U([0, θ]) and
θ ∼Pa(θ0, α). Then
θ|x1, ..., xn ∼Pa(max (θ0, x1, ..., xn), α + n)
and
δπ(x1, ..., xn) =
α + n
α + n −1 max (θ0, x1, ..., xn).
Therefore, compared with the maximum likelihood estimator,
δ0(x1, ..., xn) = max(x1, ..., xn),

4.2
Bayesian Decision Theory
177
the Bayes estimator gives a more optimistic estimation of θ, since
α + n
α + n −1 > 1 .
On the contrary, the best equivariant estimator of θ under squared error loss
is
n
n−1δ0(x1, . . . , xn) (see Chapter 9), larger than δπ when θ0 is small. This
shrinking behavior of δπ is explained by the choice of π, which decreases
with θ, thus favoring values of θ close to θ0.
∥
Similarly, recall that the estimation of a function of θ, g(θ), under qua-
dratic loss gives δπ(x) = IEπ[g(θ)|x] as the Bayes estimator.
Example 4.2.6
Consider x ∼G(ν, θ), where the shape parameter ν is
known, and θ ∼G(α, β). The parameter of interest is 1/θ, the expectation
of x. Under the quadratic loss
L(θ, δ) =

δ −1
θ
2
,
the Bayes estimator is then
δπ
1 (X)
=
(β + x)α+ν
Γ(α + ν)
 +∞
0
1
θ θα+ν−1e−(β+x)θdθ
=
β + x
α + ν −1.
∥
Under a renormalized (or weighted) quadratic loss,
L(θ, δ) = w(θ) ∥δ −θ ∥2
Q,
where Q is a p×p nonnegative symmetric matrix, the corresponding Bayes
estimator is
δπ(x) = IEπ[θw(θ)|x]
IEπ[w(θ)|x] .
Example 4.2.7 (Example 4.2.6 continued) A scale-invariant loss does
not depend on the unit of measurement and may be more relevant for the
estimation of 1/θ. For instance, the loss
L(θ, δ) = θ2

δ −1
θ
2
gives the Bayes estimator
δπ
2 (x)
=
IEπ +
θ2/θ | x
,
IEπ [θ2 | x]
=
 +∞
0
θθα+ν−1e−(β+x)θdθ
 +∞
0
θα+ν+1e−(β+x)θdθ
=
β + x
α + ν + 1 = α + ν −1
α + ν + 1 δπ
1 (x).
∥

178
Bayesian Point Estimation
4
Let us stress again that, even for conjugate distributions, the fact that
the Bayes estimator of any function of θ involves posterior expectations
does not necessarily avoid numerical computations because analytical inte-
gration may be impossible, especially in multidimensional problems.
Example 4.2.8
Consider x ∼Np(θ, Ip) and h(θ) = ||θ||2. The loss con-
sidered in Saxena and Alam (1982) is
L(θ, δ) = (δ −||θ||2)2
2||θ||2 + p
since, if δ0(x) = ||x||2 −p,
R(δ0, θ) =
1
2||θ||2 + p IE(||x||2 −||θ||2 −p)2 = 2
and δ0 has a constant risk. Without this renormalization, all estimators
have a maximum risk equal to +∞, while under L, the estimator δ0 is
minimax. Then, even for the conjugate distributions, Np(0, τ2Ip), the com-
putation of
δπ(x) = IEπ[||θ||2/(2||θ||2 + p)|x]
IEπ[1/(2||θ||2 + p)|x]
cannot be conducted analytically.
∥
In the previous examples, we resorted rather heavily to the quadratic
loss since it constitutes a standard loss and allows, as much as possible, for
explicit computations. We refer the reader to Chapter 2 for criticisms about
the arbitrariness of standard losses and the opposition between bounded
concave and unbounded convex losses, the former leading to the risk-lover
paradox and the latter to more instability in the resulting procedures (see
Kadane and Chuang (1978), Smith (1988), and Exercises 4.1 and 4.14).
Even so, it bears mentioning that, when the loss function is truly deter-
mined by the decision-maker, it is usually complex and most often calls for
a numerical minimization to determine the Bayes estimator.
4.2.3 Loss estimation
For a given loss function, L(θ, δ), it may also be of interest to assess the
performance of the Bayes estimator δπ(x). This evaluation can be per-
ceived from a decision-theoretic point of view as the estimation of the loss
L(θ, δπ(x)) by γ(x) under a loss function, like
˜L(θ, δπ, γ) = [γ(x) −L(θ, δπ(x))]2 .
(4.2.2)
Again, the quadratic loss (4.2.2) is no more justiﬁed as an automatic loss in
this context than in other estimation settings. But, apart from tractability
reasons, the choice of the quadratic loss can be grounded on a lack of utility
foundations, and thus a closer perception of the error as a variance. Under

4.2
Bayesian Decision Theory
179
(4.2.2), the Bayesian evaluation of the performances of δπ is given by the
following result.
Proposition 4.2.9
The Bayes estimator of the loss L(θ, δπ(x)) under
(4.2.2) for the prior distribution π is
γπ(x) = IEπ[L(θ, δπ(x))|x].
This result directly follows from Proposition 2.5.1, since, conditional
upon x, the purpose is to estimate a particular function of θ under quadratic
loss. Notice that the dependence of this function on x does not matter from
a Bayesian perspective since, once x is observed, x is ﬁxed. Similarly, for
the absolute error loss, the Bayes estimator of the loss is the median of
the posterior distribution of L(θ, δπ(x)), which is usually more diﬃcult to
obtain. When L is the quadratic loss, the posterior variance, varπ(x), is
thus the Bayes estimator of the loss associated with δπ.
From a frequentist perspective, loss estimation has been studied by John-
stone (1988) and Rukhin (1988a,b), the former establishing that, for a
minimax estimator with constant risk p, the evaluation γ(x) = p is not
necessarily admissible under (4.2.2). Berger (1984b, 1985b) (see also Lu
and Berger (1989a,b)) develops an additional concept for loss estimation,
called frequentist validity: an estimator γ of the loss L(θ, δ(x)) is said to be
frequency valid if
IEθ[γ(x)] ≥R(θ, δ(x)),
θ ∈Θ,
i.e., if, in the long run, this estimator never underestimates the error result-
ing from the use of δ. Such a restriction may seem intuitively appealing,
but it is based on an intuition at the basis of the notion of unbiased esti-
mation, and this restriction comes to contradict the Likelihood Principle.
Robert and Casella (1994) propose a purely decision-theoretic approach of
loss estimation in the setting of conﬁdence regions (see Chapter 5). If C(x)
is a conﬁdence region for θ, the usual loss underlying the estimation process
is the 0 −1 loss,
L(C(x), θ) = 1 −IIC(x)(θ),
and a loss estimator γ(x) thus evaluates the coverage rate of C(x), i.e.,
somehow approximates the coverage probability of the conﬁdence region.
Hwang and Brown (1991) have shown that, for the usual conﬁdence regions
C0, in the normal setting, the constant estimator
α = P(θ ̸∈C0(x))
is admissible among frequency valid estimators, while inadmissible for p > 5
without the restriction (see Section 5.5).
Example 4.2.10
Consider x ∼Np(θ, σ2Ip) and θ ∼Np(0, τ2Ip). Under
quadratic loss,
δπ(x) =
σ2
σ2 + τ 2 x
and
V π(x) =
σ2τ 2
σ2 + τ 2 p.

180
Bayesian Point Estimation
4
On the contrary, the frequentist approach gives +∞as the maximal risk
for δπ and is thus poorly adapted to this problem.
∥
4.3 Sampling models
In this section, we consider three sampling problems in which a Bayesian
approach is easy to implement. Notice that, in general, discrete models
require less prior information to build up a prior distribution. The ﬁrst
problem we consider is related to the Laplace succession rule, introduced
in 1774 by Laplace. The second problem was studied under the name of
the tramcar problem by Neyman in the 1930s. The last section studies
capture-recapture models, which are of interest in animal biology and other
population estimation modelings. The three problems have in common that
they involve an inference on a ﬁnite population or subpopulation. These
are cases where some prior information is usually available, or where the
choice of a noninformative prior can be made rather unambiguously.
4.3.1 Laplace succession rule
Consider a population of known size N divided into two subpopulations, of
respective sizes N1 and N2 = N −N1, which are unknown. When sampling
without replacement x individuals from this population, x1 individuals be-
long to the ﬁrst subpopulation and x2 = x −x1 to the second. When no
information is available on N1, a noninformative distribution is
π(N1) =
1
N + 1II{0,1,...,N}(N1)
and the corresponding posterior distribution of N1 is (x1 ≤N1 ≤N −(x−
x1))
π(N1|x1) =
	N1
x1

	N−N1
x−x1

N
i=0
	 i
x1

	 N−i
x−x1

 =
	N1
x1

	N−N1
x−x1

	N+1
x+1

.
Let E be the event that the next draw will give an individual from the ﬁrst
subpopulation, p being the probability of E. Then
P(E|N1, x1) = N1 −x1
N −x .
Therefore,
P(E, N1|x1)
=
N1 −x1
N −x
	N1
x1

	N−N1
x−x1

	N+1
x+1

=
x1 + 1
N −x
	 N1
x1+1

	N−N1
x−x1

	N+1
x+1

,

4.3
Sampling models
181
and
p = P(E|x1) = x1 + 1
N −x
	N+1
x+2

	N+1
x+1

 = x1 + 1
x + 2 ,
which does not depend on N. Therefore, the predictive distribution of the
outcome of the (x+1)th draw is a Bernoulli distribution, B(1, (x1+1)/(x+
2)). Laplace considered the special case x = x1 and derived his succession
rule: If n ﬁrst draws all give an outcome from the same subpopulation, the
probability that the next draw will also give an outcome from this population
is n+1
n+2. A consequence of the Laplace succession rule is that the probability
that the whole population is of the same kind as the n ﬁrst observations
is n+1
N+1. Some criticized this succession rule as being biased in favor of the
most common subpopulation, since rare populations will not be detected
(see also Popper (1983)). On the contrary, Jeﬀreys (1961, §3.3.3) maintains
that, in Physics at least, this rule leads quite often to rejection of the
proposed distributions.
4.3.2 The tramcar problem
Jeﬀreys (1961, p. 238) mentions the following problem, attributed to Ney-
man: “A man traveling in a foreign country has to change trains at a junc-
tion, and goes into the town, the existence of which he has only just heard.
He has no idea of its size. The ﬁrst thing that he sees is a tramcar numbered
100. What can he infer about the number of tramcars in the town? It may
be assumed that they are numbered consecutively from 1 upwards.” Clearly,
this problem has less anecdotal applications. For instance, it is related to
many coincidence problems as described in Diaconis and Mosteller (1989).
Example 4.3.1
Consider a cyclic phenomenon with unknown period T
and K possible states (stock market crises, comet occurrences, genetic mu-
tations, traﬃc lights, etc.) and the observation that, at times t1 and t2, the
phenomenon is in the same state. The inferential problem is to derive T
from the observation of the diﬀerence t2 −t1.
∥
In the case of the tramcar problem, the total number N can take the
values 1, 2, . . .. Instead of a uniform distribution on IN∗, it is preferable to
consider the noninformative distribution
π(N) = 1
N ,
since N can be interpreted as a scale parameter. Moreover, the uniform
prior does not lead to a deﬁned posterior distribution. If T is the observed
tramcar number, it is distributed as
f(t|N) = P(T = t|N) = 1
N
(t = 1, 2, . . ., N).
Therefore,
π(N|T ) ∝
1
N 2 II(N≥T )

182
Bayesian Point Estimation
4
Table 4.3.1. Probability parameters for a capture-recapture experiment.
Sample 2
captured
missed
Sample 1
captured
pi
11
pi
12
missed
pi
21
pi
22
and
P π(N ≥n0|T ) =
+∞
n=n0 1/n2
+∞
n=T 1/n2 ≈
 +∞
n0
(1/x2)dx
 +∞
T
(1/x2)dx
= T
n0
.
In this case, the posterior median is approximately N π(T ) ≈2T , which
is the estimator usually considered in the tramcar problem. In fact, notice
that the mean of T conditional upon N is N−1
2
≈N
2 .
4.3.3 Capture-recapture models
When working with an hypergeometric distribution H(N, n, p), the param-
eter p is most often the parameter of interest but it may also occur that the
population size, N, is unknown and has to be estimated. More generally,
in cases where the census of a population is impossible (or too costly), it
is necessary to provide estimation methods for its size.
Example 4.3.2
An herd of deer is living on an island of Newfoundland
that is isolated from any predator. To prevent the deer from disturbing
the ecological equilibrium of the island, it is necessary to keep the number
of deer under 40 by culling. An annual census of all deer is, however, too
time-consuming.
∥
We could mention many examples in biology, sociology, psychology, mete-
orology, ecology, etc., where an evaluation of a population size is necessary.
For instance, the capture-recapture methods exposed here are used in the
U.S. census to account for undercount on some populations badly estimated
by usual census techniques, such as nomads, homeless people or illegal im-
migrants. The usual approach is called capture-recapture because it relies
on taking at least two successive samples from the population of interest,
and because it was ﬁrst implemented in animal biology, where individu-
als were actually captured. See Seber (1983, 1986) and Pollock (1991) for
surveys.
We use in this section the general framework of Wolter (1986), who shows
that most capture-recapture models can be described by a multinomial
distribution for each individual i in the population (1 ≤i ≤N). Table 4.3.3
describes the probabilities of capture, with pi
11 + pi
12 + pi
21 + pi
22 = 1. For
instance, pi
12 represents the probability of being captured only in the ﬁrst

4.3
Sampling models
183
Table 4.3.2. Population division corresponding to the model of Table 4.3.3.
Sample 2
captured
missed
Sample 1
captured
n11
n12
missed
n21
n22
sample. After the two capture experiments, the population is divided as in
Table 4.3.3, with n11+n12+n21+n22 = N (the fourth sample size n22 being
unknown). For the simplest model, called uniform, each individual has
the same probability p of being captured in both experiments. Therefore,
p11 = p2, p12 = p21 = p(1 −p), and p22 = (1 −p)2. The likelihood can be
written
L(N, p|n11, n12, n21) =

N
n11 n21 n21

pn·(1 −p)2N−n·,
where n· = 2n11 + n12 + n21 is the total number of captured individuals
and

N
n11 n12 n21

=
N!
n11! n21! n12! n22!
is the multinomial coeﬃcient. For π(N, p) = π(N)π(p) with π(p) a beta
distribution Be(α, β), the conditional posterior distribution on p is
π(p|N, n11, n12, n21) ∝pα+n·−1(1 −p)β+2N−n·−1,
i.e.,
p|N, n· ∼Be(α + n·, β + 2N −n·).
Unfortunately, the marginal posterior distribution of N is quite complica-
ted. For instance, if π(N) = 1, it satisﬁes
π(N|n·) ∝
 N
n+
 B(α + n·, β + 2N −n·)
B(α, β)
,
(4.3.1)
where n+ = n11 +n12 +n21 is the number of diﬀerent captured individuals.
This distribution is sometimes called a beta-Pascal distribution (see Raiﬀa
and Schlaifer (1961)), but it is intractable. The same complexity occurs if
π(N) = 1/N as in Castledine (1981) or if π(N) is a Poisson distribution
P(λ) as in Raftery (1988), George and Robert (1992), and Dupuis (1995).
Obviously, N being an integer, it is always possible to approximate the
normalizing factor in (4.3.1) by summing out on N. But, apart from the
required computing time, the approximation errors can become important
when N and n+ are large. Notice that, for the Poisson prior,
N −n+|n+, p ∼P((1 −p)2λ),
therefore the conditional posterior distributions are available (Chapter 6
makes use of this property). Extensions of the uniform model are described
in Wolter (1986), George and Robert (1992) and Dupuis (1995).

184
Bayesian Point Estimation
4
A simpler model used in capture-recapture settings is the hypergeometric
model, also called the Darroch model (Darroch (1958)), in which the two
sample sizes n1 = n11 + n12 and n2 = n11 + n21 are ﬁxed. In this case, the
above description does not apply and the only remaining random variable
is n11, with distribution H(N, n2, n1
N ). In fact, the values n1 and n2 are not
ﬁxed in advance, but rather determined by a stopping rule that is generally
unknown. However, if the prior distribution on N is noninformative and
with a countable support, the computation of the Bayes estimators is of the
same order of complexity. But, since the Darroch model can be written as
a particular case of the Wolter model (see Exercise 4.34), it is then possible
to extend the approximation techniques developed for Wolter’s modeling
to this setting (see Dupuis (1995) and Chapter 6).
For the Darroch model, the classical estimator of N is the maximum
likelihood estimator
ˆN =
n1
(n11/n2),
which identiﬁes the proportion in the population (n1/N) and the proportion
in the sample (n11/n2). This estimator presents an important drawback:
it cannot be used when n11 = 0. This situation would call for a third
draw of n3 individuals, and the observation of n111 individuals common
with the ﬁrst or the second sample. Since the number of tagged individuals
increases with the number of samples, the probability of observing only new
individuals at each draw decreases. It is nonetheless unreasonable to call for
additional sampling when the initial objective of the statistical modeling
was to reduce sampling costs.
A Bayesian analysis does not suﬀer from this defect because it reaches
a conclusion even when n11 = 0. Given a prior distribution π on N, it is
formally easy to derive the posterior π(N = n|n11) and draw an inference
on N.
Example 4.3.3 (Example 4.3.2 continued) Birth and death patterns
for the deer imply that the number of deer varies between 36 and 50. A
more thorough biological study of the deer life expectancy could certainly
help to reﬁne the prior modeling on N, but we will use here a uniform
distribution on {36, . . ., 50}. Considering n1 = n2 = 5, the Bayes formula,
π(N = n|n11) =
 n1
n11

n2
n2 −n11

/
 n
n2

π(N = n)
50

k=36
 n1
n11

n2
n2 −n11

/
 k
n2

π(N = k)
,
leads to Table 4.3.3, which provides the posterior distribution of N.
Since the complete posterior distribution of N is available, we can derive
the posterior mean, median, or mode of N (or any other Bayes estimator).
Table 4.3.3 gives the posterior means for diﬀerent values of n11 (to be
compared with the classical estimator 25/n11 for n11 ̸= 0, which varies

4.3
Sampling models
185
Table 4.3.3. Posterior distribution of the deer population size, π(N|n11).
n11
N
0
1
2
3
4
5
36
0.058
0.072
0.089
0.106
0.125
0.144
37
0.059
0.072
0.085
0.098
0.111
0.124
38
0.061
0.071
0.081
0.090
0.100
0.108
39
0.062
0.070
0.077
0.084
0.089
0.094
40
0.063
0.069
0.074
0.078
0.081
0.082
41
0.065
0.068
0.071
0.072
0.073
0.072
42
0.066
0.068
0.067
0.067
0.066
0.064
43
0.067
0.067
0.065
0.063
0.060
0.056
44
0.068
0.066
0.062
0.059
0.054
0.050
45
0.069
0.065
0.060
0.055
0.050
0.044
46
0.070
0.064
0.058
0.051
0.045
0.040
47
0.071
0.063
0.056
0.048
0.041
0.035
48
0.072
0.063
0.054
0.045
0.038
0.032
49
0.073
0.062
0.052
0.043
0.035
0.028
50
0.074
0.061
0.050
0.040
0.032
0.026
Table 4.3.4. Posterior mean of the deer population size, N.
n11
0
1
2
3
4
5
IE(N|n11)
43.32
42.77
42.23
41.71
41.23
40.78
much more widely with n11).
If, instead of the squared error, we use the loss
L(N, δ) =

10(δ −N)
if δ > N,
N −δ
otherwise ,
(4.3.2)
in order to avoid an overestimation of the number of deer (this would be
more dramatic for the future of the herd than an underestimation), the
Bayes estimator is the (1/11)-quantile of π(N|n11), given in Table 4.3.3
for diﬀerent values of n11. Notice that, in this case, the estimators are
necessarily integers.
∥
A very interesting Bayesian application of capture-recapture inference is
given by Mosteller and Wallace (1984). It deals with author identiﬁcation
by statistical linguistics when the origin of some literary work is uncertain.
For instance, Mosteller and Wallace (1984) study the Federalist Papers, a
collection of articles written in 1787 in order to support the new American
constitution. Twelve of these articles can be attributed to either Hamilton
or Madison. Using authenticated writings of the two authors, Mosteller and

186
Bayesian Point Estimation
4
Table 4.3.5. Estimator of the deer population size under asymmetric loss (4.3.2).
n11
0
1
2
3
4
5
δπ(n11)
37
37
37
36
36
36
Wallace derive the frequencies of thirty current words and deduce from a
capture-recapture approach that the twelve articles should be attributed
to Madison. Efron and Thisted (1976) have also used capture-recapture to
study Shakespeare’s, vocabulary and later identify in Thisted and Efron
(1987) a recently discovered poem as quite likely to have been written by
Shakespeare.
4.4 The particular case of the normal model
4.4.1 Introduction
When Gauss introduced the normal distribution around 1810, Laplace
thought this was actually the ideal error distribution (see Example 1.2.5).
Later, relying on the Central Limit Theorem, statisticians in the ﬁrst half
of the nineteenth century were almost always referring to the normal dis-
tribution (Stigler (1986)). There are obviously many phenomena for which
a normal model is not applicable, but it is still extensively used, in par-
ticular, in econometrics and in ﬁelds where the Central Limit Theorem
approximation can be justiﬁed (particle physics, etc.). In fact, the normal
approximation is often justiﬁed for asymptotic reasons (see also Cox and
Reid (1987)). Therefore, it is of interest to study in detail this particular
distribution from a Bayesian viewpoint.
Given one observation from a multivariate normal distribution, Np(θ, Σ),
with known covariance matrix Σ, the conjugate distribution is also normal,
Np(μ, A), and the posterior distribution π(θ|x) is
Np
	
x −Σ(Σ + A)−1(x −μ), (A−1 + Σ−1)−1
.
Under quadratic loss, the Bayes estimator is then the posterior mean
δπ(x)
=
x −Σ(Σ + A)−1(x −μ)
=
	
Σ−1 + A−1
−1 	
Σ−1x + A−1μ

;
notice that δπ(x) can be written as a convex combination of the observation,
x, and of the prior mean, μ, the weights being proportional to the inverses
of the covariance matrix.
The more accurate the prior information on θ is, the closer to μ the Bayes
estimator is. Notice also that the prior information (resp., the observation
of x) brings a reduction of the variance from Σ (respectively, from A)
to
	
Σ−1 + A−1
−1. For repeated observations of the above normal model,

4.4
The particular case of the normal model
187
x1, ..., xn, the suﬃcient statistic
¯x = 1
n
n

i=1
xi ∼Np

θ, 1
nΣ

directly extends the previous analysis.
A criticism already mentioned in Chapter 3 is that the conjugate normal
prior distributions are not suﬃciently robust, and that it would be prefer-
able to use Student’s t-distributions for π(θ). The Cauchy distribution, the
limiting case of a Student’s t-distribution, can then be used because of its
heavier tails, but it still prohibits exact computation (see Example 4.2.1),
although Angers (1992) proposes an analytical resolution using conﬂuent
hypergeometric functions.
4.4.2 Estimation of variance
In most cases, the variance of the model is partly or totally unknown. It
is then necessary to consider prior distributions on the parameter (θ, Σ).
If the variance is known up to a multiplicative constant, σ2, it is usually
possible to get back to the unidimensional case, that is, when x1, . . . , xn are
i.i.d. N(θ, σ2), by suﬃciency reasons. (The particular case where σ2 only
is unknown is treated in Tables 3.3.4 and 4.3.3). If we deﬁne the statistics
¯x = 1
n
n
i=1 xi and s2 = n
i=1(xi −¯x)2, the likelihood can be written
ℓ(θ, σ | ¯x, s2) ∝σ−n exp

−1
2σ2

s2 + n (¯x −θ)2 
and the Bayes estimators only depend on ¯x and s2. We showed in Example
3.5.5 that the Jeﬀreys distribution for this model is π∗(θ, σ) =
1
σ2 and
mentioned that it is better to consider the alternative ˜π(θ, σ) =
1
σ for
invariance reasons. In this case,
ℓ(θ, σ | ¯x, s2)˜π(θ, σ) ∝σ−n−1 exp

−1
2σ2

s2 + n (¯x −θ)2 
.
(4.4.1)
Therefore,
Proposition 4.4.1
If x1, . . . , xn are i.i.d. N(θ, σ2), the posterior distri-
bution of (θ, σ) associated with ˜π is
θ|σ, ¯x, s2
∼
N

¯x, σ2
n

,
σ2|¯x, s2
∼
IG
n −1
2
, s2
2

.
(4.4.2)
Equation (4.4.2) really deﬁnes the posterior distribution of (θ, σ2), since
it provides the marginal distribution of σ2 and the distribution of θ condi-
tional on σ2. The proof of this proposition is a direct consequence of (4.4.1),

188
Bayesian Point Estimation
4
as
˜π(θ, σ2|¯x, s2) ∝σ−1e−n(¯x−θ)2/2σ2σ−ne−s2/2σ2σ−1,
and the inverse gamma distribution IG(α, β) has the density
π(x|α, β) =
βα
Γ(α)xα+1 e−β/xII(0,+∞)(x).
(4.4.3)
Thus, the marginal posterior distribution of σ2 is of the same type than
when θ is known. On the contrary, the marginal posterior distribution of θ
is modiﬁed, since it follows from (4.4.2) that
˜π(θ|¯x, s2) ∝

s2 + n(¯x −θ)2−n/2 ,
i.e.,
θ|¯x, s2 ∼T1

n −1, ¯x,
s2
n(n −1)

.
(4.4.4)
For the Jeﬀreys distribution, π∗, the equivalent of (4.4.4) is a Student’s
t-distribution with n degrees of freedom, which is always deﬁned, while
(4.4.4) is only deﬁned for n ≥2. (Notice that the exclusion of n = 1 could
be seen as an additional argument in favor of ˜π since, in a noninformative
setting, it seems diﬃcult to propose an inference about the whole parameter
(θ, σ) with a single observation.)
Conjugate posterior distributions have the same form as (4.4.2). They
exhibit a puzzling peculiarity, namely, that θ and σ2 are not a priori inde-
pendent. Therefore, the prior distribution on the mean θ depends on the
precision associated with the measure of the mean. Some settings can jus-
tify this dependence,2 but it cannot hold for every estimation problem, and
can even less be argued to be a representative standard prior distribution
(see Berger (1985a)). However, these subjective criticisms are not seconded
by particularly negative properties of the resulting estimators.
Consider then
π(θ, σ2) = π1(θ|σ2)π2(σ2),
where π1 is a normal distribution N(μ, σ2/n0) and π2 is a inverse gamma
distribution IG(ν/2, s2
0/2). The posterior distribution satisﬁes
π(θ, σ2|x)
∝
σ−n−ν−3 exp

−1
2
+
s2 + s2
0 + n0(θ −μ)2 + n(¯x −θ)2,
/σ2

=
σ−n−ν−3 exp

−1
2
+
s2
1 + n1(θ −θ1)2,
/σ2

,
where
n1
=
n + n0,
θ1 = 1
n1
(n0θ0 + n¯x) ,
s2
1
=
s2 + s2
0 +
	
n−1
0
+ n−1
−1 (θ0 −¯x)2 .
2 When the prior distribution is built from previous observations, it makes sense that
the prior variance of θ involves σ2 (conditionally).

4.4
The particular case of the normal model
189
These distributions are actually conjugate since
π
	
θ|¯x, s2, σ

∝
1
σ exp

−n1(θ −θ1)2
2σ2

,
π
	
σ2|¯x, s2
∝
σ−n−ν−2 exp

−s2
1/2σ2
.
As in the noninformative case, the marginal posterior distribution of θ is a
Student’s t-distribution. Notice that, except when π is built from previous
(or virtual) observations, n0 is not a sample size. Rather, n0/n character-
izes the relative precision of the determination of the prior distribution as
compared with the precision of the observations. In general, n0 is smaller
than the sample size n. Notice also that, if n0/n goes to 0, we get the limit-
ing case θ|¯x, σ2 ∼N(¯x, σ2/n), corresponding to the posterior distribution
associated with the Jeﬀreys prior. This fact is yet another illustration of
the phenomenon that noninformative distributions often occur as limits of
conjugate distributions.
Statistical inference based on the above conjugate distribution requires
a careful determination of the hyperparameters θ0, s2
0, n0, ν, in order to
express the Bayes estimators. If the determination of θ0 and n0 is rather
classical, it is usually more diﬃcult to get a prior information on σ2. Recall
that, if σ2 ∼IG(ν/2, s2
0/2), the ﬁrst two moments are given by
IEπ +
σ2,
=
s2
0
ν −2,
varπ(σ2) =
2s4
0
(ν −2)2(ν −4).
These formulas can then be used to model a prior information into the
conjugate form, i.e., to determine s2
0 and ν.
When the parameter (θ, Σ) is totally unknown, it is still possible to derive
conjugate prior distributions. Given n observations x1, . . . , xn of Np(θ, Σ),
a suﬃcient statistic is
¯x = 1
n
n

i=1
xi,
S =
n

i=1
(xi −¯x)(xi −¯x)t,
and
ℓ(θ, Σ|¯x, S) ∝|Σ|−n/2 exp −1
2

n(¯x −θ)tΣ−1(¯x −θ) + tr(Σ−1S

}.
The form of the likelihood function then suggests the following conjugate
distributions:
θ|Σ
∼
Np

μ, Σ
n0

,
Σ−1
∼
Wp(α, W),
(4.4.5)
where Wp denotes the Wishart distribution, deﬁned in Exercise 3.21. The
posterior distributions are then
θ|Σ, ¯x, S
∼
Np
n0μ + n¯x
n0 + n ,
Σ
n0 + n

,

190
Bayesian Point Estimation
4
Σ−1|¯x, S
∼
Wp (α + n, W1(¯x, S)) ,
with
W1(¯x, S)−1 = W −1 + S +
nn0
n + n0
(¯x −μ)(¯x −μ)t.
Notice that this multidimensional case is the generalization of the unidi-
mensional case considered above, since the Wishart distribution Wp is the
generalization in dimension p of chi-squared distributions. Let us recall here
that the ﬁrst two moments of Ξ = (ξij) ∼Wp(α, W) are
IE[Ξ] = αW,
var(ξij) = 2αw2
ij,
and that the hyperparameters of the prior distribution of Σ can be derived
from
IE[Σ] =
W −1
α −p −1,
var(σij) =
2(wij)2
(α −p −3)(α −p −1)2 ,
if Σ−1 ∼Wp(α, W) and W −1 = (wij) (see Eaton (1982) and Anderson
(1984)).
In this setting, the Jeﬀreys distribution is also a limiting case of conjugate
distributions since Geisser and Cornﬁeld (1963) have shown that it is
πJ(θ, Σ) =
1
|Σ|(p+1)/2 ,
therefore corresponding to the limit of the Wishart distribution Wp(α, W)
on Σ−1 when W −1 goes to O and α to 0. In fact, the density of Σ when
Σ−1 ∼Wp(α, W) is
f(Σ|α, W) ∝|Σ|−(α+p+1)/2 exp

−1
2tr(W −1Σ−1)

(see Anderson (1984)).
4.4.3 Linear models and G–priors
The usual regression model,
y = Xβ + ϵ,
(4.4.6)
with ϵ ∼Nk(0, Σ), β ∈IRp, can be analyzed as above if the covariance
matrix Σ is known, when working conditional upon X. In fact, a suﬃcient
statistic is then ˆβ = (XtΣ−1X)−1XtΣ−1y, the maximum likelihood esti-
mator and the least-squares estimator of β. It is distributed according to a
Np(β, (XtΣ−1X)−1) distribution. Lindley and Smith (1972) have studied
conjugate distributions of the type
β ∼Np(Aθ, C),
where θ ∈IRq (q ≤p). In this model, the regressor matrix X is considered
to be constant. In other words, the inference is made conditional upon X.
(Usually, X is also partly random, but this conditioning is justiﬁed by the

4.4
The particular case of the normal model
191
Likelihood Principle as long as the distribution of X does not depend on
the parameters of the regression model.) Therefore, A, C, or θ can depend
on X (see below for the example of the G-priors of Zellner (1971)). When
the stochastic nature of X has to be considered, the usual approach is to
study a random-eﬀect model,
y = X1β1 + X1X2β2 + ϵ,
which can be decomposed as
y|θ1
∼
Nk(X1θ1, Σ1),
θ1|θ2
∼
Np(X2θ2, Σ2),
with a prior distribution
θ2|θ3 ∼Nq(X3θ3, Σ3).
Smith (1973) analyzes this model and shows that
θ1|y, θ3 ∼Np(θ∗
1, D1),
with
θ∗
1
=
D1
/
ˆD−1
1 ˆθ1 + (Σ2 + X2Σ3Xt
2)−1X2X3θ3
0
,
D−1
1
=
ˆD−1
1
+ (Σ2 + X2Σ3Xt
2)−1,
involving the classical least-squares estimators
ˆD−1
1
=
Xt
2Σ−1
1 X2,
ˆθ1
=
ˆD1Xt
2Σ−1
1 y.
Therefore, the Bayes estimator θ∗
1 is a convex combination of the least-
squares estimator, ˆθ1, and of the prior mean, X2X3θ3.
We introduce below an example where an unknown variance structure
still allows for closed-form derivation of the Bayes estimators. However, if
the variance Σ is totally unknown, it is not possible to derive conjugate
prior distributions, as noticed by Lindley and Smith (1972). Press (1989)
proposes a resolution in a particular case when independent observations
are available for every component of β. In the general case, the Jeﬀreys
prior is again (see Geisser and Cornﬁeld (1963))
πJ(β, Σ) =
1
|Σ|(k+1)/2 .
The likelihood
ℓ(β, Σ|y) ∝|Σ|−n/2 exp

−1
2tr

Σ−1
n

i=1
(yi −Xiβ)(yi −Xiβ)t

,
then suggests the use of Wishart distributions, but the posterior marginal
distributions on β are only deﬁned for suﬃciently large sample sizes and,
moreover, are not explicit, whatever the sample size (see Exercise 4.44).

192
Bayesian Point Estimation
4
In the particular case when the variance of the model (4.4.6) is known
up to a multiplicative factor σ2, it is possible to rewrite the model as
ϵ ∼Nk(0, σ2Ik) and the least-squares estimator ˆβ has a normal distribution
Np(β, σ2(XtX)−1). A family of conjugate distributions on (β, σ2) is then
β|σ2
∼
Np

μ, σ2
n0
(XtX)−1

,
σ2
∼
IG(ν/2, s2
0/2),
(4.4.7)
since, if s2 = ||y −X ˆβ||2, the posterior distributions are
β|ˆβ, s2, σ2
∼
Np
!
n0μ + ˆβ
n0 + 1 ,
σ2
n0 + 1(XtX)−1
"
,
σ2|ˆβ, s2
∼
IG
!
k −p + ν
2
,
s2 + s2
0 +
n0
n0+1(μ −ˆβ)tXtX(μ −ˆβ)
2
"
.
Indeed,
π(β, σ2|ˆβ, s2) ∝(σ2)−k/2 exp

−1
2σ2

(β −ˆβ)tXtX(β −ˆβ) + s2 
× exp

−n0
2σ2 (β −μ)tXtX(β −μ)

σ2)−ν/2−1 exp

−s2
0
2σ2

∝(σ2)−p/2 exp
⎧
⎨
⎩−n0 + 1
2σ2
!
β −n0μ + ˆβ
n0 + 1
"t
XtX
!
β −n0μ + ˆβ
n0 + 1
"⎫
⎬
⎭
×σ−(k−p+ν+2) exp

−1
2σ2

s2
0 + s2 +
n0
n0 + 1(μ −ˆβ)tXtX(μ −ˆβ)

.
Although (4.4.7) is only a particular case of conjugate distribution, many
criticisms have been raised about this choice, developed by Zellner (1971,
1986b) under the name of G-priors. These criticisms do not usually address
the problem of the reductive aspect of conjugate modeling, a quite legiti-
mate argument already mentioned in Chapter 3, but rather the dependency
of the prior distribution on X. It can be argued that X is also a random
variable and therefore that an a priori modeling should not depend on X.
In fact, the alternative prior distributions
β|σ ∼Np(β0, σ2A)
also constitute a conjugate family which is less arguable if A is ﬁxed. How-
ever, we consider the debate to be rather vacuous because:
(1) The whole regression model is conditional on the explanatory variables.
The prior distribution (4.4.7) can then be seen as a posterior distribu-
tion with respect to these variables (or to extend the usual hypothesis
of independence between the explanatory variables and the errors to
the hypothesis of Bayesian independence with the parameters). This ap-
proach is thus justiﬁed from both the conditional and Bayesian points

4.5
Dynamic models
193
of view, the conditioning being operated in two steps.
(2) The G-prior distribution suggests a constant distribution on the mean
of y, θ = IEθ[y|X], rather than on β. The prior distribution is then
determined with respect to the subspace spanned by the columns of X,
not with respect to a special basis of this subspace.
(3) This modeling is appropriate to take into account the problems of multi-
collinearity, since it allows for a large prior variance on the components
aﬀected by multicollinearity (hence, the most diﬃcult to estimate). (See
Zellner (1971), Casella (1985b), or Steward (1987), for references on
multicollinearity.)
(4) From both practical and subjective viewpoints, the determination of a
matrix A instead of a scalar n0 requires a larger amount of prior informa-
tion. Because the use of conjugate distributions indicates a setting where
the prior information is sparse and where the derivation of hyperparame-
ters is quite diﬃcult, the resort to the covariance matrix σ2(XtX)−1/n0
prevents a possibly unrealistic determination of A.
Again, notice that these attacks on G-priors hardly ever consider its major
drawback, namely, that its choice is not based on prior information. For
applications of G-priors in regression settings, see Ghosh et al. (1989), or
Blattberg and George (1991). See Bauwens, Lubrano and Richard (1999,
Chapter 4) for alternatives to the conjugate priors in the linear model such
as poly-t priors (see Note 4.7.5 below).
4.5 Dynamic models
4.5.1 Introduction
Dynamic (or time series) models appear as a particular case of a parametric
model where the distribution of the observed variables x1, . . . , xT varies
over time, that is,
f(x1, . . . , xT |θ) =
T

t=1
ft(xt|x1:(t−1), θ) ,
(4.5.1)
where x1:(t−1) denotes the vector of the previous variables x1, . . . , xt−1, with
the convention that x1:0 is either empty or represents the initial value x0 of
the series of observations (it is then implicit on the lhs of (4.5.1)). While the
representation (4.5.1) seems to be unnecessarily restrictive, the inclusion of
unobserved components in xt provides a fairly large scope for this model,
as will be made clearer in the paragraph on state-space representations.
These models are obviously special cases of parametric models and, as
such, they can be processed like other parametric models by Bayesian tools,
once a prior distribution has been chosen, following the guidelines provided
in the previous sections. They are singled out in this section for several rea-
sons: ﬁrst, they are some of the most commonly used models in applications,

194
Bayesian Point Estimation
4
ranging from ﬁnance and economics to reliability to medical experiments
to ecology. Most of the models encountered in practice enjoy a temporal
dimension that can sometimes be concealed, but which must most often
be taken into account. This is clearly the case, for instance, for series of
pollution data, such as ozone concentration levels, or stock market prices,
whose value at time t depends on the previous value at time t −1 and also
on the previous values, for instance through their trend.
Example 4.5.1 (Example 4.1.6 continued) A ﬁnite time version of the
autoregressive AR(1) model is generally deﬁned through the distribution
of xt conditional on x1:(t−1) (1 ≤t ≤T ) as
xt = μ + ϱ(xt−1 −μ) + ϵt ,
(4.5.2)
where ϵt is independent from x1:(t−1) and can be chosen as, say, a normal
N(0, σ2) perturbation. This conditional distribution only depends on xt−1,
which shows that (xt)t≥1 is a Markov chain (Meyn and Tweedie (1993)).
The likelihood function of the AR(1) model is then
σ−T exp

−1
2σ2
T

i=1
(xt −μ −ϱ(xt−1 −μ))2

,
and thus depends on the initial condition x0. Either x0 is known and the
model is then conditional on x0, or x0 has to be integrated out by taking a
prior distribution π(x0|θ), since x0 is then an additional parameter of the
model. For instance, if x0 = 0, it is straightforward to see that IEθ[xt] = 0
and that var(xt) = ϱ2var(xt−1) + σ2, thus, if ϱ2 ̸= 1,
var(xt) = 1 −ϱ2t
1 −ϱ2 σ2 ,
(4.5.3)
which implies that var(xt) converges to σ2/(1 −ϱ2) if ϱ2 < 1 and goes to
+∞otherwise.
∥
The second reason for studying dynamic models is that they are more
challenging than the static models studied above, owing to stationarity
constraints. While we cannot present a rigorous introduction to the notion
of stationarity for stochastic processes chains (and refer the reader to Meyn
and Tweedie (1993) for a general coverage of Markov processes and to Box
and Jenkins (1976) or Brockwell and Davis (1998) for the special case of
time series), let us recall here that a process (xt) is stationary (or, more
exactly, strictly stationary) if the distribution of (xt+1, . . . , xt+d) is the
same as the distribution of (x1, . . . , xd) for every (t, d). The stationarity
problem can be illustrated in the setting of Example 4.5.1: when ϱ2 ≥
1, not only the variance var(xt) goes to inﬁnity with t, but the limiting
behavior of the chain (xt) cannot be characterized unless a dependence
between xt and ϵt is introduced (Brockwell and Davis (1998)). There is no
limiting distribution for the process (xt), because the Markov chain has no
stationary distribution, that is, there is no density f such that, if xt ∼f,

4.5
Dynamic models
195
xt+1 ∼f (Exercise 4.50). For instance, if ϱ = 1, (xt) is the random walk on
IR and, on average, it takes the chain an inﬁnite time to come back to the
set it started from (Meyn and Tweedie (1993)).
To impose the stationarity condition on a model is objectionable on the
ground that the data itself should indicate whether the underlying model
is stationary. However, for reasons ranging from asymptotics to causality
to identiﬁability (see below) to common practice, it is customary to im-
pose stationarity constraints, even though a Bayesian inference on a non-
stationary process can be conducted in principle (see Note 4.7.2). Such
constraints appear in the prior distribution as a restriction on the values
of θ. For instance, for the AR(1) model of Example 4.5.1, the constraint
is |ϱ| < 1. The practical diﬃculty is that, for more complex models, the
stationarity constraints may become much more involved and even be un-
known in some cases, as in general threshold models (Tong (1991)).
Example 4.5.2 The AR(p) model generalizes the AR(1) model by in-
creasing the lag dependence on the past values, that is (1 ≤t ≤T ),
xt −μ =
p

i=1
ϱi(xt−i −μ) + ϵt ,
ϵt ∼N(0, σ2) .
(4.5.4)
The stochastic process deﬁned by (4.5.4) can then be stationary3 only if
the roots of the polynomial
P(x) = 1 −
p

i=1
ϱixi
are all outside the unit circle in the complex plane. While this condition
is clearly deﬁned, it is also implicit with respect to the vector (ϱ1, . . . , ϱp):
to verify that a given vector satisﬁes this condition, it is necessary either
to ﬁnd the roots of the pth degree polynomial P and check that they all
are of modulus greater than 1, or to compute the partial autocorrelations
(see below in Section 4.5.2) and apply Schur’s lemma to check they are all
between −1 and 1.
∥
Example 4.5.3 A switching AR(p) model is deﬁned as an AR(p) model
whose parameters change with time according to a hidden (unobserved)
ﬁnite state Markov process, that is
xt =
p

i=1
ϱi(zt)xt−i + σ(zt)ϵt ,
ϵt ∼N(0, 1) ,
(4.5.5)
with (zt) being the unobserved Markov chain,
P(zt = i|zt−1 = j, zt−2, . . .) = πj,i ,
i, j = 1, . . . , K .
3 That (4.5.4) is stationary obviously depends on the distribution of the initial values
x(−p+1):−1. We refer the reader to Brockwell and Davis (1998, §3.1) for a more
detailed coverage of this issue, where the case of a stochastic process (xt)t∈ZZ is also
studied, resulting in diﬀerent stationarity conditions.

196
Bayesian Point Estimation
4
0
100
200
300
400
500
−10
−9
−8
−7
−6
days
Figure 4.5.1. Plot of the log transforms of averaged IBM stock prices over the
period 1992-1997.
This model was introduced by Hamilton (1989) as a way to represent se-
ries with diﬀerent dynamics over time, such as the series in Figure 4.5.1
that plots a transformation of IBM stock prices from 1992 to 1997. The
diﬃculty with the model (4.5.5) is that there was no known necessary and
suﬃcient condition for stationarity when the number of states K of the hid-
den Markov chain (zt) is larger than 2, till the recent derivation by Francq
and Zako¨ıan (2001) and Yao and Attali (2000) of such conditions.
∥
We develop in Sections 4.5.2–4.5.4 some features of the standard dynamic
models, namely the AR, MA and ARMA models, focusing on representa-
tion problems and prior modeling under stationarity. Notes 4.7.3 and 4.7.4
present two other dynamic models often encountered in practice. See West
and Harrison (1997) for a general approach to the Bayesian processing of
time series and Bauwens et al. (1999) for an econometric monograph on
this topic. Marin and Robert (2007, Chapter 7) also cover the practical
aspects of prior determination and Bayesian inference for those models.
4.5.2 The AR model
As introduced in Example 4.5.2, the AR(p) model expresses the conditional
distribution of xt given the past x1:(t−1) as a normal linear regression on
the p more recent variables, that is (t = 1, 2, . . .),
xt ∼N
!
μ +
p

i=1
ϱi(xt−i −μ), σ2
"
,
(4.5.6)
where the location parameter μ is introduced for more generality. This
model is thus Markovian, since the distribution of xt only depends on a

4.5
Dynamic models
197
ﬁxed number of past values, x(t−p):(t−1). It can be expressed as a regular
Markov chain when considering the vector zt = xt:(t−p+1), since
zt = μ1 + B(zt−1 −μ1) + εt ,
(4.5.7)
where
1 = (1, . . . , 1)t ,
B =
⎛
⎜
⎜
⎝
ϱ1
ϱ2
. . .
ϱp
1
0
. . .
0
...
0
0
0
⎞
⎟
⎟
⎠
and
εt = (ϵt, 0, . . . , 0)t .
Because the likelihood conditional on the negative time values x0, . . . , x−p+1,
can be written as
L(μ, ϱ1, . . . , ϱp, σ|x1:T , x0:(−p+1)) =
(4.5.8)
σ−T
T

t=1
exp
⎧
⎨
⎩−
!
xt −μ +
p

i=1
ϱi(xt−i −μ)
"2 7
2σ2
⎫
⎬
⎭,
a natural conjugate prior can be found for the parameter θ = (μ, ϱ1, . . . , ϱp,
σ2), that is, a normal distribution on (μ, ϱ1, . . . , ϱp) and an inverse gamma
distribution on σ2. Instead of the Jeﬀreys prior, which is controversial in
this setting (see Note 4.7.2), we can also propose a more conventional non-
informative prior like π(μ, σ, ϱ) = 1/σ.
If we impose the stationarity constraint that all the roots of P are outside
the unit circle, the parameter space is too complex for values of p larger than
3 to propose as a prior distribution the normal conjugate prior restricted
to this space. For instance, to simulate from such a prior is too costly.
A solution, called the Durbin–Levinson recursion (see Monahan (1984)),
is to propose a reparameterization from the parameters ϱi to the partial
autocorrelations ψi (Exercise 4.53) which satisfy, under the stationarity
constraint,
ψi ∈(−1, 1) , i = 1, · · · , p ,
and thus allow for a uniform prior.4 The following result provides a con-
structive connection between (ϱ1, . . . , ϱp) and (ψ1, . . . , ψp).
Lemma 4.5.4 Under stationarity of the model (4.5.6), the coeﬃcients ϱi
are deduced from the coeﬃcients ψi by the algorithm
0. Deﬁne ϕii = ψi and ϕij = ϕ(i−1)j −ψiϕ(i−1)(i−j), for i > 1 and j =
1, · · · , i −1 .
1. Take ϱi = ϕpi for i = 1, · · · , p.
4 The partial autocorrelations, also called reﬂection coeﬃcients in the signal processing
literature, can be used to test for stationarity since, according to Schur’s lemma they
must all be between −1 and 1 for the Markov kernel (4.5.6) to have a stationary
distribution.

198
Bayesian Point Estimation
4
While the resulting prior (and posterior) distribution on (ϱ1, . . . , ϱp) is
not explicit, in the sense that the computation of the prior (or of the pos-
terior) at a given value of the parameter is quite time-consuming, this
representation can be exploited for simulation purposes, as in Chapter 6
(see Barnett et al. (1996)), because of the linearity of the relation between
the ϱj’s and a given ψi, conditional upon the other ψℓ’s. Huerta and West
(2000) propose a diﬀerent approach via the real and complex roots of the
polynomial P, whose inverses are also within the unit circle. (See also Marin
and Robert (2007, Chapter 7) for an implementation of this approach.)
4.5.3 The MA model
A fundamental result in the theory of stochastic processes is the Wold
decomposition, which states that most stationary processes (xt)t≥1 can be
represented as
xt = μ +
∞

i=0
ψiϵt−i ,
(4.5.9)
where ψ0 = 1 and (ϵt)t∈ZZ is a white noise, that is, a sequence of random
variables with zero mean, ﬁxed variance and zero covariance. See Box and
Jenkins (1976) for theoretical details.
Example 4.5.5 (Example 4.1.6 continued) If xt = ϱxt−1 + ϵt, xt can
also be written as
xt = ϵt + ϱϵt−1 + ϱ2ϵt−2 + . . .
if |ϱ| < 1.
∥
The MA(q) model, where MA stands for moving average, is a special case
of (4.5.9) when the ψi’s are equal to 0 for i > q, that is,
xt = μ + ϵt −
q

j=1
ϑjϵt−j ,
ϵt ∼N(0, σ2)
(4.5.10)
In contrast with the AR(1) model, where the covariance between the terms
of the series is exponentially decreasing to 0 but always remains diﬀerent
from 0, the MA(q) process is then such that the autocovariances
γs = cov(xt, xt+s)
are equal to 0 for |s| > q. According to the Wold decomposition, the MA(q)
process is stationary, whatever the vector (ϑ1, . . . , ϑq). However, invertibil-
ity and identiﬁability considerations (see Exercise 4.58) lead to the condi-
tion that the polynomial
Q(x) = 1 −
q

j=1
ϑjxj
must have all its roots outside the unit circle.

4.5
Dynamic models
199
Example 4.5.6 In the particular case of the MA(1) model, xt = μ + ϵt −
ϑ1ϵt−1 and var(xt) = (1 + ϑ2
1)σ2, while γ1 = ϑ1σ2. Then xt can also be
written as
xt = μ + ˜ϵt−1 −1
ϑ1
˜ϵt,
˜ϵ ∼N(0, ϑ2
1σ2) ,
which shows that the couples (ϑ1, σ) and (1/ϑ1, ϑ1σ) lead to two alternative
representations of the same model. This somehow justiﬁes the restriction
to |ϑ1| < 1.
∥
Contrary to the AR(p) model, this model is not Markovian per se (even
though it can be represented as a Markov process using the state-space
representation introduced below). While the whole vector x1:T is a normal
random variable with constant mean μ and covariance matrix
Σ =
⎛
⎜
⎜
⎝
σ2
γ1
γ2
. . .
γq
0
. . .
0
0
γ1
σ2
γ1
. . .
γq−1
γq
. . .
0
0
...
0
0
0
. . .
0
0
. . .
γ1
σ2
⎞
⎟
⎟
⎠,
with (|s| ≤q)
γs = σ2
q−|s|

i=0
ϑiϑi+|s| ,
(4.5.11)
and thus provides an explicit likelihood function, the computation and
obviously the integration (or maximization) of this likelihood for a given
value of the parameter is quite costly, since it involves inverting the (n, n)
matrix Σ. A more manageable representation is to use the likelihood of
x1:T conditional on (ϵ0, . . . , ϵ−q+1),
L(μ, ϑ1, . . . , ϑq, σ|x1:T , ϵ0, . . . , ϵ−q+1) =
(4.5.12)
σ−T
T

t=1
exp
⎧
⎪
⎨
⎪
⎩
−
⎛
⎝xt −μ +
q

j=1
ϑjˆϵt−j
⎞
⎠
2
7
2σ2
⎫
⎪
⎬
⎪
⎭
,
where (t > 0)
ˆϵt = xt −μ +
q

j=1
ϑjˆϵt−j
(4.5.13)
and ˆϵ0 = ϵ0, . . ., ˆϵ1−q = ϵ1−q. This recursive deﬁnition of the likelihood is
still costly, since it involves T sums of q terms. Nonetheless, even though the
problem of the conditioning values (ϵ0, . . . , ϵ−q+1) must be treated separa-
tely, for instance via an MCMC step (Chapter 6), the complexity of this
representation is much more manageable than the normal representation
given above.
Another approach of interest to the representation of the MA(q) model
is to use the so-called state-space representation, inspired by the Kalman
ﬁlter, which provides recursive linear formulas for prediction, smoothing and

200
Bayesian Point Estimation
4
ﬁltering. Brockwell and Davis (1996, Chapter 8) provide a general coverage
of this technique, while West and Harrison (1997) present its Bayesian
version, but the general idea is to represent a time series (xt)t≥1 as a
system of two equations,
xt
=
Gyyt + εt ,
(4.5.14)
yt+1
=
Ftyt + ξt ,
(4.5.15)
where the vectors εt and ξt are multivariate normal vectors with gen-
eral covariance matrices that may depend on t and IE[εuξ′
v] = 0 for all
(u, v)’s. Equation (4.5.14) is called the observation equation and (4.5.15) is
called the state equation. This representation projects the process of inter-
est (xt)t≥1 into a larger space, the state space, where the process (yt)t≥1 is
Markovian and linear. For instance, (4.5.7) is a state-space representation
of the AR(p) model.
The MA(q) model can be written that way by deﬁning yt = (ϵt−q, . . . ,
ϵt−1, ϵt)′. Then the state equation is
yt+1 =
⎛
⎜
⎜
⎜
⎝
0
1
0
. . .
0
0
0
1
. . .
0
. . .
0
0
0
. . .
1
0
0
0
. . .
0
⎞
⎟
⎟
⎟
⎠yt + ϵt+1
⎛
⎜
⎜
⎜
⎜
⎝
0
0
...
0
1
⎞
⎟
⎟
⎟
⎟
⎠
(4.5.16)
while the observation equation is
xt = μ −( ϑq
ϑq−1
. . .
ϑ1
−1 ) yt .
This decomposition thus involves no vector εt in the observation equation,
while ξt is degenerate in the state equation. This degeneracy phenomenon
is quite common in state-space representations, but this is not a hindrance
in conditional uses of the model, as in the MCMC algorithms of Chapter
6. Notice also that the state-space representation of a model is not unique.
Example 4.5.7 (Example 4.5.6 continued)
For the MA(1) model,
the observation equation can also be xt = ( 1
0 )yt with yt = ( y1t
y2t )′
directed by the state equation
yt+1 =

0
1
0
0

yt + ϵt+1

1
ϑ1

.
∥
Whatever the representation chosen for the MA(q) model, the identiﬁ-
ability condition on Q(x) imposes to the ϑj’s to vary in a complex space,
which cannot be directly described for values of q larger than 3. The repa-
rameterization described in Lemma 4.5.4 also formally applies in this case,
with a diﬀerent interpretation of the ψi’s which are then the inverse partial
auto-correlations (Jones (1987)). A uniform prior on the ψi’s can be used
for the estimation of the ϑi’s, which necessarily goes through an MCMC
step (see Chapter 6, Chib and Greenberg (1994), Barnett et al. (1996),
Billio et al. (1999), Marin and Robert (2007, Chapter 7)).

4.6
Exercises
201
4.5.4 The ARMA model
A straightforward extension of the previous model is the ARMA(p, q) model,
where (t = 1, 2, . . .)
xt = μ +
p

i=1
ϱi(xt−i −μ) + ϵt −
q

j=1
ϑjϵt−j ,
(4.5.17)
where the ϵt’s are i.i.d. N(0, σ2). The role of such models, as compared
with both AR and MA models, is to aim towards parsimony, that is, to use
much smaller values of p and q than in a pure AR or a pure MA modeling.
As detailed in Box and Jenkins (1976), the stationarity and identiﬁability
conditions still correspond to the roots of the polynomials P and Q being
outside the unit circle, with a further condition that both polynomials have
no common root. (But this almost surely never happens under a continuous
prior on the parameters.) The reparameterization of Lemma 4.5.4 (1984)
can therefore be implemented for both the ϑi’s and the ϱj’s, still calling for
MCMC techniques owing to the complexity of the posterior distribution.
State-space representations also exist for ARMA(p, q) models, one pos-
sibility being (Brockwell and Davis (1997, Example 8.3.2))
xt = μ −( ϑr−1
ϑr−2
. . .
ϑ1
−1 ) yt
for the observation equation and
yt+1 =
⎛
⎜
⎜
⎜
⎝
0
1
0
. . .
0
0
0
1
. . .
0
. . .
0
0
0
. . .
1
ϱr
ϱr−1
ϱr−2
. . .
ϱ1
⎞
⎟
⎟
⎟
⎠yt + ϵt+1
⎛
⎜
⎜
⎜
⎜
⎝
0
0
...
0
1
⎞
⎟
⎟
⎟
⎟
⎠
,
(4.5.18)
for the state equation, with r = max(p, q + 1) and the convention that
ϱt = 0 if t > p and ϑt = 0 if t > q. As for the MA(q) models, this
representation is handy to devise MCMC algorithms (see Chapter 6) to
handle the simulation of the posterior distribution of the parameters of the
ARMA(p, q) model.
4.6 Exercises
Section 4.1
4.1 (Smith (1988)) Consider x, a random variable with mean μ, c.d.f. F, and
density f. The two functions f and f ′ are supposed to be bounded. Deﬁne a
sequence of random variables yn with c.d.f.’s
Gn(y) =

1 −1
n

F(y) + 1
nHn(y),
satisfying
(i) IEHn[y] = n2; and

202
Bayesian Point Estimation
4
(ii) H′
n = hn and h′
n are bounded.
Show that Gn →F, G′
n = gn →f, and g′
n →f ′, but that |μ −IE[yn]| →∞.
4.2 If ψ(θ|x) is a posterior distribution associated with f(x|θ) and a (possibly
improper) prior distribution π, show that
ψ(θ|x)
f(x|θ) = k(x)π(θ).
a. Deduce that, if f belongs to an exponential family, the posterior distribu-
tion also belongs to an exponential family, whatever π is.
b. Show that if ψ belongs to an exponential family, the same holds for f.
4.3 *(Berger and Wolpert (1988)) In the following setting, Stein (1962b) points
out some limitations of the Likelihood Principle. Assume that a value θ > 0
can be assessed either by x ∼N(θ, σ2) (with known σ2) or by
y ∼f(y|θ) = cy−1 exp

−d2
2

1 −θ
y
2
II[0,bθ](y),
where b is huge and d large (50, say).
a. Show that the two maximum likelihood estimators of θ are δ1(x) = x and
δ2(y) = y.
b. Consider the special case x = y = σd. Explain why the inference on θ
should be the same in both cases.
c. Explain why
[x −1.96 σ, x + 1.96 σ]
could be proposed as a conﬁdence interval on θ at the level 95%.
d. Deduce that
[y −(1.96)(y/d), y + (1.96)(y/d)]
can be used as a conﬁdence interval if y is observed.
e. Show that
P(y −(1.96)(y/d) < θ < y + (1.96)(y/d))
can be made as small as wished for an adequate choice of b.
f. Conclude that the above conﬁdence interval is not appropriate for large
values of x = y and σ and discuss the relevance of a conﬁdence interval for
the Likelihood Principle.
g. Study the problem with the prior distribution π(θ) = 1/θ.
4.4 Show that, if p ∈[0, 1], θ = p/(1−p) and if π(θ) = 1/θ, the prior distribution
π(p) is Haldane distribution.
4.5 Show that a setting opposite to Example 4.1.2 may happen, namely, a case
when the prior information is negligible. (Hint: Consider π(θ) to be C(μ, 1) and
f(x|θ) ∝exp −|x −θ|, and show that the MAP estimator does not depend on
μ.)
4.6 In the setting of Example 4.2, consider π(θ) ∝exp −a|θ| and show that, for
a small enough, the MAP estimator is not always equal to 0.

4.6
Exercises
203
4.7 A contingency table is a k × ℓmatrix such that the (i, j)-th element is nij,
the number of simultaneous occurrences of the ith modality of a ﬁrst charac-
teristic, and of the jth modality of a second characteristic in a population of
n individuals (1 ≤i ≤k, 1 ≤j ≤ℓ). The probability of this occurrence is
denoted by pij.
a. Show that these distributions belong to an exponential family.
b. Determine the distributions of the margins of the table, i.e., of ni· = ni1 +
. . .+niℓand n·j = n1j +. . .+nkj. Deduce the distributions of (n1·, . . . , nk·)
and of (n·1, . . . , n·ℓ).
c. Derive conjugate priors on p = (pij) and the Jeﬀreys prior.
d. In the particular case of independence between the two variables, the param-
eters are supposed to satisfy the relations pij = pi·p·j where (p1·, . . . , pk·)
(p·1, . . . , p·ℓ) are two vectors of probabilities. Relate these vectors to the
distributions derived in b. and construct the corresponding conjugate pri-
ors.
e. Compare the posterior expectations of pij for the conjugate priors of c. and
d. [Note: See Santner and Duﬀy (1989) for a detailed presentation of the
Bayesian processing of these models.]
4.8 Determine whether the following distributions are possible posterior distri-
butions:
(i) T1(k, μ(x), τ 2(x)) when x ∼N(θ, σ2) and σ2 is known;
(ii) a truncated normal distribution N(μ(x), τ 2(x)) when x ∼P(θ); and
(iii) Pa(α(x), μ(x)) when x ∼B(n, 1/θ).
4.9 *(Exercise 4.8 cont.) Given a sample distribution f(x|θ) and a condi-
tional distribution g(θ|x), give a necessary and suﬃcient condition for g(θ|x)
to be a posterior distribution associated with f(x|θ) and an arbitrary prior
distribution π(θ).
4.10 Let (xn)n be a Markov chain with ﬁnite state space {1, . . . , p} and transition
matrix P.
a. If the sample is x1, . . . , xn, express the likelihood function and derive con-
jugate priors for the components of P.
b. The Markov chain is now observed at random times t1 < · · · < tn. Give
the likelihood function ℓ(P|xt1, . . . , xtn) when the distribution of the ti’s
does not depend on P and examine whether the above prior distributions
are still manageable for posterior computations.
c. A random variable yt is observed for t = 1, . . . , n with conditional distri-
bution f(y|θxt). We assume the yt’s to be independent conditional on the
xt’s. Show that the marginal distribution of the yt’s is a mixture of the
distributions f(y|θk).
d. If only the yt’s are observed, the model is called a hidden Markov chain.
When f(y|θ) belongs to an exponential family, give the likelihood function
and the conjugate priors on (P, θ1, . . . , θp).
e. Consider the special case p = 2 and f(y|θ) = θ exp(−θy)IIIR+(y) to examine
whether the above priors are manageable.
4.11 Consider x ∼B(m, p) and p ∼Be(1/2, 1/2).

204
Bayesian Point Estimation
4
a. Show that this prior is equivalent to an uniform prior on θ = arcsin(√p).
How can you justify this transformation? [Note: See Feller (1970) for details
on the arcsine distribution.]
b. Let y ∼B(n, q) an independent observation with q ∼Be(1/2, 1/2). Use
the approximation arcsin x ∼N(θ, 1/4m) to give an approximate posterior
distribution of arcsin(√p) −arcsin(√q).
c. Deduce an approximation of
π(| arcsin(√p) −arcsin(√q)| < 0.1|x, y).
4.12 The logistic distribution is deﬁned by the density
e−(x−θ)/(1 + e−(x−θ))2
on IR.
a. Show that the above function is truly a density and derive the maximum
likelihood estimator of θ.
b. Show that this distribution does not belong to an exponential family (i)
directly; and (ii) using Exercise 3.20. Deduce that there is no associated
conjugate prior and propose a noninformative prior.
c. What is the maximum likelihood estimator of θ for a sample x1, . . . , xn?
Show through an example that the likelihood can be multimodal.
d. Relate logistic regression and logistic distribution by exhibiting latent lo-
gistic random variables in the logistic regression model. Is there a contra-
diction between b. and the fact that the logistic regression model belongs
to an exponential family, as shown in Example 3.3.15?
4.13 For the AR model of Example 4.1.6, show that the joint posterior distri-
bution π(ϱ, σ2|x1:(T −1)) is explicit under the conjugate prior
ϱ ∼N(0, κσ2),
σ2 ∼IG(α, β) .
Deduce the predictive density π(xT|x1:(T −1)).
Section 4.2
4.14 (Smith (1988)) A usual justiﬁcation of quadratic losses is that they provide
a second-order approximation for symmetric losses. Consider the loss
L(θ, δ) = 1 −e−(δ−θ)2/2,
and π(θ|x) = (1/2){ϕ(θ; 8, 1) + ϕ(θ; −8, 1)}, a mixture of two normal distri-
butions with means 8 and −8, and variance 1.
a. Show that π(θ|x) can actually be obtained as a posterior distribution.
b. Show that IEπ[θ|x] is a local maximum of the posterior loss.
c. Relate the loss L(θ, δ) with the intrinsic losses of §2.5.4.
4.15 Consider x ∼P(λ) and π(λ) = e−λ. The purpose of the exercise is to
compare the estimators δc(x) = cx under the quadratic loss L(λ, δ) = (δ−λ)2.
a. Compute R(δc, λ) and show that δc is not admissible for c > 1.
b. Compute r(π, δc) and deduce the optimal cπ.
c. Derive the best estimator δc for the minimax criterion.

4.6
Exercises
205
d. Solve the above questions for the loss
L′(λ, θ) =
 δ
λ −1
2
.
4.16 Show that a Bayes estimator associated with a quadratic loss and a proper
prior distribution cannot be unbiased. Does this result hold for generalized
Bayes estimators? For other losses?
4.17 Consider x ∼B(n, p) and p ∼Be(α, β).
a. Derive the posterior and marginal distributions. Deduce the Bayes estima-
tor under quadratic loss.
b. If the prior distribution is π(p) = [p(1 −p)]−1II(0,1)(p), give the generalized
Bayes estimator of p (when it is deﬁned).
c. Under what condition on (α, β) is δπ unbiased? Is there a contradiction
with Exercise 4.16?
d. Give the Bayes estimator of p under the loss
L(p, δ) = (δ −p)2
p(1 −p).
4.18 Using the estimators in Table 4.2.1, show that the estimators corresponding
to noninformative prior distributions can be written as limits of conjugate
estimators. Does this convergence extend to other posterior quantities for the
same sequence of conjugate hyperparameters? Try to derive a general result.
4.19 Consider x ∼N(θ, 1), θ ∼N(0, 1), and L(θ, δ) = II{δ<θ}. Show that, in this
case, there is no Bayes estimator.
4.20 Consider x ∼P(θ), with Θ = {θ1, θ2} and D = {d1, d2, d3}. The loss
function is deﬁned by the matrix
L =

0
20
10
50
0
20

(where Lij = L(θi, dj), i = 1, 2, j = 1, 2, 3). Show that the Bayes estimators
are of the form
δπ(x) =
 d1
if x < k −log2 3,
d2
if x > k −1,
d3
otherwise,
and deﬁne k in terms of the prior distribution π.
4.21 (Ferguson (1967))
Consider x from the renormalized negative binomial
distribution,
f(x|θ) =

r + x −1
x

θx(1 + θ)−(r+x),
x = 0, 1, . . . ,
θ ∈IR∗
+,
so that IEθ[x] = rθ (i.e., θ = p/(1 −p)). The loss function is the weighted
squared error
L(θ, δ) = (θ −δ)2
θ(1 + θ).
a. Give the maximum likelihood estimator of θ.
b. Show that δ0(x) = x/r has a constant risk and is a generalized Bayes
estimator for π(θ) = 1 if r > 1. What happens for r = 1?

206
Bayesian Point Estimation
4
c. Show that
δα,β(x) = α + x −1
β + r + 1
is a Bayes estimator for
π(θ|α, β) ∝θα−1(1 + θ)−(α+β)
and that this distribution is conjugate for f(x|θ).
d. Deduce that δ1(x) = x/(r + 1) is a minimax estimator.
4.22 (Ferguson (1967))
Consider Θ = [0, 1] and L(θ, δ) = (θ−δ)2
1−θ , for the geo-
metric distribution
f(x|θ) = θx(1 −θ)
(x ∈IN).
a. Give a power series representation of R(θ, δ) .
b. Show that the unique nonrandomized estimator with constant risk is δ0
such that
δ0(0) = 1/2,
δ0(x) = 1 if x ≥1.
c. Show that, if δπ is the Bayes estimator associated with π, δπ(n) = μn−1/μn,
where μi is the ith moment of π.
d. Show that δ0 is minimax.
4.23 *(Casella and Strawderman (1981))
Consider x ∼N(θ, 1) with |θ| ≤m
(m < 1).
a. Show that δm(x) = m tanh(mx) is a Bayes estimator associated with
πm(θ) = 1
2II{−m,m}(θ).
b. Show that, for the quadratic loss, r(πm, δm) = R(δm, ±m) and deduce that
δm is minimax. [Note: This is actually the unique minimax estimator in this
case.]
c. Compare with the estimator δU associated with the uniform prior
π(θ) =
1
2mII[−m,m](θ),
in terms of m. [Note: Gatsonis et al. (1987) give a detailed study of the
performance of δU in term of minimaxity.]
4.24 (Casella and Berger (1990)) Consider x ∼U{1,2,...,θ} and θ ∈Θ = IN∗.
a. If D = Θ, show that, under quadratic loss, IEπ[θ|x] is not necessarily the
Bayes estimator.
b. If D = [1, +∞), show that IEπ[θ|x] is the Bayes estimator (when it exists).
c. Show that δ0(x) = x is admissible, for every choice of D. (Hint: Start with
R(1, δ0).)
d. Show that δ0 is a Bayes estimator and that there exist other Bayes estima-
tors for this prior distribution with diﬀerent risk functions.
4.25 Consider x1, x2 i.i.d. with distribution f(x|θ) = (1/2) exp(−|x −θ|) and
π(θ) = 1. Determine the Bayes estimators associated with the quadratic and
absolute error losses. Same question for an additional observation. [Note: See
Example 1.2.5 for an historical motivation.]

4.6
Exercises
207
Section 4.3.1
4.26 Chrystal (1891) writes: “No one would say that, if you simply put two white
balls into a bag containing one of unknown color, equally likely to be black or
white, this action raised the odds that the unknown ball is white from even to
3 to 1,” as an argument against the Laplace succession rule. Do you consider
this criticism as acceptable? (See Zabell (1989).)
4.27 (Jeﬀreys (1961))
a. Show that
N

i=1

i
x1

N −i
x −x1

=

N + 1
x + 1

(i) algebraically, and
(ii) using combinatorics.
b. If the sample contains x = x1 + x2 individuals, show that the probability
that the following y = y1 + y2 draws will contain y1 individuals from the
ﬁrst and y2 from the second population is
P(y1, y2|x1, x2) =
y!
y1! y2!
(x1 + 1) . . . (x1 + y1)(x2 + 1) . . . (x2 + y2)
(x + 2) . . . (x + y + 1)
.
c. If x = x1, deduce that the probability that the y next draws are of the
same type is
x + 1
x + y + 1.
4.28 Generalize the Laplace succession rule for a multinomial model.
Some problems similar to the Laplace succession rule were considered by Lewis
Carroll in his Pillow Problems. Seneta (1993) gives a detailed commentary on
these problems, two of which are given below.
4.29 Consider two bags, H and K, with two balls each. Each ball is either black
or white. A white ball is added to bag H and a hidden ball is transferred at
random from bag H to bag K.
a. What is the chance of drawing a white ball from bag K?
b. A white ball is then added to bag K and a hidden ball is transferred from
bag K to bag H. What is the chance now of drawing a white ball from bag
H?
4.30 “If an inﬁnity of rods is broken, ﬁnd the chance that one at least is broken
in the middle.” While this question is poorly formulated, a discrete solution
is proposed below.
a. Assume that each rod has 2m+1 breaking points and that there are exactly
2m + 1 rods. Give the probability that no rod breaks in the middle and
derive the limiting value when m goes to inﬁnity.
b. Study the dependence of this limit upon the assumption that the number
of breaking points is equal to the number of rods.

208
Bayesian Point Estimation
4
Section 4.3.2
4.31 In the setting of Example 4.3.1, develop a Bayesian model for the distribu-
tion of (t2 −t1). Extend to the following problem: Given that a traﬃc light
has been red for one minute, what is the probability that it will turn green in
the next minute?
4.32 Show that, for the tramcar problem, the maximum likelihood estimator
ˆ
N = T is admissible under any loss function of the form L(| ˆ
N −N|) where L
is strictly increasing. (Hint: Consider the case where N = 1 ﬁrst.)
Section 4.3.3
4.33 During the launch of a new campus journal, n1 = 220 and n2 = 570 persons
bought the two test issues −1 and 0. The number of persons who bought both
issues is n11 = 180. Give a Bayes estimator of N, total number of readers,
assuming that a capture-recapture modeling applies here and that π(N) is
P(1000).
4.34 (Castledine (1981)) For the Wolter modeling introduced in Section 4.3.3,
i.e., when n1 and n2 are random variables, the temporal model considers the
case when all individuals have the same probability of capture for a given
experiment, but where the probability varies between the ﬁrst and the second
captures. These two probabilities are denoted p1 and p2 .
a. Give the likelihood and the maximum likelihood estimator associated with
this model when p1 and p2 are known.
b. Show that the posterior distribution of N given p1 and p2 only depends on
n+ = n1 + n2 −n11 and μ = 1 −(1 −p1)(1 −p2). If the prior distribution
of N is π(N) = 1/N, show that π(N|n+, μ) is Neg(n+, μ).
c. Give the posterior marginal distribution of N if p1 ∼B(α, β) and p2 ∼
B(α, β).
d. Show that, if α = 0, β = 1, we recover the Darroch model as the marginal
distribution of N. Does this decomposition facilitate the derivation of a
Bayes estimator?
Section 4.4.1
4.35 *(Robert (1990a)) The modiﬁed Bessel function Iν (ν ≥0) is a solution to
the diﬀerential equation z2f ′′ +zf ′ −(z2 +ν2)f(z) = 0 and can be represented
by the power series
Iν(z) =
z
2
ν
∞

k=0
(z/2)2k
k! Γ(ν + k + 1).
a. Show that the above series converges in IR.
b. Developing
 π
0
ez cos(θ) sin2ν(θ) dθ
in a power series, show that Iν can be written as
Iν(z) =
(z/2)ν
π1/2Γ(ν + 1
2)
 π
0
ez cos(θ) sin2ν(θ) dθ.
(4.6.1)

4.6
Exercises
209
c. Establish the following recurrence formulas:

Iν+1(z) = Iν−1(z) −(2ν/z)Iν(z),
I′
ν(z) = Iν−1(z) −(ν/z)Iν(z).
d. Derive from (4.6.1) by an integration by parts that, for z > 0,
Iν+1(z) ≤Iν(z).
e. Derive from the power series representation of Iν that t−νIν(t) is increasing
in t. If we deﬁne rν as
rν(t) = Iν+1(t)
Iν(t) ,
show that rν is increasing, concave, and that rν(t)/t is decreasing.
f. Show that
lim
t→0 rν(t) = 1,
lim
t→∞
rν(t)
t
=
1
2(ν + 1),
and that
r′
ν(t) = 1 −2ν + 1
t
rν(t) −r2
ν(t).
g. Show that the density of the noncentral chi-squared distribution with non-
centrality parameter λ and ν degrees of freedom can be expressed using a
modiﬁed Bessel function, namely,
pλ,ν(x) = 1
2
x
λ
 ν−2
4
I ν−2
2 (
√
λx)e−x+λ
2 .
4.36 *(Bock and Robert (1991)) On IRp, the sphere of radius c is deﬁned by
Sc = 
z ∈IRp; ||z||2 = c
.
a. If x ∼Np(θ, Ip), with p ≥3, and θ has the prior distribution πc, uniform
distribution on Sc, show that the marginal density of x is proportional to
mc(x) = e−||x||2/2e−c2/2 I p−2
2 (||x||c)
(c||x||)
p−2
2
.
b. Show that the proportionality coeﬃcient is independent of c and recall why
it does not appear in the posterior distribution.
c. Derive from a. the posterior mean δc by a diﬀerential computation. (Hint:
See Lemma 4.2.2.)
d. Show that, if c ≥√p, δc is a shrinkage estimator outside the ball {x; ||x|| ≤
ϱ} and an expander within this ball. Determine the boundary value ϱ.
e. Show that δc cannot be minimax. Is this estimator admissible?
f. Explain why δc never belongs to the sphere Sc while πc is concentrated on
Sc. Is δc the “true” Bayes estimator then?
g. Using the recurrence relations of Exercise 4.35, show that
δc(x) =

1 −p −2
||x||2

x + hc(||x||2)x,
where hc(t) > 0 when t ≤max(c2, p−2). Try to propose a more interesting
estimator.

210
Bayesian Point Estimation
4
4.37 Consider x1, . . . , x10 i.i.d. N(θ, θ2), with θ > 0, which represents ten ob-
servations of the speed of a star. Justify the choice π(θ) = 1/θ and determine
the generalized Bayes estimator associated with the invariant loss
L(θ, δ) =
δ
θ −1
2
.
(Hint: Use Exercise 3.33.)
4.38 *(Lindley (1965))
Consider x1, . . . , xn a sample from N(θ, σ2) with σ2
known. The prior density π(θ) is such that there exist ϵ, M, and c such that
c(1 −ϵ) ≤π(θ) ≤c(1 + ϵ) for θ ∈I = [¯x −1.96 σ/√n, ¯x + 1.96 σ/√n] and
π(θ) ≤Mc otherwise.
a. Show that these constraints are compatible, i.e., that there exists such a
prior distribution.
b. Show that
(1 −ϵ)[0.95(1 + ϵ) + 0.05M]−1 e−(x−θ)2n/2σ2
1
2πσ2/n
≤π(θ|x)
≤
(1 + ϵ)[(1 −ϵ)0.95]−1 e−(x−θ)2n/2σ2
1
2πσ2/n
if θ ∈I and
π(θ|x) ≤
M
0.95(1 −ϵ)
e−1.962/2
1
2πσ2/n
otherwise.
c. Discuss the interest of the approximations when θ ∈I and when θ ̸∈I.
Can you derive a conservative conﬁdence region?
4.39 Consider a normal random variable, x ∼N(θ, 1) and the one-to-one trans-
form η = sinh(θ).
a. When π(η) = 1, show that the resulting posterior distribution on θ is
π(θ|x) ∝exN(x + 1, 1) + e−xN(x −1, 1).
b. Compare the behavior of this posterior distribution with the usual Jeﬀreys
posterior N(x, 1) in term of posterior variance, posterior quantiles and
modes. In particular, determine the values of x for which the posterior
distribution is bimodal and those for which there are two global maxima.
c. Consider the behavior of π(θ|x) for large values of x and conclude that the
prior π(η) = 1 is unreasonable.
Section 4.4.2
4.40 (Jeﬀreys (1961))
Consider x1, . . . , xn1 i.i.d. N(θ, σ2). Let ¯x1, s2
1 be the
associated statistics. For a second sample of observations, give the predictive
distribution of (¯x2, s2
2) under the noninformative distribution π(θ, σ) =
1
σ . If
s2
2 = s2
1/y and y = ez, deduce that z follows a Fisher’s F distribution.
4.41 Show that, if x ∼G(α, β), 1/x ∼J G(α, β) as deﬁned in (4.4.3).
4.42 *(Ghosh and Yang (1996)) As in Exercise 3.46, consider x11, . . . , x1n1 and
x21, . . . , x2n2, two independent samples with xij ∼N(μi, σ2).

4.6
Exercises
211
a. Show that the Fisher information matrix is
I(μ1, μ2, σ) = σ−2
! n1
0
0
0
n2
0
0
0
2(n1 + n2)
"
.
b. Welch and Peers’s (1963) matching prior (see Section 3.5.5) for the quantity
of interest θ = (μ1 −μ2)/σ is
∂
∂μ1 (η1π) +
∂
∂μ2 (η2π) + ∂
∂σ (η3π) = 0 ,
(4.6.2)
where
(η1, η2, η3) = I−1∇θ/(∇θtI−1∇θ)1/2 .
Show that a class of solutions to (4.6.2) are of the form
/
n−1
1
+ n−1
2
+ 1
2(μ1 −μ2)2/{(n1 + n2)σ2}
01/2
g(μ1, μ2, σ)
(4.6.3)
where
g(μ1, μ2, σ) ∝+
d1(μ1 −μ2)2 + d2(n1μ2
1 + n2μ2
2) + d3σ2,c ,
c is an arbitrary constant, and (d1, d2, d3) satisfy
d1(n−1
1
+ n−1
2 ) + d2 = 1
2d3(n1 + n2)−1 .
c. Deduce that the matching prior for (θ, μ2, σ) is
π(θ, μ2, σ) ∝σ2c+1 /
n−1
1
+ n−1
2
+ 1
2θ2(n1 + n2)−10c+1/2
.
d. Show that
π(θ|¯x1, ¯x2, s) ∝
/
n−1
1
+ n−1
2
+ 1
2θ2(n1 + n2)−10c+1/2
 ∞
0
vn1+n2−2c−4 exp
−1
2

v2+
n1n2
n1 + n2 (vz −θ)2 
dv
where z = (¯x1 −¯x2)/s.
e. Show that that the distribution of z only depends on θ.
f. Show that the only choice of c which avoids the marginalization paradox
of Exercises 3.44–3.50 is c = −1.
Section 4.4.3
4.43
a. If x ∼Np(θ, Σ), show that, for every prior distribution π,
δπ(x) = x + Σ∇log mπ(x).
b. (Bock (1988))
Pseudo-Bayes estimators are deﬁned as the estimators of
the form
δ(x) = x + ∇log m(x)
when x ∼Np(θ, Ip). Show that the truncated James–Stein estimator given
in Example 4.2.3 is a pseudo-Bayes estimator (i.e., deﬁne the corresponding
m). Can this estimator be a Bayes estimator?
4.44 *For a normal model Nk(Xβ, Σ) where the covariance matrix Σ is totally
unknown, give the noninformative Jeﬀreys prior.

212
Bayesian Point Estimation
4
a. Show that the posterior distribution of Σ conditional upon β is a Wishart
distribution and deduce that there is no proper marginal posterior distri-
bution on β when the number of observations is smaller than k.
b. Explain why it is not possible to derive a conjugate distribution in this
setting. Consider the particular case when Σ has a Wishart distribution.
c. What is the fundamental diﬀerence in this model which prevents what was
possible in Section 4.4.2?
4.45 *Consider a linear regression prediction setting, where y = Xβ+ϵ has been
observed with β ∈IRk and ϵ ∼Np(0, Σ), and z = Tβ + ϵ′ is to be predicted
(with T known and ϵ′ ∼Np(0, Σ) independent of ϵ).
a. If δ is the predictor and the prediction error is evaluated through the loss
L(z, δ) = ||z −δ||2, show that the frequentist expected error is
IEz,x[L(z, δ(x))] = tr(Σ) + IEx[||δ(x) −Tβ||2].
b. Show that the problem can be expressed as the estimation of β under
the quadratic loss associated with Q = T tT. (Hint: Show ﬁrst that δ(x)
is necessarily of the form Tγ(x), with γ(x) ∈IRk, or dominated by an
estimator of this form.)
c. Deduce from the fact that Q is degenerate with a single eigenvalue diﬀerent
from 0 that a Stein eﬀect cannot occur in this setting.
d. Consider now that T is a random matrix with mean 0 and IE[T tT] = M.
Show that, when δ(x) = Tγ(x), the frequentist risk is
IEz,x,T [L(z, δ(x))] = tr(Σ) + IEx[(γ(x) −β)tM(γ(x) −β)] ,
and therefore that a Stein eﬀect is possible when M has at least three
positive eigenvalues. [Note: This phenomenon is related to the ancillarity
paradoxes developed in Brown (1990). See also Foster and George (1996).]
e. Let β ∼Nk(0, σ2Ik). Derive the Bayes predictor of z when T is ﬁxed and
when T is random. Conclude.
4.46 Tobit models are used in econometrics (see Gouri´eroux and Monfort (1996))
to represent truncated settings. Consider y|x ∼N(βtx, σ2), which is only
reported when y is positive, x being an explanatory variable in IRp.
a. Show that tobit models are a mixture of probit models (when y < 0) and
of regular regression models (when y ≥0).
b. Give the likelihood function ℓ(β, σ2|y1, . . . , yn) associated with a sample
y1, . . . , yn, x1, . . . , xn and derive suﬃcient statistics for this model.
c. Conditional upon (x1, . . . , xn), show that this model belongs to an exponen-
tial family and propose a conjugate prior on (β, σ). Are the corresponding
computations tractable?
4.47 *The inverse regression (or calibration) model is given by
y ∼Np(β, σ2Ip),
z ∼Np(λ0β, σ2Ip),
s2 ∼σ2χ2
q,
with β ∈IRp, λ0 ∈IR.
a. Give the maximum likelihood estimator of λ and show that its quadratic
risk can be inﬁnite.

4.6
Exercises
213
b. Compute the Jeﬀreys prior on (β, σ2, λ0) and show that the corresponding
posterior expectation of λ0 is the inverse regression estimator, δI(y, z, s) =
ytz/(s + ||y||2).
c. Using the technique of the reference prior introduced in Section 3.5, propose
an alternative prior distribution π(λ0, (β, σ2)) when (β, σ2) is considered
as a nuisance parameter. Derive the corresponding posterior expectation of
λ0, δR(y, z, s).
d. Show that, when q goes to inﬁnity, δI a.s. converges to 0, but that δR
is free of this inconsistency. [Note: See Osborne (1991) for a survey of
calibration models and Kubokawa and Robert (1994) for decision-theoretic
perspectives on these estimators.]
Section 4.5.1
4.48 For the AR(1) model (4.5.2), give the covariance matrix of (x1, . . . , xT ).
4.49 (Exercise 4.48 cont.)
a. Show that the variance of xt is given by (4.5.3).
b. What happens in the case ϱ = 1, since (4.5.3) has no meaning?
c. Extend to the case when x0 is an arbitrary value.
4.50 (Exercise 4.49 cont.) We want to establish that there exists no stationary
distribution for our AR(1) model when |ϱ| ≥1, that is, no density f such that,
if xt ∼f, then xt+1 ∼f.
a. Show that, when |ϱ| < 1, the stationary distribution is the normal distri-
bution N(0, σ2/(1 −ϱ2)).
b. For the case |ϱ| = 1, show that the Lebesgue measure is the stationary
measure of the chain (xt), that is, for every measurable set A,

A
dx =

A

f(y|x)dxdy ,
where f(y|x) denotes the conditional distribution of xt given xt−1, that is,
N(xt−1, σ2) in this case. Deduce from the unicity of the stationary measure
that there cannot exist a stationary distribution.
c. Extend to the case when |ϱ| ≥1, by writing xt as
xt =
t−1

i=0
ϱiϵt−i + ϱtx0
and deducing that xt is almost surely inﬁnite when t goes to inﬁnity. (Hint:
When x0 = 0, replace the above decomposition with the corresponding
decomposition conditional on x1.)
Section 4.5.2
4.51 (Bernardo and Smith (1994)) Show that, for a two dimensional vector,
( x1
x2 )t ∼N2

( μ1
μ2 )t,

σ2
1
ϱσ1σ2
ϱσ1σ2
σ2
2

,
the Jeﬀreys prior is π(θ) ∝(1 −ϱ2)−1/σ1σ2.
4.52 (Bauwens et al. (1999)) For the AR(1) model (4.5.2),

214
Bayesian Point Estimation
4
a. Show that μ is a location parameter and, therefore, that it does not appear
in the Jeﬀreys prior.
b. Show that
IE

∂2 log L(θ|x1:T )
∂σ2

= −T
2σ4 ,
IE

∂2 log L(θ|x1:T )
∂ϱ2

= −1
σ2 IE
T −1

t=0
x2
t

.
c. Using the stationary distribution of the yt’s, deduce from IE[y2
t ] = σ2/(1 −
ϱ2) that the Jeﬀreys prior πJ
1 (σ2, ϱ) = 1/σ21
1 −ϱ2.
4.53 *(Brockwell and Davis (1996))
The Durbin–Levinson algorithm derives
the partial autocorrelations as follows: deﬁne φn1, . . . , φnn recursively from
the autocovariances γ(s) as
φnn =
!
γ(n) −
n−1

j=1
φ(n−1)jγ(nj)
"
v−1
n−1
and
⎛
⎝
φn1
...
φn(n−1)
⎞
⎠=
⎛
⎝
φ(n−1)1
...
φ(n−1)(n−1)
⎞
⎠−φnn
⎛
⎝
φ(n−1)(n−1)
...
φ(n−1)1
⎞
⎠,
where vn = vn−1(1 −φnn)2, φ11 = γ(1)/γ(0) and v0 = γ(0).
a. Show that, if ψn = φnn, the reverse of the Durbin–Levinson algorithm is
given by Lemma 4.5.4.
b. Show that the partial autocorrelations ψn of an MA(q) process are 0 for
n > q.
c. Show that the partial autocorrelations ψn of a AR(1) process are given by
ψn = (−1)n+1ϑn
1 /(1 + ϑ2
1 + . . . + ϑ2n
1 ).
4.54 (Bauwens et al. (1999)) For the AR(1) model (4.5.2),
a. Using the Wold decomposition (4.5.9) obtained in Example 4.5.5, show that
IE+
x2
t
,
= IE
!
ϱtx0 +
t−1

i=0
ϱiϵt−i
"2
= 1 −ϱ2t
1 −ϱ2 σ2
when x0 = 0.
b. Deduce the Jeﬀreys prior πJ
2 .
Section 4.5.3
4.55 *Give the Wold decomposition for the stationary AR(p) model. (Hint: Use
the lag polynomial representation of the AR(p) model, that is, P(B)xt = ϵt,
where Bdxt = xt−d.)
4.56 Show that the autocorrelations γs for the MA(q) model are given by (4.5.11).
4.57 Show that that the representation (4.5.16) holds. Give an extension of the
representation in Example 4.5.7 to the general MA(q) model.

4.6
Exercises
215
Section 4.5.4
4.58 *An ARMA(p,q) model (t ≥1)
xt −μ =
p

i=1
ϱi(xt−i −μ) +
q

j=1
ϑjϵt−j + ϵt ,
is invertible (Brockwell and Davis (1996, §3.1)) if there exists a sequence (ϖj)j
such that

j
|ϖj| < ∞
and
ϵt =
∞

j=0
ϖjxt−j .
Show that invertibility is equivalent to the condition that Q(x) must have its
roots outside the unit circle. (Hint: Use the lag polynomial representation of
the ARMA(p, q) model, that is, P(B)xt = Q(B)ϵt, where Bdxt = xt−d.)
4.59 *An ARMA(p, q) model is causal (Brockwell and Davis (1996, §3.1)) if there
exists a sequence (ϕj)j such that

j
|ϕj| < ∞
and
xt =
∞

j=0
ϕjϵt−j .
Show that causality is equivalent to the condition that P(x) must have its
roots outside the unit circle. (Hint: Use the lag polynomial representation of
Exercise 4.58.)
4.60 Show that the representation (4.5.18) holds. Propose an alternative repre-
sentation.
4.61 Propose a state-space representation similar to (4.5.18) for the ARIMA
model
zt −μ =
p

i=1
ϱi(zt−i −μ) +
q

j=1
ϑjϵt−j + ϵt ,
(4.6.4)
where zt is the diﬀerence of the observations, zt = xt −xt−d, d ∈IN∗. [Note:
As detailed in Brockwell and Davis (1996, §6.5), the general ARIMA(p, d, q) is
given as an ARMA(p,q) model on the diﬀerence xt −Ψ1xt−d−. . .−ΨPxt−P d.]
Note 4.7.1
4.62 (Deely and Gupta (1968))
Consider x1 ∼N(θ1, σ2
1), . . . , xk ∼N(θk, σ2
k)
when the quantity of interest is θ[k], the largest mean among θ1, . . . , θk. The
loss function is L(θ, ϕ) = θ[k] −ϕ.
a. Show that, if σ1 = . . . = σk are known and π(θ1) = . . . = π(θk) = 1, the
Bayes estimator selects the population with the largest observation.
b. Generalize to the case where the θi’s have an exchangeable prior distribu-
tion N(0, τ 2).
4.63 *(Goel and Rubin (1977)) Show that the s∗
j’s truly constitute a complete
class when the prior distribution on θ = (θ1, . . . , θk) is symmetric. (Hint: Show
that s∗
j is optimal among the subsets of size |s∗
j|.)
4.64 (Exercise 4.63 cont.) Extend this result to the distributions f(x|θ) with
monotone likelihood ratio property in θ.

216
Bayesian Point Estimation
4
4.65 (Chernoﬀand Yahav (1977)) Extend the complete class result of Exercise
4.63 to the loss function
L(θ, s) = c(θ[k] −θs) −1
s

j∈s
θj.
(Hint: Show that, if θi1 ≤. . . ≤θij , s = {i1, . . . , ij} is dominated by the set
{ij}.)
Note 4.7.2
4.66 *For the AR(1) model (4.5.2), assume the quantity of interest is x0, the
starting value of the chain. Compute a reference prior for (x0, (ϱ, σ2)) and
derive an estimator of x0 under quadratic loss.
Note 4.7.3
4.67 Consider the factor model (t = 1, . . . , T ),

y∗
t = [α + β(y∗
t−1)2]1/2ϵ∗
t
yt = y∗
t μ + σϵt,
(4.6.5)
with ϵ∗
t ∼N(0, 1), and only the yt ∈IRp are observed.
a. Write the (complete) likelihood associated with the couples (yt, y∗
t ).
b. Show that the y∗
t ’s cannot be analytically integrated out.
c. Deduce that the factor model cannot be expressed as a special case of an
ARCH model (4.7.1).
4.68 (Bauwens et al. (1999))
Show that the ARCH(p) model is useless when
α = 0. (Hint: Show that var(yt) = 0.)
4.7 Notes
4.7.1 Ranking and selection
A great deal of eﬀort has been dedicated to the problem of estimating and
comparing several normal means. We mention brieﬂy a few approaches here
to illustrate the interest of the Bayes treatment, referring the reader to the
literature for a more developed discussion. See, for instance, Gibbons et al.
(1977), Gupta and Panchapakesan (1979), and Dudewicz and Koo (1982),
following the introductory papers of Bechofer (1954) and Gupta (1965). As
described by Berger and Deely (1988), ranking and selection techniques also
appear as substitutes for the analysis of variance approach (Chapter 10).
Given x1 ∼N(θ1, σ2
1), . . ., xk ∼N(θk, σ2
k), the problem is to select the pop-
ulation with the highest mean, θ[k]. The variances σ2
1, . . . , σ2
k are supposed to
be known but the more general setting where they are estimated by ˆσ2
1, . . . , ˆσ2
k
can be treated similarly by the Bayesian paradigm. Berger and Deely (1988)
rewrite the problem to answer the following questions: (a) Can we accept the
hypothesis H0 : θ1 = · · · = θk? (b) In the event of a negative answer to
(a), what is the largest mean? They solve the ﬁrst question by computing the
Bayes factor against H0 and then the posterior probabilities pj that θj is the
largest mean (1 ≤j ≤k). (Chapter 5 covers deﬁnition and computation of
these quantities.) The prior distributions they use are hierarchical
θi|β, σ2
π ∼N(β, σ2
π),
β ∼N(β0, A)
and
σ2
π ∼γII0(σ2
π) + (1 −γ)π∗
22(σ2
π).

4.7
Notes
217
The special form of the prior distribution on σ2
π is necessary to test whether
the θi’s are actually identical. For π∗
22, Berger and Deely (1988) suggest the
informative choice
π∗
22(σ2
π) = (m −1)C(1 + Cσ2
π)−m,
where C and m can be derived from prior quantiles. For a noninformative
choice, possibilities are π∗
22(σ2
π) = 1 and
π∗
22(σ2
π) =
k

i=1
(σ2
i + σ2
π)−1/k,
although these priors may lead to diﬃculties in the derivation of the posterior
probability of H0 (see Chapter 5).
Goel and Rubin (1977) adopt a more decision-theoretic perspective, consider-
ing as decision space D the set of all nonempty parts of {1, . . . , k}, denoted
{s1, s2, . . . , sK} where K = 2k −1. They introduce the loss function
L(θ, s) = c|s| + θ[k] −θs,
where |s| denotes the cardinality of s and θs = maxj∈s θj. This loss includes
a penalization cost c for each population involved in the decision set s. This
is quite logical because a parsimony requirement forces the decision set s
to be as small as possible, the ideal setting being |s| = 1. Goel and Rubin
(1977) ﬁrst show that a Bayes rule associated with this loss and a symmetric
prior distribution has to be chosen among the sets s∗
j = {ωk, . . . , ωk−j+1}
(1 ≤j ≤k), where ωj denotes the population of x(j). The Bayes rule sπ is
then a solution of
ϱ(π, sπ|x) =
min
j=1,...,k ϱ(π, s∗
j|x),
where ϱ(π, s|x) = c|s| + IEπ[θ[k] −θs|x]. Introducing
Δm = ϱ(π, s∗
m+1|x) −ϱ(π, s∗
m|x)
(1 ≤m ≤k −1),
the Bayes rule is s∗
k if A = {j; Δj ≥0} is empty and s∗
m otherwise, where m =
min(A). A diﬃcult point in the derivation of sπ is obviously the computation
of the posterior expectations IEπ[θ[k] −θs|x]. The authors detail the particular
case of a normal exchangeable prior on the θj’s, which still depends on the
function
tm(z) =
 +∞
−∞
Φm(z + x)Φ(−x)dx.
Nonetheless, they show that, in the noninformative case, when σ1 = · · · = σk,
the Bayes rule is s∗
1 when c/σ1 ≥1/π2.
4.7.2 Jeﬀreys prior for the AR(1) model
There is a controversy surrounding the Jeﬀreys prior in this case, owing to the
debate of whether to include the stationarity condition and the discrepancy
between both cases. If we assume xt = μ + ϱ(xt−1 −μ) + ϵt with x0 = 0, the
Jeﬀreys prior associated with the stationary representation is (Exercise 4.52)
πJ
1 (μ, σ2, ϱ) ∝1
σ2
1
1
1 −ϱ2 .

218
Bayesian Point Estimation
4
−3
−2
−1
0
1
2
3
0
1
2
3
4
5
6
7
x
pi
Figure 4.7.1. Graphs of the priors πJ
1 (ϱ) and πB(ϱ) for T = 10.
When moving to the non-stationary region |ϱ| > 1, Phillips (1991) has shown
that the Jeﬀreys prior is then (Exercise 4.54)
πJ
2 (μ, σ2, ϱ) ∝1
σ2
1
1
|1 −ϱ2|
(1 −
1 −ϱ2T
T(1 −ϱ2)
 .
While πJ
2 (μ, σ2, ϱ) is equivalent to πJ
1 (μ, σ2, ϱ) for large values of T and |ϱ| <
1, the dominant part of the prior is the non-stationary region, since it is
equivalent to ϱ2T (Bauwens et al. (1999)). Berger and Yang (1994a) have also
shown that the reference prior is πJ
1 , and is only deﬁned when the stationary
constraint holds. They then suggest to symmetrize this prior to the region
|ϱ| > 1, taking
πB(μ, σ2, ϱ) ∝1
σ2

1/
1
1 −ϱ2
if |ϱ| < 1,
1/|ϱ|
1
ϱ2 −1
if |ϱ| > 1,
,
which has a more reasonable shape that πJ
2 , as shown by Figure 4.7.2.
As detailed in Bauwens et al. (1999, §6.8), it is also possible to come up with
the Jeﬀreys priors in the stationary and non stationary cases, when including
the distribution of the initial value x0, with priors similar to πJ
1 and πJ
2 .
4.7.3 ARCH models
ARCH models, as introduced by Engle (1982), are used to represent pro-
cesses, particularly in ﬁnance, with independent errors with time-dependent
variances, as in the ARCH(p) process (t ≥1),
xt = σtϵt ,
σ2
t = α +
p

i=1
βix2
t−i ,
(4.7.1)
where the ϵt’s are i.i.d. N(0, 1). The acronym ARCH stands for autoregressive
conditional heterocedasticity, heterocedasticity being a term used by econo-
metricians for heterogeneous variance. Gouri´eroux (1997) provides a general
reference on these models, as well as classic inferential methods of estimation.
See Bauwens et al. (1999, §7.4) for Bayesian extensions to GARCH processes.
As shown in Nelson (1991) and Kleibergen and van Dijk (1993), a stationarity
condition for the ARCH(1) model is that IE[log(β1ϵ2
t)] < 0, which is equivalent
to β1 < 3.4.

4.7
Notes
219
 
 
0
100
200
300
400
500
-10
-5
0
5
10
15
20
Figure 4.7.2. Simulated sample from the stochastic volatility model (4.7.2) with
σ = 1 and ϱ = .9. (Source: Robert and Casella (2004).)
Contrary to the stochastic volatility models of Note 4.7.4, ARCH(p) models en-
joy closed-form likelihoods, when conditioning on the initial values x1, . . . , xp.
However, because of the non-linearities in the variance terms, approximations
methods as those of Chapter 6 must be used.
4.7.4 Stochastic volatility models
Stochastic volatility models are modeling the volatility, log(σ2
t ), of a series
(xt)t≥1 as a random variable. While such models are more complex to study
than their ARCH counterparts, they are often used in ﬁnance to represent
series with sudden variations in scale (see, for instance, Jacquier et al. (1994)).
A simple illustration of such models is the SV(1) model, where (t = 1, . . . , T )

y∗
t = α + ϱy∗
t−1 + σϵ∗
t−1 ,
yt = ey∗
t /2ϵt ,
(4.7.2)
and the ϵt’s and ϵ∗
t ’s are i.i.d. N(0, 1). The unobserved quantity (y∗
t ) thus
represents the volatility. (An usual assumption on the initial condition is that
y∗
0 ∼N(α, σ2).) Figure 4.7.4 plots a simulated series of stochastic volatilities
for σ = 1 and ϱ = .9.
The diﬃculty with this model is that the unobserved volatilities contain the
information about the parameters (α, ϱ, σ) because, conditional upon y∗
t , they
are independent from yt. (Obviously, the parameters do depend on the data,
albeit marginally.) Moreover, the observed likelihood L(α, ϱ, σ|y0, . . . , yT ) can-
not be expressed in closed form because the y∗
t ’s cannot be integrated out
analytically. On the opposite, the completed likelihood is explicit, that is,
Lc(α, ϱ, σ|y0, y∗
0 . . . , yT , y∗
T ) ∝
(4.7.3)
σ−T +1 exp −

(y∗
0 −α)2 +
T

t=1
(y∗
t −α −ϱy∗
t−1)2
 -
2σ2
exp −
T

t=0

y2
t e−y∗
t + y∗
t
 
/2 .
It can then be used in simulation based methods (Chapter 6), in alternance
with a simulation of the unobserved volatilities y∗
t . Figure 4.7.4 illustrates this

220
Bayesian Point Estimation
4
 
 
 
0
100
200
300
400
500
0
10000 20000 30000 40000 50000
 
 
0
100
200
300
400
500
-2 -1
0
1
2
Figure 4.7.3. Allocation map (top) and average allocations versus true volatilities
(bottom) for the model (4.7.2). The true volatilities are represented by broken
lines. (Source: Mengersen et al. (1998).)
simulation for the simulated dataset of Figure 4.7.4, where the true values
of the y∗
t ’s are known. (The blurred image on top of the graph is called an
allocation map and represents successive values of the y∗
t ’s as grey levels along
iterations of the sampling method.)
4.7.5 Poly-t priors
Poly-t priors were proposed by Dr`eze (1978) and Richard and Tompa (1980)
as robust alternatives to the conjugate priors for linear regressions. Their
motivation is given by the following example, developed in Bauwens et al.
(1999, §4.5). Consider two independent regressions,
y1 = X1β + σ1ε1, y2 = X2β + σ2ε2, ε1 ∼NT1(0, IT1), ε2 ∼NT2(0, IT2).
If π(β, σ1, σ2) = 1/σ1σ2, integration of the σi’s leads to the marginal posterior
π(β|y1, y2)
∝
[S1 + (β −ˆβ1)tM1(β −ˆβ1)]−T1/2
×[S2 + (β −ˆβ2)tM2(β −ˆβ2)]−T2/2 ,
where ˆβi is the OLS estimator (Xt
i Xi)−1Xiyi, Mi = (Xt
i X) and Si = ||yi −
Xi −ˆβi||2 (i = 1, 2). This distribution is a 2 −0 poly-t distribution.
In general, an m −n poly-t distribution is deﬁned as a product of m Student’s
t densities divided by n Student’s t densities,
ϕm,n(x)
∝
n

j=1
+
1 + (x −μ0
j)tP 0
j (x −μ0
j),ν0
j /2

4.7
Notes
221
- m

j=1
+
1 + (x −μ1
j)tP 1
j (x −μ1
j),ν1
j /2 .
As shown in Bauwens et al. (1999, Theorem A.21), the ϕm,0 density can be
expressed as a (continuous) mixture of regular Student’s t densities against
(m −1) auxiliary variables, a property that can be exploited either for direct
simulation as in Bauwens (1984), or for MCMC implementation (see Chapter
6), since the direct computation of the normalization constant of ϕm,n, or of
the corresponding posterior mean, is impossible. An additional diﬃculty with
poly-t priors is that, when compared with conjugate priors, they require the
determination of a much larger number of hyperparameters.


CHAPTER 5
Tests and Conﬁdence Regions
Twenty-six more tests were going to take the rest of daylight, maybe more.
Heat or no heat, the days still grew shorter as if winter really was coming
on, and a failed test would take a few minutes longer that one passed, just to
make certain.
Robert Jordan, Lord of Chaos, Book VI of the Wheel of Time.
5.1 Introduction
Although testing theory can be perceived as a special case of Decision The-
ory for a restricted decision space (and even as an estimation problem), we
consider testing inference in a separate chapter because there is much more
ambiguity about the real inferential purpose of testing than when estimat-
ing a regular function of the parameter. In fact, this part of statistical
inference is still incomplete, in the sense that many other answers have
been proposed, none being entirely satisfactory. In particular, there are
pronounced diﬀerences between frequentist and Bayesian testing theories.
This is nonetheless a setting where a Bayesian approach is quite appealing
because the notion of the probability of a hypothesis, like π(θ ∈Θ0|x), can
only be deﬁned through this approach.
Some Bayesians actually think that there should be no testing, or, at
least, that there should be no point-null hypothesis testing (see, for in-
stance, Gelfand et al. (1992)), and we will see in this chapter several philo-
sophical reasons that somehow motivate this radical perspective. These
reasons range from the reductive aspect of modeling (no model is right,
but some models are less wrong than others), to the unnatural modiﬁcation
of the prior imposed by point-null hypotheses, to the lack of problem-
motivated decision-theoretic structure, to the subsequent call to rudimen-
tary conventional 0 −1 losses and acceptance levels, to the impossibility of
using improper priors in both point-null hypotheses and model choice set-
tings (Chapter 7). But pragmatic considerations are such that the Bayesian
toolbox must also include testing devices, if only because users of Statistics

224
Tests and Conﬁdence Regions
5
have been accustomed to testing as a formulation of their problems, given
their strong tendency to take problems at face value.
We ﬁrst consider in Section 5.2 the usual Bayesian approach to test-
ing, i.e., through an evaluation of decisions by 0 −1 losses, and compare
the Bayesian procedures with their frequentist counterparts in Section 5.3.
We then propose in Section 5.4 an alternative decision-theoretic approach
through more adaptive losses which emphasize the “postdata” evaluation
of testing procedures (as opposed to Neyman–Pearson procedures for which
the evaluation is operated in a “predata” spirit).
This chapter exhibits a strong contrast between Bayesian and frequen-
tist approaches under diﬀerent evaluation tools; it is revealing because this
shows the incompleteness of the classical modeling, which relies on artiﬁ-
cial concepts to derive its optimal procedures. Contrary to point estimation
settings, these optimal frequentist procedures are no longer limits of Bayes
procedures and diﬀer numerically from their Bayesian counterparts. How-
ever, we moderate this rejection in Section 5.3 by showing that classical
answers may sometimes lead to similar conclusions than noninformative
Bayes procedures. Chapter 7 deals with model choice, which can be seen as
a particular case of testing point-null hypotheses, but also enjoys enough
peculiarities to deserve a chapter on its own (besides calling almost uni-
formly for the computational methods presented in Chapter 6).
5.2 A ﬁrst approach to testing theory
5.2.1 Decision-theoretic testing
Consider a statistical model f(x|θ) with θ ∈Θ. Given a subset of interest
of Θ, Θ0, which sometimes consists of a single point {θ0}, the question to
be answered is whether the true value of the parameter θ belongs to Θ0,
i.e., to test the hypothesis1
H0 : θ ∈Θ0,
usually called the null hypothesis. In linear models, Θ0 may be a subspace
of the vector space Θ and the testing problem is then a particular case of
a model choice problem (see also Chapter 7).
Example 5.2.1
Consider a logistic regression model,
Pα(y = 1) = 1 −Pα(y = 0) = exp(αtx)/(1 + exp(αtx)),
α, x ∈IRp,
which models the probability of developing a prostate cancer in a lifetime
in terms of explanatory variables x = (x1, . . . , xp). Of particular interest is
the dependence on work environment variables such as asbestos concentra-
tion xi0 and a worker union may want to test whether the coeﬃcient αi0
corresponding to xi0 is null.
∥
1 There is a certain amount of ambiguity involved in the terminology: Test simultane-
ously denotes the question, and the procedure used to answer the question.

5.2
A ﬁrst approach to testing theory
225
In the Neyman–Pearson perspective (Section 5.3), the testing problem
is formalized through a decision space D restricted to {yes, no} or, equiv-
alently, to {1, 0}. In fact, it makes sense to perceive testing problems as
an inference about the indicator function IIΘ0(θ) and therefore to propose
answers in IIΘ0(Θ) = {0, 1}. Of course, the relevance of such a restriction
is less obvious when considering that testing settings often occur as com-
ponents (or preliminary steps) or more complex inferential structures and,
in particular, that the answer to a test question has also consequences in
terms of (regular) estimation errors. It may then be more interesting to
propose procedures taking values in [0, 1]. (We discuss this approach in
Section 5.4.)
In some cases, additional information is available about the support of
θ, namely, that θ ∈Θ0 ∪Θ1 ̸= Θ. In such settings, we deﬁne the alternative
hypothesis against which we test H0 as
H1 : θ ∈Θ1.
Under this formalization, every test procedure ϕ appears as an estimator
of IIΘ0(θ) and we only need a loss function L(θ, ϕ) to derive the Bayes esti-
mators. For instance, the loss function proposed by Neyman and Pearson
is the 0 −1 loss
L(θ, ϕ) =

1
if ϕ ̸= IIΘ0(θ),
0
otherwise,
introduced in Chapter 2. For this loss, the Bayesian solution is
ϕπ(x) =
 1
if P π(θ ∈Θ0|x) > P π(θ ∈Θc
0|x),
0
otherwise.
This estimator is easily justiﬁed on an intuitive basis since it chooses the
hypothesis with the largest posterior probability. A generalization of the
above loss is to penalize diﬀerently errors when the null hypothesis is true
or false. The weighted 0 −1 losses
L(θ, ϕ) =
⎧
⎨
⎩
0
if ϕ = IIΘ0(θ),
a0
if θ ∈Θ0 and ϕ = 0,
a1
if θ ̸∈Θ0 and ϕ = 1,
(5.2.1)
are called “a0 −a1” for obvious reasons. The associated Bayes estimator is
then given by the following result.
Proposition 5.2.2 Under the loss (5.2.1), the Bayes estimator associated
with a prior distribution π is
ϕπ(x) =

1
if P π(θ ∈Θ0|x) >
a1
a0 + a1
,
0
otherwise.
Proof.
Since the posterior loss is
L(π, ϕ|x)
=

Θ
L(θ, ϕ)π(θ|x)dθ
=
a0P π(θ ∈Θ0|x)II{0}(ϕ) + a1P π(θ ̸∈Θ0|x)II{1}(ϕ),

226
Tests and Conﬁdence Regions
5
the Bayes estimator can be derived directly.
22
For this class of losses, the null hypothesis H0 is rejected when the pos-
terior probability of H0 is too small, the acceptance level a1/(a0 +a1) being
determined by the choice of the loss function. Notice that ϕπ only depends
on a0/a1 and that the larger a0/a1 is, i.e., the more important a wrong
answer under H0 is relative to H1, the smaller the posterior probability of
H0 needs to be for H0 to be accepted.
Example 5.2.3
Consider x ∼B(n, p) and Θ0 = [0, 1/2]. Under the uni-
form prior distribution π(p) = 1, the posterior probability of H0 is
P π(p ≤1/2|x) =
 1/2
0
px(1 −p)n−xdp
B(x + 1, n −x + 1)
=
(1/2)n+1
B(x + 1, n −x + 1)

1
x + 1 +
n −x
(x + 1)(x + 2) + . . . + (n −x)!x!
(n + 1)!

which can be easily computed and compared to the acceptance level.
∥
Example 5.2.4 Consider x ∼N(θ, σ2) and θ ∼N(μ, τ2). Then π(θ|x) is
the normal distribution N(μ(x), ω2) with
μ(x) = σ2μ + τ 2x
σ2 + τ 2
and
ω2 =
σ2τ 2
σ2 + τ 2 .
To test H0 : θ < 0, we compute
P π(θ < 0|x)
=
P π
θ −μ(x)
ω
< −μ(x)
ω

=
Φ (−μ(x)/ω) .
If za0,a1 is the a1/(a0 + a1) quantile, i.e., if it satisﬁes Φ(za0,a1) = a1/(a0 +
a1), H0 is accepted when
−μ(x) > za0,a1ω,
the upper acceptance bound then being
−σ2
τ 2 μ −(1 + σ2
τ 2 )ωza0,a1.
∥
Again, notice that, from a Bayesian point of view, it seems natural to
base the decision upon the posterior probability that the hypothesis is true.
In Section 5.4, we show that an alternative decision-theoretic approach
leads to this posterior probability as the Bayesian estimator itself and thus
avoids the comparison to a predetermined acceptance level. In fact, a dif-
ﬁculty with the losses (5.2.1) is the choice of the weights a0 and a1, since
they are usually selected automatically rather than determined from utility
considerations.

5.2
A ﬁrst approach to testing theory
227
5.2.2 The Bayes factor
While from a decision-theoretic point of view the Bayes factor is only a
one-to-one transform of the posterior probability, this notion came out to
be considered on its own ground in Bayesian testing.
Deﬁnition 5.2.5
The Bayes factor is the ratio of the posterior probabil-
ities of the null and the alternative hypotheses over the ratio of the prior
probabilities of the null and the alternative hypotheses, i.e.,
Bπ
01(x) = P(θ ∈Θ0 | x)
P(θ ∈Θ1 | x)
-π(θ ∈Θ0)
π(θ ∈Θ1).
This ratio evaluates the modiﬁcation of the odds of Θ0 against Θ1 due
to the observation and can naturally be compared to 1, although an exact
comparison scale can only be based upon a loss function. In the particular
case where Θ0 = {θ0} and Θ1 = {θ1}, the Bayes factor simpliﬁes to the
usual likelihood ratio
Bπ
01(x) = f(x|θ0)
f(x|θ1).
In general, the Bayes factor depends on prior information, but is still pro-
posed as an “objective” Bayesian answer, since it partly eliminates the
inﬂuence of the prior modeling and emphasizes the role of the observa-
tions. Actually, it can be perceived as a Bayesian likelihood ratio since, if
π0 is the prior distribution under H0 and π1 the prior distribution under
H1, Bπ
01(x) can be written as
Bπ
01(x) =

Θ0 f(x|θ0)π0(θ) dθ

Θ1 f(x|θ1)π1(θ) dθ = m0(x)
m1(x) ,
(5.2.2)
thus replacing the likelihoods with the marginals under both hypotheses.
Alternatively, if ˆθ0 is the maximum likelihood estimator on Θ0 and ˆθ1 the
maximum likelihood estimator on Θ1, the likelihood ratio
R(x) = f(x|ˆθ0)
f(x|ˆθ1)
= supΘ0 f(x|θ)
supΘ1 f(x|θ)
appears as a particular case of Bπ
01(x) when π0 and π1 are Dirac masses
at ˆθ0 and ˆθ1. This does not legitimize the use of R(x) in the least, though,
because π0 and π1 both depend on x.
As indicated above, the Bayes factor is, from a decision-theoretic point
of view, completely equivalent to the posterior probability of the null hy-
pothesis as, under (5.2.1), H0 is accepted when
Bπ
01(x) > a1
a0
7ϱ0
ϱ1
= a1ϱ1
a0ϱ0
,
(5.2.3)
where
ϱ0
=
π(θ ∈Θ0)
and
ϱ1
=
π(θ ∈Θ1) = 1 −ϱ0.
(5.2.4)

228
Tests and Conﬁdence Regions
5
This alternative version of Proposition 5.2.2 thus provides an illustration of
the duality existing between loss and prior distribution, already mentioned
in Chapter 2. Indeed, (5.2.3) shows that it is equivalent to weight both
hypotheses equally, ϱ0 = ϱ1 = 1/2, and to modify the error penalties into
a′
i = aiϱi (i = 0, 1) or to penalize similarly both types of errors (a1 =
a0 = 1), when the prior distribution incorporates the actual weights in the
weighted prior probabilities,
ϱ′
0 =
a0ϱ0
a0ϱ0 + a1ϱ1
,
ϱ′
1 =
a1ϱ1
a0ϱ0 + a1ϱ1
.
Following Good (1958) and Jeﬀreys (1961), many Bayesians now consider
the Bayes factor on its own ground (see, e.g., Kass and Raftery (1995) for
a detailed review). In particular, Jeﬀreys (1961) developed a scale to judge
the evidence in favor of or against H0 brought by the data, outside a true
decision-theoretic setting. The scale goes as follows:
– if log10(Bπ
10) varies between 0 and 0.5, the evidence against H0 is poor,
– if it is between 0.5 and 1, it is is substantial,
– if it is between 1 and 2, it is strong, and
– if it is above 2 it is decisive.
Obviously, this scaling of the Bayes factor gives some indication of the
strength of the evidence, but the precise bounds separating one strength
from another are a matter of convention and they can be arbitrarily changed,
as shown in Kass and Raftery (1995). This is a consequence of the lack of
decision-theoretic backup via a loss function. (But the same criticism ap-
plies to the conventional α levels of 0.05 or 0.01 used for a0/(a0 + a1) in
(5.2.1).)
Chapter 6 will give precisions on methods used to approximate Bayes
factors when the integral in (5.2.2) cannot be computed analytically, which
is often the case.
Example 5.2.6 (Kass and Raftery (1995)) The “hot hand” in basketball
is the belief that players have good and bad days, rather than a ﬁxed
probability to win a shoot. For a given player, the model under the null
hypothesis (no hot hand) is then H0 : yi ∼B(ni, p) (i = 1, . . . , G), where
G denotes the number of games and ni (yi) the number of shoots (of good
shoots) during the ith game. The model under the general alternative is
H1 : yi ∼B(ni, pi), the probability pi varying from game to game. Under
a conjugate prior pi ∼B(ξ/ω, (1 −ξ)/ω), the average IE[pi|ξ, ω] = ξ is
distributed from a uniform prior U([0, 1]), as is p under H0, and ω is ﬁxed.
The Bayes factor is then
B10
=
 1
0
.G
i=1
 1
0 pyi
i (1 −pi)ni−yipα−1
i
(1 −pi)β−1d pi
(Γ(1/ω)/[Γ(ξ/ω)Γ((1 −ξ)/ω)])G dξ
 1
0 p

i yi(1 −p)

i(ni−yi)d p

5.2
A ﬁrst approach to testing theory
229
=
 1
0
.G
i=1[Γ(yi + ξ/ω)Γ(ni −yi + (1 −ξ)/ω)/Γ(ni + 1/ω)]
(Γ(1/ω)/[Γ(ξ/ω)Γ((1 −ξ)/ω)])G dξ
Γ(
i yi + 1)Γ(
i(ni −yi) + 1)/Γ(
i ni + 2) ,
where α = ξ/ω and β = (1 −ξ)/ω. Formally, the numerator can be com-
puted exactly, despite the gamma functions, because of the simpliﬁcation
Γ(yi + ξ/ω)/Γ(ξ/ω)
=
yi

j=1
(j −1 + ξ/ω),
Γ(ni −yi + (1 −ξ)/ω)/Γ((1 −ξ)/ω)]
=
n−i−yi

j=1
(j −1 + (1 −ξ)/ω) ,
but the function of ξ to be integrated is then a high-degree polynomial. The
resolution of the integral thus calls for a software like Maple or Mathematica.
For a given player, the value of B10 is 0.16 for ω = 0.005 and G = 138,
which does not indicate any conclusive evidence in favor of the hot hand
hypothesis.
∥
5.2.3 Modiﬁcation of the prior
The notion of Bayes factor is also instrumental in pointing out an important
aspect of Bayesian testing. In fact, this factor is only deﬁned when ϱ0 ̸= 0
and ϱ1 ̸= 0. This implies that, if H0 or H1 are a priori impossible, the ob-
servations will not modify this absolute information: Null probabilities are
absorbing states! Therefore, a point-null hypothesis H0 : θ = θ0 cannot be
tested under a continuous prior distribution. More generally, model choice
is incompatible with prior distributions that are absolutely continuous with
respect to the Lebesgue measure on the largest space.
Testing of point-null hypotheses and the like thus impose a drastic mod-
iﬁcation of the prior distribution, since it requires to derive a prior distri-
bution on both subsets Θ0 and Θ1, for instance, the distributions π0 and
π1 with densities
g0(θ) ∝π(θ)IIΘ0(θ),
g1(θ) ∝π(θ)IIΘ1(θ),
(with respect to the natural measures on Θ0 and Θ1) although this deﬁni-
tion is not always free of ambiguity (see Exercise 5.5). Joined with the prior
probabilities ϱ0 and ϱ1 of Θ0 and Θ1 given by (5.2.4), π0 and π1 deﬁne the
prior π. In other words,
π(θ) = ϱ0π0(θ) + ϱ1π1(θ).
(When Θ0 = {θ0}, the prior distribution on Θ0 is just the Dirac mass
at θ0.)
This modiﬁcation of the prior is puzzling from a measure-theoretic point
of view, since it puts some weight on a set previously of measure 0. It

230
Tests and Conﬁdence Regions
5
also highlights the dichotomy imposed by the usual approach to testing
for which the null hypothesis is either right or wrong. However, unless the
decision maker is adamant about the prior distribution π, in which case
H0 should be rejected if π does not give any weight to Θ0, the testing
problem can be considered as providing some additional (although vague)
information about θ. Indeed, to test for θ ∈Θ0 implies that there is a
chance that θ truly belongs to Θ0 and therefore that some possibly ill-
deﬁned indication has been provided about this fact.
To consider testing settings as sources of information is even more con-
vincing if the ﬁnal decision is not the answer to the test but the estimation
of a function of θ, that is, if the test appears as the choice of a submodel.
A preliminary test about the vague information may then improve the es-
timation step. Moreover, keeping this model choice perspective as the real
purpose of the analysis, it also makes sense to build up a separate prior
distribution for each subspace because only one of the two Θi will be con-
sidered after the testing step. For instance, given a point-null hypothesis,
H0 : θ = θ0, the noninformative distribution π(θ) = 1 cannot be considered
as an acceptable prior on Θ because the particular value θ0 has been sin-
gled out as a possible value for θ. (In Chapter 7, we shall further defend the
perspective that similar parameters appearing in two diﬀerent models must
be considered as separate entities.) In general, to consider that the test-
ing problem occurs because of (unavailable) additional observations may
help in the derivation of a noninformative prior, even though there is no
consensus on noninformative Bayes modeling for tests (see Section 5.3.5).
5.2.4 Point-null hypotheses
A usual criticism of point-null hypothesis settings is that they are not re-
alistic (see, e.g., Casella and Berger2 (1987)). For instance, as pointed out
by Good (1980), it does not actually make sense to test whether the prob-
ability of rain for tomorrow is 0.7163891256 . . .3 However, some statistical
problems deﬁnitely call for point-null hypothesis testing. For instance, in
mixture estimation (see Section 1.1 and Section 6.4), it may be important
to know whether a mixture distribution has two or three components, so
one may test whether one of the component weights is 0. Similarly, in linear
regression, tests on the nullity of the regression coeﬃcients are useful for
the elimination of useless exogenous variates, as in Example 5.2.1. Even
more to the point, testing whether the Universe is expanding, contracting
or stable is akin to testing whether a certain constant is larger, smaller or
equal to a speciﬁc value.
More generally, two-sided hypotheses such as H0 : θ ∈Θ0 = [θ0 −ϵ, θ0 +
2 Roger Berger, not James Berger!
3 But it would still make sense to test whether the prediction of 75% given by the local
weather forecaster is exact, that is, whether the probability of rain for the given day is
0.75, or another of the probabilities announced by the forecaster (see Example 2.3.4).

5.2
A ﬁrst approach to testing theory
231
ϵ] can be approximated by H0 :
θ = θ0, with hardly any modiﬁcation
of the posterior probabilities when ϵ is small enough. For instance, this
happens when the likelihood is constant in a neighborhood of θ0 (see Berger
(1985a) and Berger and Delampady (1987)). Point-null hypotheses are also
quite important in practice; for instance, while it makes sense to determine
whether a medical treatment has a positive or negative eﬀect, the ﬁrst issue
may be to decide whether it has any eﬀect at all.
Considering the point-null hypothesis H0 :
θ = θ0, we denote by ϱ0
the prior probability that θ = θ0 and by g1 the prior density under the
alternative. The prior distribution is then π0(θ) = ϱ0IIΘ0(θ) + (1 −ϱ0)g1(θ)
and the posterior probability of H0 is given by
π(Θ0|x) =
f(x|θ0)ϱ0

f(x|θ)π(θ) dθ =
f(x|θ0)ϱ0
f(x|θ0)ϱ0 + (1 −ϱ0)m1(x),
the marginal distribution on H1 being
m1(x) =

Θ1
f(x|θ)g1(θ) dθ.
This posterior probability can also be written as
π(Θ0|x) =

1 + 1 −ϱ0
ϱ0
m1(x)
f(x|θ0)
−1
.
Similarly, the Bayes factor is
Bπ
01(x) =
f(x|θ0)ϱ0
m1(x)(1 −ϱ0)
-
ϱ0
1 −ϱ0
= f(x|θ0)
m1(x)
and we derive the following general relation between the two quantities:
π(Θ0|x) =

1 + 1 −ϱ0
ϱ0
1
Bπ
01(x)
−1
.
Example 5.2.7 (Example 5.2.3 continued) Consider the test of H0 :
p = 1/2 against p ̸= 1/2. For g1(p) = 1, the posterior probability is then
given by
π(Θ0|x)
=

1 + 1 −ϱ0
ϱ0
2nB(x + 1, n −x + 1)
−1
=

1 + 1 −ϱ0
ϱ0
x!(n −x)!
(n + 1)! 2n
−1
,
since m(x) =
	n
x

B(x + 1, n −x + 1). For instance, if n = 5, x = 3, and
ϱ0 = 1/2, the posterior probability is

1 +
1
12025
−1
= 15
19
and the corresponding Bayes factor is 15/8, close to 2. So, in the most
supporting cases, the posterior probabilities tend to favor H0. When the

232
Tests and Conﬁdence Regions
5
Table 5.2.1. Posterior probabilities of p = 1/2 when n = 10.
x
0
1
2
3
4
5
P (p = 1/2|x)
0.0106
0.0970
0.3259
0.5631
0.6928
0.7302
Table 5.2.2. Posterior probabilities of θ = 0 for diﬀerent values of z = x/σ
and for τ = σ.
z
0
0.68
1.28
1.96
π(θ = 0|z)
0.586
0.557
0.484
0.351
sample size increases, the range of the possible answers also widens. For
instance, if π(p) is Be(1/2, 1/2) and n = 10, the posterior probabilities are
given in Table 5.2.4 and support H0 for x close to 5, even though the prior
distribution is rather biased against the null hypothesis (since it heavily
weights the extreme values, 0 and 1).
∥
Example 5.2.8 (Example 5.2.4 continued) Consider the test of H0 :
θ = 0. It seems reasonable to choose π1 as N(μ, τ2) and μ = 0, if no
additional information is available. Then
m1(x)
f(x|0)
=
σ
√
σ2 + τ 2
e−x2/2(σ2+τ 2)
e−x2/2σ2
=

σ2
σ2 + τ 2 exp

τ 2x2
2σ2(σ2 + τ 2)

,
and the posterior probability can be derived as
π(θ = 0|x) =

1 + 1 −ϱ0
ϱ0

σ2
σ2 + τ 2 exp

τ 2x2
2σ2(σ2 + τ 2)
−1
.
In the special case when ϱ0 = 1/2 and τ = σ, Table 5.2.4 gives the posterior
probabilities in terms of z = x/σ.
∥
Consider now the alternative case τ2 = 10σ2; it is supposed to indicate
a more diﬀuse prior information on θ. The posterior probabilities of H0 are
then modiﬁed as shown in Table 5.2.4.
5.2.5 Improper priors
The recourse to noninformative prior distributions for testing hypotheses
is rather delicate, and DeGroot (1973) states that improper priors should
not be used at all in tests. In fact, as noticed previously, the testing setting

5.2
A ﬁrst approach to testing theory
233
Table 5.2.3. Posterior probabilities of θ = 0 for τ 2 = 10σ2 and z = x/σ.
z
0
0.68
1.28
1.96
π(θ = 0|x)
0.768
0.729
0.612
0.366
Table 5.2.4. Posterior probabilities of |θ| < 1.
x
0.0
0.5
1.0
1.5
2.0
π(|θ| ≤1|x)
0.683
0.625
0.477
0.302
0.157
is not coherent with an absolute lack of information, since it implies at
least a division of the parameter space into two subsets, out of which at
least one is of measure zero under the conventional Jeﬀreys prior. However,
the inconvenience with the use of improper prior distributions goes deeper,
since they are incompatible with most tests of point-null hypotheses.
We illustrate this diﬃculty in the normal setting, x ∼N(θ, 1), for the
point-null hypothesis H0 : θ = 0 to test against H1 : θ ̸= 0. If we use the
improper prior π(θ) = 1 on {θ ̸= 0}, i.e., if π is
π(θ) = 1
2II0(θ) + 1
2 · 1,
the posterior probability of H0 is
π(θ = 0|x) =
e−x2/2
e−x2/2 +
 +∞
−∞e−(x−θ)2/2 dθ
=
1
1 +
√
2πex2/2 .
(The particular choice of the constant 1 in the prior is crucial for the
following discussion, while being arbitrary, as seen below.) Therefore, this
posterior probability of H0 is bounded from above by 1/(1+
√
2π) = 0.285.
This implies that the posterior distribution is rather biased against H0,
even in the most favorable case. Unless the scale of comparison, that is,
the loss, is modiﬁed to account for these low values, the null hypothesis
will quite often be rejected . A similar phenomenon occurs when Θ0 is
compact. For instance, the test of H0 : |θ| ≤1 versus H1 : |θ| > 1 leads
to the following posterior probability:
π(|θ| ≤1|x)
=
 1
−1 e−(x−θ)2/2 dθ
 +∞
−∞e−(x−θ)2/2 dθ
=
Φ(1 −x) −Φ(−1 −x)
=
Φ(x + 1) −Φ(x −1),
whose numerical values are given in Table 5.2.5. Therefore, the maximal
support of H0, 0.683, is still moderate.

234
Tests and Conﬁdence Regions
5
Table 5.2.5. Posterior probabilities of θ = 0 for the Jeﬀreys prior π(θ) = 1.
x
0.0
1.0
1.65
1.96
2.58
π(θ = 0|x)
0.285
0.195
0.089
0.055
0.014
An interesting feature of the Lebesgue prior distribution can be exhi-
bited for the point-null hypothesis H0 : θ = 0. The resulting procedure
agrees with the corresponding classical answer, as shown by Table 5.2.5.
The posterior probability π(θ = 0|x) is indeed quite close to the classical
signiﬁcance levels 0.10, 0.05, and 0.01 when x is 1.65, 1.96, or 2.58 (it
will be demonstrated in Section 5.3.4 that this comparison is meaningful).
This coincidence does not hold for all values of x but shows that, for usual
signiﬁcance levels (and testing purposes), the classical answer could be
considered as a noninformative Bayes answer, even though it corresponds
to a hardly defendable prior.
Another illustration of the delicate issue of improper priors in testing
settings is provided by the Jeﬀreys–Lindley paradox. In fact, limiting argu-
ments are not valid in testing settings and prevent an alternative derivation
of noninformative answers. For instance, considering the conjugate prior
distributions introduced in Example 5.2.8, the posterior probabilities are
π(θ = 0|x) =

1 + 1 −ϱ0
ϱ0

σ2
σ2 + τ 2 exp

τ 2x2
2σ2(σ2 + τ 2)
−1
,
which converge to 1 when the prior variance τ goes to +∞, for every
x. This limit diﬀers from the “noninformative” answer derived previously
[1 +
√
2π exp(x2/2)]−1 and, more importantly, is totally useless. This phe-
nomenon can also be observed by comparing Tables 5.2.4 and 5.2.4, since
the probability is larger when τ2 = 10σ2 than when τ = σ for all the values
of z considered in the tables. See Aitkin (1991) and Robert (1993b) for
recent discussions on this paradox.
Paradoxes associated with improper priors like the Jeﬀreys–Lindley ex-
ample are actually because of a weighting indeterminacy that does not
occur for point estimation, or even for one-sided tests.
Example 5.2.9
Consider x ∼N(θ, 1) and H0 :
θ ≤0 to test versus
H1 : θ > 0. For the diﬀuse distribution π(θ) = 1,
π(θ ≤0|x)
=
1
√
2π
 0
−∞
e−(x−θ)2/2 dθ
=
Φ(−x).
In this case, the generalized Bayes answer is also the classical procedure,
called the p-value (see Section 5.3.4).
∥
For two-sided problems, if g0 and g1 are σ-ﬁnite measures corresponding

5.2
A ﬁrst approach to testing theory
235
Table 5.2.6. Posterior probabilities of θ = 0 for the Jeﬀreys prior π(θ) = 10.
x
0.0
1.0
1.65
1.96
2.58
π(θ = 0|x)
0.0384
0.0236
0.0101
0.00581
0.00143
to truncated noninformative priors on the subspaces Θ0 and Θ1, the choice
of the normalizing constants will inﬂuence the Bayesian estimator. In fact,
if gi is replaced by cigi (i = 0, 1), the Bayes factor is multiplied by c0/c1.
For instance, if the Jeﬀreys prior is uniform and g0 = c0, g1 = c1, the
posterior probability is
π(θ ∈Θ0|x)
=
ϱ0c0

Θ0 f(x|θ) dθ
ϱ0c0

Θ0 f(x|θ) dθ + (1 −ϱ0)c1

Θ1 f(x|θ) dθ
=
ϱ0

Θ0 f(x|θ) dθ
ϱ0

Θ0 f(x|θ) dθ + (1 −ϱ0)[c1/c0]

Θ1 f(x|θ) dθ ,
which does depend on the ratio c1/c0.
It is therefore necessary to extend the noninformative perspective to
these testing settings by developing a technique able to derive the weights
ci in a noninformative and acceptable way. Bernardo (1980), Spiegelhalter
and Smith (1980), Smith and Spiegelhalter (1982), Aitkin (1991), Pettit
(1992), Robert (1993b) and Berger and Pericchi (1996a,b) have made pro-
posals in this direction, as detailed in Section 5.2.6. Notice that Jeﬀreys
(1961) proposed instead to use proper priors in such settings, like C(0, σ2)
or N(0, 10σ2) in the case of x ∼N(θ, σ2) and H0 : θ = 0. The problem
is then that the choice of the proper prior distribution will inﬂuence the
answer to the test. For instance, the equivalent of Table 5.2.5 for π(θ) = 10
are given in Table 5.2.5, with important diﬀerences for most values of x,
since they vary by an order of magnitude.
Before introducing in Section 5.2.6 some of the recent developments
linked to the use of improper priors, let us make the following point: it
still does not feel right to use improper priors, like the Jeﬀreys priors, for
two-sided tests because they seem to lead to too much arbitrariness, in the
sense that many competing solutions abound, with about the same theoret-
ical background and with diﬀerent numerical values, in contradiction with
the Likelihood Principle. In other words, although the solutions proposed
in the next section are interesting and overcome, as working principles,
the diﬃculties related to the use of improper priors, they are not, strictly
speaking, pertaining to the Bayesian paradigm. We consider in Section 5.3
an alternative approach which deﬁnes a least favorable Bayesian answer as
a lower bound on the (proper) Bayes estimators.
The diﬃculties encountered with noninformative priors in testing settings
also point out that a testing problem cannot be treated in a coherent way

236
Tests and Conﬁdence Regions
5
if no prior information is available, that is, that the information brought by
the observations alone is usually not enough to infer about the truth of a
hypothesis in a categorical fashion (yes/no). This obviously reinforces the
motivation for a Bayesian treatment of such testing problems, since it is
the only coherent approach taking advantage of the residual information.
5.2.6 Pseudo-Bayes factors
Most4 of the solutions proposed to overcome the diﬃculties related to the
use of improper priors (see Section 5.2.5) are based on the idea to use part
of the data/information to transform the priors into proper distributions,
or to call to imaginary observations to obtain the same eﬀect.
Deﬁnition 5.2.10 Given an improper prior π, a sample (x1, . . . , xn) is a
training sample if the corresponding posterior π(˙|x1, . . . , xn) is proper and
is a minimal training sample if no subsample is a training sample.
Example 5.2.11 For the N(μ, σ2) model, the minimal training sample
size associated with the improper prior π0(μ, σ2) = 1/σ2 is 2 since

e−{(x1−μ)2+(x2−μ)2}/2σ2σ−4dμ dσ2
=
 ∞
0
σ−3e−s2/2σ2dσ2
=
 ∞
0
ω3/2−2e−s2ω/2dω ,
while

e−(x1−μ)2/2σ2σ−3dμ dσ2 = ∞.
If we now consider the prior π1(μ, σ2) = 1/σ, the minimal training sample
size is 3 since

e−{(x1−μ)2+(x2−μ)2}/2σ2σ−3dμ dσ2
=
 ∞
0
σ−2e−s2/2σ2dσ2
=
 ∞
0
ω−1e−s2ω/2dω = ∞,
which is a good argument in favor of π0 against π1.
∥
The idea is then to use a minimal training sample, x(ℓ) say, to “properize”
the improper prior π into π(·|x(ℓ)), and then to use this posterior distribu-
tion as if it were a regular proper prior for the remainder of the sample,
4 This section, which may be skipped on a ﬁrst reading, contains more advanced ma-
terial on the so-called intrinsic priors developed by Berger and Pericchi (1996a,b). It
will not be used in the rest of the book, except in Chapter 7. See Berger and Pericchi
(2001) for a much more detailed review, upon which this section is based.

5.2
A ﬁrst approach to testing theory
237
x(−ℓ) say, in order to avoid the data twice, as in the proposal of Aitkin
(1991). When facing a hypothesis H0 with a prior distribution π0, with a
broader alternative H1 with a prior distribution π1, if the minimal training
sample under H1 is such that π0(·|x(ℓ)) is also proper, the pseudo-Bayes
factor
B(ℓ)
10 =

Θ1 f1(x(−ℓ)|θ1)π1(θ1|x(ℓ))dθ1

Θ0 f0(x(−ℓ)|θ0)π0(θ0|x(ℓ))dθ0
(5.2.5)
is then independent from the normalizing constants used in both π0 and
π1. To see this more clearly, consider the following representation proposed
in Berger and Pericchi (2001).
Lemma 5.2.12 In the case of independent distributions, the pseudo-Bayes
factor can be written as
B(ℓ)
10 = B10(x) × B01(x(ℓ)) ,
(5.2.6)
with
B10(x) =

Θ1 f1(x|θ1)π1(θ1)dθ1

Θ0 f0(x|θ0)π0(θ0)dθ0
and
B01(x(ℓ)) =

Θ0 f0(x(ℓ)|θ0)π0(θ0)dθ0

Θ1 f1(x(ℓ)|θ1)π1(θ1)dθ1
.
In this decomposition, B10(x) and B01(x(ℓ)) are the Bayes factors com-
puted for the unormalized priors π1 and π0, for the whole sample x and
the training sample x(ℓ), respectively, as if these were regular priors. It is
then straightforward to see that multiplying π0 by c0 and π1 by c1 has
no inﬂuence on B(ℓ)
10 because these constants cancel. Notice the interesting
inversion from B10(x) to B01(x(ℓ)): the training sample eﬀect is removed
from the Bayes factor B10(x).
While the normalizing constant problem disappears, a remaining diﬃ-
culty is that the solution B(ℓ)
10 is not strictly Bayesian. Moreover, besides
sequential sampling, there is no obvious choice for x(ℓ), while this choice of
the training sample inﬂuences the resulting value of B(ℓ)
10 (and thus violates
the Likelihood Principle).
Example 5.2.13 (Example 5.2.11 continued)
If H0 : μ = 0, with
π0(σ2) = 1/σ2, and H1 : μ ̸= 0, with π1(μ, σ2) = 1/σ2, the minimal
training sample of size 2 under H1. Then
π1(μ, σ2|x1, x2) = 1
σ exp{−2(μ −¯x1)2/2σ2}s5
1σ−3e−s2
1/2σ2
and
π0(σ2|x1, x2) = s6
0
σ4 e−s2
0/2σ2 ,
with the following notations:
¯x1 = x1 + x2
2
,
s2
1 = (x1 −x2)2
2
,
s2
0 = x2
1 + x2
2 .

238
Tests and Conﬁdence Regions
5
Then
B(2)
10 = s5
1
 e−{(n−2)(¯x2−μ)2−2(μ−¯x1)2−s2
2−s2
1}/2σ2σ−n−2dμdσ2
s6
0
 ∞
0
e−{−s2
3−s2
0}/2σ2σ−n−2dσ2
depends on the choice of (x1, x2) through (¯x1 −¯x2)2, s2
1, s2
0 (see Exercise
5.14).
∥
A way to remove this dependence on the training sample is to average the
diﬀerent pseudo-Bayes factors (5.2.6) over all the possible training samples
x(ℓ). The next diﬃculty is to decide what kind of averaging should be used
here. For instance, Berger and Pericchi (1996a, 1998, 2001) list
- the arithmetic intrinsic Bayes factor,
BA
10 = 1
L

x(ℓ)
B(ℓ)
10 = B10(x) 1
L

x(ℓ)
B01(x(ℓ)) ,
(5.2.7)
where L is the number of diﬀerent training samples;
- the geometric intrinsic Bayes factor,
BG
10 = exp 1
L

x(ℓ)
log B(ℓ)
10 = B10(x) exp 1
L

x(ℓ)
log B01(x(ℓ)) ;
(5.2.8)
and
- the median intrinsic Bayes factor,
BM
10 = med B(ℓ)
10 = B10(x)med B01(x(ℓ)) ,
(5.2.9)
where med B(ℓ)
10 denotes the median of the B(ℓ)
10 over the diﬀerent training
samples.
Although these solutions all come close to a Bayesian answer, in par-
ticular by using the data only once (Exercise 5.15) by separating the part
used to “properize” the improper prior and the part used to run the test,
none of the above is a true Bayesian answer. We will discuss below more
serious drawbacks to these diﬀerent intrinsic Bayes factors, but it appears
that they sometimes correspond to genuine Bayes factors under diﬀerent
prior distributions, called intrinsic priors in Berger and Pericchi (1996a,
1998).5 (This phenomenon will also occur in Section 5.3.5 with Berger and
Sellke’s (1987) lower bounds.)
Example 5.2.14 (Berger and Pericchi (1998)) In the N(θ, 1) case, when
H0 : θ = 0 and π1(θ) = 1, for a sample (x1, . . . , xn), the arithmetic intrinsic
Bayes factor,
BA
10 = B10(x)
1
√
2π
1
n
n

i=1
e−x2
i /2,
5 The name intrinsic associated with the Bayes factor and the corresponding prior tries
to convey an idea of quantities solely derived from the distribution, but the diversity
of possible answers shows this is somehow a misnomer!

5.2
A ﬁrst approach to testing theory
239
0
2
4
6
8
10
0
1
2
3
4
5
Figure 5.2.1. Graph of the intrinsic prior associated with the exponential testing,
when θ0 = 1.
is almost identical to the regular Bayes factor associated with a normal
N(0, 2) prior under H1.
∥
Example 5.2.15 (Berger and Pericchi (1998)) For x1, . . . , xn i.i.d. obser-
vations from a translated exponential distribution, with density exp(θ −
x)IIx≥θ, if H0 : θ = θ0 and H1 : θ > θ0, with π1(θ) = 1,
BA
10 = B10(x) 1
n
n

i=1
+
exi−θ0 −1
,−1 ,
which corresponds to the regular Bayes factor associated with the proper
prior
π2(θ) = eθ0−θ 
1 −log
	
1 −eθ0−θ

,
which behaves as indicated on Figure 5.2.6.
∥
O’Hagan (1995) introduces an alternative to intrinsic Bayes factors that
avoids the selection of—and the subsequent averaging over—training sam-
ples. His idea is to use a fraction b of the likelihood to properize the prior,
that is, to take 0 < b < 1 such that

Θ0
f0(x|θ0)bπ0(θ0)dθ0 < ∞

240
Tests and Conﬁdence Regions
5
and

Θ1
f1(x|θ1)bπ1(θ1)dθ1 < ∞.
The remaining fraction (1−b) of the likelihood is then used to run the test,
as in the intrinsic Bayes factor case. The fractional Bayes factor is thus
deﬁned as
BF
10
=

Θ1 f1(x|θ1)1−bπb
1(θ1|x)dθ1

Θ0 f0(x|θ0)1−bπb
0(θ0|x)dθ0
=
B10(x)

Θ0 f0(x|θ0)bπ0(θ0)dθ0

Θ1 f1(x|θ1)bπ1(θ1)dθ1
,
(5.2.10)
where πb
0(θ0|x) and πb
1(θ1|x) denote the pseudo-posteriors associated with
f0(x|θ0)b and f1(x|θ1)b, respectively. For exponential families, the fraction
b clearly corresponds to a training sample size, since for n observations
from an exponential family with suﬃcient statistic T ,
exp{θ · n T (x) −nΨ(θ)}b = exp {θ · [bn] T (x) −[bn]Ψ(θ)} .
For other distributions, the fraction b must be determined through a more
ad-hoc approach (see O’Hagan (1995, 1997)).
As in the intrinsic Bayes factor case, there exist occurrences where this
solution is equal to a regular Bayes factor with a corresponding “intrinsic”
prior.
Example 5.2.16 (Example 5.2.14 continued) For any 0 < b < 1,
BF
10
=

e−n(1−b)(¯x−θ)2/2√
be−nb((¯x−θ)2/2dθ
√
2πe−n(1−b)¯x2/2
=
√
ben(1−b)¯x2/2 ,
(5.2.11)
which is equal to the Bayes factor associated with the proper θ ∼N(0, (1−
b)/nb) under H1.
∥
There are, however, enough diﬃculties with these pseudo-Bayes factors
to make us question their use in testing and model choice problems:
(i) Bayes factors, when associated with proper priors, do satisfy some
coherence properties such as
B12 = B10B02
and
B01 = 1/B10 .
Most pseudo-Bayes factors do not, even though the fractional Bayes
factor satisﬁes BF
01 = 1/BF
10.
(ii) When the pseudo-Bayes factors can be expressed as true Bayes fac-
tors, the corresponding intrinsic priors are not necessarily appealing,
as shown in Example 5.2.14 for the arithmetic Bayes factor and in
Example 5.2.16 for the fractional Bayes factor, and these priors do de-
pend on the choice of the improper reference priors π0 and π1, hence
are hardly intrinsic.

5.2
A ﬁrst approach to testing theory
241
(iii) Following the above point, the pseudo-Bayes factors may also exhibit
a bias in favor of one of the hypotheses, in the sense that they can be
expressed as a true Bayes factor multiplied by a certain factor.
Example 5.2.17 (Example 5.2.15 continued) For the median in-
trinsic Bayes factor,
BM
10
=
B10(x)
/
emed(xi) −θ0
0−1
=
0.69 ˜B10(x)
(5.2.12)
where ˜B10(x) is the Bayes factor associated with the prior π3(θ) ∝
(2 exp{θ −θ0} −1)−1, which, while being similar to π2, does not provide
exactly the same coverage of regions near 1.
∥
In such cases, the pseudo-Bayes factors can be seen as modifying the
probabilities of both hypotheses from the reference value 1/2, a feature
we will also encounter for least favorables bounds in Section 2.5.2.
(iv) Most often, however, the pseudo-Bayes factors do not correspond to
any true Bayes factor, and they may give strongly biased solutions.
For instance, Berger and Pericchi (2001) state that arithmetic intrinsic
Bayes factors do not have intrinsic priors for most one-sided testing
problems.
Example 5.2.18 (Example 5.2.15 continued) The fractional Bayes
factor,
BF
10 = B10(x)bn

e−bn(x(1)−θ0) −1
 −1
,
(5.2.13)
is always larger than 1, thus, always favors the alternative hypothesis,
according to Jeﬀreys’ scale. This paradoxical behavior can be related to
the fact that the fraction b does not modify the indicator function.
∥
(v) Pseudo-Bayes factors may simply not exist for a whole class of models.
Example 5.2.19 Mixtures of normal distributions
pN(μ1, σ2
1) + (1 −p)N(μ2, σ2
2)
were presented in Example 1.1.6. As shown in Exercise 1.57, improper
priors of the form π1(μ1, σ1)π2(μ2, σ2) cannot be used in this setting,
whatever the sample size n. (The fundamental reason for this is that
there is a probability (1 −p)n that no observation is associated with the
ﬁrst component N(μ1, σ2
1).) Therefore, standard noninformative priors
do not allow for training samples and intrinsic Bayes factors cannot
be constructed. The same applies to fractional Bayes factors (see Exer-
cise 5.21).
∥
(vi) As shown in this section, there are many ways of deﬁning pseudo-Bayes
factors and, while most are arguably logical, there is no coherent way

242
Tests and Conﬁdence Regions
5
of ordering them. Pseudo-Bayes factors, as deﬁned here, do agree with
the Likelihood Principle, but the multiplication of possible answers,
even if those are close, is not a good signal to users.6 Similarly, there is
no clear-cut procedure for the choice of the fraction b in the fractional
Bayes factors, since the minimal training sample size is not always
clearly deﬁned.
(vii) The issue of computing pseudo-Bayes factors has not been mentioned
so far, for lack of appropriate tools, which will be introduced in Chap-
ters 6 and 7. But each Bayes factor B(ℓ)
10 may be a complex integral and
the derivation of the averaged intrinsic Bayes factor may involve
	m
n

integrals of this kind, if m is the minimal training sample size. Frac-
tional Bayes factors are easier to compute in exponential settings, but
other distributions are much more diﬃcult to handle (Exercise 5.22).
5.3 Comparisons with the classical approach
5.3.1 UMP and UMPU tests
The classical approach to testing theory is the theory of Neyman–Pearson,
as presented, for instance, in Lehmann (1986). For the 0 −1 loss, denoted
L below, the frequentist notion of optimality is based on the power of a
test, deﬁned as follows:
Deﬁnition 5.3.1
The power of a testing procedure ϕ is the probability of
rejecting H0 under the alternative hypothesis, that is, β(θ) = 1 −IEθ[ϕ(x)]
when θ ∈Θ1. The quantity 1 −β(θ) is called type–two error, while the
type–one error is IEθ[ϕ(x)] when θ ∈Θ0.
Optimal frequentist tests are then those that minimize the risk IEθ[L(θ,
ϕ(x))] only under H1:
Deﬁnition 5.3.2
If α ∈]0, 1[ and Cα is the class of the procedures ϕ sat-
isfying the following constraint on the type I error:
sup
θ∈Θ0
IEθ[L(θ, ϕ(x))] = sup
θ∈Θ0
Pθ(ϕ(x) = 0) ≤α,
a test procedure ϕ is said to be uniformly most powerful at level α (UMP)
if it minimizes the risk IEθ[L(θ, ϕ(x))] uniformly on Θ1 in Cα.
This optimality is much weaker than the notion of admissibility devel-
oped in Section 2.4. In fact, the loss is bidimensional in this setting because
of the restriction on the type I error, namely, supΘ0 IEθ[L(θ, ϕ)] ≤α. This
6 Berger and Pericchi (2001) argue that the multiplicity of intrinsic Bayes factors is no
more worrying than the multiplicity of possible default priors. The parallel is slightly
ﬂawed, though, because every default prior induces a multiplicity of intrinsic Bayes
factors!

5.3
Comparisons with the classical approach
243
restriction is usually necessary to obtain an optimal test procedure, since
the risk functions of admissible procedures do cross, but:
(i) It produces a situation of asymmetry between the null and the alter-
native hypotheses, which induces an unnatural behavior for the test
procedures. In fact, since the type I error is ﬁxed, a balance between
the two types of error (acceptance under H1 and rejection under H0)
is impossible, hence a much bigger type II error. This asymmetry is
also responsible for the theory bypassing minimaxity considerations.
For instance, this is the case when the two hypotheses H0 and H1
are contiguous, that is, when it is possible to go from Θ0 to Θ1 by a
continuous transformation.
(ii) It implies the selection of a signiﬁcance level α by the decision maker,
in addition to the choice of the loss function L, and this generally
leads to the call to “standard” levels, 0.05 or 0.01, and the drawbacks
of these “universal” levels (see below).
(iii) It does not necessarily imply a suﬃcient reduction of the class of test
procedures and does not always allow for the selection of a unique opti-
mal procedure. It is sometimes necessary to impose further constraints
on these classes.
In the simplest case, in which null and alternative hypotheses are point
hypotheses, H0 : θ = θ0 versus H1 : θ = θ1, the Neyman–Pearson lemma
establishes that there exist UMP test procedures and that they are of the
form7
ϕ(x) =

1
if f(x|θ1) < kf(x|θ0),
0
otherwise,
k being related to the selected signiﬁcance level α. Obviously, the fact that
Θ1 is reduced to {θ1} is quite helpful, since it allows for a total ordering
of the procedures of Cα. For monotone likelihood ratio families, that is,
parametrized families for which there exists a statistic T (x) such that
f(x|θ′)
f(x|θ)
is increasing in T (x) for θ′ > θ, Karlin and Rubin (1956) have estab-
lished the following extension of the Neyman–Pearson lemma (see Lehmann
(1986, p. 79) for a proof).
Proposition 5.3.3
Consider f(x|θ) with a monotone likelihood ratio in
T (x). For H0 : θ ≤θ0 and H1 : θ > θ0 there exists a UMP test such that
ϕ(x) =
⎧
⎨
⎩
1
if T (x) < c,
γ
if T (x) = c,
0
otherwise,
7 Conserving the interpretation that a test procedure is an estimator of IIΘ0(θ), the
test procedures in this book are complements to 1 of the classical Neyman–Pearson
procedures, for which a value of 1 corresponds to the rejection of H0.

244
Tests and Conﬁdence Regions
5
γ and c being determined by the constraint
IEθ0[ϕ(x)] = α.
Karlin and Rubin (1956) have also shown that, for the loss functions of
the class (5.2.1), the test procedures provided in Theorem 5.3.3 form an
essentially complete class, that is, a class of procedures large enough to be
at least as good as any other procedure (see Chapter 8). Moreover, if the
support of the distribution f(x|θ) is independent of θ, the class obtained
in Proposition 5.3.3 is minimal essentially complete: it cannot be reduced
any further (see Lehmann (1986, pp. 82-83)), and therefore only contains
optimal procedures.
Notice that an important class of monotone likelihood ratio families con-
sists of the exponential families, since
f(x|θ′)
f(x|θ) = eθ′x−ψ(θ′)
eθx−ψ(θ) =
e(θ′−θ)x
eψ(θ′)−ψ(θ)
is increasing in x. Pfanzagl (1968) has also established a reciprocal to
Proposition 5.3.3 in the spirit of the Pitman–Koopman lemma (see Sec-
tion 3.3.3), namely, that the existence of a UMP test for every sample size
and a given level α implies that the distribution belongs to an exponential
family.
Example 5.3.4
Consider x ∼P(λ) and H0 : λ ≤λ0, H1 : λ > λ0. For
m independent observations from this distribution, a suﬃcient statistic is
s = 
i xi ∼P(mλ) and, according to Proposition 5.3.3, a UMP test is
given by
ϕ(x) =
 1
if s < k,
γ
if s = k,
0
otherwise,
for IEλ0[ϕ(x)] = Pmλ0(s > k) + γPmλ0(s = k) = α.
∥
Proposition 5.3.3 and the above example stresses a major diﬃculty with
the Neyman–Pearson approach, namely, that arbitrary signiﬁcance levels
are not necessarily attainable unless one calls for randomization. Indeed, as
the decision space is D = {0, 1}, ϕ(x) = γ means that ϕ(x) = 1 with prob-
ability γ (and 0 otherwise). Such procedures are obviously incompatible
with the Likelihood Principle, although they only appear for discrete cases.
Lehmann (1986) indicates that the signiﬁcance level α should be modiﬁed
until randomization is avoided, but this modiﬁcation induces another draw-
back: the choice of the signiﬁcance level depends on the observation, not
on a utility function.
Moreover, Proposition 5.3.3 only applies to one-sided hypotheses. In a
particular case of two-sided hypotheses, we can exhibit an optimality result
(see Lehmann (1986, pp. 101–103)).

5.3
Comparisons with the classical approach
245
Proposition 5.3.5
Consider an exponential family
f(x|θ) = eθT (x)−ψ(θ)h(x)
and H0 : θ ≤θ1 or θ ≥θ2, H1 : θ1 < θ < θ2. There exists a UMP test of
the form
ϕ(x) =
⎧
⎨
⎩
0
if c1 < T (x) < c2,
γi
if T (x) = ci
(i = 1, 2),
1
otherwise,
with (i = 1, 2)
IEθi[ϕ(x)] = α.
However, there is no corresponding UMP test for the opposite case, i.e.,
H0 :
θ1 ≤θ ≤θ2. This paradox illustrates quite forcibly the lack of
symmetry—and therefore of coherence—of the UMP criterion, but is also
quite puzzling, and casts a doubt on the validity of the Neyman–Pearson
analysis or the relevance of a symmetric loss like the 0 −1 loss. In these
cases, the Neyman–Pearson solution is to propose an additional reduction
of the class of test procedures by considering unbiased tests, i.e., those also
satisfying
sup
Θ0
Pθ(ϕ(x) = 0) ≤inf
Θ1 Pθ(ϕ(x) = 0).
In other words, ϕ must also satisfy
inf
Θ0 IEθ[ϕ(x)] ≥sup
Θ1
IEθ[ϕ(x)].
The notion of uniformly most powerful unbiased tests (UMPU) then fol-
lows. However, this restriction induces a further asymmetry between H0
and H1. Although intuitively acceptable, this notion of unbiasedness is yet
another example of the restrictions imposed by the frequentist approach to
optimality which denaturate the true purpose of Decision Theory.
Example 5.3.6 If, for x ∼N(θ, 1), we test H0 : θ = 0 versus H1 : θ ̸= 0,
there is no UMP test. A UMPU test at level α = 0.05 is
ϕ(x) =
 1
if |x| ≤1.96,
0
otherwise.
∥
5.3.2 Least favorable prior distributions
When no UMPU test is available, it gets quite diﬃcult for the classical
approach to defend, or even construct, a speciﬁc testing procedure. Apart
from restricting any further the class of acceptable procedures, a customary
approach is to consider the likelihood ratio
supθ∈Θ0 f(x|θ)
supθ∈Θ1 f(x|θ)
(5.3.1)

246
Tests and Conﬁdence Regions
5
and its distribution, or to base the test upon the asymptotic distribution
of (5.3.1). This ratio exhibits a link with the Bayesian approach, since, as
mentioned above, it appears as a Bayes factor for a prior distribution π
that is supported by ˆθ0 and ˆθ1, maximum likelihood estimators of θ on
Θ0 and Θ1. This analysis is rather formal, since Dirac masses are artiﬁcial
priors and, moreover, the ˆθi’s depend on the observation. However, it also
indicates that the likelihood ratio has a Bayesian motivation.
Relations between Bayesian testing procedures and Neyman–Pearson op-
timal tests are given in Lehmann (1986) through the notion of a least favor-
able distribution, described below.8 Consider H0 : θ ∈Θ0, H1 : θ = θ1 with
π a prior distribution on Θ0. From a Bayesian point of view, the test prob-
lem can be represented as the test of Hπ : x ∼mπ versus H1 : x ∼f(x|θ1),
where m is the marginal distribution under H0
mπ(x) =

Θ0
f(x|θ)π(θ) dθ.
Since both hypotheses (Hπ and H1) are point hypotheses, the Neyman–
Pearson lemma ensures the existence of a UMP test ϕπ, at signiﬁcance level
α, with power βπ = Pθ1(ϕπ(x) = 0). This test is of the form
ϕπ(x) =
 1
if mπ(x) > kf(x|θ1),
0
otherwise.
Deﬁnition 5.3.7
A least favorable distribution is any prior distribution
π which maximizes the power βπ.
This deﬁnition is used in the following result (Lehmann (1986, p. 105)).
Theorem 5.3.8 Consider H0 :
θ ∈Θ0 to be tested against a point al-
ternative H1 :
θ = θ1. If the UMP test ϕπ at level α for Hπ versus H1
satisﬁes
sup
θ∈Θ0
IEθ[L(θ, ϕπ)] ≤α,
then
(i) ϕπ is UMP at level α;
(ii) if ϕπ is the unique α-level test of Hπ versus H1, ϕπ is the unique UMP
test at level α to test H0 versus H1; and
(iii) π is a least favorable distribution.
The constraint in the above theorem may seem unnecessary, but notice
that ϕπ is deﬁned by

{mπ(x)>kf(x|θ1)}
mπ(x) dx = α.
8 The remainder of Section 5.3.2 is not used later. The connection stressed here is of
lesser importance than the corresponding relation obtained in minimaxity theory (see
Section 2.4.3). Moreover, it only applies to speciﬁc cases and does not validate any
further the classical answers, which still cannot be obtained as limits of Bayesian
procedures (see Section 5.4).

5.3
Comparisons with the classical approach
247
This relation does not guarantee that IEθ[L(θ, ϕπ)] ≤α for every θ ∈Θ0.
5.3.3 Criticisms
Theorem 5.3.8 exhibits a connection between Bayesian and UMP tests in
the same vein that least favorable distributions lead to minimax estimators
in point estimation problems with a value (see Section 2.4), although the
Bayes procedure corresponds to a modiﬁed testing problem involving π.
We do not pursue this connection any further because, like other authors,
we oppose the Neyman–Pearson approach as a whole. Indeed, in addition
to the randomization problems mentioned above, a major drawback of this
perspective is to restrict the decision space to the couple {0, 1}, thus to
force a categorical decision. It seems to us that a more adaptive answer is
preferable. Moreover, UMP (and UMPU) tests, when they exist, depend
on an evaluation measure (the signiﬁcance level α) not reconsidered after
the observation. For instance, in Example 5.3.6, the level being ﬁxed at
0.05, the classical answer is identical for x = 1.96 and x = 100. From a
purely decision-theoretic point of view, it also seems paradoxical to force the
inferential procedures into a restricted framework, since it can (and does)
lead to suboptimal procedures. In particular, the notion of unbiasedness,
which has been successfully removed from point estimation tools by the
Stein eﬀect (Note 2.8.2), should also disappear from the testing machinery.
A more fundamental criticism of the Neyman–Pearson approach (and,
basically, of every frequentist approach) is that it bases rejection of H0 on
unlikely events that did not occur, to quote Jeﬀreys (1961). In fact, a UMP
rejection region is of the form
{T (X) ≥T (x)}
if the distribution has monotone likelihood ratio in T , since, under the null
hypothesis,
P(T (X) ≥T (x)) < α.
(5.3.2)
However, the event which actually occurs is {T (X) = T (x)}. There is there-
fore a loss of information in the (classical) decision process which is usu-
ally biased against the null hypothesis. Indeed, the region {T (X) ≥T (x)}
is relatively more unlikely than a neighborhood of T (x), thus explaining
the more optimistic values of the Bayesian answers (see Section 5.3.5).
Of course, the only coherent approach which allows for conditioning on
{T (X) = T (x)}, that is, on the observation, is the Bayesian approach. On
the contrary, to choose a procedure on the basis of (5.3.2) involves the whole
distribution of x, and thus potentially contradicts the Likelihood Principle,
as shown by Examples 1.3.4 and 1.3.6. In fact, the Stopping Rule Principle
cannot allow for a frequency-based theory of tests because the distribution
of the sample size should not be relevant for the selection of a testing proce-
dure. There is indeed this apparent paradox with the Likelihood Principle
that a procedure based on a likelihood ratio is acceptable provided it does

248
Tests and Conﬁdence Regions
5
not involve the distribution of this ratio.
Example 5.3.9
The chi-squared test is a simple (if not always justiﬁed)
procedure to test the goodness-of-ﬁt of a sample to a distribution (or to
a family of distributions). If the sample of size n is divided into k classes,
with theoretical sizes Ni = npi and observed sizes ni, it follows from the
Central Limit Theorem that
D2 =
k

i=1
(ni −Ni)2
Ni
is approximately distributed as a χ2
ℓrandom variable, the degrees of free-
dom ℓdepending on the problem. (It is usually k −1 minus the number of
estimated parameters.) As pointed out by Jeﬀreys (1961), the classical ap-
proach rejects the null hypothesis (of goodness-of-ﬁt to the proposed family
of distributions) if D2 is too large, for instance, if
P(z > D2) < 0.05
for z ∼χ2
ℓ. However, there is no reason to accept the null hypothesis (that
is, that D2 is approximately distributed as χ2
ℓ) if
P(z < D2) ≤0.05,
since such values of D2 are not more compatible with the distribution than
when P(z > D2) ≤0.05. From this point of view, it would also be justiﬁed
to reject the validity of the null hypothesis, but the classical approach fails
to do so.
∥
Example 5.3.10
A well known Bayesian criticism of Neyman–Pearson
theory is the following opposition exhibited by Lindley (1957, 1961). Con-
sider ¯xn ∼N(0, 1/n) the average of a normal sample and θ ∼N(0, 1). To
test H0 : θ = 0 versus H1 : θ ̸= 0, the corresponding UMPU tests only
depend on zn = |xn|√n. Consider zn = 1.97. At the signiﬁcance level 5%,
the test procedure rejects H0 for every n. On the contrary, the Bayesian
posterior probability of H0 is (see Example 5.2.4)
π(θ = 0|zn) =

1 + 1 −ϱ0
ϱ0
1
√n + 1 exp{z2
nn/2(n + 1)}
−1
,
and thus goes to 1 as n goes to inﬁnity. Actually, this result holds for
most prior distributions, owing to the asymptotic normality of the posterior
distributions (see Hartigan (1983)). This paradox can be related to Kepler’s
problem (see Jeﬀreys (1961) or Berger (1985a)), which is that, in astronomy,
a null hypothesis—for instance, the elliptical nature of planet trajectories—
is always rejected from a frequentist point of view for a sample size large
enough, i.e., when enough observations have accumulated.
∥
Another major diﬃculty with the Neyman–Pearson approach is that the
selection of the level α should be equivalent to the selection of the weights

5.3
Comparisons with the classical approach
249
a0 and a1 in the loss function, and thus should be based on utility consid-
erations. Instead, the practice is to completely bypass this selection and,
following a suggestion by Fisher (1956), it became a formal rule to adapt
“classical” α-levels of 5% or 1%, no matter what the problem, the sam-
ple size, or the type II error were. Since the Neyman–Pearson approach is
quite predominant nowadays, this dogmatic attitude created a publication
bias, since results of experiments which are not “signiﬁcant at level 5%” are
most often rejected by editors or even censored by the authors themselves
in many ﬁelds, including biology, medicine, and the social sciences.
5.3.4 The p-values
Frequentists (and practitioners) tried to compensate for the drawbacks of
the Neyman–Pearson approach by removing the signiﬁcance level α and
proposing an answer taking values in [0, 1], and more importantly depend-
ing on the observations on a more adaptive way than comparing T (x) with
a given threshold separating acceptance and rejection. The following notion
was ﬁrst introduced by Fisher (1956).
Deﬁnition 5.3.11
The p-value associated with a test is the smallest sig-
niﬁcance level α for which the null hypothesis is rejected.
A general deﬁnition for point-null hypotheses (see Thompson (1989))
considers that a p-value is any statistic with a uniform distribution under
the null hypothesis, but it leads to the diﬃcult problem of selecting one
of these statistics, even though the same can be said about the test intro-
duced in the above deﬁnition. Actually, if a test with rejection region Rα is
available for every signiﬁcance level α and if these regions are nested (that
is, Rα ⊂Rβ if β > α), the procedure
p(x) = inf{α; x ∈Rα}
is uniformly distributed if IEθ0[IIRα(x)] = α (see Goutis et al. (1996)). In the
event of several contending tests, we suggest using the distribution of the
likelihood ratio under the null hypothesis, if it is a point-null hypothesis.
Example 5.3.12 (Example 5.3.6 continued) Since the critical region
(i.e., the rejection region for H0) of the UMPU test is {|x| > k}, an usual
p-value is
p(x)
=
inf{α; |x| > kα}
=
P X(|X| > |x|),
X ∼N(0, 1)
=
1 −Φ(|x|) + Φ(|x|) = 2[1 −Φ(|x|)].
Therefore, if x = 1.68, p(x) = 0.10 and, if x = 1.96, p(x) = 0.05.
∥

250
Tests and Conﬁdence Regions
5
Example 5.3.13 Consider x ∼B(n, p), when the hypothesis to be tested
is H0 :
p = 1/2 and H1 :
p ̸= 1/2. The p-value associated with the
likelihood ratio
f(x|1/2)
supp f(x|p) =
(1/2)n
	 x
n

x 	
1 −x
n

n−x ∝x−x(n −x)−(n−x)
is the function
˜p(x) = P1/2

XX(n −X)(n−X) ≤xx(n −x)(n−x)
,
where X ∼B(n, 1/2).
∥
P-values are thus adaptive procedures that can be acceptable from a
frequentist point of view and that, furthermore, meet the requirements of
Kiefer (1977) and Robinson (1979) for a conditional frequentist approach.
However, they are still exposed to criticisms because
(i) p-values also evaluate the wrong quantity, namely, the probability of
overpassing the observed value of the test statistic. They therefore
contradict the Likelihood Principle by involving the whole distribution
of the observation.
(ii) Even if derived from optimal test procedures, p-values have no intrinsic
optimality because they are not evaluated under a loss function. In
fact, as shown in Section 5.4, they may even be suboptimal.
(iii) The new decision space, D = [0, 1], lacks a decision-theoretic founda-
tion and thus the use of p-values is not made explicit. In particular,
the p-values are often perceived as providing a frequentist approxima-
tion to P(θ ∈Θ0|x), even though this expression is meaningless in a
non-Bayesian setting.
(iv) From a classical perspective, p-values do not summarize all the infor-
mation about the testing problem, since they should be compared with
type–two errors, which are usually omitted from the analysis. Berger
and Wolpert (1988) illustrate the danger of using only p-values by the
following example. If x ∼N(θ, 1/2), to test θ = −1 versus θ = 1
when x = 0 leads to an (UMP) p-value of 0.072, seemingly indicating
a strong rejection of the null hypothesis, although the corresponding
p-value for the test reversing H0 and H1 takes exactly the same value.
In fact, while a rejection of H0 should not always imply acceptance of
H1, practitioners often consider p-values as the testing procedure and
they assume that it encompasses all the information about the testing
problem, thus ending with this implication.
5.3.5 Least favorable Bayesian answers
The problem of evaluating p-values under an adapted loss is considered
in Section 5.4. We conclude this section with a comparison of p-values

5.3
Comparisons with the classical approach
251
with their Bayesian counterparts, the posterior probabilities. To consider
the lower posterior probability on a class of prior distributions provides a
Bayesian least favorable answer with respect to the null hypothesis. This
lower bound cannot be considered as a noninformative procedure, since it
enhances the prior most opposed to the null hypothesis, and is both biased
against H0 and dependent on the observation. It should be interpreted as an
indicator of the range of the posterior probabilities, the most favorable an-
swer being 1. An extensive literature is now available on this approach and
we refer to Berger and Sellke (1987), Berger and Delampady (1987) and
Berger and Mortera (1991) for additional references. Note 5.7.4 presents
a diﬀerent perspective of Berger, Boukai and Wang (1997) that reconciles
frequentist and Bayesian testing by modifying the decision-theoretic frame-
work.
Berger and Sellke (1987) and Berger and Delampady (1987) consider
the case of a point-null hypothesis, H0 : θ = θ0, against the alternative
hypothesis H1 :
θ ̸= θ0. For a family G of prior distributions on the
alternative hypothesis, the evaluation measures of the veracity of H0 are
given by the lower bounds
B(x, G)
=
inf
g∈G
f(x|θ0)

Θ f(x|θ)g(θ) dθ ,
P(x, G)
=
inf
g∈G
f(x|θ0)
f(x|θ0) +

Θ f(x|θ)g(θ) dθ
on the Bayes factors and posterior probabilities (for ϱ0 = 1/2, considered to
give equal weights to both hypotheses). These bounds can also be written as
B(x, G) =
f(x|θ0)
supg∈G

Θ f(x|θ)g(θ)dθ ,
P(x, G) =

1 +
1
B(x, G)
−1
.
They obviously vary, depending on the class G considered. In the more
general case, i.e., when G is GA, the set of all prior distributions, the
following result is straightforward.
Lemma 5.3.14 If there exists a maximum likelihood estimator of θ, ˆθ(x),
the lower bounds on the Bayes factors and posterior probabilities of H0 are,
respectively,
B(x, GA) =
f(x|θ0)
f(x|ˆθ(x))
,
P(x, GA) =

1 + f(x|ˆθ(x))
f(x|θ0)
−1
.
A consequence of Lemma 5.3.14 is that the Bayesian answer will never
strongly favor the null hypothesis, since
B(x, GA) ≤1,
P(x, GA) ≤1
2.
This behavior is not particularly surprising because the lower bounds corre-
spond to the worst possible choice of g with respect to H0. A more surprising

252
Tests and Conﬁdence Regions
5
Table 5.3.1. Comparison between p-values and Bayesian answers in the normal
case. (Source: Berger and Sellke (1987).)
p-value
0.10
0.05
0.01
0.001
P
0.205
0.128
0.035
0.004
B
0.256
0.146
0.036
0.004
phenomenon is that the decrease of these bounds when |x| increases is much
slower than for the p-values, as shown by the following example.
Example 5.3.15 (Example 5.3.6 continued) In the normal case, the
lower bounds associated with H0 : θ0 = 0 are
B(x, GA) = e−x2/2
and
P(x, GA) =

1 + ex2/2−1
,
leading to Table 5.3.5, which compares the p-values with the Bayesian least
favorable answers.
∥
Therefore, the diﬀerence with the frequentist answers is quite important.
P-values are smaller for the signiﬁcance levels of interest, thus reject the
null hypothesis H0 “too often.” Of course, for smaller values of x, the p-
values are larger than the lower bounds, but what matters is that, in the
range of values of x where the decision is the most diﬃcult to take, i.e.,
for signiﬁcance levels between 0.01 and 0.1, there is such a discrepancy
between the Bayesian and frequentist answers.
Results such as those above are quite surprising because classical proce-
dures usually belong to the range of Bayesian answers. Moreover, the class
GA is rather unreasonable, including point masses leading to the lower
bound. The only justiﬁcation for this class of priors relates to the minimax
principle and the corresponding least favorable distribution. The above ex-
ample shows that p-values are not minimax in this sense. Obviously, the
discrepancy is more important for smaller classes of distributions. For in-
stance, if G is GS, the set of distributions which are symmetric around θ0,
the equivalent of Lemma 5.3.14 is:
Lemma 5.3.16
The smallest Bayes factor when g ∈GS is
B(x, GS) =
f(x|θ0)
supξ
1
2[f(x|θ0 −ξ) + f(x|θ0 + ξ)],
which leads to the corresponding lower bound on posterior probabilities.
This result is derived from the fact that every symmetric distribution
is a mixture of distributions with a two-point support of the form {θ0 −
ξ, θ0+ξ}. For multidimensional extensions, the supremum is to be taken on
uniform distributions on spheres centered at θ0 (see Berger and Delampady
(1987)). Discrete settings call for some reﬁnements, if only to deﬁne the

5.3
Comparisons with the classical approach
253
Table 5.3.2. Comparison between p-values and Bayesian answers in the binomial
case. (Source: Berger and Delampady (1987).)
p-value
0.0093
0.0507
0.1011
P
0.0794
0.2210
0.2969
notion of a symmetric distribution. For instance, in the binomial case, the
corresponding class is GS, made of the distributions which are symmetric in
p −p0
1
p(1 −p)
.
Example 5.3.17 (Example 5.3.13 continued)
For H0 :
p = 1/2,
Table 5.3.5 provides p-values and Bayesian lower bounds associated with
GS (p0 = 1/2).
∥
Notice that in this case the p-values are not the standard levels because
of the discrete nature of the binomial distribution.
Another interesting class of priors is that of unimodal distributions sym-
metric around θ0, GSU. These distributions can be written as mixtures of
uniform symmetric distributions in dimension 1 (Berger and Sellke (1987)).
Therefore, the computation of the lower bounds is still tractable. It is nec-
essary to use such classes in multidimensional settings as the lower bounds
associated with more general classes like GA are close to 0 for most values
of the observation.
Example 5.3.18 (Example 5.3.6 continued)
In the normal case, if
|x| ≤1, B(x, GSU) = 1 and P(x, GSU) = 1/2. However, if |x| > 1, deﬁning
g(θ) = (1/2K)II{|θ| < K}, we get

f(x|θ)g(θ) dθ =
1
2K [Φ(K −x) −Φ(−K −x)]
and the lower bound is associated with K maximizing this expression. Table
5.3.5 gives the values of B and P corresponding to p-values of 0.1 and 0.01,
exhibiting a signiﬁcant discrepancy with the frequentist answer.
∥
A ﬁrst consequence of these comparisons is that, from a Bayesian view-
point, p-values are not a valid tool for conducting testing experiments on
null hypotheses. Contrary to regular point estimation settings such as those
developed in Chapter 4, frequentist answers do not seem to be expressible
as limits of Bayesian answers, and we give in Section 5.4 a formal proof
of this fact. Since p-values are strictly smaller than Bayesian answers (for
levels that really matter in a testing decision-theoretic process), the null
hypothesis H0 is rejected more often under the frequentist approach, while
the Bayesian approach shows that the ratio of the posterior likelihoods of

254
Tests and Conﬁdence Regions
5
Table 5.3.3. Bayesian answers for p-values of 0.01 (above) and 0.1 (below) in the
normal case. (Source: Berger and Delampady (1987).)
dim.
1
3
5
P
0.109
0.083
0.076
0.392
0.350
0.339
B
0.123
0.090
0.082
0.644
0.540
0.531
H0 and H1 is quite moderate for the usual signiﬁcance levels (0.05 or 0.01).
This important discrepancy between the two approaches deﬁnitely calls for
Bayesian modeling, since this approach includes more naturally the notion
of the probability of a hypothesis. It also shows that the argument of fre-
quentist validity, i.e., the long-run justiﬁcation provided by a signiﬁcance
level of 5% or of 1%, is rather illusory and that the division introduced by
the Neyman–Pearson theory in the treatment of H0 and H1 (between type
I and type II errors) leads to a bias in favor of the alternative hypothesis
for larger values of x or T (x).
5.3.6 The one-sided case
One-sided hypotheses (e.g., H0 : θ ≤θ0 versus H1 : θ > θ0) do not ex-
hibit such contrasts between frequentist and Bayesian solutions. Indeed, as
shown in Example 5.2.9, the p-value can be written as a generalized Bayes
estimator and, therefore, as a limit of Bayesian answers (since renormali-
zing does not matter). Thus, it is impossible to exhibit a dichotomy between
both approaches as in the two-sided case. Casella and Berger (1987) con-
sider this setting and generalize the above “reconciliation” phenomenon.
Theorem 5.3.19
Let x ∼f(x −θ), with f symmetrical around 0. The
null hypothesis to be tested is H0 : θ ≤0. If f is a monotone likelihood ratio
distribution, the p-value p(x) is equal to the lower bound on the posterior
probabilities, P(x, GSU), when this bound is computed for the class GSU of
unimodal symmetric prior distributions and when x > 0.
Proof.
In this case, the p-value is
p(x) = Pθ=0(X > x) =
 +∞
x
f(t) dt
and
B(x, GSU)
=
inf
π∈GSU P π(θ ≤0|x)
=
inf
π∈GSU
 0
−∞f(x −θ)π(θ) dθ
 +∞
−∞f(x −θ)π(θ) dθ

5.3
Comparisons with the classical approach
255
Table 5.3.4. Comparison between p-values and Bayesian posterior probabilities in
the case of a Cauchy distribution. (Source: Casella and Berger (1987).)
p-value
0.437
0.102
0.063
0.013
0.004
P
0.429
0.077
0.044
0.007
0.002
=
inf
K
 0
−K f(x −θ) dθ
 K
−K f(x −θ) dθ
,
(5.3.3)
owing to the representation of symmetric unimodal prior distributions as
mixtures of uniform distributions on [−K, K]. The monotone likelihood
ratio property then implies that (5.3.3) is attained for K = +∞.
22
A consequence of Theorem 5.4.1 is that the lower bound of the Bayesian
answers over all prior distributions is smaller than the p-value.
Example 5.3.20
Consider X ∼C(θ, 1), the Cauchy distribution, when
the hypothesis to be tested is H0 : θ ≤0 versus H1 : θ > 0. If the prior
distribution of θ is assumed to be in the class of distributions symmetric
around 0, the lower bounds on the Bayesian answers and the corresponding
p-values are given in Table 5.3.6. The diﬀerences in the numerical values
are not as striking as in the previous examples.
∥
The distinction between one-sided and two-sided cases calls for the fol-
lowing comments:
(i) As mentioned several times above, Bayesian modeling in a two-sided
setting is usually quite delicate, especially for point-null hypotheses,
since it implies a modiﬁcation of the prior distribution imposed by the
inferential problem. This does not contravene the Bayesian principles
if we consider that this modiﬁcation results from additional (if vague)
information; but how to use this information remains unclear. An illus-
tration of this diﬃculty is found in the case of noninformative distribu-
tions, where several (and not entirely compatible) Bayesian approaches
are competing, as detailed in Section 5.2.6.
(ii) That the p-value is close to the lower bound in the one-sided case
illustrates the conservative (or minimax) behavior of the procedure.
As it may be written as a generalized Bayes answer, this induces us
to think that the p-value could also be expressed as a noninformative
answer in the two-sided cases. Obviously, this does not necessarily
imply that this answer should be used, since an eﬀective use of the
information contained in the testing problem itself is generally possible.
(iii) P-values are derived from UMP or UMPU tests by an ad-hoc empirical
construction. The comparisons in Berger and Sellke (1987) and Casella
and Berger (1987) show that they diﬀer (or do not diﬀer) from their

256
Tests and Conﬁdence Regions
5
Bayesian counterparts. While these studies point out the existence
of a theoretical problem, they are not, from a frequentist viewpoint,
suﬃcient to reject the use of p-values. It is thus necessary to use a
decision-theoretic perspective adapted to the evaluation of p-values.
The next section deals with this comparison. It also provides theoret-
ically grounded explanations for the two-sided/one-sided dichotomy
exhibited above.
(iv) A diﬀerent perspective, which allows for a larger decision space by in-
cluding a no-decision possibility, brings both frequentist and Bayesian
answers much closer, at both the conceptual as well and the numerical
levels. It is detailed in Note 5.7.4.
5.4 A second decision-theoretic approach
As just stressed9, p-values have no intrinsic justiﬁcation, since they derive
their claimed “optimality” from the optimality of the test procedures they
are built on. In a sense, the same comment holds for the posterior probabil-
ities since, although they are intuitively justiﬁable, they are not validated
by a decision process. In this section, we construct an alternative to the
Neyman–Pearson approach in order to justify the posterior probabilities
and evaluate the p-values.
As shown in Section 5.2, the testing problem formalized by Neyman and
Pearson can be expressed as estimating the indicator function IIΘ0(θ) under
the 0 −1 loss or, equivalently, the absolute error loss
L1(θ, ϕ) = |ϕ −IIΘ0(θ)| .
(5.4.1)
Indeed, if the estimators ϕ are only taking the values 0 and 1, there are
many ways to write the 0 −1 loss, (5.4.1) being one of them. But, as
indicated above, the Neyman–Pearson theory is predominantly a predata
theory that does not provide a postdata (or more adaptive) solution. We
then turn to a less restrictive theory, according to which estimators take
values in D = [0, 1] and can be considered as indicators of the degree of
evidence in favor of H0.
Parallel to Schaafsma et al. (1989), Hwang et al. (1992) examine this
approach to testing, in which the estimators of IIΘ0(θ) belong to [0, 1].
When the restriction to {0, 1} is dropped, the choice of the loss gets more
important. For instance, (5.4.1) is too similar to the 0 −1 loss function as
it provides the same Bayes procedures
ϕπ(x) =
 1
if P π(θ ∈Θ0|x) > P π(θ ̸∈Θ0|x),
0
otherwise.
In the opposite, strictly convex losses, such as the quadratic loss
L2(θ, ϕ) = (ϕ −IIΘ0(θ))2 ,
(5.4.2)
lead to more adaptive estimators.
9 This section is at a more advanced level and can be skipped on a ﬁrst reading.

5.4
A second decision-theoretic approach
257
Proposition 5.4.1 Under the loss (5.4.2), the Bayes estimator associated
with π is the posterior probability
ϕπ(x) = P π(θ ∈Θ0|x).
Indeed, the posterior expectation of IIΘ0(θ) is nothing but the poste-
rior probability of Θ0. The quadratic loss (5.4.2) thus provides a decision-
theoretic foundation to the use of posterior probabilities as Bayesian an-
swers. Such losses are said to be proper (see Lindley (1985) and Schervish
(1989); Exercise 2.15 characterizes proper losses). There exist other proper
losses in addition of the quadratic loss, but Hwang and Pemantle (1995)
have shown that it is suﬃcient to consider the quadratic loss in terms of
admissibility and complete classes (see Chapter 8).
We consider in this section the special case of natural exponential fami-
lies,
f(x|θ) = eθx−ψ(θ),
θ ∈Θ ⊂IR,
and we introduce the following deﬁnition, due to Farrell (1968a), which
allows us to to evaluate procedures on an interval when they are constant
outside this interval.
Deﬁnition 5.4.2
For a one-sided test, i.e., for hypotheses of the form
H0 :
θ ≤θ0 versus H1 :
θ > θ0, an interval [t1, t2] is said to be a
truncation set for the estimator ϕ if ϕ(t) = 1 when t < t1 and ϕ(t) = 0
when t > t2. For a two-sided test of H0 : θ ∈[θ1, θ2], the interval [t1, t2] is
said to be a truncation set for the estimator ϕ if ϕ(t) = 0 when t ̸∈[t1, t2].
The following results have been obtained in Hwang et al. (1992), based
on a result of Brown (1986), which shows that every admissible estimator
is a pointwise limit of Bayes estimators for a sequence of measures with
ﬁnite supports (see Section 8.3.4).
Theorem 5.4.3
For the two-sided problem
H0 :
θ ∈[θ1, θ2]
versus
H1 :
θ ̸∈[θ1, θ2],
an estimator ϕ with truncation set [t1, t2] is admissible if there exist a
probability measure π0 on [θ1, θ2] and a σ-ﬁnite measure π1 on [θ1, θ2]c
such that
ϕ(x) =

f(x|θ)π0(θ) dθ

f(x|θ)π0(θ)dθ +

f(x|θ)π1(θ) dθ ,
(5.4.3)
for x ∈[t1, t2]. Conversely, if ϕ is admissible, there exist [t1, t2], π0, and
π1 such that (5.4.3) is satisﬁed.
In the one-sided case, we can only propose an admissibility necessary
condition, but it implies that the generalized Bayes estimators form a com-
plete class.

258
Tests and Conﬁdence Regions
5
Theorem 5.4.4
For the one-sided problem
H0 : θ ≤θ0
versus
H1 : θ > θ0,
(5.4.4)
if ϕ is admissible, there exists an increasing procedure ϕ′ such that ϕ′ is
(risk) equivalent to ϕ. If ϕ is an increasing admissible procedure and [t1, t2]
is a truncation set such that 0 < ϕ(x) < 1 on [t1, t2], there exist two σ-ﬁnite
measures on (−∞, θ0] and [θ0, +∞), π0, and π1, such that
1 =

et0θ−ψ(θ)(π0(θ) + π1(θ)) dθ
for t1 < t0 < t2 and ϕ is given by (5.4.3) on [t1, t2].
These two complete class theorems show that it is suﬃcient to consider
the generalized Bayes estimators to obtain admissible estimators under
quadratic loss. Theorem 5.4.4 shows in addition that the monotone esti-
mators form an essentially complete class. These results can be used to
evaluate p-values. Notice again that the Bayes estimators are underlying
(classical) optimal estimators. (Chapter 8 exposes more thoroughly the
Bayesian foundations of admissibility.)
Recall also that Casella and Berger (1987) showed that p-values were
within the variation range of Bayesian posterior probabilities in one-sided
settings. It is therefore natural to examine the admissibility of p-values. The
examples below show that they are admissible for most one-sided tests.
Example 5.4.5
Consider again x ∼N(θ, 1) and H0 of the form (5.4.4).
We showed in Example 5.2.9 that
p(x) = Pθ0(X > x) = 1 −Φ(x −θ0)
is a generalized Bayes estimator with respect to the Lebesgue measure.
Moreover, the risk of the p-value is
r(π, p)
=
 +∞
−∞
R(p, θ) dθ
=
 +∞
−∞
 +∞
−∞
(p(x) −IIΘ0(θ))2f(x|θ) dx dθ
=
 θ0
−∞
 +∞
−∞
(1 −Φ(x −θ0))2f(x|θ) dx dθ
+
 +∞
θ0
 +∞
−∞
Φ(x −θ0)2f(x|θ) dx dθ
=
2
 +∞
−∞
(1 −Φ(x −θ0))2Φ(x −θ0) dx
by the Fubini Theorem. This integral is ﬁnite. Therefore, r(π) < +∞and
p is admissible under (5.4.2) (see Section 2.4).
∥

5.5
Conﬁdence regions
259
Example 5.4.6
Consider x ∼B(n, θ). The p-value for the test of (5.4.3)
is then
p(x) = Pθ0(X ≥x) =
n

k=x
n
k

θk
0(1 −θ0)n−x,
which is also a generalized Bayes estimator under the prior distribution
π(θ) = 1/θ. It is again possible to show that p has a ﬁnite Bayes risk
and is thus admissible. A similar result can be established for a Poisson
distribution, P(θ) (see Hwang et al. (1992)).
∥
In two-sided settings, on the contrary, p-values are not admissible, as
suggested by the comparisons of Section 5.3.5.
Theorem 5.4.7
For the test of (5.4.3), when the sampling distribution is
continuous with respect to the Lebesgue measure, the p-value is inadmissible
for the loss (5.4.2).
Proof.
The result relies on the fact that the p-value p is equal to the value
1 with positive probability (see Hwang et al. (1992, §4.1.2)). In fact, if p is
admissible, it can be written under the form (5.4.3). Since it is positive,

f(x|θ)π1(θ) dθ < +∞.
Therefore, by continuity, the equality (5.4.3) holds everywhere and p(x0) =
1 implies π = π0, i.e., p(x) = 1 for every x, which cannot be true.
22
This result agrees with the observations of Berger and Sellke (1987), who
showed that p-values do not belong to the range of Bayesian answers. It thus
justiﬁes the rejection of p-values for two-sided hypotheses. Furthermore,
Hwang and Pemantle (1995) show that the inadmissibility of p-values can
be extended to most bounded proper losses. As a concluding remark, let
us point out that it is now necessary to construct estimators dominating
the p-values. In the normal case, Hwang et al. (1992) show that it cannot
be done using a proper Bayes estimator, but Hwang and Pemantle (1995)
give numerical arguments in favor of an explicit dominating estimator.
5.5 Conﬁdence regions
Apart from providing a decision-maker with approximations of the “true”
value of a parameter θ, namely, point estimators, and with answers to
questions about the inclusion of θ in a speciﬁc domain, that is, testing
procedures, it is sometimes necessary to give in addition conﬁdence regions
on θ, i.e., subsets Cx of the parameter space Θ where θ should be with high
probability (in the frequentist or in the Bayesian sense). This notion also
extends to non-bijective transforms of θ. It is also of considerable interest
in forecasting and prediction settings.

260
Tests and Conﬁdence Regions
5
Example 5.5.1 Consider the IBM stock prices of Example 4.5.3, repre-
sented in Figure 4.5.1. If the series (xt) has been observed till time T , the
value at time T + 1, xT +1, is obviously of considerable interest and it is
important to present the investor not only with the most likely value of
xT +1 given the past series, but also with a range of possible values of xT +1
so that she can take a decision against the corresponding possible gains. ∥
Once again, the Bayesian formulation that θ has a given probability to
belong to a ﬁxed region Cx is more appealing than the frequentist inter-
pretation that a random region Cx has a given probability to contain the
unknown parameter θ.
5.5.1 Credible intervals
As in the testing setting, the Bayesian paradigm proposes a notion of con-
ﬁdence regions that is more natural that its frequentist counterpart since,
again, the notation P(θ ∈Cx) is meaningful even conditional upon x.
Deﬁnition 5.5.2
For a prior distribution π, a set Cx is said to be an
α-credible set if
P π(θ ∈Cx|x) ≥1 −α.
This region is called an HPD α-credible region (for highest posterior den-
sity) if it can be written under the form10
{θ; π(θ|x) > kα} ⊂Cπ
x ⊂{θ; π(θ|x) ≥kα},
where kα is the largest bound such that
P π(θ ∈Cα
x |x) ≥1 −α.
To consider only HPD regions is motivated by the fact that they minimize
the volume among α-credible regions and, therefore, can be envisioned as
optimal solutions in a decision setting.
Example 5.5.3 If θ ∼N(0, τ2), the posterior distribution of θ is N(μ(x),
ω−2) with ω2 = τ−2 + σ−2 and μ(x) = τ2x/(τ 2 + σ2). Then
Cπ
α =
+
μ(x) −kαω−1, μ(x) + kαω−1,
,
where kα is the α/2-quantile of N(0, 1). In particular, if τ goes to +∞,
π(θ) converges to the Lebesgue measure on IR and gives
Cα = [x −kασ, x + kασ] ,
i.e., the usual conﬁdence interval, as a generalized Bayes estimator.
∥
10 This formulation allows for coverage of the special case when {θ; π(θ|x) = kα} is not
empty.

5.5
Conﬁdence regions
261
Table 5.5.1. Conﬁdence intervals for the binomial distribution.
x
0
1
2
α = 5%
[0.000, 0.38]
[0.022, 0.621]
[0.094, 0.791]
α = 10%
[0.000, 0.308]
[0.036, 0.523]
[0.128, 0.74]
Example 5.5.4
Consider x ∼B(n, p) and the noninformative distribu-
tion p ∼Be(1/2, 1/2). Then p|x ∼Be(x + 1/2, n −x + 1/2) and conﬁdence
intervals on p can be derived from the c.d.f. of the beta distribution. Table
5.5.1 gives these intervals for n = 5 and α = 5%, 10%.
∥
Notice the signiﬁcant advantage of using a Bayesian approach in this
setting of discrete distributions, as compared with a classical approach. In
fact, the usual conﬁdence intervals involve a randomization step to attain
nominal conﬁdence levels (see Blyth and Hutchinson (1961) for an illustra-
tion in the binomial case). Prior modeling avoids this addition of random
noise and, on the contrary, takes advantage of the available prior informa-
tion. Notice also that improper priors can be used in this setting, and do
not encounter the same diﬃculties as when testing point-null hypotheses.
In fact, posterior credible regions can be derived provided the posterior
distribution is deﬁned. Some classical conﬁdence regions can be expressed
as credible regions associated with generalized distributions.
Example 5.5.5 Consider x1, . . . , xn i.i.d. N(θ, σ2). The prior distribution
is the noninformative prior
π(θ, σ2) = 1
σ2 .
We showed in Section 4.4.2 that the marginal posterior distribution for
1/σ2 is a gamma distribution G
	
(n −1)/2, s2/2

with s2 = (xi −¯x)2.
Therefore
s2
σ2 |¯x, s2 ∼χ2
n−1,
and we get the same conﬁdence interval as in the classical approach, but it
is now justiﬁed conditional upon s2.
∥
Example 5.5.6
Consider x ∼B(n, p) and p ∼Be(α, β). In this case,
π(p|x) is the beta distribution Be(α + x, β + n −x). Depending on the
values of α, β, n, and x, the conﬁdence regions are of four types:
(i)
0 ≤p ≤K(x);
(ii)
K(x) ≤p ≤1;
(iii)
K1(x) ≤p ≤K2(x); and
(iv)
0 ≤p ≤K1(x) or K2(x) ≤p ≤1.

262
Tests and Conﬁdence Regions
5
The last region is quite artiﬁcial and rather useless. Notice that it corre-
sponds to the case
α + x < 1
and
β + n −x < 1,
thus implying that α and β must be suﬃciently negative since α+β < 2−n.
Hence, this feature disappears for n large enough, unless α and β depend
on n, which is not desirable from a Bayesian point of view. Notice that the
limiting case α = β = 0, which corresponds to Haldane’s (1931) distribution
π(p) = [p(1 −p)]−1,
already leads to regions of types (i)–(iii), although the posterior distribution
is not deﬁned for all x’s (Example 1.5.3).
∥
When phenomena such as case (iv) of Example 5.5.6 occur, that is, when
the conﬁdence region is not connected (see also Exercise 5.5), the usual
solution is to replace the HPD α-credible region by an interval with equal
tails, i.e., [C1(x), C2(x)] such that
P π(θ < C1(x)|x) = P π(θ > C2(x)|x) = α/2.
Berger (1985a) remarks that the occurrence of nonconnected HPD regions
also points out a discrepancy between the prior distribution and the obser-
vations and that this phenomenon should question the choice of the prior or
of the sampling distribution. It may also exhibit a non-identiﬁable structure
which is responsible for the multimodality of the posterior distribution.
If, conceptually, the determination of credible sets is rather straight-
forward, the practical derivation of these regions can be quite involved,
especially when the dimension of Θ is large or when the posterior distribu-
tion is not available in a closed form. A ﬁrst solution is to use numerical
methods similar to those developed in Chapter 6, the problem being to
assess the impending error (which can be much larger than the point esti-
mation approximation errors). (Notice that equal tail credible regions are
usually easier to approximate than HPD regions; see Eberly and Casella
(1999).) A second solution, used in Berger (1980a, 1985a), is to build up a
normal approximation, i.e., to consider that the posterior distribution of θ
is roughly Np(IEπ(θ|x), Varπ(θ|x)), and to derive from this approximation
the conﬁdence region
Cα =

θ; (θ −IEπ(θ | x))t Varπ(θ|x)−1(θ −IEπ(θ|x)) ≤k2
α

,
where k2
α is the α-quantile of χ2
p. This approximation is only justiﬁed for a
large sample size (see Hartigan (1983)), but it still provides fast and rather
eﬃcient conﬁdence regions.

5.5
Conﬁdence regions
263
5.5.2 Classical conﬁdence intervals
In the Neyman–Pearson theory, conﬁdence regions can be deduced from
UMPU tests by a duality argument: If
Cθ = {x; ϕθ(x) = 1}
is the acceptance region for the null hypothesis H0 : θ = θ0, ϕθ0 being a
UMPU test at level α, the corresponding conﬁdence region is
Cx
=
{θ; x ∈Cθ}
=
{θ; ϕθ(x) = 1}
and P(θ ∈Cx) = 1 −α. More generally, a region Cx is said to be a
conﬁdence region at level α (in the frequentist sense) if, for every θ ∈Θ,
P(θ ∈Cx) ≥1 −α.
Example 5.5.7 (Example 5.5.3 continued) If x ∼N(θ, σ2), the 95%
UMPU test is ϕθ(x) = II[0,1.96] (|x −θ|/σ) and the corresponding conﬁdence
region, when σ is known, is
Cx = [x −1.96σ, x + 1.96σ].
∥
Example 5.5.8
Consider x ∼Tp(N, θ, Ip), a t-distribution with N de-
grees of freedom, and density
f(x | θ) ∝

1 + 1
N ∥x −θ ∥2
−(N+p)/2
.
Since 1
p ∥x −θ ∥2∼F(p, N), we can derive a 1 −α% conﬁdence ball
Cx =

θ; ∥x −θ ∥2≤pfα(p, N)

,
where fα(p, N) is the α-quantile of F(p, N).
∥
These conﬁdence regions, although used quite extensively in practice (for
instance, in the case of the linear regression), have been criticized on fre-
quentist, conditional, and Bayesian grounds. First, as seen in the previous
sections, the Neyman–Pearson approach itself is not free of drawbacks, and
the optimality of UMPU tests can be contested. Therefore, conﬁdence re-
gions derived from these tests (called uniformly most accurate regions by
Lehmann (1986)) do not necessarily have a proper behavior. Moreover,
even from a frequentist perspective, the inversion of optimal test proce-
dures into conﬁdence regions does not automatically grant these regions
with a derived optimality, despite the above denomination.
Besides conditional criticisms of conﬁdence regions (see Note 5.7.3), there
also exist frequentist criticisms. Following Stein (1962a) and Lindley (1962),
Brown (1966) and Joshi (1967) have indeed established that these regions
C0
x are not always optimal because there may exist another set C′
x such
that
Pθ(θ ∈C′
x) ≥Pθ(θ ∈C0
x)
and
vol(C′
x) ≤vol(C0
x).

264
Tests and Conﬁdence Regions
5
Therefore, the set C′
x is to be preferred to C0
x since, for a smaller volume,
it has a larger probability of containing the true value of the parameter.
For instance, in the normal case, Joshi (1969) has established that, if x ∼
Np(θ, Ip), the conﬁdence region
C0
x =

θ; ||θ −x||2 ≤cα

is admissible (in the above sense) if and only if p ≤2 (see also Cohen and
Strawderman, 1973). For larger dimensions, it is possible to exhibit more
eﬃcient conﬁdence regions.
This phenomenon pertains to the Stein eﬀect, establishing the inadmis-
sibility of the maximum likelihood estimator for p ≥3 (see Note 2.8.2).
Hwang and Casella (1982) have taken advantage of this analogy to show
that, if
δJS(x) =

1 −
a
||x||2
+
x
is a truncated James–Stein estimator, the recentered conﬁdence region
CJS
x =

θ; ||θ −δJS(x)||2 ≤cα

,
has the same volume as the usual ball C0
x and satisﬁes
Pθ(θ ∈CJS
x ) > Pθ(θ ∈C0
x) = 1 −α
(5.5.1)
for a small enough. Therefore, CJS
x
dominates C0
x in the above sense.
An extensive amount of literature on this subject of recentered conﬁdence
regions has been initiated by Hwang and Casella (1982, 1984), similar to
the growth in point-estimation literature associated with the Stein eﬀect
(see Section 2.8.2). New recentered regions have been proposed in Hwang
and Casella (1984) and Casella and Hwang (1983, 1987). Hwang and Chen
(1986) and Robert and Casella (1990) have extended domination results
to spherically symmetric distributions, although the case of the unknown
variance normal problem is still unsolved (see Hwang and Ullah (1994)).
Shinozaki (1990) has also devised a conﬁdence region with exactly the same
coverage probability, but with a smaller volume, taking advantage of the
inadmissibility of the usual region the opposite way to (5.5.1). Lu and
Berger (1989a), George and Casella (1994), and Robert and Casella (1993)
have also taken advantage of (5.5.1) to propose improved conﬁdence reports
for the usual and usual and recentered sets. For the problem of estimating
a normal variance, similar improvements can be found in Cohen (1972),
Shorrock (1990), and Goutis and Casella (1991).
5.5.3 Decision-theoretic evaluation of conﬁdence sets
As the reader may have noticed, the above construction of conﬁdence re-
gions has been done in a rather oﬀ-handed manner, with limited decision-
theoretic justiﬁcations. The choice of HPD regions is usually related to a

5.5
Conﬁdence regions
265
volume minimization requirement, under the coverage constraint
P(θ ∈Cα|x) ≥1 −α.
Several authors have proposed diﬀering derivations of conﬁdence regions
according to a purely decision-theoretic criterion. They consider loss func-
tions that incorporate at once volume and coverage requirements. (In a
way, the above approach corresponds to a bidimensional loss with compo-
nents vol(C) and 1−IIC(θ).) For instance, a simple version of this decision-
theoretic perspective is to consider the linear combination
L(C, θ) = vol(C) + cIIθ /∈C,
(5.5.2)
leading to the risk
R(C, θ) = IE[vol(Cx)] + cP(θ /∈Cx).
(The constant c can be related to a particular conﬁdence level.) In addition,
Cohen and Sackrowitz (1984) have shown that the above bidimensional loss
can be related to the linear loss (5.5.2) when c is treated as an additional
parameter of the model. Meeden and Vardeman (1985) also propose diﬀer-
ent evaluations of Bayesian conﬁdence regions. They show that admissible
and Bayesian conﬁdence sets are equivalent for some criteria.
An important defect of the loss (5.5.2) has been pointed out by James
Berger (see Casella et al. (1993a,b)). The problem results as a consequence
of the unequal penalization between volume and coverage. In fact, the in-
dicator function varies between 0 and 1 while the volume can increase to
inﬁnity and this asymmetry leads to a bias in favor of small conﬁdence sets.
Example 5.5.9 Consider x1, . . . , xn i.i.d. N(θ, σ2). The classical t-interval
on θ,
Ck(¯x, s) =

¯x −k s
√n, ¯x + k s
√n

,
is an HPD region when
¯x =
n

i=1
xi/n,
s2 =
n

i=1
(xi −¯x)2/(n −1),
and
π(θ, σ2) = 1
σ2 ,
Jeﬀreys noninformative distribution. Indeed, in this case,
√n θ −¯x
s
| ¯x, s ∼Tn−1,
Student’s t-distribution with n −1 degrees of freedom. Under (5.5.2), the
posterior loss is
ϱ(π, Ck(¯x, s)|¯x, s)
=
2k s
√n −cP π(θ ∈Ck(¯x, s)|¯x, s)
=
2k s
√n −cP(|Tn−1| ≤k).
Then, it is easy to see that the HPD region is dominated by the truncated

266
Tests and Conﬁdence Regions
5
region
C′
t(¯x, s) =

Ct(¯x, s)
if s < √nc/(2k),
{¯x}
otherwise.
This domination is counterintuitive: C′
t proposes the single point {¯x} (or
equivalently ∅), seemingly indicating certainty, when the empirical variance
increases, indicating growing uncertainty. A similar phenomenon occurs
when k depends on s, i.e., the size of the credible region decreases to 0 as
s increases (see Casella et al. (1993a,b)).
∥
The above paradox exposes the limitations of the linear loss (5.5.2).
Casella et al. (1993a) propose an alternative class of loss functions which
avoid the paradox. The simplest of these losses are the so-called rational
losses
L(C, θ) =
vol(C)
vol(C) + k + IIθ /∈C
(k > 0),
where both terms are then bounded by one. The Bayes estimators asso-
ciated with these losses are still HPD regions but remain nonempty for
all conjugate priors in the normal case. The parameter k can be obtained
through techniques similar to those developed for regular losses, namely, by
comparing the volume penalizations associated with diﬀerent regions and
approximating the utility function.
We do not pursue any further the decision-theoretic study of Bayesian
conﬁdence regions. Indeed, an important aspect usually overlooked in the
derivation of conﬁdence regions deals with how they will be used, although
this very use is essential in the construction of the loss function. In fact,
the decision-maker’s purpose can be
(1) to consider set estimation as a preliminary step to point estimation
(and, for instance, to derive a prior distribution with support equal to
the estimated conﬁdence region);
(2) to rely on the obtained conﬁdence region to solve a testing problem
(and reject a null hypothesis if the conﬁdence region does not contain
a speciﬁc value);
(3) to derive from the size (volume) of the conﬁdence region an indicator of
the performances of an associated estimator, for instance, the center of
the region. A performance curve for this estimator can then be derived
by relating size and conﬁdence levels.
These three perspectives of conﬁdence region estimation deﬁnitely lead
to diﬀerent loss functions, and it may be illusory to try to build up a global
loss function unifying such contrasted purposes. In fact, separate losses are
preferable because, in accordance with the foundations of Decision Theory,
the decision-maker should select a loss function according to her needs. No-
tice also that the three purposes considered above correspond to inferential
problems already studied previously, and thus that a speciﬁc approach to
conﬁdence regions may be partly useless. Therefore, it seems to us that,

5.6
Exercises
267
at least, a more conditional approach should be used in the construction
of conﬁdence regions (see Note 5.7.3). Following Kiefer (1977), we suggest
associating to a given set Cx a conﬁdence index γ(x), evaluated under the
loss
L(C, γ, θ) = (IIC(θ) −γ)2.
(5.5.3)
The conﬁdence region is thus replaced by a conﬁdence procedure, related
to the conditional perspective of Robinson (1979). From this point of view,
the procedure [Θ, 1] is unfortunately perfect, a drawback suggesting that
an additional evaluation of Cx should be included in the loss function,
as in Rukhin (1988a,b). Similarly, the Bayesian procedure associated with
an HPD region Cα is [Cα, 1 −α], as can be veriﬁed by minimizing the
posterior loss. For an arbitrary region, Cx, the corresponding procedure is
[Cx, γπ(x)], where
γπ(x) = P π(θ ∈Cx|x).
Introducing a global loss function combining volume, coverage, and con-
ﬁdence report as in (5.5.3), the optimal procedures would then be those
that minimize the maximal posterior (or frequentist) error. However, this
direction has not yet been treated in the literature.
5.6 Exercises
Section 5.2.1
5.1 In the setting of Example 5.2.4, study the modiﬁcation of the posterior prob-
ability of H0 when x = 0 and τ/σ goes to +∞. Compare with the noninfor-
mative answer associated with π(θ) = 1.
Section 5.2.2
5.2 Consider x ∼N(θ, 1). The hypothesis to test is H0 :
|θ| ≤c versus H1 :
|θ| > c when π(θ) = 1.
a. Give the graph of the maximal probability of H0 as a function of c.
b. Determine the values of c for which this maximum is 0.95 and the Bayes
factor is 1. Are these values actually appealing?
5.3 A professor at Cornell University has to give an examination on two diﬀer-
ent days. Since students are sitting next to each other, she gives two diﬀerent
examinations alternatively to students in order to reduce cheating. She then
repeats the same technique with a diﬀerent class and the same two exam-
inations the next day. The results are as follows: n1A = 17 students took
examination A the ﬁrst day and n2A = 19 the second day, n1B = 15 took
examination B the ﬁrst day and n2B = 19 the second day. The average grades
(out of 20) are ˆμ1A = 10.3, ˆμ2A = 10.2, ˆμ1B = 7.9, and ˆμ2B = 8.7, with
standard deviations ˆσ1A = 2.67, ˆσ2A = 2.89, ˆσ1B = 2.98, and ˆσ2B = 2.91.
a. Test whether there is a class eﬀect, an examination eﬀect, or a class-and-
examination eﬀect by modeling the results in an analysis of variance set-
ting, namely, by assuming that each student grade x is normally distributed
with mean μ0 + μe + μc and variance σ2
ec (e = A, B, c = 1, 2) where
μA + μB = 0, μA + μB = 0.

268
Tests and Conﬁdence Regions
5
b. A student forgot to give back his copy of examination A the ﬁrst day. Can
you detect a cheating eﬀect on the second day?
Section 5.2.3
5.4 (Pearl (1988)) After communicating a rumor to a neighbor, you hear it again
from the same neighbor a few days after. Build up a model to test whether
your neighbor has also heard this rumor from another person.
5.5
*Consider two independent standard normal observations x and y. The
polar coordinates of (x, y) are (r, θ), with x = r cos θ and y = r sin θ.
a. Given that 2r2 = (x−y)2+(x+y)2 and that x−y and x+y are independent,
show that the distribution of r2 given x = y is G(1/2, 1).
b. Show that r and θ are independent and deduce that the distribution of r2
given θ = π/4, 5π/4 is G(1/2, 1/2).
c. Since {x = y} = {θ = π/4, 5π/4}, explain the apparent paradox, called
Borel paradox, of the two diﬀerent conditional distributions for a same
event. (Hint: Replace the conditioning in a proper perspective of σ-algebras
and compare the σ-algebras spanned by x −y and by θ.)
Section 5.2.4
5.6 When x ∼N(θ, 1) and θ ∼N(0, σ2), compare the Bayesian answers for the
two testing problems
H1
0 :
θ = 0 versus H1
1 : θ ̸= 0,
H2
0 :
|θ| ≤ϵ versus H2
1 : |θ| > ϵ,
when ϵ and σ vary.
5.7 In the setting of Example 5.2.3, if x ∼B(n, p) and H0 : p = 1/2 is to be
tested, study the variation of the Bayesian answers as a function of n for x = 0
and x = n/2 if the prior distribution is the Jeﬀreys distribution.
5.8 * (Berger and Delampady (1987)) Consider x ∼N(θ, 1). The purpose of the
exercise is to compare H0 : |θ −θ0| ≤ϵ with the approximation H∗
0 : θ = θ0.
Denote by g0 and g1 the prior densities on {|θ −θ0| ≤ϵ} and {|θ −θ0| > ϵ}.
Let g be a density on IR such that
g(θ) ∝g1(θ)
if |θ −θ0| > ϵ,
and
λ =

|θ−θ0|≤ϵ
g(θ) dθ,
is small enough. We denote
B =

|θ−θ0|≤ϵ f(x|θ)g0(θ) dθ

|θ−θ0|>ϵ f(x|θ)g1(θ) dθ
and
ˆB = f(x|θ0)
mg(x) =
f(x|θ)

f(x|θ)g(θ) dθ ,
t = (x −θ0) and
γ =
1
2ϵϕ(t)[Φ(t + ϵ) −Φ(t −ϵ)] −1.
Show that, if |t| ≥1, ϵ < |t| −1, and ˆB ≤(1 + γ)−1, then
B = ˆB(1 + ϱ)

5.6
Exercises
269
with
−λ ≤λ( ˆB −1)
1 −λ ˆB
≤ϱ ≤γ + λ(1 + γ)( ˆB −1)
1 −λ ˆB(1 + γ)
≤γ.
Section 5.2.5
5.9 Consider x ∼P(λ). The hypothesis to test is H0 : λ ≤1 versus H1 : λ > 1.
Give the posterior probability of H0 for x = 1 and λ ∼G(α, β).
a. How does this probability get modiﬁed when α and β go to 0? Does this
answer depend on the rates of convergence of α and β to 0?
b. Compare with the probability associated with the noninformative distribu-
tion π(λ) = 1/λ. Is it always possible to use this improper prior?
5.10 Consider x ∼B(n, p), H0 : p = 1/2, and H1 : p ̸= 1/2. The prior distri-
bution π(p) is a beta distribution Be(α, α). Determine the limiting posterior
probability of H0 when n = 10, x = 5 and n = 15, x = 7 as α goes to +∞. Are
these values intuitively logical? Give the posterior probabilities for Laplace,
Jeﬀreys, and Haldane noninformative priors.
5.11 Solve Exercises 5.9 and 5.10 for the Bayes factors instead of the posterior
probabilities.
5.12 In a normal setting, determine whether there exists a normalization prob-
lem associated with noninformative prior distributions for tests of one-sided
hypotheses such as
H0 : θ ∈[0, 1]
versus
H1 : θ > 1.
Replace 1 by ϵ and consider the evolution of the optimal answer as ϵ goes to 0.
Section 5.2.6
5.13 Establish the decomposition (5.2.6) from the original deﬁnition (5.2.5) of
B(ℓ)
10 . (Hint: Use Bayes formula to develop π1(θ1|x(ℓ)) and π0(θ1|x(ℓ)).)
5.14 In the setting of Example 5.2.13, show how B10(2) depends on the choice
of (x1, x2) by computing the normalizing constants of both π0(σ2|x1, x2) and
π1(μ, σ2|x1, x2) and by completing the integration in B10(2).
5.15 Aitkin (1991) suggests removing the diﬃculty with improper priors by using
the data twice: given x ∼f(x|θ) and an improper prior π, if H0 : θ = θ0 is
the hypothesis to be tested, take ˜π(θ) = π(θ|x) and use ˜π as the prior in the
Bayes factor.
a. If f(·|θ) is the N(θ, 1) density and π(θ) = 1, compute the corresponding
pseudo-Bayes factor.
b. Same question as a. when f(·|θ) is the P(λ) probability mass function and
π(λ) = 1/λ.
c. Analyze the limiting behavior of the pseudo-Bayes factor when the proce-
dure is iterated, that is, when π is iteratively updated in ˜π. [Note: From a
computational point of view, this technique can be of interest to compute
maximum likelihood and MAP estimates. See Robert and Casella (2004,
§5.2.4).]

270
Tests and Conﬁdence Regions
5
5.16 In the setting of Example 5.2.14, compute the Bayes factor when π1(θ) is
a N(0, 2) distribution and compare with the arithmetic intrinsic Bayes factor.
(Hint: Compute IE[exp(−x2/2)].)
5.17 (Exercise 5.16 cont.) For the fractional Bayes factor (5.2.11),
a. Show that the minimum value of b is 1/n.
b. Show that (5.2.11) corresponds to the intrinsic prior N(0, (1 −b)/nb).
c. Show that that a ﬁxed value of b leads to a variance shrinking to 0 in the
intrinsic prior.
d. Compare the numerical values of the arithmetic intrinsic and of the frac-
tional Bayes factors.
e. Determine whether there exists a value of b such that these two pseudo-
Bayes factors agree.
5.18 In the setting of Example 5.2.15,
a. Show that π2 does integrate to 1.
b. Show that BA
10 corresponds to a genuine Bayes factor under π2.
5.19 Coherence conditions for Bayes factors are given by
B12 = B10B02
and
B01 = 1/B10 ,
when considering three hypotheses, H0, H1 and H2, under corresponding pri-
ors π0, π1 and π2.
a. Show that these conditions are satisﬁed when the πi’s are proper priors.
b. Show that the fractional Bayes factor satisﬁes B01 = 1/B10 but not B12 =
B10B02
c. Show that neither the arithmetic not the geometric intrinsic Bayes factors
satisfy these conditions.
5.20 For the intrinsic prior considered in Example 5.2.17,
a. Show that
 ∞
θ0
	
2eθ−θ0 −1
−1 dθ = log(2).
(Hint: Use a change of variables from θ to ω = exp(θ −θ0), a fractional
decomposition of 1/ω(2ω −1).)
b. Deduce the expression (5.2.12).
5.21 In the setting of Example 5.2.19,
a. Show that
 ! n

t=1
 p
σ1 e−(xt−μ1)2/2σ2
1 + 1 −p
σ2 e−(xt−μ2)2/2σ2
2
 "b
dπ(μ, σ)
≥
 ! n

t=1
p
σ1 e−(xt−μ1)2/2σ2
1
"b
dπ(μ, σ) .
b. Deduce that the fractional Bayes factor does not exist for this model.
5.22 Consider n observations x1, . . . , xn from Student’s t-distribution T (ν, μ, σ)
and the null hypothesis H0 : μ = 0.

5.6
Exercises
271
a. Determine the minimal training sample size for the priors π0(σ) = 1/σ and
π1(μ, σ) = 1/σ.
b. Show that that the fractional Bayes factor cannot be computed analytically
in this case.
Section 5.3.1
5.23 Let f and g be two nondecreasing real functions.
a. Show that
IEθ[f(x)g(x)] ≥IEθ[f(x)]IEθ[g(x)]
for every distribution Pθ on x.
b. Use a. to show that, if f(x|θ) is a monotone likelihood ratio density in T(x),
the expectation IEθ[g(T(x))] is a nondecreasing function of θ. (Hint: Use
f(x) = 1 −f(x|θ′)/f(x|θ) and show that IEθ[f(x)] = 0.)
5.24 Show that Student’s t- and noncentral χ2 distributions have monotone
likelihood ratio.
Section 5.3.4
5.25 For the p-value ˜p deﬁned in Example 5.3.13, determine the values of ˜p(x)
for n = 15 and compare with
p(x) = P1/2 [f(X|1/2) > f(x|1/2)] .
5.26 (Johnstone and Lindley (1995)) Consider a point-null hypothesis H0 : θ =
θ0 where the p-value ϕ is well deﬁned. The only available information is that
the data is signiﬁcant at level α, i.e., that ϕ(x) < α.
a. Give the Bayes factor Rα of H0 versus H1 : θ ̸= θ0 when the data is
signiﬁcant at level α, for an arbitrary prior distribution π.
b. Given a second signiﬁcance level β with β < α, we assume that Rα < Rβ.
Establish a suﬃcient condition on π for this condition to be satisﬁed.
c. If Rα|β is the Bayes factor based on the information β < ϕ(x) < α, show
that Rα = ωRβ + (1 −ω)Rα|β and deduce that Rβ > Rα > Rα|β.
d. In the particular case when π(θ) is ϱ0IIθ0(θ)+(1−ϱ0)N(θ0, τ 2) and x1, . . . , xn
∼N(θ, σ2), show that Rα converges to (1−ϱ0)/ϱ0α when n goes to inﬁnity
but that Rα|β goes to 0.
Section 5.3.5
5.27 For x ∼N(θ, 1) and H0 : θ = 0, determine when the p-value crosses the
lower bounds P(x, GA) and P(x, GS).
5.28 (Berger and Delampady (1987)) Consider the case x ∼B(n, p) when H0 :
p = 1/2. For the following class of prior distributions
GC = {conjugate distributions with mean 1/2},
show that
P(x, GC)
=
inf
g∈GC
P(H0|x)
=

1 + 1 −π0
π0
sup
c>0
Γ(c)Γ(x + c/2)Γ(n −x + c/2)
Γ(c/2)2Γ(n + c)
−1
and derive a table of these lower bounds and the corresponding p-values for
n = 10, 20, 30 and x going from 0 to n/2.

272
Tests and Conﬁdence Regions
5
5.29
*(Casella and Berger (1987))
Establish the following lemma, used in
Lemma 5.3.16 and Theorem 5.3.19: if the family G is constituted of the mix-
tures
g(θ) =

Ξ
gξ(θ)h(ξ) dξ,
for every density h on Ξ, with gξ ∈G0 and
G0 = {gξ; ξ ∈Ξ},
then, for any f,
sup
g∈G

f(x|θ)g(θ) dθ = sup
ξ∈Ξ

f(x|θ)gξ(θ) dθ.
5.30 In the case when x ∼N(θ, 1) and H0 : θ ≤0, determine the lower bound
P(x, GSU)
=
inf
g∈GSU
P g(θ ≤0|x)
=
inf
g∈GSU
 0
−∞f(x −θ)g(θ) dθ
 +∞
−∞f(x −θ)g(θ) dθ
for x < 0. Does the conclusion of Casella and Berger (1987) still hold? Can
you explain why?
5.31
*(Casella and Berger (1987))
Consider a bounded symmetric unimodal
function, g. The family of the scale mixtures of g is deﬁned by
Gg = {πσ; πσ(θ) = (1/σ)g(θ/σ), σ > 0}.
If the sample density is f(x −θ), with f symmetric in 0, if it satisﬁes the
monotone likelihood ratio property, and if x > 0, show that
P(x, Gg) = p(x)
for the test of H0 : θ ≤0.
5.32
*(Casella and Berger (1987))
Consider the test of H0 :
θ ≤0 versus
H1 :
θ > 0 when x ∼f(x −θ). Let h and g be densities on (−∞, 0] and
(0, +∞).
a. Show that, if π(θ) = ϱ0h(θ) + (1 −ϱ0)g(θ),
sup
h
P π(θ ≤0|x) =
ϱ0f(x)
ϱ0f(x) + (1 −ϱ0)  +∞
0
f(x −θ)g(θ) dθ
and deduce that the supremum actually favors H0 by putting all the mass
at the boundary θ = 0.
b. If
π(θ) = ϱ0h(θ/σ1) 1
σ1 + (1 −ϱ0)g(θ/σ2) 1
σ2 ,
show that, when σ1 is ﬁxed
lim
σ2→∞P π(θ ≤0|x) = 1
and that, when σ2 is ﬁxed,
lim
σ1→∞P π(θ ≤0|x) = 0.

5.6
Exercises
273
5.33
*(Caron (1994)) In order to alleviate criticisms directed toward point-null
hypotheses, H0 : θ = θ0, the formulation of the null hypothesis can be modiﬁed
according to the prior distribution. For instance, given a prior distribution π on
Θ with mode in θ0 which does not give any prior weight to θ0, we can propose
the transformed hypothesis Hπ
0 : π(θ) > kπ, where the size of the HPD region
is determined by the “objective” requirement π(π(θ) > kπ) = 0.5. Consider
the case x ∼N(θ, 1) and θ0 = 0.
a. When π belongs to the family of the N(0, σ2) distributions, determine kπ
and derive the lower bound on the Bayesian answers within this family.
Compare with the posterior probabilities of Berger and Sellke (1987) for
the values of interest.
b. Determine whether the Jeﬀreys–Lindley paradox occurs in this approach.
c. For the alternative families U[−c,c] (c > 0) and π(θ|λ) ∝exp(−λ|θ|) (λ > 0),
derive the corresponding lower bounds.
5.34 *(Exercise 5.33 cont.) Consider the case x ∼C(θ, 1) when H0 : θ = 0.
a. Under Berger and Sellke (1987) approach, show that the posterior proba-
bility of H0 when πc is U[−c,c] is
πc(H0|x) = +
1 + (1 + x2)(arctan(c −x) + arctan(c + x))/2c,−1 .
b. Under the approach developed in the previous exercise, show that the cor-
responding probability is
πc(Hπ
0 |x) = arctan(c/2 −x) + arctan(c/2 + x))
arctan(c −x) + arctan(c + x))
.
c. Compute and compare the lower bounds for both approaches.
d. Show that
lim
x→∞
infc πc(Hπ
0 |x)
infc πc(H0|x) = 2
3.
Section 5.4
5.35 (Hwang et al. (1992)) Show that, under the loss (5.4.2), the p-values deﬁned
in Example 5.4.6 are indeed admissible. (Hint: Show that the Bayes risks are
ﬁnite.)
5.36 (Hwang et al. (1992))
The goal of this exercise is to show that, for the
two-sided test (5.4.3), the p-value p(x) can take the value 1. (Hint: Remember
that the UMPU test in this setting is of the form
ϕ(x) =
 0
if T(x) < c0 or T(x) > c1,
1
otherwise ,
with c0 = c0(α) and c1 = c1(α).)
a. Consider θ1 ̸= θ2 and
c∗= inf{T(x); f(x|θ2) > f(x|θ1)}.
Show that c∗∈[c0(α), c1(α)] for every 0 < α < 1.
b. Consider θ1 = θ2. Apply the previous result to
f(x|θ∗) = IEθ1[T(x)]f(x|θ1),
f(x|θ∗∗) = T(x)f(x|θ1),
and conclude.

274
Tests and Conﬁdence Regions
5
5.37 (Hwang et al. (1992))
In the normal setting, consider the point-null hy-
pothesis H0 : θ = 0. Show that, under the loss (5.4.2), the p-value cannot be
dominated by a proper posterior probability. (Hint: Show ﬁrst that, for every
a and ϵ,
Pθ(a < |x| < a + ϵ)
Pθ(|x| < a)
→+∞
when θ goes to inﬁnity.)
5.38 (Hwang et al. (1992)) Under the loss (5.4.2), show that ϕ(x) = 1/2 is the
unique minimax estimator. Extend to all strictly convex losses. In this setting,
does there exist least favorable distributions?
5.39 (Robert and Casella (1994))
A modiﬁcation of the loss function (5.4.1)
introduces a distance weight in order to penalize in a diﬀerent manner errors
made in the vicinity of the boundary between H0 and H1, and those made far
away from this boundary.
a. If the null hypothesis is H0 : θ ≤θ0 for x ∼N(θ, 1) and the loss function is
L(θ, ϕ) = (θ −θ0)2(IIH0(θ) −ϕ)2,
give the general form of the Bayes estimators.
b. If π(θ) = 1, show that the Bayes estimator is smaller than the p-value if
x > θ0 and larger if x < θ0.
5.40 (Robert and Casella (1994))
From a model-choice perspective, the loss
function incorporates the consequences of an acceptance or of a rejection of
the null hypothesis H0 : θ = θ0 in terms of estimation.
a. For the loss function
L1(θ, (ϕ, δ)) = d(θ −δ)|1 −ϕ| + d(θ0 −θ)|ϕ|,
show that the Bayes estimators are (0, δπ(x)) where δπ(x) is the regular
Bayes estimator of θ under d(θ −δ) for every d and π.
b. For the loss function
L2(θ, (ϕ, δ)) = d(θ −δ)|1 −ϕ| + d(θ0 −δ)|ϕ|,
show that the Bayes rule is (1, θ0) for every π and d.
c. For the loss function
L3(θ, (ϕ, δ)) = (δ −θ)2(IIH0(θ) −ϕ)2,
show that the associated Bayes rule is (0, θ0), i.e., that the Bayes procedure
always rejects the null hypothesis H0 : θ = θ0, but always uses θ0 as an
estimator of θ.
d. Study the Bayes procedures under the modiﬁed loss
L4(θ, (ϕ, δ)) = +
1 + (δ −θ)2,+
1 + (IIH0(θ) −ϕ)2,
,
to examine whether they are less paradoxical.
e. Show that the loss function
L5(θ, (ϕ, δ)) = ξ(δ −θ)2|1 −ϕ| + {(δ −θ0)2 + (θ −θ0)2}|ϕ|,
provides a reasonable pre-test Bayes procedure that avoids the paradoxes
of L1, L2, and L3 if and only if ξ > 1.

5.6
Exercises
275
Section 5.5.1
5.41 Consider two independent observations x1, x2 from a Cauchy distribution
C(θ, 1). For π(θ) = 1, give the shape of the α-credible HPD region. What
alternative (and more convincing) α-credible region could you propose?
5.42 Give the α-credible region when x ∼P(λ) and λ ∼G(δ, β). Study the
evolution of this region as a function of δ and β. Examine the particular case
of the noninformative distribution.
5.43
*An alternative notion of α-credible regions is studied in this exercise.
The best Bayes center at level α is the estimator δπ
α(x), center of the ball of
smallest radius with coverage 1 −α, i.e.,
P π(||θ −δπ
α(x)|| < k|x) = sup
δ
P π(||θ −δ(x)|| < k|x) = 1 −α.
a. Show that, if the posterior distribution is spherically symmetric and uni-
modal, the corresponding region is the HPD region.
b. Consider x ∼N(θ, 1), θ ∼N(0, τ 2), and π(τ 2) = 1/τ 3/2. Determine the
posterior distribution. Show that this distribution is unimodal when 0 <
x2 < 2 and bimodal otherwise, with second mode
δ(x) =
!
1 −1 −
1
1 −(2/x2)
2
"
x.
Derive the best Bayes center and show that, if α is large enough, δπ
α is
discontinuous and close to
φ(x) = (1 −
1
2x2 )+x,
i.e., that this Bayes estimator replicates the James–Stein estimator.
c. Generalize b. when π(τ 2) = τ −v.
d. Show that the best Bayes center associated with a proper prior distribution
π is admissible under the loss
L(θ, δ) = II(k,+∞)(||θ −δ||2).
5.44
*(Thatcher (1964)) Consider x ∼B(n, θ) and, for 0 < α < 1 and a prior
π on θ, deﬁne θπ
x by P π(θ ≤θπ
x|x) = α.
a. If π(θ) = (1 −θ)−1, show that Pθ(θ ≤θπ
x) ≤α for θ > 0.
b. If π(θ) = θ−1, show that Pθ(θ ≤θπ
x) ≥α for θ < 1.
c. Deﬁne θλ
x associated with π(θ) = θλ−1(1 −θ)−λ, 0 ≤λ ≤1. Show that θλ
x
is increasing in λ and deduce that
lim
θ↑θλ
x
Pθ(θ ≤θλ
x) ≥α ≥lim
θ↓θλ
x
Pθ(θ ≤θλ
x).
5.45
*(Hartigan (1983)) Consider x ∼P(λ) and for 0 < α < 1, and a prior π
on λ, deﬁne λπ
x by
P π(0 ≤λ ≤λπ
x|x) = α.
a. Show that, if π(λ) = 1/λ, Pλ(λ ≤λπ
x) ≤α for every λ.

276
Tests and Conﬁdence Regions
5
b. Show that, if π(λ) = 1, Pλ(λ ≤λπ
x) ≥α for every λ. (Hint: Use the
following relation:
∞

x=x0
e−λ λx
x! =
 ∞
0
ux0−1
(x0 −1)!e−udu.

5.46 A famous problem in classical Statistics is the Behrens–Fisher problem. It
stems from a simple setting of two normal populations with unknown means
and variances because there is no UMP or UMPU test to compare the means.
Consider x1, . . . , xn a sample from N(θ, σ2) and y1, . . . , ym a sample from
N(μ, τ 2) where θ, μ, τ, σ are unknown.
a. *Show that there is no UMPU test procedure for the hypothesis H0 : θ =
μ. (Hint: Condition on s2
x and s2
y, given below, to show that the UMPU
procedures vary with s2
x and s2
y.)
b. Explain why a reasonable test should be based on the pivotal quantity
T = (θ −μ) −(¯x −¯y)
1
s2x/n + s2y/m
with ¯x = 
i xi/n, ¯y = 
j yj/m, s2
x = 
i(xi −¯x)2/n −1, and s2
y =

j(yj −¯y)2/m −1.
c. Show that the distribution of T depends on σ/τ even when θ = μ, and that
this distribution is not a Student’s t-distribution.
d. Give the posterior of T when π(θ, μ, σ, τ) = 1/σ2τ 2 and show that it de-
pends only on (sx/√n)(sy/√m). [Note: See Robinson (1982) for a detailed
survey of the diﬀerent issues related to this problem.]
Section 5.5.2
5.47 (Casella and Berger (1990)) Consider x ∼N(μ, 1) and
Ca(x) = {μ; min(0, x −a) ≤μ ≤max(0, x + a)}.
a. Consider a = 1.645. Show that Ca is a conﬁdence interval at level 95% with
P0(0 ∈Ca(x)) = 1.
b. If π(μ) = 1 and a = 1.645, show that Ca is also a 0.1-credible region and
that
P π(μ ∈Ca(x)|x) = 0.90
if |x| ≤1.645 and
lim
|x|→+∞P π(μ ∈Ca(x)|x) = 1.
5.48 Consider x ∼f(x|θ) with θ ∈IR and π a prior distribution on θ. If we
deﬁne the α-credible set (−∞, θx) by P π(θ ≥θx|x) = α, show that this one-
sided interval cannot be at level α in the frequentist sense. (Hint: Show that
P(θ ≥θx|θ ≤θ0) > α for some θ0.)
5.49 *(Fieller (1954)) In the setting of calibration (see Exercise 4.47), conﬁdence
sets need to have inﬁnite length to maintain a ﬁxed conﬁdence level, as shown
by Gleser and Hwang (1987). Consider (x1, y1), . . . , (xn, yn) a sample from
N2(μ, Σ). The parameter of interest is θ, the ratio of the two means μx and μy.

5.6
Exercises
277
a. Deﬁne ¯zθ = ¯y −θ¯x. Show that
¯zθ ∼N

0, 1
n(σ2
y −2θσxy + θ2σ2
x

and that
ˆvθ =
1
n −1(s2
y −2θsxy + θ2sx)
is an unbiased estimator of vθ, the variance of ¯zθ, when ¯x, ¯y, s2
x, sxy, and
s2
y denote the usual empirical moments and
Σ =

σ2
x
σxy
σxy
σ2
y

.
b. Show that ¯zθ and ˆvθ are independent and that (n−1)ˆvθ/vθ ∼χ2
n−1. Deduce
that {θ; ¯zθ/ˆvθ ≤t2
n−1,α/2} deﬁnes a (1 −α) conﬁdence set.
c. Show that this conﬁdence set deﬁnes a parabola in θ and can be an interval,
the complement of an interval or the whole real line.
Section 5.5.3
5.50
*Domination of the usual estimator as center of a conﬁdence region does
not necessarily follow from the corresponding domination for the quadratic
loss. Show that, in the normal case, if
δJS
a (x) =

1 −
a
||x||2

x,
the recentered conﬁdence region
CJS
a (x) = {θ; ||θ −δJS
a (x)||2 ≤cα},
does not dominate the usual conﬁdence region, even though δJS
a
dominates δ0
when a ≤2(p −2). (Hint: Consider θ = 0.)
5.51 (Casella et al. (1993b)) Show that the rational loss given in Section 5.5,
L(θ, C) =
vol(C)
k + vol(C) −IIC(θ),
does not lead to Berger’s paradox in the normal case.
5.52
*(Casella et al. (1993b)) Consider a general loss function of the form
L(θ, C) = S(vol(C)) −IIC(θ),
with S increasing, 0 ≤S(t) ≤1.
a. Show that the Bayes estimators are the HPD regions.
b. Show that, if x ∼Np(θ, Ip) and θ ∼Np(μ, τ 2Ip), the Bayesian credible sets
Cπ are not empty if S(t) = t/(a + t).
c. Determine the smallest radius of Cπ as τ varies.
d. Consider ¯x ∼N(θ, σ2/n) and s2 ∼σ2χ2
q. Under the rational loss, show
that
Cπ(¯x, s2) =

θ; |θ −¯x| ≤t∗s
√n

,
where t∗is the solution of
min
t

2ts/√n
a + 2ts/√n −P(|Tn−1| < t)

.
Deduce that P(|Tn−1| < t∗(s)|s) ≥1/2.

278
Tests and Conﬁdence Regions
5
5.53 (Walley (1991))
Consider the double-exponential distribution, f(x|θ) =
(1/2) exp(−|x −θ|).
a. Show that Cx = (−∞, x] is a 50% conﬁdence interval.
b. Show that Pθ(θ ∈Cx|x < 0) < 0.5 for every θ.
c. Let ϕ(x) = (e2x/2)IIx<0. Show that
IEθ[IIx<0(IICx(θ) −1/2) + ϕ(x)] ≥0
and deduce that γ(x) = 1/2 is not an admissible conﬁdence report under
squared error loss for Cx.
Note 5.7.3
5.54
*(Brown (1967)) In the setting of Example 5.5.9, show that
P 	√n|¯x −θ| ≤ks|s ≤1
≤α > P 	√n|¯x −θ| ≤ks|s > 1
and derive a positively relevant subset. (Hint: Show that
P 	√n|¯x −θ| ≤ks|s
is increasing in s.)
5.55 (Walley (1991)) Consider a sample x1, . . . , xn from U[θ,θ+1].
a. Show that uniformly more accurate one-sided conﬁdence intervals are of
the form Cx = [(x(1) + 1 −K) ∧(x(n) −1), x(1) + 1] and check that the
conﬁdence level is γ = 1 −(1 −K/2)n.
b. For n = 1 and γ = 1/2, show that Cx = [x, x + 1]. Consider a strictly
decreasing bounded function f and ϕ(x) = (f(x) −f(x + 1)) ∧(f(x −1) −
f(x)). Verify that
IEθ[f(IICx(θ) −0.5)] = 0.25
 θ+1
θ
(f(x −1) −f(x)) dx
and
IEθ[ϕ(x)] ≤1
8
 θ+1
θ
(f(x −1) −f(x)) dx.
c. Deduce that
IEθ[f(IICx(θ) −0.5) −ϕ(x)] ≥0
for every θ and that γ = 1/2 is not an admissible report.
d. For n ≥2, we deﬁne
B = {(x1, . . . , xn); x(n) −x(1) ≥2 −K}.
Show that
Pθ(θ ∈C(x1, . . . , xn)|(x1, . . . , xn) ∈B) = 1
and conclude that B is a relevant subset.
Note 5.7.4
5.56 (Berger et al. (1997)) For the report γ(x) given in (5.7.1), show that
γ(x) =
s
1 + s
if
s < r,
γ(x) =
1
1 + s
if
s > a .

5.7
Notes
279
5.57 Show that, in the setting of Example 5.7.4, Ψ(1) > 1 and give the Bayes
factor in favor of H0.
5.58 (Lindley (1997))
When introducing the third decision −1 in the testing
problem, consider the following extension to the 0 −1 loss function:
L(θ, ϕ) =
 ℓi
if ϕ = 1 −i and Hi is true,
mi
if ϕ = −1 and Hi is true.
Compute the posterior losses and show that ϕ = −1 if
m1ϱ
ℓ0 −m0 < B10(x) < (ℓ1 −m1)ϱ
m0
,
where ϱ denotes the prior odds, π1/π0.
5.59 (Lindley (1997)) Show that S(x) given in (5.7.2) is not an ancillary statis-
tic, except when
τ(t) + ϱ = 1 + ϱ
t ,
t > c ,
where c is deﬁned by F0(c) = 1 −ϱF1(c) and τ(t) is given by F0(t) =
1 −ϱF1(τ(t)). Show that this property holds when B10(x) has the same dis-
tribution under m1 than B01(x) under m0. [Note: See Berger et al. (1994,
p. 1798).]
5.7 Notes
5.7.1 P-values and Bayesian decisions
A common criticism of the comparison of Section 5.3.5 is that it is meaning-
less: the two types of answers are conceptually diﬀerent and p-values are not
probabilities. The answer to this criticism is that, besides being used as prob-
abilities by practionners, p-values are addressing the same inferential problem
as Bayesian posterior probabilities, from a decision-theoretic point of view. It
thus does make sense to compare them directly.
Consider an a0 −a1 loss function as in (5.2.1). The UMPU minimax test is
then
ϕ(x) =
 1
if p(x) >
a1
a0+a1 ,
0
otherwise.
In fact, when power functions are continuous and hypotheses are contiguous
(see Lehmann (1986, Chapter 4)), a UMPU test satisﬁes
sup
Θ0
Pθ(ϕ(x) = 0) = α = inf
Θ1 Pθ(ϕ(x) = 0) = 1 −sup
Θ1
Pθ(ϕ(x) = 1).
Moreover, when ϕ is minimax under this loss, it satisﬁes
sup
Θ0
R(θ, ϕ)
=
a0 sup
Θ0
Pθ(ϕ(x) = 0)
=
sup
Θ1
R(θ, ϕ) = a1 sup
Θ1
Pθ(ϕ(x) = 1).
Therefore, under regularity assumptions satisﬁed, for instance by exponential
families, ϕ is such that
sup
Θ0
Pθ(ϕ = 0) =
a1
a1 + a0 .

280
Tests and Conﬁdence Regions
5
It then follows from Proposition 5.2.2 that it is legitimate to compare the p-
value p(x) with posterior probabilities, since the Bayesian decision procedure
is given by
γπ(x) =
 1
if P π(θ ∈Θ0|x) >
a0
a0+a1 ,
0
otherwise
and both approaches compare a continuous evaluation (p-values or posterior
probability) to the same bound.
5.7.2 Unequal prior probabilities
Another criticism of the lower bound evaluation of Section 5.3.5, found, for
instance, in Casella and Berger (1987), is that the lower bound is not computed
on the set of all prior distributions, since it only considers the prior probability
ϱ0 = 1/2. Obviously, if ϱ0 can be modiﬁed too, it is always possible to ﬁnd
a Bayesian answer smaller than the p-value, since the lower bound over all
Bayesian answers is then 0 for every x (corresponding to ϱ0 = 0). However,
for a ﬁxed value of ϱ0 > 0, there always are values of x for which the lower
bound on posterior probabilities is higher than the p-value.
A reﬁned version of this criticism is to consider that the weight ϱ0 = 1/2 is
not necessarily the most objective probability, and that it should be deter-
mined in terms of the prior π itself. In fact, as mentioned above, priors of the
form π(θ) = ϱ0IIθ0(θ) + (1 −ϱ0)π1(θ) are quite artiﬁcial. While such priors are
necessary to solve the testing problem, it is more natural to think of π as a
modiﬁcation of the original prior π1 in the light of this problem. The infer-
ential problem, that is, the fact that θ0 is of interest, contains some residual
information strong enough to justify a modiﬁcation of the prior distribution
(otherwise, the test question should be modiﬁed to become compatible with
the prior information). It thus makes sense to require that the weight ϱ0 should
appear as a function of π1. (This point will reappear in Chapter 7 with the
case of nested models in model choice: the upper model, which contains all
others, should be more likely than the others.)
Example 5.7.1 (Example 5.3.6 continued)
Since H0 : θ = 0 is to be
tested, the prior probability of H0 is null under any continuous prior π1.
Nonetheless, it is reasonable to require that H0 should have a larger prior
probability if π1 is N(0, 1) than if π1 is N(0, 10), since every neighborhood
of 0 is less probable under the latter distribution. This is why the Jeﬀreys–
Lindley paradox is deemed a “paradox”: increase in the probabilities from
Table 5.2.4 to Table 5.2.4 seems counterintuitive.
∥
Unfortunately, a determination of the weight ϱ0 as a function of π1 is quite
controversial and we only brieﬂy mention a solution proposed in Robert and
Caron (1996) (see Spiegelhalter and Smith (1980) for another approach based
on the most favorable virtual observations). The basic idea is that the weight
ϱ0 should satisfy
(1 −ϱ0)π1(θ0) = ϱ0,
in order for θ0 to be equally weighted under both hypotheses. Of course, we
are comparing a weight under the Dirac mass at 0, ϱ0, with an instantaneous
weight with respect to the Lebesgue measure, (1 −ϱ0)π1(θ0), and the compa-
rison is not justiﬁed from a mathematical point of view (since the value of the

5.7
Notes
281
density π1 at a given point like θ0 is arbitrary). Moreover, the above equation
does not always allow for a solution.
Example 5.7.2 (Example 5.3.6 continued) When π1(θ) is a normal prior
N(0, n), the above equality leads to the weight
ϱ0 =
π1(0)
1 + π1(0) =
1
1 +
√
2πn
,
and the corresponding posterior probability of H0 is

1 + 1 −ϱ0
ϱ0
m1(x)
ϕ(x)
−1
=

1 +

2π
n
n + 1ex2/2−x2/2(n+1)
−1
=
!
1 +

2πn
n + 1e
n
2(n+1) x2
"−1
.
Notice that this approach avoids the Jeﬀreys–Lindley paradox, since the lim-
iting probability (when n goes to +∞) is

1 +
√
2πex2/2−1
.
This value also happens to be the posterior probability associated with the
Lebesgue prior, π(θ) = 1.
∥
5.7.3 Conditional evaluation of conﬁdence regions
A critical assessment of the Neyman–Pearson conﬁdence regions (and more
generally of frequentist procedures) follows from the conditional analysis of
Kiefer (1977) and Robinson (1979). Lehmann (1986, Chapter 10) gives an
overview of this approach (see also Buehler (1959), Pierce (1973), Casella
(1987, 1992), Maatta and Casella (1990), and Goutis and Casella (1991, 1992)).
These works have shown that the classical conﬁdence procedures are often
suboptimal when considered from a conditional viewpoint.
Deﬁnition 5.7.3
Consider Cx, a conﬁdence region at signiﬁcance level α. A
set A ⊂X is said to be a negatively biased relevant subset for the conﬁdence
region Cx if there exists ϵ > 0 such that
Pθ(θ ∈Cx|x ∈A) ≤1 −α −ϵ
for every θ ∈Θ.
We can similarly deﬁne positively biased relevant subsets. This notion is gen-
eralized in Robinson (1979) into the notion of relevant betting procedures. The
existence of such sets questions the notion of a conﬁdence level α itself be-
cause, depending on the conditioning set, the coverage probability may vary
and even fall below the nominal minimal conﬁdence statement. Obviously, this
criticism can be transfered to testing procedures by a duality argument.
In the setting of Example 5.5.8, while working on t-tests, Brown (1967) estab-
lishes that there exist positively biased relevant sets of the form {|x| < k} and
this implies
Pθ(θ ∈Cx||x| > k) ≤1 −α

282
Tests and Conﬁdence Regions
5
(see also Exercise 5.54). Such phenomena led Kiefer (1977) to suggest a parti-
tion of the sample space X into subsets, and to allocate each of these subsets
with a diﬀerent conﬁdence level (see also Brown (1978)). Following Fisher’s
analysis, he suggested that these subsets should be indexed by ancillary statis-
tics. For instance, the adequate ancillary statistic for Example 2.3.1 is x1 −x2.
Unfortunately, in most settings, the choice of the ancillary statistic modiﬁes
the conﬁdence report, and Berger and Wolpert (1988) give an example in which
diﬀerent ancillary statistics lead to diﬀerent conﬁdence statements, a setting
incompatible with the Likelihood Principle. We consider that, fundamentally,
the problem exhibited by the existence of relevant biased sets is not related
to the conﬁdence region Cx itself but rather with the associated conﬁdence
level α, which should be replaced by a more adaptive (or more conditional)
conﬁdence statement α(x) (see Section 4.2). In fact, the existence of relevant
betting procedures is equivalent to the domination of the constant conﬁdence
report under quadratic loss (Robinson (1979)).
5.7.4 Reconciliation perspective
While Section 5.3 has shown that the frequentist answers, that is, the p-values,
were intrinsically and numerically diﬀerent from their Bayesian counterparts
(see also Note 5.7.1), a modiﬁcation of the decision setting, brought up by
Berger, Brown and Wolpert (1994), allows for a partial reconciliation of both
perspectives. While, from a Bayesian point of view, reconciliation is not an
important feature—the basic purpose of a procedure is to act in an optimal
way when faced with a decision problem, rather than to enjoy a long run
stability—it oﬀers several advantages from a practical point of view: ﬁrstly, it
makes statisticians more comfortable with using a Bayesian procedure if this
procedure simultaneously enjoys frequentist properties. Secondly, it eliminates
the diﬃculty with the interpretation of a p-value as a posterior probability.
The modiﬁcation is to introduce a “no-decision” alternative to the “accept”
and “reject” answers used in classical tests. Even though this additional pos-
sibility may seem ludicrous from a decision-making point of view, it certainly
makes sense from a statistical point of view: there are indeed cases where the
data is too inconclusive to answer satisfactorily about a given hypothesis H0
and when we want to ask the client for more observations, or for more pre-
cise prior information. In fact, this perspective has been around for sequential
testing since Wald’s sequential probability ratio test (see Lehmann (1986)).
(Notice, however, that the procedure of Berger et al. (1994) does not account
for repeated testing, which aﬀects the acceptance levels. See also Example
1.3.6.)
In the case of two simple hypotheses,
H0 : x ∼m0(x)
versus
H1 : x ∼m1(x) ,
where m0 and m1 are known densities, the Bayes factor B10 is equal to the like-
lihood ratio m1(x)/m0(x). If −1 denotes the no-decision answer, the modiﬁed
Bayesian test of Berger et al. (1994) is of the form
ϕ(x) =
 1
if B10(x) ≥a,
0
if B10(x) ≤r,
−1
if r < B10(x) < a,
(5.7.1)

5.7
Notes
283
with an associated probability error report given by
γ(x) =

1/(1 + B10(x))
if B10(x) ≥a,
B10(x)/(1 + B10(x))
if B10(x) ≤r.
Notice that γ(x) is the posterior probability of the rejected hypothesis and is
thus optimal under squared error loss. (But the procedure ϕ is arguably not
a decision procedure. See Exercise 5.58.)
Now, if we denote F0 and F1 the c.d.f.’s of B10(x) associated with m0 and
m1 respectively, and if we deﬁne Ψ(b) = F −1
0
(1 −F1(b)), then Ψ−1(b) =
F −1
1
(1 −F0(b)), Berger et al. (1994) take
r = 1
and
a = Ψ(1)
if Ψ(1) > 1
r = Ψ−1(1)
and
a = 1
if Ψ(1) < 1 .
They show that the report γ(x) is then correct from a conditional frequentist
perspective: conditional on
S(x) = min{B10(x), Ψ−1(B10(x))} ,
(5.7.2)
the procedure (ϕ, γ) satisﬁes
P0(B10(x) ≥a|S(x) = s) = γ(s) ,
P1(B10(x) ≤r|S(x) = s) = γ(s) ,
where γ(x) only depends on s (Exercise 5.56). Notice however that S(x) is not
an ancillary statistic, except in special cases (Exercise 5.59).
The extension of this result to composite hypotheses,
H0 : θ = θ0
versus
H1 : θ ∈Θ1
follows by representing H1 as in Section 5.3.5, that is, with
H1 : x ∼m1(x) =

Θ1
f(x|θ)π1(θ)dθ .
Then Berger, Boukai and Wang (1997) show that the conditional frequentist
evaluation under H0 still coincides with the Bayesian report, in a weaker sense
that, while being satisfactory from a Bayesian perspective, is less compelling
from a frequentist perspective (Hinkley (1997), Louis (1997)).
Example 5.7.4 (Berger et al. (1997))
For x1, . . . , xn i.i.d. N(θ, σ2), with
σ known, consider the test of H0 :
θ = θ0 under the conjugate prior θ ∼
N(μ, kσ2). Then, if z = √n(¯xn −θ0)/σ,
m0(z) =
1
√
2π
exp{−z2/2}
and
m1(z) =
1
√
2π
√
1 + kn
exp

−(z +
√
knΔ)2
2(1 + kn)

,
where Δ = (θ0 −μ)/
√
kσ. Then the Bayes factor is given by
B10(x) =
√
1 + kn exp

−
kn
2(1 + kn)

z −
Δ
√
kn
2
+ Δ2
2

,
Ψ(1) > 1, r = 1 and a = F −1
0
(1 −F1(1)).
∥


CHAPTER 6
Bayesian Calculations
The contraption began to quiver, steam hissing out from two or three places.
The hiss grew to a shriek, and the thing began trembling. It groaned ominously.
The shriek became ear-piercing. It shook so hard the table moved. The balding
man threw himself at the table, fumbling a plug loose on the largest cylinder.
Steam rushed out in a cloud, and the thing went still.
Robert Jordan, Lord of Chaos, Book VI of the Wheel of Time.
6.1 Implementation diﬃculties
At this stage of the book, we need to discuss a practical aspect of the
Bayesian paradigm, namely, the computation of Bayes estimators. The ul-
timate simplicity of the Bayesian approach is that, given a loss function L
and a prior distribution π, the Bayes estimate associated with an observa-
tion x is the (usually unique) decision d minimizing the posterior loss
L(π, d|x) =

Θ
L(θ, d)π(θ|x) dθ.
(6.1.1)
However, minimizing (6.1.1) can be hindered by two diﬃculties in practice:
(i) the explicit computation of the posterior distribution, π(θ|x), may be
impossible; and
(ii) even if π(θ|x) is known, this does not necessarily imply that minimizing
(6.1.1) is an easy task; indeed, when analytic integration is impossi-
ble, numerical minimization sometimes calls for a formidable amount of
computing time, especially when Θ and D have large dimensions.
Point (i) may seem to be a minor and formal diﬃculty since minimizing
(6.1.1) is actually equivalent to minimizing

Θ
L(θ, d)π(θ)f(x|θ) dθ,
which does not require an evaluation of π(θ|x). However, we saw in Chap-
ters 2 and 4 that classical losses, like the quadratic losses, lead directly to

286
Bayesian Calculations
6
estimators expressed through the posterior distribution, like the posterior
mean
δπ(x)
=

Θ
θ π(θ|x) dθ
=

Θ θ π(θ)f(x|θ) dθ

Θ π(θ)f(x|θ) dθ ,
for the quadratic loss; they thus necessitate direct computation of poste-
rior moments or other posterior quantities. A similar comment applies for
the derivation of other posterior quantities of interest, such as posterior
quantiles, Bayes factors, or conﬁdence regions.
A simplifying answer to these computational diﬃculties is to only use
sampling models, prior distributions, and losses which lead to explicit solu-
tions for the minimization of (6.1.1). This restrictive approach was techni-
cally justiﬁed when the computational tools described below were not avail-
able, but is unacceptable on subjective grounds, since loss functions and
prior distributions should be constructed according to the decision prob-
lem, not because they provide closed-form answers, as already stressed in
Chapter 3.1
This chapter is thus intended to avoid a systematic recourse to simple
prior distributions and losses by providing the reader with an array of
recent and sophisticated approximation methods that can be used when no
analytical expression of the posterior distribution or estimators is available.
This chapter is only an introduction to these methods; the reader is referred
to Robert and Casella (1999, 2004) for a more thorough treatment.
Although optimization problems like loss minimization or computation
of a MAP estimator can also be addressed by simulation techniques (see
Geyer and Thompson (1992), Geyer (1996), Robert and Casella (2004,
Chapter 5), or Doucet and Robert (2000)), we will focus in this chapter
on approximations to π(θ|x) and integrals involving π(θ|x) because this
is the cornerstone of computational diﬃculties with Bayesian inference. In
addition, if π(θ|x) can be correctly approximated, it is usually possible to
derive an approximation of L(π, d|x) for an arbitrary d, and then to use a
classical minimization method.
We now introduce a series of examples used throughout this chapter to
illustrate diﬀerent computational methods.
Example 6.1.1 Consider x1, . . . , xn a sample from C(θ, 1), a Cauchy dis-
tribution with location parameter θ, and θ ∼N(μ, σ2), with known hyper-
1 Classical illustrations resort to such simple settings because they allow for a clearer
and more concise presentation of points of interest, and this book has made intensive
use of exponential families, conjugate priors, and quadratic losses for this reason.
Nevertheless, a more adaptive approach, relying for instance on mixtures of conjugate
priors, should be adopted in practical settings.

6.1
Implementation diﬃculties
287
parameters μ and σ2. The posterior distribution of θ is then
π(θ|x1, . . . , xn) ∝e−(θ−μ)2/2σ2
n

i=1
[1 + (xi −θ)2]−1,
which cannot be integrated analytically. When δπ is the posterior mean,
δπ(x1, . . . , xn) =
 +∞
−∞θe−(θ−μ)2/2σ2 .n
i=1[1 + (xi −θ)2]−1dθ
 +∞
−∞e−(θ−μ)2/2σ2 .n
i=1[1 + (xi −θ)2]−1dθ
,
its calculation requires two numerical integrations (one for the numerator
and one for the denominator). The computation of the variance calls for an
additional integration. Moreover, the usually multimodal structure of this
distribution (see Exercise 1.28) may require special tuning for standard
integration packages.
∥
As we have seen before, the computational problem may result from the
choice of the loss, even when the prior distribution is conjugate.
Example 6.1.2
Let x|θ ∼Np(θ, σ2Ip) and θ|μ, τ ∼Np(μ, τ2Ip), with
known hyperparameters μ and τ. The posterior distribution on θ is then
quite manageable since
θ|x ∼Np
σ2μ + τ 2x
σ2 + τ 2 ,
σ2τ 2
σ2 + τ 2 Ip

.
When ||θ||2 is the parameter of interest, the usual rescaled quadratic loss is
L(θ, δ) = (δ −||θ||2)2
2||θ||2 + p ,
as in Saxena and Alam (1982). It leads to the following Bayes estimator:
δπ(x) = IEπ[||θ||2/(2||θ||2 + p)|x]
IEπ[1/(2||θ||2 + p)|x]
.
Although (σ−2 + τ −2)||θ||2 is distributed a posteriori as a χ2
p(λ) random
variable, with
λ = ||σ2μ + τ 2x||2
σ2τ 2(σ2 + τ 2),
an analytic version of δπ does not exist and numerical approximation is
again necessary. Notice that, in this case, numerical integration is more
complicated than for Example 6.1.1 because the density of χ2
p(λ) (see Ap-
pendix A) involves a modiﬁed Bessel function, I(p−2)/2(t), which must be
approximated in most settings by a series of weighted central chi-squared
densities or by a continued fraction approximation (see Exercise 4.35). An
alternative approach is to integrate instead over θ, but this is only feasible
for small p’s.
∥
Chapter 10 will also provide settings where approximations of Bayes esti-
mators are necessary. Indeed, most hierarchical Bayes estimators cannot be

288
Bayesian Calculations
6
computed analytically; for instance, this is the case for normal observations
(see Lemma 10.2.16) and graphical models (see Note 10.7.1). Moreover, a
numerical approximation of these estimators can get quite involved, espe-
cially for higher dimensions.
Example 6.1.3
The call to an auxiliary variable in a multivariate Stu-
dent’s t model reduces the number of integrations to one, as pointed out
by Dickey (1968). Let us recall that, if
x ∼Np(θ, σ2Ip),
θ ∼Tp(ν, μ, τ2Ip),
we can write
θ|ξ, x
∼
Np

ξ(x),
τ 2σ2
σ2ξ + τ 2 Ip

,
π(ξ|x)
∝
ξ(p+ν)/2−1
(ξσ2 + τ 2)p/2 exp
−1
2
||x −μ||2ξ
τ 2 + ξσ2 + ξ2ν

,
with
ξ(x) = ξσ2μ + τ 2x
ξσ2 + τ 2
(see Example 10.2.2). Consider the following generalization:
x|θ, Λ ∼Np(θ, Λ),
when θ and Λ = diag(σ2
1, . . . , σ2
p) are unknown, with prior distributions
(1 ≤i ≤p)
θi|σi ∼N

μi, σ2
i
ni

,
σ2
i ∼IG(νi/2, s2
i /2) ,
where the ni’s, si’s and νi’s are known hyperparameters. In this case (1 ≤
i ≤p),
θi|xi ∼T

νi + 1, xi + niμi
ni + 1
,
(νi + 1)−1(ni + 1)−1

s2
i +
ni
n1 + 1(xi −μi)2

,
and the call to an auxiliary variable ξi for each component θi does not
modify the complexity of the estimation problem, since it does not change
the number of integrals.
∥
The two examples below are paradoxical, in the sense that a formal ex-
plicit expression of the Bayes estimator is available, but it cannot readily
be used in practice, either because it induces numerical instability and thus
unreliability of the result (Example 6.1.4), or because the actual computa-
tion of the resulting Bayes estimator is impossible, meaning it cannot be
done in a reasonable amount of time for realistic sizes (Example 6.1.5).

6.1
Implementation diﬃculties
289
Example 6.1.4
In the setting of capture-recapture models, we consider
the following temporal model (see Section 4.3.3) with conjugate priors
xi|N, pi
∼
B(N, pi),
π(N) = 1/N,
pi
∼
Be(α, β)
(1 ≤i ≤n).
If x+ denotes the number of diﬀerent individuals captured at least once
during the n captures, the posterior distribution on N and p = (p1, . . . , pn)
is, for x = (x1, . . . , xn, x+),
π(N, p|x) ∝(N −1)!
(N −x+)!
n

i=1
pα+xi−1
i
(1 −pi)β+N−xi−1
and the marginal distribution of N can be derived as
π(N|x)
∝
(N −1)!
(N −x+)!
n

i=1
B(α + xi, β + N −xi)
∝
(N −1)!
(N −x+)!
n

i=1
Γ(β + N −xi)
Γ(α + β + N) .
Therefore, the posterior distribution π(N|x) can be written in the “explicit”
form
(N−1)!
(N−x+)!
.n
i=1 Γ(β + N −xi)/Γ(α + β + N)
+∞
M=x+
(M−1)!
(M−x+)!
.n
i=1 Γ(β + M −xi)/Γ(α + β + M)
.
(6.1.2)
Actually, because of the ratios in the numerator and denominator, for-
mula (6.1.2) does not require any computation of the gamma function, but
only the use of the recursive formula Γ(x + 1) = xΓ(x). Nonetheless, if n is
large, that is, if many captures have been undertaken, and if, moreover, the
resulting capture sizes xi are very diﬀerent, the computation of the pos-
terior distribution (6.1.2) will be quite diﬃcult. The quantities (6.1.2) can
ﬂuctuate widely and the stopping rule for the computation of the inﬁnite
sum in (6.1.2) must be devised accordingly, lest it ignore the signiﬁcant
terms corresponding to larger values of M. Moreover, the computation of
the sequence (6.1.2) through the following recurrence formula:
π(N + 1|x)
π(N|x)
=
N
N + 1 −x+
n

i=1
β + N −xi
α + β + N ,
although possible, can be quite damaging because the approximation error
increases at each step, especially when the xi’s are very diﬀerent.
The same criticism applies for the computation of the posterior mean
δπ(x) =
+∞
N=x+
N!
(N−x+)!
.n
i=1 Γ(β + N −xi)/Γ(α + β + N)
+∞
M=x+
(M−1)!
(M−x+)!
.n
i=1 Γ(β + M −xi)/Γ(α + β + M)
.
(6.1.3)
Therefore, even though this discrete model seems analytically tractable,
the explicit formulas above can only be used for the simplest examples.

290
Bayesian Calculations
6
Table 6.1.1. Parameters statistics for a lung radiograph model. (Source: Plessis
(1989).)
μ1
μ2
σ1
σ2
p
Average
105.33
188.9
32.3
18.2
0.5
Standard deviation
11.18
7.38
5.62
4.5
0.08
When the numbers of observations and of captures get large, numerical
alternatives become necessary. Furthermore, the appeal of these formulas
disappears for a hierarchical extension, since they cannot be used when a
hyperprior on (α, β) is considered (see George and Robert (1992)).
∥
Example 6.1.5
Consider a sample x1, . . . , xn from
f(x|θ) = pϕ(x; μ1, σ1) + (1 −p)ϕ(x; μ2, σ2),
(6.1.4)
that is, from a mixture of two normal distributions with means μi, variances
σ2
i (i = 1, 2) and weight p (0 < p < 1). We introduced a radiological
application of this model in Example 1.1.6. A study on a ﬁrst set of lung
radiographs showed that images were distributed with parameters varying
according to Table 6.1.5.
As a ﬁrst approximation, given the information provided by Table 6.1.5,
a prior modeling is to use “conjugate” priors on θ = (μ1, σ2
1, p, μ2, σ2
2),
μi|σi ∼N(ξi, σ2
i /ni),
σ2
i ∼IG(νi/2, s2
i /2),
p ∼Be(α, β),
and to derive the hyperparameters ξi, ni, νi, si and (α, β) from Table 6.1.5
by the moments method2. In fact, these distributions are not conjugate in
the sense of Deﬁnition 3.3.1, but the corresponding posterior distribution
is
π(θ|x1, . . . , xn) ∝
n

j=1
{pϕ(xj; μ1, σ1) + (1 −p)ϕ(xj; μ2, σ2)} π(θ) .
(6.1.5)
A straightforward rewriting of (6.1.5) is to represent it as a weighted sum
(that is, a mixture) of conjugate distributions,
π(θ|x1, . . . , xn) =
n

ℓ=0

(kt)
ω(kt)π(θ|(kt)),
(6.1.6)
where ℓdenotes the number of observations attributed to the ﬁrst com-
ponent and where the second sum takes into account every permutation
2 Notice that this modeling diﬀers from an empirical Bayes modeling (Chapter 10). In-
deed, although the resulting prior is only an approximation and the hyperparameters
are estimated by classical means, this distribution is based on previous observations,
which can be considered prior information, not on the observed sample for which the
parameter θ is unknown.

6.1
Implementation diﬃculties
291
(kt) of {1, 2, . . ., n} which gives a diﬀerent partition of {x1, . . . , xn} into
{xk1, . . . , xkℓ} and {xkℓ+1, . . . , xkn}, thus characterizing the ℓobservations
attributed to the ﬁrst component. The posterior weight of a partition (kt)
is (see below for notation)
ω(kt) ∝
Γ(α + ℓ) Γ(β + n −ℓ) Γ([ν1 + ℓ]/2)

s2
1 + ˆs1(kt) +
n1ℓ
n1+ℓ(ξ1 −¯x1(kt))2
(ν1+ℓ)/2
×
Γ([ν2 + n −ℓ]/2)
71
(n1 + ℓ)(n2 + n −ℓ)

s2
2 + ˆs2(kt) + n2(n−ℓ)
n2+n−ℓ(ξ2 −¯x2(kt))2
(ν2+n−ℓ)/2 ,
normalized so that
n

ℓ=0

(kt)
ω(kt) = 1.
For a given permutation (kt), the conditional posterior distribution is
π(θ|(kt)) = N

ξ1(kt),
σ2
1
n1 + ℓ

× IG((ν1 + ℓ)/2, s1(kt)/2)
×N

ξ2(kt),
σ2
2
n2 + n −ℓ

× IG((ν2 + n −ℓ)/2, s2(kt)/2)
×Be(α + ℓ, β + n −ℓ) ,
where
¯x1(kt)
=
1
ℓ
ℓ
t=1 xkt,
ˆs1(kt)
=
ℓ
t=1(xkt −¯x1(kt))2,
¯x2(kt)
=
1
n−ℓ
n
t=ℓ+1 xkt,
ˆs2(kt)
=
n
t=ℓ+1(xkt −¯x2(kt))2
are the usual statistics for the two subsamples induced by the permutation
and
ξ1(kt)
=
n1ξ1 + ℓ¯x1(kt)
n1 + ℓ
,
ξ2(kt) = n2ξ2 + (n −ℓ)¯x2(kt)
n2 + n −ℓ
,
s1(kt)
=
s2
1 + ˆs2
1(kt) +
n1ℓ
n1 + ℓ(ξ1 −¯x1(kt))2,
s2(kt)
=
s2
2 + ˆs2
2(kt) + n2(n −ℓ)
n2 + n −ℓ(ξ2 −¯x2(kt))2,
are the posterior updates of the hyperparameters, conditional upon the
partition (kt).
This decomposition is quite interesting because it shows that, behind a
seemingly inextricable formula, the Bayesian analysis of the mixture distri-
bution (6.1.4) is quite logical. Indeed, the posterior distribution takes into
account every possible partition of the sample, specifying from which com-
ponent each observation originated through the corresponding permutation
(kt). It then attributes a weight ω(kt) to the partition, which can be inter-
preted as the posterior probability of the selected partition, and operates as
if each observation was actually coming from its selected component, since

292
Bayesian Calculations
6
the (conditional) posterior distributions π(θ|(kt)) are identical to the usual
posterior distributions on (μ1, σ1) and (μ2, σ2) resulting from the separate
observation of xk1, . . . , xkℓand xkℓ+1, . . . , xkn. Similar comments apply to
the posterior distribution of p since, conditional upon the partition (kt),
this distribution corresponds to the posterior distribution associated with
the observation of a binomial random variable B(n, p), which is the number
of observations attributed to the ﬁrst component.
The decomposition (6.1.6) provides the following Bayes estimator of θ:
δπ(x1, . . . , xn) =
n

ℓ=0

(kt)
ω(kt)IEπ[θ|x, (kt)],
the weighted sum of the Bayes estimators for each partition. For instance,
the Bayes estimator of μ1 is
μπ
1(x1, . . . , xn) =
n

ℓ=0

(kt)
ω(kt)ξ1(kt).
(6.1.7)
These developments are very satisfactory from a theoretical point of view
because the resulting estimators are easy to interpret and intuitively con-
vincing. Quite naturally, since the origin of each observation in the sample is
unknown, the posterior distribution takes into account the possibility that
this observation was generated by the ﬁrst or second component. How-
ever, the practical calculation of (6.1.7) involves two sums with 2n terms
each, which exactly correspond to the diﬀerent partitions of the sample.
It is therefore impossible to compute a Bayes estimator this way for most
sample sizes.3
∥
Example 6.1.5 is representative of a class of statistical models where
similar problems occur, including most missing data (or latent variable)
models such as mixtures, censored models, classiﬁcation and clustering (see
Robert and Casella (1999, Chapter 9)). They are paradoxical in the sense
that explicit derivations of the Bayes estimators may be formally available,
but are practically useless because of the computing time involved. More-
over, the computational diﬃculty increases with the sample size, leading to
what could be called an information paradox, since the more information
one gets the more diﬃcult it becomes to draw an inference4 about θ. In
such settings, numerical approximation methods are seldom appropriate
and tailored solutions are necessary, as developed in Sections 6.3 and 6.4.
3 For instance, if it takes one second of CPU time to evaluate (6.1.7) for a sample of
size 20, the computation of the corresponding estimator for a sample of size 40 would
require twelve days.
4 Strictly speaking, the computational diﬃculty is always increasing with the sample
size, even in settings where there exists a suﬃcient statistic. However, in the case
of Example 6.1.5, the diﬃculty is increasing so fast (at an exponential rate) that it
completely prevents the actual computation. (Such problems are called NP-hard in
Operation Research.)

6.2
Classical approximation methods
293
6.2 Classical approximation methods
This section brieﬂy covers some classical techniques that can facilitate
Bayesian calculations, while the next section deals with a recent simulation
method that seems particularly adapted to some requirements of Bayesian
computation. A more detailed survey of these techniques is provided in
Robert and Casella (2004, Chapters 2–5), while Berger (2000) and Carlin
and Louis (2000a) survey available Bayesian software.
6.2.1 Numerical integration
Starting with the simple Simpson’s method,5 many approaches have been
devised by applied mathematicians to approximate integrals numerically.
For instance, polynomial quadrature is intended to approximate integrals
involving distributions close to the normal distribution (see Naylor and
Smith (1982), Smith et al. (1985), or Verdinelli and Wasserman (1998) for
a detailed introduction). The basic approximation is given by
 +∞
−∞
e−t2/2f(t) dt ≈
n

i=1
ωif(ti),
where
ωi =
2n−1n! √n
n2[Hn−1(ti)]2
and ti is the ith zero of the nth Hermite polynomial, Hn(t).
Other related integral approximations are also available, based upon dif-
ferent classical orthogonal bases (see Abramowitz and Stegun (1964)), or
the wavelets (see Note 1.8.2 and M¨uller and Vidakovic (1999, Chapter 1)),
but these methods usually require regularity assumptions on the function
f, as well as preliminary studies in order to determine what basis is the
most appropriate basis, and how accurate the approximation is. For in-
stance, transformations of the model may be necessary to apply the above
Hermite approximation (see Naylor and Smith (1982) and Smith and Hills
(1992)); Morris (1982) (see also Diaconis and Zabell (1991)) shows how dis-
tributions from the quadratic variance exponential families (Exercises 3.24
and 10.33) can be associated with particular orthogonal bases (Exercise
6.15).
However, it seems that, no matter the numerical integration method
used, its accuracy dramatically decreases as the dimension of Θ increases.
More speciﬁcally, the error associated with numerical methods evolves as
a power of the dimension of Θ. In fact, an empirical rule of thumb is that
most standard methods should not be used for integration in dimensions
larger than 4, although they keep improving over the years. But the size
of the part of the space irrelevant for the computation of a given integral
5 See Stigler (1986) for a closer connection between Simpson (1710–1761) and Bayesian
Statistics.

294
Bayesian Calculations
6
grows considerably with the dimension of the space. This problem is often
called the curse of dimensionality; see Robert and Casella (2004, Chapter
3) for details.
6.2.2 Monte Carlo methods
In a statistical problem, the approximation of the integral

Θ
g(θ)f(x|θ)π(θ) dθ,
(6.2.1)
should take advantage of the special nature of (6.2.1), namely, the fact
that π is a probability density (assuming it is a proper prior) or, instead,
that f(x|θ)π(θ) is proportional to a density. A natural way to do so is the
Monte Carlo method, introduced by Metropolis and Ulam (1949) and Von
Neuman (1951). For instance, if it is possible to generate random variables
θ1, . . . , θm from π(θ), the average
1
m
m

i=1
g(θi)f(x|θi)
(6.2.2)
converges (almost surely) to (6.2.1) when m goes to +∞, according to the
Law of Large Numbers. Similarly, if an i.i.d. sample of θi’s from π(θ|x) can
be produced, the average
1
m
m

i=1
g(θi)
(6.2.3)
converges to

Θ g(θ)f(x|θ)π(θ) dθ

Θ f(x|θ)π(θ) dθ
.
In addition, if the posterior variance var(g(θ)|x) is ﬁnite, the Central Limit
Theorem applies to the average (6.2.3), which is then asymptotically normal
with variance var(g(θ)|x)/m. Conﬁdence regions can then be built from
this normal approximation and, most importantly, the magnitude of the
error remains of order 1/√m, whatever the dimension of the problem, in
opposition with numerical methods.
The implementation of the method requires the production of the θi’s by
computer, using a deterministic pseudo-random generator to mimic gener-
ation from π(θ) or π(θ|x) by ﬁrst replicating i.i.d. sampling from a uniform
U([0, 1]) distribution (see Note 6.6.1) and then transforming the uniforms
into the variables of interest (see Robert and Casella (2004, Chapter 2)).6
Standard statistical techniques can also be used to ascertain the error in
the approximation of (6.2.1) by the average (6.2.2).
6 It is not surprising that Monte Carlo methods emerged exactly at the time of the ﬁrst
computer. They are simply not operational without a computer, and correspond to
some of the ﬁrst computer programs ever written.

6.2
Classical approximation methods
295
The Monte Carlo method actually applies in a much wider generality
than the above simulation from π. For instance, because (6.2.1) can be
represented in many ways, there is no need to simulate from the distribu-
tions π(·|x) or π to get a good approximation of (6.2.1). Indeed, if h is a
probability density with supp(h) including the support of g(θ)f(x|θ)π(θ),
the integral (6.2.1) can also be represented as an expectation against h,
namely
 g(θ)f(x|θ)π(θ)
h(θ)
h(θ) dθ.
This representation leads to the Monte Carlo method with importance func-
tion h: generate θ1, . . . , θm according to h and approximate (6.2.1) through
1
m
m

i=1
g(θi)ωi(θi),
with the weights ω(θi) = f(x|θi)π(θi)/h(θi). Again, by the Law of Large
Numbers, this approximation almost surely converges to (6.2.1). And an
approximation to IEπ[g(θ)|x] is given by
m
i=1 g(θi)ω(θi)
m
i=1 ω(θi)
.
(6.2.4)
since the numerator and denominator converge to

Θ
g(θ)f(x|θ)π(θ) dθ
and

Θ
f(x|θ)π(θ) dθ,
respectively, if supp(h) includes supp(f(x|·)π). Notice that the ratio (6.2.4)
does not depend on the normalizing constants in either h(θ), f(x|θ) or π(θ).
The approximation (6.2.4) can therefore be used in settings when some of
these normalizing constants are unknown.
Although (6.2.4) theoretically converges to IEπ[g(θ)|x] for all functions
h satisfying the condition on its support (Exercise 6.8), the choice of the
importance function is crucial. First, simulation according to h must be
easily implemented, requiring a fast and reliable pseudo-random generator.
(See Exercises 6.9–6.10, Devroye (1985), Fishman (1996), Gentle (1998), or
Robert and Casella (2004, Chapter 2). Moreover, the function h(θ) must be
close enough to g(θ)π(θ|x), in order to reduce the variability of (6.2.4) as
much as possible (Exercise 6.12); otherwise, most of the weights ω(θi) will
be quite small and a few will be overly inﬂuential. In fact, if IEh[g2(θ)ω2(θ)]
is not ﬁnite, the variance of the estimator (6.2.4) is inﬁnite. Obviously, the
dependence on g of the importance function h can be avoided by propos-
ing generic choices such as the posterior distribution π(θ|x) (which is not
necessarily the best choice, as shown by Exercises 6.11 and 6.12).
Example 6.2.1 (Example 6.1.2 continued) The posterior distribution
of η = ||θ||2 is well known, since π(η|x) is a noncentral chi-squared distri-
bution χ2
p(λ) rescaled by σ2τ 2/(σ2 + τ 2). Simulating a sample η1, . . . , ηm

296
Bayesian Calculations
6
from π(η|x) is straightforward: simulate
ξ1, . . . , ξn ∼N(
√
λ, 1),
ζ1, . . . , ζn ∼G
p −1
2
, 1
2

and take ηi = σ2τ 2(ξ2
i + ζi)/(σ2 + τ 2) (i = 1, . . . , n). We can then approx-
imate (6.1.3) by
ˆδπ(x) =
m
i=1 ηi/(2ηi + p)
m
i=1 1/(2ηi + p) .
(6.2.5)
Moreover, the variance of (6.2.5) controls the precision of the approximation
(and the choice of m).
∥
When the posterior distribution is not available, another simple choice of
importance function is the prior distribution π. It is obviously interesting
when π is not necessarily explicit, but easy to simulate, for instance, in
hierarchical models where both levels are proper. The same call for caution
still applies, though, as π must be close enough to π(θ|x) and the variance of
the estimator (6.2.4) ﬁnite. (Notice that this ﬁniteness condition is usually
satisﬁed since π(θ) often has fatter tails than π(θ|x).)
Example 6.2.2 (Example 6.1.1 continued) Because π(θ) is the nor-
mal distribution N(μ, σ2), it is possible to simulate a normal sample θ1, . . . ,
θM and to approximate the Bayes estimator by
ˆδπ(x1, . . . , xn) =
M
t=1 θt
.n
i=1[1 + (xi −θt)2]−1
M
t=1
.n
i=1[1 + (xi −θt)2]−1 .
(6.2.6)
In the case where the xi’s are all far from μ, this choice may be detrimental
since both the denominator and the weights of the θt’s in the numerator are
small for most θt’s, and the approximation ˆδπ is therefore quite unstable.
Consider Figure 6.2.2, which represents the result of 500 parallel estima-
tions (6.2.6) based on M = 1000 simulations each as the 90% central range
of the ˆδπ’s minus the overall mean. The variation of δπ increases rapidly
between μ = 3 and μ = 4. This shows that, when μ > 3, small changes in
the simulated θt’s can induce drastic changes in ˆδπ.
∥
Example 6.2.3
Consider the model
x ∼Np(θ, Ip) ,
θ|c ∼U{||θ||2=c} ,
and c ∼G(α, β) .
(The justiﬁcation for this setting will be made clear in Example 10.3.6.)
Although
π(θ|x) =
 +∞
0
π1(θ|x, c)π2(c|x) dc
leads to an explicit posterior distribution and an explicit Bayes estimator
(see Example 10.3.6), it might be more interesting to generate c1, . . . , cm
according to G(α, β), then the θi’s according to U{||θ||2=ci} (1 ≤i ≤m) and

6.2
Classical approximation methods
297
mu
variation
0
2
4
6
8
10
-0.5
0.0
0.5
Figure 6.2.1. 90% range of variation of the approximation (6.2.6) as μ varies, for
n = 10 observations from a Cauchy C(0, 1) distribution and M = 1000 Monte
Carlo simulations of θ from a N(μ, 1) distribution.
to approximate the posterior mean by
ˆδπ(x) =
m
i=1 θi exp{−||x −θi||2/2}
m
i=1 exp{−||x −θi||2/2} ,
since this alternative avoids the computation of conﬂuent hypergeometric
functions.
∥
When the likelihood ℓ(θ|x) can be normalized into a density, a possible
choice of importance function is h(θ) ∝ℓ(θ|x). This choice makes sense
when π(θ|x) is almost proportional to the likelihood—as it is for large
sample sizes, or for almost constant prior distributions. For instance, this
may occur in exponential settings since, if
f(x|θ) ∝eθ.x−ψ(θ),
a sample θ1, . . . , θm from
h(θ) ∝eθ.x−ψ(θ)
can easily be obtained in general (see Exercise 6.20 for a limitation to this
approach).
A ﬁnal remark about the choice of the importance function is that there
is generally a trade-oﬀbetween preliminary studies leading to a “good”
h and fast algorithms. For instance, when h is chosen because it makes
the simulation of the θi’s easier, attention should be paid to the tails of
h so that they are heavier than the tails of π(θ|x), in order to avoid slow
convergence and inﬁnite variances. On the other hand, if h is specially
tuned for the computation of a speciﬁc integral (Exercise 6.12), it may not
work so well for another integral, despite the fact that, in principle, the
same sample of θi’s can be used for the computation of arbitrary integrals.
However, barring these potential diﬃculties, importance sampling methods

298
Bayesian Calculations
6
constitute a very general tool, which often ends up being competitive with
Monte Carlo Markov chain techniques (Section 6.3), as shown for instance
by the particle ﬁlter method (see Doucet et al. (2001) and Robert and
Casella (2004)).
Compared with numerical integration methods, Monte Carlo methods
have the advantage that, once the sample θ1, . . . , θn is generated, it can be
used repeatedly for all inferential purposes, including the derivation of the
Bayes rules from the approximated posterior loss
ˆL(π, d|x) = 1
m
m

i=1
L(θi, d|x).
However, if the dimension of the problem is small and if the functions
to be integrated are fairly regular, numerical integration methods tend to
yield smaller errors with better convergence controls. Additional references
and more detailed discussions about Monte Carlo methods, including the
improved techniques of antithetic and control variates, and of their appli-
cation to Bayesian Statistics, can be found in Robert and Casella (1999,
2004), Chen, Shao and Ibrahim (2000), and Marin and Robert (2007).
6.2.3 Laplace analytic approximation
When the function to integrate in (6.2.1) is regular enough, there exists an
analytic—although asymptotic—alternative to Monte Carlo simulations.
This method was introduced by Laplace and is thus called Laplace approx-
imation. Consider the posterior expectation of interest
IEπ[g(θ)|x] =

Θ g(θ)f(x|θ)π(θ) dθ

Θ f(x|θ)π(θ) dθ
.
This ratio of integrals can be written as
IEπ[g(θ)|x] =

Θ bN(θ) exp{−nhN(θ)} dθ

Θ bD(θ) exp{−nhD(θ)} dθ ,
(6.2.7)
where the dependence on x is suppressed for simplicity’s sake and where
n is usually the sample size (although it may sometimes correspond to
the inverse prior variance, as in Robert (1993a), or in Example 6.2.6).
When hN(θ) = hD(θ), IEπ[g(θ)|x] is said to be written in standard form;
when bN(θ) = bD(θ), the posterior expectation (6.2.7) is written in fully
exponential form, in the words of Tierney and Kadane (1986). Given a
function h with a single minimum ˆθ, the Laplace expansion of a general
integral is given by

b(θ)e−nh(θ)dθ
=
√
2πσe−nˆh

ˆb + 1
2n

σ2ˆb′′ −σ4ˆb′ˆh′′′
+ 5
12
ˆb(ˆh′′′)2σ6 −1
4
ˆbˆh(4)σ4

+ O(n−2),

6.2
Classical approximation methods
299
where ˆb, ˆh, etc., denote the values of b, h, and of their derivatives for θ = ˆθ,
and σ2 = [h′′(ˆθ)]−1 (see Olver (1974) and Schervish (1995)). This second-
order approximation only requires computation of the ﬁrst two derivatives
of g, as opposed to a similar approach proposed by Lindley (1980). Assu-
ming, in addition, that hN and hD satisfy ˆhN −ˆhD = O(n−1), . . . , ˆh(4)
N −
ˆh(4)
D
= O(n−1) (as is obviously the case for the standard form), Laplace
expansion leads to the following approximation of IEπ[g(θ)|x] (with ˆbD =
bD(ˆθD), ˆbN = bN(ˆθN), and so on):
Lemma 6.2.4 If ˆbD ̸= 0,

Θ bN(θ) exp{−nhN(θ)} dθ

Θ bD(θ) exp{−nhD(θ)} dθ = σN
σD
e−n(ˆhN−ˆhD)
ˆbN
ˆbD
+
σ2
D
2nˆb2
D

ˆbDˆb′′
N
−ˆbNˆb′′
D −σ2
Dˆh′′′
D(ˆbDˆb′
N −ˆbNˆb′
D)
 0
+ O(n−2).
A proof of this result is given in Tierney et al. (1989) (see also Exercise
6.14). Lemma 6.2.4 thus implies the following development for the two
forms of the ratio (6.2.7):
Corollary 6.2.5
When IEπ[g(θ)|x] is written in standard form,
IEπ[g(θ)|x] = ˆg + σ2
Dˆb′
Dˆg′
nˆbD
+ σ2
Dˆg′′
2n
−σ4
Dˆh′′′ˆg′
2n
+ O(n−2).
(6.2.8)
For the fully exponential form, if g is positive and g(ˆθD) is uniformly (in
n) bounded away from 0,
IEπ[g(θ)|x] =
ˆbN
ˆbD
σ2
N
σ2
D
e−n(ˆhn−ˆhD) + O(n−2).
(6.2.9)
Proof.
For the standard form, hN = hD; therefore, bN = gbD, ˆθD = ˆθN.
Thus,
ˆbDˆb′
N −ˆbNˆb′
D
ˆb2
D
=
bN
bD
′
θ=ˆθD
= ˆg′
and
ˆbDˆb′′
N −ˆbNˆb′′
D
ˆb2
D
= ˆg′′ + 2
ˆb′
D
ˆbD
ˆg′.
The result then follows from Lemma 6.2.4.
In the fully exponential case, hN = hD−(1/n) log(g). Because we assume
that g(ˆθD) ≥c > 0 for every n, ˆθN −ˆθD = O(n−1). Because bD = bN, this
implies ˆb(i)
N −ˆb(i)
D = O(n−1) (i = 0, 1, 2). Additional terms in Lemma 6.2.4
can therefore be ignored.
22
Corollary 6.2.5 clearly points out the advantage of the fully exponential
interpretation of (6.2.7), since it avoids computation of the ﬁrst and second

300
Bayesian Calculations
6
derivatives, ˆg′ and ˆg′′, appearing in (6.2.8). Notice that (6.2.9) can also be
written
IEπ[g(θ)|x] = σ2
N
σ2
D
g(ˆθN)f(x|ˆθN)π(ˆθN)
f(x|ˆθD)π(ˆθD)
+ O(n−2).
The assumption on g, namely, that g is positive and bounded away from
0 in ˆθD, is however quite restrictive. Moreover, the usual decomposition
g = g+ −g−does not work in this setting. Tierney et al. (1989) overcome
this drawback by ﬁrst evaluating the moment generating function of g(θ),
M(s) = IEπ[exp{sg(θ)}|x],
obviously positive, by ˆ
M(s) through (6.2.9). They derived IEπ[g(θ)|x] as
IEπ[g(θ)|x] = d
ds(log ˆ
M(s))

s=0 + O(n−2).
They also establish the rather surprising result that this approach provides
the standard development (6.2.8) without requiring an evaluation of the
ﬁrst and second derivatives of g (see Exercise 6.15).
Example 6.2.6 (Tierney et al. (1989)) Let π(θ|x) be a Be(α, β) distribu-
tion, the posterior expectation of θ is then
δπ(x) =
α
α + β .
This exact computation can be compared with the approximations (6.2.8),
δπ(x) = α2 + αβ + 2 −4α
(α + β −2)2
+ O((α + β)−2),
and (6.2.9),
δπ(x) =
α
α + β −1

α
α −1
α−0.5 α + β −2
α + β −1
α+β−0.5
+ O((α + β)−2).
Denoting p = α/(α + β) and n = α + β, the approximation error is
ΔS = 21 −2p
n2
+ O(n−3)
in the standard case, and
ΔE = 21 −13p2
12pn2
+ O(n−3)
in the fully exponential case. The second development is then better for
the median values of p.
∥
The reader is referred to Leonard (1982), Tierney and Kadane (1986),
Tierney et al. (1989), and Kass and Steﬀey (1989) for additional results
and comments. A reservation made in Smith et al. (1985) about Laplace
approximation is that it is only justiﬁed asymptotically; the speciﬁc veriﬁca-
tions conducted in the diﬀerent papers cannot provide a global justiﬁcation

6.3
Markov chain Monte Carlo methods
301
of the method, even though it seems to perform quite well in most cases.
Other criticisms of this approach are that
(1) analytical methods always imply delicate preliminary studies about the
regularity of the integrated function that are not necessarily feasible:;
(2) the posterior distribution should be similar enough to the normal dis-
tribution (for which Laplace approximation is exact); and
(3) such methods cannot be used in settings like those of Example 6.1.5,
where the computation of the maximum likelihood estimator is quite
diﬃcult.
Extensions of Laplace methods to saddle point approximations are reviewed
in Kass (1989) (see also Rousseau (1997, 2000, 2002)).
6.3 Markov chain Monte Carlo methods
In this section we consider a more general Monte Carlo method that ap-
proximates the generation of random variables from a posterior distribution
π(θ|x) when this distribution cannot be directly simulated. Its advantage
over the classical Monte Carlo methods described in Section 6.2.2 is that it
does not require the precise construction of an importance function, while
taking into account the characteristics of π(θ|x). This extension, called
Markov chain Monte Carlo (and abbreviated as MCMC), has almost un-
limited applicability, even though its performance varies widely, depending
on the complexity of the problem. It derives its name from the idea that,
to produce acceptable approximations to integrals and to other functionals
depending on a distribution of interest, it is enough to generate a Markov
chain (θ(m))m with limiting distribution the distribution of interest.7 This
idea of using the limiting behavior of a Markov chain came almost as early
as the original Monte Carlo technique, at least in the particle Physics lit-
erature (Metropolis et al. (1953)), but it requires a computational power
that was not available in those early days.
After a brief discussion on the appeal of using a Markov chain in simula-
tion (Section 6.3.1), we introduce the two major techniques devised to cre-
ate Markov chains with a given distribution, namely, Metropolis–Hastings
algorithms (Section 6.3.2) and Gibbs sampling (Sections 6.3.3–6.3.6). We
refer the reader to Gilks et al. (1996) and Robert and Casella (2004) for
broader perspectives on this topic.
7 This section minimizes the recourse to Markov chain theory, although some notions
like ergodicity cannot be skipped. We refer the reader to Meyn and Tweedie (1993)
for a deep and pedagogical introduction to this topic. See also Robert and Casella
(2004, Chapter 6) for a more cursory treatment of the notions necessary for the
understanding of MCMC methods.

302
Bayesian Calculations
6
6.3.1 MCMC in practice
The apparent paradox with using Markov chains for simulation purposes is
that we seem to be calling twice for an asymptotic argument: ﬁrst, the chain
must converge to its stationary distribution; second, empirical averages
such as (6.2.2) must converge to the corresponding expectation IEπ[g(θ)|x].
We now describe why this is not so, thanks to the Ergodic Theorem.
By their very nature, if the Markov chains (θ(m))m produced by MCMC
algorithms are irreducible, that is, if they are guaranteed to visit any set
A such that π(A|x) > 0, then these chains are positive recurrent with
stationary distribution π(θ|x). That is, the average number of visits to
an arbitrary set A with positive measure is inﬁnite. These Markov chains
are also ergodic, which means that the distribution of θ(m) converges to
π(·|x) for almost every starting value θ(0), that is, the inﬂuence of the
starting value vanishes. (Under fairly general conditions, the chains are even
Harris-recurrent, which implies that the “almost” in the above condition
disappears.)
Therefore, for k large enough, the resulting θ(k) is approximately dis-
tributed from π(θ|x), no matter what the starting value θ(0) is. The problem
in practice is then to determine what a “large” k means, since it governs
the number of simulations to run: is it 200 or 1010? The speed of conver-
gence, that is, the type of decrease in the diﬀerence (distance) between the
distribution of θ(k) and its limit, brings an answer to this problem, but so
far it has been mainly studied from a theoretical point of view. Moreover,
this rate of convergence often depends on the starting point (otherwise, the
chain is uniformly ergodic) and a given k does not provide the same qual-
ity of approximation for diﬀerent values of θ(0). There are thus practical
hindrances in the use of Markov chains for simulation since we often ignore
whether the chain has been run long enough. But, as detailed in Robert
and Casella (2004, Chapter 12), there now are diagnostic tests and a corre-
sponding software, CODA (see Note 6.6.2), that provide diﬀerent indicators
on the stationarity of the chain, and thus reduce this diﬃculty.
Once θ1 = θ(k) is obtained, a na¨ıve way to build an i.i.d. sample θ1, . . . , θm
from π(θ|x) is to use the same algorithm with another initial value θ(0)
2
and
another sequence of k Markov transition moves to get θ2, and so on un-
til θm. As shown above, the speed of convergence often depends on the
starting value and it is thus preferable (in terms of convergence) to take
the current θ(k) as the new starting value, even though this introduces
dependence between the θi’s. However, independence is not crucial if we
are mainly interested in functionals of π(θ|x), since the Ergodic Theorem
implies that the average
1
K
K

k=1
g(θ(k))
converges to IEπ[g(θ)|x] (as long as IEπ[|g(θ)||x] is ﬁnite) when K goes

6.3
Markov chain Monte Carlo methods
303
to inﬁnity (see Meyn and Tweedie (1993)). Therefore, the inﬂuence of the
starting value also vanishes in the average (hence the ergodicity). Moreover,
this property is similarly satisﬁed by any subsequence of (θ(k)).
The Ergodic Theorem thus solves the paradox of the two asymptotics
mentioned at the beginning of this paragraph, since it extends the Law of
Large Numbers to dependent sequences of random variables and removes
the need to ﬁrst produce an i.i.d. sample, which would, moreover, be only
approximate if we used the method proposed above. Indeed, as already
noted in Geyer (1992), the available Markov chain theory does not indicate
when stationarity is attained, since, from a mathematical point of view,
this is only an asymptotic property of the chain. Therefore, it is better to
consider a single sequence (θ(k)), as each simulation step brings us closer
(in probability) to a realization from the stationary distribution, π(θ|x).
In addition, multiple-starts simulation produces a considerable waste by
rejecting most of the simulated values. However, the call to multiple chains
is quite useful in studying convergence of a Markov chain (and thereby
determining the proper k) and thus frequently appears in monitoring tech-
niques, as in the within-between method of Gelman and Rubin (1992) (see
Robert and Casella (2004, Section 12.3.4)).
When required, quasi-independence can be achieved by batch sampling,
that is, by keeping only one member of the chain out of t iterations for the
eﬀective simulated sample, with t = 5 or t = 10 say. Raftery and Lewis
(1992) propose a more advanced determination of the batch size t, which
is chain-induced and based on a binarization of the chain. (See Robert and
Casella (2004, Section 12.4.1) for an appraisal of this method.)
6.3.2 Metropolis–Hastings algorithms
Once the principle of using a Markov chain with stationary distribution
π—instead of i.i.d. variables exactly distributed from π—to approximate
quantities like (6.2.1) is accepted, the implementation of this principle re-
quires the construction of a generation mechanism to produce such Markov
chains. Surprisingly, an almost universal algorithm satisfying this constraint
does exist: it has been constructed by Metropolis et al. (1953), originally for
mechanical physics, and generalized by Hastings (1970) in a more statisti-
cal setting. It actually applies to a wide variety of problems, since its main
restriction is that the distribution of interest be known up to a constant,
but we will see later that this constraint can be lifted in many ways.
In its modern version, the Metropolis–Hastings algorithm can be de-
scribed as follows. Given a density π(θ), known up to a normalizing factor,
and a conditional density q(θ′|θ), the algorithm generates the chain (θ(m))m
by:
(i)
Start with an arbitrary initial value θ(0)
(ii)
Update from θ(m) to θ(m+1) (m = 1, 2, . . .) by

304
Bayesian Calculations
6
(a) Generate ξ ∼q(ξ|θ(m))
(b) Deﬁne
ϱ =
π(ξ) q(θ(m)|ξ)
π(θ(m)) q(ξ|θ(m)) ∧1
(c) Take
θ(m+1) =
 ξ
with probability ϱ,
θ(m)
otherwise.
The distribution with density π(θ) is often called the target or objective
distribution, whereas the distribution with density q(·|θ) is the proposal dis-
tribution. An astounding thing about this algorithm is the inﬁnite number
of proposal distributions that yield a Markov chain that converges to the
distribution of interest.
Theorem 6.3.1 If the chain (θ(m))m is irreducible, that is, such that, for
any subset A with π(A) > 0, there exists M such that Pθ(0)(θ(M) ∈A) > 0,
then π is the stationary distribution of the chain. If, in addition, the chain
is aperiodic, it is also ergodic with limiting distribution π, for almost every
initial value θ(0), that is,
lim
m→∞sup
A
Pθ(0)(θ(m) ∈A) −π(A)
 = 0
(a.s.)
The property at the core of this result is the detailed balance condition,
that is, the fact that the transition kernel of the Markov chain associated
with the above algorithm, K(θ′|θ) say, satisﬁes
π(θ)K(θ′|θ) = π(θ′)K(θ|θ′) .
(6.3.1)
When integrating both sides of (6.3.1) against θ, the rhs provides π(θ′)
because K(θ|θ′) is a (conditional) density in θ, while the lhs gives the
density of the Markov chain after one step, when θ(0) ∼π. Therefore, the
distribution π is indeed stationary for the transition kernel K(θ′|θ). (See
Exercise 6.17 and Robert and Casella (2004, Section 7.3) for more details.)
If we write down the kernel of the Metropolis–Hastings algorithm as
K(θ′|θ) = ϱ(θ, θ′)q(θ′|θ) +

[1 −ϱ(θ, ξ)]q(ξ|θ)dξ δθ(θ′) ,
where δ denotes the Dirac mass, it is then straightforward to verify that
(6.3.1) holds.
The irreducibility condition in Theorem 6.3.1 is obviously necessary for
the chain to explore the support of π. Suﬃcient conditions for irreducibility
to hold are, for instance, that the support of q(·|θ) contain the support of
π for every θ or, more generally, thatq(·|θ) be positive in a neighborhood
of θ of ﬁxed radius (see Robert and Casella (2004, Lemma 7.6)).
While Theorem 6.3.1 gives a formal condition for the chain to converge,
which covers an immense class of proposal distributions, practical selection

6.3
Markov chain Monte Carlo methods
305
of this distribution is much more delicate because a poor overlap between
the support of π and q(·|θ) may considerably slow convergence.
Example 6.3.2
Weibull distributions are used extensively in reliability
and other engineering applications, partly for their ability to describe dif-
ferent hazard rate behaviors, and partly for historic reasons. Because they
do not belong to any exponential family, being of the form
f(x) ∝αηxα−1e−xαη,
(6.3.2)
they cannot lead to explicit posterior distributions on the parameters α
and η. For θ = (α, η), consider the prior
π(θ) ∝e−αηβ−1e−ξη
and observations x1, . . . , xn from (6.3.2). A Metropolis–Hastings algorithm
for the simulation of π(θ|x1, . . . , xn) can be based upon the conditional
distribution
q(θ′|θ) = 1
αη exp

−α′
α −η′
η

,
that is, on two independent exponential distributions with means α and η.
The resulting acceptance probability is then
ϱ=1∧
η′
η
n+βα′
α
n−1! n

i=1
xi
"α′−α n

i=1
eηxα
i −η′xα′
i eα−α′+η−η′+α′/α+η′/η ,
if (α′, η′) is the simulated value and (α, η) is the current value of the pa-
rameters.
∥
The most common choice for q, starting with Hastings (1970), is the
random-walk proposal, where q(θ′|θ) is of the form f(||θ′−θ||). The proposed
value ξ in the Metropolis–Hastings algorithm is thus of the form
ξ = θ(m) + ε ,
with ε distributed as a symmetric random variable. The natural idea behind
this proposal is to perturb the current value of the chain at random, while
staying in a neighborhood of this value, and then see if the new value ξ is
likely for the distribution of interest. For this random-walk proposal, the
Metropolis–Hastings acceptance ratio is
ϱ =
π(ξ)
π(θ(m)) ∧1.
The chain (θ(m))m will thus stay longer in a given point ξ if the correspond-
ing posterior value π(ξ) is higher and, conversely, will never visit points ξ
such that π(ξ) = 0. Standard choices for the proposal q are uniform, normal
or Cauchy distributions. (Notice that Example 6.3.2 is a particular case of
the random-walk Metropolis–Hastings algorithm, since the proposal is a
random walk on (log α, log η).)

306
Bayesian Calculations
6
−4
−2
0
2
4
−4
−2
0
2
4
x
y
Figure 6.3.1. Path of the Markov chain (θ(m))m for the posterior distribution
π(θ|x) of Example 6.3.3 and repulsive points μj indicated by crosses for x = 0
and p = 15 (5000 iterations).
Example 6.3.3 For θ, x ∈IR2, consider the modiﬁed normal distribution
π(θ|x) ∝exp{−||θ −x||2/2}
p

i=1
exp

−1
||θ −μi||2

,
where the μi’s are repulsive points, that is, unlikely (or prohibited) values of
θ. A random-walk Metropolis–Hastings algorithm based on a N2(0, 0.2 I2)
perturbation leads to the result described in Figure 6.3.2 for x = 0 and
p = 15. The μj’s, which are represented by crosses, are correctly avoided
by the Markov chain, which also recover the shape of the normal p.d.f., as
shown by the picture on the cover of this book.
∥
This algorithm is clearly widely applicable and, moreover, has limited
tune-up requirements, since the distribution of the perturbation can be cho-
sen almost independently of the true density π. (In fact, this distribution
depends on a scale factor that should be calibrated by the average accep-
tance rate of the algorithm.8 See Robert and Casella (2004, Section 7.6.1
and Note 7.8.4).) While it cannot enjoy convergence properties stronger
than geometric convergence because of the heavy tails of the proposal (see
Mengersen and Tweedie (1996)), the random-walk Metropolis–Hastings al-
gorithm still appears to be the “gold standard” of MCMC techniques.
Another class of proposals, more akin to standard Monte Carlo tech-
niques, are the independent proposals, that is, densities q(·|θ) that do not
depend on θ,
q(θ′|θ) = h(θ′) .
8 The scale factor in Example 6.3.3 was deliberately chosen for being too small, towards
a better representation of the way the Markov chain avoids the repulsive points μi.

6.3
Markov chain Monte Carlo methods
307
(Because there is a positive probability of rejecting the proposal, the output
of the algorithm still is a Markov chain.) Although their theoretical prop-
erties are often better than those of random-walk Metropolis–Hastings al-
gorithms (see Mengersen and Tweedie (1996)), these methods have a more
limited applicability because the proposal has to ﬁt the target distribution
π in some sense. This proposal may sometimes be the prior distribution,
or it may be based on an asymptotic expansion of the distribution π, for
instance a saddlepoint approximation (Robert and Casella (2004, Example
7.12)), or on an approximative accept-reject algorithm as in the ARMS al-
gorithm of Gilks et al. (1995) (see also Exercise 6.10). (Notice the similarity
with the importance sampling method of Section 6.2.2: the choice of the
proposal distribution h is crucial for the practical implementation of the
method, although being almost irrelevant theoretically.)
6.3.3 The Gibbs sampler
The Metropolis–Hastings technique presented in the previous section is ap-
pealing for its universality, but, in contrast, the lack of connection between
the proposal q and the target distribution π may be detrimental to the
convergence properties of the method and, in practice, may easily prevent
convergence if the probability of reaching far-away parts of the distribution
π is too small. Using a diﬀerent perspective, the Gibbs sampling approach
is actually based on the distribution π. This method takes its name from
Gibbs random ﬁelds, where it was used for the ﬁrst time by Geman and Ge-
man (1984). (See Robert and Casella (2004, Note 10.6.1) for a brief account
of the early history of the Gibbs sampler.)
From a general point of view, the Gibbs sampler takes advantage of
hierarchical structures, i.e. when a Bayesian model can be written as
π(θ|x) =

π1(θ|x, λ)π2(λ|x) dλ.
(6.3.3)
The idea is then to simulate from the joint distribution π1(θ|x, λ)π2(λ|x)
to recover π(θ|x) as the marginal: when both distributions π1(θ|x, λ) and
π2(λ|x) are known and can be simulated, the generation of θ from π(θ|x)
is equivalent to the generation of λ from π2(λ|x), and of θ from π1(θ|x, λ).
Example 6.3.4 (Casella and George (1992)) Consider (θ, λ) ∈IN × [0, 1]
and
π(θ, λ|x) ∝
n
θ

λθ+α−1(1 −λ)n−θ+β−1,
where the parameters α and β actually depend on x. This model can be
written in the hierarchical form (6.3.3), with π1(θ|x, λ) a binomial distri-
bution, B(n, λ), and π2(λ|x) a beta distribution, Be(α, β). The marginal
distribution of θ is then
π(θ|x) =
n
θ
B(α + θ, β + n −θ)
B(α, β)
,

308
Bayesian Calculations
6
that is, a beta-binomial distribution. This distribution is not particularly
easy to work with. For instance, the computation of IE[θ/(θ+1)|x], or of the
posterior distribution of η = exp(−θ2), cannot be done explicitly and may
involve intricate computations, even from a numerical point of view, when
α, β, and n are large. Therefore, depending on the inferential problem,
it may be more advantageous to simulate (λ(1), θ(1)), . . . , (λ(m), θ(m)) with
λ(i) ∼Be(α, β) and θ(i) ∼B(n, λ(i)); for instance, IE[θ/(θ + 1)|x] can be
then approximated with
1
m
m

i=1
θ(i)7
(θ(i) + 1) .
∥
When, in contrast with Example 6.3.4, the marginal distribution π2(λ|x)
is not always available (in analytical or algorithmic forms), the classical
Monte Carlo method cannot be implemented. It is more often the case
that both conditional posterior distributions, π1(θ|x, λ) and π2(λ|x, θ), can
be simulated. Since they are suﬃciently informative about the joint distri-
bution, π(θ, λ|x), and since π(θ, λ|x) can be recovered from the conditional
densities (see Exercises 6.23 and 6.24), it seems conceptually possible to
base the simulation of π(θ|x) on these conditional distributions only.
Example 6.3.5 (Example 6.1.4 continued) For the temporal capture-
recapture model, the two conditional posterior distributions are (1 ≤i ≤n)
pi|x, N
∼
Be(α + xi, β + N −xi)
N −x+|x, p
∼
Neg(x+, ϱ),
with
ϱ = 1 −
n

i=1
(1 −pi).
On the contrary, the posterior marginal distribution π2(p|x) cannot be
obtained in a closed form or directly simulated.
∥
A ﬁrst Gibbs sampling technique called data augmentation was intro-
duced by Tanner and Wong (1987) to take advantage of the conditional
distributions according to the following iterative algorithm
Initialization: Start with an arbitrary value λ(0).
Iteration t: Given λ(t−1), generate
a.
θ(t) according to π1(θ|x, λ(t−1))
b.
λ(t) according to π2(λ|x, θ(t)).
It is then straightforward to show that π(θ, λ|x) is a stationary distribu-
tion for the above transition: if (θ(i−1), λ(i−1)) is distributed from the joint

6.3
Markov chain Monte Carlo methods
309
distribution, λ(i−1) is distributed from the marginal distribution π2(λ|x)
and, therefore, (θ(i), λ(i−1)) is still distributed from the joint distribution.
(Actually, this requires the support of the joint distribution to be equal
to the Cartesian product of the supports of π1 and π2. See Robert and
Casella (2004, Example 10.7 and Figure 10.1) for a counter-example.) The
same reasoning applies to the second step in the algorithm and the chain
(θ(t), λ(t)) is ergodic with limiting distribution π. In addition, the dual
structure of the above algorithm leads to good convergence properties, as
pinpointed in Diebolt and Robert (1994):
Lemma 6.3.6
If π1(θ|x, λ) > 0 on Θ (π2(λ|x, θ) > 0 on Λ, resp.), both
sequences (θ(m)) and (λ(m)) are ergodic Markov chains with invariant dis-
tributions π(θ|x) and π(λ|x).
Moreover, it can be shown that, if the convergence is uniformly geomet-
ric for one of the two chains, e.g., if it takes values in a ﬁnite space, the
convergence to the stationary distribution is also uniformly geometric for
the other chain. This property is now known as the Duality Principle (see
Exercise 6.25).
Example 6.3.7 (Example 6.3.4 continued) The conditional distribu-
tions are
θ|x, λ ∼B(n, λ),
λ|x, θ ∼Be(α + θ, β + n −θ)
and allow for Gibbs sampling. Figure 6.3.3 gives a comparison of the his-
togram of a sample of 5000 observations obtained by batch sampling (with
t = 10) with the histogram of a sample of 5000 observations θ simulated
directly from the beta-binomial distribution. The strong similarity shows
that the Gibbs sampling approximation is quite acceptable.
∥
6.3.4 Rao–Blackwellization
As discussed in Section 6.3.1, the sample θ(1), . . . , θ(m) produced by the
Gibbs sampler can be used similarly to those obtained by the classical
Monte Carlo method, but Gelfand and Smith (1990) remark that the con-
ditional structure of the sampling algorithm and the dual sample, λ(1), . . . ,
λ(m), should be exploited. Indeed, if the quantity of interest is IEπ[g(θ)|x],
one can use the average of the conditional expectations
δ2 = 1
m
m

i=1
IEπ[g(θ)|x, λ(m)],
when they can be computed easily, instead of using the direct average
δ1 = 1
m
m

i=1
g(θ(i)).

310
Bayesian Calculations
6
0
10
20
30
40
50
0.0
0.01
0.02
0.03
0.04
0.05
0
10
20
30
40
50
0.0
0.01
0.02
0.03
0.04
0.05
0
10
20
30
40
50
0.0
0.01
0.02
0.03
0.04
0.05
Figure 6.3.2. Histograms for samples of size 5000 from the beta-binomial distribu-
tion with parameters n = 54, α = 3.4, and β = 5.2: (dark grey) directly simulated;
(light grey) obtained by Gibbs sampling.
This modiﬁcation is based on the Rao–Blackwell theorem (see Theorem
2.4.8). Were the λ(i)’s and θ(i)’s independent,
IEπ +
(δ1 −IEπ[g(θ)|x])2|x
,
=
1
mvarπ(g(θ)|x)
≥
1
mvarπ (IEπ[g(θ)|x, λ]|x)
=
IEπ +
(δ2 −IEπ[g(θ)|x, λ])2x
,
.
Liu et al. (1994) show that this inequality also holds for Data Augmenta-
tion because cov(θ(0), θ(m)) is then positive and decreasing with m (Exercise
6.27). The estimator δ2, christened Rao–Blackwellization, therefore domi-
nates δ1. (But this domination does not necessarily extend to other MCMC
schemes, see Liu et al. (1994) and Geyer (1995).)
Example 6.3.8 (Casella and George (1992)) Consider the following con-
ditional distributions (with x omitted):
π(θ|λ)
∝
λe−θλ,
0 < θ < B,
π(λ|θ)
∝
θe−λθ,
0 < λ < B.
The marginal distribution of θ (or of λ) cannot be computed, but the con-
ditional distributions are easy to simulate, being truncated exponential dis-
tributions. Since IEπ[θ|λ] ≃1/λ for B large, IEπ[θ|x] can be approximated
by
1
m
m

i=1
θi
or
1
m
m

i=1
1
λi
.
For this particular example, the complete symmetry existing between the

6.3
Markov chain Monte Carlo methods
311
two conditional distributions implies that the two estimators have exactly
the same probabilistic properties, besides converging to the same value. ∥
The same argument leads us to propose the approximation of the poste-
rior density π(θ|x) by the average of the conditional densities
1
m
m

i=1
π(θ|x, λi),
instead of using regular kernel estimation methods (see Tanner and Wong
(1987) and Gelfand and Smith (1990)).
6.3.5 The general Gibbs sampler
A generalization of the data augmentation algorithm is to consider several
groups of parameters, θ, λ1, . . . , λp, such that
π(θ|x) =

. . .

π(θ, λ1, . . . , λp|x) dλ1 · · · dλp .
(6.3.4)
This generalization corresponds to the introduction of additional levels in
the hierarchical model (6.3.3), either for modeling or for simulation reasons,
or it may occur because of the decomposition of the hyperparameter λ or
of the parameter θ into components of smaller dimensions.
As mentioned in Section 6.3.3 with the data augmentation scheme, the
Gibbs sampler provides simulations from the joint distribution π(θ, λ1, . . . ,
λp|x), when some conditional distributions associated with π are available.
Obviously, when π(θ|x) itself can be decomposed into conditionals, there
is no need to introduce the additional parameters λi (1 ≤i ≤p).
Example 6.3.9 (Example 6.3.4 continued)
If the population size n
has a Poisson prior, P(ξ), the overall joint posterior distribution is
π(θ, λ, n|x) ∝
n
θ

λθ+α−1(1 −λ)n−θ+β−1e−ξ ξn
n!
and the marginal distribution of θ cannot be derived. On the contrary, the
full conditional distributions have explicit expressions, since
θ|x, λ, ξ
∼
B(n, λ),
λ|x, θ, ξ
∼
Be(θ + α, n −θ + β),
n −θ|x, θ, λ
∼
P(ξ(1 −λ)).
Simulation from these three conditionals is thus possible.
∥
Example 6.3.10 (Tanner and Wong (1987)) Consider a multinomial model,
y ∼M5 (n; a1μ + b1, a2μ + b2, a3η + b3, a4η + b4, c(1 −μ −η)) ,

312
Bayesian Calculations
6
parametrized by μ and η, where
0 ≤a1 + a2 = a3 + a4 = 1 −
4

i=1
bi = c ≤1
and c, ai, bi ≥0 are known. This model stems from sampling according to
x ∼M9(n; a1μ, b1, a2μ, b2, a3η, b3, a4η, b4, c(1 −μ −η)),
and aggregating some coordinates:
y1 = x1 + x2,
y2 = x3 + x4,
y3 = x5 + x6,
y4 = x7 + x8, y5 = x9.
A conjugate prior distribution on (μ, η) for the model on x is the Dirichlet
distribution D(α1, α2, α3),
π(μ, η) ∝μα1−1ηα2−1(1 −η −μ)α3−1,
where α1 = α2 = α3 = 1/2 corresponds to a noninformative modeling. In
this setting, the posterior distribution of (μ, η) cannot be derived explicitly.
However, if we introduce the missing data z = (x1, x3, x5, x7), which is not
observed (hence, is missing), x is in one-to-one correspondence with (y, z)
and
π(η, μ|y, z)
=
π(η, μ|x)
∝
μz1μz2ηz3ηz4(1 −η −μ)y5+α3−1μα1−1ηα2−1 ,
where we denote the coordinates of z as (z1, z2, z3, z4). Therefore,
μ, η|y, z ∼D(z1 + z2 + α1, z3 + z4 + α2, y5 + α3).
Moreover,
zi|y, μ, η
∼
B

yi,
aiμ
aiμ + bi

(i = 1, 2),
zi|y, μ, η
∼
B

yi,
aiη
aiη + bi

(i = 3, 4).
Deﬁning θ = (μ, η) and λ = z, it thus appears that conditional distribu-
tions can be simulated in this case. Notice that the missing data z does not
appear in the original formulation of the problem and may be artiﬁcial, in
the sense that the model at hand does not necessarily correspond to an ag-
gregated multinomial model. However, it greatly facilitates the simulation
of the θ’s. Similar behavior will occur in other missing-data models.
∥
In this general hierarchical setting, implementation of the Gibbs sampler
can be done in many ways. If the decomposition of (θ, λ) in (θ, λ1, . . . , λp)
corresponds to a division of the model into its hierarchical levels, that is,
π(θ|x) =

..

π1(θ|λ1, x)π2(λ1|λ2)..πp+1(λp) dλ1 · · dλp,
(6.3.5)

6.3
Markov chain Monte Carlo methods
313
it seems logical to simulate according to the conditional distributions
π(θ|x, λ1, . . . , λp)
=
π1(θ|λ1, x),
π(λi|x, θ, (λj)j̸=i)
=
π(λi|λi−1, λi+1)
(1 < i < p),
π(λ1|x, θ, (λj)j̸=1)
=
π(λ1|θ, λ2),
(6.3.6)
π(λp|x, θ, (λj)j̸=p)
=
π(λp|λp−1),
whatever the dimensions of θ and λj are (Exercise 6.29). For instance,
in Example 6.3.10, (μ, η) could thus be generated conditional upon (y, z)
according to a Dirichlet distribution and z conditional upon (μ, η).
An alternative Gibbs sampler also proposed in Gelfand and Smith (1990)
is the one-at-a-time Gibbs sampler, which neglects hierarchical divisions
and consider only one-dimensional parameters, to generate them condi-
tional upon the other parameters.
Example 6.3.11 (Example 6.3.10 continued) Since
μ
1 −η |y, z, η
∼
Be(z1 + z2 + α1, y5 + α3),
η
1 −μ|y, z, μ
∼
Be(z3 + z4 + α2, y5 + α3),
this version of Gibbs sampling leads to the iterative simulation of
μ(t)
∼
(1 −η(t−1))Be

z(t−1)
1
+ z(t−1)
2
+ α1, y5 + α3

,
η(t)
∼
(1 −μ(t))Be

z(t−1)
3
+ z(t−1)
4
+ α2, y5 + α3

,
z(t)
j
∼
B

yj,
ajμ(t)
ajμ(t) + bj

(j = 1, 2),
(6.3.7)
z(t)
j
∼
B

yj,
ajη(t)
ajη(t) + bj

(j = 3, 4).
The diﬀerence with the simulation in Example 6.3.10 is thus minor.
∥
The general formulation of the Gibbs sampling algorithm for a joint dis-
tribution π(θ1, . . . , θp) with full conditionals π1, . . . , πp is set forth below.
Given (θ(t)
1 , . . . , θ(t)
p ), simulate
1. θ(t+1)
1
∼π1(θ1|θ(t)
2 , . . . , θ(t)
p ),
2. θ(t+1)
2
∼π2(θ2|θ(t+1)
1
, θ(t)
3 , . . . , θ(t)
p ),
...
p. θ(t+1)
p
∼πp(θp|θ(t+1)
1
, . . . , θ(t+1)
p−1 ).
The above validation of the data-augmentation algorithm extends to this
case: the joint distribution π is stationary at each step of this algorithm,

314
Bayesian Calculations
6
since the πj’s are the full conditionals of π. Under the positivity constraint
that the support of π is the Cartesian product of the supports of the πi’s,
the resulting chain is ergodic.
Compared with the Metropolis–Hastings algorithm, the number of pos-
sible implementations of the Gibbs sampler is small and, moreover, the
diﬀerences in the convergence properties are often minor. Still, the ap-
proach of (6.3.6) (also called substitution sampling in Gelfand and Smith
(1990)) should be preferred over the one-at-a-time alternative, since the
former respects the initial hierarchical modeling and often converges more
rapidly to the stationary distribution (see Liu et al. (1994) and Roberts
and Sahu (1997)). Data augmentation is the only case of Gibbs sampling
to produce a Markov chain for both (θ(t)) and (λ(t)); in every other scheme,
the subchains are not Markov chains (Exercise 6.30).
However, in order to be able to use data augmentation or even substitu-
tion sampling, one needs the conditional distributions for each hierarchical
level (as π(η, μ|y, z) in Example 6.3.10), and they may be more diﬃcult
to derive than full conditional distributions (see Exercise 6.46). Moreover,
the Gibbs sampler does not actually require the θi’s to be one-dimensional,
and the choice of the decomposition can then be entirely based on sim-
ulation reasons. Notice also that, when reduced conditional distributions,
like π(θ|x, λi0), are available for simulation, it is obviously preferable to
use these distributions, since they increase convergence speed by reduc-
ing the dependency on the other parameters. (This is called blocking; see
for instance Roberts and Sahu (1997).) A last important remark is that,
whenever simulation from a given conditional distribution πi(θi|θj, j ̸= i)
is diﬃcult, this simulation step can be replaced with a single Metropolis–
Hastings step with target distribution πi(θi|θj, j ̸= i). This may appear to
be a crude approximation device, but this is not the case: the replacement
of a simulation from πi(θi|θj, j ̸= i) with a Metropolis–Hastings step does
not modify the stationary distribution of the chain, and is thus entirely
valid from an MCMC point of view.
Example 6.3.12 (Example 6.1.4 continued) When N, the size of the
population, is the parameter of interest, Gibbs sampling provides a sam-
ple, N1, . . . , Nm, starting with an initial value of p = (p(0)
1 , . . . , p(0)
n ) and
iteratively generating according to
N (j) −x+|x, p(j−1)
∼
Neg(x+, ϱ(j−1)),
p(j)
i |x, N (j)
∼
Be(α + xi, β + N (j) −xi)
(1 ≤i ≤n).
(This is actually a data augmentation scheme.) The sample N1, . . . , Nm
is then obtained by taking N1 = N (k0+T ), N2 = N (k0+2T ), . . ., Nm =
N (k0+mT ), where k0 represents the “burn-in” time, that is, the number
of iterations to come reasonably close to stationarity, and T is the batch
size, that is, the number of iterations to achieve approximate independence
between the points of the sample. Gibbs sampling simultaneously gives a

6.3
Markov chain Monte Carlo methods
315
sample p1, . . . , pm. The expectation IEπ[N|x] can then be approximated by
ˆδπ(x)
=
1
m
m

t=1
IEπ[N|x, pt]
=
1
m
m

t=1
!
1 −
n

i=1
(1 −pt
i)
"−1
x+,
according to the “Rao–Blackwellization” argument mentioned above. George
and Robert (1992) provide hierarchical extensions in this setting by con-
sidering diﬀerent families of hyperpriors on (α, β).
∥
A more general comparison between Metropolis–Hastings and Gibbs
sampling algorithms is meaningless: depending on the problem at hand
and on the choice of the proposals/hierarchical decompositions, one algo-
rithm may converge faster than the other. The only warning we can provide
here is that, contrary to common belief, the Gibbs sampler is not neces-
sarily the optimal solution for an MCMC implementation. Indeed, whereas
on the one hand the Gibbs sampler is constructed directly from the tar-
get distribution π, and thus does not involve subjective input from the
experimenter, on the other hand the fact that the Gibbs sampler updates
the chain one component (or one block) at a time may induce very poor
convergence properties if the distribution has a very narrow or multimodal
support. In contrast, a Metropolis–Hastings algorithm using a random-walk
proposal may be ineﬃcient if the shape or scale of the proposal is at odds
with the support of π, but it also allows for big jumps that may reach
far-away modes of π. We could then classify Gibbs samplers as local, and
random-walk Metropolis–Hastings algorithms as global, in the crude sense
that the former often provide a better picture of the neighborhood of the
starting point, while the latter explore the support of π on a larger scale
(see Besag (2000) for a more detailed discussion). The best solution to this
dilemma is then to take advantage of the positive features of these diﬀer-
ent samplers by combining them in an hybrid sampler calling for several
MCMC algorithms in a deterministic or random manner.
6.3.6 The slice sampler
Gibbs sampling may appear at this stage as a particular MCMC method
that only applies in a quite restricted setting: it involves hierarchical struc-
tures, as in (6.3.3), thus does not apply to unidimensional problems; it also
requires full conditionals, thus cannot cover complex models.
This perception of the Gibbs sampler is mistaken: as we will now see, the
Gibbs sampler also applies in unidimensional problems, it does not require
simulation from the full conditionals, and it covers the same models as
other MCMC methods. In fact, the hierarchical decomposition (6.3.3) is
not particularly restrictive; and indeed, numerous distributions (on the

316
Bayesian Calculations
6
observation or the parameter) can be written as hidden mixtures, with
totally artiﬁcial parameter λ (see Note 6.6.3). Therefore, even when the
hierarchical structure is missing in the original problem, it can often be
reintroduced in order to improve the computation of the Bayes estimators,
or even the choice of the prior distribution.
The generality of the Gibbs sampler is actually exposed in the particular
version called the slice sampler (Wakeﬁeld et al. (1991), Besag and Green
(1993), and Damien et al. (1999)). Consider a distribution π(θ) on a general
set Θ, which can be one-dimensional or multidimensional, and write π as
the product
π(θ) =
k

i=1
ϖi(θ),
(6.3.8)
where the ϖi’s are positive functions, but are not necessarily densities.
Then π(θ) can be written as the marginal
π(θ) =

k

i=1
II0≤ωi≤ϖi(θ) d ω1 · · · d ωk .
The corresponding slice sampler is then straightforward:
At iteration t, simulate
1. ω(t+1)
1
∼U[0,ϖ1(θ(t))]
...
k. ω(t+1)
k
∼U[0,ϖk(θ(t))]
k+1. θ(t+1) ∼UA(t+1), with
A(t+1) = {ξ; ϖi(ξ) ≥ω(t+1)
i
, i = 1, . . . , k}.
The ωj’s are a particular type of auxiliary variables, which are mean-
ingless for the statistical problem. Notice that there are many possible
representations (6.3.8) for the same distribution π, including the simple
π(θ) =
 1
0
II0≤ω≤π(θ)dω ,
and that the choice is purely dictated by convenience. In fact, the last step
(k+1) in the above algorithm may be far from simple to implement, since
the set A(t) is often diﬃcult to construct, but this decomposition shows
that the Gibbs sampler can provide, at least formally, a representation of
all distributions. (See Roberts and Rosenthal (1998) and Mira and Tierney
(2002) for theoretical properties of the slice sampler.)
Example 6.3.13 (Example 6.3.2 continued) The joint distribution of
(α, η) being
π(α, η|x1, . . . , xn) ∝αnηn+β−1
! n

i=1
xi
"α
exp

−η
n

i=1
xα
i −α −ξη

,

6.3
Markov chain Monte Carlo methods
317
the conditional distribution π1(η|α, x1, . . . , xn) is simply a
G(β + n, ξ +

i
xα
i )
distribution, which can be simulated easily. The conditional distribution
π2(α|η, x1, . . . , xn) is much more complex because of the exponential part
involving the xα
i ’s. If we write this distribution as αnχα exp(−η n
i=1 xα
i ),
it can be expressed as the marginal (in α) of
αnII0≤ω0≤χα
n

i=1
II0≤ωi≤exp(−ηxα
i ) .
The conditional distribution of α given η and the ωi’s is then proportional to
αnIIα log(χ)≤log(ω0)
n

i=1
IIα log(xi)≤log{−log(ωi)/η} ,
that is, a simple power distribution αn on an interval (α, α). The overall
Gibbs sampler for the Weibull posterior distribution then goes through the
iterative simulation of η, of the ωi’s and of α.
∥
Example 6.3.14 (Example 6.1.5 continued)
Because the posterior
distribution of θ = (μ1, σ2
1, p, μ2, σ2
2) is available in closed form,
π(θ|x) ∝˜π(θ|x) = π(θ)
n

i=1
{pϕ(xi; μ1, σ1) + (1 −p)ϕ(xi; μ2, σ2)} ,
a formal slice sampler with a single auxiliary variable ω can be proposed,
with θ ∼U˜π(θ|x)≥ω. But it is impossible to simulate from this uniform
distribution, since the constraint ˜π(θ|x) ≥ω cannot be inverted into a
constraint on θ. A manageable version of the slice sampler in this setting
can be constructed by introducing instead n auxiliary variables ωi so that
˜π(θ|x) reads like the marginal distribution of
π(θ)
n

i=1
IIpϕ(xi;μ1,σ1)+(1−p)ϕ(xi;μ2,σ2)≥ωi≥0 .
Although the joint distribution of θ conditional on the ωi’s is still in-
tractable, the full conditionals of the parameters μ1, σ2
1, p, μ2 and σ2
2
can be easily simulated. (As we will see in Section 6.4, which concentrates
on mixtures, the original Gibbs sampling solution also goes through the
simulation of n auxiliary variables.)
∥
6.3.7 The impact of MCMC methods on Bayesian Statistics
This section has very brieﬂy presented the bases of MCMC methods, with a
few illustrations taken from Bayesian computational problems. It is impor-
tant to stress at this point that the intrusion of these MCMC tools into the

318
Bayesian Calculations
6
Bayesian ﬁeld has had a “devastating” eﬀect! Indeed, it has radically mo-
diﬁed the way people work with models and prior assumptions, by allowing
much more complex structures to be proposed, as for instance in the case
of graphical models where relations between variables are only deﬁned at a
local level, the joint distribution being impossible to envision (see Spiegel-
halter et al. (1993) and Note 10.7.1). Similarly, latent variable models such
as ARMA, hidden Markov, or stochastic volatility models can now be cor-
rectly analyzed (see Note 6.6.5 and Robert and Casella (2004, Note 9.7.2))
where only crude approximations could be used in the past, and this has
had a tremendous eﬀect in Bayesian signal processing, econometrics and
mathematical ﬁnance.
The “devastation” mentioned above has also occurred at the level of the
rigid structures previously imposed by the need for an analytical process-
ing; for instance, conjugate priors are not compulsory anymore, even if they
are still quite useful as basic priors in the diﬀerent stages of a hierarchical
modeling (see Chapter 10). Similarly, much more ﬂuid representations can
be proposed in model choice, as we will see in Chapter 7, where the possi-
bility of envisioning many models at once leads the statistician away from
strict testing towards model averaging, where the most likely models get
the heavier weights but none are a priori excluded. See also Berger (2000),
Capp´e and Robert (2000), and Gelfand (2000) for short reviews on the
impact of MCMC methods on the ﬁeld.
As usual, a signiﬁcant increase in the facility of using a given technique
is accompanied by the corresponding increase in potential misuses of this
technique! In the case of Bayesian analysis, this means that the eﬀect of
a prior modeling is more diﬃcult to assess through the conditional dis-
tributions used in a Gibbs sampler. More crucially, the propriety of the
posterior distribution discussed in Section 1.5 may fail to hold without the
user being aware of it (see Note 6.6.4). But these drawbacks cannot com-
pete with the explosion of the range and number of Bayesian applications,
or the resolution of inferential problems never considered before.
6.4 An application to mixture estimation
We conclude this chapter by showing how MCMC methods are appropriate
for the derivation of the Bayes estimators of the parameters of a mixture
of normal distributions, considered in Example 6.1.5. Extensions to other
mixtures of distributions from an exponential family or to hidden Markov
models are straightforward (see Gruet et al. (1999) and Robert and Casella
(1999, Section 9.5.1)). As detailed in Section 6.1, a Bayesian analysis of a
mixture model leads to the information paradox that an explicit estimator
is available and intuitively justiﬁed, but cannot be computed when the
number of observations gets too large. Moreover, the maximum likelihood
estimators of the parameters of (6.1.4) are not clearly deﬁned, solving the
likelihood equations is diﬃcult, and there are even problems with analytical

6.4
An application to mixture estimation
319
approximations of the Bayes estimator. (See Crawford et al. (1992) for
an approach using Laplace approximation.) Similarly, a standard Monte
Carlo processing of mixture models is arduous, even though Casella et
al. (2000) have proposed an importance sampling solution in a conjugate
setting (Exercise 6.39). See Note 6.6.6 for further references and details on
the early history of mixture estimation.
Gibbs sampling for mixtures relies on a missing data representation, as
in Dempster et al. (1977), to build up a hierarchical structure similar to
(6.3.3). Consider
x ∼f(x|θ) =
k

i=1
piϕ(x; μi, σi),
(6.4.1)
a mixture of k normal distributions with means μi and variances σ2
i (1 ≤
i ≤k) and 
i pi = 1 (pi > 0). Given a sample x1, . . . , xn, from (6.4.1), we
deﬁne the missing values zj (1 ≤j ≤n) as the component indicator vectors
for the xj’s, that is,
zij =

1
if xj ∼ϕ(x; μi, σi),
0
otherwise,
and 
i zij = 1. This vector can also be considered as an additional param-
eter and it provides the following joint distributions (1 ≤j ≤n):
zj|θ
∼
Mp(1; p1, . . . , pk) ,
xj|zj, θ
∼
N
! k

i=1
μzij
i ,
k

i=1
σ2zij
i
"
.
A convenient prior distribution on θ = (μ1, σ1, p1, . . . , μk, σk, pk) is the
product of conjugate distributions πi(μi, σi), with πi(μi|σi) a normal distri-
bution N(ξi, σ2
i /ni), πi(σ2
i ) an inverse gamma distribution IG(νi/2, s2
i /2),
and π(p) a Dirichlet distribution, D(α1, . . . , αk), as in Example 6.1.5.
Notice that, once the allocation vectors zj (1 ≤j ≤n) are known,
the mixture structure disappears, since this additional information breaks
down the sample into subsamples according to the values of zij. Although
the posterior distribution of θ cannot be used per se, as shown in Example
6.1.5, conditioning on z = (z1, . . . , zn) removes the diﬃculty. Indeed, we
get the following posterior distributions (1 ≤j ≤n):
zj|xj, θ ∼Mk(1; p1(xj, θ), . . . , pk(xj, θ)),
(6.4.2)
with (1 ≤i ≤k)
pi(xj, θ) =
piϕ(xj; μi, σi)
k
t=1 ptϕ(xj; μt, σt)
,
and
μi|x, z, σi ∼N(ξi(x, z), σ2
i /(n + σ2
i )),
(6.4.3)
σ2
i |x, z ∼IG
νi + ni
2
, 1
2

s2
i + ˆs2
i (x, z) +
nimi(z)
ni + mi(z)(¯xi(z) −ξi)2

,

320
Bayesian Calculations
6
p|x, z ∼Dk(α1 + m1(z), . . . , αk + mk(z)),
where
mi(z) =
n

j=1
zij,
¯xi(j) =
1
mi(z)
n

j=1
zijxj,
and
ξi(x, z) = niξi + mi(z)¯xi(z)
ni + mi(z)
,
ˆs2
i (x, z) =
n

j=1
zij(xj −¯xi(z))2.
Conditional upon z, the posterior distributions only take into account the
subsamples related with each component, in a manner similar to the decom-
position (6.1.6) of the true posterior distribution. In addition, simulations
from (6.4.2) and (6.4.3) are particularly straightforward. Therefore, it is
much easier to produce a sample θ1, . . . , θm from π(θ|x) by Gibbs sampling
than to use the true posterior distribution directly.
The remark following Lemma 6.3.6 implies that Gibbs sampling leads
to uniform geometric convergence for the chain (θ(m)), since z has a ﬁnite
support. The one-dimensional Gibbs sampling version of this algorithm
only modiﬁes the simulation for σi, which is then conditional on μi (1 ≤
i ≤k),
σ2
i |x, z, μi ∼IG
⎛
⎝νi + ni + 1
2
, 1
2
⎡
⎣s2
i +
n

j=1
zij(xj −ξi)2
⎤
⎦
⎞
⎠.
Although the change is minor compared with (6.4.3), convergence results
are much more diﬃcult to establish in this setting. In particular, geometric
convergence cannot be established without imposing restrictions on the σi’s
(see Diebolt and Robert (1994)). Moreover, since data augmentation can be
implemented in this setting and is usually preferable to the one-dimensional
Gibbs sampler, there is no reason to use the latter. In the setting of hidden
Markov chains, which generalize mixture models like (6.4.1) by introduc-
ing Markovian dependence between the zj’s, this is not so straightforward
(even though there exist in some settings closed-form representations of
the likelihood that integrate out the latent variables; see Exercises 6.46
and 6.47, and Robert et al. (1999a)).
We stress as a ﬁnal remark that Gibbs sampling is not the unique so-
lution for the simulation of the posterior distribution π(θ|x). Indeed, as
shown by Example 6.3.14, this distribution is available in closed form and
can thus be used with any Metropolis–Hastings proposal (besides the slice
sampler produced in Example 6.3.14). For instance, Celeux et al. (2000)
demonstrate that the random-walk proposal can be used eﬃciently in this
setting, with better mixing properties than the Gibbs sampler.

6.5
Exercises
321
6.5 Exercises
Section 6.1
6.1 For a mixture of two normal distributions, as studied in Example 6.1.5 and
the data of Table 6.1.5, identify the hyperparameters of the conjugate distri-
butions by the moments method.
6.2 In the setting of Example 6.1.5, show that the posterior distribution can
actually be written as (6.1.6) and develop ω(kt) and π(θ|(kt)). Give the ex-
pressions of the Bayes estimators of μ1, σ1, and p for the hyperparameters
obtained in Table 6.1.5.
6.3 Establish the equivalent of Exercise 6.2 for
(i) a mixture of two exponential distributions; and
(ii) a mixture of three uniform distributions.
6.4 In the setting of Exercise 6.2, how does the computing time evolve with the
sample size when
(i) only the weight p is unknown; and
(ii) all the parameters are unknown.
6.5 *(Smith and Makov (1978)) Consider
x ∼f(x|p) =
k

i=1
pifi(x),
where pi > 0, 
i pi = 1, and the densities fi are known. The prior π(p) is a
Dirichlet distribution D(α1, . . . , αk).
a. Show that the computing time becomes prohibitive as the sample size in-
creases.
A sequential alternative which approximates the Bayes estimator is to replace
π(p|x1, . . . , xn) by D(α(n)
1
, . . . , α(n)
k ), with
α(n)
1
= α(n−1)
1
+ P(zn1 = 1|xn), . . . , α(n)
k
= α(n−1)
k
+ P(znk = 1|xn),
and zni (1 ≤i ≤k) is the component indicator vector of xn as deﬁned in
Section 6.4.
b. Justify this approximation and compare it with the updating of π(θ|x1, . . . ,
xn−1) when xn is observed.
c. Examine the performances of this approximation for a mixture of two nor-
mal distributions N(0, 1) and N(2, 1) when p = 0.1, 0.25, 0.5.
d. If πn
i = P(zni = 1|xn), show that
ˆp(n)
i
(xn) = ˆp(n−1)
i
(xn−1) −an−1{ˆp(n−1)
i
−πn
i },
where ˆp(n)
i
is the quasi-Bayesian approximation of IEπ(pi|x1, . . . , xn).

322
Bayesian Calculations
6
6.6 In the setting of Example 6.1.4, determine the posterior distribution of
π(N|x): (a) for n = 10 and similar xi’s; and (b) for n = 30 and very dif-
ferent xi’s. Consider the same problem when π(N) is a Poisson distribution
P(λ) and λ varies. Pay particular attention to the potential problems linked
with the direct evaluation.
Section 6.2.1
6.7 *(Morris (1982)) Given the quadratic variance natural exponential families
studied in Exercises 3.24 and 10.33, consider
Pm(x, μ) = V m(μ)

dm
dμm f(x|μ)
-
f(x|μ).
a. Show that Pm is a polynomial of degree m in x and in μ.
b. Show that (m > 1)
Pm+1(x, μ) =
[P1(x, μ) −mV ′(μ)]Pm(x, μ) −m[1 + (m −1)v2]V (μ)Pm−1(x, μ),
where V (μ) = v0 + v1μ + v2μ2.
c. Show that the polynomials Pm are orthogonal, and that IEμ[P 2
m(x, μ)] =
amV m(μ).
d. Give the polynomials associated with the normal, Poisson, gamma, bino-
mial, and negative binomial distributions. [Note: They are called Hermite,
Poisson–Charlier, generalized Laguerre, Krawtchouk, and Meixner polyno-
mials, respectively.]
Section 6.2.2
6.8 Show that, if the support of h does not contain the support of f(x|θ)π(θ),
the importance sampling approximation (6.2.4) does not converge.
6.9 The regular accept–reject simulation algorithm is deﬁned as follows: If f and
g are densities such that there exists M with f(x) ≤Mg(x),
1. Generate y ∼g(y) and u ∼U[0,1];
2. If u > f(y)/Mg(y), go back to 1.
3. Take x = y.
Show that this algorithm actually provides an observation x from f(x).
6.10 *(Gilks and Wild (1992))
A general accept–reject simulation method is
proposed for log-concave densities on IR. The method is based on adaptive up-
per and lower bounds on the density, which are updated after each simulation
step.
a. Given f(x) proportional to the density to be simulated, we assume there
exist u(x) and ℓ(x), upper and lower bounds on f(x) such that u is a
density. The envelope simulation algorithm is as follows:
Iterate
(a) Generate x ∼u(x) and U ∼U[0,1]
(b) Accept x if U ≤ℓ(x)/u(x)

6.5
Exercises
323
(c) Otherwise, accept x if U ≤f(x)/u(x)
till x is accepted
Show that this method actually produces a random variable with distribu-
tion f.
b. The two bounding functions can be constructed automatically for f log-
concave as follows. For the ﬁrst simulation, take three arbitrary values
x1, x2 > x1, and x3 > x2 such that at least one is on each side of the
mode of f. (Explain why this can be done without requiring an explicit
derivation of the mode.) Show that the lower bound log ℓ(x) on log f(x)
can be derived by joining the three points (xi, log f(xi)) and ℓ(x) to be 0
outside the interval [x1, x3]. The upper bound log u(x) is constructed by
taking the complements of the segments used for log ℓ(x) until they meet:
the tails are thus made of the extensions of the chords (x1, x2) and (x2, x3);
log u(x) is completed by adding the vertical lines going through x1 and x3
until they meet the two chords.
c. Derive a way to update the upper and lower bounds after each simulation
requiring the computation of f(x).
d. Show that the two functions u(x) and ℓ(x) are piecewise exponential and
indicate how one can simulate from distributions proportional to these func-
tions.
e. Illustrate the above algorithm for the simulation from a N(0, 1) distribu-
tion. When does it become more time-consuming to evaluate and simulate
from a better upper bound than to keep the current upper bound?
6.11 *(Rubinstein (1981)) Consider the integral
I =
 b
a
f(x) dx,
approximated by a Monte Carlo method with importance function h:
ˆI = 1
m
m

i=1
f(xi)/h(xi).
a. Show that the variance of ˆI is
var(ˆI) = 1
n
 b
a

f(x)
h(x) −I
2
h(x) dx
and deduce that it is minimized by h ∝|f|.
b. Consider 0 ≤f(x) ≤c, v1, . . . , vm ∼U[0,c], and u1, . . . , um ∼U[a,b]. We
deﬁne
ˆI = (b −a) 1
m
m

i=1
f(ui)
and
˜I = c(b −a) 1
m
m

i=1
IIvi≤f(ui).
Show that
I = c(b −a)P(V ≤f(U))
for U ∼U[a,b] and V ∼U[0,c].
c. Deduce that IE[˜I] = I and var(˜I) ≤var(ˆI).

324
Bayesian Calculations
6
d. Discuss the relevance of the notion of a “best” importance function h.
(Hint: Consider a sequence of normal distributions centered at the value of
interest, that is, at x∗such that f(x∗) = I and with variance decreasing to
0.)
6.12 Show that, for a given function g(θ) and a distribution of interest π(θ),
the optimal choice of the importance density h (in terms of variance of the
estimator (6.2.4)) is
h(θ) ∝|g(θ)|π(θ) .
Express the corresponding estimator and deduce that, if g is of a constant
sign, the resulting variance is 0. (Hint: See Robert and Casella (2004, Theorem
3.3.2) for a proof.)
Section 6.2.3
6.13 Justify the Laplace approximation when h(θ) = (θ −μ)2 and b(θ) is a
polynomial of degree 2. What happens if b is of higher degree? Derive the
general Laplace expansion by using Taylor series for b and h.
6.14 *(Tierney et al. (1989)) Deduce from the Laplace approximation that

bN(θ)e−nhN (θ)dθ

bD(θ)e−nhD(θ)dθ = A(N)
A(D) + O(σ−2),
where
A(K) = σK exp{−nˆhk}
/
ˆbK + 1
2n

σ2
Kˆb′′
K
−ˆh′′′
Kˆb′
Kσ2
K + 5
12
ˆbK(ˆh′′′
K)2σ6
K −1
4
ˆbKσ2
Kh(4)
K
 0
and K = N, D, if ˆhK = h(ˆθK), etc., and ˆθK is minimizing hK. Deduce Lemma
6.2.4 under the assumption that ˆh(i)
N −ˆh(i)
D
= O(n−1) for i = 0, . . . , 4 and
ˆbD ̸= 0. What happens if ˆbD = 0?
6.15 *(Tierney et al. (1989)) If M(s) is the moment generating function of g(θ)
and ˆ
M is the Laplace approximation of M for (6.2.9), with bN = bD = b > 0
and
hD(θ) = {log[fx|θ)] + log[π(θ)] −log[b(θ)]}/n,
hN(θ) = hD(θ) −sg(θ)/n, we deﬁne
ˆIE(g) = ˆ
M ′(0).
a. Show that IEπ[g(θ)|x] = ˆIE(g) + O(n−2).
b. Let ˆθ be the minimum of hD, let ˆθs be the minimum of hN, and let σ2
s =
h(2)
N (θs). Show that
ˆIE(g) = g(ˆθ) + d
ds log σs

s=0
+ d
ds log b(ˆθs)

s=0
.
c. Deduce that
ˆIE(g) = ˆg + σ2
Dˆg′′
2n
−σ4
Dˆh′′′
D ˆg′
2n
+ σ2
Dˆb′
Dˆg′
nˆbD
,
and therefore that this method actually gives the approximation (6.2.8) for
the standard form.

6.5
Exercises
325
6.16 In the setting of Example 6.2.6, choose the standard and fully exponential
representations leading to the proposed approximations.
Section 6.3.2
6.17 *Consider the Metropolis–Hastings algorithm of Section 6.3.2 that simulates
a density π(θ) from a proposal density q(θ′|θ).
a. Show that this algorithm reduces to regular simulation from π when q(θ′|θ) =
π(θ′).
b. Give the simpliﬁed version of the Metropolis–Hastings algorithm when
q(θ|θ′) is symmetric in its arguments, that is, when q(θ|θ′) = q(θ′|θ).
c. Show directly, that is, without the balance condition (6.3.1), that π(θ) is
a stationary distribution for this algorithm when the support of q contains
the support of π. (Hint: Compute the probability distribution function of
θ(m+1) when θ(m) ∼π(θ) by breaking up the integral into four parts, and
exchange the dummy variables θ and ξ in two of the four integrals.)
d. In the particular case when π is a N(0, 1) distribution and q(θ|θ′) is a
N(θ′, σ2) distribution, study the probability of acceptance of ξ in the mth
step as a function of σ. What is the exact distribution of θ(m)? Deduce the
best choice of σ.
6.18 Show that the detailed balance condition (6.3.1) holds for the Metropolis–
Hastings algorithm.
6.19 Examine whether the Metropolis–Hastings algorithm produces a reversible
Markov chain, that is, such that the distribution of (x(t), x(t+1)) is the same
as the distribution of (x(t+1), x(t)) under stationarity.
6.20 (Robert (1993a)) Consider n observations y1, . . . , yn from a general logistic
regression model with
P(yi = 1) = 1 −P(yi = 0) =
exp(θtxi)
1 + exp(θtxi),
and xi, θ ∈IRp.
a. Show that, conditional upon the xi’s, this distribution belongs to an expo-
nential family and that 
i yixi is a suﬃcient statistic.
b. Give the general form of the conjugate prior distributions for this model and
show that the normalization factor cannot be computed explicitly. Give an
interpretation of the hyperparameters (ξ, λ) of the conjugate prior in terms
of previous observations.
c. Show that the maximum likelihood estimator of θ, ˆθ, cannot be obtained ex-
plicitly, and that it satisﬁes the following implicit equations (j = 1, . . . , p):
n

i=1
exp(ˆθtxi)
1 + exp(ˆθtxi)
xij =
n

i=1
yixij.
(6.5.1)
d. Approximate a conjugate distribution by the Metropolis–Hastings algo-
rithm. [Note: If a normal conditional distribution is used, attention should
be paid to the variance factor.]

326
Bayesian Calculations
6
e. Explain why (6.5.1) can be used to control the convergence of the algorithm
for some special values of the hyperparameter vector, (ξ, λ), namely, those
for which
IEπ
ξ,λ

n

i=1
exp(θtxi)
1 + exp(θtxi)xi

=
IEπ
ξ,λ

n

i=1
exp(θtxi)
1 + exp(θtxi)xi
 y1, . . . , yn

=
n

i=1
yixi.
6.21 *Given a density of interest, π, and an available density f such that π/f ≤
M, samples from π can be produced either by accept/reject, θ(1)
1 , . . . , θ(1)
p ,
or by Metropolis–Hastings with proposal f, θ(2)
1 , . . . , θ(2)
n , or else an impor-
tance sampling sample, θ(3)
1 , . . . , θ(3)
n , can be generated from f. Compare the
variances of
1
p
p

i=1
θ(1)
i
,
1
n
n

i=1
θ(2)
i
,
1
n
n

i=1
π(θ(3)
i
)
f(θ(3)
i
)
θ(3)
i
.
[Note: p denotes the random number of observations produced after n propos-
als in the accept/reject algorithm.]
6.22 Consider a probability distribution P and a function ϱ such that 0 ≤ϱ(x) ≤
1 and IEP [1/ϱ(x)] < ∞. A Markov chain x(n) is derived as follows. Update
x(n) into x(n+1) by generating y ∼P and take
x(n+1) =

y
with probability ϱ(x(n)),
x(n)
with probability 1 −ϱ(x(n)).
a. Show that this variation of the Metropolis–Hastings algorithm is converging
to the stationary distribution with density
ϱ(x)−1/IEP [ϱ(x)−1]
with respect to P.
b. Apply to the case when P is a Be(α + 1, 1) distribution and ϱ(x) = x.
c. Study the performances of the method when α = 0.2. [Note: See Robert
and Casella (1999, Example 8.2.8, or 2004, Problems 7.5 and 7.6) for an
illustration of the poor performances of this generator.]
Section 6.3.3
6.23 The data augmentation algorithm is based on the conditional distributions,
π(θ|λ) and π(λ|θ). As described in Section 6.3, it successively simulates from
π(θ|λ) and from π(λ|θ). This exercise shows why such a simulation of π(θ, λ)
is justiﬁed from a probabilistic point of view.
a. Derive the joint distribution π(θ, λ) in terms of these conditional distribu-
tions.
b. Given two functions q(θ|λ) and s(λ|θ), what is a necessary and suﬃcient
condition for q and s to be proportional to conditional distributions?
c. Consider the above questions in the case of n levels for the completed
models, that is, when conditional distributions are available for θ, λ1, . . . ,
λn−1.

6.5
Exercises
327
6.24 (Exercise 6.23 cont.) The Hammersley–Cliﬀord theorem establishes that
the joint distribution π(ϑ) of a vector ϑ = (θ1, . . . , θp) can be derived from
the full conditional distributions, πj(θj| . . . , θj−1, θj+1, . . . , θp). Show that
π(ϑ) ∝
p

j=1
πℓj(θℓj|θℓ1, . . . , θℓj−1, θ′
ℓj+1, . . . , θ′
ℓp)
πℓj(θ′
ℓj|θℓ1, . . . , θℓj−1, θ′
ℓj+1, . . . , θ′
ℓp)
for every permutation ℓon {1, 2, . . . , p} and every θ′ ∈Θ. [Note: Cliﬀord and
Hammersley never published their result. See Hammersley (1974) and Robert
and Casella (2004, Section 9.1.4) for details.]
6.25 *(Diebolt and Robert (1994)) Consider the two Markov chains (θ(m)) and
(λ(m)) used in data augmentation with conditional distributions π1(θ|x, λ) and
π2(λ|x, θ).
a. Show that the respective transition kernels of these chains are
K(θ′|θ)
=

Λ
π1(θ′|x, λ)π2(λ|x, θ) dλ,
H(λ′|λ)
=

Θ
π2(λ′|x, θ)π1(θ|x, λ) dθ.
b. Show that π1(θ|x) and π2(λ|x) are indeed stationary for these kernels.
c. Establish that, if θ(m) ∼πm
1 (θ|x, λ(0)) and λ(m) ∼πm
2 (λ|x, λ(0)),
||πm
1 (·|x, λ(0)) −π1(·|x)||1 ≤||πm
2 (·|x, λ(0)) −π2(·|x)||1.
d. Derive Lemma 6.3.6 from c., and from the fact that irreducible Markov
chains with stationary distributions are ergodic. Show that, if (λ(m)) is
geometrically ergodic with rate ϱ, (θ(m)) is also converging with rate ϱ,
that is,
||πm
1 (·|x, λ(0)) −π1(·|x)||1 ≤Cϱm.
e. The chain (λ(m)) is ϕ-mixing if there exists ϕ, geometrically decreasing,
and a ﬁnite measure μ such that
πm
2 (λ|x, λ(0)) −π2(λ|x)
 ≤ϕ(m)μ(λ).
Show that, when (λ(m)) is ϕ-mixing,
|πm
1 (θ|x, λ(0)) −π1(θ|x)| ≤ϕ(m)

Λ
π1(θ|x, λ)μ(dλ)
and deduce that, if Λ is compact, (θ(m)) is also ϕ-mixing.
f. Similarly, show that geometric convergence of (λ(m)) and compactness of
Λ are suﬃcient to ensure that, for every function h satisfying
IEπ[||h(θ)||2|x, λ] < ∞,
there exists Ch such that
|| IEπm
1 [h(θ)|x, λ(0)] −IEπ1[h(θ)|x] ||2 ≤Chϱm.
g. Take advantage of the fact that, when Λ is ﬁnite, the chain (λ(m)) is nec-
essarily geometrically converging and ϕ-mixing (Billingsley (1986)). Assess
the importance of the above results in the setting of mixture estimation.

328
Bayesian Calculations
6
h. Extend the duality principle to the case of a hierarchical model with multi-
ple levels, using the fact that conditional distributions only depend on the
neighboring levels.
6.26 Two machines are run in parallel with breakdown times x ∼f(x|θ) and
y ∼g(y|η). The defective machine is supposed to be known when a breakdown
occurs.
a. Give the distribution of z, breakdown time of the system, and derive a
Gibbs sampling algorithm to get posterior estimators of θ and η when a
sample z1, . . . , zn is available, and when conjugate priors are used on both
θ and η.
b. Implement this algorithm in the special cases when (a) f and g are normal
densities with means θ and η, and variance 1; (b) f and g are exponential
distributions with parameters θ and η.
Section 6.3.4
6.27 For a chain (θ(t), λ(t)) produced by data augmentation,
a. Show that, for every function h,
cov(h(θ(1)), h(θ(2))) = var {IE[h(θ)|λ]} .
b. Give a corresponding representation for cov(h(θ(1)), h(θ(t))).
c. Deduce that cov(h(θ(1)), h(θ(t))) is always positive and decreasing with t.
d. Conclude about the domination of the usual average by its Rao–Blackwellized
version.
6.28 Show that, in the setting of Example 6.3.4, the marginal distributions on θ
and λ cannot be derived explicitly and that, moreover, the restriction B < +∞
is necessary for the marginal distributions to be deﬁned.
Section 6.3.5
6.29 For a hierarchical model such as (6.3.5), show that the distribution of a
given λi conditional on all the other parameters of the model π(λi|x, θ, (λj)j̸=i)
(1 ≤i ≤p) only depends on its two nearest neighbors in the vector (x, θ, λ1,
. . . , λp). (Hint: Draw a graphical representation of the model.)
6.30 Show that, if the Gibbs sampler is implemented with more than two full
conditional levels, as in, for instance, (6.3.7), the resulting subchains corre-
sponding to the various levels are not Markov chains.
6.31 Considering the multinomial model of Example 6.3.10, explain why simu-
lating from π((μ, η)|x) rather than from π(μ|x) and π(η|x) should speed up
convergence. (Hint: Study the correlation between μ(t) and μ(t+1) in both
cases.)
6.32 Show that, in a Gibbs sampling algorithm, if an arbitrary simulation step,
like the simulation from π(θ1|θ2, . . . , θk) say, is replaced by a single step of
a Metropolis–Hastings algorithm, the validity of the algorithm is preserved.
Discuss the crucial interest of this property for practical issues.
6.33 Consider a distribution π(θ1, θ2), which is not available under closed form,
but such that the two conditional distributions π(θ1|θ2) and π(θ2|θ1) are
known and can be simulated from.

6.5
Exercises
329
a. Show that the Metropolis–Hastings algorithm can be implemented. (Hint:
Show that the only diﬃculty is to simulate from π(θ1) or from π(θ2) and
use Exercise 6.23.)
b. Deduce that in every setting Gibbs sampling can be used, the same applies
to the general form of the Metropolis–Hastings algorithm.
6.34 Show that a Gibbs sampling step is a special case of the Metropolis algo-
rithm where the acceptance probability is always equal to 1.
Section 6.3.6
6.35 A truncated normal distribution Np(0, Ip) restricted to the polygon θt
ixi ≤
zi (1 ≤i ≤n) is to be simulated.
a. Give the distribution of θj conditional on θk (k ̸= j) and derive a Gibbs
implementation for the simulation of this truncated normal distribution.
(Hint: See Geweke (1991) or Robert (1995a) for accept-reject algorithms to
simulate from a one-dimensional truncated normal distribution).
b. Propose a Metropolis–Hastings alternative based upon a Np(μ, Σ) simula-
tion, when μ and Σ are derived from the polygon boundaries.
c. Propose a slice sampler based on a single auxiliary variable and a slice
sampler based on p auxiliary variables.
d. Compare these diﬀerent algorithms.
Section 6.3.7
6.36 (Rubin et al. (1992))
A study was conducted on the campus of Cornell
University to model the sexual behavior of undergraduates. Out of a popula-
tion of Rm (Rf) male (female) undergraduates, rm (rf) answered the survey
and tm (tf) were found to be sexually active (in the previous two months).
a. The ﬁrst quantities of interest are Tf and Tm, numbers of female and male
undergraduates who were sexually active. Using a hypergeometric model
on tm and tf and taking rm and rf as ﬁxed, derive a Bayes estimator of
Tf and Tm when
Ti ∼B(Ri, pi),
pi ∼Be(α, β),
π(α, β) = 1/αβ
(i = f, m).
(Numerical application: Rf = 5211, rf = 253, tf = 111, Rm = 6539,
rm = 249 and tm = 22.)
b. During the study, sexually active respondents were asked about the number
of partners they had had during the two last months, yf and ym, as well
as the number of Cornell undergraduate partners, xm and xf.
Assuming a Poisson distribution P(λi) for the number of additional partners
yi −1 and a binomial distribution B(yi, ϱi) on the number of Cornell under-
graduate partners (i = f, m), with ϱf = Tm/Nm and ϱm = Tf/Nf, derive a
Bayes estimator of the population in sexual contact with the Cornell under-
graduates, Nm and Nf. The prior distributions are
λi ∼Exp(λ0),
ϱi ∼Be(γ, δ),
π(γ, δ) = 1/γδ.
(Numerical application: ym = 54, xm = 31, yf = 135, xf = 67.)

330
Bayesian Calculations
6
c. Compare your results with the maximum likelihood estimators obtained in
the study: ˆ
Nf = 4186, ˆ
Nm = 1473, ˆTf = 2323 and ˆTm = 615.
d. Repeat the estimation for the hyperpriors
π(α, β) = e−(α+β),
π(γ, δ) = e−(γ+δ),
and
π(α, β) = 1/(α + β)2,
π(γ, δ) = 1/(γ + δ)2.
6.37 In the setting of logistic regression (see Exercise 6.20), a missing data struc-
ture can be exhibited and exploited by a Gibbs sampling algorithm.
a. Derive the distribution of zi such that the observation yi is IIzi≤xt
iθ.
b. Give the likelihood of the completed model and examine whether a Gibbs
algorithm similar to those of Section 6.4 can be constructed in the special
case θ ∼Np(μ, Σ).
c. Compare the performance of this algorithm with a more straightforward
Metropolis–Hastings algorithm of your choice.
6.38 A probit model is a qualitative regression model where the dependence on
the auxiliary variables is given by
Pθ(yi = 1) = 1 −Pθ(yi = 0) = Φ(θtxi).
a. Show that, as in Exercise 6.37, it is possible to complete the model by
exhibiting a continuous latent variable zi.
b. Propose a Gibbs sampling algorithm based upon the completed data when
θ ∼Np(μ, Σ).
Section 6.4
6.39 (Casella et al. (2000)) Gibbs sampling and other MCMC techniques un-
locked the diﬃculties with Bayesian inference on mixtures. It is, however,
possible to produce eﬃcient importance sampling estimators in this setting.
We assume that a sample (x1, . . . , xn) from
k

j=1
pjf(x|θj)
is available.
a. Given the allocation variables z1, . . . , zn, where xi|zi ∼f(x|θzi), show that
the posterior distribution of z = (z1, . . . , zn) is given by
P(z|x) =
k

j=1

Θ

{i:zi=j}
f(xi|θj)πj(θj)dθj

z∈Z
k

j=1

Θ

{i:zi=j}
f(xi|θj)πj(θj)dθj ,
(6.5.2)
where Z is the set of all kn allocation vectors z.

6.5
Exercises
331
b. Show that
P(Zi = j|xi) =
pjmj(xi)
k
j=1 pjmj(xi)
,
(6.5.3)
where mj(x) = 
f(x|θj)π(θj)dθj, (j = 1, . . . , m) is the univariate marginal
distribution of xi.
c. Deduce that, if (6.5.2) and (6.5.3) are both available up to a normalizing
constant, the Bayes estimator IE[h(θ)|x] can be approximated by impor-
tance sampling, the zi’s (i = 1, . . . , n) being generated from the marginal
distributions of b., if IE[h(θ)|(x1, z1), . . . , (xn, zn)] is also known in closed
form.
d. Apply to the case of a mixture of exponential distributions,
k

j=1
pjλj exp(−λjx),
x > 0,
under the prior
λj ∼G(αj, βj),
j = 1, . . . , k ,
when the weights pj and the hyperparameters αj, βj are known. In partic-
ular, determine transforms h(λ1, . . . , λk) such that the conditional expec-
tations IE[h(θ)|(x1, z1), . . . , (xn, zn)] are known.
6.40 For a normal mixture, detail the reasoning leading to the conditional dis-
tributions (6.4.2) and (6.4.3) and give an explicit expression of IEπ[μi|x, z].
6.41 For a small sample size, run several simulations to compare Bayesian sam-
pling with a direct computation of the Bayes estimator for a mixture of two
normal distributions.
6.42 Show that conjugate priors cannot lead to a noninformative answer in the
case of a two-component normal mixture when the variances of the prior dis-
tributions go to ∞.
6.43 (Robert and Soubiran (1993))
Derive the formulas equivalent to (6.4.2)
and (6.4.3) for a mixture of multidimensional normal distributions. (Hint: Use
Section 4.4.1 for the choice of conjugate prior distributions and detail the
simulation of Wishart distributions.)
6.44 (Binder (1978)) Consider a sample x1, . . . , xn from the mixture
x ∼f(x|θ) =
k

i=1
pifi(x),
where the densities fi and the weights pi are known. The problem is to identify
the origins of the observations, g = (g1, . . . , gn), with
gj =
k

i=1
iIIzij=1
(1 ≤j ≤n).
a. Show that calculation diﬃculties also occur in this setting for the compu-
tation of the Bayes estimators.
b. Give the Bayes estimator of g when p ∼D(1/2, . . . , 1/2) and fi(x) =
ϕ(x; μi, 1) with μi ∼N(ξi, 1).
c. How can Gibbs sampling be implemented for this problem?

332
Bayesian Calculations
6
6.45 Adapt the Gibbs sampling techniques developed in Section 6.4 in the case
of a mixture of distributions to the case of a censored model, that is, for
observations y∗
i such that
y∗
i =
 yi
if yi ≤c,
c
otherwise,
and yi ∼f(y|θ), with f(·|θ) belonging to an exponential family.
6.46 (Robert et al. (1993))
A hidden Markov model generalizes the mixture
model studied in Example 6.1.5 and in Section 6.4 by introducing some de-
pendence between the observations x1, . . . , xt. When completing these observa-
tions by (unknown) missing state indicators zi, the model becomes hierarchical
(1 ≤i ≤t):
xi|zi, θ ∼f(x|θzi)
and (zi) constitutes a Markov chain on {1, . . . , K} with transition matrix
IP = (pjk), where
pjk = P(zi = k|zi−1 = j)
(2 ≤i ≤t)
(taking z1 = 1 for identiﬁability reasons). We also assume that f(·|θ) belongs
to an exponential family.
a. Give the likelihood of this model and deduce that neither maximum like-
lihood, nor Bayesian estimation with conjugate priors on θ and IP, can be
derived explicitly in this case.
b. Considering the particular case when f(·|θ) is N(ξ, σ2) with θ = (ξ, σ2),
show that a Gibbs sampling implementation with iterative simulations from
π(θ|x, z) and π(z|x, θ) is quite time-consuming because of π(z|x, θ).
c. Show that the fully conditional distributions π(zi|x, θ, zj̸=i) only depend
on zi−1 and zi+1 and are much easier to simulate.
d. Propose a Gibbs sampling algorithm for this model. Show that the condi-
tion pkj > 0 for all 1 ≤j, k ≤K is suﬃcient to ensure geometric conver-
gence of the chains (θ(m)) and (P(m)) to the true posterior distributions.
(Hint: Arguments similar to those of Exercise 6.25 can be used.)
6.47 (Robert et al. (1999a)) In the setting of Exercise 6.46, there exists a way
to simulate the whole chain z = (z2, . . . , zn) conditional on the parameters θ
and thus to implement a data-augmentation scheme. The representation of the
conditional distribution of z is called forward–backward and has been known
for a while in the signal processing literature (Baum and Petrie (1966)).
a. Establish the so-called backward recurrence relation (1 ≤i ≤n −1)
f(xi, . . . , xn|θ, zi = j) =
K

k=1
pjkf(xi|θj)f(xi+1, . . . , xn|θ, zi+1 = k) ,
(6.5.4)
with f(xn|zn = j) = f(xn|θj).
b. Derive from the backward formulas the probability P(z1 = j|x1, . . . , xn, θ)
under the assumption that z1 is marginally distributed from the stationary
distribution associated with the transition matrix IP.
c. Compute the probabilities P(zi = j|x1, . . . , xn, θ, z1, . . . , zi−1) (i = 2, . . . , n).

6.5
Exercises
333
Table 6.5.1. Frequencies of car passages for a sequence of one-minute intervals.
Number of
cars
0
1
2
3
4 or
more
Number of
occurrences
139
128
55
25
13
d. Conclude that the vector (z1, . . . , zn) can be simulated conditional upon
the observations and on θ, thus that the data-augmentation scheme can be
implemented in some hidden Markov models.
6.48 In a mixture setting, compare the performance (in terms of computing time)
of a Gibbs sampling with that of a more straightforward Metropolis–Hastings
algorithm.
Note 6.6.3
6.49 Does the decomposition of the noncentral chi-squared distribution proposed
in Example 6.6.1 allow for implementation of the Gibbs sampling? Give an
approximation by the Metropolis–Hastings algorithm.
6.50 (Heitjan and Rubin (1991)) Coarse data are deﬁned as an aggregation of
the observations in classes. Given a “complete” random variable yi ∼f(y|θ),
taking values in Y, and a partition Aj (j ∈I) of Y, the observations are xi = j
if yi ∈Aj.
a. Give a real-life justiﬁcation for this model.
b. Propose a Gibbs sampling algorithm in the case when f(·|θ) is a normal
distribution N(ξ, σ2) with θ = (ξ, σ2) and Aj = [j, j + 1) (j ∈ZZ).
Frequencies of car passages during a one-minute period have been observed
for 360 consecutive minutes and the resulting observations are given in Table
6.50.
c. Assuming a Poisson P(θ) distribution for this model, apply Gibbs sampling
to estimate the parameter θ for this data set and the prior π(θ) = 1/θ.
Note 6.6.4
6.51 In the setting of Example 6.3.8,
a. Show that the marginal distributions associated with the full conditionals
π(θ|λ) and π(λ|θ) satisfy
π(θ)
π(λ) = θ
λ,
θ, λ < B .
b. Deduce that the joint distribution corresponding to these two conditionals
is not deﬁned when B goes to inﬁnity.

334
Bayesian Calculations
6
Note 6.6.6
6.52 For the sequence (ˆθ(j))j produced by the EM algorithm (see Note 6.6.6 and
Robert and Casella (2004, Section 5.3.2)),
a. Show that
Q(ˆθ(j+1)|ˆθ(j), x) ≥Q(ˆθ(j)|ˆθ(j), x).
b. If k(z|θ, x) denotes the conditional distribution of z given x, show that
IEˆθ(j)

log

k(z|ˆθ(j+1), x)
k(z|ˆθj, x)
 ˆθ(j), x

≤0 .
(Hint: Use Jensen’s inequality.)
c. Conclude that
L(ˆθ(j+1)|x) ≥L(ˆθ(j)|x),
with equality holding if and only if Q(ˆθ(j+1)|ˆθ(j), x) = Q(ˆθ(j)|ˆθ(j), x).
6.6 Notes
6.6.1 Pseudo-random uniform generators.
The algorithms for generating random variables from any distribution all rely
on the generation of uniform random variables on [0, 1]. Since the production
of an exact i.i.d. sequence of uniform U([0, 1]) variables is impossible, there
are methods that use a fully deterministic process to produce a sequence imi-
tating an i.i.d. U([0, 1]) sequence in the sense that the deterministic sequence
is accepted as an i.i.d. U([0, 1]) sequence for all statistical tests. For instance,
the generator proposed in Ripley (1987) is a congruencial generator, deﬁned
as follows.
1. Start with an initial arbitrary seed x0
2. Iterate
xi
=
(69069xi−1 + 1) mod 232,
ui
=
2−32xi.
The corresponding sequence of ui’s can then be considered as an i.i.d. U[0,1]
sequence, although its actual support is ﬁnite.
Pseudo-random uniform generators are available on most machines and in
most languages, and can be used as such, even though some of these generators
are not thoroughly tested and may possess undesirable features (see Robert
and Casella (2004, Exercise 2.44)).
Marsaglia and Zaman (1993) have developed a simple uniform generator with
multiple seeds whose period is larger than 295. See Robert and Casella (2004,
Section 2.6.1) for details.
6.6.2 The BUGS and CODA softwares
An MCMC software has been developed by Spiegelhalter et al. (1995a,b,c)
at the MRC Biostatistics Unit in Cambridge, England. This software oﬀers
some possibilities to run a partly automated Gibbs sampler (BUGS stands for
Bayesian inference using Gibbs sampling). Further, it comes as a computer

6.6
Notes
335
language, which is C or R like, involves declarations about the model, the data,
and the prior speciﬁcations, including hierarchical modeling, and allows for
a large range of transforms of most standard distributions. BUGS produces
the Gibbs sample made of simulated values of the parameters, after an open
number of warmup iterations, the batch size being also open.
A major restriction on the prior modeling is that conjugate priors or log-
concave distributions must be used, for either standard simulation methods
or the ARMS algorithm of Gilks et al. (1995) to apply, but more complex
distributions can be handled by discretization of their support. The other
restriction is that improper priors cannot be used and must be replaced by
vague proper priors, that is, priors with large variances.
The BUGS software is completed with a convergence diagnosis software,9 CODA,
which contains some of the most common MCMC convergence assessment
techniques. This S-Plus package has been developed by Best et al. (1995) and
can be used independently from BUGS. The techniques selected in CODA are
described in Robert and Casella (2004, Chapter 11): they include the conver-
gence diagnostics of Gelman and Rubin (1992), Geweke (1992), Heidelberger
and Welch (1983), Raftery and Lewis (1992), plus plots of autocorrelation for
each variable and of cross-correlations between variables.
6.6.3 Hidden mixtures
The hierarchical decomposition (6.3.3), which is instrumental for the Gibbs
sampler, is also interesting for prior selection when the sampling distribution
does not belong to an exponential family and there is no conjugate family of
priors. For instance, this is the case for Student’s t- and noncentral chi-squared
distributions. Decomposition of f(x|θ) of the form
f(x|θ) =

f(x|θ, z)g(z|θ) dz
may then allow for a prior modeling on θ through conjugate priors (for f(x|θ, z)
or g(z|θ)). As in Section 3.3.3, we call this representation a hidden mixture, in
contrast with standard mixture problems where the mixture structure itself is
of interest. (See also Note 3.8.3.)
Example 6.6.1
Consider x ∼χ2
p(θ), an observation from a noncentral chi-
squared distribution. It can be written as a mixture
x|θ, z
∼
χ2
p+2z,
z|θ
∼
P(θ/2).
Therefore, only g(z|θ) depends on θ and a possible prior distribution on θ is
G(α, β), since it is conjugate for the Poisson distribution.
∥
Example 6.6.2
Consider x|μ, σ ∼T (m, μ, σ2), with θ = (μ, σ) unknown.
Following Dickey’s representation (1968),
x|θ, z ∼N(μ, z),
z|σ2 ∼IG(m/2, mσ2/2),
9 Both software programs are currently available on the web site of the MRC Biostatis-
tics Unit at www.mrc-bsu.cam.ac.uk.

336
Bayesian Calculations
6
we can propose
μ ∼N(ξ, τ 2),
σ2 ∼G(α, β),
as a prior distribution and we derive
z|x, θ
∼
IG

m + 1
2
, mσ2 + (x −μ)2
2

,
σ2|x, z
∼
G(α + (m/2), β + (m/2z)),
(6.6.1)
μ|x, z
∼
N

zμ + τ 2x
z + τ 2 ,
zτ 2
z + τ 2

.
The conditional distributions (6.6.1) directly allow for simulation through
Gibbs sampling. Notice the diﬀerence with the classical normal example (see
Section 4.4). In this case, σ2 has a gamma prior instead of an inverse gamma
distribution and, more importantly, μ and σ are a priori independent. The
conditional decomposition thus leads to a modeling that is more satisfactory
than in the normal case.
∥
Resorting to a hidden mixture, either for f(x|θ) or for π(θ), is obviously helpful
to simulate π(θ|x) through Gibbs sampling when the posterior distribution is
not available.
Example 6.6.3 (Example 6.6.2 continued) If, for robustness purposes,
the prior distribution is actually
μ ∼T (ν, ξ, τ 2),
σ2 ∼G(α, β),
the corresponding hidden mixture representation is
μ|δ ∼N(ξ, δ),
δ ∼IG(ν/2, ντ 2/2),
and the simulation of π(μ, σ|x) can be done by Gibbs sampling according to
the following conditional distributions:
z|x, θ
∼
IG

m + ν
2
, mσ2 + (x −μ)2
2

,
σ2|x, z
∼
G(α + (m/2), β + (m/2z)),
μ|x, z, δ
∼
N

δμ + τ 2x
δ + τ 2 ,
δτ 2
δ + τ 2

,
δ|θ
∼
IG

ν + 1
2
, ντ 2 + (x −μ)2
2

.
∥
6.6.4 Improper posteriors
As stressed in Note 1.8.3, prior distributions π such that

Θ
π(θ)f(x|θ)dθ = ∞
cannot be used. The diﬃculties in satisfying this condition for complex models
are that (a) an analytic check is often impossible; and (b) the conditional dis-
tributions derived from π(θ)f(x|θ) may well be proper. Take, for instance, the
case of Example 6.3.8: when B goes to inﬁnity, the joint distribution on (θ, λ)

6.6
Notes
337
is not deﬁned; the conditional distributions are, however, standard exponen-
tial Exp(λ) and Exp(θ) distributions (Exercise 6.51). The additional diﬃculty
is that a Gibbs sampler implemented with these conditional distributions may
fail to expose the impropriety problem (see Hobert and Casella (1996)).
Example 6.6.4 Consider the usual random-eﬀect model (1 ≤i ≤I, 1 ≤
j ≤J)
yij = θ + ui + ϵij,
ui ∼N(0, σ2), ϵij ∼N(0, τ 2) .
The corresponding Jeﬀreys prior is π(θ, τ 2, σ2) = 1/σ2τ 2. Then (see Robert
and Casella (1999, Example 7.4.3 and Problem 7.38)), the joint posterior dis-
tribution on (θ, τ 2, σ2) is not deﬁned while the conditional distributions are
well deﬁned and can be used in a Gibbs sampler.
∥
Despite the fundamental impossibility of using improper posteriors, which are
truly measures f(x|θ)π(θ) with inﬁnite mass, for Bayesian inference, there
exist settings where such measures can be of use. Indeed, it is possible to
augment the parameter θ artiﬁcially with an auxiliary parameter α, and to
introduce an improper prior π(α) such that the joint posterior π(α, θ|x) =
π(α)π(θ)f(x|θ) is also improper, while preserving the properness of the well
deﬁned π(θ|x) within the Markov chain.
Example 6.6.5 (Meng and van Dyk (1999)) A Student’s t-distribution with
parameter θ = (μ, σ), T (ν, μ, σ2), can be written as
x = μ + σy1/(νy2)1/2,
with
y1 ∼N(0, 1), y2 ∼χ2
ν .
(See Exercise 1.1 and Example 3.3.11.) If we introduce α > 0 such that
x|y2 ∼N(μ, ασ2/(νy2)),
y2 ∼αχ2
ν ,
this does not change the model under study because the quantity α/y2 does not
depend on α. The parameter α is thus nonidentiﬁable and, with an improper
prior on α, π(α) = α−1 exp(−β/α) say, the marginal posterior distribution on
α is equal to this prior: the joint posterior distribution on (θ, α) is not deﬁned.
It is nonetheless possible to create a Markov chain (y(t)
2 , θ(t), α(t)) via a simple
data-augmentation scheme applied to the full conditionals derived from
π(α)π(μ, σ)f(x|μ, α, σ, y2)f(y2|α)
such that (a) this σ-ﬁnite measure is stationary for this chain; and (b) the
subchain (θ(t)) converges to the well-deﬁned posterior distribution π(θ|x).
∥
Improper posteriors then appear as tools to speed up the exploration of the
parameter space Θ by way of null-recurrent or even transient Markov chains
in larger spaces. See Casella (1996), Meng and van Dyk (1999), Hobert (2000),
and Liu and Wu (1999) for more details.
6.6.5 MCMC algorithms for dynamic models
In Section 4.5, we introduced several dynamic models and pointed out that
the complex parameter space induced by the stationarity constraints as well
as the lack of closed-form likelihoods necessitates the use MCMC algorithms.

338
Bayesian Calculations
6
The state-space representations of Sections 4.5.3 and 4.5.4 and the reparam-
eterization of Lemma 4.5.4 are instrumental in designing Gibbs samplers in
these models.
For instance, in the AR(p) model, the ϱj’s (1 ≤j ≤p) are linear functions of
the partial autocorrelations ψk (1 ≤k ≤p), when the ψℓ(ℓ̸= k) are ﬁxed:
ϱj = akj + bkjψk ,
where (1 ≤ℓ≤i −1)
aii
=
ψi, bii = 0, aiℓ= a(i−1)ℓ−ψia(i−1)(i−ℓ), biℓ= 0,
if
i < k
aii
=
0, bii = 1, aiℓ= a(i−1)ℓ, biℓ= −a(i−1)(i−ℓ)
if
i = k
aii
=
ψi, bii = 0, aiℓ= a(i−1)ℓ−ψia(i−1)(i−ℓ), biℓ= b(i−1)ℓ−ψib(i−1)(i−ℓ)
if
i > k
and
aik = api , bik = bpi (1 ≤i ≤p) .
Therefore, if the ψi’s are simulated one by one, the likelihood (4.5.8) has a
normal structure
T

t=1
exp

−1
2σ2
!
xt −μ −
p

j=1
(aij + bijψi)(xt−j −μ)
"2
.
A similar conditional decomposition can be used for the MA(q) and ARMA(p,q)
models of Sections 4.5.3 and 4.5.4, taking advantage of the linear structure
of the state-space representation that preserves the normal structure. Alter-
natives based on the recursive representation (4.5.12) and on Metropolis–
Hastings moves have been studied in Billio et al. (1999).
6.6.6 More on mixture estimation
The importance of mixtures of standard distributions cannot be discounted
from a modeling perspective: mixture models are located at the boundary
between parametric and nonparametric modeling and they allow for the de-
scription of more complex phenomena (compared with standard distributions),
while preserving the parsimony principle (that is, using a reasonably small
number of parameters to describe a phenomenon). This was illustrated for the
construction of priors in Notes 3.8.3 and 6.6.3. Mixture structures appear in
Bayesian nonparametric analysis, as, for instance, with Dirichlet process pri-
ors (see Notes 1.8.2 and 6.6.7). They are equally instrumental in classiﬁcation
problems (see, e.g., Bensmail et al. (1999)) and in outlier detection (Verdinelli
and Wasserman (1992)).
The classical treatment of estimation of ﬁnite mixtures of distributions is
presented in Titterington et al. (1985) and MacLachlan and Basford (1987).
It can be traced to Pearson (1894), who proposed an estimation method based
on moments involving the resolution of a 9th-degree polynomial equation.
For maximum likelihood estimation, Dempster et al. (1977) and Redner and
Walker (1984) have developed a special algorithm called EM (for Estimation–
Maximization) that has been incredibly popular (see Meng and van Dyk (1997)
and MacLachlan and Krishnan (1997)). This algorithm is based on the same
completion as the Gibbs sampler. Given a completed likelihood Lc(θ|x, z), the

6.6
Notes
339
EM algorithm runs as follows.
At iteration m,
1. Compute
Q(θ|ˆθ(m), x) = IEˆθ(m)[log Lc(θ|x, z)|x] ,
where the expectation is with respect to k(z|ˆθm, x) (E-step) .
2. Maximize Q(θ|ˆθ(m), x) in θ and take (M-step)
θ(m+1) = arg max
θ
Q(θ|ˆθ(m), x).
It is validated by the fact that the observed likelihood increases at every step
(Exercise 6.52). The sequence (ˆθ(m))m thus converges to a stationary point of
the observed likelihood (which may be a local maximum or a saddlepoint).
See Robert and Casella (1999, Section 5.3.3) for more details.
Since the convergence of the EM algorithm depends on the starting point ˆθ(0)
and since this algorithm requires the computation of the expectation in the
E-step, some authors, including Broniatowski et al. (1983), Celeux and Diebolt
(1990), Qian and Titterington (1991), and Lavielle and Moulines (1997), have
proposed stochastic extensions of the EM algorithm.
From a Bayesian point of view, a more detailed study of MCMC techniques
for mixtures is proposed in Robert (1996a), Roeder and Wasserman (1997),
Robert and Mengersen (1999), Celeux et al. (2000), or Stephens (2000). In
particular, Celeux et al. (2000) show that the ordering of the parameters used
to ensure identiﬁability of the parameters may have disastrous eﬀects on the
resulting inference, and they devise speciﬁc loss functions to overcome the
nonidentiﬁability problem.
Gibbs sampling and other MCMC schemes have thus brought considerable
improvement to the Bayesian approach to mixture models, not only for esti-
mation, as shown above, but also for testing and modeling because tests on
the number of components of a mixture have been proposed (Mengersen and
Robert (1996)). Moreover, these studies have also exhibited interesting nonin-
formative extensions. As mentioned in Exercise 1.57, the peculiar properties
of mixtures prohibit the use of improper priors of the form
k

i=1
π1(μi, σi) .
In fact, in the decomposition (6.1.6) of the posterior distribution as a sum
over all possible partitions, some of these partitions do not attribute any ob-
servation to a given component i∗of the mixture. The prior distribution on
the corresponding parameters (μi∗, σi∗) must therefore be proper.
However, as shown in Mengersen and Robert (1996), an improper prior can
still be used if the component parameters are a priori dependent. For instance,
the mixture model can be reparametrized in terms of a global location-scale
parameter (μ, τ), with a corresponding prior π(μ, τ) = 1/τ. In this case, the
prior input can be reduced to the choice of a single hyperparameter ξ > 0.

340
Bayesian Calculations
6
Indeed, if (6.4.1) is written as
p1N(μ, τ 2) + (1 −p1)
p2N(μ + τθ1, τ 2σ2
1)
+(1 −p2) 
p3N(μ + τθ1 + τσ1θ2, τ 2σ2
1σ2
2) + . . .
,
an acceptable prior distribution is of the form pi ∼Be(1/2, 1/2), θi ∼N(0, ξ2),
and σi ∼(1/2)U[0,1] + (1/2)Pa(2, 1), the later distribution being justiﬁed by a
uniform distribution on either σi or 1/σi. (See Roeder and Wasserman (1997)
and Robert and Titterington (1998) for similar proposals.)
6.6.7 A Gibbs Sampler for Dirichlet Processes
The interest of using Dirichlet processes for Bayesian nonparametric estima-
tion has been mentioned in Note 1.8.2. We indicate here how a Gibbs sampling
implementation proceeds in a normal setting.
Consider xi ∼N(θi, σ2
i ) (1 ≤i ≤n) with (θi, σ2
i ) ∼π and π distributed
as a Dirichlet process D(α, π0). As already mentioned in Note 1.8.2, π0 is
the prior expectation of π and α is a degree of concentration around π0. The
corresponding marginal distribution is a mixture of normal distributions, with
a random number of components ranging from 1 up to n. That the number
of components can be as high as the sample size is a reﬂection of the lack of
constraints on the model, and this can be related to the fact that the usual
kernel estimator always uses n components. Another important consequence
of this modeling is that the prior conditional distributions of the (θi, σ2
i )’s can
be expressed as
π[(θi, σ2
i )|(θj, σ2
j )j̸=i]
=
α(α + n −1)−1π0(θi, σ2
i )
+(α + n −1)−1 
j̸=i
II((θi, σ2
i ) = (θj, σ2
j )).
(6.6.2)
The decomposition (6.6.2) exhibits the moderating eﬀect of the Dirichlet prior:
new values of (θ, σ2) only occur with probability α/(α + n −1).
A similar conditional distribution can be obtained a posteriori, namely, that
for observations x1, . . . , xn,
π[(θi, σ2
i )|(θj, σ2
j )j̸=i, xi] = qi0π0(θi, σ2
i |xi)
+

j̸=i
qijII((θi, σ2
i ) = (θj, σ2
j )),
(6.6.3)
where qi0 + 
j̸=i qij = 1 and (i ̸= j)
qi0 ∝α

e−(xi−θi)2/2σ2
i σ−1
i
π0(θi, σ2
i )dθidσ2
i ,
qij ∝e−(xi−θj)2/2σ2
j σ−1
j
.
For the conditional distributions (6.6.3), (θi, σ2
i ) is a new parameter with prob-
ability qi0 and is equal to another parameter with probability 1 −qi0. There-
fore, a Gibbs sampling implementation can proceed by simulating successively
from those conditionals for all i’s, and propose as a marginal distribution for
(x1, . . . , xn) a mixture of k normal distributions, where k is the number of
diﬀerent values of simulated (θi, σ2
i ). (Notice that the number k will vary at
each iteration.)
Another consequence of this representation is that, if we are interested in the
predictive density f, we can simulate a sample of size T from π(θ, σ2|x1, . . . , xn),

6.6
Notes
341
(θ(t), σ(t)2) (t = 1, . . . , T ), by simulating successively (θi, σ2
i ) (1 ≤i ≤n) ac-
cording to (6.6.3) and (θn+1, σ2
n+1) according to
π(θn+1, σ2
n+1)
=
π[(θn+1, σ2
n+1)|(θi̸=n+1, σ2
i̸=n+1)]
=
α(α + n)−1π0(θn+1, σ2
n+1)
+(α + n)−1
n

j=1
II((θn+1, σ2
n+1) = (θj, σ2
j )).
The predictive density can then be estimated by
1
T
T

t=1
f(x|θ(t), σ(t)2) ,
(6.6.4)
and is therefore of the same order of complexity as a kernel density estimator,
since it involves formally T terms. In fact, the sample of the (θ(t), σ(t)2) will
include a few values simulated according to π0(θn+1, σ2
n+1) and most of the
values of the (θi, σ2
i ) (1 ≤i ≤n) that are also simulated according to π0,
but with replications. Improvements upon this direct implementation of the
Dirichlet process prior are suggested in Escobar and West (1995), such as a
derivation of the average number of components in the distribution of the
(θi, σ2
i ) (1 ≤i ≤n). However, the choice of the hyperparameters is quite
crucial to the performances of the resulting estimator.


CHAPTER 7
Model Choice
“Right this minute, wherever he is, Galad is puzzling over something he
may never have faced before. Two things that are right, but opposite.”
Robert Jordan, The Fires of Heaven, Book V of the Wheel of Time.
7.1 Introduction
As pointed out in Chapter 5, model choice can be considered a special
case of testing and, still, we feel this problem deserves special treatment
(and, hence, a separate chapter). Why is this so? Before embarking on a
more precise deﬁnition of what model choice is, we present below several
arguments in favor of this separate treatment, and which, we hope, can
be understood with only the vague idea that model choice deals with the
comparison, and maybe the selection, of models.
From a conceptual point of view, the inferential action takes place on
a wider scale than in Chapter 5: we are now dealing with models, rather
than with parameters. For instance, there seems to be more at stake when
comparing an exponential model with a Weibull model than when deciding
whether a parameter θ is equal to 1, say. In other words, in contrast with the
other chapters of this book, the sampling distribution f(x) is unknown to
a larger extent than simply depending on an unknown (ﬁnite dimensional)
parameter.
From a modeling point of view, model choice often appears to be closer to
estimation than to regular testing. While we saw in Chapter 5 that testing
H0 : θ ∈Θ0 is also equivalent to estimating the indicator function IIΘ0,
model choice may simultaneously involve many possibilities, M1, . . . , Mp
say, and the decision about “the” model is equivalent to estimating the
index μ ∈{1, . . . , p} of this model (or, more exactly, getting the posterior
distribution of the index). Obviously, many settings require a ﬁrm and
precise decision about which model is right (meaning, which model is the
most appropriate to the data at hand), but this does not sound as deﬁnitive
as deciding whether H0 is true.

344
Model Choice
7
From a computational point of view, model choice involves more complex
structures that, almost systematically, require advanced tools such as those
presented in Chapter 6. Hence the break between Chapter 5 and the current
chapter, which also allows us to come back to the computation of Bayes
factors and pseudo-Bayes factors using Monte Carlo and MCMC methods
(Section 7.3). In fact, the comparison between models implies recourse to
even more advanced tools than those of Chapter 6, and we shall introduce
in Section 7.3.4 simulation methods which handle collections of parameter
spaces (also called spaces of varying dimensions), specially designed for
model comparison.
At last, the larger inferential scope mentioned in the ﬁrst point means
that we are leaving for a while the well charted domain of solid parametric
models: we will see repeatedly in this chapter settings where the “true”
distribution f is not known, and where we are trying to assess the distance
between f and one (or several) family of distributions {fθ; θ ∈Θ}. For
instance, in the goodness-of-ﬁt tests of Section 7.6, we need to call for
a nonparametric estimate of f. Similar issues will be raised in variable
selection (Section 7.5), where an embedding model, diﬀerent from the true
model, may be introduced.
Obviously, there is also much in common with Chapter 5, since the tools
used here are mostly the same, namely posterior probabilities and Bayes
factors. Many authors actually minimize the diﬀerence between regular
testings and model choice on this basis. See, for instance, Berger and Per-
icchi (2001), whose survey on model choice mainly proposes examples with
tests of null hypotheses like H0 : θ = 0.
Model choice, and the related topics of variable selection and goodness-
of-ﬁt tests, have been the subject of considerable eﬀort in the past years,
and this partly owes to the introduction of new computational methods,
but we can only give here a partial indication of this eﬀort. We thus refer
the reader to Racugno (1999) for deeper treatments on this topic.
7.1.1 Choice between models
Model choice seems to elude the Bayesian paradigm in that the sampling
distribution f is itself uncertain, making it diﬃcult to condition on the
observation x. We will feel more acutely this shift of paradigm in Section
7.6 when the question is: Does f belong to the family {fθ; θ ∈Θ}? the al-
ternative being completely open. First, consider the more restricted setting
where several (parametric) models are in competition,
Mi : x ∼fi(x|θi),
θi ∈Θi,
i ∈I ,
the index set I being possibly inﬁnite. This reduced perspective is less
puzzling from a Bayesian point of view, in the sense that a prior distribution
can be constructed for each model Mi as if it were the only and true model
under consideration.

7.1
Introduction
345
1.0
1.5
2.0
2.5
3.0
3.5
0.0
0.5
1.0
1.5
2.0
speeds
Figure 7.1.1. Histogram of the galaxy dataset of Roeder (1992).
In the simplest case, the choice is between a small number of models that
have been chosen for convenience, historical or more motivated reasons.
Example 7.1.1 We introduced in Example 1.1.5 a dataset of Lenk (1999)
relating the monthly unemployment rate with the monthly number of acci-
dents in Michigan from 1978 to 1987. At a cursory reading, before looking
at the connection between these two variates, one may oppose
M1 : N ∼Poi(λ),
λ > 0
to
M2 : N ∼Neg(m, p),
m ∈IN∗, p ∈[0, 1]
as possible models for the number of accidents N in a given month.
∥
In more complex cases, the number of potential models may be large
because the available information is too limited to eliminate most of them.
We are then closer to the nonparametric perspective.
Example 7.1.2 A dataset used in most papers on mixture estimation is
the galaxy dataset. First treated by Roeder (1992), it has been analyzed in
Chib (1995), Escobar and West (1995), Phillips and Smith (1996), Richard-
son and Green (1997), Roeder and and Wasserman (1997) and Robert and
Mengersen (1999), among others. It consists in 82 observations of galaxy
velocities, described in Figure 7.1.1. For astrophysical reasons, the distribu-
tion of this dataset can be represented as a mixture of normal distributions
whose number of components k is unknown. (A component of the mixture
is to be interpreted as a cluster of galaxies.) The models in contention are
thus
Mi : nj ∼
i

ℓ=1
pℓi N(μℓi, σ2
ℓi) ,
(7.1.1)
where i varies between 1 and some arbitrary upper bound.
∥
In other settings, including variable selection (Section 7.5), the variety
of models stems from the large number of combinations of covariates (or
explanatory variables) that could be included in the model.

346
Model Choice
7
Table 7.1.1. Orange tree circumferences (in millimeters) against time (in days)
for 5 trees. (Source: Gelfand (1996)).
tree
number
time
1
2
3
4
5
118
30
33
30
32
30
484
58
69
51
62
49
664
87
111
75
112
81
1004
115
156
108
167
125
1231
120
172
115
179
142
1372
142
203
139
209
174
1582
145
203
140
214
177
Example 7.1.3 (Gelfand (1996)) For 5 orange trees, the growth of tree i
is measured through the circumferences yit at diﬀerent times Tt, resulting
in the data of Table 7.1.1. The models under scrutiny are (i = 1, · · · , 5,
t = 1, . . . , 7)
M1
:
yit ∼N(β10 + b1i, σ2
1)
M2
:
yit ∼N(β20 + β21Tt + b2i, σ2
2)
M3
:
yit ∼N

β30
1 + β31 exp(β32Tt), σ2
3

M4
:
yit ∼N

β40 + b4i
1 + β41 exp(β42Tt), σ2
4

,
where the bji’s are random-eﬀects, distributed as N(0, τ2). Such models
start from the plain individual eﬀect—no time eﬀect in M1, to the linear
time eﬀect in M2, to a non-linear time eﬀect in M3, with individual eﬀects
in M4.
∥
As should be obvious from Example 7.1.3, there often is a high degree
of arbitrariness involved in the selection of the models to choose from.
Similarly, in Example 7.1.2, the normality assumption corresponds to a
convenience choice, rather than being motivated by astrophysical reasons.
Examples 7.1.1 to 7.1.3 expose a fundamental diﬃculty with model choice
issues, namely, that while no model is true, several models may be appro-
priate. To be forced to choose one and only one model thus reproduces the
dilemma encountered in Chapter 5, where a test procedure taking values
in {0, 1} did not sound like the right answer. (This problem will ﬁnd a rad-
ical solution in Section 7.4 by refusing the choice of a particular model.)
In both Examples 7.1.2 and 7.1.3, we also face an additional embedding
problem, namely, that some models are submodels of others. For instance,
in Example 7.1.2, a k component mixture is a submodel of a (k+p) compo-
nent mixture, the p remaining components being associated with weights
0. From a modeling point of view, the larger model should be preferred,

7.1
Introduction
347
while, from a statistical point of view, this is not so clear, given that more
parameters need to be estimated from the same sample! The model choice
criterion must thus include parts that weight the ﬁt, as well as parts that
incorporate the estimation error.
7.1.2 Model choice: motives and uses
As motivated by the previous examples, model choice is not a monolithic
estimation procedure, but can be undertaken for various reasons, which
are not always obvious for (or made explicit by) the experimenter (or the
“client”), and which therefore make the construction of a strict decision-
theoretic framework very nearly impossible. Among these possible reasons,
we can identify the choice of a model as
(i) a ﬁrst step in model construction, as in Example 7.1.1, where a few
models come to mind and the experimenter wants to decide which one
ﬁts “best” the data at hand. This perspective is just one step beyond
nonparametric Statistics, in the sense that there is no reason to believe
that one of these models is correct.
(ii) conversely, a last step of model checking, as in Example 7.1.3. A model
or a family of models has been selected for various theoretical and
practical reasons, and one wants to know whether the data agrees
with this type of model. This is also the domain of goodness-of-ﬁt tests,
where the model is not clearly deﬁned outside the null hypothesis (as
detailed in Section 7.6).
(iii) a call for model improvement, as in the move from M1 to M2 or from
M3 to M4 in Example 7.1.3. Given a model, possibly validated by a
goodness-of-ﬁt test, the goal is to introduce possible reﬁnements in the
model to improve the ﬁt, or, in other words, to create an embedding of
the existing model in a class of models to check whether the current
model is good enough.
(iv) the reverse need of model pruning,1 where the current model is deemed
to be too complicated to be of practical use, as for instance in Example
7.1.2 with k = 50, and where, for parsimony reasons, simpler submod-
els are examined to see whether they ﬁt the data well enough. This
is in particular the setting for variable selection techniques, where a
whole range of covariates is proposed, and the aim is to reduce those
to a few important covariates.
(v) a simpler model comparison, when a few models are proposed because
they ﬁtted correctly other samples and one wonders which of these
models best ﬁts the current sample, as in Example 7.1.1.
(vi) a more ambitious purpose of hypothesis testing, as in scientiﬁc settings,
where several models are built from theoretical considerations and then
1 This expression makes literal sense when considering the tree of possible models in
variable selection, which needs to be pruned of most of its branches!

348
Model Choice
7
tested through specially designed experiments. (This is the case, for
instance, of Einstein’s theory of gravitation versus Newton’s, or of the
cosmological theories of expansion versus contraction of the Universe.)
(vii) a more limited requirement of prediction eﬃciency, as, for instance, in
ﬁnance. Contrary to (vi), the models are not considered per se since
the experimenter is only interested in the prediction performances of
diﬀerent models. In the setting of Example 7.1.2, one could think for
instance of the probability to allocate a new galaxy to the correct
group of galaxies.
The applications of model choice are obviously as wide as those of Statis-
tics, since there are very few cases when a given model or a parametric
family is accepted by one and all! Let us mention here a few settings where
model choice is particularly necessary: image analysis, when comparing
diﬀerent neighborhood structures (Cressie (1993)); graphical models and
expert systems, when considering removing some links between variables
(Spiegelhalter et al. (1993), Cowell et al. (1999)); variable dimension mod-
els, as in ARMA(p, q) models where the lags p and q are unknown; causal
inference, where the question is to decide whether A has an eﬀect on B,
given a set of variables C1, . . . , Cp (Shafer (1997), Wasserman and Robins
(2000)).
7.2 Standard framework
7.2.1 Prior modeling for model choice
As in other settings, the standard Bayesian solution is to put a prior dis-
tribution on the unknown, which means extending the prior modeling from
parameters to models. The parameter space associated with the set of mod-
els (7.1.1) can be written as
Θ =
9
i∈I
{i} × Θi ,
(7.2.1)
the model indicator μ ∈I being now part of the parameters. So, if one can
allocate probabilities pi to the indicator values, that is, to the models Mi
(i ∈I), and then deﬁne priors πi(θi) on the parameter subspaces Θi, things
fold over by virtue of Bayes’s theorem, as usual, since we can compute
p(Mi|x) = P(μ = i|x) =
pi

Θi
fi(x|θi)πi(θi)dθi

j
pj

Θj
fj(x|θj)πj(θj)dθj
.
(7.2.2)
While a common solution based on this prior modeling is simply to take
the (marginal) MAP estimator of μ, that is, to determine the model with

7.2
Standard framework
349
the largest p(Mi|x), or even to use directly the average

j
pj

Θj
fj(y|θj)πj(θj|x)dθj =

j
p(Mj|x) mj(y)
(7.2.3)
as a predictive density in y, a deeper-decision theoretic evaluation is often
necessary.
There are diﬃculties with the usual Bayesian formalism, or at least with
the prior modeling, in this setting: the solution based on the representa-
tion (7.2.1) of the collection of models requires the construction of a prior
distribution (πi, pi) for each i ∈I, which is delicate when I is inﬁnite.
Moreover, these priors πi must all be proper because there is no unique
scaling for improper priors, as already seen in Chapter 5. In addition, if
some models are embedded into others, that is, if Mi0 ⊂Mi1, there should
be some coherence in the choice of πi0 given πi1, and maybe also in the
choice of pi0 given pi1. For instance, if M1 = M2 ∪M3, one could argue
that p(M1) = p(M2) + p(M3), or at least p(M1) ≥p(M2) + p(M3),
should hold. Similarly, if two models Mi0 and Mi1 are not embedded in
one another, the prior modeling should account for the possibility of a
third model Mi2 embedding both Mi0 and Mi1. (In econometrics, this
technique of creating a super-model is called encompassing.) A last impor-
tant point, which is speciﬁc to model choice, is that parameters common
to several models must be treated as separate entities. This point is often
neglected in the literature, including Jeﬀreys (1961), because common pa-
rameters can be integrated out using the same prior, even when the prior
is improper. A milder evasion of the above recommendation is to argue,
as in Berger and Pericchi (1998), that, for common parameters, the same
improper prior should be used, thus removing the issue of the normalizing
constant (Exercise 7.4), but we cannot advise such an ad-hoc solution on
a general basis.
Example 7.2.1 (Example 7.1.3 continued) Consider models M1 and
M2: while β10 and β20 are both intercepts, and σ2
1 and σ2
2 are both vari-
ances, they must be distinguished because of the additional term β21Tt in
model M2. In particular, if M2 is the true model, β10 corresponds to β20
shifted by the average of the β21Tt’s, while σ2
1 is larger than σ2
2 to account
for the poorer ﬁt (see Exercise 7.5).
∥
From a decision-theoretic perspective, the inferential problem is also hard
to formalize because, as pointed out in Section 7.1.2, there are many con-
ﬂicting potential uses of model choice. Overall, model choice appears as one
part of a global decision process, where a model is constructed, improved
by reduction or extension (as in points (iii) and (iv) above), then selected
for future uses as the true model. To build a loss function that incorporates
all these stages is clearly impossible, but the stress can be placed on the
selection part. For instance, model averaging as in (7.2.3) is not acceptable
in this sense because, by preserving all models which are compatible with

350
Model Choice
7
the data, one adopts an estimation perspective which amounts to a lack of
decision! If no (or not enough) information can be gathered about the cost
of choosing the wrong model, and thus if a decision-based loss function
L((μ, θμ), (d, ϑ)) cannot be constructed, a solution supported at the end
of Section 7.1.1 is to ﬁght overﬁtting by introducing in the loss function
penalty terms on the number of parameters (that is, the dimension) of the
model, as detailed in Section 7.2.3. See also Carota, Parmigiani and Polson
(1996) for a decision-theoretic attempt at model criticism, which pertains
more to point (iii) above, and which uses Kullback–Leibler divergences as
in Section 7.5.
A third type of diﬃculty is associated with the computation of predic-
tives, marginals and other quantities related to the model choice proce-
dures. This is not an issue restricted to model choice (see Chapter 6),
obviously, but there are speciﬁcities that call for tailor-made solutions:
- the parameter spaces are often inﬁnite-dimensional, as in (7.1.1), which
may cause measure-theoretic complications.
- The computation of posterior or predictive quantities involves integra-
tion over diﬀerent parameter spaces and thus increases the computa-
tional burden, since there is no time savings from one subspace to the
next.
- The representation of the parameter space as a direct sum of diﬀerent
subspaces requires more advanced Markovian techniques to implement
MCMC algorithms.
- In some settings, such as variable selection, the size of the collection of
models is so large that some models cannot be explored.
In all but the simplest models, there is thus a need for computational (or
approximation) techniques because no analytic representation is available.
Section 7.3 will detail these techniques.
7.2.2 Bayes factors
Once the modeling representation (7.1.1) is accepted, and the correspond-
ing priors are selected, the inferential issues are reduced to a generic testing
problem. The solution proposed by Kass and Raftery (1995), as well as
Berger and Pericchi (2001), is then to call for Bayes factors, e.g.,
B12
=
P(M1|x)
P(M2|x)
:P(M1)
P(M2)
=

Θ1
f1(x|θ1)π1(θ1)dθ1

Θ2
f2(x|θ2)π2(θ2)dθ2

7.2
Standard framework
351
for the comparison of models M1 and M2. The setting is therefore similar
to Section 5.2 and, consequently, the diﬃculties are also the same, only
exacerbated here because we are dealing with more models (possibly an in-
ﬁnite number of them!) and are much more likely to call for noninformative
priors. Notice that the comparison of models based on Bayes factors can
proceed one pair (Mi, Mj) at a time because of the coherence property of
Bayes factors, that is Bπ
ij = Bπ
ikBπ
kj, which ensures that the model ordering
is transitive. (Recall, though, that this is not the case for the pseudo-Bayes
factors of Section 5.2.6.)
For exactly the same reason as in Section 5.2.5, improper priors cannot
be used (unless, as mentioned above, they bear on parameters common
to all models). Moreover, vague priors, that is, proper priors with a large
variance—which is the representation adopted in BUGS, see Note 6.6.2—do
not solve the diﬃculty, as already shown with the Jeﬀreys–Lindley paradox
(Section 5.2.5).
Example 7.2.2 (Example 7.1.1 continued) Consider the priors
π1(λ) = Ga(α, β) ,
π2(m, p) = 1
M II{1,···,M}(m)II[0,1](p) ,
where the second prior is uniform over the parameter space Θ2. Then the
Bayes factor
Bπ
12
=
βα
Γ(α)
 ∞
0
λα+x−1
x!
e−λβdλ
1
M
M

m=1
 1
0
 m
x −1

px(1 −p)m−xdp
=
Γ(α + x)
x! Γ(α) β−x: 1
M
M

m=1
x
(m −x + 1)(m + 1)
=
M(m + 1)(x + α −1) · · · α
x(x −1) · · · 1
β−x:
M

m=1
x
m −x + 1
depends on the choice of α, β when both go to 0 (Exercise 7.10).
∥
The solution to this fundamental diﬃculty with improper priors is to use
approximative Bayesian solutions, calling for minimal training samples or
virtual observations, as in Section 5.2.6. (Notice that one of the early pro-
posals of pseudo-Bayes factors appeared in Spiegelhalter and Smith (1980)
for model choice in linear and log-linear models.) Intrinsic and fractional
Bayes factors can then be proposed (with the same provisos as those in
Section 5.2.6) as evaluation of the models under improper priors.
Example 7.2.3 (Example 7.1.2 continued)
As detailed in Example
5.2.19, there is no minimal training sample for a mixture model, whatever

352
Model Choice
7
the number of observations is. Therefore, intrinsic and fractional Bayes
factors cannot be used there.
A ﬁrst solution, already used in Diebolt and Robert (1994) for simu-
lation purposes and further validated by Wasserman (1999), is to impose
that the sample (x1, . . . , xn) is such that enough observations (in the sense
of the training samples) come from each component (see also Green and
Richardson (1998)). While being reasonable if all components are clearly
distinguishable, this solution creates a dependence between the observa-
tions (which remain exchangeable), and the computation of the pseudo-
Bayes factors under this assumption becomes very expensive.
The second solution, adopted in Mengersen and Robert (1996) when
testing k = 1 versus k = 2, is to put a non-informative prior π(μ, τ) on the
global location-scale parameter of the model, and to express the parameters
of all components as perturbations of this location-scale parameter, using
proper priors. Since (μ, τ) is common to all components, the improper prior
does not raise so keenly the normalization issue.
∥
7.2.3 Schwartz’s criterion
In order to comment on penalty terms and (crude) approximations to
Bayesian solutions, we need to brieﬂy consider asymptotic approximations
to Bayes factors.2
Consider the Laplace expansion discussed in Section 6.2.3,

Θ
exp{n h(θ)}dθ = exp{n h(ˆθ)}(2π)p/2n−p/2|H−1(ˆθ)| + O(n−1) ,
where p is the dimension of Θ, ˆθ is the maximum of h and H is the Hessian
of h. If we apply this approximation to the Bayes factor by expanding both
numerator and denominator, we get an approximation of the Bayes factor,
Bπ
12 ≃L1,n(ˆθ1,n)
L2,n(ˆθ2,n)

H−1
1 (ˆθ1,n)
H−1
2 (ˆθ2,n)

1/2  n
2π
(p2−p1)/2
,
where p1 and p2 are the dimensions of Θ1 and Θ2, L1,n and L2,n are the
likelihood functions based on n observations, and ˆθ1,n and ˆθ1,n are the
maxima of L1 and L2, respectively. Therefore,
log(Bπ
12) ≃log λn + p2 −p1
2
log(n) + K(ˆθ1,n, ˆθ2,n) ,
(7.2.4)
where λn is the standard likelihood ratio for the comparison of M1 with
M2,
λn = L1,n(ˆθ1,n)/L2,n(ˆθ2,n),
2 This section aims at illustrating the link between Bayesian approximation and usual
penalization criteria, not at promoting such criteria. It can thus be skipped on a ﬁrst
reading.

7.2
Standard framework
353
and K(ˆθ1,n, ˆθ2,n) denotes the remainder term.
This approximation leads to Schwartz’s criterion (1978):
S = −log λn −p2 −p1
2
log(n)
when M1 ⊂M2, if the remainder term K(ˆθ1,n, ˆθ2,n) is negligible compared
with both other terms, that is, is a O(1). (See Gelfand and Dey (1994, §8)
for an example where this term is not negligible.)
Recall that, for regular models, when M1 ⊂M2, the likelihood ratio is
approximately distributed as a χ2
p2−p1 distribution,
−2 log λn ≈χ2
p2−p1
if M1 is the true model (Lehmann and Casella (1998), Gouri´eroux and
Monfort (1996)). Since
P(M2 chosen|M1)
=
P(λn < c|M1)
≃
P(χ2
p2−p1 > −2 log(c)) > 0 ,
it follows, from a frequentist point of view, that a criterion based solely on
the likelihood ratio does not converge to a sure answer under M1. This is
why penalization factors have been added to the (log) likelihood ratio to
account for this bias, starting with Akaike’s (1983) criterion,
−2 log λn −α(p2 −p1) ,
(7.2.5)
where α = log 2 also corresponds to an approximation of Aitkin’s (1991)
procedure, where the author uses the data twice, ﬁrstly to build a proper
(pseudo-) prior by taking the posterior, and secondly to derive the Bayes
factor as if this construct were a genuine prior. (See Exercise 5.15.)
Schwartz’s criterion, also called BIC (for Bayes Information Criterion),
thus provides a cursory ﬁrst-order approximation to the Bayes factor, as
defended in Kass and Raftery (1995). However, we do not see the rele-
vance of this criterion in a Bayesian setting, since (a) the dependence on
the prior assumption disappears; and (b) the approximation only works for
regular models. For instance, in Example 7.1.2, the asymptotics of the log-
likelihood ratio −2 log λn are much more complex than the χ2
p2−p1 approx-
imation (see, e.g., Dacunha-Castelle and Gassiat (2000)) and Schwartz’s
criterion does not work. See Berger and Pericchi (2001) for other examples
of irregular likelihoods. Moreover, in non-i.i.d. structures, the deﬁnition
of both n and p may be ambiguous, as stressed by Spiegelhalter et al.
(1998). At a computational level, notice that Schwartz’s criterion requires
the derivation of the maximum likelihood estimates for all models.
Example 7.2.4 (Example 7.1.2 continued) If we decompose Schwartz’s
criterion as
S
=
log

L2,n(ˆθ2,n)/L1,n(ˆθ1,n)
 
−p2 −p1
2
log(n)
=
log L2,n(ˆθ2,n) −p2
2 log(n) −log L1,n(ˆθ1,n) + p1
2 log(n) ,

354
Model Choice
7
the part owing to model Mi can be identiﬁed as
Si = log Li,n(ˆθi,n) −pi
2 log(n) .
For Mk associated with a k component model, pk = 3k −1. In the case of
the galaxy data, Raftery (1996) gets
S1 = −271.8 ,
S2 = −249.7 ,
S3 = −256.7 ,
S4 = −263.6 ,
using the EM algorithm (see Note 6.6.6) to obtain approximations of the
maximum likelihood estimates ˆθi,n when k > 1. This means that, for
Schwartz’s criterion, the model with two components is to be preferred
to the others.
∥
7.2.4 Bayesian deviance
Spiegelhalter, Best and Carlin (1998) have developed a Bayesian alterna-
tive to both AIC (Akaike’s Information Criterion) and BIC, based on the
deviance and called DIC (for Deviance Information Criterion). This crite-
rion is more satisfactory than the two former alternatives because it takes
into account the prior information and gives a natural penalization factor
to the log-likelihood. Besides, it also allows for improper priors, since each
model is considered separately.
As mentioned earlier in Section 7.2.3, for a model f(x|θ) associated with
a prior distribution π(θ), the deviance3 D(θ) = −2 log(f(x|θ)) is not a good
discriminating measure, given its bias toward higher dimensional models.
The same obviously applies to its posterior distribution. Spiegelhalter et
al. (1998) introduce a penalized deviance,
DIC
=
IE[D(θ)|x] + pD
(7.2.6)
=
IE[D(θ)|x] + {IE[D(θ)|x] −D(IE[θ|x])} .
The criterion is then implemented for model evaluation as the smaller the
value of DIC, the better the model.
The factor IE[D(θ)|x] in (7.2.6) can be interpreted as a measure of ﬁt,
while pD is a measure of complexity, also called the eﬀective number of
parameters. Since DIC = D(IE[θ|x]) + 2pD, the analogy with Akaike’s In-
formation Criterion (7.2.5) is clear. As shown in Spiegelhalter et al. (1998),
in a non-hierarchical framework where the posterior distribution of θ is
approximately normal, DIC and AIC are in fact equivalent. Notice also
that DIC reproduces the usual decomposition of the squared error as the
3 In generalized linear models (McCullagh and Nelder (1989)), the deviance is usu-
ally calibrated by an additional term f(y) like f(y|ˆθ(y)) where ˆθ(y) is an arbitrary
estimator of θ. When this term does not depend on the model, or is chosen for a
particular model such as a “full” model, the comparison of models based on D(θ) and
D(θ) + 2 log f(y|ˆθ(y)) are obviously the same.

7.2
Standard framework
355
squared bias plus the variance
IEθ[(δ −θ)2] = (IEθ[δ] −θ)2 + IEθ[(δ −IEθ[δ])2]
to a parameter-free framework (except for IE[θ|x], which depends on the
parameterization).
Example 7.2.5 (Spiegelhalter et al. (1998)
For a one-way analysis of
variance (i = 1, . . . , p)
yi = θi + σiϵi ,
ϵi ∼N(0, 1) ,
the divergence is D(θ) = 
i σ−1
i
(θi−yi)2. Therefore, if θi = θ (i = 1, . . . , p)
and π(θ) = 1,
IE[D(θ)|y1, . . . , yp] =
k

i=1
σ−1
i
(yi −IE[θ|y1, . . . , yp])2 + 1
(7.2.7)
where IE[θ|y1, . . . , yp] = 
i σ−1
i
yi/ 
i σ−1
i
. And pD = 1 in this case.
If, instead, we consider the model θi ∼N(μ, τ2) with known hyperpa-
rameters μ and τ,
IE[D(θ)|y1, . . . , yp] =
k

i=1
σ−1
i
(1 −ϱi)2(yi −μ)2 +
k

i=1
ϱi ,
(7.2.8)
where ϱi = σ2
i τ 2/(σ2
i + τ 2).
∥
The practical computation of the Bayesian deviance most often requires
the call to an MCMC algorithm, since settings such as those of Example
7.2.5 and others developed in Spiegelhalter et al. (1998) are fairly rare.
This computation is quite straightforward to implement, however, once an
MCMC sample (θ(1), . . . , θ(T )) has been generated, since IE[D(θ)|y1, . . . , yp]
is simply a posterior expectation of an explicit function of θ.
Example 7.2.6 (Spiegelhalter et al. (1998))
A study on lip cancer in
56 regions of Scotland relates the observed numbers of cases yi with the
expected national numbers Ei as
yi ∼P(λiEi) ,
λi = exp(θi) being the area-speciﬁc risk of lip cancer. Possible covariates are
xi, the percentage of the population working outdoors, and the geographic
location of the region, represented by a list Ai of adjacent regions. Some
models under consideration are then
M1 :
θi
=
α + βxi ,
M2 :
θi
=
ϕi ,
M3 :
θi
=
ϕi + βxi ,
where the ϕi’s are spatially correlated, that is
ϕi|ϕj, j ̸= i ∼N
⎛
⎝
j∈Ai
ϕj/ni, τ2/ni
⎞
⎠,

356
Model Choice
7
where ni denotes the number of adjacent regions. (This spatial model is
called the autoregressive spatial model and is often used in Spatial Statistics.
See Besag (1974) or Cressie (1992).)
Under noninformative priors for the hyperparameters (except for τ2
which is distributed as a IG(1, 1) random variable), the MCMC sampler
produces approximate DIC’s of 242.8, 88.5 and 89.0 for the three models,
with eﬀective numbers pD equal to 2.1, 31.6 and 29.4, respectively. Models
M2 and M3 are thus similar, while being better than model M1. Notice
that, while the true number of parameters in model M1 is 2, the number
of parameters in models M2 and M3 is respectively 57 and 58.
∥
Spiegelhalter et al. (1998) suggest additional uses of the Bayesian de-
viance like deviance residuals. They also notice the lack of parameterization-
invariance of D(IE[θ|x]) and suggest using the canonical parameterization
in generalized linear models.4
7.3 Monte Carlo and MCMC approximations
As in other settings, the customary diﬃculty with the Bayesian approach
is the computation of integrals of the type
mi(x) =

fi(x|θi)πi(θi)dθi
(7.3.1)
and, more crucially, of ratios of integrals

f1(x|θ1)π1(θ1)dθ1

f2(x|θ2)π2(θ2)dθ2
,
not to mention the additional diﬃculties induced by the intrinsic and frac-
tional Bayes factors. The solutions presented in Chapter 6, namely the use
of asymptotic approximations, and of Monte Carlo and MCMC simulation
methods, obviously apply in this case. But, as mentioned earlier in this
chapter and illustrated in Chen et al. (2000), special methods have also
been devised for the computation of Bayes factors and related quantities.
7.3.1 Importance sampling
This technique, introduced in Section 6.2.2, is particularly well adapted to
the computation of predictive distributions like (7.3.1). Given a proposal
(or importance) distribution, with density proportional to g, and a sample
θ(1), . . . , θ(T ), the marginal density for model Mi, mi(x), is approximated
4 Notice that
replacing IE[θ|x] with the MAP estimate does
produce
a truly
parameterization-invariant criterion, with the drawback that this estimate is more
diﬃcult to derive than the posterior mean.

7.3
Monte Carlo and MCMC approximations
357
by
mIS
i (x) =
T

t=1
fi(x|θ(t))πi(θ(t))
g(θ(t))
T

t=1
πi(θ(t))
g(θ(t))
,
where the denominator takes care of the missing normalizing constant.
(Notice that, if g is a density, the expectation of π(θ(t))/g(θ(t)) is 1.)
A compelling incentive, among others, for using importance sampling in
the setting of model choice is that the sample (θ(1), . . . , θ(T )) can be recycled
for several models Mi if they all involve the same (type of) parameters.
(For instance, this is not possible for Examples 7.1.1 and 7.1.2.) See Chen
and Shao (1997) for an illustration on Bayes factors.
The variance of mIS(x) may be inﬁnite, as already discussed in Section
6.2.2. Raftery (1996) examines the choices of importance functions in the
present setting for a given model with sampling density f(x|θ) and prior
distribution π(θ). The ﬁrst natural choice is g(θ) = π(θ), which leads to
the estimator of the marginal
mIS(x) = 1
T

t
f(x|θ(t)) ,
but is often ineﬃcient if the data is informative because most of the sim-
ulated values θ(t) fall outside the modal region of the likelihood. (In the
extreme case when π is improper, this choice is, of course, impossible.) Ob-
viously, given that the tails of π are usually wider than those of π(θ|x),
inﬁnite variance problems are rather rare with this importance function.
A second possible choice is g(θ) = f(x|θ)π(θ). Since the associated esti-
mator is then
mIS(x) = 1
- 1
T
T

t=1
1
f(x|θ(t)) ,
(7.3.2)
that is, the harmonic mean of the likelihoods, mIS(x) provides an approx-
imation of the normalization constant of g. While this solution allows for
improper priors, as long as the posteriors are deﬁned, the corresponding
variance is often inﬁnite. A solution to this problem is to call for the so-
called defensive importance sampling by taking a mixture of g (or, rather,
of π(θ|x)) and of a distribution with fat tails, ϖ(θ):
(1 −ϱ)π(θ|x) + ϱϖ(θ) ,
ϱ ≪1 .
(See Hesterberg (1998) and Owen and Zhou (2000) for detailed treatments
of this technique.) For instance, ϖ(θ) = π(θ), as proposed by Newton and
Raftery (1994).
Another solution, proposed by Gelfand and Dey (1994), is to generate a

358
Model Choice
7
sample of θ(t)’s from the posterior and to use
mIS(x) = 1
- 1
T
T

t=1
h(θ(t))
f(x|θ(t))π(θ(t)) ,
(7.3.3)
rather than (7.3.2), where h is an arbitrary density (Exercise 7.18). The
estimator (7.3.3) has furthermore a ﬁnite variance if

h2(θ)
f(x|θ)π(θ)dθ < ∞.
Since h is a free parameter, it can (in principle) be chosen so that this
condition is satisﬁed.
7.3.2 Bridge sampling
Monte Carlo methods adapted to the estimation of ratios of normalizing
constants, or, equivalently, of Bayes factors, have been developed in the
past ﬁve years. We refer the reader to Chen et al. (2000, Chapter 5) for
a complete exposition on such methods and introduce here one solution
which pertains from importance sampling.
Bridge sampling was introduced in Meng and Wong (1996), and is based
on identities used in the physics literature: if both models cover the same
parameter space Θ, if π1(θ|x) = c1˜π1(θ|x) and π2(θ|x) = c2˜π2(θ|x), then
the equality
c2
c1
= IEπ2[˜π1(θ|x) h(θ)]
IEπ1[˜π2(θ|x) h(θ)]
(7.3.4)
holds for any bridge function h(θ) such that both expectations are ﬁnite
(Exercise 7.20). The bridge sampling estimator is then
BS
12 =
1
n1
n1

i=1
˜π2(θ1i|x) h(θ1i)
1
n2
n2

i=1
˜π1(θ2i|x) h(θ2i)
,
(7.3.5)
where the θji’s are simulated from πj(θ|x) (j = 1, 2, i = 1, . . . , nj).
For instance, if
h(θ) = 1/ [˜π1(θ|x)˜π2(θ1i|x)] ,
then BS
12 is a ratio of harmonic means, generalizing (7.3.2). Meng and Wong
(1996) have derived an (asymptotically) optimal bridge function
h∗(θ) =
n1 + n2
n1π1(θ|x) + n2π2(θ|x) .
This choice is not of direct use, since the normalizing constants of π1(θ|x)
and π2(θ|x) are unknown (otherwise, we should not need to resort to such
techniques!). Nonetheless, it shows that a good bridge function should cover
the support of both posteriors, with equal weights if n1 = n2.

7.3
Monte Carlo and MCMC approximations
359
Example 7.3.1 For generalized linear models, that is, models from expo-
nential families,
f(y|θ) = h(y) eθ·y−ψ(θ)
where the mean IE[y|θ] = ∇ψ(θ) is a function of covariates, x, of the
form ∇ψ(θ) = Ψ(xtβ), the choice of the link function Ψ is never easy.
For instance, when the regressed variable y takes values in {0, 1} and
IE[y|x] = P(y = 1|x), three common choices of Ψ are (McCullagh and
Nelder (1989)
– the logit link function, Ψ(t) = exp(t)/(1 + exp(t));
– the probit link function, Ψ(t) = Φ(t), that is, the c.d.f. of the N(0, 1)
distribution; and
– the log–log link function, Ψ(t) = 1 −exp(−exp(t)).
The three contending models are then
M1
:
y|x
∼
eyxtβ1
1 + eyxtβ1
M2
:
y|x
∼
Φ(xtβ2)y [1 −Φ(xtβ2)]1−y
M3
:
y|x
∼
exp{−(1 −y) exp(xtβ3)} [1 −exp{−exp(xtβ3)}]y .
If the prior distribution π on the βi’s is a normal Np(ξ, τ2Ip), and if the
bridge function is h(β) = 1/π(β), the bridge sampling estimate is then
(1 ≤i < j ≤3)
BS
ij =
1
n
n

t=1
Lj(βit|x)
1
n
n

t=1
Li(βjt|x)
,
where the βit are generated from πi(βi|x) ∝Li(βi|x)π(βi).
∥
In a special case when both priors are equal, except for a hyperparameter,
Gelman and Meng (1998) have constructed an improvement upon bridge
sampling, called path sampling and presented in Note 7.8.1.
7.3.3 MCMC methods
While importance sampling seems quite appropriate in this setting, MCMC
methods can also be used to generate samples from complex distributions.
For instance, the bridge sampling estimate can be based on MCMC samples
rather than on i.i.d. samples if the distributions πj(θ|x) are too involved.

360
Model Choice
7
Example 7.3.2 (Example 7.3.1 continued) For model Mj (j = 1, 2, 3),
the likelihood part of the posterior is
n

i=1
Ψ(xt
iβj)yi[1 −Ψ(xt
iβj)]1−yi .
In the case of the probit link function (j = 2), Ψ(t) = Φ(t), which is not
available in closed form. A natural Gibbs sampling solution is then to create
auxiliary variables zi ∼N(0, 1) such that Ψ(xt
iβ2) = IE
/
IIzi≤xt
iβ2
0
, that is,
to generate from the joint distribution
π(β2, z1, . . . , zn) ∝π(β2)
n

i=1
IIyi
zi≤xt
iβ2II1−yi
zi≥xt
iβ2 .
For both other link functions, a standard slice sampler (see Section 6.3.6)
can be used: for the logit model, ui ≤Ψ(xt
iβ1) can be inverted into
xt
iβ1 ≥log(ui/(1 −ui))i ,
while, for the log-log model, ui ≤Ψ(xt
iβ3) is equivalent to
xt
iβ3 ≥log(−log(1 −ui)) .
For the three models, the components of the βj’s are then simulated from
multidimensional truncated normal distributions.
∥
Therefore, the approximation (7.3.3) of the marginal distribution can be
based on an MCMC sample (θ(t)) from π(θ|x).
Example 7.3.3 (Example 7.2.1 continued) If the priors for the four
models are of the form (j = 1, . . . , 4)
πj(β·j, σ2
j , τ2
j ) ∝σ2
j τ 2
j e−2(σ−2
j
+τ −2
j
) ,
where β·j denotes the vector of βij’s for model j, Gelfand (1996) suggest
to evaluate the four models by generating a sample of θ(t)
j ’s from the cor-
responding posteriors, by approximating the predictive distributions as
ˆfj(y|y1, . . . , yn) = 1
T
T

t=1
fj(y|θ(t)
j ) ,
then by drawing samples from these predictives to see whether they agree
with the sample y1, . . . , yn. Table 7.3.3 shows the result of this experiment:
these ﬁgures exclude models M1 and M2, while both models M3 and M4
agree with the predictive intervals. Obviously, this is only a ﬁrst indica-
tor of ﬁt, which should be completed with a computation of the genuine
Bayes factors, but this empirical evaluation may allow for the elimination
of inadequate models.
∥

7.3
Monte Carlo and MCMC approximations
361
Table 7.3.1. Adequacy results for the four models of orange tree growths, as per-
centage of observations within the 50% and 90% predictive intervals. (Source:
Gelfand (1996).)
Model
50%
95%
M1
89
100
M2
29
51
M3
46
100
M4
60
86
Chib (1995) proposes to use the Gibbs sampler to approximate marginal
densities, based on the Bayes representation
log m(x) = log f(x|θ) + log π(θ) −log π(θ|x) ,
which holds for any value of θ. When θ = (θ1, θ2), and when both π(θ1|θ2, x)
and π(θ2|θ1, x) are available in closed form, including the normalization
constant, the Rao–Blackwellization argument of Section 6.3.4 gives an ap-
proximation to the marginal posterior π(θ1|x) as
ˆπ(θ1|x) = 1
T
T

t=1
π(θ1|θ(t)
2 , x) ,
where the θ(t)
2 ’s are generated from by a Gibbs sampling algorithm. (Notice
that the choice of the partition of θ in (θ1, θ2) is dictated by the availabil-
ity of both π(θ1|θ2, x) and π(θ2|θ1, x).) Chib’s (1995) approximation of
log m(x) is then
log f(x|θ) + log π(ˆθ) −log π(ˆθ2|ˆθ1, x) −log ˆπ(ˆθ1|x) ,
where ˆθ = (ˆθ1, ˆθ2) is an approximation of the MAP estimator of θ, for
instance. When both conditional densities are not available in closed form,
or when one of them misses its normalizing constant, Chib (1995) suggests
an extension to more blocks in the partition, but this is much more costly
(Exercise 7.23).
The major improvement brought by MCMC techniques to model selec-
tion is, however, their ability to deal with variable dimension models, that
is, with models Mk on diﬀerent parameter sets, with no intersection and
possibly diﬀerent dimensions.
Example 7.3.4 (Example 7.1.2 continued) The dimension of the pa-
rameter space for a k component normal mixture is 3k −1, accounting for
the constraint
k

ℓ=1
pkℓ= 1 .

362
Model Choice
7
If the prior distribution on k is a Poisson P(λ) distribution, the parameter
space is inﬁnite dimensional, since k is unbounded.
∥
While, from a model-choice point of view, the diﬃculty is mainly in com-
puting the posterior probability of model Mk, π(μ = k|x), there are more
fundamental problems related to this representation, the ﬁrst one being
the very notion of model parameter, which can be written as a sequence
(θ1, . . . , θk, . . .) or as a couple (k, θk). There are also measure-theoretic diﬃ-
culties in the representation of the prior density for a direct sum of spaces.
Obviously, the corresponding MCMC samplers are much harder to con-
struct.
A ﬁrst solution, due to Carlin and Chib (1995), consists in saturating the
model, that is, in considering all models at once: for a ﬁnite range of models
Mk (k = 1, · · · , K) with corresponding priors πk(θk), and prior weights ϱk,
the parameter space is
Θ = {1, . . ., K} ×
K

k=1
Θk
and, if μ denotes the model indicator, the posterior distribution is
π(μ, θ1, . . . , θK|x) ∝ϱμfμ(x|θμ)
K

k=1
πk(θk) .
Since
m(x|μ = j) =

fj(x|θj)π(θ1, . . . , θK|μ = j) dθ =

fj(x|θj)πj(θj) dθj
does not depend on the πk(θk)’s for k ̸= i, Carlin and Chib (1995) propose
to use pseudo-priors ˜πk(θk|μ = j) to simulate the parameters θk on steps
when k ̸= j. Their method is then implemented as a Gibbs sampler on
(μ, (θ1, . . . , θK)), where μ is generated from
P(μ = j|x, θ1, . . . , θK) ∝ϱjfj(x|θj)πj(θ)

k̸=j
˜πk(θk|μ = j) .
The authors point out that the method works better when these pseudo-
priors are close to the true posteriors, but there is always some danger in the
calibration of the pseudo-priors that some important parts of the parameter
spaces Θk may be omitted. The major drawback of Carlin and Chib’s (1995)
method is that it requires simulation for all models at every stage, which
is costly when K is large. In addition, it cannot be implemented when K
is inﬁnite.
Example 7.3.5 (Carlin and Chib (1995)) On a dataset of 42 pine trees,
the grain (strength) yi is regressed against either the wood density xi, or
a modiﬁed (resin adapted) density, zi. Two contending models are
M1 : yi = α + βxi + σεi

7.3
Monte Carlo and MCMC approximations
363
and
M2 : yi = γ + δzi + τεi ,
where both (α, β, σ2) and (γ, δ, τ2) are associated with (empirical Bayes)
conjugate priors:

α
β

,

γ
δ

∼N

3000
185

,

106
0
0
104

,
σ2, τ2 ∼IG(a, b) ,
with (a, b) such that the mean and standard deviation of σ2 and τ 2 are
3002. (Notice that, in a realistic Bayesian analysis, the eﬀect of this prior
modeling should be evaluated by a robustness analysis. See Section 3.6.)
The pseudo-priors are chosen in this case as the priors for σ2 and τ 2, and
some vague conjugate priors on (α, β) and (γ, δ):
α|μ = 2
∼
N(3000, 522) ,
β|μ = 2
∼
N(185, 122) ,
γ|μ = 1
∼
N(3000, 432) ,
δ|μ = 1
∼
N(185, 92) .
In order to force visits to the model M1, the authors used disproportionate
weights, ϱ1 = .9995 and ϱ2 = .0005. (This appears to be quite a common
feature in the pseudo-prior approach, in order to compensate for the pos-
sibly poor choice of the pseudo-priors.)
The result is that B21 is estimated by 4420 (after correction for the
weights), with a (simulation) conﬁdence interval of (4353, 4487). (The con-
ﬁdence interval is simply deduced from the binomial variance on the poste-
rior probability P(μ = 1|x).) The model M2 can therefore be safely chosen
as the appropriate model.
∥
Example 7.3.6 (Example 7.1.2 continued)
For the galaxy mixture
model, if we only consider the case of 3 (model M1) versus 4 (model M2)
components, Carlin and Chib (1995) use a complete data model as in Sec-
tion 6.4, by creating allocations zk
i (i = 1, · · · , n, k = 1, 2). As in Example
7.3.5, using preliminary runs on both models, the pseudo-priors on the pa-
rameters are conjugate distributions matching the posterior estimates for
both models, while the pseudo-priors for the zk
i ’s, when μ ̸= k, are derived
from the observed frequencies. The authors evaluate the Bayes factor to
be 0.5153, with a standard error of 0.0146, thus concluding against the
four-component model. (The authors also mention that the result may be
altered in favor of the four-component model simply by modifying the prior
on the weights.)
∥
7.3.4 Reversible jump MCMC
For models with variable dimensions, Green (1995) proposes another type
of saturation technique, at a more local level than Carlin and Chib’s (1995).
The idea at the core of this technique, given two models M1 and M2 of
possibly diﬀerent dimensions, is to remove the diﬀerence in the dimensions

364
Model Choice
7
of models M1 and M2 by supplementing the corresponding parameters θ1
and θ2 by auxiliary variables u1→2 and u2→1 such that
(θ1, u1→2) and (θ2, u2→1)
are in bijection:
(θ2, u2→1) = Ψ1→2(θ1, u1→2) .
(7.3.6)
If θ1 is distributed from π1(θ1) and u1→2 is generated from g1→2(u), the
distribution of (7.3.6) is given by
π1(θ1)g1→2(u1→2)

∂Ψ1→2(θ1, u1→2)
∂(θ1, u1→2)

−1
by the Jacobian formula. Now, if we want (7.3.6) to be distributed from
π2(θ2)g2→1(u2→1), the Metropolis–Hastings acceptance probability is
min
π2(θ2)g2→1(u2→1)
π1(θ1)g1→2(u1→2)

∂Ψ1→2(θ1, u1→2)
∂(θ1, u1→2)
 , 1

.
In opposition to Carlin and Chib’s (1995) technique, this approach only
considers local moves between two models: the other θj’s and the auxiliary
variables ui→j are not explicitly used outside moves from Mi to Mj.
Obviously, the theory behind reversible jump MCMC is more advanced
than the justiﬁcation given above, if only because it requires a deeper un-
derstanding of the joint measure on (θ2, u2→1) and (θ1, u1→2), which must
satisfy a detailed balance condition as in (6.3.1). We refer the reader to
Green (1995) for details. The main issue, however, is that, given a prob-
ability ϱi→j of choosing model Mj when in model Mi, the acceptance
probability of a move is indeed given by
min
ϱjϱj→iπj(θj)gj→i(uj→i)
ϱiϱi→jπi(θi)gi→j(ui→j)

∂Ψi→j(θi, ui→j)
∂(θi, ui→j)
 , 1

,
(7.3.7)
where (θj, uj→i) = Ψi→j(θi, ui→j). The algorithm can then be completed
with additional steps within a given model Mi, or about hyperparameters
that are not model-dependent.
As discussed in Robert and Casella (2004, Section 11.2), the freedom
allowed by the reversible jump algorithm to move between many models
of diﬀerent dimensions has generated many applications, which are by no
means restricted to the model-choice setting. In connection with Example
7.1.2, Richardson and Green (1997) devised a reversible jump algorithm
for normal mixtures, with the conclusion that the number of components
for the galaxy dataset should be four. We present below the corresponding
algorithm for a mixture of exponential distributions, developed in Gruet,
Philippe and Robert (1999). (See also Robert, Ryd´en and Titterington
(1999b) for a generalization to hidden Markov models.)

7.3
Monte Carlo and MCMC approximations
365
Example 7.3.7 For a mixture of exponential distributions
k

j=1
pjkExp(λjk) ,
the reversible jump algorithm can be restricted to moves between adjacent
models, that is, between model Mk and models Mk+1 and Mk−1. These
moves are quite open, in the sense that a component can be added (or re-
moved) at random, as long as there is symmetry in the up and down moves.
For instance, a birth of component k + 1 may be proposed by generating
(p(k+1)(k+1), λ(k+1)(k+1)) from a prior ϖk+1(p, λ), assuming independent
identical priors on all components. The transformation is then
(p1(k+1), . . . , pk(k+1)) = ((1−p(k+1)(k+1))p1k, . . . , (1−p(k+1)(k+1))pkk)
(λ1(k+1), . . . , λ(k+1)(k+1)) = (λ1k, . . . , λkk, λ(k+1)(k+1)) .
The Jacobian of this transform is therefore (1−p(k+1)(k+1))k and the prob-
ability to accept the birth is
min
!
ϱk+1(1 −p(k+1)(k+1))k πk+1(p1(k+1), . . . , p(k+1)(k+1),
ϱkπk(p1k, . . . , pkk, λ1k, . . . , λkk)
λ1(k+1), . . . , λ(k+1)(k+1))
ϖk+1(p(k+1)(k+1), λ(k+1)(k+1)) , 1

,
if the probabilities of choosing a birth (move to Mk+1) or a death (move
to Mk−1) are the same.
The move from Mk to Mk+1 considered in Gruet et al. (1999) is a split
of a randomly chosen component j in such a way that the new compo-
nent parameters (pj(k+1), p(j+1)(k+1), λj(k+1), λ(j+1)(k+1)) satisfy the mo-
ment condition
pjk
=
pj(k+1) + p(j+1)(k+1)
(7.3.8)
pjkλjk
=
pj(k+1)λj(k+1) + p(j+1)(k+1)λ(j+1)(k+1) ,
the opposite move being a merge of two components j and j + 1 according
to (7.3.8). This proposal can be equivalently represented as a generation
of u1, u2 ∼U([0, 1]), with pj(k+1) = u1pjk and λj(k+1) = u2λjk, and the
Jacobian is then
∂Ψk→k+1(pjk, λjk, u1, u2)
∂(pjk, λjk, u1, u2)
= pjk/(1 −u1) .
Figure 7.3.4 presents some condensed analysis of the performances of the
reversible jump algorithm, when analyzing a dataset on hospital stays, with
a posterior mode for k at 4. The allocation map on the lower right graph
represents the successive allocations of the observations by grey levels: it
shows that the mixing properties of the chain are quite good, since no
pattern emerges. (See Gruet et al. (1999) for more details.)
∥

366
Model Choice
7
0
10000 20000 30000 40000 50000
2
4
6
8
10
sequence of k’s
2
4
6
8
10
0.00.05
0.15
0.25
0
10000 20000 30000 40000 50000
3.5
4.0
4.5
5.0
convergence of the average
0
500
1000
1500
2000
0 10000
30000
50000
allocations
Figure 7.3.1. Sequence of values k(t) simulated by reversible jump, with corre-
sponding histogram (upper right), convergence of the empirical mean (lower left)
and sequence of allocations to the components (lower right) for 50, 000 iterations
(Source: Gruet, Philippe and Robert (1999).)
Notice that, in both schemes described in Example 7.3.7, there is no
auxiliary variable uk→(k−1) for the downward moves. This setting often
occurs when one model is a completion of the other, although additional
auxiliary variables can be introduced for potential improvements in speed.
A related technique has been proposed in Ripley (1987), Grenander and
Miller (1994), Phillips and Smith (1996) and Stephens (2000), based on
birth and death continuous time processes. See Note 7.8.2.
7.4 Model averaging
A natural approach to model uncertainty, from the Bayesian point of view,
is to include all models Mk under consideration for future decisions, thus
bypassing the model-choice step. As proposed in Raftery, Madigan and
Volinsky (1996), this solution escapes the usual underestimation of un-
certainty resulting from choosing model Mk0, say, at the model-selection
stage, and thereafter ignoring the uncertainty about that choice in the sub-
sequent steps.
Obviously, this perspective is not appropriate to all settings: sometimes,
the decision maker or the statistician must select a model, as for instance
in scientiﬁc inference, or they must eliminate some superﬂuous covariates
in variable selection because of prohibitive sampling costs (Section 7.5). In
addition, model averaging seems to infringe the parsimony requirement, in

7.4
Model averaging
367
the sense that, since all models are included into the (super) model, this
generates an inﬂation of parameters and, given that most cases involve
Monte Carlo or MCMC algorithms, implies the generation and storage of
a vast number of MCMC samples. This is the case with Example 7.1.2 for
instance.
In this approach, given a sample x = (x1, . . . , xn), the predictive distri-
bution is obtained by averaging over all models
f(y|x)
=

Θ
f(y|θ)π(θ|x)dθ
=

k

Θk
fk(y|θk)π(k, θk|x)dθk
=

k
p(Mk|x)

fk(y|θk)πk(θk|x) dθk ,
where Θ denotes the overall parameter space deﬁned in (7.2.1).
Although this approach faces most of the diﬃculties encountered in Sec-
tion 7.2, including the computation of many integrals and the simulation
on a parameter space Θ, which is a sum of spaces with diﬀerent dimen-
sions, the removal of the decision step on the model label μ alleviates some
of these diﬃculties. For instance, the fact that the collection of models is
possibly inﬁnite (or simply too large, as in variable selection) is not an im-
pediment, in the sense that an MCMC algorithm that explores the space
Θ will bypass models with very small probabilities P(Mi|x).
The issue here is rather at the modeling level, as already discussed in
Section 7.2.1: when facing many models, the choice of the prior probabilities
π(k) is paramount, but diﬃcult to formalize and justify. For instance, in
the setting of variable selection (Section 7.5), the models in contention can
be represented by vectors of indicator variables,
Mk : (δk1, · · · , δkd) ,
δkj ∈{0, 1} ,
where d is the number of potential covariates. Madigan and Raftery (1991)
suggest using
π(k) ∝
d

j=1

ϱδkj
j
(1 −ϱj)1−δkj 
,
where ϱj denotes the prior probability that variable j has an eﬀect. A draw-
back to this choice is that covariates are included in a model independently
from one another, which only makes sense if they are independent, a risky
assumption in most cases. The standard alternative of putting equal weights
on all models is also open to criticism: besides the obvious drawback that
it does not work if the number of models is inﬁnite, it is awkward to use
for nested models, that is, when some models are special cases of others,
as in variable selection.
As mentioned above, MCMC techniques such as reversible jump or jump

368
Model Choice
7
diﬀusions (Note 7.8.2) naturally solve the diﬃculty of exploring a large
number of models by avoiding those with very small probabilities, assum-
ing that the corresponding algorithms have a proper convergence behav-
ior. Madigan and Raftery (1994) devise another technique, called Occam’s
Window.5 They suggest that only models such that
maxk P(Mk|x)
P(Mℓ|x)
≤C
should be considered, that is, models suﬃciently likely compared with the
most likely model. They propose in addition to exclude models Mℓ, such
that there exists a submodel Mh ⊂Mℓsuch that
P(Mh|x)
P(Mℓ|x) ≥1 .
This reduction in the number of models can only be implemented if the
number of models is suﬃciently small, though, and Clyde (1999) points out
the possible bias in the resulting probabilities resulting from this pruning.
In the case of variable selection in normal regression, y ∼N(Xβ, σ2I),
that is,
yt =
J

j=1
βjxjt + σεt
t = 1, · · · , T ,
with orthogonal regressors
XtX = diag(x′
jxj) ,
Clyde (1999) proposes to use priors of the form
βj ∼N(0, c2
jγj) ,
γj ∼B(pj) ,
where the γj’s act like 0-1 indicators for the jth regressor to be present in
the model. Then, under Madigan and Raftery’s (1991) prior,
π(γ1, . . . , γJ|y, σ) =
J

j=1
ϱγj
j (1 −ϱj)γj ,
(7.4.1)
5 William d’Occam or d’Ockham (circa 1290–circa 1349) was a English theologian (and
a Franciscan monk) from Oxford who worked on the bases of empirical induction and,
in particular, posed the principle later called Occam’s razor, which excludes a plurality
of reasons for a phenomenon if they are not supported by some experimentation
(see Adams (1987)). This principle, Pluralitas non est ponenda sine neccesitate (or
Entities are not to be multiplied without necessity), is often invoked as a parsimony
principle to choose the simplest among two equally possible explanations, and its
use is recurrent in the Bayesian literature (see, for instance, Jeﬀreys (1961, §6.12)
or Jeﬀerys and Berger (1992)). However, we refrain from using this notion because
it does not provide a working principle, and is therefore open to misappropriation.
In other words, the call to Occam’s razor does not provide further justiﬁcation for
a given method. (At a more anecdotal level, Umberto Eco’s The Name of the Rose
borrows from Occam to create the character William of Baskerville.)

7.5
Model projections
369
with
ϱj =
Oj(y, σ)
1 + Oj(y, σ)
and
Oj(y, σ)
=
pj
1 −pj
!
x′
jxj + σ2/c2
j
σ2/c2
j
"−1/2
exp

(ˆβjx′
j/σ2)2
2(x′
jxj/σ2 + 1/c2
j)

,
which means that the γj’s are a posteriori independent, and also that the
probability of a given submodel can be easily computed, including the
most likely submodel (Exercise 7.25). This is not the case with George and
McCulloch’s (1997) alternative modeling
βj ∼N
	
0, c2
jγj + [c2
j/100](1 −γj)

.
If σ2 is unknown, Clyde (1999) uses a single prior σ2 ∼IG(α, β) for all
models
π(σ2|γ, y) ∼IG(ˆα, ˆβ)
and approximates the posterior weights of the diﬀerent models with either
a “plug-in” estimate, that is, replacing σ2 with an estimate ˆσ2 in (7.4.1),
or with a Rao–Blackwell average.
While such results are interesting, they are quite diﬃcult to extend to
other settings, such as generalized linear models, where further approxima-
tions are necessary. Besides, the orthogonality assumption is quite restric-
tive because usual regressors are never orthogonal, and using a orthogonal-
izing transform such as principal components is not satisfactory because
decision-makers are often interested in the values of the coeﬃcients βj.
Furthermore, it violates the principle that common parameters should be
treated as diﬀerent entities in diﬀerent models, since the βj’s are the same
in every model where they appear.
7.5 Model projections
We now present a diﬀerent approach6 to model selection, that has been
developed by Goutis and Robert (1998) and applied to variable selection
in Dupuis and Robert (2001). The approach is based on projections of a
full model f(y|θ) on submodels, represented as restrictions on θ, and on
the assessment of the approximation error owing to these restrictions. It
applies in particular to variable selection, that is, to the determination of
a set of covariates, among a larger set of covariates.
6 This section contains more specialized material which, while being at the same level
of diﬃculty as the remainder of this chapter, can be skipped on a ﬁrst reading.

370
Model Choice
7
Example 7.5.1 In a study into the inﬂuence of dietary factors on breast
cancer, the following covariates are considered by Raftery and Richardson
(1996):
age
age of ﬁrst pregnancy
menopausal age
age at the end of study
age at menarche
body mass index
parity
fat intake (total)
alcohol consumption
fat intake (saturated)
family history of breast cancer
history of benign BC
Since the observation takes values in {0, 1}, which corresponds to the no
cancer–cancer dichotomy, the models under consideration are logistic re-
gressions involving some or all of the covariates (i = 1, · · · , 212):
Mi : P(yj = 1|xj) =
exp[αi + βt
ix(i)
j ]
1 + exp[αi + βt
ix(i)
j ]
,
where x(i) denotes the coordinates of x corresponding to the binary decom-
position of i. For instance, model M5 is associated with i = 5 = 0 · · · 0101
and therefore x(5) = (x10, x12).
∥
One of the main diﬀerences of the projection approach, compared with
the usual model-choice axioms of Section 7.2, is that it only requires the
construction of a prior π(θ) on the full model f(x|θ), instead of a prior
for every submodel, and that it accommodates improper priors. In fact,
as we will see below, it derives the submodel weights and priors from the
original prior distribution π, thus escaping marginalization and projection
paradoxes with subspaces of diﬀerent dimensions.
Given a restriction θ ∈Θ0, Goutis and Robert’s (1998) perspective is to
consider that this restriction is acceptable if
d(f(· |θ), Θ0) < ϵ ,
(7.5.1)
where d is a divergence measure and
d(f(· |θ), Θ0)
=
d(f(· |θ), f(· |θ⊥))
=
inf
θ0∈Θ0 d(f(· |θ) , f(· |θ0)) .
The parameter θ⊥is then the projection of the parameter θ onto the sub-
model. From this perspective, model choice is seen as an assessment of the
diﬀerence between the true model and a more parsimonious model. It thus
provides a realistic modeling of experimental purposes where exact nullity
is rarely at stake and escapes parameterization issues, since the represen-
tation (7.5.1) is parameter-free. In addition, this formalism only requires a
prior distribution on the full parameter θ, since the projection parameter
θ⊥is a transform of θ. The posterior probability that (7.5.1) holds can
therefore be computed with this prior distribution only. Notice that this is

7.5
Model projections
371
Table 7.5.1. Values of the parameters for Kullback–Leibler divergences of ϵ in the
case of Bernoulli, Poisson and normal distributions (Source: Goutis and Robert
(1998).)
ϵ
0
0.01
0.05
0.1
0.25
0.5
1
2
∞
B(p)
0.5
0.57
0.65
0.71
0.81
0.9
0.96
0.99
1
P(λ)
1
1.15
1.35
1.52
1.88
2.36
3.15
4.5
∞
N(μ, 1)
0
0.14
0.32
0.45
0.71
1
1.41
2
∞
diﬀerent from deriving the prior distribution on θ⊥by projection of π(θ)
and then using a standard Bayes factor, as in McCulloch and Rossi (1992)
(Exercise 7.32).
There are many choices for the divergence measure d, but a natural choice
is the Kullback–Leibler pseudo-distance
d(f, g) =

log
f(z)
g(z)

f(z) dz ,
already used in (2.5.6). As argued in Bernardo and Smith (1994), there are
many good reasons for using this measure, including information theory,
scoring rule, transitivity and additivity properties, as well as connection
with exponential families and generalized linear models.
Similarly, the factor ϵ in (7.5.1) can be chosen in many ways. For instance,
it can be calibrated on simple distributions to evaluate the proper scale of
ϵ, as in Table 7.5 (Exercise 7.28). In the case of a single restriction, ϵ can
be derived from the (proper) prior π, so that
P π(d(f(· |θ), f(· |θ⊥)) ≤ϵ) = 1/2 .
This was done in the setting of mixtures by Mengersen and Robert (1996),
but the 1/2 value can be criticized as giving a false impression of objec-
tivity (since the resulting value depends on π). At last, in variable selec-
tion settings and related embedded models, there exists a minimal or most
rudimentary model, f0, which corresponds to a regression with only one
intercept, and which may scale ϵ as ϵ = ϱd(f, f0). (Dupuis and Robert
(2001) call d(f, f0) the maximal loss of explanatory power.)
Once d and ϵ have been chosen, the method can be implemented by
either computing the posterior probability P π(d(f(· |θ), f(· |θ⊥) ≤ϵ), or by
deriving the posterior expectation of d(f(· |θ), f(· |θ⊥)). For the variable-
selection problem, when y is regressed on a vector x of p covariates, the issue
is complicated by the fact that this distance is computed by integrating over
the joint distribution of (x, y), leading to the term (Exercise 7.30)
IEx[d(f(·|x, θ), fA(·|xA, θ⊥))] ,
where A ⊂{1, . . . , p} and xA is the corresponding subset of covariates.
Because the distribution of the covariate vector x is rarely known, this

372
Model Choice
7
quantity can be approximated by the empirical average
1
n
n

i=1
IEy

log
 f(y|xi, θ)
g(y|xiA, θ⊥)
 xi

.
Besides the usual computational diﬃculties of approximating posterior ex-
pectations or posterior probabilities, an additional issue with variable selec-
tion is that, given p potential covariates, there exist 2p (or 2p−1) models in
competition. For large values of p, a complete exploration of all the models
is thus impossible. As detailed in Note 7.8.3, some transitivity and additiv-
ity properties of the Kullback–Leibler distance allow for a faster pruning of
the submodel tree: when selecting among all subsets A of covariates such
that
d(Mg, MA) = IEx[d(f(y|x, α), g(y|xA, α⊥))] < ϵ ,
the submodel with the smallest cardinal, that is, with the smallest num-
ber of covariates, it is possible to evaluate this cardinal by a downward
step—starting from the full model and descending in the submodel tree by
eliminating one covariate at a time, the farthest away from Mg, till the dis-
tance is too large—and an upward step—starting from the constant model
and adding one covariate at a time, the closest from Mg, till the distance
is smaller than ϵ—and to check afterwards that no other model with the
same cardinality p0 is closer to the full model. This last step may be very
costly, though, being of order
	 p
p0

(Exercise 7.29).
Example 7.5.2 (Example 7.5.1 continued) For a ﬂat prior on the re-
gression parameters (α, β), Dupuis and Robert (2001) obtained the results
listed in Table 7.5 via this variable-selection procedure (with ϱ = 0.9 in the
scaling of ϵ). The same submodel 100111111001 was selected at the three
stages of this procedure. From the list of explanatory variables given in
Example 7.5.1, this means that the selected submodel excludes fat intakes
from the important explanatory variables. Notice the agreement between
the approach based on the posterior expected distance (column 3) and the
posterior probability that the distance is less than ϵ (column 4).
∥
Although this approach has the appeal of relying on a loss function to
select among submodels, and of eliminating the improper prior problem, it
also has some defects. Firstly, in cases such as variable selection when the
number of submodels under consideration is large, it may require enormous
computing time to implement. Secondly, the determination of the bound
ϵ is open to criticism: for instance, why should a ﬁxed proportion of the
distance d(f, f0) be pertinent for the decision-maker? How should it depend
on the number of observations? Another drawback to this approach is that
it requires a full (or reference) model, thus only works with nested models.
Following an approach routinely used in econometrics (see, e.g., Gouri´eroux
and Monfort (1996)), Goutis and Robert (1998) propose to extend the
method to the general case by creating an encompassing model, but this is a

7.5
Model projections
373
Table 7.5.2. Submodels examined by the variable selection procedure for the breast-
cancer dataset. The result of each step is shown in bold, d(Mg, MA) denotes the
expected Kullback–Leibler divergence between the full model and the projection
on the subset of covariates A, and P(MA) is the posterior probability that the
distance d(Mg, MA) is less than ϵ. (Source: Dupuis and Robert (2001).)
step
subset A
d(Mg, MA)
P(MA)
(×740)
1.
101111111111
0.508
0.98
101111111011
1.146
0.96
100111111011
1.800
0.94
100111111001
2.726
0.91
2.
000000010000
21.78
0.29
000010010000
16.97
0.45
100010010000
13.81
0.55
100010011000
10.61
0.66
100010011001
7.601
0.75
100011011001
5.224
0.83
100111011001
3.736
0.88
100111111001
2.726
0.91
3.
111111110000
8.170
0.73
111111001010
13.72
0.55
111100111010
8.349
0.73
110011111010
5.988
0.81
001111111010
9.215
0.70
111110011001
4.542
0.85
111101011001
4.761
0.85
111011011001
3.91
0.87
110111011001
3.265
0.89
101111011001
3.017
0.90
011111011001
5.895
0.81
100111111001
2.726
0.91
100111011101
3.109
0.899
100011111101
3.826
0.88
111011010011
5.284
0.83
110110110011
6.04
0.80
101101110011
5.9
0.81
101011011011
3.576
0.88
100111011011
2.77
0.91
101010111011
5.08
0.84
011001111011
9.346
0.70
100110011111
4.151
0.87
100101011111
4.224
0.86
100011011111
3.787
0.88

374
Model Choice
7
diﬃcult issue given that this encompassing model is not the true model, and
is thus of little interest for the decision-maker. Besides, the encompassing
model can be deﬁned in many ways, which lead to diﬀerent answers. For
instance, one can oppose arithmetic and geometric averages (Exercise 7.34).
Example 7.5.3 (Example 7.1.1 continued) Given that both Poisson
and negative binomial models involve the terms
λy
y! ,
where λ = p/(1−p) in the negative binomial case, a possible encompassing
model is
f(y|λ, m, α) ∝1
y!λy e−αλ

m!
(m −y)!
1
(1 + eλ)m
1−α
0 ≤α ≤1 ,
since the Poisson model corresponds to α = 1 and the negative binomial
model to α = 0. This density is in fact the geometric average of both densi-
ties, but the normalizing constant, which depends on (λ, m, α), is unknown.
A more manageable alternative is to use an arithmetic average, that is, the
mixture
p P(λ) + (1 −p) Neg

m,
eλ
1 + eλ

0 ≤p ≤1 .
∥
7.6 Goodness-of-ﬁt
We conclude this chapter with a short introduction to the Bayesian ap-
proach to goodness-of-ﬁt, which is, in a way, the most challenging of all
model choice problems. Indeed, when considering the question Is M0 com-
patible with x? or Does f belong to the family {fθ; θ ∈Θ}? there is no
alternative to M0. For instance, in Example 7.1.1, if we only consider
the Poisson model, to assess whether this model is compatible with the
dataset is diﬃcult, given that, when it is not compatible, the model is left
undeﬁned!7
The diﬃculty here with the Bayesian paradigm seems to be that it cannot
provide an answer about the validity of the model without moving “outside”
the model, that is, without working in a larger structure (a meta-model)
that includes the model under consideration as a special case. In fact, this is
not a shortcoming of the Bayesian approach, but rather a result of the poor
formulation of the question. That the Bayesian paradigm fails to produce
an answer to an poorly posed problem does not imply that other systems
7 The frequentist approach to this problem bypasses the diﬃculty by working only
under the null hypothesis. The standard χ2 test, for instance, is based on the χ2
approximation, which only works when the model is the true model. If it is not, the
χ2 statistic diverges to inﬁnity, but nothing can be said about its distribution for a
given sample size.

7.6
Goodness-of-ﬁt
375
that produce such an answer, such as the χ2 test, are in any way validated!
Rather, by requiring the construction of an alternative model, the Bayesian
paradigm clariﬁes the issue and formalizes the deﬁnition of the meta-model
that includes the model under study.
This issue being clariﬁed, there are many ways of deﬁning this alternative
model M1, unless strong prior information is available. For instance, M1
can be an embedding model of M0. But, as discussed in Section 7.5, there
is no single choice of an embedding model, that is, there is no such thing
as the smallest (or the most natural) embedding model, besides the trivial
M0 itself! Neyman (1937) proposed the exponential family extension
f1(x|θ, ϕ) ∝f(x|θ) exp

−ϕ log
f(x|θ)
f(x|ˆθ(x))

,
ϕ ≥0 ,
where ˆθ(x) is the maximum likelihood estimator (provided it is deﬁned),
but other hierarchical extensions are possible. Moreover, embedding mod-
els give very restricted representations of the alternative, given that the
alternative in a goodness-of-ﬁt problem should be f is not in M0.
Another approach to the deﬁnition of the alternative model avoids such
restrictions, since it uses a nonparametric representation of the alternative.
We mentioned in Note 1.8.2 some standard techniques in Bayesian non-
parametrics, including the Dirichlet process prior and their generalizations,
mixtures, or wavelets. For illustration, we consider here the orthogonal poly-
nomial representation of Verdinelli and Wasserman (1998). See Castro et
al. (1999) for an alternative in the discrete case (Exercise 7.37).
The model under consideration M0 : x ∼f(x|θ), θ ∈Θ, can be ex-
pressed as
M0 : x = F −(u|θ),
θ ∈Θ,
u ∼U([0, 1]) ,
where F −(·|θ) is the generalized inverse of the c.d.f. of f(·|θ) (Exercise
7.38). We can thus write M0 as a special case of
M1 : x = F −(u|θ),
θ ∈Θ,
u ∼g(u|ψ),
ψ ∈S ,
where g(·|ψ) is a distribution on [0, 1], which contains as a special case,
the uniform distribution, g(u|ψ0) = 1, and S is an inﬁnite dimensional
space. This reparameterization of the model means that we can work on
distributions on [0, 1], rather than on a general space, and that we want to
devise a test of uniformity (conditional upon θ).
There are many possible choices for the inﬁnite dimensional family of
distributions g(·|ψ). While a possibility is to consider mixtures of beta
densities,
g(u|ψ) = ϱ0 + (1 −ϱ0)
+∞

j=1
ϱj
uαj(1 −u)βj
K(αj, βj)
,
as in Petrone and Wasserman (2000), which can be estimated by reversible
jump techniques, Verdinelli and Wasserman (1998) propose instead using

376
Model Choice
7
Legendre polynomials on [0, 1],
φj(x) =
1
2jj!
dj
dxj (x2 −1)j
to write the density as
g(u|ψ) ∝exp
⎧
⎨
⎩
+∞

j=1
ψjφj(u)
⎫
⎬
⎭.
(See Barron (1988, 1998) and Lenk (1991) for details.) The null model M0
then corresponds to ψ1 = . . . = ψp = . . . = 0.
The prior distribution on (θ, ψ) is chosen such that θ and ψ are inde-
pendent, with a reference prior on θ. This independence assumption is not
innocuous, given that θ has the same prior under M0 and M1, but is
not identiﬁable under M1 (Exercise 7.39). The ψj’s are then modeled as
independent normal rv’s,
ψj ∼N(0, τ2
j ),
with τj = τ/2j for consistency reasons (Barron (1988)), and τ is associated
with a vague proper prior, π(τ).
The posterior distribution is then given by
π(θ, ψ, τ|x1, · · · , xn) ∝
n

i=1
f(xi|θ)g(ui|ψ)π(θ) π(ψ|τ)π(τ) ,
(7.6.1)
where
ui = F(xi|θ) (Exercise 7.40). It is obviously intractable, if only
because of the dependence of the ui’s on θ. Simulation from π(θ, ψ, τ|x1, · · · ,
xn) can nonetheless be implemented via an MCMC algorithm using for
instance the Gibbs steps
θ|ψ, x1, · · · , xn
∼
n

i=1
f(xi|θ)g(ui|ψ)π(θ) ,
ψ|τ, θ, x1, · · · , xn
∼
n

i=1
g(ui|ψ)π(ψ|τ) ,
τ|ψ
∼
π(ψ|τ)π(τ) .
Some additional Metropolis–Hastings steps are necessary for the generation
of θ and ψ.
Once the posterior distribution is approximated, Verdinelli and Wasser-
man (1998) suggest using a Bayes factor
B01 =

n

i=1
f(xi|θ)π(θ)dθ

n

i=1
f(xi|θ)g(F(xi|θ)|ψ)π(θ, ψ, τ)dθdψdτ

7.7
Exercises
377
to assess whether M0 provides a good ﬁt to the data. (They show in ad-
dition that the procedure is consistent, that is, that B01 goes to 0 almost
surely if M0 is the wrong model, and to ∞in probability if it is the right
model.) An alternative evaluation is to notice that M0 also corresponds to
τ = 0, and to use a standard point-null test based on the MCMC sample.
7.7 Exercises
Section 7.1.1
7.1 The deviance associated with a model is simply the log-likelihood taken at
the maximum likelihood estimator (McCullagh and Nelder (1989)). In the
setting of Example 7.1.1, compute the maximum likelihood estimator’s ˆλ and
( ˆm, ˆp) and compare both deviances.
7.2 In the setting of Example 7.1.2, show that a mixture of k components can be
represented as a mixture of k + 1 components by either putting a component
weight to 0, or by equating the mean and variance parameters of the (k+1)-th
component to the same parameters for one of the ﬁrst kth components. Relate
this multiplicity with the non-identiﬁability property of mixtures raised in
Note 6.6.6.
7.3 For the setting of Example 7.1.3, write down the marginal distributions of
yi = (yi1, . . . , yi7) when integrating out the random-eﬀects. Is it possible to
obtain closed-form estimates with conjugate priors?
Section 7.2.1
7.4 Given two models M1 : x ∼f1(x|θ1, γ) and M2 : x ∼f2(x|θ2, γ), show
that, if the prior distribution is of the form
π(θ1, θ2, γ) = π1(θ1|γ)π2(θ2|γ)π0(γ) ,
with both π1 and π2 proper, the Bayes factor Bπ
12 does not depend on the
normalizing constant of π0 if π0 is improper.
7.5 In the setting of Example 7.2.1, assume Tt is distributed from a uniform
U[0, ¯
T ] distribution, and that β21 ∼N(0, τ 2).
a. Compute the marginal model of yit by integrating out the term β21Tt in
M2.
b. Deduce the prior distribution on the parameters of M1 if M2 is the true
model and (β20, b2i, σ2) ∼π(β20, b2i, σ2).
7.6
∗(Barbieri, Liseo and Petrella (1999)) Consider a model f(x|ϕ, ψ), (ϕ, ψ) ∈
Φ × Ψ, such that there exists ψ∗∈Ψ that satisﬁes
lim
ψ→ψ∗f(x|ϕ, ψ) = f ∗(x|ψ∗) ,
that is, the limiting distribution does not depend on ϕ.
a. Show that this setting occurs for the linear calibration model,
z1 ∼N(ψ, 1),
z2 ∼N(φψ, 1) .
b. If π(ϕ, ψ) is a proper prior with a point mass in ψ∗, show that
π(ϕ|x) = π(ϕ|ψ∗)π(ψ∗|x) + π(ϕ|ψ ̸= ψ∗, x)

ψ̸=ψ∗
π(ψ|x) dψ .

378
Model Choice
7
c. If H0 : ϕ = ϕ0 is to be tested against H1 : ϕ ̸= ϕ0, show that
B01 = π(ψ∗|x) + m(x|ψ ̸= ψ∗)
π(ϕ0)

ψ̸=ψ∗
π(ψ|x) dψ
assuming that π also has a point mass at ϕ0.
d. Deduce that the Bayes factor is heavily inﬂuenced by the prior modeling
on ψ∗, no matter what ϕ0 is.
[Note: Gleser and Hwang (1987) study such models from a frequentist point
of view and show that a conﬁdence interval at a level α on an unbounded
function of ϕ has inﬁnite volume with positive probability.]
Section 7.2.2
7.7 (Berger and Pericchi (2001)) Consider the normal linear model M2
y = α1 + z1β1 + z2β2 + ϵ,
ϵ ∼Nn(0, σ2In) ,
where β1 ∈IRk and β2 ∈IRp, and where the zi’s are centered and orthogonal,
that is, zt
1z2 = 0. The submodel M1 is associated with β2 = 0.
a. Show that, under the priors
π1(α, β1, σ) = 1/σ
and
π2(α, β1, σ, β2) = h(β2|σ)/σ ,
where h(β2|σ) is the Cauchy Cp(0, zt
2z2/nσ2) density, the Bayes factor B12
cannot be computed in closed form.
b. Show that, under the G-prior of Zellner (1986), which associates with model
M1 : y = Xβ + ϵ, ϵ ∼Nn(0, σ2In), the prior
π(σ) = 1/σ ,
π(β|σ) ∝exp{−βtXtXβ/2gσ2} ,
the associate marginal is available in closed form.
c. Show that, if M0 is associated with β = 0 and if the maximum likelihood
estimator ˆβ goes to inﬁnity, the Bayes factor B01 goes to (1 + g)(k−n)/2
(where k is the dimension of β under model M1). Conclude about the
appeal of the G-prior in this setting.
7.8
∗(Exercise 7.6 cont.) In the case of the linear calibration model,
a. Show that the Jeﬀreys prior is
πJ(ϕ, ψ) ∝|ψ| .
b. When testing H0 : ϕ = ϕ0, with π0(ψ) ∝1, show that the fractional Bayes
factor with fraction 0 < b < 1 (see (5.2.10)) is
BF
01 = b−1/2 exp

−1 −b
2
(z1 −z2ϕ0)2
1 + ϕ2
0

.
c. Show that the arithmetic intrinsic Bayes factor (see (5.2.7)) is
BA
01 =
√
2 exp

−1 −0.5
2
(z1 −z2ϕ0)2
1 + ϕ2
0

.
d. Extend to the case of n observations.

7.7
Exercises
379
7.9
∗(Exercise 7.8 cont.)
a. Show that the reference prior is
πR(ϕ, ψ) ∝
1
1
1 + ϕ2 .
b. Show that the fractional Bayes factor is
BF
01
=
b−1/2 exp

1 −b2 [(z2
1 −z2
2)(1 −ϕ0)2 −4z1z2ϕ0]
1 + ϕ2
0

I0(b(z2
1 + z2
2)/4)
I0((z2
1 + z2
2)/4) ,
where I0 is the modiﬁed Bessel function (Exercise 4.35).
7.10
∗In the setting of Example 7.2.2,
a. Show that the ﬁnal expression for Bπ
12 is correct.
b. Show that the limit of Bπ
12 varies if α/β goes to 0 as both α and β go to 0,
or if α/βN goes to c > 0, when x < N.
7.11 Give the marginal distribution of x1 if x1, . . . , xn is a sample from a two-
component normal mixture such that there are at least two observations in
each component.
Section 7.2.3
7.12 Show that, for the comparison of two linear models M1 and M2 with k1
and k2 regressors, respectively, and n observations, under the prior πj(βj) =
σ
−1−qj
j
(j = 1, 2), the BIC writes down as
B12 =
R2
R1
n/2
n(k2−k1)/2 ,
where the Rj’s are the residual sums of squares.
7.13 (Exercise 7.8 cont.) In the case of the linear calibration model, under
the Jeﬀreys prior, show that Schwartz’s criterion gives almost the same result
as the fractional Bayes factor with fraction b = 0 in the exponent.
Section 7.2.4
7.14 If f(·|θ) belongs to an exponential family, show that the eﬀective number
of parameters pD is always positive.
7.15
∗(Spiegelhalter et al. (1998)) In the setting of Example 7.2.5,
a. Show that, for the saturated model where the θi’s are independent with
ﬂat priors, pD = p and that the Bayesian deviance is equal to 2p.
b. Show that the Bayesian deviance associated with the pooled model, θi = θ
for all i’s, is given by (7.2.7).
c. Show that (7.2.8) holds.
d. Assume that θi ∼N(μ, τ 2) with τ known and π(μ) = 1. Show that
pD =
p

i=1
ϱi +
p

i=1
ϱi(1 −ϱi)
-
p

i=1
ϱi

380
Model Choice
7
and that the Bayesian deviance is equal to
DIC = τ −2
p

i=1
ϱi(1 −ϱi)(yi −¯y)2 + pD ,
where ϱi = σ2
i τ 2/(σ2
i + τ 2) and ¯y = 
i ϱiyi/ 
i ϱi.
7.16 Detail the MCMC implementation for the three models of Example 7.2.6.
(Hint: The simulation can be handled by BUGS.)
7.17
∗(Spiegelhalter et al. (1998)) Consider a general linear model
y ∼N(Aθ1, Σ1) ,
θ1 ∼N(Bθ2, Σ2) .
a. Show that the posterior distribution of θ1 is of the form N(¯θ1, Ψ) and
specify ¯θ1 and Ψ.
b. Show that IE[D(θ)|y] = D(¯θ1) + tr(A′Σ−1
1 AΨ) and deduce that pD =
tr(A′Σ−1
1 AΨ).
c. Extend to the case when θ2 is random and π(θ2) = 1.
Section 7.3.1
7.18 Check whether the expectation of
1
T
T

t=1
h(θ(t)
f(x|θ(t))π(θ(t)) ,
when the θ(t)’s are distributed from π(θ|x), is equal to m(x), whatever the
probability density h.
7.19
∗(Chen and Shao (1997))
Consider two densities, π1(θ) = c1˜π1(θ) and
π2(θ) = c2˜π2(θ), on the same parameter space Θ.
a. If π is a density on Θ, give suﬃcient conditions on the support of π for
ϱ = c2
c1 = IEπ[˜π1(θ)/π(θ)]
IEπ[˜π2(θ)/π(θ)] .
b. Show that the asymptotic variance of the estimator of
ϱUS =
n
i=1 ˜π1(θi)/π(θi)
n
i=1 ˜π2(θi)/π(θi) ,
where the θi’s are i.i.d. from π, is
ϱ2IEπ

π1(θ)
π(θ) −π2(θ)
π(θ)
2
.
c. Assuming that
ϱ−2IEπ[(ϱUS −ϱ)2] = 1
nIEπ

{π1(θ) −π2(θ)}2
π2(θ)

+ o 	
n−1
,
show that the best importance density π is
π0(θ) ∝|π1(θ) −π2(θ)| ,
if

|π1(θ) −π2(θ)| dθ < ∞.

7.7
Exercises
381
[Note: Torrie and Valleau (1977) call this method umbrella sampling.]
Section 7.3.2
7.20 Given two densities π1(θ) = c1˜π1(θ) and π2(θ) = c2˜π2(θ) on the same
parameter space Θ,
a. For an arbitrary function h, express IEπ2[h(θ)˜π1(θ|x)] as an integral in terms
of π1 and π2.
b. Deduce the equality (7.3.4).
7.21
∗Chen et al. (2000) introduce the relative mean square error
E(r, ˆr) = IE[ˆr −r]
r
as a measure of the performances of an estimator ˆr of the constant ratio r.
a. Show that, if n = n1 + n2 and if n1/n2 goes to ϱ as n goes to inﬁnity,
E(r, BS
12) =
1
nϱ(1 −ϱ)

π1(θ)π2(θ){ϱπ1(θ) + (1 −ϱ)π2(θ)}h2(θ) dθ
	
π1(θ)π2(θ) dθ
2

for the estimator (7.3.5), where the dependence on x is omitted to simplify
the expression. (Hint: Use the δ-method.)
b. Deduce that the optimal choice of h is
h∗(θ) ∝
1
ϱπ1(θ) + (1 −ϱ)π2(θ) .
7.22 For the three link functions of Example 7.3.1, identify the latent variable
structure z identifying y as the indicator IIz≤xtβ.
Section 7.3.3
7.23 Consider a posterior distribution π(θ1, θ2, θ3|x) such that the three full
conditional distributions π(θ1|θ2, θ3, x), . . . and π(θ3|θ1, θ2, x) are available.
a. Show that
log m(x)
=
log f(x|ˆθ) + log π(ˆθ) −log π(ˆθ3|ˆθ1, ˆθ2, x)
−log π(ˆθ2|ˆθ1, x) −log π(ˆθ1|x) .
b. Show that π(θ1|x) can be approximated by
ˆπ(θ1|x) = 1
T
T

t=1
π(θ1, θ(t)
2 , θ(t)
3 |x) ,
where the (θ(t)
1 , θ(t)
2 , θ(t)
3 )’s are generated by Gibbs sampling.
c. Show that π(θ2|ˆθ1, x) can be approximated by
ˆπ(θ2|ˆθ1, x) = 1
T
T

t=1
π(θ2|ˆθ1, θ(t)
3 , x)
where the (θ(t)
2 , θ(t)
3 )’s are generated by Gibbs sampling from the conditional
distributions π(θ2|ˆθ1, θ(t−1)
3
, x) and π(θ3|ˆθ1, θ(t)
2 , x), that is, with θ1 being
kept equal to ˆθ1.

382
Model Choice
7
d. Extend to the case where p full conditional distributions are available and
evaluate the computing cost of this approximation method.
Section 7.3.4
7.24 In the setting of Example 7.3.7, show that the Jacobians of both the birth
and the split moves are given by
(1 −p(k+1)(k+1))k
and
pjk/(1 −u1) .
Section 7.4
7.25 For the prior distributions proposed by Clyde (1999),
a. Show that the posterior distribution of (γ1, . . . , γJ) conditional on σ is given
by (7.4.1).
b. Deduce that the most likely submodel corresponds to the regressors Xj
with weights ϱj larger than 1/2.
7.26
∗(George and Foster (1999)) For a normal regression model
y = β1x1 + . . . + βpxp + σϵ ,
ϵ ∼N(0, I) ,
we denote γ the index of one of the 2p submodels, qγ the corresponding number
of covariates, Xγ the corresponding matrix of regressors, ˆβγ the least-squares
estimate and s2
γ the sum of squares ˆβ′
γX′
γXγ ˆβγ.
a. Consider the prior distributions
βγ|σ, γ, c ∼Nqγ

0, cσ2 	
X′
γXγ

−1
,
π(γ|ω) = ωqγ (1 −ω)p−qγ .
Identify this prior with Raftery and Madigan (1994) prior.
b. Show that
π(γ|y, σ, c, ω) ∝exp

c
2(1 + c){s2
γ/σ2 −F(c, ω)qω}

where
F(c, ω) = 1 + c
c

2 log 1 + w
w
+ log(1 + c)

.
c. Deduce that the integrated posterior π(γ|y, σ, c, ω) is an increasing function
of s2
γ/σ2 −F(c, ω)qω.
d. Conclude that, with an appropriate choice of (c, ω), the log-posterior can be
any standard model-choice criterion, ranging from AIC (with F(c, ω) = 2),
to BIC (with F(c, ω) = log n), to Foster and George’s (1994) RIC (with
F(c, ω) = 2 log p).
Section 7.5
7.27 Show that the Kullback–Leibler divergence between a normal N(0, 1) and
a normal N(μ, σ2) distribution is
log σ + μ2 + 1
2σ2
−1
2 .
Extend to the Kullback–Leibler divergence between a normal N(μ0, σ2
0) and
a normal N(μ, σ2) distribution by the appropriate change of scale.
7.28 For each of the following distributions, show that the equality holds:

7.7
Exercises
383
(i) Bernoulli B(p)
d(f(· |p0), f(· |p)) = p0 log p0
p + (1 −p0) log 1 −p0
1 −p ;
(ii) Poisson P(λ)
d(f(· |λ0), f(· |λ)) = λ −λ0 + λ0 log λ0
λ ;
and
(iii) Normal N(μ, 1)
d(f(· |μ0), f(· |μ)) = (μ −μ0)2/2 .
7.29 When considering a variable-selection problem with p covariates,
a. Show that the number of submodels is 2p −1 if all models have a constant
term and 2p −2 otherwise.
b. Show that the number of submodels with p0 covariates is 	 p
p0

.
c. Using Stirling’s approximation, show that this number is also of order 2p
when p0 = p/2.
7.30 In a model-choice problem where (x, y) ∼g(x|α)f(y|x, θ),
a. Show that, for the Kullback–Leibler divergence,
d	
g(·|α)f(·|·, θ), g(·|α′)f(·|·, θ′)
=d(g(·|α), g(·|α′)) + IEα
+
d(f(·|x, θ), f(·|x, θ′),
,
where the expectation is taken for x ∼g(x|α).
b. Deduce that, if the submodel puts restrictions on θ only, for example ϕ(θ) =
0, the projection of (α, θ) is (α, θ⊥) where θ⊥is the solution of
arg
min
θ′; ϕ(θ′)=0 IEα
+
d(f(·|x, θ), f(·|x, θ′),
.
7.31 In the case of a normal linear regression model, y ∼N(x′β, σ2),
a. Show that, if z is a subvector of x, the Kullback–Leibler divergence between
N(x′β, σ2) and N(z′γ, σ2) is ||x′β −z′γ||2/2σ2, conditional upon x.
b. Deduce that the projection β⊥is given by β⊥= (zz′)x′β.
7.32 (Exercise 7.31 cont.)
Assume β is distributed from a conjugate prior
N(β0, Σ). Give the induced prior distribution of β⊥. What happens in the
case of a ﬂat prior on β?
7.33 In the setting of Example 7.5.3, examine whether the normalizing constant
of the geometric average of the Poisson and negative binomial distributions,
f(y|λ, m, α), can be computed.
7.34 When comparing two models M1 and M2, with densities f1(·|θ1) and
f2(·|θ2) both from an exponential family,
a. Show that the geometric average
f1(·|θ1)α f2(·|θ2)1−α
still belongs to an exponential family.
b. Show that, if (i = 1, 2)
fi(y|θi) = hi(y) exp{θi · ϕi(y) −ψi(θi)} ,
(ϕ1(y), ϕ2(y)) is a suﬃcient statistic for the geometric average.

384
Model Choice
7
c. Deduce that, if (ϕ1(y), ϕ2(y)) is of full rank, the dimension of this family
(see Deﬁnition 3.3.2) is the sum of the dimensions of f1 and f2.
d. In the special case when M1 is the exponential Exp(θ1) model and M2 is
the half-normal N +(0, 1/θ2) model, show that the geometric average model
is the half-normal distribution
N +

−
αθ1
(1 −α)θ2 ,
1
(1 −α)θ2

,
and give its normalizing constant.
Section 7.6
7.35 Examine the Neyman (1937) exponential family extension when f(x|θ) is
the density of (i) a Poisson P(θ), (ii) an exponential Exp(θ) and (iii) a normal
N(θ, 1) distribution. In the three cases, determine whether the normalizing
constant can be computed.
7.36 Given a density
f(y|θ) = h(y) exp{θ · ϕ(y) −ψ(θ)}
from an exponential family of dimension d (see Deﬁnition 3.3.2), show that its
Neyman extension still belongs to an exponential family of dimension d + 1.
7.37
∗(Castro, Conigliani and O’Hagan (1999)) Consider a multinomial model
r = (r0, . . . , rk) ∼Mk+1(n; α0, . . . , αk) ,
with α = (α1, . . . , αk).
a. If we denote (0 ≤b ≤1)
q2(r; b) =

f(r|α)π2(α) dα

f b(r|α)π2(α) dα ,
show that, under the improper prior π2(α) = 1/α1 . . . αk,
q2(r; b) = Γ(bn)
Γ(n)
k

j=0
Γ(rj)
Γ(brj) ,
if all the rj’s are positive. (Recall that, if one rj is 0, the posterior is not
deﬁned.)
b. If the constraint on the αj’s is that
αj =

k
j

μj(1 −μ)k−j ,
0 < μ < 1 ,
that is, if one wants to test whether the underlying model is truly binomial,
show that, under the prior π1(μ) = 1/μ(1 −μ),
q1(r; b)
=

f(r|α(μ))π1(μ) dμ

f b(r|α(μ))π1(μ) dμ
=
B(r, kn −sr)
B(br, b(kn −sr)
 k

j=0

k
j
rj1−b
,
where sr = r1 + . . . + krk and B(a, b) is the beta Be(a, b) normalizing
constant (see Appendix A).

7.7
Exercises
385
Table 7.7.1. Number of women in a queue of 10 persons in the London Under-
ground (Source: Hoaglin, Mosteller and Tukey (1985).)
Women
0
1
2
3
4
5
6
7
8
9
10
Occurrences
1
3
4
23
25
19
18
5
1
1
0
c. Show that the fractional Bayes factor associated with the constraint in b.
is BF
12 = q1(r; b)/q2(r; b).
d. Apply b. to the data in Table d..
e. If the constraint on the αj’s is that the model is a Poisson model, αj =
e−λλj/j! (j = 0, . . . , k), show that, under the prior π1(λ) = λ−t,
q1(r; b)
=

f(r|α(λ))π1(λ) dλ

f b(r|α(λ))π1(λ) dλ
=
Γ(sr −t + 1)bbsr−t+1nsr(b−1)
Γ(bsr −t + 1)
k

j=0
[j!](b−1)rj ,
where sr is deﬁned as in b..
f. Show that, in the setting of e., intrinsic Bayes factors do not apply, unless
cells are grouped together to obtain positive rj’s.
g. Show that, for a continuous model, this strategy is the Bayesian equivalent
of the χ2 test and, therefore, that it suﬀers from the same drawback, the
arbitrary grouping of observations in k cells.
7.38 Given F a cumulative distribution function in IR, the generalized inverse
of F is deﬁned as
F −(u) = inf{x; F(x) ≥u}
a. Show that, if u ∼U([0, 1]), F −(u) ∼F.
b. Deduce a simulation technique for both the Cauchy and exponential dis-
tributions.
c. How can you generalize this result for a multidimensional distribution?
7.39 In the setting of Verdinelli and Wasserman (1998), show that the parameter
θ is not identiﬁable under the alternative model M1. (Hint: Show that, for
every c.d.f. F(x) and every θ, there exists ψ such that F −
θ ◦Gψ = F −.)
7.40 (Verdinelli and Wasserman (1998)) Show (7.6.1) by establishing that, un-
der model M1,
x ∼h(x|θ, ψ)
=
g(F(x|θ)|ψ)dF(x|θ)
dx
=
g(F(x|θ)|ψ)f(x|θ) .
Note 7.8.1
7.41 Establish (7.8.1) by showing that
 
d
dλ log ˜π(θ|λ)π(θ|λ) dλ dθ = −
 λ2
λ1
d
dλc(λ) dλ .

386
Model Choice
7
7.42
∗(Exercise 7.41 cont.)
Show that the generalization of (7.8.1) to the
multidimensional case is
log(c(λ2)/c(λ1)) =
 1
0
IEλ(t)

k

j=1
dλj(t)
dt
∂
∂λj log ˜π(θ|λ)

dt ,
where λ(t) is a continuous function from [0, 1] in Λ such that λ(0) = λ1 and
λ(1) = λ2. Deduce the corresponding path sampler. (Hint: See Gelman and
Meng (1998) for a detailed resolution.)
Note 7.8.2
7.43 In the setting of Example 7.8.1, give the reversible jump steps which cor-
respond to the birth and death moves.
7.8 Notes
7.8.1 Path sampling
Gelman and Meng (1998) generalize bridge sampling to path sampling by con-
sidering the special case when both posteriors depend on hyperparameters λ1
and λ2,
π1(θ|x)
=
π(θ|λ1)
=
˜π(θ|λ1)/c(λ1) ,
π2(θ|x)
=
π(θ|λ2)
=
˜π(θ|λ2)/c(λ2) .
If those hyperparameters are real, with λ1 < λ2, the identity
log(c(λ2)/c(λ1)) = IE

1
π0(λ)
d
dλ log ˜π(θ|λ)

,
(7.8.1)
when integrated against the density π(θ|λ)π0(λ), holds for every density π0
with support [λ1, λ2] (Exercise 7.41).
The corresponding path sampling estimate of the logarithm of the Bayes factor
is then
BP S
12 = 1
n
n

i=1
d
dλ log ˜π(θi|λi)
π0(λi)
,
with a formal optimal choice for π0,
π0(λ) ∝
(
IE
 d
dλ log ˜π(θ|λ)
2 λ

.
See Exercise 7.42 for an extension to the multidimensional case.
7.8.2 Jump processes
A technique similar to reversible jump has been proposed in the literature
(see, e.g., Ripley (1987), Grenander and Miller (1994), or Phillips and Smith
(1996)). It does apply in a very general framework, but has only been used,
so far, for variable dimension problems, such as Stephens’s (2000) approach
to the setting of Example 7.1.2.
This method is based on jump processes: it generates a continuous-time jump
process on the space (7.2.1), that is, a stochastic process (ξt)t∈IR+ that remains
in a given state (i, θi) according to an exponential schedule, T ∼Exp(ϕi(θi)),
where ϕ is called the intensity of the process, and then jumps to a new state j

7.8
Notes
387
with probability qi→j and generates θj from a density hi→j(θj|θi). Then, sim-
ilar to the discrete-time setting (see (6.3.1)), if the parameters of the process,
ϕ, q and h, satisfy a detailed balance condition
π(i, θi)ϕi(θi)qi→jhi→j(θj|θi) = π(j, θj)ϕj(θj)qj→ihj→i(θi|θj) ,
then π(i, θi) is the stationary distribution of this Markovian process. For in-
stance, if hi→j(θj|θi) = gj(θj) and qi→j = 1/k, where k is the number of
states, the balance condition is
π(i, θi)ϕi(θi)gj(θj) = π(j, θj)ϕj(θj)gi(θi)
and a choice of intensity is ϕi(θi) ∝gi(θi)/π(i, θi). (Because the intensity
ϕi(θi) is the inverse of the average stay in (i, θi), this average stay is logically
proportional to π(i, θi).)
In the particular case when the moves are restricted to neighboring states,
that is, when qi→i+1 + qi→i−1 = 1 (with appropriate modiﬁcations at the
endpoints), the process is called a birth and death jump process. It is then
customary to write ϕi(θi) = β(θi) + δ(θi), β(θi) being the birth rate and δ(θi)
the death rate, and to remove the parameters qi→j. The process stays in state
(i, θi) an exponential Exp[β(θi) + δ(θi)] time, then moves to state (i + 1, θi+1)
with probability β(θi)/(β(θi)+δ(θi)), θi+1 being generated from K+
i (θi+1|θi),
and to state (i −1, θi−1) otherwise, θi−1 being generated from K−
i (θi−1|θi).
Example 7.8.1 (Example 7.1.2 continued)
For the mixture example,
the state labels i correspond to the numbers of components, a birth to the
addition of one component and a death to the removal of one component.
Then θi = (p1i, . . . , pii, μ1i, . . . , μii, σ1i, . . . , σii). In his implementation of the
birth and death jump algorithm, Stephens (2000) generates new components
from the prior distribution (where all components are i.i.d.) and chooses a
ﬁxed birth rate β(θi) = b. The balance condition then reads as
(i + 1)β(θi+1)L[(i + 1, θi+1)|x1, . . . , xn]π(i + 1) = bL[(i, θi)|x1, . . . , xn]π(i) ,
where L(θ|x1, . . . , xn) denotes the likelihood. (The coeﬃcient (i + 1) owes to
the fact that there are (i+1) components, and thus (i+1) possible removals.)
If we denote by θi/(pℓi, μℓi, σℓi) the parameter of the (i−1) component model
where the component (pℓi, μℓi, σℓi) has been removed, the birth and death al-
gorithm can thus be written as follows.
When in state (i, θi),
1. Compute component death rates (ℓ= 1, . . . , i)
βℓ(θi) = L[(i −1, θi/(pℓi, μℓi, σℓi))|x1, . . . , xn]
L[(i, θi)|x1, . . . , xn]
and take β(θi) = i
ℓ=1 βℓ(θi)
2. Generate jump time as T ∼Exp(β(θi) + b)
3. At time T, choose removal of
(pℓi, μℓi, σℓi)|x1, . . . , xn
with probability
βℓ(θi)
β(θi) + b

388
Model Choice
7
Otherwise, create
(p(i+1)(i+1), μ(i+1)(i+1), σ(i+1)(i+1))
from the prior distribution.
Notice that in Step 3, the new weight is generated from the marginal prior
distribution of p(i+1)(i+1), which is a Be(i, 1) distribution if the prior on
(p1(i+1), . . . , p(i+1)(i+1)) is a Dirichlet Di+1(1, . . . , 1) distribution.
∥
Capp´e, Robert and Ryd´en (2003) gives a reassessment of this technique, with
the conclusion that there are little theoretical and practical diﬀerences with
reversible jump.
7.8.3 Variable selection for generalized linear models
Here we present further developments on the variable selection technique
presented in Section 7.5. Consider, thus, a general exponential family (i =
1, . . . , n)
yi|θi ∼exp [ϕi {θiyi −ψ(θi)} + c(ϕi, yi)]
with a generalized linear model structure, which relates the mean to a vector
of covariates,
g(ψ′(θi)) = xt
iβ .
In this setting, the Kullback–Leibler divergence is available in closed form,
since
d(f(· |θ), f(· |θ0)) =
n

i=1
ϕi {ψ
′(θi)(θi −θ0
i ) −ψ(θi) + ψ(θ0
i )}
and the projection equations (j = 1, . . . , p)
n

i=1
ϕi ψ
′(θi) ∂θ0
i
∂βj =
n

i=1
ϕi ψ
′(θ0
i ) ∂θ0
i
∂βj ,
(7.8.2)
are equivalent to the system of likelihood equations, a fact that facilitates their
derivation.
In the case of a logit model,
P(yi = 1|xi, α) = 1 −P(yi = 0|xi, α) =
exp(αtxi)
1 + exp(αtxi) ,
the projection α⊥of α for the covariates zi (which are subvectors of the xi’s)
is, for instance, associated with β solution of
n

i=1
exp βtzi
1 + exp βtzi zi =
n

i=1
exp αtxi
1 + exp αtxi zi ,
which indeed provides a formal equivalence with the maximum likelihood es-
timator equations
n

i=1
exp βtzi
1 + exp βtzi zi =
n

i=1
yizi .

7.8
Notes
389
A consequence of (7.8.2) is that the Kullback–Leibler projections are transitive
in the sense that, if ω is a subvector of z, itself a subvector of x, we get
n

i=1
exp γtωi
1 + exp γtωi ωi
=
n

i=1
exp βtzi
1 + exp βtzi ωi
=
n

i=1
exp αtxi
1 + exp αtxi ωi
in the logit example. This means, in other words, that the projection γ of the
projection β of α is the projection of α on the smaller subspace, a model-choice
version of the two projection theorem. Notice also that the distance between
these projections is additive:
d(f(· |α), f(· |γ)) = d(f(· |α), f(· |β)) + d(f(· |β), f(· |γ)) .
For the variable selection scheme exposed in Section 7.5, this means that,
once a submodel has been rejected as being too far from the full model, all its
submodels are also rejected. See Dupuis and Robert (2001) for more details.


CHAPTER 8
Admissibility and Complete Classes
You can turn the worse that comes to your advantage if you only think, his
father has always said, and certainly Abell Cauthon was the best horse trader
in the Two Rivers. (...) All because he thought about things from every side
that there was.
Robert Jordan, The Dragon Reborn, Book III of the Wheel of Time.
8.1 Introduction
Chapters 1 through 3 repeatedly mentioned that the Bayes estimators are
instrumental for the frequentist notions of optimality, in particular, for ad-
missibility. This chapter provides a more detailed description of this phe-
nomenon. In Section 8.2, we consider the performances of the Bayes and
generalized Bayes estimators in terms of admissibility. Then, Section 8.3
studies Stein’s suﬃcient condition in order to relate the admissibility of a
given estimator with a sequence of prior distributions. The notion of com-
plete class introduced in Section 8.4 is also fundamental because it provides
a characterization of admissible estimators, or at least a substantial reduc-
tion in the class of acceptable estimators. We show that, in many cases,
the set of the Bayes estimators constitutes a complete class and that, in
other cases, it is necessary to include generalized Bayes estimators. From
a more general, although non-Bayesian, perspective, Section 8.5 presents
a method introduced by Brown (1971), and developed by Hwang (1982b),
that provides necessary admissibility conditions. For a more technical sur-
vey of these topics, see Rukhin (1995).
8.2 Admissibility of Bayes estimators
8.2.1 General characterizations
Recall the two following results about the admissibility of (proper) Bayes
estimators, already stated in Chapter 2 (Propositions 2.4.22 and 2.4.23):

392
Admissibility and Complete Classes
8
Proposition 8.2.1
If a Bayes estimator is unique, it is admissible.
Proposition 8.2.2
When the risk function is continuous in θ for every
estimator δ, if π is equivalent to the Lebesgue measure on Θ, that is, is
absolutely continuous with a positive density on Θ, then a Bayes estimator
associated with π is admissible.
On the contrary, if the support of π is diﬀerent from the whole space, it is
possible that an associated Bayes estimator is inadmissible. Similarly, the
Bayes estimators will often be inadmissible when the Bayes risk is inﬁnite.
Example 8.2.3
Consider a normal setting x ∼N(θ, 1) with a conjugate
prior θ ∼N(0, σ2). The posterior distribution is then N(
σ2
σ2+1x,
σ2
σ2+1) and
the Bayes estimator under quadratic loss is δπ(x) =
σ2
σ2+1x, which is ad-
missible, as shown in Corollary 8.2.14. On the contrary, if the quadratic
loss is modiﬁed into
Lα(θ, δ) = eθ2/2α(θ −δ)2,
the corresponding Bayes estimator is inadmissible for α small enough. In
fact, the formal generalized Bayes estimator associated with Lα is
δπ
α(x) =
 ∞
−∞θeθ2/2αe−(θ−δπ(x))2(σ2+1)/2σ2dθ
 ∞
−∞eθ2/2αe−(θ−δπ(x))2(σ2+1)/2σ2dθ ,
provided both integrals are ﬁnite. Since
exp
 θ2
2α −(θ −δπ(x))2 σ2 + 1
2σ2

= exp

−θ2
2 (σ2 + 1
σ2
−1
α) + δπ(x)θσ2 + 1
σ2
−δπ(x)2 σ2 + 1
2σ2

,
δπ
α is deﬁned for α >
σ2
σ2+1 and
δπ
α(x)
=
σ2 + 1
σ2
σ2 + 1
σ2
−α−1
−1
δπ(x)
=
α
α −
σ2
σ2+1
δπ(x).
The corresponding Bayes estimator is
r(π) =
 +∞
−∞
eθ2/2αe−θ2/2σ2dθ,
that is, is inﬁnite for α ≤σ2. Moreover, since
α
α −
σ2
σ2+1
δπ(x)
=
α
α −
σ2
σ2+1
σ2
σ2 + 1x
=
α
α σ2+1
σ2
−1
x,

8.2
Admissibility of Bayes estimators
393
the Bayes estimator δπ
α(x) is of the form cx with c > 1 when
α > ασ2 + 1
σ2
−1,
that is, when α < σ2. And, in this case,
R(θ, δπ
α)
=
IEθ[(cx −θ)2]eθ2/2α
=
{(c −1)2θ2 + c2}eθ2/2α > eθ2/2α
implies that δπ
α is inadmissible, since it is dominated by δ0(x) = x. However,
δ0 is also a Bayes estimator under Lα when α < σ2, since the Bayes risk is
inﬁnite. It is interesting to recognize that the limiting case α = σ2 leads to
the admissible estimator δπ
σ2(x) = x with an inﬁnite Bayes risk.
∥
Example 8.2.4
Consider y ∼σ2χ2
p. The conjugate prior distribution
for σ2 is the inverse gamma distribution IG(ν/2, α/2) (see Chapter 3) and
π(σ2|y) is the distribution IG((ν +p)/2, (α+y)/2), leading to the following
posterior expectation:
δπ
ν,α(y) = IEπ[σ2|y] =
α + y
ν + p −2.
Consider ν = 2. In this case, δπ(y) = (y/p)+(α/p). Since y/p is an unbiased
estimator of σ2, the estimators δπ
2,α are not admissible under square error
(as α > 0). The same result holds when ν < 2. It is easy to check that the
Bayes risk of δπ is inﬁnite in this case (see Lehmann (1983, p. 270)).
∥
Example 8.2.5
The constant estimators δ0(x) = θ0 are the Bayes esti-
mators corresponding to a Dirac mass prior in θ0, and are almost always
admissible under quadratic losses. In fact,
IEθ0(δ(x) −θ0)2 = (IEθ0[δ(x)] −θ0)2 + varθ0(δ(x)) = 0
implies that varθ0(δ(x)) = 0 and therefore that δ(x) = θ0 uniformly, unless
the distribution is degenerated in θ0 (see Exercise 8.4).
∥
A result similar to Proposition 8.2.2 can be established in the discrete
case (the proof is straightforward and left as an exercise).
Proposition 8.2.6
If Θ is a discrete set and π(θ) > 0 for every θ ∈Θ,
then a Bayes estimator associated with π is admissible.
8.2.2 Boundary conditions
We saw in Section 3.3 that, if x has a distribution from an exponential
family
f(x|θ) = h(x)eθ.T (x)−ψ(θ),

394
Admissibility and Complete Classes
8
the conjugate distributions are also in exponential families and the posterior
expectation of the mean of T (x) is then aﬃne in T (x), that is,
IEπ[∇ψ(θ)|x] = T (x) + t0
λ + 1
=
1
λ + 1T (x) + γ0λ
λ + 1,
(8.2.1)
when
π(θ|t0, λ) = eθ.t0−λψ(θ)
and γ0 = t0/λ. In the case when θ ∈IR and the natural parameter space
is N = [θ, ¯θ], Karlin (1958) exhibits a suﬃcient admissibility condition for
these estimators of the mean (see also Exercises 8.1 and 8.2).
Theorem 8.2.7
If λ > 0, a suﬃcient condition for the estimator (8.2.1)
to be admissible under a quadratic loss is that, for every θ < θ0 < ¯θ,
 ¯θ
θ0
e−γ0λθ+λψ(θ) dθ =
 θ0
θ
e−γ0λθ+λψ(θ) dθ = +∞.
This theorem is derived from the Cram´er–Rao inequality (Lehmann and
Casella (1998)). It also appears as a corollary to Stein’s necessary and suﬃ-
cient condition (see Section 8.3.3). Berger (1982b) considers the reciprocal
to Theorem 8.2.7, that is, shows that, under a few additional assumptions,
this condition is also necessary (see Exercise 8.12).
Example 8.2.8 (Example 8.2.4 continued)
For the chi-squared dis-
tribution, the natural parameterization is
θ = 1
σ2 ,
T (y) = −1
2y,
ψ(θ) = −p
2 log(θ),
and
 c
0
e−γ0λθθ−λp/2 dθ
is inﬁnite if λp ≥2. Similarly,
 +∞
c
e−γ0λθθ−λp/2 dθ = +∞
if γ0λ < 0 or γ0λ = 0 and λp ≤2. Therefore, the Bayes estimator
δπ(y) = γ0λ
1 + λ −
1
1 + λ
y
2
is admissible if γ0 = 0 and λ = 2/p or γ0 < 0 and λ ≥2/p; these conditions
lead to the estimators
ϕ1(y) =
p
p + 2
−y
2

and
ϕ2(y) = γ0λ
1 + λ +
1
1 + λ
−y
2

,
for the estimation of IEσ(−y/2) = −p
2σ2, that is, to the following admissible
Bayes estimators of σ2:
δ1(y) =
y
p + 2
and
δ2(y) = ay + b,
b > 0, 0 ≤a ≤
1
p + 2.
∥

8.2
Admissibility of Bayes estimators
395
Example 8.2.9
Consider x ∼B(n, p). The natural parameterization is
given by θ = n log(p/q) since
f(x|θ) =
n
x

e(x/n)θ 
1 + eθ/n−n
.
Then the two integrals
 θ0
−∞
e−γ0λθ 
1 + eθ/nλn
dθ
and
 +∞
θ0
e−γ0λθ 
1 + eθ/nλn
dθ
cannot diverge simultaneously if λ < 0. Consider thus λ > 0. The second
integral is divergent at +∞if λ(1 −γ0) > 0, that is, if γ0 < 1. And the
ﬁrst integral is divergent at −∞if γ0λ ≥0. We then derive from Theorem
8.2.7 that a class of admissible Bayes estimators of p is
δπ(x) = ax
n + b,
0 ≤a ≤1,
b ≥0,
a + b ≤1.
∥
8.2.3 Inadmissible generalized Bayes estimators
Given that the Bayes estimators are not necessarily admissible, inadmis-
sibility occurs more frequently for generalized Bayes estimators. The par-
ticular case when the Bayes risk of a generalized Bayes estimator is ﬁnite
(and thus when this estimator is admissible—see Proposition 2.4.25) does
not occur very often, except in testing and other bounded loss settings (see
Example 2.4.26), and it is then necessary to use more advanced results to
establish admissibility, such as Stein’s condition (Section 8.3.3).
Example 8.2.10
Consider x ∼Np(θ, Ip) and δ0(x) = x; δ0 is a general-
ized Bayes estimator for the prior distribution π(θ) = 1. The Stein eﬀect
(Note 2.8.2) states that δ0 is admissible under quadratic loss if p ≤2 (see
Corollary 8.2.14) and inadmissible otherwise.
∥
Example 8.2.11 The prior distribution used in Example 8.2.10 can also
produce more extreme cases of inadmissibility. For instance, if π(θ) = 1
and if the parameter of interest is η = ||θ||2, Example 3.5.7 has shown that
the posterior distribution of η is a χ2
p(||x||2) distribution, leading to the
following generalized Bayes estimator:
δπ(x) = ||x||2 + p.
As already stated, this estimator is inadmissible and dominated by ˜δ(x) =
(||x||2 −p)+. Example 3.5.7 proposes an alternative prior distribution that
is more appropriate in this setting.
∥
Example 8.2.12
Consider x ∼G(α, θ) when α is known. Because θ is a
scale parameter, π(θ) = 1/θ is an appropriate noninformative distribution

396
Admissibility and Complete Classes
8
(see Chapter 9). The corresponding posterior distribution is G(α, x) and
thus
δπ(x) = α
x
is the generalized Bayes estimator of θ under quadratic loss. For an esti-
mator of the form δc(x) = c/x, the quadratic risk is
R(θ, δc) = IEθ
 c
x −θ
2
= c2IEθ(x−2) −2cθIEθ(x−1) + θ2.
For α > 2, we have
IEθ(x−2)
=
1
Γ(α)
 +∞
0
x−2xα−1θαe−θx dx
=
1
Γ(α)
 +∞
0
θαxα−3e−θx dx
=
θ2 Γ(α −2)
Γ(α)
=
θ2
(α −1)(α −2)
and
IEθ(x−1)
=
1
Γ(α)
 +∞
0
θαxα−2e−θx dx
=
θΓ(α −1)
Γ(α)
=
θ
α −1.
This implies that the best estimator of the form δc is associated with
c∗= θIEθ(x−1)
IEθ(x−2) =
θ2/(α −1)
θ2/(α −1)(α −2) = α −2,
and thus that δπ is dominated by δc∗.
∥
The three previous examples show that all behaviors are indeed possible
for generalized Bayes estimators, from the admissibility of x for p = 1, 2
(Example 8.2.10) to the strong inadmissibility of the estimators in Exam-
ples 8.2.11 and 8.2.12, including the weak1 inadmissibility of x for p ≥3
(Example 8.2.10).
8.2.4 Diﬀerential representations
For multidimensional exponential families, Brown and Hwang (1982) have
extended Theorem 8.2.7 to arbitrary generalized prior distributions. Con-
sider a random variable
x ∼f(x|θ) = h(x)eθ.x−ψ(θ),
1 In fact, δ0(x) = x is still a minimax estimator for every dimension and the estimators
that dominate δ0 only improve signiﬁcantly upon δ0 (in terms of risk) in a relatively
small region of the sample space (see, for instance, Bondar (1987)). The practical
implication of this property is that, without prior information on θ, the domination
of δ0 is mostly formal.

8.2
Admissibility of Bayes estimators
397
where θ and x belong to IRp and recall that the mean of this distribution
is ∇ψ(θ). Given a measure π with density g on Θ, we assume that
Ix(∇g) =

||∇g(θ)||eθ.x−ψ(θ) dθ < +∞.
(8.2.2)
When estimating ∇ψ(θ) under quadratic loss, the Bayes estimator associ-
ated with g can be written as the diﬀerential representation
δg(x) = x + Ix(∇g)
Ix(g) .
(8.2.3)
The following conditions on g:

{||θ||>1}
g(θ)
||θ||2 log2(||θ|| ∨2)dθ
<
∞,
(8.2.4)
 ||∇g(θ)||2
g(θ)
dθ
<
∞,
(8.2.5)
and
∀θ ∈Θ,
R(θ, δg) < ∞,
(8.2.6)
are suﬃcient to establish the admissibility of δg.
Theorem 8.2.13
Under the conditions (8.2.4), (8.2.5), and (8.2.6), the
estimator (8.2.3) is admissible.
The proof of this result is deferred until Example 8.3.5 because it relies
on Blyth’s condition given in Section 8.3.2. Notice that this result has
important consequences, since it covers the estimation of the expectation
parameter for all continuous exponential families on IRp. For instance, it
gives, as a particular application, Stein’s (1955b) admissibility result for
all exponential families. It also generalizes Zidek (1970), who was dealing
with the one-dimensional case (see Exercise 8.8).
Corollary 8.2.14
If Θ = IRp and p ≤2, the estimator δ0(x) = x is
admissible.
Proof. Consider g ≡1, then ∇g ≡0 and δg(x) = x. Conditions (8.2.4),
(8.2.5) and (8.2.6) being satisﬁed, δg is admissible.
22
Example 8.2.15 (Example 8.2.10 continued)
If x ∼Np(θ, Ip), θ is
the natural parameter of the distribution and Corollary 8.2.14 actually
reproduces Stein’s (1955a) original result. Notice that Theorem 8.2.13 also
provides a means to check the admissibility of other generalized Bayes
estimators of θ, including those considered by Strawderman (1971) (see
Exercise 10.5) and Berger (1980a).
∥
Example 8.2.16 Consider x1, x2, two random variables from P(λi) (i =
1, 2). If θi = log(λi), δ0(x) = (x1, x2) is an admissible estimator of (λ1, λ2) =
(eθ
1, eθ
2). This result does not extend to larger dimensions, as shown by
Hwang (1982a) and Johnstone (1984).
∥

398
Admissibility and Complete Classes
8
Brown and Hwang (1982) present various generalizations of Theorem
8.2.13 allowing us to include cases where Θ ̸= IRp, like the gamma and
geometric distributions. They also show that, in the special case of p obser-
vations xi from independent Poisson distributions, P(λi), the generalized
Bayes estimator
δCZ(x) =

1 −
β + p −1
β + p −1 + S

x,
with S = 
i xi, which was proposed by Clevenson and Zidek (1975) to
improve upon x = (x1, . . . , xp), is admissible for β > 0 and p ≥2 under
the loss
L(θ, δ) =
p

i=1
1
λi
(δ −λi)2.
Das Gupta and Sinha (1986) also provide suﬃcient admissibility conditions
for the estimation of independent gamma means.
8.2.5 Recurrence conditions
In the particular case of a normal multidimensional distribution, Np(θ, Σ),
when Σ is known, Brown (1971) characterizes more thoroughly the admis-
sible Bayes estimators under quadratic loss by providing a necessary and
suﬃcient condition, based on a Markovian representation of the estima-
tion problem. (Notice that Shinozaki (1975) implies that the choice Σ = Ip
does not restrict the generality of the treatment (see Section 2.5.1 and
Exercise 2.39).)
Theorem 8.2.17 Consider x ∼Np(θ, Ip). A generalized Bayes estimator
of the form
δ(x) = (1 −h(||x||))x
is
(i) inadmissible if there exist ϵ > 0 and K < +∞such that, for ||x|| > K,
||x||2h(||x||) < p −2 −ϵ;
and
(ii) admissible if there exist K1 and K2 such that h(||x||)||x|| ≤K1 for
every x and, for ||x|| > K2,
||x||2h(||x||) ≥p −2.
The proof of this result is quite advanced and the derivation of (i) and
(ii) involves the recurrence or the transience of a random process2 associ-
ated with δ. (See Srinivasan (1981) for a simpler description.) Part (i) also
2 Random walks are always recurrent in dimensions 1 or 2 and may be transient for
larger dimensions (see Feller (1971) or Meyn and Tweedie (1993)). The connection
exhibited by Brown (1971) then points out that the similar role of p = 3 as a limiting
case in both problems is not coincidental.

8.2
Admissibility of Bayes estimators
399
appears as a consequence of Lemma 8.5.1. Notice the factor (p −2), which
delineates the boundary between admissibility and inadmissibility of the
usual estimator δ0(x) = x. The relation between this result and the Stein
phenomenon is explained in Section 8.5.
Johnstone (1984) provides an equivalent to Theorem 8.2.17 in the case
of a Poisson model. If xi ∼P(λi) (i = 1, . . . , p), the parameter λ =
(λ1, . . . , λp) is estimated under the loss
p

i=1
1
λi
(δi −λi)2.
Then:
Theorem 8.2.18
A generalized Bayes estimator of the form
δ(x) = (1 −h(s))x,
where s = 
i xi, is
(i) inadmissible if there exist ϵ > 0 and K < +∞such that, for s > K,
sh(s) < (p −1 −ϵ);
and
(ii) admissible if there exist K1 and K2 such that √s h(s) ≤K1 for every
s and, for s > K2,
sh(s) ≥(p −1).
Eaton (1992) exhibits connections similar to those of Brown (1971) be-
tween the admissibility of an estimator and the recurrence of an associated
Markov chain. We mention the main results of this paper below but urge
the reader to investigate the paper not only for the proofs, but also for its
deeper implications. The problem considered by Eaton (1992) is to deter-
mine whether, for a bounded function g(θ), a generalized Bayes estimator
associated with a prior measure π is admissible under quadratic loss. As-
suming that the posterior distribution π(θ|x) is well deﬁned, we consider
the transition kernel
K(θ|η) =

X
π(θ|x)f(x|η) dx,
(8.2.7)
which is associated with a Markov chain (θ(n)) generated as follows. The
transition from θ(n) to θ(n+1) is done by generating ﬁrst x ∼f(x|θ(n)), and
then θ(n+1) ∼π(θ|x). (For the use of this kernel in Markov Chain Monte
Carlo methods, and more details about Markov chain theory, see Chapter
6.) For every measurable set C such that π(C) < +∞, we deﬁne
V (C) =

h ∈L2(π); h(θ) ≥0 and h(θ) ≥1 when θ ∈C

and
Δ(h) =
 
{h(θ) −h(η)}2 K(θ|η)π(η) dθ dη.

400
Admissibility and Complete Classes
8
The following result then characterizes admissibility for all bounded func-
tions in terms of Δ and V (C), that is, independently of the estimated
functions g:
Theorem 8.2.19
If, for every C such that π(C) < +∞,
inf
h∈V (C) Δ(h) = 0,
(8.2.8)
then the Bayes estimator IEπ[g(θ)|x] is admissible under quadratic loss for
every bounded function g.
This result is obviously quite general, but only mildly helpful in the
sense that the practical veriﬁcation of (8.2.8) for every set C can be over-
whelming. Notice also that (8.2.8) always holds when π is a proper prior
distribution, since h ≡1 belongs to L2(π) and Δ(1) = 0 in this case.
The extension to improper priors then considers approximations of 1 by
functions in V (C). (See Chapter 9 for a similar relation between amenabil-
ity and minimaxity.) Eaton (1992) exhibits a connection with the Markov
chain (θ(n)), which gives a condition equivalent to Theorem 8.2.19. First,
for a given set C, a stopping rule σC is deﬁned as the ﬁrst integer n > 0
such that (θ(n)) belongs to C (and +∞otherwise). The chain (θ(n)) is said
to be π-recurrent if the probability that σC is ﬁnite is 1 for π-almost every
starting point θ(0).
Theorem 8.2.20 For every set C such that π(C) < +∞,
inf
h∈V (C) Δ(h) =

C

1 −P(σC < +∞|θ(0) = η)
 
π(η) dη.
Therefore, the generalized Bayes estimators of bounded functions of θ are
admissible if the associated Markov chain (θ(n)) is π-recurrent.
We refer to Note 8.7.1 and to Eaton (1992, 1999) for extensions, exam-
ples, and comments on this result. Its principal appeal is that, besides its
mathematical elegance, the veriﬁcation of the recurrence of the Markov
chain (θ(n)) is much easier to operate than the determination of the lower
bound of Δ(h). Moreover, it suggests a possible numerical veriﬁcation of
admissibility based on the generation of the chain (θ(n)), which is in a way
similar to the numerical minimaxity veriﬁcation proposed in Berger and
Robert (1990).
8.3 Necessary and suﬃcient admissibility conditions
The results obtained in the previous section only apply to generalized Bayes
estimators. Moreover, some conditions are rather arduous to verify—see,
e.g., (8.2.4) or (8.2.5). We present in this section a general necessary and
suﬃcient admissibility condition that does not require estimators to be
generalized Bayes estimators. It somehow formalizes our repeated asser-
tion that “admissible estimators are limits of Bayes estimators....” A ﬁrst

8.3
Necessary and suﬃcient admissibility conditions
401
version of Stein’s condition only allows for the comparison of continuous
risk estimators, but Section 8.3.1 shows why it is usually suﬃcient to con-
sider continuous risk estimators.
8.3.1 Continuous risks
It is often necessary to restrict the scope of the study to continuous risk
function estimators in order to produce a suﬃcient admissibility condition.
However, in some settings, all estimators have continuous risks. In other
cases, the admissible estimators necessarily have continuous risks.
Lemma 8.3.1
Consider Θ ⊂IRm. The loss function L(θ, δ) is assumed
to be bounded and continuous as a function of θ for every δ ∈D. If f(x|θ)
is continuous in θ for every x, the risk function of every estimator is con-
tinuous.
Proof. Given an estimator δ, the diﬀerence of the risks in θ and θ′ ∈Θ is
|R(θ, δ) −R(θ′, δ)|
=


L(θ, δ(x))f(x|θ) dx −

L(θ′, δ(x))f(x|θ′) dx

≤
 L(θ, δ(x)) −L(θ′, δ(x))
f(x|θ) dx
+


L(θ, δ(x))(f(x|θ) −f(x|θ′)) dx
.
Since L is continuous and bounded by C, there exist η0 > 0 and a compact
set K0 such that

Kc
0
f(x|θ) dx <
ϵ
8C
and

K0
L(θ, δ(x)) −L(θ′, δ(x))
f(x|θ) dx < ϵ
4
when ||θ −θ′|| < η0. Thus,
 L(θ, δ(x)) −L(θ′, δ(x))
f(x|θ) dx < ϵ
2.
Moreover, f(x|θ) being a continuous function of θ, a similar argument can
be applied: there exist η1 > 0 and a compact set K1 such that


L(θ, δ(x))(f(x|θ) −f(x|θ′)) dx

≤
C

K1
f(x|θ) −f(x|θ′)
 dx
+ C

Kc
1
[f(x|θ) + f(x|θ′)] dx < ϵ
2
and

Kc
1
f(x|θ) dx <
ϵ
8C ,
when ||θ −θ′|| < η1. Therefore, R(θ, δ) is continuous.
22
Lemma 8.3.1 is somehow of limited interest, since the most delicate ad-
missibility problems occur when L is unbounded. Some settings still allow

402
Admissibility and Complete Classes
8
for a reduction in the class of estimators to consider to the class of contin-
uous risk estimators, that is, for a complete class characterization.
Deﬁnition 8.3.2
A class C of estimators is said to be complete if, for
every δ′ ̸∈C, there exists δ ∈C that dominates δ′. The class is essentially
complete if, for every δ′ ̸∈C, there exists δ ∈C that is at least as good as δ′.
Apart from trivial cases such as the class of all estimators, the determi-
nation of useful complete classes is not always possible. For instance, there
are cases when the class of admissible estimators is not a complete class al-
though such settings seldom occur (see Blackwell and Girshick (1954, The-
orem 5.7.1) or Brown (1976)). Section 8.4 analyzes the relations between
Bayes estimators, generalized Bayes estimators, and complete classes. The
following result is a complete class lemma giving suﬃcient conditions for
considering continuous risk estimators only.
Lemma 8.3.3
Consider a statistical decision model X, Θ ⊂IR with a
closed decision space D ⊂IR. Assume that f(x|θ) has the monotone likeli-
hood ratio property and is continuous in θ. If
(i) L(θ, d) is a continuous function of θ for every d ∈D;
(ii) L is decreasing in d for d < θ and increasing for d > θ; and
(iii) there exist two functions K1 and K2 bounded on the compact subsets
of Θ, such that
L(θ1, d) ≤K1(θ1, θ2)L(θ2, d) + K2(θ1, θ2),
then the estimators with ﬁnite and continuous risks form a complete class.
See Ferguson (1967) and Brown (1976) for additional results. For in-
stance, it is possible to show that, if the problem is monotone, then mono-
tone estimators constitute a complete class (see Exercise 8.23 and Theorem
5.4.4).
8.3.2 Blyth’s suﬃcient condition
Prior to Stein’s (1955b) derivation of his necessary and suﬃcient condition
(Section 8.3.3), Blyth (1951) proposed a suﬃcient admissibility condition,
relating admissibility of an estimator with the existence of a sequence of
prior distributions approximating this estimator.
Theorem 8.3.4
Consider a nonempty open set Θ ⊂IRp. Assume that
the estimators with continuous risk constitute a complete class. If, for a
continuous risk estimator δ0, there exists a sequence (πn) of generalized
prior distributions such that
(i) r(πn, δ0) is ﬁnite for every n;

8.3
Necessary and suﬃcient admissibility conditions
403
(ii) for every nonempty open set C ⊂Θ, there exist K > 0 and N such
that, for every n ≥N, πn(C) ≥K; and
(iii)
lim
n→+∞r(πn, δ0) −r(πn) = 0;
then the estimator δ0 is admissible.
Proof. If δ0 is not admissible, there exists an estimator δ′ dominating δ0,
that is, such that R(θ, δ) −R(θ, δ′) ≥0 and
R(θ, δ) −R(θ, δ′) > ϵ
on an open set C ⊂Θ (for ϵ small enough). It then follows from assumptions
(i) and (ii), that for n ≥N,
r(πn, δ0) −r(πn)
≥
r(πn, δ0) −r(πn, δ′)
=
IEπ[R(θ, δ0) −R(θ, δ′)]
≥

C
(R(θ, δ0) −R(θ, δ′))πn(θ) dθ
≥
ϵ

C
πn(θ) dθ ≥ϵK.
22
This result can be used to establish the admissibility of generalized Bayes
estimators, since the measures π associated with these estimators can be
written as limits of sequences of proper distributions πn. However, the
choice of these sequences is not necessarily straightforward, as shown by
Berger (1982b) or Brown and Hwang (1982). Theorem 8.3.4 also applies to
other estimators, in settings where there exist admissible estimators that
are not generalized Bayes (see Section 8.4).
Example 8.3.5
The proof of Theorem 8.2.13 is a ﬁrst illustration of
Blyth’s condition. Consider hn taking values in [0, 1], diﬀerentiable, sat-
isfying hn(θ) = 0 if ||θ|| > n and hn(θ) = 1 on a set S such that

S
g(θ) dθ > 0.
We then deﬁne a sequence of associated measures with densities gn(θ) =
h2
n(θ)g(θ) and the corresponding Bayes estimator δn. Reverting to the no-
tation Ix(.) introduced in (8.2.2), the diﬀerence of the integrated Bayes
risks is
r(πn, δg) −r(πn)
=

||δg(x) −δn(x)||2Ix(gn) dx
=
 ;;;;
Ix(∇g)
Ix(g) −Ix(h2
n∇g)
Ix(gn)
−Ix(g∇hn)
Ix(gn)
;;;;
2
Ix(gn) dx,
using the representation (8.2.3). Therefore,
r(πn, δg) −r(πn)
≤
2
 ;;;;
Ix(∇g)
Ix(g) −Ix(h2
n∇g)
Ix(gn)
;;;;
2
Ix(gn) dx

404
Admissibility and Complete Classes
8
+2
 ;;;;
Ix(g∇hn)
Ix(gn)
;;;;
2
Ix(gn) dx
=
Bn + An.
The second term, An, is bounded from above by
4

||∇hn(θ)||2g(θ) dθ.
In the particular case where hn is
hn(θ) =
⎧
⎪
⎨
⎪
⎩
1
for ||θ|| < 1,
1 −log(||θ||)
log(n)
for 1 < ||θ|| < n,
0
otherwise,
we have actually
||∇hn(θ)||2 ≤
1
||θ||2 log2(max(||θ||, 2))II||θ||>1(θ) ,
and condition (8.2.4) implies that An converges to 0 as n goes to inﬁnity.
The ﬁrst term satisﬁes
Bn
=
 ;;;;Ix

gn
Ix(∇g)
Ix(g) −h2
n∇g
;;;;
2 -
(Ix(gn)) dx
=
 ;;;;Ix

gn
Ix(∇g)
Ix(g) −∇g
g
;;;;
2 -
(Ix(gn)) dx
≤

Ix
!
g
;;;;
Ix(∇g)
Ix(g) −∇g
g
;;;;
2"
dx.
Using (8.2.5), we can then derive from the dominated convergence theorem
that Bn goes to 0, since gn converges to g. This completes the proof of
Theorem 8.2.13.
∥
In practice, a usual way to apply Blyth’s condition to a generalized Bayes
estimator, δ0, is to exhibit a sequence of proper Bayes estimators that
converge to δ0, and then to de-normalize the sequence of the associated
prior distributions by a suitable weight.
Example 8.3.6
Consider x ∼N(θ, 1) and δ0(x) = x, an estimator of θ.
Because δ0 corresponds to π(θ) = 1 under quadratic loss, we choose πn as
the measure with density
gn(x) = e−θ2/2n,
that is, the density of the normal distribution N(0, n) without the normal-
izing factor 1/
√
2πn. Because the densities gn increase with n, condition
(ii) of Theorem 8.3.4 is satisﬁed, as well as (i): the Bayes estimator for πn
is still
δn(x) =
nx
n + 1,

8.3
Necessary and suﬃcient admissibility conditions
405
since the absence of the normalizing factor is of no importance in this case,
and
r(πn)
=

IR

θ2
(n + 1)2 +
n2
(n + 1)2

gn(θ) dθ
=
√
2πn
n
n + 1,
while
r(πn, δ0) =

IR
1 gn(θ) dθ =
√
2πn.
The two risks are then ﬁnite. Moreover,
r(πn, δ0) −r(πn) =
√
2πn/(n + 1)
converges to 0. The Blyth condition thus provides another proof of admis-
sibility for δ0(x) = x in the normal case. On the contrary, the proof of the
admissibility of δ0 in dimension two necessitates a more complex sequence
(see Stein (1955a)).
∥
Example 8.3.7
Consider x ∼B(m, θ). The inferential problem is to test
the null hypothesis H0 :
θ ≤θ0 under the quadratic loss introduced in
Section 5.4,
	
II[0,θ0](θ) −γ(x)

2 .
The p-value is then
ϕ(x) = Pθ0(X ≥x) =
m

k=x
m
k

θk
0(1 −θ0)m−k.
In this case, the natural conjugate distributions are beta distributions. This
suggests approximating ϕ(x) by a sequence of estimators associated with
an appropriate sequence of beta distributions. In fact, ϕ(x) can be written
(for x ̸= 0)
ϕ(x) =
1
B(x, m −x + 1)
 θ0
0
tx−1(1 −t)m−x dt = P(T ≤θ0|x)
when T ∼Be(x, m −x + 1), which corresponds to the generalized prior
distribution
π(θ) = θ−1
(0 < θ < 1).
Consider then πn with density
gn(θ) = θαn−1
on [0, 1] and let the sequence (αn) go to 0. In this case, the Bayes proce-
dure is
γπn(x) = P πn(θ ≤θ0|x) =
1
B(x + αn, m −x + 1)
 θ0
0
tx+αn−1(1−t)m−x dt

406
Admissibility and Complete Classes
8
and
r(πn)
=
m

k=0
B(k + αn, m −k + 1)γπn(k)(1 −γπn(k)),
r(πn, ϕ)
=
m

k=0
B(k + αn, m −k + 1)(γπn(k) −2γπnϕ(k) + ϕ2(k)).
Therefore,
r(πn, ϕ) −r(πn) =
m

k=0
B(k + αn, m −k + 1)(γπn(k) −ϕ(k))2.
If k ̸= 0, it is straightforward to verify that
lim
αn→0(ϕ(k) −γπn(k))2 = 0.
Similarly, we get
lim
α→0
 θ0
0 tα−1(1 −t)m−1dt
 1
0 tα−1(1 −t)m−1dt
= 1 ,
which takes care of the case k = 0. Moreover, condition (ii) is also satisﬁed.
The p-value ϕ is then admissible in this setting. Example 5.4.6 provides a
more direct proof of this result because the Bayes risk is ﬁnite.
∥
Examples 8.3.5 and 8.3.7 take advantage of a general result, namely, the
fact that, under quadratic loss, condition (iii) of Theorem 8.3.4 implies the
quadratic convergence of the Bayes estimators to δ0 in the sense of the
marginal measures.
Proposition 8.3.8
If L is a quadratic loss and if there exists a sequence
(πn) satisfying conditions (i), (ii) and (iii) of Theorem 8.3.4, then the Bayes
estimators δπn converge quadratically to δ0 for the marginal measures
mn(x) =

Θ
f(x|θ)πn(θ) dθ.
Proof. It is enough to write the diﬀerence of the risks as
r(πn, δ0) −r(πn)
=

X

Θ
(||δ0(x) −θ||2 −||δπn(x) −θ||2)πn(θ|x) dθ mn(x) dx
=

X

||δ0(x) −δπn(x)||2
+ 2(δ0(x) −δπn(x)) ·

Θ
(δπn(x) −θ)πn(θ|x) dθ

mn(x) dx
=

X
||δ0(x) −δπn(x)||2mn(x) dx,
since

Θ
(δπn(x) −θ)πn(θ|x) dθ = 0.
22

8.3
Necessary and suﬃcient admissibility conditions
407
Unfortunately, this convergence result depends on (mn), except when it
is possible to establish a uniform equivalence with the Lebesgue measure,
or another ﬁxed measure, in which case there is quadratic convergence in
the usual sense. This is, for instance, what occurs when the sequence (mn)
is increasing, as in Examples 8.3.5, 8.3.6, and 8.3.7. Section 8.3.4 provides
a more fundamental result of Brown (1986), which shows that pointwise
convergence of the δπn to δ0, independently of the measures mn, is actually
necessary.
8.3.3 Stein’s necessary and suﬃcient condition
The completion of the previous condition by Stein (1955b) and Farrell
(1968b) is even more important than Theorem 8.3.4, since it shows that all
admissible estimators are limits of sequences of Bayes estimators (in the
sense of the Bayes risk). The assumptions in Farrell (1968b) are that
(i) f(x|θ) is continuous in θ and strictly positive on Θ; and
(ii) the loss L is strictly convex, continuous and, if E ⊂Θ is compact,
lim
∥δ∥→+∞inf
θ∈E L(θ, δ) = +∞.
Notice that this second assumption necessarily eliminates bounded losses.
Theorem 8.3.9
Under the hypotheses (i) and (ii), an estimator δ is ad-
missible if, and only if, there exist a sequence (Fn) of increasing compact
sets such that Θ = <
n Fn, a sequence (πn) of ﬁnite measures with support
Fn, and a sequence (δn) of Bayes estimators associated with πn such that
(i) there exists a compact set E0 ⊂Θ such that infn πn(E0) ≥1;
(ii) if E ⊂Θ is compact, supn πn(E) < +∞;
(iii) limn r(πn, δ) −r(πn) = 0; and
(iv) limn R(θ, δn) = R(θ, δ).
This fundamental theorem underlies most of the admissibility and com-
plete class results presented in Section 8.4. A proof of Theorem 8.3.9 is
beyond our reach; see Farrell (1968b). The suﬃcient part of this result is
close to Blyth’s condition, but the necessary part allows for the exclusion
of many inadmissible estimators.
8.3.4 Another limit theorem
Brown (1986) provides an alternative, and quite general, characterization
of admissible estimators. Consider x ∼f(x|θ), with f(x|θ) > 0, and assume
that D is a closed convex set. Moreover, the loss function L is supposed to
be lower semicontinuous and such that
lim
||δ||→+∞L(θ, δ) = +∞.

408
Admissibility and Complete Classes
8
(Notice that this is roughly assumption (ii) of Farrell (1968b).) The main
result of Brown (1986) is to show that, under these conditions, the closure
(for the pointwise convergence) of the set of the Bayes estimators is a com-
plete class. The following convergence result rephrases this completeness
(see Brown (1986, pp. 254–267)).
Proposition 8.3.10 If L is strictly convex, every admissible estimator of
θ is a pointwise limit of Bayes estimators for a sequence of prior distribu-
tions with ﬁnite supports.
This result can be compared with the results of Dalal and Hall (1983) and
Diaconis and Ylvisaker (1985), presented in Section 3.4, which show that,
in an exponential family setting, every prior distribution is a limit of mix-
tures of conjugate prior distributions. Therefore, for exponential families,
an admissible estimator is also a limit of Bayes estimators associated with
a mixture of conjugate prior distributions. When the model is invariant
under spherical transformations, the distributions with ﬁnite support can
be replaced by distributions supported on embedded spheres, since they
preserve symmetry. In this case, if πc is the uniform distribution on the
sphere with radius c,
Sc = {θ; ||θ|| = c},
and δc is the associated Bayes estimator under quadratic loss, that is, the
posterior mean, Robert (1990a) derives the following limit theorem.
Proposition 8.3.11
If x ∼Np(θ, Ip) and π is a prior distribution which
is spherically symmetric around 0, then there exist two sequences, (qi
n) and
(ci
n), such that n
i=1 qi
n = 1 and
mπ(x) =

IRp f(x|θ)π(θ) dθ =
lim
n→+∞
n

i=1
qi
nmcin(x),
where
mci
n =

IRp f(x|θ)πci
n(θ) dθ.
Moreover, under quadratic loss,
δπ(x) =
lim
n→+∞
n

i=1
qi
nmci
n(x)

j qj
nmcj
n(x)
δcin(x).
(8.3.1)
Therefore, in the normal case, every Bayes estimator associated with
a spherically symmetric prior distribution is a pointwise limit of Bayes
estimators associated with uniform distributions on spheres. Recall that
the estimators δc can be written as
δc(x) = c
Ip/2(||x||c)
Ip/2−1(||x||c)
x
||x||,
(8.3.2)
where Iν is the modiﬁed Bessel function (Exercises 4.35 and 4.36). It actu-
ally follows from Kempthorne (1988) that every admissible estimator δ(x)

8.4
Complete classes
409
can be written under the form (8.3.2) or that there exists an estimator δ′
of the form (8.3.2) equivalent to δ (in risk).
8.4 Complete classes
The previous section established in a general setting that admissible esti-
mators are limits of Bayes estimators from several points of view. In some
particular cases, it is possible to improve upon this description of admis-
sible estimators, and to show that they are necessarily generalized Bayes
estimators. Such results are interesting because, on one hand, they restrict
further the class of estimators to be considered and, on the other hand,
they point out the advantage of using solely Bayes and generalized Bayes
estimators from a frequentist point of view. This is, for instance, the case
when testing procedures are evaluated under a quadratic loss, as seen in
Section 5.4 (Theorems 5.4.3 and 5.4.4). This section provides similar results
for point estimation. Additional references are given by Brown (1986) and
Rukhin (1995).
As an introductory example, consider the extremely simple case when
Θ = {θ1, θ2}, since it allows for a graphical representation of the risk set,
R = {r = (R(θ1, δ), R(θ2, δ)), δ ∈D∗},
where D∗is the set of randomized estimators. Assume the risk set R is
bounded and closed from below, that is, such that all risk points on the
lower boundary of R are in R and have ﬁnite components. This is the case
when the loss is positive. This lower boundary, denoted Γ(R), is important
because it actually provides the admissible points of R. Indeed, if r ∈Γ(R),
there cannot exist r′ ∈R such that r′
1 ≤r1 and r′
2 ≤r2 with strict
inequality on one of the two axes. Moreover, for every r ∈Γ(R), there
exists a tangent line to R going through r, with positive slope and equation
p1r1 + p2r2 = k,
that is, such that every r′ ∈R satisﬁes p1r′
1 +p2r′
2 ≥k, as shown by Figure
8.4. (In fact, this is a consequence of the convexity of R.) This property
implies that r is a Bayes estimator for the prior distribution π(θi) = pi
(i = 1, 2), since it minimizes the Bayes risk p1r1 + p2r2. We derive from
this argument the following general result.
Proposition 8.4.1
If Θ is ﬁnite and if the risk set R is bounded and
closed from below, then the set of Bayes estimators constitutes a complete
class.
This characterization relies on a separating hyperplane theorem since, un-
der the assumptions of the theorem, there exists a hyperplane tangent to
the risk set for each point of the lower boundary, and that this hyperplane
deﬁnes a prior distribution on Θ by duality. The extension of this com-
plete class result to denumerable and non-denumerable parameter spaces

410
Admissibility and Complete Classes
8
0.0
0.5
1.0
1.5
2.0
0.0
0.5
1.0
1.5
2.0
r1
r2
R
p1r1 + p2r2
                    =k
Figure 8.4.1. Risk set and admissible estimators for Θ = {θ1, θ2}.
Θ calls for an equivalent generalization of separating hyperplane theorems
to spaces of functions on Θ. For instance, Brown (1976) mentions the fol-
lowing result, where
◦
S denotes the interior set of S.
Lemma 8.4.2
Consider S a convex subset of a topological vector space
E. If
◦
S ̸= ∅and y0 ̸∈
◦
S, there exists f ∈E∗such that S is included in
{y; f(y) ≥f(y0)}.
We derive from this lemma the following complete class result, due to
Wald (1950), which generalizes Proposition 8.4.1.
Theorem 8.4.3
Consider the case when Θ is compact and the risk set
R is convex. If all estimators have a continuous risk function, the Bayes
estimators constitute a complete class.
Proof.
This result is indeed a consequence of Lemma 8.4.2 since, if δ0 is
admissible, the risk function R(θ, δ0) belongs to the lower boundary of the
risk set. Therefore, there exists a linear functional on R, ψ∗, such that, for
every estimator δ,
ψ∗(R(·, δ)) ≥ψ∗(R(·, δ0)).
We can then derive from the Riesz representation theorem that there exists
a ﬁnite measure π on Θ such that
ψ∗(R(·, δ)) =

Θ
R(θ, δ)π(θ)dθ,
and that this measure can be renormalized into ˜π(θ) = π(θ)/π(Θ), thus
deﬁning a prior distribution. The above inequality can be rewritten as

R(θ, δ)˜π(θ) dθ ≥

R(θ, δ0)˜π(θ) dθ

8.4
Complete classes
411
and implies that δ0 is a Bayes estimator for ˜π.
22
If Θ is not a compact space, we have already seen examples where the
Bayes estimators cannot constitute a complete class. For instance, when es-
timating the mean θ of a normal random variable x ∼N(θ, 1), the estima-
tor δ(x) = x is admissible but is not a Bayes estimator. However, in many
cases, complete classes are made of generalized Bayes estimators (meaning,
obviously, Bayes and generalized Bayes estimators). For instance, Berger
and Srinivasan (1978) have established that, when estimating the natural
parameter θ of an exponential family
x ∼f(x|θ) = eθ·x−ψ(θ)h(x),
x, θ ∈IRk,
under quadratic loss, every admissible estimator is a generalized Bayes
estimator. They thus extend Brown (1971), who considered the normal
case.
Example 8.4.4
In the normal case, x ∼Np(θ, Ip), we repeatedly men-
tioned the truncated James–Stein estimator,
δJS(x) =

1 −p −2
||x||2
+
x.
(8.4.1)
Although well performing, this estimator is not admissible because, other-
wise, it would be a generalized Bayes estimator. Since δJS is not analytical,
this is impossible (see Exercise 8.26).
∥
Chow (1987) establishes a similar result for families with noncentrality
parameters, χ2
p(λ) and Fp,q(λ), thus giving an illustration of the complete-
ness of the generalized Bayes rules outside the framework of exponential
families. This complete class theorem leads, in particular, to the inadmis-
sibility of the classical estimator, (x −p)+, in the case of the distribution
χ2
p(λ), although Saxena and Alam (1982) have shown that this estimator
is already eﬃcient, since it dominates the maximum likelihood estimator
(see also Exercise 3.25).
Fraisse et al. (1990) derive a result similar to Berger and Srinivasan
(1978) in the presence of a nuisance parameter. Consider x = (u, z) with
u ∈IRk and z ∈IR. The density of x with respect to ν is
f(x|θ, δ) = h(u, z)eθ.u+δz−ψ(θ,δ) ,
with θ ∈Θ ⊂IRk and δ ∈Δ, a compact interval of IR∗
+. As in the normal
setting, the problem at hand is to estimate θ/δ under a quadratic loss. For
this model, the complete class theorem is given by:
Proposition 8.4.5
If ϕ is an admissible estimator of θ/δ, there exists a
measure π on Θ × Δ such that, for ν-almost every (u, z),
ϕ(u, z) =

Θ×Δ θeθ.u+δz−ψ(θ,δ)π(dθ, dδ)

Θ×Δ δeθ.u+δz−ψ(θ,δ)π(dθ, dδ) .
(8.4.2)

412
Admissibility and Complete Classes
8
Therefore, the complete class theorem of Berger and Srinivasan (1978) is
still valid in the presence of nuisance parameters. The proof of Proposition
8.4.5 actually relies on Proposition 8.3.10 (see Exercise 8.27).
Example 8.4.6 Consider x ∼Np(θ, σ2Ip) and s2 ∼σ2χ2
q, independent of
x. In this setting, δ0(x, s2) = x is also inadmissible for p ≥3. We consider
extensions of the James–Stein estimator (8.4.1) of the form
ϕ(x, s2) = (Ip −h(||x||2
C, s2)B)x,
where B and C are (p × p) matrices, h is a.e. diﬀerentiable, and ||x||2
C =
xtCx. These estimators are called matricial shrinkage estimators (see Judge
and Bock (1978)). Proposition 8.4.5 implies that a necessary condition for
ϕ to be admissible is that h be inﬁnitely diﬀerentiable and also that B and
C be proportional (see Exercise 8.28).
∥
Brown (1988) considers the estimation of the mean of an exponential
family, ξ(θ). In dimension one, he establishes that admissible estimators
have an integral expression close to (8.4.2). In fact, admissible estimators
are then equal to generalized Bayes estimators on intervals.
In the case of distributions with discrete support, the completeness of
generalized Bayes estimators does not always hold and complete classes
involve piecewise-Bayesian procedures (see Berger and Srinivasan (1978),
Brown (1981), and Brown and Farrell (1985)). The complete class results
obtained in Section 5.4 for testing under quadratic loss are of this type,
since we saw that admissible estimators are identical to generalized Bayes
estimators on truncation intervals.
8.5 Necessary admissibility conditions
When a complete class theorem restricting the choice of estimators to gen-
eralized Bayes estimators is not available, it is necessary to ﬁnd some other
way to exclude inadmissible estimators as much as possible. While being
necessary, Stein’s condition does not usually provide an eﬃcient tool for
the elimination of inadmissible estimators because its main practical in-
terest is Blyth’s suﬃcient condition. Moreover, the results of Section 8.3
cannot be applied in this general context, since they deal with generalized
Bayes estimators only. For quadratic losses, Hwang (1982b) developed a
technique introduced in Brown (1971) called STUB (for semi-tail upper
bounds), which gives an eﬀective necessary admissibility condition. It is
based upon the following lemma.
Lemma 8.5.1
Consider two estimators δ1 and δ2 with ﬁnite risks, such
that
R(θ, δ1) = IEθ
+
(δ1(x) −θ)tQ(δ1(x) −θ)
,
< R(θ, δ2)
for every θ ∈Θ and a given positive-deﬁnite matrix Q. Then, every

8.5
Necessary admissibility conditions
413
estimator δ satisfying a.e. the inequality
δ(x)tQ(δ1(x) −δ2(x)) < δ2(x)tQ(δ1(x) −δ2(x))
is inadmissible under every quadratic loss.
Proof.
Consider the new estimator δ′(x) = δ(x) + δ1(x) −δ2(x). Then
R(θ, δ′)
=
IEθ
+
(δ′(x) −θ)tQ(δ′(x) −θ)
,
=
R(θ, δ) + 2IEθ
+
(δ1(x) −δ2(x))tQ(δ(x) −θ)
,
+IEθ
+
(δ1(x) −δ2(x))tQ(δ1(x) −δ2(x))
,
≤
R(θ, δ) + 2IEθ
+
(δ1(x) −δ2(x))tQ(δ2(x) −θ)
,
+IEθ
+
(δ1(x) −δ2(x))tQ(δ1(x) −δ2(x))
,
=
R(θ, δ) + R(θ, δ1) −R(θ, δ2) < R(θ, δ)
and δ′ dominates δ.
22
This lemma may appear quite rudimentary at ﬁrst sight, but it is ac-
tually quite powerful because it introduces a new necessary admissibility
condition for every (risk) ordered couple (δ1, δ2). Moreover, since admissi-
bility does not depend on the matrix Q, it provides an extended battery of
inadmissibility criteria. In particular, it recovers the necessary admissibility
condition (i) of Theorem 8.2.17.
Example 8.5.2
Consider x ∼Np(θ, Ip). It follows from James and Stein
(1961) (see Note 2.8.2) that, among the estimators
δa(x) =

1 −
a
||x||2

x,
δp−2 is optimal for the usual quadratic loss. Therefore, Lemma 8.5.1 implies
that every estimator δ satisfying
δ(x)txa −(p −2)
||x||2
≤

1 −
a
||x||2

(a −(p −2))
(8.5.1)
is inadmissible. Consider δ of the form
δ(x) =

1 −h(x)
||x||2

x.
Then (8.5.1) implies that δ is inadmissible if
h(x) ≤a < p −2
or
h(x) ≥a > p −2.
Therefore, every estimator such that h is uniformly greater or smaller than
(p −2) is inadmissible. The necessary part of Theorem 8.2.17 follows by
considering instead the truncated estimators (a ≤p −2),
ϕa(x) =

1 −
a
||x||2 II[K,+∞[(||x||2)

x ,

414
Admissibility and Complete Classes
8
and showing that a∗= p −2 corresponds to the optimal estimator of this
class (see Exercise 8.20). Lemma 8.5.1 then implies that, if
h(x) ≤a < p −2
for ||x||2 > K, then the estimator δ is also inadmissible.
∥
In fact, notice also that in Lemma 8.5.1 the strict inequality R(θ, δ1) <
R(θ, δ2) does not need to hold for every θ, but only for some θ’s, as long
as R(θ, δ1) ≤R(θ, δ2) is satisﬁed for every θ ∈Θ.
Example 8.5.3 Das Gupta (1984) derives from Lemma 8.5.1 a necessary
admissibility condition for exponential distributions. If x1, . . . , xp are ran-
dom variables from Exp(θi), every estimator δ of (θ−1
1 , . . . , θ−1
p ) satisfying
p

i=1
x−3
i
δi(x) ≤
p

i=1
x−3
i
δB
c,i(x)
for xi ≤M, x = (x1, . . . , xn), and
δB
c,i(x) = xi
2
⎡
⎢⎣1 +
cx−4
i
2
p
j=1 x−2
j
2
⎤
⎥⎦,
0 < c < 2(p −1), is inadmissible. The estimator δB
c,i was introduced by
Berger (1980b) to improve upon the usual estimator, x/2, for p ≥2. Notice
that x/2 actually dominates the maximum likelihood estimator, x (see
Exercise 8.32).
∥
It is also possible to derive from Lemma 8.5.1 a necessary admissibility
condition for the estimation of a normal mean vector when the variance is
known up to a multiplicative factor, σ2, as in Example 8.4.6. Consider x ∼
Np(θ, σ2Ip) and s2 ∼σ2χ2
q an independent observation of σ2. The following
result provides a necessary admissibility condition (Robert (1987)).
Proposition 8.5.4
If, for the estimator
δ(x) = (1 −h(||x||2, s2))x,
there exist α, M1, and M2 such that
(i) for t ≥M1 and u ≤M2,
t
uh(t, u) ≤α < p −2
q + 2;
or
(ii) for t ≤M1 and u ≥M2,
t
uh(t, u) ≥α > p −2
q + 2;

8.5
Necessary admissibility conditions
415
δ is inadmissible under quadratic loss.
The proof of this result is based on the existence of an optimal estimator
in the class
ϕc(x, s2) = x −cs2
||x||2 IIA(||x||2, s2)x,
where A = [K1, +∞) × [0, M2] or A = [0, K1] × [M2, +∞). To support
Proposition 8.5.4, recall that, in this setting, the James–Stein estimators
are of the form
δJS
a (x, s2) =

1 −as2
||x||2

x
and that
a∗= p −2
q + 2
gives an optimal estimator in the class δJS
a . Therefore, δJS
a∗gives the min-
imal shrinkage factor for ||x||2/s2 large and the maximal shrinkage for
||x||2/s2 small. The estimator δJS
a∗is nonetheless inadmissible (Example
8.4.4). Fraisse et al. (1998) extend this result to exponential families with
a nuisance parameter in the same setting as in Proposition 8.4.5.
Example 8.5.5 (Example 8.4.6 continued) Among matricial shrink-
age estimators, the only interesting estimators are of the form
ϕ(x, s2) = (Ip −h(xtBx, s2)B)x,
(8.5.2)
since the other estimators are inadmissible. Proposition 8.5.4 implies that,
if, for every (t, u),
t
uh(t, u) ≤α < p −2
q + 2,
these estimators are also inadmissible. In addition, notice that a necessary
minimaxity condition under quadratic loss is
t
uh(t, u) ≤2 tr(B) −2λmax(B)
λmax(B)
1
q + 2,
where tr(B) denotes the trace, and λmax(B) the largest eigenvalue, of B (see
Brown (1975) and Cellier et al. (1989)). Therefore, a necessary condition for
the existence of an estimator satisfying both admissibility and minimaxity
requirements is
tr(B) > λmax(B) p + 2
2
,
which excludes estimators shrinking toward subspaces of small dimensions.
This result also points out that admissibility and minimaxity are not
totally compatible. In fact, an estimator admissible under a quadratic loss
is so under all quadratic losses. On the contrary, Brown (1975) shows that
the unique estimator of the form (8.5.2), which is minimax for all quadratic
losses, is δ0(x) = x. This result also relates to the U-admissibility of the
estimator δ0 established in Brown and Hwang (1989) (see Section 2.6).
∥

416
Admissibility and Complete Classes
8
8.6 Exercises
Section 8.2.1
8.1 (Lehmann (1986))
Consider a random variable x with mean μ and vari-
ance σ2.
a. Show that δ(x) = ax + b is an inadmissible estimator of μ under quadratic
loss if
(a) a > 1;
(b) a < 0; and
(c) a = 1 and b ̸= 0.
b. Generalize to the case where δ(x) = (1 + h(x))x with h(x) > 0.
8.2 (Exercise 8.1 cont.) Deduce that it is suﬃcient to consider λ ≥0 for the
estimators (8.1) used in Theorem 8.2.7.
8.3 Consider x ∼U[−θ,θ] when π(θ) is the uniform distribution U[0,1].
a. Show that
δπ
1 (x) =

1 −|x|
log(1/|x|)
if |x| ≤1,
0
otherwise,
is a Bayes estimator that is inadmissible and dominated under the usual
quadratic loss by
δπ
2 (x) =

δπ
1 (x)
if |x| ≤1,
|x|
otherwise.
b. Show that δπ
2 is also a Bayes estimator for π.
8.4 Consider x ∼B(n, p) and determine whether δ0 ≡0 is an admissible estima-
tor of p under quadratic loss.
8.5 (Johnson (1971)) Consider x ∼B(n, θ).
a. Show that δ0(x) = x is the maximum likelihood estimator of θ and also a
Bayes estimator under quadratic loss for π(θ) = 1/θ(1 −θ).
b. Show that (δ0, 1 −δ0) is admissible under the loss
L(θ, δ) = (θ −δ1)2 + (1 −θ −δ2)2.
(8.6.1)
(Hint: Use the Bayes representation of δ0 to show that

[R(θ, δ) −R(θ, (δ0, 1 −δ0))]
dθ
θ(1 −θ) ≥0
and to deduce that equality only occurs for δ1 = δ0, δ2 = 1 −δ0.)
c. Show that a complete class for the loss (8.6.1) is made of the estimators
such that δ1 = 1 −δ2.
d. Generalize the result of b. to the multinomial setting x ∼Mk(n, p1, . . . ,
pk). (Hint: Use induction.)
Section 8.2.2
8.6 Determine the beta priors Be(α, β) which correspond to the admissible esti-
mators of Example 8.2.9.

8.6
Exercises
417
Section 8.2.3
8.7 In the setting of Example 8.2.12, show that the Bayes risk of δπ is inﬁnite
and determine whether δc∗is a Bayes estimator.
8.8
∗(Zidek (1970))
For x ∼f(x|θ), θ ∈IR, such that {θ; f(x|θ) > 0} is an
interval, consider the estimation of g(θ) under quadratic loss. We want to
study a suﬃcient condition for the generalized Bayes estimator
δπ(x) =

g(θ)f(x|θ)π(θ)dθ

f(x|θ)π(θ) dθ
to be admissible when π is a measure and

R(θ, δπ)π(θ) dθ = +∞.
a. Let us deﬁne
M(x, θ) =
 +∞
θ
[g(t) −δπ(x)]2f(x|t)π(t)dt
and
h(θ) =
 
M(x, θ)
f(x|θ)π(θ)
2
f(x|θ) dx.
Show that there exists a function q(θ) such that ˜π(θ) = q(θ)π(θ) is a
probability density and that

R(θ, δπ)˜π(θ) dθ < +∞.
b. Let ˜δ be the Bayes estimator associated with ˜π. Show that
r =

[R(θ, δπ) −R(θ, δ)]˜π(θ) dθ =

[
q′(θ)M(x, θ) dθ]2

f(x|θ)π(θ) dθ
dx.
c. Denoting q(θ) by f 2(θ), derive from the Cauchy–Schwarz inequality that
r ≤4

[f ′(θ)]2h(θ)π(θ) dθ.
d. Show that if, for every (θ0, θ1) and ϵ > 0, there exist a function q such
that q(t) = 1 on (θ0, θ1) and a real number r < ϵ, then the estimator δπ is
admissible.
e. Consider the condition (E): If
 +∞
t
R(θ, δπ)π(θ) dθ = +∞,
then
 +∞
t
1
h(θ)π(θ) dθ = +∞.
Let
y(θ) =
 θ
θ1
1
h(t)π(t) dt
and
f(t) =

1 −y(t)
F

II0≤y(t)≤F .
Show that
f ′(t) = −
1
Fh(t)π(t)
(0 ≤y(t) ≤F),

418
Admissibility and Complete Classes
8
and that
 +∞
θ1
[f ′(t)]2h(t)π(t) dt = 1
F .
Deduce from (E) that it is possible to choose F such that r < ϵ. Conclude
by deriving a suﬃcient admissibility condition.
f. Repeat question e. under the symmetric assumption, that is, if
 t
−∞
R(θ, δπ)π(θ) dθ = +∞,
then
 t
−∞
1
h(θ)π(θ) dθ = +∞.
8.9 Consider the bounded loss
L(θ, δ) = 1 −e−a(θ−δ)2
(a > 0),
for the estimation of θ when x ∼N(θ, 1).
a. Determine the Bayes estimators associated with the conjugate priors θ ∼
N(μ, τ 2).
b. Determine the Bayes estimators associated with the prior distributions
π(θ) ∝exp(−λ|θ −μ|).
c. Examine the admissibility of the generalized Bayes estimator associated
with the Jeﬀreys prior π(θ) = 1 when a varies. (Hint: Determine whether
the Bayes risk is ﬁnite and apply Blyth’s method if necessary.)
Section 8.2.4
8.10 Establish the representation formula (8.2.3) and check the equalities in
Example 8.3.5.
8.11 Show that the estimators δCZ proposed in Section 8.2 in a Poisson frame-
work are indeed generalized Bayes estimators by exhibiting the corresponding
prior distributions.
8.12
∗(Berger (1982b)) Consider x distributed according to
x ∼f(x|θ) = h(x)eθx−ψ(θ)
for x ∈[a, b]. Given two positive diﬀerentiable functions, m0 and d, deﬁne
δ0(x) = m′
0(x)
m0(x) −h′(x)
h(x) ,
γ(x) = 2d′(x)
d(x) ,
and
δ(x) = δ0(x) + γ(x).
a. Show that, under quadratic loss,
R(θ, δ) −R(θ, δ0) = IEθ

4
d(x)

d′′(x) + d′(x)m′
0(x)
m0(x)

,
under some regularity conditions including
lim
x→a h(x)γ(x)eθx = lim
x→b h(x)γ(x)eθx = 0.
b. Assume that one of the two functions
g1(x) =
 x
a
1
m0(y)dy
or
g2(x) =
 b
x
1
m0(y)dy
is ﬁnite on [a, b]. Denote this function by gi. Show that if, in addition,
IEθ
 d
dx log gi

2
< +∞

8.6
Exercises
419
and
lim
x→a h(x)eθx g′
i(x)
gi(x) = lim
x→b h(x)eθx g′
i(x)
gi(x) = 0,
then δ0 is inadmissible and dominated by δ for γ(x) = 2αg′
i(x)/gi(x) if
0 ≤α ≤1.
c. Apply to the case where x ∼G(ν, θ) and
π(θ) = 1
π
1
1 + θ2 .
Section 8.2.5
8.13 Show that the transition kernel (8.2.7) is associated with the stationary
measure π. (Hint: Establish that the detailed balance condition holds.) Deduce
that the corresponding chain is either null-recurrent or transient when the
prior π is improper.
8.14 Consider x ∼N(θ, 1) and π(θ) ∝exp{−bθ2/2 + abθ}.
a. Give necessary and suﬃcient conditions on (a, b) for the posterior distri-
bution to be deﬁned. Show that, in this case, the posterior distribution is
normal with mean (x + ab)/(1 + b) and variance 1/(b + 1).
b. Show that the transition kernel (8.2.7) is then given by
η|θ ∼N

θ + ab
1 + b ,
b + 2
(1 + b)2

.
c. Deduce that the Markov chain is an AR(1) model (Section 4.5.2)
θ(t+1) =
1
1 + bθ(t) +
ab
1 + b +
√
b + 2
1 + b ϵt .
Conclude that it is transient when b < 0 and recurrent when b = 0.
Section 8.3.1
8.15 Verify that the three conditions of Lemma 8.3.3 are actually satisﬁed in the
case of a quadratic loss,
L(θ, δ) = (δ −θ)tQ(δ −θ),
for every positive-deﬁnite matrix Q.
Section 8.3.2
8.16
∗(Clevenson and Zidek (1975)) Consider (x1, . . . , xn) distributed as inde-
pendent Poisson random variables, xi ∼P(λi).
a. Use a sequence of conjugate priors and Blyth’s method to show that δ0(xi) =
xi is an admissible estimator of λi under quadratic loss.
b. For n ≥2, show that
IEλ

n

i=1
1
λi

xi

1 +
n −1
n
i=1 xi
−1
−λi
2
≤IEλ

n

i=1
1
λi (xi −λi)2

and deduce that δ0(x1, . . . , xn) = (x1, . . . , xn) is an inadmissible estimator
of λ = (λ1, . . . , λn). (Hint: Minimize (in λ) IEλ[
i λ−1
i
(axi −λi)2] and
replace the solution a by 
i xi/ 
i xi + n −1.)

420
Admissibility and Complete Classes
8
8.17 Establish the equivalent of Example 8.3.7 for the Poisson distribution, that
is, show that, if H0 : λ ≤λ0 and ϕ(x) = Pλ0(X ≥x), with X ∼P(λ0), then
ϕ is admissible under quadratic loss. (Hint: Use Blyth’s condition.)
8.18 Solve Exercise 8.17 for the gamma distribution, G(ν, θ) and H0 : θ ≤θ0.
8.19 Consider x ∼N2(θ, I2). Check whether Blyth’s condition for the admissi-
bility of δ0(x) = x is satisﬁed by the sequence πn(θ) equal to
e−||θ||2/2n.
If this sequence cannot be used, propose another.
8.20
∗(Hwang and Brown (1991)) Consider x ∼Np(θ, Ip). The usual conﬁdence
region is
Cx = {θ; ∥θ −x∥< c},
with Pθ(θ ∈Cx) = 1 −α. Using Blyth’s method, show that the evaluation
γ0(x) = 1 −α is admissible under the quadratic loss
L(θ, γ) = (γ −IICx(θ))2 ,
for p ≤4. [Note: Robert and Casella (1993) complete this result by showing
that this constant estimator is inadmissible for p ≥5. On the contrary, Hwang
and Brown (1991) establish that, under frequentist validity, γ0 is admissible
for every p (see Section 5.5).]
8.21 In the setting of Example 8.3.7, show that, for x ̸= 0,
ϕ(x) =
1
B(x, m −x + 1)
 θ0
0
tx−1(1 −t)m−x dt
and derive the corresponding representation for x = 0.
Section 8.4
8.22 A class C is said to be minimal complete if C is complete and every proper
subset of C is not complete.
a. Show that every complete class contains every admissible estimator.
b. Show that, if there exists a minimal complete class, it is exactly made of
the admissible estimators.
8.23
∗(Karlin and Rubin (1956)) Consider f(x|θ) satisfying the monotone like-
lihood ratio property (in x ∈IR), with θ ∈Θ. The estimation problem is said
to be monotone if L(θ, δ) is minimum for δ = q(θ), with q increasing in θ, and
if L(θ, δ) is an increasing function of |δ −q(θ)|.
a. Show that, if L is convex, the estimators which are increasing functions of
x constitute a complete class.
b. Show that, if δ0 is not monotone, the monotone estimator δM, deﬁned by
Pq−1(a)(δM(X) ≤a) = Pq−1(a)(δ0(X) ≤a),
∀a
dominates δ0.
c. If δM is strictly increasing, show that the above relation implies that δM(x)
is a number a such that
F(x|q−1(a)) = Pq−1(a)(δ0(X) ≤a).

8.6
Exercises
421
8.24 Apply Exercise 8.23 to the case where x ∼N(θ, 1), L(θ, δ) = (θ −δ)2 and
δ0(x) = −cx + b, with c > 0.
8.25 (Berger (1985a))
Consider Θ, a ﬁnite set of cardinality p, and assume
that the risk set R is bounded and closed from below. Let Γ(R) be the lower
boundary of R, that is,
Γ(R) = {r ∈R; ̸ ∃r′ ∈R, r′ ̸= r and r′
i ≤ri, 1 ≤i ≤p} ⊂R.
The loss L is assumed to be convex.
a. Show that the set of the estimators with risk vector in Γ(R) is a minimal
complete class.
b. Show that the set of the Bayes estimators is a complete class and that the
set of the admissible Bayes estimators is a minimal complete class.
c. Generalize to the case where L is not convex.
8.26
∗(Berger and Srinivasan (1978))
Consider x ∼Np(θ, Σ) with Σ known.
The mean θ is estimated under quadratic loss. Show that an estimator δ0 is a
generalized Bayes estimator if, and only if,
(i) g(x) = Σ−1δ0(x) is continuously diﬀerentiable, with symmetric Jacobian
matrix Jg(x) = ∇∇tg(x); and
(ii) for g(x) = ∇r(x), exp{r(x)} can be expressed as a Laplace transform.
8.27
∗(Fraisse et al. (1990)) Consider x = (u, z) with u ∈IRk and z ∈IR, with
density
f(x|θ, δ) = exp{θ · u + δz −K(θ, δ)}
with respect to a σ-ﬁnite ν measure, with θ ∈Θ ⊂IRk and δ ∈Δ, compact
subset of IR∗
+.
a. Show (or accept) the following lemma: If (μn) is a sequence of measures
with ﬁnite support such that, for almost every (u, z),
sup
n
∥∇ψμn(z)∥< +∞,
then there exist a measure μ and a subsequence (nk) such that
lim
k→+∞ψμnk (u, z) = ψμ(u, z)
and
lim
k→+∞∇ψμnk (u, z) = ∇ψμ(u, z),
with
ψμ(u, z) =

Θ×Δ
eθ·u+δzμ(dθ, dδ).
b. Deduce from Proposition 8.3.10 that, for every admissible estimator ϕ of
θ/δ under the squared error loss δ2||ϕ −θ/δ||2, there exists a sequence of
measures (ϱn) with ﬁnite supports on Θ × Δ such that
ϕ(u, z) =
lim
n→+∞

θeθ·u+δzμn(dθ, dδ)

δeθ·u+δzμn(dθ, dδ),
with μn(dθ, dδ) = e−K(θ,δ)ϱn(θ, δ).
c. Show that the condition of the above lemma is satisﬁed and that, for every
admissible estimator ϕ, there exists μ0 such that, a.e., it satisﬁes
ϕ(u, z) =

θeθ·u+δzμ0(dθ, dδ)

δeθ·u+δzμ0(dθ, dδ),
that is, ϕ is a generalized Bayes estimator associated with μ0.

422
Admissibility and Complete Classes
8
8.28 (Fraisse et al. (1990)) Consider x ∼Np(θ, σ2Ip) and s2 ∼σ2χ2
q. The mean
θ is estimated under a quadratic loss, with σ ∈[a, b].
a. Show that this model ﬁts into the framework of Exercise 8.26.
b. Consider the estimator
ϕ(x, s2) = (Ip −h(xtBx, s2)C)x.
Show that, if ϕ is admissible, there exists ϱ ∈IR∗
+ such that B = ϱC.
c. Compare with the results of Exercise 8.26.
8.29
∗(Moors (1981)) Consider x ∼Be(p), with 0.2 ≤p ≤0.8, and the param-
eter p is to be estimated under quadratic loss.
a. Show that δπ(1) = 1 −δπ(0) when the prior π(p) is symmetric around 1/2.
b. Show that δπ(1) ≤maxp[1 −2p(1 −p)] = 0.68.
c. Deduce that, if an estimator satisﬁes δ(1) = 1 −δ(0) and δ(1) > 0.68, it is
inadmissible.
8.30
∗(Johnson (1971))
Consider x ∼B(n, θ), with θ to be estimated under
quadratic loss.
a. Recall why all admissible estimators are necessarily Bayes estimators.
b. Show that the reverse is false, that is, exhibit some inadmissible Bayes
estimators.
c. Show that the set of admissible Bayes estimators is constituted of the esti-
mators
δτ(x) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0
if 0 ≤x ≤n,
 1
0
θx−n(1 −θ)n−x−n−1 dτ(θ)
 1
0
θx−n−1(1 −θ)n−x−n−1 dτ(θ)
if n < x < n,
1
if n ≤x ≤n.
d. Explain why δτ is a Bayes estimator for a whole class of prior distribu-
tions τ.
8.31 (Hwang, et al. (1992))
Consider H0 : θ ∈Θ0. Testing procedures γ are
compared under a strictly convex loss, L(IIΘ0(θ), γ).
a. Show that γ0(x) = 1/2 is the only minimax estimator.
b. Deduce that γ0 is admissible.
c. Is it possible to write γ0 as a generalized Bayes estimator? Does this phe-
nomenon contradict the complete class result of Section 5.4 (Theorems 5.4.3
and 5.4.4)?
8.32 Given x ∼Exp(θ) and δc(x) = cx, determine the best estimator δc of θ−1
under quadratic loss. Show that this estimator is a generalized Bayes estimator
and discuss its admissibility.

8.6
Exercises
423
Section 8.5
8.33 (Robert and Casella (1994)) Consider x ∼N(θ, 1) and the usual conﬁdence
set Cx = [x −c, x + c]. Instead of using the ﬁxed conﬁdence report α = Pθ(θ ∈
Cx), a procedure ϕ is proposed and evaluated under the loss
L(θ, ϕ) = d(θ, Cx)(IICx(θ) −ϕ)2,
the weight d(θ, Cx) being a measure of the distance between θ and the border
of Cx.
a. Justify the use of a data-dependent assessment of a conﬁdence interval and
of the distance weight.
b. In the particular case
d(θ, Cx) = 1 −e−ω(θ−x)2 
1 −e−ω(θ−x)2
,
show that this distance is minimal for |θ −x| = c if ω = log(2)/c2.
c. Give the general form of the Bayes estimators under this loss and show
that the complete class result obtained in Theorem 5.4.3 still applies.
d. For the class of symmetric distances of the form h(|θ −x|) and π(θ) = 1,
show that the Bayes estimators are constant and not necessarily equal to
α. Are these estimators admissible?
8.34 Derive Proposition 8.5.4 from Lemma 8.5.1.
8.35 Show that, if x ∼Np(θ, Ip), δ0(x) = x is admissible when considered under
the class of losses
L(θ, δ) = (θ −δ)tQ(θ −δ),
where Q varies in the set of symmetric positive-deﬁnite matrices.
8.36 (Hwang (1982b))
Consider x ∼Np(θ, Ip). A class of estimators of θ is
given by
φa(x) = x −
a
||x||2 II[K,+∞[(||x||2)x,
for 0 ≤a ≤(p −2).
a. Show that ϕa∗associated with a∗= p −2 is optimal among the estimators
ϕa under the usual quadratic loss.
b. Derive the result of Example 8.5.2.
c. Reproduce this technique with
ϕb(x) = x −
b
||x||2 II[0,K](||x||2)x
and b ≥(p −2). Derive a STUB condition.
Note 8.7.1
8.37
∗Show that K in (8.2.7) and K∗in (8.7.1) are either both recurrent, or
both transient. (Hint: Use the indicator variable ∞
t=1 IIB(xt) for an arbitrary
set B.)
8.38
∗In the setting of Example 8.7.1,
a. Show that (8.7.2) holds.

424
Admissibility and Complete Classes
8
b. Show that the Markov chain associated with K∗can be written as xt+1 =
(xt + b)zt+1, where the zt’s are independent with density
f(z) =
Γ(2α + a)
Γ(α + a)Γ(α)
zα−1
(z + 1)2α+a .
c. Show that zt has an inﬁnite mean when a + α ≤1 and that IE[log(zt)] is
negative when a < 0, zero when a = 0, and positive when a < 0.
d. In the case b = 0, show that (xt) is recurrent if, and only if, a = 0.
8.39
∗(Hobert and Robert (1999)) Consider x ∼P(θ) with π(θ) ∝θa−1 exp{−bθ}.
a. Give necessary and suﬃcient conditions on (a, b) for the posterior distribu-
tion to be deﬁned and the prior distribution to be improper.
b. Give the transition kernel (8.2.7) in this case. In the special case b = −1/2
and a = k/2, show that the transition is a noncentral chi-squared distribu-
tion.
c. Show that the transition kernel (8.7.1) is
K∗(x, y) = Γ(y + x + a)
y!Γ(x + a) px+a(1 −p)y ,
where p = (b + 1)/(b + 2).
d. Show that this distribution corresponds to the usual negative binomial
distribution when a is a positive integer.
e. In the general case, the distribution with mass function
P(Z = z) = Γ(z + c)
z!Γ(c) pc(1 −p)z
is called a generalized negative binomial distribution Neg(c, p). Deduce from
the probability generating function that, if z1, . . . , zn are independent with
zi ∼NB(ci, p), z1 + . . . + zn ∼Neg(c1 + . . . + cn, p).
f. Deduce that (xt) associated with the kernel K∗is a branching process,
xt+1 =
xt

i=1
ηi,t + ωt+1
where ηi,t ∼Neg(1, p) and ωt+1 ∼Neg(a, p).
g. Conclude that the chain (xt) is recurrent when b = 0 and 0 < a < 1, and
transient otherwise.
8.40
∗(Hobert and Robert (1999)) Consider x ∼Neg(k, θ) with π(θ) ∝θa−1(1−
θ)b−1.
a. Give necessary and suﬃcient conditions on (a, b) for the posterior distribu-
tion to be deﬁned and the prior distribution to be improper.
b. Give the transition kernel (8.2.7) when b = k.
c. Show that the corresponding Markov chain (θ(t)) can be written as θ(t+1) =
θ(t)/(ωt + θ(t)), where the ωt’s are i.i.d.
d. Show that IE[log ωt] = 0 if, and only if, a = 0 and deduce that the chain
(θ(t)) is recurrent when a = 0 and transient otherwise;
8.41 For the transition kernel (8.7.3),

8.7
Notes
425
a. Show that the stationary measure is π(θ)Ψ(θ), where π is the prior distri-
bution and Ψ(θ) is the normalizing constant in T(θ, η). (Hint: Establish that
the detailed balance condition holds and use the equality π(θ)f(x|θ)π(η|x) =
π(η)f(x|η)π(θ|x).)
b. Deduce that the chain is positive recurrent when the Bayes risk
 
(ϕ(θ) −IE[ϕ(θ)|x])2 f(x|θ)π(θ)dxdθ
is ﬁnite.
8.7 Notes
8.7.1 Extensions on Eaton’s suﬃcient admissibility condition
Hobert and Robert (1999) show that Eaton’s suﬃcient condition also applies
to a dual chain, with transition kernel
K∗(x, y) =

Θ
f(x|θ)π(θ|x) dθ,
(8.7.1)
since both kernels K in (8.2.7) and K∗are of the same nature, that is, they are
both recurrent, or both transient. This duality result is of interest when the
sampling space is simpler than the parameter space, for instance when K∗is on
a ﬁnite state space. (This property was used in a completely diﬀerent setting
by Diebolt and Robert (1994) to establish ﬁner convergence properties for
the Gibbs sampler in latent variable models. See Robert and Casella (1999,
Section 7.2.4).) Hobert and Robert (1999) demonstrate the interest of this
condition in some standard settings (Exercises 8.39 and 8.40).
Example 8.7.1 Consider a gamma model, x ∼Ga(α, θ), with π(θ) ∝θα−1
exp{−bθ}. The posterior is deﬁned when b ≥0 and a > −α. While the tran-
sition kernel K cannot be written in closed form, K∗can be expressed as
K∗(x, y) = Γ(2α + a)Γ(b + x)α+a
Γ(α + a)Γ(α)
yα−1
(x + y + b)2α+a .
(8.7.2)
Hobert and Robert (1999) then show that this kernel is recurrent if, and only
if, a = 0 (Exercise 8.38).
∥
Eaton (1999) generalizes Eaton’s (1992) result to arbitrary functions of θ,
ϕ(θ), estimated under squared-error loss, through another Markov represen-
tation. Instead of K(θ|η) deﬁned by (8.2.7), Eaton (1999) proposes to use the
transition kernel
T(θ|η) = Ψ(η)−1(ϕ(θ) −ϕ(η))2K(θ|η) ,
(8.7.3)
where Ψ(θ) is the normalizing factor of this density. The equivalent of Theorem
8.2.19 in this case is then that the Bayes estimator IEπ[ϕ(θ)|x] is admissible
when the Markov chain associated with T is recurrent. Although this result is
case-dependent, that is, requires a study of the Markov chain for each function
ϕ of interest, the extension to unbounded functions ϕ makes it quite valuable.
(As noted in Eaton (1999), the result extends to the estimation of vector-
valued functions ϕ(θ) under squared-error loss.)

426
Admissibility and Complete Classes
8
Example 8.7.2 In the special case of a location family, that is, when f(x|θ) =
g(x −θ), if θ is estimated under the loss function L(θ, d) = (θ −d)2, then the
Markov kernel K associated with the ﬂat prior π(θ) = c is
K(θ|η) =

g(x −θ)g(x −η)dx = r(θ −η) ,
by an appropriate change of integrand in the integral. Then,
T(θ, η) ∝(θ −η)2r(θ −η) = t(θ −η) ,
and the proportionality factor is independent of θ. Therefore, the Markov
chain associated with T is a random walk, which is recurrent in dimension one
if the ﬁrst moment of t exists. As shown in Eaton (1999), this is equivalent to
the existence of the third moment of g.
∥

CHAPTER 9
Invariance, Haar Measures,
and Equivariant Estimators
The ring certainly looked like stone, but it felt harder than steel and heavier
than lead. And the circle of it was twisted. If she ran a ﬁnger along one edge,
it would go around twice, inside as well as out; it only had one edge. She ran
her ﬁnger along that edge twice, just to convince herself.
Robert Jordan, The Dragon Reborn, Book III of the Wheel of Time.
9.1 Invariance principles
Invariance can be seen as a notion introduced in frequentist settings to re-
strict the range of acceptable estimators suﬃciently so that an optimal esti-
mator can be derived. From this point of view, it appears as an alternative
to unbiasedness, and is thus similarly at odds with the Bayesian paradigm.
However, invariance can also be justiﬁed on a non-decision-theoretic heuris-
tic, namely, that estimators should meet some consistency requirements
under a group of transformations, and it is thus of interest to consider
this notion. Moreover, optimal (equivariant) estimators are always Bayes
or generalized Bayes estimators. The corresponding measures can then be
considered as noninformative priors induced by the invariance structure.
Therefore, a Bayesian study of invariance is appealing, not because classi-
cal optimality once more relies on Bayesian estimators, but mainly because
of the connection between invariance structures and the derivation of non-
informative distributions.
A ﬁrst version of the invariance principle is to consider that the prop-
erties of a statistical procedure should not depend on the unit of measure-
ment. If x and θ are measured in unit u1, and if y and η are the transforms
of x and θ for the new unit u2, then an estimator δ2(y) of η should then
correspond to the estimator δ1(x) of θ by the same change of unit. Of
course, the notion of unit of measurement is to be understood in a general
sense: for instance, it can indicate the choice of a particular scale (cm vs.
m)—and estimators should be scale-invariant—the choice of a particular

428
Invariance
9
origin—in which case estimators should be translation-equivariant—or even
the ordering of the observations of the sample x—in which case estimators
should be symmetric.
Example 9.1.1 Consider the problem of estimating the speed of light, θ,
given an observation x, with distribution U[θ−ϵ,θ+ϵ], measured in meters per
second. A typical change of unit in this setting is the scale modiﬁcation,
y = τx, where, for instance, τ = 10−3 for a conversion from meters to
kilometers. In this case, y ∼U[η−ϵ′,η+ϵ′] with η = τθ, ϵ′ = τϵ, but the
quantity η still represents the same intrinsic quantity, that is, the speed of
light. If δ0 is an estimator of θ in the initial unit, it seems legitimate to
require that the estimator in the transformed problem, δ∗, satisfy the scale
equivariance property
δ∗(y) = τδ0(y/τ).
Moreover, assume that the loss is the scaled quadratic loss
L(θ, d) =

1 −d
θ
2
.
It satisﬁes
L(θ, d) =

1 −τd
τθ
2
=

1 −d∗
η
2
= L(η, d∗).
Therefore, the loss is invariant under this change of unit and the two es-
timation problems are formally identical. It is then natural to select the
same estimator for both problems, namely, δ∗(y) = δ0(y). Considering both
equations simultaneously, it follows that the estimator should satisfy
δ0(τy) = τδ0(y)
for all τ and y. Therefore, the decision rules complying with the invariance
requirements are necessarily of the form δ0(x) = ax, where a is a positive
constant.
∥
This principle is often extended to a formal invariance principle, ac-
cording to which two problems with identical formal structures, (X, f(x|θ),
L), should lead to the same decision rule, even though they are not related
physically, as in the restricted invariance principle. In the previous example,
the speed of light is always the same object. This extension does not seem
so natural in a Bayesian perspective because the prior information is not
necessarily the same in both problems. It is thus only in a noninformative
setting that both approaches can be compatible.
A Bayesian approach to invariance is thus justiﬁed for the following
reasons:
(i) The best invariant (or equivariant) estimator is a generalized Bayes
estimator with respect to a particular measure called a Haar measure.

9.2
The particular case of location parameters
429
(ii) This measure can be defended in a noninformative setting because
invariance provides an alternative method for constructing noninfor-
mative prior distributions.
(iii) The most eﬃcient method for deriving the best equivariant estimators
is to use a Bayesian approach.
Therefore, invariance considerations reinforce the Bayesian paradigm, since
it underlies once more a frequentist optimality criterion. In this chapter, we
justify the connection between invariance and the Bayesian approach in the
case of location parameters in Section 9.2, then present the general invariant
framework in Section 9.3, the Haar measure as a potential noninformative
prior in Section 9.4, and the Hunt–Stein theorem, which links invariance
with minimaxity, in Section 9.5. For deeper coverages of invariance, from
both general and Bayesian perspectives, see Berger (1985a, Chapter 8),
Eaton (1989), and Wijsman (1990).
9.2 The particular case of location parameters
Consider (x1, . . . , xn) with density f(x1 −θ, . . . , xn −θ), with an unknown
location parameter θ ∈IR. The natural invariance structure of this prob-
lem is invariance by translation. In fact, if (x1, . . . , xn) is transformed into
(y1, . . . , yn) = (x1 + a, . . . , xn + a), this new random variable (y1, . . . , yn)
is distributed according to f(y1 −θ −a, . . . , yn −θ −a) and η = θ +a is the
corresponding location parameter. Therefore, the transformed vector has
the same type of density, and the problem is invariant under translation
transformations. It seems logical to reproduce this structure by imposing
the following invariance restriction on the estimators δ of θ:
δ(x1 + a, . . . , xn + a) = δ(x1, . . . , xn) + a.
(9.2.1)
This condition is satisﬁed, for instance, by δ0(x1, . . . , xn) = ¯x. Moreover,
it also seems natural to impose a similar invariance restriction on the loss
function, namely, that L(θ+a, d+a) = L(θ, d) for every a. A loss compatible
with the invariance structure should then be of the form
L(θ, d) = L(0, d −θ) = ϱ(d −θ).
(9.2.2)
The estimators satisfying (9.2.1) are called equivariant and the loss func-
tions satisfying (9.2.2) invariant, both under the action of the translation
group. The main purpose of these restrictions is to reduce suﬃciently the
class of “acceptable” estimators so that there might be a single best equiv-
ariant estimator under the loss (9.2.2), since it is not possible without this
restriction (see Exercise 2.36). The following lemma shows why there can
be a single optimal estimator.
Lemma 9.2.1
Under losses of the form (9.2.2), equivariant estimators
have a constant risk.

430
Invariance
9
Proof.
In fact,
R(δ, θ)
=
IEθ[ϱ(δ(x) −θ)]
=
IEθ[ϱ(δ(x1 −θ, . . . , xn −θ))]
=
IE0[ϱ(δ(x1, . . . , xn))] = R(δ, 0).
22
Therefore, the particular setting of equivariant estimators under an equiv-
ariant loss is similar to the general case of all estimators under the Bayes
risk: there exists a total ordering on this restricted class, since the compar-
ison of estimators is equivalent to a comparison of real numbers. This is
why a best equivariant estimator can exist.
The classical determination of this best estimator proceeds by condition-
ing on a maximal ancillary statistic, such as y = (x1 −xn, . . . , xn−1 −xn).
It is then straightforward to check that every equivariant estimator can
be written as δ0(x) + v(y), where δ0 is a particular equivariant estimator,
δ0(x) = xn say.
Lemma 9.2.2
If there exists a function v∗(y) that minimizes
IE0[ϱ(δ0(x) + v(y))|y],
the best equivariant estimator under the loss (9.2.2) is given by
δ∗(x) = δ0(x) + v∗(y).
Proof.
By deﬁnition, the best equivariant estimator minimizes (in v) the
constant risk
R(δ, θ) = IE0[ϱ(δ(x))] = IE0[ϱ(δ0(x) + v(y))].
Since y is an ancillary statistic, it is possible to condition on y, decomposing
the risk as
IE0[ϱ(δ0(x) + v(y))] = IE [IE0[ϱ(δ0(x) + v(y))|y]] .
If v∗(y) minimizes the integrand for each y, δ∗minimizes the risk over the
class of the equivariant estimators.
22
In the particular case where ϱ(δ −θ) = (δ −θ)2, the optimal factor v∗is
given by
v∗(y) = −IE0[δ0(x)|y].
We have then derived the best equivariant estimator of Pitman (1939).
Corollary 9.2.3
Under the quadratic loss L(θ, d) = (θ −d)2, the best
equivariant estimator of θ is
δ∗(x1, . . . , xn) =
 +∞
−∞θf(x1 −θ, . . . , xn −θ) dθ
 +∞
−∞f(x1 −θ, . . . , xn −θ) dθ
.
Proof.
Consider δ0(x) = xn as the particular equivariant estimator used
in Lemma 9.2.2 and let us denote yn = xn to complete y. The density of

9.3
Invariant decision problems
431
(y1, . . . , yn) is then (for θ = 0)
gY (y1, . . . , yn) = f(y1 + yn, . . . , yn−1 + yn, yn),
since yi = xi −xn (i ̸= n) and the Jacobian determinant is equal to 1.
Moreover,
IE0[yn|y1, . . . , yn−1]
=
 +∞
−∞tf(y1 + t, . . . , yn−1 + t, t) dt
 +∞
−∞f(y1 + t, . . . , yn−1 + t, t) dt
=
 +∞
−∞tf(x1 −xn + t, . . . , xn−1 −xn + t, t) dt
 +∞
−∞f(x1 −xn + t, . . . , xn−1 −xn + t, t) dt
=
xn −
 +∞
−∞θf(x1 −θ, . . . , xn−1 −θ, xn −θ) dθ
 +∞
−∞f(x1 −θ, . . . , xn −θ) dθ
,
by the change of variable θ = xn −t. Since
δ∗(x1, . . . , xn) = xn −IE0[yn|y1, . . . , yn−1],
the above expression of δ∗follows.
22
The main appeal of Corollary 9.2.3, besides providing the best equivari-
ant estimator, is to exhibit this estimator as a Bayes estimator, although
the whole derivation does not involve any Bayesian input. The Pitman
estimator is indeed a Bayes estimator associated with the prior distribu-
tion π(θ) = 1, that is, the usual noninformative distribution for location
parameters (see Chapter 3). This result actually holds for the other invari-
ant losses (9.2.2), as shown in Section 9.4. Therefore, the best equivariant
estimator can be derived as the estimator δ that minimizes the posterior
loss
IEπ[L(θ, δ)|x] =
 +∞
−∞ϱ(θ −δ)f(x1 −θ, . . . , xn −θ) dθ
 +∞
−∞f(x1 −θ, . . . , xn −θ) dθ
,
or, equivalently,
 +∞
−∞
ϱ(θ −δ)f(x1 −θ, . . . , xn −θ) dθ,
and this representation drastically simpliﬁes the derivation of the best
equivariant estimators. Section 9.4 shows that the connection between best
equivariant estimators and a particular class of Bayes estimators holds in
much greater generality than for the estimation of location parameters.
9.3 Invariant decision problems
A general description of the relations between invariant problems and
Bayesian analysis involves an abstract description of invariance through the
action of invariance groups. Consider a statistical model, (X, Θ, f(x|θ)),
and an inferential problem on θ represented by a decision space, D. Assume

432
Invariance
9
in addition that a group G of transformations on X is provided with the
problem. (It can also be perceived as a particular case of prior information.)
The existence of such a group is important in the following case.
Deﬁnition 9.3.1
The statistical model is said to be invariant (or closed)
under the action of the group G if, for every g ∈G, there exists a unique
θ∗∈Θ such that y = g(x) is distributed according to the density f(y|θ∗).
We denote θ∗= ¯g(θ).
Example 9.3.2
Consider x ∼f(x −θ) and the translation group
G = {gc; gc(x) = x + c, c ∈IR} .
The statistical model is then actually invariant under the action of G. This
is not the case with the multiplicative group
G′ = {gc; gc(x) = cx, c > 0} ,
since
y = cx ∼1
cf
y −cθ
c

.
∥
When the group G has a globally invariant action on the model, it nat-
urally induces a set ¯G of transformations on Θ and the fact that ¯G is also
a group is left as an exercise for the reader. To simplify notations, we will
write g(x) as gx and ¯g(θ) as ¯gθ in the sequel.
We assume in addition that the loss function associated with the model,
L from Θ × D in IR+, is discriminant in the sense that two diﬀerent de-
cisions are associated with diﬀerent losses and, moreover, that the loss is
compatible with the invariance structure in the following sense.
Deﬁnition 9.3.3
If the model is invariant under the action of G, the loss
L is said to be invariant under G if, for every g ∈G and d ∈D, there exists
a unique decision d∗∈D such that L(θ, d) = L(¯gθ, d∗) for every θ ∈Θ.
This decision is denoted d∗= ˜g(d) and the decisional problem is said to be
invariant under G.
In this case, the group G induces a second group ˜G, acting on D. Given
these three groups G, ¯G, and ˜G, and the above assumptions on the decision
problem, it seems logical to restrict the class of estimators to the equivariant
estimators, that is, to those satisfying
δ(gx) = ˜gδ(x).
These estimators were also called invariant in the past. A particular case
of interest is when θ is estimated, that is, when D = Θ, since G = ˜G in such
settings.
Example 9.3.4 (Example 9.3.2 continued) Consider the estimation of
θ under the quadratic loss (θ−d)2. The decisional problem is then invariant
and ¯G = ˜G = G.
∥

9.3
Invariant decision problems
433
Example 9.3.5
Consider x ∼N(0, σ2). The variance σ2 is estimated
under the entropy loss,
L(σ, δ) = δ
σ2 −log(δ/σ2) −1,
introduced in Chapter 2. If the group considered in this case is the group
of scale transformations,
G = {gc; gc(x) = cx, c > 0} ,
the associated groups are
¯G = ˜G =

¯gc(σ2) = c2σ2, c > 0

and the loss, thus the decisional problem, is also invariant under the action
of G.
∥
Example 9.3.6 Consider x ∼Tp(ν, θ, Ip) and let ||θ||2 be the parameter of
interest. A natural invariance structure is then invariance under orthogonal
transformations,
G = ¯G =

gA; gA(x) = Ax, AtA = Ip

,
and the problem is invariant if the loss can be written as
L(θ, δ) = ˜L(||θ||2, δ),
since there is always an orthogonal matrix A such that Aθ = ||θ||(1, 0, . . . ,
0)t and ˜G reduces to the identity transformation. In this case, the equiv-
ariant estimators only depend on ||x||2.
∥
Deﬁnition 9.3.7
When ¯G is a group operating on Θ, θ1 and θ2 are said
to be equivalent if there exists ¯g ∈¯G with θ2 = ¯gθ1. An orbit of Θ is an
equivalence class for this relation and the group ¯G is said to be transitive if
Θ has a single orbit.
If the group G is small enough, there may be many orbits. For instance,
when x ∼B(n, p) and G is restricted to g0(x) = x and g1(x) = n −x,
¯G = {¯g0, ¯g1} with ¯g1(p) = 1 −p. Then there is an orbit associated with
every p ∈[0, 0.5]. However, when G is larger, this notion usually allows for
the generalization of the phenomenon observed in the case of the location
parameters.
Theorem 9.3.8
The risk of an equivariant estimator is constant within
each orbit of Θ, that is,
R(δ, θ) = R(δ, ¯gθ)
for every g ∈G.

434
Invariance
9
Proof.
As for the estimators of location parameters, we derive that
R(δ, θ) = IEθ [L(θ, δ(x))]
=
IEθ [L(¯gθ, ˜gδ(x))]
=
IEθ [L(¯gθ, δ(gx))]
=
IE¯gθ [L(¯gθ, δ(x))]
=
R(δ, ¯gθ)
for every g ∈G.
22
An immediate consequence of Theorem 9.3.8 is the following result.
Corollary 9.3.9 If ¯G is transitive, every equivariant estimator has a con-
stant risk.
For transitive groups, it is thus legitimate to look for the best equivariant
estimator by minimizing the constant risk R(δ, θ0) on the class of equivari-
ant estimators. However, the call to ancillary statistics, as in Section 9.2, is
not always straightforward. A classical approach is to consider the maximal
invariant statistic in order to reduce the dimension of the problem, similar
to the call to minimal suﬃcient statistics under convex losses.
Deﬁnition 9.3.10
For a group of transformations G, a statistic T (x) is
said to be invariant if T (gx) = T (x) for every x ∈X and every g ∈G. It is
said to be maximal invariant if it is invariant and T (x1) = T (x2) implies
that x1 and x2 are equivalent.
In other words, a maximal invariant statistic indexes the orbits of ¯G.
In particular, if ¯G is transitive, the only maximal invariant statistics are
constant. Moreover, it is straightforward to see that every invariant statistic
is a function of a maximal invariant statistic. Notice also that, if ¯G is
transitive, T (x) is necessarily ancillary.
Example 9.3.11
Consider a distribution with a scale parameter σ,
x = (x1, . . . , xn) ∼1
σn f
x1
σ , . . . , xn
σ

,
when G is the multiplicative group, made of the transformations
gc(x1, . . . , xn) = (cx1, . . . , cxn)
(c > 0).
Then, if z = ||x||,
T (x) =
 0
if z = 0,
x
z
otherwise,
is a maximal invariant statistic.
∥
Example 9.3.12 (Example 9.3.6 continued)
Similarly, if z = ||x||,
the statistic
T (x) =
 0
if z = 0,
x
z
otherwise,
is also maximal invariant for this problem.
∥

9.3
Invariant decision problems
435
When determining the best equivariant estimator, the maximal invariant
statistic can be used by conditioning. (Notice that the choice of a particular
maximal invariant statistic does not matter, since each generates the same
σ-algebra, being in one-to-one correspondence with each other.) In fact,
if ¯G is transitive and T is a maximal invariant statistic, every equivariant
estimator δ satisﬁes
R(δ, θ)
=
R(δ, θ0)
=
IEθ0[L(θ0, δ(x))]
=
IET
θ0{IEθ0[L(θ0, δ(x))|T (x) = t]}
for an arbitrary value θ0 (because the risk is constant). Since T is maximal
invariant, every x such that T (x) = t can be written gxt, where xt is a
selected member of the orbit of x (assuming the axiom of choice holds).
Therefore, for an equivariant estimator, δ(x) = ˜gδ(xt). It is then suﬃcient
to minimize the above quantity in δ(xt), conditional upon T , to obtain the
best equivariant estimator. Although straightforward, the above condition-
ing is actually instrumental in the determination of the best equivariant
estimators, and will be used in the sequel.
Example 9.3.13 (Example 9.3.11 continued)
Notice that, in this
case, T is also an ancillary statistic. For the entropy loss, the minimiza-
tion problem (in δ) is over
IE1[δ(x) −log δ(x)|T (x) = t]
=
IE1[δ(zt) −log δ(zt)|T (x) = t]
=
IE1[zδ(t) −log δ(t) −log(z)|T (x) = t]
(where z = ||x||). By linearity of the expectation, δ(t) is minimizing
IE1[z|T = t]δ(t) −log δ(t),
and therefore satisﬁes
δ∗(t) =
1
IE1[z|T = t].
The best equivariant estimator of σ is thus
δ∗(x) =
||x||
IE1[z|T = x/||x||] .
In the particular case when xi ∼N(0, σ2), we derive that the best equiv-
ariant estimator of σ is
δ∗(x) =
||x||
IE1(||x||) =
Γ(p/2)
√
2Γ(p + 1/2) ||x||.
∥
Further details on this technique are given in Berger (1985a, §6.5) and
Eaton (1989, §2.3).

436
Invariance
9
9.4 Best equivariant estimators and noninformative
distributions
In this section, we generalize the result obtained in Section 9.2 in the par-
ticular case of location parameters. We show that it is indeed possible to
relate the best equivariant estimator to a σ-ﬁnite measure on Θ called a
right-invariant Haar measure. For a more detailed and rigorous treatment,
see Eaton (1989) and Wijsman (1990).
Let us assume ﬁrst that, for a statistical problem invariant under the
action of G, there exists a probability distribution π∗on Θ that is also
invariant under the action of ¯G, that is, such that
π∗(¯gA) = π∗(A)
for every measurable set in Θ, that is, every A ∈B(Θ), and for every g ∈G.
In this case, the Bayes estimator associated with π∗, δ∗, minimizes

Θ
R(δ∗, θ) dπ∗(θ)
=

Θ
R(δ∗, ¯gθ) dπ∗(θ)
=

Θ
IEθ
+
L(θ, ˜g−1δ∗(gx))
,
dπ∗(θ)
and, if the Bayes estimator is unique, it satisﬁes
δ∗(x) = ˜g−1δ∗(gx)
π-almost everywhere, the set of measure zero where the equality does not
hold depending on g. Therefore, a Bayes estimator associated with an in-
variant prior and a strictly convex invariant loss is almost equivariant.
When G is not countable, the collection of the above sets of measure zero
over all g’s is not necessarily of measure zero, but it is possible to show
that, under additional conditions (see Lehmann (1986, Chapter 6, Theo-
rem 4)), there exists an equivariant estimator which is a Bayes estimator
with respect to π∗(see also Strasser (1985)).
Example 9.4.1 Consider δπ(x) = IEπ[θ|x] under an invariant proper loss.
If π∗is an invariant probability distribution, the Bayes estimator associated
with π∗satisﬁes
δπ(gx)
=

Θ θf(gx|θ) dπ∗(θ)

Θ f(gx|θ) dπ∗(θ)
=

Θ θf(x|¯g−1θ) dπ∗(θ)

Θ f(x|¯g−1θ) dπ∗(θ)
=

Θ ¯gηf(x|η) dπ∗(η)

Θ f(x|η) dπ∗(η) .
Therefore, if

Θ
¯gηf(x|η) dπ∗(η) = ¯g

Θ
ηf(x|η) dπ∗(η),

9.4
Best equivariant noninformative distributions
437
for every g ∈G, δ∗is indeed equivariant.
∥
Actually, invariant probability distributions are rare, since they can only
exist for compact groups1 ¯G (see Lehmann (1983, Chapter 4, Example 4.2)
for an illustration in a noncountable group). In other settings, it is necessary
to consider invariant measures, for which the above results do not always
hold (because formal Bayes estimators are not always deﬁned).
Example 9.4.2 (Example 9.3.2 continued) If π is invariant under the
action of the translation group, it satisﬁes π(θ) = π(θ + c) for every θ and
for every c, which implies that π(θ) = π(0) uniformly on IR and thus leads
to the Lebesgue measure as an invariant measure.
∥
Example 9.4.3
Consider x1, . . . , xn a sample from N(θ, σ2), with un-
known θ and σ2. For suﬃciency reasons, we can consider the couple (¯x, s),
where ¯x is the empirical mean and s2 is the sum of the squared errors. In
this setting, the group to consider is the aﬃne group
G = {ga,b; ga,b(¯x, s) = (a¯x + b, as), a > 0, b ∈IR} ,
and ¯G = ˜G = G if the parameter to be estimated is (θ, σ). If π is an invariant
measure, its density satisﬁes
a2π(aθ + b, aσ) = π(θ, σ),
∀a > 0, ∀b ∈IR ,
which implies
π(θ, σ) = π(0, 1)/σ2.
Therefore, an invariant measure is proportional to π(θ, σ) = 1/σ2 and
corresponds to the Jeﬀreys measure obtained in Chapter 3.
∥
In general, given a locally compact topological group G, and deﬁning
K(G) as the set of continuous real functions on G with compact support,
we introduce for g ∈G the transformation Lg on K(G) as
(Lgf)(x) = f(gx)
for f ∈K(G), x ∈G.
An integral J on K(G) is said to be left-invariant if
J(Lgf) = J(f)
for every f ∈K(G) and for every g ∈G. The Radon measure νℓassociated
with J is said to be a left Haar measure, and it can be shown (see Nachbin
(1965)) that this measure is unique up to a multiplicative factor. Deﬁning
Rg on K(G) by
(Rgf)(x) = f(xg),
for f ∈K(G), x ∈G,
1 When ¯G is not a subset of IRp, the topological structure induced by ¯G is the topology
induced by the group composition and inversion, that is, the smallest collection of
open sets such that the group composition and inversion are continuous (see Rudin
(1976)).

438
Invariance
9
we derive similarly right-invariant integrals and a right Haar measure νr,
also deﬁned up to a multiplicative constant. As mentioned above, the ﬁnite-
ness of the Haar measure, that is, the existence of an invariant probability
distribution, is in fact equivalent to the compactness of G. See Eaton (1989,
Chapter 1) for examples of Haar measures; Berger (1985a) details the case
where G ⊂IRk.
The modulus of G is deﬁned as the multiplier Δ—that is, a real-valued
function satisfying Δ(g1g2) = Δ(g1)Δ(g2)—that relates left and right Haar
measures by
νr(dx) = Δ(x−1)νℓ(dx)
(see Exercises 9.13 and 9.15). We assume the existence of a Radon measure
μ on X such that, for every f,

X
f(g−1x)μ(dx) = Δ−1(g)

X
f(x)μ(dx).
This relation shows the connection between the modulus of G and the
Jacobian of the transformation of x in gx. Consider the distributions Pθ,
θ ∈Θ, with density f(x|θ) with respect to μ. Then, for every g ∈G,
f(x|θ) = f(gx|¯gθ)Δ−1(g).
We also assume that ¯G acts transitively on Θ. As shown in Eaton (1989, p.
84), some additional assumptions then ensure the validity of a theorem `a
la Fubini: If νr is the right Haar measure on G, Q is the projection of X
on X/G, and (T f) is deﬁned on X/G by
(T f)(Q(x)) =

G
f(gx)νr(dg),
then there exists an integral J1 deﬁned on K(X/G) such that
J1(T f) =

X
f(x)μ(dx).
This can be rewritten as the fact that the integral of f with respect to μ
is the integral over all the orbits of X (that is, on X/G) of the average of
f with respect to the right Haar measure on each orbit, T f.
Consider a nonrandomized estimator δ and, for a ﬁxed θ ∈Θ, let us
deﬁne
f0(x) = L(θ, δ(x))f(x|θ),
then
R(δ, θ) =

X
f0(x)μ(dx).
It follows from the above theorem `a la Fubini that there exists an integral
J1 on K(X/G) such that
R(δ, θ) = J1(T f0),

9.4
Best equivariant noninformative distributions
439
with
(T f0)(Q(x))
=

G
L(θ, δ(gx))f(gx|θ)νr(dg)
=

G
L(¯gθ, δ(x))f(x|¯gθ)νr(dg)
(see Eaton (1989, p. 85)). We also deﬁne
H(a, x) =

G
L(¯gθ, a)f(x|¯gθ)νr(dg),
which does not depend on θ (since ¯G acts transitively on Θ). Notice that
H(δ(x), x) gives the risk of δ conditional upon the orbit of x. It is instru-
mental in the derivation of the best equivariant estimator.
Theorem 9.4.4
If there exists a0(x) such that
(i) H(a, x) ≥H(a0(x), x) for every a ∈D, x ∈X; and
(ii) a0(gx) = ˜ga0(x) for every g ∈G, x ∈X,
then δ0(x) = a0(x) is a best equivariant estimator.
Proof.
Consider an equivariant estimator δ. Then

G
L(¯gθ, δ(x))f(x|¯gθ)νr(dg) ≥

G
L(¯gθ, a0(x))f(x|¯gθ)νr(dg).
Integrating with respect to J1, it follows that R(δ, θ) ≥R(δ0, θ). The esti-
mator δ0 then dominates δ.
22
This theorem points out the relation existing between the best equiv-
ariant estimator and a particular Bayes estimator, since H(a, x) can also
be interpreted as a posterior Bayes risk. In fact, if θ0 ∈Θ is arbitrarily
selected, the function τ(g) = ¯gθ0 deﬁnes a surjection from G to Θ because
of the transitivity of ¯G. It therefore induces a measure on Θ, called a right
Haar measure on Θ, which is deﬁned by π∗(B) = νr(τ −1(B)) for every
B ∈B(Θ), and is obviously invariant under the action of ¯G. Moreover,
H(a, x) =

Θ
L(θ, a)f(x|θ) dπ∗(θ).
This extension of the right Haar measure to Θ implies the following result,
which expresses the best equivariant estimator as a Bayes estimator for
every transitive group acting on a statistical model.
Corollary 9.4.5 The best equivariant estimator of θ is the Bayes estima-
tor associated with the right Haar measure on Θ, π∗, and the corresponding
invariant loss.
Therefore, we have obtained a method that derives the best equivariant
estimators directly from the right Haar measure. (See Stein (1965) and
Zidek (1969) for similar results.)

440
Invariance
9
In the above development, the dominating measure is μ, which is rela-
tively invariant with multiplier the modulus Δ−1. In fact, if the measure μ
is relatively invariant with an arbitrary multiplier χ, that is, such that for
every f ∈K(G),

X
f(gx)μ(dx) = χ(g)

X
f(x)μ(dx),
Corollary 9.4.5 still holds (see Eaton (1989, p. 87)).
Example 9.4.6 (Example 9.4.3 continued) We obtained the following
left Haar measure on Θ:
πℓ(θ, σ) = 1/σ2.
The right Haar measure can be derived by inversion: if g = (a, b) and
g0 = (a0, b0), gg0 = (aa0, ab0 + b) for the group composition. Taking the
Jacobian into account, we want the right Haar measure to satisfy
a0πr(b0σ + θ, a0σ) = πr(θ, σ)
for every (θ, σ) and uniformly in a0, b0; this implies
πr(θ, σ) = 1/σ,
up to a multiplicative factor. Therefore, the right Haar measure is diﬀerent
from the left Haar measure and gives the noninformative alternative to the
Jeﬀreys prior (see Section 3.6). Under the invariant quadratic loss,
L((θ, σ), δ) = (θ −δ1)2
σ2
+
δ2
σ −1
2
,
(9.4.1)
the best equivariant estimator is the Bayes estimator associated with the
prior distribution πr, that is,
δ∗
1(¯x, s) = IEπr[θ/σ2|¯x, s]
IEπr[1/σ2|¯x, s],
δ∗
2(¯x, s) = IEπr[1/σ|¯x, s]
IEπr[1/σ2|¯x, s].
Since
πr(θ, σ|¯x, s) ∝σ−(n+1)e−n(¯x−θ)2/2σ2e−s2/2σ2,
this is a special case of conjugate distribution on (θ, σ) and
δ∗
1(¯x, s) = ¯x,
δ∗
2(¯x, s) =
Γ(n/2)
√
2Γ((n + 1)/2)s.
Notice that δ2 is also the estimator obtained in Example 9.3.11.
∥
Example 9.4.7 (Eaton (1989)) Consider a multiplicative model N(θ, θ2),
with n observations x1, . . . , xn. This model appears in settings where the
diﬃculty of measuring an object increases with its magnitude (particle
physics, astronomy, etc.). If we estimate θ under the loss
L(θ, d) = (θ −d)2
θ2
,

9.5
The Hunt–Stein theorem
441
the problem is invariant under the action of the multiplicative group. The
right Haar measure is then π(θ) = 1/|θ|. (It is also the left Haar measure,
since the group is commutative.)
Therefore, the best equivariant estimator of θ is
δ∗(x1, . . . , xn) = IEπ[1/θ|x1, . . . , xn]
IEπ[1/θ2|x1, . . . , xn]
and
π(θ|x)
∝
1
θ2 exp

−
n

i=1
(xi −θ)2/2θ2

∝
1
θ2 exp

−1
2
n¯x
s2 −1
θ
2
s2

,
for s2 = n
i=1 x2
i . The posterior distribution is then a generalized inverse
normal distribution IN(2, n¯x/s2, 1/s2) (Robert (1991)) and
IEπ[1/θ|¯x, s2] =
√
2s
1F1(1; 1/2; n2¯x2/2s2)
Γ(1/2)1F1(1/2; 1/2; n2¯x2/2s2).
Therefore,
δ∗(x1, . . . , xn) =
√
2 Γ(3/2)
1F1(3/2; 1/2; n2¯x2/2s2)
Γ(1/2)1F1(1; 1/2; n2¯x2/2s2) s.
In this case, the best equivariant estimator dominates the maximum like-
lihood estimator
ˆδ(¯x, s) = −¯x + (¯x2 + 4s2)1/2
2
,
which is also equivariant. For additional results on the multiplicative mod-
els, see Gleser and Healy (1976), Kariya (1984), Kariya et al. (1988), and
Perron and Giri (1990).
∥
The reader is referred to Eaton (1989), Lehmann (1986), and Berger
(1985a) for other examples of the use of Haar measures in the derivation
of best equivariant estimators in the case of tests and conﬁdence regions.
A general mathematical treatise on Haar measures is Nachbin (1965).
9.5 The Hunt–Stein theorem
If we consider the particular case discussed at the beginning of the previous
section, namely the case when G is compact and where there exists an
invariant probability distribution on Θ, the best equivariant estimator is a
(proper) Bayes estimator, and therefore admissible in most cases. Because
its risk is constant when ¯G is transitive, the best equivariant estimator is
also minimax. When G is not compact, the best equivariant estimator is a
generalized Bayes estimator associated with the right Haar measure, and
therefore not necessarily admissible. The Stein eﬀect (see Note 2.8.2) is

442
Invariance
9
an illustration of this possible suboptimality because the best equivariant
estimator of a location parameter, x, is inadmissible for the quadratic loss
in dimension 3 and above. Therefore, the question of the admissibility of the
best equivariant estimator cannot be considered in general for noncompact
groups.
On the contrary, it is possible to extend the minimaxity property beyond
than for the compact case, through the Hunt–Stein Theorem2. This result
is intuitively sound because, when a problem is invariant, there exists an
equivariant estimator with a constant risk that attains the lower bound of
the maximal risks,
inf
δ sup
θ
R(δ, θ).
Furthermore, using the natural invariant structures of the model, it seems
legitimate to improve on an estimator δ, by averaging it, that is, by inte-
grating over G
δ∗(x) =

G
δ(gx)νr(dg),
if L(θ, d) is convex in d and if the theorem `a la Fubini given in Section
9.4 applies (assuming δ∗is well deﬁned). In fact, we would then get (in an
informal way)
R(δ, θ)
=
IEθ[L(θ, δ(x))]
=
IET (IEθ[L(θ, δ(x))|Q(x) = T ])
≥
IET [L(θ, δ∗(t))]
=
R(δ∗, θ).
This improvement is similar to the domination result of the Rao–Blackwell
Theorem, when conditioning on a suﬃcient statistic.
We formalize this sketch of proof a bit further by introducing the notion
of amenable group presented in detail in Bondar and Milnes (1981). Firstly,
the following counter-example shows that intuition is not always satisfac-
tory, in particular when the invariance structures are too strong, that is,
when G is too large.
Example 9.5.1 (Stein (1965)) Consider x ∼Np(0, Σ) and y ∼Np(0, ϱΣ)
with p ≥2. The parameter ϱ is estimated under the loss function
L((ϱ, Σ), d) = II[1/2,+∞)
1 −d
ϱ


.
The problem is then invariant under the action of the linear group GLp
because, if B is a nonsingular matrix, Bx ∼Np(0, BΣBt) and By ∼Np(0,
ϱBΣBt). As ¯gB(ϱ, Σ) = (ϱ, BΣBt), the equivariant estimators are actually
invariant
δ(Bx, By) = δ(x, y)
2 This theorem is also famous for remaining without published proof for a long time,
although Kiefer (1957) established this result in a particular case.

9.5
The Hunt–Stein theorem
443
for every x, y, and B. If x and y are linearly independent (an event that
occurs with probability 1), we can ﬁnd B such that
Bx = (1, 0, . . . , 0)t
and
By = (0, 1, 0, . . ., 0)t,
which implies that the equivariant estimators are almost everywhere con-
stant. Since
R(δ0, (ϱ, Σ)) = 1
if
1 −δ0
ϱ
 > 1/2
for a given constant δ0, the minimax risk of the equivariant estimators is
1.
Deﬁning
δ1(x, y) =

y1
x1
 ,
the risk of δ1 is
R(δ1, θ)
=
Pϱ,Σ
1 −

y1
x1ϱ

 ≥1/2

=
P
1 −

z1
z2

 ≥1/2

,
where z1, z2 are i.i.d. N(0, 1). Therefore, this risk is also constant, but
strictly smaller than 1. Notice that δ1 is also an equivariant estimator
for the multiplicative group, which then appears as a more appropriate
invariance structure.
∥
For a general approach to this problem, consider a locally compact group
of transformations G, with a right Haar measure νr. Let V be an algebra of
real-valued essentially bounded measurable functions on G, such that the
constant function 1 is in V.
Deﬁnition 9.5.2
A mean on V is a linear and continuous functional m
on V such that
(i) m(1) = 1; and
(ii) m(f) ≥0 if f ∈V and f ≥0 (a.s.).
That such a functional m exists is actually a necessary and suﬃcient
condition for the Hunt–Stein Theorem to hold. In this case, it is then
possible to average on the orbits of X with respect to G, as suggested at
the beginning of this section.
Example 9.5.3
(Bondar and Milnes (1981))
For G = IR and n ∈IN,
consider
mn(f) = 1
2n
 n
−n
f(x) dx;

444
Invariance
9
then mn deﬁnes a mean on L∞(IR). Moreover, the sequence (mn) has an
accumulation point m in the sense of the weak topology on L∞: for every
f ∈L∞, ϵ > 0, and n0 ∈IN, there exists n ≥n0 such that
|mn(f) −m(f)| < ϵ.
In particular, this accumulation point satisﬁes m(f) = 0 for every f such
that f(x) goes to 0 when x goes to ±∞. Notice also that m is not σ-
additive, and that the sequence (mn) does not converge to m in the sense
of the weak topology.
∥
Deﬁnition 9.5.4
The mean m is said to be right-invariant if, for every
f ∈V and g ∈G, m(fg) = m(f), where fg(x) = f(xg). The group G
is said to be amenable if there exists a right-invariant mean on L∞(G) or,
equivalently, on CB(G), the space of the continuous bounded functions on G.
As shown in Bondar and Milnes (1981), the existence of an amenable
group is equivalent to the existence of a sequence of almost right-invariant
probability measures: in such a case, there exists a sequence (Pn) of proba-
bility measures on G such that, for every B ∈B(G) and every g ∈G,
lim
n→+∞|Pn(Bg) −Pn(B)| = 0.
Moreover, there exists a sequence (Gn) of nested compact sets such that
the density of Pn is νr(Gn)−1IIGn(g) (with respect to νr). Therefore, the ex-
istence of the sequence (Gn) allows for the approximation of the Haar mea-
sure νr by a sequence of probability distributions, and these probabilities
are almost invariant in the sense that if B∩Gn = Bg∩Gn, Pn(B) = Pn(Bg)
(see also Strasser (1985) and Lehmann (1986)). Example 9.5.3 provides a
direct illustration of this result.
Examples of amenable groups are the additive and multiplicative groups,
the group of location-scale transformations (see Example 9.4.1), and the
group Tp of invertible upper triangular matrices. On the contrary, the lin-
ear group GLp and the group SLp of matrices with determinant 1 are
not amenable. Bondar and Milnes (1981) provide many other structural
examples of amenable and nonamenable groups.
The Hunt–Stein Theorem then states the minimaxity of the best equiv-
ariant estimator.
Theorem 9.5.5
If the group G is amenable and the statistical problem
(X, f(x|θ), D, L) is invariant under the action of G, the existence of a min-
imax estimator implies the existence of a minimax equivariant estimator.
Moreover, an equivariant estimator that is minimax among the equivariant
estimators is minimax.
A proof of this theorem is provided by Berger (1985a, §6.7) in the case
where G is ﬁnite, by Lehmann (1983, §9.5) for tests, and by Le Cam (1986,

9.6
The role of invariance in Bayesian Statistics
445
§8.6) in more general settings, as a consequence of the ﬁxed-point theorem
of Markov and Kakutani. As mentioned at the beginning of this section, the
Hunt–Stein Theorem relies on an adapted version of the Fubini Theorem.
To give a quick sketch of a proof, let us assume L is convex. For a real-valued
estimator δ, deﬁne
δ∗(x) = m(˜δx),
where m is the right-invariant mean and ˜δx(g) = δ(gx). The estimator δ∗
is then equivariant since, if g0 ∈G,
δ∗(g0x)
=

G
˜g−1δ(gg0x) dm(g)
=

G
˜g0˜g−1
0 ˜g−1δ(gg0x) dm(g)
=
˜g0

G
˜g−1δ(gx) dm(g)
=
˜g0δ∗(x),
because of the right-invariance of m. Moreover,
sup
θ
R(δ∗, θ) ≤sup
θ

G

X
L
	
θ, ˜g−1δ(gx)

f(x|θ) dx dm(g)
(9.5.1)
from the convexity of L. Therefore
sup
θ
R(δ∗, θ)
≤
sup
θ

G

X
L (¯gθ, δ(gx)) f(x|θ) dx dm(g)
=
sup
θ

G
R(¯gθ, δ) dm(g)
≤
sup
θ
R(δ, θ),
which implies3 the domination of δ by δ∗.
A consequence of the Hunt–Stein Theorem is that, in the normal case, the
maximum likelihood estimator, x ∼Np(θ, Ip), is minimax for every value of
p, although inadmissible for p ≥3. The same result holds if x ∼Np(θ, σ2Ip)
and if the unknown variance σ2 is estimated by s2/q, when s2 ∼σ2χ2
q.
9.6 The role of invariance in Bayesian Statistics
To conclude this chapter, let us reiterate the reservations expressed in
Berger (1985a) over the implications of the invariance requirements on the
Bayesian approach, in particular, as a determination technique for non-
3 Notice that these indications do not constitute a rigorous proof, since the application
of the Fubini Theorem in (9.3.8) is not always justiﬁed. In fact, this “averaging” can
only be used under particular conditions. Otherwise, it would also lead to a general
admissibility result for the best equivariant estimator under convex losses, a result
negated by the Stein eﬀect.

446
Invariance
9
informative distributions, even though it provides a justiﬁcation for the
choice of an alternative to the Jeﬀreys prior (see Example 9.4.1).
One criticism of the notion of invariance is that, although intuitively
attractive, it is not devoid of ambiguity and that, since it is sometimes
possible to consider several globally invariant groups, the resulting best
equivariant estimators can lead to distinct inferences, which contradicts
the Likelihood Principle.
Another criticism is that the natural invariance structures of a statistical
model can be either too weak, and thus of no use to determine an estimator,
or too strong, and therefore too constraining. An extreme example of the
ﬁrst setting is the Poisson distribution, where there is actually no invariance
structure at all. The following example illustrates the opposite case (see also
Example 9.5.1).
Example 9.6.1 Consider a distribution family symmetric around a loca-
tion parameter θ, that is, such that x ∼f(|x −θ|). The loss function is
ϱ(|d −θ|). If the invariance by symmetry, that is, the fact that the distri-
bution of y = −x belongs to the same family, is taken into account, the
estimators that correspond to π(θ) = 1 and satisfy
δ(x + c) = δ(x) + c
and
δ(−x) = −δ(x)
reduce to δ(x) = x, which is not necessarily a good estimator.
∥
An excess of invariance can obviously be reduced by taking into account
only a part of the invariance structures, that is, by considering a subgroup
G0 of G that induces a transitive action on Θ, while being as small as
possible. Nonetheless, the choice of this subgroup, when possible, can be
crucial in the resulting inference.
As a last but important criticism, notice that a modeling of statistical
problems based on invariant structures can be damaging from a subjective
point of view, since it constrains the decision structures to be compatible
with invariance—therefore, in particular, to choose an invariant loss—and
can conﬂict with the prior information—the only compatible prior distri-
bution being the Haar measure. Such a modeling can also be damaging
from an eﬃciency point of view, since the equivariant estimators can be
strongly inadmissible, as shown by the Stein eﬀect and Example 9.5.1 (see
also Examples 4.1.4–4.2.3 in Lehmann (1983, §4.4)). Moreover, invariance
does not necessarily lead to a good noninformative distribution, as shown
by Example 9.6.1. And, in practice, the computation and derivation of right
Haar measures can be quite involved.
9.7 Exercises
Section 9.2
9.1 (Blackwell and Girshick (1954))
Consider the distribution f with weights
f(k) = 1/k(k + 1) for k = 1, 2, . . . and x ∼f(x −θ), with θ ∈IR. Under the

9.7
Exercises
447
loss function,
L(θ, d) =
 d −θ
if d > θ,
0
otherwise,
show that the equivariant estimators are of the form x −c, and that every
equivariant estimator has an inﬁnite risk. Compare with the constant estima-
tor δ0(x) = c.
9.2 Consider x an observation from a C(θ, 1) distribution. Under quadratic loss,
show that all equivariant estimators have inﬁnite risk. Propose a ﬁnite risk
estimator other than the constant estimator.
9.3 (Berger (1985a)) Consider
x = (x1, . . . , xn) ∼f(x1 −θ, . . . , xn −θ),
where θ is unknown. The hypothesis H0 : f = f0 is to be tested against
H1 : f = f1 under the 0 −1 loss.
a. Show that T(x) = (x1 −xn, . . . , xn−1 −xn) is a maximal invariant statistic
for the group of transformations
G = {gc; gc(x1, . . . , xn) = (x1 + c, . . . , xn + c), c ∈IR} .
b. Deduce that every invariant test only depends on y = T(x), and that the
optimal tests have the following rejection region:
W = {f ∗
1 (y) ≥Kf ∗
0 (y)},
where f ∗
i is the density of y under Hi.
9.4 (Berger (1985a)) Consider x distributed according to
Pθ(x = θ −1) = Pθ(x = θ + 1) = 1/2.
The associated loss function is
L(θ, d) =
 |θ −d|
if |θ −d| ≤1,
1
otherwise.
Give the best equivariant estimators for the translation group and show that
they are dominated by
δ∗(x) =
 x + 1
if x ≤0,
x −1
otherwise.
9.5 (Berger (1985a)) Consider x1, . . . , xn a sample from the truncated normal
distribution, with density
f(x|θ) =
 2
π
1/2
e−(x−θ)2/2II[θ,+∞)(x).
Show that the best equivariant estimator of θ under quadratic loss is
δ∗(x) = ¯x −exp{−n(x(1) −¯x)2/2}
√
2nπΦ(√n(x(1) −¯x))
.
Section 9.3
9.6 Consider x ∼N(θ, aθ2), with θ ∈IR and known a > 0. The parameter θ is
estimated under the loss L(θ, d) = ( d
θ −1)2.

448
Invariance
9
a. Show that the problem is invariant under the group of transformations
G = {gc; gc(x) = cx, c > 0}.
Is the action of the group transitive?
b. Give the best equivariant and the maximum likelihood estimators of θ.
c. Compare with the estimators obtained in Exercise 3.33 and in Example
9.4.7.
d. Use Exercise 3.33 to show that the best equivariant estimator δ0 is a gen-
eralized Bayes estimator.
9.7 (Lehmann (1983))
Consider the estimation of a scale parameter σ, under
the loss
L(σ, δ) =
 δ
σ −1
2
,
(9.7.1)
for n observations
x1, . . . , xn ∼1
σn f
x1
σ , . . . , xn
σ

.
a. If z = (x1/xn, . . . , xn−1/xn, xn/|xn|), show that every estimator of σ equiv-
ariant under scale transformations can be written
δ(x) = δ0(x)/ω(z),
where δ0 is a particular equivariant estimator, and that z is a maximal
invariant statistic.
b. Determine the function ω∗that minimizes
IE[L(σ, δ(x))|z]
under (9.7.1), and deduce the best equivariant estimator.
c. Write this estimator in an appropriate form in order to ﬁnd the result of
Section 9.4 with the corresponding Haar measure.
d. Consider the previous questions for the estimation of σr (r ∈IR∗
+) under
the loss
L(σ, δ) =
 δ
σr −1
2
.
9.8 Apply the results of Exercise 9.7 to the following cases:
(i) x1, . . . , xn are i.i.d. N(0, σ2);
(ii) x1, . . . , xn are i.i.d. G(α, σ); and
(iii) x1, . . . , xn are i.i.d. U[0, σ].
9.9 Examine Exercise 9.7 under the following losses:
L(σ, δ) = |δ −σ|
σ
,
L(σ, δ) = δ
σ −log(δ/σ) −1,
L(σ, δ) =
σ
δ −1
2
.
9.10 (Lehmann (1983)) Consider the estimation of σ in the case when
x = (x1, . . . , xn) ∼1
σn f
x1 −θ
σ
, . . . , xn −θ
σ

,
under the action of the aﬃne group
Ga = {ga,b; ga,b(x) = ax + b1, a > 0, b ∈IR},
where 1 = (1, . . . , 1) ∈IRn.

9.7
Exercises
449
a. Determine the best equivariant estimator under the loss (9.7.1) similarly
to Exercise 9.7. (Hint: Use the transformations yi = xi −xn and deﬁne
zi = yi/yn−1 (i ̸= n −1), zn−1 = yn−1/|yn−1|.)
b. Compare with a Bayesian formulation using the right Haar measure.
c. Consider the previous questions when estimating θ under the loss
L(θ, δ) = (θ −δ)2
σ2
.
d. Apply to the case where xi−θ ∼Exp(σ), and show that the best equivariant
estimator of θ is
δ∗(x) = x(1) −1
n2
n

i=1
(xi −x(1)).
9.11
∗(Eaton (1989)) Consider G ⊂IR∗
+ × IR, with the group operation
(a1, b1)(a2, b2) = (a1a2, a1b2 + b1).
If D = {x ∈IRn; x1 = · · · = xn}, consider X = IRn −D. Assume that G acts
on X by
(a, b)x = ax + ben,
where en = (1, . . . , 1)t. Show that the maximal invariant statistic is
f(x) = x −¯xen
s(x)
,
with ¯x = 
xi/n, s2(x) = 
(xi −¯x)2.
9.12
∗(Eaton (1989)) Verify that if there exists a multiplier ξ on G, that is, a
real-valued function such that ξ(g1g2) = ξ(g1)ξ(g2), that satisﬁes
f(x|θ) = f(gx|¯gθ)ξ(g)
uniformly on X , Θ, G, the family
P = {f(x|θ); θ ∈Θ}
is G-invariant. Deduce that, in this case, the maximum likelihood estimator is
equivariant, as is every Bayes estimator associated with a relatively invariant
prior measure, that is, such that there exists a multiplier ξ1 with π(gB) =
ξ1(g)π(B) uniformly in B and g.
9.13
∗(Delampady (1989b)) Consider x ∼Np(θ, Ip). The hypothesis H0 : θ = θ0
is tested against H1 : θ ̸= θ0. This problem is invariant under the action of the
orthogonal group Go and we only consider prior distributions in the invariant
class
I = {π; π(gA) = π(A), ∀A ∈B(IRp), ∀g ∈Go}.
a. Show that t(x) = ||x||2 is a maximal invariant statistic, distributed as a
noncentral χ2
p, with the noncentrality parameter, η(θ) = ||θ||2 (the corre-
sponding maximal invariant statistic on ¯G0), and that its density can be
written
q(t(x)|η(θ)) =

Go
f(gx|θ) dμ(g),
where μ is the Haar measure on G0.

450
Invariance
9
b. Deduce that if Bπ is the Bayes factor, it satisﬁes
inf
π∈I Bπ(x) = q(t(x)|θ0)
q(t(x)|ˆη) ,
where ˆη is the maximum likelihood estimator of η.
c. Compare with the p-value for diﬀerent values of t(x).
9.14
Show that the intrinsic losses deﬁned in Section 2.5.4 are naturally invari-
ant.
9.15 Consider x ∼N(θ, σ2) and the parameter of interest is eθ, when σ2 is
known.
a. Show that
IEθ[eax] = eaθ+a2σ2/2.
b. Among the estimators of the form δc(x) = ex+cσ2, determine the best
estimator for the quadratic loss L2, δ∗. Show that δ∗is a Bayes estimator
and determine the corresponding prior π∗. (Consider ﬁrst the Lebesgue
measure and the weighted quadratic loss
L0(θ, δ) = e−2θ(eθ −δ)2.
What is the Bayes estimator for the Lebesgue prior under L2?)
c. Consider the previous question for the absolute error loss,
L1(θ, δ) = |eθ −δ|.
Show that the best estimator is associated with π(θ) = e−θ. Is this answer
surprising from an invariance point of view?
d. Given the estimator δ∗, we want to evaluate the performances of δ∗under
L0 and L2, that is, to estimate L0(θ, δ∗(x)) and L2(θ, δ∗(x)) under the
quadratic loss
(L0(θ, δ∗(x)) −γ)2 .
(9.7.2)
Show that, for π(θ) = 1, the posterior loss IEπ[L0(θ, δ∗)|x] is constant and
equal to the constant risk of δ∗.
e. Show that, for π∗(θ) = exp(−2θ), the posterior variance of δ∗is
γπ(x) = e2x−2σ2 
1 −e−σ2
.
Show that γπ is an unbiased estimator of the risk, IEθ[L2(θ, δ∗(x))], and that
it is dominated by the Bayes estimator of L2(θ, δ∗(x))] under π(θ) = e−4θ.
Can you justify the use of this prior on invariance grounds?
Section 9.4
9.16
∗(Eaton (1989))
Show that, for a topological group G, two left-invariant
integrals, that is, functionals such that

G
f(gx)μ(dx) =

G
f(x)μ(dx)
for every f ∈L1(μ) and g ∈G, are necessarily proportional.
9.17
∗(Eaton (1989)) Consider νℓa left Haar measure, f ∈K(G) and
J1(f) =

G
f(xg−1)νℓ(dx).

9.7
Exercises
451
a. Show that J1 is left-invariant. Deduce that there exists a function Δ on G
such that
J1(f) = Δ(g)

G
f(x)νℓ(dx) = Δ(g)J(f).
The function Δ is called the modulus of G.
b. Show that Δ does not depend on the choice of J1 and that Δ(g1g2) =
Δ(g1)Δ(g2) (that is, Δ is a multiplier).
c. Consider J2 such that
J2(f) =

G
f(x)Δ(x−1)νℓ(dx).
Show that J2 is right-invariant and satisﬁes
J2(f) =

G
f(x−1)νℓ(dx).
Deduce that, if νℓis a left Haar measure,
νr(dx) = Δ(x−1)νℓ(dx)
is a right Haar measure.
d. If G is compact, show that Δ is identically equal to 1. (Hint: Use the
continuity of Δ and the fact that Δ(G) is compact.)
e. Consider G = GLn, the linear group of IRn. We denote by dx the Lebesgue
measure on Ln,n, the vector space of n × n matrices. Show that
J(f) =

G
f(x)
dx
|det(x)|n
is simultaneously right- and left-invariant. Deduce that Δ = 1. Is G com-
pact?
9.18
∗(Eaton (1989)) Consider G a compact group acting on X , and ν the unique
Haar probability distribution on G. Deﬁne U, a uniform random variable on
G, by
P(U ∈B) = ν(B).
a. Consider x ∈X . Show that μx, deﬁned by
μx(B) = P(Ux ∈B)
is the unique G-invariant probability on the orbit of x, Ox.
b. If P is a G-invariant distribution on X , show that
P =

X
μxP(dx).
c. A measurable section Y ⊂X is deﬁned by
(i) Y is measurable;
(ii) ∀x ∈X , Y ∩Ox = {y(x)}; and
(iii) the function t(x) = y(x) is measurable for the σ-algebra induced on Y
by X .
Show that, for every probability distribution Q on Y,
P =

Y
μyQ(dy)

452
Invariance
9
is G-invariant on X and that, reciprocally, every G-invariant probability can
be written this way.
d. Consider U a uniform random variable on G, Y a measurable section of X ,
and X a random variable on X . Deduce from c. the equivalence between
the following properties:
(i) the distribution of gX is independent of g ∈G; and
(ii) there exists Y , random variable on Y, independent of U, such that UY
has the same distribution as X.
e. Apply to the case X = {0, 1}n.
9.19 Consider x ∼N(θ, 1) when the quantity of interest is h(θ) = ecθ.
a. Give the risk of the Bayes estimator of h(θ) associated with π(θ) = 1 and
the quadratic loss, R(θ, δπ), and show that the Bayes estimator of h(θ)
associated with π′(θ) = R(θ, δπ)−1 dominates δπ.
b. Notice that R(θ, δπ)−1(ecθ −δ)2 is an invariant loss and derive the following
result: For every invariant loss L(θ, δ), if δπ is the estimator associated with
L and the Haar measure π, and if ω(θ) = IEθ[L(θ, δπ(x))], the estimator
associated with L and π′(θ) = π(θ)/ω(θ) is the best equivariant estimator.
Section 9.5
9.20
∗(Berger (1985a)) Consider the particular case where the group G is ﬁnite,
that is,
G = {g1, . . . , gm}.
Let us assume that the loss L(θ, a) is invariant, convex in a, and, in addition,
that the action induced by the group G on D satisﬁes
˜g
!
1
m
m

i=1
ai
"
= 1
m
m

i=1
˜g(ai).
Establish the Hunt–Stein Theorem under the additional assumption that D
is convex. (Hint: Show that, for every estimator δ, there exists an associated
invariant estimator δI which dominates δ.)
9.21
In the setting of Example 9.5.1, derive the exact risk of the estimator δ1
(Hint: Notice that z1/z2 is distributed as a Cauchy random variable.)
9.22 Examine the estimation of ϱ in Example 9.5.1 for the invariance structure
induced by the multiplicative group.
9.23 Consider (x1, . . . , xp) and (y1, . . . , yp) with normal distributions Np(0, Σ)
and Np(0, ΔΣ). The hypothesis H0 : Δ ≤Δ0 is to be tested versus H1 : Δ >
Δ0.
a. Show that the problem is invariant under GLp, the group of nonsingular
linear transformations.
b. Show that GLp is transitive on the sample space, up to a set of measure 0.
Deduce that the equivariant estimators are constant, that is, that invariant
tests at level α are ϕα(x, y) = 1 −α.
c. Consider ϕc(x, y) = IIy2
1≤cx2
1 and show that it dominates ϕα under the 0−1
loss for α = PΔ0(y2
1 > cx2
1).
d. Is GLp amenable?

9.7
Exercises
453
9.24 Consider x1, . . . , xn ∼C(μ, σ2).
a. Show that the Haar measure is πH(μ, σ) ∝1/σ.
b. Consider the reparameterization yi = 1/xi. Show that yi ∼C(ν, τ 2) and
derive ν and τ in terms of μ and σ.
c. Show that πH(ν, τ) ∝1/τ is not the transform of πH(μ, σ) and conclude
about the limitations of invariance in terms of reparameterization.
Section 9.6
9.25
∗(Villegas (1990)) Consider a family of probability distributions Pθ on X ,
with θ ∈Θ and T(x) taking values in a Euclidean aﬃne space E, such that
the likelihood function is
ℓ(θ|x) = c1(x)c2(θ) exp{−||T(x) −θ||2/2}.
This model is called Euclidean Bayesian if π(θ) = 1.
a. Deduce that the corresponding Euclidean prior distribution in the case of
a Poisson model P(λ) is π(λ) = 1/λ.
b. Show that the p-value p(x) = Pλ0(X ≥x) when testing H0 :
λ ≤λ0
against H1 :
λ > λ0 is related to this prior distribution, but that this
relation does not hold for the alternative test of H0 :
λ ≥λ0 against
H1 : λ < λ0.
c. Show that the Haldane prior distribution
π(p) =
1
p(1 −p)
(9.7.3)
also appears as a Euclidean model when x ∼B(n, p). Is (9.7.3) still the
Euclidean prior for the negative binomial distribution, Neg(n, p)?
d. If 0 < x < n, show that, in the binomial case, the p-values Pp0(X ≤x) and
Pp0(X ≥x) associated with the hypotheses H0 : p ≥p0 and H0 : p ≤p0
do not correspond to the Euclidean distribution (9.7.3).
e. In the normal case N(μ, σ2), show that the Euclidean prior distributions
are the following ones:
(i) π(θ) = 1 if θ = μ;
(ii) π(θ) = 1 if θ = σ−2; and
(iii) π(θ) = θ2 if (θ1, θ2) = (μ, σ−2).
9.26 Examine the issue of compatibility between the invariance requirements
and the Likelihood Principle. In particular, determine whether the maximum
likelihood estimator is always an invariant estimator.
9.27 *For an arbitrary loss function L(θ, δ) and a given prior distribution π,
assume that the Bayes estimator δπ is such that 0 < R(θ, δπ) < ∞for every
θ.
a. If we deﬁne Lπ(θ, δ) = L(θ, δ)/R(θ, δπ), show that δπ has constant risk 1.
Does this imply that δπ is minimax? (Hint: Notice that δπ is not necessarily
the Bayes estimator under Lπ.)
b. Consider the special case when x ∼N(θ, 1) and π is N(θ0, τ 2). Compute
R(θ, δπ) and study the behavior of the Bayes estimator associated with π
and Lπ, compared with δπ (numerically, if necessary).

454
Invariance
9
c. In the event δπ
1 is associated with π and Lπ
1 = Lπ is diﬀerent from δπ, a
sequence of estimators δπ
n can be deﬁned sequentially by Lπ
n = L/R(θ, δπ
n−1).
What can be said about the limit of the sequence (δπ
n)?
d. Study the above sequence when x ∼P(λ), π(λ) = 1, and L(λ, δ) = (1 −
λ/δ)2.
9.8 Notes
9.8.1 Invariance and marginalization paradoxes
Besides bringing a new perspective on equivariant estimation, Helland (1999)
advocates the use of right Haar measures as means of avoiding the marginal-
ization paradoxes, as already pointed out in Dawid et al. (1973). (See Exercises
3.44—3.50.)
More precisely, given a model (X , Θ, f(x|θ)) with a group G acting on X and
the corresponding group ¯G acting on Θ, a function h(θ) is said to be invariantly
estimable if h(θ1) = h(θ2) implies h(¯gθ1) = h(¯gθ2) for all ¯g ∈¯G (Hora and
Buehler (1966)). Helland (1999) supports the view that the invariance perspec-
tive and the use of the corresponding Haar measure should be restricted to the
estimation of invariantly estimable functions of the parameters. For instance,
although the Lebesgue measure is the right Haar measure for the translation
group, and thus applies to the normal distribution Np(θ, Ip), it should not be
used for the estimation of ||θ||2, thus avoiding the subeﬃciency pointed out
in Section 3.5.4. This point of view somehow relates to the construction of re-
ference priors: given a parameter of interest, the relevant invariance structure
should be determined ﬁrst, and the corresponding right Haar measure could
then be derived as the appropriate noninformative prior. Some drawbacks to
this approach are that there are functions for which non-trivial invariance
groups cannot be found, and that there is always arbitrariness involved in the
choice of these groups, when they exist.
Example 9.8.1 (Helland (1999))
When x ∼Np(θ, σ2Ip) and θ is on the
sphere of radius c, the best equivariant estimator of θ under the group of
rotations is associated with the uniform measure on the sphere and is given
by (8.3.2). (See Exercise 4.36.) If c = ||θ||2 is unknown, it can be estimated
based on ||x|| rather than x (see Examples 3.5.9 and 3.5.11), using for instance
the maximum likelihood estimator, or Saxena and Alam’s (1982) improvement
(||x||2 −p)+. If one plugs this estimator κ(x) of c in δc, the resulting estimator
˜δ = κ(x) Ip/2(κ(x)||x||)
Ip/2−1(κ(x)||x||)
x
||x||,
where Iν is the modiﬁed Bessel function, behaves similarly to a shrinkage
estimator (2.8.4) for large values of ||x||, as shown by Bock and Robert (1990)
and Beran (1996). (See Exercise 10.37.)
∥
For an invariantly estimable function h(θ), Helland (1999) considers the sub-
group of ¯G,
¯K = {¯g ∈¯G; h(¯gθ) = h(θ) for all θ ∈Θ} ,
since h(θ) is a maximal invariant for ¯K. Given the corresponding subgroup
of G,
K = {g ∈G; ¯g ∈¯K} ,

9.8
Notes
455
let z be a maximally invariant variable for K. If η = h(θ), Helland (2000)
then shows that the marginalization paradox does not occur for (η, z) when
using the right Haar measure associated with ¯G. This means that, under this
measure, if θ = (η, ξ) and x = (z, y), and if the marginal posterior distribution
of η depends only on z, it can also be obtained as a posterior distribution
based on z alone.


CHAPTER 10
Hierarchical and Empirical
Bayes Extensions
Books and papers and scrolls covered nearly every ﬂat surface, with all
sorts of odd things interspeded among the piles, and sometimes on top of
them. Strange shapes of glass or metal, spheres and tubes interlinked, and
circles held inside circles, stood among bones and skulls of every shape and
description.
Robert Jordan, The Dragon Reborn, Book III of the Wheel of Time.
10.1 Incompletely Speciﬁed Priors
In the previous chapters, we have noticed (and even stressed!) the ambiva-
lent aspect of Bayesian analysis: it is suﬃciently reductive to produce an
eﬀective decision, but this eﬃciency can also be misused. For instance, the
subjective aspects of a Bayesian analysis can always be modiﬁed to produce
conclusions established in advance. Of course, the same misappropriation
is possible in a frequentist framework through the choice of the loss or esti-
mation criteria, while the classical approach does not distinguish between
the subjective and objective inputs of an analysis. The main point here is
that, as mentioned in Chapter 3, the choice of a prior distribution should
always be justiﬁable by the statistician, that is, it must be based on sound
(or “repeatable”) arguments. So the fact that Bayesian tools may lead to
illegitimate inferences cannot be taken as a ﬂaw of the Bayesian paradigm.
A more pertinent criticism is that the prior information is rarely rich
enough to deﬁne a prior distribution exactly. In such cases, it seems neces-
sary to include this uncertainty in the Bayesian model, although the notion
of prior distribution may seem insuﬃcient to represent such a degree of ig-
norance. In fact, the residual uncertainty prompted some extensions of the
Bayesian paradigm, such as the upper and lower probabilities of Dempster
(1968), or the imprecise probabilities of Walley (1991).
The hierarchical Bayes analysis considers, nonetheless, that this is can be
done within the Bayesian paradigm. This particular modeling of the prior

458
Hierarchical and Empirical Bayes Extensions
10
information decomposes the prior distribution into several conditional lev-
els of distributions, and thus allows for a distinction between structural
and subjective items of information. According to the Bayesian paradigm,
uncertainty at any of these levels is incorporated into additional prior dis-
tributions. In the simplest cases, the hierarchical structure is reduced to two
levels, the parameters of the ﬁrst being associated with a prior distribution
deﬁned on the second. The ﬁrst-level distribution is generally a conjugate
prior, owing to the computational tractability of these distributions, and
also because the upper level somehow compensates for modeling errors at
the lower levels. (Another justiﬁcation for a conjugate modeling is provided
by Dalal and Hall (1983) and Diaconis and Ylvisaker (1985); see Section
3.4.) We have already encountered examples of such modelings in Chapter
6, such as in Example 6.3.10.
A general characteristic of the hierarchical modeling is that it improves
the robustness of the resulting Bayes estimators: while still incorporating
prior information, the estimators are also well performing from a frequentist
point of view (minimaxity and admissibility), although these two require-
ments are usually diﬃcult to reconcile.
Additional justiﬁcations of hierarchical Bayes modeling stem from real
life, since there are settings in medicine, biology, animal breeding, eco-
nomics, and so on, where the population of interest can be perceived as
a subpopulation of a meta-population, or even as a subpopulation of a
subpopulation of a global population. This is, for instance, the case with
meta-analysis, according to which several experiments about the same phe-
nomenon undertaken at diﬀerent places with diﬀerent subjects and diﬀer-
ent protocols are pooled together (see, e.g., Mosteller and Chalmers (1992),
Mengersen and Tweedie (1993), or Givens et al. (1997)).
Example 10.1.1
(Guihenneuc-Jouyaux et al. (1998)) The Human Im-
munodeﬁciency Virus (HIV) is the virus that leads to AIDS. For a given
patient, the progression of the HIV infection towards AIDS can be rep-
resented as a transition between seven stages of increasing severity, the
ﬁnal stage corresponding to AIDS. The moves between the stages are di-
rected by a continuous time Markov model with inﬁnitesimal generator
Λ. (This means that the distribution of the state at time T , given a dis-
tribution ω0 = (ω01, . . . , ω07) at time 0, is given by the matrix product
ω0 · exp{T Λ}.)1
For a given patient infected with HIV, the ﬁrst six stages are not observed
directly, but only through random variables (1 ≤i ≤n , 1 ≤j ≤ni),
xij ∼N
	
μSij, σ2
,
where i denotes the individual and j the follow-up point, and where Sij
denotes the HIV stage. The xij’s represent blood markers (CD4 counts)
1 The extension of the exponential function to multivariate settings such as this one is
obtained by using the series representation of exp(x). See Exercise 10.2.

10.1
Incompletely Speciﬁed Priors
459
δ
μ,
Λ
2
Sini
Xini
"true" states
observed marker values
measurement process parameters
infinitesimal generator of Markov
process
.......
.......
S
X
X
i1
i1
i2
Si2
σ
Figure 10.1.1. Directed acyclic graph of the hierarchical model.
(Source: Guihenneuc–Jouyaux et al. (1998).)
subject to great variability, and to measurement error. This is thus a spe-
cial case of hidden Markov model (see Exercise 6.46), with the additional
diﬃculty that the hidden Markov chain is operating in continuous time.
But the forward–backward formulas developed in Exercise 6.47 still apply
(Exercise 10.2). A similar model has been proposed by Kirby and Spiegel-
halter (1994).
The Sij’s therefore constitute the ﬁrst level of a hierarchical model, with
hyperparameters such as the generator matrix Λ corresponding to the sec-
ond level and common to all individuals. A second hyperparameter is δ, the
prior distribution on the HIV stage at the ﬁrst observation. It is usually
quite helpful to represent such hierarchical models as graphs (more exactly,
directed acyclic graphs or DAG’s). Figure 10.1 provides this representation
for the HIV model, the convention being that boxes correspond to known
or observed quantities, and circles to unknown quantities; arrows indicate
probabilistic dependence. (See Note 10.7.1 and Lauritzen (1996) for more
details on graphical models.)
∥
The empirical Bayes analysis is based on the same perception of impre-
cision over the prior information, but at a more pragmatic level. In fact,
this approach considers that it is illusory to try to model this imprecision
by several levels of conditional distributions when the ﬁrst level is already
partly or totally unknown. Rather paradoxically, the empirical Bayes anal-
ysis still relies on a conjugate prior modeling, where the hyperparameters
are estimated from the observations and this “estimated prior” is then
used as a regular prior in the subsequent inference. Needless to say, the

460
Hierarchical and Empirical Bayes Extensions
10
substitution of hyperparameters by estimated hyperparameters that is the
core of the empirical Bayes analysis deﬁnitely excludes this technique from
the Bayesian paradigm, but allows the statistician to take advantage of dif-
fuse prior information in a simpliﬁed way. Moreover, it is often the case that
the resulting estimators have good frequentist properties, although there is
too much arbitrariness in the determination of the estimated hyperparam-
eters to make a general rule of this fact. A related interest in the empirical
Bayes modeling is to provide Bayesian support for the Stein eﬀect (see Note
2.8.2). The empirical Bayes analysis may also appear as a convenient alter-
native in settings where a hierarchical Bayes analysis is too involved, even
though increasingly eﬃcient computation techniques progressively suppress
the need for such approximations, as we saw in Chapter 6.
10.2 Hierarchical Bayes analysis
This section provides only a short introduction to hierarchical Bayes analy-
sis, and focuses on a few features of interest. For a more thorough treatment
of this topic, see Berger (1985a), in relation to the robustness aspects, and
Deely and Lindley (1981), Dumouchel and Harris (1983), George (1986a),
Angers and MacGibbon (1990), Gelman et al. (1996), Hobert (2000) and
Draper (2001). For applications to animal breeding, see, e.g., Fouley et al.
(1992).
10.2.1 Hierarchical models
For reasons owing to the modeling of the observations or to the decompo-
sition of the prior information, it may happen that the Bayesian statistical
model is hierarchical, that is, involves several levels of conditional prior
distributions.
Deﬁnition 10.2.1
A hierarchical Bayes model is a Bayesian statistical
model, (f(x|θ), π(θ)), where the prior distribution π(θ) is decomposed in
conditional distributions
π1(θ|θ1), π2(θ1|θ2), . . . , πn(θn−1|θn)
and a marginal distribution πn+1(θn) such that
π(θ) =

Θ1×...×Θn
π1(θ|θ1)π2(θ1|θ2) · · · πn+1(θn) dθ1 · · · dθn+1.
(10.2.1)
The parameters θi are called hyperparameters of level i (1 ≤i ≤n).
Before we justify this decomposition by its usefulness, notice that hier-
archical structures can also occur in classical statistical models.
Example 10.2.2
A classical occurrence of hierarchical models is the in-
clusion of random-eﬀects in a linear model. This extension can be written

10.2
Hierarchical Bayes analysis
461
under the form
y|θ
∼
Np(θ, Σ1),
θ|β
∼
Np(Xβ, Σ2),
with no reference to a Bayesian modeling. The mean of y, θ, is decomposed
in ﬁxed eﬀects, Xβ, and in random-eﬀects, Zη, where η is normal with
mean 0 (the covariance matrix Σ2 can then be singular). These models
are often used in biometry, in particular in animal breeding, to distinguish
between the inﬂuence of the ﬁxed eﬀect (e.g., sires, breed, year, etc.) and
the inﬂuence of random factors (e.g., dams within sires).
∥
Another classical class of non-Bayesian hierarchical structures comprises
latent variable models, as in mixtures (Section 6.4) and other hidden mix-
ture representations (Note 6.6.3). The vector of latent variables z then con-
stitutes the ﬁrst level of a hierarchical Bayesian model, the prior modeling
per se occurring at higher levels.
Such examples also point out that the boundary between classical and
Bayesian hierarchical models is sometimes fuzzy, and mainly depends on
the interpretation of the parameters. For instance, in the random-eﬀect
model, we operate according to a classical perspective if the inference is
about the ﬁxed eﬀects (β), and according to a Bayesian perspective when
considering the global eﬀect (θ). Similarly, if the latent variables zt are
of interest, as in the stochastic volatility model (4.7.2) for the y∗
t ’s, they
are treated as missing data, whereas, if they are introduced for a more
convenient representation of the model, as in the case of mixtures used
for non-parametric modeling, the zt’s can be treated as parts of the prior
modeling.
Let us stress here that a hierarchical Bayes model is simply a special
type of Bayesian model. In fact, if
x ∼f(x|θ), θ ∼π1(θ|θ1), . . . , θn ∼πn+1(θn),
(10.2.2)
we recover the usual Bayesian model
x ∼f(x|θ),
θ ∼π(θ),
for the prior
π(θ) =

Θ1×...×Θn
π1(θ|θ1) . . . πn(θn−1|θn)πn+1(θn) dθ1 · · · dθn.
This reduction shows that hierarchical modelings are indeed included in
the Bayesian paradigm, and therefore that this approach enjoys the gen-
eral optimality properties of the Bayesian approach with some additional
advantages related to the decomposition of the prior distribution (see Sec-
tion 10.3). It also shows why it is seldom necessary to go beyond two
conditional levels in the hierarchical decomposition. In fact, if the hyper-
parameters θ1, . . . , θn are of no interest for the inference (about θ), it is

462
Hierarchical and Empirical Bayes Extensions
10
equivalent to consider the simpler hierarchical model
x|θ ∼f(x|θ),
θ|θ1 ∼π1(θ|θ1) ,
with
θ1 ∼π2(θ1) =

Θ2×...×Θn
π1(θ1|θ2) · · · πn+1(θn) dθ2 · · · dθn ,
which eliminates intermediary steps and additional hyperparameters. None-
theless, a more elaborate decomposition may still be of interest for the
construction and the practical computation of the Bayes estimators, as
already shown in Chapter 6.
Example 10.2.3 Robert and Reber (1998) consider an experiment under
which rats are intoxicated by a substance, then treated by either a placebo
or a drug. The model associated with this experiment is a linear additive
model eﬀect: given xij, yij and zij, jth responses of the ith rat at the control,
intoxication and treatment stages, respectively, we assume that (1 ≤i ≤I)
xij
∼N(θi, σ2
c),
1 ≤j ≤Jc
i ,
yij
∼N(θi + δi, σ2
a),
1 ≤j ≤Ja
i ,
zij
∼N(θi + δi + ξi, σ2
t ),
1 ≤j ≤Jt
i ,
where θi is the average control measurement, δi the average intoxication
eﬀect and ξi the average treatment eﬀect for the ith rat, the variances of
these measurements being constant for the control, the intoxication and the
treatment eﬀects. An additional (observed) variable is wi, which is equal
to 1 if the rat is treated with the drug, and 0 otherwise.
Given that the purpose of the experiment is to assess the overall eﬀect
of the tested drug, the diﬀerent individual averages are related through a
common (conjugate) prior distribution (1 ≤i ≤I),
θi ∼N(μθ, σ2
θ),
δi ∼N(μδ, σ2
δ),
and
ξi ∼N(μP , σ2
P )
or
ξi ∼N(μD, σ2
D),
depending on whether the ith rat is treated with a placebo or a drug. The
hyperparameters of the model,
μθ, μδ, μP , μD, σc, σa, σt, σθ, σδ, σP , σD ,
are then associated with Jeﬀreys’ noninformative priors. This prior leads
to a well deﬁned posterior distribution if there are at least two observations
for each stage of the experiment.
∥
10.2.2 Justiﬁcations
The hierarchical Bayes analysis is partly based on the work of Good (see
Good (1980, 1983) for references) and is exposed in Lindley and Smith

10.2
Hierarchical Bayes analysis
463
(1972) for the particular case of the linear model, where the authors use the
duality between the usual Bayesian analysis of a random-eﬀect model and
the hierarchical Bayes analysis of a regular regression model. Despite the
reduction (10.2.1), which shows that a hierarchical Bayes model is actually
a particular case of a Bayesian model, the decomposition
π(θ) =

Θ1
π1(θ|θ1)π2(θ1) dθ1
or its generalization (10.2.1) may be preferred for several reasons:
(i) Objective reasons based on the modeling of the observed phenomenon
as a special case of a meta-population may lead to the ﬁrst two lev-
els, the Bayesian approach being justiﬁed by a prior knowledge of the
meta-population. This is the case of Examples 10.2.2 and 10.2.3. More
generally, as mentioned in Section 10.1, hierarchical Bayes models nat-
urally appear in meta-analysis where several studies have to be pooled
together.
Example 10.2.4 (Berger (1985a)) Consider xi ∼N(βi, 10) (i = 1, . . . ,
7), which represent yearly and independent measures of the intelligence
quotient (IQ) of a child, for seven consecutive years. Since IQ tests are
supposed to account for an age eﬀect, it is reasonable to consider that the
βi’s have the same mean θ, the “true” value of the IQ. A corresponding
ﬁrst-level prior distribution is then
βi|θ ∼N(θ, σ2
π)
(i = 1, . . . , 7).
Moreover, if the child belongs to a thoroughly-studied population of
children, it may be the case that, for this population,
θ ∼N(ξ, τ2),
where ξ and τ are known and this provides the second level of the anal-
ysis. Otherwise, a noninformative alternative is to take π2(θ) = 1.
∥
(ii) In connection with the above justiﬁcation, a researcher may wish to
separate the prior modeling into two parts, the ﬁrst corresponding to
the structural information about the model, the second to the more
subjective information. For instance, the information may consist of
uncertain linear restrictions on the parameters of a regression model,
whereas the distribution on the hyperparameters π2(θ1) accounts for
the imprecision of these restrictions.
Example 10.2.5 Albert (1988) represents uncertainties about general-
ized linear models (McCullagh and Nelder (1989)) (i = 1, . . . , n)
yi|xi ∼exp{θi · yi −ψ(θi)} ,
∇ψ(θi) = IE[yi|xi] = h(xt
iβ) ,
where h is the link function and xi ∈IRq a vector of covariates, by
removing the linear constraint ∇ψ(θi) = h(xt
iβ) to an higher level of the

464
Hierarchical and Empirical Bayes Extensions
10
hierarchy, that is, by introducing a conjugate prior
θi ∼exp {λ [θi · ξi −ψ(θi)]}
such that IE[∇ψ(θi)] = h(xt
iβ). The regression parameter β is thus trans-
fered to a second level with, possibly, a normal prior β ∼Nq(0, τ2Iq),
including the ﬂat prior for τ = ∞. The posterior variance of ψ(θi) then
reveals how accurate the generalized linear model is, that is, allows as-
sessment of the departure from the linearity assumption.
∥
Example 10.2.6 (Example 10.2.3 continued) An alternative also
considered in Robert and Reber (1998) is to assume the prior distribution
δi ∼pN(μδ1, σ2
δ1) + (1 −p)N(μδ2, σ2
δ2),
(10.2.3)
which allows for two possible levels of intoxication, that is, for two dif-
ferent reactions to the intoxication in the rat population. As detailed in
Robert and Reber (1998), there are metabolic reasons for this modiﬁca-
tion of the prior distribution. While this mixture structure also transfers
to the marginal distribution of the yij’s, it diﬀers from a regular mixture
model, since it requires that the yij’s belong to the same component of
the mixture for 1 ≤j ≤Ja
i .
∥
(iii) On the contrary, in a noninformative setting, a hierarchical Bayes
model suggests a compromise between the Jeﬀreys noninformative dis-
tributions, which are diﬀuse but sometimes diﬃcult to use or justify,
and the conjugate distributions, which have limited subjective justiﬁ-
cation but are analytically tractable. When the hyperparameters have
a prior hyperdistribution (or hyperprior), the noninformative perspec-
tive is reinforced, while generally providing a deﬁned posterior distri-
bution on θ. A possibility is to iterate this argument by introducing a
conjugate distribution on θ1, π2(θ1|θ2), and a noninformative distribu-
tion on θ2. However, the choice of a conjugate distribution on θ1 does
not guarantee closed-form expressions for the Bayes estimators, and,
more fundamentally, does not seem to improve the robustness of the
model. Whatever the number of levels in the distribution, the averag-
ing on the unknown hyperparameters can only reinforce the robust-
ness of the prior distribution when compared with a classical conjugate
approach. See Berger (1985a) for an explanation of why hierarchical
modeling is interesting from the robustness point of view.
Example 10.2.7
Consider the usual regression model, y = Xβ + ϵ,
that is, y ∼Nn(Xβ, σ2In), where β ∈IRp. For structural reasons, the
coeﬃcients of the regression are similar. For instance, the βi’s can de-
scribe the investment rates of diﬀerent European automobile companies,
for which the rates are quite similar. We then assume that βi ∼N(ξ, σ2
π),
where ξ represents the common value. Such a model is called exchange-
able (see Note 3.8.2, Bernardo and Smith (1994) and Gelman et al.

10.2
Hierarchical Bayes analysis
465
(1996)). If additional information is available about the common value,
we can take ξ = ξ0 or ξ ∼N(ξ0, τ2). Otherwise, the second level can be
noninformative, that is, π2(ξ) = 1.
∥
Example 10.2.8 (Example 10.2.2 continued) In the setting of the
random-eﬀect linear model,
y|θ
∼
Np(θ, Σ1),
θ|β
∼
Np(Xβ, Σ2),
Lindley and Smith (1972) and Smith (1973) assume that β also satisﬁes
a linear relation and use the following prior:
β ∼Nn(Zξ, Σ3).
An alternative prior which robustiﬁes the model is
β ∼Tn(α, Zξ, Σ3),
but this distribution simply involves an additional level in the hierarchi-
cal model compared with the original normal distribution, as shown by
Dickey (1968). In fact,
β|z ∼Np(Zξ, Σ3/z),
z ∼G(α/2, α/2),
in this case. If we consider β|z ∼Np(μ, zΣ3) (conjugate distribution)
and π(z) = 1/z (noninformative distribution) the marginal distribution
β ∼Tp(p/2, μ, Σ3),
is thus a proper distribution, contrary to the noninformative distribution
π(β) = 1.
∥
(iv) Another beneﬁcial aspect of the hierarchical Bayes analysis is that it
also robustiﬁes the usual Bayesian analysis from a frequentist point of
view, since it reduces the arbitrariness of the hyperparameter choice
(sometimes transferred to a higher level), and averages the conjugate
Bayesian answers. Section 10.3 demonstrates that, in the normal case,
many prior distributions on the hyperparameters lead to minimax gen-
eralized Bayes estimators.
(v) A last justiﬁcation of the hierarchical Bayes approach is that it can
often simplify Bayesian calculations. In fact, the decomposition of a
prior distribution π in its components π1, . . . , πn (which can be, for
instance, conjugate distributions) may allow for an easier approxima-
tion of some posterior quantities by simulation, as already stressed in
Section 6.3.5 for Gibbs sampling.
10.2.3 Conditional decompositions
A particularly appealing aspect of hierarchical models is that they allow
for conditioning on all levels, and this easy decomposition of the posterior

466
Hierarchical and Empirical Bayes Extensions
10
distribution may compensate for the apparent complexity occasioned by
these successive levels. For instance, if
θ|θ1 ∼π1(θ|θ1),
θ1 ∼π2(θ1),
we have the following result.
Lemma 10.2.9
The posterior distribution of θ is
π(θ|x) =

Θ1
π(θ|θ1, x)π(θ1|x) dθ1,
where
π(θ|θ1, x)
=
f(x|θ)π1(θ|θ1)
m1(x|θ1)
,
m1(x|θ1)
=

Θ
f(x|θ)π1(θ|θ1) dθ,
π(θ1|x)
=
m1(x|θ1)π2(θ1)
m(x)
,
m(x)
=

Θ1
m1(x|θ1)π2(θ1) dθ1.
Moreover, this decomposition works for the posterior moments, that is, for
every function h,
IEπ[h(θ)|x] = IEπ(θ1|x) [IEπ1 [h(θ)|θ1, x]] ,
where
IEπ1[h(θ)|θ1, x] =

Θ
h(θ)π(θ|θ1, x) dθ.
This result is a straightforward consequence of Bayes’s Theorem, the last
equality being easily established by the Fubini Theorem. It has important
consequences in terms of the computation of Bayes estimators, though,
since it shows that π(θ|x) can be simulated by generating, ﬁrst, θ1 from
π(θ1|x) and then θ from π(θ|θ1, x), if these two conditional distributions
are easier to work with.
Example 10.2.10 (Example 10.2.3 continued) The posterior distri-
bution of the complete parameter vector is given by
π((θi, δi, ξi)i, μθ, . . . , σc, . . . |D) ∝
I
i=1

exp −{(θi −μθ)2/2σ2
θ + (δi −μδ)2/2σ2
δ}
Jc
i

j=1
exp −{(xij −θi)2/2σ2
c}
Ja
i

j=1
exp −{(yij −θi −δi)2/2σ2
a}
Jt
i

j=1
exp −{(zij −θi −δi −ξi)2/2σ2
t }


10.2
Hierarchical Bayes analysis
467

ℓi=0
exp −{(ξi −μP )2/2σ2
P }

ℓi=1
exp −{(ξi −μD)2/2σ2
D}
(10.2.4)
σ
−
i Jc
i −1
c
σ
−
i Ja
i −1
a
σ
−
i Jt
i −1
t
(σθσδ)−I−1σ−ID−1
D
σ−IP −1
P
,
where D denotes the sample. The posterior distribution thus fails to inte-
grate into explicit formulas for the marginal distributions of the parameters
of interest and does not lead to closed-form expressions for the posterior ex-
pectations of these parameters. However, the full conditional distributions
are available, as detailed in Exercise 10.14.
∥
Obviously, Lemma 10.2.9 only holds when the various integrals are well
deﬁned. In fact, because the second-level distributions are generally im-
proper, this is not always the case. The following lemma gives a suﬃcient
condition for the posterior moments to exist when x|θ ∼Np(θ, Σ) (see
Berger and Robert (1990) for a proof):
Lemma 10.2.11
If the marginal distribution
m(x) =

Θ
f(x|θ)π(θ) dθ
is ﬁnite for every x ∈IRk, then the mean and the variance of the posterior
distribution π(θ|x) always exist.
Another important feature of hierarchical models that inﬂuences the
computation of hierarchical Bayesian estimators is given by the following
result:
Lemma 10.2.12 For the hierarchical model (10.2.2), the full conditional
distribution of θi given x and the θj’s (j ̸= i) satisﬁes
π(θi|x, θ, θ1, . . . , θn) = π(θi|θi−1, θi+1)
with the convention θ0 = θ and θn+1 = 0.
Proof. Since
π(θi|x, θ, θ1, . . . , θn)
∝
f(x|θ)π1(θ|θ1) · · · πn+1(θn+1)
∝
πi−1(θi−1|θi)πi(θi|θi+1) ,
the posterior distribution only depends on the two adjacent levels in the
hierarchy.
22
This simple result is indeed of importance because it means that the
conditional distributions in a hierarchical model only involve local hyper-
parameters. In settings such as graphical or spatial models, where the joint
distribution is deﬁned locally on a group of hyperparameters (called cliques
in graphical models), Lemma 10.2.12 thus shows that computational tech-
niques such as the Gibbs sampler (Section 6.3.3) are the only way to process
complex models of this type.

468
Hierarchical and Empirical Bayes Extensions
10
10.2.4 Computational issues
A drawback to hierarchical models is that they usually prevent an explicit
derivation of the corresponding Bayes estimators, even when the successive
levels are conjugate, and therefore that they call for numerical approxima-
tions.
Example 10.2.13
Consider x ∼B(n, p) and p|m ∼Be(m, m) with m ∈
IN∗. Therefore,
π1(p|m)
=
Γ(2m)
Γ(m)2 [p(1 −p)]m−1
=
(2m −1)
2m −2
m −1

[p(1 −p)]m−1.
If the second-level prior distribution is π2(m) = 1/(2m −1), the prior
distribution on p is then
π(p)
=

IN∗π1(p|m)π2(m) dm
=
+∞

n=0
2n
n

[p(1 −p)]n.
The posterior distribution
π(p|x) =

π1(p|m, x)π2(m|x) dm
cannot be obtained in a closed form, since, although π(p|m, x) is the beta
distribution Be(m+x, m+n−x), π(m|x) is the beta-binomial distribution
(m + x −1) . . . m (m + n −x −1) . . . m
(2m + n −1) . . . (2m)(2m −1)
up to a normalization factor. Posterior quantities such as IEπ[p|x] cannot
be obtained in closed form.
∥
The most natural solution in hierarchical settings is to use a simulation-
based approach. Indeed, as mentioned above, the decomposition provided
by Lemmas 10.2.9 and 10.2.12 is most useful in this respect, since it allows
for simulations via the Gibbs sampler or alternative MCMC techniques
(Sections 6.3.2 and 6.3.3). This was already clear in the examples of Section
6.3.5 and the following examples reinforce the point that MCMC methods
are ideally suited for hierarchical models (see also Gelman et al. (1996) and
Robert and Casella (2004, §10.2.3)).
Example 10.2.14
Consider
x ∼Np(θ, Σ)
and
θ|μ, ξ ∼Np(μ, B(ξ)) ,
where B(ξ) = ξC−Σ. The positive deﬁnite matrix C is ﬁxed and ξ varies on
the half-line [λmax(C−1Σ), +∞), where λmax(A) denotes the largest eigen-
value of A. This representation of the posterior covariance matrix simpliﬁes

10.2
Hierarchical Bayes analysis
469
 
 
0
2000
4000
6000
8000 10000
1.60
1.70
1.80
1.90
 
 
 
 
0
2000
4000
6000
8000 10000
-2.90 -2.80 -2.70 -2.60
 
 
 
 
0
2000
4000
6000
8000 10000
0.40
0.50
0.60
0.70
 
 
 
 
0
2000
4000
6000
8000 10000
1.7 1.8 1.9 2.0 2.1
 
 
Figure 10.2.1. Convergence curves for μθ (top left), μδ (top right), μP (bott.
left) and μD in the experiment of Example 10.2.3. The dotted curves represent
the partial Rao–Blackwellized averages and are almost indistinguishable from the
standard averages. (Source: Robert and Reber (1999).)
the computations while providing robust estimators. For instance, a second
level modeling on (μ, ξ) may involve a noninformative distribution. How-
ever, a usual assumption is that μ = Y β for β ∈IRk, with a given regressor
Y such that Y tCY is full rank, with a noninformative distribution on β. It
can then be shown that m(x) < +∞if p > 2 + k (see Exercise 10.12).
∥
Example 10.2.15 (Example 10.2.3 continued)
Since the full con-
ditional distributions correspond to standard distributions (see Exercise
10.14), the Gibbs sampler applies in this setting. Figure 10.2.4 provides
the convergence graphs of the posterior expectations of the four means,
against the number of iterations k, for both the partial average and the
Rao-Blackwellized estimator (see Section 6.3.4). Since both quantities con-
verge to the same Bayes estimator, the strong similarity of both curves is a
partial assessment of convergence, suggesting that 10,000 iterations of the
Gibbs sampler could be suﬃcient to ensure stability.
Because we wish to assess the eﬀects of the intoxication, and of both
treatments, we focus here on the comparisons of μδ, μD, μP , and μD −μP
with 0. Table 10.2.4 provides posterior probabilities that the eﬀects are
signiﬁcant, that is, whether 0 > μδ, μD > 0, etc., as well as the conﬁdence
intervals, both approximated from the Gibbs samples represented in Figure
10.2.15. This allows us to conclude that intoxication, drug and placebo
eﬀects are signiﬁcant, although to a lesser degree for the placebo,. Also, we
see that the drug eﬀect signiﬁcantly diﬀers from the placebo eﬀect.
∥

470
Hierarchical and Empirical Bayes Extensions
10
1.0
1.2
1.4
1.6
1.8
2.0
2.2
2.4
0 20 40 60 80100120
control
 
-4.0
-3.5
-3.0
-2.5
-2.0
-1.5
0 20 40 60 80100 140
intoxication
 
-1
0
1
2
0 50100150200250300
placebo
 
0
1
2
3
0 50 100150200250
drug
 
Figure 10.2.2. Histograms of the Gibbs samples for μθ, μδ, μP and μD in the
experiment of Example 10.2.3. (Source: Robert and Reber (1999).)
Table 10.2.1. Posterior probabilities of signiﬁcance and 95% conﬁdence intervals
for the mean eﬀects.
μδ
μD
μP
μD −μP
Probability
1.00
0.9998
0.94
0.985
Conﬁdence
[-3.48,-2.17]
[0.94,2.50]
[-0.17,1.24]
[0.14,2.20]
10.2.5 Hierarchical extensions for the normal model
In this section, as well as in Section 10.3, we consider the special case of
the normal distribution,
x ∼Np(θ, Σ)
because it leads to (partly) closed-form expressions and analytic results. As
in Lindley and Smith (1972), Smith (1973) and Berger (1985a), we also use
a ﬁrst-level conjugate distribution θ ∼Np(μ, Σπ), since this choice allows
for an easier decomposition of the estimators.
Lemma 10.2.16
In the conjugate normal model, the hierarchical Bayes
estimator is
δπ(x) = IEπ2(μ,Σπ|x)[δ(x|μ, Σπ)],
with
δ(x|μ, Σπ)
=
x −ΣW(x −μ),
W
=
(Σ + Σπ)−1,
π2(μ, Σπ|x)
∝
(det W)1/2 exp{−(x −μ)tW(x −μ)/2}π2(μ, Σπ).

10.2
Hierarchical Bayes analysis
471
The proof is a direct consequence of Lemma 10.2.9, and of the fact that
the marginal m1(x|μ, Σπ) is a normal distribution Np(μ, W −1).
Example 10.2.17 (Example 10.2.14 continued) The choice of a ﬂat
prior distribution on β leads to a closed-form expression for δπ(x). In fact,
there exists a function hk (see Exercise 10.19) such that
δπ(x) = x −hp−k−2(||x||2
∗)ΣC−1(x −Px),
with
P
=
Y (Y tC−1Y )−1Y tC−1,
||x||2
∗
=
xC−1(Ip −P)x.
Notice that Px is the orthogonal projection of x on the subspace H = {μ =
Y β, β ∈IRk} according to the metrics deﬁned by C−1. The estimator δπ
is thus a weighted sum of x and of this projection. Therefore, δπ takes
the prior information into account in an adaptive way, depending on the
distance ||x||∗of x to H.
∥
Example 10.2.18
Consider the exchangeable hierarchical model:
x|θ
∼
Np(θ, σ2
1Ip),
θ|ξ
∼
Np(ξ1, σ2
πIp),
ξ
∼
N(ξ0, τ2),
where 1 = (1, . . . , 1)t ∈IRp. In this case,
δ(x|ξ, σπ) = x −
σ2
1
σ2
1 + σ2π
(x −ξ1),
π2(ξ, σ2
π|x)
∝
(σ2
1 + σ2
π)−p/2 exp{−∥x −ξ1∥2
2(σ2
1 + σ2π)}e−(ξ−ξ0)2/2τ 2π2(σ2
π)
∝
π2(σ2
π)
(σ2
1 + σ2π)p/2 exp

−p(¯x −ξ)2
2(σ2
1 + σ2π) −
s2
2(σ2
1 + σ2π) −(ξ −ξ0)2
2τ 2

with s2 = 
i(xi −¯x)2. Therefore, π2(ξ|σ2
π, x) is the normal distribution
N(μ(x, σ2
π), Vπ(σ2
π)), where
μ(x, σ2
π) = ¯x −
σ2
1 + σ2
π
σ2
1 + σ2π + pτ 2 (¯x −ξ0),
Vπ(σ2
π) =
τ 2(σ2
1 + σ2
π)
σ2
1 + σ2π + pτ 2 .
Then
δπ(x) = IEπ2(σ2
π|x)

x −
σ2
1
σ2
1 + σ2π
(x −¯x1) −
σ2
1 + σ2
π
σ2
1 + σ2π + pτ 2 (¯x −ξ0)1

and
π2(σ2
π|x) ∝
τ exp −1
2

s2
σ2
1 + σ2π
+
p(¯x −ξ0)2
pτ 2 + σ2
1 + σ2π

(σ2
1 + σ2π)(p−1)/2(σ2
1 + σ2π + pτ 2)1/2
π2(σ2
π).
(10.2.5)

472
Hierarchical and Empirical Bayes Extensions
10
Berger (1985a, pp. 184-185) provides a detailed proof of this result, as well
as the corresponding expression for the posterior variance of θ.
Notice the particular form of the hierarchical Bayes estimator
δπ(x)
=
x −IEπ2(σ2
π|x)

σ2
1
σ2
1 + σ2π

(x −¯x1)
−IEπ2(σ2
π|x)

σ2
1 + σ2
π
σ2
1 + σ2π + pτ 2

(¯x −ξ0)1.
(10.2.6)
This means that the two hierarchical levels induce two diﬀerent types of
shrinkage in the Bayes estimator. Firstly, the exchangeability assumption
justiﬁes the second term, (x−¯x1), which is shrinking the observation toward
the common mean ¯x; this would be the estimator to use in the case of an
exact relation between the parameters of the model. Similarly, the third
term originates from the assumption that the common mean varies around
ξ0.
In the event that the information about ξ0 is unreliable, a noninformative
distribution can be proposed for the second level, that is, π2(σ2
π) = 1 and
τ2 = +∞. Then, for p ≥4,
δπ(x)
=
x −IEπ2(σ2
π|x)

σ2
1
σ2
1 + σ2π

(x −¯x1)
=
x −hp−2(∥x −¯x1∥2)(x −¯x1)
(10.2.7)
and
π2(σ2
π|x) ∝(σ2
1 + σ2
π)−(p−1)/2 exp

−
s2
2(σ2
1 + σ2π)

,
(10.2.8)
the function hk being introduced in Example 10.2.7 (see also Exercise
10.19). It can be veriﬁed that (10.2.7) and (10.2.8) are derived from (10.2.5)
and (10.2.6) when τ2 goes to +∞, and that (10.2.8) only corresponds to
a proper distribution when p ≥4. The usefulness of the exchangeability
assumption in dimension 3 relies on an additional amount of information,
that is, a prior information on the location of the common mean ξ. This
constraint agrees with frequentist results on the minimaxity of (10.2.7),
which only holds for p ≥4 (Brown (1988)).
Notice that, if σ1 is also unknown, with (possibly noninformative) prior
distribution π0, the quantities (10.2.6) and (10.2.7) are still valid pro-
vided the expectations are considered with respect to the posterior dis-
tribution π(σ2
1, σ2
π|x). Similarly, if ξ is distributed according to a Student’s
t-distribution T (α, ξ0, τ2) instead of a normal distribution, we showed in
Example 10.2.2 that this distribution can be decomposed as a mixture of a
normal distribution N(ξ0, τ2/z) by a gamma distribution G(α/2, α/2) on z.
Therefore, δπ can be derived from the expressions (10.2.6) and (10.2.7) by
integrating with respect to z. See Angers (1987, 1992) for a more detailed
treatment of a prior modeling by Student’s t-distributions.
∥

10.2
Hierarchical Bayes analysis
473
Example 10.2.19 (Example 10.2.7 continued) In the setting of the
usual regression model, an exchangeability assumption on the parameters
βi (1 ≤i ≤p) leads to estimators similar to the ones derived above. When
βi ∼N(ξ, σ2
π)
and
π(ξ) = 1,
an analysis similar to Example 10.2.18 was conducted by Lindley and Smith
(1972) and provided the estimator
δπ(y) =

Ip + σ2
σ2π
(XtX)−1(Ip −p−1Jp)
−1
ˆβ,
where ˆβ is the least-squares estimator ˆβ = (XtX)−1Xty and Jp is the
(p × p) matrix made of 1. The analogy with the above example is more
striking when δπ is written in the form
δπ(y) = ¯β1 +

Ip + σ2
σ2π
(XtX)−1(Ip −p−1Jp)
−1
(ˆβ −¯β1)
(since (Ip−p−1Jp)¯β1 = 0) because the Bayes estimator is shrinking toward
the common mean ¯β (in a matricial sense). Notice that it can also be written
as
δπ(y) =

XtX + σ2
σ2π
(Ip −p−1Jp)
−1
Xty.
This expression points out how the exchangeability assumption alleviates
the numerical and statistical problems caused by near-collinearities in the
columns of X. Indeed, the matrix
σ2
σ2π
(Ip −p−1Jp)
plays the role of stabilizer in the estimator. If, in the second-level prior, we
consider instead ξ = 0, the Bayes estimator is then (see Exercise 10.23)
δπ(y)
=

Ip + σ2
σ2π
(XtX)−1
−1
ˆβ
=

XtX + σ2
σ2π
Ip
−1
Xty.
These estimators are called ridge estimators and have been introduced by
Hoerl and Kennard (1970) as a remedy to multicollinearity problems in
the matrix XtX, that is, when two (or more) of the regressors are almost
collinear. The matricial factor
+
Ip + k(XtX)−1,−1
stabilizes the least-squares estimator when some eigenvalues of XtX are
close to 0 (see also Lindley and Smith (1972) and Goldstein and Smith
(1974)). These estimators have been generalized later by considering a ma-
tricial factor of the form
+
Ip + h(y)(XtX)−1,−1 ,

474
Hierarchical and Empirical Bayes Extensions
10
which may correspond to the case when σ2
π is unknown, with prior distri-
bution π2(σ2
π), since the Bayes estimator is then
δπ(y) = IEπ2(σ2
π|y)

Ip + σ2
σ2π
(XtX)−1
−1
ˆβ.
From a classical point of view, it appears that the imperatives of a reduc-
tion of multicollinearity and of minimaxity are contradictory, since Casella
(1980, 1985b) has shown that necessary minimaxity conditions for the ridge
estimators cannot accord with a stabilizing inﬂuence of these estimators.
Robert (1988) exhibits the same phenomenon for other classes of shrink-
age estimators and points out that the antagonism owes to the unidimen-
sionality of the multicollinearity problem, which explains why a uniform
improvement over ˆβ is impossible.
∥
10.3 Optimality of hierarchical Bayes estimators
From2 a general point of view, since hierarchical Bayes estimators cannot
really be distinguished from the usual Bayes estimators, these estimators
are not more, and not less, admissible than the Bayes estimators derived in
the previous chapters. For instance, the necessary and suﬃcient conditions
obtained in Chapter 8 also apply in the case of hierarchical Bayes esti-
mators. Similarly, the invariance aspects of Chapter 9 ignore the possibly
hierarchical structure of prior distributions.
On the other hand, we will see in a particular case that it is possible to
derive a general minimaxity condition taking advantage of the speciﬁcity of
hierarchical Bayes estimators, since this condition involves the second-level
prior distributions. Such results point out the robustifying aspect of the
hierarchical Bayes approach, which assigns the more subjective aspects of
prior modeling to higher levels, and thus provides an intermediary position
between a straightforward Bayesian analysis and frequentist imperatives.
Consider again the normal model, x ∼Np(θ, Σ) where Σ is known. As
in Section 10.2.5, the ﬁrst-level prior distribution on θ is conjugate, θ ∼
Np(μ, Σπ). The prior distribution π2 of the hyperparameters μ, Σπ can be
decomposed as follows:
π2(μ, Σπ) = π1
2(Σπ|μ)π2
2(μ).
In this case,
m(x) =

IRp m(x|μ)π2
2(μ) dμ,
with
m(x|μ) =

f(x|θ)π1(θ|μ, Σπ)π1
2(Σπ|μ) dθ dΣπ.
2 This Section can be omitted in a ﬁrst reading because it deals with the minimaxity
of a particular class of hierarchical Bayes estimators in the normal case. Its purpose
is to illustrate the increased robustness brought about by hierarchical modeling.

10.3
Optimality of hierarchical Bayes estimators
475
Moreover, the Bayes estimator
δπ(x) = x + Σ∇log m(x)
(10.3.1)
can be written
δπ(x) =

δ(x|μ)π2
2(μ|x) dμ,
with
δ(x|μ)
=
x + Σ∇log m(x|μ),
π2
2(μ|x)
=
m(x|μ)π2
2(μ)
m(x)
.
These conditional decompositions will be used below.
Consider Q, a (p×p) symmetric positive-deﬁnite matrix associated with
the quadratic loss
LQ(θ, δ) = (θ −δ)tQ(θ −δ).
(10.3.2)
An estimator δ is minimax for the loss (10.3.2) if it satisﬁes
R(θ, δ) = IEθ[LQ(θ, δ(x))] ≤tr(ΣQ),
since tr(ΣQ) is the minimax risk of δ0(x) = x. The method of the unbiased
estimator of the risk has been developed by Stein (1973, 1981) to derive
suﬃcient minimaxity conditions. (See Brown (1988) and Rukhin (1995)
for detailed reviews of this method.) It consists of obtaining a diﬀerential
operator D, independent of θ, such that
R(θ, δ) = IEθ[Dδ(x)],
for every parameter θ and every estimator δ. This technique indeed gives a
suﬃcient minimaxity condition of the form Dδ(x) ≤tr(QΣ) (see Exercise
2.56). In the particular case of (10.3.1), the diﬀerential operator is provided
by the following result (Berger and Robert (1990)).
Lemma 10.3.1
If m(x) satisﬁes the three conditions
(1) IEθ∥∇log m(x)∥2 < +∞;
(2) IEθ

∂2m(x)
∂xi∂xj
-
m(x)
 < +∞;
and (1 ≤i ≤p)
(3)
lim
|xi|→+∞
∇log m(x)
 exp{−(1/2)(x −θ)tΣ−1(x −θ)} = 0,
the unbiased estimator of the risk of δπ is given by
Dδπ(x) = tr(QΣ)
+
2
m(x)tr(Hm(x) ˜Q) −(∇log m(x))t ˜Q(∇log m(x)),
where
˜Q = ΣQΣ,
Hm(x) =
∂2m(x)
∂xi∂xj

.

476
Hierarchical and Empirical Bayes Extensions
10
This unbiased estimator of the risk then leads to a suﬃcient minimaxity
condition,
2
m(x)tr(Hm(x) ˜Q) −(∇log m(x))t ˜Q(∇log m(x)) ≤0.
We denote by div the divergence operator, that is,
divf(x) =
n

i=1
∂fi
∂xi
(x),
for a diﬀerentiable function f from IRn to IRn.
Corollary 10.3.2
If m satisﬁes the conditions of Lemma 10.3.1 and if
div

˜Q∇
1
m(x)

≤0,
(10.3.3)
δπ is minimax.
Proof.
It is suﬃcient to consider the development of div( ˜Q∇
1
m(x)) to
obtain
div( ˜Q∇
1
m(x)) = 1
2div
!
˜Q ∇m(x)
1
m(x)
"
=
1
2
1
m(x)
div( ˜Q∇m(x)) −1
4
!
∇m(x)
m(x)
1
m(x)
"t
˜Q∇m(x)
=
1
m(x)
4

2
m(x)tr(Hm(x) ˜Q) −∇log m(x)t ˜Q∇log m(x)

and derive the additional term in Dδπ(x).
22
In the particular case when Σ = Q = Ip, the condition of Corollary 10.3.2
can be written more simply as a condition on the Laplacian of m(x)1/2
because it is
Δ
1
m(x) =
n

i=1
∂2
∂x2
i
(
1
m(x)) ≤0
(
1
m(x) is then said to be superharmonic). The veriﬁcation of this condi-
tion is generally quite complicated. A more explicit minimaxity condition
can be derived from Corollary 10.3.2 by conditioning on μ.
Lemma 10.3.3
The estimator δπ is minimax if
div

˜Q∇m(x|μ)

≤0.
(10.3.4)
Proof. In fact,
div( ˜Q∇m(x)) =

div

˜Q∇m(x|μ)

π2
2(μ) dμ
and (10.3.4) implies (10.3.3).
22

10.3
Optimality of hierarchical Bayes estimators
477
Therefore, if ˜Q = Ip and m(x|μ) is superharmonic, the corresponding
hierarchical Bayes estimator is minimax. This result may appear to be
trivial in its proof and statement, but it is actually quite general. In fact, it
provides a necessary and suﬃcient condition of minimaxity that does not
depend on π2
2(μ), and thus allows for every possible modeling on the hyper-
parameter μ. From a subjective point of view, to have complete freedom
of choice on the prior distribution of μ is more much important than the
alternative choice on Σπ, since it is usually easier to get information on μ
than on Σπ. The following example shows, moreover, that the condition
(10.3.4) is satisﬁed by a large class of distributions π1
2.
Example 10.3.4 (Example 10.2.14 continued)
Consider again the
case when Σπ = ξC −Σ and Q = Σ−1CΣ−1 (therefore ˜Q = C). It follows
from Lemma 10.2.11 that
m(x|μ) ∝
 ∞
0
ξ−p/2 exp

−(x −μ)tC−1(x −μ)
2ξ

π1
2(ξ|μ) dξ.
Therefore,
div

˜Q∇m(x|μ)

∝
 ∞
0

−p
ξ + (x −μ)tC−1(x −μ)
ξ2

×e−(x−μ)tC−1(x−μ)/2ξξ−p/2π1
2(ξ|μ) dξ ,
and (10.3.4) is equivalent to
ψ(a) =
 ∞
0
(2a −pξ)ξ−(p+4)/2e−a/ξπ1
2(ξ|μ) dξ ≤0,
∀a ≥0.
If π1
2 is a.e. diﬀerentiable, an integration by parts gives
ψ(a) = −2e−a/ξ0ξ−p/2
0
π′
2(ξ0|μ) −
 +∞
ξ0
ξ−p/2e−a/ξπ1
2(ξ|μ) dξ,
where ξ0 = inf(supp(π1
2)) and π′
2 is the derivative of π1
2. This expression
implies:
Proposition 10.3.5
If, for every μ ∈IRp, π1
2(ξ|μ) is nondecreasing, δπ
is minimax for every prior distribution π2
2.
Therefore, if π1
2(ξ|μ) = 1 for ξ0 ≤ξ when λmax(C−1Σ) ≤ξ0, the corre-
sponding Bayes estimator is minimax.
∥
The above example can be extended to the case when θ ∼Np(μ, σ2
πΣ)
and when π1
2(σ2
π|μ) is increasing (C = Σ and ξ = σ2
π −1). This class
obviously fails to include all hierarchical estimators or all minimax estima-
tors, but it is large enough to contain the minimax estimators proposed
by Strawderman (1971) and Berger (1976, 1980a), some of them being,
moreover, admissible (see also Kubokawa (1991) and Exercise 10.36).
Notice that Proposition 10.3.5 suggests the use of unnatural prior dis-
tributions: actually, it seems diﬃcult to argue in favor of an increasing

478
Hierarchical and Empirical Bayes Extensions
10
distribution on the variance, on a subjective or noninformative basis. On
the contrary, the prior distributions are, in general, decreasing for large
σ2
π’s. This is, for instance, the case for the Jeﬀreys noninformative distri-
bution, π(σ2
π) = 1/σ2
π. Therefore, this result stresses indirectly the artiﬁcial
aspect of the notion of minimaxity: to similarly weight a posteriori all the
possible values of the parameter is equivalent to favoring a priori the more
unlikely (or the least favorable) values.
The example below illustrates the advantage of the hierarchical Bayes
modeling from a minimax viewpoint, even when the ﬁrst-level distribution
is more rudimentary. It also exhibits a minimaxity robustness property, in
the sense that minimaxity does not depend so much on the normality of the
prior distribution as on its spherical symmetry. This result thus appears as
a Bayesian counterpart to the frequentist results of Cellier et al. (1989).
Example 10.3.6 Consider x ∼Np(θ, Ip). The mean θ is estimated under
quadratic loss. Instead of assuming a conjugate ﬁrst-level distribution, we
propose the uniform distribution on the sphere of radius c,
π1(θ|c) ∝II{||θ||2=c},
thus assuming only spherical symmetry for the overall prior distribution.
The second-level prior distribution π2(c) is a gamma distribution, G(α, β).
The Bayes estimator is then (see Robert et al. (1990))
δπ(x) = 2α
p
1
1 + 2β
1F1(α + 1; (p + 2)/2; ||x||2/(2 + 4β))
1F1(α; p/2; ||x||2/(2 + 4β))
x,
where 1F1 is the conﬂuent hypergeometric function. When α < 1 and β = 0,
we get
δπ(x) = 2α
p
1F1(α + 1; (p + 2)/2; ||x||2/2)
1F1(α; p/2; ||x||2/2)
x,
which is a minimax and admissible estimator (see Alam (1973)).
∥
10.4 The empirical Bayes alternative
The method we examine in the remainder of this chapter does not follow
from the Bayesian principles,3 since it approximates the prior distribution
by frequentist methods when the prior information is too vague. We still
consider it in this book for several reasons
(i) it can be perceived as a dual method of the hierarchical Bayes analysis
presented above;
(ii) it is asymptotically equivalent to the Bayesian approach;
(iii) it is usually classiﬁed as Bayesian by frequentists and practitioners; and
3 The appellation empirical Bayes is doubly defective because ﬁrstly, the method is not
Bayesian and, secondly, genuine Bayesian methods are empirical, since they are based
on the data!

10.4
The empirical Bayes alternative
479
(iv) it may be an acceptable approximation in problems for which a genuine
Bayes modeling is too complicated or too costly.
We will see how the empirical Bayes analysis occupies an intermediate
position between the classical and Bayesian methods, and also that the
hierarchical alternative is often preferable. Notice that this section is only
a short introduction to the empirical Bayes approach. See Morris (1983b),
Berger (1985a), Maritz and Lwin (1989) or Carlin and Louis (2000a,b) for
more extensive treatments of the topic.
10.4.1 Nonparametric empirical Bayes
Introduced by Robbins (1951, 1955, 1964, 1983), the empirical Bayes per-
spective can be stated as follows. Given (n + 1) independent observations
x1, . . . , xn+1 with densities f(xi|θi), the problem is to draw an inference on
θn+1, under the additional assumption that the θi’s have all been generated
according to the same unknown prior distribution g. From a Bayesian point
of view, this means that the sampling distribution is known, but the prior
distribution is not. The marginal distribution,
fg(x) =

f(x|θ)g(θ) dθ,
(10.4.1)
can then be used to recover the distribution g from the observations, since
x1, . . . , xn can be considered an i.i.d. sample from fg. Deriving an approx-
imation ˆgn in this manner, we can use it as a substitute for the true prior
distribution, and propose the plug-in approximation to the posterior dis-
tribution
˜π(θn+1|xn+1) ∝f(xn+1|θn+1)ˆgn(θn+1).
(10.4.2)
Obviously, this derivation is not Bayesian, although it relies on the Bayes
formula (10.4.2), and can also correspond to a classical modeling. A Bayesian
approach, arguing from the ignorance on g, would index this distribution
by a hyperparameter λ and would thus represent ignorance by a second-
level prior distribution, π2(λ).4 Deely and Lindley (1981) compare the two
approaches in the case of a Poisson distribution.
The initial approach of Robbins (1955) is essentially nonparametric and
uses the observations x1, . . . , xn+1 to estimate fg. (In the general case,
the marginal density fg can be estimated by the kernel method; see, e.g.,
Devroye and Gy¨orﬁ(1985).)
Example 10.4.1 Consider xi distributed according to P(θi) (i = 1, . . . , n).
If pk(x1, . . . , xn) is the number of observations equal to k, k ∈IN, pk(x1, . . . ,
xn) gives an estimation of the marginal distribution,
fg(k) =
 +∞
0
e−θ θk
k! g(θ) dθ.
4 Indexing by λ is not formally restrictive, as shown in Exercise 1.2.

480
Hierarchical and Empirical Bayes Extensions
10
If xn+1 ∼P(θn+1) and θn+1 is estimated under quadratic loss, the Bayes
estimator is
δg(xn+1)
=
IEg[θ|xn+1] =
 +∞
0
e−θθxn+1+1g(θ) dθ
 +∞
0
e−θθxn+1g(θ) dθ
=
fg(xn+1 + 1)
fg(xn+1)
(xn+1 + 1).
Therefore, the empirical Bayes approximation of δg is
δEB(xn+1) = pxn+1+1(x1, . . . , xn)
pxn+1(x1, . . . , xn) + 1(xn+1 + 1),
(10.4.3)
where fg is replaced by its approximation.
∥
Several problems can be identiﬁed in this method.
(a) To use nonparametric estimation, for instance of the prior density, to
initiate a parametric estimation procedure seems to be suboptimal be-
cause the errors made in the nonparametric estimation step are always
more diﬃcult to assess. For instance, in the above example, if the nu-
merator of (10.4.3) is null, the estimator is null.
(b) More generally, nonparametric estimates of mixing densities g in (10.4.1)
by maximum likelihood techniques usually are rudimentary, since they
correspond to distributions with ﬁnite support (see Laird (1978), B¨ohning
(1999), or Carlin and Louis (2000a)).5 Such priors are hardly accept-
able from a Bayesian point of view.
(c) Functional relations between the mean (or any other quantity of inter-
est) and the marginal distribution, as in Example 10.4.1, are quite rare.
When such a relation does not exist, the derivation of an estimator of
g is generally too complicated to guarantee the resulting estimators be
good approximations of the true Bayes estimators.
(d) The approximation is actually justiﬁed for large sample sizes only,
that is, when the estimator of the marginal distribution, ˆf n
g , provides
an acceptable approximation of the marginal distribution. Otherwise,
as shown by Example 10.4.1, ˆf n
g varies too widely and needs to be
smoothed to be of any use (see Maritz and Lwin (1989)).
(e) The assumption that many identical and independent problems are
available about the same prior distribution is a strong one, and can fail
to be satisﬁed in practice. Therefore, a single sample, even when very
large, cannot lead to the estimator of fg because it corresponds to an
unique observation of θ. This criticism remains valid for the parametric
approach (see, for instance, Proposition 10.4.5).
For these reasons, we shall not proceed any further in the study of the non-
parametric empirical Bayes analysis; we consider only a restricted version
called parametric empirical Bayes by Morris (1983b).
5 This is also the case with kernel or Dirichlet process prior approaches.

10.4
The empirical Bayes alternative
481
10.4.2 Parametric empirical Bayes
One appeal of the empirical Bayes techniques is to provide approximations
in noninformative settings. We showed in the previous chapters that the
Bayesian approach produces an eﬃcient tool to obtain frequentist optimal
procedures, besides providing a uniﬁed technique of statistical inference.
The empirical Bayes analysis can then be perceived as a practical approx-
imation of this tool.
In exponential family settings, the prior distribution being unavailable,
a simple choice is to take a conjugate prior associated with f(x|θ), π(θ|λ).
While the hierarchical approach introduces an additional distribution on
the hyperparameters λ, the empirical Bayes analysis proposes to estimate
these hyperparameters from the marginal distribution
m(x|λ) =

Θ
f(x|θ)π(θ|λ) dθ
by ˆλ(x) and to use π(θ|ˆλ(x), x) as a pseudo-posterior distribution. This
method then appears as the parametric version of the original approach of
Robbins (1955).
A defect with the empirical Bayes perspective is that it relies on frequen-
tist methods to estimate the hyperparameters of m(x|λ), although Bayesian
techniques could be used as well, as shown in Note 10.7.2. Therefore, a wide
range of options is available: for instance, the estimator of λ can be derived
by the moment method or the maximum likelihood method. The corre-
sponding arbitrariness of empirical Bayes analysis is the major ﬂaw of this
theory, since it prohibits a decision-theoretic treatment of the empirical
Bayes estimators, and often appears as a posterior justiﬁcation of existing
estimators, as shown in Section 10.5. The most common approach is to use
maximum likelihood estimators, for practical and theoretical reasons, in
particular because of the proximity of the maximum likelihood estimation
to the Bayesian paradigm. An additional justiﬁcation of this choice is given
below in the particular case of the estimation of the natural parameter of
an exponential family under quadratic loss.
Lemma 10.4.2
Consider
x ∼f(x|θ) = eθ·x−ψ(θ)h(x),
x ∈IRk.
If θ is distributed according to π(θ|λ), λ ∈IRp, and ˆλ(x) is the solution of
the likelihood equations associated with m(x|λ), the empirical Bayes esti-
mator of θ satisﬁes
δEB(x)
=
(∇log m(x|λ))

λ=ˆλ(x) −∇log h(x)
=
∇[log m(x|ˆλ(x))] −∇log h(x).
Proof.
In fact,
∇log m(x|ˆλ(x)) = (∇log m(x|λ))

λ=ˆλ(x) + ∇xˆλ(x)∇λm(x|λ)

λ=ˆλ(x),

482
Hierarchical and Empirical Bayes Extensions
10
where ∇λm(x|λ) is the vector with components
∂m(x|λ)
∂λi
(1 ≤i ≤p) ,
and ∇xˆλ(x) is the (k × p) matrix with components
∂ˆλi(x)
∂xj
(1 ≤i ≤p, 1 ≤j ≤k) .
By deﬁnition of ˆλ(x), the second term is null.
22
Therefore, a regular Bayesian derivation using the approximate posterior
distribution π(θ|ˆλ(x)) gives the same result as the genuine empirical Bayes
approach, where λ is replaced by ˆλ(x). This justiﬁcation is obviously quite
limited, since it only works for the posterior mean of the natural parameter
in exponential families.
Example 10.4.3 (Example 10.4.1 continued) Consider the case when
π(θ|λ) is an exponential distribution Exp(λ). Then
m(xi|λ)
=
 +∞
0
e−θ θxi
xi! λe−θλdθ
=
λ
(λ + 1)xi+1 =

1
λ + 1
xi
λ
λ + 1,
and xi|λ ∼Geo(λ/λ+1). The maximum likelihood estimator of λ is ˆλ(x) =
1/¯x and the empirical Bayes estimator of θn+1 is
δEB(xn+1) = xn+1 + 1
ˆλ + 1
=
¯x
¯x + 1(xn+1 + 1),
the average ¯x being taken on the n ﬁrst observations.
∥
Example 10.4.4
Consider x1, . . . , xn, n independent observations from
B(m, pi). Casella (1985a) (see also Morisson (1979)) applies this model to
the intentions of buying a new car in the coming year. The assumption is
that the parameters pi (1 ≤i ≤n) are distributed according to the same
conjugate prior distribution
pi ∼B(α, β).
The corresponding Bayes estimator of pi is
δπ
i (xi) =
α + β
α + β + 1
α
α + β +

1 −
α + β
α + β + 1
 xi
m
and the marginal distribution of xi is called beta-binomial,
P(xi = k|α, β) = B(k + α, m −k + β)
B(α, β)
.

10.4
The empirical Bayes alternative
483
as in Example 10.2.13. It is shown in Kendall and Stuart (1979) that, for
this marginal distribution,
IE(xi|m) =
α
α + β ,
var(xi|m) = 1
m
αβ
(α + β)2
α + β + m
α + β + 1 .
When α and β are estimated by the method of moments, the resulting
empirical Bayes estimator of pi is
γEB
i
(x1, . . . , xn) = ˆα + (xi/m)
ˆα + ˆβ + 1
.
(Exercise 10.23 provides the data of Morisson (1979).)
∥
Section 10.5 indicates how the Stein eﬀect is strongly related to the
empirical Bayes approach and how the latter can provide estimators that
perform well for point estimation, as well as for tests and conﬁdence regions.
The following result shows, on the contrary, why empirical Bayes tests are
of limited interest for a single sample.
Proposition 10.4.5 Consider the test of H0 : θ = θ0 against H1 : θ = θ1
based on a sample x1, . . . , xn, i.i.d. f(x|θ). An empirical Bayes approach
gives the likelihood ratio test procedure
ϕ(x) =

1
if .n
i=1 f(xi|θ0) > .n
i=1 f(xi|θ1),
0
otherwise,
(10.4.4)
for every conﬁdence level.
Proof.
In this setting, the unknown parameters are reduced to π0, the
prior probability of H0. The marginal distribution of x is then
m(x|π0) = π0
n

i=1
f(xi|θ0) + (1 −π0)
n

i=1
f(xi|θ1)
and gives the following maximum likelihood estimator of π0:
ˆπ0(x1, . . . , xn) =

1
if .n
i=1 f(xi|θ0) > .n
i=1 f(xi|θ1),
0
otherwise.
The Bayesian answer being
ϕπ(x1, . . . , xn) =

1
if P(θ = θ0|x1, . . . , xn, π0) > α,
0
otherwise,
the posterior probability of H0 is
P(θ = θ0|x1, . . . , xn, ˆπ0) =
ˆπ0
.n
i=1 f(xi|θ0)
ˆπ0
.n
i=1 f(xi|θ0) + (1 −ˆπ0) .n
i=1 f(xi|θ1)
and (10.4.4) follows.
22
When several testing problems are considered simultaneously, this ex-
treme behavior of the empirical Bayes tests disappears (see Maritz and

484
Hierarchical and Empirical Bayes Extensions
10
Lwin (1989)). However, it is rather rare to have to test simultaneously
hypotheses on parameters from the same distribution, and the practical
interest of the empirical Bayes approach for tests is thus quite limited. We
consider the estimation of the conﬁdence regions in Section 10.5, in relation
to the Stein eﬀect. For an alternative review, see Laird and Louis (1987)
or Carlin and Gelfand (1990).
Notice in conclusion that a reﬁnement of the empirical Bayes approach
is to consider instead mixtures of conjugate distributions, since they also
constitute a conjugate family (see Lemma 3.4.2). If xi ∼f(xi|θi) and
θi ∼
n

j=1
pjπ(θi|λj),
the marginal distribution of xi is
xi|p, λ ∼
n

j=1
pj

Θ
f(xi|θ)π(θ|λj) dθ.
See Section 6.4 and Note 6.6.6 for details on the Bayesian analysis of this
problem. Maritz and Lwin (1989) consider more particularly the application
to the empirical Bayes analysis. A drawback to this extension is obviously
that it calls for a larger number of hyperparameters, and thus for a larger
number of independent samples, while suﬀering from some of the defects
mentioned before.
Let us stress again that the legitimacy of the empirical Bayes methods
is asymptotic (see Deely and Lindley (1981)). Their popularity owes to the
good frequentist properties of some resulting estimators, and also to the
simpliﬁcation they brought to the treatment of complex problems, com-
pared with a hierarchical Bayes analysis. (See, for instance, Carter and
Rolph (1974) or Hui and Berger (1983).) For ﬁnite sample size problems,
the empirical Bayes methods are only approximations of the exact Bayesian
methods and cannot claim the same coherence. In particular, it is not pos-
sible to draw a full Bayesian inference using π(θ|x, λ(x)), because it is not
a posterior distribution. Moreover, with increasing computational power
(see Chapter 6), the need for empirical approximations to more complex
hierarchical analyses diminishes (see Berger (1985a), Berger and Berliner
(1986), and Berger and Robert (1990)).
10.5 Empirical Bayes justiﬁcations of the Stein eﬀect
The empirical Bayes analysis of the Stein eﬀect described in Note 2.8.2
uniﬁes the diﬀerent occurrences of this paradox, according to which the
simultaneous estimation of independent parameters can lead to a global
improvement in estimation performances, although each component cannot
be improved uniformly. Moreover, this analysis explains the form of the
original James–Stein estimators, and points out that they correspond to

10.5
Empirical Bayes justiﬁcations of the Stein eﬀect
485
the vague prior information that θ is close to 0.
10.5.1 Point estimation
Example 10.5.1 Consider x ∼Np(θ, Ip) and θi ∼N(0, τ2). The marginal
distribution of x is then
x|τ2 ∼Np(0, (1 + τ 2)Ip)
and leads to the following maximum likelihood estimator of τ2,
ˆτ2 =

(||x||2/p) −1
if ||x||2 > p,
0
otherwise.
The corresponding empirical Bayes estimator of θi under quadratic loss is
derived by replacing τ2 by ˆτ 2 in the Bayes estimator,
δEB(x)
=
ˆτ 2x
1 + ˆτ2
=

1 −
p
||x||2
+
x.
(10.5.1)
The estimator (10.5.1) is actually a truncated James–Stein estimator. There-
fore, these estimators can be interpreted as empirical Bayes estimators re-
lated to the vague information that the expectations of the observations
are close to 0. The original James–Stein estimator can also be expressed as
an empirical Bayes estimator, using an alternative frequentist estimation
method. In fact, given the marginal distribution of x, the best unbiased
estimator of 1/(1 + τ2) is (p −2)/||x||2, which leads to
δEB(x) =

1 −p −2
||x||2

x.
(10.5.2)
∥
This example illustrates the weakness in the justiﬁcations of the em-
pirical Bayes approach, which cannot compare the diﬀerent methods used
for estimating the hyperparameters. This lack of ordering is actually char-
acteristic of the whole frequentist approach. The comparison between the
estimators (10.5.1) and (10.5.2) must thus rely on external considerations.
Example 10.5.2 Consider two independent vectors, x ∼Np(θ, σ2Ip) and
y ∼Nq(0, σ2Iq), as for a linear regression. The parameter of interest is the
variance factor σ2, evaluated under the entropic loss,
L(σ2, d) = d
σ2 −log(d/σ2) −1.
Apart from intrinsic considerations (see Section 2.5.4), this loss is often pre-
ferred to the quadratic loss, since it gives the maximum likelihood estimator

486
Hierarchical and Empirical Bayes Extensions
10
||y||2/p + q as the best equivariant estimator6 of σ2. Under this loss, the
Bayes estimator of σ2 is
δπ(x) =
	
IEπ[σ−2|x]

−1 .
(10.5.3)
For the gamma-normal conjugate distribution on (θ, σ2),
θ|σ2 ∼Np(0, τσ2Ip),
σ−2 ∼G(ν/2, β/2) ,
the estimator (10.5.3) is then
δπ(x, y) =
1
p + q + ν
 ||x||2
1 + τ + ||y||2 + β

and maximization of the marginal likelihood (in (τ, ν, β)) leads to the fol-
lowing empirical Bayes estimator (see Kubokawa et al. (1992)):
δEB(x, y) = min
||y||2
q
, ||x||2 + ||y||2
p + q

.
(10.5.4)
Notice the intuitive aspect of this estimator, which uses the additional
information about σ2 contained in x only if ||x||2 is not too large, that is,
if θ is close to 0, as
||x||2 + ||y||2
p + q
is the best scale equivariant estimator of σ2 when θ = 0.
The real interest in this result is that (10.5.4) has been obtained in Brew-
ster and Zidek (1974) as a uniform improvement on the best equivariant
estimator δ⋆(x, y) = ||y||2/q under entropy loss. (See Maatta and Casella
(1990) for an exhaustive review of the diﬀerent perspectives in variance
estimation.)
∥
Morris (1983b) considers the Stein eﬀect more generally than in Example
10.5.1. In fact, he studies the Bayesian model
x|θ
∼
Np(θ, Λ),
θ|β, σ2
π
∼
Np(Zβ, σ2
πIp),
with Λ = diag(λ1, . . . , λp) and Z a (p × q) full rank matrix. The marginal
distribution of x is then
xi|β, σ2
π ∼N(z′
iβ, σ2
π + λi)
and the posterior distribution of θ is
θi|xi, β, σ2
π ∼N ((1 −bi)xi + biz′
iβ, λi(1 −bi)) ,
6 This argument does not justify the use of the entropic loss, since it legitimizes a
posteriori a given estimator, instead of being based on utility considerations and
leading to the determination of an estimator.

10.5
Empirical Bayes justiﬁcations of the Stein eﬀect
487
with bi = λi/(λi + σ2
π). If all the variances λi are identical and equal to σ2,
the best equivariant estimators of β and b are given by
ˆβ = (ZtZ)−1Ztx
and
ˆb = (p −q −2)σ2
s2
,
with s2 = p
i=1(xi −z′
i ˆβ)2. We deduce from these estimators of the hyper-
parameters the corresponding empirical Bayes estimator of θ
δEB(x) = Z ˆβ +
!
1 −(p −q −2)σ2
||x −Z ˆβ||2
"
(x −Z ˆβ),
(10.5.5)
which is of the form of the general Stein estimators.
In the particular case where the means are assumed to be identical (ex-
changeability), the matrix Z reduces to the vector 1 and β is a real number;
the empirical Bayes estimator is then
δEB(x) = ¯x1 +

1 −(p −3)σ2
||x −¯x1||2

(x −¯x1).
It thus provides the Stein estimator that shrinks toward the common mean,
as in Efron and Morris (1975). See Morris (1983b) for an extension to the
case when the variances λi are not identical.
10.5.2 Variance evaluation
As mentioned above, the estimation of the hyperparameters β and σ2
π con-
siderably modiﬁes the behavior of the resulting procedures. If the resulting
point estimators are generally eﬃcient, as shown in the above examples,
the estimation of the posterior variance of π(θ|x, β, b) by the empirical
variance, var(θi|x, ˆβ,ˆb), induces an underestimation of this variance. Thus,
using empirical Bayes analysis to assess the performances of δEB by esti-
mating its quadratic loss (θi −δEB
π )2 as var(θi|x, ˆβ,ˆb) is misleading because
it underrates the error resulting from using δEB.
Morris (1983b) takes into account the additional variability resulting
from the estimation of the hyperparameters through a modiﬁcation of the
estimators. In the exchangeable case, the resulting procedures are
δEB(x)
=
x −˜B(x −¯x1),
V EB
i
(x)
=

σ2 −p −1
p
˜B

+
2
p −3
ˆb(xi −¯x)2,
with
ˆb = p −3
p −1
σ2
σ2 + ˆσ2π
,
ˆσ2
π = max

0, ||x −¯x1||2
p −1
−σ2
π

and
˜B = p −3
p −1 min

1, σ2(p −1)
||x −¯x1||2

.

488
Hierarchical and Empirical Bayes Extensions
10
This last quantity estimates the ratio σ2/(σ2 + σ2
π). However, this modi-
ﬁcation, although more satisfactory, suﬀers from the general drawback of
empirical Bayes inference, namely, that the procedures are usually justiﬁed
by ad-hoc reasons that cannot be extended to a general principle (although
Kass and Steﬀey (1989) provide a partial generalization).
Notice the analogy between the modiﬁed empirical variance V EB
i
and the
hierarchical variance for the same model,
V HB
i
(x) = σ2

1 −p −1
p
IEπ

σ2
σ2 + σ2π
x

+ var

σ2
σ2 + σ2π
x

(xi −¯x)2
(see Berger (1985a)). This resemblance is not coincidental, since this mod-
iﬁcation brings an improvement in the original empirical Bayes approach
by taking advantage of the true Bayesian approach one step further. Ghosh
et al. (1989) and Blattberg and George (1991) provide econometric illus-
trations of the empirical Bayes analysis and the connection with Stein es-
timators in regression models.
10.5.3 Conﬁdence regions
Another aspect of the Stein eﬀect can be interpreted in an empirical Bayes
manner. In the case of recentered conﬁdence regions (see Section 5.5),
Hwang and Casella (1982) have shown that some of these regions allow
for a larger coverage probability than does the usual conﬁdence set for
an identical volume. These sets can also be expressed as empirical HPD
regions.
Example 10.5.3
In Hwang and Casella (1982), the usual conﬁdence re-
gion
C0(x) = {θ; ∥θ −x∥2 ≤cα},
with x ∼Np(θ, Ip), is compared with
Ca(x) = {θ; ∥θ −δa(x)∥2 ≤cα},
where δa(x) = [1 −(a/||x||2)]+x. Hwang and Casella (1982) show that, for
a small enough and p ≥4, the set Ca satisﬁes, for every θ,
Pθ(θ ∈Ca(x)) > Pθ(θ ∈C0(x)) = 1 −α.
Casella and Hwang (1983) also consider recentered regions with a variable
volume
Cv
δ (x) = {θ; ∥θ −δ(x)∥2 ≤v(x)} ,
and they determine δ and v by an empirical Bayes analysis based on an α-
credible HPD region. The center of the region is the James–Stein estimator
δ(x) =

1 −p −2
||x||2
+
x

10.5
Empirical Bayes justiﬁcations of the Stein eﬀect
489
and the radius is provided by
v(x) =
⎧
⎨
⎩

1 −p−2
cα
 /
cα −p log

1 −p−2
cα
0
if ||x||2 < cα,

1 −p−2
||x||2
 /
cα −p log

1 −p−2
||x||2
0
otherwise.
The shape of the variable radius is justiﬁed in terms of a linear loss
L(θ, C) = k vol(C) −IIC(θ),
already presented in Section 5.5 (see Exercise 10.29). This empirical Bayes
conﬁdence region has then at least a conﬁdence level of 1 −α (in the
frequentist sense), except for the smallest values of p.
∥
Example 10.5.4
A usual rejection of recentered conﬁdence regions is
based on the fact that they are useless in practice, since the reported con-
ﬁdence level is still
inf
θ Pθ(θ ∈Ca(x)) = 1 −α = Pθ(θ ∈C0(x)).
In this sense, the usual regions can be argued to be more accurate because
they coincide exactly with the reported conﬁdence level. The actual value
of such conﬁdence levels has already been discussed in Section 5.5 and the
reader is referred to Chapter 5 for criticisms of the artiﬁcial aspect of the
notion of conﬁdence levels. An alternative answer is also oﬀered at the end
of Chapter 5. It is to propose a conditional conﬁdence level, γ(x), which is
more adapted to the recentered region Ca(x), and to evaluate it under the
quadratic loss
(γ(x) −IICa(x))2.
(10.5.6)
For the model presented in Example 10.5.2, George and Casella (1994) pro-
pose an empirical Bayes solution to this evaluation problem for a recentered
region of the form
CEB(x) = {θ; ∥θ −(1 −ˆb)x∥2 ≤c}
and a conﬁdence report
γEB(x) = P(χ2
p ≤c/(1 −ˆb)).
In fact, if θ ∼Np(0, τ2Ip), the Bayesian answer would be
γπ(x)
=
P π(θ ∈CB(x)|x)
=
P π(||θ −(1 −b)x||2 ≤c|x)
=
P(χ2
p ≤c/(1 −b)),
since θ|x ∼Np((1 −b)x, (1 −b)) with 1 −b = τ 2/(σ2 + τ 2). The empirical
Bayes estimators derived by George and Casella (1994) in γEB are
1 −ˆb(x) = max

d, 1 −
a
||x||2

= ua,d(||x||2),

490
Hierarchical and Empirical Bayes Extensions
10
while CEB is centered in the truncated Stein estimator associated with a
and d ≤1. Actually, George and Casella (1994) show that the empirical
Bayes estimator obtained this way,
γEB(x) = P

χ2
p ≤
c
max{d, (||x||2 −a)/||x||2}

,
dominates the constant report 1 −α under the quadratic loss (10.5.6), for
d ≤1 and a small enough. A suggested value of d is
d =
2c
c + 2a +
1
c(c + 4a)
.
See Lu and Berger (1989b) for a diﬀerent solution.
∥
10.5.4 Comments
To conclude this overview of empirical Bayes methods, let us stress once
more their dual nature: they draw strength from both frequentist and
Bayesian methods to derive inferential procedures. It can be argued that
the improvements these estimators bring on classical frequentist estimators
owes to their mimicking of the Bayesian approach, whereas their subopti-
mality (in terms of admissibility for instance) can be attributed to the
refusal to adopt a fully Bayesian perspective, and to the subsequent arbi-
trariness in the choice of the resulting method. Fundamentally, it is quite
logical that a method that relies on classical, but suboptimal, estimators
(such as the maximum likelihood estimator of the mean in the multidi-
mensional normal case) and ad-hoc concepts lacking a decision-theoretic
basis (such as unbiased estimation or moment methods) cannot provide
optimal procedures. The domination of these estimators by genuine Bayes
estimators (see Brown (1988)) is another argument in favor of a complete
adoption of the Bayesian paradigm, even if it requires a hierarchical mod-
eling. As shown in Chapter 6, the development of new numerical tools to
deal with far more complex models than before comes as a last blow to
these methods that had earlier alleviated the computational diﬃculties of
fully Bayesian analyses.
10.6 Exercises
Section 10.1
10.1 For a model represented by a directed acyclic graph, as in Figure 10.1, show
that the full conditional distribution of a variable (or node), given the other
variables of the model, is identical to the distribution of this node, given only
the nodes it is connected with.
10.2 In the setting of Example 10.1.1,
a. Show that, if the generator Λ can be decomposed as P ˜ΛP t, where P is the
orthogonal matrix of the eigenvectors of Λ and ˜Λ is the diagonal matrix of

10.6
Exercises
491
the eigenvalues of Λ, λi (i = 1, . . . , 7), then
exp{Λ} = P
⎛
⎜
⎜
⎝
eλ1
0
. . .
0
0
eλ2
. . .
0
...
0
0
. . .
eλ7
⎞
⎟
⎟
⎠P t ,
and that exp{TΛ} = exp{Λ}T .
b. Deduce that the forward–backward formulas (6.5.4) of Exercise 6.47 also
apply in this setting when pij is replaced with p(T )
ij , (i, j)-th element of the
matrix exp{Λ}T .
c. Determine the computational time of these formulas and compare it with
the following alternative representation: introduce missing values x∗
ij so that
individuals are observed at regular intervals with inter-observation time η,
complete the sample with these missing values, and compute the forward–
backward formulas on the completed sample. (Notice that this completion
implies that exp{Λ}η is computed only once.)
Section 10.2.1
10.3 Show that the hyperprior chosen in Example 10.2.3 leads to a well deﬁned
posterior distribution provided there are at least two observations for each
stage of the experiment.
10.4 Represent the hierarchical model of Example 10.2.3 as a directed acyclic
graph, as in Figure 10.1.
10.5 Consider J ∼Mk(N; p1, . . . , pk), a multinomial random variable. Assume
that N is generated according to a Poisson distribution with parameter λ.
Determine the marginal distribution of J. Give, in particular, the covariance
matrix. Extend to the case when p = (p1, . . . , pk) ∼D(α1, . . . , αk), a Dirichlet
distribution.
10.6 A ﬂy lays N eggs according to a Poisson distribution P(λ) and each egg
survives with probability p.
a. Show that the distribution of the number of surviving eggs x is then hier-
archical
x|N ∼B(N, p),
N ∼P(λ).
b. Compute the marginal distribution of x and the posterior distribution of
N.
10.7 In the setting of Example 10.6, when p is known, give the posterior distri-
bution of N if π2(λ) = 1/λ. Examine the generalization to the case when p is
unknown and π1(p) = 1.
Section 10.2.2
10.8 If y|θ ∼Np(θ, Σ1), θ|β ∼Np(Xβ, Σ2), and β ∼Nq(μ, Σ3), compute the
prior and posterior distributions of θ.
10.9 Consider the setting of a logistic regression, that is, of observations (x1, y1),
. . . , (xn, yn) such that xi ∈IRk and yi ∈{0, 1} with
P(yi = 1|xi) = exp(xt
iβ)/(1 + exp(xt
iβ))
and derive a suﬃcient condition on π(τ) for the posterior distribution of β to
be deﬁned when β|τ ∼Nq(0, τ 2Ip). (The xi’s are deemed to be ﬁxed.)

492
Hierarchical and Empirical Bayes Extensions
10
10.10 Reproduce Exercise 10.9 in the setting of a probit model, that is, when
P(yi = 1|xi) = Φ(xt
iβ)
and Φ is the c.d.f. of the standard normal distribution.
Section 10.2.3
10.11 Establish Lemmas 10.2.9 and 10.2.11.
10.12 (Berger and Robert (1990))
In the setting of Example 10.2.14, assume
that μ ∈H = {μ = Y β; β ∈IRℓ} and π2(β, σ2
π) = 1. Show that m(x) < +∞
if p > 2 + ℓ.
10.13 Spiegelhalter et al. (1990) consider the following model: the weights yij of
60 rats are measured weekly, the ﬁrst 30 observations being associated with
the control group (1 ≤i ≤60, 1 ≤j ≤5). The corresponding model is
yij ∼N(αi + βij, σi) ,
with σi = σc for i ≤30 and σi = σt for 31 ≤i ≤60 and
(αi, βi) ∼N2 ((αc, βc), Σc)
(i = 1, . . . , 30) ,
(αi, βi) ∼N2 ((αt, βt), Σt)
(i = 31, . . . , 60) .
Complete the model with noninformative priors on the hyperparameters and
examine whether the posterior distribution is well deﬁned.
10.14 In the setting of Example 10.2.10, deﬁne the averages
xi = 1
Jc
i
Jc
i

i=1
xij,
yi = 1
Ja
i
Ja
i

i=1
yij,
zi = 1
Jt
i
Jt
i

i=1
zij
and
θ = 1
I
I

i=1
θi,
δ = 1
I
I

i=1
δi.
Show that the full conditional distributions are given by (1 ≤i ≤I)
μθ ∼N(θ, σ2
θ/I),
μδ ∼N(δ, σ2
δ/I),
μP ∼N
!
ℓi=0
ξi/IP , σ2
P /IP
"
,
μD ∼N
!
ℓi=1
ξi/ID, σ2
D/ID
"
,
θi ∼N

σ−2
θ μθ + Jc
i σ−2
c xi + Ja
i σ−2
a (yi −δi) + Jt
i σ−2
t
(zi −δi −ξi)
σ−2
θ
+ Jc
i σ−2
c
+ Ja
i σ−2
a
+ Jt
i σ−2
t
,
(σ−2
θ
+ Jc
i σ−2
c
+ Ja
i σ−2
a
+ Jt
i σ−2
t
)−1

δi ∼N

σ−2
δ μδ + Ja
i σ−2
a (yi −θi) + Jt
i σ−2
t
(zi −θi −ξi)
σ−2
δ
+ Ja
i σ−2
a
+ Jt
i σ−2
t
,

10.6
Exercises
493
(σ−2
δ
+ Ja
i σ−2
a
+ Jt
i σ−2
t
)−1

ξi ∼N

σ−2ℓi
D
σ−2(1−ℓi)
P
μℓi
Dμ1−ℓi
P
+ Jt
i σ−2
t
(zi −θi −δi)
σ−2ℓi
D
σ−2(1−ℓi)
P
+ Jt
i σ−2
t
,
(σ−2ℓi
D
σ−2(1−ℓi)
P
+ Jt
i σ−2
t
)−1

σ−2
c
∼Ga
!
i
Jc
i
2 ,

i,j
(xij −θi)2
2
"
,
σ−2
a
∼Ga
!
i
Ja
i
2 ,

i,j
(yij −θi −δi)2
2
"
,
σ−2
t
∼Ga
!
i
Jt
i
2 ,

i,j
(zij −θi −δi −ξi)2
2
"
,
σ−2
θ
∼Ga
!
I
2,

i
(θi −μθ)2
2
"
,
σ−2
δ
∼Ga
!
I
2,

i
(δi −μδ)2
2
"
,
σ−2
P
∼Ga
!
IP
2 ,

ℓi=0
(ξi −μP )2
2
"
, σ−2
D ∼Ga
!
ID
2 ,

ℓi=1
(ξi −μD)2
2
"
.
10.15 (Exercise 10.14 cont.)
When the δi’s are distributed from (10.2.3),
give the corresponding full conditionals.
Section 10.2.4
10.16 (Berger and Robert (1990))
Consider x ∼Np(θ, Σ), θ ∼Np(yβ, σ2
πIp),
and β ∼Nℓ(β0, A), with rank(A) = m.
a. Show that if, for K > 0, the two integrals
 K
0
π2(σ2
π)dσ2
π
and
 +∞
K
1
(σ2π)(p−ℓ+m)/2 π2(σ2
π)dσ2
π
are ﬁnite, then m(x) < +∞for every x ∈IRp.
b. Show that condition a. is satisﬁed if, for ϵ > 0, K1 > 0, K2 > 0,
π2(σ2
π) <
K1
K2 + (σ2π)(2+ϵ−p+ℓ−m)/2 ,
thus if π2(σ2
π) = 1 and p −l + m > 2.
10.17 (Berger (1985a)) In the setting of Example 10.2.18, compute the posterior
variance. Consider also the noninformative case.
10.18 (Lindley and Smith (1972)) Extend Example 10.2.18 to the general model
x|θ ∼Np(A1θ, Σ1),
θ|β ∼Nℓ(A2β, Σ2),
β|ξ ∼Nq(A3ξ, Σ3),
and check the results of Example 10.2.7.
10.19 (Berger (1985a))
Show that, for the model of Example 10.2.18 and a
noninformative distribution on ξ and σ2
π, the hierarchical Bayes estimator is
δπ(x) = x −hp−2(||x −¯x1||2)(x −¯x1)

494
Hierarchical and Empirical Bayes Extensions
10
with
hp(t)
=
p
2t(1 −Hp(t)),
Hp(t)
=
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
tp/2
(p/2)!

et −(p−2)/2
i=1
ti/i!
 
if p is even,
tp/2
Γ(p/2)

et[2Φ(
√
2t) −1] −(p−3)/2
i=1
t(i+3)/2
Γ(i+3/2)
 
if p is odd.
10.20 In the setting of Example 10.2.13, compute the posterior mean of p when
x = 3, n = 5.
Section 10.2.5
10.21 Compare the models
x ∼Np(θ, Ip),
θ|μ ∼Np(μ, τ 2Ip),
π2(μ, τ 2) = 1/τ 2,
and
x ∼Np(θ, Ip),
θ|μ ∼Np(μ, Ip),
μ|ξ ∼Np(ξ, τ 2Ip),
π2(ξ, τ 2) = 1/τ 2,
in terms of estimators of θ.
10.22 Consider xi ∼N(μi, σ2) and μi|μ, τ ∼N(μ, τ 2) (i = 1, . . . , n).
a. Show that π(μ, τ) = 1/τ leads to an undeﬁned posterior distribution.
b. Show that π(μ, τ) = 1 avoids the above problem.
10.23 In the setting of Example 10.2.7, show that the Bayes estimator
δπ(y) = IEπ2(σ2
π|y)

Ip + σ2
σ2π
(XtX)−1
−1
ˆβ
can be written in the form+
Ip + h(y)(XtX)−1,−1 ˆβ.
(Hint: Use a simultaneous diagonalization of Ip and XtX.) Explain how this
estimator can help to reduce multicollinearity.
Section 10.3
10.24 (Stein (1981)) Establish Lemma 10.3.1 using an integration by parts and
relate this result to Exercise 2.56.
10.25 If H is the Hessian matrix deﬁned in Lemma 10.3.1, show that the equiv-
alent of (10.3.2) for the covariance matrix is
V EB(x) = Σ + ΣH(x)
m(x)Σ −Σ(∇log m(x))(∇log m(x))tΣ.
Using a technique as in Exercise 10.24, show that an unbiased estimator of
the average matricial error
IEθ[(θ −δ(x))(θ −δ(x))t]
can be written in the diﬀerential form
ˆVδHB(x) = Σ + 2ΣH(x)
m(x)Σ −Σ(∇log m(x))(∇log m(x))tΣ.
Derive from this expression the unbiased estimator of the quadratic risk.

10.6
Exercises
495
Table 10.6.1. Car-buying intentions of households.
Intentions
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Answers
293
26
21
21
10
9
12
13
11
10
21
10.26 Use the following approximation of 1F1(a; b; z):
1F1(a; b; z) ≃Γ(b)
Γ(a)ez/2(z/2)a−b

1 + (1 −a)(b −a)
(z/2)

to provide an approximation of the estimator δπ given in Example 10.3.6 and
compare with the James–Stein estimator.
10.27 Consider x ∼Np(θ, Ip), θ ∼Np(0, τ 2Ip), and, if η = 1/(1 + τ 2), assume
π2(η) = η2−(p/2). Show that the corresponding hierarchical Bayes estimator
can be written explicitly as
δHB(x) =

1
1 −e−||x||2/2 −
2
||y||2

x ,
and determine whether it is minimax and admissible.
10.28
∗(Hartigan (1983)) Consider an observation x ∼Np(θ, Ip).
a. If f is a positive nondecreasing function bounded above by 2(p −2), show
that
δf(x) =

1 −f(||x||2)
||x||2

x
dominates δ0(x) = x for the usual quadratic loss. (Hint: Use the unbiased
estimator of the risk obtained in Exercise 2.56.)
b. Let π be a prior on θ such that, conditional upon τ 2, θ ∼Np(0, τ 2) and τ 2 ∼
π1. The hyperprior π1 is assumed to be a log-concave function of log(τ 2+1)
and (τ 2 + 1)1−απ1(τ 2) is increasing in τ 2. Using the general result of a.,
show that the hierarchical Bayes estimator associated with π dominates δ0
if 4 −2α ≤p. (Hint: Show that δπ(x) = (1 −IE[(τ 2 + 1)−1|x])x and that
IE[(τ 2 + 1)−1|x]) is increasing in ||x||2 while being obviously bounded by
2(p −2).)
c. Show that such priors can only be proper for α < 0, and therefore that
these minimax Bayes estimators are guaranteed to be admissible only for
p ≥5.
d. Show that the Bayes risk is actually ﬁnite for α < 2 and deduce that the
resulting hierarchical Bayes estimators are admissible for every p. [Note:
Strawderman (1971) considered the particular case π1(τ 2) = (1+τ 2)α−1 to
show that the limiting dimension for the existence of proper Bayes minimax
estimators is exactly p = 5.]
Section 10.4.2
10.29 (Casella (1985a))
In a survey of car-buying intentions, 447 households
provide their evaluation of the probability of buying a new car in the coming
year. The result of the survey is given in Table 10.28.

496
Hierarchical and Empirical Bayes Extensions
10
Table 10.6.2. Proportions of car acquisitions depending on the intention.
Intentions
0
0.1—0.3
0.4—0.6
0.7—0.9
1
Declared
0
0.19
0.51
0.79
1
Realized
0.07
0.19
0.41
0.48
0.583
The answers xi (1 ≤i ≤447) are modeled as issued from a renormalized bino-
mial B(10, pi) distribution, that is, 10xi ∼B(10, pi), and the pi are distributed
according to Be(α, β).
a. Use the marginal distribution to provide estimators of α and β by the
method of moments.
b. Derive an empirical Bayes estimator of the pi’s under quadratic loss.
c. The true intentions pi have actually been observed at the end of the year,
and Table 10.29 gives the diﬀerence with the declared intentions. Com-
pare the quadratic losses of the classical estimator (that is, ˆpi = xi), the
empirical Bayes estimator, and a Bayes estimator of your choice.
10.30 Establish the equivalent of Proposition 10.4.5 if the test is about H0 :
θ = θ0 versus H1 :
θ = θ1 for two independent problems with samples
x1, . . . , xn ∼f(x|θ), y1, . . . , ym ∼f(y|θ′), and P(θ = θ0) = P(θ′ = θ0) = π0.
Generalize to p samples and apply in the case of the test of θi = 0 versus
θi = 1 for xi ∼N(θi, 1) (1 ≤i ≤p).
10.31
∗(Hartigan (1983)) Consider x ∼Np(θ, σ2Ip) and θ ∼Np(0, τ 2Ip), with
σ2 unknown and s2 ∼σ2χ2
k.
a. Give an empirical Bayes estimator of θ based on the maximum likelihood
estimators of τ 2 and σ2 and determine whether the resulting estimator is
minimax. (Hint: Use Exercise 10.28.)
b. Compare with the empirical Bayes estimators based on the moment esti-
mators of σ2 and τ 2.
c. If π(σ2, τ 2) ∝(σ2 + σ2
0)α−1(σ2)β−1, show that the posterior distribution of
(σ−2, (σ2 + τ 2)−1) is
χ2
k−2β/s × χ2
p−2α/||x||2IIσ2≤σ2+τ2.
Show that the resulting estimator is minimax if
p −α
k −β −2 ≤2(p −2)
k + 1 .
(Hint: Use Theorem 2.8.1.)
10.32 (Hartigan (1983)) Consider a multinomial model Mk(n; p1, . . . , pk) and
the observation (n1, . . . , nk). A possible conjugate prior is the Dirichlet distri-
bution, D(α1, . . . , αk).
a. Show that
IE

k

i=1
n2
i

= n + (n −1) α + 1
kα + 1,

10.6
Exercises
497
and determine when the moment equation derived from this equality has
a positive solution. Derive an empirical Bayes estimator of (p1, . . . , pk) in
this case.
b. Compute an alternative empirical Bayes estimator by using maximum like-
lihood estimators of the αi’s. [Note: See Good (1975) for details on this
model.]
10.33 (Morris (1983a)) An exponential family with density
f(x|θ) = h(x)eθx−ψ(θ)
has a quadratic variance if the variance can be written
V (μ) = ψ′′(θ) = v0 + v1μ + v2μ2,
where μ = ψ′(θ) is the expectation of f(x|θ). Morris (1982) has characterized
the six families with quadratic variance (see Exercise 3.9). These distributions
are denoted NEF(μ, V (μ)).
a. Show that the conjugate distribution in μ can be written
g(μ) = K(m, μ0)emμ0θ(μ)−mψ(θ(μ))V −1(μ)
(10.6.1)
and that
IEπ[μ] = μ0,
V π(μ) = τ 2
0 = V (μ0)
m −v2 ,
Therefore, the conjugate distribution is also an exponential family with
quadratic variance. Derive a table of the correspondence between sample
and conjugate prior distributions for the six families obtained in Exercise
3.24.
b. Show that the Bayes estimator associated with (10.6.1) for n independent
observations x1, . . . , xn and quadratic loss is
δπ(x1, . . . , xn) = (1 −B)¯x + Bμ0,
where
B =
V (μ0) + v2τ 2
0
V (μ0) + (n + v2)τ 2
0
.
c. Show that, for the conjugate distribution (10.6.1), the marginal moments
of ¯x are
IE[¯x] = μ0,
var(¯x) = V (μ0)
n
m + n
m −v2 .
d. Consider k independent observations
xi|μi ∼NEF(μi, V (μi)/n)
(1 ≤i ≤k),
with independent parameters μi from the conjugate distribution (10.6.1).
If ¯x = 
i xi/k and s = 
i(xi −¯x)2 and if
IE[V (¯x)(k −1)]
IE[s]
= IE

V (¯x)(k −3)
s

(the expectations being taken under the marginal distribution), show that
an empirical Bayes estimator for μi is
δEB
i
(x1, . . . , xk) = (1 −ˆB)xi + ˆB¯x,

498
Hierarchical and Empirical Bayes Extensions
10
with
ˆB = min

v2
n + v2
k −1
k
+
n
n + v2
(k −3)V (¯x)
ns
, 1

.
Section 10.5.1
10.34 Show that, for the marginal distribution of Example 10.5.1, (p −2)/||x||2
is indeed an unbiased estimator of 1/(1 + τ 2).
10.35 Derive formula (10.5.4) of Example 10.5.2.
10.36
∗(Kubokawa (1991)) Consider δJS(x) = [1 −(p −2)/||x||2]x, the original
James–Stein estimator and x ∼Np(θ, Ip). Deﬁne λ = ||θ||2/2; fp(t; λ) is the
noncentral chi-squared density with noncentrality parameter λ.
a. For the truncation of δJS
δ1(x; c, r) =
⎧
⎨
⎩

1 −
c
||x||2

x
if ||x||2 < r,
δJS(x)
otherwise,
show that the quadratic risk of δ1(x; c, r) is minimized for
c1(r, λ) = p −2 −
2fp(r; λ)
 r
0 (1/t)fp(t; λ) dt.
b. Let us deﬁne
c1(r) = p −2 −
2
 1
0 tp/2−2e(1−t)r/2dt
.
Show that δ1(x; c1(r), r) dominates δJS for every r.
c. Using a limiting argument, show that
δ∗
1(x) =

1 −c1(||x||2)
||x||2

x
dominates δJS. [Note: This estimator is proposed in Strawderman (1971)
and Berger (1976). See Exercise 10.28.]
d. Show that δ∗
1 is admissible. (Hint: The suﬃcient condition of Theorem
8.2.13 can be used.)
10.37
∗(Bock and Robert (1991))
Consider x ∼Np(θ, Ip) and θ ∼U{∥θ∥2=c},
the uniform distribution on the sphere with radius c. Propose an empirical
Bayes estimator of θ based on ||x||2 and show that, if this estimator is derived
from the maximum likelihood estimator of c, then δEB(x) = h(x)x with

1 −
p
||x||2
+
≤h(x) ≤

1 −p −1
||x||2
+
.
Discuss the robustness of the Stein eﬀect in terms of spherical symmetry.
10.38
∗(George (1986a)) Consider y ∼Np(θ, Ip). This exercise derives an esti-
mator that selects among several partitions of y into subvectors before shrink-
ing the observation toward each of these subvectors. For k = 1, . . . , K, let us
denote
y = (yk1, . . . , ykJk)Ck
and
θ = (θk1, . . . , θkJk)Ck

10.6
Exercises
499
as the partitions of y and θ in subvectors ykj and θkj of dimension pkj (1 ≤
j ≤Jk), where Ck is a permutation matrix comprising 0’s and 1’s with a single
1 per row and per column. For k = 1, . . . , K, consider δk = (δk1, . . . , δkJk)Ck
an estimator with components
δkj(ykj) = ykj + ∇log mkj(ykj),
where the functions mkj from IRpkj in IR are twice diﬀerentiable. We also
denote
mk(y) =
Jk

j=1
mkj(ykj)
and
m∗(y) =
K

k=1
ωkmk(y),
for ωk ≥0 (1 ≤k ≤K) and 
k ωk = 1.
a. If πkj is a prior distribution on θkj and mkj is the corresponding marginal
distribution on ykj (1 ≤k ≤K, 1 ≤j ≤Jk), show that mk is the marginal
distribution of y for the prior distribution
πk(θ) =
Jk

j=1
πkj(θkj) ,
and that δk is the posterior mean for this posterior distribution.
b. Deduce that
δ∗(y) = y + ∇log m∗(y)
is the Bayes estimator for the prior distribution
π∗(θ) =
K

k=1
ωkπk(θ).
c. Show that δ∗can also be written under the form
δ∗(y) =
K

k=1
ϱk(y)δk(y),
with ϱk(y) = ωkmk(y)/m∗(y), and interpret this result.
d. Show that if, for k = 1, . . . , K,
IEθ

∂2mk(y)
∂y2
i
-
mk(y)

<
+∞,
IEθ||∇log mk(y)||2
<
+∞,
then the unbiased estimator of the risk of δ∗can be written
Dδ∗(y) = p −
K

k=1
ϱk(y)

Dδk(y) −(1/2)
K

ℓ=1
ϱℓ(y)||δk(y) −δℓ(y)||2

,
with
Dδk(y) = ||∇log mk(y)||2 −2Δmk(y)/mk(y).
(Hint: Use Lemma 10.3.1 with Q = Σ = Ip.)
e. Deduce that, if mkj is superharmonic, that is, such that Δmkj(ykj) ≤0 for
1 ≤k ≤K, 1 ≤j ≤Jk, δ∗is minimax. [Note: This result can be described
as the fact that a “proper” convex combination of minimax estimators is
still minimax.]

500
Hierarchical and Empirical Bayes Extensions
10
f. For 1 ≤k ≤K, 1 ≤j ≤Jk, we denote by Vkj a subspace of IRpkj, with
dim Vkj = pkj −qkj and qkj ≥3; Pkj is the associated orthogonal projector
from IRpjk on Vkj and skj = ||ykj −Pkjykj||2. Give the multiple shrinkage
estimator δ∗associated with
mkj(ykj) =
⎧
⎨
⎩

qkj −2
eskj
(qkj −2)/2
if skj ≥qkj −2,
exp(−skj/2)
otherwise.
(Hint: The solution is the truncated James–Stein estimator.)
Section 10.5.2
10.39 *(Kubokawa et al. (1993))
Consider x ∼Np(θ, σ2Ip), y ∼Nq(ξ, σ2Iq),
and s ∼σ2χ2
n, with unknown θ, ξ, and σ. An empirical Bayes estimator of θ
is the James–Stein estimator
δJS(x, s) =

1 −
(p −2)s
(n + 2)||x||2

x.
The purpose of this exercise is to show that the replacement of s by a more
eﬃcient estimator of σ2 can lead to an improvement in the estimation of θ.
a. Show that, if γh(y, s) = sh(||y||2/s) dominates γ0(s) = s/(n + 2) under the
invariant quadratic loss
L(σ2, γ) =
 γ
σ2 −1
2
,
δJS is dominated by
ˆδ(x, y, s) =

1 −(p −2)γ(y, s)
||x||2

x
under quadratic loss. (Hint: Recall that γ0 is the best equivariant estimator
of σ2.)
b. Consider
δg(x, y, s) =

1 −(p −2)s
||x||2
g(||y||2/s, ||x||2/s)

x.
Deﬁne
g∗(u, v) = min

g(u, v), 1 + u + v
p + q + n

,
and assume g and g∗are absolutely continuous functions of v. Show that,
if
IE

∂g∗(U, V )
∂v
−∂g(U, V )
∂v

≥0,
when U = ||y||2/s and V = ||x||2/s, δg∗dominates δg.
c. Deduce that
δ2(x, y, s) = x −p −2
||x||2 min

s
n + 2, s + ||y||2
n + q + 2, s + ||x||2 + ||y||2
n + p + q + 2

x
dominates δJS.

10.6
Exercises
501
Section 10.5.3
10.40
∗(Casella and Hwang (1983))
Consider x ∼Np(θ, Ip). Under the linear
loss,
L(θ, C) = k vol(C) −IIC(θ),
recall that the Bayes estimators are HPD regions of the form {θ; π(θ|x) ≥k}
when π({θ; π(θ|x) = k}) = 0. Moreover, if
k = k0 = e−c2/2/(2π)p/2,
Joshi (1969) has established that the usual region
C0
x = {θ; ||θ −x|| ≤c},
is minimax.
a. Show that, if θ ∼Np(0, τ 2Ip), the Bayes set is
Cπ
x =

θ; ||θ −δπ(x)||2 ≤−2τ 2
τ 2 + 1 log

k

2πτ 2
τ 2 + 1
p/2
,
where δπ(x) = (τ 2/τ 2 + 1)x is the Bayes estimator of θ. For k = k0, show
that this set can be written
Cπ
x =

θ; ||θ −δπ(x)||2 ≤
τ 2
τ 2 + 1

c2 −p log

τ 2
τ 2 + 1

.
b. Deduce that a simple empirical Bayes set is
CEB
x
= 
θ; ∥θ −δEB(x)∥2 ≤vEB(x)
,
with δEB(x) = (1 −[(p −2)/||x||2])x and
vEB(x) =

1 −p −2
||x||2
 
c2 −p log
1 −p −2
||x||2


.
c. Explain why it is preferable to consider
δ+(x) =

1 −p −2
||x||2
+
x
and
ve(x) =
⎧
⎨
⎩
	
1 −p−2
c2

 	
c2 −p log +
1 −p−2
c2
,
if ||x||2 < c,

1 −
p−2
||x||2
 
c2 −p log
/
1 −
p−2
||x||2
0
otherwise.
d. Extend to the case when x ∼Np(θ, σ2Ip) and s2 ∼σ2χ2
q.
Note 10.7.2
10.41 In the setting of Example 10.7.3,
a. Show that
L(η, ˆη) = p
2 log

η
ˆη

+ 1
2

1
ˆη −1
η

p η ,
and deduce (10.7.3).

502
Hierarchical and Empirical Bayes Extensions
10
b. Show that the posterior distribution associated with πd is well deﬁned for
d > (4 −p)/2.
c. Show that (10.7.4) and (10.7.5) hold.
d. Derive from the approximation
1F1(a, b, z) = Γ(b) z−a

1 −a(1 + a −b)
z
+ O(z2)

the equivalence (10.7.6).
10.42
∗(Exercise 10.41 cont.) Using STUB conditions (see Section 8.5) and
the approximation (10.7.6), show that the choice d = 1 is optimal.
10.43
∗(Exercise 10.41 cont.)
Derive from Alam (1973) the minimaxity of
δEB
1 . (Hint: See Example 10.3.6.)
10.44 In the setting of Example 10.7.4,
a. Show that the entropy loss is given by (10.7.7) and deduce the general form
of Bayes estimators under that loss.
b. Show that, when π(λ) = λ−d, the posterior distribution is given by
πd(λ|x) ∝λn−d(λ + 1)
−
i xi−n
and deduce that the Bayes estimator of λ is given by (10.7.8).
c. Show that the conditional distribution π(θ|x, λ) ∝θxe−(λ+1)θ and deduce
that the Bayes estimator of θi conditional on λ is
IE[θ|xi, λ] = xi + 1
λ + 1 .
10.45 (Exercise 10.44 cont.)
Show that the usual estimator of θ, that is,
γ0(x) = x, has an inﬁnite risk under entropy loss.
10.46 (Exercise 10.44 cont.) Show that the Bayes estimator associated with
the integrated prior
π(θ) =

π(θ, λ)dλ
is given by
δπ(x) =

1 −
n −d + 1
n
i=1 xi + n

(x + 1) .
Deduce from the diﬀerence of the entropy risks that the Bayes estimator dom-
inates its empirical Bayes counterpart under entropy loss.
10.7 Notes
10.7.1 Graphical models7
Graphical models analyze statistical models by graphs. They have been de-
veloped mainly to represent conditional independence relations, primarily in
the ﬁeld of expert systems (Whittaker (1990), Spiegelhalter et al. (1993)). The
Bayesian approach to these models, as a way to incorporate model uncertainty,
has been aided by the advent of MCMC techniques, as stressed by Madigan
and York (1995) in an expository paper on which this note is based.
7 This note closely follows Note 7.6.6 in Robert and Casella (1999).

10.7
Notes
503
The construction of a graphical model is based on a collection of independence
assumptions represented by a graph. We brieﬂy recall here the essentials of
graph theory and refer to Lauritzen (1996) for details. A graph is deﬁned by
a set of vertices or nodes, α ∈V, which represents the random variables or
factors under study, and by a set of edges, (α, β) ∈V2, which can be ordered
(the graph is then said to be directed) or not (the graph is undirected) and
represent the dependence connections between the variables. For an undirected
graph, the variables α and β are connected by an edge if, conditional on all
the other variables, they are not independent. For a directed graph, α is a
parent of β if (α, β) is an edge (and β is then a child of α).8 Graphs are also
often assumed to be acyclic, that is, without directed paths linking a node α
with itself. This leads to the notion of directed acyclic graphs introduced by
Kiiveri and Speed (1982), often represented by the acronym DAG.
For the construction of probabilistic models on graphs, an important notion
is that of a clique.9 A clique C is a maximal subset of nodes that are all joined
by an edge (in the sense that there is no subset containing C and satisfying
this condition). An ordering of the cliques of an undirected graph (C1, . . . , Cn)
is perfect if the nodes of each clique Ci contained in a previous clique are all
members of one previous clique (these nodes are called the separators, α ∈Si).
In this case, the joint distribution of the random variable V taking values in
V is
p(V ) =

v∈V
p(v|P(v)) ,
where P(v) denotes the parents of v. This can also be written as
p(V ) =
n

i=1
p(Ci)
n

i=1
p(Si)
,
(10.7.1)
and the model is then called decomposable; see Spiegelhalter and Lauritzen
(1990), Dawid and Lauritzen (1993) or Lauritzen (1996). As stressed by Spiegel-
halter et al. (1993), the representation (10.7.1) leads to a principle of local
computation, which enables the building of a prior distribution, or the simu-
lation from a conditional distribution on a single clique. (In other words, the
distribution is Markov with respect to the undirected graph, as shown by Dawid
and Lauritzen (1993).) The appeal of this property for a Gibbs sampling im-
plementation is then obvious.
When the densities or probabilities are parametrized, the parameters are de-
noted by θA for the marginal distribution of V ∈A, A ⊂V. (In the case
of discrete models, θ = θV may coincide with p itself; see Example 10.7.1.)
The prior distribution π(θ) must then be compatible with the graph structure:
8 Directed graphs can be turned into undirected graphs by adding edges between nodes
sharing a child, and dropping the directions.
9 Clique is a French word meaning faction, gang or group. Its usual connotation is
rather pejorative.

504
Hierarchical and Empirical Bayes Extensions
10
Dawid and Lauritzen (1993) show that a solution is of the form
π(θ) =
n

i=1
πi(θCi)
n

i=1
˜πi(θSi)
,
(10.7.2)
thus reproducing the clique decomposition (10.7.1).
Example 10.7.1 Consider a decomposable graph such that the random vari-
ables corresponding to all the nodes of V are discrete. Let w ∈W be a possible
value for the vector of these random variables and θ(w) be the associated prob-
ability. For the perfect clique decomposition (C1, . . . , Cn), θ(wi) denotes the
marginal probability that the subvector (v, v ∈Ci) takes the value wi (∈Wi)
and, similarly, θ(ws
i ) is the probability that the subvector (v, v ∈Si) takes the
value ws
i when (S1, . . . , Sn) is the associated sequence of separators. In this
case,
θ(w) =
n

i=1
θ(wi)
n

i=1
θ(ws
i )
.
As illustrated by Madigan and York (1995), a Dirichlet prior can be con-
structed on θW = (θ(w), w ∈W ). It leads to genuine Dirichlet priors on
the θWi = (θ(wi), wi ∈Wi), under the constraint that the Dirichlet weights
are identical over the intersection of two cliques. Dawid and Lauritzen (1993)
establish that this prior is unique, given the marginal priors on the cliques. ∥
Example 10.7.2 Giudici and Green (1998) provide another illustration of
prior speciﬁcation in the case of a graphical Gaussian model, X ∼Np(0, Σ),
where the precision matrix K = {kij} = Σ−1 must comply with the condi-
tional independence relations on the graph. For instance, if Xv and Xw are
independent given the rest of the graph, then kvw = 0. The likelihood can
then be factorized as
f(x|Σ) =
n

i=1
f(xCi|ΣCi)
n

i=1
f(xSi|ΣSi)
,
with the same clique and separator notations as above, where f(xC|ΣC) is the
Normal NpC (0, ΣC) density, following (10.7.1). The prior on Σ can be chosen
as the conjugate inverse Wishart priors on the ΣCi’s, under some compatibility
conditions.
∥
Madigan and York (1995) discuss an MCMC approach to model choice and
model averaging in this setting, whereas Dellaportas and Forster (1996) and
Giudici and Green (1998) implement reversible jump algorithms for determin-
ing the probable graph structures associated with a given dataset, the latter

10.7
Notes
505
under a Gaussian assumption.
10.7.2 Bayes empirical Bayes
As noticed in Section 10.4, the diﬃculty with the empirical Bayes approach is
that it is a two–stage estimation procedure, where, ﬁrstly, the hyperparameter
is estimated from the marginal distribution, and, secondly, the parameter is
estimated based on a pseudo-prior where the hyperparameter is replaced by
the estimate of the ﬁrst stage. Although the ineﬃciency of this procedure—
when compared with a genuine Bayesian approach—cannot be fully remedied,
it seems natural to aim at a maximal eﬃciency by using the most eﬃcient esti-
mation method at the ﬁrst stage, that is, a noninformative Bayesian approach.
Because the ﬁrst stage estimation is not induced by a decision problem, a loss
function is most likely unavailable and the intrinsic losses presented in Section
2.5.4 have been argued as natural default losses in such cases. This solution,
which removes the arbitrariness attached to the estimation of the hyperparam-
eters in the empirical Bayes approach, is surprisingly ignored in the literature.
We present below two examples treated in Fourdrinier and Robert (1995).
Example 10.7.3 (Example 10.5.1 continued) The marginal distribution
of x, m(x|η), is Np(0, η Ip), where η = 1 + τ 2, and the corresponding entropy
loss for the estimation of η is
L(η, ˆη)
=

log

m(x|η)
m(x|ˆη)

m(x|η)dx
=
p
2

η
ˆη −log

η
ˆη

−1

.
(10.7.3)
Since η is a scale parameter for the marginal distribution, a natural family of
noninformative priors is πd(η) = η−d on [1, ∞[ and the corresponding estima-
tor of η is
ˆηd =
 1
0 ν(p/2)+d−3e−||x||2ν/2dν
 1
0 ν(p/2)+d−2e−||x||2ν/2dν
.
(10.7.4)
The empirical Bayes estimator is thus
δEB
d (x)
=
(1 −ˆη−1)x
=
 1
0 ν(p/2)+d−3(1 −ν)e−||x||2ν/2dν
 1
0 ν(p/2)+d−2e−||x||2ν/2dν
x
=
2
p + 2d −2
1F1(2, d + p/2, ||x||2/2)
1F1(2, d + p/2 −1, ||x||2/2) x ,
(10.7.5)
where 1F1 is the conﬂuent hypergeometric function (Abramowitz and Stegun
(1964, Chapter 13)). Since δEB
d (x) is asymptotically equivalent to

1 −p + 2(d −2)
||x||2

x
(10.7.6)
the choice d = 1, that is, π(η) = 1/η provides the optimal choice of d (Exercise
10.42).
∥

506
Hierarchical and Empirical Bayes Extensions
10
Example 10.7.4 (Example 10.4.3 continued)
The entropy loss associ-
ated with m(x|λ) is
L(λ, ˆλ) = log

λ
ˆλ

+

1 + 1
λ

log
 ˆλ + 1
λ + 1

(10.7.7)
and, for π(λ) = λ−d, the corresponding Bayes estimator of λ is
ˆλ =
n −d
n
i=1 xi + n −1 .
(10.7.8)
If we also use an entropy loss for the estimation of λ, L(θ, ˆθ) = ˆθ−θ−log(ˆθ/θ),
the empirical Bayes estimator of θ = (θ1, . . . , θn) is
θEB(x) =

1 −
n −d
n
i=1 xi + n −1

(x + 1) ,
(10.7.9)
where 1 = (1, . . . , 1) ∈IRn. Fourdrinier and Robert (1995) show in addition
that there is an optimal choice d∗of d for the entropy loss, with d∗≤2, and
that there is a range of values of d for which θEB dominates ˆθ0 = x + 1.
∥

CHAPTER 11
A Defense of the Bayesian Choice
A series of steps, each taken for good cause or pure necessity, each seeming
so reasonable at the time, and each leading to things he had never imagined.
He always seemed to ﬁnd himself caught in that sort of dance.
Robert Jordan, Lord of Chaos, Book VI of the Wheel of Time.
This book has presented the main aspects of Bayesian inference in Statis-
tics from a decision-theoretic point of view. Our coverage has scarcely been
exhaustive: on one hand, the topics we consider are often treated in more
detail. On the other hand, Bayesian analysis can be applied to many ﬁelds,
and is increasingly done, thanks to the computational advances described
in Chapter 6. Among these ﬁelds, we can mention biostatistics (see, e.g.,
Berry and Stangl (1996)); econometrics (Zellner (1971, 1984), Box and Tiao
(1973), Poirier (1995), Bauwens et al. (1999), or Geweke (1999)); environ-
metrics (Parent et al. (1998)); expert systems (Gilks et al. (1993); Cowell
et al. (1999)); ﬁnance (Jacquier, Polson and Rossi (1994), Pitt and Shep-
hard (1996)); image processing and pattern recognition (Geman and Geman
(1984), Besag (1986), or Fitzgerald et al. (1999)); neural networks (Ripley
(1992), Neal (1996)); signal processing (Andrieu and Doucet (1999), An-
drieu, Doucet and Fitzgerald (2001)); Bayesian networks (Chickering and
Heckerman (2000), Kontkanen et al. (2000)). (See the recent survey by
Berger (2000), as well as Gatsonis et al. (1993, 1995, 1997, 1999), Gilks et
al. (1996), and Carlin and Louis (2000a) for additional references.)
Nonetheless, we found it important to consider Bayesian statistical analy-
sis primarily from this theoretical and decisional perspective, before paying
more attention to its potential for applications. Firstly, this study exhibits
the inherent coherence of the Bayesian approach in comparison with alter-
native classical theories. Secondly, to develop an eﬃcient approach for the
processing of applications requires a solid background in theoretical issues.

508
A Defense of the Bayesian Choice
11
We present, in this concluding chapter, a justiﬁcation of the Bayesian ap-
proach that summarizes the various arguments advanced so far.1
(1) Opting for a probabilistic representation
Proposing a distribution on the unknown parameters of a statistical
model can be characterized as a probabilization of uncertainty, that is,
as an axiomatic reduction from the notion of unknown to the notion of
random. This reduction being acceptable—and it is usually accepted by
most statisticians—for sampling models, it should be acceptable as well
for the parameters directing those models. For one thing, the distinction
between sample and parameters is not always clear-cut. Consider, for
instance, random-eﬀects models (Chapter 10) or allocation vectors in a
mixture model (Chapter 6).
More fundamentally, a probabilistic model is most often nothing but an
interpretation of a given phenomenon—as opposed to an explanation of
it. If we consider, for instance, econometric models, where the diﬀer-
ences between the realizations of endogenous variables and their linear
prediction on exogenous variables are explained by a random pertur-
bation, it is clear that the random nature of this diﬀerence is of little
importance because the experiment cannot be replicated.2 Therefore,
the representation of unknown phenomena by a probabilistic model,
at the observational level as well as at the parameter level, does not
need to correspond eﬀectively—or physically—to a generation from a
probability distribution, nor does it compel us to enter a supradeter-
ministic scheme, fundamentally because of the nonrepeatability of most
experiments.
The probabilistic representation of partly explained phenomena should
be perceived mainly as a simplifying but eﬃcient tool conveying and
quantitatively analyzing these phenomena (see point (4) below). This
perspective is really similar to the way physics can be seen as an inter-
pretation of the world and is a tool, eﬃcient enough to allow for a better
understanding of this world (and incidentally for technical progress),
while not needing to correspond to a truth deﬁnitely unattainable.3
1 The presentation of this chapter is thus quite diﬀerent from the other chapters, with no
theorem or even example, but a sequence of points (from (1) to (10)) arguing in favor
of the Bayesian approach, followed by a rebuttal of the most common criticisms of it.
The tone is thus mildly philosophical, rather than methodological (or mathematical),
and the reader can judge whether her or his impression, after going through the book,
coincides with the points set forth below.
2 To put it bluntly, an arbitrary number can always be perceived as a single realization
from an inﬁnity of distributions!
3 See also Popper (1983) for his alternative justiﬁcation of scientiﬁc modeling through
the metaphysical realism that he sets in opposition to this instrumental approach.

11.
A Defense of the Bayesian Choice
509
(2) Conditioning on the data
The basis of statistical inference is fundamentally an inversion process,
since it aims at deriving eﬀects from causes by taking into account the
probabilistic nature of the model and the inﬂuence of totally random
(that is, unexplained) factors. In both its discrete and continuous ver-
sions, Bayes’s Theorem formalizes this inversion, as does the notion of
the likelihood function ℓ(θ|x), substituted for the density f(x|θ). The
failure of Fiducial Statistics to provide a satisfactory inferential system
(see Note 1.8.1) can be attributed to a refusal to pursue this inversion
to its logical consequences and, relatedly, to a certain confusion between
observations and random variables.
From a probabilistic point of view, a quantitative analysis on the pa-
rameters θ that is operated conditional upon x strictly requires a corre-
sponding distribution on the parameter θ, π(θ), in order to invert the
probabilistic model. Taking this requirement into account, the Bayesian
approach is thus the unique coherent paradigm which respects the in-
version perspective. The practical problem of the determination of the
prior distribution π does not take place in the same conceptual space
(see point (ii), below).
(3) Exhibiting the true likelihood
In relation to points (1) and (2), notice also that the prior modeling on
the parameters of the model authorizes a complete quantitative infer-
ence on these parameters, therefore the eﬀective determination of the
likelihood of θ conditional on the observation x. On the contrary, classi-
cal Statistics fails to attain this completeness. In particular, as long as
θ is taken to be an unknown but ﬁxed quantity, the likelihood function
ℓ(θ|x) cannot be treated as a density conditional upon x, despite the
formal resemblance.
This impossibility of the classical approach to provide quantitative con-
clusions is particularly well illustrated in the case of conﬁdence regions
and tests, where it proposes an inappropriate problematic (and, conse-
quently, an inappropriate answer). As we saw in Chapter 5, the classical
procedure, whether a 95% interval or a p-value, derives its probabilis-
tic nature from a frequentist interpretation. It is not the parameter θ
that belongs to an interval with probability 95% conditional upon x, but
the interval derived from x that contains the ﬁxed value θ with prob-
ability 0.95. Again, the nonrepeatability of most practical experiments
impeaches this frequentist point of view (see also point (9) below).
(4) Using priors as tools and summaries
The choice of a prior distribution π does not require any kind of belief
in this distribution. It is actually rare to have a completely speciﬁed
prior distribution, the original example of Thomas Bayes being, para-
doxically, an exceptional counter-example where a physical knowledge
of the experiment leads to the construction of the prior distribution. In

510
A Defense of the Bayesian Choice
11
general, π should rather be considered either a tool that provides a single
inferential procedure with acceptable frequentist properties (see points
(6) and (8)), or a way to summarize the available prior information
and the uncertainty surrounding this information. That Bayesian analy-
sis can be extended to noninformative settings—with a few exceptions,
such as some testing situations—is actually an indicator of this polyva-
lence. Moreover, that many usual estimators can be recovered through
a noninformative modeling means that the use of a prior distribution
does not necessarily introduce a bias in the statistical process, but, on
the contrary, authorizes in addition the quantitative treatment already
mentioned in point (3). These coincidences actually enhance the superi-
ority of the Bayesian approach, since it provides at once a full inferential
treatment that supersedes these classical estimators.
(5) Accepting the subjective basis of knowledge
From a philosophical point of view, it is generally agreed that knowledge
stems from a confrontation between a prioris and experiments. For in-
stance, according to Kant, although knowledge starts with experimenting,
it does not follow that knowledge is entirely derived from experimenting.
In fact, without a prioris, that is, without a pre-established structure
of the world, observation is meaningless because it does not come as a
support of or as a confrontation to a referential model. Therefore, the
building of knowledge through experimentation implies the existence
of a prior representation system, which is very primitive at the begin-
ning, but gets progressively actualized via these experiments. From this
perspective, learning can be expressed as the critical examination of pre-
existent external systems subjected to experiments with respect to this
overall referential representation of the world.
This point of view is also found in Poincar´e (1902):
It is often stated that one should experiment without preconceived ideas.
This is simply impossible; not only would it make every experiment sterile,
but even if we were ready to do so, we could not implement this principle.
Everyone stands by his own conception of the world, which he cannot get
rid of so easily.
The Bayesian approach is obviously in accordance with this perspective,
since prior distributions are most often based on the results of previous
experiments. Actually, even the subjective aspect of the choice of the
prior distribution can be integrated in this theory of knowledge, since
it implies that every acquisition of knowledge is essentially subjective,
resulting from an interaction between individual perceptions and exterior
reality.4
4 In fact, Bayesian Statistics could answer this wish of Kant’s in the Introduction
to his Critique of Pure Reason: “Philosophy needs a science able to determine the
possibility, the principles and the scope of our whole prior knowledge.”

11.
A Defense of the Bayesian Choice
511
In his epistemological theory, Feyerabend (1975) also stresses that indi-
vidualism (that is, subjectivity) is an important, but blatantly ignored,
factor in scientiﬁc discoveries. Although opposed to this subjectivist ap-
proach to knowledge, Popper (1983) also recognizes the role of prior
intuitions (or systems), not always grounded on experiments, in the his-
tory of science—the most striking example being, from his point of view,
atomism, that is, the representation of matter as being constituted by
atoms, which took more than twenty centuries to ascertain experimen-
tally.
(6) Choosing a coherent and unique system of inference
The ultimate goal of Statistics is, arguably, to provide an inference about
a parameter θ given some observations x related to θ through a probabil-
ity distribution f(x|θ). Moreover, it seems only natural to seek eﬃciency
(or optimality) in this inference, the notion of optimality being deﬁned
explicitly by the statistician (or the decision-maker). To force inference
into a decision-theoretic mold through the choice of a loss function allows
for a clariﬁcation of the way inferential tools should be evaluated, and
therefore implies a conscious (although subjective) choice of the retained
optimality. In addition, when the decision-theoretic framework is com-
pleted by the choice of a prior distribution, the above inferential goals
are automatically satisﬁed, since the Bayesian approach usually leads
to a unique procedure, depending on the requested properties and the
prior knowledge. Obviously, uniqueness of the decision procedure is not
a suﬃcient validation per se, since many meaningless, although unique,
procedures can be proposed instantly.
The important feature of a Bayesian approach is, thus, that Bayes es-
timators are derived by an eminently logical process: starting from re-
quested properties, summarized in the loss function and the prior distri-
bution, the Bayesian approach derives the best solution satisfying these
properties. On the contrary, classical procedures are ad hoc in the sense
that they start from an “arbitrary” estimator (maximum likelihood es-
timator, least-squares estimator, etc.) and then examine its frequentist
properties, not necessarily in a decision-theoretic setting and with no
pretension to global optimality, as shown by the Stein eﬀect. In other
cases, classical approaches also establish a criterion for the choice of
an estimator (best unbiased estimator, best equivariant estimator, uni-
formly most powerful test, etc.), but they cannot provide a universal
method, that is, an algorithm, for the derivation of optimal estimators
(see also point (10) below), and it is even sometimes necessary to restrict
further the class of considered estimators as, for instance, in the case of
the uniformly most powerful unbiased tests.
This opposition in the logical foundations of the two theories reinforces
the coherence of the Bayesian approach, since it is the only one—when in-
corporating the case of the best equivariant estimators as a Bayesian esti-
mation method under the invariant Haar measure—to provide a universal

512
A Defense of the Bayesian Choice
11
and implementable process stemming from inferential requirements.
(7) Implementing the Likelihood Principle
The Likelihood Principle, as shown in Chapter 1, is based on the quite
logical Suﬃciency and Conditionality Principles. Therefore, it should
always direct the choice of estimation procedures, adding a desirable
property to those already discussed in point (6). The Bayesian paradigm
provides an implementation technique for this principle, since it allows
for the derivation of decisions compatible with these diﬀerent require-
ments.
Moreover, while formally incorporating the maximum likelihood estima-
tion method as a particular case (for π(θ) = 1), the Bayesian approach
can also avoid some likelihood paradoxes such as those presented in Sec-
tion 4.1, by using the Jeﬀreys noninformative distributions, even though
these priors are not entirely compatible with the Likelihood Principle.
An additional important advantage of the Bayesian approach, compared
with the maximum likelihood method, is that it can also incorporate the
requirements of a loss function, and thus enter into the framework of De-
cision Theory, while being acceptable for the Likelihood Principle.
(8) Looking for optimal frequentist procedures
From the point of view of frequentist theory, the most convincing argu-
ment in favor of the Bayesian approach is that it intersects widely with
the three notions of classical optimality, namely, minimaxity, admissibil-
ity and equivariance. Indeed, we saw in Chapters 2, 8 and 9 that most
estimators that are optimal according to one of these criteria are Bayes
estimators or limits of Bayes estimators (the notion of limit depending
on the context). Thus, not only is it possible to produce Bayes estimators
that satisfy one, two, or three of the optimality criteria, but, more impor-
tantly, the Bayes estimators are essentially the only ones to achieve this
aim. Therefore, a frequentist statistician may be opposed to any subjec-
tive input in his inferential treatment and still remain in agreement with
his principles by using only Bayes or generalized Bayes estimators, since
most of them behave satisfactorily.5 Moreover, these estimators are easy
to derive, compared with the choice of an ad-hoc estimator and the sub-
sequent veriﬁcation that it is actually a limit of Bayes estimators. From
this point of view, prior distributions are again considered an inferen-
tial tool, not an exhaustive summary of the prior information, but their
shape and their posterior uses obviously remain the same. The optimal-
ity of the Bayes procedures also holds for asymptotic criteria because,
under the conditions ensuring eﬃciency of the maximum likelihood es-
timator, most Bayes estimators are asymptotically eﬃcient and become
5 In this regard, notice that the frequentist pretensions to objectivity (as opposed to
the Bayesian inherent subjectivity) are actually quite limited. In fact, it is necessary
to select the estimators to be compared, and the choice of these estimators is partly
subjective.

11.
A Defense of the Bayesian Choice
513
equivalent to the maximum likelihood estimator as the sample size in-
creases (see Lehmann (1983) and Ibragimov and Has’minskii (1982)),
even though this type of optimality is less important in our opinion.
(9) Solving the actual problem
It is also necessary to provide an alternative to the frequentist approach
from a practical point of view. In fact, frequentist methods are justi-
ﬁed on a long-term basis. For instance, a conﬁdence interval at level
95% used for many independent problems will have a success rate close
to 95%, which is satisfactory for the statistician. On the contrary, for
a decision-maker (“the client”), these long-term properties have little
appeal because she is interested by the performances of the proposed
procedure for the problem at hand. For instance, the fact that a drug
is successful 99% of the time is not reassuring for a particular patient,
compared with that patient’s chances of recovery! This request obviously
calls for an inference that is conditional on x, and thus brings us back
to the Bayesian approach (see point (2)).
This argument does not seem to apply to statistical settings involving
repeated experiments where the decision is taken by the same individ-
ual, as in quality control. But such settings may also justify a Bayesian
implementation, since they are most likely to allow the researcher to
borrow strength from previous studies through a prior distribution.
(10) Computing procedures as a minimization problem
An important point in favor of the Bayesian choice is that the Bayesian
procedures are easier to compute than procedures of alternative theo-
ries. This assertion may appear paradoxical when considering the devel-
opments of Chapters 6 and 10 and, for instance, the diﬃculties encoun-
tered in the treatment of ﬁnite mixtures; more generally, we saw that the
Bayes estimators are seldom derived in closed form, except in the rather
special case of conjugate distributions. However, an additional appeal of
the Bayesian approach is to provide a universal method for the compu-
tation of Bayes estimators, whatever the loss and the distribution of the
observations are, which is to minimize the posterior loss, even if such
minimization requires a call to numerical or Monte Carlo algorithms.
On the contrary, the frequentist theory does not provide any indication
of the derivation of minimax or admissible estimators, except to use a
Bayesian
approach
through
least-favorable
priors
or
proper
distributions.6 Similarly, the only procedure providing a general deriva-
tion of best equivariant estimators entails using Haar measures and the
corresponding Bayesian representation, as shown in Chapter 9.
6 Although they both minimize losses, the main diﬀerence between the two approaches
is that, for the frequentist approach, the minimization is done on a functional space—
the space of estimators—whereas the Bayesian approach carries out the minimization
on a decision space—the space of estimates. The respective complexities of these two
spaces are, generally, considerably diﬀerent.

514
A Defense of the Bayesian Choice
11
From another point of view, maximum likelihood estimators also pro-
ceed from a general optimization program, but their derivation can get
quite complicated and, more importantly, this method does not provide
a complete inferential scope. Moreover, the Bayes estimators allow for
integral representations under usual losses, whereas the maximum like-
lihood method does not necessarily lead to an estimator. For instance,
this is the case for normal mixtures, where the likelihood is not bounded,
or in settings where there are several maxima of the likelihood function.
From a pragmatic and computational point of view, it can be argued
that the eﬀective calculation of the Bayes estimators is often more deli-
cate since it usually involves multiple integrations. Although a concern,
this type of drawback takes place at a material (or software) level, in the
sense that it should progressively disappear as computational methods
evolve and improve. This defect is indeed of another magnitude, though
the opening provided by Markov chain Monte Carlo methods, and the
subsequent derivation of Bayesian solutions in many new domains, shows
that computational issues may be an important slowing factor. Never-
theless, what really matters is the existence of a unique process leading
to the Bayes estimator, whatever the inferential problem, the loss and
the prior distribution. This perspective deﬁnitely singles out Bayesian
analysis.
The reader still skeptical about the advantages of a Bayesian approach
is referred to the books mentioned in the previous chapters, in particular
to Jeﬀreys (1961), Lindley (1971), Berger (1985a, §4.1 and §4.12), Berger
and Wolpert (1988), Bernardo and Smith (1994), Carlin and Louis (2000a)
and Gelman et al. (2000). Smith (1984) also provides a similar list of jus-
tiﬁcations for the Bayesian choice.
For the sake of objectivity, we should also present a corresponding list of
criticisms of the Bayesian approach by other statistical approaches. How-
ever, we believe none of these criticisms bring out strong incoherences of
the Bayesian paradigm.7 Therefore, we will only consider below three issues
about the prior distributions, which are the basis of the Bayesian paradigm
and the focus of most criticisms.
(i) The passage from prior information, which can be vague or poorly de-
ﬁned, to the prior distribution is not explained by the Bayesian paradigm.
A partial, although superﬁcial, answer to this point would be that a
similar criticism applies to the sampling distribution, which is always
assumed to be known exactly. In many cases, and under most theories,
modeling strongly inﬂuences the further developments of the analysis,
but it cannot be formalized as precisely as these subsequent steps. The
diversity of information sources, the various degrees of precision of this
7 Notice, however, that there are non-consistent Bayes estimators, as discussed in Note
1.8.4.

11.
A Defense of the Bayesian Choice
515
information and the assessment of the consequences of the prior distri-
bution selection keep modeling closer to an art than to science. Further-
more, we saw in Note 3.8.1 that some coherence axioms on the prior
likelihood ordering justify the existence of a prior distribution, albeit on
a cruder σ-algebra than expected.
From a practical point of view, the development of a prior distribution
relies on the ability of individuals to represent their knowledge, and the
limitations of this knowledge, in terms of probabilities. That individuals
are not presently able to do so does not imply that they will never be able
to assess probability distributions, nor that they should not be trained
toward this goal. In fact, a proper training could get us closer to this
aim, in the same sense that social evolution has allowed the majority of
individuals to deal constantly with ﬁgures. (See also Smith (1988).)
Another point worth mentioning is that Bayesian analysis provides, in
addition, some tools to deal with imprecisions on the prior distribution,
through robustness and hierarchical analyses. The important aspect of
the partial arbitrariness associated with the choice of a prior distribution
is the inﬂuence of the prior information modeling on the resulting infer-
ence. In fact, diﬀerent modelings can provide similar inferences. When
discrepancies occur, it is necessary to assess more thoroughly the in-
ﬂuence of the choice of the prior by a sensitivity analysis, in order to
expose the inﬂuential factors of the prior modeling, instead of rejecting
the available information. This very exposition of the inﬂuential factors
is actually an additional advantage of a Bayesian analysis (see below).
(ii) Subjectivity is nothing but a pretext for all kinds of abductions, including
the choice of the most advantageous procedures.
Again, a similar criticism could be addressed to frequentist methods
over the choice of the loss or of the estimator to be studied; for instance,
Brown (1980) shows that, for every dimension p0, there exists a loss
function such that the Stein eﬀect occurs only when the dimension of
the problem is larger than p0 (see Note 2.8.2).
Nonetheless, the above remark is justiﬁed, since the recourse to an ad-
ditional factor in the inferential model can always be diverted and mis-
appropriated. Dirac masses as prior distributions are a straightforward
illustration of such a danger, but there are also subtler devices to pro-
duce inference at will. This is unfortunately the price we have to pay
for greater freedom and superior power of adaptivity, but an implicit
requirement of the Bayesian paradigm is, however, that the choice of
the prior distribution be justiﬁable (or testable), in the sense that the
statistician must be able to account for the passage from prior infor-
mation to prior distribution—even if the justiﬁcation is partly based on
computational simplicity, or personal feelings.
This possibility of veriﬁcation is quite similar to the imperative of the
repeatability of the experiments in other ﬁelds, and is not directly present

516
A Defense of the Bayesian Choice
11
in alternative statistical approaches, which reﬂect an inherent ambigu-
ity about the choice of an estimation procedure, for instance, between
the maximum likelihood estimator and the least-squares estimator. It
can actually be argued to the contrary, namely, that the Bayesian ap-
proach is essentially more objective than other inferential methods be-
cause, ﬁrstly, it separates the diﬀerent subjective inputs of the inferential
process (sample distribution, prior, loss function), thus leaving ground
for possible modiﬁcations, and, secondly, it develops objective tools to
assess the inﬂuence of the prior distribution (noninformative distribu-
tions, sensitivity analysis, etc.). In this regard, Poincar´e (1902) brings
an additional argument following the quotation provided in point (5):
We have, for one thing, to use a language and our language is entirely
made of preconceived ideas and has to be so. However, these are unconscious
preconceived ideas, which are a million times more dangerous than the other
ones. Were we to assert that if we are including other preconceived ideas,
consciously stated, we would aggravate the evil! I do not believe so: I rather
maintain that they would balance one another.
The Stopping Rule Principle illustrates this objectivity, since the Bayesian
decision is independent of the stopping criterion, therefore is not inﬂu-
enced by the subjective motivations that led to the resulting sample size.
Again, in a frequentist framework, the choice of the statistical model and
the loss function are equally determining factors that are usually over-
looked, or ”swept under the carpet” (Good (1973)).
(iii) In a completely noninformative setting, the choice of the so-called non-
informative prior has no justiﬁcation whatsoever, and only stands as a
pretext for an extension of the Bayesian scope.
Although we see no harm in extending the Bayesian scope, points (1),
(2), (3), (4), and (6) partly address this issue. In fact, in a noninformative
setting, while the prior distribution cannot correspond to a modeling of
the prior information, it can still be perceived as an eﬃcient inferential
tool.8 In this sense, noninformative Bayesian methods are no more ad
hoc than the maximum likelihood method, since they all stem from the
distribution of the observations representing the only available informa-
tion. If a loss function is also provided by the decision-maker, it con-
stitutes an additional piece of information and the Bayesian approach
can make good use of it; this is not so with the maximum likelihood
method. Furthermore, since these methods often provide the usual es-
timators, they cannot be rejected solely on the grounds that they are
Bayesian. A Bayesian rebuttal would be that the acceptable proper-
ties of these estimators owes to their Bayesian justiﬁcation (see Jaynes
(1980)). Lastly, we insist in most of the above points on the necessity
8 The above criticism proceeds from an argument that rejects the use of the prior
information—except when it is unavailable!

11.
A Defense of the Bayesian Choice
517
of conditioning on x. This conditioning necessarily implies a probabilis-
tic modeling of θ through a prior distribution π(θ), since the maximum
likelihood approach cannot provide a complete statistical inference and
does not usually function as an “objective” distribution on θ.
Jeﬀreys’s approach thus appears as a technique that takes advantage of
the information of the model (that is, of the information brought by
x about θ), while retaining the richness of the Bayesian approach and
the compatibility with intuitive requirements, such as invariance, and in-
cluding most of the usual statistical procedures. The necessity of this ap-
proach is made quite clear in testing theory, where the Neyman–Pearson
approach has been seen to be suboptimal from several perspectives.
Although there are some technical diﬃculties in the treatment of the
nuisance parameters (as in the marginalization paradoxes in Section 3.5),
the reference prior generalization of Bernardo (1979) brings a partial
solution to this problem. The other diﬃculty mentioned in this book,
namely, mixture estimation as presented in Chapter 6, is more delicate
but fundamentally linked to an identiﬁability problem, the maximum
likelihood estimator being also undetermined in this case.
A beneﬁt of such criticisms is that they point out the necessity for
Bayesians to build up rigorously the prior distribution, and to strengthen
noninformative techniques, for instance, by taking advantage of the infor-
mation contained in the loss function. They also push toward a faster de-
velopment of “automatic” (or semiautomatic) techniques of determination
of the prior distribution for a more widespread use of Bayesian methods in
applied Statistics. Bayesian software is already available in this area (see
Note 6.6.2 and Berger (2000)). In connection with the approximation tech-
niques presented in Chapter 6, and developing robustness methods, such
techniques should encourage the diﬀusion of Bayesian methods to a wider
audience. The current explosion of applied Bayesian studies is a sure indi-
cator that this diﬀusion is under way (see Berger (2000)).
Let us conclude this book on a moderating note. External observers
may get perplexed, and even weary, by the continual bickering between
Bayesians and frequentists. Recent developments in Decision Theory have
reinforced the Bayesian foundations of the frequentist optimality notions
(see point (6)), whereas the latest works in the Bayesian robustness area
have been aiming to reduce prior misspeciﬁcation errors by taking into ac-
count these frequentist criteria (such as minimaxity or Bayes minimaxity
as in Kempthorne (1988)). Leading ﬁgures such as James O. Berger are ac-
tively working towards a reformed decision-theoretic framework that would
result in procedures acceptable to both schools, as illustrated by Note 5.7.4.
In practice, it is also often necessary to call for frequentist approximations
when the complete elicitation of a prior distribution gets too complicated,
for instance, when the Fisher information is not available in closed form, or
when the number of parameters is too large. The various developments of

518
A Defense of the Bayesian Choice
the empirical Bayes techniques provide a rather persuasive illustration9 of
the need for an interface between the Bayesian and frequentist approaches.
The Bayesian choice is thus based on the reconciliation of most classi-
cal procedures with a Bayes, or generalized Bayes, analysis, on the strong
appeal of its completeness and global coherence, and also on its ability to
push the inferential process further. It is not based on a categorical rejec-
tion of all classical procedures. This Bayesian choice really stems from the
growing realization that the Bayesian approach is indeed more appropriate
for inference, as well as being more attractive intellectually.
9 However, let us warn the reader of the dangers of an empirical Bayes analysis where
unavoidable resorts to ad-hoc manipulations mar its credibility. The approximation of
a genuine Bayesian analysis provided by the empirical Bayes methods is only partial,
yet gives the illusion of providing a true alternative prior distribution. The subopti-
mality of the resulting empirical Bayes estimators (see Chapter 10) emphasizes the
fundamental diﬀerences between the two approaches.

APPENDIX A
Probability Distributions
We recall here the density and the ﬁrst two moments of most of the distri-
butions used in this book. An exhaustive review of probability distributions
is provided by Johnson and Kotz (1969–1972), or the more recent John-
son et al. (1992, 1994, 1995). The densities are given with respect to the
Lebesgue or the counting measure, depending on the context.
A.1 Normal distribution, Np(θ, Σ)
θ ∈IRp and Σ is a (p × p) symmetric positive-deﬁnite matrix,
f(x|θ, Σ) = (det Σ)−1/2(2π)−p/2e−(x−θ)tΣ−1(x−θ)/2.
IEθ,Σ[X] = θ and IEθ,Σ[(X −θ)(X −θ)t] = Σ.
When Σ is not deﬁnite, the Np(θ, Σ) distribution has no density with
respect to Lebesgue measure on IRp. For p = 1, the log-normal distribution
is deﬁned as the distribution of eX when X ∼N(θ, σ2).
A.2 Gamma distribution, G(α, β)
α, β > 0,
f(x|α, β) =
βα
Γ(α)xα−1e−βxII[0,+∞)(x).
IEα,β[X] = α/β and varα,β(X) = α/β2.
Particular cases of the gamma distribution are the Erlang distribution,
G(α, 1), the exponential distribution G(1, β) (denoted by Exp(β)), and the
chi-squared distribution, Ga(ν/2, 1/2) (denoted by χ2
ν). Notice also that the
opposite convention is sometimes adopted for the parameter, namely that
G(α, β) may also be noted as G(α, 1/β). See, e.g., Berger (1985).
A.3 Beta distribution, Be(α, β)
α, β > 0,
f(x|α, β) = xα−1(1 −x)β−1
B(α, β)
II[0,1](x) ,
where
B(α, β) = Γ(α)Γ(β)
Γ(α + β) .

520
Probability Distributions
A
IEα,β[X] = α/(α + β) and varα,β(X) = αβ/[(α + β)2(α + β + 1)].
The beta distribution can be obtained as the distribution of Y1/(Y1 +Y2)
when Y1 ∼G(α, 1) and Y2 ∼G(β, 1).
A.4 Student’s t-distribution, Tp(ν, θ, Σ)
ν > 0, θ ∈IRp, and Σ is a (p × p) symmetric positive-deﬁnite matrix,
f(x|ν, θ, Σ) = Γ((ν + p)/2)/Γ(ν/2)
(det Σ)1/2(νπ)p/2

1 + (x −θ)tΣ−1(x −θ)
ν
−(ν+p)/2
.
IEν,θ,Σ[X] = θ (ν > 1) and IEθ,Σ[(X −θ)(X −θ)t] = νΣ/(ν −2) (ν > 2).
When p = 1, a particular case of Student’s t-distribution is the Cauchy
distribution, C(θ, σ2), which corresponds to ν = 1. Student’s t-distribution
Tp(ν, 0, Ip) can be derived as the distribution of X/Z when X ∼Np(0, Ip)
and νZ2 ∼χ2
ν.
A.5 Fisher’s F-distribution, F(ν, ϱ)
ν, ϱ > 0,
f(x|ν, ϱ) = Γ((ν + ϱ)/2)νϱ/2ϱν/2
Γ(ν/2)Γ(ϱ/2)
x(ν−2)/2
(ν + ϱx)(ν+ϱ)/2 II[0,+∞)(x).
IEν,ϱ[X] = ϱ/(ϱ−2) (ϱ > 2) and varν,ϱ(X) = 2ϱ2(ν+ϱ−2)/[ν(ϱ−4)(ϱ−2)2]
(ϱ > 4).
The distribution F(p, q) is also the distribution of (X−θ)tΣ−1(X−θ)/p
when X ∼Tp(q, θ, Σ). Moreover, if X ∼F(ν, ϱ), ϱX/(ν + ϱX) ∼Be(ν, ϱ).
A.6 Inverse gamma distribution, IG(α, β)
α, β > 0,
f(x|α, β) =
βα
Γ(α)
e−β/x
xα+1 II[0,+∞[(x).
IEα,β[X] = β/(α−1) (α > 1) and varα,β(X) = β2/((α−1)2(α−2)) (α > 2).
This distribution is the distribution of X−1 when X ∼G(α, β).
A.7 Noncentral chi-squared distribution, χ2
ν(λ)
λ ≥0,
f(x|λ) = 1
2(x/λ)(p−2)/4I(p−2)/2(
√
λx)e−(λ+x)/2.
IEλ[X] = p + λ and varλ(X) = 3p + 4λ.
This distribution can be derived as the distribution of X2
1 + · · · + X2
p
when Xi ∼N(θi, 1) and θ2
1 + . . . + θ2
p = λ.

A
Probability Distributions
521
A.8 Dirichlet distribution, Dk(α1, . . . , αk)
α1, . . . , αk > 0 and α0 = α1 + · · · + αk,
f(x|α1, . . . , αk) =
Γ(α0)
Γ(α1) . . . Γ(αk)xα1−1
1
. . . xαk−1
k
II{
xi=1}.
IEα[Xi] = αi/α0, var(Xi) = (α0 −αi)αi/[α2
0(α0 + 1)] and cov(Xi, Xj) =
−αiαj/[α2
0(α0 + 1)] (i ̸= j).
As a particular case, notice that (X, 1 −X) ∼D2(α1, α2) is equivalent
to X ∼Be(α1, α2).
A.9 Pareto distribution, Pa(α, x0)
α > 0 and x0 > 0,
f(x|α, x0) = α xα
0
xα+1 II[x0,+∞[(x).
IEα,x0[X] = αx0/(α −1) (α > 1) and varα,x0(X) = αx2
0/[(α −1)2(α −2)]
(α > 2).
A.10 Binomial distribution, B(n, p).
0 ≤p ≤1,
f(x|p) =
n
x

px(1 −p)n−xII{0,...,n}(x).
IEp(X) = np and var(X) = np(1 −p).
A.11 Multinomial distribution, Mk(n; p1, . . . , pk)
pi ≥0 (1 ≤i ≤k) and 
i pi = 1,
f(x1, . . . , xk|p1, . . . , pk) =

n
x1 . . . xk

k

i=1
pxi
i II
xi=n.
IEp(Xi) = npi, var(Xi) = npi(1 −pi), and cov(Xi, Xj) = −npipj (i ̸= j).
Notice that, if X ∼Mk(n; p1, . . . , pk), Xi ∼B(n, pi), and that the bino-
mial distribution X ∼B(n, p) corresponds to (X, n−X) ∼M2(n; p, 1−p).
A.12 Poisson distribution, P(λ)
λ > 0,
f(x|λ) = e−λ λx
x! IIIN(x).
IEλ[X] = λ and varλ(X) = λ.

522
Probability Distributions
A
A.13 Negative Binomial distribution, Neg(n, p)
0 ≤p ≤1,
f(x|p) =
n + x + 1
x

pn(1 −p)xIIIN(x).
IEp[X] = n(1 −p)/p and varp(X) = n(1 −p)/p2.
A.14 Hypergeometric distribution, Hyp(N; n; p)
0 ≤p ≤1, n < N and pN ∈IN,
f(x|p) =
	pn
x

	(1−p)N
n−x

	N
n

II{n−(1−p)N,...,pN}(x)II{0,1,...,n}(x).
IEN,n,p[X] = np and varN,n,p(X) = (N −n)np(1 −p)/(N −1).

APPENDIX B
Usual Pseudo-random Generators
This appendix provides some pseudo-random generators for the usual prob-
ability distribution. They can be of use in the implementation of the Monte
Carlo techniques described in Chapter 6. Additional details about their
performances, their limitations, and their justiﬁcation are given in Devroye
(1985), Fishman (1996), Gentle (1998), and Robert and Casella (1999. 2004,
Chapter 2). Notice that these algorithms should not be used blindly: for
extreme values of the parameters or extreme simulation needs, their eﬃ-
ciency decreases rapidly. In fact, when pseudo-random generators are al-
ready available on the machine (e.g., in Gauss, R, or Mathematica), those
are reliable enough to be used directly. The algorithms below all depend
on the generation of uniform random variables on [0, 1] (see Note 6.6.1).
B.1 Normal distribution, N(0, 1)
The Box–Muller method (1958) provides two independent normal observa-
tions out of two uniform random variables.
1. Generate U1, U2.
2. Take
x1
=
1
−2 log U1 cos(2πU2),
x2
=
1
−2 log U1 sin(2πU2).
B.2 Exponential distribution, Exp(λ)
Given that the c.d.f. of the exponential distribution is 1 −e−λx on IR+, it
can be inverted as follows.
1. Generate U.
2. Take x = −log(U)/λ.
This generator can also be used for the geometric distribution Geo(p)
because, if x ∼Geo(p), P(x = r) = P(r ≤E < r + 1), with E ∼
Exp(−log(1 −p)).

524
Usual Pseudo-random Generators
B
B.3 Student’s t-distribution, T (ν, 0, 1)
Kinderman et al. (1977) provide an alternative to the generation of a normal
random variable and a chi-square random variable.
1. Generate U1, U2.
2. If U1 < 0.5, x = 1/(4U1 −1) and v = x−2U2;
otherwise, x = 4U1 −3 and v = U2.
3. If v < 1 −(|x|/2) or v < (1 + (x2/ν))−(ν+1)/2, take x;
otherwise, repeat.
B.4 Gamma distribution, G(α, 1)
The simulation methods diﬀer according to the value of α (notice that the
scale factor β can be assumed to be 1). When α > 1, the Cheng and Feast
algorithm (1979) is:
0. Deﬁne c1 = α −1, c2 = (α −(1/6α))/c1, c3 = 2/c1, c4 = 1 + c3 and
c5 = 1/√α.
1. Repeat
generate U1, U2 and take U1 = U2 + c5(1 −1.86U1) if α > 2.5
until 0 < U1 < 1.
2. W = c2U2/U1.
3. If c3U1 + W + W −1 ≤c4 or c3 log U1 −log W + W ≤1, take c1W;
otherwise, repeat.
If α is very large (α > 50), it is better to use a normal approximation based
on the Central Limit Theorem.
When α < 1, a possible algorithm is:
1. Generate U and y ∼G(α + 1, 1).
2. Take y U 1/α.
Ahrens and Dieter (1974) propose the following alternative.
1. Generate U0, U1.
2. If U0 > e/(e + α), x = −log{(α + e)(1 −U0)/αe} and y = xα−1;
otherwise, x = {(α + e)U0/e}1/α and y = e−x.
3. If U1 < y, take x;
otherwise, repeat.
The beta, Fisher, and chi-squared distributions can also be simulated
using these algorithms, since they can be derived from the gamma distri-
bution by elementary transformations (see Appendix A). Ahrens and Dieter
(1974) and Schmeiser and Shalaby (1980) provide alternative algorithms.

B
Usual Pseudo-random Generators
525
B.5 Binomial distribution, B(n, p)
When n is reasonably small (n ≤30), an elementary algorithm is to gen-
erate n uniform random variables and to count those less than p. For large
n’s, Knuth (1981) provides an alternative algorithm.
0. Deﬁne k = n, θ = p and x = 0.
1. Repeat
i = [1 + kθ]
v ∼Be(i, k + 1 −i)
if θ > v, θ = θ/v and k = i −1;
otherwise, x = x + i, θ = (θ −v)/(1 −v) and k = k −i
until k ≤K.
2. For i = 1, 2, . . ., k,
generate Ui
if Ui < p, x = x + 1.
3. Take x.
The constant K can be chosen as a function of n in order to increase the
eﬃciency of the algorithm.
B.6 Poisson distribution, P(λ)
Again, if λ is reasonably small (λ < 30), a simple algorithm is to generate
uniform variables, in relation to the Poisson process.
0. Take p = 1, N = 0, c = e−λ.
1. Repeat
generate Ui
p = pUi, N = N + 1
until p < c.
3. Take x = N −1.
For large λ’s, Atkinson (1979) proposes a more eﬃcient alternative.
0. Deﬁne c = 0.767−(3.36/λ), β = π(3λ)−1/2, α = βλ, k = log c−λ−log β.
1. Repeat
generate U1
x = [α −log((1 −U1)/U1)]/β
until x > −1/2.
2. Generate U2.
3. Take N = [x + 0.5].

526
Usual Pseudo-random Generators
B
4. If α −βx + log{U2
7
[1 + exp(α −βx)2]} ≤k + N log λ −log N!, take N;
otherwise, repeat.
For large n’s, the negative binomial distribution Neg(n, p) can also be
generated from this algorithm since, if y ∼G(n, (1 −p)/p) and x|y ∼P(y),
then x ∼Neg(n, p) (see Devroye (1985)).

APPENDIX C
Notations
C.1 Mathematical
A ≺B
(B −A) is a positive deﬁnite matrix
|A|
determinant of the matrix A
a+
max (a, 0)
Cp
n,
	n
p

binomial coeﬃcient
Δf(z)
Laplacian of f(z), (∂2/∂z2
i )f(z)
Δ(g)
multiplier acting on a group
f(t) ∝g(t)
the functions f and g are proportional
1F1(a; b; z)
conﬂuent hypergeometric function
F −
generalized inverse of F
Γ(x)
gamma function (x > 0)
h = (h1, . . . , hn) = {hi} boldface signiﬁes a vector
H = {hij}
uppercase signiﬁes matrices
I, 1, J = 11′
Identity matrix, vector of ones, and matrix of ones
IIA(t)
indicator function (1 if t ∈A, 0 otherwise)
Iν(z)
modiﬁed Bessel function (z > 0)
λmax(A)
largest eigenvalue of the matrix A
	
n
p1...pn

multinomial coeﬃcient
∇f(z)
gradient of f(z), the vector with coeﬃcients
(∂/∂zi)f(z) (f(z) ∈IR and z ∈IRp)
∇tf(z)
divergence of f(z), (∂/∂zi)f(z)
(f(z) ∈IRp and z ∈IR)
|| · ||T V
total variation norm
O(n), o(n)
big “Oh”, little “oh.” As n →∞, O(n)
n
→constant,
or Op(n), op(n)
o(n)
n
→0, and the subscript p denotes in probability
Ψ(x)
digamma function, (d/dx)Γ(x) (x > 0)
supp(f)
support of f
tr(A)
trace of the matrix A
|x| = (
i x2
i )1/2
Euclidean norm
[x] or ⌊x⌋
greatest integer less than x
⌈x⌉
smallest integer larger than x
< x, y >
scalar product of x and y in IRp
x ∨y
maximum of x and y
x ∧y
minimum of x and y

528
Notations
C
C.2 Probabilistic
βn
β-mixing coeﬃcient
δθ0(θ)
Dirac mass at θ0
E(θ)
energy function of a Gibbs distribution
E(π)
entropy of the distribution π
IEθ[g(X)]
expectation of g(x) under the distribution X ∼f(x|θ)
IEV [h(V )]
expectation of h(v) under the distribution of V
IEπ[h(θ)|x]
expectation of h(θ) under the distribution of θ
conditional on x, π(θ|x)
i.i.d.
independent and identically distributed
F(x|θ)
cumulative distribution function of X,
conditional on the parameter θ
f(x|θ)
density of X, conditional on the parameter θ,
with respect to Lebesgue or counting measure
λ(dx)
Lebesgue measure, also denoted by dλ(x)
νr, νℓ
right and left Haar measures
Pθ
probability distribution, indexed by the parameter θ
Φ(t)
cumulative distribution function of the normal distribution
ϕ(t)
density of the normal distribution N(0, 1)
X, Y
random variable (uppercase)
X ∼f(x|θ)
X is distributed with density f(x|θ)
(X, P, B)
Probability triple: sample space, probability distribution,
and σ-algebra of sets
C.3 Distributional
B(n, p)
binomial distribution
Be(α, β)
beta distribution
C(θ, σ2)
Cauchy distribution
Dk(α1, . . . , αk)
Dirichlet distribution
Exp(λ)
exponential distribution
F(p, q)
Fisher’s F-distribution
Ga(α, β)
gamma distribution
IG(α, β)
inverse gamma distribution
χ2
p
chi-squared distribution
χ2
p(λ)
noncentral chi-squared distribution
with noncentrality parameter λ
Mk(n; p1, .., pk)
multinomial distribution
N(θ, σ2)
univariate normal distribution
Np(θ, Σ)
multivariate normal distribution
Neg(n, p)
negative binomial distribution
P(λ)
Poisson distribution
Pa(x0, α)
Pareto distribution
Tp(ν, θ, Σ)
multivariate Student’s t-distribution
U[a,b]
continuous uniform distribution

C
Notations
529
We(α, c)
Weibull distribution
Wk(p, Σ)
Wishart distribution
C.4 Decisional
D
decision space
G
group acting on X
¯g
element of ¯G associated to g ∈G
¯G
group induced by G acting on Θ
˜g
element of ˜G associated to g ∈G
˜G
group induced by G acting on D
L(θ, δ)
loss function of δ in θ
M0
model under consideration
R(θ, δ)
frequentist risk of δ in θ
r(π, δ)
the Bayes risk of δ for the prior distribution π
ϱ(π, δ|x)
posterior risk of δ for the prior distribution π
Θ
parameter space
X
observation space
C.5 Statistical
Bπ
12(x), B12
Bayes factor
BA
12(x), BG
12, BM
12
pseudo-Bayes factor
B
lower bound on Bayes factor
Cα
conﬁdence region
δJS(x)
James–Stein estimator
δπ(x)
Bayes estimator
δ+(x)
positive-part James–Stein estimator
δ⋆(x)
randomized estimator
H0
null hypothesis
H1
alternative hypothesis
I(θ)
Fisher information
L(θ, δ)
loss function, loss of estimating θ with δ
L(θ|x)
likelihood function, a function of θ for ﬁxed
x, mathematically identical to f(x|θ)
ℓ(θ|x)
the logarithm of the likelihood function
LP(θ|x), ℓP(θ|x)
proﬁle likelihood
m(x)
marginal density
P≻
Pitman closeness domination
π(θ)
generic prior density for θ
πJ(θ)
Jeﬀreys prior density for θ
π(θ|x)
generic posterior density θ
s2
sample variance
θ, λ
parameters (lowercase Greek letters)

530
Notations
C
Θ, Ω
parameter space (uppercase script Greek letters)
¯x
sample mean
X, Y
sample space (uppercase script Roman letters)
X∗, Y ∗, x∗, y∗
latent or missing variables (data)
C.6 Markov chains
AR(p)
autoregressive process of order p
ARMA(p, q)
autoregressive moving average process of order (p, q)
Kϵ
kernel of the resolvant
MA(q)
moving average process of order q
P(x, A), K(x, y)
transition kernel
P m(x, A)
transition kernel of the chain (Xmn)n
Pμ(·)
probability distribution of the chain (Xn)
with initial state X0 ∼μ
Px0(·)
probability distribution of the chain (Xn)
with initial state X0 = x0
Xt, X(t)
generic element of a Markov chain

References
Abramovich, F., Spatinas, T. and Silverman, B.W. (1998) Wavelet thresh-
olding via a Bayesian approach. J. Roy. Statist. Soc., Ser. B 60, 725–749.
Abramowitz, M. and Stegun, I. (1964) Handbook of Mathematical Func-
tions. Dover, New York.
Adams, M. (1987) William Ockham. University of Notre Dame Press, Notre
Dame, Indiana.
Ahrens, J. and Dieter, U. (1974) Computer methods for sampling from
gamma, beta, Poisson and binomial distributions. Computing 12, 223–246.
Aitkin, M. (1991) Posterior Bayes factors (with discussion). J. Roy. Statist.
Soc., Ser. B 53, 111–142.
Akaike, H. (1978) A new look at the Bayes procedure. Biometrika 65,
53–59.
Akaike, H. (1983) Information measure and model selection. Bull. Int.
Statist. Inst. 50, 277–290.
Alam, K. (1973) A family of admissible minimax estimators of the mean
of a multivariate normal distribution. Ann. Statist. 1, 517–525.
Albert, J.H. (1981) Simultaneous estimation of Poisson means. J. Multi-
variate Analysis 11, 400–417.
Albert, J.H. (1988) Computational methods using a Bayesian hierarchical
generalized linear model. J. Amer. Statist. Assoc. 83, 1037–1044.
Anderson, T.W. (1984) An Introduction to Multivariate Statistical Analysis
(2nd edition). J. Wiley, New York.
Andrieu, C. and Doucet, A. (1999) Joint Bayesian Detection and Estima-
tion of Noisy Sinusoids via Reversible Jump MCMC. IEEE Trans. Signal
Proc. 47(10), 2667–2676.
Andrieux, C., Doucet, A. and Fitzgerald, W.J. (2001) On Monte Carlo
Methods for Bayesian Data Analysis. In Nonlinear Dynamics and Statistics,
A. Mees and R.L. Smith (eds.). Birkhauser, Boston.
Angers, J.F. (1987) Development of robust Bayes estimators for a mul-
tivariate normal mean. Ph.D. thesis, Purdue University, West Lafayette,
Indiana.
Angers, J.F. (1992) Use of the Student’s t-prior for the estimation of nor-
mal means: A computational approach. In Bayesian Statistics 4, J.M.
Bernardo, J.O. Berger, A.P. Dawid and A.F.M. Smith (eds.), 567–575.
Oxford University Press, Oxford.

532
References
Angers, J.F. and MacGibbon, K.B. (1990) Hierarchical Bayes estimation in
linear models with robustness against partial prior misspeciﬁcation. Rap-
port n◦69, D´ept. de Math´ematiques et d’Informatique, Universit´e de Sher-
brooke.
Arrow, K.S. (1951) Social Choice and Individual Values. J. Wiley, New
York.
Atkinson, A. (1979) The computer generation of Poisson random variables.
Appl. Statist. 28, 29–35.
Baranchick, A.J. (1970) A family of minimax estimators of the mean of a
multivariate normal distribution. Ann. Math. Statist. 41, 642–645.
Barbieri, M., Liseo, B. and Petrella, L. (1999) Bayes factor at work in a
challenging class of problems. In Model Choice, W. Racugno (ed.), 109–132.
Collana Atti di Congressi, Pitagora Editrice, Bologna.
Bar-Lev, S., Enis, P. and Letac, G. (1994) Models which admit a given
exponential family as an a priori conjugate model. Ann. Statist. 22(3),
1555–1586.
Barnett, G., Kohn, R. and Sheather, S. (1996) Bayesian estimation of an
autoregressive model using Markov chain Monte Carlo. J. Econometrics
74, 237–254.
Barron, A. (1988) The exponential convergence of posterior probabilities
with implication for Bayes estimators of density functions. Tech. report 7,
Dept. of Statistics, University of Illinois.
Barron, A. (1998) Information-theoretic characterization of Bayes perfor-
mances and the choice of priors in parametric and nonparametric prob-
lems (with discussion). In Bayesian Statistics 6, J. Bernardo, J. Berger,
A.P. Dawid and A.F.M. Smith (eds.), 27–52. Oxford University Press,
Oxford.
Barron, A., Schervish, M.J. and Wasserman, L. (1999) The consistency
of posterior distributions in nonparametric problems. Ann. Statist. 27(2),
536–561.
Barnard, G.A. (1949) Statistical inference (with discussion). J. Roy. Statist.
Soc., Ser. B 11, 115–159.
Bartlett, M.S. (1937) Properties of suﬃciency and statistical tests. Proc.
Roy. Soc. London (Series A) 130, 268–282.
Basu, D. (1988) Statistical Information and Likelihood. J.K. Ghosh (ed.),
Springer-Verlag, New York.
Baum, L.E. and Petrie, T. (1966) Statistical inference for probabilistic func-
tions of ﬁnite state Markov chains. Ann. Math. Statist. 37, 1554–1563.
Bauwens, L. (1984) Bayesian Full Information of Simultaneous Equations
Models Using Integration by Monte Carlo. Lecture Notes in Economics and
Mathematical Systems 232. Springer-Verlag, New York.
Bauwens, L. (1991) The “pathology” of the natural conjugate prior density
in the regression model. Ann. d’Eco. et Statist. 23, 49–64.

References
533
Bauwens, L., Lubrano, M. and Richard, J.F. (1999) Bayesian inference in
dynamic econometric models. In Advanced Texts in Econometrics, C.W.J.
Granger and G.E. Mizon (eds.). Oxford University Press, Oxford.
Bauwens, L. and Richard, J.F. (1985) A 1-1 Poly-t random variable gen-
erator with application to Monte Carlo integration. J. Econometrics 29,
19–46.
Bayarri, M.J. and DeGroot, M.H. (1988) Gaining weight: a Bayesian ap-
proach. In Bayesian Statistics 3, J.M. Bernardo, M.H. DeGroot, D. Lindley
and A.F.M. Smith (eds.), 25–44. Oxford University Press, Oxford.
Bayes, T. (1763) An essay towards solving a problem in the doctrine of
chances. Phil. Trans. Roy. Soc. 53, 370–418.
Bechofer, R.E. (1954) A single-sample multiple decision procedure for rank-
ing means of normal populations with known variance. Ann. Math. Statist.
25, 16–39.
Bensmail, H., Celeux, G., Raftery, A.E. and Robert, C.P. (1997) Inference
in model-based cluster analysis. Statist. Comput. 7(1), 1–10.
Beran, R. (1996) Stein estimation in high dimension: A retrospective. In Re-
search Developments in Probability and Statistics: Madan L. Puri Festschrift,
91–110. Universiteit Utrecht.
Berg´e, P., Pommeau, Y. and Vidal, C. (1984) Ordre Within Chaos. J. Wiley,
New York.
Berger, J.O. (1975) Minimax estimation of location vectors for a wide class
of densities. Ann. Statist. 3, 1318–1328.
Berger, J.O. (1976) Admissibility results for generalized Bayes estimators
of a location vector. Ann. Statist. 4, 334–356.
Berger, J.O. (1980a) A robust generalized Bayes estimators and conﬁdence
region for a multivariate normal mean. Ann. Statist. 8, 716–761.
Berger, J.O. (1980b) Improving on inadmissible estimators in continuous
exponential families with applications to simultaneous estimation of gamma
scale parameters. Ann. Statist. 8, 545–571.
Berger, J.O. (1982a) Selecting a minimax estimator of a multivariate nor-
mal mean. Ann. Statist. 10, 81–92.
Berger, J.O. (1982b) Estimation in continuous exponential families: Ba-
yesian estimation subject to risk restrictions and inadmissibility results.
In Statistical Decision Theory and Related Topics 3, S.S. Gupta and J.O.
Berger (eds.), 109–142. Academic Press, New York.
Berger, J.O. (1984a) The robust Bayesian viewpoint (with discussion). In
Robustness of Bayesian Analysis, J. Kadane (ed.). North-Holland, Amster-
dam.
Berger, J.O. (1984b) The frequentist viewpoint and conditioning. In Pro-
ceedings of the Berkeley Conference in Honor of Kiefer and Neyman, L.
Le Cam and R. Olshen (eds.). Wadsworth, Belmont, California.

534
References
Berger, J.O. (1985a) Statistical Decision Theory and Bayesian Analysis
(2nd edition). Springer-Verlag, New York.
Berger, J.O. (1985b) Discussion of ‘Quantifying prior opinion’ by Diaconis
and Ylvisaker. In Bayesian Statistics 2, J.M. Bernardo, M. DeGroot, D.V.
Lindley and A.F.M. Smith (eds.). North-Holland, Amsterdam.
Berger, J.O. (1990a) Robust Bayesian analysis: sensitivity to the prior. J.
Statist. Plann. Inference 25, 303–328.
Berger, J.O. (1990b) On the inadmissibility of unbiased estimators. Statist.
Prob. Letters 5, 71–75.
Berger, J.O. (2000) Bayesian Analysis: A look at today and thoughts of
tomorrow. J. Amer. Statist. Assoc. 95, 1269–1277.
Berger, J.O. and Berliner, L.M. (1986) Robust Bayes and empirical Bayes
analysis with ε-contamined priors. Ann. Statist. 14, 461–486.
Berger, J.O. and Bernardo, J.M. (1989) Estimating a product of means:
Bayesian analysis with reference priors. J. Amer. Statist. Assoc. 84,
200–207.
Berger, J.O. and Bernardo, J.M. (1990) Reference priors in a variance
components problem. Tech. Report # 89–32C, Purdue University, West
Lafayette, Indiana.
Berger, J.O. and Bernardo, J.M. (1992a) Ordered group reference priors
with application to the multinomial problem. Biometrika 25, 25–37.
Berger, J.O. and Bernardo, J.M. (1992b) On the development of the refer-
ence prior method. In Bayesian Statistics 4. J.O. Berger, J.M. Bernardo,
A.P. Dawid and A.F.M. Smith (Eds.). Oxford University Press, London,
35–60. Oxford University Press, Oxford.
Berger, J.O. and Bock, M.E. (1976) Eliminating singularities of Stein-type
estimators of location vectors. J. Roy. Statist. Soc., Ser. B 39, 166–170.
Berger, J.O., Boukai, B. and Wang, Y. (1997) Uniﬁed frequentist and
Bayesian testing of a precise hypothesis (with discussion). Statistical Sci-
ence 12, 133–160.
Berger, J.0., Boukai, B. and Wang, Y. (1999) Simultaneous Bayesian–
Frequentist Sequential Testing of Nested Hypotheses. Biometrika 86, 79–92.
Berger, J.O., Brown, L. and Wolpert, R. (1994) A Uniﬁed Conditional
Frequentist and Bayesian Test for Fixed and Sequential Hypothesis Testing.
Ann. Statist. 22, 1787-1807.
Berger, J.O. and Deely, J.J. (1988) A Bayesian approach to ranking and se-
lection of related means with alternatives to Anova methodology. J. Amer.
Statist. Assoc. 83, 364–373.
Berger, J.O. and Delampady, M. (1987) Testing precise hypotheses (with
discussion). Statist. Science 2, 317–352.
Berger, J.O. and Mortera, J. (1991) Interpreting the stars in precise hy-
pothesis testing. Int. Statist. Rev. 59, 337–353.

References
535
Berger, J.O. and Pericchi, L.R. (1996a) The Intrinsic Bayes Factor for
Model Selection and Prediction. J. Amer. Statist. Assoc. 91, 109–122.
Berger, J.O. and Pericchi, L.R. (1996b) The Intrinsic Bayes Factor for
Linear Models. In Bayesian Statistics 5. J.O. Berger, J.M. Bernardo, A.P.
Dawid and A.F.M. Smith (Eds)., 23–42. Oxford University Press, Oxford.
Berger, J.O. and Pericchi, L.R. (1998) Accurate and Stable Bayesian Model
Selection: the Median Intrinsic Bayes Factor. Sankhya B 60, 1–18.
Berger, J.O. and Pericchi, L.R. (2001) Objective Bayesian methods for
model selection: introduction and comparison. J. Statist. Plan. Inf.
(to
appear).
Berger, J.O., Pericchi, L.R., and Varshavsky, J. (1998) Bayes Factors and
Marginal Distributions in Invariant Situations. Sankhya A 60, 307–321.
Berger, J.O., Philippe, A. and Robert, C.P. (1998) Estimation of quadra-
tic functions: noninformative priors for non-centrality. Statistica Sinica 8,
359–375.
Berger, J.O. and Robert, C.P. (1990) Subjective hierarchical Bayes esti-
mation of a multivariate normal mean: on the frequentist interface. Ann.
Statist. 18, 617–651.
Berger, J.O. and Sellke, T. (1987) Testing a point-null hypothesis: the irrec-
oncilability of signiﬁcance levels and evidence (with discussion). J. Amer.
Statist. Assoc. 82, 112–122.
Berger, J.O. and Srinivasan, C. (1978) Generalized Bayes estimators in
multivariate problems. Ann. Statist. 6, 783–801.
Berger, J.O. and Wolpert, R. (1988) The Likelihood Principle (2nd edition).
IMS lecture notes – Monograph Series, 9. Hayward, California.
Berger, J.O. and Yang, R. (1994a) Noninformative priors and Bayesian
testing for the AR(1) model. Econometric Theory 10, 461–482.
Berger, J.O. and Yang, R. (1994b) Estimation of a covariance matrix using
the reference prior. Ann. Statist. 22, 1195–1211.
Bergman, N., Doucet, A. and Gordon, N.J. (2001) Optimal estimation and
Cram´er-Rao bounds for partial non-Gaussian state-space models. Annals
of the Institute of Statistical Mathematics 53(1), 97-112.
Berliner, L.M. (1991) Likelihood and Bayesian prediction of chaotic models.
J. Amer. Statist. Assoc. 86, 938–952.
Berliner, L.M. (1992) Statistics, probability and chaos. Statist. Science 7,
69–122.
Bernardo, J.M. (1979) Reference posterior distributions for Bayesian infer-
ence (with discussion). J. Roy. Statist. Soc., Ser. B 41, 113–147.
Bernardo, J.M. (1980) A Bayesian analysis of classical hypothesis testing.
In Bayesian Statistics. J.M. Bernardo, M.H. deGroot, D.V. Lindley and
A.F.M. Smith (Eds.). University Press, Valencia.
Bernardo, J.M. and Gir´on, F.J. (1986) A Bayesian approach to cluster anal-
ysis. In Second Catalan International Symposium on Statistics, Barcelona,

536
References
Spain.
Bernardo, J.M. and Gir´on, F.J. (1988) A Bayesian analysis of simple mix-
ture problems. In Bayesian Statistics 3, J.M. Bernardo, M.H. DeGroot,
D.V. Lindley and A.F.M. Smith (eds.), 67–78. Oxford University Press,
Oxford.
Bernardo, J.M. and Smith, A.F.M. (1994) Bayesian Theory. J. Wiley, New
York.
Berry, D.A. and Stangl, D.K. (1996) Bayesian Biostatistics. Marcel Dekker,
New York.
Bertrand, J. (1889) Calcul des Probabilit´es. Gauthier-Villars, Paris.
Besag, J. (1974) Spatial interaction and the statistical analysis of lattice
systems (with discussion). J. Roy. Statist. Soc., Ser. B 36, 192–326.
Besag, J. (1986) Statistical analysis of dirty pictures (with discussion). J.
Roy. Statist. Soc., Ser. B 48, 259–302.
Besag, J. (2000) Markov chain Monte Carlo for statistical inference. Tech.
Report no. 9, University of Washington.
Besag, J. and Green, P.J. (1992) Spatial Statistics and Bayesian computa-
tion (with discussion). J. Roy. Statist. Soc., Ser. B 55, 25–38.
Best, N.G., Cowles, M.K. and Vines, K. (1995) CODA: Convergence di-
agnosis and output analysis software for Gibbs sampling output, Version
0.30. Tech. Report, MRC Biostatistics Unit, University of Cambridge.
Bhattacharya, R.N. and Ghosh, J.K. (1978) Validity of formal Edgeworth
expansion. Ann. Statist. 6, 434–451.
Bhattacharya, R.N. and Rao, R. (1986) Normal approximations and asymp-
totic expansions (2nd edition). J. Wiley, New York.
Bickel, P.J. (1981) Minimax estimation of the mean of a normal distribution
when the parameter space is restricted. Ann. Math. Statist. 9, 1301–1309.
Bickel, P. and Ghosh, J.K. (1990) A decomposition for the likelihood ratio
statistic and the Bartlett correction – a Bayesian argument. Ann. Statist.
18, 1070–1090.
Billingsley, P. (1965) Ergodic Theory and Information. J. Wiley, New York.
Billingsley, P. (1986) Probability and Measure (2nd edition). J. Wiley, New
York.
Billio, M., Monfort, A. and Robert, C.P. (1999) Bayesian estimation of
switching ARMA models. J. Econometrics 93, 229–255.
Bilodeau, M. (1988) On the simultaneous estimation of scale parameters.
Canad. J. Statist. 14, 169–174.
Binder, D. (1978) Bayesian cluster analysis. Biometrika 65, 31–38.
Birnbaum, A. (1962) On the foundations of statistical inference (with dis-
cussion). J. Amer. Statist. Assoc. 57, 269–326.
Bjørnstad, J. (1990) Predictive likelihood: a review. Statist. Science 5,
242–265.

References
537
Blackwell, D. and Girshick, M.A. (1954) Theory of Games and Statistical
Decisions. J. Wiley, New York.
Blattberg, R.C. and George, E.I. (1991) Shrinkage estimation of price and
promotion elasticities: seemingly unrelated equations. J. Amer. Statist. As-
soc. 86, 304–315.
Blyth, C.R. (1951) On minimax statistical decisions procedures and their
admissibility. Ann. Math. Statist. 22, 22–42.
Blyth, C.R. (1972) Some probability paradoxes in choice from among ran-
dom alternatives (with discussion). J. Amer. Statist. Assoc. 67, 366–387.
Blyth, C.R. (1993) Discussion of Robert, Hwang and Strawderman (1993).
J. Amer. Statist. Assoc. 88, 72–74.
Blyth, C.R. and Hutchinson, D. (1961) Tables of Neyman-shortest conﬁ-
dence interval for the binomial parameter. Biometrika 47, 381–391.
Blyth, C.R. and Pathak, P.K. (1985) Does an estimator distribution suﬃce?
In Proc. Berkeley Conf. in Honor of J. Neyman and J. Kiefer 1, L. Le Cam
and A. Olshen (eds.). Wadsworth, Belmont, California.
Bock, M.E. (1985) Minimax estimators that shift towards a hypersphere
for location of spherically symmetric distributions. J. Multivariate Anal.
9, 579–588.
Bock, M.E. (1988) Shrinkage estimators: pseudo-Bayes rules for normal
vectors. In Statistical Decision Theory and Related Topics 4, S.S. Gupta
and J.O. Berger (eds.), 281–297. Springer-Verlag, New York.
Bock, M.E. and Robert, C.P. (1991) Bayes estimators with respect to uni-
form distributions on spheres (I): the empirical Bayes approach. Unpub-
lished report, Purdue University, West Lafayette, Indiana.
B¨ohning, D. (1999) Computer–Assisted Analysis of Mixtures and Applica-
tions. Chapman & Hall, London.
Bondar, J.V. (1987) How much improvement can a shrinkage estimator
give? In Foundations of Statistical Inference, I. McNeill and G. Umphreys
(eds.). Reidel, Dordrecht.
Bondar, J.V. and Milnes, P. (1981) Amenability: a survey for statistical
applications of Hunt-Stein and related conditions on groups. Z. Wahrsch.
verw. Gebiete 57, 103–128.
Boole G. (1854) A Investigation of the Laws of Thought. Walton and
Maberly, London.
Bose, S. (1991) Some properties of posterior Pitman closeness. Comm.
Statist. (Ser. A), 20, 3697–3412.
Bosq, D. and Lecoutre, J.P. (1988) Th´eorie de l’Estimation Fonctionnelle.
Economica, Paris.
Box, G.E.P. and Jenkins, G.M. (1976) Time Series Analysis: Forecasting
and Control. Holden-Bay, San Francisco.
Box, G.E.P. and Muller, M. (1958) A note on the generation of random
normal variates. Ann. Math. Statist. 29, 610–611.

538
References
Box, G.E.P. and Tiao, G.C. (1973) Bayesian Inference in Statistical Anal-
ysis. Addison-Wesley, Reading, Massachusetts.
Brandwein, A. and Strawderman, W.E. (1980) Minimax estimators of loca-
tion parameters for spherically symmetric distributions with concave loss.
Ann. Statist. 8, 279–284.
Brandwein, A. and Strawderman, W.E. (1990) Stein estimation: the spher-
ically symmetric case. Statist. Science 5, 356–569.
Brandwein, A. and Strawderman, W.E. (1991) Generalizations of James-
Stein estimators under spherical symmetry. Ann. Statist. 19, 1639–1650.
Brandwein, A., Strawderman, W.E. and Ralescu, S. (1992) Stein estimation
for non-normal spherically symmetric location families in three dimensions.
J. Multivariate Analysis 42, 35–50.
Brewster, J.F. and Zidek, J.V. (1974) Improving on equivariant estimators.
Ann. Statist. 2, 21–38.
Brockwell and Davis (1998) Introduction to Time Series and Forecasting.
Springer–Verlag, New York.
Broniatowski, M., Celeux, G. and Diebolt, J. (1983) Reconnaissance de
m´elanges de densit´es par un algorithme d’apprentissage probabiliste. In
Data Analysis and Informatics 3, E. Diday (ed.). North-Holland, Amsterdam.
Brown, L.D. (1966) On the admissibility of invariant estimators of one or
more location parameters. Ann. Math. Statist. 37, 1087–1136.
Brown, L.D. (1967) The conditional level of Student’s t-test. Ann. Math.
Statist. 38, 1068–1071.
Brown, L.D. (1971) Admissible estimators, recurrent diﬀusions, and insol-
uble boundary-value problems. Ann. Math. Statist. 42, 855–903.
Brown, L.D. (1975) Estimation with incompletely speciﬁed loss functions.
J. Amer. Statist. Assoc. 70, 417–426.
Brown, L.D. (1976) Notes on Statistical Decision Theory. Unpublished lec-
ture notes, Ithaca, New York.
Brown, L.D. (1978) A contribution to Kiefer’s theory of conditional conﬁ-
dence procedures. Ann. Statist. 6, 59–71.
Brown, L.D. (1980) Examples of Berger’s phenomenon in the estimation of
independent normal means. Ann. Statist. 9, 1289–1300.
Brown, L.D. (1981) A complete class theorem for statistical problems with
ﬁnite sample spaces. Ann. Statist. 9, 1289–1300.
Brown, L.D. (1986) Foundations of Exponential Families. IMS lecture notes
– Monograph Series 6. Hayward, California.
Brown, L.D. (1988) The diﬀerential inequality of a statistical estimation
problem. In Statistical Decision Theory and Related Topics 4, S.S. Gupta
and J.O. Berger (eds.). Springer–Verlag, New York.
Brown, L.D. (1990) An ancilarity paradox which appears in multiple linear
regression (with discussion). Ann. Statist. 18, 471–538.

References
539
Brown, L.D. (1993) Minimaxity, more or less. In Statistical Decision Theory
and Related Topics 5, S.S. Gupta and J.O. Berger (eds.), 1–18. Springer-
Verlag, New York.
Brown, L.D. (2000) An essay on Statistical Decision Theory. J. Amer.
Statist. Assoc. 95, 1277–1282.
Brown, L.D. and Farrell, R.H. (1985) Complete class theorems for estima-
tion of multivariate Poisson means and related problems. Ann. Statist. 8,
377–398.
Brown, L.D. and Hwang, J.T. (1982) A uniﬁed admissibility proof. In Sta-
tistical Decision Theory and Related Topics 3, S.S. Gupta and J.O. Berger
(eds.), 205–230. Academic Press, New York.
Brown, L.D. and Hwang, J.T. (1989) Universal domination and stochas-
tic domination: U-admissibility and U-inadmissibility of the least-squares
estimator. Ann. Statist. 17, 252–267.
Buehler, R.J. (1959) Some validity criteria for statistical inference. Ann.
Math. Statist. 30, 845–863.
Capp´e, O. and Robert, C.P. (2000) MCMC: Ten years and still running! J.
Amer. Statist. Assoc. 95, 1282–1286.
Capp´e, O., Robert, C.P. and Ryd´en, T. (2003) Reversible jump, birth-and-
death, and more general continuous time MCMC samplers. J. Roy. Statist.
Soc., Ser. B 65(3), 679–700.
Carlin, B.P. and Chib, S. (1995), Bayesian Model Choice via Markov Chain
Monte Carlo. J. Roy. Statist. Soc., Ser. B 57, 473–484.
Carlin, B.P. and Gelfand, A. (1990) Approaches for empirical Bayes conﬁ-
dence intervals. J. Amer. Statist. Assoc. 85, 105–114.
Carlin, B.P. and Louis, A. (1996) Bayes and Empirical Bayes Methods for
Data Analysis. Chapman & Hall, London.
Carlin, B.P. and Louis, A. (2000a) Bayes and Empirical Bayes Methods for
Data Analysis (2nd edition). Chapman & Hall, London.
Carlin, B.P. and Louis, A. (2000b) Empirical Bayes: Past, present and
future. J. Amer. Statist. Assoc. 95, 1286–1290.
Caron, N. (1994) Approches alternatives d’une th´eorie non-informative des
tests bay´esiens. Th`ese d’universit´e, D´ept. de Math´ematique, Universit´e de
Rouen.
Carota, C., Parmigiani, G. and Polson, N.G. (1996) Diagnostic measures
for model criticism. J. Amer. Statist. Assoc. 91, 753–762.
Carter, G. and Rolph, J. (1974) Empirical Bayes methods applied to esti-
mating ﬁre alarm probabilities. J. Amer. Statist. Assoc. 69, 882–885.
Casella, G. (1980) Minimax ridge regression estimation. Ann. Statist. 8,
1036–1056.
Casella, G. (1985a) An introduction to empirical Bayes data analysis. Amer.
Statist. 39, 83–87.

540
References
Casella, G. (1985b) Condition number and minimax ridge regression esti-
mation. J. Amer. Statist. Assoc. 80, 753–758.
Casella, G. (1987) Conditionally acceptable recentered set estimators. Ann.
Statist. 15, 1364–1371.
Casella, G. (1990) Estimators with nondecreasing risks: application of a
chi-squared identity. Statist. Prob. Lett. 10, 107–109.
Casella, G. (1992) Conditional inference for conﬁdence sets. Current Issues
in Statistical Inference: Essays in Honor of D. Basu, M. Ghosh and P.K.
Pathak (eds.), 1–12. IMS lectures notes – Monograph Series 17. Hayward,
California.
Casella, G. (1996) Statistical inference and Monte Carlo algorithms (with
discussion). Test 5, 249–344.
Casella, G. and Berger, R. (1987) Reconciling Bayesian and frequentist
evidence in the one-sided testing problem. J. Amer. Statist. Assoc. 82,
106–111.
Casella, G. and Berger, R. (1990) Statistical Inference. Wadsworth, Bel-
mont, California.
Casella, G. and George, E.I. (1992) An introduction to Gibbs sampling.
Ann. Math. Statist. 46, 167-174.
Casella, G. and Hwang, J.T. (1983) Empirical Bayes conﬁdence sets for the
mean of a multivariate normal distribution. J. Amer. Statist. Assoc. 78,
688–698.
Casella, G. and Hwang, J.T. (1987) Employing vague prior information in
the construction of conﬁdence sets. J. Multivariate Anal. 21, 79–104.
Casella, G., Hwang, J.T. and Robert, C.P. (1993a) A paradox in decision-
theoretic set estimation. Statist. Sinica 3, 141–155.
Casella, G., Hwang, J.T.G. and Robert, C.P. (1993b) Loss function for
set estimation. In Statistical Decision Theory and Related Topics V, J.O.
Berger and S.S. Gupta (Eds.) Springer-Verlag, New York., 237–252. Springer–
Verlag, New York.
Casella, G. and Robert, C.P. (1988) Non-optimality of randomized conﬁ-
dence sets. Tech. Report # 88-9, Dept. of Statistics, Purdue University,
West Lafayette, Indiana.
Casella, G., Robert, C.P. and Wells, M.T. (2000) Mixture models, la-
tent variables and partitioned importance sampling. Tech. report 00-15,
CREST, INSEE, Paris.
Casella, G. and Strawderman, W.E. (1981) Estimating a bounded normal
mean. Ann. Statist. 4, 283–300.
Casella, G. and Wells, M. (1993) Discussion of Robert, Hwang and Straw-
derman (1993). J. Amer. Statist. Assoc. 88, 70–71.
Castledine, B. (1981) A Bayesian analysis of multiple-recapture sampling
for a closed population. Biometrika 67, 197–210.

References
541
Castro, I., Conigliani, C. and O’Hagan, A. (1999) Bayesian assessment of
goodness of ﬁt against nonparametric alternatives (with discussion). In
Model Selection, W. Racugno (ed.). Collana Atti di Congressi, Pitagora
Editrice Bologna.
Celeux, G. and Diebolt, J. (1985) The SEM algorithm: a probabilistic
teacher algorithm derived from the EM algorithm for the mixture prob-
lem. Comput. Statist. Quater. 2, 73–82.
Celeux, G. and Diebolt, J. (1990) Une version de type recuit simul´e de
l’algorithme EM. Notes aux Comptes Rendus de l’Acad´emie des Sciences,
310, 119–124.
Celeux, G., Hurn, M. and Robert, C.P. (2000) Computational and infer-
ential diﬃculties with mixture posterior distributions. J. Amer. Statist.
Assoc. 95(3), 957–979.
Cellier, D., Fourdrinier, D. and Robert, C.P. (1989) Robust shrinkage es-
timators of the location parameter for elliptically symmetric distributions.
J. Multivariate Anal. 29, 39–52.
Chamberlain, G. (2000) Econometrics. Springer–Verlag, New York.
Chen, M.H. and Shao, Q.M. (1997) On Monte Carlo methods for estimating
ratios of normalizing constants. Ann. Statist. 25, 1563–1594.
Chen, M.H., Shao, Q.M. and Ibrahim, J.G. (2000) Monte Carlo Methods
in Bayesian Computation. Springer–Verlag, New York.
Cheng, R. and Feast, G. (1979) Some simple gamma variate generators.
Appl. Statist. 28, 290–295.
Chernoﬀ, H. and Yahav, J.A. (1977) A subset selection employing a new
criterion. In Statistical Decision Theory and Related Topics 2, S.S. Gupta
and D. Moore (eds.). Academic Press, New York.
Chib, S. (1995) Marginal likelihood from the Gibbs output. J. Amer. Statist.
Assoc. 90 1313–1321.
Chib, S. and Greenberg (1994) Bayes Inference in Regression Models with
ARMA (p,q) Errors. J. Econometrics 64, 183–206.
Chickering, D.M. and Heckerman, D. (2000) A comparison of scientiﬁc
and engineering criteria for Bayesian model selection. Statist. Comput. 10,
55–62.
Chow, G.C. (1983) Econometrics. McGraw-Hill, New York.
Chow, M.S. (1987) A complete class theorem for estimating a non-centrality
parameter. Ann. Statist. 15, 869–876.
Chow, M.S. and Hwang, J.T. (1990) The comparison of estimators for the
noncentrality of a chi-square distribution. Tech. Report, Dept. of Mathe-
matics, Cornell University, Ithaca, New York.
Chow, Y.S. and Teicher, H. (1988) Probability Theory. Springer–Verlag,
New York.
Chrystal, G. (1891) On some fundamental principles in the theory of prob-
ability. Trans. Actuarial Soc. Edinburgh 2, 421–439.

542
References
Clarke, B. and Barron, A. (1990) Information-theoretic asymptotics of
Bayes methods. IEEE Trans. Inform. Theory 36, 453–471.
Clarke, B. and Barron, A. (1994) Jeﬀreys prior is asymptotically least fa-
vorable under entropy risk. J. Statist. Plan. Inf. 41, 36–60.
Clarke, B. and Wasserman, L. (1995) Information trade-oﬀ. TEST 4, 19–38.
Clevenson, M. and Zidek, J.V. (1975) Simultaneous estimation of the mean
of independant Poisson laws. J. Amer. Statist. Assoc. 70, 698–705.
Clyde, M. (1999), Bayesian model averaging and model search strategies. In
Bayesian Statistics 6, J.M. Bernardo, A.P. Dawid, J.O. Berger, and A.F.M.
Smith (eds.), 157–185. Oxford University Press, Oxford.
Cohen, A. (1972) Improved conﬁdence intervals for the variance of a normal
distribution. J. Amer. Statist. Assoc. 67, 382–387.
Cohen, A. and Sackrowitz, H. (1984) Decision Theoretic results for vector
risks with applications. Statist. Decisions Supplement Issue 1, 159–176.
Cohen, A. and Strawderman, W.E. (1973) Admissible conﬁdence intervals
and point estimators for translation or scale parameters. Ann. Statist. 1,
545–550.
Consonni, G. and Veronese, P. (1987) Coherent distributions and Lindley’s
paradox. In Probability and Bayesian Statistics, R. Viertl (ed.), 111–120.
Plenum, New York.
Cowell, R.G., Dawid, A.P., Lauritzen, S.L. and Spiegelhalter, D.J. (1999)
Probabilistic Networks and Expert Systems. Springer–Verlag, New York.
Cox, D.R. (1958) Some problems connected with statistical inference. Ann.
Math. Statist. 29, 357–425.
Cox, D.R. (1990) Role of models in statistical analysis. Statist. Science 5,
169–174.
Cox, D.R. and Hinkley, D. (1987) Theoretical Statistics. Chapman & Hall,
London.
Cox, D.R. and Reid, N. (1987) Orthogonal parameters and approximate
conditional inference (with discussion). J. Roy. Statist. Soc., Ser. B 49,
1–18.
Crawford, S.L., DeGroot, M.H., Kadane, J.B. and Small, M.J. (1992) Mod-
eling lake-chemistry distributions: Approximate Bayesian methods for es-
timating a ﬁnite-mixture model. Technometrics 34, 441–453.
Cressie, N. (1993) Spatial Statistics. J. Wiley, New York.
Dacunha-Castelle, D. and Gassiat, E. (1999) Testing the order of a model
using locally conic parametrization: population mixtures and stationary
ARMA processes. Ann. Statist. 27(4), 1178–1209.
Dalal, S.R. and Hall, W.J. (1983) Approximating priors by mixtures of
natural conjugate priors. J. Roy. Statist. Soc., Ser. B 45, 278–286.
Dale, A.I. (1991) A History of Inverse Probability. Springer–Verlag, New
York.

References
543
Damien, P., Wakeﬁeld, J. and Walker, S. (1999) Gibbs sampling for Bayesian
non-conjugate and hierarchical models by using auxiliary variables. J. Roy.
Statist. Soc., Ser. B 61(2), 331–344.
Darroch, J. (1958) The multiple-recapture census. I: Estimation of a closed
population. Biometrika 45, 343–359.
Das Gupta, A. (1984) Admissibility in the gamma distribution: two exam-
ples. Sankhya (Ser. A) 46, 395–407.
Das Gupta, A. and Sinha, B.K. (1986) Estimation in the multiparameter
exponential family: admissibility and inadmissibility results. Statist. Deci-
sions 4, 101–130.
Das Gupta, A. and Studden, W. (1988) Frequentist behavior of smallest
volume robust Bayes conﬁdence sets. Tech. Report, Dept. of Statistics,
Purdue University, West Lafayette, Indiana.
Datta, G.S. and Ghosh, M. (1994) On the invariance of noninformative
priors. Technical Report 94-20, Dept. of Statistics, University of Georgia.
Datta, G.S. and Ghosh, M. (1995a) On priors providing frequentist validity
for Bayesian inference. Biometrika 82, 37–45.
Datta, G.S. and Ghosh, M. (1995b) Some remarks on noninformative priors.
J. Amer. Statist. Assoc. 90, 1357–1363.
Dawid, A.P. (1984) Probability Forecasts. Research report, University Col-
lege London.
Dawid, A.P. (1992) Prequential analysis, stochastic complexity and Bay-
esian inference. In Bayesian Statistics 4, J.O. Berger, J.M. Bernardo, A.P.
Dawid and A.F.M. Smith (eds.), 109–121. Oxford University Press, Oxford.
Dawid, A.P., DeGroot, M.H. and Mortera, J. (1993) Coherent combination
of experts’ opinions. In Statistical Decision Theory and Related Topics V,
J.O. Berger and S.S. Gupta (Eds.) Springer-Verlag, New York.
Dawid, A.P. and Lauritzen, S.L. (1993) Hyper Markov laws in the statistical
analysis of decomposable graphical models. Ann. Statist. 21, 1272-1317.
Dawid, A.P., Stone, N. and Zidek, J.V. (1973) Marginalization paradoxes
in Bayesian and structural inference (with discussion). J. Roy. Statist. Soc.,
Ser. B 35, 189–233.
Deely, J.J. and Gupta, S.S. (1968) On the property of subset selection per
order. Sankhya (Ser. A), 30, 37–50.
Deely, J.J. and Lindley, D.V. (1981) Bayes empirical Bayes. J. Amer.
Statist. Assoc. 76, 833–841.
DeGroot, M.H. (1970) Optimal Statistical Decisions. McGraw-Hill, New
York.
DeGroot, M.H. (1973) Doing what comes naturally: Interpreting a tail area
as a posterior probability or as a likelihood ratio. J. Amer. Statist. Assoc.
68, 966–969.
DeGroot, M.H. and Fienberg, S. (1983) The comparison and evaluation of
forecasters. The Statistician 32, 12–22.

544
References
Delampady, M. (1989a) Lower bounds on Bayes factors for interval null
hypotheses. J. Amer. Statist. Assoc. 84, 120–124.
Dellaportas, P. and Forster, J.J. (1996) Markov chain Monte Carlo model
determination for hierarchical and graphical log-linear models. Technical
Report, University of Southampton.
Delampady, M. (1989b) Lower bounds on Bayes factors for invariant testing
situations. J. Multivariate Anal. 28, 227–246.
Delampady, M. and Berger, J.O. (1990) Lower bounds on Bayes factors for
multinomial and chi-squared tests of ﬁt. Ann. Statist. 18, 1295–1316.
Dempster, A.P. (1968) A generalization of Bayesian inference (with discus-
sion). J. Roy. Statist. Soc., Ser. B 30, 205–248.
Dempster, A.P., Laird, N.M. and Rubin, D.B. (1977) Maximum likelihood
from incomplete data via the EM algorithm (with discussion). J. Roy.
Statist. Soc., Ser. B 39, 1–38.
DeRobertis, L. and Hartigan, J.A. (1981) Bayesian inference using intervals
of measures. Ann. Statist. 9, 235–244.
De Santis, F., and Spezzaferri, F. (1997) Alternative Bayes Factors for
model selection. Canad. J. Statist., 25, 503–515.
Devroye, L. (1985) Non-Uniform Random Variate Generation. Springer–
Verlag, New York.
Devroye, L. and Gy¨orﬁ, L. (1985) Nonparametric Density Estimation: the
L1 View. J. Wiley, New York.
Dey, D., M¨uller, P. and Sinha, D. (1998) Practical Nonparametrics and
Semiparametrics in Bayesian Statistical Inference. Lecture notes in Statis-
tics 133, Springer–Verlag, New York.
Diaconis, P. (1988) Bayesian numerical analysis. In Statistical Decision
Theory and Related Topics 4, S. Gupta and J.O. Berger (eds.), 163–176.
Springer–Verlag, New York.
Diaconis, P. and Freedman, D.A. (1986) On the consistency of Bayes esti-
mates. Ann. Statist. 14, 1–26.
Diaconis, P. and Kemperman, J. (1996) Some new tools for Dirichlet priors
(with discussion). In
Bayesian Statistics 5. J.O. Berger, J.M. Bernardo,
A.P. Dawid and A.F.M. Smith (Eds)., 97–106. Oxford University Press,
Oxford.
Diaconis, P. and Mosteller, F. (1989) Methods for studying coincidences.
J. Amer. Statist. Assoc. 84, 853–861.
Diaconis, P. and Ylvisaker, D. (1979) Conjugate priors for exponential fam-
ilies. Ann. Statist. 7, 269–281.
Diaconis, P. and Ylvisaker, D. (1985) Quantifying prior opinion. In Bayesian
Statistics 2, J.M. Bernardo, M.H. DeGroot, D.V. Lindley, A. Smith (eds.),
163–175. North-Holland, Amsterdam.
Diaconis, P. and Zabell, S. (1991) Closed form summation for classical dis-
tributions: variations on a theme of De Moivre. Statist. Science 6, 284–302.

References
545
DiCiccio, T.J. and Stern, S.E. (1993) On Bartlett adjustments for approx-
imate Bayesian inference, Biometrika 80, 731–740.
DiCiccio, T.J. and Stern, S.E. (1994) Frequentist and Bayesian Bartlett
correction of test statistics based on adjusted proﬁle likelihoods. J. Roy.
Statist. Soc., Ser. B 56, 397–408.
Dickey, J.M. (1968) Three multidimensional integral identities with Bay-
esian applications. Ann. Statist. 39, 1615–1627.
Diebolt, J. and Robert, C.P. (1990a) Bayesian estimation of ﬁnite mixture
distributions, Part I: Theoretical aspects. Rapport Tech. #110, LSTA, Uni-
versit´e Paris VI.
Diebolt, J. and Robert, C.P. (1990b) Bayesian estimation of ﬁnite mix-
ture distributions, Part II: Sampling implementation. Rapport Tech. #111,
LSTA, Universit´e Paris VI.
Diebolt, J. and Robert, C.P. (1994) Estimation of ﬁnite mixture distribu-
tions by Bayesian sampling. J. Roy. Statist. Soc., Ser. B 56, 363–375.
Doucet, A., deFreitas, N. and Gordon, N. (2001) Sequential Monte Carlo
in Practice. Springer–Verlag, New York.
Draper, D. (1995) Assessment and Propagation of Model Uncertainty. J.
Roy. Statist. Soc., Ser. B 57, 45-98.
Draper, D. (2000) Bayesian Hierarchical Modelling. University of Bath (in
preparation).
Dr`eze, J.H. (1978) Bayesian regression analysis using poly-t densities. J. of
Econometrics 6, 329–354.
Dr`eze, J.H. and Morales, J.A. (1976) Bayesian full information analysis of
the simultaneous equations. J. Amer. Statist. Assoc. 71, 919–923.
Dudewicz, E.J. and Koo, J.O. (1982) The Complete Categorized Guide
to Statistical Selection and Ranking Procedures. American Science Press,
Columbus, Ohio.
Dumouchel, W.M. and Harris, J.E. (1983) Bayes methods for combining
the results of cancer studies in human and other species (with discussion).
J. Amer. Statist. Assoc. 78, 293–315.
Dupuis, J.A. (1993) Bayesian estimation of movement probabilities in open
populations using hidden Markov chains. Rapport Technique 9341, CREST,
INSEE, Paris.
Dupuis, J.A. (1995) Bayesian estimation of movement probabilities in
open populations using hidden Markov chains. Biometrika / 82(4),
761–772.
Dupuis, J.A. and Robert, C.P. (2001) Bayesian variable selection in qual-
itative models by Kullback-Leibler projections. J. Statist. Plan. Inf.
(to
appear).
Durbin, J. and Watson, G.S. (1950) Testing for serial correlation in least-
squares regression, Biometrika 37, 409–428.

546
References
Dynkin, E.B. (1951) Necessary and suﬃcient statistics for a family of prob-
ability distributions. Selected Transl. Math. Statist. Prob. 1, 23–41.
Eaton, M.L. (1982) Multivariate Statistics. J. Wiley, New York.
Eaton, M.L. (1986) A characterization of spherical distributions. J. Multi-
variate Anal. 20, 272–276.
Eaton, M.L. (1989) Group Invariance Applications in Statistics. Regional
Conference Series in Probability and Statistics, Vol. 1. Institute of Mathe-
matical Statistics, Hayward, California.
Eaton, M.L. (1992) A statistical dyptich: admissible inferences–recurrence
of symmetric Markov chains. Ann. Statist. 20, 1147–1179.
Eaton, M.L. (1999) Markov chain conditions for admissibility in estimation
problems with quadratic loss. Report PN1–R9904, Centrum voor Wiskunde
en Informatica, Amsterdam.
Eberly, L. and Casella, G. (1999) Comparison of Bayesian credible inter-
vals in hierarchical models. Division of Biostatistics Manuscript 99-004,
University of Minnesota.
Efron, B. (1975) Biased versus unbiased estimation. Adv. in Math. 16,
259–277.
Efron, B. (1982) The Jacknife, the Bootstrap and Other Resampling Plans.
Regional Conference in Applied Mathematics 38. SIAM, Philadelphia.
Efron, B. (1992) Regression percentile using asymmetric squared error loss.
Statist. Sinica 1, 93–125.
Efron, B. and Morris, C. (1973) Stein’s estimation rule and its competi-
tors—an empirical Bayes approach. J. Amer. Statist. Assoc. 68, 117–130.
Efron, B. and Morris, C. (1975) Data analysis using Stein’s estimator and
its generalizations. J. Amer. Statist. Assoc. 70, 311–319.
Efron, B. and Thisted, R.A. (1976) Estimating the number of species: How
many words did Shakespeare know? Biometrika 63, 435–447.
Eichenauer, J. and Lehn, J. (1989) Gamma-minimax estimators for a bounded
normal mean under squared error–loss. Statist. Decisions 7, 37–62.
Engle, R.F. (1982) Autoregressive conditional heteroscedasticity with es-
timates of the variance of United Kingdom inﬂation. Econometrica 50,
987–1008.
Escobar, M.D. (1989) Estimating the means of several normal populations
by estimating the distribution of the means. Unpublished Ph.D. thesis, Yale
University, New Haven, Connecticut.
Escobar, M.D. and West, M. (1995) Bayesian prediction and density esti-
mation. J. Amer. Statist. Assoc. 90, 577–588.
Evans, M., Fraser, D.A.S., and Monette, G. (1986) On principles and ar-
guments to likelihood (with discussion). Canad. J. Statist. 14, 181-199.
Fabius, J. (1964) Asymptotic behavior of Bayes estimates. Ann. Math.
Statist. 35, 846–856.

References
547
Fang, K.T. and Anderson, T.W. (1990) Statistical Inference in Elliptically
Contoured and Related Distributions. Allerton Press, New York.
Farrell, R.H. (1968a) Towards a theory of generalized Bayes tests. Ann.
Math. Statist. 38, 1–22.
Farrell, R.H. (1968b) On a necessary and suﬃcient condition for admissi-
bility of estimators when strictly convex loss is used. Ann. Math. Statist.
38, 23–28.
Farrell, R.H. (1985) Multivariate Calculation. Springer–Verlag, New York.
Feller, W. (1970) An Introduction to Probability Theory and its Applica-
tions, Vol. 1. J. Wiley, New York.
Feller, W. (1971) An Introduction to Probability Theory and its Applica-
tions, Vol. 2. J. Wiley, New York.
Ferguson, T.S. (1967) Mathematical Statistics: a Decision-Theoretic Ap-
proach. Academic Press, New York.
Ferguson, T.S. (1973) A Bayesian analysis of some nonparametric problems.
Ann. Statist. 1, 209–230.
Ferguson, T.S. (1974) Prior distributions in spaces of probability measures.
Ann. Statist. 2, 615–629.
Fernandez, C. and Steel, M. (1999) On the dangers of modelling through
continuous distributions: a Bayesian perspective (with discussion). In Bayesian
Statistics 6. J.O. Berger, J.M. Bernardo, A.P. Dawid and A.F.M. Smith
(Eds)., 213–238. Oxford University Press, Oxford.
Feyerabend, P. (1975) Against Method. New Left Books, London.
Field, A. and Ronchetti, E. (1990) Small Sample Asymptotics. IMS lecture
notes – Monograph Series. Hayward, California.
Fieller, E.C. (1954) Some problems in interval estimation. J. Roy. Statist.
Soc., Ser. B 16, 175–185.
de Finetti, B. (1972) Probability, Induction and Statistics. J. Wiley, New
York.
de Finetti, B. (1974) Theory of Probability. Vol. 1. J. Wiley, New York.
de Finetti, B. (1975) Theory of Probability. Vol. 2. J. Wiley, New York.
Fishburn, P.C. (1988) Non-Linear Preferences and Utility Theory. Har-
vester Wheatsheaf, Brighton, Sussex.
Fisher, R.A. (1912) On an absolute criterion for ﬁtting frequency curves.
Messenger of Mathematics 41, 155–160.
Fisher, R.A. (1922) On the mathematical foundations of theoretical statis-
tics. Philos. Trans. Roy. Soc. London Ser. A 222, 309–368.
Fisher, R.A. (1930) Inverse probability. Proc. Cambridge Philos. Soc. 26,
528–535.
Fisher, R.A. (1956) Statistical Methods and Scientiﬁc Inference. Oliver and
Boyd, Edinburgh.

548
References
Fisher, R.A. (1959) Mathematical probability in the natural sciences. Tech-
nometrics 1, 21–29.
Fishman, G.S. (1996) Monte Carlo. Springer–Verlag, New York.
Fitzgerald, W.J., Godsill, S., Kokaram, A.C. and Stark, J.A. (1999) Bayesian
methods in signal and image processing. In
Bayesian Statistics 6. J.O.
Berger, J.M. Bernardo, A.P. Dawid and A.F.M. Smith (Eds)., 239–254.
Oxford University Press, Oxford
Florens, J.P., Mouchart, M. and Rolin, J.M. (1990) Elements of Bayesian
Statistics. Marcel Dekker, New York.
Foster, D.P. and George, E.I. (1996) A simple ancillarity paradox. Scand.
J. Statist. 23, 233–242.
Fouley, J.L., San Cristobal, M., Gianola, D. and Im, S. (1992) Marginal
likelihood and Bayesian approaches to the analysis of heterogeneous resid-
ual variances in mixed linear gaussian models. Comput. Statist. Data Anal.
13, 291–305.
Fourdrinier, D., Strawderman, W.E. and Wells, M.T. (1998) On the con-
struction of Bayes minimax estimators. Ann. Statist. 26(2), 660–671.
Fourdrinier, D. and Wells, M. (1993) Risk comparison of variable selection
rules. Doc. Travail, Universit´e de Rouen.
Fourdrinier, D. and Wells, M. T. (1994) R`egle de s´election de variables :
une approche d´ecisionnelle. C.R. Acad. Sci. Paris 319, S´erie I, 865–870.
Fraisse, A.M., Raoult, J.P., Robert, C.P. and Roy, M. (1990) Une con-
dition n´ecessaire d’admissibilit´e et ses cons´equences sur les estimateurs
`a r´etr´ecisseur de la moyenne d’une loi normale. Canad. J. Statist. 18,
213–220.
Fraisse, A.M., Robert, C.P. and Roy, M. (1987) Estimateurs `a r´etr´ecisseur
matriciel, pour un coˆut quadratique g´en´eral. Ann. d’Eco. Statist. 8, 161–175.
Fraisse, A.M., Roy, M. and Robert, C.P. (1998) Semi-tail upper bounds
for admissible estimators in exponential families with nuisance parameters.
Statistics & Decisions 16(2) 147–162.
Francq, C. and Zako¨ıan, J.M. (2001) Stationarity of multivariate Markov–
switching ARMA models. J. Econometrics 102, 339–364.
Fraser, D.A.S., Monette, G. and Ng, K.W. (1984) Marginalization, like-
lihood and structural models. In Multivariate Analysis 6, P. Krishnaiah
(ed.). North-Holland, Amsterdam.
Gassiat, E. and Dacunha-Castelle, D. (1997) Estimation of the number of
components in a mixture. Bernoulli 3(3), 279–299.
Gatsonis, C., Hodges, J.S., Kass, R.E. and Singpurwalla, N. (1993) Case
Studies in Bayesian Statistics. Lecture Notes in Statistics 83, Springer–
Verlag, New York.
Gatsonis, C., Hodges, J.S., Kass, R.E. and Singpurwalla, N. (1995) Case
Studies in Bayesian Statistics, vol. II. Lecture Notes in Statistics 105,

References
549
Springer–Verlag, New York.
Gatsonis, C., Hodges, J.S., Kass, R.E., McCulloch, R.E. and Singpurwalla,
N. (1997) Case Studies in Bayesian Statistics, vol. III. Lecture Notes in
Statistics 121, Springer–Verlag, New York.
Gatsonis, C., Kass, R.E., Carlin, B.P., Carriquiry, A., Gelman, A. and
Verdinelli, I. (1999) Case Studies in Bayesian Statistics, vol. IV. Lecture
Notes in Statistics 140, Springer–Verlag, New York.
Gatsonis, C., MacGibbon, K.B. and Strawderman, W.E. (1987) On the
estimation of a truncated normal mean. Statist. Prob. Lett. 6, 21–30.
Gauss, C.F. (1810) M´ethode des Moindres Carr´es. M´emoire sur la Com-
bination des Observations. Transl. J. Bertrand (1955). Mallet-Bachelier,
Paris.
Geisser, S. and Cornﬁeld, J. (1963) Posterior distributions for multivariate
normal parameters. J. Roy. Statist. Soc., Ser. B 25, 368–376.
Gelfand, A.E. (1996) Model determination using sampling-based methods.
In Markov Chain Monte Carlo in Practice, W.R. Gilks, S. Richardson and
D.J. Spiegelhalter (eds.), 145–162. Chapman & Hall, London.
Gelfand, A.E. (2000) Gibbs sampling. J. Amer. Statist. Assoc. 95,
1300–1304.
Gelfand, A.E. and Dey, D.K. (1994) Bayesian Model Choice: Asymptotics
and Exact Calculations. J. Roy. Statist. Soc., Ser. B 56, 501–514.
Gelfand, A.E. Dey, D.K. and Chang, H. (1992) Model determination using
predictive distributions with implementation via sampling-based methods.
In
Bayesian Statistics 4. J.O. Berger, J.M. Bernardo, A.P. Dawid and
A.F.M. Smith (Eds.). Oxford University Press, London, 147–167. Oxford
University Press, Oxford.
Gelfand, A.E., Hills, S., Racine-Poon, A. and Smith, A.F.M. (1990) Illus-
tration of Bayesian inference in normal models using Gibbs sampling. J.
Amer. Statist. Assoc. 85, 972–982.
Gelfand, A. and Smith, A.F.M. (1990) Sampling based approaches to cal-
culating marginal densities. J. Amer. Statist. Assoc. 85, 398–409.
Gelfand, A., Smith, A.F.M. and Lee, T.M. (1992) Bayesian analysis of
constrained parameters and truncated data problems using Gibbs sampling.
J. Amer. Statist. Assoc. 87, 523–532.
Gelman, A., Carlin, J.B., Stern, H.S. and Rubin, D.B. (1995) Bayesian
Data Analysis. Chapman & Hall, London.
Gelman, A. and Meng, X.L. (1998) Simulating normalizing constants: From
importance sampling to bridge sampling to path sampling. Statist. Sci. 13
163-185.
Gelman, A. and Rubin, D.B. (1992) Inference from iterative simulation
using multiple sequences (with discussion). Statist. Sci. 7, 457–511.
Geman, S. (1988) Experiments in Bayesian image analysis. In Bayesian
Statistics 3, J.M. Bernardo, M.H. DeGroot, D.V. Lindley and A.F.M. Smith

550
References
(eds.). Oxford University Press, Oxford.
Geman, S. and Geman, D. (1984) Stochastic relaxation, Gibbs distributions
and the Bayesian restoration of images. IEEE Trans. Pattern Anal. Mach.
Intell. 6, 721–740.
Genest, C. and Zidek, J.V. (1986) Combining probability distributions: A
critique and an annotated bibliography. Statist. Science 1, 114–135.
Gentle, J.E. (1998) Random Number Generation and Monte Carlo Methods.
Springer–Verlag, New York.
George, E.I. (1986a) Combining minimax shrinkage estimators. J. Amer.
Statist. Assoc. 81, 437–445.
George, E.I. (1986b) Minimax multiple shrinkage estimators. Ann. Statist.
14, 188–205.
George, E.I. and Casella, G. (1994) Empirical Bayes conﬁdence estimation.
Statist. Sinica 4(2), 617–638.
George, E.I. and Foster, D. (1999) Empirical Bayes variable selection. In
Model Choice, W. Racugno (ed.). Collana Atti di Congressi, Pitagora Ed-
itrice, Bologna.
George, E.I. and McCulloch, R.E. (1993) Variable Selection Via Gibbs Sam-
pling. J. Amer. Statist. Assoc. 88, 881–889.
George, E.I. and McCulloch, R.E. (1997) Approaches for Bayesian variable
selection. Statistica Sinica 7, 339–374.
George, E.I. and Robert, C.P. (1992) Calculating Bayes estimates for capture-
recapture models. Biometrika 4, 677–683.
Geweke, J. (1988) Antithetic acceleration of Monte Carlo integration in
Bayesian inference. J. Econometrics 38, 73–90.
Geweke, J. (1989) Bayesian inference in econometric models using Monte
Carlo integration. Econometrica 57, 1317–1340.
Geweke, J. (1991) Eﬃcient simulation from the multivariate normal and
Student t-distributions subject to linear constraints. Computer Sciences
and Statistics: Proc. 23d Symp. Interface, 571–577.
Geweke, J. (1992) Evaluating the accuracy of sampling-based approaches to
the calculation of posterior moments (with discussion). In Bayesian Statis-
tics 4, J.M. Bernardo, J.O. Berger, A.P. Dawid and A.F.M. Smith (eds.),
169–193. Oxford University Press, Oxford.
Geweke, J. (1999) Using simulation methods for Bayesian Econometric
models: inference, development and communication (with discussion). Econo-
metric Reviews 18, 1–73.
Geyer, C.J. (1992) Practical Monte Carlo Markov Chain (with discussion).
Statist. Sci. 7, 473–511.
Geyer, C.J. (1995) Conditioning in Markov Chain Monte Carlo. J. Com-
put. Graph. Statis. 4, 148–154.
Geyer, C.J. (1996) Estimation and optimization of functions. In Markov
chain Monte Carlo in Practice, W.R. Gilks, S.T. Richardson and D.J.

References
551
Spiegelhalter (eds.). 241–258. Chapman & Hall, London.
Geyer, C.J. and Thompson, E.A. (1992) Constrained Monte Carlo maxi-
mum likelihood for dependent data (with discussion). J. Roy. Statist. Soc.,
Ser. B 54, 657–699.
Ghosh, J.K. and Mukerjee, R. (1991) Characterization of priors under
which Bayesian and frequentist Bartlett corrections are equivalent in the
multiparameter case. J. Multivariate Anal. 38, 385–393.
Ghosh, J.K. and Mukerjee, R. (1992a) Bayesian and frequentist Bartlett
corrections for likelihood ratio tests. J. Roy. Statist. Soc., Ser. B 56,
396–408.
Ghosh, J.K. and Mukerjee, R. (1992b) Noninformative priors (with discus-
sion). In Bayesian Statistics 4, J.M. Bernardo, J.O. Berger, A.P. Dawid
and A.F.M. Smith (eds.). Oxford University Press, Oxford.
Ghosh, J.K. and Mukerjee, R. (1993) Frequentist validity of highest poste-
rior density regions in the multiparameter case. Annals of the Institute of
Statistical Mathematics, 45, 293–302.
Ghosh, J.K. and Mukerjee, R. (1994) Adjusted versus conditional likeli-
hood: power properties and Bartlett-type adjustment. J. Roy. Statist. Soc.,
Ser. B 56, 185–188.
Ghosh, M., Carlin, B. and Srivastava, M.S. (1995) Probability matching
priors for linear calibration. Test 4(2), 333–358.
Ghosh, M., Hwang, J.T. and Tsui, K. (1983) Construction of improved
estimators in multiparameter estimation for discrete exponential families
(with discussion). Ann. Statist. 11, 351–376.
Ghosh, M., Keating, J.P. and Sen, P.K. (1993) Discussion of Robert, Hwang
and Strawderman (1993). J. Amer. Statist. Assoc. 88, 63–66.
Ghosh, M. and Mukerjee, R. (1992) Hierarchical and empirical Bayes mul-
tivariate estimation. In Current Issues in Statistical Inference: Essays in
Honor of D. Basu, M. Ghosh and P.K. Pathak (eds.), 1–12. IMS Lecture
Notes – Monograph Series 17. Hayward, California.
Ghosh, M. and Sen, P.K. (1989) Median unbiasedness and Pitman close-
ness. J. Amer. Statist. Assoc. 84, 1089–1091.
Ghosh, M., Sen, P.K. and Saleh, A.K.Md.E. (1989) Empirical Bayes subset
estimation in regression models. Statist. Decisions 7, 15–35.
Ghosh, M. and Yang, M.C. (1996) Noninformative priors for the two sample
normal problem Test 5, 145–157.
Gibbons, J.D., Olkin, I. and Sobel, M. (1977) Selecting and Ordering Pop-
ulations. J. Wiley, New York.
Gigerenzer, G. (1991) The Superego, the Ego and the Id in statistical rea-
soning. In Methodological and Quantitative Issues in the Analysis of Psy-
chological Data, G. Keren and C. Lewis (eds.). Erlbaum, Hillsdale, New
Jersey.

552
References
Gilks, W.R., Best, N.G. and Tan, K.K.C. (1995) Adaptive rejection Metropo-
lis sampling within Gibbs sampling. Applied Statist. Series C 44, 455–472.
Gilks, W.R., Clayton, D.G., Spiegelhalter, D.J., Best, N.G., McNeil, A.J.,
Sharples, L.D., and Kirby, A.J. (1993) Modelling complexity: applications
of Gibbs sampling in medicine (with discussion). J. Roy. Statist. Soc., Ser.
B 55, 39–52.
Gilks, W., Richardson, S. and Spiegelhalter, D. (1996) Practical Monte-
Carlo Markov Chain. Chapman & Hall, London.
Gilks, W.R. and Wild, P. (1992) Adaptive rejection sampling for Gibbs
sampling. Appl. Statist. 41, 337–348.
Gill, R.D. and Levit, B.Y. (1995) Applications of the Van Trees inequality:
a Bayesian Cram´er-Rao bound. Bernouilli 1, 59–79.
Giudici, P. and Green, P.J. (1998) Decomposable graphical Gaussian model
determination. Technical Report, Universit`a di Pavia.
Givens, G.H., Smith, D.D. and Tweedie, R.L. (1997) Publication bias in
meta-analysis: a Bayesian data-augmentation approach to account for is-
sues exempliﬁed in the passive smoking debate. Statist. Sci. 12, 221–250.
Gleick, J. (1987) Chaos. Penguin, New York.
Gleser, L.J. and Healy, J.D. (1976) Estimating the mean of a normal dis-
tribution with known coeﬃcient of variation. J. Amer. Statist. Assoc. 71,
977–981.
Gleser, L.J. and Hwang, J.T. (1987) The non-existence of 100(1 −α)%
conﬁdence sets of ﬁnite expected diameters in errors-in-variable and related
models. Ann. Statist. 15, 1351–1362.
Goel, P.K. (1988) Software for Bayesian analysis. In Bayesian Statistics
3, J.M. Bernardo, M.H. DeGroot, D.V. Lindley and A.F.M. Smith (eds.),
173–188. Oxford University Press, London.
Goel, P.K. and Rubin, H. (1977) On selecting a subset containing the best
population–a Bayesian approach. Ann. Statist. 5, 969–983.
Goldstein, M. and Smith, A.F.M. (1974) Ridge-type estimators for regres-
sion analysis. J. Roy. Statist. Soc., Ser. B 36, 284–219.
Good, I.J. (1952) Rational decisions. J. Roy. Statist. Soc., Ser. B 14,
107–114.
Good, I.J. (1973) The probabilistic explication of evidence, causality, expla-
nation and utility. In Foundations of Statistical Inference, V.P. Godambe
and D.A. Sprott (eds.). Holt, Rinehart and Winston, Toronto.
Good, I.J. (1975) Bayesian estimation methods for two-way contingency
tables. J. Roy. Statist. Soc., Ser. B 37, 23–37.
Good, I.J. (1980) Some history of the hierarchical Bayesian methodology. In
Bayesian Statistics 2, J.M. Bernardo, M.H. DeGroot, D.V. Lindley, A.F.M.
Smith (eds.). North-Holland, Amsterdam.
Good, I.J. (1983) Good Thinking: The Foundations of Probability and Its
Applications. University of Minnesota Press, Minneapolis.

References
553
Gouri´eroux, C. (1997) ARCH Model. Springer–Verlag, New York.
Gouri´eroux, C. and Monfort, A. (1996) Statistical Methods in Economet-
rics, volumes 1 and 2. Cambridge University Press, Cambridge.
Goutis, C. (1990) Ranges of posterior measures for some classes of pri-
ors with speciﬁed moments. Tech. Report 70, University College London.
[Published in a modiﬁed version without Table 3.3.4 as Goutis (1994).]
Goutis, C. (1994) Ranges of posterior measures for some classes of priors
with speciﬁed moments. International Statistical Review 62(2), 245–256.
Goutis, C. and Casella, G. (1991) Improved invariant conﬁdence intervals
for a normal variance. Ann. Statist. 19, 2015–2031.
Goutis, C. and Casella, G. (1992) Increasing the conﬁdence in Student’s
t-interval. Ann. Statist. 20(3), 1501–1513.
Goutis, C., Casella, G. and Wells, M.T. (1996) Assessing evidence in mul-
tiple hypotheses. J. Amer. Statist. Assoc. 91, 1268–1277.
Goutis, C. and Robert, C.P. (1998) Model Choice in Generalized Linear
Models: a Bayesian Approach via Kullback-Leibler Projections. Biometrika
85, 29–37.
Gradshteyn, I. and Ryzhik, I. (1980) Tables of Integrals, Series and Prod-
ucts. Academic Press, New York.
Green, P. (1995) Reversible Jump Markov Chain Monte Carlo Computation
and Bayesian Model Determination. Biometrika 82, 711–732.
Green, P. and Richardson, S. (1998) Modelling heterogeneity with and with-
out the Dirichlet process. Biometrika 82, 711–732.
Grenander, U. and Miller, M.I. (1994) Representations of knowledge in
complex systems (with discussion). J. Roy. Statist. Soc., Ser. B 56,
549–603.
Gruet, M.A., Philippe, A. and Robert, C.P. (1999) MCMC control spread-
sheets for exponential mixture estimation. J. Comp. Graph Statist.
8,
298–317.
Guihenneuc–Jouyaux, C., Richardson, S. and Lasserre, V. (1998) Conver-
gence assessment in latent variable models: apllication to longitudinal mod-
elling of a marker of HIV progression. In Discretization and MCMC Con-
vergence Assessment. C.P. Robert (ed.). Chapter 7, 147–160. Lecture Notes
in Statistics 135, Springer-Verlag, New York.
Gupta, S.S. (1965) On multiple decision (selection and ranking) rules. Tech-
nometrics, 7, 222–245.
Gupta, S.S. and Panchapakesan, S. (1979) Multiple Decision Procedures. J.
Wiley, New York.
Gutmann, S. (1982) Stein’s paradox is impossible in problems with ﬁnite
sample space. Ann. Statist. 10, 1017–1020.
Hadjicostas, P. and Berry, S.M. (1999) Improper and proper posteriors with
improper priors in a Poisson-gamma hierarchical model. Test 8, 147–166.

554
References
Haﬀ, L. and Johnstone, R.W. (1986) The superharmonic condition for si-
multaneous estimation of means in exponential families. Canad. J. Statist.
14, 43–54.
Hajek, J. and Sid`ak, Z. (1968) Theory of Rank Test. Academic Press, New
York.
Hald, A. (1998) An History of Mathematical Statistics. J. Wiley, New York.
Haldane, J. (1931) A note on inverse probability. Proc. Cambridge Philos.
Soc. 28, 55–61.
Hall, P. (1992) The Bootstrap and Edgeworth Expansion. Springer–Verlag,
New York.
Hamilton J.D. (1989), A new approach to the economic analysis of nonsta-
tionary time series and the business cycle, Econometrica, 57(2), 357-384.
Hammersley, J.M. (1974) Discussion of Besag’s paper. J. Roy. Statist. Soc.,
Ser. B 36, 230–231.
Hansen, M. and Yu, B. (2001) Model selection and minimum description
length principle. J. Amer. Statist. Assoc. 96, 746–774.
Hartigan, J.A. (1983) Bayes Theory. Springer–Verlag, New York.
Hastings, W.K. (1970) Monte Carlo sampling methods using Markov chains
and their application. Biometrika 57, 97–109.
Heath, D. and Sudderth, W. (1989) Coherent inference from improper pri-
ors and from ﬁnitely additive priors. Ann. Statist. 17, 907–919.
Heidelberger, P. and Welch, P.D. (1983) A spectral method for conﬁdence
interval generation and run length control in simulations. Comm. Assoc.
Comput. Machinery 24, 233–245.
Heitjan, D.F. and Rubin, D.B. (1991) Ignorability and coarse data. Ann.
Statist. 19, 2244–2253.
Helland, I.S. (1999) Statistical inference under a ﬁxed symmetry group.
Tech. report, Dept. of Mathematics and Statistics, University of Oslo.
Hesterberg, T. (1998) Weighted average importance sampling and defensive
mixture distributions. Technometrics 37, 185–194.
Hinkley, D. (1997) Discussion of “Uniﬁed frequentist and Bayesian testing
of a precise hypothesis.” Statistical Science 12, 155–156.
Hjort, N.L. (1990) Nonparametric Bayes estimates based on beta processes
in models for life history data. Ann. Statist. 18, 1501–1555.
Hjort, N.L. (1996) Bayesian approaches to non- and semiparametric den-
sity estimation (with discussion). In
Bayesian Statistics 5. J.O. Berger,
J.M. Bernardo, A.P. Dawid and A.F.M. Smith (Eds)., 223–253. Oxford
University Press, Oxford.
Hoaglin, D., Mosteller, F. and Tukey, J. (1985) Exploring Data Tables,
Trends, and Shapes. J. Wiley, New York.
Hobert, J. P (2000a) Stability relationships among the Gibbs sampler and
its subchains. Tech. report, University of Florida.

References
555
Hobert, J. P (2000b) Hierarchical models: a current computational per-
spective. J. Amer. Statist. Assoc. 95, 1312–1316.
Hobert, J. P and Casella, G. (1996) The eﬀect of improper priors on
Gibbs sampling in hierarchical linear models. J. Amer. Statist. Assoc. 91
1461–1473.
Hobert, J. P. and Casella, G. (1998) Functional Compatibility, Markov
Chains, and Gibbs Sampling with Improper Posteriors. J. Comp. Graph
Statist. 7, 42–60.
Hobert, J. P. and Robert, C.P. (1999) Eaton’s Markov chain, its conjugate
partner and P-admissibility. Ann. Statist. 27, 361–373.
Hoerl, A. and Kennard, R. (1970) Ridge regression: biased estimators for
non-orthogonal problems. Technometrics 12, 55–67.
Hora, R.B. and Buehler, R.J. (1966) Fiducial theory and invariant estima-
tion. Ann. Math. Stat. 37, 361–379.
Huber, P.J. (1964) Robust estimation of a location parameter. Ann. Math.
Statist. 35, 73–101.
Huber, P.J. (1972) Robust Statistics: a review. Ann. Math. Statist.
47,
1041–1067.
Huerta, G. and West, M. (2000) Bayesian inference on periodicities and
component spectral structure in time series. J. Time Series Analysis (to
appear).
Hui, S. and Berger, J.O. (1983) Empirical Bayes estimation of rates in
longitudinal studies. J. Amer. Statist. Assoc. 78, 753–760.
Huzurbazar, V.S. (1976) Suﬃcient Statistics. Marcel Dekker, New York.
Hwang, J.T. (1982a) Improving upon standard estimators in discrete expo-
nential families with applications to Poisson and negative binomial cases.
Ann. Statist. 10, 857–867.
Hwang, J.T. (1982b) Semi-tail upper bounds on the class of admissible
estimators in discrete exponential families, with applications to Poisson
and negative binomial distributions. Ann. Statist. 10, 1137–1147.
Hwang, J.T. (1985) Universal domination and stochastic domination: de-
cision theory simultaneously under a broad class of loss functions. Ann.
Statist. 13, 295–314.
Hwang, J.T. and Brown, L.D. (1991) Estimated conﬁdence under the va-
lidity constraint. Ann. Statist. 19, 1964–1977.
Hwang, J.T. and Casella, G. (1982) Minimax conﬁdence sets for the mean
of a multivariate normal distribution. Ann. Stat. 10, 868–881.
Hwang, J.T. and Casella, G. (1984) Improved set estimators for a multi-
variate normal mean. Statist. Decisions Supplement Issue 1, 3–16.
Hwang, J.T., Casella, G., Robert, C.P., Wells, M.T. and Farrel, R. (1992)
Estimation of accuracy in testing. Ann. Statist. 20, 490–509.
Hwang, J.T. and Chen, J. (1986) Improved conﬁdence sets for the coef-
ﬁcients of a linear model with spherically symmetric errors. Ann. Statist.

556
References
14, 444–460.
Hwang, J.T. and Pemantle, R. (1994) Evaluation of estimators of statistical
signiﬁcance under a class of proper loss functions. Statist. Decisions 15,
103–128.
Hwang, J.T. and Ullah, A. (1994) Conﬁdence sets recentered at James–
Stein estimators—A surprise concerning the unknown variance case. J.
Econometrics 60(1-2), 145–156.
Ibragimov, I. and Has’minskii, R. (1981) Statistical Estimation. Asymptotic
Theory. Springer-Verlag, New York.
Jacquier, E., Polson, N.G. and Rossi, P.E. (1994) Bayesian analysis of
stochastic volatility models. J. Econ. Business Statist. 12, 371–389.
James, W. and Stein, C. (1961) Estimation with quadratic loss. In Proc.
Fourth Berkeley Symp. Math. Statist. Probab. 1, 361–380. University of
California Press.
Jaynes, E.T. (1980) Marginalization and prior probabilities. In Bayesian
Analysis in Econometrics and Statistics, A. Zellner (ed.). North-Holland,
Amsterdam.
Jaynes, E.T. (1983) Papers on Probability, Statistics and Statistical Phy-
sics, R.D. Rosencrantz (ed.). Reidel, Dordrecht.
Jeﬀerys, W. and Berger, J.O. (1992) Ockham’s razor and Bayesian analysis.
American Scientist 80, 64–72.
Jeﬀreys, H. (1939) Theory of Probability. Oxford University Press, London.
Jeﬀreys, H. (1946) An invariant form for the prior probability in estimation
problems. Proceedings of the Royal Society of London (Ser. A) 186, 453–
461.
Jeﬀreys, H. (1961) Theory of Probability (third edition). Oxford University
Press, London.
Johnson, B.M. (1971) On the admissible estimators for certain ﬁxed sample
binomial problems. Ann. Math. Statist. 41, 1579–1587.
Johnson, N.L. and Kotz, S.V. (1969–1972) Distributions in Statistics (4
vols.). J. Wiley, New York.
Johnstone, D.J. and Lindley, D.V. (1995) Bayesian inference given data
“signiﬁcant at α”: tests of point-null hypotheses. Theory and Decision
38(1), 51–60.
Johnstone, I.M. (1984) Admissibility, diﬀerence equations, and recurrence
in estimating a Poisson mean. Ann. Statist. 12, 1173–1198.
Johnstone, I.M. (1986) Admissible estimation, Dirichlet principles and re-
currence of birth-death chains in ZZp
+. Z. Wahrsch. Verw. Gebiete 71,
231–270.
Johnstone, I.M. (1988) On the inadmissibility of Stein’s unbiased estimate
of loss. In Statistical Decision Theory and Related Topics 4, S.S. Gupta
and J.O. Berger (eds.). Springer-Verlag, New York.

References
557
Johnstone, I.M. and MacGibbon, B.K. (1992) Minimax estimation of a
constrained Poisson vector. Ann. Statist. 20, 807–831.
Jones, M.C. (1987) Randomly choosing parameters from the stationarity
and invertibility region of autoregressive-moving average models. Applied
Statistics (Series C) 38, 134–138.
Joshi, V.M. (1967) Admissibility of the usual conﬁdence set for the mean
of a multivariate normal population. Ann. Math. Statist. 38, 1868-1875.
Joshi, V.M. (1969) Admissibility of the usual conﬁdence set for the mean
of a univariate or bivariate normal population. Ann. Math. Statist.
40,
1042–1067.
Joshi, V.M. (1990) The censoring concept and the likelihood principle. J.
Statist. Plann. Inference 26, 109–111.
Judge, G. and Bock, M.E. (1978) Implications of Pre-Test and Stein Rule
Estimators in Econometrics. North-Holland, Amsterdam.
Kadane, J.B. and Chuang, D. (1978) Stable decision problems. Ann. Statist.
6, 1095–1111.
Kariya, T. (1984) An invariance approach to estimation in a curved model.
Tech. Report 88, Hifotsubashi University, Japan.
Kariya, T., Giri, N. and Perron, F. (1988) Invariant estimation of mean
vector μ of N(μ, Σ) with μ′Σ−1μ = 1 or Σ−1/2μ = C or Σ = δ2μμ′I. J.
Multivariate Anal. 27, 270–283.
Karlin, S. (1958) Admissibility for estimation with quadratic loss. Ann.
Math. Statist. 29, 406–436.
Karlin, S. and Rubin, H. (1956) The theory of decision procedures for distri-
butions with monotone likelihood ratio. Ann. Math. Statist. 27, 272–299.
Kass, R.E. (1989) The geometry of asymptotic inference. Statist. Science
4, 188–234.
Kass, R.E. and Raftery, A.E. (1995) Bayes factor and model uncertainty.
J. Amer. Statist. Assoc. 90, 773–795.
Kass, R.E. and Steﬀey, D. (1989) Approximate Bayesian inference in condi-
tionally independent hierarchical models (parametric empirical Bayes mod-
els). J. Amer. Statist. Assoc. 87, 717–726.
Kass, R.E. and Wasserman, L. (1995) A reference Bayesian test for nested
hypotheses and its relationship to the Schwarz criterion. J. Amer. Statist.
Assoc. 90, 928–934.
Kass, R.E. and Wasserman, L. (1996) Formal rules of selecting prior dis-
tributions: a review and annotated bibliography. J. Amer. Statist. Assoc.
91, 343–1370.
Keating, J.P. and Mason, R. (1988) James–Stein estimation from an alter-
native perspective. Amer. Statist. 42, 160–164.
Keeney, R.L. and Raiﬀa, H. (1976) Decisions with Multiple Objectives. J.
Wiley, New York.

558
References
Kelker, D. (1970) Distribution theory of spherical distributions and a location-
scale parameter generalization. Sankhya (Ser. A) 32, 419–430.
Kempthorne, P.J. (1987) Numerical speciﬁcation of discrete least favorable
prior distributions. SIAM J. Statist. Comput. 8, 178–184.
Kempthorne, P.J. (1988) Controlling risks under diﬀerent loss functions:
the compromise decision problem. Ann. Statist. 16, 1594–1608.
Kendall, M. and Stuart, A. (1979) The Advanced Theory of Statistics, Vol-
ume II: Inference and Relationships (4th edition). Macmillan, New York.
Keynes, J.M. (1921) A Treatise on Probability. Macmillan, London.
Kiefer, J. (1957) Invariance, minimax sequential estimation and continuous
time–processes. Ann. Math. Statist. 28, 573–601.
Kiefer, J. (1977) Conditional conﬁdence statements and conﬁdence estima-
tors (theory and methods). J. Amer. Statist. Assoc. 72, 789–827.
Kiiveri, H. and Speed, T.P. (1982) Structural analysis of multivariate data:
A review. In Sociological Methodology, 1982. S. Leinhardt (Ed.). 209–289.
Jossey Bass, San Francisco.
Kinderman, A., Monahan, J. and Ramage, J. (1977) Computer methods
for sampling from Student’s t-distribution. Math. Comput. 31, 1009–1018.
Kirby, A. J. and Spiegelhalter, D. J. (1994) Statistical modelling for the
precursors of cervical cancer. In Case Studies in Biometry, N. Lange (ed.).
John Wiley, New York.
Kleibergen, F. and van Dijk, H.K. (1993) Non-stationarity in GARCH mod-
els: a Bayesian analysis. J. of Appl. Econometrics 8, 41–61.
Knuth, D. (1981) The Art of Computer Programing. Volume 2: Seminu-
merical Algorithms (2nd edition). Addison-Wesley, Reading, Mass.
Kontkanen, P., Myllym¨aki, P., Silander, T., Tirri, H. and Gr¨unwald, P.
(2000) On predictive distributions and Bayesian networks. Statist. Comput.
10, 39–54.
Koopman, B. (1936) On distributions admitting a suﬃcient statistic. Trans.
Amer. Math. Soc. 39, 399–409.
Kubokawa, T. (1987) Admissible minimax estimation of a common mean
of two normal populations. Ann. Statist. 15, 1245–1256.
Kubokawa, T. (1991) An approach to improving James–Stein estimator. J.
Multivariate Analysis 36, 121–126.
Kubokawa, T., Morita, S., Makita, S. and Nagakura, K. (1993) Estima-
tion of the variance and its applications. J. Statist. Plann. Inference 35,
319–333.
Kubokawa, T. and Robert, C.P. (1994) New perspectives on linear calibra-
tion. J. Multivariate Analysis 51, 178-200.
Kubokawa, T., Robert, C.P. and Saleh, A.K.Md.E. (1991) Robust estima-
tion of common regression coeﬃcients under spherical symmetry. Ann. Inst.
Statist. Math. 43, 677–688.

References
559
Kubokawa, T., Robert, C.P. and Saleh, A.K.Md.E. (1992) Empirical Bayes
estimation of the covariance matrix of a normal distribution with unknown
mean under an entropy loss. Sankhya (Ser. A) 54, 402–410.
Kubokawa, T., Robert, C.P. and Saleh, A.K.Md.E. (1993) Estimation of
noncentrality parameters. Canad. J. Statist. 21, 54–58.
Lad, F. (1996) Operational Subjective Statistical Methods: a Mathematical,
Philosophical and Historical Introduction. J. Wiley, New York.
Laird, N.M. and Louis, T.A. (1987) Conﬁdence intervals based on bootstrap
samples. J. Amer. Statist. Assoc. 82, 739–750.
Laplace, P.S. (1773) M´emoire sur la probabilit´e des causes par les ´ev´ene-
ments. M´emoires de l’Acad´emie Royale des Sciences pr´esent´es par divers
savans 6, 621–656. [Reprinted in Laplace (1878) 8, 27–65.]
Laplace, P.S. (1786) Sur les naissances, les mariages et les morts `a Paris
depuis 1771 jusqu’`a 1784 et dans toute l’´etendue de la France, pendant les
ann´ees 1781 et 1782. M´emoires de l’Acad´emie Royale des Sciences pr´esen-
t´es par divers savans. [Reprinted in Laplace (1878), 11, 35–46.]
Laplace, P.S. (1795) Essai Philosophique sur les Probabilit´es. [Reprinted in
Christian Bourgeois, coll. Epist´em´e, 1986.]
Laplace, P.S. (1812) Th´eorie Analytique des Probabilit´es. Courcier, Paris.
Laplace, P.S. (1878–1912) Œuvres Compl`etes de Laplace. Gauthier-Villars,
Paris.
Lauritzen, S.L. (1996) Graphical Models. Oxford University Press, London.
Lavielle, M. and Moulines, E. (1997) On a stochastic approximation version
of the EM algorithm. Statist. Comput. 7, 229–236.
Lavine, M. (1992) Some aspects of P´olya tree distributions for statistical
modeling; Ann. Statist. 22, 1222–1235.
Lawley, D.N. (1956) A general method for approximating to the distribu-
tion of the likelihood ratio criteria. Biometrika 43, 295–303.
Le Cam, L. (1986) Asymptotic Methods in Statistical Decision Theory.
Springer–Verlag, New York.
Le Cam, L. (1990) Maximum likelihood: an introduction. Int. Statist. Rev.
58, 153–172.
Lee, P. (1989) Bayesian Statistics: an Introduction. Oxford University Press,
London.
Legendre, A. (1805) Nouvelles M´ethodes pour la D´etermination des Orbites
des Com`etes. Courcier, Paris.
Lehmann, E.L. (1983) Theory of Point Estimation. J. Wiley, New York.
Lehmann, E.L. (1986) Testing Statistical Hypotheses (2nd edition). J. Wi-
ley, New York.
Lehmann, E.L. (1990) Model speciﬁcation. Statist. Science 5, 160–168.
Lehmann, E.L. and Casella, G. (1998) Theory of Point Estimation (second
edition). Springer–Verlag, New York.

560
References
Lenk, P. (1999) Bayesian inference for semiparametric regression using a
Fourier representation. J. Roy. Statist. Soc., Ser. B 61, 863–879.
Leonard, T. (1982) Comments on Lejeune and Faulkenberry (1982) J.
Amer. Statist. Assoc. 77, 657–658.
Letac, G. (1990) Personal communication.
Letac, G. and Mora, M. (1990) Natural real exponential families with cubic
variance functions. Ann. Statist. 18, 1–37.
Lindley, D.V. (1957) A statistical paradox. Biometrika 44, 187–192.
Lindley, D.V. (1961) The use of prior probability distributions in statisti-
cal inference and decision. In Proc. Fourth Berkeley Symp. Math. Statist.
Probab. 1, 453–468. University of California Press.
Lindley, D.V. (1962) Discussion of Professor Stein’s paper ‘Conﬁdence sets
for the mean of a multivariate normal distribution’. J. Roy. Statist. Soc.,
Ser. B 24, 265–296.
Lindley, D.V. (1965) Introduction to Probability and Statistics from a Bayesian
Viewpoint (Parts 1 and 2). Cambridge University Press, Cambridge.
Lindley, D.V. (1971) Bayesian Statistics, A Review. SIAM, Philadelphia.
Lindley, D.V. (1980) Approximate Bayesian methods. In Bayesian Statis-
tics 2, J.M. Bernardo, M. DeGroot, D.V. Lindley and A.F.M. Smith (eds.),
North-Holland, Amsterdam.
Lindley, D.V. (1982) Scoring rules and the inevitability of probability. Int.
Statist. Rev. 50, 1–26.
Lindley, D.V. (1985) Making Decisions (2nd edition). J. Wiley, New York.
Lindley, D.V. (1990) The present position in Bayesian Statistics (with dis-
cussion). Statist. Sci. 5(1), 44–89.
Lindley, D.V. and Phillips, L.D. (1976) Inference for a Bernouilli process
(a Bayesian view). Amer. Statist. 30, 112–119.
Lindley, D.V. and Smith, A.F.M. (1972) Bayes estimates for the linear
model. J. Roy. Statist. Soc., Ser. B 34, 1–41.
Liseo, B. (1993) Elimination of nuisance parameters with reference priors.
Biometrika 80(2), 295–304.
Liu, J.S. and Wu, Y.N. (1999) Parameter expansion scheme for data aug-
mentation. J. Amer. Statist. Assoc. 94, 1264-1274.
Liu, J.S., Wong, W.H. and Kong, A. (1994) Covariance structure of the
Gibbs sampler with applications to the comparisons of estimators and sam-
pling schemes. Biometrika 81, 27–40.
Liu, J.S., Wong, W.H. and Kong, A. (1995) Correlation structure and con-
vergence rate of the Gibbs sampler with various scans. J. Roy. Statist. Soc.,
Ser. B 57, 157–169.
Louis, T. (1997) Discussion of “Uniﬁed frequentist and Bayesian testing of
a precise hypothesis”. Statistical Science 12, 152–155.

References
561
Lu, K. and Berger, J.O. (1989a) Estimated conﬁdence procedures for mul-
tivariate normal means. J. Statist. Plann. Inference 23, 1–19.
Lu, K. and Berger, J.O. (1989b) Estimation of normal means: frequentist
estimators of loss. Ann. Statist. 17, 890–907.
Maatta, J. and Casella, G. (1990) Developments in decision theoretic vari-
ance estimation (with discussion). Statist. Science 5, 90–120.
Machina, G. (1982) Expected utility analysis without the independence
axiom. Econometrica 50, 277–323.
Machina, G. (1987) Choice under uncertainty: problems solved and un-
solved. J. Econom. Perspectives 1, 121–154
MacLachlan, G. and Krishnan, T. (1997) The EM Algorithm and Exten-
sions. J. Wiley, New York.
MacLachlan, G. and Basford, K. (1987) Mixture Models. Marcel Dekker,
New York.
Madigan, D. and Raftery, A.E. (1991) Model selection and accounting for
model uncertainty in graphical models using Occam’s Window. Technical
Report 213, University of Washington.
Madigan, D. and Raftery, A.E. (1994) Model selection and accounting for
model uncertainty in graphical models using Occam’s Window. J. Amer.
Statist. Assoc. 89, 1535–1546.
Madigan, D. and Raftery, A.E. (1995) Bayesian graphical models for dis-
crete data. Int. Statist. Rev. 63, 215–232.
Madigan, D. and York, J. (1995) Bayesian graphical models for discrete
data. International Statistical Review 63, 215-232.
Maddala, G. (1977) Econometrics. McGraw-Hill, New York.
Marin, J.-M. and Robert, C.P. (2007) Bayesian Core: a Practical Approach
to Computational Bayesian Statistics, Springer–Verlag, New York.
Maritz, J.S. and Lwin, T. (1989) Empirical Bayes Methods (2nd edition).
Chapman & Hall, London.
Marsaglia, G. and Zaman, A. (1993) The KISS Generator. Tech. Report,
Dept. of Statistics, University of Florida.
McCullagh, P. and Nelder, J. (1989) Generalized Linear Models. Chapman
& Hall, London.
McCullogh, R. and Rossi, P.E. (1992) Bayes Factors for Nonlinear Hy-
potheses and Likelihood Distributions. Biometrika 79, 663–676.
Meeden, G. and Vardeman, S. (1985) Bayes and admissible set estimation.
J. Amer. Statist. Assoc. 80, 465–471.
Meng, X.L. and van Dyk, D.A. (1997) The EM algorithm–an old folk-song
sung to a new tune (with discussion). J. Roy. Statist. Soc., Ser. B 59,
511–568.
Meng, X.L. and van Dyk, D.A. (1999) Seeking eﬃcient data augmenta-
tion schemes via conditional and marginal augmentation. Biometrika 86,

562
References
301–320.
Meng, X.L. and Wong, W.H. (1996) Simulating ratios of normalizing con-
stants via a simple identity: a theoretical exploration. Statist. Sinica 6,
831–860.
Mengersen, K.L. and Robert, C.P. (1996) MCMC Convergence Diagnostics:
a “Reviewww” (with discussion). In
Bayesian Statistics 6. J.O. Berger,
J.M. Bernardo, A.P. Dawid and A.F.M. Smith (Eds)., 415–440. Oxford
University Press, London.
Mengersen, K.L. and Robert, C.P. (1996) Testing for mixtures: A Bayesian
entropic approach (with discussion). In Bayesian Statistics 5, J.O. Berger,
J.M. Bernardo, A.P. Dawid, D.V. Lindley and A.F.M. Smith (eds.). 255–
276. Oxford University Press, London.
Mengersen, K.L. and Tweedie, R.L. (1993) Meta-analysis approaches to
dose-response relationships with application in studies of lung cancer and
passive smoking. Statist. Medicine–Proc. NIH Conf. on Meta-Analysis, D.
Williamson (ed.).
Mengersen, K.L. and Tweedie, R.L. (1996) Rates of convergence of the
Hastings and Metropolis algorithms. Ann. Statist. 24 101–121.
Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, A.H., Teller,
E. (1953) Equations of state calculations by fast computing machines. J.
Chem. Phys. 21, 1087–1092.
Metropolis, N. and Ulam, S. (1949) The Monte Carlo method. J. Amer.
Statist. Assoc. 44, 335–341.
Meyer, Y. (1990) Ondelettes. Hermann, Paris.
Meyn, S.P. and Tweedie, R.L. (1993) Markov Chains and Stochastic Sta-
bility. Springer-Verlag, London.
Mira, A. and Tierney, L. (2002) On the use of auxiliary variables in Markov
chain Monte Carlo methods. Scand. J. Statist. 29(1), 1–12.
Monahan J.F. (1984) A note on enforcing stationarity in autoregressive-
moving average models. Biometrika 71, 403–404.
Moors, J.J.A. (1981) Inadmissibility of linearly invariant estimators in trun-
cated parameter spaces. J. Amer. Statist. Assoc. 76, 910–915.
Moreno, E., Bertolino, F., and Racugno, W. (1998a) An intrinsic limiting
procedure for model selection and hypothesis testing. J. Amer. Statist.
Assoc. 93, 1451–1460.
Morisson, D. (1979) Purchase intentions and purchase behavior. J. Mar-
keting 43, 65–74.
Morris, C. (1982) Natural exponential families with quadratic variance
functions. Ann. Statist. 10, 65–80.
Morris, C. (1983a) Natural exponential families with quadratic variance
functions: statistical theory. Ann. Statist. 11, 515–529.
Morris, C. (1983b) Parametric empirical Bayes inference: theory and ap-
plications. J. Amer. Statist. Assoc. 78, 47–65.

References
563
Mosteller, F. and Chalmers, T.C. (1992) Some progress and problems in
meta-analysis of clinical trials. Statist. Science. 7, 227–236.
Mosteller, F. and Wallace, D.L. (1984) Applied Bayesian and Classical In-
ference. Springer–Verlag, New York.
Mukerjee, R. and Dey, D.K. (1993) Frequentist validity of posterior quan-
tiles in the presence of a nuisance parameter: higher order asymptotics.
Biometrika 80, 499–505.
M¨uller, P. (1991) A generic approach to posterior integration and Gibbs
sampling. Tech. Report # 91–09, Purdue University, West Lafayette, Indiana.
M¨uller, P. and Vidakovic, B. (1999) Bayesian Inference in Wavelet-Based
Models. Lecture Notes in Statistics, 141, Springer–Verlag, New York.
Murphy, A.H. and Winkler, R.L. (1984) Probability forecasting in meteo-
rology. J. Amer. Statist. Assoc. 79, 489–500.
Musio, M. and Racugno, W. (1999) Discussion of Fernandez and Steel’s
paper. In Bayesian Statistics 6. J.O. Berger, J.M. Bernardo, A.P. Dawid
and A.F.M. Smith (Eds)., 231–233. Oxford University Press, London.
Mykland, P., Tierney, L. and Yu, B. (1995) Regeneration in Markov chain
samplers. J. Amer. Statist. Assoc. 90, 233–241.
Nachbin, L. (1965) The Haar Integral. Van Nostrand, New York.
Naylor, J.C. and Smith, A.F.M. (1982) Application of a method for the
eﬃcient computation of posterior distributions. Appl. Statist. 31, 214–225.
Nelson, D.B. (1990) Stationarity and persistence in the GARCH(1,1) model.
Econometric Theory 6, 318–334.
Newton, M.A. and Raftery, A.E. (1994) Approximate Bayesian inference
by the weighted likelihood boostrap (with discussion). J. Roy. Statist. Soc.,
Ser. B 56, 1–48.
Neyman, J. (1934) On the two diﬀerent aspects of the representative method:
The method of stratiﬁed sampling and the method of purposive selection.
J. Roy. Statist. Soc., Ser. A 97, 558–625.
Neyman, J. (1937) “Smooth” test for goodness of ﬁt. Skand. Aktvariebi-
dokr. 20, 150–199.
Neyman, J. and Pearson, E.S. (1933a) On the problem of the most eﬃcient
tests of statistical hypotheses. Phil. Trans. Royal Soc. Ser. A 231, 289–337.
Neyman, J. and Pearson, E.S. (1933b) The testing of statistical hypothe-
ses in relation to probabilities a priori. Proc. Cambridge Philos. Soc. 24,
492–510.
Neyman, J. and Scott, E.L. (1948) Consistent estimates based on partially
consistent observations. Econometrica 16, 1–32.
Novick, M.R. and Hall, W.J. (1965) A Bayesian indiﬀerence procedure. J.
Amer. Statist. Assoc. 60, 1104–1117.
Nummelin, E. (1984) General Irreducible Markov Chains and Non-Negative
Operators. Cambridge University Press, Cambridge.

564
References
Oh, M.S. (1989) Integration of multimodal functions by Monte Carlo im-
portance sampling, using a mixture as an importance function. Tech. Re-
port, Dept. of Statistics, University of California.
Oh, M.S. and Berger, J.O. (1993) Integration of multimodal functions by
Monte-Carlo importance sampling. J. Amer. Statist. Assoc. 88, 450–456.
O’Hagan, A. (1992) Some Bayesian numerical analysis. In Bayesian Statis-
tics 4. J.O. Berger, J.M. Bernardo, A.P. Dawid and A.F.M. Smith (Eds.).
Oxford University Press, London, 345–355. Oxford University Press, Lon-
don.
O’Hagan, A. (1994) Kendall’s Advanced Theory of Statistics. Volume 2B:
Bayesian Inference. Chapman & Hall, London.
O’Hagan, A. (1995) Fractional Bayes factors for model comparisons. J.
Roy. Statist. Soc., Ser. B 57, 99–138.
O’Hagan, A. (1997) Properties of intrinsic and fractional Bayes factors,
Test 6, 101–118.
O’Hagan, A. and Berger, J.O. (1988) Ranges of posterior probabilities for
quasi-unimodal priors with speciﬁed quantiles. J. Amer. Statist. Assoc. 83,
503–508.
Olkin, I., Petkau, A.J. and Zidek, J.V. (1981) A comparison of n estimators
for the binomial distribution. J. Amer. Statist. Assoc. 76, 637–642.
Olver, F.W.J. (1974) Asymptotics and Special Functions. Academic Press,
New York.
Osborne, C. (1991) Statistical calibration: a review. Int. Statist. Rev. 59,
309–336.
Owen, A. and Zhou, Y. (2000) Safe and eﬀective importance sampling. J.
Amer. Statist. Assoc. 95, 135–143.
Parent, E., Bob´ee, B., Hubert, P. and Miquel, J. (1998) Statistical and
Bayesian Methods in Hydrological Sciences. In Selected Proceedings from
the UNESCO conference in honner of Pr. Bernier. Unesco, IHP-V Techni-
cal Documents in Hydrology N20.
Pearl, J. (1988) Probabilistic Reasoning in Intelligent Systems. Morgan
Kaufman, Palo Alto, California.
Pearson, K. (1894) Contribution to the mathematical theory of evolution.
Proc. Trans. Roy. Soc. A 185, 71–110.
Peddada, S. and Khattree, R. (1986) On Pitman nearness and variance of
estimators. Comm. Stat. 15, 3005–3018.
Peers, H.W. (1968) Conﬁdence properties of Bayesian interval estimates.
J. Roy. Statist. Soc., Ser. B 30, 535–544.
Peers, H.W. (1965) On conﬁdence Points and Bayesian probability points
in the case of several parameters. J. Roy. Statist. Soc., Ser. B 27, 9–16.
Perk, W. (1947) Some observations on inverse probability including a new
indiﬀerence rule. J. Inst. Actuaries 73, 285–312.

References
565
Perron, F. and Giri, N. (1990) On the best equivariant estimator of mean
of a multivariate normal population. J. Multivariate Anal. 32, 1–16.
Pettit, L.I. (1992) Bayes factors for outlier models using the device of imag-
inary observations. J. Amer. Statist. Assoc. 87, 541–545.
Pfangzagl, J. (1968) A characterization of the one parameter exponential
family by existence of uniformly most powerful tests. Sankhya (Ser. A) 30,
147–156.
Phillips, D.M. and Smith, A.F.M. (1996) Bayesian model comparison via
jump diﬀusions. In Markov Chain Monte Carlo in Practice, W.R. Gilks,
S. Richardson and D.J. Spiegelhalter (eds.), 215–240. Chapman & Hall,
London.
Phillips, P.C.B. (1991) Bayesian routes and unit roots: de rebus prioribus
semper est disputandum. J. Appl. Econometrics 6, 435–474.
Pierce, D. (1973) On some diﬃculties in a frequency theory of inference.
Ann. Statist. 1, 241–250.
Pilz, J. (1991) Bayesian Estimation and Experimental Design in Linear
Regression Models (2nd edition). J. Wiley, New York.
Pitman, E.J.G. (1936) Suﬃcient statistics and intrinsic accuracy. Proc.
Cambridge Philos. Soc. 32, 567–579.
Pitman, E.J.G. (1937) The closest estimates of statistical parameters. Proc.
Cambridge Philos. Soc. 33, 212–222.
Pitman, E.J.G. (1939) The estimation of location and scale parameters of
a continuous population of any given form. Biometrika 30, 391–421.
Pitt, M.K. and Shephard, N. (1999) Filtering via simulation: auxiliary par-
ticle ﬁlter. J. Amer. Statist. Assoc. 94, 590–599.
Plessis, B. (1989) Context dependent enhancements for digitized radio-
graphs. MSc. thesis, Dept. of Electrical Engineering, University of Ottawa.
Poincar´e, H. (1902) La Science and l’Hypoth`ese. Flammarion, Paris.
[Reprinted in Champs, 1989.]
Pollock, K. (1991) Modelling capture, recapture and removal statistics for
estimation of demographic parameters for ﬁsh and wildlife populations:
past, present and future. J. Amer. Statist. Assoc. 86, 225–238.
Poirier, D.J. (1995) Intermediate Statistics and Econometrics: a Compar-
ative Approach. MIT Press, Cambridge, Mass.
Popper, K. (1983) Postface to the Logic of Scientiﬁc Discovery. I–Realism
and Science. Hutchinson, London.
Press, J.S. (1989) Bayesian Statistics. J. Wiley, New York.
Qian, W. and Titterington, D.M. (1991) Estimation of parameters in hid-
den Markov models. Phil. Trans. Roy. Soc. London A 337, 407–428.
Racugno, W. (1999) Model Selection. Collana Atti di Congressi, Pitagora
Editrice, Bologna.

566
References
Raftery, A.E. (1988) Inference for the binomial N parameter hierarchical
Bayes approach. Biometrika 75, 355–363.
Raftery, A.E. (1996) Hypothesis Testing and Model Selection Via Poste-
rior Simulation. In Markov chain Monte Carlo in Practice W.R. Gilks,
S.T. Richardson and D.J. Spiegelhalter (eds.). 115–130. Chapman & Hall,
London.
Raftery, A.E. and Lewis, S. (1992) How many iterations in the Gibbs sam-
pler? In Bayesian Statistics 4. J.O. Berger, J.M. Bernardo, A.P. Dawid and
A.F.M.
Smith
(Eds.).
Oxford
University
Press,
London,
763–773.
Oxford University Press, London.
Raftery, A., Madigan, D. and Hoeting, J. (1997) Bayesian Model Averaging
for Linear Regression Models. J. Amer. Statist. Assoc. 92, 179–191.
Raftery, A., Madigan, D. and Volinsky, C. (1996) Accounting for model
uncertainty in survival analysis improves predictive performance (with dis-
cussion). In Bayesian Statistics 5, J.O. Berger, J.M. Bernardo, A.P. Dawid,
D.V. Lindley and A.F.M. Smith (eds.), 323–349. Oxford University Press,
London.
Raftery, A. and Richardson, S. (1995) Model selection for generalized linear
models via GLIB, with application to epidemiology. In Bayesian Biostatis-
tics, D.A. Berry and D.K. Stangl (eds.). Marcel Dekker, New York.
Raiﬀa, H. (1968) Decision Analysis: Introductory Lectures on Choices under
Uncertainty. Addison-Wesley, Reading, Mass.
Raiﬀa, H. and Schlaifer, R. (1961) Applied Statistical Decision Theory.
Division of Research, Graduate School of Business Administration, Harvard
University.
Rao, C.R. (1980) Discussion of J. Berkson’s paper ‘Minimum chi-square,
not maximum likelihood’. Ann. Statist. 8, 482–485.
Rao, C.R. (1981) Some comments on the minimum mean square error as
criterion of estimation. In Statistics and Related Topics, M. Cs¨orgo, D.
Dawson, J.N.K. Rao, and A. Saleh (eds.), 123–143.
Rao, C.R., Keating, J.P. and Mason, R. (1986) The Pitman nearness crite-
rion and its determination. Comm. Statist.–Theory Methods 15, 3173–3191.
Redner, R. and Walker, H. (1984) Mixture densities, maximum likelihood
and the EM algorithm. SIAM Rev. 26, 195–239.
Revuz, D. (1984) Markov Chains (2nd edition). North-Holland, Amsterdam.
Richard, J.F. (1973) Posterior and Predictive Densities for Simultaneous
Equation Models. Springer-Verlag, Berlin.
Richard, J.F. and Tompa, H. (1980) On the evaluation of poly-t density
functions. J. Econometrics 12, 335–351.
Richardson, S., and Green, P.J. (1997) On Bayesian analysis of mixtures
with an unknown number of components (with discussion). J. Roy. Statist.
Soc., Ser. B 59, 731–792.

References
567
Ripley, B. (1986) Statistics, images and pattern recognition. Canad. J.
Statist. 14, 83–111.
Ripley, B. (1987) Stochastic Simulation. J. Wiley, New York.
Ripley, B. (1992) Neural networks. In Networks and Chaos—Statistical
and Probabilistic Aspects, O. Barnorﬀ-Nielsen et al. (eds.). Monographs
in Statistics and Applied Probabilities, Chapman & Hall, London.
Rissanen, J. (1983) A universal prior for integers and estimation by mini-
mum description length. Ann. Statist. 11, 416–431.
Rissanen, J. (1990) Complexity of models. In Complexity, Entropy, and the
Physics of Information 8, W. Zurek (ed.), Addison-Wesley, Reading, Mass.
Robbins, H. (1951) Asymptotically subminimax solutions to compound sta-
tistical decision problems. In Proc. Second Berkeley Symp. Math. Statist.
Probab. 1. University of California Press.
Robbins, H. (1955) An empirical Bayes approach to statistics. In Proc.
Third Berkeley Symp. Math. Statist. Probab.
1. University of California
Press.
Robbins, H. (1964) The empirical Bayes approach to statistical decision
problems. Ann. Math. Statist. 35, 1–20.
Robbins, H. (1983) Some thoughts on empirical Bayes estimation. Ann.
Statist. 1, 713–723.
Robert, C.P. (1988) Performances d’estimateurs `a r´etr´ecisseur en situation
de multicolin´earit´e. Ann. d’Eco. Statist. 10, 97–119.
Robert, C.P. (1990a) Modiﬁed Bessel functions and their applications in
Probability and Statistics. Statist. Prob. Lett., 9, 155–161.
Robert, C.P. (1990b) On some accurate bounds for the quantiles of a non-
central chi-squared distribution. Statist. Prob. Lett. 10, 101–106.
Robert, C.P. (1990c) Hidden mixtures and Bayesian sampling. Rapport
tech. 115, LSTA, Universit´e Paris VI.
Robert, C.P. (1991) Generalized Inverse Normal distributions. Statist. Prob.
Lett. 11, 37–41.
Robert, C.P. (1993a) Prior Feedback: A Bayesian approach to maximum
likelihood estimation. Comput. Statist. 8, 279–294.
Robert, C.P. (1993b) A Note on the Jeﬀreys-Lindley paradox. Statist.
Sinica 3, 601–608.
Robert, C.P. (1995a) Simulation of truncated normal variables. Statist.
Comput. 5, 121–125.
Robert, C.P. (1995b) Convergence control techniques for Markov chain
Monte Carlo algorithms. Statis. Science 10(3), 231–253.
Robert, C.P. (1996a) Inference in mixture models. In Markov Chain Monte
Carlo in Practice, W.R. Gilks, S. Richardson and D.J. Spiegelhalter (eds.),
441–464. Chapman & Hall, London.

568
References
Robert, C.P. (1996b) Intrinsic loss functions. Theory and Decision 40 (2),
191–214.
Robert, C.P. (1999) Two techniques of integration by parts and some appli-
cations. In Zeitschrift for Professor Saleh’s 65th Birthday, E. Amed, (ed.).
The Nova Science Publishers Inc.
Robert, C.P., Bock, M.E. and Casella, G. (1990) Bayes estimators asso-
ciated with uniform distributions on spheres (II): the hierarchical Bayes
approach. Tech. Report BU-1002-M, Cornell University.
Robert, C.P. and Caron, N. (1996) Noninformative Bayesian testing and
neutral Bayes factors. TEST 5, 411–437.
Robert, C.P. and Casella, G. (1990) Improved conﬁdence sets for spherically
symmetric distributions. J. Multivariate Anal. 32, 84–94.
Robert, C.P. and Casella, G. (1993) Improved conﬁdence statements for the
usual multivariate normal conﬁdence set. In
Statistical Decision Theory
and Related Topics V, J.O. Berger and S.S. Gupta (Eds.) Springer-Verlag,
New York., 351–368. Springer–Verlag, New York.
Robert, C.P. and Casella, G. (1994) Distance penalized losses for testing
and conﬁdence set evaluation. Test 3(1), 163–182.
Robert, C.P. and Casella, G. (1999) Monte Carlo Statistical Methods. Sprin-
ger-Verlag, New York.
Robert, C.P. and Casella, G. (2004) Monte Carlo Statistical Methods (sec-
ond edition). Springer–Verlag, New York.
Robert, C.P., Celeux, G. and Diebolt, J. (1993) Bayesian estimation of hid-
den Markov models: A stochastic implementation. Statistics & Probability
Letters 16, 77–83.
Robert, C.P. and Hwang, J.T.G. (1996) Maximum likelihood estimation
under order constraints. J. Amer. Statist. Assoc. 91, 167–173.
Robert, C.P., Hwang, J.T.G. and Strawderman, W.E. (1993) Is Pitman
closeness a reasonable criterion? (with discussion). J. Amer. Statist. Assoc.
88, 57–76.
Robert, C.P. and Mengersen, K.L. (1999) Reparametrisation issues in mix-
ture estimation and their bearings on the Gibbs sampler. Comput. Statis.
Data Ana. 29, 325–343.
Robert, C.P. and Reber, A. (1998) Bayesian Modelling of a Biopharmaceu-
tical Experiment with Heterogeneous Responses. Sankhya B 60(1),
145–160.
Robert, C.P., Ryd´en, T. and Titterington, D.M. (1999a) Convergence con-
trols for MCMC algorithms, with applications to hidden Markov chains. J.
Statist. Computat. Simulat. 64, 327–355.
Robert, C.P., Ryd´en, T. and Titterington, D.M. (1999b) Jump Markov
chain Monte Carlo algorithms for Bayesian inference in hidden Markov
models. J. Roy. Statist. Soc., Ser. B 62(1), 57–75.

References
569
Robert, C.P. and Saleh, A.K.Md.E. (1991) Point estimation and conﬁdence
set estimation in a parallelism model: an empirical Bayes approach. Ann.
d’Eco. Statist. 23, 65–89.
Robert, C.P. and Soubiran, C. (1993) Estimation of a mixture model through
Bayesian sampling and prior feedback. Test 2, 125–146.
Robert, C.P. and Titterington, M. (1998) Reparameterisation strategies for
hidden Markov models and Bayesian approaches to maximum likelihood
estimation. Statist. Comput. 8(2), 145–158.
Roberts, G.O. and Rosenthal, J.S. (1998) Markov chain Monte Carlo:
Some practical implications of theoretical results (with discussion). Can.
J. Statist. 26, 5–32.
Roberts, G. and Polson, N. (1990) A note on the geometric convergence
of the Gibbs sampler. Tech. Report, Dept. of Statistics, University of Not-
tingham.
Roberts, G.O. and Sahu, S.K. (1997) Updating schemes, covariance struc-
ture, blocking and parametrisation for the Gibbs sampler. J. Roy. Statist.
Soc., Ser. B 59, 291–318.
Robertson, T., Wright, F.T. and Dykstra, R.L. (1988) Order Restricted
Statistical Inference. J. Wiley, New York.
Robins, J. and Ritov, Y. (1997) A curse of dimensionality appropriate
(CODA) asymptotic for semiparametric models. Statist. Medicine 16,
285–319.
Robins, J. and Wasserman, L. (2000) Conditioning, likelihood and con-
cepts: A review of some foundational concepts. J. Amer. Statist. Assoc.
95, 1340–1346.
Robinson, G.K. (1976) Properties of Student’s t and of the Behrens-Fisher
solution to the two means problem. Ann. Statist. 4, 963–971.
Robinson, G.K. (1979) Conditional properties of statistical procedures.
Ann. Statist. 7, 742–755.
Robinson, G.K. (1982) Behrens-Fisher problem. In Encyclopedia of Statis-
tical Science 1, S.V. Kotz and N.J. Johnson (eds.), 205–209. J. Wiley, New
York.
Roeder, K. (1992) Density estimation with conﬁdence sets exempliﬁed by
superclusters and voids in galaxies. J. Amer. Statist. Assoc. 85, 617–624.
Roeder, K. and Wasserman, L. (1997) Practical Bayesian density estimation
using mixtures of normals. J. Amer. Statist. Assoc. 92, 894–902.
Romano, J.P. and Siegel, A.F. (1986) Counterexamples in Probability and
Statistics. Wadsworth, Belmont, California.
Rousseau, J. (1997) Performances fr´equentistes des lois de r´ef´erence et pro-
pri´et´es asymptotiques des proc´edures bay´esiennes, Ph.D. thesis, Universit´e
Paris VI.
Rousseau, J. (2000) Coverage properties of one-sided intervals in the dis-
crete case and application to matching priors. Annals of the Institute of

570
References
Statistical Mathematics 52(1), 28–42.
Rousseau, J. (2002) Coverage properties of HPD regions in the discrete
case. J. Multivariate Analysis 83(1), 1–21.
Rousseau, J. (2005) Asymptotic coverage of joint two-sided conﬁdence in-
tervals. Scan. J. Statist. 32, 639–660.
Rubin, D.B. (1984) Bayesianly justiﬁable and relevant frequency calcula-
tions for the applied statistician. Ann. Statist. 12, 1151–1172.
Rubin, G., Umbach, D., Shyu, S.F. and Castillo-Chavez, C. (1992) Using
mark-recapture methodology to estimate the size of a population at risk
for sexually transmitted diseases. Statist. Medicine 11, 1533–1549.
Rubin, H. (1987) A weak system of axioms for rational behavior and the
nonseparability of utility from prior. Statist. Decision 5, 47–58.
Rubinstein, R.Y. (1981) Simulation and the Monte Carlo Method. J. Wiley,
New York.
Rudin, W. (1976) Principles of Real Analysis. McGraw-Hill, New York.
Rue, H. (1995) New loss functions in Bayesian imaging. J. Amer. Statist.
Assoc. 90, 900–908.
Rukhin, A.L. (1978) Universal Bayes estimators. Ann. Statist. 6, 345–351.
Rukhin, A.L. (1988a) Estimated loss and admissible loss estimators. In
Statistical Decision Theory and Related Topics IV, S.S. Gupta and J.O.
Berger (eds.), 409–420. Springer–Verlag, New York.
Rukhin, A.L. (1988b) Loss functions for loss estimations. Ann. Statist. 16,
1262–1269.
Rukhin, A.L. (1995) Admissibility: Survey of a concept in progress. Intern.
Statist. Review 63, 95–115.
Santner, T.J. and Duﬀy, D. (1989) The Statistical Analysis of Discrete
Data. Springer-Verlag, New York.
Savage, L.J. (1954) The Foundations of Statistical Inference. J. Wiley, New
York.
Saxena, K. and Alam, K. (1982) Estimation of the non-centrality parameter
of a chi-squared distribution. Ann. Statist. 10, 1012–1016.
Schaafsma, W., Tolboom, J. and van der Meulen, B. (1989) Discussing
truth or falsity by computing a Q-value. In Statistics, Data Analysis and
Informatics, V. Dodge (ed.). North-Holland, Amsterdam.
Schervish, M.J. (1989) A general method for comparing probability asses-
sors. Ann. Statist. 17, 1856–1879.
Schervish, M.J. (1995) Theory of Statistics. Springer–Verlag, New York.
Schervish, M.J. and Carlin, B.P. (1992) On the convergence of successive
substitution sampling. J. Comput. Graphical Statist. 1, 111–127.
Schmeiser, B. and Shalaby, M. (1980) Acceptance/rejection methods for
beta variate generation. J. Amer. Statist. Assoc. 75, 673–678.

References
571
Schwarz, G. (1978) Estimating the dimension of a model. Annals of Statis-
tics 6, 461–464.
Seber, G.A.F. (1983) Capture-recapture methods. In Encyclopedia of Sta-
tistical Science, S. Kotz and N. Johnson (eds.). J. Wiley, New York.
Seber, G.A.F. (1986) A review of estimation of animal abundance. Biomet-
rics 42, 267–292.
Seidenfeld, T. (1987) Entropy and uncertainty. In Foundations of Statistical
Inference, I.B. MacNeill and G.J. Umphrey (eds.), 259–287. Reidel, Boston.
Seidenfeld, T. (1992) R.A. Fisher’s ﬁducial argument and Bayes’ theorem.
Statist. Sci. 7(3), 358–368.
Sen, P.K., Kubokawa, T. and Saleh, A.K.Md.E. (1989) The Stein paradox
in the sense of Pitman measure of closeness. Ann. Statist. 17, 1375–1384.
Seneta, E. (1993) Lewis Carroll’s pillow problems. Statist. Science 8, 180–186.
Severini, T.A. (1991) On the relationship between Bayesian and non-Bayesian
interval estimates. J. Roy. Statist. Soc., Ser. B 53, 611–618.
Severini, T.A. (1993) Bayesian interval estimates which are also conﬁdence
intervals. J. Roy. Statist. Soc., Ser. B 55, 533–540.
Shafer, G.R. (1996) Art of Causal Conjecture. MIT Press, MIT, Cambridge.
Shannon, C. (1948) A mathematical theory of communication. Bell System
Tech. J. 27, 379–423 and 623–656.
Shao, J. (1989) Monte Carlo approximation in Bayesian decision theory. J.
Amer. Statist. Assoc. 84, 727–732.
Shao, J. and Strawderman, W.E. (1993) Improving on truncated estima-
tors. In Statistical Decision Theory and Related Topics V, J.O. Berger and
S.S. Gupta (Eds.) Springer-Verlag, New York., 369–376. Springer–Verlag,
New York.
Shao, J. and Strawderman, W.E. (1996) Improving on the James–Stein
positive-part estimator. Statistica Sinica 6(1), 259–274.
Shinozaki, N. (1975) Ph.D. thesis, Keio University.
Shinozaki, N. (1980) Estimation of a multivariate normal mean with a class
of quadratic loss. J. Amer. Statist. Assoc. 75, 973–976.
Shinozaki, N. (1984) Simultaneous estimation of location parameters under
quadratic loss. Ann. Statist. 12, 322–335.
Shinozaki, N. (1990) Improved conﬁdence sets for the mean of a multivari-
ate normal distribution. Ann. Inst. Statist. Math. 41, 331–346.
Shorrock, G. (1990) Improved conﬁdence intervals for a normal variance.
Ann. Statist. 18, 972–980.
Silverman, B. (1980) Some asymptotic properties of the probabilistic teacher.
IEEE Trans. Inform. Theory 26, 246–249.
Sivaganesan, S. and Berger, J.O. (1989) Ranges of posterior measures for
priors with unimodal contaminations. Ann. Statist. 17, 868–889.

572
References
Small, C. (1990) A survey of multidimensional medians. Int. Statist. Rev.
58, 263–277.
Smith, A.F.M. (1973) A general Bayesian linear model. J. Roy. Statist.
Soc., Ser. B 35, 67–75.
Smith, A.F.M. (1984) Present position and potential developments: some
personal view on Bayesian statistics. J. Roy. Statist. Soc., Ser. A 147,
245–259.
Smith, A.F.M. and Hills, S. (1992) Parametrizations issues in Bayesian in-
ference. In Bayesian Statistics 4. J.O. Berger, J.M. Bernardo, A.P. Dawid
and A.F.M. Smith (Eds.). Oxford University Press, London, 227–238. Ox-
ford University Press, London.
Smith, A.F.M. and Makov, U.E. (1978) A quasi–Bayes sequential procedure
for mixtures. J. Roy. Statist. Soc., Ser. B 40, 106–112.
Smith, A.F.M. and Roberts, G.O. (1992) Bayesian computation via Gibbs
and related Markov chain Monte Carlo methods (with discussion). J. Roy.
Statist. Soc., Ser. B 55, 3–24.
Smith, A.F.M., Sken, A., Shaw, J., Naylor, J.C. and Dransﬁeld, M. (1985)
The implementations of the Bayesian paradigm. Comm. Statist.–Theory
Methods 14, 1079–1102.
Smith, A.F.M. and Spiegelhalter, D.J. (1982) Bayes factors for linear and
log–linear models with vague prior information. J. Roy. Statist. Soc., Ser.
B 44, 377–387.
Smith, J.Q. (1988) Decision Analysis: A Bayesian Approach. Chapman &
Hall, London.
Spiegelhalter, D.J., Best, N.G., and Carlin, B.P. (1998) Bayesian deviance,
the eﬀective number of parameters and the comparison of arbitrarily com-
plex models. MRC Biostatistics Unit, Cambridge University.
Spiegelhalter, D.J. and Cowell, R. (1992) Learning in probabilistic expert
systems. In Bayesian Statistics 4. J.O. Berger, J.M. Bernardo, A.P. Dawid
and A.F.M. Smith (Eds.). Oxford University Press, London, 447–460. Ox-
ford University Press, London.
Spiegelhalter, D.J., Dawid, A.P., Lauritzen, S.L. and Cowell, R.G. (1993)
Bayesian analysis in expert systems (with discussion). Statist. Science 8,
219–283.
Spiegelhalter, D. and Smith, A.F.M. (1980) Bayes factors and choice criteria
for linear models. J. Roy. Statist. Soc., Ser. B 42, 215–220.
Spiegelhalter, D.J. and Lauritzen, S.L. (1990) Sequential updating of condi-
tional probabilities on directed graphical structures. Networks 20, 579-605.
Spiegelhalter, D.J., Thomas, A., Best, N. and Gilks, W.R. (1995a) BUGS:
Bayesian Inference Using Gibbs Sampling. Version 0.50. Medical Research
Council Biostatistics Unit, Institute of Public Health, Cambridge Univer-
sity.

References
573
Spiegelhalter, D.J., Thomas, A., Best, N. and Gilks, W.R. (1995b) BUGS
Examples Volume 1, Version 0.50. MRC Biostatistics Unit, Cambridge Uni-
versity.
Spiegelhalter, D.J., Thomas, A., Best, N. and Gilks, W.R. (1995c) BUGS
Examples Volume 2, Version 0.50. MRC Biostatistics Unit, Cambridge Uni-
versity.
Srinivasan, C. (1981) Admissible generalized Bayes estimators and exterior
boundary value problems. Sankhya (Ser. A) 43, 1–25.
Srivastava, M. and Bilodeau, M. (1988) Estimation of the MSE matrix of
the Stein estimator. Canad. J. Statist. 16, 153–159.
Stein, C. (1955a) Inadmissibility of the usual estimator for the mean of
a multivariate normal distribution. In Proc. Third Berkeley Symp. Math.
Statist. Probab. 1, 197–206. University of California Press.
Stein, C. (1955b) A necessary and suﬃcient condition for admissibility.
Ann. Math. Statist. 26, 518–522.
Stein, C. (1959) An examination of wide discrepancy between ﬁducial and
conﬁdence intervals. Ann. Math. Statist. 30, 877–880.
Stein, C. (1962a) Conﬁdence sets for the mean of a multivariate normal
distribution (with discussion). J. Roy. Statist. Soc., Ser. B 24, 573–610.
Stein, C. (1962b) A remark on the likelihood principle. J. Roy. Statist.
Soc., Ser. A 125, 565–568.
Stein, C. (1965) Approximation of improper prior measures by prior proba-
bility measures. In Bernoulli, Bayes, Laplace Anniversary Volume. Springer-
Verlag, New York.
Stein, C. (1973) Estimation of the mean of a multivariate distribution. In
Proceedings of the Prague Symposium on Asymptotic Statistics.
Stein, C. (1981) Estimation of the mean of a multivariate normal distribu-
tion. Ann. Statist. 9, 1135–1151.
Stephens, M. (1997) Bayesian methods for mixtures of normal distributions.
Ph.D. thesis, Oxford University.
Stephens, M. (2000) Bayesian methods for mixtures of normal distributions.
Ann. Statist. 28, 40–74.
Steward, G. (1987) Collinearity and least-squares regression. Statist. Sci-
ence 2, 68–100.
Steward, L. (1979) Multiparameter univariate Bayesian analysis. J. Amer.
Statist. Assoc. 74, 684–693.
Steward, L. (1983) Bayesian analysis using Monte Carlo integration—a
powerful methodology for handling some diﬃcult problems. The Statisti-
cian 32, 195–200.
Stigler, S. (1986) The History of Statistics. Belknap, Harvard.
Stone, M. (1967) Generalized Bayes decision functions, admissibility and
the exponential family. Ann. Math. Statist. 38, 818–822.

574
References
Stone, M. (1976) Strong inconsistency from uniform priors (with discus-
sion). J. Amer. Statist. Soc. 71, 114–125.
Strasser, H. (1985) Mathematical Theory of Statistics. W. de Gruyter,
Berlin.
Strawderman, W.E. (1971) Proper Bayes minimax estimators of the mul-
tivariate normal mean. Ann. Math. Statist. 42, 385–388.
Strawderman, W.E. (1973) Proper Bayes minimax estimation of the mul-
tivariate normal mean. Ann. Math. Statist. , 42, 385–388.
Strawderman, W.E. (1974) Minimax estimation of location parameters
for certain spherically symmetric distributions. J. Multivariate Anal. 4,
255–264.
Strawderman, W.E. (2000) Minimaxity. J. Amer. Statist. Assoc. 95,
1364–1368.
Studden, W. (1990) Private communication.
Sweeting, T.J. (1985) Consistent prior distributions for transformed mod-
els. In Bayesian Statistics 2, J.M. Bernardo, M.H. DeGroot, D.V. Lindley
and A.F.M. Smith (eds.), 755–762. Elsevier Science Publishers, Amsterdam.
Tanner, M. (1991) Tools for Statistical Inference: Observed Data and Data
Augmentation Methods. Lecture Notes in Statistics 67, Springer–Verlag,
New York.
Tanner, M. and Wong, W.H. (1987) The calculation of posterior distribu-
tions by data augmentation. J. Amer. Statist. Assoc. 82, 528–550.
Thatcher, A.R. (1964) Relationships between Bayesian and conﬁdence lim-
its in prediction. J. Roy. Statist. Soc., Ser. B 26, 176–210.
Thisted, R.A. and Efron, B. (1987) Did Shakespeare write a newly-discovered
poem? Biometrika 74, 445–468.
Thompson, P.M. (1989) Admissibility of p-value rules. Ph.D. thesis, Uni-
versity of Illinois, Urbana.
Tibshirani, R. (1989) Noninformative priors for one parameter of many.
Biometrika 76, 604–608.
Tierney, L. (1991) Markov chains for exploring posterior distributions.
Computer Sciences and Statistics: Proc. 23d Symp. Interface, 563–570.
Tierney, L. (1994) Markov chains for exploring posterior distributions (with
discussion). Ann. Statist. 22, 1701–1786.
Tierney, L. and Kadane, J.B. (1986) Accurate approximations for posterior
moments and marginal densities. J. Amer. Statist. Assoc. 81, 82–86.
Tierney, L., Kass, R.E. and Kadane, J.B. (1989) Fully exponential Laplace
approximations to expectations and variances of non-positive functions. J.
Amer. Statist. Assoc. 84, 710–716.
Titterington, D.M., Smith, A.F.M. and Makov, U.E. (1985) Statistical
Analysis of Finite Mixture Distributions. J. Wiley, New York.

References
575
Tong, H. (1991) Non-linear Time Series: a Dynamical Systems Approach.
Oxford University Press, London.
Torrie, G.M. and Valleau, J.P. (1977) Nonphysical sampling distributions
in Monte Carlo free-energy estimation: Umbrella sampling. J. Chemical
Physics 23, 187–199.
van der Meulen, B. (1992) Assessing weights of evidence for discussing
classical statistical hypotheses. Ph.D. thesis, University of Groningen.
Van Dijk, H.K. and Kloeck, T. (1984) Experiments with some alternatives
for simple importance sampling in Monte Carlo integration. In Bayesian
Statistics II, J.M. Bernardo, M.H. DeGroot, D.V. Lindley and A.F.M.
Smith (eds.). North-Holland, Amsterdam.
van Eeden, C. and Zidek, J. (1993) Group Bayes estimation of the ex-
ponential mean: a retrospective view of the Wald theory. In
Statistical
Decision Theory and Related Topics V, J.O. Berger and S.S. Gupta (Eds.)
Springer-Verlag, New York., 35–50. Springer–Verlag, New York.
Venn, J. (1886) The Logic of Chance. Macmillan, London.
Verdinelli, I. and Wasserman, L. (1992) Bayesian analysis of outliers prob-
lems using the Gibbs sampler. Statist. Comput. 1, 105–117.
Verdinelli, I. and Wasserman, L. (1995) Computing Bayes Factors Using a
Generalization of the Savage–Dickey Density Ratio. Journal of the Ameri-
can Statistical Association 90, 614-618.
Verdinelli, I. and Wasserman, L. (1998) Bayesian goodness-of-ﬁt testing us-
ing
inﬁnite-dimensional
exponential
families.
Ann.
Statist.
26,
1215–1241.
Villegas, C. (1977) On the representation of ignorance. J. Amer. Statist.
Assoc. 72, 651–654.
Villegas, C. (1990) Bayesian inference in models with euclidian structure.
J. Amer. Statist. Assoc. 85, 1159–1164.
Von Neumann, J. (1951) Various techniques used in connection with ran-
dom digits. J. Resources of the National Bureau of Standards – Applied
Mathematics Series 12, 36–38.
Von Neumann, J. and Morgenstern, O. (1947) Theory of Games and Eco-
nomic Behavior (2nd edition). Princeton University Press, Princeton.
Wakeﬁeld, J.C., Gelfand, A.E. and Smith, A.F.M. (1991) Eﬃcient genera-
tion of random variates via the ratio-of-uniforms method. Statist. Comput.
1, 129–33.
Wald, A. (1950) Statistical Decision Functions. J. Wiley, New York.
Wallace, C.S. and Boulton, D.M. (1975) An invariant Bayes method for
point estimation. Classiﬁcation Society Bulletin 3(3), 11–34.
Walley, P. (1991) Statistical Reasoning with Imprecise Probability. Chap-
man & Hall, London.
Wasserman, L. (1992) Recent methodological advances in robust Bayesian
inference. In Bayesian Statistics 4. J.O. Berger, J.M. Bernardo, A.P. Dawid

576
References
and A.F.M. Smith (Eds.). Oxford University Press, London, 483–490. Ox-
ford University Press, London.
Wasserman, L. (1999) Asymptotic inference for mixture models by using
data-dependent priors. J. Roy. Statist. Soc., Ser. B 61(1), 159–180.
Welch, B.L. (1965) On comparisons between conﬁdence point procedures
in the case of a single parameter. J. Roy. Statist. Soc., Ser. B 27, 1–8.
Welch, B.L. and Peers, H.W. (1963) On formulae for conﬁdence points
based on integrals of weighted likelihoods. J. Roy. Statist. Soc., Ser. B 25,
318–329.
Wells, M.T. (1992) Private communication.
West, M. (1992) Modelling with mixtures. In Bayesian Statistics 4. J.O.
Berger, J.M. Bernardo, A.P. Dawid and A.F.M. Smith (Eds.). Oxford Uni-
versity Press, London, 503–525. Oxford University Press, London.
West, M. and Harrison, J. (1998) Bayesian Forecasting and Dynamic Mod-
els (2nd edition). Springer–Verlag, New York.
Whittaker, J. (1990) Graphical Models in Applied Multivariate Statistics.
Wiley, Chichester.
Wijsman, R.A. (1990) Invariant Measures on Groups and their Use in
Statistics. IMS lecture notes–Monographs Series. Hayward, California.
Wilkinson, G. (1977) On resolving the controversy in statistical inference.
J. Roy. Statist. Soc., Ser. B 39, 119–171.
Wolter, W. (1986) Some coverage error models for census data. J. Amer.
Statist. Assoc. 81, 338–346.
Zabell, S.L. (1989) R.A. Fisher on the history of inverse probability. Statist.
Science 4, 247–263.
Zabell, S.L. (1992) R.A. Fisher and the ﬁducial argument. Statist. Science
7, 369–387.
Zellner, A. (1971) An Introduction to Bayesian Inference in Econometrics.
J. Wiley, New York.
Zellner, A. (1976) Bayesian and non-Bayesian analysis of the regression
model with multivariate Student-t error term. J. Amer. Statist. Assoc. 71,
400–405.
Zellner, A. (1984) Basic Issues in Econometrics. University of Chicago
Press, Chicago.
Zellner, A. (1986a) Bayesian estimation and prediction using asymmetric
loss functions. J. Amer. Statist. Assoc. 81, 446–451.
Zellner, A. (1986b) On assessing prior distributions and Bayesian regression
analysis with g-priors distributions. In Bayesian Inference and Decision
Techniques, P. Goel and A. Zellner (eds.), 233–243. Elsevier North-Holland,
Amsterdam.
Zidek, J.V. (1969) A representation of Bayes invariant procedures in terms
of Haar measure. Ann. Inst. Statist. Math. 21, 291–308.

References
577
Zidek, J.V. (1970) Suﬃcient conditions for the admissibility under squared
error loss of formal Bayes estimators. Ann. Math. Statist. 41, 1444-1447.
Zucchini, W. (1999) Frequentist model choice. Summer school lecture, Cagliari,
Sardinia, 23rd October 1999.
Yao, J.F. and Attali, J.G. (2000) On stability of nonlinear AR processes
with Markov switching. Applied Probability 32, 394–407.


Author Index
Abramovich, F., 47
Abramowitz, M., 150, 154, 293, 505
Adams, M., 368
Aitkin, M., 234, 235, 237, 269, 353
Akaike, H., 20, 166, 353
Alam, K., 98, 136, 154, 178, 287, 411,
454, 478, 502
Albert, J.H., 463
Anderson, T.W., 100, 190
Andrieu, C., 507
Angers, J.F., 143, 187, 460, 472
Arrow, K.S., 58, 83
Attali, J.G., 196
Balakrishnan, N., 519
Bar-Lev, S., 120
Baranchick, A.J., 98
Barbieri, M., 377
Barnard, G.A., 15
Barnett, G., 198, 200
Barron, A., 49, 376
Bartlett, M.S., 138, 162
Basford, K., 339
Basu, D., 36, 168
Baum, L.E., 332
Bauwens, L., 30, 40, 143, 196, 213,
216, 218, 220, 221, 507
Bayarri, M.J., 64
Bayes, T., 9–11, 29, 45
Bechofer, R.E., 216
Bensmail, H., 339
Beran, R., 454
Berg´e, P., 2
Berger, J.O., 7, 15, 18, 20, 28, 34, 46,
58, 65, 69, 71, 79, 92, 94, 98,
99, 109, 112, 126, 131, 133,
135, 136, 140–142, 155, 167,
168, 171, 175, 179, 188, 202,
216, 218, 231, 235–239, 241,
242, 248, 251–253, 255, 259,
262, 264, 265, 268, 271, 273,
278, 279, 282, 283, 293, 318,
344, 349, 350, 353, 368, 378,
394, 397, 400, 403, 411, 412,
414, 418, 421, 435, 438, 441,
444, 445, 447, 452, 460, 463,
464, 467, 470, 472, 475, 477,
479, 484, 488, 490, 492, 493,
498, 507, 514, 517, 519
Berger, R., 206, 230, 254, 255, 258,
272, 276, 280
Bergman, N., 171
Berliner, L.M., 141, 142, 144, 484
Bernardo, J.M., vi, 45, 60, 82, 108,
131, 133, 135, 136, 159, 160,
213, 235, 371, 464, 514, 517
Berry, D.A., 507
Berry, S.M., 44
Bertrand, J., 45, 106
Besag, J., 315, 316, 356, 507
Best, N.G., 334, 335, 353–356, 379,
380, 507
Bhattacharya, R.N., 138
Bickel, P.J., 73, 138, 162
Billingsley, P., 34, 327
Billio, M., 200, 338
Bilodeau, M., 98, 99
Binder, D., 331
Birnbaum, A., 15, 18
Bjørnstad, J., 20
Blackwell, D., 69, 402, 446
Blattberg, R.C., 193, 488
Blyth, C.R., 58, 101, 102, 261, 402
Bob´ee, B., 507
Bock, M.E., 73, 98, 99, 175, 209, 211,
412, 454, 478, 498
B¨ohning, D., 480
Bondar, J.V., 99, 396, 442–444
Boole, G., 106
Bose, S., 102
Boukai, B., 251, 283
Box, G.E.P., 19, 194, 198, 201, 507
Brandwein, A.C., 98, 99, 175
Brewster, J.F., 486

580
Author Index
Brockwell, P.J., 194, 195, 200, 201,
214, 215
Broniatowski, M., 339
Brown, L.D., 65, 67, 69, 71, 84, 85, 94,
97–100, 115, 117, 122, 126,
146, 147, 160, 175, 179, 212,
257, 263, 278, 279, 281, 282,
391, 396, 398, 399, 402, 403,
407–409, 411, 412, 415, 420,
472, 475, 490, 515
Buehler, R.J., 281, 454
Capp´e, O., 318, 388
Carlin, B.P., vi, 113, 139, 293,
353–356, 362–364, 379, 380,
479, 480, 484, 507, 514
Carlin, J.B., vi, 460, 464, 468, 514
Caron, N., 273, 280
Carota, C., 350
Carriquiry, A., 507
Carter, C.K., 198
Carter, G., 484
Casella, G., vi, 2, 7, 14, 21, 31, 37, 41,
43, 48, 58, 70, 73, 79, 87, 99,
169, 179, 193, 206, 219, 230,
249, 254–259, 262, 264, 265,
269, 272–274, 276, 277, 280,
281, 286, 292–295, 298,
301–340, 353, 364, 394, 422,
425, 478, 482, 486, 488, 489,
495, 501
Castillo-Chavez, C., 329
Castledine, B., 183, 208
Castro, I., 375, 384
Celeux, G., 320, 332, 339, 340
Cellier, D., 99, 100, 415, 478
Chalmers, T.C., 25, 458
Chamberlain, G., 54
Chen, J., 264, 298
Chen, M.H., 356–358, 380, 381
Chernoﬀ, H., 216
Chib, S., 200, 345, 361–364
Chickering, D.M., 507
Chow, M.S., 136, 160, 411
Chrystal, G., 106, 207
Chuang, D., 178
Clarke, B., 136
Clayton, D.G., 507
Clevenson, M., 398, 419
Cliﬀord, M.S., 327
Clyde, M., 368, 369, 382
Cohen, A., 264, 265
Conigliani, C., 375, 384
Cornﬁeld, J., 190, 191
Cowell, R.G., 348, 502, 507
Cowles, M.K., 335
Cox, D.R., 6, 18, 44, 186
Crawford, S.L., 319
Cressie, N., 348, 356
Dacunha-Castelle, D., 353
Dalal, S.R., 127, 158, 161, 408, 458
Dale, A.I., 45
Damien, P., 316
Darroch, J., 184
Das Gupta, A., 99, 141, 398, 414
Datta, G.S., 138
Davis, P.A., 194, 195, 200, 201, 214,
215
Dawid, A.P., 108, 140, 152, 153, 348,
454, 502, 503, 507
de Finetti, B., 46, 115, 160
Deely, J.J., 215, 216, 460, 479, 484
DeGroot, M.H., 54, 56, 58, 64, 85,
108, 159, 232, 319
Delampady, M., vi, 141, 231, 251–253,
268, 271, 449
Dellaportas, P., 504
Dempster, A.P., 21, 319, 339, 457
DeRobertis, L., 142
Devroye, L., 2, 295, 479
Dey, D., 8, 138, 353, 357
Diaconis, P., 43, 48, 115, 121–123,
151, 161, 181, 293, 408, 458
DiCiccio, T.J., 138, 162
Dickey, J.M., 119, 174, 288, 465
Diebolt, J., 309, 320, 327, 332, 339,
352, 425
Doucet, A., 171, 286, 298, 507
Dransﬁeld, M., 293, 300
Draper, D., 460
Dr`eze, J.H., 143, 220
Dudewicz, E.J., 216
Duﬀy, D., 203
Dumouchel, W.M., 460
Dupuis, J.A., 107, 144, 183, 369,
371–373, 389
Dykstra, R.L., 21, 35, 169
Dynkin, E.B., 147

Author Index
581
Eaton, M.L., 99, 100, 190, 399, 400,
425, 435, 436, 438–441,
449–451
Eberly, L., 262
Eco, U., 368
Efron, B., 14, 93, 102, 186, 487
Eichenauer, J., 73
Engle, R.F., 218
Enis, P., 120
Escobar, M.D., 8, 342, 345
Evans, M., 19
Fabius, J., 47
Fang, K.T., 100
Farrell, R.H., 79, 256, 257, 259, 273,
407, 412, 422
Feller, W., 120, 204, 398
Ferguson, T.S., 8, 47, 54, 69, 206, 402
Fernandez, C., 39
Feyerabend, P., 511
Field, A., 6
Fieller, E.C., 43, 276
Fienberg, S., 64, 108
Fishburn, P.C., 54, 58
Fisher, R.A., 8, 15, 20, 45, 130, 249
Fishman, G.S., 295, 523
Fitzgerald, W.J., 507
Flury, B.K., viii
Forster, J.J., 504
Foster, D.P., 212, 382
Fouley, J.L., 460
Fourdrinier, D., 74, 79, 99, 100, 137,
415, 478, 505, 506
Fraisse, A.M., 99, 411, 415, 421
Francq, C., 196
Fraser, D.A.S., 19, 167
Freedman, D.A., 48
de Freitas, N., 298
Gassiat, E., 353
Gatsonis, C., 73, 206, 507
Gauss, C.F., 13, 52, 77, 186
Geisser, S., 190, 191
Gelfand, A.E., 223, 309, 311, 313, 314,
318, 346, 353, 357, 360, 361,
484
Gelman, A., vi, 303, 335, 359, 386,
460, 464, 468, 507, 514
Geman, D., 307, 507
Geman, S., 307, 507
Genest, C., 83
Gentle, J.E, 2
Gentle, J.E., 295, 523
George, E.I., 98, 99, 175, 183, 193,
212, 264, 290, 307, 310, 315,
369, 382, 460, 488, 489, 498
Geweke, J., 329, 335, 507
Geyer, C.J., 169, 286, 303, 310
Ghosh, J.K., 138
Ghosh, M., 98, 101, 102, 136, 138,
139, 162, 193, 210, 488
Gianola, D., 460
Gibbons, J.D., 216
Gilks, W.R., 301, 322, 334, 335, 507
Gill, R.D., 171
Gini, C., 46
Giri, N., 441
Girshick, M.A., 69, 402, 446
Giudici, P., 504
Givens, G.H., 25, 458
Gleick, J., 2
Gleser, L.J., 276, 378, 441
Godwill, S., 507
Goel, P.K., 215, 217
Goldstein, M., 473
Good, I.J., 109, 141, 144, 228, 230,
462, 497, 516
Gordon, N.J., 171, 298
Gouri´eroux, C., 32, 162, 212, 218, 353,
372
Goutis, C., viii, 112, 143, 249, 264,
281, 369–372
Green, P.J., 47, 48, 316, 345, 352, 363,
364, 504
Greenberg, E., 200
Grenander, U., 47, 366, 386
Gruet, M.A., 318, 364–366
Gr¨unwald, P., 507
Guihenneuc–Jouyaux, C., 220, 458,
459
Gupta, S.S., 215, 216
Gutmann, S., 99
Gy¨orﬁ, L., 479
Hadjicostas, P., 44
Haﬀ, L., 98
Hajek, J., 6
Hald, A., 45
Haldane, J., 29, 166, 262
Hall, P., 14

582
Author Index
Hall, W.J., 29, 127, 158, 161, 408, 458
Hamilton, J.D., 196
Hammersley, J.M., 327
Hansen, M., 140
Harris, J.E., 460
Harrison, J., 196, 200
Hartigan, J.A., 28, 142, 248, 262, 275,
495, 496
Has’minskii, R., 46, 48, 166, 513
Hastings, W.K., 303, 305
Healy, J.D., 441
Heath, D., 8
Heckerman, D., 507
Heidelberger, P., 335
Heitjan, D.F., 333
Helland, I.S., 454
Hesterberg, T., 357
Hillerman, T., ix
Hills, S., 293
Hinkley, D., 44, 283
Hjort, N.L., 47
Hoaglin, D., 385
Hobert, J.P., 31, 41, 48, 337, 338, 424,
425, 460
Hodges, J.S., 507
Hoerl, A., 473
Hora, R.B., 454
Huber, P.J., 80, 142
Hubert, P., 507
Huerta, G., 198
Hui, S., 484
Hurn, M., 320, 340
Hutchinson, D., 261
Hwang, J.T.G., 21, 78, 79, 84, 94, 95,
98, 99, 102, 136, 179, 256,
257, 259, 264, 265, 273, 276,
277, 378, 391, 396–398, 403,
412, 415, 420, 422, 423, 488,
501
Ibragimov, I., 46, 48, 166, 513
Ibrahim, J., 298, 356, 358, 381
Im, S., 460
Jacquier, E., 219, 507
James, W., 98, 101, 120, 413
Jaynes, E.T., 109, 152, 153, 516
Jeﬀerys, H., 368
Jeﬀreys, H., 45, 46, 61, 106, 108, 109,
116, 129, 131, 140, 181, 207,
210, 228, 235, 248, 349, 368,
514
Jenkins, G.M., 194, 198, 201
Johnson, B.M., 416, 422
Johnson, N. L., 519
Johnston, R.W., 98
Johnstone, D.J., 271
Johnstone, I.M., 73, 94, 99, 179, 397,
399
Jones, M.C., 200
Jordan, R., iii, v, 1, 51, 105, 165, 223,
285, 343, 391, 427, 457, 507
Joshi, V.M., 167, 263, 501
Judge, G., 98, 412
Kadane, J.B., 178, 298–300, 319, 324
Kariya, T., 441
Karlin, S., 243, 394, 420
Kass, R.E., 111, 127–129, 133, 136,
140, 145, 155, 162, 228,
299–301, 324, 350, 353, 488,
507
Keating, J.P., 101, 102
Keeney, R.L., 59
Kelker, D., 31, 100
Kemp, A. W., 519
Kemperman, J., 43
Kempthorne, P.J., 73, 144, 408, 517
Kendall, M., 483
Kennard, R., 473
Keynes, J.M., 45
Khattree, R., 101
Kiefer, J., 250, 267, 281, 282, 442
Kiiveri, H., 503
King, A., 310, 314
Kirby, A.J., 459, 507
Kleibergen, F., 218
Kohn, R., 198, 200
Kokaram, A.C., 507
Kolmogorov, A., 45
Kontkanen, P., 507
Koo, J.O., 216
Koopman, B., 116
Kotz, S., 519
Krishnan, T., 339
Kubokawa, T., 42, 99, 101, 213, 477,
486, 498, 500
Lad, F., 45
Laird, N.M., 21, 319, 339, 480, 484

Author Index
583
Laplace, P.S., 9–12, 20, 29, 45, 52, 59,
80, 127, 166, 180, 181, 186,
298
Lasserre, V., 458, 459
Lauritzen, S., 147
Lauritzen, S.L., 348, 459, 492, 502,
503, 507
Lavielle, M., 340
Lavine, M., 47
Lawley, D.N., 162
Le Cam, L., 69, 126, 167, 444
Lee, T.M., 223
Legendre, A., 13, 77
Lehmann, E.L., 6, 7, 14, 21, 31, 43,
70, 98, 99, 242, 244, 246,
263, 281, 282, 353, 393, 394,
416, 436, 437, 441, 444, 446,
448, 513
Lehn, J., 73
Lenk, P., 4, 345, 376
Leonard, T., 300
Letac, G., 118, 120
Levit, B.Y., 171
Lewis, S., 303, 335
Lindley, D.V., 13, 28, 29, 46, 53, 78,
106, 108, 138, 144, 190, 191,
210, 248, 257, 263, 271, 279,
299, 460, 462, 465, 470, 473,
479, 484, 493, 514
Liseo, B., 138, 377
Liu, J.S., 310, 314, 338
Louis, T.A., vi, 113, 283, 293, 479,
480, 484, 507, 514
Lu, K., 79, 99, 179, 264, 490
Lubrano, M., 196, 213, 216, 218, 220,
221, 507
Lwin, T., 479, 480, 484
Maatta, J., 281, 486
MacGibbon, B.K., 73, 143, 206, 460
Machina, G., 54
MacLachlan, G., 339
Madigan, D., 366–368, 382, 502, 504
Makita, S., 500
Makov, U.E., 321
Marin, J.-M., 196, 198, 200, 298
Maritz, J.S., 479, 480, 484
Marsaglia, G., 334
Mason, R., 101
McCullagh, P., 354, 359, 377, 463
McCulloch, R.E., 369, 371, 507
McNeil, A.J., 507
Meeden, G., 265
Meng, X.L., 337–339, 358, 359, 386
Mengersen, K.L., 25, 41, 220, 306,
340, 345, 352, 371, 458
Metropolis, N., 294, 301, 303
Meyn, S.P., 194, 301, 303, 398
Miller, M.I., 47, 366, 386
Milnes, P., 442–444
Miquel, J., 507
Mira, A., 316
Monahan, J.F., 197
Monette, G., 19, 167
Monfort, A., 32, 162, 200, 212, 338,
353, 372
Moors, J.J.A., 422
Mora, M., 118
Morales, J.A., 143
Morgenstern, O., 54
Morisson, D., 482, 483
Morita, S., 500
Morris, C., 118, 148, 293, 322, 479,
480, 486, 487, 497
Mortera, J., 251
Mosteller, F., 25, 181, 185, 385, 458
Moulines, E., 340
Mukerjee, R., 136, 138
M¨uller, P., 8, 47, 293
Murphy, A.H., 64
Musio, M., 40
Myllym¨aki, T., 507
Nachbin, L., 437, 441
Nagakura, K., 500
Naylor, J.C., 293, 300
Neal, R.M., 507
Nelder, J., 354, 359, 377, 463
Nelson, D.B., 218
Newton, M.A., 357
Neyman, J., 16, 46, 65, 80, 180, 375,
384
Ng, K.W., 167
Novick, M.R., 29
O’Hagan, A., vi, 141, 142, 239, 240,
375, 384
d’Occam, W., 368
Olkin, I., 36, 216
Olver, F.W.J., 299

584
Author Index
Osborne, C., 43, 213
Owen, A., 357
Panchapakesan, S., 216
Parent, E., 507
Parmigiani, G., 350
Pathak, P.K., 101, 102
Pearl, J., 268
Pearson, E.S., 16, 32, 65, 80
Pearson, K., 45, 339
Peddada, S., 101
Peers, H.W., 138, 211
Pemantle, R., 78, 257, 259
Pericchi, L.R., 235–239, 241, 242, 344,
349, 350, 353, 378
Perk, W., 146
Perron, F., 441
Petkau, A.J., 36
Petrella, L., 377
Petrie, T., 332
Petrone, S., 375
Pettit, L.I., 235
Pfanzagl, J., 244
Philippe, A., 140, 278, 318, 364–366
Phillips, D.B., 47, 218, 345, 366, 386
Pierce, D., 71, 281
Pitman, E.J.G., 101, 116, 147, 430
Pitt, M.K., 507
Plessis, B., 5, 290
Poincar´e, H., 510, 516
Poirier, D.J., 507
Pollock, K., 182
Polson, N.G., 219, 350, 507
Pommeau, Y., 2
Popper, K., 181, 508, 511
Press, J.S., 108, 191
Price, R., 45
Qian, W., 339
Racugno, W., 40, 344
Raftery, A.E., 183, 228, 303, 335, 339,
350, 353, 354, 357, 366–368,
370, 382
Raiﬀa, H., 18, 34, 40, 58, 87–89, 114,
183
Ralescu, S., 98
Rao, C.R., 101
Rao, R., 138
Raoult, J.P., 411, 421
Reber, A., 462, 464, 469, 470
Redner, R., 339
Reid, N., 186
Richard, J.F., 30, 143, 196, 213, 216,
218, 220, 221, 507
Richardson, S., 48, 301, 345, 352, 364,
370, 458, 459, 507
Ripley, B.D., 334, 366, 386, 507
Rissanen, J., 20, 140
Ritov, Y., 44, 45, 48
Robbins, H., 144, 479, 481
Robert, C.P., 2, 21, 37, 41, 42, 65, 73,
79, 82, 94, 95, 99, 100, 102,
140, 150, 151, 169, 171, 175,
179, 183, 196, 198, 200, 208,
209, 213, 219, 220, 234, 235,
256, 257, 259, 264, 265, 269,
273, 274, 277, 278, 280, 286,
290, 292–295, 298, 301–340,
345, 352, 364–366, 369–373,
388, 389, 400, 408, 411, 414,
415, 421, 422, 424, 425, 441,
454, 462, 464, 467, 469, 470,
475, 478, 484, 486, 492, 493,
498, 505, 506
Roberts, G.O., 314, 316
Robertson, T., 21, 35, 169
Robins, J., 7, 44, 45, 48, 348
Robinson, G.K., 250, 267, 276, 281,
282
Roeder, K., 340, 345
Rolph, J., 484
Romano, J.P., 33, 36
Ronchetti, E., 6
Rosenbluth, A.W., 301, 303
Rosenbluth, M.N., 301, 303
Rosenthal, J.S., 316
Rossi, P.E., 219, 371, 507
Rousseau, J., 138, 140, 301
Roy, M., 99, 411, 415, 421
Rubin, D.B., vi, 21, 58, 303, 319, 333,
335, 339, 460, 464, 468, 514
Rubin, G., 329
Rubin, H., 58, 83, 136, 215, 217, 243,
420
Rubinstein, R.Y., 323
Rudin, W., 437
Rue, H., 96, 97
Rukhin, A.L., 79, 85, 93, 99, 179, 267,
391, 409, 475

Author Index
585
Ryd´en, T., 320, 332, 364, 388
Sackrowitz, H., 265
Sahu, S.K., 314
Saleh, A.K.Md.E., 99, 101, 193, 486,
488
San Cristobal, M., 460
Santner, T.J., 203
Savage, L.J., 8, 46, 167
Saxena, K., 136, 154, 178, 287, 411,
454
Schaafsma, W., 256
Schervish, M.J., vi, 49, 64, 78, 88, 257,
299
Schlaifer, R., 18, 34, 40, 58, 88, 89,
114, 183
Schwartz, P.C., 353
Seber, G.A.F., 182
Seidenfeld, T., 45, 111, 145
Sellke, T., 141, 238, 251, 252, 255,
259, 273
Sen, P.K., 101, 102, 193, 488
Seneta, E., 207
Severini, T.A., 138
Shafer, G., 348
Shannon, C., 109, 140
Shao, J., 98, 298, 356–358, 380, 381
Sharples, L.D., 507
Shaw, J., 293, 300
Sheather, S., 200
Shephard, N., 507
Shinozaki, N., 78, 92, 99, 264, 398
Shorrock, G., 264
Shyu, S.F., 329
Sid`ak, Z., 6
Siegel, A.F., 33, 36
Silander, T., 507
Silverman, B.W., 47
Singpurwalla, N., 507
Sinha, B.K., 398
Sinha, D., 8, 99
Sivaganesan, S., 141, 142
Sken, A., 293, 300
Small, M.J., 319
Smith, A.F.M., vi, 47, 58, 60, 82, 108,
159, 160, 190, 191, 213, 223,
235, 280, 293, 300, 309, 311,
313, 314, 321, 345, 351, 366,
371, 386, 462, 464, 465, 470,
473, 493, 514
Smith, D.D., 25, 458
Smith, J.Q., 59, 60, 64, 86, 87, 108,
178, 201, 204, 515
Sobel, M., 216
Soubiran, C., 331
Spatinas, T., 47
Speed, T.P., 503
Spiegelhalter, D.J., 235, 280, 301, 318,
334, 348, 351, 353–356, 379,
380, 459, 492, 502, 507
Srinivasan, C., 99, 112, 398, 411, 412,
421
Srivastava, M.S., 98, 139
Stangl, D.K., 507
Stark, J.A., 507
Steel, M., 39
Steﬀey, D., 162, 300, 488
Stegun, I., 150, 154, 293, 505
Stein, C., 46, 94, 97–99, 101, 120, 132,
168, 175, 202, 263, 397, 402,
405, 407, 413, 439, 442, 475,
494
Stephens, D., 386, 387
Stephens, M., 47, 48, 340, 366
Stern, H.S., vi, 460, 464, 468, 514
Stern, S.E., 138, 162
Steward, G., 193
Stigler, S., 8, 12, 13, 33, 45, 186, 293
Stone, M., 128, 152, 153, 167, 168, 454
Strasser, H., 69, 436, 444
Strawderman, W.E., 65, 67, 71, 73,
94, 95, 98, 99, 102, 137, 175,
206, 264, 397, 477, 495, 498
Stuart, A., 483
Studden, W.J., 24, 38, 141
Sudderth, W.J., 8
Sweeting, T.J., 138
Tan, K.K.C, 335
Tanner, M., 308, 311
Teicher, H., 160
Teller, A.H., 301, 303
Teller, E., 301, 303
Thatcher, A.R., 275
Thisted, R.A., 186
Thomas, A., 334
Thompson, E.A., 169, 286
Thompson, P.M., 249
Tiao, G.C., 19, 507
Tibshirani, R., 132, 140

586
Author Index
Tierney, L., 298, 300, 316, 324
Tirri, H., 507
Titterington, D.M., 320, 332, 339,
340, 364
Tolboom, J., 256
Tompa, H., 143, 220
Tong, H., 195
Torrie, G.M., 381
Tsui, K., 98
Tukey, J.W., 385
Tweedie, R.L., 25, 194, 301, 303, 306,
398, 458
Ulam, S., 294
Ullah, A., 99, 264
Umbach, D., 329
Valleau, J.P., 381
van der Meulen, B., 78, 256
van Dijk, H.K., 218
van Dyk, D.A., 337–339
Van Eeden, C., 83
Vardeman, S., 265
Venn, J., 45, 106
Verdinelli, I., 293, 339, 375, 376, 385,
507
Vidakovic, B., 8, 47, 293
Vidal, C., 2
Villegas, C., 29, 453
Vines, K., 335
Volinsky, C., 366
Von Neumann, J., 54, 294
Wakeﬁeld, J.C., 316
Wald, A., 16, 46, 54, 65, 282, 410
Walker, H., 339
Wallace, D.L., 185
Walley, P., 141, 156, 278, 457
Wang, Y., 251, 283
Wasserman, L., 7, 44, 45, 49, 111,
127–129, 133, 136, 140, 141,
143, 145, 155, 293, 339, 340,
345, 348, 352, 375, 376, 385
Welch, B.L., 138, 211
Welch, P.D., 335
Wells, M.T., 58, 79, 92, 99, 137, 249,
256, 257, 259, 273, 319, 330,
422
West, M., 8, 196, 198, 200, 342, 345
Whittaker, J., 502
Wijsman, R.A., 436
Wild, P., 322
Wilkinson, G., 46
Winkler, R.L., 64
Wolpert, R., 15, 18, 20, 34, 79, 167,
168, 202, 279, 282, 514
Wolter, W., 183
Wong, W.H., 308, 310, 311, 314, 358
Wright, F.T., 21, 35, 169
Wu, Y.N., 338
Yahav, J.A., 216
Yang, M.C., 210
Yang, R., 218
Yao, Y.C., 196
Ylvisaker, D., 115, 121–123, 151, 161,
408, 458
York, J., 502, 504
Yu, B., 140
Zabell, S.L., 45, 46, 207, 293
Zako¨ıan, J.M., 196
Zaman, A., 334
Zellner, A., 92, 143, 191–193, 378, 507
Zhou, Y., 357
Zidek, J.V., 36, 83, 152, 153, 397, 398,
417, 419, 439, 454, 486
Zucchini, W., 142

Subject Index
a prioris, 510
acceptance level, 223, 226, 242
conventional, 228, 234, 249
acceptance ratio, 304, 305
accidents in Michigan, 4, 345
accuracy evaluation, 79
admissibility, 74, 75, 78, 83, 137
and recurrence, 399
necessary condition, 257
admissible estimator
as limit of Bayes estimators, 257
Bayes, 75
least-squares, 97
unique, 74
with inﬁnite Bayes risk, 393
AIC, see Akaike’s information
criterion
AIDS, 458
Akaike’s information criterion (AIC),
353, 354, 382
algorithm, 136
ARMS, 307, 335
Durbin–Levinson, see
Durbin–Levinson recursion
EM, see EM
envelope simulation, 322
Gibbs, see Gibbs sampling
MCMC, 302
Metropolis–Hastings, 303, 305,
325
random-walk, 305, 306
allocation map, 220
α-level, 249
analysis
conditional, 281
of variance, 137, 216
robustness, 141, 144
sensitivity, 141, 515
analytic integration, 285
analytical processing, 318
animal
biology, 180, 182
breeding, 460, 461
approach
conditional, 267
empirical Bayes, 25
ﬁducial, 20
frequentist, 16, 61
nonparametric, 6, 344
parametric, 5, 7
approximation, 126
Laplace, 49, 298–301, 319, 324
marginal density, 361
numerical, 292
saddlepoint, 307
second order χ2
k, 162
AR(1), see model, AR(1)
ARMA, see model, ARMA
asbestos, 224
asymptotic validation
of Bayes estimators, 48
of empirical Bayes method, 484
of nonparametric methods, 6
of the m.l.e., 21
author identiﬁcation, 185
autocorrelation
inverse partial, 200
partial, see partial
autocorrelations
autocovariance, 198
autoregressive, see model, AR
auxiliary variable, 288, 316, 317, 329
axioms, 108
Bayesian, 10, 27, 31, 158,
508–517
choice, 435
coherence, 157
gamble, 156
probability, 45
statistics, 14, 31
Bartlett correction, 138, 162
for posteriors, 162
batch sampling, 303

588
Subject Index
Bayes
best center, 275
empirical, 113, 478
and Stein eﬀect, 485–490
Bayes, 505
drawback, 488, 490
modeling, 290
nonparametric, 479
parametric, 480
test, 483
estimator, 63
admissible, 392
admissible generalized, 403
analytic, 175
and admissibility, 75
best equivariant, 439
computation of, 285, 513
diﬀerential representation, 397
eﬀective calculation of, 514
for invariant measure, 436
generalized, 64, 72, 131, 257,
395, 396, 409, 412
inadmissible, 392
inconsistent, 48
limit of, 408
linear, 115
minimax, 71, 73
minimax generalized, 465
minimax proper, 495
of loss, 179
proper, 404
proper best equivariant, 441
pseudo-, 99
randomized, 71
universal, 85
hierarchical, 113, 143
and robustness, 458
computational diﬃculties,
468–469
decomposition, 466–467
estimator, 287, 474
model, 460
motivations, 462–465
information criterion (BIC), 353,
382
minimaxity, 517
risk, 63
inﬁnite, 66
rule, 173
theorem, 8–9, 348, 509
Bayes factor, 227–229, 231, 235, 350
approximate, 228
arithmetic intrinsic, 238, 378
fractional, 240, 271, 378
geometric intrinsic, 238
improper prior, 351
median intrinsic, 238
pseudo-, 237, 240, 241
choice of, 241
coherence of, 351
computation of, 242
Bayesian
approach
coherence of, 507, 511
criticism of, 514
noninformative, 516
approximation, 321
computation, 293
computational problem, 317
deviance associated, 379
imaging, 64, 96
inference, 126
least favorable answer, 251
network, 507
nonparametric, 8, 46, 375
paradigm, 457
robustness, 144
software, 293, 517
statistical model, 9
testing, 223, 227
UMP test, 247
updating principle, 145
vs. frequentist, 224
Behrens–Fisher problem, 276
belief, 509
Berger’s
paradox, 277
phenomenon, 99
reconciliation, 282–283, 517
best equivariant estimator, 429, 434,
435, 486, 513
existence of, 430
betting procedure, 281
BIC, see Bayes information criterion
biostatistic, 507
birth and death, 365
continuous time process, 366
jump process, 386, 387
blocking, 314
Blyth’s condition, 403, 407

Subject Index
589
bootstrap, 14
bounded
loss, 57, 178, 259, 395, 401, 407
parameter, 79
parameter space, 168
risk set, 409, 421
Brier score, 87
BUGS, 334, 351
buses, 3
C, 335
calculus of uncertainty, 2
calibration, 212, 276
expert, 64
linear, 42, 133, 139, 377, 378
conﬁdence interval, 378
canonical moments, 38
capture-recapture, 3, 107, 182, 184,
186, 289, 308
censoring, 34
chaos, 2
χ2, see distribution, chi-squared
classiﬁcation, 338
clique, 467, 503
perfect, 503
closure, 27
coarse data, 333
CODA, 302, 334, 335
coherence, 7, 23, 31, 89, 90, 128, 146,
157, 509, 518
of Bayes factors, 240
coin, 123
coincidences, 181
collinearity, 473
committee, 83
complete class, 244, 257, 258, 402,
409–411
essentially, 258
minimal, 420
computational
diﬃculties, 286
method, 224
technique, 26
tool, 46
condition
detailed balance, 304, 325, 364,
387
positivity, 309
Stein necessary and suﬃcient,
407
STUB, 502
suﬃcient admissibility, 394
Blyth, see Blyth’s condition
conditional frequentist perspective,
283
conditioning, 465
conﬁdence
index, 267
interval, 202
level, 26
reported, 489
region, 17, 22, 25, 26, 99, 179, 259
α-level, 263
as credible region, 261
eﬃcient, 264
not connected, 262
recentered, 264, 277, 488, 489
conjugate family, 120
minimal, 114, 146
conjugate prior, 114, 116, 118,
121–123, 143, 318
and maximum entropy prior, 149
and the Jeﬀreys prior, 132
and BUGS, 335
classes, 141
hyperparameter, 127
mixture of, 124, 146, 161, 408,
484
natural, 160
normal, 187
consistency, 21, 134, 166, 377, 427
contingency table, 203
convergence
assessment of, 469
diagnostic, see diagnostic
geometric, 306, 309, 320, 327, 332
quadratic, 407
speed of, 302
convexity, 142
Cornell University, 267, 329
covariance matrix, 193
Coyote Waits, ix
criterion
minimax, 66
Pitman closeness, 94
curse of dimensionality, 294
DAG, see directed acyclic graph
dairy cow, 109

590
Subject Index
data augmentation, 308, 310, 313,
314, 326, 328
decision, 51
consequence of, 54
error, 52
for model choice, 349
optimal, 52, 58, 88
unique, 243, 511
penalty, 52
randomized, 54
space, 52, 60, 250
compact, 69
tree, 60
under uncertainty, 58
Decision Theory, see theory
deer, 182
degeneracy phenomenon, 200
deviance, 354, 377
Bayesian, 355
penalized, 354
deviance information criterion (DIC),
354
diagnostic
autocorrelation, 335
convergence, 302
software, 335
diﬀerential equation, 139
distance
between distributions, 344
entropy, 82
Hellinger, 49, 82
Kullback–Leibler, see divergence,
Kullback–Leibler
Prohorov, 126
total variation, 161
distribution
arcsine, 204
Bernoulli, 160, 181
beta, 32, 107, 111, 144
beta-binomial, 308, 309, 468, 482
beta-Pascal, 183
Cauchy, 21, 95, 112, 126, 187, 286
chi-squared (χ2), 162, 394
noncentral, see distribution,
noncentral chi-squared
class of, 141
conjugate, 143, 175
Dirichlet, 47, 116
double exponential, 13
F, see Fisher’s F-distribution
ﬁnite support, 142
generalized, 12
inverse normal, 118, 150, 441
geometric, 206
Haldane, 202
half-normal, 384
hypergeometric, 3, 182
improper, 28
invariant probability, 437
inverse gamma, 188, 197
inverse gaussian, 147
Jeﬀreys, 187
joint, 22
Laplace, 13, 40
least favorable, 28, 70, 71, 73, 246
for testing, 246
logistic, 204
marginal, 22, 109, 143, 481
maximum entropy, 110
mixture, see mixture
monotone likelihood ratio, 254
multimodal, 287
natural conjugate, 120
negative binomial, 205
generalized, 424
noncentral, 120
noncentral chi-squared, 21, 133,
209, 287, 295, 333, 335, 424,
449
noninformative, see
noninformative prior
normal, 107, 111, 117, 186, 197
Pareto, 116, 118, 132, 146
Poisson, 120, 446, 479
posterior, see posterior
predictive, 22
prior, see prior
selection of a prior, 65
spherically symmetric, 31, 99,
100, 147, 264
stationary, 194, 213, 302–304, 308
Student’s t, 84, 118, 143, 144,
187–189, 472
Dickey’s decomposition of,
119, 288
mixture, 221
symmetric, 146
target, 304, 307
truncated normal, 329
unimodal, 156, 253

Subject Index
591
Weibull, 21, 35, 305, 317
Wishart, 147, 189, 191, 331
divergence, 370
Kullback–Leibler, 82, 110, 136,
142, 350, 371, 372, 382
domination
stochastic, 84, 102
universal, 84
duality
between loss and prior, 53, 78
principle, 309
Durbin–Levinson recursion, 197, 214
Eaton’s suﬃcient admissibility
condition, 425–426
econometrics, 54, 212, 349, 507
Edgeworth expansion, 138
eﬃciency, 21
in inference, 511
EM algorithm, 338, 339
steps, 339
encompassing, 349
entropy, 109
loss, see loss
maximum, 109
environmetrics, 507
equation
observation, 200
state, 200
equiprobability of elementary events,
12, 127
ergodicity, 301–303
uniform, 302
error
type–one, 81, 242
type–two, 81, 242, 250
estimate, 61
estimation
and evaluation, 7
and testing, 7
mixture, 142, 230, 517
nonparametric, 480
unbiased, 179
estimator, 61
admissible minimax, 137
Bayes, see Bayes, estimator
best equivariant, 131, 429
continuous risk, 401
derivation of optimal, 65
equivariant, 432
inadmissible, 74
inconsistent, 48
inverse regression, 213
James–Stein, 96, 98, 120, 264,
275, 484, 488
positive-part, 68, 98
truncated, 175, 211, 411, 485
least-squares, 32, 97, 190
maximum a posteriori (MAP),
93, 96, 162, 166, 348
maximum likelihood, 20, 122
penalized, 166
minimax, 67, 69
equivariant, 444
monotone, 402
performance of, 266
Pitman, 431
pseudo-Bayes, 211
randomized, 65, 66, 74, 77, 80
Rao-Blackwell, 469
ridge, 473
shrinkage, 84, 98, 454, 472
matricial, 412
multiple, 99, 500
unbiased, 32
unique minimax, 75, 274
evidence, 18
exchangeability, 159
expert
calibration, 64
ordering, 86
system, 348, 502, 507
explanatory variable, 192
exponential family, 15, 82, 115, 116,
120–122, 146, 149, 174, 481
admissible estimator, 411
and monotone likelihood ratio,
244
curved, 119
extension, 375
location and, 147
minimal, 117
natural, 146, 257
form, 116
parameter, 481, 482
restricted, 148
steep, 147
pseudo-, 146
quadratic variance, 149, 293, 497
natural, 322

592
Subject Index
quasi, 116
regular, 117, 147
variance of, 148
family
exponential, see exponential
family
location, 138
Federalist Papers, 185
Fieller’s problem, 43
ﬁltering, 200
ﬁnance, 507
Fisher’s F-distribution, 32
ﬁt
best, 347
goodness-of, 347, 374
problem, 375
vs. estimation error, 347
forecaster, 64, 87, 230
form
fully exponential, 298
standard, 298
forward–backward formula, 491
fractiles, 142
frequentist
approach, 61
conditional, 250
coverage, 137
decision framework, 61, 65
long-term properties, 140
method, 513
notion of optimality, 391
paradigm, 61
properties, 137
risk, 61
test, 66, 242
validity, 179, 254, 420
function
analytic, 73
conﬂuent hypergeometric, 150,
154, 187, 297, 478, 505
convex utility, 59
cumulant generating, 118
Gamma, 122
invariantly estimable, 454
likelihood, see likelihood, 509
link, 359, 463
loss, see loss
modiﬁed Bessel, 154, 208, 209,
287, 379, 408, 454
moment generating, 300, 324
quadratic variance, 148
utility, 55, 57, 59
functional basis, 115
galaxy dataset, 345
gamble, 156
desirable, 156
Γ-minimax
regret, 144
risk, 144
generalized inverse, 385
generator
inﬁnitesimal, 458
pseudo-random, 294, 295, 334
geometric average, 374, 383
Gibbs random ﬁeld, 307
Gibbs sampling, 307–317
algorithm, 313
Dirichlet process, 340
for mixture, 320
one-at-a-time, 313
G-prior, 191–193, 378
graph, 503
directed, 503
acyclic, 490, 503
undirected, 503
gravitation, 348
group
action, 128
amenable, 442, 444
structure, 110
transitive, 433
Haar measure, 28, 428, 441, 444
ﬁniteness, 438
left, 437
right, 436, 438, 439, 443
right vs. left, 440
harmonic mean, 357, 358
Harris recurrence, 302
heavy tail, 127
heterocedasticity, 218
hidden Markov model, 47, 318, 320,
332, 459
hierarchical
modeling, 143
prior, see Bayes hierarchical
highest posterior density (HPD)
region, 25

Subject Index
593
histogram, 109, 111, 142
HIV, 459
hot hand, 228, 229
hyperbolic secant, 149
hyperparameter, 113, 118, 143, 189,
193, 460
conjugate, 205
estimate, 481
poly-t, 221
hyperprior, 113, 143, 464
hypothesis
alternative, 225
contiguous, 243
one-sided, 254
identiﬁability, 24, 198, 201, 517
image, 64, 96
black and white, 96
processing, 507
radiological, 4
satellite, 96
importance function, 323
choice of, 295, 297, 357
importance sampling, 296, 307, 319,
356, 359, 380
and inﬁnite variance, 297
defensive, 357
for model choice, 357
improper prior, 26–31, 128, 130, 339
and credible regions, 261
and point-null hypothesis, 223,
233
and test, 232, 234, 238, 372
and training sample, 236
as regular prior, 29
inadmissibility, 98
of the m.l.e., 97
p-value, 259
incoherence, 108, 131
inconsistency
of Bayes estimators, 48, 134
of the Jeﬀreys prior, 133
indeterminacy, 141
indiﬀerent alternatives, 55
inequality
Cauchy–Schwarz, 417
Cram´er–Rao, 394
Jensen’s, see Jensen’s
Van Trees, 171
inference, 1, 2, 7
causal, 348
conditional, 513
quantitative, 509
inﬁnite divisibility, 120
information, 15, 25, 230
Fisher, 129, 130, 134, 138, 140,
154
limited, 106, 111, 115
missing, 136
prior, 27, 31
sample-equivalent of, 107
structural, 463
summary of, 165
testing problem, 250
vague, 112, 170
value of sample, 89
integral
approximation, 286, 293, 294
Laplace expansion, see
approximation, Laplace
left-invariant, 437
ratio, 298
right-invariant, 438
interface
Bayesian–frequentist, 99, 251,
282
interpretation vs. explanation, 1, 508
invariance, 21, 53, 81, 114, 427
group, 431
Haar measure, 131
reparameterization, 110, 128,
130, 136
structure, 129
translation, 129, 429
Inverse Probability, 8
inversion, 8, 9, 20, 23, 509
invertibility, 198, 215
irreducibility, 302, 304
Jeﬀreys’
approach, 517
noninformative prior, 132
prior, see noninformative prior,
Jeﬀreys
and likelihood principle, 132
scale, 228
Jensen’s inequality, 70
Kalman ﬁlter, 199
Kepler’s problem, 248

594
Subject Index
Lagrange multipliers, 110
Laplace
approximation, see
approximation, Laplace
expansion, 298, 352
rule, 128
succession rule, 180
transform, 175
latent variable, 292
Law of Large Numbers, 61
least favorable answer, 251, 252
lemma
Jensen, 70
Neyman–Pearson, 243
Pitman–Koopman, 116, 146, 147,
244
Schur, 195, 197
Stein’s, 94
likelihood, 8, 15, 19, 159
explicit, 199
observed, 219
principle, see principle, likelihood
proﬁle, 168
ratio, 227, 245, 353
recursive deﬁnition, 199
limiting argument, 30
linear calibration, see calibration
linearity, 122
linearization, 56
lip cancer, 355
lizard, 107
London Underground, 385
loss, 52, 53, 60, 61, 511
absolute error, 48, 79, 92, 256
approximated posterior, 298
asymmetric squared error, 93
bidimensional, 242
bounded, 57, 58, 418
classical, 52, 53, 77, 173
convex, 64, 70, 74, 77, 80, 91
strictly, 76, 256
derivation of a prior distribution
from, 136
entropy, 82, 92, 433, 485, 486,
502, 505
estimation, 79, 178, 179
for imaging, 96–97
for mixture estimation, 339
for set estimation, 265, 266
global, 267
Hellinger, 82, 92, 155
information, 247
intrinsic, 21, 53, 81, 92
invariant, 432
linear, 265, 266
LINEX, 92
misclassiﬁcation, 96
multiple, 85
opportunity, 88
parameterization-free, 82
prediction, 172
proper, 257, 259
p-value, 250
quadratic, 52, 77, 78, 97, 100, 178
random, 83
rational, 266, 277
requirement of, 512
robustness, 98
scale-invariant, 177
0 −1, 80, 96, 225, 242, 245, 256
male births, 128
Maple, 229
marginalization, 165
paradox, 29, 128, 152
Markov chain, 194, 195, 197, 203, 301,
328
aperiodic, 304
continuous time, 458
ergodic, 309
generation of, 303
hidden, 196, 203
irreducible, 327
Monte Carlo, see MCMC
ϕ-mixing, 327
transient, 337
Mathematica, 229
maximal loss of explanatory power,
371
maximum likelihood, 481
estimation, 20
method, 512
nonparametric estimate, 480
MCMC, 301, 302, 315, 318
for dynamic model, 337
for hierarchical model, 468
for mixtures, 318–320
for nonparametric model, 376
for variable dimension model,
350, 361

Subject Index
595
generation and storage, 367
reversible jump, 364
revolution, v
mean
and Hunt–Stein theorem, 443
functional, 443
largest, 216
posterior, 78
right-invariant, 444
measure
Haar, see Haar measure
invariant, 437
Lebesgue, 12, 27
Radon, 437, 438
reference, 110
right Haar, 110, 137
measurement error, 459
meta-analysis, 25, 458, 463
meta-model, 109, 374
meteorological forecast, 64
method
ML-II, 109
moment, 109, 111
Monte Carlo, 294
with importance function, 295
particle ﬁlter, 298
universal, 513
minimax
analysis, 69
Bayes rule, 73
estimator, 67, 69
randomized, 70
strategy, 69
minimaxity, 66, 83, 137, 478
and admissibility, 74, 415
necessary and suﬃcient condition
of, 477
of hierarchical Bayes estimator,
472, 474
misclassiﬁcation, 96
rate, 96
missing data, 149, 292, 312
representation, 319
misspeciﬁcation, 27
mixed experiment, 18
mixture, 30, 39, 47, 55, 119, 146, 152,
204, 252, 272, 291, 338, 371,
464, 508
beta, 151, 375
continuous, 127
exponential, 331, 365
geometric, 24
Gibbs sampling for, 320
hidden, 119, 120, 174, 316, 335
non-identiﬁability, 377
normal, 5, 21, 40, 241, 290, 318,
345, 379
of Dirac measures, 126
scale, 272
Student’s t, 221
uniform, 156
model
AR(1), 172, 194, 195, 217, 419
AR(p), 195, 196, 199, 200, 338
ARCH(p), 216, 218
ARIMA, 215
ARMA(p, q), 201
autoregressive spatial, 356
averaging, 318, 349, 366
capture-recapture, see
capture-recapture
causal, 215
censored, 332
choice, 15, 224, 230
construction, 6
Darroch’s, 184, 208
decomposable, 503
discrete, 180
dynamic, 193, 194
embedding, 344, 349, 375
encompassing, 372, 374
exchangeable, 464
exploration of, 372
factor, 216
full, 370, 372
projection of, 369
GARCH, 218
generalized linear, 359, 369, 388
graphical, 318, 467, 502
hierarchical, 44, 109, 311, 314,
328, 335, 459
and full conditional
distribution, 467
and robustness, 465
exchangeable, 471
latent variable, 318, 425, 461
linear, 463
additive, 462
generalized, 463

596
Subject Index
random-eﬀect, see
random-eﬀect
linear calibration, see calibration
logistic, 2
MA(q), 198, 200
mixture, see mixture
multinomial, 384
nested, 280
nonparametric, 338
normal, 186–193
for hierarchical prior, 470
probit, 149, 330, 492
qualitative, 122
saturating, 362, 379
statistical, 107
invariant, 432
stochastic volatility, 219, 318, 461
switching AR(p), 195
temporal, 208
Tobit, 212
true, 107
variable dimension, 361
Wolter’s, 184, 208
model choice, 318, 343
and estimation, 343
and testing, 343
parameter space, 348
penalty terms for, 350
modulus, 438, 451
monetary rewards, 57
monitoring technique, 303
Monte Carlo
method, 301
resolution, 127
simulation, 169
moral expectation, 59
move
birth and death, 365, 386
merge, 365
split, 365
moving average (MA), 198
multicollinearity, 193, 473, 474, 494
and minimaxity, 474
multiplier, 438, 449
neural network, 47, 507
Neyman–Scott problem, 48, 133
no-decision
alternative, 282
answer, 282
node, 503
no model is true, 223, 346
nonidentiﬁability, 24
noninformative answer
p-value as, 255
for tests, 230
noninformative modeling, 27, 510
hierarchical Bayes, 464
noninformative prior, 20, 45, 46, 114,
127–141
and conjugate prior, 114
and reparameterization, 128
as limit of conjugate, 189
Haldane, 269
invariance structure, 427
Jeﬀreys, 106, 129, 130, 132, 136
Lebesgue measure as, 77, 129,
234, 281, 437, 454
tail approximation, 49
nonrepeatability of experiments, 508,
509
nontransitivity, 102
normalizing
constant, 122, 221, 349, 357, 361
ratios of, 358
term, 124
notation, 6, 527–530
null hypothesis, 224, 226
point, 223, 224, 230, 271
number of components, 340, 341, 346
unknown, 345
numerical
approximation, 292
computation, 178
integration, 287, 293, 298
and Monte Carlo methods, 298
minimization, 178, 285
objectivity, 123
observation, 6, 7
imaginary, 236
space, 60
virtual, see virtual, observation
Occam’s
razor, 368
window, 368
odds ratio, 128
oil drilling, 67
one-/two-sided dichotomy, 256
optimal

Subject Index
597
decision, see decision, optimal
procedure, 52
reward, 55
optimality
asymptotic, 136
classical, 512
orbit, 433
order
connected, 55
of consequences, 54
of rewards, 54
P, 55
relation, 158
ordering, 86, 102, 136
coarse, 159
partial, 74, 83
preference, 158
social, 58
total, 61, 66
orthogonal
base, 293
polynomials, 149
regressor, 368
outlier detection, 338
paradigm, 8
paradox, 136
ancillarity, 212
Borel, 268
Condorcet, 58
information, 292, 318
Jeﬀreys–Lindley, 30, 234, 273,
280, 281, 351
likelihood, 512
marginalization, 29, 128, 135,
152, 211, 370, 454, 455
projection, 370
Saint Petersburg, 52, 59
Simpson, 33, 58
Stein, 94, 167
parameter
bounded, see bounded
location, 27, 129, 429
natural, 82, 128
noncentrality, 209, 411, 449, 498
nuisance, 133, 136, 138, 162, 168,
411, 517
of interest, 133
order-restricted, 21
random, 10
scale, 129
space, 60
parameterization, 139, 370
choice, 53, 81
inﬂuence, 53, 82
linear, 56
mean, 160
natural, 81
parsimony, 201, 217, 347, 366
partial autocorrelation, 197, 214, 338
inverse, 200
partial autocorrelations, 195
partition of the sample, 291, 292
partitioning, 128
penalization factor, 354
performance, 52, 61, 65, 70, 74
frequentist, 99
Pillow Problems, 207
Pitman
admissibility, 95
closeness, 95, 101
domination, 94
pivotal quantity, 137
pixel, 4, 96
polar coordinate, 135
polynomial
Hermite, 293, 322
Legendre, 376
quadrature, 293
population
ﬁnite, 180
rare, 181
size estimation, 182
positivity constraint, 314
postdata evaluation, 224
posterior, 16, 22, 23
conﬁdence set, 137
derivation, 165
expected loss, 62
improper, 336, 337
information, 136
marginal, 220
mean, 78
median, 12, 80, 179
probabilities, 137
range of, 251
probability, 226
p-value, 282
proper, 28
propriety, 318

598
Subject Index
pseudo-, 240, 481
squared error, 170
power
computational, 301
distribution, 317
of a test, 242
of continuum, 32
precision, 170
prediction, 7, 171, 199
linear regression, 212
predictive density, 340, 341
prevision
coherent, 157
conjugate upper, 157
upper and lower, 156
principle
conditionality, 18, 19
duality, 309, 328
invariance, 427
formal, 428
likelihood, 15, 19, 65, 140, 191,
202, 235, 250, 282, 446, 512
and Decision Theory, 80
and Jeﬀreys’ prior, 132
Bayesian version of, 165
implementation, 20, 31, 167,
168
justiﬁcation, 18
minimax, 68
of insuﬃcient reason, 106, 127
parsimony, 338, 368
separated zeros, 73
stopping rule, 17, 247
suﬃciency, 15, 18
prior, 9
arbitrariness in, 105, 113
as a tool, 510
inferential, 512
axiomatic foundations of, 159
bias, 73
choice of, 13, 105, 144, 367, 515
closed under sampling, 114
conjugate, see conjugate prior
default, 127
derivation of, 108
determination of, 105, 106
automated, 106
exact, 141
parametrized, 111
subjective, 106, 108, 109
Dirichlet, 340, 375, 504
duality between loss and, 136,
228
existence of, 158, 159
for testing, 140
Haldane, 453
hierarchical, see Bayes
hierarchical
improper, see improper prior
inﬂuence of, 106
intrinsic, 238, 240, 270
intuition, 511
Jeﬀreys, 133, 139, 140, 155, 217,
233, 235
alternative to, 446
controversial, 197
matching, 137, 138, 211
maximum entropy, 112, 143, 145
mixture, 123
modeling, 124
eﬀect of, 318
modiﬁcation of, 229, 255
noninformative, see
noninformative prior
objective, 114
parametrized, 105, 106, 111
P´olya tree, 47
poly-t, 193, 220, 221
probability of a model, 367
pseudo, 362
reference, 127, 133, 136, 165, 213,
216
relatively invariant, 449
robust, 143
selection, 335
spherically symmetric, 408
uncertainty about the, 141
uniform, 127, 128, 136
unknown, 479
vague proper, 28
prior class
determined moment, 141
ϵ-contamination, 142
neighborhood, 142
ratio of densities, 142
underspeciﬁed, 142
prior distribution, 10
prior feedback, 151
prior information, 31, 105, 127, 141,
144, 193, 290

Subject Index
599
diﬀuse, 232
insuﬃcient, 457
subjective justiﬁcations, 131
to prior distribution, 514
probabilistic
interpretation, 2
modeling, 3, 8, 10, 11
probability
axioms, 45
imprecise, 457
theory, 12
probabilization, 508
problem
ill-posed, 374
NP-hard, 292
process
beta, 47
Dirichlet, 338
jump, see birth and death, jump
process
L´evy, 47
logical, 511
non-stationary, 195
stationary, 198
stochastic, 172
subjective input of the
inferential, 516
program
computer, 294
general optimization, 514
projection, 438, 471, 500
Kullback–Leibler, 389
properness, 30
proposal, 304
choice of, 304, 307
independent, 307
random-walk, 305, 307, 320
pseudo-prior, 362
p-value, 249, 250, 256, 405
admissible, 406
conservative behavior of, 255
quantiles, 142
R, 335, 523
radiographs, 4
random generator, see generator
pseudo-random
random walk, 195, 398
random-eﬀect, 460, 461, 465, 508
and hierarchical Bayes analysis,
463
Jeﬀreys prior, 337
modeling, 191
posterior propriety for, 41
randomization, 139, 244, 247
and discrete distributions, 261
ranking, 216
Rao–Blackwellization, 309, 310, 315,
328, 361
rationality, 58
of decision-makers, 58
realization, 6
reconciliation, 254, 282, 517, 518
recurrence, 398, 400
of a chain, 400
reduction, 7, 63, 107
and modeling, 223
reference prior, 133, 134, 136
as matching prior, 140
construction, 134
for the calibration model, 42
reverse order, 140
reﬂection coeﬃcient, 197
region
equal tail credible, 262
HPD, 137–139, 264, 266, 501
α-credible, 260
nonconnected, 262
uniformly most accurate, 263
regression
and calibration, 42
inverse, 212
isotonic, 35
linear, 122
logistic, 122, 149, 204, 224, 325,
330, 370, 491
model, 190, 192
nonparametric, 5
normal, 382
Poisson, 4
rejection of a null hypothesis, 247
relevant subset
negatively biased, 281
positively biased, 281
reparameterization, 119, 128, 197,
200, 201
invariance under, 92, 132
natural, 146
nonparametric, 375

600
Subject Index
repeatability of experiments, 61, 108,
159, 515
repeated testing, 282
representation
forward–backward, 332
hidden mixture, 336
lag polynomial, 214, 215
Markovian, 398
non-parametric, 160
orthogonal polynomial, 375
state-space, 199, 200, 338
ARMA(p,q), 201
stationary, 217
reversible jump, 367
reward, 54, 55
RIC, 382
risk
aversion, 59, 60
constant, 72, 74
continuous, 401
frequentist, 61
integrated, 62
lovers, 59, 77
maximin, 70
minimax, 67
set, 409
unbiased estimator of, 94, 99,
475, 476
vectors, 71
robustness, 83, 112, 115, 474, 478
and hierarchical analysis, 515
loss function, 144
rumor, 268
sampling
bridge, 358–359
hybrid, 315
path, 359, 386
umbrella, 381
saturation technique, 363
scale transformation, 433
scaling, 142
coherence, 108
Schwartz’s criterion, 353
scoring rule, 87
proper, 88
Scotland, 355
selection, 216
semi-tail upper bounds (STUB), 412
separator, 503
sequence
ﬁnitely exchangeable, 159
inﬁnitely exchangeable, 159
sequential sampling, 237
set estimation, 265, 266
as point estimation, 266
as testing, 266
Shakespeare’s vocabulary, 186
shrinkage, 98
estimator, see estimator
signal processing, vi, 109, 197, 332,
507
signiﬁcance level, 243, 244, 247, 249
usual, 254
Simpson’s method, 293
simulation, 286, 294
accept–reject, 322
alternative to, 298
from conditional distributions,
308, 315
iterative, 317
output, 48
simultaneous equations, 143
slice sampler, 360
slice sampling, 315–317, 320, 329
smoothing, 199
software, 293
space
natural parameter, 117
restricted parameter, 168
state, 200
varying dimension, 344
S-Plus, 335
state-space representation, see
representation, state-space
stationarity, 194, 196, 201, 302
constraint, 195
statistic
ancillary, 33, 35, 279, 282, 283,
434
complete, 33, 35
likelihood ratio, 162
linguistic, 185
maximal invariant, 434
minimal suﬃcient, 14
order, 33, 119
suﬃcient, 14, 116, 187
Statistics
conditional perspective in, 13
Fiducial, 509

Subject Index
601
ﬁducial, 45
Mathematical, 46
Stein admissibility condition, 132, 394
Stein eﬀect, 14, 84, 94, 98, 175, 247,
264, 395, 441, 445, 446, 460,
483, 484, 486, 515
and ﬁnite parameter space, 99
frequentist analysis of, 98
robust, 99
stochastic
complexity, 140
domination, 84, 102
stock
market, 194
price, 196
stopping rule, 17
Student’s t–distribution, see
distribution
subjective input, 113
substitution sampling, 314
superharmonicity, 476, 477, 499
table entry problem, 155
tails, 109
test, 224
chi-squared, 248, 375
Neyman–Pearson, 17
sequential probability ratio, 282
t-, 281
two-sided, 235
UMPU minimax, 279
unbiased, 245
uniformly most powerful (UMP),
242
uniformly most powerful
unbiased (UMPU), 245
testing, 30
as an estimation problem, 223
theorem
Basu, 33
Central Limit, 3, 186, 248
ergodic, 302
factorization, 14
Fubini, 39, 63
Hammersley–Cliﬀord, 37, 327
Hunt–Stein, 73, 442, 444, 445,
452
Kakutani, 445
Markov, 445
Rao–Blackwell, 14, 15, 70, 74, 89,
310, 442
Riesz representation, 410
separating hyperplane, 409
two projection, 389
theory
Decision, 7, 51, 58, 102, 223
Bayesian, 52, 173
foundation of, 266
frequentist, 65
information, 140
Neyman–Pearson, 81, 225, 242,
256, 263
of knowledge, 510
testing, 223
utility, 54
the problem has a value, 71
time series, 193
total
ignorance, 127
ordering, 54, 58, 61, 63
tractability, 115
training sample, 236
for mixture, 352
minimal, 236
tramcar, 181
transition kernel, 304
transitivity, 54, 58, 158
tree
decision, 60
of possible models, 347
orange, 346
ordering, 36
pine, 362
pruning, 372
truncation set, 257
unbiasedness, 14, 99
unit circle, 195, 197
universal domination, 84
utility, 54
derivation of, 159
existence of, 58
value, 73
variable selection, 345, 366, 369
downward, 372
upward, 372
variance
estimating a normal, 264

602
Subject Index
function, 148
heterogeneous, 218
underestimation of, 487
unknown, 191
vertice, 503
virtual
observation, 115, 142, 160, 189,
280
sample, 107
volatility, 219
waiting queue, 67
wavelet, 293, 375
basis, 47
Haar, 47
The Wheel of Time, iii, v, 1, 51, 105,
165, 223, 285, 343, 391, 427,
457, 507
white noise, 198
Wold decomposition, 198, 214


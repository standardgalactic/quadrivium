Calculating	  information	  in	  spike	  trains	  
Two	  methods:	  
	  
•  Information	  in	  spike	  patterns	  
•  Information	  in	  single	  spikes	  
	  

Grandma’s famous mutual information recipe	  
	  
Take one stimulus s and repeat many times to obtain 
P(R|s).	  	  
Compute variability due to noise: noise entropy	  H[R|s]	  
Repeat for all s and average: Σs	  P(s)	  H[R|s)].	  
Compute	  P(R)	  =	  Σs	  P(s)	  P(R|s)	  and the total entropy H[R]	  
Mutual	  information	  is	  the	  difference	  between	  	  
the	  total	  response	  entropy	  and	  	  the	  mean	  noise	  entropy:	  	  
	  
	  	  	  	   	  
	  	  	  	  	  	  	  	  	  	  	  I(S;R)	  =	  H[R]	  –	  Σs	  P(s)	  H[R|s)]	  .	  
	  
Calculating	  mutual	  information	  

Strong	  et	  al.,	  1997	  
Calculating	  information	  in	  spike	  patterns	  
So	  far	  only	  dealt	  with	  
single	  spikes,	  or	  Miring	  
rates.	  	  
	  
What	  information	  
is	  carried	  by	  patterns	  of	  
spikes?	  	  
	  
Analyze	  patterns	  of	  the	  code:	  	  
how	  informative	  are	  they?	  

Entropy:	  
•  Binary	  words	  w	  with	  	  
	  letter	  size	  Δt,	  length	  T.	  
•  Compute	  	  p(wi)	  
Calculating	  information	  in	  spike	  trains	  
H[w]	  =	  -­‐	  Σ	  p(wi)	  log2	  p(wi)	  	  
Strong	  et	  al.,	  1997;	  Reinagel	  and	  Reid,	  2000	  	

Information	  :	  	  	  
difference	  between	  the	  total	  	  
variability	  	  driven	  by	  stimuli	  	  
and	  that	  due	  to	  	  noise,	  averaged	  	  
over	  stimuli.	  
	  
Reinagel and Reid, ‘00 
Calculating	  information	  in	  spike	  trains	  

Take	  a	  stimulus	  sequence	  and	  	  
repeat	  many	  	  times.	  
	  
How	  to	  sample	  P(s)?	  
Average	  over	  s	  à	  average	  over	  time:	  
	  
For	  each	  time	  in	  the	  repeated	  	  
stimulus,	  get	  a	  set	  of	  words	  	  
P(w|s(t)).	  
	  
	  Hnoise	  =	  	  <	  H[P(w|si)]	  >i.	  
	  
Choose	  length	  of	  repeated	  sequence	  	  
long	  	  enough	  to	  sample	  the	  noise	  	  
entropy	  	  adequately.	  	  	    
Reinagel	  and	  Reid	  (2000)	  
Apply	  grandma’s	  recipe	  

Calculating	  information	  in	  the	  LGN	  
Reinagel	  and	  Reid	  (2000)	  

Learning	  about	  the	  LGN’s	  code	  
Reinagel	  and	  Reid	  (2000)	  

Sampling	  and	  bias	  
•  Never	  enough	  data!	  
•  Corrections	  for	  Minite	  sample	  size	  
•  Panzeri,	  Nemenman,	  ..	  

Information	  in	  single	  spikes	  
By	  how	  much	  does	  knowing	  that	  a	  particular	  stimulus	  occurred	  	  
reduce	  the	  entropy	  of	  the	  response?	  
Brenner	  et	  al.	  (2000),	  data	  Reinagel	  and	  Reid	  (2000)	  

Information	  in	  single	  spikes	  

Now	  compute	  the	  entropy	  difference: 
Every	  time	  t	  stands	  in	  for	  a	  sample	  of	  s	  
	  
A	  time	  average	  is	  equivalent	  to	  averaging	  	  
over	  the	  s	  ensemble.	  
	  
	  
	  
	  Ergodicity	  
ß	  Total	  	  
ß	  Noise	  
Information	  in	  single	  spikes	  

, 
Assuming	  	  	  	  	  	  	  	  	  	  	  	  	  	  ,	  	  
and	  using	  
To	  get	  information	  per	  spike,	  divide	  by	  	  	  	  	  	  	  	  :	  	  
Information	  in	  single	  spikes	  
ß	  Total	  
ß	  Noise	  

Information	  per	  spike:	  
•  No	  explicit	  stimulus	  dependence	  (no	  coding/decoding	  model)	  
•  The	  rate	  r	  does	  not	  have	  to	  mean	  rate	  of	  spikes;	  rate	  of	  any	  event.	  
Information	  in	  single	  spikes	  
What	  limits	  information?	  	  
Ø  spike	  precision,	  which	  blurs	  r(t)	  
Ø  the	  mean	  spike	  rate.	  	  

Information	  per	  spike:	  
Information	  in	  single	  spikes	  

Information	  per	  spike:	  
Information	  in	  single	  spikes	  

Information	  per	  spike:	  
Information	  in	  single	  spikes	  

Next	  up:	  information	  and	  coding	  efMiciency	  
•  What	  are	  the	  challenges	  posed	  by	  natural	  stimuli?	  
•  What	  do	  information	  theoretic	  concepts	  suggest	  that	  	  
	  
	  neural	  systems	  should	  do?	  
•  What	  principles	  seem	  to	  be	  at	  work	  in	  shaping	  the	  neural	  code?	  


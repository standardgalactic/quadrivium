Privacy  
Computing
Fenghua Li
Hui Li
Ben Niu
Theory and Technology

Privacy Computing

Fenghua Li • Hui Li • Ben Niu 
Privacy Computing 
Theory and Technology

Fenghua Li 
Institute of Information Engineering, 
Chinese Academy of Sciences 
Beijing, China 
Hui Li 
School of Cyber Engineering 
Xidian University 
Xian, Shaanxi, China 
Ben Niu 
Institute of Information Engineering, 
Chinese Academy of Sciences 
Beijing, China 
ISBN 978-981-99-4942-7
ISBN 978-981-99-4943-4 
(eBook) 
https://doi.org/10.1007/978-981-99-4943-4 
© Posts & Telecom Press 2024 
Jointly published with Posts & Telecom Press, Beijing, China 
The print edition is not for sale in China (Mainland). Customers from China (Mainland) please order the 
print book from: Posts & Telecom Press. ISBN: 9787115563965 
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether 
the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of 
illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and 
transmission or information storage and retrieval, electronic adaptation, computer software, or by 
similar or dissimilar methodology now known or hereafter developed. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication 
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant 
protective laws and regulations and therefore free for general use. 
The publishers, the authors, and the editors are safe to assume that the advice and information in this 
book are believed to be true and accurate at the date of publication. Neither the publishers nor the 
authors or the editors give a warranty, expressed or implied, with respect to the material contained 
herein or for any errors or omissions that may have been made. The publishers remain neutral with 
regard to jurisdictional claims in published maps and institutional afﬁliations. 
This Springer imprint is published by the registered company Springer Nature Singapore Pte Ltd. 
The registered company address is: 152 Beach Road, #21-01/04 Gateway East, Singapore 189721, 
Singapore 
Paper in this product is recyclable.

Preface 
The rapid development of information technology, the emergence of new business 
formats, and the continuous evolution of personalized services have prompted large 
internet companies to accumulate massive user data during the process of serving 
users. These data contain a signiﬁcant amount of personal sensitive information. The 
frequent cross-border, cross-system, and cross-ecosystem interaction of user data has 
become the norm, which increases the intentional or unintentional retention of 
privacy information in different information systems. This leads to increasingly 
serious issues such as the privacy preservation gap and the difﬁculties in tracing 
privacy infringements, which pose great challenges to personal information sharing, 
ﬂow, management and control, privacy information protection, etc. 
Traditional privacy-preserving technologies mainly focus on relatively isolated 
application scenarios and technical points, addressing speciﬁc issues in speciﬁc 
scenarios, lacking a comprehensive descriptive method and computational model 
that can integrate privacy information and preservation requirements, and lacking 
computing architectures that can support the requirements of on-demand privacy 
preservation during cross-system privacy information exchange, privacy informa-
tion ubiquitous sharing, and full life-circle privacy information control. As a result, 
the privacy preservation issues in typical data sharing application scenarios such as 
e-commerce and social networks have not yet been fundamentally addressed. 
Therefore, in 2015, Professor Fenghua Li, Professor Hui Li, et al. proposed the 
theory and key technologies of privacy computing for the ﬁrst time. In 2016, their 
work was formally published in the Journal of Communications in Chinese. Privacy 
computing is a computational theory and methodology aiming at preserving the 
entire lifecycle of privacy information. It is a computable model and axiomatic 
system for privacy metrics, privacy leakage cost, privacy preservation, and privacy 
analysis complexity when ownership, management, and use of privacy information 
are separated. Speciﬁcally, it refers to the operations of describing, measuring, 
evaluating, and integrating the privacy information when processing various types 
of data such as video, audio, image, graph, text, numerical data, and ubiquitous 
network behavioral information. These operations form a set of symbolic,
v

formalized, and quantitatively evaluated privacy computing theories, algorithms, 
and application technologies. They support privacy preservation across multiple 
systems by providing quantitative evaluation standards. Privacy computing covers 
all computing operations of information collectors, publishers, and users in the 
whole lifecycle process of information generation, perception, publication, dissem-
ination, storage, processing, use, destruction, etc. It also includes system design 
theories and architectures that support efﬁcient and high-performance privacy pres-
ervation for massive users and high concurrency. Privacy computing is an important 
theoretical foundation for the preservation of privacy information in the ubiquitous 
network space. 
vi
Preface
This book addresses the systematic privacy preservation needs in the ubiquitous 
interconnected environment, based on a series of papers and patents accumulated by 
the authors through years of dedicated work in this ﬁeld. It provides a concise and 
systematic introduction to the theory and key technologies of privacy computing. 
The main contents include privacy computing framework, formal deﬁnition of 
privacy computing, important characteristics of privacy computing, algorithm 
design guidelines, privacy preservation effectiveness evaluation, privacy computing 
languages, and an outlook on future research directions and unresolved issues in 
privacy computing. The publication of this book aims to comprehensively expound 
the academic connotation of privacy computing and promote its extensive research. 
In the process of painstaking academic exploration, there is a certain degree of 
serendipity and inevitability in obtaining original and innovative scientiﬁc research 
results. Serendipity refers to the chance of success when a scholar's choice of a 
research area and the process of conducting academic research are inﬂuenced by the 
subjective and objective environment. Necessity means that scholars who are able to 
achieve original innovation must have an unwavering philosophy and attitude 
towards the pursuit of original innovation and be able to persevere and explore for 
a long time. Privacy computing is proposed in the context of forward-looking 
application requirements; however, a new theory often requires a long time to be 
established and recognized by various sectors of society, overcoming various difﬁ-
culties and constantly iterating and evolving, gradually developing and improving. 
Privacy computing still requires a lot of theoretical and technical exploration and 
research. The authors strongly believe that it is necessary to publish books to guide 
and promote the theoretical research and application of privacy computing. The 
authors hope to actively promote the continuous research of privacy computing with 
my readers. 
Beijing, China
Fenghua Li 
June 2023

Acknowledgments 
This book is primarily authored by Professor Fenghua Li, Professor Hui Li, and 
Professor Ben Niu. It is a highly condensed summary of the research achievements 
of the authors in privacy computing over many years. Chapter 1 is primarily 
completed by Fenghua Li and Hui Li. Chapter 2 is primarily completed by Ben 
Niu, Hui Zhu, and Hui Li. Chapter 3 is primarily completed by Fenghua Li, Hui Li, 
and Ben Niu. Chapter 4 is primarily completed by Hui Li, Ben Niu, Fenghua Li, and 
Hui Zhu. Chapter 5 is primarily completed by Fenghua Li, Hui Li, and Ben Niu. This 
book also includes contributions from the coauthors of the referenced papers and 
patents. During the writing process, we received valuable assistance from Dr. Zhe 
Sun, Dr. Yuanyuan He, Dr. Xinyu Wang, Dr. Wenjing Zhang, Dr. Hanyi Wang, Dr. 
Peijie Yin, Dr. Xiaoguang Li, as well as doctoral candidates including Haiyang Luo 
and Yahong Chen. We also appreciate the assistance provided by master’s students 
such as Zhidong Yang and Kun He, as well as graduate students including Wenqing 
Bi and Likun Zhang. We express our sincere gratitude to all of them for their 
contributions. We would like to express our sincere gratitude to People’s Posts 
and Telecommunications Press for their strong support. We also extend our thanks 
to all the individuals involved who have worked diligently to make this book 
publication possible! 
The publication of this book has been supported by the National Key R&D 
Program 
of 
China 
(No. 
2021YFB3100300, 
No. 
2021YFB3101300, 
No. 
2017YFB0802200), the National Natural Science Foundation of China (No. 
61932015, No. 61872441, No. 61672515, No. 61502489, No. U1401251), and the 
Youth Innovation Promotion Association, CAS (No. 2018196).
vii

Contents 
1 
Introduction .  . . .  . . . .  . . .  . . . .  . . .  . . . .  . . .  . . . .  . . .  . . . .  . . .  . . . .
1  
Fenghua Li, Hui Li, and Ben Niu 
1.1 
User Data, Personal Information, and Privacy Information . . . . . .
2  
1.1.1 
User Data . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . .
2  
1.1.2 
Personal Information . . . . .  . . . . .  . . . . .  . . . . .  . . . . .  .
2  
1.1.3 
Privacy Information . . . . . . . . . . . . . . . . . . . . . . . . . . .
4 
1.2 
Privacy Protection and Privacy Desensitization . . . .  . . . . .  . . . . .
4  
1.2.1 
Privacy Protection . . . . . . . .  . . . . .  . . . . .  . . . . .  . . . . .
5  
1.2.2 
Privacy Desensitization . . . . . . . . . . . .  . . . . . . . . . .  . .
7  
1.3 
The “Four Rights” of Privacy Preservation . . . . . . .  . . . . .  . . . . .
9  
1.3.1 
Related Parties of Privacy Information . . . . . . . . . . . . . .
9 
1.3.2 
Right to Know . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9 
1.3.3 
Right to Erasure . . . . . . .  . . .  . . .  . . . .  . . .  . . .  . . . .  . . .  
10  
1.3.4 
Right to be Forgotten . . . . . . . . . . . . . . . . . . . . . . . . . . 
10 
1.3.5 
Extended Authorization . . . . . . . .  . . . . . . . .  . . . . . . .  .  
10  
1.4 
Technical Challenges in Privacy Preservation . . . .  . . .  . . . .  . . .  .  
11  
1.4.1 
Threats in Privacy Preservation . . . . . . . . .  . . .  . . .  . . . .  
11  
1.4.2 
Systematic Computational Model . . . . .  . . . . . .  . . . . . .  
12  
1.4.3 
Evaluation of Privacy-Preserving Effectiveness . . . . . . . 
13 
1.4.4 
Extended Control of Privacy Information . . . .  . . . . . . .  .  
14  
1.4.5 
Forensics of Privacy Infringement . . . . . . . . . . . . . . . . . 
14 
1.5 
Chapter Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
15 
References . . . . .  . . . . . .  . . . . .  . . . . . .  . . . . .  . . . . .  . . . . . .  . . . . .  . .  
15  
2 
Advances in Privacy Preservation Technologies .  . . .  . .  . . .  . . .  . . .  .  
17  
Fenghua Li, Hui Li, and Ben Niu 
2.1 
Privacy Protection Technology . . . . . . . . . . . . . . . . . . . . . . . . . 
17 
2.1.1 
Homomorphic Encryption . . . . . .  . . . . .  . . . . .  . . . . . .  .  
18  
2.1.2 
Secure Multiparty Computation . . . . . . . . . . . . . . . . .  . .  
18  
2.1.3 
Access Control . . . . . . . . .  . . . . . . . . . . . . . . . . . .  . . .  
18  
2.1.4 
Trusted Computing . . . . . .  . . . . . .  . . . . .  . . . . .  . . . . .  
20
ix

x
Contents
2.1.5 
Federated Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 
21 
2.2 
Privacy Desensitization Technology . . . .  . . .  . . . .  . . .  . . . .  . . .  .  
22  
2.2.1 
Anonymity-Based Privacy Desensitization . . . . . . . . .  . .  
23  
2.2.2 
Desensitization via Differential Privacy . . . . . . . .  . . . . .  
26  
2.2.3 
Information Theory Based Privacy Desensitization . . . . . 
31 
2.2.4 
Privacy Measurement and Evaluation . . . . . . . . . . . .  . .  
31  
2.3 
Privacy-Preserving Adversarial Analysis . . . . . .  . . . .  . . . .  . . . .  
32  
2.4 
Privacy Computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
34 
2.4.1 
Weaknesses of Privacy-Preserving Technologies . . . . . . 
34 
2.4.2 
The Inevitability of Privacy Computing . . . .  . . .  . . .  . . .  
35  
2.5 
Chapter Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
35 
References . . . . .  . . . . . .  . . . . .  . . . . . .  . . . . .  . . . . .  . . . . . .  . . . . .  . .  
36  
3 
Privacy Computing Theory .  . . .  . . .  . . .  . . . .  . . .  . . .  . . .  . . .  . . .  . . .  
43  
Fenghua Li, Hui Li, and Ben Niu 
3.1 
The Deﬁnition of Privacy Computing . . . . . . . . . . . . . . . . . . . . . 
44 
3.1.1 
The Basic Deﬁnition of Privacy Computing . . . . . . . . . . 
44 
3.1.2 
The Formal Description of Privacy Information . . . . . .  .  
45  
3.2 
Key Technology and Computing Framework of Privacy 
Computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
48 
3.2.1 
The Key Technology Components of Privacy 
Computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
48 
3.2.2 
The Privacy Computing Framework . . . .  . . . .  . . .  . . . .  
50  
3.3 
Important Characteristics of Privacy Computing . . . . . . . . . . . . .  
53  
3.3.1 
Atomicity . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . .  
54  
3.3.2 
Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
54 
3.3.3 
Sequentiality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
55 
3.3.4 
Reversibility . . . . . .  . . . .  . . . .  . . . . .  . . . .  . . . . .  . . . .  
55  
3.4 
Intelligent Perception and Dynamic Measurement 
of Privacy . . . . . .  . . . . . . . .  . . . . . . . . .  . . . . . . . . .  . . . . . . . .  
55  
3.4.1 
Intelligent Perception and Compressed Sensing 
of Privacy Information . . . . . . . .  . . .  . . .  . . .  . . . .  . . .  .  
56  
3.4.2 
Dynamic Measurement of Privacy Information . . . . . . . . 
57 
3.5 
Design Principles of Privacy-Preserving Algorithms . . . . . . . . . . 
58 
3.5.1 
Five Basic Design Principles for Privacy-Preserving 
Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
58 
3.5.2 
Applicability of Design Principles for 
Privacy-Preserving Algorithm . . . . .  . . . . .  . . . . .  . . . . .  
60  
3.6 
Evaluation of Privacy Preservation Effectiveness . . . . . . . .  . . . .  
62  
3.6.1 
Reversibility . . . . . .  . . . .  . . . .  . . . . .  . . . .  . . . . .  . . . .  
63  
3.6.2 
Extended Controllability . . . . . . . . . . . . . . . . . . . . . . . . 
63 
3.6.3 
Deviation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
64 
3.6.4 
Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
64 
3.6.5 
Information Loss . . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  
64

Contents
xi
3.7 
Privacy Computing Language (PCL) . . . . . . . . . . . . . . . . . . .  . .  
65  
3.7.1 
Privacy Deﬁnition Language . . . . . . . . . . . . . . . . . . . . . 
65 
3.7.2 
Privacy Operation Language . . . . . . . . . .  . . . . . . . . . .  .  
65  
3.7.3 
Privacy Control Language . . . . . . . . . . . . . . . . . . . . . .  
66  
3.8 
Determination and Traceability of Privacy Infringement . . . . . . . 
66 
3.8.1 
Traceability and Forensics Framework of Privacy 
Infringement . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . .  . .  
67  
3.8.2 
Determination of Privacy Infringement . . . . .  . . . . . . . .  
68  
3.8.3 
Traceability and Forensics of Privacy Infringement . . . . 
70 
3.9 
Privacy Information System Architecture . . . . . . . . . . . . . . . . . .  
72  
3.9.1 
Privacy Information System Architecture . . . . . . . . . . . . 
73 
3.9.2 
Example for Description and Usage of Privacy 
Information . . . . .  . . . .  . . . .  . . . .  . . . .  . . . .  . . . .  . . . .  
74  
3.9.3 
Application Examples of Privacy Computing . . . . . . . . .  
78  
3.10 
The Academic Connotations of Privacy Computing and Data 
Security . . . . . . .  . . . . . . .  . . . . . . .  . . . . . .  . . . . . . .  . . . . . . .  .  
83  
3.10.1 
The Academic Connotations of Privacy Computing . . . . 
83 
3.10.2 
The Academic Connotations of Data Security . . . .  . . .  .  
84  
3.10.3 
Privacy Computing Versus Traditional Privacy 
Preservation, Data Security, and Other Related 
Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
84 
3.11 
Chapter Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
88 
References . . . . .  . . . . . .  . . . . .  . . . . . .  . . . . .  . . . . .  . . . . . .  . . . . .  . .  
88  
4 
Privacy Computing Techniques . . . .  . . . .  . . . .  . . . .  . . . .  . . . .  . . . .  .  
89  
Fenghua Li, Hui Li, and Ben Niu 
4.1 
Theoretical Basis of Privacy-Preserving Algorithms . . . . . . . . .  .  
90  
4.1.1 
Probability Theory and Privacy Computing . . . . . . . . .  .  
90  
4.1.2 
Information Theory and Privacy Computing . . . . . . . . . 
99 
4.2 
Typical Privacy-Preserving Algorithms . . . . . . . . . .  . . . . . . . .  .  104  
4.2.1 
Privacy-Preserving Algorithms Based on Anonymity . . . 
104 
4.2.2 
Privacy-Preserving Algorithms Based on Differential 
Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
115 
4.2.3 
Privacy-Preserving Algorithm Based on Privacy-Utility 
Tradeoff . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . .  128  
4.3 
Extended Control of Privacy Information . . . . . . . . . . . . . . . . . . 
143 
4.3.1 
Deﬁnition of Extended Control . . . . . . .  . . . . . . . . . .  . .  143  
4.3.2 
Privacy Information Dissemination Control in Image 
Sharing . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . .  144  
4.3.3 
Privacy Operation Control for Document Formatting . . . 
158 
4.3.4 
Privacy Extended Control of Pictures Exchanged Across 
Systems . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . .  . . . .  160  
4.4 
The Applications of Privacy Computing . . . . .  . . . . . . .  . . . . . . .  171  
4.4.1 
Secure Computing of Multi-party Data . . . . . . . . . . . . . 
171 
4.4.2 
Federated Learning with Differential Privacy .  .  .  .  .  .  .  .  .  174

2
xii
Contents
4.4.3 
The Application of Privacy Computing in Machine 
Unlearning . . . . . . . . . . . . . .  . . .  . . .  . . .  . . .  . . .  . . .  .  177  
4.4.4 
The Application of Privacy Computing in Logistics . . .  .  18  
4.4.5 
The Application of Privacy Computing in 
Data Trading . . . . . .  . . . .  . . . . .  . . . .  . . . .  . . . . .  . . . .  183  
4.5 
Chapter Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
184 
References . . . . .  . . . . . .  . . . . .  . . . . . .  . . . . .  . . . . .  . . . . . .  . . . . .  . .  184  
5 
The Future Development Trends of Privacy Computing .  . . .  . . .  . .  .  187  
Fenghua Li, Hui Li, and Ben Niu 
5.1 
Fundamentals of Privacy Computing . . . . . . . . . . . . . . . . . . .  . .  187  
5.1.1 
Privacy Computing Model and Its Security Assurance 
Model . . . .  . . .  . . . .  . . .  . . . .  . . .  . . . .  . . . .  . . .  . . . .  .  187  
5.1.2 
Mathematical Foundations of Privacy Computing . . . . . .  188  
5.1.3 
Technical Architecture of Privacy Information Protection 
System . . . . . . .  . . . . . . . . .  . . . . . . . . .  . . . . . . . . .  . .  188  
5.2 
Privacy Perception and Dynamic Metrics . . . .  . . .  . . .  . . . .  . . .  .  189  
5.2.1 
Component Extraction and Perception of Privacy 
Information . . . . .  . . . .  . . . .  . . . .  . . . .  . . . .  . . . .  . . . .  189  
5.2.2 
Scenario-Adapted Privacy Dynamic Metrics . . . . . . . . . 
189 
5.2.3 
Quantitative Metrics for Privacy Measurement . . . . . . .  .  189  
5.3 
Privacy-Preserving Algorithm . . . . . . . . . . . . .  . . .  . . .  . . . .  . . .  190  
5.3.1 
Privacy Desensitization Primitives . . . . . . . . . . . . . . . . . 
190 
5.3.2 
Privacy-Preserving Algorithm Framework . . . . . . . . . . . 
190 
5.3.3 
Quantitative Indicators of the Protection Capability for 
Privacy-Preserving Algorithms . . . . . . . . .  . . . . . . . . .  .  191  
5.4 
Evaluation of Privacy Preservation Effectiveness . . . . . . . .  . . . .  191  
5.4.1 
Effectiveness Evaluation Metrics . . . . . . . . . . . . . . . . . . 
191 
5.4.2 
Performance Evaluation Automation . . . .  . . . . . . . . . .  .  191  
5.4.3 
Evaluation of Privacy Preservation Effectiveness Based on 
Privacy Mining . . . . . .  . . . .  . . . .  . . . .  . . . .  . . . .  . . . .  192  
5.5 
Determination and Traceability of Privacy Infringement . . . . . . . 
192 
5.5.1 
Infringement Determination Method . . . . . . . . . . . . . . . 
192 
5.5.2 
Audit Mechanism of Operation and Circulation . . . . . . . 
193 
5.5.3 
Extended Control Mechanism and Traceability of Privacy 
Infringement . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . .  . .  193  
5.6 
Chapter Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
193 
Postscript .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  195

About the Authors 
Fenghua Li received his B.S. degree in Computer Soft-
ware, M.S. and Ph.D. degrees in Computer Systems 
Architecture from Xidian University in 1987, 1990, 
and 2009, respectively. Currently, he is working as pro-
fessor and doctoral supervisor in the Institute of Infor-
mation Engineering, Chinese Academy of Sciences. He 
is also a doctoral supervisor in Xidian University and in 
the University of Science and Technology of China. His 
research interests include network security, system secu-
rity, privacy computing, and trusted computing. 
Hui Li received B.S. degree from Fudan University in 
1990 and M.S. and Ph.D. degrees from Xidian Univer-
sity in 1993 and 1998. Since June 2005, he has been the 
professor with the school of Cyber Engineering, Xidian 
University, Xi’an Shaanxi, China. His research interests 
include the areas of cryptography, wireless network 
security, information theory, and network coding. He 
is a chair of ACM SIGSAC CHINA. He served as the 
technique committee chair or co-chair of several confer-
ences. He has published more than 170 international 
academic research papers on information security and 
privacy preservation.
xiii

xiv
About the Authors
Ben Niu received his B.S. degree in Information Secu-
rity, M.S. and Ph.D. degrees in Cryptography from 
Xidian University in 2006, 2010, and 2014, respec-
tively. He was a visiting scholar in Pennsylvania State 
University from 2011 to 2013. Currently, he is working 
as professor and doctoral supervisor in the Institute of 
Information Engineering, Chinese Academy of Sci-
ences. His research interests include network security 
and privacy computing.

,
Chapter 1 
Introduction 
Fenghua Li 
, Hui Li
and Ben Niu 
The continuous evolution and pervasive application of communication technology, 
network technology, and computing technology have boosted interconnection of 
everything and ubiquitous sharing of information. With the continuous emergence of 
new information services, user data is frequently exchanged across systems, ecolo-
gies, and countries. User data contains a large amount of personal privacy informa-
tion, which is retained intentionally or unintentionally in different information 
systems. At the same time, the data protection capabilities and protection strategies 
of each information system are very different, which leads to cask principle and a 
dramatic increase in the risk of privacy leakage. Protection of personal information 
and the governance of privacy abuse have become a worldwide problem. 
Privacy preservation has received more and more attention from the society and 
extensive academic research, privacy preservation technologies for different scenar-
ios are also emerging in spurts, and there are still many misunderstandings and 
confusion about the concept of privacy preservation. In particular, data security and 
privacy preservation are easily confused. This chapter will explain the connection 
and difference between user data, personal information, and privacy information, 
clarify the difference between data protection, privacy protection, and privacy 
desensitization, and point out the threats and technical challenges faced by privacy 
preservation. 
F. Li (✉) · B. Niu 
Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China 
e-mail: lifenghua@iie.ac.cn; niuben@iie.ac.cn 
H. Li 
School of Cyber Engineering, Xidian University, Xian, Shaanxi, China 
e-mail: lihui@mail.xidian.edu.cn 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
F. Li et al., Privacy Computing, https://doi.org/10.1007/978-981-99-4943-4_1
1

2
F. Li et al.
1.1 
User Data, Personal Information, and Privacy 
Information 
Privacy preservation should cover the full life cycle protection of privacy informa-
tion. If we want the whole society to pay high attention to privacy preservation and 
put it into practice, the connotation of personal information and privacy information 
should be clariﬁed, as well as the connection and difference between user data, 
personal information, and privacy information. 
1.1.1 
User Data 
Data usually refers to a sequence of one or more symbols. Data can be observed, 
collected, processed, and analyzed, and through interpretation and analysis, data 
becomes information. From the perspective of information theory, data is the carrier 
of information. Data can be organized into many different types or data structures, 
such as lists, graphs, and objects. Data also has multiple modalities, such as numbers, 
text, images, video, and speech. Multimodal data can be exchanged across borders, 
systems, and ecosystems in a ubiquitous network environment. 
User data can be data related to individuals, as well as related to businesses, 
organizations, objects, environments, and the like. In the era of the intelligent 
interconnection of everything and ubiquitous sharing of information, data has 
become a strategic resource, which is crucial to the interests and security of individ-
uals, enterprises, society, and even the country. 
1.1.2 
Personal Information 
As deﬁned in the “Civil Code of the People’s Republic” of China, personal infor-
mation is all kinds of information recorded electronically or in other ways that can 
identify a speciﬁc natural person alone or in combination with other information, 
including the natural person’s name, date of birth, identiﬁcation (ID) number, 
biometric information, address, phone number, email, health information, where-
abouts information, etc. 
In Europe and North America, personal information mostly refers to personal data 
or Personally Identiﬁable Information (PII). Personal data is deﬁned in the European 
Union’s General Data Protection Regulation (GDPR) [1] as any information relating 
to an identiﬁed or identiﬁable natural person (“data subject”); an identiﬁable natural 
person is one who can be identiﬁed, directly or indirectly, in particular by reference 
to an identiﬁer such as a name, an identiﬁcation number, location data, an online 
identiﬁer or to one or more factors speciﬁc to the physical, physiological, genetic, 
mental, economic, cultural, or social identity of that natural person.

1
Introduction
3
Table 1.1 Classiﬁcation of personal information by the US Federal Trade Commission 
Data elements
Data segments 
Identiﬁcation data
Name, address(including latitude and longitude information), etc. 
15 items 
Sensitive identiﬁcation 
data 
Social security number, driver’s license number, etc. 5 items 
Demographic data
Age, height, gender, religion, language etc. 29 items 
Court and public record 
data 
Bankruptcy information, criminal information, judgments, etc. 7 items 
Social media and tech-
nology data 
Electronics purchases, friend connections, internet connection type, 
internet provider, social media and internet accounts, etc. 18 items 
Home and neighborhood 
data 
Census tract data, dwelling type, heating and cooling, home equity, 
move date, etc.24 items 
General interest data
Apparel preferences, preferred sports, favorite stars, political inclina-
tions, etc. 43 items 
Financial data
Purchasing power, credit history, loan information, disposable 
income, etc. 21 items 
Vehicle data
Brand preference, vehicle identiﬁcation code, preferred model, etc. 
14 items 
Travel data
Last travel time, preferred destination, preferred airline, etc. 9 items 
Purchase behavior data 
Purchase types, purchase channels, holiday gifts, and clothing sizes 
purchased, etc. 29 items 
Health data
Search tendency for illnesses and medicines, smoking status, allergy 
information, etc. 15 items 
The term PII is generally adopted by the United States. The U.S. Federal Trade 
Commission categorizes data related to natural persons, and divides personal infor-
mation into 12 categories and 221 attribute ﬁelds. See Table 1.1 for speciﬁc 
categories [2]. 
Data records of personal information contain different ﬁelds, which can be 
divided 
into 
explicit 
identiﬁers, 
quasi-identiﬁers, 
sensitive 
attributes, 
and 
non-sensitive attributes. Explicit identiﬁers are sets of attributes that can clearly 
identify the identity of the record subject, including names, social security numbers, 
phone numbers, ID numbers, and other information. A quasi-identiﬁer is a collection 
of attributes that, when combined, can potentially identify the subject of a record, 
including information such as age, gender, zip code, and more. Sensitive attributes 
contain sensitive individual-speciﬁc information such as illness and salary. Note that 
the non-sensitive properties are all other properties that are not in the above three 
categories and the sets of these four types of ﬁelds are disjoint. 
In the process of information service, personal information may exist explicitly in 
structured records, such as medical records in hospitals, student registration infor-
mation in schools, household registration information in government departments, 
and vehicle and driver information in trafﬁc management departments. It may also 
exist in unstructured data such as Twitter, Moments, and pictures shared by many 
social networks. Identifying, measuring, and protecting users’ privacy information 
for different types of data records is an extremely complex and challenging problem.

4
F. Li et al.
1.1.3 
Privacy Information 
The “Civil Code of the People’s Republic of China” deﬁnes privacy as the tranquil-
ity of the private life of a natural person and the private space, private activities and 
private information that no one else should know about. Privacy information refers 
to sensitive information in personal information, which is a collection of identiﬁers, 
quasi-identiﬁers, and sensitive attributes in personal information records. Privacy 
reﬂects the relationship between identiﬁers, quasi-identiﬁers, and sensitive 
attributes. 
Privacy information is not static, and they have two typical characteristics: 
relative stability in a certain period of time and space-time dynamics. The dynamism 
means that privacy information usually changes with the changes of subjective 
preferences of natural persons, time, and scene. For example, some people are 
willing to publish text, photos, and other information that reﬂect their personal 
preferences on social networks, thinking that these are not privacy information, so 
the dynamic nature of privacy information is also subjective. Spatiotemporal dynam-
ics bring greater technical challenges to privacy preservation. 
1.2 
Privacy Protection and Privacy Desensitization 
To promote theoretical and technical research on privacy preservation, it is necessary 
to clarify the connection and distinction between traditional data security and 
privacy protection. Data security refers to ensuring the conﬁdentiality, integrity, 
non-repudiation, and availability of data, mostly using technologies such as cryp-
tography and access control. Data security technology ensures that the protected data 
is recoverable, that is, the information is lossless. User data contains privacy 
information, so data security techniques can naturally be applied to privacy protec-
tion. This book classiﬁes such techniques as privacy protection. On the other hand, 
privacy preservation should make privacy information partially available in a ubiq-
uitous interconnected environment while being protected. It is essential to achieve a 
balance between the strength of desensitization and the usability of information, 
which is the core connotation of privacy preservation, and also the new theory and 
technology needed for privacy preservation. 
This book divides privacy preservation technologies into two categories: privacy 
protection and privacy desensitization. The privacy information protected by privacy 
protection technology is undistorted and reversible; the privacy information 
protected by privacy desensitization technology is distorted and irreversible. The 
evolution process of privacy preservation technology is shown in Fig. 1.1.

1
Introduction
5
In 2002, Rakesh 
et al. proposed 
an access 
control model 
for privacy 
preservation 
In 1978, Rivest, Adleman 
and Dertouzos found that 
RSA has some homomorphic 
characteristics and proposed 
the concept of homomorphic 
encryption; In 2009, Gentry 
constructed the first fully 
homomorphic encryption 
scheme 
In 1982, Yao first 
proposed the secure 
bipartite computing 
protocol; 
Subsequently, 
Goldreich proposed a 
secure multi-party 
computing protocol 
Cryptographic Solution 
In 2006, Cynthia et 
al. proposed 
differential privacy 
Perturbation Solution 
Access Control Solution 
In 2007, Li et al. 
proposed 
t-closeness. 
In 2016, Li Fenghua 
et al. proposed a 
Cyberspace-oriented 
access control model 
In 2006, 
Machanavaijhala 
proposed 
l-diversity 
In 1998, Samarati 
and Sweeney 
proposed 
k-anonymity 
Anonymous Solution 
In 2013, Miguel 
proposed Geo-
Indistinguishability 
In 2015, Li Fenghua et 
al.proposed the theory 
and technology system 
of privacy computing 
In 2013, John et 
al. proposed 
local differential 
privacy 
Full Lifecycle Solution 
Fig. 1.1 Evolution process of privacy protection technology 
1.2.1 
Privacy Protection 
Privacy protection technology refers to the use of encryption, secure computing, 
access control, and other technologies to protect privacy information from 
unauthorized access, and the protected privacy information is reversible. 
1.2.1.1 
Encryption 
Encryption is the most commonly used privacy protection technology, where per-
sonal information is encrypted for transmission, storage, and sharing. The encrypted 
information can only be decrypted and accessed with the decryption key. Although 
encryption protects the security of data, the encrypted data cannot be directly 
counted, handled, and processed, which will increase the complexity of using data. 
For encrypted data processing, two technology routes that are currently receiving 
wide attention from academia and industry are homomorphic encryption and conﬁ-
dential computing based on trusted computing environments. 
Homomorphic encryption means to perform function computation f(E(x)) on the 
ciphertext, and the result of decryption is equivalent to performing the corresponding 
function computation f(x) on the plaintext x  x, that is, the encryption function E(x) 
and the function computation f(x) can exchange the order, i.e., D( f(E(x))) = f(D(E(-
x))) = f(x) D( f(E(x))) = f(D(E(x))) = f(x). With the support of homomorphic 
encryption, the user can encrypt the data and then hand it over to cloud computing 
or other partners. After the partner performs corresponding operations on the 
ciphertext, the user decrypts the ciphertext to obtain the computation result of the 
plaintext. RSA [3] and Pailiar algorithm [4] have the properties of multiplication and 
addition homomorphism, respectively, but general computing needs to have

homomorphic properties for addition and multiplication at the same time. In 2009, 
Gentry [5] proposed the ﬁrst fully homomorphic algorithm, which attracted wide-
spread attention and inspired a lot of follow-up research. However, the complexity of 
the current fully homomorphic algorithm is still very high, and there still exists a 
large gap from practical application. 
6
F. Li et al.
Conﬁdential computing based on a Trusted Execution Environment (TEE) 
focuses on data protection in the computing process. The system maintains a secure 
space, decrypts the encrypted data after importing it into the secure memory space, 
calculates the plaintext, and encrypts it when it is called out of the space. This secure 
memory space is inaccessible to other users, reducing the risk of data leaking from 
other parts of the system while maintaining transparency to users. Especially in 
multi-tenant public cloud environments, conﬁdential computing keeps sensitive data 
isolated from other authorized parts of the system stack. Intel SGX (Software Guard 
Extension) is currently the main method for implementing conﬁdential computing, 
which generates an isolated environment Enclave in memory. SGX uses strong 
encryption and hardware-level isolation to ensure the conﬁdentiality of data and 
code against attacks, and can still protect applications and code even when the 
operating system and BIOS ﬁrmware are compromised. 
1.2.1.2 
Secure Multiparty Computation 
Secure Multi-Party Computation (MPC) originated from the secure two-party com-
putation protocol “Millionaire Problem” proposed by Yao [6]. Computational 
parties cooperate to complete a computing problem without revealing their own 
sensitive information. As research progresses, there are already several practical 
cases for secure multi-party computation. The Boston Women’s Workforce Council 
used MPC in 2017 to calculate compensation statistics for 166,705 employees at 
114 companies [7]. The company does not provide its raw data due to privacy 
concerns, and calculations show that the gender gap in the Boston area is even wider 
than previously estimated by the U.S. Bureau of Labor Statistics. To calculate the 
exact conversion rate from an ad to an actual purchase, Google calculated the size of 
the intersection between the list of people who viewed an ad for an item online and 
the list of people who actually bought the item. To calculate this value without 
exposing the list-speciﬁc data, Google uses a private intersection-sum protocol 
[8]. Although the protocol efﬁciency is not ideal, it is simple and can meet Google’s 
computing requirements. 
1.2.1.3 
Access Control 
Access control is one of the most important approaches to achieving privacy 
preservation. The essence of privacy preservation is to share privacy information 
with authorized entities at the right time and in the right way. In traditional access 
control 
systems, 
permissions 
are 
formulated 
and 
implemented 
by 
system

administrators. Common access control strategies include discretionary access con-
trol, mandatory access control, and role-based access control. In privacy preservation 
scenarios, permissions and access control policies are basically set by the data 
owner. In application environments such as social networks and Internet services, 
privacy information is often forwarded by friends and spread across systems and 
ecosystems among different service providers. Therefore, extended control has 
become the biggest problem faced in privacy preservation scenarios. In 2016, Li 
Fenghua et al. [9] proposed a Cyberspace-oriented access control model and an 
extended control model. 
1
Introduction
7
Encryption can also be combined with access control. Attribute Based Encryption 
(ABE) is an encryption method that effectively implements access control [10] 
where users have several attributes, and each attribute is assigned a public-private 
key pair. When encrypting a plaintext, the encryptor selects the public key of the 
corresponding attribute to construct an encryption key according to the access 
control policy. This encryption key can directly encrypt the plaintext, or encrypt 
the key used to encrypt the plaintext. If the user has a private key with attributes that 
match the access control policy, the private key of the corresponding attribute is 
selected to construct the decryption key, and the corresponding ciphertext can be 
decrypted similarly. ABE is essentially a public key encryption system with rela-
tively slow encryption and decryption speed. 
1.2.2 
Privacy Desensitization 
Privacy desensitization protects privacy information by adopting a distorted and 
irreversible method, so that the desensitized information cannot be associated with 
the data subject. Privacy desensitization of the privacy information contained in the 
data includes but is not limited to existing methods such as Generalization, Suppres-
sion, Anatomization, Permutation, and Perturbation. New theoretical innovations in 
privacy desensitization are needed in the future. Privacy desensitization is also often 
referred to as privatization or anonymization. 
1.2.2.1 
Generalization 
Generalization is a technique which replaces a speciﬁc value in a class of properties 
with a more general value. For example, if a person is 25 years old, it can be 
generalized to 20–30 years old; a person’s occupation is a programmer or a lawyer, 
and it can be generalized to white-collar workers (brain workers).

8
F. Li et al.
1.2.2.2 
Suppression 
Suppression refers to replacing an attribute, attribute value, or part of an attribute 
value with * when publishing information. For example, the mobile phone number is 
represented as 135****3675, and the credit card number is represented as 
4392********. 
1.2.2.3 
Anatomization and Permutation 
The goal of anatomization and permutation is to remove the association between 
quasi-identiﬁers and sensitive attributes without changing the values of quasi-
identiﬁers or sensitive attributes. Anatomization is to divide the original record 
table into two tables to publish, where one table publishes quasi-identiﬁer attributes, 
the other table publishes sensitive attributes, and the two tables only have the same 
GroupID as a common attribute. Permutation is used to divide a set of data records 
into groups, and permute sensitive values within the group, thereby disrupting the 
correspondence between quasi-identiﬁers and sensitive attributes. 
1.2.2.4 
Perturbation 
Data perturbation refers to the technique of replacing the original data values with 
synthetic data values. Statistical information does not change signiﬁcantly after 
perturbation, and the perturbed data lose relevance to the real data subject. Classic 
data perturbation mechanisms include noise addition, data exchange, synthetic data 
generation, etc. Noise addition is mainly used for privacy preservation of numerical 
data by generating noise values from a speciﬁc distribution of noise and adding them 
to sensitive values. The main idea of data exchange is to exchange the values of 
sensitive attributes between personal data records, which can maintain low-order 
frequency statistics or marginal distributions for statistical analysis. Synthetic data 
generation aims of building a statistical model according to data, and then 
up-sampling from the model to replace the original data. Perturbation has a long 
history of application in statistical release control because of its simplicity, effec-
tiveness, and the ability to maintain statistical information [11]. 
On the basis of the above desensitization operations, a series of privacy desensi-
tization models and methods have been developed, including k-anonymity [12], l-
diversity [13], t-closeness [14], differential privacy [15], local differential privacy 
[16], etc., which will be introduced in subsequent chapters.

1
Introduction
9
1.3 
The “Four Rights” of Privacy Preservation 
GDPR has made relevant provisions on the right to know, the right to erasure, and 
the right to be forgotten. The right to know addresses the collection and processing 
of personal information, and the right to erasure and the right to be forgotten address 
the storage of personal information. With the popularization and application of 
mobile apps, although the right to know has not been fully implemented and has 
become the root cause of out-of-scope collection of privacy information, it has been 
widely noticed by everyone. In reality, data subjects voluntarily provide some 
privacy information to obtain personalized services, but the data subjects’ right to 
erasure and to be forgotten are privacy preservation issues that deserve more 
attention. Service providers’ neglect of the right to erasure and to be forgotten is a 
source of misuse of privacy information. Under the circumstance that privacy 
information is widely exchanged and disseminated in the ubiquitous interconnected 
environment, the “extended authorization” proposed by the authors of this book is 
the core criterion to ensure the controlled sharing of privacy information, and an 
effective guarantee mechanism for balancing privacy desensitization and usability. 
1.3.1 
Related Parties of Privacy Information 
The related parties of privacy information are the participants in the processing of 
privacy information in the process of privacy preservation, including the following 
ﬁve aspects. 
1. Data subject: refers to the owner of personal data or personal information. 
2. Controller: refers to the natural or legal person, public authority, agency, or other 
body which, alone or jointly with others, determines the purposes and means of 
the processing of privacy information. 
3. Processor: refers to a natural or legal person, public authority, agency, or other 
body which processes personal data on behalf of the controller. 
4. Recipient: refers to a natural or legal person, public authority, agency, or another 
body, to which the privacy information are disclosed, whether a third party or not. 
5. Third party: refers to a natural or legal person, public authority, agency, or body 
other than the data subject, controller, processor, and persons who, under the 
direct authority of the controller or processor, are authorized to process 
personal data. 
1.3.2 
Right to Know 
The right to know requires the controller to obtain the consent of the data subject 
when collecting and processing personal information. The data subject has the right

to know how the data controller processes and stores personal information, where the 
personal information is obtained and to whom it will be transferred. When the 
purpose and method of processing personal information are changed, the individ-
ual’s consent shall be obtained again. When personal information processors transfer 
personal information to third parties, they should also inform individuals of the 
recipient’s identity and contact information. The recipient should continue to per-
form the obligations of the personal information processor. If there is any change, the 
data subject needs to be notiﬁed again and consent should be obtained. 
10
F. Li et al.
1.3.3 
Right to Erasure 
The right to erasure is the right of the data subject to ask the controller to delete his or 
her personal information. When the data subject withdraws consent or the personal 
information is no longer necessary for the purpose for which it was collected and 
processed, the data controller can be asked to delete the relevant data. If the 
controller has made the data public, it should consider taking reasonable steps, 
including technical measures, to inform other data controllers who are processing 
personal data that the data subject has asked them to delete the personal information 
of the data subject, and the data controller and processor should delete personal 
information in a deterministic and irrecoverable manner. 
1.3.4 
Right to be Forgotten 
The right to be forgotten means that when the storage period agreed between the data 
subject and the controller has expired or the processing purpose has been achieved, 
and the personal information controller or processor stops providing products or 
services, the controller or processor should take the initiative to delete personal 
information, i.e., personal information is automatically deleted after being retained 
by the data controller or processor for a certain period of time. 
1.3.5 
Extended Authorization 
In social network applications, there are widespread problems such as personal 
information being forwarded more than twice by friends across Moments and 
systems. Therefore, in the process of dissemination of privacy information, whether 
the data subject can extend authorization for the cross-system exchange of personal 
information, and the implementation of extended control is crucial to privacy 
protection. The requirement for extended authorization is not mentioned in GDPR 
and other privacy protection-related regulations, but in the era of ubiquitous

information sharing, extended authorization is the basis for ensuring controlled 
sharing of privacy information. Extended control is a technical implementation 
method of extended authorization, and an indispensable and effective mechanism 
for balancing privacy desensitization and privacy information utility. 
1
Introduction
11
Although the “Personal Information Protection Law of the People’s Republic of 
China” requires that personal consent is required for personal information 
processing, if the purpose of processing personal information, processing methods, 
and types of personal information are changed, personal consent should be obtained 
again, but in in the actual information system implementation process, if there are no 
technical means to implement extended control mechanism, the legal requirements 
are difﬁcult to implement. 
1.4 
Technical Challenges in Privacy Preservation 
The development of technologies such as the Internet, mobile Internet, Internet of 
Things, cloud computing, 5G, and satellite communications has spawned an endless 
stream of new service paradigms, and privacy information ﬂows widely across 
systems, ecosystems, and even across borders. From the perspective of the “three-
dimensional space” composed of time, scene, and privacy information, any privacy 
preservation scheme is a “point” in the three-dimensional space; It is necessary to 
form a privacy preservation algorithm system in the “three-dimensional space” that 
is continuous in time, universal in scenarios, and common to all privacy information 
modalities, so that the system can ensure the stability of the industry privacy 
information system and realize the protection of the full lifecycle, any scenario, 
and any privacy information. Although thousands of academic papers or solutions 
have been published on research in privacy preservation around the world, why 
hasn’t the problem of privacy preservation been effectively solved in practical 
applications? This is because privacy preservation has not theoretically solved a 
series of challenges in systematization, computability, etc. 
1.4.1 
Threats in Privacy Preservation 
1.4.1.1 
Over-Collection of Information 
In recent years, with the rapid development of mobile Internet technology, mobile 
App has become the main carrier of network information service and an important 
tool for users to use mobile Internet. However, many mobile apps, while providing 
users with network services, have problems such as compulsory authorization, 
excessive claiming of rights, and frequent over-scope collection of personal infor-
mation, leading to the increasingly serious threat of personal information leakage.

12
F. Li et al.
1.4.1.2 
Unauthorized Use and Excessive Retention of Information 
in the Information System 
Network information service providers have collected a large amount of personal 
information, created user proﬁles without obtaining user consent and used “big data-
enabled price discrimination”, illegally overstepping their authority to use personal 
information and illegally sharing personal information with third parties, resulting in 
excessive retention of personal information across systems and posing a huge risk of 
privacy leakage. 
1.4.1.3 
Inconsistency of Cross-System Protection Capabilities 
Privacy information ﬂows widely across systems, and the protection capabilities of 
each system may be inconsistent, so the protection capabilities of the whole life cycle 
of privacy information are limited by the system with the weakest protection 
capability. The lack of extended control mechanism for the ﬂow of privacy infor-
mation cross-system increases the risk of cross-system privacy information leakage. 
1.4.2 
Systematic Computational Model 
In order to realize the full life cycle protection of privacy information and ensure its 
implementation in a ubiquitous information system, it is necessary to build a 
systematic computing model for the preservation of privacy information, which 
supports the measurement and on-demand protection of privacy information. 
1.4.2.1 
Perception and Dynamic Measurement of Privacy Information 
Privacy information is multi-dimensionally correlated, scene-changing, and subjec-
tive, leading to dynamic changes in privacy cognition. To establish a systematic 
computing model, it is necessary to break through key bottlenecks such as the 
privacy information perception of diverse data, the ﬁne-grained division of privacy 
attribute vectors, the dynamic quantiﬁcation of privacy attributes, and the dynamic 
assessment of privacy information value and leakage risk associated with multiple 
factors, so as to solve the problem of accurate privacy perception in massive data and 
the time complexity problem of privacy dynamic measurement.

1
Introduction
13
1.4.2.2 
On-Demand Privacy Preservation and Combination 
of Privacy-Preserving Mechanisms 
Regarding the changes in application scenarios, differences in privacy preservation 
requirements of data subjects, and diverse data types, it is necessary to study key 
technologies such as privacy desensitization mechanisms in the process of personal 
information collection, processing and sharing, privacy desensitization strategies 
adapted to scenarios, and parameter selection. At the meantime, regarding the 
different privacy preferences of users, diverse privacy preservation algorithms, and 
differences in protection degree requirements, etc., we should consider speciﬁc 
characteristics of different privacy algorithms, the relevance of multimodal data, 
and privacy preservation mechanisms, explore scenario-adapted methods for efﬁ-
cient and optimal combination of privacy preservation algorithms, and break through 
key technologies such as ﬁne-grained feature description of privacy preservation 
algorithms, characterization of algorithm combination characteristics, detection, and 
resolution of multi-party privacy preference conﬂicts, to achieve automatic optimi-
zation and combination of multi-preserving algorithms for different types of data 
with differentiated privacy-preserving requirements. 
1.4.3 
Evaluation of Privacy-Preserving Effectiveness 
Considering the diverse theoretical systems of different privacy-preserving algo-
rithms, differences in application requirements and algorithm effects, etc., it is vital 
to study the multi-dimensional evaluation index system of privacy-preserving effec-
tiveness, and propose a scenario-adapted quantitative evaluation model for effec-
tiveness. It is necessary to break through key technologies such as qualitative and 
quantitative performance evaluation, privacy-preserving performance limit estima-
tion of single algorithm and combined algorithm, and quantitative evaluation of 
privacy-preserving strength for dynamic data addition, etc., so as to provide multi-
dimensional quantitative support for the evaluation of algorithm effectiveness and 
selection of algorithms. 
Moreover, it is essential to study the evaluation method of privacy-preserving 
effectiveness based on big data analysis while taking into account the characteristics 
of big data in actual Internet applications, such as large time-scale, differentiated 
sources, large sample space, differences in the effect and continuous evolution of 
privacy-preserving algorithms, and correlation to privacy in desensitized informa-
tion. It is necessary to break through key technologies such as accurate collection of 
cross-platform privacy-related background data, rapid perception and labeling of 
non-explicit privacy attributes, ﬁne-grained data owner privacy knowledge model-
ing, multi-source scene content cross-correlated privacy leak detection, and Bayes-
ian statistical inference based on mutual information between linked data, so as to

achieve a reverse evaluation of privacy-preserving effect based on privacy mining 
using big data analysis. 
14
F. Li et al.
1.4.4 
Extended Control of Privacy Information 
The GDPR and other regulations do not explicitly mention the extended authoriza-
tion of privacy, but there are widespread problems in social network applications 
such as personal information being forwarded more than twice by friends across 
Moments and systems. In view of the diverse social application scenarios, the 
differences in the sensitivity of the types of objects in the media, the diversity of 
data subjects, and the differences in the privacy-preserving capabilities of the 
platforms, considering the impact of cross-platform ﬂowing and multiple forwarding 
of multi-subject data on privacy control, and impact of subject, object, social 
platform, privacy needs, and other elements on dissemination control, it is necessary 
to establish a ﬁne-grained extended control mechanism that supports the random 
topology of social networks and the differential privacy-preserving requirements of 
multiple data subjects, and breaks through key technologies including the normal-
ized description of multi-factor constraints such as scene, space-time, content, and 
privileges, secure binding of media with followed extended-control-policy of pro-
tection strength and constraint conditions, the sharing process monitoring based on 
labeling and exchange auditing, the dynamic logical relationship generation of the 
data cross-platform ﬂowing, differential description of privacy-preserving strength 
of subject-related data objects, normalized description of data extended control 
policy across domains, so as to support the controlled sharing of data in ubiquitously 
connected environment. 
1.4.5 
Forensics of Privacy Infringement 
To address the problems of dynamic and random dissemination paths of privacy 
information, hidden privacy infringement, and fragmented spatial and temporal 
distribution of evidence in the diversity of Internet applications, it is necessary to 
study infringement monitoring for multi-type data, whole-process infringement clue 
capture and analysis, and abnormal data sharing behavior judgment and traceability, 
break through key technologies such as cross-domain exchange control and violation 
determination, cross-platform multi-dimensional reconstruction of privacy infringe-
ment events, and virtual identity positioning, so as to achieve accurate tracking of 
privacy infringements.

1
Introduction
15
1.5 
Chapter Summary 
In the continuous evolution of new business paradigms, frequent cross-border, cross-
system, and cross-ecosystem exchange of user data results in privacy information 
being retained intentionally or unintentionally in different information systems. The 
cask principle effect caused by the differences in the data protection capabilities and 
protection strategies of various information systems has led to an increasingly 
prominent risk of privacy leakage. Issues such as the lack of privacy information 
protection methods and the difﬁculty in governing the abuse of privacy information 
have become worldwide problems. Privacy preservation can be divided into two 
categories: privacy protection and privacy desensitization. Faced with the controlled 
sharing of information across systems, privacy desensitization technology is much 
needed to ensure the realization of the “four rights of privacy”. At present, many 
privacy preservation solutions have been proposed, which however cannot effec-
tively address the privacy issues in practical applications. Therefore, it is a must to 
establish a new and complete theoretical system for the protection of privacy 
information in its entire life cycle to support privacy preservation in information 
systems. The notion of privacy computing proposed by Li Fenghua et al. [17] is the 
solution to the privacy preservation problem in practical applications, which will be 
elaborated in the subsequent chapters. 
References 
1. General data protection regulation. (2018) 
2. Federal Trade Commission. Data brokers: a call for transparency and accountability. (2014) 
3. Rivest, R.L., Adleman, L., Dertouzos, M.L.: On data banks and privacy homomorphisms. 
Foundations of Secure Computation. 4(11), 169–179 (1978) 
4. Paillier, P.: Public-Key Cryptosystems Based on Composite Degree Residuosity Classes. In: 
International Conference on the Theory and Application of Cryptographic Techniques, 
pp. 223–238. Springer, Berlin (1999) 
5. Gentry, C.: A fully homomorphic encryption scheme. Stanford University, Stanford (2009) 
6. Yao, A.C.: Protocols for secure computations. In: IEEE 23rd Annual Symposium on Founda-
tions of Computer Science, pp. 160–164. IEEE Press, Piscataway (1982) 
7. Lapets, A., Jansen, F., Albab, K.D., et al.: Accessible privacy-preserving web-based data 
analysis for assessing and addressing economic inequalities. In: Proceedings of the 1st ACM 
SIGCAS Conference on Computing and Sustainable Societies, pp. 1–5. ACM Press, New York 
(2018) 
8. Ion, M., Kreuter, B., Nergiz, E., et al.: Private intersection-sum protocol with applications to 
attributing aggregate ad conversions. IACR Cryptology ePrint Archivet. 7, 738 (2017) 
9. Li, F., Wang, Y., Yin, L., et al.: Novel cyberspace-oriented access control model. J. Commun. 
37(5), 9–20 (2016) 
10. Sahai, A., Waters, B.: Fuzzy identity-based encryption. In: International Conference on Theory 
& Applications of Cryptographic Techniques, pp. 457–473. Springer, Berlin (2005) 
11. Adam, N.R., Worthmann, J.C.: Security-control methods for statistical databases: a compara-
tive study. ACM Comput. Surv. 12, 515–556 (1989)

16
F. Li et al.
12. Sweeney, L.: K-anonymity: a model for protecting privacy. Int J Uncertain Fuzz. 10(5), 
557–570 (2002) 
13. Machanavajjhala, 
A., 
Gehrke, 
J., 
Kifer, 
D., 
et 
al.: 
L-Diversity: 
Privacy 
beyond 
K-Anonymity. In: IEEE 22nd International Conference on Data Engineering, p. 24. IEEE 
Press, Piscataway (2006) 
14. Li, N., Li, T., Venkatasubramanian, S.: T-Closeness: Privacy beyond K-Anonymity and 
L-Diversity. In: IEEE 23rd International Conference on Data Engineering, pp. 106–115. 
IEEE Press, Piscataway (2007) 
15. Dwork, C.: Differential privacy: a survey of results. In: International Conference on Theory and 
Applications of Models of Computation, pp. 1–19. Springer, Berlin (2008) 
16. Duchi, J.C., Jordan, M.I., Wainwright, M.J.: Local Privacy and Statistical Minimax Rates. In: 
IEEE 54th Annual Symposium on Foundations of Computer Science, pp. 429–438. IEEE Press, 
Piscataway (2013) 
17. Li, F., Li, H., Jia, Y., et al.: Privacy computing: concept, connotation and its research 
trend. J. Commun. 37(4), 1–11 (2016)

Chapter 2 
Advances in Privacy Preservation 
Technologies 
Fenghua Li 
, Hui Li 
, and Ben Niu 
The rapid development of information technology and the continuous evolution of 
new business paradigms and personalized services have promoted frequent cross-
border, cross-system, and cross-ecosystem interactions of users’ data, increasing the 
retention of privacy information in different information systems and expanding the 
risk of privacy information leakage. Subsequently, privacy preservation has received 
more and more attention from the society, and scholars have also carried out 
extensive academic research to develop privacy preservation technologies for dif-
ferent scenarios. This chapter will sort out and summarize the research advances of 
privacy preservation technologies from the perspective of technology evolution, 
with the main focus on privacy protection technology, privacy desensitization 
technology, privacy preservation confrontation analysis, etc. This chapter shows 
the necessity of proposing privacy computing during the evolution of privacy 
preservation technology. 
2.1 
Privacy Protection Technology 
Privacy 
protection 
technology 
mainly 
protects 
privacy 
information 
from 
unauthorized access by methods such as data encryption and decryption, homomor-
phic encryption, secure multiparty computation, access control, and trusted
F. Li (✉) · B. Niu 
Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China 
e-mail: lifenghua@iie.ac.cn; niuben@iie.ac.cn 
H. Li 
School of Cyber Engineering, Xidian University, Xian, Shaanxi, China 
e-mail: lihui@mail.xidian.edu.cn 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
F. Li et al., Privacy Computing, https://doi.org/10.1007/978-981-99-4943-4_2
17

computing. It is worth noting that, after being protected by privacy protection 
technology, the information obtained usually retains its reversibility.
18
F. Li et al.
2.1.1 
Homomorphic Encryption 
Homomorphic encryption can directly perform various operations on encrypted data 
without affecting its conﬁdentiality. Rivest et al. [1] ﬁrst proposed the concept of 
fully homomorphic encryption in 1978 and presented four homomorphic encryption 
schemes; in 1999, Paillier [2] designed an additive homomorphic encryption algo-
rithm based on composite modulus, which has been widely recognized and applied 
in the information security industry [3, 4]; in 2009, Gentry [5] constructed a fully 
homomorphic encryption scheme based on ideal lattice for the ﬁrst time and made a 
detailed study on the fully homomorphic encryption; Dijk et al. [6] proposed a fully 
homomorphic encryption system on integers based on the Gentry scheme; Bost et al. 
[7] constructed a variety of machine learning schemes based on homomorphic 
encryption. Although many improved homomorphic encryption schemes have 
been proposed [8, 9], they are not applicable for privacy preservation in massive 
service data scenarios due to the high computational costs of homomorphic 
operations. 
2.1.2 
Secure Multiparty Computation 
Secure multiparty computation can solve the data protection problem of collabora-
tive computing among a group of mutually distrusting parties. It has been playing an 
important role in cryptography since its inception in the 1980s and received contin-
uous attention from academia and industry. It has made great progress in both 
theoretical research [10–12] and practical applications [13, 14] and can be widely 
used for data protection in many computing ﬁelds, such as data mining, database 
query, scientiﬁc computing, geometric or geometric relationship judgment, and 
statistical analysis. Although universal secure multiparty computation schemes 
[15] can theoretically achieve arbitrary collaborative computing, they generally 
suffer from excessive resource consumption, slow processing speed, and poor 
portability in implementation. 
2.1.3 
Access Control 
Data access control refers to controlling the access rights of the exchanged data. The 
research 
on 
authorization 
and 
extended 
control 
for 
massive 
multi-source

heterogeneous data mainly includes permission assignment, automatic permission 
adjustment, permission extended control, etc. 
2
Advances in Privacy Preservation Technologies
19
2.1.3.1 
Permission Assignment 
The current research focus in academia is attribute-based permission assignment in 
the cloud environment. Aiming at addressing the single-point bottleneck and 
low-efﬁciency issue of single-attribute authorization centers in the public cloud 
storage environment, Xue et al. [16] designed an authorization framework based 
on multi-attribute authorization centers to achieve secure and efﬁcient authorization. 
Aiming at addressing the frequent ciphertext update issue caused by the dynamic 
change of permissions in the cloud storage environment, Wang et al. [17] proposed a 
dynamic authorization access control mechanism for the cloud storage, thereby 
reducing the cost of ciphertext update and improving the ﬂexibility of authorization. 
Aiming at addressing the permission assignment issue caused by the untrusted 
authorizer or malicious attacks, Guan et al. [18] proposed an attribute encryption-
based permission assignment scheme with multiple authorization centers to improve 
the system authorization efﬁciency. Considering the characteristics of diverse device 
types and diverse user identities, Saxena et al. [19] designed an attribute-based 
authorization scheme, which greatly reduces the communication and computation 
overheads; but this scheme does not consider the error detection and fault tolerance 
of the authorization process. Considering the dynamic change of users’ relation-
ships, Zhang et al. [20] proposed a dynamic permission assignment scheme based on 
KP-ABE (Key-Policy Attribute-Based Encryption). Existing permission assignment 
schemes mainly focus on the authorization in the single-domain trust system but do 
not take the permission assignment in the multi-trust system into consideration. 
Moreover, the current permission assignment strategy is mainly based on a single 
control element, and does not consider the inﬂuence of other factors, such as the 
propagation path. 
2.1.3.2 
Automatic Permission Adjustment 
Currently, the automatic permission adjustment mainly includes two categories: 
risk-based permission adjustment and time-based permission adjustment. In the 
risk-based permission adjustment, we ﬁrst deﬁne the risk indicators faced by the 
data access according to user scenarios; then we compute the risk before and after the 
permission we adjustment. When the risk is less than a predetermined threshold, the 
access permission can be adjusted [21–24]. In the time-based permission adjustment 
scheme, the user’s data access permission is set in a certain time interval. During the 
execution process, the system automatically detects, revokes, or adjusts the assigned 
permission [25, 26] once the time exceeds the predeﬁned time interval. In addition, 
Yan et al. [27] integrated context-aware trust and reputation assessment into the 
encryption system and proposed a trust-based access control scheme, which supports

the dynamic adjustment of access permission with policy changes. To sum up, 
existing permission adjustment strategies are mainly based on factors such as risk 
and time and cannot achieve the adaptive permission adjustment with changes in 
access scenarios. 
20
F. Li et al.
2.1.3.3 
Permission Extended Control 
The current research mainly focuses on two aspects: Provenance-Based Access 
Control (PBAC) and sticky policy technology. PBAC uses the original data as the 
decision-making basis to protect the origin-sensitive resources, and the sticky policy 
technology uses cryptography technologies to bind the access control policy to the 
data, ensuring that the policy ﬂows with the data. In terms of PBAC, Sandhu’s group 
[28, 29] has made a series of pioneering work, providing a formalized policy 
speciﬁcation language and model, a dynamic separation of rights and responsibili-
ties, and an implementation framework. Regarding the sticky policy technology, 
Pearson et al. [30] and Spyra et al. [31] use an encryption mechanism to associate 
policies with data, and encode, and anonymize the policy attributes to prevent them 
from malicious use, which can support data access control throughout the life cycle. 
However, these works do not specify which elements to use for control decisions, do 
not consider audit information, and cannot trace the source of unauthorized data 
operations. 
2.1.4 
Trusted Computing 
Trusted computing is an active defense technology based on a hardware security 
mechanism. It ensures the security of sensitive operations on the computing platform 
by establishing a reliable computing environment for isolated execution, realizes the 
protection of trusted codes, and comprehensively enhances the system and network 
trust from the aspect of the system structure [32]. Academia and industry consider 
trusted computing as a technique to improve the security of computer systems by 
introducing Trusted Platform Module (TPM) to the hardware platform. At the same 
time, China has also proposed and established the Trusted Cryptographic Module 
(TCM) [33]. However, due to the constantly evolving requirements of information 
security applications, the TPM or TCM based trust chain schemes can no longer be 
adequate in real-world scenarios. Speciﬁcally, the trust chain transmission schemes 
suffer from security risks and cannot resist the time difference attack during the 
measurement process. Meanwhile, both CPU and memory are likely to be attacked. 
Therefore, various improvement methods have been proposed, such as providing a 
Dynamic Root of Trust for Measurement (DRTM), creating a dynamic chain of trust 
through special instructions in the operating system [34], enhancing the measuring 
capability of the TPM and TCM through motherboard transformation [35] and 
actively monitoring the integrity and working status of each component of the

platform [36] via the Trusted Platform Control Module (Trusted Platform Control 
Module, TPCM), etc. However, limited by the design and manufacturing capabilities 
of hardware, such as chips and motherboards, traditional hardware-based TPM and 
TCM enhancing schemes are not ideal solutions. 
2
Advances in Privacy Preservation Technologies
21
To address the issues in the design and application of TPM and TCM, the Trusted 
Execution Environment (TEE) came into being. By extending the security functions 
of CPUs and adding security features such as memory isolation, data code encryp-
tion, and integrity protection in its special security mode, TPM and TCM can 
actively monitor, measure, and intervene in host systems [37]. ARM’s TrustZone 
is a typical representative of TEE. It is designed to obtain control rights after the 
system is powered on, and to verify the subsequent loaded boot images step by step, 
providing higher access and control rights than the host. Therefore, it can create a 
computing platform with a running environment that is isolated from other hardware 
and software resources of the platform. Speciﬁcally, during the operation of 
TrustZone, the physical processor can switch between two modes, i.e., the normal 
state and the safe state. The normal state runs the host system, and the safe state runs 
the TEE system. The extended instruction of TrustZone, i.e., Secure Monitor Call 
(SMC), is responsible for switching these two modes. Subsequently, Raj et al. [38] 
proposed a ﬁrmware-based TPM scheme fTPM based on the TrustZone. The scheme 
adopts an Embedded Multimedia Memory Card (eMMC) as the persistent and 
secure storage facility to satisfy TrustZone’s functional requirements to TPM. 
Meanwhile, to accommodate TrustZone, fTPM limits the scale of cryptographic 
computations and modiﬁes parts of the semantics of the TPM. Dong et al. [39] 
proposed an active trusted TPM/TCM scheme based on TEE, which solves the 
independent trusted operation and active trusted security monitoring problems by 
using a sub-core asynchronous system architecture. Physical Unclonable Functions 
(PUF) based secure storage mechanism and Universally Unique Identiﬁer (UUID) 
based TEE secure communication mechanism solve the storage security and com-
munication security issues of trusted platform modules in the TEE environment. 
To sum up, the trusted computing only provides a trusted computing environment 
for privacy information processing. 
2.1.5 
Federated Learning 
Federated Learning (FL) [40] is a distributed machine learning architecture, which 
allows multiple participants to collaboratively train and improve a shared global 
machine learning model, achieving the goal of collective intelligence improvement 
and data sharing. Speciﬁcally, in the FL, multiple participants train and share local 
model parameters on their local devices, and then iteratively perform model aggre-
gation to build a global model. Compared with traditional centralized machine 
learning methods, FL has some inherent advantages. First, since FL distributes the 
training tasks to multiple devices, it can make full use of distributed computing 
power and reduce the model requester’s capital investment in computing equipment.

Second, the participants in the FL share local model parameters instead of raw data, 
which satisﬁes the industry’s requirements for data localization. 
22
F. Li et al.
However, although the training data in the FL never leave participants’ devices, 
the shared model parameters may still leak sensitive data information. A variety of 
inference attacks against model parameters have been proposed. For example, ZHU 
et al. proposed an attack method called Deep Gradient Leakage (DLG) [41], which 
can accurately recover the training data from local model parameters; Melis et al. 
demonstrated that the adversarial participant can infer whether there is a certain 
sample in others’ training data by analyzing the iterative global model parameters 
[42], i.e., Membership Inference Attack (MIA), etc. 
To solve the above issues, many privacy-preserving FL schemes have been 
proposed. Zhang et al. utilized Homomorphic Encryption (HE) to protect local 
model parameters and performed the model aggregation over ciphertexts based on 
the homomorphic properties [43]. In addition, they further optimized the efﬁciency 
of the FL system through parameter quantization, clipping, and encoding methods. 
Bonawitz et al. adopted Secure Multiparty Computation (MPC) as the underlying 
privacy-preserving technique [44]. They used a double masking method to ensure 
the conﬁdentiality of local model parameters, and the masking keys are secretly 
shared to tolerant failures during aggregation. The above HE-based and MPC-based 
privacy-preserving FL schemes can achieve model aggregation while protecting 
local model parameters, but their communication or computational overhead are 
high, and they cannot provide protection for global model parameters. Truex et al. 
propose a scheme called LDP-Fed, through adding random noises with differential 
privacy (DP) to local model parameters [45], which can provide a security guarantee 
for the iterative training process of the FL. The DP-based privacy-preserving FL 
scheme can provide a formal proof of privacy, but the added noise will inevitably 
reduce the accuracy of the model. 
In summary, the essence of the FL is data localization and computing power 
distribution. Beneﬁting from these advantages, the FL can reduce the risks of data 
leakage. However, the FL still lacks protection mechanisms for the exchanged model 
parameters and still has some privacy leakage problems. FL is not a privacy 
computing technology, but an artiﬁcial intelligence technology that does not solve 
the privacy leakage problem by itself. Theories and techniques of privacy computing 
need to be applied to solve the existed problem of privacy leakage. 
2.2 
Privacy Desensitization Technology 
In the ubiquitous interconnected environment, data owners (data subjects), data 
controllers, and data processors are separated. To implement effective privacy 
preservation, when exchanging privacy information between different information 
systems or managers, desensitization is the best method. 
The privacy desensitization technology modiﬁes the real original data through a 
speciﬁc strategy, making the data be partially distorted. Attackers cannot obtain the

real information through the published data, thereby achieving the effectiveness of 
privacy preservation. Currently, privacy desensitization technologies are mainly 
divided into three categories: anonymity, differential privacy, and information 
theory based privacy desensitization. 
2
Advances in Privacy Preservation Technologies
23
2.2.1 
Anonymity-Based Privacy Desensitization 
Anonymity-based privacy desensitization mainly refers to anonymization tech-
niques, such as k-anonymity [46, 47], l-diversity [48, 49], and t-closeness 
[50, 51]. The protection of privacy information is realized by applying the general-
ization, suppression, replacement, and other strategies to the original data, where l-
diversity and t-closeness are derived technologies of k-anonymity. They improve the 
anonymity effect by diversifying the types of sensitive attributes and ensuring their 
distribution in each anonymity equivalence class maintains an adjacent relationship 
with the distribution of overall records, thereby protecting user’s privacy informa-
tion better. With the development of mobile internet and cloud computing technol-
ogy, Location-Based Service (LBS) has become the most popular application. Users 
can report their location and query the service content of interest around their 
location. The LBS involves user’s location privacy, hobbies privacy, etc. In the 
following, we will introduce the research advances of anonymous technology 
applications in combination with privacy preservation in LBSs. 
2.2.1.1 
Generalization and Suppression Based Trajectory 
Privacy-Preserving Methods 
The basic idea of generalization is to generalize all the sampling points on the 
trajectory into the corresponding anonymous regions so that attackers cannot obtain 
the accurate position. The most effective method is trajectory k-anonymity. In 2020, 
Tojiboev et al. [52] proposed an anonymous trajectory database publishing model by 
adding noise trajectories in a vector grid environment. This scheme dedicates to 
improving the utility of trajectory data. The advantage is that it not only ensures that 
the anonymous database has higher data utilization, but also has a low computational 
complexity. In 2019, Wang et al. [53] proposed a Location Reorganization 
Mechanism (LRM) to achieve trajectory k-anonymity privacy preservation, which 
introduces probabilistic similarity and geographic similarity to synthesize pseudo-
trajectories satisfying the privacy-preserving requirements of basic and sampled 
trajectories. The mechanism achieves the goal of effectively protecting the privacy 
of both basic and sampled trajectories. 
The basic idea of suppression is to eliminate or remove users frequently access 
locations or some sensitive locations in existing trajectories before the ofﬁcial 
release of trajectory data. In 2020, Naghizade et al. [54] proposed an algorithm 
that adaptively preserves trajectory semantics through spatiotemporal perturbations

according to users’ privacy preferences. The algorithm either replaces sensitive 
stopping points in trajectories with movements from the same trajectory or intro-
duces a minimum detour on the same path when no safe Points of Interest (POI) can 
be found. The advantage of this algorithm is that it maximizes the preservation of the 
original trajectory while also protecting sensitive stopping points and preventing 
inferences about privacy by adversaries. In 2019, Chen et al. [55] proposed a 
trajectory privacy-preserving method based on 3D meshing, which performs posi-
tions exchange between trajectories or deletes few positions of sub-trajectories 
instead of the entire trajectory that do not meet the conditions within each 3D unit. 
It effectively reduces the information loss in the process of trajectory anonymization. 
In 2018, Dai et al. [56] proposed an effective personalized trajectory privacy-
preserving method, which improves the semantic consistency and shapes the simi-
larity of trajectory data by reasonably replacing the trajectory reconstruction after 
sensitive stopping points. It achieves privacy preservation of trajectory data and 
strikes a balance between user-deﬁned privacy requirements and data utility in 
ofﬂine trajectory publishing scenarios. 
24
F. Li et al.
Generalization and suppression of location information make it difﬁcult for 
attackers to identify the precise location of users. When users continuously release 
their perturbed locations, existing anonymity schemes may fail to protect users’ 
sensitive spatiotemporal events, such as “visited the hospital last week” or “commute 
between location 1 and location 2 regularly every morning and afternoon” (It is easy 
to deduce that location 1 and location 2 could be home and ofﬁce). Cao et al. [57] 
demonstrated the accuracy that an attacker can infer sensitive spatiotemporal events 
from a series of such locations or even protected locations, and even some state-of-
the-art protection mechanisms cannot fully preserve the privacy of spatiotemporal 
events. 
2.2.1.2 
Dummy Data Based Location and Trajectory 
Privacy-Preserving Methods 
In dummy location-based methods, each user’s query will be submitted along with a 
set of pre-constructed dummy queries, making it difﬁcult for an untrusted server to 
identify the user’s real location. The security of dummy location-based methods 
mainly depends on the quality of the constructed dummy locations, as it is often 
threatened by feature distribution-based attacks, resulting in weak security. 
CoPrivacy [58] forms an anonymous group through collaboration between users. 
Users within an anonymous group use the group’s anonymity center to issue queries 
instead of real locations and incrementally obtain neighbor query results from the 
server. Members in the group obtain the precise query result by calculating the 
distance between the neighbor query result and their positions. Furthermore, k-
anonymity is often used in conjunction with dummy location-based methods to 
measure the effectiveness of privacy preservation. However, k-anonymity is basi-
cally a privacy measure, and its effectiveness also depends on the quality of the 
constructed pseudonyms or dummy locations. Nevertheless, existing dummy

location generation algorithms only consider limited background information that is 
known to the adversary, which may face more severe challenges in practice. Shaham 
et al. [59] combined the new background information based on the continuous 
location changes of users and proposed a new privacy metric, i.e., transition entropy, 
to study location privacy preservation. Then, they proposed two algorithms to 
improve the transition entropy of false location generating algorithms. Most of the 
existing privacy-preserving schemes pay little attention to the semantic information 
of the location. However, semantics may reveal sensitive information of mobile 
users. To address this issue, Li et al. [60] proposed a new framework, i.e., PrivSem, 
which integrates location k-anonymity, segmental semantic diversity, and differen-
tial privacy to protect users’ location privacy from infringement. Gedik et al. [61] 
proposed a personalized k-anonymity privacy-preserving scheme. Users can set the 
parameters in the privacy-preserving algorithm according to the minimum degree of 
anonymity and maximum spatiotemporal correlation they can tolerate. Another 
personalized k-anonymity privacy-preserving scheme in [62], where the anonymity 
center is located in a trusted server, the privacy-preserving requirements are set 
according to the privacy proﬁle of each user, and the system overhead is reduced by 
using the data structure of a non-complete pyramid structure. 
2
Advances in Privacy Preservation Technologies
25
Privacy-preserving location-based services can sometimes compromise the accu-
racy of location-based services, since each query request submitted to the server is 
modiﬁed. Moreover, anonymous methods usually require a trusted third party, 
which is likely to suffer from performance and privacy bottlenecks. 
For trajectory privacy preservation, one or more dummy trajectories can be 
constructed based on some information of users’ real trajectories to reduce the risk 
of users’ privacy leakage. In 2020, Zhao et al. [63] proposed a privacy-preserving 
trajectory publishing method by generating secure starting and ending points. This 
method can use a bidirectional virtual trajectory generation algorithm to generate 
k - 1 paths that cannot be detected by attackers with some background knowledge. 
The advantage of this method is that the generation of candidate secure starting 
and ending points does not rely on a trusted third party but rather on a large amount 
of users’ personal data, so that the generated anonymous trajectories can maintain 
high trajectory similarity while preserving trajectory privacy. Tu et al. [64] combined 
k-anonymity technology with l-diversity and t-closeness technologies to design a 
privacy-preserving algorithm, which can resist the attackers’ semantic attack and the 
identity re-authentication attack in the user’s continuous trajectory, providing high-
strength privacy preservation. 
2.2.1.3 
Query Content Privacy Preservation 
If all queries in the user anonymity set have the same service attribute value, the 
privacy of the query content will be leaked. To this end, Xiao et al. [65] proposed to 
apply the well-known l-diversity principle in private data. Speciﬁcally, the hidden 
area used to send the request must include multiple queries with different service 
attribute values, which are classiﬁed as sensitive and insensitive. In a continuous

query scenario, in addition to requiring consistency in the anonymity set that is used 
repeatedly, it is also necessary to maintain the same set of service properties. 
Therefore, Dewri et al. [66] utilized the principle of m-invariance to generate hidden 
regions. Although the method is effective, it relies on a trusted third party. In 
addition, this method results in larger hidden areas and requires more anonymization 
time. Sei et al. [67] combined l-diversity and t-closeness techniques to propose a 
model (l1,. . .,lq)-diversity and a model (t1,. . .,tq)-closeness, respectively. It can 
achieve the multi-level division of user sensitive attributes from the perspective of 
data owners and users, and achieves the privacy preservation of users’ sensitive 
information through anonymous algorithms. 
26
F. Li et al.
Anonymity-based privacy-preserving technology suffers from correlation analy-
sis attacks initiated by attackers with background knowledge. At the same time, it is 
an NP hard problem to achieve perfect anonymity for large-scale datasets, and it is 
accompanied by large data distortion, which seriously reduces the utility of data. 
2.2.2 
Desensitization via Differential Privacy 
2.2.2.1 
Centralized Differential Privacy and Local Differential Privacy 
Anonymity-based privacy desensitization cannot provide semantic security for 
users’ privacy information. Dwork [68] proposed a privacy-preserving technology 
that does not depend on the background knowledge of attackers, namely differential 
privacy technology. Compared with traditional cryptography, differential privacy 
has the characteristics of low cost, using simple algorithm, and having the capability 
to provide semantic security for users’ privacy information. Therefore, differential 
privacy technology has quickly become a research hotspot in the ﬁeld of privacy 
preservation, attracting the attention of many researchers. Research on differential 
privacy currently focuses on two directions: centralized data models and localized 
data models. The centralized data model is that the controller collects data and 
performs uniﬁed differential privacy preservation processing on the data, including 
interactive and non-interactive differential privacy. Speciﬁcally, interactive differ-
ential privacy means that data analysts query the statistical data of differential 
privacy protection provided by the data controller through the differential privacy 
Application Program Interface (API) provided by the data controller; non-interactive 
differential privacy refers to the scenario where the controller does not know the data 
analysis task in advance, the differential privacy algorithm is used to perturb the 
overall dataset, and the differential private dataset is directly released for data 
analysts to use. The localized data model means that the user executes the differential 
privacy algorithm, and the data is transmitted to the data controller after the client 
completes the differential privacy data protection. The reason why local differential 
privacy technology is widely accepted by industry and academia is that it does not 
rely on a trusted third-party data controller. The collection of users’ data only 
involves the noised version of the data, and the original real data is completely

protected locally. It not only solves the problem that users cannot independently 
control personal privacy data but also reduces the privacy leakage risk of a large 
amount of privacy data stored in untrusted third parties. 
2
Advances in Privacy Preservation Technologies
27
Local differential privacy was proposed by Duchi et al. [69] and is currently used 
by both Apple and Google’s data collection algorithms. Differential privacy algo-
rithms are deployed in Apple’s iOS 10 system and MacOS to collect protected users’ 
input data to analyze their habits [70]; Google deploys the RAPPOR algorithm in the 
Chrome [71] to collect privacy-preserving users’ data. To improve the utility of data, 
Dwork et al. [72] proposed Centralized Differential Privacy (CDP) based on 
ε-differential privacy and (ε, δ)-differential privacy. Under the same privacy budget, 
CDP provides higher data utility and can support multiple queries without revealing 
users’ privacy information. Soria-Comas et al. [73] proposed personalized differen-
tial privacy, which effectively reduces the noise required to protect users’ privacy 
information and improves data utility. To address the data correlation problem in the 
traditional deﬁnition of differential privacy, Yang et al. [74] proposed Bayesian 
differential privacy, which describes correlated data through a Bayesian network and 
can 
solve 
the 
over 
noisy 
problem 
of 
traditional 
differential 
privacy 
on 
correlated data. 
2.2.2.2 
Applications of Differential Privacy in Machine Learning 
The perturbation-based differential privacy-preserving methods have low computa-
tional complexity, improve the application efﬁciency in the ﬁelds of big data mining 
and machine learning, and provide more explicit privacy guarantees and more 
standardized mathematical proofs. He et al. [75] directly added Gaussian noise to 
the original data; Osia et al. [76] added noise to the features extracted by the neural 
network. In addition, perturbation or masking can also be applied only for sensitive 
features that are speciﬁed by the user or automatically detected by the recognition 
network [77, 78]. Shokri et al. [79] ﬁrst designed a distributed privacy-preserving 
learning method using differential privacy mechanism. In their method, the privacy 
loss is calculated according to the parameters of the model. Since there are many 
model parameters, there may exist a signiﬁcant privacy loss. Based on this work, 
Abadi et al. [80] introduced a more efﬁcient gradient descent algorithm based on 
differential privacy, which has a smaller privacy budget and demonstrates better 
performance than Shokri et al.’s algorithm [79]. In addition, they introduced a 
privacy loss recording method, called Moment Accountant, to automate the calcu-
lation of privacy loss. 
Wei et al. [81] proposed a federated learning framework based on the concept of 
differential privacy, adding noise that satisﬁes differential privacy to the parameters 
of the client before model aggregation, and providing the theoretical convergence 
bound of the model loss function. The simple application of secure multi-party 
computation is vulnerable to privacy inference attacks, and the simple application 
of differential privacy will lead to low accuracy when the amount of data on each 
client is relatively small. Therefore, Truex et al. [82] proposed an alternative

approach that combines differential privacy with secure multi-party computation to 
balance privacy and accuracy by reducing noise injection as the number of federated 
learning parties increases. 
28
F. Li et al.
With the rapid development of artiﬁcial intelligence, machine learning has 
received extensive attention. When the training dataset contains personal sensitive 
information, the model parameters may encode personal information and thus 
increases the risk of privacy leakage. In recent years, such privacy risks have been 
further exacerbated by the trend to share and publish pre-trained models. As a 
standard privacy notion, differential privacy has been applied in various machine 
learning algorithms, such as logistic regression [83], support vector machines [84], 
and risk minimization [85], aiming to limit the training data leakage risks caused by 
the published models. Based on DPSGD, Yu et al. [86] proposed a differential 
privacy-based method to train neural networks. The proposed method uses CDP to 
perform formal and ﬁne-grained privacy loss analysis on two different data batching 
methods. Meanwhile, it adopts a dynamic privacy budget allocation method to 
further improve the accuracy of the model. Nasr et al. [87] pointed out that the 
existing methods of adding differential noise to the gradient to achieve differential 
privacy will lead to a great decrease in the accuracy of the training model. To solve 
this problem, the key technique used by Nasr et al. is to encode the gradients and 
map them into a smaller vector space to obtain differential guarantees for different 
noise distributions, thereby further realizing the selection of the noise distribution 
that can best guarantee model accuracy in the target privacy budget. In addition, Nasr 
et al. also utilized the post-processing feature of differential privacy for noise 
reduction, which further improves the accuracy of the training model. 
The goal of using differential privacy for machine learning is often to limit what 
an attacker can infer about an individual’s training data from a model. For better 
model usability, privacy-preserving machine learning implementations often choose 
larger privacy budgets, but the impact of these choices is poorly understood. 
Jayaraman et al. [88] quantiﬁed the impact of these choices on privacy through 
logistic regression and neural network models, and found that loose privacy budget 
reduces the amount of noise required to improve usability, while also increasing the 
risk of privacy leakages. Compared to general machine learning models and deep 
learning networks, the generative adversarial network uses a more complex network 
structure. As a result, it can easily remember the used training data. If the generative 
adversarial network is applied to personal sensitive data, it may reveal some sensi-
tive information. To address this problem, Xu et al. [89] proposed a differential 
privacy based GANobfuscator, which generates generative adversarial networks 
with differential privacy by adding carefully designed noise to the gradients during 
the learning process. With GANobfuscator, analysts can generate unlimited syn-
thetic data for any analysis task without exposing the privacy of the training data. 
Furthermore, Papernot et al. [90] observed that the choice of activation function is 
crucial for limiting the sensitivity of privacy-preserving deep learning and demon-
strated how a general family of bounded activation functions (tempered sigmoid) 
consistently outperforms unbounded activations functions (such as ReLU) from the 
aspects of theoretical and experimental analysis. Therefore, in addition to adding

differential noise in the training process, the privacy preservation in machine 
learning can be achieved by adopting certain model architectures that are conducive 
to protecting user privacy. 
2
Advances in Privacy Preservation Technologies
29
2.2.2.3 
Differential Privacy Application in Privacy-Preserving Data 
Publishing 
The Laplacian mechanism proposed by Dwork et al. [91] was the ﬁrst mechanism to 
achieve interactive data publishing, where Laplacian noise is added to each query 
result, but the number of queries is limited. Subsequently, Roth et al. [92] proposed a 
median mechanism that can respond to more queries, which divides queries into 
“hard query” and “easy query”. The result of “hard query” is obtained using 
independent Laplacian mechanism, while the result of an “easy query” is obtained 
from the median of the previous query answers using a Stochastic mechanism. “Easy 
query” does not consume the privacy budget, so more queries can be performed. 
Hardt et al. [93] proposed another mechanism that can increase the number of 
queries—Private Multiplicative Weight (PMW). Each query operation compares 
the query result with the result of the previous query, which has Laplacian noise 
added to it. Only when the difference is greater than the threshold, the query result 
with added noise will be published. Otherwise, the current query will be directly 
responded with the previous query result. Gupta et al. [94] proposed a general 
iterative dataset generation architecture (IDC) by using a similar idea to PMW. 
The difference is that IDC directly makes initial assumptions on the dataset rather 
than on the dataset distribution, so IDC can respond to more types of queries. 
Fan et al. [95] designed a data stream publishing algorithm FAST based on 
ﬁltering and adaptive sampling. The algorithm divides each timestamp into sampling 
points and non-sampling points. The sampling point means that the data under the 
timestamp is added to the Laplace noise, and Kalman ﬁltering is used to predict the 
data under this timestamp for non-sampled points. Kellaris et al. [96] designed an 
unlimited number of streaming data publishing algorithm, which adopts the idea of a 
sliding window and proposes two algorithms of budget allocation and budget 
recovery. The interactive data publishing algorithms can improve the number of 
queries and the utility of query data but do not provide a reasonable privacy 
budget allocation scheme. Wang et al. [97] studied the issue of real-time data 
publishing and designed a privacy budget adaptive allocation scheme. The privacy 
budget is allocated according to the changing trend of the data. When the data 
changes rapidly, only a small portion of the remaining budget is allocated to the 
next sampling point. At the same time, to protect the privacy of correlated data in big 
data, Lv et al. [98] proposed k-CRDP and r-CBDP models. First, r-CBDP uses MIC 
and machine learning to determine dependencies between data, accurately calculate 
association sensitivities, then divide big data into independent chunks, and imple-
ment k-CRDP on chunks to achieve big data-dependent differential privacy. It 
should be noted that traditional differential privacy has a serious limitation of 
providing the same level of privacy preservation for all users and has privacy

ﬂaws when dealing with duplicate query attacks. Li et al. [99] proposed a person-
alized differential privacy-preserving method for repeated queries. They generate 
new privacy-preserving instructions based on data privacy preservation require-
ments, query user permissions, and the number of times the same query is executed. 
30
F. Li et al.
The non-interactive data publishing technology mainly uses sampling histogram 
publishing technology. If the Laplacian mechanism is applied to the publication of 
histograms, it will result in low data utility. To this end, Xiao et al. [100] proposed 
the Privelet algorithm, which uses wavelet transform on the original data. Li et al. 
[101] proposed a matrix mechanism to reduce the error when Laplacian noise is 
directly added to the original data. However, these schemes do not consider the 
correlation of data. Hence, Xu et al. [102] proposed NoiseFirst and StructureFirst 
algorithms. NoiseFirst ﬁrst adds Laplace noise to the original histograms and then 
uses V-optimized histogram to group noise to the histogram. StructureFirst divides 
the privacy budget into two parts, one for picking the boundaries of the binned 
histogram buckets and grouping the histograms, and the other for adding Laplacian 
noise to each grouping. However, these two algorithms need to give the number of 
groups in advance, only consider the reconstruction error introduced by the group-
ing, and ignore the noise error introduced by the Laplacian mechanism. Acs et al. 
[103] proposed the P-HPartition algorithm to address these shortcomings, which 
balances the reconstruction error and noise error through a greedy bisection strategy 
and can adaptively ﬁnd the optimal number of binning buckets. However, the 
algorithm only considers buckets pairs that are close to each other to merge without 
considering the global neighbor relationship. Zhang et al. [104] proposed the AHP 
algorithm based on P-HPartition. The algorithm ﬁrst sorts the histograms and then 
uses the idea of greedy clustering combined with reconstruction error and noise error 
to group the histograms adaptively. Ma et al. [105] proposed a differential privacy-
based privacy-preserving mechanism RPTR, which is used to protect the real-time 
trajectory data release of vehicles. While meeting the application load and practi-
cality requirements, RPTR constructs a privacy budget allocation method based on 
regional privacy weights to provide better protection for areas with higher user 
densities. Li et al. [106] proposed a real-time privacy protection method based on 
the differential privacy model of data perturbation. This method uses the dynamic 
budget allocation method to allocate the privacy budgets reasonably and process 
public data through approximation or perturbation. Its advantage is that it effectively 
reduces the error caused by changes in statistical data in the same location. 
A large number of differential privacy-based privacy-preserving schemes have 
been proposed to address the problem of privacy leakage in data statistics and 
analysis. In these schemes, the problem of balancing data utility and privacy 
preservation has not been effectively solved. At the same time, existing solutions 
face the challenge of constructing a model to evaluate the effectiveness of differen-
tial privacy preservation.

2
Advances in Privacy Preservation Technologies
31
2.2.3 
Information Theory Based Privacy Desensitization 
Wu et al. [107] proposed a differential privacy model consistent with mutual 
information and introduced a new concept of differential privacy preservation by 
characterizing the threat of correlated data to the privacy. The differential privacy 
model will be a special case of this model when the data are independent. When the 
data are correlated, a group privacy-preserving scheme is used to achieve the 
differential privacy in the dependent situation, thereby realizing a new privacy 
model. Peng et al. [108] described the privacy-preserving system as a communica-
tion model, in which the attacker has no background knowledge about the privacy 
information in the data source and can only disclose the privacy information by 
analyzing the observed data in the data sink. Information entropy is used to describe 
the average amount of privacy information in the data sink, and the uncertainty of 
privacy information directly reﬂects the risk of privacy information leakage. 
2.2.4 
Privacy Measurement and Evaluation 
Privacy measurement and evaluation is an important basis for evaluating and 
optimizing privacy-preserving algorithms. It can be used as a privacy protection 
strength indicator for privacy-preserving algorithms and can also provide quantita-
tive indicators for reducing and controlling privacy leakage risks. 
In the privacy measurement related theoretical research, since information theory 
is an effective tool to measure the amount of information, mutual information and 
entropy have been widely used. Wu et al. [109] analyzed the uncontrollability of 
information distortion in k-anonymity, used information entropy and matrix to 
deﬁne the accurate response probability in user records in a crowdsourcing database, 
and provided estimates for the upper and lower bounds of the protection accuracy in 
the k-anonymity technology. Cuff et al. [110] used mutual information to describe 
the amount of information that an attacker obtains from the observed data. When the 
value of mutual information is less than or equal to the differential privacy budget, it 
indicates that the privacy-preserving mechanism satisﬁes differential privacy, and 
the risk of privacy information leakage is relatively low. Otherwise, the risk of 
privacy information leakage is high. Asoodeh et al. [111] used mutual information to 
describe the risk of privacy leakage. They assumed that the privacy information in 
the original cloud data D is S. After the attacker obtains the data D′ associated with 
the privacy information, the original cloud data is processed by privacy-preserving 
technology. Mutual information is used to calculate the amount of uncertainty 
reduction in the original cloud data about the privacy information S in the original 
cloud data after the attacker knows the information D′ to reﬂect the risk of privacy 
leakage. The greater the amount of uncertainty reduction, the greater the risk of 
privacy information leakage. Oya et al. [112] proposed to use conditional entropy 
and mutual information as complementary privacy metrics and used the

Blahut-Arimoto algorithm to design an approximate optimal location protection 
mechanism. Ma et al. [113] proposed a privacy metric for time series data that 
quantiﬁes the amount of information available to an adversary attempting to extrap-
olate raw data, given the range of any published data. Ding et al. [114] measured the 
privacy effect of differential privacy algorithm by generating a large number of 
counterexamples. Bichsel et al. [115] used a feedback-based sampling method to 
measure the lower limit of the input-output distance, given the privacy effect. This 
method can be used to detect insufﬁcient interference. 
32
F. Li et al.
There are also many practical applications of privacy information measurement 
and evaluation in social networks, location services, cloud computing, and other 
real-world scenarios. In the ﬁeld of social networks, Gervais et al. [116] proposed a 
privacy quantiﬁcation method covering both query level and semantic level for web 
search services; Cao et al. [117] quantiﬁed the potential privacy leakage risk under 
differential privacy technology. In the ﬁeld of location-based services, Shokri et al. 
[118] used the determination of the attack model and the background knowledge of 
the adversary to describe the accuracy, certainty, and correctness of the attack 
process through methods such as information entropy, which could measure the 
effect of privacy preservation. Based on game theory and differential privacy, Wu 
et al. [119] carried out multi-level quantiﬁcation of users involved game elements 
and realized user privacy measurement by analyzing a single dataset. Zhang et al. 
[120] used the concept of difference to quantify the privacy level of participating 
users, thereby realizing an accurate incentive mechanism. 
2.3 
Privacy-Preserving Adversarial Analysis 
Users can perform certain anonymization processing before publishing data to 
protect their privacy, but anonymous data still faces the risk of user privacy leakage 
caused by attacks. Therefore, it is necessary to analyze the effect of privacy preser-
vation and the quality of the anonymization to ﬁnd an effective anonymization 
technique, which relies on the exploration of deanonymization attack methods. At 
present, existing research on deanonymization methods and privacy-preserving 
analysis techniques can be divided into theoretical knowledge research and practical 
scenario analysis. 
In terms of theoretical knowledge research, if researchers match anonymous data 
and users’ data in the deanonymization attack method into seed pairs, the privacy-
preserving 
analysis 
techniques 
can 
be 
generalized 
as 
diffusion-based 
deanonymization methods with seed pairs, and feature information extraction and 
judgment methods without seed pairs [121]. Narayanan et al. [122] proposed a 
privacy analysis framework that relies on known seed pairs and proposed a 
de-anonymization algorithm based on network topology. The algorithm has good 
robustness to perturbation and added noise but has high requirement to the quality of 
seed pairs. Yartseva et al. [123] proposed a seed pair graph-matching method 
according to the percolation theory of graphs. Based on this method, Kazemi et al.

[124] further relaxed the requirements for seeds and reduced the number of seeds to 
be used. Backstorm et al. [125] proposed a de-anonymization method using a 
continuous attack method. This method can determine whether there is an edge 
between two speciﬁc points. Although this algorithm has better attack effects for 
anonymization algorithms that only exchange node numbers, it does not work for the 
anonymization algorithm that modiﬁes the graph structure. 
2
Advances in Privacy Preservation Technologies
33
Currently, privacy-preserving technologies have been widely used in scenarios, 
such as big data, social networking, and location-based services. It is also a current 
research hotspot to analyze the privacy-preserving technology in these scenarios and 
de-anonymize the anonymous data. In the big data environment, differential privacy 
can meet the needs of secure processing and publishing of large-scale numerical 
data, but Wang et al. [126] proposed an automatic detection scheme CheckDP that 
can verify whether a mechanism satisﬁes differential privacy. The mechanism 
automatically generates proofs for mechanisms that satisfy differential privacy, 
while generating counter-examples for mechanisms that do not. Bichsel et al. 
[115] also proposed an effective and accurate correlation sampling method 
DP-Finder, which automatically calculates the differential privacy lower bound of 
random algorithms and then obtains the unsatisfactory counter-example of privacy 
leakage for differential privacy mechanisms to evaluate differential privacy methods. 
In social networks, Crandall et al. [127] found that users who often appear in the 
same geographical location at the same time have strong social connections and used 
this conclusion to mine the user’s social structure to recommend friends to users; 
Davis et al. [128] inferred the geographic location of other users’ blog posts by using 
a majority vote method on locatable users in the user’s followers in social media. In 
location-based services, there have been many solutions to protect user location 
privacy in academia, but Ma et al. [129] argued that attackers may infer the complete 
historical trajectory of the attack target from the anonymous trajectory set; Zang et al. 
[130] analyzed the relationship between the generalization degree of anonymous 
location and the leakage of user privacy information from large-scale mobile com-
munication data, especially the impact of social networks on reducing the anony-
mous set and increasing the privacy leakage risk; Wang et al. [131] proposed a new 
trajectory privacy metric method, which describes user movement trajectories with 
weighted undirected graphs and calculates user trajectory privacy level from an 
information entropy perspective; Chang et al. [132] proposed a de-anonymization 
attack paradigm based on target movement patterns, which can accurately re-identify 
user trajectories from a set of anonymous trajectories.

34
F. Li et al.
2.4 
Privacy Computing 
2.4.1 
Weaknesses of Privacy-Preserving Technologies 
2.4.1.1 
Unable to Address Privacy Preservation for Cross-System 
Information Exchange 
Privacy protection technology is based on traditional data security technologies, 
such as encryption, decryption, and access control, and mainly aims at protecting the 
conﬁdentiality of information in a single information system and management 
domain. However, there are differences in key management mechanisms, access 
control policies, and data security protection capabilities among different manage-
ment domains. Meanwhile, the cask principle determines that privacy protection 
technology cannot fundamentally solve the privacy preservation problem in infor-
mation exchange across-information systems and across-administrative domain. 
Privacy desensitization technology protects privacy information by anonymizing 
and scrambling data. Once desensitized, privacy information is irreversible, which 
can ensure the basic protection of privacy information after cross-system exchange. 
However, in the process of cross-system exchange, involved systems are different 
and have various privacy requirements. Without an extended control mechanism, it 
is still unable to meet the privacy-preserving requirements of extensive information 
sharing in the ubiquitous interconnected environment. 
2.4.1.2 
Fragmented Privacy-Preserving Scheme 
Although privacy desensitization technology proposes privacy-preserving models, 
such as k-anonymity, l-diversity, t-closeness, and differential privacy, and has been 
applied in various application scenarios, such as location and trajectory privacy 
preservation, data release and statistical analysis, machine learning, and social 
networks, these schemes are only applicable to speciﬁc scenarios and do not start 
from the perspective of computation, neither consider that privacy is closely related 
to the subjective characteristics of time, space, type of privacy information, and the 
subject of privacy information. Therefore, a general formal deﬁnition of privacy 
information and a dynamic quantitative measurement method have not been 
established. These are the foundation and prerequisite for designing and combining 
privacy information protection algorithms, evaluating protection effectiveness, and 
implementing protection systems to achieve full-lifecycle privacy information pro-
tection. As a result, although there are thousands of papers on privacy preservation, 
the lack of a theoretical system still prevents the construction of a ubiquitous, 
dynamic, and on-demand privacy protection information system in a pervasive 
interconnected environment. This is the fundamental reason why the privacy-
preserving problem has not been solved in a ubiquitous interconnected environment.

2
Advances in Privacy Preservation Technologies
35
2.4.2 
The Inevitability of Privacy Computing 
Due to the urgent requirement of “intelligent interconnection of everything and 
ubiquitous sharing of information” for on-demand, universal privacy preservation 
in the full life cycle, it has become inevitable for the development of privacy-
preserving technology to elevate privacy preservation to a theoretical system from 
the perspective of computing. In 2015, Li Fenghua et al. emphasized in an internal 
academic seminar that privacy preservation is a requirement and privacy computing 
can represent a theoretical system. In [133], they ﬁrst gave the deﬁnition of privacy 
computing. In [134], they further formally deﬁned the privacy information as a 
six-tuple consisting of privacy information vectors, privacy attribute vectors, gener-
alized positioning information, audit control information, constraints, and propaga-
tion control operations. They also provided the privacy computing framework, 
formal deﬁnition of privacy computing, the important characteristics of privacy 
computing, algorithm design criteria, privacy protection effect evaluation, privacy 
computing language, and described the ubiquitous application of privacy computing 
with application scenarios, such as image and location privacy protection. Privacy 
computing establishes a theoretical system framework for privacy preservation and 
is an important theoretical basis for privacy information protection in ubiquitous 
cyberspace. The relevant content will be introduced in detail in Chap. 3 of this book. 
2.5 
Chapter Summary 
In the information era, the evolution and development of information technology and 
the application of inclusive technology must also be a process of gathering and 
leveraging big data. While people fully enjoy the convenience brought by informa-
tion technology to their lives, privacy information leakage and abuse can be seen 
everywhere. For this reason, traditional security technologies, such as cryptography, 
data security, and access control, have naturally become early privacy protection 
technologies. These technologies can protect privacy information to a limited extent 
in a single management domain or single information system. However, under the 
ubiquitous interconnection, “everything is intelligently interconnected and informa-
tion is ubiquitously shared”, and privacy information frequently interacts across 
borders, systems, and ecosystems in a highly dynamic and large-scale spatiotempo-
ral environment. On the premise of satisfying basic usability, the sharing of 
desensitized personal information is the core technology for implementing privacy 
protection, which is classiﬁed as privacy desensitization technology. 
This chapter systematically analyzes the evolution of privacy protection related 
technologies. The current state of research on privacy preservation is outlined with 
emphasis on privacy protection, privacy desensitization, privacy-preserving adver-
sarial analysis, and privacy computing Although the authors were the ﬁrst to propose 
the concept and theoretical system of privacy computing at home and aboard, and

continued to promote the research of privacy computing, however, in the face of 
endless privacy preservation needs and their technical challenges, current privacy 
preservation technologies still cannot fully address the balance between privacy 
preservation and personalized services through excessive personal data collection 
and individual proﬁling. It is necessary for domestic and foreign scholars to work 
together to carry out in-depth academic research and application promotion of 
privacy computing, so as to develop a comprehensive set of theoretical foundations 
for the full lifecycle protection of privacy information in the ubiquitous internet, as 
well as a computing architecture that can be easily implemented. The proposed 
privacy computing theory and technology will be elaborated in subsequent chapters. 
36
F. Li et al.
References 
1. Rivest, R.L., Adleman, L., Dertouzos, M.L.: On data banks and privacy homomorphisms. 
Found Secure Comput. 4(11), 169–180 (1978) 
2. Paillier, P.: Public-Key Cryptosystems Based on Composite Degree Residuosity Classes. In: 
EUROCRYPT’99, pp. 223–238. Springer, Berlin (1999) 
3. Lu, R.X., Liang, X.H., Li, X.D., et al.: EPPA: an efﬁcient and privacy-preserving aggregation 
scheme for secure smart grid communications. IEEE Trans Parallel Distrib Syst. 23(9), 
1621–1631 (2012) 
4. Samanthula, B.K., Elmehdwi, Y., Jiang, W.: K-nearest neighbor classiﬁcation over semanti-
cally secure encrypted relational data. IEEE Trans Knowl Data Eng. 27(5), 1261–1273 (2015) 
5. Gentry, C.: A Fully Homomorphic Encryption Scheme. Stanford University, Stanford (2009) 
6. Dijk, M.V., Gentry, C., Halevi, S., et al.: Fully homomorphic encryption over the integers. In: 
Annual International Conference on the theory and applications of cryptographic techniques, 
pp. 24–43. Springer, Berlin (2010) 
7. Bost, R., Popa, R.A., Tu, S., et al.: Machine learning classiﬁcation over encrypted data. In: 
Network and distributed system security symposium, pp. 4324–4325. Cryptology ePrint 
Archive (2015) 
8. Rao, V., Rao, P.: Improving vocal melody extraction in the presence of pitched accompani-
ment in polyphonic music. IEEE Trans. Audio Speech Lang. Process. 18(8), 2145–2154 
(2010) 
9. Soﬁanos, S., Ariyaeeinia, A., Polfreman, P.: Towards effective singing voice extraction from 
stereophonic recordings. In: Processing of IEEE International Conference on acoustics speech 
and signal processing, pp. 233–236. IEEE Press, Piscataway (2010) 
10. Damgard, I., Ishai, Y., Kroigaard, M., et al.: Scalable multiparty computation with nearly 
optimal work and resilience. In: Annual International Cryptology Conference, pp. 241–261. 
Springer, Berlin (2008) 
11. Pettai, M., Laud, P.: Automatic proofs of privacy of secure multi-party computation protocols 
against active adversaries. In: 2015 IEEE 28th Computer Security Foundations Symposium, 
pp. 75–89. IEEE Press, Piscataway (2015) 
12. Shukla, S., Sadashivappa, G.: Secure multi-party computation protocol using asymmetric 
encryption. In: International Conference on computing for sustainable global development, 
pp. 780–785. IEEE Press, Piscataway (2014) 
13. Ting, W., Wenjun, L.: Applications of secure multi-party computation in space geometry 
problems. Comput Appl Syst. 24(1), 156–160 (2015) 
14. Maohua, S.: Research on Secure Multi-Party Computation and its Application [D] . Beijing 
University of Posts and Telecommunications, Beijing (2013). http://read.nlc.cn/allSearch/ 
searchDetail?searchType=&showType=1&indexName=data_408&ﬁd=006873234

2
Advances in Privacy Preservation Technologies
37
15. Henecka, W., Sadeghi, A.R., Schneider, T., et al.: TASTY: tool for automating secure 
two-party computations. In: Proceedings of the 17th ACM Conference on computer and 
communications security, pp. 451–462. ACM Press, New York (2010) 
16. Xue, K.P., Xue, Y.J., Hong, J.N., et al.: RAAC: robust and auditable access control with 
multiple attribute authorities for public cloud storage. IEEE Trans. Inf. Forensics Secur. 12(4), 
953–967 (2017) 
17. Jing, W., Chuanhe, H., Jinhai, W.: An access control mechanism with dynamic privilege for 
cloud storage. J Comput Res Develop. 53(4), 904–920 (2016) 
18. Zhitao, G., Tingting, Y., Ruzhi, X., et al.: Multi-authority attribute-based encryption access 
control model for cloud storage. J. Commun. 36(6), 116–126 (2015) 
19. Saxena, N., Choi, B.J., Lu, R.X.: Authentication and authorization scheme for various user 
roles and devices in smart grid. IEEE Trans. Inf. Forensics Secur. 11(5), 907–921 (2016) 
20. Zhang, Y., Chen, J., Du, R., et al.: FEACS: a ﬂexible and efﬁcient access control scheme for 
cloud computing. In: Proceedings of IEEE International Conference on trust, security and 
privacy in computing and communications, pp. 310–319. IEEE Press, Piscataway (2015) 
21. Khambhammettu, H., Boulares, S., Adi, K., et al.: A framework for risk assessment in access 
control systems. Comput. Secur. 39, 86–103 (2013) 
22. Miettinen, M., Heuser, S., Kronz, W., et al.: ConXsense: automated context classiﬁcation for 
context-aware access control. In: Proceedings of ACM symposium on information, computer 
and communications security, pp. 293–304. ACM Press, New York (2014) 
23. Santos, D., Ricardo, D., Westphall, C.M., et al.: A dynamic risk-based access control archi-
tecture for cloud computing. In: Proceedings of Asia-Paciﬁc network operations and manage-
ment symposium, pp. 1–9. IEEE Press, Piscataway (2014) 
24. Zhen, H., Hao, L., Min, Z., et al.: Risk-adaptive access control model for big data in 
healthcare. J. Commun. 36(12), 190–199 (2015) 
25. Ning, J.T., Cao, Z.F., Dong, X., et al.: Auditable-time outsourced attribute-based encryption 
for access control in cloud computing. IEEE Trans. Inf. Forensics Secur. 13(1), 94–105 (2018) 
26. Yang, K., Liu, Z., Jia, X.H., et al.: Time-domain attribute-based access control for cloud-based 
video content sharing: a cryptographic approach. IEEE Trans. Multimed. 18(5), 940–950 
(2016) 
27. Yan, Z., Li, X., Wang, M.J.: Flexible data access control based on trust and reputation in cloud 
computing. IEEE Trans Cloud Comput. 5(3), 485–498 (2017) 
28. Dang, N., Park, J., Sandhu, R.: A provenance-based access control model for dynamic 
separation of duties. In: Proceedings of International Conference on privacy, security and 
trust, pp. 247–256. IEEE Press, Piscataway (2013) 
29. Sun, L.S., Park, J., Dang, N., et al.: A provenance-aware access control framework with typed 
provenance. IEEE Trans Dependable Secure Comput. 13(4), 411–423 (2016) 
30. Pearson, S., Casassa-mont, M.: Sticky policies: an approach for managing privacy across 
multiple parties. Computer. 44(9), 60–68 (2011) 
31. Spyra, G., Buchanan, W.J., Ekonomou, E.: Sticky policies approach within cloud computing. 
Comput. Secur. 70, 366–375 (2017) 
32. Dengguo, F., Jingbin, L., Qin, Y., et al.: Trusted computing theory and technology in 
innovation-driven development. Sci Sin. 50(8), 1127–1147 (2020) 
33. State Cryptography Administration. Information security technology-Function and interface 
speciﬁcation of cryptographic support platform for trusted computing. GB/T 29829–2013 
34. Francillon, A., Nguyen, Q., Rasmussen, K.B., et al.: A minimalist approach to remote 
attestation. In: 2014 design, automation & test in Europe conference & exhibition, pp. 1–6. 
IEEE Press, Piscataway (2014) 
35. Tian, J.S., Jing, Z.: Research and implementation of active dynamic measurement based on 
TPCM. Netinfo Secur. 16(6), 22 (2016) 
36. Huang, J., Shi, W.C.: The TPCM active measurement and power control design for ATX 
motherboard. Netinfo Secur. 11, 1–5 (2016)

38
F. Li et al.
37. Ekberg, J.E., Kostiainen, K., Asokan, N.: Trusted execution environments on mobile 
devices. In: Proceedings of the 2013 ACM SIGSAC Conference on Computer and Commu-
nications Security, pp. 1497–1498. ACM Press, New York (2013) 
38. Raj, H., Saroiu, S., Wolman, A., et al.: fTPM: a ﬁrmware-based TPM 2.0 implementation. 
Microsoft Res. 12, 1–22 (2015) 
39. Dong, P., Ding, Y., Jiang, Z., et al.: Design and implementation of TPM/TCM with active trust 
based on TEE. Journal of Software. 31(5), 1392–1405 (2020) 
40. Mcmahan, H.B., Moore, E., Ramage, D., et al.: Communication-efﬁcient learning of deep 
networks from decentralized data. In: Proceedings of the 20th International Conference on 
Artiﬁcial Intelligence and Statistics, pp. 1273–1282. PMLR (2017) 
41. Zhu, L.G., Liu, Z.J., Han, S.: Deep leakage from gradients. In: Advances in neural information 
processing systems, vol. 32, pp. 14774–14784. Curran Associates, Inc. (2019) 
42. Melis, L., Song, C.Z., Cristofaro, E.D., et al.: Exploiting unintended feature leakage in 
collaborative learning. In: IEEE symposium on security and privacy (SP), pp. 691–706. 
IEEE (2019) 
43. Zhang, C.L., Li, S.Y., Xia, J.Z., et al.: Batchcrypt: efﬁcient homomorphic encryption for cross-
silo federated learning. In: USENIX annual technical conference, pp. 493–506. USENIX 
Association (2020) 
44. Bonawitz, K.A., Ivanov, V., Kreuter, B., et al.: Practical secure aggregation for privacy-
preserving machine learning. In: ACM SIGSAC Conference on Computer and Communica-
tions Security (CCS), pp. 1175–1191. ACM (2017) 
45. Truex, S., Liu, L., Chow, K.H., et al.: LDP-fed: federated learning with local differential 
privacy. In: Proceedings of the third ACM international workshop on edge systems, analytics 
and networking, pp. 61–66. ACM (2020) 
46. Sweeney, L.: K-anonymity: a model for protecting privacy. Int J Uncertain Fuzz. 10(5), 
557–570 (2002) 
47. Lefevre, K., Dewitt, D.J., Ramakrishnan, R.: Incognito: efﬁcient full-domain k-anonymity. In: 
Proceedings of the 2005 ACM SIGMOD international conference on management of data, 
pp. 49–60. ACM Press, New York (2005) 
48. Machanavajjhala, A., Kifer, D., Gehrke, J., et al.: L-diversity: privacy beyond k-anonymity. 
ACM Trans. Knowl. Discov. Data. 1(1), 3 (2007) 
49. Liu, F.Y., Hua, K.A., Cai, Y.: Query L-diversity in location-based services. In: 2009 Tenth 
International Conference on mobile data management: systems, services and middleware, 
pp. 436–442. IEEE Press, Piscataway (2009) 
50. Li, N.H., Li, T.C., Venkatasubramanian, S.: T-closeness: privacy beyond k-anonymity and 
l-diversity. In: 2007 IEEE 23rd International Conference on Data Engineering, pp. 106–115. 
IEEE Press, Piscataway (2007) 
51. Rebollo-monedero, D., Forne, J., Domingo-ferrer, J.: From t-closeness- like privacy to 
postrandomization via information theory. IEEE Trans. Knowl. Data Eng. 22(11), 
1623–1636 (2010) 
52. Tojiboev, R., Lee, W., Lee, C.C.: Adding noise trajectory for providing privacy in data 
publishing by vectorization. In: 2020 IEEE International Conference on Big Data and Smart 
Computing, pp. 432–434. IEEE Press, Piscataway (2020) 
53. Wang, Y.F., Li, M.Z., Luo, S.S., et al.: LRM: a location recombination mechanism for 
achieving trajectory k-anonymity privacy protection. IEEE Access. 7, 1–20 (2019) 
54. Naghizade, E., Kulik, L., Tanin, E., et al.: Privacy- and context-aware release of trajectory 
data. ACM Trans Spat Algorithms Syst. 6(1), 1–25 (2020) 
55. Chen, C.M., Luo, Y.L., Yu, Q.Y., et al.: TPPG: privacy-preserving trajectory data publication 
based on 3D-grid partition. Intell Data Anal. 23(3), 503–533 (2019) 
56. Dai, Y., Shao, J., Wei, C.B., et al.: Personalized semantic trajectory privacy preservation 
through trajectory reconstruction. World Wide Web. 21(4), 875–914 (2018) 
57. Cao, Y., Xiao, Y.H., Xiong, L., et al.: PriSTE: protecting spatiotemporal event privacy in 
continuous location-based services. Proceed VLDB Endow. 12(12), 1866–1869 (2019)

2
Advances in Privacy Preservation Technologies
39
58. Huang, Y., Huo, Z., Meng, X.F.: CoPrivacy:A Collaborative Location Privacy-Preserving 
Method without Cloaking Region. Chin J Computers. 34(10), 1976–1985 (2011) 
59. Shaham, S., Ding, M., Liu, B., et al.: Privacy preservation in location-based services: a novel 
metric and attack model. IEEE Trans. Mob. Comput. PP(99), 1–13 (2020) 
60. Li, Y.H., Cao, X., Yuan, Y., et al.: PrivSem: protecting location privacy using semantic and 
differential privacy. World Wide Web. 22(6), 2407–2436 (2019) 
61. Gedik, B., Liu, L.: Location privacy in mobile systems: a personalized anonymization 
model. In: Proceedings of the 25th IEEE International Conference on Distributed Computing 
Systems, pp. 620–629. IEEE Press, Piscataway (2005) 
62. Mokbel, M.F., Chow, C.Y., Aref, W.G.: The new casper: query processing for location 
services without compromising privacy. In: Proceedings of the 32nd International Conference 
on Very Large Data Bases, pp. 12–15. IEEE Press, Piscataway (2006) 
63. Zhao, Y.N., Luo, Y.L., Yu, Q.Y., et al.: A privacy-preserving trajectory publication method 
based on secure start-points and end-points. Mob. Inf. Syst. 2020(12), 1–12 (2020) 
64. Tu, Z., Zhao, K., Xu, F.L., et al.: Protecting trajectory from semantic attack considering 
k-anonymity, l-diversity and t-closeness. IEEE Trans. Netw. Serv. Manag. 16(1), 264–278 
(2018) 
65. Xiao, Z., Xu, J.L., Meng, X.F.: P-sensitivity: a semantic privacy-protection model for location-
based services. In: 2008 Ninth International Conference on Mobile Data Management Work-
shops, pp. 47–54. IEEE Press, Piscataway (2008) 
66. Dewri, R., Ray, I., Ray, I., et al.: Query m-invariance: preventing query disclosures in 
continuous location-based services. In: 2010 Eleventh International Conference on Mobile 
Data Management, pp. 95–104. IEEE Press, Piscataway (2010) 
67. Sei, Y., Okumura, H., Takenouchi, T., et al.: Anonymization of sensitive quasi-identiﬁers for 
l-diversity and t-closeness. IEEE Trans Depend Secure Comput. 16(4), 580–593 (2017) 
68. Dwork, C.: Differential privacy: a survey of results. In: International Conference on Theory 
and Applications of Models of Computation, pp. 1–19. Springer, Berlin (2008) 
69. Duchi, J.C., Jordan, M.I., Wainwright, M.J.: Local privacy and statistical minimax rates. In: 
IEEE 54th Annual Symposium on Foundations of Computer Science, pp. 429–438. IEEE 
Press, Piscataway (2013) 
70. Tang, J., Korolova, A., Bai, X.L., et al.: Privacy loss in apple’s implementation of differential 
privacy on MacOs 10.12. arXiv Preprint, arXiv:1709.02753 (2017) 
71. Erlingsson, L., Pihur, V., Korolova, A.: RAPPOR: randomized aggregatable privacy-
preserving ordinal response. In: Proceedings of the 2014 ACM SIGSAC Conference on 
Computer and Communications Security, pp. 1054–1067. ACM Press, New York (2014) 
72. Dwork, 
C., 
Rothblum, 
G.N.: 
Concentrated 
differential 
privacy. 
arXiv 
Preprint, 
arXiv:1603.01887 (2016) 
73. Soria-comas, J., Domingo-ferrer, J., Sanchez, D., et al.: Individual differential privacy: a 
utility-preserving formulation of differential privacy guarantees. IEEE Trans. Inf. Forensics 
Secur. 12(6), 1418–1429 (2017) 
74. Yang, B., Sato, I., Nakagawa, H.: Bayesian differential privacy on correlated data. In: Pro-
ceedings of the 2015 ACM SIGMOD International Conference on Management of Data, 
pp. 747–762. ACM Press, New York (2015) 
75. He, K., Zhang, X.Y., Ren, S.Q., et al.: Deep residual learning for image recognition. In: 
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 
pp. 770–778. IEEE Press, Piscataway (2016) 
76. Osia, S.A., Shamsabadi, A.S., Taheri, A., et al.: Privacy-preserving deep inference for rich user 
data on the cloud. arXiv Preprint, arXiv:1710.01727 (2017) 
77. Tran, L., Kong, D., Jin, H.X., et al.: Privacy-cnh: a framework to detect photo privacy with 
convolutional neural network using hierarchical features. In: Thirtieth AAAI Conference on 
Artiﬁcial Intelligence, pp. 1–7. AAAI Press, Palo Alto (2016)

40
F. Li et al.
78. Yu, J., Zhang, B.P., Kuang, Z.Z., et al.: iPrivacy: image privacy protection by identifying 
sensitive objects via deep multi-task learning. IEEE Trans. Inf. Forensics Secur. 12(5), 
1005–1016 (2016) 
79. Shokri, R., Shmatikov, V.: Privacy-preserving deep learning. In: Proceedings of the 22nd 
ACM SIGSAC Conference on Computer and Communications Security, pp. 1310–1321. 
ACM Press, New York (2015) 
80. Abadi, M., Chu, A., Goodfellow, I., et al.: Deep learning with differential privacy. In: Pro-
ceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, 
pp. 308–318. ACM Press, New York (2016) 
81. Wei, K., Li, J., Ding, M., et al.: Federated learning with differential privacy: algorithms and 
performance analysis. IEEE Trans. Inf. Forensics Secur. 15, 3454–3469 (2020) 
82. Truex, S., Baracaldo, N., Anwar, A., et al.: A hybrid approach to privacy- preserving federated 
learning. arXiv Preprint, arXiv:1812.03224 (2018) 
83. Zhang, J., Zhang, Z.J., Xiao, X.K., et al.: Functional mechanism: regression analysis under 
differential privacy. Proceed VLDB Endow. 5(11), 1364–1375 (2012) 
84. Rubinstein, B.I.P., Bartlett, P.L., Huang, L., et al.: Learning in a large function space: privacy-
preserving mechanisms for SVM learning. arXiv Preprint, arXiv:0911.5708 (2009) 
85. Chaudhuri, K., Monteleoni, C., Sarwate, A.D.: Differentially private empirical risk 
minimization. J. Mach. Learn. Res. 12(3), 1–41 (2011) 
86. Yu, L., Liu, L., Pu, C., et al.: Differentially private model publishing for deep learning. In: 
2019 IEEE Symposium on Security and Privacy, pp. 332–349. IEEE Press, Piscataway (2019) 
87. Nasr, M., Shokri, R.: Improving deep learning with differential privacy using gradient 
encoding and denoising. arXiv Preprint, arXiv:2007.11524 (2020) 
88. Jayaraman, B., Evans, D.: Evaluating differentially private machine learning in practice. In: 
Proceedings of the 28th USENIX Conference on Security Symposium, pp. 1895–1912. 
USENIX Association, Berkeley (2019) 
89. Xu, C.G., Ren, J., Zhang, D.Y., et al.: GANobfuscator: mitigating information leakage under 
GAN via differential privacy. IEEE Trans. Inf. Forensics Secur. 14(9), 2358–2371 (2019) 
90. Papernot, N., Thakurta, A., Song, S., et al.: Tempered sigmoid activations for deep learning 
with differential privacy. arXiv Preprint, arXiv: 2007.14193 (2020) 
91. Dwork, C., Mcsherry, F., Nissim, K., et al.: Calibrating noise to sensitivity in private data 
analysis. In: Proceedings of the Third Conference on THEORY of Cryptography, 
pp. 265–284. Springer, Berlin (2006) 
92. Roth, A., Roughgarden, T.: Interactive privacy via the median mechanism. In: Proceedings of 
the Forty-Second ACM Symposium on Theory of Computing, pp. 765–774. ACM Press, 
New York (2010) 
93. Hardt, M., Rothblum, G.N.: A Multiplicative weights mechanism for privacy- preserving data 
analysis. In: 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, 
pp. 61–70. IEEE Press, Piscataway (2010) 
94. Gupta, A., Roth, A., Ullman, J.: Iterative constructions and private data release. In: Theory of 
Cryptography Conference, pp. 339–356. Springer, Berlin (2012) 
95. Fan, L.Y., Xiong, L.: An adaptive approach to real-time aggregate monitoring with differential 
privacy. IEEE Trans. Knowl. Data Eng. 26(9), 2094–2106 (2013) 
96. Kellaris, G., Papadopoulos, S., Xiao, X., et al.: Differentially private event sequences over 
inﬁnite streams. Proceed VLDB Endow. 7(12), 1155–1166 (2014) 
97. Wang, Q., Zhang, Y., Lu, X., et al.: Real-time and spatio-temporal crowd-sourced social 
network data publishing with differential privacy. IEEE Trans Depend Secure Comput. 15(4), 
591–606 (2016) 
98. Lv, D.L., Zhu, S.B.: Achieving correlated differential privacy of big data publication. Comput. 
Secur. 82, 184–195 (2019) 
99. Li, S.Y., Ji, X.S., You, W.: A personalized differential privacy protection method for repeated 
queries. In: 2019 IEEE 4th International Conference on Big Data Analytics, pp. 274–280. 
IEEE Press, Piscataway (2019)

2
Advances in Privacy Preservation Technologies
41
100. Xiao, X.K., Wang, G.Z., Gehrke, J.: Differential privacy via wavelet transforms. IEEE Trans. 
Knowl. Data Eng. 23(8), 1200–1214 (2010) 
101. Li, C., Hay, M., Rastogi, V., et al.: Optimizing linear counting queries under differential 
privacy. In: Proceedings of the Twenty-Ninth ACM SIGMOD-SIGACT-SIGART Sympo-
sium on Principles of Database Systems, pp. 123–134. ACM Press, New York (2010) 
102. Xu, J., Zhang, Z.J., Xiao, X.K., et al.: Differentially private histogram publication. VLDB 
J. 22(6), 797–822 (2013) 
103. Acs, G., Castelluccia, C., Chen, R.: Differentially private histogram publishing through lossy 
compression. In: 2012 IEEE 12th International Conference on Data Mining, pp. 1–10. IEEE 
Press, Piscataway (2012) 
104. Zhang, X.J., Chen, R., Xu, J.L., et al.: Towards accurate histogram publication under differ-
ential privacy. In: Proceedings of the 2014 SIAM International Conference on Data Mining, 
pp. 587–595. Society for Industrial and Applied Mathematics (2014) 
105. Ma, Z., Zhang, T., Liu, X.M., et al.: Real-time privacy-preserving data release over vehicle 
trajectory. IEEE Trans. Veh. Technol. 68(8), 8091–8102 (2019) 
106. Li, F.Y., Yang, J., Xue, L., et al.: Real-time trajectory data publishing method with differential 
privacy. In: 2018 14th International Conference on Mobile Ad-Hoc and Sensor Networks, 
pp. 177–182. IEEE Press, Piscataway (2018) 
107. Wu, G.Q., Xia, X.Y., He, Y.P.: Extending differential privacy for treating dependent records 
via information theory. arXiv Preprint, arXiv:1703.07474 (2017) 
108. Peng, C.G., Ding, H.F., Zhu, Y.J., et al.: Information entropy models and privacy metrics 
methods for privacy protection. J Soft. 27(8), 1891–1903 (2016) 
109. Wu, S., Wang, X., Wang, S.J., et al.: K-anonymity for crowdsourcing database. IEEE Trans. 
Knowl. Data Eng. 26(9), 2207–2221 (2013) 
110. Cuff, P., Yu, L.: Differential privacy as a mutual information constraint. In: Proceedings of the 
2016 ACM SIGSAC Conference on Computer and Communications Security, pp. 43–54. 
ACM Press, New York (2016) 
111. Asoodeh, S., Alajaji, F., Linder, T.: Notes on information-theoretic privacy. In: 2014 52nd 
Annual Allerton Conference on Communication, Control, and Computing, pp. 1272–1278. 
IEEE Press, Piscataway (2014) 
112. Oya, S., Troncoso, C., Pérez-gonzález, F.: Back to the drawing board: revisiting the design of 
optimal location privacy-preserving mechanisms. In: Proceedings of the 2017 ACM SIGSAC 
Conference on Computer and Communications Security, pp. 1959–1972. ACM Press, 
New York (2017) 
113. Ma, C.Y.T., Yau, D.K.Y.: On information-theoretic measures for quantifying privacy protec-
tion of time-series data. In: Proceedings of the 10th ACM Symposium on Information, 
Computer and Communications Security, pp. 427–438. ACM Press, New York (2015) 
114. Ding, Z.Y., Wang, Y.X., Wang, G., et al.: Detecting violations of differential privacy. In: 
Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications 
Security, pp. 475–489. ACM Press, New York (2018) 
115. Bichsel, B., Gehr, T., Drachsler-cohen, D., et al.: Dp-ﬁnder: ﬁnding differential privacy 
violations by sampling and optimization. In: Proceedings of the 2018 ACM SIGSAC Confer-
ence on Computer and Communications Security, pp. 508–524. ACM Press, New York (2018) 
116. Gervais, A., Shokri, R., Singla, A., et al.: Quantifying web-search privacy. In: Proceedings of 
the 2014 ACM SIGSAC Conference on Computer and Communications Security, 
pp. 966–977. ACM Press, New York (2014) 
117. Cao, Y., Yoshikawa, M., Xiao, Y.H., et al.: Quantifying differential privacy in continuous data 
release under temporal correlations. IEEE Trans. Knowl. Data Eng. 31(7), 1281–1295 (2018) 
118. Shokri, R., Theodorakopoulos, G., Le Boudec, J., et al.: Quantifying location privacy. In: 2011 
IEEE Symposium on Security and Privacy, pp. 247–262. IEEE Press, Piscataway (2011) 
119. Wu, X.T., Wu, T.T., Khan, M., et al.: Game theory based correlated privacy preserving 
analysis in big data. IEEE Trans Big Data. 7(4), 643–656 (2017)

42
F. Li et al.
120. Zhang, Z.K., He, S.B., Chen, J.M., et al.: REAP: an efﬁcient incentive mechanism for 
reconciling aggregation accuracy and individual privacy in crowdsensing. IEEE Trans. Inf. 
Forensics Secur. 13(99), 2995–3007 (2017) 
121. Jialin, L., Shuyang, S., Yuemei, Z., et al.: Effective and efﬁcient approach for graph 
de-anonymization. J Softw. 29(3), 772–785 (2018) 
122. Narayanan, A., Vitaly, S.: De-anonymizing social networks. In: 30th IEEE Symposium on 
Security and Privacy, pp. 173–187. IEEE Press, Piscataway (2009) 
123. Yartseva, L., Grossglauser, M.: On the performance of percolation graph matching. In: Pro-
ceedings of the First ACM Conference on Online Social Networks, pp. 119–130. ACM Press, 
New York (2013) 
124. Kazemi, E., Hassani, S.H., Grossglauser, M.: Growing a graph matching from a handful of 
seeds. ProcVLDB Endow. 8(10), 1010–1021 (2015) 
125. Backstrom, L., Dwork, C., Kleinberg, J.: Wherefore art thou R3579X? anonymized social 
networks, hidden patterns, and structural steganography. In: Proceedings of the 16th Interna-
tional Conference on World Wide Web, pp. 181–190. ACM Press, New York (2007) 
126. Wang, Y.X., Ding, Z.Y., Kifer, D., et al.: CheckDP: an automated and integrated approach for 
proving differential privacy or ﬁnding precise counterexamples. In: Proceedings of the 2020 
ACM SIGSAC Conference on Computer and Communications Security, pp. 919–938. ACM 
Press, New York (2020) 
127. Crandall, D.J., Backstrom, L., Cosley, D., et al.: Inferring social ties from geographic 
coincidences. Proc. Natl. Acad. Sci. 107(52), 22436–22441 (2010) 
128. Davis, C.A., Pappa, G.L., De Oliviera, D.R.R., et al.: Inferring the location of twitter messages 
based on user relationships. Trans. GIS. 15(6), 735–751 (2011) 
129. Ma, C.Y.T., Yau, D.K.Y., Yip, N.K., et al.: Privacy vulnerability of published anonymous 
mobility traces. IEEE/ACM Trans. Networking. 21(3), 720–733 (2013) 
130. Zang, H., Jean, B.: Anonymization of location data does not work: a large-scale measurement 
study. In: Proceedings of the 17th Annual International Conference on Mobile Computing and 
Networking, pp. 145–156. ACM Press, New York (2011) 
131. Caimei, W., Yajun, G., Yanhua, G.: Privacy metric for User’s trajectory in location-based 
services. J Softw. 23(2), 352–360 (2012) 
132. Chang, S., Li, C., Zhu, H.Z., et al.: Revealing privacy vulnerabilities of anonymous trajecto-
ries. IEEE Trans. Veh. Technol. 67(12), 12061–12071 (2018) 
133. Fenghua, L., Hui, L., Yan, J., et al.: Privacy computing:concept,connotation and its research 
trend. J. Commun. 37(4), 1–11 (2016) 
134. Li, F.H., Li, H., Niu, B., et al.: Privacy computing: concept, computing framework, and future 
development trends. ELSEVIER Eng. 5(6), 1179–1192 (2019)

Chapter 3 
Privacy Computing Theory 
Fenghua Li 
, Hui Li 
, and Ben Niu 
Over the past 20 years, privacy preservation has received extensive attention. 
However, existing research has focused on scattered privacy-preserving methods 
for speciﬁc scenarios, and there is no systematic research on privacy preservation 
from the perspective of “computing”. To establish a computing framework for the 
preservation of privacy information throughout its life cycle, the authors of this book 
are the ﬁrst to propose the concept and deﬁnition of privacy computing in 2015, 
formally described privacy computing, and analyzed its connotation and research 
scope. The formal deﬁnition has been proposed, the framework of privacy comput-
ing has been established, and the implementation details with initial results have 
been provided. 
This chapter mainly introduces the formal deﬁnition of privacy computing, key 
technology components and computing framework, important characteristics of 
privacy computing, intelligent perception and dynamic measurement of privacy, 
design principles of privacy-preserving algorithms, the evaluation of the privacy 
preservation effectiveness, privacy computing language, the determination and 
traceability of privacy infringement, the privacy information system architecture, 
and the academic connotations of privacy computing and data security. In this 
chapter, a comparative analysis of privacy computing versus traditional privacy 
preservation, data security, and other related solutions is conducted across 18 dimen-
sions. The content of this chapter will guide the systematic research of privacy 
computing. 
F. Li (✉) · B. Niu 
Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China 
e-mail: lifenghua@iie.ac.cn; niuben@iie.ac.cn 
H. Li 
School of Cyber Engineering, Xidian University, Xian, Shaanxi, China 
e-mail: lihui@mail.xidian.edu.cn 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
F. Li et al., Privacy Computing, https://doi.org/10.1007/978-981-99-4943-4_3
43

44
F. Li et al.
3.1 
The Deﬁnition of Privacy Computing 
Privacy computing requires a clear and formal deﬁnition and measurement method 
for privacy information, and establishes a complete axiomatic system, inference 
rules, and theorem-proving system. This section introduces the concept, deﬁnition, 
and research scope of privacy computing from the perspective of “computing”, and 
deﬁnes what privacy computing is. 
3.1.1 
The Basic Deﬁnition of Privacy Computing 
The key idea of privacy computing is to support the perception and quantiﬁcation of 
privacy information, establish a computable model in the operation process of 
privacy information, and describe the quantitative evolution rules of privacy com-
ponents when combining privacy operations (including arithmetic operations, con-
trol operations, etc.), the ability evaluation of privacy preservation algorithms, and 
the mapping between the quantiﬁcation of preservation effects, and determine the 
optimal privacy preservation effects that can be achieved under different constraints, 
as well as the corresponding privacy preservation algorithms and its combinations. 
The ultimate goal of privacy computing is to automate the execution of privacy 
preservation, build a system design theory and architecture that supports massive 
users, high concurrency, and high efﬁciency privacy preservation, and achieve 
effective combinations among different algorithms. 
The deﬁnition of privacy computing proposed by the authors of this book is 
privacy computing refers to a computing theory and methodology that can support 
the “full-life cycle” privacy preservation, which provides computable models and 
axiomatic systems for privacy measurement, privacy leakage cost, privacy preser-
vation, and privacy analysis complexity, where the ownership, management, and use 
rights of privacy information are separated. 
Speciﬁcally, it refers to operations of describing, measuring, evaluating, and 
integrating the privacy information when processing the video, audio, image, 
graph, numerical value, and behavior information ﬂow in the ubiquitous network. 
It comprises a set of symbolized and formulized privacy computing theories, 
algorithms, and application technologies with quantitative assessment standards and 
support for the integration of multiple information systems when processing video, 
audio, image, graphics, text, numerical value, and ubiquitous network behavior 
information ﬂow. Privacy computing covers all computing operations of information 
collectors, publishers, and users in the entire life cycle process of information 
generation, perception, release, dissemination, storage, processing, use, and destruc-
tion, and includes support for massive users, high concurrency, and high efﬁciency. 
Privacy computing is the theoretical foundation for privacy information preservation 
in ubiquitous cyberspace [1].

2
3
Privacy Computing Theory
45
3.1.2 
The Formal Description of Privacy Information 
The formal description of privacy information facilitates the way of building a 
privacy preservation model, and can also better support program control and 
on-demand preservation. 
This section ﬁrst deﬁnes the privacy information X and the six basic elements it 
involves [2], as well as related axioms, theorems, and assumptions, which are the 
basis for describing other aspects of privacy computing. The information M can be 
one type of modal data such as text, image, voice, and video, or mixed data of several 
modal data. Note that the extraction methods of privacy information vectors for any 
information M are not considered in this book, because they are subject to the 
extraction conditions of speciﬁc ﬁelds, such as natural language processing. 
Deﬁnition 3.1 The privacy information X contained in the information M is 
represented by a six-tuple hI, A, Γ, Ω, Θ, Ψi, which I represents the privacy infor-
mation vector, A represents the privacy attribute vector, Γ represents the generalized 
positioning information set, Ω represents the audit control information set, Θ 
represents the constraint condition set, Ψ represents the propagation control opera-
tions collection [2]. 
Deﬁnition 3.2 Privacy information vector I = (IID, i1, i2, . . ., ik, . . ., in), where 
ik(1 ≤ k ≤ n) is the privacy information component, which is used to represent the 
semantically informative, inseparable, and mutually disjoint atomic information in 
the information M, with the form of text, audio, video, images, etc., and semantic 
features include words, words, intonation, tone, phoneme, pitch, frame, pixel, color, 
etc. IID is the unique identiﬁer of the privacy information vector [2]. For example, the 
text message “U1 and U2 to Loc drink”, in this sentence I = (IID, i1, i2, i3, i4, i5, i6, 
i7) = (IID, U1, and, U2, go, Loc, drink, wine), n = 7. Note: Certain pieces of informa-
tion, such as proverbs, can be effectively segmented using natural language 
processing methods. 
Axiom 3.1 Given a certain natural language and its grammar rules, and the granu-
larity of words, phrases, and slang words, the number of components of the privacy 
information vector I is bounded [2]. 
Property 3.1 The privacy information vector is in ﬁrst normal form and the second 
normal form [2]. 
The privacy information component ik is deﬁned as the smallest granularity that 
cannot be subdivided and has atomic properties. 1NF is deﬁned as if and only if the 
ﬁelds of all attributes of a relational schema R are atomic, then this relational schema 
R is said to belong to the ﬁrst normal form, so ik is in ﬁrst normal form. The privacy 
information vector I has a uniquely identiﬁed IID primary key, and all the other 
elements except primary attributes depend on this primary key. The deﬁnition of 
2NF is: if R 2 1NF, and each non-primary attribute fully functionally depends on a 
unique primary key, then R 
2NF, so ik is in second normal form.

46
F. Li et al.
Deﬁnition 3.3 Constraint set Θ = (θ1, θ2, . . ., θn), θk(1 ≤ k ≤ n) represents the 
constraint vector corresponding to the privacy information component ik, and is 
used to describe the access rights required by entities to access ik in different 
scenarios [2]. For example, the information can be related to who, at what time, 
using what device, in what way to access and use the privacy information vector, and 
the duration of using the privacy information vector, etc. Only the entities that satisfy 
all the access rights in the constraint vector θk can access the privacy information 
component ik, who can possibly be the information owners, information recipients, 
information processors, and so on. 
Deﬁnition 3.4 Privacy attribute vector A = (a1, a2, . . ., ak, . . ., an, an + 1, . . ., am), ak 
represents the privacy attribute component, which is used to quantify the sensitivity 
or expected preservation degree of the privacy information component and compo-
nent combination [2]. In practical applications, different privacy information com-
ponents in different scenarios can be weighted and dynamically combined, which 
will generate new privacy information. However, based on the atomicity of the 
privacy information components, the author of this book denotes the expected 
degree of preservation of the privacy information of different ik combinations as 
privacy attribute components. When 1 ≤ k ≤ n, ak uniquely maps to ik; when 
n < k ≤ m, ak indicates the expected preservation degree of the privacy information 
after the combination of two or more privacy information components. If n < k ≤m, ak 
can be calculated by the deﬁned operator, the privacy attribute vector only needs to 
describe an. 
The value range of ak is [0, 1], where ak = 0 means that the privacy information 
owner enjoys the information exclusively in a safe and controllable environment, 
that is, the information is not shared, and there is no leakage; hence, the expected 
information is preserved with the highest degree. The mutual information between 
the preserved privacy information and the original privacy information is 0. For 
example, if it is a privacy preservation method such as encryption, it means that the 
key is lost and the information is completely irrecoverable; if it is a loss-irreversible 
privacy preservation method such as noise addition and generalization, the degree of 
information distortion makes the preserved information and the original information 
fully independent. When ak = 1, it means that the ik component does not need any 
preservation, and can be released without restriction. Different intermediate values 
represent different expected preservation degrees of privacy information compo-
nents. The lower the value, the better the expected preservation degree of privacy 
information. 
Denote the quantiﬁcation operation function of the desired degree of privacy 
preservation as σ,where manual marking, weighting function, etc. can be used as the 
quantiﬁcation operation function of the desired degree of privacy preservation. 
Consider that ik has different information types, the corresponding expressions of 
σ can also adopt different methods, denoted as ak = σ(ik, θk) (1  ≤ k ≤ n). For any 
combination 
i1, 
i2, 
. . ., 
in 
of 
the 
privacy 
information 
components 
inþj = ik1 _ ik2 _ ⋯_ iks, the _ operator represents the combination of multiple 
privacy information components, and the privacy attribute component σ is generated

þ
þ
þ
through the expected privacy preservation degree quantiﬁcation operation function 
an + j, that is, anþj = σ inþj, θk1, θk2, . . . , θks (1 ≤ k1 < ⋯ < ks ≤ n). For the privacy 
information component i1, i2, . . ., in and the privacy information component combi-
nation in + 1, in + 2, . . ., im, a privacy attribute vector A = (a1, a2, . . ., ak, . . ., an, 
an + 1, . . ., am) is generated, where the value of m is a positive integer greater than or 
equal to n. The relationship between the above privacy information vector and 
privacy attribute vector is denoted by A = σ(I, Θ). Quantization operations are 
closely related to constraints, and different entities may have different quantization 
results when accessing different scenarios. 
3
Privacy Computing Theory
47
Theorem 3.1 For a speciﬁc privacy information vector I = (i1, i2, . . ., ik, . . ., in) with 
a bounded number of components, the dimension of its privacy attribute vector 
A = (a1, a2, . . ., an, an + 1, . . ., am) is bounded. When the binary/multiple combina-
tions of each privacy information component in I uniquely map to a privacy attribute 
component, the number of privacy attribute components is m ≤ 2n - 1 [2]. 
Proof Following from Deﬁnition 3.1 and Axiom 3.1, given a privacy information 
vector ε, its dimension is bounded as n. According to the deﬁnition of a privacy 
attribute vector, the privacy attribute component corresponds to the privacy infor-
mation component and its combination, so the dimension of the privacy attribute 
vector is bounded. When the combination of privacy information components is in 
one-to-one correspondence with the privacy attribute components, the dimension of 
the privacy attribute vector is no larger than the number of all combinations of the 
privacy information components, including 2 to n-element combinations, which is 
C1 
n 
C2 
n 
⋯ 
Cn 
n = 2n - 1, so there is m ≤ 2n - 1. Certiﬁcated. 
Deﬁnition 3.5 Generalized positioning information set Γ = (γ1, γ2, . . ., γk, . . ., γn), 
γk is the generalized positioning information vector, which represents the location 
information and attribute information of the privacy information component ik in the 
information M, and can be used to efﬁciently locate the privacy information com-
ponent ik. The location information is used to describe the speciﬁc location in the 
information M [2], such as page numbers, chapters, paragraphs, serial numbers, 
coordinates, frame serial numbers, time periods, audio tracks, layers, and pixels. In a 
text ﬁle, the location information mainly includes the page number, chapter, para-
graph, serial number, etc., and the attribute information mainly includes font, font 
size, weight, italics, underline, strikethrough, superscript, subscript, style, line spac-
ing, etc.; in audio or in a video ﬁle, the attribute information includes font, size, 
thickness, line spacing, pixel, chroma, brightness, pitch, intonation, tone, etc. 
Deﬁnition 3.6 The set of audit control information Ω = (ω1, ω2, . . ., ωk, . . ., ωn), ωk 
represents a speciﬁc audit control vector in the propagation process of ik, which is 
used to record the subject and object information of the privacy information com-
ponent ik and the performed operation records during the circulation process. When 
leakage occurs, traceability can be carried out [2]. For example, in the process of 
circulation, the subject and object information contains information owners, infor-
mation 
forwarders, 
information 
receivers, 
information 
sending 
equipment,

information receiving equipment, information transmission methods, information 
transmission channels, etc., and operation records contain copying, pasting, cutting, 
forward, modify, delete, etc. 
48
F. Li et al.
Deﬁnition 3.7 Propagation control operation set Ψ = (ψ1, ψ2, . . ., ψk, . . ., ψn), 
where ψk is the propagation control operation vector, used to describe the operations 
that ik and their combination can be performed, such as copy, paste, forward, cut, 
modify, deletion, etc., which do not affect the atomicity of I [2]. Among them, 
ψl 
= 
judg(al, θl), 
the 
constraint 
condition 
vector 
θl = θk1 _ θk2 _ ⋯_ θks 
(n + 1 ≤ l ≤ m), judg is the operation discriminant function, including but not limited 
to any combination of one or more artiﬁcial marking and weighting functions. 
Axiom 3.2 In a cross-system exchange, if the management and control parties 
cannot exchange the extended authorization information completely and effectively, 
the leakage of privacy information will occur [2]. 
Assumption 3.1 Privacy computation can be deﬁned as a ﬁnite number of atomic 
operations, and other operations are obtained by combining them based on a ﬁnite 
number of atomic operations [2]. 
Assumption 3.2 Privacy calculation holds when the number of privacy information 
components is bounded [2]. 
3.2 
Key Technology and Computing Framework of Privacy 
Computing 
Privacy computing should guide the implementation of the privacy information 
preservation system, which can automatically protect different types of privacy 
information in different scenarios, and establish key technical components such as 
perception, de-identiﬁed (is called “de-identiﬁed” in components, “desensitization” 
in algorithms), storage, fusion, exchange, and destruction. This section describes the 
key technical challenges in each component. 
3.2.1 
The Key Technology Components of Privacy 
Computing 
Figure 3.1 illustrates the relationship among the six components (perception, pri-
vacy, storage, fusion, exchange, and destruction) and the key technologies in the 
theoretical system of privacy computing.

3
Privacy Computing Theory
49
perception
privacy
storage
fusion
exchange
destruction 
Store part result of fusion 
Store new and value data after exchange 
Fig. 3.1 The key technology components of privacy computing 
3.2.1.1 
The Perception Component 
The perception component mainly deals with privacy description and protocol 
mechanism, privacy component determination and quantiﬁcation, mutual informa-
tion calculation, etc. 
In terms of privacy description and speciﬁcation mechanism, it is necessary to 
solve privacy metadata extraction, privacy marking and coding, privacy description 
(including privacy information subject attributes, receiver attributes, and their map-
ping relationships), privacy information change process, inference rules, etc. 
In terms of privacy component determination and quantiﬁcation, given one or 
more data documents, is the existence of privacy and the quantitative measures of 
privacy components should be determined. 
The main focus of mutual information calculation is on the quantitative measure-
ment of the privacy components contained in the data and the mutual information 
between the privacy components contained in the data, the subjective understanding 
of the privacy components by the data subjects, controllers, processors and receivers, 
and the mutual information measurement of background knowledge, etc. 
The designed privacy computing model should be able to describe the three-
dimensional evolution of subject, time, and space. 
3.2.1.2 
The Privacy Component 
The privacy component mainly focuses on the desensitization mechanism, the 
evaluation theory and method of algorithm preservation ability, etc. 
The design of desensitization mechanism should consider how to construct 
desensitization operations suitable for privacy preservation that are different from 
traditional data encryption and decryption, where k-anonymity, confusion, general-
ization, suppression, decoupling, scrambling, etc. can be used as inner components 
in large-scale privacy preservation information systems. 
In terms of the theory and methods for algorithm evaluation, it is necessary to 
comprehensively determine and evaluate whether the output data of the algorithm 
should be fully marked, whether the marking is reasonable, whether the selected 
privacy preservation algorithm meets the corresponding preservation requirements, 
and whether it is able to defend against confrontational association analysis, and 
propose the corresponding evaluation standard theories and methods.

50
F. Li et al.
3.2.1.3 
The Storage Component 
The storage component mainly focuses on issues such as homogeneous privacy 
information de-redundancy, privacy-aware hybrid data partition storage, single-copy 
multi-user integrity veriﬁcation, new access control mechanisms supporting remote 
access and ﬁne-grained access, local data modiﬁcation, and group modiﬁcation new 
access control mechanism to support the realization of privacy preservation right to 
deletion and right to be forgotten. 
3.2.1.4 
The Fusion Component 
The fusion component mainly focuses on privacy information matching, privacy 
information transformation and privacy attribute derivation, constraint mapping, 
privacy operation, and adaptive selection of privacy preservation schemes. 
3.2.1.5 
The Exchange Component 
The exchange component mainly focuses on issues such as extended access control 
mechanism, dynamic adjustment of privacy, determination of privacy infringement, 
and traceability and evidence collection, and solves the problem of secondary 
distribution through extended authorization. 
3.2.1.6 
The Destruction Component 
The destruction component mainly focuses on issues such as deterministic deletion 
and notiﬁcation message mechanism. Deterministic deletion should ensure that the 
preserved information cannot be deprived, and it will be automatically deleted after 
receiving the user’s request to delete the instruction or agreeing with the user that the 
information storage expires. Destruction also establishes a notiﬁcation message 
mechanism and a notiﬁcation association system to notify other privacy information 
controllers and processors to delete privacy information and release storage space. 
The European Union and the United States have legislated to give users the “right 
to delete, right to be forgotten” and “eraser” laws, and deterministic deletion 
techniques are required to enact this legislation. 
3.2.2 
The Privacy Computing Framework 
The privacy computing framework is to establish the mapping relationship between 
application scenarios, preservation requirements, and computing models in each

component of the full life cycle of privacy information. Based on the scene descrip-
tion and preservation requirements, the calculation method of each component is 
adaptively selected to implement the corresponding calculation function. 
3
Privacy Computing Theory
51
Privacy 
information 
extraction 
Scene 
extraction 
Privacy 
operation 
selection 
Privacy 
protextion 
scheme 
design/selec 
tion 
Evaluation 
of privacy 
protect 
effectivenes 
s 
Effectivenes 
s evaluation 
is not good 
Change scene 
description 
Redefinate 
operation 
Redesign 
scheme 
Fig. 3.2 Privacy computing framework 
From the perspective of the whole life cycle, the author of this book proposes a 
privacy computing framework as shown in Fig. 3.2 [2]. The framework can be 
applied to plaintext information M in any format, and includes the following ﬁve 
steps. 
3.2.2.1 
Privacy Information Extraction 
According to the format and semantics of the plaintext information M, privacy 
information X is extracted, and the privacy information vector I is obtained. 
3.2.2.2 
Scene Extraction 
According to the type and semantics of each privacy information component I in ik, 
the application scenarios are deﬁned and extracted. 
3.2.2.3 
Privacy Operation Selection 
The privacy operations supported by each privacy information component are 
selected, and a set of propagation control operations is generated. 
3.2.2.4 
Privacy Preservation Scheme Design/Selection 
Select/design an appropriate privacy preservation scheme to meet certain require-
ments. Direct selection can be made if there are available and suitable solutions and 
parameters; otherwise, they should be redesigned.

52
F. Li et al.
3.2.2.5 
The Evaluation of Privacy Protect Effectiveness 
As for the evaluation criteria, entropy-based or distortion-based privacy metrics are 
used to measure the privacy-preserving effectiveness of the selected privacy-
preserving scheme. Refer Sect. 3.6 for more details on how to evaluate the effec-
tiveness of privacy preservation. 
When the evaluation result of the privacy preservation effect does not meet the 
requirements, the feedback mechanism is implemented, which includes three cases: 
(1) if the scene is not abstracted properly, the scene is re-abstracted and iterated; (2) if 
the scene is abstract and correct but the privacy operation is not selected properly, the 
privacy operation is regulated again; (3) if the scene and operation are correct, the 
privacy preservation scheme should be adjusted/improved to achieve satisfactory 
privacy preservation effect. 
Deﬁnition 3.8 Privacy calculation involves four elements (X, F, C, Q) [2], where X 
represents privacy information, F represents privacy operation set, C represents 
privacy preservation cost, and Q represents privacy preservation effect. 
Deﬁnition 3.9 Privacy operation set F = {f1, f2, . . ., fk, . . .}, F is the privacy pres-
ervation atomic operation set implemented on privacy information X, where the 
calculation can be modular addition, modular multiplication, modular exponentia-
tion, and operation can be generalization, replacement, suppression, decoupling, 
scrambling, insertion, and deletion [2]. The privacy preservation algorithm consists 
of multiple elements in the privacy operation set, and each element can be used 
repeatedly. 
Privacy perception, privacy preservation, privacy analysis, exchange and second-
ary propagation of privacy information, fusion of privacy information, update of 
privacy information, can be deﬁned as speciﬁc operations composed of several 
atomic operations. 
The preservation ability of privacy computing operations can be expressed 
quantitatively. Let the preservation capability of fi be l( fi), 0 ≤ l( fi) ≤ 1, where 
l( fi) = 0 means that optimal preservation is achieved through this operation, and 
l( fi) = 1 means that no preservation is performed. Therefore, for the privacy 
component ak with sensitivity ik, the privacy preservation operation fi with preser-
vation capability greater than l( fi), (l( fi) ≤ ak) should be selected. 
Axiom 3.3 When the privacy operation is performed on the information M, the 
privacy information vector will change from I to I' , and then the privacy attribute 
vector A will change to A' , and the number and value of its component ai 
' will also 
change [2]. That is, when I performs the privacy operation fk, and obtains I' = f(I), 
the corresponding A' ≠ A, of which A = σ(I), A' = σ(I' ) = σ( fk(I)). 
Deﬁnition 3.10 Privacy preservation complexity C, denoted by {Ck}  (1  ≤ k ≤ m), 
represents the quantiﬁcation of various resources expended to implement privacy 
preservation for information M, including computation, storage, network transmis-
sion overhead, etc. [2] Each privacy information component ik corresponds to a

ð
ð
privacy preservation complexity cost Ck, which is determined by the privacy infor-
mation component ik, the constraint condition vector θk, and the privacy operation 
component fk, which can be written as 
3
Privacy Computing Theory
53
Ck = ck ik, θk, f k
ð
Þ
3:1Þ 
Consider that each ik may have different information formats, such as text, 
images, and even insertion of an audio into a word ﬁle, each function ik 
corresponding to ck will have different expressions. 
Deﬁnition 3.11 Privacy preservation effect Q represents the preservation effect 
achieved after the privacy preservation of the information M, that is, the difference 
between the privacy measures before and after the privacy preservation [2]. Nor-
mally, it is necessary to fully consider the privacy information vector of information 
M, information access entities (including information owners, information receivers, 
information publishers, and other participants in the process of information creation 
and transmission), constraints, privacy operations, etc. The privacy metric deﬁned 
above is ak = σ(ik, θk), i.e., the expression of the privacy attribute component, where 
the function σ captures the factors of the operation vector of the privacy operation; in 
addition, the deﬁnition of the constraints also captures the factors of the information 
access entity, so the privacy preservation effect ik corresponding to the privacy 
information component γ3 can be expressed as 
Qk = Δσ ik, θk
ð
Þ = σbefore ik, θk
ð
Þ - σafter ik, θk
ð
Þ
ð3:2Þ 
where σbefore and σafter represent the privacy metric function before and after privacy 
preservation respectively. 
Deﬁnition 3.12 Privacy leakage proﬁt and loss ratio L = {Lk} represents the ratio of 
the proﬁt after the disclosure of privacy information to the loss caused by privacy 
leakage [2]. It is a function of the privacy preservation complexity cost C and privacy 
preservation effect Q in the form of: 
Lk = lk Ck, Qk
ð
Þ
3:3Þ 
The essential part of the privacy computing model is the characterization of the 
four factors of privacy computing, the proﬁt-loss ratio L of privacy leakage and their 
connection. 
3.3 
Important Characteristics of Privacy Computing 
The representation of privacy information as six-tuple is the basis for the automation 
of privacy computing. To achieve quantiﬁable privacy components, quantiﬁable 
preservation requirements, composable algorithms, and consistent evaluation of

the effectiveness of privacy preservation algorithms when exchanging privacy 
information across systems, the author of this book extracts four essential features 
of privacy computing [2]. 
54
F. Li et al.
3.3.1 
Atomicity 
Atomicity refers that he privacy information is depicted to an indivisible granularity, 
that is, to the extent that the privacy information is minimized, and the intersection 
between the privacy components is empty. The atomicity of privacy components is 
the ﬁrst characteristic of privacy computing, and the atomicity of privacy compo-
nents is the basis of the theoretical system of privacy computing. The privacy 
component itself and its combination are quantiﬁable measures. 
Natural language processing (NLP), image understanding can be applied to 
extract the semantic information that is disjoint from each other from various 
documents, and then a privacy vector is constructed with atomicity of each compo-
nent in the privacy information based on the privacy knowledge graph. The bound-
ary between privacy computing and intelligent technologies such as NLP is that 
privacy computing deals with privacy information, while the latter deal with infor-
mation extracted from multimodal documents. The privacy vector generation frame-
work is shown in Fig. 3.3. 
3.3.2 
Consistency 
The consistency of the mapping means that for the same privacy component 
information, the privacy preservation effectiveness of different algorithms cause 
the privacy metrics to approach to zero. At the same time, when mapping different 
privacy preservation systems, the quantitative relationship between the strength of 
algorithm preservation ability remains consistent before and after mapping. For 
example, the preservation capability evaluation of Algorithm A and B in System 
1 is A  > B, and when mapped to the evaluation system in System 2, A > B still 
holds. 
Multimodal 
document 
Extraction 
Privacy 
Information 
(Privacy Vector) 
Privacy 
knowledge 
graph 
Fig. 3.3 The privacy vector generation framework

3
Privacy Computing Theory
55
The principle of consistency makes the quantiﬁcation system of algorithm pres-
ervation ability consistent and comparable in different systems, so that the privacy 
information system can effectively compare the effectiveness of different preserva-
tion algorithms and facilitate their combination during the implementation process. 
3.3.3 
Sequentiality 
The privacy preservation algorithm consists of several components, and different 
combinations of different components may lead to different privacy preservation 
effects. Summarizing the evolution law of privacy preservation effect of different 
combination sequences can support the design of privacy preservation algorithm and 
the evaluation of privacy preservation effectiveness. If the execution order of the 
privacy-preserving algorithms does not affect the privacy-preserving effect, these 
algorithms can be executed in parallel; if the privacy-preserving algorithms need to 
be executed sequentially, the entire privacy-preserving algorithm can only adopt a 
sequential architecture. 
3.3.4 
Reversibility 
Reversibility is used to differentiate between privacy-preserving algorithms and 
privacy-desensitizing algorithms. If the algorithm is reversible, such as encryption-
based algorithms that can be recovered by decryption, it is considered as a privacy-
preserving method. In the application scenario of cross-system exchange, if there are 
differences in the preservation capabilities of each system, there will be the risk of 
information leakage, and the privacy preservation effect may not be maintained. The 
privacy desensitization method through generalization and scrambling is often 
irreversible, and it can ensure that the original information will not be restored 
when exchanging applications across systems, and the desensitized information 
can maintain the effect of privacy preservation. Therefore, privacy computing 
focuses on irreversible privacy desensitization algorithms. 
3.4 
Intelligent Perception and Dynamic Measurement 
of Privacy 
Privacy information depends on application scenarios, information expression 
methods, people’s emotional and psychological states, and time and space contexts, 
that is, it varies in time, identity and application scenario. Therefore, sensing privacy 
components and measuring privacy magnitudes from multi-modal data must be

intelligent and dynamic. In the process of cross-system exchange of privacy infor-
mation, there may be differences in the preservation capabilities of different systems, 
and a secondary dynamic measurement of privacy information is also required. 
Intelligent perception and dynamic measurement of privacy information are the 
foundation of privacy automation computing. 
56
F. Li et al.
3.4.1 
Intelligent Perception and Compressed Sensing 
of Privacy Information 
3.4.1.1 
The Intelligent Perception of Privacy Information 
The intelligent perception of privacy information is to form the privacy information 
component I in the privacy information description X for multi-modal data, and 
corresponding methods and tools need to be adopted for different types of data. 
Taking text data as an example, natural language processing methods can be used to 
segment the text into the smallest granularity; for image data, image understanding 
algorithms can be used to identify the semantics contained in the image data. 
Accordingly, based on the privacy intelligent perception algorithm, the privacy 
information components contained in it can be identiﬁed. 
The intelligent perception of privacy information can be realized through 
pre-built privacy identiﬁcation templates or privacy knowledge graph matching. 
To ensure the accuracy of privacy information perception, it is necessary to study 
privacy knowledge graph. 
3.4.1.2 
Compressed Sensing of Privacy Information 
Compressed sensing originated from the ﬁeld of information processing, also known 
as compressed sampling or sparse sampling, and is used to obtain and reconstruct 
sparse or compressible signals by ﬁnding sparse solutions of underdetermined linear 
systems. Compared to Nyquist sampling theorem, the entire signal can be recovered 
with fewer samples. Compressed sensing is based on the compressibility of signals, 
and realizes the perception of high-dimensional signals through uncorrelated obser-
vations of low-dimensional space, low-resolution, and under-Nyquist sampling data. 
In the ﬁeld of privacy awareness, the goal of compressed sensing is to enable 
information service providers to collect the least information to provide satisfying 
personalized services by leveraging the characteristics of privacy information, or to 
perform data mining and machine learning-oriented data exchange to achieve 
maximum level of desensitization while meeting data utility requirements.

3
Privacy Computing Theory
57
3.4.2 
Dynamic Measurement of Privacy Information 
The measurement of privacy information is the assignment of the privacy attribute 
vector A = {a1, a2, . . .} in the privacy information X. The key is to determine the 
speciﬁc form of the quantitative operation function σ, which changes dynamically 
with the scene, time, and the subjective opinion of the privacy information subject. A 
quantitative operation function is deﬁned as 
ak = σ ik
ð Þ = I ik; i0 
k 
H ik
ð Þ
ð3:4Þ 
Let the entropy of the privacy component ik be H(ik), it is expected that after the 
privacy preservation operation is f, ik becomes i0 
k, and the mutual information 
between the two is I ik; i0 
k = H ik
ð Þ - H ikji0 
k , then ak can be deﬁned as ak = 
I ik; i0 
k
ð
Þ
 
H ik
ð Þ  = 1 -
H ikji0 
k
ð
Þ
 
H ik
ð Þ  , and it is readily to prove 0 ≤ ak ≤ 1, which represents the 
expected degree of privacy preservation. This follows the fact that when no preser-
vation operation is performed, I ik; i0 
k = H ik
ð Þ, ak = 1; when fully preserved, 
I ik; i0 
k = 0, ak = 0. It can be seen that the quantitative deﬁnition of this privacy 
attribute component fully meets the requirements of Deﬁnition 3.4. 
To reﬂect the dynamics of the speciﬁc scene, time, and privacy subject, the 
quantization operation function σ should also include the corresponding parameter 
σ(ik; s, t, uid), where s represents the scene, t represents the time, and uid represents 
the subject of the privacy information. H ikji0 
k can be deﬁned as being related to the 
scene, time, and privacy subject. 
The quantization operation function can either be continuously assigned by the 
above analytical expression, or can be assigned discretely by specifying. The most 
ideal quantiﬁcation method of ak should be determined intelligently and dynamically 
through personalized long-term learning, that is, the expected privacy preservation 
strength of the privacy information component is obtained individually through 
intelligent methods. 
The overall sensitivity to privacy information X can be measured by applying 
weights. Let the privacy attribute vector of X be A = (a1, a2, . . ., am), and w1, w2, . . ., 
wm be the weight. If 
m 
k = 1 
wk = 1 
is satisﬁed, then the overall sensitivity is 
a = 
m 
k = 1 
wkak. Weights can also be determined and adjusted by training and learning 
according to personal preference.

58
F. Li et al.
3.5 
Design Principles of Privacy-Preserving Algorithms 
Although different types of privacy-preserving algorithms rely on different mathe-
matical theories, regardless of whether the mathematical foundations are the same, 
the design principles of privacy-preserving algorithm should design the overall 
framework of the algorithm from the perspective of algorithm modeling, and 
consider the relationship between the various links of the algorithm framework. 
Based on the framework, different preservation requirements can be parameterized, 
the law of the impact of algorithm combination on preservation effect can be 
analyzed, and a quantitative analysis system for the complexity and efﬁciency of 
the algorithm can be established. 
The design principles of privacy-preserving algorithm [2] are the basis for 
realizing the architecture design of the privacy preservation system and the trans-
fer/migration of the privacy preservation state, and also the basis for ensuring that the 
code of the privacy preservation system is relatively stable and easy to upgrade 
online. Algorithm modeling enables the requirements for different preservation 
effectiveness to be completed by changing the parameter settings while keeping 
the code structure unchanged. At the same time, only part of the module of the 
algorithm code needs to be upgraded, which improves the stability of the code. The 
analysis of the segmentalization and modularization of the algorithm model plays an 
important role in maintaining the algorithm running state and improving the paral-
lelism. Therefore, algorithm design principles are essential for the implementation of 
privacy computing. 
3.5.1 
Five Basic Design Principles for Privacy-Preserving 
Algorithm 
The privacy preservation requirements of different application scenarios and the 
types of information are very different, but certain common principles still need to be 
followed in the process of designing privacy-preserving algorithms. Following the 
idea of privacy computing, this section provides ﬁve basic design principles for the 
design of privacy-preserving algorithms [2]. 
Criterion 1: Preprocess 
The privacy information X is preprocessed to determine the data distribution char-
acteristics, value range, data privacy preservation sensitivity, expected value of the 
number of privacy operations, and empirical distribution of privacy operation 
results. For example, the expected value of the number of privacy operations 
time = f(I, A, Θ). 
Criterion 2: Algorithm Framework 
The mathematical foundation of the privacy preservation algorithm is determined, 
the speciﬁc steps of the algorithm and the combination relationship between the

steps are given, and the correlation between the privacy attribute vector and the 
privacy information vector is presented for different application scenarios and 
information categories. For instance, for application scenarios where the preserved 
information is not required to be reversible, privacy preservation mechanisms 
implemented generalization, obfuscation, anonymity, and differential privacy can 
be used. Taking differential privacy preservation as an example, under the guidance 
of Criterion 1, combined with elements such as I, A, Θ and C, Q, L, the speciﬁc 
noise-adding mechanism can be determined. 
3
Privacy Computing Theory
59
Criterion 3: Algorithm Parameter Design 
Considering the privacy and utility requirements, combined with Criterion 1 and 
2, the speciﬁc values of relevant parameters in the privacy preservation algorithm are 
determined. For example, in a differential privacy mechanism, the expected value of 
the number of privacy operations needs to be determined according to the privacy 
preservation requirements (for instance, in the Laplace mechanism, the value of the 
privacy budget ε needs to be determined), and the sensitivity needs to be determined 
according to the speciﬁc query function, the social experience value of the privacy 
operation result, under the premise of the noise addition mechanism determined in 
Criterion 2, combined with I, Θ, the noise distribution can be obtained. 
Criterion 4: Algorithm Combination 
According to the application scenario and information characteristics, the combina-
tion of different steps is realized within the algorithm, or the arrangement and 
combination of similar algorithms are realized to achieve the improvement of 
security or performance. For example, in the process of adopting differential privacy 
preservation, combined with I, Θ and the properties of differential privacy including 
combination, post-processing properties, sequential combination, and parallel com-
bination, etc., the dynamic combination of the same algorithm between steps is 
achieved. For application scenarios with complex privacy preservation require-
ments, for example, taking into account the statistical characteristics and anonymity 
of published data at the same time, it is necessary to fully consider the characteristics 
of various algorithms with similar mathematical mechanisms in the process of 
privacy information processing, and to ensure the satisfaction of complex privacy 
through the integration of the preservation requirements and improve the overall 
security and performance. 
Criterion 5: Complexity and Effectiveness Analysis for Algorithm 
Considering factors such as the number of privacy information components that 
need to be preserved, the value range of security parameters of an algorithm, the time 
and space complexity of the algorithm, and the expected value of the privacy 
preservation effect, the implementation cost of the privacy preservation algorithm 
should be thoroughly analyzed to evaluate whether the selected algorithm is suitable 
for the corresponding application scenarios.

2
60
F. Li et al.
3.5.2 
Applicability of Design Principles 
for Privacy-Preserving Algorithm 
In the following, the applicability of the above principles is illustrated by taking a 
differential privacy mechanism as an example. 
3.5.2.1 
Pre-processing 
In a differential privacy preservation algorithm, denote the privacy information as X, 
according to X, the set of constraints Θ and the set of propagation control operations 
Ψ, generate the corresponding set of privacy information vectors I = i(X, Θ, Ψ), 
analyze the distribution characteristics of I, and determine the value space of 
Φ = ϕ(I) or the set of values is Ran. According to the statistical query function 
g(.) deﬁned on I, the expected value of the query times t(.) and the social experience 
value of the query result v(.) are determined, the added noise value space or value set 
S = s(Φ, Ran, g(.), t(.)) is obtained, and the sensitivity of the statistical query 
function g(.) is calculated. For a statistical query function g(.) deﬁned on the subset 
I of D, its sensitivity is deﬁned as 
Δg = max g D1
ð
Þ - g D2
ð
Þ
k
kp 
where D1, D2 ⊆ I, D1, D2 are deﬁned as adjacent sets which are any two sets 
differing by at most one element, p ≥ 1 and p 
ℕ. 
3.5.2.2 
Algorithm Framework 
Based on the pre-processing results, privacy preservation complexity C, privacy 
preservation effect Q and other factors, the mathematical deﬁnition of the differential 
privacy mechanism is expressed as 
Pr Alg D1
ð
Þ 2
 
S
½
] ≤h .ð ÞPr Alg D2
ð
Þ 2
 
S
½
] þ δ .ð Þ  
where h(.) = h(λ, ε, κ) represents the extended privacy budget, and the parameter λ is 
a constant associate with the noise distribution, ε is related to the expected value of 
the number of queries, κ is related to the social experience value of the query result; 
δ(.) = δ(ε, κ) is the correction parameter, which is used to relax the conditions to 
make the algorithm satisfy differential privacy; D1, D2 is a pair of adjacent sets; Alg 
is a randomization algorithm. 
The differential privacy preservation algorithm framework is

.
.
3
Privacy Computing Theory
61
While Alg g
ð Þ=2v .ð Þ  
Do Alg g
ð Þ = g D
ð Þ þ  Noise μ .ð Þ, b .ð Þ, q .ð Þ
ð
Þ
 
In this algorithm, Noise (.) is the noise function set, and the generated noise 
satisﬁes the (h(.), δ(.)) - DP condition; μ(.) is the expectation of noise generation; 
b(.) is the scale parameter function used to control the range of noise distribution; 
q(.) is the utility function in the exponential mechanism, which controls the output of 
the data after noise addition and then probabilistic expectation of a certain outcome 
can be obtained. The noise distribution and algorithm parameters can be selected 
according to speciﬁc application scenarios and information categories. 
3.5.2.3 
Algorithm Parameter Design 
Based on the user’s application requirements for privacy preservation strength and 
usability, combined with factors including the value range (Ran) of the privacy 
information vector I, the expected value of the number of queries t(.), and other 
factors, the speciﬁc parameters of the noise distribution are determined. Among 
them, μ is related to the average demand of the output result; b(.) is determined by 
h(.), dataset sensitivity Δg, noise value space or value set S that is, b(.) = b(h(.), Δg, 
S); q(.) is a function of S and the social experience value of query results, that is, 
q( ) = q(S, v( )). 
3.5.2.4 
Algorithm Combination 
Differential privacy mechanism has the following composition properties. 
Post-Processing Property: If Alg1(.) satisﬁes ε ‐ DP, then for any algorithm 
(possibly random) Alg2(.), the combined algorithm Alg2(Alg1(.)) also satisﬁes 
ε ‐ DP. 
Sequential Composition: If Alg1(.) satisﬁes ε1 ‐ DP, and for any s, Alg2(s, .) 
satisﬁes ε2 ‐ DP, then Alg(D) = Alg2(Alg1(D), D) satisﬁes (ε1 + ε2) ‐ DP. 
Parallel Composition: If Alg1, Alg2, . . ., Algk are k algorithms that satisfy ε1 ‐ DP, 
ε2 ‐ DP,. . ., εk ‐ DP respectively, and D1, D2, . . ., Dk is a disjoint dataset, then 
Alg1(D1), Alg2(D2), . . ., Algk(Dk) satisﬁes max(ε1, ε2, . . ., εk) ‐ DP. 
The different steps of the differential privacy preservation algorithm can be 
combined using the above three properties when it is used to protect different 
datasets with various statistical queries. 
3.5.2.5 
Complexity and Effectiveness Analysis for Algorithm 
The differential privacy preservation algorithm adds noise into the privacy informa-
tion, so the complexity is dominated by the noise generation, and the privacy

.
.
.
.
.
k
k
k
k
preservation effectiveness also depends on the size of the noise. These are related to 
the parameters of noise generation such as dataset characteristics and dataset sensi-
tivity calculation, and can be described by the complexity of the algorithm Alg 
C(Alg) = c(Φ, Δg, h(.), δ(.), μ(.), b(.), q(.)) and the privacy preservation effect of the 
algorithm Alg Q(Alg) = Δσ(h( ), δ( ), μ( ), b( ), q( )). 
62
F. Li et al.
3.6 
Evaluation of Privacy Preservation Effectiveness 
The evaluation of privacy preservation effectiveness is the basis for decision-making 
to support information release, statistical query, and data exchange, as well as 
screening and automatic selection of privacy preservation algorithms. In a large-
scale privacy preservation system, the preservation effect evaluation of the algorithm 
can support the adaptive dynamic replacement algorithm according to the system 
requirements, while maintaining the relative stability of the system framework. 
The joint research on the matching or mapping relationship among effect evalu-
ation and algorithm preservation capability quantiﬁcation and privacy information 
perception quantiﬁcation is the privacy preservation effect evaluation required for 
privacy computing. The author of this book establishes an evaluation system for 
privacy preservation effects from ﬁve dimensions: reversibility, extended control, 
bias, complexity, and information loss. 
Deﬁnition 3.13 The privacy preservation algorithm/scheme is deﬁned as the com-
bined operation F in the privacy operation set f. After applying f on the privacy 
information vector I, each component in the corresponding privacy attribute vector 
A will approach 0 [2]. For I, A, where A = σ(I), if f 2 Fk , I' = f(I), A' = σ(I' ) makes 
kA'k < kAk, then f is called a privacy preservation algorithm, where k . k  represents a 
certain measure of the vector A, such as the L2 norm. 
Deﬁnition 3.14 Privacy preservation effect evaluation refers to the evaluation of the 
privacy attribute vector corresponding to the new privacy information vector I' after 
the different privacy preservation algorithms f is applied on the privacy information 
vector I [2]. The closer the value of σ( f(I)) is to 0, the better the effect of the privacy 
preservation algorithm. 
Let the privacy vector of the privacy information X be I, the attribute vector 
measure be kAk, and the attribute measures of applying of two privacy preservation 
algorithms f1, f2 on the privacy information are kA1k and kA2k K respectively, if 
A1 < A2 , then the privacy preservation effect of f1 is greater than f2. 
Axiom 3.4 The effect of privacy preservation is measurable [2]. 
The effect evaluation mainly includes the availability of privacy information, the 
irreversibility of privacy preservation, and the reversibility in a controlled 
environment.

3
Privacy Computing Theory
63
Availability of privacy information refers to the impact of new information on 
system functions or performance after applying privacy preservation algorithms on 
privacy information. Availability can be quantitatively measured by the mean square 
error or Hamming distance on the original data and the data with preservation. 
The irreversibility of privacy preservation means that a third party or attacker 
cannot infer the original privacy information from the privacy preservation algo-
rithm and information obtained by the third party or attacker based on their ability. 
Reversibility in a controlled environment means that a third party can fully or 
partially reconstruct the privacy preserved information with certain side information. 
According to these analysis, the author of this book abstracts the following ﬁve 
evaluation criteria for privacy preservation effect evaluation [2]. 
3.6.1 
Reversibility 
Reversibility refers to the ability of the privacy information to be recovered before 
and after the privacy preservation. Speciﬁcally, it represents the ability of an 
attacker/third party to infer the privacy information component ik from the observed 
privacy information component i0 
k. If the attacker/third party can accurately infer ik, it  
is reversible; otherwise, it is not reversible. 
For example, when data need to be released, the resistance of the selected privacy 
preservation scheme under different attacks should be ﬁrst evaluated, and then the 
privacy attribute vector is calculated according to the information to be released that 
has been processed by privacy preservation, and then obtain the degree of informa-
tion recovery of the unauthorized access and authorized information under different 
attacks can be obtained. 
Conjecture 3.1 After the reversible privacy preservation algorithm transmits the 
privacy information across the trust domain, if the privacy preservation policy does 
not match, it will cause privacy leakage. 
A reversible operation algorithm refers to an algorithm that can recover the 
original information content of preserved data, including but not limited to encryp-
tion algorithms and transformation algorithms. Irreversible operation algorithms 
include but are not limited to one-way lossy compression, Bloom ﬁlter blur, and 
mosaic algorithm. 
3.6.2 
Extended Controllability 
Extended controllability refers to the degree of matching between the receiver’s 
privacy information preservation effect and the sender’s preservation requirements 
in the process of cross-system exchange. Speciﬁcally, it captures the deviation of the 
privacy attribute component ak in the system Sys1 and the privacy attribute

-
0
ð
Þ þ
-
0
ð
Þ
component a0 
k in the system Sys2 after the privacy information X is transmitted from 
the system Sys1 to Sys2. For any k, in different systems, if ak = a0 
k, it means that the 
extended controllability is good, otherwise the extended controllability is biased. 
64
F. Li et al.
For example, users Alice, Bob, and Charles are friends with each other. Alice 
posts a piece of privacy information in WeChat Moments, and it is set to allow Bob 
to view it rather than Charles, and Bob forwards the information to his Sina Weibo 
without setting the access restrictions, which Charles will see at this point. In this 
case, the access control authority of user Alice on Sina Weibo does not match the 
access control authority in WeChat Moments. 
3.6.3 
Deviation 
Deviation refers to the deviation between the privacy information component ik 
before privacy preservation and the privacy information component i0 
k after privacy 
preservation that can be observed by an attacker or a third party. 
For example, in the case of location privacy, the physical distance between the 
user’s real location (m, n) and the location (m′ , n′ ) generated by the location privacy 
preservation algorithm (location offset algorithm) is
m 
m 
2 
n 
n 
2 . 
3.6.4 
Complexity 
Complexity refers to the cost required to run a privacy-preserving algorithm, that is, 
the cost of privacy-preserving complexity. 
For example, performing a permutation operation on a speciﬁc vector (replacing a 
speciﬁc keyword with *) requires less computational resources than performing a k
-anonymous operation (k = 30). 
3.6.5 
Information Loss 
Information loss refers to the loss of certain usability for the information owner after 
the information is disturbed, obfuscated by irreversible privacy preservation 
algorithms. 
For example, in the case of location privacy, when the user sends her true address 
to the server without any privacy preservation, and the server returns precise push 
information; if the user adopts k-anonymity, the server returns a coarse-grained push 
information to the user so that the proportion of unusable results increases, resulting 
in a certain loss of data utility.

3
Privacy Computing Theory
65
3.7 
Privacy Computing Language (PCL) 
Privacy Computing Language (PCL) [2] is used for efﬁcient and concise formal 
description of privacy information deﬁnition, desensitization, control, and other 
operations. The privacy computing language can easily support the cross-platform 
exchange of privacy information; it can help developers avoid from complex 
theoretical details, reduce the technical difﬁculties for developers, and improve the 
efﬁciency of system development, to quickly build a privacy preservation informa-
tion system; it can be used as a form of privacy preservation. The description 
language can accurately describe the operation of each link of privacy computing, 
which is convenient for the accurate expression of privacy computing theory, and 
easy for scholars to communicate with each other and developers to understand. 
3.7.1 
Privacy Deﬁnition Language 
The privacy deﬁnition language is used to describe the data type and data format of 
the privacy computation of information M and its associated integrity constraints. 
Among them, the data types mainly include bit string type, integer type, ﬂoating 
point type, character string type, logical type, table page data, metadata, web page 
data, text data, image data, audio data, video data, and so on. The privacy deﬁnition 
language is also used to describe the calculation steps of text, image, audio, video 
and other objects, including privacy information extraction, scene abstraction, pri-
vacy operation selection, privacy preservation scheme selection/design, privacy 
preservation effect evaluation, etc. The privacy deﬁnition language is mainly used 
in privacy perception, storage, exchange, destruction, and other links. The privacy 
deﬁnition language is currently under research, and it will take some time to form 
universal results. Up to date, XML (eXtensible Markup Language) and JSON 
(JavaScript Object Notation) can be used to assist in the implementation. 
3.7.2 
Privacy Operation Language 
The privacy operation language is used to describe the behavior of operating on the 
information M. The operations mainly include modular addition, modular multipli-
cation, modular exponentiation, XOR, replacement, disturbance, query, selection, 
deletion, modiﬁcation, copy, paste, cut, forward, etc. For example, “copy” is equiv-
alent to “Ctrl+C”, which means to temporarily store a copy of a ﬁle in the format of 
text, image, voice, video, or a selected part of the content in a memory area for 
subsequent pasting, forwarding, etc. Privacy operation language is mainly leveraged 
in privacy, storage, fusion, destruction and other links. The privacy operation 
language is currently under research, and it will take some time to form universal

results, but the domain-speciﬁc language can be considered to assist in the 
realization. 
66
F. Li et al.
3.7.3 
Privacy Control Language 
The privacy control language is used to describe the grant, authentication, and 
revocation of access control rights to information M, among which common rights 
mainly include selection, copy, paste, forward, cut, modify, deletion, query, etc. For 
example, “Copy” is similar to granting permission to a ﬁle in the format of text, 
image, voice, video, etc., or a selected part of its content, to be copied to support 
subsequent “Copy” operations. Privacy control language is mainly used in percep-
tion, storage, fusion, exchange, destruction and other links. The privacy control 
language is currently under research, and it will take some time to form universal 
results. At this stage, domain-speciﬁc languages such as XML or JSON can be 
considered to assist in the implementation. 
3.8 
Determination and Traceability of Privacy 
Infringement 
In the process of privacy exchange, although there is an extended control mecha-
nism, there are always attackers trying to ﬁnd ways to bypass or tamper with the 
control mechanism, or to perform control operations incompletely according to the 
extended control requirements. There is no technology providing foolproof preser-
vation up to date. Therefore, from the historical law of the entire technology 
development, the preservation of privacy and the abuse of privacy are contradictory 
evolutionary processes. Therefore, a mature privacy computing system should 
include privacy determination and traceability of infringement. 
In the privacy information system, realizing the judgment of privacy infringement 
is the basis of automatic evidence collection, and it is also an important key 
technology to block the proliferation of privacy infringement. The judgment tech-
nology needs to support online and ofﬂine implementation. The judgment of privacy 
infringement is to determine whether there is a violation of constraints and control 
strategies in the traceability record of privacy information according to the judgment 
criteria of privacy infringement; The operation is recorded in the audit control record 
Ω of privacy information in a non-tamperable manner, which provides a basis for 
judgment, evidence collection, and tracking. Judgment needs to be combined with 
tracing and traceability research to build an organically integrated overall mecha-
nism, rather than two separate and unrelated technologies.

s
3
Privacy Computing Theory
67
In the framework of privacy computing, privacy infringement and forensics exist 
in various steps. The traceability and evidence collection of privacy infringement 
mainly include three parts: judgment, evidence collection, and traceability. 
3.8.1 
Traceability and Forensics Framework of Privacy 
Infringement 
Based on the privacy computing framework, this book abstracts the features and 
processes of privacy infringement and integrates them into each step of the privacy 
computing framework. Figures 3.3 and 3.4 show the traceability and forensics 
framework of privacy infringement behavior tracking [2]. 
3.8.1.1 
Privacy Information Extraction 
When information M is generated, its privacy information is extracted or marked 
through the computational analysis of semantic logic, and the privacy information 
vector I, the generalized positioning information set Γ and the audit control infor-
mation set Ω are obtained, and the privacy attribute vector A can be calculated. This 
stage is mainly used to deﬁne privacy information. 
3.8.1.2 
Scenario Description 
The scenario of where the information comes from should be described, and the set 
of constraints Θ and the set of propagation control operations Ψ. This stage provides
● Cost of privacy 
preservation C
● Effectiveness of privacy 
preservation Q
● Profit and loss ratio of 
privacy disclosure L
● Privacy information 
vectorӏ
● Privacy attribute vector Α
● Location information set 
Γ
● Audit control information 
set Ω
● Privacy computation 
operation set F
● Constraint condition set 
Θ
● Privacy computation 
operation set F
● Audit control information 
set Ω
● Dissemination control 
operation set Ψ
● Constraint condition set 
Θ
● Dissemination control 
operation set Ψ
● Computing semantic 
logic analysis
● Self-forensics 
information
● Third party monitoring / 
hosting
● Monitoring data transfer 
● Multidimensional event 
reconstruction
● Privacy invasion time-
space scenario  
reconstruction 
● Multisource big data 
crosstab analysis
● Self-information 
accurately analysis 
Steps
s
t
n
e
m
e
l
E
 
n
o
it
a
r
e
p
O 
Privacy information 
extraction 
Scenario description
Privacy operation 
Solution selection or 
design 
Evaluation of privacy 
preserving effect 
Fig. 3.4 The framework of tracing evidence for privacy invasion behavior

criteria for judging privacy violations. When the above conditions are not met, the 
privacy violation is conﬁrmed.
68
F. Li et al.
3.8.1.3 
Privacy Operation 
Each privacy information component according to the scene restrictions is assigned 
with possible operations, a privacy operation set F is formed, and a propagation 
control operation set Ψ is established accordingly; the privacy operations of the 
information subject on the information is recorded, and the audit control information 
set Ω is generated or updated. Operations beyond the above two sets will also be 
judged as privacy violations. 
3.8.1.4 
Select/Design Scheme 
In this process, it is analyzed whether the operations involved in the selected/design 
scheme satisfy the privacy operation set, and whether the actions, objects, and results 
of the operations exceed the constraint condition set. The occurrence of privacy 
infringement is prevented and used as the criterion for determining privacy 
infringement. 
3.8.1.5 
Evaluation of Privacy Effectiveness 
This link includes analyzing and calculating the privacy preservation cost C, the 
privacy preservation effect Q, and the privacy leakage loss-to-income ratio L. When 
the above factors do not reach the predetermined goals, feedback and review on the 
preservation of the entire life cycle of privacy information are required. 
When a privacy infringement occurs, it is necessary to trace the source of the 
information ﬂow in the ﬁrst four steps to trace the subject of the privacy infringe-
ment. Based on the six-tuple of privacy information and third-party monitoring or 
hosting, we can deﬁne privacy information, determine privacy infringement, and 
collect evidence for abnormal behavior through the linkage of various steps in the 
privacy computing framework, ﬁnd the source of infringement, and realize trace-
ability and evidence collection. 
3.8.2 
Determination of Privacy Infringement 
3.8.2.1 
Determination Criteria Generation of Privacy Infringement 
According to the privacy preservation requirements of privacy information, the 
determination standard of privacy infringement of privacy information is generated.

Privacy preservation requirements include algorithm type, degree of privacy pres-
ervation, privacy preservation effect, etc. [3] 
3
Privacy Computing Theory
69
The criterion for determining privacy infringement of privacy information can use 
extensible markup language as a description language. The content of the criterion 
description is a set of {operation subject, operation object, operation behavior, 
operation attribute, constraint condition,. . .}, and is deﬁned on this collection logical 
expression, where the operating subjects may include information owners, informa-
tion forwarders, information receivers, information sending equipment, information 
receiving equipment, information transmission equipment, etc.; operating objects 
refer to the operated privacy information; operating behaviors include privacy 
information dissemination operations, privacy information processing operations, 
etc.; operation attributes refer to the attribute conditions that should be met to 
execute the operation behavior, including trigger conditions, environmental infor-
mation, scope of use, media types, etc., where the trigger conditions describe the 
conditions for activating the judgment of privacy infringement, such as leaving or 
entering the system boundary, leaving or enter the network boundary, before sending 
privacy information, before receiving privacy information, other custom rules, etc., 
environmental information includes role, time, spatial location, device, network, 
operating system, etc., the scope of use describes the application scenarios of privacy 
operations, such as for printing, monitor display, exchange of mobile storage media, 
network transmission, etc., media type describes the media format of privacy 
information ﬁles, such as text, picture, audio, video, hypermedia, etc.; constraint 
condition describe the authority of operation behavior, such as allowable or not 
allowable. 
3.8.2.2 
Determination of Privacy Infringement 
According to the determination criteria of privacy infringement, determining 
whether the audit control information in the six-tuple has privacy infringement 
[3]. The details are as follows: (1) Generate audit items; (2) Screen the audit control 
information according to the audit items to generate behavior forensics information; 
(3) When the behavior forensics information is credible, determine whether there is 
privacy infringement in the behavior forensics information according to the judg-
ment criteria to generate the judgment result. 
Behavioral forensics information is audit control information for selecting the 
scope deﬁned by the audit item. The steps for judging whether there is a privacy 
infringement from the behavior forensics information are as follows: (1) When the 
privacy infringement judgment standard includes the operation subject and the 
operation object, where the operation subject is in the behavior forensics informa-
tion, and the operation object is the behavior forensics information. The operation 
attributes, operation behaviors, and constraints correspond to the operation object; 
(2) When the privacy infringement judgment standard includes the operation subject 
and does not include the operation object, the operation subject is the operation 
attribute corresponding to the operation subject in the behavior forensics

information, operation behavior and constraints, etc.; (3) When the privacy infringe-
ment judgment standard does not include the operation subject and includes the 
operation object, the operation object is the operation attribute, operation behavior 
and restriction conditions corresponding to the operation object in the behavior 
forensics information. (4) Compare the operation attributes in the judgment standard 
with the environment where the operation behavior occurs in the behavior forensics 
information, and compare the obtained operation behavior and constraints with the 
operation behavior in the behavior forensics information. 
70
F. Li et al.
When one of the following conditions is met, it is determined that there is a 
privacy infringement in the behavior forensics information: (1) The operating 
environment of the operating subject in the behavior forensics information exceeds 
the obtained operational attributes; (2) The operation behavior of the operating 
subject in the behavior forensics information exceeds the constraints of the allowable 
actions. 
When all the operation behaviors of the operating subject in the behavior foren-
sics information do not exceed the operation behaviors subject to the constraints, and 
all the operating behaviors occur in an environment that does not exceed the 
obtained operation attributes, it is determined that there is no privacy infringement 
in the behavior forensics information. 
3.8.3 
Traceability and Forensics of Privacy Infringement 
To solve the key problem of spatial and temporal scene reconstruction after the 
occurrence of privacy infringement events, a practical and effective privacy infor-
mation traceability and forensics scheme [3] should be designed based on the 
forensic information recorded in the audit control information by the hexagram of 
privacy information, third-party monitoring, and cross-multi-element big data anal-
ysis. Speciﬁcally, it includes (1) Evidence sample data for generated information, 
including privacy information, traceability record information of privacy informa-
tion, and privacy infringement judgment criteria for privacy information; (2) Save 
the evidence sample data in a tuple by binding, embedding, appending, in the 
information, where the binding method is to establish a link between the privacy 
information and the evidence sample data, and the two data do not need to be stored 
in the same location; the embedding method is to store the evidence sample data in 
the custom area of the original format of the privacy information ﬁle; the appending 
method refers to modifying the ﬁle format of the original privacy information, 
modifying it into a custom ﬁle format, and adding a custom ﬁeld to store the 
evidence sample data. 
Li Fenghua et al. [3] designed a traceability and evidence collection scheme for 
image privacy infringement, including the following two stages.

3
Privacy Computing Theory
71
3.8.3.1 
The Stage of Traceability Information Record 
When the picture starts to be transmitted, the owner of the picture creates a 
traceability identiﬁcation, including privacy information, privacy information judg-
ment criteria, and traceability record information. In the process of image transmis-
sion, when each stream is transferred to a user, the traceability record function is 
used to generate traceability record information, and the user’s sharing operations, 
processing operations, and behavior occurrence environment on the privacy infor-
mation of the image are recorded in the traceability information record. 
The traceability record information is the arrangement and combination of ele-
ments in the set {operation subject, operation object, operation behavior, operation 
behavior occurrence environment}, and the mapping relationship between elements. 
The traceability record function includes mapping function, hash function, 
encryption function, signature function, etc., where the mapping function is used 
to establish a mapping relationship for the association and combination of informa-
tion such as operation subject, operation object, operation behavior, and operation 
behavior occurrence environment; hash function, encryption function, and signature 
function are used to prevent the traceability record information from being mali-
ciously tampered with or to prevent the malicious operator from denying the 
operation behavior during the forensics process. 
The scheme uses nested signatures to ensure that images are not maliciously 
tampered with and forged during the propagation process. By leverating nested 
signature, the image owner creates a traceability identiﬁer, generates the ﬁrst trace-
ability record information with a signature, and adds the record to the audit control 
information; each subsequent image forwarder checks the previous record, and then 
changes its operation behavior. The record is signed in combination with the 
previous traceability record information and its signature. This stage can be incor-
porated into the extended control mechanism to complete. 
3.8.3.2 
The Stage of Traceability and Forensics 
When an image privacy leakage occurs, a traceability chain of image privacy 
information can be obtained from the node where the situation was discovered and 
traced to the root node. Forensics specialists conﬁrm the integrity of the traceability 
identiﬁcation through layer-by-layer signature veriﬁcation, and determine whether 
there is a privacy behavior based on the traceability record information and privacy 
infringement standards. The details are as follows: (1) Determine whether there is 
any operation that violates the privacy infringement standard (or extended control 
strategy) in the traceability record information of the current node; (2) If there is no 
violation, the traceability record of the previous node will be compared. Make a 
judgment, and repeat (1) until it is conﬁrmed that the privacy infringement is 
terminated. The occurrence of a privacy infringement is determined by comparing 
the criteria for determining privacy infringement (Fig. 3.5).

72
F. Li et al.
owner
forwarder1
forwarder2
forwarderi-1
forwarderi 
forensics 
specialist 
layer-by-layer signature 
verification, determine in 
sequence whether there is 
any privacy infringement 
generate 
traceability record 
Recowner 
add traceability 
record Rec1 and 
signature 
add traceability 
record Rec2 and 
signature 
add traceability 
record Reci-1 and 
signature 
add traceability 
record Reci and 
signature 
Fig. 3.5 Traceability and evidence collection judgment 
The goal of privacy information traceability and forensics is to obtain the 
evidence sample data according to the needs of the audit, determine whether there 
is privacy infringement in the traceability record information according to the 
privacy infringement determination criteria, and generate a privacy infringement 
judgment result; trace the privacy infringement according to the privacy infringe-
ment judgment result, generate privacy infringement evidence chain. 
In addition, it is necessary to ensure the security of the privacy infringement 
evidence chain, that is, generating the audit information of the privacy infringement 
evidence chain, calculating the integrity check value of the privacy infringement 
evidence chain and the audit information, and generating digital signature for the 
user and privacy infringement evidence in the privacy infringement evidence chain. 
The audit information records the acquisition records of the forensic sample data 
in the privacy infringement evidence chain, including the judgment result of the 
privacy infringement, the information source, the forensic personnel, and the han-
dler. The integrity check value is used to ensure that the privacy infringement 
evidence chain and audit information are not maliciously tampered with, digital 
signatures are used to ensure that the privacy infringement evidence chain is not 
maliciously tampered with during the submission process. 
3.9 
Privacy Information System Architecture 
If the theory of privacy computing cannot guide the development of privacy 
information systems, or cannot solve the practical requirement of high efﬁciency 
and high concurrency of privacy information systems, then the ubiquity and appli-
cation of privacy computing will be limited, and the theory of privacy computing 
will not be well applied and developed. Therefore, the study on privacy information 
system architecture is crucial to privacy computing, which is also one of the main 
features that distinguishes privacy computing proposed by the author of this book 
from ubiquitous computing, urban computing and other “computing” concepts. 
The privacy information system architecture is framework for guiding the imple-
mentation of privacy computing. Based on the complete realization of privacy

computing, it should also have the characteristics of ubiquitous dissemination of 
privacy information, adaptive preservation of scenarios, high concurrent privacy 
information interaction, and efﬁcient distribution. 
3
Privacy Computing Theory
73
3.9.1 
Privacy Information System Architecture 
The privacy information system architecture [2] includes semantic extraction, scene 
abstraction, privacy information transformation, privacy information fusion, privacy 
operation selection, privacy preservation scheme design/selection, privacy preser-
vation effect evaluation, etc., as shown in Fig. 3.6. 
In Fig. 3.6, F is the set of privacy computing operations, A is the privacy attribute 
vector, Γ is the set of generalized positioning information, Ω is the set of audit 
control information, Θ is the set of constraints, Ψ is the set of propagation control 
operations, X is the normalized privacy information, f is the privacy calculation 
operation, and f X 
is the output of operation f on the normalized privacy 
information. 
The privacy component I is obtained from the information M through semantic 
extraction, and the setting is abstracted according to the setting description such as 
time and equipment used. The privacy component is measured to form the privacy 
attribute vector A, and the constraints are determined according to the setting 
mapping relationship, and the control operation set is propagated, the set of audit 
control information, and the set of generalized positioning information, the six-tuple 
X described by the privacy information is obtained. After the different privacy 
information Xi is normalized, the privacy information fusion operation can be 
performed to obtain the fusion privacy information X. For privacy information X, 
an corresponding privacy operation from the privacy operation set is selected to 
obtain a privacy preservation scheme and evaluate the privacy effectiveness of the 
preserved information. If the requirements are met, the scheme is applied; otherwise, 
the setting is reset according to the constraints, and the privacy operation is changed, 
and the privacy preservation scheme is designed/selected again until the goal is 
achieved. 
Privacy information 
transformation 
Integration of 
privacy 
information 
Privacy preserving 
scheme 
selection/design 
Evaluation of 
privacy 
preserving effect 
Feedback 
Normalization 
Normalization 
Normalization 
Application 
Reset the scene 
according to Θ
According to A, Θ, Ψ, Ω, 𝚪, 
adjusting the normalization method 
Plaintext 
information 
M 
i1 
i2 
i3 
F 
Scenario
Semantic extraction
extraction 
Time 
Device 
... 
Scenario description 
Privacy operation 
selection 
Change f , redesign and 
select privacy preserving 
scheme 
X1 
X2 
X3 
X1 
f´ 
f´´ 
f
X2 
X3 
f(X)
X 
Fig. 3.6 Privacy information system framework

74
F. Li et al.
3.9.2 
Example for Description and Usage of Privacy 
Information 
3.9.2.1 
Privacy Document Description Information Generation 
and Usage for Text Information 
The following examples illustrate the description and usage of privacy information 
[4]. Suppose the document information is the text information, on page 11, paragraph 
3, and line 2, represented by T, where the content is “lucy and willy go to 
Zhongguancun for dinner”, the creator of the text information is represented as 
creator, the encoding size of Chinese characters in the text information is 2 bytes, 
the ASCII code size of English characters is 1 byte, and the privacy information can 
be processed according to the following nine steps. 
Step 1: The privacy information vector generation unit receives the text infor-
mation T, the type of information is determined according to the ﬁle identiﬁer, and 
the text information T is split according to the semantic features to obtain seven 
semantically inseparable privacy information components lucy, sum, willy, go, 
Zhongguancun, eat, and meal, so the privacy information vector can be expressed 
as I = (id, lucy, and, willy, go, Zhongguancun, eat, meal). 
The generalized positioning information vectors γ1, γ2, γ3, γ4, γ5, γ6, γ7 of the 
privacy information components lucy, he, willy, go, Zhongguancun, eat, and rice in 
the text information T are obtained respectively. In this example, page number, 
paragraph, line number, start bit, and end bit are used to represent positioning 
information, and font and font size are used to represent attribute information. 
Suppose γ1 = (P11, S3, R2, 0, 4; Times New Roman, size four) means that the 
privacy information component lucy is located in the second line of the third 
paragraph on page of the text 11th information T, the start bit is 0, the end bit is 
4, and the font and font size are Times New Roman and size 4, respectively. 
Similarly, γ2, γ3, γ4, γ5, γ6, γ7 can be obtained to generate a generalized positioning 
information set Γ = {γ1, γ2, . . ., γ7}. 
The audit control information vector ω1, ω2, . . ., ω7 of the privacy information 
components lucy, sum, willy, go, Zhongguancun, eat, and rice are obtained respec-
tively. During the initialization phase, the audit control information vector can be 
empty. If it is not empty, it is assumed that ω1 = (UID1, copy, forward; UID2, 
forward, modify), indicating that the privacy information component lucy has been 
accessed and operated by the users uniquely identiﬁed as UID1 and UID2, where 
“UID1, copy, forwarding” indicates that the privacy information component lucy is 
copied and forwarded by the user UID1; when the privacy information component is 
propagated to the user UID2, the forwarding and modiﬁcation operations are 
performed. Similarly, ω2, ω3, . . ., ω7 can be obtained, and the audit control infor-
mation set Ω = {ω1, ω2, ω3, . . ., ω7} is generated. 
Step 2: The constraint condition set generation unit receives the privacy infor-
mation vector I = (id,lucy,and,willy,go,zhongguancun,eat,meal), the creator of the 
ﬁle sets the privacy information component ik as the corresponding constraint

condition vector θk = (uk, tk, dk, nk) according to the application scenario of the 
privacy information component, where k is a positive integer taking values from 
1 to 7, u is the visitor list, t is the access time, d is the access device, and n is the 
network 
identiﬁer. 
For 
example, 
suppose 
θ1 
= 
(UID1, 9  
:  
00
-
21 : 00, 9EF0038DE32, 10.10.30.13) means that only users with user ID as UID1, 
time interval as 9:00–21:00, device ID as 9EF0038DE32, and network IP address as 
10.10.30.13, can access the privacy information component lucy. Similarly, the 
creator can set constraint vector θ2, . . ., θ7 for other privacy information components. 
Therefore, the set of constraints can be denoted as Θ = {θ1, θ2, . . ., θ7}. 
3
Privacy Computing Theory
75
Step 3: The privacy attribute vector generation unit receives the privacy infor-
mation vector I = (id, lucy, and, willy, go, Zhongguancun, eat, din) and the con-
straint set Θ = {θ1, θ2, . . ., θ7}, and generates the privacy attribute component of 
the information vector is a1, a2, . . ., a7 by pre-marking or quantifying the degree of 
privacy preservation. Since lucy is a name and in an unpreserved state, its privacy 
attribute component can be assumed to be 1 (assuming that the range of the privacy 
attribute component is 0 to 1, the smaller the privacy attribute component, the higher 
the degree of preservation of the corresponding privacy information component). 
Since different privacy information components can be combined according to 
syntax or semantics, the resulting combination result will generate new privacy 
attribute components. For example, after the privacy information vector lucy and 
willy are combined, they may contain information such as their social relationship or 
intimacy, so the privacy attribute component of the combined result will be smaller 
than the original attribute component. The privacy attribute vector A = (a1, a2, . . ., 
a7, . . ., a127) = (1, 1, 1, 1, 0.4, 0.3, 0.5, . . ., 1, 1, 0.8, 1, 1, 1, 1, 1) corresponding to the 
privacy information vector and the privacy information vector combination (i1, i2, i3, 
i4, i5, i6, i7, i1i2, i1i3, i1i4, . . ., i1i2i3i4i5i6i7) is calculated according to the pre-marking 
or privacy preservation degree quantiﬁcation operation function. 
Step 4: The propagation control operation set generation unit receives the privacy 
attribute vector A = (a1, a2, . . ., a7, . . ., a127) and the constraint condition set 
Θ = {θ1, θ2, . . ., θ7}, and generates the privacy information vector I = (id, lucy, 
sum, willy, go, Zhongguancun, eat, meal) and its combined set of propagation con-
trol operations is Ψ = {ψ1, ψ2, . . ., ψ127}. During the initialization phase, the 
propagation control operation vector can be empty. If it is not empty, it is assumed 
that the propagation control operation vector of the privacy information component 
lucy is ψ1= (delete, copy), indicating that the operation that the privacy information 
component lucy in the text information M can perform is “copy, delete”. Similarly, 
the propagation control operation vector ψ2, . . ., ψ127 is obtained to form the 
propagation control operation set Ψ = {ψ1, ψ2, . . ., ψ127}. 
Step 5: The privacy document description information generation unit receives 
the privacy information vector I, the generalized positioning information set Γ, the 
audit control information set Ω, the privacy attribute vector A, the constraint 
condition set Θ and the propagation control operation set Ψ, and generates the 
privacy document description information P of the text information M, and encap-
sulate P into the index table in text information M. The privacy document description 
information P can also be encapsulated in other locations in text information M.

76
F. Li et al.
Step 6: When different users access the text information T, according to their 
identity information Receiver, the set of constraints Θ = {θ1, θ2, . . ., θ7} and the 
access threshold generation function, the access threshold vector B = (b1, b2, . . ., b7) 
is generated. In this example, it can be assumed that the generated access threshold 
vector is (b1, b2, . . ., b7) = (0.9, 0.3, 0.9, 0.3, 0.6, 0.1, 0.1). 
Step 7: The difference (a1, a2, . . ., a7) between the privacy attribute component 
(b1, b2, . . ., b7) and the access threshold component ck = ak - bk is calculated, where 
k is a positive integer taking values from 1 to 7, and the difference set C = (c1, 
c2, . . ., c7) = (0.1, 0.7, 0.1, 0.1, -0.2, 0.2, 0.4) is obtained. Since c1, c2, c3, c4, c6, c7 is 
greater than or equal to zero, the accessing entity can access the privacy information 
components (i1, i2, i3, i4, i6, i7) = (lucy, and, willy, go, eat, and eat), while other pri-
vacy information components are not accessible. 
Step 8: The access entity selects the privacy information component lucy as the 
operation object to perform the copy operation. Since the propagation control 
operation vector is ψ1 = (delete, copy), it is determined that the user can perform 
the copy operation on the privacy information component lucy. 
Step 9: The audit control information vector ω1 = (Receiver, copy) is updated 
according to the selection and copy operations performed on the privacy information 
component lucy in step 8. Since the copy operation of the privacy information 
component i1 does not change the generalized positioning information vector of 
each privacy information component, the generalized positioning information vector 
is not updated. 
3.9.2.2 
Privacy Document Description Information Generation 
for JPEG Image 
Suppose the document information is a JPEG image, the privacy information can be 
processed according to the following nine steps. 
Step 1: The privacy information vector generation unit receives the JPEG image, 
the type of information is determined according to the ﬁle identiﬁer, and the JPEG 
image content is divided to obtain three semantically inseparable pixel sets 
according to the semantic features and image semantic segmentation technology, 
which can be used as privacy information component i1, i2, i3, which can be 
expressed as I = (id, car, tree, person) [4]. 
The generalized positioning information vector γ1, γ2, γ3 of the privacy informa-
tion components car, tree, and person in the JPEG image are obtained respectively. 
In this example, the corresponding generalized positioning information vector can be 
represented by the coordinates and the number of pixels of the privacy information 
component in the JPEG image. 
The audit control information component ω1, ω2, ω3 of the privacy information 
component I = (id, car, tree, person) are obtained respectively. The initialization of 
the audit control information vector and the access process are similar to those in text 
information.

3
Privacy Computing Theory
77
Step 2: The constraint condition set generating unit receives the privacy infor-
mation vector I = (id, car, tree, person), the corresponding constraint condition 
vector θk = (uk, tk, dk, nk) can be set for the privacy information component ik 
according to the application scenario of the privacy information component, where 
k is a positive integer taking values from 1 to 3, the content and meaning of each item 
are similar to those in text information. 
Step 3: The privacy attribute vector generation unit receives the privacy infor-
mation vector I = (car, tree, person) and the constraint condition set Θ = {θ1, θ2, θ3}, 
and generates the privacy attribute components of each privacy information vector 
through a preset or privacy preservation degree quantization operation function. 
Different privacy information components can be combined with each other 
according to syntax or semantics, and the combined result will generate a new 
privacy attribute component. For example, the combination of privacy information 
vector cars and people may reveal their economic capabilities. According to the 
quantiﬁcation operation function of the degree of privacy preservation, the privacy 
attribute vector A = (a1, a2, . . ., a7) = (0.8, 1, 0.4, 1, 0.3, 0.8, 0.1) corresponding to 
the privacy information vector and the privacy information vector combination (i1, 
i2, i3, i1i2, i1i3, i2i3, i1i2i3) can be calculated successively. 
Step 4: The propagation control operation set generation unit receives the privacy 
attribute vector A = (a1, a2, . . ., a7) and the constraint condition set Θ = {θ1, θ2, θ3}, 
and generates the propagation control operation vector (ψ1, ψ2, . . ., ψ7) of each 
privacy information component and its combination according to the operation 
discriminant function judgment or manual marking. During the initialization 
phase, the propagation control operation vector can be empty. If it is non-empty, it 
is assumed that the propagation control operation vector ψ1 = (copy, forward) 
indicates that the operation that the privacy information component car in the 
JPEG image can be performed is “copy, forward”. Similarly, the propagation control 
operation vector ψ2, . . ., ψ7 is obtained to form the propagation control operation set 
Ψ = (ψ1, . . ., ψ7). 
Step 5: The privacy document description information generation unit receives 
the privacy information vector I, the generalized positioning information set Γ, the 
audit control information set Ω, the privacy attribute vector A, the constraint 
condition set Θ, and the propagation control operation set Ψ, and generates the 
privacy document description information of the JPEG image as P, which is written 
into the identiﬁer EOI in the JPEG image, and the written description information of 
the privacy document does not affect the display and usage of the JPEG image. The 
privacy document description information P can also be written to other locations in 
the JPEG image. 
Step 6: When different users access JPEG images, the access threshold vector 
B = (b1, b2, b3) is generated according to their identity information Receiver, 
constraint set Θ = {θ1, θ2, θ3} and access threshold generation function. In this 
example, it can be assumed that the generated access threshold vector is (b1, b2, 
b3) = (0.7, 0.3, 0.5). 
Step 7: The difference between the privacy attribute vector (a1, a2, a3) and the 
access threshold vector (b1, b2, b3) is calculated by equation ck = ak - bk, where k is a

positive integer taking values from 1 to 3, and obtain the difference set C = (c1, c2, 
c3) = (0.1, 0.7, -0.1). 
78
F. Li et al.
Since both c1 and c2 are greater than zero, the access entity can access the privacy 
information component (i1, i2) = (car, tree) independently, while the privacy infor-
mation component person cannot access it normally, so the combination including 
the privacy information component person is not accessible. Since the privacy 
attribute component corresponding to the i1, i2 combination is greater than b1, b2, 
the content of the JPEG image that the access entity can access is “car, tree”. 
Step 8: The access entity selects the privacy information component car as the 
operation object to perform the copy operation. Since the propagation control 
operation vector is ψ1 = (delete, copy), it is determined that the user can perform 
the copy operation on the privacy information component car. 
Step 9: The audit control information vector ω1 = (Receiver, copy) is updated 
according to the copy operation performed on the privacy information component 
car in step 8. Since the copy operation of the privacy information component car 
does not change the generalized positioning information vector of each privacy 
information component, the elements in the generalized positioning information 
set will not be updated. 
3.9.2.3 
Privacy Document Description Information Generation for MP4 
Video File 
The privacy information vector generation unit receives the above MP4 video ﬁle, 
determines the type of information according to the ﬁle identiﬁer, and splits the MP4 
video content to obtain m semantically inseparable frame sets according to the 
semantic features and segmentation algorithm [4]. Each frame set can include one 
or more time-continuous frames, where the frame set can be regarded as the privacy 
information component i1, i2, . . ., im, and the privacy information vector can be 
expressed as I = (id, i1, i2, . . ., im). 
Once the privacy vector is obtained, the subsequent processing steps are similar to 
those in text and JPEG information. 
3.9.3 
Application Examples of Privacy Computing 
3.9.3.1 
Privacy Computing Example in Information Exchange Between 
Different Domains Within the System (Example 1) 
Taking social network as an example [2], the set of registered users of social network 
1  is  U = {u1, u2, . . .}, and each user may have multiple circles of friends, which is 
marked as M = {m1, m2, . . .}. Users can share information ﬁles through the circle of 
friends, and the set of ﬁles is marked as D, where mi ⊆ 2U , that is, the circle of friends 
consists of multiple users, and the function of user circle of friends is deﬁned as

3
Privacy Computing Theory
79
①
②③
④
③
④
Multi-media file 
Userr 
Userr 
d 
User
friend circle
User
friend circle 
User 
Privacy tag 
Social network 1 
Fig. 3.7 Information interaction between different domains within the system 
hasCircle : U →2M
ð3:5Þ 
Which represents the circle of friends owned by the user, where mui,j represents 
the j-th circle of friends that the user ui has, then the following property holds 
mui,j 2 hasCircle ui
ð Þ
ð3:6Þ 
As shown in Fig. 3.7, the user u1 generates and publishes the multimedia ﬁle d in 
his circle of friends mu1,1, and the friend u2 2 mu1,1 in his circle receives the ﬁle and 
forward the ﬁle to the user u3 2 mu2,1 in his circle of friends. The process can be 
divided into the following four steps. 
Step 1: The privacy preservation requirement PRu1 of the user u1 and the scene 
description information SSu1 are preset, and the privacy label Tagu1 is generated 
through the privacy label generation function PrTag; then the tag function 
tagAppend is used to mark Tagu1 to the multimedia ﬁle after the user’s u1 operation, 
and then generate and upload the ﬁle d →u1 marked by the user’s, where the privacy 
preservation requirements need to be set by the user, including the preservation 
effect of the privacy information in the ﬁle, the scope of the ﬁle allowed to ﬂow, the 
permitted access entities, the permitted operation set, etc. The user privacy preser-
vation requirement set is denoted as PR = {pr1, pr2, . . .}, and the function for privacy 
preservation requirement settings is deﬁned as 
setPR : U →2PR
ð3:7Þ 
Once the user’s privacy preservation requirements is described, the user ui’s 
privacy preservation requirements can be expressed as

Þ
ð
80
F. Li et al.
PRui = setPR ui
ð Þ
ð3:8Þ 
The setting description information needs to be analyzed and obtained from the 
system, including the generation time of the ﬁle, the ﬁle generator, the operation of 
the ﬁle, etc., denoted as SS = {ss1, ss2, . . .}, and the function used to generate the 
setting description information is deﬁned as 
genSS : D × U →2SS
ð3:9Þ 
which indicates the description information of the user’s current setting generated by 
the system, then the setting description information of the user ui is 
SSui = genSS d, ui
ð
Þ
ð3:10Þ 
At the same time, the ﬁle operation function is deﬁned as 
operFile : U × D →D
ð3:11Þ 
which indicates that the user obtains a new ﬁle after an operation on the ﬁle, and the 
original ﬁle d after ui’s operation is denoted as. 
dui = operFile ui, d
ð
Þ
ð3:12Þ 
The privacy tag generation function prTag, which represents the privacy tag 
generated by a user subject in ﬁle forwarding, is deﬁned as 
prTag : U × D × 2PR × 2SS →X × 2F
ð3:13Þ 
Let Tag ≜ X × 2F denote the generated token, where X is the six-tuple of privacy 
information, and F is the set of privacy operations. Tagui represents the privacy mark 
generated by the user ui and is deﬁned as 
Tagui = prTag ui, d, setPR ui
ð Þ, genSS d, ui
ð
Þ
ð
3:14Þ 
The tag function tagAppend is deﬁned as 
tagAppend : D × Tag →D
ð3:15Þ 
which indicates that each time a user passes through the ﬁle transfer process, its 
generated tags will be marked on the original ﬁle, and iterate successively, and is 
deﬁned as

ð
Þ
ð
Þ
ð
Þ
3
Privacy Computing Theory
81
d →u1 = tagAppend dui, Tagui = tagAppend operFile ui, d
ð
Þ, Tagui 
ð3:16Þ 
Step 2: The mark information of the multimedia ﬁle d →u1 veriﬁed, and whether 
the user u2 satisﬁes the u1 constraint condition set Θ and the propagation control 
operation set Ψ are determined, etc. If satisﬁed, the operation on multimedia can be 
done within the allowable range, such as downloading, editing, etc. Since the ﬁle is 
allowed to be downloaded by friends in the circle of friends, the user u2 can 
download it and obtain d →u1. 
Step 3: User u2 can modify, add, delete the multimedia ﬁle obtained from u1 
within 
the 
allowable 
range 
of 
operation, 
and 
obtain 
a 
new 
ﬁle 
du1,u2 = operFile u2, d →u1
ð
Þ, where du1,u2 represents that the ﬁle d is obtained after 
the ﬁle is operated by u1 ﬁrst and then by u2, and will forward it again to the user u3 
or upload it to other circles of friends where he is. At this time, the system will mark 
the privacy label u2 of the user u1 on the multimedia ﬁle obtained from 
Tagu2 = prTag u2, d, setPR u2 , genSS d, u2 
, and get 
d →u1 →u2 = tagAppend operFile u2, d →u1
ð
Þ, Tagu2
ð3:17Þ 
Step 4: Verify if the tag information is d →u1 →u2. If the privacy requirements in 
each tag Tagu1 and Tagu2 are met, the user u3 can see the multimedia ﬁle d →u1 →u2 
published by the user u2 in the social network 1, and download or perform other 
operations within the allowable range. 
In the process of information ﬂow described above, if there is an abnormal 
behavior, such as a user’s operation or other behavior exceeding the agreed set Θ 
of constraints or the set Ψ of communication control operations, it can be determined 
that a privacy infringement has occurred. Therefore, it is necessary to trace the 
source by analyzing the privacy label information contained in the multimedia ﬁle, 
reproduce the privacy infringement scene according to the information such as the 
audit control information set Ω, trace back to which subject and which operation 
violated the abnormality, and effectively manage and control the ﬂow of privacy 
information throughout the life cycle, and achieve traceability and evidence collec-
tion of privacy infringements accordingly. 
3.9.3.2 
Privacy Computing Example in Autonomous Interaction Across 
Closed Information Systems (Example 2) 
The information interaction between two closed systems in the same enterprise 
ecosystem [2] is shown in Fig. 3.8. User u1 marks the generated multimedia ﬁle 
d according to formula (3.16) to obtain d →u1, and publishes it in the circle of friends 
mu1,1 in his social network 1. The server receives d →u1 and forwards the ﬁle to the 
social network 2 in the same ecosystem if it satisﬁes the privacy preservation 
requirements PRu1 of u1.

82
F. Li et al.
①
②
④
④
Userr 
d 
User
friend circle 
User 
Multi-media file 
Privacy tag 
Social network 1
Social network 2 
Fig. 3.8 Information interactions across closed information systems 
In Example 2, the information can be exchanged in different information systems 
without the help of users, so the step 3 in Example 1 is not required. The ﬁle 
published by this social network 2 is d →u1, which can be downloaded by user u2 
and is readable to user u2. 
In the same enterprise ecosystem, when both social network 1 and 2 have a 
common user, it is also readily to explain the autonomous interaction of users’ 
information in different closed information systems. 
3.9.3.3 
Privacy Computing Example in Information Interaction 
Between Open Systems (Example 3) 
The information interaction between open systems or between open systems and 
closed systems [2] is shown in Fig. 3.9. User u1 in open system forum Z marks the 
generated multimedia ﬁle d according to formula (3.16) to obtain d →u1, and pub-
lishes it in the system. Another user u2 in the same system obtains the information 
and operates the ﬁle d under the condition that the privacy preservation requirement 
PRu1 of user u1 is satisﬁed, and generates a new ﬁle d →u1 →u2 with its own label 
according to formula (3.17). If the privacy preservation requirements of u2 and u1 are 
PRu1 and PRu1, publish the information on the open system forum T, or log in to the 
social network 2 in the closed system, and forward it to the closed system user u3. 
The difference between example 3 and 1 lies in step 4, when the forward system is 
an open system, all users in this system can access the forward ﬁle d →u1 →u2; When 
it is a closed system, only other users who forward information in the same circle of 
friends and meet the restrictions in the relevant privacy label can access the forward 
ﬁle d →u1 →u2, other users such as u4 does not have the right of access.

3
Privacy Computing Theory
83
①
②
③
……
④
④
Forum Z
Forum T 
Userr
Userr
User
User 
d 
Multi-media file 
Privacy tag 
Fig. 3.9 Information interaction between open systems or open system with closed system 
3.10 
The Academic Connotations of Privacy Computing 
and Data Security 
3.10.1 
The Academic Connotations of Privacy Computing 
The academic connotation of privacy computing can be speciﬁcally divided into 
nine directions: privacy computing framework, extended control, privacy awareness, 
dynamic metrics, iterative on-demand desensitization, preservation effect evalua-
tion, multi-copy complete deletion, traceability and forensics, privacy computing 
language. On this basis, it can be divided into 37 research points: iterative transfer of 
control policies, operation constraints, quantitative mapping of protection capabili-
ties, privacy feature extraction, privacy components, privacy attribute vectors, sce-
nario identiﬁcation, leakage risk assessment, general framework of algorithm, 
desensitization primitives, rules of algorithm combination, differential privacy, 
localized differential privacy, personalized differential privacy, privacy budget, 
k-anonymity, l-diversity, t-closeness, de-identiﬁcation, obfuscation, scrambling, 
permutation, generalization, replacement, suppression, data synthesis, usability, 
algorithm complexity, desensitization effect evaluation, deletion effect evaluation, 
privacy mining, self-storage of operation record, transfer of ownership, infringement 
determination, infringement forensics, evidence cross-certiﬁcation, trail mining.

84
F. Li et al.
3.10.2 
The Academic Connotations of Data Security 
The academic connotation of data security is speciﬁcally divided into eight direc-
tions: conﬁdential computing, trusted computing, computing over encrypted data, 
secure multi-party computing, access control, data disaster backup, data governance, 
authentication. On this basis, it can be subdivided into 40 research points: trusted 
execution environment, homomorphic encryption, searchable encryption, exchange-
able encryption, property-preserving encryption, remote authentication, trusted 
integrity metric, trusted migration, oblivious transfer, secret sharing, threshold 
cryptology, obfuscation circuit, zero-knowledge proof, commitment protocol, pri-
vate set intersection, oblivious random oracle, permission management, autonomous 
access control, mandatory access control, role-based access control, attribute-based 
encryption access control, behavior-based access control, cyberspace oriented access 
control, identity identiﬁcation, identity-based encryption, digital signature, multi-
factor authentication, cross-authentication, off-site disaster recovery, secure storage, 
erasure correcting code, secure deletion, data cleansing, data classiﬁcation and 
grading, compliance detection, message identiﬁcation, data validation, traceability, 
data audit, ﬂow control. 
3.10.3 
Privacy Computing Versus Traditional Privacy 
Preservation, Data Security, and Other Related 
Solutions 
In order to clearly and scientiﬁcally deﬁne the academic connotation of privacy 
computing, clarify the differences in academic connotation among different solu-
tions, and help understand and judge what kind of technology is truly privacy 
computing, we propose 18 dimensions of comparison criteria. The comparison of 
privacy computing with traditional privacy preservation, data security, and other 
solutions is shown in Table 3.1. 
The comparison items in the table are described as follows: 
1. During data computation and usage, we are concerned about information leak-
age and illegal access. Privacy computing utilizes extended control and desen-
sitization. 
Traditional 
privacy 
protection 
methods 
use 
desensitization. 
Ciphertext computing (homomorphic encryption) is the computation on the 
encrypted data to get the ciphertext computing result, and then by decryption 
to get the computing result in plaintext. Secure multi-party computation protects 
the information involved in the computation from disclosure through protocols. 
Data does not leave the domain during the federated learning computation. 
Conﬁdential and trusted computing rely on the trustworthiness and controlla-
bility of the computing environment to prevent the leakage of information. 
Access control is to prevent information leakage caused by unauthorized access

Table 3.1 Privacy computing versus traditional privacy preservation, data security, and other related solutions
Comparison items
Computation
Communication
Exchange
Privacy
Desensitization
Iterative
√
√
√
√
High
High
×
√
√
×
×
√
√
√
×
×
√
Medium
High
×
×
√
×
×
√
√
√
√
×
×
×
×
×
×
×
×
Comparison items
Full life
Extended
Differentiated
Quantiﬁcation
Assessment
Forensics
Number of
Computation
√
√
×
×
×
×
×
×
×
×
×
Small
Single level
×
×
×
×
×
×
×
×
×
Small
Single level
×
×
×
×
×
×
3
Privacy Computing Theory
85
Computing
Need
Data not 
Privacy computing 
(data 
utilizing) 
√ 
overhead 
Low 
overhead 
Low 
specialized 
hardware 
× 
protection 
√
 
desensitization 
√
 
out of 
domain 
Traditional methods
√
Low
Low
×
√
×
√
×
√ 
(k-anonymity、DP etc.) 
Homomorphic encryption 
Secure multi-party
√
High
High
×
√ 
computation 
Federated learning 
Conﬁdential computing
√
Low
Low
√ 
Trusted computing
√
Low
Low
√
×
√
√ 
Access control
√
Low
Low
√
√ 
Data 
Privacy computing 
out of 
domain 
cycle
control 
√
 
protection 
√
 
and mapping 
√
√
√
 
users 
Massive 
granularity 
Object level 
Traditional methods
√
×
×
×
×
Massive
Single level 
(k-anonymity、DP etc.) 
Homomorphic encryption 
Secure multi-party
×
×
×
×
×
2 ~ 3
Single level 
computation 
Federated learning 
Conﬁdential computing
×
×
×
×
×
Small
Single level 
Trusted computing
×
×
×
×
×
Small
Single level 
Access control
×
√
Partly support 
×
Partly support
Massive
Single level

86
F. Li et al.
to information through access policy control. Therefore, these technologies are 
equivalent in terms of leakage prevention. 
2. Privacy computing, traditional privacy preserving methods, conﬁdential com-
puting, trusted computing, and access control perform computing on plaintext 
based on policy control or trusted environments with low computational over-
head. Federated learning is a distributed model training paradigm, which is not 
related to privacy preservation. Its computational overhead is the complexity of 
model training, and the moderate computational overhead is evaluated as the 
complexity of its own operations, not the privacy-preserving complexity. In 
contrast, ciphertext computation (homomorphic encryption) and secure multi-
party computation rely on complex ciphertext computation algorithms or com-
plex protocols with high computational overhead. 
3. Privacy computing, traditional privacy preserving methods, conﬁdential com-
puting, trusted computing, and access control are executed within a single closed 
environment or do not require the exchange of intermediate information and 
have low communication overhead. Ciphertext computation (homomorphic 
encryption) large ciphertext extensions, secure multi-party computation, and 
federated learning require complex multi-round interactions and high commu-
nication overhead. 
4. Conﬁdential computing and trusted computing require dedicated hardware 
support, such as CPUs and TPMs, while privacy computing, traditional privacy 
preserving methods, ciphertext computing, secure multi-party computation, 
federated learning, and access control do not require dedicated hardware 
support. 
5. Privacy computing and traditional privacy preserving methods can prevent 
information leakage during information exchange in ubiquitous interconnection 
scenarios due to desensitization of information. Federated learning exchanges 
intermediate results and parameters in the training process, leading to privacy 
leakage. Secure multi-party computing has security protection for the interaction 
information during the computation process and does not leak the exchanged 
information. Ciphertext computing, conﬁdential computing protects the infor-
mation of interaction by encryption, and there is no information leakage. Trusted 
computing, access control does not have protection measures for the interacted 
information during the exchange process, and there is information leakage. 
6. Traditional desensitization methods scramble or distort information and cannot 
achieve privacy protection. Privacy computing can support privacy protection 
through extended control technology. Ciphertext computing, secure multi-party 
computing, federated learning, conﬁdential computing, trusted computing, and 
access control have no desensitization of information, no distortion of informa-
tion, and support for privacy protection. 
7. Privacy computing and traditional privacy preserving methods support desen-
sitization of private information; none of the other techniques desensitize 
information.

3
Privacy Computing Theory
87
8. Iterative extended control in privacy computing supports iterative desensitiza-
tion of privacy information during ubiquitous sharing; no other technique 
supports iterative desensitization. 
9. All technologies such as privacy computing support data protection within 
system boundaries, and are equivalent in terms of information protection. 
10. Privacy computing and traditional methods can be applied to protect informa-
tion shared across domains because they desensitize information, while other 
techniques are only suitable for intra-domain information protection and will no 
longer provide protection once the information crosses the domain. 
11. Privacy computing and access control support full lifecycle privacy protection; 
other technologies support privacy information protection only at one stage of 
the lifecycle. 
12. Extended control is an important feature of privacy computing, and access 
control partially supports extended control. No other technology supports 
extended control for ubiquitous sharing of information. 
13. Privacy computing supports differentiated desensitization of the same informa-
tion in different application systems or different application scenarios through 
differentiated desensitization control policies, thus providing differentiated pro-
tection, which is not available in any other technology. 
14. Privacy computing supports quantitative metrics of protection capability and 
protection effectiveness, and performs quantitative mapping of protection capa-
bility and protection effectiveness when sharing information across systems to 
support consistency in privacy protection effectiveness across information sys-
tems. No other technology supports this feature. 
15. Privacy computing supports the evaluation of algorithmic protection capabilities 
and privacy protection effectiveness. No other technology supports this feature. 
16. Privacy computing enables ubiquitous, on-demand, real-time privacy operation 
compliance determination and privacy infringement forensics by binding pri-
vacy operation auditing log with privacy information; no other technology 
supports this feature. 
17. Privacy computing, traditional privacy preserving techniques and access control 
support a large number of users due to the simplicity of algorithmic state 
management and support for high concurrency. Conﬁdential computing and 
trusted computing are limited by hardware resources, ciphertext computing has 
high computation overhead, federated learning has high computation, and 
communication overhead many interactions, they can only support a small 
number of users. Secure multi-party computing is limited by multi-party pro-
tocols, which currently typically support only two or three parties. 
18. Privacy computing is to impose differentiated control on different privacy 
components of the speciﬁed information, granularity can reach the object 
level. Other techniques uniformly process the speciﬁed information without 
differentiating privacy components, thus only supporting a single level of 
protection for the speciﬁed information.

88
F. Li et al.
3.11 
Chapter Summary 
This chapter proposes the deﬁnition of privacy computing, introduces the framework 
of privacy computing, important features, design principles of privacy-preserving 
algorithm, evaluation of privacy preservation effectiveness, privacy computing 
language, the determination and traceability of privacy infringement, privacy infor-
mation system architecture, and presents several preliminary privacy computing 
examples, etc., with the goal of deﬁning privacy computing scientiﬁcally and 
rigorously from an academic perspective, and determining that the fundamental 
principle of privacy computing is the formal description, measurement, privacy 
preservation operation, preservation effect evaluation, and extended control of 
privacy information, rather than considering the concepts of secure multi-party 
computing and homomorphic encryption based on cryptography to protect data 
security as privacy computing. On the basis of clarifying the academic connotations 
of privacy computing and data security, we also make a comparative analysis of 
privacy computing versus traditional privacy preservation, data security, and other 
related solutions across 18 dimensions. However, the extension of the concept of 
privacy computing may be taken into account in the future. 
Privacy computing theory should be thorough, universal, and adaptable. Its 
theoretical basis, axiomatic system, key technologies, and implementation methods 
are in early stage and need further elaboration and improvement. The original 
innovation of the theoretical system of privacy computing needs to form a long-
term and stable research ﬁeld, and can effectively solve the privacy preservation 
problems faced by the exchange and extensive sharing of privacy information in the 
ubiquitous environment, and guide and support the implementation of the privacy 
information system. 
References 
1. Li, F.H., Li, H., Jia, Y., et al.: Privacy computing: concept, connotation and its research 
trend. J. Commun. 37(04), 1–11 (2016) 
2. Li, F.H., Li, H., Niu, B., et al.: Privacy computing: concept, computing framework, and future 
development trends. Engineering. 5(6), 1179–1192 (2019) 
3. Li F H, Li H, Niu B, et al. A method, device, and system for traceability and evidence collection 
of privacy information: 201811272731.6[P]. (2019-04-30) [2021-03-01] 
4. Li F H, Hua J F, Li H, et al. A method, device, and system for processing of privacy information: 
201711487461.6[P]. (2018-04-20)[2021-03-01]

Chapter 4 
Privacy Computing Techniques 
Fenghua Li 
, Hui Li 
, and Ben Niu 
One of the core goals of privacy computing is to guide the research of privacy-
preserving algorithms from the perspective of “computing”, and to continuously 
explore and iterate on the basic theory of privacy-preserving algorithms in response 
to the characteristics of multi-modality of privacy information, diversiﬁcation of 
applications, and dynamic differences in preservation requirements. In practical 
applications, there is a need for a quantitative evaluation system and evaluation 
method for the effectiveness of privacy-preserving algorithms, so as to facilitate the 
promotion and application of privacy-preserving algorithms in the industry. 
Based on the idea of privacy computing, the framework of privacy computing and 
the design principles of privacy-preserving algorithms, in order to effectively sup-
port the evaluation of privacy preservation effectiveness, realize a relatively stable 
privacy preservation system, and make the system easy to upgrade online, this 
chapter introduces the theoretical basis of privacy-preserving algorithms, typical 
privacy-preserving algorithms, extended control of privacy information, and the 
applications of privacy computing in multi-party data security computing, federated 
learning, machine unlearning, logistics, data trading, and other ﬁelds. 
F. Li (✉) · B. Niu 
Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China 
e-mail: lifenghua@iie.ac.cn; niuben@iie.ac.cn 
H. Li 
School of Cyber Engineering, Xidian University, Xian, Shaanxi, China 
e-mail: lihui@mail.xidian.edu.cn 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
F. Li et al., Privacy Computing, https://doi.org/10.1007/978-981-99-4943-4_4
89

90
F. Li et al.
4.1 
Theoretical Basis of Privacy-Preserving Algorithms 
Privacy-preserving algorithms need to be based on mathematical theory. In this 
section, based on an overview of common privacy-preserving algorithm theories, we 
focus on analyzing the correlation between privacy computing and other related 
theories such as probability theory and information theory, and explore the design of 
privacy-preserving algorithms by drawing on and integrating the ideas of both. In the 
future research process, it is necessary to constantly develop and improve the 
theoretical foundation applicable to privacy computing. 
4.1.1 
Probability Theory and Privacy Computing 
4.1.1.1 
The Correlation Between Probability Theory and Privacy 
Computing 
Probability theory is a branch of mathematics that studies the quantitative laws of 
random phenomena, and it is a discipline that studies the possibility of events. 
Although the occurrence of an event in a random experiment is accidental, the 
results of large number of repeated random experiments under the same conditions 
often exhibit signiﬁcant quantitative laws. The core idea of applying probability 
theory in the ﬁeld of privacy preservation is that it hides real information among 
several candidate information, making it impossible for attackers to accurately locate 
the real information from the candidate information. Common privacy preservation 
techniques based on probability theory include k-anonymity [1], l-diversity [2], t-
proximity [3], differential privacy [4], etc. 
k-Anonymity 
In order to preserve the privacy information of users involved in the publishing 
process of databases and data tables, Sweeney et al. [1] proposed k-anonymity. Each 
row of a two-dimensional data table is an individual data record, including the 
identity information and other corresponding attribute information. Each column 
of the table describes the value of a certain attribute of the individual on a certain 
determined set. Next, this section provides the formal deﬁnition of quasi-identiﬁer. 
Deﬁnition 4.1 Let B(A1, A2, . . ., An) be a data table and (A1, A2, . . ., An) be the ﬁnite 
set of attributes of table B. If  (Ai, . . ., Aj) ⊆ (A1, A2, . . ., An), for several data record 
sets t 2 B, t(Ai, . . ., Aj) denotes the value of attribute (Ai, . . ., Aj) of  t. 
Deﬁnition 4.2 Quasi-identiﬁer. Given a ﬁnite set of users U and a data table T(A1, 
A2, . . ., An) containing individual users in U, and U 2 U′. Denote the mapping

fc : U → T, fg : T → U′, and if ∃pi 2 U, fg( fc(pi)(QT)) = pi, then QT is a set of quasi-
identiﬁers of T. 
4
Privacy Computing Techniques
91
Table 4.1 Medical record
Name
Zip code
Gender
Age
Disease 
Hu Yi
710068
Male
22
Cardiovascular 
Huang Er
710071
Male
23
Pneumonia 
Wang san
710069
Female
27
Enteritis 
Liang Si
100088
Female
47
Gastric cancer 
Luo Wu
100094
Female
42
Gastritis 
Yang Liuyi
100010
Female
56
Gastric ulcer 
Yang Liuer
200010
Male
39
Pneumonia 
Wang qi
200027
Male
36
Pneumonia 
Yan Ba
200238
Female
34
Pneumonia 
Table 4.2 k-Anonymized 
medical records with zip code, 
age, and gender attribute with 
k = 3 
Name
Zip code
Age
Gender
Disease 
*
710*
20–29
Male
Cardiovascular 
*
710*
20–29
Male
Pneumonia 
*
710*
20–29
Male
Enteritis 
*
100*
>40
Female
Gastric ulcer 
*
100*
>40
Female
Gastric cancer 
*
100*
>40
Female
Gastritis 
*
200*
30–39
Male
Pneumonia 
*
200*
30–39
Male
Pneumonia 
*
200*
30–39
Male
Pneumonia 
Deﬁnition 4.3 k-anonymity. Given a ﬁnite set of users U, a dataset RT(A1, A2, . . ., 
An) about U, and its quasi-identiﬁer QIRT, RT(A1, A2, . . ., An) satisﬁes k-anonymity if 
and only if an arbitrary value sequence of RT(QIRT) occurs at least k times in RT 
(QIRT). In other words, if 8u 2 U, u(QIRT), |{v| v(QIRT) = u(QIRT), v 2 U}| ≥ k, RT  
(A1, A2, . . ., An) satisﬁes k-anonymity. 
Table 4.1 is a medical record, where the name is an explicit identiﬁer, and the zip 
code, gender, and age are quasi-identiﬁers to be anonymized, while the disease is a 
sensitive attribute. 
The k-anonymized medical record with k = 3 is shown in Table 4.2. Table 4.2 
removes the name, suppresses the zip code, generalizes the age, and individually 
replaces the gender. The records in the entire table are divided into three groups, and 
each group is called an equivalence class. The number of records with the same 
quasi-identiﬁer in an equivalence class is greater than or equal to 3. The success 
probability of inferring the sensitive attribute of a user with k-anonymity is 1/k.

92
F. Li et al.
l-Diversity 
For the k-anonymity protected data in Table 4.2, the sensitive attribute of the last 
three records is pneumonia. If an attacker knows that Yan Ba is in the data table and 
his zip code starts with 200, he can infer that Yan Ba has pneumonia. This type of 
attack is known as homogeneity attack and background knowledge attack. Due to 
the above shortcomings of k-anonymity, Machanavajjhala et al. [2] proposed l-
diversity in 2007. l-diversity means that there are at least l sensitive attribute values 
that are well represented in an equivalence class, and for a data table, l-diversity 
needs to be satisﬁed in every equivalence class. 
Deﬁnition 4.4 Equivalence Class. Given a ﬁnite user set U, a dataset RT(A1, A2, . . ., 
An) about U, and its quasi-identiﬁer QIRT, 8u 2 U, u[QIRT], a subset {v| v[QIRT] = u 
[QIRT], v 2 U} of the dataset is called a u[QIRT] equivalence class. For example, 
there are three equivalence classes in Table 4.2. 
Deﬁnition 4.5 An equivalence class is l-well represented if it contains at least l ≥ 2 
different sensitive values, satisfying the requirement that the l most frequent values 
have approximately the same occurrence frequency. For example, in Table 4.2, the 
ﬁrst two equivalence classes are 3-well represented. 
Deﬁnition 4.6 If every equivalence class in a data table is l-well represented, then 
the data table is said to satisfy l-diversity. 
The third equivalence class in Table 4.2 has only one sensitive value. If Table 4.2 
is further modiﬁed to Table 4.3, then Table 4.3 satisﬁes 3-anonymity and 2-diversity. 
l-Diversity cannot completely preserve user privacy from being leaked, because 
in a real dataset, attribute values are likely to be skewed or semantically similar, 
while l-diversity only guarantees diversity without considering the semantic simi-
larity of attribute values. Therefore, l-diversity is vulnerable to similarity attacks. For 
example, in the second equivalence class of Table 4.3, which is 3-well represented, 
the three sensitive attribute values are gastritis, gastric ulcer, and gastric cancer, so at 
least the subject of the data can be known to suffer from gastric disease. In addition, 
biased attacks against l-diversity may also cause privacy leakage. For instance, in a 
dataset containing information about a certain disease, the number of people with
Table 4.3 3-Anonymity, 
2-diversity anonymized medi-
cal records 
Name
Zip code
Age
Gender
Disease 
*
710*
20–29
Male
Cardiovascular 
*
710*
20–29
Male
Pneumonia 
*
710*
20–29
Male
Enteritis 
*
100*
>40
Female
Gastric ulcer 
*
100*
>40
Female
Gastric cancer 
*
100*
>40
Female
Gastritis 
*
200*
30–39
Male
Pneumonia 
*
200*
30–39
Male
Pneumonia 
*
200*
30–39
Male
Arthritis

ð
and without the disease in a certain equivalence class accounts for 50% respectively, 
thus satisfying 2-diversity. However, if it is known that the overall proportion of 
people with and without the disease in the normal dataset is 1% and 99%, respec-
tively, then if an individual is known to be in this equivalence class, there is a 50% 
probability of getting sick, which has already resulted in a privacy leakage.
4
Privacy Computing Techniques
93
t-Proximity 
In order to resist semantic attacks and biased attacks to provide stronger privacy 
preservation, Li et al. [3] proposed t-proximity. t-proximity allows published data to 
satisfy k-anonymity, but also requires that the difference between the distribution of 
sensitive attribute values within equivalence classes and the overall distribution of 
sensitive attribute values in the anonymous table does not exceed t. Based on l-
diversity, t-proximity considers the distribution of sensitive attributes and requires 
that the distribution of sensitive attribute values in all equivalence classes should be 
as close as possible to the global distribution of that attribute. The deﬁnition of 
distribution distance is not unique and can be deﬁned as difference distance, i.e. 
D P, Q
½
] = 
n 
i = 1 
1 
2 pi - qi
ð
Þ
4:1Þ 
It can also be deﬁned as the KL distance, i.e. 
D P, Q
½
] = 
n 
i = 1 
pi log pi 
qi 
ð4:2Þ 
Differential Privacy 
In order to preserve individual privacy in a dataset, Dwork et al. [4] proposed 
differential privacy. Differential privacy requires that each single element in the 
dataset has a limited impact on the output, which prevents attackers from inferring 
the participation information of a speciﬁc individual from the observed query results, 
thereby ensuring that the privacy information of individuals cannot be inferred from 
the query results. 
Deﬁnition 4.7 Differential Privacy [4]. For a random algorithm M, Pm is the set of 
all values that can be output by algorithm M. If for any pair of adjacent datasets 
D and D′, any subset Sm of Pm, the algorithm M satisﬁes

ð
94
F. Li et al.
Pr M D
ð Þ 2  Sm
½
] ≤eε Pr M D0
ð
Þ 2
 Sm
½
]
4:3Þ 
then the algorithm M satisﬁes ε-differential privacy, where the parameter ε is the 
privacy budget. 
Although differential privacy technology can provide strict and quantiﬁable 
privacy preservation, there is an inherent ﬂaw that data administrators need to set a 
privacy budget to control the acceptable level of privacy leakage. Speciﬁcally, every 
time the differential privacy preservation algorithm is executed, a certain amount of 
privacy leakage will occur, and the privacy budget controls the degree of privacy 
leakage by limiting the number of algorithm executions. 
4.1.1.2 
Calculation Method for Optimal Query Times 
The authors of this book take linear query and Laplace differential privacy mecha-
nism as examples, and use probability theory to derive the upper limit of the number 
of queries [5]. 
Deﬁnition 4.8 Linear Query. Assuming a dataset D = (x1, x2, . . ., xn), a linear query 
f(D) applied to the dataset is a type of queries with the following form: 
f D
ð Þ = a1x1 þ a2x2 þ ⋯þ anxn
ð4:4Þ 
where ai 2 R. 
Deﬁnition 4.9 Laplacian mechanism. Given a query function f and dataset D, the 
Laplace mechanism outputs 
M D
ð Þ = f D
ð Þ þ  r
ð4:5Þ 
where r follows the Laplacian distribution Lap(0, Δf/ε), and Δf is the global sensi-
tivity of the query function. 
Besides, since the distribution of most data in practice follows a normal distri-
bution, it is assumed that the dataset follows a normal distribution N(μ, σ2 ), and the 
parameters μ and σ2 can be estimated using parameter estimation methods, which 
will not be elaborated here. 
Query Frequency on a Continuous Dataset 
For continuous datasets that follow a normal distribution, linear queries applied to 
the dataset still follow a normal distribution. Therefore, the output of the Laplace 
mechanism is the convolution of the normal distribution (query result) and the 
Laplace distribution (noise). Due to the complexity of the convolution process, the

normal-Laplace distribution is generally used to represent the probability distribu-
tion NL(μ, σ2 , α, β) of the output result, where μ and σ2 are the parameters of the 
normal distribution, and α and β are the parameters of the Laplace distribution. 
Given the probability distribution of the output result, the mutual information I(Y; Z) 
between the perturbed output result Z and the true result Y can be used to quantify the 
privacy information leaked in the output results. 
4
Privacy Computing Techniques
95
To analyze the query frequency for security, Deﬁnition 4.10 and Deﬁnition 4.11 
provide the most aggressive queries and the weakest individuals respectively. 
Deﬁnition 4.10 The most aggressive queries. The most aggressive query refers to a 
query that maximizes the mutual information between the actual query results and 
the perturbed results. 
Deﬁnition 4.11 The weakest individual. The weakest individual refers to the 
individual with the smallest amount of self-information, that is, the weakest indi-
viduals are the ones in the dataset who are most vulnerable to privacy leakage. 
Based on this deﬁnition, a numerical conclusion can be drawn for the secure 
query frequency. When the attacker uses the most aggressive query, the attacker still 
cannot obtain the privacy of the weakest individual, and the obtained query fre-
quency is the safest in theory. The conclusion for the query frequency n is: 
n ≤ 
min 
xi2D log 
1 
p xi
ð Þ  
H Za
ð
Þ - 1 
2 log a2 
1 þ ⋯þ a2 
n - H rð Þ
ð4:6Þ 
where H(Za) is the differential entropy of the perturbed result of the most aggressive 
query, and H(r) is the differential entropy of the Laplace noise. 
Query Frequency on a Discrete Dataset 
As the discrete dataset only contains discrete data, the true value of the query result is 
limited to integers, which inevitably makes it easier for attackers to infer the true 
value. On discrete data, the conclusions of secure query results are basically the same 
as those of continuous data, but there are also differences. Firstly, parameter 
estimation is used to analyze the μ and σ2 of the dataset, and the difference is that 
the calculated result here is for continuous distribution, and the result needs to be 
discretized when calculating the mutual information. This requires studying the 
interval length of discretization, and the genetic algorithm [6] can be used to ﬁnd 
the optimal interval length Δ, which ﬁts the original dataset best. 
With the given interval length Δ, the distribution of the true query result Y still 
follows a discrete normal distribution. Theorem 4.1 gives the discrete interval length 
Δf of Y.

96
F. Li et al.
Theorem 4.1 For a discrete normal distribution Q with a discrete interval length of 
Δ, the query function f(D) = a1x1 + ⋯ + anxn follows a normal distribution with a 
discrete interval length of gcd(a1, . . ., an)Δ. 
According to the relationship between differential entropy on continuous ran-
dom variables and Shannon entropy on discrete random variables, it can be 
approximately calculated that the entropy of query results on discrete datasets is 
H(Y) = h(Y) +  log (Δ), and the conclusion that similar secure query frequency is 
n ≤ 
min 
xi2D log 
1 
p xi
ð Þ  
H Za
ð
Þ - 1 
2 log a2 
1 þ ⋯þ a2 
n - H rð Þ
ð4:7Þ 
The difference from continuous data is that it is H(Za) = h(Za) + log (Δf) here, 
while there is no log(Δf) term in the continuous case. 
4.1.1.3 
Design Principles of Privacy-Preserving Algorithm Based 
on k-Anonymity 
Pre-processing 
In k-anonymity, let the dataset be X. In the pre-processing stage, it is necessary to 
specify the identiﬁers, quasi-identiﬁers, and sensitive data of X. According to the 
constraint set Θ and the propagation control operation set Ψ, a corresponding set of 
privacy information vectors I = i(X, Θ, Ψ) is generated. Analyze the distribution 
characteristics Φ = ϕ(I) of  I, and combine the speciﬁc privacy preservation require-
ments and the social experience values v(.) of the value range, and obtain the speciﬁc 
value range of k. In general, 2 ≤ k ≤ 30. 
Desensitization 
For a given dataset X, combined with the speciﬁc value range of k, ﬁrst delete all 
identiﬁers, and then generalize or suppress the quasi-identiﬁers to ensure that the 
attribute values of the quasi-identiﬁer columns in X for multiple records are the same. 
The set of rows with the same quasi-identiﬁer column attribute values is called an 
equal set or an equivalence class. k-anonymity requires that for any row of records, 
the number of records in its corresponding equivalence class is not less than k, that is, 
at least k - 1 records have the same quasi-identiﬁed column attribute value as the 
record.

4
Privacy Computing Techniques
97
Algorithm Combination 
Considering the sequence of different links occur in a complete event, when different 
links occur in series, the overall anonymity is the product of the anonymity of each 
link, that is, serial k-anonymity improves the anonymity effect as k increases; On the 
contrary, when different processes occur in parallel, the overall anonymity is the 
worst anonymity effect among the processes. 
Algorithm Complexity and Efﬁciency Analysis 
The essence of the k-anonymity algorithm is to hide the real information among the 
perturbation items. Therefore, its complexity and privacy preservation effect mainly 
depend on the authenticity and quantity of the perturbation items, which are related 
to factors such as the generation method of the perturbation items and the back-
ground knowledge of the attacker. 
4.1.1.4 
Probability Theory-Based Privacy Computing Method 
Drawing on the ideas from probability theory, relevant computational methods are 
deﬁned for different aspects of privacy computing such as measurement, 
de-identiﬁcation, and evaluation of effectiveness. This effectively supports the 
design and implementation of privacy-preserving algorithms, with the following 
speciﬁc methods. 
Measurement Function Design 
With the idea of probability theory to quantify the degree of information hiding, the 
quantization operation of the privacy attribute component Δk in Sect. 3.1.2 is deﬁned 
as. 
ak = f 1 
k 
þ Δk; 0 < ak ≤1
ð4:8Þ 
Where, f(.) is the transition/transfer function from local privacy preservation effec-
tiveness to global privacy preservation effectiveness, and is the correction value used 
to correct the privacy attribute components in combination with social experience 
values. Speciﬁc application examples can be found in Sect. 4.2.1. This measure can 
be used to measure the sensitivity of privacy components and can also serve as a 
quantitative indicator of the de-identiﬁcation ability of privacy-preserving algo-
rithms based on probability theory. 
Deﬁne the quality of service in privacy preservation as.

Þ
98
F. Li et al.
QoSk = g i0 
k 
g ik
ð Þ  ; 0 < QoSk ≤1
ð4:9Þ 
Where g(.) represents the content of the required portion of the service data fed back 
by the service provider after the de-identiﬁcation privacy information component is 
submitted to the service provider, and speciﬁc application examples can be found in 
Sect. 4.2.1. This measure can be used to quantify the degree of loss of service quality 
caused by privacy preservation algorithms, so as to further optimize privacy-
preserving algorithms. 
Privacy-Preserving Algorithms 
The design of privacy-preserving algorithms can be deﬁned as a multi-objective 
optimization problem that takes into account privacy preservation effectiveness, data 
availability, and resource consumption. 
A* = arg max o A
ð Þ  
s:t: p A
ð Þ 2  P 
q A
ð Þ 2 Q
ð4:10Þ 
Where, A is a privacy-preserving algorithm, {o, p, q} is a permutation and combi-
nation of {privacy preservation effectiveness, data availability, resource consump-
tion}, P and Q are the expected values or expected ranges of data availability and 
resource consumption, and if this factor is not considered, it can be set as (-1, +1). 
Speciﬁc application examples can be found in Sect. 4.2.2. 
Evaluation of Privacy Preservation Effectiveness 
In real life, users often have differentiated privacy needs, and speciﬁc application 
examples can be found in Sect. 4.2.2. Deﬁning the optimal differential mechanism 
should meet the following requirements 
M*
PDP = arg min 
n 
i = 1 
εi - εi
ð
Þ
 
s:t:8i, εi ≥εi
ð4:11Þ 
Given a dataset containing n users, (ε1,. . .,εn) is the privacy preservation effec-
tiveness that users expect to obtain during the data publishing process. For a 
differential mechanism MPDP with differentiated privacy needs, ε1, . . . εn
ð
is the 
actual privacy preservation effectiveness that the mechanism can provide.

4
Privacy Computing Techniques
99
4.1.2 
Information Theory and Privacy Computing 
4.1.2.1 
The Connection Between Information Theory and Privacy 
Computing 
Shannon introduced a quantitative deﬁnition of the amount of information in data 
based on probability in his foundational paper of information theory “Mathematical 
Theory of Communication” in 1948 [7]. Information theory provides theoretical 
limits for channel transmission, undistorted source coding, and distorted source 
coding, and establishes undistorted source coding theorem, rate-distortion coding 
theorem, Slepian-Wolf correlation source coding theorem, channel coding theorem, 
etc. It pointed out the direction for information transmission and information repre-
sentation, promoted the technical development of source coding and data compres-
sion, channel coding, coding and modulation theory, information hiding, etc., and 
laid a solid theoretical foundation for the advances in communication, multimedia, 
network, and artiﬁcial intelligence. 
The core of information quantiﬁcation is based on probability. Taking the discrete 
dataset X as an example, let the probability distribution of each value of X be P(xk), 
1 ≤ k ≤ K, then the entropy (average self-information) of the dataset is 
H X
ð Þ = -
K 
k = 1 
P xk
ð
Þ log P xk
ð
Þ
ð4:12Þ 
Suppose Y is a dataset correlated with X, the distribution of Y is P(yj), 1 ≤ j ≤ J 
and the joint probability distribution of X and Y is P(xk, yj).Consider that the joint 
distribution of multiple random variables uniquely determines the marginal distri-
butions of the component random variables, the distribution of X and Y and the 
conditional distribution can be obtained from the joint distribution. 
P xk
ð
Þ = 
J 
j = 1 
P xk, yj ; P yj = 
K 
k = 1 
P xk, yj
ð4:13Þ 
P xkjyj = P xk, yj 
P yj 
; P yjjxk = P xk, yj 
P xk
ð
Þ
ð4:14Þ 
The mutual information and conditional entropy of X and Y are deﬁned as 
I X; Y
ð
Þ = 
K 
i = 1 
J 
j = 1 
P xk, yj log 
P xk, yj 
P xk
ð
ÞP yj 
ð4:15Þ

100
F. Li et al.
H XjY
ð
Þ = -
K 
i = 1 
J 
j = 1 
P xk, yj log P xkjyj
ð4:16Þ 
H YjX
ð
Þ = -
K 
i = 1 
J 
j = 1 
P xk, yj log P yjjxk
ð4:17Þ 
There is the following relationship between mutual information, entropy, and 
conditional entropy 
I X; Y
ð
Þ = H X
ð Þ - H XjY
ð
Þ = H Y
ð Þ - H YjX
ð
Þ
ð4:18Þ 
The entropy H(X) represents the uncertainty of X, and can be interpreted as the 
average number of bits in a symbol of random variable X. Conditional entropy H(X| 
Y) refers to the amount of uncertainty left in X given the background knowledge Y. 
I(X; Y) indicates the amount of information about X contained in the background 
knowledge Y. 
As information theoretic metrics, entropy and mutual information can be adopted 
in privacy computing to measure privacy information. The quantiﬁcation operation 
of the privacy attribute component ak in Sect. 3.1.2 can be deﬁned as. 
ak = I ik; i0 
k 
H ik
ð Þ  = 1 - H ikji0 
k 
H ik
ð Þ
ð4:19Þ 
where H(ik) represents the entropy of the unprotected privacy component ik, i0 
k is the 
protected version of ik, and I ik; i0 
k represents the mutual information between them. 
It is readily to prove 0 ≤ ak ≤ 1 by the properties of mutual information and entropy. 
This metric is not only a measure of the sensitivity of the privacy component or the 
degree of expected preservation, but also a quantitative indicator of the privacy 
preservation capability of an algorithm. 
Suppose the algorithm Alg1 outputs ik1 after the preservation of ik, Alg2 outputs 
ik2 after the preservation of ik, and the privacy attribute components are ak1 and ak2 
respectively, where ak1 < ak2 means the preservation ability of Alg1 is stronger than 
that of Alg2. Information theory can also be leveraged as theoretical tools to ﬁnd the 
optimal privacy-preserving algorithms in privacy computing. 
4.1.2.2 
An Information-Theoretic Model of Privacy Desensitization 
In privacy computing, the establishment of an information-theoretic model for 
privacy desensitization is of great signiﬁcance for applying information-theoretic 
methods to the characterization of the performance limits of privacy desensitization 
and the analysis of anti-attack ability of privacy preservation algorithms in the 
preservation effect evaluation. The information-theoretic model of privacy

desensitization and attacker’s privacy mining is shown in Fig. 4.1. The purpose of 
privacy desensitization is to convert the original privacy information x 2 X into 
desensitized data y 2 Y through desensitization operations. Suppose the probability 
distributions of privacy information and the desensitized information are P(x) and 
P(y) respectively, the mutual information between privacy information and 
desensitized information is 
4
Privacy Computing Techniques
101
∈
( | ) 
( | ) 
( │ 
)
∈ 
∈ 
∈ 
Desensitization 
channel 
Background 
knowledge channel 
Attackers Deprivate 
Fig. 4.1 An information-theoretic model for privacy desensitization and attacker privacy mining 
I X; Y
ð
Þ = 
x2X y2Y 
P x
ð ÞP yjx
ð
Þ log 
P yjx
ð
Þ
 
x ′
P yjx0
ð
ÞP x0
ð Þ
ð4:20Þ 
The privacy desensitization mechanism can be expressed as the conditional 
probability distribution P(y| x), that is, for a given original privacy information 
x 2 X the output y 2 Y can be sampled according to P(y| x). Attackers may obtain 
background knowledge z 2 Z about privacy information from other channels when 
conducting privacy mining, and the channels for obtaining background knowledge 
can be characterized by background knowledge channel P(z| x). After observing the 
desensitized information y 2 Y, the attacker tries to infer the original information 
from y 2 Y and z 2 Z and gets the inferred information x 2 X. 
Mutual information I(X; Y) and I X; X 
can be used as metrics to measure the 
level of privacy preservation. The smaller the mutual information, the stronger the 
privacy preservation. 
4.1.2.3 
Privacy-Utility Tradeoff 
By leveraging the rate-distortion function in information theory, the relationship 
between the degree of privacy preservation and the utility can be deﬁned as the 
privacy-utility tradeoff. 
The utility of desensitized information can be measured by the degree of distor-
tion. Let d(x, y) denote the degree of distortion between the original information

Þ
x and the desensitized information y, d(x, y) ≥ 0. d(x, y) can be Hamming distance, 
square Euclidean distance, or other distance functions deﬁned by the user. The 
average distortion on the set of X and Y is deﬁned as 
102
F. Li et al.
D = 
x 
y 
P x
ð ÞP yjx
ð
Þd x, y
ð
Þ
ð4:21Þ 
According to information theory, for a given original information distribution 
P(x), under the constraint that the average distortion of the desensitized information 
and original information is not greater than D, the mutual information I(X; Y) 
representing the degree of privacy preservation is a convex function of the condi-
tional probability distribution P(y| x). Thus, there exists an optimal solution P*(y| x) 
that achieves the minimal mutual information between X and Y. Therefore, the 
function between the minimum privacy leakage L and the utility D (privacy- utility 
tradeoff) can be obtained as 
L D
ð Þ = 
min 
P yjx
ð
Þ2PD x2X y2Y 
P x
ð ÞP yjx
ð
Þ log 
P yjx
ð
Þ
 
x0 P yjx0
ð
ÞP x0
ð Þ
ð4:22Þ 
where PD = 
P yjx
ð
Þj
 
x 
y P x
ð ÞP yjx
ð
Þd x, y
ð
Þ ≤D 
is the set of all transition distri-
butions that satisﬁes the distortion constraint. In principle, the privacy-utility tradeoff 
is analogous to the rate-distortion function in information theory [8]. The Blahut-
Arimoto iterative algorithm can be applied to solve the optimization problem of 
Eq. (4.22). 
4.1.2.4 
Blahut-Arimoto Algorithm 
The Blahut-Arimoto algorithm [9] is an alternating minimization algorithm [10] that 
ﬁnds the minimum distance between two convex sets. 
Given two convex sets A and B, the minimum distance dmin = min 
a2A min 
b2B d a, b
ð
 
between them, where d(a, b) is the Euclidean distance between point a and b, can be 
found by the following steps. First, take any point x 2 A, and ﬁnd the closest point 
y 2 B. Then ﬁx the point y 2 B, and ﬁnd the closest point to y 2 B in the set A. 
Iterative application of this process decreases the distance at each step. If both sets 
are convex and the distance metric satisﬁes certain conditions, we can prove that this 
alternating optimization algorithm converges to the minimum distance [9]. 
In particular, if the two sets in the alternating optimization algorithm are sets of 
probability distributions, and the distance measure is the relative entropy, the Blahut-
Arimoto algorithm will converge to the minimum relative entropy between the two 
sets of probability distributions. This optimization problem can be expressed as

4
Privacy Computing Techniques
103
L D
ð Þ = min 
P y
ð Þ  min 
P yjx
ð
Þ2PD x2X y2Y 
P x
ð ÞP yjx
ð
Þ log P x
ð ÞP yjx
ð
Þ
 
P x
ð ÞP y
ð Þ
ð4:23Þ 
The iterative process of the algorithm mainly consists of the following three steps. 
Step 1: Choose a parameter s and initialize distribution P(y) (e.g., uniform). 
Step 2: Apply P(y) to calculate the P(y| x) that minimizes L(D) at this time. 
P yjx
ð
Þ = 
P y
ð Þe - sd x,y
ð
Þ
 
y P y
ð Þe - sd x,y
ð
Þ
ð4:24Þ 
Step 3: After getting P(y | x), update P(y) by formula (4.24). 
P y
ð Þ = 
x 
P x
ð ÞP yjx
ð
Þ
ð4:25Þ 
Then go to step 2, repeat the above steps until the algorithm converges, and then 
the optimal solution P(y | x) of the privacy-utility tradeoff can be obtained. The 
Blahut-Arimoto algorithm can be applied to solve for the optimal conditional 
probability P*(y| x) in Eq. (4.22). 
4.1.2.5 
Privacy-Preserving Algorithm Design Principles Based 
on Privacy-Utility Tradeoff 
Pre-processing 
Determine the distribution of the original privacy information b and set the distri-
bution range of the desensitization information Y, deﬁne the distortion function, and 
set the utility constraint D. Apply the Blahut-Arimoto algorithm to obtain the 
optimal 
conditional 
probability 
distribution 
P*(y| x) 
subject 
to 
the 
utility 
constraint D. 
Algorithmic Framework 
For the speciﬁc privacy information x, the desensitization information y is obtained 
by sampling from P*(y| x). 
When the size of the alphabet of original privacy information dataset is large, 
solving formula (4.22) incurs high computation complexity. However, this issue can 
be mitigated by making certain restrictions on the transition models to achieve 
approximate optimality. Since the calculation of P*(y| x) can be calculated ofﬂine, 
once calculated, it can be efﬁciently applied to real-time desensitization. If D is 
changed, P*(y| x) needs to be recalculated. In practice, a set of D and the

corresponding P*(y| x) can be pre-computed, and then selected by the users 
according to their needs. The speciﬁc application of this framework in location 
and trajectory scenarios will be introduced in Sect. 4.2.3 later. 
104
F. Li et al.
4.2 
Typical Privacy-Preserving Algorithms 
According to the idea of privacy computing, the framework of privacy computing, 
the criteria of algorithm design, and relevant theoretical basis, this section introduces 
typical privacy-preserving algorithms from three aspects: privacy metric, algorithm 
framework, and effectiveness evaluation. At present, the content involved does not 
cover all categories of privacy-preserving algorithms. The team of this book will 
continue to study and complement. 
4.2.1 
Privacy-Preserving Algorithms Based on Anonymity 
In this section, taking the location-based service as the scenario, we quantify the 
component of privacy attributes in the location service and the success rate of long-
term observed attacks, following the measurement and desensitization components 
in the privacy computing framework in Chap. 3 and the probability theory method in 
Sect. 4.1.1. Based on the above, a series of location privacy preservation algorithms 
are constructed. 
4.2.1.1 
Caching-Aware Location Privacy Preservation 
As one of the most popular activities, Location-Based Services (LBSs) have 
enriched our life with many applications over recent years. With such applications, 
mobile users can easily obtain information about various Point of Interests (POIs) in 
the vicinity, e.g., nearby hospitals, restaurants, and bars. To enjoy these conve-
niences, mobile users need to submit queries to the untrusted LBS server. Since these 
queries include some personal information, such as users’ locations and the queried 
interests, the LBS server can easily infer who are doing what in which place. The 
server may track users directly or release their personal information to third parties 
such as advertisers. We thus need to pay much attention to protecting user privacy. 
The privacy-preserving scheme of mobile users in LBS can not only focus on 
protecting the privacy information in queries sent to the LBS server, but also achieve 
privacy preservation by reducing the number of queries sent to the LBS server. If 
fewer queries are submitted to the server, less location information is released, and 
hence there will be little chance of exposing user’s locations. A natural way of 
reducing the number of queries sent to the LBS server is to use caching, i.e., using 
the cached data obtained from previous queries to answer future queries.

4
Privacy Computing Techniques
105
The authors of this book proposed a caching-aware, dummy-based solution [11] 
to protect location privacy in LBSs. The basic idea is to cache the service data 
obtained for both the real location and dummy locations of the current query, and use 
the cached data to answer future queries so as to reduce the queries sent to the LBS 
server. The core of the mechanism is to propose an entropy-based privacy metric 
which takes caching into account. Based on this metric, we design caching-aware 
dummy selection algorithm (Caching-aware Dummy Selection Algorithm, CaDSA). 
When selecting dummies for a query, these algorithms not only maximize the 
privacy of the current query, but also maximize the dummies’ contribution to 
cache which in turn improves privacy preservation. 
Problem Statement 
Figure 4.2 further illustrates our basic idea, which focuses on two main factors, 
privacy (shown in Fig. 4.2a and Fig. 4.2b) and caching (shown in Fig. 4.2c and Fig. 
4.2d). In Fig. 4.2a and Fig. 4.2b, different shades of the cells represent different 
query probabilities, and cells marked with√indicate that these cells are candidates of 
dummy locations. In Fig. 4.2c and Fig. 4.2d, the shaded cells represent cached cells 
for which service data have been cached, while the blank cells have not been cached. 
Different gray degree means different data freshness of the cached service data in 
each cell. For example, darker cells represent that the service data cached on these 
cells is still fresh, and lighter cells mean that the service data may become out of date 
soon. The areas within the red dotted rectangles represent the user’s query regions 
based on the current location and the query range. 
To effectively provide k-anonymity against adversaries with side information, we 
prefer to assign dummy locations into cells with similar query probabilities (shown 
in Fig. 4.2b), rather than randomly select dummy locations (shown in Fig. 4.2a). In 
the second step, based on the obtained 14 candidates marked with√in Fig. 4.2b, the 
goal of our Caching-aware Dummy Selection Algorithm (CaDSA) is to improve the 
caching performance. Therefore, we prefer to select proper dummy locations which 
can contribute more to caching. Figure 4.2c shows an optimal case where all the 
chosen dummy locations have made maximum contributions (6 blank cells within 
the queried region) to the cache. As the result, both user privacy and cache hit ratio 
can be improved. However, CaDSA may not perform well in some cases due to two 
important factors: (1) users usually query for service data nearby, and (2) frequently 
queried cells in cache should be updated (i.e., cached again) before expiring. We 
consider these two factors to better evaluate dummies’ contributions to caching. This 
further improves cache hit ratio as well as privacy since higher cache hit ratio means 
less queries sent to the LBS server. The chosen dummy locations can be found in 
Fig. 4.2d. Here, we prefer dummy locations which are nearby and whose service data 
is out of date or close to expiry.

106
F. Li et al.
Fig. 4.2 The basic idea of CaDSA

anonymous set is measured based on information entropy: H = -
i = 1
pilbpi.
4
Privacy Computing Techniques
107
Location Privacy Metrics 
Then, we need to describe the location privacy measures used by CaDSA. This 
method is an instantiation of the basis of Formula (4.8) proposed in Sect. 4.1.1. It  is  
extended to the location privacy preservation scenario based on dummy location, 
and the individual and the global privacy metric method are obtained respectively as 
follows. 
Individual Privacy Metric 
Consider an attacker with background knowledge, which is represented by the 
probability that a location can be queried (i.e., service data is requested for this 
location). Speciﬁcally, we divide the map area into N × N cells. Each cell i has a 
probability of being queried (called query probability) which is denoted as qi. One 
query includes k locations, among which k-1 locations are dummies. pi is used to 
represent the probability that each location is a real location, and the calculation 
method is: pi = 
qi 
k 
j = 1 
qj 
. Finally, the degree of privacy preservation provided by the 
k 
Global Privacy Metric 
Individual privacy metric does not consider the effect of caching on the overall 
privacy degree. Actually, caching improves privacy since some queries do not have 
to be sent to the LBS server. To capture this effect, we deﬁne another privacy metric. 
Let us consider two types queries in the system. For a query answered by the LBS 
server, the uncertainty of the real location is calculated using individual privacy 
metric. For a query answered by cache, the LBS server obtains no information about 
the user’s real location from this query and every cell may be the real location. Thus, 
the uncertainty of the real location is deﬁned as lbN2 . Let Qcache denote the set of 
queries which are serviced by cache and Qserver denote the set of queries which are 
sent to and serviced by the LBS server. Then our second privacy metric is deﬁned as 
the average uncertainly of the real location in each query out of all queries: 
λ = 
Qcache
j
j
 
lbN2þ 
q2Qserver 
Hq 
Qserver
j
jþ Qcache
j
j
 , where Hq is the uncertainty of the real location in query. 
The cache hit ratio γ =
Qcache
j
j
 
Qserver
j
jþ Qcache
j
j. Thus, the global privacy metric can be 
calculated as: 
λ = 
q2Qserver 
Hq 
Qserver
j
j þ Qcache
j
j þ γlbN2
ð4:26Þ

108
F. Li et al.
The Framework of Caching-Aware Location Privacy-Preserving Algorithm 
Design of CaDSA 
According to the global privacy metric, the main idea of CaDSA is to select a set of 
realistic dummy locations to ensure high entropy for the current query and at the 
same time provide more contributions to cache hit ratio. If the query probability of a 
location is high, the data for this location is more likely to serve future queries, and 
can achieve higher cache hit ratio. Clearly, a dummy with high query probability 
contributes more to cache than a dummy with low query probability. The contribu-
tion of a dummy location to the cache is calculated as δ = qg, where g = 0 if this 
location is already cached and g = 1 otherwise. 
Since two objectives are considered, we formulate the dummy selection problem 
as a Multi-Objective Optimization Problem (MOP), which can be described as: 
Cdummy = arg max -
k 
i = 1 
pilbpi, 
k 
i = 1 
δi
ð4:27Þ 
where the ﬁrst objective max -
k 
i = 1 
pilbpi 
is to provide higher privacy degree for 
the current query and the second objective max 
k 
i = 1 
δi 
is to guarantee a better 
cache hit ratio. 
We solve the multi-objective optimization problem in two steps. We ﬁrst select a 
set of candidate dummies which can achieve high entropy for the current query: 
Cc = arg max -
k 
i = 1 
pilbpi
ð4:28Þ 
Then from these candidates, we further select k - 1 dummy locations which can 
contribute the most to cache: 
Cdummy = arg max 
k 
i = 1 
δi
ð4:29Þ 
Speciﬁcally, CaDSA sorts all cells according to the query probability. Based on 
the sorted cells, CaDSA selects 4k cells which have similar query probabilities to the 
user’s real location, and randomly selects 2k of them as candidate cells. Out of these 
2k cells, we further select a subset of k - 1 dummies which have the highest 
contribution to cache. Since the number of subsets Ck - 1 
2k 
is too large, we only 
consider s random subsets and select one of them which has the highest contribution 
to cache. Here s is a system parameter and s = 1000 by default. Finally, the set with

performance. It is deﬁned as D =
i = 1
2π d lr, li
ð
Þ.
freshness value of the cell, denoted by f, is de ned as f =
1 - T2, t ≤T.
Δ =
i = 1
δi
1 - D
ð
Þ 1 - F
ð
Þ.
the highest cache hit ratio is output from s sets according to the formula of 
k 
i = 1 
δi as 
the ﬁnal anonymous set. 
4
Privacy Computing Techniques
109
Design of Enhanced-CaDSA 
Based on CaDSA, the author of this book proposed Enhanced-CaDSA, which 
considers more factors, including normalized distance and data freshness. 
1. Normalized Distance. Since users usually query for POIs in vicinity and the data 
retrieved from the LBS server is cached locally around the real location, it is not 
very useful to cache cells far away from the real location. Thus, we prefer to select 
dummy locations not very far away from the real location to maintain a good 
cache hit ratio. The normalized distance between real location lr and the i-th 
dummy location li is di = d lr, li
ð
Þ
 
1 
2π
p 
e -
d - d lr,li
ð
Þ
ð
Þ2 
2 
, where d(lr, li) denotes the 
physical distance, and d lr, li
ð
Þ = 
k 
i = 1 
d lr, li
ð
Þ
 
k 
. We use the total normalized distance 
D to describe the effect of all the k - 1 dummy locations on the caching 
k - 1 p 
di 
2. Data Freshness. Since the data cached for a cell may become out of date, we 
prefer to update the cached data before it expires, especially for cells with high 
probability of being queried. More formally, let T denote the lifetime of cached 
data, and t denote the time that a cell’s data has already been cached. Then the 
ﬁ
t2 
For a query submitted to the LBS server, the service data obtained for each 
location (being it the real location or a dummy location) covers multiples cells 
around this location when the query range is larger than a cell. Considering this, 
we compute the average data freshness of those cells covered by the submitted 
locations as F = 
lk 
i = 1 
f i 
lk, where fi is the freshness value of cell i, and l indicates the 
number of cells within the query range. 
Finally, a set of dummy locations’ total contribution to cache can be deﬁned as 
k 
Evaluation results show that CaDSA can signiﬁcantly improve the level of 
privacy anonymity in terms of privacy entropy, and Enhanced-CaDSA can improve 
the cache hit ratio by optimizing dummy location selection, thus providing a higher 
level of anonymity.

110
F. Li et al.
4.2.1.2 
Location Privacy Preservation Against Long-Term Observation 
Attacks 
Problem Statement 
To solve the problem of privacy leakage in location-based services, Andres et al. 
[12] proposed geo-indistinguishability, a new concept of location privacy based on 
differential privacy. It ensures that all locations in a circular area centered on the 
user’s actual location can be output as obfuscated locations with a similar probability 
determined by the privacy budget. However, the traditional differential privacy is 
mainly used in datasets involving multiple users, while the existing location privacy-
preserving mechanism only involves a single user, so it cannot achieve the same 
performance of the traditional differential privacy used in statistical database. Thus, 
it cannot resist a variety of inference attacks (such as Bayesian inference attacks, 
optimal inference attacks, and long-term observation attacks). Among them, Bayes-
ian inference attacks and optimal inference attacks infer a user’s actual location 
based on a snapshot of one obfuscated location and additional prior information 
(such as population density, query probability, etc.). The long-term observation 
attacks indicate that a user’s behavior could be gathered and stored over a period 
of time. Thus, the frequency of occurrence as the obfuscated locations can be 
exploited by an adversary to improve the probability of uncovering the user’s actual 
location. The experimental results presented in Fig. 4.3 show that when the attacker 
collects enough obfusacted locations, the attacker can infer the actual location of the 
user with high probability. 
Fig. 4.3 The results of multiple perturbation using Geo [12] based mechanism (a) 200 times 
observation (b) 800 times observations

4
Privacy Computing Techniques
111
Design of Eclipse 
The user sends a request (including location and other information) to the server and 
obtains the corresponding service. We assume the server is an honest-but-curious 
adversary, who can carry out both short-term and long-term observation attacks. In 
order to defend against these two types of attacks, the authors of the book proposed a 
novel location privacy-preserving mechanism, referred to as Eclipse [13]. To resist 
the long-term observation attacks, it does not obfuscate the actual location every 
time, so that the occurrence times of the obfuscated locations follows a speciﬁc 
distribution. While considering the long-term observation attacks, it is important to 
ensure that the mechanism can also resist the Bayesian inference attacks and the 
optimal inference attacks. These two attacks mainly consider that the adversaries 
have global background knowledge (such as population density, query probability, 
etc.), and assume that the adversaries know the location privacy-preserving method. 
Figure 4.4 shows the system model and attack model of Eclipse. 
Pretreatment 
1. Privacy preservation requirements analysis. The main purpose of our mechanism 
is to effectively resist both the short-term observation and the long-term obser-
vation attacks while satisfying user’s QoS requirement. Speciﬁcally, Eclipse ﬁrst 
ﬁlters out a set of locations from the possible outputs according to the user’s QoS 
requirement. Then it chooses an anonymity set to bound the expected inference 
error in order to resist the short-term observation attacks. Finally, Eclipse pro-
duces an obfuscated location against long-term observation attacks in a differen-
tial and anonymous way based on the obtained anonymity set and the set of 
possible outputs. Figure 4.5 shows the three-phase framework of our Eclipse. 
2. Parameter analysis and setting. Eclipse involves several parameters, and users 
ﬁrst select the appropriate parameters based on their desired privacy preference 
and QoS requirement under different circumstances. These parameters affect the 
privacy, QoS, and system cost. The privacy needs to be measured in two 
dimensions, one is to measure the short-term privacy preservation level, the 
other is to measure the long-term privacy preservation level; QoS is reﬂected in 
the satisfaction of users with the data returned by the location service provider, 
which can be considered from two aspects: integrity and redundancy. Integrity 
refers to the proportion of valid data in the returned service data to the data 
required by the user. Redundancy refers to the proportion of invalid data in the 
returned service data to all returned data. The system cost mainly reﬂects in the 
cost of calculation, communication and storage. In summary, Eclipse needs to set 
the following parameters: the budget of differential privacy ε, the degree of the 
anonymity set k, the threshold of expected inference error Em, the user’s QoS 
requirement Q, and the sampling probability β. 
3. Set of Possible Outputs Determination. According to the QoS requirement and 
the actual location of the user, the output space is further reduced, so as to obtain 
the maximum space that can meet the user’s QoS requirement. Given an

112
F. Li et al.
Time 
. . . 
t3
t2
t1 
l3' 
Alice's home 
LBS provider 
Request 
Service data 
Short-term observation 
Long-term observation 
Attacks 
Time 
. . . 
t3t
t2t
t1 
l3l ' 
ovider 
Short-term observation 
Long-term observation 
Attacks 
System model
Threat model 
l2' 
l1' 
Time
t1 
t2 
t3 
t4
... 
Actual 
location 
l1 
l2 
l3 
l4
... 
Obfuscated 
location 
l1' 
l2' 
l3' 
l4' 
... 
Alice's home 
Sys 
Fig. 4.4 System and threat models of Eclipse

4
Privacy Computing Techniques
113
Fig. 4.5 The framework of 
Eclipse 
set of possible outputs 
determination 
actual 
location l 
anonymity set selection 
differential and 
anonymous location 
obfuscation 
QoS requirement Q 
sampling rate β 
obfuscated 
location l'
size of anonymity set k 
inference error Em 
prior information f 
privacy budget ε 
the set of possible outputs X 
the anonymity set Φ 
obfuscated location l′ and query range rs, the QoS can be calculated based on 
Formula (4.9). The calculation method of Formula (4.9) is extended to LBS, 
which is speciﬁcally expressed as: 
QoS l0, rs
ð
Þ = A l0, rs
ð
Þ \
 
A l, ro
ð
Þ
 
A l, ro
ð
Þ
- ω A l0, rs
ð
Þ - A l, ro
ð
Þ
 
A l0, rs
ð
Þ
ð4:30Þ 
where A(l′, rs) denotes the area covering all the obtained service data, A(l, ro) is the 
area covering all the service data the user needs. So the ﬁrst half of formula (4.30) 
represents completeness and the second half represents redundancy. 
Based on the Formula (4.30), Eclipse ﬁrst ﬁnds the maximum space that can meet 
the user’s QoS requirement as the candidate set. The shape of the space is a circle 
centered around the actual location of the user. This step ﬁnds the maximum radius 
that can meet the user’s QoS requirements by increasing radius. 
Then, according to the sampling probability, the elements in the candidate set are 
sampled, and the sampling results form the set of possible outputs. The purpose of 
sampling is to prevent the attacker from guessing the maximum space, whose center 
would be inferred as the actual location. 
The Framework of Algorithm 
1. Anonymity Set Selection. Firstly, all the small cells in the whole region are taken 
as the initial set, and the Hilbert value of each element in the set is calculated, and

The expected inference error [14] refers to the deviation between the estimated
location produced by Bayesian inference attacks and the obfuscated location. The
expected inference error via the anonymity set can be calculated by:
ð
l2Φ l Φ
f lð Þ
114
F. Li et al.
then the elements in the set are sorted according to the Hilbert value of the 
element. Hilbert curve provides a mapping from a data point in a 2-D space to 
a point in 1-D space that preserves the proximity of data. 
Then, according to the ordered set and the degree of anonymity k, a set with the 
actual location as the center and of length 2k - 1 in the ordered set is intercepted. 
Based on the ordered set, we conduct k times iterations. For each iteration, it 
extracts k consecutive elements from the set, and then calculates the expected 
inference error for those k elements. The set with k elements satisfying the 
expected inference error is placed into the candidate set. 
E Φ
ð
Þ = min 
l2Φ l2Φ 
f lð Þ  
l2Φ 
f lð Þ deuc l, l
ð4:31Þ 
where f represents prior information, such as population density or query 
probability, etc. 
Finally, it randomly chooses a set with k elements from the candidate set as the 
anonymity set. 
2. Differential and Anonymous Location Obfuscation. This step generates an obfus-
cated location by combining differential privacy and k-anonymity. First, each 
location in the output space is assigned a weight based on the set of possible 
outputs, the score function and sensitivity in the exponential mechanism. The 
exponential mechanism is a common mechanism to achieve differential privacy, 
which is mainly used to protect the privacy of non-numerical information. The 
score function is used to calculate the quality of output results in the exponential 
mechanism, which can be deﬁned by users. The score function in Eclipse is 
calculated as: 
q l, l0
ð
Þ = - deuc l, l0
ð
Þ þ
 
loss l0
ð Þ
ð
Þ
4:32Þ 
where deuc(l, l′) denotes the Euclidean distance between the actual location land 
the obfuscated location l′, loss l0
ð Þ =
f lð Þ  deuc l, l0
ð
Þ. 
2 
Then, the sensitivity required in differential privacy is calculated according to 
the anonymity set. This value will affect the magnitude of the noise used in 
differential privacy. The calculation method is: 
Δq = max 
l02L 
max 
li, lj2Φ - deuc li, l0
ð
Þ þ deuc lj, l0Þ
ð4:33Þ

4
Privacy Computing Techniques
115
Fig. 4.6 The results of multiple perturbation using Eclipse (a) 200 times observation (b) 800 times 
observations 
Finally, a location in the anonymity set is randomly selected as the actual location 
for obfuscation. The probability of selecting each location as the obfuscated location 
is determined according to the weight of each location. According to the probability, 
a location in the set of possible outputs is randomly selected as the obfuscated 
location. 
Figure 4.6 shows the results of multiple rounds of obfuscation using Eclipse. 
Even if the attacker implements a long-term observation attack, they cannot predict 
the actual location of the user with a high probability. In other words, compared with 
the existing location privacy-preserving method based on Geo-indistinguishable, the 
Eclipse scheme can make use of k-anonymity to resist long-term observation attacks. 
4.2.2 
Privacy-Preserving Algorithms Based on Differential 
Privacy 
In this section, we choose the scenario of data publishing, and study the differenti-
ated allocation of privacy budget in differential privacy, and the balance between 
data privacy, model performance, and system overhead in machine learning, while 
considering the design idea of privacy-preserving algorithm based on multi-
objective optimization. These studies are based on the privacy-preserving algorithm 
design principle in Chap. 3 and the probability theory method in Sect. 4.1.1. Finally, 
we construct a series of privacy-preserving algorithms.

116
F. Li et al.
4.2.2.1 
Utility-Aware Personalized Differential Privacy Preservation 
In recent years, mobile social networks have developed rapidly and become an 
important part of people’s daily life, providing users with rich services and experi-
ence. However, the convergence of a large number of users’ real information also 
increases the risk of privacy disclosure, so the issue of personal privacy has been 
widely concerned. As a result, many privacy-preserving schemes have emerged. 
Among them, Differential Privacy (DP) is highly appreciated by academia and 
industry because of its ability to resist the attacks of using arbitrary background 
knowledge. However, it also has some limitations. For example, the existing differ-
ential technology provides privacy preservation for all users in a dataset by using the 
same privacy budget, which leads to a uniﬁed level of privacy preservation for all 
users. However, different users can have different privacy needs, where some users 
are extremely restrictive while others are relatively loose. Personalized Differential 
Privacy (PDP) [15, 16] has been proposed to address the different privacy needs for 
users, and the architecture is presented in Fig. 4.7. In terms of privacy, PDP and DP 
have the same ability to resist attacks with arbitrary background knowledge. 
Problem Statement 
In order to achieve the goal of PDP, many elaborately designed PDP mechanisms 
have been proposed. The two most common ones are the Sampling Mechanism 
(SM) and the Personalized Exponential Mechanism (PEM) proposed by Jorgensen 
et al [16] The sampling mechanism calculates the sampling probability of each 
user’s data according to the user’s privacy requirements and sampling threshold, 
and then use the traditional differential privacy algorithm on the sampled dataset to 
obtain the corresponding the obfuscated results. The personalization exponential 
mechanism calculates an output probability for each possible query result, which is 
calculated according to the user’s personalized privacy needs, and then the person-
alization exponential mechanism randomly outputs a query result according to the 
calculated probability. Although the above two PDP mechanisms consider the user’s
PDP server 
Queriers
Users 
Fig. 4.7 The architecture of PDP

personalized privacy preference and have some improvement in utility, there are still 
some limitations. In terms of utility, the existing PDP mechanisms produce query 
results with low accuracy. In addition, the existing PDP mechanism is difﬁcult to use 
in practice. In the process of using the existing mechanism, users often need to set 
relevant parameters by themselves. For example, in SM, for a user whose privacy 
budget is higher than the sampling threshold, the user will get more privacy 
preservation than the user needs, that is, too much noise is added, resulting in poor 
utility of statistical results. The core parameter in the sampling mechanism is the 
sampling threshold, but the existing scheme does not point out how to set the 
threshold. As a result, a reasonable parameter can be set only when the user knows 
the mechanism well. In PEM, the score function is the key to realize PDP. However, 
the score function used in PEM only focuses on the privacy preference of the 
element that needs to be changed to obtain the result, but does not pay attention to 
the amount of change generated by the element. It may lead to two results that are far 
apart having the same score. As a result, they have the same output probability, 
which ultimately signiﬁcantly damages the utility of data.
4
Privacy Computing Techniques
117
To address these problems, we propose an adaptive personalized differential 
privacy framework, which is referred to as AdaPDP [17]. It can achieve personalized 
differential privacy based on sampling, which can improve the utility of the output 
results of existing PDP mechanisms. To improve data utility, our proposed AdaPDP 
calculates the optimal sampling threshold by solving the optimization problem, so as 
to reduce the waste of privacy budget in each sampling process. In addition, multiple 
rounds of sampling are conducted to reuse the wasted privacy budget, thereby 
reducing the added noise. In the meantime, to reduce the difﬁculty of the scheme, 
an adaptive parameter calculation method is designed. 
Design of AdaPDP 
We present the high-level overview of our proposed framework, AdaPDP, in 
Fig. 4.8. By taking a sensitive dataset D, all the users’ privacy budgets S and 
query function f as inputs, our framework ﬁrst selects a proper noise generation 
algorithm based on the query function. Second, it calculates the optimal values of 
parameters based on the query function f and users’ privacy budgets S. Last, 
according to the selected algorithm and calculated parameters, our framework
AdaPDP 
Multi-Round 
Sampling 
Parameter 
Calculation 
Algorithm 
Selection 
result 
D, S 
f 
Fig. 4.8 The overview of AdaPDP

performs multiple rounds of sampling to obtain the query result that can satisfy the 
personalized privacy requirement.
118
F. Li et al.
Pretreatment 
1. Algorithm Selection. Our framework selects one noise generation algorithm used 
for the differentially private computation based on the query function. It selects 
from three algorithms, including Laplace-based algorithm, Exponential-based 
algorithm, and Subsample-and-aggregate algorithm according to the type of 
query function. 
Adding noise directly to the computed quantity can completely destroy its use 
value in some scenarios, such as setting a price in an auction. According to 
whether it is allowed to add noise directly to the computed quantity, these query 
functions can be classiﬁed into two categories. For functions, that allow adding 
noise directly, they can be further divided into functions with high global 
sensitivity and low global sensitivity. Our framework selects Laplace-based 
algorithm if the query function is allowed to add noise directly, and with low 
global sensitivity. Conversely, it selects Subsample-and-aggregate algorithm if 
the query function has high global sensitivity. As for the function that adding 
noise directly can completely destroy its utility, our framework chooses 
Exponential-based algorithm. Figure 4.9 shows the algorithm selection method. 
2. Parameter Calculation. In this step, our framework calculates the optimal 
values of different parameters. AdaPDP involves three parameters, including 
the termination parameter α, the weight of sampling error ωs, and the weight of 
noise error ωn. 
Calculating the termination parameter α. AdaPDP ﬁrst obtains the sampling 
errors generated by partial termination parameters, and then conducts several 
random sampling experiments according to the given termination parameters. 
The sampling errors are obtained according to the sampling results, and then the 
functional relation between the termination parameters and sampling errors is 
ﬁtted. Similarly, AdaPDP also ﬁts a functional relationship between the termi-
nation parameter and the noise error. According to the above two functional 
expressions, the relationship between the termination parameters and the total 
error is further calculated. Using the above relation, the termination parameters 
which can minimize the total error are calculated. The termination parameter is
Fig. 4.9 Algorithm 
selection

denoted as α, and the value range is [0, 1]. The termination condition of the 
AdaPDP is to compare whether the size of the current sampling set is less than 
αN, where N denotes the size of the original dataset. If it is less, the iteration 
terminates.
4
Privacy Computing Techniques
119
Calculating two weights ωs and ωn. ωs and ωn are calculated based on the 
determined α, users’ privacy budgets S, and formula (4.11). Firstly, the maxi-
mum sampling threshold that can meet the termination condition is found 
among all the personalized privacy budgets. The calculation method is: 
tα = arg max 
t2S 
i 
πi tð Þ  ≥αN
ð4:34Þ 
where πi(t) denotes the sampling probability of tuple i given sampling 
threshold t. Then, we use tα to compute two kinds of wastes: 
wasteα 
s = 
i:Si < tα 
Si 1 - πi tα
ð Þ
ð
Þ
 
wasteα 
n = 
i:Si > tα 
Si - tα
ð4:35Þ 
Based on the two weights, the total waste can be represented as 
ωswasteα 
s þ ωnwasteα 
n. 
According 
to 
the 
Inequality 
of 
arithmetic 
a þ b ≥2 
ab
p 
, we can obtain the following formula: 
ωswasteα 
s þ ωnwasteα 
n ≥2 
ωswasteα 
s ωnwasteα 
n
ð4:36Þ 
By combining the condition where the equality sign is established in formula 
(4.36) and ωs + ωn = 1, the weight of each type of budget waste can be 
calculated as: 
ωs ¼
wasteα 
n 
wasteα 
s þ wasteα 
n 
, 
ωn ¼ 
wasteα 
s 
wasteα 
s þ wasteα 
n 
ð4:37Þ 
The Framework of Algorithm 
In the i-th iteration, it ﬁrst determines a sampling threshold ti. Then, it samples tuples 
on the entire dataset according to the threshold and computes a differentially private 
estimation ri over the sampled dataset. In addition, the proposed utility-aware 
sampling mechanism calculates the remaining privacy budget in this iteration and 
uses this remaining personalized privacy budgets to compute a sampling threshold 
for the next iteration. Our proposed method repeats the iterations until the size of the 
sampled dataset is smaller than a given value, which is determined by a termination 
parameter U. The speciﬁc termination condition can be expressed as N′ < αN, where, 
N denotes the size of the original dataset, and N′ denotes the size of the sampled

where t denotes the sampling threshold, Sr denotes the remaining personalized
privacy budget, min(Sr) and max(Sr) represent the minimum and maximum
values in the personalized privacy budget, respectively. BW(t, Sr) represents the
privacy budget waste generated when t used as sampling threshold under the
remaining personalized privacy budget Sr. For a given threshold, the budget
waste can be calculated from two perspectives. Some budget wastes appear in
the data whose personalized privacy budget is less than the sampling threshold,
which causes sampling error. The other budget wastes appear in the data whose
personalized privacy budget is larger than the sampling threshold, which causes
noise
error.
Formally,
it
can
be
calculated
as
BW t, Sr
ð
Þ = ωs
i:Si
r < t
Si
r 1 - πi tð Þ
ð
Þ þ ωn
i:Si
r > t
Si
r - t ,
where
Si
r
denotes
the
dataset. Figure 4.10 illustrates the process of our proposed utility-aware sampling 
mechanism, which involves four steps. 
120
F. Li et al.
Aggregation 
t1 
t2
t1, D1 
t2, D2 
Sr 
Threshold computation 
Sampling 
Threshold computation 
Sampling 
Differentially private 
computation 
Diﬀerentially private 
computation 
r2 
r1 
D, S 
r 
tn 
rn 
tn, Dn 
Threshold computation 
Sampling 
…
Sr 
Differentially private 
computation 
Sr 
Fig. 4.10 Overview of multi-round sampling 
1. Threshold computation. According to the calculated parameters and the current 
remaining personalized privacy budget, the dataset is sampled. It ﬁrst calculates a 
sampling threshold t according to the remaining personalized privacy budget (Sr, 
at ﬁrst S) with the goal of minimizing the budget wastes. The optimal sampling 
threshold is calculated by solving the optimization problem deﬁned as:
min tBW t, Sr
ð
Þ
ð4:38Þ 
s:t: min Sr
ð
Þ ≤t ≤ max Sr
ð
Þ
ð4:39Þ

personalized privacy requirement of i-th tuple, ωs and ωn denote the weights of
two kinds of budget wastes, πi(t) denotes the sampling probability of tuple i.
4
Privacy Computing Techniques
121
2. Sampling. After calculating the sampling threshold, the utility-aware sampling 
mechanism can independently sample each tuple according to the sampling 
probability computed by Formula (4.40):
πi tð Þ = 
eSi 
r - 1 
et - 1 , Si 
r < t 
1, otherwise 
ð4:40Þ 
where t represents the sampling threshold, πi(t) represents the sampling prob-
ability of tuple i, and Si 
r represents the personalized privacy requirements of 
tuple i. 
3. The set of results generation. According to the sampling threshold and the 
sampled 
dataset, 
statistical 
result 
satisfying 
differential 
privacy 
and 
corresponding weight is generated. Then, the remaining personalized privacy 
budget is calculated based on the sampled dataset and the sampling threshold 
used, then proceed to the next iteration. The weight for each estimate can be 
calculated as ωi = niti, where ni denotes the size ni of the sampled dataset, ti 
denotes the sampling threshold. 
4. Aggregation. Once the iterations stop, a set of results can be obtained. The 
utility-aware sampling mechanism aggregates these results based on the nor-
malized weights. The aggregated result can be obtained by r = 
k 
i = 1 
ωiri, where 
k represents k statistical results after the end of the iteration, and ωi represents the 
weight of each statistical result ri. 
Experimental results show that AdaPDP can improve the utility of statistical 
results of various query functions while satisfying users’ personalized privacy 
requirements. 
4.2.2.2 
Personalized Differential Privacy Based on Exponential 
Mechanism 
Problem Statement 
We propose the Utility-aware Personalized Exponential Mechanism (UPEM) [18] to  
address the utility problem in personalized exponential mechanism. The purpose of 
UPEM is to provide a personalized differential privacy-preserving scheme for high 
utility based on personalized exponential mechanism. On one hand, UPEM realizes 
the goal of personalization by using the same score function as the personalized 
exponential mechanism. On the other hand, UPEM achieves the goal of enhancing 
utility by introducing a new score function. The goal of personalization is achieved

by taking into account the personalized privacy budget of changing elements. The 
goal of enhanced utility is achieved by taking into account quantitative changes of 
these changing tuples. In a word, UPEM achieves the goal of enhancing data utility 
while effectively realizing personalized differential privacy by considering the above 
two factors. 
122
F. Li et al.
4.2.2.3 
Design of UPEM 
The Basic Idea of UPEM 
The UPEM is directly applied to a trusted server that provides data publishing. The 
server collects the data of different users and their corresponding privacy require-
ments. When a requestor wants to obtain some statistical results of these data, the 
server uses the UPEM to publish statistics that meet the requirements of personalized 
differential privacy. The scheme generates the output probability of each possible 
results based on the query function, and ﬁnally outputs a statistical result (such as 
mean, median, variance, etc.) randomly based on that probability. In UPEM, users 
provide raw personal data to the server, and the server provides an external query 
interface. In order to ensure the personalized privacy requirements of users, the 
server will implement the personalized differential privacy-preserving method. We 
name this server as a PDP server. 
The inputs of UPEM consist of user privacy data, personalized privacy budget, 
and query function, and the output is a query result that meets user personalized 
privacy requirements. Figure 4.11 shows the general process. First, the user uploads
Fig. 4.11 The overview 
of UPEM 
Calculating the pre-score 
based on D and S 
D,S 
Calculating the final score 
based on the pre-score 
Calculating the output probability 
based on the final score 
Statistical 
result 
pre-score 
final score

personal data and its personalized privacy budget to the PDP server. It generates a 
pre-score for these possible output results by considering the privacy budget of 
changing elements based on the possible output set of the query results generated by 
the dataset and the query function. Then, based on the pre-score, the quantitative 
changes of elements are further considered to produce a ﬁnal score for the possible 
outputs. Finally, a query result is randomly output and returned to the query based on 
the output probability of each possible result. This method can produce statistical 
results with high utility and achieve the personalized differential privacy, so that not 
only users’ personalized privacy requirements are guaranteed, but also requestors 
can obtain useful statistical values.
4
Privacy Computing Techniques
123
The Details of UPEM 
The UPEM performs the following three steps. 
1. Pretreatment. This step uses the score function to calculate a pre-score for all 
possible functional outcomes based on the objective function and the user’s 
personalized privacy budget. The score takes into account the privacy budget of 
the elements required to change to produce an output. Multiple ways of pertur-
bation may exist to generate a possible query result. According to the property of 
differential privacy, the lower the privacy budget is, the higher the privacy 
preservation intensity is. The score function of the personalized exponential 
mechanism further calculates the sum of the privacy budgets of the changing 
elements for all ways, and the way with the smallest privacy budget sum would be 
used to calculate the pre-score. The speciﬁc calculation formula is: 
scorepf D, r, S
ð
Þ = max 
f D ′
ð
Þ = r 
i2D
D ′
- Si
ð4:41Þ 
where f represents the queried function, D represents the original dataset, 
M represents the user’s personalized privacy budget set, Si represents the user’s 
privacy budget, r represents the possible output result, D 
D' represents the set 
of different elements of dataset D and dataset D' . 
2. Final score calculation. Based on the pre-score, the deviation between the 
possible query results and the real results is further considered in this step, 
which realizes the personalized differential privacy and improves the utility of 
statistical results.
The ﬁnal score takes into account not only the privacy budget of the changed 
element, but also the change in the amount of the changed element. The pre-score 
function only takes into account the privacy budget of the changed element, 
resulting in two results that are far apart ending up with the same pre-score. 
Therefore, in order to further distinguish the results with the same pre-score, 
quantitative changes of these changing elements are also taken into account for 
the ﬁnal score, and the calculation method is:

124
F. Li et al.
scoreallf D, r, S
ð
Þ = scorepf D, r, S
ð
Þ þ
 
df D, r
ð
Þ
ð4:42Þ 
where scorepf D, r, S
ð
Þ
 
denotes the calculated pre-score result, df D, r
ð
Þ =
- jf D
ð Þ - rj 
Δf max 
is used to measure the change of element quantity, and Δfmax denotes 
the difference between the upper and lower bounds of possible results, which is 
used to normalize results. 
3. Statistical results are generated. The ﬁnal score is used to calculate the 
corresponding output probability for each possible output. According to the 
calculated output probability, a result is output randomly as the ﬁnal statistical 
result. For each result r, the calculation method of its output probability is:
Pr rð Þ = 
exp 
scoreallf D, r, S
ð
Þ
 
2
1þ 
1 
Smin 
q2R 
exp 
scoreallf D, q, S
ð
Þ
 
2
1þ 
1 
Smin 
ð4:43Þ 
where Smin represents the minimum privacy budget in the privacy budget set 
and R represents the set where the possible output results r reside. 
4.2.2.4 
Privacy Preservation for Users in Deep Learning 
Deep learning is one of the most popular technologies in the era of Artiﬁcial 
Intelligence due to its unprecedented advantages in classifying and recognizing 
complex data such as images, speech, and texts. When establishing an intelligent 
system based on deep learning, various devices such as mobile terminals and the 
Internet of Things need to collect a large amount of user data for training to ensure 
the learning effect of the model; Most of the deep learning models have complex 
structures, causing huge training time and computational resources. Thus, they 
usually can only be hosted by high-performance servers. With the development of 
the Internet industry, service vendors such as Google and Baidu have provided a 
large number of cloud platforms to customize the deep learning services users need. 
Therefore, the Machine Learning as a Service (MLaaS) came into being, as shown in 
Fig. 4.12. In this architecture, the client runs locally on the user’s end and is 
responsible for collecting user data and uploading it to the server; the server receives 
the data uploaded by the client to train and generate a model and provides inference 
services to the public.

4
Privacy Computing Techniques
125
Fig. 4.12 Architecture of Machine Learning as a Service (MLaaS) 
Problem Statement 
The architecture of MLaaS has many privacy issues. First, in the process of 
uploading local data to the server, the ownership and control of the data are separated 
from each other. Thus, an honest but curious server can use the obtained data to 
restore the privacy information in the original data, and external attackers can launch 
an inversion attack on the server’s model to restore the original training data. Both 
issues pose a huge threat to the user privacy. Second, for some data-driven compa-
nies, data is their core asset and publishing or uploading it will cause the loss of data 
assets. Third, the data samples crowdsourced from the user terminals are often 
cumbersome and redundant, thus uploading them all causes great overhead during 
the transmission and training process. Moreover, the redundant or associated data are 
of limited value for subsequent learning tasks. Therefore, in the architecture of 
MLaaS, how to fully maintain the performance of the trained model while 
preventing privacy leakage is an urgent problem to be solved. 
To this end, a variety of defenses have been proposed. However, researchers 
usually consider the training process or inference process of the model separately, 
and thus fail to achieve the privacy preservation of the training process and the 
inference process simultaneously. In addition, the volume of raw data collected by 
the user’s device is huge, and uploading all the data to the server causes signiﬁcant 
communication overhead. Moreover, there is a large amount of correlated or even 
duplicate data. It is not so worthwhile to train with them, since they contribute to the 
improvement of model performance limitedly and might even harm the model 
generalization. At the same time, the outﬂow of a large amount of data will cause 
the loss of assets of the data owner, signiﬁcantly exacerbating their concerns of 
privacy leakage. 
Guided by Eq. (4.10), we propose a general privacy-preserving framework in the 
scenario of MLaaS based on the idea of representative subset selection [19]. It aims 
to strike a balance between the data privacy, model accuracy, and system overhead.

126
F. Li et al.
A Privacy-Preserving Framework in the Scenario of Deep Learning 
Basic Idea 
The privacy-preserving deep learning framework based on the representative subset 
selection can be deployed on the client side, which is overviewed in Fig. 4.13. 
Firstly, a representative data subset from the original training set is selected as 
training samples for each user, so as to reduce the losses of data assets and preserve 
user data assets and the privacy information contained in them to the greatest extent. 
Secondly, for the representative samples that must be uploaded, they pass through a 
privacy desensitization operation based on differential privacy before they are 
uploaded to the server. This ensures the privacy preservation effectiveness without 
impairing the utility of data for training. Also, the communication overhead is 
reduced when users transfer the data. 
Detailed Steps 
The framework consists of four steps: Data collection and pre-processing, parameter 
setting, representative subset selection, and disturbance data generation. 
1. Data acquisition and pre-processing. Once the client device collects new data, it 
transforms it into a numeric vector representation suitable for deep learning. 
2. Parameter setting. The client determines parameters such as representative sam-
ple ratio, nulliﬁcation rate, nulliﬁcation matrix, and noise scale according to the 
subjective and objective environment. If the client speciﬁes which elements to be 
shielded, the nulliﬁcation matrix is determined accordingly; Otherwise, a nulliﬁ-
cation matrix is randomly generated based on the nulliﬁcation rate. Speciﬁcally, 
the parameters include the representative sample ratio of the training set 
k1 2 (0, 1], the representative sample ratio of the inference set k2 2 (0, 1], the 
perturbation layer l 2 [1, L], the nulliﬁcation rate μ 2 [0, 1), the nulliﬁcation 
matrix In 2 ℝN , and the noise scale σ, where L represents the total number of 
layers of the neural network of the client and ℝN refers to the vector space of the 
client’s raw data.
Fig. 4.13 A privacy-preserving framework based on representative subset selection

4
Privacy Computing Techniques
127
3. Representative subset selection. The goal of this step is to generate a representa-
tive subset. Based on the representative sample ratio speciﬁed in step (2), the 
client applies the representative subset selection algorithm on the training set and 
the inference set and screens out a representative subset of the corresponding 
proportion that can effectively cover the characteristics of the whole set respec-
tively. The resulting representative subsets function as the real datasets after this 
sampling. 
First, based on whether a data sample is labeled, we put it into the training set 
or the inference set. The training set is then grouped by different label values. 
Then, according to the representative sample ratio of the training set k1, the client 
applies the representative subset selection algorithm on each group of training 
sets, screens out the representative training subsets of the corresponding propor-
tion in each group, and obtains the real training set after combining the results of 
this sampling. Finally, according to the representative sample ratio of the infer-
ence set k2, the client applies the representative subset selection algorithm on the 
inference set to screen out the representative inference subset of the 
corresponding proportion, which is used as the real inference set after this 
sampling. In particular, if the client has extremely high requirements for model 
performance and is willing to submit all data to the server for model training or 
inference, they can set the representative sample ratio of the training set/inference 
set to 1 and skip this step to proceed directly to step (4). 
4. Perturbation data generation. The core purpose of this step is to output the 
perturbed data representations to preserve the real data. Firstly, the corresponding 
data elements in the training subset and inference subset selected in step (2) are 
nulliﬁed according to the nulliﬁcation matrix In generated in step (3). Then the 
perturbation representation that satisﬁes the privacy budget is obtained by apply-
ing the differential perturbation algorithm. When the client collects a new batch of 
data and needs to send it to the server to complete the deep learning task, this 
method will ﬁlter and output a batch of disturbance data based on the personalized 
parameters set by the user and send it to the server. All of this is done locally on 
the client side and is completely transparent to the server.
Speciﬁcally, the nulliﬁcation matrix In is multiplied to each data sample in the 
representative subset generated in step (3) in an element-wise manner to complete 
the nulliﬁcation operation. Then, the nulliﬁed data is forward-propagated through 
the ﬁrst l layer of the client’s neural network, and the intermediate output of the l -
th layer of the neural network is calculated. According to the noise scale σ, for the 
intermediate output of each sample, a random noise vector of N′ dimension satisfy-
ing the Laplace distribution is generated and added to the intermediate output of each 
sample. Finally, the perturbed intermediate output is forward-propagated through the 
last N - l layers of the neural network to obtain the ﬁnal perturbed output. 
Experimental results show that the accuracy of the model trained with the 
representative subset does not decrease signiﬁcantly compared with that trained 
with the whole set. In addition, the defensed data perturbed by the Laplace mech-
anism can effectively resist the data reconstruction attack.

128
F. Li et al.
4.2.3 
Privacy-Preserving Algorithm Based on Privacy-Utility 
Tradeoff 
This section focuses on location and trajectory privacy preservation. Based on the 
privacy computing framework in Chap. 3, and the algorithm design principles in 
Sect. 4.1.2, we use mutual information to measure privacy leakage, design the 
optimal privacy-preserving algorithm based on the privacy-utility tradeoff under 
different utility constraints, measure data utility based on the degree of distortion, 
and perform quantitative evaluation of the effect of privacy-preserving algorithm. 
4.2.3.1 
Multi-level Location Privacy Measurement and Preservation 
In practice, users may have different usage requirements, usage rights or trust levels 
while using LBS, so it is necessary to release location data in different privacy 
preservation levels. Based on the privacy computing framework, the author of this 
book proposes a multi-level location privacy measurement and preservation algo-
rithm [20]. In the privacy information description phase, the access privilege level of 
the data user can be speciﬁed in the constraints of the six-tuple. In the privacy 
measurement phase, the privacy leakage at different levels can be measured based on 
mutual information to obtain the optimal location data privacy-preserving mecha-
nism that minimizes privacy leakage. In the privacy preservation effectiveness 
evaluation phase, we can use mutual information to analyze the information leakage 
where the attacker can infer the true location data with his owned published data at 
different levels. 
Privacy and Utility Metrics 
Let the random variables L and Vk represent the real location of the data owner and 
the disturbed location released to the data user whose level is k respectively, and the 
corresponding l and vk represent their possible values respectively. Assuming that 
the data users are untrustworthy, they will leverage the perturbed location data to 
infer the user’s real location information. Different levels of data users are allowed to 
access the location data of different degrees of perturbation. The higher the level of 
data users, the less perturbation is used on location data, that is, the perturbed data is 
closer to the real location data, and the utility is higher, and vice versa. 
Deﬁnition 4.12 The location privacy metric with the privacy preservation level k. 
When the location data owner uses the Location Privacy-Preserving Mechanism 
(LPPM) with the privacy preservation level k to generate the perturbed location data 
Vk, and releases it to the data users with the level k, the privacy leakage on L caused 
by the perturbed location Vk is deﬁned as I(L; Vk), where I(L; Vk) is the mutual 
information between the real location and the disturbed location, k is taken from a

set of positive integers, and the smaller the k, the higher the privacy preservation 
level. 
4
Privacy Computing Techniques
129
Deﬁnition 4.13 Utility Metrics for Perturbed Locations. Given the user’s true 
location L and the perturbed location Vk which is published to data users with 
level k, the utility metric for the perturbed location is deﬁned as 
D L, Vk
ð
Þ = 
l 
vk 
P lð ÞP vkjl
ð
Þd l, vk
ð
Þ
ð4:44Þ 
where P(l) is the prior probability distribution of the true location L; P(vk| l) is the 
conditional probability, or LPPM; d(l, vk) is the distortion function between the true 
location and the perturbed location (for example, Hamming distance or Euclidean 
distance). 
Proposition 4.1 Privacy-utility tradeoff of publishing perturbed location data Vk. 
Given the user’s true location L at a certain moment, the perturbed location Vk and 
the utility constraint Dk to be released to the untrusted data user with the level k at the 
current timeslot, the optimal LPPM P(vk| l) which is the solution to minimization 
problem in formula (4.22) achieves the minimum leakage of location privacy given 
the utility constraint Dk. The privacy-utility tradeoff can be expressed as 
Leakage*
k Dk
ð
Þ =
min 
P vkjl
ð
Þ:D L, Vk
ð
Þ ≤Dk I L; Vk
ð
Þ
ð4:45Þ 
However, in a scenario where there are multiple levels of data users, if a data user 
becomes a malicious attacker, he may maliciously intercept or collude with other 
level data users to obtain the perturbed location data originally intended to be 
released to data users of other level, and then the attacker can infer the true location 
data L more accurately by performing joint analysis on multiple published data with 
different privacy preservation levels. This type of attack is deﬁned as follows. 
Deﬁnition 4.14 Diversity Attack. In scenarios where data publishers have different 
degrees of trust in data users, data publishers will classify data users according to the 
level of trust. In this case, there will be an attack, where we assume that the true 
location data of the data publisher is l, and the location data released to users of 
different levels are v1v2⋯vm⋯vM, where m is the level serial number, and the 
smaller the m value, the better the data utility. In this scenario, when a data user 
with level m becomes a malicious attacker, it can obtain the dataset V\m released to 
other level data users through such as malicious interception, etc., where V\m means 
that a collection of any released data in v1v2⋯vm⋯vM except vm. 
For example, the attacker can obtain the released data v2 and vM with level 2 and 
level M, we have V\m = v2vM, and then perform joint data analysis on the released 
data vm and V\m obtained according to his own privileges, the attacker can infer the 
true location data l with higher accuracy. This type of attack is deﬁned as a diversity 
attack.

130
F. Li et al.
When there is a diversity attack in the release of location data with multi-level 
privacy preservation, it will result in more privacy leakage to the true location data 
l of the data owner. The privacy leakage caused by a diversity attack can be 
measured through an information-theoretic metric. 
Deﬁnition 4.15 Privacy Metrics for Diversity Attacks. Assume a data user with 
level m is a malicious attacker. When the multiple V\m datasets published to data 
users of different levels are obtained, the attacker can perform joint data analysis on 
vm and V\m to infer the true location L data of the data owner. The privacy leakage 
caused by this attack to the data owner’s true location data L is deﬁned as I(L; vm [ V 
\m)), where I(L; vm [ V\m)) is the mutual information between the true location L and 
the dataset vm [ V\m. 
Deﬁnitions 4.15 provides a general measure of the privacy leakage of a privacy-
preserving mechanism under a diversity attack. This method can be used to measure 
the privacy leakage of any privacy-preserving mechanism where the mutual infor-
mation I(L; vm [ V\m)) under diversity attacks can be calculated. The calculation 
process of diversity attack privacy leakage will be introduced in detail later. 
Multi-level Location Privacy-Preserving Algorithm 
The multi-level location privacy-preserving publishing mechanism can ensure that 
under certain utility constraints, the perturbed location data at each level has the 
minimum privacy leakage on the true location data. The speciﬁc process of the 
algorithm is presented as follows. 
Pre-processing 
The probability distribution of the true position P(l), the location distortion function 
d(., .), and the threshold δ for the control of algorithm convergence are initialized 
ﬁrst. The level i in the multi-level privacy-preserving location data publishing 
mechanism is determined by the input parameter s (Lagrange multiplier) in the 
algorithm, that is, the data publisher deﬁnes the level i according to s. For example, 
when the value of s is taken from the set {0.01, 2, 5}, it can be deﬁned that the 
corresponding levels of s = 5, 2, 0.01 are i = 1, 2, 3, respectively. The output 
distribution P(vi) is initialized as a uniform distribution. 
Algorithmic Framework 
The Blahut-Arimoto algorithm is adopted to generate the optimal LPPM P(vi| l) 
based on Leakage*
i Di
ð
 Þ, and the speciﬁc steps are as follows.

4
Privacy Computing Techniques
131
1. Calculate P(vi| l) 
P vijl
ð
Þ = 
P vi
ð Þe - sd l,vi
ð
Þ
 
vi 
P vi
ð Þe - sd l,vi
ð
Þ
ð4:46Þ 
2. Calculate P(vi) according to P(vi| l), 
P vi
ð Þ = 
l 
P lð ÞP vijl
ð
Þ
ð4:47Þ 
3. Calculate I(L; Vi) according to P(vi| l) and P(vi), and check the convergence of 
the algorithm according to δ. If it does not converge, go back to step (1), and 
repeat P(vi| l) and  P(vi) until the algorithm converges. 
I L; Vi
ð
Þ = 
l 
vi 
P lð ÞP vijl
ð
Þ log P vijl
ð
Þ
 
P vi
ð Þ
ð4:48Þ 
Finally, the optimal LPPM P(vi| l) can be obtained and the privacy leakage of the 
true location caused by the release of perturbed data with a privacy preservation level 
of k can be calculated by the deﬁnition of mutual information. 
Algorithm 4.1 summarizes how the data publisher obtains the LPPM (P(vi| l)), 
which is used to generate the perturbed location data published to multiple different 
levels of data users by controlling the input parameters. After obtaining P(vi| l), the 
perturbed vi to be released can be sampled from the probability distribution P(vi| l). 
Algorithm 4.1 
Location Data Release Mechanism with Multiple Privacy Preservation Levels 
Input: Lagrange multiplier (level control factor) s, data user level i, location 
probability distribution P(l), distortion function between the true and perturbed 
location data d(l, vi), threshold δ. 
Output: The LPPM P(vi| l) used when publishing the perturbed location data to 
the data user with the level of i, the minimum privacy leakage I*
i 
caused by 
publishing vi to the data user with the level i, the utility constraint Di corresponding 
to the I*
i , the marginal distribution P(vi) of the perturbed location vi. 
1. 
Initialize P0(vi) as the uniform distribution 
2. 
Calculate P0(vi| l) using P0(vi) and formula (4.46) 
3. 
Calculate P(vi) using P0(vi| l) and formula (4.47) 
4. 
Calculate I0 
i = I L; Vi
ð
Þ
 
using P0(vi), P0(vi| l), P(l) and formula (4.48) 
5. 
while true do 
6. 
Calculate P(vi| l) with P(vi) and formula (4.46) 
7. 
Calculate Ii = I(L; Vi) using P(vi), P(vi| l) and formula (4.48) 
8. 
if (j I0 
i - Ii j ≤δ ) then

l
vi
132
F. Li et al.
9. 
I*
i ←Ii 
10. 
Calculate Di =
P lð ÞP vijl
ð
Þd l, vi
ð
Þ
 
11. 
return P(vi| l), I*
i , Di 
12. 
else 
13. 
I0 
i ←Ii 
14. 
Calculate P(vi) using P(vi| l), P(l) and Eq. (4.47) 
15. 
end if 
16. 
end while 
The computational complexity of the Algorithm 4.1 can be analyzed by giving an 
expression for the computational complexity of each iteration in algorithm. In each 
iteration, the computational complexity is dominated by computing P(vi| l) and P(vi). 
The complexity of P(vi| l) in formula (4.46) is analyzed as follows: for each 
realization of the variable l, the denominator needs to be multiplied by jVij times 
for a speciﬁc vi. Considering that this denominator is used for each vk, a total of O(| 
Vi| ) computations are required. Considering all the values of the variable l, the 
complexity of calculating P(vi| l) is  O(| VikL| ).  
Similarly, the complexity of P(vi) in Eq. (4.47) is analyzed as follows: for a 
particular vi, jLj multiplications are required. The complexity of computing P(vi) is  
O(| VikL| ) considering all vi, so each iteration in Algorithm 4.1 requires O(| VikL| )  
computations approximately. 
Privacy Preservation Effect Evaluation Against Diversity Attack 
Deﬁnition 4.14 indicates that the dataset V\m published to other levels of data users 
that an attacker may intercept includes any published data other than vm. For 
simplicity, we use an example where the data users are divided into three levels to 
explain how to leverage the privacy metric for diversity attack proposed in this 
section to measure the privacy leakage caused by the attacker’s access of the 
published data of the other two levels, and provide a quantitative evaluation method 
for the privacy preservation effectiveness. 
Assuming that the true location data of the data owner in this scenario is L, the 
perturbed location data V1, V2, V3 released to three different levels of data users is 
generated through Algorithm 4.1. Let the level of the attacker be 2, when the attacker 
obtains the data V1 and V3 of the other two levels of users through interception or 
other methods, the attacker can infer the value of the true location data L by jointly 
analyzing the released data V1, V2, V3 with higher accuracy. According to the metric 
proposed in Deﬁnition 4.15, the diversity privacy leakage in this scenario is I(L; V1, 
V2, V3). 
To clearly describe how to calculate the privacy leakage caused by different 
privacy-preserving mechanisms when they are suffered from diversity attack, the 
calculation of mutual information I(L; V1, V2, V3) will be carried out as below. 
According to the deﬁnition of mutual information, we have

ð
ð
ð
4
Privacy Computing Techniques
133
I L; V1, V2, V3
ð
Þ = H V1, V2, V3
ð
Þ - H V1, V2, V3jL
ð
Þ
4:49Þ 
According to the deﬁnition of information entropy, we have 
H V1, V2, V3
ð
Þ = -
v1, v2, v3 
P v1, v2, v3
ð
Þ log P v1, v2, v3
ð
Þ
4:50Þ 
Since V1, V2, V3 are the published data generated from the true location L with 
three different privacy preservation levels, V1, V2, V3 are independent of each other. 
Furthermore, we have 
P v1, v2, v3
ð
Þ = P v1
ð
ÞP v2
ð
ÞP v3
ð
Þ
ð4:51Þ 
where the marginal distribution of v1, v2, v3 is 
P vm
ð
Þ = 
l 
P lð Þ log P vmjl
ð
Þ, m = 1, 2, 3
ð4:52Þ 
According to the deﬁnition of conditional entropy, we can get 
H V1, V2, V3
ð
Þ = -
v1, v2, v3:l 
P v1, v2, v3, l
ð
Þ log P v1, v2, v3jl
ð
Þ
4:53Þ 
where, 
P v1, v2, v3, l
ð
Þ = P v1jv2, v3, l
ð
ÞP v2jv3, l
ð
ÞP v3jl
ð
ÞP lð Þ  
= P v1jl
ð
ÞP v2jl
ð
ÞP v3jl
ð
ÞP lð Þ
ð4:54Þ 
The ﬁrst equal sign in formula (4.54) follows Bayesian formula, and the second 
equal sign is due to the fact of releasing the location at a single timestamp, when the 
true location L is given, V1 is fully determined by L and independent of other 
variables, so we get 
P v2jv3, l
ð
Þ = P v2jl
ð
Þ
ð4:55Þ 
For the same reason, we can have 
P v1, v2, v3jl
ð
Þ = P v1jl
ð
ÞP v2jl
ð
ÞP v3jl
ð
Þ
ð4:56Þ 
It can be seen that the key to calculating mutual information I(L; V1, V2, V3) is the 
access to the marginal distribution of the published data V1, V2, V3, the conditional 
probability distribution P(v1| l), P(v2| l), P(v3| l) and the probability distribution P(l) 
of the true location L.

134
F. Li et al.
The method is based on information theory, and independent of any attack. It can 
be used for location privacy metrics at a single timestamp. The experimental results 
show that the LPPM has a signiﬁcant advantage over the differential private LPPM 
in terms of the privacy-utility trade-off in both scenarios with and without diversity 
attacks. This advantage is more conspicuous when the prior probability distribution 
has certain particularly frequent locations. 
4.2.3.2 
Privacy Computing of Location Traces and Aggregated 
Locations 
A location trace is a set of locations that the user continuously reports to an LBS 
server when using LBS. For example, in a navigation application, the user needs to 
continuously upload their location information to the service provider, which forms 
a location trace and are highly temporally correlated. In addition, the location service 
provider may only need to collect and calculate aggregated location information 
periodically to identify some speciﬁc phenomenon or track some important pattern. 
According to different service purposes, the aggregation calculation can be sum, 
mean, standard deviation, density, etc. For example, using location data collected 
from a user’s mobile device, it is possible to obtain the average vehicle speed during 
peak hours and calculate the average delay when the road is congested, or count the 
number of patients infected with inﬂuenza in a particular area to monitor the spread 
of inﬂuenza. 
However, directly publishing location trace data or aggregated location data to the 
public can lead to user privacy leakage. Based on the privacy computing framework 
and the algorithm design principles in Sect. 4.1.2, this section introduces the location 
trace privacy computing method [21] and the aggregated location privacy computing 
method [22] proposed by the author of this book from three aspects: measurement, 
effectiveness evaluation, and algorithm design. 
Location Trace Privacy and Aggregated Location Privacy Metrics 
Location Trace Privacy Metrics 
The user’s location Li at time i is represented as a triple (xi,yi,i), where xi, yi, and 
i represent the latitude coordinates, longitude coordinates, and time, respectively. 
The true trace L and perturbed trace U of length T are represented as sequences of 
positions (L1,. . .,LT) and (U1,. . .,UT), respectively, where time i and length T are 
both integers. For the convenience of analysis and calculation, it is assumed that the 
user moves in N discrete locations, and the transition between locations follows a 
ﬁrst-order Markov transition model. In particular, we study the scenario where users 
send their traces to untrusted location service providers in an online manner, as 
shown in Fig. 4.14.

li, ui
ę
4
Privacy Computing Techniques
135
i 
Untrusted Location Service Provider 
ę
ę 
ę
ę 
ę 
ę 
ę 
ę 
ę
 
Fig. 4.14 Privacy-preserving location postings online 
As can be seen from Fig. 4.14, after a certain time steps 1, ⋯, T, the untrusted 
location service provider can observe the release (perturbed) trace in the form of (U1, 
⋯, UT), and the user’s privacy information can be inferred according to the 
perturbed trace. It is assumed that the location service provider has statistical 
knowledge about the user’s location, that is, the initial probability distribution of 
the user’s location and the location transition matrix, and no limitation is made on its 
computational capability. In principle, it can use this statistical knowledge and the 
released location trace to launch any type of inference attack. Next, the privacy 
leakage caused by publishing the user’s perturbed trace is analyzed in this scenario. 
Deﬁne 4.16 Privacy Metric for a Location Trace. After a certain period of time, 
given a user’s true trace L and the perturbed trajectory U to be released to the 
untrusted location service provider, the privacy leakage caused by the perturbed 
trace is deﬁned as 
I L; U
ð
Þ = I L1, ⋯, LT; U1, ⋯, UT
ð
Þ
 
where I(L; U) is the mutual information between the user’s true and perturbed traces, 
which is used as the privacy metric of the location trace. 
Deﬁnition 4.17 Utility Metric for a Location Trace. After a certain time period, 
given the user’s true trace L and the perturbed trace U to be published to untrusted 
location 
service 
providers, 
the 
utility 
metric 
of 
the 
trace 
is 
deﬁned 
as 
D L; U
ð
Þ = 
T 
i = 1 
D Li; Ui
ð
Þ. where D(Li; Ui) is the average distortion of the publishing 
location Ui at time i, D Li; Ui
ð
Þ = 
P li
ð Þq uijli
ð
Þd li, ui
ð
Þ, d(li, ui) is the distortion

trace D ≤
i = 1
Di.
function (such as Hamming distance or Euclidean distance). The utility constraint at 
location time i is deﬁned as D(Li; Ui) ≤ Di, i = 1, . . ., T, where Di is the utility 
constraint assigned at time i, which implies that the total distortion for a location 
T 
136
F. Li et al.
Deﬁnition 4.18 Ofﬂine Privacy-Utility Tradeoff for a Location Trace. After a 
certain time period, we set user’s true trace as L, the perturbed trace to be published 
to the untrusted location service provider as U, and the utility constraint is 
D ≤ 
T 
i = 1 
Di. When an LPPM is the solution to the optimization problem in 
Eq. (4.57), the LPPM q(u| l) is said to achieve the minimum leakage of trace privacy 
given the utility constraint D. 
L*
offline D
ð Þ =
min 
q ujl
ð
Þ: D Li; Ui
ð
Þ ≤Di
f
gT 
i = 1 
I L; U
ð
Þ
ð4:57Þ 
where I(L; U) is a mutual information measure of trace privacy. 
To describe the privacy-utility tradeoff of a location trace in the online scenario, 
the deﬁnition of the true privacy leakage of the online location release mechanism is 
proposed. 
Deﬁnition 4.19 Privacy Leakage Metrics for Online Location Release Mechanisms. 
After a certain period of time 1, . . ., T, when a user releases locations in an online 
manner, (i.e., the locations are sequentially released which form a released location 
trace), the actual privacy leakage introduced in this online location release setting is 
deﬁned as 
LActual 
online LPPM
ð
Þ = 
T 
i = 1 
I Li ; UijUi - 1
ð4:58Þ 
where the LPPM can be generated by any type of methods. LActual 
online LPPM
ð
Þ is used as 
the privacy metric to evaluate the actual privacy leakage for online location release 
mechanisms. 
Aggregated Location Privacy Metrics 
The user’s location proﬁle is represented by a binary matrix Am of size L × T, where 
the subscript m represents the user’s ID, L represents the number of locations, and 
T represents the number of time steps in an aggregation process. The elements in the 
matrix are denoted as Am[l, t], which is a binary variable with a value of 1 or 0,where 
1 represents that user m visited an ROI at timestamp t and 0 denotes that the user 
did not.

at the location l at time step t, and
l = 1
Pm l, t
ð
Þ = 1 holds.
t = 1
4
Privacy Computing Techniques
137
Trusted location 
data aggregation 
operator
User location profile A1
User location profile Am
User location profile AM
...
...
Original 
aggregated 
location data A
ALPPM
Perturbed 
aggregated location 
data A 
Fig. 4.15 Problem scenarios of aggregated location privacy preservation 
The prior probability of the user’s location is represented by a matrix P of size 
L × T. Each element Pm(l, t) in the matrix represents the probability that the user m is 
L 
The aggregated location data is represented by a matrix A of size L × T. Each 
element A[l, t] in the matrix represents the number of users who are at the location 
l at time step t, and 
L 
l = 1 
A l, t
½
] = M holds, where M is the total number of users 
participating in the aggregation process. 
In order to protect the privacy of the user’s aggregated location, it is necessary to 
perturb the original aggregated location data A before releasing it to the public to 
obtain the matrix A, and at the same time, it is necessary to ensure data utility. 
Figure 4.15 shows the problem setting of aggregated location data privacy studied in 
this section and the following analyzes the privacy leakage caused by publishing the 
perturbation matrix A to the original matrix A and user’s location proﬁle Am under 
this setting. 
Note that the matrix A used to represent the perturbed aggregated position data 
should have the same properties as the original matrix A, that is, the values of the 
elements in the matrix should also be integers from 0 to M, and the sum of the 
elements in each column should be equal to M. Considering that the permutation 
method can ensure the perturbed matrix to maintain the original properties, the 
permutation-based method is adopted to design an aggregated location privacy 
preservation mechanism (Aggregated LPPM, ALPPM). 
Deﬁnition 4.21 Privacy metrics for aggregated location data. Given the location 
proﬁle Am of the user m, the true aggregated location matrix A and the perturbed 
aggregated location matrix A I Am; A and I A; A are used to measure the privacy 
leakage caused by the release of the perturbed aggregated location data to the 
personal location and the true aggregated data, respectively. 
Deﬁnition 4.22. Utility metric for aggregated location data. Given the true aggre-
gated location matrix A and the perturbed aggregated location matrix A, the utility 
metric
for
the
aggregated
location
data
is
deﬁned
as 
D = D A; A = 
T 
D A tð Þ; A tð Þ  , where D A tð Þ; A tð Þ  is the average distortion

D A tð Þ; A tð Þ ≤Dt, t = 1, ⋯, T, so there is D ≤
t = 1
Dt.
of
the
aggregated
locations
at
time
t, 
i.e. D A tð Þ; A tð Þ  = 
A tð Þ, A tð Þ  
p A tð Þ, A tð Þ  d A tð Þ, A tð Þ  , where d A tð Þ; A tð Þ  
is 
the value of the vector A tð Þ - A tð Þ  Euclidean norm. The utility (or distortion 
constraint) 
of 
the 
aggregated 
position 
at 
time 
t 
is 
deﬁned 
as 
T 
138
F. Li et al.
Deﬁnition 4.23 Aggregate Privacy—Aggregate Utility tradeoff of an ALPPM. 
Given the true aggregation position matrix A and the perturbed aggregation position 
matrix A, the ALPPM q AjA 
achieves the minimum information leakage on a 
user’s location proﬁle subject to the utility constraint D ≤ 
T 
t = 1 
Dt when it is the 
solution of the following optimization problem 
L*
agg D
ð Þ =
min 
q AjA : 
D A tð Þ; A tð Þ  ≤Dt 
T 
t = 1 
I A; A tð Þ
ð4:59Þ 
where I A; A tð Þ  is the metric for aggregated location privacy. 
In addition, publishing the perturbed aggregated location data can also result in 
privacy leakage on individual user’s location information. 
Deﬁnition 4.24 Individual Privacy—Aggregate Utility tradeoff of an ALPPM. 
Given the location ﬁle Am of the user m, and the perturbed aggregated location 
data A to be released by the trusted location data aggregation service provider, and a 
distortion constraint D ≤ 
T 
t = 1 
Dt, the optimal ALPPM*
user achieves the minimum 
information leakage on a user’s location proﬁle subject to the utility constraint D 
when it is the solution of the following optimization problem: 
ALPPM*
user = arg
min 
q AjA : 
D A tð Þ; A tð Þ  ≤Dt 
T 
t = 1 
max 
1 ≤m ≤M I Am; A
ð4:60Þ 
where I Am; A 
is the mutual information between Am and A.

Þ
Þ
Þ
4
Privacy Computing Techniques
139
Evaluation of Location Trace Privacy Preservation Mechanism Based 
on Privacy-Utility Tradeoff 
Trace Privacy-Upper and Lower Bounds on Privacy-Utility Tradeoff 
Intuitively, a location is more likely correlated with those that are closer to it in terms 
of time span. Based on this intuition, a Markov restriction is made on the location 
release mechanisms that the released location Ui at time step i only depends on the 
current true location Li, the previous released location Ui - 1, and the previous true 
location Li - 1. Note that the proposed system model allows for potentially more 
complex mechanisms but for complexity issues the Markov release is considered. 
Theorem 4.2 Upper and Lower Bounds on online Privacy-Utility Tradeoff With 
Markov Release Restriction. With the Markov restriction on the location release 
mechanism, the optimal online privacy-utility tradeoff L*
online D
ð Þ  can be upper and 
lower bounded as follows: 
LMarkov 
lower 
D
ð Þ ≤L*
online D
ð Þ ≤LMarkov 
upper 
D
ð Þ
ð4:61Þ 
where, 
LMarkov 
upper 
D
ð Þ = 
T 
i = 1 
min 
q uijli, ui - 1, li - 1
ð
Þ:D Li; Ui
ð
Þ ≤Di I Li, Li - 1; UijUi - 1
ð
 
LMarkov 
lower 
D
ð Þ = 
T 
i = 1 
min 
q uijli, ui - 1, li - 1
ð
Þ:D Li; Ui
ð
Þ ≤Di I Li; UijUi - 1, Li - 1
ð
 
In the algorithm design of privacy computing, the upper bound LMarkov 
upper 
D
ð Þ  can be 
used to generate LPPM instead of the lower bound LMarkov 
lower 
D
ð Þ. This is because it is 
more reasonable to consider about how much information at most will be leaked 
when designing privacy-preserving mechanisms. 
Corollary 4.1 The actual leakage of LPPM* evaluated by LActual 
online LPPM
ð
is 
sandwiched between LMarkov 
lower 
D
ð Þ  and LMarkov 
upper 
D
ð Þ, namely 
LMarkov 
lower 
D
ð Þ ≤LActual 
online LPPM*
ð
Þ ≤LMarkov 
upper 
D
ð Þ
ð4:62Þ 
Corollary 4.1 provides the privacy guarantee that the exact information leakage for 
an entire location trace released by LPPM* sequentially from timestamp 1 to T is 
sandwiched between LMarkov 
lower 
D
ð Þ  and LMarkov 
upper 
D
ð Þ. Therefore, even though directly 
computing the exact information leakage for an entire location trace is computation-
ally challenging, we can still know the approximate information leakage by 
sandwiching the exact information leakage between LMarkov 
lower 
D
ð Þ  and LMarkov 
upper 
D
ð Þ.

140
F. Li et al.
Upper and Lower Bounds on the Aggregate Privacy—Aggregate Utility Tradeoff 
Theorem 4.3 When there is no temporal correlation in the aggregated location data, 
the main results for individual and aggregate privacy-utility tradeoff are: 
L*
user D
ð Þ ≤Luser D, ALPPM*
agg 
≤L*
agg D
ð Þ ≤Lupper 
agg 
D
ð Þ
ð4:63Þ 
where, 
Luser D, ALPPM*
agg 
= max 
1 ≤m ≤M I Am; A 
q AjA 
= ALPPM*
agg 
Lupper 
agg 
D
ð Þ = 
T 
i = 1 
min 
q A tð ÞjA tð Þ  :D A tð Þ; A tð Þ  ≤Dt 
I A tð Þ; A tð Þ  
It can be seen from Theorem 4.3 that even if the perturbed aggregate location is 
calculated with high complexity for the privacy leakage of the individual location, 
the upper bound Lupper 
agg 
D
ð Þ  can still be calculated to ensure that the leakage of 
personal privacy is not larger than Lupper 
agg 
D
ð Þ. 
Trajectory Privacy Preservation Scheme Design 
Trajectory’s Privacy-Preserving Online Publishing Algorithm 
Based on the algorithm design principles in Sect. 4.1.2, using the Blahut-Arimoto 
algorithm and the Lagrangian multiplier method, an algorithm that generates LPPM*
i 
based on LMarkov 
upper 
D
ð Þ  at time i can be designed and implemented, that is, the online 
release mechanism of trajectory privacy preservation, as shown in Algorithm 4.2. 
Algorithm 4.2 
Algorithm to Generate LPPM*
i Based on LMarkov 
upper 
D
ð Þ  at Time i 
Input: the Lagrange multiplier λ, the joint probability distribution of p(ui - 1, li -
1, ui - 2, li - 2) obtained at time i - 1, the marginal distribution p(ui - 1) of the  
disturbance position ui - 1 released at time i - 1, the distortion matrix d(li, ui), and 
the threshold set for the convergence of the algorithm result δ. 
Output: LPPM*
i q uijli, ui - 1, li - 1
ð
Þat time i, minimum privacy leakage at time i I*
i , 
distortion Di corresponding to I*
i , marginal distribution p(ui) of perturbed location ui 
published at time i, joint probability distribution of (ui, li, ui - 1, li - 1) q(ui, li, ui - 1, 
li - 1). 
1. 
Initialize r0(ui| ui - 1) for uniform distribution 
2. 
Calculate q0(ui| li, ui - 1, li - 1) using r0(ui| ui - 1) and formula (4.6) 
3. 
Calculate p(li, ui - 1, li - 1) using formula (4.10)

ui, li, ui - 1, li - 1
li, ui - 1, li - 1
4
Privacy Computing Techniques
141
4. 
Calculated with r0(ui| ui - 1), q0(ui| li, ui - 1, li - 1), p(li, ui - 1, li - 1) and 
formula (4.11) 
I0 
i = I Li, Li - 1; UijUi - 1
ð
Þ
 
5. 
Calculate r(ui| ui - 1) using q0(ui| li, ui - 1, li - 1), p(li, ui - 1, li - 1), p(ui - 1) 
and formula (4.7) 
6. 
while true do 
7. 
Calculate q(ui| li, ui - 1, li - 1) with r(ui| ui - 1) and formula (4.6) 
8. 
Calculated using r(ui| ui - 1), q(ui| li, ui - 1, li - 1), p(li, ui - 1, li - 1) and 
formula (4.11) 
Ii = I(Li, Li - 1; Ui| Ui - 1) 
9. 
if I0 
i - Ii ≤δ then 
10. 
I*
i ←Ii 
11. 
Calculated p(ui, li, ui - 1, li - 1) = q(ui| li, ui - 1, li - 1)p(li, ui - 1, li - 1) 
12. 
Calculated Di =
p ui, li, ui - 1, li - 1
ð
Þd li, ui
ð
Þ
 
13. 
Calculated p ui
ð Þ
p ui, li, ui - 1, li - 1
ð
Þ
 
14. 
return q(ui| li, ui - 1, li - 1), I*
i , Di, p(ui, li, ui - 1, li - 1) and p(ui) 
15. 
else 
16. 
I0 
i ←Ii 
17. 
Calculated using q(ui| li, ui - 1, li - 1), p(li, ui - 1, li - 1), p(ui - 1) and formula 
(4.7) 
r(ui| ui - 1) 
18. 
end if 
19. 
end while 
Privacy-Preserving Publishing Algorithm for Aggregated Location Data 
Similarly, based on the algorithm design principles in Sect. 4.1.2, the Blahut-
Arimoto algorithm can be used to design and implement an algorithm for generating 
ALPPM based on upper bound Lupper 
agg 
D
ð Þ, as shown in Algorithm 4.3. 
The experimental results show that the actual privacy leakage of the LPPM 
(i.e. LPPM*) in this section is lower than that of the differential privacy mechanism 
LPPM under the same utility constraint under the four Markov cases. This advantage 
is especially pronounced when the temporal correlation of the locations in the 
trajectory is higher. This is because the LPPM in this section has taken the temporal 
correlation into account. In addition, it can be seen that the actual privacy leakage of 
LPPM* is actually very close to the upper bound LMarkov 
upper 
D
ð Þ, so the LPPM generated 
based on LMarkov 
upper 
D
ð Þ may be sufﬁcient to take into account the temporal association 
of positions in the trajectory. 
Algorithm 4.3 
Algorithm for generating ALPPM based on upper bound Lupper 
agg 
D
ð Þ  
Input: the probability distribution p(a(t)) of a(t), the Lagrange multiplier λ, the 
distortion matrix D a  tð  Þ, a  tð  Þ
ð
Þ, and the threshold δ set for algorithm convergence

q a tð Þja tð Þ
ð
Þ =
a tð Þe
a t
r0 a tð Þ
ð
Þe
- λd
a tð Þ,a tð Þ
a t
Þ
a tð Þe
a t
r0 a tð Þ
ð
Þe
- λd
a tð Þ,a tð Þ
Þ
=
a tð Þ, a tð Þ
p a tð Þ
ð
Þq a tð Þja tð Þ
ð
Þ log
q a tð Þja tð Þ
ð
Þ
r a tð Þ
ð
Þ
r
a tð Þ =
a t
p a tð Þ
ð
Þ, q a tð Þja tð Þ
ð
Þ
142
F. Li et al.
Output: ALPPM q a tð Þja tð Þ
ð
Þ at time t, minimum information leakage I*
t at time t, 
corresponding to the distortion Dt of I*
t t 
1. 
Initialize r0 a tð Þ
ð
Þ
 
to be uniformly distributed 
2. 
Calculate q0 a tð Þja tð Þ
ð
Þ with r0 a tð Þ
ð
Þ and the formula q0 a tð Þja tð Þ
ð
Þ
 
3. 
Calculate 
I0 
t 
with r0 a tð Þ
ð
Þ, 
q0 a tð Þja tð Þ
ð
Þ, 
p(a(t)) 
and 
the 
formula
- λd
a
 tð Þ,a tð Þ  
ð Þ  
4. 
Calculate 
r a tð Þ
ð
Þ
 with 
q0 a tð Þja tð Þ
ð
Þ
 
and 
the 
formula 
r a tð Þ
ð
Þ = 
p a tð Þ
ð
Þq0 a tð Þja tð Þ
ð
Þ
 
ð Þ  
5. 
while true do 
6.
Calculate q a tð Þja tð Þ
ð
Þ
 
with r a tð Þ
ð
Þ
 
and the formula q a tð Þja tð Þ
ð
=
- λd
a
 tð Þ,a tð Þ  
ð Þ  
7.
Calculate it using r a tð Þ
ð
Þ, q a tð Þja tð Þ
ð
Þ, p(a(t)) and the formula I a  tð Þ; a tð Þ
ð
 
8.
if I0 
t - It ≤δ then 
9.
I*
t ←It 
10.
Calculate Dt = 
a tð Þ, a tð Þ  
p a tð Þ, a tð Þ
ð
Þq a tð Þja tð Þ
ð
Þd a  tð Þ, a tð Þ
ð
Þ
 
return q a tð Þ j
ð 
a(t)), I*
t , Dt  
11.
else 
12.
I0 
t ←It 
13.
Calculate 
r a tð Þ
ð
Þ
with 
q a tð Þja tð Þ
ð
Þ
and 
the 
formula 
ð Þ  
14.
end if 
15.
end while

4
Privacy Computing Techniques
143
4.3 
Extended Control of Privacy Information 
As one of the important contents of privacy computing, the depth of privacy 
extended control affects the current and future privacy protection in ubiquitous 
interconnected environments. This section ﬁrst provides the deﬁnition of extended 
control, describes the iterative extended control model, and gives a general frame-
work for iterative desensitization control. Subsequently, the key technologies related 
to privacy extended control were analyzed and discussed using two typical scenar-
ios: privacy information dissemination control in image sharing and image privacy 
extended control for cross-system exchange. 
4.3.1 
Deﬁnition of Extended Control 
Extended control refers to the iterative control of data operations, the dynamic 
adjustment of control strategies, the controllable transmission of control strategies, 
and the credible audit of control strategy execution in the process of sharing private 
data across domains, information systems, and ecosystems in an interconnected 
environment. It runs through all aspects of the entire life cycle of privacy computing. 
The following explains the meaning of extended control through the process of 
sharing privacy information. The process of sharing privacy information involves 
two roles: one is the sharer, and the other is the recipient. Once the information is 
shared by the ﬁrst sharer, all subsequent recipients can be transformed into sharers. 
In the process of information sharing, iterative desensitization extended control 
needs to be carried out based on the privacy control strategy of the previous sharer, 
their own desensitization needs, and the protection ability of the next recipient. The 
iterative desensitization control process occurs when the recipient performs desen-
sitization control after receiving the information and before receiving the informa-
tion again. As shown in Fig. 4.16, iterative desensitization control occurs when 
Buddy sends information to Calvin and David, and when Calvin and David share it 
with subsequent recipients again. 
Buddy received the desensitized information sent by Andy, which contained an 
existing desensitization control strategy (i.e., privacy operation control strategy 
shown in the ﬁgure). After Buddy extracts the desensitization control strategy 
embedded by Andy, the desensitization control strategy is generated based on 
existing desensitization control strategies and Buddy’s own desensitization require-
ments. According to this strategy, the information XBuddy is iteratively desensiti-
zation controlled and sent to Calvin and David respectively. At this time, the 
information in Buddy’s hands XBuddy should satisfy the following requirements: 
the sensitivity of XBuddy must not be greater than the sensitivity of XAndy. The two 
recipients, Calvin and David, extract the desensitization control strategy from the 
received information, generate another round of iterative desensitization control 
strategy, and perform desensitization according to the strategy, obtaining the

desensitized information XCalvin and XDavid, with XCalvin ⊆ XBuddy and 
XDavid ⊆ XBuddy, and the sensitivity of XBuddy must be higher than or equal to the 
sensitivity of XCalvin and XDavid, respectively. 
144
F. Li et al.
Andy 
Information 
Privacy Operation 
Control Strategy 
XAndy 
Buddy
David
…
Calvin
…
XBuddy 
ToBuddy 
ToDavid 
Budd
Dav
y
An
id
dy
X
X
X
⊂
⊂
 
Budd
Calvin
y
Andy
X
X
X
⊂
⊂
 
Buddy
Andy
X
X
⊂ 
Fig. 4.16 Differential iterative data-masking control 
In the case of data trading, data owners can formulate differentiated desensitiza-
tion strategies based on different user bids. For example, Calvin bids 1000 thousand 
for Andy’s data, while David bids 500 thousand. Andy can perform differential 
control in the desensitization control strategy for Buddy, which means using a 
desensitization algorithm with high data utility for the demand side bidding 1000 
thousand, and using a desensitization algorithm with lower data utility for the 
demand side bidding 500 thousand. 
In another scenario, Calvin may purchase Andy’s data for his own model training 
or statistical analysis, and no longer share or sell it to other recipients. It is also 
possible that Calvin will share the data with other recipients again. For these two 
cases, Andy provides different control strategies in the information transmitted to 
Buddy. 
4.3.2 
Privacy Information Dissemination Control in Image 
Sharing 
4.3.2.1 
Impression-Management Scheme for Privacy-Aware 
Photo-Sharing Users 
The rapid development of online social networks (OSNs) and mobile devices has 
promoted the popularity of online photo-sharing platforms (PSPs). With camera-
integrated smartphones, users can take photographs anywhere at any time, and then

share them via PSPs such as WeChat, Facebook, or Flickr. They can also view 
photos published by friends or strangers at any time and comment on them. 
4
Privacy Computing Techniques
145
However, shared photos may contain sensitive information that can be used to 
infer users’ privacy information. In general, a shared photo usually has three kinds of 
information: content (which can be used to infer “who”, “what”, etc.), proﬁle (i.e., 
metadata such as “when”, “where”, etc.), and relational information (i.e., implied 
relations between users, especially in group photos). For example, consider a typical 
scenario in which “Alice” falls in love with “Bob”. Alice is excited and wants 
everyone to know about her feelings except for her parents. However, given the 
inseparability of photo content and the relevancy of implied relations, undesirable 
information may be exposed when Alice and her friends share photos with others. 
From this point of view, Alice should carefully consider the sharing range of group 
photos with Bob (even those that include other people) before posting them on PSPs. 
Otherwise, content, proﬁle, and relational information about Alice may be revealed 
to undesirable users. In psychology, improving or maintaining the impression one 
gives to others is referred to as “impression management”; here, Alice is performing 
impression management by intentionally not revealing her relationship with Bob to 
her parents. Impression management refers to avoiding unnecessarily signiﬁcant 
impression changes, so we should avoid having it affected by privacy breaches in 
image sharing. 
Since people usually prefer to give others a positive impression rather than 
negative ones in their daily lives, impression management during photo sharing is 
becoming increasingly important. However, most of the existing privacy-preserving 
solutions have two main drawbacks: (1) Users must decide manually whether to 
share each photo with others or not, to build the desired impression; and (2) users run 
a high risk of leaking sensitive relational information implicating in group photos 
during photo sharing, such as the number of times people appearing together, the 
relative position of faces, and other information that can be used to infer the user’s 
relationship between lovers. 
The authors of this book proposed a privacy-aware image perception and dis-
semination control strategy recommendation method based on interpersonal distance 
learning called SRIM (Social Relation Impression-Management) [23], which is used 
to maintain and enhance users’ relationship impressions in image sharing. This 
method belongs to the privacy perception stage in the privacy computing framework, 
evaluates the social relationships in the desired images using a relationship impres-
sion evaluation algorithm, and divides image recipients into recommended and 
non-recommended display categories based on historical information. This method 
not only prevents the leakage of user social relationship privacy information, but also 
automatically recommends suitable image sharing strategies. In addition, based on 
image metadata and face detection results, the authors of this book designed a 
lightweight face distance measurement algorithm, which provides SRIM with efﬁ-
cient and accurate face distances and converts the measured distances into social 
relationship categories.

146
F. Li et al.
Perception and Policy Recommendation Framework 
Based on a user’s historical sharing behaviors, our SRIM scheme offers recommen-
dations and warnings. For example, they may recommend an appropriate group of 
recipients, or warn of inappropriate sharing behavior. As shown in Fig. 4.17, when a 
new group photo is uploaded, faces are ﬁrst detected and recognized by SRIM. 
Interpersonal distance thresholds are used to distinguish social relationships between 
users. If a classiﬁcation result has never appeared in someone’s history, our scheme 
ﬁlters out these users in advance and alert users. For users with shared records, this 
method analyzes the user’s existing relationship impression through an evaluation 
algorithm and classify them into two groups (i.e., a group of recommended recipients 
and a group of non-recommended recipients). Finally, after the sharing decisions 
have been made, new distances in photos are used to update the historical records. 
First, let us introduce two phenomena related to impression formation in real life. 
The ﬁrst phenomenon is that impression formation is often not based solely on 
information provided by a single user. Hence, the evaluation algorithm in this 
approach considers not only the pictures shared by the owner, but also those shared 
by stakeholders (e.g., users other than the owner in a group photo). The second 
phenomenon is that impressions deepen over time with accumulation, rather than 
being formed by a small number of pictures. Intuitively, the more comprehensive 
and stable the relationship impression in a group photo is, the easier they are for the 
recipient to detect changes. 
In abstract, the SRIM model contains three roles: the owner (the communicator of 
the impression), stakeholders (users appearing in the pictures), and the recipient (the 
receiver of the impression), deﬁned as h, s, and r, respectively. Due to the abstract
Fig. 4.17 Flow chart of our SRIM scheme

Þ
nature of impression in psychology and its different meanings in different situations, 
they are difﬁcult to have a uniﬁed and formalized deﬁnition through mathematical 
means. Researchers typically deﬁne these in a speciﬁc dimension or scenario. In this 
section, we provide the following three deﬁnitions of interpersonal relationship 
impressions based on different user roles.
4
Privacy Computing Techniques
147
Deﬁnition 4.25 The owner’s showed impression. fh = (1, 2, . . ., m) denotes the set 
of user h’s friends in the sharing photos. Each recipient in the PSP has a relational 
impression set of user h’s friends, which is formed from the historical sharing 
records. The set is described as Ih = (ih, 1, ih, 2, . . ., ih, m). To quantify the relations, 
we use historical proﬁles of face distances in order to initialize the relational 
impression of users, which is described as Iinit 
h = Dh,1, Dh,2, . . . , Dh,m
ð
. Parameter 
Dh,1 = Prelationj j 2 1, 2, . . . , 8
ð
Þ
 
represents the distribution of face distances in 
historical records, where Prelationj presents each probability of the eight relation 
types in interpersonal distance learning. These distances are classiﬁed into eight 
relations, where the probability of each relation is presented as Prelationj = mj=n, j = 1, 
2, ⋯, 8 and parameter mj represents the number of images with relation j, and 
n represents the number of total records. 
However, the photo owners are not the only ones who provide information that 
results in recipients forming an impression; other stakeholders in the photos may 
post photos to the recipients if they are friends of the recipients. 
Deﬁnition 4.26 (the recipient’s impression) The real impression is deﬁned as 
I0 
h = i0 
h,1, i0 
h,2, ⋯, i0 
h,m = D0 
h,1, D0 
h,2, ⋯, D0 
h,m
ð4:64Þ 
Since the owner cannot know the information that the stakeholders share with the 
recipient, the owner can only develop the optimum policies that lie within their 
access. 
Deﬁnition 4.27 The owner’s inferred impression. The owner can guess the rela-
tional impression of the recipients; this inferred impression is deﬁned as 
I00 
h = i00 
h,1, i00 
h,2, . . . , i00 
h,m = D00 
h,1, D00 
h,2, . . . , D00 
h,m . 
SRIM Scheme Design 
To evaluate the relationship impression changes conveyed by image owners to 
recipients, the following steps are given for the relationship impression evaluation 
algorithm. When a new image is uploaded, the SRIM algorithm will extract all 
potential recipients that may be affected by the image. The distance between the face 
of the image owner h and the stakeholders s is described as Lh, s. When recipient r 
receives a new image μ = {h, r, hh, s, Lh, si} from image owner h, image owner h can 
evaluate the recipient’s new impression i00 
h,s, which can be calculated using formula 
(4.65).

148
F. Li et al.
i00μ 
h,s = t0 
h,sD00new 
h,s þ 1 - t0 
h,s i00exist 
h,s
ð4:65Þ 
Where i00exist 
o,s 
represents the existing impression, D00new 
o,s is the new distribution of the 
shared photo set (i.e., with the new photo added and the oldest one deleted), and t0 
o,s 
represents the trust coefﬁcient between image owner h and recipient r. 
Obviously, the closer the impression set I' of the recipient is to the impression set 
I presented by the image owner, the higher the trust level of the recipient r in the 
impression set presented by the image owner h. However, the image owner h cannot 
obtain the impression set I′ of the recipient, so they can only use their own inferred 
impression set I″ to calculate an approximate t'h, s.Thus, t0 
o,s is calculated by the 
Hellinger distance, which is deﬁned as follows: 
t ′ h,s = 1 - 1 
2 
8 
j = 1 
i}h,s Prelationj -
ih,s Prelationj 
2 
ð4:66Þ 
To simplify the complexity of the model, the SRIM algorithm did not consider the 
forwarding function of the image-sharing platform in formula (4.46). After the 
evaluation algorithm is completed, the difference change between the two impres-
sion sets can be calculated by formula (4.67). 
Change = 1 - 1 
2 
8 
j = 1 
i00μ 
h,s Prelationj -
i00exist 
h,s 
Prelationj 
2 
ð4:67Þ 
By using a threshold to adjust the range of recommended users, a lower threshold 
can be selected for sensitive recipients, while a higher threshold can be selected for 
ordinary recipients. 
Resulth,s = 
Recommend, change < threshold 
Non - recommend, otherwise 
In general, the number of participants in a photo may affect the expression of a 
relational impression: The more participants appear in a photo, the less impression 
intensity the recipients feel. For example, given two group photos, A and B, and 
assuming there are two participants in A and 20 participants in B, even if the users’ 
relations are the same, a branding impression is much easier to apply for the two 
participants in A than for the 20 in B. 
We assume that recipients will pay approximately equal attention to each group 
photo. Thus, one relation in a multi-relation photo (i.e., a photo with at least three 
users) will only receive a fraction of the recipient’s attention. Each relation occupies 
2/[nu(nu - 1)] of the original size, where nu represents the number of users in the 
group photo.

4
Privacy Computing Techniques
149
Fig. 4.18 Multi-relation 
face distance measurement 
However, in reality, the recipient does not allocate equal attention to each 
relationship in the group photo. Users who are closer to the camera in the picture 
are more likely to receive more attention. As shown in Fig. 4.18, the distance 
between each user and the camera can be used to add weight to the impression 
strength between each pair of users. Therefore, the impression weight of the rela-
tionship between User A and User B can be represented as: 
Weighta,b = 2 
p - da þ db 
2 
p 
j = 1 
dj 
ð4:68Þ 
Where p is the number of relations in the photograph. 
The probability of each relation in the multi-relation photos can be decided as 
follows: 
Prelationj = 
mj 
k = 1 
relationj,kweightk 
n 
t = 1 
relationtweightt 
, j = 1, 2, . . . , 8
ð4:69Þ 
Where mj is the assigned number of relation j and n is the total record number. 
As the number of participating users in the picture increases, the complexity of 
the calculation also increases, and the number of interpersonal relationships in the 
picture grows by nu! factorial. Fortunately, people can only remember a limited 
number of things. For example, in pictures taken at parties or graduation ceremonies, 
recipients can only remember if someone is a member of the group or not. Therefore, 
the SRIM algorithm sets a threshold for the number of users in the picture. If there

are seven or more users in the picture, they are considered a group photo, and the 
default strategy is executed (ignoring the impact of the group photo on the users’ 
impression of their interpersonal relationships). 
150
F. Li et al.
SRIM uses a lightweight computational algorithm to measure the distance 
between faces in a group photo, utilizing principles of camera imaging and social 
distance learning thresholds to identify the type of interpersonal relationship 
between users. The algorithm fully utilizes metadata information captured by the 
camera 
during 
the 
photo 
shoot, 
achieving 
high 
accuracy 
with 
low 
computational cost. 
To calculate the distance between faces in a group photo, the algorithm utilizes 
facial detection technology and extracts the 35 mm equivalent focal length informa-
tion from the photo’s EXIF metadata. Modern digital cameras have mostly 
converted the camera’s actual focal length into the 35 mm equivalent focal length, 
which represents the focal length value when using 35 mm ﬁlm instead of the CCD 
(Charge-Coupled Device) sensor. Therefore, they are relatively easy to convert the 
distance between two points in an actual photo to the ratio on a 35 mm ﬁlm (36 mm 
long and 24 mm wide). When a new user creates an account on a social network, 
they need to upload several photos of their facial area as samples. As the SRIM 
method is based on facial width as a benchmark for calculating facial distance, the 
system encourages users to provide their facial width. If users do not want to provide 
facial width, a default value of 15 cm will be used. 
Figure 4.19 is a simpliﬁed camera imaging system, where parameters w1 and w2 
are the actual widths of the faces of user 1 and user 2, respectively, and parameters l1 
and l4 are the widths of the face regions of user 1 and user 2 in the image. Parameter 
l2 is the distance between user 1 and user 2 in the photo, and parameter l3 represents 
the distance between user 2 and the camera lens axis when user 2 is farther away 
from the lens. Parameter f is the focal length of the camera. Based on the above 
information, the distances of user 1 and user 2 to the camera can be calculated as 
d1 = fw1/l1 and d2 = fw2/l4, respectively. Therefore, the distance L between the two 
users’ face regions can be represented as 
L = 
d2 - d1
ð
Þ2 þ h1 þ h2 þ h3 þ h4
ð
Þ2
ð4:70Þ 
Where h2 is the distance between the face regions of user 1 and user 2′,2′ is the 
projection of user 2 on the imaging distance d1 plane, thus the face distance can be 
expressed as h2 = l2d1/f. The distance h3 between the shooting projection and the 
vertical position coordinate of user 2 can be expressed as h3 = l3(d2 - d1)/f. In  
addition, h1 and h4 are half of the face width of user 1 and user 2, respectively, and 
can be described as h1 = w1/2 and h4 = w2/2. Therefore, eq. (4.70) can be expressed 
as.

4
Privacy Computing Techniques
151
Fig. 4.19 Face distance 
measurement algorithm 
d2
X
f 
d1 
d2-d1 
h1 
h4 
h3 
h2 
l1
l4 
l2 
l3 
L 
User1 
User2 
User2' 
L = 
w1 
l1 - w2 
l4 
2 
f 2 þ w1 þ w2 
2 
þ l2 - l3
ð
Þw1 
l1 
þ l3w2 
l4 
2 
ð4:71Þ 
The scheme designs a lightweight face distance measurement that calculates the 
distance between users’ faces in photos by relying on photo metadata and facial 
detection results. Then, using proxemics, these distances are converted into relation-
ships. In addition, the authors of this book propose a relationship impression 
evaluation algorithm to evaluate and manage relationship impressions. The experi-
mental results verify the effectiveness of this scheme. 
4.3.2.2 
Sharing of Privacy Photos Within a Single Information System 
The pictures people share in daily life often involve information about friends and 
strangers. Due to privacy concerns, they do not want to be exposed to unauthorized 
recipients. Existing privacy-preserving solutions for pictures often suffer from the 
following problems: Firstly, access control schemes for picture sharing require 
participants to set policies for each picture, which results in a high time cost for 
users to set policies. Some privacy-sensitive users even choose not to share any 
pictures at all. Secondly, most picture privacy policy recommendation schemes rely 
heavily on semi-automatic tag propagation algorithms or picture classiﬁcation algo-
rithms, which heavily depend on the training sample library. When there are too few 
training samples or new privacy categories are added, the accuracy of these schemes

is not high. Thirdly, most methods do not provide special treatment for strangers in 
the pictures. 
152
F. Li et al.
The author of this book proposes a picture-based user privacy-preserving strategy 
generation method called HideMe [24], which supports extended control for picture 
sharing. Users can use rich content elements to construct objective scenes, and then 
protect their privacy through an access control model based on picture scene 
information. 
As a special participant in the photo, an accidentally captured stranger also 
participates in the generation of picture privacy policies. Unlike ordinary photo 
participants, accidentally captured strangers usually do not know when or where 
they were accidentally captured. In response to the problem of existing methods 
paying less attention to the privacy information of strangers in pictures, HideMe 
designs a shooting-distance-based stranger detection algorithm, which selects acci-
dentally captured strangers from the perspective of a shooting distance threshold. 
Face matching is one of the core modules of the HideMe solution, which aims to 
identify speciﬁc users in shared pictures on the picture sharing platform. The 
effective execution of privacy policies depends on successfully ﬁnding the correct 
users appearing in the shared pictures. HideMe proposes a face matching scheme 
based on deep features, which can use face attributes to narrow down the candidate 
set of users that need to be matched, thus effectively improving the efﬁciency of face 
matching. 
HideMe Scheme Framework 
As shown in Fig. 4.20, HideMe has three phases: photo uploading, policy generat-
ing, and photo viewing. 
Photo Uploading 
Once the photo-uploader uploads a photo, HideMe extracts valid information from 
metadata, and calculates the aforementioned control factors in turn. Firstly, HideMe 
extracts information from the metadata including date, time, longitude, latitude, 
35 mm equivalent focal length, and digital zoom ratio, etc. Then, all the faces in 
the photo are detected, and the relative pixel coordinates and size are obtained and 
stored. HideMe design a facial-attribute-based face matching module to meet the 
demands of large scale user sets. Some information such as date, time, longitude, and 
latitude can be employed as factors directly, and others can be used to calculate 
factors like PoIs and Photographic Distance. 
Policy Generating 
Each associated user in the photo has the right to decide whether to blur/show their 
face or not. However, setting policies for each photo may be hard and time wasting. 
Therefore, HideMe builds a scenario for each policy-generator in order to help them

to choose to blur/show their faces, instead of setting policies photo-by-photo. 
Speciﬁcally, when a photo is uploaded, HideMe identiﬁes all faces and obtains the 
relative user IDs from OSNs, and then associates the corresponding scenarios with 
this photo. In this way, the policy-generators do not need to set policies for each 
photo. With this method, the policy generator no longer needs to set privacy policies 
for each individual image or manually group uploaded images. 
4
Privacy Computing Techniques
153
Metadata Extract 
Photo-uploader 
Indicator Calculate 
Spatial 
Indicator 
Indicators 
Social 
Networks 
Relationships 
Polices 
Photos 
Policy-generator 
Photo-viewer 
Blur 
View Request 
Interpersonal 
Indicator Calculate 
Temporal 
Indicator 
Photography Attribute 
Indicator 
Face Matching 
Scenario-based 
Policy Generation 
Distance-based Passing 
Stranger Detection 
Scenario-based 
Access Control
 
Fig. 4.20 HideMe scheme 
Photo Viewing 
When a photo is requested by a photo-viewer, HideMe processes the policies on all 
detected faces in the photo. Speciﬁcally, HideMe ﬁrst searches the interpersonal 
factors between the photo-viewer and each policy-generator, and calculates them 
when the request comes from a new viewer to the policy-generators. The scenario-
based access control processes authorization to query related privacy policies in 
parallel and blur the users’ face region based on their privacy policy. As shown in 
Fig. 4.21, after all user policies are successfully matched, merge the images and 
show them to the photo-viewer. 
To ensure the efﬁcient and stable operation of the policy generation method, 
HideMe 
has 
designed 
two 
auxiliary 
algorithms: 
pedestrian 
detection 
and 
pre-matching of face attributes.

154
F. Li et al.
Photo Request 
Face Identify 
Face 2
Face n 
Policy 1 
Merge the Face 
Exhibit the Photo 
... 
Policy 2 
Policy n 
... 
Face 1 
Policy 1 
T 
Photo Info 
S 
A 
Relation Info 
I 
SDA 
Blur/Show
 
Fig. 4.21 Scenario-based access control model ﬂow 
Pedestrian Detection Algorithm Based on Shooting Distance 
Most existing works consider that some metadata about photographic parameters in 
images are not closely related to user privacy, such as camera focal length. However, 
this section calculates the shooting distance using the facial length and camera focal 
length and sets a threshold to detect whether the users in the image are pedestrians, 
which protects user privacy from another perspective. This method requires the 
image metadata and facial pixel coordinates at the image upload stage. 
In photography, Angle of View (AoV) describes the angular extent of a given 
scene that is taken by a camera, and they can be used to estimate suitable photo-
graphic distance for taking better photos by experienced photographer. 
Suppose the distance between a rectilinear lens and the shooting object is r1, the 
distance between the lens and the image plane is r2, and the dimension of the frame, 
which forms an image, is d (the ﬁlm or image sensor). The camera imaging principle 
can be simpliﬁed to aperture imaging (technically, the perspective center of a 
rectilinear lens can be seen as the center of aperture imaging). Then, the camera 
imaging process can be simpliﬁed as Fig. 4.22a. To project a sharp image of distant 
objects, r2 needs to be equal to the facial length f of imaging system. Therefore, the 
AoV α can be calculated from the chosen dimension d and the effective focal length 
f as follows: 
α = 180 
π . 2 arctan d 
2f
ð4:72Þ 
Where f represents physical focal length of the camera, and d represents the size of 
the ﬁlm (or sensor). As we know, digital cameras often use different lenses and 
sensors, and they have been mostly transformed into 35 mm equivalent focal length 
and 35 mm ﬁlm (36 mm wide and 24 mm high). 
So, HideMe can directly replace actual physical focal length and size of sensor 
with 35 mm equivalent focal length and 35 mm ﬁlm.

4
Privacy Computing Techniques
155
Fig. 4.22 Passer-by 
detection method based on 
shooting distance. (a) 
Camera imaging principle. 
(b) Target face area length 
calculation 
Since pixels of the entire image and the targeted face are detected by face 
recognition, we could calculate the height of the image on the sensor by using digital 
zoom radio and face length (note that, each user could set their own face length in 
their personal attributes). As shown in Fig. 4.22, the photographic distance be 
calculated by trigonometric function as: 
r1 = 
π 
180 . Pix 
2pix lz . cot α 
2
ð4:73Þ 
Where Pix, pix, l and z represent pixels of the entire image, pixels of the targeted 
face, the physical length of face and digital zoom ratio, respectively. If there are 
multiple faces in the photo, we can use different physical lengths and pixels of faces 
to calculate different distances from each face to the camera. 
Face Matching Algorithm Based on Facial Attribute Features 
Since HideMe will be deployed in OSNs (e.g., WeChat, Facebook) with massive 
number of users, there are challenges to efﬁciently detect face and match a detected 
face to a speciﬁc user. This section proposes a depth-feature-based face matching

algorithm, which can dramatically improve the efﬁciency of face matching, since we 
can use facial attributes to signiﬁcantly reduce the number of users to be compared. 
156
F. Li et al.
The speciﬁc process of the face matching algorithm based on facial attributes is 
shown in Fig. 4.23. Given a shared image, HideMe will detect all facial regions and 
input them into a facial attribute classiﬁer. Then, the facial attributes of each detected 
face will be predicted based on facial attributes, which are used to reduce the number 
of users to be compared by ﬁnding users who have the identical facial attributes in 
the face database. Finally, HideMe compares the feature vector of each detected face 
with that of possible matched users. If the difference of two feature vectors is less 
than a predeﬁned threshold, a match is identiﬁed. 
To ﬁlter out suitable facial attribute candidate user groups, we propose a facial 
attribute classiﬁer based on the Adapted Balanced Convolutional Neural Network 
(ABCNN) model, where a weighted objective function is constructed to improve the 
prediction accuracy sufﬁciently. 
Formally, we denote the set of input photos as ℙ, the number of facial attributes as 
N, possible decision functions in the hypothesis space as H, respectively, and hi(θT x) 
represents the decision function, where θ = (θ1, θ2, . . ., θN) is the network weights. 
For a given photo x 2 ℙ, yi 2 (-1, +1) is the binary label of the i-th attribute. 
i 2 (1, 2, ⋯, N) represents the index of facial attributes. We deﬁne the loss function 
of the i-th facial attribute as Lossi(hi(θT x), yi).  Lossi
ð
Þ
 
is the expected loss over the 
range of inputs P. Thus, the optimization task aims at the minimum expected squared 
error for each attribute. 
8i : hi = arg min 
hi2H 
 Lossi
ð
Þ
ð4:74Þ 
However, traditional approaches consider facial attributes as N independent tasks, 
and train independent classiﬁer for each facial attribute, so they are hard to learn the 
latent correlations between attributes. To take advantage of these latent correlations 
better, our classiﬁer is trained to learn all these facial attributes simultaneously. In 
addition, the distribution of the attribute label should be identical between the
Detected Face 
Facial Attributes
Users Groups 
[person_id:4; group_id:1111;face_id:004] 
[person_id:1; group_id:1111;face_id:005] 
[person_id:1; group_id:1111;face_id:003] 
... 
Face 
Database 
eyeglass 
young 
bald 
male 
Facial 
Attribute 
Classifier 
Face 
Matching 
Candidate Mathing List
Matching 
Results 
Male: 1      Female:-1 
Young:1     Not Young:-1 
Bald:1        Neg:-1 
Eyeglass:1 Not Wear:-1 
Fig. 4.23 The speciﬁc process of face matching algorithm based on face attribute

training set and the testing set. Therefore, balancing the dataset to facilitate in 
training a better classiﬁer is necessary.
4
Privacy Computing Techniques
157
One method of obtaining a balanced dataset is to collect an image dataset with an 
even distribution of each attribute. However, this greatly increases the workload, as 
in practical applications, most data are not evenly distributed, making it challenging 
to ﬁnd such a dataset, especially for large-scale datasets. Another method is to 
modify the loss function to simulate a balanced dataset. The ABCNN proposed in 
this section modiﬁes the objective function to address the imbalance between the 
training and testing sets. Speciﬁcally, a mixed objective function is proposed by 
considering the distribution difference between training set and testing set as adapted 
weights. Firstly, we calculate the training distribution Si for each attribute i, which 
uses the fraction of positive samples Trainþ 
i 
0 < Trainþ 
i < 1 and negative samples 
Train -
i 
0 < Train -
i 
< 1 in the training set. An adapted weight is assigned for each 
class of attribute i by given binary testing distribution Testþ 
i 
and Test -
i 
(where 
Testþ 
i þ Test -
i 
= 1). 
w ijyi = þ 1
ð
Þ = 1 þ
Diffþ 
Testþ 
i þ Trainþ 
i 
ð4:75Þ 
w ijyi = - 1
ð
Þ = 1 þ
Diff -
Test -
i 
þ Train -
i 
ð4:76Þ 
Where Diffþ = Testþ 
i - Trainþ 
i 
and Diff - = Test -
i
- Train -
i . From the above 
equations, if the fraction of positive or negative samples in the training set is less 
than the testing set, the weight of the i-th facial attribute will be increased. 
Intuitively, increasing these weights can balance the distribution difference 
between training data and test data. Similarly, the weight of positive or negative 
samples in the training data will be decreased if they are higher than those in the 
testing data. Then, these adapted weights are integrated into the mixed objective 
function. As a result, the task of this model optimization is to ﬁnd an optimal loss 
function to replace the traditional hinge loss function and minimize the variance 
between the prediction label and the target label. So, the optimization problem of an 
M-element training set X with labels Y can be expressed as: 
8i : arg min 
hi2H 
 L X, Y
ð
Þ
ð
Þ
 
= arg min 
hi2H 
 
M 
j = 1 
N 
i = 1 
w ijYji x
ð Þ  hi Xj - Yji x
ð Þ  
2
ð4:77Þ 
By replacing the standard loss layer of the deep convolutional neural network 
with formula (4.77), we can construct the ABCNN model required for face attribute 
classiﬁcation. 
HideMe can help users hide their faces on relevant photos through a one-time 
policy generation. In addition, the face matching algorithm used by HideMe not only

protects user privacy but also reduces system overhead. Experimental results dem-
onstrate the effectiveness of this solution. 
158
F. Li et al.
4.3.3 
Privacy Operation Control for Document Formatting 
4.3.3.1 
Fine-Grained Privacy Operation Control Process in Document 
Formatting 
In the scenario of unpredictable information sharing, based on privacy operation 
control policies, the information is guided to be desensitized, exchanged, and used in 
accordance with the requirements of the previous sharer during the sharing process. 
The recipient of the document is only allowed to operate on the relevant content in 
the manner speciﬁed by the preceding sharer. 
The process of ﬁne-grained privacy operation control for document formatting is 
illustrated in Fig. 4.24. 
1. Initialization: Upon receiving the formatted document X it is known that 
X consists of n information components, i.e., X = {X1, X2,. . ., Xk, . . ., Xn}, 
where each information component Xk is composed of Xk=hc, A, Γ, Ω, Ψ, ℙi 
(contents and meaning detailed in Table 4.4). The existing privacy operation 
control policies X.ℙexist are extracted, and the segmentation of information com-
ponents can be performed using existing natural language processing methods. 
2. Iterative generation of privacy operation control policies: For information com-
ponent Xk, based on the existing privacy operation control policies Xk.ℙexist, and 
considering factors such as the usage attributes Attr(Si) of the current sharer Si and 
the privacy protection capability Ability(Si + 1) of the recipient Si + 1, a compre-
hensive analysis is performed to iteratively generate the j-th privacy operation 
control strategy Xk.Pj for information component Xk, along with the adjusted 
policy Xk. Pj 
' and the corresponding policy adjustment action t. 
3. Differentiated privacy operation control for document formatting: For each 
information component Xk, one or more operations can be performed for differ-
entiated desensitization control, exchange boundary control, and local usage 
control. Building upon the privacy operation control policies X.ℙ, different 
desensitization controls are applied based on the various modalities (i.e., data 
formats) of the document, such as images, text, and so on, along with the 
corresponding protection policies. Exchange boundary control is determined by 
the sharing direction of the current sharer, deciding whether partial ﬁltering, 
complete ﬁltering, or unrestricted access is required for the document. Local 
usage control is determined based on the local device mode, governing how the 
document is displayed, copied, pasted, and other local operations.

4
Privacy Computing Techniques
159
Fig. 4.24 Fine-grained privacy operation control method graph for layout documents

160
F. Li et al.
Table 4.4 Symbols 
Symbols 
Description 
c
Content of information component 
A
Privacy attribute vector (quantifying the preservation degree of privacy information 
components and component combinations) 
Ψ
Propagation control operation set (the operations that the information components and 
their combination can be performed on) 
Γ
Generalized positioning information set 
Ω
Audit control information set (subject and object information during the sharing 
process and records of performed operations) 
P
Iterative privacy operation control policy set 
4.3.3.2 
Generation of Iterative Privacy Operation Control Policies 
The generation of differentiated iterative privacy operation control policies requires 
a comprehensive consideration of existing desensitization control policies, the 
desensitization requirements of the current sharer, and the protection capability of 
the next receiver. The generation process is as follows: 
1. Extract desensitization requirements and previously used desensitization algo-
rithms from the received existing desensitization control policies. 
2. Take into account the settings of the current sharer, combine them with a pre-built 
desensitization knowledge graph, and generate new desensitization requirements. 
Invoke a sensitive information detection function to re-identify the privacy 
information components that need to be processed for desensitization in this 
iteration. 
3. Based on the new desensitization requirements, existing desensitization control 
policies, etc., calculate the expected desensitization effect for iterative desensiti-
zation. Choose an iterative desensitization algorithm and evaluate its performance 
by simulating the desensitization of information. If the desired iterative desensi-
tization effect is achieved, the algorithm passes the evaluation and can be used for 
the actual desensitization. Otherwise, a new desensitization algorithm is selected. 
4. Update the desensitization control policies based on the new desensitization 
requirements, sensitive information detection results, the used desensitization 
algorithm, and the evaluation results. Embed the updated desensitization control 
policies into the information after iterative desensitization and share it with the 
receiver. 
4.3.4 
Privacy Extended Control of Pictures Exchanged 
Across Systems 
To realize friend interaction on social networks, the extensive dynamic ﬂow of user 
privacy pictures across multiple information systems and boundaries has become a

norm. However, once users upload pictures to social network platforms, they lose 
control over the uploaded pictures. Meanwhile, the leakage and dissemination of 
some sensitive pictures seriously damages the privacy rights and interests of relevant 
parties, as the images in pictures can directly reﬂect things in the real world. 
However, most existing privacy-preserving work faces the following problems: 
traditional access control methods mostly focus on a single system, making they 
difﬁcult to apply to forwarding scenarios across social networks; encryption-based 
picture privacy-preserving methods consider access control policies less, and 
whether a visitor can access the picture depends entirely on whether they have a 
key; due to the complexity and display issues of pictures, traditional policy pasting 
methods cannot be directly applied to picture sharing. On the other hand, most 
tracking and tracing methods store privacy information and tracing records sepa-
rately, making it impossible to judge privacy infringement when privacy information 
leaves the information system. 
4
Privacy Computing Techniques
161
The authors of this book propose a PrivacyJPEG framework [25] for privacy 
image sharing based on across systems interaction. They are applied to both 
extended control (forward) and tracking and tracing (backward) scenarios from the 
perspective of image propagation. Speciﬁcally, this method binds privacy tags and 
access control policies to images and uses encryption algorithms to ensure that only 
users with permission can access the privacy area of the image when they are spread 
to other social networks. At the same time, by adding tracking and tracing informa-
tion to privacy tags, forensic personnel can track and trace privacy infringement after 
a privacy leak event occurs. 
4.3.4.1 
The Goal of PrivacyJPEG 
The main goal of PrivacyJPEG is to protect the privacy information of users during 
the process of image sharing, especially after the image has been forwarded to other 
social networks, still in accordance with the wishes of the image owner and previous 
forwarders for dissemination and processing. Speciﬁcally, PrivacyJPEG addresses 
the following four issues. 
1. To address the issue that existing access control models do not treat forwarders as 
a special role, a propagation chain-based access control model is designed, which 
allows users to control the permissions that subsequent users can assign on the 
propagation chain, solving the problem of extending control strategy allocation 
for privacy images after forwarding. 
2. To address the difﬁculty of key management for image encryption algorithms 
when they are transmitted across social networks, a dual-layer image encryption 
algorithm is proposed. In the ﬁrst layer, symmetric encryption is used to protect 
the privacy area of the image, and in the second layer, public key encryption is 
used to protect the user’s access control policy and grant decryption rights based 
on the policy.

162
F. Li et al.
3. To address the complexity of image ﬁle features, a policy embedding format that 
supports image features is designed, and the bound policy does not affect the 
normal display of the image’s public area during propagation in social networks. 
4. To meet the need for anti-tampering and anti-forgery of privacy traceability 
records as they ﬂow across social networks, a nested signature algorithm is 
proposed. This algorithm prevents malicious users from tampering with trace-
ability record information and forging evidence during image propagation by 
signing their own records along with the previously signed records of other users. 
4.3.4.2 
Scheme Design of PrivacyJPEG 
Attack Model 
To achieve privacy image sharing across systems interaction, PrivacyJPEG catego-
rizes attackers into three types of attacks. Firstly, attackerl attempts to obtain 
unauthorized privacy information by retaining, tampering, or forging access control 
policies. Secondly, attacker2 is an internal user of the system with partial privileges 
and attempts to gain more privileges using the ones they already have. Thirdly, 
attacker3 has knowledge of background information such as policy embedding or 
traceability record formats and attempts to cover up privacy infringement behavior 
by tampering or forging access control policies or traceability records. 
To ensure the security and integrity of privacy images in the process of cross-
system circulation, PrivacyJPEG uses existing cryptographic technologies such as 
image area encryption and the Public Key Infrastructure (PKI) system. They encrypt 
the privacy area and access control policies of images transmitted between entities to 
ensure the security of privacy images during transmission and prevent passive 
attacks such as eavesdropping and interception. Therefore, only a few common 
encryption algorithms are compared for security, and no detailed security proof is 
provided for the above security issues. 
Goals of Privacy Preservation 
Based on the above attack models, the goals of privacy preservation are the follow-
ing: (1) When privacy images are circulated on different social networks, both 
internal and external users of the system cannot obtain unauthorized privacy infor-
mation; (2) Internal users of the system, except for the permissions assigned to them, 
cannot bypass the permission assigner to obtain more operational permissions; 
(3) Privacy infringement actions can be traced and tracked, and attackers cannot 
conceal privacy infringement actions by tampering with or forging traceability 
records. 
Figure 4.25 illustrates how PrivacyJPEG works, where users have three roles. 
(1) Photo Owner: the user who ﬁrst uploads a speciﬁc photo. (2) Photo Forwarder:

the user, who receives this photo, and forwards it with other users. (3) Silent 
Receiver: the user, who receives the photo, but does not forward it again. 
4
Privacy Computing Techniques
163
In the model shown in Fig. 4.25 User 1 is the photo owner, User 2 and User 4 are 
photo forwarders, User 3 and 5 are silent receivers. If user 1 uploads a picture to 
Facebook, user 2 receives the picture and forwards it to user 3 on Facebook. At the 
same time, user 2 downloads the picture from Facebook and forwards it to user 4 on 
WeChat. User 4 forwards the picture to user 5 on WeChat. The entire process 
involves forwarding across two social networks, Facebook and WeChat, and there 
is forwarding behavior within both social networks. This section combines three 
roles to form a chain, called Dissemination Chain. They describe a route of photo 
dissemination. For example, “User 1-2-3” is an internal dissemination chain, and 
“User 1-2-4.5” is a propagation chain across two social networks, Facebook and 
WeChat. 
The PrivacyJPEG framework is mainly aimed at cross-system image forwarding 
scenarios and is also applicable to single information systems with propagation 
chains. Users on the image propagation chain can control the access and extended 
control permissions of subsequent users. PrivacyJPEG framework consists of two 
main parts: (1) a client-side for preserving photo privacy areas and matching the 
access control policies, and (2) a server for managing the secret keys. In our 
framework, we assume all local client components (operating systems, applications, 
sensors, etc.) and the key management center are trusted. On the contrary, all 
propagation channels and picture sharing service providers are generally considered 
to be untrusted. 
In this system, users can perform the ﬁrst layer of encryption on the image’s 
privacy area through the client, using the same or different keys for different privacy 
areas. On this basis, users can set access control policies and extended control 
policies for the privacy areas, and perform the second layer of encryption. Then, 
the policies are bound to the image through the client. If the photo is uploaded to a 
public social network, then the social network acts as a “bridge” between users. 
After receiving the image, users can request to view or forward the image. The 
client veriﬁes the user’s identity and “unlocks” the policy bound to the image by
User 1 
User 2 
User 3 
User 4 
User 5 
Fig. 4.25 The system model of exchanging picture sharing across systems

requesting the server. Only users who meet the access control policy requirements 
can access the privacy area of the image. When forwarding the image, the receiving 
user can append new policies, and the policies set when forwarding the image must 
be a subset of the extended control policies. Therefore, by setting extended control 
policies, users on the propagation chain can restrict the propagation behavior of 
subsequent users.
164
F. Li et al.
PrivacyJPEG 
Client-side
Server-side 
User 2 
User 3 
User 4 
User 5 
User 1's Policy 
User 1 and 2's Policy 
User 1's and 2's Policy 
User 1 
Trust 
Authority 
User 1, 2 and 4's Policy 
Fig. 4.26 PrivacyJPEG system design 
For instance, as shown in Fig. 4.26, User 1 encrypts two privacy areas of the 
photo and generates corresponding access control policies. When the photo is 
uploaded to a public social networks, User 1’s policy is also uploaded together 
with the photo. User 2 downloads the photo and determine whether the privacy areas 
on the image can be browsed based on the binding strategy on the image. At the 
same time, User 2 can also add new permissions based on the extended control 
permissions granted by User 1 and set that User 1 could browse the Privacy Area 
1 and User 4 could browse the Privacy Area 2. User 4 continues to append a new 
policy, which forbids User 5 to browse all Privacy Areas. 
Extended Control 
The PrivacyJPEG framework’s extended control mainly involves a propagation 
chain-based access control model, a double-layer encryption algorithm, and the 
embedding method and format of access control policies. 
1. Propagation chain-based access control model. Permissions are authorized 
sequentially based on the propagation chain, where the propagation chain con-
sists of multiple image exchanges in image sharing. The ith exchange of photo p

Dissemination chain-based constraints are used to specify the exact constraint
that a permission can be assigned from a sender to a receiver. They can be formed
into a general form:
Þ
ð
4
Privacy Computing Techniques
165
can be denoted as Ei = (si → ri, p), where si is a sender and ri is a receiver. 
Therefore, dissemination chain is deﬁned as an ordered set DC = (E1, E2, . . ., En). 
According to the continuity of dissemination chain ordered set DC, the next 
sender is the former receiver, which can be denoted as si + 1  = ri. The ﬁrst sender 
s1 is the photo owner, and the last receiver rn is the silent receiver. Beyond that, 
the rest of users on the chain are photo forwarders. 
PrAi = Ei, Pr  Ai,1, PrAi,2
½
]
ð
Þ = 
si →ri, p
ð
Þ, c1
ð 
, pr1Þ, 
c2
ð½ 
, pr2Þ]
ð
4:78Þ 
Where Ei = (si → ri, p) speciﬁes that the permission of the i-th exchange Ei is 
assigned from the sender si to the receiver ri, and the object is the photo p. PrAi, 1  
can be the assigned permission of what receiver ri can do, and PrAi, 2  is the 
permission that receiver ri can assign to the next receiver. The square bracket in 
[PrAi, 2] implies that this parameter is optional. When [PrAi, 2] is selected as a 
default value, this means that PrAi, 2  is equal to PrAi, 1. 
Additionally, to achieve the extended control, the following users should not 
be assigned with more permissions than former one, which can be termed as PrAi, 
2 ⊆ PrAi, 1. In the equation PrAi, x = (cx, prx), x = 1, 2, cx represents a set of 
constraint conditions, such as time, location, relationship of users, etc., and prx 
represents the set of permissions, such as view, forwarding, etc. We also show an 
example in Fig. 4.26, where User 2 agrees user 4 to view the privacy area 2 in the 
image, but user 4 is prohibited from assigning user 5 permission to view the 
privacy area 2 in the image The policy can be modelled as ((user2 → user4, area2), 
(user4, view), (user5, prohibit view)). 
2. Two-layer Encryption Algorithm. To meet users’ privacy needs, after an image is 
uploaded to a social network, the encryption algorithm needs to ensure that the 
privacy information on the image is not displayed to unauthorized users, thereby 
ensuring that the policies of previous users on the propagation chain are strictly 
enforced. Layer encryption algorithm. This section assumes that each user in the 
framework has at least one client, and that the key management center assigns a 
public and private key pair to each user. 
3. Embedding method and format of access control policy. To ensure normal 
display of images in social network propagation, two aspects need to be ensured: 
the image encryption algorithm does not affect the display of the public area 
while encrypting the privacy area, and the attached policy does not destroy the 
inherent format of the image and should be embedded in the custom area of the 
standard image format. 
The ﬁrst layer of image encryption. The perturbation algorithm in literature [26] is  
used to encrypt the privacy area of the image. This algorithm achieves the encryption 
effect by modifying the Discrete Cosine Transform (DCT) coefﬁcients of the 
JPEG ﬁle.

166
F. Li et al.
Selection of privacy area and parameter settings. The algorithm divides the image 
into 16-pixel × 16 pixel minimum coding units and generates a masking matrix. 
Non-zero elements in the matrix represent the serial number of the privacy area 
selected by the user. Therefore, all privacy areas marked by users are composed of 
small squares (minimum coding units), and each privacy area does not overlap, that 
is, each minimum coding unit can only have one area serial number, and serial 
number 0 represents the public area. For each privacy area, users can assign a keyn 
and an encryption strength leveln 2 (low, medium, high). 
The encryption process involves quantizing the DCT coefﬁcients into an 8x8 
DCT matrix, labeled as xi, i = 1, 2, . . ., 64. A random sequence is generated using 
the keyn as a seed, denoted as randomi, and the image pixels are encrypted by 
multiplying the DCT coefﬁcient matrix xi with the random sequence randomi. For 
different encryption strength, the algorithm performs different levels of perturbation 
based on the encryption strength leveln. When leveln = low, the system only 
modiﬁes the AC (Alternating Current) coefﬁcients of the three color components 
(YUV) of the image, where Y represents brightness or grayscale value, and U and V 
represent 
chrominance 
for 
describing 
image 
color 
and 
saturation. 
When 
leveln = medium, both the AC and DC (Direct Current) coefﬁcients of the brightness 
(Y) component are modiﬁed. When leveln = high, both the AC and DC coefﬁcients 
of all three color components (YUV) are modiﬁed. 
Strategy embedding and second-layer encryption. In order to ensure that the 
bound strategy does not affect the normal display of the public area of the image 
on social networks, a new access control embedding format has been designed. By 
embedding the strategy into the Image File Directory (IFD) structure of the 
Exchangeable Image File (EXIF) metadata of the image ﬁle, the current ﬁle format 
can be well compatible with existing image sharing platforms without modifying 
it. EXIF metadata is widely used in various image formats such as JPEG, RAW, 
TIFF (Tag Image File Format), and RIFF (Resource Interchange File Format). The 
principle of this embedding method can be applied to other image description 
metadata formats, thereby extending the generality of this method. 
The format of the embedded strategy includes two parts: ﬁxed and variable. The 
ﬁxed part includes ﬁxed properties such as the coordinates of the privacy area of the 
image, shooting time, and shooting location, while the variable part stores a dynam-
ically changing number of access control strategies. As shown in Fig. 4.27, since the 
IFD structure itself is a nested structure, including the data ﬁeld of the current IFD 
and multiple sub-IFD structures. PrivacyJPEG treats the syntax of the access control 
strategy (and the traceability record in Sect. 5.3) as a sub-IFD structure, and 
dynamically adds a sub-IFD structure for each added strategy (or record). When 
an image is forwarded, the embedded strategy will circulate with the image in 
different social networks. The permissions assigned by the embedded strategy 
should follow the constraints of the access control model based on the propagation 
chain. 
Building on the use of symmetric key encryption to implement the ﬁrst layer of 
image privacy area encryption, PrivacyJPEG utilizes Public Key Infrastructure (PKI) 
to protect the conﬁdentiality and integrity of the policy. Users on the information

propagation chain need to “unlock” the access control policy embedded in the image 
using a public key, ensuring that the policy has not been tampered with by other 
users during the process of image circulation. 
4
Privacy Computing Techniques
167
FFE1 
SSSS 
4578 6966
 0000 
4949 2A00 
0800 0000 
XXXX. . . . 
LLLL LLLL 
APP1 
Data 
APP1 
APP1 Data Size 
Exif  Header 
IFD0 (Main) 
XXXX. . . . 
XXXX. . . . 
TIFF Header 
IFD0's Data 
0000 0000 
XXXX. . . . 
XXXX. . . . 
0000 0000 
Eixf Sub IFD 
Exif Sub IFD's Data 
Interoperability IFD 
Interoperability IFD's Data 
XXXX. . . . 
0000 0000 
XXXX. . . . 
XXXX. . . . 
0000 0000 
XXXX. . . . 
XXXX. . . . 
00000000 
XXXX. . . . 
Makernote IFD 
Makernote IFD's Data 
Privacy IFD's Data 
Privacy IFD 
Track IFD 
Track IFD's Data 
IFD1 (Thumbnail) 
IFD1's Data 
Thumbnail 
XXXX. . . . 
00000000 
XXXX. . . . 
XXXX. . . . 
FFD8XXXX. . 
XXXXFFD9 
Directory 
Directory 
Directory 
Directory 
Link End 
Link End 
Directory 
Link End 
Directory 
Link End 
Directory 
Link End 
Link to IFD1 
Link End 
Fig. 4.27 EXIF policy embedding

168
F. Li et al.
4.3.4.3 
Tracking and Tracing of Privacy Infringement in Images 
Tracking and tracing of privacy infringement in images is the reverse process of 
privacy images extended control and there are difﬁculties in tracking across infor-
mation systems. Compared to the forward propagation of privacy information of 
images, the tracking and tracing method imposes higher requirements on preventing 
tampering and forgery of tracking records. However, there is currently relatively 
little research on tracking and tracing privacy infringement in images across infor-
mation systems. To achieve tracking and tracing of privacy infringement in images 
across systems, the PrivacyJPEG framework includes a method called PhotoTracker, 
which consists of two parts: tracking information recording and evidence collection 
of privacy infringement. 
The task of tracking and tracing privacy infringement in images is to uniformly 
describe the privacy information of the images in circulation. During image circu-
lation, different operations executed by different subjects on the privacy information 
of the images are continuously recorded. By comparing the standard for judging 
privacy infringement, they are determined whether privacy infringement has 
occurred. 
PhotoTracker Scheme Design 
Figure 4.28 shows the propagation path of images during user sharing, which forms 
a tree-like structure. The root node is the image owner, the leaf node is the silent 
receiver, and the intermediate node is the image forwarder. If the privacy image 
passes through the same user 2 multiple times during the propagation process, the 
user can be seen as multiple nodes, namely, the ﬁrst forwarding node of user 2 (node 
2) and the second forwarding node of user 2 (node 6). The depth of the tree structure 
represents the number of times the privacy information of images is forwarded on 
this propagation path. Therefore, tracing from any node to the root node can obtain a 
unique and acyclic tracing chain (reverse). For example, in Fig. 4.28, the tracing 
chain “11-5-2-1” is a tracing chain. 
PhotoTracker includes the following two phases. 
1. Tracing information recording phase. When the image begins to circulate, the 
image owner creates a tracing identiﬁer, which includes privacy information, 
privacy information judgment criteria, and tracing record information. During the 
propagation of the image, each time they are transferred to a user, the user’s 
sharing operation, processing operation, and behavior environment of the privacy 
information of images are recorded in the tracing information recording through a 
plugin, and nested signatures are used to ensure that the image is not maliciously 
tampered with or forged during propagation. This phase can be integrated with 
the extended control mechanism. 
2. Traceability phase. When a privacy leak of an image is discovered, PhotoTracker 
starts from the node where the leak was detected and traces back towards the root

4
Privacy Computing Techniques
169
1 
3
2 
6
8
7
5
4 
10
9
12
11
14
13 
User 1 
User 2
User 3 
User 4
User 3
User 2 
User 4 
User 8
User 3
User 5
User 7
User 8
User 11 
User 7 
Fig. 4.28 Propagation path 
node to obtain a traceability chain of the privacy information for the image. The 
forensics personnel conﬁrm the integrity of the traceability identiﬁer through 
signature veriﬁcation and determine whether there was a privacy violation based 
on the traceability record information and privacy violation criteria. 
Nested Signature Algorithm for Traceability 
To ensure that the privacy information of the image is not maliciously tampered with 
or forged during transmission, PhotoTracker’s traceability method incorporates a 
nested signature algorithm. Using this algorithm, the image owner creates a trace-
ability identiﬁer, generates the ﬁrst traceability record information and signs 
it. Subsequently, each image forwarder veriﬁes the previous record, combines their 
own operational behavior record with the previous traceability record information 
and its signature, and signs it. When determining privacy violations, forensics 
personnel need to verify the traceability records layer by layer to conﬁrm whether 
the traceability records of each node have been maliciously tampered with or forged. 
The entire process is automatically executed by PhotoTracker, and the nested 
signature is formally described in the following steps. 
1. Pre-processing. The image owner creates a traceability identiﬁer TraTag = hPri, 
Pol, Reci. The image owner generates the traceability identiﬁer, including pri-
vacy information, privacy violation criteria, and traceability records. The privacy 
violation criteria can be consistent with the access control policy of the extended 
control. 
The traceability record function is deﬁned as:

Þ
Þ
Þ
170
F. Li et al.
REC : S × O × 2OP × 2CON × Siguser →Rec 
Where, S represents the operating subject, O represents the operating object (record 
the current hash value of the privacy information), OP = {op1, op2, ⋯} represents 
the set of operating actions, CON = {con1, con2, ⋯} represents the set of environ-
mental conditions under where the operating actions occur, and Siguser represents the 
user’s signature value. 
The signature function is deﬁned as: 
SIG : Rec × S × O × 2OP × 2CON →Siguser 
Therefore, the traceability record created by the image owner is: 
Sigowner = SIG Sowner, Oowner, OPowner, CONowner
ð
 
Recowner = REC Sowner, Oowner, OPowner, CONowner, Sigowner
ð
 
2. Initial forwarding process. According to the received traceability record 
Recowner, the process of the image forwarder forwarder1 is as follows: 
Step 1: Verify the signature value Sigowner of the received traceability record 
information. If the veriﬁcation passes, enter step 2. If the signature value does 
not match, they are considered that the traceability record has been maliciously 
tampered with and jump to step 4. 
Step 2: Compare the hash value Hashtrack of the operation object Oowner in the 
received trace record with the hash value Hashreceive of the received privacy 
information of images. If they match, continue to step 3. Otherwise, they are 
considered that the image privacy information does not match the trace record 
and the trace record has been maliciously tampered with, Jump to step 4. 
Step 3: Record the user’s own operation on the privacy information of 
images and use the signing private key issued by PKI to sign the received 
trace record information Recowner as part of a new trace record, as follows: 
Sig1 = SIG Recowner, S1, O1, OP1, CON1
ð
 
The signature content of the traceability record Rec1, including all parts of 
traceability record Recowner received before and part Rec1 except for the 
signature. 
Step 4: When signature veriﬁcation fails or the privacy hash value does not 
match, warn the current user that the traceability record has been tampered with 
and cannot be used as evidence to determine privacy infringement. Please do not 
continue to forward.
3. Intermediate 
forwarding 
processing. 
The 
intermediate 
image 
for-
warder forwarderi(1 < i ≤ n) performs the same processing ﬂow as the ﬁrst 
image forwarder forwarder1 in step 2. Each signature content of the traceability 

Þ
4
Privacy Computing Techniques
171
record Reci includes all parts of the traceability record Reci - 1 and part Reci 
except for the signature, as follows: 
Sigi = SIG Recowner, Rec1, ⋯, Reci - 1, Si, Oi, OPi, CONi
ð
 
4. Evidence collection for privacy infringement. When privacy leakage occurs, the 
evidence collection personnel need to verify the traceability record information 
of the leakage node layer by layer to ensure that the information on the entire 
information propagation chain has not been tampered with or forged. 
Determination of Infringement and Traceability Algorithm 
After conﬁrming that the traceability record information on the entire propagation 
chain has not been tampered with or forged, the evidence collection personnel need 
to determine the privacy infringement behavior. Speciﬁcally, there are two steps as 
follows: 
Step 1: Determine whether there is any action in the traceability record informa-
tion of the current node that violates the standard of privacy infringement behavior 
(or the extended control policy). 
Step 2: If there is no privacy infringement behavior in the traceability record of 
the current node, judge the traceability record of the previous node and repeat step 
1 until the privacy infringement behavior is found. 
Experiments have shown that the PhotoTracker scheme can be applied to trace-
ability forensics, providing traceability record information for discovering privacy 
infringement behavior. 
4.4 
The Applications of Privacy Computing 
4.4.1 
Secure Computing of Multi-party Data 
4.4.1.1 
Secure Computing System Solution for Multi-party Data 
The architecture of the secure computing system for multi-party data is shown in 
Fig. 4.29. The system conﬁgurations of A and B are the same, and we only describe 
the system architecture of A in detail. The system mainly consists of a desensitiza-
tion control unit, secure computing unit and data storage. The key to this solution is 
that both the desensitization control unit and the secure computing unit are secure 
computing environments. The desensitization control unit is used to negotiate the 
data to be exchanged with other participants, to perform secure transmission of 
selected data, to randomly scramble user identiﬁcation, and to store the obtained data 
in local storage after encryption. The data in the storage is decrypted during the

172
F. Li et al.
Fig. 4.29 Multi-party data secure computing architecture diagram

secure computing, and the user ID is re-scrambled and securely transferred to the 
secure computing unit. The secure computing unit completes the joint modeling or 
model inference computing of the data and returns the results to the desensitization 
control unit. Both the desensitized control unit and the secure computing unit are 
secure computing environments, and neither the operating system nor the user of 
Party A can see the data in the desensitized control unit and the secure computing 
unit. There are no additional security requirements for data storage. Due to the 
introduction of the scrambling function, there is no way for the user on the A side 
to get the real user identity corresponding to the data in the computing process. 
Therefore, it is possible to achieve secure multi-party computing while protecting 
user privacy.
4
Privacy Computing Techniques
173
Party A and Party B collaborate on a certain model training task, and A needs to 
obtain user data from B for joint modeling. The secure computation process for 
multiple parties is as follows: 
1. A and B ﬁrst negotiate a data transmission encryption key through authentication 
and key negotiation. B encrypts the data with the transmission encryption key and 
passes it to the desensitized control unit of A. 
2. A decrypts the data using the negotiated transmission encryption key, scrambles 
the user identiﬁer in the data record to obtain the scrambled identiﬁer, encrypts 
the scrambled data and stores it in the data storage system together with the 
scrambled identiﬁer. 
3. When performing the computing task, A inputs the user’s identiﬁcation, scram-
bles the identiﬁcation and ﬁnds all the encrypted data corresponding to the user’s 
identiﬁcation in the data storage, and passes the encrypted data to the encryption 
and decryption engine of the desensitization control unit. 
4. The desensitization control unit re-scrambles the user identiﬁcation to obtain the 
job identiﬁcation, decrypts the encrypted data, encrypts it with the key of the 
secure computing unit, and passes it into the secure computing unit together with 
the job identiﬁcation. 
5. The secure computing unit decrypts the data, completes the computing task, and 
passes the results along with the job identiﬁcation to the desensitized control unit. 
6. The desensitization control unit maps the job identiﬁcation back to the original 
user identiﬁcation, and outputs it together with the corresponding computing 
results. 
4.4.1.2 
Application of Secure Computing of Multi-party Data 
in Financial Scenarios 
Taking the training of credit assessment model for ﬁnancial risk control as an 
example, the credit assessment model needs to compare the borrower’s credit history 
information with the credit habits of all borrowers in the database, and check whether 
the borrower’s behavior is similar to the behavior trends of various borrowers who 
are in ﬁnancial difﬁculties such as frequent default, overdraft, or even insolvency. 
The basic algorithm of the credit assessment model is to jointly model the data of

strong ﬁnancial attributes (such as the number of times a borrower applies for a loan 
in a month), weak ﬁnancial attributes (such as the borrower’s monthly income 
balance), as well as the borrower’s social network data, e-commerce consumption 
data, and travel data using linear models such as logistic regression, random forest, 
and XGboost. The ranking and scoring level of borrowers’ default probability are 
calculated. The above data belong to different parties, such as banks, social net-
working platforms, e-commerce companies, and travel platforms, so it is necessary 
to realize the joint modeling of data from multiple parties while protecting users’ 
privacy information. 
174
F. Li et al.
Since the model of ﬁnancial risk control is also a valuable asset of ﬁnancial 
institutions, they do not want other participants to obtain it, while ﬁnancial institu-
tions want to obtain data from other participants to improve the accuracy of the 
model. In this case, the aforementioned secure computing scheme for multi-party 
data can be used to obtain the encrypted data of other participants, and complete the 
model training or inference calculation on the decrypted data with the scrambled user 
identiﬁcation in a secure computing environment such as the desensitization control 
unit and the secure computing unit, and obtain the computation results for the 
speciﬁc user. This solution protects the user privacy in the participant’s data and 
has high computational efﬁciency, and also protects the sensitive model of the 
ﬁnancial institution. 
4.4.2 
Federated Learning with Differential Privacy 
4.4.2.1 
Background 
Federated learning (FL) has recently gained popularity as a privacy-preserving 
training solution in the distributed learning paradigm [27]. In FL, multiple clients 
with private datasets collaborate to train models under the coordination of a central 
server, sharing local gradient updates instead of their private/proprietary datasets, 
thereby preserving the privacy of each participant’s raw data. 
However, despite FL’s widespread adoption, recent attacks have exposed vulner-
abilities in FL gradients, leading to privacy breaches [28–30]. If the original gradient 
is transmitted to an untrusted server, the user’s data is still at risk of privacy attacks 
[31–33]. To counter these threats, differential privacy (DP) with strict mathematical 
deﬁnitions has been integrated into FL. DP provides a quantiﬁable privacy leakage 
measure and adds carefully crafted noise to constrain loss. By injecting random noise 
into the gradient, DP offers statistical privacy guarantees for individual data, ensur-
ing that it is impossible to determine whether individual data points are involved in 
learning, or individual data do not have too much inﬂuence on the output of the 
function. This approach effectively prevents inference attacks. Moreover, local 
differential privacy (LDP) [34] provides stronger privacy guarantees, and clients 
perturb their gradient updates before uploading to the server to locally satisfy

DP. The perturb-before-upload manner of LDP also aligns with distributed system 
architectures, leading to the emergence of LDP-FL. 
4
Privacy Computing Techniques
175
4.4.2.2 
Typical LDP-FL Schemes 
Shokri et al. [35] ﬁrst applied LDP to FL, proposing a scheme that combines the 
sparse vector technique to ﬁlter out a subset of gradient updates exceeding a chosen 
threshold, perturbs the subset, and uploads it to the server for updating. All clients 
employ a consistent perturbation mechanism. The speciﬁc process is illustrated in 
the Fig. 4.30. 
To prevent further privacy leakage, both sparsiﬁcation and perturbation consume 
a privacy budget. The privacy budget used in each update round is bifurcated, with 
the ﬁrst portion devoted to adding noise to the chosen threshold during 
sparsiﬁcation. This ensures indistinguishability in subset selection. The second 
portion involves adding noise to the gradient portion exceeding the threshold. 
Both steps leverage the Laplacian mechanism for noise generation, with noise 
magnitude controlled by the privacy budget. 
Shokri et al.’s scheme effectively mitigates privacy loss arising from parameter 
selection and shared parameter values. However, it struggles to provide meaningful 
privacy guarantees for complex models with numerous parameters. 
Afterward, Bhowmick et al. [36] focused on deep learning models and presented 
a relaxed version of LDP that effectively defends against model inversion attacks. 
However, their scheme still requires a large privacy budget with limited attacker 
capabilities, and the signiﬁcantly higher number of communication rounds suggests
Dataset 
Local 
Update 
Dataset 
Local 
Gradients 
Client 1 
... 
Sparsify 
0.0594 
0.871 
0.0464 
… 
0.941 
0.959 
0.0471 
0.0874 
… 
0.0477 
0.0 
0.871 
0.0… 
0.941 
0.959 
0.0 
0.0874 
0.0 
0.891 
0.0… 
0.938 
Upload 
Sparsify 
0.0941 
0.0 
0.0891 
… 
0.0534 
0.0418 
0.0177 
… 
0.0334 
... 
LDP 
Client n
Dataset 
Perturb 
Perturb
Upload 
0.0
0.0 
… 
Perturbed 
Gradients 
Fig. 4.30 The update process of local differentially private federated learning in Shokri et al.’s 
scheme

that the scheme is not feasible in practice. Truex et al. [37] applied condensed LDP to 
FL, but this scheme only provides weak privacy guarantees. Large clipping bounds 
also consume a substantial privacy budget, and the noise with high variance 
increases the number of training rounds for the FL model.
176
F. Li et al.
Recently, Liu et al. [38] proposed a scheme to solve the dimension dependence 
problem by selecting only the most important k dimensions of gradient updates for 
noise addition. They also used gradient accumulation techniques to stabilize the 
learning process with noisy updates. However, dimension selection requires extra 
privacy budget for gradient privacy, and model convergence becomes slower due to 
the reduction of update dimension. Sun et al. [39] introduced DP noise for different 
ranges of parameters in different layers of deep neural networks, resulting in smaller 
estimation variances. Additionally, parameter shufﬂing aggregation bypasses the 
curse of dimensionality. While this scheme shows better performance in terms of 
effectiveness and efﬁciency, the evaluation error of this mechanism remains high, 
and communication overhead is not reduced. 
4.4.2.3 
Existing Limitations and Improvements 
In summary, existing LDP-FL approaches exhibit two primary limitations: 
Reduced accuracy of the FL model with LDP deployed. Current LDP schemes 
compromise the accuracy of FL models due to interference with gradient aggrega-
tion. This is caused by the need to protect gradient privacy through the introduction 
of excessive noise before uploading, which leads to a deviation in the server’s 
aggregation direction from the true gradient update trajectory. Furthermore, widely 
used element-wise perturbation introduces additional noise, resulting in severe 
gradient perturbations. 
Estimation errors in LDP affecting FL convergence. LDP introduces large esti-
mation errors during FL aggregation due to a limited number of gradient updates 
(usually equal to the number of clients), which hinders model convergence or 
extends iteration requirements. This ﬁnally increases the training cost of the FL 
model. 
Future LDP-based defenses in FL can be improved from three perspectives: 
Reduce the estimation error of LDP. The most important aspect is to reduce the 
estimation error under an equivalent LDP privacy budget (i.e., the same level of 
privacy protection) to better tighten the tradeoff between privacy and utility while 
avoiding additional privacy budget consumption. Increasing the number of LDP 
mechanism inputs can also effectively improve the accuracy of the LDP-FL model. 
Non-interference deployment. Defense mechanisms should be seamlessly inte-
grated without affecting the original FL model, such as convergence. 
Improve communication efﬁciency. Encoding transmitted data can further save 
communication overhead and provide unique advantages to DP-based defenses, 
which is not available in other schemes.

4
Privacy Computing Techniques
177
4.4.3 
The Application of Privacy Computing in Machine 
Unlearning 
4.4.3.1 
Background 
Many commercial companies and institutions usually collect users’ data as training 
datasets to improve their intelligent applications for better service quality. However, 
users’ data may contain lots of privacy like face portrait, home address, medical 
records, etc., which increases the risk of user privacy disclosure. Recently, this issue 
of over-collection of private information has raised the concern of regulators in many 
countries, and then related laws and regulations have been promulgated to stipulate 
that service providers must fulﬁll the obligation to remove data of users when they 
receive data owners’ requests, which is known as the right-to-be-forgotten. For 
example, European Union’s GDPR, the California Consumer Privacy Act in USA, 
and Personal Information Protection Law of China all impose legal obligations on 
service providers to take measures to erase personal data to protect data owners’ 
legal right. 
However, when users ask to delete the data they uploaded, the provider can 
directly remove these, but the data that has already participated in the training will 
potentially exist in the memory of trained models. Therefore, when deleting these 
data, what the model has learned from these data should also be removed. This 
spawns a new research direction of model privacy, model unlearning, to help service 
providers delete users’ data as well as the contributions that have been generated to 
the model. 
There are two main research routes for model unlearning, exact unlearning, and 
approximate unlearning. In exact unlearning, the naive and straightforward approach 
is to directly remove the data to be unlearned and retrain the model from scratch, also 
known as naive retraining. This solution often entails signiﬁcant computational and 
time overhead. Subsequent research follows this line and try to optimize model 
retraining to reduce the cost of retraining. In contrast, approximate unlearning is a 
modiﬁcation of the model parameter space so that to remove the contribution of 
certain data to the model parameter update, thus achieving a retraining-like effect. 
Although approximate unlearning schemes have signiﬁcant advantages in terms of 
unlearning cost, Thudi et al. [40] recently show that such approximate schemes have 
serious security ﬂaws. The effects of approximate unlearning cannot prove their 
reliability and could be faked to deceive users. Conversely, although the exact 
unlearning has an expensive overhead, it directly removes the data needed to unlearn 
from the dataset and is naturally reliable. Therefore, we consider the exact 
unlearning scheme and try to decrease the retraining cost.

178
F. Li et al.
4.4.3.2 
Method 
We propose an efﬁcient architecture for exact machine unlearning. We mainly focus 
on the classiﬁcation task, which is the most widely used in machine learning. Based 
on ensemble learning, we divide the dataset into several isolated sub-datasets and 
independently train sub-models on each sub-dataset to participate in decision mak-
ing. The ﬁnal judgment is output by decision aggregation. When a sample is 
requested to be deleted by the user, compared with naive retraining that needs to 
retrain the model on the whole training dataset, we only need to retrain the affected 
sub-model, which speeds up the retraining. To improve accuracy of sub-models, we 
utilize the one-class classiﬁer which performs well with few training data. 
In addition, we pre-process each sub-dataset to further accelerate the model 
retraining, including representative data selection, model training state saving, and 
data sorting by erasure probability. Data selection could remove the redundancy and 
duplication and signiﬁcantly reduce the training volume of the model. Training state 
saving allows model to retrain from the last state where there is no sample to unlearn, 
avoiding repeated computations. Sorting considers the non-uniform distribution of 
unlearning requests and places the sample most likely to be removed at the last saved 
training state to reuse the previous results as much as possible (Fig. 4.31). 
Partition by Class 
For a training set S with c labels in output domain Y , c also denotes the number of 
classes in S. Here we choose to split the dataset by class instead of split uniformly. 
The reason is that the speciﬁc sub-model trained on the entire one-class dataset 
performs better than the weaker learner trained on a uniform dataset of the same 
small size. S is partitioned into c sub-datasets. Each sub-dataset Si only contains all 
data of one class, i.e., \Si = ∅ and [Si = S, where i 2 {0, 1, . . ., c - 1}. The 
sub-model fi is trained using the sub-dataset Si. Beneﬁting from parallel computing, 
splitting datasets disjointly and training sub-models on each sub-dataset can distrib-
ute training costs and improve training efﬁciency. 
… 
… 
… 
… 
… 
C1 
… 
Ci 
… 
Cn 
… 
Sub-dataset Si 
Representative 
data selection 
… 
Training 
state saving 
… 
Block1 
Block2 
… 
Blockm 
State1 
State2 
… 
Sort 
(optional) 
… 
Partition by class 
Sub-model fi 
One-class classiﬁer
Train 
Sub-model fn 
One-class classiﬁer 
Sub-model f1 
One-class classiﬁer 
Decision 
Aggregation 
Output 
Data with less information 
Data with high information 
Data with Low probability to unlearn 
Data with High probability to unlearn 
Statem 
Block1 
Block2 
… 
Blockm 
Sub-datasetSi’ 
Training dataset S 
… 
Fig. 4.31 The dataset pre-processing method of proposed exact unlearning

Þ
4
Privacy Computing Techniques
179
Representative Data Selection 
Training model on an adequate training dataset would achieve better performance, 
but the redundant information in the dataset would also increase, which severely 
slows down the training of the model. Thus, in this part, we use information theory to 
select the most informative subset from the original training set to remove redundant 
data, which speeds up retraining without reducing the prediction accuracy. Suppose 
n samples are selected from original sub-dataset Si to form new sub-dataset S0 
i, i.e., 
S0 
i ⊂ Si and n = size S0 
i . This task is let the subset S0 
i contain as much information as 
possible. 
We use the joint entropy to measure the information amount contained in the 
subset S0 
i. For samples X1, . . . , Xn 2 S0 
i, there is 
H S0 
i = H X1, . . . , Xn
ð
Þ = -
x12X1 
. . .  
xn2Xn 
P x1, . . . , xn
ð
Þ log P x1, . . . , xn
ð
, 
where x1, . . ., xn are the different values in samples X1, . . ., Xn respectively, P(x1, . . ., 
xn) is the probability that these values appear together at the same time. We set 
P(x1, . . ., xn) log P(x1, . . ., xn) = 0 if  P(x1, . . ., xn) = 0. 
Training State Saving 
In each sub-model training and retraining procedure, we further divide the 
sub-dataset Si 
into m disjoint data blocks Bij uniformly, i.e., \Bij = ∅ and 
[Bij = Si, where j 2 {0, 1, . . ., m - 1}. The blocks maintain a constant sequential 
relationship with each other during training. In the training of sub-model fi, the data 
block Bij is sequentially fed into the model for iterative update and the corresponding 
parameters θij are saved after training. The ﬁnal result θi(m - 1) is the sub-model’s 
parameter θi. 
Sorting by Erasure Probability 
We take the distribution of the unlearning requests into consideration. Note that this 
step is not necessary but can further speed up the unlearning when the probability 
distribution of the data being requested to be unlearned is known for model owners. 
In practice, the data collector can determine which data in the dataset are more likely 
to be erased by users’ requests based on some auxiliary information. Then the data 
with the highest probability are placed into the last blocks as much as possible so that 
when unlearning is needed, the model only needs to retrain on the last blocks, which 
reuses the previous results to the greatest extent and saves time.

180
F. Li et al.
4.4.3.3 
Model Architecture 
In this part, we ﬁrstly introduce the one-class classiﬁer used and then show how to 
aggregate the multiple decisions from each classiﬁer to reach the ﬁnal decision 
(Fig. 4.32). 
One-Class Classiﬁer 
The task of a one-class classiﬁer fi(x; θ) in this work is to learn feature representations 
of its one-class data distribution and detect the data of other classes as out of 
distribution. x is input. Here we introduce one representative deep one-class classi-
ﬁer [41]. The training goal of fi(x; θ) is to learn a set of parameters θ that make 
the sample in class i as close to the class representation ri as possible, which force the 
network to learn to extract the common features of the samples in class i, i.e., the 
feature representations of class i. The objective function of the one-class classiﬁer is 
deﬁned as 
arg min 
θ 
1 
n 
n 
i = 1 
f i xi; θ
ð
Þ - r
k
ki 
2 þ λ 
2 
L 
l = 1 
W l½ ]
2 , 
where xi 2 S0 
i, n = size S0 
i . The second term is a weight decay regularizer, where λ is 
a hyperparameter greater than 0 and W[l] denotes the l-th layer weight. 
Using adaptive optimizer Adam [42] for weight θ is computationally efﬁcient and 
has little memory requirements. We empirically set ri as the average of the repre-
sentations of samples in one batch. Meanwhile, r is in the neighborhood of the initial 
network outputs, which also makes the model converge faster and more robust. After 
training, when a test sample xi inputs to the classier fi(x; θ), the sample’s abnormal
… 
… 
… 
Train 
Retrain 
Train 
Decision 
Aggregation 
… 
… 
Output
…… 
… 
Remove data 
Fig. 4.32 The model architecture of proposed exact unlearning

score can be given based on the distance between its feature representation and the 
class representation ri, i.e.,
4
Privacy Computing Techniques
181
AbnormalScore = f i xi; θ
ð
Þ - ri
k
k2 
Decision Aggregation 
The strategy adopted in this work is that when only one classiﬁer judges that the 
sample belongs to this class, the ﬁnal result output is as this class. If there are 
multiple judgments for the respective class, the class with the lowest abnormal score, 
i.e., the highest conﬁdence, is selected to be output as the result. Although the 
approach seems to jeopardize the performance of the model since each sub-model 
is trained on an extremely unbalanced dataset, only have one class, we evaluate our 
multiple one-class models in experiments to show that the loss of performance is 
very limited and can be mitigated when using an appropriate aggregation strategy. 
4.4.3.4 
Performance 
We analyze the acceleration achieved by the service provider in handling unlearning 
requests after users send requests to delete their data. Compared with the naive 
method of retraining from scratch, as well as the State-of-the-art (SOTA) method 
[43], our proposed exact unlearning scheme guarantees the accuracy of the retrained 
model and signiﬁcantly reduces the number of samples needed for retraining, thus 
shortening the retraining time. 
Negligible reduction in the accuracy of the retrained model. When there is no 
unlearning request, the accuracy of the original model on MNIST, CIFAR-10, and 
ImageNet is 96.13%, 90.10%, and 89.70% respectively. In comparison, the accuracy 
of models using our architecture is 95.28%, 89.3%, and 85.49%, respectively. This 
demonstrates that our architecture does not cause a signiﬁcant drop in the perfor-
mance of the original model. 
Extremely fast unlearning. Our architecture offers a signiﬁcant advantage in 
retraining time, especially when the number of unlearning requests is low. Compared 
with naive retraining, our scheme can process unlearning requests in only 5% of the 
time. This notable improvement is due to our dataset pre-processing method and our 
model architecture, which requires training only a few sub-models instead of the 
entire large model.

182
F. Li et al.
4.4.4 
The Application of Privacy Computing in Logistics 
As a typical application scenario of cross-system collaborative services in a ubiqui-
tous internet environment, logistics covers ﬁve typical phases, including sending, 
sorting, transportation, distribution, and signing. These phases involve different 
personnel or entities and corresponding privacy information, and require privacy 
preservation throughout the life cycle, as shown in Fig. 4.33. The personnel or 
entities involved in logistics include consignors, contractors, sorters, logistics busi-
ness departments, distribution centers, transporters, distributors, consignees, etc. The 
corresponding privacy information includes identity information, contact informa-
tion, home address, goods information, transaction information, and so on. 
In the following, starting from the typical steps of privacy information extraction, 
scene description, privacy control, privacy operation, privacy effectiveness evalua-
tion, etc., the subject, privacy information involved in different steps of privacy 
computing and various stages of logistics, as well as the preserving methods that can 
be used, are mapped. 
Privacy information extraction mainly involves the sending stage, involving the 
identity information and contact information of the consignor and consignee, as well 
as the name, type, and weight of the goods. Natural language processing, privacy 
information discovery, and other technologies should be used to extract the 
corresponding privacy information, laying foundation for subsequent operations. 
Scene description mainly involves sending, sorting, transportation, and distribu-
tion. It is necessary to abstract the demand for privacy preservation according to the 
subject and privacy information involved in different scenes, integrating temporal 
and spatial information, subjective and objective intentions, etc., so as to better guide 
desensitization. 
Privacy control mainly involves sorting, transportation, and distribution. With the 
results of the scene description, the operations that can be performed by different 
privacy information in each stage in speciﬁc scenarios are restricted, and the privacy 
control information is generated and bound with the privacy information. 
Privacy operation involves sending, sorting, transportation, distribution, signing, 
and other stages. In the process of privacy information ﬂow, the corresponding 
privacy operation is selected or designed according to the privacy information, 
privacy-preserving needs, privacy control information, etc., combined with the 
bound privacy control information, so as to the privacy information can be ﬁne-
grained controlled.
Distribution 
Center
Distribution 
Center
Distributor 
Consignee
Contractor
Barcode 
Scanner
Barcode 
Scanner
Military 
supplies
Grain
Oil
General
goods
●
Consignor's ID
●
Consignor's 
address
●
Consignor's name
●
Consignor's
phone
...
●
Consignee's 
address
●
Consignee's name
●
Consignee's phone
●
Goods information
...
●
Goods information
●
Consignor's address
●
Consignee's address
●
Consignee's phone
...
●
Goods information
●
Consignor's address
●
Consignee'ss address
●
Consignee's phone
...
●
Goods information
●
Transportation 
requirement
●
Consignor's address
●
Consignee's address
...
●
Goods information
●
Transportation 
requirement
●
Consignor's address
●
Consignee's address
...
Logistics 
Business
Department
Logistics 
Business
Department
Fig. 4.33 Privacy information in logistics

4
Privacy Computing Techniques
183
Privacy effectiveness evaluation also involves sending, sorting, transportation, 
distribution, signing, and other stages. After ﬁne-grained control of privacy infor-
mation is carried out in the privacy operation process, privacy effectiveness evalu-
ation should be carried out. According to the evaluation results, it should be timely 
fed back to the steps of privacy information extraction, scene description, privacy 
control and so on, so as to achieve iterative control. 
4.4.5 
The Application of Privacy Computing in Data Trading 
The main stages of data trading include data ownership conﬁrmation, pricing, 
transaction, and settlement audit. The main stages of privacy computing architecture 
include privacy information extraction, scenario description, operations such as 
desensitization and destruction, preservation scheme selection, preservation effec-
tiveness evaluation, infringement evidence collection, and tracing, etc. The corre-
spondence between the stages of data trading and privacy computing is as follows: 
data ownership conﬁrmation corresponds to privacy information extraction; data 
pricing corresponds to scenario description, preservation scheme selection, and 
preservation effectiveness evaluation; data transaction corresponds to scenario 
description, desensitization and destruction, preservation scheme selection, infringe-
ment evidence collection, and tracing, etc.; settlement audit corresponds to scenario 
description, infringement evidence collection, and tracing, etc. 
In the stage of data ownership conﬁrmation, there are problems such as unclear 
and unambiguous expression of constraints on privacy preservation content and its 
preservation requirements. These problems can be solved by using technologies 
such as privacy intelligent perception and privacy dynamic measurement in the 
privacy information extraction stage of privacy computing. 
In the stage of data pricing, there are issues related to privacy level, data utility, 
constraint control, and quantitative evaluation, such as application scenarios, effec-
tive information of data, data usage operations, and the preservation ability of data 
recipients, etc. These problems can be solved by using technologies such as privacy 
computing scenario perception, privacy preservation algorithm framework, auto-
mated effectiveness evaluation, and privacy preservation effectiveness evaluation 
based on large data privacy mining, as well as preservation capability quantiﬁcation 
indicators and preservation effectiveness evaluation indicators of privacy preserva-
tion algorithms. 
In the stage of data transaction, there are issues related to extended control, 
transaction audit, and other aspects, such as application scenarios, data usage 
operations, property transfer, the preservation ability of data recipients, transaction 
certiﬁcation, infringement evidence collection, and tracing, etc. These problems can 
be solved by using technologies such as privacy computing scenario perception, 
privacy preservation algorithm framework, iterative and on-demand desensitization, 
transaction ﬂow audit, infringement behavior determination, infringement evidence 
collection and tracing, and extended control mechanisms.

184
F. Li et al.
In the stage of settlement audit, there are issues related to infringement evidence 
collection, transaction evidence collection, infringement tracing, and other aspects, 
such as application scenarios, infringement evidence collection, and tracing, etc. 
These problems can be solved by using technologies such as privacy computing 
scenario perception, transaction ﬂow audit, infringement behavior determination, 
construction of infringement time-space scenarios, and behavior reconstruction. 
In summary, a single transaction of data is not a full life cycle. It is multiple 
transactions of data or interconnected transactions between multiple data market-
places that constitute the full life cycle of data. Therefore, the theory and methods of 
full life cycle preservation in privacy computing will play an irreplaceable and 
important role in data trading. 
4.5 
Chapter Summary 
This chapter introduces the theoretical basis of privacy-preserving algorithms, and 
discusses how to systematically study privacy-preserving algorithms from the per-
spective of privacy computing, how to abstract model the common parts of different 
types of algorithms, and details the typical privacy preservation algorithms based on 
anonymity, differential privacy, and privacy-utility function. It also focuses on 
analyzing the issues such as privacy information iterative extended control, and 
extended control for cross-system exchange. In addition, the applications of privacy 
computing in multi-party data secure computing, federated learning, machine 
unlearning, logistics, and data trading are also introduced. In order to effectively 
ensure the relative stability of the software architecture of the privacy information 
preservation system and the dynamic loading of the privacy-preserving algorithms, 
and promote the better and wider applications of the privacy-preserving algorithms, 
this chapter focuses on the analysis of the privacy information transmission control, 
the extended control of cross-system exchange, and other issues. 
References 
1. Sweeney, L.: K-anonymity: a model for protecting privacy. Int J Uncertain Fuzz. 10(5), 
557–570 (2002) 
2. Machanavajjhala, A., Kifer, D., Gehrke, J., et al.: L-diversity: privacy beyond k-anonymity. 
ACM Trans. Knowl. Discov. Data. 1(1), 3 (2007) 
3. Li, N.H., Li, T.C., Venkatasubramanian, S.: T-Closeness: Privacy beyond K-Anonymity and 
L-Diversity. In: 2007 IEEE 23rd International Conference on Data Engineering, pp. 106–115. 
IEEE Press, Piscataway (2007) 
4. Dwork, C.: Differential Privacy. In: International Colloquium on Automata, Languages, and 
Programming, pp. 1–12. Springer, Berlin (2006) 
5. Li, X.G., Li, H., Zhu, H., et al.: The optimal upper bound of the number of queries for Laplace 
mechanism under differential privacy. Inf. Sci. 503, 219–237 (2019)

4
Privacy Computing Techniques
185
6. Audet, C., Kokkolaras, M.: Blackbox and derivative-free optimization: theory, algorithms and 
applications. Optim. Eng. 17(1), 1–2 (2016) 
7. Shannon, C.E.: A mathematical theory of communication[J]. Bell Syst. Tech. J. 27(4), 379–423 
(1948) 
8. Shannon, C.E.: Coding theorems for a discrete source with a ﬁdelity criterion. IRE Int Conv 
Rec. 7(3), 142–163 (1959) 
9. Blahut, R.: Computation of channel capacity and rate-distortion functions. IEEE Trans. Inf. 
Theory. 18(4), 460–473 (1972) 
10. Csisz, I., Tusna Dy, G.: Information geometry and alternating minimization procedures. Stat 
Decis. 1, 205–237 (1984) 
11. Niu, B., Li, Q.H., Zhu, X.Y., et al.: Enhancing Privacy through Caching in Location-Based 
Services. In: 2015 IEEE Conference on Computer Communications, pp. 1017–1025. IEEE 
Press, Piscataway (2015) 
12. Andrés, M.E., Bordenabe, N.E., Chatzikokolakis, K., et al.: Geo-Indistinguishability: Differ-
ential Privacy for Location-Based Systems. In: ACM Conference on Computer and Communi-
cations Security, pp. 901–914. ACM Press, New York (2013) 
13. Niu, B., Chen, Y.H., Wang, Z.B., et al.: Eclipse: preserving differential location privacy against 
long-term observation attacks. IEEE Trans Mobile Comput. 21(1), 125–138 (2020) 
14. Shokri, R., Theodorakopoulos, G., Troncoso, C., et al.: Protecting Location Privacy: Optimal 
Strategy against Localization Attacks. In: ACM Conference on Computer and Communications 
Security, pp. 617–627. ACM Press, New York (2012) 
15. Alaggan, M., Gambs, S., Kermarrec, A.M.: Heterogeneous differential privacy[J]. arXiv Pre-
print, arXiv:1504.06998 (2015) 
16. Jorgensen, Z., Yu, T., Cormode, G.: Conservative or Liberal? Personalized Differential 
Privacy. In: 31st IEEE International Conference on Data Engineering, pp. 1023–1034. IEEE 
Press, Piscataway (2015) 
17. Niu, B., Chen, Y.H., Wang, B.Y., et al.: AdaPDP: Adaptive Personalized Differential 
Privacy. In: IEEE International Conference on Computer Communications. IEEE Press, 
Piscataway (2021) 
18. Niu, B., Chen, Y., Wang, B.Y., et al.: Utility-Aware Exponential Mechanism for Personalized 
Differential Privacy. In: IEEE Wireless Communications and Networking Conference, pp. 1–6. 
IEEE Press, Piscataway (2020) 
19. Niu, B., Zhang, L.K., Chen, Y.H., et al.: A Framework to Preserve User Privacy for Machine 
Learning as a Service. In: IEEE Global Communications Conference, pp. 1–6. IEEE Press, 
Piscataway (2020) 
20. Zhang, W.J., Liu, Q., Zhu, H.: Evaluation and protection of multi-level location privacy based 
on an information theoretic approach. J. Commun. 40(12), 51–59 (2019) 
21. Zhang, W.J., Li, M., Tandon, R., et al.: Online location trace privacy: an information theoretic 
approach. IEEE Trans. Inf. Forensics Secur. 14(1), 235–250 (2018) 
22. Zhang, W.J., Jiang, B., Li, M., et al.: Aggregation-based location privacy: an information 
theoretic approach. Comput. Secur. 97(4), 101953 (2020) 
23. Li, F.H., Sun, Z., Niu, B., et al.: SRIM scheme: an impression-management scheme for privacy-
aware photo-sharing users. Engineering. 4(1), 85–93 (2018) 
24. Li, F.H., Sun, Z., Li, A., et al.: HideMe: Privacy-Preserving Photo Sharing on Social 
Networks. In: IEEE International Conference on Computer Communications, p. 154.162. 
IEEE Press, Piscataway (2019) 
25. Li, F.H., Sun, Z., Niu, B., et al.: Privacy-preserving photo sharing framework cross different 
social network. J. Commun. 40(7), 1–13 (2019) 
26. Yuan, L., Korshunov, P., Ebrahimi, T.: Secure JPEG Scrambling Enabling Privacy in Photo 
Sharing. In: Proceeding of International Conference Automatic Face and Gesture Recognition, 
pp. 1–6. IEEE Press, Piscataway (2015) 
27. Li, T., Sahu, A.K., Talwalker, A., et al.: Federated learning: challenges, methods, and future 
directions. IEEE Signal Process. Mag. 37(3), 50–60 (2020)

186
F. Li et al.
28. Zhu, L., Liu, Z., Han, S.: Deep leakage from gradients. Adv. Neural Inf. Proces. Syst. 32, 8263 
(2019) 
29. Melis, L., Song, C., de Cristofaro, E., et al.: Exploiting unintended feature leakage in collab-
orative learning. In: 2019 IEEE symposium on security and privacy (SP), pp. 691–706. IEEE 
(2019) 
30. Kairouz, P., Mcmahan, H.B., Avent, B., et al.: Advances and open problems in federated 
learning. Found Trends Mach. Learn. 14(1–2), 1–210 (2021) 
31. Nasr, M., Shokri, R., Houmansadr, A.: Comprehensive privacy analysis of deep learning. In: 
Proceedings of the 2019 IEEE Symposium on Security and Privacy (SP), vol. 2018, pp. 1–15 
32. Fredrikson, M., Jha, S., Ristenpart, T.: Model inversion attacks that exploit conﬁdence infor-
mation and basic countermeasures. In: Proceedings of the 22nd ACM SIGSAC conference on 
computer and communications security, pp. 1322–1333 (2015) 
33. Wang, Z., Song, M., Zhang, Z., et al.: Beyond inferring class representatives: user-level privacy 
leakage from federated learning. In: IEEE INFOCOM 2019-IEEE conference on computer 
communications, pp. 2512–2520. IEEE (2019) 
34. Kasiviswanathan, S.P., Lee, H.K., Nissim, K., et al.: What can we learn privately? SIAM 
J. Comput. 40(3), 793–826 (2011) 
35. Shokri, R., Shmatikov, V.: Privacy-preserving deep learning. In: Proceedings of the 22nd ACM 
SIGSAC conference on computer and communications security, pp. 1310–1321 (2015) 
36. Bhowmick, A., Duchi, J., Freudiger, J., et al.: Protection against reconstruction and its appli-
cations in private federated learning. arXiv preprint arXiv:1812.00984 (2018) 
37. Truex, S., Liu, L., Chow, K.H., et al.: LDP-fed: federated learning with local differential 
privacy. In: Proceedings of the third ACM international workshop on edge systems, Analytics 
and Networking, pp. 61–66 (2020) 
38. Liu, R., Cao, Y., Yoshikawa, M., et al.: Fedsel: federated sgd under local differential privacy 
with top-k dimension selection. In: Database systems for advanced applications: 25th Interna-
tional Conference, DASFAA 2020, Jeju, South Korea, September 24–27, 2020, Proceedings, 
Part I 25, pp. 485–501. Springer International Publishing (2020) 
39. Sun, L., Qian, J., Chen, X.: Ldp-ﬂ: practical private aggregation in federated learning with local 
differential privacy, pp. 1571–1578. IJCAI (2021) 
40. Thudi, A., Jia, H., Shumailov, I., et al.: On the necessity of auditable algorithmic deﬁnitions for 
machine unlearning. In: 31st USENIX Security Symposium (USENIX Security 22), 
pp. 4007–4022 (2022) 
41. Ruff, L., Vandermeulen, R., Goernitz, N., et al.: Deep one-class classiﬁcation. In: International 
conference on machine learning, pp. 4393–4402. PMLR (2018) 
42. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: International Conference 
on Learning Representations (2015) 
43. Bourtoule, L., Chandrasekaran, V., Choquette-Choo, C.A., et al.: Machine unlearning. In: 2021 
IEEE symposium on security and privacy (SP), pp. 141–159. IEEE (2021)

Chapter 5 
The Future Development Trends of Privacy 
Computing 
Fenghua Li 
, Hui Li 
, and Ben Niu 
We propose to abstract, condense, and foresee the future development trends of 
privacy computing from three aspects: the research scope of privacy computing, the 
evolution of privacy computing theory and technology, as well as the iterative 
evolution of ubiquitous interconnection technologies and new business models. 
This chapter introduces the future development trends of privacy computing from 
ﬁve aspects: the basic theory of privacy computing, privacy perception and dynamic 
measurement, privacy preservation algorithms, privacy preservation effect evalua-
tion, and privacy infringement judgment and traceability. 
5.1 
Fundamentals of Privacy Computing 
5.1.1 
Privacy Computing Model and Its Security 
Assurance Model 
Aiming at the characteristics and problems in ubiquitous interconnected environ-
ment including diverse privacy dissemination methods, scenario differences, 
dynamic changes in privacy subjective perceptions, and difﬁculty in quantitative 
privacy analysis, we should (1) focus on the stages of privacy perception and 
dynamic measurement, privacy preservation algorithms, privacy preservation effect 
evaluation, and extended control of privacy information to further study and improve
F. Li (✉) · B. Niu 
Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China 
e-mail: lifenghua@iie.ac.cn; niuben@iie.ac.cn 
H. Li 
School of Cyber Engineering, Xidian University, Xian, Shaanxi, China 
e-mail: lihui@mail.xidian.edu.cn 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
F. Li et al., Privacy Computing, https://doi.org/10.1007/978-981-99-4943-4_5
187

the privacy computing framework, (2) reﬁne the association mechanism, operation 
control, and information transmission control among each stage, and (3) propose a 
ﬂow control model and desensitization and privacy control primitives for the whole 
process of privacy information. Meanwhile, we should study the conﬁdentiality, 
integrity, and non-repudiation guarantee mechanism of information transmission 
control between each stage of privacy computing and propose a security guarantee 
model for the privacy computing framework.
188
F. Li et al.
5.1.2 
Mathematical Foundations of Privacy Computing 
In view of the urgent need for basic theory in privacy computing and its various 
stages, we can draw on the ideas of probability theory and mathematical statistics, 
information theory, game theory, topological psychology, and other disciplines to 
study (1) the distribution characteristics of data sets, joint measurement of multi-
dimensional privacy attribute components, privacy measurements embedded with 
user subjective factors, measurement of privacy information leakage degree, 
privacy-preserving algorithm design, and quantitative evaluation of desensitization 
effect and (2) the mathematical foundations of privacy desensitization effect, data 
utility, privacy preservation cost, privacy leakage loss, revenue game strategy, etc. to 
keep exploring the basic theory of privacy computing. 
5.1.3 
Technical Architecture of Privacy Information 
Protection System 
Aiming at the characteristics and application needs of privacy information protection 
systems such as the massive amount of users, individual differences in demand, 
multi-modal and huge scale of data, high concurrent services, and random privacy 
information exchange, we should (1) study the technical architecture of the privacy 
information protection system with deep integration with business services and 
privacy computing, computing architecture of billion-level online high concurrent 
services, collaborative supervision architecture of privacy information utilization, 
reconﬁguration of privacy computing resources, customization of privacy desensi-
tization services, service model and state management of multi-layer services, etc., 
(2) propose privacy information protection solutions for typical application scenar-
ios, (3) form different privacy-preserving service capabilities, and (4) promote 
privacy computing applications.

5
The Future Development Trends of Privacy Computing
189
5.2 
Privacy Perception and Dynamic Metrics 
5.2.1 
Component Extraction and Perception of Privacy 
Information 
Aiming at the characteristics of massiveness, multimodality, fragmentation of pri-
vacy information, and subjective privacy perception in the ubiquitous interconnected 
environment, we should study the knowledge representation model of privacy 
information, classiﬁcation of privacy information, scene-sensitivity association con-
struction, privacy component extraction based on natural language processing/image 
semantic understanding, privacy information feature analysis of multimodal data, 
privacy information compressed perception for approaching optimal data utility, 
privacy component sensitivity perception, privacy component association relation-
ship mining, privacy information perception of derived data, etc., to support efﬁcient 
and accurate privacy perception of multimodal information. 
5.2.2 
Scenario-Adapted Privacy Dynamic Metrics 
To address the characteristics and application needs of scenario differences, 
multimodality, subjective perception of privacy, dynamic exchange and reorganiza-
tion of privacy information, and personalized customization of privacy-preserving 
preferences in the ubiquitous interconnected environment, we should study the 
privacy components and scenario association model, quantiﬁcation of privacy com-
ponents associated with subjective and objective factors, value range and probability 
distribution of privacy attribute components, dynamic measurements of privacy 
components for scenario migration, intelligent perception and learning of the user 
privacy preferences, etc., which solve the problem of accurate measurement of 
privacy dynamic exchanges under spatio-temporal differences and subject dynamics 
and support intelligent privacy preservation. 
5.2.3 
Quantitative Metrics for Privacy Measurement 
In view of the characteristics and application needs of scenario differences, multi-
modality, massive privacy information, dynamic exchange and reorganization of 
privacy information, and anti-AI analysis in the ubiquitous interconnected environ-
ment, we should (1) study the quantitative indicators of privacy-preserving require-
ments for privacy preservation scenarios, quantitative indicators of privacy 
information in different modalities, quantitative indicators of dynamic adjustment 
of privacy during exchange, quantitative indicators of privacy combination con-
straints, as well as the correlation and dynamic weights of these quantitative

indicators, (2) form a quantitative indicator system of privacy measurement, 
(3) build a knowledge graph of privacy measurements, and (4) support privacy 
information exchange control and on-demand desensitization in the ubiquitous 
interconnected environment. 
190
F. Li et al.
5.3 
Privacy-Preserving Algorithm 
5.3.1 
Privacy Desensitization Primitives 
In view of the characteristics and application needs such as scenario differences, 
multi-modal information, and individual differences in privacy-preserving require-
ments in the ubiquitous interconnected environment, under the framework of privacy 
computing, we should study privacy desensitization primitives based on different 
mathematical foundations for different stages, including privacy desensitization 
primitives based on probability theory/information theory/game theory, localized 
differential privacy primitives for data collection, data desensitization primitives for 
dynamic data release, and privacy desensitization primitives for graph data, etc. The 
equivalence or mapping relationship of desensitization primitives are also studied to 
support the capability evaluation of privacy-preserving algorithms and cross-system 
exchange control of privacy information in the ubiquitous interconnected 
environment. 
5.3.2 
Privacy-Preserving Algorithm Framework 
In view of the complex and diverse privacy desensitization needs in the ubiquitous 
interconnected environment, as well as the application needs of easy development, 
easy expansion, easy operation, and maintenance of privacy information protection 
systems, we should study the general framework and design guidelines of different 
types of protection algorithms, differentiated desensitization control of the whole 
process, desensitization requirements understanding and combination constraints of 
primitives, detection methods of different desensitization primitives combination 
conﬂicts, scenario-adapted privacy-preserving algorithm selection and optimal 
design, dynamic scheduling of front and back-end tasks, etc., which support the 
ﬂexible reconﬁguration of the privacy information protection systems and the 
dynamic orchestration of privacy desensitization functions, and solve the problem 
of on-demand privacy preservation in complex scenarios.

5
The Future Development Trends of Privacy Computing
191
5.3.3 
Quantitative Indicators of the Protection Capability 
for Privacy-Preserving Algorithms 
Aiming at the characteristics of diverse algorithm and differences in algorithm 
protection capabilities in the ubiquitous interconnected environment, we should 
study the evaluation of protection capability of single algorithm and multi-algorithm 
combinations, the equivalence relationship between quantitative indicators of dif-
ferent types of algorithm protection capability, the mapping of algorithm protection 
capability when privacy information is exchanged across systems, the correlation 
relationship between quantitative systems of privacy quantiﬁcation/privacy-
preserving effect quantiﬁcation/algorithm protection capability quantiﬁcation, etc. 
These studies will form the quantitative index system of algorithm protection 
capability, build a knowledge graph of algorithm protection capability, and support 
the design and capability evaluation of privacy-preserving algorithms. 
5.4 
Evaluation of Privacy Preservation Effectiveness 
5.4.1 
Effectiveness Evaluation Metrics 
Aiming at the application needs of sample desensitization effect evaluation, evalu-
ation automation, and effect evaluation of anti-big data mining, we should study the 
effect evaluation quantitative indexes of different modal data, quantitative indexes of 
a single privacy-preserving algorithm, quantitative indexes of multi-privacy-preserv-
ing algorithm combinations, quantitative indexes of spurious data and incremental 
data from the dimensions of reversibility, extended control, complexity, deviation, 
and information loss, etc. We also study the correlation and dynamic weight of these 
quantitative indicators, to form the effect evaluation index system, and construct a 
knowledge graph for effect evaluation, and support effect feedback of privacy 
preservation, and iterative optimization of privacy-preserving schemes. 
5.4.2 
Performance Evaluation Automation 
Aiming at the characteristics and application needs of massive privacy information, 
diverse algorithms and their combinations, real-time release and exchange, and fast 
evaluation in the ubiquitous interconnected environment, we should study the 
computational model of effect evaluation systems, ﬂexible architecture of automatic 
evaluation systems, automatic evaluation methods of different algorithms, descrip-
tion methods and automatic generation technology of sample data and evaluation 
samples, evaluation iterative mechanism and method based on effect feedback, 
automatic evaluation process modeling and automatic code generation, etc., which

support efﬁcient and fast effect evaluation, and optimal selection of privacy-
preserving algorithms. 
192
F. Li et al.
5.4.3 
Evaluation of Privacy Preservation Effectiveness Based 
on Privacy Mining 
Aiming at the characteristics of large-scale uncontrollable collection of desensitiza-
tion information, limitations of desensitization effects, and correlations of privacy 
information after desensitization in the ubiquitous interconnected environment, we 
should study the optimal collection method of large-scale desensitization informa-
tion, privacy association of multimodal data, privacy correlation of multi-source 
data, utility enhancement based on correlation, mutual information inference of 
linked data, privacy mining based on evolution of privacy-preserving algorithm, 
privacy mining based on machine learning, etc. These studies support capability 
evaluation of protection algorithms, desensitization effect evaluation of privacy 
information release, and evaluation of the privacy information protection system 
capability. 
5.5 
Determination and Traceability of Privacy 
Infringement 
5.5.1 
Infringement Determination Method 
Aiming at the characteristics and application needs of dynamic and frequent 
exchange of privacy information, random dissemination paths, and diverse forms 
of privacy infringement and accurate determination, we should study privacy 
infringement judgment rules and constraint representation, anti-tampering/anti-strip-
ping of extended control strategies, collaborative determination of multi-source 
association analysis, comprehensive judgment of multi-modal data fusion analysis, 
cross-layer collaborative whole-process privacy infringement evidence deposition, 
deposition of infringement scenarios and contents, identiﬁcation and determination 
of infringement events based on semantic analysis, etc., which support accurate 
determination 
of 
privacy 
infringement 
in 
the 
ubiquitous 
interconnected 
environment.

5
The Future Development Trends of Privacy Computing
193
5.5.2 
Audit Mechanism of Operation and Circulation 
Aiming at the characteristics and application needs of massive privacy information, 
multi-source and large-scale audit information, and dynamic cross-system ﬂow in 
the ubiquitous interconnected environment, we should study the collaborative 
supervision structure of privacy information ﬂow, trusted storage of audit informa-
tion, collaborative collection of multi-source audit information, optimal selection of 
large-scale multi-source audit information, content sampling and correlation analysis 
of multi-dimensional evidence, operational control constraints and audit information 
description, etc., which support traceability of privacy infringement and mainte-
nance of privacy information protection systems. 
5.5.3 
Extended Control Mechanism and Traceability 
of Privacy Infringement 
Aiming at the characteristics and application needs of invisible abuse of privacy 
information, differences in the protection capabilities of different systems, fragmen-
tation of privacy infringement evidence, and controlled exchange of privacy infor-
mation across systems, we should study the mechanism and description method of 
privacy information extended control, construction of authorization control chain, 
scenario-adapted dynamic adjustment of permissions, veriﬁable execution of control 
policies and trusted auditing, extended authorization mechanism of privacy infor-
mation across systems, privacy information deletion mechanism of extended use, 
traceability models of spatio-temporal trajectories, spatio-temporal scenario con-
struction and behavior reconstruction of privacy infringement, virtual identity anal-
ysis and traceability based on big data, etc., which support the controlled sharing of 
privacy information in the ubiquitous interconnected environment. 
5.6 
Chapter Summary 
A new research ﬁeld requires continuous and in-depth research works. Meanwhile, 
the differences in data security and privacy preservation research ﬁelds should be 
clearly distinguished instead of being enthusiastic about “old wine for a new bottle”. 
This chapter only lists some key research directions and research of privacy com-
puting, and the research scope can also be reasonably expanded. Privacy computing 
does not exclude the mathematical foundation of traditional data security, nor does it 
exclude the use of traditional methods of data security in a certain stage, such as

encryption and signature. But it needs to carry out targeted in-depth research 
centering the basic theory and each stage of privacy computing. Only in this way 
can we promote the continuous development and improvement of theory and 
technical system of privacy computing and better serve privacy preservation in the 
ubiquitous interconnected environment.
194
F. Li et al.

Postscript 
During the arduous journey of academic discovery, obtaining original and innova-
tive scientiﬁc research achievements has a certain degree of serendipity and neces-
sity. Serendipity means that when scholars choose the ﬁeld of study and conduct 
academic research work under the inﬂuence of subjective and objective circum-
stances, there is a certain chance that they can achieve success or not. Necessity 
means that scholars who are able to achieve original innovation must have an 
unwavering academic philosophy and attitude towards the pursuit of original inno-
vation and are able to persist in long-term exploration. The authors of this book 
proposed the concept of privacy computing and its research scope in order to 
condense theoretical research content and highlight original innovative research 
goals when writing the project application with their partners in 2015. Privacy 
computing is proposed in the context of forward-looking application requirements, 
reﬂecting the authors’ scientiﬁc attitude to pursuing original innovation. In the 
process of writing this book, the authors developed the idea of elaborating on the 
research history of privacy computing as a way to motivate themselves to continue 
their research more deeply and to encourage young scholars to conduct original and 
innovative theoretical research in various ﬁelds. 
The Introduction of Privacy Computing Concept 
and Deﬁnition 
From December third to sixth, 2015, when discussing privacy preservation-related 
technologies at Beijing Shounong Xiangshan International Conference Center, 
Fenghua Li, a researcher at the Institute of Information Engineering, Chinese 
Academy of Sciences, proposed to elevate privacy preservation-related research to 
a theoretical system, emphasizing that privacy protection is only an application 
demand, while it is privacy computing that represents a theoretical system. To
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
F. Li et al., Privacy Computing, https://doi.org/10.1007/978-981-99-4943-4
195

further clarify the connotation of privacy computing, during an impromptu discus-
sion at the back of the hotel lobby after 7:00 pm on December 5, 2015, Fenghua Li 
gave the deﬁnition of privacy computing: privacy computing is a computational 
theory and method oriented to the full life cycle protection of privacy information, a 
computable model and axiomatic system for privacy metrics, privacy leakage cost, 
privacy preservation, and privacy analysis complexity when the ownership, man-
agement right, and the right of using privacy information are separated. Speciﬁcally, 
it refers to the operation of describing, measuring, evaluating, and fusing the privacy 
information involved when processing information such as video, audio, images, 
graphics, text, numerical values, and ubiquitous network behavior information 
streams, forming a set of privacy computing theories, algorithms, and application 
technologies that are symbolic, formulaic, and have quantitative evaluation criteria 
to support the protection of privacy information in multi-system. Privacy computing 
covers all computing operations of information collectors, publishers, and users in 
the entire life cycle of information generation, perception, release, dissemination, 
storage, processing, use, and destruction. It also includes system design theory and 
architecture that supports massive users, high concurrency, and high-efﬁciency 
privacy preservation. Privacy computing will be an important theoretical basis for 
the protection of privacy information in ubiquitous cyberspace.
196
Postscript
The main participants in the discussion also included: Professor Hui Li from 
Xidian University, Professor Nenghai Yu from the University of Science and 
Technology of China, Professor Jian Weng from Jinan University, Associate 
Researcher Ben Niu from the Institute of Information Engineering, Chinese Acad-
emy of Sciences, Associate Professor Jin Cao from Xidian University, etc. 
Initiation and Organization of the First Symposium 
on Privacy Computing 
In order to promote the research on privacy computing, Fenghua Li and Hui Li 
actively prepared and held the ﬁrst symposium on privacy computing in Qishan 
Forest Hotel, Fuzhou City, Fujian Province from December 27–29, 2015. The 
conference is hosted by the Institute of Information Engineering, Chinese Academy 
of Sciences, Xidian University, National University of Defense Technology, Uni-
versity of Science and Technology of China, and Jinan University, organized by 
Fujian Normal University and Jinan University, and co-organized by Fujian Com-
puter Society. At the meeting, Fenghua Li made a report entitled “The Scope and 
Future of Privacy Computing”,  ofﬁcially announcing the birth of the deﬁnition and 
theoretical system of “Privacy Computing”. Figure A.1 is a photo of the ﬁrst 
Symposium on Privacy Computing.

Postscript
197
Fig. A.1 The First Symposium on Privacy Computing 
Ofﬁcial Publication of the First Academic Paper on Privacy 
Computing 
After the First Symposium on Privacy Computing, Fenghua Li and Hui Li conducted 
in-depth research on the theoretical system of privacy computing. Li Fenghua, Hui 
Li, Yan Jia, Nenghai Yu, and Jian Weng published an academic paper entitled 
“Research Scope and Development Trend of Privacy Computing” in the “Journal 
of Communications” in April 2016, which is the ﬁrst publication of the deﬁnition 
and research scope of privacy computing in the world. 
The Research Achievement of Privacy Computing Was 
Included in the Development Report of Chinese Association 
for Cryptologic Research 
In 2016, the research achievement of privacy computing was listed as one of the four 
annual results of the “Chinese Cryptography Development Report (2016–2017)” 
published by Chinese Association for Cryptologic Research.

198
Postscript
Continuing to Organize the Symposium on Privacy 
Computing 
From July 23 to 26, 2016, the Second Symposium on Privacy Computing was held at 
Huanghai Hotel, Qingdao City, Shandong Province. Fenghua Li made an academic 
report 
entitled 
“Privacy 
Computing: 
Privacy 
Preservation 
for 
Information 
Exchange”. Figure A.2 is a photo of the Second Symposium on Privacy Computing. 
The Symposium on Privacy Computing was changed to the International Sym-
posium from the third year. From November 30 to December 3, 2017, the Third 
International Symposium on Privacy Computing was held at the Novotel Melbourne 
St Kilda Hotel in Melbourne, Australia. Fenghua Li made an academic report 
entitled “Privacy Computing Theory for Full Lifecycle Information Protection” 
(Fig. A.3). 
From August sixth to ninth, 2018, the Fourth International Symposium on 
Privacy Computing was held at the Mercure Hotel Hagen in Boppard, a small 
town on the Rhine River in Germany. Fenghua Li made an academic report entitled 
“Privacy Computing: Concept, Computing Framework, and Future Development 
Trends” (Fig. A.4). 
From August 19th to 21st, 2019, the Fifth International Symposium on Privacy 
Computing was held at the University of Kent, UK. Fenghua Li made an academic 
report entitled “Privacy Computing: Theory, Computing Framework and Future 
Development Trends” (Fig. A.5). 
Affected by COVID-19, the Sixth International Symposium on Privacy Comput-
ing was changed to a domestic ofﬂine organization with the participation of foreign 
experts through video conferencing, which was held at Aloha Hotel, Lingshui, 
Hainan Province, China, from October 16 to 19, 2020. Fenghua Li gave an academic 
report entitled “Privacy Computing and its Research Scope” (Fig. A.6). 
Fig. A.2 The Second Symposium on Privacy Computing

Postscript
199
Fig. A.3 The Third International Symposium on Privacy Computing 
Fig. A.4 The Fourth International Symposium on Privacy Computing

200
Postscript
Fig. A.5 The Fifth International Symposium on Privacy Computing 
Fig. A.6 The Sixth International Symposium on Privacy Computing

Postscript
201
Fig. A.7 The Seventh International Symposium on Privacy Computing 
Affected by COVID-19, the Seventh International Symposium on Privacy Com-
puting was held online on December 19th, 2021. Fenghua Li made an academic 
report entitled “Research Progress and Development Trends of Privacy Computing” 
(Fig. A.7). 
Affected by COVID-19, the Eighth International Symposium on Privacy Com-
puting was held at Haikou and online from December 15th to 18th, 2022. Hui Li 
made an academic report entitled “Research Progress and Development Trend of 
Privacy Computing” (Fig. A.8). 
Continuing to Advance Academic Research on Privacy 
Computing 
Fenghua Li et al. published the academic paper “HideMe: Privacy- Preserving Photo 
Sharing on Social Networks” on IEEE INFOCOM 2019, which applied the extended 
control of privacy computing to controlled sharing of social photos. 
On March 19, 2019, the full version of “Privacy Computing: Concept, Comput-
ing Framework and Future Development Trends” in English completed by Fenghua 
Li, Hui Li, and Ben Niu was accepted by Engineering, the journal of the Chinese 
Academy of Engineering, and was published online on September 6, 2019. 
The research on privacy computing has been supported by the Ministry of 
Science and Technology of the People’s Republic of China. Xi’an University of 
Electronic Science and Technology, Institute of Information Engineering of Chinese 
Academy of Sciences, Shenzhen Graduate School of Harbin Institute of Technology, 
National University of Defense Technology of the People’s Liberation Army,

University of Science and Technology of China, and Jinan University have jointly 
applied National Key Research and Development Program project “Privacy Preser-
vation and Forensic Technologies under the Internet ( 2017.07–2020.12)” have been 
approved, and a subject entitled Basic Theory and Veriﬁcation of Privacy Comput-
ing under the Internet Environment was set speciﬁcally. 
202
Postscript
Fig. A.8 The Eighth International Symposium on Privacy Computing 
The research on privacy computing has been supported by the National Natural 
Science Foundation of China (NSFC), and Hui Li and Fenghua Li have successfully 
applied for the NSFC key project “Research on Privacy Computing Theory and 
Technology for Privacy Preservation and Secure Sharing of Big Data in Social 
Media (2020.01-2024.12)”. 
Continuously Promoting and Facilitating the Academic 
Research on Privacy Computing 
On July 26, 2018, Fenghua Li was invited to give a presentation titled “Privacy 
Computing: concepts, computing frameworks and future development trends” at the 
Inaugural Meeting of the Big Data Security and Privacy Preservation Professional 
Committee of the Chinese Society for Information and the Second Workshop on 
Academic Frontiers and Discipline Construction of Cyberspace Security in Xi’an, 
Shaanxi Province, China (Fig. A.9). 
On October 14, 2018, Fenghua Li gave an academic report entitled “Privacy 
Computing: concepts, computational frameworks, and its future development 
trends” at the 35th National Database Academic Conference (NDBC 2018) in 
Dalian, Liaoning Province, China.

Postscript
203
Fig. A.9 The inaugural meeting of the Big Data Security and Privacy Preservation Professional 
Committee 
On May 18, 2019, Fenghua Li, made an academic report entitled “Privacy 
Computing: theory, computational framework, and its future trends” at the 2019 
ACM China Turing Conference Symposium on Artiﬁcial Intelligence and Security 
in Chengdu, Sichuan Province, China. 
On July 13, 2019, Fenghua Li was invited to give an academic report entitled 
“Privacy Computing: theory, computational framework and its future development 
trend” at the Second Academic Conference on Big Data Security and Privacy 
Preservation held in Lanzhou, Gansu Province, China (Fig. A.10). 
On October 30, 2020, Fenghua Li was invited to give an academic report titled 
“Privacy Computing and its Research Scope” at the Third Academic Conference on 
Big Data Security and Privacy Computing held in Wuhan, Hubei Province. 
Figure A.11 shows a photo of the conference participants. 
On November 8, 2020, Hui Li made an academic report entitled “Privacy 
Computing and its Research Scope” at the Fourth Symposium on Academic Fron-
tiers and Discipline Construction of Cyberspace Security in Tianjin, China. 
Fenghua Li made an academic report entitled “Privacy Computing and Future 
Development Direction” at CCF ADL 111 Frontiers of Data Security and Privacy in 
Hangzhou, Zhejiang Province, China on Nov. 21, 2020. 
From the creation of a new theory to the recognition of the community, it often 
takes a long time to overcome various difﬁculties, iterative evolution, and gradual 
development and perfection, and privacy computing still needs a lot of theoretical 
and technical exploration and research. The authors of this book intended to main-
tain their academic determination and wait until privacy computing has been devel-
oped and perfected for “Ten years of sharpening a sword” before publishing the

book. However, some organizations have confused the concept of “privacy com-
puting” and deviated from the scope of research advocated by the authors. At a time 
when academic research on privacy preservation is in a hurry, the authors, as the 
proposers of privacy computing, have a sense of mission and responsibility for the 
protection of personal information, and feel the urgency to publish this book as soon 
as possible to guide and promote the theoretical research and application of privacy 
computing. We hope that readers will actively promote the continuous research of 
privacy computing and let us join hands to preserve privacy and beneﬁt society. 
204
Postscript
Fig. A.10 The Second Academic Conference on Big Data Security and Privacy Preservation 
Fig. A.11 The Third Academic Conference on Big Data Security and Privacy Computing


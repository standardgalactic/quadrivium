
Digital 
Imaging for 
Photographic 
Collections 
Foundations for 
Technical Standards Second Edition
Franziska S. Frey 
James M. Reilly
Image Permanence Institute 
Rochester Institute of Technology

Sponsored by the National Endowment for the Humanities, Division of Preservation and Access 
www.neh.gov
© 1999, 2006 Image Permanence Institute
First edition 1999
Second edition 2006
Image Permanence Institute 
Rochester Institute of Technology 
70 Lomb Memorial Drive 
Rochester, NY 14623-5604 
Phone: 585-475-5199	
Fax: 585-475-7230
The information in this report is also available on line at www.imagepermanenceinstitute.org.
IPI is jointly sponsored by the Society for Imaging Science and Technology and the Rochester Institute of 
Technology.

iii
Contents
About the second edition..............................................................................v
Acknowledgments....................................................................................................................v
Foreword...........................................................................................................................................vi
Introduction...................................................................................................... 1
The Phases of the Project	
4
Background of the Project.......................................................................... 5
RLG Technical Images Test Project	
5
Evaluation of Image Quality	
5
Results from Image Comparisons in the RLG Technical Images Test Project	
5
Where We Started	
6
Literature Research and Preparation of Samples	
7
Significant Projects for This Study	
7
The Salient Points	
8
Building the Image Quality Framework.................................................10
What Is Image Quality?	
10
Digital Image Quality and Its Implications for an Imaging Project	
11
Building Visual Literacy	
13
Subjective Image Quality Evaluation	
13
Definition of the Parameters for Evaluating Technical Pictorial Quality	
14
The Role of Targets in Evaluation of the Image Quality Parameters	
15
The Parameters of the Image Quality Framework	
17
Tone Reproduction	
17
Targets to Use	
19
Detail and Edge Reproduction (Resolution)	
20
What is Digital Resolution?	
21
How Is Digital Resolution Measured?	
22
The Modulation Transfer Function (MTF)	
22
Targets to Use	
24
Noise	
26
Target to Use	
27

iv
Color Reproduction	
27
Pictorial Rendering Intent	
28
Choosing a Color Space	
30
New Tools and Developments	
30
Image Artifacts	
32
Setting Up Imaging Systems	
32
Monitor Calibration	
32
Digital Master and Derivatives	
33
Quality and Process Control	
33
Benchmarking Scanner Systems	
34
Reproduction Qualities and Characteristics of the Digital Master and Derivatives	
34
Functional Qualities and Characteristics of the Digital Master and Derivatives	
35
Documentation of the Imaging Process—Administrative Metadata	
35
Image Processing	
35
Processing for Archiving	
36
Processing for Access	
36
Data Compression	
36
Lossless and Visually Lossless Compression	
36
Lossy Compression	
37
The Conference..................................................................................................38
Conclusions........................................................................................................40
References.............................................................................................................41
Bibliography........................................................................................................44
Selected Internet Resources.......................................................................45


Acknowledgments
The Image Permanence Institute gratefully acknowledges 
the support of the Division of Preservation and Access 
of the National Endowment for the Humanities for this 
project. We would like to thank the American Memory 
Team of the Library of Congress, specifically Carl Fleis-
chhauer and Phil Michel, who made it possible to translate 
our ideas into the actual project. Special thanks go to 
Sabine Süsstrunk, EPFL, Lausanne, Switzerland; Rudolf 
Gschwind, University of Basel, Basel, Switzerland; Asher 
Gelbart; Jack Holm, HP Labs, Palo Alto; Steve Puglia, 
National Archives; and Stephen Chapman, Harvard 
University Library. Thanks to Karen Santoro for design, 
editing, and illustrations. This work would not have been 
possible without the many individuals in the field working 
on digital projects who willingly shared their experiences.
about the second 
edition
Close to seven years have passed since the first printing 
of this booklet. While many technological advances have 
been made in imaging in the meantime, the fundamentals 
described in this booklet still hold true. The field is closer 
than ever to getting all the necessary standards and tools 
in place, but there are still many challenges to overcome 
to build the seamless workflows needed. We have updated 
all the cited internet resources and references to reflect the 
changes that have happened in the years since the first 
edition. We have, however, left the main text unchanged 
and are convinced that it remains a valuable resource.
Franziska Frey and James Reilly
Rochester, NY 
July 2006 

vi
Foreword
There is a well-known truth among photographers that 
copy photography—taking a picture of a picture—is tech-
nically more difficult than making a portrait or a land-
scape. It is all too easy in copy work to alter the contrast or 
lose the fine details of the original. As cultural institutions 
begin to make digital copies of their photograph collec-
tions, they are learning a variant of the old truth: it is as 
difficult to make good digital copies as good photographic 
ones. The goal of the NEH-sponsored project that made 
this publication possible was to offer some guidance to 
libraries, archives, and museums in their efforts to convert 
photographic collections to digital form. Specifically, we 
sought to identify the key issues affecting image qual-
ity, clarify the choices facing every digitizing project, and 
explore ways to measure digital image quality.
Along the way, we learned that one of the most 
important and difficult questions to answer is what level 
of quality is really needed in digital image collections. In 
the hands of expert operators, today’s best digital imag-
ing hardware is capable of capturing all the information 
in photographic originals. Such a high quality standard 
produces the most versatile digital images but requires the 
storage and manipulation of huge files. A lower quality 
standard produces more manageable files but often limits 
the utility of the files for such demanding uses as publica-
tion or exhibition. Selecting the appropriate quality level 
will always depend on careful analysis of the desired uses 
of the images in the near and long term.
In the project and in this publication, we have sought 
to clarify not only the qualitative aspects of quality choices, 
but also the technical and quantitative. Measurements 
that ensure adequate capture of detail and contrast are 
in some ways easier and more accurate in digital imag-
ing than in conventional photography because they can 
be done in software. Only when off-the-shelf software 
for this purpose becomes available can the full promise of 
digital imaging for institutional photograph collections be 
realized.
Franziska Frey and James Reilly
Rochester, NY 
September 1999

Introduction  
Introduction
The grant project, Digital Imaging for Photographic 
Collections: Foundations for Technical Standards was a 
two-year research study that investigated the use of digital 
imaging in libraries and archives.
There are no guidelines or accepted standards for 
determining the level of image quality required in the 
creation of digital image databases for photographic 
collections. Although clear imaging standards are not yet 
fully in place, and it is difficult to plan in an atmosphere 
of technological uncertainty, there are some basic rules 
that can be followed to minimize unexpected results.1-3 
Within this project we began to develop an “image quality 
framework” that will help with planning parts of digi-
tal-imaging projects. Our framework is not meant to be 
complete and ready to use; more work on various levels is 
needed. However, fruitful discussions were begun through 
this work, and we can see some of our ideas being taken 
forward in other initiatives.
The materials that make up photographs (silver or 
dyes as image-forming materials; paper, celluloid, or other 
plastics as base materials; and gelatin, albumen, or collo-
dion as binders) are not chemically stable. Environmental 
influences such as light, chemical agents, heat, humidity, 
and storage conditions affect and destroy photographic 
materials. In a general way, the life span of photographs 
can be extended only by appropriate storage at low 
temperatures and low humidity. Storage under controlled 
climatic conditions does not prevent decay, however; it 
results, at best, in a significant slowdown. Photographic 
collections are facing a dilemma: on one hand, photo-
graphic documents must be stored under correct climatic 
conditions, and, on the other hand, it is often necessary to 
have quick access to them. Frequently, in this situation, an 
unsatisfactory compromise is made.4
The long-term preservation of photographic images 
is always a very demanding task. The principles of secure 
preservation for digital data are fundamentally different 
from those for traditional analogue data.5-8 First, in tradi-
tional preservation there is a gradual decay of image qual-
ity, while digital image data either can be read accurately 

  Digital Imaging for Photographic Collections: Foundations for Technical Standards
or, in most cases, cannot be read at all. Secondly, every 
analogue duplication process results in a deterioration of 
the quality of the copy, while the duplication of the digital 
image data is possible with no loss at all.
In an idealized traditional archive, the images should 
be stored under optimal climatic conditions and never 
touched again. As a consequence, access to the images 
is severely hindered while decay is only slowed down. A 
digital archive has to follow a different strategy. The safe 
keeping of digital data requires an active and regular 
maintenance of the data. The data have to be copied to 
new media before they become unreadable. Since infor-
mation technology is evolving rapidly, the lifetime of both 
software and hardware formats is generally shorter than 
the lifetime of the recording media. However, since the 
digital data can be copied without loss, even if the media 
type and hardware change, the image is in a “frozen” 
state, and the decay has completely stopped.
The main difference between a traditional archive 
and a digital archive is that the traditional archiving ap-
proach is a passive one with images being touched as little 
as possible. Often, however, this works only in theory. If a 
document is known to be available, it is likely to be used. 
Therefore, in practice, we see an increased handling of 
original documents as soon as they become available in 
digital form. The future will show whether a good enough 
digitized copy will reduce this behavior.
The digital archive needs an active approach in 
which the digital data (and the media they are recorded 
on) is monitored continually. This constant monitoring 
and copying can be achieved with a very high degree of 
automation.
One of the big issues that institutions should consider 
prior to implementing a project is the anticipated use of 
their digital image collections. Will the images be made 
accessible on a stand-alone workstation or via the World 
Wide Web? Will they be used for printing reproductions? 
What size will the prints be? Are there restrictions on ac-
cess that must be honored? These are only a few of the 
questions that have to be answered before a digitization 
project starts.
There is a growing consensus within the preservation 
community that a number of image files must be created 
for every photograph to meet a range of uses. First, an 
The Advantages of digital 
information
•  Digital data represents a symbolic 
description of the originals; it can 
be compared to the invention of 
writing.
•  Digital information can be copied 
without loss.
•  Active sharing of digital images is 
easily possible.
•  New viewing experiences are 
possible by browsing through a 
collection without pre-selection 
by another individual. This will 
allow a completely different type 
of intellectual access to pictorial 
information.

Introduction  
“archive” or master image should be created. It should 
contain a brightness resolution greater than eight bits per 
channel, it should not be treated for any specific output 
in mind, and it should be uncompressed or compressed 
in a lossless manner. From this archive file various access 
files can be produced as needed to meet specific uses. 
The following three examples illustrate the ways in which 
the intended use drives decisions regarding digital image 
quality:
• The digital image is used only as a visual 
reference in an electronic database. The 
required digital image quality is low, in terms of 
both spatial and brightness resolution content. 
The display is usually limited to a screen or a low-
resolution print device. Thumbnail image size for 
screen viewing usually does not exceed a width of 
approximately 250 pixels. If an additional, larger 
image size is desired for low-resolution preview-
ing on a print device or larger viewing on screen, 
pixel dimensions of 1024 x 768 are sufficient for 
most applications. Exact color reproduction is not 
critical. Additionally, images can be compressed to 
save storage space and delivery time. 
• The digital image is used for reproduction. 
The requirements for the digitizing system will 
depend on the definition of the desired reproduc-
tion. Limiting output to certain spatial dimen-
sions will facilitate the decision-making process. 
For example, if the output is limited to an 8 x 10 
hard copy at a resolution of 300 dots per inch 
(dpi), the dimensions of the digital file need not 
exceed 2,400 x 3,000 pixels. Similarly, decisions 
regarding tonal reproduction are facilitated when 
modestly sized reproductions in print are the goal 
of digitization. Currently, most digitizing systems 
will allow only an eight-bit-per-color output. This 
is in most cases, a perceptual, not a colorimetric, 
rendering of the original. It is important to note 
that if these colors are not mapped correctly, the 
digital file may not always replicate the tone and 
color of the original. 
•	The digital image represents a “replace-
ment” of the original in terms of spatial and 
Digital Master versus 
Derivatives
•  The digital master is the file that 
is archived. It represents the high-
est quality file that has been digi-
tized. Since this is the information 
that is supposed to survive and be 
taken into the future, the main is-
sues in creating the digital master 
relate to longevity and quality.
•  The derivatives are the files for 
daily use. Speed of access and 
transmission and suitability for 
certain purposes are the main is-
sues to consider in the creation of 
derivative files.

  Digital Imaging for Photographic Collections: Foundations for Technical Standards
tonal information content. This goal is the most 
challenging to achieve given today’s digitizing 
technologies and the cost involved. The informa-
tion content in terms of pixel equivalency varies 
from original to original. It is defined not only by 
film format but also by emulsion type, shooting 
conditions, and processing techniques. Additional-
ly, eight-bit-per-color digital capture might be ad-
equate for visual representation on today’s output 
devices, but it might not be sufficient to represent 
all the tonal subtleties of the original. Ultimately, 
“information content” has to be defined, whether 
based on human perception, the physical proper-
ties of the original, or a combination of both.
The Phases of the Project
The first phase of the project involved searching the most 
recent technical literature, making connections with other 
people and projects, defining and planning the techni-
cal image choices to be explored, setting up an imaging 
workstation, and having a closer look at the sample images 
that had been created in another initiative. The rapid and 
ongoing changes in the field made this a constant task 
throughout the project.
The main outcome of the second phase was a frame-
work to define subjective and objective image parameters. 
Because those working with the new imaging technologies 
are only now beginning to understand all the associated 
issues, definitions of the parameters and tools to measure 
them are not readily available. IPI has defined some of the 
parameters, focusing on the materials found in photo-
graphic collections. 
The colloquium entitled Digitizing Photographic 
Collections—Where Are We Now? What Does The Future 
Hold? took place June 7‑9, 1997, at Rochester Institute 
of Technology (RIT). The event received a lot of atten-
tion and brought to Rochester over 120 attendees and 20 
speakers from around the world.

Background of the Project  
Figure 2. Image quality evaluation was performed 
by subjectively comparing all of the different 
prints to each other.
Figure 1. The test images of the RLG technical 
image project filled a number of binders, since the 
choices for duplication are numerous.
Background of the 
Project
RLG Technical Images Test Project
This project was based on the Technical Images Test Proj-
ect started by a task force of the Research Libraries Group 
in 1992.9 The project was designed to explore how image 
quality is affected by various choices in image capture, 
display, compression, and output. There are many ways 
to create and view digital images. The task force felt that, 
although there was no “best” way for every collection or 
every project, it would be helpful to define and explore a 
finite range of practical choices.
The RLG Technical Images Test Project was able to 
achieve some success in clarifying image quality issues 
for the initial step of image capture. It also became clear, 
however, that the given task was much more complex and 
consumed more time and resources than anticipated.
Evaluation of Image Quality
Fourteen images representing a range of photographic 
processes and sizes of originals were picked from the IPI 
study collection. From these photographs were produced 
negatives, positives, color internegatives, or duplicate 
transparencies (Figure 1). The same photographs were 
then used to generate positive photographic prints, in either 
color or black and white. The basic method of comparing 
quality resulting from the various intermediate formats 
and film types was to lay all the prints out on a table under 
controlled lighting conditions (Figure 2). With the naked 
eye, with a loupe, or with a stereo microscope, prints were 
compared to each other two at a time. The sharpness of the 
same selected details in each print were compared, as were 
graininess and smoothness of tone. For the color originals 
and the nineteenth-century processes, color fidelity was also 
important, but it was recognized that there is an extra layer 
of difficulty in controlling and judging color fidelity.
Results from Image Comparisons in the RLG Technical 
Images Test Project
The image comparison exercise showed us that the first 
important decision to be made at the digital capture stage 

  Digital Imaging for Photographic Collections: Foundations for Technical Standards
Figure 3. Scanning from original or intermediate? 
Top image is a contact print from an original 8 x 
10 negative. The arrow points to a detail chosen 
for comparison. Image A shows the selected detail 
from a scan of the original; image B shows the 
same detail from a scan of a 35mm intermediate.
is whether digitization should be done directly from the 
original photographs themselves or from photographic 
copies, also known as photographic intermediates (Fig-
ure 3). There are advantages and disadvantages to both 
approaches. Because every generation of photographic 
copying involves some quality loss, using intermediates 
immediately implies some decrease in quality. Intermedi-
ates may also have other uses, however; for example, they 
might serve as masters for photographic reference copies 
or as preservation surrogates. 
This leads to the question of whether the negative or 
the print should be used for digitization, assuming both 
are available. As stated above, quality will always be best if 
the first generation of an image, i.e., the negative, is used. 
However, it can happen, mainly in the domain of fine-art 
photography, that there are big differences between the 
negative and the print. The artist often spends a lot of 
time in the darkroom creating his prints. The results of all 
this work are lost if the negative, rather than the print, is 
scanned. The outcome of the digitization will be disap-
pointing. Therefore, for fine art photographs, scanning 
from the print is often recommended. Each case must be 
looked at separately, however.
One of the most important lessons learned in this 
exercise was that many of the original photographs in 
institutional collections have truly outstanding image qual-
ity. Even using 4 x 5 duplication film, excellent lenses, and 
the skill and experience of many years of photographic 
duplication, there was still a quite noticeable loss of image 
quality in the copying of an original 8 x 10 negative. Other 
images showed the same results, to varying degrees. This 
high level of image quality in original photographs sets a 
very high standard for successful reformatting projects, 
whether conventional or digital. We must be careful not to 
“reformat” the quality out of our image collections in the 
name of faster access. Instead, we must learn what it takes 
to bring that quality forward for future generations or at 
least to know that we are failing to do so.
Where We Started
The RLG Technical Images Test Project showed us that 
the question of image quality cannot be answered in a 
linear fashion. Scanning, processing, and outputting im-
A
B

Background of the Project  
Figure 4. memory.loc.gov/ammem/index.html
Figure 5. www.corbis.com
ages involve many different parameters that affect image 
quality. In any large digital imaging effort, objective pa-
rameters and ways to control them must be defined at the 
outset. IPI’s goal was to attempt to quantify image quality 
by building an image quality framework.
We learned a great deal from this process—most 
importantly, that there is a lot more work to be done. 
Calibration issues need to be solved before image quality 
can be evaluated on a monitor. Methods for testing the 
capabilities of, or benchmarking, systems must be devised. 
Most critical of all, a common language in which all in-
volved parties can communicate needs to be developed.
Literature Research and Preparation of Samples
The first phase of the project consisted of examining digital 
imaging projects in the libraries and archives field. This 
was accomplished through personal contact with people 
at institutions conducting such projects, watching various 
discussion lists and the growing number of museum sites 
on the Internet, and researching newsletters and scientific 
journals in the field. Because everything is changing so 
quickly in the digital field, literature research and exami-
nation of digital projects were ongoing tasks, proceeding 
hand-in-hand with monitoring and learning about new 
technologies.10-12 
Significant Projects for This Study
•	National Digital Library Project (NDLP) of 
the Library of Congress. The Library plans to 
convert as many as five million of its more than 
one hundred million items into digital form before 
the year 2000. The materials to be converted 
include books and pamphlets, manuscripts, prints 
and photographs, motion pictures, and sound 
recordings. NDLP used part of our image quality 
approach in their request for proposals and in the 
process control for their current scanning projects 
(Figure 4).
•	Corbis Corporation. Corbis is one of the big-
gest digital stock agencies in the world. With the 
incorporation of the Bettmann Archives, Corbis 
faces new problems similar to the ones archives 
and libraries have, due to the fact that there is a 

  Digital Imaging for Photographic Collections: Foundations for Technical Standards
whole new range of materials that will have to be 
scanned. Contact with Corbis gave us an interest-
ing insight into some of the issues to be faced by 
a digital stock agency, a company whose ultimate 
goal is to make money with digitized photographs 
(Figure 5).
•	Electronic Still Photography Standards 
Group (ANSI IT10). The scope of this commit-
tee includes specifying storage media, device inter-
faces, and image formats for electronic still picture 
imaging. The scope also includes standardizing 
measurement methods, performance ratings for 
devices and media, and definitions of technical 
terms. Within the committee, a new task group for 
characterization of scanners has been formed.13 
Some of the findings of the current project are 
being used as a basis for a standard in this area. 
The cumulative experience that was brought into 
this project is very valuable and will speed up the 
publication of much-needed documents (Figures 6 
and 7).
•	AMICO (Art Museum Image Consortium). 
RIT, together with the Image Permanence Insti-
tute, has been selected to be among the partici-
pants of the University Testbed Project. AMICO 
has been formed by twenty-three of the largest 
art museums in North America. The mission of 
this nonprofit organization is to make a library 
of digital documentation of art available under 
educational license (Figure 8).
The Salient Points
IPI’s ongoing review of work in the field showed a grow-
ing awareness of the complexity of undertaking a digital 
conversion project. In addition to the creation of a digital 
image database, which by itself brings up numerous 
problems, maintenance of the database over time must be 
considered. Questions regarding the permanence of the 
storage media, accessibility, quality control, and constant 
updating are only a few of those that must be addressed.
We also saw proof that many of the problems arising 
from the need to scan for an unknown future use are not 
yet solved and that there is a great deal of uncertainty 
Figure 6. www.i3a.org/it10.html
Figure 7. IT10 task group for the characteriza-
tion of scanners. The first work item deals with 
resolution measurements for scanners.
Figure 8. www.amico.org (In June of 2005, the 
members of the Art Museum Image Consortium 
voted to dissolve their collaboration.)

Background of the Project  
about how to proceed. Those responsible for some of the 
big digital reformatting projects report the same problem: 
rapid changes in the technology make it difficult to choose 
the best time to set up a reformatting policy that will not 
be outdated tomorrow.
The lack of communication between the technical 
field and institutions remains a formidable obstacle. It 
cannot be emphasized enough that if institutions fail to 
communicate their needs to the hardware and software 
industries, they will not get the tools they need for their 
special applications. Archives and libraries should know 
that they are involved in creating the new standards. It 
can be seen today that whoever is first on the market with 
a new product is creating a de facto digital technology 
standard for competitors. Furthermore, time to create new 
standards is very short; industry will not wait years to 
introduce a product simply because people cannot agree 
on a certain issue. Both institutions and industry are inter-
ested in a dialogue, but there is no common language.14-18
The exponential growth and use of the Internet has 
raised a whole new range of questions and problems that 
will have to be solved, but the Internet is also a great 
information resource.
A digital project cannot be looked at as a linear pro-
cess, in which  one task follows the other. Rather, it has to 
be looked at as a complex structure of interrelated tasks 
in which each decision has an influence on another one. 
The first step in penetrating this complex structure is to 
thoroughly understand each single step and find metrics 
to qualify it. Once this is done, the separate entities can be 
put together in context. We are still in the first round of this 
process, but with the benefit of all the experience gathered 
from the various digital projects in the field, we are reach-
ing the point at which the complex system can be looked 
at as a whole.
The need for 
Communication
It is up to libraries and archives 
to tell  the hardware and software 
industries exactly what they need, 
but before a fruitful dialogue can 
take place a common language 
must be developed.
Some findings from this project 
concerning spatial resolution 
have been used as a basis for the 
ISO Standard 16067-1:2003, 
Electronic scanners for photo-
graphic images —Spatial resolu-
tion measurements: Part 1, Scan-
ners for reflective media.

10  Digital Imaging for Photographic Collections: Foundations for Technical Standards
What Is Image Quality?
According to The Focal Encyclopedia of Photography,
[t]he basic purpose of a photograph is to 
reproduce an image. One of the three basic 
attributes of a reproduction image is the 
reproduction of the tones of the image. Also 
of importance are the definition of the image 
(the reproduction of edges and detail and the 
amount of noise in the image) and the color 
reproduction. It is convenient to deal with these 
attributes when evaluating an image.19
Image quality is usually separated into two classes:
•	Objective image quality is evaluated through 
physical measurements of image properties. 
Historically, the definition of image quality has 
emphasized image physics (physical image pa-
rameters), or objective image evaluation.
•	Subjective image quality is evaluated through 
judgment by human observers. Stimuli that do 
not have any measurable physical quantities 
can be evaluated by using psychometric scaling 
test methods. The stimuli are rated according 
to the reaction they produce on human observ-
ers. Psychometric methods give indications about 
response differences. Psychophysical scaling tools 
to measure subjective image quality have been 
available only for the last 25 to 35 years.20
Quantification of image quality for the new imaging 
technologies is a recent development.21-24 The theoretical 
knowledge and understanding of the different param-
eters that are involved is available now. Still missing for 
the practitioner of digital imaging are targets and tools 
to objectively measure image quality. These tools are 
available only within the companies that manufacture 
imaging systems and are used mostly to benchmark the 
companies’ own systems. Furthermore, in most cases, the 
systems being used for digital imaging projects are open 
systems, which means that they include modules from dif-
Building the Image 
Quality Framework

Building the Image Quality Framework  11
ferent manufacturers. Therefore, the overall image quality 
performance of a system cannot be predicted from the 
manufacturers’ specifications, since the different compo-
nents influence each other. 
It should be kept in mind that scanning for an archive 
is different from scanning for prepress purposes.25 In the 
latter case, the variables of the scanning process are well 
known, and the scanning parameters can be chosen ac-
cordingly. If an image is scanned for archival purposes, 
the future use of the image is not known, and neither are 
technological changes that will have taken place a few 
years from now. This leads to the conclusion that decisions 
concerning the quality of archival image scans are very 
critical.
As seen at various conferences, this is a relatively new 
concept for both the institutions and the technical field, 
and it will take some work to help both sides understand 
where the problems are. This is a topic for more research. 
The ANSI Standards task group for the characterization 
of scanners has contributed to the technical community’s 
awareness of the issue.
Image quality evaluations are important at two dif-
ferent stages of a project: at the beginning, to benchmark 
the system that will be used, and later, to check the images 
that have been produced.26,27
Digital Image Quality and Its Implications for an 
Imaging Project
There are no guidelines or accepted standards for deter-
mining the level of image quality required in the creation 
of digital image databases for access or preservation of 
photographic collections. As a result, many institutions 
have already been disappointed because their efforts 
have not lead to the results they were hoping for. Either 
the parameters chosen for the digitization process were 
not thought through, or the technology changed after the 
project started. In the first case, the failures might have 
been prevented. No one knows what technology will be 
available in a few years, however, and the task of choosing 
the right scanning parameters still needs to be researched. 
One problem is that we are currently at the beginning 
of the cycle of understanding image quality for the new 
imaging technologies. 

12  Digital Imaging for Photographic Collections: Foundations for Technical Standards
Often, imaging projects start with scanning text docu-
ments. When this type of material is scanned, quality is 
calculated according to a “Quality Index,” which is based 
on published guidelines for microfilming (Figure 9).
Quality Index is a means for relating resolu-
tion and text legibility. Whether it is used for 
microfilming or digital imaging, QI is based on 
relating text legibility to system resolution, i.e., 
the ability to capture fine detail.28
There is no common unit like character size that can 
ensure the semantic fidelity of images. The quantitative 
solutions that have evolved from scanning text therefore do 
not have a logical or practical fit with capturing images.29 
This means that other ways have to be found for the 
evaluation of the quality of an image and for determining 
how much of the image information is conveyed in the 
digital reproduction.
The more one looks at image quality and ways to 
clearly define it, the more parameters have to be taken 
into account. We have tried to develop a balanced ap-
proach that is not only usable in the archives and libraries 
world but also as complete as possible from an engineering 
point of view.
In addition, when looking at image quality, the whole 
image processing chain has to be examined. Besides issues 
concerning the scanning system, IPI has looked at com-
pression, file formats, image processing for various uses, 
and system calibration.
One of the big issues is that institutions will have to 
decide beforehand on the use of their digital images. This 
still creates a lot of questions and problems. Sometimes, 
how images will be used is not clear when a project starts. 
More often, however, institutions don’t take enough time 
to think about the potential use of the digital images. Fur-
thermore, institutions often have unrealistic expectations 
about digital projects. Even if the goals have been carefully 
defined, costs may not be worked out accordingly, or goals 
may not match the available funds.
Although the ease of use of many digitizing systems 
has fostered the perception that scanning is “simple,” 
successfully digitizing a photographic collection requires as 
much experience as conventional reformatting.30 Further, 
most of the available scanning technology is still based 
Figure 9. The main quality issue for reformatted 
paper documents is legibility. The higher the ppi, 
the more details of the character can be resolved. 
The needed ppi can be calculated for a known 
character height.

Building the Image Quality Framework  13
How Big Is the Collection?
When choosing a digitizing sys-
tem, bear in mind that approaches 
that work for a small number of 
images may not work for a large 
number of images. Very sophis-
ticated systems can be set up in 
a laboratory environment for a 
small number of images. Large 
numbers of images need a well 
thought out workflow.
on the model of immediate output on an existing output 
device with the original on hand during the reproduction 
process. Spatial resolution and color mapping are deter-
mined by the intended output device. Depending on the 
quality criteria of the project, a more sophisticated system 
and more expertise by the operator are needed to success-
fully digitize a collection in an archival environment where 
the future output device is not yet known. The charac-
teristics of scanning devices such as optical resolution, 
dynamic range, registration of color channels, bit-depth, 
noise characteristics, and quantization controls need to be 
carefully evaluated with consideration of the final use of 
the digital images.
When choosing the digitizing system, it also must be 
remembered that approaches that work for a small num-
ber of images may not be suitable for the large number of 
images usually found in collections.
Building Visual Literacy
Looking at images and judging their quality has always 
been a complex task. The viewer has to know what he/she 
is looking for. The visual literacy required when looking 
at conventional images needs to be translated for digital 
images. Much more research is needed to enable us to fully 
understand the ways in which working with images on a 
monitor differ from working with original photographs. 
Subjective Image Quality Evaluation
In most cases, the first evaluation of a scanned image will 
be made by viewing it on a monitor (Figure 10). The 
viewer will decide whether the image on the monitor 
fulfills the goals that have been stated at the beginning of 
the scanning project. This is important, because human 
judgment decides the final acceptability of an image. It 
should be emphasized that subjective quality control must 
be executed on calibrated equipment, in an appropriate, 
standardized viewing environment. If tone and color are 
evaluated, it may be necessary to transform data to suit 
display viewing conditions.
While the image is viewed on the monitor, defects 
such as dirt, “half images,” skew, and so on, can be 
detected. In addition, a target can be used to check the 
registration of the three color channels for color scans. It 
Figure 10. How not to do it: comparison of the 
original printed reproduction and a digital image 
on the monitor. To be done correctly, subjective 
image quality evaluation requires a standardized 
environment with calibrated monitors and dim 
room illumination.

14  Digital Imaging for Photographic Collections: Foundations for Technical Standards
is also important to check visual sharpness at this point. 
Mechanical malfunction of the scanner or limited depth of 
field could cause images to lack sharpness.
Definition of the Parameters for Evaluating 
Technical Pictorial Quality
There are four main parameters to consider when assess-
ing the technical pictorial quality of an image. Due to the 
lack of off-the-shelf software and targets to evaluate the 
quality parameters, IPI had to create its own software 
tools and targets. As a result, two parameters, tone repro-
duction and detail and edge production, became the focus 
of the project.
•	Tone reproduction. This refers to the degree to 
which an image conveys the luminance ranges 
of an original scene (or of an image to be repro-
duced in case of reformatting). It is the single most 
important aspect of image quality. Tone reproduc-
tion is the matching, modifying, or enhancing of 
output tones relative to the tones of the original 
document. Because all of the varied components 
of an imaging system contribute to tone reproduc-
tion, it is often difficult to control.
•	Detail and edge reproduction. Detail is defined 
as relatively small-scale parts of a subject or the 
images of those parts in a photograph or other 
reproduction. In a portrait, detail may refer to in-
dividual hairs or pores in the skin. Edge reproduc-
tion refers to the ability of a process to reproduce 
sharp edges (the visual sharpness of an image).
•	Noise. Noise refers to random variations associ-
ated with detection and reproduction systems. 
In photography, the term granularity is used to 
describe the objective measure of density nonuni-
formity that corresponds to the subjective concept 
of graininess. In electronic imaging, noise is the 
presence of unwanted energy fluctuation in the 
signal. This energy is not related to the image 
signal and degrades it.
•	Color reproduction. Several color reproduction 
intents can apply to a digital image. Perceptual 
intent, relative colorimetric intent, and absolute 

Building the Image Quality Framework  15
colorimetric intent are the terms often associated 
with  the International Color Consortium (ICC). 
Perceptual intent is to create a pleasing image on 
a given medium under given viewing conditions. 
Relative colorimetric intent is to match, as closely 
as possible, the colors of the reproduction to the 
colors of the original, taking into account output 
media and viewing conditions. Absolute colorimet-
ric intent is to reproduce colors as exactly as pos-
sible, independent of output media and viewing 
conditions.
These parameters will be looked at in greater detail in 
a later section.
The Role of Targets in Evaluation of the Image Quality 
Parameters
Targets are a vital part of the image quality framework. 
To be able to make objective measurements of each of the 
four parameters, different targets for different forms of 
images (e.g., prints, transparencies, etc.) are needed. To 
get reliable results, the targets should consist of the same 
materials as those of the items that will be scanned—pho-
tographic paper and film. After targets are scanned they 
are evaluated with a software program. Some software 
components exist as plug-ins to full-featured image brows-
ers, others as stand-alone programs.
Targets can be incorporated into the work flow in 
various ways. Full versions of the targets might be scanned 
every few hundred images and then linked to specific 
batches of production files, or smaller versions of the 
targets might be included with every image. The chosen 
method will depend on the individual digital imaging 
project.
As more institutions initiate digitization projects, hav-
ing an objective tool to compare different scanning devices 
will be more and more important. Until now, scanner 
manufacturers usually have used their own software when 
evaluating and testing systems.
For the current project, IPI examined approaches tak-
en by other research projects in similar areas and looked 
carefully at a variety of targets. Some targets were already 
available for other purposes and could be purchased; some 
had to be custom-made. A set of targets with software to 
read them are available on the market (see page 45).

16  Digital Imaging for Photographic Collections: Foundations for Technical Standards
The use of objective measurements resulting from 
the target evaluation will be twofold. Some of the results, 
together with additional information like spectral sensitivi-
ties and details about the actual image processing chain, 
will be used to characterize the scanning system. This 
assumes that spectral sensitivities are known and that a 
complete description of the image processing chain is at 
hand. These requirements are not often fulfilled, however, 
since scanner manufacturers are reluctant to provide this 
information. Other results of the target evaluation will be 
associated with each image file; this information will be 
used to perform data corrections later on as the images are 
processed for output or viewing. Therefore, standardized 
approaches and data forms are required for interchange-
ability of the data.
In “Specifics of Imaging Practice,” M. Ester wrote:
If I see shortcomings in what we are doing 
in documenting images, they are traceable 
to the lack of standards in this area. We have 
responded to a practical need in our work, 
and have settled on the information we believe 
is important to record about production and 
the resulting image resource. These recording 
procedures have become stable over time, but 
the data would become even more valuable if 
there was broad community consensus on a 
preferred framework. Compatibility of image 
data from multiple sources and the potential to 
develop software around access to a common 
framework would be some of the advantages.31
New file formats like TIFF/EP32 that include a large 
number of defined header tags will facilitate the standard-
ized storage of image attribute information (administra-
tive metadata), which, in turn, will facilitate future image 
processing. Applications do not yet support TIFF/EP, but 
it is important that collection managers are aware of these 
possibilities and start to incorporate these ideas into their 
digital projects.
Discussions with people in the field have shown that 
there is still some confusion about the role targets play in 
the digital imaging process. It is important to emphasize 
that targets are about the scanning system and not about 
collections. This means that the target evaluations are 
primarily aimed at characterizing scanning systems.
At this time, many aspects of scanning photographs 

Building the Image Quality Framework  17
still require the intervention of a well-trained operator. 
In a few years, some of these tasks will be automated, 
and manual interventions will be less and less necessary. 
One could say that targets will then be about collections, 
because no matter what original is scanned, it will auto-
matically turn out right. However, targets will always be 
useful for checking the reproduction quality of the digital 
files, e.g., for confirming that aim-point values are actually 
reached. This does not mean that scanning is done entirely 
“by the numbers,” because an operator will still be needed 
to decide when to consciously intervene to improve the 
subjective quality of the image. This does not apply if 16-
bit data are stored.
The Parameters of the Image Quality Framework
Tone Reproduction
Tone reproduction is the single most important parameter 
for determining the quality of an image. If the tone repro-
duction of an image is right, users will generally find the 
image acceptable, even if some of the other parameters are 
not optimal (Figures 11 and 12).
Tone reproduction is applicable only in the context 
of capture and display. This means that an assumption 
must be made regarding the final viewing device. Three 
mutually dependent attributes affect tone reproduction: 
the opto-electronic conversion function (OECF), dynamic 
range, and flare. OECF can be controlled to a certain 
extent via the scanning software but is also dependent on 
the A/D (analog to digital) converter of the scanning sys-
tem; dynamic range and flare are inherent in the scanner 
hardware itself. 
The OECF shows the relationship between the opti-
cal densities of an original and the corresponding digital 
values of the file. It is the equivalent of the D-log H curve 
in conventional photography (Figure 13). Dynamic range 
refers to the capacity of the scanner to capture extreme 
density variations. The dynamic range of the scanner 
should meet or exceed the dynamic range of the original. 
Flare is generated by stray light in an optical system. Flare 
reduces the dynamic range of a scanner. 
The most widely used values for bit-depth equivalen-
cy of digital images is still eight bits per pixel for mono-
chrome images and 24 bits for color images. These values 
Figure 11. The three images show different repro-
duction of the gray tones. The image on the left 
is too light. The one on the right has no details 
in the shadow areas and is too dark overall. The 
one in the middle shows the most acceptable tone 
reproduction.
Figure 12. Tone reproduction control targets for 
users looking at images on monitors are impor-
tant quality control tools. This sample shows 
a gray wedge that is available on the National 
Archives web site (www.archives.gov/research/
arc/adjust-monitor.html). The user is asked to 
change brightness and contrast on the monitor 
screen in order to see all steps on the gray wedge 
target. It is important to remember that images 
may have to be processed for viewing on a moni-
tor.

18  Digital Imaging for Photographic Collections: Foundations for Technical Standards
are reasonably accurate for good-quality image output. 
Eight bits per channel on the input side is not sufficient for 
good-quality scanning of diverse originals. To accommo-
date all kinds of originals with different dynamic ranges, 
the initial quantization on the CCD side must be larger 
than eight bits.
CCDs work linearly to intensity (transmittance or re-
flectance). To scan images having a large dynamic range, 
12 to 14 real bits (bits without noise) are necessary on the 
input side. If these bits are available to the user and can be 
saved, it is referred to as having access to the raw scan. 
High-bit-depth information is very important, espe-
cially in the case of negatives. Negatives can be considered 
a photographic intermediate; they are not yet finalized for 
an “eye-pleasing” image like prints and slides. Negatives 
show a high variability. They can have low contrast, high 
contrast, and everything in between. The dark parts of the 
negatives contain the important image information. Only 
very good scanners can resolve very well in these dark 
parts of the originals. The scanner needs a high dynamic 
range and not a lot of flare to produce real, noise-free, 
high-bit information.
Often, it is only possible to get eight-bit data out of 
the scanner. The higher-bit file is reduced internally. This 
can be done in different ways. The scanner OECF shows 
how this is done for a specific scanner at specific settings. It 
is often done nonlinearly (nonlinear in intensity, but linear 
in lightness or brightness, or density), using perceptually 
compact encoding. A distribution of the tones linear to the 
density of the original leaves headroom for further process-
ing, but, unless the images and the output device have the 
same contrast, the images will need to be processed before 
viewing. Processing images to look good on a monitor will 
limit certain processing possibilities in the future. 
The data resulting from the evaluation of the OECF 
target is the basis for all subsequent image quality param-
eter evaluations, e.g., resolution. It is therefore very impor-
tant that this evaluation is done carefully. In cases where 
data is reduced to eight bits, the OECF data provide a 
map for linearizing the data to intensity (transmittance or 
reflectance) by applying the reverse OECF function. This 
step is needed to calculate all the other parameters. In the 
case of 16-bit data, linearity to transmittance and reflec-
tance will be checked with the OECF data. Any processing 
Figure 14. Calibrated gray-scale test targets serve 
as a link back to the reality of the original docu-
ment or photograph.
Figure 13. Using a calibrated gray scale target 
and the resulting digital values, the tone repro-
duction of the scanning device can be determined. 
The reflection or transmission density of each 
step of the gray scale can be measured with a 
densitometer. Plotting these values against the 
digital values of the steps in the image file show 
the performance of the scanning device over the 
whole range of densities.

Building the Image Quality Framework  19
to linearize the data to density will follow later.
Reproducing the gray scale correctly often does not 
result in optimal reproduction of the images. However, the 
gray scale can be used as a trail marker for the protection 
of an institution’s investment in the digital scans (Figure 
14); having a calibrated gray scale associated with the 
image makes it possible to go back to the original stage 
after transformations, and it also facilitates the creation of 
derivatives. The gray scale could be part of the image, or 
the file header could contain the digital values.
Tone and color corrections on eight-bit images should 
be avoided. Such corrections cause the existing levels to be 
compressed even further, no matter what kind of operation 
is executed. To avoid the loss of additional brightness reso-
lution, all necessary image processing should be done on a 
higher-bit-depth file. Requantization to eight-bit images 
should occur after any tone and color corrections.
Often, benchmark values for the endpoints of the 
RGB levels are specified by the institution. The National 
Archives, for example, ask in their guidelines for RGB 
levels ranging from 8 to 247.33 The dynamic headroom at 
both ends of the scale is to ensure no loss of detail or clip-
ping in scanning and to accommodate the slight expan-
sion of the tonal range due to sharpening or other image 
processing steps (Figure 15).
Additionally, as part of the tone reproduction test, the 
flare of the system can be tested. Flare exists in every opti-
cal system, reducing the contrast of the original.
Targets to Use
•	OECF target for measuring linearity.34 This 
target (Figure 16) characterizes the relation-
ship between the input values and the digital 
output values of the scanning system. It is used 
to determine and change the tone reproduction. 
The target was developed based on the ongo-
ing research of the Electronic Still Photography 
Standards Group (ISO/TC 42/WG18). The target 
has been manufactured under IPI’s guidance by 
a company in Rochester. Since the specifications 
in the standard are tight, the production process 
proved to be lengthy.
•	Flare measurement target. A flare model can 
be determined with the various tone reproduction 
Figure 15. Histograms of the image files can be 
used to check whether all digital levels from 0 to 
255 are used (A), whether any clipping (loss of 
shadow and/or highlight details) occurred during 
scanning (B), or whether the digital values are 
unevenly distributed as can be the case after im-
age manipulation (C).
Figure 16. Targets for measuring linearity of the 
scanning system. The target on the lower left is 
for use with digital cameras, and the one on the 
upper right is for use with line scanners.
A
B
C

20  Digital Imaging for Photographic Collections: Foundations for Technical Standards
targets. Targets with different backgrounds and 
different dynamic ranges of the gray patches had 
to be manufactured (Figure 17).34
	 Another approach consists of using a target with 
an area of Dmin and an area of Dmax allowing the 
measurement of the flare, i.e., showing how much 
the original contrast is reduced in the digital file 
(see Figure 28 on page 24).
	 In addition, this target can be used to check the 
registration of the three color channels for color 
scans. In case of misregistration, color artifacts will 
appear at the edges. They can also be calculated 
(see ref. 34, Annex C).
Detail and Edge Reproduction (Resolution)
Monitoring digital projects showed that people are most 
concerned about spatial resolution issues. This is not 
surprising, because, of all the weak links in digital capture, 
spatial resolution has been the best understood by most 
people. Technology has evolved, however, and today “rea-
sonable” spatial resolution is neither extremely expensive 
nor does it cost a lot to store the large data files. Because 
questions concerning spatial resolution came up so often, 
we looked at this issue very closely. Spatial resolution is 
either input- or output-oriented. In the former case, the 
goal is to capture all the information that is in the original 
photograph; in the latter case, the scanning resolution is 
chosen according to a specific desired output.
Spatial resolution is the parameter to define detail 
and edge reproduction.35 Spatial resolution of a digital im-
age, i.e., the number of details an image contains, is usu-
ally defined by the number of pixels per inch (ppi). Spatial 
resolution of output devices, such as monitors or printers, 
is usually given in dots per inch (dpi).
To find the equivalent number of pixels that describe 
the information content of a specific photographic emul-
sion is not a straightforward process. Format of the origi-
nal, film grain, film resolution, resolution of the camera 
lens, f-stop, lighting conditions, focus, blur, and processing 
have to be taken into consideration to accurately deter-
mine the actual information content of a specific picture. 
The following table gives an idea of the pixel equivalencies 
for various film types.
Figure 17. Tone reproduction targets can be used 
to build a flare model for the scanning system. 
This requires  targets with different contrast and 
different backgrounds.

Building the Image Quality Framework  21
Another common question that should be answered 
before digitization starts is, given the spatial resolution of 
the files, how big an output is possible from the available 
file size? The relationship between the size of a digital 
image file, its total number of pixels, and consequently its 
maximum output size at different spatial resolutions can 
be analyzed mathematically.
To answer the question, the distinction has to be 
made between continuous-tone and halftone output. For 
optimal continuous-tone output the ratio between output 
dots and image pixels should be 1:1. In the case of print-
ing processes that require halftone images, between 1.5:1 
and 2:1 oversampling (pixels per inch of the digital file is 
one and a half to two times greater than dots per inch of 
the output) is needed.
What is Digital Resolution?
Why do we measure resolution? We do so, first, to make 
sure that the information content of the original image is 
represented in the digital image and, second, to ensure 
that the scanning unit used to digitize the image is in fo-
cus. Unlike photographic resolution, digital resolution does 
not depend on visual detection or observation of an image. 
Digital resolution is calculated directly from the physical 
center-to-center spacing between each sample or dot. This 
spacing is also called the sampling interval (Figure 18).
The number of sampling points within a given dis-
tance (usually an inch) is referred to as the device’s digital 
resolution, given in ppi. Digital resolution quantifies the 
number of sampling dots per unit distance while photo-
graphic resolution quantifies observed feature pairs per 
unit distance, as in line pairs per millimeter (lp/mm).
Translating between units of classical resolution and 
digital resolution is simply a matter of “two.” Dividing 
digital resolution values in half will yield units that are 
equivalent to photographic resolution. But there are con-
ceptual differences between the two that have to be kept 
in mind when using digital resolution. A misregistration 
Sampling Resolution for Extraction of the 
Film Resolution
Film speed very low (<64 ISO):  3500-5000 pixels/inch 
Film speed medium (200-320):  2000-2500 pixels/inch
Source: Jack Holm, HP Research Labs, The Evaluation of Digital Photography Systems, Short 
Course Notes, IS&T 49th Annual Conference, May 1996.
Figure 18. The number of sampling points within 
a given distance is referred to as the device’s 
digital resolution. In A(1), the digital resolution 
is low, and not all the image information will 
be included in the digital file. However, scanners 
often do not scan correctly at low resolution, 
showing the behavior depicted in A(2). Since the 
pixel values are taken over a certain distance and 
not averaged over a certain area, the quality of 
the low-resolution image in A(2) will be lower 
than in A(1). B(1) is sampled with a higher reso-
lution. A low-resolution derivative, B(2), that is 
calculated from this high-resolution file (using a 
resampling algorithm) will have a higher quality 
than the image originally scanned at low resolu-
tion with method A(2).
A
B
(1)
(2)
(1)
(2)

22  Digital Imaging for Photographic Collections: Foundations for Technical Standards
between image details and image sensors may give the 
impression that a certain device has less resolution than it 
actually has. Furthermore, aliasing is an issue.
In an ideal scan, the detectors and the lines of the 
target are perfectly aligned. The concept of misregistra-
tion can be easily shown by scanning a bar target. The 
detectors will only sense the light intensity of either the 
black line or the white space. If there is a misregistra-
tion between the centers of the lines and spaces relative 
to the detector centers, say by half a pixel, the outcome is 
different (Figure 19). Now each detector “sees” half a line 
and half a space. Since the output of every detector is just 
a single value, the intensities of the line and the space are 
averaged. The resulting image will therefore have the same 
digital value in every pixel. In other words, it will look like 
a gray field. The target would not be resolved. Therefore, 
the misregistration manifests itself as a contrast or signal 
loss in the digital image that affects resolution. Since it is 
impossible to predict whether a document’s features will 
align perfectly with the fixed positions of a scanner’s detec-
tors, more than two samples per line pair are required for 
reliable information scanning.
If the sampling interval is fine enough to locate the 
peaks and valleys of any given sine wave, then that fre-
quency component can be unambiguously reconstructed 
from its sampled values. This is referred to as Nyquist fre-
quency. Aliasing occurs when a wave form is insufficiently 
sampled (Figure 20). If the sampling is less frequent, then 
the samples will be seen as representing a lower-frequency 
sine wave.
The most noticeable artifact of aliasing is high spatial 
frequencies appearing as low spatial frequencies. After the 
wave form has been sampled, aliasing cannot be removed 
by filtering.
How Is Digital Resolution Measured?
The fundamental method for measuring resolution is to 
capture an image of a suitable test chart with the scanner 
being tested. The test chart must include patterns with 
sufficiently fine detail, such as edges, lines, square waves, 
or sine-wave patterns.
The Modulation Transfer Function (MTF)
The best overall measure of detail and resolution is the 
modulation transfer function or MTF. MTF was developed 
to describe image quality in classical optical systems—so-
Figure 20. Aliasing occurs when an image is 
insufficiently sampled. The samples will show 
up as a lower frequency. Example: An original 
has 500 line pairs per inch. This represents a 
function with 500 cycles per inch. To capture 
all the information (all the valleys and peaks of 
the function), at least 1000 pixels per inch are 
necessary, this is the so-called “sampling rate” 
(1000/25.4=40 pixels per mm). The maximum 
frequency a sampling device can capture is ½ 
the sampling rate. This is called the Nyquist 
frequency. In practice, more pixels per inch are 
necessary to capture all the information faithfully. 
In this sample the 1000-ppi device has a Nyquist 
frequency of 500 line pairs per inch. 
Figure 19. Misregistration between the detectors 
and the lines of the target by half a pixel can 
lead to the situation where the black-and-white 
lines of the original cannot be resolved and will 
look like a gray field in the digital image (all the 
digital values are the same, i.e., 128).

Building the Image Quality Framework  23
called linear systems. The MTF is a graphical representa-
tion of image quality that eliminates the need for decision-
making by the observer. The test objects are sine-wave 
patterns.36-42 
If the MTF is measured for a sampled-data system 
that is nonlinear, we are talking about the spatial frequency 
response (SFR) of the system. The terms MTF and SFR are 
used interchangeably. The measured results for a sampled-
data system will depend on the alignment of the target and 
the sampling sites. An average MTF can be defined, assum-
ing that the scene being imaged is randomly positioned with 
respect to the sampling sites.
The MTF is a graph that represents the image contrast 
relative to the object contrast on the vertical axis over the 
range of spatial frequencies on the horizontal axis, where 
high frequency in the test target corresponds to small detail 
in an object (Figures 21 and 22).
The MTF should be determined and reported for both 
horizontal and vertical scan directions, since the results can 
differ.
MTF is difficult and cumbersome to measure in images 
on photographic materials, requiring special equipment, 
like a microdensitometer. It is relatively easy to measure in 
digital images, however.
There are two approaches to defining the MTF of an 
imaging system. One is to use a sine-wave pattern (Figures 
23–26),43 the other is to use a slanted edge (Figures 27 and 
28).44,45 In the latter case, pixel values near slanted vertical 
and horizontal black-to-white edges are digitized and used 
Figure 22. Graph of the modulation transfer func-
tion for comparison of different imaging systems. 
If the tested system shows MTF values above the 
straight line (area with pattern), it performs well 
enough. The numbers in this figure apply for 
scanning at 500 ppi.
Figure 21. Graph of the modulation transfer 
function. The MTF shows the performance of 
two scanning systems over the whole range of 
frequencies (in the target represented by sine 
waves that are closer and closer together). The 
cutoff point of the system represents the high-
est frequency (the finest image details) that the 
scanner is able to resolve. An MTF specification 
to be met by vendors must be established. Scanner 
A has a better performance than Scanner B; scans 
from Scanner A will appear sharper.
(across a range of frequencies)
Figure 23. A wave is characterized by its amplitude and its spatial 
frequency.
Output Modulation
Input Modulation
MTF = 

24  Digital Imaging for Photographic Collections: Foundations for Technical Standards
to compute the MTF values. 
The use of a slanted edge 
allows the edge gradient to 
be measured at many phases 
relative to the image sensor 
elements, in order to eliminate 
the effects of aliasing. (This 
technique is mathematically 
equivalent to performing a 
“moving knife-edge measure-
ment.”)
Targets to Use
At this time, the bar targets designed for measurement of 
photographic resolution often are used to measure digital 
resolution (Figure 29). These targets are not suitable for 
this task, except for visually checking aliasing. Visually 
checking bar targets is not an easy task; the observer 
must know what to look for. Therefore, to measure digital 
resolution of sampling devices another approach has to be 
taken using slanted edges or sine-wave patterns.
•	Knife-edge target for resolution measure-
ment. A knife-edge target has been developed by 
the Electronic Still Photography Standards Group 
(Figure 30). A special target (QA-62) for testing 
scanners is available now as well. We used another 
target (Figure 28) that looks like the one used for 
flare measurements. It has been thoroughly tested 
using a specially developed software module.46 
Tests showed that the target in its current form 
is usable for a scan resolution of up to 600 dpi. 
Figure 26. As the bars of the sine-wave target get 
closer together at higher frequencies, the modula-
tion (i.e., variation from black to white) that is 
recorded by the scanner gets smaller and smaller. 
Figure 24. The sine waves of the test target are 
scanned and translated into digital values. If you 
were to measure how dark or light the image is at 
every point along a line across the bars, the plot 
of these points would be a perfect sine wave.
Figure 25. Input modulation/output modulation
Figure 27. Calculating the MTF using the moving knife-edge 
method. Pixel values across a slanted edge are digitized and, 
through a mathematical transformation of these values into the 
Fourier domain, the MTF of the system can be calculated.
Figure 28. Knife-edge tar-
get for resolution measure-
ment. Can also be used for 
flare measurement.

Building the Image Quality Framework  25
Figure 31. Sine Patterns sine-wave target. The 
sine waves in the two center rows of the image are 
used to calculate the MTF.
Figure 29. Bar targets 
with converging lines, 
like this star pattern, 
can be used to visually 
check the so-called 
cutoff frequency of the 
system (i.e., the small-
est features that can 
be resolved), but they 
cannot be used to get information on how the 
system is working for all the different frequencies 
in the image.
Figure 30. Resolution test chart developed for elec-
tronic still photography. The black bars are used 
to calculate the modulation transfer function.
This is due to limitations in the in-house produc-
tion process. Plans for developing a test target for 
higher scanning resolution were abandoned.
•	Sine-wave target by Sine Patterns Inc. for 
resolution measurements. This target (Fig-
ure 31) exists on photographic paper and film in 
different formats. For analysis of the sine-wave 
target, software developed by MITRE was used.47
After testing and comparing the sine-pattern and 
knife-edge techniques for determining MTF, IPI decided to 
include both methods in the test (Figures 32 and 33). The 
results indicated that both methods produce similar MTF 
characterizations under proper scanning conditions. It was 
found that the two methods responded differently to image 
sharpening, nonlinear output gamma mapping, and im-
proper resampling methods. The advantage of using both 
methods simultaneously lies in the ability to detect unde-
Figure 32. Software 
interface to compare 
MTF calculations from 
sine-wave target and 
knife-edge target.
Figure 33. User interface 
of the software tool to 
measure MTF from a 
knife-edge target devel-
oped at IPI.

26  Digital Imaging for Photographic Collections: Foundations for Technical Standards
sirable post-scan processing that has been intentionally or 
unintentionally applied. The software module developed at 
IPI to calculate the MTF for a knife edge can also compare 
the sine-wave and knife-edge techniques.
Noise
As defined earlier, noise refers to random signal variations 
associated with detection and reproduction systems. In 
conventional photography, noise in an image is the graini-
ness that can be perceived; it can be seen most easily in 
uniform density areas. The objective measure is granulari-
ty. In electronic imaging, noise is the presence of unwanted 
energy in the image signal (Figure 34). 
Noise is an important attribute of electronic im-
aging systems.48 Standardization will assist users and 
manufacturers in determining the quality of images being 
produced by these systems.49,50 The visibility of noise to 
human observers depends on the magnitude of the noise, 
the apparent tone of the area containing the noise, the 
type of noise, and the noise frequency. The magnitude of 
the noise present in an output representation depends on 
the noise present in the stored image data, the contrast 
amplification or gain applied to the data in processing, and 
the noise inherent in the output process and media. Noise 
visibility is different for the luminance (monochrome) 
channel and the color channels. 
The result of the noise test is twofold. First, it shows 
the noise level of the system, indicating how many bit 
levels of the image data are actually useful. For example, 
if the specifications for a scanner state that 16 bits per 
channel are recorded on the input side, it is important to 
know how many of these bits are actual image informa-
tion and how many are noise; in most cases, two bits are 
noise. For image quality considerations, the signal-to-noise 
ratio (S/N) is the important factor to know. The noise of 
the hardware used should not change unless the scanner 
operator changes the way she/he works or dirt is built up 
in the system.
There exist different types of noise in an imaging sys-
tem. The following definitions are part of the forthcoming 
standard for noise measurement.
•	Total noise. All the unwanted variations captured 
by a single exposure (scan).
Figure 34. Noise refers to random signal varia-
tions in electronic imaging systems, represented 
as a random pattern in the uniform gray pat-
terns. This graph shows the RMS (root mean 
square) error, a statistical evaluation representing 
noise over the density levels of the original image. 
An RMS of 0 is equal to no noise.

Building the Image Quality Framework  27
Figure 36. Matching original, soft copy on the 
monitor, and hard copy is the goal of color 
management.
•	Fixed pattern noise. The unwanted variations 
that are consistent for every exposure.
•	Temporally varying noise. Random noise 
due to sensor dark current, photon shot noise, 
analogue processing, and quantization that varies 
from one image to the next.
All of these parameters should be measured and the 
three above-described signal-to-noise ratios reported for 
the imaging system. 
Since many electronic imaging systems use extensive 
image processing to reduce the noise in uniform areas, the 
noise measured in the different large area gray patches 
may not be representative of the noise levels found in 
scans from real scenes. Therefore, another form of noise, 
so-called edge noise, will have to be looked at more closely 
in the future.
Target to Use
The OECF target shown in Figure 16 that was manufac-
tured to measure tone reproduction can be used, under 
certain circumstances, to measure noise over a wide range 
of input values. A special noise target (Figure 35) that also 
includes patches with frequency patterns has been devel-
oped by ANSI IT-10.
Color Reproduction
In addition to the image quality framework on the input 
side, color reproduction and tone reproduction in all areas 
of the image database were looked at more closely in the 
second phase of the project. This report will not go into 
detail in this area. The following shows a theoretical ap-
proach that IPI has been developing over recent months.
Because of its complexity, color reproduction (Figure 
36) was beyond the scope of this project; there are, how-
ever, a few thoughts to be considered.51-53 
Two basic types of image database can be defined, 
each needing different approaches to creation, access, 
and use of images. The first type of image database is the 
one containing large numbers of images, e.g., an image 
database for an insurance company, a government agency, 
or a historical society. In this case, the intent is to have in 
the database a good-quality image that is not necessarily 
colorimetrically true to the original. It is the information 
content of the image that is important. It is agreed upon 
Figure 35. Target used for noise measurements 
(see ISO Test Charts, p. 45).

28  Digital Imaging for Photographic Collections: Foundations for Technical Standards
from the beginning that the digital file in the image da-
tabase will not be used to reconstruct the original photo-
graph or scene.
The other type of image database will be built in en-
vironments where it is important that the digital image is 
a “replacement” of the original. Therefore, the digital data 
has to be captured and archived in such a manner that the 
original can be reconstructed.
Pictorial Rendering Intent
When targeting a wide user audience, computer platforms, 
color management systems, calibration procedures, color 
space conversions, or output devices cannot be mandated. 
Almost every institution has a different setup for access of 
the images. In addition, at the time of creation, it is usually 
not known what type of hardware and software are or will 
be available for access. Furthermore, one has to consider 
that the users of image databases are usually not trained 
in matters of digital imaging. However, they are visually 
sophisticated and are used to the quality they can get from 
conventional photographic images.
Nevertheless, decisions have to be made about spatial 
resolution, tone reproduction, and color space before im-
ages are digitized. In most cases, it will not be the goal to 
reproduce the physical properties of the original, but to 
reproduce its appearance under certain viewing conditions. 
Assumptions about the rendering device, color reproduc-
tion, and tone reproduction have to be made.
All original artwork cannot be treated equally. Differ-
ent digitizing approaches have to be established according 
to the type, condition, and perceived values of the originals. 
Current scanning technology deals reasonably well with 
current film emulsions and formats, provided that they 
have been exposed and processed correctly. However, many 
collections of high artistic and/or historical value were cap-
tured on photographic material that not only isn’t available 
anymore, but also has deteriorated to some degree.
There are several rendering intents that apply while 
digitizing original artwork.
•	The photographic image is rendered. In this 
case, the images are scanned with the intent to 
match the appearance of the original photographic 
image. The quality of the digital image can be 
evaluated by visually comparing the original to a 
reproduction on a calibrated display device with 

Building the Image Quality Framework  29
a similar contrast range. The assumption is made 
that the original photograph has been exposed and 
processed perfectly (Figure 37).
•	The photographer’s intent is rendered. There 
are many photographs with high content value 
that were not exposed or processed correctly. They 
can have a color cast, be over- or underexposed, 
or have the wrong contrast. In these cases, the 
photographer’s intent, not the original photo-
graph, needs to be rendered to achieve a pleasing 
reproduction. The scanner operator has to make 
decisions about tone and color reproduction by 
viewing the digitized image on a calibrated output 
device. This manual intervention determines the 
quality of the reproduction. Quality becomes highly 
dependent on the skill and experience of the opera-
tor (Figure 38).
•	The original appearance of the photograph 
is rendered. Often, older color photographs are 
faded and no longer have sufficient visual color 
information to make accurate judgments about the 
original. Reconstructing these photographs requires 
special scanning and processing techniques.54,55
•	The original scene is rendered. When pho-
tographic reproductions of original artwork are 
scanned, the original scene has to be rendered 
and the film characteristics have to be subtracted. 
However, this is only possible if a target is included 
on the film and the film characteristics and lighting 
conditions are known. It is therefore probably bet-
ter to scan the original artwork.
With current scanning and color management technol-
ogy, the first case, rendering the photographic image, can 
be automated if it is possible to match the dynamic range 
of the output to the original. All other cases need manual 
intervention, either in the initial scanning process or in 
subsequent image processing. Manual intervention is time-
consuming and requires highly skilled operators. As a result, 
production costs remain high for high-quality, visually 
pleasing digital images. Better automated image processing 
tools need to be developed to analyze raw sensor data and 
translate it to pictorially pleasing digital reproductions on 
defined output devices.
Figure 38. An underexposed slide might be cor-
rected during scanning to render the photogra-
pher’s intent.
Figure 37. Matching the appearance of the origi-
nal photograph will be the prime goal in many 
archives databases.

30  Digital Imaging for Photographic Collections: Foundations for Technical Standards
Choosing a Color Space
The most important attribute of a color space in an archi-
val environment is that it is well defined. Scanning for an 
image archive is different from scanning for commercial 
offset printing. When an image is scanned for archival 
purposes, the future use of the image is not known, nor 
are the technical options that will be available a few years 
from now. Will color profiles still be maintained or even 
used? An eight-bit-per-color scanning device output might 
be sufficient for visual representation on today’s output 
devices, but it might not capture all the tonal subtleties of 
the original. Operator judgments regarding color and con-
trast cannot be reversed in a 24-bit RGB color system. Any 
output mapping different from the archived image’s color 
space and gamma must be considered. On the other hand, 
saving raw scanner data of 12 or 16 bits per color with no 
tonal mapping can create problems for future output if the 
scanner characteristics are not well known and profiled.56
As an option, a transformation can be associated with 
the raw scanner data to define the pictorial intent that was 
chosen at the time of capture. However, there is currently 
no software available that allows one to define the render-
ing intent of the image in the scanner profile. Rendering 
intent is usually set during output mapping by the user. 
There is software available that allows the user to modify 
the scanner profile for individual images, and therefore to 
create “image profiles.” That process is as work-intensive 
as regular image editing with the scanner or image pro-
cessing software. It also assumes that the input profiles can 
and will be read by the operating system and application 
the image is used in, not just by current but also by future 
systems.
Archiving for each image both a raw sensor data file 
in high bit-depth and a calibrated RGB 24-bit file at high 
resolution is not an option for a lot of institutions, consid-
ering the number of digital images an archive can contain.
Because the output is not known at the time of ar-
chiving, it is best to stay as close as possible to the source, 
i.e., the scanning device. In addition, scanning devices 
should be spectrally well characterized, and the informa-
tion should be readily available from the manufacturers.
New Tools and Developments
Archiving 10- to 12-bit-per-channel standardized RGB 
color space would be optimal. Having to communicate 
International Color 
Consortium (ICC)
The ICC was established for the 
purpose of creating, promoting, 
and encouraging the standard-
ization and evolution of an open, 
vendor-neutral, cross-platform 
color management system. ICC 
standards will be important for 
the color fidelity of image da-
tabases, since ICC profiles are 
being used in some projects in the 
field. Currently, the ICC defines 
a profile format that contains 
mapping information from input 
or output device color space to a 
PCS (profile connection space). 
How this mapping is achieved is 
vendor-specific. As a result, mix-
ing profiles from different vendors 
for the same devices can result 
in different image reproduction. 
However, using profiles is only a 
temporary solution for archives. 
In the long term, other solutions 
are needed that are more open and 
that do not include any propri-
etary technology.

Building the Image Quality Framework  31
only one color space (or profile) to the user’s color man-
agement system would facilitate optimal rendering of all 
images across all platforms and devices. If the color space 
were standardized and universally recognized, the need to 
embed a profile into each image file would be eliminated. 
(Embedding profiles into each image file creates too great 
a data overhead when delivering preview files over the In-
ternet.) There would also be only one profile that needs to 
be updated when color management specifications evolve 
in the future. 
The sRGB color space proposed by Hewlett-Packard 
and Microsoft (or an extended version allowing the ac-
commodation of an unlimited gamut and out-of-gamut 
colors) is a viable color space choice for access files. It 
is sufficiently large to accommodate most photographic 
reproduction intents. Since the first future access to any 
file will most probably be some kind of a monitor using an 
RGB color space, choosing to keep the access data in the 
currently defined sRGB is a valid solution. Images in sRGB 
will display reasonably well even on uncalibrated moni-
tors. Higher bit-depth per channel would make it possible 
to communicate the predefined rendering intent for each 
image while leaving enough bit-depth for users to modify 
the image and to map to the intended output device. It 
would also give a safety factor to the archive file if future 
high-quality output devices require extensive mapping 
to as yet unknown color gamut and gamma. Also, colors 
that currently fall out of gamut could still be accounted for 
by leaving enough room on both ends of the values scale 
when defining black and white values. A standard way to 
deal with higher than eight-bit-per-channel image data 
across platforms and applications has to be developed.
It has been encouraging to see the development of 
high-quality tools destined for digital image applications 
other than prepress. However, there is still a need for more 
integrated systems to achieve a truly seamless, transpar-
ent work flow of images across platforms, devices, and, 
ultimately, time. No one imaging technology manufacturer 
will ever be able to dictate to the end-user which imag-
ing system to use. Additional standards will have to be 
developed to facilitate communication between imaging 
systems and to enable high-quality digital imaging for im-
age database applications.
The ultimate goal is a truly seam-
less, transparent work flow of 
images across platforms, devices, 
and, ultimately, time.

32  Digital Imaging for Photographic Collections: Foundations for Technical Standards
Image Artifacts
In addition to the four parameters described above, it is 
important to check for image artifacts such as drop-out 
lines, banding, etc. These artifacts can be consistent from 
image to image. There is not much that can be done about 
these types of artifacts, since they are introduced by the 
sensor or the network connection between the sensor and 
the CPU.
Setting Up Imaging Systems
A common problem when using different computer 
systems or monitors in an environment is the difference 
between the images when viewed on the various systems. 
Systems need to be set up and calibrated carefully. More 
often than not this is not done properly, leading to various 
problems. For example, even if systems are actually being 
calibrated, measurements may not be taken correctly.
Monitor Calibration
In many digitization projects monitor calibration is an im-
portant consideration, not only when working with images 
but also when discussing the quality of scans with vendors 
over the telephone. If monitors are not properly calibrated, 
the two parties will not see the same image. To solve this 
potential problem, the National Archives, for example, 
have the same system setup for their quality control station 
as their vendor.
If the monitor is the defined output device, as it is for 
many projects, it needs to be calibrated to a specific white 
point and gamma. A monitor’s gamma is a measure of the 
response curve of each of the red, green, and blue chan-
nels, from black to full intensity. Typical gamma values 
for color monitors range from 1.8 to 2.2. For PC systems 
the latter value is usually chosen; in Mac environments 
1.8 is the gamma value that is widely used. The white 
point of a monitor is the color of white produced when all 
three color channels are at full intensity. It is specified as a 
color temperature measured in Kelvin (with images getting 
bluer as their color temperatures rise). Typical for this type 
of application is a setting to 6500° Kelvin.57
There exist various calibration methods and tools that 
differ widely in complexity. Some application programs 
incorporate basic monitor calibration. Furthermore, there 
Monitor Viewing Conditions
ISO 3664 (Viewing Conditions—for 
Graphic Technology and Photogra-
phy) requires the following:
•  The chromaticity of the white 
displayed on the monitor should 
approximate that of D65. The lu-
minance level of the white displayed 
on the monitor shall be greater than 
75 cd/m2 and should be greater 
than 100 cd/m2.
•  When measured in any plane around 
the monitor or observer, the level of 
ambient illumination shall be less 
than 64 lux and should be less than 
32 lux. The color temperature of the 
ambient illumination shall be less 
than or equal to that of the monitor 
white point.
•  The area immediately surround-
ing the displayed image shall be 
neutral, preferably grey or black 
to minimize flare, and of approxi-
mately the same chromaticity as the 
white point of the monitor.
•  The monitor shall be situated so 
there are no strongly colored areas 
(including clothing) directly in the 
field of view or which may cause 
reflections in the monitor screen. 
Ideally all walls, floors, and fur-
niture in the field of view should 
be grey and free of any posters, 
notices, pictures, wording, or any 
other object which may affect the 
viewer’s vision.
•  All sources of glare should be avoided 
since they significantly degrade the 
quality of the image. The monitor 
shall be situated so that no illumi-
nation sources such as unshielded 
lamps or windows are directly in 
the field of view or are causing 
reflections from the surface of the 
monitor.

Building the Image Quality Framework  33
A few points to remember
•  The area used for scanning needs 
to be big enough to accommodate 
preparation of:
	
Images
	
Scanning
	
Quality control.
•  Critical points to be checked after 
scanning:
	
Sharpness
	
Correct file name
	
Laterally reversed images.
exist specific calibration programs. Depending on the need 
of the user, they can be very sophisticated and incorporate 
devices like photometers and colorimeters.
The best way to view a monitor is under dim illumi-
nation that has a lower correlated color temperature than 
the monitor. This reduces veiling glare, increases the moni-
tor dynamic range, and enables the human visual system 
to adapt to the monitor. This viewing condition results in 
the most aesthetically pleasing monitor images. Viewing 
originals and images on the screen side-by-side is more 
problematic, because in this case the observers are not al-
lowed to adapt to each “environment” individually.
Once calibrated, the monitor should need re-calibra-
tion only when conditions change, or on a monthly basis. 
It is a good idea to put a piece of tape over the monitor’s 
brightness and contrast controls after calibration and to 
maintain consistent lighting conditions.
Digital Master and Derivatives
It has been agreed upon in the preservation community 
that several files should be stored for every image to fulfill 
all requirements, mainly preservation and access. First, a 
so-called archive file containing more than eight bits per 
channel should be stored. It should not be treated for any 
specific output and should be uncompressed or lossless 
compressed. From the archive file, various access files 
can be produced as needed. These might be based on a 
particular use that defines tone reproduction, and color 
reproduction, and pictorial interpretation.
The highest quality file produced is referred to as 
the digital master. The quality level of the digital master 
will depend on the goals of the project, and in most cases 
its level of quality will be dictated by the project bud-
get. From this digital master several derivatives can be 
produced. Usually up to five different quality levels will be 
produced (see, for example, www.nara.gov). Again, this 
depends on the project.
Quality and Process Control
The best approach to digital image quality control includes, 
on one hand, subjective visual inspection and, on the other 
hand, objective measurements performed in software on 
the digital files themselves. Efforts should be made to 

34  Digital Imaging for Photographic Collections: Foundations for Technical Standards
standardize the procedures and equipment for subjective 
evaluations by means of monitor and printer calibration. 
For objective image quality measurement, software should 
be available which is designed to locate and evaluate spe-
cific targets and then report numbers or graphs describing 
key image quality parameters. Such software should ideally 
be a plug-in to a full-featured image browser so that all 
aspects of the image file (header info, index, and tracking 
data, etc.) can be reviewed at one time. Some software 
components already exist, others are currently being devel-
oped.
A key point is that targets and the software to evalu-
ate them are not just for checking systems—they serve to 
guarantee the long-term usefulness of the digital files and 
to protect the investments of the institution.
Since some of the targets and software described here 
are not yet commercially available, it will still be some time 
before consistent quality control can be  set up. Neverthe-
less, these tools will be available soon, and their inclusion in 
future work flow is recommended. 
Sample materials representative of the photographs in 
a collection to be scanned should always be included in the 
system tests. It is crucial that sample materials are a true 
representation of the collection. Taking these sample images 
through the whole processing chain will help to ascertain 
whether a particular quality level can be reached. Depend-
ing on the goals of the digitization project, this process may 
include working with outside vendors, printers, and the like.
Benchmarking Scanner Systems
Standards are currently being developed for benchmarking 
scanning systems. In most cases the test requirements will 
far exceed the necessary performance for actual scanning. 
Benchmarking systems will help to compare different hard-
ware, give more adequate information than that which is 
currently available from the manufacturers, and hopefully 
lead to a better understanding of the whole process.
Reproduction Qualities and Characteristics of the 
Digital Master and Derivatives
The reproduction qualities of digital images will have to 
be monitored during and controlled after their production. 
Hence, a quality-control step needs to be incorporated into 
every production step. This process serves to ensure that 

Building the Image Quality Framework  35
images actually reach the quality margins that have been 
set for the various parameters. 
Functional Qualities and Characteristics of the Digital 
Master and Derivatives
Besides tests for reproduction quality, additional tests are 
needed to make sure that the functionality of the digital 
master is inherent in the digital file. These include looking 
at file formats and performance, for example, always keep-
ing in mind the longevity of the data.
IPI recommends the use of standard file formats 
such as TIFF, which is thoroughly published. This ensures 
independence from proprietary formats that might one 
day be discontinued. New file formats like TIFF/EP,32 or 
something similar, might be more widely used in the future. 
TIFF/EP is based on TIFF 6.0 and uses a large number of 
tags to store additional information. JPEG2000 is another 
standard that should be considered. 
Documentation of the Imaging Process—Technical 
Metadata (Subcategory of Administrative Metadata)
To be able to work with images across platforms, as well as 
over time, it is important that the imaging process is well 
documented and that the information is kept with every 
file. The National Information Standards Organization  has 
published a standard called Technical Metadata for Digital 
Still Images.58 It represents a comprehensive list of techni-
cal metadata elements required to manage digital image 
collections.
Image Processing
Image quality is affected by the sequence of applying 
different image processing steps.59 It is important to be 
aware of the effects of different processing algorithms (e.g., 
resampling, sharpening and noise reduction, tone and color 
correction, defect elimination, and compression). It also has 
to be kept in mind that the future use of the images is not 
clear. Ideally, all image processing should be delayed until 
the time an image is actually used and image rendering 
and output characteristics are known. This would require 
the data to be stored with a bit-depth of more than eight 
bits per channel. Unfortunately, most work-flow solutions 
currently available do not allow this. 
Metadata: 
”Data about the Data”
•	 Discovery metadata for finding.
•	 Administrative metadata for view-
ing and monitoring.
•	 Structural metadata for naviga-
tion.
•	 Rights-management metadata for 
controlling access.

36  Digital Imaging for Photographic Collections: Foundations for Technical Standards
Processing for Archiving
Image data is best stored as raw capture data. Subsequent 
processing of this data can only reduce the information 
content, and there is always the possibility that better 
input processing algorithms will become available further 
on. The archived image data should therefore be the raw 
data, and, when possible, the associated information 
required for processing, such as sensor characteristics, il-
lumination, and linearization data.
Processing for Access
Processing for viewing is a type of output processing 
applied to produce images of good viewing quality. It is 
possible to design viewer software that can take image files 
that have undergone input processing and process them 
for output on a monitor.
Data Compression
Advances in image-data compression and storage-media 
development have helped to reduce the concern about 
storage space for large data files. Nevertheless, image com-
pression in an archival environment has to be evaluated 
very carefully. Because in this case the future use of a digi-
tal image is not yet determined, one copy of every image 
should be left uncompressed. Current lossless compression 
schemes do not bring too much in terms of reduction of 
storage space. Also, it should be remembered that the loss 
of one crucial bit could mean the loss of all of the file infor-
mation, even in the case of lossless compression.
New compression schemes, like wavelets, which do 
not produce the well-known artifacts that JPEG com-
pressed files show, are still not readily available.
Two of the most widely used compression schemes are 
briefly described below.60  
Lossless and Visually Lossless Compression
All good compression schemes sacrifice the least informa-
tion possible in achieving a reduced file size. Lossless com-
pression makes it possible to exactly reproduce the original 
image file from a compressed file. Lossless compression 
differs from visually lossless compression (compression 
where the artifacts are not visible). Although the human 
visual system provides guidelines for designing visually 
Processing for Monitor 
Viewing
•	 A linear distribution of the tones 
in a digital image compared to 
the density values of the original 
offers greater potential for future 
functionality, but images need to 
be adjusted before being viewed on 
a monitor.
•	 Adjusting master files for moni-
tor representation provides better 
viewing fidelity but means giving 
up certain processing possibilities 
in the future.
Data Compression
•	 For archiving: uncompressed
•	 For use: lossy

Building the Image Quality Framework  37
Figure 39. Even lightly compressed JPEG 
(4:1/5:1) can show some compression artifacts. 
For compression, the image is divided into 8 x 8 
pixel blocks; these blocks are a major source of 
visible artifacts. Therefore, the highest-quality 
master file should be archived using a lossless 
compression algorithm.
lossless compression schemes, ultimately the visibility of 
compression artifacts depends on the output.
•	LZW (Lossless Compression). LZW (Lempel-
Ziv-Welch) is a type of entropy-based encoding. 
It belongs to a class of lossless compression that 
is performed on a digital image file to produce 
a smaller file which nevertheless contains all the 
information of the original file. Currently, the most 
common schemes are those based on Huffman 
encoding and the proprietary LZW compression 
(used for TIFF files in Adobe Photoshop).
	      Unfortunately, the granular structure of film 
hinders effective entropy-based encoding. The film 
grain imposes a fine random noise pattern on the 
image that does not compress well. There is cur-
rently no effective lossless solution to this problem.
•	Photo CD (Visually Lossless Compression). 
The Photo CD compression scheme utilizes both 
frequency and color compression in an attempt to 
produce visually lossless compression.
Lossy Compression
JPEG stands for Joint Photographic Expert Group, 
which is the group responsible for the development of the 
compression approach named after it. JPEG is one type of 
lossy compression with a number of user-selectable options 
(Figure 39).
The advantages of JPEG compression are its user 
selectability to ensure visually lossless compression, high 
compression ratio, good computational efficiency, and 
good film grain suppression characteristics. Future devel-
opment proposed for the JPEG standard allow for tiling 
extensions, meaning that multiple-resolution versions of 
an image can be stored within the same file (similar to the 
concept behind the Photo CD files).
The concern that repeated JPEG compression causes 
deterioration of image quality is valid.61 Consequently, all 
image processing should occur before the file is com-
pressed, and the image should only be saved once using 
JPEG.

38  Digital Imaging for Photographic Collections: Foundations for Technical Standards
The Conference
The outcome of the IPI study was presented in a three-
day conference at RIT in June, 1997. During the course 
of the project we had the opportunity to build a network 
with a wide variety of individuals working in the field of 
digitization. The concept for the conference was to bring 
together experts from different backgrounds and to try to 
determine whether a dialogue was possible and desired by 
those in the field. Putting together the program was a very 
interesting process, and we were pleasantly surprised that 
all the speakers immediately agreed to come. We took this 
as an indication that the chosen approach was appreciated 
in the field.
The conference, “Digitizing Photographic Collec-
tions—Where Are We Now? What Does The Future Hold?” 
was held in the new CIMS building on the RIT campus. 
Over 120 people attended, a third of whom came from 
outside the US. The program was well received by both 
speakers and attendees. The topics presented during the 
three-day event are listed on page 39.

The Conference  39
INTRODUCTION
Digitizing Photographic Collections: the Big Picture—James Reilly and Franziska Frey, IPI
Funding Possibilities—Charles Kolb, NEH
IMAGE QUALITY
Introduction to Digital Image Quality and a Look at Tools to Control It—James Reilly and Franziska Frey, IPI
PROJECTS
Digitally Archiving Glass Plate Negatives—Vienna Wilson, University of Virginia
Adventures in Reformatting: A Hybrid Approach—Connie McCabe, Photo Preservation Services, Inc.
Digital Archive Solution for the US Holocaust Museum—Holly Frey, Applied Graphic Technologies
IMAGE INTERCHANGE AND MIGRATION—NEEDS AND TOOLS
FlashPix Image File Format—Chris Hauf, Kodak
Determining Conversion Requirements for Special Collections Material—Anne Kenney, Cornell University
The Museum Educational Site Licensing Project and Issues of Image Interchange and Metadata—Howard Besser, 
UC Berkeley
SHARING EXPERIENCES
Imaging Systems for the Creation of a Large Digital Archive—Sabine Süsstrunk, Corbis Corp.
Modern Times at the Picture Factory—Carl Fleischhauer and Phil Michel, Library of Congress
Technical Overview of the National Archives’ Electronic Access Project—Steve Puglia, National Archives
THE TECHNOLOGY—A CLOSER LOOK AT SOME DETAILS
Visible Human Images: Compression Techniques—George Thoma, National 
Library of Medicine
Scanning Technology: Now and the Future—Paul Norder, Xerox Corporation
Digital Color Printing Systems—Frank Cost, RIT
NATIONAL AND INTERNATIONAL TRENDS IN IMAGE DATABASE PROJECTS
Filling the Frame: Selected International Efforts in Photographic Digitization—
Nancy Elkington, Research Libraries Group
Digital Image Collections: Issues and Practice—Michael Ester, Luna Imaging
Digitizing Collections of Cultural Materials for Digital Libraries: Some Experi-
ences—Fred Mintzer, IBM
DIGITIZE TO PRESERVE
Imaging Investments: How to Negotiate for Quality and How 
to Measure It—Steve Chapman, Harvard University
Preservation in the Digital World—Paul Conway, Yale Univer-
sity
Meeting Preservation Priorities While Digitizing a Photograph-
ic Collection—Paul Messier, Boston Art Conservation
IMAGE QUALITY
Project Presentation: Images of African-Americans in 19-
Century Illustrations—Anthony Troncale, New York Public 
Library
PANEL DISCUSSION
Technical Issues and Foundations for Standards
Digitizing Photographic Collections 
June 7-9, 1997

40  Digital Imaging for Photographic Collections: Foundations for Technical Standards
Conclusions
With the advent of digital imaging, new approaches to 
preservation strategies are needed for photographic collec-
tions in libraries and archives. The preservation commu-
nity must be involved in designing these strategies. The 
practices that are being developed and used influence the 
development of the field as a whole. A great deal of exper-
tise has been accumulated through various projects.
However, there still remain questions on “how-to” 
that have to be solved, the sooner the better. Since budgets 
are limited, most of the scanning probably will be (and 
should be) done only once. If project outcomes are not 
satisfactory,  it will not only be disappointing for the 
participating team, it will also be considerably harder to 
get funding for a second project. We see more development 
needed in incorporating quality control into a work flow, 
solving color issues, and creating the right metadata to al-
low the files to be transferred to future systems.
The more deeply we became involved with image 
quality, the more we saw that the imaging process can-
not be viewed in a linear fashion. All the components are 
interdependent and therefore must be viewed as a whole. 
Nevertheless, understanding the components singly is the 
basis for understanding how they influence each other.
The many advantages of the emerging digital tech-
nologies for photographic collections are obvious, but there 
is still a long way to go. The technology is still young and 
in constant flux. Most importantly, communication among 
all participating parties must be improved. Due to the 
fast-changing and complex imaging technologies involved, 
collection managers need to work together with engineers 
and imaging scientists, who often lack collection-related 
knowledge. Each side must be willing to learn the special 
needs of the other. The conference that was organized 
within this project demonstrated this. This is perhaps the 
project’s most encouraging outcome for the preservation 
community. 
Finally, it must be remembered that imaging is about 
images. Visual sophistication is needed to successfully mas-
ter a digital imaging project—and this is clearly plentiful 
in the world of museums, archives, and libraries.

References  41
References
1	
I3A-WG18 Digital Photography, www.i3a.org/wg18.
html.
2	
J. Holm, “Survey of Developing Electronic Photography 
Standards,” Critical Reviews of Optical Science and 
Technology, SPIE, CR61, 1996, pp. 120–152.
3	
K. Donovan, “Anatomy of an Imaging Project,” Spectra, 
23 (2), 1995, pp. 19–22.
4	
R. Gschwind, L. Rosenthaler, and F. Frey, “The Use of 
Digital Imaging for the Preservation of Collections of 
Photographs and Motion Pictures,” Proceedings, ICOM 
11th Triennial Meeting, Edinburgh, Scotland, September 
1–6, 1996, pp. 8–15.
5	
J.-L. Bigourdan and J. M. Reilly, Environment and 
Enclosures in Film Preservation, Final Report to National 
Endowment for the Humanities, Grant #PS20802-
94, Image Permanence Institute, Rochester Institute of 
Technology, September 1997.
6	
J. Rothenberg, “Ensuring the Longevity of Digital Docu-
ments,” Scientific American, 272 (1), January 1995, pp. 
42–47.
7	
Preserving Digital Information, Report of the Task Force 
on Archiving of Digital Information (Washington, DC: 
Commission on Preservation and Access, May 1996).
8	
Time & Bits, Managing Digital Continuity, September 
1998, www.longnow.org/projects/conferences/time-and-
bits/background.
9	
P. McClung, ed., RLG Digital Image Access Project 
(Mountain View, CA: The Research Libraries Group, Inc., 
1995).
10	 L. Serenson Colet, K. Keller, and E. Landsberg, “Digi-
tizing Photographic Collections: A Case Study at the 
Museum of Modern Art, NY,” presented at the Electronic 
Imaging and the Visual Arts Conference, Paris, Septem-
ber 1997.
11	 S. Ostrow, Digitizing Historical Pictorial Collections for 
the Internet (Washington, DC: Council on Library and 
Information Resources, February 1998).
12	 A. Kenney and O. Rieger, Using Kodak Photo CD Tech-
nology for Preservation and Access, May 1998. www.
library.cornell.edu/preservation/kodak/cover.htm.
13	 ISO 16067-1, Electronic scanners for photographic 
images —Spatial resolution measurements: Part 1, 
Scanners for reflective media, 2003.
14	 F. Frey, “Digital Imaging for Photographic Collections: 
Foundations for Technical Standards,” RLG DigiNews, 1 
(3), December 1997, www.rlg.org/preserv/diginews/digi-
news3.html#com.
15	 F. Frey, “Digitization of Photographic Collections,” 
Proceedings, IS&T 50th Annual Conference, Boston, May 
1997, pp. 597–599.
16	 F. Frey, “Digitize to Preserve—Photographic Collections 
Facing the Next Millennium,” Proceedings, IS&T 50th 
Annual Conference, Boston, May 1997, pp. 713–715, 
1997.
17	 F. Frey, “Digitization of Photographic Collections,” Very 
High Resolution and Quality Imaging II, SPIE Proceed-
ings, 3025, February 1997, pp. 49–52.
18	 F. Frey and S. Süsstrunk, “Image Quality Issues for the 
Digitization of Photographic Collections,” Proceedings, 
IS&T 49th Annual Conference, Minneapolis, MN, May 
1996, pp. 349–353.
19	 L. Stroebel and R. Zakia, eds., The Focal Encyclopedia 
of Photography (Boston: Focal Press, 1993).
20	 G. A. Gescheider, Psychophysics—Method, Theory, and 
Application, 2nd ed. (London, Hillsdale, NJ: Lawrence 
Erlbaum Associates, 1985).
21	 Y. Kipman, “Image Quality Metrics for Digital Image 
Capture Devices,” Proceedings, IS&T 48th Annual Con-
ference, Washington, DC, May 1995, pp. 456–459.
22	 P. Engeldrum, “A Framework for Image Quality Mod-
els,” Journal of Imaging Science and Technology, 39 (4), 
1995, pp. 312–319.
23	 P. Engeldrum, Introduction to Image Quality, short 
course notes, IS&T 11th International Congress on Ad-
vances in Non-Impact Printing Technologies, November 
1995.
24	 R. Shaw, ed., Selected Readings in Image Evaluation 
(Washington, DC: Society of Photographic Scientists and 
Engineers, 1976).

42  Digital Imaging for Photographic Collections: Foundations for Technical Standards
25	 S. Herr, “Scanning for Gold,” Publish, December 1996, 
pp. 77–81.
26	 R. Gann and N. Shepard, Reviewing and Testing 
Desktop Scanners, (Greeley, Colorado: Hewlett Packard 
Company, 1997).
27	 R. D. Forkert, et al., Test Procedures for Verifying IAFIS 
Scanner Image Quality Requirements, MITRE Report for 
FBI Fingerprint Project, November 1994.
28	 A. Kenney and S. Chapman, Digital Resolution Require-
ments for Replacing Text-Based Material: Methods for 
Benchmarking Image Quality (Washington, DC: Com-
mission on Preservation and Access, 1995).
29	 ANSI/AIIM MS44-1988, Recommended Practice for 
Quality Control of Image Scanners (Silver Spring, MD: 
Association for Information and Image Management, 
1988).
30	 M. Ester, Digital Image Collections: Issues and Practice, 
(Washington, DC: Commission on Preservation and Ac-
cess, 1996).
31	 M. Ester, “Specifics of Imaging Practice,” Archives & 
Museum Informatics, 9, 1995, pp. 147–185.
32	 ISO 12234-2, Photography—Electronic Still Picture 
Cameras—Removable Memory—Part 2: TIFF/EP Image 
Data Format, 2001.
33	 Technical Guidelines for Digitizing Archival Materials 
for Electronic Access: Creation of Production Master 
Files—Raster Images, June 2004. www.archives.gov/re-
search/arc/digitizing-archival-materials.html.
34	 ISO 14524, Photography—Electronic Still Picture 
Cameras—Methods for Measuring Opto-Electronic 
Conversion Functions (OECFs), January 1999.
35	 ANSI/AIIM TR26-1993, Resolution as it Relates to Pho-
tographic and Electronic Imaging (Silver Spring, MD: 
Association for Information and Image Management, 
1993).
36	 P. Barten, MTF, CSF, and SQRI for Image Quality 
Analysis, short course notes, IS&T/SPIE Symposium 
on Electronic Imaging: Science & Technology, January 
1996.
37	 D. J. Braunegg, R. D. Forkert, and N. B. Nill, Rescal-
ing Digital Fingerprints: Techniques and Image Quality 
Effects, MITRE Report for FBI Fingerprint Project, June 
1995.
38	 M. A. Chambliss, J. A. Dawson, and E. J. Borg, “Measur-
ing the MTF of Undersampled IRFPA Sensors Using 2D 
Discrete Transforms,” SPIE Proceedings: Infrared Imag-
ing Systems, Design, Analysis, Modeling, and Testing VI, 
G. C. Holst, ed., 2470, January 1995, pp. 312–324.
39	 S. E. Reichenbach, et al., “Characterizing Digital Image 
Acquisition Devices,” Optical Engineering, 30 (2), 1991, 
pp. 170–177.
40	 H. S. Wong, “Effect of Knife-Edge Skew on Modulation 
Transfer Function Measurements of Charge-Coupled 
Device Imagers Employing a Scanning Knife Edge,” 
Optical Engineering, 30(9), September 1991, pp. 
1394–1398.
41	 D. Williams, “What is an MTF . . . and Why Should You 
Care?” RLG DigiNews, 2 (1), February 1998, www.rlg.
org/preserv/diginews/diginews21.html#technical.
42	 T. A. Fischer and J. Holm, “Electronic Still Picture 
Camera Spatial Frequency Response Measurement,” 
Proceedings, IS&T 47th Annual Conference, Rochester, 
NY, May 1994, pp. 626–630.
43	 R. Lamberts, “Use of Sinusoidal Test Patterns for MTF 
Evaluation,” technical information provided by Sine 
Patterns LLC, Rochester, NY.
44	 ISO 12233, “Photography—Electronic Still Picture 
Cameras—Resolution Measurements,” 2000.
45	 D. Williams, “Benchmarking of the ISO 12233 Slanted-
edge Spatial Frequency Response Plug-in, Proceedings, 
IS&T 1998 PICS Conference, Portland, OR, May 1998, 
pp. 133–137.
46	 A. Gelbart, Comparison of Sine Pattern and Knife Edge 
Techniques for Determining MTF of Digital Scanners, 
B. S. Thesis, Rochester Institute of Technology, February 
1997.
47	 N. B. Nill and B. R. Paine, A Computer Program to 
Determine the Sine Wave Modulation Transfer Function 
of Image Scanners, MITRE Report for FBI Fingerprint 
Project, September 1994.
48	 ISO 15739, Photography—Electronic Still Picture Imag-
ing—Noise Measurements, 2003.
49	 J. Holm, “Log NEQ Based Pictorial Print Noise Charac-
terization,” Proceedings, IS&T 47th Annual Conference, 
Rochester, NY, May 1994, pp. 429–432.
50	 J. Holm and S. Süsstrunk, “An EIQ-Subjective Image 

References  43
Quality Correlation Study,” Proceedings IS&T 47th 
Annual Conference, Rochester, NY, May 1994, pp. 
634–640.
51	 F. Frey and S. Süsstrunk, “Color Issues to Consider in 
Pictorial Image Data Bases,” Proceedings, IS&T Fifth 
Color Imaging Conference, Scottsdale, AZ, November 
1997, pp. 112–115.
52	 M. D. Fairchild, “Some Hidden Requirements for 
Device-Independent Color Imaging,” paper presented at 
Society for Information International Symposium, 1994.
53	 R. Poe, “Aesthetic Considerations in Tone and Color 
Management,” Proceedings, IS&T Third Color Imag-
ing Conference, Scottsdale, AZ, November 1995, pp. 
164–168.
54	 F. Frey, R. Gschwind, and L. Rosenthaler, “Electronic 
Imaging, a Tool for the Reconstruction of Faded Color 
Photographs and Motion Pictures,” Proceedings, IS&T 
Fourth Color Imaging Conference, November 1996, pp. 
39–44.
55	 F. Frey and R. Gschwind, “Mathematical Bleach-
ing Models for Photographic Three-Color Materials,” 
Journal of Imaging Science and Technology, 38 (6), 
November/December 1994, pp. 513–519.
56	 ISO 17321/WD, Graphic Technology and Photogra-
phy—Colour Characterisation of Digital Still Cameras 
(DSCs), Part 1, Stimuli, metrology, and test procedures; 
Part 2, Methods for determining transforms from raw 
DSC to scene-referred data, 2003.
57	 S. Süsstrunk, “Imaging Production Systems at Corbis 
Corporation”, RLG DigiNews, 2 (4), August 1998, www.
rlg.org/preserv/diginews/diginews2-4.html#technical.
58	 Data Dictionary—Technical Metadata for Digital Still 
Images, 2006, www.niso.org.
59	 J. Holm, “Factors to Consider in Pictorial Digital Image 
Processing,” Proceedings, IS&T 49th Annual Conference, 
Minneapolis, MN, May 1996, pp. 298–304.
60	 M. Rabbani, “Image Compression Fundamentals,” The 
Compression Experience: Proceedings of The Rochester 
Section of the SMPTE Tutorial, Rochester, NY, October 
28, 1995, pp. 7–24.
61	 H. Kinoshita and T. Yamamuro, “Effects of Repetitive 
JPEG Compressions with DCT Block Rearrangement 
Operation on Image Quality,” Journal of Imaging Science 
and Technology, 39 (6), November/December 1995, pp. 
546–558.

44  Digital Imaging for Photographic Collections: Foundations for Technical Standards
Bibliography
G. A. Baxes, Digital Image Processing (New York: John Wiley & 
Sons, Inc., 1994).
R. Berns and F. Frey, Direct Digital Capture of Cultural Heri-
tage–Benchmarking American Museum Practices and Defin-
ing Future Needs Direct Digital Capture of Cultural Heritage 
– Benchmarking American Museum Practices and Defining 
Future Needs, www.cis.rit.edu/museumSurvey/documents/
Benchmark_Final_Report_Web.pdf, August 2005.
C. Wayne Brown and B. J. Sheperd, Graphics File Formats, Refer-
ence and Guide (Upper Saddle River, NJ: Prentice Hall, 1995).
J. C. Dainty and R. Shaw, Image Science, Principles, Analysis, and 
Evaluation of Photographic-Type Imaging Processes (London: 
Academic Press, 1974).
R. G. Gann, Desktop Scanners (Upper Saddle River, NJ: Prentice 
Hall PTR, 1999).
D. Hazen, J. Horrell, and J. Merrill-Oldham, Selecting Research 
Collections for Digitization (Washington, DC: Commission on 
Preservation and Access, 1998).
J. Holm and N. Judge, “Electronic Photography at the NASA 
Langely Research Center,” Proceedings, IS&T 48th Annual 
Conference, Washington, DC, 1995, pp. 436–441.
H. Maitre, F. J. M. Schmitt, and J. Crettez, “High Quality Imaging 
in Museums from Theory to Practice,” Very High Resolution 
and Quality Imaging II, Proc. SPIE, 3025, 1997, pp. 30-39.
E. Murphy, A Review of Standards Defining Testing Procedures for 
Characterizing the Color and Spatial Quality of Digital Camer-
as Used to Image Cultural Heritage, www.cis.rit.edu/museum-
Survey/documents/StandardsReview_tp.PDF.
S. E. Ostrow, Digitizing Pictorial Collections for the Internet 
(Washington, DC: Commission on Preservation and Access, 
1998).
J. C. Russ, The Image Processing Handbook, 2nd ed. (Boca Raton, 
FL: CRC Press, 1995).
A. Smith, Why Digitize? (Washington, DC: Commission on Preser-
vation and Access, 1999).
C. Stevenson and P. McClung, eds., Delivering Digital Images­—
Cultural Heritage Resources for Education (Los Angeles, CA: 
Getty Information Institute, 1998).

Selected Internet Resources  45
Selected Internet 
Resources
Countless resources and references can be found on the In-
ternet. The following list is a selection to start with. (World 
Wide Web URLs confirmed as of February 1, 2006.)
Art Spectral Imaging 
art-si.org
Arts and Humanities Data Service 
www.ahds.ac.uk
Council on Library and Information Resources 
www.clir.org
Dublin Core 
dublincore.org
Electronic Still Picture Imaging 
www.i3a.org/it10.html
European Commission on Preservation and Access 
www.knaw.nl/ecpa
The International Color Consortium (ICC) 
www.color.org
ISO Standards 
webstore.ansi.org/ansidocstore/iso.asp
ISO Standards Tools 
www.i3a.org/pdf/resource_order_form.pdf
ISO Test Charts 
www.i3a.org/iso_test_charts.html
Library of Congress/American Memory—Technical Information  
memory.loc.gov/ammem/about/techIn.html
MCN—Museum Computer Network 
www.mcn.edu
The National Archives 
www.archives.gov
National Media Lab 
www.nml.org
RLG DigiNews 
www.rlg.org/preserv/diginews
The Visual Resources Association 
www.vraweb.org

The Image Permanence Institute
The Image Permanence Institute is an academic research laboratory located 
on the campus of the Rochester Institute of Technology in Rochester, New 
York. Since its founding in 1985, IPI’s mission has been research for the 
advancement of the permanence and preservation of imaging media and 
information resources. IPI is cosponsored by RIT and the Society for Imaging 
Science and Technology.
Research at IPI deals primarily with preservation of images and recorded 
information. IPI has achieved success with projects involving enclosure qual-
ity, silver image stability, decomposition of cellulosic plastic film supports, 
color dye fading, paper deterioration due to air pollutants, magnetic tape 
preservation, and environmental assessment and control. IPI is known for its 
accelerated-aging studies of photographic materials including acetate, nitrate, 
and polyester films, color dyes, and gelatin. These studies have underscored 
the strong role that environment plays in all modes of decay and the impor-
tance of managing storage for preservation.
Among the products and services developed by IPI are the Photographic 
Activity Test, a worldwide standard (ISO Standard 18916) for archival quality 
in photographic enclosures; A-D Strips, a test for vinegar syndrome in acetate 
film; the Preservation Environment Monitor®, a unique electronic data logger 
designed specifically for preservation use; Climate Notebook® environmental 
analysis software; and the cost-effective Environmental Analysis Service for 
libraries, archives, and museums.
IPI serves the preservation community not only through its research, 
products, and services, but also as a ready source of technical information. In 
addition, IPI provides technical and administrative support for the American 
National Standards Institute (ANSI) and the International Organization for 
Standardization (ISO) in the area of permanence and care of imaging media, 
including photographic materials, tape, and optical discs.


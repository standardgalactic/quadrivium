more information – www.cambridge.org/9781107005211


Physical Mathematics
Unique in its clarity, examples, and range, Physical Mathematics explains as
simply as possible the mathematics that graduate students and professional
physicists need in their courses and research. The author illustrates the mathe-
matics with numerous physical examples drawn from contemporary research.
In addition to basic subjects such as linear algebra, Fourier analysis, com-
plex variables, differential equations, and Bessel functions, this textbook covers
topics such as the singular-value decomposition, Lie algebras, the tensors and
forms of general relativity, the central limit theorem and Kolmogorov test of
statistics, the Monte Carlo methods of experimental and theoretical physics, the
renormalization group of condensed-matter physics, and the functional deriva-
tives and Feynman path integrals of quantum ﬁeld theory. Solutions to exercises
are available for instructors at www.cambridge.org/cahill
KEVIN CAHILL is Professor of Physics and Astronomy at the University
of New Mexico. He has done research at NIST, Saclay, Ecole Polytechnique,
Orsay, Harvard, NIH, LBL, and SLAC, and has worked in quantum optics,
quantum ﬁeld theory, lattice gauge theory, and biophysics. Physical Mathemat-
ics is based on courses taught by the author at the University of New Mexico
and at Fudan University in Shanghai.


Physical Mathematics
KEVIN CAHILL
University of New Mexico

C A M B R I D G E U N I V E R S I T Y P R E S S
Cambridge, New York, Melbourne, Madrid, Cape Town,
Singapore, São Paulo, Delhi, Mexico City
Cambridge University Press
The Edinburgh Building, Cambridge CB2 8RU, UK
Published in the United States of America by Cambridge University Press, New York
www.cambridge.org
Information on this title: www.cambridge.org/9781107005211
c⃝K. Cahill 2013
This publication is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without the written
permission of Cambridge University Press.
First published 2013
Printed and bound in the United Kingdom by the MPG Books Group
A catalog record for this publication is available from the British Library
Library of Congress Cataloging in Publication data
Cahill, Kevin, 1941–, author.
Physical mathematics / Kevin Cahill, University of New Mexico.
pages
cm
ISBN 978-1-107-00521-1 (hardback)
1. Mathematical physics.
I. Title.
QC20.C24
2012
530.15–dc23
2012036027
ISBN 978-1-107-00521-1 Hardback
Cambridge University Press has no responsibility for the persistence or
accuracy of URLs for external or third-party internet websites referred to
in this publication, and does not guarantee that any content on such
websites is, or will remain, accurate or appropriate.

For Ginette, Mike, Sean, Peter, Mia, and James,
and in honor of Muntadhar al-Zaidi.


Contents
Preface
page xvii
1
Linear algebra
1
1.1
Numbers
1
1.2
Arrays
2
1.3
Matrices
4
1.4
Vectors
7
1.5
Linear operators
9
1.6
Inner products
11
1.7
The Cauchy–Schwarz inequality
14
1.8
Linear independence and completeness
15
1.9
Dimension of a vector space
16
1.10
Orthonormal vectors
16
1.11
Outer products
18
1.12
Dirac notation
19
1.13
The adjoint of an operator
22
1.14
Self-adjoint or hermitian linear operators
23
1.15
Real, symmetric linear operators
23
1.16
Unitary operators
24
1.17
Hilbert space
25
1.18
Antiunitary, antilinear operators
26
1.19
Symmetry in quantum mechanics
26
1.20
Determinants
27
1.21
Systems of linear equations
34
1.22
Linear least squares
34
1.23
Lagrange multipliers
35
1.24
Eigenvectors
37
vii

CONTENTS
1.25
Eigenvectors of a square matrix
38
1.26
A matrix obeys its characteristic equation
41
1.27
Functions of matrices
43
1.28
Hermitian matrices
45
1.29
Normal matrices
50
1.30
Compatible normal matrices
52
1.31
The singular-value decomposition
55
1.32
The Moore–Penrose pseudoinverse
63
1.33
The rank of a matrix
65
1.34
Software
66
1.35
The tensor/direct product
66
1.36
Density operators
69
1.37
Correlation functions
69
Exercises
71
2
Fourier series
75
2.1
Complex Fourier series
75
2.2
The interval
77
2.3
Where to put the 2πs
77
2.4
Real Fourier series for real functions
79
2.5
Stretched intervals
83
2.6
Fourier series in several variables
84
2.7
How Fourier series converge
84
2.8
Quantum-mechanical examples
89
2.9
Dirac notation
96
2.10
Dirac’s delta function
97
2.11
The harmonic oscillator
101
2.12
Nonrelativistic strings
103
2.13
Periodic boundary conditions
103
Exercises
105
3
Fourier and Laplace transforms
108
3.1
The Fourier transform
108
3.2
The Fourier transform of a real function
111
3.3
Dirac, Parseval, and Poisson
112
3.4
Fourier derivatives and integrals
115
3.5
Fourier transforms in several dimensions
119
3.6
Convolutions
121
3.7
The Fourier transform of a convolution
123
3.8
Fourier transforms and Green’s functions
124
3.9
Laplace transforms
125
3.10
Derivatives and integrals of Laplace transforms
127
viii

CONTENTS
3.11
Laplace transforms and differential equations
128
3.12
Inversion of Laplace transforms
129
3.13
Application to differential equations
129
Exercises
134
4
Inﬁnite series
136
4.1
Convergence
136
4.2
Tests of convergence
137
4.3
Convergent series of functions
138
4.4
Power series
139
4.5
Factorials and the gamma function
141
4.6
Taylor series
145
4.7
Fourier series as power series
146
4.8
The binomial series and theorem
147
4.9
Logarithmic series
148
4.10
Dirichlet series and the zeta function
149
4.11
Bernoulli numbers and polynomials
151
4.12
Asymptotic series
152
4.13
Some electrostatic problems
154
4.14
Inﬁnite products
157
Exercises
158
5
Complex-variable theory
160
5.1
Analytic functions
160
5.2
Cauchy’s integral theorem
161
5.3
Cauchy’s integral formula
165
5.4
The Cauchy–Riemann conditions
169
5.5
Harmonic functions
170
5.6
Taylor series for analytic functions
171
5.7
Cauchy’s inequality
173
5.8
Liouville’s theorem
173
5.9
The fundamental theorem of algebra
174
5.10
Laurent series
174
5.11
Singularities
177
5.12
Analytic continuation
179
5.13
The calculus of residues
180
5.14
Ghost contours
182
5.15
Logarithms and cuts
193
5.16
Powers and roots
194
5.17
Conformal mapping
197
5.18
Cauchy’s principal value
198
5.19
Dispersion relations
205
ix

CONTENTS
5.20
Kramers–Kronig relations
207
5.21
Phase and group velocities
208
5.22
The method of steepest descent
210
5.23
The Abel–Plana formula and the Casimir effect
212
5.24
Applications to string theory
217
Exercises
219
6
Differential equations
223
6.1
Ordinary linear differential equations
223
6.2
Linear partial differential equations
225
6.3
Notation for derivatives
226
6.4
Gradient, divergence, and curl
228
6.5
Separable partial differential equations
230
6.6
Wave equations
233
6.7
First-order differential equations
235
6.8
Separable ﬁrst-order differential equations
235
6.9
Hidden separability
238
6.10
Exact ﬁrst-order differential equations
238
6.11
The meaning of exactness
240
6.12
Integrating factors
242
6.13
Homogeneous functions
243
6.14
The virial theorem
243
6.15
Homogeneous ﬁrst-order ordinary differential
equations
245
6.16
Linear ﬁrst-order ordinary differential equations
246
6.17
Systems of differential equations
248
6.18
Singular points of second-order ordinary differential
equations
250
6.19
Frobenius’s series solutions
251
6.20
Fuch’s theorem
253
6.21
Even and odd differential operators
254
6.22
Wronski’s determinant
255
6.23
A second solution
255
6.24
Why not three solutions?
257
6.25
Boundary conditions
258
6.26
A variational problem
259
6.27
Self-adjoint differential operators
260
6.28
Self-adjoint differential systems
262
6.29
Making operators formally self adjoint
264
6.30
Wronskians of self-adjoint operators
265
6.31
First-order self-adjoint differential operators
266
6.32
A constrained variational problem
267
x

CONTENTS
6.33
Eigenfunctions and eigenvalues of self-adjoint systems
273
6.34
Unboundedness of eigenvalues
275
6.35
Completeness of eigenfunctions
277
6.36
The inequalities of Bessel and Schwarz
284
6.37
Green’s functions
284
6.38
Eigenfunctions and Green’s functions
287
6.39
Green’s functions in one dimension
288
6.40
Nonlinear differential equations
289
Exercises
293
7
Integral equations
296
7.1
Fredholm integral equations
297
7.2
Volterra integral equations
297
7.3
Implications of linearity
298
7.4
Numerical solutions
299
7.5
Integral transformations
301
Exercises
304
8
Legendre functions
305
8.1
The Legendre polynomials
305
8.2
The Rodrigues formula
306
8.3
The generating function
308
8.4
Legendre’s differential equation
309
8.5
Recurrence relations
311
8.6
Special values of Legendre’s polynomials
312
8.7
Schlaeﬂi’s integral
313
8.8
Orthogonal polynomials
313
8.9
The azimuthally symmetric Laplacian
315
8.10
Laplacian in two dimensions
316
8.11
The Laplacian in spherical coordinates
317
8.12
The associated Legendre functions/polynomials
317
8.13
Spherical harmonics
319
Exercises
323
9
Bessel functions
325
9.1
Bessel functions of the ﬁrst kind
325
9.2
Spherical Bessel functions of the ﬁrst kind
335
9.3
Bessel functions of the second kind
341
9.4
Spherical Bessel functions of the second kind
343
Further reading
345
Exercises
345
xi

CONTENTS
10
Group theory
348
10.1
What is a group?
348
10.2
Representations of groups
350
10.3
Representations acting in Hilbert space
351
10.4
Subgroups
353
10.5
Cosets
354
10.6
Morphisms
354
10.7
Schur’s lemma
355
10.8
Characters
356
10.9
Tensor products
357
10.10 Finite groups
358
10.11 The regular representation
359
10.12 Properties of ﬁnite groups
360
10.13 Permutations
360
10.14 Compact and noncompact Lie groups
361
10.15 Lie algebra
361
10.16 The rotation group
366
10.17 The Lie algebra and representations of SU(2)
368
10.18 The deﬁning representation of SU(2)
371
10.19 The Jacobi identity
374
10.20 The adjoint representation
374
10.21 Casimir operators
375
10.22 Tensor operators for the rotation group
376
10.23 Simple and semisimple Lie algebras
376
10.24 SU(3)
377
10.25 SU(3) and quarks
378
10.26 Cartan subalgebra
379
10.27 Quaternions
379
10.28 The symplectic group Sp (2n)
381
10.29 Compact simple Lie groups
383
10.30 Group integration
384
10.31 The Lorentz group
386
10.32 Two-dimensional representations of the Lorentz group
389
10.33 The Dirac representation of the Lorentz group
393
10.34 The Poincaré group
395
Further reading
396
Exercises
397
11
Tensors and local symmetries
400
11.1
Points and coordinates
400
11.2
Scalars
401
11.3
Contravariant vectors
401
xii

CONTENTS
11.4
Covariant vectors
402
11.5
Euclidean space in euclidean coordinates
402
11.6
Summation conventions
404
11.7
Minkowski space
405
11.8
Lorentz transformations
407
11.9
Special relativity
408
11.10 Kinematics
410
11.11 Electrodynamics
411
11.12 Tensors
414
11.13 Differential forms
416
11.14 Tensor equations
419
11.15 The quotient theorem
420
11.16 The metric tensor
421
11.17 A basic axiom
422
11.18 The contravariant metric tensor
422
11.19 Raising and lowering indices
423
11.20 Orthogonal coordinates in euclidean n-space
423
11.21 Polar coordinates
424
11.22 Cylindrical coordinates
425
11.23 Spherical coordinates
425
11.24 The gradient of a scalar ﬁeld
426
11.25 Levi-Civita’s tensor
427
11.26 The Hodge star
428
11.27 Derivatives and afﬁne connections
431
11.28 Parallel transport
433
11.29 Notations for derivatives
433
11.30 Covariant derivatives
434
11.31 The covariant curl
435
11.32 Covariant derivatives and antisymmetry
436
11.33 Afﬁne connection and metric tensor
436
11.34 Covariant derivative of the metric tensor
437
11.35 Divergence of a contravariant vector
438
11.36 The covariant Laplacian
441
11.37 The principle of stationary action
443
11.38 A particle in a gravitational ﬁeld
446
11.39 The principle of equivalence
447
11.40 Weak, static gravitational ﬁelds
449
11.41 Gravitational time dilation
449
11.42 Curvature
451
11.43 Einstein’s equations
453
11.44 The action of general relativity
455
11.45 Standard form
455
xiii

CONTENTS
11.46 Schwarzschild’s solution
456
11.47 Black holes
456
11.48 Cosmology
457
11.49 Model cosmologies
463
11.50 Yang–Mills theory
469
11.51 Gauge theory and vectors
471
11.52 Geometry
474
Further reading
475
Exercises
475
12
Forms
479
12.1
Exterior forms
479
12.2
Differential forms
481
12.3
Exterior differentiation
486
12.4
Integration of forms
491
12.5
Are closed forms exact?
496
12.6
Complex differential forms
498
12.7
Frobenius’s theorem
499
Further reading
500
Exercises
500
13
Probability and statistics
502
13.1
Probability and Thomas Bayes
502
13.2
Mean and variance
505
13.3
The binomial distribution
508
13.4
The Poisson distribution
511
13.5
The Gaussian distribution
512
13.6
The error function erf
515
13.7
The Maxwell–Boltzmann distribution
518
13.8
Diffusion
519
13.9
Langevin’s theory of brownian motion
520
13.10 The Einstein–Nernst relation
523
13.11 Fluctuation and dissipation
524
13.12 Characteristic and moment-generating functions
528
13.13 Fat tails
530
13.14 The central limit theorem and Jarl Lindeberg
532
13.15 Random-number generators
537
13.16 Illustration of the central limit theorem
538
13.17 Measurements, estimators, and Friedrich Bessel
543
13.18 Information and Ronald Fisher
546
13.19 Maximum likelihood
550
13.20 Karl Pearson’s chi-squared statistic
551
xiv

CONTENTS
13.21 Kolmogorov’s test
554
Further reading
560
Exercises
560
14
Monte Carlo methods
563
14.1
The Monte Carlo method
563
14.2
Numerical integration
563
14.3
Applications to experiments
566
14.4
Statistical mechanics
572
14.5
Solving arbitrary problems
575
14.6
Evolution
576
Further reading
577
Exercises
577
15
Functional derivatives
578
15.1
Functionals
578
15.2
Functional derivatives
578
15.3
Higher-order functional derivatives
581
15.4
Functional Taylor series
582
15.5
Functional differential equations
583
Exercises
585
16
Path integrals
586
16.1
Path integrals and classical physics
586
16.2
Gaussian integrals
586
16.3
Path integrals in imaginary time
588
16.4
Path integrals in real time
590
16.5
Path integral for a free particle
593
16.6
Free particle in imaginary time
595
16.7
Harmonic oscillator in real time
595
16.8
Harmonic oscillator in imaginary time
597
16.9
Euclidean correlation functions
599
16.10 Finite-temperature ﬁeld theory
600
16.11 Real-time ﬁeld theory
603
16.12 Perturbation theory
605
16.13 Application to quantum electrodynamics
609
16.14 Fermionic path integrals
613
16.15 Application to nonabelian gauge theories
619
16.16 The Faddeev–Popov trick
620
16.17 Ghosts
622
Further reading
624
Exercises
624
xv

CONTENTS
17
The renormalization group
626
17.1
The renormalization group in quantum ﬁeld theory
626
17.2
The renormalization group in lattice ﬁeld theory
630
17.3
The renormalization group in condensed-matter physics
632
Exercises
634
18
Chaos and fractals
635
18.1
Chaos
635
18.2
Attractors
639
18.3
Fractals
639
Further reading
642
Exercises
642
19
Strings
643
19.1
The inﬁnities of quantum ﬁeld theory
643
19.2
The Nambu–Goto string action
643
19.3
Regge trajectories
646
19.4
Quantized strings
647
19.5
D-branes
647
19.6
String–string scattering
648
19.7
Riemann surfaces and moduli
649
Further reading
650
Exercises
650
References
651
Index
656
xvi

Preface
To the students: you will ﬁnd some physics crammed in amongst the mathemat-
ics. Don’t let the physics bother you. As you study the math, you’ll learn some
physics without extra effort. The physics is a freebie. I have tried to explain the
math you need for physics and have left out the rest.
To the professors: the book is for students who also are taking mechanics,
electrodynamics, quantum mechanics, and statistical mechanics nearly simulta-
neously and who soon may use probability or path integrals in their research.
Linear algebra and Fourier analysis are the keys to physics, so the book starts
with them, but you may prefer to skip the algebra or postpone the Fourier
analysis. The book is intended to support a one- or two-semester course for
graduate students or advanced undergraduates. The ﬁrst seven, eight, or nine
chapters ﬁt in one semester, the others in a second. A list of errata is main-
tained at panda.unm.edu/cahill, and solutions to all the exercises are available
for instructors at www.cambridge.org/cahill.
Several friends – Susan Atlas, Bernard Becker, Steven Boyd, Robert Bur-
ckel, Sean Cahill, Colston Chandler, Vageli Coutsias, David Dunlap, Daniel
Finley, Franco Giuliani, Roy Glauber, Pablo Gondolo, Igor Gorelov, Jiaxing
Hong, Fang Huang, Dinesh Loomba, Yin Luo, Lei Ma, Michael Malik, Kent
Morrison, Sudhakar Prasad, Randy Reeder, Dmitri Sergatskov, and David
Waxman – have given me valuable advice. Students have helped with questions,
ideas, and corrections, especially Thomas Beechem, Marie Cahill, Chris Cesare,
Yihong Cheng, Charles Cherqui, Robert Cordwell, Amo-Kwao Godwin, Aram
Gragossian, Aaron Hankin, Kangbo Hao, Tiffany Hayes, Yiran Hu, Shanshan
Huang, Tyler Keating, Joshua Koch, Zilong Li, Miao Lin, ZuMou Lin, Sheng
Liu, Yue Liu, Ben Oliker, Boleszek Osinski, Ravi Raghunathan, Akash Rakho-
lia, Xingyue Tian, Toby Tolley, Jiqun Tu, Christopher Vergien, Weizhen Wang,
George Wendelberger, Xukun Xu, Huimin Yang, Zhou Yang, Daniel Young,
Mengzhen Zhang, Lu Zheng, Lingjun Zhou, and Daniel Zirzow.
xvii


1
Linear algebra
1.1 Numbers
The natural numbers are the positive integers and zero. Rational numbers are
ratios of integers. Irrational numbers have decimal digits dn
x =
∞

n=mx
dn
10n
(1.1)
that do not repeat. Thus the repeating decimals 1/2 = 0.50000 . . . and 1/3 =
0.¯3 ≡0.33333 . . . are rational, while π = 3.141592654 . . . is irrational. Deci-
mal arithmetic was invented in India over 1500 years ago but was not widely
adopted in the Europe until the seventeenth century.
The real numbers R include the rational numbers and the irrational numbers;
they correspond to all the points on an inﬁnite line called the real line.
The complex numbers C are the real numbers with one new number i whose
square is −1. A complex number z is a linear combination of a real number x
and a real multiple i y of i
z = x + iy.
(1.2)
Here x = Rez is the real part of z, and y = Imz is its imaginary part. One adds
complex numbers by adding their real and imaginary parts
z1 + z2 = x1 + iy1 + x2 + iy2 = x1 + x2 + i(y1 + y2).
(1.3)
Since i2 = −1, the product of two complex numbers is
z1z2 = (x1 + iy1)(x2 + iy2) = x1x2 −y1y2 + i(x1y2 + y1x2).
(1.4)
The polar representation z = r exp(iθ) of z = x + iy is
z = x + iy = reiθ = r(cos θ + i sin θ)
(1.5)
1

LINEAR ALGEBRA
in which r is the modulus or absolute value of z
r = |z| =

x2 + y2
(1.6)
and θ is its phase or argument
θ = arctan (y/x).
(1.7)
Since exp(2πi) = 1, there is an inevitable ambiguity in the deﬁnition of
the phase of any complex number: for any integer n, the phase θ + 2πn gives
the same z as θ. In various computer languages, the function atan2(y, x) returns
the angle θ in the interval −π < θ ≤π for which (x, y) = r(cos θ, sin θ).
There are two common notations z∗and ¯z for the complex conjugate of a
complex number z = x + iy
z∗= ¯z = x −iy.
(1.8)
The square of the modulus of a complex number z = x + iy is
|z|2 = x2 + y2 = (x + iy)(x −iy) = ¯zz = z∗z.
(1.9)
The inverse of a complex number z = x + iy is
z−1 = (x + iy)−1 =
x −iy
(x −iy)(x + iy) = x −iy
x2 + y2 = z∗
z∗z = z∗
|z|2 .
(1.10)
Grassmann numbers θi are anticommuting numbers, that is, the anti-
commutator of any two Grassmann numbers vanishes
{θi, θj} ≡[θi, θj]+ ≡θiθj + θjθi = 0.
(1.11)
So the square of any Grassmann number is zero, θ2
i = 0. We won’t use these
numbers until chapter 16, but they do have amusing properties. The highest
monomial in N Grassmann numbers θi is the product θ1θ2 . . . θN. So the most
complicated power series in two Grassmann numbers is just
f (θ1, θ2) = f0 + f1 θ1 + f2 θ2 + f12 θ1θ2
(1.12)
(Hermann Grassmann, 1809–1877).
1.2 Arrays
An array is an ordered set of numbers. Arrays play big roles in computer science,
physics, and mathematics. They can be of any (integral) dimension.
A one-dimensional array (a1, a2, . . . , an) is variously called an n-tuple, a row
vector when written horizontally, a column vector when written vertically, or an
n-vector. The numbers ak are its entries or components.
A two-dimensional array aik with i running from 1 to n and k from 1 to m is
an n × m matrix. The numbers aik are its entries, elements, or matrix elements.
2

1.2 ARRAYS
One can think of a matrix as a stack of row vectors or as a queue of column
vectors. The entry aik is in the ith row and the kth column.
One can add together arrays of the same dimension and shape by adding
their entries. Two n-tuples add as
(a1, . . . , an) + (b1, . . . , bn) = (a1 + b1, . . . , an + bn)
(1.13)
and two n × m matrices a and b add as
(a + b)ik = aik + bik.
(1.14)
One can multiply arrays by numbers. Thus z times the three-dimensional
array aijk is the array with entries z aijk. One can multiply two arrays together
no matter what their shapes and dimensions. The outer product of an n-tuple a
and an m-tuple b is an n × m matrix with elements
(a b)ik = ai bk
(1.15)
or an m × n matrix with entries (ba)ki = bkai. If a and b are complex, then one
also can form the outer products (a b)ik = ai bk, (b a)ki = bk ai, and (b a)ki =
bk ai. The outer product of a matrix aik and a three-dimensional array bjℓm is a
ﬁve-dimensional array
(a b)ikjℓm = aik bjℓm.
(1.16)
An inner product is possible when two arrays are of the same size in one of
their dimensions. Thus the inner product (a, b) ≡⟨a|b⟩or dot-product a · b of
two real n-tuples a and b is
(a, b) = ⟨a|b⟩= a · b = (a1, . . . , an) · (b1, . . . , bn) = a1b1 + · · · + anbn.
(1.17)
The inner product of two complex n-tuples often is deﬁned as
(a, b) = ⟨a|b⟩= a · b = (a1, . . . , an) · (b1, . . . , bn) = a1 b1 + · · · + an bn
(1.18)
or as its complex conjugate
(a, b)∗= ⟨a|b⟩∗= (a · b)∗= (b, a) = ⟨b|a⟩= b · a
(1.19)
so that the inner product of a vector with itself is nonnegative (a, a) ≥0.
The product of an m×n matrix aik times an n-tuple bk is the m-tuple b′ whose
ith component is
b′
i = ai1b1 + ai2b2 + · · · + ainbn =
n

k=1
aikbk.
(1.20)
This product is b′ = a b in matrix notation.
If the size n of the second dimension of a matrix a matches that of the ﬁrst
dimension of a matrix b, then their product a b is a matrix with entries
(a b)iℓ= ai1 b1ℓ+ · · · + ain bnℓ.
(1.21)
3

LINEAR ALGEBRA
1.3 Matrices
Apart from n-tuples, the most important arrays in linear algebra are the two-
dimensional arrays called matrices.
The trace of an n × n matrix a is the sum of its diagonal elements
Tr a = tr a = a11 + a22 + · · · + ann =
n

i=1
aii.
(1.22)
The trace of two matrices is independent of their order
Tr (a b) =
n

i=1
n

k=1
aikbki =
n

k=1
n

i=1
bkiaik = Tr (ba)
(1.23)
as long as the matrix elements are numbers that commute with each other. It
follows that the trace is cyclic
Tr (a b . . . z) = Tr (b . . . z a) .
(1.24)
The transpose of an n × ℓmatrix a is an ℓ× n matrix aT with entries

aT
ij = aji.
(1.25)
Some mathematicians use a prime to mean transpose, as in a′ = aT, but physi-
cists tend to use primes to label different objects or to indicate differentiation.
One may show that
(a b) T = bT aT.
(1.26)
A matrix that is equal to its transpose
a = aT
(1.27)
is symmetric.
The (hermitian) adjoint of a matrix is the complex conjugate of its transpose
(Charles Hermite, 1822–1901). That is, the (hermitian) adjoint a† of an N × L
complex matrix a is the L × N matrix with entries
(a†)ij = (aji)∗= a∗
ji.
(1.28)
One may show that
(a b)† = b† a†.
(1.29)
A matrix that is equal to its adjoint
(a†)ij = (aji)∗= a∗
ji = aij
(1.30)
4

1.3 MATRICES
(and which must be a square matrix) is hermitian or self adjoint
a = a†.
(1.31)
Example 1.1 (The Pauli matrices)
σ1 =
0
1
1
0

,
σ2 =
0
−i
i
0

,
and
σ3 =
1
0
0
−1

(1.32)
are all hermitian (Wolfgang Pauli, 1900–1958).
A real hermitian matrix is symmetric. If a matrix a is hermitian, then the
quadratic form
⟨v|a|v⟩=
N

i=1
N

j=1
v∗
i aijvj ∈R
(1.33)
is real for all complex n-tuples v.
The Kronecker delta δik is deﬁned to be unity if i = k and zero if
i ̸= k (Leopold Kronecker, 1823–1891). The identity matrix I has entries
Iik = δik.
The inverse a−1 of an n × n matrix a is a square matrix that satisﬁes
a−1 a = a a−1 = I
(1.34)
in which I is the n × n identity matrix.
So far we have been writing n-tuples and matrices and their elements with
lower-case letters. It is equally common to use capital letters, and we will do so
for the rest of this section.
A matrix U whose adjoint U† is its inverse
U†U = UU† = I
(1.35)
is unitary. Unitary matrices are square.
A real unitary matrix O is orthogonal and obeys the rule
OTO = OOT = I.
(1.36)
Orthogonal matrices are square.
An N × N hermitian matrix A is nonnegative
A ≥0
(1.37)
if for all complex vectors V the quadratic form
⟨V|A|V⟩=
N

i=1
N

j=1
V∗
i AijVj ≥0
(1.38)
5

LINEAR ALGEBRA
is nonnegative. It is positive or positive deﬁnite if
⟨V|A|V⟩> 0
(1.39)
for all nonzero vectors |V⟩.
Example 1.2 (Kinds of positivity)
The nonsymmetric, nonhermitian 2 × 2
matrix
 1
1
−1
1

(1.40)
is positive on the space of all real 2-vectors but not on the space of all complex
2-vectors.
Example 1.3 (Representations of imaginary and Grassmann numbers)
The
2 × 2 matrix
0
−1
1
0

(1.41)
can represent the number i since
0
−1
1
0
 0
−1
1
0

=
−1
0
0
−1

= −I.
(1.42)
The 2 × 2 matrix
0
0
1
0

(1.43)
can represent a Grassmann number since
0
0
1
0
 0
0
1
0

=
0
0
0
0

= 0.
(1.44)
To represent two Grassmann numbers, one needs 4 × 4 matrices, such as
θ1 =
⎛
⎜⎜⎝
0
0
1
0
0
0
0
−1
0
0
0
0
0
0
0
0
⎞
⎟⎟⎠
and
θ2 =
⎛
⎜⎜⎝
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
⎞
⎟⎟⎠.
(1.45)
The matrices that represent n Grassmann numbers are 2n × 2n.
Example 1.4 (Fermions)
The matrices (1.45) also can represent lowering or
annihilation operators for a system of two fermionic states. For a1
= θ1
and a2 = θ2 and their adjoints a†
1 and a†
2, the creation operators satisfy the
anticommutation relations
{ai, a†
k} = δik
and
{ai, ak} = {a†
i , a†
k} = 0
(1.46)
6

1.4 VECTORS
where i and k take the values 1 or 2. In particular, the relation (a†
i )2 = 0 imple-
ments Pauli’s exclusion principle, the rule that no state of a fermion can be doubly
occupied.
1.4 Vectors
Vectors are things that can be multiplied by numbers and added together to
form other vectors in the same vector space. So if U and V are vectors in a
vector space S over a set F of numbers x and y and so forth, then
W = x U + y V
(1.47)
also is a vector in the vector space S.
A basis for a vector space S is a set of vectors Bk for k = 1, . . . , N in terms
of which every vector U in S can be expressed as a linear combination
U = u1B1 + u2B2 + · · · + uNBN
(1.48)
with numbers uk in F. The numbers uk are the components of the vector U in
the basis Bk.
Example 1.5 (Hardware store)
Suppose the vector W represents a certain kind
of washer and the vector N represents a certain kind of nail. Then if n and m are
natural numbers, the vector
H = nW + mN
(1.49)
would represent a possible inventory of a very simple hardware store. The vector
space of all such vectors H would include all possible inventories of the store.
That space is a two-dimensional vector space over the natural numbers, and the
two vectors W and N form a basis for it.
Example 1.6 (Complex numbers)
The complex numbers are a vector space.
Two of its vectors are the number 1 and the number i; the vector space of
complex numbers is then the set of all linear combinations
z = x1 + yi = x + iy.
(1.50)
So the complex numbers are a two-dimensional vector space over the real
numbers, and the vectors 1 and i are a basis for it.
The complex numbers also form a one-dimensional vector space over the
complex numbers. Here any nonzero real or complex number, for instance the
number 1, can be a basis consisting of the single vector 1. This one-dimensional
vector space is the set of all z = z1 for arbitrary complex z.
7

LINEAR ALGEBRA
Example 1.7 (2-space)
Ordinary ﬂat two-dimensional space is the set of all
linear combinations
r = xˆx + yˆy
(1.51)
in which x and y are real numbers and ˆx and ˆy are perpendicular vectors of unit
length (unit vectors). This vector space, called R2, is a 2-d space over the reals.
Note that the same vector r can be described either by the basis vectors ˆx and
ˆy or by any other set of basis vectors, such as −ˆy and ˆx
r = xˆx + yˆy = −y(−ˆy) + xˆx.
(1.52)
So the components of the vector r are (x, y) in the
ˆx, ˆy

basis and (−y, x) in the

−ˆy, ˆx

basis. Each vector is unique, but its components depend upon the basis.
Example 1.8 (3-space)
Ordinary ﬂat three-dimensional space is the set of all
linear combinations
r = xˆx + yˆy + zˆz
(1.53)
in which x, y, and z are real numbers. It is a 3-d space over the reals.
Example 1.9 (Matrices)
Arrays of a given dimension and size can be added
and multiplied by numbers, and so they form a vector space. For instance, all
complex three-dimensional arrays aijk in which 1 ≤i ≤3, 1 ≤j ≤4, and
1 ≤k ≤5 form a vector space over the complex numbers.
Example 1.10 (Partial derivatives)
Derivatives are vectors, so are partial deriva-
tives. For instance, the linear combinations of x and y partial derivatives taken
at x = y = 0
a ∂
∂x + b ∂
∂y
(1.54)
form a vector space.
Example 1.11 (Functions)
The space of all linear combinations of a set of
functions fi(x) deﬁned on an interval [a, b]
f (x) =

i
zi fi(x)
(1.55)
is a vector space over the natural, real, or complex numbers {zi}.
Example 1.12 (States)
In quantum mechanics, a state is represented by a vec-
tor, often written as ψ or in Dirac’s notation as |ψ⟩. If c1 and c2 are complex
numbers, and |ψ1⟩and |ψ2⟩are any two states, then the linear combination
|ψ⟩= c1|ψ1⟩+ c2|ψ2⟩
(1.56)
also is a possible state of the system.
8

1.5 LINEAR OPERATORS
1.5 Linear operators
A linear operator A maps each vector U in its domain into a vector U′ = A(U) ≡
AU in its range in a way that is linear. So if U and V are two vectors in its
domain and b and c are numbers, then
A(bU + cV) = bA(U) + cA(V) = bAU + cAV.
(1.57)
If the domain and the range are the same vector space S, then A maps each
basis vector Bi of S into a linear combination of the basis vectors Bk
ABi = a1iB1 + a2iB2 + · · · + aNiBN =
N

k=1
aki Bk.
(1.58)
The square matrix aki represents the linear operator A in the Bk basis. The effect
of A on any vector U = u1B1 + u2B2 + · · · + uNBN in S then is
AU = A
 N

i=1
uiBi

=
N

i=1
uiABi =
N

i=1
ui
N

k=1
aki Bk
=
N

k=1
 N

i=1
akiui

Bk.
(1.59)
So the kth component u′
k of the vector U′ = AU is
u′
k = ak1u1 + ak2u2 + · · · + akNuN =
N

i=1
aki ui.
(1.60)
Thus the column vector u′ of the components u′
k of the vector U′ = AU is
the product u′ = a u of the matrix with elements aki that represents the linear
operator A in the Bk basis and the column vector with components ui that rep-
resents the vector U in that basis. So in each basis, vectors and linear operators
are represented by column vectors and matrices.
Each linear operator is unique, but its matrix depends upon the basis. If we
change from the Bk basis to another basis B′
k
Bk =
N

ℓ=1
uℓk B′
ℓ
(1.61)
in which the N × N matrix uℓk has an inverse matrix u−1
ki so that
N

k=1
u−1
ki Bk =
N

k=1
u−1
ki
N

ℓ=1
uℓkB′
ℓ=
N

ℓ=1
 N

k=1
uℓku−1
ki

B′
ℓ=
N

ℓ=1
δℓiB′
ℓ= B′
i,
(1.62)
9

LINEAR ALGEBRA
then the new basis vectors B′
i are given by
B′
i =
N

k=1
u−1
ki Bk.
(1.63)
Thus (exercise 1.9) the linear operator A maps the basis vector B′
i to
AB′
i =
N

k=1
u−1
ki ABk =
N

j,k=1
u−1
ki ajkBj =
N

j,k,ℓ=1
uℓjajku−1
ki B′
ℓ.
(1.64)
So the matrix a′ that represents A in the B′ basis is related to the matrix a that
represents it in the B basis by a similarity transformation
a′
ℓi =
N

jk=1
uℓjajku−1
ki
or
a′ = u a u−1
(1.65)
in matrix notation.
Example 1.13 (Change of basis)
Let the action of the linear operator A on the
basis vectors {B1, B2} be AB1 = B2 and AB2 = 0. If the column vectors
b1 =
1
0

and
b2 =
0
1

(1.66)
represent the basis vectors B1 and B2, then the matrix
a =
0
0
1
0

(1.67)
represents the linear operator A. But if we use the basis vectors
B′
1 =
1
√
2
(B1 + B2)
and
B′
2 =
1
√
2
(B1 −B2)
(1.68)
then the vectors
b′
1 =
1
√
2
1
1

and
b′
2 =
1
√
2
 1
−1

(1.69)
would represent B1 and B2, and the matrix
a′ = 1
2
 1
1
−1
−1

(1.70)
would represent the linear operator A (exercise 1.10).
A linear operator A also may map a vector space S with basis Bk into a
different vector space T with its own basis Ck. In this case, A maps the basis
vector Bi into a linear combination of the basis vectors Ck
10

1.6 INNER PRODUCTS
ABi =
M

k=1
aki Ck
(1.71)
and an arbitrary vector U = u1B1 + · · · + uNBN in S into the vector
AU =
M

k=1
 N

i=1
aki ui

Ck
(1.72)
in T.
1.6 Inner products
Most of the vector spaces used by physicists have an inner product. A positive-
deﬁnite inner product associates a number ( f , g) with every ordered pair of
vectors f and g in the vector space V and satisﬁes the rules
( f , g) = (g, f )∗
(1.73)
( f , z g + w h) = z (f , g) + w (f , h)
(1.74)
(f , f ) ≥0
and
(f , f ) = 0 ⇐⇒f = 0
(1.75)
in which f , g, and h are vectors, and z and w are numbers. The ﬁrst rule says that
the inner product is hermitian; the second rule says that it is linear in the second
vector z g + w h of the pair; and the third rule says that it is positive deﬁnite. The
ﬁrst two rules imply that (exercise 1.11) the inner product is antilinear in the
ﬁrst vector of the pair
(z g + w h, f ) = z∗(g, f ) + w∗(h, f ).
(1.76)
A Schwarz inner product satisﬁes the ﬁrst two rules (1.73, 1.74) for an inner
product and the fourth (1.76) but only the ﬁrst part of the third (1.75)
( f , f ) ≥0.
(1.77)
This condition of nonnegativity implies (exercise 1.15) that a vector f of zero
length must be orthogonal to all vectors g in the vector space V
( f , f ) = 0 ⇒(g, f ) = 0 for all g ∈V.
(1.78)
So a Schwarz inner product is almost positive deﬁnite.
Inner products of 4-vectors can be negative. To accommodate them we deﬁne
an indeﬁnite inner product without regard to positivity as one that satisﬁes the
ﬁrst two rules (1.73 & 1.74) and therefore also the fourth rule (1.76) and that
instead of being positive deﬁnite is nondegenerate
( f , g) = 0 for all f ∈V ⇒g = 0.
(1.79)
11

LINEAR ALGEBRA
This rule says that only the zero vector is orthogonal to all the vectors of
the space. The positive-deﬁnite condition (1.75) is stronger than and implies
nondegeneracy (1.79) (exercise 1.14).
Apart from the indeﬁnite inner products of 4-vectors in special and general
relativity, most of the inner products physicists use are Schwarz inner products
or positive-deﬁnite inner products. For such inner products, we can deﬁne the
norm | f | = ∥f ∥of a vector f as the square-root of the nonnegative inner
product ( f , f )
∥f ∥=

(f , f ).
(1.80)
The distance between two vectors f and g is the norm of their difference
∥f −g ∥.
(1.81)
Example 1.14 (Euclidean space)
The space of real vectors U, V with N com-
ponents Ui, Vi forms an N-dimensional vector space over the real numbers with
an inner product
(U, V) =
N

i=1
Ui Vi
(1.82)
that is nonnegative when the two vectors are the same
(U, U) =
N

i=1
Ui Ui =
N

i=1
U2
i ≥0
(1.83)
and vanishes only if all the components Ui are zero, that is, if the vector U = 0.
Thus the inner product (1.82) is positive deﬁnite. When (U, V) is zero, the vectors
U and V are orthogonal.
Example 1.15 (Complex euclidean space)
The space of complex vectors with
N components Ui, Vi forms an N-dimensional vector space over the complex
numbers with inner product
(U, V) =
N

i=1
U∗
i Vi = (V, U)∗.
(1.84)
The inner product (U, U) is nonnegative and vanishes
(U, U) =
N

i=1
U∗
i Ui =
N

i=1
|Ui|2 ≥0
(1.85)
only if U = 0. So the inner product (1.84) is positive deﬁnite. If (U, V) is zero,
then U and V are orthogonal.
12

1.6 INNER PRODUCTS
Example 1.16 (Complex matrices)
For the vector space of N×L complex matri-
ces A, B, . . ., the trace of the adjoint (1.28) of A multiplied by B is an inner
product
(A, B) = TrA†B =
N

i=1
L

j=1
(A†)jiBij =
N

i=1
L

j=1
A∗
ijBij
(1.86)
that is nonnegative when the matrices are the same
(A, A) = TrA†A =
N

i=1
L

j=1
A∗
ijAij =
N

i=1
L

j=1
|Aij|2 ≥0
(1.87)
and zero only when A = 0. So this inner product is positive deﬁnite.
A vector space with a positive-deﬁnite inner product (1.73–1.77) is called an
inner-product space, a metric space, or a pre-Hilbert space.
A sequence of vectors fn is a Cauchy sequence if for every ϵ > 0 there is
an integer N(ϵ) such that ∥fn −fm∥< ϵ whenever both n and m exceed N(ϵ).
A sequence of vectors fn converges to a vector f if for every ϵ > 0 there is
an integer N(ϵ) such that ∥f −fn∥< ϵ whenever n exceeds N(ϵ). An inner-
product space with a norm deﬁned as in (1.80) is complete if each of its Cauchy
sequences converges to a vector in that space. A Hilbert space is a complete
inner-product space. Every ﬁnite-dimensional inner-product space is complete
and so is a Hilbert space. But the term Hilbert space more often is used to
describe inﬁnite-dimensional complete inner-product spaces, such as the space
of all square-integrable functions (David Hilbert, 1862–1943).
Example 1.17 (The Hilbert space of square-integrable functions)
For the vector
space of functions (1.55), a natural inner product is
( f , g) =
 b
a
dx f ∗(x)g(x).
(1.88)
The squared norm ∥f ∥of a function f (x) is
∥f ∥2=
 b
a
dx | f (x)|2.
(1.89)
A function is square integrable if its norm is ﬁnite. The space of all square-
integrable functions is an inner-product space; it also is complete and so is a
Hilbert space.
Example 1.18 (Minkowski inner product)
The Minkowski or Lorentz inner
product (p, x) of two 4-vectors p = (E/c, p1, p2, p3) and x = (ct, x1, x2, x3) is
13

LINEAR ALGEBRA
p · x −Et . It is indeﬁnite, nondegenerate, and invariant under Lorentz transfor-
mations, and often is written as p · x or as px. If p is the 4-momentum of a freely
moving physical particle of mass m, then
p · p = p · p −E2/c2 = −c2m2 ≤0.
(1.90)
The Minkowski inner product satisﬁes the rules (1.73, 1.75, and 1.79), but it is
not positive deﬁnite, and it does not satisfy the Schwarz inequality (Hermann
Minkowski, 1864–1909; Hendrik Lorentz, 1853–1928).
1.7 The Cauchy–Schwarz inequality
For any two vectors f and g, the Schwarz inequality
( f , f ) (g, g) ≥|( f , g)|2
(1.91)
holds for any Schwarz inner product (and so for any positive-deﬁnite inner
product). The condition (1.77) of nonnegativity ensures that for any complex
number λ the inner product of the vector f −λg with itself is nonnegative
(f −λg, f −λg) = ( f , f ) −λ∗(g, f ) −λ( f , g) + |λ|2(g, g) ≥0.
(1.92)
Now if (g, g) = 0, then for (f −λg, f −λg) to remain nonnegative for all com-
plex values of λ it is necessary that ( f , g) = 0 also vanish (exercise 1.15). Thus
if (g, g) = 0, then the Schwarz inequality (1.91) is trivially true because both
sides of it vanish. So we assume that (g, g) > 0 and set λ = (g, f )/(g, g). The
inequality (1.92) then gives us
( f −λg, f −λg) =

f −(g, f )
(g, g) g, f −(g, f )
(g, g) g

= ( f , f ) −( f , g)(g, f )
(g, g)
≥0
which is the Schwarz inequality (1.91) (Hermann Schwarz, 1843–1921)
( f , f )(g, g) ≥|( f , g)|2.
(1.93)
Taking the square-root of each side, we get
∥f ∥∥g ∥≥|( f , g)|.
(1.94)
Example 1.19 (Some Schwarz inequalities)
For the dot-product of two real
3-vectors r and R, the Cauchy–Schwarz inequality is
(r · r) (R · R) ≥(r · R)2 = (r · r) (R · R) cos2 θ
(1.95)
where θ is the angle between r and R.
The Schwarz inequality for two real n-vectors x is
(x · x) (y · y) ≥(x · y)2 = (x · x) (y · y) cos2 θ
(1.96)
14

1.8 LINEAR INDEPENDENCE AND COMPLETENESS
and it implies (Exercise 1.16) that
∥x∥+ ∥y∥≥∥x + y∥.
(1.97)
For two complex n-vectors u and v, the Schwarz inequality is

u∗· u
 
v∗· v

≥
u∗· v
2 =

u∗· u
 
v∗· v

cos2 θ
(1.98)
and it implies (exercise 1.17) that
∥u∥+ ∥v∥≥∥u + v∥.
(1.99)
The inner product (1.88) of two complex functions f and g provides a
somewhat different instance
 b
a
dx |f (x)|2
 b
a
dx |g(x)|2 ≥

 b
a
dx f ∗(x)g(x)

2
(1.100)
of the Schwarz inequality.
1.8 Linear independence and completeness
A set of N vectors V1, V2, ..., VN is linearly dependent if there exist numbers
ci, not all zero, such that the linear combination
c1V1 + · · · + cNVN = 0
(1.101)
vanishes. A set of vectors is linearly independent if it is not linearly dependent.
A set {Vi} of linearly independent vectors is maximal in a vector space S if
the addition of any other vector U in S to the set {Vi} makes the enlarged set
{U, Vi} linearly dependent.
A set of N linearly independent vectors V1, V2, ..., VN that is maximal in a
vector space S can represent any vector U in the space S as a linear combination
of its vectors, U = u1V1 + · · · + uNVN. For if we enlarge the maximal set {Vi}
by including in it any vector U not already in it, then the bigger set {U, Vi} will
be linearly dependent. Thus there will be numbers c, c1, ..., cN, not all zero,
that make the sum
c U + c1V1 + · · · + cNVN = 0
(1.102)
vanish. Now if c were 0, then the set {Vi} would be linearly dependent. Thus
c ̸= 0, and so we may divide by c and express the arbitrary vector U as a linear
combination of the vectors Vi
U = −1
c (c1V1 + · · · + cNVN) = u1V1 + · · · + uNVN
(1.103)
with uk = −ck/c. So a set of linearly independent vectors {Vi} that is maxi-
mal in a space S can represent every vector U in S as a linear combination
15

LINEAR ALGEBRA
U = u1V1 + . . . + uNVN of its vectors. The set {Vi} spans the space S; it is a
complete set of vectors in the space S.
A set of vectors {Vi} that spans a vector space S provides a basis for that space
because the set lets us represent an arbitrary vector U in S as a linear combi-
nation of the basis vectors {Vi}. If the vectors of a basis are linearly dependent,
then at least one of them is superﬂuous, and so it is convenient to have the
vectors of a basis be linearly independent.
1.9 Dimension of a vector space
If V1, ..., VN and W1, ..., WM are two maximal sets of N and M linearly
independent vectors in a vector space S, then N = M.
Suppose M < N. Since the Us are complete, they span S, and so we may
express each of the N vectors Vi in terms of the M vectors Wj
Vi =
M

j=1
AijWj.
(1.104)
Let Aj be the vector with components Aij. There are M < N such vec-
tors, and each has N > M components. So it is always possible to ﬁnd a
nonzero N-dimensional vector C with components ci that is orthogonal to all
M vectors Aj
N

i=1
ciAij = 0.
(1.105)
Thus the linear combination
N

i=1
ciVi =
N

i=1
M

j=1
ci Aij Wj = 0
(1.106)
vanishes, which implies that the N vectors Vi are linearly dependent. Since these
vectors are by assumption linearly independent, it follows that N ≤M.
Similarly, one may show that M ≤N. Thus M = N.
The number of vectors in a maximal set of linearly independent vectors in a
vector space S is the dimension of the vector space. Any N linearly independent
vectors in an N-dimensional space form a basis for it.
1.10 Orthonormal vectors
Suppose the vectors V1, V2, ..., VN are linearly independent. Then we can
make out of them a set of N vectors Ui that are orthonormal
(Ui, Uj) = δij.
(1.107)
16

1.10 ORTHONORMAL VECTORS
There are many ways to do this, because there are many such sets of orthonor-
mal vectors. We will use the Gram–Schmidt method. We set
U1 =
V1

(V1, V1)
,
(1.108)
so the ﬁrst vector U1 is normalized. Next we set u2 = V2 + c12U1 and require
that u2 be orthogonal to U1
0 = (U1, u2) = (U1, c12U1 + V2) = c12 + (U1, V2).
(1.109)
Thus c12 = −(U1, V2), and so
u2 = V2 −(U1, V2) U1.
(1.110)
The normalized vector U2 then is
U2 =
u2

(u2, u2)
.
(1.111)
We next set u3 = V3 + c13U1 + c23U2 and ask that u3 be orthogonal to U1
0 = (U1, u3) = (U1, c13U1 + c23U2 + V3) = c13 + (U1, V3)
(1.112)
and also to U2
0 = (U2, u3) = (U2, c13U1 + c23U2 + V3) = c23 + (U2, V3).
(1.113)
So c13 = −(U1, V3) and c23 = −(U2, V3), and we have
u3 = V3 −(U1, V3) U1 −(U2, V3) U2.
(1.114)
The normalized vector U3 then is
U3 =
u3

(u3, u3)
.
(1.115)
We may continue in this way until we reach the last of the N linearly
independent vectors. We require the kth unnormalized vector uk
uk = Vk +
k−1

i=1
cik Ui
(1.116)
to be orthogonal to the k −1 vectors Ui and ﬁnd that cik = −(Ui, Vk) so that
uk = Vk −
k−1

i=1
(Ui, Vk) Ui.
(1.117)
The normalized vector then is
Uk =
uk

(uk, uk)
.
(1.118)
A basis is more convenient if its vectors are orthonormal.
17

LINEAR ALGEBRA
1.11 Outer products
From any two vectors f and g, we may make an operator A that takes any vector
h into the vector f with coefﬁcient (g, h)
A h = f (g, h).
(1.119)
Since for any vectors e, h and numbers z, w
A (z h + w e) = f (g, z h + w e) = zf (g, h) + wf (g, e) = z A h + w A e
(1.120)
it follows that A is linear.
If in some basis f , g, and h are vectors with components fi, gi, and hi, then
the linear transformation is
(Ah)i =
N

j=1
Aij hj = fi
N

j=1
g∗
j hj
(1.121)
and in that basis A is the matrix with entries
Aij = fi g∗
j .
(1.122)
It is the outer product of the vectors f and g.
Example 1.20 (Outer product)
If in some basis the vectors f and g are
f =
2
3

and
g =
⎛
⎝
i
1
3i
⎞
⎠
(1.123)
then their outer product is the matrix
A =
2
3
 −i
1
−3i
=
−2i
2
−6i
−3i
3
−9i

.
(1.124)
Dirac developed a notation that handles outer products very easily.
Example 1.21 (Outer products)
If the vectors f = |f ⟩and g = |g⟩are
|f ⟩=
⎛
⎝
a
b
c
⎞
⎠
and
|g⟩=
 z
w

(1.125)
then their outer products are
|f ⟩⟨g| =
⎛
⎝
az∗
aw∗
bz∗
bw∗
cz∗
cw∗
⎞
⎠
and
|g⟩⟨f | =
za∗
zb∗
zc∗
wa∗
wb∗
wc∗

(1.126)
18

1.12 DIRAC NOTATION
as well as
|f ⟩⟨f | =
⎛
⎝
aa∗
ab∗
ac∗
ba∗
bb∗
bc∗
ca∗
cb∗
cc∗
⎞
⎠
and
|g⟩⟨g| =
zz∗
zw∗
wz∗
ww∗

.
(1.127)
Students should feel free to write down their own examples.
1.12 Dirac notation
Outer products are important in quantum mechanics, and so Dirac invented
a notation for linear algebra that makes them easy to write. In his notation, a
vector f is a ket f = |f ⟩. The new thing in his notation is the bra ⟨g|. The inner
product of two vectors (g, f ) is the bracket (g, f ) = ⟨g|f ⟩. A matrix element
(g, Af ) is then (g, Af ) = ⟨g|A|f ⟩in which the bra and ket bracket the operator.
In Dirac notation, the outer product A h = f (g, h) reads A |h⟩= |f ⟩⟨g|h⟩, so
that the outer product A itself is A = |f ⟩⟨g|. Before Dirac, bras were implicit in
the deﬁnition of the inner product, but they did not appear explicitly; there was
no way to write the bra ⟨g| or the operator |f ⟩⟨g|.
If the kets |n⟩form an orthonormal basis in an N-dimensional vector space,
then we can expand an arbitrary ket in the space as
|f ⟩=
N

n=1
cn|n⟩.
(1.128)
Since the basis vectors are orthonormal ⟨ℓ|n⟩= δℓn, we can identify the
coefﬁcients cn by forming the inner product
⟨ℓ|f ⟩=
N

n=1
cn ⟨ℓ|n⟩=
N

n=1
cn δℓ,n = cℓ.
(1.129)
The original expansion (1.128) then must be
|f ⟩=
N

n=1
cn|n⟩=
N

n=1
⟨n|f ⟩|n⟩=
N

n=1
|n⟩⟨n|f ⟩=
 N

n=1
|n⟩⟨n|

|f ⟩.
(1.130)
Since this equation must hold for every vector |f ⟩in the space, it follows that
the sum of outer products within the parentheses is the identity operator for the
space
I =
N

n=1
|n⟩⟨n|.
(1.131)
19

LINEAR ALGEBRA
Every set of kets |αn⟩that forms an orthonormal basis ⟨αn|αℓ⟩= δnℓfor the
space gives us an equivalent representation of the identity operator
I =
N

n=1
|αn⟩⟨αn| =
N

n=1
|n⟩⟨n|.
(1.132)
Before Dirac, one could not write such equations. They provide for every vector
|f ⟩in the space the expansions
|f ⟩=
N

n=1
|αn⟩⟨αn|f ⟩=
N

n=1
|n⟩⟨n|f ⟩.
(1.133)
Example 1.22 (Inner-product rules)
In Dirac’s notation, the rules (1.73–1.76)
of a positive-deﬁnite inner product are
⟨f |g⟩= ⟨g|f ⟩∗
(1.134)
⟨f |z1g1 + z2g2⟩= z1⟨f |g1⟩+ z2⟨f |g2⟩
(1.135)
⟨z1f1 + z2f2|g⟩= z∗
1⟨f1|g⟩+ z∗
2⟨f2|g⟩
(1.136)
⟨f |f ⟩≥0
and
⟨f |f ⟩= 0
⇐⇒
f = 0.
(1.137)
Usually states in Dirac notation are labeled |ψ⟩or by their quantum numbers
|n, l, m⟩, and one rarely sees plus signs or complex numbers or operators inside
bras or kets. But one should.
Example 1.23 (Gram–Schmidt)
In Dirac notation, the formula (1.117) for the
kth orthogonal linear combination of the vectors |Vℓ⟩is
|uk⟩= |Vk⟩−
k−1

i=1
|Ui⟩⟨Ui|Vk⟩=

I −
k−1

i=1
|Ui⟩⟨Ui|

|Vk⟩
(1.138)
and the formula (1.118) for the kth orthonormal linear combination of the
vectors |Vℓ⟩is
|Uk⟩=
|uk⟩
√⟨uk|uk⟩.
(1.139)
The vectors |Uk⟩are not unique; they vary with the order of the |Vk⟩.
Vectors and linear operators are abstract. The numbers we compute with are
inner products like ⟨g|f ⟩and ⟨g|A|f ⟩. In terms of N orthonormal basis vectors
|n⟩with fn = ⟨n|f ⟩and g∗
n = ⟨g|n⟩, we can use the expansion (1.131) to write
these inner products as
20

1.12 DIRAC NOTATION
⟨g|f ⟩= ⟨g|I|f ⟩=
N

n=1
⟨g|n⟩⟨n|f ⟩=
N

n=1
g∗
nfn,
⟨g|A|f ⟩= ⟨g|IAI|f ⟩=
N

n,ℓ=1
⟨g|n⟩⟨n|A|ℓ⟩⟨ℓ|f ⟩=
N

n,ℓ=1
g∗
n Anℓfℓ
(1.140)
in which Anℓ= ⟨n|A|ℓ⟩. We often gather the inner products fℓ= ⟨ℓ|f ⟩into a
column vector f with components fℓ= ⟨ℓ|f ⟩
f =
⎛
⎜⎜⎜⎝
⟨1|f ⟩
⟨2|f ⟩
...
⟨N|f ⟩
⎞
⎟⎟⎟⎠=
⎛
⎜⎜⎜⎝
f1
f2
...
f3
⎞
⎟⎟⎟⎠
(1.141)
and the ⟨n|A|ℓ⟩into a matrix A with matrix elements Anℓ= ⟨n|A|ℓ⟩. If we also
line up the inner products ⟨g|n⟩= ⟨g|n⟩∗in a row vector that is the transpose of
the complex conjugate of the column vector g
g† =

⟨1|g⟩∗, ⟨2|g⟩∗, . . . , ⟨N|g⟩∗
=

g∗
1, g∗
2, . . . , g∗
N

(1.142)
then we can write inner products in matrix notation as ⟨g|f ⟩= g†f and as
⟨g|A|f ⟩= g†Af .
If we switch to a different basis, say from |n⟩s to |αn⟩s, then the components
of the column vectors change from fn = ⟨n|f ⟩to f ′
n = ⟨αn|f ⟩, and similarly
those of the row vectors g† and of the matrix A change, but the bras, the kets,
the linear operators, and the inner products ⟨g|f ⟩and ⟨g|A|f ⟩do not change
because the identity operator is basis independent (1.132)
⟨g|f ⟩=
N

n=1
⟨g|n⟩⟨n|f ⟩=
N

n=1
⟨g|αn⟩⟨αn|f ⟩,
⟨g|A|f ⟩=
N

n,ℓ=1
⟨g|n⟩⟨n|A|ℓ⟩⟨ℓ|f ⟩=
N

n,ℓ=1
⟨g|αn⟩⟨αn|A|αℓ⟩⟨αℓ|f ⟩. (1.143)
Dirac’s outer products show how to change from one basis to another. The
sum of outer products
U =
N

n=1
|αn⟩⟨n|
(1.144)
maps the ket |ℓ⟩of the orthonormal basis we started with into |αℓ⟩
U|ℓ⟩=
N

n=1
|αn⟩⟨n|ℓ⟩=
N

n=1
|αn⟩δnℓ= |αℓ⟩.
(1.145)
21

LINEAR ALGEBRA
Example 1.24 (A simple change of basis)
If the ket |αn⟩of the new basis is
simply |αn⟩= |n + 1⟩with |αN⟩= |N + 1⟩≡|1⟩then the operator that maps the
N kets |n⟩into the kets |αn⟩is
U =
N

n=1
|αn⟩⟨n| =
N

n=1
|n + 1⟩⟨n|.
(1.146)
The square U2 of U also changes the basis; it sends |n⟩to |n + 2⟩. The set of
operators Uk for k = 1, 2, . . . , N forms a group known as ZN.
1.13 The adjoint of an operator
In Dirac’s notation, the most general linear operator on an N-dimensional vec-
tor space is a sum of dyadics like z |n⟩⟨ℓ| in which z is a complex number and
the kets |n⟩and |ℓ⟩are two of the N orthonormal kets that make up a basis for
the space. The adjoint of this basic linear operator is
(z |n⟩⟨ℓ|)† = z∗|ℓ⟩⟨n|.
(1.147)
Thus with z = ⟨n|A|ℓ⟩, the most general linear operator on the space is
A = IAI =
N

n,ℓ=1
|n⟩⟨n|A|ℓ⟩⟨ℓ|
(1.148)
and its adjoint A† is the operator IA†I
A† =
N

n,ℓ=1
|n⟩⟨n|A†|ℓ⟩⟨ℓ| =
N

n,ℓ=1
|ℓ⟩⟨n|A|ℓ⟩∗⟨n| =
N

n,ℓ=1
|n⟩⟨ℓ|A|n⟩∗⟨ℓ|.
It follows that ⟨n|A†|ℓ⟩= ⟨ℓ|A|n⟩∗so that the matrix A†
nℓthat represents A† in
this basis is
A†
nℓ= ⟨n|A†|ℓ⟩= ⟨ℓ|A|n⟩∗= A∗
ℓn = A∗T
nℓ
(1.149)
in agreement with our deﬁnition (1.28) of the adjoint of a matrix as the
transpose of its complex conjugate, A† = A∗T. We also have
⟨g|A†f ⟩= ⟨g|A†|f ⟩= ⟨f |A|g⟩∗= ⟨f |Ag⟩∗= ⟨Ag|f ⟩.
(1.150)
Taking the adjoint of the adjoint is by (1.147)

(z |n⟩⟨ℓ|)††
=

z∗|ℓ⟩⟨n|
† = z |n⟩⟨ℓ|
(1.151)
the same as doing nothing at all. This also follows from the matrix formula
(1.149) because both (A∗)∗= A and (AT)T = A, and so

A††
=

A∗T∗T = A,
(1.152)
the adjoint of the adjoint of a matrix is the original matrix.
22

1.15 REAL, SYMMETRIC LINEAR OPERATORS
Before Dirac, the adjoint A† of a linear operator A was deﬁned by
(g, A†f ) = (A g, f ) = ( f , A g)∗.
(1.153)
This deﬁnition also implies that A†† = A since
(g, A††f ) = (A†g, f ) = ( f , A†g)∗= (Af , g)∗= (g, Af ).
(1.154)
We also have (g, Af ) = (g, A††f ) = (A†g, f ).
1.14 Self-adjoint or hermitian linear operators
An operator A that is equal to its adjoint, A† = A, is self adjoint or hermi-
tian. In view of (1.149), the matrix elements of a self-adjoint linear operator A
satisfy ⟨n|A†|ℓ⟩= ⟨ℓ|A|n⟩∗= ⟨n|A|ℓ⟩in any orthonormal basis. So a matrix
that represents a hermitian operator is equal to the transpose of its complex
conjugate
Anℓ= ⟨n|A|ℓ⟩= ⟨n|A†|ℓ⟩= ⟨ℓ|A|n⟩∗= A∗T
nℓ= A†
nℓ.
(1.155)
We also have
⟨g| A |f ⟩= ⟨A g|f ⟩= ⟨f |A g⟩∗= ⟨f | A |g⟩∗
(1.156)
and in pre-Dirac notation
(g, A f ) = (A g, f ) = ( f , A g)∗.
(1.157)
A matrix Aij that is real and symmetric or imaginary and antisymmetric is her-
mitian. But a self-adjoint linear operator A that is represented by a matrix Aij
that is real and symmetric (or imaginary and antisymmetric) in one orthonor-
mal basis will not in general be represented by a matrix that is real and
symmetric (or imaginary and antisymmetric) in a different orthonormal basis,
but it will be represented by a hermitian matrix in every orthonormal basis.
A ket |a′⟩is an eigenvector of a linear operator A with eigenvalue a′ if A|a′⟩=
a′|a′⟩. As we’ll see in section 1.28, hermitian matrices have real eigenvalues and
complete sets of orthonormal eigenvectors. Hermitian operators and matrices
represent physical variables in quantum mechanics.
1.15 Real, symmetric linear operators
In quantum mechanics, we usually consider complex vector spaces, that is,
spaces in which the vectors |f ⟩are complex linear combinations
|f ⟩=
N

i=1
zi |i⟩
(1.158)
of complex orthonormal basis vectors |i⟩.
23

LINEAR ALGEBRA
But real vector spaces also are of interest. A real vector space is a vector
space in which the vectors |f ⟩are real linear combinations
|f ⟩=
N

n=1
xn |n⟩
(1.159)
of real orthonormal basis vectors, x∗
n = xn and |n⟩∗= |n⟩.
A real linear operator A on a real vector space
A =
N

n,m=1
|n⟩⟨n|A|m⟩⟨m| =
N

n,m=1
|n⟩Anm⟨m|
(1.160)
is represented by a real matrix A∗
nm = Anm. A real linear operator A that is self
adjoint on a real vector space satisﬁes the condition (1.157) of hermiticity but
with the understanding that complex conjugation has no effect
(g, A f ) = (A g, f ) = ( f , A g)∗= ( f , A g).
(1.161)
Thus its matrix elements are symmetric, ⟨g|A|f ⟩= ⟨f |A|g⟩. Since A is hermitian
as well as real, the matrix Anm that represents it (in a real basis) is real and
hermitian, and so is symmetric Anm = A∗
mn = Amn.
1.16 Unitary operators
A unitary operator U is one whose adjoint is its inverse
U U† = U† U = I.
(1.162)
Any operator that changes from one orthonormal basis |n⟩to another |αn⟩
U =
N

n=1
|αn⟩⟨n|
(1.163)
is unitary since
UU† =
N

n=1
|αn⟩⟨n|
N

m=1
|m⟩⟨αm| =
N

n,m=1
|αn⟩⟨n|m⟩⟨αm|
=
N

n,m=1
|αn⟩δn,m⟨αm| =
N

n=1
|αn⟩⟨αn| = I
(1.164)
as well as
U†U =
N

m=1
|m⟩⟨αm|
N

n=1
|αn⟩⟨n| =
N

n=1
|n⟩⟨n| = I.
(1.165)
24

1.17 HILBERT SPACE
A unitary operator maps any orthonormal basis |n⟩into another orthonormal
basis |αn⟩. For if |αn⟩= U|n⟩, then ⟨αn|αm⟩= δn,m (exercise 1.22). If we multiply
the relation |αn⟩= U|n⟩by the bra ⟨n| and then sum over the index n, we get
N

n=1
|αn⟩⟨n| =
N

n=1
U|n⟩⟨n| = U
N

n=1
|n⟩⟨n| = U.
(1.166)
Every unitary operator is a basis-changing operator, and vice versa.
Inner products do not change under unitary transformations because ⟨g|f ⟩=
⟨g|U† U|f ⟩= ⟨Ug|U|f ⟩= ⟨Ug|Uf ⟩, which in pre-Dirac notation is (g, f ) =
(g, U† Uf ) = (Ug, Uf ).
Unitary matrices have unimodular determinants because the determinant of
the product of two matrices is the product of their determinants (1.204) and
because transposition doesn’t change the value of a determinant (1.194)
1 = |I| = |UU†| = |U||U†| = |U||UT|∗= |U||U|∗.
(1.167)
A unitary matrix that is real is orthogonal and satisﬁes
OOT = OTO = I.
(1.168)
1.17 Hilbert space
We have mostly been talking about linear operators that act on ﬁnite-
dimensional vector spaces and that can be represented by matrices. But
inﬁnite-dimensional vector spaces and the linear operators that act on them
play central roles in electrodynamics and quantum mechanics. For instance, the
Hilbert space H of all “wave” functions ψ(x, t) that are square integrable over
three-dimensional space at all times t is of inﬁnite dimension.
In one space dimension, the state |x′⟩represents a particle at position x′ and
is an eigenstate of the hermitian position operator x with eigenvalue x′, that
is, x|x′⟩= x′|x′⟩. These states form a basis that is orthogonal in the sense that
⟨x|x′⟩= 0 for x ̸= x′ and normalized in the sense that ⟨x|x′⟩= δ(x −x′) in
which δ(x −x′) is Dirac’s delta function. The delta function δ(x −x′) actually
is a functional δx′ that maps any suitably smooth function f into
δx′[f ] =

δ(x −x′) f (x) dx = f (x′),
(1.169)
its value at x′.
Another basis for the Hilbert space of one-dimensional quantum mechanics
is made of the states |p⟩of well-deﬁned momentum. The state |p′⟩represents
a particle or system with momentum p′. It is an eigenstate of the hermitian
25

LINEAR ALGEBRA
momentum operator p with eigenvalue p′, that is, p|p′⟩= p′|p′⟩. The momentum
states also are orthonormal in Dirac’s sense, ⟨p|p′⟩= δ(p −p′).
The operator that translates a system in space by a distance a is
U(a) =

|x + a⟩⟨x| dx.
(1.170)
It maps the state |x′⟩to the state |x′ +a⟩and is unitary (exercise 1.23). Remark-
ably, this translation operator is an exponential of the momentum operator
U(a) = exp(−i p a/¯h) in which ¯h = h/2π = 1.054×10−34 Js is Planck’s constant
divided by 2π.
In two dimensions, with basis states |x, y⟩that are orthonormal in Dirac’s
sense, ⟨x, y|x′, y′⟩= δ(x −x′)δ(y −y′), the unitary operator
U(θ) =

|x cos θ −y sin θ, x sin θ + y cos θ⟩⟨x, y| dxdy
(1.171)
rotates a system in space by the angle θ. This rotation operator is the exponen-
tial U(θ) = exp(−i θ Lz/¯h) in which the z component of the angular momentum
is Lz = x py −y px.
We may carry most of our intuition about matrices over to these unitary
transformations that change from one inﬁnite basis to another. But we must
use common sense and keep in mind that inﬁnite sums and integrals do not
always converge.
1.18 Antiunitary, antilinear operators
Certain maps on states |ψ⟩→|ψ′⟩, such as those involving time reversal, are
implemented by operators K that are antilinear
K (zψ + wφ) = K (z|ψ⟩+ w|φ⟩) = z∗K|ψ⟩+w∗K|φ⟩= z∗Kψ+w∗Kφ (1.172)
and antiunitary
(Kφ, Kψ) = ⟨Kφ|Kψ⟩= (φ, ψ)∗= ⟨φ|ψ⟩∗= ⟨ψ|φ⟩= (ψ, φ) .
(1.173)
In Dirac notation, these rules are K(z|ψ⟩) = z∗⟨ψ| and K(w⟨φ|) = w∗|φ⟩.
1.19 Symmetry in quantum mechanics
In quantum mechanics, a symmetry is a map of states |ψ⟩→|ψ′⟩and |φ⟩→|φ′⟩
that preserves inner products and probabilities
|⟨φ′|ψ′⟩|2 = |⟨φ|ψ⟩|2.
(1.174)
26

1.20 DETERMINANTS
Eugene Wigner (1902–1995) showed that every symmetry in quantum mechan-
ics can be represented either by an operator U that is linear and unitary or
by an operator K that is antilinear and antiunitary. The antilinear, antiuni-
tary case seems to occur only when the symmetry involves time reversal. Most
symmetries are represented by operators that are linear and unitary. Unitary
operators are of great importance in quantum mechanics. We use them to rep-
resent rotations, translations, Lorentz transformations, and internal-symmetry
transformations.
1.20 Determinants
The determinant of a 2 × 2 matrix A is
det A = |A| = A11A22 −A21A12.
(1.175)
In terms of the 2 × 2 antisymmetric (eij = −eji) matrix e12 = 1 = −e21 with
e11 = e22 = 0, this determinant is
det A =
2

i=1
2

j=1
eijAi1Aj2.
(1.176)
It’s also true that
ekℓdet A =
2

i=1
2

j=1
eijAikAjℓ.
(1.177)
These deﬁnitions and results extend to any square matrix. If A is a 3 × 3
matrix, then its determinant is
det A =
3

i,j,k=1
eijkAi1Aj2Ak3
(1.178)
in which eijk is totally antisymmetric with e123 = 1, and the sums over i, j, and
k run from 1 to 3. More explicitly, this determinant is
det A =
3

i,j,k=1
eijkAi1Aj2Ak3
=
3

i=1
Ai1
3

j,k=1
eijkAj2Ak3
= A11 (A22A33 −A32A23) + A21 (A32A13 −A12A33)
+ A31 (A12A23 −A22A13) .
(1.179)
27

LINEAR ALGEBRA
The terms within parentheses are the 2 × 2 determinants (called minors) of the
matrix A without column 1 and row i, multiplied by (−1)1+i:
det A = A11(−1)2 (A22A33 −A32A23) + A21(−1)3 (A12A33 −A32A13)
+ A31(−1)4 (A12A23 −A22A13)
= A11C11 + A21C21 + A31C31
(1.180)
The minors multiplied by (−1)1+i are called cofactors:
C11 = A22A33 −A23A32,
C21 = A32A13 −A12A33,
C31 = A12A23 −A22A13.
(1.181)
Example 1.25 (Determinant of a 3 × 3 matrix)
The determinant of a 3 × 3
matrix is the dot-product of the vector of its ﬁrst row with the cross-product of
the vectors of its second and third rows:

U1
U2
U3
V1
V2
V3
W1
W2
W3

=
3

i,j,k=1
eijkUiVjWk =
3

i=1
Ui(V × W)i = U · (V × W)
which is called the scalar triple product.
Laplace used the totally antisymmetric symbol ei1i2...iN with N indices and
with e123...N = 1 to deﬁne the determinant of an N × N matrix A as
det A =
N

i1,i2,...,iN=1
ei1i2...iNAi11Ai22 . . . AiNN
(1.182)
in which the sums over i1 . . . iN run from 1 to N. In terms of cofactors, two
forms of his expansion of this determinant are
det A =
N

i=1
AikCik =
N

k=1
AikCik
(1.183)
in which the ﬁrst sum is over the row index i but not the (arbitrary) column
index k, and the second sum is over the column index k but not the (arbitrary)
row index i. The cofactor Cik is (−1)i+kMik in which the minor Mik is the deter-
minant of the (N −1) × (N −1) matrix A without its ith row and kth column.
It’s also true that
ek1k2...kN det A =
N

i1,i2,...,iN=1
ei1i2...iNAi1k1Ai2k2 . . . AiNkN.
(1.184)
28

1.20 DETERMINANTS
The key feature of a determinant is that it is an antisymmetric combination
of products of the elements Aik of a matrix A. One implication of this antisym-
metry is that the interchange of any two rows or any two columns changes the
sign of the determinant. Another is that if one adds a multiple of one column
to another column, for example a multiple xAi2 of column 2 to column 1, then
the determinant
det A′ =
N

i1,i2,...,in=1
ei1i2...iN

Ai11 + xAi12

Ai22 . . . AiNN
(1.185)
is unchanged. The reason is that the extra term δ det A vanishes
δ det A =
N

i1,i2,...,iN=1
x ei1i2...iN Ai12Ai22 . . . AiNN = 0
(1.186)
because it is proportional to a sum of products of a factor ei1i2...iN that is anti-
symmetric in i1 and i2 and a factor Ai12Ai22 that is symmetric in these indices.
For instance, when i1 and i2 are 5 & 7 and 7 & 5, the two terms cancel
e57...iNA52A72 . . . AiNN + e75...iNA72A52 . . . AiNN = 0
(1.187)
because e57...iN = −e75...iN.
By repeated additions of x2Ai2, x3Ai3, etc. to Ai1, we can change the ﬁrst
column of the matrix A to a linear combination of all the columns
Ai1 −→Ai1 +
N

k=2
xkAik
(1.188)
without changing det A. In this linear combination, the coefﬁcients xk are
arbitrary. The analogous operation with arbitrary yk
Aiℓ−→Aiℓ+
N

k=1,k̸=ℓ
ykAik
(1.189)
replaces the ℓth column by a linear combination of all the columns without
changing det A.
Suppose that the columns of an N × N matrix A are linearly dependent
(section 1.8), so that the linear combination of columns
N

k=1
ykAik = 0
for i = 1, . . . N
(1.190)
vanishes for some coefﬁcients yk not all zero. Suppose y1 ̸= 0. Then by adding
suitable linear combinations of columns 2 through N to column 1, we could
make all the modiﬁed elements A′
i1 of column 1 vanish without changing det A.
29

LINEAR ALGEBRA
But then det A as given by (1.182) would vanish. Thus the determinant of any
matrix whose columns are linearly dependent must vanish.
The converse also is true: if columns of a matrix are linearly independent,
then the determinant of that matrix can not vanish. The reason is that any lin-
early independent set of vectors is complete (section 1.8). Thus if the columns of
a matrix A are linearly independent and therefore complete, some linear combi-
nation of all columns 2 through N when added to column 1 will convert column
1 into a (nonzero) multiple of the N-dimensional column vector (1, 0, 0, . . . 0),
say (c1, 0, 0, . . . 0). Similar operations will convert column 2 into a (nonzero)
multiple of the column vector (0, 1, 0, . . . 0), say (0, c2, 0, . . . 0). Continuing in
this way, we may convert the matrix A to a matrix with nonzero entries along
the main diagonal and zeros everywhere else. The determinant det A is then the
product of the nonzero diagonal entries c1c2 . . . cN ̸= 0, and so det A can not
vanish.
We may extend these arguments to the rows of a matrix. The addition to row
k of a linear combination of the other rows
Aki −→Aki +
N

ℓ=1,ℓ̸=k
zℓAℓi
(1.191)
does not change the value of the determinant. In this way, one may show that
the determinant of a matrix vanishes if and only if its rows are linearly depen-
dent. The reason why these results apply to the rows as well as to the columns
is that the determinant of a matrix A may be deﬁned either in terms of the
columns as in deﬁnitions (1.182 & 1.184) or in terms of the rows:
det A =
N

i1,i2,...,iN=1
ei1i2...iNA1i1A2i2 . . . ANiN,
(1.192)
ek1k2...kN det A =
N

i1,i2,...,iN=1
ei1i2...iNAk1i1Ak2i2 . . . AkNiN.
(1.193)
These and other properties of determinants follow from a study of permutations
(section 10.13). Detailed proofs are in Aitken (1959).
By comparing the row (1.182 & 1.184) and column (1.192 & 1.193) deﬁni-
tions of determinants, we see that the determinant of the transpose of a matrix
is the same as the determinant of the matrix itself:
det

AT
= det A.
(1.194)
Let us return for a moment to Laplace’s expansion (1.183) of the determinant
det A of an N × N matrix A as a sum of AikCik over the row index i with the
column index k held ﬁxed
30

1.20 DETERMINANTS
det A =
N

i=1
AikCik
(1.195)
in order to prove that
δkℓdet A =
N

i=1
AikCiℓ.
(1.196)
For k = ℓ, this formula just repeats Laplace’s expansion (1.195). But for k ̸= ℓ,
it is Laplace’s expansion for the determinant of a matrix A′ that is the same as
A but with its ℓth column replaced by its kth one. Since the matrix A′ has two
identical columns, its determinant vanishes, which explains (1.196) for k ̸= ℓ.
This rule (1.196) provides a formula for the inverse of a matrix A whose
determinant does not vanish. Such matrices are said to be nonsingular. The
inverse A−1 of an N × N nonsingular matrix A is the transpose of the matrix of
cofactors divided by det A

A−1
ℓi =
Ciℓ
det A
or
A−1 =
CT
det A.
(1.197)
To verify this formula, we use it for A−1 in the product A−1A and note that by
(1.196) the ℓkth entry of the product A−1A is just δℓk

A−1A

ℓk =
N

i=1

A−1
ℓi Aik =
N

i=1
Ciℓ
det AAik = δℓk.
(1.198)
Example 1.26 (Inverting a 2×2 matrix)
Let’s apply our formula (1.197) to ﬁnd
the inverse of the general 2 × 2 matrix
A =
a
b
c
d

.
(1.199)
We ﬁnd then
A−1 =
1
ad −bc
 d
−b
−c
a

,
(1.200)
which is the correct inverse as long as ad ̸= bc.
The simple example of matrix multiplication
⎛
⎝
a
b
c
d
e
f
g
h
i
⎞
⎠
⎛
⎝
1
x
y
0
1
z
0
0
1
⎞
⎠=
⎛
⎝
a
xa + b
ya + zb + c
d
xd + e
yd + ze + f
g
xg + h
yg + zh + i
⎞
⎠
(1.201)
shows that the operations (1.189) on columns that don’t change the value of the
determinant can be written as matrix multiplication from the right by a matrix
31

LINEAR ALGEBRA
that has unity on its main diagonal and zeros below. Now consider the matrix
product
 A
0
−I
B
 I
B
0
I

=
 A
AB
−I
0

(1.202)
in which A and B are N × N matrices, I is the N × N identity matrix, and 0 is
the N ×N matrix of all zeros. The second matrix on the left-hand side has unity
on its main diagonal and zeros below, and so it does not change the value of the
determinant of the matrix to its left, which then must equal that of the matrix
on the right-hand side:
det
 A
0
−I
B

= det
 A
AB
−I
0

.
(1.203)
By using Laplace’s expansion (1.183) along the ﬁrst column to evaluate the
determinant on the left-hand side and his expansion along the last row to com-
pute the determinant on the right-hand side, one ﬁnds that the determinant of
the product of two matrices is the product of the determinants
det A det B = det AB.
(1.204)
Example 1.27 (Two 2 × 2 matrices)
When the matrices A and B are both 2 × 2,
the two sides of (1.203) are
det
 A
0
−I
B

= det
⎛
⎜⎜⎝
a11
a12
0
0
a21
a22
0
0
−1
0
b11
b12
0
−1
b21
b22
⎞
⎟⎟⎠
= a11a22 det B −a21a12 det B = det A det B
(1.205)
and
det
 A
AB
−I
0

= det
⎛
⎜⎜⎝
a11
a12
ab11
ab12
a21
a22
ab21
ab22
−1
0
0
0
0
−1
0
0
⎞
⎟⎟⎠
= (−1)C42 = (−1)(−1) det AB = det AB
(1.206)
and so they give the product rule det A det B = det AB.
Often one uses the notation |A| = det A to denote a determinant. In this
more compact notation, the obvious generalization of the product rule is
32

1.20 DETERMINANTS
|ABC . . . Z| = |A||B| . . . |Z|.
(1.207)
The product rule (1.204) implies that det

A−1
is 1/ det A since
1 = det I = det

AA−1
= det A det

A−1
.
(1.208)
Incidentally, Gauss, Jordan, and modern mathematicians have developed
much faster ways of computing determinants and matrix inverses than those
(1.183 & 1.197) due to Laplace. Octave, Matlab, Maple, and Mathematica use
these modern techniques, which also are freely available as programs in C and
FORTRAN from www.netlib.org/lapack.
Example 1.28 (Numerical tricks)
Adding multiples of rows to other rows does
not change the value of a determinant, and interchanging two rows only changes
a determinant by a minus sign. So we can use these operations, which leave
determinants invariant, to make a matrix upper triangular, a form in which its
determinant is just the product of the factors on its diagonal. For instance, to
make the matrix
A =
⎛
⎝
1
2
1
−2
−6
3
4
2
−5
⎞
⎠
(1.209)
upper triangular, we add twice the ﬁrst row to the second row
⎛
⎝
1
2
1
0
−2
5
4
2
−5
⎞
⎠
(1.210)
and then subtract four times the ﬁrst row from the third
⎛
⎝
1
2
1
0
−2
5
0
−6
−9
⎞
⎠.
(1.211)
Next, we subtract three times the second row from the third
⎛
⎝
1
2
1
0
−2
5
0
0
−24
⎞
⎠.
(1.212)
We now ﬁnd as the determinant of A the product of its diagonal elements:
|A| = 1(−2)(−24) = 48.
(1.213)
The Matlab command is d = det(A).
33

LINEAR ALGEBRA
1.21 Systems of linear equations
Suppose we wish to solve the system of N linear equations
N

k=1
Aikxk = yi
(1.214)
for N unknowns xk. In matrix notation, with A an N × N matrix and x and y
N-vectors, this system of equations is A x = y. If the matrix A is nonsingular,
that is, if det(A) ̸= 0, then it has an inverse A−1 given by (1.197), and we may
multiply both sides of A x = y by A−1 and so ﬁnd x as x = A−1 y. When A is
nonsingular, this is the unique solution to (1.214).
When A is singular, det(A) = 0, and so its columns are linearly dependent
(section 1.20). In this case, the linear dependence of the columns of A implies
that A z = 0 for some nonzero vector z. Thus if x is a solution, so that A x = y,
then A(x + cz) = A x + c A z = y implies that x + cz for all c also is a solution.
So if det(A) = 0, then there may be solutions, but there can be no unique
solution. Whether equation (1.214) has any solutions when det(A) = 0 depends
on whether the vector y can be expressed as a linear combination of the columns
of A. Since these columns are linearly dependent, they span a subspace of fewer
than N dimensions, and so (1.214) has solutions only when the N-vector y lies
in that subspace.
A system of M < N equations
N

k=1
Aikxk = yi
for
i = 1, 2, . . . , M
(1.215)
in N unknowns is under-determined. As long as at least M of the N columns Aik
of the matrix A are linearly independent, such a system always has solutions,
but they will not be unique.
1.22 Linear least squares
Suppose we have a system of M > N equations in N unknowns xk
N

k=1
Aikxk = yi
for
i = 1, 2, . . . , M.
(1.216)
This problem is over-determined and, in general, has no solution, but it does
have an approximate solution due to Carl Gauss (1777–1855).
If the matrix A and the vector y are real, then Gauss’s solution is the N values
xk that minimize the sum E of the squares of the errors
34

1.23 LAGRANGE MULTIPLIERS
E =
M

i=1

yi −
N

k=1
Aikxk
2
.
(1.217)
The minimizing values xk make the N derivatives of E vanish
∂E
∂xℓ
= 0 =
M

i=1
2

yi −
N

k=1
Aikxk

(−Aiℓ)
(1.218)
or in matrix notation ATy = ATAx. Since A is real, the matrix ATA is nonnega-
tive (1.38); if it also is positive (1.39), then it has an inverse, and our least-squares
solution is
x =

ATA
−1 ATy.
(1.219)
If the matrix A and the vector y are complex, and if the matrix A†A is positive,
then one may show (exercise 1.25) that Gauss’s solution is
x =

A†A
−1
A†y.
(1.220)
1.23 Lagrange multipliers
The maxima and minima of a function f (x) of several variables x1, x2, . . . , xn
are among the points at which its gradient vanishes
∇f (x) = 0.
(1.221)
These are the stationary points of f .
Example 1.29 (Minimum)
For instance, if f (x) = x2
1 + 2x2
2 + 3x2
3, then its
minimum is at
∇f (x) = (2x1, 4x2, 6x3) = 0
(1.222)
that is, at x1 = x2 = x3 = 0.
But how do we ﬁnd the extrema of f (x) if x must satisfy a constraint? We use
a Lagrange multiplier (Joseph-Louis Lagrange, 1736–1813).
In the case of one constraint c(x) = 0, we no longer expect the gradient ∇f (x)
to vanish, but its projection dx · ∇f (x) must vanish in those directions dx that
preserve the constraint. So dx · ∇f (x) = 0 for all dx that make the dot-product
dx · ∇c(x) vanish. This means that ∇f (x) and ∇c(x) must be parallel. So the
extrema of f (x) subject to the constraint c(x) = 0 satisfy two equations
∇f (x) = λ ∇c(x)
and
c(x) = 0.
(1.223)
35

LINEAR ALGEBRA
These equations deﬁne the extrema of the unconstrained function
L(x, λ) = f (x) −λ c(x)
(1.224)
of the n + 1 variables x, . . . , xn, λ
∇L(x, λ) = ∇f (x) −λ ∇c(x) = 0
and
∂L(x, λ)
∂λ
= −c(x) = 0.
(1.225)
The variable λ is a Lagrange multiplier.
In the case of k constraints c1(x) = 0, ..., ck(x) = 0, the projection ∇f must
vanish in those directions dx that preserve all the constraints. So dx · ∇f (x) = 0
for all dx that make all dx · ∇cj(x) = 0 for j = 1, . . . , k. The gradient ∇f will
satisfy this requirement if it’s a linear combination
∇f = λ1 ∇c1 + · · · + λk ∇ck
(1.226)
of the k gradients because then dx·∇f will vanish if dx·∇cj = 0 for j = 1, . . . , k.
The extrema also must satisfy the constraints
c1(x) = 0, . . . , ck(x) = 0.
(1.227)
Equations (1.226 & 1.227) deﬁne the extrema of the unconstrained function
L(x, λ) = f (x) −λ1 c1(x) + · · · λk ck(x)
(1.228)
of the n + k variables x and λ
∇L(x, λ) = ∇f (x) −λ ∇c1(x) −· · · −λ ∇ck(x) = 0
(1.229)
and
∂L(x, λ)
∂λj
= −cj(x) = 0
for
j = 1, . . . , k.
(1.230)
Example 1.30 (Constrained extrema and eigenvectors)
Suppose we want to ﬁnd
the extrema of a real, symmetric quadratic form f (x) = xTA x subject to the
constraint c(x) = x·x−1, which says that the vector x is of unit length. We form
the function
L(x, λ) = xTA x −λ (x · x −1)
(1.231)
and since the matrix A is real and symmetric, we ﬁnd its unconstrained extrema
as
∇L(x, λ) = 2A x −2λ x = 0
and
x · x = 1.
(1.232)
The extrema of f (x) = xTA x subject to the constraint c(x) = x · x −1 are the
normalized eigenvectors
A x = λ x
and
x · x = 1
(1.233)
of the real, symmetric matrix A.
36

1.24 EIGENVECTORS
1.24 Eigenvectors
If a linear operator A maps a nonzero vector |u⟩into a multiple of itself
A|u⟩= λ|u⟩
(1.234)
then the vector |u⟩is an eigenvector of A with eigenvalue λ. (The German
adjective eigen means special or proper.)
If the vectors {|k⟩} for k = 1, ..., N form a basis for the vector space in which
A acts, then we can write the identity operator for the space as I = |1⟩⟨1| +
· · ·+|N⟩⟨N|. By inserting this formula for I twice into the eigenvector equation
(1.234), we can write it as
N

ℓ=1
⟨k|A|ℓ⟩⟨ℓ|u⟩= λ ⟨k|u⟩.
(1.235)
In matrix notation, with Akℓ= ⟨k|A|ℓ⟩and uℓ= ⟨ℓ|u⟩, this is A u = λ u.
Example 1.31 (Eigenvalues of an orthogonal matrix)
The matrix equation
 cos θ
sin θ
−sin θ
cos θ
  1
±i

= e±iθ
 1
±i

(1.236)
tells us that the eigenvectors of this 2 × 2 orthogonal matrix are the 2-tuples
(1, ±i) with eigenvalues e±iθ. The eigenvalues λ of a unitary (and of an orthogo-
nal) matrix are unimodular, |λ| = 1 (exercise 1.26).
Example 1.32 (Eigenvalues of an antisymmetric matrix)
Let us consider an
eigenvector equation for a matrix A that is antisymmetric
N

k=1
Aik uk = λ ui.
(1.237)
The antisymmetry Aik = −Aki of A implies that
N

i,k=1
ui Aik uk = 0.
(1.238)
Thus the last two relations imply that
0 =
N

i,k=1
ui Aik uk = λ
N

i=1
u2
i = 0.
(1.239)
Thus either the eigenvalue λ or the dot-product of the eigenvector with itself
vanishes.
37

LINEAR ALGEBRA
A subspace cℓ|uℓ⟩+· · ·+cr|ur⟩spanned by any set of eigenvectors of a matrix
A is left invariant by its action, that is
A (cℓ|uℓ⟩+ · · · + cr|ur⟩) = cℓλℓ|uℓ⟩+ · · · + crλr|ur⟩.
(1.240)
Eigenvectors span invariant subspaces.
1.25 Eigenvectors of a square matrix
Let A be an N × N matrix with complex entries Aik. A vector V with N entries
Vk (not all zero) is an eigenvector of A with eigenvalue λ if
AV = λV ⇐⇒
N

k=1
AikVk = λVi.
(1.241)
Every N × N matrix A has N eigenvectors V(ℓ) and eigenvalues λℓ
AV(ℓ) = λℓV(ℓ)
(1.242)
for ℓ= 1 . . . N. To see why, we write the top equation (1.241) as
N

k=1
(Aik −λδik) Vk = 0
(1.243)
or in matrix notation as (A −λ I) V = 0 in which I is the N × N matrix with
entries Iik = δik. This equation and (1.243) say that the columns of the matrix
A −λI, considered as vectors, are linearly dependent, as deﬁned in section 1.8.
We saw in section 1.20 that the columns of a matrix A −λI are linearly depen-
dent if and only if the determinant |A −λI| vanishes. Thus a nonzero solution
of the eigenvalue equation (1.241) exists if and only if the determinant
det (A −λI) = |A −λI| = 0
(1.244)
vanishes. This requirement that the determinant of A−λI vanishes is called the
characteristic equation. For an N × N matrix A, it is a polynomial equation of
the Nth degree in the unknown eigenvalue λ
|A −λI| ≡P(λ, A) = |A| + · · · + (−1)N−1λN−1 TrA + (−1)NλN
=
N

k=0
pk λk = 0
(1.245)
in which p0 = |A|, pN−1 = (−1)N−1TrA, and pN = (−1)N. (All the pks are
basis independent.) By the fundamental theorem of algebra (section 5.9), the
characteristic equation always has N roots or solutions λℓlying somewhere in
the complex plane. Thus the characteristic polynomial has the factored form
P(λ, A) = (λ1 −λ)(λ2 −λ) . . . (λN −λ).
(1.246)
38

1.25 EIGENVECTORS OF A SQUARE MATRIX
For every root λℓ, there is a nonzero eigenvector V(ℓ) whose components V(ℓ)
k
are the coefﬁcients that make the N vectors Aik −λℓδik that are the columns
of the matrix A −λℓI sum to zero in (1.243). Thus every N × N matrix has N
eigenvalues λℓand N eigenvectors V(ℓ).
The N × N diagonal matrix Dkℓ= δkℓλℓis the canonical form of the matrix
A; the matrix Vkℓ= V(ℓ)
k
whose columns are the eigenvectors V(ℓ) of A is the
modal matrix; and AV = VD.
Example 1.33 (The canonical form of a 3 × 3 matrix)
If in Matlab we set A =
[0 1 2; 3 4 5; 6 7 8] and enter [V, D] = eig(A), then we get
V =
⎛
⎝
0.1648
0.7997
0.4082
0.5058
0.1042
−0.8165
0.8468
−0.5913
0.4082
⎞
⎠
and
D =
⎛
⎝
13.3485
0
0
0
−1.3485
0
0
0
0
⎞
⎠
and one may check that AV = VD.
Setting λ = 0 in the factored form (1.246) of P(λ, A) and in the characteristic
equation (1.245), we see that the determinant of every N×N matrix is the product
of its N eigenvalues
P(0, A) = |A| = p0 = λ1λ2 . . . λN.
(1.247)
These N roots usually are all different, and when they are, the eigenvectors
V(ℓ) are linearly independent. The ﬁrst eigenvector is trivially linearly indepen-
dent. Let’s assume that the ﬁrst K < N eigenvectors are linearly independent;
we’ll show that the ﬁrst K +1 eigenvectors are linearly independent. If they were
linearly dependent, then there would be K + 1 numbers cℓ, not all zero, such
that
K+1

ℓ=1
cℓV(ℓ) = 0.
(1.248)
First we multiply this equation from the left by the linear operator A and use
the eigenvalue equation (1.242)
A
K+1

ℓ=1
cℓV(ℓ) =
K+1

ℓ=1
cℓAV(ℓ) =
K+1

ℓ=1
cℓλℓV(ℓ) = 0.
(1.249)
Now we multiply the same equation (1.248) by λK+1
K+1

ℓ=1
cℓλN V(ℓ) = 0
(1.250)
39

LINEAR ALGEBRA
and subtract the product (1.250) from (1.249). The terms with ℓ= K +1 cancel
leaving
K

ℓ=1
cℓ(λℓ−λN) V(ℓ) = 0
(1.251)
in which all the factors (λℓ−λK+1) are different from zero since by assump-
tion all the eigenvalues are different. But this last equation says that the ﬁrst
K eigenvectors are linearly dependent, which contradicts our assumption that
they were linearly independent. This contradiction tells us that if all N eigenvec-
tors of an N × N square matrix have different eigenvalues, then they are linearly
independent.
An eigenvalue λ that is a single root of the characteristic equation (1.245) is
associated with a single eigenvector; it is called a simple eigenvalue. An eigen-
value λ that is an nth root of the characteristic equation is associated with n
eigenvectors; it is said to be an n-fold degenerate eigenvalue or to have algebraic
multiplicity n. Its geometric multiplicity is the number n′ ≤n of linearly inde-
pendent eigenvectors with eigenvalue λ. A matrix with n′ < n for any eigenvalue
λ is defective. Thus an N × N matrix with fewer than N linearly independent
eigenvectors is defective.
Example 1.34 (A defective 2 × 2 matrix)
Each of the 2 × 2 matrices
0
1
0
0

and
0
0
1
0

(1.252)
has only one linearly independent eigenvector and so is defective.
Suppose A is an N ×N matrix that is not defective. We may use its N linearly
independent eigenvectors V(ℓ) = |ℓ⟩to deﬁne the columns of an N × N matrix
S as Skℓ= V(ℓ)
k . In terms of S, the eigenvalue equation (1.242) takes the form
N

k=1
AikSkℓ= λℓSiℓ.
(1.253)
Since the columns of S are linearly independent, the determinant of S does not
vanish – the matrix S is nonsingular – and so its inverse S−1 is well deﬁned by
(1.197). So we may multiply this equation by S−1 and get
N

i,k=1

S−1
ni AikSkℓ=
N

i=1
λℓ

S−1
ni Siℓ= λnδnℓ= λℓ
(1.254)
or in matrix notation
S−1AS = A(d)
(1.255)
40

1.26 A MATRIX OBEYS ITS CHARACTERISTIC EQUATION
in which A(d) is the diagonal form of the matrix A in which its eigenvalues λℓare
arranged along its main diagonal with zeros elsewhere. This equation (1.255) is
a similarity transformation. Thus every nondefective square matrix can be diag-
onalized by a similarity transformation S−1AS = A(d) and can be generated
from its diagonal form by the inverse A = SA(d)S−1 of that similarity transfor-
mation. By using the product rule (1.207), we see that the determinant of any
nondefective square matrix is the product of its eigenvalues
|A| = |SA(d)S−1| = |S| |A(d)| |S−1| = |SS−1| |A(d)| = |A(d)| =
N

ℓ=1
λℓ, (1.256)
which is a special case of (1.247).
1.26 A matrix obeys its characteristic equation
Every square matrix obeys its characteristic equation (1.245). That is, the
characteristic equation
P(λ, A) = |A −λI| =
N

k=0
pk λk = 0
(1.257)
remains true when the matrix A replaces the variable λ
P(A, A) =
N

k=0
pk Ak = 0.
(1.258)
To see why, we use the formula (1.197) for the inverse of the matrix A −λI
(A −λI)−1 = C(λ, A)T
|A −λI|
(1.259)
in which C(λ, A)T is the transpose of the matrix of cofactors of the matrix A −
λI. Since |A −λI| = P(λ, A), we have, rearranging,
(A −λI) C(λ, A)T = |A −λI| I = P(λ, A) I.
(1.260)
The transpose of the matrix of cofactors of the matrix A −λI is a polynomial
in λ with matrix coefﬁcients
C(λ, A)T = C0 + C1λ + · · · + CN−1λN−1.
(1.261)
The left-hand side of equation (1.260) is then
(A −λI)C(λ, A)T = AC0 + (AC1 −C0)λ + (AC2 −C1)λ2 + · · ·
+ (ACN−1 −CN−2)λN−1 −CN−1λN.
(1.262)
41

LINEAR ALGEBRA
Equating equal powers of λ on both sides of (1.260), we have, using (1.257) and
(1.262),
AC0 = p0I,
AC1 −C0 = p1I,
AC2 −C1 = p2I,
· · · = · · ·
ACN−1 −CN−2 = pN−1I,
−CN−1 = pNI.
(1.263)
We now multiply from the left the ﬁrst of these equations by I, the second by A,
the third by A2, ..., and the last by AN and then add the resulting equations.
All the terms on the left-hand sides cancel, while the sum of those on the right
gives P(A, A). Thus a square matrix A obeys its characteristic equation 0 =
P(A, A) or
0 =
N

k=0
pk Ak = |A| I + p1A + · · · + (−1)N−1(TrA) AN−1 + (−1)N AN, (1.264)
a result known as the Cayley–Hamilton theorem (Arthur Cayley, 1821–1895,
and William Hamilton, 1805–1865). This derivation is due to Israel Gelfand
(1913–2009) (Gelfand, 1961, pp. 89–90).
Because every N×N matrix A obeys its characteristic equation, its Nth power
AN can be expressed as a linear combination of its lesser powers
AN = (−1)N−1 
|A| I + p1A + p2A2 + · · · + (−1)N−1(TrA) AN−1
.
(1.265)
For instance, the square A2 of every 2 × 2 matrix is given by
A2 = −|A|I + (TrA)A.
(1.266)
Example 1.35 (Spin-one-half rotation matrix)
If θ is a real 3-vector and σ is
the 3-vector of Pauli matrices (1.32), then the square of the traceless 2×2 matrix
A = θ · σ is
(θ · σ)2 −|θ · σ| = −

θ3
θ1 −iθ2
θ1 + iθ2
−θ3
 I = θ2 I
(1.267)
in which θ2 = θ · θ. One may use this identity to show (exercise (1.28)) that
exp (−iθ · σ/2) = cos(θ/2) −i ˆθ · σ sin(θ/2)
(1.268)
in which ˆθ is a unit 3-vector. For a spin-one-half object, this matrix represents a
right-handed rotation of θ radians about the axis ˆθ.
42

1.27 FUNCTIONS OF MATRICES
1.27 Functions of matrices
What sense can we make of a function f of an N × N matrix A and how
would we compute it? One way is to use the characteristic equation (1.265)
to express every power of A in terms of I, A, ..., AN−1 and the coefﬁ-
cients p0 = |A|, p1, p2, . . . , pN−2, and pN−1 = (−1)N−1TrA. Then if f (x) is a
polynomial or a function with a convergent power series
f (x) =
∞

k=0
ck xk
(1.269)
in principle we may express f (A) in terms of N functions fk(p) of the coefﬁcients
p ≡(p0, . . . , pN−1) as
f (A) =
N−1

k=0
fk(p) Ak.
(1.270)
The identity (1.268) for exp (−iθ · σ/2) is an N = 2 example of this technique,
which can become challenging when N > 3.
Example 1.36 (The 3×3 rotation matrix)
In exercise (1.29), one ﬁnds the char-
acteristic equation (1.264) for the 3×3 matrix −iθ · J in which (Jk)ij = iϵikj, and
ϵijk is totally antisymmetric with ϵ123 = 1. The generators Jk satisfy the com-
mutation relations [Ji, Jj] = iϵijkJk in which sums over repeated indices from
1 to 3 are understood. In exercise (1.31), one uses this characteristic equation
for −iθ · J to show that the 3×3 real orthogonal matrix exp(−iθ · J), which
represents a right-handed rotation by θ radians about the axis ˆθ, is
exp(−iθ · J) = cos θ I −i ˆθ · J sin θ + (1 −cos θ) ˆθ( ˆθ)T
(1.271)
or
exp(−iθ · J)ij = δij cos θ −sin θ ϵijk ˆθk + (1 −cos θ) ˆθi ˆθj
(1.272)
in terms of indices.
Direct use of the characteristic equation can become unwieldy for larger val-
ues of N. Fortunately, another trick is available if A is a nondefective square
matrix, and if the power series (1.269) for f (x) converges. For then A is related
to its diagonal form A(d) by a similarity transformation (1.255), and we may
deﬁne f (A) as
f (A) = Sf (A(d))S−1
(1.273)
in which f (A(d)) is the diagonal matrix with entries f (aℓ)
43

LINEAR ALGEBRA
f (A(d)) =
⎛
⎜⎜⎜⎝
f (a1)
0
0
. . .
0
f (a2)
0
. . .
...
...
...
...
0
0
. . .
f (aN)
⎞
⎟⎟⎟⎠,
(1.274)
in which a1, a2, . . . , aN are the eigenvalues of the matrix A. This deﬁnition
makes sense if f (A) is a series in powers of A because then
f (A) =
∞

n=0
cnAn =
∞

n=0
cn

SA(d)S−1n
.
(1.275)
So since S−1S = I, we have

SA(d)S−1n = S

A(d)n S−1 and thus
f (A) = S
 ∞

n=0
cn

A(d)n

S−1 = Sf (A(d))S−1,
(1.276)
which is (1.273).
Example 1.37 (The time-evolution operator)
In quantum mechanics, the time-
evolution operator is the exponential exp(−iHt/¯h) where H = H† is a hermitian
linear operator, the hamiltonian (William Rowan Hamilton, 1805–1865), and
¯h = h/(2π) = 1.054 × 10−34 Js where h is constant (Max Planck, 1858–1947).
As we’ll see in the next section, hermitian operators are never defective, so H
can be diagonalized by a similarity transformation
H = SH(d)S−1.
(1.277)
The diagonal elements of the diagonal matrix H(d) are the energies Eℓof
the states of the system described by the hamiltonian H. The time-evolution
operator U(t) then is
U(t) = S exp(−iH(d)t/¯h) S−1.
(1.278)
For a three-state system with angular frequencies ωi = Ei/¯h, it is
U(t) = S
⎛
⎝
e−iω1t
0
0
0
e−iω2t
0
0
e−iω3t
⎞
⎠S−1
(1.279)
in which the angular frequencies are ωℓ= Eℓ/¯h.
Example 1.38 (Entropy)
The entropy S of a system described by a density
operator ρ is the trace
S = −k Tr (ρ ln ρ)
(1.280)
in which k = 1.38 × 10−23 J/K is the constant named after Ludwig Boltzmann
(1844–1906). The density operator ρ is hermitian, nonnegative, and of unit trace.
44

1.28 HERMITIAN MATRICES
Since ρ is hermitian, the matrix that represents it is never defective (section 1.28),
and so it can be diagonalized by a similarity transformation ρ = S ρ(d) S−1. By
(1.24), TrABC = TrBCA, so we can write S as
S = −kTr

S ρ(d) S−1 S ln(ρ(d)) S−1
= −kTr

ρ(d) ln(ρ(d))

.
(1.281)
A vanishing eigenvalue ρ(d)
k
=
0 contributes nothing to this trace since
limx→0 x ln x = 0. If the system has three states, populated with probabilities
ρi, the elements of ρ(d), then the sum
S = −k (ρ1 ln ρ1 + ρ2 ln ρ2 + ρ3 ln ρ3)
= k [ρ1 ln (1/ρ1) + ρ2 ln (1/ρ2) + ρ3 ln (1/ρ3)]
(1.282)
is its entropy.
1.28 Hermitian matrices
Hermitian matrices have very nice properties. By deﬁnition (1.30), a hermitian
matrix A is square and unchanged by hermitian conjugation A† = A. Since it is
square, the results of section 1.25 ensure that an N × N hermitian matrix A has
N eigenvectors |n⟩with eigenvalues an
A|n⟩= an|n⟩.
(1.283)
In fact, all its eigenvalues are real. To see why, we take the adjoint
⟨n|A† = a∗
n⟨n|
(1.284)
and use the property A† = A to ﬁnd
⟨n|A† = ⟨n|A = a∗
n⟨n|.
(1.285)
We now form the inner product of both sides of this equation with the ket |n⟩
and use the eigenvalue equation (1.283) to get
⟨n|A|n⟩= an⟨n|n⟩= a∗
n⟨n|n⟩,
(1.286)
which (since ⟨n|n⟩> 0) tells us that the eigenvalues are real
a∗
n = an.
(1.287)
Since A† = A, the matrix elements of A between two of its eigenvectors satisfy
a∗
m⟨m|n⟩= (am⟨n|m⟩)∗= ⟨n|A|m⟩∗= ⟨m|A†|n⟩= ⟨m|A|n⟩= an⟨m|n⟩, (1.288)
which implies that

a∗
m −an

⟨m|n⟩= 0.
(1.289)
45

LINEAR ALGEBRA
But by (1.287), the eigenvalues am are real, and so we have
(am −an) ⟨m|n⟩= 0,
(1.290)
which tells us that when the eigenvalues are different, the eigenvectors are
orthogonal. In the absence of a symmetry, all n eigenvalues usually are different,
and so the eigenvectors usually are mutually orthogonal.
When two or more eigenvectors |nα⟩of a hermitian matrix have the same
eigenvalue an, their eigenvalues are said to be degenerate. In this case, any linear
combination of the degenerate eigenvectors also will be an eigenvector with the
same eigenvalue an
A

α∈D
cα|nα⟩

= an

α∈D
cα|nα⟩

(1.291)
where D is the set of labels α of the eigenvectors with the same eigenvalue. If
the degenerate eigenvectors |nα⟩are linearly independent, then we may use the
Gramm–Schmidt procedure (1.108–1.118) to choose the coefﬁcients cα so as to
construct degenerate eigenvectors that are orthogonal to each other and to the
nondegenerate eigenvectors. We then may normalize these mutually orthogonal
eigenvectors.
But two related questions arise. Are the degenerate eigenvectors |nα⟩linearly
independent? And if so, what orthonormal linear combinations of them should
we choose for a given physical problem? Let’s consider the second question ﬁrst.
We know (section 1.16) that unitary transformations preserve the orthonor-
mality of a basis. Any unitary transformation that commutes with the matrix A
[A, U] = 0
(1.292)
maps each set of orthonormal degenerate eigenvectors of A into another set of
orthonormal degenerate eigenvectors of A with the same eigenvalue because
AU|nα⟩= UA|nα⟩= an U|nα⟩.
(1.293)
So there’s a huge spectrum of choices for the orthonormal degenerate eigenvec-
tors of A with the same eigenvalue. What is the right set for a given physical
problem?
A sensible way to proceed is to add to the matrix A a second hermitian matrix
B multiplied by a tiny, real scale factor ϵ
A(ϵ) = A + ϵB.
(1.294)
The matrix B must completely break whatever symmetry led to the degeneracy
in the eigenvalues of A. Ideally, the matrix B should be one that represents a
modiﬁcation of A that is physically plausible and relevant to the problem at
46

1.28 HERMITIAN MATRICES
hand. The hermitian matrix A(ϵ) then will have N different eigenvalues an(ϵ)
and N orthonormal nondegenerate eigenvectors
A(ϵ)|nβ, ϵ⟩= anβ(ϵ)|nβ, ϵ⟩.
(1.295)
These eigenvectors |nβ, ϵ⟩of A(ϵ) are orthogonal to each other
⟨nβ, ϵ|nβ′, ϵ⟩= δβ,β′
(1.296)
and to the eigenvectors of A(ϵ) with other eigenvalues, and they remain so as
we take the limit
|nβ⟩= lim
ϵ→0 |nβ, ϵ⟩.
(1.297)
We may choose them as the orthogonal degenerate eigenvectors of A. Since one
always may ﬁnd a crooked hermitian matrix B that breaks any particular sym-
metry, it follows that every N ×N hermitian matrix A possesses N orthonormal
eigenvectors, which are complete in the vector space in which A acts. (Any N lin-
early independent vectors span their N-dimensional vector space, as explained
in section 1.9.)
Now let’s return to the ﬁrst question and again show that an N×N hermitian
matrix has N orthogonal eigenvectors. To do this, we’ll ﬁrst show that the space
of vectors orthogonal to an eigenvector |n⟩of a hermitian operator A
A|n⟩= λ|n⟩
(1.298)
is invariant under the action of A – that is, ⟨n|y⟩= 0 implies ⟨n|A|y⟩= 0. We use
successively the deﬁnition of A†, the hermiticity of A, the eigenvector equation
(1.298), the deﬁnition of the inner product, and the reality of the eigenvalues of
a hermitian matrix:
⟨n|A|y⟩= ⟨A†n|y⟩= ⟨An|y⟩= ⟨λn|y⟩= ¯λ⟨n|y⟩= λ⟨n|y⟩= 0.
(1.299)
Thus the space of vectors orthogonal to an eigenvector of a hermitian operator
A is invariant under the action of that operator.
Now a hermitian operator A acting on an N-dimensional vector space S is
represented by an N×N hermitian matrix, and so it has at least one eigenvector
|1⟩. The subspace of S consisting of all vectors orthogonal to |1⟩is an (N −1)-
dimensional vector space SN−1 that is invariant under the action of A. On this
space SN−1, the operator A is represented by an (N −1) × (N −1) hermitian
matrix AN−1. This matrix has at least one eigenvector |2⟩. The subspace of SN−1
consisting of all vectors orthogonal to |2⟩is an (N −2)-dimensional vector
space SN−2 that is invariant under the action of A. On SN−2, the operator A is
represented by an (N −2) × (N −2) hermitian matrix AN−2, which has at least
one eigenvector |3⟩. By construction, the vectors |1⟩, |2⟩, and |3⟩are mutually
orthogonal. Continuing in this way, we see that A has N orthogonal eigenvectors
|k⟩for k = 1, 2, . . . , N. Thus no hermitian matrix is defective.
47

LINEAR ALGEBRA
The N orthogonal eigenvectors |k⟩of an N × N matrix A can be normalized
and used to write the N × N identity operator I as
I =
N

k=1
|k⟩⟨k|.
(1.300)
On multiplying from the left by the matrix A, we ﬁnd
A = AI = A
N

k=1
|k⟩⟨k| =
N

k=1
ak|k⟩⟨k|,
(1.301)
which is the diagonal form of the hermitian matrix A. This expansion of A
as a sum over outer products of its eigenstates multiplied by their eigenvalues
exhibits the possible values ak of the physical quantity represented by the matrix
A when selective, nondestructive measurements |k⟩⟨k| of the quantity A are
done.
The hermitian matrix A is diagonal in the basis of its eigenstates |k⟩
Akj = ⟨k|A|j⟩= akδkj.
(1.302)
But in any other basis |αk⟩, the matrix A appears as
Akℓ= ⟨αk|A|αℓ⟩=
N

n=1
⟨αk|n⟩an⟨n|αℓ⟩.
(1.303)
The unitary matrix Ukn = ⟨αk|n⟩relates the matrix Akℓin an arbitrary basis
to its diagonal form A = UA(d)U† in which A(d) is the diagonal matrix A(d)
nm =
an δnm. An arbitrary N×N hermitian matrix A can be diagonalized by a unitary
transformation.
A matrix that is real and symmetric is hermitian; so is one that is imagi-
nary and antisymmetric. A real, symmetric matrix R can be diagonalized by
an orthogonal transformation
R = O R(d)OT
(1.304)
in which the matrix O is a real unitary matrix, that is, an orthogonal matrix
(1.168).
Example 1.39 (The seesaw mechanism)
Suppose we wish to ﬁnd the eigenvalues
of the real, symmetric mass matrix
M =
0
m
m
M

(1.305)
in which m is an ordinary mass and M is a huge mass. The eigenvalues μ of
this hermitian mass matrix satisfy det (M −μI) = μ(μ −M) −m2 = 0 with
48

1.28 HERMITIAN MATRICES
solutions μ± =

M ±

M2 + 4m2

/2. The larger mass μ+ ≈M + m2/M is
approximately the huge mass M and the smaller mass μ−≈−m2/M is very
tiny. The physical mass of a fermion is the absolute value of its mass parameter,
here m2/M.
The product of the two eigenvalues is the constant μ+μ−= det M = −m2
so as μ−goes down, μ+ must go up. In 1975, Gell-Mann, Ramond, Slansky,
and Jerry Stephenson invented this “seesaw” mechanism as an explanation of
why neutrinos have such small masses, less than 1 eV/c2. If mc2 = 10 MeV, and
μ−c2 ≈0.01 eV, which is a plausible light-neutrino mass, then the rest energy of
the huge mass would be Mc2 = 107 GeV. This huge mass would point at new
physics, beyond the standard model. Yet the small masses of the neutrinos may
be related to the weakness of their interactions.
If we return to the orthogonal transformation (1.304) and multiply column
ℓof the matrix O and row ℓof the matrix O
T by

|R(d)
ℓ|, then we arrive at the
congruency transformation of Sylvester’s theorem
R = C ˆR(d)C
T
(1.306)
in which the diagonal entries ˆR(d)
ℓ
are either ±1 or 0 because the matrices C and
CT have absorbed the moduli |R(d)
ℓ|.
Example 1.40 (Equivalence principle)
If G is a real, symmetric 4 × 4 matrix
then there’s a real 4 × 4 matrix D = C
T−1 such that
Gd = DTGD =
⎛
⎜⎜⎝
g1
0
0
0
0
g2
0
0
0
0
g3
0
0
0
0
g4
⎞
⎟⎟⎠
(1.307)
in which the diagonal entries gi are ±1 or 0. Thus there’s a real 4 × 4 matrix D
that casts the real nonsingular symmetric metric gik of space-time at any given
point into the diagonal metric ηjℓof ﬂat space-time by the congruence
gd = DTgD =
⎛
⎜⎜⎝
−1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
⎞
⎟⎟⎠= η.
(1.308)
Usually one needs different Ds at different points. Since one can implement the
congruence by changing coordinates, it follows that in any gravitational ﬁeld,
one may choose free-fall coordinates in which all physical laws take the same
form as in special relativity without acceleration or gravitation at least over
suitably small volumes of space-time (section 11.39).
49

LINEAR ALGEBRA
1.29 Normal matrices
The largest set of matrices that can be diagonalized by a unitary transformation
is the set of normal matrices. These are square matrices that commute with their
adjoints
[A, A†] = AA† −A†A = 0.
(1.309)
This broad class of matrices includes not only hermitian matrices but also
unitary matrices since
[U, U†] = UU† −U†U = I −I = 0.
(1.310)
To see why a normal matrix can be diagonalized by a unitary transformation,
let us consider an N×N normal matrix V which (since it is square (section 1.25))
has N eigenvectors |n⟩with eigenvalues vn
(V −vnI) |n⟩= 0.
(1.311)
The square of the norm (1.80) of this vector must vanish
∥(V −vnI) |n⟩∥2= ⟨n| (V −vnI)† (V −vnI) |n⟩= 0.
(1.312)
But since V is normal, we also have
⟨n| (V −vnI)† (V −vnI) |n⟩= ⟨n| (V −vnI) (V −vnI)† |n⟩.
(1.313)
So the square of the norm of the vector

V† −v∗
nI

|n⟩= (V −vnI)† |n⟩also
vanishes ∥

V† −v∗
nI

|n⟩∥2= 0, which tells us that |n⟩also is an eigenvector of
V† with eigenvalue v∗
n
V†|n⟩= v∗
n|n⟩
and so
⟨n|V = vn⟨n|.
(1.314)
If now |m⟩is an eigenvector of V with eigenvalue vm
V|m⟩= vm|m⟩= 0
(1.315)
then we have
⟨n|V|m⟩= vm⟨n|m⟩
(1.316)
and from (1.314)
⟨n|V|m⟩= vn⟨n|m⟩.
(1.317)
Subtracting (1.316) from (1.317), we get
(vn −vm) ⟨m|n⟩= 0,
(1.318)
which shows that any two eigenvectors of a normal matrix V with different
eigenvalues are orthogonal.
Usually, all N eigenvalues of an N × N normal matrix are different. In this
case, all the eigenvectors are orthogonal and may be individually normalized.
50

1.29 NORMAL MATRICES
But even when a set D of eigenvectors has the same (degenerate) eigenvalue,
one may use the argument (1.291–1.297) to ﬁnd a suitable set of orthonor-
mal eigenvectors with that eigenvalue. Thus every N × N normal matrix has
N orthonormal eigenvectors. It follows then from the argument of equations
(1.300–1.303) that every N × N normal matrix V can be diagonalized by an
N × N unitary matrix U
V = UV(d)U†
(1.319)
whose nth column Ukn = ⟨αk|n⟩is the eigenvector |n⟩in the arbitrary basis |αk⟩
of the matrix Vkℓ= ⟨αk|V|αℓ⟩as in (1.303).
Since the eigenstates |n⟩of a normal matrix A
A|n⟩= an|n⟩
(1.320)
are complete and orthonormal, we can write the identity operator I as
I =
N

n=1
|n⟩⟨n|.
(1.321)
The product AI is A itself, so
A = AI = A
N

n=1
|n⟩⟨n| =
N

n=1
an |n⟩⟨n|.
(1.322)
It follows therefore that if f is a function, then f (A) is
f (A) =
N

n=1
f (an) |n⟩⟨n|,
(1.323)
which is simpler than the expression (1.273) for an arbitrary nondefective
matrix. This is a good way to think about functions of normal matrices.
Example 1.41
How do we handle the operator exp(−iHt/¯h) that translates
states in time by t? The hamiltonian H is hermitian and so is normal. Its
orthonormal eigenstates |n⟩are the energy levels En
H|n⟩= En|n⟩.
(1.324)
So we apply (1.323) with A →H and get
e−iHt/¯h =
N

n=1
e−iEnt/¯h |n⟩⟨n|,
(1.325)
51

LINEAR ALGEBRA
which lets us compute the time evolution of any state |ψ⟩as
e−iHt/¯h|ψ⟩=
N

n=1
e−iEnt/¯h |n⟩⟨n|ψ⟩
(1.326)
if we know the eigenstates |n⟩and eigenvalues En of the hamiltonian H.
The determinant |V| of a normal matrix V satisﬁes the identities
|V| = exp [Tr(ln V)] ,
ln |V| = Tr(ln V),
and
δ ln |V| = Tr

V−1δV

.
(1.327)
1.30 Compatible normal matrices
Two normal matrices A and B that commute
[A, B ] ≡AB −BA = 0
(1.328)
are said to be compatible. Since these operators are normal, they have complete
sets of orthonormal eigenvectors. If |u⟩is an eigenvector of A with eigenvalue
z, then so is B|u⟩since
AB|u⟩= BA|u⟩= Bz|u⟩= z B|u⟩.
(1.329)
We have seen that any normal matrix A can be written as a sum (1.322) of outer
products
A =
N

n=1
|an⟩an⟨an|
(1.330)
of its orthonormal eigenvectors |an⟩, which are complete in the N-dimensional
vector space S on which A acts. Suppose now that the eigenvalues an of A are
nondegenerate, and that B is another normal matrix acting on S and that the
matrices A and B are compatible. Then in the basis provided by the eigenvectors
(or eigenstates) |an⟩of the matrix A, the matrix B must satisfy
0 = ⟨an|AB −BA|ak⟩= (an −ak) ⟨an|B|ak⟩,
(1.331)
which says that ⟨an|B|ak⟩is zero unless an = ak. Thus if the eigenvalues an of
the operator A are nondegenerate, then the operator B is diagonal
B = IBI =
N

n=1
|an⟩⟨an|B
N

k=1
|ak⟩⟨ak| =
N

n=1
|an⟩⟨an|B|an⟩⟨an|
(1.332)
52

1.30 COMPATIBLE NORMAL MATRICES
in the |an⟩basis. Moreover B maps each eigenket |ak⟩of A into
B|ak⟩=
N

n=1
|an⟩⟨an|B|an⟩⟨an|ak⟩=
N

n=1
|an⟩⟨an|B|an⟩δnk = ⟨ak|B|ak⟩|ak⟩,
(1.333)
which says that each eigenvector |ak⟩of the matrix A also is an eigenvec-
tor of the matrix B with eigenvalue ⟨ak|B|ak⟩. Thus two compatible normal
matrices can be simultaneously diagonalized if one of them has nondegenerate
eigenvalues.
If A’s eigenvalues an are degenerate, each eigenvalue an may have dn orthonor-
mal eigenvectors |an, k⟩for k = 1, . . . , dn. In this case, the matrix elements
⟨an, k|B|am, k′⟩of B are zero unless the eigenvalues are the same, an = am.
The matrix representing the operator B in this basis consists of square, dn × dn,
normal submatrices ⟨an, k|B|an, k′⟩arranged along its main diagonal; it is said
to be in block-diagonal form. Since each submatrix is a dn × dn normal matrix,
we may ﬁnd linear combinations |an, bk⟩of the degenerate eigenvectors |an, k⟩
that are orthonormal eigenvectors of both compatible operators
A|an, bk⟩= an|an, bk⟩
and
B|an, bk⟩= bk|an, bk⟩.
(1.334)
Thus one can simultaneously diagonalize any two compatible operators.
The converse also is true: if the operators A and B can be simultaneously
diagonalized as in (1.334), then they commute
AB|an, bk⟩= Abk|an, bk⟩= anbk|an, bk⟩= anB|an, bk⟩= BA|an, bk⟩
and so are compatible. Normal matrices can be simultaneously diagonalized if
and only if they are compatible, that is, if and only if they commute.
In quantum mechanics, compatible hermitian operators represent physical
observables that can be measured simultaneously to arbitrary precision (in
principle). A set of compatible hermitian operators {A, B, C, . . . } is said to
be complete if to every set of eigenvalues {an, bk, cℓ, . . . } there is only a single
eigenvector |an, bk, cℓ, . . .⟩.
Example 1.42 (Compatible photon observables)
The state of a photon is com-
pletely characterized by its momentum and its angular momentum about its
direction of motion. For a photon, the momentum operator P and the dot-
product J · P of the angular momentum J with the momentum form a complete
set of compatible hermitian observables. Incidentally, because its mass is zero,
the angular momentum J of a photon about its direction of motion can have
only two values ±¯h, which correspond to its two possible states of circular
polarization.
53

LINEAR ALGEBRA
Example 1.43 (Thermal density operator)
A density operator ρ is the most gen-
eral description of a quantum-mechanical system. It is hermitian, positive, and
of unit trace. Since it is hermitian, it can be diagonalized (section 1.28)
ρ =

n
|n⟩⟨n|ρ|n⟩⟨n|
(1.335)
and its eigenvalues ρn = ⟨n|ρ|n⟩are real. Each ρn is the probability that the
system is in the state |n⟩and so is nonnegative. The unit-trace rule

n
ρn = 1
(1.336)
ensures that these probabilities add up to one – the system is in some state.
The mean value of an operator F is the trace, ⟨F⟩= Tr(ρF). So the aver-
age energy E is the trace, E = ⟨H⟩= Tr(ρH). The entropy operator S is the
negative logarithm of the density operator multiplied by Boltzmann’s constant
S = −k ln ρ, and the mean entropy S is S = ⟨S⟩= −kTr(ρ ln ρ).
A density operator that describes a system in thermal equilibrium at a con-
stant temperature T is time independent and so commutes with the hamiltonian,
[ρ, H] = 0. Since ρ and H commute, they are compatible operators (1.328),
and so they can be simultaneously diagonalized. Each eigenstate |n⟩of ρ is an
eigenstate of H; its energy En is its eigenvalue, H|n⟩= En|n⟩.
If we have no information about the state of the system other than its mean
energy E, then we take ρ to be the density operator that maximizes the mean
entropy S while respecting the constraints
c1 =

n
ρn −1 = 0
and
c2 = Tr(ρH) −E = 0.
(1.337)
We introduce two Lagrange multipliers (section 1.23) and maximize the uncon-
strained function
L(ρ, λ1, λ2) = S −λ1 c1 −λ2 c2
= −k

n
ρn ln ρn −λ1

n
ρn −1

−λ2

n
ρnEn −E

(1.338)
by setting its derivatives with respect to ρn, λ1, and λ2 equal to zero
∂L
∂ρn
= −k (ln ρn + 1) −λ1 −λ2En = 0,
(1.339)
∂L
∂λ1
=

n
ρn −1 = 0,
(1.340)
∂L
∂λ2
=

n
ρnEn −E = 0.
(1.341)
The ﬁrst (1.339) of these conditions implies that
ρn = exp [−(λ1 + λ2En + k)/k] .
(1.342)
54

1.31 THE SINGULAR-VALUE DECOMPOSITION
We satisfy the second condition (1.340) by choosing λ1 so that
ρn =
exp(−λ2En/k)

n exp(−λ2En/k).
(1.343)
Setting λ2 = 1/T, we deﬁne the temperature T so that ρ satisﬁes the third
condition (1.341). Its eigenvalue ρn then is
ρn =
exp(−En/kT)

n exp(−En/kT).
(1.344)
In terms of the inverse temperature β ≡1/(kT), the density operator is
ρ =
e−βH
Tr

e−βH,
(1.345)
which is the Boltzmann distribution.
1.31 The singular-value decomposition
Every complex M × N rectangular matrix A is the product of an M × M uni-
tary matrix U, an M × N rectangular matrix  that is zero except on its main
diagonal, which consists of its nonnegative singular values Sk, and an N × N
unitary matrix V†
A = U  V†.
(1.346)
This singular-value decomposition (SVD) is a key theorem of matrix algebra.
Suppose A is a linear operator that maps vectors in an N-dimensional vec-
tor space VN into vectors in an M-dimensional vector space VM. The spaces
VN and VM will have inﬁnitely many orthonormal bases {|n, a⟩∈VN} and
{|m, b⟩∈VM} labeled by continuous parameters a and b. Each pair of bases
provides a resolution of the identity operator IN for VN and IM for VM
IN =
N

n=1
|n, a⟩⟨n, a|
and
IM =
M

m=1
|m, b⟩⟨m, b|.
(1.347)
These identity operators give us many ways of writing the linear operator A
A = IMAIN =
M

m=1
N

n=1
|m, b⟩⟨m, b|A|n, a⟩⟨n, a|,
(1.348)
in which the ⟨m, b|A|n, a⟩are the elements of a complex M × N matrix. The
singular-value decomposition of the linear operator A is a choice among all
these expressions for IN and IM that expresses A as
A =
min(M,N)

k=1
|Uk⟩Sk⟨Vk|
(1.349)
55

LINEAR ALGEBRA
in which the min(M, N) singular values Sk are nonnegative
Sk ≥0.
(1.350)
Let’s use the notation |An⟩≡A|n⟩for the image of a vector |n⟩in an
orthonormal basis {|n⟩} of VN under the map A. We seek a special orthonormal
basis {|n⟩} of VN that has the property that the vectors |An⟩are orthogonal. This
special basis {|n⟩} of VN is the set of N orthonormal eigenstates of the N × N
(nonnegative) hermitian operator A†A
A†A|n⟩= en|n⟩.
(1.351)
For since A|n′⟩= |An′⟩and A†A|n⟩= en|n⟩, it follows that
⟨An′|An⟩= ⟨n′|A†A|n⟩= en⟨n′|n⟩= enδn′n,
(1.352)
which shows that the vectors |An⟩are orthogonal and that their eigenval-
ues en = ⟨An|An⟩are nonnegative. This is the essence of the singular-value
decomposition.
If N = M, so that matrices ⟨m, b|A|n, a⟩representing the linear operator A
are square, then the N = M singular values Sn are the nonnegative square-roots
of the eigenvalues en
Sn = √en =

⟨An|An⟩≥0.
(1.353)
We therefore may normalize each vector |An⟩whose singular value Sn is
positive as
|mn⟩= 1
Sn
|An⟩
for
Sn > 0
(1.354)
so that the vectors {|mn⟩} with positive singular values are orthonormal
⟨mn′|mn⟩= δn′,n.
(1.355)
If only P < N of the singular values are positive, then we may augment this
set of P vectors {|mn⟩} with N −P = M −P new normalized vectors |mn′⟩
that are orthogonal to each other and to the P vectors deﬁned by (1.354) (with
positive singular values Sn > 0) so that the set of N = M vectors {|mn⟩, |mn′}⟩
are complete and orthonormal in the space VM=N.
If N > M, then A maps the N-dimensional space VN into the smaller
M-dimensional space VM, and so A must annihilate N −M basis vectors
A|n′⟩= 0
for
M < n′ ≤N.
(1.356)
In this case, there are only M singular values Sn of which Z may be zero.
The Z vectors |An⟩= A|n⟩with vanishing Sns are vectors of length zero;
for these values of n, the matrix A maps the vector |n⟩to the zero vector. If
there are more than N −M zero-length vectors |An⟩= A|n⟩, then we must
56

1.31 THE SINGULAR-VALUE DECOMPOSITION
replace the extra ones by new normalized vectors |mn′⟩that are orthogonal to
each other and to the vectors deﬁned by (1.354) so that we have M orthonor-
mal vectors in the augmented set {|mn⟩, |mn′⟩}. These vectors then form a basis
for VM.
When N ≤M, there are only N singular values Sn of which Z may be zero.
If Z of the Sns vanish, then one must add Q = Z + M −N new normalized
vectors |mn′⟩that are orthogonal to each other and to the vectors deﬁned by
(1.354)
⟨mn′|mn⟩= 1
Sn
⟨mn′|A|n⟩= 0
for
n′ > N −Z
and
Sn > 0
(1.357)
so that we have M orthonormal vectors in the augmented set {|mn⟩, |mn′⟩}.
These vectors then form a basis for VM.
In both cases, N > M and M ≥N, there are min(M, N) singular values, Z of
which may be zero. We may choose the new vectors {|mn′⟩} arbitrarily – as long
as the augmented set {|mn⟩, |mn′⟩} includes all the vectors deﬁned by (1.354) and
forms an orthonormal basis for VM.
We now have two special orthonormal bases: the N N-dimensional eigenvec-
tors |n⟩∈VN that satisfy (1.351) and the M M-dimensional vectors |mn⟩∈VM.
To make the singular-value decomposition of the linear operator A, we choose
as the identity operators IN for the N-dimensional space VN and IM for the
M-dimensional space VM the sums
IN =
N

n=1
|n⟩⟨n|
and
IM =
M

n′=1
|mn′⟩⟨mn′|.
(1.358)
The singular-value decomposition of A then is
A = IMAIN =
M

n′=1
|mn′⟩⟨mn′|A
N

n=1
|n⟩⟨n|.
(1.359)
There are min(M, N) singular values Sn, all nonnegative. For the positive sin-
gular values, equations (1.352 & 1.354) show that the matrix element ⟨mn′|A|n⟩
vanishes unless n′ = n
⟨mn′|A|n⟩= 1
Sn′ ⟨An′|An⟩= Sn′ δn′n.
(1.360)
For the Z vanishing singular values, equation (1.353) shows that A|n⟩= 0
and so
⟨mn′|A|n⟩= 0.
(1.361)
Thus only the min(M, N)−Z singular values that are positive contribute to the
singular-value decomposition (1.359). If N > M, then there can be at most M
nonzero eigenvalues en. If N ≤M, there can be at most N nonzero ens. The ﬁnal
57

LINEAR ALGEBRA
form of the singular-value decomposition then is a sum of dyadics weighted by
the positive singular values
A =
min(M,N)

n=1
|mn⟩Sn ⟨n| =
min(M,N)−Z

n=1
|mn⟩Sn ⟨n|.
(1.362)
The vectors |mn⟩and |n⟩respectively are the left and right singular vectors. The
nonnegative numbers Sn are the singular values.
The linear operator A maps the min(M, N) right singular vectors |n⟩into the
min(M, N) left singular vectors Sn|mn⟩scaled by their singular values
A|n⟩= Sn|mn⟩
(1.363)
and its adjoint A† maps the min(M, N) left singular vectors |mn⟩into the
min(M, N) right singular vectors |n⟩scaled by their singular values
A†|mn⟩= Sn|n⟩.
(1.364)
The N-dimensional vector space VN is the domain of the linear operator A.
If N > M, then A annihilates (at least) N −M of the basis vectors |n⟩. The
null space or kernel of A is the space spanned by the basis vectors |n⟩that A
annihilates. The vector space spanned by the left singular vectors |mn⟩with
nonzero singular values Sn > 0 is the range or image of A. It follows from the
singular-value decomposition (1.362) that the dimension N of the domain is
equal to the dimension of the kernel N −M plus that of the range M, a result
called the rank-nullity theorem.
Incidentally, the vectors |mn⟩are the eigenstates of the hermitian matrix
A A† as one may see from the explicit product of the expansion (1.362) with
its adjoint
A A† =
min(M,N)

n=1
|mn⟩Sn ⟨n|
min(M,N)

n′=1
|n′⟩Sn′ ⟨mn′|
=
min(M,N)

n=1
min(M,N)

n′=1
|mn⟩Sn δnn′Sn′ ⟨mn′|
=
min(M,N)

n=1
|mn⟩S2
n⟨mn|,
(1.365)
which shows that |mn⟩is an eigenvector of A A† with eigenvalue en = S2
n,
A A†|mn⟩= S2
n|mn⟩.
(1.366)
The SVD expansion (1.362) usually is written as a product of three explicit
matrices, A = UV†. The middle matrix  is an M × N matrix with the
min(M, N) singular values Sn = √en on its main diagonal and zeros elsewhere.
58

1.31 THE SINGULAR-VALUE DECOMPOSITION
By convention, one writes the Sn in decreasing order with the biggest Sn as entry
11. The ﬁrst matrix U and the third matrix V† depend upon the bases one uses
to represent the linear operator A. If these basis vectors are |αk⟩and |βℓ⟩, then
Akℓ= ⟨αk|A|βℓ⟩=
min(M,N)

n=1
⟨αk|mn⟩Sn ⟨n|βℓ⟩
(1.367)
so that the k, nth entry in the matrix U is Ukn = ⟨αk|mn⟩. The columns of the
matrix U are the left singular vectors of the matrix A:
⎛
⎜⎜⎜⎝
U1n
U2n
...
UMn
⎞
⎟⎟⎟⎠=
⎛
⎜⎜⎜⎝
⟨α1|mn⟩
⟨α2|mn⟩
...
⟨αM|mn⟩
⎞
⎟⎟⎟⎠.
(1.368)
Similarly, the n, ℓth entry of the matrix V† is

V†
n,ℓ= ⟨n|βℓ⟩. Thus Vℓ,n =

V T
n,ℓ= ⟨n|βℓ⟩∗= ⟨βℓ|n⟩. The columns of the matrix V are the right singular
vectors of the matrix A
⎛
⎜⎜⎜⎝
V1n
V2n
...
VNn
⎞
⎟⎟⎟⎠=
⎛
⎜⎜⎜⎝
⟨β1|n⟩
⟨β2|n⟩
...
⟨βN|n⟩
⎞
⎟⎟⎟⎠.
(1.369)
Since the columns of U and of V respectively are M and N orthonormal vec-
tors, both of these matrices are unitary, that is U†U = IM and V†V = IN are
the M × M and N × N identity matrices. The matrix form of the singular-value
decomposition of A then is
Akℓ=
M

m=1
N

n=1
UkmmnV†
nℓ=
min(M,N)

n=1
UknSnV†
nℓ
(1.370)
or in matrix notation
A = UV†.
(1.371)
The usual statement of the SVD theorem is: Every M × N complex matrix A
can be written as the matrix product of an M ×M unitary matrix U, an M ×N
matrix  that is zero except for its min(M, N) nonnegative diagonal elements,
and an N × N unitary matrix V†
A = U  V†.
(1.372)
The ﬁrst min(M, N) diagonal elements of S are the singular values Sk. They
are real and nonnegative. The ﬁrst min(M, N) columns of U and V are the left
and right singular vectors of A. The last max(N −M, 0) + Z columns (1.369)
59

LINEAR ALGEBRA
of the matrix V span the null space or kernel of A, and the ﬁrst min(M, N) −Z
columns (1.368) of the matrix U span the range of A.
Example 1.44 (Singular-value decomposition of a 2 × 3 matrix)
If A is
A =
0
1
0
1
0
1

(1.373)
then the positive hermitian matrix A†A is
A†A =
⎛
⎝
1
0
1
0
1
0
1
0
1
⎞
⎠.
(1.374)
The normalized eigenvectors and eigenvalues of A†A are
|1⟩=
1
√
2
⎛
⎝
1
0
1
⎞
⎠, e1 = 2;
|2⟩=
⎛
⎝
0
1
0
⎞
⎠, e2 = 1;
|3⟩=
1
√
2
⎛
⎝
−1
0
1
⎞
⎠, e3 = 0.
(1.375)
The third eigenvalue e3 had to vanish because A is a 3 × 2 matrix.
The vector A|1⟩is (as a row vector) |A1⟩= A|1⟩= (0,
√
2), and its norm is

⟨1|A†A|1⟩=
√
2, so the normalized vector |m1⟩is |m1⟩= |A1⟩/
√
2 = (0, 1).
Similarly, the vector |m2⟩is |m2⟩= A|2⟩/

⟨2|A†A|2⟩= (1, 0). The SVD of A
then is
A =
2

n=1
|mn⟩Sn⟨n| = UV†
(1.376)
where Sn = √en. The unitary matrices Uk,n = ⟨αk|mn⟩and Vk,n = ⟨βk|n⟩are
U =
0
1
1
0

and
V =
1
√
2
⎛
⎝
1
0
−1
0
√
2
0
1
0
1
⎞
⎠
(1.377)
and the diagonal matrix  is
 =
√
2
0
0
0
1
0

.
(1.378)
So ﬁnally the SVD of A = UV† is
A =
0
1
1
0
 √
2
0
0
0
1
0
 1
√
2
⎛
⎝
1
0
1
0
√
2
0
−1
0
1
⎞
⎠.
(1.379)
The null space or kernel of A is the set of vectors that are real multiples c
NA =
c
√
2
⎛
⎝
−1
0
1
⎞
⎠
(1.380)
of the third column of the matrix V displayed in (1.377).
60

1.31 THE SINGULAR-VALUE DECOMPOSITION
Example 1.45 (Matlab’s SVD)
Matlab’s command [U,S,V] = svd(X) performs
the singular-value decomposition of the matrix X. For instance
>> X = rand(3,3) + i*rand(3,3)
0.6551 + 0.2551i
0.4984 + 0.8909i
0.5853 + 0.1386i
X = 0.1626 + 0.5060i
0.9597 + 0.9593i
0.2238 + 0.1493i
0.1190 + 0.6991i
0.3404 + 0.5472i
0.7513 + 0.2575i
>> [U,S,V] = svd(X)
-0.3689 - 0.4587i
0.4056 - 0.2075i
0.4362 - 0.5055i
U = -0.3766 - 0.5002i -0.5792 - 0.2810i
0.0646 + 0.4351i
-0.2178 - 0.4626i
0.1142 + 0.6041i -0.5938 - 0.0901i
2.2335
0
0
S =
0
0.7172
0
0
0
0.3742
-0.4577
0.5749
0.6783
V = -0.7885 - 0.0255i -0.6118 - 0.0497i -0.0135 + 0.0249i
-0.3229 - 0.2527i
0.3881 + 0.3769i -0.5469 - 0.4900i.
The singular values are 2.2335, 0.7172, and 0.3742.
We may use the SVD to solve, when possible, the matrix equation
A |x⟩= |y⟩
(1.381)
for the N-dimensional vector |x⟩in terms of the M-dimensional vector |y⟩and
the M × N matrix A. Using the SVD expansion (1.362), we have
min(M,N)

n=1
|mn⟩Sn ⟨n|x⟩= |y⟩.
(1.382)
The orthonormality (1.355) of the vectors |mn⟩then tells us that
Sn ⟨n|x⟩= ⟨mn|y⟩.
(1.383)
If the singular value is positive Sn > 0 whenever ⟨mn|y⟩̸= 0, then we may divide
by the singular value to get ⟨n|x⟩= ⟨mn|y⟩/Sn and so ﬁnd the solution
|x⟩=
min(M,N)

n=1
⟨mn|y⟩
Sn
|n⟩.
(1.384)
But this solution is not always available or unique.
61

LINEAR ALGEBRA
For instance, if for some n′ the inner product ⟨mn′|y⟩̸= 0 while the singular
value Sn′ = 0, then there is no solution to equation (1.381). This problem often
occurs when M > N.
Example 1.46
Suppose A is the 3 × 2 matrix
A =
⎛
⎝
r1
p1
r2
p2
r3
p3
⎞
⎠
(1.385)
and the vector |y⟩is the cross-product |y⟩= L = r × p. Then no solution |x⟩
exists to the equation A|x⟩= |y⟩(unless r and p are parallel) because A|x⟩is a
linear combination of the vectors r and p while |y⟩= L is perpendicular to both
r and p.
Even when the matrix A is square, the equation (1.381) sometimes has no
solutions. For instance, if A is a square matrix that vanishes, A = 0, then (1.381)
has no solutions whenever |y⟩̸= 0. And when N > M, as in for instance
a
b
c
d
e
f
 ⎛
⎝
x1
x2
x3
⎞
⎠=
y1
y2

(1.386)
the solution (1.384) is never unique, for we may add to it any linear combination
of the vectors |n⟩that A annihilates for M < n ≤N
|x⟩=
min(M,N)

n=1
⟨mn|y⟩
Sn
|n⟩+
N

n=M+1
xn|n⟩.
(1.387)
These are the vectors |n⟩for M < n ≤N which A maps to zero since they do
not occur in the sum (1.362), which stops at n = min(M, N) < N.
Example 1.47 (The CKM matrix)
In the standard model, the mass matrix of
the d, s, and b quarks is a 3 × 3 complex, symmetric matrix M. Since M is
symmetric (M = MT), its adjoint is its complex conjugate, M† = M∗. So the
right singular vectors |n⟩are the eigenstates of M∗M as in (1.351)
M∗M|n⟩= S2
n|n⟩
(1.388)
and the left singular vectors |mn⟩are the eigenstates of MM∗as in (1.366)
MM∗|mn⟩=

M∗M
∗|mn⟩= S2
n|mn⟩.
(1.389)
Thus the left singular vectors are just the complex conjugates of the right singu-
lar vectors, |mn⟩= |n⟩∗. But this means that the unitary matrix V is the complex
conjugate of the unitary matrix U, so the SVD of M is (Autonne, 1915)
62

1.32 THE MOORE–PENROSE PSEUDOINVERSE
M = UUT.
(1.390)
The masses of the quarks then are the nonnegative singular values Sn along
the diagonal of the matrix . By redeﬁning the quark ﬁelds, one may make the
(CKM) matrix U real – except for a single complex phase, which causes a viola-
tion of charge-conjugation-parity (CP) symmetry. A similar matrix determines
the neutrino masses.
1.32 The Moore–Penrose pseudoinverse
Although a matrix A has an inverse A−1 if and only if it is square and has a
nonzero determinant, one may use the singular-value decomposition to make
a pseudoinverse A+ for an arbitrary M × N matrix A. If the singular-value
decomposition of the matrix A is
A = U  V†
(1.391)
then the Moore–Penrose pseudoinverse (Eliakim H. Moore, 1862–1932, Roger
Penrose, 1931–) is
A+ = V + U†
(1.392)
in which + is the transpose of the matrix  with every nonzero entry
replaced by its inverse (and the zeros left as they are). One may show that the
pseudoinverse A+ satisﬁes the four relations
A A+ A = A
and
A+ A A+ = A+,

A A+† = A A+
and

A+ A
† = A+ A
(1.393)
and that it is the only matrix that does so.
Suppose that all the singular values of the M × N matrix A are positive. In
this case, if A has more rows than columns, so that M > N, then the product
AA+ is the N × N identity matrix IN
A+A = V†+V = V†INV = IN
(1.394)
and AA+ is an M × M matrix that is not the identity matrix IM. If instead A
has more columns than rows, so that N > M, then AA+ is the M × M identity
matrix IM
AA+ = U+U† = UIMU† = IM
(1.395)
63

LINEAR ALGEBRA
but A+A is an N × N matrix that is not the identity matrix IN. If the matrix A
is square with positive singular values, then it has a true inverse A−1 which is
equal to its pseudoinverse
A−1 = A+.
(1.396)
If the columns of A are linearly independent, then the matrix A†A has an
inverse, and the pseudoinverse is
A+ =

A†A
−1
A†.
(1.397)
The solution (1.220) to the complex least-squares method used this pseudoin-
verse.
If the rows of A are linearly independent, then the matrix AA† has an inverse,
and the pseudoinverse is
A+ = A† 
AA†−1
.
(1.398)
If both the rows and the columns of A are linearly independent, then the matrix
A has an inverse A−1 which is its pseudoinverse
A−1 = A+.
(1.399)
Example 1.48 (The pseudoinverse of a 2 × 3 matrix)
The pseudoinverse A+ of
the matrix A
A =
0
1
0
1
0
1

(1.400)
with singular-value decomposition (1.379) is
A+ = V + U†
=
1
√
2
⎛
⎝
1
0
−1
0
√
2
0
1
0
1
⎞
⎠
⎛
⎝
1/
√
2
0
0
1
0
0
⎞
⎠
0
1
1
0

=
⎛
⎝
0
1/2
1
0
0
1/2
⎞
⎠,
(1.401)
which satisﬁes the four conditions (1.393). The product A A+ gives the 2 × 2
identity matrix
A A+ =
0
1
0
1
0
1
 ⎛
⎝
0
1/2
1
0
0
1/2
⎞
⎠=
1
0
0
1

,
(1.402)
64

1.33 THE RANK OF A MATRIX
which is an instance of (1.395). Moreover, the rows of A are linearly independent,
and so the simple rule (1.398) works:
A+ = A† 
AA†−1
=
⎛
⎝
1
0
0
1
1
0
⎞
⎠
⎛
⎝
0
1
0
1
0
1
 ⎛
⎝
1
0
0
1
1
0
⎞
⎠
⎞
⎠
−1
=
⎛
⎝
1
0
0
1
1
0
⎞
⎠
0
1
2
0
−1
=
⎛
⎝
1
0
0
1
1
0
⎞
⎠
0
1/2
1
0

=
⎛
⎝
0
1/2
1
0
0
1/2
⎞
⎠,
(1.403)
which is (1.401).
The columns of the matrix A are not linearly independent, however, and so
the simple rule (1.397) fails. Thus the product A+A
A+A =
⎛
⎝
0
1/2
1
0
0
1/2
⎞
⎠
0
1
0
1
0
1

= 1
2
⎛
⎝
1
0
1
0
2
0
1
0
1
⎞
⎠
(1.404)
is not the 3 × 3 identity matrix which it would be if (1.397) held.
1.33 The rank of a matrix
Four equivalent deﬁnitions of the rank R(A) of an M × N matrix A are:
1 the number of its linearly independent rows,
2 the number of its linearly independent columns,
3 the number of its nonzero singular values, and
4 the number of rows in its biggest square nonsingular submatrix.
A matrix of rank zero has no nonzero singular values and so is zero.
Example 1.49 (Rank)
The 3 × 4 matrix
A =
⎛
⎝
1
0
1
−2
2
2
0
2
4
3
1
1
⎞
⎠
(1.405)
has three rows, so its rank can be at most 3. But twice the ﬁrst row added to
thrice the second row equals twice the third row or
2r1 + 3r2 −2r3 = 0
(1.406)
so R(A) ≤2. The ﬁrst two rows obviously are not parallel, so they are linearly
independent. Thus the number of linearly independent rows of A is 2, and so A
has rank 2.
65

LINEAR ALGEBRA
1.34 Software
Free, high-quality software for virtually all numerical problems in linear alge-
bra are available in LAPACK – the Linear Algebra PACKage. The FORTRAN
version is available at the web-site www.netlib.org/lapack/ and the C++ version
at math.nist.gov/tnt/.
Matlab is a superb commercial program for numerical problems. A free
GNU version of it is available at www.gnu.org/software/octave/. Maple and
Mathematica are good commercial programs for symbolic problems.
1.35 The tensor/direct product
The tensor product (also called the direct product) is simple, but it can confuse
students if they see it for the ﬁrst time in a course on quantum mechanics.
The tensor product is used to describe composite systems, such as an angular
momentum composed of orbital and spin angular momenta.
If A is an M × N matrix with elements Aij and  is a K × L matrix with
elements αβ, then their direct product C = A ⊗ is an MK × NL matrix with
elements Ciα,jβ = Aij αβ. This direct-product matrix A ⊗ maps the vector
Vjβ into the vector
Wiα =
N

j=1
L

β=1
Ciα,jβ Vjβ =
N

j=1
L

β=1
Aij αβ Vjβ.
(1.407)
In this sum, the second indices of A and  match those of the vector V. The
most important case is when both A and  are square matrices, as will be their
product C = A ⊗. We’ll focus on this case in the rest of this section.
The key idea here is that the direct product is a product of two operators that
act on two different spaces. The operator A acts on the space S spanned by the
N kets |i⟩, and the operator  acts on the space  spanned by the K kets |α⟩.
Let us assume that both operators map into these spaces, so that we may write
them as
A = ISAIS =
N

i,j=1
|i⟩⟨i|A|j⟩⟨j|
(1.408)
and as
 = II =
K

α,β=1
|α⟩⟨α||β⟩⟨β|.
(1.409)
Then the direct product C = A ⊗
C = A ⊗ =
N

i,j=1
K

α,β=1
|i⟩⊗|α⟩⟨i|A|j⟩⟨α||β⟩⟨j| ⊗⟨β|
(1.410)
66

1.35 THE TENSOR/DIRECT PRODUCT
acts on the direct product of the two vector spaces S ⊗, which is spanned by
the direct-product kets |i, α⟩= |i⟩|α⟩= |i⟩⊗|α⟩.
In general, the direct-product space S ⊗ is much bigger than the spaces S
and . For although S ⊗ is spanned by the direct-product kets |i⟩⊗|α⟩, most
vectors in the space S ⊗ are of the form
|ψ⟩=
N

i=1
K

α=1
ψ(i, α)|i⟩⊗|α⟩
(1.411)
and not the direct product |s⟩⊗|σ⟩of a pair of vectors |s⟩∈S and |σ⟩∈
|s⟩⊗|σ⟩=
 N

i=1
si|i⟩

⊗
 K

α=1
σα|α⟩

=
N

i=1
K

α=1
siσα|i⟩⊗|α⟩.
(1.412)
Using the simpler notation |i, α⟩for |i⟩⊗|α⟩, we may write the action of the
direct-product operator A ⊗ on the state
|ψ⟩=
N

i=1
K

α=1
|i, α⟩⟨i, α|ψ⟩
(1.413)
as
(A ⊗)|ψ⟩=
N

i,j=1
K

α,β=1
|i, α⟩⟨i|A|j⟩⟨α||β⟩⟨j, β|ψ⟩.
(1.414)
Example 1.50 (States of the hydrogen atom)
Suppose the states |n, ℓ, m⟩are the
eigenvectors of the hamiltonian H, the square L2 of the orbital angular momen-
tum L, and the third component of the orbital angular momentum L3 for a
hydrogen atom without spin:
H|n, ℓ, m⟩= En|n, ℓ, m⟩,
L2|n, ℓ, m⟩= ¯h2ℓ(ℓ+ 1)|n, ℓ, m⟩,
L3|n, ℓ, m⟩= ¯hm|n, ℓ, m⟩.
(1.415)
Suppose the states |σ⟩for σ = ± are the eigenstates of the third component S3
of the operator S that represents the spin of the electron
S3|σ⟩= σ ¯h
2|σ⟩.
(1.416)
Then the direct- or tensor-product states
|n, ℓ, m, σ⟩≡|n, ℓ, m⟩⊗|σ⟩≡|n, ℓ, m⟩|σ⟩
(1.417)
67

LINEAR ALGEBRA
represent a hydrogen atom including the spin of its electron. They are eigenvec-
tors of all four operators H, L2, L3, and S3:
H|n, ℓ, m, σ⟩= En|n, ℓ, m, σ⟩,
L2|n, ℓ, m, σ⟩= ¯h2ℓ(ℓ+ 1)|n, ℓ, m, σ⟩,
L3|n, ℓ, m, σ⟩= ¯hm|n, ℓ, m, σ⟩,
S3|n, ℓ, m, σ⟩= σ ¯h|n, ℓ, m, σ⟩.
(1.418)
Suitable linear combinations of these states are eigenvectors of the square J2 of
the composite angular momentum J = L + S as well as of J3, L3, and S3.
Example 1.51 (Adding two spins)
The smallest positive value of angular
momentum is ¯h/2. The spin-one-half angular momentum operators S are
represented by three 2 × 2 matrices
Sa = ¯h
2 σa
(1.419)
in which the σa are the Pauli matrices
σ1 =
0
1
1
0

,
σ2 =
0
−i
i
0

,
and
σ3 =
1
0
0
−1

.
(1.420)
Consider two spin operators S(1) and S(2) acting on two spin-one-half systems.
The states |±⟩1 are eigenstates of S(1)
3 , and the states |±⟩2 are eigenstates of S(2)
3
S(1)
3 |±⟩1 = ± ¯h
2|±⟩1
and
S(2)
3 |±⟩2 = ± ¯h
2|±⟩2.
(1.421)
Then the direct-product states |±, ±⟩= |±⟩1|±⟩2 = |±⟩1 ⊗|±⟩2 are eigenstates
of both S(1)
3
and S(2)
3
S(1)
3 |±, s2⟩= ± ¯h
2 |+, s2⟩
and
S(2)
3 |s1, ±⟩= ± ¯h
2 |s1, ±⟩.
(1.422)
These states also are eigenstates of the third component of the spin operator of
the combined system
S3 = S(1)
3
+ S(2)
3 ,
that is
S3|s1, s2⟩= ¯h
2 (s1 + s2) |s1, s2⟩.
(1.423)
Thus S3|+, +⟩= ¯h|+, +⟩, and S3|−, −⟩= −¯h|−, −⟩, while S3|+, −⟩= 0 and
S3|−, +⟩= 0.
Now let’s consider the effect of the operator S2
1 on the state | + +⟩
S2
1| + +⟩=

S(1)
1
+ S(2)
1
2
| + +⟩= ¯h2
4

σ (1)
1
+ σ (2)
1
2
| + +⟩
= ¯h2
2

1 + σ (1)
1 σ (2)
1

| + +⟩= ¯h2
2

| + +⟩+ σ (1)
1 |+⟩σ (2)
1 |+⟩

= ¯h2
2 (| + +⟩+ | −−⟩) .
(1.424)
The rest of this example will be left to exercise 1.36.
68

1.37 CORRELATION FUNCTIONS
1.36 Density operators
A general quantum-mechanical system is represented by a density operator ρ
that is hermitian ρ† = ρ, of unit trace Trρ = 1, and positive ⟨ψ|ρ|ψ⟩≥0 for
all kets |ψ⟩.
If the state |ψ⟩is normalized, then ⟨ψ|ρ|ψ⟩is the nonnegative probability
that the system is in that state. This probability is real because the density matrix
is hermitian. If {|n⟩} is any complete set of orthonormal states,
I =

n
|n⟩⟨n|,
(1.425)
then the probability that the system is in the state |n⟩is
pn = ⟨n|ρ|n⟩= Tr (ρ|n⟩⟨n|) .
(1.426)
Since Trρ = 1, the sum of these probabilities is unity

n
pn =

n
⟨n|ρ|n⟩= Tr

ρ

n
|n⟩⟨n|

= Tr (ρI) = Trρ = 1.
(1.427)
A system that is measured to be in a state |n⟩cannot simultaneously be mea-
sured to be in an orthogonal state |m⟩. The probabilities sum to unity because
the system must be in some state.
Since the density operator ρ, is hermitian, it has a complete, orthonormal set
of eigenvectors |k⟩, all of which have nonnegative eigenvalues ρk
ρ|k⟩= ρk|k⟩.
(1.428)
They afford for it the expansion
ρ =
N

k=1
ρk|k⟩⟨k|
(1.429)
in which the eigenvalue ρk is the probability that the system is in the state |k⟩.
1.37 Correlation functions
We can deﬁne two Schwarz inner products for a density matrix ρ. If |f ⟩and |g⟩
are two states, then the inner product
(f , g) ≡⟨f |ρ|g⟩
(1.430)
for g = f is nonnegative, (f , f ) = ⟨f |ρ|f ⟩≥0, and satisﬁes the other conditions
(1.73, 1.74, & 1.76) for a Schwarz inner product.
69

LINEAR ALGEBRA
The second Schwarz inner product applies to operators A and B and is
deﬁned (Titulaer and Glauber, 1965) as
(A, B) = Tr

ρA†B

= Tr

BρA†
= Tr

A†Bρ

.
(1.431)
This inner product is nonnegative when A = B and obeys the other rules (1.73,
1.74, & 1.76) for a Schwarz inner product.
These two degenerate inner products are not inner products in the strict sense
of (1.73–1.79), but they are Schwarz inner products, and so (1.92–1.93) they
satisfy the Schwarz inequality (1.93)
( f , f )(g, g) ≥|( f , g)|2.
(1.432)
Applied to the ﬁrst, vector, Schwarz inner product (1.430), the Schwarz
inequality gives
⟨f |ρ|f ⟩⟨g|ρ|g⟩≥|⟨f |ρ|g⟩|2,
(1.433)
which is a useful property of density matrices. Application of the Schwarz
inequality to the second, operator, Schwarz inner product (1.431) gives
(Titulaer and Glauber, 1965)
Tr

ρA†A

Tr

ρB†B

≥
Tr

ρA†B

2
.
(1.434)
The operator Ei(x) that represents the ith component of the electric ﬁeld at
the point x is the hermitian sum of the “positive-frequency” part E(+)
i
(x) and
its adjoint E(−)
i
(x) = (E(+)
i
(x))†
Ei(x) = E(+)
i
(x) + E(−)
i
(x).
(1.435)
Glauber has deﬁned the ﬁrst-order correlation function G(1)
ij (x, y) as (Glauber,
1963b)
G(1)
ij (x, y) = Tr

ρE(−)
i
(x)E(+)
j
(y)

(1.436)
or in terms of the operator inner product (1.431) as
G(1)
ij (x, y) =

E(+)
i
(x), E(+)
j
(y)

.
(1.437)
By setting A = E(+)
i
(x), etc., it follows then from the Schwarz inequality (1.434)
that the correlation function G(1)
ij (x, y) is bounded by (Titulaer and Glauber,
1965)
|G(1)
ij (x, y)|2 ≤G(1)
ii (x, x)G(1)
jj (y, y).
(1.438)
Interference fringes are sharpest when this inequality is saturated:
|G(1)
ij (x, y)|2 = G(1)
ii (x, x)G(1)
jj (y, y),
(1.439)
70

EXERCISES
which can occur only if the correlation function G(1)
ij (x, y) factorizes (Titulaer
and Glauber, 1965)
G(1)
ij (x, y) = E∗
i (x)Ej(y)
(1.440)
as it does when the density operator is an outer product of coherent states
ρ = |{αk}⟩⟨{αk}|,
(1.441)
which are eigenstates of E(+)
i
(x) with eigenvalue Ei(x) (Glauber, 1963b, a)
E(+)
i
(x)|{αk}⟩= Ei(x)|{αk}⟩.
(1.442)
The higher-order correlation functions
G(n)
i1...i2n(x1 . . . x2n) = Tr

ρE(−)
i1 (x1) . . . E(−)
in (xn)E(+)
in+1(xn+1) . . . E(+)
i2n (xn)

(1.443)
satisfy similar inequalities (Glauber, 1963b), which also follow from the
Schwarz inequality (1.434).
Exercises
1.1
Why is the most complicated function of two Grassmann numbers a polyno-
mial with at most four terms as in (1.12)?
1.2
Derive the cyclicity (1.24) of the trace from (1.23).
1.3
Show that (AB) T = BTAT, which is (1.26).
1.4
Show that a real hermitian matrix is symmetric.
1.5
Show that (AB)† = B†A†, which is (1.29).
1.6
Show that the matrix (1.40) is positive on the space of all real 2-vectors but
not on the space of all complex 2-vectors.
1.7
Show that the two 4 × 4 matrices (1.45) satisfy Grassmann’s algebra (1.11) for
N = 2.
1.8
Show that the operators ai = θi deﬁned in terms of the Grassmann matrices
(1.45) and their adjoints a†
i = θ†
i satisfy the anticommutation relations (1.46)
of the creation and annihilation operators for a system with two fermionic
states.
1.9
Derive (1.64) from (1.61–1.63).
1.10 Fill in the steps leading to the formulas (1.69) for the vectors b′
1 and b′
2 and
the formula (1.70) for the matrix a′.
1.11 Show that the antilinearity (1.76) of the inner product follows from its ﬁrst
two properties (1.73 & 1.74).
1.12 Show that the Minkowski product (x, y) = x0y0 −x · y of two 4-vectors x and
y is an inner product that obeys the rules (1.73, 1.74, and 1.79).
1.13 Show that if f = 0, then the linearity (1.74) of the inner product implies that
(f , f ) and (g, f ) vanish.
71

LINEAR ALGEBRA
1.14 Show that the condition (1.75) of being positive deﬁnite implies nondegener-
acy (1.79).
1.15 Show that the nonnegativity (1.77) of the Schwarz inner product implies the
condition (1.78). Hint: the inequality (f −λg, f −λg) ≥0 must hold for every
complex λ and for all vectors f and g.
1.16 Show that the inequality (1.97) follows from the Schwarz inequality (1.96).
1.17 Show that the inequality (1.99) follows from the Schwarz inequality (1.98).
1.18 Use the Gram–Schmidt method to ﬁnd orthonormal linear combinations of
the three vectors
s1 =
⎛
⎝
1
0
0
⎞
⎠,
s2 =
⎛
⎝
1
1
0
⎞
⎠,
s3 =
⎛
⎝
1
1
1
⎞
⎠.
(1.444)
1.19 Now use the Gram–Schmidt method to ﬁnd orthonormal linear combina-
tions of the same three vectors but in a different order
s′
1 =
⎛
⎝
1
1
1
⎞
⎠,
s′
2 =
⎛
⎝
1
1
0
⎞
⎠,
s′
3 =
⎛
⎝
1
0
0
⎞
⎠.
(1.445)
Did you get the same orthonormal vectors as in the previous exercise?
1.20 Derive the linearity (1.120) of the outer product from its deﬁnition (1.119).
1.21 Show that a linear operator A that is represented by a hermitian matrix (1.155)
in an orthonormal basis satisﬁes (g, A f ) = (A g, f ).
1.22 Show that a unitary operator maps one orthonormal basis into another.
1.23 Show that the integral (1.170) deﬁnes a unitary operator that maps the state
|x′⟩to the state |x′ + a⟩.
1.24 For the 2 × 2 matrices
A =
1
2
3
−4

and
B =
2
−1
4
−3

(1.446)
verify equations (1.202–1.204).
1.25 Derive the least-squares solution (1.220) for complex A, x, and y when the
matrix A†A is positive.
1.26 Show that the eigenvalues λ of a unitary matrix are unimodular, that is,
|λ| = 1.
1.27 What are the eigenvalues and eigenvectors of the two defective matrices
(1.252)?
1.28 Use (1.267) to derive expression (1.268) for the 2 × 2 rotation matrix
exp(−iθ · σ/2).
1.29 Compute the characteristic equation for the matrix −iθ · J in which the
generators are (Jk)ij = iϵikj and ϵijk is totally antisymmetric with ϵ123 = 1.
1.30 Show that the sum of the eigenvalues of a normal antisymmetric matrix
vanishes.
1.31 Use the characteristic equation of exercise 1.29 to derive identities (1.271) and
(1.272) for the 3×3 real orthogonal matrix exp(−iθ · J).
72

EXERCISES
1.32 Consider the 2 × 3 matrix A
A =

1
2
3
−3
0
1

.
(1.447)
Perform the singular value decomposition A = USVT, where VT is the trans-
pose of V. Find the singular values and the real orthogonal matrices U and
V. Students may use Lapack, Octave, Matlab, Maple or any other program to
do this exercise.
1.33 Consider the 6 × 9 matrix A with elements
Aj,k = x + xj + i(y −yk)
(1.448)
in which x = 1.1 and y = 1.02. Find the singular values, and the ﬁrst left and
right singular vectors. Students may use Lapack, Octave, Matlab, Maple or
any other program to do this exercise.
1.34 Show that the totally antisymmetric Levi-Civita symbol ϵijk satisﬁes the useful
relation
3

i=1
ϵijk ϵinm = δjn δkm −δjm δkn.
(1.449)
1.35 Consider the hamiltonian
H = 1
2 ¯hωσ3
(1.450)
where σ3 is deﬁned in (1.420). The entropy S of this system at temperature T is
S = −kTr [ρ ln(ρ)]
(1.451)
in which the density operator ρ is
ρ =
e−H/(kT)
Tr

e−H/(kT).
(1.452)
Find expressions for the density operator ρ and its entropy S.
1.36 Find the action of the operator S2 =

S(1) + S(2)2
deﬁned by (1.419) on the
four states | ± ±⟩and then ﬁnd the eigenstates and eigenvalues of S2 in the
space spanned by these four states.
1.37 A system that has three fermionic states has three creation operators a†
i and
three annihilation operators ak which satisfy the anticommutation relations
{ai, a†
k} = δik and {ai, ak} = {a†
i , a†
k} = 0 for i, k = 1, 2, 3. The eight states
of the system are |v, u, t⟩≡(a†
3)t(a†
2)u(a†
1)v|0, 0, 0⟩. We can represent them by
eight 8-vectors, each of which has seven 0s with a 1 in position 5v+3u+t. How
big should the matrices that represent the creation and annihilation operators
be? Write down the three matrices that represent the three creation operators.
1.38 Show that the Schwarz inner product (1.430) is degenerate because it can
violate (1.79) for certain density operators and certain pairs of states.
73

LINEAR ALGEBRA
1.39 Show that the Schwarz inner product (1.431) is degenerate because it can
violate (1.79) for certain density operators and certain pairs of operators.
1.40 The coherent state |{αk}⟩is an eigenstate of the annihilation operator ak
with eigenvalue αk for each mode k of the electromagnetic ﬁeld, ak|{αk}⟩=
αk|{αk}⟩. The positive-frequency part E(+)
i
(x) of the electric ﬁeld is a linear
combination of the annihilation operators
E(+)
i
(x) =

k
ak E(+)
i
(k) ei(kx−ωt).
(1.453)
Show that |{αk}⟩is an eigenstate of E(+)
i
(x) as in (1.442) and ﬁnd its eigenvalue
Ei(x).
74

2
Fourier series
2.1 Complex Fourier series
The phases exp(inx)/
√
2π, one for each integer n, are orthonormal on an
interval of length 2π
 2π
0
 eimx
√
2π
∗einx
√
2π
dx =
 2π
0
ei(n−m)x
2π
dx = δm,n
(2.1)
where δn,m = 1 if n = m, and δn,m = 0 if n ̸= m. So if a function f (x) is a sum of
these phases
f (x) =
∞

n=−∞
fn
einx
√
2π
(2.2)
then their orthonormality (2.1) gives the nth coefﬁcient fn as the integral
 2π
0
e−inx
√
2π
f (x) dx =
 2π
0
e−inx
√
2π
∞

m=−∞
fm
eimx
√
2π
dx =
∞

m=−∞
δn,m fm = fn
(2.3)
(Joseph Fourier, 1768–1830).
The Fourier series (2.2) is periodic with period 2π because the phases are
periodic with period 2π, exp(in(x + 2π)) = exp(inx). Thus even if the function
f (x) which we use in (2.3) to make the Fourier coefﬁcients fn is not periodic,
its Fourier series (2.2) will nevertheless be strictly periodic, as illustrated by
Figs. 2.2 & 2.4.
75

FOURIER SERIES
If the Fourier series (2.2) converges uniformly (section 2.7), then the term-
by-term integration implicit in the formula (2.3) for fn is permitted.
How is the Fourier series for the complex-conjugate function f ∗(x) related to
the series for f (x)? The complex conjugate of the Fourier series (2.2) is
f ∗(x) =
∞

n=−∞
f ∗
n
e−inx
√
2π
=
∞

n=−∞
f ∗
−n
einx
√
2π
(2.4)
so the coefﬁcients fn(f ∗) for f ∗(x) are related to those fn(f ) for f (x) by
fn(f ∗) = f ∗
−n(f ).
(2.5)
Thus if the function f (x) is real, then
fn(f ) = fn(f ∗) = f ∗
−n(f ).
(2.6)
Dropping all reference to the functions, we see that the Fourier coefﬁcients fn
for a real function f (x) satisfy
fn = f ∗
−n.
(2.7)
Example 2.1 (Fourier series by inspection)
The doubly exponential function
exp(exp(ix)) has the Fourier series
exp

eix
=
∞

n=0
1
n! einx
(2.8)
in which n! = n(n −1) . . . 1 is n-factorial with 0! ≡1.
Example 2.2 (Beats)
The sum of two sines f (x) = sin ω1x + sin ω2x of similar
frequencies ω1 ≈ω2 is the product (exercise 2.1)
f (x) = 2 cos 1
2(ω1 −ω2)x sin 1
2(ω1 + ω2)x
(2.9)
in which the ﬁrst factor cos 1
2(ω1 −ω2)x is the beat which modulates the second
factor sin 1
2(ω1 + ω2)x, as illustrated by Fig. 2.1.
76

2.3 WHERE TO PUT THE 2πS
0
1
2
3
4
5
6
7
8
9
10
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Beats
x
sin(ω1x) + sin(ω2x)
Figure 2.1
The curve sin ω1x + sin ω2x for ω1 = 30 and ω2 = 32.
2.2 The interval
In equations (2.1–2.3), we singled out the interval [0, 2π], but to represent a
periodic function with period 2π, we could have used any interval of length 2π,
such as the interval [ −π, π] or [r, r + 2π]
fn =
 r+2π
r
e−inx f (x) dx
√
2π
.
(2.10)
This integral is independent of its lower limit r as long as the function f (x) is
periodic with period 2π. The choice r = −π often is convenient. With this
choice of interval, the coefﬁcient fn is the integral (2.3) shifted by −π
fn =
 π
−π
e−inx f (x) dx
√
2π
.
(2.11)
But if the function f (x) is not periodic with period 2π, then the Fourier
coefﬁcients (2.10) do depend upon r.
2.3 Where to put the 2πs
In equations (2.2 & 2.3), we used the orthonormal functions exp(inx)/
√
2π,
and so we had factors of 1/
√
2π in both equations. If one gets tired of having
77

FOURIER SERIES
so many explicit square-roots, then one may set dn = fn/
√
2π and write (2.2)
and (2.3) as
f (x) =
∞

n=−∞
dn einx
and
dn = 1
2π
 2π
0
dx e−inx f (x).
(2.12)
One also may use the rules
f (x) = 1
2π
∞

n=−∞
cneinx
and
cn =
 π
−π
f (x)e−inx dx.
(2.13)
Example 2.3 (Fourier series for exp(−m|x|))
Let’s compute the Fourier series
for the real function f (x) = exp(−m|x|) on the interval (−π, π). Using (2.10)
for the shifted interval and the 2π-placement convention (2.12), we ﬁnd for the
coefﬁcient dn
dn =
 π
−π
dx
2π e−inx e−m|x|,
(2.14)
which we may split into the two pieces
dn =
 0
−π
dx
2π e(m−in)x +
 π
0
dx
2π e−(m+in)x.
(2.15)
After doing the integrals, we ﬁnd
dn = 1
π
m
m2 + n2

1 −(−1)n e−πm
.
(2.16)
Here, since m is real, dn = d∗
n, but also dn = d−n. So the coefﬁcients dn satisfy the
condition (2.7) that holds when the function f (x) is real, dn = d∗
−n. The Fourier
series for exp(−m|x|) with dn given by (2.16) is
e−m|x| =
∞

n=−∞
dneinx =
∞

n=−∞
1
π
m
m2 + n2

1 −(−1)n e−πm
einx
=
1
mπ

1 −e−πm
+ 2
∞

n=1
1
π
m
m2 + n2

1 −(−1)n e−πm
cos(nx).
(2.17)
In Fig. 2.2, the 10-term (dashes) Fourier series for m = 2 is plotted from
x = −2π to x = 2π. The function exp(−2|x|) itself is represented by a solid
line. Although it is not periodic, its Fourier series is periodic with period 2π.
The 10-term Fourier series represents the function exp(−2|x|) so well that the
100-term series would have been hard to distinguish from the function.
78

2.4 REAL FOURIER SERIES FOR REAL FUNCTIONS
−6
−4
−2
0
2
4
6
0
0.2
0.4
0.6
0.8
1
x
e−2|x|
Fourier series for e−2|x|
Figure 2.2
The 10-term (dashes) Fourier series (2.17) for the function exp(−2|x|)
on the interval (−π, π) are plotted from −2π to 2π. All Fourier series are periodic,
but the function exp(−2|x|) (solid) is not.
In what follows, we usually won’t bother to use different letters to distinguish
between the symmetric (2.2 & 2.3) and asymmetric (2.12 & 2.13) conventions on
the placement of the 2πs.
2.4 Real Fourier series for real functions
The Fourier series outlined above are simple and apply to functions that are
continuous and periodic – whether complex or real. If the function f (x) is real,
then by (2.7) d−n = d∗
n, whence d0 = d∗
0, so d0 is real. Thus the Fourier series
(2.12) for a real function f (x) is
f (x) = d0 +
∞

n=1
dn einx +
−1

n=−∞
dn einx
= d0 +
∞

n=1

dn einx + d−n e−inx
= d0 +
∞

n=1

dn einx + d∗
n e−inx
= d0 +
∞

n=1
dn (cos nx + i sin nx) + d∗
n (cos nx −i sin nx)
= d0 +
∞

n=1
(dn + d∗
n) cos nx + i(dn −d∗
n) sin nx.
(2.18)
79

FOURIER SERIES
Let’s write dn as
dn = 1
2 (an −ibn),
so that
an = dn + d∗
n
and
bn = i(dn −d∗
n).
(2.19)
Then the Fourier series (2.18) for a real function f (x) is
f (x) = a0
2 +
∞

n=1
an cos nx + bn sin nx.
(2.20)
What are the formulas for an and bn? By (2.19 & 2.12), the coefﬁcient an is
an =
 2π
0

e−inx f (x) + einx f ∗(x)
 dx
2π =
 2π
0

e−inx + einx
2
f (x) dx
2π
(2.21)
since the function f (x) is real. So the coefﬁcient an of cos nx in (2.20) is the
cosine integral of f (x)
an =
 2π
0
cos nx f (x) dx
π =
 π
−π
cos nx f (x) dx
π .
(2.22)
Similarly, (2.19 & 2.12) and the reality of f (x) imply that the coefﬁcient bn is the
sine integral of f (x)
bn =
 2π
0
i

e−inx −einx
2
f (x) dx
π =
 2π
0
sin nx f (x) dx
π =
 π
−π
sin nx f (x) dx
π .
(2.23)
The real Fourier series (2.20) and the cosine (2.22) and sine (2.23) integrals
for the coefﬁcients an and bn also follow from the orthogonality relations
 2π
0
sin mx sin nx dx =
 π
if n = m ̸= 0
0
otherwise,
(2.24)
 2π
0
cos mx cos nx dx =
⎧
⎨
⎩
π
if n = m ̸= 0
2π
if n = m = 0
0
otherwise, and
(2.25)
 2π
0
sin mx cos nx dx = 0,
(2.26)
which hold for integer values of n and m.
What if a function f (x) is not periodic? The Fourier series for a function
that is not periodic is itself strictly periodic. In such cases, the Fourier series
differs somewhat from the function near the ends of the interval and differs
markedly from it outside the interval, where the series but not the function is
periodic.
80

2.4 REAL FOURIER SERIES FOR REAL FUNCTIONS
Example 2.4 (The Fourier series for x2)
The function x2 is even and so the
integrals (2.23) for its sine Fourier coefﬁcients bn all vanish. Its cosine coefﬁcients
an are given by (2.22)
an =
 π
−π
cos nx f (x) dx
π =
 π
−π
cos nx x2 dx
π .
(2.27)
Integrating twice by parts, we ﬁnd for n ̸= 0
an = −2
n
 π
−π
x sin nx dx
π = (−1)n 4
n2
(2.28)
and
a0 =
 π
−π
x2 dx
π = 2π2
3 .
(2.29)
Equation (2.20) now gives for x2 the cosine Fourier series
x2 = a0
2 +
∞

n=1
an cos nx = π2
3 + 4
∞

n=1
(−1)n cos nx
n2
.
(2.30)
This series rapidly converges within the interval (−π, π) as shown in Fig. 2.3,
but not near its endpoints ±π.
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
−0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
x
x2
Fourier series for x2
Figure 2.3
The function x2 (solid) and its Fourier series of seven terms (dot dash)
and 20 terms (dashes). The Fourier series (2.30) for x2 quickly converges well inside
the interval (−π, π).
81

FOURIER SERIES
Example 2.5 (The Gibbs overshoot)
The function f (x) = x on the interval
(−π, π) is not periodic. So we expect trouble if we represent it as a Fourier series.
Since x is an odd function, equation (2.22) tells us that the coefﬁcients an all
vanish. By (2.23), the bns are
bn =
 π
−π
dx
π x sin nx = 2 (−1)n+1 1
n.
(2.31)
As shown in Fig. 2.4, the series
∞

n=1
2 (−1)n+1 1
n sin nx
(2.32)
differs by about 2π from the function f (x) = x for −3π < x < −π and for
π < x < 3π because the series is periodic while the function x isn’t.
Within the interval (−π, π), the series with 100 terms is very accurate except
for x ≳−π and x ≲π, where it overshoots by about 9% of the 2π discontinuity,
a defect called the Gibbs phenomenon or the Gibbs overshoot (J. Willard Gibbs,
1839–1903; incidentally Gibbs’s father successfully defended the Africans of the
schooner Amistad). Any time we use a Fourier series to represent an aperiodic
function, a Gibbs phenomenon will occur near the endpoints of the interval.
−6
−4
−2
0
2
4
6
−6
−4
−2
0
2
4
6
Fourier series for the aperiodic function x
x
−3
−2
−1
0
1
2
3
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
Gibbs overshoot for the function x
x
Overshoot
Figure 2.4
(top) The Fourier series (2.32) for the function x (solid line) with ten
terms (dots) and 100 terms (solid curve) for −2π < x < 2π. The Fourier series is
periodic, but the function x is not. (bottom) The differences between x and the ten-
term (dots) and the 100-term (solid curve) on (−π, π) exhibit a Gibbs overshoot of
about 9% at x ≳−π and at x ≲π.
82

2.5 STRETCHED INTERVALS
2.5 Stretched intervals
If the interval of periodicity is of length L instead of 2π, then we may use the
phases exp(i2πn/
√
L), which are orthonormal on the interval (0, L)
 L
0
dx

ei2πnx/L
√
L
∗ei2πmx/L
√
L
= δnm.
(2.33)
The Fourier series
f (x) =
∞

n=−∞
fn
ei2πnx/L
√
L
(2.34)
is periodic with period L. The coefﬁcient fn is the integral
fn =
 L
0
e−i2πnx/L
√
L
f (x) dx.
(2.35)
These relations (2.33–2.35) generalize to the interval [0, L] our earlier formulas
(2.1–2.3) for the interval [0, 2π].
If the function f (x) is periodic f (x ± L) = f (x) with period L, then we may
shift the domain of integration by any real number r
fn =
 L+r
r
e−i2πnx/L
√
L
f (x) dx.
(2.36)
An obvious choice is r = −L/2 for which (2.34) and (2.35) give
f (x) =
∞

n=−∞
fn
ei2πnx/L
√
L
and
fn =
 L/2
−L/2
e−i2πnx/L
√
L
f (x) dx.
(2.37)
If the function f (x) is real, then on the interval [0, L] in place of
equations (2.20, 2.22, & 2.23) one has
f (x) = a0
2 +
∞

n=1
an cos
2πnx
L

+ bn sin
2πnx
L

,
(2.38)
an = 2
L
 L
0
dx cos
2πnx
L

f (x),
(2.39)
and
bn = 2
L
 L
0
dx sin
2πnx
L

f (x).
(2.40)
83

FOURIER SERIES
The corresponding orthogonality relations, which follow from equations (2.24,
2.25, & 2.26), are:
 L
0
dx sin
2πmx
L

sin
2πnx
L

=
 L/2
if n = m ̸= 0
0
otherwise
(2.41)
 L
0
dx cos
2πmx
L

cos
2πnx
L

=
⎧
⎨
⎩
L/2
if n = m ̸= 0
L
if n = m = 0
0
otherwise
(2.42)
 L
0
dx sin
2πmx
L

cos
2πnx
L

= 0.
(2.43)
They hold for integer values of n and m, and they imply equations (2.38–2.40).
2.6 Fourier series in several variables
On the interval [−L, L], the Fourier-series formulas (2.34 & 2.35) are
f (x) =
∞

n=−∞
fn
eiπnx/L
√
2L
(2.44)
fn =
 L
−L
e−iπnx/L
√
2L
f (x) dx.
(2.45)
We may generalize these equations from a single variable to N variables x =
(x1, . . . , xN) with n · x = n1x1 + · · · + nNxN
f (x) =
∞

n1=−∞
· · ·
∞

nN=−∞
fn
eiπn·x/L
(2L)N/2
(2.46)
fn =
 L
−L
dx1 . . .
 L
−L
dxN
e−iπn·x/L
(2L)N/2 f (x).
(2.47)
2.7 How Fourier series converge
A Fourier series represents a function f (x) as the limit of a sequence of functions
fN(x) given by
fN(x) =
N

k=−N
fk
ei2πkx/L
√
L
in which
fk =
 L
0
f (x) e−i2πkx/L dx
√
L
.
(2.48)
Since the exponentials are periodic with period L, a Fourier series always is peri-
odic. So if the function f (x) is not periodic, then its Fourier series will represent
the periodic extension fp of f deﬁned by fp(x + nL) = f (x) for 0 ≤x ≤L.
84

2.7 HOW FOURIER SERIES CONVERGE
A sequence of functions fN(x) converges to a function f (x) on a (closed) inter-
val [a, b] if for every ϵ > 0 and each point a ≤x ≤b there exists an integer
N(ϵ, x) such that
 f (x) −fN(x)
 < ϵ
for all
N > N(ϵ, x).
(2.49)
If this holds for an N(ϵ) that is independent of x ∈[a, b], then the sequence of
functions fN(x) converges uniformly to f (x) on the interval [a, b].
A function f (x) is continuous on an open interval (a, b) if for every point a <
x < b the two limits
f (x −0) ≡
lim
0<ϵ→0 f (x −ϵ)
and
f (x + 0) ≡
lim
0<ϵ→0 f (x + ϵ)
(2.50)
agree; it also is continuous on the closed interval [a, b] if f (a + 0) = f (a) and
f (b −0) = f (b). A function continuous on [a, b] is bounded there.
If a sequence of continuous functions fN(x) converges uniformly to a func-
tion f (x) on a closed interval a ≤x ≤b, then we know that | fN(x) −f (x)| < ϵ
for N > N(ϵ), and so

 b
a
fN(x) dx −
 b
a
f (x) dx
 ≤
 b
a
 fN(x) −f (x)
 dx < (b −a)ϵ.
(2.51)
Thus one may integrate a uniformly convergent sequence of continuous func-
tions on a closed interval [a, b] term by term
lim
N→∞
 b
a
fN(x) dx =
 b
a
f (x) dx.
(2.52)
A function is piecewise continuous on [a, b] if it is continuous there except
for ﬁnite jumps from f (x −0) to f (x + 0) at a ﬁnite number of points x. At
such jumps, we shall deﬁne the periodically extended function fp to be the mean
fp(x) = [fp(x −0) + fp(x + 0)]/2.
Fourier’s convergence theorem (Courant, 1937, p. 439) The Fourier series of
a function f (x) that is piecewise continuous with a piecewise continuous ﬁrst
derivative converges to its periodic extension fp(x). This convergence is uniform
on every closed interval on which the function f (x) is continuous (and absolute
if the function f (x) has no discontinuities). Examples 2.11 and 2.12 illustrate
this result.
A function whose kth derivative is continuous is in class Ck. On the interval
[ −π, π], its Fourier coefﬁcients (2.13) are
fn =
 π
−π
f (x) e−inx dx.
(2.53)
85

FOURIER SERIES
If f is both periodic and in Ck, then one integration by parts gives
fn =
 π
−π
 d
dx
#
f (x) e−inx
−in
$
−f ′(x) e−inx
−in
%
dx =
 π
−π
f ′(x) e−inx
in
dx
and k integrations by parts give
fn =
 π
−π
f (k)(x) e−inx
(in)k dx
(2.54)
since the derivatives f (ℓ)(x) of a Ck periodic function also are periodic.
Moreover, if f (k+1) is piecewise continuous, then
fn =
 π
−π
 d
dx
#
f (k)(x)
e−inx
−(in)k+1
$
−f (k+1)(x)
e−inx
−(in)k+1
%
dx
=
 π
−π
f (k+1)(x) e−inx
(in)k+1 dx.
(2.55)
Since f (k+1)(x) is piecewise continuous on the closed interval [−π, π], it is
bounded there in absolute value by, let us say, M. So the Fourier coefﬁcients
of a Ck periodic function with f (k+1) piecewise continuous are bounded by
| fn| ≤
1
nk+1
 π
−π
| f (k+1)(x)| dx ≤2πM
nk+1 .
(2.56)
We often can carry this derivation one step further. In most simple exam-
ples, the piecewise continuous periodic function f (k+1)(x) actually is piecewise
continuously differentiable between its successive jumps at xj. In this case, the
derivative f (k+2)(x) is a piecewise continuous function plus a sum of a ﬁnite
number of delta functions with ﬁnite coefﬁcients. Thus we can integrate once
more by parts. If for instance the function f (k+1)(x) jumps J times between −π
and π by f (k+1)
j
, then the Fourier coefﬁcients are
fn =
 π
−π
f (k+2)(x) e−inx
(in)k+2 dx
=
J

j=1
 xj+1
xj
f (k+2)
s
(x) e−inx
(in)k+2 dx +
J

j=1
f (k+2)
j
e−inxj
√
2π (in)k+1
(2.57)
in which the subscript s means that we’ve separated out the delta functions. The
Fourier coefﬁcients then would be bounded by
|fn| ≤2πM
nk+2
(2.58)
in which M is related to the maximum absolute values of f (k+2)
s
(x) and of the
f (k+1)
j
. The Fourier series of periodic Ck functions converge very rapidly if k
is big.
86

2.7 HOW FOURIER SERIES CONVERGE
Example 2.6 (Fourier series of a C0 function)
The function deﬁned by
f (x) =
⎧
⎨
⎩
0,
−π ≤x < 0
x,
0 ≤x < π/2
π −x,
π/2 ≤x ≤π
(2.59)
is continuous on the interval [−π, π] and its ﬁrst derivative is piecewise continu-
ous on that interval. By (2.56), its Fourier coefﬁcients fn should be bounded by
M/n. In fact they are (exercise 2.8)
fn =
 π
−π
f (x)e−inx
dx
√
2π
= (−1)n+1
√
2π
(in −1)2
n2
(2.60)
bounded by 2√2/π/n2 in agreement with the stronger inequality (2.58).
Example 2.7 (Fourier series for a C1 function)
The function deﬁned by f (x) =
1 + cos 2x for |x| ≤π/2 and f (x) = 0 for |x| ≥π/2 has a periodic extension fp
that is continuous with a continuous ﬁrst derivative and a piecewise continuous
second derivative. Its Fourier coefﬁcients (2.53)
fn =
 π/2
−π/2
(1 + cos 2x)e−inx dx
√
2π
=
8 sin nπ/2
√
2π(4n −n3)
satisfy the inequalities (2.56) and (2.58) for k = 1.
Example 2.8 (The Fourier series for cos μx)
The Fourier series for the even
function f (x) = cos μx has only cosines with coefﬁcients (2.22)
an =
 π
−π
cos nx cos μx dx
π =
 π
0
[cos(μ + n)x + cos(μ −n)x]dx
π
= 1
π
#sin(μ + n)π
μ + n
+ sin(μ −n)π
μ −n
$
= 2
π
μ(−1)n
μ2 −n2 sin μπ.
(2.61)
Thus, whether or not μ is an integer, the series (2.20) gives us
cos μx = 2μ sin μπ
π
 1
2μ2 −
cos x
μ2 −12 + cos 2x
μ2 −22 −cos 3x
μ2 −32 + −· · ·

, (2.62)
which is continuous at x = ±π (Courant, 1937, chap. IX).
Example 2.9 (The sine as an inﬁnite product)
In our series (2.62) for cos μx,
we set x = π, divide by sin μπ, replace μ with x, and so ﬁnd for the cotangent
the expansion
cot πx = 2x
π
 1
2x2 +
1
x2 −12 +
1
x2 −22 +
1
x2 −32 + · · ·

(2.63)
87

FOURIER SERIES
or equivalently
cot πx −1
πx = −2x
π

1
12 −x2 +
1
22 −x2 +
1
32 −x2 + · · ·

.
(2.64)
For 0 ≤x ≤q < 1, the absolute value of the nth term on the right is less than
2q/(π(n2 −q2)). Thus this series converges uniformly on [0, x], and so we may
integrate it term by term. We ﬁnd (exercise 2.11)
π
 x
0

cot πt −1
πt

dt = ln sin πx
πx
=
∞

n=1
 x
0
−2t dt
n2 −t2 =
∞

n=1
ln

1 −x2
n2

.
(2.65)
Exponentiating, we get the inﬁnite-product formula
sin πx
πx
= exp
 ∞

n=1
ln

1 −x2
n2

=
∞

n=1

1 −x2
n2

(2.66)
for the sine, from which one can derive the inﬁnite product (exercise 2.12)
cos πx =
∞

n=1
⎛
⎜⎝1 −
x2

n −1
2
2
⎞
⎟⎠
(2.67)
for the cosine (Courant, 1937, chap. IX).
Fourier series can represent a much wider class of functions than those that
are continuous. If a function f (x) is square integrable on an interval [a, b], then
its N-term Fourier series fN(x) will converge to f (x) in the mean, that is
lim
N→∞
 b
a
dx| f (x) −fN(x)|2 = 0.
(2.68)
What happens to the convergence of a Fourier series if we integrate or
differentiate term by term? If we integrate the series
f (x) =
∞

n=−∞
fn
ei2πnx/L
√
L
,
(2.69)
then we get a series
F(x) =
 x
a
dx′f (x′) = f0
(x −a)
√
L
−i
√
L
2π
∞

n=−∞
fn
n

ei2πnx/L −ei2πna/L
(2.70)
that converges better because of the extra factor of 1/n. An integrated function
f (x) is smoother, and so its Fourier series converges better.
88

2.8 QUANTUM-MECHANICAL EXAMPLES
But if we differentiate the same series, then we get a series
f ′(x) = i 2π
L3/2
∞

n=−∞
n fn ei2πnx/L
(2.71)
that converges less well because of the extra factor of n. A differentiated
function is rougher, and so its Fourier series converges less well.
2.8 Quantum-mechanical examples
Suppose a particle of mass m is trapped in an inﬁnitely deep one-dimensional
square well of potential energy
V(x) =
 0
if 0 < x < L
∞
otherwise.
(2.72)
The hamiltonian operator is
H = −¯h2
2m
d2
dx2 + V(x),
(2.73)
in which ¯h is Planck’s constant divided by 2π. This tiny bit of action,
¯h = 1.055 × 10−34 J s, sets the scale at which quantum mechanics becomes
important; quantum-mechanical corrections to classical predictions often are
important in processes whose action is less than ¯h.
An eigenfunction ψ(x) of the hamiltonian H with energy E satisﬁes the
equation Hψ(x) = Eψ(x), which breaks into two simple equations:
−¯h2
2m
d2ψ(x)
dx2
= Eψ(x)
for
0 < x < L
(2.74)
and
−¯h2
2m
d2ψ(x)
dx2
+ ∞ψ(x) = Eψ(x)
for
x < 0
and for
x > L.
(2.75)
Every solution of these equations with ﬁnite energy E must vanish outside the
interval 0 < x < L. So we must ﬁnd solutions of the ﬁrst equation (2.74) that
satisfy the boundary conditions
ψ(x) = 0
for
x ≤0 and x ≥L.
(2.76)
For any integer n ̸= 0, the function
ψn(x) =
&
2
L sin
πnx
L

for
x ∈[0, L]
(2.77)
89

FOURIER SERIES
and ψn(x) = 0 for x /∈(0, L) satisﬁes the boundary conditions (2.76). When
inserted into equation (2.74)
−¯h2
2m
d2
dx2 ψn(x) = ¯h2
2m
nπ
L
2
ψn(x) = Enψn(x)
(2.78)
it reveals its energy to be En = (nπ ¯h/L)2/2m.
These eigenfunctions ψn(x) are complete in the sense that they span the space
of all functions f (x) that are square-integrable on the interval (0, L) and vanish
at its endpoints. They provide for such functions the sine Fourier series
f (x) =
∞

n=1
fn
&
2
L sin
πnx
L

,
(2.79)
which also is the Fourier series for a function that is odd f (−x) = −f (x) on the
interval (−L, L) and zero at both ends.
Example 2.10 (Time evolution of an initially piecewise continuous wave func-
tion)
Suppose now that at time t = 0 the particle is conﬁned to the middle half
of the well with the square-wave wave function
ψ(x, 0) =
&
2
L
for
L
4 < x < 3L
4
(2.80)
and zero otherwise. This piecewise continuous wave function is discontinuous at
x = L/4 and at x = 3L/4. Since the functions ⟨x|n⟩= ψn(x) are orthonormal
on [0, L]
 L
0
dx
&
2
L sin
πnx
L
 &
2
L sin
πmx
L

= δnm,
(2.81)
the coefﬁcients fn in the Fourier series
ψ(x, 0) =
∞

n=1
fn
&
2
L sin
πnx
L

(2.82)
are the inner products
fn = ⟨n|ψ, 0⟩=
 L
0
dx
&
2
L sin
πnx
L

ψ(x, 0).
(2.83)
They are proportional to 1/n in accord with (2.58)
fn = 2
L
 3L/4
L/4
dx sin
πnx
L

= 2
πn
#
cos
πn
4

−cos
3πn
4
$
.
(2.84)
Figure 2.5 plots the wave function ψ(x, 0) (2.80, straight solid lines) and
its ten-term (solid curve) and 100-term (dashes) Fourier series (2.82) for an
interval of length L = 2. Gibbs’s overshoot reaches 1.093 at x = 0.52 for
90

2.8 QUANTUM-MECHANICAL EXAMPLES
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
x
ψ (x , 0)
Fourier series for a piecewise continuous wave function
Figure 2.5
The piecewise continuous wave function ψ(x, 0) for L = 2 (2.80,
straight solid lines) and its Fourier series (2.82) with ten terms (solid curve) and
100 terms (dashes). Gibbs overshoots occur near the discontinuities at x = 1/2 and
x = 3/2.
100 terms and 1.0898 at x = 0.502 for 1000 terms (not shown), amounting
to about 9% of the unit discontinuity at x = 1/2. A similar overshoot occurs
at x = 3/2.
How does ψ(x, 0) evolve with time? Since ψn(x), the Fourier component
(2.77), is an eigenfunction of H with energy En, the time-evolution operator
U(t) = exp(−iHt/¯h) takes ψ(x, 0) into
ψ(x, t) = e−iHt/¯h ψ(x, 0) =
∞

n=1
fn
&
2
L sin
πnx
L

e−iEnt/¯h.
(2.85)
Because En = (nπ ¯h)2/2m, the wave function at time t is
ψ(x, t) =
∞

n=1
fn
&
2
L sin
πnx
L

e−i¯h(nπ)2t/(2mL2).
(2.86)
It is awkward to plot complex functions, so Fig. 2.6 displays the probability
distributions P(x, t) = |ψ(x, t)|2 of the 1000-term Fourier series (2.86) for the
wave function ψ(x, t) at t = 0 (thick curve), t = 10−3 τ (medium curve), and
τ = 2mL2/¯h (thin curve). The discontinuities in the initial wave function ψ(x, 0)
cause both the Gibbs overshoots at x = 1/2 and x = 3/2 seen in the series
for ψ(x, 0) plotted in Fig. 2.5 and the choppiness of the probability distribution
P(x, t) exhibited in Fig. (2.6).
91

FOURIER SERIES
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
x
|ψ (x , t)|2
|ψ (x , t)|2 for a piecewise continuous wave function
Figure 2.6
For an interval of length L = 2, the probability distributions P(x, t) =
|ψ(x, t)|2 of the 1000-term Fourier series (2.86) for the wave function ψ(x, t) at
t = 0 (thick curve), t = 10−3 τ (medium curve), and τ = 2mL2/¯h (thin curve). The
jaggedness of P(x, t) arises from the two discontinuities in the initial wave function
ψ(x, 0) (2.87) at x = L/4 and x = 3L/4.
Example 2.11 (Time evolution of a continuous function)
What does the Fourier
series of a continuous function look like? How does it evolve with time? Let us
take as the wave function at t = 0 the function
ψ(x, 0) =
2
√
L
sin
2π(x −L/4)
L

for
L
4 < x < 3L
4
(2.87)
and zero otherwise. This initial wave function is a continuous function with a
piecewise continuous ﬁrst derivative on the interval [0, L], and it satisﬁes the peri-
odic boundary condition ψ(0, 0) = ψ(L, 0). It therefore satisﬁes the conditions
of Fourier’s convergence theorem (Courant, 1937, p. 439), and so its Fourier
series converges uniformly (and absolutely) to ψ(x, 0) on [0, L].
As in equation (2.83), the Fourier coefﬁcients fn are given by the integrals
fn =
 L
0
dx
&
2
L sin
πnx
L

ψ(x, 0),
(2.88)
which now take the form
fn = 2
√
2
L
 3L/4
L/4
dx sin
πnx
L

sin
2π(x −L/4)
L

.
(2.89)
92

2.8 QUANTUM-MECHANICAL EXAMPLES
Doing the integral, one ﬁnds for fn that for n ̸= 2
fn = −
√
2
π
4
n2 −4 [sin(3nπ/4) + sin(nπ/4)]
(2.90)
while c2 = 0. These Fourier coefﬁcients satisfy the inequalities (2.56) and (2.58)
for k = 0. The factor of 1/n2 in fn guarantees the absolute convergence of the
series
ψ(x, 0) =
∞

n=1
fn
&
2
L sin
πnx
L

(2.91)
because asymptotically the coefﬁcient fn is bounded by fn ≤A/n2 where A is
a constant (A = 144/(5π
√
L) will do) and the sum of 1/n2 converges to the
Riemann zeta function (4.92)
∞

n=1
1
n2 = ζ(2) = π2
6 .
(2.92)
Figure 2.7 plots the ten-term Fourier series (2.91) for ψ(x, 0) for L = 2.
Because this series converges absolutely and uniformly on [0, 2], the 100-term
and 1000-term series were too close to ψ(x, 0) to be seen clearly in the ﬁgure
and so were omitted.
As time goes by, the wave function ψ(x, t) evolves from ψ(x, 0) to
ψ(x, t) =
∞

n=1
fn
&
2
L sin
πnx
L

e−i¯h(nπ)2t/(2mL2)
(2.93)
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
x
ψ (x, 0)
Fourier series of a continuous function
Figure 2.7
The continuous wave function ψ(x, 0) (2.87, solid) and its ten-term
Fourier series (2.90–2.91, dashes) are plotted for the interval [0, 2].
93

FOURIER SERIES
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.5
1
1.5
2
2.5
x
|ψ (x, t)|2
Probability distribution of a continuous wave function
Figure 2.8
For the interval [0, 2], the probability distributions P(x, t) = |ψ(x, t)|2
of the 1000-term Fourier series (2.93) for the wave function ψ(x, t) at t = 0, 10−2 τ,
10−1 τ, τ = 2mL2/¯h, 10τ, and 100τ are plotted as successively thinner curves.
in which the Fourier coefﬁcients are given by (2.90). Because ψ(x, 0) is contin-
uous and periodic with a piecewise continuous ﬁrst derivative, its evolution in
time is much calmer than that of the piecewise continuous square wave (2.80).
Figure 2.8 shows this evolution in successively thinner curves at times t = 0,
10−2 τ, 10−1 τ, τ = 2mL2/¯h, 10τ, and 100τ. The curves at t = 0 and t = 10−2 τ
are smooth, but some wobbles appear at t = 10−1 τ and at t = τ due to the
discontinuities in the ﬁrst derivative of ψ(x, 0) at x = 0.5 and at x = 1.5.
Example 2.12 (Time evolution of a smooth wave function)
Finally, let’s try a
wave function ψ(x, 0) that is periodic and inﬁnitely differentiable on [0, L]. An
inﬁnitely differentiable function is said to be smooth or C∞. The inﬁnite square-
well potential V(x) of equation (2.72) imposes the periodic boundary conditions
ψ(0, 0) = ψ(L, 0) = 0, so we try
ψ(x, 0) =
&
2
3L
#
1 −cos
2πx
L
$
.
(2.94)
Its Fourier series
ψ(x, 0) =
&
1
6L

2 −e2πix/L −e−2πix/L
(2.95)
has coefﬁcients that satisfy the upper bounds (2.56) by vanishing for |n| > 1.
The coefﬁcients of the Fourier sine series for the wave function ψ(x, 0) are
given by the integrals (2.83)
94

2.8 QUANTUM-MECHANICAL EXAMPLES
fn =
 L
0
dx
&
2
L sin
πnx
L

ψ(x, 0)
=
2
√
3 L
 L
0
dx sin
πnx
L
 #
1 −cos
2πx
L
$
= 8 [(−1)n −1]
π
√
3 n(n2 −4)
(2.96)
with all the even coefﬁcients zero, c2n = 0. The fns are proportional to 1/n3,
which is more than enough to ensure the absolute and uniform convergence of
its Fourier sine series
ψ(x, 0) =
∞

n=1
fn
&
2
L sin
πnx
L

.
(2.97)
As time goes by, it evolves to
ψ(x, t) =
∞

n=1
fn
&
2
L sin
πnx
L

e−i¯h(nπ)2t/(2mL2)
(2.98)
and remains absolutely convergent for all times t.
The effects of the absolute and uniform convergence with fn ∝1/n3 are obvi-
ous in the graphs. Figure 2.9 shows (for L = 2) that only ten terms are required
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
x
ψ (x, 0)
Fourier series of a smooth function
Figure 2.9
The wave function ψ(x, 0) (2.94) is inﬁnitely differentiable, and so the
ﬁrst ten terms of its uniformly convergent Fourier series (2.97) offer a very good
approximation to it.
95

FOURIER SERIES
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
x
|ψ (x , t)|2
Probability distribution of a smooth wave function
Figure 2.10
The probability distributions P(x, t) = |ψ(x, t)|2 of the 1000-term
Fourier series (2.98) for the wave function ψ(x, t) at t = 0, 10−2 τ, 10−1 τ,
τ = 2mL2/¯h, 10τ, and 100τ are plotted as successively thinner curves. The time
evolution is calm because the wave function ψ(x, 0) is smooth.
to nearly overlap the initial wave function ψ(x, 0). Figure 2.10 shows that the
evolution of the probability distribution |ψ(x, t)|2 with time is smooth, with no
sign of the jaggedness of Fig. 2.6 or the wobbles of Fig. 2.8. Because ψ(x, 0) is
smooth and periodic, it evolves calmly as time passes.
2.9 Dirac notation
Vectors | j⟩that are orthonormal, ⟨k|j⟩= δk,j span a vector space and express
the identity operator I of the space as (1.132)
I =
N

j=1
| j⟩⟨j|.
(2.99)
Multiplying from the right by any vector |g⟩in the space, we get
|g⟩= I|g⟩=
N

j=1
| j⟩⟨j|g⟩,
(2.100)
which says that every vector |g⟩in the space has an expansion (1.133) in terms
of the N orthonormal basis vectors | j⟩. The coefﬁcients ⟨j|g⟩of the expansion
are inner products of the vector |g⟩with the basis vectors | j⟩.
96

2.10 DIRAC’S DELTA FUNCTION
These properties of ﬁnite-dimensional vector spaces also are true of inﬁnite-
dimensional vector spaces of functions. We may use as basis vectors the phases
exp(inx)/
√
2π. They are orthonormal with inner product (2.1)
(m, n) =
 2π
0
 eimx
√
2π
∗einx
√
2π
dx =
 2π
0
ei(n−m)x
2π
dx = δm,n,
(2.101)
which in Dirac notation with ⟨x|n⟩= exp(inx)/
√
2π and ⟨m|x⟩= ⟨x|m⟩∗is
⟨m|n⟩=
 2π
0
⟨m|x⟩⟨x|n⟩dx =
 2π
0
ei(n−m)x
2π
dx = δm,n.
(2.102)
The identity operator for Fourier’s space of functions is
I =
∞

n=−∞
|n⟩⟨n|.
(2.103)
So we have
|f ⟩= I|f ⟩=
∞

n=−∞
|n⟩⟨n|f ⟩
(2.104)
and
⟨x|f ⟩= ⟨x|I|f ⟩=
∞

n=−∞
⟨x|n⟩⟨n|f ⟩=
∞

n=−∞
einx
√
2π
⟨n|f ⟩,
(2.105)
which with ⟨n|f ⟩= fn is the Fourier series (2.2). The coefﬁcients ⟨n|f ⟩= fn are
the inner products (2.3)
⟨n|f ⟩=
 2π
0
⟨n|x⟩⟨x|f ⟩dx =
 2π
0
e−inx
√
2π
⟨x|f ⟩dx =
 2π
0
e−inx
√
2π
f (x) dx.
(2.106)
2.10 Dirac’s delta function
A Dirac delta function is a (continuous, linear) map from a space of functions
into the real or complex numbers. It is a functional that associates a number
with each function in the function space. Thus δ(x −y) associates the number
f (y) with the function f (x). We may write this association as
f (y) =

f (x) δ(x −y) dx.
(2.107)
97

FOURIER SERIES
Delta functions pop up all over physics. The inner product of two of the
kets |x⟩that appear in the Fourier-series formulas (2.105) and (2.106) is a delta
function, ⟨x|y⟩= δ(x−y). The formula (2.106) for the coefﬁcient ⟨n|f ⟩becomes
obvious if we write the identity operator for functions deﬁned on the interval
[0, 2π] as
I =
 2π
0
|x⟩⟨x| dx
(2.108)
for then
⟨n|f ⟩= ⟨n|I|f ⟩=
 2π
0
⟨n|x⟩⟨x|f ⟩dx =
 2π
0
e−inx
√
2π
⟨x|f ⟩dx.
(2.109)
The equation |y⟩= I|y⟩with the identity operator (2.108) gives
|y⟩= I|y⟩=
 2π
0
|x⟩⟨x|y⟩dx.
(2.110)
Multiplying (2.108) from the right by |f ⟩and from the left by ⟨y|, we get
f (y) = ⟨y|I|f ⟩=
 2π
0
⟨y|x⟩⟨x|f ⟩dx =
 2π
0
⟨y|x⟩f (x) dx.
(2.111)
These relations (2.110) and (2.111) say that the inner product ⟨y|x⟩is a delta
function, ⟨y|x⟩= ⟨x|y⟩= δ(x −y).
The Fourier-series formulas (2.105) and (2.106) lead to a statement about the
completeness of the phases exp(inx)/
√
2π
f (x) =
∞

n=−∞
fn
einx
√
2π
=
∞

n=−∞
 2π
0
e−iny
√
2π
f (y) einx
√
2π
dy.
(2.112)
Interchanging and rearranging, we have
f (x) =
 2π
0

∞

n=−∞
ein(x−y)
2π

f (y) dy.
(2.113)
But the phases (2.112) are periodic with period 2π, so we also have
f (x + 2πi) =
 2π
0

∞

n=−∞
ein(x−y)
2π

f (y) dy.
(2.114)
98

2.10 DIRAC’S DELTA FUNCTION
Thus we arrive at the Dirac comb
∞

n=−∞
ein(x−y)
2π
=
∞

ℓ=−∞
δ(x −y −2πℓ)
(2.115)
or more simply
∞

n=−∞
einx
2π = 1
2π

1 + 2
∞

n=1
cos(nx)

=
∞

ℓ=−∞
δ(x −2πℓ).
(2.116)
Example 2.13 (Dirac’s comb)
The sum of the ﬁrst 100,000 terms of this cosine
series (2.116) for the Dirac comb is plotted for the interval (−15, 15) in Fig. 2.11.
Gibbs overshoots appear at the discontinuities. The integral of the ﬁrst 100,000
terms from −15 to 15 is 5.0000.
−15
−10
−5
0
5
10
15
−1
−0.5
0
0.5
1
1.5
2
2.5
3
3.5 x 104
x
Sum of series
Dirac comb
Figure 2.11
The sum of the ﬁrst 100,000 terms of this cosine series (2.116) for
the Dirac comb is plotted for –15 ≤x ≤15.
The stretched Dirac comb is
∞

n=−∞
e2πin(x−y)/L
L
=
∞

ℓ=−∞
δ(x −y −ℓL).
(2.117)
99

FOURIER SERIES
Example 2.14 (Parseval’s identity)
Using our formula (2.35) for the Fourier
coefﬁcients of a stretched interval, we can relate a sum of products f ∗
n gn of the
Fourier coefﬁcients of the functions f (x) and g(x) to an integral of the product
f ∗(x) g(x)
∞

n=−∞
f ∗
n gn =
∞

n=−∞
 L
0
dx ei2πnx/L
√
L
f ∗(x)
 L
0
dy e−i2πny/L
√
L
g(y).
(2.118)
This sum contains Dirac’s comb (2.117) and so
∞

n=−∞
f ∗
n gn =
 L
0
dx
 L
0
dy f ∗(x) g(y) 1
L
∞

n=−∞
ei2πn(x−y)/L
=
 L
0
dx
 L
0
dy f ∗(x) g(y)
∞

ℓ=−∞
δ(x −y −2πℓL).
(2.119)
But because only the ℓ= 0 tooth of the comb lies in the interval [0, L], we have
more simply
∞

n=−∞
f ∗
n gn =
 L
0
dx
 L
0
dy f ∗(x) g(y) δ(x −y) =
 L
0
dx f ∗(x) g(x).
(2.120)
In particular, if the two functions are the same, then
∞

n=−∞
| fn|2 =
 L
0
dx | f (x)|2,
(2.121)
which is Parseval’s identity. Thus if a function is square integrable on an interval,
then the sum of the squares of the absolute values of its Fourier coefﬁcients is
the integral of the square of its absolute value.
Example 2.15 (Derivatives of delta functions)
Delta functions and other gen-
eralized functions or distributions map smooth functions that vanish at inﬁnity
into numbers in ways that are linear and continuous. Derivatives of delta
functions are deﬁned so as to allow integration by parts. Thus the nth deriva-
tive of the delta function δ(n)(x−y) maps the function f (x) to (−1)n times its nth
derivative f (n)(y) at y

δ(n)(x −y) f (x) dx =

δ(x −y) (−1)n f (n)(x) dx = (−1)n f (n)(y)
(2.122)
with no surface term.
Example 2.16 (The equation xf (x) = a)
Dirac’s delta function sometimes
appears unexpectedly. For instance, the general solution to the equation
100

2.11 THE HARMONIC OSCILLATOR
x f (x) = a is f (x) = a/x + b δ(x) where b is an arbitrary constant (Dirac, 1967,
sec. 15, Waxman and Peck, 1998). Similarly, the general solution to the equation
x2 f (x) = a is f (x) = a/x2 + b δ(x)/x + c δ(x) + d δ′(x) in which δ′(x) is the
derivative of the delta function, and b, c, and d are arbitrary constants.
2.11 The harmonic oscillator
The hamiltonian for the harmonic oscillator is
H = p2
2m + 1
2mω2q2.
(2.123)
The commutation relation [q, p] ≡qp −pq = i¯h implies that the lowering and
raising operators
a =
&mω
2¯h

q + ip
mω

and
a† =
&mω
2¯h

q −ip
mω

(2.124)
obey the commutation relation [a, a†] = 1. In terms of a and a†, which also are
called the annihilation and creation operators, the hamiltonian H has the simple
form
H = ¯hω

a†a + 1
2

.
(2.125)
There is a unique state |0⟩that is annihilated by the operator a, as may be
seen by solving the differential equation
⟨q′|a|0⟩=
&mω
2¯h ⟨q′|

q + ip
mω

|0⟩= 0.
(2.126)
Since ⟨q′|q = q′⟨q′| and
⟨q′|p|0⟩= ¯h
i
d⟨q′|0⟩
dq′
(2.127)
the resulting differential equation is
d⟨q′|0⟩
dq′
= −mω
¯h q′⟨q′|0⟩.
(2.128)
Its suitably normalized solution is the wave function for the ground state of the
harmonic oscillator
⟨q′|0⟩=
mω
π ¯h
1/4
exp

−mωq′2
2¯h

.
(2.129)
101

FOURIER SERIES
For n = 0, 1, 2, . . ., the nth eigenstate of the hamiltonian H is
|n⟩=
1
√
n!

a†n
|0⟩
(2.130)
where n! ≡n(n −1) . . . 1 is n-factorial and 0! = 1. Its energy is
H|n⟩= ¯hω

n + 1
2

|n⟩.
(2.131)
The identity operator is
I =
∞

n=0
|n⟩⟨n|.
(2.132)
An arbitrary state |ψ⟩has an expansion in terms of the eigenstates |n⟩
|ψ⟩= I|ψ⟩=
∞

n=0
|n⟩⟨n|ψ⟩
(2.133)
and evolves in time like a Fourier series
|ψ, t⟩= e−iHt/¯h|ψ⟩= e−iHt/¯h
∞

n=0
|n⟩⟨n|ψ⟩= e−iωt/2
∞

n=0
e−inωt|n⟩⟨n|ψ⟩(2.134)
with wave function
ψ(q, t) = ⟨q|ψ, t⟩= e−iωt/2
∞

n=0
e−inωt⟨q|n⟩⟨n|ψ⟩.
(2.135)
The wave functions ⟨q|n⟩of the energy eigenstates are related to the Hermite
polynomials (example 8.6)
Hn(x) = (−1)nex2 dn
dxn e−x2
(2.136)
by a change of variables x =

mω/¯h q ≡sq and a normalization factor
ψn(q) = ⟨q|n⟩=
√s e−(sq)2/2

2nn!√π
Hn(sq) = mω
π ¯h
1/4 e−mωq2/2¯h
√
2nn!
Hn
mω
¯h
1/2
q

.
(2.137)
The coherent state |α⟩
|α⟩= e−|α|2/2eαa†|0⟩= e−|α|2/2
∞

n=0
αn
√
n!
|n⟩
(2.138)
is an eigenstate a|α⟩= α|α⟩of the lowering (or annihilation) operator a with
eigenvalue α. Its time evolution is simply
|α, t⟩= e−iωt/2e−|α|2/2
∞

n=0

αe−iωtn
√
n!
|n⟩= e−iωt/2 |αe−iωt⟩.
(2.139)
102

2.13 PERIODIC BOUNDARY CONDITIONS
2.12 Nonrelativistic strings
If we clamp the ends of a nonrelativistic string at x = 0 and x = L, then the
amplitude y(x, t) will obey the boundary conditions
y(0, t) = y(L, t) = 0
(2.140)
and the wave equation
v2 ∂2y
∂x2 = ∂2y
∂t2
(2.141)
as long as y(x, t) remains small. The functions
yn(x, t) = sin nπx
L

fn sin nπvt
L
+ dn cos nπvt
L

(2.142)
satisfy this wave equation (2.141) and the boundary conditions (2.140). They
represent waves traveling along the x-axis with speed v.
The space SL of functions f (x) that satisfy the boundary condition (2.140) is
spanned by the functions sin(nπx/L). One may use the integral formula
 L
0
sin nπx
L
sin mπx
L
dx = L
2 δnm
(2.143)
to derive for any function f ∈SL the Fourier series
f (x) =
∞

n=1
fn sin nπx
L
(2.144)
with coefﬁcients
fn = 2
L
 L
0
sin nπx
L
f (x) dx
(2.145)
and the representation
∞

m=−∞
δ(x −z −2mL) = 2
L
∞

n=1
sin nπx
L
sin nπz
L
(2.146)
for the Dirac comb on SL.
2.13 Periodic boundary conditions
The inﬁnite square-well potential (2.72) enforced the boundary conditions
ψ(L, t) = ψ(0, t) = 0, which are periodic. But periodic boundary conditions
occur in other ways too. For instance, rather than studying an inﬁnitely long
one-dimensional system, we might study the same system, but of length L. The
103

FOURIER SERIES
ends cause effects not present in the inﬁnite system. To avoid them, we imagine
that the system forms a circle and impose the periodic boundary condition
ψ(x ± L, t) = ψ(x, t).
(2.147)
In three dimensions, the analogous conditions are
ψ(L, y, z, t) = ψ(0, y, z, t),
ψ(x, L, z, t) = ψ(x, 0, z, t),
ψ(x, y, L, t) = ψ(x, y, 0, t).
(2.148)
The eigenstates |p⟩of the free hamiltonian H = p2/2m have wave functions
ψp(x) = ⟨x|p⟩= eix·p/¯h/(2π ¯h)3/2.
(2.149)
The periodic boundary conditions (2.148) require that each component pi of
the momentum satisfy Lpi/¯h = 2πni or
p = 2π ¯hn
L
= hn
L
(2.150)
where n is a vector of integers, which may be positive or negative or zero.
Periodic boundary conditions arise naturally in the study of solids. The
atoms of a perfect crystal are at the vertices of a Bravais lattice
xi = x0 +
3

i=1
niai
(2.151)
in which the three vectors ai are the primitive vectors of the lattice and the ni
are three integers. The hamiltonian of such an inﬁnite crystal is invariant under
translations in space by
3

i=1
niai.
(2.152)
To keep the notation simple, let’s restrict ourselves to a cubic lattice with
lattice spacing a. Then since the momentum operator p generates translations
in space, the invariance of H under translations by a n
exp(ian · p)H exp(−ian · p) = H
(2.153)
implies that p and H are compatible observables [p, H] = 0. As explained in
section 1.30, it follows that we may choose the eigenstates of H also to be
eigenstates of p
eiap·n/¯h|ψ⟩= eiak·n |ψ⟩,
(2.154)
which implies that
ψ(x + an, t) = eiak·n ψ(x, t).
(2.155)
104

EXERCISES
Setting
ψ(x) = eik·x u(x)
(2.156)
we see that condition (2.155) implies that u(x) is periodic
u(x + an) = u(x).
(2.157)
For a general Bravais lattice, this Born–von Karman periodic boundary condi-
tion is
u

x +
3

i=1
niai, t

= u(x, t).
(2.158)
Equations (2.155) and (2.157) are known as Bloch’s theorem.
Exercises
2.1
Show that sin ω1x + sin ω2x is the same as (2.9).
2.2
Find the Fourier series for the function exp(ax) on the interval −π < x ≤π.
2.3
Find the Fourier series for the function (x2 −π2)2 on the same interval
(−π, π].
2.4
Find the Fourier series for the function (1 + cos x) sin ax on the interval
(−π, π].
2.5
Show that the Fourier series for the function x cos x on the interval [ −π, π]
is
x cos x = −1
2 sin x + 2
∞

n=2
(−1)n n
n2 −1 sin nx.
(2.159)
2.6
(a) Show that the Fourier series for the function |x| on the interval [ −π, π] is
|x| = π
2 −4
π
∞

n=0
cos(2n + 1)x
(2n + 1)2
.
(2.160)
(b) Use this result to ﬁnd a neat formula for π2/8. Hint: set x = 0.
2.7
Show that the Fourier series for the function | sin x| on the interval [ −π, π] is
| sin x| = 2
π −4
π
∞

n=1
cos 2nx
4n2 −1.
(2.161)
2.8
Show that the Fourier coefﬁcients of the C 0 function (2.59) on the interval
[ −π, π] are given by (2.60).
2.9
Find by inspection the Fourier series for the function exp[exp(−ix)].
2.10 Fill in the steps in the computation (2.28) of the Fourier series for x2.
2.11 Do the ﬁrst integral in equation (2.65). Hint: differentiate ln(sin x/x).
105

FOURIER SERIES
2.12 Use the inﬁnite-product formula (2.66) for the sine and the relation cos πx =
sin 2πx/(2 sin πx) to derive the inﬁnite-product formula (2.67) for the cosine.
Hint:
∞

n=1

1 −x2
1
4n2

=
∞

n=1

1 −
x2
1
4(2n −1)2
 
1 −
x2
1
4(2n)2

.
(2.162)
2.13 What’s the general solution to the equation x3f (x) = a?
2.14 Suppose we wish to approximate the real square-integrable function f (x) by
the Fourier series with N terms
fN(x) = a0
2 +
N

n=1
(an cos nx + bn sin nx) .
(2.163)
Then the error
EN =
 2π
0
[f (x) −fN(x)]2 dx
(2.164)
will depend upon the 2N + 1 coefﬁcients an and bn. The best coefﬁcients
minimize this error and satisfy the conditions
∂EN
∂an
= ∂EN
∂bn
= 0.
(2.165)
By using these conditions, ﬁnd them.
2.15 Find the Fourier series for the function
f (x) = θ(a2 −x2)
(2.166)
on the interval [−π, π] for the case a2 < π2. The Heaviside step function θ(x)
is zero for x < 0, one-half for x = 0, and unity for x > 0 (Oliver Heaviside,
1850–1925). The value assigned to θ(0) seldom matters, and you need not
worry about it in this problem.
2.16 Derive or infer the formula (2.117) for the stretched Dirac comb.
2.17 Use the commutation relation [q, p] = i¯h to show that the annihilation and
creation operators (2.124) satisfy the commutation relation [a, a†] = 1.
2.18 Show that the state |n⟩= (a†)n|0⟩/
√
n! is an eigenstate of the hamiltonian
(2.125) with energy ¯hω(n + 1/2).
2.19 Show that the coherent state |α⟩(2.138) is an eigenstate of the annihilation
operator a with eigenvalue α.
2.20 Derive equations (2.145 & 2.146) from the expansion (2.144) and the integral
formula (2.143).
2.21 Consider a string like the one described in section 2.12, which satisﬁes the
boundary conditions (2.140) and the wave equation (2.141). The string is at
rest at time t = 0
y(x, 0) = 0
(2.167)
106

EXERCISES
and is struck precisely at t = 0 and x = a so that
∂y(x, t)
∂t

t=0
= Lv0δ(x −a).
(2.168)
Find y(x, t) and ˙y(x, t), where the dot means time derivative.
2.22 Same as exercise (2.21), but now the initial conditions are
u(x, 0) = f (x)
and
˙u(x, 0) = g(x)
(2.169)
in which f (0) = f (L) = 0 and g(0) = g(L) = 0. Find the motion of the
amplitude u(x, t) of the string.
2.23 (a) Find the Fourier series for the function f (x) = x2 on the interval [−π, π].
(b) Use your result at x = π to show that
∞

n=1
1
n2 = π2
6
(2.170)
which is the value of Riemann’s zeta function (4.92) ζ(x) at x = 2.
107

3
Fourier and Laplace transforms
The complex exponentials exp(i2πnx/L) are orthonormal and easy to
differentiate (and to integrate), but they are periodic with period L. If one wants
to represent functions that are not periodic, then a better choice is the complex
exponentials exp(ikx), where k is an arbitrary real number. These orthonormal
functions are the basis of the Fourier transform. The choice of complex k leads
to the transforms of Laplace, Mellin, and Bromwich.
3.1 The Fourier transform
The interval [−L/2, L/2] is arbitrary in the Fourier series pair (2.37)
f (x) =
∞

n=−∞
fn
ei2πnx/L
√
L
and
fn =
 L/2
−L/2
f (x) e−i2πnx/L
√
L
dx.
(3.1)
What happens when we stretch this interval without limit, letting L →∞?
We may use the nearest-integer function [y] to convert the coefﬁcients fn into
a continuous function ˆf (y) ≡f[y] such that ˆf (y) = fn when |y −n| < 1/2. In
terms of this function ˆf (y), the Fourier series (3.1) for the function f (x) is
f (x) =
∞

n=−∞
 n+1/2
n−1/2
ˆf (y) ei2π[y]x/L
√
L
dy =
 ∞
−∞
ˆf (y) ei2π[y]x/L
√
L
dy.
(3.2)
Since [y] and y differ by no more than 1/2, the absolute value of the difference
between exp(iπ[y]x/L) and exp(iπyx/L) for ﬁxed x is
ei2π[y]x/L −ei2πyx/L
 =
ei2π([y]−y)x/L −1
 ≈π|x|
L ,
(3.3)
108

3.1 THE FOURIER TRANSFORM
which goes to zero as L →∞. So in this limit, we may replace [y] by y and
express f (x) as
f (x) =
 ∞
−∞
ˆf (y) ei2πyx/L
√
L
dy.
(3.4)
We now change variables to k = 2πy/L and ﬁnd for f (x) the integral
f (x) =
 ∞
−∞
ˆf
Lk
2π
 eikx
√
L
L
2π dk =
 ∞
−∞
&
L
2π
ˆf
Lk
2π

eikx
dk
√
2π
.
(3.5)
So in terms of the Fourier transform ˜f (k) deﬁned as
˜f (k) =
&
L
2π
˜f
Lk
2π

(3.6)
the integral (3.5) for f (x) is the inverse Fourier transform
f (x) =
 ∞
−∞
˜f (k) eikx
dk
√
2π
.
(3.7)
To ﬁnd ˜f (k), we use its deﬁnition (3.6), the deﬁnition (3.1) of fn, our formula
ˆf (y) = f[y], and the inequality |2π [Lk/2π] /L −k| ≤π/2L to write
˜f (k) =
&
L
2π f
Lk
2π
 =
&
L
2π
 L/2
−L/2
f (x) e
−i2π

Lk
2π

x
L
√
L
dx ≈
 L/2
−L/2
f (x) e−ikx dx
√
2π
.
This formula becomes exact in the limit L →∞,
˜f (k) =
 ∞
−∞
f (x) e−ikx
dx
√
2π
(3.8)
and so we have the Fourier transformations
f (x) =
 ∞
−∞
˜f (k) eikx
dk
√
2π
and
˜f (k) =
 ∞
−∞
f (x) e−ikx
dx
√
2π
.
(3.9)
The function ˜f (k) is the Fourier transform of f (x), and f (x) is the inverse Fourier
transform of ˜f (k).
In these symmetrical relations (3.9), the distinction between a Fourier trans-
form and an inverse Fourier transform is entirely a matter of convention. There
is no rule for which sign, ikx or −ikx, goes with which transform or for where
to put the 2πs. Thus one often sees
f (x) =
 ∞
−∞
˜f (k) e±ikx dk
and
˜f (k) =
 ∞
−∞
f (x) e∓ikx dx
2π
(3.10)
109

FOURIER AND LAPLACE TRANSFORMS
as well as
f (x) =
 ∞
−∞
˜f (k) e±ikx dk
2π
and
˜f (k) =
 ∞
−∞
f (x) e∓ikx dx.
(3.11)
One often needs to go back and forth between the representations of a given
function as a Fourier series and as a Fourier transform. So stripping away fac-
tors of
√
2π and
√
L with the notation `fn = fn/
√
L, `f (k) = ˜f (k)/
√
2π, and
kn = 2πn/L, let’s compare the Fourier series (3.1) for the function f (x) with its
Fourier transform (3.9) in the limit of very large L
f (x) =
∞

n=−∞
`fn eiknx =
 ∞
−∞
`f (k) eikx dk.
(3.12)
Using the deﬁnition (3.6) of ˜f (k) as

L/(2π) fn, we have
`f (k) =
˜f (k)
√
2π
=
√
L
2π fn = L
2π
`fn
(3.13)
in which n = Lkn/(2π) is the integer nearest to Lk/(2π). That is,
`fn = `f[Lk/(2π)] = 2π
L
`f (k).
(3.14)
Example 3.1 (The Fourier transform of a gaussian is a gaussian)
The Fourier
transform of the gaussian f (x) = exp(−m2 x2) is
˜f (k) =
 ∞
−∞
dx
√
2π
e−ikx e−m2 x2.
(3.15)
We complete the square in the exponent:
˜f (k) = e−k2/4m2  ∞
−∞
dx
√
2π
e−m2 (x+ik/2m2)2.
(3.16)
As we shall see in section 5.14 when we study analytic functions, we may shift x
to x −ik/2m2, so the term ik/2m2 in the exponential has no effect on the value
of the x-integral.
˜f (k) = e−k2/4m2  ∞
−∞
dx
√
2π
e−m2 x2 =
1
√
2 m
e−k2/4m2.
(3.17)
Thus, the Fourier transform of a gaussian is another gaussian
˜f (k) =
 ∞
−∞
dx
√
2π
e−ikx e−m2 x2 =
1
√
2 m
e−k2/4m2.
(3.18)
But the two gaussians are very different: if the gaussian f (x) = exp(−m2x2)
decreases slowly as x →∞because m is small (or quickly because m is big), then
110

3.2 THE FOURIER TRANSFORM OF A REAL FUNCTION
its gaussian Fourier transform ˜f (k) = exp(−k2/4m2)/m
√
2 decreases quickly as
k →∞because m is small (or slowly because m is big).
Can we invert ˜f (k) to get f (x)? The inverse Fourier transform (3.7) says
f (x) =
 ∞
−∞
dk
√
2π
˜f (k) eikx =
 ∞
−∞
dk
√
2π
1
m
√
2
eikx−k2/4m2.
(3.19)
By again completing the square in the exponent
f (x) = e−m2x2  ∞
−∞
dk
√
2π
1
m
√
2
e−(k−i2m2x)2/4m2
(3.20)
and shifting the variable of integration k to k + i2m2x, we ﬁnd
f (x) = e−m2x2  ∞
−∞
dk
√
2π
1
m
√
2
e−k2/(4m2) = e−m2x2,
(3.21)
which is reassuring.
Using (3.17) for ˜f (k) and the connections (3.12–3.14) between Fourier series
and transforms, we see that a Fourier series for this gaussian is in the limit of
L ≫x
f (x) = e−m2x2 = 2π
L
∞

n=−∞
1
√
4π m
e−k2n/(4m2)eiknx
(3.22)
in which kn = 2πn/L.
3.2 The Fourier transform of a real function
If the function f (x) is real, then its Fourier transform (3.8)
˜f (k) =
 ∞
−∞
dx
√
2π
f (x) e−ikx
(3.23)
obeys the relation
˜f ∗(k) = ˜f (−k).
(3.24)
For since f ∗(x) = f (x), the complex conjugate of (3.23) is
˜f ∗(k) =
 ∞
−∞
dx
√
2π
f (x) eikx = ˜f (−k).
(3.25)
It follows that a real function f (x) satisﬁes the relation
f (x) = 1
π
 ∞
0
dk
 ∞
−∞
f (y) cos k(y −x) dy
(3.26)
(exercise 3.1) as well as
f (x) = 2
π
 ∞
0
cos kx dk
 ∞
0
f (y) cos ky dy
(3.27)
111

FOURIER AND LAPLACE TRANSFORMS
if it is even, and
f (x) = 2
π
 ∞
0
sin kx dk
 ∞
0
f (y) sin ky dy
(3.28)
if it is odd (exercise 3.2).
Example 3.2 (Dirichlet’s discontinuous factor)
Using (3.27), one may write the
square wave
f (x) =
⎧
⎨
⎩
1,
|x| < 1
1
2,
|x| = 1
0,
|x| > 1
(3.29)
as Dirichlet’s discontinuous factor
f (x) = 2
π
 ∞
0
sin k cos kx
k
dk
(3.30)
(exercise 3.3).
Example 3.3 (Even and odd exponentials)
By using the Fourier-transform
formulas (3.27 & 3.28), one may show that the Fourier transform of the even
exponential exp(−β|x|) is
e−β|x| = 2
π
 ∞
0
β cos kx
β2 + k2 dk
(3.31)
while that of the odd exponential x exp(−β|x|)/|x| is
x
|x|e−β|x| = 2
π
 ∞
0
k sin kx
β2 + k2 dk
(3.32)
(exercise 3.4).
3.3 Dirac, Parseval, and Poisson
Combining the basic equations (3.9) that deﬁne the Fourier transform, we may
do something apparently useless: we may write the function f (x) in terms of
itself as
f (x) =
 ∞
−∞
dk
√
2π
˜f (k) eikx =
 ∞
−∞
dk
√
2π
eikx
 ∞
−∞
dy
√
2π
e−iky f (y).
(3.33)
Let’s compare this equation
f (x) =
 ∞
−∞
dy
 ∞
−∞
dk
2π exp[ik(x −y)]

f (y)
(3.34)
112

3.3 DIRAC, PARSEVAL, AND POISSON
with one (2.107) that describes Dirac’s delta function
f (x) =
 ∞
−∞
dy δ(x −y) f (y).
(3.35)
Thus for functions with sensible Fourier transforms, the delta function is
δ(x −y) =
 ∞
−∞
dk
2π exp[ik(x −y)].
(3.36)
The inner product (f , g) of two functions, f (x) with Fourier transform ˜f (k)
and g(x) with Fourier transform ˆg(k), is
⟨f |g⟩= (f , g) =
 ∞
−∞
dx f ∗(x) g(x).
(3.37)
Since f (x) and g(x) are related to ˜f (k) and ˜g(k) by the Fourier transform (3.8),
their inner product (f , g) is
(f , g) =
 ∞
−∞
dx
 ∞
−∞
dk
√
2π

˜f (k) eikx∗ ∞
−∞
dk′
√
2π
˜g(k′) eik′x
=
 ∞
−∞
dk
 ∞
−∞
dk′
 ∞
−∞
dx
2π eix(k′−k) ˜f ∗(k) ˜g(k′)
(3.38)
=
 ∞
−∞
dk
 ∞
−∞
dk′ δ(k′ −k) ˜f ∗(k) ˜g(k′) =
 ∞
−∞
dk ˜f ∗(k) ˜g(k).
Thus we arrive at Parseval’s relation
(f , g) =
 ∞
−∞
dx f ∗(x) g(x) =
 ∞
−∞
dk ˜f ∗(k) ˜g(k) = (˜f , ˜g),
(3.39)
which says that the inner product of two functions is the same as the inner prod-
uct of their Fourier transforms. The Fourier transform is a unitary transform.
In particular, if f = g, then
⟨f |f ⟩= (f , f ) =
 ∞
−∞
dx |f (x)|2 =
 ∞
−∞
dk | ˜f (k)|2
(3.40)
(Marc-Antoine Parseval des Chênes, 1755–1836).
In fact, one may show that the Fourier transform maps the space of
(Lebesgue) square-integrable functions onto itself in a one-to-one manner.
Thus the natural space for the Fourier transform is the space of square-
integrable functions, and so the representation (3.36) of Dirac’s delta function
is suitable for continuous square-integrable functions.
This may be a good place to say a few words about how to evaluate integrals
involving delta functions of more complicated arguments, such as
J =

δ(g(x)) f (x) dx.
(3.41)
113

FOURIER AND LAPLACE TRANSFORMS
To see how this works, let’s assume that g(x) vanishes at a single point x0 at
which its derivative g′(x0) ̸= 0 isn’t zero. Then the integral J involves f only as
f (x0), which we can bring outside as a prefactor
J = f (x0)

δ(g(x)) dx.
(3.42)
Near x0 the function g(x) is approximately g′(x0)(x −x0), and so the integral is
J = f (x0)

δ(g′(x0)(x −x0)) dx.
(3.43)
Since the delta function is nonnegative, we can write
J =
f (x0)
|g′(x0)|

δ(g′(x0)(x −x0))|g′(x0)| dx
=
f (x0)
|g′(x0)|

δ(g −g0) dg =
f (x0)
|g′(x0)|.
(3.44)
Thus for a function g(x) that has a single zero, we have

δ(g(x)) f (x) dx =
f (x0)
|g′(x0)|.
(3.45)
If g(x) has several zeros x0k, then we must sum over them

δ(g(x)) f (x) dx =

k
f (x0k)
|g′(x0k)|.
(3.46)
Replacing the dummy variable n by −k in our Dirac-comb formula (2.116),
we ﬁnd
∞

k=−∞
e−ikx
2π
=
∞

ℓ=−∞
δ(x −2πℓ).
(3.47)
Multiplying both sides of this comb by a function f (x) and integrating over the
real line, we have
∞

k=−∞
 ∞
−∞
e−ikx
2π
f (x) dx =
∞

ℓ=−∞
 ∞
−∞
δ(x −2πℓ) f (x) dx.
(3.48)
Our delta function formula (2.107) or (3.34) and our Fourier-transform rela-
tions (3.9) now give us the Poisson summation formula
1
√
2π
∞

k=−∞
˜f (k) =
∞

ℓ=−∞
f (2πℓ),
(3.49)
114

3.4 FOURIER DERIVATIVES AND INTEGRALS
in which k and ℓare summed over all the integers. The stretched version of the
Poisson summation formula is
√
2π
L
∞

k=−∞
˜f (2πk/L) =
∞

ℓ=−∞
f (ℓL).
(3.50)
Both sides of these formulas make sense for continuous functions that are
square integrable on the real line.
Example 3.4 (Poisson and Gauss)
In example 3.1, we saw that the gaussian
f (x) = exp(−m2x2) has ˆf (k) = exp(−k2/4m2)/
√
2 m as its Fourier transform.
So in this case, the Poisson summation formula (3.49) gives
1
2√π m
∞

k=−∞
e−k2/4m2 =
∞

ℓ=−∞
e−(2πℓm)2.
(3.51)
For m ≫1, the left-hand sum converges slowly, while the right-hand sum con-
verges quickly. For m ≪1, the right-hand sum converges slowly, while the
left-hand sum converges quickly.
A sum that converges slowly in space often converges quickly in momen-
tum space. Ewald summation is a technique for summing electrostatic energies,
which fall off only with a power of the distance, by summing their Fourier
transforms (Darden et al., 1993).
3.4 Fourier derivatives and integrals
By differentiating the inverse Fourier-transform relation (3.7)
f (x) =
 ∞
−∞
dk
√
2π
˜f (k) eikx
(3.52)
we see that the Fourier transform of the derivative f ′(x) is ik ˆf (k)
f ′(x) =
 ∞
−∞
dk
√
2π
ik ˜f (k) eikx.
(3.53)
Differentiation with respect to x corresponds to multiplication by ik.
We may repeat the process and express the second derivative as
f
′′(x) =
 ∞
−∞
dk
√
2π
(−k2) ˜f (k) eikx
(3.54)
and the nth derivative as
f (n)(x) =
 ∞
−∞
dk
√
2π
(ik)n ˜f (k) eikx.
(3.55)
115

FOURIER AND LAPLACE TRANSFORMS
The indeﬁnite integral of the inverse Fourier transform (3.52) is
‵f (x) ≡
 x
dx1 f (x1) =
 ∞
−∞
dk
√
2π
˜f (k) eikx
ik
(3.56)
and the nth indeﬁnite integral is
(n)f (x) ≡
 x
dx1 . . .
 xn−1
dxn f (xn) =
 ∞
−∞
dk
√
2π
˜f (k) eikx
(ik)n .
(3.57)
Whether these derivatives and integrals converge better or worse than f (x)
depends upon the behavior of ˆf (k) near k = 0 and as |k| →∞.
Example 3.5 (Momentum and momentum space)
Let’s write the inverse
Fourier transform (3.7) with ψ instead of f and with the wave number k replaced
by k = p/¯h
ψ(x) =
 ∞
−∞
˜ψ(k) eikx
dk
√
2π
=
 ∞
−∞
˜ψ(p/¯h)
√
¯h
eipx/¯h
dp
√2π ¯h
.
(3.58)
For a normalized wave function ψ(x), Parseval’s relation (3.40) implies
1 =
 ∞
−∞
|ψ(x)|2 dx =
 ∞
−∞
| ˜ψ(k)|2 dk =
 ∞
−∞
|
˜ψ(p/¯h)
√
¯h
|2 dp,
(3.59)
or with ψ(x) = ⟨x|ψ⟩and ϕ(p) = ⟨p|ψ⟩= ˜ψ(p/¯h)/√
¯h
1 = ⟨ψ|ψ⟩=
 ∞
−∞
|ψ(x)|2 dx =
 ∞
−∞
⟨ψ|x⟩⟨x|ψ⟩dx
=
 ∞
−∞
⟨ψ|p⟩⟨p|ψ⟩dp =
 ∞
−∞
|ϕ(p)|2 dp.
(3.60)
The inner product of any two states |ψ⟩and |φ⟩is
⟨ψ|φ⟩=
 ∞
−∞
ψ∗(x)φ(x) dx =
 ∞
−∞
⟨ψ|x⟩⟨x|φ⟩dx
=
 ∞
−∞
ψ∗(p)φ(p) dp =
 ∞
−∞
⟨ψ|p⟩⟨p|φ⟩dp
(3.61)
so the outer products |x⟩⟨x| and |p⟩⟨p| can represent the identity operator
I =
 ∞
−∞
dx |x⟩⟨x| =
 ∞
−∞
dp |p⟩⟨p|.
(3.62)
The Fourier transform (3.58) relating the wave function in momentum space
to that in position space is
ψ(x) =
 ∞
−∞
eipx/¯h ϕ(p)
dp
√2π ¯h
(3.63)
116

3.4 FOURIER DERIVATIVES AND INTEGRALS
and the inverse Fourier transform is
ϕ(p) =
 ∞
−∞
e−ipx/¯h ψ(x)
dx
√2π ¯h
.
(3.64)
In Dirac notation, the ﬁrst equation (3.63) of this pair is
ψ(x) = ⟨x|ψ⟩=
 ∞
−∞
⟨x|p⟩⟨p|ψ⟩dp =
 ∞
−∞
eipx/¯h
√2π ¯h
ϕ(p) dp
(3.65)
so we identify ⟨x|p⟩with
⟨x|p⟩= eipx/¯h
√2π ¯h
,
(3.66)
which in turn is consistent with the delta function relation (3.36)
δ(x −y) = ⟨x|y⟩=
 ∞
−∞
⟨x|p⟩⟨p|y⟩dp =
 ∞
−∞
eipx/¯h
√2π ¯h
e−ipy/¯h
√2π ¯h
dp
=
 ∞
−∞
eip(x−y)/¯h
2π ¯h
dp =
 ∞
−∞
eik(x−y) dk
2π .
(3.67)
If we differentiate ψ(x) as given by (3.65), then we ﬁnd as in (3.53)
¯h
i
d
dxψ(x) =
 ∞
−∞
p ϕ(p) eipx/¯h
dp
√2π ¯h
(3.68)
or
¯h
i
d
dxψ(x) = ⟨x|p|ψ⟩=
 ∞
−∞
⟨x|p|p′⟩⟨p′|ψ⟩dp′ =
 ∞
−∞
p′ ϕ(p′) eip′x/¯h
dp′
√2π ¯h
in Dirac notation.
Example 3.6 (The uncertainty principle)
Let’s ﬁrst normalize the gaussian
ψ(x) = N exp(−(x/a)2) to unity over the real axis
1 = N2
 ∞
−∞
e−2(x/a)2 dx =
&π
2 a N2,
(3.69)
which gives N2 = √2/π/a. So the normalized wave-function is
ψ(x) ≡⟨x|ψ⟩=
 2
π
1/4
1
√a e−(x/a)2.
(3.70)
The mean value ⟨A⟩of an operator A in a state |ψ⟩is
⟨A⟩≡⟨ψ|A|ψ⟩.
(3.71)
More generally, the mean value of an operator A for a system described by a
density operator ρ is the trace
⟨A⟩≡Tr (ρA) .
(3.72)
117

FOURIER AND LAPLACE TRANSFORMS
Since the gaussian (3.70) is an even function of x (that is, ψ(−x) = ψ(x)), the
mean value of the position operator x in the state (3.70) vanishes
⟨x⟩= ⟨ψ|x|ψ⟩=
 ∞
−∞
x |ψ(x)|2 dx = 0.
(3.73)
The variance of an operator A with mean value ⟨A⟩in a state |ψ⟩is the mean
value of the square of the difference A −⟨A⟩
(A)2 ≡⟨ψ| (A −⟨A⟩)2 |ψ⟩.
(3.74)
For a system with density operator ρ, the variance of A is
(A)2 ≡Tr

ρ (A −⟨A⟩)2
.
(3.75)
Since ⟨x⟩= 0, the variance of the position operator x is
(x)2 = ⟨ψ|(x −⟨x⟩)2|ψ⟩= ⟨ψ|x2|ψ⟩
=
 ∞
−∞
x2 |ψ(x)|2 dx = a2
4 .
(3.76)
We can use the Fourier transform to ﬁnd the variance of the momentum
operator. By (3.64), the wave-function ϕ(p) in momentum space is
ϕ(p) = ⟨p|ψ⟩=
 ∞
−∞
⟨p|x⟩⟨x|ψ⟩dx.
(3.77)
By (3.66), the inner product ⟨p|x⟩= ⟨x|p⟩∗is ⟨p|x⟩= e−ipx/¯h/√2π ¯h, so
ϕ(p) = ⟨p|ψ⟩=
 ∞
−∞
dx
√2π ¯h
e−ipx/¯h⟨x|ψ⟩.
(3.78)
Thus by (3.69 & 3.70), ϕ(p) is the Fourier transform
ϕ(p) =
 ∞
−∞
dx
√2π ¯h
e−ipx/¯h
 2
π
1/4
1
√a e−(x/a)2.
(3.79)
Using our formula (3.18) for the Fourier transform of a gaussian, we get
ϕ(p) =
& a
2¯h
 2
π
1/4
e−(ap)2/(2¯h)2.
(3.80)
Since the gaussian ϕ(p) is an even function of p, the mean value ⟨p⟩of the
momentum operator vanishes, like that of the position operator. So the variance
of the momentum operator is
(p)2 = ⟨ψ|(p −⟨p⟩)2|ψ⟩= ⟨ψ|p2 |ψ⟩=
 ∞
−∞
p2 |ϕ(p)|2 dp
=
&
2
π
 ∞
−∞
p2 a
2¯h e−(ap)2/2¯h2 dp = ¯h2
a2 .
(3.81)
118

3.5 FOURIER TRANSFORMS IN SEVERAL DIMENSIONS
Thus in this case, the product of the two variances is
(x)2 (p)2 = a2
4
¯h2
a2 = ¯h2
4 .
(3.82)
This is an example of Heisenberg’s uncertainty principle
x p ≥¯h
2,
(3.83)
which follows from the Fourier-transform relations between the conjugate
variables x and p.
The state |ψ⟩of a free particle at time t = 0
|ψ, 0⟩=
 ∞
−∞
|p⟩⟨p|ψ⟩dp =
 ∞
−∞
|p⟩ϕ(p) dp
(3.84)
evolves under the inﬂuence of the hamiltonian H = p2/(2m) to the state
e−iHt/¯h|ψ, 0⟩=
 ∞
−∞
e−iHt/¯h|p⟩ϕ(p) dp =
 ∞
−∞
e−ip2t/(2¯hm)|p⟩ϕ(p) dp
(3.85)
at time t.
Example 3.7 (The characteristic function)
If P(x) is a probability distribution
normalized to unity over the range of x

P(x) dx = 1
(3.86)
then its Fourier transform is the characteristic function
χ(k) = ˆP(k) =

eikxP(x) dx.
(3.87)
The expected value of a function f (x) is the integral
E[f (x)] =

f (x) P(x) dx.
(3.88)
So the characteristic function χ(k) = E[exp(ikx)] is the expected value of the
exponential exp(ikx), and its derivatives at k = 0 are the moments E[xn] ≡μn of
the probability distribution
E[xn] =

xn P(x) dx = (−i)n dnχ(k)
dkn

k=0
.
(3.89)
We shall pick up this thread in section 13.12.
3.5 Fourier transforms in several dimensions
If f (x1, x2) is a function of two variables, then its double Fourier transform
˜f (k1, k2) is
119

FOURIER AND LAPLACE TRANSFORMS
˜f (k1, k2) =
 ∞
−∞
dx1
√
2π
 ∞
−∞
dx2
√
2π
e−ik1x1−ik2x2 f (x1, x2).
(3.90)
By twice using the Fourier representation (3.36) of Dirac’s delta function, we
may invert this double Fourier transformation
 ∞
−∞
 ∞
−∞
dk1dk2
2π
ei(k1x1+k2x2) ˜f (k1, k2)
=
 ∞
−∞
 ∞
−∞
dk1dk2
2π
 ∞
−∞
 ∞
−∞
dx′
1dx′
2
2π
eik1(x1−x′
1)+ik2(x2−x′
2) f (x′
1, x′
2)
=
 ∞
−∞
dk2
2π
 ∞
−∞
 ∞
−∞
dx′
1dx′
2 eik2(x2−x′
2) δ(x1 −x′
1) f (x′
1, x′
2)
=
 ∞
−∞
 ∞
−∞
dx′
1dx′
2 δ(x1 −x′
1)δ(x2 −x′
2) f (x′
1, x′
2) = f (x1, x2).
(3.91)
That is
f (x1, x2) =
 ∞
−∞
 ∞
−∞
dk1dk2
2π
ei(k1x1+k2x2) ˜f (k1, k2).
(3.92)
The Fourier transform of a function f (x1, . . . , xn) of n variables is
˜f (k1, . . . , kn) =
 ∞
−∞
. . .
 ∞
−∞
dx1 . . . dxn
(2π)n/2
e−i(k1x1+···+knxn) f (x1, . . . , xn)
(3.93)
and its inverse is
f (x1, . . . , xn) =
 ∞
−∞
. . .
 ∞
−∞
dk1 . . . dkn
(2π)n/2
ei(k1x1+···+knxn) ˜f (k1, . . . , kn),
(3.94)
in which all the integrals run from −∞to ∞.
If we generalize the relations (3.12–3.14) between Fourier series and trans-
forms from one to n dimensions, then we ﬁnd that the Fourier series corre-
sponding to the Fourier transform (3.94) is
f (x1, . . . , xn) =
2π
L
n
∞

j1=−∞
. . .
∞

jn=−∞
ei(kj1x1+···+kjnxn) ˜f (kj1, . . . , kjn)
(2π)n/2
(3.95)
in which kjℓ= 2πjℓ/L. Thus, for n = 3 we have
f (x) = (2π)3
V
∞

j1=−∞
∞

j2=−∞
∞

j3=−∞
eikj·x
˜f (kj)
(2π)3/2 ,
(3.96)
in which kj = (kj1, kj2, kj3) and V = L3 is the volume of the box.
120

3.6 CONVOLUTIONS
Example 3.8 (The Feynman propagator)
For a spinless quantum ﬁeld of mass
m, Feynman’s propagator is the four-dimensional Fourier transform
△F (x) =

exp(ik · x)
k2 + m2 −iϵ
d4k
(2π)4
(3.97)
where k · x = k · x −k0x0, all physical quantities are in natural units (c = ¯h = 1),
and x0 = ct = t. The tiny imaginary term −iϵ makes △F(x −y) proportional to
the mean value in the vacuum state |0⟩of the time-ordered product of the ﬁelds
φ(x) and φ(y) (section 5.34)
−i △F (x −y) = ⟨0|T [φ(x)φ(y)] |0⟩
(3.98)
≡θ(x0 −y0)⟨0|φ(x)φ(y)|0⟩+ θ(y0 −x0)⟨0|φ(y)φ(x)|0⟩
in which θ(a) = (a + |a|)/2|a| is the Heaviside function (2.166).
3.6 Convolutions
The convolution of f (x) with g(x) is the integral
f ∗g(x) =
 ∞
−∞
dy
√
2π
f (x −y) g(y).
(3.99)
The convolution product is symmetric
f ∗g(x) = g ∗f (x)
(3.100)
because, setting z = x −y, we have
f ∗g(x) =
 ∞
−∞
dy
√
2π
f (x −y) g(y) = −
 −∞
∞
dz
√
2π
f (z) g(x −z)
=
 ∞
−∞
dz
√
2π
g(x −z) f (z) = g ∗f (x).
(3.101)
Convolutions may look strange at ﬁrst, but they often occur in physics in the
three-dimensional form
F(x) =

G(x −x′) S(x′) d3x,
(3.102)
in which G is a Green’s function and S is a source.
Example 3.9 (Gauss’s law)
The divergence of the electric ﬁeld E is the micro-
scopic charge density ρ divided by the electric permittivity of the vacuum
ϵ0 = 8.854 × 10−12 F/m, that is, ∇· E = ρ/ϵ0. This constraint is known as
Gauss’s law. If the charges and ﬁelds are independent of time, then the electric
121

FOURIER AND LAPLACE TRANSFORMS
ﬁeld E is the gradient of a scalar potential E = −∇φ. These last two equations
imply that φ obeys Poisson’s equation
−∇2φ = ρ
ϵ0
.
(3.103)
We may solve this equation by using Fourier transforms as described in
section 3.13. If ˜φ(k) and ˜ρ(k) respectively are the Fourier transforms of φ(x)
and ρ(x), then Poisson’s differential equation (3.103) gives
−∇2φ(x) = −∇2

eik·x ˜φ(k) d3k =

k2 eik·x ˜φ(k) d3k
= ρ(x)
ϵ0
=

eik·x ˜ρ(k)
ϵ0
d3k,
(3.104)
which implies the algebraic equation ˜φ(k) = ˜ρ(k)/ϵ0k2, which is an instance of
(3.166). Performing the inverse Fourier transformation, we ﬁnd for the scalar
potential
φ(x) =

eik·x ˜φ(k) d3k =

eik·x ˜ρ(k)
ϵ0 k2 d3k
(3.105)
=

eik·x 1
k2

e−ik·x′ ρ(x′)
ϵ0
d3x′d3k
(2π)3
=

G(x −x′) ρ(x′)
ϵ0
d3x′,
in which
G(x −x′) =

d3k
(2π)3
1
k2 eik·(x−x′).
(3.106)
This function G(x −x′) is the Green’s function for the differential operator −∇2
in the sense that
−∇2G(x −x′) =

d3k
(2π)3 eik·(x−x′) = δ(3)(x −x′).
(3.107)
This Green’s function ensures that expression (3.105) for φ(x) satisﬁes Poisson’s
equation (3.103). To integrate (3.106) and compute G(x −x′), we use spherical
coordinates with the z-axis parallel to the vector x −x′
G(x −x′) =

d3k
(2π)3
1
k2 eik·(x−x′) =
 ∞
0
dk
(2π)2
 1
−1
d cos θ eik|x−x′| cos θ
=
 ∞
0
dk
(2π)2
eik|x−x′| −e−ik|x−x′|
ik|x −x′|
(3.108)
=
1
2π2|x −x′|
 ∞
0
sin k|x −x′| dk
k
=
1
2π2|x −x′|
 ∞
0
sin k dk
k
.
In example 5.35 of section 5.18 on Cauchy’s principal value, we’ll show that
 ∞
0
sin k
k
dk = π
2 .
(3.109)
122

3.7 THE FOURIER TRANSFORM OF A CONVOLUTION
Using this result, we have

d3k
(2π)3
1
k2 eik·(x−x′) = G(x −x′) =
1
4π|x −x′|.
(3.110)
Finally, by substituting this formula for G(x −x′) into Equation (3.105), we ﬁnd
that the Fourier transform φ(x) of the product ˆρ(k)/k2 of the functions ˆρ(k) and
1/k2 is the convolution
φ(x) =
1
4πϵ0

ρ(x′)
|x −x′| d3x
(3.111)
of their Fourier transforms 1/|x −x′| and ρ(x′). The Fourier transform of the
product of any two functions is the convolution of their Fourier transforms, as
we’ll see in the next section (George Green, 1793–1841).
Example 3.10 (The magnetic vector potential)
The magnetic induction B
has zero divergence (as long as there are no magnetic monopoles) and so
may be written as the curl B = ∇× A of a vector potential A. For time-
independent currents, Ampère’s law is ∇× B = μ0J, in which μ0 = 1/(ϵ0c2) =
4π × 10−7 N A−2 is the permeability of the vacuum. It follows that in the
Coulomb gauge ∇· A = 0, the magnetostatic vector potential A satisﬁes the
equation
∇× B = ∇× (∇× A) = ∇(∇· A) −∇2A = −∇2A = μ0J.
(3.112)
Applying the Fourier-transform technique (3.103–3.111), we ﬁnd that the
Fourier transforms of A and J satisfy the algebraic equation
ˆA(k) = μ0
ˆJ(k)
k2 ,
(3.113)
which is an instance of (3.166). Performing the inverse Fourier transform, we see
that A is the convolution
A(x) = μ0
4π

d3x′
J(x′)
|x −x′|.
(3.114)
If in the solution (3.111) of Poisson’s equation, ρ(x) is translated by a, then so
is φ(x). That is, if ρ′(x) = ρ(x + a) then φ′(x) = φ(x + a). Similarly, if the cur-
rent J(x) in (3.114) is translated by a, then so is the potential A(x). Convolutions
respect translational invariance. That’s one reason why they occur so often in the
formulas of physics.
3.7 The Fourier transform of a convolution
The Fourier transform of the convolution f ∗g is the product of the Fourier
transforms ˜f and ˜g:
'
f ∗g(k) = ˜f (k) ˜g(k).
(3.115)
123

FOURIER AND LAPLACE TRANSFORMS
To see why, we form the Fourier transform '
f ∗g(k) of the convolution f ∗g(x)
'
f ∗g(k) =
 ∞
−∞
dx
√
2π
e−ikx f ∗g(x)
=
 ∞
−∞
dx
√
2π
e−ikx
 ∞
−∞
dy
√
2π
f (x −y) g(y).
(3.116)
Now we write f (x−y) and g(y) in terms of their Fourier transforms ˜f (p) and ˜g(q)
'
f ∗g(k) =
 ∞
−∞
dx
√
2π
e−ikx
 ∞
−∞
dy
√
2π
 ∞
−∞
dp
√
2π
˜f (p) eip(x−y)
 ∞
−∞
dq
√
2π
˜g(q) eiqy
(3.117)
and use the representation (3.36) of Dirac’s delta function twice to get
'
f ∗g(k) =
 ∞
−∞
dy
2π
 ∞
−∞
dp
 ∞
−∞
dq δ(p −k) ˜f (p) ˜g(q) ei(q−p)y
=
 ∞
−∞
dp
 ∞
−∞
dq δ(p −k) δ(q −p) ˜f (p) ˜g(q)
=
 ∞
−∞
dp δ(p −k) ˜f (p) ˜g(p) = ˜f (k) ˜g(k),
(3.118)
which is (3.115). Examples 3.9 and 3.10 were illustrations of this result.
3.8 Fourier transforms and Green’s functions
A Green’s function G(x) for a differential operator P turns into a delta function
when acted upon by P, that is, PG(x) = δ(x). If the differential operator is
a polynomial P(∂) ≡P(∂1, . . . , ∂n) in the derivatives ∂1, . . . , ∂n with constant
coefﬁcients, then a suitable Green’s function G(x) ≡G(x1, . . . , xn) will satisfy
P(∂)G(x) = δ(n)(x).
(3.119)
Expressing both G(x) and δ(n)(x) as Fourier transforms, we get
P(∂)G(x) =

dnk P(ik) eik·x ˜G(k) = δ(n)(x) =

dnk
(2π)n eik·x,
(3.120)
which gives us the algebraic equation
˜G(k) =
1
(2π)n P(ik).
(3.121)
Thus the Green’s function GP for the differential operator P(∂) is
GP(x) =

dnk
(2π)n
eik·x
P(ik).
(3.122)
124

3.9 LAPLACE TRANSFORMS
Example 3.11 (Green and Yukawa)
In 1935, Hideki Yukawa (1907–1981)
proposed the partial differential equation
PY(∂)GY(x) ≡(−△+ m2)GY(x) = (−∇2 + m2)GY(x) = 0.
(3.123)
Our (3.122) gives as the Green’s function for PY(∂) the Yukawa potential
GY(x) =

d3k
(2π)3
eik·x
PY(ik) =

d3k
(2π)3
eik·x
k2 + m2 = e−mr
4πr ,
(3.124)
an integration done in example 5.21.
3.9 Laplace transforms
The Laplace transform f (s) of a function F(t) is the integral
f (s) =
 ∞
0
dt e−st F(t).
(3.125)
Because the integration is over positive values of t, the exponential exp(−st)
falls off rapidly with the real part of s. As Re s increases, the Laplace transform
f (s) becomes smoother and smaller. For Re s > 0, the exponential exp(−st) lets
many functions F(t) that are not integrable over the half line [0, ∞) have well-
behaved Laplace transforms.
For instance, the function F(t) = 1 is not integrable over the half line [0, ∞),
but its Laplace transform
f (s) =
 ∞
0
dt e−st F(t) =
 ∞
0
dt e−st = 1
s
(3.126)
is well deﬁned for Re s > 0 and square integrable for Re s > ϵ.
The function F(t) = exp(kt) diverges exponentially for Re k > 0, but its
Laplace transform
f (s) =
 ∞
0
dt e−st F(t) =
 ∞
0
dt e−(s−k)t =
1
s −k
(3.127)
is well deﬁned for Re s > k with a simple pole at s = k (section 5.10) and is
square integrable for Re s > k + ϵ.
The Laplace transforms of cosh kt and sinh kt are
f (s) =
 ∞
0
dt e−st cosh kt = 1
2
 ∞
0
dt e−st 
ekt + e−kt
=
s
s2 −k2
(3.128)
and
f (s) =
 ∞
0
dt e−st sinh kt = 1
2
 ∞
0
dt e−st 
ekt −e−kt
=
k
s2 −k2 .
(3.129)
125

FOURIER AND LAPLACE TRANSFORMS
The Laplace transform of cos ωt is
f (s) =
 ∞
0
dt e−st cos ωt = 1
2
 ∞
0
dt e−st 
eiωt + e−iωt
=
s
s2 + ω2
(3.130)
and that of sin ωt is
f (s) =
 ∞
0
dt e−st sin ωt = 1
2i
 ∞
0
dt e−st 
eiωt −e−iωt
=
ω
s2 + ω2 .
(3.131)
Example 3.12 (Lifetime of a ﬂuorophore)
Fluorophores are molecules that
emit visible light when excited by photons. The probability P(t, t′) that a ﬂuo-
rophore with a lifetime τ will emit a photon at time t if excited by a photon at
time t′ is
P(t, t′) = τ e−(t−t′)/τ θ(t −t′)
(3.132)
in which θ(t −t′) = (t −t′ + |t −t′|)/2|t −t′| is the Heaviside function. One way
to measure the lifetime τ of a ﬂuorophore is to modulate the exciting laser beam
at a frequency ν = 2πω of the order of 60 MHz and to detect the phase-shift φ
in the light L(t) emitted by the ﬂuorophore. That light is the integral of P(t, t′)
times the modulated beam sin ωt or equivalently the convolution of e−t/τθ(t)
with sin ωt
L(t) =
 ∞
−∞
P(t, t′) sin(ωt′) dt′ =
 ∞
−∞
τ e−(t−t′)/τθ(t −t′) sin(ωt′) dt′
=
 t
−∞
τ e−(t−t′)/τ sin(ωt′) dt′.
(3.133)
Letting u = t −t′ and using the trigonometric formula
sin(a −b) = sin a cos b −cos a sin b
(3.134)
we may relate this integral to the Laplace transforms of a sine (3.131) and a
cosine (3.130)
L(t) = −τ
 ∞
0
e−u/τ sin ω(u −t) du
= −τ
 ∞
0
e−u/τ (sin ωu cos ωt −cos ωu sin ωt) du
= τ
 sin(ωt)/τ
1/τ 2 + ω2 −ω cos ωt
1/τ 2 + ω2

.
(3.135)
Setting cos φ = (1/τ)/

1/τ 2 + ω2 and sin φ = ω/

1/τ 2 + ω2, we have
L(t) =
τ

1/τ 2 + ω2 (sin ωt cos φ −cos ωt sin φ) =
τ

1/τ 2 + ω2 sin(ωt −φ).
(3.136)
126

3.10 DERIVATIVES AND INTEGRALS OF LAPLACE TRANSFORMS
The phase-shift φ then is given by
φ = arcsin
ω

1/τ 2 + ω2 ≤π
2 .
(3.137)
So by inverting this formula, we get the lifetime of the ﬂuorophore
τ = (1/ω) tan φ
(3.138)
in terms of the phase-shift φ, which is much easier to measure.
3.10 Derivatives and integrals of Laplace transforms
The derivatives of a Laplace transform f (s) are by its deﬁnition (3.125)
dnf (s)
dsn
=
 ∞
0
dt (−t)n e−st F(t).
(3.139)
They usually are well deﬁned if f (s) is well deﬁned. For instance, if we differ-
entiate the Laplace transform f (s) = 1/s of the function F(t) = 1 as given by
(3.126), then we ﬁnd
(−1)n dns−1
dsn
=
n!
sn+1 =
 ∞
0
dt e−st tn,
(3.140)
which tells us that the Laplace transform of tn is n!/sn+1.
The result of differentiating the function F(t) also has a simple form.
Integrating by parts, we ﬁnd for the Laplace transform of F′(t)
 ∞
0
dt e−st F′(t) =
 ∞
0
dt
 d
dt

e−st F(t)

−F(t) d
dt e−st
%
= −F(0) +
 ∞
0
dt F(t) s e−st
= −F(0) + s f (s).
(3.141)
The indeﬁnite integral of the Laplace transform (3.125) is
‵f (s) ≡

ds1 f (s1) =
 ∞
0
dt e−st
(−t) F(t)
(3.142)
and its nth indeﬁnite integral is
(n)f (s) ≡

dsn . . .

ds1 f (s1) =
 ∞
0
dt e−st
(−t)n F(t).
(3.143)
If f (s) is a well-behaved function, then these indeﬁnite integrals usually are well
deﬁned, except possibly at s = 0.
127

FOURIER AND LAPLACE TRANSFORMS
3.11 Laplace transforms and differential equations
Suppose we wish to solve the differential equation
P(d/ds) f (s) = j(s).
(3.144)
By writing f (s) and j(s) as Laplace transforms
f (s) =
 ∞
0
e−st F(t) dt
j(s) =
 ∞
0
e−st J(t) dt
(3.145)
and using the formula (3.139) for the nth derivative of a Laplace transform, we
see that the differential equation (3.144) amounts to
 ∞
0
e−st P(−t) F(t) dt =
 ∞
0
e−st J(t) dt,
(3.146)
which is equivalent to the algebraic equation
F(t) =
J(t)
P(−t).
(3.147)
A particular solution to the inhomogeneous equation (3.144) is then the
Laplace transform of this ratio
f (s) =
 ∞
0
e−st J(t)
P(−t) dt.
(3.148)
A fairly general solution of the associated homogeneous equation
P(d/ds) f (s) = 0
(3.149)
is the Laplace transform
f (s) =
 ∞
0
e−st δ(P(−t)) H(t) dt,
(3.150)
in which the function H(t) is arbitrary. Thus our solution of the inhomogeneous
equation (3.144) is the sum of the two
f (s) =
 ∞
0
e−st J(t)
P(−t) dt +
 ∞
0
e−st δ(P(−t)) H(t) dt.
(3.151)
One may generalize this method to differential equations in n variables. But to
carry out this procedure, one must be able to ﬁnd the inverse Laplace transform
J(t) of the source function j(s) as outlined in the next section.
128

3.13 APPLICATION TO DIFFERENTIAL EQUATIONS
3.12 Inversion of Laplace transforms
How do we invert the Laplace transform
f (s) =
 ∞
0
dt e−st F(t)?
(3.152)
First we extend the Laplace transform from real s to s + iu
f (s + iu) =
 ∞
0
dt e−(s+iu)t F(t).
(3.153)
Next we choose s to be sufﬁciently positive that the integral
 ∞
−∞
du
2π e(s+iu)t f (s + iu) =
 ∞
−∞
du
2π
 ∞
0
dt′ e(s+iu)t e−(s+iu)t′ F(t′)
(3.154)
converges, and then we apply to it the delta function formula (3.36)
 ∞
−∞
du
2π e(s+iu)t f (s + iu) =
 ∞
0
dt′ es(t−t′) F(t′)
 ∞
−∞
du
2π eiu(t−t′)
=
 ∞
0
dt′ es(t−t′) F(t′) δ(t −t′) = F(t). (3.155)
So our inversion formula is
F(t) = est
 ∞
−∞
du
2π eiut f (s + iu)
(3.156)
for sufﬁciently large s. Some call this inversion formula a Bromwich integral,
others a Fourier–Mellin integral.
3.13 Application to differential equations
Let us consider a linear partial differential equation in n variables
P(∂1, . . . , ∂n) f (x1, . . . , xn) = g(x1, . . . , xn),
(3.157)
in which P is a polynomial in the derivatives
∂j ≡
∂
∂xj
(3.158)
with constant coefﬁcients. If g = 0, the equation is homogeneous; otherwise it
is inhomogeneous. We expand solution and source as integral transforms
f (x1, . . . , xn) =

˜f (k1, . . . , kn) ei(k1x1+···+knxn)dnk,
g(x1, . . . , xn) =

˜g(k1, . . . , kn) ei(k1x1+···+knxn)dnk,
(3.159)
129

FOURIER AND LAPLACE TRANSFORMS
in which the k integrals may run from −∞to ∞as in a Fourier transform or
up the imaginary axis from 0 to ∞as in a Laplace transform.
The correspondence (3.55) between differentiation with respect to xj and
multiplication by ikj tells us that ∂m
j acting on f gives
∂m
j f (x1, . . . , xn) =

˜f (k1, . . . , kn) (ikj)m ei(k1x1+···+knxn) dnk.
(3.160)
If we abbreviate f (x1, . . . , xn) by f (x) and do the same for g, then we may write
our partial differential equation (3.157) as
P(∂1, . . . , ∂n)f (x) =

˜f (k) P(ik1, . . . , ikn) ei(k1x1+···+knxn) dnk
=

˜g(k) ei(k1x1+···+knxn) dnk.
(3.161)
Thus the inhomogeneous partial differential equation
P(∂1, . . . , ∂n) fi(x1, . . . , xn) = g(x1, . . . , xn)
(3.162)
becomes an algebraic equation in k-space
P(ik1, . . . , ikn) ˜fi(k1, . . . , kn) = ˜g(k1, . . . , kn)
(3.163)
where ˜g(k1, . . . , kn) is the mixed Fourier–Laplace transform of g(x1, . . . , xn). So
one solution of the inhomogeneous differential equation (3.157) is
fi(x1, . . . , xn) =

ei(k1x1+···+knxn) ˜g(k1, . . . , kn)
P(ik1, . . . , ikn) dnk.
(3.164)
The space of solutions to the homogeneous form of equation (3.157)
P(∂1, . . . , ∂n) fh(x1, . . . , xn) = 0
(3.165)
is vast. We will focus on those that satisfy the algebraic equation
P(ik1, . . . , ikn) ˜fh(k1, . . . , kn) = 0
(3.166)
and that we can write in terms of Dirac’s delta function as
˜fh(k1, . . . , kn) = δ(P(ik1, . . . , ikn)) h(k1, . . . , kn),
(3.167)
in which the function h(k) is arbitrary. That is
fh(x) =

ei(k1x1+···+knxn)δ(P(ik1, . . . , ikn)) h(k) dnk.
(3.168)
130

3.13 APPLICATION TO DIFFERENTIAL EQUATIONS
Our solution to the differential equation (3.157) then is a sum of a partic-
ular solution (3.164) of the inhomogeneous equation (3.163) and our solution
(3.168) of the associated homogeneous equation (3.165)
f (x1, . . . , xn) =

ei(k1x1+···+knxn)
# ˜g(k1, . . . , kn)
P(ik1, . . . , ikn)
+ δ(P(ik1, . . . , ikn)) h(k1, . . . , kn)
$
dnk
(3.169)
in which h(k1, . . . , kn) is an arbitrary function. The wave equation and the
diffusion equation will provide examples of this formula
f (x) =

eik·x
# ˜g(k)
P(ik) + δ(P(ik))h(k)
$
dnk.
(3.170)
Example 3.13 (Wave equation for a scalar ﬁeld)
A free scalar ﬁeld φ(x) of mass
m in ﬂat space-time obeys the wave equation

∇2 −∂2
t −m2
φ(x) = 0
(3.171)
in natural units (¯h = c = 1). We may use a four-dimensional Fourier transform
to represent the ﬁeld φ(x) as
φ(x) =

eik·x ˜φ(k) d4k
(2π)2 ,
(3.172)
in which k · x = k · x −k0t is the Lorentz-invariant inner product.
The homogeneous wave equation (3.171) then says

∇2 −∂2
t −m2
φ(x) =
 
−k2 + (k0)2 −m2
eik·x ˜φ(k) d4k
(2π)2 = 0,
(3.173)
which implies the algebraic equation

−k2 + (k0)2 −m2
˜φ(k) = 0
(3.174)
an instance of (3.166). Our solution (3.168) is
φ(x) =

δ

−k2 + (k0)2 −m2
eik·x h(k) d4k
(2π)2 ,
(3.175)
in which h(k) is an arbitrary function. The argument of the delta function
gk(k0) ≡(k0)2 −k2 −m2 =

k0 −

k2 + m2
 
k0 +

k2 + m2

(3.176)
has zeros at k0 = ±

k2 + m2 ≡±ωk with |g′
k(±ωk)| = 2ωk. So using our
formula (3.46) for integrals involving delta functions of functions, we have
131

FOURIER AND LAPLACE TRANSFORMS
φ(x) =
 
ei(k·x−ωkt) h+(k) + ei(k·x+ωkt) h−(k)

d3k
(2π)22ωk
(3.177)
where h±(k) ≡h(±ωk, k). Since ωk is an even function of k, we can write
φ(x) =
 
ei(k·x−ωkt) h+(k) + e−i(k·x−ωkt) h−(−k)

d3k
(2π)22ωk
.
(3.178)
If φ(x) is a real-valued classical ﬁeld, then (3.24) tells us that h−(−k) = h+(k)∗.
If it is a hermitian quantum ﬁeld, then h−(−k) = h†
+(k). One sets a(k) ≡
h+(k)/√4πωk and writes φ(x) as an integral over a(k), an annihilation operator,
φ(x) =
 
ei(k·x−ωkt) a(k) + e−i(k·x−ωkt) a†(k)

d3k

(2π)32ωk
,
(3.179)
and its adjoint a†(k), a creation operator.
The momentum π canonically conjugate to the ﬁeld is its time derivative
π(x) = −i
 
ei(k·x−ωkt) a(k) −e−i(k·x−ωkt) a†(k)
 &
ωk
2(2π)3 d3k.
(3.180)
If the operators a and a† obey the commutation relations
[a(k), a†(k′)] = δ(k −k′)
and
[a(k), a(k′)] = [a†(k), a†(k′)] = 0
(3.181)
then the ﬁeld φ(x, t) and its conjugate momentum π(y, t) satisfy (exercise 3.11)
the equal-time commutation relations
[φ(x, t), π(y, t)] = iδ(x −y)
and
[φ(x, t), φ(y, t)] = [π(x, t), π(y, t)] = 0,
(3.182)
which generalize the commutation relations of quantum mechanics
[qj, pℓ] = i¯hδj,ℓ
and
[qj, qℓ] = [pj, pℓ] = 0
(3.183)
for a set of coordinates qj and conjugate momenta pℓ.
Example 3.14 (Fourier series for a scalar ﬁeld)
For a ﬁeld deﬁned in a cube of
volume V = L3, one often imposes periodic boundary conditions (section 2.13)
in which a displacement of any spatial coordinate by ±L does not change the
value of the ﬁeld. A Fourier series can represent a periodic ﬁeld. Using the rela-
tionship (3.96) between Fourier-transform and Fourier-series representations in
three dimensions, we expect the Fourier series representation for the ﬁeld (3.179)
to be
φ(x) = (2π)3
V

k
1

(2π)32ωk

a(k)ei(k·x−ωkt) + a†(k)e−i(k·x−ωkt)
=

k
1
√2ωkV
(
(2π)3
V

a(k)ei(k·x−ωkt) + a†(k)e−i(k·x−ωkt)
, (3.184)
132

3.13 APPLICATION TO DIFFERENTIAL EQUATIONS
in which the sum over k = (2π/L)(ℓ, n, m) is over all (positive and negative)
integers ℓ, n, and m. One can set
ak ≡
(
(2π)3
V
a(k)
(3.185)
and write the ﬁeld as
φ(x) =

k
1
√2ωk V

ak ei(k·x−ωkt) + a†
k e−i(k·x−ωkt)
.
(3.186)
The commutator of Fourier-series annihilation and creation operators is by
(3.36, 3.181, & 3.185)
[ak, a†
k′] = (2π)3
V
[a(k), a†(k′)] = (2π)3
V
δ(k −k′)
= (2π)3
V

ei(k−k′)·x d3x
(2π)3 = (2π)3
V
V
(2π)3 δk,k′ = δk,k′, (3.187)
in which the Kronecker delta δk,k′ is δℓ,ℓ′δn,n′δm,m′.
Example 3.15 (Diffusion)
The ﬂow rate J (per unit area, per unit time) of a
ﬁxed number of randomly moving particles, such as molecules of a gas or a
liquid, is proportional to the negative gradient of their density ρ(x, t)
J(x, t) = −D∇ρ(x, t)
(3.188)
where D is the diffusion constant, an equation known as Fick’s law (Adolf Fick,
1829–1901). Since the number of particles is conserved, the 4-vector J = (ρ, J)
obeys the conservation law
∂
∂t

ρ(x, t) d3x = −
)
J(x, t) · da = −

∇· J(x, t)d3x,
(3.189)
which with Fick’s law (3.188) gives the diffusion equation
˙ρ(x, t) = −∇· J(x, t) = D∇2ρ(x, t)
or

D∇2 −∂t

ρ(x, t) = 0.
(3.190)
Fourier had in mind such equations when he invented his transform.
If we write the density ρ(x, t) as the transform
ρ(x, t) =

eik·x+iωt ˜ρ(k, ω) d3kdω
(3.191)
then the diffusion equation becomes

D∇2 −∂t

ρ(x, t) =

eik·x+iωt 
−Dk2 −iω

˜ρ(k, ω) d3kdω = 0,
(3.192)
which implies the algebraic equation

Dk2 + iω

˜ρ(k, ω) = 0.
(3.193)
133

FOURIER AND LAPLACE TRANSFORMS
Our solution (3.168) of this homogeneous equation is
ρ(x, t) =

eik·x+iωtδ

−Dk2 −iω

h(k, ω) d3kdω,
(3.194)
in which h(k, ω) is an arbitrary function. Dirac’s delta function requires ω to be
imaginary ω = iDk2, with Dk2 > 0. So the ω-integration is up the imaginary
axis. It is a Laplace transform, and we have
ρ(x, t) =
 ∞
−∞
eik·x−Dk2t ˜ρ(k) d3k,
(3.195)
in which ˜ρ(k) ≡h(k, iDk2). Thus the function ˜ρ(k) is the Fourier transform of
the initial density ρ(x, 0)
ρ(x, 0) =
 ∞
−∞
eik·x ˜ρ(k) d3k.
(3.196)
So if the initial density ρ(x, 0) is concentrated at y
ρ(x, 0) = δ(x −y) =
 ∞
−∞
eik·(x−y) d3k
(2π)3
(3.197)
then its Fourier transform ˜ρ(k) is
˜ρ(k) = e−ik·y
(2π)3
(3.198)
and at later times the density ρ(x, t) is given by (3.195) as
ρ(x, t) =
 ∞
−∞
eik·(x−y)−Dk2t d3k
(2π)3 .
(3.199)
Using our formula (3.18) for the Fourier transform of a gaussian, we ﬁnd
ρ(x, t) =
1
(4πDt)3/2 e−(x−y)2/(4Dt).
(3.200)
Since the diffusion equation is linear, it follows (exercise 3.12) that an arbitrary
initial distribution ρ(y, 0) evolves to the distribution
ρ(x, t) =
1
(4πDt)3/2

e−(x−y)2/(4Dt) ρ(y, 0) d3y
(3.201)
at time t. Such convolutions often occur in physics (section 3.6).
Exercises
3.1
Show that the Fourier integral formula (3.26) for real functions follows from
(3.9) and (3.25).
3.2
Show that the Fourier integral formula (3.26) for real functions implies (3.27)
if f is even and (3.28) if it is odd.
134

EXERCISES
3.3
Derive the formula (3.30) for the square wave (3.29).
3.4
By using the Fourier-transform formulas (3.27 & 3.28), derive the formu-
las (3.31) and (3.32) for the even and odd extensions of the exponential
exp(−β|x|).
3.5
For the state |ψ, t⟩given by equations (3.85 & 3.80), ﬁnd the wave-function
ψ(x, t) = ⟨x|ψ, t⟩at time t. Then ﬁnd the variance of the position operator at
that time. Does it grow as time goes by?
3.6
At time t = 0, a particle of mass m is in a gaussian superposition of momen-
tum eigenstates centered at p = ¯hK:
ψ(x, 0) = N
 ∞
−∞
eikxe−l2(k−K)2dk.
(3.202)
(a) Shift k by K and do the integral. Where is the particle most likely to be
found? (b) At time t, the wave function ψ(x, t) is ψ(x, 0) but with ikx replaced
by ikx−i¯hk2t/2m. Shift k by K and do the integral. Where is the particle most
likely to be found? (c) Does the wave packet spread out like t or like √t as in
classical diffusion?
3.7
Express a probability distribution P(x) as the Fourier transform of its
characteristic function (3.87).
3.8
Express the characteristic function (3.87) of a probability distribution as a
power series in its moments (3.89).
3.9
Find the characteristic function (3.87) of the gaussian probability distribution
PG(x, μ, σ) =
1
σ
√
2π
exp

−(x −μ)2
2σ 2

.
(3.203)
3.10 Find the moments μn = E[xn] for n = 0, . . . , 3 of the gaussian probability
distribution PG(x, μ, σ).
3.11 Show that the commutation relations (3.181) of the annihilation and creation
operators imply the equal-time commutation relations (3.182) for the ﬁeld φ
and its conjugate momentum π.
3.12 Use the linearity of the diffusion equation and equations (3.197–3.200) to
derive the general solution (3.201) of the diffusion equation.
3.13 Derive (3.112) from B = ∇× A and Ampère’s law ∇× B = μ0J.
3.14 Derive (3.113) from (3.112).
3.15 Derive (3.114) from (3.113).
3.16 Use the Green’s function relations (3.107) and (3.108) to show that (3.114)
satisﬁes (3.112).
3.17 Show that the Laplace transform of tz−1 is the gamma function (4.55) divided
by sz
f (s) =
 ∞
0
e−st tz−1 dt = s−z (z).
(3.204)
3.18 Compute the Laplace transform of 1/√t. Hint: let t = u2.
135

4
Inﬁnite series
4.1 Convergence
A sequence of partial sums
SN =
N

n=0
cn
(4.1)
converges to a number S if for every ϵ > 0 there exists an integer N(ϵ) such that
|S −SN| < ϵ
for all
N > N(ϵ).
(4.2)
The number S is then said to be the limit of the convergent inﬁnite series
S =
∞

n=0
cn = lim
N→∞SN = lim
N→∞
N

n=0
cn.
(4.3)
Some series converge; others wander or oscillate; and others diverge.
A series whose absolute values converge
S =
∞

n=0
|cn|
(4.4)
is said to converge absolutely. A convergent series that is not absolutely
convergent is said to converge conditionally.
Example 4.1 (Two inﬁnite series)
The series of inverse factorials converges to
the number e = 2.718281828 . . .
∞

n=0
1
n! = e.
(4.5)
136

4.2 TESTS OF CONVERGENCE
But the harmonic series of inverse integers diverges
∞

k=1
1
k →∞
(4.6)
as one may see by grouping its terms
1 + 1
2 +
1
3 + 1
4

+
1
5 + 1
6 + 1
7 + 1
8

+ · · · ≥1 + 1
2 + 1
2 + 1
2 + · · ·
(4.7)
to form a series that obviously diverges. This series up to 1/n approaches the
natural logarithm ln n to within a constant
γ = lim
n→∞
 n

k=1
1
k −ln n

= 0.5772156649 . . .
(4.8)
known as the Euler–Mascheroni constant (Leonhard Euler, 1707–1783; Lorenzo
Mascheroni, 1750–1800).
4.2 Tests of convergence
The Cauchy criterion for the convergence of a sequence SN is that for every
ϵ > 0 there is an integer N(ϵ) such that for N > N(ϵ) and M > N(ϵ) one has
|SN −SM| < ϵ.
(4.9)
Cauchy’s criterion is equivalent to the deﬁning condition (4.2).
Suppose the convergent series
∞

n=0
bn
(4.10)
has only positive terms bn ≥0, and that |cn| ≤bn for all n. Then the series
∞

n=0
cn
(4.11)
also (absolutely) converges. This is the comparison test.
Similarly, if for all n, the inequality 0 ≤cn ≤bn holds and the series of
numbers cn diverges, then so does the series of numbers bn.
If for some N, the terms cn satisfy
|cn|1/n ≤x < 1
(4.12)
for all n > N, then the series
∞

n=0
cn
(4.13)
converges by the Cauchy root test.
137

INFINITE SERIES
In the ratio test of d’Alembert, the series 
n cn converges if
lim
n→∞

cn+1
cn
 = r < 1
(4.14)
and diverges if r > 1.
Probably the most useful test is the Intel test, in which one writes a computer
program to sum the ﬁrst N terms of the series and then runs it for N = 100,
N = 10, 000, N = 1, 000, 000, ..., as seems appropriate.
4.3 Convergent series of functions
A sequence of partial sums
SN(z) =
N

n=0
fn(z)
(4.15)
of functions fn(z) converges to a function S(z) on a set D if for every ϵ > 0 and
every z ∈D, there exists an integer N(ϵ, z) such that
|S(z) −SN(z)| < ϵ
for all
N > N(ϵ, z).
(4.16)
The numbers z may be real or complex. The function S(z) is said to be the limit
on D of the convergent inﬁnite series of functions
S(z) =
∞

n=0
fn(z).
(4.17)
A sequence of partial sums SN(z) of functions converges uniformly on the set
D if the integers N(ϵ, z) can be chosen independently of the point z ∈D, that is,
if for every ϵ > 0 and every z ∈D, there exists an integer N(ϵ) such that
|S(z) −SN(z)| < ϵ
for all
N > N(ϵ).
(4.18)
The limit (2.52) of the integral over a closed interval a ≤x ≤b of a uniformly
convergent sequence of partial sums SN(x) of continuous functions is equal to
the integral of the limit
lim
N→∞
 b
a
SN(x) dx =
 b
a
S(x) dx.
(4.19)
A real or complex-valued function f (x) of a real variable x is square integrable
on an interval [a, b] if the integral
 b
a
|f (x)|2 dx
(4.20)
138

4.4 POWER SERIES
exists and is ﬁnite. A sequence of partial sums
SN(x) =
N

n=0
fn(x)
(4.21)
of square-integrable functions fn(x) converges in the mean to a function S(x) if
lim
N→∞
 b
a
|S(x) −SN(x)|2 dx = 0.
(4.22)
Convergence in the mean sometimes is deﬁned as
lim
N→∞
 b
a
ρ(x) |S(x) −SN(x)|2 dx = 0,
(4.23)
in which ρ(x) ≥0 is a weight function that is positive except at isolated points
where it may vanish. If the functions fn are real, then this deﬁnition of
convergence in the mean takes the slightly simpler form
lim
N→∞
 b
a
ρ(x) (S(x) −SN(x))2 dx = 0.
(4.24)
4.4 Power series
A power series is a series of functions with fn(z) = cn zn
S(z) =
∞

n=0
cn zn.
(4.25)
By the ratio test (4.14), this power series converges if
lim
n→∞

cn+1zn+1
cnzn
 = |z| lim
n→∞

cn+1
cn
 ≡|z|
R < 1,
(4.26)
that is, if z lies within a circle of radius R
|z| < R
(4.27)
given by
R =

lim
n→∞
|cn+1|
|cn|
−1
.
(4.28)
Within this circle, the convergence is uniform and absolute.
139

INFINITE SERIES
Example 4.2 (geometric series)
For any positive integer N, the simple identity
(1 −z)(1 + z + z2 + · · · + zN) = 1 −zN+1
(4.29)
implies that
SN(z) =
N

n=0
zn = 1 −zN+1
1 −z
.
(4.30)
For |z| < 1, the term |zN+1| →0 as N →∞, and so the power series
S∞(z) =
∞

n=0
zn =
1
1 −z
(4.31)
converges to the function 1/(1 −z) as long as the absolute value of z is less than
unity. The radius of convergence R is unity in agreement with the estimate (4.28)
R =

lim
n→∞
|cn+1|
|cn|
−1
=

lim
n→∞1
−1
= 1.
(4.32)
For tiny z, the approximation
1
1 ± z ≈1 ∓z
(4.33)
is useful.
Example 4.3 (Credit)
If a man deposits $100 in a bank, he has a credit of $100.
Suppose banks are required to retain as reserves 10% of their deposits and are
free to lend the other 90%. Then the bank getting the $100 deposit can lend out
$90 to a borrower. That borrower can deposit $90 in another bank. That bank
can then lend $81 to another borrower. Now three people have credits of $100 +
$90 + $81 = $271. This multiplication of money is the miracle of credit.
If P is the original deposit and r is the fraction of deposits that banks must
retain as reserves, then the total credit due to P is
P + P(1 −r) + P(1 −r)2 + · · · = P
∞

n=0
(1 −r)n = P

1
1 −(1 −r)

= P
r . (4.34)
An initial deposit of P = $100 with r = 10% can produce total credits of P/r =
$1000. A reserve requirement of 1% can lead to total credits of $10,000. Since
banks charge a higher rate of interest on money they lend than the rate they
pay to their depositors, bank proﬁts soar as r →0. This is why bankers love
deregulation.
The funds all the banks hold in reserve due to a deposit P is
Pr + (1 −r)Pr + (1 −r)2Pr + · · · = Pr
∞

n=0
(1 −r)n = P,
(4.35)
P itself.
140

4.5 FACTORIALS AND THE GAMMA FUNCTION
4.5 Factorials and the gamma function
For any positive integer n, the product
n! ≡n(n −1)(n −2) . . . 3 · 2 · 1
(4.36)
is n-factorial, with zero-factorial deﬁned as unity
0! ≡1.
(4.37)
To estimate n!, one can use Stirling’s approximation
n! ≈
√
2π n (n/e)n
(4.38)
or Ramanujan’s correction to it
n! ≈
√
2π n (n/e)n 
1 + 1/2n + 1/8n21/6
(4.39)
or Mermin’s ﬁrst
n! ≈
√
2π n
n
e
n
exp
 1
12 n

(4.40)
or second approximation
n! ≈
√
2π n
n
e
n
exp
 1
12 n −
1
360 n3 +
1
1260 n5

,
(4.41)
which follow from his exact inﬁnite-product formula
n! =
√
2π n
n
e
n ∞

j=1

(1 + 1/j)j+1/2
e

.
(4.42)
Figure 4.1 plots the relative error of these estimates E(n!) of n!
108
E(n!) −n!
n!

(4.43)
magniﬁed by 108, except for Stirling’s formula (4.38), whose relative error is
off the chart. Mermin’s second approximation (4.40) is the most accurate, fol-
lowed by Ramanujan’s correction (4.39), and by Mermin’s ﬁrst approximation
(4.40) (James Stirling, 1692–1770; Srinivasa Ramanujan, 1887–1920; N. David
Mermin, 1935–).
The binomial coefﬁcient is a ratio of factorials
n
k

≡
n!
k! (n −k)!.
(4.44)
141

INFINITE SERIES
0
10
20
30
40
50
60
70
80
90
100
−5
−4
−3
−2
−1
0
1
2
3
4
5 x 10−8
Relative accuracy of these approximations to n!
√⎯⎯
2π n (n/e)n exp(1/12n − 1/360n3 + 1/1260n5)
√⎯⎯
2π n (n/e)n exp(1/12n)
√⎯⎯
2π n (n/e)n (1 + 1/2n + 1/8n2)1/6
n
108(E(n!) − n!)/n!
Figure 4.1
The magniﬁed relative error 108[E(n!)−n!]/n! of Ramanujan’s (4.39) and
Mermin’s (4.40 & 4.41) estimates E(n!) of n! are plotted for n = 1, 2, . . . , 100.
Example 4.4 (The Leibniz rule)
We can use the notation
f (n)(x) ≡dn
dxn f (x)
(4.45)
to state Leibniz’s rule for the derivatives of the product of two functions
dn
dxn [f (x) g(x)] =
n

k=0
n
k

f (k)(x) g(n−k)(x).
(4.46)
One may use mathematical induction to prove this rule, which is obviously true
for n = 0 and n = 1 (exercise 4.4).
Example 4.5 (The exponential function)
The power series with coefﬁcients cn =
1/n! deﬁnes the exponential function
ez =
∞

n=0
zn
n! .
(4.47)
142

4.5 FACTORIALS AND THE GAMMA FUNCTION
Formula (4.28) shows that the radius of convergence R of this power series is
inﬁnite
R =

lim
n→∞
|cn+1|
|cn|
−1
=

lim
n→∞
1
n + 1
−1
= ∞.
(4.48)
The series converges uniformly and absolutely inside every circle.
Example 4.6 (Bessel’s series)
For any integer n, the series
Jn(ρ) = ρn
2nn!

1 −
ρ2
2(2n + 2) +
ρ4
2 · 4(2n + 2)(2n + 4) −· · ·

=
ρ
2
n
∞

m=0
(−1)m
m!(m + n)!
ρ
2
2m
(4.49)
deﬁnes the cylindrical Bessel function of the ﬁrst kind, which is ﬁnite at the
origin ρ = 0. This series converges even faster (exercise 4.5) than the one (4.47)
for the exponential function.
Double factorials also are useful and are deﬁned as
(2n −1)!! ≡(2n −1)(2n −3)(2n −5) · · · 1,
(4.50)
(2n)!! ≡2n(2n −2)(2n −4) · · · 2
(4.51)
with 0!! and (−1)!! both deﬁned as unity
0!! = (−1)!! = 1.
(4.52)
Thus 5!! = 5 · 3 · 1 = 15, and 6!! = 6 · 4 · 2 = 48.
One may extend the deﬁnition (4.36) of n-factorial from positive integers to
complex numbers by means of the integral formula
z! ≡
 ∞
0
e−t tz dt
(4.53)
for Re z > −1. In particular
0! =
 ∞
0
e−t dt = 1,
(4.54)
which explains the deﬁnition (4.37). The factorial function (z −1)! in turn
deﬁnes the gamma function for Re z > 0 as
(z) =
 ∞
0
e−t tz−1 dt = (z −1)!
(4.55)
143

INFINITE SERIES
as may be seen from (4.53). By differentiating this formula and integrating it by
parts, we see that the gamma function satisﬁes the key identity
(z + 1) =
 ∞
0

−d
dte−t

tz dt =
 ∞
0
e−t
 d
dttz

dt =
 ∞
0
e−t z tz−1 dt
= z (z)
(4.56)
with (1) = 0! = 1. We may use this identity (4.56) to extend the deﬁnition
(5.102) of the gamma function in unit steps into the left half-plane
(z) = 1
z (z + 1) = 1
z
1
z + 1 (z + 2) = 1
z
1
z + 1
1
z + 2 (z + 3) = · · · (4.57)
as long as we avoid the negative integers and zero. This extension leads to
Euler’s deﬁnition
(z) = lim
n→∞
1 · 2 · 3 · · · n
z(z + 1)(z + 2) · · · (z + n) nz
(4.58)
and to Weierstrass’s (exercise 4.6)
(z) = 1
z e−γ z
 ∞

n=1

1 + z
n

e−z/n
−1
(4.59)
(Karl Theodor Wilhelm Weierstrass, 1815–1897), and is an example of analytic
continuation (section 5.12).
One may show (exercise 4.8) that another formula for (z) is
(z) = 2
 ∞
0
e−t2t2z−1 dt
(4.60)
for Re z > 0 and that
(n + 1
2) = (2n)!
n! 2n
√π.
(4.61)
Identity (4.56) and formula (4.61) imply (exercise 4.11) that

2n + 1
2

= (2n −1)!!
2n
√π.
(4.62)
Example 4.7 (Bessel function of nonintegral index)
We can use the gamma-
function formula (4.55) for n! to extend the deﬁnition (4.49) of the Bessel
function of the ﬁrst kind Jn(ρ) to nonintegral values ν of the index n. Replacing
n by ν and (m + n)! by (m + ν + 1), we get
Jν(ρ) =
ρ
2
ν
∞

m=0
(−1)m
m! (m + ν + 1)
ρ
2
2m
,
(4.63)
which makes sense even for complex values of ν.
144

4.6 TAYLOR SERIES
Example 4.8 (Spherical Bessel function)
The spherical Bessel function is
deﬁned as
jℓ(ρ) ≡
& π
2ρ Jℓ+1/2(ρ).
(4.64)
For small values of its argument |ρ| ≪1, the ﬁrst term in the series (4.63)
dominates and so (exercise 4.7)
jℓ(ρ) ≈
&π
2
ρ
2
ℓ
1
(ℓ+ 3/2) = ℓ! (2ρ)ℓ
(2ℓ+ 1)! =
ρℓ
(2ℓ+ 1)!!
(4.65)
as one may show by repeatedly using the key identity (z + 1) = z (z).
4.6 Taylor series
If the function f (x) is a real-valued function of a real variable x with a
continuous Nth derivative, then Taylor’s expansion for it is
f (x + a) = f (x) + af ′(x) + a2
2 f ′′(x) + · · · +
an−1
(n −1)!f (N−1) + EN
=
N−1

n=0
an
n! f (n)(x) + EN,
(4.66)
in which the error EN is
EN = aN
N! f (N)(x + y)
(4.67)
for some 0 ≤y ≤a.
For many functions f (x) the errors go to zero, EN →0, as N →∞; for these
functions, the inﬁnite Taylor series converges:
f (x + a) =
∞

n=0
an
n! f (n)(x) = exp

a d
dx

f (x) = eiap/¯h f (x),
(4.68)
in which
p = ¯h
i
d
dx
(4.69)
is the displacement operator or equivalently the momentum operator.
145

INFINITE SERIES
4.7 Fourier series as power series
The Fourier series (2.37)
f (x) =
∞

n=−∞
cn
ei2πnx/L
√
L
(4.70)
with coefﬁcients (2.45)
cn =
 L/2
−L/2
e−i2πnx/L
√
L
f (x) dx
(4.71)
is a pair of power series
f (x) =
1
√
L
 ∞

n=0
cn zn +
∞

n=1
c−n (z−1)n

(4.72)
in the variables
z = ei2πx/L
and
z−1 = e−i2πx/L.
(4.73)
Formula (4.28) tells us that the radii of convergence of these two power series
are given by
R−1
+ = lim
n→∞
|cn+1|
|cn|
and
R−1
−= lim
n→∞
|c−n−1|
|c−n| .
(4.74)
Thus the pair of power series (4.72) will converge uniformly and absolutely as
long as z satisﬁes the two inequalities
|z| < R+
and
1
|z| < R−.
(4.75)
Since |z| = 1, the Fourier series (4.70) converges if R−1
−< |1| < R+.
Example 4.9 (A uniform and absolutely convergent Fourier series)
The Fourier
series
f (x) =
∞

n=−∞
1
1 + |n|n
ei2πnx/L
√
L
(4.76)
converges uniformly and absolutely because R+ = R−= ∞.
146

4.8 THE BINOMIAL SERIES AND THEOREM
4.8 The binomial series and theorem
The Taylor series for the function f (x) = (1 + x)a is
(1 + x)a =
∞

n=0
xn
n!
dn
dxn (1 + x)a
x=0
= 1 + ax + 1
2a(a −1)x2 + · · ·
= 1 +
∞

n=1
a(a −1) · · · (a −n + 1)
n!
xn.
(4.77)
If a is a positive integer a = N, then the nth power of x in this series is multiplied
by a binomial coefﬁcient (4.44)
(1 + x)N =
N

n=0
N!
n!(N −n)! xn =
N

n=0
N
n

xn.
(4.78)
The series (4.77) and (4.78) respectively imply (exercise 4.13)
(x + y)a = ya +
∞

n=1
a(a −1) · · · (a −n + 1)
n!
xn ya−n
(4.79)
and
(x + y)N =
N

n=0
N
n

xn yN−n.
(4.80)
We can use these versions of the binomial theorem to compute approximately or
exactly.
Example 4.10
The phase difference φ between two highly relativistic neutri-
nos of momentum p going a distance L in a time t ≈L varies with their masses
m1 and m2 as
φ = t E = LE
p E = LE
p

p2 + m2
1 −

p2 + m2
2

(4.81)
in natural units. We can approximate this phase by using the ﬁrst two terms of
the binomial expansion (4.79) with y = 1 and x = m2
i /p2
φ = LE

1 + m2
1/p2 −

1 + m2
2/p2

≈LEm2
p2
≈Lm2
E
(4.82)
or in ordinary units φ ≈Lm2c3/(¯hE).
147

INFINITE SERIES
Example 4.11
We can use the binomial expansion (4.80) to compute
9993 =

103 −1
3
= 109 −3 × 106 + 3 × 103 −1 = 997002999
(4.83)
exactly.
When a is not a positive integer, the series (4.77) does not terminate. For
instance, the binomial series for
√
1 + x and 1/
√
1 + x are (exercise 4.14)
(1 + x)1/2 = 1 +
∞

n=1
1
2

1
2 −1

· · ·

1
2 −n + 1

n!
xn
= 1 +
∞

n=1
(−1)n−1
2n
(2n −3)!!
n!
xn
(4.84)
and
(1 + x)−1/2 = 1 +
∞

n=1
−1
2

−3
2

· · ·

−1
2 −n + 1

n!
xn
=
∞

n=0
(−1)n
2n
(2n −1)!!
n!
xn.
(4.85)
4.9 Logarithmic series
The Taylor series for the function f (x) = ln(1 + x) is
ln(1 + x) =
∞

n=0
xn
n!
dn
dxn ln(1 + x)|x=0
(4.86)
in which
f (0)(0) = ln(1 + x)|x=0 = 0,
f (1)(0) =
1
1 + x

x=0
= 1,
f (n)(0) = (−1)n−1 (n −1)!
(1 + x)n

x=0
= (−1)n−1 (n −1)!.
(4.87)
So the series for ln(1 + x) is
ln(1 + x) =
∞

n=1
(−1)n−1 xn
n
= x −1
2x2 + 1
3x3 ± · · · ,
(4.88)
148

4.10 DIRICHLET SERIES AND THE ZETA FUNCTION
which converges slowly for −1 < x ≤1. Letting x →−x, we see that
ln(1 −x) = −
∞

n=1
xn
n .
(4.89)
So the series for the logarithm of the ratio (1 + x)/(1 −x) is
ln
1 + x
1 −x

= 2
∞

n=0
x2n+1
2n + 1.
(4.90)
4.10 Dirichlet series and the zeta function
A Dirichlet series is one in which the nth term is proportional to 1/nz
f (z) =
∞

n=1
cn
nz .
(4.91)
An important example is the Riemann zeta function ζ(z)
ζ(z) =
∞

n=1
n−z,
(4.92)
which converges for Re z > 1.
Euler showed that for Re z > 1, the Riemann zeta function is the inﬁnite
product
ζ(z) =

p
1
1 −p−z
(4.93)
over all prime numbers p = 2, 3, 5, 7, 11, . . .. Some speciﬁc values are ζ(2) =
π2/6 ≈1.645, ζ(4) = π4/90 ≈1.0823, and ζ(6) = π6/945 ≈1.0173.
Example 4.12 (Planck’s distribution)
Max Planck (1858–1947) showed that the
electromagnetic energy in a closed cavity of volume V at a temperature T in the
frequency interval dν about ν is
dU(β, ν, V) = 8πhV
c3
ν3
eβhν −1 dν,
(4.94)
in which β = 1/(kT), k = 1.3806503 × 10−23 J/K is Boltzmann’s constant, and
h = 6.626068×10−34 Js is Planck’s constant. The total energy then is the integral
U(β, V) = 8πhV
c3
 ∞
0
ν3
eβhν −1 dν,
(4.95)
149

INFINITE SERIES
which we may do by letting x = βhν and using the geometric series (4.31)
U(β, V) = 8π(kT)4V
(hc)3
 ∞
0
x3
ex −1 dx
= 8π(kT)4V
(hc)3
 ∞
0
x3e−x
1 −e−x dx
= 8π(kT)4V
(hc)3
 ∞
0
x3e−x
∞

n=0
e−nx dx.
(4.96)
The geometric series is absolutely and uniformly convergent for x > 0, and we
may interchange the limits of summation and integration. After another change
of variables, the gamma-function formula (5.102) gives
U(β, V) = 8π(kT)4V
(hc)3
∞

n=0
 ∞
0
x3e−(n+1)x dx
= 8π(kT)4V
(hc)3
∞

n=0
1
(n + 1)4
 ∞
0
y3e−y dy
= 8π(kT)4V
(hc)3
3! ζ(4) = 8π5(kT)4V
15(hc)3
.
(4.97)
It follows that the power radiated by a “black body” is proportional to the fourth
power of its temperature and to its area A
P = σ A T4,
(4.98)
in which
σ = 2π5k4
15(hc)3 = 5.670400(40) × 10−8 W m−2 K−4
(4.99)
is Stefan’s constant.
The number of photons in the black-body distribution (4.94) at inverse
temperature β in the volume V is
N(β, V) = 8πV
c3
 ∞
0
ν2
eβhν −1 dν = 8πV
(cβh)3
 ∞
0
x2
ex −1 dx
= 8πV
(cβh)3
 ∞
0
x2e−x
1 −e−x dx = 8πV
(cβh)3
 ∞
0
x2e−x
∞

n=0
e−nx dx
= 8πV
(cβh)3
∞

n=0
 ∞
0
x2e−(n+1)x dx = 8πV
(cβh)3
∞

n=0
1
(n + 1)3
 ∞
0
y2e−y dy
= 8πV
(cβh)3 ζ(3)2! = 8π(kT)3V
(ch)3
ζ(3)2!.
(4.100)
150

4.11 BERNOULLI NUMBERS AND POLYNOMIALS
The mean energy ⟨E⟩of a photon in the black-body distribution (4.94) is the
energy U(β, V) divided by the number of photons N(β, V)
⟨E⟩= ⟨hν⟩= 3! ζ(4)
2! ζ(3) kT =
π4
30 ζ(3)
(4.101)
or ⟨E⟩≈2.70118 kT since Apéry’s constant ζ(3) is 1.2020569032 ...(Roger
Apéry, 1916–1994).
Example 4.13 (The Lerch transcendent)
The Lerch transcendent is the series
(z, s, α) =
∞

n=0
zn
(n + α)s .
(4.102)
It converges when |z| < 1 and Re s > 0 and Re α > 0.
4.11 Bernoulli numbers and polynomials
The Bernoulli numbers Bn are deﬁned by the inﬁnite series
x
ex −1 =
∞

n=0
xn
n!
# dn
dxn
x
ex −1
$
x=0
=
∞

n=0
Bn
xn
n!
(4.103)
for the generating function x/(ex −1). They are the successive derivatives
Bn = dn
dxn
x
ex −1

x=0
.
(4.104)
So B0 = 1 and B1 = −1/2. The remaining odd Bernoulli numbers vanish
B2n+1 = 0
for n > 0
(4.105)
and the remaining even ones are given by Euler’s zeta function (4.92) formula
B2n = (−1)n−12(2n)!
(2π)2n
ζ(2n)
for n > 0.
(4.106)
The Bernoulli numbers occur in the power series for many transcendental
functions, for instance
coth x = 1
x +
∞

k=1
22kB2k
(2k)! x2k−1
for x2 < π2.
(4.107)
Bernoulli’s polynomials Bn(y) are deﬁned by the series
xexy
ex −1 =
∞

n=0
Bn(y) xn
n!
(4.108)
for the generating function xexy/(ex −1).
151

INFINITE SERIES
Some authors (Whittaker and Watson, 1927, p. 125–127) deﬁne Bernoulli’s
numbers instead by
Bn = 2(2n)!
(2π)2n ζ(2n) = 4n
 ∞
0
t2n−1 dt
e2πt −1,
(4.109)
a result due to Carda.
4.12 Asymptotic series
A series
sn(x) =
n

k=0
ak
xk
(4.110)
is an asymptotic expansion for a real function f (x) if the remainder Rn
Rn(x) = f (x) −sn(x)
(4.111)
satisﬁes the condition
lim
x→∞xnRn(x) = 0
(4.112)
for ﬁxed n. In this case, one writes
f (x) ≈
n

k=0
ak
xk
(4.113)
where the wavy equal sign indicates equality in the sense of (4.112). Some
authors add the condition
lim
n→∞xnRn(x) = ∞
(4.114)
for ﬁxed x in order to exclude convergent series in powers of 1/x.
Example 4.14 (The asymptotic series for E1)
Let’s develop an asymptotic
expansion for the function
E1(x) =
 ∞
x
e−y dy
y ,
(4.115)
which is related to the exponential-integral function
Ei(x) =
 x
−∞
ey dy
y
(4.116)
by the tricky formula E1(x) = −Ei(−x). Since
e−y
y
= −d
dy
e−y
y

−e−y
y2
(4.117)
152

4.12 ASYMPTOTIC SERIES
we may integrate by parts, getting
E1(x) = e−x
x
−
 ∞
x
e−y dy
y2 .
(4.118)
Integrating by parts again, we ﬁnd
E1(x) = e−x
x
−e−x
x2 + 2
 ∞
x
e−y dy
y3 .
(4.119)
Eventually, we develop the series
E1(x) = e−x
0!
x −1!
x2 + 2!
x3 −3!
x4 + 4!
x5 −· · ·

(4.120)
with remainder
Rn(x) = (−1)n n!
 ∞
x
e−y
dy
y n+1 .
(4.121)
Setting y = u + x, we have
Rn(x) = (−1)n n! e−x
xn+1
 ∞
0
e−u
du

1 + u
x
n+1 ,
(4.122)
which satisﬁes the condition (4.112) that deﬁnes an asymptotic series
lim
x→∞xnRn(x) = lim
x→∞(−1)n n! e−x
x
 ∞
0
e−u
du

1 + u
x
n+1
= lim
x→∞(−1)n n! e−x
x
 ∞
0
e−u du
= lim
x→∞(−1)n n! e−x
x
= 0
(4.123)
for ﬁxed n.
Asymptotic series often occur in physics. In such physical problems, a small
parameter λ usually plays the role of 1/x. A perturbative series
Sn(λ) =
n

k=0
ak λk
(4.124)
is an asymptotic expansion of the physical quantity S(λ) if the remainder
Rn(λ) = S(λ) −Sn(λ)
(4.125)
satisﬁes for ﬁxed n
lim
λ→0 λ−n Rn(λ) = 0.
(4.126)
The WKB approximation and the Dyson series for quantum electrodynamics
are asymptotic expansions in this sense.
153

INFINITE SERIES
4.13 Some electrostatic problems
Gauss’s law ∇· D = ρ equates the divergence of the electric displacement D
to the density ρ of free charges (charges that are free to move in or out of
the dielectric medium – as opposed to those that are part of the medium and
bound to it by molecular forces). In electrostatic problems, Maxwell’s equa-
tions reduce to Gauss’s law and the static form ∇× E = 0 of Faraday’s law,
which implies that the electric ﬁeld E is the gradient of an electrostatic potential
E = −∇V.
Across an interface with normal vector ˆn between two dielectrics, the tangen-
tial electric ﬁeld is continuous while the normal electric displacement jumps by
the surface charge density σ
ˆn × (E2 −E1) = 0
and
σ = ˆn · (D2 −D1) .
(4.127)
In a linear dielectric, the electric displacement D is proportional to the electric
ﬁeld D = ϵm E, where the permittivity ϵm = ϵ0 +χm = Km ϵ0 of the material dif-
fers from that of the vacuum ϵ0 by the electric susceptibility χm and the relative
permittivity Km. The permittivity of the vacuum is the electric constant ϵ0.
An electric ﬁeld E exerts on a charge q a force F = qE even in a dielec-
tric medium. The electrostatic energy W of a system of linear dielectrics is the
volume integral
W = 1
2

D · E d3r.
(4.128)
Example 4.15 (Field of a charge near an interface)
Consider two semi-inﬁnite
dielectrics of permittivities ϵ1 and ϵ2 separated by an inﬁnite horizontal x-y-
plane. What is the electrostatic potential due to a charge q in region 1 at a height
h above the interface?
The easy way to solve this problem is to put an image charge q′ at the same
distance from the interface in region 2 so that the potential in region 1 is
V1(r) =
1
4πϵ1

q

x2 + y2 + (z −h)2 +
q′

x2 + y2 + (z + h)2

.
(4.129)
This potential satisﬁes Gauss’s law ∇· D = ρ in region 1. In region 2, the
potential
V2(r) =
1
4πϵ2
q′′

x2 + y2 + (z −h)2
(4.130)
also satisﬁes Gauss’s law. The continuity (4.127) of the tangential component of
E tells us that the partial derivatives of V1 and V2 in the x (or y) direction must
be the same at z = 0
154

4.13 SOME ELECTROSTATIC PROBLEMS
∂V1(x, y, 0)
∂x
= ∂V2(x, y, 0)
∂x
.
(4.131)
The discontinuity equation (4.127) for the electric displacement says that at the
interface at z = 0 with no surface charge
ϵ1
∂V1(x, y, 0)
∂z
= ϵ2
∂V2(x, y, 0)
∂z
.
(4.132)
These two equations (4.131 & 4.132) allow one to solve for q′ and q′′
q′ = ϵ1 −ϵ2
ϵ1 + ϵ2
q
and
q′′ =
2ϵ2
ϵ1 + ϵ2
q.
(4.133)
In the limit h →0, the potential in region 1 becomes
V1(r) =
1
4πϵ1
q

x2 + y2 + z2

1 + ϵ1 −ϵ2
ϵ1 + ϵ2

=
q
4π ¯ϵr,
(4.134)
in which ¯ϵr is the mean permittivity ¯ϵ = (ϵ1 + ϵ2)/2. Similarly, in region 2 the
potential is
V2(r) =
1
4πϵ2
q

x2 + y2 + z2
2ϵ2
ϵ1 + ϵ2
=
q
4π ¯ϵr
(4.135)
in the limit h →0.
Example 4.16 (A charge near a plasma membrane )
A eukaryotic cell (the kind
with a nucleus) is surrounded by a plasma membrane, which is a phospholipid
bilayer about 5 nm thick. Both sides of the plasma membrane are in contact
with salty water. The permittivity of the water is ϵw ≈80ϵ0 while that of the
membrane considered as a simple lipid slab is ϵℓ≈2ϵ0.
Now let’s think about the potential felt by an ion in the water outside a cell
but near its membrane, and let us for simplicity imagine the membrane to be
inﬁnitely thick so that we can use the simple formulas we’ve derived. The poten-
tial due to the ion, if its charge is q, is then given by equation (4.129) with ϵ1 = ϵw
and ϵ2 = ϵℓ. The image-charge term in V1(r) is the potential due to the polar-
ization of the membrane and the water by the ion. It is the potential felt by the
ion. Since the image charge by (4.133) is q′ ≈q, the potential the ion feels is
Vi(z) ≈q/8πewz. The force on the ion then is
F = −qV′
i (z) =
q2
8πewz.
(4.136)
It always is positive no matter what the sign of the charge is. A lipid slab in water
repels ions. Similarly, a charge in a lipid slab is attracted to the water outside
the slab.
155

INFINITE SERIES
Next imagine an electric dipole in water near a lipid slab. Now there are two
equal and opposite charges and two equal and opposite mirror charges. The net
effect is that the slab repels the dipole. So lipids repel water molecules; they are
said to be hydrophobic. This is one of the reasons why folding proteins move their
hydrophobic amino acids inside and their polar or hydrophilic ones outside.
With some effort, one may use the method of images to compute the electric
potential of a charge in or near a plasma membrane taken to be a lipid slab of
ﬁnite thickness.
The electric potential in the lipid bilayer Vℓ(ρ, z) of thickness t due to a charge
q in the extracellular environment at a height h above the bilayer is
Vℓ(ρ, z) =
q
4πϵwℓ
∞

n=0
(pp′)n

1

ρ2 + (z −2nt −h)2
−
p′

ρ2 + (z + 2(n + 1)t + h)2

,
(4.137)
in which p = (ϵw −ϵℓ)/(ϵw + ϵℓ), p′ = (ϵc −ϵℓ)/(ϵc + ϵℓ), and ϵwℓ= (ϵw + ϵℓ)/2.
That in the extracellular environment is
Vw(ρ, z) =
q
4πϵw

1
r +
p

ρ2 + (z + h)2
−ϵwϵℓ
ϵ2
wℓ
∞

n=1
pn−1p′n

ρ2 + (z + 2nt + h)2

,
(4.138)
in which r is the distance from the charge q. Finally, the potential in the cytosol
is
Vc(ρ, z) =
q ϵℓ
4πϵwℓϵℓc
∞

n=0
(pp′)n

ρ2 + (z −2nt −h)2
(4.139)
where ϵℓc = (ϵℓ+ ϵc)/2.
The ﬁrst 1000 terms of these three series (4.137–4.139) are plotted in Fig. 4.2
for the case of a positive charge q = |e| at (ρ, z) = (0, 0) (top curve), (0, 1)
(middle curve), (0, 2) (third curve), and (0, 6) nm (bottom curve). Although
the potential V(ρ, z) is continuous across the two interfaces, its normal deriva-
tive isn’t due to the different dielectric constants in the three media. Because
the potential is small and ﬂat in the cytosol (z < −5 nm), charges in the
extracellular environment (z > 0) are nearly decoupled from those in the
cytosol.
Real plasma membranes are phospholipid bilayers. The lipids avoid the water
and so are on the inside. The phosphate groups are dipoles (and phosphatidylser-
ine is negatively charged). So a real membrane is a 4 nm thick lipid layer bounded
on each side by dipole layers, each about 0.5 nm thick. The net effect is to weakly
attract ions that are within 0.5 nm of the membrane.
156

4.14 INFINITE PRODUCTS
−10
−8
−6
−4
−2
0
2
4
6
8
10
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
Height z (nm) above lipid bilayer
Electric potential V (ρ , z) (V)
Potential due to a charge near a lipid slab
Figure 4.2
The electric potential V(ρ, z) from (4.137–4.139) in volts for ρ = 1
nm as a function of the height z (nm) above (or below) a lipid slab for a unit charge
q = |e| at (ρ, z) = (0, 0) (top curve), (0, 1) (second curve), (0, 2) (third curve),
and (0, 6) nm (bottom curve). The lipid slab extends from z = 0 to z = −5 nm,
and the cytosol lies below z = −5 nm. The relative permittivities were taken to be
ϵw/ϵ0 = ϵc/ϵ0 = 80 and ϵℓ/ϵ0 = 2.
4.14 Inﬁnite products
Weierstrass’s deﬁnition (4.59) of the gamma function, Euler’s formula (4.93)
for the zeta function, and Mermin’s formula (4.42) for n! are useful inﬁnite
products. Other examples are the expansions of the trigonometric functions
(2.66 & 2.67)
sin z = z
∞

n=1

1 −
z2
π2n2

and
cos z =
∞

n=1

1 −
z2
π2(n −1/2)2

,
(4.140)
which imply those of the hyperbolic functions
sinh z = z
∞

n=1

1 +
z2
π2n2

and
cosh z =
∞

n=1

1 +
z2
π2(n −1/2)2

. (4.141)
157

INFINITE SERIES
Exercises
4.1
Test the following series for convergence:
(a)
∞

n=2
1
(ln n)2 ,
(b)
∞

n=1
n!
20n ,
(c)
∞

n=1
1
n(n + 2),
(d)
∞

n=2
1
n ln n.
In each case, say whether the series converges and how you found out.
4.2
Olber’s paradox: assume a static universe with a uniform density of stars. With
you at the origin, divide space into successive shells of thickness t, and assume
that the stars in each shell subtend the same solid angle ω (as follows from the
ﬁrst assumption). Take into account the occulting of distant stars by nearer
ones and show that the total solid angle subtended by all the stars would be
4π. The sky would be dazzlingly bright at night.
4.3
Use the geometric formula (4.30) to derive the trigonometric summation
formula
1
2 + cos α + cos 2α + · · · + cos nα = sin(n + 1
2)α
2 sin 1
2α
.
(4.142)
Hint: write cos nα as [exp(inα) + exp(−inα)]/2.
4.4
Show that
n −1
k

+
n −1
k −1

=
n
k

(4.143)
and then use mathematical induction to prove Leibniz’s rule (4.46).
4.5
(a) Find the radius of convergence of the series (4.49) for the Bessel function
Jn(ρ). (b) Show that this series converges even faster than the one (4.47) for
the exponential function.
4.6
Use the formula (4.8) for the Euler–Mascheroni constant to show that Euler’s
deﬁnition (4.58) of the gamma function implies Weierstrass’s (4.59).
4.7
Derive the approximation (4.65) for jℓ(ρ) for |ρ| ≪1.
4.8
Derive formula (4.60) for the gamma function from its deﬁnition (4.55).
4.9
Use formula (4.60) to compute (1/2).
4.10 Show that z! = (z + 1) diverges when z is a negative integer.
4.11 Derive formula (4.62) for ((2n + 1)/2).
4.12 Show that the area of the surface of the unit sphere in d dimensions is
Ad = 2πd/2/(d/2).
(4.144)
Hint: compute the integral of the gaussian exp(−x2) in d dimensions using
both rectangular and spherical coordinates. This formula (4.144) is used in
dimensional regularization (Weinberg, 1995, p. 477).
4.13 Derive (4.80) from (4.78) and (4.79) from (4.77).
4.14 Derive the expansions (4.84 & 4.85) for
√
1 + x and 1/
√
1 + x.
4.15 Find the radii of convergence of the series (4.84) and (4.85).
158

EXERCISES
4.16 Find the ﬁrst three Bernoulli polynomials Bn(y) by using their generating
function (4.108).
4.17 How are the two deﬁnitions (4.106) and (4.109) of the Bernoulli numbers
related?
4.18 Show that the Lerch transcendent (z, s, α) deﬁned by the series (4.102)
converges when |z| < 1 and Re s > 0 and Re α > 0.
4.19 Langevin’s classical formula for the electrical polarization of a gas or liquid
of molecules of electric dipole moment p is
P(x) = Np
cosh x
sinh x −1
x

(4.145)
where x = pE/(kT), E is the applied electric ﬁeld, and N is the number density
of the molecules per unit volume. (a) Expand P(x) for small x as an inﬁ-
nite power series involving the Bernoulli numbers. (b) What are the ﬁrst three
terms expressed in terms of familiar constants? (c) Find the saturation limit
of P(x) as x →∞.
4.20 Show that the energy of a charge q spread on the surface of a sphere of radius
a in an inﬁnite lipid of permittivity ϵℓis W = q2/8πϵℓa.
4.21 If the lipid of exercise 4.20 has ﬁnite thickness t and is surrounded on both
sides by water of permittivity ϵw, then the image charges lower the energy W
by (Parsegian, 1969)
W =
q2
4πϵℓt
∞

n=1
1
n
ϵℓ−ϵw
ϵℓ+ ϵw
n
.
(4.146)
Sum this series. Hint: read section 4.9 carefully.
4.22 Consider a stack of three dielectrics of inﬁnite extent in the x-y-plane sepa-
rated by the two inﬁnite x-y-planes z = t/2 and z = −t/2. Suppose the upper
region z > t/2 is a uniform linear dielectric of permittivity ϵ1, the central
region −t/2 < z < t/2 is a uniform linear dielectric of permittivity ϵ2, and
the lower region z < −t/2 is a uniform linear dielectric of permittivity ϵ3.
Suppose the lower inﬁnite x-y-plane z = −t/2 has a uniform surface charge
density −σ, while the upper plane z = t/2 has a uniform surface charge den-
sity σ. What is the energy per unit area of this system? What is the pressure
on the second dielectric? What is the capacitance of the stack?
159

5
Complex-variable theory
5.1 Analytic functions
A complex-valued function f (z) of a complex variable z is differentiable at z
with derivative f ′(z) if the limit
f ′(z) = lim
z′→z
f (z′) −f (z)
z′ −z
(5.1)
exists as z′ approaches z from any direction in the complex plane. The limit must
exist no matter how or from what direction z′ approaches z.
If the function f (z) is differentiable in a small disk around a point z0, then
f (z) is said to be analytic at z0 (and at all points inside the disk).
Example 5.1 (Polynomials)
If f (z) = zn for some integer n, then for tiny dz and
z′ = z + dz, the difference f (z′) −f (z) is
f (z′) −f (z) = (z + dz)n −zn ≈nzn−1 dz
(5.2)
and so the limit
lim
z′→z
f (z′) −f (z)
z′ −z
= lim
dz→0
nzn−1 dz
dz
= nzn−1
(5.3)
exists and is nzn−1 independently of how z′ approaches z. Thus the function zn is
analytic at z for all z with derivative
dzn
dz = nzn−1.
(5.4)
160

5.2 CAUCHY’S INTEGRAL THEOREM
A function that is analytic everywhere is entire. All polynomials
P(z) =
N

n=0
cn zn
(5.5)
are entire.
Example 5.2 (A function that’s not analytic)
To see what can go wrong when
a function is not analytic, consider the function f (x, y) = x2 + y2 = z¯z for
z = x + iy. If we compute its derivative at (x, y) = (1, 0) by setting x = 1 + ϵ and
y = 0, then the limit is
lim
ϵ→0
f (1 + ϵ, 0) −f (1, 0)
ϵ
= lim
ϵ→0
(1 + ϵ)2 −1
ϵ
= 2
(5.6)
while if we instead set x = 1 and y = ϵ, then the limit is
lim
ϵ→0
f (1, ϵ) −f (1, 0)
iϵ
= lim
ϵ→0
1 + ϵ2 −1
iϵ
= −i lim
ϵ→0 ϵ = 0.
(5.7)
So the derivative depends upon the direction through which z →1.
5.2 Cauchy’s integral theorem
If f (z) is analytic at z0, then near z0 and to ﬁrst order in z −z0
f (z) ≈f (z0) + f ′(z0) (z −z0).
(5.8)
Let’s compute the contour integral of f (z) along a small circle of radius ϵ and
center z0. The points on the contour are
z = z0 + ϵ eiθ
(5.9)
for θ ∈[0, 2π]. So dz = iϵ eiθdθ, and the contour integral is
)
f (z) dz =
 2π
0

f (z0) + f ′(z0) (z −z0)

iϵ eiθdθ.
(5.10)
Since z −z0 = ϵ eiθ, the contour integral breaks into two pieces
)
f (z) dz = f (z0)
 2π
0
iϵ eiθdθ + f ′(z0)
 2π
0
ϵ eiθ iϵ eiθdθ,
(5.11)
which vanish because the θ-integrals are zero. So the contour integral of the
analytic function f (z)
)
f (z) dz = 0
(5.12)
is zero around the tiny circle – at least to order ϵ2.
161

COMPLEX-VARIABLE THEORY
What about the contour integral of an analytic function f (z) around a tiny
square of size ϵ? Again we use the analyticity of f (z) at z = z0 to expand it as
f (z) ≈f (z0) + f ′(z0) (z −z0)
(5.13)
on the tiny square. The square contour consists of the four complex segments
dz1 = ϵ, dz2 = i ϵ, dz3 = −ϵ, and dz4 = −i ϵ. The centers zn of these segments are
displaced by z1 −z0 = −i ϵ/2, z2 −z0 = ϵ/2, z3 −z0 = i ϵ/2, and z4 −z0 = −ϵ/2
from z0. The integral of f (z) around the square
)
f (z) dz =
4

n=1
f (zn) dzn =
4

n=1

f (z0) + (zn −z0) f ′(z0)

dzn
(5.14)
splits into two pieces
)
f (z) dz = f (z0) I1 + f ′(z0) I2.
(5.15)
The four segments dzn form a path that goes around the square and ends where
it started, so the ﬁrst piece f (z0)I1 is zero
f (z0) I1 = f (z0) [ϵ + i ϵ + (−ϵ) + (−i ϵ)] = 0.
(5.16)
And so is the second one f ′(z0) I2
f ′(z0)I2 = f ′(z0) [(z1 −z0)dz1 + (z2 −z0)dz2 + (z3 −z0)dz3 + (z4 −z0)dz4]
= f ′(z0) [(−i ϵ/2) ϵ + (ϵ/2) i ϵ + (i ϵ/2) (−ϵ) + (−ϵ/2) (−i ϵ)]
= f ′(z0) (ϵ2/2) [−i + i −i + i] = 0.
(5.17)
So the contour integral of an analytic function f (z) around a tiny square of side
ϵ is zero to order ϵ2. Thus, the integral around such a square can be at most of
order ϵ3. This is very important. We’ll use it to prove Cauchy’s integral theorem.
Let’s consider a function f (z) that is analytic on a square of side L, as pictured
in Fig. 5.1. The contour integral of f (z) around the square can be expressed as
the sum of L2/ϵ2 contour integrals around tiny squares of side ϵ. All interior
integrals cancel, leaving the integral around the perimeter. Each contour inte-
gral around its tiny square is at most of order ϵ3. So the sum of the L2/ϵ2 tiny
contour integrals is at most (L2/ϵ2) ϵ3 = ϵ L2, which vanishes as ϵ →0. Thus
the contour integral of a function f (z) along the perimeter of a square of side
L vanishes if f (z) is analytic on the perimeter and inside the square. This is an
example of Cauchy’s integral theorem.
Suppose a function f (z) is analytic in a region R and that I is a contour
integral along a straight line within that region from z1 to z2. The contour inte-
gral of f (z) around any square inside the region R of analyticity is zero. So by
successively adding contour integrals around small squares to the straight-line
162

5.2 CAUCHY’S INTEGRAL THEOREM
>
<
∨
∧
∧
∨
>
>
<
<
The Cauchy integral theorem
Figure 5.1
The sum of two contour integrals around two adjacent tiny squares is equal
to the contour integral around the perimeter of the two tiny squares because the up
integral along the right side of the left square cancels (dots) the down integral along the
left side of the right square. A contour integral around a big L × L square is equal to
the sum of the contour integrals around the L2/ϵ2 tiny ϵ × ϵ squares that tile the big
square.
contour integral, one may deform the straight-line contour into an arbitrary
contour from z1 to z2 without changing its value.
So a contour integral from z1 to z2 of a function f (z) that is analytic in a
region R remains invariant as we continuously deform the contour C to C′ as
long as these contours and all the intermediate contours lie entirely within the
region R and have the same ﬁxed endpoints z1 and z2 as in Fig. 5.2
I =
 z2
z1C
f (z) dz =
 z2
z1C′ f (z) dz.
(5.18)
Thus a contour integral depends upon its endpoints and upon the function f (z)
but not upon the actual contour as long as the deformations of the contour do
not push it outside the region R in which f (z) is analytic.
If the endpoints z1 and z2 are the same, then the contour C is closed, and we
write the integral as
I =
) z1
z1C
f (z) dz ≡
)
C
f (z) dz
(5.19)
with a little circle to denote that the contour is a closed loop. The value of
that integral is independent of the contour as long as our deformations of the
163

COMPLEX-VARIABLE THEORY
∧
∧
∧
∧
region of analyticity
Four equal contour integrals
Figure 5.2
As long as the four contours are within the domain of analyticity of f (z)
and have the same endpoints, the four contour integrals of that function are all equal.
contour keep it within the domain of analyticity of the function and as long as
the contour starts and ends at z1 = z2. Now suppose that the function f (z) is
analytic along the contour and at all points within it. Then we can shrink the
contour, staying within the domain of analyticity of the function, until the area
enclosed is zero and the contour is of zero length – all this without changing
the value of the integral. But the value of the integral along such a null contour
of zero length is zero. Thus the value of the original contour integral also must
be zero
) z1
z1C
f (z) dz = 0.
(5.20)
And so we arrive at Cauchy’s integral theorem: The contour integral of a func-
tion f (z) around a closed contour C lying entirely within the domain R of
analyticity of the function vanishes
)
C
f (z) dz = 0
(5.21)
as long as the function f (z) is analytic at all points within the contour.
A region in the complex plane is simply connected if we can shrink every loop
in the region to a point while keeping the loop in the region. A slice of American
cheese is simply connected, but a slice of Swiss cheese is not. A dime is simply
connected, but a washer isn’t. The surface of a sphere is simply connected, but
the surface of a bagel isn’t.
With this deﬁnition, we can restate the integral theorem of Cauchy: The
contour integral of a function f (z) around a closed contour C vanishes
164

5.3 CAUCHY’S INTEGRAL FORMULA
)
C
f (z) dz = 0
(5.22)
if the contour lies within a simply connected domain of analyticity of the
function f (z) (Augustin-Louis Cauchy, 1789–1857).
If a region R is simply connected, then we may deform any contour C from
z1 to z2 in R into any other contour C′ from z1 to z2 in R while keeping the
moving contour in the region R. So another way of understanding the Cauchy
integral theorem is to ask, what is the value of the contour integral
IM =
 z1
z2C′ f (z) dz ?
(5.23)
This integral is the same as the integral along C from z1 to z2, except for the
sign of the dzs and the order in which the terms are added, and so
IM =
 z1
z2C′ f (z) dz = −
 z2
z1C
f (z) dz.
(5.24)
Now consider a closed contour running along the contour C from z1 to z2
and backwards along C′ from z2 to z1 all within a simply connected region R
of analyticity. Since IM = −I, the integral of f (z) along this closed contour
vanishes:
)
f (z) dz = I + IM = I −I = 0
(5.25)
and we have again derived Cauchy’s integral theorem.
Example 5.3 (Polynomials are entire functions)
Every polynomial
P(z) =
N

n=0
cnzn
(5.26)
is entire (everywhere analytic), and so its integral along any closed contour
)
P(z) dz = 0
(5.27)
must vanish.
5.3 Cauchy’s integral formula
Let f (z) be analytic in a simply connected region R and z0 a point inside this
region. We ﬁrst will integrate the function f (z)/(z−z0) along a tiny closed coun-
terclockwise contour around the point z0. The contour is a circle of radius ϵ
with center at z0 with points z = z0 + ϵ eiθ for 0 ≤θ ≤2π, and dz = iϵ eiθdθ.
Since z −z0 = ϵ eiθ, the contour integral in the limit ϵ →0 is
165

COMPLEX-VARIABLE THEORY
)
ϵ
f (z)
z −z0
dz =
 2π
0

f (z0) + f ′(z0) (z −z0)

z −z0
iϵ eiθdθ
=
 2π
0

f (z0) + f ′(z0) ϵ eiθ
ϵ eiθ
iϵ eiθdθ
=
 2π
0

f (z0) + f ′(z0) ϵ eiθ
idθ.
(5.28)
The θ-integral involving f ′(z0) vanishes, and so we have
f (z0) =
1
2πi
)
ϵ
f (z)
z −z0
dz,
(5.29)
which is a miniature version of Cauchy’s integral formula.
Now consider the counterclockwise contour C′ in Fig. 5.3, which is a big
counterclockwise circle, a small clockwise circle, and two parallel straight lines,
all within a simply connected region R in which f (z) is analytic. The function
f (z)/(z−z0) is analytic everywhere in R except at the point z0. We can withdraw
the contour C′ to the left of the point z0 and shrink it to a point without having
the contour C′ cross z0. During this process, the integral of f (z)/(z −z0) does
not change. Its ﬁnal value is zero. So its initial value also is zero
>
>
<
<
z0
C´
The contour C´ around z0
Figure 5.3
The full contour is the sum of a big counterclockwise contour and a small
clockwise contour, both around z0, and two straight lines that cancel.
166

5.3 CAUCHY’S INTEGRAL FORMULA
0 =
1
2πi
)
C′
f (z)
z −z0
dz.
(5.30)
We let the two straight-line segments approach each other so that they can-
cel. What remains of contour C′ is a big counterclockwise contour C around
z0 and a tiny clockwise circle of radius ϵ around z0. The tiny clockwise
circle integral is the negative of the counterclockwise integral (5.29), so we
have
0 =
1
2πi
)
C′
f (z)
z −z0
dz =
1
2πi
)
C
f (z)
z −z0
dz −
1
2πi
)
ϵ
f (z)
z −z0
dz.
(5.31)
Using the miniature result (5.29), we ﬁnd
f (z0) =
1
2πi
)
C
f (z)
z −z0
dz,
(5.32)
which is Cauchy’s integral formula.
We can use this formula to compute the ﬁrst derivative f ′(z) of f (z)
f ′(z) = f (z + dz) −f (z)
dz
=
1
2πi
1
dz
)
dz′ f (z′)

1
z′ −z −dz −
1
z′ −z

=
1
2πi
)
dz′
f (z′)
(z′ −z −dz)(z′ −z).
(5.33)
So in the limit dz →0, we get
f ′(z) =
1
2πi
)
dz′
f (z′)
(z′ −z)2 .
(5.34)
The second derivative f (2)(z) of f (z) then is
f (2)(z) =
2
2πi
)
dz′
f (z′)
(z′ −z)3 .
(5.35)
And its nth derivative f (n)(z) is
f (n)(z) = n!
2πi
)
dz′
f (z′)
(z′ −z)n+1 .
(5.36)
In these formulas, the contour runs counterclockwise about the point z and lies
within the simply connected domain R in which f (z) is analytic.
Thus a function f (z) that is analytic in a region R is inﬁnitely differentiable
there.
167

COMPLEX-VARIABLE THEORY
Example 5.4 (Schlaeﬂi’s formula for the Legendre polynomials)
Rodrigues
showed that the Legendre polynomial Pn(x) is the nth derivative
Pn(x) =
1
2n n!
 d
dx
n
(x2 −1)n.
(5.37)
Schlaeﬂi used this expression and Cauchy’s integral formula (5.36) to represent
Pn(z) as the contour integral (exercise 5.8)
Pn(z) =
1
2n 2πi
)
(z′2 −1)n
(z′ −z)n+1 dz′,
(5.38)
in which the contour encircles the complex point z counterclockwise. This
formula tells us that at z = 1 the Legendre polynomial is
Pn(1) =
1
2n 2πi
)
(z′2 −1)n
(z′ −1)n+1 dz′ =
1
2n 2πi
) (z′ + 1)n
(z′ −1) dz′ = 1,
(5.39)
in which we applied Cauchy’s integral formula (5.32) to f (z) = (z + 1)n.
Example 5.5 (Bessel functions of the ﬁrst kind)
The counterclockwise integral
around the unit circle z = eiθ of the ratio zm/zn in which both m and n are
integers is
1
2πi
)
dz zm
zn =
1
2πi
 2π
0
ieiθdθ ei(m−n)θ = 1
2π
 2π
0
dθ ei(m+1−n)θ.
(5.40)
If m + 1 −n ̸= 0, this integral vanishes because exp 2πi(m + 1 −n) = 1
1
2π
 2π
0
dθ ei(m+1−n)θ = 1
2π

ei(m+1−n)θ
i(m + 1 −n)
2π
0
= 0.
(5.41)
If m + 1 −n = 0, the exponential is unity, exp i(m + 1 −n)θ = 1, and the integral
is 2π/2π = 1. Thus the original integral is the Kronecker delta
1
2πi
)
dz zm
zn = δm+1, n.
(5.42)
The generating function (9.5) for Bessel functions Jm of the ﬁrst kind is
et(z−1/z)/2 =
∞

m=−∞
zmJm(t).
(5.43)
Applying our integral formula (5.42) to it, we ﬁnd
1
2πi
)
dz et(z−1/z)/2
1
zn+1 =
1
2πi
)
dz
∞

m=−∞
zm
zn+1 Jm(t)
=
∞

m=−∞
δm+1,n+1 Jm(t) = Jn(t).
(5.44)
168

5.4 THE CAUCHY–RIEMANN CONDITIONS
Thus, letting z = eiθ, we have
Jn(t) = 1
2π
 2π
0
dθ exp

t

eiθ −e−iθ
2
−inθ

(5.45)
or more simply
Jn(t) = 1
2π
 2π
0
dθ ei(t sin θ−nθ) = 1
π
 π
0
dθ cos(t sin θ −nθ)
(5.46)
(exercise 5.3).
5.4 The Cauchy–Riemann conditions
We can write any complex-valued function of two real variables x and y as
f = u + iv where u(x, y) and v(x, y) are real. If we use subscripts for partial
differentiation ux = ∂u/∂x, uy = ∂u/∂y, and so forth, then the change in f due
to small changes in x and y is df = (ux + ivx) dx + (uy + ivy) dy. But if f is a
function of z = x + iy, rather than just of x and y, and if f is analytic at z, then
the change in f due to small changes in x and y is
df = (ux + ivx) dx + (uy + ivy) dy = f ′(z) dz = f ′(z)(dx + idy).
(5.47)
Setting ﬁrst dy and then dx equal to zero, we get ﬁrst ux + ivx = f ′ and then
−iuy + vy = f ′, which give us the Cauchy–Riemann conditions
ux = vy
and
uy = −vx.
(5.48)
These conditions (5.48) hold because if f is analytic at z, then its derivative f ′
is independent of the direction from which dz →0. Thus the derivatives in the
x-direction and the iy-direction are the same
fx = ux + ivx = fy/i =

uy + ivy

/i = −iuy + vy,
(5.49)
which again gives us the Cauchy–Riemann conditions (5.48).
The directions in the x-y plane in which the real u and imaginary v parts of
an analytic function change most rapidly are the vectors (ux, uy) and (vx, vy).
The Cauchy–Riemann conditions (5.48) imply that these directions must be
perpendicular
(ux, uy) · (vx, vy) = uxvx + uyvy = vyvx −vxvy = 0.
(5.50)
The Cauchy–Riemann conditions (5.48) let us relate Cauchy’s integral theo-
rem (5.21) to Stokes’s theorem in the x-y plane. The real and imaginary parts
of a closed contour integral of a function f = u + iv
)
C
f (z) dz =
)
C
(u + iv)(dx + idy) =
)
C
u dx −v dy + i
)
C
v dx + u dy
(5.51)
169

COMPLEX-VARIABLE THEORY
are loop integrals of the functions a = (u, −v, 0) and b = (v, u, 0). By Stokes’s
theorem, these loop integrals are surface integrals of (∇× a)z and (∇× b)z over
the area enclosed by the contour C
)
C
udx −vdy =

C
a · (dx, dy, 0) =

S
(∇× a)z dxdy =

S
−vx −uy dxdy = 0,
)
C
vdx + udy =

C
b · (dx, dy, 0) =

S
(∇× b)z dxdy =

S
ux −vy dxdy = 0
which vanish by the Cauchy–Riemann conditions (5.48).
5.5 Harmonic functions
The Cauchy–Riemann conditions (5.48) tell us something about the Laplacian
of the real part u of an analytic function f = u+iv. First, the second x-derivative
uxx is uxx = vyx = vxy = −uyy. So the real part u of an analytic function f is a
harmonic function
uxx + uyy = 0
(5.52)
that is, one with a vanishing Laplacian. Similarly vxx = −uyx = −vyy, so the
imaginary part of an analytic function f also is a harmonic function
vxx + vyy = 0.
(5.53)
A harmonic function h(x, y) can have saddle points, but not local minima or
maxima because at a local minimum both hxx > 0 and hyy > 0, while at a local
maximum both hxx < 0 and hyy < 0. So in its domain of analyticity, the real
and imaginary parts of an analytic function f have neither minima nor maxima.
For static ﬁelds, the electrostatic potential φ(x, y, z) is a harmonic function of
the three spatial variables x, y, and z in regions that are free of charge because
the electric ﬁeld is E = −∇φ, and its divergence vanishes ∇· E = 0 where the
charge density is zero. Thus the Laplacian of the electrostatic potential φ(x, y, z)
vanishes
∇· ∇φ = φxx + φyy + φzz = 0
(5.54)
and φ(x, y, z) is harmonic where there is no charge. The location of each positive
charge is a local maximum of the electrostatic potential φ(x, y, z) and the loca-
tion of each negative charge is a local minimum of φ(x, y, z). But in the absence
of charges, the electrostatic potential has neither local maxima nor local min-
ima. Thus one can not trap charged particles with an electrostatic potential, a
result known as Earnshaw’s theorem.
We have seen (5.52 & 5.53) that the real and imaginary parts of an ana-
lytic function are harmonic functions with two-dimensional gradients that are
mutually perpendicular (5.50). And we know that the electrostatic potential is a
170

5.6 TAYLOR SERIES FOR ANALYTIC FUNCTIONS
harmonic function. Thus the real part u(x, y) (or the imaginary part v(x, y)) of
any analytic function f (z) = u(x, y) + iv(x, y) describes the electrostatic poten-
tial φ(x, y) for some electrostatic problem that does not involve the third spatial
coordinate z. The surfaces of constant u(x, y) are the equipotential surfaces, and
since the two gradients are orthogonal, the surfaces of constant v(x, y) are the
electric ﬁeld lines.
Example 5.6 (Two-dimensional potentials)
The function
f (z) = u + iv = E z = E x + i E y
(5.55)
can represent a potential V(x, y, z) = E x for which the electric-ﬁeld lines E =
−E ˆx are lines of constant y. It also can represent a potential V(x, y, z) = E y
in which E points in the negative y-direction, which is to say along lines of
constant x.
Another simple example is the function
f (z) = u + iv = z2 = x2 −y2 + 2ixy
(5.56)
for which u = x2 −y2 and v = 2xy. This function gives us a potential V(x, y, z)
whose equipotentials are the hyperbolas u = x2 −y2 = c2 and whose electric-
ﬁeld lines are the perpendicular hyperbolas v = 2xy = d2. Equivalently, we may
take these last hyperbolas 2xy = d2 to be the equipotentials and the other ones
x2 −y2 = c2 to be the lines of the electric ﬁeld.
For a third example, we write the variable z as z = reiθ = exp(ln r + iθ) and
use the function
f (z) = u(x, y) + iv(x, y) = −
λ
2πϵ0
ln z = −
λ
2πϵ0
(ln r + iθ) ,
(5.57)
which describes the potential V(x, y, z) = −(λ/2πϵ0) ln

x2 + y2 due to a line of
charge per unit length λ = q/L. The electric-ﬁeld lines are the lines of constant v
E =
λ
2πϵ0
(x, y, 0)
x2 + y2
(5.58)
or equivalently of constant θ.
5.6 Taylor series for analytic functions
Let’s consider the contour integral of the function f (z′)/(z′ −z) along a circle
C inside a simply connected region R in which f (z) is analytic. For any point z
inside the circle, Cauchy’s integral formula (5.32) tells us that
f (z) =
1
2πi
)
C
f (z′)
z′ −z dz′.
(5.59)
171

COMPLEX-VARIABLE THEORY
We add and subtract the center z0 from the denominator z′ −z
f (z) =
1
2πi
)
C
f (z′)
z′ −z0 −(z −z0) dz′
(5.60)
and then factor the denominator
f (z) =
1
2πi
)
C
f (z′)
(z′ −z0)

1 −z−z0
z′−z0
 dz′.
(5.61)
From Fig. 5.4, we see that the modulus of the ratio (z −z0)/(z′ −z0) is less than
unity, and so the power series

1 −z −z0
z′ −z0
−1
=
∞

n=0
 z −z0
z′ −z0
n
(5.62)
by (4.25–4.28) converges absolutely and uniformly on the circle. We therefore
are allowed to integrate the series
f (z) =
1
2πi
)
C
f (z′)
z′ −z0
∞

n=0
 z −z0
z′ −z0
n
dz′
(5.63)
term by term
f (z) =
∞

n=0
(z −z0)n
1
2πi
)
C
f (z′) dz′
(z′ −z0)n+1 .
(5.64)
>
Taylor-series contour around z0
z0
z
z
Figure 5.4
Contour of integral for the Taylor series (5.64).
172

5.8 LIOUVILLE’S THEOREM
By equation (5.36), the integral is just the nth derivative f (n)(z) divided by
n-factorial. Thus the function f (z) possesses the Taylor series
f (z) =
∞

n=0
(z −z0)n
n!
f (n)(z0),
(5.65)
which converges as long as the point z is inside a circle centered at z0 that lies
within a simply connected region R in which f (z) is analytic.
5.7 Cauchy’s inequality
Suppose a function f (z) is analytic in a region that includes the disk |z| ≤R and
that f (z) is bounded by |f (z)| ≤M on the circle z = R eiθ that is the perimeter
of the disk. Then by using Cauchy’s integral formula (5.36), we may bound the
nth derivative f (n)(0) of f (z) at z = 0 by
|f (n)(0)| ≤n!
2π
) |f (z)||dz|
|z|n+1
≤n!M
2π
 2π
0
R dθ
Rn+1 = n!M
Rn
(5.66)
which is Cauchy’s inequality.
5.8 Liouville’s theorem
Suppose now that f (z) is analytic everywhere (entire) and bounded by
|f (z)| ≤M
for all
|z| ≥R0.
(5.67)
Then by applying Cauchy’s inequality (5.66) at successively larger values of R,
we have
|f (n)(0)| ≤lim
R→∞
n!M
Rn = 0
for
n ≥1,
(5.68)
which shows that every derivative of f (z) vanishes
f (n)(0) = 0
for
n ≥1
(5.69)
at z = 0. But then the Taylor series (4.66) about z = 0 for the function f (z)
consists of only a single term, and f (z) is a constant
f (z) =
∞

n=0
zn
n! f (n)(0) = f (0)(0) = f (0).
(5.70)
So every bounded entire function is a constant, which is Liouville’s theorem.
173

COMPLEX-VARIABLE THEORY
5.9 The fundamental theorem of algebra
Gauss applied Liouville’s theorem to the function
f (z) =
1
PN(z) =
1
c0 + c1z + c2z2 + · · · + cNzN
(5.71)
which is the inverse of an arbitrary polynomial of order N. Suppose that the
polynomial PN(z) had no zero, that is, no root anywhere in the complex plane.
Then f (z) would be analytic everywhere. Moreover, for sufﬁciently large |z|,
the polynomial PN(z) is approximately PN(z) ≈cNzN, and so f (z) would be
bounded by something like
|f (z)| ≤
1
|cN|RN
0
≡M
for all
|z| ≥R0.
(5.72)
So if PN(z) had no root, then the function f (z) would be a bounded entire func-
tion and so would be a constant by Liouville’s theorem (5.70). But of course,
f (z) = 1/PN(z) is not a constant unless N = 0. Thus any polynomial PN(z) that
is not a constant must have a root, a pole of f (z), so that f (z) is not entire. This
is the only exit from the contradiction.
If the root of PN(z) is at z = z1, then PN(z) = (z −z1) PN−1(z), in which
PN−1(z) is a polynomial of order N −1, and we may repeat the argument for
its reciprocal f1(z) = 1/PN−1(z). In this way, one arrives at the fundamental
theorem of algebra: Every polynomial PN(z) = c0 + c1z + · · · + cNzN has N
roots somewhere in the complex plane
PN(z) = cN (z −z1)(z −z2) · · · (z −zN).
(5.73)
5.10 Laurent series
Consider a function f (z) that is analytic in a region that contains an outer circle
C1 of radius R1, an inner circle C2 of radius R2, and the annulus between the two
circles as in Fig. 5.5. We will integrate f (z) along a contour C12 that encircles
the point z in a counterclockwise fashion by following C1 counterclockwise and
C2 clockwise and a line joining them in both directions. By Cauchy’s integral
formula (5.32), this contour integral yields f (z)
f (z) =
1
2πi
)
C12
f (z′)
z′ −z dz′.
(5.74)
The integrations in opposite directions along the line joining C1 and C2 cancel,
and we are left with a counterclockwise integral around the outer circle C1 and
174

5.10 LAURENT SERIES
>
><
<
z0
z
C1
C2
Two contours around z0
z
z
Figure 5.5
The contour consisting of two concentric circles with center at z0 encircles
the point z in a counterclockwise sense. The asterisks are poles or other singularities of
the function f (z).
a clockwise one around C2 or minus a counterclockwise integral around C2
f (z) =
1
2πi
)
C1
f (z′)
z′ −z dz′ −
1
2πi
)
C2
f (z′′)
z′′ −z dz′′.
(5.75)
Now from the ﬁgure (5.5), the center z0 of the two concentric circles is closer
to the points z′′ on the inner circle C2 than it is to z and also closer to z than to
the points z′ on C1

z′′ −z0
z −z0
 < 1
and

z −z0
z′ −z0
 < 1.
(5.76)
To use these inequalities, as we did in the series (5.62), we add and subtract z0
from each of the denominators and absorb the minus sign before the second
integral into its denominator
f (z) =
1
2πi
)
C1
f (z′)
z′ −z0 −(z −z0) dz′ +
1
2πi
)
C2
f (z′′)
z −z0 −(z′′ −z0) dz′′. (5.77)
After factoring the two denominators
f (z) =
1
2πi
)
C1
f (z′)
(z′ −z0) [1 −(z −z0)/(z′ −z0)] dz′
+
1
2πi
)
C2
f (z′′)
(z −z0) [1 −(z′′ −z0)/(z −z0)] dz′′
(5.78)
175

COMPLEX-VARIABLE THEORY
we expand them, as in the series (5.62), in power series that converge absolutely
and uniformly on the two contours
f (z) =
∞

n=0
(z −z0)n
1
2πi
)
C1
f (z′)
(z′ −z0)n+1 dz′
+
∞

m=0
1
(z −z0)m+1
1
2πi
)
C2
(z′′ −z0)m f (z′′) dz′′.
(5.79)
Having removed the point z from the two integrals, we now apply cosmetics.
Since the functions being integrated are analytic between the two circles, we may
shift them to a common counterclockwise (ccw) contour C about any circle of
radius R2 ≤R ≤R1 between the two circles C1 and C2. Then we set m = −n−1,
or n = −m−1, so as to combine the two sums into one sum on n from −∞to ∞
f (z) =
∞

n=−∞
(z −z0)n
1
2πi
)
C
f (z′)
(z′ −z0)n+1 dz′.
(5.80)
This Laurent series often is written as
f (z) =
∞

n=−∞
an(z0) (z −z0)n
(5.81)
with
an(z0) =
1
2πi
)
C
f (z) dz
(z −z0)n+1
(5.82)
(Pierre Laurent, 1813–1854). The coefﬁcient a−1(z0) is called the residue of the
function f (z) at z0. Its signiﬁcance will be discussed in section 5.13.
Most functions have Laurent series that start at some least integer L
f (z) =
∞

n=L
an(z0) (z −z0)n
(5.83)
rather than at −∞. For such functions, we can pick off the coefﬁcients an one
by one without doing the integrals (5.82). The ﬁrst one aL is the limit
aL(z0) = lim
z→z0(z −z0)−Lf (z).
(5.84)
The second is given by
aL+1(z0) = lim
z→z0(z −z0)−L−1 
f (z) −(z −z0)LaL(z0)

.
(5.85)
The third requires two subtractions, and so forth.
176

5.11 SINGULARITIES
5.11 Singularities
A function f (z) that is analytic for all z is called entire or holomorphic. Entire
functions have no singularities, except possibly as |z| →∞, which is called the
point at inﬁnity.
A function f (z) has an isolated singularity at z0 if it is analytic in a small disk
about z0 but not analytic that point.
A function f (z) has a pole of order n > 0 at a point z0 if (z −z0)n f (z) is
analytic at z0 but (z −z0)n−1 f (z) has an isolated singularity at z0. A pole of
order n = 1 is called a simple pole. Poles are isolated singularities. A function is
meromorphic if it is analytic for all z except for poles.
Example 5.7 (Poles)
The function
f (z) =
n

j=1
1
(z −j)j
(5.86)
has a pole of order j at z = j for j = 1, 2, . .., n. It is meromorphic.
An essential singularity is a pole of inﬁnite order. If a function f (z) has an
essential singularity at z0, then its Laurent series (5.80) really runs from n =
−∞and not from n = L as in (5.83). Essential singularities are spooky: if a
function f (z) has an essential singularity at w, then inside every disk around
w, f (z) takes on every complex number, with at most one exception, an inﬁnite
number of times – a result due to Picard (1856–1941).
Example 5.8 (An essential singularity)
The function f (z) = exp(1/z) has an
essential singularity at z = 0 because its Laurent series (5.80)
f (z) = e1/z =
∞

m=0
1
m!
1
zm =
0

n=−∞
1
|n|! zn
(5.87)
runs from n = −∞. Near z = 0, f (z) = exp(1/z) takes on every complex number
except 0 an inﬁnite number of times.
Example 5.9 (A meromorphic function with two poles)
The function f (z) =
1/z(z + 1) has poles at z = 0 and at z = −1 but otherwise is analytic; it is
meromorphic. We may expand it in a Laurent series (5.81–5.82)
f (z) =
1
z(z + 1) =
∞

n=−∞
anzn
(5.88)
177

COMPLEX-VARIABLE THEORY
about z = 0 for |z| < 1. The coefﬁcient an is the integral
an =
1
2πi
)
C
dz
z n+2 (z + 1),
(5.89)
in which the contour C is a counterclockwise circle of radius r < 1. Since |z| < 1,
we may expand 1/(1 + z) as the series
1
1 + z =
∞

m=0
(−z)m.
(5.90)
Doing the integrals, we ﬁnd
an =
∞

m=0
1
2πi
)
C
(−z)m dz
zn+2 =
∞

m=0
(−1)m rm−n−1 δm,n+1
(5.91)
for n ≥−1 and zero otherwise. So the Laurent series for f (z) is
f (z) =
1
z(z + 1) =
∞

n=−1
(−1)n+1 zn.
(5.92)
The series starts at n = −1, not at n = −∞, because f (z) is meromorphic with
only a simple pole at z = 0.
Example 5.10 (The argument principle)
Consider the counterclockwise integral
1
2πi
)
C
f (z) g′(z)
g(z) dz
(5.93)
along a contour C that lies inside a simply connected region R in which f (z) is
analytic and g(z) meromorphic. If the function g(z) has a zero or a pole of order
n at w ∈R and no other singularity in R,
g(z) = an(w)(z −w)n
(5.94)
then the ratio g′/g is
g′(z)
g(z) = n(z −w)n−1
(z −w)n
=
n
z −w
(5.95)
and the integral is
1
2πi
)
C
f (z) g′(z)
g(z) dz =
1
2πi
)
C
f (z)
n
z −w dz = n f (w).
(5.96)
Any function g(z) meromorphic in R will possess a Laurent series
g(z) =
∞

k=n
ak(w)(z −w)k
(5.97)
about each point w ∈R. One may show (exercise 5.18) that as z →w the ratio
g′/g again approaches (5.95). It follows that the integral (5.93) is a sum of f (wℓ)
at the zeros of g(z) minus a similar sum at the poles of g(z)
178

5.12 ANALYTIC CONTINUATION
1
2πi
)
C
f (z) g′(z)
g(z) dz =

ℓ
1
2πi
)
C
f (z)
nℓ
z −wℓ
=

ℓ
nℓf (wℓ)
(5.98)
in which |nℓ| is the multiplicity of the ℓth zero or pole.
5.12 Analytic continuation
We saw in section 5.6 that a function f (z) that is analytic within a circle of radius
R about a point z0 possesses a Taylor series (5.65)
f (z) =
∞

n=0
(z −z0)n
n!
f (n)(z0)
(5.99)
that converges for all z inside the disk |z −z0| < R. Suppose z′ is the singularity
of f (z) that is closest to z0. Pick a point z1 in the disk |z −z0| < R that is not on
the line from z0 to the nearest singularity z′. The function f (z) is analytic at z1
because z1 is within the circle of radius R about the point z0, and so f (z) has a
Taylor series expansion like (5.99) but about the point z1. Usually the circle of
convergence of this power series about z1 will extend beyond the original disk
|z −z0| < R. If so, the two power series, one about z0 and the other about z1,
deﬁne the function f (z) and extend its domain of analyticity beyond the original
disk |z−z0| < R. Such an extension of the range of an analytic function is called
analytic continuation.
We often can analytically continue a function more easily than by the
successive construction of Taylor series.
Example 5.11 (The geometric series)
The power series
f (z) =
∞

n=0
zn
(5.100)
converges and deﬁnes an analytic function for |z| < 1. But for such z, we may
sum the series to
f (z) =
1
1 −z.
(5.101)
By summing the series (5.100), we have analytically continued the function f (z)
to the whole complex plane apart from its simple pole at z = 1.
Example 5.12 (The gamma function)
Euler’s form of the gamma function is
the integral
(z) =
 ∞
0
e−t tz−1 dt = (z −1)!,
(5.102)
179

COMPLEX-VARIABLE THEORY
which makes (z) analytic in the right half-plane Re z > 0. But by successively
using the relation (z + 1) = z (z), we may extend (z) into the left half-plane
(z) = 1
z (z + 1) = 1
z
1
z + 1 (z + 2) = 1
z
1
z + 1
1
z + 2 (z + 3).
(5.103)
The last expression deﬁnes (z) as a function that is analytic for Re z > −3 apart
from simple poles at z = 0, −1, and −2. Proceeding in this way, we may analyt-
ically continue the gamma function to the whole complex plane apart from the
negative integers and zero. The analytically continued gamma function may be
represented by the formula
(z) = 1
z e−γ z
 ∞

n=1

1 + z
n

e−z/n
−1
(5.104)
due to Weierstrass.
Example 5.13 (Dimensional regularization)
The loop diagrams of quantum
ﬁeld theory involve badly divergent integrals like
I(4) =

d4q
(2π)4

q2a

q2 + α2b
(5.105)
where often a = 0, b = 2, and α2 > 0. Gerardus ’t Hooft (1946–) and Martinus
J. G. Veltman (1931–) promoted the number of space-time dimensions from four
to a complex number d. The resulting integral has the value (Srednicki, 2007,
p. 102)
I(d) =

ddq
(2π)d

q2a

q2 + α2b = (b −a −d/2) (a + d/2)
(4π)d/2 (b) (d/2)
1
(α2)b−a−d/2
(5.106)
and so deﬁnes a function of the complex variable d that is analytic everywhere
except for simple poles at d = 2(n −a + b) where n = 0, 1, 2, . . . , ∞. At these
poles, the formula
(−n + z) = (−1)n
n!

1
z −γ +
n

k=1
1
k + O(z)

(5.107)
where γ = 0.5772... is the Euler–Mascheroni constant (4.8) can be useful.
5.13 The calculus of residues
A contour integral of an analytic function f (z) does not change unless the
endpoints move or the contour crosses a singularity or leaves the region of
analyticity (section 5.2). Let us consider the integral of a function f (z) along
a counterclockwise contour C that encircles n poles at zk for k = 1, . . . , n in a
simply connected region R in which f (z) is meromorphic. We may shrink the
180

5.13 THE CALCULUS OF RESIDUES
area within the contour C without changing the value of the integral until the
area is inﬁnitesimal and the contour is the sum of n tiny counterclockwise circles
Ck around the n poles
)
C
f (z) dz =
n

k=1
)
Ck
f (z) dz.
(5.108)
These tiny counterclockwise integrals around the poles at zi are the residues
a−1(zi) deﬁned by (5.82) for n = −1 apart from the factor 2πi. So the whole
ccw integral is 2πi times the sum of the residues of the function f (z) at the
enclosed poles
)
C
f (z) dz = 2πi
n

k=1
a−1(zk),
(5.109)
a result known as the residue theorem.
In general, one must do each tiny ccw integral about each pole zi, but simple
poles are an important special case. If w is a simple pole of the function f (z),
then near it f (z) is given by its Laurent series (5.81) as
f (z) = a−1(w)
z −w +
∞

n=0
an(w) (z −w)n.
(5.110)
In this case, its residue is by (5.84) with L = −1
a−1(w) = lim
z→w (z −w) f (z),
(5.111)
which usually is easier to do than the integral (5.82)
a−1(w) =
1
2πi
)
C
f (z)dz.
(5.112)
Example 5.14 (Cauchy’s integral formula)
Suppose the function f (z) is analytic
within a region R and that C is a ccw contour that encircles a point w ∈R. Then
the ccw contour C encircles the simple pole at w of the function f (z)/(z −w),
which is its only singularity in R. By applying the residue theorem and formula
(5.111) for the residue a−1(w) of the function f (z)/(z −w), we ﬁnd
)
C
f (z)
z −w dz = 2πi a−1(w) = 2πi lim
z→w (z −w) f (z)
z −w = 2πi f (w).
(5.113)
So Cauchy’s integral formula (5.32) is an example of the calculus of residues.
Example 5.15 (A meromorphic function)
By the residue theorem (5.109), the
integral of the function
f (z) =
1
z −1
1
(z −2)2
(5.114)
181

COMPLEX-VARIABLE THEORY
along the circle C = 4eiθ for 0 ≤θ ≤2π is the sum of the residues at z = 1
and z = 2
)
C
f (z) dz = 2πi [a−1(1) + a−1(2)] .
(5.115)
The function f (z) has a simple pole at z = 1, and so we may use the formula
(5.111) to evaluate the residue a−1(1) as
a−1(1) = lim
z→1 (z −1) f (z) = lim
z→1
1
(z −2)2 = 1
(5.116)
instead of using Cauchy’s integral formula (5.32) to do the integral of f (z) along
a tiny circle about z = 1, which gives the same result
a−1(1) =
1
2πi
)
dz
z −1
1
(z −2)2 =
1
(1 −2)2 = 1.
(5.117)
The residue a−1(2) is the integral of f (z) along a tiny circle about z = 2, which
we do by using Cauchy’s integral formula (5.34)
a−1(2) =
1
2πi
)
dz
(z −2)2
1
z −1 = d
dz
1
z −1

z=2
= −
1
(2 −1)2 = −1.
(5.118)
The sum of these two residues is zero, and so the integral (5.115) vanishes.
Another way of evaluating this integral is to deform it, not into two tiny circles
about the two poles, but rather into a huge circle z = Reiθ and to notice that as
R →∞the modulus of this integral vanishes

)
f (z) dz
 ≈2π
R2 →0.
(5.119)
This contour is an example of a ghost contour.
5.14 Ghost contours
Often one needs to do an integral that is not a closed counterclockwise con-
tour. Integrals along the real axis occur frequently. One sometimes can convert
a line integral into a closed contour by adding a contour along which the inte-
gral vanishes, a ghost contour. We have just seen an example (5.119) of a ghost
contour, and we shall see more of them in what follows.
Example 5.16 (Best case)
Consider the integral
I =
 ∞
−∞
1
(x −i)(x −2i)(x −3i) dx.
(5.120)
We could do the integral by adding a contour Reiθ from θ = 0 to θ = π. In
the limit R →∞, the integral of 1/[(z −i)(z −2i)(z −3i)] along this contour
vanishes; it is a ghost contour. The original integral I and the ghost contour
182

5.14 GHOST CONTOURS
encircle the three poles, and so we could compute I by evaluating the residues at
those poles. But we also could add a ghost contour around the lower half-plane.
This contour and the real line encircle no poles. So we get I = 0 without doing
any work at all.
Example 5.17 (Fourier transform of a gaussian)
During our computation of
the Fourier transform of a gaussian (3.15–3.18), we promised to justify the shift
in the variable of integration from x to x + ik/2m2 in this chapter. So let us
consider the contour integral of the entire function f (z) = exp(−m2z2) over the
rectangular closed contour along the real axis from −R to R and then from
z = R to z = R + ic and then from there to z = −R + ic and then to z = −R.
Since the f (z) is analytic within the contour, the integral is zero
)
dze−m2z2 =
 R
−R
e−m2z2dz +
 R+ic
R
e−m2z2dz +
 −R+ic
R+ic
e−m2z2dz +
 −R
−R+ic
e−m2z2dz = 0
for all ﬁnite positive values of R and so also in the limit R →∞. The two
contours in the imaginary direction are of length c and are damped by the factor
exp(−m2R2), and so they vanish in the limit R →∞. They are ghost contours.
It follows then from this last equation in the limit R →∞that
 ∞
−∞
dx e−m2(x+ic)2 =
 ∞
−∞
dx e−m2x2 =
√π
m ,
(5.121)
which is the promised result.
It implies (exercise 5.20) that
 ∞
−∞
dx e−m2(x+z)2 =
 ∞
−∞
dx e−m2x2 =
√π
m
(5.122)
for m > 0 and arbitrary complex z.
Example 5.18 (A cosine integral)
To compute the integral
I =
 ∞
0
cos x
q2 + x2 dx
(5.123)
we use the evenness of the integrand to extend the integration
I = 1
2
 ∞
−∞
cos x
q2 + x2 dx,
(5.124)
write the cosine as [exp(ix) + exp(−ix)]/2, and factor the denominator
I = 1
4
 ∞
−∞
eix
(x −iq)(x + iq) dx + 1
4
 ∞
−∞
e−ix
(x −iq)(x + iq) dx.
(5.125)
We promote x to a complex variable z and add the contours z = Reiθ and z =
Re−iθ as θ goes from 0 to π respectively to the ﬁrst and second integrals. The
term exp(iz)dz/(q2+z2) = exp(iR cos θ−R sin θ)iReiθdθ/(q2+R2e2iθ) vanishes in
183

COMPLEX-VARIABLE THEORY
the limit R →∞, so the ﬁrst contour is a ccw ghost contour. A similar argument
applies to the second (clockwise) contour, and we have
I = 1
4
)
eiz
(z −iq)(z + iq) dz + 1
4
)
e−iz
(z −iq)(z + iq) dz.
(5.126)
The ﬁrst integral picks up the pole at iq and the second the pole at −iq
I = iπ
2
e−q
2iq + e−q
2iq

= πe−q
2q .
(5.127)
So the value of the integral is πe−q/2q.
Example 5.19 (Third-harmonic microscopy)
An ultra-short laser pulse
intensely focused in a medium generates a third-harmonic electric ﬁeld E3 in
the forward direction proportional to the integral (Boyd, 2000)
E3 ∝χ(3) E3
0
 ∞
−∞
ei k z
dz
(1 + 2iz/b)2
(5.128)
along the axis of the beam as in Fig. 5.6. Here b = 2πt2
0n/λ = kt2
0 in which
n = n(ω) is the index of refraction of the medium, λ is the wave-length of the
laser light in the medium, and t0 is the transverse or waist radius of the gaussian
beam, deﬁned by E(r) = E exp(−r2/t2
0).
When the dispersion is normal, that is when dn(ω)/dω > 0, the shift in the
wave vector k = 3ω[n(ω) −n(3ω)]/c is negative. Since k < 0, the exponential
is damped when z = x + iy is in the lower half-plane (LHP)
ei k z = ei k (x+iy) = ei k x e−k y.
(5.129)
So as we did in example 5.18, we will add a contour around the lower half-plane
(z = R eiθ, π ≤θ ≤2π, and dz = iReiθdθ) because in the limit R →∞, the
integral along it vanishes; it is a ghost contour.
The function f (z) = exp(i k z)/(1 + 2iz/b)2 has a double pole at z = ib/2,
which is in the UHP since the length b > 0, but no singularity in the LHP y < 0.
So the integral of f (z) along the closed contour from z = −R to z = R and then
along the ghost contour vanishes. But since the integral along the ghost contour
vanishes, so does the integral from −R to R. Thus when the dispersion is normal,
the third-harmonic signal vanishes, E3 = 0, as long as the medium effectively
extends from −∞to ∞so that its edges are in the unfocused region like the
dotted lines of Fig. 5.6. But an edge in the focused region like the solid line of
the ﬁgure does make a third-harmonic signal E3. Third-harmonic microscopy
lets us see edges or features instead of background.
Example 5.20 (Green and Bessel)
Let us evaluate the integral
I(x) =
 ∞
−∞
dk
eikx
k2 + m2 ,
(5.130)
184

5.14 GHOST CONTOURS
Third-harmonic microscopy
−L
L
visible
unseen
unseen
Figure 5.6
In the limit in which the distance L is much larger than the wave-length
λ, the integral (5.128) is nonzero when an edge (solid line) lies where the beam is
focused but not when a feature (dots) lies where the beam is not focused. Only
features within the focused region are visible.
which is the Fourier transform of the function 1/(k2 + m2). If x > 0, then the
exponential deceases with k in the upper half-plane. So as in example 5.18, the
semicircular contour k = R eiθ, 0 ≤θ ≤π, and dk = iReiθdθ is a ghost contour.
So if x > 0, then we can add this contour to the integral I(x) without changing
it. Thus I(x) is equal to the closed contour integral along the real axis and the
semicircular ghost contour
I(x) =
)
dk
eikx
k2 + m2 =
)
dk
eikx
(k + im)(k −im).
(5.131)
This closed contour encircles the simple pole at k = im and no other singularity,
and so we may shrink the contour into a tiny circle around the pole. Along that
tiny circle, the function eikx/(k + im) is simply e−mx/2im, and so
I(x) = e−mx
2im
)
dk
k −im = 2πi e−mx
2im = πe−mx
m
for
x > 0.
(5.132)
Similarly, if x < 0, we can add the semicircular ghost contour k = R eiθ,
π ≤θ ≤2π, dk = iReiθdθ with k running around the perimeter of the lower
half-plane. So if x < 0, then we can write the integral I(x) as a shrunken closed
contour that runs clockwise around the pole at k = −im
185

COMPLEX-VARIABLE THEORY
I(x) =
emx
−2im
)
dk
k + im = −2πi emx
−2im = πemx
m
for
x < 0.
(5.133)
We combine the two cases (5.132) and (5.133) into the result
 ∞
−∞
dk
eikx
k2 + m2 = π
m e−m|x|.
(5.134)
We can use this formula to develop an expression for the Green’s function of
the Laplacian in cylindrical coordinates. Setting x′ = 0 and r = |x| =

ρ2 + z2
in the Coulomb Green’s function (3.110), we have
G(r) =
1
4πr =
1
4π

ρ2 + z2 =

d3k
(2π)3
1
k2 eik·x.
(5.135)
The integral over the z-component of k is (5.134) with m2 = k2
x + k2
y ≡k2
 ∞
−∞
dkz
eikzz
k2z + k2 = π
k e−k|z|.
(5.136)
So with kxx + kyy ≡kρ cos φ, the Green’s function is
1
4π

ρ2 + z2 =
 ∞
0
πdk
(2π)3
 2π
0
dφ eikρ cos φ e−k|z|.
(5.137)
The φ integral is a representation (5.46, 9.7) of the Bessel function J0(kρ)
J0(kρ) =
 2π
0
dφ
2π eikρ cos φ.
(5.138)
Thus we arrive at Bessel’s formula for the Coulomb Green’s function
1
4π

ρ2 + z2 =
 ∞
0
dk
4π J0(kρ) e−k|z|
(5.139)
in cylindrical coordinates (Schwinger et al., 1998, p. 166).
Example 5.21 (Yukawa and Green)
We saw in example 3.11 that the Green’s
function for Yukawa’s differential operator (3.123) is
GY(x) =

d3k
(2π)3
eik·x
k2 + m2 .
(5.140)
Letting k · x = kr cos θ in which r = |x|, we ﬁnd
GY(r) =
 ∞
0
k2dk
(2π)2
 1
−1
eikr cos θ
k2 + m2 d cos θ = 1
ir
 ∞
0
dk
(2π)2
k
k2 + m2

eikr −e−ikr
= 1
ir
 ∞
−∞
dk
(2π)2
k
k2 + m2 eikr = 1
ir
 ∞
−∞
dk
(2π)2
k
(k −im)(k + im) eikr.
186

5.14 GHOST CONTOURS
We add a ghost contour that loops over the upper half-plane and get
GY(r) =
2πi
(2π)2ir
im
2im e−mr = e−mr
4πr ,
(5.141)
which Yukawa proposed as the potential between two hadrons due to the
exchange of a particle of mass m, the pion. Because the mass of the pion is
140 MeV, the range of the Yukawa potential is ¯h/mc = 1.4 × 10−15 m.
Example 5.22 (The Green’s function for the Laplacian in n dimensions)
The
Green’s function for the Laplacian −△G(x) = δ(n)(x) is
G(x) =

1
k2 eik·x dnk
(2π)n
(5.142)
in n dimensions. We use the formula
1
k2 =
 ∞
0
e−λk2 dλ
(5.143)
to write it as a gaussian integral
G(x) =

e−λk2+ik·x dλ dnk
(2π)n .
(5.144)
We now complete the square in the exponent
−λk2 + ik · x = −λ (k −ix/2λ)2 −x2/4λ
(5.145)
and use our gaussian formula (5.121) to write the Green’s function as
G(x) =
 ∞
0
dλ
 dnk
(2π)n e−x2/4λe−λ(k−ix/2λ)2 =
 ∞
0
dλ
 dnk
(2π)n e−x2/4λe−λ k2
=
 ∞
0
e−x2/4λ
dλ
(4πλ)n/2 = (x2)1−n/2
4πn/2
 ∞
0
e−ααn/2−2dα
=
(n/2 −1)
4πn/2(x2)(n/2−1) .
(5.146)
Since (1/2) = √π, this formula for n = 3 gives G(x) = 1/4π|x|, which is
(3.110); since (1) = 1, it gives
G(x) =
1
4π2x2
(5.147)
for n = 4.
Example 5.23 (The Yukawa Green’s function in n dimensions)
The Yukawa
Green’s function which satisﬁes (−△+ m2)G(x) = δ(n)(x) in n dimensions is the
integral (5.142) with k2 replaced by k2 + m2
G(x) =

1
k2 + m2 eik·x dnk
(2π)n .
(5.148)
187

COMPLEX-VARIABLE THEORY
Using the integral formula (5.143), we write it as a gaussian integral
G(x) =

e−λ(k2+m2)+ik·x dλdnk
(2π)n .
(5.149)
Completing the square as in (5.145), we have
G(x) =

e−x2/4λe−λ(k−ix/2λ)2−λm2 dλdnk
(2π)n =

e−x2/4λe−λ (k2+m2) dλdnk
(2π)n
=
 ∞
0
e−x2/4λ−λm2
dλ
(4πλ)n/2 .
(5.150)
We can relate this to a Bessel function by setting λ = |x|/2m exp(−α)
G(x) =
1
(4π)n/2
2m
x
(n/2−1)  ∞
−∞
e−mx cosh α+(n/2−1)α dα
=
2
(4π)n/2
2m
x
(n/2−1)  ∞
0
e−mx cosh α cosh(n/2 −1)α dα
=
2
(4π)n/2
2m
x
(n/2−1)
Kn/2−1(mx)
(5.151)
where x = |x| =
√
x2 and K is a modiﬁed Bessel function of the second kind
(9.98). If n = 3, this is (exercise 5.27) the Yukawa potential (5.141).
Example 5.24 (A Fourier transform)
As another example, let’s consider the
integral
J(x) =
 ∞
−∞
eikx
(k2 + m2)2 dk.
(5.152)
We may add ghost contours as in the preceding example, but now the integrand
has double poles at k = ±im, and so we must use Cauchy’s integral formula
(5.36) for the case of n = 1, which is (5.34). For x > 0, we add a ghost contour
in the UHP and ﬁnd
J(x) =
)
eikx
(k + im)2(k −im)2 dk = 2πi d
dk
eikx
(k + im)2

k=im
=
π
2m2

x + 1
m

e−mx.
(5.153)
If x < 0, then we add a ghost contour in the LHP and ﬁnd
J(x) =
)
eikx
(k + im)2(k −im)2 dk = −2πi d
dk
eikx
(k −im)2

k=−im
=
π
2m2

−x + 1
m

emx.
(5.154)
188

5.14 GHOST CONTOURS
Putting the two together, we get
J(x) =
 ∞
−∞
eikx
(k2 + m2)2 dk =
π
2m2

|x| + 1
m

e−m|x|
(5.155)
as the Fourier transform of 1/(k2 + m2)2.
Example 5.25 (Integral of a complex gaussian)
As another example of the use
of ghost contours, let us use one to do the integral
I =
 ∞
−∞
ewx2 dx,
(5.156)
in which the real part of the nonzero complex number w = u + iv = ρeiφ is
negative or zero
u ≤0
⇐⇒
π
2 ≤φ ≤3π
2 .
(5.157)
We ﬁrst write the integral I as twice that along half the x-axis
I = 2
 ∞
0
ewx2 dx.
(5.158)
If we promote x to a complex variable z = reiθ, then wz2 will be negative if
φ + 2θ = π, that is, if θ = (π −φ) /2 where in view of (5.157) θ lies in the
interval −π/4 ≤θ ≤π/4.
The closed pie-shaped contour of Fig. 5.7 (down the real axis from z = 0 to
z = R, along the arc z = R exp(iθ′) as θ′ goes from 0 to θ, and then down the
line z = r exp(iθ) from z = R exp(iθ) to z = 0) encloses no singularities of the
function f (z) = exp(wz2). Hence the integral of exp(wz2) along that contour
vanishes.
To show that the arc is a ghost contour, we bound it by

 θ
0
e(u+iv)R2e2iθ′
R dθ′
 ≤
 θ
0
exp

uR2 cos 2θ′ −vR2 sin 2θ′
R dθ′
≤
 θ
0
e−vR2 sin 2θ′R dθ′.
(5.159)
Here v sin 2θ′ ≥0, and so if v is positive, then so is θ′. Then 0 ≤θ′ ≤π/4, and
so sin(2θ′) ≥4θ′/π. Thus since u < 0, we have the upper bound

 θ
0
e(u+iv)R2e2iθ′
R dθ′
 ≤
 θ
0
e−4vR2θ′/πR dθ′ = π(e−4vR2θ′/π −1)
4vR
,
(5.160)
which vanishes in the limit R →∞. (If v is negative, then so is θ′, the pie-shaped
contour is in the fourth quadrant, sin(2θ′) ≤4θ′/π, and the inequality (5.160)
holds with absolute-value signs around the second integral.)
Since by Cauchy’s integral theorem (5.22) the integral along the pie-shaped
contour of Fig. 5.7 vanishes, it follows that
189

COMPLEX-VARIABLE THEORY
Pie-shaped contour
>
θ
z = 0
z = R
z = R eiθ
Figure 5.7
The integral of the entire function exp(wz2) along the pie-shaped
closed contour vanishes by Cauchy’s theorem.
1
2I +
 0
Reiθ ewz2dz = 0.
(5.161)
But the choice θ = (π −φ) /2 implies that on the line z = r exp(iθ) the quantity
wz2 is negative, wz2 = −ρr2. Thus with dz = exp(iθ)dr, we have
I = 2
 Reiθ
0
ewz2 dz = 2eiθ
 R
0
e−ρr2 dr
(5.162)
so that as R →∞
I = 2eiθ
 ∞
0
e−ρr2 dr = eiθ
&π
ρ =
&
π
ρe−2iθ .
(5.163)
Finally, from θ = (π −φ) /2 and w = ρ exp(iφ), we ﬁnd that for Re w ≤0
 ∞
−∞
ewx2 dx =
& π
−w
(5.164)
as long as w ̸= 0. Shifting x by a complex number b, we still have
 ∞
−∞
ew(x−b)2 dx =
& π
−w
(5.165)
190

5.14 GHOST CONTOURS
as long as Re w < 0. If w = ia ̸= 0 and b is real, then
 ∞
−∞
eia(x−b)2 dx =
&
iπ
a .
(5.166)
The simpler integral (5.122) applies when m > 0 and z is an arbitrary complex
number
 ∞
−∞
e−m2(x+z)2 dx =
√π
m .
(5.167)
These last two formulas are used in chapter 16 on path integrals.
Let us try to express the line integral of a not necessarily analytic function
f (x, y) = u(x, y) + iv(x, y) along a closed ccw contour C as an integral over the
surface enclosed by the contour. The contour integral is
)
C
(u + iv)(dx + idy) =
)
C
(u dx −v dy) + i
)
C
(v dx + u dy).
(5.168)
Now since the contour C is counterclockwise, the differential dx is negative
at the top of the curve with coordinates (x, y+(x)) and positive at the bottom
(x, y−(x)). So the ﬁrst line integral is the surface integral
)
C
u dx =

[u(x, y−(x)) −u(x, y+(x))] dx
= −
  y+(x)
y−(x)
uy(x, y)dy

dx
= −

uy |dxdy| = −

uy da,
(5.169)
in which da = |dxdy| is a positive element of area. Similarly, we ﬁnd
i
)
C
v dx = −i

vy |dxdy| = −i

vy da.
(5.170)
The dy integrals are then
−
)
C
v dy = −

vx |dxdy| = −

vx da,
(5.171)
i
)
C
u dy = i

ux |dxdy| = i

ux da.
(5.172)
Combining (5.168–5.172), we ﬁnd
)
C
(u + iv)(dx + idy) = −

(uy + vx) da + i

(−vy + ux) da.
(5.173)
This formula holds whether or not the function f (x, y) is analytic. But if f (x, y)
is analytic on and within the contour C, then it satisﬁes the Cauchy–Riemann
191

COMPLEX-VARIABLE THEORY
conditions (5.48) within the contour, and so both surface integrals vanish. The
contour integral then is zero, which is Cauchy’s integral theorem (5.21).
The contour integral of the function f (x, y) = u(x, y) + iv(x, y) differs from
zero (its value if f (x, y) is analytic in z = x + iy) by the surface integrals of
uy + vx and ux −vy

)
C
f (z)dz

2
=

)
C
(u + iv)(dx + idy)

2
=


(uy + vx)da

2
+


(ux −vy)da

2
,
(5.174)
which vanish when f = u + iv satisﬁes the Cauchy–Riemann conditions (5.48).
Example 5.26 (The integral of a nonanalytic function)
The integral formula
(5.173) can help us evaluate contour integrals of functions that are not analytic.
The function
f (x, y) =
1
x + iy + iϵ
1
1 + x2 + y2
(5.175)
is the product of an analytic function 1/(z+iϵ), where ϵ is tiny and positive, and
a nonanalytic real one r(x, y) = 1/(1 + z∗z). The iϵ pushes the pole in u + iv =
1/(z + iϵ) into the lower half-plane. The real and imaginary parts of f are
U(x, y) = u(x, y) r(x, y) =
x
x2 + (y + ϵ)2
1
1 + x2 + y2
(5.176)
and
V(x, y) = v(x, y) r(x, y) =
−y −ϵ
x2 + (y + ϵ)2
1
1 + x2 + y2 .
(5.177)
We will use (5.173) to compute the contour integral I of f along the real axis
from −∞to ∞and then along the ghost contour z = x+iy = Reiθ for 0 ≤θ ≤π
and R →∞around the upper half-plane
I =
)
f (x, y) dz =
 ∞
−∞
dx
 ∞
0
dy

−Uy −Vx + i

−Vy + Ux

.
(5.178)
Since u and v satisfy the Cauchy–Riemann conditions (5.48), the terms in the
area integral simplify to −Uy −Vx = −ury −vrx and −Vy + Ux = −vry + urx.
So the integral I is
I =
 ∞
−∞
dx
 ∞
0
dy

−ury −vrx + i(−vry + urx)

(5.179)
or explicitly
I =
 ∞
−∞
dx
 ∞
0
dy
−2ϵx −2i(x2 + y2 + ϵy)

x2 + (y + ϵ)2 
1 + x2 + y22 .
(5.180)
192

5.15 LOGARITHMS AND CUTS
We let ϵ →0 and ﬁnd
I = −2i
 ∞
−∞
dx
 ∞
0
dy
1

1 + x2 + y22 .
(5.181)
Changing variables to ρ2 = x2 + y2, we have
I = −4πi
 ∞
0
dρ
ρ
(1 + ρ2)2 = 2πi
 ∞
0
dρ d
dρ
1
1 + ρ2 = −2πi,
(5.182)
which is simpler than evaluating the integral (5.178) directly.
5.15 Logarithms and cuts
By deﬁnition, a function f is single valued; it maps every number z in its domain
into a unique image f (z). A function that maps only one number z in its domain
into each f (z) in its range is said to be one to one. A one-to-one function f (z)
has a well-deﬁned inverse function f −1(z).
The exponential function is one to one when restricted to the real num-
bers. It maps every real number x into a positive number exp(x). It has an
inverse function ln(x) that maps every positive number exp(x) back into x. But
the exponential function is not one to one on the complex numbers because
exp(z + 2πni) = exp(z) for every integer n. The exponential function is many to
one. Thus on the complex numbers, the exponential function has no inverse
function. Its would-be inverse function ln(exp(z)) is z + 2πni, which is not
unique. It has in it an arbitrary integer n.
In other words, when exponentiated, the logarithm of a complex number z
returns exp(ln z) = z. So if z = r exp(iθ), then a suitable logarithm is ln z =
ln r + iθ. But what is θ? In the polar representation of z, the argument θ can
just as well be θ + 2πn because both give z = r exp(iθ) = r exp(iθ + i2πn). So
ln r + iθ + i2πn is a correct value for ln[r exp(iθ)] for every integer n.
People usually want one of the correct values of a logarithm, rather than all
of them. Two conventions are common. In the ﬁrst convention, the angle θ is
zero along the positive real axis and increases continuously as the point z moves
counterclockwise around the origin, until at points just below the positive real
axis, θ = 2π −ϵ is slightly less than 2π. In this convention, the value of θ
drops by 2π as one crosses the positive real axis moving counterclockwise. This
discontinuity on the positive real axis is called a cut.
The second common convention puts the cut on the negative real axis. Here
the value of θ is the same as in the ﬁrst convention when the point z is in the
upper half-plane. But in the lower half-plane, θ decreases from 0 to −π as the
point z moves clockwise from the positive real axis to just below the negative
real axis, where θ = −π + ϵ. As one crosses the negative real axis moving
193

COMPLEX-VARIABLE THEORY
clockwise or up, θ jumps by 2π while crossing the cut. The two conventions
agree in the upper half-plane but differ by 2π in the lower half-plane.
Sometimes it is convenient to place the cut on the positive or negative imag-
inary axis – or along a line that makes an arbitrary angle with the real axis. In
any particular calculation, we are at liberty to deﬁne the polar angle θ by plac-
ing the cut anywhere we like, but we must not change from one convention to
another in the same computation.
5.16 Powers and roots
The logarithm is the key to many other functions to which it passes its
arbitrariness. For instance, any power a of z = r exp(iθ) is deﬁned as
za = exp (a ln z) = exp [a (ln r + iθ + i2πn)] = ra eiaθ ei2πna.
(5.183)
So za is not unique unless a is an integer. The square-root, for example,
√z = exp

1
2(ln r + iθ + i2πn)]

= √r eiθ/2 einπ = (−1)n √r eiθ/2
(5.184)
changes sign when we change θ by 2π as we cross a cut. The mth root
m√z = z1/m = exp
ln z
m

(5.185)
changes by exp(±2πi/m) when we cross a cut and change θ by 2π. And when
a = u + iv is a complex number, za is
za = ea ln z = e(u+iv)(ln r+iθ+i2πn) = ru+iv e(−v+iu)(θ+2πn),
(5.186)
which changes by exp[2π(−v + iu)] as we cross a cut.
Example 5.27 (ii)
The number i = exp(iπ/2 + i2πn) for any integer n. So the
general value of ii is ii = exp[i(iπ/2 + i2πn)] = exp(−π/2 −2πn).
One can deﬁne a sequence of mth-root functions

z1/m
n = exp
ln r + i(θ + 2πn)
m

,
(5.187)
one for each integer n. These functions are the branches of the mth-root
function. One can merge all the branches into one multivalued mth-root func-
tion. Using a convention for θ, one would extend the n = 0 branch to the
n = 1 branch by winding counterclockwise around the point z = 0. One
would encounter no discontinuity as one passed from one branch to another.
194

5.16 POWERS AND ROOTS
The point z = 0, where any cut starts, is called a branch point because, by
winding around it, one passes smoothly from one branch to another. Such
branches, introduced by Riemann, can be associated with any multivalued
analytic function, not just with the mth root.
Example 5.28 (Explicit square-roots)
If the cut in the square-root √z is on the
negative real axis, then an explicit formula for the square-root of x + iy is

x + iy =
(
x2 + y2 + x
2
+ i sign(y)
(
x2 + y2 −x
2
,
(5.188)
in which sign(y) = sgn(y) = y/|y|. On the other hand, if the cut in the square-
root √z is on the positive real axis, then an explicit formula for the square-root
of x + iy is

x + iy = sign(y)
(
x2 + y2 + x
2
+ i
(
x2 + y2 −x
2
(5.189)
(exercise 5.28).
Example 5.29 (Cuts)
Cuts are discontinuities, so people place them where they
do the least harm. For the function
f (z) =

z2 −1 =

(z −1)(z + 1)
(5.190)
two principal conventions work well. We could put the cut in the deﬁnition of the
angle θ along either the positive or the negative real axis. And we’d get a bonus:
the sign discontinuity (a factor of −1) from
√
z −1 would cancel the one from
√
z + 1 except for −1 ≤z ≤1. So the function f (z) would have a discontinuity
or a cut only for −1 ≤z ≤1.
But now suppose we had to work with the function
f (z) =

z2 + 1 =

(z −i)(z + i).
(5.191)
If we used one of the usual conventions, we’d have two semi-inﬁnite cuts. So we
put the θ-cut on the positive or negative imaginary axis, and the function f (z)
now has a cut running along the imaginary axis only from −i to i.
Example 5.30 (Integral with a square-root)
Consider the integral
I =
 1
−1
1
(x −k)

1 −x2 dx,
(5.192)
in which the constant k lies anywhere in the complex plane but not on the inter-
val [ −1, 1]. Let’s promote x to a complex variable z and write the square-root
as

1 −x2 = i

x2 −1 = i

(z −1)(z + 1). As in the last example (5.29), if
in both of the square-roots we put the cut on the negative (or the positive)
195

COMPLEX-VARIABLE THEORY
real axis, then the function f (z) = 1/[(z −k)i

(z −1)(z + 1)] will be analytic
everywhere except along a cut on the interval [ −1, 1] and at z = k. The cir-
cle z = Reiθ for 0 ≤θ ≤2π is a ghost contour as R →∞. We shrink-wrap
this ccw contour around the pole at z = k and the interval [ −1, 1], we get
0 = −2I + 2πi/

1 −k2, so
I =
πi

1 −k2 =
π

k2 −1
.
(5.193)
For instance, if k = −2, the integral is I = π/
√
3 ≈1.8138.
Example 5.31 (Contour integral with a cut)
Let’s compute the integral
I =
 ∞
0
xa
(x + 1)2 dx
(5.194)
for −1 < a < 1. We promote x to a complex variable z and put the cut on the
positive real axis. Since
lim
|z|→∞
|z|a+1
|z + 1|2 = 0,
(5.195)
the integrand vanishes faster than 1/|z|, and we may add two ghost contours, G+
counterclockwise around the upper half-plane and G−counterclockwise around
the lower half-plane, as shown in Fig. 5.8.
We add a contour C−that runs from −∞to the double pole at z = −1, loops
around that pole, and then runs back to −∞; the two long contours along the
negative real axis cancel because the cut in θ lies on the positive real axis. So the
contour integral along C−is just the clockwise integral around the double pole,
which by Cauchy’s integral formula (5.34) is
)
C−
za
(z −(−1))2 dz = −2πi dza
dz

z=−1
= 2πi a eπai.
(5.196)
We also add the integral I−from ∞to 0 just below the real axis
I−=
 0
∞
(x −iϵ)a
(x −iϵ + 1)2 dx =
 0
∞
exp(a(ln(x) + 2πi))
(x + 1)2
dx,
(5.197)
which is
I−= −e2πai
 ∞
0
xa
(x + 1)2 dx = −e2πai I.
(5.198)
Now the sum of all these contour integrals is zero because it is a closed contour
that encloses no singularity. So we have
0 =

1 −e2πai
I + 2πi a eπai
(5.199)
196

5.17 CONFORMAL MAPPING
>
<
>
<
>
<
>
Ghost contours and a cut
Figure 5.8
The integral of f (z) = za/(z+1) along the ghost contours G+ and G−,
the contour C−, the contour I−, and the contour I vanishes because the combined
contour encircles no poles of f (z). The cut (solid line) runs from the origin to inﬁnity
along the positive real axis.
or
I =
 ∞
0
xa
(x + 1)2 dx =
πa
sin(πa)
(5.200)
as the value of the integral (5.194).
5.17 Conformal mapping
An analytic function f (z) maps curves in the z plane into curves in the f (z)
plane. In general, this mapping preserves angles. To see why, we consider the
angle dθ between two tiny complex lines dz = ϵ exp(iθ) and dz′ = ϵ exp(iθ′)
that radiate from the same point z. This angle dθ = θ′ −θ is the phase of the
ratio
dz′
dz = ϵeiθ′
ϵeiθ = ei(θ′−θ).
(5.201)
197

COMPLEX-VARIABLE THEORY
Let’s use w = ρeiφ for f (z). Then the analytic function f (z) maps dz into
dw = f (z + dz) −f (z) ≈f ′(z) dz
(5.202)
and dz′ into
dw′ = f (z + dz′) −f (z) ≈f ′(z) dz′.
(5.203)
The angle dφ = φ′ −φ between dw and dw′ is the phase of the ratio
dw′
dw = eiφ′
eiφ = f ′(z) dz′
f ′(z) dz = dz′
dz = eiθ′
eiθ = ei(θ′−θ).
(5.204)
So as long as the derivative f ′(z) does not vanish, the angle in the w-plane is the
same as the angle in the z-plane
dφ = dθ.
(5.205)
Analytic functions preserve angles. They are conformal maps.
What if f ′(z) = 0? In this case, dw ≈f
′′(z) dz2/2 and dw′ ≈f
′′(z) dz′2/2, and
so the angle dφ = dφ′ −dφ between these two tiny complex lines is the phase
of the ratio
dw′
dw = eiφ′
eiφ = f
′′(z) dz′2
f
′′(z) dz2 = dz′2
dz2 = e2i(θ′−θ).
(5.206)
So angles are doubled, dφ = 2dθ.
In general, if the ﬁrst nonzero derivative is f (n)(z), then
dw′
dw = eiφ′
eiφ = f (n)(z) dz′n
f (n)(z) dzn = dz′n
dzn = eni(θ′−θ)
(5.207)
and so dφ = ndθ. The angles increase by a factor of n.
Example 5.32 (zn)
The function f (z) = czn has only one nonzero derivative at
the origin z = 0
f (k)(0) = c n! δnk
(5.208)
so at z = 0 the conformal map z →czn scales angles by n, dφ = n dθ.
For examples of conformal mappings see (Lin, 2011, section 3.5.7).
5.18 Cauchy’s principal value
Suppose that f (x) is differentiable or analytic at and near the point x = 0, and
that we wish to evaluate the integral
K = lim
ϵ→0
 b
−a
dx f (x)
x −iϵ
(5.209)
198

5.18 CAUCHY’S PRINCIPAL VALUE
for a > 0 and b > 0. First we regularize the pole at x = 0 by using a method
devised by Cauchy
K = lim
δ→0

lim
ϵ→0
 −δ
−a
dx f (x)
x −iϵ +
 δ
−δ
dx f (x)
x −iϵ +
 b
δ
dx f (x)
x −iϵ

.
(5.210)
In the ﬁrst and third integrals, since |x| ≥δ, we may set ϵ = 0
K = lim
δ→0
 −δ
−a
dx f (x)
x
+
 b
δ
dx f (x)
x

+ lim
δ→0 lim
ϵ→0
 δ
−δ
dx f (x)
x −iϵ .
(5.211)
We’ll discuss the ﬁrst two integrals before analyzing the last one.
The limit of the ﬁrst two integrals is called Cauchy’s principal value
P
 b
−a
dx f (x)
x
≡lim
δ→0
 −δ
−a
dx f (x)
x
+
 b
δ
dx f (x)
x

.
(5.212)
If the function f (x) is nearly constant near x = 0, then the large negative values
of 1/x for x slightly less than zero cancel the large positive values of 1/x for
x slightly greater than zero. The point x = 0 is not special; Cauchy’s principal
value about x = y is deﬁned by the limit
P
 b
−a
dx f (x)
x −y ≡lim
δ→0
 y−δ
−a
dx f (x)
x −y +
 b
y+δ
dx f (x)
x −y

.
(5.213)
Using Cauchy’s principal value, we may write the quantity K as
K = P
 b
−a
dx f (x)
x
+ lim
δ→0 lim
ϵ→0
 δ
−δ
dx f (x)
x −iϵ .
(5.214)
To evaluate the second integral, we use differentiability of f (x) near x = 0 to
write f (x) = f (0) + xf ′(0) and then extract the constants f (0) and f ′(0)
lim
δ→0 lim
ϵ→0
 δ
−δ
dx f (x)
x −iϵ = lim
δ→0 lim
ϵ→0
 δ
−δ
dx f (0) + x f ′(0)
x −iϵ
= f (0) lim
δ→0 lim
ϵ→0
 δ
−δ
dx
x −iϵ + f ′(0) lim
δ→0 lim
ϵ→0
 δ
−δ
x dx
x −iϵ
= f (0) lim
δ→0 lim
ϵ→0
 δ
−δ
dx
x −iϵ + f ′(0) lim
δ→0 2δ
= f (0) lim
δ→0 lim
ϵ→0
 δ
−δ
dx
x −iϵ .
(5.215)
Now since 1/(z −iϵ) is analytic in the lower half-plane, we may deform the
straight contour from x = −δ to x = δ into a tiny semicircle that avoids the
point x = 0 by setting z = δ eiθ and letting θ run from π to 2π
199

COMPLEX-VARIABLE THEORY
K = P
 b
−a
dx f (x)
x
+ f (0) lim
δ→0 lim
ϵ→0
 δ
−δ
dz
1
z −iϵ .
(5.216)
We now can set ϵ = 0 and so write K as
K = P
 b
−a
dx f (x)
x
+ f (0) lim
δ→0
 2π
π
iδeiθdθ
1
δeiθ
= P
 b
−a
dx f (x)
x
+ iπf (0).
(5.217)
Recalling the deﬁnition (5.209) of K, we have
lim
ϵ→0
 b
−a
dx f (x)
x −iϵ = P
 b
−a
dx f (x)
x
+ iπf (0)
(5.218)
for any function f (x) that is differentiable at x = 0. Physicists write this as
1
x −iϵ = P1
x + iπδ(x)
and
1
x + iϵ = P1
x −iπδ(x)
(5.219)
or as
1
x −y ± iϵ = P
1
x −y ∓iπδ(x −y).
(5.220)
Example 5.33 (Cauchy’s trick)
We use (5.219) to evaluate the integral
I =
 ∞
−∞
dx
1
x + iϵ
1
1 + x2
(5.221)
as
I = P
 ∞
−∞
dx 1
x
1
1 + x2 −iπ
 ∞
−∞
dx δ(x)
1 + x2 .
(5.222)
Because the function 1/x(1 + x2) is odd, the principal part is zero. The integral
over the delta function gives unity, so we have I = −iπ.
Example 5.34 (Cauchy’s principal value)
By explicit use of the formula

dx
x2 −a2 = −1
2a ln x + a
x −a
(5.223)
one may show (exercise 5.30) that
P
 ∞
0
dx
x2 −a2 =
 a−δ
0
dx
x2 −a2 +
 ∞
a+δ
dx
x2 −a2 = 0,
(5.224)
a result we’ll use in section 5.21.
200

5.18 CAUCHY’S PRINCIPAL VALUE
Example 5.35 (sin k/k)
To compute the integral
I =
 ∞
0
dk
k sin k,
(5.225)
which we used to derive the formula (3.110) for the Green’s function of the
Laplacian in three dimensions, we ﬁrst express I as an integral along the whole
real axis
I =
 ∞
0
dk
2ik

eik −e−ik
=
 ∞
−∞
dk
2ik eik,
(5.226)
by which we actually mean the Cauchy principal part
I = lim
δ→0
 −δ
−∞
dk eik
2ik +
 ∞
δ
dk eik
2ik

= P
 ∞
−∞
dk eik
2ik.
(5.227)
Using Cauchy’s trick (5.219), we have
I = P
 ∞
−∞
dk eik
2ik =
 ∞
−∞
dk
eik
2i(k + iϵ) +
 ∞
−∞
dk iπ δ(k) eik
2i .
(5.228)
To the ﬁrst integral, we add a ghost contour around the upper half-plane. For
the contour from k = L to k = L + iH and then to k = −L + iH and then
down to k = −L, one may show (exercise 5.33) that the integral of exp(ik)/k
vanishes in the double limit L →∞and H →∞. With this ghost contour,
the ﬁrst integral therefore vanishes because the pole at k = −iϵ is in the lower
half-plane. The delta function in the second integral then gives π/2, so that
I =
)
dk
eik
2i(k + iϵ) + π
2 = π
2
(5.229)
as stated in (3.109).
Example 5.36 (The Feynman propagator)
Adding ±iϵ to the denominator of a
pole term of an integral formula for a function f (x) can slightly shift the pole into
the upper or lower half-plane, causing the pole to contribute if a ghost contour
goes around the upper half-plane or the lower half-plane. Such an iϵ can impose
a boundary condition on Green’s function.
The Feynman propagator F(x) is a Green’s function for the Klein–Gordon
differential operator (Weinberg, 1995, pp. 274–280)
(m2 −2)F(x) = δ4(x)
(5.230)
in which x = (x0, x) and
2 = △−∂2
∂t2 = △−
∂2
∂(x0)2
(5.231)
is the four-dimensional version of the Laplacian △≡∇· ∇. Here δ4(x) is the
four-dimensional Dirac delta function (3.36)
201

COMPLEX-VARIABLE THEORY
δ4(x) =

d4q
(2π)4 exp[i(q · x −q0x0)] =

d4q
(2π)4 eiqx,
(5.232)
in which qx = q · x −q0x0 is the Lorentz-invariant inner product of the 4-
vectors q and x. There are many Green’s functions that satisfy equation (5.230).
Feynman’s propagator F(x) is the one that satisﬁes boundary conditions that
will become evident when we analyze the effect of its iϵ
F(x) =

d4q
(2π)4
exp(iqx)
q2 + m2 −iϵ =

d3q
(2π)3
 ∞
−∞
dq0
2π
eiq·x−iq0x0
q2 + m2 −iϵ .
(5.233)
The quantity q0 = Eq =

q2 + m2 is the energy of a particle of mass m
and momentum q in natural units with the speed of light c = 1. Using this
abbreviation and setting ϵ′ = ϵ/2Eq, we may write the denominator as
q2 + m2 −iϵ = q · q −

q02
+ m2 −iϵ =

Eq −iϵ′ −q0 
Eq −iϵ′ + q0
+ ϵ′2,
(5.234)
in which ϵ′2 is negligible. Dropping the prime on ϵ, we do the q0 integral
I(q) = −
 ∞
−∞
dq0
2π e−iq0x0
1

q0 −(Eq −iϵ)
 
q0 −(−Eq + iϵ)
.
(5.235)
As shown in Fig. 5.9, the integrand
e−iq0x0
1

q0 −(Eq −iϵ)
 
q0 −(−Eq + iϵ)

(5.236)
has poles at Eq −iϵ and at −Eq + iϵ. When x0 > 0, we can add a ghost contour
that goes clockwise around the lower half-plane and get
I(q) = ie−iEqx0
1
2Eq
,
x0 > 0.
(5.237)
When x0 < 0, our ghost contour goes counterclockwise around the upper half-
plane, and we get
I(q) = ieiEqx0
1
2Eq
,
x0 < 0.
(5.238)
Using the step function θ(x) = (x + |x|)/2, we combine (5.237) and (5.238)
−iI(q) =
1
2Eq

θ(x0) e−iEqx0 + θ(−x0) eiEqx0
.
(5.239)
In terms of the Lorentz-invariant function
+(x) =
1
(2π)3
 d3q
2Eq
exp[i(q · x −Eqx0)]
(5.240)
202

5.18 CAUCHY’S PRINCIPAL VALUE
>
<
<
x0 < 0
x0 > 0
Eq − iϵ
Eq + iϵ
Ghost contours and the Feynman propagator
Figure 5.9
In equation (5.236), the function f (q0) has poles at ±(Eq −iϵ), and the
function exp(−iq0x0) is exponentially suppressed in the lower half-plane if x0 > 0
and in the upper half-plane if x0 < 0. So we can add a ghost contour (dots) in the
LHP if x0 > 0 and in the UHP if x0 < 0.
and with a factor of −i, Feynman’s propagator (5.233) is
−iF(x) = θ(x0) +(x) + θ(−x0) +(x, −x0).
(5.241)
The integral (5.240) deﬁning +(x) is insensitive to the sign of q, and so
+(−x) =
1
(2π)3
 d3q
2Eq
exp[i(−q · x + Eqx0)]
=
1
(2π)3
 d3q
2Eq
exp[i(q · x + Eqx0)] = +(x, −x0).
(5.242)
Thus we arrive at the standard form of the Feynman propagator
−iF(x) = θ(x0) +(x) + θ(−x0) +(−x).
(5.243)
The annihilation operators a(q) and the creation operators a†(p) of a scalar
ﬁeld φ(x) satisfy the commutation relations
203

COMPLEX-VARIABLE THEORY
[a(q), a†(p)] = δ3(q −p)
and
[a(q), a(p)] = [a†(q), a†(p)] = 0.
(5.244)
Thus the commutator of the positive-frequency part
φ+(x) =

d3p

(2π)32p0 exp[i(p · x −p0x0)] a(p)
(5.245)
of a scalar ﬁeld φ = φ+ + φ−with its negative-frequency part
φ−(y) =

d3q

(2π)32q0 exp[−i(q · y −q0y0)] a†(q)
(5.246)
is the Lorentz-invariant function +(x −y)
[φ+(x), φ−(y)] =

d3p d3q
(2π)32

q0p0 eipx−iqy [a(p), a†(q)]
=

d3p
(2π)32p0 eip(x−y) = +(x −y)
(5.247)
in which p(x −y) = p · (x −y) −p0(x0 −y0).
At points x that are space-like, that is, for which x2 = x2 −(x0)2 ≡r2 > 0,
the Lorentz-invariant function +(x) depends only upon r = +
√
x2 and has the
value (Weinberg, 1995, p. 202)
+(x) =
m
4π2r K1(mr),
(5.248)
in which the Hankel function K1 is
K1(z) = −π
2 [J1(iz) + iN1(iz)] = 1
z +
z
2j + 2
#
ln
 z
2

+ γ −
1
2j + 2
$
+ · · ·
(5.249)
where J1 is the ﬁrst Bessel function, N1 is the ﬁrst Neumann function, and γ =
0.57721 . . . is the Euler–Mascheroni constant.
The Feynman propagator arises most simply as the mean value in the vacuum
of the time-ordered product of the ﬁelds φ(x) and φ(y)
T {φ(x)φ(y)} ≡θ(x0 −y0)φ(x)φ(y) + θ(y0 −x0)φ(y)φ(x).
(5.250)
The operators a(p) and a†(p) respectively annihilate the vacuum ket a(p)|0⟩= 0
and bra ⟨0|a†(p) = 0, and so by (5.245 & 5.246) do the positive- and negative-
frequency parts of the ﬁeld φ+(z)|0⟩= 0 and ⟨0|φ−(z) = 0. Thus the mean value
in the vacuum of the time-ordered product is
⟨0|T {φ(x)φ(y)} |0⟩= ⟨0|θ(x0−y0)φ(x)φ(y) + θ(y0−x0)φ(y)φ(x)|0⟩
= ⟨0|θ(x0−y0)φ+(x)φ−(y) + θ(y0−x0)φ+(y)φ−(x)|0⟩
= ⟨0|θ(x0−y0)[φ+(x), φ−(y)]
+ θ(y0−x0)[φ+(y), φ−(x)]|0⟩.
(5.251)
204

5.19 DISPERSION RELATIONS
But by (5.247), these commutators are +(x−y) and +(y−x). Thus the mean
value in the vacuum of the time-ordered product
⟨0|T {φ(x)φ(y)} |0⟩= θ(x0 −y0)+(x −y) + θ(y0 −x0)+(y −x)
= −iF(x −y)
(5.252)
is the Feynman propagator (5.241) multiplied by −i.
5.19 Dispersion relations
In many physical contexts, functions occur that are analytic in the upper
half-plane (UHP). Suppose for instance that ˆf (t) is a transfer function that
determines an effect e(t) due to a cause c(t)
e(t) =
 ∞
−∞
dt′ ˆf (t −t′) c(t′).
(5.253)
If the system is causal, then the transfer function ˆf (t −t′) is zero for t −t′ < 0,
and so its Fourier transform
f (z) =
 ∞
−∞
dt
√
2π
ˆf (t) eizt =
 ∞
0
dt
√
2π
ˆf (t) eizt
(5.254)
will be analytic in the upper half-plane and will shrink as the imaginary part of
z = x + iy increases.
So let us assume that the function f (z) is analytic in the upper half-plane and
on the real axis and further that
lim
r→∞|f (reiθ)| = 0
for
0 ≤θ ≤π.
(5.255)
By Cauchy’s integral formula (5.32), if z0 lies in the upper half-plane, then f (z0)
is given by the closed counterclockwise contour integral
f (z0) =
1
2πi
)
f (z)
z −z0
dz,
(5.256)
in which the contour runs along the real axis and then loops over the semicircle
lim
r→∞reiθ
for
0 ≤θ ≤π.
(5.257)
Our assumption (5.255) about the behavior of f (z) in the UHP implies that this
contour (5.257) is a ghost contour because its modulus is bounded by
lim
r→∞
1
2π
 |f (reiθ)|r
r
dθ = lim
r→∞|f (reiθ)| = 0.
(5.258)
So we may drop the ghost contour and write f (z0) as
f (z0) =
1
2πi
 ∞
−∞
f (x)
x −z0
dx.
(5.259)
205

COMPLEX-VARIABLE THEORY
Letting the imaginary part y0 of z0 = x0 + iy0 shrink to ϵ
f (x0) =
1
2πi
 ∞
−∞
f (x)
x −x0 −iϵ dx
(5.260)
and using Cauchy’s trick (5.220), we get
f (x0) =
1
2πi P
 ∞
−∞
f (x)
x −x0
dx + iπ
2πi
 ∞
−∞
f (x) δ(x −x0) dx
(5.261)
or
f (x0) =
1
2πi P
 ∞
−∞
f (x)
x −x0
dx + 1
2 f (x0),
(5.262)
which is the dispersion relation
f (x0) = 1
πi P
 ∞
−∞
f (x)
x −x0
dx.
(5.263)
If we break f (z) = u(z)+iv(z) into its real u(z) and imaginary v(z) parts, then
this dispersion relation (5.263)
u(x0) + iv(x0) = 1
πi P
 ∞
−∞
u(x) + iv(x)
x −x0
dx
(5.264)
= 1
π P
 ∞
−∞
v(x)
x −x0
dx −i
π P
 ∞
−∞
u(x)
x −x0
dx
breaks into its real and imaginary parts
u(x0) = 1
π P
 ∞
−∞
v(x)
x −x0
dx
and
v(x0) = −1
π P
 ∞
−∞
u(x)
x −x0
dx,
(5.265)
which express u and v as Hilbert transforms of each other.
In applications of dispersion relations, the function f (x) for x < 0 sometimes
is either physically meaningless or experimentally inaccessible. In such cases,
there may be a symmetry that relates f (−x) to f (x). For instance, if f (x) is the
Fourier transform of a real function ˆf (k), then by equation (3.25) it obeys the
symmetry relation
f ∗(x) = u(x) −iv(x) = f (−x) = u(−x) + iv(−x),
(5.266)
which says that u is even, u(−x) = u(x), and v odd, v(−x) = −v(x). Using
these symmetries, one may show (exercise 5.36) that the Hilbert transformations
(5.265) become
u(x0) = 2
π P
 ∞
0
x v(x)
x2 −x2
0
dx
and
v(x0) = −2x0
π P
 ∞
0
u(x)
x2 −x2
0
dx,
(5.267)
which do not require input at negative values of x.
206

5.20 KRAMERS–KRONIG RELATIONS
5.20 Kramers–Kronig relations
If we use σE for the current density J and E(t) = e−iωtE(t) for the electric ﬁeld,
then Maxwell’s equation ∇× B = μJ + ϵμ ˙E becomes
∇× B = −iωϵμ

1 + i σ
ϵω

E ≡−iωn2ϵ0μ0E
(5.268)
and reveals the squared index of refraction as
n2(ω) = ϵμ
ϵ0μ0

1 + i σ
ϵω

.
(5.269)
The imaginary part of n2 represents the scattering of light mainly by electrons.
At high frequencies in nonmagnetic materials n2(ω) →1, and so Kramers and
Kronig applied the Hilbert-transform relations (5.267) to the function n2(ω)−1
in order to satisfy condition (5.255). Their relations are
Re(n2(ω0)) = 1 + 2
π P
 ∞
0
ω Im(n2(ω))
ω2 −ω2
0
dω
(5.270)
and
Im(n2(ω0)) = −2ω0
π
P
 ∞
0
Re(n2(ω)) −1
ω2 −ω2
0
dω.
(5.271)
What Kramers and Kronig actually wrote was slightly different from these
dispersion relations (5.270 & 5.271). H. A. Lorentz had shown that the index
of refraction n(ω) is related to the forward scattering amplitude f (ω) for the
scattering of light by a density N of scatterers (Sakurai, 1982)
n(ω) = 1 + 2πc2
ω2 Nf (ω).
(5.272)
They used this formula to infer that the real part of the index of refraction
approached unity in the limit of inﬁnite frequency and applied the Hilbert
transform (5.267)
Re[n(ω)] = 1 + 2
π P
 ∞
0
ω′ Im[n(ω′)]
ω′2 −ω2
dω′.
(5.273)
The Lorentz relation (5.272) expresses the imaginary part Im[n(ω)] of the index
of refraction in terms of the imaginary part of the forward scattering amplitude
f (ω)
Im[n(ω)] = 2π(c/ω)2NIm[f (ω)].
(5.274)
And the optical theoremrelates Im[f (ω)] to the total cross-section
σtot = 4π
|k| Im[f (ω)] = 4πc
ω Im[f (ω)].
(5.275)
207

COMPLEX-VARIABLE THEORY
Thus we have Im[n(ω)] = cNσtot/(2ω), and by the Lorentz relation (5.272)
Re[n(ω)] = 1 + 2π(c/ω)2NRe[f (ω)]. Insertion of these formulas into the
Kramers–Kronig integral (5.273) gives a dispersion relation for the real part
of the forward scattering amplitude f (ω) in terms of the total cross-section
Re[f (ω)] =
ω2
2π2c P
 ∞
0
σtot(ω′)
ω′2 −ω2 dω′.
(5.276)
5.21 Phase and group velocities
Suppose A(x, t) is the amplitude
A(x, t) =

ei(p·x−Et)/¯h A(p) d3p =

ei(k·x−ωt) B(k) d3k
(5.277)
where B(k) = ¯h3A(¯hk) varies slowly compared to the phase exp[i(k · x −ωt)].
The phase velocity vp is the linear relation x = vp t between x and t that keeps
the phase φ = p · x −Et constant as a function of the time
0 = p · dx −E dt = (p · vp −E) dt
⇐⇒
vp = E
p ˆp = ω
k
ˆk,
(5.278)
in which p = |p|, and k = |k|. For light in the vacuum, vp = c = (ω/k) ˆk.
The group velocity vg is the linear relation x = vg t between x and t that
maximizes the amplitude A(x, t) by keeping the phase φ = p · x −Et constant
as a function of the momentum p
∇p(px −Et) = x −∇pE(p) t = 0
(5.279)
at the maximum of A(p). This condition of stationary phase gives the group
velocity as
vg = ∇pE(p) = ∇kω(k).
(5.280)
If E = p2/(2m), then vg = p/m.
When light traverses a medium with a complex index of refraction n(k), the
wave vector k becomes complex, and its (positive) imaginary part represents
the scattering of photons in the forward direction, typically by the electrons
of the medium. For simplicity, we’ll consider the propagation of light through
a medium in one dimension, that of the forward direction of the beam. Then
the (real) frequency ω(k) and the (complex) wave-number k are related by k =
n(k) ω(k)/c, and the phase velocity of the light is
vp =
ω
Re(k) =
c
Re(n(k)).
(5.281)
208

5.21 PHASE AND GROUP VELOCITIES
If we regard the index of refraction as a function of the frequency ω, instead
of the wave-number k, then by differentiating the real part of the relation
ωn(ω) = ck with respect to ω, we ﬁnd
nr(ω) + ω dnr(ω)
dω
= cdkr
dω ,
(5.282)
in which the subscript r means real part. Thus the group velocity (5.280) of the
light is
vg = dω
dkr
=
c
nr(ω) + ω dnr/dω.
(5.283)
Optical physicists call the denominator the group index of refraction
ng(ω) = nr(ω) + ω dnr(ω)
dω
(5.284)
so that as in the expression (5.281) for the phase velocity vp = c/nr(ω), the
group velocity is vg = c/ng(ω).
In some media, the derivative dnr/dω is large and positive, and the group
velocity vg of light there can be much less than c (Steinberg et al., 1993; Wang
and Zhang, 1995) – as slow as 17 m/s (Hau et al., 1999). This effect is called slow
light. In certain other media, the derivative dn/dω is so negative that the group
index of refraction ng(ω) is less than unity, and in them the group velocity vg
exceeds c ! This effect is called fast light. In some media, the derivative dnr/dω
is so negative that dnr/dω < −nr(ω)/ω, and then ng(ω) is not only less than
unity but also less than zero. In such a medium, the group velocity vg of light is
negative! This effect is called backwards light.
Sommerfeld and Brillouin (Brillouin, 1960, ch. II & III) anticipated fast light
and concluded that it would not violate special relativity as long as the signal
velocity – deﬁned as the speed of the front of a square pulse – remained less than
c. Fast light does not violate special relativity (Stenner et al., 2003; Brunner
et al., 2004) (Léon Brillouin, 1889–1969; Arnold Sommerfeld, 1868–1951).
Slow, fast, and backwards light can occur when the frequency ω of the light
is near a peak or resonance in the total cross-section σtot for the scattering of
light by the atoms of the medium. To see why, recall that the index of refraction
n(ω) is related to the forward scattering amplitude f (ω) and the density N of
scatterers by the formula (5.272)
n(ω) = 1 + 2πc2
ω2 Nf (ω)
(5.285)
and that the real part of the forward scattering amplitude is given by the
Kramers–Kronig integral (5.276) of the total cross-section
Re(f (ω)) =
ω2
2π2c P
 ∞
0
σtot(ω′) dω′
ω′2 −ω2 .
(5.286)
209

COMPLEX-VARIABLE THEORY
So the real part of the index of refraction is
nr(ω) = 1 + cN
π P
 ∞
0
σtot(ω′) dω′
ω′2 −ω2 .
(5.287)
If the amplitude for forward scattering is of the Breit–Wigner form
f (ω) = f0
/2
ω0 −ω −i/2
(5.288)
then by (5.285) the real part of the index of refraction is
nr(ω) = 1 +
πc2Nf0(ω0 −ω)
ω2 
(ω −ω0)2 + 2/4

(5.289)
and by (5.283) the group velocity is
vg = c

1 + πc2Nf0 ω0
ω2

(ω −ω0)2 −2/4


(ω −ω0)2 + 2/4
2
−1
.
(5.290)
This group velocity vg is less than c whenever (ω −ω0)2 > 2/4. But we get fast
light vg > c, if (ω −ω0)2 < 2/4, and even backwards light, vg < 0, if ω ≈ω0
with 4πc2Nf0/ω0 ≫1. Robert W. Boyd’s papers explain how to make slow
and fast light (Bigelow et al., 2003) and backwards light (Gehring et al., 2006).
We can use the principal-part identity (5.224) to subtract
0 = cN
π P
 ∞
0
σtot(ω)
ω′2 −ω2 dω′
(5.291)
from the Kramers–Kronig integral (5.287) so as to write the index of refraction
in the regularized form
nr(ω) = 1 + cN
π
 ∞
0
σtot(ω′) −σtot(ω)
ω′2 −ω2
dω′,
(5.292)
which we can differentiate and use in the group-velocity formula (5.283)
vg(ω) = c

1 + cN
π P
 ∞
0

σtot(ω′) −σtot(ω)

(ω′2 + ω2)
(ω′2 −ω2)2
dω′
−1
.
(5.293)
5.22 The method of steepest descent
Suppose we want to approximate for big x > 0 the integral
I(x) =
 b
a
dz h(z) exp(xf (z)),
(5.294)
210

5.22 THE METHOD OF STEEPEST DESCENT
in which the functions h(z) and f (z) are analytic in a simply connected region
that includes the points a and b in its interior. The value of the integral I(x) is
independent of the contour between the endpoints a and b. In the limit x →∞,
the integral I(x) is dominated by the exponential. So the key factor is the real
part u of f = u + iv. But since f (z) is analytic, its real and imaginary parts u(z)
and v(z) are harmonic functions which have no minima or maxima, only saddle
points (5.52).
For simplicity, we’ll assume that the real part u(z) of f (z) has only one saddle
point between the points a and b. (If it has more than one, then we must repeat
the computation that follows.) If w is the saddle point, then ux = uy = 0, which
by the Cauchy–Riemann equations (5.48) implies that vx = vy = 0. Thus the
derivative of the function f also vanishes at the saddle point f ′(w) = 0, and so
near w we may approximate f (z) as
f (z) ≈f (w) + 1
2(z −w)2f
′′(w).
(5.295)
Let’s write the second derivative as f
′′(w) = ρ eiφ and choose our contour
through the saddle point w to be a straight line z = w + y eiθ with θ ﬁxed
for z near w. As we vary y along this line, we want
(z −w)2f
′′(w) = y2 ρ e2iθ eiφ < 0
(5.296)
so we keep 2θ + φ = π so that near z = w
f (z) ≈f (w) −1
2 ρ y2.
(5.297)
Since z = w + y eiθ, its differential is dz = eiθ dy, and the integral I(x) is
I(x) ≈
 ∞
−∞
h(w) exp

x
#
f (w) + 1
2(z −w)2f
′′(w)
$%
dz
(5.298)
= h(w) eiθ exf (w)
 ∞
−∞
exp

−1
2xρy2
dy = h(w) eiθ exf (w)
(
2π
xρ .
Moving the phase eiθ inside the square-root
I(x) ≈h(w) exf (w)
(
2π
xρ e−2iθ
(5.299)
and using f
′′(w) = ρ eiφ and 2θ + φ = π to show that
ρ e−2iθ = ρ eiφ−iπ = −ρ eiφ = −f
′′(w)
(5.300)
we get our formula for the saddle-point integral (5.294)
I(x) ≈

2π
−xf
′′(w)
1/2
h(w) exf (w).
(5.301)
211

COMPLEX-VARIABLE THEORY
If there are n saddle points wj for j = 1, . . . , n, then the integral I(x) is the sum
I(x) ≈
N

j=1

2π
−xf
′′(wj)
1/2
h(wj) exf (wj).
(5.302)
5.23 The Abel–Plana formula and the Casimir effect
This section is optional on a ﬁrst reading.
Suppose the function f (z) is analytic and bounded for n1 ≤Re z ≤n2. Let
C+ and C−be two contours that respectively run counterclockwise along the
rectangles with vertices n1, n2, n2 + i∞, n1 + i∞and n1, n2, n2 −i∞, n1 −i∞
indented with tiny semicircles and quarter-circles so as to avoid the integers
z = n1, n1 + 1, n1 + 2, ..., n2 while keeping Imz > 0 in the upper rectangle and
Imz < 0 in the lower one (and n1 < Re z < n2). Then the contour integrals
I± =

C±
f (z)
e∓2πiz −1 dz = 0
(5.303)
vanish by Cauchy’s theorem (5.22) since the poles of the integrand lie outside
the indented rectangles.
The absolute value of the exponential exp(−2πiz) is arbitrarily large on the
top of the upper rectangle C+ where Imz = ∞, and so that leg of the con-
tour integral I+ vanishes. Similarly, the bottom leg of the contour integral I−
vanishes. Thus we can separate the difference I+ −I−into a term Tx due to
the integrals near the x-axis between n1 and n2, a term T1 involving integrals
between n1 and n1 ± i∞, and a term T2 involving integrals between n2 and
n2 ± i∞, that is, 0 = I+ −I−= Tx + T1 + T2.
The term Tx = Ix + S consists of the integrals Ix along the segments of the
x-axis from n1 to n2 and a sum S over the tiny integrals along the semicircles
and quarter-circles that avoid the integers from n1 to n2. Elementary algebra
simpliﬁes the integral Ix to
Ix =
 n2
n1
f (x)
#
1
e−2πix −1 +
1
e+2πix −1
$
dx = −
 n2
n1
f (x) dx.
(5.304)
The sum S is over the semicircles that avoid n1 + 1, . . . , n2 −1 and over the
quarter-circles that avoid n1 and n2. For any integer n1 < n < n2, the integral
along the semicircle of Cn+ minus that along the semicircle of Cn−, both around
n, contributes to S the quantity
Sn =

SCn+
f (z)
e−2πiz −1 dz −

SCn−
f (z)
e2πiz −1 dz
=

SCn+
f (z)
e−2πi(z−n) −1 dz −

SCn−
f (z)
e2πi(z−n) −1 dz
(5.305)
212

5.23 THE ABEL–PLANA FORMULA AND THE CASIMIR EFFECT
since exp(±2πin) = 1. The ﬁrst integral is clockwise in the upper half-plane,
the second clockwise in the lower half-plane. So if we make both integrals
counterclockwise, inserting minus signs, we ﬁnd as the radii of these semicircles
shrink to zero
Sn =
)
f (z)
2πi(z −n)dz = f (n).
(5.306)
One may show (exercise 5.39) that the quarter-circles around n1 and n2
contribute (f (n1) + f (n2))/2 to the sum S. Thus the term Tx is
Tx = 1
2f (n1) +
n2−1

n=n1+1
f (n) + 1
2f (n2) −
 n2
n1
f (x) dx.
(5.307)
Since exp(−2πin1) = 1, the difference between the integrals along the
imaginary axes above and below n1 is (exercise 5.40)
T1 =
 n1
n1+i∞
f (z)
e−2πiz −1 dz −
 n1−i∞
n1
f (z)
e2πiz −1 dz
(5.308)
= −i
 ∞
0
f (n1 + iy) −f (n1 −iy)
e2πy −1
dy.
(5.309)
Similarly, the difference between the integrals along the imaginary axes above
and below n2 is (exercise 5.41)
T2 =
 n2+i∞
n2
f (z)
e−2πiz −1 dz −
 n2
n2−i∞
f (z)
e2πiz −1 dz
(5.310)
= i
 ∞
0
f (n2 + iy) −f (n2 −iy)
e2πy −1
dy.
(5.311)
Since I+ −I−= Tx + T1 + T2 = 0, we can use (5.307) and (5.309–5.311) to
build the Abel–Plana formula (Whittaker and Watson, 1927, p. 145)
1
2f (n1) +
n2−1

n=n1+1
f (n) + 1
2f (n2) −
 n2
n1
f (x) dx
= i
 ∞
0
f (n1 + iy) −f (n1 −iy) −f (n2 + iy) + f (n2 −iy)
e2πy −1
dy
(5.312)
(Niels Abel, 1802–1829; Giovanni Plana, 1781–1864).
In particular, if f (z) = z, the integral over y vanishes, and the Abel–Plana
formula (5.312) gives
1
2n1 +
n2−1

n=n1+1
n + 1
2n2 =
 n2
n1
x dx,
(5.313)
which is an example of the trapezoidal rule.
213

COMPLEX-VARIABLE THEORY
Example 5.37 (The Casimir effect)
The Abel–Plana formula provides one of
the clearer formulations of the Casimir effect. We will assume that the hamilto-
nian for the electromagnetic ﬁeld in empty space is a sum over two polarizations
and an integral over all momenta of a symmetric product
H0 =
2

s=1

¯h ω(k) 1
2

a†
s(k)as(k) + as(k)a†
s(k)

d3k
(5.314)
of the annihilation and creation operators as(k) and a†
s(k), which satisfy the
commutation relations
[as(k), a†
s′(k′)] = δss′ δ(k −k′) and [as(k), as′(k′)] = 0 = [a†
s(k), a†
s′(k′)]. (5.315)
The vacuum state |0⟩has no photons, and so on it as(k)|0⟩= 0 (and ⟨0|a†
s(k) =
0). But because the operators in H0 are symmetrically ordered, the energy E0 of
the vacuum as given by (5.314) is not zero; instead it is quarticly divergent
E0 = ⟨0|H0|0⟩=
2

s=1

¯h ω(k) 1
2δ(0) d3k = V

¯h ω(k) d3k
(2π)3 ,
(5.316)
in which we used the delta function formula
δ(k −k′) =

e±i(k−k′)·x d3x
(2π)3
(5.317)
to identify δ(0) as the volume V of empty space divided by (2π)3. Since the pho-
ton has no mass, its (angular) frequency ω(k) is c|k|, and so the energy density
E0/V is
E0
V = ¯hc

k3 dk
2π2 = ¯hc K4
8π2 =
¯hc
8π2
1
d4 ,
(5.318)
in which we cut off the integral at some short distance d = K−1 below which the
hamiltonian (5.314) and the commutation relations (5.315) are no longer valid.
But the energy density of empty space is
ρc = 3H2
0/8πG ≈
¯hc
8π2
1
(2.8 × 10−5 m)4 ,
(5.319)
which corresponds to a distance scale d of 28 micrometers. Since quantum elec-
trodynamics works well down to about 10−18 m, this distance scale is too big by
thirteen orders of magnitude.
If the Universe were inside an enormous, perfectly conducting, metal cube of
side L, then the tangential electric and normal magnetic ﬁelds would vanish on
the surface of the cube Et(r, t) = 0 = Bn(r, t). The available wave-numbers of the
electromagnetic ﬁeld inside the cube then would be kn = 2π(n1, n2, n3)/L, and
the energy density would be
214

5.23 THE ABEL–PLANA FORMULA AND THE CASIMIR EFFECT
E0
V = 2π ¯hc
L4

n

n2.
(5.320)
The Casimir effect exploits the difference between the continuous (5.318) and
discrete (5.320) energy densities for the case of two metal plates of area A
separated by a short distance ℓ≪
√
A.
If the plates are good conductors, then at low frequencies the boundary con-
ditions Et(r, t) = 0 = Bn(r, t) hold, and the tangential electric and normal
magnetic ﬁelds vanish on the surfaces of the metal plates. At high frequen-
cies, above the plasma frequency ωp of the metal, these boundary conditions
fail because the relative electric permittivity of the metal
ϵ(ω) ≈1 −
ω2
p
ω2

1 −
i
ωτ

(5.321)
has a positive real part. Here τ is the mean time between electron collisions.
The modes that satisfy the low-frequency boundary conditions Et(r, t) = 0 =
Bn(r, t) are (Bordag et al., 2009, p. 30)
ω(k⊥, n) ≡c
&
k2
⊥+
πn
ℓ
2
(5.322)
where n · k⊥= 0. The difference between the zero-point energies of these modes
and those of the continuous modes in the absence of the two plates per unit area
would be
E(ℓ)
A
= π ¯hc
ℓ
 ∞
0
k⊥dk⊥
2π
⎡
⎣
∞

n=0
(
ℓ2k2
⊥
π2
+ n2 −
 ∞
0
(
ℓ2k2
⊥
π2
+ x2 dx −ℓk⊥
2π
⎤
⎦
(5.323)
if the boundary conditions held at all frequencies. With p = ℓk⊥/π, we will
represent the failure of these boundary conditions at the plasma frequency ωp
by means of a cutoff function like c(n) = (1 + n/np)−4 where np = ωpℓ/πc. In
terms of such a cutoff function, the energy difference per unit area is
E(ℓ)
A
= π2¯hc
2ℓ3
 ∞
0
p dp
 ∞

n=0
c(n)

p2 + n2 −
 ∞
0
c(x)

p2 + x2 dx −p
2

.
(5.324)
Since c(n) falls off as (np/n)4 for n ≫np, we may neglect terms in the sum and
integral beyond some integer M that is much larger than np
E(ℓ)
A
= π2 ¯hc
2ℓ3
 ∞
0
p dp
 M

n=0
c(n)

p2 + n2 −
 M
0
c(x)

p2 + x2 dx −p
2

.
(5.325)
The function
f (z) = c(z)

p2 + z2 =

p2 + z2
(1 + z/np)4
(5.326)
215

COMPLEX-VARIABLE THEORY
is analytic in the right half-plane Re z = x > 0 (exercise 5.42) and tends to zero
limx→∞|f (x + iy)| →0 as Re z = x →∞. So we can apply the Abel–Plana
formula (5.312) with n1 = 0 and n2 = M to the term in the square brackets in
(5.325) and get
E(ℓ)
A
= π2 ¯hc
2ℓ3
 ∞
0
p dp
c(M)
2

p2 + M2
+i
 ∞
0
#
c(iy)

p2 + (ϵ + iy)2 −c(−iy)

p2 + (ϵ −iy)2
−c(M + iy)

p2 + (M + iy)2
+c(M −iy)

p2 + (M −iy)2
$
dy
e2πy −1
%
,
(5.327)
in which the inﬁnitesimal ϵ reminds us that the contour lies inside the right half-
plane.
We now take advantage of the properties of the cutoff function c(z). Since
M
≫
np, we can neglect the term c(M)

p2 + M2/2. The denominator
exp(2πy) −1 also allows us to neglect the terms ∓c(M ∓iy)

p2 + (M ∓iy)2.
We are left with
E(ℓ)
A
= π2¯hc
2ℓ3
 ∞
0
p dp
× i
 ∞
0
#
c(iy)

p2 + (ϵ + iy)2 −c(−iy)

p2 + (ϵ −iy)2
$
dy
e2πy −1. (5.328)
Since the y integration involves the factor 1/(exp(2πy) −1), we can neglect the
detailed behavior of the cutoff functions c(iy) and c(−iy) for y > np where they
differ appreciably from unity. The energy now is
E(ℓ)
A
= π2 ¯hc
2ℓ3
 ∞
0
p dp
 ∞
0
i

p2 + (ϵ + iy)2 −

p2 + (ϵ −iy)2
e2πy −1
dy.
(5.329)
When y < p, the square-roots with the ϵs cancel. But for y > p, they are

p2 −y2 ± 2iϵy = ±i

y2 −p2.
(5.330)
Their difference is 2i

y2 −p2, and so E(ℓ) is
E(ℓ)
A
= π2 ¯hc
2ℓ3
 ∞
0
p dp
 ∞
0
−2

y2 −p2 θ(y −p)
e2πy −1
dy,
(5.331)
in which the Heaviside step function θ(x) ≡(x + |x|)/(2|x|) keeps y > p
E(ℓ)
A
= −π2¯hc
ℓ3
 y
0
p dp
 ∞
0

y2 −p2
e2πy −1 dy.
(5.332)
216

5.24 APPLICATIONS TO STRING THEORY
The p-integration is elementary, and so the energy difference is
E(ℓ)
A
= −π2¯hc
3ℓ3
 ∞
0
y3 dy
e2πy −1 = −π2 ¯hc
3ℓ3
B2
8 = −π2 ¯hc
720 ℓ3 ,
(5.333)
in which B2 is the second Bernoulli number (4.109). The pressure pushing the
plates together then is
p = −1
A
∂E(ℓ)
∂ℓ
= −π2¯hc
240 ℓ4 ,
(5.334)
a result due to Casimir (Hendrik Brugt Gerhard Casimir, 1909–2000).
Although the Casimir effect is very attractive because of its direct connec-
tion with the symmetric ordering of the creation and annihilation operators
in the hamiltonian (5.314), the reader should keep in mind that neutral atoms
are mutually attractive, which is why most gases are diatomic, and that Lifshitz
explained the effect in terms of the mutual attraction of the atoms in the metal
plates (Lifshitz, 1956; Milonni and Shih, 1992) (Evgeny Mikhailovich Lifshitz,
1915–1985).
5.24 Applications to string theory
This section is optional on a ﬁrst reading.
String theory may or may not have anything to do with physics, but it does
provide many amusing applications of complex-variable theory. The coordi-
nates σ and τ of the world sheet of a string form a complex variable z = e2(τ−iσ).
The product of two operators u(z) and v(w) often has poles in z −w as z →w
but is well deﬁned if z and w are radially ordered
R {u(z)v(w)} ≡u(z) v(w) θ(|z| −|w|) + v(w) u(z) θ(|w| −|z|),
(5.335)
in which θ(x) = (x + |x|)/2|x| is the step function. Since the modulus of z =
e2(τ−iσ) depends only upon τ, radial order is time order in τz and τw.
The modes Ln of the principal component of the energy–momentum tensor
T(z) are deﬁned by its Laurent series
T(z) =
∞

n=−∞
Ln
zn+2
(5.336)
and the inverse relation
Ln =
1
2πi
)
zn+1 T(z) dz.
(5.337)
Thus the commutator of two modes involves two loop integrals
[Lm, Ln] =
# 1
2πi
)
zm+1 T(z) dz, 1
2πi
)
wn+1 T(w) dw
$
,
(5.338)
217

COMPLEX-VARIABLE THEORY
<
>
<
<
w
U(z) V(w)
|z| > |w|
V(w) U(z)
|w| > |z|
Radial order
Figure 5.10
The two counterclockwise circles about the origin preserve radial order
when z is near w by veering slightly to |z| > |w| for the product T(z)T(w) and to
|w| > |z| for the product T(w)T(z).
which we may deform as long as we cross no poles. Let’s hold w ﬁxed and
deform the z loop so as to keep the Ts radially ordered when z is near w as
in Fig. 5.10. The operator-product expansion of the radially ordered product
R{T(z)T(w)} is
R{T(z)T(w)} =
c/2
(z −w)4 +
2
(z −w)2 T(w) +
1
z −wT′(w) + · · · ,
(5.339)
in which the prime means derivative, c is a constant, and the dots denote terms
that are analytic in z and w. The commutator introduces a minus sign that
cancels most of the two contour integrals and converts what remains into an
integral along a tiny circle Cw about the point w as in Fig. 5.10
[Lm, Ln] =
) dw
2πiwn+1
)
Cw
dz
2πizm+1
#
c/2
(z −w)4 + 2T(w)
(z −w)2 + T′(w)
z −w
$
. (5.340)
After doing the z-integral, which is left as a homework exercise (5.43), one may
use the Laurent series (5.336) for T(w) to do the w-integral, which one may
choose to be along a tiny circle about w = 0, and so ﬁnd the commutator
218

EXERCISES
[Lm, Ln] = (m −n) Lm+n + c
12 m(m2 −1) δm+n,0
(5.341)
of the Virasoro algebra.
Exercises
5.1
Compute the two limits (5.6) and (5.7) of example 5.2 but for the function
f (x, y) = x2 −y2 + 2ixy. Do the limits now agree? Explain.
5.2
Show that if f (z) is analytic in a disk, then the integral of f (z) around a tiny
triangle of side ϵ ≪1 inside the disk is zero to order ϵ2.
5.3
Derive the two integral representations (5.46) for Bessel’s functions Jn(t) of the
ﬁrst kind from the integral formula (5.45). Hint: think of the integral (5.45)
as running from −π to π.
5.4
Do the integral
)
C
dz
z2 −1
in which the contour C is counterclockwise about the circle |z| = 2.
5.5
The function f (z) = 1/z is analytic in the region |z| > 0. Compute the integral
of f (z) counterclockwise along the unit circle z = eiθ for 0 ≤θ ≤2π. The
contour lies entirely within the domain of analyticity of the function f (z).
Did you get zero? Why? If not, why not?
5.6
Let P(z) be the polynomial
P(z) = (z −a1)(z −a2)(z −a3)
(5.342)
with roots a1, a2, and a3. Let R be the maximum of the three moduli |ak|. (a)
If the three roots are all different, evaluate the integral
I =
)
C
dz
P(z)
(5.343)
along the counterclockwise contour z = 2Reiθ for 0 ≤θ ≤2π. (b) Same
exercise, but for a1 = a2 ̸= a3.
5.7
Compute the integral of the function f (z) = eaz/(z2 −3z + 2) along the
counterclockwise contour C□that follows the perimeter of a square of side
6 centered at the origin. That is, ﬁnd
I =
)
C□
eaz
z2 −3z + 2 dz.
(5.344)
5.8
Use Cauchy’s integral formula (5.36) and Rodrigues’s expression (5.37) for
Legendre’s polynomial Pn(x) to derive Schlaeﬂi’s formula (5.38).
5.9
Use Schlaeﬂi’s formula (5.38) for the Legendre polynomials and Cauchy’s
integral formula (5.32) to compute the value of Pn(−1).
5.10 Evaluate the counterclockwise integral around the unit circle |z| = 1
)
3 sinh2 2z −4 cosh3 z dz
z .
(5.345)
219

COMPLEX-VARIABLE THEORY
5.11 Evaluate the counterclockwise integral around the circle |z| = 2
)
z3
z4 −1 dz.
(5.346)
5.12 Evaluate the contour integral of the function f (z) = sin wz/(z −5)3 along the
curve z = 6 + 4(cos t + i sin t).
5.13 Evaluate the contour integral of the function f (z) = sin wz/(z −5)3 along the
curve z = −6 + 4(cos t + i sin t).
5.14 Is the function f (x, y) = x2 + iy2 analytic?
5.15 Is the function f (x, y) = x3 −3xy2 + 3ix2y −iy3 analytic? Is the function
x3 −3xy2 harmonic? Does it have a minimum or a maximum? If so, what
are they?
5.16 Is the function f (x, y) = x2 + y2 + i(x2 + y2) analytic? Is x2 + y2 a harmonic
function? What is its minimum, if it has one?
5.17 Derive the ﬁrst three nonzero terms of the Laurent series for f (z) = 1/(ez −1)
about z = 0.
5.18 Assume that a function g(z) is meromorphic in R and has a Laurent series
(5.97) about a point w ∈R. Show that as z →w, the ratio g′(z)/g(z)
becomes (5.95).
5.19 Find the poles and residues of the functions 1/ sin z and 1/ cos z.
5.20 Derive the integral formula (5.122) from (5.121).
5.21 Show that if Re w < 0, then for arbitrary complex z
 ∞
−∞
ew(x+z)2 dx =
& π
−w.
(5.347)
5.22 Use a ghost contour to evaluate the integral
 ∞
−∞
x sin x
x2 + a2 dx.
Show your work; do not just quote the result of a commercial math program.
5.23 For a > 0 and b2 −4ac < 0, use a ghost contour to do the integral
 ∞
−∞
dx
ax2 + bx + c.
(5.348)
5.24 Show that
 ∞
0
cos ax e−x2 dx = 1
2
√π e−a2/4.
(5.349)
5.25 Show that
 ∞
−∞
dx
1 + x4 = π
√
2
.
(5.350)
5.26 Evaluate the integral
 ∞
0
cos x
1 + x4 dx.
(5.351)
220

EXERCISES
5.27 Show that the Yukawa Green’s function (5.151) reproduces the Yukawa
potential (5.141) when n = 3. Use K1/2(x) = √π/2x e−x (9.99).
5.28 Derive the two explicit formulas (5.188) and (5.189) for the square-root of a
complex number.
5.29 What is (−i)i? What is the most general value of this expression?
5.30 Use the indeﬁnite integral (5.223) to derive the principal-part formula (5.224).
5.31 The Bessel function Jn(x) is given by the integral
Jn(x) =
1
2πi
)
C
e(x/2)(z+1/z) dz
zn+1
(5.352)
along a counterclockwise contour about the origin. Find the generating func-
tion for these Bessel functions, that is, the function G(x, z) whose Laurent
series has the Jn(x)s as coefﬁcients
G(x, z) =
∞

n=−∞
Jn(x) zn.
(5.353)
5.32 Show that the Heaviside function θ(y) = (y+|y|)/(2|y|) is given by the integral
θ(y) =
1
2πi
 ∞
−∞
eiyx
dx
x −iϵ ,
(5.354)
in which ϵ is an inﬁnitesimal positive number.
5.33 Show that the integral of exp(ik)/k along the contour from k = L to k =
L + iH and then to k = −L + iH and then down to k = −L vanishes in the
double limit L →∞and H →∞.
5.34 Use a ghost contour and a cut to evaluate the integral
I =
 1
−1
dx
(x2 + 1)

1 −x2
(5.355)
by imitating example 5.30. Be careful when picking up the poles at z = ±i. If
necessary, use the explicit square-root formulas (5.188) and (5.189).
5.35 Redo the previous exercise (5.34) by deﬁning the square-roots so that the cuts
run from −∞to −1 and from 1 to ∞. Take advantage of the evenness of the
integrand and integrate on a contour that is slightly above the whole real axis.
Then add a ghost contour around the upper half-plane.
5.36 Show that if u is even and v is odd, then the Hilbert transforms (5.265) imply
(5.267).
5.37 Show why the principal-part identity (5.224) lets one write the Kramers–
Kronig integral (5.287) for the index of refraction in the regularized form
(5.292).
5.38 Use the formula (5.283) for the group velocity and the regularized expression
(5.292) for the real part of the index of refraction nr(ω) to derive a formula
(5.293) for the group velocity.
5.39 Show that the quarter-circles of the Abel–Plana contours C± contribute
1
2(f (n1) + f (n2)) to the sum S in the formula Tx = Ix + S.
221

COMPLEX-VARIABLE THEORY
5.40 Derive the integral formula (5.309) from (5.308).
5.41 Derive the integral formula (5.311) from (5.310).
5.42 Show that the function (5.326) is analytic in the RHP Re z > 0.
5.43 (a) Perform the z-integral in equation (5.340). (b) Use the result of part (a) to
ﬁnd the commutator [Lm, Ln] of the Virasoro algebra. Hint: use the Laurent
series (5.336).
5.44 Assume that ϵ(z) is analytic in a disk that contains a tiny circular contour Cw
about the point w as in Fig 5.10. Do the contour integral
)
Cw
ϵ(z)
#
c/2
(z −w)4 + 2T(w)
(z −w)2 + T′(w)
z −w
$ dz
2πi
(5.356)
and express your result in terms of ϵ(w), T(w), and their derivatives.
222

6
Differential equations
6.1 Ordinary linear differential equations
There are many kinds of differential equation – linear and nonlinear, ordinary
and partial, homogeneous and inhomogeneous. Any way of correctly solving
any of them is ﬁne. We start our overview with some deﬁnitions.
An operator of the form
L =
n

m=0
hm(x) dm
dxm
(6.1)
is an nth-order, ordinary, linear differential operator. It is nth order because the
highest derivative is dn/dxn. It is ordinary because all the derivatives are with
respect to the same independent variable x. It is linear because derivatives are
linear operators
L [a1f1(x) + a2f2(x)] = a1 L f1(x) + a2 L f2(x).
(6.2)
If all the hm(x) in L are constants, independent of x, then L is an nth-order,
ordinary, linear differential operator with constant coefﬁcients.
Example 6.1 (Second-order linear differential operators)
The operator
L = −d2
dx2 + k2
(6.3)
is a second-order, linear differential operator with constant coefﬁcients. The
second-order linear differential operator
L = −d
dx

p(x) d
dx

+ q(x)
(6.4)
is in self-adjoint form (section 6.27).
223

DIFFERENTIAL EQUATIONS
The differential equation L f (x) = 0 is homogeneous because each of its terms
is linear in f or one of its derivatives f (m) – there is no term that is not propor-
tional to f or one of its derivatives. The equation Lf (x) = s(x) is inhomogeneous
because of the source term s(x).
If a differential equation is linear and homogeneous, then we can add solu-
tions. If f1(x) and f2(x) are two solutions of the same linear homogeneous
differential equation
L f1(x) = 0
and
L f2(x) = 0
(6.5)
then any linear combination of these solutions f (x) = a1f1(x) + a2f2(x) with
constant coefﬁcients a1 and a2 also is a solution since
L f (x) = L [a1f1(x) + a2f2(x)] = a1 L f1(x) + a2 L f2(x) = 0.
(6.6)
This additivity of solutions often makes it possible to ﬁnd general solutions of
linear homogeneous differential equations.
Example 6.2 (Sines and cosines)
Two solutions of the second-order, linear,
homogeneous, ordinary differential equation (ODE)

d2
dx2 + k2

f (x) = 0
(6.7)
are sin kx and cos kx, and the most general solution is the linear combination
f (x) = a1 sin kx + a2 cos kx.
The functions y1(x), ..., yn(x) are linearly independent if the only numbers
k1, ..., kn for which the linear combination vanishes for all x
k1 y1(x) + k2 y2(x) + · · · + kn yn(x) = 0
(6.8)
are k1 = · · · = kn = 0. Otherwise they are linearly dependent.
Suppose that an nth-order homogeneous, linear, ordinary differential equa-
tion L f (x) = 0 has n linearly independent solutions fj(x), and that all other
solutions to this ODE are linear combinations of these n solutions. Then these n
solutions are complete in the space of solutions of this equation and form a basis
for this space. The general solution to L f (x) = 0 is then a linear combination of
the fjs with n arbitrary constant coefﬁcients
f (x) =
n

j=1
ajfj(x).
(6.9)
224

6.2 LINEAR PARTIAL DIFFERENTIAL EQUATIONS
With a source term s(x), the differential equation L f (x) = 0 becomes an
inhomogeneous linear ordinary differential equation
L fi(x) = s(x).
(6.10)
If fi1(x) and fi2(x) are any two solutions of this inhomogeneous differential
equation, then their difference fi1(x) −fi2(x) is a solution of the associated
homogeneous equation L f (x) = 0
L [fi1(x) −fi2(x)] = L fi1(x) −L fi2(x) = s(x) −s(x) = 0.
(6.11)
Thus this difference must be given by the general solution (6.9) of the homoge-
neous equation for some constants aj
fi1(x) −fi2(x) =
N

j=1
ajfj(x).
(6.12)
It follows therefore that every solution fi1(x) of the inhomogeneous differential
equation (6.10) is the sum of a particular solution fi2(x) of that equation and
some solution (6.9) of the associated homogeneous equation L f = 0
fi1(x) = fi2(x) +
N

j=1
ajfj(x).
(6.13)
In other words, the general solution of a linear inhomogeneous equation is a par-
ticular solution of that inhomogeneous equation plus the general solution of the
associated homogeneous equation.
A nonlinear differential equation is one in which a power f n(x) of the
unknown function or of one of its derivatives

f (k)(x)
n other than n = 1 or
n = 0 appears or in which the unknown function f appears in some other
nonlinear way. For instance, the equations
−f ′′(x) = f 3(x),

f ′(x)
2 = f (x),
and
f ′(x) = e−f (x)
(6.14)
are nonlinear differential equations. We can’t add two solutions of a nonlin-
ear equation and expect to get a third solution. Nonlinear equations are much
harder to solve.
6.2 Linear partial differential equations
An equation of the form
L f (x) =
n1,...,nk

m1,...,mk=0
gm1,...,mk(x)
∂m1+···+mk
∂xm1
1 . . . ∂xmk
k
f (x) = 0
(6.15)
225

DIFFERENTIAL EQUATIONS
in which x stands for x1, . . . , xk is a linear partial differential equation of order
n = n1 + · · · + nk in the k variables x1, . . . , xk. (A partial differential equation
is a whole differential equation that has partial derivatives.)
Linear combinations of solutions of a linear homogeneous partial differen-
tial equation also are solutions of the equation. So if f1 and f2 are solutions of
L f = 0, and a1 and a2 are constants, then f = a1f1 + a2f2 is a solution since
L f = a1 L f1 + a2 L f2 = 0. Additivity of solutions is a property of all linear
homogeneous differential equations, whether ordinary or partial.
The general solution f (x) = f (x1, . . . , xk) of a linear homogeneous partial
differential equation (6.15) is a sum f (x) = 
j ajfj(x) over a complete set of
solutions fj(x) of the equation with arbitrary coefﬁcients aj. A linear partial
differential equation L fi(x) = s(x) with a source term s(x) = s(x1, . . . , xk) is an
inhomogeneous linear partial differential equation because of the added source
term.
Just as with ordinary differential equations, the difference fi1 −fi2 of two
solutions of the inhomogeneous linear partial differential equation L fi = s is a
solution of the associated homogeneous equation L f = 0 (6.15)
L [fi1(x) −fi2(x)] = s(x) −s(x) = 0.
(6.16)
So we can expand this difference in terms of the complete set of solutions fj of
the inhomogeneous linear partial differential equation L f = 0
fi1(x) −fi2(x) =

j
ajfj(x).
(6.17)
Thus the general solution of the inhomogeneous linear partial differential equa-
tion L f = s is the sum of a particular solution fi2 of L f = s and the general
solution 
j aj fj of the associated homogeneous equation (6.15)
fi1(x) = fi2(x) +

j
ajfj(x).
(6.18)
6.3 Notation for derivatives
One often uses primes or dots to denote derivatives as in
f ′ = df
dx
or
f ′′ = d2f
dx2
and
˙f = df
dt
or
¨f = d2f
dt2 .
For higher or partial derivatives, one sometimes uses superscripts
f (k) = dkf
dxk
and
f (k,ℓ) = ∂k+ℓf
∂xk∂yℓ
(6.19)
226

6.3 NOTATION FOR DERIVATIVES
or subscripted letters, sometimes preceded by commas
fx = f,x = ∂f
∂x
and
fxyy = f,xyy =
∂3f
∂x∂y2
(6.20)
or subscripted indices, sometimes preceded by commas
f,k = ∂kf = ∂f
∂xk
and
f,kℓ= ∂k∂ℓf =
∂2f
∂xk∂xℓ
(6.21)
where the independent variables are x = x1, . . . , xn.
In special relativity, one writes the time and space coordinates ct and x as x0,
x1, x2, and x3 or as the 4-vector (x0, x). To form the invariant inner product
px ≡x · x −p0x0 = x · x −Et as paxa with a summed from 0 to 3, one attaches
a minus sign to the time components of 4-vectors with lowered indices so that
p0 = −p0 and x0 = −x0. The derivatives ∂af and ∂af are
∂af = ∂f
∂xa
and
∂af = ∂f
∂xa
.
(6.22)
In rectangular coordinates, the gradient ∇of a scalar f is
∇f = (fx, fy, fz) = (f,x, f,y, f,z) = (∂xf , ∂yf , ∂zf ) =
∂f
∂x, ∂f
∂y, ∂f
∂z

and the divergence of a vector v = (vx, vy, vz) is the scalar
∇· v = vx,x + vy,y + vz,z = ∂xvx + ∂yvy + ∂zvz = ∂vx
∂x + ∂vy
∂y + ∂vz
∂z .
(6.23)
Physicists sometimes write the Laplacian ∇· ∇f as ∇2f or as △f .
Example 6.3 (Laplace’s equation)
The equation for the electrostatic potential
in empty space is Laplace’s equation
L φ(x, y, z) = ∇· ∇φ(x, y, z) =

∂2
∂x2 + ∂2
∂y2 + ∂2
∂z2

φ(x, y, z) = 0.
(6.24)
It is a second-order linear homogeneous partial differential equation.
Example 6.4 (Poisson’s equation)
Poisson’s equation for the electrostatic poten-
tial φ is
−△φ(x, y, z) ≡−

∂2
∂x2 + ∂2
∂y2 + ∂2
∂z2

φ(x, y, z) = ρ(x, y, z)
ϵ0
,
(6.25)
in which ρ is the charge density and ϵ0 is the electric constant. It is a second-
order linear inhomogeneous partial differential equation.
227

DIFFERENTIAL EQUATIONS
6.4 Gradient, divergence, and curl
In cylindrical coordinates, the change dp in a physical point p due to changes
dρ, dφ, and dz in its coordinates is dp = ˆρ dρ+ρ ˆφ dφ+ˆz dz. In spherical coordi-
nates, the change is dp = ˆr dr+r ˆθ dθ +r sin θ ˆφ dφ. In general orthogonal coor-
dinates, the change dp in a physical point p due to changes dui in its coordinates
is dp = h1 ˆe1 du1 +h2 ˆe2 du2 +h3 ˆe3 du3 where the basis vectors are orthonormal
ˆek·ˆeℓ= δkℓ. In cylindrical coordinates, the scale factors are hρ = 1, hφ = ρ, and
hz = 1, while in spherical coordinates they are hr = 1, hθ = r, and hφ = r sin θ.
The dot-product of the gradient ∇f of a scalar function f with the change dp
in the point is the change df ∇f · dp which is df = ∂1f du1 + ∂2f du2 + ∂3f du3.
So the gradient in orthogonal coordinates is
∇f = ˆe1
h1
∂f
∂u1
+ ˆe2
h2
∂f
∂u2
+ ˆe3
h3
∂f
∂u3
.
(6.26)
Thus the gradient in cylindrical coordinates is
∇f = ˆρ ∂f
∂ρ +
ˆφ
ρ
∂f
∂φ + ˆz ∂f
∂z
(6.27)
and in spherical coordinates it is
∇f = ˆr ∂f
∂r +
ˆθ
r
∂f
∂θ +
ˆφ
r sin θ
∂f
∂φ .
(6.28)
The divergence of a vector v at the center of a tiny cube is the surface integral
dS of v over the boundary ∂dV of the cube divided by its tiny volume dV =
h1h2h3 du1du2du3. The surface integral dS is the sum of the differences of the
integrals of v1, v2, and v3 over the cube’s three pairs of opposite faces dS =
[∂(v1h2h3)/∂u1 + ∂(v2h1h3)/∂u2 + ∂(v3h1h2)/∂u3] du1du2du3. So the divergence
∇· v is the ratio dS/dV, which is
∇· v =
1
h1h2h3
#∂(v1h2h3)
∂u1
+ ∂(v2h1h3)
∂u2
+ ∂(v3h1h2)
∂u3
$
.
(6.29)
Thus the divergence in cylindrical coordinates is
∇· v = 1
ρ
#∂(vρρ)
∂ρ
+ ∂vφ
∂φ + ∂(vzρ)
∂z
$
= 1
ρ
∂(ρvρ)
∂ρ
+ 1
ρ
∂vφ
∂φ + ∂vz
∂z
(6.30)
and in spherical coordinates it is
∇· v = 1
r2
∂(vr r2)
∂r
+
1
r sin θ
∂(vθ sin θ)
∂θ
+
1
r sin θ
∂vφ
∂φ .
(6.31)
By assembling a large number of tiny cubes, one may create a ﬁnite volume
V. The integral of the divergence ∇· v over the tiny volumes dV of the tiny
cubes that make up the volume V is the sum of the surface integrals dS over
228

6.4 GRADIENT, DIVERGENCE, AND CURL
the faces of these tiny cubes. The integrals over the interior faces cancel leaving
just the surface integral over the boundary ∂V of the ﬁnite volume V. Thus we
arrive at Stokes’s theorem 
V
∇· v dV =

∂V
v · dS.
(6.32)
The Laplacian is the divergence (6.29) of the gradient (6.26). So in general
orthogonal coordinates it is
△f = ∇· ∇f =
1
h1h2h3
 3

k=1
∂
∂uk

h1h2h3
h2
k
∂f
∂uk

.
(6.33)
Thus in cylindrical coordinates, the Laplacian is
△f = 1
ρ

∂
∂ρ

ρ ∂f
∂ρ

+ 1
ρ
∂2f
∂φ2 + ρ ∂2f
∂z2

= 1
ρ
∂
∂ρ

ρ ∂f
∂ρ

+ 1
ρ2
∂2f
∂φ2 + ∂2f
∂z2
(6.34)
and in spherical coordinates it is
△f =
1
r2 sin θ
# ∂
∂r

r2 sin θ ∂f
∂r

+ ∂
∂θ

sin θ ∂f
∂θ

+ ∂
∂φ

1
sin θ
∂f
∂φ
$
= 1
r2
∂
∂r

r2 ∂f
∂r

+
1
r2 sin θ
∂
∂θ

sin θ ∂f
∂θ

+
1
r2 sin2 θ
∂2f
∂φ2 .
(6.35)
The area dS of a tiny square dS whose sides are the tiny perpendicular vectors
hiˆeidui and hjˆejduj (no sum) is their cross-product
dS = hiˆeidui × hjˆejduj = ˆek hihj duiduj,
(6.36)
in which the perpendicular unit vectors ˆei, ˆej, and ˆek obey the right-hand rule.
The dot-product of this area with the curl of a vector v, which is (∇× v) · dS =
(∇× v)k hihj duiduj, is the line integral dL of v along the boundary ∂dS of the
square
(∇× v)k hihjduiduj =

∂i(hjvj) −∂j(hivi)

duiduj.
(6.37)
Thus the kth component of the curl is
(∇× v)k =
1
hihj
∂(hjvj)
∂ui
−∂(hivi)
∂uj

(no sum).
(6.38)
In terms of the Levi-Civita symbol ϵijk, which is totally antisymmetric with
ϵ123 = 1, the curl is
∇× v = 1
2
3

i,j,k=1
ϵijk
ˆek
hihj
#∂(hjvj)
∂ui
−∂(hivi)
∂uj
$
=
3

i,j,k=1
ϵijk
ˆek
hihj
∂(hjvj)
∂ui
,
(6.39)
in which the sums over i, j, and k run from 1 to 3. In rectangular coordinates,
each scale factor hi = 1, and the ith component of ∇× v is
229

DIFFERENTIAL EQUATIONS
(∇× v)i =
3

j, k=1
ϵijk
∂vk
∂xj
=
3

j, k=1
ϵijk∂jvk
(6.40)
or (∇× v)i = ϵijk∂jvk if we sum implicitly over j and k.
We can write the curl as a determinant
∇× v =
1
h1h2h3

h1ˆe1
h2ˆe2
h3ˆe3
∂1
∂2
∂3
h1v1
h2v2
h3v3

.
(6.41)
Thus in cylindrical coordinates, where h1 = 1, h2 = ρ, and h3 = 1, the curl is
∇× v = 1
ρ

ˆρ
ρ ˆφ
ˆz
∂ρ
∂φ
∂z
vρ
ρvφ
vz

(6.42)
and in spherical coordinates, where h1 = 1, h2 = r, and h3 = r sin θ, it is
∇× v =
1
r2 sin θ

ˆr
r ˆθ
r sin θ ˆφ
∂r
∂θ
∂φ
vr
r vθ
r sin θ vφ

.
(6.43)
By assembling a large number of tiny squares, one may create an arbitrary
ﬁnite surface S. The surface integral of the curl ∇× v over the tiny squares dS
that make up the surface S is the sum of the line integrals dL over the sides of
these tiny squares. The line integrals over the interior sides cancel leaving just
the line integral along the boundary ∂S of the ﬁnite surface S. Thus we arrive
at Stokes’s theorem

S
∇× v · dS =

∂S
v · dℓ.
(6.44)
6.5 Separable partial differential equations
A linear partial differential equation (PDE) is separable if it can be decom-
posed into ordinary differential equations (ODEs). One then ﬁnds solutions to
the ODEs and thus to the original PDE. The general solution to the PDE is
then a sum over all of its linearly independent solutions with arbitrary coefﬁ-
cients. Sometimes the separability of a differential operator or of a differential
equation depends upon the choice of coordinates.
Example 6.5 (The Helmholtz equation in two dimensions)
In several coordi-
nate systems, one can convert Helmholtz’s linear homogeneous partial differen-
tial equation −∇·∇f (x) = −△f (x) = k2f (x) into ordinary differential equations
by writing the function f (x) as a product of functions of a single variable.
In two dimensions and in rectangular coordinates (x, y), the function f (x, y) =
X(x) Y(y) is a solution of the Helmholtz equation as long as X and Y satisfy
230

6.5 SEPARABLE PARTIAL DIFFERENTIAL EQUATIONS
−X′′
a (x) = a2Xa(x) and −Y′′
b (y) = b2Yb(y) with a2 + b2 = k2. One sets Xa(x) =
α sin ax + β cos ax with a similar equation for Yb(y). Any linear combination of
the functions Xa(x) Yb(y) with a2 + b2 = k2 will be a solution of Helmholtz’s
equation −△f = k2f .
The z-independent part of (6.34) is the Laplacian in polar coordinates
∇· ∇f = △f = ∂2f
∂ρ2 + 1
ρ
∂f
∂ρ + 1
ρ2
∂2f
∂φ2 ,
(6.45)
in which Helmholtz’s equation −△f = k2f also is separable. We let f (ρ, φ) =
P(ρ) (φ) and get P′′ + P′/ρ + P′′/ρ2 = −k2P . Multiplying both sides
by ρ2/P , we have
ρ2 P′′
P + ρ P′
P + ρ2k2 = −′′
 = n2,
(6.46)
in which the ﬁrst three terms are functions of ρ, the fourth term −′′/ is a
function of φ, and the last term n2 is a constant. The constant n must be an
integer if n(φ) = a sin(n φ) + b cos(n φ) is to be single valued on the interval
[0, 2π]. The function Pk,n(ρ) = Jn(kρ) satisﬁes
ρ2P′′
k,n + ρP′
k,n + ρ2k2Pk,n = n2Pk,n,
(6.47)
because the Bessel function of the ﬁrst kind Jn(x) satisﬁes
x2J′′
n + xJ′
n + x2Jn = n2Jn,
(6.48)
which is Bessel’s equation (9.4) (Friedrich Bessel, 1784–1846). So the product
fk,n(ρ, φ) = Pk,n(ρ) n(φ) is a solution to Helmholtz’s equation −△f = k2f , as
is any linear combination of such products for different ns.
Example 6.6 (The Helmholtz equation in three dimensions)
In three dimen-
sions and in rectangular coordinates r = (x, y, z), the function f (x, y, z) =
X(x)Y(y)Z(z) is a solution of the ODE −△f = k2f as long as X, Y, and Z
satisfy −X′′
a = a2Xa, −Y′′
b = b2Yb, and −Z′′
c = c2Zc with a2 + b2 + c2 = k2. We
set Xa(x) = α sin ax + β cos ax and so forth. Arbitrary linear combinations of
the products Xa Yb Zc also are solutions of Helmholtz’s equation −△f = k2f
as long as a2 + b2 + c2 = k2.
In cylindrical coordinates (ρ, φ, z), the Laplacian (6.34) is
∇· ∇f = △f = 1
ρ
#
ρ f,ρ

,ρ + 1
ρ f,φφ + ρ f,zz
$
(6.49)
and so if we substitute f (ρ, φ, z) = P(ρ) (φ) Z(z) into Helmholtz’s equation
−△f = α2f and multiply both sides by −ρ2/P  Z, then we get
ρ2
f △f = ρ2P′′ + ρP′
P
+ ′′
 + ρ2 Z′′
Z = −α2ρ2.
(6.50)
231

DIFFERENTIAL EQUATIONS
If we set Zk(z) = ekz, then this equation becomes (6.46) with k2 replaced by
α2 + k2. Its solution then is
f (ρ, φ, z) = Jn(

α2 + k2ρ) einφ ekz,
(6.51)
in which n must be an integer if the solution is to apply to the full range of
φ from 0 to 2π. The case in which α = 0 corresponds to Laplace’s equation
with solution f (ρ, φ, z) = Jn(kρ)einφekz. We could have required Z to satisfy
Z′′ = −k2Z. The solution (6.51) then would be
f (ρ, φ, z) = Jn(

α2 −k2 ρ) einφ eikz.
(6.52)
But if α2 −k2 < 0, we write this solution in terms of the modiﬁed Bessel function
In(x) = i−nJn(ix) (section 9.3) as
f (ρ, φ, z) = In(

k2 −α2 ρ) einφ eikz.
(6.53)
In spherical coordinates, the Laplacian (6.35) is
△f = 1
r2
∂
∂r

r2 ∂f
∂r

+
1
r2 sin θ
∂
∂θ

sin θ ∂f
∂θ

+
1
r2 sin2 θ
∂2f
∂φ2 .
(6.54)
If we set f (r, θ, φ) = R(r) (θ) m(φ) where m = eimφ and multiply both sides
of the Helmholtz equation −△f = k2f by −r2/R, then we get

r2R′′
R
+

sin θ ′′
sin θ 
−
m2
sin2 θ
= −k2.
(6.55)
The ﬁrst term is a function of r, the next two terms are functions of θ, and the
last term is a constant. So we set the r-dependent terms equal to a constant
ℓ(ℓ+ 1) −k2 and the θ-dependent terms equal to −ℓ(ℓ+ 1), and we require the
associated Legendre function ℓ,m(θ) to satisfy (8.91)

sin θ ′
ℓ,m
′ / sin θ +

ℓ(ℓ+ 1) −m2/ sin2 θ

ℓ,m = 0.
(6.56)
If (φ) = eimφ is to be single valued for 0 ≤φ ≤2π, then the parameter m must
be an integer. As we’ll see in chapter 8, the constant ℓalso must be an integer
with −ℓ≤m ≤ℓif ℓ,m(θ) is to be single valued and ﬁnite for 0 ≤θ ≤π. The
product f = R   then will obey Helmholtz’s equation −△f = k2f if the radial
function Rk,ℓ(r) = jℓ(kr) satisﬁes
r2R′′
k,ℓ+ 2rR′
k,ℓ+

k2r2 −ℓ(ℓ+ 1)

Rk,ℓ= 0,
(6.57)
which it does because the spherical Bessel function jℓ(x) obeys Bessel’s equation
(9.63)
x2 j′′
ℓ+ 2x j′
ℓ+ [x2 −ℓ(ℓ+ 1)] jℓ= 0.
(6.58)
In three dimensions, Helmholtz’s equation separates in 11 standard coordinate
systems (Morse and Feshbach, 1953, pp. 655–664).
232

6.6 WAVE EQUATIONS
6.6 Wave equations
You can easily solve some of the linear homogeneous partial differential
equations of electrodynamics (Exercise 6.6) and quantum ﬁeld theory.
Example 6.7 (The Klein–Gordon equation)
In Minkowski space, the analog of
the Laplacian in natural units (¯h = c = 1) is (summing over a from 0 to 3)
2 = ∂a∂a = △−
∂2
∂x02 = △−∂2
∂t2
(6.59)
and the Klein–Gordon wave equation is

2 −m2
A(x) =

△−∂2
∂t2 −m2

A(x) = 0.
(6.60)
If we set A(x) = B(px) where px = paxa = p · x −p0x0, then the kth partial
derivative of A is pk times the ﬁrst derivative of B
∂
∂xk A(x) =
∂
∂xk B(px) = pkB′(px)
(6.61)
and so the Klein–Gordon equation (6.60) becomes

2 −m2
A = (p2 −(p0)2)B′′ = p2B′′ −m2B = 0,
(6.62)
in which p2 = p2 −(p0)2. Thus if B(p · x) = exp(ip · x) so that B′′ = −B,
and if the energy–momentum 4-vector (p0, p) satisﬁes p2 + m2 = 0, then A(x)
will satisfy the Klein–Gordon equation. The condition p2 + m2 = 0 relates the
energy p0 =

p2 + m2 to the momentum p for a particle of mass m.
Example 6.8 (Field of a spinless boson)
The quantum ﬁeld
φ(x) =

d3p

2p0(2π)3

a(p)eipx + a†(p)e−ipx
(6.63)
describes spinless bosons of mass m. It satisﬁes the Klein–Gordon equation

2 −m2
φ(x) = 0 because p0 =

p2 + m2. The operators a(p) and a†(p)
respectively represent the annihilation and creation of the bosons and obey the
commutation relations
[a(p), a†(p′)] = δ3(p −p′) and [a(p), a(p′)] = [a†(p), a†(p′)] = 0
(6.64)
in units with ¯h = c = 1. These relations make the ﬁeld φ(x) and its time
derivative ˙φ(y) satisfy the canonical equal-time commutation relations
[φ(x, t), ˙φ(y, t)] = i δ3(x −y) and [φ(x, t), φ(y, t)] = [ ˙φ(x, t), ˙φ(y, t)] = 0, (6.65)
in which the dot means time derivative.
233

DIFFERENTIAL EQUATIONS
Example 6.9 (Field of the photon)
The electromagnetic ﬁeld has four compo-
nents, but in the Coulomb or radiation gauge ∇· A(x) = 0, the component A0
is a function of the charge density, and the vector potential A in the absence
of charges and currents satisﬁes the wave equation 2A(x) = 0 for a spin-one
massless particle. We write it as
A(x) =
2

s=1

d3p

2p0(2π)3

e(p, s) a(p, s) eipx + e∗(p, s) a†(p, s) e−ipx
,
(6.66)
in which the sum is over the two possible polarizations s. The energy p0 is equal
to the modulus |p| of the momentum because the photon is massless, p2 = 0.
The dot-product of the polarization vectors e(p, s) with the momentum vanishes
p·e(p, s) = 0 so as to respect the gauge condition ∇· A(x) = 0. The annihilation
and creation operators obey the commutation relations
[a(p, s), a†(p′, s′)] = δ3(p −p′) δs,s′
[a(p, s), a(p′, s′)] = [a†(p, s), a†(p′, s′)] = 0
(6.67)
but the commutation relations of the vector potential A(x) involve the transverse
delta function

Ai(t, x), ˙Aj(t, y)

= iδijδ(3)(x −y) + i
∂2
∂xi∂xj
1
4π|x −y|
= i

eik·(x−y)

δij −kikj
k2
 d3k
(2π)3
(6.68)
because of the Coulomb-gauge condition ∇· A(x) = 0.
Example 6.10 (Dirac’s equation)
Fields χb(x) that describe particles of spin
one-half have four components, b = 1, . . . , 4. In the absence of interactions,
they satisfy the Dirac equation

γa
bc∂a + mδbc

χc(x) = 0,
(6.69)
in which repeated indices are summed over – b, c from 1 to 4 and a from 0 to 3.
In matrix notation, the Dirac equation is

γa∂a + m

χ(x) = 0.
(6.70)
The four Dirac gamma matrices are deﬁned by the 16 rules
{γa, γb} ≡γaγb + γbγa = 2ηab,
(6.71)
in which η is the 4 × 4 diagonal matrix η00 = η00 = −1 and ηbc = ηbc = δbc for
b, c = 1, 2, or 3.
If φ(x) is a 4-component ﬁeld that satisﬁes the Klein–Gordon equation (2 −
m2)φ = 0, then the ﬁeld χ(x) = (γb∂b −m)φ(x) satisﬁes (exercise 6.7) the Dirac
equation (6.70)
234

6.8 SEPARABLE FIRST-ORDER DIFFERENTIAL EQUATIONS

γa∂a + m

χ(x) =

γa∂a + m

(γb∂b −m)φ(x)
=

γaγb∂a∂b −m2
φ(x)
=

1
2

{γa, γb} + [γa, γb]

∂a∂b −m2
φ(x)
=

ηab∂a∂b −m2
φ(x) = (2 −m2)φ(x) = 0.
The simplest Dirac ﬁeld is the Majorana ﬁeld
χb(x) =

d3p
(2π)3/2

s

ub(p, s) a(p, s)eipx + vb(p, s) a†(p, s)e−ipx
(6.72)
in which p0 =

p2 + m2, s labels the two spin states, and the operators a and a†
obey the anticommutation relations
{a(p, s), a†(p′, s′)} ≡a(p, s) a†(p′, s′) + a†(p′, s′) a(p, s) = δss′ δ(p −p′),
{a(p, s), a(p′, s′)} = {a†(p, s), a†(p′, s′)} = 0.
(6.73)
It describes a neutral particle of mass m.
If two Majorana ﬁelds χ1 and χ2 represent particles of the same mass, then
one may combine them into one Dirac ﬁeld
ψ(x) =
1
√
2
[χ1(x) + iχ2(x)] ,
(6.74)
which describes a charged particle such as a quark or a lepton.
6.7 First-order differential equations
The equation
dy
dx = f (x, y) = −P(x, y)
Q(x, y)
(6.75)
or system
P(x, y) dx + Q(x, y) dy = 0
(6.76)
is a ﬁrst-order ordinary differential equation.
6.8 Separable ﬁrst-order differential equations
If in a ﬁrst-order ordinary differential equation like (6.76) one can separate the
dependent variable y from the independent variable x
F(x) dx + G(y) dy = 0
(6.77)
then the equation (6.76) is separable and (6.77) is separated.
235

DIFFERENTIAL EQUATIONS
Once the variables are separated, one can integrate and so obtain an
equation, called the general integral
0 =
 x
x0
F(x′) dx′ +
 y
y0
G(y′) dy′
(6.78)
relating y to x and providing a solution y(x) of the differential equation.
Example 6.11 (Zipf’s law)
In 1913, Auerbach noticed that many quantities are
distributed as (Gell-Mann, 1994, pp. 92–100)
dn = −a dx
xk+1
(6.79)
an ODE that is separable and separated. For k ̸= 0, we may integrate this to
n + c = a/kxk or
x =

a
k(n + c)
1/k
(6.80)
in which c is a constant.
The case k = 1 occurs frequently x = a/(n + c) and is called Zipf’s law. With
c = 0, it applies approximately to the populations of cities: if the largest city
(n = 1) has population x, then the populations of the second, third, and fourth
cities (n = 2, 3, 4) will be x/2, x/3, and x/4.
Again with c = 0, Zipf’s law applies to the occurrence of numbers x in a
table of some sort. Since x = a/n, the rank n of the number x is approximately
n = a/x. So the number of numbers that occur with ﬁrst digit d and, say, 4
trailing digits will be
n(d0000) −n(d9999) = a

1
d0000 −
1
d9999

= a

9999
d0000 × d9999

≈a

104
d(d + 1) 108

=
a 10−4
d(d + 1).
(6.81)
The ratio of the number of numbers with ﬁrst digit d to the number with ﬁrst
digit d′ is then d′(d′ +1)/d(d +1). For example, the ﬁrst digit is more likely to be
1 than 9 by a factor of 45. The German government uses such formulas to catch
tax evaders.
Example 6.12 (The logistic equation)
dy
dt = ay

1 −y
Y

(6.82)
is separable and separated. It describes a wide range of phenomena whose evolu-
tion with time t is sigmoidal such as (Gell-Mann, 2008) the cumulative number
of casualties in a war, the cumulative number of deaths in London’s great plague,
236

6.8 SEPARABLE FIRST-ORDER DIFFERENTIAL EQUATIONS
and the cumulative number of papers in an academic’s career. It also describes
the effect y on an animal of a given dose t of a drug.
With f = y/Y, the logistic equation (6.82) is ˙f = af (1 −f ) or
a dt =
df
f (1 −f ) = df
f +
df
1 −f ,
(6.83)
which we may integrate to a(t −th) = ln [f /(1 −f )]. Taking the exponential of
both sides, we ﬁnd exp[a(t −th)] = f /(1 −f ), which we can solve for f
f (t) =
ea(t−th)
1 + ea(t−th) .
(6.84)
The sigmoidal shape of f (t) is like a smoothed Heaviside function.
Example 6.13 (Lattice QCD)
In lattice ﬁeld theory, the beta function
β(g) ≡−dg
d ln a
(6.85)
tells us how we must adjust the coupling constant g in order to keep the physical
predictions of the theory constant as we vary the lattice spacing a. In quantum
chromodynamics β(g) = −β0 g3 −β1 g5 + · · · where
β0 =
1
(4π)2

11 −2
3 nf

and β1 =
1
(4π)4

102 −10 nf −8
3nf

,
(6.86)
in which nf is the number of light quark ﬂavors. Combining the deﬁnition (6.85)
of the β-function with the ﬁrst term of its expansion β(g) = −β0 g3 for small g,
one arrives at the differential equation
dg
d ln a = β0 g3,
(6.87)
which one may integrate

d ln a = ln a + c =

dg
β0g3 = −
1
2β0g2
(6.88)
to ﬁnd
 a(g) = e−1/2β0g2,
(6.89)
in which  is a constant of integration. As g approaches 0, which is an essen-
tial singularity (section 5.11), the lattice spacing a(g) goes to zero very fast (as
long as nf ≤16). The inverse of this relation g(a) ≈1/

β0 ln(1/a22) shows
that the coupling constant g(a) slowly goes to zero as the lattice spacing (or
shortest wave-length) a goes to zero. The strength of the interaction shrinks
logarithmically as the energy 1/a increases in this lattice version of asymptotic
freedom.
237

DIFFERENTIAL EQUATIONS
6.9 Hidden separability
As long as each of the functions P(x, y) and Q(x, y) in the ODE
P(x, y)dx + Q(x, y)dy = U(x)V(y)dx + R(x)S(y)dy = 0
(6.90)
can be factored P(x, y) = U(x)V(y) and Q(x, y) = R(x)S(y) into the prod-
uct of a function of x times a function of y, then the ODE is separable.
Following Ince (1956), we divide the ODE by R(x)V(y), separate the variables
U(x)
R(x) dx + S(y)
V(y) dy = 0,
(6.91)
and integrate
 U(x)
R(x) dx +

S(y)
V(y) dy = C,
(6.92)
in which C is a constant of integration.
Example 6.14 (Hidden separability)
We separate the variables in
x(y2 −1) dx −y(x2 −1) dy = 0
(6.93)
by dividing by (y2 −1)(x2 −1) so as to get
x
x2 −1 dx −
y
y2 −1 dy = 0.
(6.94)
Integrating, we ﬁnd ln(x2 −1) −ln(y2 −1) = −ln C or C (x2 −1) = y2 −1,
which we solve for y(x) =

1 + C(x2 −1).
6.10 Exact ﬁrst-order differential equations
The differential equation
P(x, y) dx + Q(x, y) dy = 0
(6.95)
is exact if its left-hand side is the differential of some function φ(x, y)
P dx + Q dy = dφ = φx dx + φy dy.
(6.96)
We’ll have more to say about the exterior derivative d in section 12.2.
The criteria of exactness are
P(x, y) = ∂φ(x, y)
∂x
≡φx(x, y) and Q(x, y) = ∂φ(x, y)
∂y
≡φy(x, y).
(6.97)
238

6.10 EXACT FIRST-ORDER DIFFERENTIAL EQUATIONS
Thus, if the ODE (6.95) is exact, then
Py(x, y) = φyx(x, y) = φxy(x, y) = Qx(x, y),
(6.98)
which is called the condition of integrability. This condition implies that the
ODE (6.95) is exact and integrable, as we’ll see in section 6.11.
A ﬁrst-order ODE that is separable and separated
P(x)dx + Q(y)dy = 0
(6.99)
is exact because
Py = 0 = Qx.
(6.100)
But a ﬁrst-order ODE may be exact without being separable.
Example 6.15 (Boyle’s law)
At a ﬁxed temperature T, changes in the pressure
P and volume V of an ideal gas are related by
PdV + VdP = 0.
(6.101)
This ODE is exact because PdV + VdP = d(PV). Its integrated form is the
ideal-gas law
PV = NkT,
(6.102)
in which N is the number of molecules in the gas and k is Boltzmann’s constant,
k = 1.38066 × 10−23 J/K = 8.617385×10−5 eV/K.
Incidentally, a more accurate formula, proposed by van der Waals (1837–1923)
in his doctoral thesis in 1873, is

P +
N
V
2
a′


V −Nb′
= NkT,
(6.103)
in which a′ represents the mutual attraction of the molecules and has the dimen-
sions of energy times volume and b′ is the effective volume of a single molecule.
This equation was one of many signs that molecules were real particles, inde-
pendent of the imagination of chemists. Lamentably, most physicists refused
to accept the reality of molecules until 1905 when Einstein related the viscous-
friction coefﬁcient ζ and the diffusion constant D to the energy kT of a thermal
ﬂuctuation by the equation ζ D = kT, as explained in section 13.9 (Albert
Einstein, 1879–1955).
Example 6.16 (Human population growth)
If the number of people rises as the
square of the population, then ˙N = N2/b. The separated and hence exact form
of this differential equation is
dN
N2 = dt
b ,
(6.104)
239

DIFFERENTIAL EQUATIONS
which we integrate to N(t) = b/(T −t) where T is the time at which the popula-
tion becomes inﬁnite. With T = 2025 years and b = 2 × 1011 years, this formula
is a fair model of the world’s population between the years 1 and 1970. For a
more accurate account, see von Foerster et al. (1960).
6.11 The meaning of exactness
We can integrate the differentials of a ﬁrst-order ODE
P(x, y) dx + Q(x, y) dy = 0
(6.105)
along any contour C in the x-y plane, but in general we’d get a functional
φ(x, y, C, x0, y0) =
 (x,y)
(x0,y0)C
P(x′, y′) dx′ + Q(x′, y′) dy′
(6.106)
that depends upon the contour C of integration as well as upon the endpoints
(x0, y0) and (x, y).
But if the differential Pdx + Qdy is exact, then it’s the differential or exterior
derivative dφ = P(x, y) dx + Q(x, y) dy of a function φ(x, y) that depends upon
the variables x and y without any reference to a contour of integration. Thus if
Pdx + Qdy = dφ, then the contour integral (6.105) is
 (x,y)
(x0,y0)C
P(x′, y′) dx′ + Q(x′, y′) dy′ =
 (x,y)
(x0,y0)
dφ = φ(x, y) −φ(x0, y0).
(6.107)
This integral deﬁnes a function φ(x, y, x0, y0) ≡φ(x, y) −φ(x0, y0) whose dif-
ferential vanishes dφ = Pdx + Qdy = 0 according to the original differential
equation (6.105). Thus the ODE and its exactness lead to an equation
φ(x, y, x0, y0) = B
(6.108)
that we can solve for y, our solution of the ODE (6.105)
dφ(x, y, x0, y0) = P(x, y) dx + Q(x, y) dy = 0.
(6.109)
Example 6.17 (Explicit use of exactness)
We’ll now explicitly use the criteria of
exactness
P(x, y) = ∂φ(x, y)
∂x
≡φx(x, y) and Q(x, y) = ∂φ(x, y)
∂y
≡φy(x, y)
(6.110)
to integrate the general exact differential equation
P(x, y) dx + Q(x, y) dy = 0.
(6.111)
240

6.11 THE MEANING OF EXACTNESS
We use the ﬁrst criterion P = φx to integrate the condition φx = P in the x-
direction getting a known integral R(x, y) and an unknown function C(y)
φ(x, y) =

P(x, y) dx + C(y) = R(x, y) + C(y).
(6.112)
The second criterion Q = φy tells us that
Q(x, y) = φy(x, y) = Ry(x, y) + Cy(y).
(6.113)
We get C(y) by integrating its known derivative Cy = Q −Ry
C(y) =

Q(x, y) −Ry(x, y) dy + D.
(6.114)
We now put C into the formula φ = R + C, which is (6.112). Setting φ = E, a
constant, we ﬁnd an equation
φ(x, y) = R(x, y) + C(y)
= R(x, y) +

Q(x, y) −Ry(x, y) dy + D = E
(6.115)
that we can solve for y.
Example 6.18 (Using exactness)
The functions P and Q in the differential
equation
P(x, y) dx + Q(x, y) dy = ln(y2 + 1) dx + 2y(x −1)
y2 + 1
dy = 0
(6.116)
are factorized, so the ODE is separable. It’s also exact since
Py =
2y
y2 + 1 = Qx
(6.117)
and so we can apply the method just outlined. First, as in (6.112), we integrate
φx = P in the x-direction
φ(x, y) =

ln(y2 + 1) dx + C(y) = x ln(y2 + 1) + C(y).
(6.118)
Then as in (6.113), we use φy = Q
φ(x, y)y =
2xy
y2 + 1 + Cy(y) = Q(x, y) = 2y(x −1)
y2 + 1
(6.119)
to ﬁnd that Cy = −2y/(y2+1), which we integrate in the y-direction as in (6.114)
C(y) = −ln(y2 + 1) + D.
(6.120)
We now put C(y) into our formula (6.118) for φ(x, y)
φ(x, y) = x ln(y2 + 1) −ln(y2 + 1) + D
= (x −1) ln(y2 + 1) + D,
(6.121)
241

DIFFERENTIAL EQUATIONS
which we set equal to a constant
φ(x, y) = (x −1) ln(y2 + 1) + D = E
(6.122)
or more simply (x −1) ln(y2 + 1) = F. Unraveling this equation we ﬁnd
y(x) =

eF/(x−1) −1
1/2
(6.123)
as our solution to the differential equation (6.116).
6.12 Integrating factors
With great luck, one might invent an integrating factor α(x, y) that makes an
ordinary differential equation P dx + Q dy = 0 exact
α P dx + α Q dy = dφ
(6.124)
and therefore integrable. Such an integrating factor α must satisfy both
α P = φx
and
α Q = φy
(6.125)
so that
(α P)y = φxy = (α Q)x.
(6.126)
Example 6.19 (Two simple integrating factors)
The ODE ydx −xdy = 0 is not
exact, but α(x, y) = 1/x2 is an integrating factor. For after multiplying by α, we
have
−y
x2 dx + 1
xdy = 0
(6.127)
so that P = −y/x2, Q = 1/x, and
Py = −1
x2 = Qx,
(6.128)
which shows that (6.127) is exact.
Another integrating factor is α(x, y) = 1/xy, which separates the variables
dx
x = dy
y
(6.129)
so that we can integrate and get ln(y/y0) = ln(x/x0) or ln(yx0/xy0) = 0, which
implies that y = (y0/x0)x.
242

6.14 THE VIRIAL THEOREM
6.13 Homogeneous functions
A function f (x) = f (x1, . . . , xk) of k variables xi is homogeneous of degree n if
f (tx) = f (tx1, . . . , txk) = tn f (x).
(6.130)
For instance, z2 ln(x/y) is homogeneous of degree 2 because
(tz)2 ln(tx/ty) = t2 
z2 ln(x/y)

.
(6.131)
By differentiating (6.130) with respect to t, we ﬁnd
d
dtf (tx) =
k

i=1
dtxi
dt
∂f (tx)
∂txi
=
k

i=1
xi
∂f (tx)
∂txi
= ntn−1 f (x).
(6.132)
Setting t = 1, we see that a function that is homogeneous of degree n satisﬁes
k

i=1
xi
∂f (x)
∂xi
= n f (x),
(6.133)
which is one of Euler’s many theorems.
6.14 The virial theorem
Consider N particles moving nonrelativistically in a potential V(x) of 3N vari-
ables that is homogeneous of degree n. Their virial is the sum of the products of
the coordinates xi multiplied by the momenta pi
G =
3N

i=1
xi pi.
(6.134)
In terms of the kinetic energy T = (v1p1 + · · · + v3Np3N)/2, the time derivative
of the virial is
dG
dt =
3N

i=1
(vi pi + xi Fi) = 2T +
3N

i=1
xi Fi,
(6.135)
in which the time derivative of a momentum ˙pi = Fi is a component of the
force. We now form the inﬁnite time average of both sides of this equation
lim
t→∞
G(t) −G(0)
t
=
.dG
dt
/
= 2 ⟨T⟩+
0 3N

i=1
xi Fi
1
.
(6.136)
If the particles are bound by a potential V, then it is reasonable to assume that
the positions and momenta of the particles and their virial G(t) are bounded
243

DIFFERENTIAL EQUATIONS
for all times, and we will make this assumption. It follows that as t →∞, the
time average of the time derivative ˙G of the virial must vanish
0 = 2 ⟨T⟩+
0 3N

i=1
xi Fi
1
.
(6.137)
Newton’s law
Fi = −∂V(x)
∂xi
(6.138)
now implies that
2 ⟨T⟩=
0 3N

i=1
xi
∂V(x)
xi
1
.
(6.139)
If, further, the potential V(x) is a homogeneous function of degree n, then Euler’s
theorem (6.133) gives us xi∂iV = nV and the virial theorem
⟨T⟩= n
2 ⟨V(x)⟩.
(6.140)
The long-term time average of the kinetic energy of particles trapped in a homo-
geneous potential of degree n is n/2 times the long-term time average of their
potential energy.
Example 6.20 (Coulomb forces)
A 1/r gravitational or electrostatic potential
is homogeneous of degree −1, and so the virial theorem asserts that particles
bound in such wells must have long-term time averages that satisfy
⟨T⟩= −1
2 ⟨V(x)⟩.
(6.141)
In natural units (¯h = c = 1), the energy of an electron of momentum p a
distance r from a proton is E = p2/2m −e2/r in which e is the charge of the
electron. The uncertainty principle (example 3.6) gives us an approximate lower
bound on the product r p >∼1, which we will use in the form r p = 1 to estimate the
energy E of the ground state of the hydrogen atom. Using 1/r = p, we have E =
p2/2m−e2p. Differentiating, we ﬁnd the minimum of E is at 0 = p/m−e2. Thus
the kinetic energy of the ground state is T = p2/2m = me4/2 while its potential
energy is V = −e2p = −me4. Since T = −V/2, these values satisfy the virial
theorem. They give the ground-state energy as E = −me4/2 = −mc2(e2/¯hc)2 =
13.6 eV.
Example 6.21 (Harmonic forces)
Particles conﬁned in a harmonic potential
V(r) = 
k mkω2
kr2
k, which is homogeneous of degree 2, must have long-term
time averages that satisfy ⟨T⟩= ⟨V(x)⟩.
244

6.15 HOMOGENEOUS FIRST-ORDER ORDINARY DIFFERENTIAL EQUATIONS
6.15 Homogeneous ﬁrst-order ordinary differential equations
Suppose the functions P(x, y) and Q(x, y) in the ﬁrst-order ODE
P(x, y) dx + Q(x, y) dy = 0
(6.142)
are homogeneous of degree n (Ince, 1956). We change variables from x and y to
x and y(x) = xv(x) so that dy = xdv + vdx, and so
P(x, xv)dx + Q(x, xv)(xdv + vdx) = 0.
(6.143)
The homogeneity of P(x, y) and Q(x, y) implies that
xnP(1, v)dx + xnQ(1, v)(xdv + vdx) = 0.
(6.144)
Rearranging this equation, we are able to separate the variables
dx
x +
Q(1, v)
P(1, v) + vQ(1, v) dv = 0.
(6.145)
We integrate this equation
ln x +

Q(1, v)
P(1, v) + vQ(1, v) dv = C
(6.146)
and ﬁnd v(x) and so too the solution y(x) = xv(x).
Example 6.22 (Using homogeneity)
In the differential equation
(x2 −y2) dx + 2xy dy = 0
(6.147)
the coefﬁcients of the differentials P(x, y) = x2 −y2 and Q(x, y) = 2xy are
homogeneous functions of degree n = 2, so the above method applies. With
y(x) = xv(x), we have
x2(1 −v2)dx + 2x2v(vdx + xdv) = 0,
(6.148)
in which x2 cancels out, leaving (1 + v2)dx + 2vxdv = 0. Separating variables
and integrating, we ﬁnd
 dx
x +

2v dv
1 + v2 = ln C
(6.149)
or ln(1 + v2) + ln x = ln C. So (1 + v2)x = C, which leads to the general
integral x2 + y2 = Cx and so to y(x) =

Cx −x2 as the solution of the
ODE (6.147).
245

DIFFERENTIAL EQUATIONS
6.16 Linear ﬁrst-order ordinary differential equations
The general form of a linear ﬁrst-order ODE is
dy
dx + r(x)y = s(x).
(6.150)
We always can ﬁnd an integrating factor α(x) that makes
0 = α(ry −s)dx + αdy
(6.151)
exact. If P ≡α(ry −s) and Q ≡α, then the condition (6.98) for this equation
to be exact is Py = αr = Qx = αx or αx/α = r. So
d ln α
dx
= r,
(6.152)
which we integrate to
α(x) = α(x0) exp
 x
x0
r(x′)dx′

.
(6.153)
Now since αr = αx, the original equation (6.150) multiplied by this integrating
factor is
αyx + αry = αyx + αxy = (αy)x = αs.
(6.154)
Integrating, we ﬁnd
α(x)y(x) = α(x0)y(x0) +
 x
x0
α(x′)s(x′)dx′
(6.155)
so that
y(x) = α(x0)y(x0)
α(x)
+
1
α(x)
 x
x0
α(x′)s(x′)dx′,
(6.156)
in which α(x) is the exponential (6.153). More explicitly, y(x) is
y(x) = exp

−
 x
x0
r(x′)dx′
 
y(x0) +
 x
x0
exp
 x′
x0
r(x′′)dx′′

s(x′)dx′

.
(6.157)
The ﬁrst term in the square brackets multiplied by the prefactor α(x0)/α(x) is
the general solution of the homogeneous equation yx+ry = 0. The second term
in the square brackets multiplied by the prefactor α(x0)/α(x) is a particular
solution of the inhomogeneous equation yx + ry = s. Thus equation (6.157)
expresses the general solution of the inhomogeneous equation (6.150) as the
sum of a particular solution of the inhomogeneous equation and the general
solution of the associated homogeneous equation.
246

6.16 LINEAR FIRST-ORDER ORDINARY DIFFERENTIAL EQUATIONS
We were able to ﬁnd an integrating factor α because the original equation
(6.150) was linear in y. So we could set P(x, y) = r(x)y −s(x) and Q(x, y) = 1.
When P and Q are more complicated, integrating factors are harder to ﬁnd or
nonexistent.
Example 6.23 (Bodies falling in air)
The downward speed v of a mass m in a
gravitational ﬁeld of constant acceleration g is described by the inhomogeneous
ﬁrst-order ODE mvt = mg −bv, in which b represents air resistance. This equa-
tion is like (6.150) but with t instead of x as the independent variable, r = b/m,
and s = g. Thus by (6.157), its solution is
v(t) = mg
b +

v(0) −mg
b

e−bt/m.
(6.158)
The terminal speed mg/b is nearly 200 km/h for a falling man. A diving Peregrine
falcon can exceed 320 km/h; so can a falling bullet. But mice can fall down mine
shafts and run off unhurt, and insects and birds can ﬂy.
If the falling bodies are microscopic, a statistical model is appropriate. The
potential energy of a mass m at height h is V = mgh. The heights of particles at
temperature T K follow Boltzmann’s distribution (1.345)
P(h) = P(0)e−mgh/kT,
(6.159)
in which k = 1.380 6504 × 10−23 J/K = 8.617 343 × 10−5eV/K is his constant.
The probability depends exponentially upon the mass m and drops by a factor
of e with the scale height S = kT/mg, which can be a few kilometers for a small
molecule.
Example 6.24 (R-C circuit)
The capacitance C of a capacitor is the charge Q it
holds (on each plate) divided by the applied voltage V, that is, C = Q/V. The
current I through the capacitor is the time derivative of the charge I = ˙Q = C ˙V.
The voltage across a resistor of R  (Ohms) through which a current I ﬂows
is V = IR by Ohm’s law. So if a time-dependent voltage V(t) is applied to a
capacitor in series with a resistor, then V(t) = Q/C +IR. The current I therefore
obeys the ﬁrst-order differential equation
˙I + I/RC = ˙V/R
(6.160)
or (6.150) with x →t, y →I, r →1/RC, and s →˙V/R. Since r is a constant,
the integrating factor α(x) →α(t) is
α(t) = α(t0) e(t−t0)/RC.
(6.161)
Our general solution (6.157) of the linear ﬁrst-order ODE gives us the expression
I(t) = e−(t−t0)/(RC)

I(t0) +
 t
t0
e(t′−t0)/(RC) ˙V(t′)
R
dt′

(6.162)
for the current I(t).
247

DIFFERENTIAL EQUATIONS
Example 6.25 (Emission rate from ﬂuorophores)
A ﬂuorophore is a molecule
that emits light when illuminated. The frequency of the emitted photon usually
is less than that of the incident one. Consider a population of N ﬂuorophores of
which N+ are excited and can emit light and N−= N −N+ are unexcited. If the
ﬂuorophores are exposed to an illuminating photon ﬂux I, and the cross-section
for the excitation of an unexcited ﬂuorophore is σ, then the rate at which unex-
cited ﬂuorophores become excited is IσN−. The time derivative of the number
of excited ﬂuorophores is then
˙N+ = IσN−−1
τ N+ = −1
τ N+ + Iσ (N −N+) ,
(6.163)
in which 1/τ is the decay rate (also the emission rate) of the excited ﬂuorophores.
Using the shorthand a = Iσ +1/τ, we have ˙N+ = −aN+ +IσN, which we solve
using the general formula (6.157) with r = a and s = IσN
N+(t) = e−at
#
N+(0) +
 t
0
eat′I(t′)σN dt′
$
.
(6.164)
If the illumination I(t) is constant, then by doing the integral we ﬁnd
N+(t) = IσN
a

1 −e−at
+ N+(0)e−at.
(6.165)
The emission rate E = N+(t)/τ of photons from the N+(t) excited ﬂuorophores
then is
E = IσN
aτ

1 −e−at
+ N+(0)
τ
e−at,
(6.166)
which with a = Iσ + 1/1τ gives for the emission rate per ﬂuorophore
E
N =
Iσ
1 + Iστ

1 −e−(Iσ+1/τ)t
(6.167)
if no ﬂuorophores were excited at t = 0, so that N+(0) = 0.
6.17 Systems of differential equations
Actual physical problems often involve several differential equations. The
motion of n particles in three dimensions is described by 3n equations, electro-
dynamics by the four Maxwell equations (11.82 & 11.83), and the concentra-
tions of different molecular species in a cell by thousands of coupled differential
equations.
This ﬁeld is too vast to cover in these pages, but we may hint at some of its fea-
tures by considering the motion of n particles in three dimensions as described
by a lagrangian L(q, ˙q, t), in which q stands for the 3n coordinates q1, q2, ..., q3n
and ˙q for their time derivatives. The action of a motion q(t) is the time integral
of the lagrangian
248

6.17 SYSTEMS OF DIFFERENTIAL EQUATIONS
S =
 t2
t1
L(q, ˙q, t) dt.
(6.168)
If q(t) changes by a little bit δq, then the ﬁrst-order change in the action is
δS =
 t2
t1
3n

i=1
#∂L(q, ˙q, t)
∂qi
δqi(t) + ∂L(q, ˙q, t)
∂˙qi
δ˙qi(t)
$
dt.
(6.169)
The change in ˙qi is
δ dqi
dt = d(qi + δqi)
dt
−dqi
dt = d δqi
dt ,
(6.170)
the time derivative of the change δqi, so we have
δS =
 t2
t1
3n

i=1
#∂L(q, ˙q, t)
∂qi
δqi(t) + ∂L(q, ˙q, t)
∂˙qi
d δqi(t)
dt
$
dt.
(6.171)
We can integrate this by parts
δS =
 t2
t1
3n

i=1
# ∂L
∂qi
−d
dt
∂L
∂˙qi

δqi(t)
$
dt +
 3n

i=1
∂L
∂˙qi
δqi(t)
t2
t1
.
(6.172)
A classical process is one that makes the action stationary to ﬁrst order in δq(t)
for changes that vanish at the endpoints δq(t1) = 0 = δq(t2). Thus a classical
process satisﬁes Lagrange’s equations
d
dt
∂L
∂˙qi
−∂L
∂qi
= 0
for
i = 1, . . . , 3n.
(6.173)
Moreover, if the lagrangian does not depend explicitly on the time t, as is usually
the case, then the hamiltonian
H =
3n

i=1
∂L
∂˙qi
˙qi −L ≡
3n

i=1
pi ˙qi −L
(6.174)
does not change with time because its time derivative is the vanishing explicit
time dependence of the lagrangian −∂L/∂t = 0. That is
˙H =
3n

i=1
d
dt
∂L
∂˙qi
˙qi + ∂L
∂˙qi
¨qi −˙L =
3n

i=1
∂L
∂qi
˙qi + ∂L
∂˙qi
¨qi −˙L
= −∂L
∂t = 0.
(6.175)
249

DIFFERENTIAL EQUATIONS
Example 6.26 (Small oscillations)
The lagrangian
L =
3n

i=1
mi
2 ˙x2
i −V(x)
(6.176)
describes n particles of mass mi interacting through a potential U(q) that has no
explicit time dependence. By letting qi = √mi/m xi we may scale the masses to
the same value m and set V(q) = U(x), so that we have
L = m
2
3n

i=1
˙q2
i −V(q) = m
2 ˙q · ˙q −V(q),
(6.177)
which describes n particles of mass m interacting through a potential V(q). The
hamiltonian is conserved, and if it has a minimum energy H0 at q0, then its ﬁrst
derivatives there vanish. So near q0 the potential V to lowest order is a quadratic
form in the displacements ri ≡qi −qi0 from the minima, and the lagrangian,
apart from the constant V(q0), is
L ≈m
2
3n

i=1
˙r2
i −1
2
3n

j,k=1
rj rk
∂2V(q0)
∂qj∂qk
.
(6.178)
The matrix V′′ of second derivatives is real and symmetric, and so we may diag-
onalize it V′′ = OT V′′
d O by an orthogonal transformation O. The lagrangian is
diagonal in the new coordinates s = O r
L ≈1
2
3n

i=1

m ˙s2
i −V′′
di s2
i

(6.179)
and Lagrange’s equations are m ¨si = −V′′
di si. These normal modes are uncoupled
harmonic oscillators si(t) = ai cos

V′′
di/m t + bi sin

V′′
di/m t with frequencies
that are real because q0 is the minimum of the potential.
6.18 Singular points of second-order ordinary differential equations
If in the ODE y′′ = f (x, y, y′), the acceleration y′′ = f (x0, y, y′) is ﬁnite for
all ﬁnite y and y′, then x0 is a regular point of the ODE. If y′′ = f (x0, y, y′) is
inﬁnite for any ﬁnite y and y′, then x0 is a singular point of the ODE.
If a second-order ODE y′′ + P(x)y′ + Q(x)y = 0 is linear and homogeneous
and both P(x0) and Q(x0) are ﬁnite, then x0 is a regular point of the ODE. But
if P(x0) or Q(x0) or both are inﬁnite, then x0 is a singular point.
Some singular points are regular. If P(x) or Q(x) diverges as x →x0, but
both (x −x0)P(x) and (x −x0)2Q(x) remain ﬁnite as x →x0, then x0 is a
250

6.19 FROBENIUS’S SERIES SOLUTIONS
regular singular point or equivalently a nonessential singular point. But if either
(x−x0)P(x) or (x−x0)2Q(x) diverges as x →x0, then x0 is an irregular singular
point or equivalently an essential singularity.
To treat the point at inﬁnity, one sets z = 1/x. Then if (2z −P(1/z))/z2 and
Q(1/z)/z4 remain ﬁnite as z →0, the point x0 = ∞is a regular point of the
ODE. If they don’t remain ﬁnite, but (2z −P(1/z))/z and Q(1/z)/z2 do remain
ﬁnite as z →0, then x0 = ∞is a regular singular point. Otherwise the point at
inﬁnity is an irregular singular point or an essential singularity.
Example 6.27 (Legendre’s equation)
Its self-adjoint form is

1 −x2
y′′
+ ℓ(ℓ+ 1)y = 0,
(6.180)
which is (1 −x2)y′′ −2xy′ + ℓ(ℓ+ 1)y = 0 or
y′′ −
2x
1 −x2 y′ + ℓ(ℓ+ 1)
1 −x2 y = 0.
(6.181)
It has regular singular points at x = ±1 and x = ∞(exercise 6.15 ).
6.19 Frobenius’s series solutions
Frobenius showed how to ﬁnd a power-series solution of a second-order linear
homogeneous ordinary differential equation y′′ + P(x) y′ + Q(x) y = 0 at any of
its regular or regular singular points. Writing the equation in the form x2y′′ +
x p(x) y′ + q(x) y = 0, we will assume that p and q are polynomials or analytic
functions, and that x = 0 is a regular or regular singular point of the ODE so
that p(0) and q(0) are both ﬁnite.
We expand y as a power series in x about x = 0
y(x) = xr
∞

n=0
an xn,
(6.182)
in which a0 ̸= 0 is the coefﬁcient of the lowest power of x in y(x). Differentiat-
ing, we have
y′(x) =
∞

n=0
(r + n) an xr+n−1
(6.183)
and
y′′(x) =
∞

n=0
(r + n)(r + n −1) an xr+n−2.
(6.184)
251

DIFFERENTIAL EQUATIONS
When we substitute the three series (6.182–6.184) into our differential equation
x2y′′ + xp(x)y′ + q(x)y = 0, we ﬁnd
∞

n=0
[(n + r)(n + r −1) + (n + r)p(x) + q(x)] anxn+r.
(6.185)
If this equation is to be satisﬁed for all x, then the coefﬁcient of every power
of x must vanish. The lowest power of x is xr, and it occurs when n = 0 with
coefﬁcient [r(r −1 + p(0)) + q(0)] a0. Thus since a0 ̸= 0, we have
r(r −1 + p(0)) + q(0) = 0.
(6.186)
This quadratic indicial equation has two roots r1 and r2.
To analyze higher powers of x, we introduce the notation
p(x) =
∞

j=0
pjxj
and
q(x) =
∞

j=0
qjxj,
(6.187)
in which p0 = p(0) and q0 = q(0). The requirement (exercise 6.16) that the
coefﬁcient of xr+k vanishes gives us a recurrence relation
ak = −
#
1
(r + k)(r + k −1 + p0) + q0
$ k−1

j=0

(j + r)pk−j + qk−j

aj
(6.188)
that expresses ak in terms of a0, a1, ..., ak−1. When p(x) and q(x) are polyno-
mials of low degree, these equations become much simpler.
Example 6.28 (Sines and cosines)
To apply Frobenius’s method to the ODE
y′′ + ω2y = 0, we ﬁrst write it in the form x2y′′ + xp(x)y′ + q(x)y = 0, in which
p(x) = 0 and q(x) = ω2x2. So both p(0) = p0 = 0 and q(0) = q0 = 0, and the
indicial equation (6.186) is r(r −1) = 0 with roots r1 = 0 and r2 = 1.
We ﬁrst set r = r1 = 0. Since the ps and qs vanish except for q2 = ω2, the
recurrence relation (6.188) is ak = −q2 ak−2/k(k−1) = −ω2ak−2/k(k−1). Thus
a2 = −ω2a0/2, and a2n = (−1)nω2na0/(2n)!. The recurrence relation (6.188)
gives no information about a1, so to ﬁnd the simplest solution, we set a1 = 0.
The recurrence relation ak = −ω2ak−2/k(k −1) then makes all the terms a2n+1
of odd index vanish. Our solution for the ﬁrst root r1 = 0 then is
y(x) =
∞

n=0
an xn = a0
∞

n=0
(−1)n (ωx)2n
(2n)! = a0 cos ωx.
(6.189)
Similarly, the recurrence relation (6.188) for the second root r2 = 1 is ak =
−ω2ak−2/k(k + 1), so that a2n = (−1)nω2na0/(2n + 1)!, and we again set all the
252

6.20 FUCH’S THEOREM
terms of odd index equal to zero. Thus we have
y(x) = x
∞

n=0
an xn = a0
ω
∞

n=0
(−1)n (ωx)2n+1
(2n + 1)! = a0
ω sin ωx
(6.190)
as our solution for the second root r2 = 1.
Frobenius’s method sometimes shows that solutions exist only when a
parameter in the ODE assumes a special value called an eigenvalue.
Example 6.29 (Legendre’s equation)
If one rewrites Legendre’s equation
(1 −x2)y′′ −2xy′ + λy = 0 as x2y′′ + xpy′ + qy = 0, then one ﬁnds p(x) =
−2x2/(1 −x2) and q(x) = x2/(1 −x2), which are analytic but not polynomials.
In this case, it is simpler to substitute the expansions (6.182–6.184) directly into
Legendre’s equation (1 −x2)y′′ −2xy′ + λy = 0. We then ﬁnd
∞

n=0

(n + r)(n + r −1)(1 −x2)xn+r−2 −2(n + r)xn+r + λxn+r
an = 0.
The coefﬁcient of the lowest power of x is r(r−1)a0, and so the indicial equation
is r(r −1) = 0. For r = 0, we shift the index n on the term n(n −1)xn−2an to
n = j + 2 and replace n by j in the other terms:
∞

j=0

(j + 2)(j + 1) aj+2 −[j(j −1) + 2j −λ] aj

xj = 0.
(6.191)
Since the coefﬁcient of xj must vanish, we get the recursion relation
aj+2 = j(j + 1) −λ
(j + 2)(j + 1)aj,
(6.192)
which for big j says that aj+2 ≈aj. Thus the series (6.182) does not converge for
|x| ≥1 unless λ = j(j + 1) for some integer j in which case the series (6.182) is a
Legendre polynomial (chapter 8).
Frobenius’s method also allows one to expand solutions about x0 ̸= 0
y(x) = (x −x0)k
∞

n=0
an (x −x0)n.
(6.193)
6.20 Fuch’s theorem
The method of Frobenius can run amok, especially if one expands about a sin-
gular point x0. One can get only one solution or none at all. But Fuch has
253

DIFFERENTIAL EQUATIONS
shown that if one applies Frobenius’s method to a linear homogeneous second-
order ODE and expands about a regular point or a regular singular point, then
one always gets at least one power-series solution:
1 if the two roots of the indicial equation are equal, one gets only one solution;
2 if the two roots differ by a noninteger, one gets two solutions;
3 if the two roots differ by an integer, then the bigger root yields a solution.
Example 6.30 (Roots that differ by an integer)
If one applies the method
of Frobenius to Legendre’s equation as in example 6.29, then one ﬁnds (exer-
cise 6.18) that the k = 0 and k = 1 roots lead to the same solution.
6.21 Even and odd differential operators
Under the parity transformation x →−x, a typical term transforms as
xn
 d
dx
p
xk =
k!
(k −p)! xn+k−p →(−1)n+k−p
k!
(k −p)! xn+k−p
(6.194)
and so the corresponding differential operator transforms as
xn
 d
dx
p
→(−1)n−p xn
 d
dx
p
.
(6.195)
The reﬂected form of the second-order linear differential operator
L(x) = h0(x) + h1(x) d
dx + h2(x) d2
dx2
(6.196)
therefore is
L(−x) = h0(−x) −h1(−x) d
dx + h2(−x) d2
dx2 .
(6.197)
The operator L(x) is even if it is unchanged by reﬂection, that is, if h0(−x) =
h0(x), h1(−x) = −h1(x), and h2(−x) = h2(x), so that
L(−x) = L(x).
(6.198)
It is odd if it changes sign under reﬂection, that is, if h0(−x) = −h0(x), h1(−x) =
h1(x), and h2(−x) = −h2(x), so that
L(−x) = −L(x).
(6.199)
Not every differential operator L(x) is even or odd. But just as we can write
every function f (x) whose reﬂected form f (−x) is well deﬁned as the sum of
[f (x) + f (−x)]/2, which is even, and [f (x) −f (−x)]/2, which is odd,
f (x) = 1
2[f (x) + f (−x)] + 1
2[f (x) −f (−x)]
(6.200)
254

6.23 A SECOND SOLUTION
so too we can write every differential operator L(x) whose reﬂected form L(−x)
is well deﬁned as the sum of one that is even and one that is odd
L(x) = 1
2[L(x) + L(−x)] + 1
2[L(x) −L(−x)].
(6.201)
Many of the standard differential operators have h0 = 1 and are even.
If y(x) is a solution of the ODE L(x) y(x) = 0 and L(−x) is well deﬁned,
then we have L(−x) y(−x) = 0. If further L(−x) = ±L(x), then y(−x) also is
a solution L(x) y(−x) = 0. Thus if a differential operator L(x) has a deﬁnite
parity, that is, if L(x) is either even or odd, then y(−x) is a solution if y(x) is,
and solutions come in pairs y(x) ± y(−x), one even, one odd.
6.22 Wronski’s determinant
If the N functions y1(x), . . . , yN(x) are linearly dependent, then by (6.8) there is
a set of coefﬁcients k1, . . . , kN, not all zero, such that the sum
0 = k1 y1(x) + · · · + kN yN(x)
(6.202)
vanishes for all x. Differentiating i times, we get
0 = k1 y(i)
1 (x) + · · · + kN y(i)
N (x)
(6.203)
for all x. So if we use the yj and their derivatives to deﬁne the matrix
Yij(x) ≡y(i−1)
j
(x)
(6.204)
then we may express the linear dependence (6.202) and (6.203) of the func-
tions y1, . . . , yN in matrix notation as 0 = Y(x) k for some nonzero vector
k = (k1, k2, . . . , kN). Since the matrix Y(x) maps the nonzero vector k to zero,
its determinant must vanish: det(Y(x)) ≡|Y(x)| = 0. This determinant
W(x) = |Y(x)| =
y(i−1)
j
(x)

(6.205)
is called Wronski’s determinant or the wronskian. It vanishes on an interval if
and only if the functions yj(x) or their derivatives are linearly dependent on the
interval.
6.23 A second solution
If we have one solution to a second-order linear homogeneous ODE, then we
may use the wronskian to ﬁnd a second solution. Here’s how: if y1 and y2
are two linearly independent solutions of the second-order linear homogeneous
ordinary differential equation
y′′(x) + P(x) y′(x) + Q(x) y(x) = 0
(6.206)
255

DIFFERENTIAL EQUATIONS
then their wronskian does not vanish
W(x) =

y1(x)
y2(x)
y′
1(x)
y′
2(x)
 = y1(x) y′
2(x) −y2(x) y′
1(x) ̸= 0
(6.207)
except perhaps at isolated points. Its derivative
W′ = y′
1 y′
2 + y1 y′′
2 −y′
2 y′
1 −y2 y′′
1
= y1 y′′
2 −y2 y′′
1
(6.208)
must obey
W′ = −y1

P y′
2 + Q y2

+ y2

P y′
1 + Q y1

= −P

y1 y′
2 −y2 y′
1

(6.209)
or W′(x) = −P(x) W(x), which we integrate to
W(x) = W(x0) exp
#
−
 x
x0
P(x′)dx′
$
.
(6.210)
This is Abel’s formula for the wronskian (Niels Abel, 1802–1829).
Having expressed the wronskian in terms of the known function P(x), we
now use it to ﬁnd y2(x) from y1(x). We note that
W = y1 y′
2 −y2 y′
1 = y2
1
d
dx
y2
y1

.
(6.211)
So
d
dx
y2
y1

= W
y2
1
,
(6.212)
which we integrate to
y2(x) = y1(x)
 x W(x′)
y2
1(x′)
dx′ + c

.
(6.213)
Using our formula (6.210) for the wronskian, we ﬁnd as the second solution
y2(x) = y1(x)
 x
1
y2
1(x′)
exp

−
 x′
P(x′′)dx′′

dx′
(6.214)
apart from additive and multiplicative constants.
In the important special case in which P(x) = 0 the wronskian is a constant,
W′(x) = 0, and the second solution is simply
y2(x) = y1(x)
 x
dx′
y2
1(x′)
.
(6.215)
By Fuchs’s theorem, Frobenius’s expansion about a regular point or a reg-
ular singular point yields at least one solution. From this solution, we can
256

6.24 WHY NOT THREE SOLUTIONS?
use Wronski’s trick to ﬁnd a second (linearly independent) solution. So we
always get two linearly independent solutions if we expand a second-order
linear homogeneous ODE about a regular point or a regular singular point.
6.24 Why not three solutions?
We have seen that a second-order linear homogeneous ODE has two linearly
independent solutions. Why not three?
If y1, y2, and y3 were three linearly independent solutions of the second-order
linear homogeneous ODE
0 = y′′
j + P y′
j + Q yj,
(6.216)
then their third-order wronskian
W =

y1
y2
y3
y′
1
y′
2
y′
3
y′′
1
y′′
2
y′′
3

(6.217)
would not vanish except at isolated points.
But the ODE (6.216) relates the second derivatives y′′
j = −(P y′
j + Q yj) to
the y′
j and the yj, and so the third row of this third-order wronskian is a linear
combination of the ﬁrst two rows. Thus it vanishes identically
W =

y1
y2
y3
y′
1
y′
2
y′
3
−Py′
1 −Qy1
−Py′
2 −Qy2
−Py′
3 −Qy3

= 0
(6.218)
and so any three solutions of a second-order ODE (6.216) are linearly
dependent.
One may extend this argument to show that an nth-order linear homoge-
neous ODE can have at most n linearly independent solutions. To do so, we’ll
use superscript notation (6.19) in which y(n) denotes the nth derivative of y(x)
with respect to x
y(n) ≡dny
dxn .
(6.219)
Suppose there were n + 1 linearly independent solutions yj of the ODE
y(n) + P1 y(n−1) + P2 y(n−2) + · · · + Pn−1 y′ + Pn y = 0,
(6.220)
in which the Pks are functions of x. Then we could form a wronskian of order
(n+1) in which row 1 would be y1, ..., yn+1, row 2 would be the ﬁrst derivatives
y′
1, ..., y′
n+1, and row n+1 would be the nth derivatives y(n)
1 , ..., y(n)
n+1. We could
then replace each term y(n)
k in the last row by
y(n)
k
= −P1 y(n−1)
k
−P2 y(n−2)
k
−· · · −Pn−1 y′
k −Pn yk.
(6.221)
257

DIFFERENTIAL EQUATIONS
But then the last row would be a linear combination of the ﬁrst n rows, the
determinant would vanish, and the n+1 solutions would be linearly dependent.
This is why an nth-order linear homogeneous ODE can have at most n linearly
independent solutions.
6.25 Boundary conditions
Since an nth-order linear homogeneous ordinary differential equation can have
at most n linearly independent solutions, it follows that we can make a solution
unique by requiring it to satisfy n boundary conditions. We’ll see that the n
arbitrary coefﬁcients ck of the general solution
y(x) =
n

k=1
ck yk(x)
(6.222)
of the differential equation (6.220) are ﬁxed by the n boundary conditions
y(x1) = b1,
y(x2) = b2,
. . .
y(xn) = bn
(6.223)
as long as the functions yk(x) are linearly independent, which is to say, as long
as the matrix Y with entries Yjk = yk(xj) is nonsingular, that is, det Y ̸= 0.
In matrix notation, with B a vector with components bj and C a vector with
components ck, the n boundary conditions (6.223) are
y(xj) =
n

k=1
ck yk(xj) = bj
or
Y C = B.
(6.224)
Thus since det Y ̸= 0, the coefﬁcients are uniquely given by C = Y−1 B.
The boundary conditions can involve the derivatives y
(ℓj)
k (xj). One may
show (exercise 6.20) that in this case as long as the matrix Yjk = y
(ℓj)
k (xj) is
nonsingular, the n boundary conditions
y(ℓj)(xj) =
n

k=1
ck y
(ℓj)
k (xj) = bj
(6.225)
are Y C = B, and so the n coefﬁcients are uniquely C = Y−1 B.
But what if all the bj are zero? If all the boundary conditions are homoge-
neous Y C = 0, and det Y ̸= 0, then Y−1 Y C = C = 0, and the only solution
is yk(x) ≡0. So there is no solution if B = 0 and the matrix Y is nonsingular.
But if the n×n matrix Y has rank n−1, then (section 1.33) it maps a unique vec-
tor C to zero (apart from an overall factor). So if all the boundary conditions
are homogeneous, and the matrix Y has rank n −1, then the solution y = ckyk
is unique. But if the rank of Y is less than n −1, the solution is not unique.
258

6.26 A VARIATIONAL PROBLEM
Since a matrix of rank zero vanishes identically, any nonzero 2 × 2 matrix
Y must be of rank 1 or 2. Thus a second-order ODE with two homogeneous
boundary conditions has either a unique solution or none at all.
Example 6.31 (Boundary conditions and eigenvalues)
The solutions yk of the
differential equation −y′′ = k2 y are y1(x) = sin kx and y2(x) = cos kx. If
we impose the boundary conditions y(−a) = 0 and y(a) = 0, then the matrix
Yjk = yk(xj) is
Y =
 −sin ka
cos ka
sin ka
cos ka

(6.226)
with determinant det Y =
−2 sin ka cos ka = −sin 2ka. This determinant
vanishes only if ka = nπ/2 for some integer n, so if ka ̸= nπ/2, then no solu-
tion y of the differential equation −y′′ = k2 y satisﬁes the boundary conditions
y(−a) = 0 = y(a). But if ka = nπ/2, then there is a solution, and it is unique
because for even (odd) n, the ﬁrst (second) column of Y vanishes, but not the
second (ﬁrst), which implies that Y has rank 1. One may regard the condition
ka = nπ/2 either as determining the eigenvalue k2 or as telling us what interval
to use.
6.26 A variational problem
For what functions u(x) is the “energy” functional
E[u] ≡
 b
a

p(x)u′2(x) + q(x)u2(x)

dx
(6.227)
stationary? That is, for what functions u is E[u + δu] unchanged to ﬁrst order
in δu when u(x) is changed by an arbitrary but tiny function δu(x) to u(x) +
δu(x)? Our equations will be less cluttered if we drop explicit mention of the
x-dependence of p, q, and u, which we assume to be real functions of x.
The ﬁrst-order change in E is
δE[u] ≡
 b
a

p 2u′ δu′ + q 2u δu

dx,
(6.228)
in which the change in the derivative of u is δu′ = u′ + (δu)′ −u′ = (δu)′. Setting
δE = 0 and integrating by parts, we have
0 = δE =
 b
a

p u′(δu)′ + q u δu

dx
=
 b
a

p u′δu
′ −

p u′′ δu + q u δu

dx
=
 b
a

−

p u′′ + q u

δu dx +

p u′δu
b
a.
(6.229)
259

DIFFERENTIAL EQUATIONS
So if E is to be stationary with respect to all tiny changes δu that vanish at the
endpoints a and b, then u must satisfy the differential equation
L u = −

p u′′ + q u = 0.
(6.230)
If instead E is to be stationary with respect to all tiny changes δu, then u
must satisfy the differential equation (6.230) as well as the natural boundary
conditions
0 = p(b) u′(b)
and
0 = p(a) u′(a).
(6.231)
If p(a) ̸= 0 ̸= p(b), then these natural boundary conditions imply Neumann’s
boundary conditions
u′(a) = 0
and
u′(b) = 0
(6.232)
(Carl Neumann, 1832–1925).
6.27 Self-adjoint differential operators
If p(x) and q(x) are real, then the differential operator
L = −d
dx

p(x) d
dx

+ q(x)
(6.233)
is formally self adjoint. Such operators are interesting because if we take any two
functions u and v that are twice differentiable on an interval [a, b] and integrate
v L u twice by parts over the interval, we get
(v, L u) =
 b
a
v L u dx =
 b
a
v

−

pu′′ + qu

dx
=
 b
a

pu′v′ + uqv

dx −

vpu′b
a
=
 b
a

−(pv′)′ + qv

u dx +

puv′ −vpu′b
a
=
 b
a
(L v) u dx +

p(uv′ −vu′)
b
a .
(6.234)
Interchanging u and v and subtracting, we ﬁnd Green’s formula

(vL u −u L v) dx =

p(uv′ −vu′)
b
a = [pW(u, v)]b
a
(6.235)
(George Green, 1793–1841).Its differential form is Lagrange’s identity
vL u −u L v =

p W(u, v)
′
(6.236)
260

6.27 SELF-ADJOINT DIFFERENTIAL OPERATORS
(Joseph-Louis Lagrange, 1736–1813). Thus if the twice-differentiable functions
u and v satisfy boundary conditions at x = a and x = b that make the boundary
term (6.235) vanish

p(uv′ −vu′)
b
a = [pW(u, v)]b
a = 0
(6.237)
then the real differential operator L is symmetric
(v, L u) =
 b
a
v L u dx =
 b
a
u L v dx = (u, L v).
(6.238)
A real linear operator A that acts in a real vector space and satisﬁes the
analogous relation (1.161)
(g, A f ) = (f , A g)
(6.239)
for all vectors in the space is said to be symmetric and self adjoint. In this sense,
the differential operator (6.233) is self adjoint on the space of functions that
satisfy the boundary condition (6.237).
In quantum mechanics, we often deal with wave functions that are complex.
So keeping L real, let’s replace u and v by twice-differentiable, complex-valued
functions ψ = u1 + iu2 and χ = v1 + iv2. If u1, u2, v1, and v2 satisfy boundary
conditions at x = a and x = b that make the boundary terms (6.237) vanish

p(uiv′
j −vju′
i)
b
a =

pW(ui, vj)
b
a = 0
for
i, j = 1, 2
(6.240)
then (6.238) implies that
 b
a
vj L ui dx =
 b
a

L vj

ui dx
for
i, j = 1, 2.
(6.241)
Under these assumptions, one may show (exercise 6.21) that the boundary
condition (6.240) makes the complex boundary term vanish

p W(ψ, χ∗)
b
a =

p

ψ χ∗′ −ψ′ χ∗b
a = 0
(6.242)
and (exercise 6.22) that since L is real, the identity (6.241) holds for complex
functions
(χ, L ψ) =
 b
a
χ∗L ψ dx =
 b
a
(L χ)∗ψ dx = (L χ, ψ).
(6.243)
A linear operator A that satisﬁes the analogous relation (1.157)
(g, A f ) = (A g, f )
(6.244)
is said to be self adjoint or hermitian. In this sense, the differential opera-
tor (6.233) is self adjoint on the space of functions that satisfy the boundary
condition (6.242).
261

DIFFERENTIAL EQUATIONS
The formally self-adjoint differential operator (6.233) will satisfy the inner-
product integral equations (6.238 or 6.243) only when the function p and
the twice-differentiable functions u and v or ψ and χ conspire to make the
boundary terms (6.237 or 6.242) vanish. This requirement leads us to deﬁne a
self-adjoint differential system.
6.28 Self-adjoint differential systems
A self-adjoint differential system consists of a real formally self-adjoint differ-
ential operator, a differential equation on an interval, boundary conditions, and
a set of twice-differentiable functions that obey them.
A second-order differential equation needs two boundary conditions to make
a solution unique (section 6.25). In a self-adjoint differential system, the two
boundary conditions are linear and homogeneous so that the set of all twice-
differentiable functions u that satisfy them is a vector space. This space D is
the domain of the system. For an interval [a, b], Dirichlet’s boundary conditions
(Johann Dirichlet, 1805–1859) are
u(a) = 0
and
u(b) = 0
(6.245)
and Neumann’s (6.232) are
u′(a) = 0
and
u′(b) = 0.
(6.246)
We will require that the functions in the domain D all obey either Dirichlet or
Neumann boundary conditions.
The adjoint domain D∗of a differential system is the set of all twice-
differentiable functions v that make the boundary term (6.237) vanish

p(uv′ −vu′)
b
a = [pW(u, v)]b
a = 0
(6.247)
for all functions u that are in the domain D, that is, that satisfy either Dirichlet
or Neumann boundary conditions.
A differential system is regular and self adjoint if the differential operator
Lu = −(pu′)′ + qu is formally self-adjoint, if the interval [a, b] is ﬁnite, if p, p′,
and q are continuous real functions of x on the interval, if p(x) > 0 on [a, b],
and if the two domains D and D∗coincide, D = D∗.
One may show (exercises 6.23 and 6.24) that if D is the set of all twice-
differentiable functions u(x) on [a, b] that satisfy either Dirichlet’s boundary
conditions (6.245) or Neumann’s boundary conditions (6.246), and if the func-
tion p(x) is continuous and positive on [a, b], then the adjoint set D∗is the
same as D. A real formally self-adjoint differential operator Lu = −(pu′)′ + qu
therefore forms together with Dirichlet (6.245) or Neumann (6.246) boundary
conditions forms a regular and self-adjoint system if p, p′, and q are real and
continuous on a ﬁnite interval [a, b], and p is positive on [a, b].
262

6.28 SELF-ADJOINT DIFFERENTIAL SYSTEMS
Since any two functions u and v in the domain D of a regular and self-adjoint
differential system make the boundary term (6.247) vanish, a real formally
self-adjoint differential operator L is symmetric and self adjoint (6.238) on all
functions in its domain
(v, L u) =
 b
a
v L u dx =
 b
a
u L v dx = (u, L v).
(6.248)
If functions in the domain are complex, then by (6.242 & 6.243) the operator L
is self adjoint or hermitian
(χ, L ψ) =
 b
a
χ∗L ψ dx =
 b
a
(L χ)∗ψ dx = (L χ, ψ)
(6.249)
on all complex functions ψ and χ in its domain.
Example 6.32 (Sines and cosines)
The differential system with the formally
self-adjoint differential operator
L = −d2
dx2
(6.250)
on an interval [a, b] and the differential equation L u =
−u′′ = λ u has the
function p(x) = 1. If we choose the interval to be [−π, π] and the domain D to
be the set of all functions that are twice differentiable on this interval and sat-
isfy Dirichlet boundary conditions (6.245), then we get a self-adjoint differential
system in which the domain includes linear combinations of un(x) = sin nx. If
instead we impose Neumann boundary conditions (6.246), then the domain D
contains linear combinations of un(x) = cos nx. In both cases, the system is
regular and self adjoint.
Some important differential systems are self adjoint but singular because the
function p(x) vanishes at one or both of the endpoints of the interval [a, b] or
because the interval is inﬁnite, for instance [0, ∞) or (−∞, ∞). In these singular,
self-adjoint differential systems, the boundary term (6.247) vanishes if u and v
are in the domain D = D∗.
Example 6.33 (Legendre’s system)
Legendre’s formally self-adjoint differential
operator is
L = −d
dx
#
(1 −x2) d
dx
$
(6.251)
and his differential equation is
L u = −

(1 −x2)u′′
= ℓ(ℓ+ 1) u
(6.252)
263

DIFFERENTIAL EQUATIONS
on the interval [−1, 1]. The function p(x) = 1−x2 vanishes at both endpoints x =
±1, and so this self-adjoint system is singular. Because p(±1) = 0, the boundary
term (6.247) is zero as long as the functions u and v are differentiable on the
interval. The domain D is the set of all functions that are twice differentiable on
the interval [−1, 1].
Example 6.34 (Hermite’s system)
Hermite’s formally self-adjoint differential
operator is
L = −d2
dx2 + x2
(6.253)
and his differential equation is
L u = −u′′ + x2 u = (2n + 1) u
(6.254)
on the interval (−∞, ∞). This system has p(x) = 1 and q(x) = x2. It is self
adjoint but singular because the interval is inﬁnite. The domain D consists of all
functions that are twice differentiable and that go to zero as x →±∞faster than
1/x3/2, which ensures that the relevant integrals converge and that the boundary
term (6.247) vanishes.
6.29 Making operators formally self adjoint
We can make a generic real second-order linear homogeneous differential
operator
L0 = h2
d2
dx2 + h1
d
dx + h0
(6.255)
formally self adjoint
L = −d
dx
#
p(x) d
dx
$
+ q(x) = −p(x) d2
dx2 −p′(x) d
dx + q(x)
(6.256)
by ﬁrst dividing through by −h2(x)
L1 = −1
h2
L0 = −d2
dx2 −h1
h2
d
dx −h0
h2
(6.257)
and then by multiplying L1 by the positive prefactor
p(x) = exp
 x h1(y)
h2(y) dy

> 0.
(6.258)
The product p L1 then is formally self adjoint
L = p(x) L1 = −exp
 x h1(y)
h2(y) dy
 
d2
dx2 + h1(x)
h2(x)
d
dx + h0(x)
h2(x)

264

6.30 WRONSKIANS OF SELF-ADJOINT OPERATORS
= −d
dx
#
exp
 x h1(y)
h2(y) dy
 d
dx
$
−exp
 x h1(y)
h2(y) dy
 h0(x)
h2(x)
= −d
dx

p d
dx

+ q
(6.259)
with q(x) = −p(x) h0(x)/h2(x). So we may turn any second-order linear homo-
geneous differential operator L into a formally self-adjoint operator L by
multiplying it by
ρ(x) = −exp
2 x h1(y)/h2(y)dy

h2(x)
= −p(x)
h2(x).
(6.260)
The two differential equations L0u = 0 and Lu = ρL0u = 0 have the same
solutions, and so we can restrict our attention to formally self-adjoint differ-
ential equations. But under the transformation (6.260), an eigenvalue equation
L0u = λ u becomes Lu = ρL0u = ρλu, which is an eigenvalue equation
Lu = −(pu′)′ + qu = λ ρ u
(6.261)
with a weight function ρ(x). Such an eigenvalue problem is known as a Sturm–
Liouville problem (Jacques Sturm, 1803–1855; Joseph Liouville, 1809–1882).
If h2(x) is negative (as for many positive operators), then the weight function
ρ(x) = −p(x)/h2(x) is positive.
6.30 Wronskians of self-adjoint operators
We saw in (6.206–6.210) that if y1(x) and y2(x) are two linearly independent
solutions of the ODE
y′′(x) + P(x) y′(x) + Q(x) y(x) = 0
(6.262)
then their wronskian W(x) = y1(x) y′
2(x) −y2(x) y′
1(x) is
W(x) = W(x0) exp
#
−
 x
x0
P(x′)dx′
$
.
(6.263)
Thus if we convert the ODE (6.262) to its formally self-adjoint form
−

p(x)y′(x)
′ + q(x)y(x) = −p(x)d2y(x)
dx2
−p′(x)dy(x)
dx
+ q(x)y(x) = 0 (6.264)
then P(x) = p′(x)/p(x), and so the wronskian (6.263) is
W(x) = W(x0) exp
#
−
 x
x0
p′(x′)/p(x′)dx′
$
,
(6.265)
which we may integrate directly to
W(x) = W(x0) exp [−ln [p(x)/p(x0)]] = W(x0) p(x0)
p(x) .
(6.266)
265

DIFFERENTIAL EQUATIONS
We learned in (6.206–6.214) that if we had one solution y1(x) of the ODE
(6.262 or 6.264), then we could ﬁnd another solution y2(x) that is linearly
independent of y1(x) as
y2(x) = y1(x)
 x W(x′)
y2
1(x′)
dx′.
(6.267)
In view of (6.263), this is an iterated integral. But if the ODE is formally self
adjoint, then the formula (6.266) reduces it to
y2(x) = y1(x)
 x
1
p(x′) y2
1(x′)
dx′
(6.268)
apart from a constant factor.
Example 6.35 (Legendre functions of the second kind)
Legendre’s self-adjoint
differential equation (6.252) is
−

(1 −x2) y′′
= ℓ(ℓ+ 1) y
(6.269)
and an obvious solution for ℓ= 0 is y(x) ≡P0(x) = 1. Since p(x) = 1 −x2, the
integral formula (6.268) gives us as a second solution
Q0(x) = P0(x)
 x
1
p(x′) P2
0(x′)
dx′ =
 x
1
(1 −x2) dx′ = 1
2 ln
1 + x
1 −x

. (6.270)
This second solution Q0(x) is singular at both ends of the interval [−1, 1] and so
does not satisfy the Dirichlet (6.245) or Neumann (6.246) boundary conditions
that make the system self adjoint or hermitian.
6.31 First-order self-adjoint differential operators
The ﬁrst-order differential operator
L = u d
dx + v
(6.271)
will be self adjoint if
 b
a
χ∗Lψ dx =
 b
a

L†χ
∗
ψ dx =
 b
a
(Lχ)∗ψ dx.
(6.272)
Starting from the ﬁrst term, we ﬁnd
 b
a
χ∗Lψ dx =
 b
a
χ∗
u ψ′ + vψ

dx
=
 b
a

(−χ∗u)′ + χ∗v

ψ dx +

χ∗uψ
b
a
266

6.32 A CONSTRAINED VARIATIONAL PROBLEM
=
 b
a

(−χu∗)′ + χv∗∗ψ dx +

χ∗uψ
b
a
=
 b
a

−u∗χ′ + (v∗−u∗′)χ
∗ψ dx +

χ∗uψ
b
a .
(6.273)
So if the boundary terms vanish

χ∗uψ
b
a = 0
(6.274)
and if both u∗= −u and v∗−u∗′ = v, then
 b
a
χ∗Lψ dx =
 b
a

uχ′ + vχ
∗ψ dx =
 b
a
(Lχ)∗ψ dx
(6.275)
and so L will be self adjoint or hermitian, L† = L. The general form of a
ﬁrst-order self-adjoint linear operator is then
L = ir(x) d
dx + s(x) + i
2r′(x)
(6.276)
in which r and s are arbitrary real functions of x.
Example 6.36 (Momentum and angular momentum)
The momentum operator
p = ¯h
i
d
dx
(6.277)
has r = −¯h, which is real, and s = 0 and so is formally self adjoint. The boundary
terms (6.274) are zero if the functions ψ and χ vanish at a and b, which often
are ±∞.
The angular-momentum operators Li = ϵijk xj pk, where pk = −i¯h ∂k, also are
formally self adjoint because the total antisymmetry of ϵijk ensures that j and k
are different as they are summed from 1 to 3.
Example 6.37 (Momentum in a magnetic ﬁeld)
In a magnetic ﬁeld B = ∇× A,
the differential operator
¯h
i ∇−e A
(6.278)
that (in mks units) represents the kinetic momentum mv is formally self adjoint
as is its Yang–Mills analog (11.471) when divided by i.
6.32 A constrained variational problem
In quantum mechanics, we usually deal with normalizable wave-functions. So
let’s ﬁnd the function u(x) that minimizes the energy functional
E[u] =
 b
a

p(x) u′2(x) + q(x) u2(x)

dx
(6.279)
267

DIFFERENTIAL EQUATIONS
subject to the constraint that u(x) be normalized on [a, b] with respect to a
positive weight function ρ(x)
N[u] = ∥u∥2 =
 b
a
ρ(x) u2(x) dx = 1.
(6.280)
Introducing λ as a Lagrange multiplier (section 1.23) and suppressing explicit
mention of the x-dependence of the real functions p, q, ρ, and u, we minimize
the unconstrained functional
E[u, λ] =
 b
a

p u′2 + q u2
dx −λ
 b
a
ρ u2 dx −1

,
(6.281)
which will be stationary at the function u that minimizes it. The ﬁrst-order
change in E[u, λ] is
δE[u, λ] =
 b
a

p 2u′ δu′ + q 2u δu −λ ρ 2u δu

dx,
(6.282)
in which the change in the derivative of u is δu′ = u′ + (δu)′ −u′ = (δu)′. Setting
δE = 0 and integrating by parts, we have
0 = 1
2 δE =
 b
a

p u′(δu)′ + (q −λ ρ) u δu

dx
=
 b
a

p u′δu
′ −

p u′′ δu + (q −λ ρ) u δu

dx
=
 b
a

−

p u′′ + (q −λ ρ) u

δu dx +

p u′δu
b
a.
(6.283)
So if E is to be stationary with respect to all tiny changes δu, then u must satisfy
both the self-adjoint differential equation
0 = −

p u′′ + (q −λ ρ) u
(6.284)
and the natural boundary conditions
0 = p(b) u′(b)
and
0 = p(a) u′(a).
(6.285)
If instead we require E[u, λ] to be stationary with respect to all variations δu that
vanish at the endpoints, δu(a) = δu(b) = 0, then u must satisfy the differential
equation (6.284) but need not satisfy the natural boundary conditions (6.285).
In both cases, the function u(x) that minimizes the energy E[u] subject to
the normalization condition N[u] = 1 is an eigenfunction of the formally self-
adjoint differential operator
L = −d
dx

p(x) d
dx

+ q(x)
(6.286)
268

6.32 A CONSTRAINED VARIATIONAL PROBLEM
with eigenvalue λ
Lu = −

p u′′ + q u = λ ρ u.
(6.287)
The Lagrange multiplier λ has become an eigenvalue of a Sturm–Liouville
equation (6.261).
Is the eigenvalue λ related to E[u] and N[u]? To keep things simple, we restrict
ourselves to a regular and self-adjoint differential system (section 6.28) con-
sisting of the self-adjoint differential operator (6.286), the differential equation
(6.287), and a domain D = D∗of functions u(x) that are twice differentiable on
[a, b] and that satisfy two homogeneous Dirichlet (6.245) or Neumann (6.246)
boundary conditions on [a, b]. All functions u in the domain D therefore satisfy

upu′b
a = 0.
(6.288)
We now multiply the Sturm–Liouville equation (6.287) from the left by u and
integrate from a to b. After integrating by parts and noting the vanishing of the
boundary terms (6.288), we ﬁnd
λ
 b
a
ρ u2 dx =
 b
a
u Lu dx =
 b
a
u

−

p u′′ + q u

dx
=
 b
a

p u′2 + q u2
dx −

upu′b
a
=
 b
a

p u′2 + q u2
dx = E[u].
(6.289)
Thus in view of the normalization constraint (6.280), we see that the eigenvalue
λ is the ratio of the energy E[u] to the norm N[u]
λ =
 b
a

p u′2 + q u2
dx
 b
a
ρ u2 dx
= E[u]
N[u].
(6.290)
But is the function that minimizes the ratio
R[u] ≡E[u]
N[u]
(6.291)
the eigenfunction u of the Sturm–Liouville equation (6.287)? And is the mini-
mum of R[u] the least eigenvalue λ of the Sturm–Liouville equation (6.287)? To
see that the answers are yes and yes, we require δR[u] to vanish
δR[u] = δE[u]
N[u] −E[u] δN[u]
N2[u]
= 0
(6.292)
269

DIFFERENTIAL EQUATIONS
to ﬁrst order in tiny changes δu(x) that are zero at the endpoints of the interval,
δu(a) = δu(b) = 0. Multiplying both sides by N[u], we have
δE[u] = R[u] δN[u].
(6.293)
Referring back to our derivation (6.281–6.283) of the Sturm–Liouville equa-
tion, we see that since δu(a) = δu(b) = 0, the change δE is
δE[u] = 2
 b
a

−

p u′′ + q u

δu dx + 2

p u′δu
b
a
= 2
 b
a

−

p u′′ + q u

δu dx
(6.294)
while δN is
δN[u] = 2
 b
a
ρ u δu dx.
(6.295)
Substituting these changes (6.294) and (6.295) into the condition (6.293) that
R[u] be stationary, we ﬁnd that the integral
 b
a

−

p u′′ + (q −R[u] ρ ) u

δu dx = 0
(6.296)
must vanish for all tiny changes δu(x) that are zero at the endpoints of the inter-
val. Thus on [a, b], the function u that minimizes the ratio R[u] must satisfy the
Sturm–Liouville equation (6.287)
−

p u′′ + q u = R[u] ρ u
(6.297)
with an eigenvalue λ ≡R[u] that is the minimum value of the ratio R[u].
So the eigenfunction u1 with the smallest eigenvalue λ1 is the one that min-
imizes the ratio R[u], and λ1 = R[u1]. What about other eigenfunctions with
larger eigenvalues? How do we ﬁnd the eigenfunction u2 with the next smallest
eigenvalue λ2? Simple: we minimize R[u] with respect to all functions u that are
in the domain D and that are orthogonal to u1.
Example 6.38 (Inﬁnite square well)
Let us consider a particle of mass m
trapped in an interval [a, b] by a potential that is V for a < x < b but inﬁnite for
x < a and for x > b. Because the potential is inﬁnite outside the interval, the
wave-function u(x) will satisfy the boundary conditions
u(a) = u(b) = 0.
(6.298)
The mean value of the hamiltonian is then the energy functional
⟨u|H|u⟩= E[u] =
 b
a

p(x) u′2(x) + q(x) u2(x)

dx,
(6.299)
270

6.32 A CONSTRAINED VARIATIONAL PROBLEM
in which p(x) = ¯h2/2m and q(x) = V, a constant independent of x. Wave-
functions in quantum mechanics are normalized when possible. So we need to
minimize the functional
E[u] =
 b
a

¯h2
2m u′2(x) + V u2(x)

dx
(6.300)
subject to the constraint
c =
 b
a
u2(x) dx −1 = 0
(6.301)
for all tiny variations δu that vanish at the endpoints of the interval. The weight
function ρ(x) = 1, and the eigenvalue equation (6.287) is
−¯h2
2m u′′ + V u = λ u.
(6.302)
For any positive integer n, the normalized function
un(x) =

2
b −a
1/2
sin

nπ x −a
b −a

(6.303)
satisﬁes the boundary conditions (6.298) and the eigenvalue equation (6.302)
with energy eigenvalue
λn = E[un] = 1
2m
 nπ ¯h
b −a
2
+ V.
(6.304)
The second eigenfunction u2 minimizes the energy functional E[u] over the space
of normalized functions that satisfy the boundary conditions (6.298) and are
orthogonal to the ﬁrst eigenfunction u1. The eigenvalue λ2 is higher than λ1
(four times higher). As the quantum number n increases, the energy λn = E[un]
goes to inﬁnity as n2. That λn →∞as n →∞is related (section 6.35) to the
completeness of the eigenfunctions un.
Example 6.39 (Bessel’s system)
Bessel’s energy functional is
E[u] =
 1
0

x u′2(x) + n2
x u2(x)

dx,
(6.305)
in which n ≥0 is an integer. We seek the minimum of this functional over the set
of twice-differentiable functions u(x) on [0, 1] that are normalized
N[u] = ∥u∥2 =
 1
0
x u2(x) dx = 1
(6.306)
and that satisfy the boundary conditions u(0) = 0 for n > 0 and u(1) = 0.
We’ll use a Lagrange multiplier λ (section 1.23) and minimize the unconstrained
functional E[u] −λ (N[u] −1). Proceeding as in (6.279–6.287), we ﬁnd that u
must obey the formally self-adjoint differential equation
271

DIFFERENTIAL EQUATIONS
L u = −(x u′)′ + n2
x u = λ x u.
(6.307)
The ratio formula (6.290) and the positivity of Bessel’s energy functional (6.305)
tell us that the eigenvalues λ = E[u]/N[u] are positive (exercise 6.25). As we’ll
see in a moment, the boundary conditions largely determine these eigenvalues
λn,m ≡k2
n,m. By changing variables to ρ = kn,mx and letting u(x) = Jn(ρ), we
arrive (exercise 6.26) at
d2Jn
dρ2 + 1
ρ
dJn
dρ +

1 −n2
ρ2

Jn = 0,
(6.308)
which is Bessel’s equation. The eigenvalues are determined by the condition
u(1) = Jn(kn,m) = 0; they are the squares of the zeros of Jn(ρ). The eigenfunc-
tion of the self-adjoint differential equation (6.307) with eigenvalue λn,m = k2
n,m
is um(x) = Jn(kn,mx). The parameter n labels the differential system; it is not
an eigenvalue. Asymptotically as m →∞, one has (Courant and Hilbert, 1955,
p. 416)
lim
m→∞
λn,m
m2π2 = 1,
(6.309)
which shows that the eigenvalues λn,m rise like m2 as m →∞.
Example 6.40 (Harmonic oscillator)
We’ll minimize the energy
E[u] =
 ∞
−∞

¯h2
2m u′2(x) + 1
2m ω2 x2 u2(x)

dx
(6.310)
subject to the normalization condition
N[u] = ∥u∥2 =
 ∞
−∞
u2(x) dx = 1.
(6.311)
We introduce λ as a Lagrange multiplier and ﬁnd the minimum of the uncon-
strained function E[u]−λ (N[u] −1). Following equations (6.279–6.287), we ﬁnd
that u must satisfy Schrödinger’s equation
−¯h2
2mu′′ + 1
2m ω2 x2 u = λu,
(6.312)
which we write as
¯hω
#mω
2¯h

x −
¯h
mω
d
dx
 
x +
¯h
mω
d
dx

+ 1
2
$
u = λu.
(6.313)
The lowest eigenfunction u0 is mapped to zero by the second factor

x +
¯h
mω
d
dx

u0(x) = 0
(6.314)
272

6.33 EIGENFUNCTIONS AND EIGENVALUES
so its eigenvalue λ0 is ¯hω/2. Integrating this differential equation, we get
u0(x) =
mω
π ¯h
1/4
exp

−mωx2
2¯h

,
(6.315)
in which the prefactor is a normalization constant. As in section 2.11, one may
get the higher eigenfunctions by acting on u0 with powers of the ﬁrst factor inside
the square brackets (6.313)
un(x) =
1
√
n!
mω
2¯h
n/2 
x −
¯h
mω
d
dx
n
u0(x).
(6.316)
The eigenvalue of un is λn = ¯hω(n + 1/2). Again, λn →∞as n →∞.
6.33 Eigenfunctions and eigenvalues of self-adjoint systems
A regular Sturm–Liouville system is a set of regular and self-adjoint differ-
ential systems (section 6.28) that have the same differential operator, interval
[a, b], boundary conditions, and domain, and whose differential equations are
of Sturm–Liouville (6.287) type
L ψ = −(p ψ′)′ + q ψ = λ ρ ψ,
(6.317)
each distinguished by an eigenvalue λ. The functions p, q, and ρ are real and
continuous, p and ρ are positive on [a, b], but the weight function ρ may vanish
at isolated points of the interval.
Since the differential systems are self adjoint, the real or complex functions in
the common domain D are twice differentiable on the interval [a, b] and satisfy
two homogeneous boundary conditions that make the boundary terms (6.247)
vanish
p W(ψ′, ψ∗)
b
a = 0
(6.318)
and so the differential operator L obeys the condition (6.249)
(χ, L ψ) =
 b
a
χ∗L ψ dx =
 b
a
(L χ)∗ψ dx = (L χ, ψ)
(6.319)
of being self adjoint or hermitian.
Let ψi and ψj be eigenfunctions of L with eigenvalues λi and λj
L ψi = λi ρ ψi
and
L ψj = λj ρ ψj
(6.320)
in a regular Sturm–Liouville system. Multiplying the ﬁrst of these eigenvalue
equations by ψ∗
j and the complex conjugate of the second by ψi, we get
ψ∗
j L ψi = ψ∗
j λi ρ ψi
and
ψi(L ψj)∗= ψiλ∗
j ρ ψ∗
j .
(6.321)
273

DIFFERENTIAL EQUATIONS
Integrating the difference of these equations over the interval [a, b] and using
(6.319) in the form
2 b
a ψ∗
j L ψi dx =
2 b
a (L ψj)∗ψi dx, we have
0 =
 b
a

ψ∗
j L ψi −(L ψj)∗ψi

dx =

λi −λ∗
j
  b
a
ψ∗
j ψi ρ dx.
(6.322)
Setting i = j, we ﬁnd
0 =

λ∗
i −λi
  b
a
ρ |ψi|2 dx,
(6.323)
which, since the integral is positive, shows that the eigenvalue λi must be
real. All the eigenvalues of a regular Sturm–Liouville system are real. Using
λ∗
j = λj in (6.322), we see that eigenfunctions that have different eigenvalues are
orthogonal on the interval [a, b] with weight function ρ(x)
0 =

λi −λj
  b
a
ψ∗
j ρ ψi dx.
(6.324)
Since the differential operator L, the eigenvalues λi, and the weight function
ρ are all real, we may write the ﬁrst of the eigenvalue equations in (6.320) both
as L ψi = λi ρ ψi and as L ψ∗
i = λi ρ ψ∗
i . By adding these two equations, we see
that the real part of ψi satisﬁes them, and by subtracting them, we see that the
imaginary part of ψi also satisﬁes them. So it might seem that ψi = ui + ivi is
made of two real eigenfunctions with the same eigenvalue.
But each eigenfunction ui in the domain D satisﬁes two homogeneous
boundary conditions as well as its second-order differential equation
−(p u′
i)′ + q ui = λi ρ ui
(6.325)
and so ui is the unique solution in D to this equation. There can be no other
eigenfunction in D with the same eigenvalue. In a regular Sturm–Liouville sys-
tem, there is no degeneracy. All the eigenfunctions ui are orthogonal and can be
normalized on the interval [a, b] with weight function ρ(x)
 b
a
u∗
j ρ ui dx = δij.
(6.326)
They may be taken to be real.
It is true that the eigenfunctions of a second-order differential equation come
in pairs because one can use Wronski’s formula (6.268)
y2(x) = y1(x)
 x
dx′
p(x′) y2
1(x′)
(6.327)
to ﬁnd a linearly independent second solution with the same eigenvalue. But
the second solutions don’t obey the boundary conditions of the domain. Bessel
functions of the second kind, for example, are inﬁnite at the origin.
274

6.34 UNBOUNDEDNESS OF EIGENVALUES
A set of eigenfunctions ui is complete in the mean in a space S of functions if
every function f ∈S can be represented as a series
f (x) =
∞

i=1
aiui(x)
(6.328)
(called a Fourier series) that converges in the mean, that is
lim
N→∞
 b
a
ρf (x) −
N

i=1
aiui(x)

2
ρ(x) dx = 0.
(6.329)
The natural space S is the space L2(a, b) of all functions f that are square-
integrable on the interval [a, b]
 b
a
|f (x)|2 ρ(x) dx < ∞.
(6.330)
The orthonormal eigenfunctions of every regular Sturm–Liouville system on
an interval [a, b] are complete in the mean in L2(a, b). The completeness of these
eigenfunctions follows (section 6.35) from the fact that the eigenvalues λn of
a regular Sturm–Liouville system are unbounded: when arranged in ascending
order λn < λn+1 they go to inﬁnity with the index n
lim
n→∞λn = ∞
(6.331)
as we’ll see in the next section.
6.34 Unboundedness of eigenvalues
We have seen (section 6.32) that the function u(x) that minimizes the ratio
R[u] = E[u]
N[u] =
 b
a

p u′2 + q u2
dx
 b
a
ρ u2 dx
(6.332)
is a solution of the Sturm–Liouville equation
Lu = −

p u′′ + q u = λ ρ u
(6.333)
with eigenvalue
λ = E[u]
N[u].
(6.334)
275

DIFFERENTIAL EQUATIONS
Let us call this least value of the ratio (6.332) λ1; it also is the smallest eigenvalue
of the differential equation (6.333). The second smallest eigenvalue λ2 is the
minimum of the same ratio (6.332) but for functions that are orthogonal to u1
 b
a
ρ u1 u2 dx = 0.
(6.335)
And λ3 is the minimum of the ratio R[u] but for functions that are orthogonal
to both u1 and u2. Continuing in this way, we make a sequence of orthogonal
eigenfunctions un(x) (which we can normalize, N[un] = 1) with eigenvalues λ1 ≤
λ2 ≤λ3 ≤· · · λn. How do the eigenvalues λn behave as n →∞?
Since the function p(x) is positive for a < x < b, it is clear that the energy
functional (6.279)
E[u] =
 b
a

p u′2 + q u2
dx
(6.336)
gets bigger as u′2 increases. In fact, if we let the function u(x) zigzag up and
down about a given curve ¯u, then the kinetic energy
2
pu′2dx will rise but the
potential energy
2
qu2dx will remain approximately constant. Thus by increas-
ing the frequency of the zigzags, we can drive the energy E[u] to inﬁnity. For
instance, if u(x) = sin x, then its zigzag version uω(x) = u(x)(1 + 0.2 sin ωx) will
have higher energy. The case of ω = 100 is illustrated in Fig. 6.1. As ω →∞,
its energy E[uω] →∞.
It is therefore intuitively clear (or at least plausible) that if the real functions
p(x), q(x), and ρ(x) are continuous on [a, b] and if p(x) > 0 and ρ(x) > 0
on (a, b), then there are inﬁnitely many energy eigenvalues λn, and that they
increase without limit as n →∞
lim
n→∞λn = ∞.
(6.337)
Courant and Hilbert (Richard Courant, 1888–1972, and David Hilbert,
1862–1943) provide several proofs of this result (Courant and Hilbert, 1955,
pp. 397–429). One of their proofs involves the change of variables f = (pρ)1/4
and v = fu, after which the eigenvalue equation
L u = −

p u′′ + q u = λρu
(6.338)
becomes Lf v = −v′′ +rv = λvv with r = f ′′/f +q/ρ. Were this r(x) a constant,
the eigenfunctions of Lf would be vn(x) = sin(nπ/(b −a)) with eigenvalues
λvn =
 n π
b −a
2
+ r
(6.339)
276

6.35 COMPLETENESS OF EIGENFUNCTIONS
0
0.5
1
1.5
2
2.5
3
0
0.2
0.4
0.6
0.8
1
1.2
x
u (x), uω (x)
Two functions with very diﬀerent kinetic energies
Figure 6.1
The energy functional E[u] of equation (6.279) assigns a much higher
energy to the function uω(x) = u(x)(1 + 0.2 sin(ωx)) (zigzag curve with ω = 100) than
to the function u(x) = sin(x) (smooth curve). As the frequency ω →∞, the energy
E[u2] →∞.
rising as n2. Courant and Hilbert show that as long as r(x) is bounded for a ≤
x ≤b, the actual eigenvalues of Lf are λv,n = c n2 + dn in which dn is bounded
and that the eigenvalues λn of L differ from the λv,n by a scale factor, so that
they too diverge as n →∞
lim
n→∞
n2
λn
= g
(6.340)
where g is a constant.
6.35 Completeness of eigenfunctions
We have seen in section 6.34 that the eigenvalues of every regular Sturm–
Liouville system when arranged in ascending order tend to inﬁnity with the
index n
lim
n→∞λn = ∞.
(6.341)
277

DIFFERENTIAL EQUATIONS
We’ll now use this property to show that the corresponding eigenfunctions un(x)
are complete in the mean (6.329) in the domain D of the system.
To do so, we follow Courant and Hilbert (Courant and Hilbert, 1955,
pp. 397–428) and extend the energy E and norm N functionals to inner products
on the domain of the system
E[f , g] ≡
 b
a

p(x) f ′(x) g′(x) + q(x) f (x) g(x)

dx,
(6.342)
N[f , g] ≡
 b
a
ρ(x) f (x) g(x) dx
(6.343)
for any f and g in D. Integrating E[f , g] by parts, we have
E[f , g] =
 b
a

p f g′′ −f

pg′′ + q f g

dx
=
 b
a

−f

pg′′ + f q g

dx +

p f g′b
a
(6.344)
or in terms of the self-adjoint differential operator L of the system
E[f , g] =
 b
a
f L g dx +

p f g′b
a .
(6.345)
Since the boundary term vanishes (6.288) when the functions f and g are in the
domain D of the system, it follows that for f and g in D
E[f , g] =
 b
a
f L g dx.
(6.346)
We can use the ﬁrst n orthonormal eigenfunctions uk of the system
L uk = λk ρ uk
(6.347)
to approximate an arbitrary function in f ∈D as the linear combination
f (x) ∼
n

k=1
ck uk(x)
(6.348)
with coefﬁcients ck given by
ck = N[f , uk] =
 b
a
ρ f uk dx.
(6.349)
We’ll show that this series converges in the mean to the function f .
By construction (6.349), the remainder or error of the nth sum
rn(x) = f (x) −
n

k=1
ck uk(x)
(6.350)
278

6.35 COMPLETENESS OF EIGENFUNCTIONS
is orthogonal to the ﬁrst n eigenfunctions
N[rn, uk] = 0
for
k = 1, . . . , n.
(6.351)
The next eigenfunction un+1 minimizes the ratio
R[φ] = E[φ, φ]
N[φ, φ]
(6.352)
over all φ that are orthogonal to the ﬁrst n eigenfunctions uk in the sense that
N[φ, uk] = 0 for k = 1, . . . , n. That minimum is the eigenvalue λn+1
R[un+1] = λn+1,
(6.353)
which therefore must be less than the ratio R[rn]
λn+1 ≤R[rn] = E[rn, rn]
N[rn, rn].
(6.354)
Thus the square of the norm of the remainder is bounded by the ratio
∥rn∥2 ≡N[rn, rn] ≤E[rn, rn]
λn+1
.
(6.355)
So since λn+1 →∞as n →∞, we’re done if we can show that the energy
E[rn, rn] of the remainder is bounded.
This energy is
E[rn, rn] = E

f −
n

k=1
ckuk, f −
n

k=1
ckuk

= E[f , f ] −
n

k=1
ck (E[f , uk] + E[uk, f ]) +
n

k=1
n

ℓ=1
ckcℓE[uk, uℓ]
= E[f , f ] −2
n

k=1
ck E[f , uk] +
n

k=1
n

ℓ=1
ckcℓE[uk, uℓ].
(6.356)
Since f and all the uk are in the domain of the system, they satisfy the boundary
condition (6.247 or 6.318), and so (6.345, 6.347, & 6.326) imply that
E[f , uk] =
 b
a
f Luk dx = λk
 b
a
ρ f uk dx = λk ck
(6.357)
and that
E[uk, uℓ] =
 b
a
uk Luℓdx = λℓ
 b
a
ρ uk uℓdx = λk δk,ℓ.
(6.358)
Using these relations to simplify our formula (6.356) for E[rn, rn] we ﬁnd
E[rn, rn] = E[f , f ] −
n

k=1
λkc2
k.
(6.359)
279

DIFFERENTIAL EQUATIONS
Since λn →∞as n →∞, we can be sure that for high enough n, the sum
n

k=1
λkc2
k > 0
for
n > N
(6.360)
is positive. It follows from (6.359) that the energy of the remainder rn is bounded
by that of the function f
E[rn, rn] = E[f , f ] −
n

k=1
λkc2
k ≤E[f , f ].
(6.361)
By substituting this upper bound E[f , f ] on E[rn, rn] into our upper bound
(6.355) on the squared norm ∥rn∥2 of the remainder, we ﬁnd
∥rn∥2 ≤E[f , f ]
λn+1
.
(6.362)
Thus since λn →∞as n →∞, we see that the series (6.348) converges in the
mean (section 4.3) to f
lim
n→∞∥rn∥2 = lim
n→∞∥f −
n

k=1
ckuk∥2 ≤lim
n→∞
E[f , f ]
λn+1
= 0.
(6.363)
The eigenfunctions uk of a regular Sturm–Liouville system are therefore
complete in the mean in the domain D of the system. They span D.
It is a short step from spanning D to spanning the space L2(a, b) of functions
that are square integrable on the interval [a, b] of the system. To take this step,
we assume that the domain D is dense in L2(a, b), that is, that for every function
g ∈L2(a, b) there is a sequence of functions fn ∈D that converges to it in the
mean so that for any ϵ > 0 there is an integer N1 such that
∥g −fn∥2 ≡
 b
a
|g(x) −fn(x)|2 ρ(x) dx < ϵ
for
n > N1.
(6.364)
Since fn ∈D, we can ﬁnd a series of eigenfunctions uk of the system that con-
verges in the mean to fn so that for any ϵ > 0 there is an integer N2 such
that
∥fn −
N

k=1
cn,k uk∥2 ≡
 b
a
fn(x) −
N

k=1
cn,k uk(x)

2
ρ(x) dx < ϵ
for
N > N2.
(6.365)
The Schwarz inequality (1.99) applies to these inner products, and so
∥g −
N

k=1
cn,k uk∥≤∥g −fn∥+ ∥fn(x) −
N

k=1
cn,k uk∥.
(6.366)
280

6.35 COMPLETENESS OF EIGENFUNCTIONS
Combining the last three inequalities, we have for n > N1 and N > N2
∥g −
N

k=1
cn,k uk∥< 2 √ϵ.
(6.367)
So the eigenfunctions uk of a regular Sturm–Liouville system span the space of
functions that are square integrable on its interval L2(a, b).
One may further show (Courant and Hilbert, 1955, p. 360; Stakgold, 1967,
p. 220) that the eigenfunctions uk(x) of any regular Sturm–Liouville system
form a complete orthonormal set in the sense that every function f (x) that
satisﬁes Dirichlet (6.245) or Neumann (6.246) boundary conditions and has a
continuous ﬁrst and a piecewise continuous second derivative may be expanded
in a series
f (x) =
∞

k=1
ak uk(x)
(6.368)
that converges absolutely and uniformly on the interval [a, b] of the system.
Our discussion (6.341–6.363) of the completeness of the eigenfunctions of a
regular Sturm–Liouville system was insensitive to the ﬁnite length of the inter-
val [a, b] and to the positivity of p(x) on [a, b]. What was essential was the
vanishing of the boundary terms (6.247), which can happen if p vanishes at the
endpoints of a ﬁnite interval or if the functions u and v tend to zero as |x| →∞
on an inﬁnite one. This is why the results of this section have been extended to
singular Sturm–Liouville systems made of self-adjoint differential systems that
are singular because the interval is inﬁnite or has p vanishing at one or both of
its ends.
If the eigenfunctions uk are orthonormal with weight function ρ(x)
δkℓ=
 b
a
uk(x) ρ(x) uℓ(x) dx
(6.369)
then the coefﬁcients ak of the expansion (6.348) are given by the integrals
(6.349)
ak =
 b
a
uk(x) ρ(x) f (x) dx.
(6.370)
By combining equations (6.328) and (6.370), we have
f (x) =
∞

k=1
 b
a
uk(y) ρ(y) f (y) dy uk(x)
(6.371)
or rearranging
f (x) =
 b
a
f (y)
 ∞

k=1
uk(y) uk(x) ρ(y)

dy,
(6.372)
281

DIFFERENTIAL EQUATIONS
which implies the representation
δ(x −y) = ρ(y)
∞

k=1
uk(x) uk(y)
(6.373)
of Dirac’s delta function. But since this series is nonzero only for x = y, the
weight function ρ(y) is just a scale factor, and we can write for 0 ≤α ≤1
δ(x −y) = ρα(x) ρ1−α(y)
∞

k=1
uk(x) uk(y).
(6.374)
These representations of the delta functional are suitable for functions f in the
domain D of the regular Sturm–Liouville system.
Example 6.41 (A Bessel representation of the delta function)
Bessel’s nth
system L u =
−(x u′)′ + n2 u/x = λ x u has eigenvalues λ = z2
n,k that
are the squares of the zeros of the Bessel function Jn(x). The eigenfunctions
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
−200
0
200
400
600
800
1000
x
J0 Series
1000-term Bessel series for Dirac delta function
Figure 6.2
The sum of the ﬁrst 1000 terms of the Bessel representation (6.376)
for the Dirac delta function δ(x −y) is plotted for y = 1/3 and α = 0, for y = 1/2
and α = 1/2, and for y = 2/3 and α = 1.
282

6.35 COMPLETENESS OF EIGENFUNCTIONS
(section 9.1) that are orthonormal with weight function ρ(x) = x are u(n)
k (x) =
√
2 Jn(zn,kx)/Jn+1(zn,k). Thus, by (6.374), we can represent Dirac’s delta func-
tional for functions in the domain D of Bessel’s system as
δ(x −y) = xα y1−α
∞

k=1
u(n)
k (x) u(n)
k (y).
(6.375)
For n = 0, this Bessel representation is
δ(x −y) = 2 xα y1−α
∞

k=1
J0(z0,kx)J0(z0,ky)
J2
1(z0,k)
.
(6.376)
Figure 6.2 plots the ﬁrst 1000 terms of this sum (6.376) for α = 0 and y = 1/3,
for α = 1/2 and y = 1/2, and for α = 1 and y = 2/3. Figure 6.3 plots the ﬁrst
10,000 terms of the same series but for α = 0 and y = 0.47, for α = 1/2 and
y = 1/2, and for α = 1 and y = 0.53. The integrals of these 10,000-term sums
from 0 to 1 respectively are 0.9966, 0.9999, and 0.9999. These plots illustrate the
Sturm–Liouville representation (6.374) of the delta function.
0.46
0.47
0.48
0.49
0.5
0.51
0.52
0.53
0.54
−2000
0
2000
4000
6000
8000
10000
x
J0 Series
10,000-term Bessel series for Dirac delta function
Figure 6.3
The sum of the ﬁrst 10,000 terms of the Bessel representation (6.376)
for the Dirac delta function δ(x −y) is plotted for y = 0.47 and α = 0, for y = 1/2
and α = 1/2, and for y = 0.53 and α = 1.
283

DIFFERENTIAL EQUATIONS
6.36 The inequalities of Bessel and Schwarz
The inequality
 b
a
ρ(x)
f (x) −
N

k=1
akuk(x)

2
dx ≥0
(6.377)
and the formula (6.370) for ak lead (exercise 6.27) to Bessel’s inequality
 b
a
ρ(x) |f (x)|2 dx ≥
∞

k=1
|ak|2.
(6.378)
The argument we used to derive the Schwarz inequality (1.94) for vectors
applies also to functions and leads to the Schwarz inequality
 b
a
ρ(x)|f (x)|2 dx
 b
a
ρ(x)|g(x)|2 dx ≥

 b
a
ρ(x)g∗(x)f (x) dx

2
.
(6.379)
6.37 Green’s functions
Physics is full of equations of the form
L G(x) = δ(n)(x),
(6.380)
in which L is a differential operator in n variables. The solution G(x) is a Green’s
function (section 3.8) for the operator L.
Example 6.42 (Poisson’s Green’s function)
Probably the most important
Green’s function arises when the interaction is of long range as in gravity and
electrodynamics. The divergence of the electric ﬁeld is related to the charge den-
sity ρ by Gauss’s law ∇· E = ρ/ϵ0 where ϵ0 = 8.854 × 10−12 F/m is the electric
constant. The electric ﬁeld is E = −∇φ −˙A in which φ is the scalar potential. In
the Coulomb or radiation gauge, the divergence of A vanishes, ∇· A = 0, and so
−△φ = −∇· ∇φ = ρ/ϵ0. The needed Green’s function satisﬁes
−△G(x) = −∇· ∇G(x) = δ(3)(x)
(6.381)
and expresses the scalar potential φ as the integral
φ(t, x) =

G(x −x′) ρ(t, x′)
ϵ0
d3x′.
(6.382)
284

6.37 GREEN’S FUNCTIONS
For when we apply (minus) the Laplacian to it, we get
−△φ(t, x) = −

△G(x −x′) ρ(t, x′)
ϵ0
d3x′
=

δ(3)(x −x′) ρ(t, x′)
ϵ0
d3x′ = ρ(t, x)
ϵ0
,
(6.383)
which is Poisson’s equation.
The reader might wonder how the potential φ(t, x) can depend upon the
charge density ρ(t, x′) at different points at the same time. The scalar poten-
tial is instantaneous because of the Coulomb gauge condition ∇· A = 0, which
is not Lorentz invariant. The gauge-invariant physical ﬁelds E and B are not
instantaneous and do describe Lorentz-invariant electrodynamics.
It is easy to ﬁnd the Green’s function G(x) by expressing it as a Fourier
transform
G(x) =

eik·x g(k) d3k
(6.384)
and by using the three-dimensional version
δ(3)(x) =

d3k
(2π)3 eik·x
(6.385)
of Dirac’s delta function (3.36). If we insert these Fourier transforms into the
equation (6.381) that deﬁnes the Green’s function G(x), then we ﬁnd
−△G(x) = −△

eik·x g(k) d3k
=

eik·x k2 g(k) d3k = δ(3)(x) =

eik·x d3k
(2π)3 .
(6.386)
Thus the Green’s function G(x) is the Fourier transform
G(x) =
 eik·x
k2
d3k
(2π)3 ,
(6.387)
which we may integrate to
G(x) =
1
4π|x| =
1
4πr
(6.388)
where r = |x| is the length of the vector x. This formula is generalized to n
dimensions in example 5.22.
Example 6.43 (Helmholtz’s Green’s functions)
The Green’s function for the
Helmholtz equation (−△−m2)V(x) = ρ(x) must satisfy
(−△−m2) GH(x) = δ(3)(x).
(6.389)
By using the Fourier-transform method of the previous example, one may show
that GH is
285

DIFFERENTIAL EQUATIONS
GH(x) = eimr
4πr,
(6.390)
in which r = |x| and m has units of inverse length.
Similarly, the Green’s function GmH for the modiﬁed Helmholtz equation
(−△+ m2) GmH(x) = δ(3)(x)
(6.391)
is (example 5.21)
GmH(x) = e−mr
4πr ,
(6.392)
which is a Yukawa potential.
Of these Green’s functions, probably the most important is G(x) = 1/4πr,
which has the expansion
G(x −x′) =
1
4π|x −x′| =
∞

ℓ=0
ℓ

m=−ℓ
1
2ℓ+ 1
rℓ
<
rℓ+1
>
Yℓ,m(θ, φ)Y∗
ℓ,m(θ′, φ′) (6.393)
in terms of the spherical harmonics Yℓ,m(θ, φ). Here r, θ, and φ are the spherical
coordinates of the point x, and r′, θ′, and φ′ are those of the point x′; r> is
the larger of r and r′, and r< is the smaller of r and r′. If we substitute this
expansion (6.393) into the formula (6.382) for the potential φ, then we arrive at
the multipole expansion
φ(t, x) =

G(x −x′) ρ(t, x′)
ϵ0
d3x′
(6.394)
=
∞

ℓ=0
ℓ

m=−ℓ
1
2ℓ+ 1

rℓ
<
rℓ+1
>
Yℓ,m(θ, φ)Y∗
ℓ,m(θ′, φ′) ρ(t, x′)
ϵ0
d3x′.
Physicists often use this expansion to compute the potential at x due to a local-
ized, remote distribution of charge ρ(t, x′). In this case, the integration is only
over the restricted region where ρ(t, x′) ̸= 0, and so r< = r′ and r> = r, and the
multipole expansion is
φ(t, x) =
∞

ℓ=0
1
2ℓ+ 1
ℓ

m=−ℓ
Yℓ,m(θ, φ)
rℓ+1

r′ℓY∗
ℓ,m(θ′, φ′) ρ(t, x′)
ϵ0
d3x′.
(6.395)
In terms of the multipoles
Qm
ℓ=

r′ℓY∗
ℓ,m(θ′, φ′) ρ(t, x′)
ϵ0
d3x′
(6.396)
286

6.38 EIGENFUNCTIONS AND GREEN’S FUNCTIONS
the potential is
φ(t, x) =
∞

ℓ=0
1
2ℓ+ 1
1
rℓ+1
ℓ

m=−ℓ
Qm
ℓYℓ,m(θ, φ).
(6.397)
The spherical harmonics provide for the Legendre polynomial the expansion
Pℓ(ˆx · ˆx′) =
4π
2ℓ+ 1
ℓ

m=−ℓ
Yℓ,m(θ, φ)Y∗
ℓ,m(θ′, φ′),
(6.398)
which abbreviates the Green’s function formula (6.393) to
G(x −x′) =
1
4π|x −x′| = 1
4π
∞

ℓ=0
rℓ
<
rℓ+1
>
Pℓ(ˆx · ˆx′).
(6.399)
Example 6.44 (Feynman’s propagator)
The Feynman propagator
F(x) =

d4q
(2π)4
exp(iqx)
q2 + m2 −iϵ
(6.400)
is a Green’s function (5.230) for the operator L = m2 −2
(m2 −2)F(x) = δ4(x).
(6.401)
By integrating over q0 while respecting the iϵ (example 5.36), one may write the
propagator in terms of the Lorentz-invariant function
+(x) =
1
(2π)3
 d3q
2Eq
exp[i(q · x −Eqx0)]
(6.402)
as (5.241)
F(x) = iθ(x0) +(x) + iθ(−x0) +(x, −x0),
(6.403)
which for space-like x, that is, for x2 = x2 −(x0)2 ≡r2 > 0, depends only upon
r = +
√
x2 and has the value (Weinberg, 1995, p. 202)
+(x) =
m
4π2r K1(mr),
(6.404)
in which K1 is the Hankel function (5.249).
6.38 Eigenfunctions and Green’s functions
The Green’s function (6.387)
G(x −y) =

dk
2π
1
k2 + m2 eik (x−y)
(6.405)
287

DIFFERENTIAL EQUATIONS
is based on the resolution (6.385) of the delta function
δ(x −y) =

dk
2π eik (x−y)
(6.406)
in terms of the eigenfunctions exp(ik x) of the differential operator −∂2 + m2
with eigenvalues k2 + m2.
We may generalize this way of making Green’s functions to a regular Sturm–
Liouville system (section 6.33) with a differential operator L, eigenvalues λn
L un(x) = λn ρ(x) un(x),
(6.407)
and eigenfunctions un(x) that are orthonormal with respect to a positive weight
function ρ(x)
δnℓ= (un, uk) =

ρ(x) un(x)uk(x) dx
(6.408)
and that span in the mean the domain D of the system.
To make a Green’s function G(x −y) that satisﬁes
L G(x −y) = δ(x −y)
(6.409)
we write G(x −y) in terms of the complete set of eigenfunctions uk as
G(x −y) =
∞

k=1
uk(x)uk(y)
λk
(6.410)
so that the action Luk = λkρuk turns G into
L G(x −y) =
∞

k=1
L uk(x)uk(y)
λk
=
∞

k=1
ρ(x) uk(x) uk(y) = δ(x −y),
(6.411)
our α = 1 series expansion (6.374) of the delta function.
6.39 Green’s functions in one dimension
In one dimension, we can explicitly solve the inhomogeneous ordinary differ-
ential equation L f (x) = g(x) in which
L = −d
dx

p(x) d
dx

+ q(x)
(6.412)
is formally self adjoint. We’ll build a Green’s function from two solutions u and
v of the homogeneous equation L u(x) = L v(x) = 0 as
G(x, y) = −1
A [θ(x −y)u(y)v(x) + θ(y −x)u(x)v(y)] ,
(6.413)
288

6.40 NONLINEAR DIFFERENTIAL EQUATIONS
in which θ(x) = (x + |x|)/(2|x|) is the Heaviside step function (Oliver Heaviside,
1850–1925), and A is a constant which we’ll presently identify. We’ll show that
the expression
f (x) =
 b
a
G(x, y) g(y) dy = −v(x)
A
 x
a
u(y) g(y) dy −u(x)
A
 b
x
v(y) g(y) dy
solves
our
inhomogeneous
equation.
Differentiating,
we
ﬁnd
after
a
cancellation
f ′(x) = −v′(x)
A
 x
a
u(y) g(y) dy −u′(x)
A
 b
x
v(y) g(y) dy.
(6.414)
Differentiating again, we have
f ′′(x) = −v′′(x)
A
 x
a
u(y) g(y) dy −u′′(x)
A
 b
x
v(y) g(y) dy
−v′(x)u(x)g(x)
A
+ u′(x)v(x)g(x)
A
= −v′′(x)
A
 x
a
u(y) g(y) dy −u′′(x)
A
 b
x
v(y) g(y) dy
−W(x)
A
g(x),
(6.415)
in which W(x) is the wronskian W(x) = u(x)v′(x) −u′(x)v(x). By applying
the result (6.266) for the wronskian of two linearly independent solutions of
a self-adjoint homogeneous ODE, we ﬁnd W(x) = W(x0) p(x0)/p(x). We set
the constant A = W(x0)p(x0) so that the last term in (6.415) is −g(x)/p(x). It
follows that
Lf (x) = −[Lv(x)]
A
 x
a
u(y) g(y) dy −[Lu(x)]
A
 b
x
v(y) g(y) dy + g(x) = g(x).
(6.416)
But Lu(x) = Lv(x) = 0, so we see that f satisﬁes our inhomogeneous equation
Lf (x) = g(x).
6.40 Nonlinear differential equations
The ﬁeld of nonlinear differential equations is too vast to cover here, but we
may hint at some of its features by considering some examples from cosmology
and particle physics.
The Friedmann equations of general relativity (11.410 & 11.412) for the scale
factor a(t) of a homogeneous, isotropic universe are
289

DIFFERENTIAL EQUATIONS
¨a
a = −4πG
3
(ρ + 3p)
and
 ˙a
a
2
= 8πG
3
ρ −k
a2 ,
(6.417)
in which k respectively is 1, 0, and −1 for closed, ﬂat, and open geometries.
(The scale factor a(t) tells how much space has expanded or contracted by the
time t.) These equations become more tractable when the energy density ρ is
due to a single constituent whose pressure p is related to it by an equation of
state p = wρ. Conservation of energy ˙ρ = −3(ρ + p)/a (11.426–11.431) then
ensures (exercise 6.30) that the product ρ a3(1+w) is independent of time. The
constant w respectively is 1/3, 0, and −1 for radiation, matter, and the vacuum.
The Friedmann equations then are
a2+3w ¨a = −4πG
3
(1 + 3w) ρ a3(1+w) ≡−f ,
(6.418)
where f is a constant that is positive when w > −1/3, and
a1+3w 
˙a2 + k

=
2f
1 + 3w.
(6.419)
Example 6.45 (An open universe of radiation)
Here k = −1 and the parameter
w = 1/3, so the ﬁrst-order Friedmann equation (6.419) becomes
˙a2 = f
a2 + 1.
(6.420)
The universe is expanding, so we take the positive square-root and get
dt =
a da

a2 + f
,
(6.421)
which leads to the general integral t =

a2 + f + C. If we choose the constant
of integration C = −

f , then we ﬁnd
a(t) =
&
t +

f
2
−f ,
(6.422)
a scale factor that vanishes at time zero and approaches t as t →∞.
Example 6.46 (A closed universe of matter)
Here w = 0 and k = 1, and so the
ﬁrst-order Friedmann equation (6.419) is
˙a2 = 2f
a −1.
(6.423)
290

6.40 NONLINEAR DIFFERENTIAL EQUATIONS
Since the universe is expanding, we take the positive square-root
˙a =
&
2f
a −1,
(6.424)
which leads to the general integral
t =

√a da

2f −a
= −

a(2f −a) −f arcsin (1 −a/f ) + C,
(6.425)
in which C is a constant of integration.
Example 6.47 (An open universe of matter)
Here w = 0 and k = −1, and so
the ﬁrst-order Friedmann equation (6.419) is ˙a2 = 2f /a + 1, which leads to the
general integral
t =

√a da

2f + a
=

a(2f + a) −f ln

a(2f + a) + a + f

+ C,
(6.426)
in which C is a constant of integration.
The equations of particle physics are nonlinear. Physicists usually use pertur-
bation theory to cope with the nonlinearities. But occasionally they focus on the
nonlinearities and treat the quantum aspects classically or semi-classically. To
keep things relatively simple, we’ll work in a space-time of only two dimensions
and consider a model ﬁeld theory described by the action density
L = 1
2

˙φ2 −φ′2
−V(φ),
(6.427)
in which V is a simple function of the ﬁeld φ. Lagrange’s equation for this
theory is
¨φ −φ′′ = dV
dφ .
(6.428)
We can convert this partial differential equation to an ordinary one by making
the ﬁeld φ depend only upon the combination u = x −vt rather than upon
both x and t. We then have ˙φ = −vφ′. With this restriction to traveling-wave
solutions, Lagrange’s equation reduces to
(v2 −1)φ′′ = dV
dφ .
(6.429)
We multiply both sides of this equation by φ′
(v2 −1) φ′ φ′′ = dV
dφ φ′.
(6.430)
291

DIFFERENTIAL EQUATIONS
We now integrate both sides to get (v2 −1) 1
2 φ′2 = V −E, in which E is a
constant of integration and a kind of energy
E = 1
2(1 −v2)φ′2 + V(φ).
(6.431)
We can convert (exercise 6.37) this equation into a problem of integration
u −u0 =


1 −v2

2(E −V(φ))
dφ.
(6.432)
By inverting the resulting equation relating u to φ, we may ﬁnd the soliton
solution φ(u −u0), which is a lump of energy traveling with speed v.
Example 6.48 (Soliton of the φ4 theory)
To simplify the integration (6.432), we
take as the action density
L = 1
2

˙φ2 −φ′2
−

E −λ2
2

φ2 −φ2
0
2

.
(6.433)
−10
−8
−6
−4
−2
0
2
4
6
8
10
−1.5
−1
−0.5
0
0.5
1
1.5
Soliton
x
φ (x) = tanh x
Figure 6.4
The ﬁeld φ(x) of the soliton (6.435) at rest (v = 0) at position x0 = 0
for λ = 1 = φ0. The energy density of the ﬁeld vanishes when φ = ±φ0 = ±1.
The energy of this soliton is concentrated at x0 = 0.
292

EXERCISES
Our formal solution (6.432) gives
u −u0 = ±


1 −v2
λ

φ2 −φ2
0
 dφ = ∓

1 −v2
λφ0
tanh−1(φ/φ0)
(6.434)
or
φ(x −vt) = ∓φ0 tanh

λφ0
x −x0 −v(t −t0)

1 −v2

,
(6.435)
which is a soliton (or an antisoliton) at x0 + v(t −t0). A unit soliton at rest
is plotted in Fig. 6.4. Its energy is concentrated at x = 0 where |φ2 −φ2
0| is
maximal.
Exercises
6.1
In rectangular coordinates, the curl of a curl is by deﬁnition (6.40)
(∇× (∇× E))i =
3

j,k=1
ϵijk∂j(∇× E)k =
3

j,k,ℓ,m=1
ϵijk∂jϵkℓm∂ℓEm.
(6.436)
Use Levi-Civita’s identity (1.449) to show that
∇× (∇× E) = ∇(∇· E) −△E.
(6.437)
This formula deﬁnes △E in any system of orthogonal coordinates.
6.2
Show that since the Bessel function Jn(x) satisﬁes Bessel’s equation (6.48), the
function Pn(ρ) = Jn(kρ) satisﬁes (6.47).
6.3
Show that (6.58) implies that Rk,ℓ(r) = jℓ(kr) satisﬁes (6.57).
6.4
Use (6.56, 6.57), and ′′
m = −m2m to show in detail that the product
f (r, θ, φ) = Rk,ℓ(r) ℓ,m(θ) m(φ) satisﬁes −△f = k2f .
6.5
Replacing Helmholtz’s k2 by E −V(r), we get Schrödinger’s equation
−(¯h2/2m)△ψ(r, θ, φ) + V(r)ψ(r, θ, φ) = Eψ(r, θ, φ).
(6.438)
Let ψ(r, θ, φ) = Rn,ℓ(r)ℓ,m(θ)eimφ in which ℓ,m satisﬁes (6.56) and show
that the radial function Rn,ℓmust obey
−

r2R′
n,ℓ
′
/r2 +

ℓ(ℓ+ 1)/r2 + 2mV/¯h2
Rn,ℓ= 2mEn,ℓ/¯h2.
(6.439)
6.6
Use
the
empty-space
Maxwell’s
equations
∇· B = 0,
∇× E + ˙B = 0,
∇· E = 0, and ∇× B −˙E/c2 = 0 and the formula (6.437) to show that in
vacuum △E = ¨E/c2 and △B = ¨B/c2.
6.7
Argue from symmetry and antisymmetry that [γ a, γ a′]∂a∂a′ = 0, in which the
sums over a and b run from 0 to 3.
6.8
Suppose a voltage V(t) = V sin(ωt) is applied to a resistor of R () in series
with a capacitor of capacitance C (F). If the current through the circuit at
time t = 0 is zero, what is the current at time t?
293

DIFFERENTIAL EQUATIONS
6.9
(a) Is the ODE
(1 + y2)y dx + (1 + x2)x dy

1 + x2 + y23/2
= 0
exact? (b) Find its general integral and solution y(x). Use section 6.11.
6.10 (a) Separate the variables of the ODE (1+y2)y dx+(1+x2)x dy = 0. (b) Find
its general integral and solution y(x).
6.11 Find the general solution to the differential equation y′ + y/x = c/x.
6.12 Find the general solution to the differential equation y′ + xy = ce−x2/2.
6.13 James Bernoulli studied ODEs of the form y′ + p y = q yn, in which p and
q are functions of x. Division by yn and the substitution v = y1−n gives
v′ + (1 −n)p = (1 −n) q, which is soluble as shown in section 6.16. Use
this method to solve the ODE y′ −y/2x = 5x2y5.
6.14 Integrate the ODE (xy + 1) dx + 2x2(2xy −1) dy = 0. Hint: use the variable
v(x) = xy(x) instead of y(x).
6.15 Show that the points x = ±1 and ∞are regular singular points of Legendre’s
equation (6.181).
6.16 Use the vanishing of the coefﬁcient of every power of x in (6.185) and the
notation (6.187) to derive the recurrence relation (6.188).
6.17 In example 6.29, derive the recursion relation for k = 1 and discuss the
resulting eigenvalue equation.
6.18 In example 6.29, show that the solutions associated with the roots k = 0 and
k = 1 are the same.
6.19 For a hydrogen atom, we set V(r) = −e2/4πϵ0r ≡−q2/r in (6.439) and get
(r2R′
n,ℓ)′ +

(2m/¯h2)

En,ℓ+ Zq2/r

r2 −ℓ(ℓ+ 1)

Rn,ℓ= 0. So at big r, R′′
n,ℓ≈
−2mEn,ℓRn,ℓ/¯h2 and Rn,ℓ∼exp(−

−2mEn,ℓr/¯h). At tiny r, (r2R′
n,ℓ)′ ≈
ℓ(ℓ+ 1)Rn,ℓand Rn,ℓ(r) ∼rℓ. Set Rn,ℓ(r) = rℓexp(−

−2mEn,ℓr/¯h)Pn,ℓ(r)
and apply the method of Frobenius to ﬁnd the values of En,ℓfor which Rn,ℓis
suitably normalizable.
6.20 Show that as long as the matrix Ykj = y
(ℓj)
k (xj) is nonsingular, the n boundary
conditions
bj = y(ℓj)(xj) =
n

k=1
ck y
(ℓj)
k (xj)
(6.440)
determine the n coefﬁcients ck of the expansion (6.222) to be
CT = B Y−1
or
Ck =
n

j=1
bjY−1
jk .
(6.441)
6.21 Show that if the real and imaginary parts u1, u2, v1, and v2 of ψ and χ satisfy
boundary conditions at x = a and x = b that make the boundary term (6.235)
vanish, then its complex analog (6.242) also vanishes.
294

EXERCISES
6.22 Show that if the real and imaginary parts u1, u2, v1, and v2 of ψ and χ satisfy
boundary conditions at x = a and x = b that make the boundary term (6.235)
vanish, and if the differential operator L is real and self adjoint, then (6.238)
implies (6.243).
6.23 Show that if D is the set of all twice-differentiable functions u(x) on [a, b]
that satisfy Dirichlet’s boundary conditions (6.245) and if the function p(x)
is continuous and positive on [a, b], then the adjoint set D∗deﬁned as the set
of all twice-differentiable functions v(x) that make the boundary term (6.247)
vanish for all functions u ∈D is D itself.
6.24 Same as exercise (6.23) but for Neumann boundary conditions (6.246).
6.25 Use Bessel’s equation (6.307) and the boundary conditions u(0) = 0 for n > 0
and u(1) = 0 to show that the eigenvalues λ are all positive.
6.26 Show that after the change of variables u(x) = Jn(kx) = Jn(ρ) the self-adjoint
differential equation (6.307) becomes Bessel’s equation (6.308).
6.27 Derive Bessel’s inequality (6.378) from the inequality (6.377).
6.28 Repeat example 6.41 using J1s instead of J0s. Hint: the Mathematica com-
mand Do[Print[N[BesselJZero[1, k], 10]], {k, 1, 100, 1}] gives the ﬁrst 100
zeros z1,k of the Bessel function J1(x) to ten signiﬁcant ﬁgures.
6.29 Derive the Yukawa potential (6.392) as the Green’s function for the modiﬁed
Helmholtz equation (6.391).
6.30 Derive the relation ρ = ρ(a/a)3(1+w) between the energy density ρ and
the Robertson–Walker scale factor a(t) from the conservation law dρ/da =
−3(ρ + p)/a and the equation of state p = wρ.
6.31 For a closed universe (k = 1) of radiation (w = 1/3), use Friedmann’s equa-
tions (6.418 & 6.419) to derive the solution (11.448) subject to the boundary
condition a(0) = 0. When does the universe collapse in a big crunch?
6.32 For a ﬂat universe (k = 0) of matter (w = 0), use Friedmann’s equa-
tions (6.418 & 6.419) to derive the solution (11.454) subject to the boundary
condition a(0) = 0.
6.33 Derive the time evolution of a(t) for a ﬂat (k = 0) universe dominated by
radiation (w = 1/3) subject to the boundary condition a(0) = 0. Use (6.419).
6.34 Derive the time evolution of a(t) for an open (k = −1) universe with only dark
energy (w = −1) subject to the boundary condition a(0) = 0. Use (6.419).
6.35 Use Friedmann’s equations (6.418 & 6.419) to derive the evolution of the scale
factor for a closed universe dominated by dark energy subject to the boundary
condition a(0) = √3/8πGρ in which ρ is a constant density of dark energy.
6.36 Use Friedmann’s equations (6.418 & 6.419) to derive the evolution of a(t) for
a ﬂat (k = 0) expanding universe dominated by dark energy (w = −1) subject
to the boundary condition a(0) = α in which ρ is a constant density of dark
energy.
6.37 Derive the soliton solution (6.432) from the energy equation (6.431).
295

7
Integral equations
Differential equations when integrated become integral equations with built-in
boundary conditions. Thus if we integrate the ﬁrst-order ODE
du(x)
dx
≡ux(x) = p(x) u(x) + q(x)
(7.1)
then we get the integral equation
u(x) =
 x
a
p(y) u(y) dy +
 x
a
q(y) dy + u0
(7.2)
and the boundary condition u(a) = f (a) = u0.
With a little more effort, we may integrate the second-order ODE
u′′ = pu′ + qu + r
(7.3)
(exercises 7.1 & 7.2) to
u(x) = f (x) +
 x
a
k(x, y) u(y) dy
(7.4)
with
k(x, y) = p(y) + (x −y)

q(y) −p′(y)

(7.5)
and
f (x) = u(a) + (x −a)

u′(a) −p(a) u(a)

+
 x
a
(x −y)r(y) dy.
(7.6)
In some physical problems, integral equations arise independently of dif-
ferential equations. Whatever their origin, integral equations tend to have
properties more suitable to mathematical analysis because derivatives are
unbounded operators.
296

7.2 VOLTERRA INTEGRAL EQUATIONS
7.1 Fredholm integral equations
An equation of the form
 b
a
k(x, y) u(y) dy = λ u(x) + f (x)
(7.7)
for a ≤x ≤b with a given kernel k(x, y) and a speciﬁed function f (x) is an
inhomogeneous Fredholm equation of the second kind for the function u(x) and
the parameter λ (Erik Ivar Fredholm, 1866–1927).
If f (x) = 0, then it is a homogeneous Fredholm equation of the second kind
 b
a
k(x, y) u(y) dy = λ u(x),
a ≤x ≤b.
(7.8)
Such an equation typically has nontrivial solutions only for certain eigenvalues
λ. Each solution u(x) is an eigenfunction.
If λ = 0 but f (x) ̸= 0, then equation (7.7) is an inhomogeneous Fredholm
equation of the ﬁrst kind
 b
a
k(x, y) u(y) dy = f (x),
a ≤x ≤b.
(7.9)
Finally, if both λ = 0 and f (x) = 0, then (7.7) is a homogeneous Fredholm
equation of the ﬁrst kind
 b
a
k(x, y) u(y) dy = 0,
a ≤x ≤b.
(7.10)
These Fredholm equations are linear because they involve only the ﬁrst (and
zeroth) power of the unknown function u(x).
7.2 Volterra integral equations
If the kernel k(x, y) in the equations (7.7–7.10) that deﬁne the Fredholm integral
equations is causal, that is, if
k(x, y) = k(x, y) θ(x −y),
(7.11)
in which θ(x) = (x+|x|)/2|x| is the Heaviside function, then the corresponding
equations bear the name Volterra (Vito Volterra, 1860–1941). Thus, an equation
of the form
 x
a
k(x, y) u(y) dy = λ u(x) + f (x),
(7.12)
in which the kernel k(x, y) and the function f (x) are given, is an inhomogeneous
Volterra equation of the second kind for the function u(x) and the parameter λ.
297

INTEGRAL EQUATIONS
If f (x) = 0, then it is a homogeneous Volterra equation of the second kind
 x
a
k(x, y) u(y) dy = λ u(x).
(7.13)
Such an equation typically has nontrivial solutions only for certain eigenvalues
λ. The solutions u(x) are the eigenfunctions.
If λ = 0 but f (x) ̸= 0, then equation (7.12) is an inhomogeneous Volterra
equation of the ﬁrst kind
 x
a
k(x, y) u(y) dy = f (x).
(7.14)
Finally, if both λ = 0 and f (x) = 0, then it is a homogeneous Volterra equation
of the ﬁrst kind
 x
a
k(x, y) u(y) dy = 0.
(7.15)
These Volterra equations are linear because they involve only the ﬁrst (and
zeroth) power of the unknown function u(x).
In what follows, we’ll mainly discuss Fredholm integral equations, since those
of the Volterra type are a special case of the Fredholm type.
7.3 Implications of linearity
Because the Fredholm and Volterra integral equations are linear, one may add
solutions of the homogeneous equations (7.8, 7.10, 7.13, & 7.15) and get new
solutions. Thus if u1, u2, . . . are eigenfunctions
 b
a
k(x, y) uj(y) dy = λ uj(x),
a ≤x ≤b
(7.16)
with the same eigenvalue λ, then the sum 
j ajuj(x) also is an eigenfunction
with the same eigenvalue
 b
a
k(x, y)
⎛
⎝
j
ajuj(y)
⎞
⎠dy =

j
aj
 b
a
k(x, y) uj(y) dy
=

j
ajλ uj(x) = λ
⎛
⎝
j
ajuj(x)
⎞
⎠.
(7.17)
It also is true that the difference between any two solutions ui
1(x) and ui
2(x)
of one of the inhomogeneous Fredholm (7.7, 7.9) or Volterra (7.12, 7.14) equa-
tions is a solution of the associated homogeneous equation (7.8, 7.10, 7.13, or
298

7.4 NUMERICAL SOLUTIONS
7.15). Thus if ui
1(x) and ui
2(x) satisfy the inhomogeneous Fredholm equation of
the second kind
 b
a
k(x, y) ui
j(y) dy = λ ui
j(x) + f (x),
j = 1, 2
(7.18)
then their difference ui
1(x)−ui
2(x) satisﬁes the homogeneous Fredholm equation
of the second kind
 b
a
k(x, y)

ui
1(y) −ui
2(y)

dy = λ

ui
1(x) −ui
2(x)

.
(7.19)
Thus, the most general solution ui(x) of the inhomogeneous Fredholm equation
of the second kind (7.18) is a particular solution ui
p(x) of that equation plus the
general solution of the homogeneous Fredholm equation of the second kind
(7.16)
ui(x) = ui
p(x) +

j
ajuj(x).
(7.20)
Linear integral equations are much easier to solve than nonlinear ones.
7.4 Numerical solutions
Let us break the real interval [a, b] into N segments [yk, yk+1] of equal length
y = (b −a)/N with y0 = a, yk = a + k y, and yN = b. We’ll also set xk = yk
and deﬁne U as the vector with entries Uk = u(yk) and K as the (N+1)×(N+1)
square matrix with elements Kkℓ= k(xk, yℓ) y. Then we may approximate the
homogeneous Fredholm equation of the second kind (7.8)
 b
a
k(x, y) u(y) dy = λ u(x),
a ≤x ≤b
(7.21)
as the algebraic equation
N

ℓ=0
Kk,ℓUℓ= λ Uk
(7.22)
or, in matrix notation,
K U = λ U.
(7.23)
We saw in section 1.25 that every such equation has N + 1 eigenvectors
U(α) and eigenvalues λ(α), and that the eigenvalues λ(α) are the solutions of
the characteristic equation (1.244)
det(K −λ(α)I) =
K −λ(α)I
 = 0.
(7.24)
299

INTEGRAL EQUATIONS
In general, as N →∞and y →0, the number N + 1 of eigenvalues λ(α) and
eigenvectors U(α) becomes inﬁnite.
We may apply the same technique to the inhomogeneous Fredholm equation
of the ﬁrst kind
 b
a
k(x, y) u(y) dy = f (x)
for
a ≤x ≤b.
(7.25)
The resulting matrix equation is
K U = F,
(7.26)
in which the kth entry in the vector F is Fk = f (xk). This equation has the
solution
U = K−1 F
(7.27)
as long as the matrix K is nonsingular, that is, as long as
det K ̸= 0.
(7.28)
This technique applied to the inhomogeneous Fredholm equation of the
second kind
 b
a
k(x, y) u(y) dy = λ u(x) + f (x)
(7.29)
leads to the matrix equation
K U = λ U + F.
(7.30)
The associated homogeneous matrix equation
K U = λ U
(7.31)
has N + 1 eigenvalues λ(α) and eigenvectors U(α) ≡|α⟩. For any value of λ that
is not one of the eigenvalues λ(α), the matrix K −λI has a nonzero determinant
and hence an inverse, and so the vector
Ui = (K −λ I)−1 F
(7.32)
is a solution of the inhomogeneous matrix equation (7.30).
If λ = λ(β) is one of the eigenvalues λ(α) of the homogeneous matrix equation
(7.31), then the matrix K −λ(β)I will not have an inverse, but it will have a
pseudoinverse (section 1.32). If its singular-value decomposition (1.362) is
K −λ(β)I =
N+1

n=1
|mn⟩Sn⟨n|
(7.33)
300

7.5 INTEGRAL TRANSFORMATIONS
then its pseudoinverse (1.392) is

K −λ(β)I
+
=
N+1

n=1
Sn̸=0
|n⟩S−1
n ⟨mn|,
(7.34)
in which the sum is over the positive singular values. So if the vector F is a linear
combination of the left singular vectors |mn⟩whose singular values are positive
F =
N+1

n=1
Sn̸=0
fn|mn⟩
(7.35)
then the vector
Ui =

K −λ(β)I
+
F
(7.36)
will be a solution of the inhomogeneous matrix Fredholm equation (7.30). For
in this case

K −λ(β)I

Ui =

K −λ(β)I
 
K −λ(β)I
+
F
=
N+1

n′′=1
|mn′′⟩Sn′′⟨n′′|
N+1

n′=1
Sn′̸=0
|n′⟩S−1
n′ ⟨mn′|
N+1

n=1
Sn̸=0
fn|mn⟩
=
N+1

n=1
Sn̸=0
fn|mn⟩= F.
(7.37)
The most general solution will be the sum of this particular solution of the inho-
mogeneous equation (7.30) and the most general solution of the homogeneous
equation (7.31)
U = Ui +

k
fβ,k U(β,k) =

K −λ(β)I
+
F +

k
fβ,k U(β,k).
(7.38)
Open-source programs are available in C++ (math.nist.gov/tnt/) and in FOR-
TRAN (www.netlib.org/lapack/) that can solve such equations for the N + 1
eigenvalues λ(α) and eigenvectors U(α) and for the inverse K−1 for N = 100,
1000, 10,000, and so forth in milliseconds on a PC.
7.5 Integral transformations
Integral transformations (Courant and Hilbert, 1955, chap. VII) help us solve
linear homogeneous differential equations like
L u + c u = 0,
(7.39)
301

INTEGRAL EQUATIONS
in which L is a linear operator involving derivatives of u(z) with respect to its
complex argument z = x + iy and c is a constant. We choose a kernel K(z, w)
analytic in both variables and write u(z) as an integral along a contour in the
complex w-plane weighted by an unknown function v(w)
u(z) =

C
K(z, w) v(w) dw.
(7.40)
If the differential operator L commutes with the contour integration as it
usually would, then our differential equation (7.39) is

C
[L K(z, w) + c K(z, w)] v(w) dw = 0.
(7.41)
The next step is to ﬁnd a linear operator M that acting on K(z, w) with w-
derivatives (but no z-derivatives) gives L acting on K(z, w)
M K(z, w) = L K(z, w).
(7.42)
We then get an integral equation

C
[M K(z, w) + c K(z, w)] v(w) dw = 0
(7.43)
involving w-derivatives that we can integrate by parts. We choose the contour C
so that the resulting boundary terms vanish. By using our freedom to pick the
kernel and the contour, we often can make the resulting differential equation
for v simpler than the one (7.39) we started with.
Example 7.1 (Fourier, Laplace, and Euler kernels)
We already are familiar with
the most important integral transforms. In chapter 3, we learned that the kernel
K(z, w) = exp(izw) leads to the Fourier transform
u(z) =
 ∞
−∞
eizw v(w) dw
(7.44)
and the kernel K(z, w) = exp(−zw) to the Laplace transform
u(z) =
 ∞
0
e−zw v(w) dw
(7.45)
of section 3.9. Euler’s kernel K(z, w) = (z −w)a occurs in many applications of
Cauchy’s integral theorem (5.21) and integral formula (5.36). These kernels help
us solve differential equations.
Example 7.2 (Bessel functions)
The differential operator L for Bessel’s equation
(6.308)
z2 u′′ + z u′ + z2 u −λ2 u = 0
(7.46)
302

7.5 INTEGRAL TRANSFORMATIONS
is
L = z2 d2
dz2 + z d
dz + z2
(7.47)
and the constant c is −λ2. If we choose M = −d2/dw2, then the kernel should
satisfy (7.42)
L K −M K = z2 Kzz + z Kz + z2 K + Kww = 0,
(7.48)
in which subscripts indicate differentiation as in (6.20). The kernel
K(z, w) = e±iz sin w
(7.49)
is a solution of (7.48) that is entire in both variables (exercise 7.3). In terms of it,
our integral equation (7.43) is

C

Kww(z, w) + λ2 K(z, w)

v(w) dw = 0.
(7.50)
We now integrate by parts once

C
#
−Kw v′ + λ2 K v + dKw v
dw
$
dw
(7.51)
and then again

C
#
K

v′′ + λ2 v

+ d(Kw v −Kv′)
dw
$
dw.
(7.52)
If we choose the contour so that Kw v −Kv′ = 0 at its ends, then the unknown
function v must satisfy the differential equation
v′′ + λ2 v = 0,
(7.53)
which is vastly simpler than Bessel’s; the solution v(w) = exp(iλw) is an entire
function of w for every complex λ. Our solution u(z) then is
u(z) =

C
K(z, w) v(w) dw =

C
e±iz sin w eiλw dw.
(7.54)
For Re(z) > 0 and any complex λ, the contour C1 that runs from −i∞to the
origin w = 0, then to w = −π, and ﬁnally up to −π + i∞has Kw v −Kv′ = 0
at its ends (exercise 7.4) provided we use the minus sign in the exponential. The
function deﬁned by this choice
H(1)
λ (z) = −1
π

C1
e−iz sin w+iλw dw
(7.55)
is the ﬁrst Hankel function (Hermann Hankel, 1839–1873). The second Hankel
function is deﬁned for Re(z) > 0 and any complex λ by a contour C2 that runs
from π + i∞to w = π, then to w = 0, and lastly to −i∞
H(2)
λ (z) = −1
π

C2
e−iz sin w+iλw dw.
(7.56)
303

INTEGRAL EQUATIONS
Because the integrand exp(−iz sin w + iλw) is an entire function of z and w,
one may deform the contours C1 and C2 and analytically continue the Hankel
functions beyond the right half-plane (Courant and Hilbert, 1955, chap. VII).
One may verify (exercise 7.5) that the Hankel functions are related by complex
conjugation
H(1)
λ (z) = H(2)∗
λ
(z)
(7.57)
when both z > 0 and λ are real.
Exercises
7.1
Show that
 x
a
dz
 z
a
dy f (y) =
 x
a
(x −y) f (y) dy.
(7.58)
Hint: differentiate both sides with respect to x.
7.2
Use this identity (7.58) to integrate (7.3) and derive equations (7.4, 7.5, & 7.6).
7.3
Show that the kernel K(z, w)
=
exp(±iz sin w) satisﬁes the differential
equation (7.48).
7.4
Show that for Re z > 0 and arbitrary complex λ, the boundary terms in the
integral (7.52) vanish for the two contours C1 and C2 that deﬁne the two
Hankel functions.
7.5
Show that the Hankel functions are related by complex conjugation (7.57)
when both z > 0 and λ are real.
304

8
Legendre functions
8.1 The Legendre polynomials
The monomials xn span the space of functions f (x) that have power-series
expansions on an interval about the origin
f (x) =
∞

n=0
cn xn =
∞

n=0
f (n)(0)
n!
xn.
(8.1)
They are complete but not orthogonal or normalized. We can make them into
real, orthogonal polynomials Pn(x) of degree n on the interval [−1, 1]
(Pn, Pm) =
 1
−1
Pn(x) Pm(x) dx = 0,
n ̸= m
(8.2)
by requiring that each Pn(x) be orthogonal to all monomials xm for m < n
 1
−1
Pn(x) xm dx = 0,
m < n.
(8.3)
If we impose the normalization condition
Pn(1) = 1
(8.4)
then they are unique and are the Legendre polynomials as in Fig. 8.1.
The coefﬁcients ak of the nth Legendre polynomial
Pn(x) = a0 + a1x + · · · + anxn
(8.5)
must satisfy (exercise 8.3) the n conditions (8.3) of orthogonality
 1
−1
Pn(x) xm dx =
n

k=0
1 −(−1)m+k+1
m + k + 1
ak = 0
for
0 ≤m < n
(8.6)
and the normalization condition (8.4)
Pn(1) = a0 + a1 + · · · + an = 1.
(8.7)
305

LEGENDRE FUNCTIONS
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
Pn (x)
Twenty Legendre polynomials
x
Figure 8.1
The ﬁrst 20 Legendre polynomials in successively ﬁner linewidths. The
straight lines are P0(x) = 1 and P1(x) = x.
Example 8.1 (Building the Legendre polynomials)
Conditions (8.6) and (8.7)
give P0(x) = 1 and P1(x) = x. To make P2(x), we set n = 2 in the orthogonality
condition (8.6) and ﬁnd 2a0 + 2a2/3 = 0 for m = 0, and 2a1/3 = 0 for m = 1.
The normalization condition (8.7) then says that a0 + a1 + a2 = 1. These three
equations give P2(x) = (3x2 −1)/2. Similarly, one ﬁnds P3(x) = (5x3 −3x)/2
and P4(x) = (35x4 −30x2 + 3)/8.
8.2 The Rodrigues formula
Perhaps the easiest way to compute the Legendre polynomials is to apply
Leibniz’s rule (4.46) to the Rodrigues formula
Pn(x) =
1
2nn!
dn(x2 −1)n
dxn
,
(8.8)
which leads to (exercise 8.5)
Pn(x) = 1
2n
n

k=0
n
k
2
(x −1)n−k(x + 1)k.
(8.9)
306

8.2 THE RODRIGUES FORMULA
This formula at x = 1 is
Pn(1) = 1
2n
n

k=0
n
k
2
0n−k2k = 1
2n
n
n
2
2n = 1,
(8.10)
which shows that Rodrigues got the normalization right (Benjamin Rodrigues,
1795–1851).
Example 8.2 (Using Rodrigues’s formula)
By (8.8) or (8.9) and with more
effort, one ﬁnds
P5(x) =
1
255!
d5(x2 −1)5
dx5
= 1
8

63x5 −70x3 + 15x

(8.11)
P6(x) = 1
26
6

k=0
6
k
2
(x −1)6−k(x + 1)k = 1
16

231x6 −315x4 + 105x2 −5

P7(x) = (x −1)7
27
7

k=0
7
k
2 x + 1
x −1
k
= 1
16

429x7 −693x5 + 315x3 −35x

.
In MATLAB, mfun(’P’,n,x) returns the numerical value of Pn(x).
To check that the polynomial Pn(x) generated by Rodrigues’s formula (8.8)
is orthogonal to xm for m < n, we integrate xm Pn(x) by parts n times and drop
all the surface terms (which vanish because x2 −1 is zero at x = ±1)
 1
−1
xm Pn(x) dx =
1
2nn!
 1
−1
xm dn
dxn (x2 −1)n dx
= (−1)n
2nn!
 1
−1
(x2 −1)n dnxm
dxn dx = 0
for
n > m. (8.12)
Thus the polynomial Pn(x) generated by Rodrigues’s formula (8.8) satisﬁes
the orthogonality condition (8.3). It also satisﬁes the normalization condi-
tion (8.4) as shown by (8.10). The Rodrigues formula does generate Legendre’s
polynomials.
One may show (exercises 8.9, 8.10, & 8.11) that the inner product of two
Legendre polynomials is
 1
−1
Pn(x) Pm(x) dx =
2
2n + 1 δnm.
(8.13)
307

LEGENDRE FUNCTIONS
8.3 The generating function
In the expansion
g(t, x) =

1 −2xt + t2−1/2
=
∞

n=0
pn(x) tn
(8.14)
the coefﬁcient pn(x) is the nth partial derivative of g(t, x)
pn(x) = ∂n
∂tn

1 −2xt + t2−1/2
t=0
(8.15)
and is a function of x alone. Explicit calculation shows that it is a polynomial
of degree n.
To identify these polynomials pn(x), we use the integral formula
 1
−1
g(t, x) g(v, x) dx =
 1
−1
dx

1 −2xt + t2
1 −2xv + v2 =
1
√tv ln 1 + √tv
1 −√tv
(8.16)
and the logarithmic series (4.90)
1
√tv ln 1 + √tv
1 −√tv =
∞

k=0
2
2k + 1 (tv)k
(8.17)
to express the integral of g(t, x) g(v, x) over the interval −1 ≤x ≤1 as
 1
−1
g(t, x) g(v, x) dx =
 1
−1
∞

n,m=0
pn(x) pm(x) tn vm dx =
∞

k=0
2
2k + 1 (tv)k.
(8.18)
Equating the coefﬁcients of tnvm in the second and third terms of this equation,
we see that the polynomials pn(x) satisfy
 1
−1
pn(x) pm(x) dx =
2
2n + 1 δn,m,
(8.19)
which is the inner product rule (8.13) obeyed by the Legendre polynomials.
Next, setting x = 1 in the deﬁnition (8.14) of g(t, x), we get from (4.31)
1
1 −t =
∞

n=0
tn =
∞

n=0
pn(1) tn,
(8.20)
which says that pn(1) = 1 for all nonnegative integers n = 0, 1, 2, and so forth.
The polynomials pn(x) are therefore the Legendre polynomials Pn(x), and the
function g(t, x) is their generating function
1

1 −2xt + t2 =
∞

n=0
tn Pn(x).
(8.21)
308

8.4 LEGENDRE’S DIFFERENTIAL EQUATION
Example 8.3 (The Green’s function for Poisson’s equation)
The Green’s func-
tion (3.110) for the Laplacian is
G(R −r) =
1
4π|R −r| =
1
4π

R2 −2R · r + r2 ,
(8.22)
in which R = |R| and r = |r|. It occurs throughout physics and satisﬁes
−∇2G(R −r) = δ(3)(R −r)
(8.23)
where the derivatives can act on R or on r.
We set x = cos θ = R · r/rR and t = r/R, and then factor out 1/R
1
|R −r| =
1

R2 −2Rr cos θ + r2
= 1
R
1

1 −2(r/R)x + (r/R)2
= 1
R
1

1 −2xt + t2 = 1
R g(t, x).
(8.24)
With t = r/R and so forth, this series is the well-known expansion
1
|R −r| = 1
R
∞

n=0
 r
R
n
Pn(cos θ)
(8.25)
of the Green’s function G(R −r) = 1/4π|R −r| = g(t, x)/4πR.
8.4 Legendre’s differential equation
Apart from the prefactor 1/ (2nn!), the Legendre polynomial Pn(x) is the nth
derivative u(n) of u = (x2 −1)n. Since u′ = 2nx(x2 −1)n−1, the function u
satisﬁes (x2 −1)u′ = 2nxu. Using Leibniz’s rule (4.46) to differentiate (n + 1)
times both sides of this equation 2nxu = (x2 −1)u′, we ﬁnd
(2nxu)(n+1) = 2n
n+1

k=0
n + 1
k

x(k) u(n+1−k) = 2n

x u(n+1) + (n + 1) u(n)
(8.26)
and

(x2 −1)u′(n+1)
=
n+1

k=0
n + 1
k

(x2 −1)(k) u(n+2−k)
= (x2 −1)u(n+2) + 2(n + 1)xu(n+1) + n(n + 1)u(n).
(8.27)
309

LEGENDRE FUNCTIONS
Equating the two and setting u(n) = 2nn!Pn, we get
−

(1 −x2) P′
n
′
= n(n + 1) Pn,
(8.28)
which is Legendre’s equation in self-adjoint form.
The differential operator
L = −d
dx p(x) d
dx = −d
dx (1 −x2) d
dx
(8.29)
is formally self adjoint and the real function p(x) = 1 −x2 is positive on the
open interval (−1, 1) and vanishes at x = ±1, so Legendre’s differential opera-
tor L, his differential equation (8.28), and the domain D of functions that are
twice differentiable on the interval [−1, 1] form a singular self-adjoint system
(example 6.33). The Legendre polynomial Pn(x) is an eigenfunction of L with
eigenvalue n(n + 1) and weight function w(x) = 1. The orthogonality relation
(6.324) tells us that eigenfunctions of a self-adjoint differential operator that
have different eigenvalues are orthogonal on the interval [−1, 1] with respect to
the weight function w(x). Thus Pn(x) and Pm(x) are orthogonal for n ̸= m
 1
−1
Pn(x) Pm(x) dx =
2
2n + 1 δnm
(8.30)
as we saw (8.13) directly from the Rodrigues formula.
The eigenvalues n(n + 1) increase without limit, and so the argument of sec-
tion 6.35 shows that the eigenfunctions Pn(x) are complete. Since the weight
function of the Legendre polynomials is unity w(x) = 1, the expansion (6.374)
of Dirac’s delta function here is
δ(x −x′) =
∞

n=0
2n + 1
2
Pn(x′) Pn(x),
(8.31)
which leads to the Fourier–Legendre expansion
f (x) =
∞

n=0
2n + 1
2
Pn(x)
 1
−1
Pn(x′) f (x′) dx′
(8.32)
at least for functions f that are twice differentiable on [ −1, 1].
Changing variables to cos θ = x, we have (1 −x2) = sin2 θ and
d
dθ = d cos θ
dθ
d
dx = −sin θ d
dx
(8.33)
so that
d
dx = −
1
sin θ
d
dθ .
(8.34)
310

8.5 RECURRENCE RELATIONS
Thus in spherical coordinates, Legendre’s equation (8.28) appears as
1
sin θ
d
dθ
#
sin θ d
dθ Pn(cos θ)
$
+ n(n + 1) Pn(cos θ) = 0.
(8.35)
8.5 Recurrence relations
The t-derivative of the generating function g(t, x) = 1/

1 −2xt + t2 is
∂g(t, x)
∂t
=
x −t
(1 −2xt + t2)3/2 =
∞

n=1
n Pn(x) tn−1,
(8.36)
which we can rewrite as
(1 −2xt + t2)
∞

n=1
n Pn(x) tn−1 = (x −t) g(t, x) = (x −t)
∞

n=0
Pn(x) tn.
(8.37)
By equating the coefﬁcients of tn in the ﬁrst and last of these expressions, we
arrive at the recurrence relation
Pn+1(x) =
1
n + 1 [(2n + 1) x Pn(x) −n Pn−1(x)] .
(8.38)
Example 8.4 (Building the Legendre polynomials)
Since P1(x) = x and
P0(x) = 1, this recurrence relation for n = 1 gives
P2(x) = 1
2 [3 x P1(x) −P0(x)] = 1
2

3x2 −1

.
(8.39)
Similarly for n = 2 it gives
P3(x) = 1
3 [5 x P2(x) −2 P1(x)] = 1
2(5x2 −3x).
(8.40)
It builds Legendre polynomials faster than Rodrigues’s formula (8.8).
The x-derivative of the generating function is
∂g(t, x)
∂x
=
t
(1 −2xt + t2)3/2 =
∞

n=1
P′
n(x) tn,
(8.41)
which we can rewrite as
(1 −2xt + t2)
∞

n=1
P′
n(x) tn = t g(t, x) =
∞

n=0
Pn(x) tn+1.
(8.42)
Equating coefﬁcients of tn, we have
P′
n+1(x) + P′
n−1(x) = 2x P′
n(x) + Pn(x).
(8.43)
311

LEGENDRE FUNCTIONS
By differentiating the recurrence relation (8.38) and combining it with this last
equation, we get
P′
n+1(x) −P′
n−1(x) = (2n + 1) Pn(x).
(8.44)
The last two recurrence relations (8.43 & 8.44) lead to several more:
P′
n+1(x) = (n + 1) Pn(x) + x P′
n(x),
(8.45)
P′
n−1(x) = −n Pn(x) + x P′
n(x),
(8.46)
(1 −x2) P′
n(x) = n Pn−1(x) −nx Pn(x),
(8.47)
(1 −x2) P′
n(x) = (n + 1)x Pn(x) −(n + 1) Pn+1(x).
(8.48)
By differentiating (8.48) and using (8.45) for P′
n+1, we recover Legendre’s
equation −[(1 −x2)P′
n]′ = n(n + 1)Pn.
8.6 Special values of Legendre’s polynomials
At x = −1, the generating function is
g(t, −1) =

1 + t2 + 2t
−1/2
=
1
1 + t =
∞

n=0
(−t)n =
∞

n=0
Pn(−1) tn,
(8.49)
which implies that
Pn(−1) = (−1)n
(8.50)
and reminds us of the normalization condition (8.4), Pn(1) = 1.
The generating function g(t, x) is even under the reﬂection of both indepen-
dent variables, so
g(t, x) =
∞

n=0
tn Pn(x) =
∞

n=0
(−t)n Pn(−x) = g(−t, −x),
(8.51)
which implies that
Pn(−x) = (−1)n Pn(x)
whence
P2n+1(0) = 0.
(8.52)
With more effort, one can show that
P2n(0) = (−1)n (2n −1)!!
(2n)!!
and that
|Pn(x)| ≤1.
(8.53)
312

8.8 ORTHOGONAL POLYNOMIALS
8.7 Schlaeﬂi’s integral
Schlaeﬂi used Rodrigues’s formula
Pn(x) =
1
2n n!
 d
dx
n
(x2 −1)n
(8.54)
to express Pn(z′) as the counterclockwise contour integral
Pn(z′) =
1
2n 2πi
)
(z2 −1)n
(z −z′)n+1 dz′.
(8.55)
8.8 Orthogonal polynomials
Rodrigues’s formula (8.8) generates other families of orthogonal polynomials.
The n-th order polynomials Rn
Rn(x) =
1
enw(x)
dn
dxn [w(x) Qn(x)]
(8.56)
are orthogonal on the interval from a to b with weight function w(x)
 b
a
Rn(x) Rk(x) w(x) dx = Nn δnk
(8.57)
as long as the product w(x) Qn(x) vanishes at a and b (exercise 8.8)
w(a) Qn(a) = w(b) Qn(b) = 0.
(8.58)
Example 8.5 (Jacobi’s polynomials)
The choice Q(x) = (x2 −1) with weight
function w(x) = (1 −x)α(1 + x)β and normalization en = 2nn! leads for α > −1
and β > −1 to the Jacobi polynomials
P(α,β)
n
(x) =
1
2nn!(1 −x)−α(1 + x)−β dn
dxn

(1 −x)α(1 + x)β (x2 −1)n
,
(8.59)
which are orthogonal on [−1, 1]
 1
−1
P(α,β)
n
(x) P(α,β)
m
(x) w(x) dx = 2α+β+1(n + α + 1) (n + β + 1)
(2n + α + β + 1) (n + α + β + 1) δnm (8.60)
and satisfy the normalization condition
P(α,β)
n
(1) =
n + α
n

(8.61)
and the differential equation
(1 −x2) y′′ + (β −α −(α + β + 2)x) y′ + n(n + α + β + 1) y = 0.
(8.62)
In terms of R(x, y) =

1 −2xy + y2, their generating function is
313

LEGENDRE FUNCTIONS
2α+β(1 −y + R(x, y))−α(1 + w + R(x, y))−β/R(x, y) =
∞

n=0
P(α,β)
n
(x)yn. (8.63)
When α = β, they are the Gegenbauer polynomials, which for α = β = ±1/2
are the Chebyshev polynomials (of the second and ﬁrst kind, respectively). For
α = β = 0, they are Legendre’s polynomials.
Example 8.6 (Hermite’s polynomials)
The choice Q(x) = 1 with weight
function w(x) = exp( −x2) leads to the Hermite polynomials
Hn(x) = (−1)nex2 dn
dxn e−x2 = ex2/2

x −d
dx
n
e−x2/2 = 2n e−D2/4 xn
(8.64)
where D = d/dx is the x-derivative. They are orthogonal on the real line
 ∞
−∞
Hn(x) Hm(x) e−x2 dx = √π 2n n! δnm
(8.65)
and satisfy the differential equation
y′′ −2 x y′ + 2 n y = 0.
(8.66)
Their generating function is
e2xy−y2 =
∞

n=0
Hn(x) yn
n! .
(8.67)
The nth excited state of the harmonic oscillator of mass m and angular fre-
quency ω is proportional to Hn(x) in which x =

mω/¯h q is the dimensionless
position of the oscillator.
Example 8.7 (Laguerre’s polynomials)
The choices Q(x) = x and weight
function w(x) = xαe−x lead to the generalized Laguerre polynomials
L(α)
n (x) =
ex
n! xα
dn
dxn

e−xxn+α
.
(8.68)
They are orthogonal on the interval [0, ∞)
 ∞
0
L(α)
n (x) L(α)
m (x) xαe−x dx = (n + α + 1)
n!
δn,m
(8.69)
and satisfy the differential equation
x y′′ + (α + 1 −x) y′ + n y = 0.
(8.70)
Their generating function is
(1 −y)−α−1 exp
 x y
y −1

=
∞

n=0
L(α)
n (x) yn.
(8.71)
314

8.9 THE AZIMUTHALLY SYMMETRIC LAPLACIAN
The radial wave-function for the state of the nonrelativistic hydrogen atom with
quantum numbers n and ℓis ρℓL2ℓ+1
n−ℓ−1(ρ) e−ρ/2 in which ρ = 2r/na0 and a0 is
the Bohr radius a0 = 4πϵ0 ¯h2/mee2 .
8.9 The azimuthally symmetric Laplacian
We saw in section 6.5 that the Laplacian △= ∇· ∇separates in spherical
coordinates r, θ, φ. A system with no dependence on the angle φ is said to have
azimuthal symmetry. An azimuthally symmetric function
f (r, θ, φ) = Rk,ℓ(r) ℓ(θ)
(8.72)
will be a solution of Helmholtz’s equation
−△f = k2f
(8.73)
if the functions Rk,ℓ(r) and ℓ(θ) satisfy
1
r2
d
dr

r2 dRk,ℓ
dr

+
#
k2 −ℓ(ℓ+ 1)
r2
$
Rk,ℓ= 0
(8.74)
for a nonnegative integer ℓand Legendre’s equation (8.35)
1
sin θ
d
dθ

sin θ dℓ
dθ

+ ℓ(ℓ+ 1)ℓ= 0
(8.75)
so that we may set ℓ(θ) = Pℓ(cos θ). For k > 0, the solutions of the radial
equation (8.74) that are regular at r = 0 are the spherical Bessel functions
Rk,ℓ(r) = jℓ(kr),
(8.76)
which are given by Rayleigh’s formula (9.68)
jℓ(x) = (−1)ℓxℓ
 d
xdx
ℓsin x
x

.
(8.77)
So the general azimuthally symmetric solution of the Helmholtz equation (8.73)
that is regular at r = 0 is
f (r, θ) =
∞

ℓ=0
ak,ℓjℓ(kr) Pℓ(cos θ),
(8.78)
in which the ak,ℓare constants. If the solution need not be regular at the origin,
then the Neumann functions
nℓ(x) = −(−1)ℓxℓ
 d
xdx
ℓcos x
x

(8.79)
315

LEGENDRE FUNCTIONS
must be included, and the general solution then is
f (r, θ) =
∞

ℓ=0

ak,ℓjℓ(kr) + bk,ℓnℓ(kr)

Pℓ(cos θ),
(8.80)
in which the ak,ℓand bk,ℓare constants.
When k = 0, Helmholtz’s equation reduces to Laplace’s
△f = 0,
(8.81)
which describes the Coulomb-gauge electric potential in the absence of charges
and the Newtonian gravitational potential in the absence of masses. Now the
radial equation is simply
d
dr

r2 dRℓ
dr

= ℓ(ℓ+ 1)Rℓ
(8.82)
since k = 0. We try setting
Rℓ(r) = rn,
(8.83)
which works if n(n + 1) = ℓ(ℓ+ 1), that is, if n = ℓor n = −(ℓ+ 1). So the
general solution to (8.81) is
f (r, θ) =
∞

ℓ=0

aℓrℓ+ bℓr−ℓ−1
Pℓ(cos θ).
(8.84)
If the solution must be regular at r = 0, then all the bℓs must vanish.
8.10 Laplacian in two dimensions
In section 6.5, we saw that Helmholtz’s equation separates in cylindrical coor-
dinates, and that the equation for P(ρ) is Bessel’s equation (6.47). But if α = 0,
Helmholtz’s equation reduces to Laplace’s equation △f = 0, and if the poten-
tial f also is independent of z, then simpler solutions exist. For now α = 0 = k,
and so if ′′
m = −m2m, then equation (6.47) becomes
ρ d
dρ

ρ dPm
dρ

= m2 Pm.
(8.85)
The function (φ) may be taken to be (φ) = exp(imφ) or a linear combination
of cos(mφ) and sin(mφ). If the whole range of φ from 0 to 2π is physically
relevant, then (φ) must be periodic, and so m must be an integer. To solve this
equation (8.85) for Pm, we set Pm = ρn and get
n2 ρn = m2 ρn,
(8.86)
316

8.12 THE ASSOCIATED LEGENDRE FUNCTIONS/POLYNOMIALS
which says that n = ±m. The general z-independent solution of Laplace’s
equation in cylindrical coordinates then is
f (ρ, φ) =
∞

m=0
(am cos(mφ) + bm sin(mφ))

cmρm + dmρ−m
.
(8.87)
8.11 The Laplacian in spherical coordinates
The Laplacian △separates in spherical coordinates, as we saw in section 6.5.
Thus a function
f (r, θ) = Rk,ℓ(r) ℓ,m(θ) m(φ)
(8.88)
will be a solution of the Helmholtz equation −△f = k2f if Rk,ℓis a linear
combination of the spherical Bessel functions jℓ(8.77) and nℓ(8.79)
Rk,ℓ(r) = ak,ℓjℓ(kr) + bk,ℓnℓ(kr)
(8.89)
if m = eimφ, and if ℓ,m satisﬁes the associated Legendre equation
1
sin θ
d
dθ

sin θ dℓ,m
dθ

+

ℓ(ℓ+ 1) −
m2
sin2 θ

ℓ,m = 0.
(8.90)
8.12 The associated Legendre functions/polynomials
The associated Legendre functions Pm
ℓ(x) ≡Pℓ,m(x) are polynomials in sin θ
and cos θ. They arise as solutions of the separated θ equation (8.90)
1
sin θ
d
dθ

sin θ dPℓ,m
dθ

+

ℓ(ℓ+ 1) −
m2
sin2 θ

Pℓ,m = 0
(8.91)
of the Laplacian in spherical coordinates. In terms of x = cos θ, this self-adjoint
ordinary differential equation (ODE) is

(1 −x2)P′
ℓ,m(x)
′
+

ℓ(ℓ+ 1) −
m2
1 −x2

Pℓ,m(x) = 0.
(8.92)
To ﬁnd the Pℓ,ms, we use Leibniz’s rule (4.46) to differentiate Legendre’s
equation (8.28)

(1 −x2) P′
ℓ
′
+ ℓ(ℓ+ 1) Pℓ= 0
(8.93)
m times, obtaining
(1 −x2)P(m+2)
ℓ
−2x(m + 1)P(m+1)
ℓ
+ (ℓ−m)(ℓ+ m + 1)P(m)
ℓ
= 0.
(8.94)
317

LEGENDRE FUNCTIONS
We may make this equation self adjoint by using the prefactor formula
(6.258)
F =
1
1 −x2 exp
#
(m + 1)
 x −2x′
1 −x′2 dx′
$
=
1
1 −x2 exp

(m + 1) ln(1 −x2)

= (1 −x2)m.
(8.95)
The resulting ordinary differential equation

(1 −x2)m+1P′(m)
ℓ
′
+ (1 −x2)m(ℓ−m)(ℓ+ m + 1)P(m)
ℓ
= 0
(8.96)
is self adjoint, but it is not (8.92).
Instead, we deﬁne Pℓ,m in terms of the mth derivative P(m)
ℓ
as
P(m)
ℓ(x) = (1 −x2)−m/2 Pℓ,m(x)
(8.97)
and compute the derivatives
P(m+1)
ℓ
=

P′
ℓ,m + mxPℓ,m
1 −x2

(1 −x2)−m/2
(8.98)
P(m+2)
ℓ
=

P′′
ℓ,m +
2mxP′
ℓ,m
1 −x2
+ mPℓ,m
1 −x2 + m(m + 2)x2Pℓ,m
(1 −x2)2

(1 −x2)−m/2.
When we put these three expressions in equation (8.94), then we get the desired
ODE (8.92). The associated Legendre functions are
Pℓ,m(x) = (1 −x2)m/2 P(m)
ℓ(x) = (1 −x2)m/2 dm
dxm Pℓ(x).
(8.99)
They are simple polynomials in x = cos θ and

1 −x2 = sin θ
Pℓ,m(cos θ) = sinm θ
dm
d cosm θ Pℓ(cos θ).
(8.100)
It follows from Rodrigues’s formula (8.8) for the Legendre polynomial Pℓ(x)
that Pℓ,m(x) is given by the similar formula
Pℓ,m(x) = (1 −x2)m/2
2ℓℓ!
dℓ+m
dxℓ+m (x2 −1)ℓ,
(8.101)
which tells us that under parity Pm
ℓ(x) changes by (−1)ℓ+m
Pℓ,m(−x) = (−1)ℓ+m Pℓ,m(x).
(8.102)
Rodrigues’s formula (8.101) for the associated Legendre function makes sense
as long as ℓ+ m ≥0. This last condition is the requirement in quantum
mechanics that m not be less than −ℓ. And if m exceeds ℓ, then Pℓ,m(x) is given
318

8.13 SPHERICAL HARMONICS
by more than 2ℓderivatives of a polynomial of degree 2ℓ; so Pℓ,m(x) = 0 if
m > ℓ. This last condition is the requirement in quantum mechanics that m not
be greater than ℓ. So we have
−ℓ≤m ≤ℓ.
(8.103)
One may show that
Pℓ,−m(x) = (−1)m (ℓ−m)!
(ℓ+ m)! Pℓ,m(x).
(8.104)
In fact, since m occurs only as m2 in the ordinary differential equation (8.92),
Pℓ,−m(x) must be proportional to Pℓ,m(x).
Under reﬂections, the parity of Pℓ,m is (−1)ℓ+m, that is
Pℓ,m(−x) = (−1)ℓ+m Pℓ,m(x).
(8.105)
If m ̸= 0, then Pℓ,m(x) has a power of

1 −x2 in it, so
Pℓ,m(±1) = 0.
(8.106)
We may consider either ℓ(ℓ+ 1) or m2 as the eigenvalue in the ODE (8.92)

(1 −x2)P′
ℓ,m(x)
′
+

ℓ(ℓ+ 1) −
m2
1 −x2

Pℓ,m(x) = 0.
(8.107)
If ℓ(ℓ+1) is the eigenvalue, then the weight function is unity, and since this ODE
is self adjoint on the interval [−1, 1] (at the ends of which p(x) = (1 −x2) = 0),
the eigenfunctions Pℓ,m(x) and Pℓ′,m(x) must be orthogonal on that interval
when ℓ̸= ℓ′. The full integral formula is
 1
−1
Pℓ,m(x) Pℓ′,m(x) dx =
2
2ℓ+ 1
(ℓ+ m)!
(ℓ−m)! δℓ,ℓ′.
(8.108)
If m2 for ﬁxed ℓis the eigenvalue, then the weight function is 1/(1 −x2), and
the eigenfunctions Pℓ,m(x) and Pℓ′,m(x) must be orthogonal on [−1, 1] when
m ̸= m′. The full formula is
 1
−1
Pℓ,m(x) Pℓ,m′(x)
dx
1 −x2 = (ℓ+ m)!
m(ℓ−m)! δm,m′.
(8.109)
8.13 Spherical harmonics
The spherical harmonic Ym
ℓ(θ, φ) ≡Yℓ,m(θ, φ) is the product
Yℓ,m(θ, φ) = ℓ,m(θ) m(φ)
(8.110)
319

LEGENDRE FUNCTIONS
in which ℓ,m(θ) is proportional to the associated Legendre function Pℓ,m
ℓ,m(θ) = (−1)m
(
2ℓ+ 1
2
(ℓ−m)!
(ℓ+ m)! Pℓ,m(cos θ)
(8.111)
and
m(φ) = eimφ
√
2π
.
(8.112)
The big square-root in the deﬁnition (8.111) ensures that
 2π
0
dφ
 π
0
sin θ dθ Y∗
ℓ,m(θ, φ) Yℓ′,m′(θ, φ) = δℓℓ′ δmm′.
(8.113)
In spherical coordinates, the parity transformation
x′ = −x
(8.114)
is r′ = r, θ′ = π −θ, and φ′ = φ ± π. So under parity, cos θ′ = −cos θ and
exp(imφ′) = (−1)m exp(imφ). This factor of (−1)m cancels the m-dependence
(8.102) of Pℓ,m(θ) under parity, so that under parity
Yℓ,m(θ′, φ′) = Yℓ,m(π −θ, φ ± π) = (−1)ℓYℓ,m(θ, φ).
(8.115)
Thus the parity of the state |n, ℓ, m⟩is (−1)ℓ.
The spherical harmonics are complete on the unit sphere. They may be used
to expand any smooth function f (θ, φ) as
f (θ, φ) =
∞

ℓ=0
ℓ

m=−ℓ
aℓmYℓ,m(θ, φ).
(8.116)
The orthonormality relation (8.113) says that the coefﬁcients aℓm are
aℓm =
 2π
0
dφ
 π
0
sin θ dθ Y∗
ℓ,m(θ, φ) f (θ, φ).
(8.117)
Putting the last two equations together, we ﬁnd
f (θ, φ) =
 2π
0
dφ′
 π
0
sin θ′ dθ′
 ∞

ℓ=0
ℓ

m=−ℓ
Y∗
ℓ,m(θ′, φ′) Yℓ,m(θ, φ)

f (θ′, φ′)
(8.118)
and so we may identify the sum within the brackets as an angular delta
function
320

8.13 SPHERICAL HARMONICS
∞

ℓ=0
ℓ

m=−ℓ
Y∗
ℓ,m(θ′, φ′) Yℓ,m(θ, φ) =
1
sin θ δ(θ −θ′) δ(φ −φ′),
(8.119)
which sometimes is abbreviated as
∞

ℓ=0
ℓ

m=−ℓ
Y∗
ℓ,m(′) Yℓ,m() = δ(2)( −′).
(8.120)
The spherical-harmonic expansion (8.116) of the Legendre polynomial
Pℓ(ˆn · ˆn′) of the cosine ˆn · ˆn′ in which the polar angles of the unit vectors
respectively are θ, φ and θ′, φ′ is the addition theorem
Pℓ(ˆn · ˆn′) =
4π
2ℓ+ 1
ℓ

m=−ℓ
Yℓ,m(θ, φ)Y∗
ℓ,m(θ′, φ′)
=
4π
2ℓ+ 1
ℓ

m=−ℓ
Y∗
ℓ,m(θ, φ)Yℓ,m(θ′, φ′).
(8.121)
Example 8.8 (CMB radiation)
Instruments on the Wilkinson Microwave
Anisotropy Probe (WMAP) and Planck satellites in orbit at the Lagrange point
L2 (in the Earth’s shadow, 1.5×106 km farther from the Sun) measure the
temperature T(θ, φ) of the cosmic microwave background (CMB) radiation as
a function of the polar angles θ and φ in the sky as shown in Fig. 8.2. This
radiation is photons last scattered when the visible Universe became transparent
Figure 8.2
The CMB temperature ﬂuctuations over the celestial sphere as mea-
sured over 7 years by the WMAP satellite. The average temperature is 2.725 K.
White regions are warmer, and black ones colder by about 0.0002 degrees. Courtesy
of the NASA/WMAP Science Team.
321

LEGENDRE FUNCTIONS
at an age of 360,000 years and a temperature (3,000 K) cool enough for hydrogen
atoms to be stable. This initial transparency is usually (and inexplicably) called
recombination.
Since the spherical harmonics Yℓ,m(θ, φ) are complete on the sphere, we can
expand the temperature as
T(θ, φ) =
∞

ℓ=0
ℓ

m=−ℓ
aℓ,m Yℓ,m(θ, φ),
(8.122)
in which the coefﬁcients are by (8.117)
aℓ,m =
 2π
0
dφ
 π
0
sin θ dθ Y∗
ℓ,m(θ, φ) T(θ, φ).
(8.123)
The average temperature T contributes only to a0,0 = T = 2.725 K. The other
coefﬁcients describe the difference T(θ, φ) = T(θ, φ) −T. The angular power
spectrum is
Cℓ=
1
2ℓ+ 1
ℓ

m=−ℓ
|aℓ,m|2.
(8.124)
If we let the unit vector ˆn point in the direction θ, φ and use the addition theorem
(8.121), then we can write the angular power spectrum as
Cℓ= 1
4π

d2 ˆn

d2 ˆn′ Pℓ(ˆn · ˆn′) T(ˆn) T(ˆn′).
(8.125)
In Fig. 8.3, the measured values (Larson et al., 2011) of the power spectrum
ℓ(ℓ+ 1) Cℓ/2π are plotted against ℓfor 1 < ℓ< 1300 with the angles and
distances decreasing with ℓ. The power spectrum is a snapshot at the moment
of transparency of the temperature distribution of the plasma of electrons and
nuclei undergoing dissipative plasma oscillations in which gravity opposes radi-
ation pressure. These acoustic oscillations are slowest when they are at their
maxima and minima; the temperature is high at these maxima and minima.
Oscillations at their ﬁrst maximum form the ﬁrst peak. Those at their ﬁrst mini-
mum form the second peak, and those at their second maximum form the third
peak, and so forth.
The solid curve represents the prediction of an inﬂationary cosmological
model with cold dark matter and a cosmological constant . In this 	CDM
cosmology, the age of the visible Universe is 13.77 Gyr; the Hubble constant is
H0 = 70.4 km/sMpc; the total energy density of the Universe is enough to make
the Universe ﬂat as required by inﬂation; and the fractions of the energy density
respectively due to baryons, dark matter, and dark energy are 4.55%, 22.8%, and
72.7% (Edwin Hubble, 1889–1953).
322

EXERCISES
Multipole moment l
10
100
500
1000 
6000
5000
4000
3000
2000
1000
0 90˚
2˚
0.5˚
0.2˚
Angular size
Temperature fluctuations [μk2]
Figure 8.3
The power spectrum ℓ(ℓ+ 1)Cℓ/2π of the CMB temperature ﬂuctu-
ations in μK2 as measured by WMAP (Larson et al., 2011) over 7 years is plotted
against the angular size and the multipole moment ℓ. The solid curve is the CDM
prediction.
Much is known about Legendre functions. The books A Course of Modern
Analysis (Whittaker and Watson, 1927, chap. XV) and Methods of Mathemati-
cal Physics (Courant and Hilbert, 1955) are outstanding.
Exercises
8.1
Use conditions (8.6) and (8.7) to ﬁnd P0(x) and P1(x).
8.2
Using the Gram–Schmidt method (section 1.10) to turn the functions xn into
a set of functions Ln(x) that are orthonormal on the interval [−1, 1] with inner
product (8.2), ﬁnd Ln(x) for n = 0, 1, 2, and 3. Isn’t Rodrigues’s formula (8.8)
easier to use?
8.3
Derive the conditions (8.6–8.7) on the coefﬁcients ak of the Legendre polyno-
mial Pn(x) = a0 + a1x + · · · + anxn. Hint: ﬁrst show that the orthogonality of
the Pns implies (8.12).
8.4
Use equations (8.6–8.7) to ﬁnd P3(x) and P4(x).
8.5
In superscript notation (6.19), Leibniz’s rule (4.46) for derivatives of products
u v of functions is
(uv)(n) =
n

k=0
n
k

u(n−k) v(k).
(8.126)
Use it and Rodrigues’s formula (8.8) to derive the explicit formula (8.9).
323

LEGENDRE FUNCTIONS
8.6
The product rule for derivatives in superscript notation (6.19) is
(uv)(n) =
n

k=0
n
k

u(n−k) v(k).
(8.127)
Apply it to Rodrigues’s formula (8.8) with x2 −1 = (x −1)(x + 1) and show
that the Legendre polynomials satisfy Pn(1) = 1.
8.7
Use Cauchy’s integral formula (5.36) and Rodrigues’s formula (8.54) to derive
Schlaeﬂi’s integral formula (8.55).
8.8
Show that the polynomials (8.56) are orthogonal (8.57) as long as they satisfy
the endpoint condition (8.58).
8.9
Derive the orthogonality relation (8.2) from Rodrigues’s formula (8.8).
8.10 (a) Use the fact that the quantities w = x2 −1 and wn = wn vanish at the
endpoints ±1 to show by repeated integrations by parts that in superscript
notation (6.19)
 1
−1
w(n)
n w(n)
n dx = −
 1
−1
w(n−1)
n
w(n+1)
n
dx = (−1)n
 1
−1
wnw(2n)
n
dx.
(8.128)
(b) Show that the ﬁnal integral is equal to
In = (2n)!
 1
−1
(1 −x)n (1 + x)n dx.
(8.129)
8.11 (a) Show by integrating by parts that In = (n!)2 22n+1/(2n + 1).
(b) Prove (8.13).
8.12 Suppose that Pn(x) and Qn(x) are two solutions of (8.28). Find an expression
for their wronskian, apart from an over-all constant.
8.13 Use the method of sections 6.23 and 6.30 and the solution f (r) = rℓto ﬁnd a
second solution of the ODE (8.82).
8.14 For a uniformly charged circle of radius a, ﬁnd the resulting scalar potential
φ(r, θ) for r < a.
8.15 (a) Find the electrostatic potential V(r, θ) outside an uncharged perfectly
conducting sphere of radius R in a vertical uniform static electric ﬁeld that
tends to E = Eˆz as r →∞. (b) Find the potential if the free charge on the
sphere is qf.
8.16 Derive (8.125) from (8.123) and (8.124).
324

9
Bessel functions
9.1 Bessel functions of the ﬁrst kind
Friedrich Bessel (1784–1846) invented functions for problems with circular
symmetry. The most useful ones are deﬁned for any integer n by the series
Jn(z) =
zn
2nn!

1 −
z2
2(2n + 2) +
z4
2 · 4(2n + 2)(2n + 4) −. . .

=
z
2
n ∞

m=0
(−1)m
m! (m + n)!
z
2
2m
.
(9.1)
The ﬁrst term of this series tells us that for small |z| ≪1
Jn(z) ≈
zn
2nn!.
(9.2)
The alternating signs in (9.1) make the waves plotted in Fig. 9.1, and we have
for big |z| ≫1 the approximation (Courant and Hilbert, 1955, chap. VII)
Jn(z) ≈
&
2
πz cos

z −nπ
2 −π
4

+ O(|z|−3/2).
(9.3)
The Jn(z) are entire transcendental functions. They obey Bessel’s equation
d2Jn
dz2 + 1
z
dJn
dz +

1 −n2
z2

Jn = 0
(9.4)
(6.308) as one may show (exercise 9.1) by substituting the series (9.1) into the
differential equation (9.4). Their generating function is
exp
z
2 (u −1/u)

=
∞

n=−∞
un Jn(z),
(9.5)
325

BESSEL FUNCTIONS
0
2
4
6
8
10
12
−0.5
0
0.5
1
ρ
The Bessel functions J0(ρ), J1(ρ), and J2(ρ)
0
2
4
6
8
10
12
−0.4
−0.2
0
0.2
0.4
ρ
The Bessel functions J3(ρ), J4(ρ), and J5(ρ)
Figure 9.1
Top: plots of J0(ρ) (solid curve), J1(ρ) (dot-dash), and J2(ρ) (dashed) for
real ρ. Bottom: plots of J3(ρ) (solid curve), J4(ρ) (dot-dash), and J5(ρ) (dashed). The
points at which Bessel functions cross the ρ-axis are called zeros or roots; we use them
to satisfy boundary conditions.
from which one may derive (exercise 9.5) the series expansion (9.1) and
(exercise 9.6) the integral representation (5.46)
Jn(z) = 1
π
 π
0
cos(z sin θ −nθ) dθ = J−n(−z) = (−1)nJ−n(z)
(9.6)
for all complex z. For n = 0, this integral is (exercise 9.7) more simply
J0(z) = 1
2π
 2π
0
eiz cos θ dθ = 1
2π
 2π
0
eiz sin θ dθ.
(9.7)
These integrals (exercise 9.8) give Jn(0) = 0 for n ̸= 0, and J0(0) = 1.
By differentiating the generating function (9.5) with respect to u and identi-
fying the coefﬁcients of powers of u, one ﬁnds the recursion relation
Jn−1(z) + Jn+1(z) = 2n
z Jn(z).
(9.8)
Similar reasoning after taking the z derivative gives (exercise 9.10)
Jn−1(z) −Jn+1(z) = 2 J′
n(z).
(9.9)
326

9.1 BESSEL FUNCTIONS OF THE FIRST KIND
By using the gamma function (section 5.12), one may extend Bessel’s
equation (9.4) and its solutions Jn(z) to nonintegral values of n
Jv(z) =
z
2
v
∞

m=0
(−1)m
m! (m + v + 1)
z
2
2m
.
(9.10)
Letting z = a x, we arrive (exercise 9.11) at the self-adjoint form (6.307) of
Bessel’s equation
−d
dx

x d
dxJn(ax)

+ n2
x Jn(ax) = a2xJn(ax).
(9.11)
In the notation of equation (6.287), p(x) = x, a2 is an eigenvalue, and ρ(x) = x
is a weight function. To have a self-adjoint system (section 6.28) on an interval
[0, b], we need the boundary condition (6.247)
0 =

p(Jnv′ −J′
nv)
b
0 =

x(Jnv′ −J′
nv)
b
0
(9.12)
for all functions v(x) in the domain D of the system. Since p(x) = x, J0(0) = 1,
and Jn(0) = 0 for integers n > 0, the terms in this boundary condition vanish
at x = 0 as long as the domain consists of functions v(x) that are continuous
on the interval [0, b]. To make these terms vanish at x = b, we require that
Jn(ab) = 0 and that v(b) = 0. So ab must be a zero zn,m of Jn(z), that is Jn(ab) =
Jn(zn,m) = 0. With a = zn,m/b, Bessel’s equation (9.11) is
−d
dx

x d
dxJn

zn,mx/b

+ n2
x Jn

zn,mx/b

=
z2
n,m
b2 x Jn

zn,mx/b

.
(9.13)
For ﬁxed n, the eigenvalue a2 = z2
n,m/b2 is different for each positive integer m.
Moreover as m →∞, the zeros zn,m of Jn(x) rise as mπ as one might expect
since the leading term of the asymptotic form (9.3) of Jn(x) is proportional
to cos(x −nπ/2 −π/4), which has zeros at mπ + (n + 1)π/2 + π/4. It fol-
lows that the eigenvalues a2 ≈(mπ)2/b2 increase without limit as m →∞in
accordance with the general result of section 6.34. It follows then from the argu-
ment of section 6.35 and from the orthogonality relation (6.326) that for every
ﬁxed n, the eigenfunctions Jn(zn,mx/b), one for each zero, are complete in the
mean, orthogonal, and normalizable on the interval [0, b] with weight function
ρ(x) = x
 b
0
x Jn
zn,mx
b

Jn
zn,m′x
b

dx = δm,m′ b2
2 J′2
n (zn,m) = δm,m′ b2
2 J2
n+1(zn,m)
(9.14)
327

BESSEL FUNCTIONS
and a normalization constant (exercise 9.12) that depends upon the ﬁrst deriva-
tive of the Bessel function or the square of the next Bessel function at the
zero.
The analogous relation on an inﬁnite interval is
 ∞
0
x Jn(kx) Jn(k′x) dx = 1
k δ(k −k′).
(9.15)
One may generalize these relations (9.11–9.15) from integral n to real nonnega-
tive ν (and to ν > −1/2).
Example 9.1 (Bessel’s drum)
The top of a drum is a circular membrane with
a ﬁxed circumference 2πrd. The membrane’s potential energy is approximately
proportional to the extra area it has when it’s not ﬂat. Let h(x, y) be the dis-
placement of the membrane in the z direction normal to the x–y plane of the
ﬂat membrane, and let hx and hy denote its partial derivatives (6.20). The extra
length of a line segment dx on the stretched membrane is

1 + h2x dx, and so the
extra area of an element dx dy is
dA ≈

1 + h2x + h2y −1

dx dy ≈1
2

h2
x + h2
y

dx dy.
(9.16)
The (nonrelativistic) kinetic energy of the area element is proportional to its
speed squared. So if σ is the surface tension and μ the mass density of the
membrane, then to lowest order in derivatives the action functional is
S[h] =
 μ
2 h2
t −σ
2

h2
x + h2
y

dx dy dt.
(9.17)
We minimize this action for hs that vanish on the boundary x2 + y2 = r2
d
0 = δS[h] =
 
μ ht δht −σ

hx δhx + hy δhy

dx dy dt.
(9.18)
Since (6.170) δht = (δh)t, δhx = (δh)x, and δhy = (δh)y, we can integrate by parts
and get
0 = δS[h] =
 
−μ htt + σ

hxx + hyy

δh dx dy dt
(9.19)
apart from a surface term proportional to δh, which vanishes because δh = 0
on the circumference of the membrane. The membrane therefore obeys the wave
equation
μ htt = σ

hxx + hyy

≡σ △h.
(9.20)
This equation is separable, and so letting h(x, y, t) = s(t) v(x, y), we have
stt
s = σ
μ
△v
v
= −ω2.
(9.21)
328

9.1 BESSEL FUNCTIONS OF THE FIRST KIND
The eigenvalues of the Helmholtz equation
−△v = λ v give the angular
frequencies as ω = √σλ/μ. The time dependence then is
s(t) = a sin

σλ/μ (t −t0)

,
(9.22)
in which a and t0 are constants.
In polar coordinates, Helmholtz’s equation is separable (6.45–6.48)
−△v = −vrr −r−1vr −r−2vθθ = λv.
(9.23)
We set v(r, θ) = u(r)h(θ) and ﬁnd −u′′h −r−1u′h −r−2uh′′ = λuh. After
multiplying both sides by r2/uh, we get
r2 u′′
u + ru′
u + λr2 = −h′′
h = n2.
(9.24)
The general solution for h then is h(θ) = b sin(n(θ −θ0)) in which b and θ0 are
constants and n must be an integer so that h is single valued on the circumference
of the membrane.
The function u thus is an eigenfunction of the self-adjoint differential equation
(6.307) −

r u′′ + n2 u/r = λ r u whose eigenvalues λ ≡z2/r2
d are all posi-
tive. By changing variables to ρ = zr/rd and letting u(r) = Jn(ρ), we arrive
(exercise 6.26) at
d2Jn
dρ2 + 1
ρ
dJn
dρ +

1 −n2
ρ2

Jn = 0,
(9.25)
which is Bessel’s equation (6.308).
The eigenvalues λ = z2/r2
d are determined by the boundary condition u(rd) =
Jn(z) = 0. For each integer n ≥0, there are an inﬁnite number of zeros zn,m
at which the Bessel function vanishes, Jn(zn,m) = 0. Thus λ = λn,m = z2
n,m/r2
d
and so the frequency is ω = (zn,m/rd) √σ/μ. The general solution to the wave
equation (9.20) of the membrane then is
h(r, θ, t) =
∞

n=0
∞

m=1
cn,m sin
#zn,m
rd
&σ
μ (t −t0)
$
sin [n(θ −θ0)] Jn

zn,m
r
rd

.
(9.26)
For any n, the zeros zn,m are the square-roots of the dimensionless eigenvalues
(6.309) and rise like mπ as m →∞.
We learned in section 6.5 that in three dimensions Helmholtz’s equation
−△V = α2 V separates in cylindrical coordinates (and in several other coor-
dinate systems). That is, the function V(ρ, φ, z) = B(ρ)(φ)Z(z) satisﬁes the
equation
−△V = −1
ρ
#
ρ V,ρ

,ρ + 1
ρ V,φφ + ρ V,zz
$
= α2 V
(9.27)
329

BESSEL FUNCTIONS
if B(ρ) obeys Bessel’s equation
ρ d
dρ

ρ dB
dρ

+

(α2 + k2)ρ2 −n2
B = 0
(9.28)
and  and Z respectively satisfy
−d2
dφ2 = n2(φ)
and
d2Z
dz2 = k2Z(z)
(9.29)
or if B(ρ) obeys the Bessel equation
ρ d
dρ

ρ dB
dρ

+

(α2 −k2)ρ2 −n2
B = 0
(9.30)
and  and Z satisfy
−d2
dφ2 = n2(φ)
and
d2Z
dz2 = −k2Z(z).
(9.31)
In the ﬁrst case (9.28 & 9.29), the solution V is
Vk,n(ρ, φ, z) = Jn

α2 + k2 ρ

e±inφe±kz
(9.32)
while in the second case (9.30 & 9.31), it is
Vk,n(ρ, φ, z) = Jn

α2 −k2 ρ

e±inφe±ikz.
(9.33)
In both cases, n must be an integer if the solution is to be single valued on the
full range of φ from 0 to 2π.
When α = 0, the Helmholtz equation reduces to Laplace’s equation △V = 0
of electrostatics, which the simpler functions
Vk,n(ρ, φ, z) = Jn(kρ)e±inφe±kz
and
Vk,n(ρ, φ, z) = Jn(ikρ)e±inφe±ikz
(9.34)
satisfy.
The product i−ν Jν(ikρ) is real and is known as the modiﬁed Bessel function
Iν(kρ) ≡i−ν Jν(ikρ).
(9.35)
It occurs in various solutions of the diffusion equation △V = α2V. The
function V(ρ, φ, z) = B(ρ)(φ)Z(z) satisﬁes
△V = 1
ρ
#
ρ V,ρ

,ρ + 1
ρ V,φφ + ρ V,zz
$
= α2 V
(9.36)
330

9.1 BESSEL FUNCTIONS OF THE FIRST KIND
if B(ρ) obeys Bessel’s equation
ρ d
dρ

ρ dB
dρ

−

(α2 −k2)ρ2 + n2
B = 0
(9.37)
and  and Z respectively satisfy
−d2
dφ2 = n2(φ)
and
d2Z
dz2 = k2Z(z)
(9.38)
or if B(ρ) obeys the Bessel equation
ρ d
dρ

ρ dB
dρ

−

(α2 + k2)ρ2 + n2
B = 0
(9.39)
and  and Z satisfy
−d2
dφ2 = n2(φ)
and
d2Z
dz2 = −k2Z(z).
(9.40)
In the ﬁrst case (9.37 & 9.38), the solution V is
Vk,n(ρ, φ, z) = In

α2 −k2 ρ

e±inφe±kz
(9.41)
while in the second case (9.39 & 9.40), it is
Vk,n(ρ, φ, z) = In

α2 + k2 ρ

e±inφe±ikz.
(9.42)
In both cases, n must be an integer if the solution is to be single valued on the
full range of φ from 0 to 2π.
Example 9.2 (Charge near a membrane)
We will use ρ to denote the density of
free charges – those that are free to move into or out of a dielectric medium, as
opposed to those that are part of the medium, bound in it by molecular forces.
The time-independent Maxwell equations are Gauss’s law ∇· D = ρ for the
divergence of the electric displacement D, and the static form ∇× E = 0 of Fara-
day’s law, which implies that the electric ﬁeld E is the gradient of an electrostatic
potential E = −∇V.
Across an interface between two dielectrics with normal vector ˆn, the tangen-
tial electric ﬁeld is continuous, ˆn × E2 = ˆn × E1, while the normal component
of the electric displacement jumps by the surface density σ of free charge,
ˆn·(D2 −D1) = σ. In a linear dielectric, the electric displacement D is the electric
ﬁeld multiplied by the permittivity ϵ of the material, D = ϵ E.
The membrane of a eukaryotic cell is a phospholipid bilayer whose area is
some 3 × 108 nm2, and whose thickness t is about 5 nm. On a scale of nanome-
ters, the membrane is ﬂat. We will take it to be a slab extending to inﬁnity in the
x and y directions. If the interface between the lipid bilayer and the extracellular
salty water is at z = 0, then the cytosol extends thousands of nm down from
331

BESSEL FUNCTIONS
z = −t = −5 nm. We will ignore the phosphate head groups and set the permit-
tivity ϵℓof the lipid bilayer to twice that of the vacuum ϵℓ≈2ϵ0; the permittivity
of the extracellular water and that of the cytosol are ϵw ≈ϵc ≈80ϵ0.
We will compute the electrostatic potential V due to a charge q at a point
(0, 0, h) on the z-axis above the membrane. This potential is cylindrically sym-
metric about the z-axis, so V = V(ρ, z). The functions Jn(kρ) einφ e±kz form a
complete set of solutions of Laplace’s equation but, due to the symmetry, we
only need the n = 0 functions J0(kρ) e±kz. Since there are no free charges in the
lipid bilayer or in the cytosol, we may express the potential in the lipid bilayer
Vℓand in the cytosol Vc as
Vℓ(ρ, z) =
 ∞
0
dk J0(kρ)

m(k) ekz + f (k) e−kz
,
Vc(ρ, z) =
 ∞
0
dk J0(kρ) d(k) ekz.
(9.43)
The Green’s function (3.110) for Poisson’s equation −△G(x) = δ(3)(x) in
cylindrical coordinates is (5.139)
G(x) =
1
4π|x| =
1
4π

ρ2 + z2 =
 ∞
0
dk
4π J0(kρ) e−k|z|.
(9.44)
Thus we may expand the potential in the salty water as
Vw(ρ, z) =
 ∞
0
dk J0(kρ)
#
q
4πϵw
e−k|z−h| + u(k) e−kz
$
.
(9.45)
Using ˆn × E2 = ˆn × E1 and ˆn · (D2 −D1) = σ, suppressing k, and setting
β ≡qe−kh/4πϵw and y = e2kt, we get four equations
m + f −u = β
and
ϵℓm −ϵℓf + ϵwu = ϵwβ,
ϵℓm −ϵℓyf −ϵcd = 0
and
m + yf −d = 0.
(9.46)
In terms of the abbreviations ϵwℓ= (ϵw + ϵℓ) /2 and ϵcℓ= (ϵc + ϵℓ) /2 as well as
p = (ϵw −ϵℓ)/(ϵw + ϵℓ) and p′ = (ϵc −ϵℓ)/(ϵc + ϵℓ), the solutions are
u(k) = β p −p′/y
1 −pp′/y
and
m(k) = β ϵw
ϵwℓ
1
1 −pp′/y,
f (k) = −β ϵw
ϵwℓ
p′/y
1 −pp′/y
and
d(k) = β ϵwϵℓ
ϵwℓϵcℓ
1
1 −pp′/y.
(9.47)
Inserting these solutions into the Bessel expansions (9.43) for the potentials,
expanding their denominators
1
1 −pp′/y =
∞

0
(pp′)n e−2nkt,
(9.48)
and using the integral (9.44), we ﬁnd that the potential Vw in the extracellular
water of a charge q at (0, 0, h) in the water is
332

9.1 BESSEL FUNCTIONS OF THE FIRST KIND
Vw(ρ, z) =
q
4πϵw

1
r +
p

ρ2 + (z + h)2 −
∞

n=1
p′ 
1 −p2
(pp′)n−1

ρ2 + (z + 2nt + h)2

, (9.49)
in which r =

ρ2 + (z −h)2 is the distance to the charge q. The principal image
charge pq is at (0, 0, −h). Similarly, the potential Vℓin the lipid bilayer is
Vℓ(ρ, z) =
q
4πϵwℓ
∞

n=0

(pp′)n

ρ2 + (z −2nt −h)2 −
pnp′n+1

ρ2 + (z + 2(n + 1)t + h)2

(9.50)
and that in the cytosol is
Vc(ρ, z) =
q ϵℓ
4πϵwℓϵcℓ
∞

n=0
(pp′)n

ρ2 + (z −2nt −h)2 .
(9.51)
These potentials are the same as those of example 4.16, but this derivation is
much simpler and less error prone than the method of images.
Since p = (ϵw−ϵℓ)/(ϵw+ϵℓ) > 0, the principal image charge pq at (0, 0, −h) has
the same sign as the charge q and so contributes a positive term proportional to
pq2 to the energy. So a lipid membrane repels a nearby charge in water no matter
what the sign of the charge. A cell membrane is a phospholipid bilayer. The lipids
avoid water and form a 4-nm-thick layer that lies between two 0.5-nm layers of
phosphate groups which are electric dipoles. These electric dipoles cause the cell
membrane to weakly attract ions that are within 0.5 nm of the membrane.
Example 9.3 (Cylindrical wave-guides)
An electromagnetic wave traveling in
the z-direction down a cylindrical wave-guide looks like
E ei(kz−ωt)
and
B ei(kz−ωt),
(9.52)
in which E and B depend upon ρ and φ
E = Eρ ˆρ + Eφ ˆφ + Ezˆz
and
B = Bρ ˆρ + Bφ ˆφ + Bzˆz
(9.53)
in cylindrical coordinates (11.164–11.169 & 11.241). If the wave-guide is an
evacuated, perfectly conducting cylinder of radius r, then on the surface of the
wave-guide the parallel components of E and the normal component of B must
vanish, which leads to the boundary conditions
Ez(r, φ) = 0,
Eφ(r, φ) = 0,
and
Bρ(r, φ) = 0.
(9.54)
Since the E and B ﬁelds have subscripts, we will use commas to denote deriva-
tives as in ∂Ez/∂φ ≡Ez,φ and ∂(ρEφ)/∂ρ ≡(ρEφ),ρ and so forth. In this
notation, the vacuum forms ∇× E = −˙B and ∇× B = ˙E/c2 of the Faraday
and Maxwell–Ampère laws give us (exercise 9.14) the ﬁeld equations
333

BESSEL FUNCTIONS
Ez,φ/ρ −ikEφ = iωBρ,
Bz,φ/ρ −ikBφ = −iωEρ/c2,
ikEρ −Ez,ρ = iωBφ,
ikBρ −Bz,ρ = −iωEφ/c2,

(ρEφ),ρ −Eρ,φ

/ρ = iωBz,

(ρBφ),ρ −Bρ,φ

/ρ = −iωEz/c2.
(9.55)
Solving them for the ρ and φ components of E and B in terms of their z
components (exercise 9.15), we ﬁnd
Eρ = −ikEz,ρ + ωBz,φ/ρ
k2 −ω2/c2
,
Eφ = −ikEz,φ/ρ −ωBz,ρ
k2 −ω2/c2
,
Bρ = −ikBz,ρ −ωEz,φ/c2ρ
k2 −ω2/c2
,
Bφ = −ikBz,φ/ρ + ωEz,ρ/c2
k2 −ω2/c2
. (9.56)
The ﬁelds Ez and Bz obey the separable wave equations (11.91), exercise 6.6,
−△Ez = −¨Ez/c2 = ω2Ez/c2
and
−△Bz = −¨Bz/c2 = ω2Bz/c2.
(9.57)
Because their z-dependence (9.52) is periodic, they are (exercise 9.16) linear
combinations of Jn(

ω2/c2 −k2 ρ)einφei(kz−ωt).
Modes with Bz = 0 are transverse magnetic or TM modes. For them the
boundary conditions (9.54) will be satisﬁed if

ω2/c2 −k2 r is a zero zn,m of
Jn. So the frequency ωn,m(k) of the n, m TM mode is
ωn,m(k) = c

k2 + z2n,m/r2.
(9.58)
Since the ﬁrst zero of a Bessel function is z0,1 ≈2.4048, the minimum frequency
ω0,1(0) = c z0,1/r ≈2.4048 c/r occurs for n = 0 and k = 0. If the radius of the
wave-guide is r = 1 cm, then ω0,1(0)/2π is about 11 GHz, which is a microwave
frequency with a wave-length of 2.6 cm. In terms of the frequencies (9.58), the
ﬁeld of a pulse moving in the +z-direction is
Ez(ρ, φ, z, t) =
∞

n=0
∞

m=1
 ∞
0
cn,m(k) Jn
zn,m ρ
r

einφ exp i

kz −ωn,m(k)t

dk.
(9.59)
Modes with Ez = 0 are transverse electric or TE modes. For them the bound-
ary conditions (9.54) will be satisﬁed (exercise 9.18) if

ω2/c2 −k2 r is a zero
z′
n,m of J′
n. Their frequencies are ωn,m(k) = c

k2 + z′2n,m/r2. Since the ﬁrst zero
of a ﬁrst derivative of a Bessel function is z′
1,1 ≈1.8412, the minimum frequency
ω1,1(0) = c z′
1,1/r ≈1.8412 c/r occurs for n = 1 and k = 0. If the radius of the
wave-guide is r = 1 cm, then ω1,1(0)/2π is about 8.8 GHz, which is a microwave
frequency with a wave-length of 3.4 cm.
Example 9.4 (Cylindrical cavity)
The modes of an evacuated, perfectly con-
ducting cylindrical cavity of radius r and height h are like those of a cylindrical
wave-guide (example 9.3) but with extra boundary conditions
334

9.2 SPHERICAL BESSEL FUNCTIONS OF THE FIRST KIND
Bz(ρ, φ, 0, t) = 0
and
Bz(ρ, φ, h, t) = 0,
Eρ(ρ, φ, 0, t) = 0
and
Eρ(ρ, φ, h, t) = 0,
Eφ(ρ, φ, 0, t) = 0
and
Eφ(ρ, φ, h, t) = 0
(9.60)
at the two ends of the cylinder. If ℓis an integer and if

ω2/c2 −π2ℓ2/h2 r is a
zero z′
n,m of J′
n, then the TE ﬁelds Ez = 0 and
Bz = Jn(z′
n,m ρ/r) einφ sin(πℓz/h) e−iωt
(9.61)
satisfy both these (9.60) boundary conditions at z = 0 and h and those (9.54)
at ρ = r as well as the separable wave equations (9.57). The frequencies of the
resonant TE modes then are ωn,m,ℓ= c

z′2n,m/r2 + π2ℓ2/h2.
The TM modes are Bz = 0 and
Ez = Jn(zn,m ρ/r) einφ cos(πℓz/h) e−iωt
(9.62)
with resonant frequencies ωn,m,ℓ= c

z2n,m/r2 + π2ℓ2/h2.
9.2 Spherical Bessel functions of the ﬁrst kind
If in Bessel’s equation (9.4), one sets n = ℓ+ 1/2 and jℓ= √π/2x Jℓ+1/2, then
one may show (exercise 9.21) that
x2 j′′
ℓ(x) + 2x j′
ℓ(x) + [x2 −ℓ(ℓ+ 1)] jℓ(x) = 0,
(9.63)
which is the equation for the spherical Bessel function jℓ.
We saw in example 6.6 that by setting V(r, θ, φ) = Rk,ℓ(r) ℓ,m(θ) m(φ) we
could separate the variables of Helmholtz’s equation −△V = k2V in spherical
coordinates
r2△V
V
=
(r2R′
k,ℓ)′
Rk,ℓ
+
(sin θ ′
ℓ,m)′
sin θ ℓ,m
+
′′
sin2 θ 
= −k2r2.
(9.64)
Thus if m(φ) = eimφ so that ′′
m = −m2m, and if ℓ,m satisﬁes the associated
Legendre equation (8.91)
sin θ

sin θ ′
ℓ,m
′ + [ℓ(ℓ+ 1) sin2 θ −m2] ℓ,m = 0
(9.65)
then the product V(r, θ, φ) = Rk,ℓ(r) ℓ,m(θ) m(φ) will obey (9.64) because in
view of (9.63) the radial function Rk,ℓ(r) = jℓ(kr) satisﬁes
(r2R′
k,ℓ)′ + [k2r2 −ℓ(ℓ+ 1)]Rk,ℓ= 0.
(9.66)
In terms of the spherical harmonic Yℓ,m(θ, φ) = ℓ,m(θ) m(φ), the solution is
V(r, θ, φ) = jℓ(kr) Yℓ,m(θ, φ).
335

BESSEL FUNCTIONS
Rayleigh’s formula gives the spherical Bessel function
jℓ(x) ≡
& π
2x Jℓ+1/2(x)
(9.67)
as the ℓth derivative of sin x/x
jℓ(x) = (−1)ℓxℓ
1
x
d
dx
ℓsin x
x

(9.68)
(Lord Rayleigh (John William Strutt), 1842–1919). In particular, j0(x) = sin x/x
and j1(x) = sin x/x2−cos x/x. Rayleigh’s formula leads to the recursion relation
(exercise 9.22)
jℓ+1(x) = ℓ
xjℓ(x) −j ′
ℓ(x),
(9.69)
with which one can show (exercise 9.23) that the spherical Bessel functions as
deﬁned by Rayleigh’s formula do satisfy their differential equation (9.66) with
x = kr.
The spherical Bessel functions jℓ(kr) satisfy the self-adjoint Sturm–Liouville
(6.333) equation (9.66)
−r2j′′
ℓ−2rj′
ℓ+ ℓ(ℓ+ 1)jℓ= k2r2jℓ
(9.70)
with eigenvalue k2 and weight function ρ = r2. If jℓ(zℓ,n) = 0, then the functions
jℓ(kr) = jℓ(zℓ,nr/a) vanish at r = a and form an orthogonal basis
 a
0
jℓ(zℓ,nr/a) jℓ(zℓ,mr/a) r2 dr = a3
2 j2
ℓ+1(zℓ,n) δn,m
(9.71)
for a self-adjoint system on the interval [0, a]. Moreover, since the eigenvalues
k2
ℓ,n = z2
ℓ,n/a2 ≈(nπ)2/a2 →∞as n →∞, the eigenfunctions jℓ(zℓ,nr/a) also
are complete in the mean (section 6.35).
On an inﬁnite interval, the analogous relation is
 ∞
0
jℓ(kr) jℓ(k′r) r2 dr = π
2k2 δ(k −k′).
(9.72)
If we write the spherical Bessel function j0(x) as the integral
j0(z) = sin z
z
= 1
2
 1
−1
eizx dx
(9.73)
336

9.2 SPHERICAL BESSEL FUNCTIONS OF THE FIRST KIND
and use Rayleigh’s formula (9.68), we may ﬁnd an integral for jℓ(z)
jℓ(z) = (−1)ℓzℓ
1
z
d
dz
ℓsin z
z

= (−1)ℓzℓ
1
z
d
dz
ℓ1
2
 1
−1
eizx dx
= zℓ
2
 1
−1
(1 −x2)ℓ
2ℓℓ!
eizx dx = (−i)ℓ
2
 1
−1
(1 −x2)ℓ
2ℓℓ!
dℓ
dxℓeizx dx
= (−i)ℓ
2
 1
−1
eizx dℓ
dxℓ
(x2 −1)ℓ
2ℓℓ!
dx = (−i)ℓ
2
 1
−1
Pℓ(x) eizx dx
(9.74)
(exercise 9.24) that contains Rodrigues’s formula (8.8) for the Legendre polyno-
mial Pℓ(x). With z = kr and x = cos θ, this formula
iℓjℓ(kr) = 1
2
 1
−1
Pℓ(cos θ)eikr cos θ d cos θ
(9.75)
and the Fourier–Legendre expansion (8.32) give
eikr cos θ =
∞

ℓ=0
2ℓ+ 1
2
Pℓ(cos θ)
 1
−1
Pℓ(cos θ′) eikr cos θ′ d cos θ′
=
∞

ℓ=0
(2ℓ+ 1) Pℓ(cos θ) iℓjℓ(kr).
(9.76)
If θ, φ and θ′, φ′ are the polar angles of the vectors r and k, then by using the
addition theorem (8.121) we get
eik·r =
∞

ℓ=0
4π iℓjℓ(kr) Yℓ,m(θ, φ) Y∗
ℓ,m(θ′, φ′).
(9.77)
The series expansion (9.1) for Jn and the deﬁnition (9.67) of jℓgive us for
small |ρ| ≪1 the approximation
jℓ(ρ) ≈ℓ! (2ρ)ℓ
(2ℓ+ 1)! =
ρℓ
(2ℓ+ 1)!!.
(9.78)
To see how jℓ(ρ) behaves for large |ρ| ≫1, we use Rayleigh’s formula (9.68)
to compute j1(ρ) and notice that the derivative d/dρ
j1(ρ) = −d
dρ
sin ρ
ρ

= −cos ρ
ρ
+ sin ρ
ρ2
(9.79)
adds a factor of 1/ρ when it acts on 1/ρ but not when it acts on sin ρ. Thus the
dominant term is the one in which all the derivatives act on the sine, and so for
large |ρ| ≫1, we have approximately
337

BESSEL FUNCTIONS
0
2
4
6
8
10
12
−0.5
0
0.5
ρ
j1(ρ)
0
2
4
6
8
10
12
−0.5
0
0.5
ρ
j2(ρ)
The spherical Bessel function j2(ρ) for ρ
1 and ρ
1
The spherical Bessel function j1(ρ) for ρ
1 and ρ
1
Figure 9.2
Top: plot of j1(ρ) (solid curve) and its approximations ρ/3 for small ρ
(9.78, dashes) and sin(ρ −π/2)/ρ for big ρ (9.80, dot-dash). Bottom: plot of j2(ρ)
(solid curve) and its approximations ρ2/15 for small ρ (9.78, dashed) and sin(ρ −π)/ρ
for big ρ (9.80, dot-dash). The values of ρ at which jℓ(ρ) = 0 are the zeros or roots of
jℓ; we use them to ﬁt boundary conditions.
jℓ(ρ) ≈(−1)ℓ1
ρ
dℓsin ρ
dρℓ
= 1
ρ sin

ρ −ℓπ
2

(9.80)
with an error that falls off as 1/ρ2. The quality of the approximation, which is
exact for ℓ= 0, is illustrated for ℓ= 1 and 2 in Fig. 9.2.
Example 9.5 (Partial waves)
Spherical Bessel functions occur in the wave-
functions of free particles with well-deﬁned angular momentum.
The hamiltonian H0 = p2/2m for a free particle of mass m and the square L2
of the orbital angular momentum operator are both invariant under rotations;
thus they commute with the orbital angular-momentum operator L. Since the
operators H0, L2, and Lz commute with each other, simultaneous eigenstates
|k, ℓ, m⟩of these compatible operators (section 1.30) exist
H0 |k, ℓ, m⟩= p2
2m |k, ℓ, m⟩=
(¯h k)2
2m
|k, ℓ, m⟩,
L2 |k, ℓ, m⟩= ¯h2 ℓ(ℓ+ 1) |k, ℓ, m⟩,
and
Lz |k, ℓ, m⟩= ¯h m |k, ℓ, m⟩.
(9.81)
338

9.2 SPHERICAL BESSEL FUNCTIONS OF THE FIRST KIND
Their wave-functions are products of spherical Bessel functions and spherical
harmonics (8.110)
⟨r|k, ℓ, m⟩= ⟨r, θ, φ|k, ℓ, m⟩=
&
2
π k jℓ(kr) Yℓ,m(θ, φ).
(9.82)
They satisfy the normalization condition
⟨k, ℓ, m|k′, ℓ′, m′⟩= 2kk′
π
 ∞
0
jℓ(kr)jℓ(k′r) r2 dr

Ym∗
ℓ(θ, φ)Ym′
ℓ′ (θ, φ) d
= δ(k −k′) δℓ,ℓ′ δm,m′
(9.83)
and the completeness relation
1 =
 ∞
0
dk
∞

ℓ=0
ℓ

m=−ℓ
|k, ℓ, m⟩⟨k, ℓ, m|.
(9.84)
Their inner products with an eigenstate |k′⟩of a free particle of momentum p′ =
¯hk′ are
⟨k, ℓ, m|k′⟩= iℓ
k δ(k −k′) Ym∗
ℓ(θ′, φ′)
(9.85)
in which the polar coordinates of k′ are θ′, φ′.
Using the resolution (9.84) of the identity operator and the inner-product
formulas (9.82 & 9.85), we recover the expansion (9.77)
eik′·r
(2π)3/2 = ⟨r|k′⟩=
 ∞
0
dk
∞

ℓ=0
ℓ

m=−ℓ
⟨r|k, ℓ, m⟩⟨k, ℓ, m|k′⟩
=
∞

ℓ=0
&
2
π iℓjℓ(kr) Ym
ℓ(θ, φ) Ym∗
ℓ(θ′, φ′).
(9.86)
The small kr approximation (9.78) and the deﬁnition (9.82) tell us that the
probability that a particle with angular momentum ¯hℓabout the origin has r =
|r| ≪1/k is
P(r) = 2k2
π
 r
0
j2
ℓ(kr)r2dr ≈
2
π((2ℓ+ 1)!!)2
 r
0
(kr)2ℓ+2dr = (4ℓ+ 6)(kr)2ℓ+3
π((2ℓ+ 3)!!)2k ,
(9.87)
which is very small for big ℓand tiny k. So a short-range potential can only affect
partial waves of low angular momentum. When physicists found that nuclei scat-
tered low-energy hadrons into s-waves, they knew that the range of the nuclear
force was short, about 10−15m.
If the potential V(r) that scatters a particle is of short range, then at big r the
radial wave-function uℓ(r) of the scattered wave should look like that of a free
particle (9.86), which by the big kr approximation (9.80) is
339

BESSEL FUNCTIONS
u(0)
ℓ(r) = jℓ(kr) ≈sin(kr −ℓπ/2)
kr
=
1
2ikr

ei(kr−ℓπ/2) −e−i(kr−ℓπ/2)
.
(9.88)
Thus at big r the radial wave-function uℓ(r) differs from u(0)
ℓ(r) only by a phase
shift δℓ
uℓ(r) ≈sin(kr −ℓπ/2 + δℓ)
kr
=
1
2ikr

ei(kr−ℓπ/2+δℓ) −e−i(kr−ℓπ/2+δℓ)
.
(9.89)
The phase shifts determine the cross-section σ to be (Cohen-Tannoudji et al.,
1977, chap. VIII)
σ = 4π
k2
∞

ℓ=0
(2ℓ+ 1) sin2 δℓ.
(9.90)
If the potential V(r) is negligible for r > r0, then for momenta k ≪1/r0 the
cross-section is σ ≈4π sin2 δ0/k2.
Example 9.6 (Quantum dots)
The active region of some quantum dots is a
CdSe sphere whose radius a is less than 2 nm. Photons from a laser excite
electron–hole pairs, which ﬂuoresce in nanoseconds.
I will model a quantum dot simply as an electron trapped in a sphere of radius
a. Its wave-function ψ(r, θ, φ) satisﬁes Schrödinger’s equation
−¯h2
2m △ψ = Eψ
(9.91)
with the boundary condition ψ(a, θ, φ) = 0. With k2 = 2mE/¯h2 = z2
ℓ,n/a2, the
unnormalized eigenfunctions are
ψn,ℓ,m(r, θ, φ) = jℓ(zℓ,nr/a) Yℓ,m(θ, φ) θ(a −r),
(9.92)
in which the Heaviside function θ(a −r) makes ψ vanish for r > a, and ℓand m
are integers with −ℓ≤m ≤ℓbecause ψ must be single valued for all angles θ
and φ.
The zeros zℓ,n of jℓ(x) ﬁx the energy levels as En,ℓ,m = (¯hzℓ,n/a)2/2m. Since
z0,n = nπ, the ℓ= 0 levels are En,0,0 = (¯hnπ/a)2/2m. If the coupling to a photon
is via a term like p · A, then one expects ℓ= 1. The energy gap from the n, ℓ= 1
state to the n = 1, ℓ= 0 ground state thus is
En = En,1,0 −E1,0,0 = (z2
1,n −π2)
¯h2
2ma2 .
(9.93)
Inserting factors of c2 and using ¯hc = 197 eV nm, and mc2 = 0.511 MeV, we
ﬁnd from the zero z1,2 = 7.72525 that E2 = 1.89 (nm/a)2 eV, which is red
light if a = 1 nm. The next zero z1,3 = 10.90412 gives E3 = 4.14 (nm/a)2
eV, which is in the visible if 1.2 < a < 1.5 nm. The Mathematica command
Do[Print[N[BesselJZero[1.5, k]]], {k, 1, 5, 1}] gives the ﬁrst ﬁve zeros of j1(x) to
six signiﬁcant ﬁgures.
340

9.3 BESSEL FUNCTIONS OF THE SECOND KIND
9.3 Bessel functions of the second kind
In section 7.5 we derived integral representations (7.55 & 7.56) for the Han-
kel functions H(1)
λ (z) and H(2)
λ (z) for Re z > 0. One may analytically continue
them (Courant and Hilbert, 1955, chap. VII) to the upper
H(1)
λ (z) = 1
πi e−iλ/2
 ∞
−∞
eiz cosh x−λx dx,
Imz ≥0
(9.94)
and lower
H(2)
λ (z) = −1
πi e+iλ/2
 ∞
−∞
e−iz cosh x−λx dx,
Imz ≤0
(9.95)
half z-planes. When both z = ρ and λ = ν are real, the two Hankel functions
are complex conjugates of each other
H(1)
ν (ρ) = H(2)∗
ν
(ρ).
(9.96)
Hankel functions, called Bessel functions of the third kind, are linear combina-
tions of Bessel functions of the ﬁrst Jλ(z) and second Yλ(z) kind
H(1)
λ (z) = Jλ(z) + iYλ(z),
H(2)
λ (z) = Jλ(z) −iYλ(z).
(9.97)
Bessel functions of the second kind are also called Neumann functions; the sym-
bols Yλ(z) = Nλ(z) refer to the same function. They are inﬁnite at z = 0 as
illustrated in Fig. 9.3.
When z = ix is imaginary, we get the modiﬁed Bessel functions
Iα(x) = i−αJα(ix) =
∞

m=0
1
m! (m + α + 1)
x
2
2m+α
,
Kα(x) = π
2 iα+1H(1)
α (ix) =
 ∞
0
e−x cosh t cosh αt dt.
(9.98)
Some simple cases are
I−1/2(z) =
&
2
πz cosh z, I1/2(z) =
&
2
πz sinh z, and K1/2(z) =
& π
2ze−z.
(9.99)
When do we need to use these functions? If we are representing functions
that are ﬁnite at the origin ρ = 0, then we don’t need them. But if the point
341

BESSEL FUNCTIONS
0
2
4
6
8
10
12
−1
−0.5
0
0.5
ρ
The Bessel functions of the second kind Y0(ρ), Y1(ρ), Y2(ρ)
2
4
6
8
10
12
14
−1
−0.5
0
0.5
ρ
The Bessel functions of the second kind Y3(ρ), Y4(ρ), Y5(ρ)
Figure 9.3
Top: Y0(ρ) (solid curve), Y1(ρ) (dot-dash), and Y2(ρ) (dashed) for 0 <
ρ < 12. Bottom: Y3(ρ) (solid curve), Y4(ρ) (dot-dash), and Y5(ρ) (dashed) for 2 <
ρ < 14. The points at which Bessel functions cross the ρ-axis are called zeros or roots;
we use them to satisfy boundary conditions.
ρ = 0 lies outside the region of interest or if the function we are representing is
inﬁnite at that point, then we do need the Yν(ρ)s.
Example 9.7 (Coaxial wave-guides)
An ideal coaxial wave-guide is perfectly
conducting for ρ < r0 and ρ > r, and the waves occupy the region r0 < ρ < r.
Since points with ρ = 0 are not in the physical domain of the problem, the
electric ﬁeld E(ρ, φ) exp(i(kz −ωt)) is a linear combination of Bessel functions
of the ﬁrst and second kinds with
Ez(ρ, φ) =
#
a Jn(

ω2/c2 −k2 ρ) + b Yn(

ω2/c2 −k2 ρ)
$
(9.100)
in the notation of example 9.3. A similar equation represents the magnetic
ﬁeld Bz. The ﬁelds E and B obey the equations and boundary conditions of
example 9.3 as well as
Ez(r0, φ) = 0,
Eφ(r0, φ) = 0,
and
Bρ(r0, φ) = 0
(9.101)
342

9.4 SPHERICAL BESSEL FUNCTIONS OF THE SECOND KIND
at ρ = r0. In TM modes with Bz = 0, one may show (exercise 9.27) that the
boundary conditions Ez(r0, φ) = 0 and Ez(r, φ) = 0 can be satisﬁed if
Jn(x) Yn(vx) −Jn(vx) Yn(x) = 0
(9.102)
in which v = r/r0 and x =

ω2/c2 −k2 r0. One can use the Matlab code
n = 0.; v = 10.;
f=@(x)besselj(n,x).*bessely(n,v*x)-besselj(n,v*x).
*bessely(n,x)
x=linspace(0,5,1000);
figure
plot(x,f(x)) % we use the figure to guess at the roots
grid on
options=optimset(’tolx’,1e-9);
fzero(f,0.3) % we tell fzero to look near 0.3
fzero(f,0.7)
fzero(f,1)
to ﬁnd that for n = 0 and v = 10, the ﬁrst three solutions are x0,1 = 0.3314,
x0,2 = 0.6858, and x0,3 = 1.0377. Setting n = 1 and adjusting the guesses
in the code, one ﬁnds x1,1 = 0.3941, x1,2 = 0.7331, and x1,3 = 1.0748. The
corresponding dispersion relations are ωn,i(k) = c

k2 + x2
n,i/r2
0.
9.4 Spherical Bessel functions of the second kind
Spherical Bessel functions of the second kind are deﬁned as
yℓ(ρ) =
& π
2ρ Yℓ+1/2(ρ)
(9.103)
and Rayleigh formulas express them as
yℓ(ρ) = (−1)ℓ+1ρℓ

d
ρ dρ
ℓcos ρ
ρ

.
(9.104)
The term in which all the derivatives act on the cosine dominates at big ρ
yℓ(ρ) ≈(−1)ℓ+1 1
ρ
dℓcos ρ
dρℓ
= −cos (ρ −ℓπ/2) /ρ.
(9.105)
The second kind of spherical Bessel functions at small ρ are approximately
yℓ(ρ) ≈−(2ℓ−1)!!/ρℓ+1.
(9.106)
They all are inﬁnite at x = 0 as illustrated in Fig. 9.4.
343

BESSEL FUNCTIONS
1
2
3
4
5
6
7
8
9
10
11
12
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
ρ
1
2
3
4
5
6
7
8
9
10
11
12
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
ρ
The spherical Bessel function y2(ρ) forρ
1 and ρ
1
The spherical Bessel function y1(ρ) for ρ
1 and ρ
1
Figure 9.4
Top: plot of y1(ρ) (solid curve) and its approximations −1/ρ2 for small
ρ (9.106, dot-dash) and −cos(ρ −π/2)/ρ for big ρ (9.105, dashed). Bottom: plot of
y2(ρ) (solid curve) and its approximations −3/ρ3 for small ρ (9.106, dot-dash) and
−cos(ρ −π)/ρ for big ρ (9.105, dashed). The values of ρ at which yℓ(ρ) = 0 are
the zeros or roots of yℓ; we use them to ﬁt boundary conditions. All six plots run from
ρ = 1 to ρ = 12.
Example 9.8 (Scattering off a hard sphere)
In the notation of example 9.5, the
potential of a hard sphere of radius r0 is V(r) = ∞θ(r0 −r) in which θ(x) =
(x + |x|)/2|x| is Heaviside’s function. Since the point r = 0 is not in the physical
region, the scattered wave-function is a linear combination of spherical Bessel
functions of the ﬁrst and second kinds
uℓ(r) = cℓjℓ(kr) + dℓyℓ(kr).
(9.107)
The boundary condition uℓ(kr0) = 0 ﬁxes the ratio vℓ= dℓ/cℓof the constants
cℓand dℓ. Thus for ℓ= 0, Rayleigh’s formulas (9.68 & 9.104) and the boundary
condition say that kr0 u0(r0) = c0 sin(kr0) −d0 cos(kr0) = 0 or d0/c0 = tan kr0.
The s-wave then is u0(kr) = c0 sin(kr −kr0)/(kr cos kr0), which tells us that the
phase shift is δ0(k) = −kr0. By (9.90), the cross-section at low energy is σ ≈4πr2
0
or four times the classical value.
344

EXERCISES
Similarly, one ﬁnds (exercise 9.28) that the p-wave phase shift is
δ1(k) = kr0 cos kr0 −sin kr0
cos kr0 + kr0 sin kr0
.
(9.108)
For kr0 ≪1, we have δ1(k) ≈−(kr0)3/6; more generally the ℓth phase shift
δℓ(k) ≈(kr0)2ℓ+1 for a potential of range r0 at low energy k ≪1/r0.
Further reading
A great deal is known about Bessel functions. Students may ﬁnd Mathematical
Methods for Physics and Engineering (Riley et al., 2006) as well as the clas-
sics A Treatise on the Theory of Bessel Functions (Watson, 1995), A Course of
Modern Analysis (Whittaker and Watson, 1927, chap. XVII), and Methods of
Mathematical Physics (Courant and Hilbert, 1955) of special interest.
Exercises
9.1
Show that the series (9.1) for Jn(ρ) satisﬁes Bessel’s equation (9.4).
9.2
Show that the generating function exp(z(u −1/u)/2) for the Bessel functions
is invariant under the substitution u →−1/u.
9.3
Use the invariance of exp(z(u−1/u)/2) under u →−1/u to show that J−n(z) =
(−1)nJn(z).
9.4
By writing the generating function (9.5) as the product of the exponentials
exp(zu/2) and exp(−z/2u), derive the expansion
exp
 z
2

u −u−1
=
∞

m=−n
∞

n=0
 z
2
m+n
um+n
(m + n)!

−z
2
n u−n
n! .
(9.109)
9.5
From this expansion (9.109) of the generating function (9.5), derive the
power-series expansion (9.1) for Jn(z).
9.6
In the formula (9.5) for the generating function exp(z(u −1/u)/2), replace u
by exp iθ and then derive the integral representation (9.6) for Jn(z).
9.7
From the general integral representation (9.6) for Jn(z), derive the two integral
formulas (9.7) for J0(z).
9.8
Show that the integral representations (9.6 & 9.7) imply that for any integer
n ̸= 0, Jn(0) = 0, while J0(0) = 1.
9.9
By differentiating the generating function (9.5) with respect to u and identify-
ing the coefﬁcients of powers of u, derive the recursion relation
Jn−1(z) + Jn+1(z) = 2n
z Jn(z).
(9.110)
345

BESSEL FUNCTIONS
9.10 By differentiating the generating function (9.5) with respect to z and identify-
ing the coefﬁcients of powers of u, derive the recursion relation
Jn−1(z) −Jn+1(z) = 2 J′
n(z).
(9.111)
9.11 Show that the change of variables ρ = ax turns (9.4) into the self-adjoint form
of Bessel’s equation (9.11).
9.12 If y = Jn(ax), then equation (9.11) is (xy′)′ + (xa2 −n2/x)y = 0. Multiply
this equation by xy′, integrate from 0 to b, and so show that if ab = zn,m and
Jn(zn,m) = 0, then
2
 b
0
x J2
n(ax) dx = b2J′2
n (zn,m),
(9.112)
which is the normalization condition (9.14).
9.13 Show that with λ ≡z2/r2
d, the change of variables ρ = zr/rd and u(r) = Jn(ρ)
turns −

r u′′ + n2 u/r = λ r u into (9.25).
9.14 Use the formula (6.42) for the curl in cylindrical coordinates and the vacuum
forms ∇× E = −˙B and ∇× B = ˙E/c2 of the laws of Faraday and Maxwell–
Ampère to derive the ﬁeld equations (9.55).
9.15 Derive equations (9.56) from (9.55).
9.16 Show that Jn(

ω2/c2 −k2 ρ)einφei(kz−ωt) is a traveling-wave solution (9.52) of
the wave equations (9.57).
9.17 Find expressions for the nonzero TM ﬁelds in terms of the formula (9.59)
for Ez.
9.18 Show that the ﬁeld Bz
= Jn(

ω2/c2 −k2 ρ)einφei(kz−ωt) will satisfy the
boundary conditions (9.54) if

ω2/c2 −k2 r is a zero z′
n,m of J′
n.
9.19 Show that if ℓis an integer and if

ω2/c2 −π2ℓ2/h2 r is a zero z′
n,m of J′
n, then
the ﬁelds Ez = 0 and Bz = Jn(z′
n,mρ/r) einφ sin(ℓπz/h) e−iωt satisfy both the
boundary conditions (9.54) at ρ = r and those (9.60) at z = 0 and h as well as
the wave equations (9.57). Hint: use Maxwell’s equations ∇× E = −˙B and
∇× B = ˙E/c2 as in (9.55).
9.20 Show that the resonant frequencies of the TM modes of the cavity of
example 9.4 are ωn,m,ℓ= c

z2n,m/r2 + π2ℓ2/h2.
9.21 By setting n = ℓ+ 1/2 and jℓ= √π/2x Jℓ+1/2, show that Bessel’s equation
(9.4) implies that the spherical Bessel function jℓsatisﬁes (9.63).
9.22 Show that Rayleigh’s formula (9.68) implies the recursion relation (9.69).
9.23 Use the recursion relation (9.69) to show by induction that the spherical
Bessel functions jℓ(x) as given by Rayleigh’s formula (9.68) satisfy their
differential equation (9.66) which with x = kr is
−x2j′′
ℓ−2xj′
ℓ+ ℓ(ℓ+ 1)jℓ= x2jℓ.
(9.113)
Hint: start by showing that j0(x) = sin(x)/x satisﬁes (9.66). This problem
involves some tedium.
346

EXERCISES
9.24 Iterate the trick
d
zdz
 1
−1
eizx dx = i
z
 1
−1
xeizx dx = i
2z
 1
−1
eizx d(x2 −1)
= −i
2z
 1
−1
(x2 −1)deizx = 1
2
 1
−1
(x2 −1)eizxdx
(9.114)
to show that (Schwinger et al., 1998, p. 227)
 d
zdz
ℓ 1
−1
eizx dx =
 1
−1
(x2 −1)ℓ
2ℓℓ!
eizx dx.
(9.115)
9.25 Use the expansions (9.76 and 9.77) to show that the inner product of the ket
|r⟩that represents a particle at r with polar angles θ and φ and the one |k⟩
that represents a particle with momentum ρ = ¯hk with polar angles θ′ and φ′
is, with k · r = kr cos θ,
⟨r|k⟩=
1
(2π)3/2 eikr cos θ =
1
(2π)3/2
∞

ℓ=0
(2ℓ+ 1) Pℓ(cos θ) iℓjℓ(kr)
=
1
(2π)3/2 eik·r =
&
2
π
∞

ℓ=0
iℓjℓ(kr) Yℓ,m(θ, φ) Y∗
ℓ,m(θ′, φ′).
(9.116)
9.26 Show that (−1)ℓdℓsin ρ/dρℓ= sin(ρ −πℓ/2) and so complete the derivation
of the approximation (9.80) for jℓ(ρ) for big ρ.
9.27 In the context of examples 9.3 and 9.7, show that the boundary conditions
Ez(r0, φ) = 0 and Ez(r, φ) = 0 imply (9.102).
9.28 Show that for scattering off a hard sphere of radius r0 as in example 9.8, the
p-wave phase shift is given by (9.108).
347

10
Group theory
10.1 What is a group?
A group G is a set of objects f , g, h, . . . and an operation called multiplication
such that:
1 if f ∈G and g ∈G, the product fg ∈G (closure);
2 if f , g, and h are in G, then f (gh) = (fg)h (associativity);
3 there is an identity e ∈G such that if g ∈G, then ge = eg = g;
4 every g ∈G has an inverse g−1 ∈G such that gg−1 = g−1g = e.
Physical transformations naturally form groups. The product T′ T represents
the transformation T followed by the transformation T′. And both T′′ (T′ T)
and (T′′ T′) T represent the transformation T followed by the transformation
T′ and then by T′′. So transformations are associative. The identity element e
is the null transformation, the one that does nothing. The inverse T−1 is the
transformation that reverses the effect of T. Such a set {T} of transformations
will form a group if any two successive transformations is a transformation in
the set (closure). Closure occurs naturally when the criterion for membership in
the group is that a transformation not change something. For if both T and T′
leave that thing unchanged, then so will their product T′ T.
Example 10.1 (Groups of coordinate transformations)
The set of all trans-
formations that leave invariant the distance from the origin of every point in
n-dimensional space is the group O(n) of rotations and reﬂections. The rotations
in Rn form the group SO(n).
The set of all transformations that leave invariant the spatial difference x −y
between every two points x and y in n-dimensional space is the group of
translations. In this case, group multiplication is vector addition.
348

10.1 WHAT IS A GROUP?
The set of all linear transformations that leave invariant the square of the
Minkowski distance x2
1 + x2
2 + x2
3 −x2
0 between any 4-vector x and the ori-
gin is the Lorentz group (Hermann Minkowski, 1864–1909; Hendrik Lorentz,
1853–1928).
The set of all linear transformations that leave invariant the square of the
Minkowski distance (x1−y1)2+(x2−y2)2+(x3−y3)2−(x0−y0)2 between any two
4-vectors x and y is the Poincaré group, which includes Lorentz transformations
and translations (Henri Poincaré, 1854–1912).
Except for the group of translations, the order of the physical transforma-
tions in these examples matters: the transformation T′ T is not in general the
same as T T′. Such groups are called nonabelian. A group whose elements all
commute
[T′, T] ≡T′ T −T T′ = 0
(10.1)
is said to be abelian (Niels Abel, 1802–1829).
Matrices naturally form groups. Since matrix multiplication is associative,
any set {D} of n × n nonsingular matrices that includes the inverse D−1 of every
matrix in the set as well as the identity matrix I automatically satisﬁes properties
2–4 with group multiplication deﬁned as matrix multiplication. Only property
1, closure under multiplication, is uncertain. A set {D} of matrices will form a
group as long as the product of any two matrices is in the set. As with phys-
ical transformations, one way to ensure closure is to have every matrix leave
something unchanged.
Example 10.2 (Orthogonal groups)
The set of all n × n real matrices that leave
the quadratic form x2
1 + x2
2 + · · · + x2
n unchanged forms the orthogonal group
O(n) of all n × n orthogonal (1.36) matrices (exercises 10.1 & 10.2). The n × n
orthogonal matrices that have unit determinant form the special orthogonal
group SO(n). The group SO(3) describes rotations.
Example 10.3 (Unitary groups)
The set of all n×n complex matrices that leave
invariant the quadratic form x∗
1x1 + x∗
2x2 + · · · + x∗
nxn forms the unitary group
U(n) of all n × n unitary (1.35) matrices (exercises 10.3 & 10.4). Those of unit
determinant form the special unitary group SU(n) (exercise 10.5).
Like SO(3), the group SU(2) represents rotations. The group SU(3) is the sym-
metry group of the strong interactions, quantum chromodynamics. Physicists
have used the groups SU(5) and SO(10) to unify the electro weak and strong
interactions; whether Nature also does so is unclear.
The number of elements in a group is the order of the group. A ﬁnite group is
a group with a ﬁnite number of elements, or equivalently a group of ﬁnite order.
349

GROUP THEORY
Example 10.4 (Z2 and Zn)
The parity group whose elements are 1 and −1
under ordinary multiplication is the ﬁnite group Z2. It is abelian and of order 2.
The group Zn for any positive integer n is made of the phases exp(i2kπ/n) for
k = 1, 2, ..., n. It is abelian and of order n.
A group whose elements g = g({α}) depend continuously upon a set of
parameters αk is a continuous group or a Lie group. Continuous groups are of
inﬁnite order.
A group G of matrices D is compact if the (squared) norm as given by the
trace
Tr

D†D

≤M
(10.2)
is bounded for all the D ∈G.
Example 10.5 (SO(n), O(n), SU(n), and U(n))
The groups SO(n), O(n), SU(n),
and U(n) are continuous Lie groups of inﬁnite order. Since for any matrix D in
one of these groups
Tr

D†D

= TrI = n ≤M
(10.3)
these groups also are compact.
Example 10.6 (Noncompact groups)
The set of all real n × n matrices forms
the general linear group GL(n, R); those of unit determinant form the special lin-
ear group SL(n, R). The corresponding groups of matrices with complex entries
are GL(n, C) and SL(n, C). These four groups have matrix elements that are
unbounded; they are noncompact. They are continuous Lie groups of inﬁnite
order like the orthogonal and unitary groups. The group SL(2, C) represents
Lorentz transformations.
10.2 Representations of groups
If one can associate with every element g of a group G a square matrix D(g) and
have matrix multiplication imitate group multiplication
D(f ) D(g) = D(fg)
(10.4)
for all elements f and g of the group G, then the set of matrices D(g) is said
to form a representation of the group G. If the matrices of the representation
are n × n, then n is the dimension of the representation. The dimension of a
representation also is the dimension of the vector space on which the matrices
act. If the matrices D(g) are unitary D†(g) = D−1(g), then they form a unitary
representation of the group.
350

10.3 REPRESENTATIONS ACTING IN HILBERT SPACE
Compact groups possess ﬁnite-dimensional unitary representations; noncom-
pact groups do not. A group of bounded (10.2) matrices is compact. An abstract
group of elements g({α}) is compact if its space of parameters {α} is closed and
bounded. (A set is closed if the limit of every convergent sequence of its points
lies in the set. A set is open if each of its elements lies in a neighborhood that
lies in the set. For example, the interval [a, b] ≡{x|a ≤x ≤b} is closed, and
(a, b) ≡{x|a < x < b} is open.) The group of rotations is compact, but the
group of translations and the Lorentz group are noncompact.
Every n × n matrix S that is nonsingular (det S ̸= 0) maps any n × n repre-
sentation D(g) of a group G into an equivalent representation D′(g) through the
similarity transformation
D′(g) = S−1D(g)S,
(10.5)
which preserves the law of multiplication
D′(f ) D′(g) = S−1D(f )S S−1D(g)S
= S−1D(f ) D(g)S = S−1D(fg)S = D′(fg).
(10.6)
A proper subspace W of a vector space V is a subspace of lower (but not
zero) dimension. A proper subspace W is invariant under the action of a repre-
sentation D(g) if D(g) maps every vector v ∈W to a vector D(g) v = v′ ∈W. A
representation that has a proper invariant subspace is reducible. A representation
that is not reducible is irreducible.
There is no need to keep track of several equivalent irreducible representa-
tions D, D′, D′′ of any group. So in what follows, we shall choose one of these
equivalent irreducible representations and use it exclusively.
A representation is completely reducible if it is equivalent to a representation
whose matrices are in block-diagonal form
⎛
⎜⎝
D1(g)
0
. . .
0
D2(g)
. . .
...
...
...
⎞
⎟⎠
(10.7)
in which each representation Di(g) irreducible. A representation in block-
diagonal form is said to be a direct sum of the irreducible representations
Di
D1 ⊕D2 ⊕· · · .
(10.8)
10.3 Representations acting in Hilbert space
A symmetry transformation g is a map (1.174) of states ψ →ψ′ that preserves
their inner products
|⟨φ′|ψ′⟩|2 = |⟨φ|ψ⟩|2
(10.9)
351

GROUP THEORY
and so their predicted probabilities. The action of a group G of symmetry trans-
formations g on the Hilbert space of a quantum theory can be represented
either by operators U(g) that are linear and unitary (the usual case) or by ones
K(g) that are antilinear (1.172) and antiunitary (1.173), as in the case of time
reversal. Wigner proved this theorem in the 1930s, and Weinberg improved it
in his 1995 classic (Weinberg, 1995, p. 51) (Eugene Wigner, 1902–1995; Steven
Weinberg, 1933–).
Two operators F1 and F2 that commute F1 F2 = F2 F1 are compatible (1.328).
A set of compatible operators F1, F2, ... is complete if to every set of eigenvalues
there belongs only a single eigenvector (section 1.30).
Example 10.7 (Rotation operators)
Suppose that the hamiltonian H, the
square of the angular momentum J2, and its z-component Jz form a complete
set of compatible observables, so that the identity operator can be expressed as
a sum over the eigenvectors of these operators
I =

E,j,m
|E, j, m⟩⟨E, j, m|.
(10.10)
Then the matrix element of a unitary operator U(g) between two states |ψ⟩
and |φ⟩is
⟨φ|U(g)|ψ⟩= ⟨φ|

E′,j′,m′
|E′, j′, m′⟩⟨E′, j′, m′| U(g)

E,j,m
|E, j, m⟩⟨E, j, m|ψ⟩.
(10.11)
Let H and J2 be invariant under the action of U(g) so that U†(g)HU(g) = H
and U†(g)J2U(g) = J2. Then HU(g) = U(g)H and J2U(g) = U(g)J2, and so if
H|E, j, m⟩= E|E, j, m⟩and J2|E, j, m⟩= j( j + 1)|E, j, m⟩, we have
HU(g)|E, j, m⟩= U(g)H|E, j, m⟩= EU(g)|E, j, m⟩,
J2U(g)|E, j, m⟩= U(g)J2|E, j, m⟩= j( j + 1)U(g)|E, j, m⟩.
(10.12)
Thus U(g) can not change E or j, and so
⟨E′, j′, m′|U(g)|E, j, m⟩= δE′Eδj′j⟨m′|U(g)|m⟩= δE′Eδj′jD( j)
m′m(g).
(10.13)
The matrix element (10.11) then is a single sum over E and j in which the
irreducible representations D( j)
m′m(g) of the rotation group SU(2) appear
⟨φ|U(g)|ψ⟩=

E,j,m′,m
⟨φ|E, j, m′⟩D( j)
m′m(g)⟨E, j, m|ψ⟩.
(10.14)
This is how the block-diagonal form (10.7) usually appears in calculations. The
matrices D( j)
m′m(g) inherit the unitarity of the operator U(g).
352

10.4 SUBGROUPS
10.4 Subgroups
If all the elements of a group S also are elements of a group G, then S is a
subgroup of G. Every group G has two trivial subgroups – the identity element
e and the whole group G itself. Many groups have more interesting subgroups.
For example, the rotations about a ﬁxed axis is an abelian subgroup of the group
of all rotations in three-dimensional space.
A subgroup S ⊂G is an invariant subgroup if every element s of the subgroup
S is left inside the subgroup under the action of every element g of the whole
group G, that is, if
g−1s g = s′ ∈S
for all
g ∈G.
(10.15)
This condition often is written as g−1Sg = S for all g ∈G or as
S g = g S
for all g ∈G.
(10.16)
Invariant subgroups also are called normal subgroups.
A set C ⊂G is called a conjugacy class if it’s invariant under the action of the
whole group G, that is, if Cg = g C or
g−1C g = C
for all g ∈G.
(10.17)
A subgroup that is the union of a set of conjugacy classes is invariant.
The center C of a group G is the set of all elements c ∈G that commute with
every element g of the group, that is, their commutators
[c, g] ≡cg −gc = 0
(10.18)
vanish for all g ∈G.
Example 10.8 (Centers are abelian subgroups)
Does the center C always form
an abelian subgroup of its group G? The product g1g2 of any two elements g1
and g2 of the center must commute with every element g of G since g1g2g =
g1gg2 = gg1g2. So the center is closed under multiplication. The identity element
e commutes with every g ∈G, so e ∈C. If g′ ∈C, then g′g = gg′ for all g ∈G,
and so multiplication of this equation from the left and the right by g′−1 gives
gg′−1 = g′−1g, which shows that g′−1 ∈C.
So the center of any group always is one of its abelian invariant subgroups.
The center may be trivial, however, consisting either of the identity or of
the whole group. But a group with a nontrivial center can not be simple or
semisimple (section 10.23).
353

GROUP THEORY
10.5 Cosets
If H is a subgroup of a group G, then for every element g ∈G the set of elements
Hg ≡{hg|h ∈H, g ∈G} is a right coset of the subgroup H ⊂G. (Here ⊂means
is a subset of or equivalently is contained in.)
If H is a subgroup of a group G, then for every element g ∈G the set of
elements gH is a left coset of the subgroup H ⊂G.
The number of elements in a coset is the same as the number of elements of
H, which is the order of H.
An element g of a group G is in one and only one right coset (and in one
and only one left coset) of the subgroup H ⊂G. For suppose instead that g
were in two right cosets g ∈Hg1 and g ∈Hg2, so that g = h1g1 = h2g2
for suitable h1, h2 ∈H and g1, g2 ∈G. Then since H is a (sub)group, we
have g2 = h−1
2 h1g1 = h3g1, which says that g2 ∈Hg1. But this means that
every element of hg2 ∈Hg2 is of the form hg2 = hh3g1 = h4g1 ∈Hg1.
So every element of hg2 ∈Hg2 is in Hg1: the two right cosets are identical,
Hg1 = Hg2.
The right (or left) cosets are the points of the quotient coset space G/H.
If H is an invariant subgroup of G, then by deﬁnition (10.16) Hg =
gH for all g ∈G, and so the left cosets are the same sets as the right cosets.
In this case, the coset space G/H is itself a group with multiplication deﬁned by
(Hg1) (Hg2) =

hig1hjg2|hi, hj ∈H

=
3
hig1hjg−1
1 g1g2|hi, hj ∈H
4
= {hihkg1g2|hi, hk ∈H}
= {hℓg1g2|hℓ∈H} = Hg1g2,
(10.19)
which is the multiplication rule of the group G. This group G/H is called the
factor group of G by H.
10.6 Morphisms
An isomorphism is a one-to-one map between groups that respects their multi-
plication laws. For example, the relation between two equivalent representations
D′(g) = S−1D(g)S
(10.20)
is an isomorphism (exercise 10.8). An automorphism is an isomorphism between
a group and itself. The map gi →g gi g−1 is one to one because g g1 g−1 =
g g2 g−1 implies that g g1 = g g2, and so that g1 = g2. This map also preserves
the law of multiplication since g g1 g−1 g g2 g−1 = g g1 g2 g−1. So the map
G →gGg−1
(10.21)
354

10.7 SCHUR’S LEMMA
is an automorphism. It is called an inner automorphism because g is an ele-
ment of G. An automorphism not of this form (10.21) is called an outer
automorphism.
10.7 Schur’s lemma
Part 1 If D1 and D2 are inequivalent, irreducible representations of a group G,
and if D1(g)A = AD2(g) for some matrix A and for all g ∈G, then the matrix
A must vanish, A = 0.
Proof First suppose that A annihilates some vector |x⟩, that is, A|x⟩= 0. Let
P be the projection operator into the subspace that A annihilates, which is of at
least one dimension. This subspace, incidentally, is called the null space N (A)
or the kernel of the matrix A. The representation D2 must leave this null space
N (A) invariant since
AD2(g)P = D1(g)AP = 0.
(10.22)
If N (A) were a proper subspace, then it would be a proper invariant subspace
of the representation D2, and so D2 would be reducible, which is contrary to
our assumption that D1 and D2 are irreducible. So the null space N (A) must be
the whole space upon which A acts, that is, A = 0.
A similar argument shows that if ⟨y|A = 0 for some bra ⟨y|, then A = 0.
So either A is zero or it annihilates no ket and no bra. In the latter case, A
must be square and invertible, which would imply that D2(g) = A−1D1(g)A,
that is, that D1 and D2 are equivalent representations, which is contrary
to our assumption that they are inequivalent. The only way out is that A
vanishes.
Part 2 If for a ﬁnite-dimensional, irreducible representation D(g) of a group
G, we have D(g)A = AD(g) for some matrix A and for all g ∈G, then A = cI.
That is, any matrix that commutes with every element of a ﬁnite-dimensional,
irreducible representation must be a multiple of the identity matrix.
Proof Every square matrix A has at least one eigenvector |x⟩and eigenvalue
c so that A|x⟩= c|x⟩because its characteristic equation det(A −cI) = 0 always
has at least one root by the fundamental theorem of algebra (5.73). So the null
space N (A −cI) has dimension greater than zero. The assumption D(g)A =
AD(g) for all g ∈G implies that D(g)(A −cI) = (A −cI)D(g) for all g ∈
G. Let P be the projection operator onto the null space N (A −cI). Then we
have (A −cI)D(g)P = D(g)(A −cI)P = 0 for all g ∈G, which implies that
D(g)P maps vectors into the null space N (A −cI). This null space therefore
is invariant under D(g), which means that D is reducible unless the null space
N (A −cI) is the whole space. Since by assumption D is irreducible, it follows
that N (A −cI) is the whole space, that is, that A = cI (Issai Schur, 1875–
1941).
355

GROUP THEORY
Example 10.9 (Schur, Wigner, and Eckart)
Suppose an arbitrary observable O
is invariant under the action of the rotation group SU(2) represented by unitary
operators U(g) for g ∈SU(2)
U†(g)OU(g) = O
or
[O, U(g)] = 0.
(10.23)
These unitary rotation operators commute with the square J2 of the angu-
lar momentum [J2, U] = 0. Suppose that they also leave the hamiltonian H
unchanged [H, U] = 0. Then as shown in example 10.7, the state U|E, j, m⟩is a
sum of states all with the same values of j and E. It follows that

m′
⟨E, j, m|O|E′, j′, m′⟩⟨E′, j′, m′|U(g)|E′, j′, m′′⟩
=

m′
⟨E, j, m|U(g)|E, j, m′⟩⟨E, j, m′|O|E′, j′, m′′⟩
(10.24)
or more simply in view of (10.13)

m′
⟨E, j, m|O|E′, j′, m′⟩Dj′(g)m′m′′ =

m′
D( j)(g)mm′⟨E, j, m′|O|E′, j′, m′′⟩.
(10.25)
Now Part 1 of Schur’s lemma tells us that the matrix ⟨E, j, m|O|E′, j′, m′⟩must
vanish unless the representations are equivalent, which is to say unless j = j′. So
we have

m′
⟨E, j, m|O|E′, j, m′⟩Dj(g)m′m′′
=

m′
D( j)(g)mm′⟨E, j, m′|O|E′, j, m′′⟩.
(10.26)
Now Part 2 of Schur’s lemma tells us that the matrix ⟨E, j, m|O|E′, j, m′⟩must be
a multiple of the identity. Thus the symmetry of O under rotations simpliﬁes the
matrix element to
⟨E, j, m|O|E′, j′, m′⟩= δjj′δmm′Oj(E, E′).
(10.27)
This result is a special case of the Wigner–Eckart theorem (Eugene Wigner,
1902–1995; Carl Eckart, 1902–1973).
10.8 Characters
Suppose the n × n matrices Dij(g) form a representation of a group G ∋g. The
character χD(g) of the matrix D(g) is the trace
χD(g) = TrD(g) =
n

i=1
Dii(g).
(10.28)
356

10.9 TENSOR PRODUCTS
Traces are cyclic, that is, TrABC = TrBCA = TrCAB. So if two representations
D and D′ are equivalent, so that D′(g) = S−1D(g)S, then they have the same
characters because
χD′(g) = TrD′(g) = Tr

S−1D(g)S

= Tr

D(g)SS−1
= TrD(g) = χD(g).
(10.29)
If two group elements g1 and g2 are in the same conjugacy class, that is,
if g2 = gg1g−1 for all g ∈G, then they have the same character in a given
representation D(g) because
χD(g2) = TrD(g2) = TrD(gg1g−1) = Tr

D(g)D(g1)D(g−1)

= Tr

D(g1)D−1(g)D(g)

= TrD(g1) = χD(g1).
(10.30)
10.9 Tensor products
Suppose D1(g) is a k-dimensional representation of a group G, and D2(g) is
an n-dimensional representation of the same group. Suppose the vectors |ℓ⟩
for ℓ= 1, . . . , k are the basis vectors of the k-dimensional space Vk on which
D1(g) acts, and that the vectors |m⟩for m = 1, . . . , n are the basis vectors of
the n-dimensional space Vn on which D2(g) acts. The k × n vectors |ℓ, m⟩are
basis vectors for the kn-dimensional tensor-product space Vkn. The matrices
DD1⊗D2(g) deﬁned as
⟨ℓ′, m′|DD1⊗D2(g)|ℓ, m⟩= ⟨ℓ′|D1(g)|ℓ⟩⟨m′|D2(g)|m⟩
(10.31)
act in this kn-dimensional space Vkn and form a representation of the group
G; this tensor-product representation usually is reducible. Many tricks help
one to decompose reducible tensor-product representations into direct sums
of irreducible representations (Georgi, 1999, p. 309).
Example 10.10 (Adding angular momenta)
The addition of angular momenta
illustrates both the tensor product and its reduction to a direct sum of irreducible
representations. Let Dj1(g) and Dj2(g) respectively be the (2j1 + 1) × (2j1 + 1)
and the (2j2 + 1) × (2j2 + 1) representations of the rotation group SU(2). The
tensor-product representation DDj1⊗Dj2
⟨m′
1, m′
2|DDj1⊗Dj2|m1, m2⟩= ⟨m′
1|Dj1(g)|m1⟩⟨m′
2|Dj2(g)|m2⟩
(10.32)
is reducible into a direct sum of all the irreducible representations of SU(2) from
Dj1+j2(g) down to D| j1−j2|(g) in integer steps:
DDj1⊗Dj2 = Dj1+j2 ⊕Dj1+j2−1 ⊕· · · ⊕D| j1−j2|+1 ⊕D| j1−j2|,
(10.33)
each irreducible representation occurring once in the direct sum.
357

GROUP THEORY
Example 10.11 (Adding two spins)
When one adds j1 = 1/2 to j2 = 1/2, one
ﬁnds that the tensor-product matrix DD1/2⊗D1/2 is equivalent to the direct sum
D1 ⊕D0
DD1/2⊗D1/2(θ) = S−1
D1(θ)
0
0
D0(θ)

S
(10.34)
where the matrices S, D1, and D0 respectively are 4 × 4, 3 × 3, and 1 × 1.
10.10 Finite groups
A ﬁnite group is one that has a ﬁnite number of elements. The number of
elements in a group is the order of the group.
Example 10.12 (Z2)
The group Z2 consists of two elements e and p with
multiplication rules
ee = e,
ep = p,
pe = p,
pp = e.
(10.35)
Clearly, Z2 is abelian, and its order is 2. The identiﬁcation e →1 and p →−1
gives a 1-dimensional representation of the group Z2 in terms of 1 × 1 matrices,
which are just numbers.
It is tedious to write the multiplication rules as individual equations. Nor-
mally people compress them into a multiplication table like this:
×
e
p
e
e
p
p
p
e
,
(10.36)
A simple generalization of Z2 is the group Zn whose elements may be rep-
resented as exp(i2πm/n) for m = 1, . . . , n. This group is also abelian, and its
order is n.
Example 10.13 (Z3)
The multiplication table for Z3 is
×
e
a
b
e
e
a
b
a
a
b
e
b
b
e
a
,
(10.37)
whichsays that a2 = b, b2 = a, and ab = ba = e.
358

10.11 THE REGULAR REPRESENTATION
10.11 The regular representation
For any ﬁnite group G we can associate an orthonormal vector |gi⟩with each
element gi of the group. So ⟨gi|gj⟩= δij. These orthonormal vectors |gi⟩form a
basis for a vector space whose dimension is the order of the group. The matrix
D(gk) of the regular representation of G is deﬁned to map any vector |gi⟩into
the vector |gkgi⟩associated with the product gkgi
D(gk)|gi⟩= |gkgi⟩.
(10.38)
Since group multiplication is associative, we have
D(gj)D(gk)|gi⟩= D(gj)|gkgi⟩= |gj(gkgi)⟩= |(gjgk)gi)⟩= D(gjgk)|gi⟩. (10.39)
Because the vector |gi⟩was an arbitrary basis vector, it follows that
D(gj)D(gk) = D(gjgk),
(10.40)
which means that the matrices D(g) satisfy the closure criterion (10.4) for their
being a representation of the group G. The matrix D(g) has entries
[D(g)]ij = ⟨gi|D(g)|gj⟩.
(10.41)
The sum of dyadics |gℓ⟩⟨gℓ| over all the elements gℓof a ﬁnite group G is the
unit matrix

gℓ∈G
|gℓ⟩⟨gℓ| = In,
(10.42)
in which n is the order of G, that is, the number of elements in G. So by taking
the m, n matrix element of the multiplication law (10.40), we ﬁnd
[D(gjgk)]m,n = ⟨gm|D(gjgk)|gn⟩= ⟨gm|D(gj)D(gk)|gn⟩
=

gℓ∈G
⟨gm|D(gj)|gℓ⟩⟨gℓ|D(gk)|gn⟩
=

gℓ∈G
[D(gj)]m,ℓ[D(gk)]ℓ,n.
(10.43)
Example 10.14 (Z3’s regular representation)
The regular representation of Z3 is
D(e) =
⎛
⎝
1
0
0
0
1
0
0
0
1
⎞
⎠,
D(a) =
⎛
⎝
0
0
1
1
0
0
0
1
0
⎞
⎠,
D(b) =
⎛
⎝
0
1
0
0
0
1
1
0
0
⎞
⎠
(10.44)
so D(a)2 = D(b), D(b)2 = D(a), and D(a)D(b) = D(b)D(a) = D(e).
359

GROUP THEORY
10.12 Properties of ﬁnite groups
In his book (Georgi, 1999, ch. 1), Georgi proves the following theorems.
1 Every
representation
of
a
ﬁnite
group
is
equivalent
to
a
unitary
representation.
2 Every representation of a ﬁnite group is completely reducible.
3 The irreducible representations of a ﬁnite abelian group are one dimensional.
4 If D(a)(g) and D(b)(g) are two unitary irreducible representations of dimen-
sions na and nb of a group G of N elements g1, . . . , gN, then the functions
&na
N D(a)
jk (g)
(10.45)
are orthonormal and complete in the sense that
na
N
N

j=1
D(a)∗
ik (gj)D(b)
ℓm(gj) = δabδiℓδkm.
(10.46)
5 The order N of a ﬁnite group is the sum of the squares of the dimensions of
its inequivalent irreducible representations
N =

a
n2
a.
(10.47)
Example 10.15 (ZN)
The abelian cyclic group ZN with elements
gj = e2πij/N
(10.48)
has N one-dimensional irreducible representations
D(a)(gj) = e2πiaj/N
(10.49)
for a = 1, 2, . . . , N. Their orthonormality relation (10.46) is the Fourier formula
1
N
N

j=1
e−2πiaj/Ne2πibj/N = δab.
(10.50)
The na are all unity, there are N of them, and the sum of the n2
a is N as required
by the sum rule (10.47).
10.13 Permutations
The permutation group on n objects is called Sn. Permutations are made of
cycles that change the order of some of the n objects. For instance, the permu-
tation (1 2) = (2 1) is a 2-cycle that means x1 →x2 →x1; the unitary operator
360

10.15 LIE ALGEBRA
U((1 2)) that represents it interchanges states like this:
U((1 2))|+, −⟩= U((1 2))|+, 1⟩|−, 2⟩= |−, 1⟩, |+, 2⟩= |−, +⟩.
(10.51)
The 2-cycle (3 4) means x3 →x4 →x3, it changes (a, b, c, d) into (a, b, d, c).
The 3-cycle (1 2 3) = (2 3 1) = (3 1 2) means x1 →x2 →x3 →x1, it changes
(a, b, c, d) into (b, c, a, d). The 4-cycle (1 3 2 4) means x1 →x3 →x2 →x4 →
x1 and changes (a, b, c, d) into (c, d, b, a). The 1-cycle (2) means x2 →x2 and
leaves everything unchanged.
The identity element of Sn is the product of 1-cycles e = (1)(2) . . . (n). The
inverse of the cycle (1 3 2 4) must invert x1 →x3 →x2 →x4 →x1, so it
must be (1 4 2 3), which means x1 →x4 →x2 →x3 →x1 so that it changes
(c, d, b, a) back into (a, b, c, d). Every element of Sn has each integer from 1 to n
in one and only one cycle. So an arbitrary element of Sn with ℓk k-cycles must
satisfy
n

k=1
k ℓk = n.
(10.52)
10.14 Compact and noncompact Lie groups
Imagine rotating an object repeatedly. Notice that the biggest rotation is by an
angle of ±π about some axis. The possible angles form a circle; the space of
parameters is a circle. The parameter space of a compact group is compact –
closed and bounded. The rotations form a compact group.
Now consider the translations. Imagine moving a pebble to the Sun, then
moving it to the next-nearest star, then moving it to the nearest galaxy. If space
is ﬂat, then there is no limit to how far one can move a pebble. The param-
eter space of a noncompact group is not compact. The translations form a
noncompact group.
We’ll see that compact Lie groups possess unitary representations, with N×N
unitary matrices D(α), while noncompact ones don’t.
10.15 Lie algebra
Continuous groups can be very complicated. So one uses not only algebra but
also calculus, and one studies the simplest part of the group – the elements
g(dα) that are near the identity e = g(0) for which all αa = 0.
If D(g({αa})) is a representation of a Lie group with parameters {αa}, it gets
tedious to write D(g({αa})) over and over. So instead one writes g(α) = g({αa})
and
D(α) = D(g(α)) = D(g({αa}))
(10.53)
leaving out the explicit mentions both of g and of {αa}.
361

GROUP THEORY
Any matrix D(dα) representing a group element g(dα) that is near the identity
is approximately
D(dα) = I + i

a
ta dαa
(10.54)
where the generators ta of the group are the partial derivatives
ta = −i
∂
∂αa
D(α)

α=0
.
(10.55)
The i is inserted so that if the matrices D(α) are unitary, then the generators are
hermitian matrices
t†
a = ta.
(10.56)
Compact groups have ﬁnite-dimensional, unitary representations and hermi-
tian generators.
Our formulas will look nicer if we adopt the convention that we sum over all
indices that occur twice in a monomial. That is, we drop the summation symbol
when summing over a repeated index so that (10.54) looks like this
D(dα) = I + i ta dαa.
(10.57)
Unless the parameters αa are redundant, the N(G) generators are linearly
independent. They span a vector space over the real numbers and any linear
combination t = αata may be called a generator.
By using the Gram–Schmidt procedure, we may make the N(G) generators
ta orthogonal with respect to the inner product (1.86)
(ta, tb) = Tr

t†
atb

= k δab,
(10.58)
in which k is a nonnegative normalization constant that in general depends
upon the representation. The reason why we don’t normalize the generators
and so make k unity will become apparent shortly.
Since group multiplication is closed, any power gn(dα) ∈G, and so we may
take the limit
D(α) = lim
n→∞Dn(α/n) =

I + iαata
n
n
= eiαata.
(10.59)
This parametrization of a representation of a group is called the exponential
parametrization.
Now for tiny ϵ the product
eiϵtb eiϵta e−iϵtb e−iϵta ≈

1 + iϵ tb −ϵ2
2 t2
b
 
1 + iϵ ta −ϵ2
2 t2
a

×

1 −iϵ tb −ϵ2
2 t2
b
 
1 −iϵ ta −ϵ2
2 t2
a

(10.60)
362

10.15 LIE ALGEBRA
to order ϵ2 is
eiϵtb eiϵta e−iϵtb e−iϵta ≈1 + ϵ2(ta tb −tb ta) = 1 + ϵ2[ta, tb].
(10.61)
Since this product represents a group element near the identity, the commutator
must be a linear combination of generators of order ϵ2
eiϵtb eiϵta e−iϵtb e−iϵta = eiϵ2f c
abtc ≈1 + iϵ2 f c
ab tc.
(10.62)
By matching (10.61) with (10.62) we have
[ta, tb] = i f c
ab tc.
(10.63)
The numbers f c
ab are the structure constants of the group G.
By taking the trace of equation (10.63) multiplied by t†
d and by using the
orthogonality relation (10.58), we ﬁnd
Tr

[ta, tb] t†
d

= i f c
ab Tr

tc t†
d

= i f c
ab k δcd = ik f d
ab,
(10.64)
which implies that the structure constant f c
ab is the trace
f c
ab =

−i
k

Tr

[ta, tb] t†
c

.
(10.65)
Because of the antisymmetry of the commutator [ta, tb], the structure constant
f c
ab is antisymmetric in its lower indices
f c
ab = −f c
ba.
(10.66)
From any n × n matrix A, one may make a hermitian matrix A + A† and an
antihermitian one A −A†. Thus, one may separate the N(G) generators into a
set that are hermitian t(h)
a
and a set that are antihermitian t(ah)
a
. The exponential
of any imaginary linear combination of n × n hermitian generators D(α) =
exp

iαa t(h)
a

is an n × n unitary matrix since
D†(α) = exp

−iαa t†(h)
a

= exp

−iαa t(h)
a

= D−1(α).
(10.67)
A group with only hermitian generators is compact and has ﬁnite-dimensional
unitary representations.
On the other hand, the exponential of any imaginary linear combination of
antihermitian generators D(α) = exp

iαa t(ah)
a

is a real exponential of their
hermitian counterparts i t(ah)
a
whose squared norm
∥D(α)∥2 = Tr

D(α)†D(α)

= Tr

exp

2αa it(ah)
a

(10.68)
grows exponentially and without limit as the parameters αa →± ∞. A group
with some antihermitian generators is noncompact and does not have
363

GROUP THEORY
ﬁnite-dimensional unitary representations. (The unitary representations of the
translations and of the Lorentz and Poincaré groups are inﬁnite dimensional.)
Compact Lie groups have hermitian generators, and so the structure-
constant formula (10.65) reduces in this case to
f c
ab = (−i/k)Tr

[ta, tb] t†
c

= (−i/k)Tr ([ta, tb] tc) .
(10.69)
Now, since the trace is cyclic, we have
f b
ac = (−i/k)Tr ([ta, tc] tb) = (−i/k)Tr (tatctb −tctatb)
= (−i/k)Tr (tbtatc −tatbtc)
= (−i/k)Tr ([tb, ta] tc) = f c
ba = −f c
ab.
(10.70)
Interchanging a and b, we get
f a
bc = f c
ab = −f c
ba.
(10.71)
Finally, interchanging b and c gives
f c
ab = f b
ca = −f b
ac.
(10.72)
Combining (10.70, 10.71, & 10.72), we see that the structure constants of a
compact Lie group are totally antisymmetric
f b
ac = −f b
ca = f c
ba = −f c
ab = −f a
bc = f a
cb.
(10.73)
Because of this antisymmetry, it is usual to lower the upper index
fabc ≡f c
ab
(10.74)
and write the antisymmetry of the structure constants of compact Lie groups
more simply as
facb = −fcab = fbac = −fabc = −fbca = fcba.
(10.75)
For compact Lie groups, the generators are hermitian, and so the struc-
ture constants fabc are real, as we may see by taking the complex conjugate of
equation (10.69)
f ∗
abc = (i/k)Tr (tc [tb, ta]) = (−i/k)Tr ([ta, tb] tc) = fabc.
(10.76)
All the representations of a given group must obey the same multipli-
cation law, that of the group. Thus in exponential parametrization, if the
representation D1 satisﬁes (10.62)
eiϵtb eiϵta e−iϵtb e−iϵta ≈eiϵ2f c
abtc,
(10.77)
364

10.15 LIE ALGEBRA
that is, if with ϵa being the vector with kth component ϵδak and ϵb being the
vector with kth component ϵδbk, we have
D1(ϵb) D1(ϵa) D1(−ϵb) D1(−ϵa) ≈D1(ϵ2f c
ab)
(10.78)
then any other representation D2 must satisfy the same relation with 2
replacing 1:
D2(ϵb)D2(ϵa)D2(−ϵb)D2(−ϵa) ≈D2(ϵ2f c
ab).
(10.79)
Such uniformity will occur if the structure constants (10.65) are the same for
all representations of a compact or a noncompact Lie group. To ensure that this
is so, we must allow each representation Dr(α) to have its own normalization
parameter kr in the trace relation (10.65). The structure constants fabc then are
a property of the group G and are independent of the particular representation
Dr(α). This is why we didn’t make the generators ta orthonormal.
It follows from (10.63 & 10.74–10.76) that the commutator of any two
generators of a Lie group is a linear combination
[ta, tb] = i f c
ab tc
(10.80)
of its generators tc, and that the structure constants fabc = f c
ab are real and totally
antisymmetric if the group is compact.
Example 10.16 (Gauge transformation)
The action density of a Yang–Mills
theory is unchanged when a space-time dependent unitary matrix U(x) changes
a vector ψ(x) of matter ﬁelds to ψ′(x) = U(x)ψ(x). Terms like ψ†ψ are invari-
ant because ψ†(x)U†(x)U(x)ψ(x) = ψ†(x)ψ(x), but how can kinetic terms like
∂iψ† ∂iψ be made invariant? Yang and Mills introduced matrices Ai of gauge
ﬁelds, replaced ordinary derivatives ∂i by covariant derivatives Di ≡∂i + Ai, and
required that D′
iψ′ = UDiψ or that

∂i + A′
i

U = ∂iU + U∂i + A′
iU = U (∂i + Ai) .
(10.81)
Their nonabelian gauge transformation is
ψ′(x) = U(x)ψ(x)
A′
i(x) = U(x)Ai(x)U†(x) −(∂iU(x)) U†(x).
(10.82)
One often writes the unitary matrix as U(x) = exp(−ig θa(x) ta) in which g is
a coupling constant, the functions θa(x) parametrize the gauge transformation,
and the generators ta belong to the representation that acts on the vector ψ(x)
of matter ﬁelds.
365

GROUP THEORY
10.16 The rotation group
The rotations and reﬂections in three-dimensional space form a compact group
O(3) whose elements R are real 3 × 3 matrices that leave invariant the dot-
product of any two 3-vectors
(Rx) · (Ry) = xTRTR y = xTIy = x · y.
(10.83)
These matrices therefore are orthogonal (1.168)
RTR = I.
(10.84)
Taking the determinant of both sides and using the transpose (1.194) and
product (1.207) rules, we have
(det R)2 = 1
(10.85)
whence det R = ±1. The subgroup with det R = 1 is the group SO(3). An SO(3)
element near the identity R = I + ω must satisfy
(I + ω) T (I + ω) = I.
(10.86)
Neglecting the tiny quadratic term, we ﬁnd that the inﬁnitesimal matrix ω is
antisymmetric
ωT = −ω.
(10.87)
One complete set of real 3 × 3 antisymmetric matrices is
ω1 =
⎛
⎝
0
0
0
0
0
−1
0
1
0
⎞
⎠,
ω2 =
⎛
⎝
0
0
1
0
0
0
−1
0
0
⎞
⎠,
ω3 =
⎛
⎝
0
−1
0
1
0
0
0
0
0
⎞
⎠, (10.88)
which we may write as
[ωb]ac = ϵabc,
(10.89)
in which ϵabc is the Levi-Civita symbol, which is totally antisymmetric with
ϵ123 = 1 (Tullio Levi-Civita, 1873–1941). The ωb are antihermitian, but we
make them hermitian by multiplying by i
tb = i ωb
(10.90)
so that R = I −iθb tb.
The three hermitian generators ta satisfy (exercise 10.15) the commutation
relations
[ta, tb] = i fabc tc
(10.91)
in which the structure constants are given by the Levi-Civita symbol ϵabc
fabc = ϵabc
(10.92)
366

10.16 THE ROTATION GROUP
so that
[ta, tb] = iϵabc tc.
(10.93)
They are the generators of SO(3) in the adjoint representation (section 10.20).
Physicists usually scale the generators by ¯h and deﬁne the angular-
momentum generator La as
La = ¯hta
(10.94)
so that the eigenvalues of the angular-momentum operators are the physical
values of the angular momenta. With ¯h, the commutation relations are
[La, Lb] = i¯h ϵabc Lc.
(10.95)
The matrix that represents a right-handed rotation (of an object) of angle θ =
|θ| about an axis θ is
D(θ) = e−iθ·t = e−iθ·L/¯h,
(10.96)
By using the fact (1.264) that a matrix obeys its characteristic equation, one
may show (exercise 10.17) that the 3 × 3 matrix D(θ) that represents a right-
handed rotation of θ radians about the axis θ is
Dij(θ) = cos θ δij −sin θ ϵijk θk/θ + (1 −cos θ) θiθj/θ2,
(10.97)
in which a sum over k = 1, 2, 3 is understood.
Example 10.17 (Demonstration of commutation relations)
Take a big sphere
with a distinguished point and orient the sphere so that the point lies in the y-
direction from the center of the sphere. Now rotate the sphere by a small angle,
say 15 degrees or ϵ = π/12, right-handedly about the x-axis, then right-handedly
about the y-axis by the same angle, then left-handedly about the x-axis and
then left-handedly about the y-axis. These rotations amount to a smaller, left-
handed rotation about the (vertical) z-axis in accordance with equation (10.77)
with ¯hta = L1 = Lx, ¯htb = L2 = Ly, and ¯hfabctc = ϵ12cLc = L3 = Lz
eiϵLy/¯h eiϵLx/¯h e−iϵLy/¯h e−iϵLx/¯h ≈eiϵ2Lz/¯h.
(10.98)
The magnitude of that rotation should be about ϵ2 = (π/12)2 ≈0.069 or about
3.9 degrees. Photographs of an actual demonstration are displayed in Fig. 10.1.
By expanding both sides of the demonstrated equation (10.98) in powers
of ϵ and keeping only the biggest terms that don’t cancel, you may show
(exercise 10.16) that the generators Lx and Ly satisfy the commutation relation
[Lx, Ly] = i¯hLz
(10.99)
of the rotation group.
367

GROUP THEORY
Physical demonstration of the commutation relations
Figure 10.1
Demonstration of equation (10.98) and the commutation relation
(10.99). Upper left: black ball with a white stick pointing in the y-direction; the
x-axis is to the reader’s left, the z-axis is vertical. Upper right: ball after a small
right-handed rotation about the x-axis. Center left: ball after that rotation is followed
by a small right-handed rotation about the y-axis. Center right: ball after these rota-
tions are followed by a small left-handed rotation about the x-axis. Bottom: ball after
these rotations are followed by a small left-handed rotation about the y-axis. The net
effect is approximately a small left-handed rotation about the z-axis.
10.17 The Lie algebra and representations of SU(2)
The three generators of SU(2) in its 2 × 2 deﬁning representation are the Pauli
matrices divided by 2, ta = σa/2. The structure constants of SU(2) are fabc =
ϵabc, which is totally antisymmetric with ϵ123 = 1
368

10.17 THE LIE ALGEBRA AND REPRESENTATIONS OF SU(2)
[ta, tb] = ifabctc =
σa
2 , σb
2

= iϵabc
σc
2 .
(10.100)
For every half-integer
j = n
2
for
n = 0, 1, 2, 3, . . .
(10.101)
there is an irreducible representation of SU(2)
D( j)(θ) = e−iθ·J( j),
(10.102)
in which the three generators t( j)
a
≡J( j)
a
are (2j + 1) × (2j + 1) square hermitian
matrices. In a basis in which J( j)
3
is diagonal, the matrix elements of the complex
linear combinations J( j)
± ≡J( j)
1
± iJ( j)
2
are

J( j)
1
± iJ( j)
2

s′,s = δs′,s±1

( j ∓s)( j ± s + 1),
(10.103)
where s and s′ run from −j to j in integer steps, and those of J( j)
3
are

J( j)
3

s′,s = s δs′,s.
(10.104)
The sum of the squares of the three generators J( j)
a
is a multiple of the
(2j + 1) × (2j + 1) identity matrix

J( j)
a
2
= j( j + 1) I.
(10.105)
Combinations of generators that are a multiple of the identity are called Casimir
operators.
Example 10.18 (Spin-two)
For j = 2, the spin-two matrices J(2)
+ and J(2)
3
are
J(2)
+ =
⎛
⎜⎜⎜⎜⎝
0
2
0
0
0
0
0
√
6
0
0
0
0
0
√
6
0
0
0
0
0
2
0
0
0
0
0
⎞
⎟⎟⎟⎟⎠
and
J(2)
3
=
⎛
⎜⎜⎜⎜⎝
2
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
−1
0
0
0
0
0
−2
⎞
⎟⎟⎟⎟⎠
(10.106)
and J−=

J(2)
+
†
.
The tensor product of any two irreducible representations D( j) and D(k) of
SU(2) is equivalent to the direct sum of all the irreducible representations Dℓ
for | j −k| ≤ℓ≤j + k
369

GROUP THEORY
D( j) ⊗D(k) =
j+k
5
ℓ=| j−k|
Dℓ,
(10.107)
each Dℓoccurring once.
Under a rotation R, a ﬁeld ψℓ(x) that transforms under the D( j) representa-
tion of SU(2) responds as
U(R) ψℓ(x) U−1(R) = D( j)
ℓℓ′(R−1) ψℓ′(Rx).
(10.108)
Example 10.19 (Spin and statistics)
Suppose |a, m⟩and |b, m⟩are any eigen-
states of the rotation operator J3 with eigenvalue m (in units with ¯h = c = 1).
Let u and v be any two points whose separation u −v is space-like (u −v)2 > 0.
Then, in some Lorentz frame, the two points are at the same time t, and we may
chose our coordinate system so that u′ = (t, x, 0, 0) and v′ = (t, −x, 0, 0). Let U
be the unitary operator that represents a right-handed rotation by π about the
3-axis or z-axis of this Lorentz frame. Then
U|a, m⟩= e−imπ|a, m⟩
and
⟨b, m|U−1 = ⟨b, m|eimπ.
(10.109)
And by (10.108), U transforms a ﬁeld ψ of spin j with x ≡(x, 0, 0) to
U(R) ψℓ(t, x) U−1(R) = D( j)
ℓℓ′(R−1) ψℓ′(t, −x) = eiπℓψℓ(t, −x).
(10.110)
Thus by inserting the identity operator in the form I = U−1U and using both
(10.109) and (10.108), we ﬁnd, since the phase factors exp(−imπ) and exp(imπ)
cancel,
⟨b, m|ψℓ(t, x) ψℓ(t, −x)|a, m⟩= ⟨b, m|Uψℓ(t, x)U−1Uψℓ(t, −x)U−1|a, m⟩
= e2iπℓ⟨b, m|ψℓ(t, −x)ψℓ(t, x)|a, m⟩.
(10.111)
Now if j is an integer, then so is ℓ, and the phase factor exp(2iπℓ) = 1 is
unity. In this case, we ﬁnd that the mean value of the equal-time commutator
vanishes
⟨b, m|[ψℓ(t, x), ψℓ(t, −x)]|a, m⟩= 0.
(10.112)
On the other hand, if j is half an odd integer, that is, j = (2n + 1)/2, where n is
an integer, then the phase factor exp(2iπℓ) is −1. In this case, the mean value of
the equal-time anticommutator vanishes
⟨b, m|{ψℓ(t, x), ψℓ(t, −x)}|a, m⟩= 0.
(10.113)
While not a proof of the spin-statistics theorem, this argument shows that the
behavior of ﬁelds under rotations does determine their statistics.
370

10.18 THE DEFINING REPRESENTATION OF SU(2)
10.18 The deﬁning representation of SU(2)
The smallest positive value of angular momentum is ¯h/2. The spin-one-half
angular momentum operators are represented by three 2 × 2 matrices
Sa = ¯h
2 σa,
(10.114)
in which the σa are the Pauli matrices
σ1 =
0
1
1
0

,
σ2 =
0
−i
i
0

,
and
σ3 =
1
0
0
−1

,
(10.115)
which obey the multiplication law
σiσj = δij + i
3

k=1
ϵijk σk.
(10.116)
The Pauli matrices divided by 2 satisfy the commutation relations (10.93) of
the rotation group
#1
2σa, 1
2σb
$
= iϵabc
1
2σc
(10.117)
and generate the elements of the group SU(2)
exp

i θ · σ
2

= I cos θ
2 + i ˆθ · σ sin θ
2,
(10.118)
in which I is the 2×2 identity matrix, θ =

θ2 and ˆθ = θ/θ.
It follows from (10.117) that the spin operators satisfy
[Sa, Sb] = i¯hϵabc Sc.
(10.119)
The raising and lowering operators
S± = S1 ± iS2
(10.120)
have simple commutators with S3
[S3, S±] = ±i¯hS±.
(10.121)
This relation implies that if the state | j, m⟩is an eigenstate of S3 with eigen-
value ¯hm, then the states S±| j, m⟩either vanish or are eigenstates of S3 with
eigenvalues ¯h(m ± 1)
S3S±| j, m⟩= S±S3| j, m⟩± i¯hS±| j, m⟩= ¯h(m ± 1)S±| j, m⟩.
(10.122)
Thus the raising and lowering operators raise and lower the eigenvalues of S3.
When j = 1/2, the possible values of m are m = ±1/2, and so with the usual
sign and normalization conventions
371

GROUP THEORY
S+|−⟩= ¯h|+⟩
and
S−|+⟩= ¯h|−⟩
(10.123)
while
S+|+⟩= 0
and
S−|−⟩= 0.
(10.124)
The square of the total spin operator is simply related to the raising and
lowering operators and to S3
S2 = S2
1 + S2
2 + S2
3 = 1
2S+S−+ 1
2S−S+ + S2
3.
(10.125)
But the squares of the Pauli matrices are unity, and so S2
a = (¯h/2)2 for all three
values of a. Thus
S2 = 3
4 ¯h2
(10.126)
is a Casimir operator (10.105) for a spin-one-half system.
Example 10.20 (Two spin-one-half systems)
Consider two spin operators S(1)
and S(2) as deﬁned by (10.114) acting on two spin-one-half systems. Let the
tensor-product states
|±, ±⟩= |±⟩1|±⟩2 = |±⟩1 ⊗|±⟩2
(10.127)
be eigenstates of S(1)
3
and S(2)
3
so that
S(1)
3 |+, ±⟩= ¯h
2 |+, ±⟩
and
S(2)
3 |±, +⟩= ¯h
2 |±, +⟩,
S(1)
3 |−, ±⟩= −¯h
2 |−, ±⟩
and
S(2)
3 |±, −⟩= −¯h
2 |±, −⟩. (10.128)
The total spin of the system is the sum of the two spins S = S(1) + S(2), so
S2 =

S(1) + S(2)2
and
S3 = S(1)
3
+ S(2)
3 .
(10.129)
The state |+, +⟩is an eigenstate of S3 with eigenvalue ¯h
S3|+, +⟩= S(1)
3 |+, +⟩+ S(2)
3 |+, +⟩
= ¯h
2|+, +⟩+ ¯h
2|+, +⟩= ¯h|+, +⟩.
(10.130)
So the state of angular momentum ¯h in the 3-direction is |1, 1⟩= |+, +⟩.
Similarly, the state |−, −⟩is an eigenstate of S3 with eigenvalue −¯h
S3|−, −⟩= S(1)
3 |−, −⟩+ S(2)
3 |−, −⟩
= −¯h
2|−, −⟩−¯h
2|−, −⟩= −¯h|−, −⟩
(10.131)
372

10.18 THE DEFINING REPRESENTATION OF SU(2)
and so the state of angular momentum ¯h in the negative 3-direction is |1, −1⟩=
|−, −⟩. The states |+, −⟩and |−, +⟩are eigenstates of S3 with eigenvalue 0
S3|+, −⟩= S(1)
3 |+, −⟩+ S(2)
3 |+, −⟩= ¯h
2|+, −⟩−¯h
2|+, −⟩= 0,
S3|−, +⟩= S(1)
3 |−, +⟩+ S(2)
3 |−, +⟩= −¯h
2|−, +⟩+ ¯h
2|−, +⟩= 0. (10.132)
To see which states are eigenstates of S2, we use the lowering operator for the
combined system S−= S(1)
−+ S(2)
−and the rules (10.103, 10.123, & 10.124) to
lower the state |1, 1⟩
S−|+, +⟩=

S(1)
−+ S(2)
−

|+, +⟩= ¯h (|−, +⟩+ |+, −⟩) = ¯h
√
2 |1, 0⟩.
Thus the state |1, 0⟩is
|1, 0⟩=
1
√
2
(|+, −⟩+ |−, +⟩) .
(10.133)
The orthogonal and normalized combination of |+, −⟩and |−, +⟩must be the
state of spin zero
|0, 0⟩=
1
√
2
(|+, −⟩−|−, +⟩)
(10.134)
with the usual sign convention.
To check that the states |1, 0⟩and |0, 0⟩really are eigenstates of S2, we use
(10.125 & 10.126) to write S2 as
S2 =

S(1) + S(2)2
= 3
2 ¯h2 + 2S(1) · S(2)
= 3
2 ¯h2 + S(1)
+ S(2)
−+ S(1)
−S(2)
+ + 2S(1)
3 S(2)
3 .
(10.135)
Now the sum S(1)
+ S(2)
−+ S(1)
−S(2)
+ merely interchanges the states |+, −⟩and |−, +⟩
and multiplies them by ¯h2, so
S2|1, 0⟩= 3
2 ¯h2|1, 0⟩+ ¯h2|1, 0⟩−2
4 ¯h2|1, 0⟩
= 2¯h2|1, 0⟩= s(s + 1)¯h2|1, 0⟩,
(10.136)
which conﬁrms that s = 1. Because of the relative minus sign in formula (10.134)
for the state |0, 0⟩, we have
S2|0, 0⟩= 3
2 ¯h2|0, 0⟩−¯h2|1, 0⟩−1
2 ¯h2|1, 0⟩
= 0¯h2|1, 0⟩= s(s + 1)¯h2|1, 0⟩,
(10.137)
which conﬁrms that s = 0.
373

GROUP THEORY
10.19 The Jacobi identity
Any three square matrices A, B, and C satisfy the commutator-product rule
[A, BC] = ABC −BCA = ABC −BAC + BAC −BCA
= [A, B]C + B[A, C].
(10.138)
Interchanging B and C gives
[A, CB] = [A, C]B + C[A, B].
(10.139)
Subtracting the second equation from the ﬁrst, we get the Jacobi identity
[A, [B, C]] = [[A, B], C] + [B, [A, C]]
(10.140)
and its equivalent cyclic form
[A, [B, C]] + [B, [C, A]] + [C, [A, B]] = 0.
(10.141)
Another Jacobi identity uses the anticommutator {A, B} ≡AB + BA
{[A, B], C} + {[A, C], B} + [{B, C}, A] = 0.
(10.142)
10.20 The adjoint representation
Any three generators ta, tb, and tc satisfy the Jacobi identity (10.141)
[ta, [tb, tc]] + [tb, [tc, ta]] + [tc, [ta, tb]] = 0.
(10.143)
By using the structure-constant formula (10.80), we may express each of these
double commutators as a linear combination of the generators
[ta, [tb, tc]] = [ta, if d
bctd] = −f d
bc f e
adte,
[tb, [tc, ta]] = [tb, if d
catd] = −f d
ca f e
bdte,
[tc, [ta, tb]] = [tc, if d
abtd] = −f d
ab f e
cdte.
(10.144)
So the Jacobi identity (10.143) implies that

f d
bc f e
ad + f d
ca f e
bd + f d
ab f e
cd

te = 0
(10.145)
or since the generators are linearly independent
f d
bc f e
ad + f d
ca f e
bd + f d
ab f e
cd = 0.
(10.146)
If we deﬁne a set of matrices Ta by
(Tb)ac = i f c
ab
(10.147)
374

10.21 CASIMIR OPERATORS
then, since the structure constants are antisymmetric in their lower indices, we
may write the three terms in the preceding equation (10.146) as
f d
bc f e
ad = f d
cb f e
da = (−TbTa)ce,
(10.148)
f d
ca f e
bd = −f d
ca f e
db = (TaTb)ce,
(10.149)
and
f d
ab f e
cd = −if d
ab(Td)ce
(10.150)
or in matrix notation
[Ta, Tb] = if c
abTc.
(10.151)
So the matrices Ta, which we made out of the structure constants by the rule
(Tb)ac = ifabc (10.147), obey the same algebra (10.63) as do the generators
ta. They are the generators in the adjoint representation of the Lie algebra. If
the Lie algebra has N generators ta, then the N generators Ta in the adjoint
representation are N × N matrices.
10.21 Casimir operators
For any compact Lie algebra, the sum of the squares of all the generators
C =
N

n=1
tata ≡tata
(10.152)
commutes with every generator tb
[C, tb] = [tata, tb] = [ta, tb]ta + ta[ta, tb]
= ifabctcta + taifabctc = i (fabc + fcba) tcta = 0
(10.153)
because of the total antisymmetry (10.75) of the structure constants. This sum,
called a Casimir operator, commutes with every matrix
[C, D(α)] = [C, exp(iαata)] = 0
(10.154)
of the representation generated by the tas. Thus by part 2 of Schur’s lemma
(section 10.7), it must be a multiple of the identity matrix
C = tata = cI.
(10.155)
The constant c depends upon the representation D(α) and is called the quadratic
Casimir.
The generators of some noncompact groups come in pairs ta and ita, and so
the sum of the squares of these generators vanishes, C = tata −tata = 0.
375

GROUP THEORY
10.22 Tensor operators for the rotation group
Suppose A( j)
m is a set of 2j + 1 operators whose commutation relations with the
generators Ji of rotations are
[Ji, A( j)
m ] = A( j)
ℓ(J( j)
i )ℓm
(10.156)
in which the sum over ℓruns from −j to j. Then A( j) is said to be a spin-j tensor
operator for the group SU(2).
Example 10.21 (A spin-one tensor operator)
For instance, if j = 1, then
(J(1)
i )ℓm = i¯hϵℓim, and so a spin-one tensor operator of SU(2) is a vector A(1)
m
that transforms as
[Ji, A(1)
m ] = A(1)
ℓi¯hϵℓim = i¯hϵimℓA(1)
ℓ
(10.157)
under rotations.
Let’s rewrite the deﬁnition (10.156) as
JiA( j)
m = A( j)
ℓ(J( j)
i )ℓm + A( j)
m Ji
(10.158)
and specialize to the case i = 3 so that (J( j)
3 )ℓm is diagonal, (J( j)
3 )ℓm = ¯hmδℓm
J3A( j)
m = A( j)
ℓ(J( j)
3 )ℓm+A( j)
m J3 = A( j)
ℓ¯hmδℓm+A( j)
m J3 = A( j)
m (¯hm + J3) . (10.159)
Thus if the state | j, m′, E⟩is an eigenstate of J3 with eigenvalue ¯hm′, then the
state A( j)
m | j, m′, E⟩is an eigenstate of J3 with eigenvalue ¯h(m + m′)
J3A( j)
m | j, m′, E⟩= A( j)
m (¯hm + J3) | j, m′, E⟩= ¯h

m + m′
A( j)
m | j, m′, E⟩.
(10.160)
The J3 eigenvalues of the tensor operator A( j)
m and the state | j, m′, E⟩add.
10.23 Simple and semisimple Lie algebras
An invariant subalgebra is a set of generators t(i)
a whose commutator with every
generator tb of the group is a linear combination of the t(i)
c s
[t(i)
a , tb] = i fabct(i)
c .
(10.161)
The whole algebra and the null algebra are trivial invariant subalgebras.
An algebra with no nontrivial invariant subalgebras is a simple algebra. A
simple algebra generates a simple group. An algebra that has no nontrivial
abelian invariant subalgebras is a semisimple algebra. A semisimple algebra
generates a semisimple group.
376

10.24 SU(3)
Example 10.22 (Some simple Lie groups)
The groups of unitary matrices of
unit determinant SU(2), SU(3), ... are simple. So are the groups of orthogonal
matrices of unit determinant SO(2), SO(3), ... and the groups of symplectic
matrices Sp(2), Sp(4), .. .(section 10.28).
Example 10.23 (Uniﬁcation and grand uniﬁcation)
The symmetry group of
the standard model of particle physics is a direct product of an SU(3) group that
acts on colored ﬁelds, an SU(2) group that acts on left-handed quark and lep-
ton ﬁelds, and a U(1) group that acts on ﬁelds that carry hypercharge. Each
of these three groups is an invariant subgroup of the full symmetry group
SU(3)c ⊗SU(2)ℓ⊗U(1)Y, and the last one is abelian. Thus the symmetry
group of the standard model is neither simple nor semisimple. A simple sym-
metry group relates all its quantum numbers, and so physicists have invented
grand uniﬁcation in which a simple symmetry group G contains the symmetry
group of the standard model. Georgi and Glashow suggested the group SU(5) in
1976 (Howard Georgi, 1947– ; Sheldon Glashow, 1932– ). Others have proposed
SO(10) and even bigger groups.
10.24 SU(3)
The Gell-Mann matrices are
λ1 =
⎛
⎝
0
1
0
1
0
0
0
0
0
⎞
⎠,
λ2 =
⎛
⎝
0
−i
0
i
0
0
0
0
0
⎞
⎠,
λ3 =
⎛
⎝
1
0
0
0
−1
0
0
0
0
⎞
⎠,
λ4 =
⎛
⎝
0
0
1
0
0
0
1
0
0
⎞
⎠,
λ5 =
⎛
⎝
0
0
−i
0
0
0
i
0
0
⎞
⎠,
λ6 =
⎛
⎝
0
0
0
0
0
1
0
1
0
⎞
⎠,
λ7 =
⎛
⎝
0
0
0
0
0
−i
0
i
0
⎞
⎠,
and
λ8 =
1
√
3
⎛
⎝
1
0
0
0
1
0
0
0
−2
⎞
⎠.
(10.162)
The generators ta of the 3 × 3 deﬁning representation of SU(3) are these Gell-
Mann matrices divided by 2
ta = λa/2
(10.163)
(Murray Gell-Mann, 1929–).
The eight generators ta are orthogonal with k = 1/2
Tr (tatb) = 1
2δab
(10.164)
377

GROUP THEORY
and satisfy the commutation relation
[ta, tb] = ifabc tc.
(10.165)
The trace formula (10.65) gives us the SU(3) structure constants as
fabc = −2iTr ([ta, tb]tc) .
(10.166)
They are real and totally antisymmetric with f123 = 1, f458 = f678 =
√
3/2, and
f147 = −f156 = f246 = f257 = f345 = −f367 = 1/2.
While no two generators of SU(2) commute, two generators of SU(3) do. In
the representation (10.162, 10.163), t3 and t8 are diagonal and so commute
[t3, t8] = 0.
(10.167)
They generate the Cartan subalgebra (section 10.26) of SU(3).
10.25 SU(3) and quarks
The generators deﬁned by equations (10.163 & 10.162) give us the 3 × 3
representation
D(α) = exp (iαata)
(10.168)
in which the sum a = 1, 2, . . . , 8 is over the eight generators ta. This representa-
tion acts on complex 3-vectors and is called the 3.
Note that if
D(α1)D(α2) = D(α3)
(10.169)
then the complex conjugates of these matrices obey the same multiplication rule
D∗(α1)D∗(α2) = D∗(α3)
(10.170)
and so form another representation of SU(3). It turns out that (unlike in SU(2))
this representation is inequivalent to the 3; it is the 3.
There are three quarks with masses less than about 100 MeV/c2 – the u, d,
and s quarks. The other three quarks c, b, and t are more massive by factors of
12, 45, and 173. Nobody knows why. Gell-Mann suggested that the low-energy
strong interactions were approximately invariant under unitary transforma-
tions of the three light quarks, which he represented by a 3, and of the three
light antiquarks, which he represented by a 3. He imagined that the eight light
pseudo-scalar mesons, that is, the three pions π−, π0, π+, the neutral η, and
the four kaons K0, K+, K−, K
0, were composed of a quark and an antiquark.
So they should transform as the tensor product
3 ⊗3 = 8 ⊕1.
(10.171)
He put the eight pseudo-scalar mesons into an 8.
378

10.27 QUATERNIONS
He imagined that the eight light baryons – the two nucleons N and P, the
three sigmas −, 0, +, the neutral lambda , and the two cascades −and
0 – were each made of three quarks. They should transform as the tensor
product
3 ⊗3 ⊗3 = 10 ⊕8 ⊕8 ⊕1.
(10.172)
He put the eight light baryons into one of these 8s. When he was writing these
papers, there were nine spin-3/2 resonances with masses somewhat heavier than
1200 MeV/c2 – four s, three ∗s, and two ∗s. He put these into the ten and
predicted the tenth and its mass. When a tenth spin-3/2 resonance, the −,
was found with a mass close to his prediction of 1680 MeV/c2, his SU(3) the-
ory became wildly popular among high-energy physicists. Within a few years, a
SLAC team had discovered quarks, and Gell-Mann had won the Nobel prize.
10.26 Cartan subalgebra
In any Lie group, the maximum set of mutually commuting generators Ha
generates the Cartan subalgebra
[Ha, Hb] = 0,
(10.173)
which is an abelian subalgebra. The number of generators in the Cartan sub-
algebra is the rank of the Lie algebra. The Cartan generators Ha can be
simultaneously diagonalized, and their eigenvalues or diagonal elements are the
weights
Ha|μ, x, D⟩= μa|μ, x, D⟩,
(10.174)
in which D labels the representation and x whatever other variables are needed
to specify the state. The vector μ is the weight vector. The roots are the weights
of the adjoint representation.
10.27 Quaternions
If z and w are any two complex numbers, then the 2 × 2 matrix
q =
 z
w
−w∗
z∗

(10.175)
is a quaternion. The quaternions are closed under addition and multiplica-
tion and under multiplication by a real number (exercise 10.21), but not under
multiplication by an arbitrary complex number. The squared norm of q is its
determinant
∥q∥2 = |z|2 + |w|2 = det q.
(10.176)
379

GROUP THEORY
The matrix products q†q and q q† are the squared norm ∥q∥2 multiplied by the
2 × 2 identity matrix
q†q = q q† = ∥q∥2 I.
(10.177)
The 2 × 2 matrix
iσ2 =
 0
1
−1
0

(10.178)
provides another expression for ∥q∥2 in terms of q and its transpose qT
qTiσ2 q = ∥q∥2 iσ2.
(10.179)
Clearly ∥q∥= 0 implies q = 0. The norm of a product of quaternions is the
product of their norms
∥q1q2∥=

det(q1q2) =

det q1 det q2 = ∥q1∥∥q2∥.
(10.180)
The quaternions therefore form an associative division algebra (over the real
numbers); the only others are the real numbers and the complex numbers; the
octonions are a nonassociative division algebra.
One may use the Pauli matrices to deﬁne for any real 4-vector x a quaternion
q(x) as
q(x) = x0 −iσkxk = x0 −iσ · x
=
x0 −ix3
−x2 −ix1
x2 −ix1
x0 + ix3

.
(10.181)
The product rule (10.116) for the Pauli matrices tells us that the product of two
quaternions is
q(x) q(y) = (x0 −iσ · x)(y0 −iσ · y)
= x0y0 −iσ · (y0x + x0y) −i(x × y) · σ −x · y
(10.182)
so their commutator is
[q(x), q(y)] = −2i(x × y) · σ.
(10.183)
Example 10.24 (Lack of analyticity)
One may deﬁne a function f (q) of a
quaternionic variable and then ask what functions are analytic in the sense that
the derivative
f ′(q) = lim
q′→0
f (q + q′) −f (q)
q′
(10.184)
exists and is independent of the direction through which q′ →0. This space of
functions is extremely limited and does not even include the function f (q) = q2
(exercise 10.22).
380

10.28 THE SYMPLECTIC GROUP SP (2N)
10.28 The symplectic group Sp (2n)
The symplectic group Sp(2n) consists of 2n × 2n matrices W that map n-tuples
q of quaternions into n-tuples q′ = Wq of quaternions with the same value of
the quadratic quaternionic form
∥q′∥2 = ∥q′
1∥2 + ∥q′
2∥2 + · · · + ∥q′
n∥2 = ∥q1∥2 + ∥q2∥2 + · · · + ∥qn∥2 = ∥q∥2.
(10.185)
By (10.177), the quadratic form ∥q′∥2 times the 2 × 2 identity matrix I is equal
to the hermitian form q′†q′
∥q′∥2 I = q′†q′ = q′†
1 q′
1 + · · · + q′†
n q′
n = q†W†Wq
(10.186)
and so any matrix W that is both a 2n × 2n unitary matrix and an n × n matrix
of quaternions keeps ∥q′∥2 = ∥q∥2
∥q′∥2 I = q†W†Wq = q†q = ∥q∥2 I.
(10.187)
The group Sp(2n) thus consists of all 2n×2n unitary matrices that also are n×n
matrices of quaternions. (This last requirement is needed so that q′ = Wq is an
n-tuple of quaternions.)
The generators ta of the symplectic group Sp(2n) are 2n × 2n direct-product
matrices of the form
I ⊗A,
σ1 ⊗S1,
σ2 ⊗S2,
and
σ3 ⊗S3,
(10.188)
in which I is the 2 × 2 identity matrix, the three σis are the Pauli matrices, A is
an imaginary n × n antisymmetric matrix, and the Si are n × n real symmetric
matrices. These generators ta close under commutation
[ta, tb] = ifabctc.
(10.189)
Any imaginary linear combination iαata of these generators is not only a 2n×2n
antihermitian matrix but also an n×n matrix of quaternions. Thus the matrices
D(α) = eiαata
(10.190)
are both unitary 2n × 2n matrices and n × n quaternionic matrices and so are
elements of the group Sp(2n).
Example 10.25 (Sp(2) = SU(2))
There is no 1 × 1 antisymmetric matrix, and
there is only one 1 × 1 symmetric matrix. So the generators ta of the group
Sp(2) are the Pauli matrices ta = σa, and Sp(2) = SU(2). The elements g(α) of
SU(2) are quaternions of unit norm (exercise 10.20), and so the product g(α)q is
a quaternion
∥g(α)q∥2 = det(g(α)q) = det(g(α)) det q = det q = ∥q∥2
(10.191)
with the same squared norm.
381

GROUP THEORY
Example 10.26 (Sp(4) = SO(5))
Apart from scale factors, there are three real
symmetric 2 × 2 matrices S1 = σ1, S2 = I, and S3 = σ3 and one imaginary
antisymmetric 2×2 matrix A = σ2. So there are ten generators of Sp(4) = SO(5)
t1 = I ⊗σ2 =
0
−iI
iI
0

, tk1 = σk ⊗σ1 =
 0
σk
σk
0

tk2 = σk ⊗I =
σk
0
0
σk

, tk3 = σk ⊗σ3 =
σk
0
0
−σk

(10.192)
where k runs from 1 to 3.
We may see Sp(2n) from a different viewpoint if we use (10.179) to write the
quadratic form ∥q∥2 in terms of a 2n × 2n matrix J that has n copies of iσ2 on
its 2 × 2 diagonal
J =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
iσ2
0
0
0
. . .
0
0
iσ2
0
0
. . .
0
0
0
iσ2
0
. . .
0
0
0
0
...
. . .
0
...
...
...
...
...
0
0
0
0
0
0
iσ2
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
(10.193)
(and zeros elsewhere) as
∥q∥2J = qTJq.
(10.194)
Thus any n × n matrix of quaternions W that satisﬁes
W TJW = J
(10.195)
also satisﬁes
∥Wq∥2J = qTW TJWq = qTJq = ∥q∥2J
(10.196)
and so leaves invariant the quadratic form (10.185). The group Sp(2n) therefore
consists of all 2n × 2n matrices W that satisfy (10.195) and that also are n × n
matrices of quaternions.
The symplectic group is something of a physics orphan. Its best-known
application is in classical mechanics, and that application uses the noncompact
symplectic group Sp(2n, R), not the compact symplectic group Sp(2n). The ele-
ments of Sp(2n, R) are all real 2n × 2n matrices T that satisfy T TJT = J with
the J of (10.193); those near the identity are of the form T = exp(JS) in which
S is a 2n × 2n real symmetric matrix (exercise 10.24).
382

10.29 COMPACT SIMPLE LIE GROUPS
Example 10.27 (Sp(2, R))
The matrices (exercise 10.25)
T = ±
cosh θ
sinh θ
sinh θ
cosh θ

(10.197)
are elements of the noncompact symplectic group Sp(2, R).
A dynamical map M takes the phase-space 2n-tuple z = (q1, p1, . . . , qn, pn)
from z(t1) to z(t2). One may show that M’s jacobian matrix
Mab = ∂za(t2)
∂zb(t1)
(10.198)
is in Sp(2n, R) if and only if its dynamics are hamiltonian
˙qa = ∂H
∂pa
and
˙pa = −∂H
∂qa
(10.199)
(Carl
Jacobi,
1804–1851;
William
Hamilton,
1805–1865,
inventor
of
quaternions).
10.29 Compact simple Lie groups
Élie Cartan (1869–1951) showed that all compact, simple Lie groups fall into
four inﬁnite classes and ﬁve discrete cases. For n = 1, 2, . . ., his four classes are
• An = SU(n + 1), which are (n + 1) × (n + 1) unitary matrices with unit
determinant,
• Bn = SO(2n+1), which are (2n+1)×(2n+1) orthogonal matrices with unit
determinant,
• Cn = Sp(2n), which are 2n × 2n symplectic matrices, and
• Dn = SO(2n), which are 2n × 2n orthogonal matrices with unit determinant.
The ﬁve discrete cases are the exceptional groups G2, F4, E6, E7, and E8.
The exceptional groups are associated with the octonians
a + bαiα
(10.200)
where the α-sum runs from 1 to 7; the eight numbers a and bα are real; and the
seven iαs obey the multiplication law
iα iβ = −δαβ + gαβγ iγ ,
(10.201)
in which gαβγ is totally antisymmetric with
g123 = g247 = g451 = g562 = g634 = g375 = g716 = 1.
(10.202)
383

GROUP THEORY
Like the quaternions and the complex numbers, the octonians form a division
algebra with an absolute value
|a + bαiα| =

a2 + b2
α
1/2
(10.203)
that satisﬁes
|AB| = |A||B|
(10.204)
but they lack associativity.
The group G2 is the subgroup of SO(7) that leaves the gαβγ s of (10.201)
invariant.
10.30 Group integration
Suppose we need to integrate some function f (g) over a group. Naturally, we
want to do so in a way that gives equal weight to every element of the group. In
particular, if g′ is any group element, we want the integral of the shifted function
f (g′g) to be the same as the integral of f (g)

f (g) dg =

f (g′g) dg.
(10.205)
Such a measure dg is said to be left invariant (Creutz, 1983, chap. 8).
Let’s use the letters a = a1, . . . , an, b = b1, . . . , bn, and so forth to label the
elements g(a), g(b), so that an integral over the group is

f (g) dg =

f (g(a)) m(a) dna,
(10.206)
in which m(a) is the left-invariant measure and the integration is over the n-
space of as that label all the elements of the group.
To ﬁnd the left-invariant measure m(a), we use the multiplication law of the
group
g(a(c, b)) ≡g(c) g(b)
(10.207)
and impose the requirement (10.205) of left invariance with g′ ≡g(c)

f (g(b)) m(b) dnb =

f (g(c)g(b)) m(b) dnb =

f (g(a(c, b))) m(b) dnb.
(10.208)
We change variables from b to a = a(c, b) by using the jacobian det(∂b/∂a),
which gives us dnb = det(∂b/∂a) dna

f (g(b)) m(b) dnb =

f (g(a)) det(∂b/∂a) m(b) dna.
(10.209)
384

10.30 GROUP INTEGRATION
Replacing b by a = a(c, b) on the left-hand side of this equation, we ﬁnd
m(a) = det(∂b/∂a) m(b)
(10.210)
or since det(∂b/∂a) = 1/ det(∂a(c, b)/∂b)
m(a(c, b)) = m(b)/ det(∂a(c, b)/∂b).
(10.211)
So if we let g(b) →g(0) = e, the identity element of the group, and set m(e) = 1,
then we ﬁnd for the measure
m(a) = m(c) = m(a(c, b))|b=0 = 1/ det(∂a(c, b)/∂b)|b=0 .
(10.212)
Example 10.28 (The invariant measure for SU(2))
A general element of the
group SU(2) is given by (10.118) as
exp

i θ · σ
2

= I cos θ
2 + i ˆθ · σ sin θ
2.
(10.213)
Setting a0 = cos(θ/2) and a = ˆθ sin(θ/2), we have
g(a) = a0 + i a · σ,
(10.214)
in which a2 ≡a2
0 + a · a = 1. Thus, the parameter space for SU(2) is the unit
sphere S3 in four dimensions. Its invariant measure is

δ(1 −a2) d4a =

δ(1 −a2
0 −a2) d4a =

(1 −a2)−1/2 d3a
(10.215)
or
m(a) = (1 −a2)−1/2.
(10.216)
We also can write the arbitrary element (10.214) of SU(2) as
g(a) = ±

1 −a2 + i a · σ
(10.217)
and the group-multiplication law (10.207) as

1 −a2 + i a · σ =

1 −c2 + i c · σ
 
1 −b2 + i b · σ

.
(10.218)
Thus, by multiplying both sides of this equation by σi and taking the trace, we
ﬁnd (exercise 10.26) that the parameters a(c, b) that describe the product g(c) g(b)
are
a(c, b) =

1 −c2 b +

1 −b2 c −c × b.
(10.219)
To compute the jacobian of our formula (10.212) for the invariant measure, we
differentiate this expression (10.219) at b = 0 and so ﬁnd (exercise 10.27)
m(a) = 1/ det(∂a(c, b)/∂b)|b=0 = (1 −a2)−1/2
(10.220)
as the left-invariant measure in agreement with (10.216).
385

GROUP THEORY
10.31 The Lorentz group
The Lorentz group O(3, 1) is the set of all linear transformations L that leave
invariant the Minkowski inner product
xy ≡x · y −x0y0 = xTηy
(10.221)
in which η is the diagonal matrix
η =
⎛
⎜⎜⎝
−1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
⎞
⎟⎟⎠.
(10.222)
So L is in O(3, 1) if for all 4-vectors x and y
(Lx) T ηL y = xTLT η Ly = xTη y.
(10.223)
Since x and y are arbitrary, this condition amounts to
LTη L = η.
(10.224)
Taking the determinant of both sides and using the transpose (1.194) and
product (1.207) rules, we have
(det L)2 = 1.
(10.225)
So det L = ±1, and every Lorentz transformation L has an inverse. Multiplying
(10.224) by η, we ﬁnd
ηLTηL = η2 = I,
(10.226)
which identiﬁes L−1 as
L−1 = ηLTη.
(10.227)
The subgroup of O(3, 1) with det L = 1 is the proper Lorentz group SO(3, 1).
To ﬁnd its Lie algebra, we consider a Lorentz matrix L = I + ω that dif-
fers from the identity matrix I by a tiny matrix ω and require it to satisfy the
condition (10.224) for membership in the Lorentz group

I + ωT
η (I + ω) = η + ωTη + η ω + ωTω = η.
(10.228)
Neglecting ωTω, we have ωTη = −η ω or since η2 = I
ωT = −η ω η.
(10.229)
This equation says (exercise 10.29) that under transposition the time-time and
space-space elements of ω change sign, while the time-space and space-time
386

10.31 THE LORENTZ GROUP
elements do not. That is, the tiny matrix ω must be for inﬁnitesimal θ and λ a
linear combination
ω = θ · R + λ · B
(10.230)
of the six matrices
R1 =
⎛
⎜⎜⎝
0
0
0
0
0
0
0
0
0
0
0
−1
0
0
1
0
⎞
⎟⎟⎠,
R2 =
⎛
⎜⎜⎝
0
0
0
0
0
0
0
1
0
0
0
0
0
−1
0
0
⎞
⎟⎟⎠,
R3 =
⎛
⎜⎜⎝
0
0
0
0
0
0
−1
0
0
1
0
0
0
0
0
0
⎞
⎟⎟⎠
(10.231)
and
B1 =
⎛
⎜⎜⎝
0
1
0
0
1
0
0
0
0
0
0
0
0
0
0
0
⎞
⎟⎟⎠,
B2 =
⎛
⎜⎜⎝
0
0
1
0
0
0
0
0
1
0
0
0
0
0
0
0
⎞
⎟⎟⎠,
B3 =
⎛
⎜⎜⎝
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
⎞
⎟⎟⎠,
(10.232)
which satisfy condition (10.229). The three Rj are 4 × 4 versions of the rotation
generators (10.88); the three Bj generate Lorentz boosts.
If we write L = I + ω as
L = I −iθℓiRℓ−iλjiBj ≡I −iθℓJℓ−iλjKj
(10.233)
then the three matrices Jℓ= iRℓare imaginary and antisymmetric, and there-
fore hermitian. But the three matrices Kj = iBj are imaginary and symmetric,
and so are antihermitian. Thus, the 4 × 4 matrix L is not unitary. The reason is
that the Lorentz group is not compact.
One may verify (exercise 10.30) that the six generators Jℓand Kj satisfy three
sets of commutation relations:
[Ji, Jj] = iϵijkJk,
(10.234)
[Ji, Kj] = iϵijkKk,
(10.235)
[Ki, Kj] = −iϵijkJk.
(10.236)
The ﬁrst (10.234) says that the three Jℓgenerate the rotation group SO(3); the
second (10.235) says that the three boost generators transform as a 3-vector
under SO(3); and the third (10.236) implies that four canceling inﬁnitesimal
boosts can amount to a rotation. These three sets of commutation relations
form the Lie algebra of the Lorentz group SO(3, 1). Incidentally, one may show
(exercise 10.31) that if J and K satisfy these commutation relations (10.234–
10.236), then so do
J and −K.
(10.237)
387

GROUP THEORY
The inﬁnitesimal Lorentz transformation (10.233) is the 4 × 4 matrix
L = I + ω = I + θℓRℓ+ λjBj =
⎛
⎜⎜⎝
1
λ1
λ2
λ3
λ1
1
−θ3
θ2
λ2
θ3
1
−θ1
λ3
−θ2
θ1
1
⎞
⎟⎟⎠.
(10.238)
It moves any 4-vector x to x′ = L x or in components x′a = La
b xb
x′0 = x0 + λ1x1 + λ2x2 + λ3x3,
x′1 = λ1x0 + x1 −θ3x2 + θ2x3,
x′2 = λ2x0 + θ3x1 + x2 −θ1x3,
x′3 = λ3x0 −θ2x1 + θ1x2 + x3.
(10.239)
More succinctly with t = x0, this is
t′ = t + λ · x,
x′ = x + tλ + θ ∧x,
(10.240)
in which ∧≡× means cross-product.
For arbitrary real θ and λ, the matrices
L = e−iθℓJℓ−iλjKj
(10.241)
form the subgroup of SO(3, 1) that is connected to the identity matrix I. This
subgroup preserves the sign of the time of any time-like vector, that is, if x2 < 0,
and y = Lx, then y0x0 > 0. It is called the proper orthochronous Lorentz
group. The rest of the (homogeneous) Lorentz group can be obtained from it
by space P, time T , and space-time PT reﬂections.
The task of ﬁnding all the ﬁnite-dimensional irreducible representations of
the proper orthochronous homogeneous Lorentz group becomes vastly sim-
pler when we write the commutation relations (10.234–10.236) in terms of the
nonhermitian matrices
J±
ℓ= 1
2 (Jℓ± iKℓ) ,
(10.242)
which generate two independent rotation groups
[J+
i , J+
j ] = iϵijkJ+
k ,
[J−
i , J−
j ] = iϵijkJ−
k ,
[J+
i , J−
j ] = 0.
(10.243)
Thus the Lie algebra of the Lorentz group is equivalent to two copies of the Lie
algebra (10.100) of SU(2). Its ﬁnite-dimensional irreducible representations are
the direct products
D(j,j′)(θ, λ) = e−iθℓJℓ−iλℓKℓ= e(−iθℓ−λℓ)J+
ℓe(−iθℓ+λℓ)J−
ℓ
(10.244)
388

10.32 TWO-DIMENSIONAL REPRESENTATIONS OF THE LORENTZ GROUP
of the nonunitary representations D(j)(θ, λ) = e(−iθℓ−λℓ)J+
ℓand D(j′)(θ, λ) =
e(−iθℓ+λℓ)J−
ℓgenerated by the three (2j + 1) × (2j + 1) matrices J+
ℓand by the
three (2j′ +1)×(2j′ +1) matrices J−
ℓ. Under a Lorentz transformation L, a ﬁeld
ψ(j,j′)
m,m′(x) that transforms under the D( j,j′) representation of the Lorentz group
responds as
U(L) ψ(j,j′)
m,m′(x) U−1(L) = D(j)
mm′′(L−1) D(j′)
m′m′′′(L−1) ψ(j,j′)
m′′,m′′′(Lx).
(10.245)
Although these representations are not unitary, the SO(3) subgroup of the
Lorentz group is represented unitarily by the hermitian matrices
J = J+ + J−.
(10.246)
Thus, the representation D( j,j′) describes objects of the spins s that can arise
from the direct product of spin- j with spin- j′ (Weinberg, 1995, p. 231)
s = j + j′, j + j′ −1, . . . , | j −j′|.
(10.247)
For instance, D(0,0) describes a spinless ﬁeld or particle, while D(1/2,0) and
D(0,1/2) respectively describe right-handed and left-handed spin-1/2 ﬁelds or
particles. The representation D(1/2,1/2) describes objects of spin 1 and spin 0 –
the spatial and time components of a 4-vector.
The generators Kj of the Lorentz boosts are related to J± by
K = −iJ+ + iJ−,
(10.248)
which like (10.246) follows from the deﬁnition (10.242).
The interchange of J+ and J−replaces the generators J and K with J and
−K, a substitution that we know (10.237) is legitimate.
10.32 Two-dimensional representations of the Lorentz group
The generators of the representation D(1/2,0) with j = 1/2 and j′ = 0 are given
by (10.246 & 10.248) with J+ = σ/2 and J−= 0. They are
J = 1
2σ
and
K = −i1
2σ.
(10.249)
The 2 × 2 matrix D(1/2,0) that represents the Lorentz transformation (10.241)
L = e−iθℓJℓ−iλjKj
(10.250)
is
D(1/2,0)(θ, λ) = exp (−iθ · σ/2 −λ · σ/2) .
(10.251)
389

GROUP THEORY
And so the generic D(1/2,0) matrix is
D(1/2,0)(θ, λ) = e−z·σ/2
(10.252)
with λ = Rez and θ = Imz. It is nonunitary and of unit determinant; it is
a member of the group SL(2, C) of complex unimodular 2 × 2 matrices. The
group SL(2, C) relates to the Lorentz group SO(3, 1) as SU(2) relates to the
rotation group SO(3).
Example 10.29 (The standard left-handed boost)
For a particle of mass m > 0,
the “standard” boost that takes the 4-vector k = (m, 0) to p = (p0, p), where
p0 =

m2 + p2, is a boost in the ˆp direction
B(p) = R(ˆp) B3(p0) R−1(ˆp) = exp

α ˆp · B

(10.253)
in which cosh α = p0/m and sinh α = |p|/m, as one may show by expanding the
exponential (exercise 10.33).
For λ = α ˆp, one may show (exercise 10.34) that the matrix D(1/2,0)(0, λ) is
D(1/2,0)(0, α ˆp) = e−αˆp·σ/2 = I cosh(α/2) −ˆp · σ sinh(α/2)
= I

(p0 + m)/(2m) −ˆp · σ

(p0 −m)/(2m)
= p0 + m −p · σ

2m(p0 + m)
(10.254)
in the third line of which the 2 × 2 identity matrix I is suppressed.
Under D(1/2,0), the vector (−I, σ) transforms like a 4-vector. For tiny θ and
λ, one may show (exercise 10.36) that the vector (−I, σ) transforms as
D†(1/2,0)(θ, λ)(−I)D(1/2,0)(θ, λ) = −I + λ · σ,
D†(1/2,0)(θ, λ) σ D(1/2,0)(θ, λ) = σ + (−I)λ + θ ∧σ,
(10.255)
which is how the 4-vector (t, x) transforms (10.240). Under a ﬁnite Lorentz
transformation L the 4-vector Sa ≡(−I, σ) becomes
D†(1/2,0)(L) Sa D(1/2,0)(L) = La
bSb.
(10.256)
A ﬁeld ξ(x) that responds to a unitary Lorentz transformation U(L) like
U(L) ξ(x) U−1(L) = D(1/2,0)(L−1) ξ(Lx)
(10.257)
is called a left-handed Weyl spinor. We will see in example 10.30 why the action
density for such spinors
Lℓ(x) = i ξ†(x) (∂0I −∇· σ) ξ(x)
(10.258)
390

10.32 TWO-DIMENSIONAL REPRESENTATIONS OF THE LORENTZ GROUP
is Lorentz covariant, that is
U(L) Lℓ(x) U−1(L) = Lℓ(Lx).
(10.259)
Example 10.30 (Why Lℓis Lorentz covariant)
We ﬁrst note that the derivatives
∂′
b in Lℓ(Lx) are with respect to x′ = Lx. Since the inverse matrix L−1 takes x′
back to x = L−1x′ or in tensor notation xa = L−1a
b x′b, the derivative ∂′
b is
∂′
b =
∂
∂x′b = ∂xa
∂x′b
∂
∂xa = L−1a
b
∂
∂xa = ∂a L−1a
b.
(10.260)
Now using the abbreviation ∂0I −∇· σ ≡−∂aSa and the transformation laws
(10.256 & 10.257), we have
U(L) Lℓ(x) U−1(L) = i ξ†(Lx)D(1/2,0)†(L−1)( −∂aSa)D(1/2,0)(L−1) ξ(Lx)
= i ξ†(Lx)( −∂aL−1a
bSb) ξ(Lx)
= i ξ†(Lx)( −∂′
bSb) ξ(Lx) = Lℓ(Lx),
(10.261)
which shows that Lℓis Lorentz covariant.
Incidentally, the rule (10.260) ensures, among other things, that the diver-
gence ∂aVa is invariant

∂aVa′ = ∂′
aV′a = ∂b L−1b
a La
cVc = ∂b δb
c Vc = ∂b Vb.
(10.262)
Example 10.31 (Why ξ is left-handed)
The space-time integral S of the action
density Lℓis stationary when ξ(x) satisﬁes the wave equation
(∂0I −∇· σ) ξ(x) = 0
(10.263)
or in momentum space
(E + p · σ) ξ(p) = 0.
(10.264)
Multiplying from the left by (E −p · σ), we see that the energy of a particle
created or annihilated by the ﬁeld ξ is the same as its momentum E = |p| in
accord with the absence of a mass term in the action density Lℓ. And because the
spin of the particle is represented by the matrix J = σ/2, the momentum-space
relation (10.264) says that ξ(p) is an eigenvector of ˆp · J
ˆp · J ξ(p) = −1
2 ξ(p)
(10.265)
with eigenvalue −1/2. A particle whose spin is opposite to its momentum is
said to have negative helicity or to be left-handed. Nearly massless neutrinos are
nearly left-handed.
391

GROUP THEORY
One may add to this action density the Majorana mass term
LM(x) = 1
2

m ξ T(x) σ2 ξ(x) +

m ξ T(x) σ2 ξ(x)
†
,
(10.266)
which is Lorentz covariant because the matrices σ1 and σ3 anticommute with σ2,
which is antisymmetric (exercise 10.38). Since charge is conserved, only neutral
ﬁelds like neutrinos can have Majorana mass terms.
The generators of the representation D(0,1/2) with j = 0 and j′ = 1/2 are
given by (10.246 & 10.248) with J+ = 0 and J−= σ/2; they are
J = 1
2σ
and
K = i1
2σ.
(10.267)
Thus the 2 × 2 matrix D(0,1/2)(θ, λ) that represents the Lorentz transformation
(10.241)
L = e−iθℓJℓ−iλjKj
(10.268)
is
D(0,1/2)(θ, λ) = exp (−iθ · σ/2 + λ · σ/2) = D(1/2,0)(θ, −λ),
(10.269)
which differs from D(1/2,0)(θ, λ) only by the sign of λ. The generic D(0,1/2) matrix
is the complex unimodular 2 × 2 matrix
D(0,1/2)(θ, λ) = ez∗·σ/2
(10.270)
with λ = Rez and θ = Imz.
Example 10.32 (The standard right-handed boost)
For a particle of mass m >
0, the “standard” boost (10.253) that transforms k = (m, 0) to p = (p0, p) is the
4 × 4 matrix B(p) = exp

α ˆp · B

in which cosh α = p0/m and sinh α = |p|/m.
This Lorentz transformation with θ = 0 and λ = α ˆp is represented by the matrix
(exercise 10.35)
D(0,1/2)(0, α ˆp) = eαˆp·σ/2 = I cosh(α/2) + ˆp · σ sinh(α/2)
= I

(p0 + m)/(2m) + ˆp · σ

(p0 −m)/(2m)
= p0 + m + p · σ

2m(p0 + m)
,
(10.271)
in the third line of which the 2 × 2 identity matrix I is suppressed.
Under D(0,1/2), the vector (I, σ) transforms as a 4-vector; for tiny z
D†(0,1/2)(θ, λ) I D(0,1/2)(θ, λ) = I + λ · σ,
D†(0,1/2)(θ, λ) σ D(0,1/2)(θ, λ) = σ + Iλ + θ ∧σ
(10.272)
as in (10.240).
392

10.33 THE DIRAC REPRESENTATION OF THE LORENTZ GROUP
A ﬁeld ζ(x) that responds to a unitary Lorentz transformation U(L) as
U(L) ζ(x) U−1(L) = D(0,1/2)(L−1) ζ(Lx)
(10.273)
is called a right-handed Weyl spinor. One may show (exercise 10.37) that the
action density
Lr(x) = i ζ †(x) (∂0I + ∇· σ) ζ(x)
(10.274)
is Lorentz covariant
U(L) L(x) U−1(L) = L(Lx).
(10.275)
Example 10.33 (Why ζ is right-handed)
An argument like that of example
(10.31) shows that the ﬁeld ζ(x) satisﬁes the wave equation
(∂0I + ∇· σ) ζ(x) = 0
(10.276)
or in momentum space
(E −p · σ) ζ(p) = 0.
(10.277)
Thus, E = |p|, and ζ(p) is an eigenvector of ˆp · J
ˆp · J ζ(p) = 1
2 ζ(p)
(10.278)
with eigenvalue 1/2. A particle whose spin is parallel to its momentum is said
to have positive helicity or to be right-handed. Nearly massless antineutrinos are
nearly right-handed.
The Majorana mass term
LM(x) = 1
2

im ζ T(x) σ2 ζ(x) +

im ζ T(x) σ2 ζ(x)
†
(10.279)
like (10.266) is Lorentz covariant.
10.33 The Dirac representation of the Lorentz group
Dirac’s representation of SO(3, 1) is the direct sum D(1/2,0) ⊕D(0,1/2) of D(1/2,0)
and D(0,1/2). Its generators are the 4 × 4 matrices
J = 1
2
σ
0
0
σ

and
K = i
2
−σ
0
0
σ

.
(10.280)
Dirac’s representation uses the Clifford algebra of the gamma matrices γ a,
which satisfy the anticommutation relation
{γ a, γ b} ≡γ a γ b + γ b γ a = 2ηabI,
(10.281)
393

GROUP THEORY
in which η is the 4 × 4 diagonal matrix (10.222) with η00 = −1 and ηjj = 1 for
j = 1, 2, and 3 and I is the 4 × 4 identity matrix.
Remarkably, the generators of the Lorentz group
Jij = ϵijkJk
and
J0j = Kj
(10.282)
may be represented as commutators of gamma matrices
Jab = −i
4[γ a, γ b].
(10.283)
They transform the gamma matrices as a 4-vector
[Jab, γ c] = −iγ a ηbc + iγ b ηac
(10.284)
(exercise 10.39) and satisfy the commutation relations
i[Jab, Jcd] = ηbc Jad −ηac Jbd −ηda Jcb + ηdb Jca
(10.285)
of the Lorentz group (Weinberg, 1995, pp. 213–217) (exercise 10.40).
The gamma matrices γ a are not unique; if S is any 4 × 4 matrix with an
inverse, then the matrices γ ′a ≡Sγ aS−1 also satisfy the deﬁnition (10.281).
The choice
γ 0 = −i
0
1
1
0

and
γ = −i
 0
σ
−σ
0

(10.286)
is useful in high-energy physics because it lets us assemble a left-handed spinor
and a right-handed spinor into a 4-component Majorana spinor
ψM =
ξ
ζ

.
(10.287)
If two Majorana spinors ψ(1)
M and ψ(2)
M have the same mass, then one may
combine them into a Dirac spinor
ψD =
1
√
2

ψ(1)
M + iψ(2)
M

=
1
√
2
ξ(1) + iξ(2)
ζ (1) + iζ (2)

=
ξD
ζD

.
(10.288)
The action for a Majorana or Dirac 4-spinor often is written as
L = −ψ

γ a∂a + m

ψ ≡−ψ (̸∂+ m) ψ,
(10.289)
in which
ψ ≡iψ†γ 0 = ψ†
0
1
1
0

=

ζ †
ξ†
.
(10.290)
The kinetic part is the sum of the left-handed Lℓand right-handed Lr action
densities (10.258 & 10.274)
−ψ γ a∂aψ = iξ† (∂0I −∇· σ) ξ + i ζ † (∂0I + ∇· σ) ζ.
(10.291)
394

10.34 THE POINCARÉ GROUP
The Dirac mass term
−m ψψ = −m

ζ †ξ + ξ†ζ

(10.292)
conserves charge even if ψ is a charged Dirac 4-spinor ψD
ψD =
1
√
2

ψ(1) + iψ(2)
,
(10.293)
in which case it is
−mψDψD = −m

ζ †
DξD + ξ†
DζD

= −m
2

ζ (1)† −iζ (2)† 
ξ(1) + iξ(2)
+

ξ(1)† −iξ(2)† 
ζ (1) + iζ (2)
.
(10.294)
One may show (exercise 10.41) that if ξ is a left-handed spinor transforming
as (10.257), then the spinor
ζ = σ2 ξ∗≡
0
−i
i
0
 
ξ†
1
ξ†
2

(10.295)
transforms as a right-handed spinor (10.273), that is
ez∗·σ/2σ2 ξ∗= σ2

e−z·σ/2ξ
∗
.
(10.296)
Similarly, ξ = σ2 ζ ∗is left-handed if ζ is right-handed. Thus
ζ †ξ = ξ Tσ2 ξ = ζ †σ2 ζ ∗.
(10.297)
One therefore can write a Dirac mass term (10.298) as a speciﬁc combination
of Majorana mass terms
−mψDψD = −m
2

ξ(1)T −iξ(2)T
σ2

ξ(1) + iξ(2)
+

ζ (1)T −iζ (2)T
σ2

ζ (1) + iζ (2)
(10.298)
or entirely in terms of either left-handed ξ or right-handed ζ spinors.
10.34 The Poincaré group
The elements of the Poincaré group are products of Lorentz transformations
and translations in space and time. The Lie algebra of the Poincaré group
therefore includes the generators J and K of the Lorentz group as well as the
hamiltonian H and the momentum operator P, which respectively generate
translations in time and space.
395

GROUP THEORY
Suppose T(y) is a translation that takes a 4-vector x to x + y and T(z) is a
translation that takes a 4-vector x to x + z. Then T(z)T(y) and T(y)T(z) both
take x to x + y + z. So if a translation T(y) = T(t, y) is represented by a unitary
operator U(t, y) = exp(iHt−iP · y), then the hamiltonian H and the momentum
operator P commute with each other
[H, Pj] = 0
and
[Pi, Pj] = 0.
(10.299)
We can ﬁgure out the commutation relations of H and P with the angular-
momentum J and boost K operators by realizing that Pa = (H, P) is a 4-vector.
Let
U(θ, λ) = e−iθ·J−iλ·K
(10.300)
be the (inﬁnite-dimensional) unitary operator that represents (in Hilbert space)
the inﬁnitesimal Lorentz transformation
L = I + θ · R + λ · B
(10.301)
where R and B are the six 4 × 4 matrices (10.231 & 10.232). Then because P is
a 4-vector under Lorentz transformations, we have
U−1(θ, λ)PU(θ, λ) = e+iθ·J+iλ·KPe−iθ·J−iλ·K = (I + θ · R + λ · B) P (10.302)
or using (10.272)
(I + iθ · J + iλ · K) H (I −iθ · J −iλ · K) = H + λ · P,
(I + iθ · J + iλ · K) P (I −iθ · J −iλ · K) = P + Hλ + θ ∧P.
(10.303)
Thus, one ﬁnds (exercise 10.41) that H is invariant under rotations, while P
transforms as a 3-vector
[Ji, H] = 0
and
[Ji, Pj] = iϵijkPk
(10.304)
and that
[Ki, H] = −iPi
and
[Ki, Pj] = iδijH.
(10.305)
By combining these equations with (10.285), one may write (exercise 10.43) the
Lie algebra of the Poincaré group as
i[Jab, Jcd] = ηbc Jad −ηac Jbd −ηda Jcb + ηdb Jca,
i[Pa, Jbc] = ηabPc −ηacPb,
[Pa, Pb] = 0.
(10.306)
Further reading
The classic Lie Algebras in Particle Physics (Georgi, 1999), which inspired much
of this chapter, is outstanding.
396

EXERCISES
Exercises
10.1
Show that all n×n (real) orthogonal matrices O leave invariant the quadratic
form x2
1 + x2
2 + · · · + x2
n, that is, that if x′ = Ox, then x′2 = x2.
10.2
Show that the set of all n × n orthogonal matrices forms a group.
10.3
Show that all n × n unitary matrices U leave invariant the quadratic form
|x1|2 + |x2|2 + · · · + |xn|2, that is, that if x′ = Ux, then |x|′2 = |x|2.
10.4
Show that the set of all n × n unitary matrices forms a group.
10.5
Show that the set of all n × n unitary matrices with unit determinant forms
a group.
10.6
Show that the matrix D( j)
m′m(g) = ⟨j, m′|U(g)| j, m⟩is unitary because the
rotation operator U(g) is unitary ⟨j, m′|U†(g)U(g)| j, m⟩= δm′m.
10.7
Invent a group of order 3 and compute its multiplication table. For extra
credit, prove that the group is unique.
10.8
Show that the relation (10.20) between two equivalent representations is an
isomorphism.
10.9
Suppose that D1 and D2 are equivalent, irreducible representations of a ﬁnite
group G so that D2(g) = SD1(g)S−1 for all g ∈G. What can you say about
a matrix A that satisﬁes D2(g) A = A D1(g) for all g ∈G?
10.10 Find all components of the matrix exp(iαA) in which
A =
⎛
⎝
0
0
−i
0
0
0
i
0
0
⎞
⎠.
(10.307)
10.11 If [A, B] = B, ﬁnd eiαABe−iαA. Hint: what are the α-derivatives of this
expression?
10.12 Show that the tensor-product matrix (10.31) of two representations D1 and
D2 is a representation.
10.13 Find a 4×4 matrix S that relates the tensor-product representation D1/2⊗1/2
to the direct sum D1 ⊕D0.
10.14 Find the generators in the adjoint representation of the group with structure
constants fabc = ϵabc where a, b, c run from 1 to 3. Hint: the answer is three
3 × 3 matrices ta, often written as La.
10.15 Show that the generators (10.90) satisfy the commutation relations (10.93).
10.16 Show that the demonstrated equation (10.98) implies the commutation
relation (10.99).
10.17 Use the Cayley–Hamilton theorem (1.264) to show that the 3 × 3 matrix
(10.96) that represents a right-handed rotation of θ radians about the axis θ
is given by (10.97).
10.18 Verify the mixed Jacobi identity (10.142).
10.19 For the group SU(3), ﬁnd the structure constants f123 and f231.
10.20 Show that every 2 × 2 unitary matrix of unit determinant is a quaternion of
unit norm.
397

GROUP THEORY
10.21 Show that the quaternions as deﬁned by (10.175) are closed under addition
and multiplication and that the product xq is a quaternion if x is real and q
is a quaternion.
10.22 Show that the derivative f ′(q) (10.184) of the quaternionic function f (q) = q2
depends upon the direction along which q′ →0.
10.23 Show that the generators (10.188) of Sp(2n) obey commutation relations of
the form (10.189) for some real structure constants fabc.
10.24 Show that for 0 < ϵ ≪1, the real 2n × 2n matrix T = exp(ϵJS) satisﬁes
TTJT = J (at least up to terms of order ϵ2) and so is in Sp(2n, R).
10.25 Show that the matrices T of (10.197) are in Sp(2, R).
10.26 Using the parametrization (10.217) of the group SU(2), show that the
parameters a(c, b) that describe the product g(a(c, b)) = g(c) g(b) are those
of (10.219).
10.27 Use formulas (10.219) and (10.212) to show that the left-invariant measure
for SU(2) is given by (10.220).
10.28 In tensor notation, which is explained in chapter 11, the condition (10.229)
that I + ω be an inﬁnitesimal Lorentz transformation reads

ωTa
b = ω a
b =
−ηbc ωc
d ηda in which sums over c and d from 0 to 3 are understood. In this
notation, the matrix ηef lowers indices and ηgh raises them, so that ω a
b
=
−ωbd ηda. (Both ηef and ηgh are numerically equal to the matrix η displayed
in equation (10.222).) Multiply both sides of the condition (10.229) by ηae
and use the relation ηda ηae = ηd
e ≡δd
e to show that the matrix ωab with
both indices lowered (or raised) is antisymmetric, that is,
ωba = −ωab
and
ωba = −ωab.
(10.308)
10.29 Show that the six matrices (10.231) and (10.232) satisfy the SO(3, 1) condi-
tion (10.229).
10.30 Show that the six generators J and K obey the commutations relations
(10.234–10.236).
10.31 Show that if J and K satisfy the commutation relations (10.234–10.236) of
the Lie algebra of the Lorentz group, then so do J and −K.
10.32 Show that the six generators J+ and J−obey the commutation relations
(10.243).
10.33 Relate the parameter α in the deﬁnition (10.253) of the standard boost B(p)
to the 4-vector p and the mass m.
10.34 Derive the formulas for D(1/2,0)(0, α ˆp) given in equation (10.254).
10.35 Derive the formulas for D(0,1/2)(0, α ˆp) given in equation (10.271).
10.36 For inﬁnitesimal complex z, derive the 4-vector properties (10.255 & 10.272)
of (−I, σ) under D(1/2,0) and of (I, σ) under D(0,1/2).
10.37 Show that under the unitary Lorentz transformation (10.273), the action
density (10.258) is Lorentz covariant (10.259).
10.38 Show that under the unitary Lorentz transformations (10.257 & 10.273), the
Majorana mass terms (10.266 & 10.279) are Lorentz covariant.
398

EXERCISES
10.39 Show that the deﬁnitions of the gamma matrices (10.281) and of the gener-
ators (10.283) imply that the gamma matrices transform as a 4-vector under
Lorentz transformations (10.284).
10.40 Show that (10.283) and (10.284) imply that the generators Jab satisfy the
commutation relations of the Lorentz group.
10.41 Show that the spinor ζ = σ2ξ∗deﬁned by (10.295) is right-handed (10.273)
if ξ is left-handed (10.257).
10.42 Use (10.303) to get (10.304 & 10.305).
10.43 Derive (10.306) from (10.285, 10.299, & 10.305).
399

11
Tensors and local symmetries
11.1 Points and coordinates
A point on a curved surface or in a curved space also is a point in a
higher-dimensional ﬂat space called an embedding space. For instance, a point
on a sphere also is a point in three-dimensional euclidean space and in four-
dimensional space-time. One always can add extra dimensions, but it’s simpler
to use as few as possible, three in the case of a sphere.
On a sufﬁciently small scale, any reasonably smooth space locally looks like
n-dimensional euclidean space. Such a space is called a manifold. Incidentally,
according to Whitney’s embedding theorem, every n-dimensional connected,
smooth manifold can be embedded in 2n-dimensional euclidean space R2n. So
the embedding space for such spaces in general relativity has no more than eight
dimensions.
We use coordinates to label points. For example, we can choose a polar axis
and a meridian and label a point on the sphere by its polar and azimuthal
angles (θ, φ) with respect to that axis and meridian. If we use a different axis
and meridian, then the coordinates (θ′, φ′) for the same point will change.
Points are physical, coordinates are metaphysical. When we change our system
of coordinates, the points don’t change, but their coordinates do.
Most points p have unique coordinates xi(p) and x′i(p) in their coordinate
systems. For instance, polar coordinates (θ, φ) are unique for all points on a
sphere – except the north and south poles which are labeled by θ = 0 and θ = π
and all 0 ≤φ < 2π. By using more than one coordinate system, one usually
can arrange to label every point uniquely. In the ﬂat three-dimensional space in
which the sphere is a surface, each point of the sphere has unique coordinates,
⃗p = (x, y, z).
400

11.3 CONTRAVARIANT VECTORS
We will use coordinate systems that represent points on the manifold
uniquely and smoothly at least in local patches, so that the maps
x′i = x′i(p) = x′i(p(x)) = x′i(x)
(11.1)
and
xi = xi(p) = xi(p(x′)) = xi(x′)
(11.2)
are well deﬁned, differentiable, and one to one in the patches. We’ll often group
the n coordinates xi together and write them collectively as x without a super-
script. Since the coordinates x(p) label the point p, we sometimes will call them
“the point x.” But p and x are different. The point p is unique with inﬁnitely
many coordinates x, x′, x′′, ... in inﬁnitely many coordinate systems.
11.2 Scalars
A scalar is a quantity B that is the same in all coordinate systems
B′ = B.
(11.3)
If it also depends upon the coordinates x of the space-time point p, and
B′(x′) = B(x),
(11.4)
then it is a scalar ﬁeld.
11.3 Contravariant vectors
The change dx′i due to changes in the unprimed coordinates is
dx′i =

j
∂x′i
∂xj dxj.
(11.5)
This rule deﬁnes contravariant vectors: a quantity Ai is a contravariant vector if
it transforms like dxi
A′i =

j
∂x′i
∂xj Aj.
(11.6)
The coordinate differentials dxi form a contravariant vector. A contravariant
vector Ai(x) that depends on the coordinates x and transforms as
A′i(x′) =

j
∂x′i
∂xj Aj(x)
(11.7)
is a contravariant vector ﬁeld.
401

TENSORS AND LOCAL SYMMETRIES
11.4 Covariant vectors
The chain rule for partial derivatives
∂
∂x′i =

j
∂xj
∂x′i
∂
∂xj
(11.8)
deﬁnes covariant vectors: a vector Ci that transforms as
C′
i =

j
∂xj
∂x′i Cj
(11.9)
is a covariant vector. If it also is a function of x, then it is a covariant vector ﬁeld
and
C′
i(x′) =

j
∂xj
∂x′i Cj(x).
(11.10)
Example 11.1 (Gradient of a scalar)
The derivatives of a scalar ﬁeld form a
covariant vector ﬁeld. For by using the chain rule to differentiate the equation
B′(x′) = B(x) that deﬁnes a scalar ﬁeld, one ﬁnds
∂B′(x′)
∂x′i
= ∂B(x)
∂x′i
=

j
∂xj
∂x′i
∂B(x)
∂xj ,
(11.11)
which shows that the gradient ∂B(x)/∂xj is a covariant vector ﬁeld.
11.5 Euclidean space in euclidean coordinates
If we use euclidean coordinates to describe points in euclidean space, then
covariant and contravariant vectors are the same.
Euclidean space has a natural inner product (section 1.6), the usual dot-
product, which is real and symmetric. In a euclidean space of n dimensions,
we may choose any n ﬁxed, orthonormal basis vectors ei
(ei, ej) ≡ei · ej =
n

k=1
ek
i ek
j = δij
(11.12)
and use them to represent any point p as the linear combination
p =
n

i=1
ei xi.
(11.13)
402

11.5 EUCLIDEAN SPACE IN EUCLIDEAN COORDINATES
The coefﬁcients xi are the euclidean coordinates in the ei basis. Since the basis
vectors ei are orthonormal, each xi is an inner product or dot-product
xi = ei · p =
n

j=1
ei · ej xj =
n

j=1
δij xj.
(11.14)
The dual vectors ei are deﬁned as those vectors whose inner products with the
ej are (ei, ej) = δi
j. In this section, they are the same as the vectors ei, and so we
shall not bother to distinguish ei from ei = ei.
If we use different orthonormal vectors e′
i as a basis
p =
n

i=1
e′
i x′i
(11.15)
then we get new euclidean coordinates x′
i = e′
i · p for the same point p. These
two sets of coordinates are related by the equations
x′i = e′
i · p =
n

j=1
e′
i · ej xj,
xj = ej · p =
n

k=1
ej · e′
k x′k.
(11.16)
Because the basis vectors e and e′ are all independent of x, the coefﬁcients
∂x′i/∂xj of the transformation laws for contravariant (11.6) and covariant (11.9)
vectors are
contravariant
∂x′i
∂xj = e′
i · ej
and
∂xj
∂x′i = ej · e′
i
covariant.
(11.17)
But the dot-product (1.82) is symmetric, and so these are the same:
∂x′i
∂xj = e′
i · ej = ej · e′
i = ∂xj
∂x′i .
(11.18)
Contravariant and covariant vectors transform the same way in euclidean space
with euclidean coordinates.
The relations between x′i and xj imply that
x′i =
n

j,k=1

e′
i · ej
 
ej · e′
k

x′k.
(11.19)
Since this holds for all coordinates x′i, we have
n

j=1

e′
i · ej
 
ej · e′
k

= δik.
(11.20)
403

TENSORS AND LOCAL SYMMETRIES
The coefﬁcients e′
i · ej form an orthogonal matrix, and the linear operator
n

i=1
eie′
i
T =
n

i=1
|ei⟩⟨e′
i|
(11.21)
is an orthogonal (real, unitary) transformation. The change x →x′ is a rotation
plus a possible reﬂection (exercise 11.2).
Example 11.2 (A euclidean space of two dimensions)
In two-dimensional
euclidean space, one can describe the same point by euclidean (x, y) and polar
(r, θ) coordinates. The derivatives
∂r
∂x = x
r = ∂x
∂r
and
∂r
∂y = y
r = ∂y
∂r
(11.22)
respect the symmetry (11.18), but (exercise 11.1) these derivatives
∂θ
∂x = −y
r2 ̸= ∂x
∂θ = −yr
x
and
∂θ
∂y = x
r2 ̸= ∂y
∂θ = x
(11.23)
do not.
11.6 Summation conventions
When a given index is repeated in a product, that index usually is being summed
over. So to avoid distracting summation symbols, one writes
Ai Bi ≡
n

i=1
Ai Bi.
(11.24)
The sum is understood to be over the relevant range of indices, usually from
0 or 1 to 3 or n. Where the distinction between covariant and contravari-
ant indices matters, an index that appears twice in the same monomial, once
as a subscript and once as a superscript, is a dummy index that is summed
over as in
Ai Bi ≡
n

i=1
Ai Bi.
(11.25)
These summation conventions make tensor notation almost as compact as
matrix notation. They make equations easier to read and write.
404

11.7 MINKOWSKI SPACE
Example 11.3 (The Kronecker delta)
The summation convention and the chain
rule imply that
∂x′i
∂xk
∂xk
∂x′j = ∂x′i
∂x′j = δi
j =
 1
if i = j,
0
if i ̸= j.
(11.26)
The repeated index k has disappeared in this contraction.
11.7 Minkowski space
Minkowski space has one time dimension, labeled by k = 0, and n space
dimensions. In special relativity n = 3, and the Minkowski metric η
ηkl = ηkl =
⎧
⎨
⎩
−1
if k = l = 0,
1
if 1 ≤k = l ≤3 ,
0
if k ̸= l
(11.27)
deﬁnes an inner product between points p and q with coordinates xk
p and xℓ
q as
(p, q) = p · q = pk ηkl ql = (q, p).
(11.28)
If one time component vanishes, the Minkowski inner product reduces to the
euclidean dot-product (1.82).
We can use different sets {ei} and {e′
i} of n + 1 Lorentz-orthonormal basis
vectors
(ei, ej) = ei · ej = ek
i ηkl el
j = ηij = e′
i · e′
j = (e′
i, e′
j)
(11.29)
to represent any point p in the space either as a linear combination of the vec-
tors ei with coefﬁcients xi or as a linear combination of the vectors e′
i with
coefﬁcients x′i
p = ei xi = e′
i x′i.
(11.30)
The dual vectors, which carry upper indices, are deﬁned as
ei = ηij ej
and
e′i = ηij e′
j.
(11.31)
They are orthonormal to the vectors ei and e′
i because
(ei, ej) = ei · ej = ηik ek · ej = ηik ηkj = δi
j
(11.32)
and similarly (e′i, e′
j) = e′i·e′
j = δi
j. Since the square of the matrix η is the identity
matrix ηℓiηij = δj
ℓ, it follows that
ei = ηij ej
and
e′
i = ηij e′j.
(11.33)
The metric η raises (11.31) and lowers (11.33) the index of a basis vector.
405

TENSORS AND LOCAL SYMMETRIES
The component x′i is related to the components xj by the linear map
x′i = e′i · p = e′i · ej xj.
(11.34)
Such a map from a 4-vector x to a 4-vector x′ is a Lorentz transformation
x′i = Li
j xj
with matrix
Li
j = e′i · ej.
(11.35)
The inner product (p, q) of two points p = ei xi = e′
i x′i and q = ek yk = e′
k y′k
is physical and so is invariant under Lorentz transformations
(p, q) = xi yk ei · ek = ηik xi yk = x′i y′k e′
i · e′
k = ηik x′i y′k.
(11.36)
With x′i = Li
r xr and y′k = Lk
s xs, this invariance is
ηrs xrys = ηik Li
r xr Lk
s ys
(11.37)
or since xr and ys are arbitrary
ηrs = ηik Li
r Lk
s = Li
r ηik Lk
s.
(11.38)
In matrix notation, a left index labels a row, and a right index labels a column.
Transposition interchanges rows and columns Li
r = LTi
r, so
ηrs = LTi
r ηik Lk
s
or
η = LT η L
(11.39)
in matrix notation. In such matrix products, the height of an index – whether
it is up or down – determines whether it is contravariant or covariant but does
not affect its place in its matrix.
Example 11.4 (A boost)
The matrix
L =
⎛
⎜⎜⎝
γ

γ 2 −1
0
0

γ 2 −1
γ
0
0
0
0
1
0
0
0
0
1
⎞
⎟⎟⎠
(11.40)
where γ = 1/

1 −v2/c2 represents a Lorentz transformation that is a boost
in the x-direction. Boosts and rotations are Lorentz transformations. Working
with 4 × 4 matrices can get tedious, so students are advised to think in terms of
scalars, like p · x = piηijxj = p · x −Et whenever possible.
If the basis vectors e and e′ are independent of p and of x, then the coefﬁcients
of the transformation law (11.6) for contravariant vectors are
∂x′i
∂xj = e′i · ej.
(11.41)
406

11.8 LORENTZ TRANSFORMATIONS
Similarly, the component xj is xj = ej · p = ej · e′
i x′i, so the coefﬁcients of the
transformation law (11.9) for covariant vectors are
∂xj
∂x′i = ej · e′
i.
(11.42)
Using η to raise and lower the indices in the formula (11.41) for the coefﬁcients
of the transformation law (11.6) for contravariant vectors, we ﬁnd
∂x′i
∂xj = e′i · ej = ηik ηjℓe′
k · eℓ= ηik ηjℓ
∂xℓ
∂x′k ,
(11.43)
which is ± ∂xj/∂x′i. So if we use coordinates associated with ﬁxed basis vectors
ei in Minkowski space, then the coefﬁcients for the two kinds of transformation
laws differ only by occasional minus signs.
So if Ai is a contravariant vector
A
′i = ∂x′i
∂xj Aj
(11.44)
then the relation (11.43) between the two kinds of coefﬁcient implies that
ηsi A
′i = ηsi
∂x′i
∂xj Aj = ηsi ηik ηjℓ
∂xℓ
∂x′k Aj = δk
s
∂xℓ
∂x′k ηjℓAj = ∂xℓ
∂x′s ηℓj Aj,
(11.45)
which shows that Aℓ= ηℓj Aj transforms covariantly
A′
s = ∂xℓ
∂x′s Aℓ.
(11.46)
The metric η turns a contravariant vector into a covariant one. It also switches
a covariant vector Aℓback to its contravariant form Ak
ηkℓAℓ= ηkℓηℓjAj = δk
j Aj = Ak.
(11.47)
In Minkowski space, one uses η to raise and lower indices
Ai = ηij Aj and Ai = ηij Aj.
(11.48)
In general relativity, the space-time metric g raises and lowers indices.
11.8 Lorentz transformations
In section 11.7, Lorentz transformations arose as linear maps of the coordinates
due to a change of basis. They also are linear maps of the basis vectors ei that
preserve the inner products
(ei, ej) = ei · ej = ηij = e′
i · e′
j = (e′
i, e′
j).
(11.49)
407

TENSORS AND LOCAL SYMMETRIES
The vectors ei are four linearly independent four-dimensional vectors, and so
they span four-dimensional Minkowski space and can represent the vectors e′
i as
e′
i =  k
i
ek
(11.50)
where the coefﬁcients  k
i
are real numbers. The requirement that the new basis
vectors e′
i are Lorentz orthonormal gives
ηij = e′
i · e′
j =  k
i
ek ·  ℓ
j eℓ=  k
i
ek · eℓ ℓ
j
=  k
i
ηkℓ ℓ
j
(11.51)
or in matrix notation
η =  η T
(11.52)
where T is the transpose (T)ℓ
j =  ℓ
j . Evidently T satisﬁes the deﬁnition
(11.39) of a Lorentz transformation. What Lorentz transformation is it? The
point p must remain invariant, so by (11.35 & 11.50) one has
p = e′
i x′i =  k
i
ek Li
jxj = δk
j ek xj = ej xj
(11.53)
whence  k
i
Li
j = δk
j or TL = I. So T = L−1.
By multiplying condition (11.52) by the metric η ﬁrst from the left and then
from the right and using the fact that η2 = I, we ﬁnd
1 = η2 = η  η T =  η T η,
(11.54)
which gives us the inverse matrices
−1 = η T η = LT
and
(T)−1 = η  η = L.
(11.55)
In special relativity, contravariant vectors transform as
dx′i = Li
j dxj
(11.56)
and since xj = L−1j
i x′i, the covariant ones transform as
∂
∂x′i = ∂xj
∂x′i
∂
∂xj = L−1j
i
∂
∂xj =  j
i
∂
∂xj .
(11.57)
By taking the determinant of both sides of (11.52) and using the transpose
(1.194) and product (1.207) rules for determinants, we ﬁnd that det  = ± 1.
11.9 Special relativity
The space-time of special relativity is ﬂat, four-dimensional Minkowski space.
The inner product (p −q) · (p −q) of the interval p −q between two points
is physical and independent of the coordinates and therefore invariant. If the
408

11.9 SPECIAL RELATIVITY
points p and q are close neighbors with coordinates xi + dxi for p and xi for q,
then that invariant inner product is
(p −q) · (p −q) = ei dxi · ej dxj = dxi ηij dxj = dx2 −(dx0)2
(11.58)
with dx0 = c dt. (At some point in what follows, we’ll measure distance in light-
seconds so that c = 1.) If the points p and q are on the trajectory of a massive
particle moving at velocity v, then this invariant quantity is the square of the
invariant distance
ds2 = dx2 −c2dt2 =

v2 −c2
dt2,
(11.59)
which always is negative since v < c. The time in the rest frame of the particle
is the proper time. The square of its differential element is
dτ 2 = −ds2/c2 =

1 −v2/c2
dt2.
(11.60)
A particle of mass zero moves at the speed of light, and so its proper time is
zero. But for a particle of mass m > 0 moving at speed v, the element of proper
time dτ is smaller than the corresponding element of laboratory time dt by the
factor

1 −v2/c2. The proper time is the time in the rest frame of the particle,
dτ = dt when v = 0. So if T(0) is the lifetime of a particle (at rest), then the
apparent lifetime T(v) when the particle is moving at speed v is
T(v) = dt =
dτ

1 −v2/c2 =
T(0)

1 −v2/c2 ,
(11.61)
which is longer – an effect known as time dilation.
Example 11.5 (Time dilation in muon decay)
A muon at rest has a mean life
of T(0) = 2.2 × 10−6 seconds. Cosmic rays hitting nitrogen and oxygen nuclei
make pions high in the Earth’s atmosphere. The pions rapidly decay into muons
in 2.6 × 10−8 s. A muon moving at the speed of light from 10 km takes at least
t = 10 km/300, 000 (km/sec) = 3.3 × 10−5 s to hit the ground. Were it not for
time dilation, the probability P of such a muon reaching the ground as a muon
would be
P = e−t/T(0) = exp(−33/2.2) = e−15 = 2.6 × 10−7.
(11.62)
The (rest) mass of a muon is 105.66 MeV. So a muon of energy E = 749 MeV
has by (11.69) a time-dilation factor of
1

1 −v2/c2 =
E
mc2 = 749
105.7 = 7.089 =
1

1 −(0.99)2 .
(11.63)
409

TENSORS AND LOCAL SYMMETRIES
So a muon moving at a speed of v = 0.99 c has an apparent mean life T(v) given
by equation (11.61) as
T(v) =
E
mc2 T(0) =
T(0)

1 −v2/c2 = 2.2 × 10−6 s

1 −(0.99)2 = 1.6 × 10−5 s.
(11.64)
The probability of survival with time dilation is
P = e−t/T(v) = exp(−33/16) = 0.12
(11.65)
so that 12% survive. Time dilation increases the chance of survival by a factor of
460,000 – no small effect.
11.10 Kinematics
From the scalar dτ, and the contravariant vector dxi, we can make the 4-vector
ui = dxi
dτ = dt
dτ

dx0
dt , dx
dt

=
1

1 −v2/c2 (c, v) ,
(11.66)
in which u0 = c dt/dτ = c/

1 −v2/c2 and u = u0 v/c. The product mui is the
energy–momentum 4-vector pi
pi = m ui = m dxi
dτ = m dt
dτ
dxi
dt =
m

1 −v2/c2
dxi
dt
=
m

1 −v2/c2 (c, v) =
E
c , p

.
(11.67)
Its invariant inner product is a constant characteristic of the particle and
proportional to the square of its mass
c2 pi pi = mc ui mc ui = −E2 + c2 p 2 = −m2 c4.
(11.68)
Note that the time-dilation factor is the ratio of the energy of a particle to its
rest energy
1

1 −v2/c2 =
E
mc2
(11.69)
and the velocity of the particle is its momentum divided by its equivalent mass
E/c2
v =
p
E/c2 .
(11.70)
The analog of F = m a is
m d2xi
dτ 2 = m dui
dτ = dpi
dτ = f i,
(11.71)
in which p0 = E, and f i is a 4-vector force.
410

11.11 ELECTRODYNAMICS
Example 11.6 (Time dilation and proper time)
In the frame of a laboratory,
a particle of mass m with 4-momentum pi
lab = (E/c, p, 0, 0) travels a distance
L in a time t for a 4-vector displacement of xi
lab = (ct, L, 0, 0). In its own rest
frame, the particle’s 4-momentum and 4-displacement are pi
rest = (mc, 0, 0, 0)
and xi
rest = (cτ, 0, 0, 0). Since the Minkowski inner product of two 4-vectors is
Lorentz invariant, we have

pixi

rest =

pixi

lab
or
Et −pL = mc2τ = mc2t

1 −v2/c2
(11.72)
so a massive particle’s phase exp(−ipixi/¯h) is exp(imc2τ/¯h).
Example 11.7 (p + π → + K)
What is the minimum energy that a beam of
pions must have to produce a sigma hyperon and a kaon by striking a proton at
rest? Conservation of the energy–momentum 4-vector gives pp + pπ = p + pK.
We set c = 1 and use this equality in the invariant form (pp +pπ)2 = (p +pK)2.
We compute (pp + pπ)2 in the pp = (mp, 0) frame and set it equal to (p + pK)2
in the frame in which the spatial momenta of the  and the K cancel:
(pp + pπ)2 = p2
p + p2
π + 2pp · pπ = −m2
p −m2
π −2mpEπ
= (p + pK)2 = −(m + mK)2 .
(11.73)
Thus, since the relevant masses (in MeV) are m+ = 1189.4, mK+ = 493.7,
mp = 938.3, and mπ+ = 139.6, the minimum total energy of the pion is
Eπ =
(m + mK)2 −m2
p −m2
π
2mp
≈1030
MeV,
(11.74)
of which 890 MeV is kinetic.
11.11 Electrodynamics
In electrodynamics and in MKSA (SI) units, the three-dimensional vector
potential A and the scalar potential φ form a covariant 4-vector potential
Ai =
−φ
c , A

.
(11.75)
The contravariant 4-vector potential is Ai = (φ/c, A). The magnetic induction is
B = ∇× A
or
Bi = ϵijk∂jAk,
(11.76)
in which ∂j = ∂/∂xj, the sum over the repeated indices j and k runs from 1 to 3,
and ϵijk is totally antisymmetric with ϵ123 = 1. The electric ﬁeld is
Ei = c
∂A0
∂xi −∂Ai
∂x0

= −∂φ
∂xi −∂Ai
∂t
(11.77)
411

TENSORS AND LOCAL SYMMETRIES
where x0 = ct. In 3-vector notation, E is given by the gradient of φ and the
time-derivative of A
E = −∇φ −˙A.
(11.78)
In terms of the second-rank, antisymmetric Faraday ﬁeld-strength tensor
Fij = ∂Aj
∂xi −∂Ai
∂xj = −Fji
(11.79)
the electric ﬁeld is Ei = c Fi0 and the magnetic ﬁeld Bi is
Bi = 1
2ϵijk Fjk = 1
2ϵijk
∂Ak
∂xj −∂Aj
∂xk

= (∇× A)i
(11.80)
where the sum over repeated indices runs from 1 to 3. The inverse equation
Fjk = ϵjkiBi for spatial j and k follows from the Levi-Civita identity (1.449)
ϵjkiBi = 1
2ϵjkiϵinm Fnm = 1
2ϵijkϵinm Fnm
= 1
2

δjn δkm −δjm δkn

Fnm = 1
2

Fjk −Fkj

= Fjk.
(11.81)
In 3-vector notation and MKSA = SI units, Maxwell’s equations are a ban
on magnetic monopoles and Faraday’s law, both homogeneous,
∇· B = 0
and
∇× E + ˙B = 0
(11.82)
and Gauss’s law and the Maxwell-Ampère law, both inhomogeneous,
∇· D = ρf
and
∇× H = jf + ˙D.
(11.83)
Here ρf is the density of free charge and jf is the free current density. By free,
we understand charges and currents that do not arise from polarization and are
not restrained by chemical bonds. The divergence of ∇× H vanishes (like that
of any curl), and so the Maxwell–Ampère law and Gauss’s law imply that free
charge is conserved
0 = ∇· (∇× H) = ∇· jf + ∇· ˙D = ∇· jf + ˙ρf.
(11.84)
If we use this continuity equation to replace ∇· jf with −˙ρf in its middle form
0 = ∇·jf+∇· ˙D, then we see that the Maxwell–Ampère law preserves the Gauss
law constraint in time
0 = ∇· jf + ∇· ˙D = ∂
∂t (−ρf + ∇· D) .
(11.85)
Similarly, Faraday’s law preserves the constraint ∇· B = 0
0 = −∇· (∇× E) = ∂
∂t∇· B = 0.
(11.86)
412

11.11 ELECTRODYNAMICS
In a linear, isotropic medium, the electric displacement D is related to the
electric ﬁeld E by the permittivity D = ϵE and the magnetic or magnetizing ﬁeld
H differs from the magnetic induction B by the permeability H = B/μ.
On a sub-nanometer scale, the microscopic form of Maxwell’s equations
applies. On this scale, the homogeneous equations (11.82) are unchanged, but
the inhomogeneous ones are
∇· E = ρ
ϵ0
and
∇× B = μ0 j + ϵ0 μ0 ˙E = μ0 j +
˙E
c2 ,
(11.87)
in which ρ and j are the total charge and current densities, and ϵ0 = 8.854 ×
10−12 F/m and μ0 = 4π × 10−7 N/A2 are the electric and magnetic constants,
whose product is the inverse of the square of the speed of light, ϵ0μ0 = 1/c2.
Gauss’s law and the Maxwell–Ampère law (11.87) imply (exercise 11.6) that the
microscopic (total) current 4-vector j = (cρ, j) obeys the continuity equation
˙ρ + ∇· j = 0. Electric charge is conserved.
In vacuum, ρ = j = 0, D = ϵ0E, and H = B/μ0, and Maxwell’s equations
become
∇· B = 0
and
∇× E + ˙B = 0,
∇· E = 0
and
∇× B = 1
c2 ˙E.
(11.88)
Two of these equations ∇· B = 0 and ∇· E = 0 are constraints. Taking the curl
of the other two equations, we ﬁnd
∇× (∇× E) = −1
c2 ¨E
and
∇× (∇× B) = −1
c2 ¨B.
(11.89)
One may use the Levi-Civita identity (1.449) to show (exercise 11.8) that
∇× (∇× E) = ∇(∇· E) −△E and ∇× (∇× B) = ∇(∇· B) −△B, (11.90)
in which △≡∇2. Since in vacuum the divergence of E vanishes, and since that
of B always vanishes, these identities and the curl–curl equations (11.89) tell us
that waves of E and B move at the speed of light
1
c2 ¨E −△E = 0
and
1
c2 ¨B −△B = 0.
(11.91)
We may write the two homogeneous Maxwell equations (11.82) as
∂iFjk + ∂kFij + ∂jFki = ∂i

∂jAk −∂kAj

+ ∂k

∂iAj −∂jAi

+∂j (∂kAi −∂iAk) = 0
(11.92)
(exercise 11.9). This relation, known as the Bianchi identity, actually is a
generally covariant tensor equation
ϵℓijk∂iFjk = 0,
(11.93)
413

TENSORS AND LOCAL SYMMETRIES
in which ϵℓijk is totally antisymmetric, as explained in section 11.32. There are
four versions of this identity (corresponding to the four ways of choosing three
different indices i, j, k from among four and leaving out one, ℓ). The ℓ= 0
case gives the scalar equation ∇· B = 0, and the three that have ℓ̸= 0 give the
vector equation ∇× E + ˙B = 0.
In tensor notation, the microscopic form of the two inhomogeneous equa-
tions (11.87) – the laws of Gauss and Ampère – are
∂iFki = μ0 jk,
(11.94)
in which jk is the current 4-vector
jk = (cρ, j) .
(11.95)
The Lorentz force law for a particle of charge q is
m d2xi
dτ 2 = m dui
dτ = dpi
dτ = f i = q Fij dxj
dτ = q Fij uj.
(11.96)
We may cancel a factor of dt/dτ from both sides and ﬁnd for i = 1, 2, 3
dpi
dt = q

−Fi0 + ϵijkBkvj

or
dp
dt = q (E + v × B)
(11.97)
and for i = 0
dE
dt = q E · v,
(11.98)
which shows that only the electric ﬁeld does work. The only special-relativistic
correction needed in Maxwell’s electrodynamics is a factor of 1/

1 −v2/c2 in
these equations. That is, we use p = mu = mv/

1 −v2/c2 not p = mv in
(11.97), and we use the total energy E not the kinetic energy in (11.98). The rea-
son why so little of classical electrodynamics was changed by special relativity
is that electric and magnetic effects were accessible to measurement during the
1800s. Classical electrodynamics was almost perfect.
Keeping track of factors of the speed of light is a lot of trouble and a
distraction; in what follows, we’ll often use units with c = 1.
11.12 Tensors
Tensors are structures that transform like products of vectors. A ﬁrst-rank
tensor is a covariant or a contravariant vector. Second-rank tensors also are
distinguished by how they transform under changes of coordinates:
414

11.12 TENSORS
contravariant
M′ij = ∂x′i
∂xk
∂x′j
∂xl Mkl,
mixed
N′i
j = ∂x′i
∂xk
∂xl
∂x′j Nk
l ,
covariant
F′
ij = ∂xk
∂x′i
∂xl
∂x′j Fkl.
(11.99)
We can deﬁne tensors of higher rank by extending these deﬁnitions to quantities
with more indices.
Example 11.8 (Some second-rank tensors)
If Ak and Bℓare covariant vec-
tors, and Cm and Dn are contravariant vectors, then the product Cm Dn is a
second-rank contravariant tensor, and all four products Ak Cm, Ak Dn, Bk Cm,
and Bk Dn are second-rank mixed tensors, while Cm Dn as well as Cm Cn and
Dm Dn are second-rank contravariant tensors.
Since the transformation laws that deﬁne tensors are linear, any linear com-
bination of tensors of a given rank and kind is a tensor of that rank and
kind. Thus if Fij and Gij are both second-rank covariant tensors, then so is
their sum
Hij = Fij + Gij.
(11.100)
A covariant tensor is symmetric if it is independent of the order of its indices.
That is, if Sik = Ski, then S is symmetric. Similarly, a contravariant ten-
sor is symmetric if permutations of its indices leave it unchanged. Thus A is
symmetric if Aik = Aki.
A covariant or contravariant tensor is antisymmetric if it changes sign when
any two of its indices are interchanged. So Aik, Bik, and Cijk are antisymmetric if
Aik = −Aki
and
Bik = −Bki,
and
Cijk = Cjki = Ckij = −Cjik = −Cikj = −Ckji.
(11.101)
Example 11.9 (Three important tensors)
The Maxwell ﬁeld strength Fkl(x)
is a second-rank covariant tensor; so is the metric of space-time gij(x). The
Kronecker delta δi
j is a mixed second-rank tensor; it transforms as
δ′i
j = ∂x′i
∂xk
∂xl
∂x′j δk
l = ∂x′i
∂xk
∂xk
∂x′j = ∂x′i
∂xj = δi
j.
(11.102)
So it is invariant under changes of coordinates.
415

TENSORS AND LOCAL SYMMETRIES
Example 11.10 (Contractions)
Although the product Ak Cℓis a mixed second-
rank tensor, the product Ak Ck transforms as a scalar because
A′
k C′k = ∂xℓ
∂x′k
∂x′k
∂xm AℓCm = ∂xℓ
∂xm AℓCm = δℓ
mAℓCm = AℓCℓ.
(11.103)
A sum in which an index is repeated once covariantly and once contravariantly
is a contraction as in the Kronecker-delta equation (11.26). In general, the rank
of a tensor is the number of uncontracted indices.
11.13 Differential forms
By (11.10 & 11.5), a covariant vector ﬁeld contracted with contravariant
coordinate differentials is invariant under arbitrary coordinate transformations
A′ = A′
i dx′i = ∂xj
∂x′i Aj
∂x′i
∂xk dxk = δj
k Aj dxk = Ak dxk = A.
(11.104)
This invariant quantity A = Ak dxk is a called a 1-form in the language of
differential forms introduced about a century ago by Élie Cartan (1869–1951,
son of a blacksmith).
The wedge product dx ∧dy of two coordinate differentials is the directed area
spanned by the two differentials and is deﬁned to be antisymmetric
dx ∧dy = −dy ∧dx
and
dx ∧dx = dy ∧dy = 0
(11.105)
so as to transform correctly under a change of coordinates. In terms of the
coordinates u = u(x, y) and v = v(x, y), the new element of area is
du ∧dv =
∂u
∂xdx + ∂u
∂ydy

∧
∂v
∂xdx + ∂v
∂ydy

.
(11.106)
Labeling partial derivatives by subscripts (6.20) and using the antisymmetry
(11.105), we see that the new element of area du ∧dv is the old area dx ∧dy
multiplied by the Jacobian J(u, v; x, y) of the transformation x, y →u, v
du ∧dv =

uxdx + uydy

∧

vxdx + vydy

= ux vx dx ∧dx + ux vy dx ∧dy + uy vx dy ∧dx + uy vy dy ∧dy
=

ux vy −uyvx

dx ∧dy
=

ux
uy
vx
vy
 dx ∧dy = J(u, v; x, y) dx ∧dy.
(11.107)
A contraction H = 1
2Hik dxi ∧dxk of a second-rank covariant tensor with a
wedge product of two differentials is a 2-form. A p -form is a rank-p covariant
tensor contracted with a wedge product of p differentials
416

11.13 DIFFERENTIAL FORMS
K = 1
p! Ki1...ip dxi1 ∧. . . dxip.
(11.108)
The exterior derivative d differentiates and adds a differential; it turns a
p-form into a (p + 1)-form. It converts a function or a 0-form f into a 1-form
df = ∂f
∂xi dxi
(11.109)
and a 1-form A = Aj dxj into a 2-form dA = d(Aj dxj) = (∂iAj) dxi ∧dxj.
Example 11.11 (The curl)
The exterior derivative of the 1-form
A = Ax dx + Ay dy + Az dz
(11.110)
is the 2-form
dA = ∂yAx dy ∧dx + ∂zAx dz ∧dx
+ ∂xAy dx ∧dy + ∂zAy dz ∧dy
+ ∂xAz dx ∧dz + ∂yAz dy ∧dz
=

∂yAz −∂zAy

dy ∧dz
+ (∂zAx −∂xAz) dz ∧dx
+

∂xAy −∂yAx

dx ∧dy
= (∇× A)x dy ∧dz + (∇× A)y dz ∧dx + (∇× A)z dx ∧dy,
(11.111)
in which we recognize the curl (6.39) of A.
The exterior derivative of the 1-form A = Aj dxj is the 2-form
dA = dAj ∧dxj = ∂iAj dxi ∧dxj = 1
2 Fij dxi ∧dxj = F,
(11.112)
in which ∂i = ∂/∂xi. So d turns the electromagnetic 1-form A – the 4-vector
potential or gauge ﬁeld Aj – into the Faraday 2-form – the tensor Fij. Its square
vanishes: dd applied to any p-form Q is zero
ddQi...dxi ∧· · · = d(∂rQi...)∧dxr ∧dxi ∧· · · = (∂s∂rQi...)dxs ∧dxr ∧dxi ∧· · · = 0
(11.113)
because ∂s∂rQ is symmetric in r and s while dxs ∧dxr is antisymmetric.
Some writers drop the wedges and write dxi ∧dxj as dxidxj while keeping
the rules of antisymmetry dxidxj = −dxjdxi and

dxi2 = 0. But this economy
prevents one from using invariant quantities like S = 1
2 Sik dxidxk, in which Sik
is a second-rank covariant symmetric tensor. If Mik is a covariant second-rank
tensor with no particular symmetry, then (exercise 11.7) only its antisymmet-
ric part contributes to the 2-form Mik dxi ∧dxk and only its symmetric part
contributes to the quantity Mik dxidxk.
417

TENSORS AND LOCAL SYMMETRIES
The exterior derivative d applied to the Faraday 2-form F = dA gives
dF = ddA = 0,
(11.114)
which is the Bianchi identity (11.93). A p-form H is closed if dH = 0. By
(11.114), the Faraday 2-form is closed, dF = 0.
A p-form H is exact if there is a (p+1)-form K whose differential is H = dK.
The identity (12.64) or dd = 0 implies that every exact form is closed. The
lemma of Poincaré shows that every closed form is locally exact.
If the Ai in the 1-form A = Aidxi commute with each other, then the 2-form
A2 = 0. But if the Ai don’t commute because they are matrices or operators or
Grassmann variables, then A2 need not vanish.
Example 11.12 (A static electric ﬁeld is closed and locally exact)
If ˙B = 0,
then by Faraday’s law (11.82) the curl of the electric ﬁeld vanishes, ∇× E = 0.
Writing the electrostatic ﬁeld as the 1-form E = Ei dxi for i = 1, 2, 3, we may
express the vanishing of its curl as
dE = ∂jEi dxj dxi = 1
2

∂jEi −∂iEj

dxj dxi = 0,
(11.115)
which says that E is closed. We can deﬁne a quantity VP(x) as a line integral of
the 1-form E along a path P to x from some starting point x0
VP(x) = −
 x
P, x0
Ei dxi = −

P
E
(11.116)
and so VP(x) will depend on the path P as well as on x0 and x. But if ∇× E =
0 in some ball (or neighborhood) around x and x0, then within that ball the
dependence on the path P drops out because the difference VP′(x) −VP(x) is the
line integral of E around a closed loop in the ball, which by Stokes’s theorem
(6.44) is an integral of the vanishing curl ∇× E over any surface S in the ball
whose boundary ∂S is the closed curve P′ −P
VP′(x) −VP(x) =
)
P′−P
Ei dxi =

S
(∇× E) · da = 0
(11.117)
or
VP′(x) −VP(x) =

∂S
E =

S
dE = 0
(11.118)
in the language of forms (George Stokes, 1819–1903). Thus the potential
VP(x) = V(x) is independent of the path, E =
−∇V(x), and the 1-form
E = Ei dxi = −∂iV dxi = −dV is exact.
The general form of Stokes’s theorem is that the integral of any p-form H
over the boundary ∂R of any (p + 1)-dimensional, simply connected, orientable
region R is equal to the integral of the (p + 1)-form dH over R
418

11.14 TENSOR EQUATIONS

∂R
H =

R
dH,
(11.119)
which for p = 1 gives (6.44).
Example 11.13 (Stokes’s theorem for 0-forms)
Here p = 0, the region R = [a, b]
is one-dimensional, H is a 0-form, and Stokes’s theorem is
H(b) −H(a) =

∂R
H =

R
dH =
 b
a
dH(x) =
 b
a
H′(x) dx,
(11.120)
familiar from elementary calculus.
Example 11.14 (Exterior derivatives anticommute with differentials)
The exte-
rior derivative acting on two 1-forms A = Aidxi and B = Bjdxj is
d(A B) = d(Aidxi ∧Bjdxj) = ∂k(AiBj) dxk ∧dxi ∧dxj
= (∂kAi)Bj dxk ∧dxi ∧dxj + Ai(∂kBj) dxk ∧dxi ∧dxj
= (∂kAi)Bj dxk ∧dxi ∧dxj −Ai(∂kBj) dxi ∧dxk ∧dxj
= (∂kAi)dxk ∧dxi ∧Bjdxj −Aidxi ∧(∂kBj) dxk ∧dxj
= dA ∧B −A ∧dB.
(11.121)
If A is a p-form, then d(A ∧B) = dA ∧B + (−1)pA ∧dB (exercise 11.10).
11.14 Tensor equations
Maxwell’s equations (11.93 & 11.94) relate the derivatives of the ﬁeld-strength
tensor to the current density
∂Fik
∂xk = μ0 ji
(11.122)
and the derivatives of the ﬁeld-strength tensor to each other
0 = ∂iFjk + ∂kFij + ∂jFki.
(11.123)
They are generally covariant tensor equations (sections 11.31 & 11.32). We also
can write Maxwell’s equations in terms of invariant forms; his homogeneous
equations are simply the Bianchi identity (11.114)
dF = ddA = 0
(11.124)
and we’ll write his inhomogeneous ones in terms of forms in section 11.26.
If we can write a physical law in one coordinate system as a tensor equation
Kkl = 0
(11.125)
419

TENSORS AND LOCAL SYMMETRIES
then in any other coordinate system, the corresponding tensor equation
K′ij = 0
(11.126)
also is valid since
K′ij = ∂x′i
∂xk
∂x′j
∂xl Kkl = 0.
(11.127)
Similarly, physical laws remain the same when expressed in terms of invariant
forms. Thus by writing a theory in terms of tensors or forms, one gets a theory
that is true in all coordinate systems if it is true in any. Only such “covariant”
theories have a chance at being right in our coordinate system, which is not
special. One way to make a covariant theory is to start with an action that is
invariant under all coordinate transformations.
11.15 The quotient theorem
Suppose that the product B A of a quantity B (with unknown transformation
properties) with an arbitrary tensor A (of a given rank and kind) is a tensor.
Then B is itself a tensor. The simplest example is when BiAi is a scalar for all
contravariant vectors Ai
B′
iA′i = BjAj.
(11.128)
Then since Ai is a contravariant vector
B′
iA′i = B′
i
∂x′i
∂xj Aj = BjAj
(11.129)
or

B′
i
∂x′i
∂xj −Bj

Aj = 0.
(11.130)
Since this equation holds for all vectors A, we may promote it to the level of a
vector equation
B′
i
∂x′i
∂xj −Bj = 0.
(11.131)
Multiplying both sides by ∂xj/∂x′k and summing over j
B′
i
∂x′i
∂xj
∂xj
∂x′k = Bj
∂xj
∂x′k
(11.132)
we see that the unknown quantity Bi does transform as a covariant vector
B′
k = ∂xj
∂x′k Bj.
(11.133)
The quotient rule works for unknowns B and tensors A of arbitrary rank and
kind. The proof in each case is very similar to the one given here.
420

11.16 THE METRIC TENSOR
11.16 The metric tensor
So far we have been considering coordinate systems with constant basis vectors
ei that do not vary with the physical point p. Now we shall assume only that we
can write the change in the point p(x) due to an inﬁnitesimal change dxi(p) in
its coordinates xi(p) as
dp(x) = ei(x) dxi.
(11.134)
In a different system of coordinates x′, this displacement is dp = e′
i(x′) dx′i. The
basis vectors ei and e′
i are partial derivatives of the point p
ei(x) = ∂p
∂xi
and
e′
i(x′) = ∂p
∂x′i .
(11.135)
They are linearly related to each other, transforming as covariant vectors
e′
i(x′) = ∂p
∂x′i = ∂xj
∂x′i
∂p
∂xj = ∂xj
∂x′i ej(x).
(11.136)
They also are vectors in the n-dimensional embedding space with inner product
ei(x) · ej(x) =
n

a=1
n

b=1
ea
i (x) ηab eb
j (x),
(11.137)
which will be positive-deﬁnite (1.75) if all the eigenvalues of the real symmetric
matrix η are positive. For instance, the eigenvalues are positive in euclidean
3-space with cylindrical or spherical coordinates but not in Minkowski 4-space
where η is a diagonal matrix with main diagonal (−1, 1, 1, 1).
The basis vectors ei(x) constitute a moving frame, a concept introduced by
Élie Cartan. In general, they are not normalized or orthogonal. Their inner
products deﬁne the metric of the manifold or of space-time
gij(x) = ei(x) · ej(x).
(11.138)
An inner product by deﬁnition (1.73) satisﬁes (f , g) = (g, f )∗and so a real inner
product is symmetric. For real coordinates on a real manifold the basis vectors
are real, so the metric tensor is real and symmetric
gij = gji.
(11.139)
The basis vectors e′
j(x′) of a different coordinate system deﬁne the metric
in that coordinate system g′
ij(x′) = e′
i(x′) · e′
j(x′). Since the basis vectors ei are
covariant vectors, the metric gij is a second-rank covariant tensor
g′
ij(x′) = e′
i(x′) · e′
j(x′) = ∂xk
∂x′i ek(x) · ∂xℓ
∂x′j eℓ(x) = ∂xk
∂x′i
∂xℓ
∂x′j gkℓ(x).
(11.140)
421

TENSORS AND LOCAL SYMMETRIES
Example 11.15 (The sphere)
Let the point p be a euclidean 3-vector represent-
ing a point on the two-dimensional surface of a sphere of radius r. The spherical
coordinates (θ, φ) label the point p, and the basis vectors are
eθ = ∂p
∂θ = r ˆθ
and
eφ = ∂p
∂φ = r sin θ ˆφ.
(11.141)
Their inner products are the components (11.138) of the sphere’s metric tensor,
which is the matrix
gθθ
gθφ
gφθ
gφφ

=
eθ · eθ
eθ · eφ
eφ · eθ
eφ · eφ

=
r2
0
0
r2 sin2 θ

(11.142)
with determinant r4 sin2 φ.
11.17 A basic axiom
Points are physical, coordinate systems metaphysical. So p, q, p −q, and
(p −q) · (p −q) are all invariant quantities. When p and q = p + dp both lie
on the (space-time) manifold and are inﬁnitesimally close to each other, the
vector dp = ei dxi is the sum of the basis vectors multiplied by the changes in
the coordinates xi. Both dp and the inner product dp·dp are physical and so are
independent of the coordinates. The (squared) distance dp2 is the same in one
coordinate system
dp2 ≡dp · dp = (ei dxi) · (ej dxj) = gij dxidxj
(11.143)
as in another
dp2 ≡dp · dp = (e′
i dx′i) · (e′
j dx′j) = g′
ij dx′idx′j.
(11.144)
This invariance and the quotient rule provide a second reason why gij is a
second-rank covariant tensor.
We want dp to be inﬁnitesimal so that it is tangent to the manifold.
11.18 The contravariant metric tensor
The inverse gik of the covariant metric tensor gkj satisﬁes
g′ikg′
kj = δi
j = gikgkj
(11.145)
in all coordinate systems. To see how it transforms, we use the transformation
law (11.140) of gkj
δi
j = g′ikg′
kj = g′ik ∂xt
∂x′k gtu
∂xu
∂x′j .
(11.146)
422

11.20 ORTHOGONAL COORDINATES IN EUCLIDEAN N-SPACE
Thus in matrix notation, we have as I = g′−1 H g H, which implies g′−1 =
H−1 g−1 H−1 or in tensor notation
g′iℓ= ∂x′i
∂xv
∂x′ℓ
∂xw gvw.
(11.147)
Thus the inverse gik of the covariant metric tensor is a second-rank contravari-
ant tensor called the contravariant metric tensor.
11.19 Raising and lowering indices
The contraction of a contravariant vector Ai with any rank-2 covariant tensor
gives a covariant vector. We reserve the symbol Ai for the covariant vector that
is the contraction of Aj with the metric tensor
Ai = gijAj.
(11.148)
This operation is called lowering the index on Aj.
Similarly the contraction of a covariant vector Bj with any rank-2 con-
travariant tensor is a contravariant vector. But we reserve the symbol Bi for
contravariant vector that is the contraction
Bi = gijBj
(11.149)
of Bj with the inverse of the metric tensor. This is called raising the index on Bj.
The vectors ei, for instance, are given by
ei = gijej.
(11.150)
They are therefore orthonormal or dual to the basis vectors ei
ei · ej = ei · gjkek = gjkei · ek = gjkgik = gjkgki = δj
i.
(11.151)
11.20 Orthogonal coordinates in euclidean n-space
In ﬂat n-dimensional euclidean space, it is convenient to use orthogonal basis
vectors and orthogonal coordinates. A change dxi in the coordinates moves the
point p by (11.134)
dp = ei dxi.
(11.152)
The metric gij is the inner product (11.138)
gij = ei · ej.
(11.153)
Since the vectors ei are orthogonal, the metric is diagonal
gij = ei · ej = h2
i δij.
(11.154)
423

TENSORS AND LOCAL SYMMETRIES
The inverse metric
gij = h−2
i
δij
(11.155)
raises indices. For instance, the dual vectors
e i = gij ej = h−2
i
ei
satisfy
ei · ek = δi
k.
(11.156)
The invariant squared distance dp2 between nearby points (11.143) is
dp2 = dp · dp = gij dxi dxj = h2
i (dxi)2
(11.157)
and the invariant volume element is
dV = dnp = h1 . . . hn dx1 ∧. . . ∧dxn = g dx1 ∧. . . ∧dxn = g dnx,
(11.158)
in which g =

det gij is the square-root of the positive determinant of gij.
The important special case in which all the scale factors hi are unity is
cartesian coordinates in euclidean space (section 11.5).
We also can use basis vectors ˆei that are orthonormal. By (11.154 & 11.156),
these vectors
ˆei = ei/hi = hi e i
satisfy
ˆei · ˆej = δij.
(11.159)
In terms of them, a physical and invariant vector V takes the form
V = ei Vi = hi ˆei Vi = e i Vi = h−1
i
ˆei Vi = ˆei Vi
(11.160)
where
Vi ≡hi Vi = h−1
i
Vi
(no sum).
(11.161)
The dot-product is then
V · U = gij Vi Vj = Vi Ui.
(11.162)
In euclidean n-space, we even can choose coordinates xi so that the vectors
ei deﬁned by dp = ei dxi are orthonormal. The metric tensor is then the n × n
identity matrix gik = ei · ek = Iik = δik. But since this is euclidean n-space, we
also can expand the n ﬁxed orthonormal cartesian unit vectors ˆℓin terms of the
ei(x) which vary with the coordinates as ˆℓ= ei(x)(ei(x) · ˆℓ).
11.21 Polar coordinates
In polar coordinates in ﬂat 2-space, the change dp in a point p due to changes
in its coordinates is dp = ˆr dr + ˆθ r dθ so dp = er dr + eθ dθ with er = ˆer = ˆr and
eθ = r ˆeθ = r ˆθ. The metric tensor for polar coordinates is
(gij) = (ei · ej) =
1
0
0
r2

.
(11.163)
424

11.23 SPHERICAL COORDINATES
The contravariant basis vectors are e r = ˆr and e θ = ˆeθ/r. A physical vector V
is V = Vi ei = Vi e i = Vr ˆr + Vθ ˆθ.
11.22 Cylindrical coordinates
For cylindrical coordinates in ﬂat 3-space, the change dp in a point p due to
changes in its coordinates is
dp = ˆρ dρ + ˆφ ρ dφ + ˆz dz = eρ dρ + eφ dφ + ez dz
(11.164)
with eρ = ˆeρ = ˆρ, eφ = ρ ˆeφ = ρ ˆφ, and ez = ˆez = ˆz. The metric tensor for
cylindrical coordinates is
(gij) = (ei · ej) =
⎛
⎝
1
0
0
0
ρ2
0
0
0
1
⎞
⎠
(11.165)
with determinant det gij ≡g = ρ2. The invariant volume element is
dV = ρ dx1 ∧dx2 ∧dx3 = √g dρdφdz = ρ dρdφdz.
(11.166)
The contravariant basis vectors are e ρ = ˆρ, e φ = ˆeφ/ρ, and e z = ˆz. A
physical vector V is
V = Vi ei = Vi e i = Vρ ˆρ + Vφ ˆφ + Vz ˆz.
(11.167)
Incidentally, since
p = (ρ cos φ, ρ sin φ, z)
(11.168)
the formulas for the basis vectors of cylindrical coordinates in terms of those of
rectangular coordinates are (exercise 11.13)
ˆρ = cos φ ˆx + sin φ ˆy,
ˆφ = −sin φ ˆx + cos φ ˆy,
ˆz = ˆz.
(11.169)
11.23 Spherical coordinates
For spherical coordinates in ﬂat 3-space, the change dp in a point p due to
changes in its coordinates is
dp = ˆr dr + ˆθ r dθ + ˆφ r sin θ dφ = er dr + eθ dθ + eφ dφ
(11.170)
so er = ˆr, eθ = r ˆθ, and eφ = r sin θ ˆφ. The metric tensor for spherical
coordinates is
(gij) = (ei · ej) =
⎛
⎝
1
0
0
0
r2
0
0
0
r2 sin2 θ
⎞
⎠
(11.171)
425

TENSORS AND LOCAL SYMMETRIES
with determinant det gij ≡g = r4 sin2 θ. The invariant volume element is
dV = r2 sin2 θ dx1 ∧dx2 ∧dx3 = √g drdθdφ = r2 sin θ drdθdφ.
(11.172)
The orthonormal basis vectors are ˆer = ˆr, ˆeθ = ˆθ, and ˆeφ = ˆφ. The con-
travariant basis vectors are e r = ˆr, e θ = ˆθ/r, e φ = ˆφ/r sin θ. A physical vector
V is
V = Vi ei = Vi e i = Vr ˆr + Vθ ˆθ + Vφ ˆφ.
(11.173)
Incidentally, since
p = (r sin θ cos φ, r sin θ sin φ, r cos θ)
(11.174)
the formulas for the basis vectors of spherical coordinates in terms of those of
rectangular coordinates are (exercise 11.14)
ˆr = sin θ cos φ ˆx + sin θ sin φ ˆy + cos θ ˆz,
ˆθ = cos θ cos φ ˆx + cos θ sin φ ˆy −sin θ ˆz,
ˆφ = −sin φ ˆx + cos φ ˆy.
(11.175)
11.24 The gradient of a scalar ﬁeld
If f (x) is a scalar ﬁeld, then the difference between it and f (x + dx) deﬁnes the
gradient ∇f as (6.26)
df (x) = f (x + dx) −f (x) = ∂f (x)
∂xi dxi = ∇f (x) · dp.
(11.176)
Since dp = ej dxj, the invariant form
∇f = ei ∂f
∂xi = ˆei
hi
∂f
∂xi
(11.177)
satisﬁes this deﬁnition (11.176) of the gradient
∇f · dp = ∂f
∂xi ei · ejdxj = ∂f
∂xi δi
j dxj = ∂f
∂xi dxi = df .
(11.178)
In two polar coordinates, the gradient is
∇f = ei ∂f
∂xi = ˆei
hi
∂f
∂xi = ˆr ∂f
∂r +
ˆθ
r
∂f
∂θ .
(11.179)
In three cylindrical coordinates, it is (6.27)
∇f = ei ∂f
∂xi = ˆei
hi
∂f
∂xi = ∂f
∂ρ ˆρ + 1
ρ
∂f
∂φ
ˆφ + ∂f
∂z ˆz
(11.180)
426

11.25 LEVI-CIVITA’S TENSOR
and in three spherical coordinates it is (6.28)
∇f = ∂f
∂xi ei = ˆei
hi
∂f
∂xi = ∂f
∂r ˆr + 1
r
∂f
∂θ
ˆθ +
1
r sin θ
∂f
∂φ
ˆφ.
(11.181)
11.25 Levi-Civita’s tensor
In three dimensions, Levi-Civita’s symbol ϵijk ≡ϵijk is totally antisymmetric
with ϵ123 = 1 in all coordinate systems.
We can turn his symbol into something that transforms as a tensor by multi-
plying it by the square-root of the determinant of a rank-2 covariant tensor. A
natural choice is the metric tensor. Thus the Levi-Civita tensor ηijk is the totally
antisymmetric rank-3 covariant (pseudo-)tensor
ηijk = √g ϵijk
(11.182)
in which g = | det gmn| is the absolute value of the determinant of the metric
tensor gmn. The determinant’s deﬁnition (1.184) and product rule (1.207) imply
that Levi-Civita’s tensor ηijk transforms as
η′
ijk =

g′ ϵ′
ijk =

g′ ϵijk =
(det
 ∂xt
∂x′m
∂xu
∂x′n gtu
 ϵijk
=
(det
 ∂xt
∂x′m

det
 ∂xu
∂x′n

det (gtu)
 ϵijk
=
det
 ∂x
∂x′

√g ϵijk = σ det
 ∂x
∂x′
 √g ϵijk
= σ ∂xt
∂x′i
∂xu
∂x′j
∂xv
∂x′k
√g ϵtuv = σ ∂xt
∂x′i
∂xu
∂x′j
∂xv
∂x′k ηtuv
(11.183)
in which σ is the sign of the Jacobian det(∂x/∂x′). Levi-Civita’s tensor is a
pseudo-tensor because it doesn’t change sign under the parity transformation
x′i = −xi.
We get η with upper indices by using the inverse gnm of the metric tensor
ηijk = git gju gkv ηtuv = git gju gkv √g ϵtuv
= √g ϵijk/ det(gmn) = sϵijk/√g = sϵijk/√g,
(11.184)
in which s is the sign of the determinant det gij = sg.
Similarly in four dimensions, Levi-Civita’s symbol ϵijkℓ≡ϵijkℓis totally anti-
symmetric with ϵ0123 = 1 in all coordinate systems. No meaning attaches to
whether the indices of the Levi-Civita symbol are up or down; some authors
even use the notation ϵ(ijkℓ) or ϵ[ijkℓ] to emphasize this fact.
427

TENSORS AND LOCAL SYMMETRIES
In four dimensions, the Levi-Civita pseudo-tensor is
ηijkℓ= √g ϵijkℓ.
(11.185)
It transforms as
η′
ijkℓ=

g′ ϵijkℓ=
det
 ∂x
∂x′

√g ϵijkℓ= σ det
 ∂x
∂x′
 √g ϵijkℓ
= σ ∂xt
∂x′i
∂xu
∂x′j
∂xv
∂x′k
∂xw
∂x′ℓ
√g ϵtuvw = σ ∂xt
∂x′i
∂xu
∂x′j
∂xv
∂x′k
∂xw
∂x′ℓηtuvw (11.186)
where σ is the sign of the Jacobian det(∂x/∂x′).
Raising the indices on η with det gij = sg we have
ηijkℓ= git gju gkv gℓw ηtuvw = git gju gkv gℓw √g ϵtuvw
= √g ϵijkℓ/ det(gmn) = s ϵijkℓ/√g ≡s ϵijkℓ/√g.
(11.187)
In n dimensions, one may deﬁne Levi-Civita’s symbol ϵ(i1 . . . in) as totally
antisymmetric with ϵ(1 . . . n) = 1 and his tensor as ηi1...in = √g ϵ(i1 . . . in).
11.26 The Hodge star
In three cartesian coordinates, the Hodge dual turns 1-forms into 2-forms
∗dx = dy ∧dz,
∗dy = dz ∧dx,
∗dz = dx ∧dy
(11.188)
and 2-forms into 1-forms
∗(dx ∧dy) = dz,
∗(dy ∧dz) = dx,
∗(dz ∧dx) = dy.
(11.189)
It also maps the 0-form 1 and the volume 3-form into each other
∗1 = dx ∧dy ∧dz,
∗(dx ∧dy ∧dz) = 1
(11.190)
(William Vallance Douglas Hodge, 1903–1975). More generally in 3-space, we
deﬁne the Hodge dual, also called the Hodge star, as
∗1 = 1
3!ηℓjkdxℓ∧dxj ∧dxk,
∗(dxℓ∧dxj ∧dxk) = gℓtgjugkvηtuv,
∗dxi = 1
2 giℓηℓjk dxj ∧dxk,
∗(dxi ∧dxj) = gik gjℓηkℓm dxm
(11.191)
and so if the sign of det gij is s = +1, then ∗∗1 = 1, ∗∗dxi = dxi, ∗∗(dxi∧dxk) =
dxi ∧dxk, and ∗∗(dxi ∧dxj ∧dxk) = dxi ∧dxj ∧dxk.
Example 11.16 (Divergence and Laplacian)
The dual of the 1-form
df = ∂f
∂x dx + ∂f
∂y dy + ∂f
∂z dz
(11.192)
428

11.26 THE HODGE STAR
is the 2-form
∗df = ∂f
∂x dy ∧dz + ∂f
∂y dz ∧dx + ∂f
∂z dx ∧dy
(11.193)
and its exterior derivative is the Laplacian
d ∗df =

∂2f
∂x2 + ∂2f
∂y2 + ∂2f
∂z2

dx ∧dy ∧dz
(11.194)
multiplied by the volume 3-form.
Similarly, the dual of the 1-form
A = Ax dx + Ay dy + Az dz
(11.195)
is the 2-form
∗A = Ax dy ∧dz + Ay dz ∧dx + Az dx ∧dy
(11.196)
and its exterior derivative is the divergence
d ∗A =
∂Ax
∂x + ∂Ay
∂y + ∂Az
∂z

dx ∧dy ∧dz
(11.197)
times dx ∧dy ∧dz.
In ﬂat Minkowski 4-space with c = 1, the Hodge dual turns 1-forms into
3-forms
∗dt = −dx ∧dy ∧dz,
∗dx = −dy ∧dz ∧dt,
∗dy = −dz ∧dx ∧dt,
∗dz = −dx ∧dy ∧dt,
(11.198)
2-forms into 2-forms
∗(dx ∧dt) = dy ∧dz,
∗(dx ∧dy) = −dz ∧dt,
∗(dy ∧dt) = dz ∧dx,
∗(dy ∧dz) = −dx ∧dt,
∗(dz ∧dt) = dx ∧dy,
∗(dz ∧dx) = −dy ∧dt,
(11.199)
3-forms into 1-forms
∗(dx ∧dy ∧dz) = −dt,
∗(dy ∧dz ∧dt) = −dx,
∗(dz ∧dx ∧dt) = −dy,
∗(dx ∧dy ∧dt) = −dz,
(11.200)
and interchanges 0-forms and 4-forms
∗1 = dt ∧dx ∧dy ∧dz,
∗(dt ∧dx ∧dy ∧dz) = −1.
(11.201)
429

TENSORS AND LOCAL SYMMETRIES
More generally in four dimensions, we deﬁne the Hodge star as
∗1 = 1
4! ηkℓmn dxk ∧dxℓ∧dxm ∧dxn,
∗dxi = 1
3! gik ηkℓmn dxℓ∧dxm ∧dxn,
∗(dxi ∧dxj) = 1
2 gik gjℓηkℓmn dxm ∧dxn,
∗(dxi ∧dxj ∧dxk) = git gju gkv ηtuvw dxw,
∗

dxi ∧dxj ∧dxk ∧dxℓ
= git gju gkv gℓwηtuvw = ηijkℓ.
(11.202)
Thus (exercise 11.16) if the determinant det gij of the metric is negative, then
∗∗dxi = dxi,
∗∗(dxi ∧dxj) = −dxi ∧dxj,
∗∗(dxi ∧dxj ∧dxk) = dxi ∧dxj ∧dxk,
∗∗1 = −1. (11.203)
In n dimensions, the Hodge star turns p-forms into (n −p)-forms
∗

dxi1 ∧. . . ∧dxip
= gi1k1 . . . gipkp ηk1...kpℓ1...ℓn−p
(n −p)!
dxℓ1 ∧. . .∧dxℓn−p. (11.204)
Example 11.17 (The inhomogeneous Maxwell equations)
Since the homoge-
neous Maxwell equations are
dF = ddA = 0
(11.205)
we ﬁrst form the dual ∗F = ∗dA
∗F = 1
2Fij ∗

dxi ∧dxj
= 1
4Fijgikgjℓηkℓmndxm ∧dxn = 1
4Fkℓηkℓmndxm ∧dxn
and then apply the exterior derivative
d ∗F = 1
4d

Fkℓηkℓmndxm ∧dxn
= 1
4∂p

Fkℓηkℓmn

dxp ∧dxm ∧dxn.
To get back to a 1-form like j = jkdxk, we apply a second Hodge star
∗d ∗F = 1
4 ∂p

Fkℓηkℓmn

∗

dxp ∧dxm ∧dxn
= 1
4 ∂p

Fkℓηkℓmn

gpsgmtgnuηstuv dxv
= 1
4 ∂p
√g Fkℓ
ϵkℓmn gpsgmtgnu√g ϵstuv dxv
= 1
4 ∂p
√g Fkℓ
ϵkℓmn gpsgmtgnugwv ϵstuv
√g dx
= 1
4 ∂p
√g Fkℓ
ϵkℓmn ϵpmnw
√g
det gij
dxw,
(11.206)
430

11.27 DERIVATIVES AND AFFINE CONNECTIONS
in which we used the deﬁnition (1.184) of the determinant. Levi-Civita’s
4-symbol obeys the identity (exercise 11.17)
ϵkℓmn ϵpwmn = 2

δp
k δw
ℓ−δw
k δp
ℓ

.
(11.207)
Applying it to ∗d ∗F, we get
∗d ∗F =
s
2√g∂p
√g Fkℓ 
δp
k δw
ℓ−δw
k δp
ℓ

dxw = −
s
√g ∂p
√g Fkp
dxk.
In our space-time s = −1. Setting ∗d ∗F equal to j = jk dxk = jk dxk mul-
tiplied by the permeability μ0 of the vacuum, we arrive at expressions for the
microscopic inhomogeneous Maxwell equations in terms of both tensors and
forms
∂p
√g Fkp
= μ0
√g jk
and
∗d ∗F = μ0 j.
(11.208)
They and the homogeneous Bianchi identity (11.93, 11.114, & 11.247)
ϵijkℓ∂ℓFjk = dF = d dA = 0
(11.209)
are invariant under general coordinate transformations.
11.27 Derivatives and afﬁne connections
If F(x) is a vector ﬁeld, then its invariant description in terms of space-time
dependent basis vectors ei(x) is
F(x) = Fi(x) ei(x).
(11.210)
Since the basis vectors ei(x) vary with x, the derivative of F(x) contains two
terms
∂F
∂xℓ= ∂Fi
∂xℓei + Fi ∂ei
∂xℓ.
(11.211)
In general, the derivative of a vector ei is not a linear combination of the
basis vectors ek. For instance, on the two-dimensional surface of a sphere in
three dimensions, the derivative
∂eθ
∂θ = −ˆr
(11.212)
points to the sphere’s center and isn’t a linear combination of eθ and eφ.
The inner product of a derivative ∂ei/∂xℓwith a dual basis vector ek is the
Levi-Civita afﬁne connection
k
ℓi = ek · ∂ei
∂xℓ,
(11.213)
431

TENSORS AND LOCAL SYMMETRIES
which relates spaces that are tangent to the manifold at inﬁnitesimally separated
points. It is called an afﬁne connection because the different tangent spaces lack
a common origin.
In terms of the afﬁne connection (11.213 ), the inner product of the derivative
of (11.211) with ek is
ek · ∂F
∂xℓ= ek · ∂Fi
∂xℓei + Fi ek · ∂ei
∂xℓ= ∂Fk
∂xℓ+ k
ℓi Fi
(11.214)
a combination that is called a covariant derivative (section 11.30)
DℓFk ≡∇ℓFk ≡∂Fk
∂xℓ+ k
ℓi Fi.
(11.215)
Some physicists write the afﬁne connection k
iℓas
 k
iℓ
%
= k
iℓ
(11.216)
and call it a Christoffel symbol of the second kind.
The vectors ei are the space-time derivatives (11.135) of the point p, and so
the afﬁne connection (11.213) is a double derivative of p
k
ℓi = ek · ∂ei
∂xℓ= ek ·
∂2p
∂xℓ∂xi = ek ·
∂2p
∂xi∂xℓ= ek · ∂eℓ
∂xi = k
iℓ
(11.217)
and thus is symmetric in its two lower indices
k
iℓ= k
ℓi.
(11.218)
Afﬁne connections are not tensors. Tensors transform homogeneously; con-
nections transform inhomogeneously. The connection k
iℓtransforms as
′k
iℓ= e′k · ∂e′
ℓ
∂x′i = ∂x′k
∂xp ep · ∂xm
∂x′i
∂
∂xm
 ∂xn
∂x′ℓen

= ∂x′k
∂xp
∂xm
∂x′i
∂xn
∂x′ℓep · ∂en
∂xm + ∂x′k
∂xp
∂2xp
∂x′i∂x′ℓ
= ∂x′k
∂xp
∂xm
∂x′i
∂xn
∂x′ℓp
mn + ∂x′k
∂xp
∂2xp
∂x′i∂x′ℓ.
(11.219)
The electromagnetic ﬁeld Ai(x) and other gauge ﬁelds are connections.
Since the Levi-Civita connection k
iℓis symmetric in i and ℓ, in four-
dimensional space-time, there are ten of them for k, or 40 in all. The ten
correspond to three rotations, three boosts, and four translations.
Einstein–Cartan theories do not assume that the space-time manifold is
embedded in a ﬂat space of higher dimension. So their basis vectors need not
432

11.29 NOTATIONS FOR DERIVATIVES
be partial derivatives of a point in the embedding space, and their afﬁne con-
nections a
bc need not be symmetric in their lower indices. The antisymmetric
part is the torsion tensor
Ta
bc = a
bc −a
cb.
(11.220)
11.28 Parallel transport
The movement of a vector along a curve on a manifold so that its direction
in successive tangent spaces does not change is called parallel transport. If the
vector is F = Fiei, then we want ek·dF to vanish along the curve. But this is just
the condition that the covariant derivative of F should vanish along the curve
ek · ∂F
∂xℓ= ek · ∂Fi
∂xℓei + Fi ek · ∂ei
∂xℓ= ∂Fk
∂xℓ+ k
ℓi Fi = DℓFk = 0.
(11.221)
Example 11.18 (Parallel transport on a sphere)
The tangent space on a 2-sphere
is spanned by the unit basis vectors
ˆθ = (cos θ cos φ, cos θ sin φ, −sin θ) ,
ˆφ = (−sin φ, cos φ, 0) .
(11.222)
We can parallel-transport the vector ˆφ down from the north pole along the
meridian φ = 0 to the equator; all along this path ˆφ = (0, 1, 0). Then we can
parallel-transport it along the equator to φ = π/2 where it is (−1, 0, 0). Then we
can parallel-transport it along the meridian φ = π/2 up to the north pole where
it is (−1, 0, 0) as it was on the equator. The change from (0, 1, 0) to (−1, 0, 0) is
due to the curvature of the sphere.
11.29 Notations for derivatives
We have various notations for derivatives. We can use the variables x, y, and so
forth as subscripts to label derivatives
fx = ∂xf = ∂f
∂x
and
fy = ∂yf = ∂f
∂y.
(11.223)
If we use indices to label variables, then we can use commas
f,i = ∂if = ∂f
∂xi
and
f,ik = ∂k∂if =
∂2f
∂xk∂xi
(11.224)
and f,k′ = ∂f /∂x′k. For instance, we may write part of (11.217) as ei,ℓ= eℓ,i.
433

TENSORS AND LOCAL SYMMETRIES
11.30 Covariant derivatives
In comma notation, the derivative of a contravariant vector ﬁeld F = Fi ei is
F,ℓ= Fi
,ℓei + Fi ei,ℓ,
(11.225)
which in general lies outside the space spanned by the basis vectors ei. So we
use the afﬁne connections (11.213) to form the inner product
ek · F,ℓ= ek ·

Fi
,ℓei + Fiei,ℓ

= Fi
,ℓδk
i + Fi k
ℓi = Fk
,ℓ+ k
ℓi Fi.
(11.226)
This covariant derivative of a contravariant vector ﬁeld often is written with a
semicolon
Fk
;ℓ= ek · F,ℓ= Fk
,ℓ+ k
ℓi Fi.
(11.227)
It transforms as a mixed second-rank tensor. The invariant change dF projected
onto ek is
ek · dF = ek · F,ℓdxℓ= Fk
;ℓdxℓ.
(11.228)
In terms of its covariant components, the derivative of a vector V is
V,ℓ= (Vk ek),ℓ= Vk,ℓek + Vk ek
,ℓ.
(11.229)
To relate the derivatives of the vectors ei to the afﬁne connections k
iℓ, we
differentiate the orthonormality relation
δk
i = ek · ei,
(11.230)
which gives us
0 = ek
,ℓ· ei + ek · ei,ℓ
or
ek
,ℓ· ei = −ek · ei,ℓ= −k
iℓ.
(11.231)
Since ei · ek
,ℓ= −k
iℓ, the inner product of ei with the derivative of V is
ei · V,ℓ= ei ·

Vk,ℓek + Vk ek
,ℓ

= Vi,ℓ−Vkk
iℓ.
(11.232)
This covariant derivative of a covariant vector ﬁeld also is often written with a
semicolon
Vi;ℓ= ei · V,ℓ= Vi,ℓ−Vkk
iℓ.
(11.233)
It transforms as a rank-2 covariant tensor. Note the minus sign in Vi;ℓand the
plus sign in Fk
;ℓ. The change ei · dV is
ei · dV = ei · V,ℓdxℓ= Vi;ℓdxℓ.
(11.234)
Since dV is invariant, ei covariant, and dxℓcontravariant, the quotient rule
(section 11.15) conﬁrms that the covariant derivative Vi;ℓof a covariant vector
Vi is a rank-2 covariant tensor.
434

11.31 THE COVARIANT CURL
11.31 The covariant curl
Because the connection k
iℓis symmetric (11.218) in its lower indices, the
covariant curl of a covariant vector Vi is simply its ordinary curl
Vℓ;i −Vi;ℓ= Vℓ,i −Vk k
ℓi −Vi,ℓ+ Vk k
iℓ= Vℓ,i −Vi,ℓ.
(11.235)
Thus the Faraday ﬁeld-strength tensor Fiℓwhich is deﬁned as the curl of the
covariant vector ﬁeld Ai
Fiℓ= Aℓ,i −Ai,ℓ
(11.236)
is a generally covariant second-rank tensor.
In orthogonal coordinates, the curl is deﬁned (6.39, 11.111) in terms of the
totally antisymmetric Levi-Civita symbol ϵijk (with ϵ123 = ϵ123 = 1), as
∇× V =
3

i=1
(∇× V)i ˆei =
1
h1h2h3
3

ijk=1
ei ϵijk Vk;j,
(11.237)
which, in view of (11.235) and the antisymmetry of ϵijk, is
∇× V =
3

i=1
(∇× V)i ˆei =
3

ijk=1
1
hihjhk
ei ϵijk Vk,j
(11.238)
or by (11.159 & 11.161)
∇× V =
3

ijk=1
1
hihjhk
hiˆei ϵijk Vk,j =
3

ijk=1
1
hihjhk
hiˆei ϵijk (hkVk),j .
(11.239)
Often one writes this as a determinant
∇× V =
1
h1h2h3

e1
e2
e3
∂1
∂2
∂3
V1
V2
V3

=
1
h1h2h3

h1ˆe1
h2ˆe2
h3ˆe3
∂1
∂2
∂3
h1V1
h2V2
h3V3

.
(11.240)
In cylindrical coordinates, the curl is
∇× V = 1
ρ

ˆρ
ρ ˆφ
ˆz
∂ρ
∂φ
∂z
Vρ
ρVφ
Vz

.
(11.241)
In spherical coordinates, it is
∇× V =
1
r2 sin θ

ˆr
r ˆθ
r sin θ ˆφ
∂r
∂θ
∂φ
Vr
rVθ
r sin θ Vφ

.
(11.242)
435

TENSORS AND LOCAL SYMMETRIES
In more formal language, the curl is
dV = d

Vkdxk
= Vk,i dxi ∧dxk = 1
2

Vk,i −Vi,k

dxi ∧dxk.
(11.243)
11.32 Covariant derivatives and antisymmetry
By applying our rule (11.233) for the covariant derivative of a covariant vector
to a second-rank tensor Aiℓ, we get
Aiℓ;k = Aiℓ,k −m
ikAmℓ−m
ℓkAim.
(11.244)
Suppose now that our tensor is antisymmetric
Aiℓ= −Aℓi.
(11.245)
Then by adding together the three cyclic permutations of the indices iℓk we ﬁnd
that the antisymmetry of the tensor and the symmetry (11.218) of the afﬁne
connection conspire to cancel the nonlinear terms
Aiℓ;k + Aki;ℓ+ Aℓk;i = Aiℓ,k −m
ikAmℓ−m
ℓkAim
+Aki,ℓ−m
kℓAmi −m
iℓAkm
+Aℓk,i −m
ℓiAmk −m
kiAℓm
= Aiℓ,k + Aki,ℓ+ Aℓk,i,
(11.246)
an identity named after Luigi Bianchi (1856–1928).
The Maxwell ﬁeld-strength tensor Fiℓis antisymmetric by construction
(Fiℓ= Aℓ,i −Ai,ℓ), and so the homogeneous Maxwell equations
ϵijkℓFjk,ℓ= Fjk,ℓ+ Fkℓ,j + Fℓj,k = 0
(11.247)
are tensor equations valid in all coordinate systems. This is another example of
how amazingly right Maxwell was in the middle of the nineteenth century.
11.33 Afﬁne connection and metric tensor
To relate the afﬁne connection m
ℓi to the derivatives of the metric tensor gkℓ, we
lower the contravariant index m to get
kℓi = gkm m
ℓi = gkm m
iℓ= kiℓ,
(11.248)
which is symmetric in its last two indices and which some call a Christoffel
symbol of the ﬁrst kind, written [ℓi, k]. One can raise the index k back up by
using the inverse of the metric tensor
gmk kℓi = gmk gkn n
ℓi = δm
n n
ℓi = m
ℓi.
(11.249)
436

11.34 COVARIANT DERIVATIVE OF THE METRIC TENSOR
Although we can raise and lower these indices, the connections m
ℓi and kℓi are
not tensors.
The deﬁnition (11.213) of the afﬁne connection tells us that
kℓi = gkm m
ℓi = gkm em · eℓ,i = ek · eℓ,i = kiℓ= ek · ei,ℓ.
(11.250)
By differentiating the deﬁnition giℓ= ei · eℓof the metric tensor, we ﬁnd
giℓ,k = ei,k · eℓ+ ei · eℓ,k = eℓ· ei,k + ei · eℓ,k = ℓik + iℓk.
(11.251)
Permuting the indices cyclicly, we have
gki,ℓ= ikℓ+ kiℓ,
gℓk,i = kℓi + ℓki.
(11.252)
If we now subtract relation (11.251) from the sum of the two formulas (11.252)
keeping in mind the symmetry abc = acb, then we ﬁnd that four of the six
terms cancel
gki,ℓ+ gℓk,i −giℓ,k = ikℓ+ kiℓ+ kℓi + ℓki −ℓik −iℓk = 2kℓi (11.253)
leaving a formula for kℓi
kℓi = 1
2

gki,ℓ+ gℓk,i −giℓ,k

.
(11.254)
Thus the connection is three derivatives of the metric tensor
s
iℓ= gskkℓi = 1
2gsk 
gki,ℓ+ gℓk,i −giℓ,k

.
(11.255)
11.34 Covariant derivative of the metric tensor
Covariant derivatives of second-rank and higher-rank tensors are formed by
iterating our formulas for the covariant derivatives of vectors. For instance, the
covariant derivative of the metric tensor is
giℓ;k ≡giℓ,k −m
ik gmℓ−n
kℓgin.
(11.256)
One way to derive this formula is to proceed as in section 11.30 by differentiat-
ing the invariant metric tensor giℓei ⊗eℓin which the vector product ei ⊗eℓis
a kind of direct product
g,k = (giℓei ⊗eℓ),k = giℓ,k ei ⊗eℓ+ giℓei
,k ⊗eℓ+ giℓei ⊗eℓ
,k.
(11.257)
We now take the inner product of this derivative with em ⊗en
(em ⊗en, g,k) = giℓ,k em · ei en · eℓ+ giℓem · ei
,k en · eℓ+ giℓem · ei en · eℓ
,k (11.258)
and use the rules em · ei = δi
m and em · ei
,k = −i
mk (11.231) to write
(em ⊗en, g,k) = gmn;k = gmn,k −giℓi
mkδℓ
n −giℓδi
mℓ
nk
(11.259)
437

TENSORS AND LOCAL SYMMETRIES
or
gmn;k = gmn,k −i
mkgin −ℓ
nkgmℓ,
(11.260)
which is (11.256) inasmuch as both giℓand k
iℓare symmetric in their two lower
indices.
If we now substitute our formula (11.255) for the connections l
ik and n
kℓ
giℓ;k = giℓ,k −1
2gms 
gis,k + gsk,i −gik,s

gmℓ−1
2gns 
gℓs,k + gsk,ℓ−gℓk,s

gin
(11.261)
and use the fact (11.145) that the metric tensors giℓand gℓk are mutually inverse,
then we ﬁnd
giℓ;k = giℓ,k −1
2δs
ℓ

gis,k + gsk,i −gik,s

−1
2δs
i

gℓs,k + gsk,ℓ−gℓk,s

= giℓ,k −1
2

giℓ,k + gℓk,i −gik,ℓ

−1
2

gℓi,k + gik,ℓ−gℓk,i

= 0.
(11.262)
The covariant derivative of the metric tensor vanishes. This result follows from
our choice of the Levi-Civita connection (11.213); it is not true for some other
connections.
11.35 Divergence of a contravariant vector
The contraction of the covariant derivative of a contravariant vector is a scalar
known as the divergence,
∇· V = Vi
;i = Vi
,i + i
ikVk.
(11.263)
Because two indices in the connection
i
ik = 1
2gim 
gim,k + gkm,i −gik,m

(11.264)
are contracted, its last two terms cancel because they differ only by the
interchange of the dummy indices i and m
gimgkm,i = gmigkm,i = gimgki,m = gimgik,m.
(11.265)
So the contracted connection collapses to
i
ik = 1
2gimgim,k.
(11.266)
There is a nice formula for this last expression involving the absolute value
of the determinant det g ≡det gmn of the metric tensor considered as a matrix
g ≡gmn. To derive it, we recall that like any determinant, the determinant det(g)
of the metric tensor is given by the cofactor sum (1.195)
det(g) =

ℓ
giℓCiℓ
(11.267)
438

11.35 DIVERGENCE OF A CONTRAVARIANT VECTOR
along any row or column, that is, over ℓfor ﬁxed i or over i for ﬁxed ℓ, where Ciℓ
is the cofactor deﬁned as (−1)i+ℓtimes the determinant of the reduced matrix
consisting of the matrix giℓwith row i and column ℓomitted. Thus the partial
derivative of det g with respect to the iℓth element giℓis
∂det(g)
∂giℓ
= Ciℓ,
(11.268)
in which we consider giℓand gℓi to be independent variables for the purposes of
this differentiation. The inverse giℓof the metric tensor g, like the inverse (1.197)
of any matrix, is the transpose of the cofactor matrix divided by its determinant
det g,
giℓ=
Cℓi
det(g) =
1
det(g)
∂det(g)
∂gℓi
.
(11.269)
The chain rule gives us the derivative of the determinate det(g) as
det(g),k = giℓ,k
∂det(g)
∂giℓ
= giℓ,k det(g) gℓi
(11.270)
and so, since giℓ= gℓi, the contracted connection (11.266) is
i
ik = 1
2gimgim,k =
det(g),k
2 det(g) =
| det(g)|,k
2| det(g)| = g,k
2g = (√g),k
√g ,
(11.271)
in which g ≡
det(g)
 is the absolute value of the determinant of the metric
tensor.
Thus from (11.263), we arrive at our formula for the covariant divergence of
a contravariant vector:
∇· V = Vi
;i = Vi
,i + i
ikVk = Vk
,k + (√g),k
√g Vk = (√g Vk),k
√g
.
(11.272)
More formally, the Hodge dual (11.202) of the 1-form V = Vi dxi is
∗V = Vi ∗dxi = Vi
1
3! gik ηkℓmn dxℓ∧dxm ∧dxn
= 1
3!
√g Vk ϵkℓmn dxℓ∧dxm ∧dxn,
(11.273)
in which g is the absolute value of the determinant of the metric tensor gij. The
exterior derivative now gives
d ∗V = 1
3!
√g Vk
,p ϵkℓmn dxp ∧dxℓ∧dxm ∧dxn.
(11.274)
439

TENSORS AND LOCAL SYMMETRIES
So using (11.202) to apply a second Hodge star, we get (exercise 11.19)
∗d ∗V = 1
3!
√g Vk
,p ϵkℓmn ∗

dxp ∧dxℓ∧dxm ∧dxn
= 1
3!
√g Vk
,p ϵkℓmn gpt gℓu gmv gnwηtuvw
= 1
3!
√g Vk
,p ϵkℓmn gpt gℓu gmv gnwϵtuvw
√g
= 1
3!
√g Vk
,p ϵkℓmn
√g
det gij
ϵpℓmn
=
s
√g
√g Vk
,p δp
k =
s
√g
√g Vk
,k .
(11.275)
So in our space-time with det gij = −g
−∗d ∗V =
1
√g
√g Vk
,k .
(11.276)
In 3-space the Hodge star (11.191) of a 1-form V = Vi dxi is
∗V = Vi ∗dxi = Vi
1
2 giℓηℓjk dxj ∧dxk = 1
2
√g Vℓϵℓjk dxj ∧dxk.
(11.277)
Applying the exterior derivative, we get the invariant form
d ∗V = 1
2
√g Vℓ
,p ϵℓjk dxp ∧dxj ∧dxk.
(11.278)
We add a star by using the deﬁnition (11.191) of the Hodge dual in a 3-space in
which the determinant det gij is positive and the identity (exercise 11.18)
ϵℓjk ϵpjk = 2 δp
ℓ
(11.279)
as well as the deﬁnition (1.184) of the determinant
∗d ∗V = 1
2
√g Vℓ
,p ϵℓjk ∗

dxp ∧dxj ∧dxk
= 1
2
√g Vℓ
,p ϵℓjk gptgjugkvηtuv
= 1
2
√g Vℓ
,p ϵℓjk gptgjugkvϵtuv
√g
= 1
2
√g Vℓ
,p ϵℓjk ϵpjk
√g
det gij
=
1
√g
√g Vℓ
,p δp
ℓ=
1
√g
√g Vp
,p .
(11.280)
440

11.36 THE COVARIANT LAPLACIAN
Example 11.19 (Divergence in orthogonal coordinates)
In two orthogonal
coordinates, equations (11.154 & 11.161) imply that √g = h1h2 and Vk =
Vk/hk, and so the divergence of a vector V is
∇· V =
1
h1h2
2

k=1
h1h2
hk
Vk

,k
,
(11.281)
which in polar coordinates (section 11.21), with hr = 1 and hθ = r, is
∇· V = 1
r

r Vr

,r +

Vθ

,θ

= 1
r

r Vr

,r + Vθ,θ

.
(11.282)
In three orthogonal coordinates, equations (11.154 & 11.161) give √g =
h1h2h3 and Vk = Vk/hk, and so the divergence of a vector V is (6.29)
∇· V =
1
h1h2h3
3

k=1
h1h2h3
hk
Vk

,k
.
(11.283)
In cylindrical coordinates (section 11.22), hρ = 1, hφ = ρ, and hz = 1; so
∇· V = 1
ρ

ρ Vρ

,ρ +

Vφ

,φ +

ρ Vz

,z

= 1
ρ

ρ Vρ

,ρ + Vφ,φ + ρ Vz,z

.
(11.284)
In spherical coordinates (section 11.23), hr = 1, hθ = r, hφ = r sin θ, g =
| det g| = r4 sin2 θ and the inverse gij of the metric tensor is
(gij) =
⎛
⎝
1
0
0
0
r−2
0
0
0
r−2 sin−2 θ
⎞
⎠.
(11.285)
So our formula (11.281) gives us
∇· V =
1
r2 sin θ
#
r2 sin θ Vr

,r +

r sin θ Vθ

,θ +

r Vφ

,φ
$
=
1
r2 sin θ
#
sin θ

r2Vr

,r + r

sin θ Vθ

,θ + rVφ,φ
$
(11.286)
as the divergence ∇· V.
11.36 The covariant Laplacian
In ﬂat 3-space, we write the Laplacian as ∇· ∇= ∇2 or as △. In euclidean
coordinates, both mean ∂2
x + ∂2
y + ∂2
z . In ﬂat Minkowski space, one often turns
the triangle into a square and writes the 4-Laplacian as 2 = △−∂2
t .
441

TENSORS AND LOCAL SYMMETRIES
Since the gradient of a scalar ﬁeld f is a covariant vector, we may use the
inverse metric tensor gij to write the Laplacian 2f of a scalar f as the covariant
divergence of the contravariant vector gikf,k
2f = (gikf,k);i.
(11.287)
The divergence formula (11.272) now expresses the invariant Laplacian as
2f = (√g gikf,k),i
√g
= (√g f ,i),i
√g
.
(11.288)
To ﬁnd the Laplacian 2f in terms of forms, we apply the exterior derivative
to the Hodge dual (11.202) of the 1-form df = f,idxi
d ∗df = d

f,i ∗dxi
= d
 1
3! f,i gik ηkℓmn dxℓ∧dxm ∧dxn

= 1
3!

f ,k √g

,p ϵkℓmn dxp ∧dxℓ∧dxm ∧dxn
(11.289)
and then add a star using (11.202)
∗d ∗df = 1
3!

f ,k √g

,p ϵkℓmn ∗

dxp ∧dxℓ∧dxm ∧dxn
= 1
3!

f ,k √g

,p ϵkℓmn gptgℓugmvgnw√g ϵtuvw.
(11.290)
The deﬁnition (1.184) of the determinant now gives (exercise 11.19)
∗d ∗df = 1
3!

f ,k √g

,p ϵkℓmn ϵpℓmn
√g
det g
=

f ,k √g

,p δp
k
s
√g =
s
√g

f ,k √g

,k .
(11.291)
In our space-time det gij = sg = −g, and so the Laplacian is
2f = −∗d ∗df =
1
√g

f ,k √g

,k .
(11.292)
Example 11.20 (Invariant Laplacians)
In two orthogonal coordinates, equa-
tions (11.154 & 11.155) imply that √g =

| det(gij)| = h1h2 and that f ,i =
gik f,k = h−2
i
f,i, and so the Laplacian of a scalar f is
△f =
1
h1h2
 2

i=1
h1h2
h2
i
f,i

,i.
(11.293)
In polar coordinates, where h1 = 1, h2 = r, and g = r2, the Laplacian is
△f = 1
r
#
rf,r

,r +

r−1f,θ

,θ
$
= f,rr + r−1f,r + r−2f,θθ.
(11.294)
442

11.37 THE PRINCIPLE OF STATIONARY ACTION
In three orthogonal coordinates, equations (11.154 & 11.155) imply that √g =

| det(gij)| = h1h2h3 and that f ,i = gik f,k = h−2
i
f,i, and so the Laplacian of a
scalar f is (6.33)
△f =
1
h1h2h3
 3

i=1
h1h2h3
h2
i
f,i

,i.
(11.295)
In cylindrical coordinates (section 11.22), hρ = 1, hφ = ρ, hz = 1, g = ρ2, and
the Laplacian is
△f = 1
ρ
#
ρ f,ρ

,ρ + 1
ρ f,φφ + ρ f,zz
$
= f,ρρ + 1
ρ f,ρ + 1
ρ2 f,φφ + f,zz.
(11.296)
In spherical coordinates (section 11.23), hr = 1, hθ = r, hφ = r sin θ, and
g = | det g| = r4 sin2 θ. So (11.295) gives us the Laplacian of f as
△f =

r2 sin θf,r

,r +

sin θf,θ

,θ +

f,φ/ sin θ

,φ
r2 sin θ
=

r2f,r

,r
r2
+

sin θf,θ

,θ
r2 sin θ
+
f,φφ
r2 sin2 θ
.
(11.297)
If the function f is a function only of the radial variable r, then the Laplacian is
simply
△f (r) = 1
r2

r2f ′(r)
′
= 1
r [rf (r)]′′ = f ′′(r) + 2
r f ′(r),
(11.298)
in which the primes denote r-derivatives.
11.37 The principle of stationary action
It follows from a path-integral formulation of quantum mechanics that the clas-
sical motion of a particle is given by the principle of stationary action δS = 0. In
the simplest case of a free nonrelativistic particle, the lagrangian is L = m˙x2/2
and the action is
S =
 t2
t1
m
2 ˙x2 dt.
(11.299)
The classical trajectory is the one that when varied slightly by δx (with δx(t1) =
δx(t2) = 0) does not change the action to ﬁrst order in δx. We ﬁrst note that the
change δ˙x in the velocity is the time derivative of the change in the path
δ˙x = ˙x′ −˙x = d
dt(x′ −x) = d
dtδx.
(11.300)
443

TENSORS AND LOCAL SYMMETRIES
So since δx(t1) = δx(t2) = 0, the stationary path satisﬁes
0 = δS =
 t2
t1
m˙x · δ˙x dt =
 t2
t1
m˙x · dδx
dt dt
=
 t2
t1
#
m d
dt (˙x · δx) −m¨x · δx
$
dt
= m [˙x · δx]t2
t1 −m
 t2
t1
¨x · δx dt = −m
 t2
t1
¨x · δx dt.
(11.301)
If the ﬁrst-order change in the action is to vanish for arbitrary small variations
δx in the path, then the acceleration must vanish
¨x = 0,
(11.302)
which is the classical equation of motion for a free particle.
If the particle is moving under the inﬂuence of a potential V(x), then the
action is
S =
 t2
t1
m
2 ˙x2 −V(x)

dt.
(11.303)
Since δV(x) = ∇V(x) · δx, the principle of stationary action requires that
0 = δS =
 t2
t1
(−m¨x −∇V) · δx dt
(11.304)
or
m¨x = −∇V,
(11.305)
which is the classical equation of motion for a particle of mass m in a
potential V.
The action for a free particle of mass m in special relativity is
S = −m
 τ2
τ1
dτ = −
 t2
t1
m

1 −˙x2 dt
(11.306)
where c = 1 and ˙x = dx/dt. The requirement of stationary action is
0 = δS = −δ
 t2
t1
m

1 −˙x2 dt = m
 t2
t1
˙x · δ˙x

1 −˙x2 dt.
(11.307)
But 1/

1 −˙x2 = dt/dτ and so
0 = δS = m
 t2
t1
dx
dt · dδx
dt
dt
dτ dt = m
 τ2
τ1
dx
dt · dδx
dt
dt
dτ
dt
dτ dτ
= m
 τ2
τ1
dx
dτ · dδx
dτ dτ.
(11.308)
444

11.37 THE PRINCIPLE OF STATIONARY ACTION
So, integrating by parts, keeping in mind that δx(τ2) = δx(τ1) = 0, we have
0 = δS = m
 τ2
τ1

d
dτ (˙x · δx) −d2x
dτ 2 · δx

dτ = −m
 τ2
τ1
d2x
dτ 2 ·δx dτ. (11.309)
To have this hold for arbitrary δx, we need
d2x
dτ 2 = 0,
(11.310)
which is the equation of motion for a free particle in special relativity.
What about a charged particle in an electromagnetic ﬁeld Ai? Its action is
S = −m
 τ2
τ1
dτ + q
 x2
x1
Ai(x) dxi =
 τ2
τ1

−m + qAi(x)dxi
dτ

dτ.
(11.311)
We now treat the ﬁrst term in a four-dimensional manner
δdτ = δ

−ηikdxidxk = −ηikdxiδdxk

−ηikdxidxk = −ukδdxk = −ukdδxk,
(11.312)
in which uk = dxk/dτ is the 4-velocity (11.66) and η is the Minkowski metric
(11.27) of ﬂat space-time. The variation of the other term is
δ

Ai dxi
= (δAi) dxi + Ai δdxi = Ai,kδxk dxi + Ai dδxi.
(11.313)
Putting them together, we get for δS
δS =
 τ2
τ1

muk
dδxk
dτ
+ qAi,kδxk dxi
dτ + qAi
dδxi
dτ

dτ.
(11.314)
After integrating by parts the last term, dropping the boundary terms, and
changing a dummy index, we get
δS =
 τ2
τ1

−mduk
dτ δxk + qAi,kδxk dxi
dτ −qdAk
dτ δxk

dτ
=
 τ2
τ1
#
−mduk
dτ + q

Ai,k −Ak,i
 dxi
dτ
$
δxk dτ.
(11.315)
If this ﬁrst-order variation of the action is to vanish for arbitrary δxk, then the
particle must follow the path
0 = −mduk
dτ + q

Ai,k −Ak,i
 dxi
dτ
or
dpk
dτ = qFkiui,
(11.316)
which is the Lorentz force law (11.96).
445

TENSORS AND LOCAL SYMMETRIES
11.38 A particle in a gravitational ﬁeld
The invariant action for a particle of mass m moving along a path xi(t) is
S = −m
 τ2
τ1
dτ = −m
 
−giℓdxidxℓ 1
2 .
(11.317)
Proceeding as in equation (11.312), we compute the variation δdτ as
δdτ = δ

−giℓdxidxℓ= −δ(giℓ)dxidxℓ−2giℓdxiδdxℓ
2

−giℓdxidxℓ
= −1
2giℓ,kδxkuiuℓdτ −giℓuiδdxℓ
= −1
2giℓ,kδxkuiuℓdτ −giℓuidδxℓ,
(11.318)
in which uℓ= dxℓ/dτ is the 4-velocity (11.66). The condition of stationary
action then is
0 = δS = −m
 τ2
τ1
δdτ = m
 τ2
τ1

1
2giℓ,kδxkuiuℓ+ giℓui dδxℓ
dτ

dτ,
(11.319)
which we integrate by parts keeping in mind that δxℓ(τ2) = δxℓ(τ1) = 0
0 = m
 τ2
τ1

1
2giℓ,kδxkuiuℓ−d(giℓui)
dτ
δxℓ

dτ
= m
 τ2
τ1

1
2giℓ,kδxkuiuℓ−giℓ,kuiukδxℓ−giℓ
dui
dτ δxℓ

dτ.
(11.320)
Now interchanging the dummy indices ℓand k on the second and third terms,
we have
0 = m
 τ2
τ1

1
2giℓ,kuiuℓ−gik,ℓuiuℓ−gik
dui
dτ

δxkdτ
(11.321)
or since δxk is arbitrary
0 = 1
2giℓ,kuiuℓ−gik,ℓuiuℓ−gik
dui
dτ .
(11.322)
If we multiply this equation of motion by grk and note that gik,ℓuiuℓ= gℓk,iuiuℓ,
then we ﬁnd
0 = dur
dτ + 1
2grk 
gik,ℓ+ gℓk,i −giℓ,k

uiuℓ.
(11.323)
So, using the symmetry giℓ= gℓi and the formula (11.255) for r
iℓ, we get
0 = dur
dτ + r
iℓuiuℓ
or
0 = d2xr
dτ 2 + r
iℓ
dxi
dτ
dxℓ
dτ ,
(11.324)
446

11.39 THE PRINCIPLE OF EQUIVALENCE
which is the geodesic equation. In empty space, particles fall along geodesics
independently of their masses.
The right-hand side of the geodesic equation (11.324) is a contravariant vec-
tor because (Weinberg, 1972) under general coordinate transformations, the
inhomogeneous terms arising from ¨xr cancel those from r
iℓ˙xi ˙xℓ. Here and often
in what follows we’ll use dots to mean proper-time derivatives.
The action for a particle of mass m and charge q in a gravitational ﬁeld r
iℓ
and an electromagnetic ﬁeld Ai is
S = −m
 
−giℓdxidxℓ 1
2 + q
 τ2
τ1
Ai(x) dxi
(11.325)
because the interaction q
2
Aidxi is invariant under general coordinate transfor-
mations. By (11.315 & 11.321), the ﬁrst-order change in S is
δS = m
 τ2
τ1
#
1
2giℓ,kuiuℓ−gik,ℓuiuℓ−gik
dui
dτ + q

Ai,k −Ak,i

ui
$
δxkdτ
(11.326)
and so by combining the Lorentz force law (11.316) and the geodesic equation
(11.324) and by writing Fri ˙xi as Fr
i ˙xi, we have
0 = d2xr
dτ 2 + r
iℓ
dxi
dτ
dxℓ
dτ −q
m Fr
i
dxi
dτ
(11.327)
as the equation of motion of a particle of mass m and charge q. It is striking
how nearly perfect the electromagnetism of Faraday and Maxwell is.
11.39 The principle of equivalence
The principle of equivalence says that in any gravitational ﬁeld, one may choose
free-fall coordinates in which all physical laws take the same form as in special
relativity without acceleration or gravitation – at least over a suitably small vol-
ume of space-time. Within this volume and in these coordinates, things behave
as they would at rest deep in empty space far from any matter or energy.
The volume must be small enough so that the gravitational ﬁeld is constant
throughout it.
Example 11.21 (Elevators)
When a modern elevator starts going down from a
high ﬂoor, it accelerates downward at something less than the local acceleration
of gravity. One feels less pressure on one’s feet; one feels lighter. (This is as close
to free fall as I like to get.) After accelerating downward for a few seconds, the
elevator assumes a constant downward speed, and then one feels the normal
pressure of one’s weight on one’s feet. The elevator seems to be slowing down
for a stop, but actually it has just stopped accelerating downward.
447

TENSORS AND LOCAL SYMMETRIES
If in those ﬁrst few seconds the elevator really were falling, then the physics
in it would be the same as if it were at rest in empty space far from any gravita-
tional ﬁeld. A clock in it would tick as fast as it would at rest in the absence of
gravity.
The transformation from arbitrary coordinates xk to free-fall coordinates
yi changes the metric gjℓto the diagonal metric ηik of ﬂat space-time η =
diag(−1, 1, 1, 1), which has two indices and is not a Levi-Civita tensor. Alge-
braically, this transformation is a congruence (1.308)
ηik = ∂xj
∂yi gjℓ
∂xℓ
∂yk .
(11.328)
The geodesic equation (11.324) follows from the principle of equiva-
lence (Weinberg, 1972; Hobson et al., 2006). Suppose a particle is moving under
the inﬂuence of gravitation alone. Then one may choose free-fall coordinates
y(x) so that the particle obeys the force-free equation of motion
d2yi
dτ 2 = 0
(11.329)
with dτ the proper time dτ 2 = −ηik dyidyk. The chain rule applied to yi(x) in
(11.329) gives
0 = d
dτ

∂yi
∂xk
dxk
dτ

= ∂yi
∂xk
d2xk
dτ 2 +
∂2yi
∂xk∂xℓ
dxk
dτ
dxℓ
dτ .
(11.330)
We multiply by ∂xm/∂yi and use the identity
∂xm
∂yi
∂yi
∂xk = δm
k
(11.331)
to get the equation of motion (11.329) in the x-coordinates
d2xm
dτ 2 + m
kℓ
dxk
dτ
dxℓ
dτ = 0,
(11.332)
in which the afﬁne connection is
m
kℓ= ∂xm
∂yi
∂2yi
∂xk∂xℓ.
(11.333)
So the principle of equivalence tells us that a particle in a gravitational ﬁeld
obeys the geodesic equation (11.324).
448

11.41 GRAVITATIONAL TIME DILATION
11.40 Weak, static gravitational ﬁelds
Slow motion in a weak, static gravitational ﬁeld is an important example.
Because the motion is slow, we neglect ui compared to u0 and simplify the
geodesic equation (11.324) to
0 = dur
dτ + r
00 (u0)2.
(11.334)
Because the gravitational ﬁeld is static, we neglect the time derivatives gk0,0 and
g0k,0 in the connection formula (11.255) and ﬁnd for r
00
r
00 = 1
2 grk 
g0k,0 + g0k,0 −g00,k

= −1
2 grk g00,k
(11.335)
with 0
00 = 0. Because the ﬁeld is weak, the metric can differ from ηij by
only a tiny tensor gij = ηij + hij so that to ﬁrst order in |hij| ≪1 we have
r
00 = −1
2 h00,r for r = 1, 2, 3. With these simpliﬁcations, the geodesic equation
(11.324) reduces to
d2xr
dτ 2 = 1
2 (u0)2 h00,r
or
d2xr
dτ 2 = 1
2

dx0
dτ
2
h00,r.
(11.336)
So for slow motion, the ordinary acceleration is described by Newton’s law
d2x
dt2 = c2
2 ∇h00.
(11.337)
If φ is his potential, then for slow motion in weak static ﬁelds
g00 = −1 + h00 = −1 −2φ/c2
and so
h00 = −2φ/c2.
(11.338)
Thus, if the particle is at a distance r from a mass M, then φ = −GM/r and
h00 = −2φ/c2 = 2GM/rc2 and so
d2x
dt2 = −∇φ = ∇GM
r
= −GM r
r3 .
(11.339)
How weak are the static gravitational ﬁelds we know about? The dimension-
less ratio φ/c2 is 10−39 on the surface of a proton, 10−9 on the Earth, 10−6 on
the surface of the Sun, and 10−4 on the surface of a white dwarf.
11.41 Gravitational time dilation
Suppose we have a system of coordinates xi with a metric gik and a clock at rest
in this system. Then the proper time dτ between ticks of the clock is
dτ = (1/c)

−gij dxi dxj =

−g00 dt
(11.340)
449

TENSORS AND LOCAL SYMMETRIES
where dt is the time between ticks in the xi coordinates, which is the lab-
oratory frame in the gravitational ﬁeld g00. By the principle of equivalence
(section 11.39), the proper time dτ between ticks is the same as the time between
ticks when the same clock is at rest deep in empty space.
If the clock is in a weak static gravitational ﬁeld due to a mass M at a distance
r, then
−g00 = 1 + 2φ/c2 = 1 −2GM/c2r
(11.341)
is a little less than unity, and the interval of proper time between ticks
dτ =

−g00 dt =

1 −2GM/c2r dt
(11.342)
is slightly less than the interval dt between ticks in the coordinate system of an
observer at x in the rest frame of the clock and the mass, and in its gravita-
tional ﬁeld. Since dt > dτ, the laboratory time dt between ticks is greater than
the proper or intrinsic time dτ between ticks of the clock unaffected by any
gravitational ﬁeld. Clocks near big masses run slow.
Now suppose we have two identical clocks at different heights above sea level.
The time Tℓfor the lower clock to make N ticks will be longer than the time Tu
for the upper clock to make N ticks. The ratio of the clock times will be
Tmℓ
Tu
=

1 −2GM/c2(r + h)

1 −2GM/c2r
≈1 + gh
c2 .
(11.343)
Now imagine that a photon going down passes the upper clock, which mea-
sures its frequency as νu, and then passes the lower clock, which measures its
frequency as νℓ. The slower clock will measure a higher frequency. The ratio of
the two frequencies will be the same as the ratio of the clock times
νℓ
νu
= 1 + gh
c2 .
(11.344)
As measured by the lower, slower clock, the photon is blue shifted.
Example 11.22 (Pound, Rebka, and Mössbauer)
Pound and Rebka in 1960
used the Mössbauer effect to measure the blue shift of light falling down a 22.6 m
shaft. They found
νℓ−νu
ν
= gh
c2 = 2.46 × 10−15
(11.345)
(Robert Pound, 1919–2010; Glen Rebka, 1931–; Rudolf Mössbauer, 1929–2011).
Example 11.23 (Redshift of the Sun)
A photon emitted with frequency ν0
at a distance r from a mass M would be observed at spatial inﬁnity to have
frequency ν
450

11.42 CURVATURE
ν = ν0

−g00 = ν0

1 −2MG/c2r
(11.346)
for a redshift of ν = ν0 −ν. Since the Sun’s dimensionless potential φ⊙/c2 is
−MG/c2r = −2.12 × 10−6 at its surface, sunlight is shifted to the red by 2 parts
per million.
11.42 Curvature
The curvature tensor or Riemann tensor is
Ri
mnk = i
mn,k −i
mk,n + i
kj j
nm −i
nj j
km,
(11.347)
which we may write as the commutator
Ri
mnk = (Rnk)i
m = [∂k + k, ∂n + n]i
m
=

n,k −k,n + k n −n k
i
m ,
(11.348)
in which the s are treated as matrices
k =
⎛
⎜⎜⎜⎜⎝
0
k 0
0
k 1
0
k 2
0
k 3
1
k 0
1
k 1
1
k 2
1
k 3
2
k 0
2
k 1
2
k 2
2
k 3
3
k 0
3
k 1
3
k 2
3
k 3
⎞
⎟⎟⎟⎟⎠
(11.349)
with (k n)i
m = i
kj j
nm and so forth. Just as there are two conventions for
the Faraday tensor Fik, which differ by a minus sign, so too there are two
conventions for the curvature tensor Ri
mnk. Weinberg (1972) uses the deﬁnition
(11.347); Carroll (2003) uses an extra minus sign.
The Ricci tensor is a contraction of the curvature tensor
Rmk = Rn
mnk
(11.350)
and the curvature scalar is a further contraction
R = gmk Rmk.
(11.351)
Example 11.24 (Curvature of a sphere)
While in four-dimensional space-time
indices run from 0 to 3, on the sphere they are just θ and φ. There are only eight
possible afﬁne connections, and because of the symmetry (11.218) in their lower
indices i
θφ = i
φθ, only six are independent.
The point p on a sphere of radius r has cartesian coordinates
p = r (sin θ cos φ, sin θ sin φ, cos θ)
(11.352)
451

TENSORS AND LOCAL SYMMETRIES
so the two 3-vectors are
eθ = ∂p
∂θ = r (cos θ cos φ, cos θ sin φ, −sin θ) = r ˆθ
eφ = ∂p
∂φ = r sin θ (−sin φ, cos φ, 0) = r sin θ ˆφ
(11.353)
and the metric gij = ei · ej is
(gij) =
r2
0
0
r2 sin2 θ

.
(11.354)
Differentiating the vectors eθ and eφ, we ﬁnd
eθ,θ = −r (sin θ cos φ, sin θ sin φ, cos θ) = −r ˆr,
(11.355)
eθ,φ = r cos θ (−sin φ, cos φ, 0) = r cos θ ˆφ,
(11.356)
eφ,θ = eθ,φ,
(11.357)
eφ,φ = −r sin θ (cos φ, sin φ, 0) .
(11.358)
The metric with upper indices (gij) is the inverse of the metric (gij)
(gij) =
r−2
0
0
r−2 sin−2 θ

(11.359)
so the dual vectors ei are
e θ = r−1 (cos θ cos φ, cos θ sin φ, −sin θ) = r−1 ˆθ,
e φ = =
1
r sin θ (−sin φ, cos φ, 0) =
1
r sin θ
ˆφ.
(11.360)
The afﬁne connections are given by (11.213) as
i
jk = i
kj = e i · ej,k.
(11.361)
Since both eθ and e φ are perpendicular to ˆr, the afﬁne connections θ
θθ and φ
θθ
both vanish. Also, eφ,φ is orthogonal to ˆφ, so φ
φφ = 0 as well. Similarly, eθ,φ is
perpendicular to ˆθ, so θ
θφ = θ
φθ also vanishes.
The two nonzero afﬁne connections are
φ
θφ = e φ · eθ,φ = r−1 sin−1 θ ˆφ · r cos θ ˆφ = cot θ
(11.362)
and
θ
φφ = e θ · eφ,φ
= −sin θ (cos θ cos φ, cos θ sin φ, −sin θ) · (cos φ, sin φ, 0)
= −sin θ cos θ.
(11.363)
In terms of the two nonzero afﬁne connections φ
θφ = φ
φθ = cot θ and θ
φφ =
−sin θ cos θ, the two Christoffel matrices (11.349) are
452

11.43 EINSTEIN’S EQUATIONS
θ =

0
0
0
φ
θφ

=
0
0
0
cot θ

(11.364)
and
φ =

0
θ
φφ
φ
φθ
0

=
 0
−sin θ cos θ
cot θ
0

.
(11.365)
Their commutator is
[θ, φ] =

0
cos2 θ
cot2 θ
0

= −[φ, θ]
(11.366)
and both [θ, θ] and [φ, φ] vanish.
So the commutator formula (11.348) gives for Riemann’s curvature tensor
Rθ
θθθ = [∂θ + θ, ∂θ + θ]θ
θ = 0,
Rφ
θφθ = [∂θ + θ, ∂φ + φ]φ
θ =

φ,θ
φ
θ + [θ, φ]φ
θ
= (cot θ),θ + cot2 θ = −1,
Rθ
φθφ = [∂φ + φ, ∂θ + θ]θ
φ = −

φ,θ
θ
φ + [ φ, θ]θ
φ
= cos2 θ −sin2 θ −cos2 θ = −sin2 θ,
Rφ
φφφ = [∂φ + φ, ∂φ + φ]φ
φ = 0.
(11.367)
The Ricci tensor (11.350) is the contraction Rmk = Rn
mnk, and so
Rθθ = Rθ
θθθ + Rφ
θφθ = −1,
Rφφ = Rθ
φθφ + Rφ
φφφ = −sin2 θ.
(11.368)
The curvature scalar (11.351) is the contraction R = gkmRmk, and so since gθθ =
r−2 and gφφ = r−2 sin−2 θ, it is
R = gθθ Rθθ + gφφ Rφφ
= −r−2 −sin2 θ r−2 sin−2 θ = −2
r2
(11.369)
for a 2-sphere of radius r.
Gauss invented a formula for the curvature K of a surface; for all two-
dimensional surfaces, his K = −R/2.
11.43 Einstein’s equations
The source of the gravitational ﬁeld is the energy–momentum tensor Tij. In
many astrophysical and most cosmological models, the energy–momentum ten-
sor is assumed to be that of a perfect ﬂuid, which is isotropic in its rest frame,
does not conduct heat, and has zero viscosity. For a perfect ﬂuid of pressure p
453

TENSORS AND LOCAL SYMMETRIES
and density ρ with 4-velocity ui (deﬁned by (11.66)), the energy–momentum or
stress–energy tensor Tij is
Tij = p gij + (p + ρ) ui uj,
(11.370)
in which gij is the space-time metric.
An important special case is the energy–momentum tensor due to a nonzero
value of the energy density of the vacuum. In this case p = −ρ and the energy–
momentum tensor is
Tij = −ρ gij,
(11.371)
in which ρ is the (presumably constant) value of the energy density of the
ground state of the theory. This energy density ρ is a plausible candidate for
the dark-energy density. It is equivalent to a cosmological constant  = 8πGρ.
Whatever its nature, the energy–momentum tensor usually is deﬁned so as to
satisfy the conservation law
0 =

Ti
j

;i = ∂iTi
j + i
icTc
j −Ti
cc
ji.
(11.372)
Einstein’s equations relate the Ricci tensor (11.350) and the scalar curvature
(11.351) to the energy–momentum tensor
Rij −1
2 gij R = −8π G
c4
Tij,
(11.373)
in which G = 6.7087 × 10−39 ¯hc (GeV/c2)−2 = 6.6742 × 10−11 m3 kg−1 s−2 is
Newton’s constant. Taking the trace and using gji gij = δj
j = 4, we relate the
scalar curvature to the trace T = Ti
i of the energy–momentum tensor
R = 8πG
c4
T.
(11.374)
So another form of Einstein’s equations (11.373) is
Rij = −8πG
c4

Tij −T
2 gij

.
(11.375)
On small scales, such as that of our Solar System, one may neglect dark
energy. So in empty space and on small scales, the energy–momentum tensor
vanishes Tij = 0 along with its trace and the scalar curvature T = 0 = R, and
Einstein’s equations are
Rij = 0.
(11.376)
454

11.45 STANDARD FORM
11.44 The action of general relativity
If we make an action that is a scalar, invariant under general coordinate trans-
formations, and then apply to it the principle of stationary action, we will get
tensor ﬁeld equations that are invariant under general coordinate transforma-
tions. If the metric of space-time is among the ﬁelds of the action, then the
resulting theory will be a possible theory of gravity. If we make the action as
simple as possible, it will be Einstein’s theory.
To make the action of the gravitational ﬁeld, we need a scalar. Apart from
the volume 4-form √g d4x, the only scalar we can form from the metric tensor
and its ﬁrst and second derivatives is the scalar curvature R, which gives us the
Einstein–Hilbert action
SEH = −
c4
16πG

∗R = −
c4
16πG

R √g d4x.
(11.377)
If δgik(x) is a tiny change in the inverse metric that vanishes as any coordinate
xj →±∞, then one may write the ﬁrst-order change in the action SEH as
δSEH = −
c4
16πG
 
Rik −1
2 gikR
 √g δgik d4x.
(11.378)
The principle of least action δSEH = 0 now leads to Einstein’s equations
Gik = Rik −1
2 gik R = 0
(11.379)
for empty space in which Gik is Einstein’s tensor.
The stress–energy tensor Tik is deﬁned so that the change in the action of the
matter ﬁelds due to a tiny change δgik(x) (vanishing at inﬁnity) in the metric is
δSm = −1
2

Tik
√g δgik d4x.
(11.380)
So the principle of least action δS = δSEH + δSm = 0 implies Einstein’s
equations (11.373, 11.375) in the presence of matter and energy
Rik −1
2 gik R = −8πG
c4 Tij
or
Rij = −8πG
c4

Tij −T
2 gij

.
(11.381)
11.45 Standard form
Tensor equations are independent of the choice of coordinates, so it’s wise
to choose coordinates that simplify one’s work. For a static and isotropic
gravitational ﬁeld, this choice is the standard form (Weinberg, 1972, ch. 8)
dτ 2 = B(r) dt2 −A(r) dr2 −r2 
dθ2 + sin2 θ dφ2
,
(11.382)
455

TENSORS AND LOCAL SYMMETRIES
in which c = 1, and B(r) and A(r) are functions that one may ﬁnd by solving
the ﬁeld equations (11.373). Since dτ 2 =
−ds2 = −gij dxidxj, the nonzero
components of the metric tensor are grr = A(r), gθθ = r2, gφφ = r2 sin2 θ,
and g00 = −B(r), and those of its inverse are grr = A−1(r), gθθ = r−2,
gφφ = r−2 sin−2 θ, and g00 = −B−1(r). By differentiating the metric tensor
and using (11.255), one gets the components of the connection i
kℓ, such as
θ
φφ = −sin θ cos θ, and the components (11.350) of the Ricci tensor Rij, such
as (Weinberg, 1972, ch. 8)
Rrr = B′′(r)
2B(r) −1
4
B′(r)
B(r)
 A′(r)
A(r) + B′(r)
B(r)

−1
r
A′(r)
A(r)

,
(11.383)
in which the primes mean d/dr.
11.46 Schwarzschild’s solution
If one ignores the small dark-energy parameter , one may solve Einstein’s ﬁeld
equations (11.376) in empty space
Rij = 0
(11.384)
outside a mass M for the standard form of the Ricci tensor. One ﬁnds (Wein-
berg, 1972) that A(r) B(r) = 1 and that r B(r) = r plus a constant, and
one determines the constant by invoking the Newtonian limit g00 = −B →
−1 + 2MG/c2r as r →∞. In 1916, Schwarzschild found the solution
dτ 2 =

1 −2MG
c2r

c2dt2 −

1 −2MG
c2r
−1
dr2 −r2 
dθ2 + sin2 θ dφ2
,
(11.385)
which one can use to analyze orbits around a star. The singularity in
grr =

1 −2MG
c2r
−1
(11.386)
at the Schwarzschild radius r = 2MG/c2 is an artifact of the coordinates;
the scalar curvature R and other invariant curvatures are not singular at
the Schwarzschild radius. Moreover, for the Sun, the Schwarzschild radius
r = 2MG/c2 is only 2.95 km, far less than the radius of the Sun, which is
6.955 × 105 km. So the surface at r = 2MG/c2 is far from the empty space in
which Schwarzschild’s solution applies (Karl Schwarzschild, 1873–1916).
11.47 Black holes
Suppose an uncharged, spherically symmetric star of mass M has collapsed
within a sphere of radius rb less than its Schwarzschild radius r = 2MG/c2.
456

11.48 COSMOLOGY
Then for r > rb, the Schwarzschild metric (11.385) is correct. By (11.340), the
apparent time dt of a process of proper time dτ at r ≥2MG/c2 is
dt = dτ/

−g00 = dτ/
&
1 −2MG
c2r .
(11.387)
The apparent time dt becomes inﬁnite as r →2MG/c2. To outside observers,
the star seems frozen in time.
Due to the gravitational redshift (11.346), light of frequency νp emitted at
r ≥2MG/c2 will have frequency ν
ν = νp

−g00 = νp
&
1 −2MG
c2r
(11.388)
when observed at great distances. Light coming from the surface at r = 2MG/c2
is redshifted to zero frequency ν = 0. The star is black. It is a black hole with
a surface or horizon at its Schwarzschild radius r = 2MG/c2, although there
is no singularity there. If the radius of the Sun were less than its Schwarzschild
radius of 2.95 km, then the Sun would be a black hole. The radius of the Sun is
6.955 × 105 km.
Black holes are not really black. Stephen Hawking (1942–) has shown that
the intense gravitational ﬁeld of a black hole of mass M radiates at temperature
T =
¯h c3
8π k G M ,
(11.389)
in which k = 8.617343×10−5 eV K−1 is Boltzmann’s constant, and ¯h is Planck’s
constant h = 6.6260693 × 10−34 J s divided by 2π, ¯h = h/(2π).
The black hole is entirely converted into radiation after a time
t = 5120 π G2
¯h c4
M3
(11.390)
proportional to the cube of its mass.
11.48 Cosmology
Astrophysical observations tell us that on the largest observable scales, space is
ﬂat or very nearly ﬂat; that the visible Universe contains at least 1090 particles;
and that the cosmic microwave background radiation is isotropic to one part in
105 apart from a Doppler shift due the motion of the Earth. These and other
observations suggest that potential energy expanded our Universe by exp(60) =
1026 during an era of inﬂation that could have been as brief as 10−35 s. The
potential energy that powered inﬂation became the radiation of the Big Bang.
During the ﬁrst three minutes, some of that radiation became hydrogen,
helium, neutrinos, and dark matter. But for 65,000 years after the Big Bang,
457

TENSORS AND LOCAL SYMMETRIES
most of the energy of the visible Universe was radiation. Because the momen-
tum of a particle but not its mass falls with the expansion of the Universe, this
era of radiation gradually gave way to an era of matter. This transition happened
when the temperature kT of the Universe fell to 1.28 eV.
The era of matter lasted for 8.8 billion years. After 360,000 years, the Uni-
verse had cooled to kT = 0.26 eV, and fewer than 1% of the atoms were ionized.
Photons no longer scattered off a plasma of electrons and ions. The Universe
became transparent. The photons that last scattered just before this initial trans-
parency became the cosmic microwave background radiation or CMBR that now
surrounds us, red shifted to 2.725 ±0.001 K.
The era of matter was followed by the current era of dark energy during which
the energy of the visible Universe is mostly a potential energy called dark energy
(something like a cosmological constant). Dark energy has been accelerating the
expansion of the Universe for the past 5 billion years and may continue to do
so forever.
It is now 13.75 billion years after the Big Bang, and the dark-energy density
is ρde = 1.37 × 10−29 c2 g cm−3 or 75.7 percent (± 2.1%) of the critical energy
density ρc = 3H2
0/8πG = 1.87837 × 10−29 f 2 c2 g cm−3 needed to make the
Universe ﬂat. Here H0 = 100 h km s−1 Mpc−1 = 1.022/(1010yr) is the Hub-
ble constant, one parsec is 3.262 light-years, and h = 0.72 ± 0.03 is not to be
confused with Planck’s constant.
Matter makes up 24.6 ± 2.8% of the critical density, and baryons only 4.2 ±
0.2% of it. Baryons are 17% of the total matter in the visible Universe. The
other 83% does not interact with light and is called dark matter.
Einstein’s equations (11.373) are second-order, nonlinear partial differential
equations for ten unknown functions gij(x) in terms of the energy–momentum
tensor Tij(x) throughout the Universe, which of course we don’t know. The
problem is not quite hopeless, however. The ability to choose arbitrary coordi-
nates, the appeal to symmetry, and the choice of a reasonable form for Tij all
help.
Hubble showed us that the Universe is expanding. The cosmic microwave
background radiation looks the same in all spatial directions (apart from a
Doppler shift due to the motion of the Earth relative to the local super-cluster
of galaxies). Observations of clusters of galaxies reveal a Universe that is homo-
geneous on suitably large scales of distance. So it is plausible that the Universe
is homogeneous and isotropic in space, but not in time. One may show (Car-
roll, 2003) that for a universe of such symmetry, the line element in comoving
coordinates is
ds2 = −dt2 + a2

dr2
1 −k r2 + r2 
dθ2 + sin2 θ dφ2
.
(11.391)
458

11.48 COSMOLOGY
Whitney’s embedding theorem tells us that any smooth four-dimensional
manifold can be embedded in a ﬂat space of eight dimensions with a suit-
able signature. We need only four or ﬁve dimensions to embed the space-time
described by the line element (11.391). If the Universe is closed, then the signa-
ture is (−1, 1, 1, 1, 1), and our three-dimensional space is the 3-sphere, which is
the surface of a four-dimensional sphere in four space dimensions. The points
of the Universe then are
p = (t, a sin χ sin θ cos φ, a sin χ sin θ sin φ, a sin χ cos θ, a cos χ),
(11.392)
in which 0 ≤χ ≤π, 0 ≤θ ≤π, and 0 ≤φ ≤2π. If the Universe is ﬂat, then
the embedding space is ﬂat, four-dimensional Minkowski space with points
p = (t, ar sin θ cos φ, ar sin θ sin φ, ar cos θ),
(11.393)
in which 0 ≤θ ≤π and 0 ≤φ ≤2π. If the Universe is open, then the embed-
ding space is a ﬂat ﬁve-dimensional space with signature (−1, 1, 1, 1, −1), and
our three-dimensional space is a hyperboloid in a ﬂat Minkowski space of four
dimensions. The points of the Universe then are
p = (t, a sinh χ sin θ cos φ, a sinh χ sin θ sin φ, a sinh χ cos θ, a cosh χ),
(11.394)
in which 0 ≤χ ≤∞, 0 ≤θ ≤π, and 0 ≤φ ≤2π.
In all three cases, the corresponding Robertson–Walker metric is
gij =
⎛
⎜⎜⎝
−1
0
0
0
0
a2/(1 −kr2)
0
0
0
0
a2 r2
0
0
0
0
a2 r2 sin2 θ
⎞
⎟⎟⎠,
(11.395)
in which the coordinates (t, r, θ, φ) are numbered (0, 1, 2, 3), the speed of light is
c = 1, and k is a constant. One always may choose coordinates (exercise 11.30)
such that k is either 0 or ±1. This constant determines whether the spatial Uni-
verse is open k = −1, ﬂat k = 0, or closed k = 1. The scale factor a, which
in general is a function of time a(t), tells us how space expands and contracts.
These coordinates are called comoving because a point at rest (ﬁxed r, θ, φ) sees
the same Doppler shift in all directions.
The metric (11.395) is diagonal; its inverse gij also is diagonal; and so we may
use our formula (11.255) to compute the afﬁne connections k
iℓ, such as
0
ℓℓ= 1
2g0k 
gℓk,ℓ+ gℓk,ℓ−gℓℓ,k

= 1
2g00 
gℓ0,ℓ+ gℓ0,ℓ−gℓℓ,0

= 1
2gℓℓ,0
(11.396)
so that
0
11 =
a˙a
1 −kr2
0
22 = a˙a r2
and
0
22 = a˙a r2 sin2 θ,
(11.397)
459

TENSORS AND LOCAL SYMMETRIES
in which a dot means a time-derivative. The other 0
ijs vanish. Similarly, for
ﬁxed ℓ= 1, 2, or 3
ℓ
0ℓ= 1
2gℓk 
g0k,ℓ+ gℓk,0 −g0ℓ,k

= 1
2gℓℓ
g0ℓ,ℓ+ gℓℓ,0 −g0ℓ,ℓ

= 1
2gℓℓgℓℓ,0 = ˙a
a = ℓ
ℓ0,
no sum over ℓ.
(11.398)
The other nonzero s are
1
22 = −r (1 −kr2),
1
33 = −r (1 −kr2) sin2 θ,
(11.399)
2
12 = 3
13 = 1
r = 2
21 = 3
31,
(11.400)
2
33 = −sin θ cos θ,
3
23 = cos θ = 3
32.
(11.401)
Our formulas (11.350 & 11.348) for the Ricci and curvature tensors give
R00 = Rn
0n0 = [∂0 + 0, ∂n + n]n
0.
(11.402)
Clearly the commutator of 0 with itself vanishes, and one may use the formulas
(11.397–11.401) for the other connections to check that
[0, n]n
0 = n
0 k k
n 0 −n
n k k
0 0 = 3
 ˙a
a
2
(11.403)
and that
∂0 n
n 0 = 3 ∂0
 ˙a
a

= 3 ¨a
a −3
 ˙a
a
2
(11.404)
while ∂nn
0 0 = 0. So the 00-component of the Ricci tensor is
R00 = 3 ¨a
a.
(11.405)
Similarly, one may show that the other nonzero components of Ricci’s tensor
are
R11 = −
A
1 −kr2 ,
R22 = −r2A,
and
R33 = −r2A sin2 θ,
(11.406)
in which A = a¨a + 2˙a2 + 2k. The scalar curvature (11.351) is
R = gabRba = −6
a2

a¨a + ˙a2 + k

.
(11.407)
460

11.48 COSMOLOGY
In comoving coordinates such as those of the Robertson–Walker metric
(11.395) ui = (1, 0, 0, 0), and so the energy–momentum tensor (11.370) is
Tij =
⎛
⎜⎜⎝
ρ
0
0
0
0
p g11
0
0
0
0
p g22
0
0
0
0
p g33
⎞
⎟⎟⎠.
(11.408)
Its trace is
T = gij Tij = −ρ + 3p.
(11.409)
Thus, using our formula (11.395) for g00 = −1, (11.405) for R00, (11.408) for
Tij, and (11.409) for T, we ﬁnd that the 00 Einstein equation (11.375) becomes
the second-order equation
¨a
a = −4πG
3
(ρ + 3p) ,
(11.410)
which is nonlinear because ρ and 3p depend upon a. The sum ρ+3p determines
the acceleration ¨a of the scale factor a(t). When it is negative, it accelerates the
expansion of the Universe.
Because of the isotropy of the metric, the three nonzero spatial Einstein
equations (11.375) give us only one relation
¨a
a + 2
 ˙a
a
2
+ 2 k
a2 = 4πG (ρ −p) .
(11.411)
Using the 00-equation (11.410) to eliminate the second derivative ¨a, we have
 ˙a
a
2
= 8πG
3
ρ −k
a2 ,
(11.412)
which is a ﬁrst-order nonlinear equation. It and the second-order equation
(11.410) are known as the Friedmann equations.
The LHS of the ﬁrst-order Friedmann equation (11.412) is the square of the
Hubble rate
H = ˙a
a,
(11.413)
which is an inverse time. Its present value H0 is the Hubble constant. In terms
of H, Friedmann’s ﬁrst-order equation (11.412) is
H2 = 8πG
3
ρ −k
a2 .
(11.414)
The energy density of a ﬂat universe with k = 0 is the critical energy density
ρc = 3H2
8πG.
(11.415)
461

TENSORS AND LOCAL SYMMETRIES
The ratio of the energy density ρ to the critical energy density is called 
 = ρ
ρc
= 8πG
3H2 ρ.
(11.416)
From (11.414), we see that  is
 = 1 +
k
(aH)2 = 1 + k
˙a2 .
(11.417)
Thus  = 1 both in a ﬂat universe (k = 0) and as aH →∞. One use of inﬂation
is to expand a by 1026 so as to force  to almost exactly unity.
Something like inﬂation is needed because in a universe in which the energy
density is due to matter and/or radiation, the present value of 
0 = 1.003 ± 0.01
(11.418)
is unlikely. To see why, we note that conservation of energy ensures that a3 times
the matter density ρm is constant. Radiation red shifts by a, so energy conser-
vation implies that a4 times the radiation density ρr is constant. So with n = 3
for matter and 4 for radiation, ρan ≡3F2/8πG is a constant. In terms of F and
n, Friedmann’s ﬁrst-order equation (11.412) is
˙a2 = 8πG
3
ρa2 −k = F2
an−2 −k.
(11.419)
In small-a limit of the early Universe, we have
˙a = F/a(n−2)/2
or
a(n−2)/2da = Fdt,
(11.420)
which we integrate to a ∼t2/n so that ˙a ∼t2/n−1. Now (11.417) says that
| −1| = 1
˙a2 ∝t2−4/n =
 t
radiation,
t2/3
matter.
(11.421)
That is, the ratio  deviates from unity with the passage of time. So without
inﬂation (or some other way of vastly expanding the scale factor), the present
value of this ratio 0 = 1.003 ± 0.010 could be so close to unity after 13.8
billion years only if the ratio  at t = 1 second had been unity to within one
part in 1015.
Manipulating our relation (11.417) between  and aH, we see that
(aH)2 =
k
 −1.
(11.422)
So  > 1 implies k = 1, and  < 1 implies k = −1, and as  →1 the product
aH →∞, which is the essence of ﬂatness since curvature vanishes as the scale
factor a →∞. Imagine blowing up a balloon.
462

11.49 MODEL COSMOLOGIES
Staying for the moment with a universe without inﬂation and with an energy
density composed of radiation and/or matter, we note that the ﬁrst-order equa-
tion (11.419) in the form ˙a2 = F2/an−2 −k tells us that for a closed (k = 1)
universe, in the limit a →∞we’d have ˙a2 →−1, which is impossible. Thus
a closed universe eventually collapses, which is incompatible with the ﬂatness
(11.422) implied by the present value 0 = 1.003 ± 0.010.
The ﬁrst-order Friedmann equation (11.412) tells us that ρ a2 ≥3k/8πG.
So in a closed universe (k = 1), the energy density ρ is positive and increases
without limit as a →0 as in a collapse. In open (k < 0) and ﬂat (k = 0)
universes, the same Friedmann equation (11.412) in the form ˙a2 = 8πGρa2/3−
k tells us that if ρ is positive, then ˙a2 > 0, which means that ˙a never vanishes.
Hubble told us that ˙a > 0 now. So if our Universe is open or ﬂat, then it always
expands.
Due to the expansion of the Universe, the wave-length of radiation grows
with the scale factor a(t). A photon emitted at time t and scale factor a(t) with
wave-length λ(t) will be seen now at time t0 and scale factor a(t0) to have a
longer wave-length λ(t0)
λ(t0)
λ(t) = a(t0)
a(t) = z + 1,
(11.423)
in which the redshift z is the ratio
z = λ(t0) −λ(t)
λ(t)
= λ
λ .
(11.424)
Now H = ˙a/a = da/(adt) implies dt = da/(aH), and z = a0/a −1 implies
dz = −a0da/a2, so we ﬁnd
dt = −
dz
(1 + z)H(z),
(11.425)
which relates time intervals to redshift intervals. An on-line calculator is
available for macroscopic intervals (Wright, 2006).
11.49 Model cosmologies
The 0-component of the energy–momentum conservation law (11.372) is
0 =

Ta
0

;a = ∂aTa
0 + a
acTc
0 −Ta
cc
0a
= −∂0T00 −a
a0T00 −gccTccc
0c
= −˙ρ −3 ˙a
aρ −3p ˙a
a = −˙ρ −3 ˙a
a (ρ + p)
(11.426)
or
dρ
da = −3
a (ρ + p) .
(11.427)
463

TENSORS AND LOCAL SYMMETRIES
The energy density ρ is composed of fractions ρk each contributing its own
partial pressure pk according to its own equation of state
pk = wkρk,
(11.428)
in which wk is a constant. In terms of these components, the energy–momentum
conservation law (11.427) is

k
dρk
da = −3
a

k
(1 + wk) ρk
(11.429)
with solution
ρ =

k
ρk
a
a
3(1+wk)
=

k
ρk
a
a
3(1+pk0/ρk)
.
(11.430)
Simple cosmological models take the energy density and pressure each to
have a single component with p = wρ, and in this case
ρ = ρ
a
a
3(1+w)
= ρ
a
a
3(1+p0/ρ)
.
(11.431)
Example 11.25 (w = −1/3, no acceleration)
If w = −1/3, then p = w ρ =
−ρ/3 and ρ + 3p = 0. The second-order Friedmann equation (11.410) then tells
us that ¨a = 0. The scale factor does not accelerate.
To ﬁnd its constant speed, we use its equation of state (11.431)
ρ = ρ
a
a
3(1+w)
= ρ
a
a
2
.
(11.432)
Now all the terms in Friedmann’s ﬁrst-order equation (11.412) have a common
factor of 1/a2, which cancels leaving us with the square of the constant speed
˙a2 = 8πG
3
ρ a2 −k.
(11.433)
Incidentally, ρ a2 must exceed 3k/8πG. The scale factor grows linearly with time
as
a(t) =
8πG
3
ρ a2 −k
1/2
(t −t0) + a(t0).
(11.434)
Setting t0 = 0 and a(0) = 0, we use the deﬁnition of the Hubble parameter
H = ˙a/a to write the constant linear growth ˙a as aH and the time as
t =
 a
0
da′/a′H = (1/aH)
 a
0
da′ = 1/H.
(11.435)
464

11.49 MODEL COSMOLOGIES
So in a universe without acceleration, the age of the universe is the inverse of the
Hubble rate. For our Universe, the present Hubble time is 1/H0 = 13.6 billion
years, which isn’t far from the actual age of 13.75 billion years. Presumably, a
slower Hubble rate during the era of matter has been compensated by a higher
rate during the era of dark energy.
Example 11.26 (w = −1, inﬂation)
Inﬂation occurs when the ground state of
the theory has a positive and constant energy density ρ > 0 that dwarfs the
energy densities of the matter and radiation. The internal energy of the Universe
then is proportional to its volume U = ρ V, and the pressure p as given by the
thermodynamic relation
p = −∂U
∂V = −ρ
(11.436)
is negative. The equation of state (11.428) tells us that in this case w = −1. The
second-order Friedmann equation (11.410) becomes
¨a
a = −4πG
3
(ρ + 3p) = 8πGρ
3
≡g2.
(11.437)
By it and the ﬁrst-order Friedmann equation (11.412) and by choosing t = 0 as
the time at which the scale factor a is minimal, one may show (exercise 11.37)
that in a closed (k = 1) universe
a(t) = cosh g t
g
.
(11.438)
Similarly, in an open (k = −1) universe with a(0) = 0, we have
a(t) = sinh g t
g
.
(11.439)
Finally, in a ﬂat (k = 0) expanding universe, the scale factor is
a(t) = a(0) exp(g t).
(11.440)
Studies of the cosmic microwave background radiation suggest that inﬂation
did occur in the very early Universe, possibly on a time scale as short as 10−35 s.
What is the origin of the vacuum energy density ρ that drove inﬂation? Current
theories attribute it to the assumption by at least one scalar ﬁeld φ of a mean
value ⟨φ⟩different from the one ⟨0|φ|0⟩that minimizes the energy density of
the vacuum. When ⟨φ⟩settled to ⟨0|φ|0⟩, the vacuum energy was released as
radiation and matter in a Big Bang.
Example 11.27 (w = 1/3, the era of radiation)
Until a redshift of z ≳3000
or somewhat less than 65,000 years after inﬂation, our Universe was dominated
by radiation (Frieman et al., 2008). During The First Three Minutes (Weinberg,
1988) of the era of radiation, the quarks and gluons formed hadrons, which
465

TENSORS AND LOCAL SYMMETRIES
decayed into protons and neutrons. As the neutrons decayed (τ = 885.7 s), they
and the protons formed the light elements – principally hydrogen, deuterium,
and helium – in a process called big-bang nucleosynthesis.
We can guess the value of w for radiation by noticing that the energy–
momentum tensor of the electromagnetic ﬁeld (in suitable units)
Tab = Fa
cFbc −1
4gabFcdFcd
(11.441)
is traceless
T = Ta
a = Fa
cF c
a −1
4δa
aFcdFcd = 0.
(11.442)
But by (11.409) its trace must be T = 3p −ρ. So for radiation p = ρ/3 and
w = 1/3. The relation (11.431) between the energy density and the scale factor
then is
ρ = ρ
a
a
4
.
(11.443)
The energy drops both with the volume a3 and with the scale factor a due to a
redshift; so it drops as 1/a4. Thus the quantity
f 2 ≡8πGρa4
3
(11.444)
is a constant. The Friedmann equations (11.410 & 11.411) now are
¨a
a = −4πG
3
(ρ + 3p) = −8πGρ
3
or
¨a = −f 2
a3
(11.445)
and
˙a2 + k = f 2
a2 .
(11.446)
With calendars chosen so that a(0) = 0, this last equation (11.446) tells us that
for a ﬂat universe (k = 0)
a(t) = (2f t)1/2
(11.447)
while for a closed universe (k = 1)
a(t) =

f 2 −(t −f )2
(11.448)
and for an open universe (k = −1)
a(t) =

(t + f )2 −f 2
(11.449)
as we saw in (6.422). The scale factor (11.448) of a closed universe of radiation
has a maximum a = f at t = f and falls back to zero at t = 2f .
Example 11.28 (w = 0, the era of matter)
A universe composed only of dust or
nonrelativistic collisionless matter has no pressure. Thus p = wρ = 0 with ρ ̸= 0,
466

11.49 MODEL COSMOLOGIES
and so w = 0. Conservation of energy (11.430), or equivalently (11.431), implies
that the energy density falls with the volume
ρ = ρ
a
a
3
.
(11.450)
As the scale factor a(t) increases, the matter energy density, which falls as 1/a3,
eventually dominates the radiation energy density, which falls as 1/a4. This
happened in our Universe somewhat less than 65,000 years after inﬂation at
a temperature of kT = 1.28 eV. Were baryons most of the matter, the era of
radiation dominance would have lasted for a few hundred thousand years. But
the kind of matter that we know about, which interacts with photons, is only
about 17% of the total; the rest – an unknown substance called dark matter –
shortened the era of radiation dominance by nearly 2 million years.
Since ρ ∝1/a3, the quantity
m2 = 4πGρa3
3
(11.451)
is a constant. For a matter-dominated universe, the Friedmann equations
(11.410 & 11.411) then are
¨a
a = −4πG
3
(ρ + 3p) = −4πGρ
3
or
¨a = −m2
a2
(11.452)
and
˙a2 + k = 2m2/a.
(11.453)
For a ﬂat universe, k = 0, we get
a(t) =
# 3m
√
2
t
$2/3
.
(11.454)
For a closed universe, k = 1, we use example 6.46 to integrate
˙a =

2m2/a −1
(11.455)
to
t −t0 = −

a(2m2 −a) −m2 arcsin(1 −a/m2).
(11.456)
With a suitable calendar and choice of t0, one may parametrize this solution in
terms of the development angle φ(t) as
a(t) = m2 [1 −cos φ(t)] ,
t = m2 [φ(t) −sin φ(t)] .
(11.457)
For an open universe, k = −1, we use example 6.47 to integrate
˙a =

2m2/a + 1
(11.458)
467

TENSORS AND LOCAL SYMMETRIES
to
t −t0 =

a(2m2 + a)
1/2
−m2 ln

2

a(2m2 + a)
1/2
+ 2a + 2m2
%
.
(11.459)
The conventional parametrization is
a(t) = m2 [cosh φ(t) −1] ,
t = m2 [sinh φ(t) −φ(t)] .
(11.460)
Transparency
Some 360,000 years after inﬂation at a redshift of about z =
1100, the Universe had cooled to kT = 0.26 eV – a temperature at which less
than 1% of the hydrogen is ionized. Ordinary matter became a gas of neutral
atoms rather than a plasma of ions and electrons, and the Universe suddenly
became transparent to light. Some scientists call this moment of last scattering
or ﬁrst transparency recombination.
Example 11.29 (w = −1, the era of dark energy)
Somewhat more than 8.8
billion years after inﬂation at a redshift of z ≳0.5, the matter density falling
as 1/a3 dropped below the very small but positive value of the energy density
ρ = 31 meV4 of the vacuum. The present time is 13.8 billion years after inﬂation.
So for the past 5 billion years, this constant energy density, called dark energy,
has accelerated the expansion of the Universe approximately as (11.439)
a(t) = a(tm) exp

(t −tm)

8πGρ/3

,
(11.461)
in which tm = 8.8 × 109 years.
Observations and measurements on the largest scales indicate that the Uni-
verse is ﬂat: k = 0. So the evolution of the scale factor a(t) is given by the k = 0
equations (11.440, 11.447, 11.454, & 11.461) for a ﬂat universe. During the brief
era of inﬂation, the scale factor a(t) grew as (11.440)
a(t) = a(0) exp

t

8πGρi/3

,
(11.462)
in which ρi is the positive energy density that drove inﬂation.
During the 65,000-year era of radiation, a(t) grew as √t as in (11.447)
a(t) =

2 (t −ti)

8πGρ(t′r)a4(t′r)/3
1/2
+ a(ti)
(11.463)
where ti is the time at the end of inﬂation, and t′
r is any time during the era of
radiation. During this era, the energy of highly relativistic particles dominated
the energy density, and ρa4 ∝T4a4 was approximately constant, so that T(t) ∝
1/a(t) ∝1/√t. When the temperature was in the range 1012 > T > 1010 K
468

11.50 YANG–MILLS THEORY
or mμc2 > kT > mec2, where mμ is the mass of the muon and me that of the
electron, the radiation was mostly electrons, positrons, photons, and neutrinos,
and the relation between the time t and the temperature T was (Weinberg, 2010,
ch. 3)
t = 0.994 sec ×

1010 K
T
2
+ constant.
(11.464)
By 109 K, the positrons had annihilated with electrons, and the neutrinos
fallen out of equilibrium. Between 109 K and 106 K, when the energy den-
sity of nonrelativistic particles became relevant, the time–temperature relation
was (Weinberg, 2010, ch. 3)
t = 1.78 sec ×

1010 K
T
2
+ constant′.
(11.465)
During the 8.8 billion years of the matter era, a(t) grew as (11.454)
a(t) =
#
(t −tr)

3πGρ(t′m)a(t′m) + a3/2(tr)
$2/3
+ a(tr),
(11.466)
where tr is the time at the end of the radiation era, and t′
m is any time in the mat-
ter era. By 360,000 years, the temperature had dropped to 3000 K, the Universe
had become transparent, and the CMBR had begun to travel freely.
Over the past 5 billion years of the era of vacuum dominance, a(t) has been
growing exponentially (11.461)
a(t) = a(tm) exp

(t −tm)

8πGρv/3

,
(11.467)
in which tm is the time at the end of the matter era, and ρv is the density of dark
energy, which, while vastly less than the energy density ρi that drove inﬂation,
currently amounts to 76% of the total energy density.
11.50 Yang–Mills theory
The gauge transformation of an abelian gauge theory like electrodynamics mul-
tiplies a single charged ﬁeld by a space-time-dependent phase factor φ′(x) =
exp(iqθ(x)) φ(x). Yang and Mills generalized this gauge transformation to one
that multiplies a vector φ of matter ﬁelds by a space-time dependent unitary
matrix U(x)
φ′
a(x) =
n

b=1
Uab(x) φb(x)
or
φ′(x) = U(x) φ(x)
(11.468)
469

TENSORS AND LOCAL SYMMETRIES
and showed how to make the action of the theory invariant under such
nonabelian gauge transformations. (The ﬁelds φ are scalars for simplicity.)
Since the matrix U is unitary, inner products like φ†(x) φ(x) are automatically
invariant

φ†(x) φ(x)
′
= φ†(x)U†(x)U(x)φ(x) = φ†(x)φ(x).
(11.469)
But inner products of derivatives ∂iφ† ∂iφ are not invariant because the deriva-
tive acts on the matrix U(x) as well as on the ﬁeld φ(x).
Yang and Mills made derivatives Diφ that transform like the ﬁelds φ
(Diφ)′ = U Diφ.
(11.470)
To do so, they introduced gauge-ﬁeld matrices Ai that play the role of the
connections i in general relativity and set
Di = ∂i + Ai,
(11.471)
in which Ai, like ∂i, is antihermitian. They required that under the gauge trans-
formation (11.468), the gauge-ﬁeld matrix Ai transform to A′
i in such a way as
to make the derivatives transform as in (11.470)
(Diφ)′ =

∂i + A′
i

φ′ =

∂i + A′
i

Uφ = U Diφ = U (∂i + Ai) φ.
(11.472)
So they set

∂i + A′
i

Uφ = U (∂i + Ai) φ
or
(∂iU) φ + A′
i Uφ = UAi φ
(11.473)
and made the gauge-ﬁeld matrix Ai transform as
A′
i = UAiU−1 −(∂iU) U−1.
(11.474)
Thus under the gauge transformation (11.468), the derivative Diφ transforms
as in (11.470), like the vector φ in (11.468), and the inner product of covariant
derivatives

Diφ
† Diφ
′
=

Diφ
† U†UDiφ =

Diφ
† Diφ
(11.475)
remains invariant.
To make an invariant action density for the gauge-ﬁeld matrices Ai, they
used the transformation law (11.472), which implies that D′
i Uφ = UDi φ or
D′
i = UDi U−1. So they deﬁned their generalized Faraday tensor as
Fik = [Di, Dk] = ∂iAk −∂kAi + [Ai, Ak],
(11.476)
which transforms covariantly
F′
ik = UFikU−1.
(11.477)
470

11.51 GAUGE THEORY AND VECTORS
They then generalized the action density FikFik of electrodynamics to the trace
Tr

FikFik
of the square of the Faraday matrices which is invariant under gauge
transformations since
Tr

UFikU−1UFikU−1
= Tr

UFikFikU−1
= Tr

FikFik
.
(11.478)
As an action density for fermionic matter ﬁelds, they replaced the ordinary
derivative in Dirac’s formula ψ(γ i∂i + m)ψ by the covariant derivative (11.471)
to get ψ(γ iDi + m)ψ (Chen-Ning Yang, 1922–; Robert L. Mills, 1927–1999).
In an abelian gauge theory, the square of the 1-form A = Ai dxi vanishes
A2 = Ai Ak dxi ∧dxk = 0, but in a nonabelian gauge theory the gauge ﬁelds are
matrices, and A2 ̸= 0. The sum dA + A2 is the Faraday 2-form
F = dA + A2 = (∂i Ak + Ai Ak) dxi ∧dxk
= 1
2 (∂i Ak −∂k Ai + [Ai Ak]) dxi ∧dxk
= 1
2Fik dxi ∧dxk.
(11.479)
The scalar matter ﬁelds φ may have self-interactions described by a poten-
tial V(φ) such as V(φ) = λ(φ†φ −m2/λ)2, which is positive unless φ†φ =
m2/λ. The kinetic action of these ﬁelds is (Diφ)†Diφ. At low temperatures,
these scalar ﬁelds assume mean values ⟨0|φ|0⟩= φ0 in the vacuum with
φ†
0φ0 = m2/λ so as to minimize their potential energy density V(φ). Their
kinetic action (Diφ)†Diφ = (∂iφ + Aiφ)†(∂iφ + Aiφ) then is in effect φ†
0 Ai Aiφ0.
The gauge-ﬁeld matrix Ai
ab = tα
abAi
α is a linear combination of the genera-
tors tα of the gauge group. So the action of the scalar ﬁelds contains the term
φ†
0 Ai Ai φ0 = M2
αβ Ai
α Aiβ in which the mass-squared matrix for the gauge ﬁelds
is M2
αβ = φ∗a
0 tα
ab tβ
bc φc
0. This Higgs mechanism gives masses to those linear
combinations bβi Aβ of the gauge ﬁelds for which M2
αβ bβi = m2
i bαi ̸= 0 .
The Higgs mechanism also gives masses to the fermions. The mass term m
in the Yang–Mills–Dirac action is replaced by something like c φ in which c
is a constant different for each fermion. In the vacuum and at low tempera-
tures, each fermion in effect acquires as its mass c φ0. On 4 July 2012, physicists
at CERN’s Large Hadron Collider announced the discovery of a Higgs-like
particle with a mass near 126 GeV/c2 (Peter Higgs, 1929–).
11.51 Gauge theory and vectors
This section is optional on a ﬁrst reading.
We can formulate Yang–Mills theory in terms of vectors as we did relativity.
To accommodate noncompact groups, we will generalize the unitary matrices
U(x) of the Yang–Mills gauge group to nonsingular matrices V(x) that act on
n matter ﬁelds ψa(x) as
471

TENSORS AND LOCAL SYMMETRIES
ψ′a(x) =
n

a=1
Va
b(x) ψa(x).
(11.480)
The ﬁeld
(x) =
n

a=1
ea(x) ψa(x)
(11.481)
will be gauge invariant ′(x) = (x) if the vectors ea(x) transform as
e′
a(x) =
n

b=1
eb(x) V−1b
a(x).
(11.482)
In what follows, we will sum over repeated indices from 1 to n and often will
suppress explicit mention of the space-time coordinates. In this compressed
notation, the ﬁeld  is gauge invariant because
′ = e′
a ψ′a = eb V−1b
a Va
c ψc = eb δb
c ψc = eb ψb = ,
(11.483)
which is e′Tψ′ = eTV−1Vψ = eTψ in matrix notation.
The inner product of two basis vectors is an internal “metric tensor”
e∗
a · eb =
N

α=1
N

β=1
eα∗
a ηαβeα
b =
N

α=1
eα∗
a eα
b = gab,
(11.484)
in which for simplicity I used the N-dimensional identity matrix for the metric
η. As in relativity, we’ll assume the matrix gab to be nonsingular. We then can
use its inverse to construct dual vectors ea = gabeb that satisfy ea† · eb = δa
b.
The free Dirac action density of the invariant ﬁeld 
(γ i∂i + m) = ψaea†(γ i∂i + m)ebψb = ψa

γ i(δa
b∂i + ea† · eb,i) + mδa
b

ψb
(11.485)
is the full action of the component ﬁelds ψb
(γ i∂i + m) = ψa(γ iDa
i b + m δa
b)ψb = ψa

γ i(δa
b∂i + Aa
i b) + m δa
b

ψb
(11.486)
if we identify the gauge-ﬁeld matrix as Aa
i b = ea† · eb,i in harmony with the
deﬁnition (11.213) of the afﬁne connection k
iℓ= ek · eℓ,i.
Under the gauge transformation e′
a = eb V−1b
a, the metric matrix trans-
forms as
g′
ab = V−1c∗
a gcd V−1d
b
or as
g′ = V−1† g V−1
(11.487)
in matrix notation. Its inverse goes as g′−1 = V g−1 V†.
472

11.51 GAUGE THEORY AND VECTORS
The gauge-ﬁeld matrix Aa
i b = ea† · eb,i = gace†
c · eb,i transforms as
A′a
i b = g′ace′†
a · e′
b,i = Va
cAc
idV−1d
b
+ Va
cV−1c
b,i
(11.488)
or as A′
i = VAiV−1 + V∂iV−1 = VAiV−1 −(∂iV) V−1.
By using the identity ea† · ec,i = −ea†
,i · ec, we may write (exercise 11.44) the
Faraday tensor as
Fa
ijb = [Di, Dj]a
b = ea†
,i ·eb,j −ea†
,i ·ec ec†·eb,j −ea†
,j ·eb,i +ea†
,j ·ec ec†·eb,i. (11.489)
If n = N, then
n

c=1
eα
c eβc∗= δαβ
and
Fa
ijb = 0.
(11.490)
The Faraday tensor vanishes when n = N because the dimension of the
embedding space is too small to allow the tangent space to have different orien-
tations at different points x of space-time. The Faraday tensor, which represents
internal curvature, therefore must vanish. One needs at least three dimensions
in which to bend a sheet of paper. The embedding space must have N > 2
dimensions for SU(2), N > 3 for SU(3), and N > 5 for SU(5).
The covariant derivative of the internal metric matrix
g;i = g,i −gAi −A†
i g
(11.491)
does not vanish and transforms as

g;i
′ = V−1†g,iV−1. A suitable action den-
sity for it is the trace Tr(g;ig−1g;ig−1). If the metric matrix assumes a (constant,
hermitian) mean value g0 in the vacuum at low temperatures, then its action is
m2Tr

(g0Ai + A†
i g0)g−1
0 (g0Ai + Ai†g0)g−1
0

,
(11.492)
which is a mass term for the matrix of gauge bosons
Wi = g1/2
0
Ai g−1/2
0
+ g−1/2
0
A†
i g1/2
0 .
(11.493)
This mass mechanism also gives masses to the fermions. To see how, we write
the Dirac action density (11.486) as
ψa

γ i(δa
b∂i + Aa
i b) + m δa
b

ψb = ψa 
γ i(gab∂i + gacAc
i b) + m gab

ψb.
(11.494)
Each fermion now gets a mass m ci proportional to an eigenvalue ci of the
hermitian matrix g0.
This mass mechanism does not leave behind scalar bosons. Whether Nature
uses it is unclear.
473

TENSORS AND LOCAL SYMMETRIES
11.52 Geometry
This section is optional on a ﬁrst reading.
In gauge theory, what plays the role of space-time? Could it be the group
manifold? Let us consider the gauge group SU(2) whose group manifold is the
3-sphere in ﬂat euclidean 4-space. A point on the 3-sphere is
p =

±

1 −r2, r1, r2, r3
(11.495)
as explained in example 10.28. The coordinates ra = ra are not vectors. The
three basis vectors are
ea = ∂p
∂ra =

∓
ra

1 −r2 , δ1
a, δ2
a, δ3
a

(11.496)
and so the metric gab = ea · eb is
gab = ra rb
1 −r2 + δab
(11.497)
or
∥g ∥=
1
1 −r2
⎛
⎝
1 −r2
2 −r2
3
r1 r2
r1 r3
r2 r1
1 −r2
1 −r2
3
r2 r3
r3 r1
r3 r2
1 −r2
1 −r2
2
⎞
⎠.
(11.498)
The inverse matrix is
gbc = δbc −rb rc.
(11.499)
The dual vectors
eb = gbcec =

∓rb

1 −r2, δb
1 −rbr1, δb
2 −rbr2, δb
3 −rbr3

(11.500)
satisfy eb · ea = δb
a.
There are two kinds of afﬁne connection eb·ea,c and eb·ea,i. If we differentiate
ea with respect to an SU(2) coordinate rc, then
Eb
c a = eb · ea,c = rb

δac + ra rc
1 −r2

,
(11.501)
in which we used E (for Einstein) instead of  for the afﬁne connection. If we
differentiate ea with respect to a space-time coordinate xi, then
Eb
i a = eb · ea,i = eb · ea,c rc
,i = rb rc
,i

δac + ra rc
1 −r2

.
(11.502)
But if the group coordinates ra are functions of the space-time coordinates xi,
then there are four new basis 4-vectors ei = eara,i. The metric then is a 7 × 7
474

EXERCISES
matrix ∥g ∥with entries ga,b = ea · eb, ga,k = ea · ek, gi,b = ei · eb, and
gi,k = ei · ek or
∥g ∥=
 ga,b
ga,b rb,k
ga,b ra,i
ga,b ra,i rb,k.

.
(11.503)
Further reading
The classics Gravitation and Cosmology (Weinberg, 1972), Gravitation (Mis-
ner et al., 1973), and Cosmology (Weinberg, 2010) as well as the terse General
Theory of Relativity (Dirac, 1996) and the very accessible Spacetime and
Geometry (Carroll, 2003) are of special interest, as is Daniel Finley’s website
(panda.unm.edu/Courses/Finley/p570.html).
Exercises
11.1
Compute the derivatives (11.22 & 11.23).
11.2
Show that the transformation x →x′ deﬁned by (11.16) is a rotation and a
reﬂection.
11.3
Show that the matrix (11.40) satisﬁes the Lorentz condition (11.39).
11.4
If η = L η LT, show that  = L−1 satisﬁes the deﬁnition (11.39) of a Lorentz
transformation η = Tη .
11.5
The LHC is designed to collide 7 TeV protons against 7 TeV protons for a
total collision energy of 14 TeV. Suppose one used a linear accelerator to ﬁre
a beam of protons at a target of protons at rest at one end of the accelerator.
What energy would you need to see the same physics as at the LHC?
11.6
Use Gauss’s law and the Maxwell–Ampère law (11.87) to show that the
microscopic (total) current 4-vector j = (cρ, j) obeys the continuity equation
˙ρ + ∇· j = 0.
11.7
Show that if Mik is a covariant second-rank tensor with no particular sym-
metry, then only its antisymmetric part contributes to the 2-form Mik dxi ∧
dxk and only its symmetric part contributes to the quantity Mik dxidxk.
11.8
In rectangular coordinates, use the Levi-Civita identity (1.449) to derive the
curl–curl equations (11.90).
11.9
Derive the Bianchi identity (11.92) from the deﬁnition (11.79) of the Fara-
day ﬁeld-strength tensor, and show that it implies the two homogeneous
Maxwell equations (11.82).
11.10 Show that if A is a p-form, then d(AB) = dA ∧B + (−1)pA ∧dB.
11.11 Show that if ω = aijdxi ∧dxj/2 with aij = −aji, then
dω = 1
3!

∂kaij + ∂iajk + ∂jaki

dxi ∧dxj ∧dxk.
(11.504)
11.12 Using tensor notation throughout, derive (11.147) from (11.145 & 11.146).
475

TENSORS AND LOCAL SYMMETRIES
11.13 Use the ﬂat-space formula (11.168) to compute the change dp due to dρ,
dφ, and dz, and so derive the expressions (11.169) for the orthonormal basis
vectors ˆρ, ˆφ, and ˆz.
11.14 Similarly, derive (11.175) from (11.174).
11.15 Use the deﬁnition (11.191) to show that in ﬂat 3-space, the dual of the Hodge
dual is the identity: ∗∗dxi = dxi and ∗∗(dxi ∧dxk) = dxi ∧dxk.
11.16 Use the deﬁnition of the Hodge star (11.202) to derive (a) two of the four
identities (11.203) and (b) the other two.
11.17 Show that Levi-Civita’s 4-symbol obeys the identity (11.207).
11.18 Show that ϵℓmn ϵpmn = 2 δp
ℓ.
11.19 Show that ϵkℓmn ϵpℓmn = 3! δp
k.
11.20 Using the formulas (11.175) for the basis vectors of spherical coordinates
in terms of those of rectangular coordinates, compute the derivatives of the
unit vectors ˆr, ˆθ, and ˆφ with respect to the variables r, θ, and φ. Your formu-
las should express these derivatives in terms of the basis vectors ˆr, ˆθ, and ˆφ.
(b) Using the formulas of (a), derive the formula (11.297) for the Laplacian
∇· ∇.
11.21 Consider the torus with coordinates θ, φ labeling the arbitrary point
p = (cos φ(R + r sin θ), sin φ(R + r sin θ), r cos θ)
(11.505)
in which R > r. Both θ and φ run from 0 to 2π. (a) Find the basis vectors eθ
and eφ. (b) Find the metric tensor and its inverse.
11.22 For the same torus, (a) ﬁnd the dual vectors eθ and eφ and (b) ﬁnd the
nonzero connections i
jk where i, j, and k take the values θ and φ.
11.23 For the same torus, (a) ﬁnd the two Christoffel matrices θ and φ, (b) ﬁnd
their commutator [θ, φ], and (c) ﬁnd the elements Rθ
θθθ, Rφ
θφθ, Rθ
φθφ, and
Rφ
φφφ of the curvature tensor.
11.24 Find the curvature scalar R of the torus with points (11.505). Hint: in these
four problems, you may imitate the corresponding calculation for the sphere
in section 11.42.
11.25 By differentiating the identity gik gkℓ= δi
ℓ, show that δgik = −gisgktδgst.
11.26 Just to get an idea of the sizes involved in black holes, imagine an isolated
sphere of matter of uniform density ρ that as an initial condition is all at rest
within a radius rb. Its radius will be less than its Schwarzschild radius if
rb < 2MG
c2
= 2
4
3πr3
bρ
 G
c2 .
(11.506)
If the density ρ is that of water under standard conditions (1 gram per cc),
for what range of radii rb might the sphere be or become a black hole? Same
question if ρ is the density of dark energy.
11.27 For the points (11.392), derive the metric (11.395) with k = 1. Don’t forget
to relate dχ to dr.
11.28 For the points (11.393), derive the metric (11.395) with k = 0.
476

EXERCISES
11.29 For the points (11.394), derive the metric (11.395) with k = −1. Don’t forget
to relate dχ to dr.
11.30 Suppose the constant k in the Robertson–Walker metric (11.391 or 11.395)
is some number other than 0 or ±1. Find a coordinate transforma-
tion such that in the new coordinates, the Robertson–Walker metric has
k = k/|k| = ±1.
11.31 Derive the afﬁne connections in equation (11.399).
11.32 Derive the afﬁne connections in equation (11.400).
11.33 Derive the afﬁne connections in equation (11.401).
11.34 Derive the spatial Einstein equation (11.411) from (11.375, 11.395, 11.406,
11.408, & 11.409).
11.35 Assume there had been no inﬂation and that there were no dark energy. In
this case, the magnitude of the difference |−1| would have increased as t2/3
over the past 13.8 billion years. Show explicitly how close to unity  would
have had to have been at t = 1 s so as to satisfy the observational constraint
0 = 1.003 ± 0.010 on the present value of .
11.36 Derive the relation (11.431) between the energy density ρ and the
Robertson–Walker scale factor a(t) from the conservation law (11.427) and
the equation of state p = wρ.
11.37 Use the Friedmann equations (11.410 & 11.412) with ρ = −p and k = 1 to
derive (11.438) subject to the boundary condition that a(t) has its minimum
at t = 0.
11.38 Use the Friedmann equations (11.410 & 11.412) with w = −1 and k = −1
to derive (11.439) subject to the boundary condition that a(0) = 0.
11.39 Use the Friedmann equations (11.410 & 11.412) with w = −1 and k = 0 to
derive (11.440). Show why a linear combination of the two solutions (11.440)
does not work.
11.40 Use the Friedmann equations (11.410 & 11.412) with w = 1/3 and k = 0 to
derive (11.447) subject to the boundary condition that a(0) = 0.
11.41 Show that if the matrix U(x) is nonsingular, then
(∂i U) U−1 = −U ∂i U−1.
(11.507)
11.42 The gauge-ﬁeld matrix is a linear combination Ak = −i g tbAb
k of the gen-
erators tb of a representation of the gauge group. The generators obey the
commutation relations
[ta, tb] = ifabctc,
(11.508)
in which the fabc are the structure constants of the gauge group. Show that
under a gauge transformation (11.474)
A′
i = UAiU−1 −(∂iU) U−1
(11.509)
by the unitary matrix U = exp(−igλata) in which λa is inﬁnitesimal, the
gauge-ﬁeld matrix Ai transforms as
−igA′a
i ta = −igAa
i ta −ig2fabcλaAb
i tc + ig∂iλata.
(11.510)
477

TENSORS AND LOCAL SYMMETRIES
Show further that the gauge ﬁeld transforms as
A′a
i = Aa
i −∂iλa −gfabcAb
i λc.
(11.511)
11.43 Show that if the vectors ea(x) are orthonormal, then ea† · ec,i = −ea†
,i · ec.
11.44 Use the identity of exercise 11.43 to derive the formula (11.489) for the
nonabelian Faraday tensor.
478

12
Forms
12.1 Exterior forms
1-forms
A 1-form is a linear function ω that maps vectors into numbers. Thus,
if A and B are vectors in Rn and z and w are numbers, then
ω(zA + wB) = z ω(A) + w ω(B).
(12.1)
The n coordinates x1, . . . , xn are 1-forms; they map a vector A into its coordi-
nates: x1(A) = A1, . . . , xn(A) = An. Every 1-form may be expanded in terms of
these basic 1-forms as
ω = B1x1 + · · · + Bnxn
(12.2)
so that
ω(A) = B1x1(A) + · · · + Bnxn(A)
= B1A1 + · · · + BnAn
= (B, A) = B · A.
(12.3)
Thus, every 1-form is associated with a (dual) vector, in this case B.
2-forms
A 2-form is a function that maps pairs of vectors into numbers linearly
and skew-symmetrically. Thus, if A, B, and C are vectors in Rn and z and w are
numbers, then
ω2(zA + wB, C) = z ω2(A, C) + w ω2(B, C),
ω2(A, B) = −ω2(B, A).
(12.4)
One often drops the superscript and writes the addition of two 2-forms as
(ω1 + ω2)(A, B) = ω1(A, B) + ω2(A, B).
(12.5)
479

FORMS
Example 12.1 (Parallelogram)
The oriented area of the parallelogram deﬁned
by two 2-vectors A and B is the determinant
ω(A, B) =

A1
A2
B1
B2
 .
(12.6)
This 2-form maps the ordered pair of vectors (A, B) into the oriented area (± the
usual area) of the parallelogram they describe. To check that this 2-form gives
the area to within a sign, rotate the coordinates so that the 2-vector A runs from
the origin along the x-axis. Then A2 = 0, and the 2-form gives A1B2 which is the
base A1 of the parallelogram times its height B2.
Example 12.2 (Parallelepiped)
The triple scalar product of three 3-vectors
ω2
A(B, C) = A · B × C =

A1
A2
A3
B1
B2
B3
C1
C2
C3

= ω3(A, B, C)
(12.7)
is both a 2-form that depends upon the vector A and also a 3-form that maps the
triplet of vectors A, B, C into the signed volume of their parallelepiped.
k-forms
A k-form (or an exterior form of degree k) is a linear function of k
vectors that is antisymmetric. For vectors A1, . . . , Ak and numbers z and w
ω(zA′
1 + wA′′
1, A2, . . . , Ak) = z ω(A′
1, A2, . . . , Ak) + w ω(A′′
1, A2, . . . , Ak) (12.8)
and the interchange of any two vectors makes a minus sign
ω(A2, A1, . . . , Ak) = −ω(A1, A2, . . . , Ak).
(12.9)
Exterior product of two 1-forms
The 1-form ω1 maps the vectors A and B
into the numbers ω1(A) and ω1(B), and the 1-form ω2 does the same thing with
1 →2. The value of the exterior product ω1 ∧ω2 on the two vectors A and B is
the 2-form deﬁned by the 2 × 2 determinant
ω1 ∧ω2(A, B) =

ω1(A)
ω2(A)
ω1(B)
ω2(B)
 = ω1(A)ω2(B) −ω2(A)ω1(B)
(12.10)
or more formally
ω1 ∧ω2 = ω1 ⊗ω2 −ω2 ⊗ω1.
(12.11)
The most general 2-form on Rn is a linear combination of the basic 2-forms
xi ∧xj
ω2 =

1≤i<k≤n
aik xi ∧xk.
(12.12)
480

12.2 DIFFERENTIAL FORMS
If the unit vectors in the n orthogonal directions of Rn are e1, . . . , en, then
xi(ek) = δik and so
ω2(ei, ek) = aik

xi(ei)
xk(ei)
xi(ek)
xk(ek)
 = aik

1
0
0
1
 = aik.
(12.13)
Exterior product of k 1-forms
The exterior product of k 1-forms ω1, . . . , ωk
maps the k n-vectors A1, A2, . . . , Ak to the determinant
ω1 ∧ω2 ∧· · · ∧ωk(A1, A2, . . . , Ak) =

ω1(A1)
. . .
ωk(A1)
...
...
...
ω1(Ak)
. . .
ωk(Ak)

.
(12.14)
The most general k-form on Rn is a linear combination of the various exterior
products of k basic 1-forms xi1 ∧· · · ∧xik
ωk =

1≤i1<...ik≤n
ai1...ik xi1 ∧· · · ∧xik.
(12.15)
Exterior multiplication
The exterior multiplication of a k-form with an ℓ-
form is linear, associative, and antisymmetric
ωk ∧ωℓ= (−1)kℓωℓ∧ωk.
(12.16)
Restriction
of
forms
A
p-form
ωp
is
a
map
from
the
product
V × · · · × V of p copies of some vector space V into the real numbers. The
restriction ωp|U (A1, A2, . . . , Ap) of the p-form ωp to a subspace U ⊂V is the
same p-form ωp but with its domain restricted to vectors Ai ∈U.
12.2 Differential forms
A manifold is a set of points that can be labeled locally by coordinates in Rn in
such a way that the coordinates make sense when the local regions overlap. The
k-dimensional surface Sk of the unit sphere in Rk+1
k+1

i=1
y2
i = 1
(12.17)
is an example of a manifold. A smooth function f (x1, . . . , xn) is one that
is inﬁnitely differentiable with respect to all combinations of its arguments
x1, . . . , xn.
There are two ways of thinking about differential forms. The Russian lit-
erature views a manifold as embedded in Rn and so is somewhat more
straightforward. We will discuss it ﬁrst.
481

FORMS
The Russian way
Suppose x(t) is a curve with x(0) = x on some manifold
M, and f (x(t)) is a smooth function f : Rn →R that maps points x(t) into
numbers. Then the differential df (˙x(t)) maps ˙x(t) at x into
df
 d
dtx(t)

≡d
dtf (x(t)) =
n

j=1
˙x(t)j
∂f (x(t))
∂xj
= ˙x(t) · ∇f (x(t))
(12.18)
all at t = 0. As physicists, we think of df as a number – the change in the
function f (x) when its argument x is changed by dx. Russian mathematicians
think of df as a linear map of tangent vectors ˙x at x into numbers. Since this
map is linear, we may multiply the deﬁnition (12.18) by dt and arrive at the more
familiar formula
dt df
 d
dtx(t)

= df

dt d
dtx(t)

= df (dx(t)) = dx(t) · ∇f (x(t))
(12.19)
all at t = 0. So
df (dx) = dx · ∇f
(12.20)
is the physicist’s df .
Since the differential df is a linear map of vectors ˙x(0) into numbers, it is a
1-form; since it is deﬁned on vectors like ˙x(0), it is a differential 1-form. The term
differential 1-form underscores the fact that the actual value of the differential
df depends upon the vector ˙x(0) and the point x = x(0). Mathematicians call
the space of vectors ˙x(0) at the point x = x(0) the tangent space TMx. They say
df is a smooth map of the tangent bundle TM, which is the union of the tangent
spaces for all points x in the manifold M, to the real line, so df : TM →R.
Consider a curve x(t) on a smooth surface with coordinates xi. In the special
case in which xi(t) = δikt, the differential dxi(˙x(t)) by (12.18) is
dxi(˙x(t)) = d
dtxi(t) = d
dtδikt = δik.
(12.21)
These dxis are the basic differentials. Using A for the vector ˙x(t), we ﬁnd from
our deﬁnition (12.18) that
dxi(A) =
n

j=1
Aj
∂xi
∂xj
=
n

j=1
Ajδij = Ai
(12.22)
as well as
df (A) =
n

j=1
Aj
∂f (x)
∂xj
=
n

j=1
∂f (x)
∂xj
dxj(A)
(12.23)
or
df =
n

j=1
∂f (x)
∂xj
dxj.
(12.24)
482

12.2 DIFFERENTIAL FORMS
Example 12.3 (dr2)
If r2 = x2
1 + x2
2, then the differential 1-form dr2 is
dr2 = 2x1 dx1 + 2x2 dx2.
(12.25)
It takes the 2-vector A into the number
dr2(A) = 2x1 dx1(A) + 2x2 dx2(A) = 2x1 A1 + 2x2 A2.
(12.26)
So if A = (ϵ1, ϵ2), then dr2(A) = 2x1 ϵ1 + 2x2 ϵ2.
The other way
Most American, French, and English mathematicians use a
more abstract approach. They abstract from the basic deﬁnition (12.18) the rule
df
 d
dt

= d
dtf
(12.27)
or more generally
df
 ∂
∂xk

=
∂
∂xk
f .
(12.28)
In particular, if f (x) = xi, then
dxi
 ∂
∂xk

=
∂
∂xk
xi = δik.
(12.29)
In this frequently used notation, the idea is that the derivatives
∂k ≡
∂
∂xk
(12.30)
form a set of orthonormal vectors to which the forms dxi are dual
dxi(∂k) = δik.
(12.31)
In the nonRussian literature, equations (12.27–12.31) deﬁne how the basic
1-forms dxi act on the vectors ∂k.
Change of variables
Suppose that x1, . . . , xn and y1, . . . , yn are two systems
of coordinates on Rn, and that dx1, . . . , dxn and dy1, . . . , dyn are two sets of
basic differentials. Then by applying the formula (12.24) to the function yk(x),
we get
dyk =
n

j=1
∂yk(x)
∂xj
dxj,
(12.32)
which is the familiar rule for changing variables.
483

FORMS
The most general differential 1-form ω on the space Rn with coordinates
x1, . . . , xn is a linear combination of the basic differentials dxi with coefﬁcients
ai(x) that are smooth functions of x = (x1, . . . , xn)
ω = a1(x) dx1 + · · · + an(x) dxn.
(12.33)
The basic differential 2-forms are dxi ∧dxk deﬁned as
dxi ∧dxk(A, B) =

dxi(A)
dxk(A)
dxi(B)
dxk(B)
 =

Ai
Ak
Bi
Bk
 = AiBk −AkBi. (12.34)
So in particular
dxi ∧dxi = 0.
(12.35)
The basic differential k-forms dx1 ∧· · · ∧dxk are deﬁned as
dx1∧· · ·∧dxk(A1, . . . Ak) =

dx1(A1)
. . .
dxk(A1)
...
...
...
dx1(Ak)
. . .
dxk(Ak)

=

A11
. . .
A1k
...
...
...
Ak1
. . .
Akk

.
(12.36)
Example 12.4 (dx3 ∧dr2)
If r2 = x2
1 + x2
2 + x2
3, then dr2 is
dr2 = 2(xdx1 + x2dx2 + x3dx3)
(12.37)
and the differential 2-form ω = dx3 ∧dr2 is
ω = dx3 ∧2(xdx1 + x2dx2 + x3dx3) = 2x1dx3 ∧dx1 + 2x2dx3 ∧dx2
(12.38)
since in view of (12.35) dx3∧dx3 = 0. So the value of the 2-form ω on the vectors
A = (1, 2, 3) and B = (2, 1, 1) at the point x = (3, 0, 3) is
ω(A, B) = 2x1dx3 ∧dx1(A, B) = 6

dx3(A)
dx1(A)
dx3(B)
dx1(B)
 = 6

3
1
1
2
 = 30.
(12.39)
On the vectors, C = (1, 0, 0) and D = (0, 0, 1) at x = (2, 3, 4), this 2-form has the
value ω(C, D) = −4.
The most general differential k-form ωk on the space Rn with coordinates
x1, . . . , xn is
ωk =

1≤i1<...ik≤n
ai1...ik(x) dxi1 ∧· · · ∧dxik,
(12.40)
in which the functions ai1...ik(x) are smooth on Rn.
484

12.2 DIFFERENTIAL FORMS
Example 12.5 (Change of variables)
If x1, x2, x3 and y1, y2, y3 are two coor-
dinate systems on R3, then in terms of the basic 1-forms dyk, the 2-form
ω = Xdx2 ∧dx3 is by (12.32)
ω = Xdx2 ∧dx3 = X
 3

k=1
∂x2
∂yj
dyj

∧
 3

k=1
∂x3
∂yk
dyk

,
(12.41)
in which jacobians appear such as
∂(x2, x3)
∂(y1, y2) = ∂x2
∂y1
∂x3
∂y2
−∂x2
∂y2
∂x3
∂y1
.
(12.42)
In terms of these jacobians, the 2-form ω = Xdx2 ∧dx3 is (exercise 12.2)
ω = X
∂(x2, x3)
∂(y1, y2) dy1 ∧dy2 + ∂(x2, x3)
∂(y2, y3) dy2 ∧dy3 + ∂(x2, x3)
∂(y3, y1) dy3 ∧dy1

.
(12.43)
On the vectors of example 12.4, both forms of ω give ω(A, B) = −X.
Example 12.6 (Euclidean 3-space)
Let us recall the formula (11.157) for the
square ds2 of the length of a vector dx in orthogonal coordinates
ds2 = h2
1 dx2
1 + h2
2 dx2
2 + h2
3 dx2
3
(12.44)
as well as our formula (11.177) for the gradient
∇f = 1
hi
∂f
∂xi
ˆei.
(12.45)
Then in cylindrical coordinates (ρ, φ, z), we have hρ = 1, hφ = ρ, and hz = 1,
while in spherical coordinates (r, θ, φ), we have hr = 1, hθ = r, and hφ = r sin θ.
The value of the form dxi on the unit vector ˆej is by (12.20)
dxk(ˆej) = ˆej · ∇xk = ˆej · 1
hi
∂xk
∂xi
ˆei = 1
hj
∂xk
∂xj
= δkj
hj
.
(12.46)
Thus dρ(ˆeρ) = 1, dφ(ˆeφ) = 1/ρ, and dz(ˆez) = 1.
Example 12.7 (Three-dimensional vectors and their forms)
Any three-
dimensional vector A deﬁnes a 1-form as the dot-product
ω1
A(U) = A · U
(12.47)
and a 2-form as the triple cross-product
ω2
A(U, V) = A · (U × V).
(12.48)
Here we assume that we have a right-handed set of basis vectors ˆe1, ˆe2, and ˆe3
with ˆe1 × ˆe2 = ˆe3 so as to deﬁne the cross-product U × V. Such a manifold is
said to be oriented.
485

FORMS
The quantity
A = A1ˆe1 + A2ˆe2 + A3ˆe3
(12.49)
is a vector ﬁeld A(x). So if we use (12.44) for the squared length ds2, then we can
write the 1-form (12.47) as
ω1
A = A1 h1 dx1 + A2 h2 dx2 + A3 h3 dx3
(12.50)
because by (12.46) and summing over i and k we get
ω1
A(U) = Ai hi dxi

Ukˆek

= Ai hi Uk δik/hk = AiUi = A · U.
(12.51)
Similarly (exercise 12.6), the 2-form (12.48) is
ω2
A = A1 h2h3 dx2 ∧dx3 + A2 h3h1 dx3 ∧dx1 + A3 h1h2 dx1 ∧dx2.
(12.52)
By analogy with the deﬁnition (12.50), the gradient 1-form ω1
∇f is
ω1
∇f = (∇f )k hk dxk
(12.53)
summed over repeated indices. The relation ω1
∇f = df gives
ω1
∇f = (∇f )k hk dxk = df = ∂f
∂xk
dxk
(12.54)
according to the deﬁnition (12.24) of df . So the vector ﬁeld ∇f is
∇f =
3

k=1
1
hk
∂f
∂xk
ˆek,
(12.55)
which in cylindrical and spherical coordinates is
∇f = ∂f
∂ρ ˆeρ + 1
ρ
∂f
∂φ ˆeφ + ∂f
∂z ˆez = ∂f
∂r ˆer + 1
r
∂f
∂θ ˆeθ +
1
r sin θ
∂f
∂φ ˆeφ
(12.56)
in agreement with (11.180–11.181).
12.3 Exterior differentiation
Exterior differentiation is nifty. The differential (12.24)
df =
n

k=1
∂f
∂xk
dxk
(12.57)
is the exterior derivative of the function f (x), itself a 0-form. The operator
d =
n

k=1
∂
∂xk
dxk
(12.58)
turns the 0-form f into the differential 1-form df .
486

12.3 EXTERIOR DIFFERENTIATION
Applied to the 1-form
ω1 =
n

i=1
ai(x) dxi
(12.59)
the exterior derivative d generates the 2-form
dω1 = d
 n

i=1
ai(x) dxi

=
n

i,k=1
ai,k(x) dxk ∧dxi,
(12.60)
in which ai,k = ∂kai. But a second application of d gives zero:
ddω1 = dd
 n

i=1
ai dxi

= d
⎛
⎝
n

i,k=1
ai,k dxk ∧dxi
⎞
⎠
=
n

i,k,ℓ=1
ai,kℓdxℓ∧dxk ∧dxi = 0
(12.61)
because the double partial derivative ai,kℓ= ∂ℓ∂kai is symmetric in k and ℓwhile
the wedge product dxℓ∧dxk is antisymmetric in these indices.
We have seen (12.40) that the most general differential k-form is
ωk =

1≤i1<···ik≤n
ai1...ik(x) dxi1 ∧· · · ∧dxik
(12.62)
in which the functions ai1...ik(x) are smooth on Rn. The exterior derivative
operator d turns ωk into the (k + 1)-form
dωk =

1≤i1<···ik≤n
d

ai1...ik(x) dxi1 ∧· · · ∧dxik

=

1≤ℓ,i1<···ik≤n
ai1...ik,ℓ(x) dxℓ∧dxi1 ∧· · · ∧dxik.
(12.63)
Once again d d ωk = 0 so quite generally
d d = 0.
(12.64)
If ω is the wedge product of two 1-forms ωa = ai dxi and ωb = bk dxk
ω = ωa ∧ωb = ai dxi ∧bk dxk
(12.65)
487

FORMS
then d maps it to
d ω = d (ai dxi ∧bk dxk) = d (ai bk dxi ∧dxk)
= (ai bk),ℓdxℓ∧dxi ∧dxk
= (ai,ℓbk + ai bk,ℓ) dxℓ∧dxi ∧dxk
= ai,ℓdxℓ∧dxi ∧bk dxk + ai bk,ℓdxℓ∧dxi ∧dxk
= (ai,ℓdxℓ∧dxi) ∧bk dxk −ai dxi ∧(bk,ℓdxℓ∧dxk)
= d ωa ∧ωb −ωa ∧d ωb.
(12.66)
More generally, the exterior derivative operator d maps the wedge product of a
k-form ωk and a p-form ωp to
d

ωk ∧ωp
=

dωk
∧ωp + (−1)kωk ∧

dωp
.
(12.67)
Example 12.8 (Phase space)
If ω1 is the 1-form
ω1 = p1dq1 + · · · + pndqn = p · dq
(12.68)
with coordinates p1, . . . , pn, q1, . . . , qn, then d ω1 is the 2-form
d ω1 = d
 n

i=1
pi dqi

=
n

i,k=1
(∂pkpi) dpk ∧dqi =
n

i,k=1
δik dpk ∧dqi
= dp1 ∧dq1 + · · · + dpn ∧dqn = dp ∧dq.
(12.69)
It follows that d (dp ∧dq) = d d ω1 = 0.
Example 12.9 (A Poincaré invariant)
The 1-form ω1 = p · dq maps a tiny piece
∂sq ds of a phase-space trajectory into a small element of action p · dq(∂sq ds) =
p · ∂sq ds. The sum of these pieces along a closed trajectory
A =

∂S
ω1 =
)
p · dq =
)
n

i=1
pi dqi =
)
p · ∂q
∂s ds
(12.70)
is a Poincaré invariant. Because the trajectory is a loop, we may integrate the
second term in its time derivative by parts
˙A =
) 
˙p · ∂q
∂s + p · ∂2q
∂s∂t

ds =
) 
˙p · ∂q
∂s −∂p
∂s · ˙q

ds
(12.71)
without acquiring an extra term. The trajectory is physical, and so we can use
Hamilton’s equations
˙pi = −∂H
∂qi
and
˙qi = ∂H
∂pi
for
i = 1, . . . , n
(12.72)
488

12.3 EXTERIOR DIFFERENTIATION
to write ˙A as
˙A =
) 
−∂H
∂q · ∂q
∂s −∂p
∂s · ∂H
∂p

ds = −
) ∂H
∂s ds = −
)
dH = 0
(12.73)
because the trajectory is closed.
Example 12.10 (The Bohr model)
In 1912, Bohr considered an electron in a cir-
cular orbit around a proton, set Poincaré’s invariant equal to an integral multiple
of Planck’s constant
A =
)
pdq = 2πrp = nh
(12.74)
and so quantized the orbital angular momentum as L = rp = n¯h. One can derive
the energy levels of the hydrogen atom from this rule (exercise 12.12). In 1924,
Arnold Sommerfeld applied this trick to a more general orbit of a relativistic
electron about a proton and got the energy levels that Dirac would four years
later.
Example 12.11 (Constant area)
Consider three nearby points in phase space
(p, q), (p + δp, q + δq), and (p + p, q + q) that move according to Hamilton’s
equations (12.72). The time derivatives of the tiny displacements δpi and δqi are
(exercise 12.9)
d
dtδpi = δ˙pi =
n

k=1
−∂2H
∂qi∂qk
δqk −
∂2H
∂qi∂pk
δpk
d
dtδqi = δ˙qi =
n

k=1
∂2H
∂pi∂qk
δqk +
∂2H
∂pi∂pk
δpk.
(12.75)
Similar equations give the derivatives of the small differences pi and qi.
The 2-form (12.69) maps the n pairs of 2-vectors (δp, δq) and (p, q) into a
sum of areas of parallelograms
dω1(δp, δq; p, q) =

δp1
δq1
p1
q1
 + · · · +

δpn
δqn
pn
qn
 .
(12.76)
By using the time derivatives (12.75) of δpi and δqi and those of pi and qi,
one may show (exercise 12.10) that this sum of areas remains constant
d
dt dω1(δp, δq; p, q) = 0
(12.77)
along the trajectories in phase space (Gutzwiller, 1990, chap. 7).
Example 12.12 (The curl)
We saw in example 12.7 that the 1-form (12.50) of a
vector ﬁeld A is ωA = A1 h1 dx1+A2 h2 dx2+A3 h3 dx3 in which the hks are those
that determine (12.44) the squared length ds2 = h2
k dx2
k of the triply orthogonal
coordinate system with unit vectors ˆe1, ˆe2, ˆe3. So the exterior derivative of the
1-form ωA is
489

FORMS
dωA =
3

i,k=1
∂k(Ai hi) dxk ∧dxi
=
∂(A3 h3)
∂x2
−∂(A2 h2)
∂3

dx2 ∧dx3
+
∂(A2 h2)
∂x1
−∂(A1 h1)
∂x2

dx1 ∧dx2
+
∂A1 h1
∂x3
−∂(A3 h3)
∂x1

dx3 ∧dx1 ≡ω∇×A.
(12.78)
Comparison with equation (12.52) shows that the curl of A is
∇× A =
1
h2 h3
∂A3 h3
∂x2
−∂A2 h2
∂3

dx2 ∧dx3 ˆe1 + · · ·
=
1
h1 h2 h3

h1ˆe1
h2ˆe2
h3ˆe3
∂1
∂2
∂3
A1h1
A2h2
A3h3

=
1
h1h2h3
3

i,j,k=1
ϵijkhi ˆei
∂(Akhk)
∂xj
(12.79)
as we saw in (11.240). This formula gives our earlier expressions for the curl in
cylindrical and spherical coordinates (11.241 & 11.242).
Example 12.13 (The divergence)
We have seen in equations (12.48, 12.49, &
12.52) that the 2-form ωA(U, V) = A · (U × V) of the vector ﬁeld A = A1ˆe1 +
A2ˆe2 + A3ˆe3 is
ω2
A = A1 h2 h3 dx2 ∧dx3 + A2 h3 h1 dx3 ∧dx1 + A3 h1 h2 dx1 ∧dx2.
(12.80)
The exterior derivative of this 2-form is
d ωA =
3

k=1
∂
∂xk
ωA
= ∂A1 h2h3
∂x1
dx1 ∧dx2 ∧dx3 + ∂A2 h3 h1
∂x2
dx2 ∧dx3 ∧dx1
+ ∂A3 h1 h2
∂x3
dx3 ∧dx1 ∧dx2
=
 3

k=1
∂(Ak h1 h2 h3/hk)
∂xk

dx1 ∧dx2 ∧dx3.
(12.81)
If one deﬁnes the divergence ∇· A as
dωA = (∇· A) h1 h2 h3 dx1 ∧dx2 ∧dx3
(12.82)
490

12.4 INTEGRATION OF FORMS
then ∇· A must be
∇· A =
1
h1 h2 h3
 3

k=1
∂(Ak h1 h2 h3/hk)
∂xk

(12.83)
in agreement with (11.283) from which the speciﬁc formulas for cylindrical
(11.284) and spherical (11.286) coordinates follow.
Example 12.14 (The divergence of a gradient)
By combining our expression
(12.83) for the divergence with our formula (12.55) for the gradient of a function
f , we ﬁnd that its Laplacian △f in orthogonal coordinates is
△f (x) ≡∇· ∇f (x) =
1
h1 h2 h3
 3

k=1
∂
∂xk

h1 h2 h3
h2
k
∂f (x)
∂xk

,
(12.84)
which agrees with (11.295) and so yields (11.296) for cylindrical coordinates and
(11.297) for spherical ones.
12.4 Integration of forms
Let’s follow the Russian approach at ﬁrst. Let γ (t) be a smooth map from the
unit interval [0, 1] into some manifold M ⊂Rn. We divide this interval into tiny
segments [ti, ti+1] of length dt = ti+1 −ti, which γ maps into vectors dγ (dti) =
˙γ (ti) dt that are tangent to the manifold at the point γ (ti). The integral of a
1-form ω along the curve γ is then the usual Riemann sum

γ
ω = lim
dt→0

i
ω( ˙γ (ti)) dt.
(12.85)
If for example, the 1-form is ωA(U) = A · U, then

γ
ω =

γ
ωA( ˙γ (ti)) dt =

A · ˙γ (ti) dt =

A · dγ .
(12.86)
And if ω = ak(x) dxk, then since dt dxk( ˙γ ) = dγk, the integral

γ
ω = lim
dt→0
n

i=1
ω( ˙γ (ti) dt) = lim
dt→0
n

i=1
ak(x) dxk( ˙γ (ti)) dt =

ak(x) dγk
(12.87)
is a line integral on the manifold

γ
ω =

ak dxk =

ak dγk.
(12.88)
Suppose now that our 1-form ω is exact, that is, that
ω = dα = α,k dxk
(12.89)
491

FORMS
where α(x) is a 0-form, that is a function, deﬁned on the manifold. Then by
(12.88) the integral of dα is

γ
dα =

α,k dxk = α(γ (1)) −α(γ (0)).
(12.90)
The signed endpoints γ (1) and −γ (0) are the boundary of the curve γ (t), which
one writes as ∂γ . In this notation, we have

γ
dα =

∂γ
α.
(12.91)
Example 12.15 (Green’s theorem)
Let ξ and η be two inﬁnitesimal vectors that
form a parallelogram  in the tangent space. We will compute the line integral
of the 1-form ω = a1(x1, x2) dx1 + a2(x1, x2) dx2 around the boundary ∂ this
parallelogram. This boundary ∂ is a chain of four maps t →tξ, t →ξ + tη,
t →η + tξ, and t →tη of the unit interval 0 ≤t ≤1 into the plane deﬁned
by the two vectors ξ and η. We assign multiplicities 1, 1, −1, and −1 to these
four maps, that is, the full chain runs from a point that we’ll call the origin to
the point ξ, then from ξ to ξ + η, and then from ξ + η to η, and then from
η back to the origin. On the curve γ (t) = tξ, the differentials dxk map ˙γ (t) dt
into dxk( ˙γ (t) dt) = dxk(ξ dt) = ξk dt. Similarly, on the curve γ (t) = tη, we have
dxk( ˙γ (t) dt) = ηk dt. So summing over k = 1, 2, we ﬁnd

∂ 
ω =
 1
0
{[ak(tξ) −ak(tξ + η)] ξk −[ak(tη) −ak(tη + ξ)] ηk} dt.
(12.92)
Since the tangent vectors ξ and η are inﬁnitesimal, the square brackets are
ak(tξ) −ak(tξ + η) = −ηj
∂ak
∂xj
,
ak(tη) −ak(tη + ξ) = −ξj
∂ak
∂xj
(12.93)
and so we have

∂ 
ω =
 1
0

−ηj
∂ak
∂xj
ξk + ξj
∂ak
∂xj
ηk

dt = ∂ak
∂xj

ξj ηk −ξk ηj

.
(12.94)
But the exterior derivative of ω is
d ω = d (ak dxk) = ∂ak
∂xj
dxj ∧dxk
(12.95)
so the last term in (12.94) is just the 2-form d ω applied to the tangent vectors ξ
and η
d ω(ξ, η) = ∂ak
∂xj
dxj ∧dxk(ξ, η) = ∂ak
∂xj

ξjηk −ξkηj

.
(12.96)
492

12.4 INTEGRATION OF FORMS
And dxj ∧dxk(ξ, η) = ξjηk −ξkηj is the area of the tiny parallelogram  . So this
last expression (12.96) is dω integrated over the tiny parallelogram  formed by
the tangent vectors ξ and η, and we have

 
dω =

∂ 
ω
(12.97)
for an inﬁnitesimal parallelogram.
It is easy to extend this identity to an arbitrary surface S of ﬁnite extent. To do
this, we tile the surface S with inﬁnitesimal parallelograms  α with boundaries
∂ α. The integral over the ﬁnite surface S is then the sum of the integrals over
the  α that tile S

S
dω =

α

 α
dω,
(12.98)
which by (12.96) is a sum of integrals over the boundaries ∂ α

S
dω =

α

 α
dω =

α

∂ α
ω.
(12.99)
In the sum of the integrals over the boundaries ∂ α, the internal boundaries all
cancel, leaving us with the integral over the boundary ∂S of the surface S. Thus
we have

S
dω =

α

 α
dω =

α

∂ α
ω =

∂S
ω
(12.100)
or more simply

S
dω =

∂S
ω,
(12.101)
which generalizes the identity (12.91) from 0-forms to 1-forms.
In the notation of ordinary vector calculus, this relation is

S
(∇× A) · dS =
)
∂S
A · dx
(12.102)
in accord (exercise 12.13) with the curl formulas (12.78–12.79).
Example 12.16 (How electric motors and generators work)
Since by (11.80) the
curl of the vector potential A is the magnetic ﬁeld B, this last identity (12.102)
implies that the magnetic ﬂux  through a surface S is the line integral of the
vector potential A around the edge of the surface
 =

S
B · dS =

S
(∇× A) · dS =
)
∂S
A · dx.
(12.103)
If we take the time derivative of this relation and remember (11.78) that the time
derivative of the vector potential is ˙A = −E −∇φ, then we ﬁnd that the rate of
change of the magnetic ﬂux through a surface is the negative of the line integral
of the electric ﬁeld along the boundary of the surface
493

FORMS
˙ =

S
˙B · dS =

S
(∇× ˙A) · dS = −
)
∂S
E · dx
(12.104)
or minus the voltage (−∇φ drops out because its curl vanishes).
Example 12.17 (Stokes’s theorem)
Suppose ξ, η, and ζ form a triplet of
inﬁnitesimal vectors oriented so as to form a right-handed coordinate system,
ξ × η · ζ > 0. These vectors form a tiny parallelepiped  . We want to integrate
the 2-form ω = ajkdxj ∧dxk over the surface ∂ of this tiny parallelepiped  .
We ﬁnd

∂ 
ω =
 1
0
dt
 1
0
ds

ajk(tξ + sη + ζ) −ajk(tξ + sη

dxj ∧dxk(ξ, η)
+
 1
0
dt
 1
0
ds

ajk(tη + sζ + ξ) −ajk(tη + sζ

dxj ∧dxk(η, ζ)
+
 1
0
dt
 1
0
ds

ajk(tζ + sξ + η) −ajk(tζ + sξ

dxj ∧dxk(ζ, ξ)
= ajk,ℓζℓ

ξjηk −ηjξk

+ ajk,ℓξℓ

ηjζk −ζjηk

+ ajk,ℓηℓ

ζjξk −ξjζk

= ajk,ℓdxℓ∧dxj ∧dxk(ξ, η, ζ) = dω(ξ, η, ζ) =

 
dω
(12.105)
for an inﬁnitesimal parallelepiped  .
It is easy to extend this identity to an arbitrary volume V of ﬁnite extent. To do
this, we tile the volume V with inﬁnitesimal parallelepipeds  α with boundaries
∂ α. The integral over the ﬁnite volume V is then the sum of the integrals over
the  α

V
dω =

α

 α
dω,
(12.106)
which by (12.105) is a sum of the integrals over the boundaries ∂ α

V
dω =

α

 α
dω =

α

∂ 
ω.
(12.107)
In this sum over surface integrals, the internal boundaries all cancel, leaving us
with the integral over the boundary ∂V of the volume V, so that

V
dω =

α

 α
dω =

α

∂ α
ω =

∂V
ω.
(12.108)
Thus we have

V
dω =

∂V
ω,
(12.109)
which generalizes the identity (12.91) from 1-forms to 2-forms.
Before leaving this example, it may be instructive to examine the value of the
integral of ω on one of the faces of the inﬁnitesimal parallelepiped  as well as
that of the integral of dω on the inﬁnitesimal parallelepiped  . The 2-form ω on
494

12.4 INTEGRATION OF FORMS
the upper ξ, η face of the surface ∂ of  may be seen from equation (12.105)
to be
ω(ξ, η) = ajk dxj ∧dxk(ξ, η) = ajk

ξjηk −ηjξk

.
(12.110)
The wedge dxj ∧dxk(ξ, η) deﬁnes an area vector S = ξ × η with components
Si = ϵijkξjηk. In terms of S, the 2-form ω is
ω(ξ, η) = (a23 −a32) S1 + (a31 −a13) S2 + (a12 −a21) S3,
(12.111)
which suggests deﬁning the vector ﬁeld Ak = ϵkijaij. In terms of S and A, the
2-form ω on ξ, η is ω(ξ, η) = A · S.
The integral of the 3-form dω on the inﬁnitesimal parallelepiped  is

 
dω = dω(ξ, η, ζ) = ajk,ℓdxℓ∧dxj ∧dxk(ξ, η, ζ).
(12.112)
The wedge product is ϵℓjk times the determinant det(ξ, η, ζ) of the 3 × 3 matrix
that has ξ as its ﬁrst row, η as its second, and ζ as its third row
dxℓ∧dxj ∧dxk(ξ, η, ζ) = ϵℓjk det(ξ, η, ζ).
(12.113)
So dω(ξ, η, ζ) = ajk,ℓϵℓjk det(ξ, η, ζ) or more explicitly
dω(ξ, η, ζ) =

(a23 −a32),1 + (a31 −a13),2 + (a12 −a21),3

det(ξ, η, ζ),
which we recognize as the divergence of the vector ﬁeld Ak = ϵijkaij
dω(ξ, η, ζ) = ∇· A det(ξ, η, ζ).
(12.114)
Thus we have rediscovered the vector identity

V
∇· A dV =
)
∂V
A · dS.
(12.115)
Example 12.18 (Gauss’s law)
The divergence of the electric displacement D is
the density ρf of free charge, ∇· D = ρf, and so this last identity (12.115) gives
QfV =

V
ρf dV =

V
∇· D dV =
)
∂V
D · dS,
(12.116)
which is the integral form of Gauss’s law.
One may generalize these examples to what has been called the Newton–
Leibniz–Gauss–Green–Ostrogradskii–Stokes–Poincaré theorem

∂C
ω =

C
dω,
(12.117)
in which ω is a k-form and C is any (k + 1)-chain on a manifold.
Example 12.19 (Poincaré’s invariant action)
In example 12.9, we saw that
Poincaré’s action
A =

∂S
ω1 =
)
p · dq =
)
n

i=1
pi ∧dqi
(12.118)
495

FORMS
does not change with time, ˙A = 0. In example 12.11, we learned that the element
of area dω1 and therefore its surface integral
I =

S
dω1
(12.119)
does not change with time. The identity (12.117) in the form
A =

∂S
ω1 =

S
dω1 = I
(12.120)
relates these two examples.
12.5 Are closed forms exact?
A form ω is said to be closed if its exterior derivative vanishes
dω = 0.
(12.121)
A form ω is exact if it’s the exterior derivative of another form ψ
ω = dψ.
(12.122)
We have seen in (12.64) that the exterior derivative of any exterior derivative
vanishes; in effect, dd = 0. Thus the exterior derivative of any exact form ω
must be zero
dω = ddψ = 0.
(12.123)
So every exact form is closed.
But are closed forms exact? Poincaré’s lemma provides the answer. A form
that is deﬁned and closed on a simply connected part of a manifold is exact there.
More technically, if a form ω is deﬁned and closed on a region U of a manifold
M and if U can be mapped by a one-to-one differentiable map onto the interior
of the unit ball in Rn, then there is a form ψ such that dψ = ω on U. The unit
ball in Rn is the interior of the sphere Sn−1 deﬁned by x2
1 + · · · + x2
n = 1. You
may ﬁnd a proof of this result in section 4.19 of Schutz’s book (Schutz, 1980).
Example 12.20 (Two dimensions)
Suppose that the 1-form ω = fdx + gdy is
closed
dω = f,y dy ∧dx + g,x dx ∧dy =

g,x −f,y

dx ∧dy = 0
(12.124)
on the real plane R2. Since R2 is simply connected, Poincaré’s lemma tells us that
there exists a 0-form h whose exterior derivative is ω
ω = fdx + gdy = dh = h,xdx + h,ydy.
(12.125)
496

12.5 ARE CLOSED FORMS EXACT?
We may construct such a function h(x, y) as the line integral
h(x, y) =
 1
0
[f (u(t), v(t)) ˙u(t) + g(u(t), v(t)) ˙v(t)] dt
(12.126)
along any differentiable curve γ (t) = (u(t), v(t)) that goes from (u(0), v(0)) =
(x0, y0) to (u(1), v(1)) = (x, y). Clearly the exterior derivative of this 0-form is
dh = h,xdx + h,ydy = f (x, y) dx + g(x, y) dy = ω.
(12.127)
So the real issue here is whether the line integral (12.126)
h =

γ
ω
(12.128)
deﬁnes a function h(x, y) that is the same for any two curves γ1(t) and γ2(t) that
both go from (x0, y0) to (x, y). The difference h1(x, y) −h2(x, y) is an integral of
ω along a closed curve  = γ1 −γ2 that runs from (x0, y0) to (x, y) along γ1(t)
and then from (x, y) to (x0, y0) backwards along γ2(t). The closed curve  is the
boundary ∂S of the (plane) surface S that it encloses. Thus by Stokes’s theorem
(12.117)
h1 −h2 =


ω =

∂S
ω =

S
dω = 0,
(12.129)
in which we used the fact that ω is closed so that dω = 0. The two curves γ1(t)
and γ2(t) deﬁne the same function h(x, y), and ω = dh is exact.
What if the region in which dω = 0 is not simply connected? Consider for
example the 1-form
ω = −
y
x2 + y2 dx +
x
x2 + y2 dy,
(12.130)
which is well deﬁned except at the origin. The plane R2 minus the origin is not
simply connected. One may check that ω is closed
dω = 0
(12.131)
except at the origin. But ω is not exact. In fact, by writing it as
ω = arctan(y/x),y dy ∧dx + arctan(y/x),x dx ∧dy
(12.132)
we see that ω is almost exact. It is the exterior derivative of
θ = arctan(y/x),
(12.133)
which would be a 0-form if it were single valued.
Example 12.21 (Some exact forms)
It’s easy to make lots of exact forms; one
just applies the exterior derivative d to any form. For instance, taking the exterior
derivative of the 0-form ω0 = x y2 exp(zw), we get the 1-form
dω0 = y2 ezw dx + 2x y ezw dy + x y2 w ezw dz + x y2 z ezw dw,
(12.134)
497

FORMS
which is exact (and closed). Applying d to the 1-form ω1 = y2zdx + x3dy, we get
the exact 2-form dω1 = (3x2 −2yz) dx∧dy−y2dx∧dz. Incidentally, any n-form
in n variables, such as f (x, y, z) dx ∧dy ∧dz, is closed.
12.6 Complex differential forms
Any function f (x, y) of two real variables also is a function of the two complex
variables z = x + iy and ¯z = x −iy. For instance, 4xy = −i(z + ¯z)(z −¯z) and
x2 +y2 = z¯z. We can write any 1-form ω = adx+bdy with complex coefﬁcients
a and b in terms of the complex differentials dz = dx+idy and d¯z = dx−idy as
ω = 1
2(a −ib)dz + 1
2(a + ib) d¯z.
(12.135)
A 1-form of the variables z1, . . . , zn and ¯z1, . . . , ¯zn is a sum of their differentials
ω = ajdzj + bjd¯zj. The expression
ω1,1 = a dz1 ∧d¯z1 + b dz1 ∧d¯z2 + c dz2 ∧d¯z1 + d dz2 ∧d¯z2
(12.136)
is a 1,1-form in z1 and z2, while 2,0- and 0,2-forms look like
ω2,0 = e dz1 ∧dz2
and
ω0,2 = f d¯z1 ∧d¯z2.
(12.137)
The complex differentials anticommute: d¯zj ∧dzk = −dzk ∧d¯zj as well as dzj ∧
dzk = −dzk ∧dzj and d¯zj ∧d¯zk = −d¯zk ∧d¯zj.
There are two exterior derivatives ∂and ¯∂deﬁned by
∂=
n

j=1
∂
∂zj
dzj ∧
and
¯∂=
n

j=1
∂
∂¯zj
d¯zj ∧.
(12.138)
Their sum is the ordinary exterior derivative ∂+ ¯∂= d, and one has
∂2 = ¯∂2 = ∂¯∂+ ¯∂∂= 0.
(12.139)
Example 12.22 (∂+ ¯∂= d)
We illustrate the rule ∂+ ¯∂= d for the 1-form
ω = z¯zdz = (x2 + y2)(dx + idy). The sum ∂+ ¯∂acting on ω gives

∂+ ¯∂

z¯z dz = ¯∂z¯z d¯z ∧dz = z d¯z ∧dz
= (x + iy)(dx −idy) ∧(dx + idy) = 2i(x + iy)dx ∧dy
(12.140)
while dω is
d(x2 + y2)(dx + idy) = 2xidx ∧dy + 2ydy ∧dx = 2i(x + iy)dx ∧dy, (12.141)
which is the same as

∂+ ¯∂

ω.
498

12.7 FROBENIUS’S THEOREM
12.7 Frobenius’s theorem
This section, optional on a ﬁrst reading, begins with some deﬁnitions.
• If ω is a k-form that maps all vectors V of the tangent space TP at the point P
into numbers ω(V1, . . . , Vk), and S ⊂TP is a subspace of the tangent space
TP, then the restriction ωS of the form ω to S maps the vectors Si ∈S into
the numbers ω(S1, . . . , Sk).
• The annihilator (actually the annihilated) of a set of forms βi at a point P
of a manifold is the subspace of vectors XP that every βi maps to zero. (The
vectors XP are in the tangent space TP.)
• The complete ideal of a set B of forms βi is the set of all forms at P
whose restriction to B’s annihilator XP vanishes. Note that for any form
α and any vectors Xℓin the annihilator XP, the wedge product α ∧
βi(X1, . . . , Xn) vanishes, so α ∧βi is in the complete ideal of the set B of
forms βi.
• The complete ideal of a set B of forms βi has a set αi of linearly independent
1-forms that generates it, that is, whose complete ideal is the same as that of
the set B.
• The complete ideal of a set B of ﬁelds βi is the set of ﬁelds that map the
annihilator XP of B to zero at each point P of the manifold.
• If the exterior derivative dαi is in an ideal whenever αi is, then the ideal is a
differential ideal.
• A set A of 1-forms αi has a closed ideal if every dai is in the complete ideal
generated by the αis. (Some authors call such a set A of 1-forms closed, but
this terminology can be confusing.)
Example 12.23 (A rank-2 annihilator)
Consider a manifold with coordi-
nates x1, . . . , xn, tangent vectors ∂1, . . . , ∂n, and 1-forms dx1, . . . , dxn with
dxi(∂k) = δik as in (12.31). The subspace {c1∂1 + c2∂2 | c1, c2 real} is the anni-
hilator of the set {dx3, . . . , dxn} of 1-forms. Any linear combination of these
1-forms
ω =
n

k=3
ak(x) dxk
(12.142)
is in the complete ideal of the 1-forms dx3, . . . , dxn. So is any 2-form
ω2 =
n

i,k=3
aik(x) dxi ∧dxk.
(12.143)
499

FORMS
For any forms αk the linear combination
ω =
n

k=3
αk ∧dxk
(12.144)
is in the complete ideal of the set of 1-forms dx1, . . . , dxn. In fact, each member
of this complete ideal is such a linear combination.
Frobenius’s theorem
Let ω1, . . . , ωn be a linearly independent set of 1-form
ﬁelds in an open region U of a k-dimensional manifold M. Then there exist
functions Pℓj and Qj for i, j = 1, . . . , n that express the n 1-forms ωℓas
ωℓ=
n

j=1
PℓjdQj
(12.145)
if and only if every dωℓis in the complete ideal generated by the ωℓ’s (Schutz,
1980, secs. 3.8 & 4.26).
Further reading
Three good discussions of differential forms are Mathematical Methods of
Classical Mechanics (Arnold, 1989), Geometrical Methods of Mathematical
Physics (Schutz, 1980), and Classical Mechanics (Matzner and Shepley, 1991).
They inspired this chapter.
Exercises
12.1
Why do you think mathematicians use the deﬁnition (12.18) rather than our
(12.19)?
12.2
Show explicitly that the 2-form ω = Xdx2 ∧dx3 is given by (12.43) in terms
of the 1-forms dyk.
12.3
Show explicitly that for the two 3-vectors of example 12.4, the 2-form
ω(A, B) = Xdx2 ∧dx3(A, B) = −X.
12.4
In example 12.5, let x1 = y1 + y2, x2 = y1 −y2, and x3 = y1 −y3. Show
explicitly that the y-version (12.43) of the 2-form ω(A, B) = Xdx2 ∧dx3
maps the two 3-vectors of example 12.4 into the same real number −X as its
x-version. Hint: in the y-version, ﬁrst express dyk(A) and dyk(B) in terms of
dxj(A) and dxj(B).
12.5
Compute dr(ˆer), dθ(ˆeθ), and dφ(ˆeφ) as well as the six off-diagonal ones
dr(ˆeθ), dr(ˆeφ), and so forth.
12.6
Show that the 2-form (12.52) applied to the vectors U and V gives the triple
scalar product (12.48).
12.7
Show that d d ωk = 0 for the general k-form (12.62).
12.8
Show that if α and β are both 2-forms, then d(α ∧β) = (dα) ∧β + α ∧dβ.
500

EXERCISES
12.9
Use Hamilton’s equations (12.72) to derive the formula (12.75) for the time
derivatives δ˙pi and δ˙qi.
12.10 Use Hamilton’s equations (12.72) to compute the time derivatives of the
n pairs of tiny displacements pj, qj. Then use your resulting formulas
and those (12.75) for the time derivatives of the n pairs of small differences
δpj, δqj to show that the time derivative of the sum (12.76) of areas of tiny
parallelograms vanishes (12.77).
12.11 In the early days of quantum mechanics, Bohr and Sommerfeld set action
integrals like those of example 12.9 equal to a multiple of Planck’s constant,
A =
6
p · dq = nh. Why do you think they chose invariant quantities to
quantize in this way?
12.12 Use Bohr’s quantization of angular momentum L = rp = n¯h to ﬁnd the
energy levels of an electron in a circular orbit about a proton. Take the
energy as E = p2/2m −Ze2/4πϵ0r and balance radial forces mv2/r =
Ze2/4πϵ0r2 where p = mv.
12.13 Work out the details of using the curl formulas (12.78–12.79) to derive
the curl formula (12.102) from the general identity (12.101). Do this in
rectangular, cylindrical, and spherical coordinates.
12.14 Is the 1-form y2 ezw dx + 2x y ezw dy + x y2 w ezw dz + x y2 z ezw dw closed?
Why? Why not?
12.15 Is the 1-form zey/xw + zey ln x/w + ey ln x/w −zey ln x/w2 closed? Why?
Why not?
12.16 Show that ∂and ¯∂satisfy (12.139).
501

13
Probability and statistics
13.1 Probability and Thomas Bayes
The probability P(A) of an outcome in a set A is the sum of the probabilities Pj
of all the different (mutually exclusive) outcomes j in A
P(A) =

j∈A
Pj.
(13.1)
For instance, if one throws two fair dice, then the probability that the sum is 2 is
P(1, 1) = 1/36, while the probability that the sum is 3 is P(1, 2)+P(2, 1) = 1/18.
If A and B are two sets of possible outcomes, then the probability of an out-
come in the union A ∪B is the sum of the probabilities P(A) and P(B) minus
that of their intersection A ∩B
P(A ∪B) = P(A) + P(B) −P(A ∩B).
(13.2)
If the outcomes are mutually exclusive, then P(A ∩B) = 0, and the probability
of the union is the sum P(A∪B) = P(A)+P(B). The joint probability P(A, B) ≡
P(A ∩B) is the probability of an outcome that is in both sets A and B. If the
joint probability is the product P(A, B) = P(A) P(B), then the outcomes in sets
A and B are statistically independent.
The probability that a result in set B also is in set A is the conditional
probability P(A|B), the probability of A given B
P(A|B) = P(A ∩B)
P(B)
.
(13.3)
502

13.1 PROBABILITY AND THOMAS BAYES
Also P(B|A) = P(A ∩B)/P(A). The substitution B →B ∩C in (13.3) gives
P(A|B, C) = P(A ∩B ∩C)/P(B ∩C). If we multiply (13.3) by P(B), we get
P(A, B) = P(A ∩B) = P(B|A) P(A) = P(A|B) P(B).
(13.4)
Combination of (13.3 & 13.4) gives Bayes’s theorem (Riley et al., 2006, p. 1132)
P(A|B) = P(B|A) P(A)
P(B)
(13.5)
(Thomas Bayes, 1702–1761).
If there are N mutually exclusive theories, causes, or ways Aj that B can
happen, then we must sum over them
P(B) =
N

j=1
P(B|Aj) P(Aj).
(13.6)
The probabilities P(Aj) are called a priori probabilities. In this case, Bayes’s
theorem is (Roe, 2001, p. 119)
P(Ak|B) =
P(B|Ak) P(Ak)
N
j=1 P(B|Aj) P(Aj)
.
(13.7)
If there are several Bs, then a third form of Bayes’s theorem is
P(Ak|Bℓ) =
P(Bℓ|Ak) P(Ak)
N
j=1 P(Bℓ|Aj) P(Aj)
.
(13.8)
Example 13.1 (The low-base-rate problem)
Suppose the incidence of a rare
disease in a population is P(D) = 0.001. Suppose a test for the disease has
a sensitivity of 99%, that is, the probability that a carrier will test positive is
P(+|D) = 0.99. Suppose the test also is highly selective with a false-positive
rate of only P(+|N) = 0.005. Then the probability that a random person in the
population would test positive is by (13.6)
P(+) = P(+|D) P(D) + P(+|N) P(N) = 0.005993.
(13.9)
And by Bayes’s theorem (13.5), the probability that a person who tests positive
actually has the disease is only
P(D|+) = P(+|D) P(D)
P(+)
= 0.99 × 0.001
0.005993
= 0.165
(13.10)
and the probability that a person testing positive actually is healthy is P(N|+) =
1 −P(D|+) = 0.835.
Even with an excellent test, screening for rare diseases is problematic. Sim-
ilarly, screening for rare behaviors, such as drug use in the CIA or disloyalty
503

PROBABILITY AND STATISTICS
in the army, is difﬁcult with a good test and absurd with a poor one like a
polygraph.
Example 13.2 (The three-door problem)
A prize lies behind one of three closed
doors. A contestant gets to pick which door to open, but before the chosen door
is opened, a door that does not lead to the prize and was not picked by the
contestant swings open. Should the contestant switch and choose a different
door?
We note that a contestant who picks the wrong door and switches always
wins, so P(W|Sw, WD) = 1, while one who picks the right door and switches
never does P(W|Sw, RD) = 0. Since the probability of picking the wrong door
is P(WD) = 2/3, the probability of winning if one switches is
P(W|Sw) = P(W|Sw, WD) P(WD) + P(W|Sw, RD) P(RD) = 2/3.
(13.11)
The probability of picking the right door is P(RD) = 1/3, and the probability
of winning if one picks the right door and stays put is P(W|Sp, RD) = 1. So the
probability of winning if one stays put is
P(W|Sp) = P(W|Sp, RD) P(RD) + P(W|Sp, WD) P(WD) = 1/3.
(13.12)
Thus, one should switch after the door opens.
If the set A is the interval (x −dx/2, x + dx/2) of the real line, then P(A) =
P(x) dx, and the second version of Bayes’s theorem (13.7) says
P(x|B) =
P(B|x) P(x)
2 ∞
−∞P(B|x′) P(x′) dx′ .
(13.13)
Example 13.3 (A tiny poll)
We ask four people if they will vote for Nancy
Pelosi, and three say yes. If the probability that a random voter will vote for her
is y, then the probability that three in our sample of four will is
P(3|y) = 4 y3 (1 −y).
(13.14)
We don’t know the prior probability distribution P(y), so we set it equal to unity
on the interval (0, 1). Then the continuous form of Bayes’s theorem (13.13) and
our cheap poll give the probability distribution of the fraction y who will vote
for her as
P(y|3) =
P(3|y) P(y)
2 1
0 P(3|y′) P(y′) dy′ =
P(3|y)
2 1
0 P(3|y′) dy′
=
4 y3 (1 −y)
2 1
0 4 y′3 (1 −y′) dy′ = 20 y3 (1 −y).
(13.15)
504

13.2 MEAN AND VARIANCE
Our best guess then for the probability that she will win the election is
 1
1/2
P(y|3) dy =
 1
1/2
20 y3 (1 −y) dy = 13
16,
(13.16)
which is slightly higher that the naive estimate of 3/4.
13.2 Mean and variance
In roulette and many other games, N outcomes xj can occur with probabilities
Pj that sum to unity
N

j=1
Pj = 1.
(13.17)
The expected value E[x] of the outcome x is its mean μ or average value ⟨x⟩= x
E[x] = μ = ⟨x⟩= x =
N

j=1
xj Pj.
(13.18)
The expected value E[x] also is called the expectation of x or expectation value
of x.
The ℓth moment is
E[xℓ] = μℓ= ⟨xℓ⟩=
N

j=1
xℓ
j Pj
(13.19)
and the ℓth central moment is
E[(x −μ)ℓ] = νℓ=
N

j=1
(xj −μ)ℓPj
(13.20)
where always μ0 = ν0 = 1 and ν1 = 0 (exercise 13.2).
The variance V[x] is the second central moment
V[x] ≡E[(x −⟨x⟩)2] =
N

j=1

xj −⟨x⟩
2 Pj,
(13.21)
which one may write as (exercise 13.4)
V[x] = ⟨x2⟩−⟨x⟩2
(13.22)
and the standard deviation σ is its square-root
σ =

V[x].
(13.23)
505

PROBABILITY AND STATISTICS
If the values of x are distributed continuously according to a probability
distribution or density P(x) normalized to unity

P(x) dx = 1
(13.24)
then the mean value is
E[x] = μ = ⟨x⟩=

x P(x) dx
(13.25)
and the ℓth moment is
E[xℓ] = μℓ= ⟨xℓ⟩=

xℓP(x) dx.
(13.26)
The ℓth central moment is
E[(x −μ)ℓ] = νℓ=

(x −μ)ℓP(x) dx.
(13.27)
The variance of the distribution is the second central moment
V[x] = ν2 =

(x −⟨x⟩)2 P(x) dx = μ2 −μ2
(13.28)
and the standard deviation σ is its square-root σ = √V[x].
Many authors use f (x) for the probability distribution P(x) and F(x) for the
cumulative probability Pr(−∞, x) of an outcome in the interval (−∞, x)
F(x) ≡Pr(−∞, x) =
 x
−∞
P(x′) dx′ =
 x
−∞
f (x′) dx′,
(13.29)
a function that is necessarily monotonic
F′(x) = Pr′(−∞, x) = f (x) = P(x) ≥0.
(13.30)
Some mathematicians reserve the term probability distribution for probabilities
like Pr(−∞, x) and Pj and call a continuous distribution P(x) a probabil-
ity density function. But usage of the Maxwell–Boltzmann distribution is too
widespread in physics for me to observe this distinction.
Although a probability distribution P(x) is normalized (13.24), it can have
fat tails, which are important in ﬁnancial applications (Bouchaud and Potters,
2003). Fat tails can make the variance and even the mean absolute deviation
Eabs ≡

|x −μ| P(x) dx
(13.31)
diverge.
Example 13.4 (Heisenberg’s uncertainty principle)
In quantum mechanics, the
absolute-value squared |ψ(x)|2 of a wave-function ψ(x) is the probability dis-
tribution P(x) = |ψ(x)|2 of the position x of the particle, and P(x) dx is the
probability that the particle is found between x−dx/2 and x+dx/2. The variance
506

13.2 MEAN AND VARIANCE
⟨(x −⟨x⟩)2⟩of the position operator x is written as the square (x)2 of the stan-
dard deviation σ = x, which is the uncertainty in the position of the particle.
Similarly, the square of the uncertainty in the momentum (p)2 is the variance
⟨(p −⟨p⟩)2⟩of the momentum.
For the wave-function (3.70)
ψ(x) =
 2
π
1/4 1
√a e−(x/a)2
(13.32)
these uncertainties are x = a/2 and p = ¯h/a. They provide a (saturated)
example x p = ¯h/2 of Heisenberg’s uncertainty principle
x p ≥¯h
2.
(13.33)
If x and y are two random variables that occur with a joint distribution
P(x, y), then the expected value of the linear combination axnym + bxpyq is
E[axnym + bxpyq] =

(axnym + bxpyq) P(x, y) dxdy
= a

xnym P(x, y) dxdy + b

xpyq P(x, y) dxdy
= a E[xnym] + b E[xpyq].
(13.34)
This result and its analog for discrete probability distributions show that
expected values are linear.
The correlation coefﬁcient or covariance of two variables x and y that occur
with a joint distribution P(x, y) is
C[x, y] ≡

P(x, y)(x−x)(y−y) dxdy = ⟨(x−x)(y−y)⟩= ⟨x y⟩−⟨x⟩⟨y⟩. (13.35)
The variables x and y are said to be independent if
P(x, y) = P(x) P(y).
(13.36)
Independence implies that the covariance vanishes, but C[x, y] = 0 does not
guarantee that x and y are independent (Roe, 2001, p. 9).
The variance of x + y
⟨(x + y)2⟩−⟨x + y⟩2 = ⟨x2⟩−⟨x⟩2 + ⟨y2⟩−⟨y⟩2 + 2 (⟨x y⟩−⟨x⟩⟨y⟩) (13.37)
is the sum
V[x + y] = V[x] + V[y] + 2 C[x, y].
(13.38)
It follows (exercise 13.6) that for any constants a and b the variance of ax+by is
V[ax + by] = a2 V[x] + b2 V[y] + 2 ab C[x, y].
(13.39)
507

PROBABILITY AND STATISTICS
More generally (exercise 13.7), the variance of the sum a1x1 + a2x2 + · · · +
aNxN is
V[a1x1 + · · · + aNxN] =
N

j=1
a2
j V[xj] +
N

j,k=1,j<k
2ajakC[xj, xk].
(13.40)
If the variables xj and xk are independent for j ̸= k, then their covariances
vanish C[xj, xk] = 0, and the variance of the sum a1x1 + · · · + aNxN is
V[a1x1 + · · · + aNxN] =
N

j=1
a2
j V[xj].
(13.41)
13.3 The binomial distribution
If the probability of success is p on each try, then we expect that in N tries the
mean number of successes will be
⟨n⟩= N p.
(13.42)
The probability of failure on each try is q = 1 −p. So the probability of a par-
ticular sequence of successes and failures, such as n successes followed by N −n
failures is pn qN−n. There are N!/n! (N −n)! different sequences of n successes
and N −n failures, all with the same probability pn qN−n. So the probability of
n successes (and N −n failures) in N tries is
PB(n, p, N) =
N!
n! (N −n)! pn qN−n =
N
n

pn (1 −p)N−n.
(13.43)
This binomial distribution also is called Bernoulli’s distribution (Jacob Bernoulli,
1654–1705).
The sum of the probabilities PB(n, p, N) for all possible values of n is unity
N

n=0
PB(n, p, N) = (p + 1 −p)N = 1.
(13.44)
In Fig. 13.1, the probabilities PB(n, p, N) for 0 ≤n ≤250 and p = 0.2 are
plotted for N = 125, 250, 500, and 1000 tries.
The mean number of successes
μ = ⟨n⟩B =
N

n=0
n PB(n, p, N) =
N

n=0
n
N
n

pnqN−n
(13.45)
is a partial derivative with respect to p with q held ﬁxed
508

13.3 THE BINOMIAL DISTRIBUTION
0
50
100
150
200
250
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
Some binomial distributions
n
PB(n, p, N)
Figure 13.1
If the probability of success on any try is p, then the probability
PB(n, p, N) of n successes in N tries is given by equation (13.43). For p = 0.2, this
binomial probability distribution PB(n, p, N) is plotted against n for N = 125 (solid),
250 (dashes), 500 (dot dash), and 1000 tries (dots).
⟨n⟩B = p ∂
∂p
N

n=0
N
n

pnqN−n
= p ∂
∂p (p + q)N = Np (p + q)N = Np,
(13.46)
which veriﬁes the estimate (13.42).
One may show (exercise 13.9) that the variance (13.21) of the binomial
distribution is
VB = ⟨(n −⟨n⟩)2⟩= p (1 −p) N.
(13.47)
Its standard deviation (13.23) is
σB =

VB =

p (1 −p) N.
(13.48)
The ratio of the width to the mean
σB
⟨n⟩B
=

p (1 −p) N
Np
=
(
1 −p
Np
(13.49)
decreases with N as 1/
√
N.
509

PROBABILITY AND STATISTICS
Example 13.5 (Avogadro’s number)
A mole of gas is Avogadro’s number NA =
6 × 1023 of molecules. If the gas is in a cubical box, then the chance that each
molecule will be in the left half of the cube is p = 1/2. The mean number of
molecules there is ⟨n⟩B = pNA = 3 × 1023, and the uncertainty in n is σB =

p (1 −p) N =

3 × 1023/4 = 3 × 1011. So the numbers of gas molecules in
the two halves of the box are equal to within σB/⟨n⟩B = 10−12 or to 1 part
in 1012.
Because N! increases very rapidly with N, the rule
PB(n + 1, p, N) =
p
1 −p
N −n
n + 1 PB(n, p, N)
(13.50)
is helpful when N is big. But when N exceeds a few hundred, the formula (13.43)
for PB(n, p, N) becomes unmanageable even in quadruple precision. One way of
computing PB(n, p, N) for large N is to use Srinivasa Ramanujan’s correction to
Stirling’s formula N! ≈
√
2πN(N/e)N
N! ≈
√
2πN
N
e
N 
1 + 1
2N +
1
8N2
1/6
.
(13.51)
When N and N −n, but not n, are big, one may use (13.51) for N! and (N −n)!
in the formula (13.43) for PB(n, p, N) and so may show (exercise 13.11) that
PB(n, p, N) ≈(pN)n
n!
qN−n R2(n, N),
(13.52)
in which
R2(n, N) =

1 −n
N
n−N−1/2 
1 + 1
2N +
1
8N2
1/6
×
#
1 +
1
2(N −n) +
1
8(N −n)2
$−1/6
(13.53)
tends to unity as N →∞for any ﬁxed n.
When all three factorials in PB(n, p, N) are huge, one may use Ramanujan’s
approximation (13.51) to show (exercise 13.12) that
PB(n, p, N) ≈
(
N
2πn(N −n)
pN
n
n  qN
N −n
N−n
R3(n, N)
(13.54)
510

13.4 THE POISSON DISTRIBUTION
where
R3(n, N) =

1 + 1
2n +
1
8n2
−1/6 
1 + 1
2N +
1
8N2
1/6
×
#
1 +
1
2(N −n) +
1
8(N −n)2
$−1/6
(13.55)
tends to unity as N →∞, N −n →∞, and n →∞.
Another way of coping with the unwieldy factorials in the binomial formula
PB(n, p, N) is to use limiting forms of (13.43) due to Poisson and to Gauss.
13.4 The Poisson distribution
Poisson took the two limits N →∞and p = ⟨n⟩/N →0. So we let N and
N −n, but not n, tend to inﬁnity, and use (13.52) for the binomial distribution
(13.43). Since R2(n, N) →1 as N →∞, we get
PB(n, p, N) ≈(pN)n
n!
qN−n = ⟨n⟩n
n! qN−n.
(13.56)
Now q = 1 −p = 1 −⟨n⟩/N, and so for any ﬁxed n we have
lim
N→∞qN−n = lim
N→∞

1 −⟨n⟩
N
N 
1 −⟨n⟩
N
−n
= e−⟨n⟩.
(13.57)
Thus as N →∞with pN ﬁxed at ⟨n⟩, the binomial distribution becomes the
Poisson distribution
PP(n, ⟨n⟩) = ⟨n⟩n
n! e−⟨n⟩.
(13.58)
(Siméon-Denis Poisson, 1781–1840. Incidentally, poisson means ﬁsh and sounds
like pwahsahn.)
The Poisson distribution is normalized to unity
∞

n=0
PP(n, ⟨n⟩) =
∞

n=0
⟨n⟩n
n! e−⟨n⟩= e⟨n⟩e−⟨n⟩= 1.
(13.59)
Its mean μ is the parameter ⟨n⟩= pN of the binomial distribution
μ =
∞

n=0
n PP(n, ⟨n⟩) =
∞

n=1
n ⟨n⟩n
n! e−⟨n⟩= ⟨n⟩
∞

n=1
⟨n⟩(n−1)
(n −1)! e−⟨n⟩
= ⟨n⟩
∞

n=0
⟨n⟩n
n! e−⟨n⟩= ⟨n⟩.
(13.60)
511

PROBABILITY AND STATISTICS
As N →∞and p →0 with p N = ⟨n⟩ﬁxed, the variance (13.47) of the
binomial distribution tends to the limit
VP = lim
N→∞
p→0
VB = lim
N→∞
p→0
p (1 −p) N = ⟨n⟩.
(13.61)
Thus the mean and the variance of a Poisson distribution are equal
VP = ⟨(n −⟨n⟩)2⟩= ⟨n⟩= μ
(13.62)
as one may show directly (exercise 13.13).
Example 13.6 (Coherent states)
The coherent state |α⟩introduced in equation
(2.138)
|α⟩= e−|α|2/2eαa†|0⟩= e−|α|2/2
∞

n=0
αn
√
n!
|n⟩
(13.63)
is an eigenstate a|α⟩= α|α⟩of the annihilation operator a with eigenvalue α.
The probability P(n) of ﬁnding n quanta in the state |α⟩is the square of the
absolute value of the inner product ⟨n|α⟩
P(n) = |⟨n|α⟩|2 = |α|2n
n!
e−|α|2,
(13.64)
which is a Poisson distribution P(n) = PP(n, |α|2) with mean and variance μ =
⟨n⟩= V(α) = |α|2.
13.5 The Gaussian distribution
Gauss considered the binomial distribution in the limit N →∞with the
probability p ﬁxed. In this limit, the binomial probability
PB(n, p, N) =
N!
n! (N −n)! pn qN−n
(13.65)
is very tiny unless n is near pN, which means that n ≈pN and N −n ≈(1 −
p)N = qN are comparable. So the limit N →∞effectively is one in which n
and N −n also tend to inﬁnity. The approximation (13.54)
PB(n, p, N) ≈
(
N
2πn(N −n)
pN
n
n  qN
N −n
N−n
R3(n, N)
(13.66)
applies in which R3(n, N) →1 as N, N −n, and n all increase without limit.
Because the probability PB(n, p, N) is negligible unless n ≈pN, we set y =
n−pN and treat y/n as small. Since n = pN+y and N−n = (1−p)N+pN−n =
qN −y, we may write the square-root as
512

13.5 THE GAUSSIAN DISTRIBUTION
(
N
2π n (N −n) =
1

2π N (pN + y)/N (qN −y)/N
=
1

2π pqN (1 + y/pN) (1 −y/qN)
.
(13.67)
Since y remains ﬁnite as N →∞, we get in this limit
lim
N→∞
(
N
2π n (N −n) =
1

2π pqN
.
(13.68)
Substituting pN + y for n and qN −y for N −n in (13.66), we ﬁnd
PB(n, p, N) ≈
1

2π pqN

pN
pN + y
pN+y 
qN
qN −y
qN−y
=
1

2π pqN

1 + y
pN
−(pN+y) 
1 −y
qN
−(qN−y)
,
(13.69)
which implies
ln

PB(n, p, N)

2π pqN

≈−(pN + y) ln
#
1 + y
pN
$
−(qN −y) ln
#
1 −y
qN
$
.
(13.70)
The ﬁrst two terms of the power series (4.88) for ln(1 + ϵ) are
ln(1 + ϵ) ≈ϵ −1
2ϵ2.
(13.71)
So, using this expansion for ln(1 + y/pN) and also for ln(1 −y/qN), we get
ln

PB(n, p, N)

2π pqN

≈−(pN + y)

y
pN −1
2
 y
pN
2
−(qN −y)

−y
qN −1
2
 y
qN
2
≈−
y2
2pqN .
(13.72)
Gauss’s approximation to the binomial probability distribution thus is
PBG(n, p, N) =
1

2πpqN
exp

−(n −pN)2
2pqN

,
(13.73)
in which we’ve replaced y by n −pN and 1 −p by q.
513

PROBABILITY AND STATISTICS
Extending the integer n to a continuous variable x, we have
PG(x, p, N) =
1

2πpqN
exp

−(x −pN)2
2pqN

,
(13.74)
which is (exercise 13.14) a normalized probability distribution with mean ⟨x⟩=
μ = pN and variance ⟨(x −μ)2⟩= σ 2 = pqN. Replacing pN by μ and pqN by
σ 2, we get the standard form of Gauss’s distribution
PG(x, μ, σ) =
1
σ
√
2π
exp

−(x −μ)2
2σ 2

.
(13.75)
This distribution occurs so often in mathematics and in Nature that it is often
called the normal distribution. Its odd central moments all vanish ν2n+1 = 0,
and its even ones are ν2n = (2n −1)!! σ 2n (exercise 13.16).
Example 13.7 (Single-molecule super-resolution microscopy)
If the wave-
length of visible light were a nanometer, microscopes would yield much sharper
images. Each photon from a (single-molecule) ﬂuorophore entering the lens of a
microscope would follow ray optics and be focused within a tiny circle of about
a nanometer on a detector. Instead, a photon arrives not at x = (x1, x2) but at
yi = (y1i, y2i) with gaussian probability
P(yi) =
1
2πσ 2 e−(yi−x)2/2σ 2
(13.76)
where σ ≈150 nm is about a quarter of a wave-length. What to do?
In the centroid method, one collects N ≈500 points yi and ﬁnds the point x
that maximizes the joint probability of the N image points
P =
N

i=1
P(yi) = dN
N

i=1
e−(yi−x)2/(2σ 2) = dN exp

−
N

i=1
(yi −x)2/(2σ 2)

(13.77)
where d = 1/2πσ 2, by solving for k = 1 and 2 the equations
∂P
∂xk
= 0 = P ∂P
∂xk

−
N

i=1
(yi −x)2/(2σ 2)

= P
σ 2
N

i=1
(yik −xk) .
(13.78)
This maximum-likelihood estimate of the image point x is the average of the
observed points yi
x = 1
N
N

i=1
yi.
(13.79)
This method is an improvement, but it is biased by auto-ﬂuorescence and
out-of-focus ﬂuorophores. Fang Huang and Keith Lidke use direct stochastic
514

13.6 THE ERROR FUNCTION ERF
Actin fibers in HELA cells
Figure 13.2
Conventional (left, fuzzy) and dSTORM (right, sharp) images of actin
ﬁbers in HELA cells. The actin is labeled with Alexa Fluor 647 Phalloidin. The
white rectangles are 5 microns in length. Images courtesy of Fang Huang and Keith
Lidke.
optical reconstruction microscopy (dSTORM) to locate the image point x of
the ﬂuorophore in ways that account for the ﬁnite accuracy of their pixilated
detector and the randomness of photo-detection.
Actin ﬁlaments are double helices of the protein actin some 5–9 nm wide. They
occur throughout a eukaryotic cell but are concentrated near its surface and
determine its shape. Together with tubulin and intermediate ﬁlaments, they form
a cell’s cytoskeleton. Figure 13.2 shows conventional (left, fuzzy) and dSTORM
(right, sharp) images of actin ﬁlaments. The ﬁnite size of the ﬂuorophore and
the motion of the molecules of living cells limit dSTORM’s improvement in
resolution to a factor of 10 to 20.
13.6 The error function erf
The probability that a random variable x distributed according to Gauss’s
distribution (13.75) has a value between μ −δ and μ + δ is
P(|x −μ| < δ) =
 μ+δ
μ−δ
PG(x, μ, σ) dx =
1
σ
√
2π
 μ+δ
μ−δ
exp

−(x −μ)2
2σ 2

dx
=
1
σ
√
2π
 δ
−δ
exp

−x2
2σ 2

dx =
2
√π
 δ/σ
√
2
0
e−t2 dt.
(13.80)
The last integral is the error function
515

PROBABILITY AND STATISTICS
erf (x) =
2
√π
 x
0
e−t2dt,
(13.81)
so in terms of it the probability that x lies within δ of the mean μ is
P(|x −μ| < δ) = erf

δ
σ
√
2

.
(13.82)
In particular, the probabilities that x falls within one, two, or three standard
deviations of μ are
P(|x −μ| < σ) = erf (1/
√
2) = 0.6827,
P(|x −μ| < 2σ) = erf (2/
√
2) = 0.9545,
P(|x −μ| < 3σ) = erf (3/
√
2) = 0.9973.
(13.83)
The error function erf (x) is plotted in Fig. 13.3 in which the vertical lines are at
x = δ/(σ
√
2) for δ = σ, 2σ, and 3σ.
The probability that x falls between a and b is (exercise 13.17)
P(a < x < b) = 1
2
#
erf
b −μ
σ
√
2

−erf
a −μ
σ
√
2
$
.
(13.84)
0
0.5
1
1.5
2
2.5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
The error function
x
erf(x)
1σ
2σ
3σ
Figure 13.3
The error function erf (x) is plotted for 0 < x < 2.5. The vertical lines
are at x = δ/(σ
√
2) for δ = σ, 2σ, and 3σ with σ = 1/
√
2.
516

13.6 THE ERROR FUNCTION ERF
In particular, the cumulative probability P(−∞, x) that the random variable is
less than x is for μ = 0 and σ = 1
P(−∞, x) = 1
2
#
erf
 x
√
2

−erf
−∞
√
2
$
= 1
2
#
erf
 x
√
2

+ 1
$
.
(13.85)
The complement erfc of the error function is deﬁned as
erfc (x) =
2
√π
 ∞
x
e−t2dt = 1 −erf (x)
(13.86)
and is numerically useful for large x where round-off errors may occur in sub-
tracting erf(x) from unity. Both erf and erfc are intrinsic functions in FORTRAN
available without any effort on the part of the programmer.
Example 13.8 (Summing binomial probabilities)
To add up several binomial
probabilities when the factorials in PB(n, p, N) are too big to handle, we ﬁrst use
Gauss’s approximation (13.73)
PB(n, p, N) =
N!
n! (N −n)! pn qN−n ≈
1

2πpqN
exp

−(n −pN)2
2pqN

.
(13.87)
Then, using (13.84) with μ = pN, we ﬁnd (exercise 13.15)
PB(n, p, N) ≈1
2

erf

n + 1
2 −pN

2pqN

−erf

n −1
2 −pN

2pqN

,
(13.88)
which we can sum over the integer n to get
n2

n=n1
PB(n, p, N) ≈1
2

erf

n2 + 1
2 −pN

2pqN

−erf

n1 −1
2 −pN

2pqN

,
(13.89)
which is easy to evaluate.
Example 13.9 (Polls)
Suppose in a poll of 1000 likely voters, 600 have said they
would vote for Nancy Pelosi. Repeating the analysis of example 13.3, we see that
if the probability that a random voter will vote for her is y, then the probability
that 600 in our sample of 1000 will is by (13.87)
P(600|y) = PB(600, y) =
1000
600

y600 (1 −y)400
≈
1
10

20πy(1 −y)
exp

−20(3 −5y)2
y(1 −y)

(13.90)
517

PROBABILITY AND STATISTICS
and so the probability density that a fraction y of the voters will vote for her is
P(y|600) =
P(600|y)
2 1
0 P(600, y′) dy′
=
[y(1 −y)]−1/2 exp

−20(3−5y)2
y(1−y)

2 1
0 [y′(1 −y′)]−1/2 exp

−20(3−5y′)2
y′(1−y′)

dy′ .
(13.91)
This normalized probability distribution is negligible except for y near 3/5
(exercise 13.18), where it is approximately Gauss’s distribution
P(y|600) ≈
1
σ
√
2π
exp

−(y −3/5)2
2σ 2

(13.92)
with mean μ = 3/5 and variance
σ 2 =
3
12500 = 2.4 × 10−4.
(13.93)
The probability that y > 1/2 then is by (13.84)
P(a < x < b) = 1
2
#
erf
1 −μ
σ
√
2

−erf
1/2 −μ
σ
√
2
$
= 1
2
#
erf
 20
√
1.2

−erf
 −5
√
1.2
$
≈1.
(13.94)
The probability that y < 1/2 is 5.4 × 10−11.
13.7 The Maxwell–Boltzmann distribution
It is a small jump from Gauss’s distribution (13.75) to the Maxwell–Boltzmann
distribution of velocities of molecules in a gas. We start in one dimension and
focus on a single molecule that is being hit fore and aft with equal probabilities
by other molecules. If each hit increases or decreases its speed by dv, then after
n aft hits and N −n fore hits, the speed vx of a molecule initially at rest would
be
vx = ndv −(N −n)dv = (2n −N)dv.
(13.95)
The probability of this speed is given by Gauss’s approximation (13.73) to the
binomial distribution PB(n, 1
2, N) as
PBG(n, 1
2, N) =
&
2
πN exp

−(2n −N)2
2N

=
&
2
πN exp

−
v2
x
2Ndv2

.
(13.96)
This argument applies to any physical variable subject to unbiased random
ﬂuctuations. It is why Gauss’s distribution describes statistical errors and why
it occurs so often in Nature as to be called the normal distribution.
518

13.8 DIFFUSION
We now write the argument of the exponential in terms of the temperature T
and Boltzmann’s constant k by setting N = kT/(m dv2) so that
−
1
2v2
x
Ndv2 = −
1
2mv2
x
mNdv2 = −
1
2mv2
x
kT .
(13.97)
Then with dvx = 2dv, we have
PG(vx)dvx =
&
2m
πkT dv exp

−
1
2mv2
x
kT

=
&
m
2πkT dvx exp

−
1
2mv2
x
kT

.
(13.98)
Gauss’s distribution is normalized to unity because it is the limit of the
binomial distribution (13.44)
 ∞
−∞
&
m
2πkT exp

−
1
2mv2
x
kT

dvx = 1
(13.99)
as you may verify by explicit integration.
In three space dimensions, the Maxwell–Boltzmann distribution PMB(v) is
the product
PMB(v)d3v = PG(vx) PG(vy) PG(vz)d3v =

m
2πkT
3/2
e−1
2mv2/(kT)4πv2dv.
(13.100)
The mean value of the velocity of a Maxwell–Boltzmann gas vanishes
⟨v⟩=

v PMB(v)d3v = 0
(13.101)
but the mean value of the square of the velocity v2 = v · v is the sum of the
three variances σ 2
x = σ 2
y = σ 2
z = kT/m
⟨v2⟩= V[v2] =

v2 PMB(v) d3v = 3kT/m
(13.102)
which is the familiar statement
1
2m⟨v2⟩= 3
2 kT
(13.103)
that each degree of freedom gets kT/2 of energy.
13.8 Diffusion
We may apply the same reasoning as in the preceding section (13.7) to the dif-
fusion of a gas of particles treated as a random walk with step size dx. In one
dimension, after n steps forward and N −n steps backward, a particle starting
at x = 0 is at x = (2n −N)dx. Thus, as in (13.96), the probability of being
519

PROBABILITY AND STATISTICS
at x is given by Gauss’s approximation (13.73) to the binomial distribution
PB(n, 1
2, N) as
PBG(n, 1
2, N) =
&
2
πN exp

−(2n −N)2
2N

=
&
2
πN exp

−
x2
2Ndx2

.
(13.104)
In terms of the diffusion constant
D = Ndx2
2t
(13.105)
this distribution is
PG(x) =

1
4πDt
1/2
exp

−x2
4Dt

(13.106)
when normalized to unity on (−∞, ∞).
In three dimensions, this gaussian distribution is the product
P(r, t) = PG(x) PG(y) PG(z) =

1
4πDt
3/2
exp

−r2
4Dt

.
(13.107)
The variance σ 2 = 2Dt gives the average of the squared displacement of each
of the three coordinates. Thus the mean of the squared displacement ⟨r2⟩rises
linearly with the time as
⟨r2⟩= V[r] = 3σ 2 =

r2 P(r, t) d3r = 6 D t.
(13.108)
The distribution P(r, t) satisﬁes the diffusion equation
˙P(r, t) = D ∇2P(r, t),
(13.109)
in which the dot means time derivative.
13.9 Langevin’s theory of brownian motion
Einstein made the ﬁrst theory of brownian motion in 1905, but Langevin’s
approach (Langevin, 1908) is simpler. A tiny particle of colloidal size and mass
m in a ﬂuid is buffeted by a force F(t) due to the 1021 collisions per second it
suffers with the molecules of the surrounding ﬂuid. Its equation of motion is
m dv(t)
dt
= F(t).
(13.110)
Langevin suggested that the force F(t) is the sum of a viscous drag −v(t)/B and
a rapidly ﬂuctuating part f (t)
F(t) = −v(t)/B + f (t)
(13.111)
520

13.9 LANGEVIN’S THEORY OF BROWNIAN MOTION
so that
m dv(t)
dt
= −v(t)
B + f (t).
(13.112)
The parameter B is called the mobility. The ensemble average (the average over
the set of particles) of the ﬂuctuating force f (t) is zero
⟨f (t)⟩= 0.
(13.113)
Thus the ensemble average of the velocity satisﬁes
m d⟨v⟩
dt
= −⟨v⟩
B
(13.114)
whose solution with τ = mB is
⟨v(t)⟩= ⟨v(0)⟩e−t/τ.
(13.115)
The instantaneous equation (13.112) divided by the mass m is
dv(t)
dt
= −v(t)
τ
+ a(t),
(13.116)
in which a(t) = f (t)/m is the acceleration. The ensemble average of the scalar
product of the position vector r with this equation is
.
r · dv
dt
/
= −⟨r · v⟩
τ
+ ⟨r · a⟩.
(13.117)
But since the ensemble average ⟨r · a⟩of the scalar product of the position
vector r with the random, ﬂuctuating part a of the acceleration vanishes, we
have
.
r · dv
dt
/
= −⟨r · v⟩
τ
.
(13.118)
Now
1
2
d r2
dt = 1
2
d
dt (r · r) = r · v
(13.119)
and so
1
2
d2r2
dt2 = r · dv
dt + v2.
(13.120)
The ensemble average of this equation gives us
d2⟨r2⟩
dt2
= 2
.
r · dv
dt
/
+ 2⟨v2⟩
(13.121)
or in view of (13.118)
d2⟨r2⟩
dt2
= −2⟨r · v⟩
τ
+ 2⟨v2⟩.
(13.122)
521

PROBABILITY AND STATISTICS
We now use (13.119) to replace ⟨r · v⟩with half the ﬁrst time derivative of ⟨r2⟩
so that we have
d2⟨r2⟩
dt2
= −1
τ
d⟨r2⟩
dt
+ 2⟨v2⟩.
(13.123)
If the ﬂuid is in equilibrium, then the ensemble average of v2 is given by the
Maxwell–Boltzmann value (13.103)
⟨v2⟩= 3kT
m
(13.124)
and so the acceleration (13.123) is
d2⟨r2⟩
dt2
+ 1
τ
d⟨r2⟩
dt
= 6kT
m ,
(13.125)
which we can integrate.
The general solution (6.13) to a second-order linear inhomogeneous differ-
ential equation is the sum of any particular solution to the inhomogeneous
equation plus the general solution of the homogeneous equation. The function
⟨r2(t)⟩pi = 6kTtτ/m is a particular solution of the inhomogeneous equa-
tion. The general solution to the homogeneous equation is ⟨r2(t)⟩gh = U +
W exp(−t/τ) where U and W are constants. So ⟨r2(t)⟩is
⟨r2(t)⟩= U + W e−t/τ + 6kTτt/m
(13.126)
where U and W make ⟨r2(t)⟩ﬁt the boundary conditions. If the individual
particles start out at the origin r = 0, then one boundary condition is
⟨r2(0)⟩= 0,
(13.127)
which implies that
U + W = 0.
(13.128)
And since the particles start out at r = 0 with an isotropic distribution of initial
velocities, the formula (13.119) for ˙r2 implies that at t = 0
d ⟨r2⟩
dt

t=0
= 2⟨r(0) · v(0)⟩= 0.
(13.129)
This boundary condition means that our solution (13.126) must satisfy
d ⟨r2(t)⟩
dt

t=0
= −W
τ + 6kTτ
m
= 0.
(13.130)
Thus W = −U = 6kTτ 2/m, and so our solution (13.126) is
⟨r2(t)⟩= 6kTτ 2
m
# t
τ + e−t/τ −1
$
.
(13.131)
522

13.10 THE EINSTEIN–NERNST RELATION
At times short compared to τ, the ﬁrst two terms in the power series for the
exponential exp(−t/τ) cancel the terms −1 + t/τ, leaving
⟨r2(t)⟩= 6kTτ 2
m

t2
2τ 2

= 3kT
m t2 = ⟨v2⟩t2.
(13.132)
But at times long compared to τ, the exponential vanishes, leaving
⟨r2(t)⟩= 6kTτ
m
t = 6 B kT t.
(13.133)
The diffusion constant D is deﬁned by
⟨r2(t)⟩= 6 D t
(13.134)
and so we arrive at Einstein’s relation
D = B kT,
(13.135)
which often is written in terms of the viscous-friction coefﬁcient ζ
ζ ≡1
B = m
τ
(13.136)
as
ζ D = kT.
(13.137)
This equation expresses Boltzmann’s constant k in terms of three quantities
ζ, D, and T that were accessible to measurement in the ﬁrst decade of the
twentieth century. It enabled scientists to measure Boltzmann’s constant k
for the ﬁrst time. And since Avogadro’s number NA was the known gas con-
stant R divided by k, the number of molecules in a mole was revealed to be
NA = 6.022 × 1023. Chemists could then divide the mass of a mole of any
pure substance by 6.022 × 1023 and ﬁnd the mass of the molecules that com-
posed it. Suddenly the masses of the molecules of chemistry became known,
and molecules were recognized as real particles and not tricks for balancing
chemical equations.
13.10 The Einstein–Nernst relation
If a particle of mass m carries an electric charge q and is exposed to an electric
ﬁeld E, then in addition to viscosity −v/B and random buffeting f , the constant
force qE acts on it
m dv
dt = −v
B + qE + f .
(13.138)
The mean value of its velocity will then satisfy the differential equation
.dv
dt
/
= −⟨v⟩
τ
+ qE
m
(13.139)
523

PROBABILITY AND STATISTICS
where τ = mB. A particular solution of this inhomogeneous equation is
⟨v(t)⟩pi = qτE
m
= qBE.
(13.140)
The general solution of its homogeneous version is ⟨v(t)⟩gh = A exp(−t/τ) in
which the constant A is chosen to give ⟨v(0)⟩at t = 0. So by (6.13), the general
solution ⟨v(t)⟩to equation (13.139) is (exercise 13.42) the sum of ⟨v(t)⟩pi and
⟨v(t)⟩gh
⟨v(t)⟩= qBE + [⟨v(0)⟩−qBE] e−t/τ.
(13.141)
By applying the tricks of the previous section (13.9), one may show (exer-
cise 13.43) that the variance of the position r about its mean qBE t is
7
(r −qBE t)28
= 6kTτ 2
m
 t
τ −1 + e−t/τ

.
(13.142)
So for times t ≫τ, this variance is
7
(r −qBE t)28
= 6kTτt
m
.
(13.143)
Since the diffusion constant D is deﬁned by (13.134) as
7
(r −qBE t)28
= 6 D t
(13.144)
we arrive at the Einstein–Nernst relation
D = BkT = qB
q kT = μ
q kT,
(13.145)
in which the electric mobility is μ = qB.
13.11 Fluctuation and dissipation
Let’s look again at Langevin’s equation (13.116) but with u as the independent
variable
dv(u)
du
+ v(u)
τ
= a(u).
(13.146)
If we multiply both sides by the exponential exp(u/τ)
dv
du + v
τ

eu/τ = d
du

v eu/τ
= a(u) eu/τ
(13.147)
and integrate from 0 to t
 t
0
d
du

v eu/τ
du = v(t) et/τ −v(0) =
 t
0
a(u) eu/τ du
(13.148)
524

13.11 FLUCTUATION AND DISSIPATION
then we get
v(t) = e−t/τ v(0) + e−t/τ
 t
0
a(u) eu/τ du.
(13.149)
Thus the ensemble average of the square of the velocity is
⟨v2(t)⟩= e−2t/τ ⟨v2(0)⟩+ 2e−2t/τ
 t
0
⟨a(u)⟩eu/τ du
+ e−2t/τ
 t
0
 t
0
⟨a(u1) · a(u2)⟩e(u1+u2)/τ du1du2.
(13.150)
The second term on the RHS is zero, so we have
⟨v2(t)⟩= e−2t/τ ⟨v2(0)⟩+e−2t/τ
 t
0
 t
0
⟨a(u1)·a(u2)⟩e(u1+u2)/τ du1du2. (13.151)
The ensemble average
C(u1, u2) = ⟨a(u1) · a(u2)⟩
(13.152)
is an example of an autocorrelation function.
All autocorrelation functions have some simple properties, which are easy to
prove (Pathria, 1972, p. 458).
1 If the system is independent of time, then its autocorrelation function for any
given variable A(t) depends only upon the time delay s:
C(t, t + s) = ⟨A(t) · A(t + s)⟩≡C(s).
(13.153)
2 The autocorrelation function for s = 0 is necessarily nonnegative
C(t, t) = ⟨A(t) · A(t)⟩= ⟨A(t)2⟩≥0.
(13.154)
If the system is time independent, then C(t, t) = C(0) ≥0.
3 The absolute value of C(t1, t2) is never greater than the average of C(t1, t1)
and C(t2, t2) because
⟨|A(t1) ± A(t2)|2⟩= ⟨A(t1)2⟩+ ⟨A(t2)2⟩± 2⟨A(t1) · A(t2)⟩≥0,
(13.155)
which implies that
−C(t1, t2) ≤1
2 (C(t1, t1) + C(t2, t2)) ≥C(t1, t2)
(13.156)
or
|C(t1, t2)| ≤1
2 (C(t1, t1) + C(t2, t2)) .
(13.157)
For a time-independent system, this inequality is |C(s)| ≤C(0) for every time
delay s.
525

PROBABILITY AND STATISTICS
4 If the variables A(t1) and A(t2) commute, then their autocorrelation function
is symmetric
C(t1, t2) = ⟨A(t1) · A(t2)⟩= ⟨A(t2) · A(t1)⟩= C(t2, t1).
(13.158)
For a time-independent system, this symmetry is C(s) = C(−s).
5 If the variable A(t) is randomly ﬂuctuating with zero mean, then we expect
both that its ensemble average vanishes
⟨A(t)⟩= 0
(13.159)
and that there is some characteristic time scale T beyond which the correla-
tion function falls to zero:
⟨A(t1) · A(t2)⟩→⟨A(t1)⟩· ⟨A(t2)⟩= 0
(13.160)
when |t1 −t2| ≫T.
In terms of the autocorrelation function C(u1, u2) = ⟨a(u1) · a(u2)⟩of the
acceleration, the variance of the velocity (13.152) is
⟨v2(t)⟩= e−2t/τ ⟨v2(0)⟩+ e−2t/τ
 t
0
 t
0
C(u1, u2) e(u1+u2)/τ du1du2.
(13.161)
Since C(u1, u2) is big only for tiny values of |u2 −u1|, it makes sense to change
variables to
s = u2 −u1
and
w = 1
2(u1 + u2).
(13.162)
The element of area then is by (12.6–12.14)
du1 ∧du2 = dw ∧ds
(13.163)
and the limits of integration are −2w ≤s ≤2w for 0 ≤w ≤t/2 and −2(t−w) ≤
s ≤2(t −w) for t/2 ≤w ≤t. So ⟨v2(t)⟩is
⟨v2(t)⟩= e−2t/τ ⟨v2(0)⟩+ e−2t/τ
 t/2
0
e2w/τdw
 2w
−2w
C(s) ds
+ e−2t/τ
 t
t/2
e2w/τdw
 2(t−w)
−2(t−w)
C(s) ds.
(13.164)
Since by (13.160) the autocorrelation function C(s) vanishes outside a narrow
window of width 2T, we may approximate each of the s-integrals by
C =
 ∞
−∞
C(s) ds.
(13.165)
526

13.11 FLUCTUATION AND DISSIPATION
It follows then that
⟨v2(t)⟩= e−2t/τ ⟨v2(0)⟩+ C e−2t/τ
 t
0
e2w/τdw
= e−2t/τ ⟨v2(0)⟩+ C e−2t/τ τ
2

e2t/τ −1

= e−2t/τ ⟨v2(0)⟩+ C τ
2

1 −e−2t/τ
.
(13.166)
As t →∞, ⟨v2(t)⟩must approach its equilibrium value of 3kT/m, and so
lim
t→∞⟨v2(t)⟩= C τ
2 = 3kT
m ,
(13.167)
which implies that
C = 6kT
mτ
or
1
B = m2C
6kT .
(13.168)
Our ﬁnal formula for ⟨v2(t)⟩then is
⟨v2(t)⟩= e−2t/τ ⟨v2(0)⟩+ 3kT
m

1 −e−2t/τ
.
(13.169)
Referring back to the deﬁnition (13.136) of the viscous-friction coefﬁcient
ζ = 1/B, we see that ζ is related to the integral
ζ = 1
B = m2
6kT C = m2
6kT
 ∞
−∞
⟨a(0)·a(s)⟩ds =
1
6kT
 ∞
−∞
⟨f (0)·f (s)⟩ds (13.170)
of the autocorrelation function of the random acceleration a(t) or equivalently
of the random force f (t). This equation relates the dissipation of viscous fric-
tion to the random ﬂuctuations. It is an example of a ﬂuctuation–dissipation
theorem.
If we substitute our formula (13.169) for ⟨v2(t)⟩into the expression (13.123)
for the acceleration of ⟨r2⟩, then we get
d2⟨r2(t)⟩
dt2
= −1
τ
d⟨r2(t)⟩
dt
+ 2e−2t/τ ⟨v2(0)⟩+ 6kT
m

1 −e−2t/τ
.
(13.171)
The solution with both ⟨r2(0)⟩= 0 and d⟨r2(0)⟩/dt = 0 is (exercise 13.21)
⟨r2(t)⟩= ⟨v2(0)⟩τ 2 
1 −e−t/τ2 −3kT
m τ 2 
1 −e−t/τ
3 −e−t/τ
+ 6kTτ
m
t.
(13.172)
527

PROBABILITY AND STATISTICS
13.12 Characteristic and moment-generating functions
The Fourier transform (3.9) of a probability distribution P(x) is its characteris-
tic function ˆP(k), sometimes written as χ(k)
ˆP(k) ≡χ(k) ≡E[eikx] =

eikx P(x) dx.
(13.173)
The probability distribution P(x) is the inverse Fourier transform (3.9)
P(x) =

e−ikx ˆP(k) dk
2π .
(13.174)
Example 13.10 (Gauss)
The characteristic function of the gaussian
PG(x, μ, σ) =
1
σ
√
2π
exp

−(x −μ)2
2σ 2

(13.175)
is by (3.18)
ˆPG(k, μ, σ) =
1
σ
√
2π

exp

ikx −(x −μ)2
2σ 2

dx
=
eikμ
σ
√
2π

exp

ikx −x2
2σ 2

dx = exp

iμk −1
2σ 2k2

. (13.176)
For a discrete probability distribution Pn the characteristic function is
χ(k) ≡E[eikx] =

n
eikxn Pn.
(13.177)
The normalization of both continuous and discrete probability distributions
implies that their characteristic functions satisfy ˆP(0) = χ(0) = 1.
Example 13.11 (Poisson)
The Poisson distribution (13.58)
PP(n, ⟨n⟩) = ⟨n⟩n
n! e−⟨n⟩
(13.178)
has the characteristic function
χ(k) =
∞

n=0
eikn ⟨n⟩n
n! e−⟨n⟩= e−⟨n⟩
∞

n=0
(⟨n⟩eik)n
n!
= exp

⟨n⟩

eik −1

. (13.179)
528

13.12 CHARACTERISTIC AND MOMENT-GENERATING FUNCTIONS
The moment-generating function is the characteristic function evaluated at an
imaginary argument
M(k) ≡E[ekx] = ˆP(−ik) = χ(−ik).
(13.180)
For a continuous probability distribution P(x), it is
M(k) = E[ekx] =

ekx P(x) dx
(13.181)
and for a discrete probability distribution Pn it is
M(k) = E[ekx] =

n
ekxn Pn.
(13.182)
In both cases, the normalization of the probability distribution implies that
M(0) = 1.
Derivatives of the moment-generating function and of the characteristic
function give the moments
E[xn] = μn = dnM(k)
dkn

k=0
= (−i)n dn ˆP(k)
dkn

k=0
.
(13.183)
Example 13.12 (Gauss and Poisson)
The moment-generating functions for the
distributions of Gauss (13.175) and Poisson (13.178) are
MG(k, μ, σ) = exp

μk + 1
2σ 2k2

and MP(k, ⟨n⟩) = exp

⟨n⟩

ek −1

.
(13.184)
They give as the ﬁrst three moments of these distributions
μG0 = 1,
μG1 = μ,
μG2 = μ2 + σ 2,
(13.185)
μP0 = 1,
μP1 = ⟨n⟩,
μP2 = ⟨n⟩+ ⟨n⟩2
(13.186)
(exercise 13.22).
Since the characteristic and moment-generating functions have derivatives
(13.183) proportional to the moments μn, their Taylor series are
ˆP(k) = E[eikx] =
∞

n=0
(ik)n
n! E[xn] =
∞

n=0
(ik)n
n!
μn
(13.187)
and
M(k) = E[ekx] =
∞

n=0
kn
n! E[xn] =
∞

n=0
kn
n! μn.
(13.188)
529

PROBABILITY AND STATISTICS
The cumulants cn of a probability distribution are the derivatives of the
logarithm of its moment-generating function
cn = dn ln M(k)
dkn

t=0
= (−i)n dn ln ˆP(k)
dkn

t=0
.
(13.189)
One may show (exercise 13.24) that the ﬁrst ﬁve cumulants of an arbitrary
probability distribution are
c0 = 0,
c1 = μ,
c2 = σ 2,
c3 = ν3,
and
c4 = ν4 −3σ 4
(13.190)
where the νs are its central moments (13.27). The third and fourth normalized
cumulants are the skewness ζ = c3/σ 3 = ν3/σ 3 and the kurtosis κ = c4/σ 4 =
ν4/σ 4 −3.
Example 13.13 (Gaussian cumulants)
The logarithm of the moment-
generating function (13.184) of Gauss’s distribution is μk + σ 2k2/2. Thus by
(13.189), PG(x, μ, σ) has no skewness or kurtosis, its cumulants vanish cGn = 0
for n > 2, and its fourth central moment is ν4 = 3σ 4.
13.13 Fat tails
The gaussian probability distribution PG(x, μ, σ) falls off for |x −μ| ≫σ very
fast – as exp

−(x −μ)2/2σ 2
. Many other probability distributions fall off
more slowly; they have fat tails. Rare “black-swan” events – wild ﬂuctuations,
market bubbles, and crashes – lurk in their fat tails.
Gosset’s distribution, which is known as Student’s t-distribution with ν degrees
of freedom
PS(x, ν, a) =
1
√π
((1 + ν)/2)
(ν/2)
aν
(a2 + x2)(1+ν)/2 ,
(13.191)
has power-law tails. Its even moments are
μ2n = (2n −1)!! (ν/2 −n)
(ν/2)

a2
2
n
(13.192)
for 2n < ν and inﬁnite otherwise. For ν = 1, it coincides with the Breit–Wigner
or Cauchy distribution
PS(x, 1, a) = 1
π
a
a2 + x2 ,
(13.193)
in which x = E −E0 and a = /2 is the half-width at half-maximum.
Two representative cumulative probabilities are (Bouchaud and Potters,
2003, p.15–16)
530

13.13 FAT TAILS
Pr(x, ∞) =
 ∞
x
PS(x′, 3, 1) dx′ = 1
2 + 1
π
#
arctan x +
x
1 + x2
$
, (13.194)
Pr(x, ∞) =
 ∞
x
PS(x′, 4,
√
2) dx′ = 1
2 −3
4u + 1
4u3
(13.195)
where u = x/

2 + x2 and a is picked so σ 2 = 1. William Gosset (1876–1937),
who worked for Guinness, wrote as Student because Guinness didn’t let its
employees publish.
The log-normal probability distribution on (0, ∞)
Pln(x) =
1
σx
√
2π
exp

−ln2(x/x0)
2σ 2

(13.196)
describes distributions of rates of return (Bouchaud and Potters, 2003, p. 9). Its
moments are (exercise 13.27)
μn = xn
0 en2σ 2/2.
(13.197)
The exponential distribution on [0, ∞)
Pe(x) = αe−αx
(13.198)
has (exercise 13.28) mean μ = 1/α and variance σ 2 = 1/α2. The sum of n
independent exponentially and identically distributed random variables x =
x1 + · · · + xn is distributed on [0, ∞) as (Feller, 1966, p.10)
Pn,e(x) = α (αx)n−1
(n −1)!e−αx.
(13.199)
The sum of the squares x2 = x2
1 + · · · + x2
n of n independent normally and
identically distributed random variables of zero mean and variance σ 2 gives rise
to Pearson’s chi-squared distribution on (0, ∞)
Pn,G(x, σ)dx =
√
2
σ
1
(n/2)

x
σ
√
2
n−1
e−x2/(2σ 2)dx,
(13.200)
which for x = v, n = 3, and σ 2 = kT/m is (exercise 13.29) the Maxwell–
Boltzmann distribution (13.100). In terms of χ = x/σ, it is
Pn,G(χ2/2) dχ2 =
1
(n/2)

χ2
2
n/2−1
e−χ2/2d

χ2/2

.
(13.201)
It has mean and variance
μ = n
and
σ 2 = 2n
(13.202)
and is used in the chi-squared test (Pearson, 1900).
531

PROBABILITY AND STATISTICS
Personal income, the amplitudes of catastrophes, the price changes of ﬁnan-
cial assets, and many other phenomena occur on both small and large scales.
Lévy distributions describe such multi-scale phenomena. The characteristic
function for a symmetric Lévy distribution is for ν ≤2
ˆLν(k, aν) = exp

−aν|k|ν
.
(13.203)
Its inverse Fourier transform (13.174) is for ν = 1 (exercise 13.30) the Cauchy
or Lorentz distribution
L1(x, a1) =
a1
π(x2 + a2
1)
(13.204)
and for ν = 2 the gaussian
L2(x, a2) = PG(x, μ,

2a2) =
1
σ
√
2
exp

−x2
4a2
2

(13.205)
but for other values of ν no simple expression for Lν(x, aν) is available. For
0 < ν < 2 and as x →±∞, it falls off as |x|−(1+ν), and for ν > 2 it assumes
negative values, ceasing to be a probability distribution (Bouchaud and Potters,
2003, pp. 10–13).
13.14 The central limit theorem and Jarl Lindeberg
We have seen in sections 13.7 and 13.8 that unbiased ﬂuctuations tend to dis-
tribute the position and velocity of molecules according to Gauss’s distribution
(13.75). Gaussian distributions occur very frequently. The central limit theorem
suggests why they occur so often.
Let x1, . . . , xN be N independent random variables described by probability
distributions P1(x1), . . . , PN(xN) with ﬁnite means μj and ﬁnite variances σ 2
j .
The Pjs may be all different. The central limit theorem says that as N →∞the
probability distribution P(N)(y) for the average of the xjs
y = 1
N (x1 + x2 + · · · + xN)
(13.206)
tends to a gaussian in y quite independently of what the underlying probability
distributions Pj(xj) happen to be.
Because expected values are linear (13.34), the mean value of the average y is
the average of the N means
μy = E[y] = E[(x1 + · · · + xN) /N] = 1
N (E[x1] + · · · + E[xN])
= 1
N (μ1 + · · · + μN) .
(13.207)
532

13.14 THE CENTRAL LIMIT THEOREM AND JARL LINDEBERG
Similarly, our rule (13.41) for the variance of a linear combination of indepen-
dent variables tells us that the variance of the average y is
σ 2
y = V[(x1 + · · · + xN) /N] = 1
N2

σ 2
1 + · · · + σ 2
N

.
(13.208)
The independence of the random variables x1, x2, . . . , xN implies (13.36) that
their joint probability distribution factorizes
P(x1, . . . , xN) = P1(x1)P2(x2) · · · PN(xN).
(13.209)
We can use a delta function (3.36) to write the probability distribution P(N)(y)
for the average y = (x1 + x2 + · · · + xN)/N of the xjs as
P(N)(y) =

P(x1, . . . , xN) δ((x1 + x2 + · · · + xN)/N −y) dNx
(13.210)
where dNx = dx1 . . . dxN. Its characteristic function
ˆP(N)(k) =

eiky P(N)(y) dy
=

eiky

P(x1, . . . , xN) δ((x1 + x2 + · · · + xN)/N −y) dNx dy
=

exp
#ik
N (x1 + x2 + · · · + xN)
$
P(x1, . . . , xN) dNx
=

exp
#ik
N (x1 + x2 + · · · + xN)
$
P1(x1)P2(x2) · · · PN(xN) dNx
(13.211)
is then the product
ˆP(N)(k) = ˆP1(k/N) ˆP2(k/N) · · · ˆPN(k/N)
(13.212)
of the characteristic functions
ˆPj(k/N) =

eikxj/N Pj(xj) dx
(13.213)
of the probability distributions P1(x1), . . . , PN(xN).
The Taylor series (13.187) for each characteristic function is
ˆPj(k/N) =
∞

n=0
(ik)n
n! Nn μnj
(13.214)
and so for big N we can use the approximation
ˆPj(k/N) ≈1 + ik
N μj −k2
2N2 μ2j,
(13.215)
533

PROBABILITY AND STATISTICS
in which μ2j = σ 2
j + μ2
j by the formula (13.22) for the variance. So we have
ˆPj(k/N) ≈1 + ik
N μj −k2
2N2

σ 2
j + μ2
j

(13.216)
or for large N
ˆPj(k/N) ≈exp

ik
N μj −k2
2N2 σ 2
j

.
(13.217)
Thus as N →∞, the characteristic function (13.212) for the variable y con-
verges to
ˆP(N)(k) =
N

j=1
ˆPj(k/N) =
N

j=1
exp

ik
N μj −k2
2N2 σ 2
j

= exp
⎡
⎣
N

j=1

ik
N μj −k2
2N2 σ 2
j
⎤
⎦= exp

iμyk −1
2σ 2
y k2

, (13.218)
which is the characteristic function (13.176) of a gaussian (13.175) with mean
and variance
μy = 1
N
N

j=1
μj
and
σ 2
y = 1
N2
N

j=1
σ 2
j .
(13.219)
The inverse Fourier transform (13.174) now gives the probability distribution
P(N)(y) for the average y = (x1 + x2 + · · · + xN)/N as
P(N)(y) =
 ∞
−∞
e−iky ˆP(N)(k) dk
2π ,
(13.220)
which in view of (13.218) and (13.176) tends as N →∞to Gauss’s distribution
PG(y, μy, σy)
lim
N→∞P(N)(y) =
 ∞
−∞
e−iky
lim
N→∞
ˆP(N)(k) dk
2π
=
 ∞
−∞
e−iky exp

iμyk −1
2σ 2
y k2
 dk
2π
= PG(y, μy, σy) =
1
σy
√
2π
exp

−(y −μy)2
2σ 2y

(13.221)
with mean μy and variance σ 2
y as given by (13.219). The sense in which P(N)(y)
converges to PG(y, μy, σy) is that for all a and b the probability PrN(a < y < b)
that y lies between a and b as determined by P(N)(y) converges as N →∞
534

13.14 THE CENTRAL LIMIT THEOREM AND JARL LINDEBERG
to the probability that y lies between a and b as determined by the gaussian
PG(y, μy, σy)
lim
N→∞PrN(a < y < b) = lim
N→∞
 b
a
P(N)(y) dy =
 b
a
PG(y, μy, σy) dy. (13.222)
This type of convergence is called convergence in probability (Feller, 1966,
pp. 231, 241–248).
For the special case in which all the means and variances are the same, with
μj = μ and σ 2
j = σ 2, the deﬁnitions in (13.219) imply that μy = μ and σ 2
y =
σ 2/N. In this case, one may show (exercise 13.32) that in terms of the variable
u ≡
√
N(y −μ)
σ
=
N
n=1 xj

−Nμ
√
N σ
(13.223)
P(N)(y) converges to a distribution that is normal
lim
N→∞P(N)(y) dy =
1
√
2π
e−u2/2 du.
(13.224)
To get a clearer idea of when the central limit theorem holds, let us write the
sum of the N variances as
SN ≡
N

j=1
σ 2
j =
N

j=1
 ∞
−∞
(xj −μj)2 Pj(xj) dxj
(13.225)
and the part of this sum due to the regions within δ of the means μj as
SN(δ) ≡
N

j=1
 μj+δ
μj−δ
(xj −μj)2 Pj(xj) dxj.
(13.226)
In terms of these deﬁnitions, Jarl Lindeberg (1876–1932) showed that P(N)(y)
converges (in probability) to the gaussian (13.221) as long as the part SN(δ) is
most of SN in the sense that for every ϵ > 0
lim
N→∞
SN

ϵ√SN

SN
= 1.
(13.227)
This is Lindeberg’s condition (Feller, 1968, p. 254; Feller, 1966, pp. 252–259;
Gnedenko, 1968, p. 304).
Because we dropped all but the ﬁrst three terms of the series (13.214) for
the characteristic functions ˆPj(k/N), we may infer that the convergence of the
distribution P(N)(y) to a gaussian is quickest near its mean μy. If the higher
moments μnj are big, then for ﬁnite N the distribution P(N)(y) can have tails
that are fatter than those of the limiting gaussian PG(y, μy, σy).
535

PROBABILITY AND STATISTICS
Example 13.14 (Illustration of the central limit theorem)
The simplest proba-
bility distribution is a random number x uniformly distributed on the interval
(0, 1). The probability distribution P(2)(y) of the mean of two such random
numbers is the integral
P(2)(y) =
 1
0
dx1
 1
0
dx2 δ((x1 + x2)/2 −y).
(13.228)
Letting u1 = x1/2 and u2 = x2/2, we ﬁnd
P(2)(y) = 4
 min(y, 1
2 )
max(0,y−1
2 )
θ( 1
2 +u1−y) du1 = 4y θ( 1
2 −y)+4(1−y) θ(y−1
2), (13.229)
which is the dot-dashed triangle in Fig. 13.4. The probability distribution P(4)(y)
is the dashed somewhat gaussian curve in the ﬁgure, while P(8)(y) is the solid,
nearly gaussian curve.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.5
1
1.5
2
2.5
3
3.5
4
Central limit theorem for uniform random numbers
y
P (N)(y)
Figure 13.4
The probability distributions P(N)(y) (equation 13.210) for the mean
y = (x1 + · · · + xN)/N of N random variables drawn from the uniform distribution
are plotted for N = 1 (dots), 2 (dot dash), 4 (dashes), and 8 (solid). The distribu-
tions P(N)(y) rapidly approach gaussians with the same mean μy = 1/2 but with
shrinking variances σ 2 = 1/12N.
536

13.15 RANDOM-NUMBER GENERATORS
To work through a more complicated example of the central limit theorem,
we ﬁrst need to learn how to generate random numbers that follow an arbitrary
distribution.
13.15 Random-number generators
To generate truly random numbers, one might use decaying nuclei or an
electronic device that makes white noise. But people usually settle for pseudo-
random numbers computed by a mathematical algorithm. Such algorithms are
deterministic, so the numbers they generate are not truly random. But for most
purposes, they are random enough.
The easiest way to generate pseudo-random numbers is to use a random-
number algorithm that is part of one’s favorite FORTRAN, C, or C++ compiler.
To run it, one ﬁrst gives it a random starting point called a seed, which is a num-
ber or a vector. For instance, to start the GNU or Intel FORTRAN90 compiler,
one includes in the code the line
call random_seed()
before using the line
call random_number(x)
to generate a random number x uniformly distributed on the interval 0 < x < 1,
or an array of such random numbers.
Some applications require random numbers of very high quality. For
such applications, one might use Lüscher’s RANLUX (Lüscher, 1994; James,
1994).
Most random-number generators are periodic with very long periods. The
Mersenne twister (Saito and Matsumoto, 2007) has the exceptionally long
period 219937 −1 ≳4.3 × 106001. Matlab uses it.
Random-number generators distribute random numbers uniformly on the
interval (0, 1). How do we make them follow an arbitrary distribution P(x)? If
the distribution is strictly positive P(x) > 0 on the relevant interval (a, b), then
its integral
F(x) =
 x
a
P(x′) dx′
(13.230)
is a strictly increasing function on (a, b), that is, a < x < y < b implies F(x) <
F(y). Moreover, the function F(x) rises from F(a) = 0 to F(b) = 1 and takes on
every value 0 < y < 1 for exactly one x in the interval (a, b). Thus the inverse
function F−1(y)
x = F−1(y)
if and only if
y = F(x)
(13.231)
537

PROBABILITY AND STATISTICS
is well deﬁned on the interval (0, 1).
Our random-number generator gives us random numbers u that are uniform
on (0, 1). We want a random variable r whose probability Pr(r < x) of being
less than x is F(x). The trick (Knuth, 1981, p. 116) is to set
r = F−1(u)
(13.232)
so that Pr(r < x) = Pr(F−1(u) < x). For by (13.231) F−1(u) < x if and only if
u < F(x). So Pr(r < x) = Pr(F−1(u) < x) = Pr(u < F(x)) = F(x). The trick
works.
Example 13.15 (P(r) = 3r2)
To turn a distribution of random numbers u uni-
form on (0, 1) into a distribution P(r) = 3r2 of random numbers r, we integrate
and ﬁnd
F(x) =
 x
0
P(x′) dx′ =
 x
0
3x′2 dx′ = x3.
(13.233)
We then set r = F−1(u) = u1/3.
13.16 Illustration of the central limit theorem
To make things simple, we’ll take all the probability distributions Pj(x) to be the
same and equal to Pj(xj) = 3x2
j on the interval (0, 1) and zero elsewhere. Our
random-number generator gives us random numbers u that are uniformly dis-
tributed on (0, 1), so by the example (13.15) the variable r = u1/3 is distributed
as Pj(x) = 3x2.
The central limit theorem tells us that the distribution
P(N)(y) =

3x2
1 3x2
2 . . . 3x2
N δ((x1 + x2 + · · · + xN)/N −y) dNx
(13.234)
of the mean y = (x1 + · · · + xN)/N tends as N →∞to Gauss’s distribution
lim
N→∞P(N)(y) =
1
σy
√
2π
exp

−(x −μy)2
2σ 2y

(13.235)
with mean μy and variance σ 2
y given by (13.219). Since the Pjs are all the same,
they all have the same mean
μy = μj =
 1
0
3x3dx = 3
4
(13.236)
and the same variance
538

13.16 ILLUSTRATION OF THE CENTRAL LIMIT THEOREM
σ 2
j =
 1
0
3x4dx −
3
4
2
= 3
5 −9
16 = 3
80.
(13.237)
By (13.219), the variance of the mean y is then σ 2
y
= 3/80N. Thus as N
increases, the mean y tends to a gaussian with mean μy = 3/4 and ever
narrower peaks.
For N = 1, the probability distribution P(1)(y) is
P(1)(y) =

3x2
1 δ(x1 −y) dx1 = 3y2,
(13.238)
which is the probability distribution we started with. In Fig. 13.5, this is the
quadratic, dotted curve.
For N = 2, the probability distribution P(1)(y) is (exercise 13.31)
P(2)(y) =

3x2
1 3x2
2 δ((x1 + x2)/2 −y) dx1 dx2
= θ( 1
2 −y) 96
5 y5 + θ(y −1
2)
36
5 −96
5 y5 + 48y2 −36y

. (13.239)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
1
2
3
4
5
6
Illustration of the central limit theorem
y
P (N)(y)
Figure 13.5
The probability distributions P(N)(y) (equation 13.234) for the mean y =
(x1 + · · · + xN)/N of N random variables drawn from the quadratic distribution P(x) =
3x2 are plotted for N = 1 (dots), 2 (dot dash), 4 (dashes), and 8 (solid). The four
distributions P(N)(y) rapidly approach gaussians with the same mean μy = 3/4 but
with shrinking variances σ 2y = 3/80N.
539

PROBABILITY AND STATISTICS
The probability distributions P(N)(y) for N = 2j can be obtained by running
the FORTAN95 program
program clt
implicit none ! avoids typos
character(len=1)::ch_i1
integer,parameter::dp = kind(1.d0) !define double
precision
integer::j,k,n,m
integer,dimension(100)::plot = 0
real(dp)::y
real(dp),dimension(100)::rplot
real(dp),allocatable,dimension(:)::r,u
real(dp),parameter::onethird = 1.d0/3.d0
write(6,*)’What is j?’; read(5,*) j
allocate(u(2**j),r(2**j))
call init_random_seed() ! set new seed, see below
do k = 1, 10000000
! Make the N = 2**j plot
call random_number(u)
r = u**onethird
y = sum(r)/2**j
n = 100*y + 1
plot(n) = plot(n) + 1
end do
rplot = 100*real(plot)/sum(plot)
write(ch_i1,"(i1)") j ! turns integer j into
character ch_i1
open(7,file=’plot’//ch_i1) ! opens and names files
do m = 1, 100
write(7,*) 0.01d0*(m-0.5), rplot(m)
end do
end program clt
subroutine init_random_seed()
implicit none
integer i, n, clock
integer, dimension(:), allocatable :: seed
call random_seed(size = n) ! find size of seed
allocate(seed(n))
call system_clock(count=clock)!get time of processor
clock
seed = clock + 37 * (/ (i-1, i=1, n) /) ! make seed
call random_seed(put=seed) ! set seed
540

13.16 ILLUSTRATION OF THE CENTRAL LIMIT THEOREM
deallocate(seed)
end subroutine init_random_seed
The distributions P(N)(y) for N = 1, 2, 4, and 8 are plotted in Fig. 13.5.
P(1)(y) = 3y2 is the original distribution. P(2)(y) is trying to be a gaussian,
while P(4)(y) and P(8)(y) have almost succeeded. The variance σ 2
y = 3/80N
shrinks with N.
Although FORTRAN95 is an ideal language for computation, C++ is more
versatile, more modular, and more suited to large projects involving many
programmers. An equivalent C++ code written by Sean Cahill is:
#include <stdlib.h>
#include <time.h>
#include <math.h>
#include <string>
#include <iostream>
#include <fstream>
#include <sstream>
#include <iomanip>
#include <valarray>
using namespace std;
// Fills the array val with random numbers between
0 and 1 void rand01(valarray<double>& val)
{
// Records the size
unsigned int size = val.size();
// Loops through the size
unsigned int i=0;
for (i=0; i<size; i++)
{
// Generates a random number between 0 and 1
val[i] = static_cast<double>(rand()) / RAND_MAX;
}
}
void clt ()
{
// Declares local constants
const int PLOT_SIZE
= 100;
541

PROBABILITY AND STATISTICS
const int LOOP_CALC_ITR
= 10000000;
const double ONE_THIRD
= 1.0 / 3.0;
// Inits local variables
double y=0;
int i=0, j=0, n=0;
// Gets the value of J
cout << "What is J? ";
cin >> j;
// Bases the vec size on J
const int VEC_SIZE = static_cast<int>(pow(2.0,j));
// Inits vectors
valarray<double> plot(PLOT_SIZE);
valarray<double> rplot(PLOT_SIZE);
valarray<double> r(VEC_SIZE);
// Seeds random number generator
srand ( time(NULL) );
// Performs the calculations
for (i=0; i<LOOP_CALC_ITR; i++)
{
rand01(r);
r = pow(r, ONE_THIRD);
y = r.sum() / VEC_SIZE;
n = static_cast<int>(100 * y);
plot[n]++;
}
// Normalizes RPLOT
rplot = plot * (100.0 / plot.sum());
// Opens a data file
ostringstream fileName;
fileName << "plot_" << j << ".txt";
ofstream fileHandle;
fileHandle.open (fileName.str().c_str());
// Sets precision
542

13.17 MEASUREMENTS, ESTIMATORS, AND FRIEDRICH BESSEL
fileHandle.setf(ios::fixed,ios::floatfield);
fileHandle.precision(7);
// Writes the data to a file
for (i=1; i<=PLOT_SIZE; i++)
fileHandle << 0.01*(i-0.5) << "
" << rplot[i-1]
<< endl;
// Closes the data file
fileHandle.close();
}
13.17 Measurements, estimators, and Friedrich Bessel
A probability distribution P(x; θ) for a stochastic variable x may depend upon
one or more unknown parameters θ = (θ1, . . . , θm) such as the mean μ and the
variance σ 2.
Experimenters seek to determine the unknown parameters θ by collecting
data in the form of values x = x1, . . . , xN of the stochastic variable x. They
assume that the probability distribution for the sequence x = (x1, . . . , xN) is
the product of N factors of the physical distribution P(x; θ)
P(x; θ) =
N

j=1
P(xj; θ).
(13.240)
They approximate the unknown value of a parameter θℓas the mean value of
an estimator u(N)
ℓ
(x) of θℓ
E[u(N)
ℓ
] =

u(N)
ℓ
(x) P(x; θ) dNx = θℓ+ b(N)
ℓ
,
(13.241)
in which the bias b(N)
ℓ
depends upon θ and N. If as N →∞, the bias b(N)
ℓ
→0,
then the estimator u(N)
ℓ
(x) is consistent.
Inasmuch as the mean (13.25) is the integral of the physical distribution
μ =

x P(x; θ) dx
(13.242)
a natural estimator for the mean is
u(N)
μ (x) = (x1 + · · · + xN)/N.
(13.243)
Its expected value is
E[u(N)
μ ] =

u(N)
μ (x) P(x; θ) dNx =
 x1 + · · · + xN
N
P(x; θ) dNx
543

PROBABILITY AND STATISTICS
= 1
N
N

k=1

xk P(xk; θ) dxk
N

k̸=j=1

P(xj; θ) dxj
= 1
N
N

k=1
μ = μ.
(13.244)
Thus the natural estimator u(N)
μ (x) of the mean (13.243) has b(N)
ℓ
= 0, and so it
is a consistent and unbiased estimator for the mean.
Since the variance (13.28) of the probability distribution P(x; θ) is the
integral
σ 2 =

(x −μ)2 P(x; θ) dx,
(13.245)
that of the estimator uN
μ is
V[uN
μ ] =
 
u(N)
μ (x) −μ
2
P(x; θ) dNx =
 ⎡
⎣1
N
N

j=1

xj −μ

⎤
⎦
2
P(x; θ) dNx
= 1
N2
N

j,k=1
 
xj −μ

(xk −μ) P(x; θ) dNx
= 1
N2
N

j,k=1
δjk
 
xj −μ
2 P(x; θ) dNx = 1
N2
N

j,k=1
δjk σ 2 = σ 2
N , (13.246)
in which σ 2 is the variance (13.245) of the physical distribution P(x; θ). We’ll
learn in the next section that no estimator of the mean can have a lower variance
than this.
A natural estimator for the variance of the probability distribution P(x; θ) is
u(N)
σ 2 (x) = B
N

j=1

xj −u(N)
μ (x)
2
,
(13.247)
in which B = B(N) is a constant of proportionality. The naive choice B(N) =
1/N leads to a biased estimator. To ﬁnd the correct value of B, we set the
expected value E[u(N)
σ 2 ] equal to σ 2
E[u(N)
σ 2 ] =

B
N

j=1

xj −u(N)
μ (x)
2
P(x; θ) dNx = σ 2
(13.248)
544

13.17 MEASUREMENTS, ESTIMATORS, AND FRIEDRICH BESSEL
and solve for B. Subtracting the mean μ from both xj and u(N)
μ (x), we express
σ 2/B as the sum of three terms
σ 2
B =
N

j=1
 
xj −μ −

u(N)
μ (x) −μ
2
P(x; θ) dNx = Sjj+Sjμ+Sμμ (13.249)
the ﬁrst of which is
Sjj =
N

j=1
 
xj −μ
2 P(x; θ) dNx = Nσ 2.
(13.250)
The cross-term Sjμ is
Sjμ = −2
N

j=1
 
xj −μ
 
u(N)
μ (x) −μ

P(x; θ) dNx
(13.251)
= −2
N
N

j=1
 
xj −μ
 N

k=1
(xk −μ) P(x; θ) dNx = −2σ 2.
The third term is related to the variance (13.246)
Sμμ =
N

j=1
 
u(N)
μ (x) −μ
2
P(x; θ) dNx = NV[uN
μ ] = σ 2.
(13.252)
Thus the factor B must satisfy
σ 2/B = Nσ 2 −2σ 2 + σ 2 = (N −1)σ 2,
(13.253)
which tells us that B = 1/(N −1), which is Bessel’s correction. Our estimator
for the variance of the probability distribution P(x; θ) then is
u(N)
σ 2 (x) =
1
N −1
N

j=1

xj −u(N)
μ (x)
2
=
1
N −1
N

j=1

xj −1
N
N

k=1
xk
2
.
(13.254)
It is consistent and unbiased since E[u(N)
σ 2 ] = σ 2 by construction (13.248). It
gives for the variance σ 2 of a single measurement the undeﬁned ratio 0/0, as it
should, whereas the naive choice B = 1/N absurdly gives zero.
On the basis of N measurements x1, . . . , xN we can estimate the mean of the
unknown probability distribution P(x; θ) as μN = (x1 + · · · + xN)/N. And we
can use Bessel’s formula (13.254) to estimate the variance σ 2
N of the unknown
545

PROBABILITY AND STATISTICS
distribution P(x; θ). Our formula (13.246) for the variance σ 2(μN) of the mean
μN then gives
σ 2(μN) = σ 2
N
N =
1
N(N −1)
N

j=1

xj −1
N
N

k=1
xk
2
.
(13.255)
Thus we can use N measurements xj to estimate the mean μ to within a
standard error or standard deviation of
σ(μN) =
(
σ 2
N
N =
9
:
:
:
;
1
N(N −1)
N

j=1

xj −1
N
N

k=1
xk
2
.
(13.256)
Few formulas have seen so much use.
13.18 Information and Ronald Fisher
The Fisher information matrix of a distribution P(x; θ) is the mean of products
of its partial logarithmic derivatives
Fkℓ(θ) ≡E
#∂ln P(x; θ)
∂θk
∂ln P(x; θ)
∂θℓ
$
=
 ∂ln P(x; θ)
∂θk
∂ln P(x; θ)
∂θℓ
P(x; θ) dNx
(13.257)
(Ronald Fisher, 1890–1962). Fisher’s matrix (exercise 13.33) is symmetric Fkℓ=
Fℓk and nonnegative (1.38), and when it is positive (1.39), it has an inverse. By
differentiating the normalization condition

P(x; θ) dNx = 1
(13.258)
we have
0 =
 ∂P(x; θ)
∂θk
dNx =
 ∂ln P(x; θ)
∂θk
P(x; θ) dNx,
(13.259)
which says that the mean value of the logarithmic derivative of the proba-
bility distribution, a quantity called the score, vanishes. Using the notation
P,k ≡∂P/∂θk and (ln P),k ≡∂ln P/∂θk and differentiating again, one has
(exercise 13.34)

(ln P),k (ln P),ℓP dNx = −

(ln P),k,ℓP dNx
(13.260)
so that another form of Fisher’s information matrix is
Fkℓ(θ) = −E

(ln P),k,ℓ

= −

(ln P),k,ℓP dNx.
(13.261)
546

13.18 INFORMATION AND RONALD FISHER
Cramér and Rao used Fisher’s information matrix to form a lower bound
on the covariance (13.35) matrix C[uk, uℓ] of any two estimators. To see how
this works, we use the vanishing (13.259) of the mean of the score to write the
covariance of the kth score Vk ≡(ln P(x; θ)),k with the ℓth estimator uℓ(x) as a
derivative ⟨uℓ⟩,k of the mean ⟨uℓ⟩
C[Vk, uℓ] =

(ln P),k (uℓ−bℓ−θℓ) P dNx =

(ln P),k uℓP dNx
=

P,k(x; θ) uℓ(x) dNx = ⟨uℓ⟩,k.
(13.262)
Thus for any two sets of constants yk and wℓ, we have with P =
√
P
√
P
m

ℓ,k=1
yk ∂k⟨uℓ⟩wℓ=

m

ℓ,k=1
yk (ln P),k
√
P (uℓ−bℓ−θℓ) wℓ
√
P dNx. (13.263)
We can suppress some indices by grouping the yjs, the wjs, and so forth into
the vectors Y T = (y1, . . . , ym), W T = (w1, . . . , wm), UT = (u1, . . . , um), BT =
(b1, . . . , bm), and T = (θ1, . . . , θm), and by grouping the ∂k⟨uℓ⟩s into a matrix
(∇U)kl, which by (13.241) is
(∇U)kl ≡∂k⟨uℓ⟩= ∂k (θℓ+ bℓ) = δkl + ∂kbℓ.
(13.264)
In this compact notation, our relation (13.263) is
Y T ∇U W =

Y T(∇ln P)
√
P (UT −BT −T)W
√
P dNx.
(13.265)
Squaring, we apply Schwarz’s inequality (6.379)

Y T ∇U W
2 =
#
Y T(∇ln P)
√
P (UT −BT −T)W
√
P dNx
$2
≤
 
Y T(∇ln P)
√
P
2
dNx
 
(UT −BT −T)W
√
P
2
dNx
=
 
Y T∇ln P
2 P dNx
 
(UT −BT −T)W
2 P dNx.
(13.266)
In the last line, we recognize the ﬁrst integral as Y TFY, where F is Fisher’s
matrix (13.257), and the second as W TCW in which C is the covariance of the
estimators
Ckℓ≡C[U, U]kℓ= C[uk −bk −θk, uℓ−bℓ−θℓ].
(13.267)
547

PROBABILITY AND STATISTICS
So (13.266) says

Y T∇UW
2 ≤Y TFY W TCW.
(13.268)
Thus as long as the symmetric nonnegative matrix F is positive and so has an
inverse, we can set the arbitrary constant vector Y = F−1∇U W and get
(W T∇UTF−1∇UW)2 ≤W T∇UTF−1∇UW W TCW.
(13.269)
Canceling a common factor, we obtain the Cramér–Rao inequality
W TC W ≥W T∇UTF−1 ∇U W
(13.270)
often written as
C ≥∇UT F−1 ∇U.
(13.271)
By (13.264), the matrix ∇U is the identity matrix I plus the gradient of the
bias B
∇U = I + ∇B.
(13.272)
Thus another form of the Cramér–Rao inequality is
C ≥(I + ∇B) T F−1 (I + ∇B)
(13.273)
or in terms of the arbitrary vector W
W TC W ≥W T (I + ∇B) T F−1 (I + ∇B) W.
(13.274)
Letting the arbitrary vector W be Wj = δjk, one arrives at (exercise 13.35) the
Cramér–Rao lower bound on the variance V[uk] = C[uk, uk]
V[uk] ≥(F−1)kk +
m

ℓ=1
2(F−1)kℓ∂lbk +
m

ℓ,j=1
(F−1)ℓj∂lbk∂jbk.
(13.275)
If the estimator uk(x) is unbiased, then this lower bound simpliﬁes to
V[uk] ≥(F−1)kk.
(13.276)
Example 13.16 (Cramér–Rao bound for a gaussian)
The elements of Fisher’s
information matrix for the mean μ and variance σ 2 of Gauss’s distribution for
N data points x1, . . . , xN
P(N)
G (x, μ, σ) =
N

j=1
PG(xj; μ, σ) =

1
σ
√
2π
N
exp
⎛
⎝−
N

j=1
(xj −μ)2
2σ 2
⎞
⎠
(13.277)
548

13.18 INFORMATION AND RONALD FISHER
are
Fμμ =
 #
ln P(N)
G (x, μ, σ)

,μ
$2
P(N)
G (x, μ, σ) dNx
=
N

i,j=1
 xi −μ
σ 2
 xj −μ
σ 2

P(N)
G (x, μ, σ) dNx
=
N

i=1
 xi −μ
σ 2
2
P(N)
G (x, μ, σ) dNx = N
σ 2 ,
(13.278)
Fμσ 2 =

(ln P(N)
G (x, μ, σ)),μ (ln P(N)
G (x, μ, σ)),σ 2 P(N)
G (x, μ, σ) dNx
=
N

i,j=1
 #xi −μ
σ 2
$ 
(xj −μ)2
2σ 4
−
1
2σ 2

P(N)
G (x, μ, σ) dNx = 0,
Fσ 2μ = Fμσ 2 = 0, and
Fσ 2σ 2 =
 
(ln P(N)
G (x, μ, σ)),σ 2
2
P(N)
G (x, μ, σ) dNx
=
N

i,j=1
 
(xi −μ)2
2σ 4
−
1
2σ 2
 
(xj −μ)2
2σ 4
−
1
2σ 2

P(N)
G (x, μ, σ) dNx
= N
2σ 4 .
(13.279)
The inverse of Fisher’s matrix then is diagonal with (F−1)μμ = σ 2/N and
(F−1)σ 2σ 2 = 2σ 4/N.
The variance of any unbiased estimator uμ(x) of the mean must exceed its
Cramér–Rao lower bound (13.276), and so V[uμ] ≥(F−1)μμ = σ 2/N. The
variance V[u(N)
μ ] of the natural estimator of the mean u(N)
μ (x) = (x1+· · ·+xN)/N
is σ 2/N by (13.246), and so it respects and saturates the lower bound (13.276)
V[u(N)
μ ] = E[(u(N)
μ
−μ)2] = σ 2/N = (F−1)μμ.
(13.280)
One may show (exercise 13.36) that the variance V[u(N)
σ 2 ] of Bessel’s estimator
(13.254) of the variance is (Riley et al., 2006, p. 1248)
V[u(N)
σ 2 ] = 1
N

ν4 −N −3
N −1σ 4

(13.281)
where ν4 is the fourth central moment (13.26) of the probability distribution.
For the gaussian PG(x; μ, σ) one may show (exercise 13.37) that this moment is
ν4 = 3σ 4, and so for it
VG[u(N)
σ 2 ] =
2
N −1 σ 4.
(13.282)
549

PROBABILITY AND STATISTICS
Thus the variance of Bessel’s estimator of the variance respects but does not
saturate its Cramér–Rao lower bound (13.276, 13.279)
VG[u(N)
σ 2 ] =
2
N −1 σ 4 > 2
N σ 4.
(13.283)
Estimators that saturate their Cramér–Rao lower bounds are efﬁcient. The
natural estimator u(N)
μ (x) of the mean is efﬁcient as well as consistent and unbi-
ased, and Bessel’s estimator u(N)
σ 2 (x) of the variance is consistent and unbiased
but not efﬁcient.
13.19 Maximum likelihood
Suppose we measure some quantity x at various values of another variable t
and ﬁnd the values x1, x2, . . . , xN at the known points t1, t2, . . . , tN. We might
want to ﬁt these measurements to a curve x = f (t; α) where α = α1, . . . , αM is
a set of M < N parameters. In view of the central limit theorem, we’ll assume
that the points xj fall in Gauss’s distribution about the values xj = f (tj; α) with
some known variance σ 2. The probability of getting the N values x1, . . . , xN
then is
P(x) =
N

j=1
P(xj, tj, σ) =

1
σ
√
2π
N
exp
⎛
⎝−
N

j=1
(xj −f (tj; α))2
2σ 2
⎞
⎠.
(13.284)
To ﬁnd the M parameters α, we maximize the likelihood P(x) by minimizing
the argument of its exponential
0 =
∂
∂αℓ
N

j=1

xj −f (tj; α)
2 = −2
N

j=1

xj −f (tj; α)
 ∂f (tj; α)
∂αℓ
.
(13.285)
If the function f (t; α) depends nonlinearly upon the parameters α, then we may
need to use numerical methods to solve this least-squares problem.
But if the function f (t; α) depends linearly upon the M parameters α
f (t; α) =
M

k=1
gk(t) αk
(13.286)
then the equations (13.285) that determine these parameters α are linear
0 =
N

j=1

xj −
M

k=1
gk(tj) αk

gℓ(tj).
(13.287)
550

13.20 KARL PEARSON’S CHI-SQUARED STATISTIC
In matrix notation with G the N × M rectangular matrix with entries Gjk =
gk(tj), they are
GT x = GT Gα.
(13.288)
The basis functions gk(t) may depend nonlinearly upon the independent vari-
able t. If one chooses them to be sufﬁciently different that the columns of G are
linearly independent, then the rank of G is M, and the nonnegative matrix GT G
has an inverse. The matrix G then has a pseudoinverse (1.397)
G+ =

GT G
−1 GT
(13.289)
and it maps the N-vector x into our parameters α
α = G+ x.
(13.290)
The product G+ G = IM is the M × M identity matrix, while
G G+ = P
(13.291)
is an N × N projection operator (exercise 13.38) onto the M × M subspace
for which G+G = IM is the identity operator. Like all projection operators, P
satisﬁes P2 = P.
13.20 Karl Pearson’s chi-squared statistic
The argument of the exponential (13.284) in P(x) is (the negative of) Karl
Pearson’s chi-squared statistic (Pearson, 1900)
χ2 ≡
N

j=1
(xj −f (tj; α))2
2σ 2
.
(13.292)
When the function f (t; α) is linear (13.286) in α, the N-vector f (tj; α) is f = G α.
Pearson’s χ2 then is
χ2 = (x −G α)2/2σ 2.
(13.293)
Now (13.290) tells us that α = G+ x, and so in terms of the projection operator
P = G G+, the vector x −G α is
x −G α = x −G G+ x =

I −G G+
x = (I −P) x.
(13.294)
So χ2 is proportional to the squared length
χ2 = ˜x2/2σ 2
(13.295)
of the vector
˜x ≡(I −P) x.
(13.296)
551

PROBABILITY AND STATISTICS
Thus if the matrix G has rank M, and the vector x has N independent
components, then the vector ˜x has only N −M independent components.
Example 13.17 (Two position measurements)
Suppose we measure a position
twice with error σ and get x1 and x2. Then the single parameter α is their average
α = (x1 + x2)/2, and χ2 is
χ2 =
3
[x1 −(x1 + x2)/2]2 + [x2 −(x1 + x2)/2]24<
2σ 2
=
3
[(x1 −x2)/2]2 + [(x2 −x1)/2]24<
2σ 2
=

(x1 −x2)/
√
2
2 <
2σ 2.
(13.297)
Thus instead of having two independent components x1 and x2, χ2 just has one
(x1 −x2)/
√
2.
We can see how this happens more generally if we use as basis vectors the
N −M orthonormal vectors |j⟩in the kernel of P (that is, the |j⟩s annihilated
by P)
P|j⟩= 0,
1 ≤j ≤N −M
(13.298)
and the M that lie in the range of the projection operator P
P|k⟩= |k⟩,
N −M + 1 ≤k ≤N.
(13.299)
In terms of these basis vectors, the N-vector x is
x =
N−M

j=1
xj|j⟩+
N

k=N−M+1
xk|k⟩
(13.300)
and the last M components of the vector ˜x vanish
˜x = (I −P) x =
N−M

j=1
xj|j⟩.
(13.301)
Example 13.18 (N position measurements)
Suppose the N values of xj are the
measured values of the position f (tj; α) = α of some object. Then M = 1, and
Gj1 = g1(tj) = 1 for j = 1, . . . , N. Now GTG = N is a 1 × 1 matrix, the number
N, and the parameter α is the mean x
α = G+ x =

GT G
−1 GT x = 1
N
N

j=1
xj = x
(13.302)
552

13.20 KARL PEARSON’S CHI-SQUARED STATISTIC
of the N position measurements xj. So the vector ˜x has components ˜xj = xj −x
and is orthogonal to GT = (1, 1, . . . , 1)
GT ˜x =
⎛
⎝
N

j=1
xj
⎞
⎠−Nx = 0.
(13.303)
The matrix GT has rank 1, and the vector ˜x has N −1 independent components.
Suppose now that we have determined our M parameters α and have a
theoretical ﬁt
x = f (t; α) =
M

k=1
gk(t) αk,
(13.304)
which when we apply it to N measurements xj gives χ2 as
χ2 = (˜x)2 /2σ 2.
(13.305)
How good is our ﬁt?
A χ2 distribution with N −M degrees of freedom has by (13.202) mean
E[χ2] = N −M
(13.306)
and variance
V[χ2] = 2(N −M).
(13.307)
So our χ2 should be about
χ2 ≈N −M ±

2(N −M).
(13.308)
If it lies within this range, then (13.304) is a good ﬁt to the data. But if it exceeds
N −M +

2(N −M), then the ﬁt isn’t so good. On the other hand, if χ2 is
less than N −M −

2(N −M), then we may have used too many parameters.
Indeed, by using N parameters with G G+ = IN, we could get χ2 = 0 every
time.
The probability that χ2 exceeds χ2
0 is the integral (13.201)
Prn(χ2 > χ2
0) =
 ∞
χ2
0
Pn(χ2/2) dχ2 =
 ∞
χ2
0
1
2(n/2)

χ2
2
n/2−1
e−χ2/2dχ2,
(13.309)
in which n = N −M is the number of data points minus the number of param-
eters, and (n/2) is the gamma function (5.102, 4.62). So an M-parameter ﬁt
to N data points has only a chance of ϵ of being right if its χ2 is greater than a
χ2
0 for which PrN−M(χ2 > χ2
0) = ϵ. These probabilities PrN−M(χ2 > χ2
0) are
553

PROBABILITY AND STATISTICS
0
5
10
15
20
25
30
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
The chi-squared test
χ2
0
Prn(χ2 > χ2
0)
n = 2
4
6
8
10
Figure 13.6
The probabilities Prn(χ2 > χ2
0 ) are plotted from left to right for n =
N −M = 2, 4, 6, 8, and 10 degrees of freedom as functions of χ2
0 .
plotted in Fig. 13.6 for n = N −M = 2, 4, 6, 8, and 10. In particular, the prob-
ability of a value of χ2 greater than χ2
0 = 20 respectively is 0.000045, 0.000499,
0.00277, 0.010336, and 0.029253 for n = N −M = 2, 4, 6, 8, and 10.
13.21 Kolmogorov’s test
Suppose we want to use a sequence of N measurements xj to determine
the probability distribution that they come from. Our empirical probability
distribution is
P(N)
e
(x) = 1
N
N

j=1
δ(x −xj).
(13.310)
Our cumulative probability for events less than x then is
Pr(N)
e
(−∞, x) =
 x
−∞
P(N)
e
(x′) dx′ =
 x
−∞
1
N
N

j=1
δ(x′ −xj) dx′.
(13.311)
So if we label our events in increasing order x1 ≤x2 ≤· · · ≤xN, then the
probability of an event less than x is
554

13.21 KOLMOGOROV’S TEST
Pr(N)
e
(−∞, x) = j
N
for
xj < x < xj+1.
(13.312)
Having approximately and experimentally determined our empirical cumu-
lative probability distribution Pr(N)
e
(−∞, x), we might want to know whether it
comes from some hypothetical, theoretical cumulative probability distribution
Prt(−∞, x). One way to do this is to compute the distance DN between the two
cumulative probability distributions
DN =
sup
−∞<x<∞
Pr(N)
e
(−∞, x) −Prt(−∞, x)
 ,
(13.313)
in which sup stands for supremum and means least upper bound. Since cumu-
lative probabilities lie between zero and one, it follows (exercise 13.39) that the
Kolmogorov distance is bounded by
0 ≤DN ≤1.
(13.314)
The simpler Smirnov distances
D+
N =
sup
−∞<x<∞

Pr(N)
e
(−∞, x) −Prt(−∞, x)

,
D−
N =
sup
−∞<x<∞

Prt(−∞, x) −Pr(N)
e
(−∞, x)

(13.315)
provide (exercise 13.40) an expression for DN as the greater of the two
DN = max(D+
N, D−
N).
(13.316)
Using our explicit expression (13.312) for the empirical cumulative probability
Pr(N)
e
(−∞, x) and the monotonicity (13.30) of cumulative probabilities such as
Prt(−∞, x), one may show (exercise 13.41) that the Smirnov distances are given
by
D+
N = sup
1≤j≤N
 j
N −Prt(−∞, xj)

D−
N = sup
1≤j≤N

Prt(−∞, xj) −j −1
N

.
(13.317)
In general, as the number N of data points increases, we expect that
our empirical distribution Pr(N)
e
(−∞, x) should approach the actual empiri-
cal distribution Pre(−∞, x) from which the events xj came. In this case, the
Kolmogorov distance DN should converge to a limiting value D∞
lim
N→∞DN = D∞=
sup
−∞<x<∞
|Pre(−∞, x) −Prt(−∞, x)| ∈[0, 1].
(13.318)
555

PROBABILITY AND STATISTICS
If the empirical distribution Pre(−∞, x) is the same as the theoretical distribu-
tion Prt(−∞, x), then we expect that D∞= 0. This expectation is conﬁrmed by
a theorem due to Glivenko (Glivenko, 1933; Cantelli, 1933) according to which
the probability that the Kolmogorov distance DN should go to zero as N →∞
is unity
Pr(D∞= 0) = 1.
(13.319)
The real issue is how fast DN should decrease with N if our events xj
do come from Prt(−∞, x). This question was answered by Kolmogorov, who
showed (Kolmogorov, 1933) that if the theoretical distribution Prt(−∞, x) is
continuous, then for large N (and for u > 0) the probability that
√
N DN is less
than u is given by the Kolmogorov function K(u)
lim
N→∞Pr(
√
N DN < u) = K(u) ≡1 + 2
∞

k=1
(−1)ke−2k2u2,
(13.320)
which is universal and independent of the particular probability distributions
Pre(−∞, x) and Prt(−∞, x).
On the other hand, if our events xj come from a different probability dis-
tribution Pre(−∞, x), then as N →∞we should expect that Pr(N)
e
(−∞, x) →
Pre(−∞, x), and so that DN should converge to a positive constant D∞∈(0, 1].
In this case, we expect that as N →∞the quantity
√
N DN should grow with
N as
√
N D∞.
Example 13.19 (Kolmogorov’s test)
How do we use (13.320)? As illustrated in
Fig. 13.7, Kolmogorov’s distribution K(u) rises from zero to unity on (0, ∞),
reaching 0.9993 already at u = 2. So if our points xj come from the theoretical
distribution, then Kolmogorov’s theorem (13.320) tells us that as N →∞, the
probability that
√
N DN is less than 2 is more than 99.9%. But if the experimen-
tal points xj do not come from the theoretical distribution, then the quantity
√
N DN should grow as
√
N D∞as N →∞.
To see what this means in practice, I took as the theoretical distribution
Pt(x) = PG(x, 0, 1), which has the cumulative probability distribution (13.85)
Prt(−∞, x) = 1
2

erf

x/
√
2

+ 1

.
(13.321)
I generated N = 10m points xj for m = 1, 2, 3, 4, 5, and 6 from the theoretical dis-
tribution Pt(x) = PG(x, 0, 1) and computed uN =
√
10m D10m for these points.
I found
√
10m D10m = 0.6928, 0.7074, 1.2000, 0.7356, 1.2260, and 1.0683. All
were less than 2, as expected since I had taken the experimental points xj from
the theoretical distribution.
To see what happens when the experimental points do not come from the
theoretical distribution Pt(x) = PG(x, 0, 1), I generated N = 10m points xj
556

13.21 KOLMOGOROV’S TEST
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Kolmogorov’s distribution
u
K(u)
Figure 13.7
Kolmogorov’s cumulative probability distribution K(u) deﬁned by
(13.320) rises from zero to unity as u runs from zero to about two.
for m = 1, 2, 3, 4, 5, and 6 from Gosset’s Student’s distribution PS(x, 3, 1) deﬁned
by (13.191) with ν = 3 and a = 1. Both Pt(x) = PG(x, 0, 1) and PS(x, 3, 1)
have the same mean μ = 0 and standard deviation σ = 1, as illustrated in
Fig. 13.8. For these points, I computed uN =
√
N DN and found
√
10m D10m =
0.7741, 1.4522, 3.3837, 9.0478, 27.6414, and 87.8147. Only the ﬁrst two are less
than 2, and the last four grow as
√
N, indicating that the xj had not come from
the theoretical distribution. In fact, we can approximate the limiting value of
DN as D∞≈u106/
√
106 = 0.0878. The exact value is (exercise 13.42) D∞=
0.0868552356.
At the risk of overemphasizing this example, I carried it one step further. I
generated ℓ= 1, 2, . . . , 100 sets of N = 10m points x(ℓ)
j
for m = 2, 3, and 4
drawn from PG(x, 0, 1) and from PS(x, 3, 1) and used them to form 100 empiri-
cal cumulative probabilities Pr(ℓ,10m)
e,G
(−∞, x) and Pr(ℓ,10m)
e,S
(−∞, x) as deﬁned by
(13.310–13.312). Next, I computed the distances D(ℓ)
G,G,10m and D(ℓ)
S,G,10m of each
of these cumulative probabilities from the gaussian distribution PG(x, 0, 1). I
labeled the two sets of 100 quantities u(ℓ,m)
G,G
=
√
10m D(ℓ)
G,G,10m and u(ℓ,m)
S,G
=
√
10m D(ℓ)
S,G,10m in increasing order as u(m)
G,G,1 ≤u(m)
G,G,2 ≤· · · ≤u(m)
G,G,100 and
u(m)
S,G,1 ≤u(m)
S,G,2 ≤· · · ≤u(m)
S,G,100. I then used (13.310–13.312) to form the
cumulative probabilities
Pr(m)
e,G,G(−∞, u) =
j
Ns
for
u(m)
G,G,j < u < u(m)
G,G,j+1,
(13.322)
557

PROBABILITY AND STATISTICS
−5
−4
−3
−2
−1
0
1
2
3
4
5
0
0.1
0.2
0.3
0.4
0.5
0.6
Comparison of PG(x, 0, 1) with PS(x, 3, 1)
x
P (x)
Gauss’s
Gosset’s Student’s
Figure 13.8
The probability distributions of Gauss PG(x, 0, 1) and Gosset/Student
PS(x, 3, 1) with zero mean and unit variance.
and
Pr(m)
e,S,G(−∞, u) =
j
Ns
for
u(m)
S,G,j < u < u(m)
S,G,j+1
(13.323)
for Ns = 100 sets of 10m points.
I plotted these cumulative probabilities in Fig. 13.9. The thick smooth curve
is Kolmogorov’s universal cumulative probability distribution K(u) deﬁned by
(13.320). The thin jagged curve that clings to K(u) is the cumulative probabil-
ity distribution Pr(4)
e,G,G(−∞, u) made from 100 sets of 104 points taken from
PG(x, 0, 1). As the number of sets increases beyond 100 and the number of
points 10m rises further, the probability distributions Pr(m)
e,G,G(−∞, u) converge to
the universal cumulative probability distribution K(u) and provide a numerical
veriﬁcation of Kolmogorov’s theorem. Such curves make poor ﬁgures, how-
ever, because they hide beneath K(u). The curves labeled Pr(m)
e,S,G(−∞, u) for
m = 2 and 3 are made from 100 sets of N = 10m points taken from PS(x, 3, 1)
and tested as to whether they instead come from PG(x, 0, 1). Note that as
N = 10m increases from 100 to 1000, the cumulative probability distribution
Pr(m)
e,S,G(−∞, u) moves farther from Kolmogorov’s universal cumulative proba-
bility distribution K(u). In fact, the curve Pr(4)
e,S,G(−∞, u) made from 100 sets of
104 points lies beyond u > 8, too far to the right to ﬁt in the ﬁgure. Kolmogorov’s
test gets more conclusive as the number of points N →∞.
558

13.21 KOLMOGOROV’S TEST
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Kolmogorov’s test
u
Pr (u)
Pre, S, G
(2)
Pre, S, G
(3)
Pre, G, G
(4)
Figure 13.9
Kolmogorov’s test is applied to points xj taken from Gauss’s distribu-
tion PG(x, 0, 1) and from Gosset’s Student’s distribution PS(x, 3, 1) to see whether
the xj came from PG(x, 0, 1). The thick smooth curve is Kolmogorov’s universal
cumulative probability distribution K(u) deﬁned by (13.320). The thin jagged curve
that clings to K(u) is the cumulative probability distribution Pr(4)
e,G,G(−∞, u) made
(13.322) from points taken from PG(x, 0, 1). The other curves Pr(m)
e,S,G(−∞, u) for
m = 2 and 3 are made (13.323) from 10m points taken from PS(x, 3, 1).
Warning, mathematical hazard
While binned data are ideal for chi-squared
ﬁts, they ruin Kolmogorov tests. The reason is that if the data are in bins of
width w, then the empirical cumulative probability distribution Pr(N)
e
(−∞, x)
is a staircase function with steps as wide as the bin-width w even in the limit
N →∞. Thus even if the data come from the theoretical distribution, the limiting
value D∞of the Kolmogorov distance will be positive. In fact, one may show
(exercise 13.21) that when the data do come from the theoretical probability
distribution Pt(x), assumed to be continuous, then the value of D∞is
D∞=
sup
−∞<x<∞
w Pt(x)
2
.
(13.324)
Thus, in this case, the quantity
√
N DN would diverge as
√
N D∞and lead one
to believe that the data had not come from Pt(x).
Suppose we have made some changes in our experimental apparatus and our
software, and we want to see whether the new data x′
1, x′
2, . . . , x′
N′ we took after
559

PROBABILITY AND STATISTICS
the changes are consistent with the old data x1, x2, . . . , xN we took before the
changes. Then, following equations (13.310–13.312), we can make two empiri-
cal cumulative probability distributions – one Pr(N)
e
(−∞, x) made from the N
old points xj and the other Pr(N′)
e
(−∞, x) made from the N′ new points x′
j. Next,
we compute the distances
D+
N,N′ =
sup
−∞<x<∞

Pr(N)
e
(−∞, x) −Pr(N′)
e
(−∞, x)

,
DN,N′ =
sup
−∞<x<∞
Pr(N)
e
(−∞, x) −Pr(N′)
e
(−∞, x)
 ,
(13.325)
which are analogous to (13.313–13.316). Smirnov (Smirnov, 1939; Gnedenko,
1968, p. 453) has shown that as N, N′ →∞the probabilities that
u+
N,N′ =
&
NN′
N + N′ D+
N,N′
and
uN,N′ =
&
NN′
N + N′ DN,N′
(13.326)
are less than u are
lim
N,N′→∞Pr(u+
N,N′ < u) = 1 −e−2u2,
lim
N,N′→∞Pr(uN,N′ < u) = K(u),
(13.327)
in which K(u) is Kolmogorov’s distribution (13.320).
Further reading
Students can learn more about probability and statistics in Mathematical
Methods for Physics and Engineering (Riley et al., 2006), An Introduction
to Probability Theory and Its Applications I, II (Feller, 1968, 1966), Theory
of Financial Risk and Derivative Pricing (Bouchaud and Potters, 2003), and
Probability and Statistics in Experimental Physics (Roe, 2001).
Exercises
13.1
Find the probabilities that the sum on two thrown fair dice is 4, 5, or 6.
13.2
Show that the zeroth moment μ0 and the zeroth central moment ν0 always
are unity, and that the ﬁrst central moment ν1 always vanishes.
13.3
Compute the variance of the uniform distribution on (0, 1).
13.4
In the formulas (13.21 & 13.28) for the variances of discrete and continuous
distributions, show that E[(x −⟨x⟩)2] = μ2 −μ2.
13.5
A convex function is one that lies above its tangents: if f (x) is convex, then
f (x) ≥f (y)+(x−y)f ′(y). For instance, ex ≥1+x. Show that for any convex
function f (x) that f (x) ≥f (⟨x⟩)+(x−⟨x⟩) f ′(⟨x⟩) and so that ⟨f (x)⟩≥f (⟨x⟩)
or E [ f (x)] ≥f (E[x]) (Johan Jensen, 1859–1925).
560

EXERCISES
13.6
(a) Show that the covariance ⟨(x −x)(y −y)⟩is equal to ⟨x y⟩−⟨x⟩⟨y⟩as
asserted in (13.35). (b) Derive (13.39) for the variance V[ax + by].
13.7
Derive expression (13.40) for the variance of a sum of N variables.
13.8
Find the range of pq = p(1 −p) for 0 ≤p ≤1.
13.9
Show that the variance of the binomial distribution (13.43) is given by
(13.47).
13.10 Redo the polling example (13.14–13.16) for the case of a slightly better poll
in which 16 people were asked and 13 said they’d vote for Nancy Pelosi.
What’s the probability that she’ll win the election? (You may use Maple or
some other program to do the tedious integral.)
13.11 For the case in which N and N−n are big, derive (13.52 & 13.53) from (13.43
& 13.51).
13.12 For the case in which N, N −n, and n are big, derive (13.54 & 13.55) from
(13.43 & 13.51).
13.13 Without using the fact that the Poisson distribution is a limiting form of the
binomial distribution, show from its deﬁnition (13.58) and its mean (13.60)
that its variance is equal to its mean, as in (13.62).
13.14 Show that the mean and variance of Gauss’s approximation (13.74) to the
binomial distribution is a normalized probability distribution with mean
⟨x⟩= μ = pN and variance V[x] = pqN.
13.15 Derive the approximations (13.88 & 13.89) for binomial probabilities for
large N.
13.16 Compute the central moments (13.27) of the gaussian (13.75).
13.17 Derive formula (13.84) for the probability that a gaussian random variable
falls within an interval.
13.18 Show that the expression (13.91) for P(y|600) is negligible on the interval
(0, 1) except for y near 3/5.
13.19 Determine the constant A of the homogeneous solution ⟨v(t)⟩gh and derive
expression (13.141) for the general solution ⟨v(t)⟩to (13.139).
13.20 Derive equation (13.142) for the variance of the position r about its mean
qBE t.
13.21 Derive equation (13.172) for the ensemble average ⟨r2(t)⟩for the case in
which ⟨r2(0)⟩= 0 and d⟨r2(0)⟩/dt = 0.
13.22 Use (13.183) to derive the lower moments (13.185 & 13.186) of the distribu-
tions of Gauss and Poisson.
13.23 Find the third and fourth moments μ3 and μ4 for the distributions of
Poisson (13.178) and Gauss (13.175).
13.24 Derive formula (13.190) for the ﬁrst ﬁve cumulants of an arbitrary probabil-
ity distribution.
13.25 Show that, like the characteristic function, the moment-generating func-
tion M(t) for an average of several independent random variables factorizes
M(t) = M1(t/N) M2(t/N) · · · MN(t/N).
13.26 Derive formula (13.197) for the moments of the log-normal probability
distribution (13.196).
561

PROBABILITY AND STATISTICS
13.27 Why doesn’t the log-normal probability distribution (13.196) have a sensible
power-series about x = 0? What are its derivatives there?
13.28 Compute the mean and variance of the exponential distribution (13.198).
13.29 Show that the chi-square distribution P3,G(v, σ) with variance σ 2 = kT/m
is the Maxwell–Boltzmann distribution (13.100).
13.30 Compute the inverse Fourier transform (13.174) of the characteristic func-
tion (13.203) of the symmetric Lévy distribution for ν = 1 and 2.
13.31 Show that the integral that deﬁnes P(2)(y) gives formula (13.239) with two
Heaviside functions. Hint: keep x1 and x2 in the interval (0, 1).
13.32 Derive the normal distribution (13.224) in the variable (13.223) from the cen-
tral limit theorem (13.221) for the case in which all the means and variances
are the same.
13.33 Show that Fisher’s matrix (13.257) is symmetric Fkℓ= Fℓk and nonnegative
(1.38), and that when it is positive (1.39), it has an inverse.
13.34 Derive the integral equations (13.259 & 13.260) from the normalization
condition
2
P(x; θ) dNx = 1.
13.35 Derive the Cramér–Rao lower bound (13.275) on the variance V[tk] from
the inequality (13.270).
13.36 Show that the variance V[t(N)
σ 2 ] of Bessel’s estimator (13.254) is given by
(13.281).
13.37 Compute the fourth central moment (13.26) of Gauss’s probability distribu-
tion PG(x; μ, σ 2).
13.38 Show that when the matrix G has rank M, the matrices P = G G+ and
P⊥= 1−P are projection operators that are mutually orthogonal P(I−P) =
(I −P)P = 0.
13.39 Show that Kolmogorov’s distance DN is bounded as in (13.314).
13.40 Show that Kolmogorov’s distance DN is the greater of D+
N and D−
N.
13.41 Derive the formulas (13.317) for D+
N and D−
N.
13.42 Compute the exact limiting value D∞of the Kolmogorov distance between
PG(x, 0, 1) and PS(x, 3, 1). Use the cumulative probabilities (13.321 &
13.194) to ﬁnd the value of x that maximizes their difference. Using Maple
or some other program, you should ﬁnd x = 0.6276952185 and then
D∞= 0.0868552356.
13.43 Show that when the data do come from the theoretical probability distribu-
tion (assumed to be continuous) but are in bins of width w, then the limiting
value D∞of the Kolmogorov distance is given by (13.324).
562

14
Monte Carlo methods
14.1 The Monte Carlo method
The Monte Carlo method is simple, robust, and useful. It was invented by
Enrico Fermi and developed by Metropolis (Metropolis et al., 1953). It has
many applications. One can use it for numerical integration. One can use it to
decide whether an odd signal is random noise or something to evaluate. One
can use it to generate sequences of conﬁgurations that are random but occur
according to a probability distribution, such as the Boltzmann distribution of
statistical mechanics. One even can use it to solve virtually any problem for
which one has a criterion to judge the quality of an arbitrary solution and a way
of generating a suitably huge space of possible solutions. That’s how evolution
invented us.
14.2 Numerical integration
Suppose one wants to numerically integrate a function f (x) of a vector x =
(x1, . . . , xn) over a region R. One generates a large number N of random val-
ues for the n coordinates x within a hypercube of length L that contains the
region R, keeps the NR points xk = (x1k, . . . , xnk) that fall within the region
R, computes the average ⟨f (xk)⟩, and multiplies by the hypervolume VR of the
region

R
f (x) dnx ≈VR
NR
NR

k=1
f (xk).
(14.1)
If the hypervolume VR is hard to compute, you can have the Monte Carlo
code compute it for you. The hypervolume VR is the volume Ln of the enclosing
563

MONTE CARLO METHODS
hypercube multiplied by the number NR of times the N points fall within the
region R
VR = NR
N Ln.
(14.2)
The integral formula (14.1) then becomes

R
f (x) dnx ≈Ln NR
N2
R
NR

k=1
f (xk).
(14.3)
The utility of the Monte Carlo method of numerical integration rises sharply
with the dimension n of the hypervolume.
Example 14.1 (Numerical integration)
Suppose one wants to integrate the
function
f (x, y) =
e−2x−3y

x2 + y2 + 1
(14.4)
over the quarter of the unit disk in which x and y are positive. In this case, VR
is the area π/4 of the quarter disk.
To generate fresh random numbers, one must set the seed for the code that
computes them. The following program sets the seed by using the subroutine
init_random_seed deﬁned in a FORTRAN95 program in section 13.16. With
some compilers, one can just write “call random_seed()”.
program integrate
implicit none ! catches typos
integer :: k, N
integer, parameter :: dp = kind(1.0d0)
real(dp) :: x, y, sum = 0.0d0, f
real(dp), dimension(2) :: rdn
real(dp), parameter :: area = atan(1.0d0) ! pi/4
f(x,y) = exp(-2*x - 3*y)/sqrt(x**2 + y**2 + 1.0d0)
write(6,*) ’How many points?’
read(5,*) N
call init_random_seed() ! set new seed
do k = 1, N
10
call random_number(rdn); x= rdn(1); y = rdn(2)
if (x**2+y**2 > 1.0d0) then
go to 10
end if
sum = sum + f(x,y)
end do
! integral = area times mean value < f > of f
sum = area*sum/real(N,dp)
564

14.2 NUMERICAL INTEGRATION
write(6,*) ’The integral is ’,sum
end program integrate
I ran this code with npoints = 10ℓfor ℓ= 1, 2, 3, 4, 5, 6, 7, and 8 and
found respectively the results 0.059285, 0.113487, 0.119062, 0.115573, 0.118349,
0.117862, 0.117868, and 0.117898. The integral is approximately 0.1179.
An equivalent C++ code by Sean Cahill is:
#include <math.h>
#include <iostream>
#include <stdlib.h>
using namespace std;
// The function to integrate
double f(const double& x, const double& y)
{
double numer = exp(-2*x - 3*y);
double denom = sqrt(x*x + y*y + 1);
double retval = numer / denom;
return retval;
}
void integrate ()
{
// Declares local constants
const double area = atan(1); // pi/4
// Inits local variables
int n=0;
double sum=0,x=0,y=0;
// Seeds random number generator
srand ( time(NULL) );
// Gets the value of N
cout << "What is N? ";
cin >> n;
// Loops the given number of times
565

MONTE CARLO METHODS
for (int i=0; i<n; i++)
{
// Loops until criteria met
while (true)
{
// Generates random points between 0 and 1
x = static_cast<double>(rand()) / RAND_MAX;
y = static_cast<double>(rand()) / RAND_MAX;
// Checks if the points are suitable
if ((x*x + y*y) <= 1)
{
// If so, break out of the while loop
break;
}
}
// Updates our sum with the given points
sum += f(x,y);
}
// Integral = area times mean value < f > of f
sum = area * sum / n;
cout << ‘‘ The integral is’’ << sum << endl;
}
14.3 Applications to experiments
Physicists accumulate vast quantities of data and sometimes must decide
whether a particular signal is due to a defect in the detector, to a random ﬂuc-
tuation in the real events that they are measuring, or to a new and unexpected
phenomenon. For simplicity, let us assume that the background can be ignored
and that the real events arrive randomly in time apart from extraordinary phe-
nomena. One reliable way to evaluate an ambiguous signal is to run a Monte
Carlo program that generates the kind of real random events to which one’s
detector is sensitive and to use these events to compute the probability that the
signal occurred randomly.
To illustrate the use of random-event generators, we will consider the work
of a graduate student who spent 100 days counting muons produced by atmo-
spheric GeV neutrinos in an underground detector. Each of the very large
566

14.3 APPLICATIONS TO EXPERIMENTS
number N of primary cosmic rays that hit the Earth each day can collide with a
nucleus and make a shower of pions which in turn produce atmospheric neutri-
nos that can make muons in the detector. The probability p that a given cosmic
ray will make a muon in the detector is very small, but the number N of primary
cosmic rays is very large. In this experiment, their product pN was ⟨n⟩= 0.1
muons per day. Since N is huge and p tiny, the probability distribution is Pois-
son, and so by (13.58) the probability that n muons would be detected on any
particular day is
P(n, ⟨n⟩) = ⟨n⟩n
n! e−⟨n⟩
(14.5)
in the absence of a failure of the anticoincidence shield or some other problem
with the detector – or some hard-to-imagine astrophysical event.
The graduate student might have used the following program to generate
1,000,000 random histories of 100 days of events distributed according to the
Poisson distribution (14.5) with ⟨n⟩= 0.1:
program muons
implicit none
interface
function factorial(n)
implicit none
integer, intent(in) :: n
double precision :: factorial
end function factorial
end interface
integer :: k, m, day, number
integer, parameter :: N = 1000000 ! number of data
sets
integer, dimension(N,100) :: histories
integer, dimension(0:100) :: maxEvents = 0,
sumEvents = 0
double precision :: prob, x, numMuons, totMuons
double precision, dimension(0:100) :: p
double precision, parameter :: an = 0.1 ! <n> events
per day
prob = exp(-an); p(0) = prob; maxEvents = 0
! p(k) is the probability of fewer than k+1 events per
day
do k = 1, 100 ! make Poisson distribution
prob = prob + an**k*exp(-an)/factorial(k)
567

MONTE CARLO METHODS
p(k) = prob
end do
call init_random_seed() ! sets random seed
do k = 1, N ! do N histories
do day = 1, 100 ! do day of kth history
call random_number(x)
do m = 100, 0, -1
if (x < p(m)) then
number = m
end if
end do
histories(k,day) = number
end do
numMuons = maxval(histories(k,:))
totMuons = sum(histories(k,:))
maxEvents(numMuons) = maxEvents(numMuons) + 1
sumEvents(totMuons) = sumEvents(totMuons) + 1
end do
open(7,file="maxEvents"); open(8,file="totEvents")
do k = 0, 100
write(7,*) k, maxEvents(k); write(8,*) k,
sumEvents(k)
end do
end program muons
function factorial(n)
result(fact)
implicit none
integer, intent(in) :: n
integer, parameter :: dp = kind(1.0d0)
real(dp) :: fact
fact = 1.0d0
do i = 1, n
fact = i*fact
end do
end function factorial
Figure 14.1 plots the results from this simple Monte Carlo of 1,000,000
runs of 100 days each. The boxes show that the maximum number of muons
detected on a single day respectively was n = 1, 2, and 3 on 62.6%, 35.9%,
and 1.5% of the runs – and respectively was n = 0, 4, 5, and 6 on only 36,
410, 9, and 1 runs. Thus if the actual run detected no muons at all, that would
be by (13.83) about a 4σ event, while a run with more than four muons on a
single day would be an event of more than 4σ. Either would be a reason to
568

14.3 APPLICATIONS TO EXPERIMENTS
0
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
6
A million runs of 100 days each
Number n of muons detected
log10(histories)
in 100 days
in 1 day
Figure 14.1
The number (out of 1,000,000) of histories of 100 days in which a
maximum of n muons is detected on a single day (boxes) and in 100 days (curve).
examine the apparatus or the heavens; the Monte Carlo can’t tell us which. The
curve shows how many runs had a total of n muons; 125,142 histories had ten
muons.
Of course, one could compute the data of Fig. 14.1 by hand without running
a Monte Carlo. But suppose one’s aging phototubes reduced the mean number
of muons detected per day to ⟨n⟩= 0.1(1 −αd/100) on day d? Or suppose one
needed the probability of detecting more than one muon on two days separated
by one day of zero muons? In such cases, the analytic computation would be
difﬁcult and error prone, but the student would need to change only a few lines
in the Monte Carlo program.
An equivalent C++ code by Sean Cahill is:
#include <stdlib.h>
#include <time.h>
#include <math.h>
#include <iostream>
#include <fstream>
#include <iomanip>
#include <vector>
#include <valarray>
569

MONTE CARLO METHODS
using namespace std;
// Calculates the factorial of n
double factorial(const int& n)
{
double f = 1;
int i=0;
for(i = 1; i <= n; i++)
{
f *= i;
}
return f;
}
void muons()
{
// Declares constants
const int N = 1000000; // Number of data sets
const int LOOP_ITR = 101;
const double AN = 1; // Number of events per day
// Inits local variables
int k=0, m=0, day=0, num=0, numMuons=0, totMuons=0;
int maxEvents[LOOP_ITR];
int totEvents[LOOP_ITR];
memset (maxEvents, 0, sizeof(int) * LOOP_ITR);
memset (totEvents, 0, sizeof(int) * LOOP_ITR);
// Creates our 2d histories array
vector<valarray<int> > histories(N, LOOP_ITR);
double prob=0,tmpProb=0,fact=0, x=0;
double p[LOOP_ITR];
// probability of no events
p[0] = exp(-AN);
prob = p[0];
// p(k) is the probability of fewer than k+1 events
per day
for (k=1; k<=LOOP_ITR; k++)
570

14.3 APPLICATIONS TO EXPERIMENTS
{
fact = factorial (k);
tmpProb = k * exp(-AN) / fact;
prob += pow(AN, tmpProb);
p[k] = prob;
}
// Random seed
srand ( time(NULL) );
// Goes through all the histories
for (k=0; k<N; k++)
{
// Goes through all the days
for (day=1; day<LOOP_ITR; day++)
{
// Generates a random number between 0 and 1
x = static_cast<double>(rand()) / RAND_MAX;
// Finds an M with p(M) < X
for (m=100; m>=0; m--)
{
if (x < p[m])
{
num = m;
}
}
histories[k][day] = num;
}
// Calculates max and sum
numMuons = histories[k].max();
totMuons = histories[k].sum();
// Updates our records
maxEvents[numMuons]++;
totEvents[totMuons]++;
}
// Opens a data file
ofstream fhMaxEvents, fhSumEvents;
fhMaxEvents.open ("maxEvents.txt");
571

MONTE CARLO METHODS
fhSumEvents.open ("totEvents.txt");
// Sets precision
fhMaxEvents.setf(ios::fixed,ios::floatfield);
fhMaxEvents.precision(7);
fhSumEvents.setf(ios::fixed,ios::floatfield);
fhSumEvents.precision(7);
// Writes the data to a file
for (k=0; k<LOOP_ITR; k++)
{
fhMaxEvents << k << "
" << maxEvents[k] << endl;
fhSumEvents << k << "
" << totEvents[k] << endl;
}
}
14.4 Statistical mechanics
The Metropolis algorithm can generate a sequence of states or conﬁgurations
of a system distributed according to the Boltzmann probability distribution
(1.345). Suppose the state of the system is described by a vector x of many com-
ponents. For instance, if the system is a protein, the vector x might be the 3N
spatial coordinates of the N atoms of the protein. A protein composed of 200
amino acids has about 4000 atoms, and so the vector x would have some 12,000
components. Suppose E(x) is the energy of conﬁguration x of the protein in its
cellular environment of salty water crowded with macromolecules. How do we
generate a sequence of “native states” of the protein at temperature T?
We start with some random or artiﬁcial initial conﬁguration x0 and then
make random changes δx in successive conﬁgurations x. One way to do this is
to make a small, random change δxi in coordinate xi and then to test whether
to accept this change by comparing the energies E(x) and E(x′) of the two con-
ﬁgurations x and x′, which differ by δxi in coordinate xi. (Estimating these
energies is not trivial; Gromacs and TINKER can help.) It is important that this
random walk be symmetric, that is, the choice of testing whether to go from
x to x′ when one is at x should be exactly as likely as the choice of testing
whether to go from x′ to x when one is at x′. Also, the sequences of conﬁg-
urations should be ergodic; that is, from any conﬁguration x, one should be
able to get to any other conﬁguration x′ by a suitable sequence of changes
δxi = x′
i −xi.
How do we decide whether to accept or reject δxi? We use the following
Metropolis step. If the energy E′ = E(x′) of the new conﬁguration x′ is less
572

14.4 STATISTICAL MECHANICS
than the energy E(x) of the current conﬁguration x, then we accept the new
conﬁguration x′. If E′ > E, then we accept x′ with probability
P(x →x′) = e−(E′−E)/kT.
(14.6)
In practice, one generates a pseudo-random number r ∈[0, 1] and accepts x′ if
r < e−(E′−E)/kT.
(14.7)
If one does not accept x′, then the system remains in conﬁguration x.
In FORTRAN90, the Metropolis step might be
if ( newE <= oldE ) then ! accept
x(i) = x(i) + dx
else ! accept conditionally
call random_number(r)
if ( r <= exp(-beta*(newE - oldE))) then ! accept
x(i) = x(i) + dx
end if
end if
in which β = 1/kT.
One then varies another coordinate, such as xi+1. Once one has varied all
of the coordinates, one has ﬁnished a sweep through the system. After thou-
sands or millions of such sweeps, the system is said to be thermalized. Once
the system is thermalized, one can start measuring properties of the system.
One computes a physical quantity every hundred or every thousand sweeps and
takes the average of these measurements. That average is the mean value of the
physical quantity at temperature T.
Why does this work? Consider two conﬁgurations x and x′, which respec-
tively have energies E
=
E(x) and E′
=
E(x′) and are occupied with
probabilities Pt(x) and Pt(x′) as the system is thermalizing. If E′ > E, then the
rate R(x′ →x) of going from x′ to x is the rate v of choosing to test x when one
is at x′ times the probability Pt(x) of being at x, that is, R(x′ →x) = v Pt(x′).
The reverse rate is R(x →x′) = v Pt(x) e−(E′−E)/kT with the same v since the
random walk is symmetric. The net rate from x′ →x then is
R(x′ →x) −R(x →x′) = v

Pt(x′) −Pt(x) e−(E′−E)/kT
.
(14.8)
This net ﬂow of probability from x′ →x is positive if and only if
Pt(x′)/Pt(x) > e−(E′−E)/kT.
(14.9)
The probability distribution Pt(x) therefore ﬂows with each sweep toward the
Boltzmann distribution exp(−E(x)/kT). The ﬂow slows and stops when the two
573

MONTE CARLO METHODS
rates are equal R(x′ →x) = R(x →x′), a condition called detailed balance. At
this equilibrium, the distribution Pt(x) satisﬁes Pt(x) = Pt(x′) e−(E−E′)/kT, in
which Pt(x′) eE′/kT is independent of x. So the thermalizing distribution Pt(x)
approaches the distribution P(x) = c e−E/kT, in which c is independent of x.
Since the sum of these probabilities must be unity, we have

x
P(x) = c

x
e−E/kT = 1,
(14.10)
which means that the constant c is the inverse of the partition function
Z(T) =

x
e−E(x)/kT.
(14.11)
The thermalizing distribution approaches Boltzmann’s distribution (1.345)
Pt(x) →PB(x) = e−E(x)/kT/Z(T).
(14.12)
Example 14.2 (Z2 lattice gauge theory)
First, one replaces space-time with a
lattice of points in d dimensions. Two nearest neighbor points are separated by
the lattice spacing a and joined by a link. Next, one puts an element U of the
gauge group on each link. For the Z2 gauge group (example 10.4), one assigns
an action Sp to each elementary square or plaquette of the lattice with vertices
1, 2, 3, and 4
Sp = 1 −1
3ReTr

U1,2U2,3U3,4U4,1

.
(14.13)
Then, one replaces E(x)/kT with βS, in which the action S is a sum of all
the plaquette actions Sp. More details are available at Michael Creutz’s website
(latticeguy.net/lattice.html).
Although the generation of conﬁgurations distributed according to the
Boltzmann probability distribution (1.345) is one of its most useful appli-
cations, the Monte Carlo method is much more general. It can generate
conﬁgurations x distributed according to any probability distribution P(x).
To generate conﬁgurations distributed according to P(x), we accept any new
conﬁguration x′ if P(x′) ≥P(x) and also accept x′ with probability
P(x →x′) = P(x′)/P(x)
(14.14)
if P(x) > P(x′).
This works for the same reason that the Boltzmann version works. Consider
two conﬁgurations x and x′. If the system is thermalized, then the probabili-
ties Pt(x) and Pt(x′) have reached equilibrium, and so the rate R(x →x′) from
x →x′ must equal that R(x′ →x) from x′ →x. If P(x′) > P(x), then R(x′ →x) is
R(x′ →x) = v Pt(x′),
(14.15)
574

14.5 SOLVING ARBITRARY PROBLEMS
in which v is the rate of choosing δx = x′ −x, while the rate R(x →x′) is
R(x →x′) = v Pt(x) P(x′)/P(x)
(14.16)
with the same v since the random walk is symmetric. Equating the two rates
R(x′ →x) = R(x →x′)
(14.17)
we ﬁnd that the ﬂow of probability stops when
Pt(x) = P(x) Pt(x′)/P(x′) = cP(x),
(14.18)
where c is independent of x′. Thus Pt(x) →P(x).
So far we have assumed that the rate of choosing x →x′ is the same as the
rate of choosing x′ →x. In smart Monte Carlo schemes, physicists arrange the
rates vx→x′ and vx′→x so as to steer the ﬂow and speed-up thermalization. To
compensate for this asymmetry, they change the second part of the Metropolis
step from x →x′ when E′ = E(x′) > E = E(x) to accept conditionally with
probability
P(x →x′) = P(x′) vx′→x/ [P(x) vx→x′] .
(14.19)
Now if P(x′) > P(x), then R(x′ →x) is
R(x′ →x) = vx′→x Pt(x′)
(14.20)
while the rate R(x →x′) is
R(x →x′) = vx→x′ Pt(x) P(x′) vx′→x/ [P(x) vx→x′] .
(14.21)
Equating the two rates R(x′ →x) = R(x →x′), we ﬁnd
Pt(x′) = Pt(x) P(x′)/P(x).
(14.22)
That is Pt(x) = P(x) Pt(x′)/P(x′), which gives
Pt(x) = N P(x)
(14.23)
where N is a constant of normalization.
14.5 Solving arbitrary problems
If you know how to generate a suitably large space of trial solutions to a
problem, and you also know how to compare the quality of any two of your
solutions, then you can use a Monte Carlo method to solve it. The hard parts
of this seemingly magical method are characterizing a big enough space of solu-
tions s and constructing a quality function or functional that assigns a number
Q(s) to every solution in such a way that if s is a better solution than s′, then
Q(s) > Q(s′).
(14.24)
575

MONTE CARLO METHODS
But once one has characterized the space of possible solutions s and has
constructed the quality function Q(s), then one simply generates huge numbers
of random solutions and selects the one that maximizes the function Q(s) over
the space of all solutions.
If one can characterize the solutions as vectors of a certain dimension,
s = (x1, . . . , xn), then one may use the Monte Carlo method of the previous
section (14.4) by replacing −E(s) with Q(s) and kT with a parameter of the
same dimension as Q(s), nominally dimensionless.
14.6 Evolution
The reader may think that the use of Monte Carlo methods to solve arbi-
trary problems is quite a stretch. Yet Nature has applied them to the problem
of evolving species that survive. As a measure of the quality Q(s) of a given
solution s, Nature used the time derivative of the logarithm of its population
˙P(t)/P(t). The space of solutions is the set of possible genomes. We may ideal-
ize each solution or genome as a sequence of nucleotides s = b1b2 . . . bN some
thousands or billions of bases long, each base bk being adenine, cytosine, gua-
nine, or thymine (A, C, G, or T). Since there are four choices for each base,
the set of solutions is huge. The genome for homo sapiens has some 3 billion
bases (or base pairs, DNA being double stranded), and so the solution space is
a set with
N = 43×109 = 101.8×109
(14.25)
elements. By comparison, a googol is only 10100.
In evolution, a Metropolis step begins with a random change in the sequence
of bases; changes in a germ-line cell can create a new individual. Some of these
changes are due to errors in the normal mechanisms by which genomes are
copied and repaired. The (holo)enzyme DNA polymerase copies DNA with
remarkable ﬁdelity, making one error in every billion base pairs copied. Along a
given line of descent, only about one nucleotide pair in a thousand is randomly
changed in the germ line every million years. Yet in a population of 10,000
diploid individuals, every possible nucleotide substitution will have been tried
out on about 20 occasions during a million years (Alberts et al., 2008).
RNA polymerases transcribe DNA into RNA, and RNAs play many roles:
Ribosomes translate messenger RNAs (mRNAs) into proteins, which are
sequences of amino acids; ribosomal RNAs (rRNAs) combine with proteins to
form ribosomes; long noncoding RNAs (ncRNAs) regulate the rates at which
different genes are transcribed; micro RNAs (miRNAs) regulate the rates at
which different mRNAs are translated into proteins; and other RNAs have
other as yet unknown functions. So a change of one base, e.g. from A to C,
might alter a protein or change the expression of a gene or be silent.
576

EXERCISES
Sexual reproduction makes bigger random changes in genomes. In meio-
sis, the paternal and maternal versions of each of our 23 chromosomes are
duplicated, and the four versions swap segments of DNA in a process called
genetic recombination or crossing-over. The cell then divides twice produc-
ing four haploid germ cells each with a single paternal, maternal, or mixed
version of each chromosome. This second kind of Metropolis step makes evo-
lution more ergodic, which is why most complex modern organisms use sexual
reproduction.
Other genomic changes occur when a virus inserts its DNA into that of a cell
and when transposable elements (transposons) of DNA move to different sites
in a genome.
In evolution, the rest of the Metropolis step is done by the new individual:
if he or she survives and multiplies, then the change is accepted; if he or she
dies without progeny, then the change is rejected. Evolution is slow, but it has
succeeded in turning a soup of simple molecules into humans with brains of
100 billion neurons, each with 1000 connections to other neurons.
John Holland and others have incorporated analogs of these Metropolis
steps into Monte Carlo techniques called genetic algorithms for solving wide
classes of problems (Holland, 1975; Vose, 1999; Schmitt, 2001).
Evolution also occurs at the cellular level when a cell mutates enough to
escape the control imposed on its proliferation by its neighbors and transforms
into a cancer cell.
Further reading
The classic Quarks, Gluons, and Lattices (Creutz, 1983) is a marvelous intro-
duction to the subject; Creutz’s Website (latticeguy.net/lattice.html) is an
extraordinary resource.
Exercises
14.1 Go to Michael Creutz’s website (latticeguy.net/lattice.html) and get his C-code
for Z2 lattice gauge theory. Compile and run it, and make a graph that exhibits
strong hysteresis as you raise and lower β = 1/kT.
14.2 Modify his code and produce a graph showing the coexistence of two phases
at the critical coupling βt = 0.5 ln(1 +
√
2). Hint: do a cold start and then
100 updates at βt, then do a random start and do 100 updates at βt. Plot the
values of the action against the update number 1, 2, 3, ..., 100.
14.3 Modify Creutz’s C code for Z2 lattice gauge theory so as to be able to vary the
dimension d of space-time. Show that for d = 2, there’s no phase transition;
for d = 3, there’s a second-order phase transition; and for d = 4, there’s a
ﬁrst-order phase transition.
14.4 What happens when d = 5?
577

15
Functional derivatives
15.1 Functionals
A functional G[f ] is a map from a space of functions to a set of numbers. For
instance, the action functional S[q] for a particle in one dimension maps the
coordinate q(t), which is a function of the time t, into a number – the action of
the process. If the particle has mass m and is moving slowly and freely, then for
the interval (t1, t2) its action is
S0[q] =
 t2
t1
dt m
2
dq(t)
dt
2
.
(15.1)
If the particle is moving in a potential V(q(t)), then its action is
S[q] =
 t2
t1
dt

m
2
dq(t)
dt
2
−V(q(t))

.
(15.2)
15.2 Functional derivatives
A functional derivative is a functional
δG[f ][h] = d
dϵ G[f + ϵh]

ϵ=0
(15.3)
of a functional. For instance, if Gn[f ] is the functional
Gn[f ] =

dx f n(x)
(15.4)
578

15.2 FUNCTIONAL DERIVATIVES
then its functional derivative is the functional that maps the pair of functions
f , h to the number
δGn[f ][h] = d
dϵ Gn[f + ϵh]

ϵ=0
= d
dϵ

dx (f (x) + ϵh(x))n

ϵ=0
=

dx nf n−1(x)h(x).
(15.5)
Physicists often use the less elaborate notation
δG[f ]
δf (y) = δG[f ][δy],
(15.6)
in which the function h(x) is δy(x) = δ(x −y). Thus in the preceding example
δG[f ]
δf (y) =

dx nf n−1(x)δ(x −y) = nf n−1(y).
(15.7)
Functional derivatives of functionals that involve powers of derivatives also
are easily dealt with. Suppose that the functional involves the square of the
derivative f ′(x)
G[f ] =

dx

f ′(x)
2 .
(15.8)
Then its functional derivative is
δG[f ][h] = d
dϵ G[f + ϵh]

ϵ=0
= d
dϵ

dx

f ′(x) + ϵh′(x)
2

ϵ=0
=

dx 2f ′(x)h′(x) = −2

dx f ′′(x)h(x),
(15.9)
in which we have integrated by parts and used suitable boundary conditions on
h(x) to drop the surface terms. In physics notation, we have
δG[f ]
δf (y) = −2

dx f ′′(x)δ(x −y) = −2f ′′(y).
(15.10)
Let’s now compute the functional derivative of the action (15.2), which
involves the square of the time-derivative ˙q(t) and the potential energy V(q(t))
579

FUNCTIONAL DERIVATIVES
δS[q][h] = d
dϵ S[q + ϵh]

ϵ=0
= d
dϵ

dt
m
2
˙q(t) + ϵ˙h(t)
2 −V(q(t) + ϵh(t))

ϵ=0
=

dt

m˙q(t)˙h(t) −V′(q(t))h(t)

=

dt

−m¨q(t) −V′(q(t))

h(t)
(15.11)
where we once again have integrated by parts and used suitable boundary
conditions to drop the surface terms. In physics notation, this is
δS[q]
δq(t) =

dt′ 
−m¨q(t′) −V′(q(t′))

δ(t −t′) = −m¨q(t) −V′(q(t)).
(15.12)
In these terms, the stationarity of the action S[q] is the vanishing of its
functional derivative either in the form
δS[q][h] = 0
(15.13)
for arbitrary functions h(t) (that satisfy the boundary conditions) or equiva-
lently in the form
δS[q]
δq(t) = 0,
(15.14)
which is Lagrange’s equation of motion
m¨q(t) = −V′(q(t)).
(15.15)
Physicists also use the compact notation
δ2Z[j]
δj(y)δj(z) ≡∂2Z[j + ϵδy + ϵ′δz]
∂ϵ ∂ϵ′

ϵ=ϵ′=0
(15.16)
in which δy(x) = δ(x −y) and δz(x) = δ(x −z).
Example 15.1 (Shortest path is a straight line)
On a plane, the length of the
path (x, y(x)) from (x0, y0) to (x1, y1) is
L[y] =
 x1
x0

dx2 + dy2 =
 x1
x0

1 + y′2 dx.
(15.17)
The shortest path y(x) minimizes this length L[y]
δL[f ]
δy
= d
dϵ L[y + ϵh]

ϵ=0
= d
dϵ
 x1
x0

1 + (y′ + ϵh′)2 dx

ϵ=0
=
 x1
x0
y′h′

1 + y′2 dx = −
 x1
x0
h d
dx
y′

1 + y′2 dx
(15.18)
580

15.3 HIGHER-ORDER FUNCTIONAL DERIVATIVES
since h(x) satisﬁes h(x0) = h(x1) = 0. Differentiating, we set
δL[y][h] =
 x1
x0
h
y′′
1 + y′2 dx = 0,
(15.19)
which can vanish for arbitrary functions h(x) only if y′′ = 0, which is to say only
if y(x) is a straight line, y = mx + b.
15.3 Higher-order functional derivatives
The second functional derivative is
δ2G[f ][h] = d2
dϵ2 G[f + ϵh]|ϵ=0 .
(15.20)
So if GN[f ] is the functional
GN[f ] =

f N(x)dx
(15.21)
then
δ2GN[f ][h] = d2
dϵ2 GN[f + ϵh]|ϵ=0
= d2
dϵ2

(f (x) + ϵh(x))N dx

ϵ=0
= d2
dϵ2
 N
2

ϵ2h2(x)f N−2(x) dx

ϵ=0
= N(N −1)

f N−2(x)h2(x)dx.
(15.22)
Example 15.2 (δ2S0)
The second functional derivative of the action S0[q]
(15.1) is
δ2S0[q][h] = d2
dϵ2
 t2
t1
dt m
2
dq(t)
dt
+ ϵ dh(t)
dt
2
ϵ=0
=
 t2
t1
dt m
dh(t)
dt
2
≥0
(15.23)
and is positive for all functions h(t). The stationary classical trajectory
q(t) = t −t1
t2 −t1
q(t2) + t2 −t
t2 −t1
q(t1)
(15.24)
is a minimum of the action S0[q].
581

FUNCTIONAL DERIVATIVES
The second functional derivative of the action S[q] (15.2) is
δ2S[q][h] = d2
dϵ2
 t2
t1
dt

m
2
dq(t)
dt
+ ϵ dh(t)
dt
2
−V(q(t) + ϵh(t))

ϵ=0
=
 t2
t1
dt

m
dh(t)
dt
2
−2∂2V(q(t))
∂q2(t)
h2(t)

(15.25)
and it can be positive, zero, or negative. Chaos sometimes arises in systems of
several particles when the second variation of S[q] about a stationary path is
negative, δ2S[q][h] < 0, while δS[q][h] = 0.
The nth functional derivative is deﬁned as
δnG[f ][h] = dn
dϵn G[f + ϵh]|ϵ=0 .
(15.26)
The nth functional derivative of the same functional (15.21) is
δnGN[f ][h] =
N!
(N −n)!

f N−n(x)hn(x)dx.
(15.27)
15.4 Functional Taylor series
It follows from the Taylor-series theorem (section 4.6) that
eδG[f ][h] =
∞

n=0
δn
n! G[f ][h] =
∞

n=0
1
n!
dn
dϵn G[f + ϵh]

ϵ=0
= G[f + ϵh],
(15.28)
which illustrates an advantage of the present mathematical notation.
The functional S0[q] of Equation (15.1) provides a simple example of the
functional Taylor series (15.28):
eδS0[q][h] =

1 + d
dϵ + 1
2
d2
dϵ2

S0[q + ϵh]

ϵ=0
= m
2
 t2
t1

1 + d
dϵ + 1
2
d2
dϵ2

˙q(t) + ϵ˙h(t)
2 dt

ϵ=0
= m
2
 t2
t1

˙q2(t) + 2˙q(t)˙h(t) + ˙h2(t)

dt
= m
2
 t2
t1
˙q(t) + ˙h(t)
2 dt = S0[q + h].
(15.29)
Note that if the function q(t) makes the action S0[q] stationary, and if h(t) is
smooth and vanishes at the endpoints of the time interval, then by (15.23)
S0[q + h] = S0[q] + S0[h],
(15.30)
582

15.5 FUNCTIONAL DIFFERENTIAL EQUATIONS
in which the functions q(t) and h(t) respectively satisfy the boundary conditions
on h(x) q(ti) = qi and h(t1) = h(t2) = 0.
More generally, if q(t) makes the action S[q] stationary, and h(t) is any loop
from and to the origin, then
S[q + h] = eδS[q][h] = S[q] +
∞

n=2
1
n!
dn
dϵn S[q + ϵh]|ϵ=0 .
(15.31)
If further S2[q] is purely quadratic in q and ˙q, like the harmonic oscillator, then
S2[q + h] = S2[q] + S2[h].
(15.32)
15.5 Functional differential equations
In inner products like ⟨q′|f ⟩, we represent the momentum operator as
p = ¯h
i
d
dq′
(15.33)
because then
⟨q′|p q|f ⟩= ¯h
i
d
dq′ ⟨q′|q|f ⟩= ¯h
i
d
dq′

q′⟨q′|f ⟩

=

¯h
i + q′ ¯h
i
d
dq′

⟨q′|f ⟩,
(15.34)
which respects the commutation relation [q, p] = i¯h.
So too in inner products ⟨φ′|f ⟩of eigenstates |φ′⟩of φ(x, t)
φ(x, t)|φ′⟩= φ′(x)|φ′⟩
(15.35)
we can represent the momentum π(x, t) canonically conjugate to the ﬁeld φ(x, t)
as the functional derivative
π(x, t) = ¯h
i
δ
δφ′(x)
(15.36)
because then
⟨φ′|π(x′, t)φ(x, t)|f ⟩= ¯h
i
δ
δφ′(x′)⟨φ′|φ(x, t)|f ⟩
= ¯h
i
δ
δφ′(x′)

φ′(x)⟨φ′|f ⟩

(15.37)
= ¯h
i
δ
δφ′(x′)

δ(x −x′) φ′(x′) d3x′ ⟨φ′|f ⟩

= ¯h
i

δ(x −x′) + φ′(x)
δ
δφ′(x′)

⟨φ′|f ⟩
= ⟨φ′| −i¯hδ(x −x′) + φ(x, t) π(x′, t)|f ⟩,
583

FUNCTIONAL DERIVATIVES
which respects the equal-time commutation relation
[φ(x, t), π(x′, t)] = i ¯h δ(x −x′).
(15.38)
We can use the representation (15.36) for π(x) to ﬁnd the wave-function of
the ground state |0⟩of the hamiltonian
H = 1
2
 
π2 + (∇φ)2 + m2φ2
d3x
(15.39)
where we set ¯h = c = 1. We will use a trick used to ﬁnd the ground state |0⟩of
the harmonic-oscillator hamiltonian
H0 = p2
2m + mω2q2
2
.
(15.40)
In that trick, one writes
H0 = 1
2m(mωq −ip)(mωq + ip) + iω
2 [p, q]
= 1
2m(mωq −ip)(mωq + ip) + 1
2 ¯hω
(15.41)
and seeks a state |0⟩that is annihilated by mωq + ip
⟨q′|mωq + ip|0⟩=

mωq′ + ¯h d
dq′

⟨q′|0⟩= 0.
(15.42)
The solution to this differential equation
d
dq′ ⟨q′|0⟩= −mωq′
¯h
⟨q′|0⟩
(15.43)
is
⟨q′|0⟩=
mω
π ¯h
1/4
exp

−mωq′2
2¯h

,
(15.44)
in which the prefactor is a constant of normalization.
So extending that trick to the hamiltonian (15.39), we factor H
H = 1
2
 
−∇2 + m2 φ −iπ
 
−∇2 + m2 φ + iπ

d3x + C,
(15.45)
in which C is the (inﬁnite) constant
C = i
2

[π,

−∇2 + m2 φ] d3x.
(15.46)
The ground state |0⟩of H must therefore satisfy the functional differential
equation
⟨φ′|

−∇2 + m2 φ + iπ|0⟩= 0
(15.47)
584

EXERCISES
or
δ⟨φ′|0⟩
δφ′(x) = −

−∇2 + m2 φ′(x) ⟨φ′|0⟩.
(15.48)
The solution is
⟨φ′|0⟩= N exp

−1
2

φ′(x)

−∇2 + m2 φ′(x) d3x

,
(15.49)
in which N is a normalization constant. The spatial Fourier transform ˜φ′(p)
φ′(x) =

eip·x ˜φ′(p) d3p
(2π)3
(15.50)
satisﬁes ˜φ′(−p) = ˜φ′∗(p) since φ′ is real. In terms of it, the ground-state wave-
function is
⟨φ′|0⟩= N exp

−1
2

| ˜φ′(p)|2

p2 + m2 d3p

.
(15.51)
Exercises
15.1 Compute the action S0[q] (15.1) for the classical path (15.24).
15.2 Use (15.25) to ﬁnd a formula for the second functional derivative of the action
(15.2) of the harmonic oscillator for which V(q) = mω2q2/2.
15.3 Derive (15.51) from equations (15.49 & 15.50).
585

16
Path integrals
16.1 Path integrals and classical physics
Since Richard Feynman invented them over 60 years ago, path integrals have
been used with increasing frequency in high-energy and condensed-matter
physics, in ﬁnance, and in biophysics (Kleinert, 2009). Feynman used them to
express matrix elements of the time-evolution operator exp(−itH/¯h) in terms
of the classical action. Others have used them to compute matrix elements of
the Boltzmann operator exp(−H/kT), which in the limit of zero temperature
projects out the ground state |E0⟩of the system
lim
T→0 e−(H−E0)/kT = lim
T→0
∞

n=0
|En⟩e−(En−E0)/kT⟨En| = |E0⟩⟨E0|,
(16.1)
a trick used in lattice gauge theory.
Path integrals magically express the quantum-mechanical probability ampli-
tude for a process as a sum of exponentials exp(iS/¯h) of the classical action S
of the various ways that process might occur.
16.2 Gaussian integrals
The path integrals we can do are gaussian integrals of inﬁnite order. So we begin
by recalling the basic integral formula (5.166)
 ∞
−∞
exp

−ia

x −b
2a
2
dx =
&π
ia ,
(16.2)
586

16.2 GAUSSIAN INTEGRALS
which holds for real a and b, and also the one (5.167)
 ∞
−∞
exp
#
−r

x −c
2r
2$
dx =
&π
r ,
(16.3)
which is true for positive r and complex c. Equivalent formulas for real a and b,
positive r, and complex c are
 ∞
−∞
exp

−iax2 + ibx

dx =
& π
ia exp

i b2
4a

,
(16.4)
 ∞
−∞
exp

−rx2 + cx

dx =
&π
r exp

c2
4r

.
(16.5)
This last formula will be useful with x = p, r = ϵ/(2m), and c = iϵ˙q
 ∞
−∞
exp

−ϵ p2
2m + iϵ ˙q p

dp =
&
2πm
ϵ
exp

−ϵ 1
2m˙q2

(16.6)
as will (16.2) with x = p, a = ϵ/(2m), and b = ϵ˙q
 ∞
−∞
exp

−iϵ p2
2m + iϵ ˙q p

dp =
&
2πm
iϵ
exp

iϵ 1
2m˙q2

.
(16.7)
Doable path integrals are multiple gaussian integrals. One may show (exer-
cise 16.1) that for positive r1, . . . , rN and complex c1, . . . , cN, the integral (16.5)
leads to
 ∞
−∞
exp

i
−rix2
i + cixi
 N

i=1
dxi =
 N

i=1
&π
ri

exp

1
4

i
c2
i
ri

.
(16.8)
If R is the N × N diagonal matrix with positive entries {r1, r2, . . . , rN}, and X
and C are N-vectors with real {xi} and complex {ci} entries, then this formula
(16.8) in matrix notation is
 ∞
−∞
exp

−X TRX + CTX
 N

i=1
dxi =
(
πN
det(R) exp
1
4 CTR−1C

.
(16.9)
Now every positive symmetric matrix S is of the form S
=
OROT for
some positive diagonal matrix R. So inserting R = OTSO into the previous
equation (16.9) and using the invariance of determinants under orthogonal
transformations, we ﬁnd
 ∞
−∞
exp

−X TOTSOX + CTX
 N

i=1
dxi =
(
πN
det(S) exp
#1
4 CTOTS−1OC
$
.
(16.10)
587

PATH INTEGRALS
The jacobian of the orthogonal transformations Y = OX and D = OC is unity,
and so
 ∞
−∞
exp

−Y TSY + DTY
 N

i=1
dyi =
(
πN
det(S) exp
1
4 DTS−1D

,
(16.11)
in which S is a positive symmetric matrix, and D is a complex vector.
The other basic gaussian integral (16.4) leads for real S and D to (exer-
cise 16.2)
 ∞
−∞
exp

−iY TSY + iDTY
 N

i=1
dyi =
(
πN
det(iS) exp
 i
4 DTS−1D

.
(16.12)
The vector Y that makes the argument −iY TSY + iDTY of the exponential
of this multiple gaussian integral (16.12) stationary is (exercise 16.3)
Y = 1
2S−1D.
(16.13)
The exponential of that integral evaluated at its stationary point Y is
exp

−iY TSY + iDTY

= exp
 i
4 DTS−1D

.
(16.14)
Thus, the multiple gaussian integral (16.12) is equal to its exponential evalu-
ated at its stationary point Y, apart from a prefactor involving the determinant
det iS.
Similarly, the vector Y that makes the argument −Y TSY +DTY of the expo-
nential of the multiple gaussian integral (16.11) stationary is Y = S−1D/2, and
that exponential evaluated at Y is
exp

−Y TSY + DTY

= exp
1
4 DTS−1D

.
(16.15)
Once again, a multiple gaussian integral is simply its exponential evaluated at
its stationary point Y, apart from a prefactor involving the determinant det S.
16.3 Path integrals in imaginary time
At the imaginary time t = −iβ ¯h, the time-evolution operator exp(−itH/¯h) is
exp(−βH), in which the inverse temperature β = 1/kT is the reciprocal of
Boltzmann’s constant k = 8.617 × 10−5 eV/K times the absolute temperature
T. In the low-temperature limit, exp(−βH) is a projection operator (16.1) on
the ground state of the system. These path integrals in imaginary time are called
euclidean path integrals.
588

16.3 PATH INTEGRALS IN IMAGINARY TIME
Let us consider a quantum-mechanical system with hamiltonian
H = p2
2m + V(q),
(16.16)
in which the commutator of the position q and momentum p operators is
[q, p]
=
i in units in which ¯h
=
1. For tiny ϵ, the corrections to the
approximation
exp

−ϵ

p2
2m + V(q)

≈exp

−ϵ p2
2m

exp ( −ϵ V(q)) + O(ϵ2) (16.17)
are of second order in ϵ.
To evaluate the matrix element ⟨q′′| exp(−ϵH)|q′⟩, we insert the identity
operator I in the form of an integral over the momentum eigenstates
I =
 ∞
−∞
|p′⟩⟨p′|dp′
(16.18)
and use the inner product ⟨q′′|p′⟩= exp(iq′′p′)/
√
2π so as to get as ϵ →0
⟨q′′| exp(−ϵH)|q′⟩=
 ∞
−∞
⟨q′′| exp

−ϵ p2
2m

|p′⟩⟨p′| exp (−ϵ V(q)) |q′⟩dp′
= e−ϵ V(q′)
 ∞
−∞
exp

−ϵ p′2
2m + i p′ (q′′ −q′)

dp′
2π .
(16.19)
We now adopt the suggestive notation
q′′ −q′
ϵ
= ˙q′
(16.20)
and use the integral formula (16.6) so as to obtain
⟨q′′| exp( −ϵ H)|q′⟩= 1
2π e−ϵ V(q′)
 ∞
−∞
exp

−ϵ p′2
2m + i ϵ p′ ˙q′

dp′
=
 m
2πϵ
1/2
exp
3
−ϵ

1
2 m ˙q ′2 + V(q′)
4
,
(16.21)
in which q′′ enters through the notation (16.20).
The next step is to link two of these matrix elements together
⟨q′′′|e−2ϵH|q′⟩=
 ∞
−∞
⟨q′′′|e−ϵH|q′′⟩⟨q′′|e−ϵH|q′⟩dq′′
=
m
2πϵ
 ∞
−∞
exp
3
−ϵ

1
2 m ˙q ′′2 + V(q′′) + 1
2 m ˙q ′2 + V(q′)
4
dq′′.
(16.22)
589

PATH INTEGRALS
Linking three of these matrix elements together and using subscripts instead of
primes, we have
⟨q3|e−3ϵH|q0⟩=
 ∞
−∞
⟨q3|e−ϵH|q2⟩⟨q2|e−ϵH|q1⟩⟨q1|e−ϵH|q0⟩dq1dq2
=
 m
2πϵ
3/2 ∞
−∞
exp
⎧
⎨
⎩−ϵ
2

j=0

1
2 m ˙q2
j + V(qj)

⎫
⎬
⎭dq1dq2.
(16.23)
Boldly passing from 3 to n and suppressing some integral signs, we get
⟨qn|e−nϵH|q0⟩=
 ∞
−∞
⟨qn|e−ϵH|qn−1⟩· · · ⟨q1|e−ϵH|q0⟩dq1 · · · dqn−1
=
 m
2πϵ
n/2 ∞
−∞
exp
⎧
⎨
⎩−ϵ
n−1

j=0

1
2 m ˙q2
j + V(qj)

⎫
⎬
⎭dq1 · · · dqn−1.
(16.24)
Writing dt for ϵ and taking the limits ϵ →0 and nϵ →β, we ﬁnd that the
matrix element ⟨qβ|e−βH|q0⟩is a path integral of the exponential of the average
energy multiplied by −β
⟨qβ|e−βH|q0⟩= N

exp
#
−
 β
0
1
2m˙q2(t) + V(q(t)) dt
$
Dq,
(16.25)
in which Dq = (n m/2πβ)n/2dq1dq2 · · · dqn−1 as n →∞. We sum over all paths
q(t) that go from q(0) = q0 at time 0 to q(β) = qβ at time β.
In the limit β →∞, the operator exp(−βH) becomes proportional to a
projection operator (16.1) on the ground state of the theory.
In three-dimensional space, q(t) replaces q(t) in equation (16.25)
⟨qβ|e−βH|q0⟩= N

exp
#
−
 β
0
1
2m˙q2 + V(q) dt
$
Dq.
(16.26)
Path integrals in imaginary time are called euclidean mainly to distinguish
them from Minkowski path integrals, which represent matrix elements of the
time-evolution operator exp(−itH) in real time.
16.4 Path integrals in real time
Path integrals in real time represent the time-evolution operator exp(−itH).
Using the integral formula (16.7), we ﬁnd in the limit ϵ →0
590

16.4 PATH INTEGRALS IN REAL TIME
⟨q′′|e−i ϵ H|q′⟩=
 ∞
−∞
⟨q′′| exp

−i ϵ p2
2m

|p′⟩⟨p′| exp [ −i ϵ V(q)] |q′⟩dp′
= 1
2π e−i ϵ V(q′)
 ∞
−∞
exp

−i ϵ p′2
2m + i p′ (q′′ −q′)

dp′
= 1
2π e−i ϵ V(q′)
 ∞
−∞
exp

−i ϵ p′2
2m + i ϵ p′˙q′

dp′
=
 m
2πiϵ
1/2
exp

i ϵ

m ˙q′2
2
−V(q′)

.
(16.27)
When we link together n of these matrix elements, we get the real-time version
of (16.24)
⟨qn|e−inϵH|q0⟩=
 m
2πiϵ
n/2 ∞
−∞
exp
⎧
⎨
⎩iϵ
n−1

j=0

1
2 m ˙q2
j −V(qj)

⎫
⎬
⎭dq1 · · · dqn−1.
(16.28)
Writing dt for ϵ and taking the limits ϵ →0 and nϵ →t, we ﬁnd that the
amplitude ⟨qt|e−itH|q0⟩is the path integral
⟨qt|e−itH|q0⟩= N

exp
#
i
 t
0
1
2 m ˙q2 −V(q) dt′
$
Dq,
(16.29)
in which Dq differs from the one that appears in euclidean path integrals by the
substitution β →it:
Dq = lim
n→∞
 nm
2πit
n/2
dq1dq2 · · · dqn.
(16.30)
The integral in the exponent is the classical action
S[q] =
 t
0
1
2 m ˙q2 −V(q) dt′
(16.31)
for a process q(t′) that runs from q(0) = q0 to q(t) = qt. We sum over all such
processes.
In three-dimensional space
⟨qt|e−itH|q0⟩=

exp
#
i
 t
0
1
2 m ˙q2 −V(q) dt′
$
Dq
(16.32)
replaces (16.29).
The units of action are energy × time, and the argument of the exponential
must be dimensionless, so in ordinary units the amplitude (16.29) is
⟨qt|e−itH/¯h|q0⟩=

eiS[q]/¯hDq.
(16.33)
591

PATH INTEGRALS
When is this amplitude big? When is it tiny? Suppose there is a process q(t) =
qc(t) that goes from qc(0) = q0 to qc(t) = qt in time t and that obeys the classical
equation of motion (15.14–15.15)
δS[qc]
δqc
= m¨qc + V′(qc) = 0.
(16.34)
The action of such a classical process is stationary, that is, S[qc+dq] differs from
S[qc] only by terms of second order in δq. So there are inﬁnitely many other
processes that have the same action to within a fraction of ¯h. These processes
add with nearly the same phase to the path integral (16.33) and so make a huge
contribution to the amplitude ⟨qt|e−itH/¯h|q0⟩.
But if no classical process goes from q0 to qt in time t, then the nonclassi-
cal processes from q0 to qt in time t have actions that differ among themselves
by large multiples of ¯h. Their amplitudes cancel each other, and so the result-
ing amplitude is tiny. Thus the real-time path integral explains the principle of
stationary action (section 11.37).
Does this path integral satisfy Schrödinger’s equation? To see that it does,
we’ll use (16.27) in the more explicit form
⟨q′′|e−iϵH|q′⟩=
 m
2πiϵ
1/2
exp

i m(q′′ −q′)2
2ϵ
−i ϵ V(q′)

(16.35)
to write ψ(q′′, t + ϵ) = ⟨q′′|ψ, t + ϵ⟩as an integral of ψ(q′, t) = ⟨q′|ψ, t⟩
⟨q′′|ψ, t + ϵ⟩=

⟨q′′|e−iϵH|q′⟩⟨q′|ψ, t⟩dq′
=
 m
2πiϵ
1/2 
exp

i m(q′′ −q′)2
2ϵ
−i ϵ V(q′)

⟨q′|ψ, t⟩dq′.
(16.36)
Keeping only leading terms in ϵ, we have
ψ(q′′, t + ϵ) =
 m
2πiϵ
1/2
e−iϵV(q′′)
 ∞
−∞
exp

im(q′′ −q′)2
2ϵ

ψ(q′, t) dq′.
(16.37)
Letting x = q′ −q′′ and q′ = q′′ + x, we ﬁnd
ψ(q′′, t + ϵ) =
 m
2πiϵ
1/2
e−iϵV(q′′)

exp

imx2
2ϵ

ψ(q′′ + x, t) dx.
(16.38)
We now expand ψ(q′′ + x, t) as
ψ(q′′ + x, t) = ψ(q′′, t) + xψ′(q′′, t) + 1
2x2ψ′′(q′′, t) + · · ·
(16.39)
592

16.5 PATH INTEGRAL FOR A FREE PARTICLE
and ψ(q′′, t + ϵ) as
ψ(q′′, t + ϵ) = ψ(q′′, t) + ϵ ˙ψ(q′′, t) + · · · .
(16.40)
The integral formula (16.2) implies
 ∞
−∞
eimx2/2ϵ dx =
2πiϵ
m
1/2
(16.41)
and its derivative with respect to im/2ϵ gives
 ∞
−∞
x2 eimx2/2ϵ dx = iϵ
m
2πiϵ
m
1/2
while
 ∞
−∞
x eimx2/2ϵ dx = 0.
(16.42)
Substituting the expansions (16.39 & 16.40) for ψ(q′′ + x, t) and ψ(q′′, t + ϵ)
into the integral (16.38) and using the integral formulas (16.41 & 16.42), we get
ψ(q′′, t) + ϵ ˙ψ(q′′, t) =

1 −iϵV(q′′)
 #
ψ(q′′, t) + iϵ
m ψ′′(q′′, t)
$
,
(16.43)
which is Schrödinger’s equation
i ˙ψ = −1
2m ψ′′ + V ψ
(16.44)
in natural units or i¯h ˙ψ = −¯h2 ψ′′/2m + Vψ in arbitrary units.
16.5 Path integral for a free particle
The amplitude for a free nonrelativistic particle to go from the origin to the
point q in time t is the path integral (16.32)
⟨q|e−itH|q = 0⟩=

eiS0[q] Dq =

exp

i
 t
0
1
2 m ˙q2(t′) dt′

Dq.
(16.45)
The classical path that goes from 0 to q in time t is qc(t′) = (t′/t) q. The general
path q(t′) over which we integrate is q(t′) = qc(t′) + δq(t′). Since both q(t′) and
qc(t′) go from 0 to q in time t, the otherwise arbitrary path δq(t′) must be a loop
that goes from δq(0) = 0 to δq(t) = 0 in time t. The velocity ˙q = ˙qc + ˙δq is the
sum of the constant classical velocity ˙qc = q/t and the loop velocity ˙δq. The
ﬁrst-order change vanishes
m
 t2
t1
˙qc · dδq
dt dt = m ˙qc ·
 t2
t1
dδq
dt dt = m ˙qc · [δq(t2) −δq(t1)] = 0
(16.46)
and so the action S0[q] is the classical action plus the loop action
S0[q] = 1
2 m
 t
0

˙qc + ˙δq
2
dt′ = S0[qc] + S0[δq].
(16.47)
593

PATH INTEGRALS
The path integral therefore factorizes
⟨q|e−i t H|0⟩=

eiS0[q] Dq = N

eiS0[qc+δq] Dδq
=

eiS0[qc] eiS0[δq] Dδq
= eiS0[qc]

eiS0[δq] Dδq
(16.48)
into the phase of the classical action times a path integral over the loops. The
loop integral L is independent of the spatial points q and 0 and so can only
depend upon the time interval, L = L(t). Thus the amplitude is the product
⟨q|e−i t H|0⟩= eiS0[qc] L(t).
(16.49)
Since the classical velocity is ˙qc = q/t, the classical action is
S0[qc] =
 T
0
m
2 ˙q2
c(t) dt = m
2
q2
t .
(16.50)
So the amplitude is
⟨q|e−i(t2−t1)H|0⟩= eimq2/2t L(t).
(16.51)
Since the position eigenstates are orthogonal, this amplitude must reduce to
a delta function as t →0
lim
t→0 ⟨q|e−i t H|0⟩= ⟨q|0⟩= δ3(q).
(16.52)
One of the many representations of Dirac’s delta function is
δ3(q) = lim
t→0

m
2πi¯ht
3/2
eimq2/2¯ht.
(16.53)
Thus L(t) = (m/2πit)3/2 and
⟨q|e−itH/¯h|0⟩=

m
2πi¯ht
3/2
eimq2/2¯ht
(16.54)
in unnatural units. You can verify (exercise 16.6) this result by inserting a
complete set of momentum dyadics |p⟩⟨p| and doing the resulting Fourier
transform.
Example 16.1 (The Bohm–Aharonov effect)
From our formula (11.311) for the
action of a relativistic particle of mass m and charge q, we infer (exercise 16.7)
that the action of a nonrelativistic particle in an electromagnetic ﬁeld with no
scalar potential is
594

16.7 HARMONIC OSCILLATOR IN REAL TIME
S =
 x2
x1
#1
2mv + q A
$
·dx .
(16.55)
Now imagine that we shoot a beam of such particles past but not through a
narrow cylinder in which a magnetic ﬁeld is conﬁned. The particles can go either
way around the cylinder of area S but can not enter the region of the magnetic
ﬁeld. The difference in the phases of the amplitudes is the loop integral from the
source to the detector and back to the source
S
¯h
=
) mv
2 + q A

· dx
¯h =
) mv · dx
2¯h
+ q
¯h

S
B · dS =
) mv · dx
2¯h
+ q
¯h ,
(16.56)
in which  is the magnetic ﬂux through the cylinder.
16.6 Free particle in imaginary time
If we mimic the steps of the preceding section (16.5) in which the hamiltonian
is H = p2/2m, set β = it/¯h = 1/kT, and use Dirac’s delta function
δ3(q) = lim
t→0
 m
2π ¯ht
3/2
e−mq2/2¯ht
(16.57)
then we get
⟨q|e−βH|0⟩=

m
2π ¯h2β
3/2
exp

−mq2
2¯h2β

=
 mkT
2π ¯h2
3/2
e−mkTq2/2¯h2. (16.58)
To study the ground state of the system, we set β = t/¯h and let t →∞in
⟨q|e−tH/¯h|0⟩=
 m
2π ¯ht
3/2
exp

−m
2
q2
¯ht

,
(16.59)
which for D = ¯h/(2m) is the solution (3.200 & 13.107) of the diffusion equation.
16.7 Harmonic oscillator in real time
Biologists have mice; physicists have harmonic oscillators with hamiltonian
H = p2
2m + mω2q2
2
.
(16.60)
For this hamiltonian, our formula (16.29) for the coordinate matrix elements of
the time-evolution operator exp(−itH) is
⟨q′′|e−i t H|q′⟩=

eiS[q]Dq
(16.61)
with action
S[q] =
 t
0
1
2m˙q2(t′) −1
2mω2q2(t′) dt′.
(16.62)
595

PATH INTEGRALS
The classical solution qc(t) = q′ cos ωt+ ˙q0 sin(ωt)/ω in which q′ = qc(0) and
˙q0 = ˙qc(0) are the initial position and velocity makes the action S[q] stationary
d
dϵ S[q + ϵh]

ϵ=0
= 0
(16.63)
and satisﬁes the classical equation of motion ¨qc(t) = −ω2qc(t).
We now apply the trick (16.46–16.48) we used for the free particle. We write
an arbitrary process q(t) is the sum of the classical process qc(t) and a loop δq(t)
with δq(0) = δq(t) = 0. Since the action S[q] is quadratic in the variables q and
˙q, the functional Taylor series (15.31) for S[qc + δq] has only two terms
S[qc + δq] = S[qc] + S[δq].
(16.64)
Thus we can write the path integral (16.61) as
⟨q′′|e−itH|q′⟩=

eiS[q] Dq =

eiS[qc+δq] Dδq
=

eiS[qc]+iS[δq] Dδq = eiS[qc]

eiS[δq] Dδq.
(16.65)
The remaining path integral over the loops δq does not involve the endpoints
q′ and q′′ and so must be a function L(t) of the time t but not of q′ or q′′
⟨q′′|e−itH|q′⟩= eiS[qc] L(t).
(16.66)
The action S[qc] is (exercise 16.8)
S[qc] =
mω
2 sin(ωt)

q′2 + q′′2
cos(ωt) −2q′q′′
.
(16.67)
The action S[δq] of a loop
δq(t′) =
n−1

j=1
aj sin jπt′
t
(16.68)
is (exercise 16.9)
S[δq] =
n−1

j=1
mt
4 a2
j

(jπ)2
t2
−ω2

.
(16.69)
The path integral over the loops is then, apart from a constant jacobian J,

eiS[δq] Dδq = J
 nm
2πit
n/2 
exp
⎧
⎨
⎩
n−1

j=1
imt
4 a2
j

(jπ)2
t2
−ω2
⎫
⎬
⎭
n−1

j=1
daj
= J
 nm
2πit
n/2 n−1

j=1
 ∞
−∞
exp
@
imt
4 a2
j

(jπ)2
t2
−ω2
A
daj.
(16.70)
596

16.8 HARMONIC OSCILLATOR IN IMAGINARY TIME
Using the gaussian integral (16.2) and the inﬁnite product (4.140), we get

eiS[δq] Dδq = Jnn/2
& m
2πit
n−1

j=1
√
2
jπ

1 −ω2t2
π2j2
−1/2
=
&
mω
2πi sin ωt
⎛
⎝lim
n→∞Jnn/2
n−1

j=1
√
2
jπ
⎞
⎠.
(16.71)
Using (16.66) and (16.67), we see that the number within the parentheses is
unity because in that case we have (Feynman and Hibbs, 1965, ch. 3)
⟨q′′|e−itH/¯h|q′⟩=
&
mω
2πi¯h sin(ωt) exp

imω

q′2 + q′′2
cos(ωt) −2q′q′′
2¯h sin(ωt)

,
(16.72)
which agrees with the amplitude (16.54) in the limit t →0 (exercise 16.11).
16.8 Harmonic oscillator in imaginary time
For the harmonic oscillator with hamiltonian (16.60), our formula (16.25) for
euclidean path integrals becomes
⟨q′′|e−βH|q′⟩= N

exp

−
 β
0
1
2m˙q2(t) + 1
2mω2q2(t) dt
%
Dq.
(16.73)
The euclidean action, which is a time integral of the energy of the oscillator,
Se[q] =
 β
0

1
2m˙q2(t) + 1
2mω2q2(t)

dt
(16.74)
is purely quadratic, and so we may play the trick (15.32) if we can ﬁnd a path
qe(t) that makes it stationary
δSe[qe][h] = d
dϵ
 β
0
1
2m(˙qe(t) + ϵ˙h(t))2 + 1
2mω2(qe(t) + ϵh(t))2 dt

ϵ=0
=
 β
0
m˙qe(t)˙h(t) + mω2qe(t)h(t) dt
=
 β
0

−m¨qe(t) + mω2qe(t)

h(t)dt = 0.
(16.75)
The path qe(t) must satisfy the euclidean equation of motion
¨qe(t) = ω2qe(t)
(16.76)
whose general solution is
qe(t) = Aeωt + Be−ωt.
(16.77)
597

PATH INTEGRALS
The path from qe(0) = q′ to qe(β) = q′′ must have
A = q′′e−ωβ −q′e−2ωβ
1 −e−2ωβ
and
B = q′ −A.
(16.78)
Its action Se[qe] is (exercise 16.12)
Se[qe] = 1
2mω

A2 
e2ωβ −1

−B2 
e−2ωβ −1

.
(16.79)
Since the action is purely quadratic, the trick (15.32) tells us that the action
Se[q] of the arbitrary path q(t) = qe(t) + δq(t) is the sum
Se[q] = Se[qe] + Se[δq],
(16.80)
in which the action Se[δq] of the loop δq(t) depends but upon t but not upon
qβ or q0. It follows then that for some loop function L(β) of β alone
⟨q′′|e−βH|q′⟩= exp (−Se[qe]) L(β)
= L(β) exp
3
−1
2mω

A2 
e2ωβ −1

−B2 
e−2ωβ −1
4
.
(16.81)
To study the ground state of the harmonic oscillator, we let β →∞in this
equation. Inserting a complete set of eigenstates H|n⟩= En|n⟩, we see that the
limit of the left-hand side is
lim
β→∞⟨q′′|e−βH|q′⟩= lim
β→∞⟨q′′|n⟩e−βEn⟨n|q′⟩= e−βE0⟨q′′|0⟩⟨0|q′⟩.
(16.82)
Our formulas (16.78) for A and B say that A →q′′e−ωβ and B →q′ as β →∞,
and so in this limit by (16.81 & 16.82) we have
e−βE0⟨q′′|0⟩⟨0|q′⟩= L(β) exp

−1
2mω

q′′2 + q′2
,
(16.83)
from which we may infer our earlier formula (15.44) for the wave-function of
the ground state
⟨q|0⟩=
mω
π ¯h
1/4
exp

−1
2
mωq2
¯h

,
(16.84)
in which the prefactor ensures the normalization
1 =
 ∞
−∞
|⟨q|0⟩|2dq.
(16.85)
Euclidean path integrals help one study ground states.
598

16.9 EUCLIDEAN CORRELATION FUNCTIONS
16.9 Euclidean correlation functions
In the Heisenberg picture, the position operator q(t) is
q(t) = eitHq e−itH,
(16.86)
in which q = q(0) is the position operator at time t = 0 or equivalently the posi-
tion operator in the Schrödinger picture. The analogous operator in imaginary
time is the euclidean position operator qe(t) deﬁned as
qe(t) = etHq e−tH
(16.87)
obtained from q(t) by replacing t by −it.
The euclidean product of two euclidean position operators is
T [qe(t1) qe(t2)] = θ(t1 −t2)qe(t1) qe(t2) + θ(t2 −t1)qe(t2) qe(t1),
(16.88)
in which θ(x) = (x + |x|)/2|x| is Heaviside’s function. We can use the method
of section 16.3 to compute the matrix element of the euclidean-time-ordered
product T [q(t1)q(t2)] sandwiched between two factors of exp(−tH). For t1 ≥t2,
this matrix element is
⟨qt|e−tHqe(t1)qe(t2)e−tH|q−t⟩= ⟨qt|e−(t−t1)Hqe−(t1−t2)Hqe−(t+t2)H|q−t⟩.
(16.89)
Then instead of the path-integral formula (16.25), we get
⟨qt|e−tHT [qe(t1)qe(t2)] e−tH|q−t⟩=

q(t1)q(t2)e−Se[q,t,−t] Dq
(16.90)
where as in (16.25) Se[q, t, −t] is the euclidean action
Se[q, t, −t] =
 t
−t
1
2m˙q2(t′) + V(q(t′)) dt′
(16.91)
or the time integral of the energy. As in the path integral (16.25), the integration
is over all paths that go from q(−t) = q−t to q(t) = qt. The analog of (16.25) is
⟨qt|e−2tH|q−t⟩=

e−Se[q,t,−t] Dq
(16.92)
and the factors (nm/2πβ)n/2 cancel in the ratio of (16.90) to (16.92)
⟨qt|e−tHT [qe(t1)qe(t2)] e−tH|q−t⟩
⟨qt|e−2tH|q−t⟩
=

q(t1)q(t2)e−Se[q,t,−t] Dq

e−Se[q,t,−t] Dq
.
(16.93)
599

PATH INTEGRALS
In the limit t →∞, the operator exp(−tH) projects out the ground state |0⟩
of the system
lim
t→∞e−tH|q−t⟩= lim
t→∞
∞

n=0
e−tH|n⟩⟨n|q−t⟩= lim
t→∞e−tE0|0⟩⟨0|q−t⟩,
(16.94)
which we assume to be unique and normalized to unity. In the ratio (16.93),
most of these factors cancel, leaving us with
⟨0|T [qe(t1)qe(t2)] |0⟩=

q(t1)q(t2)e−Se[q,∞,−∞] Dq

e−Se[q,∞,−∞] Dq
.
(16.95)
More generally, the mean value in the ground state |0⟩of any euclidean-time-
ordered product of position operators q(ti) is a ratio of path integrals
⟨0| T [q(t1) · · · q(tn)] |0⟩=

q(t1) · · · q(tn) e−Se[q] Dq

e−Se[q] Dq
,
(16.96)
in which Se[q] stands for Se[q, ∞, −∞]. Why do we need the time-ordered prod-
uct T on the LHS? Because successive factors of exp(−(tk −tℓ)H) lead to the
path integral of exp(−Se[q]). Why don’t we need T on the RHS? Because the
q(ti)s are real numbers which commute with each other.
The result (16.96) is important because it can be generalized to all quantum
theories, including ﬁeld theories.
16.10 Finite-temperature ﬁeld theory
Matrix elements of the operator exp(−βH) where β = 1/kT tell us what a
system is like at temperature T. In the low-temperature limit, they describe the
ground state of the system.
Quantum mechanics imposes upon n coordinates qi and conjugate momenta
πk the commutation relations
[qi, pk] = i δi,k
and
[qi, qk] = [pi, pk] = 0.
(16.97)
In quantum ﬁeld theory, we associate a coordinate qx ≡φ(x) and a conjugate
momentum px ≡π(x) with each point x of space and impose upon them the
very similar commutation relations
[φ(x), π(y)] = i δ(x −y)
and
[φ(x), φ(y)] = [π(x), π(y)] = 0.
(16.98)
Just as in quantum mechanics the time derivative of a coordinate is its commu-
tator with a hamiltonian ˙qi = i[H, qi], so too in quantum ﬁeld theory the time
600

16.10 FINITE-TEMPERATURE FIELD THEORY
derivative of a ﬁeld ˙φ(x, t) ≡˙φ(x) is ˙φ(x) = i[H, φ(x)]. A typical hamiltonian
for a single scalar ﬁeld φ is
H =
 #1
2π2(x) + 1
2 (∇φ(x))2 + 1
2m2φ2(x) + P(φ(x))
$
d3x,
(16.99)
in which P is a quartic polynomial.
Since quantum ﬁeld theory is just the quantum mechanics of many variables,
we can use the methods of sections 16.3 & 16.4 to write matrix elements of
exp(−βH) as path integrals. We deﬁne a potential
V(φ(x)) = 1
2 (∇φ(x))2 + 1
2m2φ2(x) + P(φ(x))
(16.100)
and write the hamiltonian H as
H =
 #1
2π2(x) + V(φ(x))
$
d3x.
(16.101)
Like |q′⟩and |p′⟩, the states |φ′⟩and |π′⟩are eigenstates of the hermitian
operators φ(x, 0) and π(x, 0)
φ(x, 0)|φ′⟩= φ′(x)|φ′⟩
and
π(x, 0)|π′⟩= π′(x)|π′⟩.
(16.102)
The analog of ⟨q′|p′⟩is
⟨φ′|π′⟩= f exp
#
i

φ′(x)π′(x)d3x
$
,
(16.103)
in which f is a factor which eventually will cancel.
Repeating our derivation of equation (16.21) with
Dπ′ ≡

x
dπ′(x)
(16.104)
we ﬁnd in the limit ϵ →0
⟨φ′′| exp(−ϵH)|φ′⟩=

⟨φ′′| exp

−ϵ
2

π2(x)d3x

|π′⟩
× ⟨π′| exp

−ϵ

V(φ(x)) d3x

|φ′⟩Dπ′
= |f |2 exp

−ϵ

V(φ′(x)) d3x

×

exp
#
−1
2ϵπ′2(x) + iπ′(x)[φ′′(x) −φ′(x)]d3x
$
Dπ′.
(16.105)
Using the abbreviation
˙φ′(x) ≡φ′′(x) −φ′(x)
ϵ
(16.106)
601

PATH INTEGRALS
and the integral formula (16.6), we get
⟨φ′′| exp(−ϵH)|φ′⟩= f ′ exp

−ϵ
 
1
2 ˙φ′2(x) + V(φ′(x))

d3x
%
.
Putting together n = β/ϵ such terms and integrating over the intermediate
states |φ′′′⟩⟨φ′′′|, and absorbing the normalizing factors into Dφ, we have
⟨φβ|e−βH|φ0⟩=
 φβ
φ0
exp
#
−
 β
0

1
2 ˙φ2(x) + V(φ(x)) d3xdt
$
Dφ.
(16.107)
Replacing the potential V(φ) with its deﬁnition (16.100), we ﬁnd
⟨φβ|e−βH|φ0⟩=
 φβ
φ0
exp
#
−
 β
0

1
2

˙φ2 + (∇φ)2 + m2φ2
+ P(φ) d3xdt
$
Dφ,
(16.108)
in which the limits φ0 and φβ remind us that we are to integrate over all ﬁelds
φ(x, t) that run from φ(x, 0) = φ0(x) to φ(x, β) = φβ(x).
In terms of the energy density
H(φ) ≡1
2

(∂aφ)2 + m2φ2
+ P(φ),
(16.109)
in which a is summed from 0 to 3, the path integral (16.108) is
⟨φβ|e−βH|φ0⟩=
 φβ
φ0
exp
#
−
 β
0

H(φ) d3xdt
$
Dφ.
(16.110)
The partition function Z(β) – deﬁned as the trace Z(β) ≡Tr e−βH over all
states of the system – is an integral over all loop ﬁelds (ones for which φβ and
φ0 coincide)
Z(β) ≡Tr e−βH =
 φ0
φ0
⟨φ|e−βH|φ⟩Dφ = N
 φ0
φ0
exp
#
−
 β
0

H(φ) d3xdt
$
Dφ.
(16.111)
Because the four space-time derivatives in H(φ) occur with the same sign, ﬁnite-
temperature ﬁeld theory is called euclidean quantum ﬁeld theory. The density
operator ρ for the system described by the hamiltonian H in equilibrium at
temperature T is ρ = exp(−βH)/Z(β).
Like the deﬁnition (16.87) of the euclidean position operator qe(t), the
euclidean ﬁeld operator φe(x) is deﬁned as
φe(x, t) = etHφ(x, 0)e−tH.
(16.112)
The euclidean-time-ordered product (16.88 ) of two ﬁelds is
T [φe(x1, t1)φe(x2, t2)] = θ(t1 −t2)et1Hφe(x1, 0)e−(t1−t2)Hφe(x2, 0)e−t2H
+ θ(t2 −t1)et2Hφe(x2, 0)e−(t2−t1)Hφe(x1, 0)e−t1H.
602

16.11 REAL-TIME FIELD THEORY
The logic of equations (16.87–16.96) leads us to write its mean value in a sys-
tem described by a stationary density operator – one that commutes with the
hamiltonian – as the ratio
⟨T [φe(x1)φe(x2)]⟩= Tr {ρ T [φe(x1)φe(x2)]}
= Tr

e−βH T [φe(x1)φe(x2)]

Tr

e−βH
=
 φ0
φ0
φ(x1)φ(x2) exp
#
−
 β
0

H(φ) d3x dt
$
Dφ
 φ0
φ0
exp
#
−
 β
0

H(φ) d3x dt
$
Dφ
,
(16.113)
in which all normalization factors have canceled.
In the zero-temperature (β →∞) limit, the density operator ρ becomes the
projection operator |0⟩⟨0| on the ground state, and mean-value formulas like
(16.113) become
⟨0|T [φe(x1) · · · φe(xn)] |0⟩=

φ(x1) · · · φ(xn) exp
#
−

H(φ) d4x
$
Dφ

exp
#
−

H(φ) d4x
$
Dφ
,
(16.114)
in which Hamilton’s density H(φ) is integrated over all of euclidean space-time
and over all ﬁelds that are periodic on the inﬁnite time interval. Statistical ﬁeld
theory and lattice gauge theory use formulas like (16.113) and (16.114).
16.11 Real-time ﬁeld theory
We now follow the derivation of section 16.10 using the same notation but for
real time. In (16.105), we replace −ϵH by −iϵH and follow the logic of sections
(16.4 & 16.10). We ﬁnd in the limit ϵ →0 with ˙φ′ ≡(φ′′ −φ′)/ϵ
⟨φ′′|e−iϵH|φ′⟩=

⟨φ′′|e−iϵ
2
π2/2 d3x|π′⟩⟨π′|e−iϵ
2
V(φ) d3x|φ′⟩Dπ′
= |f |2e−iϵ
2
V(φ′)d3x

e−iϵ
2
π′2/2+iπ′(φ′′−φ′) d3xDπ′
= f ′ exp
#
iϵ

1
2 ˙φ′2 −V(φ′) d3x
$
.
(16.115)
Putting together 2t/ϵ similar factors and integrating over all the intermediate
states |φ⟩⟨φ|, we arrive at the path integral
⟨φ′′|e−i2tH|φ′⟩=
 φ′′
φ′ exp
#
i

1
2 ˙φ2(x) −V(φ(x)) d4x
$
Dφ,
(16.116)
603

PATH INTEGRALS
in which we integrate over all ﬁelds φ(x) that run from φ′(x, −t) to φ′′(x, t). After
expanding the deﬁnition (16.100) of the potential V(φ), we have
⟨φ′′|e−i2tH|φ′⟩=
 φ′′
φ′ exp
#
i

1
2

˙φ2 −(∇φ)2 −m2φ2
−P(φ) d4x
$
Dφ.
(16.117)
This amplitude is a path integral
⟨φ′′|e−i2tH|φ′⟩=
 φ′′
φ′ eiS[φ] Dφ
(16.118)
of phases exp(iS[φ]) that are exponentials of the classical action
S[φ] =

1
2

˙φ2 −(∇φ)2 −m2φ2
−P(φ) d4x.
(16.119)
The time dependence of the Heisenberg ﬁeld operator φ(x, t) is
φ(x, t) = eitHφ(x, 0)e−itH.
(16.120)
The time-ordered product of two ﬁelds, as in (16.88), is the sum
T [φ(x1)φ(x2)] = θ(x0
1 −x0
2)φ(x1)φ(x2) + θ(x0
2 −x0
1)φ(x2)φ(x1).
(16.121)
Between two factors of exp(−itH), it is for t1 > t2
e−itHT [φ(x1)φ(x2)] e−itH = e−i(t−t1)Hφ(x1, 0)e−i(t1−t2)Hφ(x2, 0)e−i(t−t2)H.
So by the logic that led to the path-integral formulas (16.113) and (16.118), we
can write a matrix element of the time-ordered product (16.121) as
⟨φ′′|e−itHT [φ(x1)φ(x2)] e−itH|φ′⟩=
 φ′′
φ′ φ(x1) φ(x2) eiS[φ] Dφ,
(16.122)
in which we integrate over ﬁelds that go from φ′ at time −t to φ′′ at time t. The
time-ordered product of any combination of ﬁelds is then
⟨φ′′|e−itHT [φ(x1) · · · φ(xn)] e−itH|φ′⟩=

φ(x1) · · · φ(xn) eiS[φ] Dφ.
(16.123)
Like the position eigenstates |q′⟩of quantum mechanics, the eigenstates |φ′⟩
are states of inﬁnite energy that overlap most states. Yet we often are interested
in the ground state |0⟩or in states of a few particles. To form such matrix ele-
ments, we multiply both sides of equations (16.118 & 16.123) by ⟨0|φ′′⟩⟨φ′|0⟩
604

16.12 PERTURBATION THEORY
and integrate over φ′ and φ′′. Since the ground state is a normalized eigenstate
of the hamiltonian H|0⟩= E0|0⟩with eigenvalue E0, we ﬁnd from (16.118)

⟨0|φ′′⟩⟨φ′′|e−i2tH|φ′⟩⟨φ′|0⟩Dφ′′Dφ′ = ⟨0|e−i2tH|0⟩
= e−i2tE0 =

⟨0|φ′′⟩eiS[φ]⟨φ′|0⟩DφDφ′′Dφ′
(16.124)
and from (16.123) suppressing the differentials Dφ′′Dφ′,
e−2itE0⟨0|T [φ(x1) · · · φ(xn)] |0⟩=

⟨0|φ′′⟩φ(x1) · · · φ(xn) eiS[φ]⟨φ′|0⟩Dφ.
(16.125)
The mean value in the ground state of a time-ordered product of ﬁeld operators
is then a ratio of these path integrals
⟨0|T [φ(x1) · · · φ(xn)] |0⟩=

⟨0|φ′′⟩φ(x1) · · · φ(xn) eiS[φ]⟨φ′|0⟩Dφ

⟨0|φ′′⟩eiS[φ]⟨φ′|0⟩Dφ
,
(16.126)
in which factors involving E0 have canceled and the integration is over all ﬁelds
that go from φ(x, −t) = φ′(x) to φ(x, t) = φ′′(x) and over φ′(x) and φ′′(x).
16.12 Perturbation theory
Field theories with hamiltonians that are quadratic in their ﬁelds like
H0 =

1
2

π2(x) + (∇φ(x))2 + m2φ2(x)

d3x
(16.127)
are soluble. Their ﬁelds evolve in time as
φ(x, t) = eitH0φ(x, 0)e−itH0.
(16.128)
The mean value in the ground state of H0 of a time-ordered product of these
ﬁelds is by (16.126) a ratio of path integrals
⟨0|T [φ(x1) · · · φ(xn)] |0⟩=

⟨0|φ′′⟩φ(x1) · · · φ(xn) eiS0[φ]⟨φ′|0⟩Dφ

⟨0|φ′′⟩eiS0[φ]⟨φ′|0⟩Dφ
,
(16.129)
605

PATH INTEGRALS
in which the action S0[φ] is quadratic in the ﬁelds
S0[φ] =

1
2

˙φ2(x) −(∇φ(x))2 −m2φ2(x)

d4x
=

1
2

−∂aφ(x)∂aφ(x) −m2φ2(x)

d4x.
(16.130)
So the path integrals in the ratio (16.129) are gaussian and doable.
The Fourier transforms
˜φ(p) =

e−ipxφ(x) d4x
and
φ(x) =

eipx ˜φ(p) d4p
(2π)4
(16.131)
turn the space-time derivatives in the action into a quadratic form
S0[φ] = −1
2

| ˜φ(p)|2 (p2 + m2) d4p
(2π)4 ,
(16.132)
in which p2 = p2 −p02, and ˜φ(−p) = ˜φ∗(p) by (3.25) since the ﬁeld φ is real.
The initial ⟨φ′|0⟩and ﬁnal ⟨0|φ′′⟩wave-functions produce the iϵ in the
Feynman propagator (5.233). Although its exact form doesn’t matter here, the
wave-function ⟨φ′|0⟩of the ground state of H0 is the exponential (15.51)
⟨φ′|0⟩= c exp

−1
2

| ˜φ′(p)|2

p2 + m2 d3p
(2π)3

,
(16.133)
in which ˜φ′(p) is the spatial Fourier transform
˜φ′(p) =

e−ip·x φ′(x) d3x
(16.134)
and c is a normalization factor that will cancel in ratios of path integrals.
Apart from −2i ln c, which we will not keep track of, the wave-functions
⟨φ′|0⟩and ⟨0|φ′′⟩add to the action S0[φ] the term
S0[φ] = i
2
 
p2 + m2

| ˜φ(p, t)|2 + | ˜φ(p, −t)|2
d3p
(2π)3 ,
(16.135)
in which we envision taking the limit t →∞with φ(x, t) = φ′′(x) and φ(x, −t) =
φ′(x). The identity (Weinberg, 1995, pp. 386–388)
f (+∞) + f (−∞) = lim
ϵ→0+ ϵ
 ∞
−∞
f (t) e−ϵ|t| dt
(16.136)
allows us to write S0[φ] as
S0[φ] = lim
ϵ→0+
iϵ
2
 
p2 + m2
 ∞
−∞
| ˜φ(p, t)|2 e−ϵ|t| dt d3p
(2π)3 .
(16.137)
606

16.12 PERTURBATION THEORY
To ﬁrst order in ϵ, the change in the action is (exercise 16.15)
S0[φ] = lim
ϵ→0+
iϵ
2
 
p2 + m2
 ∞
−∞
| ˜φ(p, t)|2 dt d3p
(2π)3
= lim
ϵ→0+
iϵ
2
 
p2 + m2 | ˜φ(p)|2 d4p
(2π)4 .
(16.138)
The modiﬁed action is therefore
S0[φ, ϵ] = S0[φ] + S0[φ] = −1
2

| ˜φ(p)|2

p2 + m2 −iϵ

p2 + m2

d4p
(2π)4
= −1
2

| ˜φ(p)|2 
p2 + m2 −iϵ

d4p
(2π)4
(16.139)
since the square-root is positive. In terms of the modiﬁed action, our formula
(16.129) for the time-ordered product is the ratio
⟨0|T [φ(x1) · · · φ(xn)] |0⟩=

φ(x1) · · · φ(xn) eiS0[φ,ϵ] Dφ

eiS0[φ,ϵ] Dφ
.
(16.140)
We can use this formula (16.140) to express the mean value in the vacuum |0⟩
of the time-ordered exponential of a space-time integral of j(x)φ(x), in which
j(x) is a classical (c-number, external) current j(x), as the ratio
Z0[j] ≡⟨0| T

exp
#
i

j(x) φ(x) d4x
$%
|0⟩
=

exp
#
i

j(x) φ(x) d4x
$
eiS0[φ,ϵ] Dφ

eiS0[φ,ϵ] Dφ
.
(16.141)
Since the state |0⟩is normalized, the mean value Z0[0] is unity,
Z0[0] = 1.
(16.142)
If we absorb the current into the action
S0[φ, ϵ, j] = S0[φ, ϵ] +

j(x) φ(x) d4x
(16.143)
then in terms of the current’s Fourier transform
˜j(p) =

e−ipx j(x) d4x
(16.144)
607

PATH INTEGRALS
the modiﬁed action S0[φ, ϵ, j] is (exercise 16.15)
S0[φ, ϵ, j] = −1
2
 
| ˜φ(p)|2 
p2 + m2 −iϵ

−˜j∗(p) ˜φ(p) −˜φ∗(p)˜j(p)
 d4p
(2π)4 .
(16.145)
Changing variables to
˜ψ(p) = ˜φ(p) −˜j(p)/(p2 + m2 −iϵ)
(16.146)
we write the action S0[φ, ϵ, j] as (exercise 16.17)
S0[φ, ϵ, j] = −1
2
 
| ˜ψ(p)|2 
p2 + m2 −iϵ

−
˜j∗(p)˜j(p)
(p2 + m2 −iϵ)

d4p
(2π)4
= S0[ψ, ϵ] + 1
2
 
˜j∗(p)˜j(p)
(p2 + m2 −iϵ)

d4p
(2π)4 .
(16.147)
And since Dφ = Dψ, our formula (16.141) gives simply (exercise 16.18)
Z0[j] = exp

i
2

|˜j(p)|2
p2 + m2 −iϵ
d4p
(2π)4

.
(16.148)
Going back to position space, one ﬁnds (exercise 16.19)
Z0[j] = exp
# i
2

j(x) (x −x′) j(x′) d4x d4x′
$
,
(16.149)
in which (x −x′) is Feynman’s propagator (5.233)
(x −x′) =

eip(x−x′)
p2 + m2 −iϵ
d4p
(2π)4 .
(16.150)
The functional derivative (chapter 15) of Z0[j], deﬁned by (16.141), is
1
i
δZ0[j]
δj(x) = ⟨0| T
#
φ(x) exp

i

j(x′)φ(x′)d4x′
$
|0⟩
(16.151)
while that of equation (16.149) is
1
i
δZ0[j]
δj(x) = Z0[j]

(x −x′) j(x′) d4x′.
(16.152)
Thus the second functional derivative of Z0[j] evaluated at j = 0 gives
⟨0| T

φ(x)φ(x′)

|0⟩= 1
i2
δ2Z0[j]
δj(x)δj(x′)

j=0
= −i (x −x′).
(16.153)
608

16.13 APPLICATION TO QUANTUM ELECTRODYNAMICS
Similarly, one may show (exercise 16.20) that
⟨0| T [φ(x1)φ(x2)φ(x3)φ(x4)] |0⟩= 1
i4
δ4Z0[j]
δj(x1)δj(x2)δj(x3)δj(x4)

j=0
= −(x1 −x2)(x3 −x4) −(x1 −x3)(x2 −x4)
−(x1 −x4)(x2 −x3).
(16.154)
Suppose now that we add a potential V = P(φ) to the free hamiltonian
(16.127). Scattering amplitudes are matrix elements of the time-ordered expo-
nential T exp

−i
2
P(φ) d4x

. Our formula (16.140) for the mean value in the
ground state |0⟩of the free hamiltonian H0 of any time-ordered product of
ﬁelds leads us to
⟨0|T

exp
#
−i

P(φ) d4x
$%
|0⟩=

exp
#
−i

P(φ) d4x
$
eiS0[φ,ϵ] Dφ

eiS0[φ,ϵ] Dφ
.
(16.155)
Using (16.153 & 16.154), we can cast this expression into the magical form
⟨0|T

exp
#
−i

P(φ) d4x
$%
|0⟩= exp
#
−i

P

δ
iδj(x)

d4x
$
Z0[j]

j=0
.
(16.156)
The generalization of the path-integral formula (16.140) to the ground state
|⟩of an interacting theory with action S is
⟨|T [φ(x1) · · · φ(xn)] |⟩=

φ(x1) · · · φ(xn) eiS[φ,ϵ] Dφ

eiS[φ,ϵ] Dφ
,
(16.157)
in which a term like iϵφ2 is added to make the modiﬁed action S[φ, ϵ].
These are some of the techniques one uses to make states of incoming
and outgoing particles and to compute scattering amplitudes (Weinberg, 1995,
1996; Srednicki, 2007; Zee, 2010).
16.13 Application to quantum electrodynamics
In the Coulomb gauge ∇· A = 0, the QED hamiltonian is
H = Hm +
 
1
2π2 + 1
2(∇× A)2 −A · j

d3x + VC,
(16.158)
609

PATH INTEGRALS
in which Hm is the matter hamiltonian, and VC is the Coulomb term
VC = 1
2
 j0(x, t)j0(y, t)
4π|x −y|
d3xd3y.
(16.159)
The operators A and π are canonically conjugate, but they satisfy the Coulomb-
gauge conditions
∇· A = 0
and
∇· π = 0.
(16.160)
One may show (Weinberg, 1995, pp. 413–418) that in this theory, the analog
of equation (16.157) is
⟨|T [O1 · · · On] |⟩=

O1 · · · On eiSC δ[∇· A] DA Dψ

eiSC δ[∇· A] DA Dψ
,
(16.161)
in which the Coulomb-gauge action is
SC =

1
2 ˙A
2 −1
2(∇× A)2 + A · j + Lm d4x −

VC dt
(16.162)
and the functional delta function
δ[∇· A] =

x
δ(∇· A(x))
(16.163)
enforces the Coulomb-gauge condition. The term Lm is the action density of
the matter ﬁeld ψ.
Tricks are available. We introduce a new ﬁeld A0(x) and consider the factor
F =

exp
#
i

1
2

∇A0 + ∇△−1j02
d4x
$
DA0,
(16.164)
which is just a number independent of the charge density j0 since we can cancel
the j0 term by shifting A0. By integrating by parts, we can write the number F
as (exercise 16.21)
F =

exp
#
i

1
2

∇A02
−A0j0 −1
2j0△−1j0 d4x
$
DA0
=

exp
#
i

1
2

∇A02
−A0j0 d4x + i

VC dt
$
DA0.
(16.165)
So when we multiply the numerator and denominator of the amplitude (16.161)
by F, the awkward Coulomb term cancels, and we get
⟨|T [O1 · · · On] |⟩=

O1 · · · On eiS′ δ[∇· A] DA Dψ

eiS′ δ[∇· A] DA Dψ
(16.166)
610

16.13 APPLICATION TO QUANTUM ELECTRODYNAMICS
where now DA includes all four components Aμ and
S′ =

1
2 ˙A
2 −1
2(∇× A)2 + 1
2

∇A02
+ A · j −A0j0 + Lm d4x.
(16.167)
Since the delta function δ[∇· A] enforces the Coulomb-gauge condition, we can
add to the action S′ the term (∇· ˙A) A0, which is −˙A · ∇A0, after we integrate
by parts and drop the surface term. This extra term makes the action gauge
invariant
S =

1
2( ˙A −∇A0)2 −1
2(∇× A)2 + A · j −A0j0 + Lm d4x
=

−1
4Fab Fab + Abjb + Lm d4x.
(16.168)
Thus at this point we have
⟨|T [O1 · · · On] |⟩=

O1 · · · On eiS δ[∇· A] DA Dψ

eiS δ[∇· A] DA Dψ
,
(16.169)
in which S is the gauge-invariant action (16.168), and the integral is over all
ﬁelds. The only relic of the Coulomb gauge is the gauge-ﬁxing delta functional
δ[∇· A].
We now make the gauge transformation
A′
b(x) = Ab(x) + ∂b(x)
and
ψ′(x) = eiq(x)ψ(x)
(16.170)
and replace the ﬁelds Ab(x) and ψ(x) everywhere in the numerator and (sepa-
rately) in the denominator in the ratio (16.169) of path integrals by their gauge
transforms (16.170) A′
μ(x) and ψ′(x). This change of variables changes nothing;
it’s like replacing
2 ∞
−∞f (x) dx by
2 ∞
−∞f (y) dy, and so
⟨|T [O1 · · · On] |⟩= ⟨|T [O1 · · · On] |⟩′,
(16.171)
in which the prime refers to the gauge transformation (16.170).
We’ve seen that the action S is gauge invariant. So is the measure DA Dψ,
and we now restrict ourselves to operators O1 . . . On that are gauge invariant.
So in the right-hand side of equation (16.171), the replacement of the ﬁelds by
their gauge transforms affects only the term δ[∇· A] that enforces the Coulomb-
gauge condition
⟨|T [O1 · · · On] |⟩=

O1 · · · On eiS δ[∇· A + △) DA Dψ

eiS δ[∇· A + △) DA Dψ
.
(16.172)
611

PATH INTEGRALS
We now have two choices. If we integrate over all gauge functions (x) in
both the numerator and the denominator of this ratio (16.172), then apart from
over-all constants that cancel, the mean value in the vacuum of the time-ordered
product is the ratio
⟨|T [O1 · · · On] |⟩=

O1 · · · On eiS DA Dψ

eiS DA Dψ
,
(16.173)
in which we integrate over all matter ﬁelds, gauge ﬁelds, and gauges. That is, we
do not ﬁx the gauge.
The analogous formula for the euclidean time-ordered product is
⟨|Te [O1 · · · On] |⟩=

O1 · · · On e−Se DA Dψ

e−Se DA Dψ
,
(16.174)
in which the euclidean action Se is the space-time integral of the energy den-
sity. This formula is quite general; it holds in nonabelian gauge theories and is
important in lattice gauge theory.
Our second choice is to multiply the numerator and the denominator of
the ratio (16.172) by the exponential exp[−i 1
2α
2
(−△)2 d4x] and then inte-
grate over (x) separately in the numerator and denominator. This operation
just multiplies the numerator and denominator by the same constant fac-
tor, which cancels. But if before integrating over all gauge transformations
(x), we shift  so that △ decreases by ˙A0, then the exponential factor
is exp[−i 1
2α
2
( ˙A0 −△)2 d4x]. Now when we integrate over (x), the delta
function δ(∇· A + △) replaces △ by
−∇· A in the inserted exponen-
tial, converting it to exp[−i 1
2α
2
( ˙A0 + ∇· A)2 d4x]. The result is to replace the
gauge-invariant action (16.168) with the gauge-ﬁxed action
Sα =

−1
4Fab Fab −α
2 (∂bAb)2 + Abjb + Lm d4x.
(16.175)
This action is Lorentz invariant and so is much easier to work with than the one
(16.162) with the Coulomb term. We can use it to compute scattering ampli-
tudes perturbatively. The mean value of a time-ordered product of operators in
the ground state |0⟩of the free theory is
⟨0|T [O1 · · · On] |0⟩=

O1 · · · On eiSα DA Dψ

eiSα DA Dψ
.
(16.176)
612

16.14 FERMIONIC PATH INTEGRALS
By following steps analogous to those that led to (16.150), one may show
(exercise 16.22) that in Feynman’s gauge, α = 1, the photon propagator is
⟨0|T

Aμ(x)Aν(y)

|0⟩= −i△μν(x−y) = −i

ημν
q2 −iϵ eiq·(x−y) d4q
(2π)4 . (16.177)
16.14 Fermionic path integrals
In our brief introduction (1.11–1.12) and (1.43–1.45) to Grassmann variables,
we learned that because
θ2 = 0
(16.178)
the most general function f (θ) of a single Grassmann variable θ is
f (θ) = a + b θ.
(16.179)
So a complete integral table consists of the integral of this linear function

f (θ) dθ =

a + b θ dθ = a

dθ + b

θ dθ.
(16.180)
This equation has two unknowns, the integral
2
dθ of unity and the integral
2
θ dθ of θ. We choose them so that the integral of f (θ + ζ)

f (θ + ζ) dθ =

a + b (θ + ζ) dθ = (a + b ζ)

dθ + b

θ dθ
(16.181)
is the same as the integral (16.180) of f (θ). Thus the integral
2
dθ of unity must
vanish, while the integral
2
θ dθ of θ can be any constant, which we choose to
be unity. Our complete table of integrals is then

dθ = 0
and

θ dθ = 1.
(16.182)
The anticommutation relations for a fermionic degree of freedom ψ are
{ψ, ψ†} ≡ψ ψ† + ψ†ψ = 1
and
{ψ, ψ} = {ψ†, ψ†} = 0.
(16.183)
Because ψ has ψ†, it is conventional to introduce a variable θ∗= θ† that
anticommutes with itself and with θ
{θ∗, θ∗} = {θ∗, θ} = {θ, θ} = 0.
(16.184)
The logic that led to (16.182) now gives

dθ∗= 0
and

θ∗dθ∗= 1.
(16.185)
We deﬁne the reference state |0⟩as |0⟩≡ψ|s⟩for a state |s⟩that is not
annihilated by ψ. Since ψ2 = 0, the operator ψ annihilates the state |0⟩
613

PATH INTEGRALS
ψ|0⟩= ψ2|s⟩= 0.
(16.186)
The effect of the operator ψ on the state
|θ⟩= exp

ψ†θ −1
2θ∗θ

|0⟩=

1 + ψ†θ −1
2θ∗θ

|0⟩
(16.187)
is
ψ|θ⟩= ψ(1 + ψ†θ −1
2θ∗θ)|0⟩= ψψ†θ|0⟩= (1 −ψ†ψ)θ|0⟩= θ|0⟩(16.188)
while that of θ on |θ⟩is
θ|θ⟩= θ(1 + ψ†θ −1
2θ∗θ)|0⟩= θ|0⟩.
(16.189)
The state |θ⟩therefore is an eigenstate of ψ with eigenvalue θ
ψ|θ⟩= θ|θ⟩.
(16.190)
The bra corresponding to the ket |ζ⟩is
⟨ζ| = ⟨0|

1 + ζ ∗ψ −1
2ζ ∗ζ

(16.191)
and the inner product ⟨ζ|θ⟩is (exercise 16.23)
⟨ζ|θ⟩= ⟨0|

1 + ζ ∗ψ −1
2ζ ∗ζ
 
1 + ψ†θ −1
2θ∗θ

|0⟩
= ⟨0|1 + ζ ∗ψψ†θ −1
2ζ ∗ζ −1
2θ∗θ + 1
4ζ ∗ζθ∗θ|0⟩
= ⟨0|1 + ζ ∗θ −1
2ζ ∗ζ −1
2θ∗θ + 1
4ζ ∗ζθ∗θ|0⟩
= exp
#
ζ ∗θ −1
2

ζ ∗ζ + θ∗θ
$
.
(16.192)
Example 16.2 (A gaussian integral)
For any number c, we can compute the
integral of exp(c θ∗θ) by expanding the exponential

ec θ∗θ dθ∗dθ =

(1 + c θ∗θ) dθ∗dθ =

(1 −c θ θ∗) dθ∗dθ = −c, (16.193)
a formula that we’ll use over and over.
The identity operator for the space of states
c|0⟩+ d|1⟩≡c|0⟩+ dψ†|0⟩
(16.194)
is (exercise 16.24) the integral
I =

|θ⟩⟨θ| dθ∗dθ = |0⟩⟨0| + |1⟩⟨1|,
(16.195)
614

16.14 FERMIONIC PATH INTEGRALS
in which the differentials anticommute with each other and with other
fermionic variables: {dθ, dθ∗} = 0, {dθ, θ} = 0, {dθ, ψ} = 0, and so forth.
The case of several Grassmann variables θ1, θ2, . . . , θn and several Fermi
operators ψ1, ψ2, . . . , ψn is similar. The θk anticommute among themselves
{θi, θj} = {θi, θ∗
j } = {θ∗
i , θ∗
j } = 0
(16.196)
while the ψk satisfy
{ψk, ψ†
ℓ} = δkℓ
and
{ψk, ψl} = {ψ†
k, ψ†
ℓ} = 0.
(16.197)
The reference state |0⟩is
|0⟩=
 n

k=1
ψk

|s⟩,
(16.198)
in which |s⟩is any state not annihilated by any ψk (so the resulting |0⟩isn’t zero).
The direct-product state
|θ⟩≡exp
 n

k=1
ψ†
kθk −1
2θ∗
kθk

|0⟩=
 n

k=1

1 + ψ†
kθk −1
2θ∗
kθk

|0⟩
(16.199)
is (exercise 16.25) a simultaneous eigenstate of each ψk
ψk|θ⟩= θk|θ⟩.
(16.200)
It follows that
ψℓψk|θ⟩= ψℓθk|θ⟩= −θkψℓ|θ⟩= −θkθℓ|θ⟩= θℓθk|θ⟩
(16.201)
and so too ψkψℓ|θ⟩= θkθℓ|θ⟩. Since the ψs anticommute, their eigenvalues
must also
θℓθk|θ⟩= ψℓψk|θ⟩= −ψkψℓ|θ = −θkθℓ|θ⟩,
(16.202)
which is why they must be Grassmann variables.
The inner product ⟨ζ|θ⟩is
⟨ζ|θ⟩= ⟨0|
 n

k=1
(1 + ζ ∗
k ψk −1
2ζ ∗
k ζk)
  n

ℓ=1
(1 + ψ†
ℓθℓ−1
2θ∗
ℓθℓ)

|0⟩
= exp
 n

k=1
ζ ∗
k θk −1
2

ζ ∗
k ζk + θ∗
kθk


= eζ †θ−(ζ †ζ+θ†θ)/2.
(16.203)
The identity operator is
I =

|θ⟩⟨θ|
n

k=1
dθ∗
kdθk.
(16.204)
615

PATH INTEGRALS
Example 16.3 (Gaussian Grassmann integral)
For any 2 × 2 matrix A, we may
compute the gaussian integral
g(A) =

e−θ†Aθ dθ∗
1 dθ1dθ∗
2 dθ2
(16.205)
by expanding the exponential. The only terms that survive are the ones that have
exactly one of each of the four variables θ1, θ2, θ∗
1 , and θ∗
2 . Thus, the integral is
the determinant of the matrix A
g(A) =
 1
2

θ†
kAkℓθℓ
2
dθ∗
1 dθ1dθ∗
2 dθ2
=
 
θ∗
1 A11θ1 θ∗
2 A22θ2 + θ∗
1 A12θ2 θ∗
2 A21θ1

dθ∗
1 dθ1dθ∗
2 dθ2
= A11A22 −A12A21 = det A.
(16.206)
The natural generalization to n dimensions

e−θ†Aθ
n

k=1
dθ∗
kdθk = det A
(16.207)
is true for any n×n matrix A. If A is invertible, then the invariance of Grassmann
integrals under translations implies that

e−θ†Aθ+θ†ζ+ζ †θ
n

k=1
dθ∗
kdθk =

e−θ†A(θ+A−1ζ)+θ†ζ+ζ †(θ+A−1ζ)
n

k=1
dθ∗
kdθk
=

e−θ†Aθ+ζ †θ+ζ †A−1ζ
n

k=1
dθ∗
kdθk
=

e−(θ†+ζ †A−1)Aθ+ζ †θ+ζ †A−1ζ
n

k=1
dθ∗
kdθk
=

e−θ†Aθ+ζ †A−1ζ
n

k=1
dθ∗
kdθk
= det A eζ †A−1ζ.
(16.208)
The values of θ and θ† that make the argument −θ†Aθ + θ†ζ + ζ †θ of the
exponential stationary are θ = A−1ζ and θ† = ζ †A−1. So a gaussian Grassmann
integral is equal to its exponential evaluated at its stationary point, apart from a
prefactor involving the determinant det A. This result is a fermionic echo of the
bosonic results (16.13–16.15).
616

16.14 FERMIONIC PATH INTEGRALS
One may further extend these deﬁnitions to a Grassmann ﬁeld χm(x) and an
associated Dirac ﬁeld ψm(x). The χm(x)s anticommute among themselves and
with all fermionic variables at all points of space-time
{χm(x), χn(x′)} = {χ∗
m(x), χn(x′)} = {χ∗
m(x), χ∗
n (x′)} = 0
(16.209)
and the Dirac ﬁeld ψm(x) obeys the equal-time anticommutation relations
{ψm(x, t), ψ†
n(x′, t)} = δmn δ(x −x′) (n, m = 1, . . . , 4)
{ψm(x, t), ψn(x′, t)} = {ψ†
m(x, t), ψ†
n(x′, t)} = 0.
(16.210)
As in (16.102 & 16.198), we use eigenstates of the ﬁeld ψ at t = 0. If |0⟩is
deﬁned in terms of a state |s⟩that is not annihilated by any ψm(x, 0) as
|0⟩=

m,x
ψm(x, 0)

|s⟩
(16.211)
then (exercise 16.26) the state
|χ⟩= exp
 
m
ψ†
m(x, 0) χm(x) −1
2χ∗
m(x)χm(x) d3x

|0⟩
= exp

ψ†χ −1
2χ†χ d3x

|0⟩
(16.212)
is an eigenstate of the operator ψm(x, 0) with eigenvalue χm(x)
ψm(x, 0)|χ⟩= χm(x)|χ⟩.
(16.213)
The inner product of two such states is (exercise 16.27)
⟨χ′|χ⟩= exp
#
χ′†χ −1
2χ′†χ′ −1
2χ†χ d3x
$
.
(16.214)
The identity operator is the integral
I =

|χ⟩⟨χ| Dχ∗Dχ,
(16.215)
in which
Dχ∗Dχ ≡

m,x
dχ∗
m(x)dχm(x).
(16.216)
The hamiltonian for a free Dirac ﬁeld ψ of mass m is the spatial integral
H0 =

ψ (γ · ∇+ m) ψ d3x,
(16.217)
in which ψ ≡iψ†γ 0 and the gamma matrices (10.286) satisfy
{γ a, γ b} = 2 ηab
(16.218)
617

PATH INTEGRALS
where η is the 4×4 diagonal matrix with entries (−1, 1, 1, 1). Since ψ|χ⟩= χ|χ⟩
and ⟨χ′|ψ† = ⟨χ′|χ′†, the quantity ⟨χ′| exp( −iϵH0)|χ⟩is by (16.214)
⟨χ′|e−iϵH0|χ⟩=

⟨χ′|χ⟩exp
#
−iϵ

χ′ (γ · ∇+ m) χ d3x
$
=

exp
#
1
2(χ′† −χ†)χ −1
2χ′†(χ′ −χ) −iϵχ′(γ ·∇+ m)χd3x
$
=

exp

ϵ
 
1
2 ˙χ†χ −1
2χ′† ˙χ −iχ′ (γ · ∇+ m) χ

d3x
%
λ,
(16.219)
in which χ′†−χ† = ϵ ˙χ† and χ′−χ = ϵ ˙χ. Everything within the square brackets
is multiplied by ϵ, so we may replace χ′† by χ† and χ′ by χ so as to write to ﬁrst
order in ϵ
⟨χ′|e−iϵH0|χ⟩=

exp
#
ϵ

1
2 ˙χ†χ −1
2χ† ˙χ −iχ (γ · ∇+ m) χ d3x
$
,
(16.220)
in which the dependence upon χ′ is through the time derivatives.
Putting together n
=
2t/ϵ such matrix elements, integrating over all
intermediate-state dyadics |χ⟩⟨χ|, and using our formula (16.215), we ﬁnd
⟨χt|e−2itH0|χ−t⟩=

exp
#
1
2 ˙χ†χ −1
2χ† ˙χ −iχ (γ ·∇+ m) χd4x
$
Dχ∗Dχ.
(16.221)
Integrating ˙χ†χ by parts and dropping the surface term, we get
⟨χt|e−2itH0|χ−t⟩=

exp
#
−χ† ˙χ −iχ (γ ·∇+ m) χ d4x
$
Dχ∗Dχ. (16.222)
Since −χ† ˙χ = −iχγ 0 ˙χ, the argument of the exponential is
i

−χγ 0 ˙χ −χ (γ · ∇+ m) χ d4x = i

−χ

γ μ∂μ + m

χ d4x.
(16.223)
We then have
⟨χt|e−2itH0|χ−t⟩=

exp

i

L0(χ) d4x

Dχ∗Dχ,
(16.224)
in which L0(χ) = −χ

γ μ∂μ + m

χ is the action density (10.289) for a free
Dirac ﬁeld. Thus the amplitude is a path integral with phases given by the
classical action S0[χ]
⟨χt|e−2itH0|χ−t⟩=

ei
2
L0(χ) d4xDχ∗Dχ =

eiS0[χ]Dχ∗Dχ
(16.225)
618

16.15 APPLICATION TO NONABELIAN GAUGE THEORIES
and the integral is over all ﬁelds that go from χ(x, −t) = χ−t(x) to χ(x, t) =
χt(x). Any normalization factor will cancel in ratios of such integrals.
Since Fermi ﬁelds anticommute, their time-ordered product has an extra
minus sign
T

ψ(x1)ψ(x2)

= θ(x0
1 −x0
2) ψ(x1) ψ(x2) −θ(x0
2 −x0
1) ψ(x2) ψ(x1). (16.226)
The logic behind our formulas (16.123) and (16.129) for the time-ordered prod-
uct of bosonic ﬁelds now leads to an expression for the time-ordered product of
2n Dirac ﬁelds (with Dχ′′ and Dχ′ suppressed)
⟨0|T

ψ(x1) · · · ψ(x2n)

|0⟩=

⟨0|χ′′⟩χ(x1) · · · χ(x2n) eiS0[χ]⟨χ′|0⟩Dχ∗Dχ

⟨0|χ′′⟩eiS0[χ]⟨χ′|0⟩Dχ∗Dχ
.
(16.227)
As in (16.140), the effect of the inner products ⟨0|χ′′⟩and ⟨χ′|0⟩is to insert
ϵ-terms, which modify the Dirac propagators
⟨0|T

ψ(x1) · · · ψ(x2n)

|0⟩=

χ(x1) · · · χ(x2n) eiS0[χ,ϵ] Dχ∗Dχ

eiS0[χ,ϵ] Dχ∗Dχ
.
(16.228)
Imitating (16.141), we introduce a Grassmann external current ζ(x) and
deﬁne a fermionic analog of Z0[j]
Z0[ζ] ≡⟨0| T

ei
2
ζψ+ψζ d4x
|0⟩=

ei
2
ζχ+χζ d4xeiS0[χ,ϵ]Dχ∗Dχ

eiS0[χ,ϵ]Dχ∗Dχ
.
(16.229)
16.15 Application to nonabelian gauge theories
The action of a generic nonabelian gauge theory is
S =

−1
4FaμνFμν
a
−ψ

γ μDμ + m

ψ d4x,
(16.230)
in which the Maxwell ﬁeld is
Faμν ≡∂μAaν −∂νAaμ + g fabc Abμ Acν
(16.231)
and the covariant derivative is
Dμψ ≡∂μψ −ig ta Aaμ ψ.
(16.232)
Here g is a coupling constant, fabc is a structure constant (10.63), and ta is a
generator (10.55) of the Lie algebra (section 10.15) of the gauge group.
619

PATH INTEGRALS
One may show (Weinberg, 1996, pp. 14–18) that the analog of equation
(16.169) for quantum electrodynamics is
⟨|T [O1 · · · On] |⟩=

O1 · · · On eiS δ[Aa3] DA Dψ

eiS δ[Aa3] DA Dψ
,
(16.233)
in which the functional delta function
δ[Aa3] ≡

x,b
δ(Aa3(x))
(16.234)
enforces the axial-gauge condition, and Dψ stands for Dψ∗Dψ.
Initially, physicists had trouble computing nonabelian amplitudes beyond the
lowest order of perturbation theory. Then DeWitt showed how to compute to
second order (DeWitt, 1967), and Faddeev and Popov, using path integrals,
showed how to compute to all orders (Faddeev and Popov, 1967).
16.16 The Faddeev–Popov trick
The path-integral tricks of Faddeev and Popov are described in Weinberg (1996,
pp. 19–27). We will use gauge-ﬁxing functions Ga(x) to impose a gauge
condition on our nonabelian gauge ﬁelds Aa
μ(x). For instance, we can use
Ga(x) = A3
a(x) to impose an axial gauge or Ga(x) = i∂μAμ
a (x) to impose a
Lorentz-invariant gauge.
Under an inﬁnitesimal gauge transformation (11.511)
Aλ
aμ = Aaμ −∂μλa −g fabc Abμ λc
(16.235)
the gauge ﬁelds change, and so the gauge-ﬁxing functions Gb(x), which depend
upon them, also change. The jacobian J of that change is
J = det
δGλ
a(x)
δλb(y)

λ=0
≡DGλ
Dλ

λ=0
(16.236)
and it typically involves the delta function δ4(x −y).
Let B[G] be any functional of the gauge-ﬁxing functions Gb(x) such as
B[G] =

x,a
δ(Ga(x)) =

x,a
δ(A3
a(x))
(16.237)
in an axial gauge or
B[G] = exp
# i
2

(Ga(x))2 d4x
$
= exp
#
−i
2
 
∂μAμ
a (x)
2 d4x
$
(16.238)
in a Lorentz-invariant gauge.
620

16.16 THE FADDEEV–POPOV TRICK
We want to understand functional integrals like (16.233)
⟨|T [O1 · · · On] |⟩=

O1 · · · On eiS B[G] J DA Dψ

eiS B[G] J DA Dψ
,
(16.239)
in which the operators Ok, the action functional S[A], and the differen-
tials DADψ (but not the gauge-ﬁxing functional B[G] or the Jacobian J)
are gauge invariant. The axial-gauge formula (16.233) is a simple example in
which B[G] = δ[Aa3] enforces the axial-gauge condition Aa3(x) = 0 and the
determinant J = det (δab∂3δ(x −y)) is a constant that cancels.
If we translate the gauge ﬁelds by a gauge transformation , then the ratio
(16.239) does not change
⟨|T [O1 · · · On] |⟩=

O
1 · · · O
n eiS B[G] J DA Dψ

eiS B[G] J DA Dψ
(16.240)
any more than
2
f (y) dy is different from
2
f (x) dx. Since the operators Ok, the
action functional S[A], and the differentials DADψ are gauge invariant, most
of the -dependence goes away
⟨|T [O1 · · · On] |⟩=

O1 · · · On eiS B[G] J DA Dψ

eiS B[G] J DA Dψ
.
(16.241)
Let λ be a gauge transformation  followed by an inﬁnitesimal gauge trans-
formation λ. The jacobian J is a determinant of a product of matrices, which
is a product of their determinants
J = det
δGλ
a (x)
δλb(y)

λ=0
= det
 δGλ
a (x)
δλc(z)
δλc(z)
δλb(y) d4z

λ=0
= det
δGλ
a (x)
δλc(z)

λ=0
det
δλc(z)
δλb(y)

λ=0
= det
δG
a (x)
δc(z)

det
δλc(z)
δλb(y)

λ=0
≡DG
D
Dλ
Dλ

λ=0
.
(16.242)
Now we integrate over the gauge transformation  with weight function
ρ() = (Dλ/Dλ|λ=0)−1 and ﬁnd, since the ratio (16.241) is -independent,
⟨|T [O1 · · · On] |⟩=

O1 · · · On eiS B[G] DG
D D DA Dψ

eiS B[G] DG
D D DA Dψ
621

PATH INTEGRALS
=

O1 · · · On eiS B[G] DG DA Dψ

eiS B[G] DG DA Dψ
=

O1 · · · On eiS DA Dψ

eiS DA Dψ
.
(16.243)
Thus the mean-value in the vacuum of a time-ordered product of gauge-
invariant operators is a ratio of path integrals over all gauge ﬁelds without
any gauge ﬁxing. No matter what gauge condition G or gauge-ﬁxing functional
B[G] we use, the resulting gauge-ﬁxed ratio (16.239) is equal to the ratio (16.243)
of path integrals over all gauge ﬁelds without any gauge ﬁxing. All gauge-ﬁxed
ratios (16.239) give the same time-ordered products, and so we can use whatever
gauge condition G or gauge-ﬁxing functional B[G] is most convenient.
The analogous formula for the euclidean time-ordered product is
⟨|Te [O1 · · · On] |⟩=

O1 · · · On e−Se DA Dψ

e−Se DA Dψ
(16.244)
where the euclidean action Se is the space-time integral of the energy density.
This formula is the basis for lattice gauge theory.
The path-integral formulas (16.173 & 16.244) derived for quantum electro-
dynamics therefore also apply to nonabelian gauge theories.
16.17 Ghosts
Faddeev and Popov showed how to do perturbative calculations in which one
does ﬁx the gauge. To continue our description of their tricks, we return to the
gauge-ﬁxed expression (16.239) for the time-ordered product
⟨|T [O1 · · · On] |⟩=

O1 · · · On eiS B[G] J DA Dψ

eiS B[G] J DA Dψ
,
(16.245)
set Gb(x) = −i∂μAμ
b (x), and use (16.238) as the gauge-ﬁxing functional B[G]
B[G] = exp
# i
2

(Ga(x))2 d4x
$
= exp
#
−i
2
 
∂μAμ
a (x)
2 d4x
$
.
(16.246)
622

16.17 GHOSTS
This functional adds to the action density the term −(∂μAμ
a )2/2, which leads
to a gauge-ﬁeld propagator like the photon’s (16.177)
⟨0|T

Aa
μ(x)Ab
ν(y)

|0⟩= −iδab△μν(x −y) = −i
 ημνδab
q2 −iϵ eiq·(x−y) d4q
(2π)4 .
(16.247)
What about the determinant J? Under an inﬁnitesimal gauge transformation
(16.235), the gauge ﬁeld becomes
Aλ
aμ = Aaμ −∂μλa −g fabc Abμ λc
(16.248)
and so Gλ
a(x) = i∂μAλ
aμ(x) is
Gλ
a(x) = i∂μAaμ(x) + i∂μ
 
−δac∂μ −g fabcAbμ(x)

δ4(x −y)λc(y) d4y.
(16.249)
The jacobian J then is the determinant (16.236) of the matrix
δGλ
a(x)
δλc(y)

λ=0
= −iδac 2 δ4(x −y) −ig fabc
∂
∂xμ

Aμ
b (x)δ4(x −y)

, (16.250)
that is
J = det

−iδac 2 δ4(x −y) −ig fabc
∂
∂xμ

Aμ
b (x)δ4(x −y)

.
(16.251)
But we’ve seen (16.207) that a determinant can be written as a fermionic path
integral
det A =

e−θ†A θ
n

k=1
dθ∗
kdθk.
(16.252)
So we can write the jacobian J as
J =

exp
#
iω∗
a2ωa + igfabcω∗
a∂μ(Aμ
b ωc) d4x
$
Dω∗Dω,
(16.253)
which contributes the terms −∂μω∗
a∂μωa and
−∂μω∗
a g fabc Aμ
b ωc = ∂μω∗
a g fabc Aμ
c ωb
(16.254)
to the action density.
Thus we can do perturbation theory by using the modiﬁed action density
L′ = −1
4FaμνFμν
a
−1
2

∂μAμ
a
2 −∂μω∗
a∂μωa +∂μω∗
a g fabc Aμ
c ωb −ψ (̸D + m) ψ,
(16.255)
623

PATH INTEGRALS
in which ̸D ≡γ μDμ = γ μ(∂μ −igtaAaμ). The ghost ﬁeld ω is a mathematical
device, not a physical ﬁeld describing real particles, which would be spinless
fermions violating the spin-statistics theorem (example 10.20).
Further reading
The detailed Quantum Field Theory (Srednicki, 2007), the classics The Quantum
Theory of Fields I, II, III (Weinberg, 1995, 1996, 2005), and the delightfully
readable Quantum Field Theory in a Nutshell (Zee, 2010) all provide excellent
treatments of path integrals.
Exercises
16.1
Derive the multiple gaussian integral (16.8) from (5.167).
16.2
Derive the multiple gaussian integral (16.12) from (5.166).
16.3
Show that the vector Y that makes the argument of the multiple gaussian
integral (16.12) stationary is given by (16.13), and that the multiple gaussian
integral (16.12) is equal to its exponential evaluated at its stationary point Y
apart from a prefactor involving det iS.
16.4
Repeat the previous exercise for the multiple gaussian integral (16.11).
16.5
Compute the double integral (16.23) for the case V(qj) = 0.
16.6
Insert into the LHS of (16.54) a complete set of momentum dyadics |p⟩⟨p|,
use the inner product ⟨q|p⟩= exp(iqp)/
√
2π, do the resulting Fourier
transform, and so verify the free-particle path integral (16.54).
16.7
By taking the nonrelativistic limit of the formula (11.311) for the action of
a relativistic particle of mass m and charge q, derive the expression (16.55)
for the action of a nonrelativistic particle in an electromagnetic ﬁeld with no
scalar potential.
16.8
Show that for the hamiltonian (16.60) of the simple harmonic oscillator the
action S[qc] of the classical path is (16.67).
16.9
Show that the harmonic-oscillator action of the loop (16.68) is (16.69).
16.10 Show that the harmonic-oscillator amplitude (16.72) for q′ = 0 and q′′ =
q reduces as t →0 to the one-dimensional version of the free-particle
amplitude (16.54).
16.11 Work out the path-integral formula for the amplitude for a mass m to
fall to the ground from height h in a gravitational ﬁeld of local accelera-
tion g to lowest order and then including loops. Hint: use the technique of
section 16.7.
16.12 Show that the action (16.74) of the stationary solution (16.77) is (16.79).
16.13 Derive formula (16.132) for the action S0[φ] from (16.130 & 16.131).
16.14 Derive identity (16.136). Split the time integral at t = 0 into two halves, use
ϵ e±ϵt = ± d
dt e±ϵt,
(16.256)
and then integrate each half by parts.
624

EXERCISES
16.15 Derive the third term in equation (16.138) from the second term.
16.16 Use (16.143) and the Fourier transform (16.144) of the external current to
derive formula (16.145).
16.17 Derive equation (16.147) from equations (16.145 & 16.146).
16.18 Derive the formula (16.148) for Z0[j] from the expression (16.147) for
S0[φ, ϵ, j].
16.19 Derive equations (16.149 & 16.150) from formula (16.148).
16.20 Derive equation (16.154) from the formula (16.149) for Z0[j].
16.21 Show that the time integral of the Coulomb term (16.159) is the negative of
the term that is quadratic in j0 in the number F deﬁned by (16.164).
16.22 By following steps analogous to those that led to (16.150), derive the formula
(16.177) for the photon propagator in Feynman’s gauge.
16.23 Derive expression (16.192) for the inner product ⟨ζ|θ⟩.
16.24 Derive the representation (16.195) of the identity operator I for a single
fermionic degree of freedom from the rules (16.182 & 16.185) for Grassmann
integration and the anticommutation relations (16.178 & 16.184).
16.25 Derive the eigenvalue equation (16.200) from the deﬁnition (16.198 &
16.199) of the eigenstate |θ⟩and the anticommutation relations (16.196 &
16.197).
16.26 Derive the eigenvalue relation (16.213) for the Fermi ﬁeld ψm(x, t) from the
anticommutation relations (16.209 & 16.210) and the deﬁnitions (16.211 &
16.212).
16.27 Derive the formula (16.214) for the inner product ⟨χ′|χ⟩from the deﬁnition
(16.212) of the ket |χ⟩.
625

17
The renormalization group
17.1 The renormalization group in quantum ﬁeld theory
Most quantum ﬁeld theories are nonlinear with inﬁnitely many degrees of
freedom and, because they describe point particles, they are rife with inﬁni-
ties. But short-distance effects, probably the ﬁnite sizes of the fundamental
constituents of matter, mitigate these inﬁnities so that we can cope with them
consistently without knowing what happens at very short distances and very
high energies. This procedure is called renormalization.
For instance, in the theory described by the Lagrange density
L = −1
2∂νφ ∂νφ −1
2m2φ2 −g
24φ4
(17.1)
we can cut off divergent integrals at some high energy . The amplitude for
the elastic scattering of two bosons of initial 4-momenta p1 and p2 and ﬁnal
momenta p′
1 and p′
2 to one-loop order (Weinberg, 1996, chap. 18) then takes the
simple form (Zee, 2010, chaps. III & VI)
A = g −
g2
32π2

ln

6
stu

+ iπ + 3

(17.2)
as long as the absolute values of the Mandelstam variables s = −(p1 + p2)2, t =
−(p1 −p′
1)2, and u = −(p1 −p′
2)2, which satisfy stu > 0 and s+t+u = 4m2, are
all much larger than m2 (Stanley Mandelstam, 1928–). We deﬁne the physical
coupling constant gμ, as opposed to the bare one g that comes with L, to be the
real part of the amplitude A at s = −t = −u = μ2
gμ = g −3g2
32π2

ln

2
μ2

+ 1

.
(17.3)
626

17.1 THE RENORMALIZATION GROUP IN QUANTUM FIELD THEORY
Thus the bare coupling constant is g = gμ + 3g2 
ln(2/μ2) + 1

, and using
this formula, we can write our expression (17.2) for the amplitude A in a form
in which the cut off  no longer appears
A = gμ −
g2
32π2

ln

μ6
stu

+ iπ

.
(17.4)
This is the magic of renormalization.
The physical coupling “constant” gμ is the right coupling at energy μ because
when all the Mandelstam variables are near the renormalization point stu = μ6,
the one-loop correction is tiny, and A ≈gμ.
How does the physical coupling gμ depend upon the energy μ? The ampli-
tude A must be independent of the renormalization energy μ, and so
dA
dμ = dgμ
dμ −
g2
32π2
6
μ = 0,
(17.5)
which is a version of the Callan–Symanzik equation.
We assume that when the cut off  is big but ﬁnite, the bare and running
coupling constants g and gμ are so tiny that they differ by terms of order g2
or g2
μ. Then to lowest order in g and gμ, we can replace g2 by g2
μ in (17.5) and
arrive at the simple differential equation
μ dgμ
dμ ≡β(gμ) =
3 g2
μ
16π2 ,
(17.6)
which we can integrate
ln E
M =
 E
M
dμ
μ =
 gE
gM
dgμ
β(gμ) = 16π2
3
 gE
gM
dgμ
g2μ
= 16π2
3
 1
gM
−1
gE

(17.7)
to ﬁnd the running physical coupling constant gμ at energy μ = E
gE =
gM
1 −3 gM ln(E/M)/16π2 .
(17.8)
As the energy E = √s rises above M, while staying below the singular value
E = M exp(16π2/3gM), the running coupling gE slowly increases. And so does
the scattering amplitude, A ≈gE.
Example 17.1 (Quantum electrodynamics)
Vacuum polarization makes the
amplitude for the scattering of two electrons proportional to
A(q2) = e2 
1 + π(q2)

(17.9)
627

THE RENORMALIZATION GROUP
rather than to e2. Here e is the renormalized charge, q = p′
1 −p1 is the
4-momentum transferred to the ﬁrst electron, and
π(q2) = e2
2π2
 1
0
x(1 −x) ln

1 + q2x(1 −x)
m2

dx
(17.10)
represents the polarization of the vacuum. We deﬁne the square of the running
coupling constant e2
μ to be the amplitude (17.9) at q2 = μ2
e2
μ = A(μ2) = e2 
1 + π(μ2)

.
(17.11)
For μ2 ≫m2, the vacuum polarization term π(μ2) is (exercise 17.1)
π(μ2) ≈e2
6π2
#
ln μ
m −5
6
$
.
(17.12)
The amplitude (17.9) then is
A(q2) = e2
μ
1 + π(q2)
1 + π(μ2)
(17.13)
and, since it must be independent of μ, we have
0 = d
dμ
A(q2)
1 + π(q2) = d
dμ
e2
μ
1 + π(μ2) ≈d
dμ
3
e2
μ

1 −π(μ2)
4
.
(17.14)
So we ﬁnd
0 = 2eμ
deμ
dμ
 
1 −π(μ2)

−e2
μ
dπ(μ2)
dμ
= 2eμ
deμ
dμ
 
1 −π(μ2)

−e2
μ
e2
6π2μ.
(17.15)
Thus, since by (17.10 & 17.11) π(μ2) = O(e2) and e2
μ = e2 + O(e4), we ﬁnd to
lowest order in eμ
μdeμ
dμ ≡β(μ) =
e3
μ
12π2 .
(17.16)
We can integrate this differential equation
ln E
M =
 E
M
dμ
μ =
 eE
eM
deμ
β(eμ) = 12π2
 eE
eM
deμ
e3μ
= 6π2

1
e2
M
−1
e2
E

(17.17)
and so get for the running coupling constant the formula
e2
E =
e2
M
1 −e2
M ln(E/M)/6π2 ,
(17.18)
which shows that it slowly increases with the energy E. Thus, the ﬁne-structure
constant e2
μ/4π rises from α = 1/137.036 at me to
e2(45.5 GeV)
4π
=
α
1 −2α ln(45.5/0.00051)/3π =
1
134.6
(17.19)
628

17.1 THE RENORMALIZATION GROUP IN QUANTUM FIELD THEORY
at √s = 91 GeV. When all light charged particles are included, one ﬁnds that
the ﬁne-structure constant rises to α = 1/128.87 at E = 91 GeV.
Example 17.2 (Quantum chromodynamics)
Because of the cubic interaction of
the gauge ﬁelds of a nonabelian gauge theory, the running coupling constant gμ
can slowly decrease with rising energy. If the gauge group is SU(3), then due to
this cubic interaction and that of the ghost ﬁelds (16.255), the running coupling
constant gμ is to order g3
M
gμ = gM

1 −11g2
M
16π2 ln
 μ
M

.
(17.20)
It differs from gM only by terms of order g3
M and so satisﬁes the differential
equation
μdgμ
dμ ≡β(gμ) = −11g3
M
16π2 ≈−
11g3
μ
16π2 ,
(17.21)
in which the β-function is negative. Integrating
ln E
M =
 E
M
dμ
μ =
 gG
gM
dgμ
β(gμ) = −16π2
11
 gE
gM
dgμ
g3μ
= 8π2
11

1
g2
M
−1
g2
E

(17.22)
we ﬁnd
g2
E = g2
M

1 + 11g2
M
8π2
ln E
M
−1
,
(17.23)
which shows that as the energy E of a scattering process increases, the run-
ning coupling slowly decreases, going to zero at inﬁnite energy, an effect
calledasymptotic freedom.
If the gauge group is SU(N), and the theory has nf ﬂavors of quarks with
masses below μ, then the beta function is
β(gμ) = −
g3
μ
4π2
11N
12 −nf
6

,
(17.24)
which remains negative as long as nf < 11N/2. Using this beta function with
N = 3 and again integrating, we get instead of (17.23)
g2
E = g2
M

1 + (11 −2nf/3)g2
M
16π2
ln E2
M2
−1
.
(17.25)
So with
2 ≡M2 exp

−
16π2
(11 −2nf/3)g2
M

(17.26)
629

THE RENORMALIZATION GROUP
20
40
60
80
100
120
140
0.1
0.12
0.14
0.16
0.18
0.2
0.22
Asymptotic freedom
Energy E (GeV)
Coupling strength αs(E)
Figure 17.1
The strong-structure constant αs(E) as given by the one-loop formula
(17.27) (thin curve) and by a three-loop formula (thick curve) with  = 230 MeV
and nf = 5 is plotted for mb ≪E ≪mt.
we ﬁnd (exercise 17.2)
αs(E) ≡g2(E)
4π
=
12π
(33 −2nf) ln(E2/2).
(17.27)
This formula expresses the dimensionless strong-structure constant αs(E) appro-
priate to energy E in terms of a parameter  that has the dimension of energy.
Some call this dimensional transmutation. For  = 230 MeV and nf = 5,
Fig. 17.1 displays αs(E) in the range 4.19 = mb ≪E ≪mt = 172 GeV
as given by the one-loop formula (17.27) (thin curve) and a three-loop for-
mula (Weinberg, 1996, p. 156) (thick curve).
17.2 The renormalization group in lattice ﬁeld theory
Let us consider a quantum ﬁeld theory on a lattice (Gattringer and Lang,
2010, chap. 3) in which the strength of the nonlinear interactions depends upon
a single dimensionless coupling constant g. The spacing a of the lattice regulates
the inﬁnities, which return as a →0. The value of an observable P computed on
630

17.2 THE RENORMALIZATION GROUP IN LATTICE FIELD THEORY
this lattice will depend upon the lattice spacing a and on the coupling constant
g, and so will be a function P(a, g) of these two parameters. The right value of
the coupling constant is the value that makes the result of the computation be as
close as possible to the physical value P. So the correct coupling constant is not
a constant at all, but rather a function g(a) that varies with the lattice spacing
or cut off a. Thus, as we vary the lattice spacing and go to the continuum limit
a →0, we must adjust the coupling function g(a) so that what we compute,
P(a, g(a)), is equal to the physical value P. That is, g(a) must vary with a so as
to keep P(a, g(a)) = P. But then P(a, g(a)) must remain constant as a varies, so
dP(a, g(a))
da
= 0.
(17.28)
Writing this condition as a dimensionless derivative
a dP(a, g(a))
da
=
da
d ln a
dP(a, g(a))
da
= dP(a, g(a))
d ln a
= 0
(17.29)
we arrive at the Callan–Symanzik equation
0 = dP(a, g(a))
d ln a
=

∂
∂ln a +
dg
d ln a
∂
∂g

P(a, g(a)).
(17.30)
The coefﬁcient of the second partial derivative with a minus sign
βL(g) ≡−dg
d ln a
(17.31)
is the lattice β-function. Since the lattice spacing a and the energy scale μ
are inversely related, the lattice β-function differs from the continuum beta
function by a minus sign.
In SU(N) gauge theory, the ﬁrst two terms of the lattice β-function for small
g are
βL(g) = −β0 g3 −β1 g5
(17.32)
where for nf ﬂavors of light quarks
β0 =
1
(4π)2
11
3 N −2
3nf

β1 =
1
(4π)4

34
3 N2 −10
3 Nnf −N2 −1
N
nf

.
(17.33)
In quantum chromodynamics, N = 3.
Combining the deﬁnition (17.31) of the β-function with its expansion (17.32)
for small g, one arrives at the differential equation
dg
d ln a = β0 g3 + β1 g5,
(17.34)
631

THE RENORMALIZATION GROUP
which one may integrate

d ln a = ln a + c =

dg
β0g3 + β1g5 = −
1
2β0g2 + β1
2β2
0
ln

β0 + β1g2
g2

(17.35)
to ﬁnd
a(g) = d

β0 + β1g2
g2
β1/2β2
0
e−1/2β0g2,
(17.36)
in which d is a constant of integration. The term β1g2 is of higher order in g,
and if one drops it and absorbs a factor of β2
0 into a new constant of integration
, then one gets
a(g) = 1


β0g2−β1/2β2
0 e−1/2β0g2.
(17.37)
As g →0, the lattice spacing a(g) goes to zero very fast (as long as nf < 17 for
N = 3). The inverse of this relation (17.37) is
g(a) ≈

β0 ln(a−2−2) + (β1/β0) ln

ln(a−2−2)
−1/2
.
(17.38)
It shows that the coupling constant slowly goes to zero with a, which is a lattice
version of asymptotic freedom.
17.3 The renormalization group in condensed-matter physics
The study of condensed matter is concerned mainly with properties that emerge
in the bulk, such as the melting point, the boiling point, or the conductivity. So
we want to see what happens to the physics when we increase the distance scale
many orders of magnitude beyond the size a of an individual molecule or the
distance between nearest neighbors.
As a simple example, let’s consider a euclidean action in d dimensions
S =

ddx

1
2(∂φ)2 +

n
gnφn

,
(17.39)
in which g2φ2 ≡m2φ2/2 is a mass term and g4φ4 ≡λφ4/24 is a quartic self-
interaction. In terms of an ultraviolet cut off  = 1/a, we may deﬁne a partition
function
Z() =


e−SDφ
(17.40)
to be one in which the ﬁeld
φ(x) =


eikxφ(k) ddk
(2π)d
(17.41)
632

17.3 THE RENORMALIZATION GROUP IN CONDENSED-MATTER PHYSICS
only has Fourier coefﬁcients φ(k) with k2 < 2. Corresponding to each such
ﬁeld φ(x), we introduce a “stretched” ﬁeld
φL(x) = A(L) φ(x/L)
for
L ≥1,
(17.42)
in which A(L) is a scale factor that we will use to keep the kinetic part of the
action invariant. Since
φL(x) = A(L) φ(x/L) = A(L)


exp

ikx
L

φ(k) ddk
(2π)d
(17.43)
the momenta of the stretched ﬁeld are reduced by the factor 1/L.
We may deﬁne a new partition function in which we integrate over the
stretched ﬁelds φL(x)
Z(/L) =

/L
e−SDφ ≡


e−SDφL.
(17.44)
The kinetic action of a stretched ﬁeld is
Sk =

ddx A2(L)
2
∂φ(x/L)
∂x
2
=

dd(x/L) Ld A2(L)
2
∂φ(x/L)
L∂x/L
2
(17.45)
and so if we choose
A(L) = L−(d−2)/2
(17.46)
then letting x′ = x/L, we ﬁnd that the kinetic action Sk is invariant
Sk =

ddx′ 1
2
∂φ(x′)
∂x′
2
.
(17.47)
The full action of a stretched ﬁeld is
S(φL) =

ddx

1
2(∂φ)2 +

n
gd,n(L)φn

,
(17.48)
in which
gd,n(L) = LdAn(L) gn = Ld−n(d−2)/2 gn.
(17.49)
The beta function
β(gn) ≡
L
gd,n(L)
dgd,n(L)
dL
= d −n(d −2)/2
(17.50)
is just the exponent of the coupling “constant” gd,n(L). If it is positive, then the
coupling constant gd,n(L) gets stronger as L →∞; such couplings are called
relevant. Couplings with vanishing exponents are insensitive to changes in L
and are marginal. Those with negative exponents shrink with increasing L; they
are irrelevant.
633

THE RENORMALIZATION GROUP
The coupling constant gd,n,p of a term with p derivatives and n powers of the
ﬁeld in a space of d dimensions φ varies as
gd,n,p(L) = Ld An(L) L−p gn,p = Ld−n(d−2)/2−p gn,p.
(17.51)
Example 17.3 (QCD)
In quantum chromodynamics, there is a cubic term
g fabc Aa
0Ab
i ∂0Ac
i , which in effect looks like g fabc φaφb ˙φc. Is it relevant? Well, if
we stretch space but not time, then the time derivative has no effect, and d = 3.
So the cubic, n = 3, grows as L3/2
g3,3,0(L) = Ld−n(d−2)/2 gabc = L3/2 g3,0.
(17.52)
Since this cubic term drives asymptotic freedom, its strengthening as space is
stretched by the dimensionless factor L may point to a qualitative explana-
tion of conﬁnement. For if g3,3,0(L) grows with distance as L3/2, then αs(L) =
g2
3,3,0(L)/4π grows as L3, and so the strength αs(Lr)/(Lr)2 of the force between
two quarks separated by a distance Lr grows linearly with the distance
F(Lr) = αs(Lr)
(Lr)2 = L3 αs(r)
(Lr)2
= Lr αs(r)
r3 ,
(17.53)
which is more than enough for quark conﬁnement.
On the other hand, if we stretch both space and time, then the cubic g4,3,1(L)
and quartic g4,4,0(L) couplings are marginal.
Exercises
17.1 Show that for μ2 ≫m2, the vacuum polarization term (17.10) reduces to
(17.12). Hint: use ln a b = ln a + ln b when integrating.
17.2 Show that by choosing the energy scale  according to (17.26), one can derive
(17.27) from (17.25).
17.3 Show that if we stretch both space and time, then in the notation of (17.51),
the cubic g4,3,1(L) and quartic g4,4,0(L) couplings are marginal, that is, are
independent of L.
634

18
Chaos and fractals
18.1 Chaos
Early in the last century, Henri Poincaré studied the three-body problem and
found very complicated orbits. In this and other systems, he found that after a
transient period, classical motion assumes one of four forms:
1 periodic (a limit cycle),
2 steady or damped or stopped,
3 quasi-periodic (more than one frequency),
4 chaotic.
Example 18.1 (Dufﬁng’s equation)
If one attaches a thin piece of iron to the
end of a rod that moves sinusoidally in the x direction at frequency ω near two
magnets then the x coordinate is described by the forced Dufﬁng equation
¨x + a˙x + bx3 + cx = g sin(ωt + φ).
(18.1)
For suitable values of a, b, c, g, ω, and φ, the coordinate x varies chaotically.
Example 18.2 (Dripping faucet)
Drops from a slowly dripping faucet tend to
fall regularly at times tn separated by a constant interval t = tn+1 −tn. At
a slightly higher ﬂow rate, the drops fall separated by intervals that alternate
in their durations t, T, t, T, t, T in a period-two sequence. At some
higher ﬂow rates, no regularity is apparent.
Example 18.3 (Rayleigh-Benard convection)
Consider a ﬂuid in a gravitational
ﬁeld lies above a hot plate and below a cold one. If the difference T is small
enough, then steady convective cellular ﬂow occurs. But if T is above the
chaotic threshold, the ﬂuid boils chaotically.
635

CHAOS AND FRACTALS
x1
x2
x3
x4
x5
x6
>
>
An orbit and its crossings of a line
Figure 18.1
The orbit or trajectory of a dynamical system in N dimensions
generates a map of crossings in N −1 dimensions.
The N equations
˙xi = Fi(x)
(18.2)
represent an N-dimensional dynamical system; they are said to be autonomous
because they involve x(t) but not t itself, that is, ˙xi = Fi(x), not Fi(x, t). The
crossings of a suitably oriented plane (or more generally of a Poincaré surface of
section) lead to an invertible map
xn+1 = M(xn)
(18.3)
in a space of N −1 dimensions, as shown in Fig. 18.1.
A ﬁrst-order, autonomous dynamical system like (18.2) can behave chaotically
only if
N ≥3.
(18.4)
Example 18.4 (Driven, damped pendulum)
The angle θ of a sinusoidally
driven, damped pendulum follows the differential equation
¨θ + f ˙θ + sin θ = T sin(ωt),
(18.5)
which is second order and nonautonomous. Will this system exhibit chaos? We
put it into autonomous form by deﬁning x1 = ˙θ, x2 = θ, and x3 = ωt. In these
variables, the pendulum equation (18.5) is the ﬁrst-order autonomous system
636

18.1 CHAOS
˙x1 = T sin x3 −sin x2 −fx1,
˙x2 = x1,
˙x3 = ω
(18.6)
with N = 3 dependent variables. So chaos is not excluded and is exhibited by
numerical solutions for suitable values of the parameters f , T, and ω.
An invertible map like (18.3)
xn = M−1(xn+1)
(18.7)
can be chaotic only if it has at least two dimensions. This condition is consistent
with condition (18.4) because the Poincaré section of an N-dimensional dynam-
ical system like (18.2) is an (N-1)-dimensional invertible map as in Fig. 18.1.
Example 18.5 (Logistic map)
A map that is not invertible can display chaos
even in one dimension. The one-dimensional logistic map
xn+1 = q xn(xn −1),
(18.8)
which is not invertible (because the quadratic equation for xn in terms of xn+1
has two solutions), displays chaos in increasingly striking forms as q exceeds a
number slightly greater than 3.57. And at q = 3.8, two sequences respectively
starting at x0 = 0.2 and x′
0 = 0.20001 differ by more than 0.2 after only 21
iterations (Fig. 18.2).
0
5
10
15
20
25
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Two diverging sequences of the logistic map
n
xn
x0 = 0.20000
x0 = 0.20001
Figure 18.2
At q = 3.8, the logistic map (18.8) is so sensitive to initial conditions
that after only 21 iterations the sequences starting from x0 = 0.2 and x0 = 0.20001
differ by 0.218.
637

CHAOS AND FRACTALS
By q = 4, the logistic map (18.8) is totally chaotic and is equivalent to the tent
map
xn+1 = 1 −2|xn −1|.
(18.9)
A similar but simpler chaotic map is the 2x modulo 1 map
xn+1 = 2xn mod 1.
(18.10)
Example 18.6 (The Bernoulli shift)
The simplest chaotic map is the Bernoulli
shift, in which the initial point x0 is an arbitrary number between 0 and 1 with
the binary-decimal expansion
x0 =
∞

j=1
2−jaj = 0.a1a2a3a4 . . .
(18.11)
and successive points lack a1, then a2, etc.:
x1 = 0.a2a3a4a5 . . .
x2 = 0.a3a4a5a6 . . .
x3 = 0.a4a5a6a7 . . . .
(18.12)
Two unequal irrational numbers x0 and x′
0, no matter how close, generate
sequences that roam independently, irregularly, and ergotically over the interval
(0, 1).
Example 18.7 (Hénon’s map)
The two-dimensional map
xn+1 = f (xn) + B yn,
yn+1 = xn,
(18.13)
for B ̸= 0 is invertible. If f (xn) = A −x2
n, it is Hénon’s map, which for A = 1.4
and B = 0.3 is chaotic and converges to the attractor in Fig. 18.6.
The Lyapunov exponent of a smooth map xn+1 = f (xn) is the limit
h(x1) = lim
n→∞
1
n

ln |f ′(x1)| + · · · + ln |f ′(xn)|

.
(18.14)
A bounded sequence that has a positive Lyapunov exponent and that does not
converge to a periodic sequence is chaotic (Alligood et al., 1996, p. 110). Other
aspects of chaos lead to other deﬁnitions.
638

18.3 FRACTALS
Attractors
Figure 18.3
On the left, the origin is an attractor; on the right, the circle is a limit
cycle.
18.2 Attractors
If the states of a dynamical system converge, as for instance in Fig. 18.3, to a
point or set of points, that point or set is called an attractor.
Example 18.8 (The van der Pol equation)
If the attractor is a loop that the
states run around, then the attractor is called a limit cycle. The van der Pol
equation
¨y + (y2 −η)˙y + ω2y = 0,
(18.15)
which we may write as the ﬁrst-order system
x1 = ˙y,
˙x1 = −ω2x2 −(x2
2 −η)x1,
x2 = y,
˙x2 = x1,
(18.16)
has a limit cycle. Physicists used the van der Pol equation in the 1920s to study
vacuum-tube oscillators.
18.3 Fractals
A fractal set has a dimension that is not an integer. How can that be?
Felix Hausdorff and Abram Besicovitch showed how to deﬁne the dimension
of a weird set of points. To compute the fractal or box-counting dimension of
a set, we cover it with line segments, squares, cubes, or n-dimensional “boxes”
639

CHAOS AND FRACTALS
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
Toward the Cantor set
Figure 18.4
The ﬁrst four approximations to the Cantor set.
of side ϵ. If we need N(ϵ) boxes, then the fractal dimension Db is the limit as
ϵ →0
Db = lim
ϵ→0
ln(N(ϵ))
ln(1/ϵ) .
(18.17)
For instance, we can cover the interval [a, b] with N(ϵ) = (b−a)/ϵ line segments
of length ϵ, so the dimension of the segment [a, b] is
Db = lim
ϵ→0
ln(N(ϵ))
ln(1/ϵ) = lim
ϵ→0
ln((b −a)/ϵ)
ln(1/ϵ)
= 1 + ln(b −a)
ln(1/ϵ) = 1
(18.18)
as it should be.
Example 18.9 (Cantor set)
The Cantor set is deﬁned by a limiting process in
which the set at the nth stage consists of 2n line segments each of length 1/3n.
The ﬁrst four approximations to the Cantor set are sketched in Fig. 18.4. We
can cover the nth approximation with N(ϵ) = 2n line segments each of length
ϵn = 1/3n, and so the fractal dimension is
Db,C = lim
ϵ→0
ln(N(ϵ))
ln(1/ϵ) = lim
n→∞
ln(N(ϵn))
ln(1/ϵn) = lim
n→∞
ln(2n)
ln(3n) = ln 2
ln 3 = 0.6309297 . . . ,
(18.19)
which is not an integer or even a rational number.
640

18.3 FRACTALS
Example 18.10 (Koch snowﬂake)
In 1904, the Swedish mathematician Helge
von Koch described the Koch curve (or the Koch snowﬂake), whose construc-
tion is shown in Fig. 18.5. With each step, there are four times as many line
segments, each one being three times smaller. The length L of the curve at step n
is thus
L =
4
3
n
,
(18.20)
which grows without limit as n →∞. Its box dimension is
Db,K = lim
n→∞
ln(N(ϵn))
ln(1/ϵn) = lim
n→∞
ln(4n)
ln(3n) = ln 4
ln 3 = 1.2618595 . . .
(18.21)
The Koch snowflake
Figure 18.5
Curve of von Koch: steps 0, 1, 2, and 3 of construction (http://en-
wikipedia.org/wiki/File:kochFlake.org).
Closely related to the box-counting dimension is the self-similar dimension
Ds. To deﬁne it, we consider the number of self-similar structures of linear size
x needed to cover the ﬁgure after n steps and take the limit
Ds = lim
x→0
ln N(x)
ln 1/x .
(18.22)
In the case of von Koch’s curve, x = 1/3n and N(x) = 4n. So the self-similar
dimension of von Koch’s curve is
Ds,K = lim
x→0
ln N(x)
ln 1/x = lim
n→∞
ln 4n
ln 3n = n ln 4
n ln 3 = ln 4
ln 3 = 1.2618595 . . .
(18.23)
which is equal to its box dimension Db,K given by (18.21).
641

CHAOS AND FRACTALS
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Hénon’s strange attractor
x
y
Figure 18.6
Points 20–10,020 of Hénon’s map (18.7) with A = 1.4 and B = 0.3 and
(x1, y1) = (0, 0).
Attractors of fractal dimension are strange. Hénon’s map (18.7) with A =
1.4 and B
=
0.3 is chaotic with a strange attractor, as illustrated by
Fig. 18.6.Chaotic systems often have strange attractors; but chaotic systems can
have nonfractal attractors, and nonchaotic systems can have strange attractors.
Further reading
The book Chaos: an Introduction to Dynamical Systems (Alligood et al., 1996)
is superb.
Exercises
18.1 A period-one sequence of a map xn+1 = f (xn) is a point p for which p = f (p).
Find the period-one sequences of xn+1 = rxn(1 −xn/K).
18.2 A period-two sequence of a map xn+1 = f (xn) is two different points p and
q for which q = f (p) and p = f (q). Estimate the period-two sequences of the
logistic map f (x) = ax(1 −x) for a = 1, 2, and 3. Hint: graph the functions
f (f (x)) and I(x) = x on the interval [0,1].
18.3 A period-three sequence of a map xn+1 = f (xn) is three different points p, q,
and r for which q = f (p), r = f (q), and p = f (r). Li and Yorke have shown
that a map with a period-three sequence is chaotic. Estimate the period-three
sequences of the map f (x) = 4x(1 −x).
642

19
Strings
19.1 The inﬁnities of quantum ﬁeld theory
Quantum ﬁeld theory is plagued with inﬁnities. Even exactly soluble theories of
free ﬁelds have ground-state energies that diverge quarticly, as +4 for a theory
of bosons and as −4 for a theory of fermions as  →∞. Theories with the
same number of boson and fermion ﬁelds are less divergent, and theories with
unbroken supersymmetry actually have ground-state energies that vanish.
One may be able to eliminate some of the inﬁnities of a generic quantum ﬁeld
theory without spoiling the symmetries of its action density L by replacing L
in the path integral by L′ = L/(1 −L/M4) or L′ = M4 
exp(L/M4) −1

. In
euclidean space, the substitution
L′
e =
Le
1 −Le/M4
(19.1)
with the understanding that L′
e = ∞when |Le| > M4 almost certainly removes
many inﬁnities, but whether it preserves all the symmetries is less obvious.
These substitutions change quantum ﬁeld theory, but in the limit M →∞,
one recovers standard quantum ﬁeld theory.
A more physical approach is to represent elementary particles as objects that
have ﬁnite size. Those that are one-dimensional are called strings.
19.2 The Nambu–Goto string action
If we give up the idea of point particle then the next simplest choice is a one-
dimensional string. We’ll use 0 ≤σ ≤σ1 and τi ≤τ ≤τf to parametrize
the space-time coordinates Xμ(σ, τ) of the string. Nambu and Goto suggested
using as the action the area
643

STRINGS
S = −T0
c
 τf
τi
 σ1
0
 ˙X · X′2 −
 ˙X
2 (X′)2 dτ dσ,
(19.2)
in which
˙Xμ = ∂Xμ
∂τ
and
X′μ = ∂Xμ
∂σ
(19.3)
and a Lorentz metric ημν = diag(−1, 1, 1, . . . ) is used to form the inner
products like
˙X · X′ = ˙XμημνXν′
etc.
(19.4)
This action is the area swept out by a string of length σ1 in time τf −τi.
If ˙Xdτ = dt points in the time direction and X′dσ = dr points in a spatial
direction, then it is easy to see that ˙X · X′ = 0, that −
 ˙X
2 dτ 2 = dt2, and that

X′2 dσ 2 = dr2. So in this simple case, the action (19.2)
S = −T0
c
 tf
ti
 r1
0
dt dr = −T0
c (tf −ti)r1
(19.5)
is the area the string sweeps out. The other term within the square-root ensures
that the action is the area swept out for all ˙X and X′, and that it is invariant
under arbitrary reparametrizations σ →σ ′ and τ →τ ′.
The equation of motion for the relativistic string follows from the require-
ment that the action (19.2) be stationary, δS = 0. Since
δ ˙Xμ = δ ∂Xμ
∂τ
= ∂( ˙Xμ + δXμ)
∂τ
−∂Xμ
∂τ
= ∂δXμ
∂τ
(19.6)
and similarly
δX′μ = ∂δXμ
∂σ
(19.7)
we may express the change in the action in terms of derivatives of the Lagrange
density
L = −T0
c
 ˙X · X′2 −
 ˙X
2 (X′)2
(19.8)
as
δS =
 τf
τi
 σ1
0
# ∂L
∂˙Xμ
∂δXμ
∂τ
+
∂L
∂X′μ
∂δXμ
∂σ
$
dτ dσ.
(19.9)
Its derivatives, which we’ll call Pτ
μ and Pσ
μ, are
Pτ
μ = ∂L
∂˙Xμ = −T0
c
( ˙X · X′)X′
μ −(X′)2 ˙Xμ
 ˙X · X′2 −
 ˙X
2 (X′)2
(19.10)
644

19.2 THE NAMBU–GOTO STRING ACTION
and
Pσ
μ =
∂L
∂X′μ = −T0
c
( ˙X · X′) ˙Xμ −(X′)2X′
μ
 ˙X · X′2 −
 ˙X
2 (X′)2
.
(19.11)
In terms of them, the change in the action is
δS =
 τf
τi
 σ1
0
# ∂
∂τ

δXμPτ
μ

+ ∂
∂σ

δXμPσ
μ

−δXμ
∂Pτ
μ
∂τ
+
∂Pσ
μ
∂σ
$
dτ dσ.
(19.12)
The total τ-derivative integrates to a term involving the variation δXμ, which
we require to vanish at the initial and ﬁnal values of τ. So we drop that term
and ﬁnd that the net change in the action is
δS =
 τf
τi

δXμPσ
μ
σ1
0 dτ −
 τf
τi
 σ1
0
δXμ
∂Pτ
μ
∂τ
+
∂Pσ
μ
∂σ

dτ dσ.
(19.13)
Thus the equations of motion for the string are
∂Pτ
μ
∂τ
+
∂Pσ
μ
∂σ
= 0
(19.14)
but the action is stationary only if the boundary condition
δXμ(τ, σ1)Pσ
μ(τ, σ1) −δXμ(τ, 0)Pσ
μ(τ, 0) = 0
(19.15)
is satisﬁed for all τ. This is a condition on the ends of open strings; closed
strings satisfy it automatically.
Usually the boundary condition (19.15) is interpreted as 2D = 2(d + 1)
conditions – one for each end σ∗of the string and each dimension μ of
space-time:
δXμ(τ, σ∗)Pσ
μ(τ, σ∗) = 0,
no sum over μ.
(19.16)
A Dirichlet boundary condition ﬁxes a spatial component at an end of the
string by
˙Xi(τ, σ∗) = 0
(19.17)
or equivalently by δXμ(τ, σ∗) = 0. The time component X0 can not have a
vanishing τ derivative, so it must obey a free-endpoint-boundary condition
Pσ
μ(τ, σ∗) = 0,
(19.18)
which also may apply to any dimension and any end.
645

STRINGS
19.3 Regge trajectories
The quantity Pτ
μ(τ, σ) deﬁned as the derivative (19.10) turns out to be the
momentum density of the string. The angular momentum M12 of a string
rigidly rotating in the x, y plane is
M12(τ) =
 σ1
0
X1Pτ
2 (τ, σ) −X2Pτ
1 (τ, σ) dσ.
(19.19)
In a parametrization of the string with τ = t and dσ proportional to the energy
density dE of the string, the x, y coordinates of the string are
⃗X(t, σ) = σ1
π cos πσ
σ1

cos πct
σ1
, sin πct
σ1

.
(19.20)
The x, y components of the momentum density are
⃗Pτ(t, σ) = T0
c
∂⃗X
∂t = T0
c cos πσ
σ1

−sin πct
σ1
, cos πct
σ1

.
(19.21)
The angular momentum (19.19) is then given by the integral
M12 = σ1
π
T0
c
 σ1
0
cos2 πσ
σ1
dσ = σ 2
1 T0
2πc .
(19.22)
Now the parametrization dσ ∝dE implies that σ1 ∝E, and in fact the energy
of the string is E = T0σ1. Thus the angular momentum J = |M12| of a classical
relativistic string is proportional to the square of its total energy
J =
E2
2πT0c.
(19.23)
This rule is obeyed by many meson and baryon resonances. The nucleon and
ﬁve baryon resonances ﬁt it with nearly the same value of the string tension
T0 ≈0.92 GeV/fm
(19.24)
as shown by Fig. 19.1, which displays the Regge trajectories of the N and 
resonances on a single curve. Other N and  resonances, however, do not fall
on this curve.
A string theory of hadrons took off in 1968 when Gabriel Veneziano pub-
lished his amplitude for π + π scattering as a sum of three Euler beta
functions (Veneziano, 1968). But after eight years of intense work, this effort
was largely abandoned with the discovery of quarks at SLAC and the promise
of QCD as a theory of the strong interactions. In 1974, Joël Scherk and John
H. Schwarz proposed increasing the string tension by 38 orders of magnitude
so as to use strings to make a quantum theory that included gravity (Scherk
646

19.5 D-BRANES
0.8
1
1.2
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
0
1
2
3
4
5
6
Regge trajectory of the nucleon
Energy E = mc2 (GeV) of baryon resonance
Spin J/h¯
N(938)
Δ(1232)
N(1680)
Δ(1950)
N(2220)
Δ(2420)
Figure 19.1
The angular momentum and energy of the nucleon and ﬁve baryon res-
onances approximately ﬁt the curve J/¯h = T0 E2 with string tension T0 = 0.92
GeV/fm.
and Schwarz, 1974). They identiﬁed the graviton as an excitation of the closed
string.
19.4 Quantized strings
The coordinates Xμ may be quantized most easily in light-cone coordinates.
The resulting relativistic bosonic string must live in a space-time of exactly D =
d + 1 = 26 dimensions with a tachyon. But if one adds fermionic variables
ψμ
1 (τ, σ) and ψμ
2 (τ, σ) in a supersymmetric way, then the tachyon goes away,
and the number of space-time dimensions drops to exactly 10. There are ﬁve
distinct superstring theories – types I, IIA, and IIB; E8 ⊗E8 heterotic; and
SO(32) heterotic. All ﬁve may be related to a single theory in 11 dimensions
called M-theory, which is not a string theory. M-theory contains membranes
(2-branes) and 5-branes, which are not D-branes.
19.5 D-branes
One may satisfy Dirichlet boundary conditions (19.17) by requiring the ends of
a string to be attached to a spatial manifold, called a D-brane after Dirichlet.
If the manifold to which the string is stuck has p dimensions, then it’s called a
647

STRINGS
Two strings stuck on a D2-brane
Figure 19.2
Two strings stuck on a D2-brane.
Dp-brane. Figure (19.2) shows a string whose ends are free to move only within
a D2-brane.
Dp-branes offer a natural way to explain the extra six dimensions required
in a universe of superstrings. One imagines that the ends of all the strings are
free to move only in our four-dimensional space-time; the strings are stuck on
a D3-brane, which is the three-dimensional space of our physical Universe. The
tension of the superstring then keeps it from wandering far enough into the
extra six spatial dimensions for us ever to have noticed.
This explanation conﬂicts, however, with one interpretation of gravity in
string theory. In that view, the graviton and the spin-3/2 gravitino are modes of
the closed string, are not attached to any D-brane, and propagate in all d space
dimensions. Thus if the extra space dimensions were huge, then the gravitational
force, which is mainly due to the graviton, would fall off with the distance r as
1/rd−1 = 1/r8 for superstrings (1/r24 for bosonic strings), both much faster
than 1/r2.
19.6 String–string scattering
Strings interact by joining and by breaking. Figure 19.3 shows two open strings
joining to form one open string and then breaking into two open strings. Fig-
ure 19.4 shows two closed strings joining to form one closed string and then
breaking into two closed strings. The interactions of strings do not occur at
points.
Because the fundamental things of string theory are extended objects, string
theory is intrinsically free of ultraviolet divergences. It provides a ﬁnite theory
of quantum gravity.
648

19.7 RIEMANN SURFACES AND MODULI
Two-to-two scattering of open strings
Space →
Time →
Figure 19.3
A space-time diagram of the scattering of two open strings into two open
strings.
Two-to-two scattering of closed strings
Space →
Time →
Figure 19.4
A space-time diagram of the scattering of two closed strings into two
closed strings.
19.7 Riemann surfaces and moduli
A homeomorphism is a map that is one to one and continuous with a continu-
ous inverse. A Riemann surface is a two-dimensional real manifold whose open
sets Uα are mapped onto open sets of the complex plane C by homeomorphisms
649

STRINGS
zα whose transition functions zα ◦z1
β are analytic on the images of the inter-
sections Uα ∩Uβ. Two Riemann surfaces are equivalent if they are related by a
continuous analytic map that is one to one and onto.
A parameter that distinguishes a Riemann surface from other, inequiv-
alent Riemann surfaces is called a modulus. Some Riemann surfaces have
several moduli; others have one modulus; others none at all. Some moduli are
continuous parameters; others are discrete.
Further reading
Students who want to learn more about strings should look at the excellent
textbook A First Course in String Theory (Zwiebach, 2009).
Exercises
19.1 Derive formulas (19.10) and (19.11).
19.2 Derive equation (19.12) from (19.9, 19.10, & 19.11).
650

References
Aitken, A. C. 1959. Determinants and Matrices. Oliver and Boyd.
Alberts, Bruce, Johnson, Alexander, Lewis, Julian, Raff, Martin, Roberts, Keith,
and Walter, Peter. 2008. Molecular Biology of the Cell. 5th edn. Garland Science.
Page 246.
Alligood, Kathleen T., Sauer, Tim D., and Yorke, James A. 1996. Chaos: an
Introduction to Dynamical Systems. Springer-Verlag.
Arnold, V. I. 1989. Mathematical Methods of Classical Mechanics. 2nd edn.
Springer. Chapter 7.
Autonne, L. 1915. Sur les Matrices Hypohermitiennes et sur les Matrices Unitaires.
Ann. Univ. Lyon, Nouvelle Série I, Fasc. 38, 1–77.
Bigelow, Matthew S., Lepeshkin, Nick N., and Boyd, Robert W. 2003. Superlumi-
nal and slow light propagation in a room-temperature solid. Science, 301(5630),
200–202.
Bordag, Michael, Klimchitskaya, Galina Leonidovna, Mohideen, Umar, and
Mostepanenko, Vladimir Mikhaylovich. 2009. Advances in the Casimir Effect.
Oxford University Press.
Bouchaud, Jean-Philippe, and Potters, Marc. 2003. Theory of Financial Risk and
Derivative Pricing. 2nd edn. Cambridge University Press.
Boyd, Robert W. 2000. Nonlinear Optics. 2nd edn. Academic Press.
Brillouin, L. 1960. Wave Propagation and Group Velocity. Academic Press.
Brunner, N., Scarani, V., Wegmüller, M., Legré, M., and Gisin, N. 2004. Direct
measurement of superluminal group velocity and signal velocity in an optical
ﬁber. Phys. Rev. Lett., 93(20), 203902.
Cantelli, F. P. 1933. Sulla determinazione empirica di una legge di distribuzione.
Giornale dell’Instituto Italiano degli Attuari, 4, 221–424.
Carroll, Sean. 2003. Spacetime and Geometry: an Introduction to General Relativity.
Benjamin Cummings.
Cohen-Tannoudji, Claude, Diu, Bernard, and Laloë, Frank. 1977. Quantum
Mechanics. Hermann & John Wiley.
651

REFERENCES
Courant, Richard. 1937. Differential and Integral Calculus, Vol. I. Interscience.
Courant, Richard and Hilbert, David. 1955. Methods of Mathematical Physics,
Vol. I. Interscience.
Creutz, Michael. 1983. Quarks, Gluons, and Lattices. Cambridge University Press.
Darden, Tom, York, Darrin, and Pedersen, Lee. 1993. Particle mesh Ewald: an
Nlog(N) method for Ewald sums in large systems. J. Chem. Phys., 98(12), 10089.
DeWitt, Bryce S. 1967. Quantum theory of gravity. II. The manifestly covariant
theory. Phys. Rev., 162(5), 1195–1239.
Dirac, P. A. M. 1967. The Principles of Quantum Mechanics. 4th edn. Oxford
University Press.
Dirac, P. A. M. 1996. General Theory of Relativity. Princeton University Press.
Faddeev, L. D. and Popov, V. N. 1967. Feynman diagrams for the Yang–Mills ﬁeld.
Phys. Lett. B, 25(1), 29–30.
Feller, William. 1966. An Introduction to Probability Theory and Its Applications.
Vol. II. Wiley.
Feller, William. 1968. An Introduction to Probability Theory and Its Applications.
3rd edn. Vol. I. Wiley.
Feynman, Richard P. and Hibbs, A. R. 1965. Quantum Mechanics and Path
Integrals. McGraw-Hill.
Frieman, Joshua A., Turner, Michael S., and Huterer, Dragan. 2008. Dark energy
and the accelerating Universe. Ann. Rev. Astron. Astrophys., 46, 385–432.
arXiv:0803.0982v1 [astro-ph].
Gattringer, Christof and Lang, Christian B. 2010. Quantum Chromodynamics on the
Lattice: an Introductory Presentation. Springer (Lecture Notes in Physics).
Gehring, G. M., Schweinsberg, A., Barsi, C., Kostinski, N., and Boyd, R. W. 2006.
Observation of backwards pulse propagation through a medium with a negative
group velocity. Science, 312(5775), 895–897.
Gelfand, Israel M. 1961. Lectures on Linear Algebra. Interscience.
Gell-Mann, Murray. 1994. The Quark and the Jaguar. W. H. Freeman.
Gell-Mann, Murray. 2008. Plectics. Lectures at the University of New Mexico.
Georgi, H. 1999. Lie Algebras in Particle Physics. 2nd edn. Perseus Books.
Glauber, Roy J. 1963a. Coherent and incoherent states of the radiation ﬁeld. Phys.
Rev., 131(6), 2766–2788.
Glauber, Roy J. 1963b. The quantum theory of optical coherence. Phys. Rev., 130(6),
2529–2539.
Glivenko, V. 1933. Sulla determinazione empirica di una legge di distribuzione.
Giornale dell’Instituto Italiano degli Attuari, 4, 92–99.
Gnedenko, B. V. 1968. The Theory of Probability. Chelsea Publishing Co.
Gutzwiller, Martin C. 1990. Chaos in Classical and Quantum Mechanics. Springer.
Hau, L.V., Harris, S. E., Dutton, Z., and Behroozi, C. H. 1999. Light speed
reduction to 17 metres per second in an ultracold atomic gas. Nature, 397, 594.
Hobson, M. P., Efstathiou, G. P., and Lasenby, A. N. 2006. General Relativity: an
Introduction for Physicists. Cambridge University Press.
Holland, John H. 1975. Adaptation in Natural and Artiﬁcial Systems. University of
Michigan Press.
652

REFERENCES
Ince, E. L. 1956. Integration of Ordinary Differential Equations. 7th edn. Oliver and
Boyd, Ltd. Chapter 1.
James, F. 1994. RANLUX: a Fortran implementation of the high-quality pseudo-
random number generator of Lüscher. Comp. Phys. Comm., 79, 110.
Kleinert, Hagen. 2009. Path Integrals in Quantum Mechanics, Statistics, Polymer
Physics, and Financial Markets. World Scientiﬁc.
Knuth, Donald E. 1981. The Art of Computer Programming, Volume 2: Seminumer-
ical Algorithms. 2nd edn. Addison-Wesley.
Kolmogorov, Andrei Nikolaevich. 1933. Sulla determinazione empirica di una legge
di distribuzione. Giornale dell’Instituto Italiano degli Attuari, 4, 83–91.
Langevin, Paul. 1908. Sur la théorie du mouvement brownien. Comptes Rend. Acad.
Sci. Paris, 146, 530–533.
Larson, D., Dunkley, J., Hinshaw, G., Komatsu, E., Nolta, M.R., et al. 2011. Seven-
year Wilkinson microwave anisotropy probe (WMAP) observations: power
spectra and WMAP-derived parameters. Astrophys. J. Suppl., 192, 16.
Lifshitz, E. M. 1956. The theory of molecular attractive forces between solids. Sov.
Phys. JETP, 2, 73.
Lin, I-Hsiung. 2011. Classic Complex Analysis. World Scientiﬁc.
Lüscher, M. 1994. A portable high-quality random number generator for lattice
ﬁeld theory simulations. Comp. Phys. Comm., 79, 100.
Matzner, Richard A. and Shepley, Lawrence C. 1991. Classical Mechanics. Prentice
Hall.
McCauley, Joseph L. 1994. Chaos, Dynamics, and Fractals. Cambridge University
Press.
Metropolis, Nicholas, Rosenbluth, Arianna W., Rosenbluth, Marshall N., Teller,
Augusta H., and Teller, Edward. 1953. Equation of state calculations by fast
computing machines. J. Chem. Phys., 21(6), 1087–1092.
Milonni, Peter W. and Shih, M.-L. 1992. Source theory of the Casimir force. Phys.
Rev. A, 45(7), 4241–4253.
Misner, Charles W., Thorne, Kip S., and Wheeler, John Archibald. 1973. Gravita-
tion. W. H. Freeman.
Morse, Philip M. and Feshbach, Herman. 1953. Methods of Theoretical Physics.
Vol. I. McGraw-Hill.
Parsegian, Adrian. 1969. Energy of an ion crossing a low dielectric membrane:
solutions to four relevant electrostatic problems. Nature, 221, 844–846.
Pathria, R. K. 1972. Statistical Mechanics. Pergamon Press. Chapter 13.
Pearson, Karl. 1900. On the criterion that a given system of deviations from the
probable in the case of correlated system of variables is such that it can be
reasonably supposed to have arisen from random sampling. Phil. Mag., 50(5),
157–175.
Riley, Ken, Hobson, Mike, and Bence, Stephen. 2006. Mathematical Methods for
Physics and Engineering. 3rd edn. Cambridge University Press.
Roe, Byron P. 2001. Probability and Statistics in Experimental Physics. Springer.
Saito,
Mutsuo,
and
Matsumoto,
Makoto.
2007.
www.math.sci.hiroshima-
u.ac.jp/m-mat/MT/emt.html.
653

REFERENCES
Sakurai, J. J. 1982. Advanced Quantum Mechanics. 1st edn. Addison Wesley. Pages
62–63.
Scherk, Joël, and Schwarz, John H. 1974. Dual models for non-hadrons. Nucl.
Phys., B81, 118.
Schmitt, Lothar M. 2001. Theory of genetic algorithms. Theoretical Computer
Science, 259, 1–61.
Schutz, Bernard. 1980. Geometrical Methods of Mathematical Physics. Cambridge
University Press.
Schwinger, Julian, Deraad, Lester, Milton, Kimball A., and Tsai, Wu-yang. 1998.
Classical Electrodynamics. Westview Press.
Smirnov, N. V. 1939. Estimation of the deviation between empirical distribution
curves for two independent random samples. Bull. Moscow State Univ., 2(2),
3–14.
Srednicki, Mark. 2007. Quantum Field Theory. Cambridge University Press.
Stakgold, Ivar. 1967. Boundary Value Problems of Mathematical Physics, Vol. I.
Macmillan.
Steinberg, A. M., Kwiat, P. G., and Chiao, R. Y. 1993. Measurement of the single-
photon tunneling time. Phys. Rev. Lett., 71(5), 708–711.
Stenner, Michael D., Gauthier, Daniel J., and Neifeld, Mark A. 2003. The speed of
information in a ‘fast-light’ optical medium. Nature, 425, 695–698.
Titulaer, U. M., and Glauber, R. J. 1965. Correlation functions for coherent ﬁelds.
Phys. Rev., 140(3B), B676–682.
Veneziano, Gabriel. 1968. Construction of a crossing-symmetric Regge-behaved
amplitude for linearly rising regge trajectories. Nuovo Cim., 57A, 190.
von Foerster, Heinz, Mora, Patricia M., and Amiot, Lawrence W. 1960. Doomsday:
Friday, 13 November, A.D. 2026. Science, 132, 1291–1295.
Vose, Michael D. 1999. The Simple Genetic Algorithm: Foundations and Theory.
MIT Press.
Wang, Yun-ping and Zhang, Dian-lin. 1995. Reshaping, path uncertainty, and
superluminal traveling. Phys. Rev. A, 52(4), 2597–2600.
Watson, George Neville. 1995. A Treatise on the Theory of Bessel Functions.
Cambridge University Press.
Waxman, David and Peck, Joel R. 1998. Pleiotropy and the preservation of
perfection. Science, 279.
Weinberg, Steven. 1972. Gravitation and Cosmology. John Wiley & Sons.
Weinberg, Steven. 1988. The First Three Minutes. Basic Books.
Weinberg, Steven. 1995. The Quantum Theory of Fields. Vol. I: Foundations.
Cambridge University Press.
Weinberg, Steven. 1996. The Quantum Theory of Fields. Vol. II: Modern applica-
tions. Cambridge University Press.
Weinberg, Steven. 2005. The Quantum Theory of Fields. Vol. III: Supersymmetry.
Cambridge University Press.
Weinberg, Steven. 2010. Cosmology. Oxford University Press.
Whittaker, E. T. and Watson, G. N. 1927. A Course of Modern Analysis. 4th edn.
Cambridge University Press.
654

REFERENCES
Wright, Ned. 2006. A cosmology calculator for the World Wide Web. Publ. Astron.
Soc. Paciﬁc, 118(850), 1711–1715. www.astro.ucla.edu/wright/CosmoCalc.html.
Zee, Anthony. 2010. Quantum Field Theory in a Nutshell. 2nd edn. Princeton
University Press.
Zwiebach, Barton. 2009. A First Course in String Theory. 2nd edn. Cambridge
University Press.
655

Index
adjoint domain, 262
analytic continuation, 179–180
dimensional regularization, 180
analytic functions, 160–222
branch of, 194
deﬁnition of, 160
entire, 161, 177
essential singularity, 177
harmonic functions, 170–171
holomorphic, 177
isolated singularity, 177
meromorphic, 177
multivalued, 194
pole, 177
simple pole, 177
angular momentum
lowering operators, 371
raising operators, 371
spin, 371
annihilation and creation operators, 132, 233
arrays, 2–3
associated Legendre functions, 317–323
Rodrigues’s formula for, 318
associated Legendre polynomials, 317–323
Rodrigues’s formula for, 318
asymptotic freedom, 237, 629–634
average value, 117
basis, 7, 16
beats, 77
Bessel functions, 325–347
and charge near a membrane, 331–333
and coaxial wave-guides, 342–343
and cylindrical wave-guides, 333–335
and scattering off a hard sphere, 344–345
exercises, 345–347
Hankel functions, 341–343
modiﬁed, 330–331, 341–343
Neumann functions, 341–343
of the ﬁrst kind, 143, 144, 325–341
of the second kind, 341–345
spherical, 145
and partial waves, 338–340
and quantum dots, 340–341
Rayleigh’s formula for, 336
spherical Bessel functions of the second kind,
343–345
Bessel inequality, 284
Bessel’s equation, 327
Bianchi identity, 413
binomial coefﬁcient, 141
Bloch’s theorem, 105
bodies falling in air, 247
Boltzmann distribution, 54–55
Boltzmann’s constant, 149
boundary conditions
Dirichlet, 262
natural, 260, 268
Neumann, 260
Bravais lattice, 104
Bromwich integral, 129
calculus of variations, 443–447
in nonrelativistic mechanics, 443–444
in relativistic electrodynamics, 445
in relativistic mechanics, 444–445
particle in a gravitational ﬁeld, 445–447
strings, 643–645
Callan–Symanzik equation, 630–634
canonical commutation relations, 132, 233
Cartan subalgebras, 379
Casimir effect, 214–217
Cauchy’s principal value, 199
Feynman’s propagator, 201–205
656

INDEX
trick, 200
chaos, 635–642
attractors, 639–642
limit cycle, 639
of fractal dimension, 642
strange, 642
Bernoulli shift, 638
chaotic threshold, 635
Dufﬁng’s equation, 635
dynamical system, 636
autonomous, 636
fractals, 639–642
Cantor set, 640
fractal dimension, 640
Koch snowﬂake, 640
self-similar dimension, 641
Hénon’s map, 638
invertible map, 636, 637
map, 636
period-two sequence, 635
Poincaré surface of section, 636
Rayleigh–Benard convection, 635
van der Pol’s equation, 639
characteristic function, 119
and moments, 119
class Ck of functions, 85
Clifford algebra, 393
commutators, 353
compact, 351
complex arithmetic, 2
complex-variable theory, 160–222
Abel–Plana formula, 212–217
analytic continuation, 179–180
analyticity, 160–161
and string theory, 217–219
applications to string theory
radial order, 217
argument principle, 178–179
calculus of residues, 180–182
Cauchy’s inequality, 173
Cauchy’s integral formula, 165–169
Cauchy’s integral theorem, 161–165
and Stoke’s theorem, 169
Cauchy’s principal value, 198–205
Cauchy–Riemann conditions, 169–170
conformal mapping, 197–198
contour integral with cut, 196
cuts, 193–197
dispersion relations, 205–208
essential singularity, 177
and Picard’s theorem, 177
exercises, 219–222
fundamental theorem of algebra, 174
ghost contours, 182–191
harmonic functions, 170–171
isolated singularity, 177
Laurent series, 174–179
Liouville’s theorem, 173–174
logarithms, 193–197
method of steepest descent, 210–212
phase and group velocities, 208–210
pole, 177
residue, 176
roots, 194
simple pole, 177
singularities, 177–179
Taylor series, 171–173
conformal mapping, 197–198
contractions, 416
contravariant vector ﬁeld, 401
contravariant vectors, 401
convergence
of functions, 85
uniform, 85
uniform
and term-by-term integration, 85
convergence in the mean, 138–139
convex function, 560
convolutions, 121–124, 134
and Gauss’s law, 121–123
and Green’s functions, 121–123
and translational invariance, 123
coordinates, 400–401
correlation functions
Glauber and Titulaer, 69–71
cosmology, 289–291, 457–469
, 462
time evolution of, 461–463
comoving coordinates, 458
cosmic microwave background radiation
(CMBR), 458, 469
critical energy density, 458, 462
dark matter, 457
era of dark energy, 458
era of matter, 458
era of radiation, 457
ﬁrst three minutes, 457
homogeneous and isotropic
line element, 458
Hubble constant, 458
Hubble rate, 461
inﬂation, 457
models, 463–469
dark-energy dominated, 468
equation of state, 464
inﬂation dominated, 465
matter dominated, 466–468
radiation dominated, 465–466
transparency, 468
without acceleration, 464–465
recombination, 468
redshift, 463
Robertson–Walker metric, 459–463
energy–momentum tensor of, 461
Friedmann equations, 461
transparency, 458
covariant derivatives
in Yang–Mills theory, 365
covariant vector ﬁeld, 402
covariant vectors, 402
657

INDEX
decuplet of baryon resonances, 646–647
degenerate eigenvalue, 40
delta function, 97–101, 112–115, 120, 130, 131,
281–283, 287–288
and Green’s functions, 122
Dirac comb, 98–101, 114–115
eigenfunction expansion of, 281–283, 287–288
for continuous square-integrable functions,
112–115
for real periodic functions, 101, 103
for twice differentiable functions on [−1, 1], 310
of a function, 113
density operators, 54–55
determinants, 27–33
and antisymmetry, 28, 29
and Levi–Civita symbol, 28
and linear dependence, 29
and linear independence, 29
and permutations, 30
and the inverse of a matrix, 31
cofactors, 28
invariances of, 28
Laplace expansion, 28
minors, 28
product rule for, 32
3 × 3, 27
2 × 2, 27
dielectrics, 154
differential equations, 223–295
exercises, 293–295
terminal velocity
of mice, men, falcons, and bullets, 247
diffusion, 133–134
Fick’s law, 133
dimensional regularization, 180
Dirac mass term, 395
Dirac notation, 19–27, 96–101
and change of basis, 22
and inner-product rules, 20
and self-adjoint linear operators, 23
and the adjoint of an operator, 22–23
bra, 19
bracket, 19
examples, 22
ket, 19
outer products, 21
tricks, 51
Dirac’s delta function, 285
Dirac’s gamma matrices, 393
direct product, 377
adding spins, 68
and hydrogen atom, 68
dispersion relations, 205–208
and causality, 205
Kramers and Kronig, 206–208
divergence, 228–230
division algebra, 380, 384
double-factorials, 143
eigenfunctions, 267–283
complete, 274, 277–283
orthonormal
Gram–Schmidt procedure, 274
eigenvalues, 267–283
algebraic multiplicity, 40
degenerate, 39
geometric multiplicity, 40
nondegenerate, 39
simple, 40
unbounded, 275–283
eigenvectors, 36–55
Einstein’s summation convention, 404–405
electric constant, 154, 413
electric displacement, 413
electric susceptibility, 154
electrodynamics, 411–414
electrostatic energy, 154
electrostatic potential
multipole expansion, 286
electrostatics
dielectrics, 153–157
emission rate from a ﬂuorophore, 247
energy–momentum 4-vector, 410
entropy, 54–55
euclidean coordinates, 402–404
euclidean space, 402–404
Ewald summation, 115
expected value, 117
exterior derivative, 417–419
factorials, 141–145
double, 143, 145
Mermin’s approximation, 141
Mermin’s inﬁnite-product formula, 141
Ramanujan’s approximation, 141
Stirling’s approximation, 141
Faraday’s law, 154, 412
Feynman’s propagator, 120–121, 201–205
as a Green’s function, 201
ﬁeld of charge near a membrane, 155–157
ﬁeld of charge near dielectric interface, 154–157
forms, 416–419, 427–431, 479–501
closed, 496–498
differential forms, 416–419, 481–501
p-forms, 417
1-forms, 416
2-forms, 416
and exterior derivative, 417
and gauge theory, 471
and Stokes’s theorem, 418
closed, 418
complex, 498
curl, 436
exact, 418
Hodge star, 439–440, 442
invariance of, 416
wedge product, 416
exact, 496–498
exercises, 500–501
exterior derivatives, 486–491
658

INDEX
exterior forms, 479–481
Frobenius’s theorem, 498–500
integration, 491–496
Stokes’s theorem, 495
Fourier series, 75–107
and scalar ﬁelds, 132
better convergence of integrated series, 88
complex, 75–79, 83–89, 96–107
for real functions, 76
of nonperiodic functions, 78–89
convergence, 84–89
convergence theorem, 85
exercises, 105–107
Gibbs overshoot, 81–82
nonrelativistic strings, 103
of a C1 function, 87
of nonperiodic functions, 78–89
Parseval’s identity, 100
periodic boundary conditions, 103–105
Born–von Karman, 105
poorer convergence of differentiated series, 89
quantum-mechanical examples, 89–96
real functions, 79–82
Gibbs overshoot, 81
several variables, 84
stretched intervals, 83–84
the interval, 77
where to put the 2πs, 77
Fourier transforms, 108–125, 129–134
and Ampère’s law, 123
and characteristic functions, 119
and convolutions, 121–124
and differential equations, 129–134
and diffusion equation, 133–134
and Fourier series, 110, 120
and Green’s functions, 121–125
and momentum space, 116–119
and Parseval’s relation, 113
and scalar wave equation, 131
and the delta function, 112–115
and the Feynman propagator, 120–121
and the uncertainty principle, 117–119
derivatives of, 115–119
exercises, 134–135
in several dimensions, 119–121
integrals of, 115–119
inverse of, 109
of a gaussian, 110–111, 183
of real functions, 111–112
Fourier–Legendre expansion, 310
Fourier–Mellin integral, 129
function
f (x ± 0), 85
continuous, 85
piecewise continuous, 85
functional derivatives, 578–585
and delta functions, 579–580
and variational methods, 579–582
exercises, 585
functional differential equation for ground
state of a ﬁeld theory, 583–585
functional differential equations, 583–585
notation used in physics, 579–580
of higher order, 581–582
Taylor series of, 582–583
functionals, 578
functions
analytic, 160–161
differentiable, 160
fundamental theorem of algebra, 174
gamma function, 141–145, 179–180
Gauss’s law, 154, 284, 412
gaussian integrals, 586–588
Gell-Mann’s SU(3) matrices, 377
general relativity, 289–291, 445–469
black holes, 456–457
cosmological constant, 454
cosmology, 457–469
dark energy, 454
Einstein’s equations, 453–456
Einstein–Hilbert action, 454–455
model cosmologies, 463–469
dark-energy dominated, 468
equation of state, 464
inﬂation dominated, 465
matter dominated, 466–468
radiation dominated, 465–466
transparency, 468
without acceleration, 464–465
Schwarzschild’s solution, 456–457
static and isotropic gravitational ﬁeld, 455–457
Schwarzschild’s solution, 456–457
standard form, 455–456
geometric series, 139–140
gradient, 228–230
grand uniﬁcation, 377
Grassmann numbers, 2, 6–7
Grassmann polynomials, 2
Grassmann variables, 613–619
Green’s function
for Helmholtz’s equation, 286
for Helmholtz’s modiﬁed equation, 286
for Laplacian, 123
for Poisson’s equation
and Legendre functions, 287
Green’s functions, 284–289
and eigenfunctions, 287–289
Feynman’s propagator, 287
for Helmholtz’s equation, 285–286
for Poisson’s equation, 284–287
of a self-adjoint operator, 288
Poisson’s equation, 285
group index of refraction, 209
groups, 348–399
O(n), 349
SO(3)
adjoint representation of, 366–367
SO(n), 349
659

INDEX
SU(2), 368–376
deﬁning representation of, 371–374
spin and statistics, 370
tensor products of representations, 369
SU(3), 377–379
SU(3) structure constants, 378
Z2, 358
Z3, 358
Zn, 358
abelian, 349
and symmetries in quantum mechanics, 351
and Yang–Mills gauge theory, 365
automorphism, 355
inner, 355
outer, 355
block-diagonal representations of, 351
centers of, 353
characters, 356–357
compact, 350–351
compact Lie groups
real structure constants of, 364
totally antisymmetric structure constants of,
364
completely reducible representations of, 351
conjugacy classes of, 353
continuous, 348–350
deﬁnition, 348
direct sum of representations of, 351
equivalent representations of, 351
exercises, 396–399
factor groups of by subgroups, 354
ﬁnite, 350, 358–361
multiplication table, 358
regular representation of, 359
further reading, 396
Gell-Mann matrices, 377–379
generators of adjoint representations of,
374–375
invariant integration, 384–385
irreducible representations of, 351
isomorphism, 354
Lie algebras, 361–399
SU(2), 368–374
Lie groups, 348–350, 361–399
SO(3), 366–367
SU(3), 377–379
SU(3) structure constants, 378
adjoint representations of, 374–375
antisymmetry of structure constants, 363
Cartan subalgebra of SU(3), 378
Cartan subalgebras of, 379
Cartan’s list of compact simple Lie groups,
383–384
Casimir operators of, 369, 375
compact, 361–385
deﬁning representation of SU(2), 371–374
deﬁnition of, 361
exponential parametrization, 362
generators of, 362
generators of adjoint representation, 374
hermitian generators of, 363
Jacobi identity, 374
noncompact, 361–364
nonhermitian generators of, 363
of rotations, 366–374
simple and semisimple, 376–377
structure constants of, 363, 374–375
SU(2) tensor product, 372
SU(3) generators, 377
symplectic group Sp(2n,R), 382
symplectic groups, 381–383
Lie groups have structure constants that are
independent of the representation, 365
Lorentz, 349
Lorentz group, 386–396
Dirac representation of, 393–395
Lie algebra of, 386–389
two-dimensional representations of, 389–393
matrix, 349
morphism, 354
noncompact, 350
nonabelian, 349
of matrices, 349–350
of orthogonal matrices, 349–350
of permutations, 360–361
of rotations, 366–374
adjoint representation of, 366–367
explicit 3 × 3 representation of, 367
generators of, 366–367
spin and statistics, 370
tensor operators of, 376
of transformations, 348–350
Lorentz, 348
Poincaré, 348
rotations and reﬂections, 348
translations, 348
of unitary matrices, 349–350
order of, 349, 358
Poincaré, 349
Poincaré group, 395–396
Lie algebra of, 395–396
reducible representations of, 351
representations of, 350–352
dimensions of, 350
in Hilbert space, 351–352
rotations
representations of, 352
Schur’s lemma, 355–356
semisimple, 353
similarity transformations, 351
simple, 353
simple and semisimple Lie algebras, 376–377
subgroups, 353–355
cosets of, 354
invariant, 353
left cosets of, 354
normal, 353
quotient coset space, 354
right cosets of, 354
trivial, 353
660

INDEX
symmetry
antilinear, antiunitary representations of, 352
linear, unitary representations of, 352
tensor product
addition of angular momenta, 357
tensor products, 357–358
translation, 348
unitary representations of, 351
harmonic function, 170
harmonic oscillator, 101–102, 272–273
Heaviside step function, 106
helicity
positive, 393
right-handed, 393
Helmholtz’s equation
in cylindrical coordinates
and Bessel functions, 328–335
in spherical coordinates
and spherical Bessel functions, 335–341
in three dimensions, 231–232
in two dimensions, 230–231
rectangular coordinates, 231
spherical coordinates, 232
and associated Legendre functions, 317
and spherical Bessel functions, 317
with azimuthal symmetry, 315–316
and Legendre polynomials, 315–316
and spherical Bessel functions, 315–316
Hermite functions, 264
Hermite’s system, 264
hermitian differential operators, 261
Hilbert spaces, 13–14, 25–26
homogeneous functions, 243–245
Euler’s theorem, 243
virial theorem, 243–244
index of refraction, 207
inﬁnite products, 157–158
inﬁnite series, 136–159
absolute convergence, 136
asymptotic, 152–153
WKB & Dyson, 153
Bernoulli numbers and polynomials, 151–152
binomial series, 148
binomial theorem, 147, 148
Cauchy’s criterion, 137
Cauchy’s root test, 137
comparison test, 137
conditional convergence, 136
convergence, 136–140
d’Alembert’s ratio test, 138
divergence of, 136
exercises, 158–159
Intel test, 138
logarithmic series, 148–149
of functions
convergence, 138–139
convergence in the mean, 138–139
Dirichlet series, 149–151
Fourier series, 146
geometric series, 139–140
power series, 139–140, 146
Taylor series, 145–149
uniform convergence, 139
Riemann zeta function, 149
uniform convergence, 138
and term-by-term integration, 139
inner products, 3, 11–14
and distance, 12
and norm, 12
degenerate, 12
hermitian, 12
indeﬁnite, 12, 14
Minkowski, 14
nondegenerate, 12
of functions, 13
positive deﬁnite, 11
Schwarz, 11
inner-product spaces, 13–14
integral equations, 296–304
exercises, 304
Fredholm, 297–301
eigenfunctions, 297–301
eigenvalues, 297–301
ﬁrst kind, 297
homogeneous, 297
inhomogeneous, 297
second kind, 297
implications of linearity, 298–304
integral transformations, 301–304
and Bessel functions, 302–304
Fourier, Laplace, and Euler kernels, 302
kernel, 297
numerical solutions, 299–301
Volterra, 297–301
eigenfunctions, 297–301
eigenvalues, 297–301
ﬁrst kind, 297
homogeneous, 297
inhomogeneous, 297
second kind, 297
integral transformations, 301–304
and Bessel functions, 302–304
Fourier, Laplace, and Euler kernels, 302
invariant distance, 409
invariant subspace, 37
Jacobi identity, 374
kernel of a matrix, 355
Kramers–Kronig relations, 206–208
Kronecker delta, 5, 415
Lagrange multipliers, 35–36, 54–55, 267–283
Lapack, 33, 66
Laplace transforms, 125–134
and convolutions, 134
and differential equations, 128–134
derivatives of, 127–128
661

INDEX
examples, 125
integrals of, 127
inversion of, 129
Laplace’s equation
in two dimensions, 316–317
Laplacian, 228–230
Legendre functions, 263–264, 287, 305–324
exercises, 323–324
second kind, 266
Legendre polynomials, 305–313
addition theorem for, 321
generating function, 307–309
Helmholtz’s equation
with azimuthal symmetry, 315–316
Legendre’s differential equation, 309–311
normalization, 305
recurrence relations, 311–312
Rodrigues’s formula, 306–307
Schlaeﬂi’s integral for, 312–313
special values of, 312
Legendre’s system, 263–264
Leibniz’s rule, 141
Lerch transcendent, 151
Levi-Civita symbol, 366
Lie algebras
ranks of, 379
roots of, 379
weight vector, 379
weights of, 379
light
slow, fast, and backwards, 209–210
linear algebra, 1–74
exercises, 71–74
linear dependence, 15–16, 224
and determinants, 29
linear independence, 15–16, 224
and completeness, 15
and determinants, 29
linear least squares, 34–35
linear operators, 9–11
and matrices, 9
density operator, 69
domain, 9
hermitian, 23
range, 9
real, symmetric, 23–24
self-adjoint, 23
unitary, 24–25
Lorentz force, 414
Lorentz transformations, 405–411
boost, 406
invariance under, 406
magnetic constant, 413
magnetic ﬁeld, 413
magnetic induction, 411
Majorana ﬁeld, 235
Majorana mass term, 391, 393
Maple, 33, 66
Mathematica, 33, 66
Matlab, 33, 61, 66
matrices, 4–7
adjoint, 4
and linear operators, 9
change of basis, 10
characteristic equation of, 38, 41–42
CKM, 63
and CP violation, 63
congruency transformation, 49
defective, 40
density operator, 69
diagonal form of square nondefective matrix,
41
functions of, 43–45
gamma, 393
hermitian, 5, 45–49
and diagonalization by a unitary
transformation, 48
complete and orthonormal eigenvectors, 47
degenerate eigenvalues, 46
eigenvalues of, 45
eigenvectors and eigenvalues of, 48
eigenvectors of, 46
identity, 5
imaginary and antisymmetric, 48
inverse, 5
inverses of, 31
nonnegative, 6
nonsingular, 40
normal, 50–55
compatible, 52–55
diagonalization by a unitary transformation,
50
orthogonal, 5, 25
Pauli, 5, 68, 371
positive, 6
positive deﬁnite, 6
rank of, 65
example, 65
rank-nullity theorem, 58
real and symmetric, 48
similarity transformation, 10, 41
singular-value decomposition, 55–63
example, 62
quark mass matrix, 62
square, 38–42
eigenvalues of, 38–42
eigenvectors of, 38–42
trace, 4
cyclic, 4
unitary, 5
upper triangular, 33
Maxwell’s equations, 412
in vacuum, 413
Maxwell–Ampère law, 412
mean value, 117
method of steepest descent, 210–212
metric spaces, 13–14
Minkowski space, 405–407
Monte Carlo methods, 563–577
662

INDEX
and evolution, 576–577
and lattice gauge theory, 574, 577
detailed balance, 574
exercises, 577
genetic algorithms, 577
in statistical mechanics, 572–575
Metropolis step, 572
Metropolis’s algorithm, 572–575
more general applications, 575–577
of data analysis, 566–572
example, 566–572
of numerical integration, 563–566
partition function, 574
smart schemes, 575
sweeps, 573
thermalization, 573
Moore–Penrose pseudoinverse, 63–65
natural units, 121
nearest-integer function, 108
nonlinear differential equations, 289–293
general relativity, 289–291
solitons, 291–293
notation for derivatives, 226–228
null space of a matrix, 355
numbers
complex, 1, 7
atan2, 2
phase of, 2
irrational, 1
natural, 1
rational, 1
real, 1
Octave, 33, 66
octet of baryons, 379
octet of pseudo-scalar mesons, 378
octonians, 384
open, 351
operators
adjoint of, 22–23
antilinear, 26–27
antiunitary, 26–27
compatible, 352
complete, 352
orthogonal, 25
real, symmetric, 23–24
unitary, 24–25
optical theorem, 207
ordinary differential equations, 223–225
and variational problems, 259–260
boundary conditions, 258–260
differential operators of deﬁnite parity, 255
even and odd differential operators, 254–255
exact, 238–242
Boyle’s law, 239
condition of integrability, 239
Einstein’s law, 239
human population growth, 239
integrating factors, 242
integration, 240–242
van der Waals’s equation, 239
ﬁrst-order, 235–248
exact, 238–242
separable, 235–238
self-adjoint, 266–267
Frobenius’s series solutions, 251–254
Fuch’s theorem, 253–254
indicial equation, 252
recurrence relations, 252
Green’s formula, 260
hermitian operators, 267
homogeneous ﬁrst-order, 245
homogeneous functions, 243–245
Lagrange’s identity, 260
linear, 223–225
general solution, 224, 225
homogeneous, 224
inhomogeneous, 224, 225
order of, 223
linear dependence of solutions, 224
linear independence of solutions, 224
linear, ﬁrst-order, 246–248
exact, 246
integrating factor, 246
meaning of exactness, 240–242
nonlinear, 225
second-order
eigenfunctions, 273–275
eigenvalues, 273–275
essential singularity of, 251
Green’s functions, 288
irregular singular point of, 251
making operators self adjoint, 264–265
nonessential singular point of, 251
regular singular point of, 251
second solution, 255–257
self-adjoint, 260–283
self-adjoint form, 223
singular points at inﬁnity, 251
singular points of, 250–251
weight function, 273
why not three solutions?, 257–258
wronskians of self-adjoint operators,
265–266
self-adjoint, 260–283
self-adjoint operators, 265
separable, 235–238
general integral, 236
hidden separability, 238
logistic equation, 236
Zipf’s law, 236
separated, 235
singular points of Legendre’s equation, 251
Sturm–Liouville problem, 265, 267–283
systems of, 248–250, 289–293
Friedmann’s equations, 289–291
Lagrange’s equations, 248–250
Wronski’s determinant, 255–258
orthogonal coordinates, 228–230
663

INDEX
divergence in, 228–230
gradient in, 228–230
Laplacian in, 228–230
orthogonal polynomials, 313–315
Hermite’s, 314
Jacobi’s, 313–314
Laguerre’s, 314–315
outer products, 18–22
example, 18
in Dirac’s notation, 21
partial differential equations, 225–235
Dirac equation, 234–235
general solution, 226
homogeneous, 226
inhomogeneous, 226
Klein–Gordon equation, 233
linear, 225–235
separable, 230–235
Helmholtz’s equation, 230–232
wave equations, 233–235
photon, 233–234
spin-one-half ﬁelds, 234–235
spinless bosons, 233
path integrals, 586–625
and gaussian integrals, 586–588
and lattice gauge theories, 622
and nonabelian gauge theories, 619–624
ghosts, 622–624
the method of Faddeev and Popov, 620–624
and perturbative ﬁeld theory, 605–624
and quantum electrodynamics, 609–613
and Schrödinger’s equation, 592–593
and the Bohm–Aharonov effect, 594–595
and the principle of stationary action, 591–592
euclidean, 588–590
euclidean correlation functions, 599–600
exercises, 624–625
fermionic, 613–619
ﬁnite temperature, 588–590
for a free particle in imaginary time, 595
for a free particle in real time, 593–595
for harmonic oscillator in imaginary time,
597–598
for harmonic oscillator in real time, 595–597
in ﬁeld theory, 603–624
in ﬁnite-temperature ﬁeld theory, 600–603
in imaginary time, 588–590
in real time, 590–593
Minkowski, 590–593
of ﬁelds, 600–624
of ﬁelds in euclidean space, 600–603
of ﬁelds in imaginary time, 600–603
ratios of and time-ordered products, 604–624
Pauli matrices, 371
permittivity, 154, 413
permutations, 360–361
and determinants, 30
cycles, 360
phase and group velocities, 208–210
slow, fast, and backwards light, 209
Planck’s constant, 149
Planck’s distribution, 149–151
points, 400–401
Poisson summation, 114–115
power series, 139–140
pre-Hilbert spaces, 13–14
principle of least action, 259–260
principle of stationary action, 248–250, 267–273,
443–447
in nonrelativistic mechanics, 328, 443–444
in quantum mechanics, 267–273
in relativistic electrodynamics, 445
in relativistic mechanics, 444–445
particle in a gravitational ﬁeld, 445–447
probability and statistics, 502–562
Bayes’s theorem, 502–505
Bernoulli’s distribution, 508
binomial distribution, 508–511
brownian motion, 520–527
Einstein–Nernst relation, 520–524
Langevin’s theory of, 520–527
Cauchy distributions, 532
central limit theorem, 532–543
illustrations of, 535–543
central moments, 505–508
centroid method, 514
characteristic functions, 527–530
chi-squared distribution, 531
chi-squared statistic, 551–554
convergence in probability, 535
correlation coefﬁcient, 507
covariance, 507
lower bound of Cramér and Rao, 546–550
cumulants, 529
diffusion, 519–527
diffusion constant, 523
direct stochastic optical reconstruction
microscopy, 515
Einstein–Nernst relation, 520–524
Einstein’s relation, 523
ensemble average, 521
error function, 515–518
estimators, 543–550
Bessel’s correction, 545
bias, 543
consistent, 543
standard deviation, 546
standard error, 546
exercises, 560–562
expectation, 505–508
expected value, 505–508
exponential distribution, 531
fat tails, 530–532
Fisher’s information matrix, 546–550
ﬂuctuation and dissipation, 524–527
gaussian distribution, 512–519
Gosset’s distribution, 530
Heisenberg’s uncertainty principle, 506–507
information, 546–550
664

INDEX
information matrix, 546–550
Kolmogorov’s function, 556
Kolmogorov’s test, 554–560
kurtosis, 530
Lévy distributions, 532
Lindeberg’s condition, 535
log-normal distribution, 531
Lorentz distributions, 532
maximum likelihood, 550–551
Maxwell–Boltzmann distribution, 518–519
mean, 505–508
moment-generating functions, 527–530
moments, 505–508
normal distribution, 514
Pearson’s distribution, 531
Poisson distribution, 511–512
coherent states, 512
power-law tails, 530
probability density, 505–519
probability distribution, 505–519
random-number generators, 537–538
skewness, 530
Student’s t-distribution, 530
variance, 505–508
lower bound of Cramér and Rao, 546–550
viscous-friction coefﬁcient, 523
proper time, 409
and time dilation, 410–411
pseudoinverse, 63–65, 551
quantum mechanics, 267–283
quaternions, 379–383
and the Pauli matrices, 380
R-C circuit, 247
regular and self-adjoint differential system, 262
relative permittivity, 154
renormalization group, 237, 626–634
exercises, 634
in condensed-matter physics, 632–634
in lattice ﬁeld theory, 630–632
in quantum ﬁeld theory, 626–634
rotations, 366–374
scalar ﬁelds, 131, 401
scalars, 401
Schwarz inequality, 14–15, 284
examples, 14
Schwarz inner products, 69–71
seesaw mechanism, 49
self-adjoint differential operators, 260–283
self-adjoint differential systems, 262–283
sequence of functions
convergence in the mean, 88
simple and semisimple Lie algebras, 376–377
simple and semisimple Lie groups, 376–377
simple eigenvalue, 40
simply connected, 164
slow, fast, and backwards light, 209
and Kramers–Kronig relations, 210
small oscillations, 250
solitons, 291–293
special relativity, 408–414
4-vector force, 410
and time dilation, 410–411
electrodynamics, 411–414
energy–momentum 4-vector, 410
kinematics, 410–411
spherical harmonics, 319–323
spin and statistics, 370
standard model of particle physics, 377
states
coherent, 71
Stefan’s constant, 150
Stokes’s theorem, 170
strings, 643–650
and inﬁnities of quantum ﬁeld theory, 643
Dirichlet boundary condition, 645
free-endpoint boundary condition, 645
Nambu–Goto action, 643–647
quantized, 647–650
D-branes, 647–648
Regge trajectories, 646–647
Riemann surfaces and moduli, 649–650
scattering of, 648
Sturm–Liouville equation, 265, 267–283
SU(3) and quarks, 378–379
subspace, 351
invariant, 351
proper, 351
summation convention, 404–405
symmetric differential operators, 261
symmetry
in quantum mechanics, 26
systems of linear equations, 34–35
Taylor series, 145–149
tensor products, 377
adding angular momenta, 357–358, 371–374
adding spins, 68, 371–374
and hydrogen atom, 68
tensors, 400–478
afﬁne connections, 431–433
and metric tensor, 436–437
and general relativity, 445–469
antisymmetric, 415
basic axiom of relativity, 422
basis vectors, 421
Bianchi identity, 436
Christoffel symbols, 431–433
connections, 431–433
contractions, 416
contravariant metric tensor, 422
covariant curl, 434–436
covariant derivatives, 431–434
and antisymmetry, 436
metric tensor, 437–438
covariant metric tensor, 422
curvature, 451–453
curvature of a sphere, 451–453
665

INDEX
curvature scalar, 451
cylindrical coordinates, 425
divergence of a contravariant vector, 438–443
and Hodge star, 439–440
and Laplacian, 441–443
Einstein’s equations, 453–456
exercises, 475–478
gauge theory, 469–475
geometry of, 474–475
role of vectors, 471–473
standard model, 469–473
gradient, 426–427
Hodge star, 428–431
and divergence, 428
and Laplacian, 428
and Maxwell’s equations, 430–431
Laplacian, 441–443
and Hodge star, 442
Levi-Civita’s symbol, 427–431
Levi-Civita’s tensor, 427–431
metric of sphere, 421
metric tensor, 420–427
moving frame, 421
notation for derivatives, 433
orthogonal coordinates, 423–426
parallel transport, 433
particle in a weak, static gravitational ﬁeld,
448–451
gravitational redshift, 450
gravitational time dilation, 449–450
perfect ﬂuid, 453
polar coordinates, 424
principle of equivalence, 447–448
geodesic equation, 448
quotient theorem, 420
raising, lowering indices, 423
Ricci tensor, 451
Riemann tensor, 451
second-rank, 414–416
spherical coordinates, 425–426
symmetric, 415
tensor equations, 419–420
torsion tensor, 433
third-harmonic microscopy, 184
time dependence of Heisenberg operators, 604
time dilation, 409–410
in muon decay, 409–410
time-ordered product, 121, 604
total cross-section, 207
uncertainty principle, 117–119, 244
variance
of an operator, 118–119
variational methods, 248–250, 259–260, 267–273,
443–447
in nonrelativistic mechanics, 328, 443–444
in quantum mechanics, 267–273
in relativistic electrodynamics, 445
in relativistic mechanics, 444–445
particle in a gravitational ﬁeld, 445–447
strings, 643–645
vector space, 16
dimension of, 16
vectors, 7–8
basis, 7, 16, 17
complete, 16
components, 7
direct product, 66–68
example, 68
eigenvalues, 37–55
example, 37
eigenvectors, 37–55
example, 37
eigenvectors of square matrix, 40
functions as, 8
orthonormal, 16–17
Gram–Schmidt method, 16–17
partial derivatives as, 8
span, 16
span a space, 16
states as, 8
tensor product, 66–68
Virasoro’s algebra, 219, 222
virial theorem, 243–244
wedge product, 416
Weyl spinor
left-handed, 390
right-handed, 393
Wigner–Eckart theorem
special case of, 355–356
wronskian, 255–258
Yang–Mills theory, 469–475
geometry of, 474–475
role of vectors, 471–473
standard model, 469–473
Yukawa potential, 125, 286
zeta function, 149–151
666


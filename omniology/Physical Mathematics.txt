more information ‚Äì www.cambridge.org/9781107005211


Physical Mathematics
Unique in its clarity, examples, and range, Physical Mathematics explains as
simply as possible the mathematics that graduate students and professional
physicists need in their courses and research. The author illustrates the mathe-
matics with numerous physical examples drawn from contemporary research.
In addition to basic subjects such as linear algebra, Fourier analysis, com-
plex variables, differential equations, and Bessel functions, this textbook covers
topics such as the singular-value decomposition, Lie algebras, the tensors and
forms of general relativity, the central limit theorem and Kolmogorov test of
statistics, the Monte Carlo methods of experimental and theoretical physics, the
renormalization group of condensed-matter physics, and the functional deriva-
tives and Feynman path integrals of quantum Ô¨Åeld theory. Solutions to exercises
are available for instructors at www.cambridge.org/cahill
KEVIN CAHILL is Professor of Physics and Astronomy at the University
of New Mexico. He has done research at NIST, Saclay, Ecole Polytechnique,
Orsay, Harvard, NIH, LBL, and SLAC, and has worked in quantum optics,
quantum Ô¨Åeld theory, lattice gauge theory, and biophysics. Physical Mathemat-
ics is based on courses taught by the author at the University of New Mexico
and at Fudan University in Shanghai.


Physical Mathematics
KEVIN CAHILL
University of New Mexico

C A M B R I D G E U N I V E R S I T Y P R E S S
Cambridge, New York, Melbourne, Madrid, Cape Town,
Singapore, S√£o Paulo, Delhi, Mexico City
Cambridge University Press
The Edinburgh Building, Cambridge CB2 8RU, UK
Published in the United States of America by Cambridge University Press, New York
www.cambridge.org
Information on this title: www.cambridge.org/9781107005211
c‚ÉùK. Cahill 2013
This publication is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without the written
permission of Cambridge University Press.
First published 2013
Printed and bound in the United Kingdom by the MPG Books Group
A catalog record for this publication is available from the British Library
Library of Congress Cataloging in Publication data
Cahill, Kevin, 1941‚Äì, author.
Physical mathematics / Kevin Cahill, University of New Mexico.
pages
cm
ISBN 978-1-107-00521-1 (hardback)
1. Mathematical physics.
I. Title.
QC20.C24
2012
530.15‚Äìdc23
2012036027
ISBN 978-1-107-00521-1 Hardback
Cambridge University Press has no responsibility for the persistence or
accuracy of URLs for external or third-party internet websites referred to
in this publication, and does not guarantee that any content on such
websites is, or will remain, accurate or appropriate.

For Ginette, Mike, Sean, Peter, Mia, and James,
and in honor of Muntadhar al-Zaidi.


Contents
Preface
page xvii
1
Linear algebra
1
1.1
Numbers
1
1.2
Arrays
2
1.3
Matrices
4
1.4
Vectors
7
1.5
Linear operators
9
1.6
Inner products
11
1.7
The Cauchy‚ÄìSchwarz inequality
14
1.8
Linear independence and completeness
15
1.9
Dimension of a vector space
16
1.10
Orthonormal vectors
16
1.11
Outer products
18
1.12
Dirac notation
19
1.13
The adjoint of an operator
22
1.14
Self-adjoint or hermitian linear operators
23
1.15
Real, symmetric linear operators
23
1.16
Unitary operators
24
1.17
Hilbert space
25
1.18
Antiunitary, antilinear operators
26
1.19
Symmetry in quantum mechanics
26
1.20
Determinants
27
1.21
Systems of linear equations
34
1.22
Linear least squares
34
1.23
Lagrange multipliers
35
1.24
Eigenvectors
37
vii

CONTENTS
1.25
Eigenvectors of a square matrix
38
1.26
A matrix obeys its characteristic equation
41
1.27
Functions of matrices
43
1.28
Hermitian matrices
45
1.29
Normal matrices
50
1.30
Compatible normal matrices
52
1.31
The singular-value decomposition
55
1.32
The Moore‚ÄìPenrose pseudoinverse
63
1.33
The rank of a matrix
65
1.34
Software
66
1.35
The tensor/direct product
66
1.36
Density operators
69
1.37
Correlation functions
69
Exercises
71
2
Fourier series
75
2.1
Complex Fourier series
75
2.2
The interval
77
2.3
Where to put the 2œÄs
77
2.4
Real Fourier series for real functions
79
2.5
Stretched intervals
83
2.6
Fourier series in several variables
84
2.7
How Fourier series converge
84
2.8
Quantum-mechanical examples
89
2.9
Dirac notation
96
2.10
Dirac‚Äôs delta function
97
2.11
The harmonic oscillator
101
2.12
Nonrelativistic strings
103
2.13
Periodic boundary conditions
103
Exercises
105
3
Fourier and Laplace transforms
108
3.1
The Fourier transform
108
3.2
The Fourier transform of a real function
111
3.3
Dirac, Parseval, and Poisson
112
3.4
Fourier derivatives and integrals
115
3.5
Fourier transforms in several dimensions
119
3.6
Convolutions
121
3.7
The Fourier transform of a convolution
123
3.8
Fourier transforms and Green‚Äôs functions
124
3.9
Laplace transforms
125
3.10
Derivatives and integrals of Laplace transforms
127
viii

CONTENTS
3.11
Laplace transforms and differential equations
128
3.12
Inversion of Laplace transforms
129
3.13
Application to differential equations
129
Exercises
134
4
InÔ¨Ånite series
136
4.1
Convergence
136
4.2
Tests of convergence
137
4.3
Convergent series of functions
138
4.4
Power series
139
4.5
Factorials and the gamma function
141
4.6
Taylor series
145
4.7
Fourier series as power series
146
4.8
The binomial series and theorem
147
4.9
Logarithmic series
148
4.10
Dirichlet series and the zeta function
149
4.11
Bernoulli numbers and polynomials
151
4.12
Asymptotic series
152
4.13
Some electrostatic problems
154
4.14
InÔ¨Ånite products
157
Exercises
158
5
Complex-variable theory
160
5.1
Analytic functions
160
5.2
Cauchy‚Äôs integral theorem
161
5.3
Cauchy‚Äôs integral formula
165
5.4
The Cauchy‚ÄìRiemann conditions
169
5.5
Harmonic functions
170
5.6
Taylor series for analytic functions
171
5.7
Cauchy‚Äôs inequality
173
5.8
Liouville‚Äôs theorem
173
5.9
The fundamental theorem of algebra
174
5.10
Laurent series
174
5.11
Singularities
177
5.12
Analytic continuation
179
5.13
The calculus of residues
180
5.14
Ghost contours
182
5.15
Logarithms and cuts
193
5.16
Powers and roots
194
5.17
Conformal mapping
197
5.18
Cauchy‚Äôs principal value
198
5.19
Dispersion relations
205
ix

CONTENTS
5.20
Kramers‚ÄìKronig relations
207
5.21
Phase and group velocities
208
5.22
The method of steepest descent
210
5.23
The Abel‚ÄìPlana formula and the Casimir effect
212
5.24
Applications to string theory
217
Exercises
219
6
Differential equations
223
6.1
Ordinary linear differential equations
223
6.2
Linear partial differential equations
225
6.3
Notation for derivatives
226
6.4
Gradient, divergence, and curl
228
6.5
Separable partial differential equations
230
6.6
Wave equations
233
6.7
First-order differential equations
235
6.8
Separable Ô¨Årst-order differential equations
235
6.9
Hidden separability
238
6.10
Exact Ô¨Årst-order differential equations
238
6.11
The meaning of exactness
240
6.12
Integrating factors
242
6.13
Homogeneous functions
243
6.14
The virial theorem
243
6.15
Homogeneous Ô¨Årst-order ordinary differential
equations
245
6.16
Linear Ô¨Årst-order ordinary differential equations
246
6.17
Systems of differential equations
248
6.18
Singular points of second-order ordinary differential
equations
250
6.19
Frobenius‚Äôs series solutions
251
6.20
Fuch‚Äôs theorem
253
6.21
Even and odd differential operators
254
6.22
Wronski‚Äôs determinant
255
6.23
A second solution
255
6.24
Why not three solutions?
257
6.25
Boundary conditions
258
6.26
A variational problem
259
6.27
Self-adjoint differential operators
260
6.28
Self-adjoint differential systems
262
6.29
Making operators formally self adjoint
264
6.30
Wronskians of self-adjoint operators
265
6.31
First-order self-adjoint differential operators
266
6.32
A constrained variational problem
267
x

CONTENTS
6.33
Eigenfunctions and eigenvalues of self-adjoint systems
273
6.34
Unboundedness of eigenvalues
275
6.35
Completeness of eigenfunctions
277
6.36
The inequalities of Bessel and Schwarz
284
6.37
Green‚Äôs functions
284
6.38
Eigenfunctions and Green‚Äôs functions
287
6.39
Green‚Äôs functions in one dimension
288
6.40
Nonlinear differential equations
289
Exercises
293
7
Integral equations
296
7.1
Fredholm integral equations
297
7.2
Volterra integral equations
297
7.3
Implications of linearity
298
7.4
Numerical solutions
299
7.5
Integral transformations
301
Exercises
304
8
Legendre functions
305
8.1
The Legendre polynomials
305
8.2
The Rodrigues formula
306
8.3
The generating function
308
8.4
Legendre‚Äôs differential equation
309
8.5
Recurrence relations
311
8.6
Special values of Legendre‚Äôs polynomials
312
8.7
SchlaeÔ¨Çi‚Äôs integral
313
8.8
Orthogonal polynomials
313
8.9
The azimuthally symmetric Laplacian
315
8.10
Laplacian in two dimensions
316
8.11
The Laplacian in spherical coordinates
317
8.12
The associated Legendre functions/polynomials
317
8.13
Spherical harmonics
319
Exercises
323
9
Bessel functions
325
9.1
Bessel functions of the Ô¨Årst kind
325
9.2
Spherical Bessel functions of the Ô¨Årst kind
335
9.3
Bessel functions of the second kind
341
9.4
Spherical Bessel functions of the second kind
343
Further reading
345
Exercises
345
xi

CONTENTS
10
Group theory
348
10.1
What is a group?
348
10.2
Representations of groups
350
10.3
Representations acting in Hilbert space
351
10.4
Subgroups
353
10.5
Cosets
354
10.6
Morphisms
354
10.7
Schur‚Äôs lemma
355
10.8
Characters
356
10.9
Tensor products
357
10.10 Finite groups
358
10.11 The regular representation
359
10.12 Properties of Ô¨Ånite groups
360
10.13 Permutations
360
10.14 Compact and noncompact Lie groups
361
10.15 Lie algebra
361
10.16 The rotation group
366
10.17 The Lie algebra and representations of SU(2)
368
10.18 The deÔ¨Åning representation of SU(2)
371
10.19 The Jacobi identity
374
10.20 The adjoint representation
374
10.21 Casimir operators
375
10.22 Tensor operators for the rotation group
376
10.23 Simple and semisimple Lie algebras
376
10.24 SU(3)
377
10.25 SU(3) and quarks
378
10.26 Cartan subalgebra
379
10.27 Quaternions
379
10.28 The symplectic group Sp (2n)
381
10.29 Compact simple Lie groups
383
10.30 Group integration
384
10.31 The Lorentz group
386
10.32 Two-dimensional representations of the Lorentz group
389
10.33 The Dirac representation of the Lorentz group
393
10.34 The Poincar√© group
395
Further reading
396
Exercises
397
11
Tensors and local symmetries
400
11.1
Points and coordinates
400
11.2
Scalars
401
11.3
Contravariant vectors
401
xii

CONTENTS
11.4
Covariant vectors
402
11.5
Euclidean space in euclidean coordinates
402
11.6
Summation conventions
404
11.7
Minkowski space
405
11.8
Lorentz transformations
407
11.9
Special relativity
408
11.10 Kinematics
410
11.11 Electrodynamics
411
11.12 Tensors
414
11.13 Differential forms
416
11.14 Tensor equations
419
11.15 The quotient theorem
420
11.16 The metric tensor
421
11.17 A basic axiom
422
11.18 The contravariant metric tensor
422
11.19 Raising and lowering indices
423
11.20 Orthogonal coordinates in euclidean n-space
423
11.21 Polar coordinates
424
11.22 Cylindrical coordinates
425
11.23 Spherical coordinates
425
11.24 The gradient of a scalar Ô¨Åeld
426
11.25 Levi-Civita‚Äôs tensor
427
11.26 The Hodge star
428
11.27 Derivatives and afÔ¨Åne connections
431
11.28 Parallel transport
433
11.29 Notations for derivatives
433
11.30 Covariant derivatives
434
11.31 The covariant curl
435
11.32 Covariant derivatives and antisymmetry
436
11.33 AfÔ¨Åne connection and metric tensor
436
11.34 Covariant derivative of the metric tensor
437
11.35 Divergence of a contravariant vector
438
11.36 The covariant Laplacian
441
11.37 The principle of stationary action
443
11.38 A particle in a gravitational Ô¨Åeld
446
11.39 The principle of equivalence
447
11.40 Weak, static gravitational Ô¨Åelds
449
11.41 Gravitational time dilation
449
11.42 Curvature
451
11.43 Einstein‚Äôs equations
453
11.44 The action of general relativity
455
11.45 Standard form
455
xiii

CONTENTS
11.46 Schwarzschild‚Äôs solution
456
11.47 Black holes
456
11.48 Cosmology
457
11.49 Model cosmologies
463
11.50 Yang‚ÄìMills theory
469
11.51 Gauge theory and vectors
471
11.52 Geometry
474
Further reading
475
Exercises
475
12
Forms
479
12.1
Exterior forms
479
12.2
Differential forms
481
12.3
Exterior differentiation
486
12.4
Integration of forms
491
12.5
Are closed forms exact?
496
12.6
Complex differential forms
498
12.7
Frobenius‚Äôs theorem
499
Further reading
500
Exercises
500
13
Probability and statistics
502
13.1
Probability and Thomas Bayes
502
13.2
Mean and variance
505
13.3
The binomial distribution
508
13.4
The Poisson distribution
511
13.5
The Gaussian distribution
512
13.6
The error function erf
515
13.7
The Maxwell‚ÄìBoltzmann distribution
518
13.8
Diffusion
519
13.9
Langevin‚Äôs theory of brownian motion
520
13.10 The Einstein‚ÄìNernst relation
523
13.11 Fluctuation and dissipation
524
13.12 Characteristic and moment-generating functions
528
13.13 Fat tails
530
13.14 The central limit theorem and Jarl Lindeberg
532
13.15 Random-number generators
537
13.16 Illustration of the central limit theorem
538
13.17 Measurements, estimators, and Friedrich Bessel
543
13.18 Information and Ronald Fisher
546
13.19 Maximum likelihood
550
13.20 Karl Pearson‚Äôs chi-squared statistic
551
xiv

CONTENTS
13.21 Kolmogorov‚Äôs test
554
Further reading
560
Exercises
560
14
Monte Carlo methods
563
14.1
The Monte Carlo method
563
14.2
Numerical integration
563
14.3
Applications to experiments
566
14.4
Statistical mechanics
572
14.5
Solving arbitrary problems
575
14.6
Evolution
576
Further reading
577
Exercises
577
15
Functional derivatives
578
15.1
Functionals
578
15.2
Functional derivatives
578
15.3
Higher-order functional derivatives
581
15.4
Functional Taylor series
582
15.5
Functional differential equations
583
Exercises
585
16
Path integrals
586
16.1
Path integrals and classical physics
586
16.2
Gaussian integrals
586
16.3
Path integrals in imaginary time
588
16.4
Path integrals in real time
590
16.5
Path integral for a free particle
593
16.6
Free particle in imaginary time
595
16.7
Harmonic oscillator in real time
595
16.8
Harmonic oscillator in imaginary time
597
16.9
Euclidean correlation functions
599
16.10 Finite-temperature Ô¨Åeld theory
600
16.11 Real-time Ô¨Åeld theory
603
16.12 Perturbation theory
605
16.13 Application to quantum electrodynamics
609
16.14 Fermionic path integrals
613
16.15 Application to nonabelian gauge theories
619
16.16 The Faddeev‚ÄìPopov trick
620
16.17 Ghosts
622
Further reading
624
Exercises
624
xv

CONTENTS
17
The renormalization group
626
17.1
The renormalization group in quantum Ô¨Åeld theory
626
17.2
The renormalization group in lattice Ô¨Åeld theory
630
17.3
The renormalization group in condensed-matter physics
632
Exercises
634
18
Chaos and fractals
635
18.1
Chaos
635
18.2
Attractors
639
18.3
Fractals
639
Further reading
642
Exercises
642
19
Strings
643
19.1
The inÔ¨Ånities of quantum Ô¨Åeld theory
643
19.2
The Nambu‚ÄìGoto string action
643
19.3
Regge trajectories
646
19.4
Quantized strings
647
19.5
D-branes
647
19.6
String‚Äìstring scattering
648
19.7
Riemann surfaces and moduli
649
Further reading
650
Exercises
650
References
651
Index
656
xvi

Preface
To the students: you will Ô¨Ånd some physics crammed in amongst the mathemat-
ics. Don‚Äôt let the physics bother you. As you study the math, you‚Äôll learn some
physics without extra effort. The physics is a freebie. I have tried to explain the
math you need for physics and have left out the rest.
To the professors: the book is for students who also are taking mechanics,
electrodynamics, quantum mechanics, and statistical mechanics nearly simulta-
neously and who soon may use probability or path integrals in their research.
Linear algebra and Fourier analysis are the keys to physics, so the book starts
with them, but you may prefer to skip the algebra or postpone the Fourier
analysis. The book is intended to support a one- or two-semester course for
graduate students or advanced undergraduates. The Ô¨Årst seven, eight, or nine
chapters Ô¨Åt in one semester, the others in a second. A list of errata is main-
tained at panda.unm.edu/cahill, and solutions to all the exercises are available
for instructors at www.cambridge.org/cahill.
Several friends ‚Äì Susan Atlas, Bernard Becker, Steven Boyd, Robert Bur-
ckel, Sean Cahill, Colston Chandler, Vageli Coutsias, David Dunlap, Daniel
Finley, Franco Giuliani, Roy Glauber, Pablo Gondolo, Igor Gorelov, Jiaxing
Hong, Fang Huang, Dinesh Loomba, Yin Luo, Lei Ma, Michael Malik, Kent
Morrison, Sudhakar Prasad, Randy Reeder, Dmitri Sergatskov, and David
Waxman ‚Äì have given me valuable advice. Students have helped with questions,
ideas, and corrections, especially Thomas Beechem, Marie Cahill, Chris Cesare,
Yihong Cheng, Charles Cherqui, Robert Cordwell, Amo-Kwao Godwin, Aram
Gragossian, Aaron Hankin, Kangbo Hao, Tiffany Hayes, Yiran Hu, Shanshan
Huang, Tyler Keating, Joshua Koch, Zilong Li, Miao Lin, ZuMou Lin, Sheng
Liu, Yue Liu, Ben Oliker, Boleszek Osinski, Ravi Raghunathan, Akash Rakho-
lia, Xingyue Tian, Toby Tolley, Jiqun Tu, Christopher Vergien, Weizhen Wang,
George Wendelberger, Xukun Xu, Huimin Yang, Zhou Yang, Daniel Young,
Mengzhen Zhang, Lu Zheng, Lingjun Zhou, and Daniel Zirzow.
xvii


1
Linear algebra
1.1 Numbers
The natural numbers are the positive integers and zero. Rational numbers are
ratios of integers. Irrational numbers have decimal digits dn
x =
‚àû

n=mx
dn
10n
(1.1)
that do not repeat. Thus the repeating decimals 1/2 = 0.50000 . . . and 1/3 =
0.¬Ø3 ‚â°0.33333 . . . are rational, while œÄ = 3.141592654 . . . is irrational. Deci-
mal arithmetic was invented in India over 1500 years ago but was not widely
adopted in the Europe until the seventeenth century.
The real numbers R include the rational numbers and the irrational numbers;
they correspond to all the points on an inÔ¨Ånite line called the real line.
The complex numbers C are the real numbers with one new number i whose
square is ‚àí1. A complex number z is a linear combination of a real number x
and a real multiple i y of i
z = x + iy.
(1.2)
Here x = Rez is the real part of z, and y = Imz is its imaginary part. One adds
complex numbers by adding their real and imaginary parts
z1 + z2 = x1 + iy1 + x2 + iy2 = x1 + x2 + i(y1 + y2).
(1.3)
Since i2 = ‚àí1, the product of two complex numbers is
z1z2 = (x1 + iy1)(x2 + iy2) = x1x2 ‚àíy1y2 + i(x1y2 + y1x2).
(1.4)
The polar representation z = r exp(iŒ∏) of z = x + iy is
z = x + iy = reiŒ∏ = r(cos Œ∏ + i sin Œ∏)
(1.5)
1

LINEAR ALGEBRA
in which r is the modulus or absolute value of z
r = |z| =

x2 + y2
(1.6)
and Œ∏ is its phase or argument
Œ∏ = arctan (y/x).
(1.7)
Since exp(2œÄi) = 1, there is an inevitable ambiguity in the deÔ¨Ånition of
the phase of any complex number: for any integer n, the phase Œ∏ + 2œÄn gives
the same z as Œ∏. In various computer languages, the function atan2(y, x) returns
the angle Œ∏ in the interval ‚àíœÄ < Œ∏ ‚â§œÄ for which (x, y) = r(cos Œ∏, sin Œ∏).
There are two common notations z‚àóand ¬Øz for the complex conjugate of a
complex number z = x + iy
z‚àó= ¬Øz = x ‚àíiy.
(1.8)
The square of the modulus of a complex number z = x + iy is
|z|2 = x2 + y2 = (x + iy)(x ‚àíiy) = ¬Øzz = z‚àóz.
(1.9)
The inverse of a complex number z = x + iy is
z‚àí1 = (x + iy)‚àí1 =
x ‚àíiy
(x ‚àíiy)(x + iy) = x ‚àíiy
x2 + y2 = z‚àó
z‚àóz = z‚àó
|z|2 .
(1.10)
Grassmann numbers Œ∏i are anticommuting numbers, that is, the anti-
commutator of any two Grassmann numbers vanishes
{Œ∏i, Œ∏j} ‚â°[Œ∏i, Œ∏j]+ ‚â°Œ∏iŒ∏j + Œ∏jŒ∏i = 0.
(1.11)
So the square of any Grassmann number is zero, Œ∏2
i = 0. We won‚Äôt use these
numbers until chapter 16, but they do have amusing properties. The highest
monomial in N Grassmann numbers Œ∏i is the product Œ∏1Œ∏2 . . . Œ∏N. So the most
complicated power series in two Grassmann numbers is just
f (Œ∏1, Œ∏2) = f0 + f1 Œ∏1 + f2 Œ∏2 + f12 Œ∏1Œ∏2
(1.12)
(Hermann Grassmann, 1809‚Äì1877).
1.2 Arrays
An array is an ordered set of numbers. Arrays play big roles in computer science,
physics, and mathematics. They can be of any (integral) dimension.
A one-dimensional array (a1, a2, . . . , an) is variously called an n-tuple, a row
vector when written horizontally, a column vector when written vertically, or an
n-vector. The numbers ak are its entries or components.
A two-dimensional array aik with i running from 1 to n and k from 1 to m is
an n √ó m matrix. The numbers aik are its entries, elements, or matrix elements.
2

1.2 ARRAYS
One can think of a matrix as a stack of row vectors or as a queue of column
vectors. The entry aik is in the ith row and the kth column.
One can add together arrays of the same dimension and shape by adding
their entries. Two n-tuples add as
(a1, . . . , an) + (b1, . . . , bn) = (a1 + b1, . . . , an + bn)
(1.13)
and two n √ó m matrices a and b add as
(a + b)ik = aik + bik.
(1.14)
One can multiply arrays by numbers. Thus z times the three-dimensional
array aijk is the array with entries z aijk. One can multiply two arrays together
no matter what their shapes and dimensions. The outer product of an n-tuple a
and an m-tuple b is an n √ó m matrix with elements
(a b)ik = ai bk
(1.15)
or an m √ó n matrix with entries (ba)ki = bkai. If a and b are complex, then one
also can form the outer products (a b)ik = ai bk, (b a)ki = bk ai, and (b a)ki =
bk ai. The outer product of a matrix aik and a three-dimensional array bj‚Ñìm is a
Ô¨Åve-dimensional array
(a b)ikj‚Ñìm = aik bj‚Ñìm.
(1.16)
An inner product is possible when two arrays are of the same size in one of
their dimensions. Thus the inner product (a, b) ‚â°‚ü®a|b‚ü©or dot-product a ¬∑ b of
two real n-tuples a and b is
(a, b) = ‚ü®a|b‚ü©= a ¬∑ b = (a1, . . . , an) ¬∑ (b1, . . . , bn) = a1b1 + ¬∑ ¬∑ ¬∑ + anbn.
(1.17)
The inner product of two complex n-tuples often is deÔ¨Åned as
(a, b) = ‚ü®a|b‚ü©= a ¬∑ b = (a1, . . . , an) ¬∑ (b1, . . . , bn) = a1 b1 + ¬∑ ¬∑ ¬∑ + an bn
(1.18)
or as its complex conjugate
(a, b)‚àó= ‚ü®a|b‚ü©‚àó= (a ¬∑ b)‚àó= (b, a) = ‚ü®b|a‚ü©= b ¬∑ a
(1.19)
so that the inner product of a vector with itself is nonnegative (a, a) ‚â•0.
The product of an m√ón matrix aik times an n-tuple bk is the m-tuple b‚Ä≤ whose
ith component is
b‚Ä≤
i = ai1b1 + ai2b2 + ¬∑ ¬∑ ¬∑ + ainbn =
n

k=1
aikbk.
(1.20)
This product is b‚Ä≤ = a b in matrix notation.
If the size n of the second dimension of a matrix a matches that of the Ô¨Årst
dimension of a matrix b, then their product a b is a matrix with entries
(a b)i‚Ñì= ai1 b1‚Ñì+ ¬∑ ¬∑ ¬∑ + ain bn‚Ñì.
(1.21)
3

LINEAR ALGEBRA
1.3 Matrices
Apart from n-tuples, the most important arrays in linear algebra are the two-
dimensional arrays called matrices.
The trace of an n √ó n matrix a is the sum of its diagonal elements
Tr a = tr a = a11 + a22 + ¬∑ ¬∑ ¬∑ + ann =
n

i=1
aii.
(1.22)
The trace of two matrices is independent of their order
Tr (a b) =
n

i=1
n

k=1
aikbki =
n

k=1
n

i=1
bkiaik = Tr (ba)
(1.23)
as long as the matrix elements are numbers that commute with each other. It
follows that the trace is cyclic
Tr (a b . . . z) = Tr (b . . . z a) .
(1.24)
The transpose of an n √ó ‚Ñìmatrix a is an ‚Ñì√ó n matrix aT with entries

aT
ij = aji.
(1.25)
Some mathematicians use a prime to mean transpose, as in a‚Ä≤ = aT, but physi-
cists tend to use primes to label different objects or to indicate differentiation.
One may show that
(a b) T = bT aT.
(1.26)
A matrix that is equal to its transpose
a = aT
(1.27)
is symmetric.
The (hermitian) adjoint of a matrix is the complex conjugate of its transpose
(Charles Hermite, 1822‚Äì1901). That is, the (hermitian) adjoint a‚Ä† of an N √ó L
complex matrix a is the L √ó N matrix with entries
(a‚Ä†)ij = (aji)‚àó= a‚àó
ji.
(1.28)
One may show that
(a b)‚Ä† = b‚Ä† a‚Ä†.
(1.29)
A matrix that is equal to its adjoint
(a‚Ä†)ij = (aji)‚àó= a‚àó
ji = aij
(1.30)
4

1.3 MATRICES
(and which must be a square matrix) is hermitian or self adjoint
a = a‚Ä†.
(1.31)
Example 1.1 (The Pauli matrices)
œÉ1 =
0
1
1
0

,
œÉ2 =
0
‚àíi
i
0

,
and
œÉ3 =
1
0
0
‚àí1

(1.32)
are all hermitian (Wolfgang Pauli, 1900‚Äì1958).
A real hermitian matrix is symmetric. If a matrix a is hermitian, then the
quadratic form
‚ü®v|a|v‚ü©=
N

i=1
N

j=1
v‚àó
i aijvj ‚ààR
(1.33)
is real for all complex n-tuples v.
The Kronecker delta Œ¥ik is deÔ¨Åned to be unity if i = k and zero if
i Ã∏= k (Leopold Kronecker, 1823‚Äì1891). The identity matrix I has entries
Iik = Œ¥ik.
The inverse a‚àí1 of an n √ó n matrix a is a square matrix that satisÔ¨Åes
a‚àí1 a = a a‚àí1 = I
(1.34)
in which I is the n √ó n identity matrix.
So far we have been writing n-tuples and matrices and their elements with
lower-case letters. It is equally common to use capital letters, and we will do so
for the rest of this section.
A matrix U whose adjoint U‚Ä† is its inverse
U‚Ä†U = UU‚Ä† = I
(1.35)
is unitary. Unitary matrices are square.
A real unitary matrix O is orthogonal and obeys the rule
OTO = OOT = I.
(1.36)
Orthogonal matrices are square.
An N √ó N hermitian matrix A is nonnegative
A ‚â•0
(1.37)
if for all complex vectors V the quadratic form
‚ü®V|A|V‚ü©=
N

i=1
N

j=1
V‚àó
i AijVj ‚â•0
(1.38)
5

LINEAR ALGEBRA
is nonnegative. It is positive or positive deÔ¨Ånite if
‚ü®V|A|V‚ü©> 0
(1.39)
for all nonzero vectors |V‚ü©.
Example 1.2 (Kinds of positivity)
The nonsymmetric, nonhermitian 2 √ó 2
matrix
 1
1
‚àí1
1

(1.40)
is positive on the space of all real 2-vectors but not on the space of all complex
2-vectors.
Example 1.3 (Representations of imaginary and Grassmann numbers)
The
2 √ó 2 matrix
0
‚àí1
1
0

(1.41)
can represent the number i since
0
‚àí1
1
0
 0
‚àí1
1
0

=
‚àí1
0
0
‚àí1

= ‚àíI.
(1.42)
The 2 √ó 2 matrix
0
0
1
0

(1.43)
can represent a Grassmann number since
0
0
1
0
 0
0
1
0

=
0
0
0
0

= 0.
(1.44)
To represent two Grassmann numbers, one needs 4 √ó 4 matrices, such as
Œ∏1 =
‚éõ
‚éú‚éú‚éù
0
0
1
0
0
0
0
‚àí1
0
0
0
0
0
0
0
0
‚éû
‚éü‚éü‚é†
and
Œ∏2 =
‚éõ
‚éú‚éú‚éù
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
‚éû
‚éü‚éü‚é†.
(1.45)
The matrices that represent n Grassmann numbers are 2n √ó 2n.
Example 1.4 (Fermions)
The matrices (1.45) also can represent lowering or
annihilation operators for a system of two fermionic states. For a1
= Œ∏1
and a2 = Œ∏2 and their adjoints a‚Ä†
1 and a‚Ä†
2, the creation operators satisfy the
anticommutation relations
{ai, a‚Ä†
k} = Œ¥ik
and
{ai, ak} = {a‚Ä†
i , a‚Ä†
k} = 0
(1.46)
6

1.4 VECTORS
where i and k take the values 1 or 2. In particular, the relation (a‚Ä†
i )2 = 0 imple-
ments Pauli‚Äôs exclusion principle, the rule that no state of a fermion can be doubly
occupied.
1.4 Vectors
Vectors are things that can be multiplied by numbers and added together to
form other vectors in the same vector space. So if U and V are vectors in a
vector space S over a set F of numbers x and y and so forth, then
W = x U + y V
(1.47)
also is a vector in the vector space S.
A basis for a vector space S is a set of vectors Bk for k = 1, . . . , N in terms
of which every vector U in S can be expressed as a linear combination
U = u1B1 + u2B2 + ¬∑ ¬∑ ¬∑ + uNBN
(1.48)
with numbers uk in F. The numbers uk are the components of the vector U in
the basis Bk.
Example 1.5 (Hardware store)
Suppose the vector W represents a certain kind
of washer and the vector N represents a certain kind of nail. Then if n and m are
natural numbers, the vector
H = nW + mN
(1.49)
would represent a possible inventory of a very simple hardware store. The vector
space of all such vectors H would include all possible inventories of the store.
That space is a two-dimensional vector space over the natural numbers, and the
two vectors W and N form a basis for it.
Example 1.6 (Complex numbers)
The complex numbers are a vector space.
Two of its vectors are the number 1 and the number i; the vector space of
complex numbers is then the set of all linear combinations
z = x1 + yi = x + iy.
(1.50)
So the complex numbers are a two-dimensional vector space over the real
numbers, and the vectors 1 and i are a basis for it.
The complex numbers also form a one-dimensional vector space over the
complex numbers. Here any nonzero real or complex number, for instance the
number 1, can be a basis consisting of the single vector 1. This one-dimensional
vector space is the set of all z = z1 for arbitrary complex z.
7

LINEAR ALGEBRA
Example 1.7 (2-space)
Ordinary Ô¨Çat two-dimensional space is the set of all
linear combinations
r = xÀÜx + yÀÜy
(1.51)
in which x and y are real numbers and ÀÜx and ÀÜy are perpendicular vectors of unit
length (unit vectors). This vector space, called R2, is a 2-d space over the reals.
Note that the same vector r can be described either by the basis vectors ÀÜx and
ÀÜy or by any other set of basis vectors, such as ‚àíÀÜy and ÀÜx
r = xÀÜx + yÀÜy = ‚àíy(‚àíÀÜy) + xÀÜx.
(1.52)
So the components of the vector r are (x, y) in the
ÀÜx, ÀÜy

basis and (‚àíy, x) in the

‚àíÀÜy, ÀÜx

basis. Each vector is unique, but its components depend upon the basis.
Example 1.8 (3-space)
Ordinary Ô¨Çat three-dimensional space is the set of all
linear combinations
r = xÀÜx + yÀÜy + zÀÜz
(1.53)
in which x, y, and z are real numbers. It is a 3-d space over the reals.
Example 1.9 (Matrices)
Arrays of a given dimension and size can be added
and multiplied by numbers, and so they form a vector space. For instance, all
complex three-dimensional arrays aijk in which 1 ‚â§i ‚â§3, 1 ‚â§j ‚â§4, and
1 ‚â§k ‚â§5 form a vector space over the complex numbers.
Example 1.10 (Partial derivatives)
Derivatives are vectors, so are partial deriva-
tives. For instance, the linear combinations of x and y partial derivatives taken
at x = y = 0
a ‚àÇ
‚àÇx + b ‚àÇ
‚àÇy
(1.54)
form a vector space.
Example 1.11 (Functions)
The space of all linear combinations of a set of
functions fi(x) deÔ¨Åned on an interval [a, b]
f (x) =

i
zi fi(x)
(1.55)
is a vector space over the natural, real, or complex numbers {zi}.
Example 1.12 (States)
In quantum mechanics, a state is represented by a vec-
tor, often written as œà or in Dirac‚Äôs notation as |œà‚ü©. If c1 and c2 are complex
numbers, and |œà1‚ü©and |œà2‚ü©are any two states, then the linear combination
|œà‚ü©= c1|œà1‚ü©+ c2|œà2‚ü©
(1.56)
also is a possible state of the system.
8

1.5 LINEAR OPERATORS
1.5 Linear operators
A linear operator A maps each vector U in its domain into a vector U‚Ä≤ = A(U) ‚â°
AU in its range in a way that is linear. So if U and V are two vectors in its
domain and b and c are numbers, then
A(bU + cV) = bA(U) + cA(V) = bAU + cAV.
(1.57)
If the domain and the range are the same vector space S, then A maps each
basis vector Bi of S into a linear combination of the basis vectors Bk
ABi = a1iB1 + a2iB2 + ¬∑ ¬∑ ¬∑ + aNiBN =
N

k=1
aki Bk.
(1.58)
The square matrix aki represents the linear operator A in the Bk basis. The effect
of A on any vector U = u1B1 + u2B2 + ¬∑ ¬∑ ¬∑ + uNBN in S then is
AU = A
 N

i=1
uiBi

=
N

i=1
uiABi =
N

i=1
ui
N

k=1
aki Bk
=
N

k=1
 N

i=1
akiui

Bk.
(1.59)
So the kth component u‚Ä≤
k of the vector U‚Ä≤ = AU is
u‚Ä≤
k = ak1u1 + ak2u2 + ¬∑ ¬∑ ¬∑ + akNuN =
N

i=1
aki ui.
(1.60)
Thus the column vector u‚Ä≤ of the components u‚Ä≤
k of the vector U‚Ä≤ = AU is
the product u‚Ä≤ = a u of the matrix with elements aki that represents the linear
operator A in the Bk basis and the column vector with components ui that rep-
resents the vector U in that basis. So in each basis, vectors and linear operators
are represented by column vectors and matrices.
Each linear operator is unique, but its matrix depends upon the basis. If we
change from the Bk basis to another basis B‚Ä≤
k
Bk =
N

‚Ñì=1
u‚Ñìk B‚Ä≤
‚Ñì
(1.61)
in which the N √ó N matrix u‚Ñìk has an inverse matrix u‚àí1
ki so that
N

k=1
u‚àí1
ki Bk =
N

k=1
u‚àí1
ki
N

‚Ñì=1
u‚ÑìkB‚Ä≤
‚Ñì=
N

‚Ñì=1
 N

k=1
u‚Ñìku‚àí1
ki

B‚Ä≤
‚Ñì=
N

‚Ñì=1
Œ¥‚ÑìiB‚Ä≤
‚Ñì= B‚Ä≤
i,
(1.62)
9

LINEAR ALGEBRA
then the new basis vectors B‚Ä≤
i are given by
B‚Ä≤
i =
N

k=1
u‚àí1
ki Bk.
(1.63)
Thus (exercise 1.9) the linear operator A maps the basis vector B‚Ä≤
i to
AB‚Ä≤
i =
N

k=1
u‚àí1
ki ABk =
N

j,k=1
u‚àí1
ki ajkBj =
N

j,k,‚Ñì=1
u‚Ñìjajku‚àí1
ki B‚Ä≤
‚Ñì.
(1.64)
So the matrix a‚Ä≤ that represents A in the B‚Ä≤ basis is related to the matrix a that
represents it in the B basis by a similarity transformation
a‚Ä≤
‚Ñìi =
N

jk=1
u‚Ñìjajku‚àí1
ki
or
a‚Ä≤ = u a u‚àí1
(1.65)
in matrix notation.
Example 1.13 (Change of basis)
Let the action of the linear operator A on the
basis vectors {B1, B2} be AB1 = B2 and AB2 = 0. If the column vectors
b1 =
1
0

and
b2 =
0
1

(1.66)
represent the basis vectors B1 and B2, then the matrix
a =
0
0
1
0

(1.67)
represents the linear operator A. But if we use the basis vectors
B‚Ä≤
1 =
1
‚àö
2
(B1 + B2)
and
B‚Ä≤
2 =
1
‚àö
2
(B1 ‚àíB2)
(1.68)
then the vectors
b‚Ä≤
1 =
1
‚àö
2
1
1

and
b‚Ä≤
2 =
1
‚àö
2
 1
‚àí1

(1.69)
would represent B1 and B2, and the matrix
a‚Ä≤ = 1
2
 1
1
‚àí1
‚àí1

(1.70)
would represent the linear operator A (exercise 1.10).
A linear operator A also may map a vector space S with basis Bk into a
different vector space T with its own basis Ck. In this case, A maps the basis
vector Bi into a linear combination of the basis vectors Ck
10

1.6 INNER PRODUCTS
ABi =
M

k=1
aki Ck
(1.71)
and an arbitrary vector U = u1B1 + ¬∑ ¬∑ ¬∑ + uNBN in S into the vector
AU =
M

k=1
 N

i=1
aki ui

Ck
(1.72)
in T.
1.6 Inner products
Most of the vector spaces used by physicists have an inner product. A positive-
deÔ¨Ånite inner product associates a number ( f , g) with every ordered pair of
vectors f and g in the vector space V and satisÔ¨Åes the rules
( f , g) = (g, f )‚àó
(1.73)
( f , z g + w h) = z (f , g) + w (f , h)
(1.74)
(f , f ) ‚â•0
and
(f , f ) = 0 ‚áê‚áíf = 0
(1.75)
in which f , g, and h are vectors, and z and w are numbers. The Ô¨Årst rule says that
the inner product is hermitian; the second rule says that it is linear in the second
vector z g + w h of the pair; and the third rule says that it is positive deÔ¨Ånite. The
Ô¨Årst two rules imply that (exercise 1.11) the inner product is antilinear in the
Ô¨Årst vector of the pair
(z g + w h, f ) = z‚àó(g, f ) + w‚àó(h, f ).
(1.76)
A Schwarz inner product satisÔ¨Åes the Ô¨Årst two rules (1.73, 1.74) for an inner
product and the fourth (1.76) but only the Ô¨Årst part of the third (1.75)
( f , f ) ‚â•0.
(1.77)
This condition of nonnegativity implies (exercise 1.15) that a vector f of zero
length must be orthogonal to all vectors g in the vector space V
( f , f ) = 0 ‚áí(g, f ) = 0 for all g ‚ààV.
(1.78)
So a Schwarz inner product is almost positive deÔ¨Ånite.
Inner products of 4-vectors can be negative. To accommodate them we deÔ¨Åne
an indeÔ¨Ånite inner product without regard to positivity as one that satisÔ¨Åes the
Ô¨Årst two rules (1.73 & 1.74) and therefore also the fourth rule (1.76) and that
instead of being positive deÔ¨Ånite is nondegenerate
( f , g) = 0 for all f ‚ààV ‚áíg = 0.
(1.79)
11

LINEAR ALGEBRA
This rule says that only the zero vector is orthogonal to all the vectors of
the space. The positive-deÔ¨Ånite condition (1.75) is stronger than and implies
nondegeneracy (1.79) (exercise 1.14).
Apart from the indeÔ¨Ånite inner products of 4-vectors in special and general
relativity, most of the inner products physicists use are Schwarz inner products
or positive-deÔ¨Ånite inner products. For such inner products, we can deÔ¨Åne the
norm | f | = ‚à•f ‚à•of a vector f as the square-root of the nonnegative inner
product ( f , f )
‚à•f ‚à•=

(f , f ).
(1.80)
The distance between two vectors f and g is the norm of their difference
‚à•f ‚àíg ‚à•.
(1.81)
Example 1.14 (Euclidean space)
The space of real vectors U, V with N com-
ponents Ui, Vi forms an N-dimensional vector space over the real numbers with
an inner product
(U, V) =
N

i=1
Ui Vi
(1.82)
that is nonnegative when the two vectors are the same
(U, U) =
N

i=1
Ui Ui =
N

i=1
U2
i ‚â•0
(1.83)
and vanishes only if all the components Ui are zero, that is, if the vector U = 0.
Thus the inner product (1.82) is positive deÔ¨Ånite. When (U, V) is zero, the vectors
U and V are orthogonal.
Example 1.15 (Complex euclidean space)
The space of complex vectors with
N components Ui, Vi forms an N-dimensional vector space over the complex
numbers with inner product
(U, V) =
N

i=1
U‚àó
i Vi = (V, U)‚àó.
(1.84)
The inner product (U, U) is nonnegative and vanishes
(U, U) =
N

i=1
U‚àó
i Ui =
N

i=1
|Ui|2 ‚â•0
(1.85)
only if U = 0. So the inner product (1.84) is positive deÔ¨Ånite. If (U, V) is zero,
then U and V are orthogonal.
12

1.6 INNER PRODUCTS
Example 1.16 (Complex matrices)
For the vector space of N√óL complex matri-
ces A, B, . . ., the trace of the adjoint (1.28) of A multiplied by B is an inner
product
(A, B) = TrA‚Ä†B =
N

i=1
L

j=1
(A‚Ä†)jiBij =
N

i=1
L

j=1
A‚àó
ijBij
(1.86)
that is nonnegative when the matrices are the same
(A, A) = TrA‚Ä†A =
N

i=1
L

j=1
A‚àó
ijAij =
N

i=1
L

j=1
|Aij|2 ‚â•0
(1.87)
and zero only when A = 0. So this inner product is positive deÔ¨Ånite.
A vector space with a positive-deÔ¨Ånite inner product (1.73‚Äì1.77) is called an
inner-product space, a metric space, or a pre-Hilbert space.
A sequence of vectors fn is a Cauchy sequence if for every œµ > 0 there is
an integer N(œµ) such that ‚à•fn ‚àífm‚à•< œµ whenever both n and m exceed N(œµ).
A sequence of vectors fn converges to a vector f if for every œµ > 0 there is
an integer N(œµ) such that ‚à•f ‚àífn‚à•< œµ whenever n exceeds N(œµ). An inner-
product space with a norm deÔ¨Åned as in (1.80) is complete if each of its Cauchy
sequences converges to a vector in that space. A Hilbert space is a complete
inner-product space. Every Ô¨Ånite-dimensional inner-product space is complete
and so is a Hilbert space. But the term Hilbert space more often is used to
describe inÔ¨Ånite-dimensional complete inner-product spaces, such as the space
of all square-integrable functions (David Hilbert, 1862‚Äì1943).
Example 1.17 (The Hilbert space of square-integrable functions)
For the vector
space of functions (1.55), a natural inner product is
( f , g) =
 b
a
dx f ‚àó(x)g(x).
(1.88)
The squared norm ‚à•f ‚à•of a function f (x) is
‚à•f ‚à•2=
 b
a
dx | f (x)|2.
(1.89)
A function is square integrable if its norm is Ô¨Ånite. The space of all square-
integrable functions is an inner-product space; it also is complete and so is a
Hilbert space.
Example 1.18 (Minkowski inner product)
The Minkowski or Lorentz inner
product (p, x) of two 4-vectors p = (E/c, p1, p2, p3) and x = (ct, x1, x2, x3) is
13

LINEAR ALGEBRA
p ¬∑ x ‚àíEt . It is indeÔ¨Ånite, nondegenerate, and invariant under Lorentz transfor-
mations, and often is written as p ¬∑ x or as px. If p is the 4-momentum of a freely
moving physical particle of mass m, then
p ¬∑ p = p ¬∑ p ‚àíE2/c2 = ‚àíc2m2 ‚â§0.
(1.90)
The Minkowski inner product satisÔ¨Åes the rules (1.73, 1.75, and 1.79), but it is
not positive deÔ¨Ånite, and it does not satisfy the Schwarz inequality (Hermann
Minkowski, 1864‚Äì1909; Hendrik Lorentz, 1853‚Äì1928).
1.7 The Cauchy‚ÄìSchwarz inequality
For any two vectors f and g, the Schwarz inequality
( f , f ) (g, g) ‚â•|( f , g)|2
(1.91)
holds for any Schwarz inner product (and so for any positive-deÔ¨Ånite inner
product). The condition (1.77) of nonnegativity ensures that for any complex
number Œª the inner product of the vector f ‚àíŒªg with itself is nonnegative
(f ‚àíŒªg, f ‚àíŒªg) = ( f , f ) ‚àíŒª‚àó(g, f ) ‚àíŒª( f , g) + |Œª|2(g, g) ‚â•0.
(1.92)
Now if (g, g) = 0, then for (f ‚àíŒªg, f ‚àíŒªg) to remain nonnegative for all com-
plex values of Œª it is necessary that ( f , g) = 0 also vanish (exercise 1.15). Thus
if (g, g) = 0, then the Schwarz inequality (1.91) is trivially true because both
sides of it vanish. So we assume that (g, g) > 0 and set Œª = (g, f )/(g, g). The
inequality (1.92) then gives us
( f ‚àíŒªg, f ‚àíŒªg) =

f ‚àí(g, f )
(g, g) g, f ‚àí(g, f )
(g, g) g

= ( f , f ) ‚àí( f , g)(g, f )
(g, g)
‚â•0
which is the Schwarz inequality (1.91) (Hermann Schwarz, 1843‚Äì1921)
( f , f )(g, g) ‚â•|( f , g)|2.
(1.93)
Taking the square-root of each side, we get
‚à•f ‚à•‚à•g ‚à•‚â•|( f , g)|.
(1.94)
Example 1.19 (Some Schwarz inequalities)
For the dot-product of two real
3-vectors r and R, the Cauchy‚ÄìSchwarz inequality is
(r ¬∑ r) (R ¬∑ R) ‚â•(r ¬∑ R)2 = (r ¬∑ r) (R ¬∑ R) cos2 Œ∏
(1.95)
where Œ∏ is the angle between r and R.
The Schwarz inequality for two real n-vectors x is
(x ¬∑ x) (y ¬∑ y) ‚â•(x ¬∑ y)2 = (x ¬∑ x) (y ¬∑ y) cos2 Œ∏
(1.96)
14

1.8 LINEAR INDEPENDENCE AND COMPLETENESS
and it implies (Exercise 1.16) that
‚à•x‚à•+ ‚à•y‚à•‚â•‚à•x + y‚à•.
(1.97)
For two complex n-vectors u and v, the Schwarz inequality is

u‚àó¬∑ u
 
v‚àó¬∑ v

‚â•
u‚àó¬∑ v
2 =

u‚àó¬∑ u
 
v‚àó¬∑ v

cos2 Œ∏
(1.98)
and it implies (exercise 1.17) that
‚à•u‚à•+ ‚à•v‚à•‚â•‚à•u + v‚à•.
(1.99)
The inner product (1.88) of two complex functions f and g provides a
somewhat different instance
 b
a
dx |f (x)|2
 b
a
dx |g(x)|2 ‚â•

 b
a
dx f ‚àó(x)g(x)

2
(1.100)
of the Schwarz inequality.
1.8 Linear independence and completeness
A set of N vectors V1, V2, ..., VN is linearly dependent if there exist numbers
ci, not all zero, such that the linear combination
c1V1 + ¬∑ ¬∑ ¬∑ + cNVN = 0
(1.101)
vanishes. A set of vectors is linearly independent if it is not linearly dependent.
A set {Vi} of linearly independent vectors is maximal in a vector space S if
the addition of any other vector U in S to the set {Vi} makes the enlarged set
{U, Vi} linearly dependent.
A set of N linearly independent vectors V1, V2, ..., VN that is maximal in a
vector space S can represent any vector U in the space S as a linear combination
of its vectors, U = u1V1 + ¬∑ ¬∑ ¬∑ + uNVN. For if we enlarge the maximal set {Vi}
by including in it any vector U not already in it, then the bigger set {U, Vi} will
be linearly dependent. Thus there will be numbers c, c1, ..., cN, not all zero,
that make the sum
c U + c1V1 + ¬∑ ¬∑ ¬∑ + cNVN = 0
(1.102)
vanish. Now if c were 0, then the set {Vi} would be linearly dependent. Thus
c Ã∏= 0, and so we may divide by c and express the arbitrary vector U as a linear
combination of the vectors Vi
U = ‚àí1
c (c1V1 + ¬∑ ¬∑ ¬∑ + cNVN) = u1V1 + ¬∑ ¬∑ ¬∑ + uNVN
(1.103)
with uk = ‚àíck/c. So a set of linearly independent vectors {Vi} that is maxi-
mal in a space S can represent every vector U in S as a linear combination
15

LINEAR ALGEBRA
U = u1V1 + . . . + uNVN of its vectors. The set {Vi} spans the space S; it is a
complete set of vectors in the space S.
A set of vectors {Vi} that spans a vector space S provides a basis for that space
because the set lets us represent an arbitrary vector U in S as a linear combi-
nation of the basis vectors {Vi}. If the vectors of a basis are linearly dependent,
then at least one of them is superÔ¨Çuous, and so it is convenient to have the
vectors of a basis be linearly independent.
1.9 Dimension of a vector space
If V1, ..., VN and W1, ..., WM are two maximal sets of N and M linearly
independent vectors in a vector space S, then N = M.
Suppose M < N. Since the Us are complete, they span S, and so we may
express each of the N vectors Vi in terms of the M vectors Wj
Vi =
M

j=1
AijWj.
(1.104)
Let Aj be the vector with components Aij. There are M < N such vec-
tors, and each has N > M components. So it is always possible to Ô¨Ånd a
nonzero N-dimensional vector C with components ci that is orthogonal to all
M vectors Aj
N

i=1
ciAij = 0.
(1.105)
Thus the linear combination
N

i=1
ciVi =
N

i=1
M

j=1
ci Aij Wj = 0
(1.106)
vanishes, which implies that the N vectors Vi are linearly dependent. Since these
vectors are by assumption linearly independent, it follows that N ‚â§M.
Similarly, one may show that M ‚â§N. Thus M = N.
The number of vectors in a maximal set of linearly independent vectors in a
vector space S is the dimension of the vector space. Any N linearly independent
vectors in an N-dimensional space form a basis for it.
1.10 Orthonormal vectors
Suppose the vectors V1, V2, ..., VN are linearly independent. Then we can
make out of them a set of N vectors Ui that are orthonormal
(Ui, Uj) = Œ¥ij.
(1.107)
16

1.10 ORTHONORMAL VECTORS
There are many ways to do this, because there are many such sets of orthonor-
mal vectors. We will use the Gram‚ÄìSchmidt method. We set
U1 =
V1

(V1, V1)
,
(1.108)
so the Ô¨Årst vector U1 is normalized. Next we set u2 = V2 + c12U1 and require
that u2 be orthogonal to U1
0 = (U1, u2) = (U1, c12U1 + V2) = c12 + (U1, V2).
(1.109)
Thus c12 = ‚àí(U1, V2), and so
u2 = V2 ‚àí(U1, V2) U1.
(1.110)
The normalized vector U2 then is
U2 =
u2

(u2, u2)
.
(1.111)
We next set u3 = V3 + c13U1 + c23U2 and ask that u3 be orthogonal to U1
0 = (U1, u3) = (U1, c13U1 + c23U2 + V3) = c13 + (U1, V3)
(1.112)
and also to U2
0 = (U2, u3) = (U2, c13U1 + c23U2 + V3) = c23 + (U2, V3).
(1.113)
So c13 = ‚àí(U1, V3) and c23 = ‚àí(U2, V3), and we have
u3 = V3 ‚àí(U1, V3) U1 ‚àí(U2, V3) U2.
(1.114)
The normalized vector U3 then is
U3 =
u3

(u3, u3)
.
(1.115)
We may continue in this way until we reach the last of the N linearly
independent vectors. We require the kth unnormalized vector uk
uk = Vk +
k‚àí1

i=1
cik Ui
(1.116)
to be orthogonal to the k ‚àí1 vectors Ui and Ô¨Ånd that cik = ‚àí(Ui, Vk) so that
uk = Vk ‚àí
k‚àí1

i=1
(Ui, Vk) Ui.
(1.117)
The normalized vector then is
Uk =
uk

(uk, uk)
.
(1.118)
A basis is more convenient if its vectors are orthonormal.
17

LINEAR ALGEBRA
1.11 Outer products
From any two vectors f and g, we may make an operator A that takes any vector
h into the vector f with coefÔ¨Åcient (g, h)
A h = f (g, h).
(1.119)
Since for any vectors e, h and numbers z, w
A (z h + w e) = f (g, z h + w e) = zf (g, h) + wf (g, e) = z A h + w A e
(1.120)
it follows that A is linear.
If in some basis f , g, and h are vectors with components fi, gi, and hi, then
the linear transformation is
(Ah)i =
N

j=1
Aij hj = fi
N

j=1
g‚àó
j hj
(1.121)
and in that basis A is the matrix with entries
Aij = fi g‚àó
j .
(1.122)
It is the outer product of the vectors f and g.
Example 1.20 (Outer product)
If in some basis the vectors f and g are
f =
2
3

and
g =
‚éõ
‚éù
i
1
3i
‚éû
‚é†
(1.123)
then their outer product is the matrix
A =
2
3
 ‚àíi
1
‚àí3i
=
‚àí2i
2
‚àí6i
‚àí3i
3
‚àí9i

.
(1.124)
Dirac developed a notation that handles outer products very easily.
Example 1.21 (Outer products)
If the vectors f = |f ‚ü©and g = |g‚ü©are
|f ‚ü©=
‚éõ
‚éù
a
b
c
‚éû
‚é†
and
|g‚ü©=
 z
w

(1.125)
then their outer products are
|f ‚ü©‚ü®g| =
‚éõ
‚éù
az‚àó
aw‚àó
bz‚àó
bw‚àó
cz‚àó
cw‚àó
‚éû
‚é†
and
|g‚ü©‚ü®f | =
za‚àó
zb‚àó
zc‚àó
wa‚àó
wb‚àó
wc‚àó

(1.126)
18

1.12 DIRAC NOTATION
as well as
|f ‚ü©‚ü®f | =
‚éõ
‚éù
aa‚àó
ab‚àó
ac‚àó
ba‚àó
bb‚àó
bc‚àó
ca‚àó
cb‚àó
cc‚àó
‚éû
‚é†
and
|g‚ü©‚ü®g| =
zz‚àó
zw‚àó
wz‚àó
ww‚àó

.
(1.127)
Students should feel free to write down their own examples.
1.12 Dirac notation
Outer products are important in quantum mechanics, and so Dirac invented
a notation for linear algebra that makes them easy to write. In his notation, a
vector f is a ket f = |f ‚ü©. The new thing in his notation is the bra ‚ü®g|. The inner
product of two vectors (g, f ) is the bracket (g, f ) = ‚ü®g|f ‚ü©. A matrix element
(g, Af ) is then (g, Af ) = ‚ü®g|A|f ‚ü©in which the bra and ket bracket the operator.
In Dirac notation, the outer product A h = f (g, h) reads A |h‚ü©= |f ‚ü©‚ü®g|h‚ü©, so
that the outer product A itself is A = |f ‚ü©‚ü®g|. Before Dirac, bras were implicit in
the deÔ¨Ånition of the inner product, but they did not appear explicitly; there was
no way to write the bra ‚ü®g| or the operator |f ‚ü©‚ü®g|.
If the kets |n‚ü©form an orthonormal basis in an N-dimensional vector space,
then we can expand an arbitrary ket in the space as
|f ‚ü©=
N

n=1
cn|n‚ü©.
(1.128)
Since the basis vectors are orthonormal ‚ü®‚Ñì|n‚ü©= Œ¥‚Ñìn, we can identify the
coefÔ¨Åcients cn by forming the inner product
‚ü®‚Ñì|f ‚ü©=
N

n=1
cn ‚ü®‚Ñì|n‚ü©=
N

n=1
cn Œ¥‚Ñì,n = c‚Ñì.
(1.129)
The original expansion (1.128) then must be
|f ‚ü©=
N

n=1
cn|n‚ü©=
N

n=1
‚ü®n|f ‚ü©|n‚ü©=
N

n=1
|n‚ü©‚ü®n|f ‚ü©=
 N

n=1
|n‚ü©‚ü®n|

|f ‚ü©.
(1.130)
Since this equation must hold for every vector |f ‚ü©in the space, it follows that
the sum of outer products within the parentheses is the identity operator for the
space
I =
N

n=1
|n‚ü©‚ü®n|.
(1.131)
19

LINEAR ALGEBRA
Every set of kets |Œ±n‚ü©that forms an orthonormal basis ‚ü®Œ±n|Œ±‚Ñì‚ü©= Œ¥n‚Ñìfor the
space gives us an equivalent representation of the identity operator
I =
N

n=1
|Œ±n‚ü©‚ü®Œ±n| =
N

n=1
|n‚ü©‚ü®n|.
(1.132)
Before Dirac, one could not write such equations. They provide for every vector
|f ‚ü©in the space the expansions
|f ‚ü©=
N

n=1
|Œ±n‚ü©‚ü®Œ±n|f ‚ü©=
N

n=1
|n‚ü©‚ü®n|f ‚ü©.
(1.133)
Example 1.22 (Inner-product rules)
In Dirac‚Äôs notation, the rules (1.73‚Äì1.76)
of a positive-deÔ¨Ånite inner product are
‚ü®f |g‚ü©= ‚ü®g|f ‚ü©‚àó
(1.134)
‚ü®f |z1g1 + z2g2‚ü©= z1‚ü®f |g1‚ü©+ z2‚ü®f |g2‚ü©
(1.135)
‚ü®z1f1 + z2f2|g‚ü©= z‚àó
1‚ü®f1|g‚ü©+ z‚àó
2‚ü®f2|g‚ü©
(1.136)
‚ü®f |f ‚ü©‚â•0
and
‚ü®f |f ‚ü©= 0
‚áê‚áí
f = 0.
(1.137)
Usually states in Dirac notation are labeled |œà‚ü©or by their quantum numbers
|n, l, m‚ü©, and one rarely sees plus signs or complex numbers or operators inside
bras or kets. But one should.
Example 1.23 (Gram‚ÄìSchmidt)
In Dirac notation, the formula (1.117) for the
kth orthogonal linear combination of the vectors |V‚Ñì‚ü©is
|uk‚ü©= |Vk‚ü©‚àí
k‚àí1

i=1
|Ui‚ü©‚ü®Ui|Vk‚ü©=

I ‚àí
k‚àí1

i=1
|Ui‚ü©‚ü®Ui|

|Vk‚ü©
(1.138)
and the formula (1.118) for the kth orthonormal linear combination of the
vectors |V‚Ñì‚ü©is
|Uk‚ü©=
|uk‚ü©
‚àö‚ü®uk|uk‚ü©.
(1.139)
The vectors |Uk‚ü©are not unique; they vary with the order of the |Vk‚ü©.
Vectors and linear operators are abstract. The numbers we compute with are
inner products like ‚ü®g|f ‚ü©and ‚ü®g|A|f ‚ü©. In terms of N orthonormal basis vectors
|n‚ü©with fn = ‚ü®n|f ‚ü©and g‚àó
n = ‚ü®g|n‚ü©, we can use the expansion (1.131) to write
these inner products as
20

1.12 DIRAC NOTATION
‚ü®g|f ‚ü©= ‚ü®g|I|f ‚ü©=
N

n=1
‚ü®g|n‚ü©‚ü®n|f ‚ü©=
N

n=1
g‚àó
nfn,
‚ü®g|A|f ‚ü©= ‚ü®g|IAI|f ‚ü©=
N

n,‚Ñì=1
‚ü®g|n‚ü©‚ü®n|A|‚Ñì‚ü©‚ü®‚Ñì|f ‚ü©=
N

n,‚Ñì=1
g‚àó
n An‚Ñìf‚Ñì
(1.140)
in which An‚Ñì= ‚ü®n|A|‚Ñì‚ü©. We often gather the inner products f‚Ñì= ‚ü®‚Ñì|f ‚ü©into a
column vector f with components f‚Ñì= ‚ü®‚Ñì|f ‚ü©
f =
‚éõ
‚éú‚éú‚éú‚éù
‚ü®1|f ‚ü©
‚ü®2|f ‚ü©
...
‚ü®N|f ‚ü©
‚éû
‚éü‚éü‚éü‚é†=
‚éõ
‚éú‚éú‚éú‚éù
f1
f2
...
f3
‚éû
‚éü‚éü‚éü‚é†
(1.141)
and the ‚ü®n|A|‚Ñì‚ü©into a matrix A with matrix elements An‚Ñì= ‚ü®n|A|‚Ñì‚ü©. If we also
line up the inner products ‚ü®g|n‚ü©= ‚ü®g|n‚ü©‚àóin a row vector that is the transpose of
the complex conjugate of the column vector g
g‚Ä† =

‚ü®1|g‚ü©‚àó, ‚ü®2|g‚ü©‚àó, . . . , ‚ü®N|g‚ü©‚àó
=

g‚àó
1, g‚àó
2, . . . , g‚àó
N

(1.142)
then we can write inner products in matrix notation as ‚ü®g|f ‚ü©= g‚Ä†f and as
‚ü®g|A|f ‚ü©= g‚Ä†Af .
If we switch to a different basis, say from |n‚ü©s to |Œ±n‚ü©s, then the components
of the column vectors change from fn = ‚ü®n|f ‚ü©to f ‚Ä≤
n = ‚ü®Œ±n|f ‚ü©, and similarly
those of the row vectors g‚Ä† and of the matrix A change, but the bras, the kets,
the linear operators, and the inner products ‚ü®g|f ‚ü©and ‚ü®g|A|f ‚ü©do not change
because the identity operator is basis independent (1.132)
‚ü®g|f ‚ü©=
N

n=1
‚ü®g|n‚ü©‚ü®n|f ‚ü©=
N

n=1
‚ü®g|Œ±n‚ü©‚ü®Œ±n|f ‚ü©,
‚ü®g|A|f ‚ü©=
N

n,‚Ñì=1
‚ü®g|n‚ü©‚ü®n|A|‚Ñì‚ü©‚ü®‚Ñì|f ‚ü©=
N

n,‚Ñì=1
‚ü®g|Œ±n‚ü©‚ü®Œ±n|A|Œ±‚Ñì‚ü©‚ü®Œ±‚Ñì|f ‚ü©. (1.143)
Dirac‚Äôs outer products show how to change from one basis to another. The
sum of outer products
U =
N

n=1
|Œ±n‚ü©‚ü®n|
(1.144)
maps the ket |‚Ñì‚ü©of the orthonormal basis we started with into |Œ±‚Ñì‚ü©
U|‚Ñì‚ü©=
N

n=1
|Œ±n‚ü©‚ü®n|‚Ñì‚ü©=
N

n=1
|Œ±n‚ü©Œ¥n‚Ñì= |Œ±‚Ñì‚ü©.
(1.145)
21

LINEAR ALGEBRA
Example 1.24 (A simple change of basis)
If the ket |Œ±n‚ü©of the new basis is
simply |Œ±n‚ü©= |n + 1‚ü©with |Œ±N‚ü©= |N + 1‚ü©‚â°|1‚ü©then the operator that maps the
N kets |n‚ü©into the kets |Œ±n‚ü©is
U =
N

n=1
|Œ±n‚ü©‚ü®n| =
N

n=1
|n + 1‚ü©‚ü®n|.
(1.146)
The square U2 of U also changes the basis; it sends |n‚ü©to |n + 2‚ü©. The set of
operators Uk for k = 1, 2, . . . , N forms a group known as ZN.
1.13 The adjoint of an operator
In Dirac‚Äôs notation, the most general linear operator on an N-dimensional vec-
tor space is a sum of dyadics like z |n‚ü©‚ü®‚Ñì| in which z is a complex number and
the kets |n‚ü©and |‚Ñì‚ü©are two of the N orthonormal kets that make up a basis for
the space. The adjoint of this basic linear operator is
(z |n‚ü©‚ü®‚Ñì|)‚Ä† = z‚àó|‚Ñì‚ü©‚ü®n|.
(1.147)
Thus with z = ‚ü®n|A|‚Ñì‚ü©, the most general linear operator on the space is
A = IAI =
N

n,‚Ñì=1
|n‚ü©‚ü®n|A|‚Ñì‚ü©‚ü®‚Ñì|
(1.148)
and its adjoint A‚Ä† is the operator IA‚Ä†I
A‚Ä† =
N

n,‚Ñì=1
|n‚ü©‚ü®n|A‚Ä†|‚Ñì‚ü©‚ü®‚Ñì| =
N

n,‚Ñì=1
|‚Ñì‚ü©‚ü®n|A|‚Ñì‚ü©‚àó‚ü®n| =
N

n,‚Ñì=1
|n‚ü©‚ü®‚Ñì|A|n‚ü©‚àó‚ü®‚Ñì|.
It follows that ‚ü®n|A‚Ä†|‚Ñì‚ü©= ‚ü®‚Ñì|A|n‚ü©‚àóso that the matrix A‚Ä†
n‚Ñìthat represents A‚Ä† in
this basis is
A‚Ä†
n‚Ñì= ‚ü®n|A‚Ä†|‚Ñì‚ü©= ‚ü®‚Ñì|A|n‚ü©‚àó= A‚àó
‚Ñìn = A‚àóT
n‚Ñì
(1.149)
in agreement with our deÔ¨Ånition (1.28) of the adjoint of a matrix as the
transpose of its complex conjugate, A‚Ä† = A‚àóT. We also have
‚ü®g|A‚Ä†f ‚ü©= ‚ü®g|A‚Ä†|f ‚ü©= ‚ü®f |A|g‚ü©‚àó= ‚ü®f |Ag‚ü©‚àó= ‚ü®Ag|f ‚ü©.
(1.150)
Taking the adjoint of the adjoint is by (1.147)

(z |n‚ü©‚ü®‚Ñì|)‚Ä†‚Ä†
=

z‚àó|‚Ñì‚ü©‚ü®n|
‚Ä† = z |n‚ü©‚ü®‚Ñì|
(1.151)
the same as doing nothing at all. This also follows from the matrix formula
(1.149) because both (A‚àó)‚àó= A and (AT)T = A, and so

A‚Ä†‚Ä†
=

A‚àóT‚àóT = A,
(1.152)
the adjoint of the adjoint of a matrix is the original matrix.
22

1.15 REAL, SYMMETRIC LINEAR OPERATORS
Before Dirac, the adjoint A‚Ä† of a linear operator A was deÔ¨Åned by
(g, A‚Ä†f ) = (A g, f ) = ( f , A g)‚àó.
(1.153)
This deÔ¨Ånition also implies that A‚Ä†‚Ä† = A since
(g, A‚Ä†‚Ä†f ) = (A‚Ä†g, f ) = ( f , A‚Ä†g)‚àó= (Af , g)‚àó= (g, Af ).
(1.154)
We also have (g, Af ) = (g, A‚Ä†‚Ä†f ) = (A‚Ä†g, f ).
1.14 Self-adjoint or hermitian linear operators
An operator A that is equal to its adjoint, A‚Ä† = A, is self adjoint or hermi-
tian. In view of (1.149), the matrix elements of a self-adjoint linear operator A
satisfy ‚ü®n|A‚Ä†|‚Ñì‚ü©= ‚ü®‚Ñì|A|n‚ü©‚àó= ‚ü®n|A|‚Ñì‚ü©in any orthonormal basis. So a matrix
that represents a hermitian operator is equal to the transpose of its complex
conjugate
An‚Ñì= ‚ü®n|A|‚Ñì‚ü©= ‚ü®n|A‚Ä†|‚Ñì‚ü©= ‚ü®‚Ñì|A|n‚ü©‚àó= A‚àóT
n‚Ñì= A‚Ä†
n‚Ñì.
(1.155)
We also have
‚ü®g| A |f ‚ü©= ‚ü®A g|f ‚ü©= ‚ü®f |A g‚ü©‚àó= ‚ü®f | A |g‚ü©‚àó
(1.156)
and in pre-Dirac notation
(g, A f ) = (A g, f ) = ( f , A g)‚àó.
(1.157)
A matrix Aij that is real and symmetric or imaginary and antisymmetric is her-
mitian. But a self-adjoint linear operator A that is represented by a matrix Aij
that is real and symmetric (or imaginary and antisymmetric) in one orthonor-
mal basis will not in general be represented by a matrix that is real and
symmetric (or imaginary and antisymmetric) in a different orthonormal basis,
but it will be represented by a hermitian matrix in every orthonormal basis.
A ket |a‚Ä≤‚ü©is an eigenvector of a linear operator A with eigenvalue a‚Ä≤ if A|a‚Ä≤‚ü©=
a‚Ä≤|a‚Ä≤‚ü©. As we‚Äôll see in section 1.28, hermitian matrices have real eigenvalues and
complete sets of orthonormal eigenvectors. Hermitian operators and matrices
represent physical variables in quantum mechanics.
1.15 Real, symmetric linear operators
In quantum mechanics, we usually consider complex vector spaces, that is,
spaces in which the vectors |f ‚ü©are complex linear combinations
|f ‚ü©=
N

i=1
zi |i‚ü©
(1.158)
of complex orthonormal basis vectors |i‚ü©.
23

LINEAR ALGEBRA
But real vector spaces also are of interest. A real vector space is a vector
space in which the vectors |f ‚ü©are real linear combinations
|f ‚ü©=
N

n=1
xn |n‚ü©
(1.159)
of real orthonormal basis vectors, x‚àó
n = xn and |n‚ü©‚àó= |n‚ü©.
A real linear operator A on a real vector space
A =
N

n,m=1
|n‚ü©‚ü®n|A|m‚ü©‚ü®m| =
N

n,m=1
|n‚ü©Anm‚ü®m|
(1.160)
is represented by a real matrix A‚àó
nm = Anm. A real linear operator A that is self
adjoint on a real vector space satisÔ¨Åes the condition (1.157) of hermiticity but
with the understanding that complex conjugation has no effect
(g, A f ) = (A g, f ) = ( f , A g)‚àó= ( f , A g).
(1.161)
Thus its matrix elements are symmetric, ‚ü®g|A|f ‚ü©= ‚ü®f |A|g‚ü©. Since A is hermitian
as well as real, the matrix Anm that represents it (in a real basis) is real and
hermitian, and so is symmetric Anm = A‚àó
mn = Amn.
1.16 Unitary operators
A unitary operator U is one whose adjoint is its inverse
U U‚Ä† = U‚Ä† U = I.
(1.162)
Any operator that changes from one orthonormal basis |n‚ü©to another |Œ±n‚ü©
U =
N

n=1
|Œ±n‚ü©‚ü®n|
(1.163)
is unitary since
UU‚Ä† =
N

n=1
|Œ±n‚ü©‚ü®n|
N

m=1
|m‚ü©‚ü®Œ±m| =
N

n,m=1
|Œ±n‚ü©‚ü®n|m‚ü©‚ü®Œ±m|
=
N

n,m=1
|Œ±n‚ü©Œ¥n,m‚ü®Œ±m| =
N

n=1
|Œ±n‚ü©‚ü®Œ±n| = I
(1.164)
as well as
U‚Ä†U =
N

m=1
|m‚ü©‚ü®Œ±m|
N

n=1
|Œ±n‚ü©‚ü®n| =
N

n=1
|n‚ü©‚ü®n| = I.
(1.165)
24

1.17 HILBERT SPACE
A unitary operator maps any orthonormal basis |n‚ü©into another orthonormal
basis |Œ±n‚ü©. For if |Œ±n‚ü©= U|n‚ü©, then ‚ü®Œ±n|Œ±m‚ü©= Œ¥n,m (exercise 1.22). If we multiply
the relation |Œ±n‚ü©= U|n‚ü©by the bra ‚ü®n| and then sum over the index n, we get
N

n=1
|Œ±n‚ü©‚ü®n| =
N

n=1
U|n‚ü©‚ü®n| = U
N

n=1
|n‚ü©‚ü®n| = U.
(1.166)
Every unitary operator is a basis-changing operator, and vice versa.
Inner products do not change under unitary transformations because ‚ü®g|f ‚ü©=
‚ü®g|U‚Ä† U|f ‚ü©= ‚ü®Ug|U|f ‚ü©= ‚ü®Ug|Uf ‚ü©, which in pre-Dirac notation is (g, f ) =
(g, U‚Ä† Uf ) = (Ug, Uf ).
Unitary matrices have unimodular determinants because the determinant of
the product of two matrices is the product of their determinants (1.204) and
because transposition doesn‚Äôt change the value of a determinant (1.194)
1 = |I| = |UU‚Ä†| = |U||U‚Ä†| = |U||UT|‚àó= |U||U|‚àó.
(1.167)
A unitary matrix that is real is orthogonal and satisÔ¨Åes
OOT = OTO = I.
(1.168)
1.17 Hilbert space
We have mostly been talking about linear operators that act on Ô¨Ånite-
dimensional vector spaces and that can be represented by matrices. But
inÔ¨Ånite-dimensional vector spaces and the linear operators that act on them
play central roles in electrodynamics and quantum mechanics. For instance, the
Hilbert space H of all ‚Äúwave‚Äù functions œà(x, t) that are square integrable over
three-dimensional space at all times t is of inÔ¨Ånite dimension.
In one space dimension, the state |x‚Ä≤‚ü©represents a particle at position x‚Ä≤ and
is an eigenstate of the hermitian position operator x with eigenvalue x‚Ä≤, that
is, x|x‚Ä≤‚ü©= x‚Ä≤|x‚Ä≤‚ü©. These states form a basis that is orthogonal in the sense that
‚ü®x|x‚Ä≤‚ü©= 0 for x Ã∏= x‚Ä≤ and normalized in the sense that ‚ü®x|x‚Ä≤‚ü©= Œ¥(x ‚àíx‚Ä≤) in
which Œ¥(x ‚àíx‚Ä≤) is Dirac‚Äôs delta function. The delta function Œ¥(x ‚àíx‚Ä≤) actually
is a functional Œ¥x‚Ä≤ that maps any suitably smooth function f into
Œ¥x‚Ä≤[f ] =

Œ¥(x ‚àíx‚Ä≤) f (x) dx = f (x‚Ä≤),
(1.169)
its value at x‚Ä≤.
Another basis for the Hilbert space of one-dimensional quantum mechanics
is made of the states |p‚ü©of well-deÔ¨Åned momentum. The state |p‚Ä≤‚ü©represents
a particle or system with momentum p‚Ä≤. It is an eigenstate of the hermitian
25

LINEAR ALGEBRA
momentum operator p with eigenvalue p‚Ä≤, that is, p|p‚Ä≤‚ü©= p‚Ä≤|p‚Ä≤‚ü©. The momentum
states also are orthonormal in Dirac‚Äôs sense, ‚ü®p|p‚Ä≤‚ü©= Œ¥(p ‚àíp‚Ä≤).
The operator that translates a system in space by a distance a is
U(a) =

|x + a‚ü©‚ü®x| dx.
(1.170)
It maps the state |x‚Ä≤‚ü©to the state |x‚Ä≤ +a‚ü©and is unitary (exercise 1.23). Remark-
ably, this translation operator is an exponential of the momentum operator
U(a) = exp(‚àíi p a/¬Øh) in which ¬Øh = h/2œÄ = 1.054√ó10‚àí34 Js is Planck‚Äôs constant
divided by 2œÄ.
In two dimensions, with basis states |x, y‚ü©that are orthonormal in Dirac‚Äôs
sense, ‚ü®x, y|x‚Ä≤, y‚Ä≤‚ü©= Œ¥(x ‚àíx‚Ä≤)Œ¥(y ‚àíy‚Ä≤), the unitary operator
U(Œ∏) =

|x cos Œ∏ ‚àíy sin Œ∏, x sin Œ∏ + y cos Œ∏‚ü©‚ü®x, y| dxdy
(1.171)
rotates a system in space by the angle Œ∏. This rotation operator is the exponen-
tial U(Œ∏) = exp(‚àíi Œ∏ Lz/¬Øh) in which the z component of the angular momentum
is Lz = x py ‚àíy px.
We may carry most of our intuition about matrices over to these unitary
transformations that change from one inÔ¨Ånite basis to another. But we must
use common sense and keep in mind that inÔ¨Ånite sums and integrals do not
always converge.
1.18 Antiunitary, antilinear operators
Certain maps on states |œà‚ü©‚Üí|œà‚Ä≤‚ü©, such as those involving time reversal, are
implemented by operators K that are antilinear
K (zœà + wœÜ) = K (z|œà‚ü©+ w|œÜ‚ü©) = z‚àóK|œà‚ü©+w‚àóK|œÜ‚ü©= z‚àóKœà+w‚àóKœÜ (1.172)
and antiunitary
(KœÜ, Kœà) = ‚ü®KœÜ|Kœà‚ü©= (œÜ, œà)‚àó= ‚ü®œÜ|œà‚ü©‚àó= ‚ü®œà|œÜ‚ü©= (œà, œÜ) .
(1.173)
In Dirac notation, these rules are K(z|œà‚ü©) = z‚àó‚ü®œà| and K(w‚ü®œÜ|) = w‚àó|œÜ‚ü©.
1.19 Symmetry in quantum mechanics
In quantum mechanics, a symmetry is a map of states |œà‚ü©‚Üí|œà‚Ä≤‚ü©and |œÜ‚ü©‚Üí|œÜ‚Ä≤‚ü©
that preserves inner products and probabilities
|‚ü®œÜ‚Ä≤|œà‚Ä≤‚ü©|2 = |‚ü®œÜ|œà‚ü©|2.
(1.174)
26

1.20 DETERMINANTS
Eugene Wigner (1902‚Äì1995) showed that every symmetry in quantum mechan-
ics can be represented either by an operator U that is linear and unitary or
by an operator K that is antilinear and antiunitary. The antilinear, antiuni-
tary case seems to occur only when the symmetry involves time reversal. Most
symmetries are represented by operators that are linear and unitary. Unitary
operators are of great importance in quantum mechanics. We use them to rep-
resent rotations, translations, Lorentz transformations, and internal-symmetry
transformations.
1.20 Determinants
The determinant of a 2 √ó 2 matrix A is
det A = |A| = A11A22 ‚àíA21A12.
(1.175)
In terms of the 2 √ó 2 antisymmetric (eij = ‚àíeji) matrix e12 = 1 = ‚àíe21 with
e11 = e22 = 0, this determinant is
det A =
2

i=1
2

j=1
eijAi1Aj2.
(1.176)
It‚Äôs also true that
ek‚Ñìdet A =
2

i=1
2

j=1
eijAikAj‚Ñì.
(1.177)
These deÔ¨Ånitions and results extend to any square matrix. If A is a 3 √ó 3
matrix, then its determinant is
det A =
3

i,j,k=1
eijkAi1Aj2Ak3
(1.178)
in which eijk is totally antisymmetric with e123 = 1, and the sums over i, j, and
k run from 1 to 3. More explicitly, this determinant is
det A =
3

i,j,k=1
eijkAi1Aj2Ak3
=
3

i=1
Ai1
3

j,k=1
eijkAj2Ak3
= A11 (A22A33 ‚àíA32A23) + A21 (A32A13 ‚àíA12A33)
+ A31 (A12A23 ‚àíA22A13) .
(1.179)
27

LINEAR ALGEBRA
The terms within parentheses are the 2 √ó 2 determinants (called minors) of the
matrix A without column 1 and row i, multiplied by (‚àí1)1+i:
det A = A11(‚àí1)2 (A22A33 ‚àíA32A23) + A21(‚àí1)3 (A12A33 ‚àíA32A13)
+ A31(‚àí1)4 (A12A23 ‚àíA22A13)
= A11C11 + A21C21 + A31C31
(1.180)
The minors multiplied by (‚àí1)1+i are called cofactors:
C11 = A22A33 ‚àíA23A32,
C21 = A32A13 ‚àíA12A33,
C31 = A12A23 ‚àíA22A13.
(1.181)
Example 1.25 (Determinant of a 3 √ó 3 matrix)
The determinant of a 3 √ó 3
matrix is the dot-product of the vector of its Ô¨Årst row with the cross-product of
the vectors of its second and third rows:

U1
U2
U3
V1
V2
V3
W1
W2
W3

=
3

i,j,k=1
eijkUiVjWk =
3

i=1
Ui(V √ó W)i = U ¬∑ (V √ó W)
which is called the scalar triple product.
Laplace used the totally antisymmetric symbol ei1i2...iN with N indices and
with e123...N = 1 to deÔ¨Åne the determinant of an N √ó N matrix A as
det A =
N

i1,i2,...,iN=1
ei1i2...iNAi11Ai22 . . . AiNN
(1.182)
in which the sums over i1 . . . iN run from 1 to N. In terms of cofactors, two
forms of his expansion of this determinant are
det A =
N

i=1
AikCik =
N

k=1
AikCik
(1.183)
in which the Ô¨Årst sum is over the row index i but not the (arbitrary) column
index k, and the second sum is over the column index k but not the (arbitrary)
row index i. The cofactor Cik is (‚àí1)i+kMik in which the minor Mik is the deter-
minant of the (N ‚àí1) √ó (N ‚àí1) matrix A without its ith row and kth column.
It‚Äôs also true that
ek1k2...kN det A =
N

i1,i2,...,iN=1
ei1i2...iNAi1k1Ai2k2 . . . AiNkN.
(1.184)
28

1.20 DETERMINANTS
The key feature of a determinant is that it is an antisymmetric combination
of products of the elements Aik of a matrix A. One implication of this antisym-
metry is that the interchange of any two rows or any two columns changes the
sign of the determinant. Another is that if one adds a multiple of one column
to another column, for example a multiple xAi2 of column 2 to column 1, then
the determinant
det A‚Ä≤ =
N

i1,i2,...,in=1
ei1i2...iN

Ai11 + xAi12

Ai22 . . . AiNN
(1.185)
is unchanged. The reason is that the extra term Œ¥ det A vanishes
Œ¥ det A =
N

i1,i2,...,iN=1
x ei1i2...iN Ai12Ai22 . . . AiNN = 0
(1.186)
because it is proportional to a sum of products of a factor ei1i2...iN that is anti-
symmetric in i1 and i2 and a factor Ai12Ai22 that is symmetric in these indices.
For instance, when i1 and i2 are 5 & 7 and 7 & 5, the two terms cancel
e57...iNA52A72 . . . AiNN + e75...iNA72A52 . . . AiNN = 0
(1.187)
because e57...iN = ‚àíe75...iN.
By repeated additions of x2Ai2, x3Ai3, etc. to Ai1, we can change the Ô¨Årst
column of the matrix A to a linear combination of all the columns
Ai1 ‚àí‚ÜíAi1 +
N

k=2
xkAik
(1.188)
without changing det A. In this linear combination, the coefÔ¨Åcients xk are
arbitrary. The analogous operation with arbitrary yk
Ai‚Ñì‚àí‚ÜíAi‚Ñì+
N

k=1,kÃ∏=‚Ñì
ykAik
(1.189)
replaces the ‚Ñìth column by a linear combination of all the columns without
changing det A.
Suppose that the columns of an N √ó N matrix A are linearly dependent
(section 1.8), so that the linear combination of columns
N

k=1
ykAik = 0
for i = 1, . . . N
(1.190)
vanishes for some coefÔ¨Åcients yk not all zero. Suppose y1 Ã∏= 0. Then by adding
suitable linear combinations of columns 2 through N to column 1, we could
make all the modiÔ¨Åed elements A‚Ä≤
i1 of column 1 vanish without changing det A.
29

LINEAR ALGEBRA
But then det A as given by (1.182) would vanish. Thus the determinant of any
matrix whose columns are linearly dependent must vanish.
The converse also is true: if columns of a matrix are linearly independent,
then the determinant of that matrix can not vanish. The reason is that any lin-
early independent set of vectors is complete (section 1.8). Thus if the columns of
a matrix A are linearly independent and therefore complete, some linear combi-
nation of all columns 2 through N when added to column 1 will convert column
1 into a (nonzero) multiple of the N-dimensional column vector (1, 0, 0, . . . 0),
say (c1, 0, 0, . . . 0). Similar operations will convert column 2 into a (nonzero)
multiple of the column vector (0, 1, 0, . . . 0), say (0, c2, 0, . . . 0). Continuing in
this way, we may convert the matrix A to a matrix with nonzero entries along
the main diagonal and zeros everywhere else. The determinant det A is then the
product of the nonzero diagonal entries c1c2 . . . cN Ã∏= 0, and so det A can not
vanish.
We may extend these arguments to the rows of a matrix. The addition to row
k of a linear combination of the other rows
Aki ‚àí‚ÜíAki +
N

‚Ñì=1,‚ÑìÃ∏=k
z‚ÑìA‚Ñìi
(1.191)
does not change the value of the determinant. In this way, one may show that
the determinant of a matrix vanishes if and only if its rows are linearly depen-
dent. The reason why these results apply to the rows as well as to the columns
is that the determinant of a matrix A may be deÔ¨Åned either in terms of the
columns as in deÔ¨Ånitions (1.182 & 1.184) or in terms of the rows:
det A =
N

i1,i2,...,iN=1
ei1i2...iNA1i1A2i2 . . . ANiN,
(1.192)
ek1k2...kN det A =
N

i1,i2,...,iN=1
ei1i2...iNAk1i1Ak2i2 . . . AkNiN.
(1.193)
These and other properties of determinants follow from a study of permutations
(section 10.13). Detailed proofs are in Aitken (1959).
By comparing the row (1.182 & 1.184) and column (1.192 & 1.193) deÔ¨Åni-
tions of determinants, we see that the determinant of the transpose of a matrix
is the same as the determinant of the matrix itself:
det

AT
= det A.
(1.194)
Let us return for a moment to Laplace‚Äôs expansion (1.183) of the determinant
det A of an N √ó N matrix A as a sum of AikCik over the row index i with the
column index k held Ô¨Åxed
30

1.20 DETERMINANTS
det A =
N

i=1
AikCik
(1.195)
in order to prove that
Œ¥k‚Ñìdet A =
N

i=1
AikCi‚Ñì.
(1.196)
For k = ‚Ñì, this formula just repeats Laplace‚Äôs expansion (1.195). But for k Ã∏= ‚Ñì,
it is Laplace‚Äôs expansion for the determinant of a matrix A‚Ä≤ that is the same as
A but with its ‚Ñìth column replaced by its kth one. Since the matrix A‚Ä≤ has two
identical columns, its determinant vanishes, which explains (1.196) for k Ã∏= ‚Ñì.
This rule (1.196) provides a formula for the inverse of a matrix A whose
determinant does not vanish. Such matrices are said to be nonsingular. The
inverse A‚àí1 of an N √ó N nonsingular matrix A is the transpose of the matrix of
cofactors divided by det A

A‚àí1
‚Ñìi =
Ci‚Ñì
det A
or
A‚àí1 =
CT
det A.
(1.197)
To verify this formula, we use it for A‚àí1 in the product A‚àí1A and note that by
(1.196) the ‚Ñìkth entry of the product A‚àí1A is just Œ¥‚Ñìk

A‚àí1A

‚Ñìk =
N

i=1

A‚àí1
‚Ñìi Aik =
N

i=1
Ci‚Ñì
det AAik = Œ¥‚Ñìk.
(1.198)
Example 1.26 (Inverting a 2√ó2 matrix)
Let‚Äôs apply our formula (1.197) to Ô¨Ånd
the inverse of the general 2 √ó 2 matrix
A =
a
b
c
d

.
(1.199)
We Ô¨Ånd then
A‚àí1 =
1
ad ‚àíbc
 d
‚àíb
‚àíc
a

,
(1.200)
which is the correct inverse as long as ad Ã∏= bc.
The simple example of matrix multiplication
‚éõ
‚éù
a
b
c
d
e
f
g
h
i
‚éû
‚é†
‚éõ
‚éù
1
x
y
0
1
z
0
0
1
‚éû
‚é†=
‚éõ
‚éù
a
xa + b
ya + zb + c
d
xd + e
yd + ze + f
g
xg + h
yg + zh + i
‚éû
‚é†
(1.201)
shows that the operations (1.189) on columns that don‚Äôt change the value of the
determinant can be written as matrix multiplication from the right by a matrix
31

LINEAR ALGEBRA
that has unity on its main diagonal and zeros below. Now consider the matrix
product
 A
0
‚àíI
B
 I
B
0
I

=
 A
AB
‚àíI
0

(1.202)
in which A and B are N √ó N matrices, I is the N √ó N identity matrix, and 0 is
the N √óN matrix of all zeros. The second matrix on the left-hand side has unity
on its main diagonal and zeros below, and so it does not change the value of the
determinant of the matrix to its left, which then must equal that of the matrix
on the right-hand side:
det
 A
0
‚àíI
B

= det
 A
AB
‚àíI
0

.
(1.203)
By using Laplace‚Äôs expansion (1.183) along the Ô¨Årst column to evaluate the
determinant on the left-hand side and his expansion along the last row to com-
pute the determinant on the right-hand side, one Ô¨Ånds that the determinant of
the product of two matrices is the product of the determinants
det A det B = det AB.
(1.204)
Example 1.27 (Two 2 √ó 2 matrices)
When the matrices A and B are both 2 √ó 2,
the two sides of (1.203) are
det
 A
0
‚àíI
B

= det
‚éõ
‚éú‚éú‚éù
a11
a12
0
0
a21
a22
0
0
‚àí1
0
b11
b12
0
‚àí1
b21
b22
‚éû
‚éü‚éü‚é†
= a11a22 det B ‚àía21a12 det B = det A det B
(1.205)
and
det
 A
AB
‚àíI
0

= det
‚éõ
‚éú‚éú‚éù
a11
a12
ab11
ab12
a21
a22
ab21
ab22
‚àí1
0
0
0
0
‚àí1
0
0
‚éû
‚éü‚éü‚é†
= (‚àí1)C42 = (‚àí1)(‚àí1) det AB = det AB
(1.206)
and so they give the product rule det A det B = det AB.
Often one uses the notation |A| = det A to denote a determinant. In this
more compact notation, the obvious generalization of the product rule is
32

1.20 DETERMINANTS
|ABC . . . Z| = |A||B| . . . |Z|.
(1.207)
The product rule (1.204) implies that det

A‚àí1
is 1/ det A since
1 = det I = det

AA‚àí1
= det A det

A‚àí1
.
(1.208)
Incidentally, Gauss, Jordan, and modern mathematicians have developed
much faster ways of computing determinants and matrix inverses than those
(1.183 & 1.197) due to Laplace. Octave, Matlab, Maple, and Mathematica use
these modern techniques, which also are freely available as programs in C and
FORTRAN from www.netlib.org/lapack.
Example 1.28 (Numerical tricks)
Adding multiples of rows to other rows does
not change the value of a determinant, and interchanging two rows only changes
a determinant by a minus sign. So we can use these operations, which leave
determinants invariant, to make a matrix upper triangular, a form in which its
determinant is just the product of the factors on its diagonal. For instance, to
make the matrix
A =
‚éõ
‚éù
1
2
1
‚àí2
‚àí6
3
4
2
‚àí5
‚éû
‚é†
(1.209)
upper triangular, we add twice the Ô¨Årst row to the second row
‚éõ
‚éù
1
2
1
0
‚àí2
5
4
2
‚àí5
‚éû
‚é†
(1.210)
and then subtract four times the Ô¨Årst row from the third
‚éõ
‚éù
1
2
1
0
‚àí2
5
0
‚àí6
‚àí9
‚éû
‚é†.
(1.211)
Next, we subtract three times the second row from the third
‚éõ
‚éù
1
2
1
0
‚àí2
5
0
0
‚àí24
‚éû
‚é†.
(1.212)
We now Ô¨Ånd as the determinant of A the product of its diagonal elements:
|A| = 1(‚àí2)(‚àí24) = 48.
(1.213)
The Matlab command is d = det(A).
33

LINEAR ALGEBRA
1.21 Systems of linear equations
Suppose we wish to solve the system of N linear equations
N

k=1
Aikxk = yi
(1.214)
for N unknowns xk. In matrix notation, with A an N √ó N matrix and x and y
N-vectors, this system of equations is A x = y. If the matrix A is nonsingular,
that is, if det(A) Ã∏= 0, then it has an inverse A‚àí1 given by (1.197), and we may
multiply both sides of A x = y by A‚àí1 and so Ô¨Ånd x as x = A‚àí1 y. When A is
nonsingular, this is the unique solution to (1.214).
When A is singular, det(A) = 0, and so its columns are linearly dependent
(section 1.20). In this case, the linear dependence of the columns of A implies
that A z = 0 for some nonzero vector z. Thus if x is a solution, so that A x = y,
then A(x + cz) = A x + c A z = y implies that x + cz for all c also is a solution.
So if det(A) = 0, then there may be solutions, but there can be no unique
solution. Whether equation (1.214) has any solutions when det(A) = 0 depends
on whether the vector y can be expressed as a linear combination of the columns
of A. Since these columns are linearly dependent, they span a subspace of fewer
than N dimensions, and so (1.214) has solutions only when the N-vector y lies
in that subspace.
A system of M < N equations
N

k=1
Aikxk = yi
for
i = 1, 2, . . . , M
(1.215)
in N unknowns is under-determined. As long as at least M of the N columns Aik
of the matrix A are linearly independent, such a system always has solutions,
but they will not be unique.
1.22 Linear least squares
Suppose we have a system of M > N equations in N unknowns xk
N

k=1
Aikxk = yi
for
i = 1, 2, . . . , M.
(1.216)
This problem is over-determined and, in general, has no solution, but it does
have an approximate solution due to Carl Gauss (1777‚Äì1855).
If the matrix A and the vector y are real, then Gauss‚Äôs solution is the N values
xk that minimize the sum E of the squares of the errors
34

1.23 LAGRANGE MULTIPLIERS
E =
M

i=1

yi ‚àí
N

k=1
Aikxk
2
.
(1.217)
The minimizing values xk make the N derivatives of E vanish
‚àÇE
‚àÇx‚Ñì
= 0 =
M

i=1
2

yi ‚àí
N

k=1
Aikxk

(‚àíAi‚Ñì)
(1.218)
or in matrix notation ATy = ATAx. Since A is real, the matrix ATA is nonnega-
tive (1.38); if it also is positive (1.39), then it has an inverse, and our least-squares
solution is
x =

ATA
‚àí1 ATy.
(1.219)
If the matrix A and the vector y are complex, and if the matrix A‚Ä†A is positive,
then one may show (exercise 1.25) that Gauss‚Äôs solution is
x =

A‚Ä†A
‚àí1
A‚Ä†y.
(1.220)
1.23 Lagrange multipliers
The maxima and minima of a function f (x) of several variables x1, x2, . . . , xn
are among the points at which its gradient vanishes
‚àáf (x) = 0.
(1.221)
These are the stationary points of f .
Example 1.29 (Minimum)
For instance, if f (x) = x2
1 + 2x2
2 + 3x2
3, then its
minimum is at
‚àáf (x) = (2x1, 4x2, 6x3) = 0
(1.222)
that is, at x1 = x2 = x3 = 0.
But how do we Ô¨Ånd the extrema of f (x) if x must satisfy a constraint? We use
a Lagrange multiplier (Joseph-Louis Lagrange, 1736‚Äì1813).
In the case of one constraint c(x) = 0, we no longer expect the gradient ‚àáf (x)
to vanish, but its projection dx ¬∑ ‚àáf (x) must vanish in those directions dx that
preserve the constraint. So dx ¬∑ ‚àáf (x) = 0 for all dx that make the dot-product
dx ¬∑ ‚àác(x) vanish. This means that ‚àáf (x) and ‚àác(x) must be parallel. So the
extrema of f (x) subject to the constraint c(x) = 0 satisfy two equations
‚àáf (x) = Œª ‚àác(x)
and
c(x) = 0.
(1.223)
35

LINEAR ALGEBRA
These equations deÔ¨Åne the extrema of the unconstrained function
L(x, Œª) = f (x) ‚àíŒª c(x)
(1.224)
of the n + 1 variables x, . . . , xn, Œª
‚àáL(x, Œª) = ‚àáf (x) ‚àíŒª ‚àác(x) = 0
and
‚àÇL(x, Œª)
‚àÇŒª
= ‚àíc(x) = 0.
(1.225)
The variable Œª is a Lagrange multiplier.
In the case of k constraints c1(x) = 0, ..., ck(x) = 0, the projection ‚àáf must
vanish in those directions dx that preserve all the constraints. So dx ¬∑ ‚àáf (x) = 0
for all dx that make all dx ¬∑ ‚àácj(x) = 0 for j = 1, . . . , k. The gradient ‚àáf will
satisfy this requirement if it‚Äôs a linear combination
‚àáf = Œª1 ‚àác1 + ¬∑ ¬∑ ¬∑ + Œªk ‚àáck
(1.226)
of the k gradients because then dx¬∑‚àáf will vanish if dx¬∑‚àácj = 0 for j = 1, . . . , k.
The extrema also must satisfy the constraints
c1(x) = 0, . . . , ck(x) = 0.
(1.227)
Equations (1.226 & 1.227) deÔ¨Åne the extrema of the unconstrained function
L(x, Œª) = f (x) ‚àíŒª1 c1(x) + ¬∑ ¬∑ ¬∑ Œªk ck(x)
(1.228)
of the n + k variables x and Œª
‚àáL(x, Œª) = ‚àáf (x) ‚àíŒª ‚àác1(x) ‚àí¬∑ ¬∑ ¬∑ ‚àíŒª ‚àáck(x) = 0
(1.229)
and
‚àÇL(x, Œª)
‚àÇŒªj
= ‚àícj(x) = 0
for
j = 1, . . . , k.
(1.230)
Example 1.30 (Constrained extrema and eigenvectors)
Suppose we want to Ô¨Ånd
the extrema of a real, symmetric quadratic form f (x) = xTA x subject to the
constraint c(x) = x¬∑x‚àí1, which says that the vector x is of unit length. We form
the function
L(x, Œª) = xTA x ‚àíŒª (x ¬∑ x ‚àí1)
(1.231)
and since the matrix A is real and symmetric, we Ô¨Ånd its unconstrained extrema
as
‚àáL(x, Œª) = 2A x ‚àí2Œª x = 0
and
x ¬∑ x = 1.
(1.232)
The extrema of f (x) = xTA x subject to the constraint c(x) = x ¬∑ x ‚àí1 are the
normalized eigenvectors
A x = Œª x
and
x ¬∑ x = 1
(1.233)
of the real, symmetric matrix A.
36

1.24 EIGENVECTORS
1.24 Eigenvectors
If a linear operator A maps a nonzero vector |u‚ü©into a multiple of itself
A|u‚ü©= Œª|u‚ü©
(1.234)
then the vector |u‚ü©is an eigenvector of A with eigenvalue Œª. (The German
adjective eigen means special or proper.)
If the vectors {|k‚ü©} for k = 1, ..., N form a basis for the vector space in which
A acts, then we can write the identity operator for the space as I = |1‚ü©‚ü®1| +
¬∑ ¬∑ ¬∑+|N‚ü©‚ü®N|. By inserting this formula for I twice into the eigenvector equation
(1.234), we can write it as
N

‚Ñì=1
‚ü®k|A|‚Ñì‚ü©‚ü®‚Ñì|u‚ü©= Œª ‚ü®k|u‚ü©.
(1.235)
In matrix notation, with Ak‚Ñì= ‚ü®k|A|‚Ñì‚ü©and u‚Ñì= ‚ü®‚Ñì|u‚ü©, this is A u = Œª u.
Example 1.31 (Eigenvalues of an orthogonal matrix)
The matrix equation
 cos Œ∏
sin Œ∏
‚àísin Œ∏
cos Œ∏
  1
¬±i

= e¬±iŒ∏
 1
¬±i

(1.236)
tells us that the eigenvectors of this 2 √ó 2 orthogonal matrix are the 2-tuples
(1, ¬±i) with eigenvalues e¬±iŒ∏. The eigenvalues Œª of a unitary (and of an orthogo-
nal) matrix are unimodular, |Œª| = 1 (exercise 1.26).
Example 1.32 (Eigenvalues of an antisymmetric matrix)
Let us consider an
eigenvector equation for a matrix A that is antisymmetric
N

k=1
Aik uk = Œª ui.
(1.237)
The antisymmetry Aik = ‚àíAki of A implies that
N

i,k=1
ui Aik uk = 0.
(1.238)
Thus the last two relations imply that
0 =
N

i,k=1
ui Aik uk = Œª
N

i=1
u2
i = 0.
(1.239)
Thus either the eigenvalue Œª or the dot-product of the eigenvector with itself
vanishes.
37

LINEAR ALGEBRA
A subspace c‚Ñì|u‚Ñì‚ü©+¬∑ ¬∑ ¬∑+cr|ur‚ü©spanned by any set of eigenvectors of a matrix
A is left invariant by its action, that is
A (c‚Ñì|u‚Ñì‚ü©+ ¬∑ ¬∑ ¬∑ + cr|ur‚ü©) = c‚ÑìŒª‚Ñì|u‚Ñì‚ü©+ ¬∑ ¬∑ ¬∑ + crŒªr|ur‚ü©.
(1.240)
Eigenvectors span invariant subspaces.
1.25 Eigenvectors of a square matrix
Let A be an N √ó N matrix with complex entries Aik. A vector V with N entries
Vk (not all zero) is an eigenvector of A with eigenvalue Œª if
AV = ŒªV ‚áê‚áí
N

k=1
AikVk = ŒªVi.
(1.241)
Every N √ó N matrix A has N eigenvectors V(‚Ñì) and eigenvalues Œª‚Ñì
AV(‚Ñì) = Œª‚ÑìV(‚Ñì)
(1.242)
for ‚Ñì= 1 . . . N. To see why, we write the top equation (1.241) as
N

k=1
(Aik ‚àíŒªŒ¥ik) Vk = 0
(1.243)
or in matrix notation as (A ‚àíŒª I) V = 0 in which I is the N √ó N matrix with
entries Iik = Œ¥ik. This equation and (1.243) say that the columns of the matrix
A ‚àíŒªI, considered as vectors, are linearly dependent, as deÔ¨Åned in section 1.8.
We saw in section 1.20 that the columns of a matrix A ‚àíŒªI are linearly depen-
dent if and only if the determinant |A ‚àíŒªI| vanishes. Thus a nonzero solution
of the eigenvalue equation (1.241) exists if and only if the determinant
det (A ‚àíŒªI) = |A ‚àíŒªI| = 0
(1.244)
vanishes. This requirement that the determinant of A‚àíŒªI vanishes is called the
characteristic equation. For an N √ó N matrix A, it is a polynomial equation of
the Nth degree in the unknown eigenvalue Œª
|A ‚àíŒªI| ‚â°P(Œª, A) = |A| + ¬∑ ¬∑ ¬∑ + (‚àí1)N‚àí1ŒªN‚àí1 TrA + (‚àí1)NŒªN
=
N

k=0
pk Œªk = 0
(1.245)
in which p0 = |A|, pN‚àí1 = (‚àí1)N‚àí1TrA, and pN = (‚àí1)N. (All the pks are
basis independent.) By the fundamental theorem of algebra (section 5.9), the
characteristic equation always has N roots or solutions Œª‚Ñìlying somewhere in
the complex plane. Thus the characteristic polynomial has the factored form
P(Œª, A) = (Œª1 ‚àíŒª)(Œª2 ‚àíŒª) . . . (ŒªN ‚àíŒª).
(1.246)
38

1.25 EIGENVECTORS OF A SQUARE MATRIX
For every root Œª‚Ñì, there is a nonzero eigenvector V(‚Ñì) whose components V(‚Ñì)
k
are the coefÔ¨Åcients that make the N vectors Aik ‚àíŒª‚ÑìŒ¥ik that are the columns
of the matrix A ‚àíŒª‚ÑìI sum to zero in (1.243). Thus every N √ó N matrix has N
eigenvalues Œª‚Ñìand N eigenvectors V(‚Ñì).
The N √ó N diagonal matrix Dk‚Ñì= Œ¥k‚ÑìŒª‚Ñìis the canonical form of the matrix
A; the matrix Vk‚Ñì= V(‚Ñì)
k
whose columns are the eigenvectors V(‚Ñì) of A is the
modal matrix; and AV = VD.
Example 1.33 (The canonical form of a 3 √ó 3 matrix)
If in Matlab we set A =
[0 1 2; 3 4 5; 6 7 8] and enter [V, D] = eig(A), then we get
V =
‚éõ
‚éù
0.1648
0.7997
0.4082
0.5058
0.1042
‚àí0.8165
0.8468
‚àí0.5913
0.4082
‚éû
‚é†
and
D =
‚éõ
‚éù
13.3485
0
0
0
‚àí1.3485
0
0
0
0
‚éû
‚é†
and one may check that AV = VD.
Setting Œª = 0 in the factored form (1.246) of P(Œª, A) and in the characteristic
equation (1.245), we see that the determinant of every N√óN matrix is the product
of its N eigenvalues
P(0, A) = |A| = p0 = Œª1Œª2 . . . ŒªN.
(1.247)
These N roots usually are all different, and when they are, the eigenvectors
V(‚Ñì) are linearly independent. The Ô¨Årst eigenvector is trivially linearly indepen-
dent. Let‚Äôs assume that the Ô¨Årst K < N eigenvectors are linearly independent;
we‚Äôll show that the Ô¨Årst K +1 eigenvectors are linearly independent. If they were
linearly dependent, then there would be K + 1 numbers c‚Ñì, not all zero, such
that
K+1

‚Ñì=1
c‚ÑìV(‚Ñì) = 0.
(1.248)
First we multiply this equation from the left by the linear operator A and use
the eigenvalue equation (1.242)
A
K+1

‚Ñì=1
c‚ÑìV(‚Ñì) =
K+1

‚Ñì=1
c‚ÑìAV(‚Ñì) =
K+1

‚Ñì=1
c‚ÑìŒª‚ÑìV(‚Ñì) = 0.
(1.249)
Now we multiply the same equation (1.248) by ŒªK+1
K+1

‚Ñì=1
c‚ÑìŒªN V(‚Ñì) = 0
(1.250)
39

LINEAR ALGEBRA
and subtract the product (1.250) from (1.249). The terms with ‚Ñì= K +1 cancel
leaving
K

‚Ñì=1
c‚Ñì(Œª‚Ñì‚àíŒªN) V(‚Ñì) = 0
(1.251)
in which all the factors (Œª‚Ñì‚àíŒªK+1) are different from zero since by assump-
tion all the eigenvalues are different. But this last equation says that the Ô¨Årst
K eigenvectors are linearly dependent, which contradicts our assumption that
they were linearly independent. This contradiction tells us that if all N eigenvec-
tors of an N √ó N square matrix have different eigenvalues, then they are linearly
independent.
An eigenvalue Œª that is a single root of the characteristic equation (1.245) is
associated with a single eigenvector; it is called a simple eigenvalue. An eigen-
value Œª that is an nth root of the characteristic equation is associated with n
eigenvectors; it is said to be an n-fold degenerate eigenvalue or to have algebraic
multiplicity n. Its geometric multiplicity is the number n‚Ä≤ ‚â§n of linearly inde-
pendent eigenvectors with eigenvalue Œª. A matrix with n‚Ä≤ < n for any eigenvalue
Œª is defective. Thus an N √ó N matrix with fewer than N linearly independent
eigenvectors is defective.
Example 1.34 (A defective 2 √ó 2 matrix)
Each of the 2 √ó 2 matrices
0
1
0
0

and
0
0
1
0

(1.252)
has only one linearly independent eigenvector and so is defective.
Suppose A is an N √óN matrix that is not defective. We may use its N linearly
independent eigenvectors V(‚Ñì) = |‚Ñì‚ü©to deÔ¨Åne the columns of an N √ó N matrix
S as Sk‚Ñì= V(‚Ñì)
k . In terms of S, the eigenvalue equation (1.242) takes the form
N

k=1
AikSk‚Ñì= Œª‚ÑìSi‚Ñì.
(1.253)
Since the columns of S are linearly independent, the determinant of S does not
vanish ‚Äì the matrix S is nonsingular ‚Äì and so its inverse S‚àí1 is well deÔ¨Åned by
(1.197). So we may multiply this equation by S‚àí1 and get
N

i,k=1

S‚àí1
ni AikSk‚Ñì=
N

i=1
Œª‚Ñì

S‚àí1
ni Si‚Ñì= ŒªnŒ¥n‚Ñì= Œª‚Ñì
(1.254)
or in matrix notation
S‚àí1AS = A(d)
(1.255)
40

1.26 A MATRIX OBEYS ITS CHARACTERISTIC EQUATION
in which A(d) is the diagonal form of the matrix A in which its eigenvalues Œª‚Ñìare
arranged along its main diagonal with zeros elsewhere. This equation (1.255) is
a similarity transformation. Thus every nondefective square matrix can be diag-
onalized by a similarity transformation S‚àí1AS = A(d) and can be generated
from its diagonal form by the inverse A = SA(d)S‚àí1 of that similarity transfor-
mation. By using the product rule (1.207), we see that the determinant of any
nondefective square matrix is the product of its eigenvalues
|A| = |SA(d)S‚àí1| = |S| |A(d)| |S‚àí1| = |SS‚àí1| |A(d)| = |A(d)| =
N

‚Ñì=1
Œª‚Ñì, (1.256)
which is a special case of (1.247).
1.26 A matrix obeys its characteristic equation
Every square matrix obeys its characteristic equation (1.245). That is, the
characteristic equation
P(Œª, A) = |A ‚àíŒªI| =
N

k=0
pk Œªk = 0
(1.257)
remains true when the matrix A replaces the variable Œª
P(A, A) =
N

k=0
pk Ak = 0.
(1.258)
To see why, we use the formula (1.197) for the inverse of the matrix A ‚àíŒªI
(A ‚àíŒªI)‚àí1 = C(Œª, A)T
|A ‚àíŒªI|
(1.259)
in which C(Œª, A)T is the transpose of the matrix of cofactors of the matrix A ‚àí
ŒªI. Since |A ‚àíŒªI| = P(Œª, A), we have, rearranging,
(A ‚àíŒªI) C(Œª, A)T = |A ‚àíŒªI| I = P(Œª, A) I.
(1.260)
The transpose of the matrix of cofactors of the matrix A ‚àíŒªI is a polynomial
in Œª with matrix coefÔ¨Åcients
C(Œª, A)T = C0 + C1Œª + ¬∑ ¬∑ ¬∑ + CN‚àí1ŒªN‚àí1.
(1.261)
The left-hand side of equation (1.260) is then
(A ‚àíŒªI)C(Œª, A)T = AC0 + (AC1 ‚àíC0)Œª + (AC2 ‚àíC1)Œª2 + ¬∑ ¬∑ ¬∑
+ (ACN‚àí1 ‚àíCN‚àí2)ŒªN‚àí1 ‚àíCN‚àí1ŒªN.
(1.262)
41

LINEAR ALGEBRA
Equating equal powers of Œª on both sides of (1.260), we have, using (1.257) and
(1.262),
AC0 = p0I,
AC1 ‚àíC0 = p1I,
AC2 ‚àíC1 = p2I,
¬∑ ¬∑ ¬∑ = ¬∑ ¬∑ ¬∑
ACN‚àí1 ‚àíCN‚àí2 = pN‚àí1I,
‚àíCN‚àí1 = pNI.
(1.263)
We now multiply from the left the Ô¨Årst of these equations by I, the second by A,
the third by A2, ..., and the last by AN and then add the resulting equations.
All the terms on the left-hand sides cancel, while the sum of those on the right
gives P(A, A). Thus a square matrix A obeys its characteristic equation 0 =
P(A, A) or
0 =
N

k=0
pk Ak = |A| I + p1A + ¬∑ ¬∑ ¬∑ + (‚àí1)N‚àí1(TrA) AN‚àí1 + (‚àí1)N AN, (1.264)
a result known as the Cayley‚ÄìHamilton theorem (Arthur Cayley, 1821‚Äì1895,
and William Hamilton, 1805‚Äì1865). This derivation is due to Israel Gelfand
(1913‚Äì2009) (Gelfand, 1961, pp. 89‚Äì90).
Because every N√óN matrix A obeys its characteristic equation, its Nth power
AN can be expressed as a linear combination of its lesser powers
AN = (‚àí1)N‚àí1 
|A| I + p1A + p2A2 + ¬∑ ¬∑ ¬∑ + (‚àí1)N‚àí1(TrA) AN‚àí1
.
(1.265)
For instance, the square A2 of every 2 √ó 2 matrix is given by
A2 = ‚àí|A|I + (TrA)A.
(1.266)
Example 1.35 (Spin-one-half rotation matrix)
If Œ∏ is a real 3-vector and œÉ is
the 3-vector of Pauli matrices (1.32), then the square of the traceless 2√ó2 matrix
A = Œ∏ ¬∑ œÉ is
(Œ∏ ¬∑ œÉ)2 ‚àí|Œ∏ ¬∑ œÉ| = ‚àí

Œ∏3
Œ∏1 ‚àíiŒ∏2
Œ∏1 + iŒ∏2
‚àíŒ∏3
 I = Œ∏2 I
(1.267)
in which Œ∏2 = Œ∏ ¬∑ Œ∏. One may use this identity to show (exercise (1.28)) that
exp (‚àíiŒ∏ ¬∑ œÉ/2) = cos(Œ∏/2) ‚àíi ÀÜŒ∏ ¬∑ œÉ sin(Œ∏/2)
(1.268)
in which ÀÜŒ∏ is a unit 3-vector. For a spin-one-half object, this matrix represents a
right-handed rotation of Œ∏ radians about the axis ÀÜŒ∏.
42

1.27 FUNCTIONS OF MATRICES
1.27 Functions of matrices
What sense can we make of a function f of an N √ó N matrix A and how
would we compute it? One way is to use the characteristic equation (1.265)
to express every power of A in terms of I, A, ..., AN‚àí1 and the coefÔ¨Å-
cients p0 = |A|, p1, p2, . . . , pN‚àí2, and pN‚àí1 = (‚àí1)N‚àí1TrA. Then if f (x) is a
polynomial or a function with a convergent power series
f (x) =
‚àû

k=0
ck xk
(1.269)
in principle we may express f (A) in terms of N functions fk(p) of the coefÔ¨Åcients
p ‚â°(p0, . . . , pN‚àí1) as
f (A) =
N‚àí1

k=0
fk(p) Ak.
(1.270)
The identity (1.268) for exp (‚àíiŒ∏ ¬∑ œÉ/2) is an N = 2 example of this technique,
which can become challenging when N > 3.
Example 1.36 (The 3√ó3 rotation matrix)
In exercise (1.29), one Ô¨Ånds the char-
acteristic equation (1.264) for the 3√ó3 matrix ‚àíiŒ∏ ¬∑ J in which (Jk)ij = iœµikj, and
œµijk is totally antisymmetric with œµ123 = 1. The generators Jk satisfy the com-
mutation relations [Ji, Jj] = iœµijkJk in which sums over repeated indices from
1 to 3 are understood. In exercise (1.31), one uses this characteristic equation
for ‚àíiŒ∏ ¬∑ J to show that the 3√ó3 real orthogonal matrix exp(‚àíiŒ∏ ¬∑ J), which
represents a right-handed rotation by Œ∏ radians about the axis ÀÜŒ∏, is
exp(‚àíiŒ∏ ¬∑ J) = cos Œ∏ I ‚àíi ÀÜŒ∏ ¬∑ J sin Œ∏ + (1 ‚àícos Œ∏) ÀÜŒ∏( ÀÜŒ∏)T
(1.271)
or
exp(‚àíiŒ∏ ¬∑ J)ij = Œ¥ij cos Œ∏ ‚àísin Œ∏ œµijk ÀÜŒ∏k + (1 ‚àícos Œ∏) ÀÜŒ∏i ÀÜŒ∏j
(1.272)
in terms of indices.
Direct use of the characteristic equation can become unwieldy for larger val-
ues of N. Fortunately, another trick is available if A is a nondefective square
matrix, and if the power series (1.269) for f (x) converges. For then A is related
to its diagonal form A(d) by a similarity transformation (1.255), and we may
deÔ¨Åne f (A) as
f (A) = Sf (A(d))S‚àí1
(1.273)
in which f (A(d)) is the diagonal matrix with entries f (a‚Ñì)
43

LINEAR ALGEBRA
f (A(d)) =
‚éõ
‚éú‚éú‚éú‚éù
f (a1)
0
0
. . .
0
f (a2)
0
. . .
...
...
...
...
0
0
. . .
f (aN)
‚éû
‚éü‚éü‚éü‚é†,
(1.274)
in which a1, a2, . . . , aN are the eigenvalues of the matrix A. This deÔ¨Ånition
makes sense if f (A) is a series in powers of A because then
f (A) =
‚àû

n=0
cnAn =
‚àû

n=0
cn

SA(d)S‚àí1n
.
(1.275)
So since S‚àí1S = I, we have

SA(d)S‚àí1n = S

A(d)n S‚àí1 and thus
f (A) = S
 ‚àû

n=0
cn

A(d)n

S‚àí1 = Sf (A(d))S‚àí1,
(1.276)
which is (1.273).
Example 1.37 (The time-evolution operator)
In quantum mechanics, the time-
evolution operator is the exponential exp(‚àíiHt/¬Øh) where H = H‚Ä† is a hermitian
linear operator, the hamiltonian (William Rowan Hamilton, 1805‚Äì1865), and
¬Øh = h/(2œÄ) = 1.054 √ó 10‚àí34 Js where h is constant (Max Planck, 1858‚Äì1947).
As we‚Äôll see in the next section, hermitian operators are never defective, so H
can be diagonalized by a similarity transformation
H = SH(d)S‚àí1.
(1.277)
The diagonal elements of the diagonal matrix H(d) are the energies E‚Ñìof
the states of the system described by the hamiltonian H. The time-evolution
operator U(t) then is
U(t) = S exp(‚àíiH(d)t/¬Øh) S‚àí1.
(1.278)
For a three-state system with angular frequencies œâi = Ei/¬Øh, it is
U(t) = S
‚éõ
‚éù
e‚àíiœâ1t
0
0
0
e‚àíiœâ2t
0
0
e‚àíiœâ3t
‚éû
‚é†S‚àí1
(1.279)
in which the angular frequencies are œâ‚Ñì= E‚Ñì/¬Øh.
Example 1.38 (Entropy)
The entropy S of a system described by a density
operator œÅ is the trace
S = ‚àík Tr (œÅ ln œÅ)
(1.280)
in which k = 1.38 √ó 10‚àí23 J/K is the constant named after Ludwig Boltzmann
(1844‚Äì1906). The density operator œÅ is hermitian, nonnegative, and of unit trace.
44

1.28 HERMITIAN MATRICES
Since œÅ is hermitian, the matrix that represents it is never defective (section 1.28),
and so it can be diagonalized by a similarity transformation œÅ = S œÅ(d) S‚àí1. By
(1.24), TrABC = TrBCA, so we can write S as
S = ‚àíkTr

S œÅ(d) S‚àí1 S ln(œÅ(d)) S‚àí1
= ‚àíkTr

œÅ(d) ln(œÅ(d))

.
(1.281)
A vanishing eigenvalue œÅ(d)
k
=
0 contributes nothing to this trace since
limx‚Üí0 x ln x = 0. If the system has three states, populated with probabilities
œÅi, the elements of œÅ(d), then the sum
S = ‚àík (œÅ1 ln œÅ1 + œÅ2 ln œÅ2 + œÅ3 ln œÅ3)
= k [œÅ1 ln (1/œÅ1) + œÅ2 ln (1/œÅ2) + œÅ3 ln (1/œÅ3)]
(1.282)
is its entropy.
1.28 Hermitian matrices
Hermitian matrices have very nice properties. By deÔ¨Ånition (1.30), a hermitian
matrix A is square and unchanged by hermitian conjugation A‚Ä† = A. Since it is
square, the results of section 1.25 ensure that an N √ó N hermitian matrix A has
N eigenvectors |n‚ü©with eigenvalues an
A|n‚ü©= an|n‚ü©.
(1.283)
In fact, all its eigenvalues are real. To see why, we take the adjoint
‚ü®n|A‚Ä† = a‚àó
n‚ü®n|
(1.284)
and use the property A‚Ä† = A to Ô¨Ånd
‚ü®n|A‚Ä† = ‚ü®n|A = a‚àó
n‚ü®n|.
(1.285)
We now form the inner product of both sides of this equation with the ket |n‚ü©
and use the eigenvalue equation (1.283) to get
‚ü®n|A|n‚ü©= an‚ü®n|n‚ü©= a‚àó
n‚ü®n|n‚ü©,
(1.286)
which (since ‚ü®n|n‚ü©> 0) tells us that the eigenvalues are real
a‚àó
n = an.
(1.287)
Since A‚Ä† = A, the matrix elements of A between two of its eigenvectors satisfy
a‚àó
m‚ü®m|n‚ü©= (am‚ü®n|m‚ü©)‚àó= ‚ü®n|A|m‚ü©‚àó= ‚ü®m|A‚Ä†|n‚ü©= ‚ü®m|A|n‚ü©= an‚ü®m|n‚ü©, (1.288)
which implies that

a‚àó
m ‚àían

‚ü®m|n‚ü©= 0.
(1.289)
45

LINEAR ALGEBRA
But by (1.287), the eigenvalues am are real, and so we have
(am ‚àían) ‚ü®m|n‚ü©= 0,
(1.290)
which tells us that when the eigenvalues are different, the eigenvectors are
orthogonal. In the absence of a symmetry, all n eigenvalues usually are different,
and so the eigenvectors usually are mutually orthogonal.
When two or more eigenvectors |nŒ±‚ü©of a hermitian matrix have the same
eigenvalue an, their eigenvalues are said to be degenerate. In this case, any linear
combination of the degenerate eigenvectors also will be an eigenvector with the
same eigenvalue an
A

Œ±‚ààD
cŒ±|nŒ±‚ü©

= an

Œ±‚ààD
cŒ±|nŒ±‚ü©

(1.291)
where D is the set of labels Œ± of the eigenvectors with the same eigenvalue. If
the degenerate eigenvectors |nŒ±‚ü©are linearly independent, then we may use the
Gramm‚ÄìSchmidt procedure (1.108‚Äì1.118) to choose the coefÔ¨Åcients cŒ± so as to
construct degenerate eigenvectors that are orthogonal to each other and to the
nondegenerate eigenvectors. We then may normalize these mutually orthogonal
eigenvectors.
But two related questions arise. Are the degenerate eigenvectors |nŒ±‚ü©linearly
independent? And if so, what orthonormal linear combinations of them should
we choose for a given physical problem? Let‚Äôs consider the second question Ô¨Årst.
We know (section 1.16) that unitary transformations preserve the orthonor-
mality of a basis. Any unitary transformation that commutes with the matrix A
[A, U] = 0
(1.292)
maps each set of orthonormal degenerate eigenvectors of A into another set of
orthonormal degenerate eigenvectors of A with the same eigenvalue because
AU|nŒ±‚ü©= UA|nŒ±‚ü©= an U|nŒ±‚ü©.
(1.293)
So there‚Äôs a huge spectrum of choices for the orthonormal degenerate eigenvec-
tors of A with the same eigenvalue. What is the right set for a given physical
problem?
A sensible way to proceed is to add to the matrix A a second hermitian matrix
B multiplied by a tiny, real scale factor œµ
A(œµ) = A + œµB.
(1.294)
The matrix B must completely break whatever symmetry led to the degeneracy
in the eigenvalues of A. Ideally, the matrix B should be one that represents a
modiÔ¨Åcation of A that is physically plausible and relevant to the problem at
46

1.28 HERMITIAN MATRICES
hand. The hermitian matrix A(œµ) then will have N different eigenvalues an(œµ)
and N orthonormal nondegenerate eigenvectors
A(œµ)|nŒ≤, œµ‚ü©= anŒ≤(œµ)|nŒ≤, œµ‚ü©.
(1.295)
These eigenvectors |nŒ≤, œµ‚ü©of A(œµ) are orthogonal to each other
‚ü®nŒ≤, œµ|nŒ≤‚Ä≤, œµ‚ü©= Œ¥Œ≤,Œ≤‚Ä≤
(1.296)
and to the eigenvectors of A(œµ) with other eigenvalues, and they remain so as
we take the limit
|nŒ≤‚ü©= lim
œµ‚Üí0 |nŒ≤, œµ‚ü©.
(1.297)
We may choose them as the orthogonal degenerate eigenvectors of A. Since one
always may Ô¨Ånd a crooked hermitian matrix B that breaks any particular sym-
metry, it follows that every N √óN hermitian matrix A possesses N orthonormal
eigenvectors, which are complete in the vector space in which A acts. (Any N lin-
early independent vectors span their N-dimensional vector space, as explained
in section 1.9.)
Now let‚Äôs return to the Ô¨Årst question and again show that an N√óN hermitian
matrix has N orthogonal eigenvectors. To do this, we‚Äôll Ô¨Årst show that the space
of vectors orthogonal to an eigenvector |n‚ü©of a hermitian operator A
A|n‚ü©= Œª|n‚ü©
(1.298)
is invariant under the action of A ‚Äì that is, ‚ü®n|y‚ü©= 0 implies ‚ü®n|A|y‚ü©= 0. We use
successively the deÔ¨Ånition of A‚Ä†, the hermiticity of A, the eigenvector equation
(1.298), the deÔ¨Ånition of the inner product, and the reality of the eigenvalues of
a hermitian matrix:
‚ü®n|A|y‚ü©= ‚ü®A‚Ä†n|y‚ü©= ‚ü®An|y‚ü©= ‚ü®Œªn|y‚ü©= ¬ØŒª‚ü®n|y‚ü©= Œª‚ü®n|y‚ü©= 0.
(1.299)
Thus the space of vectors orthogonal to an eigenvector of a hermitian operator
A is invariant under the action of that operator.
Now a hermitian operator A acting on an N-dimensional vector space S is
represented by an N√óN hermitian matrix, and so it has at least one eigenvector
|1‚ü©. The subspace of S consisting of all vectors orthogonal to |1‚ü©is an (N ‚àí1)-
dimensional vector space SN‚àí1 that is invariant under the action of A. On this
space SN‚àí1, the operator A is represented by an (N ‚àí1) √ó (N ‚àí1) hermitian
matrix AN‚àí1. This matrix has at least one eigenvector |2‚ü©. The subspace of SN‚àí1
consisting of all vectors orthogonal to |2‚ü©is an (N ‚àí2)-dimensional vector
space SN‚àí2 that is invariant under the action of A. On SN‚àí2, the operator A is
represented by an (N ‚àí2) √ó (N ‚àí2) hermitian matrix AN‚àí2, which has at least
one eigenvector |3‚ü©. By construction, the vectors |1‚ü©, |2‚ü©, and |3‚ü©are mutually
orthogonal. Continuing in this way, we see that A has N orthogonal eigenvectors
|k‚ü©for k = 1, 2, . . . , N. Thus no hermitian matrix is defective.
47

LINEAR ALGEBRA
The N orthogonal eigenvectors |k‚ü©of an N √ó N matrix A can be normalized
and used to write the N √ó N identity operator I as
I =
N

k=1
|k‚ü©‚ü®k|.
(1.300)
On multiplying from the left by the matrix A, we Ô¨Ånd
A = AI = A
N

k=1
|k‚ü©‚ü®k| =
N

k=1
ak|k‚ü©‚ü®k|,
(1.301)
which is the diagonal form of the hermitian matrix A. This expansion of A
as a sum over outer products of its eigenstates multiplied by their eigenvalues
exhibits the possible values ak of the physical quantity represented by the matrix
A when selective, nondestructive measurements |k‚ü©‚ü®k| of the quantity A are
done.
The hermitian matrix A is diagonal in the basis of its eigenstates |k‚ü©
Akj = ‚ü®k|A|j‚ü©= akŒ¥kj.
(1.302)
But in any other basis |Œ±k‚ü©, the matrix A appears as
Ak‚Ñì= ‚ü®Œ±k|A|Œ±‚Ñì‚ü©=
N

n=1
‚ü®Œ±k|n‚ü©an‚ü®n|Œ±‚Ñì‚ü©.
(1.303)
The unitary matrix Ukn = ‚ü®Œ±k|n‚ü©relates the matrix Ak‚Ñìin an arbitrary basis
to its diagonal form A = UA(d)U‚Ä† in which A(d) is the diagonal matrix A(d)
nm =
an Œ¥nm. An arbitrary N√óN hermitian matrix A can be diagonalized by a unitary
transformation.
A matrix that is real and symmetric is hermitian; so is one that is imagi-
nary and antisymmetric. A real, symmetric matrix R can be diagonalized by
an orthogonal transformation
R = O R(d)OT
(1.304)
in which the matrix O is a real unitary matrix, that is, an orthogonal matrix
(1.168).
Example 1.39 (The seesaw mechanism)
Suppose we wish to Ô¨Ånd the eigenvalues
of the real, symmetric mass matrix
M =
0
m
m
M

(1.305)
in which m is an ordinary mass and M is a huge mass. The eigenvalues Œº of
this hermitian mass matrix satisfy det (M ‚àíŒºI) = Œº(Œº ‚àíM) ‚àím2 = 0 with
48

1.28 HERMITIAN MATRICES
solutions Œº¬± =

M ¬±

M2 + 4m2

/2. The larger mass Œº+ ‚âàM + m2/M is
approximately the huge mass M and the smaller mass Œº‚àí‚âà‚àím2/M is very
tiny. The physical mass of a fermion is the absolute value of its mass parameter,
here m2/M.
The product of the two eigenvalues is the constant Œº+Œº‚àí= det M = ‚àím2
so as Œº‚àígoes down, Œº+ must go up. In 1975, Gell-Mann, Ramond, Slansky,
and Jerry Stephenson invented this ‚Äúseesaw‚Äù mechanism as an explanation of
why neutrinos have such small masses, less than 1 eV/c2. If mc2 = 10 MeV, and
Œº‚àíc2 ‚âà0.01 eV, which is a plausible light-neutrino mass, then the rest energy of
the huge mass would be Mc2 = 107 GeV. This huge mass would point at new
physics, beyond the standard model. Yet the small masses of the neutrinos may
be related to the weakness of their interactions.
If we return to the orthogonal transformation (1.304) and multiply column
‚Ñìof the matrix O and row ‚Ñìof the matrix O
T by

|R(d)
‚Ñì|, then we arrive at the
congruency transformation of Sylvester‚Äôs theorem
R = C ÀÜR(d)C
T
(1.306)
in which the diagonal entries ÀÜR(d)
‚Ñì
are either ¬±1 or 0 because the matrices C and
CT have absorbed the moduli |R(d)
‚Ñì|.
Example 1.40 (Equivalence principle)
If G is a real, symmetric 4 √ó 4 matrix
then there‚Äôs a real 4 √ó 4 matrix D = C
T‚àí1 such that
Gd = DTGD =
‚éõ
‚éú‚éú‚éù
g1
0
0
0
0
g2
0
0
0
0
g3
0
0
0
0
g4
‚éû
‚éü‚éü‚é†
(1.307)
in which the diagonal entries gi are ¬±1 or 0. Thus there‚Äôs a real 4 √ó 4 matrix D
that casts the real nonsingular symmetric metric gik of space-time at any given
point into the diagonal metric Œ∑j‚Ñìof Ô¨Çat space-time by the congruence
gd = DTgD =
‚éõ
‚éú‚éú‚éù
‚àí1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
‚éû
‚éü‚éü‚é†= Œ∑.
(1.308)
Usually one needs different Ds at different points. Since one can implement the
congruence by changing coordinates, it follows that in any gravitational Ô¨Åeld,
one may choose free-fall coordinates in which all physical laws take the same
form as in special relativity without acceleration or gravitation at least over
suitably small volumes of space-time (section 11.39).
49

LINEAR ALGEBRA
1.29 Normal matrices
The largest set of matrices that can be diagonalized by a unitary transformation
is the set of normal matrices. These are square matrices that commute with their
adjoints
[A, A‚Ä†] = AA‚Ä† ‚àíA‚Ä†A = 0.
(1.309)
This broad class of matrices includes not only hermitian matrices but also
unitary matrices since
[U, U‚Ä†] = UU‚Ä† ‚àíU‚Ä†U = I ‚àíI = 0.
(1.310)
To see why a normal matrix can be diagonalized by a unitary transformation,
let us consider an N√óN normal matrix V which (since it is square (section 1.25))
has N eigenvectors |n‚ü©with eigenvalues vn
(V ‚àívnI) |n‚ü©= 0.
(1.311)
The square of the norm (1.80) of this vector must vanish
‚à•(V ‚àívnI) |n‚ü©‚à•2= ‚ü®n| (V ‚àívnI)‚Ä† (V ‚àívnI) |n‚ü©= 0.
(1.312)
But since V is normal, we also have
‚ü®n| (V ‚àívnI)‚Ä† (V ‚àívnI) |n‚ü©= ‚ü®n| (V ‚àívnI) (V ‚àívnI)‚Ä† |n‚ü©.
(1.313)
So the square of the norm of the vector

V‚Ä† ‚àív‚àó
nI

|n‚ü©= (V ‚àívnI)‚Ä† |n‚ü©also
vanishes ‚à•

V‚Ä† ‚àív‚àó
nI

|n‚ü©‚à•2= 0, which tells us that |n‚ü©also is an eigenvector of
V‚Ä† with eigenvalue v‚àó
n
V‚Ä†|n‚ü©= v‚àó
n|n‚ü©
and so
‚ü®n|V = vn‚ü®n|.
(1.314)
If now |m‚ü©is an eigenvector of V with eigenvalue vm
V|m‚ü©= vm|m‚ü©= 0
(1.315)
then we have
‚ü®n|V|m‚ü©= vm‚ü®n|m‚ü©
(1.316)
and from (1.314)
‚ü®n|V|m‚ü©= vn‚ü®n|m‚ü©.
(1.317)
Subtracting (1.316) from (1.317), we get
(vn ‚àívm) ‚ü®m|n‚ü©= 0,
(1.318)
which shows that any two eigenvectors of a normal matrix V with different
eigenvalues are orthogonal.
Usually, all N eigenvalues of an N √ó N normal matrix are different. In this
case, all the eigenvectors are orthogonal and may be individually normalized.
50

1.29 NORMAL MATRICES
But even when a set D of eigenvectors has the same (degenerate) eigenvalue,
one may use the argument (1.291‚Äì1.297) to Ô¨Ånd a suitable set of orthonor-
mal eigenvectors with that eigenvalue. Thus every N √ó N normal matrix has
N orthonormal eigenvectors. It follows then from the argument of equations
(1.300‚Äì1.303) that every N √ó N normal matrix V can be diagonalized by an
N √ó N unitary matrix U
V = UV(d)U‚Ä†
(1.319)
whose nth column Ukn = ‚ü®Œ±k|n‚ü©is the eigenvector |n‚ü©in the arbitrary basis |Œ±k‚ü©
of the matrix Vk‚Ñì= ‚ü®Œ±k|V|Œ±‚Ñì‚ü©as in (1.303).
Since the eigenstates |n‚ü©of a normal matrix A
A|n‚ü©= an|n‚ü©
(1.320)
are complete and orthonormal, we can write the identity operator I as
I =
N

n=1
|n‚ü©‚ü®n|.
(1.321)
The product AI is A itself, so
A = AI = A
N

n=1
|n‚ü©‚ü®n| =
N

n=1
an |n‚ü©‚ü®n|.
(1.322)
It follows therefore that if f is a function, then f (A) is
f (A) =
N

n=1
f (an) |n‚ü©‚ü®n|,
(1.323)
which is simpler than the expression (1.273) for an arbitrary nondefective
matrix. This is a good way to think about functions of normal matrices.
Example 1.41
How do we handle the operator exp(‚àíiHt/¬Øh) that translates
states in time by t? The hamiltonian H is hermitian and so is normal. Its
orthonormal eigenstates |n‚ü©are the energy levels En
H|n‚ü©= En|n‚ü©.
(1.324)
So we apply (1.323) with A ‚ÜíH and get
e‚àíiHt/¬Øh =
N

n=1
e‚àíiEnt/¬Øh |n‚ü©‚ü®n|,
(1.325)
51

LINEAR ALGEBRA
which lets us compute the time evolution of any state |œà‚ü©as
e‚àíiHt/¬Øh|œà‚ü©=
N

n=1
e‚àíiEnt/¬Øh |n‚ü©‚ü®n|œà‚ü©
(1.326)
if we know the eigenstates |n‚ü©and eigenvalues En of the hamiltonian H.
The determinant |V| of a normal matrix V satisÔ¨Åes the identities
|V| = exp [Tr(ln V)] ,
ln |V| = Tr(ln V),
and
Œ¥ ln |V| = Tr

V‚àí1Œ¥V

.
(1.327)
1.30 Compatible normal matrices
Two normal matrices A and B that commute
[A, B ] ‚â°AB ‚àíBA = 0
(1.328)
are said to be compatible. Since these operators are normal, they have complete
sets of orthonormal eigenvectors. If |u‚ü©is an eigenvector of A with eigenvalue
z, then so is B|u‚ü©since
AB|u‚ü©= BA|u‚ü©= Bz|u‚ü©= z B|u‚ü©.
(1.329)
We have seen that any normal matrix A can be written as a sum (1.322) of outer
products
A =
N

n=1
|an‚ü©an‚ü®an|
(1.330)
of its orthonormal eigenvectors |an‚ü©, which are complete in the N-dimensional
vector space S on which A acts. Suppose now that the eigenvalues an of A are
nondegenerate, and that B is another normal matrix acting on S and that the
matrices A and B are compatible. Then in the basis provided by the eigenvectors
(or eigenstates) |an‚ü©of the matrix A, the matrix B must satisfy
0 = ‚ü®an|AB ‚àíBA|ak‚ü©= (an ‚àíak) ‚ü®an|B|ak‚ü©,
(1.331)
which says that ‚ü®an|B|ak‚ü©is zero unless an = ak. Thus if the eigenvalues an of
the operator A are nondegenerate, then the operator B is diagonal
B = IBI =
N

n=1
|an‚ü©‚ü®an|B
N

k=1
|ak‚ü©‚ü®ak| =
N

n=1
|an‚ü©‚ü®an|B|an‚ü©‚ü®an|
(1.332)
52

1.30 COMPATIBLE NORMAL MATRICES
in the |an‚ü©basis. Moreover B maps each eigenket |ak‚ü©of A into
B|ak‚ü©=
N

n=1
|an‚ü©‚ü®an|B|an‚ü©‚ü®an|ak‚ü©=
N

n=1
|an‚ü©‚ü®an|B|an‚ü©Œ¥nk = ‚ü®ak|B|ak‚ü©|ak‚ü©,
(1.333)
which says that each eigenvector |ak‚ü©of the matrix A also is an eigenvec-
tor of the matrix B with eigenvalue ‚ü®ak|B|ak‚ü©. Thus two compatible normal
matrices can be simultaneously diagonalized if one of them has nondegenerate
eigenvalues.
If A‚Äôs eigenvalues an are degenerate, each eigenvalue an may have dn orthonor-
mal eigenvectors |an, k‚ü©for k = 1, . . . , dn. In this case, the matrix elements
‚ü®an, k|B|am, k‚Ä≤‚ü©of B are zero unless the eigenvalues are the same, an = am.
The matrix representing the operator B in this basis consists of square, dn √ó dn,
normal submatrices ‚ü®an, k|B|an, k‚Ä≤‚ü©arranged along its main diagonal; it is said
to be in block-diagonal form. Since each submatrix is a dn √ó dn normal matrix,
we may Ô¨Ånd linear combinations |an, bk‚ü©of the degenerate eigenvectors |an, k‚ü©
that are orthonormal eigenvectors of both compatible operators
A|an, bk‚ü©= an|an, bk‚ü©
and
B|an, bk‚ü©= bk|an, bk‚ü©.
(1.334)
Thus one can simultaneously diagonalize any two compatible operators.
The converse also is true: if the operators A and B can be simultaneously
diagonalized as in (1.334), then they commute
AB|an, bk‚ü©= Abk|an, bk‚ü©= anbk|an, bk‚ü©= anB|an, bk‚ü©= BA|an, bk‚ü©
and so are compatible. Normal matrices can be simultaneously diagonalized if
and only if they are compatible, that is, if and only if they commute.
In quantum mechanics, compatible hermitian operators represent physical
observables that can be measured simultaneously to arbitrary precision (in
principle). A set of compatible hermitian operators {A, B, C, . . . } is said to
be complete if to every set of eigenvalues {an, bk, c‚Ñì, . . . } there is only a single
eigenvector |an, bk, c‚Ñì, . . .‚ü©.
Example 1.42 (Compatible photon observables)
The state of a photon is com-
pletely characterized by its momentum and its angular momentum about its
direction of motion. For a photon, the momentum operator P and the dot-
product J ¬∑ P of the angular momentum J with the momentum form a complete
set of compatible hermitian observables. Incidentally, because its mass is zero,
the angular momentum J of a photon about its direction of motion can have
only two values ¬±¬Øh, which correspond to its two possible states of circular
polarization.
53

LINEAR ALGEBRA
Example 1.43 (Thermal density operator)
A density operator œÅ is the most gen-
eral description of a quantum-mechanical system. It is hermitian, positive, and
of unit trace. Since it is hermitian, it can be diagonalized (section 1.28)
œÅ =

n
|n‚ü©‚ü®n|œÅ|n‚ü©‚ü®n|
(1.335)
and its eigenvalues œÅn = ‚ü®n|œÅ|n‚ü©are real. Each œÅn is the probability that the
system is in the state |n‚ü©and so is nonnegative. The unit-trace rule

n
œÅn = 1
(1.336)
ensures that these probabilities add up to one ‚Äì the system is in some state.
The mean value of an operator F is the trace, ‚ü®F‚ü©= Tr(œÅF). So the aver-
age energy E is the trace, E = ‚ü®H‚ü©= Tr(œÅH). The entropy operator S is the
negative logarithm of the density operator multiplied by Boltzmann‚Äôs constant
S = ‚àík ln œÅ, and the mean entropy S is S = ‚ü®S‚ü©= ‚àíkTr(œÅ ln œÅ).
A density operator that describes a system in thermal equilibrium at a con-
stant temperature T is time independent and so commutes with the hamiltonian,
[œÅ, H] = 0. Since œÅ and H commute, they are compatible operators (1.328),
and so they can be simultaneously diagonalized. Each eigenstate |n‚ü©of œÅ is an
eigenstate of H; its energy En is its eigenvalue, H|n‚ü©= En|n‚ü©.
If we have no information about the state of the system other than its mean
energy E, then we take œÅ to be the density operator that maximizes the mean
entropy S while respecting the constraints
c1 =

n
œÅn ‚àí1 = 0
and
c2 = Tr(œÅH) ‚àíE = 0.
(1.337)
We introduce two Lagrange multipliers (section 1.23) and maximize the uncon-
strained function
L(œÅ, Œª1, Œª2) = S ‚àíŒª1 c1 ‚àíŒª2 c2
= ‚àík

n
œÅn ln œÅn ‚àíŒª1

n
œÅn ‚àí1

‚àíŒª2

n
œÅnEn ‚àíE

(1.338)
by setting its derivatives with respect to œÅn, Œª1, and Œª2 equal to zero
‚àÇL
‚àÇœÅn
= ‚àík (ln œÅn + 1) ‚àíŒª1 ‚àíŒª2En = 0,
(1.339)
‚àÇL
‚àÇŒª1
=

n
œÅn ‚àí1 = 0,
(1.340)
‚àÇL
‚àÇŒª2
=

n
œÅnEn ‚àíE = 0.
(1.341)
The Ô¨Årst (1.339) of these conditions implies that
œÅn = exp [‚àí(Œª1 + Œª2En + k)/k] .
(1.342)
54

1.31 THE SINGULAR-VALUE DECOMPOSITION
We satisfy the second condition (1.340) by choosing Œª1 so that
œÅn =
exp(‚àíŒª2En/k)

n exp(‚àíŒª2En/k).
(1.343)
Setting Œª2 = 1/T, we deÔ¨Åne the temperature T so that œÅ satisÔ¨Åes the third
condition (1.341). Its eigenvalue œÅn then is
œÅn =
exp(‚àíEn/kT)

n exp(‚àíEn/kT).
(1.344)
In terms of the inverse temperature Œ≤ ‚â°1/(kT), the density operator is
œÅ =
e‚àíŒ≤H
Tr

e‚àíŒ≤H,
(1.345)
which is the Boltzmann distribution.
1.31 The singular-value decomposition
Every complex M √ó N rectangular matrix A is the product of an M √ó M uni-
tary matrix U, an M √ó N rectangular matrix  that is zero except on its main
diagonal, which consists of its nonnegative singular values Sk, and an N √ó N
unitary matrix V‚Ä†
A = U  V‚Ä†.
(1.346)
This singular-value decomposition (SVD) is a key theorem of matrix algebra.
Suppose A is a linear operator that maps vectors in an N-dimensional vec-
tor space VN into vectors in an M-dimensional vector space VM. The spaces
VN and VM will have inÔ¨Ånitely many orthonormal bases {|n, a‚ü©‚ààVN} and
{|m, b‚ü©‚ààVM} labeled by continuous parameters a and b. Each pair of bases
provides a resolution of the identity operator IN for VN and IM for VM
IN =
N

n=1
|n, a‚ü©‚ü®n, a|
and
IM =
M

m=1
|m, b‚ü©‚ü®m, b|.
(1.347)
These identity operators give us many ways of writing the linear operator A
A = IMAIN =
M

m=1
N

n=1
|m, b‚ü©‚ü®m, b|A|n, a‚ü©‚ü®n, a|,
(1.348)
in which the ‚ü®m, b|A|n, a‚ü©are the elements of a complex M √ó N matrix. The
singular-value decomposition of the linear operator A is a choice among all
these expressions for IN and IM that expresses A as
A =
min(M,N)

k=1
|Uk‚ü©Sk‚ü®Vk|
(1.349)
55

LINEAR ALGEBRA
in which the min(M, N) singular values Sk are nonnegative
Sk ‚â•0.
(1.350)
Let‚Äôs use the notation |An‚ü©‚â°A|n‚ü©for the image of a vector |n‚ü©in an
orthonormal basis {|n‚ü©} of VN under the map A. We seek a special orthonormal
basis {|n‚ü©} of VN that has the property that the vectors |An‚ü©are orthogonal. This
special basis {|n‚ü©} of VN is the set of N orthonormal eigenstates of the N √ó N
(nonnegative) hermitian operator A‚Ä†A
A‚Ä†A|n‚ü©= en|n‚ü©.
(1.351)
For since A|n‚Ä≤‚ü©= |An‚Ä≤‚ü©and A‚Ä†A|n‚ü©= en|n‚ü©, it follows that
‚ü®An‚Ä≤|An‚ü©= ‚ü®n‚Ä≤|A‚Ä†A|n‚ü©= en‚ü®n‚Ä≤|n‚ü©= enŒ¥n‚Ä≤n,
(1.352)
which shows that the vectors |An‚ü©are orthogonal and that their eigenval-
ues en = ‚ü®An|An‚ü©are nonnegative. This is the essence of the singular-value
decomposition.
If N = M, so that matrices ‚ü®m, b|A|n, a‚ü©representing the linear operator A
are square, then the N = M singular values Sn are the nonnegative square-roots
of the eigenvalues en
Sn = ‚àöen =

‚ü®An|An‚ü©‚â•0.
(1.353)
We therefore may normalize each vector |An‚ü©whose singular value Sn is
positive as
|mn‚ü©= 1
Sn
|An‚ü©
for
Sn > 0
(1.354)
so that the vectors {|mn‚ü©} with positive singular values are orthonormal
‚ü®mn‚Ä≤|mn‚ü©= Œ¥n‚Ä≤,n.
(1.355)
If only P < N of the singular values are positive, then we may augment this
set of P vectors {|mn‚ü©} with N ‚àíP = M ‚àíP new normalized vectors |mn‚Ä≤‚ü©
that are orthogonal to each other and to the P vectors deÔ¨Åned by (1.354) (with
positive singular values Sn > 0) so that the set of N = M vectors {|mn‚ü©, |mn‚Ä≤}‚ü©
are complete and orthonormal in the space VM=N.
If N > M, then A maps the N-dimensional space VN into the smaller
M-dimensional space VM, and so A must annihilate N ‚àíM basis vectors
A|n‚Ä≤‚ü©= 0
for
M < n‚Ä≤ ‚â§N.
(1.356)
In this case, there are only M singular values Sn of which Z may be zero.
The Z vectors |An‚ü©= A|n‚ü©with vanishing Sns are vectors of length zero;
for these values of n, the matrix A maps the vector |n‚ü©to the zero vector. If
there are more than N ‚àíM zero-length vectors |An‚ü©= A|n‚ü©, then we must
56

1.31 THE SINGULAR-VALUE DECOMPOSITION
replace the extra ones by new normalized vectors |mn‚Ä≤‚ü©that are orthogonal to
each other and to the vectors deÔ¨Åned by (1.354) so that we have M orthonor-
mal vectors in the augmented set {|mn‚ü©, |mn‚Ä≤‚ü©}. These vectors then form a basis
for VM.
When N ‚â§M, there are only N singular values Sn of which Z may be zero.
If Z of the Sns vanish, then one must add Q = Z + M ‚àíN new normalized
vectors |mn‚Ä≤‚ü©that are orthogonal to each other and to the vectors deÔ¨Åned by
(1.354)
‚ü®mn‚Ä≤|mn‚ü©= 1
Sn
‚ü®mn‚Ä≤|A|n‚ü©= 0
for
n‚Ä≤ > N ‚àíZ
and
Sn > 0
(1.357)
so that we have M orthonormal vectors in the augmented set {|mn‚ü©, |mn‚Ä≤‚ü©}.
These vectors then form a basis for VM.
In both cases, N > M and M ‚â•N, there are min(M, N) singular values, Z of
which may be zero. We may choose the new vectors {|mn‚Ä≤‚ü©} arbitrarily ‚Äì as long
as the augmented set {|mn‚ü©, |mn‚Ä≤‚ü©} includes all the vectors deÔ¨Åned by (1.354) and
forms an orthonormal basis for VM.
We now have two special orthonormal bases: the N N-dimensional eigenvec-
tors |n‚ü©‚ààVN that satisfy (1.351) and the M M-dimensional vectors |mn‚ü©‚ààVM.
To make the singular-value decomposition of the linear operator A, we choose
as the identity operators IN for the N-dimensional space VN and IM for the
M-dimensional space VM the sums
IN =
N

n=1
|n‚ü©‚ü®n|
and
IM =
M

n‚Ä≤=1
|mn‚Ä≤‚ü©‚ü®mn‚Ä≤|.
(1.358)
The singular-value decomposition of A then is
A = IMAIN =
M

n‚Ä≤=1
|mn‚Ä≤‚ü©‚ü®mn‚Ä≤|A
N

n=1
|n‚ü©‚ü®n|.
(1.359)
There are min(M, N) singular values Sn, all nonnegative. For the positive sin-
gular values, equations (1.352 & 1.354) show that the matrix element ‚ü®mn‚Ä≤|A|n‚ü©
vanishes unless n‚Ä≤ = n
‚ü®mn‚Ä≤|A|n‚ü©= 1
Sn‚Ä≤ ‚ü®An‚Ä≤|An‚ü©= Sn‚Ä≤ Œ¥n‚Ä≤n.
(1.360)
For the Z vanishing singular values, equation (1.353) shows that A|n‚ü©= 0
and so
‚ü®mn‚Ä≤|A|n‚ü©= 0.
(1.361)
Thus only the min(M, N)‚àíZ singular values that are positive contribute to the
singular-value decomposition (1.359). If N > M, then there can be at most M
nonzero eigenvalues en. If N ‚â§M, there can be at most N nonzero ens. The Ô¨Ånal
57

LINEAR ALGEBRA
form of the singular-value decomposition then is a sum of dyadics weighted by
the positive singular values
A =
min(M,N)

n=1
|mn‚ü©Sn ‚ü®n| =
min(M,N)‚àíZ

n=1
|mn‚ü©Sn ‚ü®n|.
(1.362)
The vectors |mn‚ü©and |n‚ü©respectively are the left and right singular vectors. The
nonnegative numbers Sn are the singular values.
The linear operator A maps the min(M, N) right singular vectors |n‚ü©into the
min(M, N) left singular vectors Sn|mn‚ü©scaled by their singular values
A|n‚ü©= Sn|mn‚ü©
(1.363)
and its adjoint A‚Ä† maps the min(M, N) left singular vectors |mn‚ü©into the
min(M, N) right singular vectors |n‚ü©scaled by their singular values
A‚Ä†|mn‚ü©= Sn|n‚ü©.
(1.364)
The N-dimensional vector space VN is the domain of the linear operator A.
If N > M, then A annihilates (at least) N ‚àíM of the basis vectors |n‚ü©. The
null space or kernel of A is the space spanned by the basis vectors |n‚ü©that A
annihilates. The vector space spanned by the left singular vectors |mn‚ü©with
nonzero singular values Sn > 0 is the range or image of A. It follows from the
singular-value decomposition (1.362) that the dimension N of the domain is
equal to the dimension of the kernel N ‚àíM plus that of the range M, a result
called the rank-nullity theorem.
Incidentally, the vectors |mn‚ü©are the eigenstates of the hermitian matrix
A A‚Ä† as one may see from the explicit product of the expansion (1.362) with
its adjoint
A A‚Ä† =
min(M,N)

n=1
|mn‚ü©Sn ‚ü®n|
min(M,N)

n‚Ä≤=1
|n‚Ä≤‚ü©Sn‚Ä≤ ‚ü®mn‚Ä≤|
=
min(M,N)

n=1
min(M,N)

n‚Ä≤=1
|mn‚ü©Sn Œ¥nn‚Ä≤Sn‚Ä≤ ‚ü®mn‚Ä≤|
=
min(M,N)

n=1
|mn‚ü©S2
n‚ü®mn|,
(1.365)
which shows that |mn‚ü©is an eigenvector of A A‚Ä† with eigenvalue en = S2
n,
A A‚Ä†|mn‚ü©= S2
n|mn‚ü©.
(1.366)
The SVD expansion (1.362) usually is written as a product of three explicit
matrices, A = UV‚Ä†. The middle matrix  is an M √ó N matrix with the
min(M, N) singular values Sn = ‚àöen on its main diagonal and zeros elsewhere.
58

1.31 THE SINGULAR-VALUE DECOMPOSITION
By convention, one writes the Sn in decreasing order with the biggest Sn as entry
11. The Ô¨Årst matrix U and the third matrix V‚Ä† depend upon the bases one uses
to represent the linear operator A. If these basis vectors are |Œ±k‚ü©and |Œ≤‚Ñì‚ü©, then
Ak‚Ñì= ‚ü®Œ±k|A|Œ≤‚Ñì‚ü©=
min(M,N)

n=1
‚ü®Œ±k|mn‚ü©Sn ‚ü®n|Œ≤‚Ñì‚ü©
(1.367)
so that the k, nth entry in the matrix U is Ukn = ‚ü®Œ±k|mn‚ü©. The columns of the
matrix U are the left singular vectors of the matrix A:
‚éõ
‚éú‚éú‚éú‚éù
U1n
U2n
...
UMn
‚éû
‚éü‚éü‚éü‚é†=
‚éõ
‚éú‚éú‚éú‚éù
‚ü®Œ±1|mn‚ü©
‚ü®Œ±2|mn‚ü©
...
‚ü®Œ±M|mn‚ü©
‚éû
‚éü‚éü‚éü‚é†.
(1.368)
Similarly, the n, ‚Ñìth entry of the matrix V‚Ä† is

V‚Ä†
n,‚Ñì= ‚ü®n|Œ≤‚Ñì‚ü©. Thus V‚Ñì,n =

V T
n,‚Ñì= ‚ü®n|Œ≤‚Ñì‚ü©‚àó= ‚ü®Œ≤‚Ñì|n‚ü©. The columns of the matrix V are the right singular
vectors of the matrix A
‚éõ
‚éú‚éú‚éú‚éù
V1n
V2n
...
VNn
‚éû
‚éü‚éü‚éü‚é†=
‚éõ
‚éú‚éú‚éú‚éù
‚ü®Œ≤1|n‚ü©
‚ü®Œ≤2|n‚ü©
...
‚ü®Œ≤N|n‚ü©
‚éû
‚éü‚éü‚éü‚é†.
(1.369)
Since the columns of U and of V respectively are M and N orthonormal vec-
tors, both of these matrices are unitary, that is U‚Ä†U = IM and V‚Ä†V = IN are
the M √ó M and N √ó N identity matrices. The matrix form of the singular-value
decomposition of A then is
Ak‚Ñì=
M

m=1
N

n=1
UkmmnV‚Ä†
n‚Ñì=
min(M,N)

n=1
UknSnV‚Ä†
n‚Ñì
(1.370)
or in matrix notation
A = UV‚Ä†.
(1.371)
The usual statement of the SVD theorem is: Every M √ó N complex matrix A
can be written as the matrix product of an M √óM unitary matrix U, an M √óN
matrix  that is zero except for its min(M, N) nonnegative diagonal elements,
and an N √ó N unitary matrix V‚Ä†
A = U  V‚Ä†.
(1.372)
The Ô¨Årst min(M, N) diagonal elements of S are the singular values Sk. They
are real and nonnegative. The Ô¨Årst min(M, N) columns of U and V are the left
and right singular vectors of A. The last max(N ‚àíM, 0) + Z columns (1.369)
59

LINEAR ALGEBRA
of the matrix V span the null space or kernel of A, and the Ô¨Årst min(M, N) ‚àíZ
columns (1.368) of the matrix U span the range of A.
Example 1.44 (Singular-value decomposition of a 2 √ó 3 matrix)
If A is
A =
0
1
0
1
0
1

(1.373)
then the positive hermitian matrix A‚Ä†A is
A‚Ä†A =
‚éõ
‚éù
1
0
1
0
1
0
1
0
1
‚éû
‚é†.
(1.374)
The normalized eigenvectors and eigenvalues of A‚Ä†A are
|1‚ü©=
1
‚àö
2
‚éõ
‚éù
1
0
1
‚éû
‚é†, e1 = 2;
|2‚ü©=
‚éõ
‚éù
0
1
0
‚éû
‚é†, e2 = 1;
|3‚ü©=
1
‚àö
2
‚éõ
‚éù
‚àí1
0
1
‚éû
‚é†, e3 = 0.
(1.375)
The third eigenvalue e3 had to vanish because A is a 3 √ó 2 matrix.
The vector A|1‚ü©is (as a row vector) |A1‚ü©= A|1‚ü©= (0,
‚àö
2), and its norm is

‚ü®1|A‚Ä†A|1‚ü©=
‚àö
2, so the normalized vector |m1‚ü©is |m1‚ü©= |A1‚ü©/
‚àö
2 = (0, 1).
Similarly, the vector |m2‚ü©is |m2‚ü©= A|2‚ü©/

‚ü®2|A‚Ä†A|2‚ü©= (1, 0). The SVD of A
then is
A =
2

n=1
|mn‚ü©Sn‚ü®n| = UV‚Ä†
(1.376)
where Sn = ‚àöen. The unitary matrices Uk,n = ‚ü®Œ±k|mn‚ü©and Vk,n = ‚ü®Œ≤k|n‚ü©are
U =
0
1
1
0

and
V =
1
‚àö
2
‚éõ
‚éù
1
0
‚àí1
0
‚àö
2
0
1
0
1
‚éû
‚é†
(1.377)
and the diagonal matrix  is
 =
‚àö
2
0
0
0
1
0

.
(1.378)
So Ô¨Ånally the SVD of A = UV‚Ä† is
A =
0
1
1
0
 ‚àö
2
0
0
0
1
0
 1
‚àö
2
‚éõ
‚éù
1
0
1
0
‚àö
2
0
‚àí1
0
1
‚éû
‚é†.
(1.379)
The null space or kernel of A is the set of vectors that are real multiples c
NA =
c
‚àö
2
‚éõ
‚éù
‚àí1
0
1
‚éû
‚é†
(1.380)
of the third column of the matrix V displayed in (1.377).
60

1.31 THE SINGULAR-VALUE DECOMPOSITION
Example 1.45 (Matlab‚Äôs SVD)
Matlab‚Äôs command [U,S,V] = svd(X) performs
the singular-value decomposition of the matrix X. For instance
>> X = rand(3,3) + i*rand(3,3)
0.6551 + 0.2551i
0.4984 + 0.8909i
0.5853 + 0.1386i
X = 0.1626 + 0.5060i
0.9597 + 0.9593i
0.2238 + 0.1493i
0.1190 + 0.6991i
0.3404 + 0.5472i
0.7513 + 0.2575i
>> [U,S,V] = svd(X)
-0.3689 - 0.4587i
0.4056 - 0.2075i
0.4362 - 0.5055i
U = -0.3766 - 0.5002i -0.5792 - 0.2810i
0.0646 + 0.4351i
-0.2178 - 0.4626i
0.1142 + 0.6041i -0.5938 - 0.0901i
2.2335
0
0
S =
0
0.7172
0
0
0
0.3742
-0.4577
0.5749
0.6783
V = -0.7885 - 0.0255i -0.6118 - 0.0497i -0.0135 + 0.0249i
-0.3229 - 0.2527i
0.3881 + 0.3769i -0.5469 - 0.4900i.
The singular values are 2.2335, 0.7172, and 0.3742.
We may use the SVD to solve, when possible, the matrix equation
A |x‚ü©= |y‚ü©
(1.381)
for the N-dimensional vector |x‚ü©in terms of the M-dimensional vector |y‚ü©and
the M √ó N matrix A. Using the SVD expansion (1.362), we have
min(M,N)

n=1
|mn‚ü©Sn ‚ü®n|x‚ü©= |y‚ü©.
(1.382)
The orthonormality (1.355) of the vectors |mn‚ü©then tells us that
Sn ‚ü®n|x‚ü©= ‚ü®mn|y‚ü©.
(1.383)
If the singular value is positive Sn > 0 whenever ‚ü®mn|y‚ü©Ã∏= 0, then we may divide
by the singular value to get ‚ü®n|x‚ü©= ‚ü®mn|y‚ü©/Sn and so Ô¨Ånd the solution
|x‚ü©=
min(M,N)

n=1
‚ü®mn|y‚ü©
Sn
|n‚ü©.
(1.384)
But this solution is not always available or unique.
61

LINEAR ALGEBRA
For instance, if for some n‚Ä≤ the inner product ‚ü®mn‚Ä≤|y‚ü©Ã∏= 0 while the singular
value Sn‚Ä≤ = 0, then there is no solution to equation (1.381). This problem often
occurs when M > N.
Example 1.46
Suppose A is the 3 √ó 2 matrix
A =
‚éõ
‚éù
r1
p1
r2
p2
r3
p3
‚éû
‚é†
(1.385)
and the vector |y‚ü©is the cross-product |y‚ü©= L = r √ó p. Then no solution |x‚ü©
exists to the equation A|x‚ü©= |y‚ü©(unless r and p are parallel) because A|x‚ü©is a
linear combination of the vectors r and p while |y‚ü©= L is perpendicular to both
r and p.
Even when the matrix A is square, the equation (1.381) sometimes has no
solutions. For instance, if A is a square matrix that vanishes, A = 0, then (1.381)
has no solutions whenever |y‚ü©Ã∏= 0. And when N > M, as in for instance
a
b
c
d
e
f
 ‚éõ
‚éù
x1
x2
x3
‚éû
‚é†=
y1
y2

(1.386)
the solution (1.384) is never unique, for we may add to it any linear combination
of the vectors |n‚ü©that A annihilates for M < n ‚â§N
|x‚ü©=
min(M,N)

n=1
‚ü®mn|y‚ü©
Sn
|n‚ü©+
N

n=M+1
xn|n‚ü©.
(1.387)
These are the vectors |n‚ü©for M < n ‚â§N which A maps to zero since they do
not occur in the sum (1.362), which stops at n = min(M, N) < N.
Example 1.47 (The CKM matrix)
In the standard model, the mass matrix of
the d, s, and b quarks is a 3 √ó 3 complex, symmetric matrix M. Since M is
symmetric (M = MT), its adjoint is its complex conjugate, M‚Ä† = M‚àó. So the
right singular vectors |n‚ü©are the eigenstates of M‚àóM as in (1.351)
M‚àóM|n‚ü©= S2
n|n‚ü©
(1.388)
and the left singular vectors |mn‚ü©are the eigenstates of MM‚àóas in (1.366)
MM‚àó|mn‚ü©=

M‚àóM
‚àó|mn‚ü©= S2
n|mn‚ü©.
(1.389)
Thus the left singular vectors are just the complex conjugates of the right singu-
lar vectors, |mn‚ü©= |n‚ü©‚àó. But this means that the unitary matrix V is the complex
conjugate of the unitary matrix U, so the SVD of M is (Autonne, 1915)
62

1.32 THE MOORE‚ÄìPENROSE PSEUDOINVERSE
M = UUT.
(1.390)
The masses of the quarks then are the nonnegative singular values Sn along
the diagonal of the matrix . By redeÔ¨Åning the quark Ô¨Åelds, one may make the
(CKM) matrix U real ‚Äì except for a single complex phase, which causes a viola-
tion of charge-conjugation-parity (CP) symmetry. A similar matrix determines
the neutrino masses.
1.32 The Moore‚ÄìPenrose pseudoinverse
Although a matrix A has an inverse A‚àí1 if and only if it is square and has a
nonzero determinant, one may use the singular-value decomposition to make
a pseudoinverse A+ for an arbitrary M √ó N matrix A. If the singular-value
decomposition of the matrix A is
A = U  V‚Ä†
(1.391)
then the Moore‚ÄìPenrose pseudoinverse (Eliakim H. Moore, 1862‚Äì1932, Roger
Penrose, 1931‚Äì) is
A+ = V + U‚Ä†
(1.392)
in which + is the transpose of the matrix  with every nonzero entry
replaced by its inverse (and the zeros left as they are). One may show that the
pseudoinverse A+ satisÔ¨Åes the four relations
A A+ A = A
and
A+ A A+ = A+,

A A+‚Ä† = A A+
and

A+ A
‚Ä† = A+ A
(1.393)
and that it is the only matrix that does so.
Suppose that all the singular values of the M √ó N matrix A are positive. In
this case, if A has more rows than columns, so that M > N, then the product
AA+ is the N √ó N identity matrix IN
A+A = V‚Ä†+V = V‚Ä†INV = IN
(1.394)
and AA+ is an M √ó M matrix that is not the identity matrix IM. If instead A
has more columns than rows, so that N > M, then AA+ is the M √ó M identity
matrix IM
AA+ = U+U‚Ä† = UIMU‚Ä† = IM
(1.395)
63

LINEAR ALGEBRA
but A+A is an N √ó N matrix that is not the identity matrix IN. If the matrix A
is square with positive singular values, then it has a true inverse A‚àí1 which is
equal to its pseudoinverse
A‚àí1 = A+.
(1.396)
If the columns of A are linearly independent, then the matrix A‚Ä†A has an
inverse, and the pseudoinverse is
A+ =

A‚Ä†A
‚àí1
A‚Ä†.
(1.397)
The solution (1.220) to the complex least-squares method used this pseudoin-
verse.
If the rows of A are linearly independent, then the matrix AA‚Ä† has an inverse,
and the pseudoinverse is
A+ = A‚Ä† 
AA‚Ä†‚àí1
.
(1.398)
If both the rows and the columns of A are linearly independent, then the matrix
A has an inverse A‚àí1 which is its pseudoinverse
A‚àí1 = A+.
(1.399)
Example 1.48 (The pseudoinverse of a 2 √ó 3 matrix)
The pseudoinverse A+ of
the matrix A
A =
0
1
0
1
0
1

(1.400)
with singular-value decomposition (1.379) is
A+ = V + U‚Ä†
=
1
‚àö
2
‚éõ
‚éù
1
0
‚àí1
0
‚àö
2
0
1
0
1
‚éû
‚é†
‚éõ
‚éù
1/
‚àö
2
0
0
1
0
0
‚éû
‚é†
0
1
1
0

=
‚éõ
‚éù
0
1/2
1
0
0
1/2
‚éû
‚é†,
(1.401)
which satisÔ¨Åes the four conditions (1.393). The product A A+ gives the 2 √ó 2
identity matrix
A A+ =
0
1
0
1
0
1
 ‚éõ
‚éù
0
1/2
1
0
0
1/2
‚éû
‚é†=
1
0
0
1

,
(1.402)
64

1.33 THE RANK OF A MATRIX
which is an instance of (1.395). Moreover, the rows of A are linearly independent,
and so the simple rule (1.398) works:
A+ = A‚Ä† 
AA‚Ä†‚àí1
=
‚éõ
‚éù
1
0
0
1
1
0
‚éû
‚é†
‚éõ
‚éù
0
1
0
1
0
1
 ‚éõ
‚éù
1
0
0
1
1
0
‚éû
‚é†
‚éû
‚é†
‚àí1
=
‚éõ
‚éù
1
0
0
1
1
0
‚éû
‚é†
0
1
2
0
‚àí1
=
‚éõ
‚éù
1
0
0
1
1
0
‚éû
‚é†
0
1/2
1
0

=
‚éõ
‚éù
0
1/2
1
0
0
1/2
‚éû
‚é†,
(1.403)
which is (1.401).
The columns of the matrix A are not linearly independent, however, and so
the simple rule (1.397) fails. Thus the product A+A
A+A =
‚éõ
‚éù
0
1/2
1
0
0
1/2
‚éû
‚é†
0
1
0
1
0
1

= 1
2
‚éõ
‚éù
1
0
1
0
2
0
1
0
1
‚éû
‚é†
(1.404)
is not the 3 √ó 3 identity matrix which it would be if (1.397) held.
1.33 The rank of a matrix
Four equivalent deÔ¨Ånitions of the rank R(A) of an M √ó N matrix A are:
1 the number of its linearly independent rows,
2 the number of its linearly independent columns,
3 the number of its nonzero singular values, and
4 the number of rows in its biggest square nonsingular submatrix.
A matrix of rank zero has no nonzero singular values and so is zero.
Example 1.49 (Rank)
The 3 √ó 4 matrix
A =
‚éõ
‚éù
1
0
1
‚àí2
2
2
0
2
4
3
1
1
‚éû
‚é†
(1.405)
has three rows, so its rank can be at most 3. But twice the Ô¨Årst row added to
thrice the second row equals twice the third row or
2r1 + 3r2 ‚àí2r3 = 0
(1.406)
so R(A) ‚â§2. The Ô¨Årst two rows obviously are not parallel, so they are linearly
independent. Thus the number of linearly independent rows of A is 2, and so A
has rank 2.
65

LINEAR ALGEBRA
1.34 Software
Free, high-quality software for virtually all numerical problems in linear alge-
bra are available in LAPACK ‚Äì the Linear Algebra PACKage. The FORTRAN
version is available at the web-site www.netlib.org/lapack/ and the C++ version
at math.nist.gov/tnt/.
Matlab is a superb commercial program for numerical problems. A free
GNU version of it is available at www.gnu.org/software/octave/. Maple and
Mathematica are good commercial programs for symbolic problems.
1.35 The tensor/direct product
The tensor product (also called the direct product) is simple, but it can confuse
students if they see it for the Ô¨Årst time in a course on quantum mechanics.
The tensor product is used to describe composite systems, such as an angular
momentum composed of orbital and spin angular momenta.
If A is an M √ó N matrix with elements Aij and  is a K √ó L matrix with
elements Œ±Œ≤, then their direct product C = A ‚äó is an MK √ó NL matrix with
elements CiŒ±,jŒ≤ = Aij Œ±Œ≤. This direct-product matrix A ‚äó maps the vector
VjŒ≤ into the vector
WiŒ± =
N

j=1
L

Œ≤=1
CiŒ±,jŒ≤ VjŒ≤ =
N

j=1
L

Œ≤=1
Aij Œ±Œ≤ VjŒ≤.
(1.407)
In this sum, the second indices of A and  match those of the vector V. The
most important case is when both A and  are square matrices, as will be their
product C = A ‚äó. We‚Äôll focus on this case in the rest of this section.
The key idea here is that the direct product is a product of two operators that
act on two different spaces. The operator A acts on the space S spanned by the
N kets |i‚ü©, and the operator  acts on the space  spanned by the K kets |Œ±‚ü©.
Let us assume that both operators map into these spaces, so that we may write
them as
A = ISAIS =
N

i,j=1
|i‚ü©‚ü®i|A|j‚ü©‚ü®j|
(1.408)
and as
 = II =
K

Œ±,Œ≤=1
|Œ±‚ü©‚ü®Œ±||Œ≤‚ü©‚ü®Œ≤|.
(1.409)
Then the direct product C = A ‚äó
C = A ‚äó =
N

i,j=1
K

Œ±,Œ≤=1
|i‚ü©‚äó|Œ±‚ü©‚ü®i|A|j‚ü©‚ü®Œ±||Œ≤‚ü©‚ü®j| ‚äó‚ü®Œ≤|
(1.410)
66

1.35 THE TENSOR/DIRECT PRODUCT
acts on the direct product of the two vector spaces S ‚äó, which is spanned by
the direct-product kets |i, Œ±‚ü©= |i‚ü©|Œ±‚ü©= |i‚ü©‚äó|Œ±‚ü©.
In general, the direct-product space S ‚äó is much bigger than the spaces S
and . For although S ‚äó is spanned by the direct-product kets |i‚ü©‚äó|Œ±‚ü©, most
vectors in the space S ‚äó are of the form
|œà‚ü©=
N

i=1
K

Œ±=1
œà(i, Œ±)|i‚ü©‚äó|Œ±‚ü©
(1.411)
and not the direct product |s‚ü©‚äó|œÉ‚ü©of a pair of vectors |s‚ü©‚ààS and |œÉ‚ü©‚àà
|s‚ü©‚äó|œÉ‚ü©=
 N

i=1
si|i‚ü©

‚äó
 K

Œ±=1
œÉŒ±|Œ±‚ü©

=
N

i=1
K

Œ±=1
siœÉŒ±|i‚ü©‚äó|Œ±‚ü©.
(1.412)
Using the simpler notation |i, Œ±‚ü©for |i‚ü©‚äó|Œ±‚ü©, we may write the action of the
direct-product operator A ‚äó on the state
|œà‚ü©=
N

i=1
K

Œ±=1
|i, Œ±‚ü©‚ü®i, Œ±|œà‚ü©
(1.413)
as
(A ‚äó)|œà‚ü©=
N

i,j=1
K

Œ±,Œ≤=1
|i, Œ±‚ü©‚ü®i|A|j‚ü©‚ü®Œ±||Œ≤‚ü©‚ü®j, Œ≤|œà‚ü©.
(1.414)
Example 1.50 (States of the hydrogen atom)
Suppose the states |n, ‚Ñì, m‚ü©are the
eigenvectors of the hamiltonian H, the square L2 of the orbital angular momen-
tum L, and the third component of the orbital angular momentum L3 for a
hydrogen atom without spin:
H|n, ‚Ñì, m‚ü©= En|n, ‚Ñì, m‚ü©,
L2|n, ‚Ñì, m‚ü©= ¬Øh2‚Ñì(‚Ñì+ 1)|n, ‚Ñì, m‚ü©,
L3|n, ‚Ñì, m‚ü©= ¬Øhm|n, ‚Ñì, m‚ü©.
(1.415)
Suppose the states |œÉ‚ü©for œÉ = ¬± are the eigenstates of the third component S3
of the operator S that represents the spin of the electron
S3|œÉ‚ü©= œÉ ¬Øh
2|œÉ‚ü©.
(1.416)
Then the direct- or tensor-product states
|n, ‚Ñì, m, œÉ‚ü©‚â°|n, ‚Ñì, m‚ü©‚äó|œÉ‚ü©‚â°|n, ‚Ñì, m‚ü©|œÉ‚ü©
(1.417)
67

LINEAR ALGEBRA
represent a hydrogen atom including the spin of its electron. They are eigenvec-
tors of all four operators H, L2, L3, and S3:
H|n, ‚Ñì, m, œÉ‚ü©= En|n, ‚Ñì, m, œÉ‚ü©,
L2|n, ‚Ñì, m, œÉ‚ü©= ¬Øh2‚Ñì(‚Ñì+ 1)|n, ‚Ñì, m, œÉ‚ü©,
L3|n, ‚Ñì, m, œÉ‚ü©= ¬Øhm|n, ‚Ñì, m, œÉ‚ü©,
S3|n, ‚Ñì, m, œÉ‚ü©= œÉ ¬Øh|n, ‚Ñì, m, œÉ‚ü©.
(1.418)
Suitable linear combinations of these states are eigenvectors of the square J2 of
the composite angular momentum J = L + S as well as of J3, L3, and S3.
Example 1.51 (Adding two spins)
The smallest positive value of angular
momentum is ¬Øh/2. The spin-one-half angular momentum operators S are
represented by three 2 √ó 2 matrices
Sa = ¬Øh
2 œÉa
(1.419)
in which the œÉa are the Pauli matrices
œÉ1 =
0
1
1
0

,
œÉ2 =
0
‚àíi
i
0

,
and
œÉ3 =
1
0
0
‚àí1

.
(1.420)
Consider two spin operators S(1) and S(2) acting on two spin-one-half systems.
The states |¬±‚ü©1 are eigenstates of S(1)
3 , and the states |¬±‚ü©2 are eigenstates of S(2)
3
S(1)
3 |¬±‚ü©1 = ¬± ¬Øh
2|¬±‚ü©1
and
S(2)
3 |¬±‚ü©2 = ¬± ¬Øh
2|¬±‚ü©2.
(1.421)
Then the direct-product states |¬±, ¬±‚ü©= |¬±‚ü©1|¬±‚ü©2 = |¬±‚ü©1 ‚äó|¬±‚ü©2 are eigenstates
of both S(1)
3
and S(2)
3
S(1)
3 |¬±, s2‚ü©= ¬± ¬Øh
2 |+, s2‚ü©
and
S(2)
3 |s1, ¬±‚ü©= ¬± ¬Øh
2 |s1, ¬±‚ü©.
(1.422)
These states also are eigenstates of the third component of the spin operator of
the combined system
S3 = S(1)
3
+ S(2)
3 ,
that is
S3|s1, s2‚ü©= ¬Øh
2 (s1 + s2) |s1, s2‚ü©.
(1.423)
Thus S3|+, +‚ü©= ¬Øh|+, +‚ü©, and S3|‚àí, ‚àí‚ü©= ‚àí¬Øh|‚àí, ‚àí‚ü©, while S3|+, ‚àí‚ü©= 0 and
S3|‚àí, +‚ü©= 0.
Now let‚Äôs consider the effect of the operator S2
1 on the state | + +‚ü©
S2
1| + +‚ü©=

S(1)
1
+ S(2)
1
2
| + +‚ü©= ¬Øh2
4

œÉ (1)
1
+ œÉ (2)
1
2
| + +‚ü©
= ¬Øh2
2

1 + œÉ (1)
1 œÉ (2)
1

| + +‚ü©= ¬Øh2
2

| + +‚ü©+ œÉ (1)
1 |+‚ü©œÉ (2)
1 |+‚ü©

= ¬Øh2
2 (| + +‚ü©+ | ‚àí‚àí‚ü©) .
(1.424)
The rest of this example will be left to exercise 1.36.
68

1.37 CORRELATION FUNCTIONS
1.36 Density operators
A general quantum-mechanical system is represented by a density operator œÅ
that is hermitian œÅ‚Ä† = œÅ, of unit trace TrœÅ = 1, and positive ‚ü®œà|œÅ|œà‚ü©‚â•0 for
all kets |œà‚ü©.
If the state |œà‚ü©is normalized, then ‚ü®œà|œÅ|œà‚ü©is the nonnegative probability
that the system is in that state. This probability is real because the density matrix
is hermitian. If {|n‚ü©} is any complete set of orthonormal states,
I =

n
|n‚ü©‚ü®n|,
(1.425)
then the probability that the system is in the state |n‚ü©is
pn = ‚ü®n|œÅ|n‚ü©= Tr (œÅ|n‚ü©‚ü®n|) .
(1.426)
Since TrœÅ = 1, the sum of these probabilities is unity

n
pn =

n
‚ü®n|œÅ|n‚ü©= Tr

œÅ

n
|n‚ü©‚ü®n|

= Tr (œÅI) = TrœÅ = 1.
(1.427)
A system that is measured to be in a state |n‚ü©cannot simultaneously be mea-
sured to be in an orthogonal state |m‚ü©. The probabilities sum to unity because
the system must be in some state.
Since the density operator œÅ, is hermitian, it has a complete, orthonormal set
of eigenvectors |k‚ü©, all of which have nonnegative eigenvalues œÅk
œÅ|k‚ü©= œÅk|k‚ü©.
(1.428)
They afford for it the expansion
œÅ =
N

k=1
œÅk|k‚ü©‚ü®k|
(1.429)
in which the eigenvalue œÅk is the probability that the system is in the state |k‚ü©.
1.37 Correlation functions
We can deÔ¨Åne two Schwarz inner products for a density matrix œÅ. If |f ‚ü©and |g‚ü©
are two states, then the inner product
(f , g) ‚â°‚ü®f |œÅ|g‚ü©
(1.430)
for g = f is nonnegative, (f , f ) = ‚ü®f |œÅ|f ‚ü©‚â•0, and satisÔ¨Åes the other conditions
(1.73, 1.74, & 1.76) for a Schwarz inner product.
69

LINEAR ALGEBRA
The second Schwarz inner product applies to operators A and B and is
deÔ¨Åned (Titulaer and Glauber, 1965) as
(A, B) = Tr

œÅA‚Ä†B

= Tr

BœÅA‚Ä†
= Tr

A‚Ä†BœÅ

.
(1.431)
This inner product is nonnegative when A = B and obeys the other rules (1.73,
1.74, & 1.76) for a Schwarz inner product.
These two degenerate inner products are not inner products in the strict sense
of (1.73‚Äì1.79), but they are Schwarz inner products, and so (1.92‚Äì1.93) they
satisfy the Schwarz inequality (1.93)
( f , f )(g, g) ‚â•|( f , g)|2.
(1.432)
Applied to the Ô¨Årst, vector, Schwarz inner product (1.430), the Schwarz
inequality gives
‚ü®f |œÅ|f ‚ü©‚ü®g|œÅ|g‚ü©‚â•|‚ü®f |œÅ|g‚ü©|2,
(1.433)
which is a useful property of density matrices. Application of the Schwarz
inequality to the second, operator, Schwarz inner product (1.431) gives
(Titulaer and Glauber, 1965)
Tr

œÅA‚Ä†A

Tr

œÅB‚Ä†B

‚â•
Tr

œÅA‚Ä†B

2
.
(1.434)
The operator Ei(x) that represents the ith component of the electric Ô¨Åeld at
the point x is the hermitian sum of the ‚Äúpositive-frequency‚Äù part E(+)
i
(x) and
its adjoint E(‚àí)
i
(x) = (E(+)
i
(x))‚Ä†
Ei(x) = E(+)
i
(x) + E(‚àí)
i
(x).
(1.435)
Glauber has deÔ¨Åned the Ô¨Årst-order correlation function G(1)
ij (x, y) as (Glauber,
1963b)
G(1)
ij (x, y) = Tr

œÅE(‚àí)
i
(x)E(+)
j
(y)

(1.436)
or in terms of the operator inner product (1.431) as
G(1)
ij (x, y) =

E(+)
i
(x), E(+)
j
(y)

.
(1.437)
By setting A = E(+)
i
(x), etc., it follows then from the Schwarz inequality (1.434)
that the correlation function G(1)
ij (x, y) is bounded by (Titulaer and Glauber,
1965)
|G(1)
ij (x, y)|2 ‚â§G(1)
ii (x, x)G(1)
jj (y, y).
(1.438)
Interference fringes are sharpest when this inequality is saturated:
|G(1)
ij (x, y)|2 = G(1)
ii (x, x)G(1)
jj (y, y),
(1.439)
70

EXERCISES
which can occur only if the correlation function G(1)
ij (x, y) factorizes (Titulaer
and Glauber, 1965)
G(1)
ij (x, y) = E‚àó
i (x)Ej(y)
(1.440)
as it does when the density operator is an outer product of coherent states
œÅ = |{Œ±k}‚ü©‚ü®{Œ±k}|,
(1.441)
which are eigenstates of E(+)
i
(x) with eigenvalue Ei(x) (Glauber, 1963b, a)
E(+)
i
(x)|{Œ±k}‚ü©= Ei(x)|{Œ±k}‚ü©.
(1.442)
The higher-order correlation functions
G(n)
i1...i2n(x1 . . . x2n) = Tr

œÅE(‚àí)
i1 (x1) . . . E(‚àí)
in (xn)E(+)
in+1(xn+1) . . . E(+)
i2n (xn)

(1.443)
satisfy similar inequalities (Glauber, 1963b), which also follow from the
Schwarz inequality (1.434).
Exercises
1.1
Why is the most complicated function of two Grassmann numbers a polyno-
mial with at most four terms as in (1.12)?
1.2
Derive the cyclicity (1.24) of the trace from (1.23).
1.3
Show that (AB) T = BTAT, which is (1.26).
1.4
Show that a real hermitian matrix is symmetric.
1.5
Show that (AB)‚Ä† = B‚Ä†A‚Ä†, which is (1.29).
1.6
Show that the matrix (1.40) is positive on the space of all real 2-vectors but
not on the space of all complex 2-vectors.
1.7
Show that the two 4 √ó 4 matrices (1.45) satisfy Grassmann‚Äôs algebra (1.11) for
N = 2.
1.8
Show that the operators ai = Œ∏i deÔ¨Åned in terms of the Grassmann matrices
(1.45) and their adjoints a‚Ä†
i = Œ∏‚Ä†
i satisfy the anticommutation relations (1.46)
of the creation and annihilation operators for a system with two fermionic
states.
1.9
Derive (1.64) from (1.61‚Äì1.63).
1.10 Fill in the steps leading to the formulas (1.69) for the vectors b‚Ä≤
1 and b‚Ä≤
2 and
the formula (1.70) for the matrix a‚Ä≤.
1.11 Show that the antilinearity (1.76) of the inner product follows from its Ô¨Årst
two properties (1.73 & 1.74).
1.12 Show that the Minkowski product (x, y) = x0y0 ‚àíx ¬∑ y of two 4-vectors x and
y is an inner product that obeys the rules (1.73, 1.74, and 1.79).
1.13 Show that if f = 0, then the linearity (1.74) of the inner product implies that
(f , f ) and (g, f ) vanish.
71

LINEAR ALGEBRA
1.14 Show that the condition (1.75) of being positive deÔ¨Ånite implies nondegener-
acy (1.79).
1.15 Show that the nonnegativity (1.77) of the Schwarz inner product implies the
condition (1.78). Hint: the inequality (f ‚àíŒªg, f ‚àíŒªg) ‚â•0 must hold for every
complex Œª and for all vectors f and g.
1.16 Show that the inequality (1.97) follows from the Schwarz inequality (1.96).
1.17 Show that the inequality (1.99) follows from the Schwarz inequality (1.98).
1.18 Use the Gram‚ÄìSchmidt method to Ô¨Ånd orthonormal linear combinations of
the three vectors
s1 =
‚éõ
‚éù
1
0
0
‚éû
‚é†,
s2 =
‚éõ
‚éù
1
1
0
‚éû
‚é†,
s3 =
‚éõ
‚éù
1
1
1
‚éû
‚é†.
(1.444)
1.19 Now use the Gram‚ÄìSchmidt method to Ô¨Ånd orthonormal linear combina-
tions of the same three vectors but in a different order
s‚Ä≤
1 =
‚éõ
‚éù
1
1
1
‚éû
‚é†,
s‚Ä≤
2 =
‚éõ
‚éù
1
1
0
‚éû
‚é†,
s‚Ä≤
3 =
‚éõ
‚éù
1
0
0
‚éû
‚é†.
(1.445)
Did you get the same orthonormal vectors as in the previous exercise?
1.20 Derive the linearity (1.120) of the outer product from its deÔ¨Ånition (1.119).
1.21 Show that a linear operator A that is represented by a hermitian matrix (1.155)
in an orthonormal basis satisÔ¨Åes (g, A f ) = (A g, f ).
1.22 Show that a unitary operator maps one orthonormal basis into another.
1.23 Show that the integral (1.170) deÔ¨Ånes a unitary operator that maps the state
|x‚Ä≤‚ü©to the state |x‚Ä≤ + a‚ü©.
1.24 For the 2 √ó 2 matrices
A =
1
2
3
‚àí4

and
B =
2
‚àí1
4
‚àí3

(1.446)
verify equations (1.202‚Äì1.204).
1.25 Derive the least-squares solution (1.220) for complex A, x, and y when the
matrix A‚Ä†A is positive.
1.26 Show that the eigenvalues Œª of a unitary matrix are unimodular, that is,
|Œª| = 1.
1.27 What are the eigenvalues and eigenvectors of the two defective matrices
(1.252)?
1.28 Use (1.267) to derive expression (1.268) for the 2 √ó 2 rotation matrix
exp(‚àíiŒ∏ ¬∑ œÉ/2).
1.29 Compute the characteristic equation for the matrix ‚àíiŒ∏ ¬∑ J in which the
generators are (Jk)ij = iœµikj and œµijk is totally antisymmetric with œµ123 = 1.
1.30 Show that the sum of the eigenvalues of a normal antisymmetric matrix
vanishes.
1.31 Use the characteristic equation of exercise 1.29 to derive identities (1.271) and
(1.272) for the 3√ó3 real orthogonal matrix exp(‚àíiŒ∏ ¬∑ J).
72

EXERCISES
1.32 Consider the 2 √ó 3 matrix A
A =

1
2
3
‚àí3
0
1

.
(1.447)
Perform the singular value decomposition A = USVT, where VT is the trans-
pose of V. Find the singular values and the real orthogonal matrices U and
V. Students may use Lapack, Octave, Matlab, Maple or any other program to
do this exercise.
1.33 Consider the 6 √ó 9 matrix A with elements
Aj,k = x + xj + i(y ‚àíyk)
(1.448)
in which x = 1.1 and y = 1.02. Find the singular values, and the Ô¨Årst left and
right singular vectors. Students may use Lapack, Octave, Matlab, Maple or
any other program to do this exercise.
1.34 Show that the totally antisymmetric Levi-Civita symbol œµijk satisÔ¨Åes the useful
relation
3

i=1
œµijk œµinm = Œ¥jn Œ¥km ‚àíŒ¥jm Œ¥kn.
(1.449)
1.35 Consider the hamiltonian
H = 1
2 ¬ØhœâœÉ3
(1.450)
where œÉ3 is deÔ¨Åned in (1.420). The entropy S of this system at temperature T is
S = ‚àíkTr [œÅ ln(œÅ)]
(1.451)
in which the density operator œÅ is
œÅ =
e‚àíH/(kT)
Tr

e‚àíH/(kT).
(1.452)
Find expressions for the density operator œÅ and its entropy S.
1.36 Find the action of the operator S2 =

S(1) + S(2)2
deÔ¨Åned by (1.419) on the
four states | ¬± ¬±‚ü©and then Ô¨Ånd the eigenstates and eigenvalues of S2 in the
space spanned by these four states.
1.37 A system that has three fermionic states has three creation operators a‚Ä†
i and
three annihilation operators ak which satisfy the anticommutation relations
{ai, a‚Ä†
k} = Œ¥ik and {ai, ak} = {a‚Ä†
i , a‚Ä†
k} = 0 for i, k = 1, 2, 3. The eight states
of the system are |v, u, t‚ü©‚â°(a‚Ä†
3)t(a‚Ä†
2)u(a‚Ä†
1)v|0, 0, 0‚ü©. We can represent them by
eight 8-vectors, each of which has seven 0s with a 1 in position 5v+3u+t. How
big should the matrices that represent the creation and annihilation operators
be? Write down the three matrices that represent the three creation operators.
1.38 Show that the Schwarz inner product (1.430) is degenerate because it can
violate (1.79) for certain density operators and certain pairs of states.
73

LINEAR ALGEBRA
1.39 Show that the Schwarz inner product (1.431) is degenerate because it can
violate (1.79) for certain density operators and certain pairs of operators.
1.40 The coherent state |{Œ±k}‚ü©is an eigenstate of the annihilation operator ak
with eigenvalue Œ±k for each mode k of the electromagnetic Ô¨Åeld, ak|{Œ±k}‚ü©=
Œ±k|{Œ±k}‚ü©. The positive-frequency part E(+)
i
(x) of the electric Ô¨Åeld is a linear
combination of the annihilation operators
E(+)
i
(x) =

k
ak E(+)
i
(k) ei(kx‚àíœât).
(1.453)
Show that |{Œ±k}‚ü©is an eigenstate of E(+)
i
(x) as in (1.442) and Ô¨Ånd its eigenvalue
Ei(x).
74

2
Fourier series
2.1 Complex Fourier series
The phases exp(inx)/
‚àö
2œÄ, one for each integer n, are orthonormal on an
interval of length 2œÄ
 2œÄ
0
 eimx
‚àö
2œÄ
‚àóeinx
‚àö
2œÄ
dx =
 2œÄ
0
ei(n‚àím)x
2œÄ
dx = Œ¥m,n
(2.1)
where Œ¥n,m = 1 if n = m, and Œ¥n,m = 0 if n Ã∏= m. So if a function f (x) is a sum of
these phases
f (x) =
‚àû

n=‚àí‚àû
fn
einx
‚àö
2œÄ
(2.2)
then their orthonormality (2.1) gives the nth coefÔ¨Åcient fn as the integral
 2œÄ
0
e‚àíinx
‚àö
2œÄ
f (x) dx =
 2œÄ
0
e‚àíinx
‚àö
2œÄ
‚àû

m=‚àí‚àû
fm
eimx
‚àö
2œÄ
dx =
‚àû

m=‚àí‚àû
Œ¥n,m fm = fn
(2.3)
(Joseph Fourier, 1768‚Äì1830).
The Fourier series (2.2) is periodic with period 2œÄ because the phases are
periodic with period 2œÄ, exp(in(x + 2œÄ)) = exp(inx). Thus even if the function
f (x) which we use in (2.3) to make the Fourier coefÔ¨Åcients fn is not periodic,
its Fourier series (2.2) will nevertheless be strictly periodic, as illustrated by
Figs. 2.2 & 2.4.
75

FOURIER SERIES
If the Fourier series (2.2) converges uniformly (section 2.7), then the term-
by-term integration implicit in the formula (2.3) for fn is permitted.
How is the Fourier series for the complex-conjugate function f ‚àó(x) related to
the series for f (x)? The complex conjugate of the Fourier series (2.2) is
f ‚àó(x) =
‚àû

n=‚àí‚àû
f ‚àó
n
e‚àíinx
‚àö
2œÄ
=
‚àû

n=‚àí‚àû
f ‚àó
‚àín
einx
‚àö
2œÄ
(2.4)
so the coefÔ¨Åcients fn(f ‚àó) for f ‚àó(x) are related to those fn(f ) for f (x) by
fn(f ‚àó) = f ‚àó
‚àín(f ).
(2.5)
Thus if the function f (x) is real, then
fn(f ) = fn(f ‚àó) = f ‚àó
‚àín(f ).
(2.6)
Dropping all reference to the functions, we see that the Fourier coefÔ¨Åcients fn
for a real function f (x) satisfy
fn = f ‚àó
‚àín.
(2.7)
Example 2.1 (Fourier series by inspection)
The doubly exponential function
exp(exp(ix)) has the Fourier series
exp

eix
=
‚àû

n=0
1
n! einx
(2.8)
in which n! = n(n ‚àí1) . . . 1 is n-factorial with 0! ‚â°1.
Example 2.2 (Beats)
The sum of two sines f (x) = sin œâ1x + sin œâ2x of similar
frequencies œâ1 ‚âàœâ2 is the product (exercise 2.1)
f (x) = 2 cos 1
2(œâ1 ‚àíœâ2)x sin 1
2(œâ1 + œâ2)x
(2.9)
in which the Ô¨Årst factor cos 1
2(œâ1 ‚àíœâ2)x is the beat which modulates the second
factor sin 1
2(œâ1 + œâ2)x, as illustrated by Fig. 2.1.
76

2.3 WHERE TO PUT THE 2œÄS
0
1
2
3
4
5
6
7
8
9
10
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
2
Beats
x
sin(œâ1x) + sin(œâ2x)
Figure 2.1
The curve sin œâ1x + sin œâ2x for œâ1 = 30 and œâ2 = 32.
2.2 The interval
In equations (2.1‚Äì2.3), we singled out the interval [0, 2œÄ], but to represent a
periodic function with period 2œÄ, we could have used any interval of length 2œÄ,
such as the interval [ ‚àíœÄ, œÄ] or [r, r + 2œÄ]
fn =
 r+2œÄ
r
e‚àíinx f (x) dx
‚àö
2œÄ
.
(2.10)
This integral is independent of its lower limit r as long as the function f (x) is
periodic with period 2œÄ. The choice r = ‚àíœÄ often is convenient. With this
choice of interval, the coefÔ¨Åcient fn is the integral (2.3) shifted by ‚àíœÄ
fn =
 œÄ
‚àíœÄ
e‚àíinx f (x) dx
‚àö
2œÄ
.
(2.11)
But if the function f (x) is not periodic with period 2œÄ, then the Fourier
coefÔ¨Åcients (2.10) do depend upon r.
2.3 Where to put the 2œÄs
In equations (2.2 & 2.3), we used the orthonormal functions exp(inx)/
‚àö
2œÄ,
and so we had factors of 1/
‚àö
2œÄ in both equations. If one gets tired of having
77

FOURIER SERIES
so many explicit square-roots, then one may set dn = fn/
‚àö
2œÄ and write (2.2)
and (2.3) as
f (x) =
‚àû

n=‚àí‚àû
dn einx
and
dn = 1
2œÄ
 2œÄ
0
dx e‚àíinx f (x).
(2.12)
One also may use the rules
f (x) = 1
2œÄ
‚àû

n=‚àí‚àû
cneinx
and
cn =
 œÄ
‚àíœÄ
f (x)e‚àíinx dx.
(2.13)
Example 2.3 (Fourier series for exp(‚àím|x|))
Let‚Äôs compute the Fourier series
for the real function f (x) = exp(‚àím|x|) on the interval (‚àíœÄ, œÄ). Using (2.10)
for the shifted interval and the 2œÄ-placement convention (2.12), we Ô¨Ånd for the
coefÔ¨Åcient dn
dn =
 œÄ
‚àíœÄ
dx
2œÄ e‚àíinx e‚àím|x|,
(2.14)
which we may split into the two pieces
dn =
 0
‚àíœÄ
dx
2œÄ e(m‚àíin)x +
 œÄ
0
dx
2œÄ e‚àí(m+in)x.
(2.15)
After doing the integrals, we Ô¨Ånd
dn = 1
œÄ
m
m2 + n2

1 ‚àí(‚àí1)n e‚àíœÄm
.
(2.16)
Here, since m is real, dn = d‚àó
n, but also dn = d‚àín. So the coefÔ¨Åcients dn satisfy the
condition (2.7) that holds when the function f (x) is real, dn = d‚àó
‚àín. The Fourier
series for exp(‚àím|x|) with dn given by (2.16) is
e‚àím|x| =
‚àû

n=‚àí‚àû
dneinx =
‚àû

n=‚àí‚àû
1
œÄ
m
m2 + n2

1 ‚àí(‚àí1)n e‚àíœÄm
einx
=
1
mœÄ

1 ‚àíe‚àíœÄm
+ 2
‚àû

n=1
1
œÄ
m
m2 + n2

1 ‚àí(‚àí1)n e‚àíœÄm
cos(nx).
(2.17)
In Fig. 2.2, the 10-term (dashes) Fourier series for m = 2 is plotted from
x = ‚àí2œÄ to x = 2œÄ. The function exp(‚àí2|x|) itself is represented by a solid
line. Although it is not periodic, its Fourier series is periodic with period 2œÄ.
The 10-term Fourier series represents the function exp(‚àí2|x|) so well that the
100-term series would have been hard to distinguish from the function.
78

2.4 REAL FOURIER SERIES FOR REAL FUNCTIONS
‚àí6
‚àí4
‚àí2
0
2
4
6
0
0.2
0.4
0.6
0.8
1
x
e‚àí2|x|
Fourier series for e‚àí2|x|
Figure 2.2
The 10-term (dashes) Fourier series (2.17) for the function exp(‚àí2|x|)
on the interval (‚àíœÄ, œÄ) are plotted from ‚àí2œÄ to 2œÄ. All Fourier series are periodic,
but the function exp(‚àí2|x|) (solid) is not.
In what follows, we usually won‚Äôt bother to use different letters to distinguish
between the symmetric (2.2 & 2.3) and asymmetric (2.12 & 2.13) conventions on
the placement of the 2œÄs.
2.4 Real Fourier series for real functions
The Fourier series outlined above are simple and apply to functions that are
continuous and periodic ‚Äì whether complex or real. If the function f (x) is real,
then by (2.7) d‚àín = d‚àó
n, whence d0 = d‚àó
0, so d0 is real. Thus the Fourier series
(2.12) for a real function f (x) is
f (x) = d0 +
‚àû

n=1
dn einx +
‚àí1

n=‚àí‚àû
dn einx
= d0 +
‚àû

n=1

dn einx + d‚àín e‚àíinx
= d0 +
‚àû

n=1

dn einx + d‚àó
n e‚àíinx
= d0 +
‚àû

n=1
dn (cos nx + i sin nx) + d‚àó
n (cos nx ‚àíi sin nx)
= d0 +
‚àû

n=1
(dn + d‚àó
n) cos nx + i(dn ‚àíd‚àó
n) sin nx.
(2.18)
79

FOURIER SERIES
Let‚Äôs write dn as
dn = 1
2 (an ‚àíibn),
so that
an = dn + d‚àó
n
and
bn = i(dn ‚àíd‚àó
n).
(2.19)
Then the Fourier series (2.18) for a real function f (x) is
f (x) = a0
2 +
‚àû

n=1
an cos nx + bn sin nx.
(2.20)
What are the formulas for an and bn? By (2.19 & 2.12), the coefÔ¨Åcient an is
an =
 2œÄ
0

e‚àíinx f (x) + einx f ‚àó(x)
 dx
2œÄ =
 2œÄ
0

e‚àíinx + einx
2
f (x) dx
2œÄ
(2.21)
since the function f (x) is real. So the coefÔ¨Åcient an of cos nx in (2.20) is the
cosine integral of f (x)
an =
 2œÄ
0
cos nx f (x) dx
œÄ =
 œÄ
‚àíœÄ
cos nx f (x) dx
œÄ .
(2.22)
Similarly, (2.19 & 2.12) and the reality of f (x) imply that the coefÔ¨Åcient bn is the
sine integral of f (x)
bn =
 2œÄ
0
i

e‚àíinx ‚àíeinx
2
f (x) dx
œÄ =
 2œÄ
0
sin nx f (x) dx
œÄ =
 œÄ
‚àíœÄ
sin nx f (x) dx
œÄ .
(2.23)
The real Fourier series (2.20) and the cosine (2.22) and sine (2.23) integrals
for the coefÔ¨Åcients an and bn also follow from the orthogonality relations
 2œÄ
0
sin mx sin nx dx =
 œÄ
if n = m Ã∏= 0
0
otherwise,
(2.24)
 2œÄ
0
cos mx cos nx dx =
‚éß
‚é®
‚é©
œÄ
if n = m Ã∏= 0
2œÄ
if n = m = 0
0
otherwise, and
(2.25)
 2œÄ
0
sin mx cos nx dx = 0,
(2.26)
which hold for integer values of n and m.
What if a function f (x) is not periodic? The Fourier series for a function
that is not periodic is itself strictly periodic. In such cases, the Fourier series
differs somewhat from the function near the ends of the interval and differs
markedly from it outside the interval, where the series but not the function is
periodic.
80

2.4 REAL FOURIER SERIES FOR REAL FUNCTIONS
Example 2.4 (The Fourier series for x2)
The function x2 is even and so the
integrals (2.23) for its sine Fourier coefÔ¨Åcients bn all vanish. Its cosine coefÔ¨Åcients
an are given by (2.22)
an =
 œÄ
‚àíœÄ
cos nx f (x) dx
œÄ =
 œÄ
‚àíœÄ
cos nx x2 dx
œÄ .
(2.27)
Integrating twice by parts, we Ô¨Ånd for n Ã∏= 0
an = ‚àí2
n
 œÄ
‚àíœÄ
x sin nx dx
œÄ = (‚àí1)n 4
n2
(2.28)
and
a0 =
 œÄ
‚àíœÄ
x2 dx
œÄ = 2œÄ2
3 .
(2.29)
Equation (2.20) now gives for x2 the cosine Fourier series
x2 = a0
2 +
‚àû

n=1
an cos nx = œÄ2
3 + 4
‚àû

n=1
(‚àí1)n cos nx
n2
.
(2.30)
This series rapidly converges within the interval (‚àíœÄ, œÄ) as shown in Fig. 2.3,
but not near its endpoints ¬±œÄ.
‚àí1
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
1
‚àí0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
x
x2
Fourier¬†series¬†for¬†x2
Figure 2.3
The function x2 (solid) and its Fourier series of seven terms (dot dash)
and 20 terms (dashes). The Fourier series (2.30) for x2 quickly converges well inside
the interval (‚àíœÄ, œÄ).
81

FOURIER SERIES
Example 2.5 (The Gibbs overshoot)
The function f (x) = x on the interval
(‚àíœÄ, œÄ) is not periodic. So we expect trouble if we represent it as a Fourier series.
Since x is an odd function, equation (2.22) tells us that the coefÔ¨Åcients an all
vanish. By (2.23), the bns are
bn =
 œÄ
‚àíœÄ
dx
œÄ x sin nx = 2 (‚àí1)n+1 1
n.
(2.31)
As shown in Fig. 2.4, the series
‚àû

n=1
2 (‚àí1)n+1 1
n sin nx
(2.32)
differs by about 2œÄ from the function f (x) = x for ‚àí3œÄ < x < ‚àíœÄ and for
œÄ < x < 3œÄ because the series is periodic while the function x isn‚Äôt.
Within the interval (‚àíœÄ, œÄ), the series with 100 terms is very accurate except
for x ‚â≥‚àíœÄ and x ‚â≤œÄ, where it overshoots by about 9% of the 2œÄ discontinuity,
a defect called the Gibbs phenomenon or the Gibbs overshoot (J. Willard Gibbs,
1839‚Äì1903; incidentally Gibbs‚Äôs father successfully defended the Africans of the
schooner Amistad). Any time we use a Fourier series to represent an aperiodic
function, a Gibbs phenomenon will occur near the endpoints of the interval.
‚àí6
‚àí4
‚àí2
0
2
4
6
‚àí6
‚àí4
‚àí2
0
2
4
6
Fourier series for the aperiodic function x
x
‚àí3
‚àí2
‚àí1
0
1
2
3
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
Gibbs overshoot for the function¬†x
x
Overshoot
Figure 2.4
(top) The Fourier series (2.32) for the function x (solid line) with ten
terms (dots) and 100 terms (solid curve) for ‚àí2œÄ < x < 2œÄ. The Fourier series is
periodic, but the function x is not. (bottom) The differences between x and the ten-
term (dots) and the 100-term (solid curve) on (‚àíœÄ, œÄ) exhibit a Gibbs overshoot of
about 9% at x ‚â≥‚àíœÄ and at x ‚â≤œÄ.
82

2.5 STRETCHED INTERVALS
2.5 Stretched intervals
If the interval of periodicity is of length L instead of 2œÄ, then we may use the
phases exp(i2œÄn/
‚àö
L), which are orthonormal on the interval (0, L)
 L
0
dx

ei2œÄnx/L
‚àö
L
‚àóei2œÄmx/L
‚àö
L
= Œ¥nm.
(2.33)
The Fourier series
f (x) =
‚àû

n=‚àí‚àû
fn
ei2œÄnx/L
‚àö
L
(2.34)
is periodic with period L. The coefÔ¨Åcient fn is the integral
fn =
 L
0
e‚àíi2œÄnx/L
‚àö
L
f (x) dx.
(2.35)
These relations (2.33‚Äì2.35) generalize to the interval [0, L] our earlier formulas
(2.1‚Äì2.3) for the interval [0, 2œÄ].
If the function f (x) is periodic f (x ¬± L) = f (x) with period L, then we may
shift the domain of integration by any real number r
fn =
 L+r
r
e‚àíi2œÄnx/L
‚àö
L
f (x) dx.
(2.36)
An obvious choice is r = ‚àíL/2 for which (2.34) and (2.35) give
f (x) =
‚àû

n=‚àí‚àû
fn
ei2œÄnx/L
‚àö
L
and
fn =
 L/2
‚àíL/2
e‚àíi2œÄnx/L
‚àö
L
f (x) dx.
(2.37)
If the function f (x) is real, then on the interval [0, L] in place of
equations (2.20, 2.22, & 2.23) one has
f (x) = a0
2 +
‚àû

n=1
an cos
2œÄnx
L

+ bn sin
2œÄnx
L

,
(2.38)
an = 2
L
 L
0
dx cos
2œÄnx
L

f (x),
(2.39)
and
bn = 2
L
 L
0
dx sin
2œÄnx
L

f (x).
(2.40)
83

FOURIER SERIES
The corresponding orthogonality relations, which follow from equations (2.24,
2.25, & 2.26), are:
 L
0
dx sin
2œÄmx
L

sin
2œÄnx
L

=
 L/2
if n = m Ã∏= 0
0
otherwise
(2.41)
 L
0
dx cos
2œÄmx
L

cos
2œÄnx
L

=
‚éß
‚é®
‚é©
L/2
if n = m Ã∏= 0
L
if n = m = 0
0
otherwise
(2.42)
 L
0
dx sin
2œÄmx
L

cos
2œÄnx
L

= 0.
(2.43)
They hold for integer values of n and m, and they imply equations (2.38‚Äì2.40).
2.6 Fourier series in several variables
On the interval [‚àíL, L], the Fourier-series formulas (2.34 & 2.35) are
f (x) =
‚àû

n=‚àí‚àû
fn
eiœÄnx/L
‚àö
2L
(2.44)
fn =
 L
‚àíL
e‚àíiœÄnx/L
‚àö
2L
f (x) dx.
(2.45)
We may generalize these equations from a single variable to N variables x =
(x1, . . . , xN) with n ¬∑ x = n1x1 + ¬∑ ¬∑ ¬∑ + nNxN
f (x) =
‚àû

n1=‚àí‚àû
¬∑ ¬∑ ¬∑
‚àû

nN=‚àí‚àû
fn
eiœÄn¬∑x/L
(2L)N/2
(2.46)
fn =
 L
‚àíL
dx1 . . .
 L
‚àíL
dxN
e‚àíiœÄn¬∑x/L
(2L)N/2 f (x).
(2.47)
2.7 How Fourier series converge
A Fourier series represents a function f (x) as the limit of a sequence of functions
fN(x) given by
fN(x) =
N

k=‚àíN
fk
ei2œÄkx/L
‚àö
L
in which
fk =
 L
0
f (x) e‚àíi2œÄkx/L dx
‚àö
L
.
(2.48)
Since the exponentials are periodic with period L, a Fourier series always is peri-
odic. So if the function f (x) is not periodic, then its Fourier series will represent
the periodic extension fp of f deÔ¨Åned by fp(x + nL) = f (x) for 0 ‚â§x ‚â§L.
84

2.7 HOW FOURIER SERIES CONVERGE
A sequence of functions fN(x) converges to a function f (x) on a (closed) inter-
val [a, b] if for every œµ > 0 and each point a ‚â§x ‚â§b there exists an integer
N(œµ, x) such that
 f (x) ‚àífN(x)
 < œµ
for all
N > N(œµ, x).
(2.49)
If this holds for an N(œµ) that is independent of x ‚àà[a, b], then the sequence of
functions fN(x) converges uniformly to f (x) on the interval [a, b].
A function f (x) is continuous on an open interval (a, b) if for every point a <
x < b the two limits
f (x ‚àí0) ‚â°
lim
0<œµ‚Üí0 f (x ‚àíœµ)
and
f (x + 0) ‚â°
lim
0<œµ‚Üí0 f (x + œµ)
(2.50)
agree; it also is continuous on the closed interval [a, b] if f (a + 0) = f (a) and
f (b ‚àí0) = f (b). A function continuous on [a, b] is bounded there.
If a sequence of continuous functions fN(x) converges uniformly to a func-
tion f (x) on a closed interval a ‚â§x ‚â§b, then we know that | fN(x) ‚àíf (x)| < œµ
for N > N(œµ), and so

 b
a
fN(x) dx ‚àí
 b
a
f (x) dx
 ‚â§
 b
a
 fN(x) ‚àíf (x)
 dx < (b ‚àía)œµ.
(2.51)
Thus one may integrate a uniformly convergent sequence of continuous func-
tions on a closed interval [a, b] term by term
lim
N‚Üí‚àû
 b
a
fN(x) dx =
 b
a
f (x) dx.
(2.52)
A function is piecewise continuous on [a, b] if it is continuous there except
for Ô¨Ånite jumps from f (x ‚àí0) to f (x + 0) at a Ô¨Ånite number of points x. At
such jumps, we shall deÔ¨Åne the periodically extended function fp to be the mean
fp(x) = [fp(x ‚àí0) + fp(x + 0)]/2.
Fourier‚Äôs convergence theorem (Courant, 1937, p. 439) The Fourier series of
a function f (x) that is piecewise continuous with a piecewise continuous Ô¨Årst
derivative converges to its periodic extension fp(x). This convergence is uniform
on every closed interval on which the function f (x) is continuous (and absolute
if the function f (x) has no discontinuities). Examples 2.11 and 2.12 illustrate
this result.
A function whose kth derivative is continuous is in class Ck. On the interval
[ ‚àíœÄ, œÄ], its Fourier coefÔ¨Åcients (2.13) are
fn =
 œÄ
‚àíœÄ
f (x) e‚àíinx dx.
(2.53)
85

FOURIER SERIES
If f is both periodic and in Ck, then one integration by parts gives
fn =
 œÄ
‚àíœÄ
 d
dx
#
f (x) e‚àíinx
‚àíin
$
‚àíf ‚Ä≤(x) e‚àíinx
‚àíin
%
dx =
 œÄ
‚àíœÄ
f ‚Ä≤(x) e‚àíinx
in
dx
and k integrations by parts give
fn =
 œÄ
‚àíœÄ
f (k)(x) e‚àíinx
(in)k dx
(2.54)
since the derivatives f (‚Ñì)(x) of a Ck periodic function also are periodic.
Moreover, if f (k+1) is piecewise continuous, then
fn =
 œÄ
‚àíœÄ
 d
dx
#
f (k)(x)
e‚àíinx
‚àí(in)k+1
$
‚àíf (k+1)(x)
e‚àíinx
‚àí(in)k+1
%
dx
=
 œÄ
‚àíœÄ
f (k+1)(x) e‚àíinx
(in)k+1 dx.
(2.55)
Since f (k+1)(x) is piecewise continuous on the closed interval [‚àíœÄ, œÄ], it is
bounded there in absolute value by, let us say, M. So the Fourier coefÔ¨Åcients
of a Ck periodic function with f (k+1) piecewise continuous are bounded by
| fn| ‚â§
1
nk+1
 œÄ
‚àíœÄ
| f (k+1)(x)| dx ‚â§2œÄM
nk+1 .
(2.56)
We often can carry this derivation one step further. In most simple exam-
ples, the piecewise continuous periodic function f (k+1)(x) actually is piecewise
continuously differentiable between its successive jumps at xj. In this case, the
derivative f (k+2)(x) is a piecewise continuous function plus a sum of a Ô¨Ånite
number of delta functions with Ô¨Ånite coefÔ¨Åcients. Thus we can integrate once
more by parts. If for instance the function f (k+1)(x) jumps J times between ‚àíœÄ
and œÄ by f (k+1)
j
, then the Fourier coefÔ¨Åcients are
fn =
 œÄ
‚àíœÄ
f (k+2)(x) e‚àíinx
(in)k+2 dx
=
J

j=1
 xj+1
xj
f (k+2)
s
(x) e‚àíinx
(in)k+2 dx +
J

j=1
f (k+2)
j
e‚àíinxj
‚àö
2œÄ (in)k+1
(2.57)
in which the subscript s means that we‚Äôve separated out the delta functions. The
Fourier coefÔ¨Åcients then would be bounded by
|fn| ‚â§2œÄM
nk+2
(2.58)
in which M is related to the maximum absolute values of f (k+2)
s
(x) and of the
f (k+1)
j
. The Fourier series of periodic Ck functions converge very rapidly if k
is big.
86

2.7 HOW FOURIER SERIES CONVERGE
Example 2.6 (Fourier series of a C0 function)
The function deÔ¨Åned by
f (x) =
‚éß
‚é®
‚é©
0,
‚àíœÄ ‚â§x < 0
x,
0 ‚â§x < œÄ/2
œÄ ‚àíx,
œÄ/2 ‚â§x ‚â§œÄ
(2.59)
is continuous on the interval [‚àíœÄ, œÄ] and its Ô¨Årst derivative is piecewise continu-
ous on that interval. By (2.56), its Fourier coefÔ¨Åcients fn should be bounded by
M/n. In fact they are (exercise 2.8)
fn =
 œÄ
‚àíœÄ
f (x)e‚àíinx
dx
‚àö
2œÄ
= (‚àí1)n+1
‚àö
2œÄ
(in ‚àí1)2
n2
(2.60)
bounded by 2‚àö2/œÄ/n2 in agreement with the stronger inequality (2.58).
Example 2.7 (Fourier series for a C1 function)
The function deÔ¨Åned by f (x) =
1 + cos 2x for |x| ‚â§œÄ/2 and f (x) = 0 for |x| ‚â•œÄ/2 has a periodic extension fp
that is continuous with a continuous Ô¨Årst derivative and a piecewise continuous
second derivative. Its Fourier coefÔ¨Åcients (2.53)
fn =
 œÄ/2
‚àíœÄ/2
(1 + cos 2x)e‚àíinx dx
‚àö
2œÄ
=
8 sin nœÄ/2
‚àö
2œÄ(4n ‚àín3)
satisfy the inequalities (2.56) and (2.58) for k = 1.
Example 2.8 (The Fourier series for cos Œºx)
The Fourier series for the even
function f (x) = cos Œºx has only cosines with coefÔ¨Åcients (2.22)
an =
 œÄ
‚àíœÄ
cos nx cos Œºx dx
œÄ =
 œÄ
0
[cos(Œº + n)x + cos(Œº ‚àín)x]dx
œÄ
= 1
œÄ
#sin(Œº + n)œÄ
Œº + n
+ sin(Œº ‚àín)œÄ
Œº ‚àín
$
= 2
œÄ
Œº(‚àí1)n
Œº2 ‚àín2 sin ŒºœÄ.
(2.61)
Thus, whether or not Œº is an integer, the series (2.20) gives us
cos Œºx = 2Œº sin ŒºœÄ
œÄ
 1
2Œº2 ‚àí
cos x
Œº2 ‚àí12 + cos 2x
Œº2 ‚àí22 ‚àícos 3x
Œº2 ‚àí32 + ‚àí¬∑ ¬∑ ¬∑

, (2.62)
which is continuous at x = ¬±œÄ (Courant, 1937, chap. IX).
Example 2.9 (The sine as an inÔ¨Ånite product)
In our series (2.62) for cos Œºx,
we set x = œÄ, divide by sin ŒºœÄ, replace Œº with x, and so Ô¨Ånd for the cotangent
the expansion
cot œÄx = 2x
œÄ
 1
2x2 +
1
x2 ‚àí12 +
1
x2 ‚àí22 +
1
x2 ‚àí32 + ¬∑ ¬∑ ¬∑

(2.63)
87

FOURIER SERIES
or equivalently
cot œÄx ‚àí1
œÄx = ‚àí2x
œÄ

1
12 ‚àíx2 +
1
22 ‚àíx2 +
1
32 ‚àíx2 + ¬∑ ¬∑ ¬∑

.
(2.64)
For 0 ‚â§x ‚â§q < 1, the absolute value of the nth term on the right is less than
2q/(œÄ(n2 ‚àíq2)). Thus this series converges uniformly on [0, x], and so we may
integrate it term by term. We Ô¨Ånd (exercise 2.11)
œÄ
 x
0

cot œÄt ‚àí1
œÄt

dt = ln sin œÄx
œÄx
=
‚àû

n=1
 x
0
‚àí2t dt
n2 ‚àít2 =
‚àû

n=1
ln

1 ‚àíx2
n2

.
(2.65)
Exponentiating, we get the inÔ¨Ånite-product formula
sin œÄx
œÄx
= exp
 ‚àû

n=1
ln

1 ‚àíx2
n2

=
‚àû

n=1

1 ‚àíx2
n2

(2.66)
for the sine, from which one can derive the inÔ¨Ånite product (exercise 2.12)
cos œÄx =
‚àû

n=1
‚éõ
‚éú‚éù1 ‚àí
x2

n ‚àí1
2
2
‚éû
‚éü‚é†
(2.67)
for the cosine (Courant, 1937, chap. IX).
Fourier series can represent a much wider class of functions than those that
are continuous. If a function f (x) is square integrable on an interval [a, b], then
its N-term Fourier series fN(x) will converge to f (x) in the mean, that is
lim
N‚Üí‚àû
 b
a
dx| f (x) ‚àífN(x)|2 = 0.
(2.68)
What happens to the convergence of a Fourier series if we integrate or
differentiate term by term? If we integrate the series
f (x) =
‚àû

n=‚àí‚àû
fn
ei2œÄnx/L
‚àö
L
,
(2.69)
then we get a series
F(x) =
 x
a
dx‚Ä≤f (x‚Ä≤) = f0
(x ‚àía)
‚àö
L
‚àíi
‚àö
L
2œÄ
‚àû

n=‚àí‚àû
fn
n

ei2œÄnx/L ‚àíei2œÄna/L
(2.70)
that converges better because of the extra factor of 1/n. An integrated function
f (x) is smoother, and so its Fourier series converges better.
88

2.8 QUANTUM-MECHANICAL EXAMPLES
But if we differentiate the same series, then we get a series
f ‚Ä≤(x) = i 2œÄ
L3/2
‚àû

n=‚àí‚àû
n fn ei2œÄnx/L
(2.71)
that converges less well because of the extra factor of n. A differentiated
function is rougher, and so its Fourier series converges less well.
2.8 Quantum-mechanical examples
Suppose a particle of mass m is trapped in an inÔ¨Ånitely deep one-dimensional
square well of potential energy
V(x) =
 0
if 0 < x < L
‚àû
otherwise.
(2.72)
The hamiltonian operator is
H = ‚àí¬Øh2
2m
d2
dx2 + V(x),
(2.73)
in which ¬Øh is Planck‚Äôs constant divided by 2œÄ. This tiny bit of action,
¬Øh = 1.055 √ó 10‚àí34 J s, sets the scale at which quantum mechanics becomes
important; quantum-mechanical corrections to classical predictions often are
important in processes whose action is less than ¬Øh.
An eigenfunction œà(x) of the hamiltonian H with energy E satisÔ¨Åes the
equation Hœà(x) = Eœà(x), which breaks into two simple equations:
‚àí¬Øh2
2m
d2œà(x)
dx2
= Eœà(x)
for
0 < x < L
(2.74)
and
‚àí¬Øh2
2m
d2œà(x)
dx2
+ ‚àûœà(x) = Eœà(x)
for
x < 0
and for
x > L.
(2.75)
Every solution of these equations with Ô¨Ånite energy E must vanish outside the
interval 0 < x < L. So we must Ô¨Ånd solutions of the Ô¨Årst equation (2.74) that
satisfy the boundary conditions
œà(x) = 0
for
x ‚â§0 and x ‚â•L.
(2.76)
For any integer n Ã∏= 0, the function
œàn(x) =
&
2
L sin
œÄnx
L

for
x ‚àà[0, L]
(2.77)
89

FOURIER SERIES
and œàn(x) = 0 for x /‚àà(0, L) satisÔ¨Åes the boundary conditions (2.76). When
inserted into equation (2.74)
‚àí¬Øh2
2m
d2
dx2 œàn(x) = ¬Øh2
2m
nœÄ
L
2
œàn(x) = Enœàn(x)
(2.78)
it reveals its energy to be En = (nœÄ ¬Øh/L)2/2m.
These eigenfunctions œàn(x) are complete in the sense that they span the space
of all functions f (x) that are square-integrable on the interval (0, L) and vanish
at its endpoints. They provide for such functions the sine Fourier series
f (x) =
‚àû

n=1
fn
&
2
L sin
œÄnx
L

,
(2.79)
which also is the Fourier series for a function that is odd f (‚àíx) = ‚àíf (x) on the
interval (‚àíL, L) and zero at both ends.
Example 2.10 (Time evolution of an initially piecewise continuous wave func-
tion)
Suppose now that at time t = 0 the particle is conÔ¨Åned to the middle half
of the well with the square-wave wave function
œà(x, 0) =
&
2
L
for
L
4 < x < 3L
4
(2.80)
and zero otherwise. This piecewise continuous wave function is discontinuous at
x = L/4 and at x = 3L/4. Since the functions ‚ü®x|n‚ü©= œàn(x) are orthonormal
on [0, L]
 L
0
dx
&
2
L sin
œÄnx
L
 &
2
L sin
œÄmx
L

= Œ¥nm,
(2.81)
the coefÔ¨Åcients fn in the Fourier series
œà(x, 0) =
‚àû

n=1
fn
&
2
L sin
œÄnx
L

(2.82)
are the inner products
fn = ‚ü®n|œà, 0‚ü©=
 L
0
dx
&
2
L sin
œÄnx
L

œà(x, 0).
(2.83)
They are proportional to 1/n in accord with (2.58)
fn = 2
L
 3L/4
L/4
dx sin
œÄnx
L

= 2
œÄn
#
cos
œÄn
4

‚àícos
3œÄn
4
$
.
(2.84)
Figure 2.5 plots the wave function œà(x, 0) (2.80, straight solid lines) and
its ten-term (solid curve) and 100-term (dashes) Fourier series (2.82) for an
interval of length L = 2. Gibbs‚Äôs overshoot reaches 1.093 at x = 0.52 for
90

2.8 QUANTUM-MECHANICAL EXAMPLES
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
‚àí0.2
0
0.2
0.4
0.6
0.8
1
1.2
x
œà (x , 0)
Fourier series for a piecewise continuous wave function
Figure 2.5
The piecewise continuous wave function œà(x, 0) for L = 2 (2.80,
straight solid lines) and its Fourier series (2.82) with ten terms (solid curve) and
100 terms (dashes). Gibbs overshoots occur near the discontinuities at x = 1/2 and
x = 3/2.
100 terms and 1.0898 at x = 0.502 for 1000 terms (not shown), amounting
to about 9% of the unit discontinuity at x = 1/2. A similar overshoot occurs
at x = 3/2.
How does œà(x, 0) evolve with time? Since œàn(x), the Fourier component
(2.77), is an eigenfunction of H with energy En, the time-evolution operator
U(t) = exp(‚àíiHt/¬Øh) takes œà(x, 0) into
œà(x, t) = e‚àíiHt/¬Øh œà(x, 0) =
‚àû

n=1
fn
&
2
L sin
œÄnx
L

e‚àíiEnt/¬Øh.
(2.85)
Because En = (nœÄ ¬Øh)2/2m, the wave function at time t is
œà(x, t) =
‚àû

n=1
fn
&
2
L sin
œÄnx
L

e‚àíi¬Øh(nœÄ)2t/(2mL2).
(2.86)
It is awkward to plot complex functions, so Fig. 2.6 displays the probability
distributions P(x, t) = |œà(x, t)|2 of the 1000-term Fourier series (2.86) for the
wave function œà(x, t) at t = 0 (thick curve), t = 10‚àí3 œÑ (medium curve), and
œÑ = 2mL2/¬Øh (thin curve). The discontinuities in the initial wave function œà(x, 0)
cause both the Gibbs overshoots at x = 1/2 and x = 3/2 seen in the series
for œà(x, 0) plotted in Fig. 2.5 and the choppiness of the probability distribution
P(x, t) exhibited in Fig. (2.6).
91

FOURIER SERIES
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
x
|œà (x , t)|2
|œà (x , t)|2 for a piecewise continuous wave function
Figure 2.6
For an interval of length L = 2, the probability distributions P(x, t) =
|œà(x, t)|2 of the 1000-term Fourier series (2.86) for the wave function œà(x, t) at
t = 0 (thick curve), t = 10‚àí3 œÑ (medium curve), and œÑ = 2mL2/¬Øh (thin curve). The
jaggedness of P(x, t) arises from the two discontinuities in the initial wave function
œà(x, 0) (2.87) at x = L/4 and x = 3L/4.
Example 2.11 (Time evolution of a continuous function)
What does the Fourier
series of a continuous function look like? How does it evolve with time? Let us
take as the wave function at t = 0 the function
œà(x, 0) =
2
‚àö
L
sin
2œÄ(x ‚àíL/4)
L

for
L
4 < x < 3L
4
(2.87)
and zero otherwise. This initial wave function is a continuous function with a
piecewise continuous Ô¨Årst derivative on the interval [0, L], and it satisÔ¨Åes the peri-
odic boundary condition œà(0, 0) = œà(L, 0). It therefore satisÔ¨Åes the conditions
of Fourier‚Äôs convergence theorem (Courant, 1937, p. 439), and so its Fourier
series converges uniformly (and absolutely) to œà(x, 0) on [0, L].
As in equation (2.83), the Fourier coefÔ¨Åcients fn are given by the integrals
fn =
 L
0
dx
&
2
L sin
œÄnx
L

œà(x, 0),
(2.88)
which now take the form
fn = 2
‚àö
2
L
 3L/4
L/4
dx sin
œÄnx
L

sin
2œÄ(x ‚àíL/4)
L

.
(2.89)
92

2.8 QUANTUM-MECHANICAL EXAMPLES
Doing the integral, one Ô¨Ånds for fn that for n Ã∏= 2
fn = ‚àí
‚àö
2
œÄ
4
n2 ‚àí4 [sin(3nœÄ/4) + sin(nœÄ/4)]
(2.90)
while c2 = 0. These Fourier coefÔ¨Åcients satisfy the inequalities (2.56) and (2.58)
for k = 0. The factor of 1/n2 in fn guarantees the absolute convergence of the
series
œà(x, 0) =
‚àû

n=1
fn
&
2
L sin
œÄnx
L

(2.91)
because asymptotically the coefÔ¨Åcient fn is bounded by fn ‚â§A/n2 where A is
a constant (A = 144/(5œÄ
‚àö
L) will do) and the sum of 1/n2 converges to the
Riemann zeta function (4.92)
‚àû

n=1
1
n2 = Œ∂(2) = œÄ2
6 .
(2.92)
Figure 2.7 plots the ten-term Fourier series (2.91) for œà(x, 0) for L = 2.
Because this series converges absolutely and uniformly on [0, 2], the 100-term
and 1000-term series were too close to œà(x, 0) to be seen clearly in the Ô¨Ågure
and so were omitted.
As time goes by, the wave function œà(x, t) evolves from œà(x, 0) to
œà(x, t) =
‚àû

n=1
fn
&
2
L sin
œÄnx
L

e‚àíi¬Øh(nœÄ)2t/(2mL2)
(2.93)
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
‚àí0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
x
œà (x, 0)
Fourier series of a continuous function
Figure 2.7
The continuous wave function œà(x, 0) (2.87, solid) and its ten-term
Fourier series (2.90‚Äì2.91, dashes) are plotted for the interval [0, 2].
93

FOURIER SERIES
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.5
1
1.5
2
2.5
x
|œà (x, t)|2
Probability distribution of a continuous wave function
Figure 2.8
For the interval [0, 2], the probability distributions P(x, t) = |œà(x, t)|2
of the 1000-term Fourier series (2.93) for the wave function œà(x, t) at t = 0, 10‚àí2 œÑ,
10‚àí1 œÑ, œÑ = 2mL2/¬Øh, 10œÑ, and 100œÑ are plotted as successively thinner curves.
in which the Fourier coefÔ¨Åcients are given by (2.90). Because œà(x, 0) is contin-
uous and periodic with a piecewise continuous Ô¨Årst derivative, its evolution in
time is much calmer than that of the piecewise continuous square wave (2.80).
Figure 2.8 shows this evolution in successively thinner curves at times t = 0,
10‚àí2 œÑ, 10‚àí1 œÑ, œÑ = 2mL2/¬Øh, 10œÑ, and 100œÑ. The curves at t = 0 and t = 10‚àí2 œÑ
are smooth, but some wobbles appear at t = 10‚àí1 œÑ and at t = œÑ due to the
discontinuities in the Ô¨Årst derivative of œà(x, 0) at x = 0.5 and at x = 1.5.
Example 2.12 (Time evolution of a smooth wave function)
Finally, let‚Äôs try a
wave function œà(x, 0) that is periodic and inÔ¨Ånitely differentiable on [0, L]. An
inÔ¨Ånitely differentiable function is said to be smooth or C‚àû. The inÔ¨Ånite square-
well potential V(x) of equation (2.72) imposes the periodic boundary conditions
œà(0, 0) = œà(L, 0) = 0, so we try
œà(x, 0) =
&
2
3L
#
1 ‚àícos
2œÄx
L
$
.
(2.94)
Its Fourier series
œà(x, 0) =
&
1
6L

2 ‚àíe2œÄix/L ‚àíe‚àí2œÄix/L
(2.95)
has coefÔ¨Åcients that satisfy the upper bounds (2.56) by vanishing for |n| > 1.
The coefÔ¨Åcients of the Fourier sine series for the wave function œà(x, 0) are
given by the integrals (2.83)
94

2.8 QUANTUM-MECHANICAL EXAMPLES
fn =
 L
0
dx
&
2
L sin
œÄnx
L

œà(x, 0)
=
2
‚àö
3 L
 L
0
dx sin
œÄnx
L
 #
1 ‚àícos
2œÄx
L
$
= 8 [(‚àí1)n ‚àí1]
œÄ
‚àö
3 n(n2 ‚àí4)
(2.96)
with all the even coefÔ¨Åcients zero, c2n = 0. The fns are proportional to 1/n3,
which is more than enough to ensure the absolute and uniform convergence of
its Fourier sine series
œà(x, 0) =
‚àû

n=1
fn
&
2
L sin
œÄnx
L

.
(2.97)
As time goes by, it evolves to
œà(x, t) =
‚àû

n=1
fn
&
2
L sin
œÄnx
L

e‚àíi¬Øh(nœÄ)2t/(2mL2)
(2.98)
and remains absolutely convergent for all times t.
The effects of the absolute and uniform convergence with fn ‚àù1/n3 are obvi-
ous in the graphs. Figure 2.9 shows (for L = 2) that only ten terms are required
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
x
œà (x, 0)
Fourier series of a smooth function
Figure 2.9
The wave function œà(x, 0) (2.94) is inÔ¨Ånitely differentiable, and so the
Ô¨Årst ten terms of its uniformly convergent Fourier series (2.97) offer a very good
approximation to it.
95

FOURIER SERIES
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
x
|œà (x , t)|2
Probability distribution of a smooth wave function
Figure 2.10
The probability distributions P(x, t) = |œà(x, t)|2 of the 1000-term
Fourier series (2.98) for the wave function œà(x, t) at t = 0, 10‚àí2 œÑ, 10‚àí1 œÑ,
œÑ = 2mL2/¬Øh, 10œÑ, and 100œÑ are plotted as successively thinner curves. The time
evolution is calm because the wave function œà(x, 0) is smooth.
to nearly overlap the initial wave function œà(x, 0). Figure 2.10 shows that the
evolution of the probability distribution |œà(x, t)|2 with time is smooth, with no
sign of the jaggedness of Fig. 2.6 or the wobbles of Fig. 2.8. Because œà(x, 0) is
smooth and periodic, it evolves calmly as time passes.
2.9 Dirac notation
Vectors | j‚ü©that are orthonormal, ‚ü®k|j‚ü©= Œ¥k,j span a vector space and express
the identity operator I of the space as (1.132)
I =
N

j=1
| j‚ü©‚ü®j|.
(2.99)
Multiplying from the right by any vector |g‚ü©in the space, we get
|g‚ü©= I|g‚ü©=
N

j=1
| j‚ü©‚ü®j|g‚ü©,
(2.100)
which says that every vector |g‚ü©in the space has an expansion (1.133) in terms
of the N orthonormal basis vectors | j‚ü©. The coefÔ¨Åcients ‚ü®j|g‚ü©of the expansion
are inner products of the vector |g‚ü©with the basis vectors | j‚ü©.
96

2.10 DIRAC‚ÄôS DELTA FUNCTION
These properties of Ô¨Ånite-dimensional vector spaces also are true of inÔ¨Ånite-
dimensional vector spaces of functions. We may use as basis vectors the phases
exp(inx)/
‚àö
2œÄ. They are orthonormal with inner product (2.1)
(m, n) =
 2œÄ
0
 eimx
‚àö
2œÄ
‚àóeinx
‚àö
2œÄ
dx =
 2œÄ
0
ei(n‚àím)x
2œÄ
dx = Œ¥m,n,
(2.101)
which in Dirac notation with ‚ü®x|n‚ü©= exp(inx)/
‚àö
2œÄ and ‚ü®m|x‚ü©= ‚ü®x|m‚ü©‚àóis
‚ü®m|n‚ü©=
 2œÄ
0
‚ü®m|x‚ü©‚ü®x|n‚ü©dx =
 2œÄ
0
ei(n‚àím)x
2œÄ
dx = Œ¥m,n.
(2.102)
The identity operator for Fourier‚Äôs space of functions is
I =
‚àû

n=‚àí‚àû
|n‚ü©‚ü®n|.
(2.103)
So we have
|f ‚ü©= I|f ‚ü©=
‚àû

n=‚àí‚àû
|n‚ü©‚ü®n|f ‚ü©
(2.104)
and
‚ü®x|f ‚ü©= ‚ü®x|I|f ‚ü©=
‚àû

n=‚àí‚àû
‚ü®x|n‚ü©‚ü®n|f ‚ü©=
‚àû

n=‚àí‚àû
einx
‚àö
2œÄ
‚ü®n|f ‚ü©,
(2.105)
which with ‚ü®n|f ‚ü©= fn is the Fourier series (2.2). The coefÔ¨Åcients ‚ü®n|f ‚ü©= fn are
the inner products (2.3)
‚ü®n|f ‚ü©=
 2œÄ
0
‚ü®n|x‚ü©‚ü®x|f ‚ü©dx =
 2œÄ
0
e‚àíinx
‚àö
2œÄ
‚ü®x|f ‚ü©dx =
 2œÄ
0
e‚àíinx
‚àö
2œÄ
f (x) dx.
(2.106)
2.10 Dirac‚Äôs delta function
A Dirac delta function is a (continuous, linear) map from a space of functions
into the real or complex numbers. It is a functional that associates a number
with each function in the function space. Thus Œ¥(x ‚àíy) associates the number
f (y) with the function f (x). We may write this association as
f (y) =

f (x) Œ¥(x ‚àíy) dx.
(2.107)
97

FOURIER SERIES
Delta functions pop up all over physics. The inner product of two of the
kets |x‚ü©that appear in the Fourier-series formulas (2.105) and (2.106) is a delta
function, ‚ü®x|y‚ü©= Œ¥(x‚àíy). The formula (2.106) for the coefÔ¨Åcient ‚ü®n|f ‚ü©becomes
obvious if we write the identity operator for functions deÔ¨Åned on the interval
[0, 2œÄ] as
I =
 2œÄ
0
|x‚ü©‚ü®x| dx
(2.108)
for then
‚ü®n|f ‚ü©= ‚ü®n|I|f ‚ü©=
 2œÄ
0
‚ü®n|x‚ü©‚ü®x|f ‚ü©dx =
 2œÄ
0
e‚àíinx
‚àö
2œÄ
‚ü®x|f ‚ü©dx.
(2.109)
The equation |y‚ü©= I|y‚ü©with the identity operator (2.108) gives
|y‚ü©= I|y‚ü©=
 2œÄ
0
|x‚ü©‚ü®x|y‚ü©dx.
(2.110)
Multiplying (2.108) from the right by |f ‚ü©and from the left by ‚ü®y|, we get
f (y) = ‚ü®y|I|f ‚ü©=
 2œÄ
0
‚ü®y|x‚ü©‚ü®x|f ‚ü©dx =
 2œÄ
0
‚ü®y|x‚ü©f (x) dx.
(2.111)
These relations (2.110) and (2.111) say that the inner product ‚ü®y|x‚ü©is a delta
function, ‚ü®y|x‚ü©= ‚ü®x|y‚ü©= Œ¥(x ‚àíy).
The Fourier-series formulas (2.105) and (2.106) lead to a statement about the
completeness of the phases exp(inx)/
‚àö
2œÄ
f (x) =
‚àû

n=‚àí‚àû
fn
einx
‚àö
2œÄ
=
‚àû

n=‚àí‚àû
 2œÄ
0
e‚àíiny
‚àö
2œÄ
f (y) einx
‚àö
2œÄ
dy.
(2.112)
Interchanging and rearranging, we have
f (x) =
 2œÄ
0

‚àû

n=‚àí‚àû
ein(x‚àíy)
2œÄ

f (y) dy.
(2.113)
But the phases (2.112) are periodic with period 2œÄ, so we also have
f (x + 2œÄi) =
 2œÄ
0

‚àû

n=‚àí‚àû
ein(x‚àíy)
2œÄ

f (y) dy.
(2.114)
98

2.10 DIRAC‚ÄôS DELTA FUNCTION
Thus we arrive at the Dirac comb
‚àû

n=‚àí‚àû
ein(x‚àíy)
2œÄ
=
‚àû

‚Ñì=‚àí‚àû
Œ¥(x ‚àíy ‚àí2œÄ‚Ñì)
(2.115)
or more simply
‚àû

n=‚àí‚àû
einx
2œÄ = 1
2œÄ

1 + 2
‚àû

n=1
cos(nx)

=
‚àû

‚Ñì=‚àí‚àû
Œ¥(x ‚àí2œÄ‚Ñì).
(2.116)
Example 2.13 (Dirac‚Äôs comb)
The sum of the Ô¨Årst 100,000 terms of this cosine
series (2.116) for the Dirac comb is plotted for the interval (‚àí15, 15) in Fig. 2.11.
Gibbs overshoots appear at the discontinuities. The integral of the Ô¨Årst 100,000
terms from ‚àí15 to 15 is 5.0000.
‚àí15
‚àí10
‚àí5
0
5
10
15
‚àí1
‚àí0.5
0
0.5
1
1.5
2
2.5
3
3.5 x 104
x
Sum of series
Dirac comb
Figure 2.11
The sum of the Ô¨Årst 100,000 terms of this cosine series (2.116) for
the Dirac comb is plotted for ‚Äì15 ‚â§x ‚â§15.
The stretched Dirac comb is
‚àû

n=‚àí‚àû
e2œÄin(x‚àíy)/L
L
=
‚àû

‚Ñì=‚àí‚àû
Œ¥(x ‚àíy ‚àí‚ÑìL).
(2.117)
99

FOURIER SERIES
Example 2.14 (Parseval‚Äôs identity)
Using our formula (2.35) for the Fourier
coefÔ¨Åcients of a stretched interval, we can relate a sum of products f ‚àó
n gn of the
Fourier coefÔ¨Åcients of the functions f (x) and g(x) to an integral of the product
f ‚àó(x) g(x)
‚àû

n=‚àí‚àû
f ‚àó
n gn =
‚àû

n=‚àí‚àû
 L
0
dx ei2œÄnx/L
‚àö
L
f ‚àó(x)
 L
0
dy e‚àíi2œÄny/L
‚àö
L
g(y).
(2.118)
This sum contains Dirac‚Äôs comb (2.117) and so
‚àû

n=‚àí‚àû
f ‚àó
n gn =
 L
0
dx
 L
0
dy f ‚àó(x) g(y) 1
L
‚àû

n=‚àí‚àû
ei2œÄn(x‚àíy)/L
=
 L
0
dx
 L
0
dy f ‚àó(x) g(y)
‚àû

‚Ñì=‚àí‚àû
Œ¥(x ‚àíy ‚àí2œÄ‚ÑìL).
(2.119)
But because only the ‚Ñì= 0 tooth of the comb lies in the interval [0, L], we have
more simply
‚àû

n=‚àí‚àû
f ‚àó
n gn =
 L
0
dx
 L
0
dy f ‚àó(x) g(y) Œ¥(x ‚àíy) =
 L
0
dx f ‚àó(x) g(x).
(2.120)
In particular, if the two functions are the same, then
‚àû

n=‚àí‚àû
| fn|2 =
 L
0
dx | f (x)|2,
(2.121)
which is Parseval‚Äôs identity. Thus if a function is square integrable on an interval,
then the sum of the squares of the absolute values of its Fourier coefÔ¨Åcients is
the integral of the square of its absolute value.
Example 2.15 (Derivatives of delta functions)
Delta functions and other gen-
eralized functions or distributions map smooth functions that vanish at inÔ¨Ånity
into numbers in ways that are linear and continuous. Derivatives of delta
functions are deÔ¨Åned so as to allow integration by parts. Thus the nth deriva-
tive of the delta function Œ¥(n)(x‚àíy) maps the function f (x) to (‚àí1)n times its nth
derivative f (n)(y) at y

Œ¥(n)(x ‚àíy) f (x) dx =

Œ¥(x ‚àíy) (‚àí1)n f (n)(x) dx = (‚àí1)n f (n)(y)
(2.122)
with no surface term.
Example 2.16 (The equation xf (x) = a)
Dirac‚Äôs delta function sometimes
appears unexpectedly. For instance, the general solution to the equation
100

2.11 THE HARMONIC OSCILLATOR
x f (x) = a is f (x) = a/x + b Œ¥(x) where b is an arbitrary constant (Dirac, 1967,
sec. 15, Waxman and Peck, 1998). Similarly, the general solution to the equation
x2 f (x) = a is f (x) = a/x2 + b Œ¥(x)/x + c Œ¥(x) + d Œ¥‚Ä≤(x) in which Œ¥‚Ä≤(x) is the
derivative of the delta function, and b, c, and d are arbitrary constants.
2.11 The harmonic oscillator
The hamiltonian for the harmonic oscillator is
H = p2
2m + 1
2mœâ2q2.
(2.123)
The commutation relation [q, p] ‚â°qp ‚àípq = i¬Øh implies that the lowering and
raising operators
a =
&mœâ
2¬Øh

q + ip
mœâ

and
a‚Ä† =
&mœâ
2¬Øh

q ‚àíip
mœâ

(2.124)
obey the commutation relation [a, a‚Ä†] = 1. In terms of a and a‚Ä†, which also are
called the annihilation and creation operators, the hamiltonian H has the simple
form
H = ¬Øhœâ

a‚Ä†a + 1
2

.
(2.125)
There is a unique state |0‚ü©that is annihilated by the operator a, as may be
seen by solving the differential equation
‚ü®q‚Ä≤|a|0‚ü©=
&mœâ
2¬Øh ‚ü®q‚Ä≤|

q + ip
mœâ

|0‚ü©= 0.
(2.126)
Since ‚ü®q‚Ä≤|q = q‚Ä≤‚ü®q‚Ä≤| and
‚ü®q‚Ä≤|p|0‚ü©= ¬Øh
i
d‚ü®q‚Ä≤|0‚ü©
dq‚Ä≤
(2.127)
the resulting differential equation is
d‚ü®q‚Ä≤|0‚ü©
dq‚Ä≤
= ‚àímœâ
¬Øh q‚Ä≤‚ü®q‚Ä≤|0‚ü©.
(2.128)
Its suitably normalized solution is the wave function for the ground state of the
harmonic oscillator
‚ü®q‚Ä≤|0‚ü©=
mœâ
œÄ ¬Øh
1/4
exp

‚àímœâq‚Ä≤2
2¬Øh

.
(2.129)
101

FOURIER SERIES
For n = 0, 1, 2, . . ., the nth eigenstate of the hamiltonian H is
|n‚ü©=
1
‚àö
n!

a‚Ä†n
|0‚ü©
(2.130)
where n! ‚â°n(n ‚àí1) . . . 1 is n-factorial and 0! = 1. Its energy is
H|n‚ü©= ¬Øhœâ

n + 1
2

|n‚ü©.
(2.131)
The identity operator is
I =
‚àû

n=0
|n‚ü©‚ü®n|.
(2.132)
An arbitrary state |œà‚ü©has an expansion in terms of the eigenstates |n‚ü©
|œà‚ü©= I|œà‚ü©=
‚àû

n=0
|n‚ü©‚ü®n|œà‚ü©
(2.133)
and evolves in time like a Fourier series
|œà, t‚ü©= e‚àíiHt/¬Øh|œà‚ü©= e‚àíiHt/¬Øh
‚àû

n=0
|n‚ü©‚ü®n|œà‚ü©= e‚àíiœât/2
‚àû

n=0
e‚àíinœât|n‚ü©‚ü®n|œà‚ü©(2.134)
with wave function
œà(q, t) = ‚ü®q|œà, t‚ü©= e‚àíiœât/2
‚àû

n=0
e‚àíinœât‚ü®q|n‚ü©‚ü®n|œà‚ü©.
(2.135)
The wave functions ‚ü®q|n‚ü©of the energy eigenstates are related to the Hermite
polynomials (example 8.6)
Hn(x) = (‚àí1)nex2 dn
dxn e‚àíx2
(2.136)
by a change of variables x =

mœâ/¬Øh q ‚â°sq and a normalization factor
œàn(q) = ‚ü®q|n‚ü©=
‚àös e‚àí(sq)2/2

2nn!‚àöœÄ
Hn(sq) = mœâ
œÄ ¬Øh
1/4 e‚àímœâq2/2¬Øh
‚àö
2nn!
Hn
mœâ
¬Øh
1/2
q

.
(2.137)
The coherent state |Œ±‚ü©
|Œ±‚ü©= e‚àí|Œ±|2/2eŒ±a‚Ä†|0‚ü©= e‚àí|Œ±|2/2
‚àû

n=0
Œ±n
‚àö
n!
|n‚ü©
(2.138)
is an eigenstate a|Œ±‚ü©= Œ±|Œ±‚ü©of the lowering (or annihilation) operator a with
eigenvalue Œ±. Its time evolution is simply
|Œ±, t‚ü©= e‚àíiœât/2e‚àí|Œ±|2/2
‚àû

n=0

Œ±e‚àíiœâtn
‚àö
n!
|n‚ü©= e‚àíiœât/2 |Œ±e‚àíiœât‚ü©.
(2.139)
102

2.13 PERIODIC BOUNDARY CONDITIONS
2.12 Nonrelativistic strings
If we clamp the ends of a nonrelativistic string at x = 0 and x = L, then the
amplitude y(x, t) will obey the boundary conditions
y(0, t) = y(L, t) = 0
(2.140)
and the wave equation
v2 ‚àÇ2y
‚àÇx2 = ‚àÇ2y
‚àÇt2
(2.141)
as long as y(x, t) remains small. The functions
yn(x, t) = sin nœÄx
L

fn sin nœÄvt
L
+ dn cos nœÄvt
L

(2.142)
satisfy this wave equation (2.141) and the boundary conditions (2.140). They
represent waves traveling along the x-axis with speed v.
The space SL of functions f (x) that satisfy the boundary condition (2.140) is
spanned by the functions sin(nœÄx/L). One may use the integral formula
 L
0
sin nœÄx
L
sin mœÄx
L
dx = L
2 Œ¥nm
(2.143)
to derive for any function f ‚ààSL the Fourier series
f (x) =
‚àû

n=1
fn sin nœÄx
L
(2.144)
with coefÔ¨Åcients
fn = 2
L
 L
0
sin nœÄx
L
f (x) dx
(2.145)
and the representation
‚àû

m=‚àí‚àû
Œ¥(x ‚àíz ‚àí2mL) = 2
L
‚àû

n=1
sin nœÄx
L
sin nœÄz
L
(2.146)
for the Dirac comb on SL.
2.13 Periodic boundary conditions
The inÔ¨Ånite square-well potential (2.72) enforced the boundary conditions
œà(L, t) = œà(0, t) = 0, which are periodic. But periodic boundary conditions
occur in other ways too. For instance, rather than studying an inÔ¨Ånitely long
one-dimensional system, we might study the same system, but of length L. The
103

FOURIER SERIES
ends cause effects not present in the inÔ¨Ånite system. To avoid them, we imagine
that the system forms a circle and impose the periodic boundary condition
œà(x ¬± L, t) = œà(x, t).
(2.147)
In three dimensions, the analogous conditions are
œà(L, y, z, t) = œà(0, y, z, t),
œà(x, L, z, t) = œà(x, 0, z, t),
œà(x, y, L, t) = œà(x, y, 0, t).
(2.148)
The eigenstates |p‚ü©of the free hamiltonian H = p2/2m have wave functions
œàp(x) = ‚ü®x|p‚ü©= eix¬∑p/¬Øh/(2œÄ ¬Øh)3/2.
(2.149)
The periodic boundary conditions (2.148) require that each component pi of
the momentum satisfy Lpi/¬Øh = 2œÄni or
p = 2œÄ ¬Øhn
L
= hn
L
(2.150)
where n is a vector of integers, which may be positive or negative or zero.
Periodic boundary conditions arise naturally in the study of solids. The
atoms of a perfect crystal are at the vertices of a Bravais lattice
xi = x0 +
3

i=1
niai
(2.151)
in which the three vectors ai are the primitive vectors of the lattice and the ni
are three integers. The hamiltonian of such an inÔ¨Ånite crystal is invariant under
translations in space by
3

i=1
niai.
(2.152)
To keep the notation simple, let‚Äôs restrict ourselves to a cubic lattice with
lattice spacing a. Then since the momentum operator p generates translations
in space, the invariance of H under translations by a n
exp(ian ¬∑ p)H exp(‚àíian ¬∑ p) = H
(2.153)
implies that p and H are compatible observables [p, H] = 0. As explained in
section 1.30, it follows that we may choose the eigenstates of H also to be
eigenstates of p
eiap¬∑n/¬Øh|œà‚ü©= eiak¬∑n |œà‚ü©,
(2.154)
which implies that
œà(x + an, t) = eiak¬∑n œà(x, t).
(2.155)
104

EXERCISES
Setting
œà(x) = eik¬∑x u(x)
(2.156)
we see that condition (2.155) implies that u(x) is periodic
u(x + an) = u(x).
(2.157)
For a general Bravais lattice, this Born‚Äìvon Karman periodic boundary condi-
tion is
u

x +
3

i=1
niai, t

= u(x, t).
(2.158)
Equations (2.155) and (2.157) are known as Bloch‚Äôs theorem.
Exercises
2.1
Show that sin œâ1x + sin œâ2x is the same as (2.9).
2.2
Find the Fourier series for the function exp(ax) on the interval ‚àíœÄ < x ‚â§œÄ.
2.3
Find the Fourier series for the function (x2 ‚àíœÄ2)2 on the same interval
(‚àíœÄ, œÄ].
2.4
Find the Fourier series for the function (1 + cos x) sin ax on the interval
(‚àíœÄ, œÄ].
2.5
Show that the Fourier series for the function x cos x on the interval [ ‚àíœÄ, œÄ]
is
x cos x = ‚àí1
2 sin x + 2
‚àû

n=2
(‚àí1)n n
n2 ‚àí1 sin nx.
(2.159)
2.6
(a) Show that the Fourier series for the function |x| on the interval [ ‚àíœÄ, œÄ] is
|x| = œÄ
2 ‚àí4
œÄ
‚àû

n=0
cos(2n + 1)x
(2n + 1)2
.
(2.160)
(b) Use this result to Ô¨Ånd a neat formula for œÄ2/8. Hint: set x = 0.
2.7
Show that the Fourier series for the function | sin x| on the interval [ ‚àíœÄ, œÄ] is
| sin x| = 2
œÄ ‚àí4
œÄ
‚àû

n=1
cos 2nx
4n2 ‚àí1.
(2.161)
2.8
Show that the Fourier coefÔ¨Åcients of the C 0 function (2.59) on the interval
[ ‚àíœÄ, œÄ] are given by (2.60).
2.9
Find by inspection the Fourier series for the function exp[exp(‚àíix)].
2.10 Fill in the steps in the computation (2.28) of the Fourier series for x2.
2.11 Do the Ô¨Årst integral in equation (2.65). Hint: differentiate ln(sin x/x).
105

FOURIER SERIES
2.12 Use the inÔ¨Ånite-product formula (2.66) for the sine and the relation cos œÄx =
sin 2œÄx/(2 sin œÄx) to derive the inÔ¨Ånite-product formula (2.67) for the cosine.
Hint:
‚àû

n=1

1 ‚àíx2
1
4n2

=
‚àû

n=1

1 ‚àí
x2
1
4(2n ‚àí1)2
 
1 ‚àí
x2
1
4(2n)2

.
(2.162)
2.13 What‚Äôs the general solution to the equation x3f (x) = a?
2.14 Suppose we wish to approximate the real square-integrable function f (x) by
the Fourier series with N terms
fN(x) = a0
2 +
N

n=1
(an cos nx + bn sin nx) .
(2.163)
Then the error
EN =
 2œÄ
0
[f (x) ‚àífN(x)]2 dx
(2.164)
will depend upon the 2N + 1 coefÔ¨Åcients an and bn. The best coefÔ¨Åcients
minimize this error and satisfy the conditions
‚àÇEN
‚àÇan
= ‚àÇEN
‚àÇbn
= 0.
(2.165)
By using these conditions, Ô¨Ånd them.
2.15 Find the Fourier series for the function
f (x) = Œ∏(a2 ‚àíx2)
(2.166)
on the interval [‚àíœÄ, œÄ] for the case a2 < œÄ2. The Heaviside step function Œ∏(x)
is zero for x < 0, one-half for x = 0, and unity for x > 0 (Oliver Heaviside,
1850‚Äì1925). The value assigned to Œ∏(0) seldom matters, and you need not
worry about it in this problem.
2.16 Derive or infer the formula (2.117) for the stretched Dirac comb.
2.17 Use the commutation relation [q, p] = i¬Øh to show that the annihilation and
creation operators (2.124) satisfy the commutation relation [a, a‚Ä†] = 1.
2.18 Show that the state |n‚ü©= (a‚Ä†)n|0‚ü©/
‚àö
n! is an eigenstate of the hamiltonian
(2.125) with energy ¬Øhœâ(n + 1/2).
2.19 Show that the coherent state |Œ±‚ü©(2.138) is an eigenstate of the annihilation
operator a with eigenvalue Œ±.
2.20 Derive equations (2.145 & 2.146) from the expansion (2.144) and the integral
formula (2.143).
2.21 Consider a string like the one described in section 2.12, which satisÔ¨Åes the
boundary conditions (2.140) and the wave equation (2.141). The string is at
rest at time t = 0
y(x, 0) = 0
(2.167)
106

EXERCISES
and is struck precisely at t = 0 and x = a so that
‚àÇy(x, t)
‚àÇt

t=0
= Lv0Œ¥(x ‚àía).
(2.168)
Find y(x, t) and Àôy(x, t), where the dot means time derivative.
2.22 Same as exercise (2.21), but now the initial conditions are
u(x, 0) = f (x)
and
Àôu(x, 0) = g(x)
(2.169)
in which f (0) = f (L) = 0 and g(0) = g(L) = 0. Find the motion of the
amplitude u(x, t) of the string.
2.23 (a) Find the Fourier series for the function f (x) = x2 on the interval [‚àíœÄ, œÄ].
(b) Use your result at x = œÄ to show that
‚àû

n=1
1
n2 = œÄ2
6
(2.170)
which is the value of Riemann‚Äôs zeta function (4.92) Œ∂(x) at x = 2.
107

3
Fourier and Laplace transforms
The complex exponentials exp(i2œÄnx/L) are orthonormal and easy to
differentiate (and to integrate), but they are periodic with period L. If one wants
to represent functions that are not periodic, then a better choice is the complex
exponentials exp(ikx), where k is an arbitrary real number. These orthonormal
functions are the basis of the Fourier transform. The choice of complex k leads
to the transforms of Laplace, Mellin, and Bromwich.
3.1 The Fourier transform
The interval [‚àíL/2, L/2] is arbitrary in the Fourier series pair (2.37)
f (x) =
‚àû

n=‚àí‚àû
fn
ei2œÄnx/L
‚àö
L
and
fn =
 L/2
‚àíL/2
f (x) e‚àíi2œÄnx/L
‚àö
L
dx.
(3.1)
What happens when we stretch this interval without limit, letting L ‚Üí‚àû?
We may use the nearest-integer function [y] to convert the coefÔ¨Åcients fn into
a continuous function ÀÜf (y) ‚â°f[y] such that ÀÜf (y) = fn when |y ‚àín| < 1/2. In
terms of this function ÀÜf (y), the Fourier series (3.1) for the function f (x) is
f (x) =
‚àû

n=‚àí‚àû
 n+1/2
n‚àí1/2
ÀÜf (y) ei2œÄ[y]x/L
‚àö
L
dy =
 ‚àû
‚àí‚àû
ÀÜf (y) ei2œÄ[y]x/L
‚àö
L
dy.
(3.2)
Since [y] and y differ by no more than 1/2, the absolute value of the difference
between exp(iœÄ[y]x/L) and exp(iœÄyx/L) for Ô¨Åxed x is
ei2œÄ[y]x/L ‚àíei2œÄyx/L
 =
ei2œÄ([y]‚àíy)x/L ‚àí1
 ‚âàœÄ|x|
L ,
(3.3)
108

3.1 THE FOURIER TRANSFORM
which goes to zero as L ‚Üí‚àû. So in this limit, we may replace [y] by y and
express f (x) as
f (x) =
 ‚àû
‚àí‚àû
ÀÜf (y) ei2œÄyx/L
‚àö
L
dy.
(3.4)
We now change variables to k = 2œÄy/L and Ô¨Ånd for f (x) the integral
f (x) =
 ‚àû
‚àí‚àû
ÀÜf
Lk
2œÄ
 eikx
‚àö
L
L
2œÄ dk =
 ‚àû
‚àí‚àû
&
L
2œÄ
ÀÜf
Lk
2œÄ

eikx
dk
‚àö
2œÄ
.
(3.5)
So in terms of the Fourier transform Àúf (k) deÔ¨Åned as
Àúf (k) =
&
L
2œÄ
Àúf
Lk
2œÄ

(3.6)
the integral (3.5) for f (x) is the inverse Fourier transform
f (x) =
 ‚àû
‚àí‚àû
Àúf (k) eikx
dk
‚àö
2œÄ
.
(3.7)
To Ô¨Ånd Àúf (k), we use its deÔ¨Ånition (3.6), the deÔ¨Ånition (3.1) of fn, our formula
ÀÜf (y) = f[y], and the inequality |2œÄ [Lk/2œÄ] /L ‚àík| ‚â§œÄ/2L to write
Àúf (k) =
&
L
2œÄ f
Lk
2œÄ
 =
&
L
2œÄ
 L/2
‚àíL/2
f (x) e
‚àíi2œÄ

Lk
2œÄ

x
L
‚àö
L
dx ‚âà
 L/2
‚àíL/2
f (x) e‚àíikx dx
‚àö
2œÄ
.
This formula becomes exact in the limit L ‚Üí‚àû,
Àúf (k) =
 ‚àû
‚àí‚àû
f (x) e‚àíikx
dx
‚àö
2œÄ
(3.8)
and so we have the Fourier transformations
f (x) =
 ‚àû
‚àí‚àû
Àúf (k) eikx
dk
‚àö
2œÄ
and
Àúf (k) =
 ‚àû
‚àí‚àû
f (x) e‚àíikx
dx
‚àö
2œÄ
.
(3.9)
The function Àúf (k) is the Fourier transform of f (x), and f (x) is the inverse Fourier
transform of Àúf (k).
In these symmetrical relations (3.9), the distinction between a Fourier trans-
form and an inverse Fourier transform is entirely a matter of convention. There
is no rule for which sign, ikx or ‚àíikx, goes with which transform or for where
to put the 2œÄs. Thus one often sees
f (x) =
 ‚àû
‚àí‚àû
Àúf (k) e¬±ikx dk
and
Àúf (k) =
 ‚àû
‚àí‚àû
f (x) e‚àìikx dx
2œÄ
(3.10)
109

FOURIER AND LAPLACE TRANSFORMS
as well as
f (x) =
 ‚àû
‚àí‚àû
Àúf (k) e¬±ikx dk
2œÄ
and
Àúf (k) =
 ‚àû
‚àí‚àû
f (x) e‚àìikx dx.
(3.11)
One often needs to go back and forth between the representations of a given
function as a Fourier series and as a Fourier transform. So stripping away fac-
tors of
‚àö
2œÄ and
‚àö
L with the notation `fn = fn/
‚àö
L, `f (k) = Àúf (k)/
‚àö
2œÄ, and
kn = 2œÄn/L, let‚Äôs compare the Fourier series (3.1) for the function f (x) with its
Fourier transform (3.9) in the limit of very large L
f (x) =
‚àû

n=‚àí‚àû
`fn eiknx =
 ‚àû
‚àí‚àû
`f (k) eikx dk.
(3.12)
Using the deÔ¨Ånition (3.6) of Àúf (k) as

L/(2œÄ) fn, we have
`f (k) =
Àúf (k)
‚àö
2œÄ
=
‚àö
L
2œÄ fn = L
2œÄ
`fn
(3.13)
in which n = Lkn/(2œÄ) is the integer nearest to Lk/(2œÄ). That is,
`fn = `f[Lk/(2œÄ)] = 2œÄ
L
`f (k).
(3.14)
Example 3.1 (The Fourier transform of a gaussian is a gaussian)
The Fourier
transform of the gaussian f (x) = exp(‚àím2 x2) is
Àúf (k) =
 ‚àû
‚àí‚àû
dx
‚àö
2œÄ
e‚àíikx e‚àím2 x2.
(3.15)
We complete the square in the exponent:
Àúf (k) = e‚àík2/4m2  ‚àû
‚àí‚àû
dx
‚àö
2œÄ
e‚àím2 (x+ik/2m2)2.
(3.16)
As we shall see in section 5.14 when we study analytic functions, we may shift x
to x ‚àíik/2m2, so the term ik/2m2 in the exponential has no effect on the value
of the x-integral.
Àúf (k) = e‚àík2/4m2  ‚àû
‚àí‚àû
dx
‚àö
2œÄ
e‚àím2 x2 =
1
‚àö
2 m
e‚àík2/4m2.
(3.17)
Thus, the Fourier transform of a gaussian is another gaussian
Àúf (k) =
 ‚àû
‚àí‚àû
dx
‚àö
2œÄ
e‚àíikx e‚àím2 x2 =
1
‚àö
2 m
e‚àík2/4m2.
(3.18)
But the two gaussians are very different: if the gaussian f (x) = exp(‚àím2x2)
decreases slowly as x ‚Üí‚àûbecause m is small (or quickly because m is big), then
110

3.2 THE FOURIER TRANSFORM OF A REAL FUNCTION
its gaussian Fourier transform Àúf (k) = exp(‚àík2/4m2)/m
‚àö
2 decreases quickly as
k ‚Üí‚àûbecause m is small (or slowly because m is big).
Can we invert Àúf (k) to get f (x)? The inverse Fourier transform (3.7) says
f (x) =
 ‚àû
‚àí‚àû
dk
‚àö
2œÄ
Àúf (k) eikx =
 ‚àû
‚àí‚àû
dk
‚àö
2œÄ
1
m
‚àö
2
eikx‚àík2/4m2.
(3.19)
By again completing the square in the exponent
f (x) = e‚àím2x2  ‚àû
‚àí‚àû
dk
‚àö
2œÄ
1
m
‚àö
2
e‚àí(k‚àíi2m2x)2/4m2
(3.20)
and shifting the variable of integration k to k + i2m2x, we Ô¨Ånd
f (x) = e‚àím2x2  ‚àû
‚àí‚àû
dk
‚àö
2œÄ
1
m
‚àö
2
e‚àík2/(4m2) = e‚àím2x2,
(3.21)
which is reassuring.
Using (3.17) for Àúf (k) and the connections (3.12‚Äì3.14) between Fourier series
and transforms, we see that a Fourier series for this gaussian is in the limit of
L ‚â´x
f (x) = e‚àím2x2 = 2œÄ
L
‚àû

n=‚àí‚àû
1
‚àö
4œÄ m
e‚àík2n/(4m2)eiknx
(3.22)
in which kn = 2œÄn/L.
3.2 The Fourier transform of a real function
If the function f (x) is real, then its Fourier transform (3.8)
Àúf (k) =
 ‚àû
‚àí‚àû
dx
‚àö
2œÄ
f (x) e‚àíikx
(3.23)
obeys the relation
Àúf ‚àó(k) = Àúf (‚àík).
(3.24)
For since f ‚àó(x) = f (x), the complex conjugate of (3.23) is
Àúf ‚àó(k) =
 ‚àû
‚àí‚àû
dx
‚àö
2œÄ
f (x) eikx = Àúf (‚àík).
(3.25)
It follows that a real function f (x) satisÔ¨Åes the relation
f (x) = 1
œÄ
 ‚àû
0
dk
 ‚àû
‚àí‚àû
f (y) cos k(y ‚àíx) dy
(3.26)
(exercise 3.1) as well as
f (x) = 2
œÄ
 ‚àû
0
cos kx dk
 ‚àû
0
f (y) cos ky dy
(3.27)
111

FOURIER AND LAPLACE TRANSFORMS
if it is even, and
f (x) = 2
œÄ
 ‚àû
0
sin kx dk
 ‚àû
0
f (y) sin ky dy
(3.28)
if it is odd (exercise 3.2).
Example 3.2 (Dirichlet‚Äôs discontinuous factor)
Using (3.27), one may write the
square wave
f (x) =
‚éß
‚é®
‚é©
1,
|x| < 1
1
2,
|x| = 1
0,
|x| > 1
(3.29)
as Dirichlet‚Äôs discontinuous factor
f (x) = 2
œÄ
 ‚àû
0
sin k cos kx
k
dk
(3.30)
(exercise 3.3).
Example 3.3 (Even and odd exponentials)
By using the Fourier-transform
formulas (3.27 & 3.28), one may show that the Fourier transform of the even
exponential exp(‚àíŒ≤|x|) is
e‚àíŒ≤|x| = 2
œÄ
 ‚àû
0
Œ≤ cos kx
Œ≤2 + k2 dk
(3.31)
while that of the odd exponential x exp(‚àíŒ≤|x|)/|x| is
x
|x|e‚àíŒ≤|x| = 2
œÄ
 ‚àû
0
k sin kx
Œ≤2 + k2 dk
(3.32)
(exercise 3.4).
3.3 Dirac, Parseval, and Poisson
Combining the basic equations (3.9) that deÔ¨Åne the Fourier transform, we may
do something apparently useless: we may write the function f (x) in terms of
itself as
f (x) =
 ‚àû
‚àí‚àû
dk
‚àö
2œÄ
Àúf (k) eikx =
 ‚àû
‚àí‚àû
dk
‚àö
2œÄ
eikx
 ‚àû
‚àí‚àû
dy
‚àö
2œÄ
e‚àíiky f (y).
(3.33)
Let‚Äôs compare this equation
f (x) =
 ‚àû
‚àí‚àû
dy
 ‚àû
‚àí‚àû
dk
2œÄ exp[ik(x ‚àíy)]

f (y)
(3.34)
112

3.3 DIRAC, PARSEVAL, AND POISSON
with one (2.107) that describes Dirac‚Äôs delta function
f (x) =
 ‚àû
‚àí‚àû
dy Œ¥(x ‚àíy) f (y).
(3.35)
Thus for functions with sensible Fourier transforms, the delta function is
Œ¥(x ‚àíy) =
 ‚àû
‚àí‚àû
dk
2œÄ exp[ik(x ‚àíy)].
(3.36)
The inner product (f , g) of two functions, f (x) with Fourier transform Àúf (k)
and g(x) with Fourier transform ÀÜg(k), is
‚ü®f |g‚ü©= (f , g) =
 ‚àû
‚àí‚àû
dx f ‚àó(x) g(x).
(3.37)
Since f (x) and g(x) are related to Àúf (k) and Àúg(k) by the Fourier transform (3.8),
their inner product (f , g) is
(f , g) =
 ‚àû
‚àí‚àû
dx
 ‚àû
‚àí‚àû
dk
‚àö
2œÄ

Àúf (k) eikx‚àó ‚àû
‚àí‚àû
dk‚Ä≤
‚àö
2œÄ
Àúg(k‚Ä≤) eik‚Ä≤x
=
 ‚àû
‚àí‚àû
dk
 ‚àû
‚àí‚àû
dk‚Ä≤
 ‚àû
‚àí‚àû
dx
2œÄ eix(k‚Ä≤‚àík) Àúf ‚àó(k) Àúg(k‚Ä≤)
(3.38)
=
 ‚àû
‚àí‚àû
dk
 ‚àû
‚àí‚àû
dk‚Ä≤ Œ¥(k‚Ä≤ ‚àík) Àúf ‚àó(k) Àúg(k‚Ä≤) =
 ‚àû
‚àí‚àû
dk Àúf ‚àó(k) Àúg(k).
Thus we arrive at Parseval‚Äôs relation
(f , g) =
 ‚àû
‚àí‚àû
dx f ‚àó(x) g(x) =
 ‚àû
‚àí‚àû
dk Àúf ‚àó(k) Àúg(k) = (Àúf , Àúg),
(3.39)
which says that the inner product of two functions is the same as the inner prod-
uct of their Fourier transforms. The Fourier transform is a unitary transform.
In particular, if f = g, then
‚ü®f |f ‚ü©= (f , f ) =
 ‚àû
‚àí‚àû
dx |f (x)|2 =
 ‚àû
‚àí‚àû
dk | Àúf (k)|2
(3.40)
(Marc-Antoine Parseval des Ch√™nes, 1755‚Äì1836).
In fact, one may show that the Fourier transform maps the space of
(Lebesgue) square-integrable functions onto itself in a one-to-one manner.
Thus the natural space for the Fourier transform is the space of square-
integrable functions, and so the representation (3.36) of Dirac‚Äôs delta function
is suitable for continuous square-integrable functions.
This may be a good place to say a few words about how to evaluate integrals
involving delta functions of more complicated arguments, such as
J =

Œ¥(g(x)) f (x) dx.
(3.41)
113

FOURIER AND LAPLACE TRANSFORMS
To see how this works, let‚Äôs assume that g(x) vanishes at a single point x0 at
which its derivative g‚Ä≤(x0) Ã∏= 0 isn‚Äôt zero. Then the integral J involves f only as
f (x0), which we can bring outside as a prefactor
J = f (x0)

Œ¥(g(x)) dx.
(3.42)
Near x0 the function g(x) is approximately g‚Ä≤(x0)(x ‚àíx0), and so the integral is
J = f (x0)

Œ¥(g‚Ä≤(x0)(x ‚àíx0)) dx.
(3.43)
Since the delta function is nonnegative, we can write
J =
f (x0)
|g‚Ä≤(x0)|

Œ¥(g‚Ä≤(x0)(x ‚àíx0))|g‚Ä≤(x0)| dx
=
f (x0)
|g‚Ä≤(x0)|

Œ¥(g ‚àíg0) dg =
f (x0)
|g‚Ä≤(x0)|.
(3.44)
Thus for a function g(x) that has a single zero, we have

Œ¥(g(x)) f (x) dx =
f (x0)
|g‚Ä≤(x0)|.
(3.45)
If g(x) has several zeros x0k, then we must sum over them

Œ¥(g(x)) f (x) dx =

k
f (x0k)
|g‚Ä≤(x0k)|.
(3.46)
Replacing the dummy variable n by ‚àík in our Dirac-comb formula (2.116),
we Ô¨Ånd
‚àû

k=‚àí‚àû
e‚àíikx
2œÄ
=
‚àû

‚Ñì=‚àí‚àû
Œ¥(x ‚àí2œÄ‚Ñì).
(3.47)
Multiplying both sides of this comb by a function f (x) and integrating over the
real line, we have
‚àû

k=‚àí‚àû
 ‚àû
‚àí‚àû
e‚àíikx
2œÄ
f (x) dx =
‚àû

‚Ñì=‚àí‚àû
 ‚àû
‚àí‚àû
Œ¥(x ‚àí2œÄ‚Ñì) f (x) dx.
(3.48)
Our delta function formula (2.107) or (3.34) and our Fourier-transform rela-
tions (3.9) now give us the Poisson summation formula
1
‚àö
2œÄ
‚àû

k=‚àí‚àû
Àúf (k) =
‚àû

‚Ñì=‚àí‚àû
f (2œÄ‚Ñì),
(3.49)
114

3.4 FOURIER DERIVATIVES AND INTEGRALS
in which k and ‚Ñìare summed over all the integers. The stretched version of the
Poisson summation formula is
‚àö
2œÄ
L
‚àû

k=‚àí‚àû
Àúf (2œÄk/L) =
‚àû

‚Ñì=‚àí‚àû
f (‚ÑìL).
(3.50)
Both sides of these formulas make sense for continuous functions that are
square integrable on the real line.
Example 3.4 (Poisson and Gauss)
In example 3.1, we saw that the gaussian
f (x) = exp(‚àím2x2) has ÀÜf (k) = exp(‚àík2/4m2)/
‚àö
2 m as its Fourier transform.
So in this case, the Poisson summation formula (3.49) gives
1
2‚àöœÄ m
‚àû

k=‚àí‚àû
e‚àík2/4m2 =
‚àû

‚Ñì=‚àí‚àû
e‚àí(2œÄ‚Ñìm)2.
(3.51)
For m ‚â´1, the left-hand sum converges slowly, while the right-hand sum con-
verges quickly. For m ‚â™1, the right-hand sum converges slowly, while the
left-hand sum converges quickly.
A sum that converges slowly in space often converges quickly in momen-
tum space. Ewald summation is a technique for summing electrostatic energies,
which fall off only with a power of the distance, by summing their Fourier
transforms (Darden et al., 1993).
3.4 Fourier derivatives and integrals
By differentiating the inverse Fourier-transform relation (3.7)
f (x) =
 ‚àû
‚àí‚àû
dk
‚àö
2œÄ
Àúf (k) eikx
(3.52)
we see that the Fourier transform of the derivative f ‚Ä≤(x) is ik ÀÜf (k)
f ‚Ä≤(x) =
 ‚àû
‚àí‚àû
dk
‚àö
2œÄ
ik Àúf (k) eikx.
(3.53)
Differentiation with respect to x corresponds to multiplication by ik.
We may repeat the process and express the second derivative as
f
‚Ä≤‚Ä≤(x) =
 ‚àû
‚àí‚àû
dk
‚àö
2œÄ
(‚àík2) Àúf (k) eikx
(3.54)
and the nth derivative as
f (n)(x) =
 ‚àû
‚àí‚àû
dk
‚àö
2œÄ
(ik)n Àúf (k) eikx.
(3.55)
115

FOURIER AND LAPLACE TRANSFORMS
The indeÔ¨Ånite integral of the inverse Fourier transform (3.52) is
‚Äµf (x) ‚â°
 x
dx1 f (x1) =
 ‚àû
‚àí‚àû
dk
‚àö
2œÄ
Àúf (k) eikx
ik
(3.56)
and the nth indeÔ¨Ånite integral is
(n)f (x) ‚â°
 x
dx1 . . .
 xn‚àí1
dxn f (xn) =
 ‚àû
‚àí‚àû
dk
‚àö
2œÄ
Àúf (k) eikx
(ik)n .
(3.57)
Whether these derivatives and integrals converge better or worse than f (x)
depends upon the behavior of ÀÜf (k) near k = 0 and as |k| ‚Üí‚àû.
Example 3.5 (Momentum and momentum space)
Let‚Äôs write the inverse
Fourier transform (3.7) with œà instead of f and with the wave number k replaced
by k = p/¬Øh
œà(x) =
 ‚àû
‚àí‚àû
Àúœà(k) eikx
dk
‚àö
2œÄ
=
 ‚àû
‚àí‚àû
Àúœà(p/¬Øh)
‚àö
¬Øh
eipx/¬Øh
dp
‚àö2œÄ ¬Øh
.
(3.58)
For a normalized wave function œà(x), Parseval‚Äôs relation (3.40) implies
1 =
 ‚àû
‚àí‚àû
|œà(x)|2 dx =
 ‚àû
‚àí‚àû
| Àúœà(k)|2 dk =
 ‚àû
‚àí‚àû
|
Àúœà(p/¬Øh)
‚àö
¬Øh
|2 dp,
(3.59)
or with œà(x) = ‚ü®x|œà‚ü©and œï(p) = ‚ü®p|œà‚ü©= Àúœà(p/¬Øh)/‚àö
¬Øh
1 = ‚ü®œà|œà‚ü©=
 ‚àû
‚àí‚àû
|œà(x)|2 dx =
 ‚àû
‚àí‚àû
‚ü®œà|x‚ü©‚ü®x|œà‚ü©dx
=
 ‚àû
‚àí‚àû
‚ü®œà|p‚ü©‚ü®p|œà‚ü©dp =
 ‚àû
‚àí‚àû
|œï(p)|2 dp.
(3.60)
The inner product of any two states |œà‚ü©and |œÜ‚ü©is
‚ü®œà|œÜ‚ü©=
 ‚àû
‚àí‚àû
œà‚àó(x)œÜ(x) dx =
 ‚àû
‚àí‚àû
‚ü®œà|x‚ü©‚ü®x|œÜ‚ü©dx
=
 ‚àû
‚àí‚àû
œà‚àó(p)œÜ(p) dp =
 ‚àû
‚àí‚àû
‚ü®œà|p‚ü©‚ü®p|œÜ‚ü©dp
(3.61)
so the outer products |x‚ü©‚ü®x| and |p‚ü©‚ü®p| can represent the identity operator
I =
 ‚àû
‚àí‚àû
dx |x‚ü©‚ü®x| =
 ‚àû
‚àí‚àû
dp |p‚ü©‚ü®p|.
(3.62)
The Fourier transform (3.58) relating the wave function in momentum space
to that in position space is
œà(x) =
 ‚àû
‚àí‚àû
eipx/¬Øh œï(p)
dp
‚àö2œÄ ¬Øh
(3.63)
116

3.4 FOURIER DERIVATIVES AND INTEGRALS
and the inverse Fourier transform is
œï(p) =
 ‚àû
‚àí‚àû
e‚àíipx/¬Øh œà(x)
dx
‚àö2œÄ ¬Øh
.
(3.64)
In Dirac notation, the Ô¨Årst equation (3.63) of this pair is
œà(x) = ‚ü®x|œà‚ü©=
 ‚àû
‚àí‚àû
‚ü®x|p‚ü©‚ü®p|œà‚ü©dp =
 ‚àû
‚àí‚àû
eipx/¬Øh
‚àö2œÄ ¬Øh
œï(p) dp
(3.65)
so we identify ‚ü®x|p‚ü©with
‚ü®x|p‚ü©= eipx/¬Øh
‚àö2œÄ ¬Øh
,
(3.66)
which in turn is consistent with the delta function relation (3.36)
Œ¥(x ‚àíy) = ‚ü®x|y‚ü©=
 ‚àû
‚àí‚àû
‚ü®x|p‚ü©‚ü®p|y‚ü©dp =
 ‚àû
‚àí‚àû
eipx/¬Øh
‚àö2œÄ ¬Øh
e‚àíipy/¬Øh
‚àö2œÄ ¬Øh
dp
=
 ‚àû
‚àí‚àû
eip(x‚àíy)/¬Øh
2œÄ ¬Øh
dp =
 ‚àû
‚àí‚àû
eik(x‚àíy) dk
2œÄ .
(3.67)
If we differentiate œà(x) as given by (3.65), then we Ô¨Ånd as in (3.53)
¬Øh
i
d
dxœà(x) =
 ‚àû
‚àí‚àû
p œï(p) eipx/¬Øh
dp
‚àö2œÄ ¬Øh
(3.68)
or
¬Øh
i
d
dxœà(x) = ‚ü®x|p|œà‚ü©=
 ‚àû
‚àí‚àû
‚ü®x|p|p‚Ä≤‚ü©‚ü®p‚Ä≤|œà‚ü©dp‚Ä≤ =
 ‚àû
‚àí‚àû
p‚Ä≤ œï(p‚Ä≤) eip‚Ä≤x/¬Øh
dp‚Ä≤
‚àö2œÄ ¬Øh
in Dirac notation.
Example 3.6 (The uncertainty principle)
Let‚Äôs Ô¨Årst normalize the gaussian
œà(x) = N exp(‚àí(x/a)2) to unity over the real axis
1 = N2
 ‚àû
‚àí‚àû
e‚àí2(x/a)2 dx =
&œÄ
2 a N2,
(3.69)
which gives N2 = ‚àö2/œÄ/a. So the normalized wave-function is
œà(x) ‚â°‚ü®x|œà‚ü©=
 2
œÄ
1/4
1
‚àöa e‚àí(x/a)2.
(3.70)
The mean value ‚ü®A‚ü©of an operator A in a state |œà‚ü©is
‚ü®A‚ü©‚â°‚ü®œà|A|œà‚ü©.
(3.71)
More generally, the mean value of an operator A for a system described by a
density operator œÅ is the trace
‚ü®A‚ü©‚â°Tr (œÅA) .
(3.72)
117

FOURIER AND LAPLACE TRANSFORMS
Since the gaussian (3.70) is an even function of x (that is, œà(‚àíx) = œà(x)), the
mean value of the position operator x in the state (3.70) vanishes
‚ü®x‚ü©= ‚ü®œà|x|œà‚ü©=
 ‚àû
‚àí‚àû
x |œà(x)|2 dx = 0.
(3.73)
The variance of an operator A with mean value ‚ü®A‚ü©in a state |œà‚ü©is the mean
value of the square of the difference A ‚àí‚ü®A‚ü©
(A)2 ‚â°‚ü®œà| (A ‚àí‚ü®A‚ü©)2 |œà‚ü©.
(3.74)
For a system with density operator œÅ, the variance of A is
(A)2 ‚â°Tr

œÅ (A ‚àí‚ü®A‚ü©)2
.
(3.75)
Since ‚ü®x‚ü©= 0, the variance of the position operator x is
(x)2 = ‚ü®œà|(x ‚àí‚ü®x‚ü©)2|œà‚ü©= ‚ü®œà|x2|œà‚ü©
=
 ‚àû
‚àí‚àû
x2 |œà(x)|2 dx = a2
4 .
(3.76)
We can use the Fourier transform to Ô¨Ånd the variance of the momentum
operator. By (3.64), the wave-function œï(p) in momentum space is
œï(p) = ‚ü®p|œà‚ü©=
 ‚àû
‚àí‚àû
‚ü®p|x‚ü©‚ü®x|œà‚ü©dx.
(3.77)
By (3.66), the inner product ‚ü®p|x‚ü©= ‚ü®x|p‚ü©‚àóis ‚ü®p|x‚ü©= e‚àíipx/¬Øh/‚àö2œÄ ¬Øh, so
œï(p) = ‚ü®p|œà‚ü©=
 ‚àû
‚àí‚àû
dx
‚àö2œÄ ¬Øh
e‚àíipx/¬Øh‚ü®x|œà‚ü©.
(3.78)
Thus by (3.69 & 3.70), œï(p) is the Fourier transform
œï(p) =
 ‚àû
‚àí‚àû
dx
‚àö2œÄ ¬Øh
e‚àíipx/¬Øh
 2
œÄ
1/4
1
‚àöa e‚àí(x/a)2.
(3.79)
Using our formula (3.18) for the Fourier transform of a gaussian, we get
œï(p) =
& a
2¬Øh
 2
œÄ
1/4
e‚àí(ap)2/(2¬Øh)2.
(3.80)
Since the gaussian œï(p) is an even function of p, the mean value ‚ü®p‚ü©of the
momentum operator vanishes, like that of the position operator. So the variance
of the momentum operator is
(p)2 = ‚ü®œà|(p ‚àí‚ü®p‚ü©)2|œà‚ü©= ‚ü®œà|p2 |œà‚ü©=
 ‚àû
‚àí‚àû
p2 |œï(p)|2 dp
=
&
2
œÄ
 ‚àû
‚àí‚àû
p2 a
2¬Øh e‚àí(ap)2/2¬Øh2 dp = ¬Øh2
a2 .
(3.81)
118

3.5 FOURIER TRANSFORMS IN SEVERAL DIMENSIONS
Thus in this case, the product of the two variances is
(x)2 (p)2 = a2
4
¬Øh2
a2 = ¬Øh2
4 .
(3.82)
This is an example of Heisenberg‚Äôs uncertainty principle
x p ‚â•¬Øh
2,
(3.83)
which follows from the Fourier-transform relations between the conjugate
variables x and p.
The state |œà‚ü©of a free particle at time t = 0
|œà, 0‚ü©=
 ‚àû
‚àí‚àû
|p‚ü©‚ü®p|œà‚ü©dp =
 ‚àû
‚àí‚àû
|p‚ü©œï(p) dp
(3.84)
evolves under the inÔ¨Çuence of the hamiltonian H = p2/(2m) to the state
e‚àíiHt/¬Øh|œà, 0‚ü©=
 ‚àû
‚àí‚àû
e‚àíiHt/¬Øh|p‚ü©œï(p) dp =
 ‚àû
‚àí‚àû
e‚àíip2t/(2¬Øhm)|p‚ü©œï(p) dp
(3.85)
at time t.
Example 3.7 (The characteristic function)
If P(x) is a probability distribution
normalized to unity over the range of x

P(x) dx = 1
(3.86)
then its Fourier transform is the characteristic function
œá(k) = ÀÜP(k) =

eikxP(x) dx.
(3.87)
The expected value of a function f (x) is the integral
E[f (x)] =

f (x) P(x) dx.
(3.88)
So the characteristic function œá(k) = E[exp(ikx)] is the expected value of the
exponential exp(ikx), and its derivatives at k = 0 are the moments E[xn] ‚â°Œºn of
the probability distribution
E[xn] =

xn P(x) dx = (‚àíi)n dnœá(k)
dkn

k=0
.
(3.89)
We shall pick up this thread in section 13.12.
3.5 Fourier transforms in several dimensions
If f (x1, x2) is a function of two variables, then its double Fourier transform
Àúf (k1, k2) is
119

FOURIER AND LAPLACE TRANSFORMS
Àúf (k1, k2) =
 ‚àû
‚àí‚àû
dx1
‚àö
2œÄ
 ‚àû
‚àí‚àû
dx2
‚àö
2œÄ
e‚àíik1x1‚àíik2x2 f (x1, x2).
(3.90)
By twice using the Fourier representation (3.36) of Dirac‚Äôs delta function, we
may invert this double Fourier transformation
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àû
dk1dk2
2œÄ
ei(k1x1+k2x2) Àúf (k1, k2)
=
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àû
dk1dk2
2œÄ
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àû
dx‚Ä≤
1dx‚Ä≤
2
2œÄ
eik1(x1‚àíx‚Ä≤
1)+ik2(x2‚àíx‚Ä≤
2) f (x‚Ä≤
1, x‚Ä≤
2)
=
 ‚àû
‚àí‚àû
dk2
2œÄ
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àû
dx‚Ä≤
1dx‚Ä≤
2 eik2(x2‚àíx‚Ä≤
2) Œ¥(x1 ‚àíx‚Ä≤
1) f (x‚Ä≤
1, x‚Ä≤
2)
=
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àû
dx‚Ä≤
1dx‚Ä≤
2 Œ¥(x1 ‚àíx‚Ä≤
1)Œ¥(x2 ‚àíx‚Ä≤
2) f (x‚Ä≤
1, x‚Ä≤
2) = f (x1, x2).
(3.91)
That is
f (x1, x2) =
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àû
dk1dk2
2œÄ
ei(k1x1+k2x2) Àúf (k1, k2).
(3.92)
The Fourier transform of a function f (x1, . . . , xn) of n variables is
Àúf (k1, . . . , kn) =
 ‚àû
‚àí‚àû
. . .
 ‚àû
‚àí‚àû
dx1 . . . dxn
(2œÄ)n/2
e‚àíi(k1x1+¬∑¬∑¬∑+knxn) f (x1, . . . , xn)
(3.93)
and its inverse is
f (x1, . . . , xn) =
 ‚àû
‚àí‚àû
. . .
 ‚àû
‚àí‚àû
dk1 . . . dkn
(2œÄ)n/2
ei(k1x1+¬∑¬∑¬∑+knxn) Àúf (k1, . . . , kn),
(3.94)
in which all the integrals run from ‚àí‚àûto ‚àû.
If we generalize the relations (3.12‚Äì3.14) between Fourier series and trans-
forms from one to n dimensions, then we Ô¨Ånd that the Fourier series corre-
sponding to the Fourier transform (3.94) is
f (x1, . . . , xn) =
2œÄ
L
n
‚àû

j1=‚àí‚àû
. . .
‚àû

jn=‚àí‚àû
ei(kj1x1+¬∑¬∑¬∑+kjnxn) Àúf (kj1, . . . , kjn)
(2œÄ)n/2
(3.95)
in which kj‚Ñì= 2œÄj‚Ñì/L. Thus, for n = 3 we have
f (x) = (2œÄ)3
V
‚àû

j1=‚àí‚àû
‚àû

j2=‚àí‚àû
‚àû

j3=‚àí‚àû
eikj¬∑x
Àúf (kj)
(2œÄ)3/2 ,
(3.96)
in which kj = (kj1, kj2, kj3) and V = L3 is the volume of the box.
120

3.6 CONVOLUTIONS
Example 3.8 (The Feynman propagator)
For a spinless quantum Ô¨Åeld of mass
m, Feynman‚Äôs propagator is the four-dimensional Fourier transform
‚ñ≥F (x) =

exp(ik ¬∑ x)
k2 + m2 ‚àíiœµ
d4k
(2œÄ)4
(3.97)
where k ¬∑ x = k ¬∑ x ‚àík0x0, all physical quantities are in natural units (c = ¬Øh = 1),
and x0 = ct = t. The tiny imaginary term ‚àíiœµ makes ‚ñ≥F(x ‚àíy) proportional to
the mean value in the vacuum state |0‚ü©of the time-ordered product of the Ô¨Åelds
œÜ(x) and œÜ(y) (section 5.34)
‚àíi ‚ñ≥F (x ‚àíy) = ‚ü®0|T [œÜ(x)œÜ(y)] |0‚ü©
(3.98)
‚â°Œ∏(x0 ‚àíy0)‚ü®0|œÜ(x)œÜ(y)|0‚ü©+ Œ∏(y0 ‚àíx0)‚ü®0|œÜ(y)œÜ(x)|0‚ü©
in which Œ∏(a) = (a + |a|)/2|a| is the Heaviside function (2.166).
3.6 Convolutions
The convolution of f (x) with g(x) is the integral
f ‚àóg(x) =
 ‚àû
‚àí‚àû
dy
‚àö
2œÄ
f (x ‚àíy) g(y).
(3.99)
The convolution product is symmetric
f ‚àóg(x) = g ‚àóf (x)
(3.100)
because, setting z = x ‚àíy, we have
f ‚àóg(x) =
 ‚àû
‚àí‚àû
dy
‚àö
2œÄ
f (x ‚àíy) g(y) = ‚àí
 ‚àí‚àû
‚àû
dz
‚àö
2œÄ
f (z) g(x ‚àíz)
=
 ‚àû
‚àí‚àû
dz
‚àö
2œÄ
g(x ‚àíz) f (z) = g ‚àóf (x).
(3.101)
Convolutions may look strange at Ô¨Årst, but they often occur in physics in the
three-dimensional form
F(x) =

G(x ‚àíx‚Ä≤) S(x‚Ä≤) d3x,
(3.102)
in which G is a Green‚Äôs function and S is a source.
Example 3.9 (Gauss‚Äôs law)
The divergence of the electric Ô¨Åeld E is the micro-
scopic charge density œÅ divided by the electric permittivity of the vacuum
œµ0 = 8.854 √ó 10‚àí12 F/m, that is, ‚àá¬∑ E = œÅ/œµ0. This constraint is known as
Gauss‚Äôs law. If the charges and Ô¨Åelds are independent of time, then the electric
121

FOURIER AND LAPLACE TRANSFORMS
Ô¨Åeld E is the gradient of a scalar potential E = ‚àí‚àáœÜ. These last two equations
imply that œÜ obeys Poisson‚Äôs equation
‚àí‚àá2œÜ = œÅ
œµ0
.
(3.103)
We may solve this equation by using Fourier transforms as described in
section 3.13. If ÀúœÜ(k) and ÀúœÅ(k) respectively are the Fourier transforms of œÜ(x)
and œÅ(x), then Poisson‚Äôs differential equation (3.103) gives
‚àí‚àá2œÜ(x) = ‚àí‚àá2

eik¬∑x ÀúœÜ(k) d3k =

k2 eik¬∑x ÀúœÜ(k) d3k
= œÅ(x)
œµ0
=

eik¬∑x ÀúœÅ(k)
œµ0
d3k,
(3.104)
which implies the algebraic equation ÀúœÜ(k) = ÀúœÅ(k)/œµ0k2, which is an instance of
(3.166). Performing the inverse Fourier transformation, we Ô¨Ånd for the scalar
potential
œÜ(x) =

eik¬∑x ÀúœÜ(k) d3k =

eik¬∑x ÀúœÅ(k)
œµ0 k2 d3k
(3.105)
=

eik¬∑x 1
k2

e‚àíik¬∑x‚Ä≤ œÅ(x‚Ä≤)
œµ0
d3x‚Ä≤d3k
(2œÄ)3
=

G(x ‚àíx‚Ä≤) œÅ(x‚Ä≤)
œµ0
d3x‚Ä≤,
in which
G(x ‚àíx‚Ä≤) =

d3k
(2œÄ)3
1
k2 eik¬∑(x‚àíx‚Ä≤).
(3.106)
This function G(x ‚àíx‚Ä≤) is the Green‚Äôs function for the differential operator ‚àí‚àá2
in the sense that
‚àí‚àá2G(x ‚àíx‚Ä≤) =

d3k
(2œÄ)3 eik¬∑(x‚àíx‚Ä≤) = Œ¥(3)(x ‚àíx‚Ä≤).
(3.107)
This Green‚Äôs function ensures that expression (3.105) for œÜ(x) satisÔ¨Åes Poisson‚Äôs
equation (3.103). To integrate (3.106) and compute G(x ‚àíx‚Ä≤), we use spherical
coordinates with the z-axis parallel to the vector x ‚àíx‚Ä≤
G(x ‚àíx‚Ä≤) =

d3k
(2œÄ)3
1
k2 eik¬∑(x‚àíx‚Ä≤) =
 ‚àû
0
dk
(2œÄ)2
 1
‚àí1
d cos Œ∏ eik|x‚àíx‚Ä≤| cos Œ∏
=
 ‚àû
0
dk
(2œÄ)2
eik|x‚àíx‚Ä≤| ‚àíe‚àíik|x‚àíx‚Ä≤|
ik|x ‚àíx‚Ä≤|
(3.108)
=
1
2œÄ2|x ‚àíx‚Ä≤|
 ‚àû
0
sin k|x ‚àíx‚Ä≤| dk
k
=
1
2œÄ2|x ‚àíx‚Ä≤|
 ‚àû
0
sin k dk
k
.
In example 5.35 of section 5.18 on Cauchy‚Äôs principal value, we‚Äôll show that
 ‚àû
0
sin k
k
dk = œÄ
2 .
(3.109)
122

3.7 THE FOURIER TRANSFORM OF A CONVOLUTION
Using this result, we have

d3k
(2œÄ)3
1
k2 eik¬∑(x‚àíx‚Ä≤) = G(x ‚àíx‚Ä≤) =
1
4œÄ|x ‚àíx‚Ä≤|.
(3.110)
Finally, by substituting this formula for G(x ‚àíx‚Ä≤) into Equation (3.105), we Ô¨Ånd
that the Fourier transform œÜ(x) of the product ÀÜœÅ(k)/k2 of the functions ÀÜœÅ(k) and
1/k2 is the convolution
œÜ(x) =
1
4œÄœµ0

œÅ(x‚Ä≤)
|x ‚àíx‚Ä≤| d3x
(3.111)
of their Fourier transforms 1/|x ‚àíx‚Ä≤| and œÅ(x‚Ä≤). The Fourier transform of the
product of any two functions is the convolution of their Fourier transforms, as
we‚Äôll see in the next section (George Green, 1793‚Äì1841).
Example 3.10 (The magnetic vector potential)
The magnetic induction B
has zero divergence (as long as there are no magnetic monopoles) and so
may be written as the curl B = ‚àá√ó A of a vector potential A. For time-
independent currents, Amp√®re‚Äôs law is ‚àá√ó B = Œº0J, in which Œº0 = 1/(œµ0c2) =
4œÄ √ó 10‚àí7 N A‚àí2 is the permeability of the vacuum. It follows that in the
Coulomb gauge ‚àá¬∑ A = 0, the magnetostatic vector potential A satisÔ¨Åes the
equation
‚àá√ó B = ‚àá√ó (‚àá√ó A) = ‚àá(‚àá¬∑ A) ‚àí‚àá2A = ‚àí‚àá2A = Œº0J.
(3.112)
Applying the Fourier-transform technique (3.103‚Äì3.111), we Ô¨Ånd that the
Fourier transforms of A and J satisfy the algebraic equation
ÀÜA(k) = Œº0
ÀÜJ(k)
k2 ,
(3.113)
which is an instance of (3.166). Performing the inverse Fourier transform, we see
that A is the convolution
A(x) = Œº0
4œÄ

d3x‚Ä≤
J(x‚Ä≤)
|x ‚àíx‚Ä≤|.
(3.114)
If in the solution (3.111) of Poisson‚Äôs equation, œÅ(x) is translated by a, then so
is œÜ(x). That is, if œÅ‚Ä≤(x) = œÅ(x + a) then œÜ‚Ä≤(x) = œÜ(x + a). Similarly, if the cur-
rent J(x) in (3.114) is translated by a, then so is the potential A(x). Convolutions
respect translational invariance. That‚Äôs one reason why they occur so often in the
formulas of physics.
3.7 The Fourier transform of a convolution
The Fourier transform of the convolution f ‚àóg is the product of the Fourier
transforms Àúf and Àúg:
'
f ‚àóg(k) = Àúf (k) Àúg(k).
(3.115)
123

FOURIER AND LAPLACE TRANSFORMS
To see why, we form the Fourier transform '
f ‚àóg(k) of the convolution f ‚àóg(x)
'
f ‚àóg(k) =
 ‚àû
‚àí‚àû
dx
‚àö
2œÄ
e‚àíikx f ‚àóg(x)
=
 ‚àû
‚àí‚àû
dx
‚àö
2œÄ
e‚àíikx
 ‚àû
‚àí‚àû
dy
‚àö
2œÄ
f (x ‚àíy) g(y).
(3.116)
Now we write f (x‚àíy) and g(y) in terms of their Fourier transforms Àúf (p) and Àúg(q)
'
f ‚àóg(k) =
 ‚àû
‚àí‚àû
dx
‚àö
2œÄ
e‚àíikx
 ‚àû
‚àí‚àû
dy
‚àö
2œÄ
 ‚àû
‚àí‚àû
dp
‚àö
2œÄ
Àúf (p) eip(x‚àíy)
 ‚àû
‚àí‚àû
dq
‚àö
2œÄ
Àúg(q) eiqy
(3.117)
and use the representation (3.36) of Dirac‚Äôs delta function twice to get
'
f ‚àóg(k) =
 ‚àû
‚àí‚àû
dy
2œÄ
 ‚àû
‚àí‚àû
dp
 ‚àû
‚àí‚àû
dq Œ¥(p ‚àík) Àúf (p) Àúg(q) ei(q‚àíp)y
=
 ‚àû
‚àí‚àû
dp
 ‚àû
‚àí‚àû
dq Œ¥(p ‚àík) Œ¥(q ‚àíp) Àúf (p) Àúg(q)
=
 ‚àû
‚àí‚àû
dp Œ¥(p ‚àík) Àúf (p) Àúg(p) = Àúf (k) Àúg(k),
(3.118)
which is (3.115). Examples 3.9 and 3.10 were illustrations of this result.
3.8 Fourier transforms and Green‚Äôs functions
A Green‚Äôs function G(x) for a differential operator P turns into a delta function
when acted upon by P, that is, PG(x) = Œ¥(x). If the differential operator is
a polynomial P(‚àÇ) ‚â°P(‚àÇ1, . . . , ‚àÇn) in the derivatives ‚àÇ1, . . . , ‚àÇn with constant
coefÔ¨Åcients, then a suitable Green‚Äôs function G(x) ‚â°G(x1, . . . , xn) will satisfy
P(‚àÇ)G(x) = Œ¥(n)(x).
(3.119)
Expressing both G(x) and Œ¥(n)(x) as Fourier transforms, we get
P(‚àÇ)G(x) =

dnk P(ik) eik¬∑x ÀúG(k) = Œ¥(n)(x) =

dnk
(2œÄ)n eik¬∑x,
(3.120)
which gives us the algebraic equation
ÀúG(k) =
1
(2œÄ)n P(ik).
(3.121)
Thus the Green‚Äôs function GP for the differential operator P(‚àÇ) is
GP(x) =

dnk
(2œÄ)n
eik¬∑x
P(ik).
(3.122)
124

3.9 LAPLACE TRANSFORMS
Example 3.11 (Green and Yukawa)
In 1935, Hideki Yukawa (1907‚Äì1981)
proposed the partial differential equation
PY(‚àÇ)GY(x) ‚â°(‚àí‚ñ≥+ m2)GY(x) = (‚àí‚àá2 + m2)GY(x) = 0.
(3.123)
Our (3.122) gives as the Green‚Äôs function for PY(‚àÇ) the Yukawa potential
GY(x) =

d3k
(2œÄ)3
eik¬∑x
PY(ik) =

d3k
(2œÄ)3
eik¬∑x
k2 + m2 = e‚àímr
4œÄr ,
(3.124)
an integration done in example 5.21.
3.9 Laplace transforms
The Laplace transform f (s) of a function F(t) is the integral
f (s) =
 ‚àû
0
dt e‚àíst F(t).
(3.125)
Because the integration is over positive values of t, the exponential exp(‚àíst)
falls off rapidly with the real part of s. As Re s increases, the Laplace transform
f (s) becomes smoother and smaller. For Re s > 0, the exponential exp(‚àíst) lets
many functions F(t) that are not integrable over the half line [0, ‚àû) have well-
behaved Laplace transforms.
For instance, the function F(t) = 1 is not integrable over the half line [0, ‚àû),
but its Laplace transform
f (s) =
 ‚àû
0
dt e‚àíst F(t) =
 ‚àû
0
dt e‚àíst = 1
s
(3.126)
is well deÔ¨Åned for Re s > 0 and square integrable for Re s > œµ.
The function F(t) = exp(kt) diverges exponentially for Re k > 0, but its
Laplace transform
f (s) =
 ‚àû
0
dt e‚àíst F(t) =
 ‚àû
0
dt e‚àí(s‚àík)t =
1
s ‚àík
(3.127)
is well deÔ¨Åned for Re s > k with a simple pole at s = k (section 5.10) and is
square integrable for Re s > k + œµ.
The Laplace transforms of cosh kt and sinh kt are
f (s) =
 ‚àû
0
dt e‚àíst cosh kt = 1
2
 ‚àû
0
dt e‚àíst 
ekt + e‚àíkt
=
s
s2 ‚àík2
(3.128)
and
f (s) =
 ‚àû
0
dt e‚àíst sinh kt = 1
2
 ‚àû
0
dt e‚àíst 
ekt ‚àíe‚àíkt
=
k
s2 ‚àík2 .
(3.129)
125

FOURIER AND LAPLACE TRANSFORMS
The Laplace transform of cos œât is
f (s) =
 ‚àû
0
dt e‚àíst cos œât = 1
2
 ‚àû
0
dt e‚àíst 
eiœât + e‚àíiœât
=
s
s2 + œâ2
(3.130)
and that of sin œât is
f (s) =
 ‚àû
0
dt e‚àíst sin œât = 1
2i
 ‚àû
0
dt e‚àíst 
eiœât ‚àíe‚àíiœât
=
œâ
s2 + œâ2 .
(3.131)
Example 3.12 (Lifetime of a Ô¨Çuorophore)
Fluorophores are molecules that
emit visible light when excited by photons. The probability P(t, t‚Ä≤) that a Ô¨Çuo-
rophore with a lifetime œÑ will emit a photon at time t if excited by a photon at
time t‚Ä≤ is
P(t, t‚Ä≤) = œÑ e‚àí(t‚àít‚Ä≤)/œÑ Œ∏(t ‚àít‚Ä≤)
(3.132)
in which Œ∏(t ‚àít‚Ä≤) = (t ‚àít‚Ä≤ + |t ‚àít‚Ä≤|)/2|t ‚àít‚Ä≤| is the Heaviside function. One way
to measure the lifetime œÑ of a Ô¨Çuorophore is to modulate the exciting laser beam
at a frequency ŒΩ = 2œÄœâ of the order of 60 MHz and to detect the phase-shift œÜ
in the light L(t) emitted by the Ô¨Çuorophore. That light is the integral of P(t, t‚Ä≤)
times the modulated beam sin œât or equivalently the convolution of e‚àít/œÑŒ∏(t)
with sin œât
L(t) =
 ‚àû
‚àí‚àû
P(t, t‚Ä≤) sin(œât‚Ä≤) dt‚Ä≤ =
 ‚àû
‚àí‚àû
œÑ e‚àí(t‚àít‚Ä≤)/œÑŒ∏(t ‚àít‚Ä≤) sin(œât‚Ä≤) dt‚Ä≤
=
 t
‚àí‚àû
œÑ e‚àí(t‚àít‚Ä≤)/œÑ sin(œât‚Ä≤) dt‚Ä≤.
(3.133)
Letting u = t ‚àít‚Ä≤ and using the trigonometric formula
sin(a ‚àíb) = sin a cos b ‚àícos a sin b
(3.134)
we may relate this integral to the Laplace transforms of a sine (3.131) and a
cosine (3.130)
L(t) = ‚àíœÑ
 ‚àû
0
e‚àíu/œÑ sin œâ(u ‚àít) du
= ‚àíœÑ
 ‚àû
0
e‚àíu/œÑ (sin œâu cos œât ‚àícos œâu sin œât) du
= œÑ
 sin(œât)/œÑ
1/œÑ 2 + œâ2 ‚àíœâ cos œât
1/œÑ 2 + œâ2

.
(3.135)
Setting cos œÜ = (1/œÑ)/

1/œÑ 2 + œâ2 and sin œÜ = œâ/

1/œÑ 2 + œâ2, we have
L(t) =
œÑ

1/œÑ 2 + œâ2 (sin œât cos œÜ ‚àícos œât sin œÜ) =
œÑ

1/œÑ 2 + œâ2 sin(œât ‚àíœÜ).
(3.136)
126

3.10 DERIVATIVES AND INTEGRALS OF LAPLACE TRANSFORMS
The phase-shift œÜ then is given by
œÜ = arcsin
œâ

1/œÑ 2 + œâ2 ‚â§œÄ
2 .
(3.137)
So by inverting this formula, we get the lifetime of the Ô¨Çuorophore
œÑ = (1/œâ) tan œÜ
(3.138)
in terms of the phase-shift œÜ, which is much easier to measure.
3.10 Derivatives and integrals of Laplace transforms
The derivatives of a Laplace transform f (s) are by its deÔ¨Ånition (3.125)
dnf (s)
dsn
=
 ‚àû
0
dt (‚àít)n e‚àíst F(t).
(3.139)
They usually are well deÔ¨Åned if f (s) is well deÔ¨Åned. For instance, if we differ-
entiate the Laplace transform f (s) = 1/s of the function F(t) = 1 as given by
(3.126), then we Ô¨Ånd
(‚àí1)n dns‚àí1
dsn
=
n!
sn+1 =
 ‚àû
0
dt e‚àíst tn,
(3.140)
which tells us that the Laplace transform of tn is n!/sn+1.
The result of differentiating the function F(t) also has a simple form.
Integrating by parts, we Ô¨Ånd for the Laplace transform of F‚Ä≤(t)
 ‚àû
0
dt e‚àíst F‚Ä≤(t) =
 ‚àû
0
dt
 d
dt

e‚àíst F(t)

‚àíF(t) d
dt e‚àíst
%
= ‚àíF(0) +
 ‚àû
0
dt F(t) s e‚àíst
= ‚àíF(0) + s f (s).
(3.141)
The indeÔ¨Ånite integral of the Laplace transform (3.125) is
‚Äµf (s) ‚â°

ds1 f (s1) =
 ‚àû
0
dt e‚àíst
(‚àít) F(t)
(3.142)
and its nth indeÔ¨Ånite integral is
(n)f (s) ‚â°

dsn . . .

ds1 f (s1) =
 ‚àû
0
dt e‚àíst
(‚àít)n F(t).
(3.143)
If f (s) is a well-behaved function, then these indeÔ¨Ånite integrals usually are well
deÔ¨Åned, except possibly at s = 0.
127

FOURIER AND LAPLACE TRANSFORMS
3.11 Laplace transforms and differential equations
Suppose we wish to solve the differential equation
P(d/ds) f (s) = j(s).
(3.144)
By writing f (s) and j(s) as Laplace transforms
f (s) =
 ‚àû
0
e‚àíst F(t) dt
j(s) =
 ‚àû
0
e‚àíst J(t) dt
(3.145)
and using the formula (3.139) for the nth derivative of a Laplace transform, we
see that the differential equation (3.144) amounts to
 ‚àû
0
e‚àíst P(‚àít) F(t) dt =
 ‚àû
0
e‚àíst J(t) dt,
(3.146)
which is equivalent to the algebraic equation
F(t) =
J(t)
P(‚àít).
(3.147)
A particular solution to the inhomogeneous equation (3.144) is then the
Laplace transform of this ratio
f (s) =
 ‚àû
0
e‚àíst J(t)
P(‚àít) dt.
(3.148)
A fairly general solution of the associated homogeneous equation
P(d/ds) f (s) = 0
(3.149)
is the Laplace transform
f (s) =
 ‚àû
0
e‚àíst Œ¥(P(‚àít)) H(t) dt,
(3.150)
in which the function H(t) is arbitrary. Thus our solution of the inhomogeneous
equation (3.144) is the sum of the two
f (s) =
 ‚àû
0
e‚àíst J(t)
P(‚àít) dt +
 ‚àû
0
e‚àíst Œ¥(P(‚àít)) H(t) dt.
(3.151)
One may generalize this method to differential equations in n variables. But to
carry out this procedure, one must be able to Ô¨Ånd the inverse Laplace transform
J(t) of the source function j(s) as outlined in the next section.
128

3.13 APPLICATION TO DIFFERENTIAL EQUATIONS
3.12 Inversion of Laplace transforms
How do we invert the Laplace transform
f (s) =
 ‚àû
0
dt e‚àíst F(t)?
(3.152)
First we extend the Laplace transform from real s to s + iu
f (s + iu) =
 ‚àû
0
dt e‚àí(s+iu)t F(t).
(3.153)
Next we choose s to be sufÔ¨Åciently positive that the integral
 ‚àû
‚àí‚àû
du
2œÄ e(s+iu)t f (s + iu) =
 ‚àû
‚àí‚àû
du
2œÄ
 ‚àû
0
dt‚Ä≤ e(s+iu)t e‚àí(s+iu)t‚Ä≤ F(t‚Ä≤)
(3.154)
converges, and then we apply to it the delta function formula (3.36)
 ‚àû
‚àí‚àû
du
2œÄ e(s+iu)t f (s + iu) =
 ‚àû
0
dt‚Ä≤ es(t‚àít‚Ä≤) F(t‚Ä≤)
 ‚àû
‚àí‚àû
du
2œÄ eiu(t‚àít‚Ä≤)
=
 ‚àû
0
dt‚Ä≤ es(t‚àít‚Ä≤) F(t‚Ä≤) Œ¥(t ‚àít‚Ä≤) = F(t). (3.155)
So our inversion formula is
F(t) = est
 ‚àû
‚àí‚àû
du
2œÄ eiut f (s + iu)
(3.156)
for sufÔ¨Åciently large s. Some call this inversion formula a Bromwich integral,
others a Fourier‚ÄìMellin integral.
3.13 Application to differential equations
Let us consider a linear partial differential equation in n variables
P(‚àÇ1, . . . , ‚àÇn) f (x1, . . . , xn) = g(x1, . . . , xn),
(3.157)
in which P is a polynomial in the derivatives
‚àÇj ‚â°
‚àÇ
‚àÇxj
(3.158)
with constant coefÔ¨Åcients. If g = 0, the equation is homogeneous; otherwise it
is inhomogeneous. We expand solution and source as integral transforms
f (x1, . . . , xn) =

Àúf (k1, . . . , kn) ei(k1x1+¬∑¬∑¬∑+knxn)dnk,
g(x1, . . . , xn) =

Àúg(k1, . . . , kn) ei(k1x1+¬∑¬∑¬∑+knxn)dnk,
(3.159)
129

FOURIER AND LAPLACE TRANSFORMS
in which the k integrals may run from ‚àí‚àûto ‚àûas in a Fourier transform or
up the imaginary axis from 0 to ‚àûas in a Laplace transform.
The correspondence (3.55) between differentiation with respect to xj and
multiplication by ikj tells us that ‚àÇm
j acting on f gives
‚àÇm
j f (x1, . . . , xn) =

Àúf (k1, . . . , kn) (ikj)m ei(k1x1+¬∑¬∑¬∑+knxn) dnk.
(3.160)
If we abbreviate f (x1, . . . , xn) by f (x) and do the same for g, then we may write
our partial differential equation (3.157) as
P(‚àÇ1, . . . , ‚àÇn)f (x) =

Àúf (k) P(ik1, . . . , ikn) ei(k1x1+¬∑¬∑¬∑+knxn) dnk
=

Àúg(k) ei(k1x1+¬∑¬∑¬∑+knxn) dnk.
(3.161)
Thus the inhomogeneous partial differential equation
P(‚àÇ1, . . . , ‚àÇn) fi(x1, . . . , xn) = g(x1, . . . , xn)
(3.162)
becomes an algebraic equation in k-space
P(ik1, . . . , ikn) Àúfi(k1, . . . , kn) = Àúg(k1, . . . , kn)
(3.163)
where Àúg(k1, . . . , kn) is the mixed Fourier‚ÄìLaplace transform of g(x1, . . . , xn). So
one solution of the inhomogeneous differential equation (3.157) is
fi(x1, . . . , xn) =

ei(k1x1+¬∑¬∑¬∑+knxn) Àúg(k1, . . . , kn)
P(ik1, . . . , ikn) dnk.
(3.164)
The space of solutions to the homogeneous form of equation (3.157)
P(‚àÇ1, . . . , ‚àÇn) fh(x1, . . . , xn) = 0
(3.165)
is vast. We will focus on those that satisfy the algebraic equation
P(ik1, . . . , ikn) Àúfh(k1, . . . , kn) = 0
(3.166)
and that we can write in terms of Dirac‚Äôs delta function as
Àúfh(k1, . . . , kn) = Œ¥(P(ik1, . . . , ikn)) h(k1, . . . , kn),
(3.167)
in which the function h(k) is arbitrary. That is
fh(x) =

ei(k1x1+¬∑¬∑¬∑+knxn)Œ¥(P(ik1, . . . , ikn)) h(k) dnk.
(3.168)
130

3.13 APPLICATION TO DIFFERENTIAL EQUATIONS
Our solution to the differential equation (3.157) then is a sum of a partic-
ular solution (3.164) of the inhomogeneous equation (3.163) and our solution
(3.168) of the associated homogeneous equation (3.165)
f (x1, . . . , xn) =

ei(k1x1+¬∑¬∑¬∑+knxn)
# Àúg(k1, . . . , kn)
P(ik1, . . . , ikn)
+ Œ¥(P(ik1, . . . , ikn)) h(k1, . . . , kn)
$
dnk
(3.169)
in which h(k1, . . . , kn) is an arbitrary function. The wave equation and the
diffusion equation will provide examples of this formula
f (x) =

eik¬∑x
# Àúg(k)
P(ik) + Œ¥(P(ik))h(k)
$
dnk.
(3.170)
Example 3.13 (Wave equation for a scalar Ô¨Åeld)
A free scalar Ô¨Åeld œÜ(x) of mass
m in Ô¨Çat space-time obeys the wave equation

‚àá2 ‚àí‚àÇ2
t ‚àím2
œÜ(x) = 0
(3.171)
in natural units (¬Øh = c = 1). We may use a four-dimensional Fourier transform
to represent the Ô¨Åeld œÜ(x) as
œÜ(x) =

eik¬∑x ÀúœÜ(k) d4k
(2œÄ)2 ,
(3.172)
in which k ¬∑ x = k ¬∑ x ‚àík0t is the Lorentz-invariant inner product.
The homogeneous wave equation (3.171) then says

‚àá2 ‚àí‚àÇ2
t ‚àím2
œÜ(x) =
 
‚àík2 + (k0)2 ‚àím2
eik¬∑x ÀúœÜ(k) d4k
(2œÄ)2 = 0,
(3.173)
which implies the algebraic equation

‚àík2 + (k0)2 ‚àím2
ÀúœÜ(k) = 0
(3.174)
an instance of (3.166). Our solution (3.168) is
œÜ(x) =

Œ¥

‚àík2 + (k0)2 ‚àím2
eik¬∑x h(k) d4k
(2œÄ)2 ,
(3.175)
in which h(k) is an arbitrary function. The argument of the delta function
gk(k0) ‚â°(k0)2 ‚àík2 ‚àím2 =

k0 ‚àí

k2 + m2
 
k0 +

k2 + m2

(3.176)
has zeros at k0 = ¬±

k2 + m2 ‚â°¬±œâk with |g‚Ä≤
k(¬±œâk)| = 2œâk. So using our
formula (3.46) for integrals involving delta functions of functions, we have
131

FOURIER AND LAPLACE TRANSFORMS
œÜ(x) =
 
ei(k¬∑x‚àíœâkt) h+(k) + ei(k¬∑x+œâkt) h‚àí(k)

d3k
(2œÄ)22œâk
(3.177)
where h¬±(k) ‚â°h(¬±œâk, k). Since œâk is an even function of k, we can write
œÜ(x) =
 
ei(k¬∑x‚àíœâkt) h+(k) + e‚àíi(k¬∑x‚àíœâkt) h‚àí(‚àík)

d3k
(2œÄ)22œâk
.
(3.178)
If œÜ(x) is a real-valued classical Ô¨Åeld, then (3.24) tells us that h‚àí(‚àík) = h+(k)‚àó.
If it is a hermitian quantum Ô¨Åeld, then h‚àí(‚àík) = h‚Ä†
+(k). One sets a(k) ‚â°
h+(k)/‚àö4œÄœâk and writes œÜ(x) as an integral over a(k), an annihilation operator,
œÜ(x) =
 
ei(k¬∑x‚àíœâkt) a(k) + e‚àíi(k¬∑x‚àíœâkt) a‚Ä†(k)

d3k

(2œÄ)32œâk
,
(3.179)
and its adjoint a‚Ä†(k), a creation operator.
The momentum œÄ canonically conjugate to the Ô¨Åeld is its time derivative
œÄ(x) = ‚àíi
 
ei(k¬∑x‚àíœâkt) a(k) ‚àíe‚àíi(k¬∑x‚àíœâkt) a‚Ä†(k)
 &
œâk
2(2œÄ)3 d3k.
(3.180)
If the operators a and a‚Ä† obey the commutation relations
[a(k), a‚Ä†(k‚Ä≤)] = Œ¥(k ‚àík‚Ä≤)
and
[a(k), a(k‚Ä≤)] = [a‚Ä†(k), a‚Ä†(k‚Ä≤)] = 0
(3.181)
then the Ô¨Åeld œÜ(x, t) and its conjugate momentum œÄ(y, t) satisfy (exercise 3.11)
the equal-time commutation relations
[œÜ(x, t), œÄ(y, t)] = iŒ¥(x ‚àíy)
and
[œÜ(x, t), œÜ(y, t)] = [œÄ(x, t), œÄ(y, t)] = 0,
(3.182)
which generalize the commutation relations of quantum mechanics
[qj, p‚Ñì] = i¬ØhŒ¥j,‚Ñì
and
[qj, q‚Ñì] = [pj, p‚Ñì] = 0
(3.183)
for a set of coordinates qj and conjugate momenta p‚Ñì.
Example 3.14 (Fourier series for a scalar Ô¨Åeld)
For a Ô¨Åeld deÔ¨Åned in a cube of
volume V = L3, one often imposes periodic boundary conditions (section 2.13)
in which a displacement of any spatial coordinate by ¬±L does not change the
value of the Ô¨Åeld. A Fourier series can represent a periodic Ô¨Åeld. Using the rela-
tionship (3.96) between Fourier-transform and Fourier-series representations in
three dimensions, we expect the Fourier series representation for the Ô¨Åeld (3.179)
to be
œÜ(x) = (2œÄ)3
V

k
1

(2œÄ)32œâk

a(k)ei(k¬∑x‚àíœâkt) + a‚Ä†(k)e‚àíi(k¬∑x‚àíœâkt)
=

k
1
‚àö2œâkV
(
(2œÄ)3
V

a(k)ei(k¬∑x‚àíœâkt) + a‚Ä†(k)e‚àíi(k¬∑x‚àíœâkt)
, (3.184)
132

3.13 APPLICATION TO DIFFERENTIAL EQUATIONS
in which the sum over k = (2œÄ/L)(‚Ñì, n, m) is over all (positive and negative)
integers ‚Ñì, n, and m. One can set
ak ‚â°
(
(2œÄ)3
V
a(k)
(3.185)
and write the Ô¨Åeld as
œÜ(x) =

k
1
‚àö2œâk V

ak ei(k¬∑x‚àíœâkt) + a‚Ä†
k e‚àíi(k¬∑x‚àíœâkt)
.
(3.186)
The commutator of Fourier-series annihilation and creation operators is by
(3.36, 3.181, & 3.185)
[ak, a‚Ä†
k‚Ä≤] = (2œÄ)3
V
[a(k), a‚Ä†(k‚Ä≤)] = (2œÄ)3
V
Œ¥(k ‚àík‚Ä≤)
= (2œÄ)3
V

ei(k‚àík‚Ä≤)¬∑x d3x
(2œÄ)3 = (2œÄ)3
V
V
(2œÄ)3 Œ¥k,k‚Ä≤ = Œ¥k,k‚Ä≤, (3.187)
in which the Kronecker delta Œ¥k,k‚Ä≤ is Œ¥‚Ñì,‚Ñì‚Ä≤Œ¥n,n‚Ä≤Œ¥m,m‚Ä≤.
Example 3.15 (Diffusion)
The Ô¨Çow rate J (per unit area, per unit time) of a
Ô¨Åxed number of randomly moving particles, such as molecules of a gas or a
liquid, is proportional to the negative gradient of their density œÅ(x, t)
J(x, t) = ‚àíD‚àáœÅ(x, t)
(3.188)
where D is the diffusion constant, an equation known as Fick‚Äôs law (Adolf Fick,
1829‚Äì1901). Since the number of particles is conserved, the 4-vector J = (œÅ, J)
obeys the conservation law
‚àÇ
‚àÇt

œÅ(x, t) d3x = ‚àí
)
J(x, t) ¬∑ da = ‚àí

‚àá¬∑ J(x, t)d3x,
(3.189)
which with Fick‚Äôs law (3.188) gives the diffusion equation
ÀôœÅ(x, t) = ‚àí‚àá¬∑ J(x, t) = D‚àá2œÅ(x, t)
or

D‚àá2 ‚àí‚àÇt

œÅ(x, t) = 0.
(3.190)
Fourier had in mind such equations when he invented his transform.
If we write the density œÅ(x, t) as the transform
œÅ(x, t) =

eik¬∑x+iœât ÀúœÅ(k, œâ) d3kdœâ
(3.191)
then the diffusion equation becomes

D‚àá2 ‚àí‚àÇt

œÅ(x, t) =

eik¬∑x+iœât 
‚àíDk2 ‚àíiœâ

ÀúœÅ(k, œâ) d3kdœâ = 0,
(3.192)
which implies the algebraic equation

Dk2 + iœâ

ÀúœÅ(k, œâ) = 0.
(3.193)
133

FOURIER AND LAPLACE TRANSFORMS
Our solution (3.168) of this homogeneous equation is
œÅ(x, t) =

eik¬∑x+iœâtŒ¥

‚àíDk2 ‚àíiœâ

h(k, œâ) d3kdœâ,
(3.194)
in which h(k, œâ) is an arbitrary function. Dirac‚Äôs delta function requires œâ to be
imaginary œâ = iDk2, with Dk2 > 0. So the œâ-integration is up the imaginary
axis. It is a Laplace transform, and we have
œÅ(x, t) =
 ‚àû
‚àí‚àû
eik¬∑x‚àíDk2t ÀúœÅ(k) d3k,
(3.195)
in which ÀúœÅ(k) ‚â°h(k, iDk2). Thus the function ÀúœÅ(k) is the Fourier transform of
the initial density œÅ(x, 0)
œÅ(x, 0) =
 ‚àû
‚àí‚àû
eik¬∑x ÀúœÅ(k) d3k.
(3.196)
So if the initial density œÅ(x, 0) is concentrated at y
œÅ(x, 0) = Œ¥(x ‚àíy) =
 ‚àû
‚àí‚àû
eik¬∑(x‚àíy) d3k
(2œÄ)3
(3.197)
then its Fourier transform ÀúœÅ(k) is
ÀúœÅ(k) = e‚àíik¬∑y
(2œÄ)3
(3.198)
and at later times the density œÅ(x, t) is given by (3.195) as
œÅ(x, t) =
 ‚àû
‚àí‚àû
eik¬∑(x‚àíy)‚àíDk2t d3k
(2œÄ)3 .
(3.199)
Using our formula (3.18) for the Fourier transform of a gaussian, we Ô¨Ånd
œÅ(x, t) =
1
(4œÄDt)3/2 e‚àí(x‚àíy)2/(4Dt).
(3.200)
Since the diffusion equation is linear, it follows (exercise 3.12) that an arbitrary
initial distribution œÅ(y, 0) evolves to the distribution
œÅ(x, t) =
1
(4œÄDt)3/2

e‚àí(x‚àíy)2/(4Dt) œÅ(y, 0) d3y
(3.201)
at time t. Such convolutions often occur in physics (section 3.6).
Exercises
3.1
Show that the Fourier integral formula (3.26) for real functions follows from
(3.9) and (3.25).
3.2
Show that the Fourier integral formula (3.26) for real functions implies (3.27)
if f is even and (3.28) if it is odd.
134

EXERCISES
3.3
Derive the formula (3.30) for the square wave (3.29).
3.4
By using the Fourier-transform formulas (3.27 & 3.28), derive the formu-
las (3.31) and (3.32) for the even and odd extensions of the exponential
exp(‚àíŒ≤|x|).
3.5
For the state |œà, t‚ü©given by equations (3.85 & 3.80), Ô¨Ånd the wave-function
œà(x, t) = ‚ü®x|œà, t‚ü©at time t. Then Ô¨Ånd the variance of the position operator at
that time. Does it grow as time goes by?
3.6
At time t = 0, a particle of mass m is in a gaussian superposition of momen-
tum eigenstates centered at p = ¬ØhK:
œà(x, 0) = N
 ‚àû
‚àí‚àû
eikxe‚àíl2(k‚àíK)2dk.
(3.202)
(a) Shift k by K and do the integral. Where is the particle most likely to be
found? (b) At time t, the wave function œà(x, t) is œà(x, 0) but with ikx replaced
by ikx‚àíi¬Øhk2t/2m. Shift k by K and do the integral. Where is the particle most
likely to be found? (c) Does the wave packet spread out like t or like ‚àöt as in
classical diffusion?
3.7
Express a probability distribution P(x) as the Fourier transform of its
characteristic function (3.87).
3.8
Express the characteristic function (3.87) of a probability distribution as a
power series in its moments (3.89).
3.9
Find the characteristic function (3.87) of the gaussian probability distribution
PG(x, Œº, œÉ) =
1
œÉ
‚àö
2œÄ
exp

‚àí(x ‚àíŒº)2
2œÉ 2

.
(3.203)
3.10 Find the moments Œºn = E[xn] for n = 0, . . . , 3 of the gaussian probability
distribution PG(x, Œº, œÉ).
3.11 Show that the commutation relations (3.181) of the annihilation and creation
operators imply the equal-time commutation relations (3.182) for the Ô¨Åeld œÜ
and its conjugate momentum œÄ.
3.12 Use the linearity of the diffusion equation and equations (3.197‚Äì3.200) to
derive the general solution (3.201) of the diffusion equation.
3.13 Derive (3.112) from B = ‚àá√ó A and Amp√®re‚Äôs law ‚àá√ó B = Œº0J.
3.14 Derive (3.113) from (3.112).
3.15 Derive (3.114) from (3.113).
3.16 Use the Green‚Äôs function relations (3.107) and (3.108) to show that (3.114)
satisÔ¨Åes (3.112).
3.17 Show that the Laplace transform of tz‚àí1 is the gamma function (4.55) divided
by sz
f (s) =
 ‚àû
0
e‚àíst tz‚àí1 dt = s‚àíz (z).
(3.204)
3.18 Compute the Laplace transform of 1/‚àöt. Hint: let t = u2.
135

4
InÔ¨Ånite series
4.1 Convergence
A sequence of partial sums
SN =
N

n=0
cn
(4.1)
converges to a number S if for every œµ > 0 there exists an integer N(œµ) such that
|S ‚àíSN| < œµ
for all
N > N(œµ).
(4.2)
The number S is then said to be the limit of the convergent inÔ¨Ånite series
S =
‚àû

n=0
cn = lim
N‚Üí‚àûSN = lim
N‚Üí‚àû
N

n=0
cn.
(4.3)
Some series converge; others wander or oscillate; and others diverge.
A series whose absolute values converge
S =
‚àû

n=0
|cn|
(4.4)
is said to converge absolutely. A convergent series that is not absolutely
convergent is said to converge conditionally.
Example 4.1 (Two inÔ¨Ånite series)
The series of inverse factorials converges to
the number e = 2.718281828 . . .
‚àû

n=0
1
n! = e.
(4.5)
136

4.2 TESTS OF CONVERGENCE
But the harmonic series of inverse integers diverges
‚àû

k=1
1
k ‚Üí‚àû
(4.6)
as one may see by grouping its terms
1 + 1
2 +
1
3 + 1
4

+
1
5 + 1
6 + 1
7 + 1
8

+ ¬∑ ¬∑ ¬∑ ‚â•1 + 1
2 + 1
2 + 1
2 + ¬∑ ¬∑ ¬∑
(4.7)
to form a series that obviously diverges. This series up to 1/n approaches the
natural logarithm ln n to within a constant
Œ≥ = lim
n‚Üí‚àû
 n

k=1
1
k ‚àíln n

= 0.5772156649 . . .
(4.8)
known as the Euler‚ÄìMascheroni constant (Leonhard Euler, 1707‚Äì1783; Lorenzo
Mascheroni, 1750‚Äì1800).
4.2 Tests of convergence
The Cauchy criterion for the convergence of a sequence SN is that for every
œµ > 0 there is an integer N(œµ) such that for N > N(œµ) and M > N(œµ) one has
|SN ‚àíSM| < œµ.
(4.9)
Cauchy‚Äôs criterion is equivalent to the deÔ¨Åning condition (4.2).
Suppose the convergent series
‚àû

n=0
bn
(4.10)
has only positive terms bn ‚â•0, and that |cn| ‚â§bn for all n. Then the series
‚àû

n=0
cn
(4.11)
also (absolutely) converges. This is the comparison test.
Similarly, if for all n, the inequality 0 ‚â§cn ‚â§bn holds and the series of
numbers cn diverges, then so does the series of numbers bn.
If for some N, the terms cn satisfy
|cn|1/n ‚â§x < 1
(4.12)
for all n > N, then the series
‚àû

n=0
cn
(4.13)
converges by the Cauchy root test.
137

INFINITE SERIES
In the ratio test of d‚ÄôAlembert, the series 
n cn converges if
lim
n‚Üí‚àû

cn+1
cn
 = r < 1
(4.14)
and diverges if r > 1.
Probably the most useful test is the Intel test, in which one writes a computer
program to sum the Ô¨Årst N terms of the series and then runs it for N = 100,
N = 10, 000, N = 1, 000, 000, ..., as seems appropriate.
4.3 Convergent series of functions
A sequence of partial sums
SN(z) =
N

n=0
fn(z)
(4.15)
of functions fn(z) converges to a function S(z) on a set D if for every œµ > 0 and
every z ‚ààD, there exists an integer N(œµ, z) such that
|S(z) ‚àíSN(z)| < œµ
for all
N > N(œµ, z).
(4.16)
The numbers z may be real or complex. The function S(z) is said to be the limit
on D of the convergent inÔ¨Ånite series of functions
S(z) =
‚àû

n=0
fn(z).
(4.17)
A sequence of partial sums SN(z) of functions converges uniformly on the set
D if the integers N(œµ, z) can be chosen independently of the point z ‚ààD, that is,
if for every œµ > 0 and every z ‚ààD, there exists an integer N(œµ) such that
|S(z) ‚àíSN(z)| < œµ
for all
N > N(œµ).
(4.18)
The limit (2.52) of the integral over a closed interval a ‚â§x ‚â§b of a uniformly
convergent sequence of partial sums SN(x) of continuous functions is equal to
the integral of the limit
lim
N‚Üí‚àû
 b
a
SN(x) dx =
 b
a
S(x) dx.
(4.19)
A real or complex-valued function f (x) of a real variable x is square integrable
on an interval [a, b] if the integral
 b
a
|f (x)|2 dx
(4.20)
138

4.4 POWER SERIES
exists and is Ô¨Ånite. A sequence of partial sums
SN(x) =
N

n=0
fn(x)
(4.21)
of square-integrable functions fn(x) converges in the mean to a function S(x) if
lim
N‚Üí‚àû
 b
a
|S(x) ‚àíSN(x)|2 dx = 0.
(4.22)
Convergence in the mean sometimes is deÔ¨Åned as
lim
N‚Üí‚àû
 b
a
œÅ(x) |S(x) ‚àíSN(x)|2 dx = 0,
(4.23)
in which œÅ(x) ‚â•0 is a weight function that is positive except at isolated points
where it may vanish. If the functions fn are real, then this deÔ¨Ånition of
convergence in the mean takes the slightly simpler form
lim
N‚Üí‚àû
 b
a
œÅ(x) (S(x) ‚àíSN(x))2 dx = 0.
(4.24)
4.4 Power series
A power series is a series of functions with fn(z) = cn zn
S(z) =
‚àû

n=0
cn zn.
(4.25)
By the ratio test (4.14), this power series converges if
lim
n‚Üí‚àû

cn+1zn+1
cnzn
 = |z| lim
n‚Üí‚àû

cn+1
cn
 ‚â°|z|
R < 1,
(4.26)
that is, if z lies within a circle of radius R
|z| < R
(4.27)
given by
R =

lim
n‚Üí‚àû
|cn+1|
|cn|
‚àí1
.
(4.28)
Within this circle, the convergence is uniform and absolute.
139

INFINITE SERIES
Example 4.2 (geometric series)
For any positive integer N, the simple identity
(1 ‚àíz)(1 + z + z2 + ¬∑ ¬∑ ¬∑ + zN) = 1 ‚àízN+1
(4.29)
implies that
SN(z) =
N

n=0
zn = 1 ‚àízN+1
1 ‚àíz
.
(4.30)
For |z| < 1, the term |zN+1| ‚Üí0 as N ‚Üí‚àû, and so the power series
S‚àû(z) =
‚àû

n=0
zn =
1
1 ‚àíz
(4.31)
converges to the function 1/(1 ‚àíz) as long as the absolute value of z is less than
unity. The radius of convergence R is unity in agreement with the estimate (4.28)
R =

lim
n‚Üí‚àû
|cn+1|
|cn|
‚àí1
=

lim
n‚Üí‚àû1
‚àí1
= 1.
(4.32)
For tiny z, the approximation
1
1 ¬± z ‚âà1 ‚àìz
(4.33)
is useful.
Example 4.3 (Credit)
If a man deposits $100 in a bank, he has a credit of $100.
Suppose banks are required to retain as reserves 10% of their deposits and are
free to lend the other 90%. Then the bank getting the $100 deposit can lend out
$90 to a borrower. That borrower can deposit $90 in another bank. That bank
can then lend $81 to another borrower. Now three people have credits of $100 +
$90 + $81 = $271. This multiplication of money is the miracle of credit.
If P is the original deposit and r is the fraction of deposits that banks must
retain as reserves, then the total credit due to P is
P + P(1 ‚àír) + P(1 ‚àír)2 + ¬∑ ¬∑ ¬∑ = P
‚àû

n=0
(1 ‚àír)n = P

1
1 ‚àí(1 ‚àír)

= P
r . (4.34)
An initial deposit of P = $100 with r = 10% can produce total credits of P/r =
$1000. A reserve requirement of 1% can lead to total credits of $10,000. Since
banks charge a higher rate of interest on money they lend than the rate they
pay to their depositors, bank proÔ¨Åts soar as r ‚Üí0. This is why bankers love
deregulation.
The funds all the banks hold in reserve due to a deposit P is
Pr + (1 ‚àír)Pr + (1 ‚àír)2Pr + ¬∑ ¬∑ ¬∑ = Pr
‚àû

n=0
(1 ‚àír)n = P,
(4.35)
P itself.
140

4.5 FACTORIALS AND THE GAMMA FUNCTION
4.5 Factorials and the gamma function
For any positive integer n, the product
n! ‚â°n(n ‚àí1)(n ‚àí2) . . . 3 ¬∑ 2 ¬∑ 1
(4.36)
is n-factorial, with zero-factorial deÔ¨Åned as unity
0! ‚â°1.
(4.37)
To estimate n!, one can use Stirling‚Äôs approximation
n! ‚âà
‚àö
2œÄ n (n/e)n
(4.38)
or Ramanujan‚Äôs correction to it
n! ‚âà
‚àö
2œÄ n (n/e)n 
1 + 1/2n + 1/8n21/6
(4.39)
or Mermin‚Äôs Ô¨Årst
n! ‚âà
‚àö
2œÄ n
n
e
n
exp
 1
12 n

(4.40)
or second approximation
n! ‚âà
‚àö
2œÄ n
n
e
n
exp
 1
12 n ‚àí
1
360 n3 +
1
1260 n5

,
(4.41)
which follow from his exact inÔ¨Ånite-product formula
n! =
‚àö
2œÄ n
n
e
n ‚àû

j=1

(1 + 1/j)j+1/2
e

.
(4.42)
Figure 4.1 plots the relative error of these estimates E(n!) of n!
108
E(n!) ‚àín!
n!

(4.43)
magniÔ¨Åed by 108, except for Stirling‚Äôs formula (4.38), whose relative error is
off the chart. Mermin‚Äôs second approximation (4.40) is the most accurate, fol-
lowed by Ramanujan‚Äôs correction (4.39), and by Mermin‚Äôs Ô¨Årst approximation
(4.40) (James Stirling, 1692‚Äì1770; Srinivasa Ramanujan, 1887‚Äì1920; N. David
Mermin, 1935‚Äì).
The binomial coefÔ¨Åcient is a ratio of factorials
n
k

‚â°
n!
k! (n ‚àík)!.
(4.44)
141

INFINITE SERIES
0
10
20
30
40
50
60
70
80
90
100
‚àí5
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
5 x 10‚àí8
Relative accuracy of these approximations to n!
‚àö‚éØ‚éØ
2œÄ n (n/e)n exp(1/12n ‚àí 1/360n3 + 1/1260n5)
‚àö‚éØ‚éØ
2œÄ n (n/e)n exp(1/12n)
‚àö‚éØ‚éØ
2œÄ n (n/e)n (1 + 1/2n + 1/8n2)1/6
n
108(E(n!) ‚àí n!)/n!
Figure 4.1
The magniÔ¨Åed relative error 108[E(n!)‚àín!]/n! of Ramanujan‚Äôs (4.39) and
Mermin‚Äôs (4.40 & 4.41) estimates E(n!) of n! are plotted for n = 1, 2, . . . , 100.
Example 4.4 (The Leibniz rule)
We can use the notation
f (n)(x) ‚â°dn
dxn f (x)
(4.45)
to state Leibniz‚Äôs rule for the derivatives of the product of two functions
dn
dxn [f (x) g(x)] =
n

k=0
n
k

f (k)(x) g(n‚àík)(x).
(4.46)
One may use mathematical induction to prove this rule, which is obviously true
for n = 0 and n = 1 (exercise 4.4).
Example 4.5 (The exponential function)
The power series with coefÔ¨Åcients cn =
1/n! deÔ¨Ånes the exponential function
ez =
‚àû

n=0
zn
n! .
(4.47)
142

4.5 FACTORIALS AND THE GAMMA FUNCTION
Formula (4.28) shows that the radius of convergence R of this power series is
inÔ¨Ånite
R =

lim
n‚Üí‚àû
|cn+1|
|cn|
‚àí1
=

lim
n‚Üí‚àû
1
n + 1
‚àí1
= ‚àû.
(4.48)
The series converges uniformly and absolutely inside every circle.
Example 4.6 (Bessel‚Äôs series)
For any integer n, the series
Jn(œÅ) = œÅn
2nn!

1 ‚àí
œÅ2
2(2n + 2) +
œÅ4
2 ¬∑ 4(2n + 2)(2n + 4) ‚àí¬∑ ¬∑ ¬∑

=
œÅ
2
n
‚àû

m=0
(‚àí1)m
m!(m + n)!
œÅ
2
2m
(4.49)
deÔ¨Ånes the cylindrical Bessel function of the Ô¨Årst kind, which is Ô¨Ånite at the
origin œÅ = 0. This series converges even faster (exercise 4.5) than the one (4.47)
for the exponential function.
Double factorials also are useful and are deÔ¨Åned as
(2n ‚àí1)!! ‚â°(2n ‚àí1)(2n ‚àí3)(2n ‚àí5) ¬∑ ¬∑ ¬∑ 1,
(4.50)
(2n)!! ‚â°2n(2n ‚àí2)(2n ‚àí4) ¬∑ ¬∑ ¬∑ 2
(4.51)
with 0!! and (‚àí1)!! both deÔ¨Åned as unity
0!! = (‚àí1)!! = 1.
(4.52)
Thus 5!! = 5 ¬∑ 3 ¬∑ 1 = 15, and 6!! = 6 ¬∑ 4 ¬∑ 2 = 48.
One may extend the deÔ¨Ånition (4.36) of n-factorial from positive integers to
complex numbers by means of the integral formula
z! ‚â°
 ‚àû
0
e‚àít tz dt
(4.53)
for Re z > ‚àí1. In particular
0! =
 ‚àû
0
e‚àít dt = 1,
(4.54)
which explains the deÔ¨Ånition (4.37). The factorial function (z ‚àí1)! in turn
deÔ¨Ånes the gamma function for Re z > 0 as
(z) =
 ‚àû
0
e‚àít tz‚àí1 dt = (z ‚àí1)!
(4.55)
143

INFINITE SERIES
as may be seen from (4.53). By differentiating this formula and integrating it by
parts, we see that the gamma function satisÔ¨Åes the key identity
(z + 1) =
 ‚àû
0

‚àíd
dte‚àít

tz dt =
 ‚àû
0
e‚àít
 d
dttz

dt =
 ‚àû
0
e‚àít z tz‚àí1 dt
= z (z)
(4.56)
with (1) = 0! = 1. We may use this identity (4.56) to extend the deÔ¨Ånition
(5.102) of the gamma function in unit steps into the left half-plane
(z) = 1
z (z + 1) = 1
z
1
z + 1 (z + 2) = 1
z
1
z + 1
1
z + 2 (z + 3) = ¬∑ ¬∑ ¬∑ (4.57)
as long as we avoid the negative integers and zero. This extension leads to
Euler‚Äôs deÔ¨Ånition
(z) = lim
n‚Üí‚àû
1 ¬∑ 2 ¬∑ 3 ¬∑ ¬∑ ¬∑ n
z(z + 1)(z + 2) ¬∑ ¬∑ ¬∑ (z + n) nz
(4.58)
and to Weierstrass‚Äôs (exercise 4.6)
(z) = 1
z e‚àíŒ≥ z
 ‚àû

n=1

1 + z
n

e‚àíz/n
‚àí1
(4.59)
(Karl Theodor Wilhelm Weierstrass, 1815‚Äì1897), and is an example of analytic
continuation (section 5.12).
One may show (exercise 4.8) that another formula for (z) is
(z) = 2
 ‚àû
0
e‚àít2t2z‚àí1 dt
(4.60)
for Re z > 0 and that
(n + 1
2) = (2n)!
n! 2n
‚àöœÄ.
(4.61)
Identity (4.56) and formula (4.61) imply (exercise 4.11) that

2n + 1
2

= (2n ‚àí1)!!
2n
‚àöœÄ.
(4.62)
Example 4.7 (Bessel function of nonintegral index)
We can use the gamma-
function formula (4.55) for n! to extend the deÔ¨Ånition (4.49) of the Bessel
function of the Ô¨Årst kind Jn(œÅ) to nonintegral values ŒΩ of the index n. Replacing
n by ŒΩ and (m + n)! by (m + ŒΩ + 1), we get
JŒΩ(œÅ) =
œÅ
2
ŒΩ
‚àû

m=0
(‚àí1)m
m! (m + ŒΩ + 1)
œÅ
2
2m
,
(4.63)
which makes sense even for complex values of ŒΩ.
144

4.6 TAYLOR SERIES
Example 4.8 (Spherical Bessel function)
The spherical Bessel function is
deÔ¨Åned as
j‚Ñì(œÅ) ‚â°
& œÄ
2œÅ J‚Ñì+1/2(œÅ).
(4.64)
For small values of its argument |œÅ| ‚â™1, the Ô¨Årst term in the series (4.63)
dominates and so (exercise 4.7)
j‚Ñì(œÅ) ‚âà
&œÄ
2
œÅ
2
‚Ñì
1
(‚Ñì+ 3/2) = ‚Ñì! (2œÅ)‚Ñì
(2‚Ñì+ 1)! =
œÅ‚Ñì
(2‚Ñì+ 1)!!
(4.65)
as one may show by repeatedly using the key identity (z + 1) = z (z).
4.6 Taylor series
If the function f (x) is a real-valued function of a real variable x with a
continuous Nth derivative, then Taylor‚Äôs expansion for it is
f (x + a) = f (x) + af ‚Ä≤(x) + a2
2 f ‚Ä≤‚Ä≤(x) + ¬∑ ¬∑ ¬∑ +
an‚àí1
(n ‚àí1)!f (N‚àí1) + EN
=
N‚àí1

n=0
an
n! f (n)(x) + EN,
(4.66)
in which the error EN is
EN = aN
N! f (N)(x + y)
(4.67)
for some 0 ‚â§y ‚â§a.
For many functions f (x) the errors go to zero, EN ‚Üí0, as N ‚Üí‚àû; for these
functions, the inÔ¨Ånite Taylor series converges:
f (x + a) =
‚àû

n=0
an
n! f (n)(x) = exp

a d
dx

f (x) = eiap/¬Øh f (x),
(4.68)
in which
p = ¬Øh
i
d
dx
(4.69)
is the displacement operator or equivalently the momentum operator.
145

INFINITE SERIES
4.7 Fourier series as power series
The Fourier series (2.37)
f (x) =
‚àû

n=‚àí‚àû
cn
ei2œÄnx/L
‚àö
L
(4.70)
with coefÔ¨Åcients (2.45)
cn =
 L/2
‚àíL/2
e‚àíi2œÄnx/L
‚àö
L
f (x) dx
(4.71)
is a pair of power series
f (x) =
1
‚àö
L
 ‚àû

n=0
cn zn +
‚àû

n=1
c‚àín (z‚àí1)n

(4.72)
in the variables
z = ei2œÄx/L
and
z‚àí1 = e‚àíi2œÄx/L.
(4.73)
Formula (4.28) tells us that the radii of convergence of these two power series
are given by
R‚àí1
+ = lim
n‚Üí‚àû
|cn+1|
|cn|
and
R‚àí1
‚àí= lim
n‚Üí‚àû
|c‚àín‚àí1|
|c‚àín| .
(4.74)
Thus the pair of power series (4.72) will converge uniformly and absolutely as
long as z satisÔ¨Åes the two inequalities
|z| < R+
and
1
|z| < R‚àí.
(4.75)
Since |z| = 1, the Fourier series (4.70) converges if R‚àí1
‚àí< |1| < R+.
Example 4.9 (A uniform and absolutely convergent Fourier series)
The Fourier
series
f (x) =
‚àû

n=‚àí‚àû
1
1 + |n|n
ei2œÄnx/L
‚àö
L
(4.76)
converges uniformly and absolutely because R+ = R‚àí= ‚àû.
146

4.8 THE BINOMIAL SERIES AND THEOREM
4.8 The binomial series and theorem
The Taylor series for the function f (x) = (1 + x)a is
(1 + x)a =
‚àû

n=0
xn
n!
dn
dxn (1 + x)a
x=0
= 1 + ax + 1
2a(a ‚àí1)x2 + ¬∑ ¬∑ ¬∑
= 1 +
‚àû

n=1
a(a ‚àí1) ¬∑ ¬∑ ¬∑ (a ‚àín + 1)
n!
xn.
(4.77)
If a is a positive integer a = N, then the nth power of x in this series is multiplied
by a binomial coefÔ¨Åcient (4.44)
(1 + x)N =
N

n=0
N!
n!(N ‚àín)! xn =
N

n=0
N
n

xn.
(4.78)
The series (4.77) and (4.78) respectively imply (exercise 4.13)
(x + y)a = ya +
‚àû

n=1
a(a ‚àí1) ¬∑ ¬∑ ¬∑ (a ‚àín + 1)
n!
xn ya‚àín
(4.79)
and
(x + y)N =
N

n=0
N
n

xn yN‚àín.
(4.80)
We can use these versions of the binomial theorem to compute approximately or
exactly.
Example 4.10
The phase difference œÜ between two highly relativistic neutri-
nos of momentum p going a distance L in a time t ‚âàL varies with their masses
m1 and m2 as
œÜ = t E = LE
p E = LE
p

p2 + m2
1 ‚àí

p2 + m2
2

(4.81)
in natural units. We can approximate this phase by using the Ô¨Årst two terms of
the binomial expansion (4.79) with y = 1 and x = m2
i /p2
œÜ = LE

1 + m2
1/p2 ‚àí

1 + m2
2/p2

‚âàLEm2
p2
‚âàLm2
E
(4.82)
or in ordinary units œÜ ‚âàLm2c3/(¬ØhE).
147

INFINITE SERIES
Example 4.11
We can use the binomial expansion (4.80) to compute
9993 =

103 ‚àí1
3
= 109 ‚àí3 √ó 106 + 3 √ó 103 ‚àí1 = 997002999
(4.83)
exactly.
When a is not a positive integer, the series (4.77) does not terminate. For
instance, the binomial series for
‚àö
1 + x and 1/
‚àö
1 + x are (exercise 4.14)
(1 + x)1/2 = 1 +
‚àû

n=1
1
2

1
2 ‚àí1

¬∑ ¬∑ ¬∑

1
2 ‚àín + 1

n!
xn
= 1 +
‚àû

n=1
(‚àí1)n‚àí1
2n
(2n ‚àí3)!!
n!
xn
(4.84)
and
(1 + x)‚àí1/2 = 1 +
‚àû

n=1
‚àí1
2

‚àí3
2

¬∑ ¬∑ ¬∑

‚àí1
2 ‚àín + 1

n!
xn
=
‚àû

n=0
(‚àí1)n
2n
(2n ‚àí1)!!
n!
xn.
(4.85)
4.9 Logarithmic series
The Taylor series for the function f (x) = ln(1 + x) is
ln(1 + x) =
‚àû

n=0
xn
n!
dn
dxn ln(1 + x)|x=0
(4.86)
in which
f (0)(0) = ln(1 + x)|x=0 = 0,
f (1)(0) =
1
1 + x

x=0
= 1,
f (n)(0) = (‚àí1)n‚àí1 (n ‚àí1)!
(1 + x)n

x=0
= (‚àí1)n‚àí1 (n ‚àí1)!.
(4.87)
So the series for ln(1 + x) is
ln(1 + x) =
‚àû

n=1
(‚àí1)n‚àí1 xn
n
= x ‚àí1
2x2 + 1
3x3 ¬± ¬∑ ¬∑ ¬∑ ,
(4.88)
148

4.10 DIRICHLET SERIES AND THE ZETA FUNCTION
which converges slowly for ‚àí1 < x ‚â§1. Letting x ‚Üí‚àíx, we see that
ln(1 ‚àíx) = ‚àí
‚àû

n=1
xn
n .
(4.89)
So the series for the logarithm of the ratio (1 + x)/(1 ‚àíx) is
ln
1 + x
1 ‚àíx

= 2
‚àû

n=0
x2n+1
2n + 1.
(4.90)
4.10 Dirichlet series and the zeta function
A Dirichlet series is one in which the nth term is proportional to 1/nz
f (z) =
‚àû

n=1
cn
nz .
(4.91)
An important example is the Riemann zeta function Œ∂(z)
Œ∂(z) =
‚àû

n=1
n‚àíz,
(4.92)
which converges for Re z > 1.
Euler showed that for Re z > 1, the Riemann zeta function is the inÔ¨Ånite
product
Œ∂(z) =

p
1
1 ‚àíp‚àíz
(4.93)
over all prime numbers p = 2, 3, 5, 7, 11, . . .. Some speciÔ¨Åc values are Œ∂(2) =
œÄ2/6 ‚âà1.645, Œ∂(4) = œÄ4/90 ‚âà1.0823, and Œ∂(6) = œÄ6/945 ‚âà1.0173.
Example 4.12 (Planck‚Äôs distribution)
Max Planck (1858‚Äì1947) showed that the
electromagnetic energy in a closed cavity of volume V at a temperature T in the
frequency interval dŒΩ about ŒΩ is
dU(Œ≤, ŒΩ, V) = 8œÄhV
c3
ŒΩ3
eŒ≤hŒΩ ‚àí1 dŒΩ,
(4.94)
in which Œ≤ = 1/(kT), k = 1.3806503 √ó 10‚àí23 J/K is Boltzmann‚Äôs constant, and
h = 6.626068√ó10‚àí34 Js is Planck‚Äôs constant. The total energy then is the integral
U(Œ≤, V) = 8œÄhV
c3
 ‚àû
0
ŒΩ3
eŒ≤hŒΩ ‚àí1 dŒΩ,
(4.95)
149

INFINITE SERIES
which we may do by letting x = Œ≤hŒΩ and using the geometric series (4.31)
U(Œ≤, V) = 8œÄ(kT)4V
(hc)3
 ‚àû
0
x3
ex ‚àí1 dx
= 8œÄ(kT)4V
(hc)3
 ‚àû
0
x3e‚àíx
1 ‚àíe‚àíx dx
= 8œÄ(kT)4V
(hc)3
 ‚àû
0
x3e‚àíx
‚àû

n=0
e‚àínx dx.
(4.96)
The geometric series is absolutely and uniformly convergent for x > 0, and we
may interchange the limits of summation and integration. After another change
of variables, the gamma-function formula (5.102) gives
U(Œ≤, V) = 8œÄ(kT)4V
(hc)3
‚àû

n=0
 ‚àû
0
x3e‚àí(n+1)x dx
= 8œÄ(kT)4V
(hc)3
‚àû

n=0
1
(n + 1)4
 ‚àû
0
y3e‚àíy dy
= 8œÄ(kT)4V
(hc)3
3! Œ∂(4) = 8œÄ5(kT)4V
15(hc)3
.
(4.97)
It follows that the power radiated by a ‚Äúblack body‚Äù is proportional to the fourth
power of its temperature and to its area A
P = œÉ A T4,
(4.98)
in which
œÉ = 2œÄ5k4
15(hc)3 = 5.670400(40) √ó 10‚àí8 W m‚àí2 K‚àí4
(4.99)
is Stefan‚Äôs constant.
The number of photons in the black-body distribution (4.94) at inverse
temperature Œ≤ in the volume V is
N(Œ≤, V) = 8œÄV
c3
 ‚àû
0
ŒΩ2
eŒ≤hŒΩ ‚àí1 dŒΩ = 8œÄV
(cŒ≤h)3
 ‚àû
0
x2
ex ‚àí1 dx
= 8œÄV
(cŒ≤h)3
 ‚àû
0
x2e‚àíx
1 ‚àíe‚àíx dx = 8œÄV
(cŒ≤h)3
 ‚àû
0
x2e‚àíx
‚àû

n=0
e‚àínx dx
= 8œÄV
(cŒ≤h)3
‚àû

n=0
 ‚àû
0
x2e‚àí(n+1)x dx = 8œÄV
(cŒ≤h)3
‚àû

n=0
1
(n + 1)3
 ‚àû
0
y2e‚àíy dy
= 8œÄV
(cŒ≤h)3 Œ∂(3)2! = 8œÄ(kT)3V
(ch)3
Œ∂(3)2!.
(4.100)
150

4.11 BERNOULLI NUMBERS AND POLYNOMIALS
The mean energy ‚ü®E‚ü©of a photon in the black-body distribution (4.94) is the
energy U(Œ≤, V) divided by the number of photons N(Œ≤, V)
‚ü®E‚ü©= ‚ü®hŒΩ‚ü©= 3! Œ∂(4)
2! Œ∂(3) kT =
œÄ4
30 Œ∂(3)
(4.101)
or ‚ü®E‚ü©‚âà2.70118 kT since Ap√©ry‚Äôs constant Œ∂(3) is 1.2020569032 ...(Roger
Ap√©ry, 1916‚Äì1994).
Example 4.13 (The Lerch transcendent)
The Lerch transcendent is the series
(z, s, Œ±) =
‚àû

n=0
zn
(n + Œ±)s .
(4.102)
It converges when |z| < 1 and Re s > 0 and Re Œ± > 0.
4.11 Bernoulli numbers and polynomials
The Bernoulli numbers Bn are deÔ¨Åned by the inÔ¨Ånite series
x
ex ‚àí1 =
‚àû

n=0
xn
n!
# dn
dxn
x
ex ‚àí1
$
x=0
=
‚àû

n=0
Bn
xn
n!
(4.103)
for the generating function x/(ex ‚àí1). They are the successive derivatives
Bn = dn
dxn
x
ex ‚àí1

x=0
.
(4.104)
So B0 = 1 and B1 = ‚àí1/2. The remaining odd Bernoulli numbers vanish
B2n+1 = 0
for n > 0
(4.105)
and the remaining even ones are given by Euler‚Äôs zeta function (4.92) formula
B2n = (‚àí1)n‚àí12(2n)!
(2œÄ)2n
Œ∂(2n)
for n > 0.
(4.106)
The Bernoulli numbers occur in the power series for many transcendental
functions, for instance
coth x = 1
x +
‚àû

k=1
22kB2k
(2k)! x2k‚àí1
for x2 < œÄ2.
(4.107)
Bernoulli‚Äôs polynomials Bn(y) are deÔ¨Åned by the series
xexy
ex ‚àí1 =
‚àû

n=0
Bn(y) xn
n!
(4.108)
for the generating function xexy/(ex ‚àí1).
151

INFINITE SERIES
Some authors (Whittaker and Watson, 1927, p. 125‚Äì127) deÔ¨Åne Bernoulli‚Äôs
numbers instead by
Bn = 2(2n)!
(2œÄ)2n Œ∂(2n) = 4n
 ‚àû
0
t2n‚àí1 dt
e2œÄt ‚àí1,
(4.109)
a result due to Carda.
4.12 Asymptotic series
A series
sn(x) =
n

k=0
ak
xk
(4.110)
is an asymptotic expansion for a real function f (x) if the remainder Rn
Rn(x) = f (x) ‚àísn(x)
(4.111)
satisÔ¨Åes the condition
lim
x‚Üí‚àûxnRn(x) = 0
(4.112)
for Ô¨Åxed n. In this case, one writes
f (x) ‚âà
n

k=0
ak
xk
(4.113)
where the wavy equal sign indicates equality in the sense of (4.112). Some
authors add the condition
lim
n‚Üí‚àûxnRn(x) = ‚àû
(4.114)
for Ô¨Åxed x in order to exclude convergent series in powers of 1/x.
Example 4.14 (The asymptotic series for E1)
Let‚Äôs develop an asymptotic
expansion for the function
E1(x) =
 ‚àû
x
e‚àíy dy
y ,
(4.115)
which is related to the exponential-integral function
Ei(x) =
 x
‚àí‚àû
ey dy
y
(4.116)
by the tricky formula E1(x) = ‚àíEi(‚àíx). Since
e‚àíy
y
= ‚àíd
dy
e‚àíy
y

‚àíe‚àíy
y2
(4.117)
152

4.12 ASYMPTOTIC SERIES
we may integrate by parts, getting
E1(x) = e‚àíx
x
‚àí
 ‚àû
x
e‚àíy dy
y2 .
(4.118)
Integrating by parts again, we Ô¨Ånd
E1(x) = e‚àíx
x
‚àíe‚àíx
x2 + 2
 ‚àû
x
e‚àíy dy
y3 .
(4.119)
Eventually, we develop the series
E1(x) = e‚àíx
0!
x ‚àí1!
x2 + 2!
x3 ‚àí3!
x4 + 4!
x5 ‚àí¬∑ ¬∑ ¬∑

(4.120)
with remainder
Rn(x) = (‚àí1)n n!
 ‚àû
x
e‚àíy
dy
y n+1 .
(4.121)
Setting y = u + x, we have
Rn(x) = (‚àí1)n n! e‚àíx
xn+1
 ‚àû
0
e‚àíu
du

1 + u
x
n+1 ,
(4.122)
which satisÔ¨Åes the condition (4.112) that deÔ¨Ånes an asymptotic series
lim
x‚Üí‚àûxnRn(x) = lim
x‚Üí‚àû(‚àí1)n n! e‚àíx
x
 ‚àû
0
e‚àíu
du

1 + u
x
n+1
= lim
x‚Üí‚àû(‚àí1)n n! e‚àíx
x
 ‚àû
0
e‚àíu du
= lim
x‚Üí‚àû(‚àí1)n n! e‚àíx
x
= 0
(4.123)
for Ô¨Åxed n.
Asymptotic series often occur in physics. In such physical problems, a small
parameter Œª usually plays the role of 1/x. A perturbative series
Sn(Œª) =
n

k=0
ak Œªk
(4.124)
is an asymptotic expansion of the physical quantity S(Œª) if the remainder
Rn(Œª) = S(Œª) ‚àíSn(Œª)
(4.125)
satisÔ¨Åes for Ô¨Åxed n
lim
Œª‚Üí0 Œª‚àín Rn(Œª) = 0.
(4.126)
The WKB approximation and the Dyson series for quantum electrodynamics
are asymptotic expansions in this sense.
153

INFINITE SERIES
4.13 Some electrostatic problems
Gauss‚Äôs law ‚àá¬∑ D = œÅ equates the divergence of the electric displacement D
to the density œÅ of free charges (charges that are free to move in or out of
the dielectric medium ‚Äì as opposed to those that are part of the medium and
bound to it by molecular forces). In electrostatic problems, Maxwell‚Äôs equa-
tions reduce to Gauss‚Äôs law and the static form ‚àá√ó E = 0 of Faraday‚Äôs law,
which implies that the electric Ô¨Åeld E is the gradient of an electrostatic potential
E = ‚àí‚àáV.
Across an interface with normal vector ÀÜn between two dielectrics, the tangen-
tial electric Ô¨Åeld is continuous while the normal electric displacement jumps by
the surface charge density œÉ
ÀÜn √ó (E2 ‚àíE1) = 0
and
œÉ = ÀÜn ¬∑ (D2 ‚àíD1) .
(4.127)
In a linear dielectric, the electric displacement D is proportional to the electric
Ô¨Åeld D = œµm E, where the permittivity œµm = œµ0 +œám = Km œµ0 of the material dif-
fers from that of the vacuum œµ0 by the electric susceptibility œám and the relative
permittivity Km. The permittivity of the vacuum is the electric constant œµ0.
An electric Ô¨Åeld E exerts on a charge q a force F = qE even in a dielec-
tric medium. The electrostatic energy W of a system of linear dielectrics is the
volume integral
W = 1
2

D ¬∑ E d3r.
(4.128)
Example 4.15 (Field of a charge near an interface)
Consider two semi-inÔ¨Ånite
dielectrics of permittivities œµ1 and œµ2 separated by an inÔ¨Ånite horizontal x-y-
plane. What is the electrostatic potential due to a charge q in region 1 at a height
h above the interface?
The easy way to solve this problem is to put an image charge q‚Ä≤ at the same
distance from the interface in region 2 so that the potential in region 1 is
V1(r) =
1
4œÄœµ1

q

x2 + y2 + (z ‚àíh)2 +
q‚Ä≤

x2 + y2 + (z + h)2

.
(4.129)
This potential satisÔ¨Åes Gauss‚Äôs law ‚àá¬∑ D = œÅ in region 1. In region 2, the
potential
V2(r) =
1
4œÄœµ2
q‚Ä≤‚Ä≤

x2 + y2 + (z ‚àíh)2
(4.130)
also satisÔ¨Åes Gauss‚Äôs law. The continuity (4.127) of the tangential component of
E tells us that the partial derivatives of V1 and V2 in the x (or y) direction must
be the same at z = 0
154

4.13 SOME ELECTROSTATIC PROBLEMS
‚àÇV1(x, y, 0)
‚àÇx
= ‚àÇV2(x, y, 0)
‚àÇx
.
(4.131)
The discontinuity equation (4.127) for the electric displacement says that at the
interface at z = 0 with no surface charge
œµ1
‚àÇV1(x, y, 0)
‚àÇz
= œµ2
‚àÇV2(x, y, 0)
‚àÇz
.
(4.132)
These two equations (4.131 & 4.132) allow one to solve for q‚Ä≤ and q‚Ä≤‚Ä≤
q‚Ä≤ = œµ1 ‚àíœµ2
œµ1 + œµ2
q
and
q‚Ä≤‚Ä≤ =
2œµ2
œµ1 + œµ2
q.
(4.133)
In the limit h ‚Üí0, the potential in region 1 becomes
V1(r) =
1
4œÄœµ1
q

x2 + y2 + z2

1 + œµ1 ‚àíœµ2
œµ1 + œµ2

=
q
4œÄ ¬Øœµr,
(4.134)
in which ¬Øœµr is the mean permittivity ¬Øœµ = (œµ1 + œµ2)/2. Similarly, in region 2 the
potential is
V2(r) =
1
4œÄœµ2
q

x2 + y2 + z2
2œµ2
œµ1 + œµ2
=
q
4œÄ ¬Øœµr
(4.135)
in the limit h ‚Üí0.
Example 4.16 (A charge near a plasma membrane )
A eukaryotic cell (the kind
with a nucleus) is surrounded by a plasma membrane, which is a phospholipid
bilayer about 5 nm thick. Both sides of the plasma membrane are in contact
with salty water. The permittivity of the water is œµw ‚âà80œµ0 while that of the
membrane considered as a simple lipid slab is œµ‚Ñì‚âà2œµ0.
Now let‚Äôs think about the potential felt by an ion in the water outside a cell
but near its membrane, and let us for simplicity imagine the membrane to be
inÔ¨Ånitely thick so that we can use the simple formulas we‚Äôve derived. The poten-
tial due to the ion, if its charge is q, is then given by equation (4.129) with œµ1 = œµw
and œµ2 = œµ‚Ñì. The image-charge term in V1(r) is the potential due to the polar-
ization of the membrane and the water by the ion. It is the potential felt by the
ion. Since the image charge by (4.133) is q‚Ä≤ ‚âàq, the potential the ion feels is
Vi(z) ‚âàq/8œÄewz. The force on the ion then is
F = ‚àíqV‚Ä≤
i (z) =
q2
8œÄewz.
(4.136)
It always is positive no matter what the sign of the charge is. A lipid slab in water
repels ions. Similarly, a charge in a lipid slab is attracted to the water outside
the slab.
155

INFINITE SERIES
Next imagine an electric dipole in water near a lipid slab. Now there are two
equal and opposite charges and two equal and opposite mirror charges. The net
effect is that the slab repels the dipole. So lipids repel water molecules; they are
said to be hydrophobic. This is one of the reasons why folding proteins move their
hydrophobic amino acids inside and their polar or hydrophilic ones outside.
With some effort, one may use the method of images to compute the electric
potential of a charge in or near a plasma membrane taken to be a lipid slab of
Ô¨Ånite thickness.
The electric potential in the lipid bilayer V‚Ñì(œÅ, z) of thickness t due to a charge
q in the extracellular environment at a height h above the bilayer is
V‚Ñì(œÅ, z) =
q
4œÄœµw‚Ñì
‚àû

n=0
(pp‚Ä≤)n

1

œÅ2 + (z ‚àí2nt ‚àíh)2
‚àí
p‚Ä≤

œÅ2 + (z + 2(n + 1)t + h)2

,
(4.137)
in which p = (œµw ‚àíœµ‚Ñì)/(œµw + œµ‚Ñì), p‚Ä≤ = (œµc ‚àíœµ‚Ñì)/(œµc + œµ‚Ñì), and œµw‚Ñì= (œµw + œµ‚Ñì)/2.
That in the extracellular environment is
Vw(œÅ, z) =
q
4œÄœµw

1
r +
p

œÅ2 + (z + h)2
‚àíœµwœµ‚Ñì
œµ2
w‚Ñì
‚àû

n=1
pn‚àí1p‚Ä≤n

œÅ2 + (z + 2nt + h)2

,
(4.138)
in which r is the distance from the charge q. Finally, the potential in the cytosol
is
Vc(œÅ, z) =
q œµ‚Ñì
4œÄœµw‚Ñìœµ‚Ñìc
‚àû

n=0
(pp‚Ä≤)n

œÅ2 + (z ‚àí2nt ‚àíh)2
(4.139)
where œµ‚Ñìc = (œµ‚Ñì+ œµc)/2.
The Ô¨Årst 1000 terms of these three series (4.137‚Äì4.139) are plotted in Fig. 4.2
for the case of a positive charge q = |e| at (œÅ, z) = (0, 0) (top curve), (0, 1)
(middle curve), (0, 2) (third curve), and (0, 6) nm (bottom curve). Although
the potential V(œÅ, z) is continuous across the two interfaces, its normal deriva-
tive isn‚Äôt due to the different dielectric constants in the three media. Because
the potential is small and Ô¨Çat in the cytosol (z < ‚àí5 nm), charges in the
extracellular environment (z > 0) are nearly decoupled from those in the
cytosol.
Real plasma membranes are phospholipid bilayers. The lipids avoid the water
and so are on the inside. The phosphate groups are dipoles (and phosphatidylser-
ine is negatively charged). So a real membrane is a 4 nm thick lipid layer bounded
on each side by dipole layers, each about 0.5 nm thick. The net effect is to weakly
attract ions that are within 0.5 nm of the membrane.
156

4.14 INFINITE PRODUCTS
‚àí10
‚àí8
‚àí6
‚àí4
‚àí2
0
2
4
6
8
10
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
Height z (nm) above lipid bilayer
Electric potential V (œÅ , z) (V)
Potential due to a charge near a lipid slab
Figure 4.2
The electric potential V(œÅ, z) from (4.137‚Äì4.139) in volts for œÅ = 1
nm as a function of the height z (nm) above (or below) a lipid slab for a unit charge
q = |e| at (œÅ, z) = (0, 0) (top curve), (0, 1) (second curve), (0, 2) (third curve),
and (0, 6) nm (bottom curve). The lipid slab extends from z = 0 to z = ‚àí5 nm,
and the cytosol lies below z = ‚àí5 nm. The relative permittivities were taken to be
œµw/œµ0 = œµc/œµ0 = 80 and œµ‚Ñì/œµ0 = 2.
4.14 InÔ¨Ånite products
Weierstrass‚Äôs deÔ¨Ånition (4.59) of the gamma function, Euler‚Äôs formula (4.93)
for the zeta function, and Mermin‚Äôs formula (4.42) for n! are useful inÔ¨Ånite
products. Other examples are the expansions of the trigonometric functions
(2.66 & 2.67)
sin z = z
‚àû

n=1

1 ‚àí
z2
œÄ2n2

and
cos z =
‚àû

n=1

1 ‚àí
z2
œÄ2(n ‚àí1/2)2

,
(4.140)
which imply those of the hyperbolic functions
sinh z = z
‚àû

n=1

1 +
z2
œÄ2n2

and
cosh z =
‚àû

n=1

1 +
z2
œÄ2(n ‚àí1/2)2

. (4.141)
157

INFINITE SERIES
Exercises
4.1
Test the following series for convergence:
(a)
‚àû

n=2
1
(ln n)2 ,
(b)
‚àû

n=1
n!
20n ,
(c)
‚àû

n=1
1
n(n + 2),
(d)
‚àû

n=2
1
n ln n.
In each case, say whether the series converges and how you found out.
4.2
Olber‚Äôs paradox: assume a static universe with a uniform density of stars. With
you at the origin, divide space into successive shells of thickness t, and assume
that the stars in each shell subtend the same solid angle œâ (as follows from the
Ô¨Årst assumption). Take into account the occulting of distant stars by nearer
ones and show that the total solid angle subtended by all the stars would be
4œÄ. The sky would be dazzlingly bright at night.
4.3
Use the geometric formula (4.30) to derive the trigonometric summation
formula
1
2 + cos Œ± + cos 2Œ± + ¬∑ ¬∑ ¬∑ + cos nŒ± = sin(n + 1
2)Œ±
2 sin 1
2Œ±
.
(4.142)
Hint: write cos nŒ± as [exp(inŒ±) + exp(‚àíinŒ±)]/2.
4.4
Show that
n ‚àí1
k

+
n ‚àí1
k ‚àí1

=
n
k

(4.143)
and then use mathematical induction to prove Leibniz‚Äôs rule (4.46).
4.5
(a) Find the radius of convergence of the series (4.49) for the Bessel function
Jn(œÅ). (b) Show that this series converges even faster than the one (4.47) for
the exponential function.
4.6
Use the formula (4.8) for the Euler‚ÄìMascheroni constant to show that Euler‚Äôs
deÔ¨Ånition (4.58) of the gamma function implies Weierstrass‚Äôs (4.59).
4.7
Derive the approximation (4.65) for j‚Ñì(œÅ) for |œÅ| ‚â™1.
4.8
Derive formula (4.60) for the gamma function from its deÔ¨Ånition (4.55).
4.9
Use formula (4.60) to compute (1/2).
4.10 Show that z! = (z + 1) diverges when z is a negative integer.
4.11 Derive formula (4.62) for ((2n + 1)/2).
4.12 Show that the area of the surface of the unit sphere in d dimensions is
Ad = 2œÄd/2/(d/2).
(4.144)
Hint: compute the integral of the gaussian exp(‚àíx2) in d dimensions using
both rectangular and spherical coordinates. This formula (4.144) is used in
dimensional regularization (Weinberg, 1995, p. 477).
4.13 Derive (4.80) from (4.78) and (4.79) from (4.77).
4.14 Derive the expansions (4.84 & 4.85) for
‚àö
1 + x and 1/
‚àö
1 + x.
4.15 Find the radii of convergence of the series (4.84) and (4.85).
158

EXERCISES
4.16 Find the Ô¨Årst three Bernoulli polynomials Bn(y) by using their generating
function (4.108).
4.17 How are the two deÔ¨Ånitions (4.106) and (4.109) of the Bernoulli numbers
related?
4.18 Show that the Lerch transcendent (z, s, Œ±) deÔ¨Åned by the series (4.102)
converges when |z| < 1 and Re s > 0 and Re Œ± > 0.
4.19 Langevin‚Äôs classical formula for the electrical polarization of a gas or liquid
of molecules of electric dipole moment p is
P(x) = Np
cosh x
sinh x ‚àí1
x

(4.145)
where x = pE/(kT), E is the applied electric Ô¨Åeld, and N is the number density
of the molecules per unit volume. (a) Expand P(x) for small x as an inÔ¨Å-
nite power series involving the Bernoulli numbers. (b) What are the Ô¨Årst three
terms expressed in terms of familiar constants? (c) Find the saturation limit
of P(x) as x ‚Üí‚àû.
4.20 Show that the energy of a charge q spread on the surface of a sphere of radius
a in an inÔ¨Ånite lipid of permittivity œµ‚Ñìis W = q2/8œÄœµ‚Ñìa.
4.21 If the lipid of exercise 4.20 has Ô¨Ånite thickness t and is surrounded on both
sides by water of permittivity œµw, then the image charges lower the energy W
by (Parsegian, 1969)
W =
q2
4œÄœµ‚Ñìt
‚àû

n=1
1
n
œµ‚Ñì‚àíœµw
œµ‚Ñì+ œµw
n
.
(4.146)
Sum this series. Hint: read section 4.9 carefully.
4.22 Consider a stack of three dielectrics of inÔ¨Ånite extent in the x-y-plane sepa-
rated by the two inÔ¨Ånite x-y-planes z = t/2 and z = ‚àít/2. Suppose the upper
region z > t/2 is a uniform linear dielectric of permittivity œµ1, the central
region ‚àít/2 < z < t/2 is a uniform linear dielectric of permittivity œµ2, and
the lower region z < ‚àít/2 is a uniform linear dielectric of permittivity œµ3.
Suppose the lower inÔ¨Ånite x-y-plane z = ‚àít/2 has a uniform surface charge
density ‚àíœÉ, while the upper plane z = t/2 has a uniform surface charge den-
sity œÉ. What is the energy per unit area of this system? What is the pressure
on the second dielectric? What is the capacitance of the stack?
159

5
Complex-variable theory
5.1 Analytic functions
A complex-valued function f (z) of a complex variable z is differentiable at z
with derivative f ‚Ä≤(z) if the limit
f ‚Ä≤(z) = lim
z‚Ä≤‚Üíz
f (z‚Ä≤) ‚àíf (z)
z‚Ä≤ ‚àíz
(5.1)
exists as z‚Ä≤ approaches z from any direction in the complex plane. The limit must
exist no matter how or from what direction z‚Ä≤ approaches z.
If the function f (z) is differentiable in a small disk around a point z0, then
f (z) is said to be analytic at z0 (and at all points inside the disk).
Example 5.1 (Polynomials)
If f (z) = zn for some integer n, then for tiny dz and
z‚Ä≤ = z + dz, the difference f (z‚Ä≤) ‚àíf (z) is
f (z‚Ä≤) ‚àíf (z) = (z + dz)n ‚àízn ‚âànzn‚àí1 dz
(5.2)
and so the limit
lim
z‚Ä≤‚Üíz
f (z‚Ä≤) ‚àíf (z)
z‚Ä≤ ‚àíz
= lim
dz‚Üí0
nzn‚àí1 dz
dz
= nzn‚àí1
(5.3)
exists and is nzn‚àí1 independently of how z‚Ä≤ approaches z. Thus the function zn is
analytic at z for all z with derivative
dzn
dz = nzn‚àí1.
(5.4)
160

5.2 CAUCHY‚ÄôS INTEGRAL THEOREM
A function that is analytic everywhere is entire. All polynomials
P(z) =
N

n=0
cn zn
(5.5)
are entire.
Example 5.2 (A function that‚Äôs not analytic)
To see what can go wrong when
a function is not analytic, consider the function f (x, y) = x2 + y2 = z¬Øz for
z = x + iy. If we compute its derivative at (x, y) = (1, 0) by setting x = 1 + œµ and
y = 0, then the limit is
lim
œµ‚Üí0
f (1 + œµ, 0) ‚àíf (1, 0)
œµ
= lim
œµ‚Üí0
(1 + œµ)2 ‚àí1
œµ
= 2
(5.6)
while if we instead set x = 1 and y = œµ, then the limit is
lim
œµ‚Üí0
f (1, œµ) ‚àíf (1, 0)
iœµ
= lim
œµ‚Üí0
1 + œµ2 ‚àí1
iœµ
= ‚àíi lim
œµ‚Üí0 œµ = 0.
(5.7)
So the derivative depends upon the direction through which z ‚Üí1.
5.2 Cauchy‚Äôs integral theorem
If f (z) is analytic at z0, then near z0 and to Ô¨Årst order in z ‚àíz0
f (z) ‚âàf (z0) + f ‚Ä≤(z0) (z ‚àíz0).
(5.8)
Let‚Äôs compute the contour integral of f (z) along a small circle of radius œµ and
center z0. The points on the contour are
z = z0 + œµ eiŒ∏
(5.9)
for Œ∏ ‚àà[0, 2œÄ]. So dz = iœµ eiŒ∏dŒ∏, and the contour integral is
)
f (z) dz =
 2œÄ
0

f (z0) + f ‚Ä≤(z0) (z ‚àíz0)

iœµ eiŒ∏dŒ∏.
(5.10)
Since z ‚àíz0 = œµ eiŒ∏, the contour integral breaks into two pieces
)
f (z) dz = f (z0)
 2œÄ
0
iœµ eiŒ∏dŒ∏ + f ‚Ä≤(z0)
 2œÄ
0
œµ eiŒ∏ iœµ eiŒ∏dŒ∏,
(5.11)
which vanish because the Œ∏-integrals are zero. So the contour integral of the
analytic function f (z)
)
f (z) dz = 0
(5.12)
is zero around the tiny circle ‚Äì at least to order œµ2.
161

COMPLEX-VARIABLE THEORY
What about the contour integral of an analytic function f (z) around a tiny
square of size œµ? Again we use the analyticity of f (z) at z = z0 to expand it as
f (z) ‚âàf (z0) + f ‚Ä≤(z0) (z ‚àíz0)
(5.13)
on the tiny square. The square contour consists of the four complex segments
dz1 = œµ, dz2 = i œµ, dz3 = ‚àíœµ, and dz4 = ‚àíi œµ. The centers zn of these segments are
displaced by z1 ‚àíz0 = ‚àíi œµ/2, z2 ‚àíz0 = œµ/2, z3 ‚àíz0 = i œµ/2, and z4 ‚àíz0 = ‚àíœµ/2
from z0. The integral of f (z) around the square
)
f (z) dz =
4

n=1
f (zn) dzn =
4

n=1

f (z0) + (zn ‚àíz0) f ‚Ä≤(z0)

dzn
(5.14)
splits into two pieces
)
f (z) dz = f (z0) I1 + f ‚Ä≤(z0) I2.
(5.15)
The four segments dzn form a path that goes around the square and ends where
it started, so the Ô¨Årst piece f (z0)I1 is zero
f (z0) I1 = f (z0) [œµ + i œµ + (‚àíœµ) + (‚àíi œµ)] = 0.
(5.16)
And so is the second one f ‚Ä≤(z0) I2
f ‚Ä≤(z0)I2 = f ‚Ä≤(z0) [(z1 ‚àíz0)dz1 + (z2 ‚àíz0)dz2 + (z3 ‚àíz0)dz3 + (z4 ‚àíz0)dz4]
= f ‚Ä≤(z0) [(‚àíi œµ/2) œµ + (œµ/2) i œµ + (i œµ/2) (‚àíœµ) + (‚àíœµ/2) (‚àíi œµ)]
= f ‚Ä≤(z0) (œµ2/2) [‚àíi + i ‚àíi + i] = 0.
(5.17)
So the contour integral of an analytic function f (z) around a tiny square of side
œµ is zero to order œµ2. Thus, the integral around such a square can be at most of
order œµ3. This is very important. We‚Äôll use it to prove Cauchy‚Äôs integral theorem.
Let‚Äôs consider a function f (z) that is analytic on a square of side L, as pictured
in Fig. 5.1. The contour integral of f (z) around the square can be expressed as
the sum of L2/œµ2 contour integrals around tiny squares of side œµ. All interior
integrals cancel, leaving the integral around the perimeter. Each contour inte-
gral around its tiny square is at most of order œµ3. So the sum of the L2/œµ2 tiny
contour integrals is at most (L2/œµ2) œµ3 = œµ L2, which vanishes as œµ ‚Üí0. Thus
the contour integral of a function f (z) along the perimeter of a square of side
L vanishes if f (z) is analytic on the perimeter and inside the square. This is an
example of Cauchy‚Äôs integral theorem.
Suppose a function f (z) is analytic in a region R and that I is a contour
integral along a straight line within that region from z1 to z2. The contour inte-
gral of f (z) around any square inside the region R of analyticity is zero. So by
successively adding contour integrals around small squares to the straight-line
162

5.2 CAUCHY‚ÄôS INTEGRAL THEOREM
>
<
‚à®
‚àß
‚àß
‚à®
>
>
<
<
The Cauchy integral theorem
Figure 5.1
The sum of two contour integrals around two adjacent tiny squares is equal
to the contour integral around the perimeter of the two tiny squares because the up
integral along the right side of the left square cancels (dots) the down integral along the
left side of the right square. A contour integral around a big L √ó L square is equal to
the sum of the contour integrals around the L2/œµ2 tiny œµ √ó œµ squares that tile the big
square.
contour integral, one may deform the straight-line contour into an arbitrary
contour from z1 to z2 without changing its value.
So a contour integral from z1 to z2 of a function f (z) that is analytic in a
region R remains invariant as we continuously deform the contour C to C‚Ä≤ as
long as these contours and all the intermediate contours lie entirely within the
region R and have the same Ô¨Åxed endpoints z1 and z2 as in Fig. 5.2
I =
 z2
z1C
f (z) dz =
 z2
z1C‚Ä≤ f (z) dz.
(5.18)
Thus a contour integral depends upon its endpoints and upon the function f (z)
but not upon the actual contour as long as the deformations of the contour do
not push it outside the region R in which f (z) is analytic.
If the endpoints z1 and z2 are the same, then the contour C is closed, and we
write the integral as
I =
) z1
z1C
f (z) dz ‚â°
)
C
f (z) dz
(5.19)
with a little circle to denote that the contour is a closed loop. The value of
that integral is independent of the contour as long as our deformations of the
163

COMPLEX-VARIABLE THEORY
‚àß
‚àß
‚àß
‚àß
region of analyticity
Four equal contour integrals
Figure 5.2
As long as the four contours are within the domain of analyticity of f (z)
and have the same endpoints, the four contour integrals of that function are all equal.
contour keep it within the domain of analyticity of the function and as long as
the contour starts and ends at z1 = z2. Now suppose that the function f (z) is
analytic along the contour and at all points within it. Then we can shrink the
contour, staying within the domain of analyticity of the function, until the area
enclosed is zero and the contour is of zero length ‚Äì all this without changing
the value of the integral. But the value of the integral along such a null contour
of zero length is zero. Thus the value of the original contour integral also must
be zero
) z1
z1C
f (z) dz = 0.
(5.20)
And so we arrive at Cauchy‚Äôs integral theorem: The contour integral of a func-
tion f (z) around a closed contour C lying entirely within the domain R of
analyticity of the function vanishes
)
C
f (z) dz = 0
(5.21)
as long as the function f (z) is analytic at all points within the contour.
A region in the complex plane is simply connected if we can shrink every loop
in the region to a point while keeping the loop in the region. A slice of American
cheese is simply connected, but a slice of Swiss cheese is not. A dime is simply
connected, but a washer isn‚Äôt. The surface of a sphere is simply connected, but
the surface of a bagel isn‚Äôt.
With this deÔ¨Ånition, we can restate the integral theorem of Cauchy: The
contour integral of a function f (z) around a closed contour C vanishes
164

5.3 CAUCHY‚ÄôS INTEGRAL FORMULA
)
C
f (z) dz = 0
(5.22)
if the contour lies within a simply connected domain of analyticity of the
function f (z) (Augustin-Louis Cauchy, 1789‚Äì1857).
If a region R is simply connected, then we may deform any contour C from
z1 to z2 in R into any other contour C‚Ä≤ from z1 to z2 in R while keeping the
moving contour in the region R. So another way of understanding the Cauchy
integral theorem is to ask, what is the value of the contour integral
IM =
 z1
z2C‚Ä≤ f (z) dz ?
(5.23)
This integral is the same as the integral along C from z1 to z2, except for the
sign of the dzs and the order in which the terms are added, and so
IM =
 z1
z2C‚Ä≤ f (z) dz = ‚àí
 z2
z1C
f (z) dz.
(5.24)
Now consider a closed contour running along the contour C from z1 to z2
and backwards along C‚Ä≤ from z2 to z1 all within a simply connected region R
of analyticity. Since IM = ‚àíI, the integral of f (z) along this closed contour
vanishes:
)
f (z) dz = I + IM = I ‚àíI = 0
(5.25)
and we have again derived Cauchy‚Äôs integral theorem.
Example 5.3 (Polynomials are entire functions)
Every polynomial
P(z) =
N

n=0
cnzn
(5.26)
is entire (everywhere analytic), and so its integral along any closed contour
)
P(z) dz = 0
(5.27)
must vanish.
5.3 Cauchy‚Äôs integral formula
Let f (z) be analytic in a simply connected region R and z0 a point inside this
region. We Ô¨Årst will integrate the function f (z)/(z‚àíz0) along a tiny closed coun-
terclockwise contour around the point z0. The contour is a circle of radius œµ
with center at z0 with points z = z0 + œµ eiŒ∏ for 0 ‚â§Œ∏ ‚â§2œÄ, and dz = iœµ eiŒ∏dŒ∏.
Since z ‚àíz0 = œµ eiŒ∏, the contour integral in the limit œµ ‚Üí0 is
165

COMPLEX-VARIABLE THEORY
)
œµ
f (z)
z ‚àíz0
dz =
 2œÄ
0

f (z0) + f ‚Ä≤(z0) (z ‚àíz0)

z ‚àíz0
iœµ eiŒ∏dŒ∏
=
 2œÄ
0

f (z0) + f ‚Ä≤(z0) œµ eiŒ∏
œµ eiŒ∏
iœµ eiŒ∏dŒ∏
=
 2œÄ
0

f (z0) + f ‚Ä≤(z0) œµ eiŒ∏
idŒ∏.
(5.28)
The Œ∏-integral involving f ‚Ä≤(z0) vanishes, and so we have
f (z0) =
1
2œÄi
)
œµ
f (z)
z ‚àíz0
dz,
(5.29)
which is a miniature version of Cauchy‚Äôs integral formula.
Now consider the counterclockwise contour C‚Ä≤ in Fig. 5.3, which is a big
counterclockwise circle, a small clockwise circle, and two parallel straight lines,
all within a simply connected region R in which f (z) is analytic. The function
f (z)/(z‚àíz0) is analytic everywhere in R except at the point z0. We can withdraw
the contour C‚Ä≤ to the left of the point z0 and shrink it to a point without having
the contour C‚Ä≤ cross z0. During this process, the integral of f (z)/(z ‚àíz0) does
not change. Its Ô¨Ånal value is zero. So its initial value also is zero
>
>
<
<
z0
C¬¥
The contour C¬¥ around z0
Figure 5.3
The full contour is the sum of a big counterclockwise contour and a small
clockwise contour, both around z0, and two straight lines that cancel.
166

5.3 CAUCHY‚ÄôS INTEGRAL FORMULA
0 =
1
2œÄi
)
C‚Ä≤
f (z)
z ‚àíz0
dz.
(5.30)
We let the two straight-line segments approach each other so that they can-
cel. What remains of contour C‚Ä≤ is a big counterclockwise contour C around
z0 and a tiny clockwise circle of radius œµ around z0. The tiny clockwise
circle integral is the negative of the counterclockwise integral (5.29), so we
have
0 =
1
2œÄi
)
C‚Ä≤
f (z)
z ‚àíz0
dz =
1
2œÄi
)
C
f (z)
z ‚àíz0
dz ‚àí
1
2œÄi
)
œµ
f (z)
z ‚àíz0
dz.
(5.31)
Using the miniature result (5.29), we Ô¨Ånd
f (z0) =
1
2œÄi
)
C
f (z)
z ‚àíz0
dz,
(5.32)
which is Cauchy‚Äôs integral formula.
We can use this formula to compute the Ô¨Årst derivative f ‚Ä≤(z) of f (z)
f ‚Ä≤(z) = f (z + dz) ‚àíf (z)
dz
=
1
2œÄi
1
dz
)
dz‚Ä≤ f (z‚Ä≤)

1
z‚Ä≤ ‚àíz ‚àídz ‚àí
1
z‚Ä≤ ‚àíz

=
1
2œÄi
)
dz‚Ä≤
f (z‚Ä≤)
(z‚Ä≤ ‚àíz ‚àídz)(z‚Ä≤ ‚àíz).
(5.33)
So in the limit dz ‚Üí0, we get
f ‚Ä≤(z) =
1
2œÄi
)
dz‚Ä≤
f (z‚Ä≤)
(z‚Ä≤ ‚àíz)2 .
(5.34)
The second derivative f (2)(z) of f (z) then is
f (2)(z) =
2
2œÄi
)
dz‚Ä≤
f (z‚Ä≤)
(z‚Ä≤ ‚àíz)3 .
(5.35)
And its nth derivative f (n)(z) is
f (n)(z) = n!
2œÄi
)
dz‚Ä≤
f (z‚Ä≤)
(z‚Ä≤ ‚àíz)n+1 .
(5.36)
In these formulas, the contour runs counterclockwise about the point z and lies
within the simply connected domain R in which f (z) is analytic.
Thus a function f (z) that is analytic in a region R is inÔ¨Ånitely differentiable
there.
167

COMPLEX-VARIABLE THEORY
Example 5.4 (SchlaeÔ¨Çi‚Äôs formula for the Legendre polynomials)
Rodrigues
showed that the Legendre polynomial Pn(x) is the nth derivative
Pn(x) =
1
2n n!
 d
dx
n
(x2 ‚àí1)n.
(5.37)
SchlaeÔ¨Çi used this expression and Cauchy‚Äôs integral formula (5.36) to represent
Pn(z) as the contour integral (exercise 5.8)
Pn(z) =
1
2n 2œÄi
)
(z‚Ä≤2 ‚àí1)n
(z‚Ä≤ ‚àíz)n+1 dz‚Ä≤,
(5.38)
in which the contour encircles the complex point z counterclockwise. This
formula tells us that at z = 1 the Legendre polynomial is
Pn(1) =
1
2n 2œÄi
)
(z‚Ä≤2 ‚àí1)n
(z‚Ä≤ ‚àí1)n+1 dz‚Ä≤ =
1
2n 2œÄi
) (z‚Ä≤ + 1)n
(z‚Ä≤ ‚àí1) dz‚Ä≤ = 1,
(5.39)
in which we applied Cauchy‚Äôs integral formula (5.32) to f (z) = (z + 1)n.
Example 5.5 (Bessel functions of the Ô¨Årst kind)
The counterclockwise integral
around the unit circle z = eiŒ∏ of the ratio zm/zn in which both m and n are
integers is
1
2œÄi
)
dz zm
zn =
1
2œÄi
 2œÄ
0
ieiŒ∏dŒ∏ ei(m‚àín)Œ∏ = 1
2œÄ
 2œÄ
0
dŒ∏ ei(m+1‚àín)Œ∏.
(5.40)
If m + 1 ‚àín Ã∏= 0, this integral vanishes because exp 2œÄi(m + 1 ‚àín) = 1
1
2œÄ
 2œÄ
0
dŒ∏ ei(m+1‚àín)Œ∏ = 1
2œÄ

ei(m+1‚àín)Œ∏
i(m + 1 ‚àín)
2œÄ
0
= 0.
(5.41)
If m + 1 ‚àín = 0, the exponential is unity, exp i(m + 1 ‚àín)Œ∏ = 1, and the integral
is 2œÄ/2œÄ = 1. Thus the original integral is the Kronecker delta
1
2œÄi
)
dz zm
zn = Œ¥m+1, n.
(5.42)
The generating function (9.5) for Bessel functions Jm of the Ô¨Årst kind is
et(z‚àí1/z)/2 =
‚àû

m=‚àí‚àû
zmJm(t).
(5.43)
Applying our integral formula (5.42) to it, we Ô¨Ånd
1
2œÄi
)
dz et(z‚àí1/z)/2
1
zn+1 =
1
2œÄi
)
dz
‚àû

m=‚àí‚àû
zm
zn+1 Jm(t)
=
‚àû

m=‚àí‚àû
Œ¥m+1,n+1 Jm(t) = Jn(t).
(5.44)
168

5.4 THE CAUCHY‚ÄìRIEMANN CONDITIONS
Thus, letting z = eiŒ∏, we have
Jn(t) = 1
2œÄ
 2œÄ
0
dŒ∏ exp

t

eiŒ∏ ‚àíe‚àíiŒ∏
2
‚àíinŒ∏

(5.45)
or more simply
Jn(t) = 1
2œÄ
 2œÄ
0
dŒ∏ ei(t sin Œ∏‚àínŒ∏) = 1
œÄ
 œÄ
0
dŒ∏ cos(t sin Œ∏ ‚àínŒ∏)
(5.46)
(exercise 5.3).
5.4 The Cauchy‚ÄìRiemann conditions
We can write any complex-valued function of two real variables x and y as
f = u + iv where u(x, y) and v(x, y) are real. If we use subscripts for partial
differentiation ux = ‚àÇu/‚àÇx, uy = ‚àÇu/‚àÇy, and so forth, then the change in f due
to small changes in x and y is df = (ux + ivx) dx + (uy + ivy) dy. But if f is a
function of z = x + iy, rather than just of x and y, and if f is analytic at z, then
the change in f due to small changes in x and y is
df = (ux + ivx) dx + (uy + ivy) dy = f ‚Ä≤(z) dz = f ‚Ä≤(z)(dx + idy).
(5.47)
Setting Ô¨Årst dy and then dx equal to zero, we get Ô¨Årst ux + ivx = f ‚Ä≤ and then
‚àíiuy + vy = f ‚Ä≤, which give us the Cauchy‚ÄìRiemann conditions
ux = vy
and
uy = ‚àívx.
(5.48)
These conditions (5.48) hold because if f is analytic at z, then its derivative f ‚Ä≤
is independent of the direction from which dz ‚Üí0. Thus the derivatives in the
x-direction and the iy-direction are the same
fx = ux + ivx = fy/i =

uy + ivy

/i = ‚àíiuy + vy,
(5.49)
which again gives us the Cauchy‚ÄìRiemann conditions (5.48).
The directions in the x-y plane in which the real u and imaginary v parts of
an analytic function change most rapidly are the vectors (ux, uy) and (vx, vy).
The Cauchy‚ÄìRiemann conditions (5.48) imply that these directions must be
perpendicular
(ux, uy) ¬∑ (vx, vy) = uxvx + uyvy = vyvx ‚àívxvy = 0.
(5.50)
The Cauchy‚ÄìRiemann conditions (5.48) let us relate Cauchy‚Äôs integral theo-
rem (5.21) to Stokes‚Äôs theorem in the x-y plane. The real and imaginary parts
of a closed contour integral of a function f = u + iv
)
C
f (z) dz =
)
C
(u + iv)(dx + idy) =
)
C
u dx ‚àív dy + i
)
C
v dx + u dy
(5.51)
169

COMPLEX-VARIABLE THEORY
are loop integrals of the functions a = (u, ‚àív, 0) and b = (v, u, 0). By Stokes‚Äôs
theorem, these loop integrals are surface integrals of (‚àá√ó a)z and (‚àá√ó b)z over
the area enclosed by the contour C
)
C
udx ‚àívdy =

C
a ¬∑ (dx, dy, 0) =

S
(‚àá√ó a)z dxdy =

S
‚àívx ‚àíuy dxdy = 0,
)
C
vdx + udy =

C
b ¬∑ (dx, dy, 0) =

S
(‚àá√ó b)z dxdy =

S
ux ‚àívy dxdy = 0
which vanish by the Cauchy‚ÄìRiemann conditions (5.48).
5.5 Harmonic functions
The Cauchy‚ÄìRiemann conditions (5.48) tell us something about the Laplacian
of the real part u of an analytic function f = u+iv. First, the second x-derivative
uxx is uxx = vyx = vxy = ‚àíuyy. So the real part u of an analytic function f is a
harmonic function
uxx + uyy = 0
(5.52)
that is, one with a vanishing Laplacian. Similarly vxx = ‚àíuyx = ‚àívyy, so the
imaginary part of an analytic function f also is a harmonic function
vxx + vyy = 0.
(5.53)
A harmonic function h(x, y) can have saddle points, but not local minima or
maxima because at a local minimum both hxx > 0 and hyy > 0, while at a local
maximum both hxx < 0 and hyy < 0. So in its domain of analyticity, the real
and imaginary parts of an analytic function f have neither minima nor maxima.
For static Ô¨Åelds, the electrostatic potential œÜ(x, y, z) is a harmonic function of
the three spatial variables x, y, and z in regions that are free of charge because
the electric Ô¨Åeld is E = ‚àí‚àáœÜ, and its divergence vanishes ‚àá¬∑ E = 0 where the
charge density is zero. Thus the Laplacian of the electrostatic potential œÜ(x, y, z)
vanishes
‚àá¬∑ ‚àáœÜ = œÜxx + œÜyy + œÜzz = 0
(5.54)
and œÜ(x, y, z) is harmonic where there is no charge. The location of each positive
charge is a local maximum of the electrostatic potential œÜ(x, y, z) and the loca-
tion of each negative charge is a local minimum of œÜ(x, y, z). But in the absence
of charges, the electrostatic potential has neither local maxima nor local min-
ima. Thus one can not trap charged particles with an electrostatic potential, a
result known as Earnshaw‚Äôs theorem.
We have seen (5.52 & 5.53) that the real and imaginary parts of an ana-
lytic function are harmonic functions with two-dimensional gradients that are
mutually perpendicular (5.50). And we know that the electrostatic potential is a
170

5.6 TAYLOR SERIES FOR ANALYTIC FUNCTIONS
harmonic function. Thus the real part u(x, y) (or the imaginary part v(x, y)) of
any analytic function f (z) = u(x, y) + iv(x, y) describes the electrostatic poten-
tial œÜ(x, y) for some electrostatic problem that does not involve the third spatial
coordinate z. The surfaces of constant u(x, y) are the equipotential surfaces, and
since the two gradients are orthogonal, the surfaces of constant v(x, y) are the
electric Ô¨Åeld lines.
Example 5.6 (Two-dimensional potentials)
The function
f (z) = u + iv = E z = E x + i E y
(5.55)
can represent a potential V(x, y, z) = E x for which the electric-Ô¨Åeld lines E =
‚àíE ÀÜx are lines of constant y. It also can represent a potential V(x, y, z) = E y
in which E points in the negative y-direction, which is to say along lines of
constant x.
Another simple example is the function
f (z) = u + iv = z2 = x2 ‚àíy2 + 2ixy
(5.56)
for which u = x2 ‚àíy2 and v = 2xy. This function gives us a potential V(x, y, z)
whose equipotentials are the hyperbolas u = x2 ‚àíy2 = c2 and whose electric-
Ô¨Åeld lines are the perpendicular hyperbolas v = 2xy = d2. Equivalently, we may
take these last hyperbolas 2xy = d2 to be the equipotentials and the other ones
x2 ‚àíy2 = c2 to be the lines of the electric Ô¨Åeld.
For a third example, we write the variable z as z = reiŒ∏ = exp(ln r + iŒ∏) and
use the function
f (z) = u(x, y) + iv(x, y) = ‚àí
Œª
2œÄœµ0
ln z = ‚àí
Œª
2œÄœµ0
(ln r + iŒ∏) ,
(5.57)
which describes the potential V(x, y, z) = ‚àí(Œª/2œÄœµ0) ln

x2 + y2 due to a line of
charge per unit length Œª = q/L. The electric-Ô¨Åeld lines are the lines of constant v
E =
Œª
2œÄœµ0
(x, y, 0)
x2 + y2
(5.58)
or equivalently of constant Œ∏.
5.6 Taylor series for analytic functions
Let‚Äôs consider the contour integral of the function f (z‚Ä≤)/(z‚Ä≤ ‚àíz) along a circle
C inside a simply connected region R in which f (z) is analytic. For any point z
inside the circle, Cauchy‚Äôs integral formula (5.32) tells us that
f (z) =
1
2œÄi
)
C
f (z‚Ä≤)
z‚Ä≤ ‚àíz dz‚Ä≤.
(5.59)
171

COMPLEX-VARIABLE THEORY
We add and subtract the center z0 from the denominator z‚Ä≤ ‚àíz
f (z) =
1
2œÄi
)
C
f (z‚Ä≤)
z‚Ä≤ ‚àíz0 ‚àí(z ‚àíz0) dz‚Ä≤
(5.60)
and then factor the denominator
f (z) =
1
2œÄi
)
C
f (z‚Ä≤)
(z‚Ä≤ ‚àíz0)

1 ‚àíz‚àíz0
z‚Ä≤‚àíz0
 dz‚Ä≤.
(5.61)
From Fig. 5.4, we see that the modulus of the ratio (z ‚àíz0)/(z‚Ä≤ ‚àíz0) is less than
unity, and so the power series

1 ‚àíz ‚àíz0
z‚Ä≤ ‚àíz0
‚àí1
=
‚àû

n=0
 z ‚àíz0
z‚Ä≤ ‚àíz0
n
(5.62)
by (4.25‚Äì4.28) converges absolutely and uniformly on the circle. We therefore
are allowed to integrate the series
f (z) =
1
2œÄi
)
C
f (z‚Ä≤)
z‚Ä≤ ‚àíz0
‚àû

n=0
 z ‚àíz0
z‚Ä≤ ‚àíz0
n
dz‚Ä≤
(5.63)
term by term
f (z) =
‚àû

n=0
(z ‚àíz0)n
1
2œÄi
)
C
f (z‚Ä≤) dz‚Ä≤
(z‚Ä≤ ‚àíz0)n+1 .
(5.64)
>
Taylor-series contour around z0
z0
z
z
Figure 5.4
Contour of integral for the Taylor series (5.64).
172

5.8 LIOUVILLE‚ÄôS THEOREM
By equation (5.36), the integral is just the nth derivative f (n)(z) divided by
n-factorial. Thus the function f (z) possesses the Taylor series
f (z) =
‚àû

n=0
(z ‚àíz0)n
n!
f (n)(z0),
(5.65)
which converges as long as the point z is inside a circle centered at z0 that lies
within a simply connected region R in which f (z) is analytic.
5.7 Cauchy‚Äôs inequality
Suppose a function f (z) is analytic in a region that includes the disk |z| ‚â§R and
that f (z) is bounded by |f (z)| ‚â§M on the circle z = R eiŒ∏ that is the perimeter
of the disk. Then by using Cauchy‚Äôs integral formula (5.36), we may bound the
nth derivative f (n)(0) of f (z) at z = 0 by
|f (n)(0)| ‚â§n!
2œÄ
) |f (z)||dz|
|z|n+1
‚â§n!M
2œÄ
 2œÄ
0
R dŒ∏
Rn+1 = n!M
Rn
(5.66)
which is Cauchy‚Äôs inequality.
5.8 Liouville‚Äôs theorem
Suppose now that f (z) is analytic everywhere (entire) and bounded by
|f (z)| ‚â§M
for all
|z| ‚â•R0.
(5.67)
Then by applying Cauchy‚Äôs inequality (5.66) at successively larger values of R,
we have
|f (n)(0)| ‚â§lim
R‚Üí‚àû
n!M
Rn = 0
for
n ‚â•1,
(5.68)
which shows that every derivative of f (z) vanishes
f (n)(0) = 0
for
n ‚â•1
(5.69)
at z = 0. But then the Taylor series (4.66) about z = 0 for the function f (z)
consists of only a single term, and f (z) is a constant
f (z) =
‚àû

n=0
zn
n! f (n)(0) = f (0)(0) = f (0).
(5.70)
So every bounded entire function is a constant, which is Liouville‚Äôs theorem.
173

COMPLEX-VARIABLE THEORY
5.9 The fundamental theorem of algebra
Gauss applied Liouville‚Äôs theorem to the function
f (z) =
1
PN(z) =
1
c0 + c1z + c2z2 + ¬∑ ¬∑ ¬∑ + cNzN
(5.71)
which is the inverse of an arbitrary polynomial of order N. Suppose that the
polynomial PN(z) had no zero, that is, no root anywhere in the complex plane.
Then f (z) would be analytic everywhere. Moreover, for sufÔ¨Åciently large |z|,
the polynomial PN(z) is approximately PN(z) ‚âàcNzN, and so f (z) would be
bounded by something like
|f (z)| ‚â§
1
|cN|RN
0
‚â°M
for all
|z| ‚â•R0.
(5.72)
So if PN(z) had no root, then the function f (z) would be a bounded entire func-
tion and so would be a constant by Liouville‚Äôs theorem (5.70). But of course,
f (z) = 1/PN(z) is not a constant unless N = 0. Thus any polynomial PN(z) that
is not a constant must have a root, a pole of f (z), so that f (z) is not entire. This
is the only exit from the contradiction.
If the root of PN(z) is at z = z1, then PN(z) = (z ‚àíz1) PN‚àí1(z), in which
PN‚àí1(z) is a polynomial of order N ‚àí1, and we may repeat the argument for
its reciprocal f1(z) = 1/PN‚àí1(z). In this way, one arrives at the fundamental
theorem of algebra: Every polynomial PN(z) = c0 + c1z + ¬∑ ¬∑ ¬∑ + cNzN has N
roots somewhere in the complex plane
PN(z) = cN (z ‚àíz1)(z ‚àíz2) ¬∑ ¬∑ ¬∑ (z ‚àízN).
(5.73)
5.10 Laurent series
Consider a function f (z) that is analytic in a region that contains an outer circle
C1 of radius R1, an inner circle C2 of radius R2, and the annulus between the two
circles as in Fig. 5.5. We will integrate f (z) along a contour C12 that encircles
the point z in a counterclockwise fashion by following C1 counterclockwise and
C2 clockwise and a line joining them in both directions. By Cauchy‚Äôs integral
formula (5.32), this contour integral yields f (z)
f (z) =
1
2œÄi
)
C12
f (z‚Ä≤)
z‚Ä≤ ‚àíz dz‚Ä≤.
(5.74)
The integrations in opposite directions along the line joining C1 and C2 cancel,
and we are left with a counterclockwise integral around the outer circle C1 and
174

5.10 LAURENT SERIES
>
><
<
z0
z
C1
C2
Two contours around z0
z
z
Figure 5.5
The contour consisting of two concentric circles with center at z0 encircles
the point z in a counterclockwise sense. The asterisks are poles or other singularities of
the function f (z).
a clockwise one around C2 or minus a counterclockwise integral around C2
f (z) =
1
2œÄi
)
C1
f (z‚Ä≤)
z‚Ä≤ ‚àíz dz‚Ä≤ ‚àí
1
2œÄi
)
C2
f (z‚Ä≤‚Ä≤)
z‚Ä≤‚Ä≤ ‚àíz dz‚Ä≤‚Ä≤.
(5.75)
Now from the Ô¨Ågure (5.5), the center z0 of the two concentric circles is closer
to the points z‚Ä≤‚Ä≤ on the inner circle C2 than it is to z and also closer to z than to
the points z‚Ä≤ on C1

z‚Ä≤‚Ä≤ ‚àíz0
z ‚àíz0
 < 1
and

z ‚àíz0
z‚Ä≤ ‚àíz0
 < 1.
(5.76)
To use these inequalities, as we did in the series (5.62), we add and subtract z0
from each of the denominators and absorb the minus sign before the second
integral into its denominator
f (z) =
1
2œÄi
)
C1
f (z‚Ä≤)
z‚Ä≤ ‚àíz0 ‚àí(z ‚àíz0) dz‚Ä≤ +
1
2œÄi
)
C2
f (z‚Ä≤‚Ä≤)
z ‚àíz0 ‚àí(z‚Ä≤‚Ä≤ ‚àíz0) dz‚Ä≤‚Ä≤. (5.77)
After factoring the two denominators
f (z) =
1
2œÄi
)
C1
f (z‚Ä≤)
(z‚Ä≤ ‚àíz0) [1 ‚àí(z ‚àíz0)/(z‚Ä≤ ‚àíz0)] dz‚Ä≤
+
1
2œÄi
)
C2
f (z‚Ä≤‚Ä≤)
(z ‚àíz0) [1 ‚àí(z‚Ä≤‚Ä≤ ‚àíz0)/(z ‚àíz0)] dz‚Ä≤‚Ä≤
(5.78)
175

COMPLEX-VARIABLE THEORY
we expand them, as in the series (5.62), in power series that converge absolutely
and uniformly on the two contours
f (z) =
‚àû

n=0
(z ‚àíz0)n
1
2œÄi
)
C1
f (z‚Ä≤)
(z‚Ä≤ ‚àíz0)n+1 dz‚Ä≤
+
‚àû

m=0
1
(z ‚àíz0)m+1
1
2œÄi
)
C2
(z‚Ä≤‚Ä≤ ‚àíz0)m f (z‚Ä≤‚Ä≤) dz‚Ä≤‚Ä≤.
(5.79)
Having removed the point z from the two integrals, we now apply cosmetics.
Since the functions being integrated are analytic between the two circles, we may
shift them to a common counterclockwise (ccw) contour C about any circle of
radius R2 ‚â§R ‚â§R1 between the two circles C1 and C2. Then we set m = ‚àín‚àí1,
or n = ‚àím‚àí1, so as to combine the two sums into one sum on n from ‚àí‚àûto ‚àû
f (z) =
‚àû

n=‚àí‚àû
(z ‚àíz0)n
1
2œÄi
)
C
f (z‚Ä≤)
(z‚Ä≤ ‚àíz0)n+1 dz‚Ä≤.
(5.80)
This Laurent series often is written as
f (z) =
‚àû

n=‚àí‚àû
an(z0) (z ‚àíz0)n
(5.81)
with
an(z0) =
1
2œÄi
)
C
f (z) dz
(z ‚àíz0)n+1
(5.82)
(Pierre Laurent, 1813‚Äì1854). The coefÔ¨Åcient a‚àí1(z0) is called the residue of the
function f (z) at z0. Its signiÔ¨Åcance will be discussed in section 5.13.
Most functions have Laurent series that start at some least integer L
f (z) =
‚àû

n=L
an(z0) (z ‚àíz0)n
(5.83)
rather than at ‚àí‚àû. For such functions, we can pick off the coefÔ¨Åcients an one
by one without doing the integrals (5.82). The Ô¨Årst one aL is the limit
aL(z0) = lim
z‚Üíz0(z ‚àíz0)‚àíLf (z).
(5.84)
The second is given by
aL+1(z0) = lim
z‚Üíz0(z ‚àíz0)‚àíL‚àí1 
f (z) ‚àí(z ‚àíz0)LaL(z0)

.
(5.85)
The third requires two subtractions, and so forth.
176

5.11 SINGULARITIES
5.11 Singularities
A function f (z) that is analytic for all z is called entire or holomorphic. Entire
functions have no singularities, except possibly as |z| ‚Üí‚àû, which is called the
point at inÔ¨Ånity.
A function f (z) has an isolated singularity at z0 if it is analytic in a small disk
about z0 but not analytic that point.
A function f (z) has a pole of order n > 0 at a point z0 if (z ‚àíz0)n f (z) is
analytic at z0 but (z ‚àíz0)n‚àí1 f (z) has an isolated singularity at z0. A pole of
order n = 1 is called a simple pole. Poles are isolated singularities. A function is
meromorphic if it is analytic for all z except for poles.
Example 5.7 (Poles)
The function
f (z) =
n

j=1
1
(z ‚àíj)j
(5.86)
has a pole of order j at z = j for j = 1, 2, . .., n. It is meromorphic.
An essential singularity is a pole of inÔ¨Ånite order. If a function f (z) has an
essential singularity at z0, then its Laurent series (5.80) really runs from n =
‚àí‚àûand not from n = L as in (5.83). Essential singularities are spooky: if a
function f (z) has an essential singularity at w, then inside every disk around
w, f (z) takes on every complex number, with at most one exception, an inÔ¨Ånite
number of times ‚Äì a result due to Picard (1856‚Äì1941).
Example 5.8 (An essential singularity)
The function f (z) = exp(1/z) has an
essential singularity at z = 0 because its Laurent series (5.80)
f (z) = e1/z =
‚àû

m=0
1
m!
1
zm =
0

n=‚àí‚àû
1
|n|! zn
(5.87)
runs from n = ‚àí‚àû. Near z = 0, f (z) = exp(1/z) takes on every complex number
except 0 an inÔ¨Ånite number of times.
Example 5.9 (A meromorphic function with two poles)
The function f (z) =
1/z(z + 1) has poles at z = 0 and at z = ‚àí1 but otherwise is analytic; it is
meromorphic. We may expand it in a Laurent series (5.81‚Äì5.82)
f (z) =
1
z(z + 1) =
‚àû

n=‚àí‚àû
anzn
(5.88)
177

COMPLEX-VARIABLE THEORY
about z = 0 for |z| < 1. The coefÔ¨Åcient an is the integral
an =
1
2œÄi
)
C
dz
z n+2 (z + 1),
(5.89)
in which the contour C is a counterclockwise circle of radius r < 1. Since |z| < 1,
we may expand 1/(1 + z) as the series
1
1 + z =
‚àû

m=0
(‚àíz)m.
(5.90)
Doing the integrals, we Ô¨Ånd
an =
‚àû

m=0
1
2œÄi
)
C
(‚àíz)m dz
zn+2 =
‚àû

m=0
(‚àí1)m rm‚àín‚àí1 Œ¥m,n+1
(5.91)
for n ‚â•‚àí1 and zero otherwise. So the Laurent series for f (z) is
f (z) =
1
z(z + 1) =
‚àû

n=‚àí1
(‚àí1)n+1 zn.
(5.92)
The series starts at n = ‚àí1, not at n = ‚àí‚àû, because f (z) is meromorphic with
only a simple pole at z = 0.
Example 5.10 (The argument principle)
Consider the counterclockwise integral
1
2œÄi
)
C
f (z) g‚Ä≤(z)
g(z) dz
(5.93)
along a contour C that lies inside a simply connected region R in which f (z) is
analytic and g(z) meromorphic. If the function g(z) has a zero or a pole of order
n at w ‚ààR and no other singularity in R,
g(z) = an(w)(z ‚àíw)n
(5.94)
then the ratio g‚Ä≤/g is
g‚Ä≤(z)
g(z) = n(z ‚àíw)n‚àí1
(z ‚àíw)n
=
n
z ‚àíw
(5.95)
and the integral is
1
2œÄi
)
C
f (z) g‚Ä≤(z)
g(z) dz =
1
2œÄi
)
C
f (z)
n
z ‚àíw dz = n f (w).
(5.96)
Any function g(z) meromorphic in R will possess a Laurent series
g(z) =
‚àû

k=n
ak(w)(z ‚àíw)k
(5.97)
about each point w ‚ààR. One may show (exercise 5.18) that as z ‚Üíw the ratio
g‚Ä≤/g again approaches (5.95). It follows that the integral (5.93) is a sum of f (w‚Ñì)
at the zeros of g(z) minus a similar sum at the poles of g(z)
178

5.12 ANALYTIC CONTINUATION
1
2œÄi
)
C
f (z) g‚Ä≤(z)
g(z) dz =

‚Ñì
1
2œÄi
)
C
f (z)
n‚Ñì
z ‚àíw‚Ñì
=

‚Ñì
n‚Ñìf (w‚Ñì)
(5.98)
in which |n‚Ñì| is the multiplicity of the ‚Ñìth zero or pole.
5.12 Analytic continuation
We saw in section 5.6 that a function f (z) that is analytic within a circle of radius
R about a point z0 possesses a Taylor series (5.65)
f (z) =
‚àû

n=0
(z ‚àíz0)n
n!
f (n)(z0)
(5.99)
that converges for all z inside the disk |z ‚àíz0| < R. Suppose z‚Ä≤ is the singularity
of f (z) that is closest to z0. Pick a point z1 in the disk |z ‚àíz0| < R that is not on
the line from z0 to the nearest singularity z‚Ä≤. The function f (z) is analytic at z1
because z1 is within the circle of radius R about the point z0, and so f (z) has a
Taylor series expansion like (5.99) but about the point z1. Usually the circle of
convergence of this power series about z1 will extend beyond the original disk
|z ‚àíz0| < R. If so, the two power series, one about z0 and the other about z1,
deÔ¨Åne the function f (z) and extend its domain of analyticity beyond the original
disk |z‚àíz0| < R. Such an extension of the range of an analytic function is called
analytic continuation.
We often can analytically continue a function more easily than by the
successive construction of Taylor series.
Example 5.11 (The geometric series)
The power series
f (z) =
‚àû

n=0
zn
(5.100)
converges and deÔ¨Ånes an analytic function for |z| < 1. But for such z, we may
sum the series to
f (z) =
1
1 ‚àíz.
(5.101)
By summing the series (5.100), we have analytically continued the function f (z)
to the whole complex plane apart from its simple pole at z = 1.
Example 5.12 (The gamma function)
Euler‚Äôs form of the gamma function is
the integral
(z) =
 ‚àû
0
e‚àít tz‚àí1 dt = (z ‚àí1)!,
(5.102)
179

COMPLEX-VARIABLE THEORY
which makes (z) analytic in the right half-plane Re z > 0. But by successively
using the relation (z + 1) = z (z), we may extend (z) into the left half-plane
(z) = 1
z (z + 1) = 1
z
1
z + 1 (z + 2) = 1
z
1
z + 1
1
z + 2 (z + 3).
(5.103)
The last expression deÔ¨Ånes (z) as a function that is analytic for Re z > ‚àí3 apart
from simple poles at z = 0, ‚àí1, and ‚àí2. Proceeding in this way, we may analyt-
ically continue the gamma function to the whole complex plane apart from the
negative integers and zero. The analytically continued gamma function may be
represented by the formula
(z) = 1
z e‚àíŒ≥ z
 ‚àû

n=1

1 + z
n

e‚àíz/n
‚àí1
(5.104)
due to Weierstrass.
Example 5.13 (Dimensional regularization)
The loop diagrams of quantum
Ô¨Åeld theory involve badly divergent integrals like
I(4) =

d4q
(2œÄ)4

q2a

q2 + Œ±2b
(5.105)
where often a = 0, b = 2, and Œ±2 > 0. Gerardus ‚Äôt Hooft (1946‚Äì) and Martinus
J. G. Veltman (1931‚Äì) promoted the number of space-time dimensions from four
to a complex number d. The resulting integral has the value (Srednicki, 2007,
p. 102)
I(d) =

ddq
(2œÄ)d

q2a

q2 + Œ±2b = (b ‚àía ‚àíd/2) (a + d/2)
(4œÄ)d/2 (b) (d/2)
1
(Œ±2)b‚àía‚àíd/2
(5.106)
and so deÔ¨Ånes a function of the complex variable d that is analytic everywhere
except for simple poles at d = 2(n ‚àía + b) where n = 0, 1, 2, . . . , ‚àû. At these
poles, the formula
(‚àín + z) = (‚àí1)n
n!

1
z ‚àíŒ≥ +
n

k=1
1
k + O(z)

(5.107)
where Œ≥ = 0.5772... is the Euler‚ÄìMascheroni constant (4.8) can be useful.
5.13 The calculus of residues
A contour integral of an analytic function f (z) does not change unless the
endpoints move or the contour crosses a singularity or leaves the region of
analyticity (section 5.2). Let us consider the integral of a function f (z) along
a counterclockwise contour C that encircles n poles at zk for k = 1, . . . , n in a
simply connected region R in which f (z) is meromorphic. We may shrink the
180

5.13 THE CALCULUS OF RESIDUES
area within the contour C without changing the value of the integral until the
area is inÔ¨Ånitesimal and the contour is the sum of n tiny counterclockwise circles
Ck around the n poles
)
C
f (z) dz =
n

k=1
)
Ck
f (z) dz.
(5.108)
These tiny counterclockwise integrals around the poles at zi are the residues
a‚àí1(zi) deÔ¨Åned by (5.82) for n = ‚àí1 apart from the factor 2œÄi. So the whole
ccw integral is 2œÄi times the sum of the residues of the function f (z) at the
enclosed poles
)
C
f (z) dz = 2œÄi
n

k=1
a‚àí1(zk),
(5.109)
a result known as the residue theorem.
In general, one must do each tiny ccw integral about each pole zi, but simple
poles are an important special case. If w is a simple pole of the function f (z),
then near it f (z) is given by its Laurent series (5.81) as
f (z) = a‚àí1(w)
z ‚àíw +
‚àû

n=0
an(w) (z ‚àíw)n.
(5.110)
In this case, its residue is by (5.84) with L = ‚àí1
a‚àí1(w) = lim
z‚Üíw (z ‚àíw) f (z),
(5.111)
which usually is easier to do than the integral (5.82)
a‚àí1(w) =
1
2œÄi
)
C
f (z)dz.
(5.112)
Example 5.14 (Cauchy‚Äôs integral formula)
Suppose the function f (z) is analytic
within a region R and that C is a ccw contour that encircles a point w ‚ààR. Then
the ccw contour C encircles the simple pole at w of the function f (z)/(z ‚àíw),
which is its only singularity in R. By applying the residue theorem and formula
(5.111) for the residue a‚àí1(w) of the function f (z)/(z ‚àíw), we Ô¨Ånd
)
C
f (z)
z ‚àíw dz = 2œÄi a‚àí1(w) = 2œÄi lim
z‚Üíw (z ‚àíw) f (z)
z ‚àíw = 2œÄi f (w).
(5.113)
So Cauchy‚Äôs integral formula (5.32) is an example of the calculus of residues.
Example 5.15 (A meromorphic function)
By the residue theorem (5.109), the
integral of the function
f (z) =
1
z ‚àí1
1
(z ‚àí2)2
(5.114)
181

COMPLEX-VARIABLE THEORY
along the circle C = 4eiŒ∏ for 0 ‚â§Œ∏ ‚â§2œÄ is the sum of the residues at z = 1
and z = 2
)
C
f (z) dz = 2œÄi [a‚àí1(1) + a‚àí1(2)] .
(5.115)
The function f (z) has a simple pole at z = 1, and so we may use the formula
(5.111) to evaluate the residue a‚àí1(1) as
a‚àí1(1) = lim
z‚Üí1 (z ‚àí1) f (z) = lim
z‚Üí1
1
(z ‚àí2)2 = 1
(5.116)
instead of using Cauchy‚Äôs integral formula (5.32) to do the integral of f (z) along
a tiny circle about z = 1, which gives the same result
a‚àí1(1) =
1
2œÄi
)
dz
z ‚àí1
1
(z ‚àí2)2 =
1
(1 ‚àí2)2 = 1.
(5.117)
The residue a‚àí1(2) is the integral of f (z) along a tiny circle about z = 2, which
we do by using Cauchy‚Äôs integral formula (5.34)
a‚àí1(2) =
1
2œÄi
)
dz
(z ‚àí2)2
1
z ‚àí1 = d
dz
1
z ‚àí1

z=2
= ‚àí
1
(2 ‚àí1)2 = ‚àí1.
(5.118)
The sum of these two residues is zero, and so the integral (5.115) vanishes.
Another way of evaluating this integral is to deform it, not into two tiny circles
about the two poles, but rather into a huge circle z = ReiŒ∏ and to notice that as
R ‚Üí‚àûthe modulus of this integral vanishes

)
f (z) dz
 ‚âà2œÄ
R2 ‚Üí0.
(5.119)
This contour is an example of a ghost contour.
5.14 Ghost contours
Often one needs to do an integral that is not a closed counterclockwise con-
tour. Integrals along the real axis occur frequently. One sometimes can convert
a line integral into a closed contour by adding a contour along which the inte-
gral vanishes, a ghost contour. We have just seen an example (5.119) of a ghost
contour, and we shall see more of them in what follows.
Example 5.16 (Best case)
Consider the integral
I =
 ‚àû
‚àí‚àû
1
(x ‚àíi)(x ‚àí2i)(x ‚àí3i) dx.
(5.120)
We could do the integral by adding a contour ReiŒ∏ from Œ∏ = 0 to Œ∏ = œÄ. In
the limit R ‚Üí‚àû, the integral of 1/[(z ‚àíi)(z ‚àí2i)(z ‚àí3i)] along this contour
vanishes; it is a ghost contour. The original integral I and the ghost contour
182

5.14 GHOST CONTOURS
encircle the three poles, and so we could compute I by evaluating the residues at
those poles. But we also could add a ghost contour around the lower half-plane.
This contour and the real line encircle no poles. So we get I = 0 without doing
any work at all.
Example 5.17 (Fourier transform of a gaussian)
During our computation of
the Fourier transform of a gaussian (3.15‚Äì3.18), we promised to justify the shift
in the variable of integration from x to x + ik/2m2 in this chapter. So let us
consider the contour integral of the entire function f (z) = exp(‚àím2z2) over the
rectangular closed contour along the real axis from ‚àíR to R and then from
z = R to z = R + ic and then from there to z = ‚àíR + ic and then to z = ‚àíR.
Since the f (z) is analytic within the contour, the integral is zero
)
dze‚àím2z2 =
 R
‚àíR
e‚àím2z2dz +
 R+ic
R
e‚àím2z2dz +
 ‚àíR+ic
R+ic
e‚àím2z2dz +
 ‚àíR
‚àíR+ic
e‚àím2z2dz = 0
for all Ô¨Ånite positive values of R and so also in the limit R ‚Üí‚àû. The two
contours in the imaginary direction are of length c and are damped by the factor
exp(‚àím2R2), and so they vanish in the limit R ‚Üí‚àû. They are ghost contours.
It follows then from this last equation in the limit R ‚Üí‚àûthat
 ‚àû
‚àí‚àû
dx e‚àím2(x+ic)2 =
 ‚àû
‚àí‚àû
dx e‚àím2x2 =
‚àöœÄ
m ,
(5.121)
which is the promised result.
It implies (exercise 5.20) that
 ‚àû
‚àí‚àû
dx e‚àím2(x+z)2 =
 ‚àû
‚àí‚àû
dx e‚àím2x2 =
‚àöœÄ
m
(5.122)
for m > 0 and arbitrary complex z.
Example 5.18 (A cosine integral)
To compute the integral
I =
 ‚àû
0
cos x
q2 + x2 dx
(5.123)
we use the evenness of the integrand to extend the integration
I = 1
2
 ‚àû
‚àí‚àû
cos x
q2 + x2 dx,
(5.124)
write the cosine as [exp(ix) + exp(‚àíix)]/2, and factor the denominator
I = 1
4
 ‚àû
‚àí‚àû
eix
(x ‚àíiq)(x + iq) dx + 1
4
 ‚àû
‚àí‚àû
e‚àíix
(x ‚àíiq)(x + iq) dx.
(5.125)
We promote x to a complex variable z and add the contours z = ReiŒ∏ and z =
Re‚àíiŒ∏ as Œ∏ goes from 0 to œÄ respectively to the Ô¨Årst and second integrals. The
term exp(iz)dz/(q2+z2) = exp(iR cos Œ∏‚àíR sin Œ∏)iReiŒ∏dŒ∏/(q2+R2e2iŒ∏) vanishes in
183

COMPLEX-VARIABLE THEORY
the limit R ‚Üí‚àû, so the Ô¨Årst contour is a ccw ghost contour. A similar argument
applies to the second (clockwise) contour, and we have
I = 1
4
)
eiz
(z ‚àíiq)(z + iq) dz + 1
4
)
e‚àíiz
(z ‚àíiq)(z + iq) dz.
(5.126)
The Ô¨Årst integral picks up the pole at iq and the second the pole at ‚àíiq
I = iœÄ
2
e‚àíq
2iq + e‚àíq
2iq

= œÄe‚àíq
2q .
(5.127)
So the value of the integral is œÄe‚àíq/2q.
Example 5.19 (Third-harmonic microscopy)
An ultra-short laser pulse
intensely focused in a medium generates a third-harmonic electric Ô¨Åeld E3 in
the forward direction proportional to the integral (Boyd, 2000)
E3 ‚àùœá(3) E3
0
 ‚àû
‚àí‚àû
ei k z
dz
(1 + 2iz/b)2
(5.128)
along the axis of the beam as in Fig. 5.6. Here b = 2œÄt2
0n/Œª = kt2
0 in which
n = n(œâ) is the index of refraction of the medium, Œª is the wave-length of the
laser light in the medium, and t0 is the transverse or waist radius of the gaussian
beam, deÔ¨Åned by E(r) = E exp(‚àír2/t2
0).
When the dispersion is normal, that is when dn(œâ)/dœâ > 0, the shift in the
wave vector k = 3œâ[n(œâ) ‚àín(3œâ)]/c is negative. Since k < 0, the exponential
is damped when z = x + iy is in the lower half-plane (LHP)
ei k z = ei k (x+iy) = ei k x e‚àík y.
(5.129)
So as we did in example 5.18, we will add a contour around the lower half-plane
(z = R eiŒ∏, œÄ ‚â§Œ∏ ‚â§2œÄ, and dz = iReiŒ∏dŒ∏) because in the limit R ‚Üí‚àû, the
integral along it vanishes; it is a ghost contour.
The function f (z) = exp(i k z)/(1 + 2iz/b)2 has a double pole at z = ib/2,
which is in the UHP since the length b > 0, but no singularity in the LHP y < 0.
So the integral of f (z) along the closed contour from z = ‚àíR to z = R and then
along the ghost contour vanishes. But since the integral along the ghost contour
vanishes, so does the integral from ‚àíR to R. Thus when the dispersion is normal,
the third-harmonic signal vanishes, E3 = 0, as long as the medium effectively
extends from ‚àí‚àûto ‚àûso that its edges are in the unfocused region like the
dotted lines of Fig. 5.6. But an edge in the focused region like the solid line of
the Ô¨Ågure does make a third-harmonic signal E3. Third-harmonic microscopy
lets us see edges or features instead of background.
Example 5.20 (Green and Bessel)
Let us evaluate the integral
I(x) =
 ‚àû
‚àí‚àû
dk
eikx
k2 + m2 ,
(5.130)
184

5.14 GHOST CONTOURS
Third-harmonic microscopy
‚àíL
L
visible
unseen
unseen
Figure 5.6
In the limit in which the distance L is much larger than the wave-length
Œª, the integral (5.128) is nonzero when an edge (solid line) lies where the beam is
focused but not when a feature (dots) lies where the beam is not focused. Only
features within the focused region are visible.
which is the Fourier transform of the function 1/(k2 + m2). If x > 0, then the
exponential deceases with k in the upper half-plane. So as in example 5.18, the
semicircular contour k = R eiŒ∏, 0 ‚â§Œ∏ ‚â§œÄ, and dk = iReiŒ∏dŒ∏ is a ghost contour.
So if x > 0, then we can add this contour to the integral I(x) without changing
it. Thus I(x) is equal to the closed contour integral along the real axis and the
semicircular ghost contour
I(x) =
)
dk
eikx
k2 + m2 =
)
dk
eikx
(k + im)(k ‚àíim).
(5.131)
This closed contour encircles the simple pole at k = im and no other singularity,
and so we may shrink the contour into a tiny circle around the pole. Along that
tiny circle, the function eikx/(k + im) is simply e‚àímx/2im, and so
I(x) = e‚àímx
2im
)
dk
k ‚àíim = 2œÄi e‚àímx
2im = œÄe‚àímx
m
for
x > 0.
(5.132)
Similarly, if x < 0, we can add the semicircular ghost contour k = R eiŒ∏,
œÄ ‚â§Œ∏ ‚â§2œÄ, dk = iReiŒ∏dŒ∏ with k running around the perimeter of the lower
half-plane. So if x < 0, then we can write the integral I(x) as a shrunken closed
contour that runs clockwise around the pole at k = ‚àíim
185

COMPLEX-VARIABLE THEORY
I(x) =
emx
‚àí2im
)
dk
k + im = ‚àí2œÄi emx
‚àí2im = œÄemx
m
for
x < 0.
(5.133)
We combine the two cases (5.132) and (5.133) into the result
 ‚àû
‚àí‚àû
dk
eikx
k2 + m2 = œÄ
m e‚àím|x|.
(5.134)
We can use this formula to develop an expression for the Green‚Äôs function of
the Laplacian in cylindrical coordinates. Setting x‚Ä≤ = 0 and r = |x| =

œÅ2 + z2
in the Coulomb Green‚Äôs function (3.110), we have
G(r) =
1
4œÄr =
1
4œÄ

œÅ2 + z2 =

d3k
(2œÄ)3
1
k2 eik¬∑x.
(5.135)
The integral over the z-component of k is (5.134) with m2 = k2
x + k2
y ‚â°k2
 ‚àû
‚àí‚àû
dkz
eikzz
k2z + k2 = œÄ
k e‚àík|z|.
(5.136)
So with kxx + kyy ‚â°kœÅ cos œÜ, the Green‚Äôs function is
1
4œÄ

œÅ2 + z2 =
 ‚àû
0
œÄdk
(2œÄ)3
 2œÄ
0
dœÜ eikœÅ cos œÜ e‚àík|z|.
(5.137)
The œÜ integral is a representation (5.46, 9.7) of the Bessel function J0(kœÅ)
J0(kœÅ) =
 2œÄ
0
dœÜ
2œÄ eikœÅ cos œÜ.
(5.138)
Thus we arrive at Bessel‚Äôs formula for the Coulomb Green‚Äôs function
1
4œÄ

œÅ2 + z2 =
 ‚àû
0
dk
4œÄ J0(kœÅ) e‚àík|z|
(5.139)
in cylindrical coordinates (Schwinger et al., 1998, p. 166).
Example 5.21 (Yukawa and Green)
We saw in example 3.11 that the Green‚Äôs
function for Yukawa‚Äôs differential operator (3.123) is
GY(x) =

d3k
(2œÄ)3
eik¬∑x
k2 + m2 .
(5.140)
Letting k ¬∑ x = kr cos Œ∏ in which r = |x|, we Ô¨Ånd
GY(r) =
 ‚àû
0
k2dk
(2œÄ)2
 1
‚àí1
eikr cos Œ∏
k2 + m2 d cos Œ∏ = 1
ir
 ‚àû
0
dk
(2œÄ)2
k
k2 + m2

eikr ‚àíe‚àíikr
= 1
ir
 ‚àû
‚àí‚àû
dk
(2œÄ)2
k
k2 + m2 eikr = 1
ir
 ‚àû
‚àí‚àû
dk
(2œÄ)2
k
(k ‚àíim)(k + im) eikr.
186

5.14 GHOST CONTOURS
We add a ghost contour that loops over the upper half-plane and get
GY(r) =
2œÄi
(2œÄ)2ir
im
2im e‚àímr = e‚àímr
4œÄr ,
(5.141)
which Yukawa proposed as the potential between two hadrons due to the
exchange of a particle of mass m, the pion. Because the mass of the pion is
140 MeV, the range of the Yukawa potential is ¬Øh/mc = 1.4 √ó 10‚àí15 m.
Example 5.22 (The Green‚Äôs function for the Laplacian in n dimensions)
The
Green‚Äôs function for the Laplacian ‚àí‚ñ≥G(x) = Œ¥(n)(x) is
G(x) =

1
k2 eik¬∑x dnk
(2œÄ)n
(5.142)
in n dimensions. We use the formula
1
k2 =
 ‚àû
0
e‚àíŒªk2 dŒª
(5.143)
to write it as a gaussian integral
G(x) =

e‚àíŒªk2+ik¬∑x dŒª dnk
(2œÄ)n .
(5.144)
We now complete the square in the exponent
‚àíŒªk2 + ik ¬∑ x = ‚àíŒª (k ‚àíix/2Œª)2 ‚àíx2/4Œª
(5.145)
and use our gaussian formula (5.121) to write the Green‚Äôs function as
G(x) =
 ‚àû
0
dŒª
 dnk
(2œÄ)n e‚àíx2/4Œªe‚àíŒª(k‚àíix/2Œª)2 =
 ‚àû
0
dŒª
 dnk
(2œÄ)n e‚àíx2/4Œªe‚àíŒª k2
=
 ‚àû
0
e‚àíx2/4Œª
dŒª
(4œÄŒª)n/2 = (x2)1‚àín/2
4œÄn/2
 ‚àû
0
e‚àíŒ±Œ±n/2‚àí2dŒ±
=
(n/2 ‚àí1)
4œÄn/2(x2)(n/2‚àí1) .
(5.146)
Since (1/2) = ‚àöœÄ, this formula for n = 3 gives G(x) = 1/4œÄ|x|, which is
(3.110); since (1) = 1, it gives
G(x) =
1
4œÄ2x2
(5.147)
for n = 4.
Example 5.23 (The Yukawa Green‚Äôs function in n dimensions)
The Yukawa
Green‚Äôs function which satisÔ¨Åes (‚àí‚ñ≥+ m2)G(x) = Œ¥(n)(x) in n dimensions is the
integral (5.142) with k2 replaced by k2 + m2
G(x) =

1
k2 + m2 eik¬∑x dnk
(2œÄ)n .
(5.148)
187

COMPLEX-VARIABLE THEORY
Using the integral formula (5.143), we write it as a gaussian integral
G(x) =

e‚àíŒª(k2+m2)+ik¬∑x dŒªdnk
(2œÄ)n .
(5.149)
Completing the square as in (5.145), we have
G(x) =

e‚àíx2/4Œªe‚àíŒª(k‚àíix/2Œª)2‚àíŒªm2 dŒªdnk
(2œÄ)n =

e‚àíx2/4Œªe‚àíŒª (k2+m2) dŒªdnk
(2œÄ)n
=
 ‚àû
0
e‚àíx2/4Œª‚àíŒªm2
dŒª
(4œÄŒª)n/2 .
(5.150)
We can relate this to a Bessel function by setting Œª = |x|/2m exp(‚àíŒ±)
G(x) =
1
(4œÄ)n/2
2m
x
(n/2‚àí1)  ‚àû
‚àí‚àû
e‚àímx cosh Œ±+(n/2‚àí1)Œ± dŒ±
=
2
(4œÄ)n/2
2m
x
(n/2‚àí1)  ‚àû
0
e‚àímx cosh Œ± cosh(n/2 ‚àí1)Œ± dŒ±
=
2
(4œÄ)n/2
2m
x
(n/2‚àí1)
Kn/2‚àí1(mx)
(5.151)
where x = |x| =
‚àö
x2 and K is a modiÔ¨Åed Bessel function of the second kind
(9.98). If n = 3, this is (exercise 5.27) the Yukawa potential (5.141).
Example 5.24 (A Fourier transform)
As another example, let‚Äôs consider the
integral
J(x) =
 ‚àû
‚àí‚àû
eikx
(k2 + m2)2 dk.
(5.152)
We may add ghost contours as in the preceding example, but now the integrand
has double poles at k = ¬±im, and so we must use Cauchy‚Äôs integral formula
(5.36) for the case of n = 1, which is (5.34). For x > 0, we add a ghost contour
in the UHP and Ô¨Ånd
J(x) =
)
eikx
(k + im)2(k ‚àíim)2 dk = 2œÄi d
dk
eikx
(k + im)2

k=im
=
œÄ
2m2

x + 1
m

e‚àímx.
(5.153)
If x < 0, then we add a ghost contour in the LHP and Ô¨Ånd
J(x) =
)
eikx
(k + im)2(k ‚àíim)2 dk = ‚àí2œÄi d
dk
eikx
(k ‚àíim)2

k=‚àíim
=
œÄ
2m2

‚àíx + 1
m

emx.
(5.154)
188

5.14 GHOST CONTOURS
Putting the two together, we get
J(x) =
 ‚àû
‚àí‚àû
eikx
(k2 + m2)2 dk =
œÄ
2m2

|x| + 1
m

e‚àím|x|
(5.155)
as the Fourier transform of 1/(k2 + m2)2.
Example 5.25 (Integral of a complex gaussian)
As another example of the use
of ghost contours, let us use one to do the integral
I =
 ‚àû
‚àí‚àû
ewx2 dx,
(5.156)
in which the real part of the nonzero complex number w = u + iv = œÅeiœÜ is
negative or zero
u ‚â§0
‚áê‚áí
œÄ
2 ‚â§œÜ ‚â§3œÄ
2 .
(5.157)
We Ô¨Årst write the integral I as twice that along half the x-axis
I = 2
 ‚àû
0
ewx2 dx.
(5.158)
If we promote x to a complex variable z = reiŒ∏, then wz2 will be negative if
œÜ + 2Œ∏ = œÄ, that is, if Œ∏ = (œÄ ‚àíœÜ) /2 where in view of (5.157) Œ∏ lies in the
interval ‚àíœÄ/4 ‚â§Œ∏ ‚â§œÄ/4.
The closed pie-shaped contour of Fig. 5.7 (down the real axis from z = 0 to
z = R, along the arc z = R exp(iŒ∏‚Ä≤) as Œ∏‚Ä≤ goes from 0 to Œ∏, and then down the
line z = r exp(iŒ∏) from z = R exp(iŒ∏) to z = 0) encloses no singularities of the
function f (z) = exp(wz2). Hence the integral of exp(wz2) along that contour
vanishes.
To show that the arc is a ghost contour, we bound it by

 Œ∏
0
e(u+iv)R2e2iŒ∏‚Ä≤
R dŒ∏‚Ä≤
 ‚â§
 Œ∏
0
exp

uR2 cos 2Œ∏‚Ä≤ ‚àívR2 sin 2Œ∏‚Ä≤
R dŒ∏‚Ä≤
‚â§
 Œ∏
0
e‚àívR2 sin 2Œ∏‚Ä≤R dŒ∏‚Ä≤.
(5.159)
Here v sin 2Œ∏‚Ä≤ ‚â•0, and so if v is positive, then so is Œ∏‚Ä≤. Then 0 ‚â§Œ∏‚Ä≤ ‚â§œÄ/4, and
so sin(2Œ∏‚Ä≤) ‚â•4Œ∏‚Ä≤/œÄ. Thus since u < 0, we have the upper bound

 Œ∏
0
e(u+iv)R2e2iŒ∏‚Ä≤
R dŒ∏‚Ä≤
 ‚â§
 Œ∏
0
e‚àí4vR2Œ∏‚Ä≤/œÄR dŒ∏‚Ä≤ = œÄ(e‚àí4vR2Œ∏‚Ä≤/œÄ ‚àí1)
4vR
,
(5.160)
which vanishes in the limit R ‚Üí‚àû. (If v is negative, then so is Œ∏‚Ä≤, the pie-shaped
contour is in the fourth quadrant, sin(2Œ∏‚Ä≤) ‚â§4Œ∏‚Ä≤/œÄ, and the inequality (5.160)
holds with absolute-value signs around the second integral.)
Since by Cauchy‚Äôs integral theorem (5.22) the integral along the pie-shaped
contour of Fig. 5.7 vanishes, it follows that
189

COMPLEX-VARIABLE THEORY
Pie-shaped contour
>
Œ∏
z = 0
z = R
z = R eiŒ∏
Figure 5.7
The integral of the entire function exp(wz2) along the pie-shaped
closed contour vanishes by Cauchy‚Äôs theorem.
1
2I +
 0
ReiŒ∏ ewz2dz = 0.
(5.161)
But the choice Œ∏ = (œÄ ‚àíœÜ) /2 implies that on the line z = r exp(iŒ∏) the quantity
wz2 is negative, wz2 = ‚àíœÅr2. Thus with dz = exp(iŒ∏)dr, we have
I = 2
 ReiŒ∏
0
ewz2 dz = 2eiŒ∏
 R
0
e‚àíœÅr2 dr
(5.162)
so that as R ‚Üí‚àû
I = 2eiŒ∏
 ‚àû
0
e‚àíœÅr2 dr = eiŒ∏
&œÄ
œÅ =
&
œÄ
œÅe‚àí2iŒ∏ .
(5.163)
Finally, from Œ∏ = (œÄ ‚àíœÜ) /2 and w = œÅ exp(iœÜ), we Ô¨Ånd that for Re w ‚â§0
 ‚àû
‚àí‚àû
ewx2 dx =
& œÄ
‚àíw
(5.164)
as long as w Ã∏= 0. Shifting x by a complex number b, we still have
 ‚àû
‚àí‚àû
ew(x‚àíb)2 dx =
& œÄ
‚àíw
(5.165)
190

5.14 GHOST CONTOURS
as long as Re w < 0. If w = ia Ã∏= 0 and b is real, then
 ‚àû
‚àí‚àû
eia(x‚àíb)2 dx =
&
iœÄ
a .
(5.166)
The simpler integral (5.122) applies when m > 0 and z is an arbitrary complex
number
 ‚àû
‚àí‚àû
e‚àím2(x+z)2 dx =
‚àöœÄ
m .
(5.167)
These last two formulas are used in chapter 16 on path integrals.
Let us try to express the line integral of a not necessarily analytic function
f (x, y) = u(x, y) + iv(x, y) along a closed ccw contour C as an integral over the
surface enclosed by the contour. The contour integral is
)
C
(u + iv)(dx + idy) =
)
C
(u dx ‚àív dy) + i
)
C
(v dx + u dy).
(5.168)
Now since the contour C is counterclockwise, the differential dx is negative
at the top of the curve with coordinates (x, y+(x)) and positive at the bottom
(x, y‚àí(x)). So the Ô¨Årst line integral is the surface integral
)
C
u dx =

[u(x, y‚àí(x)) ‚àíu(x, y+(x))] dx
= ‚àí
  y+(x)
y‚àí(x)
uy(x, y)dy

dx
= ‚àí

uy |dxdy| = ‚àí

uy da,
(5.169)
in which da = |dxdy| is a positive element of area. Similarly, we Ô¨Ånd
i
)
C
v dx = ‚àíi

vy |dxdy| = ‚àíi

vy da.
(5.170)
The dy integrals are then
‚àí
)
C
v dy = ‚àí

vx |dxdy| = ‚àí

vx da,
(5.171)
i
)
C
u dy = i

ux |dxdy| = i

ux da.
(5.172)
Combining (5.168‚Äì5.172), we Ô¨Ånd
)
C
(u + iv)(dx + idy) = ‚àí

(uy + vx) da + i

(‚àívy + ux) da.
(5.173)
This formula holds whether or not the function f (x, y) is analytic. But if f (x, y)
is analytic on and within the contour C, then it satisÔ¨Åes the Cauchy‚ÄìRiemann
191

COMPLEX-VARIABLE THEORY
conditions (5.48) within the contour, and so both surface integrals vanish. The
contour integral then is zero, which is Cauchy‚Äôs integral theorem (5.21).
The contour integral of the function f (x, y) = u(x, y) + iv(x, y) differs from
zero (its value if f (x, y) is analytic in z = x + iy) by the surface integrals of
uy + vx and ux ‚àívy

)
C
f (z)dz

2
=

)
C
(u + iv)(dx + idy)

2
=


(uy + vx)da

2
+


(ux ‚àívy)da

2
,
(5.174)
which vanish when f = u + iv satisÔ¨Åes the Cauchy‚ÄìRiemann conditions (5.48).
Example 5.26 (The integral of a nonanalytic function)
The integral formula
(5.173) can help us evaluate contour integrals of functions that are not analytic.
The function
f (x, y) =
1
x + iy + iœµ
1
1 + x2 + y2
(5.175)
is the product of an analytic function 1/(z+iœµ), where œµ is tiny and positive, and
a nonanalytic real one r(x, y) = 1/(1 + z‚àóz). The iœµ pushes the pole in u + iv =
1/(z + iœµ) into the lower half-plane. The real and imaginary parts of f are
U(x, y) = u(x, y) r(x, y) =
x
x2 + (y + œµ)2
1
1 + x2 + y2
(5.176)
and
V(x, y) = v(x, y) r(x, y) =
‚àíy ‚àíœµ
x2 + (y + œµ)2
1
1 + x2 + y2 .
(5.177)
We will use (5.173) to compute the contour integral I of f along the real axis
from ‚àí‚àûto ‚àûand then along the ghost contour z = x+iy = ReiŒ∏ for 0 ‚â§Œ∏ ‚â§œÄ
and R ‚Üí‚àûaround the upper half-plane
I =
)
f (x, y) dz =
 ‚àû
‚àí‚àû
dx
 ‚àû
0
dy

‚àíUy ‚àíVx + i

‚àíVy + Ux

.
(5.178)
Since u and v satisfy the Cauchy‚ÄìRiemann conditions (5.48), the terms in the
area integral simplify to ‚àíUy ‚àíVx = ‚àíury ‚àívrx and ‚àíVy + Ux = ‚àívry + urx.
So the integral I is
I =
 ‚àû
‚àí‚àû
dx
 ‚àû
0
dy

‚àíury ‚àívrx + i(‚àívry + urx)

(5.179)
or explicitly
I =
 ‚àû
‚àí‚àû
dx
 ‚àû
0
dy
‚àí2œµx ‚àí2i(x2 + y2 + œµy)

x2 + (y + œµ)2 
1 + x2 + y22 .
(5.180)
192

5.15 LOGARITHMS AND CUTS
We let œµ ‚Üí0 and Ô¨Ånd
I = ‚àí2i
 ‚àû
‚àí‚àû
dx
 ‚àû
0
dy
1

1 + x2 + y22 .
(5.181)
Changing variables to œÅ2 = x2 + y2, we have
I = ‚àí4œÄi
 ‚àû
0
dœÅ
œÅ
(1 + œÅ2)2 = 2œÄi
 ‚àû
0
dœÅ d
dœÅ
1
1 + œÅ2 = ‚àí2œÄi,
(5.182)
which is simpler than evaluating the integral (5.178) directly.
5.15 Logarithms and cuts
By deÔ¨Ånition, a function f is single valued; it maps every number z in its domain
into a unique image f (z). A function that maps only one number z in its domain
into each f (z) in its range is said to be one to one. A one-to-one function f (z)
has a well-deÔ¨Åned inverse function f ‚àí1(z).
The exponential function is one to one when restricted to the real num-
bers. It maps every real number x into a positive number exp(x). It has an
inverse function ln(x) that maps every positive number exp(x) back into x. But
the exponential function is not one to one on the complex numbers because
exp(z + 2œÄni) = exp(z) for every integer n. The exponential function is many to
one. Thus on the complex numbers, the exponential function has no inverse
function. Its would-be inverse function ln(exp(z)) is z + 2œÄni, which is not
unique. It has in it an arbitrary integer n.
In other words, when exponentiated, the logarithm of a complex number z
returns exp(ln z) = z. So if z = r exp(iŒ∏), then a suitable logarithm is ln z =
ln r + iŒ∏. But what is Œ∏? In the polar representation of z, the argument Œ∏ can
just as well be Œ∏ + 2œÄn because both give z = r exp(iŒ∏) = r exp(iŒ∏ + i2œÄn). So
ln r + iŒ∏ + i2œÄn is a correct value for ln[r exp(iŒ∏)] for every integer n.
People usually want one of the correct values of a logarithm, rather than all
of them. Two conventions are common. In the Ô¨Årst convention, the angle Œ∏ is
zero along the positive real axis and increases continuously as the point z moves
counterclockwise around the origin, until at points just below the positive real
axis, Œ∏ = 2œÄ ‚àíœµ is slightly less than 2œÄ. In this convention, the value of Œ∏
drops by 2œÄ as one crosses the positive real axis moving counterclockwise. This
discontinuity on the positive real axis is called a cut.
The second common convention puts the cut on the negative real axis. Here
the value of Œ∏ is the same as in the Ô¨Årst convention when the point z is in the
upper half-plane. But in the lower half-plane, Œ∏ decreases from 0 to ‚àíœÄ as the
point z moves clockwise from the positive real axis to just below the negative
real axis, where Œ∏ = ‚àíœÄ + œµ. As one crosses the negative real axis moving
193

COMPLEX-VARIABLE THEORY
clockwise or up, Œ∏ jumps by 2œÄ while crossing the cut. The two conventions
agree in the upper half-plane but differ by 2œÄ in the lower half-plane.
Sometimes it is convenient to place the cut on the positive or negative imag-
inary axis ‚Äì or along a line that makes an arbitrary angle with the real axis. In
any particular calculation, we are at liberty to deÔ¨Åne the polar angle Œ∏ by plac-
ing the cut anywhere we like, but we must not change from one convention to
another in the same computation.
5.16 Powers and roots
The logarithm is the key to many other functions to which it passes its
arbitrariness. For instance, any power a of z = r exp(iŒ∏) is deÔ¨Åned as
za = exp (a ln z) = exp [a (ln r + iŒ∏ + i2œÄn)] = ra eiaŒ∏ ei2œÄna.
(5.183)
So za is not unique unless a is an integer. The square-root, for example,
‚àöz = exp

1
2(ln r + iŒ∏ + i2œÄn)]

= ‚àör eiŒ∏/2 einœÄ = (‚àí1)n ‚àör eiŒ∏/2
(5.184)
changes sign when we change Œ∏ by 2œÄ as we cross a cut. The mth root
m‚àöz = z1/m = exp
ln z
m

(5.185)
changes by exp(¬±2œÄi/m) when we cross a cut and change Œ∏ by 2œÄ. And when
a = u + iv is a complex number, za is
za = ea ln z = e(u+iv)(ln r+iŒ∏+i2œÄn) = ru+iv e(‚àív+iu)(Œ∏+2œÄn),
(5.186)
which changes by exp[2œÄ(‚àív + iu)] as we cross a cut.
Example 5.27 (ii)
The number i = exp(iœÄ/2 + i2œÄn) for any integer n. So the
general value of ii is ii = exp[i(iœÄ/2 + i2œÄn)] = exp(‚àíœÄ/2 ‚àí2œÄn).
One can deÔ¨Åne a sequence of mth-root functions

z1/m
n = exp
ln r + i(Œ∏ + 2œÄn)
m

,
(5.187)
one for each integer n. These functions are the branches of the mth-root
function. One can merge all the branches into one multivalued mth-root func-
tion. Using a convention for Œ∏, one would extend the n = 0 branch to the
n = 1 branch by winding counterclockwise around the point z = 0. One
would encounter no discontinuity as one passed from one branch to another.
194

5.16 POWERS AND ROOTS
The point z = 0, where any cut starts, is called a branch point because, by
winding around it, one passes smoothly from one branch to another. Such
branches, introduced by Riemann, can be associated with any multivalued
analytic function, not just with the mth root.
Example 5.28 (Explicit square-roots)
If the cut in the square-root ‚àöz is on the
negative real axis, then an explicit formula for the square-root of x + iy is

x + iy =
(
x2 + y2 + x
2
+ i sign(y)
(
x2 + y2 ‚àíx
2
,
(5.188)
in which sign(y) = sgn(y) = y/|y|. On the other hand, if the cut in the square-
root ‚àöz is on the positive real axis, then an explicit formula for the square-root
of x + iy is

x + iy = sign(y)
(
x2 + y2 + x
2
+ i
(
x2 + y2 ‚àíx
2
(5.189)
(exercise 5.28).
Example 5.29 (Cuts)
Cuts are discontinuities, so people place them where they
do the least harm. For the function
f (z) =

z2 ‚àí1 =

(z ‚àí1)(z + 1)
(5.190)
two principal conventions work well. We could put the cut in the deÔ¨Ånition of the
angle Œ∏ along either the positive or the negative real axis. And we‚Äôd get a bonus:
the sign discontinuity (a factor of ‚àí1) from
‚àö
z ‚àí1 would cancel the one from
‚àö
z + 1 except for ‚àí1 ‚â§z ‚â§1. So the function f (z) would have a discontinuity
or a cut only for ‚àí1 ‚â§z ‚â§1.
But now suppose we had to work with the function
f (z) =

z2 + 1 =

(z ‚àíi)(z + i).
(5.191)
If we used one of the usual conventions, we‚Äôd have two semi-inÔ¨Ånite cuts. So we
put the Œ∏-cut on the positive or negative imaginary axis, and the function f (z)
now has a cut running along the imaginary axis only from ‚àíi to i.
Example 5.30 (Integral with a square-root)
Consider the integral
I =
 1
‚àí1
1
(x ‚àík)

1 ‚àíx2 dx,
(5.192)
in which the constant k lies anywhere in the complex plane but not on the inter-
val [ ‚àí1, 1]. Let‚Äôs promote x to a complex variable z and write the square-root
as

1 ‚àíx2 = i

x2 ‚àí1 = i

(z ‚àí1)(z + 1). As in the last example (5.29), if
in both of the square-roots we put the cut on the negative (or the positive)
195

COMPLEX-VARIABLE THEORY
real axis, then the function f (z) = 1/[(z ‚àík)i

(z ‚àí1)(z + 1)] will be analytic
everywhere except along a cut on the interval [ ‚àí1, 1] and at z = k. The cir-
cle z = ReiŒ∏ for 0 ‚â§Œ∏ ‚â§2œÄ is a ghost contour as R ‚Üí‚àû. We shrink-wrap
this ccw contour around the pole at z = k and the interval [ ‚àí1, 1], we get
0 = ‚àí2I + 2œÄi/

1 ‚àík2, so
I =
œÄi

1 ‚àík2 =
œÄ

k2 ‚àí1
.
(5.193)
For instance, if k = ‚àí2, the integral is I = œÄ/
‚àö
3 ‚âà1.8138.
Example 5.31 (Contour integral with a cut)
Let‚Äôs compute the integral
I =
 ‚àû
0
xa
(x + 1)2 dx
(5.194)
for ‚àí1 < a < 1. We promote x to a complex variable z and put the cut on the
positive real axis. Since
lim
|z|‚Üí‚àû
|z|a+1
|z + 1|2 = 0,
(5.195)
the integrand vanishes faster than 1/|z|, and we may add two ghost contours, G+
counterclockwise around the upper half-plane and G‚àícounterclockwise around
the lower half-plane, as shown in Fig. 5.8.
We add a contour C‚àíthat runs from ‚àí‚àûto the double pole at z = ‚àí1, loops
around that pole, and then runs back to ‚àí‚àû; the two long contours along the
negative real axis cancel because the cut in Œ∏ lies on the positive real axis. So the
contour integral along C‚àíis just the clockwise integral around the double pole,
which by Cauchy‚Äôs integral formula (5.34) is
)
C‚àí
za
(z ‚àí(‚àí1))2 dz = ‚àí2œÄi dza
dz

z=‚àí1
= 2œÄi a eœÄai.
(5.196)
We also add the integral I‚àífrom ‚àûto 0 just below the real axis
I‚àí=
 0
‚àû
(x ‚àíiœµ)a
(x ‚àíiœµ + 1)2 dx =
 0
‚àû
exp(a(ln(x) + 2œÄi))
(x + 1)2
dx,
(5.197)
which is
I‚àí= ‚àíe2œÄai
 ‚àû
0
xa
(x + 1)2 dx = ‚àíe2œÄai I.
(5.198)
Now the sum of all these contour integrals is zero because it is a closed contour
that encloses no singularity. So we have
0 =

1 ‚àíe2œÄai
I + 2œÄi a eœÄai
(5.199)
196

5.17 CONFORMAL MAPPING
>
<
>
<
>
<
>
Ghost contours and a cut
Figure 5.8
The integral of f (z) = za/(z+1) along the ghost contours G+ and G‚àí,
the contour C‚àí, the contour I‚àí, and the contour I vanishes because the combined
contour encircles no poles of f (z). The cut (solid line) runs from the origin to inÔ¨Ånity
along the positive real axis.
or
I =
 ‚àû
0
xa
(x + 1)2 dx =
œÄa
sin(œÄa)
(5.200)
as the value of the integral (5.194).
5.17 Conformal mapping
An analytic function f (z) maps curves in the z plane into curves in the f (z)
plane. In general, this mapping preserves angles. To see why, we consider the
angle dŒ∏ between two tiny complex lines dz = œµ exp(iŒ∏) and dz‚Ä≤ = œµ exp(iŒ∏‚Ä≤)
that radiate from the same point z. This angle dŒ∏ = Œ∏‚Ä≤ ‚àíŒ∏ is the phase of the
ratio
dz‚Ä≤
dz = œµeiŒ∏‚Ä≤
œµeiŒ∏ = ei(Œ∏‚Ä≤‚àíŒ∏).
(5.201)
197

COMPLEX-VARIABLE THEORY
Let‚Äôs use w = œÅeiœÜ for f (z). Then the analytic function f (z) maps dz into
dw = f (z + dz) ‚àíf (z) ‚âàf ‚Ä≤(z) dz
(5.202)
and dz‚Ä≤ into
dw‚Ä≤ = f (z + dz‚Ä≤) ‚àíf (z) ‚âàf ‚Ä≤(z) dz‚Ä≤.
(5.203)
The angle dœÜ = œÜ‚Ä≤ ‚àíœÜ between dw and dw‚Ä≤ is the phase of the ratio
dw‚Ä≤
dw = eiœÜ‚Ä≤
eiœÜ = f ‚Ä≤(z) dz‚Ä≤
f ‚Ä≤(z) dz = dz‚Ä≤
dz = eiŒ∏‚Ä≤
eiŒ∏ = ei(Œ∏‚Ä≤‚àíŒ∏).
(5.204)
So as long as the derivative f ‚Ä≤(z) does not vanish, the angle in the w-plane is the
same as the angle in the z-plane
dœÜ = dŒ∏.
(5.205)
Analytic functions preserve angles. They are conformal maps.
What if f ‚Ä≤(z) = 0? In this case, dw ‚âàf
‚Ä≤‚Ä≤(z) dz2/2 and dw‚Ä≤ ‚âàf
‚Ä≤‚Ä≤(z) dz‚Ä≤2/2, and
so the angle dœÜ = dœÜ‚Ä≤ ‚àídœÜ between these two tiny complex lines is the phase
of the ratio
dw‚Ä≤
dw = eiœÜ‚Ä≤
eiœÜ = f
‚Ä≤‚Ä≤(z) dz‚Ä≤2
f
‚Ä≤‚Ä≤(z) dz2 = dz‚Ä≤2
dz2 = e2i(Œ∏‚Ä≤‚àíŒ∏).
(5.206)
So angles are doubled, dœÜ = 2dŒ∏.
In general, if the Ô¨Årst nonzero derivative is f (n)(z), then
dw‚Ä≤
dw = eiœÜ‚Ä≤
eiœÜ = f (n)(z) dz‚Ä≤n
f (n)(z) dzn = dz‚Ä≤n
dzn = eni(Œ∏‚Ä≤‚àíŒ∏)
(5.207)
and so dœÜ = ndŒ∏. The angles increase by a factor of n.
Example 5.32 (zn)
The function f (z) = czn has only one nonzero derivative at
the origin z = 0
f (k)(0) = c n! Œ¥nk
(5.208)
so at z = 0 the conformal map z ‚Üíczn scales angles by n, dœÜ = n dŒ∏.
For examples of conformal mappings see (Lin, 2011, section 3.5.7).
5.18 Cauchy‚Äôs principal value
Suppose that f (x) is differentiable or analytic at and near the point x = 0, and
that we wish to evaluate the integral
K = lim
œµ‚Üí0
 b
‚àía
dx f (x)
x ‚àíiœµ
(5.209)
198

5.18 CAUCHY‚ÄôS PRINCIPAL VALUE
for a > 0 and b > 0. First we regularize the pole at x = 0 by using a method
devised by Cauchy
K = lim
Œ¥‚Üí0

lim
œµ‚Üí0
 ‚àíŒ¥
‚àía
dx f (x)
x ‚àíiœµ +
 Œ¥
‚àíŒ¥
dx f (x)
x ‚àíiœµ +
 b
Œ¥
dx f (x)
x ‚àíiœµ

.
(5.210)
In the Ô¨Årst and third integrals, since |x| ‚â•Œ¥, we may set œµ = 0
K = lim
Œ¥‚Üí0
 ‚àíŒ¥
‚àía
dx f (x)
x
+
 b
Œ¥
dx f (x)
x

+ lim
Œ¥‚Üí0 lim
œµ‚Üí0
 Œ¥
‚àíŒ¥
dx f (x)
x ‚àíiœµ .
(5.211)
We‚Äôll discuss the Ô¨Årst two integrals before analyzing the last one.
The limit of the Ô¨Årst two integrals is called Cauchy‚Äôs principal value
P
 b
‚àía
dx f (x)
x
‚â°lim
Œ¥‚Üí0
 ‚àíŒ¥
‚àía
dx f (x)
x
+
 b
Œ¥
dx f (x)
x

.
(5.212)
If the function f (x) is nearly constant near x = 0, then the large negative values
of 1/x for x slightly less than zero cancel the large positive values of 1/x for
x slightly greater than zero. The point x = 0 is not special; Cauchy‚Äôs principal
value about x = y is deÔ¨Åned by the limit
P
 b
‚àía
dx f (x)
x ‚àíy ‚â°lim
Œ¥‚Üí0
 y‚àíŒ¥
‚àía
dx f (x)
x ‚àíy +
 b
y+Œ¥
dx f (x)
x ‚àíy

.
(5.213)
Using Cauchy‚Äôs principal value, we may write the quantity K as
K = P
 b
‚àía
dx f (x)
x
+ lim
Œ¥‚Üí0 lim
œµ‚Üí0
 Œ¥
‚àíŒ¥
dx f (x)
x ‚àíiœµ .
(5.214)
To evaluate the second integral, we use differentiability of f (x) near x = 0 to
write f (x) = f (0) + xf ‚Ä≤(0) and then extract the constants f (0) and f ‚Ä≤(0)
lim
Œ¥‚Üí0 lim
œµ‚Üí0
 Œ¥
‚àíŒ¥
dx f (x)
x ‚àíiœµ = lim
Œ¥‚Üí0 lim
œµ‚Üí0
 Œ¥
‚àíŒ¥
dx f (0) + x f ‚Ä≤(0)
x ‚àíiœµ
= f (0) lim
Œ¥‚Üí0 lim
œµ‚Üí0
 Œ¥
‚àíŒ¥
dx
x ‚àíiœµ + f ‚Ä≤(0) lim
Œ¥‚Üí0 lim
œµ‚Üí0
 Œ¥
‚àíŒ¥
x dx
x ‚àíiœµ
= f (0) lim
Œ¥‚Üí0 lim
œµ‚Üí0
 Œ¥
‚àíŒ¥
dx
x ‚àíiœµ + f ‚Ä≤(0) lim
Œ¥‚Üí0 2Œ¥
= f (0) lim
Œ¥‚Üí0 lim
œµ‚Üí0
 Œ¥
‚àíŒ¥
dx
x ‚àíiœµ .
(5.215)
Now since 1/(z ‚àíiœµ) is analytic in the lower half-plane, we may deform the
straight contour from x = ‚àíŒ¥ to x = Œ¥ into a tiny semicircle that avoids the
point x = 0 by setting z = Œ¥ eiŒ∏ and letting Œ∏ run from œÄ to 2œÄ
199

COMPLEX-VARIABLE THEORY
K = P
 b
‚àía
dx f (x)
x
+ f (0) lim
Œ¥‚Üí0 lim
œµ‚Üí0
 Œ¥
‚àíŒ¥
dz
1
z ‚àíiœµ .
(5.216)
We now can set œµ = 0 and so write K as
K = P
 b
‚àía
dx f (x)
x
+ f (0) lim
Œ¥‚Üí0
 2œÄ
œÄ
iŒ¥eiŒ∏dŒ∏
1
Œ¥eiŒ∏
= P
 b
‚àía
dx f (x)
x
+ iœÄf (0).
(5.217)
Recalling the deÔ¨Ånition (5.209) of K, we have
lim
œµ‚Üí0
 b
‚àía
dx f (x)
x ‚àíiœµ = P
 b
‚àía
dx f (x)
x
+ iœÄf (0)
(5.218)
for any function f (x) that is differentiable at x = 0. Physicists write this as
1
x ‚àíiœµ = P1
x + iœÄŒ¥(x)
and
1
x + iœµ = P1
x ‚àíiœÄŒ¥(x)
(5.219)
or as
1
x ‚àíy ¬± iœµ = P
1
x ‚àíy ‚àìiœÄŒ¥(x ‚àíy).
(5.220)
Example 5.33 (Cauchy‚Äôs trick)
We use (5.219) to evaluate the integral
I =
 ‚àû
‚àí‚àû
dx
1
x + iœµ
1
1 + x2
(5.221)
as
I = P
 ‚àû
‚àí‚àû
dx 1
x
1
1 + x2 ‚àíiœÄ
 ‚àû
‚àí‚àû
dx Œ¥(x)
1 + x2 .
(5.222)
Because the function 1/x(1 + x2) is odd, the principal part is zero. The integral
over the delta function gives unity, so we have I = ‚àíiœÄ.
Example 5.34 (Cauchy‚Äôs principal value)
By explicit use of the formula

dx
x2 ‚àía2 = ‚àí1
2a ln x + a
x ‚àía
(5.223)
one may show (exercise 5.30) that
P
 ‚àû
0
dx
x2 ‚àía2 =
 a‚àíŒ¥
0
dx
x2 ‚àía2 +
 ‚àû
a+Œ¥
dx
x2 ‚àía2 = 0,
(5.224)
a result we‚Äôll use in section 5.21.
200

5.18 CAUCHY‚ÄôS PRINCIPAL VALUE
Example 5.35 (sin k/k)
To compute the integral
I =
 ‚àû
0
dk
k sin k,
(5.225)
which we used to derive the formula (3.110) for the Green‚Äôs function of the
Laplacian in three dimensions, we Ô¨Årst express I as an integral along the whole
real axis
I =
 ‚àû
0
dk
2ik

eik ‚àíe‚àíik
=
 ‚àû
‚àí‚àû
dk
2ik eik,
(5.226)
by which we actually mean the Cauchy principal part
I = lim
Œ¥‚Üí0
 ‚àíŒ¥
‚àí‚àû
dk eik
2ik +
 ‚àû
Œ¥
dk eik
2ik

= P
 ‚àû
‚àí‚àû
dk eik
2ik.
(5.227)
Using Cauchy‚Äôs trick (5.219), we have
I = P
 ‚àû
‚àí‚àû
dk eik
2ik =
 ‚àû
‚àí‚àû
dk
eik
2i(k + iœµ) +
 ‚àû
‚àí‚àû
dk iœÄ Œ¥(k) eik
2i .
(5.228)
To the Ô¨Årst integral, we add a ghost contour around the upper half-plane. For
the contour from k = L to k = L + iH and then to k = ‚àíL + iH and then
down to k = ‚àíL, one may show (exercise 5.33) that the integral of exp(ik)/k
vanishes in the double limit L ‚Üí‚àûand H ‚Üí‚àû. With this ghost contour,
the Ô¨Årst integral therefore vanishes because the pole at k = ‚àíiœµ is in the lower
half-plane. The delta function in the second integral then gives œÄ/2, so that
I =
)
dk
eik
2i(k + iœµ) + œÄ
2 = œÄ
2
(5.229)
as stated in (3.109).
Example 5.36 (The Feynman propagator)
Adding ¬±iœµ to the denominator of a
pole term of an integral formula for a function f (x) can slightly shift the pole into
the upper or lower half-plane, causing the pole to contribute if a ghost contour
goes around the upper half-plane or the lower half-plane. Such an iœµ can impose
a boundary condition on Green‚Äôs function.
The Feynman propagator F(x) is a Green‚Äôs function for the Klein‚ÄìGordon
differential operator (Weinberg, 1995, pp. 274‚Äì280)
(m2 ‚àí2)F(x) = Œ¥4(x)
(5.230)
in which x = (x0, x) and
2 = ‚ñ≥‚àí‚àÇ2
‚àÇt2 = ‚ñ≥‚àí
‚àÇ2
‚àÇ(x0)2
(5.231)
is the four-dimensional version of the Laplacian ‚ñ≥‚â°‚àá¬∑ ‚àá. Here Œ¥4(x) is the
four-dimensional Dirac delta function (3.36)
201

COMPLEX-VARIABLE THEORY
Œ¥4(x) =

d4q
(2œÄ)4 exp[i(q ¬∑ x ‚àíq0x0)] =

d4q
(2œÄ)4 eiqx,
(5.232)
in which qx = q ¬∑ x ‚àíq0x0 is the Lorentz-invariant inner product of the 4-
vectors q and x. There are many Green‚Äôs functions that satisfy equation (5.230).
Feynman‚Äôs propagator F(x) is the one that satisÔ¨Åes boundary conditions that
will become evident when we analyze the effect of its iœµ
F(x) =

d4q
(2œÄ)4
exp(iqx)
q2 + m2 ‚àíiœµ =

d3q
(2œÄ)3
 ‚àû
‚àí‚àû
dq0
2œÄ
eiq¬∑x‚àíiq0x0
q2 + m2 ‚àíiœµ .
(5.233)
The quantity q0 = Eq =

q2 + m2 is the energy of a particle of mass m
and momentum q in natural units with the speed of light c = 1. Using this
abbreviation and setting œµ‚Ä≤ = œµ/2Eq, we may write the denominator as
q2 + m2 ‚àíiœµ = q ¬∑ q ‚àí

q02
+ m2 ‚àíiœµ =

Eq ‚àíiœµ‚Ä≤ ‚àíq0 
Eq ‚àíiœµ‚Ä≤ + q0
+ œµ‚Ä≤2,
(5.234)
in which œµ‚Ä≤2 is negligible. Dropping the prime on œµ, we do the q0 integral
I(q) = ‚àí
 ‚àû
‚àí‚àû
dq0
2œÄ e‚àíiq0x0
1

q0 ‚àí(Eq ‚àíiœµ)
 
q0 ‚àí(‚àíEq + iœµ)
.
(5.235)
As shown in Fig. 5.9, the integrand
e‚àíiq0x0
1

q0 ‚àí(Eq ‚àíiœµ)
 
q0 ‚àí(‚àíEq + iœµ)

(5.236)
has poles at Eq ‚àíiœµ and at ‚àíEq + iœµ. When x0 > 0, we can add a ghost contour
that goes clockwise around the lower half-plane and get
I(q) = ie‚àíiEqx0
1
2Eq
,
x0 > 0.
(5.237)
When x0 < 0, our ghost contour goes counterclockwise around the upper half-
plane, and we get
I(q) = ieiEqx0
1
2Eq
,
x0 < 0.
(5.238)
Using the step function Œ∏(x) = (x + |x|)/2, we combine (5.237) and (5.238)
‚àíiI(q) =
1
2Eq

Œ∏(x0) e‚àíiEqx0 + Œ∏(‚àíx0) eiEqx0
.
(5.239)
In terms of the Lorentz-invariant function
+(x) =
1
(2œÄ)3
 d3q
2Eq
exp[i(q ¬∑ x ‚àíEqx0)]
(5.240)
202

5.18 CAUCHY‚ÄôS PRINCIPAL VALUE
>
<
<
x0 < 0
x0 > 0
Eq ‚àí iœµ
Eq + iœµ
Ghost contours and the Feynman propagator
Figure 5.9
In equation (5.236), the function f (q0) has poles at ¬±(Eq ‚àíiœµ), and the
function exp(‚àíiq0x0) is exponentially suppressed in the lower half-plane if x0 > 0
and in the upper half-plane if x0 < 0. So we can add a ghost contour (dots) in the
LHP if x0 > 0 and in the UHP if x0 < 0.
and with a factor of ‚àíi, Feynman‚Äôs propagator (5.233) is
‚àíiF(x) = Œ∏(x0) +(x) + Œ∏(‚àíx0) +(x, ‚àíx0).
(5.241)
The integral (5.240) deÔ¨Åning +(x) is insensitive to the sign of q, and so
+(‚àíx) =
1
(2œÄ)3
 d3q
2Eq
exp[i(‚àíq ¬∑ x + Eqx0)]
=
1
(2œÄ)3
 d3q
2Eq
exp[i(q ¬∑ x + Eqx0)] = +(x, ‚àíx0).
(5.242)
Thus we arrive at the standard form of the Feynman propagator
‚àíiF(x) = Œ∏(x0) +(x) + Œ∏(‚àíx0) +(‚àíx).
(5.243)
The annihilation operators a(q) and the creation operators a‚Ä†(p) of a scalar
Ô¨Åeld œÜ(x) satisfy the commutation relations
203

COMPLEX-VARIABLE THEORY
[a(q), a‚Ä†(p)] = Œ¥3(q ‚àíp)
and
[a(q), a(p)] = [a‚Ä†(q), a‚Ä†(p)] = 0.
(5.244)
Thus the commutator of the positive-frequency part
œÜ+(x) =

d3p

(2œÄ)32p0 exp[i(p ¬∑ x ‚àíp0x0)] a(p)
(5.245)
of a scalar Ô¨Åeld œÜ = œÜ+ + œÜ‚àíwith its negative-frequency part
œÜ‚àí(y) =

d3q

(2œÄ)32q0 exp[‚àíi(q ¬∑ y ‚àíq0y0)] a‚Ä†(q)
(5.246)
is the Lorentz-invariant function +(x ‚àíy)
[œÜ+(x), œÜ‚àí(y)] =

d3p d3q
(2œÄ)32

q0p0 eipx‚àíiqy [a(p), a‚Ä†(q)]
=

d3p
(2œÄ)32p0 eip(x‚àíy) = +(x ‚àíy)
(5.247)
in which p(x ‚àíy) = p ¬∑ (x ‚àíy) ‚àíp0(x0 ‚àíy0).
At points x that are space-like, that is, for which x2 = x2 ‚àí(x0)2 ‚â°r2 > 0,
the Lorentz-invariant function +(x) depends only upon r = +
‚àö
x2 and has the
value (Weinberg, 1995, p. 202)
+(x) =
m
4œÄ2r K1(mr),
(5.248)
in which the Hankel function K1 is
K1(z) = ‚àíœÄ
2 [J1(iz) + iN1(iz)] = 1
z +
z
2j + 2
#
ln
 z
2

+ Œ≥ ‚àí
1
2j + 2
$
+ ¬∑ ¬∑ ¬∑
(5.249)
where J1 is the Ô¨Årst Bessel function, N1 is the Ô¨Årst Neumann function, and Œ≥ =
0.57721 . . . is the Euler‚ÄìMascheroni constant.
The Feynman propagator arises most simply as the mean value in the vacuum
of the time-ordered product of the Ô¨Åelds œÜ(x) and œÜ(y)
T {œÜ(x)œÜ(y)} ‚â°Œ∏(x0 ‚àíy0)œÜ(x)œÜ(y) + Œ∏(y0 ‚àíx0)œÜ(y)œÜ(x).
(5.250)
The operators a(p) and a‚Ä†(p) respectively annihilate the vacuum ket a(p)|0‚ü©= 0
and bra ‚ü®0|a‚Ä†(p) = 0, and so by (5.245 & 5.246) do the positive- and negative-
frequency parts of the Ô¨Åeld œÜ+(z)|0‚ü©= 0 and ‚ü®0|œÜ‚àí(z) = 0. Thus the mean value
in the vacuum of the time-ordered product is
‚ü®0|T {œÜ(x)œÜ(y)} |0‚ü©= ‚ü®0|Œ∏(x0‚àíy0)œÜ(x)œÜ(y) + Œ∏(y0‚àíx0)œÜ(y)œÜ(x)|0‚ü©
= ‚ü®0|Œ∏(x0‚àíy0)œÜ+(x)œÜ‚àí(y) + Œ∏(y0‚àíx0)œÜ+(y)œÜ‚àí(x)|0‚ü©
= ‚ü®0|Œ∏(x0‚àíy0)[œÜ+(x), œÜ‚àí(y)]
+ Œ∏(y0‚àíx0)[œÜ+(y), œÜ‚àí(x)]|0‚ü©.
(5.251)
204

5.19 DISPERSION RELATIONS
But by (5.247), these commutators are +(x‚àíy) and +(y‚àíx). Thus the mean
value in the vacuum of the time-ordered product
‚ü®0|T {œÜ(x)œÜ(y)} |0‚ü©= Œ∏(x0 ‚àíy0)+(x ‚àíy) + Œ∏(y0 ‚àíx0)+(y ‚àíx)
= ‚àíiF(x ‚àíy)
(5.252)
is the Feynman propagator (5.241) multiplied by ‚àíi.
5.19 Dispersion relations
In many physical contexts, functions occur that are analytic in the upper
half-plane (UHP). Suppose for instance that ÀÜf (t) is a transfer function that
determines an effect e(t) due to a cause c(t)
e(t) =
 ‚àû
‚àí‚àû
dt‚Ä≤ ÀÜf (t ‚àít‚Ä≤) c(t‚Ä≤).
(5.253)
If the system is causal, then the transfer function ÀÜf (t ‚àít‚Ä≤) is zero for t ‚àít‚Ä≤ < 0,
and so its Fourier transform
f (z) =
 ‚àû
‚àí‚àû
dt
‚àö
2œÄ
ÀÜf (t) eizt =
 ‚àû
0
dt
‚àö
2œÄ
ÀÜf (t) eizt
(5.254)
will be analytic in the upper half-plane and will shrink as the imaginary part of
z = x + iy increases.
So let us assume that the function f (z) is analytic in the upper half-plane and
on the real axis and further that
lim
r‚Üí‚àû|f (reiŒ∏)| = 0
for
0 ‚â§Œ∏ ‚â§œÄ.
(5.255)
By Cauchy‚Äôs integral formula (5.32), if z0 lies in the upper half-plane, then f (z0)
is given by the closed counterclockwise contour integral
f (z0) =
1
2œÄi
)
f (z)
z ‚àíz0
dz,
(5.256)
in which the contour runs along the real axis and then loops over the semicircle
lim
r‚Üí‚àûreiŒ∏
for
0 ‚â§Œ∏ ‚â§œÄ.
(5.257)
Our assumption (5.255) about the behavior of f (z) in the UHP implies that this
contour (5.257) is a ghost contour because its modulus is bounded by
lim
r‚Üí‚àû
1
2œÄ
 |f (reiŒ∏)|r
r
dŒ∏ = lim
r‚Üí‚àû|f (reiŒ∏)| = 0.
(5.258)
So we may drop the ghost contour and write f (z0) as
f (z0) =
1
2œÄi
 ‚àû
‚àí‚àû
f (x)
x ‚àíz0
dx.
(5.259)
205

COMPLEX-VARIABLE THEORY
Letting the imaginary part y0 of z0 = x0 + iy0 shrink to œµ
f (x0) =
1
2œÄi
 ‚àû
‚àí‚àû
f (x)
x ‚àíx0 ‚àíiœµ dx
(5.260)
and using Cauchy‚Äôs trick (5.220), we get
f (x0) =
1
2œÄi P
 ‚àû
‚àí‚àû
f (x)
x ‚àíx0
dx + iœÄ
2œÄi
 ‚àû
‚àí‚àû
f (x) Œ¥(x ‚àíx0) dx
(5.261)
or
f (x0) =
1
2œÄi P
 ‚àû
‚àí‚àû
f (x)
x ‚àíx0
dx + 1
2 f (x0),
(5.262)
which is the dispersion relation
f (x0) = 1
œÄi P
 ‚àû
‚àí‚àû
f (x)
x ‚àíx0
dx.
(5.263)
If we break f (z) = u(z)+iv(z) into its real u(z) and imaginary v(z) parts, then
this dispersion relation (5.263)
u(x0) + iv(x0) = 1
œÄi P
 ‚àû
‚àí‚àû
u(x) + iv(x)
x ‚àíx0
dx
(5.264)
= 1
œÄ P
 ‚àû
‚àí‚àû
v(x)
x ‚àíx0
dx ‚àíi
œÄ P
 ‚àû
‚àí‚àû
u(x)
x ‚àíx0
dx
breaks into its real and imaginary parts
u(x0) = 1
œÄ P
 ‚àû
‚àí‚àû
v(x)
x ‚àíx0
dx
and
v(x0) = ‚àí1
œÄ P
 ‚àû
‚àí‚àû
u(x)
x ‚àíx0
dx,
(5.265)
which express u and v as Hilbert transforms of each other.
In applications of dispersion relations, the function f (x) for x < 0 sometimes
is either physically meaningless or experimentally inaccessible. In such cases,
there may be a symmetry that relates f (‚àíx) to f (x). For instance, if f (x) is the
Fourier transform of a real function ÀÜf (k), then by equation (3.25) it obeys the
symmetry relation
f ‚àó(x) = u(x) ‚àíiv(x) = f (‚àíx) = u(‚àíx) + iv(‚àíx),
(5.266)
which says that u is even, u(‚àíx) = u(x), and v odd, v(‚àíx) = ‚àív(x). Using
these symmetries, one may show (exercise 5.36) that the Hilbert transformations
(5.265) become
u(x0) = 2
œÄ P
 ‚àû
0
x v(x)
x2 ‚àíx2
0
dx
and
v(x0) = ‚àí2x0
œÄ P
 ‚àû
0
u(x)
x2 ‚àíx2
0
dx,
(5.267)
which do not require input at negative values of x.
206

5.20 KRAMERS‚ÄìKRONIG RELATIONS
5.20 Kramers‚ÄìKronig relations
If we use œÉE for the current density J and E(t) = e‚àíiœâtE(t) for the electric Ô¨Åeld,
then Maxwell‚Äôs equation ‚àá√ó B = ŒºJ + œµŒº ÀôE becomes
‚àá√ó B = ‚àíiœâœµŒº

1 + i œÉ
œµœâ

E ‚â°‚àíiœân2œµ0Œº0E
(5.268)
and reveals the squared index of refraction as
n2(œâ) = œµŒº
œµ0Œº0

1 + i œÉ
œµœâ

.
(5.269)
The imaginary part of n2 represents the scattering of light mainly by electrons.
At high frequencies in nonmagnetic materials n2(œâ) ‚Üí1, and so Kramers and
Kronig applied the Hilbert-transform relations (5.267) to the function n2(œâ)‚àí1
in order to satisfy condition (5.255). Their relations are
Re(n2(œâ0)) = 1 + 2
œÄ P
 ‚àû
0
œâ Im(n2(œâ))
œâ2 ‚àíœâ2
0
dœâ
(5.270)
and
Im(n2(œâ0)) = ‚àí2œâ0
œÄ
P
 ‚àû
0
Re(n2(œâ)) ‚àí1
œâ2 ‚àíœâ2
0
dœâ.
(5.271)
What Kramers and Kronig actually wrote was slightly different from these
dispersion relations (5.270 & 5.271). H. A. Lorentz had shown that the index
of refraction n(œâ) is related to the forward scattering amplitude f (œâ) for the
scattering of light by a density N of scatterers (Sakurai, 1982)
n(œâ) = 1 + 2œÄc2
œâ2 Nf (œâ).
(5.272)
They used this formula to infer that the real part of the index of refraction
approached unity in the limit of inÔ¨Ånite frequency and applied the Hilbert
transform (5.267)
Re[n(œâ)] = 1 + 2
œÄ P
 ‚àû
0
œâ‚Ä≤ Im[n(œâ‚Ä≤)]
œâ‚Ä≤2 ‚àíœâ2
dœâ‚Ä≤.
(5.273)
The Lorentz relation (5.272) expresses the imaginary part Im[n(œâ)] of the index
of refraction in terms of the imaginary part of the forward scattering amplitude
f (œâ)
Im[n(œâ)] = 2œÄ(c/œâ)2NIm[f (œâ)].
(5.274)
And the optical theoremrelates Im[f (œâ)] to the total cross-section
œÉtot = 4œÄ
|k| Im[f (œâ)] = 4œÄc
œâ Im[f (œâ)].
(5.275)
207

COMPLEX-VARIABLE THEORY
Thus we have Im[n(œâ)] = cNœÉtot/(2œâ), and by the Lorentz relation (5.272)
Re[n(œâ)] = 1 + 2œÄ(c/œâ)2NRe[f (œâ)]. Insertion of these formulas into the
Kramers‚ÄìKronig integral (5.273) gives a dispersion relation for the real part
of the forward scattering amplitude f (œâ) in terms of the total cross-section
Re[f (œâ)] =
œâ2
2œÄ2c P
 ‚àû
0
œÉtot(œâ‚Ä≤)
œâ‚Ä≤2 ‚àíœâ2 dœâ‚Ä≤.
(5.276)
5.21 Phase and group velocities
Suppose A(x, t) is the amplitude
A(x, t) =

ei(p¬∑x‚àíEt)/¬Øh A(p) d3p =

ei(k¬∑x‚àíœât) B(k) d3k
(5.277)
where B(k) = ¬Øh3A(¬Øhk) varies slowly compared to the phase exp[i(k ¬∑ x ‚àíœât)].
The phase velocity vp is the linear relation x = vp t between x and t that keeps
the phase œÜ = p ¬∑ x ‚àíEt constant as a function of the time
0 = p ¬∑ dx ‚àíE dt = (p ¬∑ vp ‚àíE) dt
‚áê‚áí
vp = E
p ÀÜp = œâ
k
ÀÜk,
(5.278)
in which p = |p|, and k = |k|. For light in the vacuum, vp = c = (œâ/k) ÀÜk.
The group velocity vg is the linear relation x = vg t between x and t that
maximizes the amplitude A(x, t) by keeping the phase œÜ = p ¬∑ x ‚àíEt constant
as a function of the momentum p
‚àáp(px ‚àíEt) = x ‚àí‚àápE(p) t = 0
(5.279)
at the maximum of A(p). This condition of stationary phase gives the group
velocity as
vg = ‚àápE(p) = ‚àákœâ(k).
(5.280)
If E = p2/(2m), then vg = p/m.
When light traverses a medium with a complex index of refraction n(k), the
wave vector k becomes complex, and its (positive) imaginary part represents
the scattering of photons in the forward direction, typically by the electrons
of the medium. For simplicity, we‚Äôll consider the propagation of light through
a medium in one dimension, that of the forward direction of the beam. Then
the (real) frequency œâ(k) and the (complex) wave-number k are related by k =
n(k) œâ(k)/c, and the phase velocity of the light is
vp =
œâ
Re(k) =
c
Re(n(k)).
(5.281)
208

5.21 PHASE AND GROUP VELOCITIES
If we regard the index of refraction as a function of the frequency œâ, instead
of the wave-number k, then by differentiating the real part of the relation
œân(œâ) = ck with respect to œâ, we Ô¨Ånd
nr(œâ) + œâ dnr(œâ)
dœâ
= cdkr
dœâ ,
(5.282)
in which the subscript r means real part. Thus the group velocity (5.280) of the
light is
vg = dœâ
dkr
=
c
nr(œâ) + œâ dnr/dœâ.
(5.283)
Optical physicists call the denominator the group index of refraction
ng(œâ) = nr(œâ) + œâ dnr(œâ)
dœâ
(5.284)
so that as in the expression (5.281) for the phase velocity vp = c/nr(œâ), the
group velocity is vg = c/ng(œâ).
In some media, the derivative dnr/dœâ is large and positive, and the group
velocity vg of light there can be much less than c (Steinberg et al., 1993; Wang
and Zhang, 1995) ‚Äì as slow as 17 m/s (Hau et al., 1999). This effect is called slow
light. In certain other media, the derivative dn/dœâ is so negative that the group
index of refraction ng(œâ) is less than unity, and in them the group velocity vg
exceeds c ! This effect is called fast light. In some media, the derivative dnr/dœâ
is so negative that dnr/dœâ < ‚àínr(œâ)/œâ, and then ng(œâ) is not only less than
unity but also less than zero. In such a medium, the group velocity vg of light is
negative! This effect is called backwards light.
Sommerfeld and Brillouin (Brillouin, 1960, ch. II & III) anticipated fast light
and concluded that it would not violate special relativity as long as the signal
velocity ‚Äì deÔ¨Åned as the speed of the front of a square pulse ‚Äì remained less than
c. Fast light does not violate special relativity (Stenner et al., 2003; Brunner
et al., 2004) (L√©on Brillouin, 1889‚Äì1969; Arnold Sommerfeld, 1868‚Äì1951).
Slow, fast, and backwards light can occur when the frequency œâ of the light
is near a peak or resonance in the total cross-section œÉtot for the scattering of
light by the atoms of the medium. To see why, recall that the index of refraction
n(œâ) is related to the forward scattering amplitude f (œâ) and the density N of
scatterers by the formula (5.272)
n(œâ) = 1 + 2œÄc2
œâ2 Nf (œâ)
(5.285)
and that the real part of the forward scattering amplitude is given by the
Kramers‚ÄìKronig integral (5.276) of the total cross-section
Re(f (œâ)) =
œâ2
2œÄ2c P
 ‚àû
0
œÉtot(œâ‚Ä≤) dœâ‚Ä≤
œâ‚Ä≤2 ‚àíœâ2 .
(5.286)
209

COMPLEX-VARIABLE THEORY
So the real part of the index of refraction is
nr(œâ) = 1 + cN
œÄ P
 ‚àû
0
œÉtot(œâ‚Ä≤) dœâ‚Ä≤
œâ‚Ä≤2 ‚àíœâ2 .
(5.287)
If the amplitude for forward scattering is of the Breit‚ÄìWigner form
f (œâ) = f0
/2
œâ0 ‚àíœâ ‚àíi/2
(5.288)
then by (5.285) the real part of the index of refraction is
nr(œâ) = 1 +
œÄc2Nf0(œâ0 ‚àíœâ)
œâ2 
(œâ ‚àíœâ0)2 + 2/4

(5.289)
and by (5.283) the group velocity is
vg = c

1 + œÄc2Nf0 œâ0
œâ2

(œâ ‚àíœâ0)2 ‚àí2/4


(œâ ‚àíœâ0)2 + 2/4
2
‚àí1
.
(5.290)
This group velocity vg is less than c whenever (œâ ‚àíœâ0)2 > 2/4. But we get fast
light vg > c, if (œâ ‚àíœâ0)2 < 2/4, and even backwards light, vg < 0, if œâ ‚âàœâ0
with 4œÄc2Nf0/œâ0 ‚â´1. Robert W. Boyd‚Äôs papers explain how to make slow
and fast light (Bigelow et al., 2003) and backwards light (Gehring et al., 2006).
We can use the principal-part identity (5.224) to subtract
0 = cN
œÄ P
 ‚àû
0
œÉtot(œâ)
œâ‚Ä≤2 ‚àíœâ2 dœâ‚Ä≤
(5.291)
from the Kramers‚ÄìKronig integral (5.287) so as to write the index of refraction
in the regularized form
nr(œâ) = 1 + cN
œÄ
 ‚àû
0
œÉtot(œâ‚Ä≤) ‚àíœÉtot(œâ)
œâ‚Ä≤2 ‚àíœâ2
dœâ‚Ä≤,
(5.292)
which we can differentiate and use in the group-velocity formula (5.283)
vg(œâ) = c

1 + cN
œÄ P
 ‚àû
0

œÉtot(œâ‚Ä≤) ‚àíœÉtot(œâ)

(œâ‚Ä≤2 + œâ2)
(œâ‚Ä≤2 ‚àíœâ2)2
dœâ‚Ä≤
‚àí1
.
(5.293)
5.22 The method of steepest descent
Suppose we want to approximate for big x > 0 the integral
I(x) =
 b
a
dz h(z) exp(xf (z)),
(5.294)
210

5.22 THE METHOD OF STEEPEST DESCENT
in which the functions h(z) and f (z) are analytic in a simply connected region
that includes the points a and b in its interior. The value of the integral I(x) is
independent of the contour between the endpoints a and b. In the limit x ‚Üí‚àû,
the integral I(x) is dominated by the exponential. So the key factor is the real
part u of f = u + iv. But since f (z) is analytic, its real and imaginary parts u(z)
and v(z) are harmonic functions which have no minima or maxima, only saddle
points (5.52).
For simplicity, we‚Äôll assume that the real part u(z) of f (z) has only one saddle
point between the points a and b. (If it has more than one, then we must repeat
the computation that follows.) If w is the saddle point, then ux = uy = 0, which
by the Cauchy‚ÄìRiemann equations (5.48) implies that vx = vy = 0. Thus the
derivative of the function f also vanishes at the saddle point f ‚Ä≤(w) = 0, and so
near w we may approximate f (z) as
f (z) ‚âàf (w) + 1
2(z ‚àíw)2f
‚Ä≤‚Ä≤(w).
(5.295)
Let‚Äôs write the second derivative as f
‚Ä≤‚Ä≤(w) = œÅ eiœÜ and choose our contour
through the saddle point w to be a straight line z = w + y eiŒ∏ with Œ∏ Ô¨Åxed
for z near w. As we vary y along this line, we want
(z ‚àíw)2f
‚Ä≤‚Ä≤(w) = y2 œÅ e2iŒ∏ eiœÜ < 0
(5.296)
so we keep 2Œ∏ + œÜ = œÄ so that near z = w
f (z) ‚âàf (w) ‚àí1
2 œÅ y2.
(5.297)
Since z = w + y eiŒ∏, its differential is dz = eiŒ∏ dy, and the integral I(x) is
I(x) ‚âà
 ‚àû
‚àí‚àû
h(w) exp

x
#
f (w) + 1
2(z ‚àíw)2f
‚Ä≤‚Ä≤(w)
$%
dz
(5.298)
= h(w) eiŒ∏ exf (w)
 ‚àû
‚àí‚àû
exp

‚àí1
2xœÅy2
dy = h(w) eiŒ∏ exf (w)
(
2œÄ
xœÅ .
Moving the phase eiŒ∏ inside the square-root
I(x) ‚âàh(w) exf (w)
(
2œÄ
xœÅ e‚àí2iŒ∏
(5.299)
and using f
‚Ä≤‚Ä≤(w) = œÅ eiœÜ and 2Œ∏ + œÜ = œÄ to show that
œÅ e‚àí2iŒ∏ = œÅ eiœÜ‚àíiœÄ = ‚àíœÅ eiœÜ = ‚àíf
‚Ä≤‚Ä≤(w)
(5.300)
we get our formula for the saddle-point integral (5.294)
I(x) ‚âà

2œÄ
‚àíxf
‚Ä≤‚Ä≤(w)
1/2
h(w) exf (w).
(5.301)
211

COMPLEX-VARIABLE THEORY
If there are n saddle points wj for j = 1, . . . , n, then the integral I(x) is the sum
I(x) ‚âà
N

j=1

2œÄ
‚àíxf
‚Ä≤‚Ä≤(wj)
1/2
h(wj) exf (wj).
(5.302)
5.23 The Abel‚ÄìPlana formula and the Casimir effect
This section is optional on a Ô¨Årst reading.
Suppose the function f (z) is analytic and bounded for n1 ‚â§Re z ‚â§n2. Let
C+ and C‚àíbe two contours that respectively run counterclockwise along the
rectangles with vertices n1, n2, n2 + i‚àû, n1 + i‚àûand n1, n2, n2 ‚àíi‚àû, n1 ‚àíi‚àû
indented with tiny semicircles and quarter-circles so as to avoid the integers
z = n1, n1 + 1, n1 + 2, ..., n2 while keeping Imz > 0 in the upper rectangle and
Imz < 0 in the lower one (and n1 < Re z < n2). Then the contour integrals
I¬± =

C¬±
f (z)
e‚àì2œÄiz ‚àí1 dz = 0
(5.303)
vanish by Cauchy‚Äôs theorem (5.22) since the poles of the integrand lie outside
the indented rectangles.
The absolute value of the exponential exp(‚àí2œÄiz) is arbitrarily large on the
top of the upper rectangle C+ where Imz = ‚àû, and so that leg of the con-
tour integral I+ vanishes. Similarly, the bottom leg of the contour integral I‚àí
vanishes. Thus we can separate the difference I+ ‚àíI‚àíinto a term Tx due to
the integrals near the x-axis between n1 and n2, a term T1 involving integrals
between n1 and n1 ¬± i‚àû, and a term T2 involving integrals between n2 and
n2 ¬± i‚àû, that is, 0 = I+ ‚àíI‚àí= Tx + T1 + T2.
The term Tx = Ix + S consists of the integrals Ix along the segments of the
x-axis from n1 to n2 and a sum S over the tiny integrals along the semicircles
and quarter-circles that avoid the integers from n1 to n2. Elementary algebra
simpliÔ¨Åes the integral Ix to
Ix =
 n2
n1
f (x)
#
1
e‚àí2œÄix ‚àí1 +
1
e+2œÄix ‚àí1
$
dx = ‚àí
 n2
n1
f (x) dx.
(5.304)
The sum S is over the semicircles that avoid n1 + 1, . . . , n2 ‚àí1 and over the
quarter-circles that avoid n1 and n2. For any integer n1 < n < n2, the integral
along the semicircle of Cn+ minus that along the semicircle of Cn‚àí, both around
n, contributes to S the quantity
Sn =

SCn+
f (z)
e‚àí2œÄiz ‚àí1 dz ‚àí

SCn‚àí
f (z)
e2œÄiz ‚àí1 dz
=

SCn+
f (z)
e‚àí2œÄi(z‚àín) ‚àí1 dz ‚àí

SCn‚àí
f (z)
e2œÄi(z‚àín) ‚àí1 dz
(5.305)
212

5.23 THE ABEL‚ÄìPLANA FORMULA AND THE CASIMIR EFFECT
since exp(¬±2œÄin) = 1. The Ô¨Årst integral is clockwise in the upper half-plane,
the second clockwise in the lower half-plane. So if we make both integrals
counterclockwise, inserting minus signs, we Ô¨Ånd as the radii of these semicircles
shrink to zero
Sn =
)
f (z)
2œÄi(z ‚àín)dz = f (n).
(5.306)
One may show (exercise 5.39) that the quarter-circles around n1 and n2
contribute (f (n1) + f (n2))/2 to the sum S. Thus the term Tx is
Tx = 1
2f (n1) +
n2‚àí1

n=n1+1
f (n) + 1
2f (n2) ‚àí
 n2
n1
f (x) dx.
(5.307)
Since exp(‚àí2œÄin1) = 1, the difference between the integrals along the
imaginary axes above and below n1 is (exercise 5.40)
T1 =
 n1
n1+i‚àû
f (z)
e‚àí2œÄiz ‚àí1 dz ‚àí
 n1‚àíi‚àû
n1
f (z)
e2œÄiz ‚àí1 dz
(5.308)
= ‚àíi
 ‚àû
0
f (n1 + iy) ‚àíf (n1 ‚àíiy)
e2œÄy ‚àí1
dy.
(5.309)
Similarly, the difference between the integrals along the imaginary axes above
and below n2 is (exercise 5.41)
T2 =
 n2+i‚àû
n2
f (z)
e‚àí2œÄiz ‚àí1 dz ‚àí
 n2
n2‚àíi‚àû
f (z)
e2œÄiz ‚àí1 dz
(5.310)
= i
 ‚àû
0
f (n2 + iy) ‚àíf (n2 ‚àíiy)
e2œÄy ‚àí1
dy.
(5.311)
Since I+ ‚àíI‚àí= Tx + T1 + T2 = 0, we can use (5.307) and (5.309‚Äì5.311) to
build the Abel‚ÄìPlana formula (Whittaker and Watson, 1927, p. 145)
1
2f (n1) +
n2‚àí1

n=n1+1
f (n) + 1
2f (n2) ‚àí
 n2
n1
f (x) dx
= i
 ‚àû
0
f (n1 + iy) ‚àíf (n1 ‚àíiy) ‚àíf (n2 + iy) + f (n2 ‚àíiy)
e2œÄy ‚àí1
dy
(5.312)
(Niels Abel, 1802‚Äì1829; Giovanni Plana, 1781‚Äì1864).
In particular, if f (z) = z, the integral over y vanishes, and the Abel‚ÄìPlana
formula (5.312) gives
1
2n1 +
n2‚àí1

n=n1+1
n + 1
2n2 =
 n2
n1
x dx,
(5.313)
which is an example of the trapezoidal rule.
213

COMPLEX-VARIABLE THEORY
Example 5.37 (The Casimir effect)
The Abel‚ÄìPlana formula provides one of
the clearer formulations of the Casimir effect. We will assume that the hamilto-
nian for the electromagnetic Ô¨Åeld in empty space is a sum over two polarizations
and an integral over all momenta of a symmetric product
H0 =
2

s=1

¬Øh œâ(k) 1
2

a‚Ä†
s(k)as(k) + as(k)a‚Ä†
s(k)

d3k
(5.314)
of the annihilation and creation operators as(k) and a‚Ä†
s(k), which satisfy the
commutation relations
[as(k), a‚Ä†
s‚Ä≤(k‚Ä≤)] = Œ¥ss‚Ä≤ Œ¥(k ‚àík‚Ä≤) and [as(k), as‚Ä≤(k‚Ä≤)] = 0 = [a‚Ä†
s(k), a‚Ä†
s‚Ä≤(k‚Ä≤)]. (5.315)
The vacuum state |0‚ü©has no photons, and so on it as(k)|0‚ü©= 0 (and ‚ü®0|a‚Ä†
s(k) =
0). But because the operators in H0 are symmetrically ordered, the energy E0 of
the vacuum as given by (5.314) is not zero; instead it is quarticly divergent
E0 = ‚ü®0|H0|0‚ü©=
2

s=1

¬Øh œâ(k) 1
2Œ¥(0) d3k = V

¬Øh œâ(k) d3k
(2œÄ)3 ,
(5.316)
in which we used the delta function formula
Œ¥(k ‚àík‚Ä≤) =

e¬±i(k‚àík‚Ä≤)¬∑x d3x
(2œÄ)3
(5.317)
to identify Œ¥(0) as the volume V of empty space divided by (2œÄ)3. Since the pho-
ton has no mass, its (angular) frequency œâ(k) is c|k|, and so the energy density
E0/V is
E0
V = ¬Øhc

k3 dk
2œÄ2 = ¬Øhc K4
8œÄ2 =
¬Øhc
8œÄ2
1
d4 ,
(5.318)
in which we cut off the integral at some short distance d = K‚àí1 below which the
hamiltonian (5.314) and the commutation relations (5.315) are no longer valid.
But the energy density of empty space is
œÅc = 3H2
0/8œÄG ‚âà
¬Øhc
8œÄ2
1
(2.8 √ó 10‚àí5 m)4 ,
(5.319)
which corresponds to a distance scale d of 28 micrometers. Since quantum elec-
trodynamics works well down to about 10‚àí18 m, this distance scale is too big by
thirteen orders of magnitude.
If the Universe were inside an enormous, perfectly conducting, metal cube of
side L, then the tangential electric and normal magnetic Ô¨Åelds would vanish on
the surface of the cube Et(r, t) = 0 = Bn(r, t). The available wave-numbers of the
electromagnetic Ô¨Åeld inside the cube then would be kn = 2œÄ(n1, n2, n3)/L, and
the energy density would be
214

5.23 THE ABEL‚ÄìPLANA FORMULA AND THE CASIMIR EFFECT
E0
V = 2œÄ ¬Øhc
L4

n

n2.
(5.320)
The Casimir effect exploits the difference between the continuous (5.318) and
discrete (5.320) energy densities for the case of two metal plates of area A
separated by a short distance ‚Ñì‚â™
‚àö
A.
If the plates are good conductors, then at low frequencies the boundary con-
ditions Et(r, t) = 0 = Bn(r, t) hold, and the tangential electric and normal
magnetic Ô¨Åelds vanish on the surfaces of the metal plates. At high frequen-
cies, above the plasma frequency œâp of the metal, these boundary conditions
fail because the relative electric permittivity of the metal
œµ(œâ) ‚âà1 ‚àí
œâ2
p
œâ2

1 ‚àí
i
œâœÑ

(5.321)
has a positive real part. Here œÑ is the mean time between electron collisions.
The modes that satisfy the low-frequency boundary conditions Et(r, t) = 0 =
Bn(r, t) are (Bordag et al., 2009, p. 30)
œâ(k‚ä•, n) ‚â°c
&
k2
‚ä•+
œÄn
‚Ñì
2
(5.322)
where n ¬∑ k‚ä•= 0. The difference between the zero-point energies of these modes
and those of the continuous modes in the absence of the two plates per unit area
would be
E(‚Ñì)
A
= œÄ ¬Øhc
‚Ñì
 ‚àû
0
k‚ä•dk‚ä•
2œÄ
‚é°
‚é£
‚àû

n=0
(
‚Ñì2k2
‚ä•
œÄ2
+ n2 ‚àí
 ‚àû
0
(
‚Ñì2k2
‚ä•
œÄ2
+ x2 dx ‚àí‚Ñìk‚ä•
2œÄ
‚é§
‚é¶
(5.323)
if the boundary conditions held at all frequencies. With p = ‚Ñìk‚ä•/œÄ, we will
represent the failure of these boundary conditions at the plasma frequency œâp
by means of a cutoff function like c(n) = (1 + n/np)‚àí4 where np = œâp‚Ñì/œÄc. In
terms of such a cutoff function, the energy difference per unit area is
E(‚Ñì)
A
= œÄ2¬Øhc
2‚Ñì3
 ‚àû
0
p dp
 ‚àû

n=0
c(n)

p2 + n2 ‚àí
 ‚àû
0
c(x)

p2 + x2 dx ‚àíp
2

.
(5.324)
Since c(n) falls off as (np/n)4 for n ‚â´np, we may neglect terms in the sum and
integral beyond some integer M that is much larger than np
E(‚Ñì)
A
= œÄ2 ¬Øhc
2‚Ñì3
 ‚àû
0
p dp
 M

n=0
c(n)

p2 + n2 ‚àí
 M
0
c(x)

p2 + x2 dx ‚àíp
2

.
(5.325)
The function
f (z) = c(z)

p2 + z2 =

p2 + z2
(1 + z/np)4
(5.326)
215

COMPLEX-VARIABLE THEORY
is analytic in the right half-plane Re z = x > 0 (exercise 5.42) and tends to zero
limx‚Üí‚àû|f (x + iy)| ‚Üí0 as Re z = x ‚Üí‚àû. So we can apply the Abel‚ÄìPlana
formula (5.312) with n1 = 0 and n2 = M to the term in the square brackets in
(5.325) and get
E(‚Ñì)
A
= œÄ2 ¬Øhc
2‚Ñì3
 ‚àû
0
p dp
c(M)
2

p2 + M2
+i
 ‚àû
0
#
c(iy)

p2 + (œµ + iy)2 ‚àíc(‚àíiy)

p2 + (œµ ‚àíiy)2
‚àíc(M + iy)

p2 + (M + iy)2
+c(M ‚àíiy)

p2 + (M ‚àíiy)2
$
dy
e2œÄy ‚àí1
%
,
(5.327)
in which the inÔ¨Ånitesimal œµ reminds us that the contour lies inside the right half-
plane.
We now take advantage of the properties of the cutoff function c(z). Since
M
‚â´
np, we can neglect the term c(M)

p2 + M2/2. The denominator
exp(2œÄy) ‚àí1 also allows us to neglect the terms ‚àìc(M ‚àìiy)

p2 + (M ‚àìiy)2.
We are left with
E(‚Ñì)
A
= œÄ2¬Øhc
2‚Ñì3
 ‚àû
0
p dp
√ó i
 ‚àû
0
#
c(iy)

p2 + (œµ + iy)2 ‚àíc(‚àíiy)

p2 + (œµ ‚àíiy)2
$
dy
e2œÄy ‚àí1. (5.328)
Since the y integration involves the factor 1/(exp(2œÄy) ‚àí1), we can neglect the
detailed behavior of the cutoff functions c(iy) and c(‚àíiy) for y > np where they
differ appreciably from unity. The energy now is
E(‚Ñì)
A
= œÄ2 ¬Øhc
2‚Ñì3
 ‚àû
0
p dp
 ‚àû
0
i

p2 + (œµ + iy)2 ‚àí

p2 + (œµ ‚àíiy)2
e2œÄy ‚àí1
dy.
(5.329)
When y < p, the square-roots with the œµs cancel. But for y > p, they are

p2 ‚àíy2 ¬± 2iœµy = ¬±i

y2 ‚àíp2.
(5.330)
Their difference is 2i

y2 ‚àíp2, and so E(‚Ñì) is
E(‚Ñì)
A
= œÄ2 ¬Øhc
2‚Ñì3
 ‚àû
0
p dp
 ‚àû
0
‚àí2

y2 ‚àíp2 Œ∏(y ‚àíp)
e2œÄy ‚àí1
dy,
(5.331)
in which the Heaviside step function Œ∏(x) ‚â°(x + |x|)/(2|x|) keeps y > p
E(‚Ñì)
A
= ‚àíœÄ2¬Øhc
‚Ñì3
 y
0
p dp
 ‚àû
0

y2 ‚àíp2
e2œÄy ‚àí1 dy.
(5.332)
216

5.24 APPLICATIONS TO STRING THEORY
The p-integration is elementary, and so the energy difference is
E(‚Ñì)
A
= ‚àíœÄ2¬Øhc
3‚Ñì3
 ‚àû
0
y3 dy
e2œÄy ‚àí1 = ‚àíœÄ2 ¬Øhc
3‚Ñì3
B2
8 = ‚àíœÄ2 ¬Øhc
720 ‚Ñì3 ,
(5.333)
in which B2 is the second Bernoulli number (4.109). The pressure pushing the
plates together then is
p = ‚àí1
A
‚àÇE(‚Ñì)
‚àÇ‚Ñì
= ‚àíœÄ2¬Øhc
240 ‚Ñì4 ,
(5.334)
a result due to Casimir (Hendrik Brugt Gerhard Casimir, 1909‚Äì2000).
Although the Casimir effect is very attractive because of its direct connec-
tion with the symmetric ordering of the creation and annihilation operators
in the hamiltonian (5.314), the reader should keep in mind that neutral atoms
are mutually attractive, which is why most gases are diatomic, and that Lifshitz
explained the effect in terms of the mutual attraction of the atoms in the metal
plates (Lifshitz, 1956; Milonni and Shih, 1992) (Evgeny Mikhailovich Lifshitz,
1915‚Äì1985).
5.24 Applications to string theory
This section is optional on a Ô¨Årst reading.
String theory may or may not have anything to do with physics, but it does
provide many amusing applications of complex-variable theory. The coordi-
nates œÉ and œÑ of the world sheet of a string form a complex variable z = e2(œÑ‚àíiœÉ).
The product of two operators u(z) and v(w) often has poles in z ‚àíw as z ‚Üíw
but is well deÔ¨Åned if z and w are radially ordered
R {u(z)v(w)} ‚â°u(z) v(w) Œ∏(|z| ‚àí|w|) + v(w) u(z) Œ∏(|w| ‚àí|z|),
(5.335)
in which Œ∏(x) = (x + |x|)/2|x| is the step function. Since the modulus of z =
e2(œÑ‚àíiœÉ) depends only upon œÑ, radial order is time order in œÑz and œÑw.
The modes Ln of the principal component of the energy‚Äìmomentum tensor
T(z) are deÔ¨Åned by its Laurent series
T(z) =
‚àû

n=‚àí‚àû
Ln
zn+2
(5.336)
and the inverse relation
Ln =
1
2œÄi
)
zn+1 T(z) dz.
(5.337)
Thus the commutator of two modes involves two loop integrals
[Lm, Ln] =
# 1
2œÄi
)
zm+1 T(z) dz, 1
2œÄi
)
wn+1 T(w) dw
$
,
(5.338)
217

COMPLEX-VARIABLE THEORY
<
>
<
<
w
U(z) V(w)
|z| > |w|
V(w) U(z)
|w| > |z|
Radial order
Figure 5.10
The two counterclockwise circles about the origin preserve radial order
when z is near w by veering slightly to |z| > |w| for the product T(z)T(w) and to
|w| > |z| for the product T(w)T(z).
which we may deform as long as we cross no poles. Let‚Äôs hold w Ô¨Åxed and
deform the z loop so as to keep the Ts radially ordered when z is near w as
in Fig. 5.10. The operator-product expansion of the radially ordered product
R{T(z)T(w)} is
R{T(z)T(w)} =
c/2
(z ‚àíw)4 +
2
(z ‚àíw)2 T(w) +
1
z ‚àíwT‚Ä≤(w) + ¬∑ ¬∑ ¬∑ ,
(5.339)
in which the prime means derivative, c is a constant, and the dots denote terms
that are analytic in z and w. The commutator introduces a minus sign that
cancels most of the two contour integrals and converts what remains into an
integral along a tiny circle Cw about the point w as in Fig. 5.10
[Lm, Ln] =
) dw
2œÄiwn+1
)
Cw
dz
2œÄizm+1
#
c/2
(z ‚àíw)4 + 2T(w)
(z ‚àíw)2 + T‚Ä≤(w)
z ‚àíw
$
. (5.340)
After doing the z-integral, which is left as a homework exercise (5.43), one may
use the Laurent series (5.336) for T(w) to do the w-integral, which one may
choose to be along a tiny circle about w = 0, and so Ô¨Ånd the commutator
218

EXERCISES
[Lm, Ln] = (m ‚àín) Lm+n + c
12 m(m2 ‚àí1) Œ¥m+n,0
(5.341)
of the Virasoro algebra.
Exercises
5.1
Compute the two limits (5.6) and (5.7) of example 5.2 but for the function
f (x, y) = x2 ‚àíy2 + 2ixy. Do the limits now agree? Explain.
5.2
Show that if f (z) is analytic in a disk, then the integral of f (z) around a tiny
triangle of side œµ ‚â™1 inside the disk is zero to order œµ2.
5.3
Derive the two integral representations (5.46) for Bessel‚Äôs functions Jn(t) of the
Ô¨Årst kind from the integral formula (5.45). Hint: think of the integral (5.45)
as running from ‚àíœÄ to œÄ.
5.4
Do the integral
)
C
dz
z2 ‚àí1
in which the contour C is counterclockwise about the circle |z| = 2.
5.5
The function f (z) = 1/z is analytic in the region |z| > 0. Compute the integral
of f (z) counterclockwise along the unit circle z = eiŒ∏ for 0 ‚â§Œ∏ ‚â§2œÄ. The
contour lies entirely within the domain of analyticity of the function f (z).
Did you get zero? Why? If not, why not?
5.6
Let P(z) be the polynomial
P(z) = (z ‚àía1)(z ‚àía2)(z ‚àía3)
(5.342)
with roots a1, a2, and a3. Let R be the maximum of the three moduli |ak|. (a)
If the three roots are all different, evaluate the integral
I =
)
C
dz
P(z)
(5.343)
along the counterclockwise contour z = 2ReiŒ∏ for 0 ‚â§Œ∏ ‚â§2œÄ. (b) Same
exercise, but for a1 = a2 Ã∏= a3.
5.7
Compute the integral of the function f (z) = eaz/(z2 ‚àí3z + 2) along the
counterclockwise contour C‚ñ°that follows the perimeter of a square of side
6 centered at the origin. That is, Ô¨Ånd
I =
)
C‚ñ°
eaz
z2 ‚àí3z + 2 dz.
(5.344)
5.8
Use Cauchy‚Äôs integral formula (5.36) and Rodrigues‚Äôs expression (5.37) for
Legendre‚Äôs polynomial Pn(x) to derive SchlaeÔ¨Çi‚Äôs formula (5.38).
5.9
Use SchlaeÔ¨Çi‚Äôs formula (5.38) for the Legendre polynomials and Cauchy‚Äôs
integral formula (5.32) to compute the value of Pn(‚àí1).
5.10 Evaluate the counterclockwise integral around the unit circle |z| = 1
)
3 sinh2 2z ‚àí4 cosh3 z dz
z .
(5.345)
219

COMPLEX-VARIABLE THEORY
5.11 Evaluate the counterclockwise integral around the circle |z| = 2
)
z3
z4 ‚àí1 dz.
(5.346)
5.12 Evaluate the contour integral of the function f (z) = sin wz/(z ‚àí5)3 along the
curve z = 6 + 4(cos t + i sin t).
5.13 Evaluate the contour integral of the function f (z) = sin wz/(z ‚àí5)3 along the
curve z = ‚àí6 + 4(cos t + i sin t).
5.14 Is the function f (x, y) = x2 + iy2 analytic?
5.15 Is the function f (x, y) = x3 ‚àí3xy2 + 3ix2y ‚àíiy3 analytic? Is the function
x3 ‚àí3xy2 harmonic? Does it have a minimum or a maximum? If so, what
are they?
5.16 Is the function f (x, y) = x2 + y2 + i(x2 + y2) analytic? Is x2 + y2 a harmonic
function? What is its minimum, if it has one?
5.17 Derive the Ô¨Årst three nonzero terms of the Laurent series for f (z) = 1/(ez ‚àí1)
about z = 0.
5.18 Assume that a function g(z) is meromorphic in R and has a Laurent series
(5.97) about a point w ‚ààR. Show that as z ‚Üíw, the ratio g‚Ä≤(z)/g(z)
becomes (5.95).
5.19 Find the poles and residues of the functions 1/ sin z and 1/ cos z.
5.20 Derive the integral formula (5.122) from (5.121).
5.21 Show that if Re w < 0, then for arbitrary complex z
 ‚àû
‚àí‚àû
ew(x+z)2 dx =
& œÄ
‚àíw.
(5.347)
5.22 Use a ghost contour to evaluate the integral
 ‚àû
‚àí‚àû
x sin x
x2 + a2 dx.
Show your work; do not just quote the result of a commercial math program.
5.23 For a > 0 and b2 ‚àí4ac < 0, use a ghost contour to do the integral
 ‚àû
‚àí‚àû
dx
ax2 + bx + c.
(5.348)
5.24 Show that
 ‚àû
0
cos ax e‚àíx2 dx = 1
2
‚àöœÄ e‚àía2/4.
(5.349)
5.25 Show that
 ‚àû
‚àí‚àû
dx
1 + x4 = œÄ
‚àö
2
.
(5.350)
5.26 Evaluate the integral
 ‚àû
0
cos x
1 + x4 dx.
(5.351)
220

EXERCISES
5.27 Show that the Yukawa Green‚Äôs function (5.151) reproduces the Yukawa
potential (5.141) when n = 3. Use K1/2(x) = ‚àöœÄ/2x e‚àíx (9.99).
5.28 Derive the two explicit formulas (5.188) and (5.189) for the square-root of a
complex number.
5.29 What is (‚àíi)i? What is the most general value of this expression?
5.30 Use the indeÔ¨Ånite integral (5.223) to derive the principal-part formula (5.224).
5.31 The Bessel function Jn(x) is given by the integral
Jn(x) =
1
2œÄi
)
C
e(x/2)(z+1/z) dz
zn+1
(5.352)
along a counterclockwise contour about the origin. Find the generating func-
tion for these Bessel functions, that is, the function G(x, z) whose Laurent
series has the Jn(x)s as coefÔ¨Åcients
G(x, z) =
‚àû

n=‚àí‚àû
Jn(x) zn.
(5.353)
5.32 Show that the Heaviside function Œ∏(y) = (y+|y|)/(2|y|) is given by the integral
Œ∏(y) =
1
2œÄi
 ‚àû
‚àí‚àû
eiyx
dx
x ‚àíiœµ ,
(5.354)
in which œµ is an inÔ¨Ånitesimal positive number.
5.33 Show that the integral of exp(ik)/k along the contour from k = L to k =
L + iH and then to k = ‚àíL + iH and then down to k = ‚àíL vanishes in the
double limit L ‚Üí‚àûand H ‚Üí‚àû.
5.34 Use a ghost contour and a cut to evaluate the integral
I =
 1
‚àí1
dx
(x2 + 1)

1 ‚àíx2
(5.355)
by imitating example 5.30. Be careful when picking up the poles at z = ¬±i. If
necessary, use the explicit square-root formulas (5.188) and (5.189).
5.35 Redo the previous exercise (5.34) by deÔ¨Åning the square-roots so that the cuts
run from ‚àí‚àûto ‚àí1 and from 1 to ‚àû. Take advantage of the evenness of the
integrand and integrate on a contour that is slightly above the whole real axis.
Then add a ghost contour around the upper half-plane.
5.36 Show that if u is even and v is odd, then the Hilbert transforms (5.265) imply
(5.267).
5.37 Show why the principal-part identity (5.224) lets one write the Kramers‚Äì
Kronig integral (5.287) for the index of refraction in the regularized form
(5.292).
5.38 Use the formula (5.283) for the group velocity and the regularized expression
(5.292) for the real part of the index of refraction nr(œâ) to derive a formula
(5.293) for the group velocity.
5.39 Show that the quarter-circles of the Abel‚ÄìPlana contours C¬± contribute
1
2(f (n1) + f (n2)) to the sum S in the formula Tx = Ix + S.
221

COMPLEX-VARIABLE THEORY
5.40 Derive the integral formula (5.309) from (5.308).
5.41 Derive the integral formula (5.311) from (5.310).
5.42 Show that the function (5.326) is analytic in the RHP Re z > 0.
5.43 (a) Perform the z-integral in equation (5.340). (b) Use the result of part (a) to
Ô¨Ånd the commutator [Lm, Ln] of the Virasoro algebra. Hint: use the Laurent
series (5.336).
5.44 Assume that œµ(z) is analytic in a disk that contains a tiny circular contour Cw
about the point w as in Fig 5.10. Do the contour integral
)
Cw
œµ(z)
#
c/2
(z ‚àíw)4 + 2T(w)
(z ‚àíw)2 + T‚Ä≤(w)
z ‚àíw
$ dz
2œÄi
(5.356)
and express your result in terms of œµ(w), T(w), and their derivatives.
222

6
Differential equations
6.1 Ordinary linear differential equations
There are many kinds of differential equation ‚Äì linear and nonlinear, ordinary
and partial, homogeneous and inhomogeneous. Any way of correctly solving
any of them is Ô¨Åne. We start our overview with some deÔ¨Ånitions.
An operator of the form
L =
n

m=0
hm(x) dm
dxm
(6.1)
is an nth-order, ordinary, linear differential operator. It is nth order because the
highest derivative is dn/dxn. It is ordinary because all the derivatives are with
respect to the same independent variable x. It is linear because derivatives are
linear operators
L [a1f1(x) + a2f2(x)] = a1 L f1(x) + a2 L f2(x).
(6.2)
If all the hm(x) in L are constants, independent of x, then L is an nth-order,
ordinary, linear differential operator with constant coefÔ¨Åcients.
Example 6.1 (Second-order linear differential operators)
The operator
L = ‚àíd2
dx2 + k2
(6.3)
is a second-order, linear differential operator with constant coefÔ¨Åcients. The
second-order linear differential operator
L = ‚àíd
dx

p(x) d
dx

+ q(x)
(6.4)
is in self-adjoint form (section 6.27).
223

DIFFERENTIAL EQUATIONS
The differential equation L f (x) = 0 is homogeneous because each of its terms
is linear in f or one of its derivatives f (m) ‚Äì there is no term that is not propor-
tional to f or one of its derivatives. The equation Lf (x) = s(x) is inhomogeneous
because of the source term s(x).
If a differential equation is linear and homogeneous, then we can add solu-
tions. If f1(x) and f2(x) are two solutions of the same linear homogeneous
differential equation
L f1(x) = 0
and
L f2(x) = 0
(6.5)
then any linear combination of these solutions f (x) = a1f1(x) + a2f2(x) with
constant coefÔ¨Åcients a1 and a2 also is a solution since
L f (x) = L [a1f1(x) + a2f2(x)] = a1 L f1(x) + a2 L f2(x) = 0.
(6.6)
This additivity of solutions often makes it possible to Ô¨Ånd general solutions of
linear homogeneous differential equations.
Example 6.2 (Sines and cosines)
Two solutions of the second-order, linear,
homogeneous, ordinary differential equation (ODE)

d2
dx2 + k2

f (x) = 0
(6.7)
are sin kx and cos kx, and the most general solution is the linear combination
f (x) = a1 sin kx + a2 cos kx.
The functions y1(x), ..., yn(x) are linearly independent if the only numbers
k1, ..., kn for which the linear combination vanishes for all x
k1 y1(x) + k2 y2(x) + ¬∑ ¬∑ ¬∑ + kn yn(x) = 0
(6.8)
are k1 = ¬∑ ¬∑ ¬∑ = kn = 0. Otherwise they are linearly dependent.
Suppose that an nth-order homogeneous, linear, ordinary differential equa-
tion L f (x) = 0 has n linearly independent solutions fj(x), and that all other
solutions to this ODE are linear combinations of these n solutions. Then these n
solutions are complete in the space of solutions of this equation and form a basis
for this space. The general solution to L f (x) = 0 is then a linear combination of
the fjs with n arbitrary constant coefÔ¨Åcients
f (x) =
n

j=1
ajfj(x).
(6.9)
224

6.2 LINEAR PARTIAL DIFFERENTIAL EQUATIONS
With a source term s(x), the differential equation L f (x) = 0 becomes an
inhomogeneous linear ordinary differential equation
L fi(x) = s(x).
(6.10)
If fi1(x) and fi2(x) are any two solutions of this inhomogeneous differential
equation, then their difference fi1(x) ‚àífi2(x) is a solution of the associated
homogeneous equation L f (x) = 0
L [fi1(x) ‚àífi2(x)] = L fi1(x) ‚àíL fi2(x) = s(x) ‚àís(x) = 0.
(6.11)
Thus this difference must be given by the general solution (6.9) of the homoge-
neous equation for some constants aj
fi1(x) ‚àífi2(x) =
N

j=1
ajfj(x).
(6.12)
It follows therefore that every solution fi1(x) of the inhomogeneous differential
equation (6.10) is the sum of a particular solution fi2(x) of that equation and
some solution (6.9) of the associated homogeneous equation L f = 0
fi1(x) = fi2(x) +
N

j=1
ajfj(x).
(6.13)
In other words, the general solution of a linear inhomogeneous equation is a par-
ticular solution of that inhomogeneous equation plus the general solution of the
associated homogeneous equation.
A nonlinear differential equation is one in which a power f n(x) of the
unknown function or of one of its derivatives

f (k)(x)
n other than n = 1 or
n = 0 appears or in which the unknown function f appears in some other
nonlinear way. For instance, the equations
‚àíf ‚Ä≤‚Ä≤(x) = f 3(x),

f ‚Ä≤(x)
2 = f (x),
and
f ‚Ä≤(x) = e‚àíf (x)
(6.14)
are nonlinear differential equations. We can‚Äôt add two solutions of a nonlin-
ear equation and expect to get a third solution. Nonlinear equations are much
harder to solve.
6.2 Linear partial differential equations
An equation of the form
L f (x) =
n1,...,nk

m1,...,mk=0
gm1,...,mk(x)
‚àÇm1+¬∑¬∑¬∑+mk
‚àÇxm1
1 . . . ‚àÇxmk
k
f (x) = 0
(6.15)
225

DIFFERENTIAL EQUATIONS
in which x stands for x1, . . . , xk is a linear partial differential equation of order
n = n1 + ¬∑ ¬∑ ¬∑ + nk in the k variables x1, . . . , xk. (A partial differential equation
is a whole differential equation that has partial derivatives.)
Linear combinations of solutions of a linear homogeneous partial differen-
tial equation also are solutions of the equation. So if f1 and f2 are solutions of
L f = 0, and a1 and a2 are constants, then f = a1f1 + a2f2 is a solution since
L f = a1 L f1 + a2 L f2 = 0. Additivity of solutions is a property of all linear
homogeneous differential equations, whether ordinary or partial.
The general solution f (x) = f (x1, . . . , xk) of a linear homogeneous partial
differential equation (6.15) is a sum f (x) = 
j ajfj(x) over a complete set of
solutions fj(x) of the equation with arbitrary coefÔ¨Åcients aj. A linear partial
differential equation L fi(x) = s(x) with a source term s(x) = s(x1, . . . , xk) is an
inhomogeneous linear partial differential equation because of the added source
term.
Just as with ordinary differential equations, the difference fi1 ‚àífi2 of two
solutions of the inhomogeneous linear partial differential equation L fi = s is a
solution of the associated homogeneous equation L f = 0 (6.15)
L [fi1(x) ‚àífi2(x)] = s(x) ‚àís(x) = 0.
(6.16)
So we can expand this difference in terms of the complete set of solutions fj of
the inhomogeneous linear partial differential equation L f = 0
fi1(x) ‚àífi2(x) =

j
ajfj(x).
(6.17)
Thus the general solution of the inhomogeneous linear partial differential equa-
tion L f = s is the sum of a particular solution fi2 of L f = s and the general
solution 
j aj fj of the associated homogeneous equation (6.15)
fi1(x) = fi2(x) +

j
ajfj(x).
(6.18)
6.3 Notation for derivatives
One often uses primes or dots to denote derivatives as in
f ‚Ä≤ = df
dx
or
f ‚Ä≤‚Ä≤ = d2f
dx2
and
Àôf = df
dt
or
¬®f = d2f
dt2 .
For higher or partial derivatives, one sometimes uses superscripts
f (k) = dkf
dxk
and
f (k,‚Ñì) = ‚àÇk+‚Ñìf
‚àÇxk‚àÇy‚Ñì
(6.19)
226

6.3 NOTATION FOR DERIVATIVES
or subscripted letters, sometimes preceded by commas
fx = f,x = ‚àÇf
‚àÇx
and
fxyy = f,xyy =
‚àÇ3f
‚àÇx‚àÇy2
(6.20)
or subscripted indices, sometimes preceded by commas
f,k = ‚àÇkf = ‚àÇf
‚àÇxk
and
f,k‚Ñì= ‚àÇk‚àÇ‚Ñìf =
‚àÇ2f
‚àÇxk‚àÇx‚Ñì
(6.21)
where the independent variables are x = x1, . . . , xn.
In special relativity, one writes the time and space coordinates ct and x as x0,
x1, x2, and x3 or as the 4-vector (x0, x). To form the invariant inner product
px ‚â°x ¬∑ x ‚àíp0x0 = x ¬∑ x ‚àíEt as paxa with a summed from 0 to 3, one attaches
a minus sign to the time components of 4-vectors with lowered indices so that
p0 = ‚àíp0 and x0 = ‚àíx0. The derivatives ‚àÇaf and ‚àÇaf are
‚àÇaf = ‚àÇf
‚àÇxa
and
‚àÇaf = ‚àÇf
‚àÇxa
.
(6.22)
In rectangular coordinates, the gradient ‚àáof a scalar f is
‚àáf = (fx, fy, fz) = (f,x, f,y, f,z) = (‚àÇxf , ‚àÇyf , ‚àÇzf ) =
‚àÇf
‚àÇx, ‚àÇf
‚àÇy, ‚àÇf
‚àÇz

and the divergence of a vector v = (vx, vy, vz) is the scalar
‚àá¬∑ v = vx,x + vy,y + vz,z = ‚àÇxvx + ‚àÇyvy + ‚àÇzvz = ‚àÇvx
‚àÇx + ‚àÇvy
‚àÇy + ‚àÇvz
‚àÇz .
(6.23)
Physicists sometimes write the Laplacian ‚àá¬∑ ‚àáf as ‚àá2f or as ‚ñ≥f .
Example 6.3 (Laplace‚Äôs equation)
The equation for the electrostatic potential
in empty space is Laplace‚Äôs equation
L œÜ(x, y, z) = ‚àá¬∑ ‚àáœÜ(x, y, z) =

‚àÇ2
‚àÇx2 + ‚àÇ2
‚àÇy2 + ‚àÇ2
‚àÇz2

œÜ(x, y, z) = 0.
(6.24)
It is a second-order linear homogeneous partial differential equation.
Example 6.4 (Poisson‚Äôs equation)
Poisson‚Äôs equation for the electrostatic poten-
tial œÜ is
‚àí‚ñ≥œÜ(x, y, z) ‚â°‚àí

‚àÇ2
‚àÇx2 + ‚àÇ2
‚àÇy2 + ‚àÇ2
‚àÇz2

œÜ(x, y, z) = œÅ(x, y, z)
œµ0
,
(6.25)
in which œÅ is the charge density and œµ0 is the electric constant. It is a second-
order linear inhomogeneous partial differential equation.
227

DIFFERENTIAL EQUATIONS
6.4 Gradient, divergence, and curl
In cylindrical coordinates, the change dp in a physical point p due to changes
dœÅ, dœÜ, and dz in its coordinates is dp = ÀÜœÅ dœÅ+œÅ ÀÜœÜ dœÜ+ÀÜz dz. In spherical coordi-
nates, the change is dp = ÀÜr dr+r ÀÜŒ∏ dŒ∏ +r sin Œ∏ ÀÜœÜ dœÜ. In general orthogonal coor-
dinates, the change dp in a physical point p due to changes dui in its coordinates
is dp = h1 ÀÜe1 du1 +h2 ÀÜe2 du2 +h3 ÀÜe3 du3 where the basis vectors are orthonormal
ÀÜek¬∑ÀÜe‚Ñì= Œ¥k‚Ñì. In cylindrical coordinates, the scale factors are hœÅ = 1, hœÜ = œÅ, and
hz = 1, while in spherical coordinates they are hr = 1, hŒ∏ = r, and hœÜ = r sin Œ∏.
The dot-product of the gradient ‚àáf of a scalar function f with the change dp
in the point is the change df ‚àáf ¬∑ dp which is df = ‚àÇ1f du1 + ‚àÇ2f du2 + ‚àÇ3f du3.
So the gradient in orthogonal coordinates is
‚àáf = ÀÜe1
h1
‚àÇf
‚àÇu1
+ ÀÜe2
h2
‚àÇf
‚àÇu2
+ ÀÜe3
h3
‚àÇf
‚àÇu3
.
(6.26)
Thus the gradient in cylindrical coordinates is
‚àáf = ÀÜœÅ ‚àÇf
‚àÇœÅ +
ÀÜœÜ
œÅ
‚àÇf
‚àÇœÜ + ÀÜz ‚àÇf
‚àÇz
(6.27)
and in spherical coordinates it is
‚àáf = ÀÜr ‚àÇf
‚àÇr +
ÀÜŒ∏
r
‚àÇf
‚àÇŒ∏ +
ÀÜœÜ
r sin Œ∏
‚àÇf
‚àÇœÜ .
(6.28)
The divergence of a vector v at the center of a tiny cube is the surface integral
dS of v over the boundary ‚àÇdV of the cube divided by its tiny volume dV =
h1h2h3 du1du2du3. The surface integral dS is the sum of the differences of the
integrals of v1, v2, and v3 over the cube‚Äôs three pairs of opposite faces dS =
[‚àÇ(v1h2h3)/‚àÇu1 + ‚àÇ(v2h1h3)/‚àÇu2 + ‚àÇ(v3h1h2)/‚àÇu3] du1du2du3. So the divergence
‚àá¬∑ v is the ratio dS/dV, which is
‚àá¬∑ v =
1
h1h2h3
#‚àÇ(v1h2h3)
‚àÇu1
+ ‚àÇ(v2h1h3)
‚àÇu2
+ ‚àÇ(v3h1h2)
‚àÇu3
$
.
(6.29)
Thus the divergence in cylindrical coordinates is
‚àá¬∑ v = 1
œÅ
#‚àÇ(vœÅœÅ)
‚àÇœÅ
+ ‚àÇvœÜ
‚àÇœÜ + ‚àÇ(vzœÅ)
‚àÇz
$
= 1
œÅ
‚àÇ(œÅvœÅ)
‚àÇœÅ
+ 1
œÅ
‚àÇvœÜ
‚àÇœÜ + ‚àÇvz
‚àÇz
(6.30)
and in spherical coordinates it is
‚àá¬∑ v = 1
r2
‚àÇ(vr r2)
‚àÇr
+
1
r sin Œ∏
‚àÇ(vŒ∏ sin Œ∏)
‚àÇŒ∏
+
1
r sin Œ∏
‚àÇvœÜ
‚àÇœÜ .
(6.31)
By assembling a large number of tiny cubes, one may create a Ô¨Ånite volume
V. The integral of the divergence ‚àá¬∑ v over the tiny volumes dV of the tiny
cubes that make up the volume V is the sum of the surface integrals dS over
228

6.4 GRADIENT, DIVERGENCE, AND CURL
the faces of these tiny cubes. The integrals over the interior faces cancel leaving
just the surface integral over the boundary ‚àÇV of the Ô¨Ånite volume V. Thus we
arrive at Stokes‚Äôs theorem 
V
‚àá¬∑ v dV =

‚àÇV
v ¬∑ dS.
(6.32)
The Laplacian is the divergence (6.29) of the gradient (6.26). So in general
orthogonal coordinates it is
‚ñ≥f = ‚àá¬∑ ‚àáf =
1
h1h2h3
 3

k=1
‚àÇ
‚àÇuk

h1h2h3
h2
k
‚àÇf
‚àÇuk

.
(6.33)
Thus in cylindrical coordinates, the Laplacian is
‚ñ≥f = 1
œÅ

‚àÇ
‚àÇœÅ

œÅ ‚àÇf
‚àÇœÅ

+ 1
œÅ
‚àÇ2f
‚àÇœÜ2 + œÅ ‚àÇ2f
‚àÇz2

= 1
œÅ
‚àÇ
‚àÇœÅ

œÅ ‚àÇf
‚àÇœÅ

+ 1
œÅ2
‚àÇ2f
‚àÇœÜ2 + ‚àÇ2f
‚àÇz2
(6.34)
and in spherical coordinates it is
‚ñ≥f =
1
r2 sin Œ∏
# ‚àÇ
‚àÇr

r2 sin Œ∏ ‚àÇf
‚àÇr

+ ‚àÇ
‚àÇŒ∏

sin Œ∏ ‚àÇf
‚àÇŒ∏

+ ‚àÇ
‚àÇœÜ

1
sin Œ∏
‚àÇf
‚àÇœÜ
$
= 1
r2
‚àÇ
‚àÇr

r2 ‚àÇf
‚àÇr

+
1
r2 sin Œ∏
‚àÇ
‚àÇŒ∏

sin Œ∏ ‚àÇf
‚àÇŒ∏

+
1
r2 sin2 Œ∏
‚àÇ2f
‚àÇœÜ2 .
(6.35)
The area dS of a tiny square dS whose sides are the tiny perpendicular vectors
hiÀÜeidui and hjÀÜejduj (no sum) is their cross-product
dS = hiÀÜeidui √ó hjÀÜejduj = ÀÜek hihj duiduj,
(6.36)
in which the perpendicular unit vectors ÀÜei, ÀÜej, and ÀÜek obey the right-hand rule.
The dot-product of this area with the curl of a vector v, which is (‚àá√ó v) ¬∑ dS =
(‚àá√ó v)k hihj duiduj, is the line integral dL of v along the boundary ‚àÇdS of the
square
(‚àá√ó v)k hihjduiduj =

‚àÇi(hjvj) ‚àí‚àÇj(hivi)

duiduj.
(6.37)
Thus the kth component of the curl is
(‚àá√ó v)k =
1
hihj
‚àÇ(hjvj)
‚àÇui
‚àí‚àÇ(hivi)
‚àÇuj

(no sum).
(6.38)
In terms of the Levi-Civita symbol œµijk, which is totally antisymmetric with
œµ123 = 1, the curl is
‚àá√ó v = 1
2
3

i,j,k=1
œµijk
ÀÜek
hihj
#‚àÇ(hjvj)
‚àÇui
‚àí‚àÇ(hivi)
‚àÇuj
$
=
3

i,j,k=1
œµijk
ÀÜek
hihj
‚àÇ(hjvj)
‚àÇui
,
(6.39)
in which the sums over i, j, and k run from 1 to 3. In rectangular coordinates,
each scale factor hi = 1, and the ith component of ‚àá√ó v is
229

DIFFERENTIAL EQUATIONS
(‚àá√ó v)i =
3

j, k=1
œµijk
‚àÇvk
‚àÇxj
=
3

j, k=1
œµijk‚àÇjvk
(6.40)
or (‚àá√ó v)i = œµijk‚àÇjvk if we sum implicitly over j and k.
We can write the curl as a determinant
‚àá√ó v =
1
h1h2h3

h1ÀÜe1
h2ÀÜe2
h3ÀÜe3
‚àÇ1
‚àÇ2
‚àÇ3
h1v1
h2v2
h3v3

.
(6.41)
Thus in cylindrical coordinates, where h1 = 1, h2 = œÅ, and h3 = 1, the curl is
‚àá√ó v = 1
œÅ

ÀÜœÅ
œÅ ÀÜœÜ
ÀÜz
‚àÇœÅ
‚àÇœÜ
‚àÇz
vœÅ
œÅvœÜ
vz

(6.42)
and in spherical coordinates, where h1 = 1, h2 = r, and h3 = r sin Œ∏, it is
‚àá√ó v =
1
r2 sin Œ∏

ÀÜr
r ÀÜŒ∏
r sin Œ∏ ÀÜœÜ
‚àÇr
‚àÇŒ∏
‚àÇœÜ
vr
r vŒ∏
r sin Œ∏ vœÜ

.
(6.43)
By assembling a large number of tiny squares, one may create an arbitrary
Ô¨Ånite surface S. The surface integral of the curl ‚àá√ó v over the tiny squares dS
that make up the surface S is the sum of the line integrals dL over the sides of
these tiny squares. The line integrals over the interior sides cancel leaving just
the line integral along the boundary ‚àÇS of the Ô¨Ånite surface S. Thus we arrive
at Stokes‚Äôs theorem

S
‚àá√ó v ¬∑ dS =

‚àÇS
v ¬∑ d‚Ñì.
(6.44)
6.5 Separable partial differential equations
A linear partial differential equation (PDE) is separable if it can be decom-
posed into ordinary differential equations (ODEs). One then Ô¨Ånds solutions to
the ODEs and thus to the original PDE. The general solution to the PDE is
then a sum over all of its linearly independent solutions with arbitrary coefÔ¨Å-
cients. Sometimes the separability of a differential operator or of a differential
equation depends upon the choice of coordinates.
Example 6.5 (The Helmholtz equation in two dimensions)
In several coordi-
nate systems, one can convert Helmholtz‚Äôs linear homogeneous partial differen-
tial equation ‚àí‚àá¬∑‚àáf (x) = ‚àí‚ñ≥f (x) = k2f (x) into ordinary differential equations
by writing the function f (x) as a product of functions of a single variable.
In two dimensions and in rectangular coordinates (x, y), the function f (x, y) =
X(x) Y(y) is a solution of the Helmholtz equation as long as X and Y satisfy
230

6.5 SEPARABLE PARTIAL DIFFERENTIAL EQUATIONS
‚àíX‚Ä≤‚Ä≤
a (x) = a2Xa(x) and ‚àíY‚Ä≤‚Ä≤
b (y) = b2Yb(y) with a2 + b2 = k2. One sets Xa(x) =
Œ± sin ax + Œ≤ cos ax with a similar equation for Yb(y). Any linear combination of
the functions Xa(x) Yb(y) with a2 + b2 = k2 will be a solution of Helmholtz‚Äôs
equation ‚àí‚ñ≥f = k2f .
The z-independent part of (6.34) is the Laplacian in polar coordinates
‚àá¬∑ ‚àáf = ‚ñ≥f = ‚àÇ2f
‚àÇœÅ2 + 1
œÅ
‚àÇf
‚àÇœÅ + 1
œÅ2
‚àÇ2f
‚àÇœÜ2 ,
(6.45)
in which Helmholtz‚Äôs equation ‚àí‚ñ≥f = k2f also is separable. We let f (œÅ, œÜ) =
P(œÅ) (œÜ) and get P‚Ä≤‚Ä≤ + P‚Ä≤/œÅ + P‚Ä≤‚Ä≤/œÅ2 = ‚àík2P . Multiplying both sides
by œÅ2/P , we have
œÅ2 P‚Ä≤‚Ä≤
P + œÅ P‚Ä≤
P + œÅ2k2 = ‚àí‚Ä≤‚Ä≤
 = n2,
(6.46)
in which the Ô¨Årst three terms are functions of œÅ, the fourth term ‚àí‚Ä≤‚Ä≤/ is a
function of œÜ, and the last term n2 is a constant. The constant n must be an
integer if n(œÜ) = a sin(n œÜ) + b cos(n œÜ) is to be single valued on the interval
[0, 2œÄ]. The function Pk,n(œÅ) = Jn(kœÅ) satisÔ¨Åes
œÅ2P‚Ä≤‚Ä≤
k,n + œÅP‚Ä≤
k,n + œÅ2k2Pk,n = n2Pk,n,
(6.47)
because the Bessel function of the Ô¨Årst kind Jn(x) satisÔ¨Åes
x2J‚Ä≤‚Ä≤
n + xJ‚Ä≤
n + x2Jn = n2Jn,
(6.48)
which is Bessel‚Äôs equation (9.4) (Friedrich Bessel, 1784‚Äì1846). So the product
fk,n(œÅ, œÜ) = Pk,n(œÅ) n(œÜ) is a solution to Helmholtz‚Äôs equation ‚àí‚ñ≥f = k2f , as
is any linear combination of such products for different ns.
Example 6.6 (The Helmholtz equation in three dimensions)
In three dimen-
sions and in rectangular coordinates r = (x, y, z), the function f (x, y, z) =
X(x)Y(y)Z(z) is a solution of the ODE ‚àí‚ñ≥f = k2f as long as X, Y, and Z
satisfy ‚àíX‚Ä≤‚Ä≤
a = a2Xa, ‚àíY‚Ä≤‚Ä≤
b = b2Yb, and ‚àíZ‚Ä≤‚Ä≤
c = c2Zc with a2 + b2 + c2 = k2. We
set Xa(x) = Œ± sin ax + Œ≤ cos ax and so forth. Arbitrary linear combinations of
the products Xa Yb Zc also are solutions of Helmholtz‚Äôs equation ‚àí‚ñ≥f = k2f
as long as a2 + b2 + c2 = k2.
In cylindrical coordinates (œÅ, œÜ, z), the Laplacian (6.34) is
‚àá¬∑ ‚àáf = ‚ñ≥f = 1
œÅ
#
œÅ f,œÅ

,œÅ + 1
œÅ f,œÜœÜ + œÅ f,zz
$
(6.49)
and so if we substitute f (œÅ, œÜ, z) = P(œÅ) (œÜ) Z(z) into Helmholtz‚Äôs equation
‚àí‚ñ≥f = Œ±2f and multiply both sides by ‚àíœÅ2/P  Z, then we get
œÅ2
f ‚ñ≥f = œÅ2P‚Ä≤‚Ä≤ + œÅP‚Ä≤
P
+ ‚Ä≤‚Ä≤
 + œÅ2 Z‚Ä≤‚Ä≤
Z = ‚àíŒ±2œÅ2.
(6.50)
231

DIFFERENTIAL EQUATIONS
If we set Zk(z) = ekz, then this equation becomes (6.46) with k2 replaced by
Œ±2 + k2. Its solution then is
f (œÅ, œÜ, z) = Jn(

Œ±2 + k2œÅ) einœÜ ekz,
(6.51)
in which n must be an integer if the solution is to apply to the full range of
œÜ from 0 to 2œÄ. The case in which Œ± = 0 corresponds to Laplace‚Äôs equation
with solution f (œÅ, œÜ, z) = Jn(kœÅ)einœÜekz. We could have required Z to satisfy
Z‚Ä≤‚Ä≤ = ‚àík2Z. The solution (6.51) then would be
f (œÅ, œÜ, z) = Jn(

Œ±2 ‚àík2 œÅ) einœÜ eikz.
(6.52)
But if Œ±2 ‚àík2 < 0, we write this solution in terms of the modiÔ¨Åed Bessel function
In(x) = i‚àínJn(ix) (section 9.3) as
f (œÅ, œÜ, z) = In(

k2 ‚àíŒ±2 œÅ) einœÜ eikz.
(6.53)
In spherical coordinates, the Laplacian (6.35) is
‚ñ≥f = 1
r2
‚àÇ
‚àÇr

r2 ‚àÇf
‚àÇr

+
1
r2 sin Œ∏
‚àÇ
‚àÇŒ∏

sin Œ∏ ‚àÇf
‚àÇŒ∏

+
1
r2 sin2 Œ∏
‚àÇ2f
‚àÇœÜ2 .
(6.54)
If we set f (r, Œ∏, œÜ) = R(r) (Œ∏) m(œÜ) where m = eimœÜ and multiply both sides
of the Helmholtz equation ‚àí‚ñ≥f = k2f by ‚àír2/R, then we get

r2R‚Ä≤‚Ä≤
R
+

sin Œ∏ ‚Ä≤‚Ä≤
sin Œ∏ 
‚àí
m2
sin2 Œ∏
= ‚àík2.
(6.55)
The Ô¨Årst term is a function of r, the next two terms are functions of Œ∏, and the
last term is a constant. So we set the r-dependent terms equal to a constant
‚Ñì(‚Ñì+ 1) ‚àík2 and the Œ∏-dependent terms equal to ‚àí‚Ñì(‚Ñì+ 1), and we require the
associated Legendre function ‚Ñì,m(Œ∏) to satisfy (8.91)

sin Œ∏ ‚Ä≤
‚Ñì,m
‚Ä≤ / sin Œ∏ +

‚Ñì(‚Ñì+ 1) ‚àím2/ sin2 Œ∏

‚Ñì,m = 0.
(6.56)
If (œÜ) = eimœÜ is to be single valued for 0 ‚â§œÜ ‚â§2œÄ, then the parameter m must
be an integer. As we‚Äôll see in chapter 8, the constant ‚Ñìalso must be an integer
with ‚àí‚Ñì‚â§m ‚â§‚Ñìif ‚Ñì,m(Œ∏) is to be single valued and Ô¨Ånite for 0 ‚â§Œ∏ ‚â§œÄ. The
product f = R   then will obey Helmholtz‚Äôs equation ‚àí‚ñ≥f = k2f if the radial
function Rk,‚Ñì(r) = j‚Ñì(kr) satisÔ¨Åes
r2R‚Ä≤‚Ä≤
k,‚Ñì+ 2rR‚Ä≤
k,‚Ñì+

k2r2 ‚àí‚Ñì(‚Ñì+ 1)

Rk,‚Ñì= 0,
(6.57)
which it does because the spherical Bessel function j‚Ñì(x) obeys Bessel‚Äôs equation
(9.63)
x2 j‚Ä≤‚Ä≤
‚Ñì+ 2x j‚Ä≤
‚Ñì+ [x2 ‚àí‚Ñì(‚Ñì+ 1)] j‚Ñì= 0.
(6.58)
In three dimensions, Helmholtz‚Äôs equation separates in 11 standard coordinate
systems (Morse and Feshbach, 1953, pp. 655‚Äì664).
232

6.6 WAVE EQUATIONS
6.6 Wave equations
You can easily solve some of the linear homogeneous partial differential
equations of electrodynamics (Exercise 6.6) and quantum Ô¨Åeld theory.
Example 6.7 (The Klein‚ÄìGordon equation)
In Minkowski space, the analog of
the Laplacian in natural units (¬Øh = c = 1) is (summing over a from 0 to 3)
2 = ‚àÇa‚àÇa = ‚ñ≥‚àí
‚àÇ2
‚àÇx02 = ‚ñ≥‚àí‚àÇ2
‚àÇt2
(6.59)
and the Klein‚ÄìGordon wave equation is

2 ‚àím2
A(x) =

‚ñ≥‚àí‚àÇ2
‚àÇt2 ‚àím2

A(x) = 0.
(6.60)
If we set A(x) = B(px) where px = paxa = p ¬∑ x ‚àíp0x0, then the kth partial
derivative of A is pk times the Ô¨Årst derivative of B
‚àÇ
‚àÇxk A(x) =
‚àÇ
‚àÇxk B(px) = pkB‚Ä≤(px)
(6.61)
and so the Klein‚ÄìGordon equation (6.60) becomes

2 ‚àím2
A = (p2 ‚àí(p0)2)B‚Ä≤‚Ä≤ = p2B‚Ä≤‚Ä≤ ‚àím2B = 0,
(6.62)
in which p2 = p2 ‚àí(p0)2. Thus if B(p ¬∑ x) = exp(ip ¬∑ x) so that B‚Ä≤‚Ä≤ = ‚àíB,
and if the energy‚Äìmomentum 4-vector (p0, p) satisÔ¨Åes p2 + m2 = 0, then A(x)
will satisfy the Klein‚ÄìGordon equation. The condition p2 + m2 = 0 relates the
energy p0 =

p2 + m2 to the momentum p for a particle of mass m.
Example 6.8 (Field of a spinless boson)
The quantum Ô¨Åeld
œÜ(x) =

d3p

2p0(2œÄ)3

a(p)eipx + a‚Ä†(p)e‚àíipx
(6.63)
describes spinless bosons of mass m. It satisÔ¨Åes the Klein‚ÄìGordon equation

2 ‚àím2
œÜ(x) = 0 because p0 =

p2 + m2. The operators a(p) and a‚Ä†(p)
respectively represent the annihilation and creation of the bosons and obey the
commutation relations
[a(p), a‚Ä†(p‚Ä≤)] = Œ¥3(p ‚àíp‚Ä≤) and [a(p), a(p‚Ä≤)] = [a‚Ä†(p), a‚Ä†(p‚Ä≤)] = 0
(6.64)
in units with ¬Øh = c = 1. These relations make the Ô¨Åeld œÜ(x) and its time
derivative ÀôœÜ(y) satisfy the canonical equal-time commutation relations
[œÜ(x, t), ÀôœÜ(y, t)] = i Œ¥3(x ‚àíy) and [œÜ(x, t), œÜ(y, t)] = [ ÀôœÜ(x, t), ÀôœÜ(y, t)] = 0, (6.65)
in which the dot means time derivative.
233

DIFFERENTIAL EQUATIONS
Example 6.9 (Field of the photon)
The electromagnetic Ô¨Åeld has four compo-
nents, but in the Coulomb or radiation gauge ‚àá¬∑ A(x) = 0, the component A0
is a function of the charge density, and the vector potential A in the absence
of charges and currents satisÔ¨Åes the wave equation 2A(x) = 0 for a spin-one
massless particle. We write it as
A(x) =
2

s=1

d3p

2p0(2œÄ)3

e(p, s) a(p, s) eipx + e‚àó(p, s) a‚Ä†(p, s) e‚àíipx
,
(6.66)
in which the sum is over the two possible polarizations s. The energy p0 is equal
to the modulus |p| of the momentum because the photon is massless, p2 = 0.
The dot-product of the polarization vectors e(p, s) with the momentum vanishes
p¬∑e(p, s) = 0 so as to respect the gauge condition ‚àá¬∑ A(x) = 0. The annihilation
and creation operators obey the commutation relations
[a(p, s), a‚Ä†(p‚Ä≤, s‚Ä≤)] = Œ¥3(p ‚àíp‚Ä≤) Œ¥s,s‚Ä≤
[a(p, s), a(p‚Ä≤, s‚Ä≤)] = [a‚Ä†(p, s), a‚Ä†(p‚Ä≤, s‚Ä≤)] = 0
(6.67)
but the commutation relations of the vector potential A(x) involve the transverse
delta function

Ai(t, x), ÀôAj(t, y)

= iŒ¥ijŒ¥(3)(x ‚àíy) + i
‚àÇ2
‚àÇxi‚àÇxj
1
4œÄ|x ‚àíy|
= i

eik¬∑(x‚àíy)

Œ¥ij ‚àíkikj
k2
 d3k
(2œÄ)3
(6.68)
because of the Coulomb-gauge condition ‚àá¬∑ A(x) = 0.
Example 6.10 (Dirac‚Äôs equation)
Fields œáb(x) that describe particles of spin
one-half have four components, b = 1, . . . , 4. In the absence of interactions,
they satisfy the Dirac equation

Œ≥a
bc‚àÇa + mŒ¥bc

œác(x) = 0,
(6.69)
in which repeated indices are summed over ‚Äì b, c from 1 to 4 and a from 0 to 3.
In matrix notation, the Dirac equation is

Œ≥a‚àÇa + m

œá(x) = 0.
(6.70)
The four Dirac gamma matrices are deÔ¨Åned by the 16 rules
{Œ≥a, Œ≥b} ‚â°Œ≥aŒ≥b + Œ≥bŒ≥a = 2Œ∑ab,
(6.71)
in which Œ∑ is the 4 √ó 4 diagonal matrix Œ∑00 = Œ∑00 = ‚àí1 and Œ∑bc = Œ∑bc = Œ¥bc for
b, c = 1, 2, or 3.
If œÜ(x) is a 4-component Ô¨Åeld that satisÔ¨Åes the Klein‚ÄìGordon equation (2 ‚àí
m2)œÜ = 0, then the Ô¨Åeld œá(x) = (Œ≥b‚àÇb ‚àím)œÜ(x) satisÔ¨Åes (exercise 6.7) the Dirac
equation (6.70)
234

6.8 SEPARABLE FIRST-ORDER DIFFERENTIAL EQUATIONS

Œ≥a‚àÇa + m

œá(x) =

Œ≥a‚àÇa + m

(Œ≥b‚àÇb ‚àím)œÜ(x)
=

Œ≥aŒ≥b‚àÇa‚àÇb ‚àím2
œÜ(x)
=

1
2

{Œ≥a, Œ≥b} + [Œ≥a, Œ≥b]

‚àÇa‚àÇb ‚àím2
œÜ(x)
=

Œ∑ab‚àÇa‚àÇb ‚àím2
œÜ(x) = (2 ‚àím2)œÜ(x) = 0.
The simplest Dirac Ô¨Åeld is the Majorana Ô¨Åeld
œáb(x) =

d3p
(2œÄ)3/2

s

ub(p, s) a(p, s)eipx + vb(p, s) a‚Ä†(p, s)e‚àíipx
(6.72)
in which p0 =

p2 + m2, s labels the two spin states, and the operators a and a‚Ä†
obey the anticommutation relations
{a(p, s), a‚Ä†(p‚Ä≤, s‚Ä≤)} ‚â°a(p, s) a‚Ä†(p‚Ä≤, s‚Ä≤) + a‚Ä†(p‚Ä≤, s‚Ä≤) a(p, s) = Œ¥ss‚Ä≤ Œ¥(p ‚àíp‚Ä≤),
{a(p, s), a(p‚Ä≤, s‚Ä≤)} = {a‚Ä†(p, s), a‚Ä†(p‚Ä≤, s‚Ä≤)} = 0.
(6.73)
It describes a neutral particle of mass m.
If two Majorana Ô¨Åelds œá1 and œá2 represent particles of the same mass, then
one may combine them into one Dirac Ô¨Åeld
œà(x) =
1
‚àö
2
[œá1(x) + iœá2(x)] ,
(6.74)
which describes a charged particle such as a quark or a lepton.
6.7 First-order differential equations
The equation
dy
dx = f (x, y) = ‚àíP(x, y)
Q(x, y)
(6.75)
or system
P(x, y) dx + Q(x, y) dy = 0
(6.76)
is a Ô¨Årst-order ordinary differential equation.
6.8 Separable Ô¨Årst-order differential equations
If in a Ô¨Årst-order ordinary differential equation like (6.76) one can separate the
dependent variable y from the independent variable x
F(x) dx + G(y) dy = 0
(6.77)
then the equation (6.76) is separable and (6.77) is separated.
235

DIFFERENTIAL EQUATIONS
Once the variables are separated, one can integrate and so obtain an
equation, called the general integral
0 =
 x
x0
F(x‚Ä≤) dx‚Ä≤ +
 y
y0
G(y‚Ä≤) dy‚Ä≤
(6.78)
relating y to x and providing a solution y(x) of the differential equation.
Example 6.11 (Zipf‚Äôs law)
In 1913, Auerbach noticed that many quantities are
distributed as (Gell-Mann, 1994, pp. 92‚Äì100)
dn = ‚àía dx
xk+1
(6.79)
an ODE that is separable and separated. For k Ã∏= 0, we may integrate this to
n + c = a/kxk or
x =

a
k(n + c)
1/k
(6.80)
in which c is a constant.
The case k = 1 occurs frequently x = a/(n + c) and is called Zipf‚Äôs law. With
c = 0, it applies approximately to the populations of cities: if the largest city
(n = 1) has population x, then the populations of the second, third, and fourth
cities (n = 2, 3, 4) will be x/2, x/3, and x/4.
Again with c = 0, Zipf‚Äôs law applies to the occurrence of numbers x in a
table of some sort. Since x = a/n, the rank n of the number x is approximately
n = a/x. So the number of numbers that occur with Ô¨Årst digit d and, say, 4
trailing digits will be
n(d0000) ‚àín(d9999) = a

1
d0000 ‚àí
1
d9999

= a

9999
d0000 √ó d9999

‚âàa

104
d(d + 1) 108

=
a 10‚àí4
d(d + 1).
(6.81)
The ratio of the number of numbers with Ô¨Årst digit d to the number with Ô¨Årst
digit d‚Ä≤ is then d‚Ä≤(d‚Ä≤ +1)/d(d +1). For example, the Ô¨Årst digit is more likely to be
1 than 9 by a factor of 45. The German government uses such formulas to catch
tax evaders.
Example 6.12 (The logistic equation)
dy
dt = ay

1 ‚àíy
Y

(6.82)
is separable and separated. It describes a wide range of phenomena whose evolu-
tion with time t is sigmoidal such as (Gell-Mann, 2008) the cumulative number
of casualties in a war, the cumulative number of deaths in London‚Äôs great plague,
236

6.8 SEPARABLE FIRST-ORDER DIFFERENTIAL EQUATIONS
and the cumulative number of papers in an academic‚Äôs career. It also describes
the effect y on an animal of a given dose t of a drug.
With f = y/Y, the logistic equation (6.82) is Àôf = af (1 ‚àíf ) or
a dt =
df
f (1 ‚àíf ) = df
f +
df
1 ‚àíf ,
(6.83)
which we may integrate to a(t ‚àíth) = ln [f /(1 ‚àíf )]. Taking the exponential of
both sides, we Ô¨Ånd exp[a(t ‚àíth)] = f /(1 ‚àíf ), which we can solve for f
f (t) =
ea(t‚àíth)
1 + ea(t‚àíth) .
(6.84)
The sigmoidal shape of f (t) is like a smoothed Heaviside function.
Example 6.13 (Lattice QCD)
In lattice Ô¨Åeld theory, the beta function
Œ≤(g) ‚â°‚àídg
d ln a
(6.85)
tells us how we must adjust the coupling constant g in order to keep the physical
predictions of the theory constant as we vary the lattice spacing a. In quantum
chromodynamics Œ≤(g) = ‚àíŒ≤0 g3 ‚àíŒ≤1 g5 + ¬∑ ¬∑ ¬∑ where
Œ≤0 =
1
(4œÄ)2

11 ‚àí2
3 nf

and Œ≤1 =
1
(4œÄ)4

102 ‚àí10 nf ‚àí8
3nf

,
(6.86)
in which nf is the number of light quark Ô¨Çavors. Combining the deÔ¨Ånition (6.85)
of the Œ≤-function with the Ô¨Årst term of its expansion Œ≤(g) = ‚àíŒ≤0 g3 for small g,
one arrives at the differential equation
dg
d ln a = Œ≤0 g3,
(6.87)
which one may integrate

d ln a = ln a + c =

dg
Œ≤0g3 = ‚àí
1
2Œ≤0g2
(6.88)
to Ô¨Ånd
 a(g) = e‚àí1/2Œ≤0g2,
(6.89)
in which  is a constant of integration. As g approaches 0, which is an essen-
tial singularity (section 5.11), the lattice spacing a(g) goes to zero very fast (as
long as nf ‚â§16). The inverse of this relation g(a) ‚âà1/

Œ≤0 ln(1/a22) shows
that the coupling constant g(a) slowly goes to zero as the lattice spacing (or
shortest wave-length) a goes to zero. The strength of the interaction shrinks
logarithmically as the energy 1/a increases in this lattice version of asymptotic
freedom.
237

DIFFERENTIAL EQUATIONS
6.9 Hidden separability
As long as each of the functions P(x, y) and Q(x, y) in the ODE
P(x, y)dx + Q(x, y)dy = U(x)V(y)dx + R(x)S(y)dy = 0
(6.90)
can be factored P(x, y) = U(x)V(y) and Q(x, y) = R(x)S(y) into the prod-
uct of a function of x times a function of y, then the ODE is separable.
Following Ince (1956), we divide the ODE by R(x)V(y), separate the variables
U(x)
R(x) dx + S(y)
V(y) dy = 0,
(6.91)
and integrate
 U(x)
R(x) dx +

S(y)
V(y) dy = C,
(6.92)
in which C is a constant of integration.
Example 6.14 (Hidden separability)
We separate the variables in
x(y2 ‚àí1) dx ‚àíy(x2 ‚àí1) dy = 0
(6.93)
by dividing by (y2 ‚àí1)(x2 ‚àí1) so as to get
x
x2 ‚àí1 dx ‚àí
y
y2 ‚àí1 dy = 0.
(6.94)
Integrating, we Ô¨Ånd ln(x2 ‚àí1) ‚àíln(y2 ‚àí1) = ‚àíln C or C (x2 ‚àí1) = y2 ‚àí1,
which we solve for y(x) =

1 + C(x2 ‚àí1).
6.10 Exact Ô¨Årst-order differential equations
The differential equation
P(x, y) dx + Q(x, y) dy = 0
(6.95)
is exact if its left-hand side is the differential of some function œÜ(x, y)
P dx + Q dy = dœÜ = œÜx dx + œÜy dy.
(6.96)
We‚Äôll have more to say about the exterior derivative d in section 12.2.
The criteria of exactness are
P(x, y) = ‚àÇœÜ(x, y)
‚àÇx
‚â°œÜx(x, y) and Q(x, y) = ‚àÇœÜ(x, y)
‚àÇy
‚â°œÜy(x, y).
(6.97)
238

6.10 EXACT FIRST-ORDER DIFFERENTIAL EQUATIONS
Thus, if the ODE (6.95) is exact, then
Py(x, y) = œÜyx(x, y) = œÜxy(x, y) = Qx(x, y),
(6.98)
which is called the condition of integrability. This condition implies that the
ODE (6.95) is exact and integrable, as we‚Äôll see in section 6.11.
A Ô¨Årst-order ODE that is separable and separated
P(x)dx + Q(y)dy = 0
(6.99)
is exact because
Py = 0 = Qx.
(6.100)
But a Ô¨Årst-order ODE may be exact without being separable.
Example 6.15 (Boyle‚Äôs law)
At a Ô¨Åxed temperature T, changes in the pressure
P and volume V of an ideal gas are related by
PdV + VdP = 0.
(6.101)
This ODE is exact because PdV + VdP = d(PV). Its integrated form is the
ideal-gas law
PV = NkT,
(6.102)
in which N is the number of molecules in the gas and k is Boltzmann‚Äôs constant,
k = 1.38066 √ó 10‚àí23 J/K = 8.617385√ó10‚àí5 eV/K.
Incidentally, a more accurate formula, proposed by van der Waals (1837‚Äì1923)
in his doctoral thesis in 1873, is

P +
N
V
2
a‚Ä≤


V ‚àíNb‚Ä≤
= NkT,
(6.103)
in which a‚Ä≤ represents the mutual attraction of the molecules and has the dimen-
sions of energy times volume and b‚Ä≤ is the effective volume of a single molecule.
This equation was one of many signs that molecules were real particles, inde-
pendent of the imagination of chemists. Lamentably, most physicists refused
to accept the reality of molecules until 1905 when Einstein related the viscous-
friction coefÔ¨Åcient Œ∂ and the diffusion constant D to the energy kT of a thermal
Ô¨Çuctuation by the equation Œ∂ D = kT, as explained in section 13.9 (Albert
Einstein, 1879‚Äì1955).
Example 6.16 (Human population growth)
If the number of people rises as the
square of the population, then ÀôN = N2/b. The separated and hence exact form
of this differential equation is
dN
N2 = dt
b ,
(6.104)
239

DIFFERENTIAL EQUATIONS
which we integrate to N(t) = b/(T ‚àít) where T is the time at which the popula-
tion becomes inÔ¨Ånite. With T = 2025 years and b = 2 √ó 1011 years, this formula
is a fair model of the world‚Äôs population between the years 1 and 1970. For a
more accurate account, see von Foerster et al. (1960).
6.11 The meaning of exactness
We can integrate the differentials of a Ô¨Årst-order ODE
P(x, y) dx + Q(x, y) dy = 0
(6.105)
along any contour C in the x-y plane, but in general we‚Äôd get a functional
œÜ(x, y, C, x0, y0) =
 (x,y)
(x0,y0)C
P(x‚Ä≤, y‚Ä≤) dx‚Ä≤ + Q(x‚Ä≤, y‚Ä≤) dy‚Ä≤
(6.106)
that depends upon the contour C of integration as well as upon the endpoints
(x0, y0) and (x, y).
But if the differential Pdx + Qdy is exact, then it‚Äôs the differential or exterior
derivative dœÜ = P(x, y) dx + Q(x, y) dy of a function œÜ(x, y) that depends upon
the variables x and y without any reference to a contour of integration. Thus if
Pdx + Qdy = dœÜ, then the contour integral (6.105) is
 (x,y)
(x0,y0)C
P(x‚Ä≤, y‚Ä≤) dx‚Ä≤ + Q(x‚Ä≤, y‚Ä≤) dy‚Ä≤ =
 (x,y)
(x0,y0)
dœÜ = œÜ(x, y) ‚àíœÜ(x0, y0).
(6.107)
This integral deÔ¨Ånes a function œÜ(x, y, x0, y0) ‚â°œÜ(x, y) ‚àíœÜ(x0, y0) whose dif-
ferential vanishes dœÜ = Pdx + Qdy = 0 according to the original differential
equation (6.105). Thus the ODE and its exactness lead to an equation
œÜ(x, y, x0, y0) = B
(6.108)
that we can solve for y, our solution of the ODE (6.105)
dœÜ(x, y, x0, y0) = P(x, y) dx + Q(x, y) dy = 0.
(6.109)
Example 6.17 (Explicit use of exactness)
We‚Äôll now explicitly use the criteria of
exactness
P(x, y) = ‚àÇœÜ(x, y)
‚àÇx
‚â°œÜx(x, y) and Q(x, y) = ‚àÇœÜ(x, y)
‚àÇy
‚â°œÜy(x, y)
(6.110)
to integrate the general exact differential equation
P(x, y) dx + Q(x, y) dy = 0.
(6.111)
240

6.11 THE MEANING OF EXACTNESS
We use the Ô¨Årst criterion P = œÜx to integrate the condition œÜx = P in the x-
direction getting a known integral R(x, y) and an unknown function C(y)
œÜ(x, y) =

P(x, y) dx + C(y) = R(x, y) + C(y).
(6.112)
The second criterion Q = œÜy tells us that
Q(x, y) = œÜy(x, y) = Ry(x, y) + Cy(y).
(6.113)
We get C(y) by integrating its known derivative Cy = Q ‚àíRy
C(y) =

Q(x, y) ‚àíRy(x, y) dy + D.
(6.114)
We now put C into the formula œÜ = R + C, which is (6.112). Setting œÜ = E, a
constant, we Ô¨Ånd an equation
œÜ(x, y) = R(x, y) + C(y)
= R(x, y) +

Q(x, y) ‚àíRy(x, y) dy + D = E
(6.115)
that we can solve for y.
Example 6.18 (Using exactness)
The functions P and Q in the differential
equation
P(x, y) dx + Q(x, y) dy = ln(y2 + 1) dx + 2y(x ‚àí1)
y2 + 1
dy = 0
(6.116)
are factorized, so the ODE is separable. It‚Äôs also exact since
Py =
2y
y2 + 1 = Qx
(6.117)
and so we can apply the method just outlined. First, as in (6.112), we integrate
œÜx = P in the x-direction
œÜ(x, y) =

ln(y2 + 1) dx + C(y) = x ln(y2 + 1) + C(y).
(6.118)
Then as in (6.113), we use œÜy = Q
œÜ(x, y)y =
2xy
y2 + 1 + Cy(y) = Q(x, y) = 2y(x ‚àí1)
y2 + 1
(6.119)
to Ô¨Ånd that Cy = ‚àí2y/(y2+1), which we integrate in the y-direction as in (6.114)
C(y) = ‚àíln(y2 + 1) + D.
(6.120)
We now put C(y) into our formula (6.118) for œÜ(x, y)
œÜ(x, y) = x ln(y2 + 1) ‚àíln(y2 + 1) + D
= (x ‚àí1) ln(y2 + 1) + D,
(6.121)
241

DIFFERENTIAL EQUATIONS
which we set equal to a constant
œÜ(x, y) = (x ‚àí1) ln(y2 + 1) + D = E
(6.122)
or more simply (x ‚àí1) ln(y2 + 1) = F. Unraveling this equation we Ô¨Ånd
y(x) =

eF/(x‚àí1) ‚àí1
1/2
(6.123)
as our solution to the differential equation (6.116).
6.12 Integrating factors
With great luck, one might invent an integrating factor Œ±(x, y) that makes an
ordinary differential equation P dx + Q dy = 0 exact
Œ± P dx + Œ± Q dy = dœÜ
(6.124)
and therefore integrable. Such an integrating factor Œ± must satisfy both
Œ± P = œÜx
and
Œ± Q = œÜy
(6.125)
so that
(Œ± P)y = œÜxy = (Œ± Q)x.
(6.126)
Example 6.19 (Two simple integrating factors)
The ODE ydx ‚àíxdy = 0 is not
exact, but Œ±(x, y) = 1/x2 is an integrating factor. For after multiplying by Œ±, we
have
‚àíy
x2 dx + 1
xdy = 0
(6.127)
so that P = ‚àíy/x2, Q = 1/x, and
Py = ‚àí1
x2 = Qx,
(6.128)
which shows that (6.127) is exact.
Another integrating factor is Œ±(x, y) = 1/xy, which separates the variables
dx
x = dy
y
(6.129)
so that we can integrate and get ln(y/y0) = ln(x/x0) or ln(yx0/xy0) = 0, which
implies that y = (y0/x0)x.
242

6.14 THE VIRIAL THEOREM
6.13 Homogeneous functions
A function f (x) = f (x1, . . . , xk) of k variables xi is homogeneous of degree n if
f (tx) = f (tx1, . . . , txk) = tn f (x).
(6.130)
For instance, z2 ln(x/y) is homogeneous of degree 2 because
(tz)2 ln(tx/ty) = t2 
z2 ln(x/y)

.
(6.131)
By differentiating (6.130) with respect to t, we Ô¨Ånd
d
dtf (tx) =
k

i=1
dtxi
dt
‚àÇf (tx)
‚àÇtxi
=
k

i=1
xi
‚àÇf (tx)
‚àÇtxi
= ntn‚àí1 f (x).
(6.132)
Setting t = 1, we see that a function that is homogeneous of degree n satisÔ¨Åes
k

i=1
xi
‚àÇf (x)
‚àÇxi
= n f (x),
(6.133)
which is one of Euler‚Äôs many theorems.
6.14 The virial theorem
Consider N particles moving nonrelativistically in a potential V(x) of 3N vari-
ables that is homogeneous of degree n. Their virial is the sum of the products of
the coordinates xi multiplied by the momenta pi
G =
3N

i=1
xi pi.
(6.134)
In terms of the kinetic energy T = (v1p1 + ¬∑ ¬∑ ¬∑ + v3Np3N)/2, the time derivative
of the virial is
dG
dt =
3N

i=1
(vi pi + xi Fi) = 2T +
3N

i=1
xi Fi,
(6.135)
in which the time derivative of a momentum Àôpi = Fi is a component of the
force. We now form the inÔ¨Ånite time average of both sides of this equation
lim
t‚Üí‚àû
G(t) ‚àíG(0)
t
=
.dG
dt
/
= 2 ‚ü®T‚ü©+
0 3N

i=1
xi Fi
1
.
(6.136)
If the particles are bound by a potential V, then it is reasonable to assume that
the positions and momenta of the particles and their virial G(t) are bounded
243

DIFFERENTIAL EQUATIONS
for all times, and we will make this assumption. It follows that as t ‚Üí‚àû, the
time average of the time derivative ÀôG of the virial must vanish
0 = 2 ‚ü®T‚ü©+
0 3N

i=1
xi Fi
1
.
(6.137)
Newton‚Äôs law
Fi = ‚àí‚àÇV(x)
‚àÇxi
(6.138)
now implies that
2 ‚ü®T‚ü©=
0 3N

i=1
xi
‚àÇV(x)
xi
1
.
(6.139)
If, further, the potential V(x) is a homogeneous function of degree n, then Euler‚Äôs
theorem (6.133) gives us xi‚àÇiV = nV and the virial theorem
‚ü®T‚ü©= n
2 ‚ü®V(x)‚ü©.
(6.140)
The long-term time average of the kinetic energy of particles trapped in a homo-
geneous potential of degree n is n/2 times the long-term time average of their
potential energy.
Example 6.20 (Coulomb forces)
A 1/r gravitational or electrostatic potential
is homogeneous of degree ‚àí1, and so the virial theorem asserts that particles
bound in such wells must have long-term time averages that satisfy
‚ü®T‚ü©= ‚àí1
2 ‚ü®V(x)‚ü©.
(6.141)
In natural units (¬Øh = c = 1), the energy of an electron of momentum p a
distance r from a proton is E = p2/2m ‚àíe2/r in which e is the charge of the
electron. The uncertainty principle (example 3.6) gives us an approximate lower
bound on the product r p >‚àº1, which we will use in the form r p = 1 to estimate the
energy E of the ground state of the hydrogen atom. Using 1/r = p, we have E =
p2/2m‚àíe2p. Differentiating, we Ô¨Ånd the minimum of E is at 0 = p/m‚àíe2. Thus
the kinetic energy of the ground state is T = p2/2m = me4/2 while its potential
energy is V = ‚àíe2p = ‚àíme4. Since T = ‚àíV/2, these values satisfy the virial
theorem. They give the ground-state energy as E = ‚àíme4/2 = ‚àímc2(e2/¬Øhc)2 =
13.6 eV.
Example 6.21 (Harmonic forces)
Particles conÔ¨Åned in a harmonic potential
V(r) = 
k mkœâ2
kr2
k, which is homogeneous of degree 2, must have long-term
time averages that satisfy ‚ü®T‚ü©= ‚ü®V(x)‚ü©.
244

6.15 HOMOGENEOUS FIRST-ORDER ORDINARY DIFFERENTIAL EQUATIONS
6.15 Homogeneous Ô¨Årst-order ordinary differential equations
Suppose the functions P(x, y) and Q(x, y) in the Ô¨Årst-order ODE
P(x, y) dx + Q(x, y) dy = 0
(6.142)
are homogeneous of degree n (Ince, 1956). We change variables from x and y to
x and y(x) = xv(x) so that dy = xdv + vdx, and so
P(x, xv)dx + Q(x, xv)(xdv + vdx) = 0.
(6.143)
The homogeneity of P(x, y) and Q(x, y) implies that
xnP(1, v)dx + xnQ(1, v)(xdv + vdx) = 0.
(6.144)
Rearranging this equation, we are able to separate the variables
dx
x +
Q(1, v)
P(1, v) + vQ(1, v) dv = 0.
(6.145)
We integrate this equation
ln x +

Q(1, v)
P(1, v) + vQ(1, v) dv = C
(6.146)
and Ô¨Ånd v(x) and so too the solution y(x) = xv(x).
Example 6.22 (Using homogeneity)
In the differential equation
(x2 ‚àíy2) dx + 2xy dy = 0
(6.147)
the coefÔ¨Åcients of the differentials P(x, y) = x2 ‚àíy2 and Q(x, y) = 2xy are
homogeneous functions of degree n = 2, so the above method applies. With
y(x) = xv(x), we have
x2(1 ‚àív2)dx + 2x2v(vdx + xdv) = 0,
(6.148)
in which x2 cancels out, leaving (1 + v2)dx + 2vxdv = 0. Separating variables
and integrating, we Ô¨Ånd
 dx
x +

2v dv
1 + v2 = ln C
(6.149)
or ln(1 + v2) + ln x = ln C. So (1 + v2)x = C, which leads to the general
integral x2 + y2 = Cx and so to y(x) =

Cx ‚àíx2 as the solution of the
ODE (6.147).
245

DIFFERENTIAL EQUATIONS
6.16 Linear Ô¨Årst-order ordinary differential equations
The general form of a linear Ô¨Årst-order ODE is
dy
dx + r(x)y = s(x).
(6.150)
We always can Ô¨Ånd an integrating factor Œ±(x) that makes
0 = Œ±(ry ‚àís)dx + Œ±dy
(6.151)
exact. If P ‚â°Œ±(ry ‚àís) and Q ‚â°Œ±, then the condition (6.98) for this equation
to be exact is Py = Œ±r = Qx = Œ±x or Œ±x/Œ± = r. So
d ln Œ±
dx
= r,
(6.152)
which we integrate to
Œ±(x) = Œ±(x0) exp
 x
x0
r(x‚Ä≤)dx‚Ä≤

.
(6.153)
Now since Œ±r = Œ±x, the original equation (6.150) multiplied by this integrating
factor is
Œ±yx + Œ±ry = Œ±yx + Œ±xy = (Œ±y)x = Œ±s.
(6.154)
Integrating, we Ô¨Ånd
Œ±(x)y(x) = Œ±(x0)y(x0) +
 x
x0
Œ±(x‚Ä≤)s(x‚Ä≤)dx‚Ä≤
(6.155)
so that
y(x) = Œ±(x0)y(x0)
Œ±(x)
+
1
Œ±(x)
 x
x0
Œ±(x‚Ä≤)s(x‚Ä≤)dx‚Ä≤,
(6.156)
in which Œ±(x) is the exponential (6.153). More explicitly, y(x) is
y(x) = exp

‚àí
 x
x0
r(x‚Ä≤)dx‚Ä≤
 
y(x0) +
 x
x0
exp
 x‚Ä≤
x0
r(x‚Ä≤‚Ä≤)dx‚Ä≤‚Ä≤

s(x‚Ä≤)dx‚Ä≤

.
(6.157)
The Ô¨Årst term in the square brackets multiplied by the prefactor Œ±(x0)/Œ±(x) is
the general solution of the homogeneous equation yx+ry = 0. The second term
in the square brackets multiplied by the prefactor Œ±(x0)/Œ±(x) is a particular
solution of the inhomogeneous equation yx + ry = s. Thus equation (6.157)
expresses the general solution of the inhomogeneous equation (6.150) as the
sum of a particular solution of the inhomogeneous equation and the general
solution of the associated homogeneous equation.
246

6.16 LINEAR FIRST-ORDER ORDINARY DIFFERENTIAL EQUATIONS
We were able to Ô¨Ånd an integrating factor Œ± because the original equation
(6.150) was linear in y. So we could set P(x, y) = r(x)y ‚àís(x) and Q(x, y) = 1.
When P and Q are more complicated, integrating factors are harder to Ô¨Ånd or
nonexistent.
Example 6.23 (Bodies falling in air)
The downward speed v of a mass m in a
gravitational Ô¨Åeld of constant acceleration g is described by the inhomogeneous
Ô¨Årst-order ODE mvt = mg ‚àíbv, in which b represents air resistance. This equa-
tion is like (6.150) but with t instead of x as the independent variable, r = b/m,
and s = g. Thus by (6.157), its solution is
v(t) = mg
b +

v(0) ‚àímg
b

e‚àíbt/m.
(6.158)
The terminal speed mg/b is nearly 200 km/h for a falling man. A diving Peregrine
falcon can exceed 320 km/h; so can a falling bullet. But mice can fall down mine
shafts and run off unhurt, and insects and birds can Ô¨Çy.
If the falling bodies are microscopic, a statistical model is appropriate. The
potential energy of a mass m at height h is V = mgh. The heights of particles at
temperature T K follow Boltzmann‚Äôs distribution (1.345)
P(h) = P(0)e‚àímgh/kT,
(6.159)
in which k = 1.380 6504 √ó 10‚àí23 J/K = 8.617 343 √ó 10‚àí5eV/K is his constant.
The probability depends exponentially upon the mass m and drops by a factor
of e with the scale height S = kT/mg, which can be a few kilometers for a small
molecule.
Example 6.24 (R-C circuit)
The capacitance C of a capacitor is the charge Q it
holds (on each plate) divided by the applied voltage V, that is, C = Q/V. The
current I through the capacitor is the time derivative of the charge I = ÀôQ = C ÀôV.
The voltage across a resistor of R  (Ohms) through which a current I Ô¨Çows
is V = IR by Ohm‚Äôs law. So if a time-dependent voltage V(t) is applied to a
capacitor in series with a resistor, then V(t) = Q/C +IR. The current I therefore
obeys the Ô¨Årst-order differential equation
ÀôI + I/RC = ÀôV/R
(6.160)
or (6.150) with x ‚Üít, y ‚ÜíI, r ‚Üí1/RC, and s ‚ÜíÀôV/R. Since r is a constant,
the integrating factor Œ±(x) ‚ÜíŒ±(t) is
Œ±(t) = Œ±(t0) e(t‚àít0)/RC.
(6.161)
Our general solution (6.157) of the linear Ô¨Årst-order ODE gives us the expression
I(t) = e‚àí(t‚àít0)/(RC)

I(t0) +
 t
t0
e(t‚Ä≤‚àít0)/(RC) ÀôV(t‚Ä≤)
R
dt‚Ä≤

(6.162)
for the current I(t).
247

DIFFERENTIAL EQUATIONS
Example 6.25 (Emission rate from Ô¨Çuorophores)
A Ô¨Çuorophore is a molecule
that emits light when illuminated. The frequency of the emitted photon usually
is less than that of the incident one. Consider a population of N Ô¨Çuorophores of
which N+ are excited and can emit light and N‚àí= N ‚àíN+ are unexcited. If the
Ô¨Çuorophores are exposed to an illuminating photon Ô¨Çux I, and the cross-section
for the excitation of an unexcited Ô¨Çuorophore is œÉ, then the rate at which unex-
cited Ô¨Çuorophores become excited is IœÉN‚àí. The time derivative of the number
of excited Ô¨Çuorophores is then
ÀôN+ = IœÉN‚àí‚àí1
œÑ N+ = ‚àí1
œÑ N+ + IœÉ (N ‚àíN+) ,
(6.163)
in which 1/œÑ is the decay rate (also the emission rate) of the excited Ô¨Çuorophores.
Using the shorthand a = IœÉ +1/œÑ, we have ÀôN+ = ‚àíaN+ +IœÉN, which we solve
using the general formula (6.157) with r = a and s = IœÉN
N+(t) = e‚àíat
#
N+(0) +
 t
0
eat‚Ä≤I(t‚Ä≤)œÉN dt‚Ä≤
$
.
(6.164)
If the illumination I(t) is constant, then by doing the integral we Ô¨Ånd
N+(t) = IœÉN
a

1 ‚àíe‚àíat
+ N+(0)e‚àíat.
(6.165)
The emission rate E = N+(t)/œÑ of photons from the N+(t) excited Ô¨Çuorophores
then is
E = IœÉN
aœÑ

1 ‚àíe‚àíat
+ N+(0)
œÑ
e‚àíat,
(6.166)
which with a = IœÉ + 1/1œÑ gives for the emission rate per Ô¨Çuorophore
E
N =
IœÉ
1 + IœÉœÑ

1 ‚àíe‚àí(IœÉ+1/œÑ)t
(6.167)
if no Ô¨Çuorophores were excited at t = 0, so that N+(0) = 0.
6.17 Systems of differential equations
Actual physical problems often involve several differential equations. The
motion of n particles in three dimensions is described by 3n equations, electro-
dynamics by the four Maxwell equations (11.82 & 11.83), and the concentra-
tions of different molecular species in a cell by thousands of coupled differential
equations.
This Ô¨Åeld is too vast to cover in these pages, but we may hint at some of its fea-
tures by considering the motion of n particles in three dimensions as described
by a lagrangian L(q, Àôq, t), in which q stands for the 3n coordinates q1, q2, ..., q3n
and Àôq for their time derivatives. The action of a motion q(t) is the time integral
of the lagrangian
248

6.17 SYSTEMS OF DIFFERENTIAL EQUATIONS
S =
 t2
t1
L(q, Àôq, t) dt.
(6.168)
If q(t) changes by a little bit Œ¥q, then the Ô¨Årst-order change in the action is
Œ¥S =
 t2
t1
3n

i=1
#‚àÇL(q, Àôq, t)
‚àÇqi
Œ¥qi(t) + ‚àÇL(q, Àôq, t)
‚àÇÀôqi
Œ¥Àôqi(t)
$
dt.
(6.169)
The change in Àôqi is
Œ¥ dqi
dt = d(qi + Œ¥qi)
dt
‚àídqi
dt = d Œ¥qi
dt ,
(6.170)
the time derivative of the change Œ¥qi, so we have
Œ¥S =
 t2
t1
3n

i=1
#‚àÇL(q, Àôq, t)
‚àÇqi
Œ¥qi(t) + ‚àÇL(q, Àôq, t)
‚àÇÀôqi
d Œ¥qi(t)
dt
$
dt.
(6.171)
We can integrate this by parts
Œ¥S =
 t2
t1
3n

i=1
# ‚àÇL
‚àÇqi
‚àíd
dt
‚àÇL
‚àÇÀôqi

Œ¥qi(t)
$
dt +
 3n

i=1
‚àÇL
‚àÇÀôqi
Œ¥qi(t)
t2
t1
.
(6.172)
A classical process is one that makes the action stationary to Ô¨Årst order in Œ¥q(t)
for changes that vanish at the endpoints Œ¥q(t1) = 0 = Œ¥q(t2). Thus a classical
process satisÔ¨Åes Lagrange‚Äôs equations
d
dt
‚àÇL
‚àÇÀôqi
‚àí‚àÇL
‚àÇqi
= 0
for
i = 1, . . . , 3n.
(6.173)
Moreover, if the lagrangian does not depend explicitly on the time t, as is usually
the case, then the hamiltonian
H =
3n

i=1
‚àÇL
‚àÇÀôqi
Àôqi ‚àíL ‚â°
3n

i=1
pi Àôqi ‚àíL
(6.174)
does not change with time because its time derivative is the vanishing explicit
time dependence of the lagrangian ‚àí‚àÇL/‚àÇt = 0. That is
ÀôH =
3n

i=1
d
dt
‚àÇL
‚àÇÀôqi
Àôqi + ‚àÇL
‚àÇÀôqi
¬®qi ‚àíÀôL =
3n

i=1
‚àÇL
‚àÇqi
Àôqi + ‚àÇL
‚àÇÀôqi
¬®qi ‚àíÀôL
= ‚àí‚àÇL
‚àÇt = 0.
(6.175)
249

DIFFERENTIAL EQUATIONS
Example 6.26 (Small oscillations)
The lagrangian
L =
3n

i=1
mi
2 Àôx2
i ‚àíV(x)
(6.176)
describes n particles of mass mi interacting through a potential U(q) that has no
explicit time dependence. By letting qi = ‚àömi/m xi we may scale the masses to
the same value m and set V(q) = U(x), so that we have
L = m
2
3n

i=1
Àôq2
i ‚àíV(q) = m
2 Àôq ¬∑ Àôq ‚àíV(q),
(6.177)
which describes n particles of mass m interacting through a potential V(q). The
hamiltonian is conserved, and if it has a minimum energy H0 at q0, then its Ô¨Årst
derivatives there vanish. So near q0 the potential V to lowest order is a quadratic
form in the displacements ri ‚â°qi ‚àíqi0 from the minima, and the lagrangian,
apart from the constant V(q0), is
L ‚âàm
2
3n

i=1
Àôr2
i ‚àí1
2
3n

j,k=1
rj rk
‚àÇ2V(q0)
‚àÇqj‚àÇqk
.
(6.178)
The matrix V‚Ä≤‚Ä≤ of second derivatives is real and symmetric, and so we may diag-
onalize it V‚Ä≤‚Ä≤ = OT V‚Ä≤‚Ä≤
d O by an orthogonal transformation O. The lagrangian is
diagonal in the new coordinates s = O r
L ‚âà1
2
3n

i=1

m Àôs2
i ‚àíV‚Ä≤‚Ä≤
di s2
i

(6.179)
and Lagrange‚Äôs equations are m ¬®si = ‚àíV‚Ä≤‚Ä≤
di si. These normal modes are uncoupled
harmonic oscillators si(t) = ai cos

V‚Ä≤‚Ä≤
di/m t + bi sin

V‚Ä≤‚Ä≤
di/m t with frequencies
that are real because q0 is the minimum of the potential.
6.18 Singular points of second-order ordinary differential equations
If in the ODE y‚Ä≤‚Ä≤ = f (x, y, y‚Ä≤), the acceleration y‚Ä≤‚Ä≤ = f (x0, y, y‚Ä≤) is Ô¨Ånite for
all Ô¨Ånite y and y‚Ä≤, then x0 is a regular point of the ODE. If y‚Ä≤‚Ä≤ = f (x0, y, y‚Ä≤) is
inÔ¨Ånite for any Ô¨Ånite y and y‚Ä≤, then x0 is a singular point of the ODE.
If a second-order ODE y‚Ä≤‚Ä≤ + P(x)y‚Ä≤ + Q(x)y = 0 is linear and homogeneous
and both P(x0) and Q(x0) are Ô¨Ånite, then x0 is a regular point of the ODE. But
if P(x0) or Q(x0) or both are inÔ¨Ånite, then x0 is a singular point.
Some singular points are regular. If P(x) or Q(x) diverges as x ‚Üíx0, but
both (x ‚àíx0)P(x) and (x ‚àíx0)2Q(x) remain Ô¨Ånite as x ‚Üíx0, then x0 is a
250

6.19 FROBENIUS‚ÄôS SERIES SOLUTIONS
regular singular point or equivalently a nonessential singular point. But if either
(x‚àíx0)P(x) or (x‚àíx0)2Q(x) diverges as x ‚Üíx0, then x0 is an irregular singular
point or equivalently an essential singularity.
To treat the point at inÔ¨Ånity, one sets z = 1/x. Then if (2z ‚àíP(1/z))/z2 and
Q(1/z)/z4 remain Ô¨Ånite as z ‚Üí0, the point x0 = ‚àûis a regular point of the
ODE. If they don‚Äôt remain Ô¨Ånite, but (2z ‚àíP(1/z))/z and Q(1/z)/z2 do remain
Ô¨Ånite as z ‚Üí0, then x0 = ‚àûis a regular singular point. Otherwise the point at
inÔ¨Ånity is an irregular singular point or an essential singularity.
Example 6.27 (Legendre‚Äôs equation)
Its self-adjoint form is

1 ‚àíx2
y‚Ä≤‚Ä≤
+ ‚Ñì(‚Ñì+ 1)y = 0,
(6.180)
which is (1 ‚àíx2)y‚Ä≤‚Ä≤ ‚àí2xy‚Ä≤ + ‚Ñì(‚Ñì+ 1)y = 0 or
y‚Ä≤‚Ä≤ ‚àí
2x
1 ‚àíx2 y‚Ä≤ + ‚Ñì(‚Ñì+ 1)
1 ‚àíx2 y = 0.
(6.181)
It has regular singular points at x = ¬±1 and x = ‚àû(exercise 6.15 ).
6.19 Frobenius‚Äôs series solutions
Frobenius showed how to Ô¨Ånd a power-series solution of a second-order linear
homogeneous ordinary differential equation y‚Ä≤‚Ä≤ + P(x) y‚Ä≤ + Q(x) y = 0 at any of
its regular or regular singular points. Writing the equation in the form x2y‚Ä≤‚Ä≤ +
x p(x) y‚Ä≤ + q(x) y = 0, we will assume that p and q are polynomials or analytic
functions, and that x = 0 is a regular or regular singular point of the ODE so
that p(0) and q(0) are both Ô¨Ånite.
We expand y as a power series in x about x = 0
y(x) = xr
‚àû

n=0
an xn,
(6.182)
in which a0 Ã∏= 0 is the coefÔ¨Åcient of the lowest power of x in y(x). Differentiat-
ing, we have
y‚Ä≤(x) =
‚àû

n=0
(r + n) an xr+n‚àí1
(6.183)
and
y‚Ä≤‚Ä≤(x) =
‚àû

n=0
(r + n)(r + n ‚àí1) an xr+n‚àí2.
(6.184)
251

DIFFERENTIAL EQUATIONS
When we substitute the three series (6.182‚Äì6.184) into our differential equation
x2y‚Ä≤‚Ä≤ + xp(x)y‚Ä≤ + q(x)y = 0, we Ô¨Ånd
‚àû

n=0
[(n + r)(n + r ‚àí1) + (n + r)p(x) + q(x)] anxn+r.
(6.185)
If this equation is to be satisÔ¨Åed for all x, then the coefÔ¨Åcient of every power
of x must vanish. The lowest power of x is xr, and it occurs when n = 0 with
coefÔ¨Åcient [r(r ‚àí1 + p(0)) + q(0)] a0. Thus since a0 Ã∏= 0, we have
r(r ‚àí1 + p(0)) + q(0) = 0.
(6.186)
This quadratic indicial equation has two roots r1 and r2.
To analyze higher powers of x, we introduce the notation
p(x) =
‚àû

j=0
pjxj
and
q(x) =
‚àû

j=0
qjxj,
(6.187)
in which p0 = p(0) and q0 = q(0). The requirement (exercise 6.16) that the
coefÔ¨Åcient of xr+k vanishes gives us a recurrence relation
ak = ‚àí
#
1
(r + k)(r + k ‚àí1 + p0) + q0
$ k‚àí1

j=0

(j + r)pk‚àíj + qk‚àíj

aj
(6.188)
that expresses ak in terms of a0, a1, ..., ak‚àí1. When p(x) and q(x) are polyno-
mials of low degree, these equations become much simpler.
Example 6.28 (Sines and cosines)
To apply Frobenius‚Äôs method to the ODE
y‚Ä≤‚Ä≤ + œâ2y = 0, we Ô¨Årst write it in the form x2y‚Ä≤‚Ä≤ + xp(x)y‚Ä≤ + q(x)y = 0, in which
p(x) = 0 and q(x) = œâ2x2. So both p(0) = p0 = 0 and q(0) = q0 = 0, and the
indicial equation (6.186) is r(r ‚àí1) = 0 with roots r1 = 0 and r2 = 1.
We Ô¨Årst set r = r1 = 0. Since the ps and qs vanish except for q2 = œâ2, the
recurrence relation (6.188) is ak = ‚àíq2 ak‚àí2/k(k‚àí1) = ‚àíœâ2ak‚àí2/k(k‚àí1). Thus
a2 = ‚àíœâ2a0/2, and a2n = (‚àí1)nœâ2na0/(2n)!. The recurrence relation (6.188)
gives no information about a1, so to Ô¨Ånd the simplest solution, we set a1 = 0.
The recurrence relation ak = ‚àíœâ2ak‚àí2/k(k ‚àí1) then makes all the terms a2n+1
of odd index vanish. Our solution for the Ô¨Årst root r1 = 0 then is
y(x) =
‚àû

n=0
an xn = a0
‚àû

n=0
(‚àí1)n (œâx)2n
(2n)! = a0 cos œâx.
(6.189)
Similarly, the recurrence relation (6.188) for the second root r2 = 1 is ak =
‚àíœâ2ak‚àí2/k(k + 1), so that a2n = (‚àí1)nœâ2na0/(2n + 1)!, and we again set all the
252

6.20 FUCH‚ÄôS THEOREM
terms of odd index equal to zero. Thus we have
y(x) = x
‚àû

n=0
an xn = a0
œâ
‚àû

n=0
(‚àí1)n (œâx)2n+1
(2n + 1)! = a0
œâ sin œâx
(6.190)
as our solution for the second root r2 = 1.
Frobenius‚Äôs method sometimes shows that solutions exist only when a
parameter in the ODE assumes a special value called an eigenvalue.
Example 6.29 (Legendre‚Äôs equation)
If one rewrites Legendre‚Äôs equation
(1 ‚àíx2)y‚Ä≤‚Ä≤ ‚àí2xy‚Ä≤ + Œªy = 0 as x2y‚Ä≤‚Ä≤ + xpy‚Ä≤ + qy = 0, then one Ô¨Ånds p(x) =
‚àí2x2/(1 ‚àíx2) and q(x) = x2/(1 ‚àíx2), which are analytic but not polynomials.
In this case, it is simpler to substitute the expansions (6.182‚Äì6.184) directly into
Legendre‚Äôs equation (1 ‚àíx2)y‚Ä≤‚Ä≤ ‚àí2xy‚Ä≤ + Œªy = 0. We then Ô¨Ånd
‚àû

n=0

(n + r)(n + r ‚àí1)(1 ‚àíx2)xn+r‚àí2 ‚àí2(n + r)xn+r + Œªxn+r
an = 0.
The coefÔ¨Åcient of the lowest power of x is r(r‚àí1)a0, and so the indicial equation
is r(r ‚àí1) = 0. For r = 0, we shift the index n on the term n(n ‚àí1)xn‚àí2an to
n = j + 2 and replace n by j in the other terms:
‚àû

j=0

(j + 2)(j + 1) aj+2 ‚àí[j(j ‚àí1) + 2j ‚àíŒª] aj

xj = 0.
(6.191)
Since the coefÔ¨Åcient of xj must vanish, we get the recursion relation
aj+2 = j(j + 1) ‚àíŒª
(j + 2)(j + 1)aj,
(6.192)
which for big j says that aj+2 ‚âàaj. Thus the series (6.182) does not converge for
|x| ‚â•1 unless Œª = j(j + 1) for some integer j in which case the series (6.182) is a
Legendre polynomial (chapter 8).
Frobenius‚Äôs method also allows one to expand solutions about x0 Ã∏= 0
y(x) = (x ‚àíx0)k
‚àû

n=0
an (x ‚àíx0)n.
(6.193)
6.20 Fuch‚Äôs theorem
The method of Frobenius can run amok, especially if one expands about a sin-
gular point x0. One can get only one solution or none at all. But Fuch has
253

DIFFERENTIAL EQUATIONS
shown that if one applies Frobenius‚Äôs method to a linear homogeneous second-
order ODE and expands about a regular point or a regular singular point, then
one always gets at least one power-series solution:
1 if the two roots of the indicial equation are equal, one gets only one solution;
2 if the two roots differ by a noninteger, one gets two solutions;
3 if the two roots differ by an integer, then the bigger root yields a solution.
Example 6.30 (Roots that differ by an integer)
If one applies the method
of Frobenius to Legendre‚Äôs equation as in example 6.29, then one Ô¨Ånds (exer-
cise 6.18) that the k = 0 and k = 1 roots lead to the same solution.
6.21 Even and odd differential operators
Under the parity transformation x ‚Üí‚àíx, a typical term transforms as
xn
 d
dx
p
xk =
k!
(k ‚àíp)! xn+k‚àíp ‚Üí(‚àí1)n+k‚àíp
k!
(k ‚àíp)! xn+k‚àíp
(6.194)
and so the corresponding differential operator transforms as
xn
 d
dx
p
‚Üí(‚àí1)n‚àíp xn
 d
dx
p
.
(6.195)
The reÔ¨Çected form of the second-order linear differential operator
L(x) = h0(x) + h1(x) d
dx + h2(x) d2
dx2
(6.196)
therefore is
L(‚àíx) = h0(‚àíx) ‚àíh1(‚àíx) d
dx + h2(‚àíx) d2
dx2 .
(6.197)
The operator L(x) is even if it is unchanged by reÔ¨Çection, that is, if h0(‚àíx) =
h0(x), h1(‚àíx) = ‚àíh1(x), and h2(‚àíx) = h2(x), so that
L(‚àíx) = L(x).
(6.198)
It is odd if it changes sign under reÔ¨Çection, that is, if h0(‚àíx) = ‚àíh0(x), h1(‚àíx) =
h1(x), and h2(‚àíx) = ‚àíh2(x), so that
L(‚àíx) = ‚àíL(x).
(6.199)
Not every differential operator L(x) is even or odd. But just as we can write
every function f (x) whose reÔ¨Çected form f (‚àíx) is well deÔ¨Åned as the sum of
[f (x) + f (‚àíx)]/2, which is even, and [f (x) ‚àíf (‚àíx)]/2, which is odd,
f (x) = 1
2[f (x) + f (‚àíx)] + 1
2[f (x) ‚àíf (‚àíx)]
(6.200)
254

6.23 A SECOND SOLUTION
so too we can write every differential operator L(x) whose reÔ¨Çected form L(‚àíx)
is well deÔ¨Åned as the sum of one that is even and one that is odd
L(x) = 1
2[L(x) + L(‚àíx)] + 1
2[L(x) ‚àíL(‚àíx)].
(6.201)
Many of the standard differential operators have h0 = 1 and are even.
If y(x) is a solution of the ODE L(x) y(x) = 0 and L(‚àíx) is well deÔ¨Åned,
then we have L(‚àíx) y(‚àíx) = 0. If further L(‚àíx) = ¬±L(x), then y(‚àíx) also is
a solution L(x) y(‚àíx) = 0. Thus if a differential operator L(x) has a deÔ¨Ånite
parity, that is, if L(x) is either even or odd, then y(‚àíx) is a solution if y(x) is,
and solutions come in pairs y(x) ¬± y(‚àíx), one even, one odd.
6.22 Wronski‚Äôs determinant
If the N functions y1(x), . . . , yN(x) are linearly dependent, then by (6.8) there is
a set of coefÔ¨Åcients k1, . . . , kN, not all zero, such that the sum
0 = k1 y1(x) + ¬∑ ¬∑ ¬∑ + kN yN(x)
(6.202)
vanishes for all x. Differentiating i times, we get
0 = k1 y(i)
1 (x) + ¬∑ ¬∑ ¬∑ + kN y(i)
N (x)
(6.203)
for all x. So if we use the yj and their derivatives to deÔ¨Åne the matrix
Yij(x) ‚â°y(i‚àí1)
j
(x)
(6.204)
then we may express the linear dependence (6.202) and (6.203) of the func-
tions y1, . . . , yN in matrix notation as 0 = Y(x) k for some nonzero vector
k = (k1, k2, . . . , kN). Since the matrix Y(x) maps the nonzero vector k to zero,
its determinant must vanish: det(Y(x)) ‚â°|Y(x)| = 0. This determinant
W(x) = |Y(x)| =
y(i‚àí1)
j
(x)

(6.205)
is called Wronski‚Äôs determinant or the wronskian. It vanishes on an interval if
and only if the functions yj(x) or their derivatives are linearly dependent on the
interval.
6.23 A second solution
If we have one solution to a second-order linear homogeneous ODE, then we
may use the wronskian to Ô¨Ånd a second solution. Here‚Äôs how: if y1 and y2
are two linearly independent solutions of the second-order linear homogeneous
ordinary differential equation
y‚Ä≤‚Ä≤(x) + P(x) y‚Ä≤(x) + Q(x) y(x) = 0
(6.206)
255

DIFFERENTIAL EQUATIONS
then their wronskian does not vanish
W(x) =

y1(x)
y2(x)
y‚Ä≤
1(x)
y‚Ä≤
2(x)
 = y1(x) y‚Ä≤
2(x) ‚àíy2(x) y‚Ä≤
1(x) Ã∏= 0
(6.207)
except perhaps at isolated points. Its derivative
W‚Ä≤ = y‚Ä≤
1 y‚Ä≤
2 + y1 y‚Ä≤‚Ä≤
2 ‚àíy‚Ä≤
2 y‚Ä≤
1 ‚àíy2 y‚Ä≤‚Ä≤
1
= y1 y‚Ä≤‚Ä≤
2 ‚àíy2 y‚Ä≤‚Ä≤
1
(6.208)
must obey
W‚Ä≤ = ‚àíy1

P y‚Ä≤
2 + Q y2

+ y2

P y‚Ä≤
1 + Q y1

= ‚àíP

y1 y‚Ä≤
2 ‚àíy2 y‚Ä≤
1

(6.209)
or W‚Ä≤(x) = ‚àíP(x) W(x), which we integrate to
W(x) = W(x0) exp
#
‚àí
 x
x0
P(x‚Ä≤)dx‚Ä≤
$
.
(6.210)
This is Abel‚Äôs formula for the wronskian (Niels Abel, 1802‚Äì1829).
Having expressed the wronskian in terms of the known function P(x), we
now use it to Ô¨Ånd y2(x) from y1(x). We note that
W = y1 y‚Ä≤
2 ‚àíy2 y‚Ä≤
1 = y2
1
d
dx
y2
y1

.
(6.211)
So
d
dx
y2
y1

= W
y2
1
,
(6.212)
which we integrate to
y2(x) = y1(x)
 x W(x‚Ä≤)
y2
1(x‚Ä≤)
dx‚Ä≤ + c

.
(6.213)
Using our formula (6.210) for the wronskian, we Ô¨Ånd as the second solution
y2(x) = y1(x)
 x
1
y2
1(x‚Ä≤)
exp

‚àí
 x‚Ä≤
P(x‚Ä≤‚Ä≤)dx‚Ä≤‚Ä≤

dx‚Ä≤
(6.214)
apart from additive and multiplicative constants.
In the important special case in which P(x) = 0 the wronskian is a constant,
W‚Ä≤(x) = 0, and the second solution is simply
y2(x) = y1(x)
 x
dx‚Ä≤
y2
1(x‚Ä≤)
.
(6.215)
By Fuchs‚Äôs theorem, Frobenius‚Äôs expansion about a regular point or a reg-
ular singular point yields at least one solution. From this solution, we can
256

6.24 WHY NOT THREE SOLUTIONS?
use Wronski‚Äôs trick to Ô¨Ånd a second (linearly independent) solution. So we
always get two linearly independent solutions if we expand a second-order
linear homogeneous ODE about a regular point or a regular singular point.
6.24 Why not three solutions?
We have seen that a second-order linear homogeneous ODE has two linearly
independent solutions. Why not three?
If y1, y2, and y3 were three linearly independent solutions of the second-order
linear homogeneous ODE
0 = y‚Ä≤‚Ä≤
j + P y‚Ä≤
j + Q yj,
(6.216)
then their third-order wronskian
W =

y1
y2
y3
y‚Ä≤
1
y‚Ä≤
2
y‚Ä≤
3
y‚Ä≤‚Ä≤
1
y‚Ä≤‚Ä≤
2
y‚Ä≤‚Ä≤
3

(6.217)
would not vanish except at isolated points.
But the ODE (6.216) relates the second derivatives y‚Ä≤‚Ä≤
j = ‚àí(P y‚Ä≤
j + Q yj) to
the y‚Ä≤
j and the yj, and so the third row of this third-order wronskian is a linear
combination of the Ô¨Årst two rows. Thus it vanishes identically
W =

y1
y2
y3
y‚Ä≤
1
y‚Ä≤
2
y‚Ä≤
3
‚àíPy‚Ä≤
1 ‚àíQy1
‚àíPy‚Ä≤
2 ‚àíQy2
‚àíPy‚Ä≤
3 ‚àíQy3

= 0
(6.218)
and so any three solutions of a second-order ODE (6.216) are linearly
dependent.
One may extend this argument to show that an nth-order linear homoge-
neous ODE can have at most n linearly independent solutions. To do so, we‚Äôll
use superscript notation (6.19) in which y(n) denotes the nth derivative of y(x)
with respect to x
y(n) ‚â°dny
dxn .
(6.219)
Suppose there were n + 1 linearly independent solutions yj of the ODE
y(n) + P1 y(n‚àí1) + P2 y(n‚àí2) + ¬∑ ¬∑ ¬∑ + Pn‚àí1 y‚Ä≤ + Pn y = 0,
(6.220)
in which the Pks are functions of x. Then we could form a wronskian of order
(n+1) in which row 1 would be y1, ..., yn+1, row 2 would be the Ô¨Årst derivatives
y‚Ä≤
1, ..., y‚Ä≤
n+1, and row n+1 would be the nth derivatives y(n)
1 , ..., y(n)
n+1. We could
then replace each term y(n)
k in the last row by
y(n)
k
= ‚àíP1 y(n‚àí1)
k
‚àíP2 y(n‚àí2)
k
‚àí¬∑ ¬∑ ¬∑ ‚àíPn‚àí1 y‚Ä≤
k ‚àíPn yk.
(6.221)
257

DIFFERENTIAL EQUATIONS
But then the last row would be a linear combination of the Ô¨Årst n rows, the
determinant would vanish, and the n+1 solutions would be linearly dependent.
This is why an nth-order linear homogeneous ODE can have at most n linearly
independent solutions.
6.25 Boundary conditions
Since an nth-order linear homogeneous ordinary differential equation can have
at most n linearly independent solutions, it follows that we can make a solution
unique by requiring it to satisfy n boundary conditions. We‚Äôll see that the n
arbitrary coefÔ¨Åcients ck of the general solution
y(x) =
n

k=1
ck yk(x)
(6.222)
of the differential equation (6.220) are Ô¨Åxed by the n boundary conditions
y(x1) = b1,
y(x2) = b2,
. . .
y(xn) = bn
(6.223)
as long as the functions yk(x) are linearly independent, which is to say, as long
as the matrix Y with entries Yjk = yk(xj) is nonsingular, that is, det Y Ã∏= 0.
In matrix notation, with B a vector with components bj and C a vector with
components ck, the n boundary conditions (6.223) are
y(xj) =
n

k=1
ck yk(xj) = bj
or
Y C = B.
(6.224)
Thus since det Y Ã∏= 0, the coefÔ¨Åcients are uniquely given by C = Y‚àí1 B.
The boundary conditions can involve the derivatives y
(‚Ñìj)
k (xj). One may
show (exercise 6.20) that in this case as long as the matrix Yjk = y
(‚Ñìj)
k (xj) is
nonsingular, the n boundary conditions
y(‚Ñìj)(xj) =
n

k=1
ck y
(‚Ñìj)
k (xj) = bj
(6.225)
are Y C = B, and so the n coefÔ¨Åcients are uniquely C = Y‚àí1 B.
But what if all the bj are zero? If all the boundary conditions are homoge-
neous Y C = 0, and det Y Ã∏= 0, then Y‚àí1 Y C = C = 0, and the only solution
is yk(x) ‚â°0. So there is no solution if B = 0 and the matrix Y is nonsingular.
But if the n√ón matrix Y has rank n‚àí1, then (section 1.33) it maps a unique vec-
tor C to zero (apart from an overall factor). So if all the boundary conditions
are homogeneous, and the matrix Y has rank n ‚àí1, then the solution y = ckyk
is unique. But if the rank of Y is less than n ‚àí1, the solution is not unique.
258

6.26 A VARIATIONAL PROBLEM
Since a matrix of rank zero vanishes identically, any nonzero 2 √ó 2 matrix
Y must be of rank 1 or 2. Thus a second-order ODE with two homogeneous
boundary conditions has either a unique solution or none at all.
Example 6.31 (Boundary conditions and eigenvalues)
The solutions yk of the
differential equation ‚àíy‚Ä≤‚Ä≤ = k2 y are y1(x) = sin kx and y2(x) = cos kx. If
we impose the boundary conditions y(‚àía) = 0 and y(a) = 0, then the matrix
Yjk = yk(xj) is
Y =
 ‚àísin ka
cos ka
sin ka
cos ka

(6.226)
with determinant det Y =
‚àí2 sin ka cos ka = ‚àísin 2ka. This determinant
vanishes only if ka = nœÄ/2 for some integer n, so if ka Ã∏= nœÄ/2, then no solu-
tion y of the differential equation ‚àíy‚Ä≤‚Ä≤ = k2 y satisÔ¨Åes the boundary conditions
y(‚àía) = 0 = y(a). But if ka = nœÄ/2, then there is a solution, and it is unique
because for even (odd) n, the Ô¨Årst (second) column of Y vanishes, but not the
second (Ô¨Årst), which implies that Y has rank 1. One may regard the condition
ka = nœÄ/2 either as determining the eigenvalue k2 or as telling us what interval
to use.
6.26 A variational problem
For what functions u(x) is the ‚Äúenergy‚Äù functional
E[u] ‚â°
 b
a

p(x)u‚Ä≤2(x) + q(x)u2(x)

dx
(6.227)
stationary? That is, for what functions u is E[u + Œ¥u] unchanged to Ô¨Årst order
in Œ¥u when u(x) is changed by an arbitrary but tiny function Œ¥u(x) to u(x) +
Œ¥u(x)? Our equations will be less cluttered if we drop explicit mention of the
x-dependence of p, q, and u, which we assume to be real functions of x.
The Ô¨Årst-order change in E is
Œ¥E[u] ‚â°
 b
a

p 2u‚Ä≤ Œ¥u‚Ä≤ + q 2u Œ¥u

dx,
(6.228)
in which the change in the derivative of u is Œ¥u‚Ä≤ = u‚Ä≤ + (Œ¥u)‚Ä≤ ‚àíu‚Ä≤ = (Œ¥u)‚Ä≤. Setting
Œ¥E = 0 and integrating by parts, we have
0 = Œ¥E =
 b
a

p u‚Ä≤(Œ¥u)‚Ä≤ + q u Œ¥u

dx
=
 b
a

p u‚Ä≤Œ¥u
‚Ä≤ ‚àí

p u‚Ä≤‚Ä≤ Œ¥u + q u Œ¥u

dx
=
 b
a

‚àí

p u‚Ä≤‚Ä≤ + q u

Œ¥u dx +

p u‚Ä≤Œ¥u
b
a.
(6.229)
259

DIFFERENTIAL EQUATIONS
So if E is to be stationary with respect to all tiny changes Œ¥u that vanish at the
endpoints a and b, then u must satisfy the differential equation
L u = ‚àí

p u‚Ä≤‚Ä≤ + q u = 0.
(6.230)
If instead E is to be stationary with respect to all tiny changes Œ¥u, then u
must satisfy the differential equation (6.230) as well as the natural boundary
conditions
0 = p(b) u‚Ä≤(b)
and
0 = p(a) u‚Ä≤(a).
(6.231)
If p(a) Ã∏= 0 Ã∏= p(b), then these natural boundary conditions imply Neumann‚Äôs
boundary conditions
u‚Ä≤(a) = 0
and
u‚Ä≤(b) = 0
(6.232)
(Carl Neumann, 1832‚Äì1925).
6.27 Self-adjoint differential operators
If p(x) and q(x) are real, then the differential operator
L = ‚àíd
dx

p(x) d
dx

+ q(x)
(6.233)
is formally self adjoint. Such operators are interesting because if we take any two
functions u and v that are twice differentiable on an interval [a, b] and integrate
v L u twice by parts over the interval, we get
(v, L u) =
 b
a
v L u dx =
 b
a
v

‚àí

pu‚Ä≤‚Ä≤ + qu

dx
=
 b
a

pu‚Ä≤v‚Ä≤ + uqv

dx ‚àí

vpu‚Ä≤b
a
=
 b
a

‚àí(pv‚Ä≤)‚Ä≤ + qv

u dx +

puv‚Ä≤ ‚àívpu‚Ä≤b
a
=
 b
a
(L v) u dx +

p(uv‚Ä≤ ‚àívu‚Ä≤)
b
a .
(6.234)
Interchanging u and v and subtracting, we Ô¨Ånd Green‚Äôs formula

(vL u ‚àíu L v) dx =

p(uv‚Ä≤ ‚àívu‚Ä≤)
b
a = [pW(u, v)]b
a
(6.235)
(George Green, 1793‚Äì1841).Its differential form is Lagrange‚Äôs identity
vL u ‚àíu L v =

p W(u, v)
‚Ä≤
(6.236)
260

6.27 SELF-ADJOINT DIFFERENTIAL OPERATORS
(Joseph-Louis Lagrange, 1736‚Äì1813). Thus if the twice-differentiable functions
u and v satisfy boundary conditions at x = a and x = b that make the boundary
term (6.235) vanish

p(uv‚Ä≤ ‚àívu‚Ä≤)
b
a = [pW(u, v)]b
a = 0
(6.237)
then the real differential operator L is symmetric
(v, L u) =
 b
a
v L u dx =
 b
a
u L v dx = (u, L v).
(6.238)
A real linear operator A that acts in a real vector space and satisÔ¨Åes the
analogous relation (1.161)
(g, A f ) = (f , A g)
(6.239)
for all vectors in the space is said to be symmetric and self adjoint. In this sense,
the differential operator (6.233) is self adjoint on the space of functions that
satisfy the boundary condition (6.237).
In quantum mechanics, we often deal with wave functions that are complex.
So keeping L real, let‚Äôs replace u and v by twice-differentiable, complex-valued
functions œà = u1 + iu2 and œá = v1 + iv2. If u1, u2, v1, and v2 satisfy boundary
conditions at x = a and x = b that make the boundary terms (6.237) vanish

p(uiv‚Ä≤
j ‚àívju‚Ä≤
i)
b
a =

pW(ui, vj)
b
a = 0
for
i, j = 1, 2
(6.240)
then (6.238) implies that
 b
a
vj L ui dx =
 b
a

L vj

ui dx
for
i, j = 1, 2.
(6.241)
Under these assumptions, one may show (exercise 6.21) that the boundary
condition (6.240) makes the complex boundary term vanish

p W(œà, œá‚àó)
b
a =

p

œà œá‚àó‚Ä≤ ‚àíœà‚Ä≤ œá‚àób
a = 0
(6.242)
and (exercise 6.22) that since L is real, the identity (6.241) holds for complex
functions
(œá, L œà) =
 b
a
œá‚àóL œà dx =
 b
a
(L œá)‚àóœà dx = (L œá, œà).
(6.243)
A linear operator A that satisÔ¨Åes the analogous relation (1.157)
(g, A f ) = (A g, f )
(6.244)
is said to be self adjoint or hermitian. In this sense, the differential opera-
tor (6.233) is self adjoint on the space of functions that satisfy the boundary
condition (6.242).
261

DIFFERENTIAL EQUATIONS
The formally self-adjoint differential operator (6.233) will satisfy the inner-
product integral equations (6.238 or 6.243) only when the function p and
the twice-differentiable functions u and v or œà and œá conspire to make the
boundary terms (6.237 or 6.242) vanish. This requirement leads us to deÔ¨Åne a
self-adjoint differential system.
6.28 Self-adjoint differential systems
A self-adjoint differential system consists of a real formally self-adjoint differ-
ential operator, a differential equation on an interval, boundary conditions, and
a set of twice-differentiable functions that obey them.
A second-order differential equation needs two boundary conditions to make
a solution unique (section 6.25). In a self-adjoint differential system, the two
boundary conditions are linear and homogeneous so that the set of all twice-
differentiable functions u that satisfy them is a vector space. This space D is
the domain of the system. For an interval [a, b], Dirichlet‚Äôs boundary conditions
(Johann Dirichlet, 1805‚Äì1859) are
u(a) = 0
and
u(b) = 0
(6.245)
and Neumann‚Äôs (6.232) are
u‚Ä≤(a) = 0
and
u‚Ä≤(b) = 0.
(6.246)
We will require that the functions in the domain D all obey either Dirichlet or
Neumann boundary conditions.
The adjoint domain D‚àóof a differential system is the set of all twice-
differentiable functions v that make the boundary term (6.237) vanish

p(uv‚Ä≤ ‚àívu‚Ä≤)
b
a = [pW(u, v)]b
a = 0
(6.247)
for all functions u that are in the domain D, that is, that satisfy either Dirichlet
or Neumann boundary conditions.
A differential system is regular and self adjoint if the differential operator
Lu = ‚àí(pu‚Ä≤)‚Ä≤ + qu is formally self-adjoint, if the interval [a, b] is Ô¨Ånite, if p, p‚Ä≤,
and q are continuous real functions of x on the interval, if p(x) > 0 on [a, b],
and if the two domains D and D‚àócoincide, D = D‚àó.
One may show (exercises 6.23 and 6.24) that if D is the set of all twice-
differentiable functions u(x) on [a, b] that satisfy either Dirichlet‚Äôs boundary
conditions (6.245) or Neumann‚Äôs boundary conditions (6.246), and if the func-
tion p(x) is continuous and positive on [a, b], then the adjoint set D‚àóis the
same as D. A real formally self-adjoint differential operator Lu = ‚àí(pu‚Ä≤)‚Ä≤ + qu
therefore forms together with Dirichlet (6.245) or Neumann (6.246) boundary
conditions forms a regular and self-adjoint system if p, p‚Ä≤, and q are real and
continuous on a Ô¨Ånite interval [a, b], and p is positive on [a, b].
262

6.28 SELF-ADJOINT DIFFERENTIAL SYSTEMS
Since any two functions u and v in the domain D of a regular and self-adjoint
differential system make the boundary term (6.247) vanish, a real formally
self-adjoint differential operator L is symmetric and self adjoint (6.238) on all
functions in its domain
(v, L u) =
 b
a
v L u dx =
 b
a
u L v dx = (u, L v).
(6.248)
If functions in the domain are complex, then by (6.242 & 6.243) the operator L
is self adjoint or hermitian
(œá, L œà) =
 b
a
œá‚àóL œà dx =
 b
a
(L œá)‚àóœà dx = (L œá, œà)
(6.249)
on all complex functions œà and œá in its domain.
Example 6.32 (Sines and cosines)
The differential system with the formally
self-adjoint differential operator
L = ‚àíd2
dx2
(6.250)
on an interval [a, b] and the differential equation L u =
‚àíu‚Ä≤‚Ä≤ = Œª u has the
function p(x) = 1. If we choose the interval to be [‚àíœÄ, œÄ] and the domain D to
be the set of all functions that are twice differentiable on this interval and sat-
isfy Dirichlet boundary conditions (6.245), then we get a self-adjoint differential
system in which the domain includes linear combinations of un(x) = sin nx. If
instead we impose Neumann boundary conditions (6.246), then the domain D
contains linear combinations of un(x) = cos nx. In both cases, the system is
regular and self adjoint.
Some important differential systems are self adjoint but singular because the
function p(x) vanishes at one or both of the endpoints of the interval [a, b] or
because the interval is inÔ¨Ånite, for instance [0, ‚àû) or (‚àí‚àû, ‚àû). In these singular,
self-adjoint differential systems, the boundary term (6.247) vanishes if u and v
are in the domain D = D‚àó.
Example 6.33 (Legendre‚Äôs system)
Legendre‚Äôs formally self-adjoint differential
operator is
L = ‚àíd
dx
#
(1 ‚àíx2) d
dx
$
(6.251)
and his differential equation is
L u = ‚àí

(1 ‚àíx2)u‚Ä≤‚Ä≤
= ‚Ñì(‚Ñì+ 1) u
(6.252)
263

DIFFERENTIAL EQUATIONS
on the interval [‚àí1, 1]. The function p(x) = 1‚àíx2 vanishes at both endpoints x =
¬±1, and so this self-adjoint system is singular. Because p(¬±1) = 0, the boundary
term (6.247) is zero as long as the functions u and v are differentiable on the
interval. The domain D is the set of all functions that are twice differentiable on
the interval [‚àí1, 1].
Example 6.34 (Hermite‚Äôs system)
Hermite‚Äôs formally self-adjoint differential
operator is
L = ‚àíd2
dx2 + x2
(6.253)
and his differential equation is
L u = ‚àíu‚Ä≤‚Ä≤ + x2 u = (2n + 1) u
(6.254)
on the interval (‚àí‚àû, ‚àû). This system has p(x) = 1 and q(x) = x2. It is self
adjoint but singular because the interval is inÔ¨Ånite. The domain D consists of all
functions that are twice differentiable and that go to zero as x ‚Üí¬±‚àûfaster than
1/x3/2, which ensures that the relevant integrals converge and that the boundary
term (6.247) vanishes.
6.29 Making operators formally self adjoint
We can make a generic real second-order linear homogeneous differential
operator
L0 = h2
d2
dx2 + h1
d
dx + h0
(6.255)
formally self adjoint
L = ‚àíd
dx
#
p(x) d
dx
$
+ q(x) = ‚àíp(x) d2
dx2 ‚àíp‚Ä≤(x) d
dx + q(x)
(6.256)
by Ô¨Årst dividing through by ‚àíh2(x)
L1 = ‚àí1
h2
L0 = ‚àíd2
dx2 ‚àíh1
h2
d
dx ‚àíh0
h2
(6.257)
and then by multiplying L1 by the positive prefactor
p(x) = exp
 x h1(y)
h2(y) dy

> 0.
(6.258)
The product p L1 then is formally self adjoint
L = p(x) L1 = ‚àíexp
 x h1(y)
h2(y) dy
 
d2
dx2 + h1(x)
h2(x)
d
dx + h0(x)
h2(x)

264

6.30 WRONSKIANS OF SELF-ADJOINT OPERATORS
= ‚àíd
dx
#
exp
 x h1(y)
h2(y) dy
 d
dx
$
‚àíexp
 x h1(y)
h2(y) dy
 h0(x)
h2(x)
= ‚àíd
dx

p d
dx

+ q
(6.259)
with q(x) = ‚àíp(x) h0(x)/h2(x). So we may turn any second-order linear homo-
geneous differential operator L into a formally self-adjoint operator L by
multiplying it by
œÅ(x) = ‚àíexp
2 x h1(y)/h2(y)dy

h2(x)
= ‚àíp(x)
h2(x).
(6.260)
The two differential equations L0u = 0 and Lu = œÅL0u = 0 have the same
solutions, and so we can restrict our attention to formally self-adjoint differ-
ential equations. But under the transformation (6.260), an eigenvalue equation
L0u = Œª u becomes Lu = œÅL0u = œÅŒªu, which is an eigenvalue equation
Lu = ‚àí(pu‚Ä≤)‚Ä≤ + qu = Œª œÅ u
(6.261)
with a weight function œÅ(x). Such an eigenvalue problem is known as a Sturm‚Äì
Liouville problem (Jacques Sturm, 1803‚Äì1855; Joseph Liouville, 1809‚Äì1882).
If h2(x) is negative (as for many positive operators), then the weight function
œÅ(x) = ‚àíp(x)/h2(x) is positive.
6.30 Wronskians of self-adjoint operators
We saw in (6.206‚Äì6.210) that if y1(x) and y2(x) are two linearly independent
solutions of the ODE
y‚Ä≤‚Ä≤(x) + P(x) y‚Ä≤(x) + Q(x) y(x) = 0
(6.262)
then their wronskian W(x) = y1(x) y‚Ä≤
2(x) ‚àíy2(x) y‚Ä≤
1(x) is
W(x) = W(x0) exp
#
‚àí
 x
x0
P(x‚Ä≤)dx‚Ä≤
$
.
(6.263)
Thus if we convert the ODE (6.262) to its formally self-adjoint form
‚àí

p(x)y‚Ä≤(x)
‚Ä≤ + q(x)y(x) = ‚àíp(x)d2y(x)
dx2
‚àíp‚Ä≤(x)dy(x)
dx
+ q(x)y(x) = 0 (6.264)
then P(x) = p‚Ä≤(x)/p(x), and so the wronskian (6.263) is
W(x) = W(x0) exp
#
‚àí
 x
x0
p‚Ä≤(x‚Ä≤)/p(x‚Ä≤)dx‚Ä≤
$
,
(6.265)
which we may integrate directly to
W(x) = W(x0) exp [‚àíln [p(x)/p(x0)]] = W(x0) p(x0)
p(x) .
(6.266)
265

DIFFERENTIAL EQUATIONS
We learned in (6.206‚Äì6.214) that if we had one solution y1(x) of the ODE
(6.262 or 6.264), then we could Ô¨Ånd another solution y2(x) that is linearly
independent of y1(x) as
y2(x) = y1(x)
 x W(x‚Ä≤)
y2
1(x‚Ä≤)
dx‚Ä≤.
(6.267)
In view of (6.263), this is an iterated integral. But if the ODE is formally self
adjoint, then the formula (6.266) reduces it to
y2(x) = y1(x)
 x
1
p(x‚Ä≤) y2
1(x‚Ä≤)
dx‚Ä≤
(6.268)
apart from a constant factor.
Example 6.35 (Legendre functions of the second kind)
Legendre‚Äôs self-adjoint
differential equation (6.252) is
‚àí

(1 ‚àíx2) y‚Ä≤‚Ä≤
= ‚Ñì(‚Ñì+ 1) y
(6.269)
and an obvious solution for ‚Ñì= 0 is y(x) ‚â°P0(x) = 1. Since p(x) = 1 ‚àíx2, the
integral formula (6.268) gives us as a second solution
Q0(x) = P0(x)
 x
1
p(x‚Ä≤) P2
0(x‚Ä≤)
dx‚Ä≤ =
 x
1
(1 ‚àíx2) dx‚Ä≤ = 1
2 ln
1 + x
1 ‚àíx

. (6.270)
This second solution Q0(x) is singular at both ends of the interval [‚àí1, 1] and so
does not satisfy the Dirichlet (6.245) or Neumann (6.246) boundary conditions
that make the system self adjoint or hermitian.
6.31 First-order self-adjoint differential operators
The Ô¨Årst-order differential operator
L = u d
dx + v
(6.271)
will be self adjoint if
 b
a
œá‚àóLœà dx =
 b
a

L‚Ä†œá
‚àó
œà dx =
 b
a
(Lœá)‚àóœà dx.
(6.272)
Starting from the Ô¨Årst term, we Ô¨Ånd
 b
a
œá‚àóLœà dx =
 b
a
œá‚àó
u œà‚Ä≤ + vœà

dx
=
 b
a

(‚àíœá‚àóu)‚Ä≤ + œá‚àóv

œà dx +

œá‚àóuœà
b
a
266

6.32 A CONSTRAINED VARIATIONAL PROBLEM
=
 b
a

(‚àíœáu‚àó)‚Ä≤ + œáv‚àó‚àóœà dx +

œá‚àóuœà
b
a
=
 b
a

‚àíu‚àóœá‚Ä≤ + (v‚àó‚àíu‚àó‚Ä≤)œá
‚àóœà dx +

œá‚àóuœà
b
a .
(6.273)
So if the boundary terms vanish

œá‚àóuœà
b
a = 0
(6.274)
and if both u‚àó= ‚àíu and v‚àó‚àíu‚àó‚Ä≤ = v, then
 b
a
œá‚àóLœà dx =
 b
a

uœá‚Ä≤ + vœá
‚àóœà dx =
 b
a
(Lœá)‚àóœà dx
(6.275)
and so L will be self adjoint or hermitian, L‚Ä† = L. The general form of a
Ô¨Årst-order self-adjoint linear operator is then
L = ir(x) d
dx + s(x) + i
2r‚Ä≤(x)
(6.276)
in which r and s are arbitrary real functions of x.
Example 6.36 (Momentum and angular momentum)
The momentum operator
p = ¬Øh
i
d
dx
(6.277)
has r = ‚àí¬Øh, which is real, and s = 0 and so is formally self adjoint. The boundary
terms (6.274) are zero if the functions œà and œá vanish at a and b, which often
are ¬±‚àû.
The angular-momentum operators Li = œµijk xj pk, where pk = ‚àíi¬Øh ‚àÇk, also are
formally self adjoint because the total antisymmetry of œµijk ensures that j and k
are different as they are summed from 1 to 3.
Example 6.37 (Momentum in a magnetic Ô¨Åeld)
In a magnetic Ô¨Åeld B = ‚àá√ó A,
the differential operator
¬Øh
i ‚àá‚àíe A
(6.278)
that (in mks units) represents the kinetic momentum mv is formally self adjoint
as is its Yang‚ÄìMills analog (11.471) when divided by i.
6.32 A constrained variational problem
In quantum mechanics, we usually deal with normalizable wave-functions. So
let‚Äôs Ô¨Ånd the function u(x) that minimizes the energy functional
E[u] =
 b
a

p(x) u‚Ä≤2(x) + q(x) u2(x)

dx
(6.279)
267

DIFFERENTIAL EQUATIONS
subject to the constraint that u(x) be normalized on [a, b] with respect to a
positive weight function œÅ(x)
N[u] = ‚à•u‚à•2 =
 b
a
œÅ(x) u2(x) dx = 1.
(6.280)
Introducing Œª as a Lagrange multiplier (section 1.23) and suppressing explicit
mention of the x-dependence of the real functions p, q, œÅ, and u, we minimize
the unconstrained functional
E[u, Œª] =
 b
a

p u‚Ä≤2 + q u2
dx ‚àíŒª
 b
a
œÅ u2 dx ‚àí1

,
(6.281)
which will be stationary at the function u that minimizes it. The Ô¨Årst-order
change in E[u, Œª] is
Œ¥E[u, Œª] =
 b
a

p 2u‚Ä≤ Œ¥u‚Ä≤ + q 2u Œ¥u ‚àíŒª œÅ 2u Œ¥u

dx,
(6.282)
in which the change in the derivative of u is Œ¥u‚Ä≤ = u‚Ä≤ + (Œ¥u)‚Ä≤ ‚àíu‚Ä≤ = (Œ¥u)‚Ä≤. Setting
Œ¥E = 0 and integrating by parts, we have
0 = 1
2 Œ¥E =
 b
a

p u‚Ä≤(Œ¥u)‚Ä≤ + (q ‚àíŒª œÅ) u Œ¥u

dx
=
 b
a

p u‚Ä≤Œ¥u
‚Ä≤ ‚àí

p u‚Ä≤‚Ä≤ Œ¥u + (q ‚àíŒª œÅ) u Œ¥u

dx
=
 b
a

‚àí

p u‚Ä≤‚Ä≤ + (q ‚àíŒª œÅ) u

Œ¥u dx +

p u‚Ä≤Œ¥u
b
a.
(6.283)
So if E is to be stationary with respect to all tiny changes Œ¥u, then u must satisfy
both the self-adjoint differential equation
0 = ‚àí

p u‚Ä≤‚Ä≤ + (q ‚àíŒª œÅ) u
(6.284)
and the natural boundary conditions
0 = p(b) u‚Ä≤(b)
and
0 = p(a) u‚Ä≤(a).
(6.285)
If instead we require E[u, Œª] to be stationary with respect to all variations Œ¥u that
vanish at the endpoints, Œ¥u(a) = Œ¥u(b) = 0, then u must satisfy the differential
equation (6.284) but need not satisfy the natural boundary conditions (6.285).
In both cases, the function u(x) that minimizes the energy E[u] subject to
the normalization condition N[u] = 1 is an eigenfunction of the formally self-
adjoint differential operator
L = ‚àíd
dx

p(x) d
dx

+ q(x)
(6.286)
268

6.32 A CONSTRAINED VARIATIONAL PROBLEM
with eigenvalue Œª
Lu = ‚àí

p u‚Ä≤‚Ä≤ + q u = Œª œÅ u.
(6.287)
The Lagrange multiplier Œª has become an eigenvalue of a Sturm‚ÄìLiouville
equation (6.261).
Is the eigenvalue Œª related to E[u] and N[u]? To keep things simple, we restrict
ourselves to a regular and self-adjoint differential system (section 6.28) con-
sisting of the self-adjoint differential operator (6.286), the differential equation
(6.287), and a domain D = D‚àóof functions u(x) that are twice differentiable on
[a, b] and that satisfy two homogeneous Dirichlet (6.245) or Neumann (6.246)
boundary conditions on [a, b]. All functions u in the domain D therefore satisfy

upu‚Ä≤b
a = 0.
(6.288)
We now multiply the Sturm‚ÄìLiouville equation (6.287) from the left by u and
integrate from a to b. After integrating by parts and noting the vanishing of the
boundary terms (6.288), we Ô¨Ånd
Œª
 b
a
œÅ u2 dx =
 b
a
u Lu dx =
 b
a
u

‚àí

p u‚Ä≤‚Ä≤ + q u

dx
=
 b
a

p u‚Ä≤2 + q u2
dx ‚àí

upu‚Ä≤b
a
=
 b
a

p u‚Ä≤2 + q u2
dx = E[u].
(6.289)
Thus in view of the normalization constraint (6.280), we see that the eigenvalue
Œª is the ratio of the energy E[u] to the norm N[u]
Œª =
 b
a

p u‚Ä≤2 + q u2
dx
 b
a
œÅ u2 dx
= E[u]
N[u].
(6.290)
But is the function that minimizes the ratio
R[u] ‚â°E[u]
N[u]
(6.291)
the eigenfunction u of the Sturm‚ÄìLiouville equation (6.287)? And is the mini-
mum of R[u] the least eigenvalue Œª of the Sturm‚ÄìLiouville equation (6.287)? To
see that the answers are yes and yes, we require Œ¥R[u] to vanish
Œ¥R[u] = Œ¥E[u]
N[u] ‚àíE[u] Œ¥N[u]
N2[u]
= 0
(6.292)
269

DIFFERENTIAL EQUATIONS
to Ô¨Årst order in tiny changes Œ¥u(x) that are zero at the endpoints of the interval,
Œ¥u(a) = Œ¥u(b) = 0. Multiplying both sides by N[u], we have
Œ¥E[u] = R[u] Œ¥N[u].
(6.293)
Referring back to our derivation (6.281‚Äì6.283) of the Sturm‚ÄìLiouville equa-
tion, we see that since Œ¥u(a) = Œ¥u(b) = 0, the change Œ¥E is
Œ¥E[u] = 2
 b
a

‚àí

p u‚Ä≤‚Ä≤ + q u

Œ¥u dx + 2

p u‚Ä≤Œ¥u
b
a
= 2
 b
a

‚àí

p u‚Ä≤‚Ä≤ + q u

Œ¥u dx
(6.294)
while Œ¥N is
Œ¥N[u] = 2
 b
a
œÅ u Œ¥u dx.
(6.295)
Substituting these changes (6.294) and (6.295) into the condition (6.293) that
R[u] be stationary, we Ô¨Ånd that the integral
 b
a

‚àí

p u‚Ä≤‚Ä≤ + (q ‚àíR[u] œÅ ) u

Œ¥u dx = 0
(6.296)
must vanish for all tiny changes Œ¥u(x) that are zero at the endpoints of the inter-
val. Thus on [a, b], the function u that minimizes the ratio R[u] must satisfy the
Sturm‚ÄìLiouville equation (6.287)
‚àí

p u‚Ä≤‚Ä≤ + q u = R[u] œÅ u
(6.297)
with an eigenvalue Œª ‚â°R[u] that is the minimum value of the ratio R[u].
So the eigenfunction u1 with the smallest eigenvalue Œª1 is the one that min-
imizes the ratio R[u], and Œª1 = R[u1]. What about other eigenfunctions with
larger eigenvalues? How do we Ô¨Ånd the eigenfunction u2 with the next smallest
eigenvalue Œª2? Simple: we minimize R[u] with respect to all functions u that are
in the domain D and that are orthogonal to u1.
Example 6.38 (InÔ¨Ånite square well)
Let us consider a particle of mass m
trapped in an interval [a, b] by a potential that is V for a < x < b but inÔ¨Ånite for
x < a and for x > b. Because the potential is inÔ¨Ånite outside the interval, the
wave-function u(x) will satisfy the boundary conditions
u(a) = u(b) = 0.
(6.298)
The mean value of the hamiltonian is then the energy functional
‚ü®u|H|u‚ü©= E[u] =
 b
a

p(x) u‚Ä≤2(x) + q(x) u2(x)

dx,
(6.299)
270

6.32 A CONSTRAINED VARIATIONAL PROBLEM
in which p(x) = ¬Øh2/2m and q(x) = V, a constant independent of x. Wave-
functions in quantum mechanics are normalized when possible. So we need to
minimize the functional
E[u] =
 b
a

¬Øh2
2m u‚Ä≤2(x) + V u2(x)

dx
(6.300)
subject to the constraint
c =
 b
a
u2(x) dx ‚àí1 = 0
(6.301)
for all tiny variations Œ¥u that vanish at the endpoints of the interval. The weight
function œÅ(x) = 1, and the eigenvalue equation (6.287) is
‚àí¬Øh2
2m u‚Ä≤‚Ä≤ + V u = Œª u.
(6.302)
For any positive integer n, the normalized function
un(x) =

2
b ‚àía
1/2
sin

nœÄ x ‚àía
b ‚àía

(6.303)
satisÔ¨Åes the boundary conditions (6.298) and the eigenvalue equation (6.302)
with energy eigenvalue
Œªn = E[un] = 1
2m
 nœÄ ¬Øh
b ‚àía
2
+ V.
(6.304)
The second eigenfunction u2 minimizes the energy functional E[u] over the space
of normalized functions that satisfy the boundary conditions (6.298) and are
orthogonal to the Ô¨Årst eigenfunction u1. The eigenvalue Œª2 is higher than Œª1
(four times higher). As the quantum number n increases, the energy Œªn = E[un]
goes to inÔ¨Ånity as n2. That Œªn ‚Üí‚àûas n ‚Üí‚àûis related (section 6.35) to the
completeness of the eigenfunctions un.
Example 6.39 (Bessel‚Äôs system)
Bessel‚Äôs energy functional is
E[u] =
 1
0

x u‚Ä≤2(x) + n2
x u2(x)

dx,
(6.305)
in which n ‚â•0 is an integer. We seek the minimum of this functional over the set
of twice-differentiable functions u(x) on [0, 1] that are normalized
N[u] = ‚à•u‚à•2 =
 1
0
x u2(x) dx = 1
(6.306)
and that satisfy the boundary conditions u(0) = 0 for n > 0 and u(1) = 0.
We‚Äôll use a Lagrange multiplier Œª (section 1.23) and minimize the unconstrained
functional E[u] ‚àíŒª (N[u] ‚àí1). Proceeding as in (6.279‚Äì6.287), we Ô¨Ånd that u
must obey the formally self-adjoint differential equation
271

DIFFERENTIAL EQUATIONS
L u = ‚àí(x u‚Ä≤)‚Ä≤ + n2
x u = Œª x u.
(6.307)
The ratio formula (6.290) and the positivity of Bessel‚Äôs energy functional (6.305)
tell us that the eigenvalues Œª = E[u]/N[u] are positive (exercise 6.25). As we‚Äôll
see in a moment, the boundary conditions largely determine these eigenvalues
Œªn,m ‚â°k2
n,m. By changing variables to œÅ = kn,mx and letting u(x) = Jn(œÅ), we
arrive (exercise 6.26) at
d2Jn
dœÅ2 + 1
œÅ
dJn
dœÅ +

1 ‚àín2
œÅ2

Jn = 0,
(6.308)
which is Bessel‚Äôs equation. The eigenvalues are determined by the condition
u(1) = Jn(kn,m) = 0; they are the squares of the zeros of Jn(œÅ). The eigenfunc-
tion of the self-adjoint differential equation (6.307) with eigenvalue Œªn,m = k2
n,m
is um(x) = Jn(kn,mx). The parameter n labels the differential system; it is not
an eigenvalue. Asymptotically as m ‚Üí‚àû, one has (Courant and Hilbert, 1955,
p. 416)
lim
m‚Üí‚àû
Œªn,m
m2œÄ2 = 1,
(6.309)
which shows that the eigenvalues Œªn,m rise like m2 as m ‚Üí‚àû.
Example 6.40 (Harmonic oscillator)
We‚Äôll minimize the energy
E[u] =
 ‚àû
‚àí‚àû

¬Øh2
2m u‚Ä≤2(x) + 1
2m œâ2 x2 u2(x)

dx
(6.310)
subject to the normalization condition
N[u] = ‚à•u‚à•2 =
 ‚àû
‚àí‚àû
u2(x) dx = 1.
(6.311)
We introduce Œª as a Lagrange multiplier and Ô¨Ånd the minimum of the uncon-
strained function E[u]‚àíŒª (N[u] ‚àí1). Following equations (6.279‚Äì6.287), we Ô¨Ånd
that u must satisfy Schr√∂dinger‚Äôs equation
‚àí¬Øh2
2mu‚Ä≤‚Ä≤ + 1
2m œâ2 x2 u = Œªu,
(6.312)
which we write as
¬Øhœâ
#mœâ
2¬Øh

x ‚àí
¬Øh
mœâ
d
dx
 
x +
¬Øh
mœâ
d
dx

+ 1
2
$
u = Œªu.
(6.313)
The lowest eigenfunction u0 is mapped to zero by the second factor

x +
¬Øh
mœâ
d
dx

u0(x) = 0
(6.314)
272

6.33 EIGENFUNCTIONS AND EIGENVALUES
so its eigenvalue Œª0 is ¬Øhœâ/2. Integrating this differential equation, we get
u0(x) =
mœâ
œÄ ¬Øh
1/4
exp

‚àímœâx2
2¬Øh

,
(6.315)
in which the prefactor is a normalization constant. As in section 2.11, one may
get the higher eigenfunctions by acting on u0 with powers of the Ô¨Årst factor inside
the square brackets (6.313)
un(x) =
1
‚àö
n!
mœâ
2¬Øh
n/2 
x ‚àí
¬Øh
mœâ
d
dx
n
u0(x).
(6.316)
The eigenvalue of un is Œªn = ¬Øhœâ(n + 1/2). Again, Œªn ‚Üí‚àûas n ‚Üí‚àû.
6.33 Eigenfunctions and eigenvalues of self-adjoint systems
A regular Sturm‚ÄìLiouville system is a set of regular and self-adjoint differ-
ential systems (section 6.28) that have the same differential operator, interval
[a, b], boundary conditions, and domain, and whose differential equations are
of Sturm‚ÄìLiouville (6.287) type
L œà = ‚àí(p œà‚Ä≤)‚Ä≤ + q œà = Œª œÅ œà,
(6.317)
each distinguished by an eigenvalue Œª. The functions p, q, and œÅ are real and
continuous, p and œÅ are positive on [a, b], but the weight function œÅ may vanish
at isolated points of the interval.
Since the differential systems are self adjoint, the real or complex functions in
the common domain D are twice differentiable on the interval [a, b] and satisfy
two homogeneous boundary conditions that make the boundary terms (6.247)
vanish
p W(œà‚Ä≤, œà‚àó)
b
a = 0
(6.318)
and so the differential operator L obeys the condition (6.249)
(œá, L œà) =
 b
a
œá‚àóL œà dx =
 b
a
(L œá)‚àóœà dx = (L œá, œà)
(6.319)
of being self adjoint or hermitian.
Let œài and œàj be eigenfunctions of L with eigenvalues Œªi and Œªj
L œài = Œªi œÅ œài
and
L œàj = Œªj œÅ œàj
(6.320)
in a regular Sturm‚ÄìLiouville system. Multiplying the Ô¨Årst of these eigenvalue
equations by œà‚àó
j and the complex conjugate of the second by œài, we get
œà‚àó
j L œài = œà‚àó
j Œªi œÅ œài
and
œài(L œàj)‚àó= œàiŒª‚àó
j œÅ œà‚àó
j .
(6.321)
273

DIFFERENTIAL EQUATIONS
Integrating the difference of these equations over the interval [a, b] and using
(6.319) in the form
2 b
a œà‚àó
j L œài dx =
2 b
a (L œàj)‚àóœài dx, we have
0 =
 b
a

œà‚àó
j L œài ‚àí(L œàj)‚àóœài

dx =

Œªi ‚àíŒª‚àó
j
  b
a
œà‚àó
j œài œÅ dx.
(6.322)
Setting i = j, we Ô¨Ånd
0 =

Œª‚àó
i ‚àíŒªi
  b
a
œÅ |œài|2 dx,
(6.323)
which, since the integral is positive, shows that the eigenvalue Œªi must be
real. All the eigenvalues of a regular Sturm‚ÄìLiouville system are real. Using
Œª‚àó
j = Œªj in (6.322), we see that eigenfunctions that have different eigenvalues are
orthogonal on the interval [a, b] with weight function œÅ(x)
0 =

Œªi ‚àíŒªj
  b
a
œà‚àó
j œÅ œài dx.
(6.324)
Since the differential operator L, the eigenvalues Œªi, and the weight function
œÅ are all real, we may write the Ô¨Årst of the eigenvalue equations in (6.320) both
as L œài = Œªi œÅ œài and as L œà‚àó
i = Œªi œÅ œà‚àó
i . By adding these two equations, we see
that the real part of œài satisÔ¨Åes them, and by subtracting them, we see that the
imaginary part of œài also satisÔ¨Åes them. So it might seem that œài = ui + ivi is
made of two real eigenfunctions with the same eigenvalue.
But each eigenfunction ui in the domain D satisÔ¨Åes two homogeneous
boundary conditions as well as its second-order differential equation
‚àí(p u‚Ä≤
i)‚Ä≤ + q ui = Œªi œÅ ui
(6.325)
and so ui is the unique solution in D to this equation. There can be no other
eigenfunction in D with the same eigenvalue. In a regular Sturm‚ÄìLiouville sys-
tem, there is no degeneracy. All the eigenfunctions ui are orthogonal and can be
normalized on the interval [a, b] with weight function œÅ(x)
 b
a
u‚àó
j œÅ ui dx = Œ¥ij.
(6.326)
They may be taken to be real.
It is true that the eigenfunctions of a second-order differential equation come
in pairs because one can use Wronski‚Äôs formula (6.268)
y2(x) = y1(x)
 x
dx‚Ä≤
p(x‚Ä≤) y2
1(x‚Ä≤)
(6.327)
to Ô¨Ånd a linearly independent second solution with the same eigenvalue. But
the second solutions don‚Äôt obey the boundary conditions of the domain. Bessel
functions of the second kind, for example, are inÔ¨Ånite at the origin.
274

6.34 UNBOUNDEDNESS OF EIGENVALUES
A set of eigenfunctions ui is complete in the mean in a space S of functions if
every function f ‚ààS can be represented as a series
f (x) =
‚àû

i=1
aiui(x)
(6.328)
(called a Fourier series) that converges in the mean, that is
lim
N‚Üí‚àû
 b
a
œÅf (x) ‚àí
N

i=1
aiui(x)

2
œÅ(x) dx = 0.
(6.329)
The natural space S is the space L2(a, b) of all functions f that are square-
integrable on the interval [a, b]
 b
a
|f (x)|2 œÅ(x) dx < ‚àû.
(6.330)
The orthonormal eigenfunctions of every regular Sturm‚ÄìLiouville system on
an interval [a, b] are complete in the mean in L2(a, b). The completeness of these
eigenfunctions follows (section 6.35) from the fact that the eigenvalues Œªn of
a regular Sturm‚ÄìLiouville system are unbounded: when arranged in ascending
order Œªn < Œªn+1 they go to inÔ¨Ånity with the index n
lim
n‚Üí‚àûŒªn = ‚àû
(6.331)
as we‚Äôll see in the next section.
6.34 Unboundedness of eigenvalues
We have seen (section 6.32) that the function u(x) that minimizes the ratio
R[u] = E[u]
N[u] =
 b
a

p u‚Ä≤2 + q u2
dx
 b
a
œÅ u2 dx
(6.332)
is a solution of the Sturm‚ÄìLiouville equation
Lu = ‚àí

p u‚Ä≤‚Ä≤ + q u = Œª œÅ u
(6.333)
with eigenvalue
Œª = E[u]
N[u].
(6.334)
275

DIFFERENTIAL EQUATIONS
Let us call this least value of the ratio (6.332) Œª1; it also is the smallest eigenvalue
of the differential equation (6.333). The second smallest eigenvalue Œª2 is the
minimum of the same ratio (6.332) but for functions that are orthogonal to u1
 b
a
œÅ u1 u2 dx = 0.
(6.335)
And Œª3 is the minimum of the ratio R[u] but for functions that are orthogonal
to both u1 and u2. Continuing in this way, we make a sequence of orthogonal
eigenfunctions un(x) (which we can normalize, N[un] = 1) with eigenvalues Œª1 ‚â§
Œª2 ‚â§Œª3 ‚â§¬∑ ¬∑ ¬∑ Œªn. How do the eigenvalues Œªn behave as n ‚Üí‚àû?
Since the function p(x) is positive for a < x < b, it is clear that the energy
functional (6.279)
E[u] =
 b
a

p u‚Ä≤2 + q u2
dx
(6.336)
gets bigger as u‚Ä≤2 increases. In fact, if we let the function u(x) zigzag up and
down about a given curve ¬Øu, then the kinetic energy
2
pu‚Ä≤2dx will rise but the
potential energy
2
qu2dx will remain approximately constant. Thus by increas-
ing the frequency of the zigzags, we can drive the energy E[u] to inÔ¨Ånity. For
instance, if u(x) = sin x, then its zigzag version uœâ(x) = u(x)(1 + 0.2 sin œâx) will
have higher energy. The case of œâ = 100 is illustrated in Fig. 6.1. As œâ ‚Üí‚àû,
its energy E[uœâ] ‚Üí‚àû.
It is therefore intuitively clear (or at least plausible) that if the real functions
p(x), q(x), and œÅ(x) are continuous on [a, b] and if p(x) > 0 and œÅ(x) > 0
on (a, b), then there are inÔ¨Ånitely many energy eigenvalues Œªn, and that they
increase without limit as n ‚Üí‚àû
lim
n‚Üí‚àûŒªn = ‚àû.
(6.337)
Courant and Hilbert (Richard Courant, 1888‚Äì1972, and David Hilbert,
1862‚Äì1943) provide several proofs of this result (Courant and Hilbert, 1955,
pp. 397‚Äì429). One of their proofs involves the change of variables f = (pœÅ)1/4
and v = fu, after which the eigenvalue equation
L u = ‚àí

p u‚Ä≤‚Ä≤ + q u = ŒªœÅu
(6.338)
becomes Lf v = ‚àív‚Ä≤‚Ä≤ +rv = Œªvv with r = f ‚Ä≤‚Ä≤/f +q/œÅ. Were this r(x) a constant,
the eigenfunctions of Lf would be vn(x) = sin(nœÄ/(b ‚àía)) with eigenvalues
Œªvn =
 n œÄ
b ‚àía
2
+ r
(6.339)
276

6.35 COMPLETENESS OF EIGENFUNCTIONS
0
0.5
1
1.5
2
2.5
3
0
0.2
0.4
0.6
0.8
1
1.2
x
u (x), uœâ (x)
Two functions with very diÔ¨Äerent kinetic energies
Figure 6.1
The energy functional E[u] of equation (6.279) assigns a much higher
energy to the function uœâ(x) = u(x)(1 + 0.2 sin(œâx)) (zigzag curve with œâ = 100) than
to the function u(x) = sin(x) (smooth curve). As the frequency œâ ‚Üí‚àû, the energy
E[u2] ‚Üí‚àû.
rising as n2. Courant and Hilbert show that as long as r(x) is bounded for a ‚â§
x ‚â§b, the actual eigenvalues of Lf are Œªv,n = c n2 + dn in which dn is bounded
and that the eigenvalues Œªn of L differ from the Œªv,n by a scale factor, so that
they too diverge as n ‚Üí‚àû
lim
n‚Üí‚àû
n2
Œªn
= g
(6.340)
where g is a constant.
6.35 Completeness of eigenfunctions
We have seen in section 6.34 that the eigenvalues of every regular Sturm‚Äì
Liouville system when arranged in ascending order tend to inÔ¨Ånity with the
index n
lim
n‚Üí‚àûŒªn = ‚àû.
(6.341)
277

DIFFERENTIAL EQUATIONS
We‚Äôll now use this property to show that the corresponding eigenfunctions un(x)
are complete in the mean (6.329) in the domain D of the system.
To do so, we follow Courant and Hilbert (Courant and Hilbert, 1955,
pp. 397‚Äì428) and extend the energy E and norm N functionals to inner products
on the domain of the system
E[f , g] ‚â°
 b
a

p(x) f ‚Ä≤(x) g‚Ä≤(x) + q(x) f (x) g(x)

dx,
(6.342)
N[f , g] ‚â°
 b
a
œÅ(x) f (x) g(x) dx
(6.343)
for any f and g in D. Integrating E[f , g] by parts, we have
E[f , g] =
 b
a

p f g‚Ä≤‚Ä≤ ‚àíf

pg‚Ä≤‚Ä≤ + q f g

dx
=
 b
a

‚àíf

pg‚Ä≤‚Ä≤ + f q g

dx +

p f g‚Ä≤b
a
(6.344)
or in terms of the self-adjoint differential operator L of the system
E[f , g] =
 b
a
f L g dx +

p f g‚Ä≤b
a .
(6.345)
Since the boundary term vanishes (6.288) when the functions f and g are in the
domain D of the system, it follows that for f and g in D
E[f , g] =
 b
a
f L g dx.
(6.346)
We can use the Ô¨Årst n orthonormal eigenfunctions uk of the system
L uk = Œªk œÅ uk
(6.347)
to approximate an arbitrary function in f ‚ààD as the linear combination
f (x) ‚àº
n

k=1
ck uk(x)
(6.348)
with coefÔ¨Åcients ck given by
ck = N[f , uk] =
 b
a
œÅ f uk dx.
(6.349)
We‚Äôll show that this series converges in the mean to the function f .
By construction (6.349), the remainder or error of the nth sum
rn(x) = f (x) ‚àí
n

k=1
ck uk(x)
(6.350)
278

6.35 COMPLETENESS OF EIGENFUNCTIONS
is orthogonal to the Ô¨Årst n eigenfunctions
N[rn, uk] = 0
for
k = 1, . . . , n.
(6.351)
The next eigenfunction un+1 minimizes the ratio
R[œÜ] = E[œÜ, œÜ]
N[œÜ, œÜ]
(6.352)
over all œÜ that are orthogonal to the Ô¨Årst n eigenfunctions uk in the sense that
N[œÜ, uk] = 0 for k = 1, . . . , n. That minimum is the eigenvalue Œªn+1
R[un+1] = Œªn+1,
(6.353)
which therefore must be less than the ratio R[rn]
Œªn+1 ‚â§R[rn] = E[rn, rn]
N[rn, rn].
(6.354)
Thus the square of the norm of the remainder is bounded by the ratio
‚à•rn‚à•2 ‚â°N[rn, rn] ‚â§E[rn, rn]
Œªn+1
.
(6.355)
So since Œªn+1 ‚Üí‚àûas n ‚Üí‚àû, we‚Äôre done if we can show that the energy
E[rn, rn] of the remainder is bounded.
This energy is
E[rn, rn] = E

f ‚àí
n

k=1
ckuk, f ‚àí
n

k=1
ckuk

= E[f , f ] ‚àí
n

k=1
ck (E[f , uk] + E[uk, f ]) +
n

k=1
n

‚Ñì=1
ckc‚ÑìE[uk, u‚Ñì]
= E[f , f ] ‚àí2
n

k=1
ck E[f , uk] +
n

k=1
n

‚Ñì=1
ckc‚ÑìE[uk, u‚Ñì].
(6.356)
Since f and all the uk are in the domain of the system, they satisfy the boundary
condition (6.247 or 6.318), and so (6.345, 6.347, & 6.326) imply that
E[f , uk] =
 b
a
f Luk dx = Œªk
 b
a
œÅ f uk dx = Œªk ck
(6.357)
and that
E[uk, u‚Ñì] =
 b
a
uk Lu‚Ñìdx = Œª‚Ñì
 b
a
œÅ uk u‚Ñìdx = Œªk Œ¥k,‚Ñì.
(6.358)
Using these relations to simplify our formula (6.356) for E[rn, rn] we Ô¨Ånd
E[rn, rn] = E[f , f ] ‚àí
n

k=1
Œªkc2
k.
(6.359)
279

DIFFERENTIAL EQUATIONS
Since Œªn ‚Üí‚àûas n ‚Üí‚àû, we can be sure that for high enough n, the sum
n

k=1
Œªkc2
k > 0
for
n > N
(6.360)
is positive. It follows from (6.359) that the energy of the remainder rn is bounded
by that of the function f
E[rn, rn] = E[f , f ] ‚àí
n

k=1
Œªkc2
k ‚â§E[f , f ].
(6.361)
By substituting this upper bound E[f , f ] on E[rn, rn] into our upper bound
(6.355) on the squared norm ‚à•rn‚à•2 of the remainder, we Ô¨Ånd
‚à•rn‚à•2 ‚â§E[f , f ]
Œªn+1
.
(6.362)
Thus since Œªn ‚Üí‚àûas n ‚Üí‚àû, we see that the series (6.348) converges in the
mean (section 4.3) to f
lim
n‚Üí‚àû‚à•rn‚à•2 = lim
n‚Üí‚àû‚à•f ‚àí
n

k=1
ckuk‚à•2 ‚â§lim
n‚Üí‚àû
E[f , f ]
Œªn+1
= 0.
(6.363)
The eigenfunctions uk of a regular Sturm‚ÄìLiouville system are therefore
complete in the mean in the domain D of the system. They span D.
It is a short step from spanning D to spanning the space L2(a, b) of functions
that are square integrable on the interval [a, b] of the system. To take this step,
we assume that the domain D is dense in L2(a, b), that is, that for every function
g ‚ààL2(a, b) there is a sequence of functions fn ‚ààD that converges to it in the
mean so that for any œµ > 0 there is an integer N1 such that
‚à•g ‚àífn‚à•2 ‚â°
 b
a
|g(x) ‚àífn(x)|2 œÅ(x) dx < œµ
for
n > N1.
(6.364)
Since fn ‚ààD, we can Ô¨Ånd a series of eigenfunctions uk of the system that con-
verges in the mean to fn so that for any œµ > 0 there is an integer N2 such
that
‚à•fn ‚àí
N

k=1
cn,k uk‚à•2 ‚â°
 b
a
fn(x) ‚àí
N

k=1
cn,k uk(x)

2
œÅ(x) dx < œµ
for
N > N2.
(6.365)
The Schwarz inequality (1.99) applies to these inner products, and so
‚à•g ‚àí
N

k=1
cn,k uk‚à•‚â§‚à•g ‚àífn‚à•+ ‚à•fn(x) ‚àí
N

k=1
cn,k uk‚à•.
(6.366)
280

6.35 COMPLETENESS OF EIGENFUNCTIONS
Combining the last three inequalities, we have for n > N1 and N > N2
‚à•g ‚àí
N

k=1
cn,k uk‚à•< 2 ‚àöœµ.
(6.367)
So the eigenfunctions uk of a regular Sturm‚ÄìLiouville system span the space of
functions that are square integrable on its interval L2(a, b).
One may further show (Courant and Hilbert, 1955, p. 360; Stakgold, 1967,
p. 220) that the eigenfunctions uk(x) of any regular Sturm‚ÄìLiouville system
form a complete orthonormal set in the sense that every function f (x) that
satisÔ¨Åes Dirichlet (6.245) or Neumann (6.246) boundary conditions and has a
continuous Ô¨Årst and a piecewise continuous second derivative may be expanded
in a series
f (x) =
‚àû

k=1
ak uk(x)
(6.368)
that converges absolutely and uniformly on the interval [a, b] of the system.
Our discussion (6.341‚Äì6.363) of the completeness of the eigenfunctions of a
regular Sturm‚ÄìLiouville system was insensitive to the Ô¨Ånite length of the inter-
val [a, b] and to the positivity of p(x) on [a, b]. What was essential was the
vanishing of the boundary terms (6.247), which can happen if p vanishes at the
endpoints of a Ô¨Ånite interval or if the functions u and v tend to zero as |x| ‚Üí‚àû
on an inÔ¨Ånite one. This is why the results of this section have been extended to
singular Sturm‚ÄìLiouville systems made of self-adjoint differential systems that
are singular because the interval is inÔ¨Ånite or has p vanishing at one or both of
its ends.
If the eigenfunctions uk are orthonormal with weight function œÅ(x)
Œ¥k‚Ñì=
 b
a
uk(x) œÅ(x) u‚Ñì(x) dx
(6.369)
then the coefÔ¨Åcients ak of the expansion (6.348) are given by the integrals
(6.349)
ak =
 b
a
uk(x) œÅ(x) f (x) dx.
(6.370)
By combining equations (6.328) and (6.370), we have
f (x) =
‚àû

k=1
 b
a
uk(y) œÅ(y) f (y) dy uk(x)
(6.371)
or rearranging
f (x) =
 b
a
f (y)
 ‚àû

k=1
uk(y) uk(x) œÅ(y)

dy,
(6.372)
281

DIFFERENTIAL EQUATIONS
which implies the representation
Œ¥(x ‚àíy) = œÅ(y)
‚àû

k=1
uk(x) uk(y)
(6.373)
of Dirac‚Äôs delta function. But since this series is nonzero only for x = y, the
weight function œÅ(y) is just a scale factor, and we can write for 0 ‚â§Œ± ‚â§1
Œ¥(x ‚àíy) = œÅŒ±(x) œÅ1‚àíŒ±(y)
‚àû

k=1
uk(x) uk(y).
(6.374)
These representations of the delta functional are suitable for functions f in the
domain D of the regular Sturm‚ÄìLiouville system.
Example 6.41 (A Bessel representation of the delta function)
Bessel‚Äôs nth
system L u =
‚àí(x u‚Ä≤)‚Ä≤ + n2 u/x = Œª x u has eigenvalues Œª = z2
n,k that
are the squares of the zeros of the Bessel function Jn(x). The eigenfunctions
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
‚àí200
0
200
400
600
800
1000
x
J0 Series
1000-term Bessel series for Dirac delta function
Figure 6.2
The sum of the Ô¨Årst 1000 terms of the Bessel representation (6.376)
for the Dirac delta function Œ¥(x ‚àíy) is plotted for y = 1/3 and Œ± = 0, for y = 1/2
and Œ± = 1/2, and for y = 2/3 and Œ± = 1.
282

6.35 COMPLETENESS OF EIGENFUNCTIONS
(section 9.1) that are orthonormal with weight function œÅ(x) = x are u(n)
k (x) =
‚àö
2 Jn(zn,kx)/Jn+1(zn,k). Thus, by (6.374), we can represent Dirac‚Äôs delta func-
tional for functions in the domain D of Bessel‚Äôs system as
Œ¥(x ‚àíy) = xŒ± y1‚àíŒ±
‚àû

k=1
u(n)
k (x) u(n)
k (y).
(6.375)
For n = 0, this Bessel representation is
Œ¥(x ‚àíy) = 2 xŒ± y1‚àíŒ±
‚àû

k=1
J0(z0,kx)J0(z0,ky)
J2
1(z0,k)
.
(6.376)
Figure 6.2 plots the Ô¨Årst 1000 terms of this sum (6.376) for Œ± = 0 and y = 1/3,
for Œ± = 1/2 and y = 1/2, and for Œ± = 1 and y = 2/3. Figure 6.3 plots the Ô¨Årst
10,000 terms of the same series but for Œ± = 0 and y = 0.47, for Œ± = 1/2 and
y = 1/2, and for Œ± = 1 and y = 0.53. The integrals of these 10,000-term sums
from 0 to 1 respectively are 0.9966, 0.9999, and 0.9999. These plots illustrate the
Sturm‚ÄìLiouville representation (6.374) of the delta function.
0.46
0.47
0.48
0.49
0.5
0.51
0.52
0.53
0.54
‚àí2000
0
2000
4000
6000
8000
10000
x
J0 Series
10,000-term Bessel series for Dirac delta function
Figure 6.3
The sum of the Ô¨Årst 10,000 terms of the Bessel representation (6.376)
for the Dirac delta function Œ¥(x ‚àíy) is plotted for y = 0.47 and Œ± = 0, for y = 1/2
and Œ± = 1/2, and for y = 0.53 and Œ± = 1.
283

DIFFERENTIAL EQUATIONS
6.36 The inequalities of Bessel and Schwarz
The inequality
 b
a
œÅ(x)
f (x) ‚àí
N

k=1
akuk(x)

2
dx ‚â•0
(6.377)
and the formula (6.370) for ak lead (exercise 6.27) to Bessel‚Äôs inequality
 b
a
œÅ(x) |f (x)|2 dx ‚â•
‚àû

k=1
|ak|2.
(6.378)
The argument we used to derive the Schwarz inequality (1.94) for vectors
applies also to functions and leads to the Schwarz inequality
 b
a
œÅ(x)|f (x)|2 dx
 b
a
œÅ(x)|g(x)|2 dx ‚â•

 b
a
œÅ(x)g‚àó(x)f (x) dx

2
.
(6.379)
6.37 Green‚Äôs functions
Physics is full of equations of the form
L G(x) = Œ¥(n)(x),
(6.380)
in which L is a differential operator in n variables. The solution G(x) is a Green‚Äôs
function (section 3.8) for the operator L.
Example 6.42 (Poisson‚Äôs Green‚Äôs function)
Probably the most important
Green‚Äôs function arises when the interaction is of long range as in gravity and
electrodynamics. The divergence of the electric Ô¨Åeld is related to the charge den-
sity œÅ by Gauss‚Äôs law ‚àá¬∑ E = œÅ/œµ0 where œµ0 = 8.854 √ó 10‚àí12 F/m is the electric
constant. The electric Ô¨Åeld is E = ‚àí‚àáœÜ ‚àíÀôA in which œÜ is the scalar potential. In
the Coulomb or radiation gauge, the divergence of A vanishes, ‚àá¬∑ A = 0, and so
‚àí‚ñ≥œÜ = ‚àí‚àá¬∑ ‚àáœÜ = œÅ/œµ0. The needed Green‚Äôs function satisÔ¨Åes
‚àí‚ñ≥G(x) = ‚àí‚àá¬∑ ‚àáG(x) = Œ¥(3)(x)
(6.381)
and expresses the scalar potential œÜ as the integral
œÜ(t, x) =

G(x ‚àíx‚Ä≤) œÅ(t, x‚Ä≤)
œµ0
d3x‚Ä≤.
(6.382)
284

6.37 GREEN‚ÄôS FUNCTIONS
For when we apply (minus) the Laplacian to it, we get
‚àí‚ñ≥œÜ(t, x) = ‚àí

‚ñ≥G(x ‚àíx‚Ä≤) œÅ(t, x‚Ä≤)
œµ0
d3x‚Ä≤
=

Œ¥(3)(x ‚àíx‚Ä≤) œÅ(t, x‚Ä≤)
œµ0
d3x‚Ä≤ = œÅ(t, x)
œµ0
,
(6.383)
which is Poisson‚Äôs equation.
The reader might wonder how the potential œÜ(t, x) can depend upon the
charge density œÅ(t, x‚Ä≤) at different points at the same time. The scalar poten-
tial is instantaneous because of the Coulomb gauge condition ‚àá¬∑ A = 0, which
is not Lorentz invariant. The gauge-invariant physical Ô¨Åelds E and B are not
instantaneous and do describe Lorentz-invariant electrodynamics.
It is easy to Ô¨Ånd the Green‚Äôs function G(x) by expressing it as a Fourier
transform
G(x) =

eik¬∑x g(k) d3k
(6.384)
and by using the three-dimensional version
Œ¥(3)(x) =

d3k
(2œÄ)3 eik¬∑x
(6.385)
of Dirac‚Äôs delta function (3.36). If we insert these Fourier transforms into the
equation (6.381) that deÔ¨Ånes the Green‚Äôs function G(x), then we Ô¨Ånd
‚àí‚ñ≥G(x) = ‚àí‚ñ≥

eik¬∑x g(k) d3k
=

eik¬∑x k2 g(k) d3k = Œ¥(3)(x) =

eik¬∑x d3k
(2œÄ)3 .
(6.386)
Thus the Green‚Äôs function G(x) is the Fourier transform
G(x) =
 eik¬∑x
k2
d3k
(2œÄ)3 ,
(6.387)
which we may integrate to
G(x) =
1
4œÄ|x| =
1
4œÄr
(6.388)
where r = |x| is the length of the vector x. This formula is generalized to n
dimensions in example 5.22.
Example 6.43 (Helmholtz‚Äôs Green‚Äôs functions)
The Green‚Äôs function for the
Helmholtz equation (‚àí‚ñ≥‚àím2)V(x) = œÅ(x) must satisfy
(‚àí‚ñ≥‚àím2) GH(x) = Œ¥(3)(x).
(6.389)
By using the Fourier-transform method of the previous example, one may show
that GH is
285

DIFFERENTIAL EQUATIONS
GH(x) = eimr
4œÄr,
(6.390)
in which r = |x| and m has units of inverse length.
Similarly, the Green‚Äôs function GmH for the modiÔ¨Åed Helmholtz equation
(‚àí‚ñ≥+ m2) GmH(x) = Œ¥(3)(x)
(6.391)
is (example 5.21)
GmH(x) = e‚àímr
4œÄr ,
(6.392)
which is a Yukawa potential.
Of these Green‚Äôs functions, probably the most important is G(x) = 1/4œÄr,
which has the expansion
G(x ‚àíx‚Ä≤) =
1
4œÄ|x ‚àíx‚Ä≤| =
‚àû

‚Ñì=0
‚Ñì

m=‚àí‚Ñì
1
2‚Ñì+ 1
r‚Ñì
<
r‚Ñì+1
>
Y‚Ñì,m(Œ∏, œÜ)Y‚àó
‚Ñì,m(Œ∏‚Ä≤, œÜ‚Ä≤) (6.393)
in terms of the spherical harmonics Y‚Ñì,m(Œ∏, œÜ). Here r, Œ∏, and œÜ are the spherical
coordinates of the point x, and r‚Ä≤, Œ∏‚Ä≤, and œÜ‚Ä≤ are those of the point x‚Ä≤; r> is
the larger of r and r‚Ä≤, and r< is the smaller of r and r‚Ä≤. If we substitute this
expansion (6.393) into the formula (6.382) for the potential œÜ, then we arrive at
the multipole expansion
œÜ(t, x) =

G(x ‚àíx‚Ä≤) œÅ(t, x‚Ä≤)
œµ0
d3x‚Ä≤
(6.394)
=
‚àû

‚Ñì=0
‚Ñì

m=‚àí‚Ñì
1
2‚Ñì+ 1

r‚Ñì
<
r‚Ñì+1
>
Y‚Ñì,m(Œ∏, œÜ)Y‚àó
‚Ñì,m(Œ∏‚Ä≤, œÜ‚Ä≤) œÅ(t, x‚Ä≤)
œµ0
d3x‚Ä≤.
Physicists often use this expansion to compute the potential at x due to a local-
ized, remote distribution of charge œÅ(t, x‚Ä≤). In this case, the integration is only
over the restricted region where œÅ(t, x‚Ä≤) Ã∏= 0, and so r< = r‚Ä≤ and r> = r, and the
multipole expansion is
œÜ(t, x) =
‚àû

‚Ñì=0
1
2‚Ñì+ 1
‚Ñì

m=‚àí‚Ñì
Y‚Ñì,m(Œ∏, œÜ)
r‚Ñì+1

r‚Ä≤‚ÑìY‚àó
‚Ñì,m(Œ∏‚Ä≤, œÜ‚Ä≤) œÅ(t, x‚Ä≤)
œµ0
d3x‚Ä≤.
(6.395)
In terms of the multipoles
Qm
‚Ñì=

r‚Ä≤‚ÑìY‚àó
‚Ñì,m(Œ∏‚Ä≤, œÜ‚Ä≤) œÅ(t, x‚Ä≤)
œµ0
d3x‚Ä≤
(6.396)
286

6.38 EIGENFUNCTIONS AND GREEN‚ÄôS FUNCTIONS
the potential is
œÜ(t, x) =
‚àû

‚Ñì=0
1
2‚Ñì+ 1
1
r‚Ñì+1
‚Ñì

m=‚àí‚Ñì
Qm
‚ÑìY‚Ñì,m(Œ∏, œÜ).
(6.397)
The spherical harmonics provide for the Legendre polynomial the expansion
P‚Ñì(ÀÜx ¬∑ ÀÜx‚Ä≤) =
4œÄ
2‚Ñì+ 1
‚Ñì

m=‚àí‚Ñì
Y‚Ñì,m(Œ∏, œÜ)Y‚àó
‚Ñì,m(Œ∏‚Ä≤, œÜ‚Ä≤),
(6.398)
which abbreviates the Green‚Äôs function formula (6.393) to
G(x ‚àíx‚Ä≤) =
1
4œÄ|x ‚àíx‚Ä≤| = 1
4œÄ
‚àû

‚Ñì=0
r‚Ñì
<
r‚Ñì+1
>
P‚Ñì(ÀÜx ¬∑ ÀÜx‚Ä≤).
(6.399)
Example 6.44 (Feynman‚Äôs propagator)
The Feynman propagator
F(x) =

d4q
(2œÄ)4
exp(iqx)
q2 + m2 ‚àíiœµ
(6.400)
is a Green‚Äôs function (5.230) for the operator L = m2 ‚àí2
(m2 ‚àí2)F(x) = Œ¥4(x).
(6.401)
By integrating over q0 while respecting the iœµ (example 5.36), one may write the
propagator in terms of the Lorentz-invariant function
+(x) =
1
(2œÄ)3
 d3q
2Eq
exp[i(q ¬∑ x ‚àíEqx0)]
(6.402)
as (5.241)
F(x) = iŒ∏(x0) +(x) + iŒ∏(‚àíx0) +(x, ‚àíx0),
(6.403)
which for space-like x, that is, for x2 = x2 ‚àí(x0)2 ‚â°r2 > 0, depends only upon
r = +
‚àö
x2 and has the value (Weinberg, 1995, p. 202)
+(x) =
m
4œÄ2r K1(mr),
(6.404)
in which K1 is the Hankel function (5.249).
6.38 Eigenfunctions and Green‚Äôs functions
The Green‚Äôs function (6.387)
G(x ‚àíy) =

dk
2œÄ
1
k2 + m2 eik (x‚àíy)
(6.405)
287

DIFFERENTIAL EQUATIONS
is based on the resolution (6.385) of the delta function
Œ¥(x ‚àíy) =

dk
2œÄ eik (x‚àíy)
(6.406)
in terms of the eigenfunctions exp(ik x) of the differential operator ‚àí‚àÇ2 + m2
with eigenvalues k2 + m2.
We may generalize this way of making Green‚Äôs functions to a regular Sturm‚Äì
Liouville system (section 6.33) with a differential operator L, eigenvalues Œªn
L un(x) = Œªn œÅ(x) un(x),
(6.407)
and eigenfunctions un(x) that are orthonormal with respect to a positive weight
function œÅ(x)
Œ¥n‚Ñì= (un, uk) =

œÅ(x) un(x)uk(x) dx
(6.408)
and that span in the mean the domain D of the system.
To make a Green‚Äôs function G(x ‚àíy) that satisÔ¨Åes
L G(x ‚àíy) = Œ¥(x ‚àíy)
(6.409)
we write G(x ‚àíy) in terms of the complete set of eigenfunctions uk as
G(x ‚àíy) =
‚àû

k=1
uk(x)uk(y)
Œªk
(6.410)
so that the action Luk = ŒªkœÅuk turns G into
L G(x ‚àíy) =
‚àû

k=1
L uk(x)uk(y)
Œªk
=
‚àû

k=1
œÅ(x) uk(x) uk(y) = Œ¥(x ‚àíy),
(6.411)
our Œ± = 1 series expansion (6.374) of the delta function.
6.39 Green‚Äôs functions in one dimension
In one dimension, we can explicitly solve the inhomogeneous ordinary differ-
ential equation L f (x) = g(x) in which
L = ‚àíd
dx

p(x) d
dx

+ q(x)
(6.412)
is formally self adjoint. We‚Äôll build a Green‚Äôs function from two solutions u and
v of the homogeneous equation L u(x) = L v(x) = 0 as
G(x, y) = ‚àí1
A [Œ∏(x ‚àíy)u(y)v(x) + Œ∏(y ‚àíx)u(x)v(y)] ,
(6.413)
288

6.40 NONLINEAR DIFFERENTIAL EQUATIONS
in which Œ∏(x) = (x + |x|)/(2|x|) is the Heaviside step function (Oliver Heaviside,
1850‚Äì1925), and A is a constant which we‚Äôll presently identify. We‚Äôll show that
the expression
f (x) =
 b
a
G(x, y) g(y) dy = ‚àív(x)
A
 x
a
u(y) g(y) dy ‚àíu(x)
A
 b
x
v(y) g(y) dy
solves
our
inhomogeneous
equation.
Differentiating,
we
Ô¨Ånd
after
a
cancellation
f ‚Ä≤(x) = ‚àív‚Ä≤(x)
A
 x
a
u(y) g(y) dy ‚àíu‚Ä≤(x)
A
 b
x
v(y) g(y) dy.
(6.414)
Differentiating again, we have
f ‚Ä≤‚Ä≤(x) = ‚àív‚Ä≤‚Ä≤(x)
A
 x
a
u(y) g(y) dy ‚àíu‚Ä≤‚Ä≤(x)
A
 b
x
v(y) g(y) dy
‚àív‚Ä≤(x)u(x)g(x)
A
+ u‚Ä≤(x)v(x)g(x)
A
= ‚àív‚Ä≤‚Ä≤(x)
A
 x
a
u(y) g(y) dy ‚àíu‚Ä≤‚Ä≤(x)
A
 b
x
v(y) g(y) dy
‚àíW(x)
A
g(x),
(6.415)
in which W(x) is the wronskian W(x) = u(x)v‚Ä≤(x) ‚àíu‚Ä≤(x)v(x). By applying
the result (6.266) for the wronskian of two linearly independent solutions of
a self-adjoint homogeneous ODE, we Ô¨Ånd W(x) = W(x0) p(x0)/p(x). We set
the constant A = W(x0)p(x0) so that the last term in (6.415) is ‚àíg(x)/p(x). It
follows that
Lf (x) = ‚àí[Lv(x)]
A
 x
a
u(y) g(y) dy ‚àí[Lu(x)]
A
 b
x
v(y) g(y) dy + g(x) = g(x).
(6.416)
But Lu(x) = Lv(x) = 0, so we see that f satisÔ¨Åes our inhomogeneous equation
Lf (x) = g(x).
6.40 Nonlinear differential equations
The Ô¨Åeld of nonlinear differential equations is too vast to cover here, but we
may hint at some of its features by considering some examples from cosmology
and particle physics.
The Friedmann equations of general relativity (11.410 & 11.412) for the scale
factor a(t) of a homogeneous, isotropic universe are
289

DIFFERENTIAL EQUATIONS
¬®a
a = ‚àí4œÄG
3
(œÅ + 3p)
and
 Àôa
a
2
= 8œÄG
3
œÅ ‚àík
a2 ,
(6.417)
in which k respectively is 1, 0, and ‚àí1 for closed, Ô¨Çat, and open geometries.
(The scale factor a(t) tells how much space has expanded or contracted by the
time t.) These equations become more tractable when the energy density œÅ is
due to a single constituent whose pressure p is related to it by an equation of
state p = wœÅ. Conservation of energy ÀôœÅ = ‚àí3(œÅ + p)/a (11.426‚Äì11.431) then
ensures (exercise 6.30) that the product œÅ a3(1+w) is independent of time. The
constant w respectively is 1/3, 0, and ‚àí1 for radiation, matter, and the vacuum.
The Friedmann equations then are
a2+3w ¬®a = ‚àí4œÄG
3
(1 + 3w) œÅ a3(1+w) ‚â°‚àíf ,
(6.418)
where f is a constant that is positive when w > ‚àí1/3, and
a1+3w 
Àôa2 + k

=
2f
1 + 3w.
(6.419)
Example 6.45 (An open universe of radiation)
Here k = ‚àí1 and the parameter
w = 1/3, so the Ô¨Årst-order Friedmann equation (6.419) becomes
Àôa2 = f
a2 + 1.
(6.420)
The universe is expanding, so we take the positive square-root and get
dt =
a da

a2 + f
,
(6.421)
which leads to the general integral t =

a2 + f + C. If we choose the constant
of integration C = ‚àí

f , then we Ô¨Ånd
a(t) =
&
t +

f
2
‚àíf ,
(6.422)
a scale factor that vanishes at time zero and approaches t as t ‚Üí‚àû.
Example 6.46 (A closed universe of matter)
Here w = 0 and k = 1, and so the
Ô¨Årst-order Friedmann equation (6.419) is
Àôa2 = 2f
a ‚àí1.
(6.423)
290

6.40 NONLINEAR DIFFERENTIAL EQUATIONS
Since the universe is expanding, we take the positive square-root
Àôa =
&
2f
a ‚àí1,
(6.424)
which leads to the general integral
t =

‚àöa da

2f ‚àía
= ‚àí

a(2f ‚àía) ‚àíf arcsin (1 ‚àía/f ) + C,
(6.425)
in which C is a constant of integration.
Example 6.47 (An open universe of matter)
Here w = 0 and k = ‚àí1, and so
the Ô¨Årst-order Friedmann equation (6.419) is Àôa2 = 2f /a + 1, which leads to the
general integral
t =

‚àöa da

2f + a
=

a(2f + a) ‚àíf ln

a(2f + a) + a + f

+ C,
(6.426)
in which C is a constant of integration.
The equations of particle physics are nonlinear. Physicists usually use pertur-
bation theory to cope with the nonlinearities. But occasionally they focus on the
nonlinearities and treat the quantum aspects classically or semi-classically. To
keep things relatively simple, we‚Äôll work in a space-time of only two dimensions
and consider a model Ô¨Åeld theory described by the action density
L = 1
2

ÀôœÜ2 ‚àíœÜ‚Ä≤2
‚àíV(œÜ),
(6.427)
in which V is a simple function of the Ô¨Åeld œÜ. Lagrange‚Äôs equation for this
theory is
¬®œÜ ‚àíœÜ‚Ä≤‚Ä≤ = dV
dœÜ .
(6.428)
We can convert this partial differential equation to an ordinary one by making
the Ô¨Åeld œÜ depend only upon the combination u = x ‚àívt rather than upon
both x and t. We then have ÀôœÜ = ‚àívœÜ‚Ä≤. With this restriction to traveling-wave
solutions, Lagrange‚Äôs equation reduces to
(v2 ‚àí1)œÜ‚Ä≤‚Ä≤ = dV
dœÜ .
(6.429)
We multiply both sides of this equation by œÜ‚Ä≤
(v2 ‚àí1) œÜ‚Ä≤ œÜ‚Ä≤‚Ä≤ = dV
dœÜ œÜ‚Ä≤.
(6.430)
291

DIFFERENTIAL EQUATIONS
We now integrate both sides to get (v2 ‚àí1) 1
2 œÜ‚Ä≤2 = V ‚àíE, in which E is a
constant of integration and a kind of energy
E = 1
2(1 ‚àív2)œÜ‚Ä≤2 + V(œÜ).
(6.431)
We can convert (exercise 6.37) this equation into a problem of integration
u ‚àíu0 =


1 ‚àív2

2(E ‚àíV(œÜ))
dœÜ.
(6.432)
By inverting the resulting equation relating u to œÜ, we may Ô¨Ånd the soliton
solution œÜ(u ‚àíu0), which is a lump of energy traveling with speed v.
Example 6.48 (Soliton of the œÜ4 theory)
To simplify the integration (6.432), we
take as the action density
L = 1
2

ÀôœÜ2 ‚àíœÜ‚Ä≤2
‚àí

E ‚àíŒª2
2

œÜ2 ‚àíœÜ2
0
2

.
(6.433)
‚àí10
‚àí8
‚àí6
‚àí4
‚àí2
0
2
4
6
8
10
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
Soliton
x
œÜ (x) = tanh x
Figure 6.4
The Ô¨Åeld œÜ(x) of the soliton (6.435) at rest (v = 0) at position x0 = 0
for Œª = 1 = œÜ0. The energy density of the Ô¨Åeld vanishes when œÜ = ¬±œÜ0 = ¬±1.
The energy of this soliton is concentrated at x0 = 0.
292

EXERCISES
Our formal solution (6.432) gives
u ‚àíu0 = ¬±


1 ‚àív2
Œª

œÜ2 ‚àíœÜ2
0
 dœÜ = ‚àì

1 ‚àív2
ŒªœÜ0
tanh‚àí1(œÜ/œÜ0)
(6.434)
or
œÜ(x ‚àívt) = ‚àìœÜ0 tanh

ŒªœÜ0
x ‚àíx0 ‚àív(t ‚àít0)

1 ‚àív2

,
(6.435)
which is a soliton (or an antisoliton) at x0 + v(t ‚àít0). A unit soliton at rest
is plotted in Fig. 6.4. Its energy is concentrated at x = 0 where |œÜ2 ‚àíœÜ2
0| is
maximal.
Exercises
6.1
In rectangular coordinates, the curl of a curl is by deÔ¨Ånition (6.40)
(‚àá√ó (‚àá√ó E))i =
3

j,k=1
œµijk‚àÇj(‚àá√ó E)k =
3

j,k,‚Ñì,m=1
œµijk‚àÇjœµk‚Ñìm‚àÇ‚ÑìEm.
(6.436)
Use Levi-Civita‚Äôs identity (1.449) to show that
‚àá√ó (‚àá√ó E) = ‚àá(‚àá¬∑ E) ‚àí‚ñ≥E.
(6.437)
This formula deÔ¨Ånes ‚ñ≥E in any system of orthogonal coordinates.
6.2
Show that since the Bessel function Jn(x) satisÔ¨Åes Bessel‚Äôs equation (6.48), the
function Pn(œÅ) = Jn(kœÅ) satisÔ¨Åes (6.47).
6.3
Show that (6.58) implies that Rk,‚Ñì(r) = j‚Ñì(kr) satisÔ¨Åes (6.57).
6.4
Use (6.56, 6.57), and ‚Ä≤‚Ä≤
m = ‚àím2m to show in detail that the product
f (r, Œ∏, œÜ) = Rk,‚Ñì(r) ‚Ñì,m(Œ∏) m(œÜ) satisÔ¨Åes ‚àí‚ñ≥f = k2f .
6.5
Replacing Helmholtz‚Äôs k2 by E ‚àíV(r), we get Schr√∂dinger‚Äôs equation
‚àí(¬Øh2/2m)‚ñ≥œà(r, Œ∏, œÜ) + V(r)œà(r, Œ∏, œÜ) = Eœà(r, Œ∏, œÜ).
(6.438)
Let œà(r, Œ∏, œÜ) = Rn,‚Ñì(r)‚Ñì,m(Œ∏)eimœÜ in which ‚Ñì,m satisÔ¨Åes (6.56) and show
that the radial function Rn,‚Ñìmust obey
‚àí

r2R‚Ä≤
n,‚Ñì
‚Ä≤
/r2 +

‚Ñì(‚Ñì+ 1)/r2 + 2mV/¬Øh2
Rn,‚Ñì= 2mEn,‚Ñì/¬Øh2.
(6.439)
6.6
Use
the
empty-space
Maxwell‚Äôs
equations
‚àá¬∑ B = 0,
‚àá√ó E + ÀôB = 0,
‚àá¬∑ E = 0, and ‚àá√ó B ‚àíÀôE/c2 = 0 and the formula (6.437) to show that in
vacuum ‚ñ≥E = ¬®E/c2 and ‚ñ≥B = ¬®B/c2.
6.7
Argue from symmetry and antisymmetry that [Œ≥ a, Œ≥ a‚Ä≤]‚àÇa‚àÇa‚Ä≤ = 0, in which the
sums over a and b run from 0 to 3.
6.8
Suppose a voltage V(t) = V sin(œât) is applied to a resistor of R () in series
with a capacitor of capacitance C (F). If the current through the circuit at
time t = 0 is zero, what is the current at time t?
293

DIFFERENTIAL EQUATIONS
6.9
(a) Is the ODE
(1 + y2)y dx + (1 + x2)x dy

1 + x2 + y23/2
= 0
exact? (b) Find its general integral and solution y(x). Use section 6.11.
6.10 (a) Separate the variables of the ODE (1+y2)y dx+(1+x2)x dy = 0. (b) Find
its general integral and solution y(x).
6.11 Find the general solution to the differential equation y‚Ä≤ + y/x = c/x.
6.12 Find the general solution to the differential equation y‚Ä≤ + xy = ce‚àíx2/2.
6.13 James Bernoulli studied ODEs of the form y‚Ä≤ + p y = q yn, in which p and
q are functions of x. Division by yn and the substitution v = y1‚àín gives
v‚Ä≤ + (1 ‚àín)p = (1 ‚àín) q, which is soluble as shown in section 6.16. Use
this method to solve the ODE y‚Ä≤ ‚àíy/2x = 5x2y5.
6.14 Integrate the ODE (xy + 1) dx + 2x2(2xy ‚àí1) dy = 0. Hint: use the variable
v(x) = xy(x) instead of y(x).
6.15 Show that the points x = ¬±1 and ‚àûare regular singular points of Legendre‚Äôs
equation (6.181).
6.16 Use the vanishing of the coefÔ¨Åcient of every power of x in (6.185) and the
notation (6.187) to derive the recurrence relation (6.188).
6.17 In example 6.29, derive the recursion relation for k = 1 and discuss the
resulting eigenvalue equation.
6.18 In example 6.29, show that the solutions associated with the roots k = 0 and
k = 1 are the same.
6.19 For a hydrogen atom, we set V(r) = ‚àíe2/4œÄœµ0r ‚â°‚àíq2/r in (6.439) and get
(r2R‚Ä≤
n,‚Ñì)‚Ä≤ +

(2m/¬Øh2)

En,‚Ñì+ Zq2/r

r2 ‚àí‚Ñì(‚Ñì+ 1)

Rn,‚Ñì= 0. So at big r, R‚Ä≤‚Ä≤
n,‚Ñì‚âà
‚àí2mEn,‚ÑìRn,‚Ñì/¬Øh2 and Rn,‚Ñì‚àºexp(‚àí

‚àí2mEn,‚Ñìr/¬Øh). At tiny r, (r2R‚Ä≤
n,‚Ñì)‚Ä≤ ‚âà
‚Ñì(‚Ñì+ 1)Rn,‚Ñìand Rn,‚Ñì(r) ‚àºr‚Ñì. Set Rn,‚Ñì(r) = r‚Ñìexp(‚àí

‚àí2mEn,‚Ñìr/¬Øh)Pn,‚Ñì(r)
and apply the method of Frobenius to Ô¨Ånd the values of En,‚Ñìfor which Rn,‚Ñìis
suitably normalizable.
6.20 Show that as long as the matrix Ykj = y
(‚Ñìj)
k (xj) is nonsingular, the n boundary
conditions
bj = y(‚Ñìj)(xj) =
n

k=1
ck y
(‚Ñìj)
k (xj)
(6.440)
determine the n coefÔ¨Åcients ck of the expansion (6.222) to be
CT = B Y‚àí1
or
Ck =
n

j=1
bjY‚àí1
jk .
(6.441)
6.21 Show that if the real and imaginary parts u1, u2, v1, and v2 of œà and œá satisfy
boundary conditions at x = a and x = b that make the boundary term (6.235)
vanish, then its complex analog (6.242) also vanishes.
294

EXERCISES
6.22 Show that if the real and imaginary parts u1, u2, v1, and v2 of œà and œá satisfy
boundary conditions at x = a and x = b that make the boundary term (6.235)
vanish, and if the differential operator L is real and self adjoint, then (6.238)
implies (6.243).
6.23 Show that if D is the set of all twice-differentiable functions u(x) on [a, b]
that satisfy Dirichlet‚Äôs boundary conditions (6.245) and if the function p(x)
is continuous and positive on [a, b], then the adjoint set D‚àódeÔ¨Åned as the set
of all twice-differentiable functions v(x) that make the boundary term (6.247)
vanish for all functions u ‚ààD is D itself.
6.24 Same as exercise (6.23) but for Neumann boundary conditions (6.246).
6.25 Use Bessel‚Äôs equation (6.307) and the boundary conditions u(0) = 0 for n > 0
and u(1) = 0 to show that the eigenvalues Œª are all positive.
6.26 Show that after the change of variables u(x) = Jn(kx) = Jn(œÅ) the self-adjoint
differential equation (6.307) becomes Bessel‚Äôs equation (6.308).
6.27 Derive Bessel‚Äôs inequality (6.378) from the inequality (6.377).
6.28 Repeat example 6.41 using J1s instead of J0s. Hint: the Mathematica com-
mand Do[Print[N[BesselJZero[1, k], 10]], {k, 1, 100, 1}] gives the Ô¨Årst 100
zeros z1,k of the Bessel function J1(x) to ten signiÔ¨Åcant Ô¨Ågures.
6.29 Derive the Yukawa potential (6.392) as the Green‚Äôs function for the modiÔ¨Åed
Helmholtz equation (6.391).
6.30 Derive the relation œÅ = œÅ(a/a)3(1+w) between the energy density œÅ and
the Robertson‚ÄìWalker scale factor a(t) from the conservation law dœÅ/da =
‚àí3(œÅ + p)/a and the equation of state p = wœÅ.
6.31 For a closed universe (k = 1) of radiation (w = 1/3), use Friedmann‚Äôs equa-
tions (6.418 & 6.419) to derive the solution (11.448) subject to the boundary
condition a(0) = 0. When does the universe collapse in a big crunch?
6.32 For a Ô¨Çat universe (k = 0) of matter (w = 0), use Friedmann‚Äôs equa-
tions (6.418 & 6.419) to derive the solution (11.454) subject to the boundary
condition a(0) = 0.
6.33 Derive the time evolution of a(t) for a Ô¨Çat (k = 0) universe dominated by
radiation (w = 1/3) subject to the boundary condition a(0) = 0. Use (6.419).
6.34 Derive the time evolution of a(t) for an open (k = ‚àí1) universe with only dark
energy (w = ‚àí1) subject to the boundary condition a(0) = 0. Use (6.419).
6.35 Use Friedmann‚Äôs equations (6.418 & 6.419) to derive the evolution of the scale
factor for a closed universe dominated by dark energy subject to the boundary
condition a(0) = ‚àö3/8œÄGœÅ in which œÅ is a constant density of dark energy.
6.36 Use Friedmann‚Äôs equations (6.418 & 6.419) to derive the evolution of a(t) for
a Ô¨Çat (k = 0) expanding universe dominated by dark energy (w = ‚àí1) subject
to the boundary condition a(0) = Œ± in which œÅ is a constant density of dark
energy.
6.37 Derive the soliton solution (6.432) from the energy equation (6.431).
295

7
Integral equations
Differential equations when integrated become integral equations with built-in
boundary conditions. Thus if we integrate the Ô¨Årst-order ODE
du(x)
dx
‚â°ux(x) = p(x) u(x) + q(x)
(7.1)
then we get the integral equation
u(x) =
 x
a
p(y) u(y) dy +
 x
a
q(y) dy + u0
(7.2)
and the boundary condition u(a) = f (a) = u0.
With a little more effort, we may integrate the second-order ODE
u‚Ä≤‚Ä≤ = pu‚Ä≤ + qu + r
(7.3)
(exercises 7.1 & 7.2) to
u(x) = f (x) +
 x
a
k(x, y) u(y) dy
(7.4)
with
k(x, y) = p(y) + (x ‚àíy)

q(y) ‚àíp‚Ä≤(y)

(7.5)
and
f (x) = u(a) + (x ‚àía)

u‚Ä≤(a) ‚àíp(a) u(a)

+
 x
a
(x ‚àíy)r(y) dy.
(7.6)
In some physical problems, integral equations arise independently of dif-
ferential equations. Whatever their origin, integral equations tend to have
properties more suitable to mathematical analysis because derivatives are
unbounded operators.
296

7.2 VOLTERRA INTEGRAL EQUATIONS
7.1 Fredholm integral equations
An equation of the form
 b
a
k(x, y) u(y) dy = Œª u(x) + f (x)
(7.7)
for a ‚â§x ‚â§b with a given kernel k(x, y) and a speciÔ¨Åed function f (x) is an
inhomogeneous Fredholm equation of the second kind for the function u(x) and
the parameter Œª (Erik Ivar Fredholm, 1866‚Äì1927).
If f (x) = 0, then it is a homogeneous Fredholm equation of the second kind
 b
a
k(x, y) u(y) dy = Œª u(x),
a ‚â§x ‚â§b.
(7.8)
Such an equation typically has nontrivial solutions only for certain eigenvalues
Œª. Each solution u(x) is an eigenfunction.
If Œª = 0 but f (x) Ã∏= 0, then equation (7.7) is an inhomogeneous Fredholm
equation of the Ô¨Årst kind
 b
a
k(x, y) u(y) dy = f (x),
a ‚â§x ‚â§b.
(7.9)
Finally, if both Œª = 0 and f (x) = 0, then (7.7) is a homogeneous Fredholm
equation of the Ô¨Årst kind
 b
a
k(x, y) u(y) dy = 0,
a ‚â§x ‚â§b.
(7.10)
These Fredholm equations are linear because they involve only the Ô¨Årst (and
zeroth) power of the unknown function u(x).
7.2 Volterra integral equations
If the kernel k(x, y) in the equations (7.7‚Äì7.10) that deÔ¨Åne the Fredholm integral
equations is causal, that is, if
k(x, y) = k(x, y) Œ∏(x ‚àíy),
(7.11)
in which Œ∏(x) = (x+|x|)/2|x| is the Heaviside function, then the corresponding
equations bear the name Volterra (Vito Volterra, 1860‚Äì1941). Thus, an equation
of the form
 x
a
k(x, y) u(y) dy = Œª u(x) + f (x),
(7.12)
in which the kernel k(x, y) and the function f (x) are given, is an inhomogeneous
Volterra equation of the second kind for the function u(x) and the parameter Œª.
297

INTEGRAL EQUATIONS
If f (x) = 0, then it is a homogeneous Volterra equation of the second kind
 x
a
k(x, y) u(y) dy = Œª u(x).
(7.13)
Such an equation typically has nontrivial solutions only for certain eigenvalues
Œª. The solutions u(x) are the eigenfunctions.
If Œª = 0 but f (x) Ã∏= 0, then equation (7.12) is an inhomogeneous Volterra
equation of the Ô¨Årst kind
 x
a
k(x, y) u(y) dy = f (x).
(7.14)
Finally, if both Œª = 0 and f (x) = 0, then it is a homogeneous Volterra equation
of the Ô¨Årst kind
 x
a
k(x, y) u(y) dy = 0.
(7.15)
These Volterra equations are linear because they involve only the Ô¨Årst (and
zeroth) power of the unknown function u(x).
In what follows, we‚Äôll mainly discuss Fredholm integral equations, since those
of the Volterra type are a special case of the Fredholm type.
7.3 Implications of linearity
Because the Fredholm and Volterra integral equations are linear, one may add
solutions of the homogeneous equations (7.8, 7.10, 7.13, & 7.15) and get new
solutions. Thus if u1, u2, . . . are eigenfunctions
 b
a
k(x, y) uj(y) dy = Œª uj(x),
a ‚â§x ‚â§b
(7.16)
with the same eigenvalue Œª, then the sum 
j ajuj(x) also is an eigenfunction
with the same eigenvalue
 b
a
k(x, y)
‚éõ
‚éù
j
ajuj(y)
‚éû
‚é†dy =

j
aj
 b
a
k(x, y) uj(y) dy
=

j
ajŒª uj(x) = Œª
‚éõ
‚éù
j
ajuj(x)
‚éû
‚é†.
(7.17)
It also is true that the difference between any two solutions ui
1(x) and ui
2(x)
of one of the inhomogeneous Fredholm (7.7, 7.9) or Volterra (7.12, 7.14) equa-
tions is a solution of the associated homogeneous equation (7.8, 7.10, 7.13, or
298

7.4 NUMERICAL SOLUTIONS
7.15). Thus if ui
1(x) and ui
2(x) satisfy the inhomogeneous Fredholm equation of
the second kind
 b
a
k(x, y) ui
j(y) dy = Œª ui
j(x) + f (x),
j = 1, 2
(7.18)
then their difference ui
1(x)‚àíui
2(x) satisÔ¨Åes the homogeneous Fredholm equation
of the second kind
 b
a
k(x, y)

ui
1(y) ‚àíui
2(y)

dy = Œª

ui
1(x) ‚àíui
2(x)

.
(7.19)
Thus, the most general solution ui(x) of the inhomogeneous Fredholm equation
of the second kind (7.18) is a particular solution ui
p(x) of that equation plus the
general solution of the homogeneous Fredholm equation of the second kind
(7.16)
ui(x) = ui
p(x) +

j
ajuj(x).
(7.20)
Linear integral equations are much easier to solve than nonlinear ones.
7.4 Numerical solutions
Let us break the real interval [a, b] into N segments [yk, yk+1] of equal length
y = (b ‚àía)/N with y0 = a, yk = a + k y, and yN = b. We‚Äôll also set xk = yk
and deÔ¨Åne U as the vector with entries Uk = u(yk) and K as the (N+1)√ó(N+1)
square matrix with elements Kk‚Ñì= k(xk, y‚Ñì) y. Then we may approximate the
homogeneous Fredholm equation of the second kind (7.8)
 b
a
k(x, y) u(y) dy = Œª u(x),
a ‚â§x ‚â§b
(7.21)
as the algebraic equation
N

‚Ñì=0
Kk,‚ÑìU‚Ñì= Œª Uk
(7.22)
or, in matrix notation,
K U = Œª U.
(7.23)
We saw in section 1.25 that every such equation has N + 1 eigenvectors
U(Œ±) and eigenvalues Œª(Œ±), and that the eigenvalues Œª(Œ±) are the solutions of
the characteristic equation (1.244)
det(K ‚àíŒª(Œ±)I) =
K ‚àíŒª(Œ±)I
 = 0.
(7.24)
299

INTEGRAL EQUATIONS
In general, as N ‚Üí‚àûand y ‚Üí0, the number N + 1 of eigenvalues Œª(Œ±) and
eigenvectors U(Œ±) becomes inÔ¨Ånite.
We may apply the same technique to the inhomogeneous Fredholm equation
of the Ô¨Årst kind
 b
a
k(x, y) u(y) dy = f (x)
for
a ‚â§x ‚â§b.
(7.25)
The resulting matrix equation is
K U = F,
(7.26)
in which the kth entry in the vector F is Fk = f (xk). This equation has the
solution
U = K‚àí1 F
(7.27)
as long as the matrix K is nonsingular, that is, as long as
det K Ã∏= 0.
(7.28)
This technique applied to the inhomogeneous Fredholm equation of the
second kind
 b
a
k(x, y) u(y) dy = Œª u(x) + f (x)
(7.29)
leads to the matrix equation
K U = Œª U + F.
(7.30)
The associated homogeneous matrix equation
K U = Œª U
(7.31)
has N + 1 eigenvalues Œª(Œ±) and eigenvectors U(Œ±) ‚â°|Œ±‚ü©. For any value of Œª that
is not one of the eigenvalues Œª(Œ±), the matrix K ‚àíŒªI has a nonzero determinant
and hence an inverse, and so the vector
Ui = (K ‚àíŒª I)‚àí1 F
(7.32)
is a solution of the inhomogeneous matrix equation (7.30).
If Œª = Œª(Œ≤) is one of the eigenvalues Œª(Œ±) of the homogeneous matrix equation
(7.31), then the matrix K ‚àíŒª(Œ≤)I will not have an inverse, but it will have a
pseudoinverse (section 1.32). If its singular-value decomposition (1.362) is
K ‚àíŒª(Œ≤)I =
N+1

n=1
|mn‚ü©Sn‚ü®n|
(7.33)
300

7.5 INTEGRAL TRANSFORMATIONS
then its pseudoinverse (1.392) is

K ‚àíŒª(Œ≤)I
+
=
N+1

n=1
SnÃ∏=0
|n‚ü©S‚àí1
n ‚ü®mn|,
(7.34)
in which the sum is over the positive singular values. So if the vector F is a linear
combination of the left singular vectors |mn‚ü©whose singular values are positive
F =
N+1

n=1
SnÃ∏=0
fn|mn‚ü©
(7.35)
then the vector
Ui =

K ‚àíŒª(Œ≤)I
+
F
(7.36)
will be a solution of the inhomogeneous matrix Fredholm equation (7.30). For
in this case

K ‚àíŒª(Œ≤)I

Ui =

K ‚àíŒª(Œ≤)I
 
K ‚àíŒª(Œ≤)I
+
F
=
N+1

n‚Ä≤‚Ä≤=1
|mn‚Ä≤‚Ä≤‚ü©Sn‚Ä≤‚Ä≤‚ü®n‚Ä≤‚Ä≤|
N+1

n‚Ä≤=1
Sn‚Ä≤Ã∏=0
|n‚Ä≤‚ü©S‚àí1
n‚Ä≤ ‚ü®mn‚Ä≤|
N+1

n=1
SnÃ∏=0
fn|mn‚ü©
=
N+1

n=1
SnÃ∏=0
fn|mn‚ü©= F.
(7.37)
The most general solution will be the sum of this particular solution of the inho-
mogeneous equation (7.30) and the most general solution of the homogeneous
equation (7.31)
U = Ui +

k
fŒ≤,k U(Œ≤,k) =

K ‚àíŒª(Œ≤)I
+
F +

k
fŒ≤,k U(Œ≤,k).
(7.38)
Open-source programs are available in C++ (math.nist.gov/tnt/) and in FOR-
TRAN (www.netlib.org/lapack/) that can solve such equations for the N + 1
eigenvalues Œª(Œ±) and eigenvectors U(Œ±) and for the inverse K‚àí1 for N = 100,
1000, 10,000, and so forth in milliseconds on a PC.
7.5 Integral transformations
Integral transformations (Courant and Hilbert, 1955, chap. VII) help us solve
linear homogeneous differential equations like
L u + c u = 0,
(7.39)
301

INTEGRAL EQUATIONS
in which L is a linear operator involving derivatives of u(z) with respect to its
complex argument z = x + iy and c is a constant. We choose a kernel K(z, w)
analytic in both variables and write u(z) as an integral along a contour in the
complex w-plane weighted by an unknown function v(w)
u(z) =

C
K(z, w) v(w) dw.
(7.40)
If the differential operator L commutes with the contour integration as it
usually would, then our differential equation (7.39) is

C
[L K(z, w) + c K(z, w)] v(w) dw = 0.
(7.41)
The next step is to Ô¨Ånd a linear operator M that acting on K(z, w) with w-
derivatives (but no z-derivatives) gives L acting on K(z, w)
M K(z, w) = L K(z, w).
(7.42)
We then get an integral equation

C
[M K(z, w) + c K(z, w)] v(w) dw = 0
(7.43)
involving w-derivatives that we can integrate by parts. We choose the contour C
so that the resulting boundary terms vanish. By using our freedom to pick the
kernel and the contour, we often can make the resulting differential equation
for v simpler than the one (7.39) we started with.
Example 7.1 (Fourier, Laplace, and Euler kernels)
We already are familiar with
the most important integral transforms. In chapter 3, we learned that the kernel
K(z, w) = exp(izw) leads to the Fourier transform
u(z) =
 ‚àû
‚àí‚àû
eizw v(w) dw
(7.44)
and the kernel K(z, w) = exp(‚àízw) to the Laplace transform
u(z) =
 ‚àû
0
e‚àízw v(w) dw
(7.45)
of section 3.9. Euler‚Äôs kernel K(z, w) = (z ‚àíw)a occurs in many applications of
Cauchy‚Äôs integral theorem (5.21) and integral formula (5.36). These kernels help
us solve differential equations.
Example 7.2 (Bessel functions)
The differential operator L for Bessel‚Äôs equation
(6.308)
z2 u‚Ä≤‚Ä≤ + z u‚Ä≤ + z2 u ‚àíŒª2 u = 0
(7.46)
302

7.5 INTEGRAL TRANSFORMATIONS
is
L = z2 d2
dz2 + z d
dz + z2
(7.47)
and the constant c is ‚àíŒª2. If we choose M = ‚àíd2/dw2, then the kernel should
satisfy (7.42)
L K ‚àíM K = z2 Kzz + z Kz + z2 K + Kww = 0,
(7.48)
in which subscripts indicate differentiation as in (6.20). The kernel
K(z, w) = e¬±iz sin w
(7.49)
is a solution of (7.48) that is entire in both variables (exercise 7.3). In terms of it,
our integral equation (7.43) is

C

Kww(z, w) + Œª2 K(z, w)

v(w) dw = 0.
(7.50)
We now integrate by parts once

C
#
‚àíKw v‚Ä≤ + Œª2 K v + dKw v
dw
$
dw
(7.51)
and then again

C
#
K

v‚Ä≤‚Ä≤ + Œª2 v

+ d(Kw v ‚àíKv‚Ä≤)
dw
$
dw.
(7.52)
If we choose the contour so that Kw v ‚àíKv‚Ä≤ = 0 at its ends, then the unknown
function v must satisfy the differential equation
v‚Ä≤‚Ä≤ + Œª2 v = 0,
(7.53)
which is vastly simpler than Bessel‚Äôs; the solution v(w) = exp(iŒªw) is an entire
function of w for every complex Œª. Our solution u(z) then is
u(z) =

C
K(z, w) v(w) dw =

C
e¬±iz sin w eiŒªw dw.
(7.54)
For Re(z) > 0 and any complex Œª, the contour C1 that runs from ‚àíi‚àûto the
origin w = 0, then to w = ‚àíœÄ, and Ô¨Ånally up to ‚àíœÄ + i‚àûhas Kw v ‚àíKv‚Ä≤ = 0
at its ends (exercise 7.4) provided we use the minus sign in the exponential. The
function deÔ¨Åned by this choice
H(1)
Œª (z) = ‚àí1
œÄ

C1
e‚àíiz sin w+iŒªw dw
(7.55)
is the Ô¨Årst Hankel function (Hermann Hankel, 1839‚Äì1873). The second Hankel
function is deÔ¨Åned for Re(z) > 0 and any complex Œª by a contour C2 that runs
from œÄ + i‚àûto w = œÄ, then to w = 0, and lastly to ‚àíi‚àû
H(2)
Œª (z) = ‚àí1
œÄ

C2
e‚àíiz sin w+iŒªw dw.
(7.56)
303

INTEGRAL EQUATIONS
Because the integrand exp(‚àíiz sin w + iŒªw) is an entire function of z and w,
one may deform the contours C1 and C2 and analytically continue the Hankel
functions beyond the right half-plane (Courant and Hilbert, 1955, chap. VII).
One may verify (exercise 7.5) that the Hankel functions are related by complex
conjugation
H(1)
Œª (z) = H(2)‚àó
Œª
(z)
(7.57)
when both z > 0 and Œª are real.
Exercises
7.1
Show that
 x
a
dz
 z
a
dy f (y) =
 x
a
(x ‚àíy) f (y) dy.
(7.58)
Hint: differentiate both sides with respect to x.
7.2
Use this identity (7.58) to integrate (7.3) and derive equations (7.4, 7.5, & 7.6).
7.3
Show that the kernel K(z, w)
=
exp(¬±iz sin w) satisÔ¨Åes the differential
equation (7.48).
7.4
Show that for Re z > 0 and arbitrary complex Œª, the boundary terms in the
integral (7.52) vanish for the two contours C1 and C2 that deÔ¨Åne the two
Hankel functions.
7.5
Show that the Hankel functions are related by complex conjugation (7.57)
when both z > 0 and Œª are real.
304

8
Legendre functions
8.1 The Legendre polynomials
The monomials xn span the space of functions f (x) that have power-series
expansions on an interval about the origin
f (x) =
‚àû

n=0
cn xn =
‚àû

n=0
f (n)(0)
n!
xn.
(8.1)
They are complete but not orthogonal or normalized. We can make them into
real, orthogonal polynomials Pn(x) of degree n on the interval [‚àí1, 1]
(Pn, Pm) =
 1
‚àí1
Pn(x) Pm(x) dx = 0,
n Ã∏= m
(8.2)
by requiring that each Pn(x) be orthogonal to all monomials xm for m < n
 1
‚àí1
Pn(x) xm dx = 0,
m < n.
(8.3)
If we impose the normalization condition
Pn(1) = 1
(8.4)
then they are unique and are the Legendre polynomials as in Fig. 8.1.
The coefÔ¨Åcients ak of the nth Legendre polynomial
Pn(x) = a0 + a1x + ¬∑ ¬∑ ¬∑ + anxn
(8.5)
must satisfy (exercise 8.3) the n conditions (8.3) of orthogonality
 1
‚àí1
Pn(x) xm dx =
n

k=0
1 ‚àí(‚àí1)m+k+1
m + k + 1
ak = 0
for
0 ‚â§m < n
(8.6)
and the normalization condition (8.4)
Pn(1) = a0 + a1 + ¬∑ ¬∑ ¬∑ + an = 1.
(8.7)
305

LEGENDRE FUNCTIONS
‚àí1
‚àí0.5
0
0.5
1
‚àí1
‚àí0.5
0
0.5
1
Pn (x)
Twenty Legendre polynomials
x
Figure 8.1
The Ô¨Årst 20 Legendre polynomials in successively Ô¨Åner linewidths. The
straight lines are P0(x) = 1 and P1(x) = x.
Example 8.1 (Building the Legendre polynomials)
Conditions (8.6) and (8.7)
give P0(x) = 1 and P1(x) = x. To make P2(x), we set n = 2 in the orthogonality
condition (8.6) and Ô¨Ånd 2a0 + 2a2/3 = 0 for m = 0, and 2a1/3 = 0 for m = 1.
The normalization condition (8.7) then says that a0 + a1 + a2 = 1. These three
equations give P2(x) = (3x2 ‚àí1)/2. Similarly, one Ô¨Ånds P3(x) = (5x3 ‚àí3x)/2
and P4(x) = (35x4 ‚àí30x2 + 3)/8.
8.2 The Rodrigues formula
Perhaps the easiest way to compute the Legendre polynomials is to apply
Leibniz‚Äôs rule (4.46) to the Rodrigues formula
Pn(x) =
1
2nn!
dn(x2 ‚àí1)n
dxn
,
(8.8)
which leads to (exercise 8.5)
Pn(x) = 1
2n
n

k=0
n
k
2
(x ‚àí1)n‚àík(x + 1)k.
(8.9)
306

8.2 THE RODRIGUES FORMULA
This formula at x = 1 is
Pn(1) = 1
2n
n

k=0
n
k
2
0n‚àík2k = 1
2n
n
n
2
2n = 1,
(8.10)
which shows that Rodrigues got the normalization right (Benjamin Rodrigues,
1795‚Äì1851).
Example 8.2 (Using Rodrigues‚Äôs formula)
By (8.8) or (8.9) and with more
effort, one Ô¨Ånds
P5(x) =
1
255!
d5(x2 ‚àí1)5
dx5
= 1
8

63x5 ‚àí70x3 + 15x

(8.11)
P6(x) = 1
26
6

k=0
6
k
2
(x ‚àí1)6‚àík(x + 1)k = 1
16

231x6 ‚àí315x4 + 105x2 ‚àí5

P7(x) = (x ‚àí1)7
27
7

k=0
7
k
2 x + 1
x ‚àí1
k
= 1
16

429x7 ‚àí693x5 + 315x3 ‚àí35x

.
In MATLAB, mfun(‚ÄôP‚Äô,n,x) returns the numerical value of Pn(x).
To check that the polynomial Pn(x) generated by Rodrigues‚Äôs formula (8.8)
is orthogonal to xm for m < n, we integrate xm Pn(x) by parts n times and drop
all the surface terms (which vanish because x2 ‚àí1 is zero at x = ¬±1)
 1
‚àí1
xm Pn(x) dx =
1
2nn!
 1
‚àí1
xm dn
dxn (x2 ‚àí1)n dx
= (‚àí1)n
2nn!
 1
‚àí1
(x2 ‚àí1)n dnxm
dxn dx = 0
for
n > m. (8.12)
Thus the polynomial Pn(x) generated by Rodrigues‚Äôs formula (8.8) satisÔ¨Åes
the orthogonality condition (8.3). It also satisÔ¨Åes the normalization condi-
tion (8.4) as shown by (8.10). The Rodrigues formula does generate Legendre‚Äôs
polynomials.
One may show (exercises 8.9, 8.10, & 8.11) that the inner product of two
Legendre polynomials is
 1
‚àí1
Pn(x) Pm(x) dx =
2
2n + 1 Œ¥nm.
(8.13)
307

LEGENDRE FUNCTIONS
8.3 The generating function
In the expansion
g(t, x) =

1 ‚àí2xt + t2‚àí1/2
=
‚àû

n=0
pn(x) tn
(8.14)
the coefÔ¨Åcient pn(x) is the nth partial derivative of g(t, x)
pn(x) = ‚àÇn
‚àÇtn

1 ‚àí2xt + t2‚àí1/2
t=0
(8.15)
and is a function of x alone. Explicit calculation shows that it is a polynomial
of degree n.
To identify these polynomials pn(x), we use the integral formula
 1
‚àí1
g(t, x) g(v, x) dx =
 1
‚àí1
dx

1 ‚àí2xt + t2
1 ‚àí2xv + v2 =
1
‚àötv ln 1 + ‚àötv
1 ‚àí‚àötv
(8.16)
and the logarithmic series (4.90)
1
‚àötv ln 1 + ‚àötv
1 ‚àí‚àötv =
‚àû

k=0
2
2k + 1 (tv)k
(8.17)
to express the integral of g(t, x) g(v, x) over the interval ‚àí1 ‚â§x ‚â§1 as
 1
‚àí1
g(t, x) g(v, x) dx =
 1
‚àí1
‚àû

n,m=0
pn(x) pm(x) tn vm dx =
‚àû

k=0
2
2k + 1 (tv)k.
(8.18)
Equating the coefÔ¨Åcients of tnvm in the second and third terms of this equation,
we see that the polynomials pn(x) satisfy
 1
‚àí1
pn(x) pm(x) dx =
2
2n + 1 Œ¥n,m,
(8.19)
which is the inner product rule (8.13) obeyed by the Legendre polynomials.
Next, setting x = 1 in the deÔ¨Ånition (8.14) of g(t, x), we get from (4.31)
1
1 ‚àít =
‚àû

n=0
tn =
‚àû

n=0
pn(1) tn,
(8.20)
which says that pn(1) = 1 for all nonnegative integers n = 0, 1, 2, and so forth.
The polynomials pn(x) are therefore the Legendre polynomials Pn(x), and the
function g(t, x) is their generating function
1

1 ‚àí2xt + t2 =
‚àû

n=0
tn Pn(x).
(8.21)
308

8.4 LEGENDRE‚ÄôS DIFFERENTIAL EQUATION
Example 8.3 (The Green‚Äôs function for Poisson‚Äôs equation)
The Green‚Äôs func-
tion (3.110) for the Laplacian is
G(R ‚àír) =
1
4œÄ|R ‚àír| =
1
4œÄ

R2 ‚àí2R ¬∑ r + r2 ,
(8.22)
in which R = |R| and r = |r|. It occurs throughout physics and satisÔ¨Åes
‚àí‚àá2G(R ‚àír) = Œ¥(3)(R ‚àír)
(8.23)
where the derivatives can act on R or on r.
We set x = cos Œ∏ = R ¬∑ r/rR and t = r/R, and then factor out 1/R
1
|R ‚àír| =
1

R2 ‚àí2Rr cos Œ∏ + r2
= 1
R
1

1 ‚àí2(r/R)x + (r/R)2
= 1
R
1

1 ‚àí2xt + t2 = 1
R g(t, x).
(8.24)
With t = r/R and so forth, this series is the well-known expansion
1
|R ‚àír| = 1
R
‚àû

n=0
 r
R
n
Pn(cos Œ∏)
(8.25)
of the Green‚Äôs function G(R ‚àír) = 1/4œÄ|R ‚àír| = g(t, x)/4œÄR.
8.4 Legendre‚Äôs differential equation
Apart from the prefactor 1/ (2nn!), the Legendre polynomial Pn(x) is the nth
derivative u(n) of u = (x2 ‚àí1)n. Since u‚Ä≤ = 2nx(x2 ‚àí1)n‚àí1, the function u
satisÔ¨Åes (x2 ‚àí1)u‚Ä≤ = 2nxu. Using Leibniz‚Äôs rule (4.46) to differentiate (n + 1)
times both sides of this equation 2nxu = (x2 ‚àí1)u‚Ä≤, we Ô¨Ånd
(2nxu)(n+1) = 2n
n+1

k=0
n + 1
k

x(k) u(n+1‚àík) = 2n

x u(n+1) + (n + 1) u(n)
(8.26)
and

(x2 ‚àí1)u‚Ä≤(n+1)
=
n+1

k=0
n + 1
k

(x2 ‚àí1)(k) u(n+2‚àík)
= (x2 ‚àí1)u(n+2) + 2(n + 1)xu(n+1) + n(n + 1)u(n).
(8.27)
309

LEGENDRE FUNCTIONS
Equating the two and setting u(n) = 2nn!Pn, we get
‚àí

(1 ‚àíx2) P‚Ä≤
n
‚Ä≤
= n(n + 1) Pn,
(8.28)
which is Legendre‚Äôs equation in self-adjoint form.
The differential operator
L = ‚àíd
dx p(x) d
dx = ‚àíd
dx (1 ‚àíx2) d
dx
(8.29)
is formally self adjoint and the real function p(x) = 1 ‚àíx2 is positive on the
open interval (‚àí1, 1) and vanishes at x = ¬±1, so Legendre‚Äôs differential opera-
tor L, his differential equation (8.28), and the domain D of functions that are
twice differentiable on the interval [‚àí1, 1] form a singular self-adjoint system
(example 6.33). The Legendre polynomial Pn(x) is an eigenfunction of L with
eigenvalue n(n + 1) and weight function w(x) = 1. The orthogonality relation
(6.324) tells us that eigenfunctions of a self-adjoint differential operator that
have different eigenvalues are orthogonal on the interval [‚àí1, 1] with respect to
the weight function w(x). Thus Pn(x) and Pm(x) are orthogonal for n Ã∏= m
 1
‚àí1
Pn(x) Pm(x) dx =
2
2n + 1 Œ¥nm
(8.30)
as we saw (8.13) directly from the Rodrigues formula.
The eigenvalues n(n + 1) increase without limit, and so the argument of sec-
tion 6.35 shows that the eigenfunctions Pn(x) are complete. Since the weight
function of the Legendre polynomials is unity w(x) = 1, the expansion (6.374)
of Dirac‚Äôs delta function here is
Œ¥(x ‚àíx‚Ä≤) =
‚àû

n=0
2n + 1
2
Pn(x‚Ä≤) Pn(x),
(8.31)
which leads to the Fourier‚ÄìLegendre expansion
f (x) =
‚àû

n=0
2n + 1
2
Pn(x)
 1
‚àí1
Pn(x‚Ä≤) f (x‚Ä≤) dx‚Ä≤
(8.32)
at least for functions f that are twice differentiable on [ ‚àí1, 1].
Changing variables to cos Œ∏ = x, we have (1 ‚àíx2) = sin2 Œ∏ and
d
dŒ∏ = d cos Œ∏
dŒ∏
d
dx = ‚àísin Œ∏ d
dx
(8.33)
so that
d
dx = ‚àí
1
sin Œ∏
d
dŒ∏ .
(8.34)
310

8.5 RECURRENCE RELATIONS
Thus in spherical coordinates, Legendre‚Äôs equation (8.28) appears as
1
sin Œ∏
d
dŒ∏
#
sin Œ∏ d
dŒ∏ Pn(cos Œ∏)
$
+ n(n + 1) Pn(cos Œ∏) = 0.
(8.35)
8.5 Recurrence relations
The t-derivative of the generating function g(t, x) = 1/

1 ‚àí2xt + t2 is
‚àÇg(t, x)
‚àÇt
=
x ‚àít
(1 ‚àí2xt + t2)3/2 =
‚àû

n=1
n Pn(x) tn‚àí1,
(8.36)
which we can rewrite as
(1 ‚àí2xt + t2)
‚àû

n=1
n Pn(x) tn‚àí1 = (x ‚àít) g(t, x) = (x ‚àít)
‚àû

n=0
Pn(x) tn.
(8.37)
By equating the coefÔ¨Åcients of tn in the Ô¨Årst and last of these expressions, we
arrive at the recurrence relation
Pn+1(x) =
1
n + 1 [(2n + 1) x Pn(x) ‚àín Pn‚àí1(x)] .
(8.38)
Example 8.4 (Building the Legendre polynomials)
Since P1(x) = x and
P0(x) = 1, this recurrence relation for n = 1 gives
P2(x) = 1
2 [3 x P1(x) ‚àíP0(x)] = 1
2

3x2 ‚àí1

.
(8.39)
Similarly for n = 2 it gives
P3(x) = 1
3 [5 x P2(x) ‚àí2 P1(x)] = 1
2(5x2 ‚àí3x).
(8.40)
It builds Legendre polynomials faster than Rodrigues‚Äôs formula (8.8).
The x-derivative of the generating function is
‚àÇg(t, x)
‚àÇx
=
t
(1 ‚àí2xt + t2)3/2 =
‚àû

n=1
P‚Ä≤
n(x) tn,
(8.41)
which we can rewrite as
(1 ‚àí2xt + t2)
‚àû

n=1
P‚Ä≤
n(x) tn = t g(t, x) =
‚àû

n=0
Pn(x) tn+1.
(8.42)
Equating coefÔ¨Åcients of tn, we have
P‚Ä≤
n+1(x) + P‚Ä≤
n‚àí1(x) = 2x P‚Ä≤
n(x) + Pn(x).
(8.43)
311

LEGENDRE FUNCTIONS
By differentiating the recurrence relation (8.38) and combining it with this last
equation, we get
P‚Ä≤
n+1(x) ‚àíP‚Ä≤
n‚àí1(x) = (2n + 1) Pn(x).
(8.44)
The last two recurrence relations (8.43 & 8.44) lead to several more:
P‚Ä≤
n+1(x) = (n + 1) Pn(x) + x P‚Ä≤
n(x),
(8.45)
P‚Ä≤
n‚àí1(x) = ‚àín Pn(x) + x P‚Ä≤
n(x),
(8.46)
(1 ‚àíx2) P‚Ä≤
n(x) = n Pn‚àí1(x) ‚àínx Pn(x),
(8.47)
(1 ‚àíx2) P‚Ä≤
n(x) = (n + 1)x Pn(x) ‚àí(n + 1) Pn+1(x).
(8.48)
By differentiating (8.48) and using (8.45) for P‚Ä≤
n+1, we recover Legendre‚Äôs
equation ‚àí[(1 ‚àíx2)P‚Ä≤
n]‚Ä≤ = n(n + 1)Pn.
8.6 Special values of Legendre‚Äôs polynomials
At x = ‚àí1, the generating function is
g(t, ‚àí1) =

1 + t2 + 2t
‚àí1/2
=
1
1 + t =
‚àû

n=0
(‚àít)n =
‚àû

n=0
Pn(‚àí1) tn,
(8.49)
which implies that
Pn(‚àí1) = (‚àí1)n
(8.50)
and reminds us of the normalization condition (8.4), Pn(1) = 1.
The generating function g(t, x) is even under the reÔ¨Çection of both indepen-
dent variables, so
g(t, x) =
‚àû

n=0
tn Pn(x) =
‚àû

n=0
(‚àít)n Pn(‚àíx) = g(‚àít, ‚àíx),
(8.51)
which implies that
Pn(‚àíx) = (‚àí1)n Pn(x)
whence
P2n+1(0) = 0.
(8.52)
With more effort, one can show that
P2n(0) = (‚àí1)n (2n ‚àí1)!!
(2n)!!
and that
|Pn(x)| ‚â§1.
(8.53)
312

8.8 ORTHOGONAL POLYNOMIALS
8.7 SchlaeÔ¨Çi‚Äôs integral
SchlaeÔ¨Çi used Rodrigues‚Äôs formula
Pn(x) =
1
2n n!
 d
dx
n
(x2 ‚àí1)n
(8.54)
to express Pn(z‚Ä≤) as the counterclockwise contour integral
Pn(z‚Ä≤) =
1
2n 2œÄi
)
(z2 ‚àí1)n
(z ‚àíz‚Ä≤)n+1 dz‚Ä≤.
(8.55)
8.8 Orthogonal polynomials
Rodrigues‚Äôs formula (8.8) generates other families of orthogonal polynomials.
The n-th order polynomials Rn
Rn(x) =
1
enw(x)
dn
dxn [w(x) Qn(x)]
(8.56)
are orthogonal on the interval from a to b with weight function w(x)
 b
a
Rn(x) Rk(x) w(x) dx = Nn Œ¥nk
(8.57)
as long as the product w(x) Qn(x) vanishes at a and b (exercise 8.8)
w(a) Qn(a) = w(b) Qn(b) = 0.
(8.58)
Example 8.5 (Jacobi‚Äôs polynomials)
The choice Q(x) = (x2 ‚àí1) with weight
function w(x) = (1 ‚àíx)Œ±(1 + x)Œ≤ and normalization en = 2nn! leads for Œ± > ‚àí1
and Œ≤ > ‚àí1 to the Jacobi polynomials
P(Œ±,Œ≤)
n
(x) =
1
2nn!(1 ‚àíx)‚àíŒ±(1 + x)‚àíŒ≤ dn
dxn

(1 ‚àíx)Œ±(1 + x)Œ≤ (x2 ‚àí1)n
,
(8.59)
which are orthogonal on [‚àí1, 1]
 1
‚àí1
P(Œ±,Œ≤)
n
(x) P(Œ±,Œ≤)
m
(x) w(x) dx = 2Œ±+Œ≤+1(n + Œ± + 1) (n + Œ≤ + 1)
(2n + Œ± + Œ≤ + 1) (n + Œ± + Œ≤ + 1) Œ¥nm (8.60)
and satisfy the normalization condition
P(Œ±,Œ≤)
n
(1) =
n + Œ±
n

(8.61)
and the differential equation
(1 ‚àíx2) y‚Ä≤‚Ä≤ + (Œ≤ ‚àíŒ± ‚àí(Œ± + Œ≤ + 2)x) y‚Ä≤ + n(n + Œ± + Œ≤ + 1) y = 0.
(8.62)
In terms of R(x, y) =

1 ‚àí2xy + y2, their generating function is
313

LEGENDRE FUNCTIONS
2Œ±+Œ≤(1 ‚àíy + R(x, y))‚àíŒ±(1 + w + R(x, y))‚àíŒ≤/R(x, y) =
‚àû

n=0
P(Œ±,Œ≤)
n
(x)yn. (8.63)
When Œ± = Œ≤, they are the Gegenbauer polynomials, which for Œ± = Œ≤ = ¬±1/2
are the Chebyshev polynomials (of the second and Ô¨Årst kind, respectively). For
Œ± = Œ≤ = 0, they are Legendre‚Äôs polynomials.
Example 8.6 (Hermite‚Äôs polynomials)
The choice Q(x) = 1 with weight
function w(x) = exp( ‚àíx2) leads to the Hermite polynomials
Hn(x) = (‚àí1)nex2 dn
dxn e‚àíx2 = ex2/2

x ‚àíd
dx
n
e‚àíx2/2 = 2n e‚àíD2/4 xn
(8.64)
where D = d/dx is the x-derivative. They are orthogonal on the real line
 ‚àû
‚àí‚àû
Hn(x) Hm(x) e‚àíx2 dx = ‚àöœÄ 2n n! Œ¥nm
(8.65)
and satisfy the differential equation
y‚Ä≤‚Ä≤ ‚àí2 x y‚Ä≤ + 2 n y = 0.
(8.66)
Their generating function is
e2xy‚àíy2 =
‚àû

n=0
Hn(x) yn
n! .
(8.67)
The nth excited state of the harmonic oscillator of mass m and angular fre-
quency œâ is proportional to Hn(x) in which x =

mœâ/¬Øh q is the dimensionless
position of the oscillator.
Example 8.7 (Laguerre‚Äôs polynomials)
The choices Q(x) = x and weight
function w(x) = xŒ±e‚àíx lead to the generalized Laguerre polynomials
L(Œ±)
n (x) =
ex
n! xŒ±
dn
dxn

e‚àíxxn+Œ±
.
(8.68)
They are orthogonal on the interval [0, ‚àû)
 ‚àû
0
L(Œ±)
n (x) L(Œ±)
m (x) xŒ±e‚àíx dx = (n + Œ± + 1)
n!
Œ¥n,m
(8.69)
and satisfy the differential equation
x y‚Ä≤‚Ä≤ + (Œ± + 1 ‚àíx) y‚Ä≤ + n y = 0.
(8.70)
Their generating function is
(1 ‚àíy)‚àíŒ±‚àí1 exp
 x y
y ‚àí1

=
‚àû

n=0
L(Œ±)
n (x) yn.
(8.71)
314

8.9 THE AZIMUTHALLY SYMMETRIC LAPLACIAN
The radial wave-function for the state of the nonrelativistic hydrogen atom with
quantum numbers n and ‚Ñìis œÅ‚ÑìL2‚Ñì+1
n‚àí‚Ñì‚àí1(œÅ) e‚àíœÅ/2 in which œÅ = 2r/na0 and a0 is
the Bohr radius a0 = 4œÄœµ0 ¬Øh2/mee2 .
8.9 The azimuthally symmetric Laplacian
We saw in section 6.5 that the Laplacian ‚ñ≥= ‚àá¬∑ ‚àáseparates in spherical
coordinates r, Œ∏, œÜ. A system with no dependence on the angle œÜ is said to have
azimuthal symmetry. An azimuthally symmetric function
f (r, Œ∏, œÜ) = Rk,‚Ñì(r) ‚Ñì(Œ∏)
(8.72)
will be a solution of Helmholtz‚Äôs equation
‚àí‚ñ≥f = k2f
(8.73)
if the functions Rk,‚Ñì(r) and ‚Ñì(Œ∏) satisfy
1
r2
d
dr

r2 dRk,‚Ñì
dr

+
#
k2 ‚àí‚Ñì(‚Ñì+ 1)
r2
$
Rk,‚Ñì= 0
(8.74)
for a nonnegative integer ‚Ñìand Legendre‚Äôs equation (8.35)
1
sin Œ∏
d
dŒ∏

sin Œ∏ d‚Ñì
dŒ∏

+ ‚Ñì(‚Ñì+ 1)‚Ñì= 0
(8.75)
so that we may set ‚Ñì(Œ∏) = P‚Ñì(cos Œ∏). For k > 0, the solutions of the radial
equation (8.74) that are regular at r = 0 are the spherical Bessel functions
Rk,‚Ñì(r) = j‚Ñì(kr),
(8.76)
which are given by Rayleigh‚Äôs formula (9.68)
j‚Ñì(x) = (‚àí1)‚Ñìx‚Ñì
 d
xdx
‚Ñìsin x
x

.
(8.77)
So the general azimuthally symmetric solution of the Helmholtz equation (8.73)
that is regular at r = 0 is
f (r, Œ∏) =
‚àû

‚Ñì=0
ak,‚Ñìj‚Ñì(kr) P‚Ñì(cos Œ∏),
(8.78)
in which the ak,‚Ñìare constants. If the solution need not be regular at the origin,
then the Neumann functions
n‚Ñì(x) = ‚àí(‚àí1)‚Ñìx‚Ñì
 d
xdx
‚Ñìcos x
x

(8.79)
315

LEGENDRE FUNCTIONS
must be included, and the general solution then is
f (r, Œ∏) =
‚àû

‚Ñì=0

ak,‚Ñìj‚Ñì(kr) + bk,‚Ñìn‚Ñì(kr)

P‚Ñì(cos Œ∏),
(8.80)
in which the ak,‚Ñìand bk,‚Ñìare constants.
When k = 0, Helmholtz‚Äôs equation reduces to Laplace‚Äôs
‚ñ≥f = 0,
(8.81)
which describes the Coulomb-gauge electric potential in the absence of charges
and the Newtonian gravitational potential in the absence of masses. Now the
radial equation is simply
d
dr

r2 dR‚Ñì
dr

= ‚Ñì(‚Ñì+ 1)R‚Ñì
(8.82)
since k = 0. We try setting
R‚Ñì(r) = rn,
(8.83)
which works if n(n + 1) = ‚Ñì(‚Ñì+ 1), that is, if n = ‚Ñìor n = ‚àí(‚Ñì+ 1). So the
general solution to (8.81) is
f (r, Œ∏) =
‚àû

‚Ñì=0

a‚Ñìr‚Ñì+ b‚Ñìr‚àí‚Ñì‚àí1
P‚Ñì(cos Œ∏).
(8.84)
If the solution must be regular at r = 0, then all the b‚Ñìs must vanish.
8.10 Laplacian in two dimensions
In section 6.5, we saw that Helmholtz‚Äôs equation separates in cylindrical coor-
dinates, and that the equation for P(œÅ) is Bessel‚Äôs equation (6.47). But if Œ± = 0,
Helmholtz‚Äôs equation reduces to Laplace‚Äôs equation ‚ñ≥f = 0, and if the poten-
tial f also is independent of z, then simpler solutions exist. For now Œ± = 0 = k,
and so if ‚Ä≤‚Ä≤
m = ‚àím2m, then equation (6.47) becomes
œÅ d
dœÅ

œÅ dPm
dœÅ

= m2 Pm.
(8.85)
The function (œÜ) may be taken to be (œÜ) = exp(imœÜ) or a linear combination
of cos(mœÜ) and sin(mœÜ). If the whole range of œÜ from 0 to 2œÄ is physically
relevant, then (œÜ) must be periodic, and so m must be an integer. To solve this
equation (8.85) for Pm, we set Pm = œÅn and get
n2 œÅn = m2 œÅn,
(8.86)
316

8.12 THE ASSOCIATED LEGENDRE FUNCTIONS/POLYNOMIALS
which says that n = ¬±m. The general z-independent solution of Laplace‚Äôs
equation in cylindrical coordinates then is
f (œÅ, œÜ) =
‚àû

m=0
(am cos(mœÜ) + bm sin(mœÜ))

cmœÅm + dmœÅ‚àím
.
(8.87)
8.11 The Laplacian in spherical coordinates
The Laplacian ‚ñ≥separates in spherical coordinates, as we saw in section 6.5.
Thus a function
f (r, Œ∏) = Rk,‚Ñì(r) ‚Ñì,m(Œ∏) m(œÜ)
(8.88)
will be a solution of the Helmholtz equation ‚àí‚ñ≥f = k2f if Rk,‚Ñìis a linear
combination of the spherical Bessel functions j‚Ñì(8.77) and n‚Ñì(8.79)
Rk,‚Ñì(r) = ak,‚Ñìj‚Ñì(kr) + bk,‚Ñìn‚Ñì(kr)
(8.89)
if m = eimœÜ, and if ‚Ñì,m satisÔ¨Åes the associated Legendre equation
1
sin Œ∏
d
dŒ∏

sin Œ∏ d‚Ñì,m
dŒ∏

+

‚Ñì(‚Ñì+ 1) ‚àí
m2
sin2 Œ∏

‚Ñì,m = 0.
(8.90)
8.12 The associated Legendre functions/polynomials
The associated Legendre functions Pm
‚Ñì(x) ‚â°P‚Ñì,m(x) are polynomials in sin Œ∏
and cos Œ∏. They arise as solutions of the separated Œ∏ equation (8.90)
1
sin Œ∏
d
dŒ∏

sin Œ∏ dP‚Ñì,m
dŒ∏

+

‚Ñì(‚Ñì+ 1) ‚àí
m2
sin2 Œ∏

P‚Ñì,m = 0
(8.91)
of the Laplacian in spherical coordinates. In terms of x = cos Œ∏, this self-adjoint
ordinary differential equation (ODE) is

(1 ‚àíx2)P‚Ä≤
‚Ñì,m(x)
‚Ä≤
+

‚Ñì(‚Ñì+ 1) ‚àí
m2
1 ‚àíx2

P‚Ñì,m(x) = 0.
(8.92)
To Ô¨Ånd the P‚Ñì,ms, we use Leibniz‚Äôs rule (4.46) to differentiate Legendre‚Äôs
equation (8.28)

(1 ‚àíx2) P‚Ä≤
‚Ñì
‚Ä≤
+ ‚Ñì(‚Ñì+ 1) P‚Ñì= 0
(8.93)
m times, obtaining
(1 ‚àíx2)P(m+2)
‚Ñì
‚àí2x(m + 1)P(m+1)
‚Ñì
+ (‚Ñì‚àím)(‚Ñì+ m + 1)P(m)
‚Ñì
= 0.
(8.94)
317

LEGENDRE FUNCTIONS
We may make this equation self adjoint by using the prefactor formula
(6.258)
F =
1
1 ‚àíx2 exp
#
(m + 1)
 x ‚àí2x‚Ä≤
1 ‚àíx‚Ä≤2 dx‚Ä≤
$
=
1
1 ‚àíx2 exp

(m + 1) ln(1 ‚àíx2)

= (1 ‚àíx2)m.
(8.95)
The resulting ordinary differential equation

(1 ‚àíx2)m+1P‚Ä≤(m)
‚Ñì
‚Ä≤
+ (1 ‚àíx2)m(‚Ñì‚àím)(‚Ñì+ m + 1)P(m)
‚Ñì
= 0
(8.96)
is self adjoint, but it is not (8.92).
Instead, we deÔ¨Åne P‚Ñì,m in terms of the mth derivative P(m)
‚Ñì
as
P(m)
‚Ñì(x) = (1 ‚àíx2)‚àím/2 P‚Ñì,m(x)
(8.97)
and compute the derivatives
P(m+1)
‚Ñì
=

P‚Ä≤
‚Ñì,m + mxP‚Ñì,m
1 ‚àíx2

(1 ‚àíx2)‚àím/2
(8.98)
P(m+2)
‚Ñì
=

P‚Ä≤‚Ä≤
‚Ñì,m +
2mxP‚Ä≤
‚Ñì,m
1 ‚àíx2
+ mP‚Ñì,m
1 ‚àíx2 + m(m + 2)x2P‚Ñì,m
(1 ‚àíx2)2

(1 ‚àíx2)‚àím/2.
When we put these three expressions in equation (8.94), then we get the desired
ODE (8.92). The associated Legendre functions are
P‚Ñì,m(x) = (1 ‚àíx2)m/2 P(m)
‚Ñì(x) = (1 ‚àíx2)m/2 dm
dxm P‚Ñì(x).
(8.99)
They are simple polynomials in x = cos Œ∏ and

1 ‚àíx2 = sin Œ∏
P‚Ñì,m(cos Œ∏) = sinm Œ∏
dm
d cosm Œ∏ P‚Ñì(cos Œ∏).
(8.100)
It follows from Rodrigues‚Äôs formula (8.8) for the Legendre polynomial P‚Ñì(x)
that P‚Ñì,m(x) is given by the similar formula
P‚Ñì,m(x) = (1 ‚àíx2)m/2
2‚Ñì‚Ñì!
d‚Ñì+m
dx‚Ñì+m (x2 ‚àí1)‚Ñì,
(8.101)
which tells us that under parity Pm
‚Ñì(x) changes by (‚àí1)‚Ñì+m
P‚Ñì,m(‚àíx) = (‚àí1)‚Ñì+m P‚Ñì,m(x).
(8.102)
Rodrigues‚Äôs formula (8.101) for the associated Legendre function makes sense
as long as ‚Ñì+ m ‚â•0. This last condition is the requirement in quantum
mechanics that m not be less than ‚àí‚Ñì. And if m exceeds ‚Ñì, then P‚Ñì,m(x) is given
318

8.13 SPHERICAL HARMONICS
by more than 2‚Ñìderivatives of a polynomial of degree 2‚Ñì; so P‚Ñì,m(x) = 0 if
m > ‚Ñì. This last condition is the requirement in quantum mechanics that m not
be greater than ‚Ñì. So we have
‚àí‚Ñì‚â§m ‚â§‚Ñì.
(8.103)
One may show that
P‚Ñì,‚àím(x) = (‚àí1)m (‚Ñì‚àím)!
(‚Ñì+ m)! P‚Ñì,m(x).
(8.104)
In fact, since m occurs only as m2 in the ordinary differential equation (8.92),
P‚Ñì,‚àím(x) must be proportional to P‚Ñì,m(x).
Under reÔ¨Çections, the parity of P‚Ñì,m is (‚àí1)‚Ñì+m, that is
P‚Ñì,m(‚àíx) = (‚àí1)‚Ñì+m P‚Ñì,m(x).
(8.105)
If m Ã∏= 0, then P‚Ñì,m(x) has a power of

1 ‚àíx2 in it, so
P‚Ñì,m(¬±1) = 0.
(8.106)
We may consider either ‚Ñì(‚Ñì+ 1) or m2 as the eigenvalue in the ODE (8.92)

(1 ‚àíx2)P‚Ä≤
‚Ñì,m(x)
‚Ä≤
+

‚Ñì(‚Ñì+ 1) ‚àí
m2
1 ‚àíx2

P‚Ñì,m(x) = 0.
(8.107)
If ‚Ñì(‚Ñì+1) is the eigenvalue, then the weight function is unity, and since this ODE
is self adjoint on the interval [‚àí1, 1] (at the ends of which p(x) = (1 ‚àíx2) = 0),
the eigenfunctions P‚Ñì,m(x) and P‚Ñì‚Ä≤,m(x) must be orthogonal on that interval
when ‚ÑìÃ∏= ‚Ñì‚Ä≤. The full integral formula is
 1
‚àí1
P‚Ñì,m(x) P‚Ñì‚Ä≤,m(x) dx =
2
2‚Ñì+ 1
(‚Ñì+ m)!
(‚Ñì‚àím)! Œ¥‚Ñì,‚Ñì‚Ä≤.
(8.108)
If m2 for Ô¨Åxed ‚Ñìis the eigenvalue, then the weight function is 1/(1 ‚àíx2), and
the eigenfunctions P‚Ñì,m(x) and P‚Ñì‚Ä≤,m(x) must be orthogonal on [‚àí1, 1] when
m Ã∏= m‚Ä≤. The full formula is
 1
‚àí1
P‚Ñì,m(x) P‚Ñì,m‚Ä≤(x)
dx
1 ‚àíx2 = (‚Ñì+ m)!
m(‚Ñì‚àím)! Œ¥m,m‚Ä≤.
(8.109)
8.13 Spherical harmonics
The spherical harmonic Ym
‚Ñì(Œ∏, œÜ) ‚â°Y‚Ñì,m(Œ∏, œÜ) is the product
Y‚Ñì,m(Œ∏, œÜ) = ‚Ñì,m(Œ∏) m(œÜ)
(8.110)
319

LEGENDRE FUNCTIONS
in which ‚Ñì,m(Œ∏) is proportional to the associated Legendre function P‚Ñì,m
‚Ñì,m(Œ∏) = (‚àí1)m
(
2‚Ñì+ 1
2
(‚Ñì‚àím)!
(‚Ñì+ m)! P‚Ñì,m(cos Œ∏)
(8.111)
and
m(œÜ) = eimœÜ
‚àö
2œÄ
.
(8.112)
The big square-root in the deÔ¨Ånition (8.111) ensures that
 2œÄ
0
dœÜ
 œÄ
0
sin Œ∏ dŒ∏ Y‚àó
‚Ñì,m(Œ∏, œÜ) Y‚Ñì‚Ä≤,m‚Ä≤(Œ∏, œÜ) = Œ¥‚Ñì‚Ñì‚Ä≤ Œ¥mm‚Ä≤.
(8.113)
In spherical coordinates, the parity transformation
x‚Ä≤ = ‚àíx
(8.114)
is r‚Ä≤ = r, Œ∏‚Ä≤ = œÄ ‚àíŒ∏, and œÜ‚Ä≤ = œÜ ¬± œÄ. So under parity, cos Œ∏‚Ä≤ = ‚àícos Œ∏ and
exp(imœÜ‚Ä≤) = (‚àí1)m exp(imœÜ). This factor of (‚àí1)m cancels the m-dependence
(8.102) of P‚Ñì,m(Œ∏) under parity, so that under parity
Y‚Ñì,m(Œ∏‚Ä≤, œÜ‚Ä≤) = Y‚Ñì,m(œÄ ‚àíŒ∏, œÜ ¬± œÄ) = (‚àí1)‚ÑìY‚Ñì,m(Œ∏, œÜ).
(8.115)
Thus the parity of the state |n, ‚Ñì, m‚ü©is (‚àí1)‚Ñì.
The spherical harmonics are complete on the unit sphere. They may be used
to expand any smooth function f (Œ∏, œÜ) as
f (Œ∏, œÜ) =
‚àû

‚Ñì=0
‚Ñì

m=‚àí‚Ñì
a‚ÑìmY‚Ñì,m(Œ∏, œÜ).
(8.116)
The orthonormality relation (8.113) says that the coefÔ¨Åcients a‚Ñìm are
a‚Ñìm =
 2œÄ
0
dœÜ
 œÄ
0
sin Œ∏ dŒ∏ Y‚àó
‚Ñì,m(Œ∏, œÜ) f (Œ∏, œÜ).
(8.117)
Putting the last two equations together, we Ô¨Ånd
f (Œ∏, œÜ) =
 2œÄ
0
dœÜ‚Ä≤
 œÄ
0
sin Œ∏‚Ä≤ dŒ∏‚Ä≤
 ‚àû

‚Ñì=0
‚Ñì

m=‚àí‚Ñì
Y‚àó
‚Ñì,m(Œ∏‚Ä≤, œÜ‚Ä≤) Y‚Ñì,m(Œ∏, œÜ)

f (Œ∏‚Ä≤, œÜ‚Ä≤)
(8.118)
and so we may identify the sum within the brackets as an angular delta
function
320

8.13 SPHERICAL HARMONICS
‚àû

‚Ñì=0
‚Ñì

m=‚àí‚Ñì
Y‚àó
‚Ñì,m(Œ∏‚Ä≤, œÜ‚Ä≤) Y‚Ñì,m(Œ∏, œÜ) =
1
sin Œ∏ Œ¥(Œ∏ ‚àíŒ∏‚Ä≤) Œ¥(œÜ ‚àíœÜ‚Ä≤),
(8.119)
which sometimes is abbreviated as
‚àû

‚Ñì=0
‚Ñì

m=‚àí‚Ñì
Y‚àó
‚Ñì,m(‚Ä≤) Y‚Ñì,m() = Œ¥(2)( ‚àí‚Ä≤).
(8.120)
The spherical-harmonic expansion (8.116) of the Legendre polynomial
P‚Ñì(ÀÜn ¬∑ ÀÜn‚Ä≤) of the cosine ÀÜn ¬∑ ÀÜn‚Ä≤ in which the polar angles of the unit vectors
respectively are Œ∏, œÜ and Œ∏‚Ä≤, œÜ‚Ä≤ is the addition theorem
P‚Ñì(ÀÜn ¬∑ ÀÜn‚Ä≤) =
4œÄ
2‚Ñì+ 1
‚Ñì

m=‚àí‚Ñì
Y‚Ñì,m(Œ∏, œÜ)Y‚àó
‚Ñì,m(Œ∏‚Ä≤, œÜ‚Ä≤)
=
4œÄ
2‚Ñì+ 1
‚Ñì

m=‚àí‚Ñì
Y‚àó
‚Ñì,m(Œ∏, œÜ)Y‚Ñì,m(Œ∏‚Ä≤, œÜ‚Ä≤).
(8.121)
Example 8.8 (CMB radiation)
Instruments on the Wilkinson Microwave
Anisotropy Probe (WMAP) and Planck satellites in orbit at the Lagrange point
L2 (in the Earth‚Äôs shadow, 1.5√ó106 km farther from the Sun) measure the
temperature T(Œ∏, œÜ) of the cosmic microwave background (CMB) radiation as
a function of the polar angles Œ∏ and œÜ in the sky as shown in Fig. 8.2. This
radiation is photons last scattered when the visible Universe became transparent
Figure 8.2
The CMB temperature Ô¨Çuctuations over the celestial sphere as mea-
sured over 7 years by the WMAP satellite. The average temperature is 2.725 K.
White regions are warmer, and black ones colder by about 0.0002 degrees. Courtesy
of the NASA/WMAP Science Team.
321

LEGENDRE FUNCTIONS
at an age of 360,000 years and a temperature (3,000 K) cool enough for hydrogen
atoms to be stable. This initial transparency is usually (and inexplicably) called
recombination.
Since the spherical harmonics Y‚Ñì,m(Œ∏, œÜ) are complete on the sphere, we can
expand the temperature as
T(Œ∏, œÜ) =
‚àû

‚Ñì=0
‚Ñì

m=‚àí‚Ñì
a‚Ñì,m Y‚Ñì,m(Œ∏, œÜ),
(8.122)
in which the coefÔ¨Åcients are by (8.117)
a‚Ñì,m =
 2œÄ
0
dœÜ
 œÄ
0
sin Œ∏ dŒ∏ Y‚àó
‚Ñì,m(Œ∏, œÜ) T(Œ∏, œÜ).
(8.123)
The average temperature T contributes only to a0,0 = T = 2.725 K. The other
coefÔ¨Åcients describe the difference T(Œ∏, œÜ) = T(Œ∏, œÜ) ‚àíT. The angular power
spectrum is
C‚Ñì=
1
2‚Ñì+ 1
‚Ñì

m=‚àí‚Ñì
|a‚Ñì,m|2.
(8.124)
If we let the unit vector ÀÜn point in the direction Œ∏, œÜ and use the addition theorem
(8.121), then we can write the angular power spectrum as
C‚Ñì= 1
4œÄ

d2 ÀÜn

d2 ÀÜn‚Ä≤ P‚Ñì(ÀÜn ¬∑ ÀÜn‚Ä≤) T(ÀÜn) T(ÀÜn‚Ä≤).
(8.125)
In Fig. 8.3, the measured values (Larson et al., 2011) of the power spectrum
‚Ñì(‚Ñì+ 1) C‚Ñì/2œÄ are plotted against ‚Ñìfor 1 < ‚Ñì< 1300 with the angles and
distances decreasing with ‚Ñì. The power spectrum is a snapshot at the moment
of transparency of the temperature distribution of the plasma of electrons and
nuclei undergoing dissipative plasma oscillations in which gravity opposes radi-
ation pressure. These acoustic oscillations are slowest when they are at their
maxima and minima; the temperature is high at these maxima and minima.
Oscillations at their Ô¨Årst maximum form the Ô¨Årst peak. Those at their Ô¨Årst mini-
mum form the second peak, and those at their second maximum form the third
peak, and so forth.
The solid curve represents the prediction of an inÔ¨Çationary cosmological
model with cold dark matter and a cosmological constant . In this 	CDM
cosmology, the age of the visible Universe is 13.77 Gyr; the Hubble constant is
H0 = 70.4 km/sMpc; the total energy density of the Universe is enough to make
the Universe Ô¨Çat as required by inÔ¨Çation; and the fractions of the energy density
respectively due to baryons, dark matter, and dark energy are 4.55%, 22.8%, and
72.7% (Edwin Hubble, 1889‚Äì1953).
322

EXERCISES
Multipole moment l
10
100
500
1000 
6000
5000
4000
3000
2000
1000
0 90Àö
2Àö
0.5Àö
0.2Àö
Angular size
Temperature fluctuations [Œºk2]
Figure 8.3
The power spectrum ‚Ñì(‚Ñì+ 1)C‚Ñì/2œÄ of the CMB temperature Ô¨Çuctu-
ations in ŒºK2 as measured by WMAP (Larson et al., 2011) over 7 years is plotted
against the angular size and the multipole moment ‚Ñì. The solid curve is the CDM
prediction.
Much is known about Legendre functions. The books A Course of Modern
Analysis (Whittaker and Watson, 1927, chap. XV) and Methods of Mathemati-
cal Physics (Courant and Hilbert, 1955) are outstanding.
Exercises
8.1
Use conditions (8.6) and (8.7) to Ô¨Ånd P0(x) and P1(x).
8.2
Using the Gram‚ÄìSchmidt method (section 1.10) to turn the functions xn into
a set of functions Ln(x) that are orthonormal on the interval [‚àí1, 1] with inner
product (8.2), Ô¨Ånd Ln(x) for n = 0, 1, 2, and 3. Isn‚Äôt Rodrigues‚Äôs formula (8.8)
easier to use?
8.3
Derive the conditions (8.6‚Äì8.7) on the coefÔ¨Åcients ak of the Legendre polyno-
mial Pn(x) = a0 + a1x + ¬∑ ¬∑ ¬∑ + anxn. Hint: Ô¨Årst show that the orthogonality of
the Pns implies (8.12).
8.4
Use equations (8.6‚Äì8.7) to Ô¨Ånd P3(x) and P4(x).
8.5
In superscript notation (6.19), Leibniz‚Äôs rule (4.46) for derivatives of products
u v of functions is
(uv)(n) =
n

k=0
n
k

u(n‚àík) v(k).
(8.126)
Use it and Rodrigues‚Äôs formula (8.8) to derive the explicit formula (8.9).
323

LEGENDRE FUNCTIONS
8.6
The product rule for derivatives in superscript notation (6.19) is
(uv)(n) =
n

k=0
n
k

u(n‚àík) v(k).
(8.127)
Apply it to Rodrigues‚Äôs formula (8.8) with x2 ‚àí1 = (x ‚àí1)(x + 1) and show
that the Legendre polynomials satisfy Pn(1) = 1.
8.7
Use Cauchy‚Äôs integral formula (5.36) and Rodrigues‚Äôs formula (8.54) to derive
SchlaeÔ¨Çi‚Äôs integral formula (8.55).
8.8
Show that the polynomials (8.56) are orthogonal (8.57) as long as they satisfy
the endpoint condition (8.58).
8.9
Derive the orthogonality relation (8.2) from Rodrigues‚Äôs formula (8.8).
8.10 (a) Use the fact that the quantities w = x2 ‚àí1 and wn = wn vanish at the
endpoints ¬±1 to show by repeated integrations by parts that in superscript
notation (6.19)
 1
‚àí1
w(n)
n w(n)
n dx = ‚àí
 1
‚àí1
w(n‚àí1)
n
w(n+1)
n
dx = (‚àí1)n
 1
‚àí1
wnw(2n)
n
dx.
(8.128)
(b) Show that the Ô¨Ånal integral is equal to
In = (2n)!
 1
‚àí1
(1 ‚àíx)n (1 + x)n dx.
(8.129)
8.11 (a) Show by integrating by parts that In = (n!)2 22n+1/(2n + 1).
(b) Prove (8.13).
8.12 Suppose that Pn(x) and Qn(x) are two solutions of (8.28). Find an expression
for their wronskian, apart from an over-all constant.
8.13 Use the method of sections 6.23 and 6.30 and the solution f (r) = r‚Ñìto Ô¨Ånd a
second solution of the ODE (8.82).
8.14 For a uniformly charged circle of radius a, Ô¨Ånd the resulting scalar potential
œÜ(r, Œ∏) for r < a.
8.15 (a) Find the electrostatic potential V(r, Œ∏) outside an uncharged perfectly
conducting sphere of radius R in a vertical uniform static electric Ô¨Åeld that
tends to E = EÀÜz as r ‚Üí‚àû. (b) Find the potential if the free charge on the
sphere is qf.
8.16 Derive (8.125) from (8.123) and (8.124).
324

9
Bessel functions
9.1 Bessel functions of the Ô¨Årst kind
Friedrich Bessel (1784‚Äì1846) invented functions for problems with circular
symmetry. The most useful ones are deÔ¨Åned for any integer n by the series
Jn(z) =
zn
2nn!

1 ‚àí
z2
2(2n + 2) +
z4
2 ¬∑ 4(2n + 2)(2n + 4) ‚àí. . .

=
z
2
n ‚àû

m=0
(‚àí1)m
m! (m + n)!
z
2
2m
.
(9.1)
The Ô¨Årst term of this series tells us that for small |z| ‚â™1
Jn(z) ‚âà
zn
2nn!.
(9.2)
The alternating signs in (9.1) make the waves plotted in Fig. 9.1, and we have
for big |z| ‚â´1 the approximation (Courant and Hilbert, 1955, chap. VII)
Jn(z) ‚âà
&
2
œÄz cos

z ‚àínœÄ
2 ‚àíœÄ
4

+ O(|z|‚àí3/2).
(9.3)
The Jn(z) are entire transcendental functions. They obey Bessel‚Äôs equation
d2Jn
dz2 + 1
z
dJn
dz +

1 ‚àín2
z2

Jn = 0
(9.4)
(6.308) as one may show (exercise 9.1) by substituting the series (9.1) into the
differential equation (9.4). Their generating function is
exp
z
2 (u ‚àí1/u)

=
‚àû

n=‚àí‚àû
un Jn(z),
(9.5)
325

BESSEL FUNCTIONS
0
2
4
6
8
10
12
‚àí0.5
0
0.5
1
œÅ
The Bessel functions J0(œÅ), J1(œÅ), and J2(œÅ)
0
2
4
6
8
10
12
‚àí0.4
‚àí0.2
0
0.2
0.4
œÅ
The Bessel functions J3(œÅ), J4(œÅ), and J5(œÅ)
Figure 9.1
Top: plots of J0(œÅ) (solid curve), J1(œÅ) (dot-dash), and J2(œÅ) (dashed) for
real œÅ. Bottom: plots of J3(œÅ) (solid curve), J4(œÅ) (dot-dash), and J5(œÅ) (dashed). The
points at which Bessel functions cross the œÅ-axis are called zeros or roots; we use them
to satisfy boundary conditions.
from which one may derive (exercise 9.5) the series expansion (9.1) and
(exercise 9.6) the integral representation (5.46)
Jn(z) = 1
œÄ
 œÄ
0
cos(z sin Œ∏ ‚àínŒ∏) dŒ∏ = J‚àín(‚àíz) = (‚àí1)nJ‚àín(z)
(9.6)
for all complex z. For n = 0, this integral is (exercise 9.7) more simply
J0(z) = 1
2œÄ
 2œÄ
0
eiz cos Œ∏ dŒ∏ = 1
2œÄ
 2œÄ
0
eiz sin Œ∏ dŒ∏.
(9.7)
These integrals (exercise 9.8) give Jn(0) = 0 for n Ã∏= 0, and J0(0) = 1.
By differentiating the generating function (9.5) with respect to u and identi-
fying the coefÔ¨Åcients of powers of u, one Ô¨Ånds the recursion relation
Jn‚àí1(z) + Jn+1(z) = 2n
z Jn(z).
(9.8)
Similar reasoning after taking the z derivative gives (exercise 9.10)
Jn‚àí1(z) ‚àíJn+1(z) = 2 J‚Ä≤
n(z).
(9.9)
326

9.1 BESSEL FUNCTIONS OF THE FIRST KIND
By using the gamma function (section 5.12), one may extend Bessel‚Äôs
equation (9.4) and its solutions Jn(z) to nonintegral values of n
Jv(z) =
z
2
v
‚àû

m=0
(‚àí1)m
m! (m + v + 1)
z
2
2m
.
(9.10)
Letting z = a x, we arrive (exercise 9.11) at the self-adjoint form (6.307) of
Bessel‚Äôs equation
‚àíd
dx

x d
dxJn(ax)

+ n2
x Jn(ax) = a2xJn(ax).
(9.11)
In the notation of equation (6.287), p(x) = x, a2 is an eigenvalue, and œÅ(x) = x
is a weight function. To have a self-adjoint system (section 6.28) on an interval
[0, b], we need the boundary condition (6.247)
0 =

p(Jnv‚Ä≤ ‚àíJ‚Ä≤
nv)
b
0 =

x(Jnv‚Ä≤ ‚àíJ‚Ä≤
nv)
b
0
(9.12)
for all functions v(x) in the domain D of the system. Since p(x) = x, J0(0) = 1,
and Jn(0) = 0 for integers n > 0, the terms in this boundary condition vanish
at x = 0 as long as the domain consists of functions v(x) that are continuous
on the interval [0, b]. To make these terms vanish at x = b, we require that
Jn(ab) = 0 and that v(b) = 0. So ab must be a zero zn,m of Jn(z), that is Jn(ab) =
Jn(zn,m) = 0. With a = zn,m/b, Bessel‚Äôs equation (9.11) is
‚àíd
dx

x d
dxJn

zn,mx/b

+ n2
x Jn

zn,mx/b

=
z2
n,m
b2 x Jn

zn,mx/b

.
(9.13)
For Ô¨Åxed n, the eigenvalue a2 = z2
n,m/b2 is different for each positive integer m.
Moreover as m ‚Üí‚àû, the zeros zn,m of Jn(x) rise as mœÄ as one might expect
since the leading term of the asymptotic form (9.3) of Jn(x) is proportional
to cos(x ‚àínœÄ/2 ‚àíœÄ/4), which has zeros at mœÄ + (n + 1)œÄ/2 + œÄ/4. It fol-
lows that the eigenvalues a2 ‚âà(mœÄ)2/b2 increase without limit as m ‚Üí‚àûin
accordance with the general result of section 6.34. It follows then from the argu-
ment of section 6.35 and from the orthogonality relation (6.326) that for every
Ô¨Åxed n, the eigenfunctions Jn(zn,mx/b), one for each zero, are complete in the
mean, orthogonal, and normalizable on the interval [0, b] with weight function
œÅ(x) = x
 b
0
x Jn
zn,mx
b

Jn
zn,m‚Ä≤x
b

dx = Œ¥m,m‚Ä≤ b2
2 J‚Ä≤2
n (zn,m) = Œ¥m,m‚Ä≤ b2
2 J2
n+1(zn,m)
(9.14)
327

BESSEL FUNCTIONS
and a normalization constant (exercise 9.12) that depends upon the Ô¨Årst deriva-
tive of the Bessel function or the square of the next Bessel function at the
zero.
The analogous relation on an inÔ¨Ånite interval is
 ‚àû
0
x Jn(kx) Jn(k‚Ä≤x) dx = 1
k Œ¥(k ‚àík‚Ä≤).
(9.15)
One may generalize these relations (9.11‚Äì9.15) from integral n to real nonnega-
tive ŒΩ (and to ŒΩ > ‚àí1/2).
Example 9.1 (Bessel‚Äôs drum)
The top of a drum is a circular membrane with
a Ô¨Åxed circumference 2œÄrd. The membrane‚Äôs potential energy is approximately
proportional to the extra area it has when it‚Äôs not Ô¨Çat. Let h(x, y) be the dis-
placement of the membrane in the z direction normal to the x‚Äìy plane of the
Ô¨Çat membrane, and let hx and hy denote its partial derivatives (6.20). The extra
length of a line segment dx on the stretched membrane is

1 + h2x dx, and so the
extra area of an element dx dy is
dA ‚âà

1 + h2x + h2y ‚àí1

dx dy ‚âà1
2

h2
x + h2
y

dx dy.
(9.16)
The (nonrelativistic) kinetic energy of the area element is proportional to its
speed squared. So if œÉ is the surface tension and Œº the mass density of the
membrane, then to lowest order in derivatives the action functional is
S[h] =
 Œº
2 h2
t ‚àíœÉ
2

h2
x + h2
y

dx dy dt.
(9.17)
We minimize this action for hs that vanish on the boundary x2 + y2 = r2
d
0 = Œ¥S[h] =
 
Œº ht Œ¥ht ‚àíœÉ

hx Œ¥hx + hy Œ¥hy

dx dy dt.
(9.18)
Since (6.170) Œ¥ht = (Œ¥h)t, Œ¥hx = (Œ¥h)x, and Œ¥hy = (Œ¥h)y, we can integrate by parts
and get
0 = Œ¥S[h] =
 
‚àíŒº htt + œÉ

hxx + hyy

Œ¥h dx dy dt
(9.19)
apart from a surface term proportional to Œ¥h, which vanishes because Œ¥h = 0
on the circumference of the membrane. The membrane therefore obeys the wave
equation
Œº htt = œÉ

hxx + hyy

‚â°œÉ ‚ñ≥h.
(9.20)
This equation is separable, and so letting h(x, y, t) = s(t) v(x, y), we have
stt
s = œÉ
Œº
‚ñ≥v
v
= ‚àíœâ2.
(9.21)
328

9.1 BESSEL FUNCTIONS OF THE FIRST KIND
The eigenvalues of the Helmholtz equation
‚àí‚ñ≥v = Œª v give the angular
frequencies as œâ = ‚àöœÉŒª/Œº. The time dependence then is
s(t) = a sin

œÉŒª/Œº (t ‚àít0)

,
(9.22)
in which a and t0 are constants.
In polar coordinates, Helmholtz‚Äôs equation is separable (6.45‚Äì6.48)
‚àí‚ñ≥v = ‚àívrr ‚àír‚àí1vr ‚àír‚àí2vŒ∏Œ∏ = Œªv.
(9.23)
We set v(r, Œ∏) = u(r)h(Œ∏) and Ô¨Ånd ‚àíu‚Ä≤‚Ä≤h ‚àír‚àí1u‚Ä≤h ‚àír‚àí2uh‚Ä≤‚Ä≤ = Œªuh. After
multiplying both sides by r2/uh, we get
r2 u‚Ä≤‚Ä≤
u + ru‚Ä≤
u + Œªr2 = ‚àíh‚Ä≤‚Ä≤
h = n2.
(9.24)
The general solution for h then is h(Œ∏) = b sin(n(Œ∏ ‚àíŒ∏0)) in which b and Œ∏0 are
constants and n must be an integer so that h is single valued on the circumference
of the membrane.
The function u thus is an eigenfunction of the self-adjoint differential equation
(6.307) ‚àí

r u‚Ä≤‚Ä≤ + n2 u/r = Œª r u whose eigenvalues Œª ‚â°z2/r2
d are all posi-
tive. By changing variables to œÅ = zr/rd and letting u(r) = Jn(œÅ), we arrive
(exercise 6.26) at
d2Jn
dœÅ2 + 1
œÅ
dJn
dœÅ +

1 ‚àín2
œÅ2

Jn = 0,
(9.25)
which is Bessel‚Äôs equation (6.308).
The eigenvalues Œª = z2/r2
d are determined by the boundary condition u(rd) =
Jn(z) = 0. For each integer n ‚â•0, there are an inÔ¨Ånite number of zeros zn,m
at which the Bessel function vanishes, Jn(zn,m) = 0. Thus Œª = Œªn,m = z2
n,m/r2
d
and so the frequency is œâ = (zn,m/rd) ‚àöœÉ/Œº. The general solution to the wave
equation (9.20) of the membrane then is
h(r, Œ∏, t) =
‚àû

n=0
‚àû

m=1
cn,m sin
#zn,m
rd
&œÉ
Œº (t ‚àít0)
$
sin [n(Œ∏ ‚àíŒ∏0)] Jn

zn,m
r
rd

.
(9.26)
For any n, the zeros zn,m are the square-roots of the dimensionless eigenvalues
(6.309) and rise like mœÄ as m ‚Üí‚àû.
We learned in section 6.5 that in three dimensions Helmholtz‚Äôs equation
‚àí‚ñ≥V = Œ±2 V separates in cylindrical coordinates (and in several other coor-
dinate systems). That is, the function V(œÅ, œÜ, z) = B(œÅ)(œÜ)Z(z) satisÔ¨Åes the
equation
‚àí‚ñ≥V = ‚àí1
œÅ
#
œÅ V,œÅ

,œÅ + 1
œÅ V,œÜœÜ + œÅ V,zz
$
= Œ±2 V
(9.27)
329

BESSEL FUNCTIONS
if B(œÅ) obeys Bessel‚Äôs equation
œÅ d
dœÅ

œÅ dB
dœÅ

+

(Œ±2 + k2)œÅ2 ‚àín2
B = 0
(9.28)
and  and Z respectively satisfy
‚àíd2
dœÜ2 = n2(œÜ)
and
d2Z
dz2 = k2Z(z)
(9.29)
or if B(œÅ) obeys the Bessel equation
œÅ d
dœÅ

œÅ dB
dœÅ

+

(Œ±2 ‚àík2)œÅ2 ‚àín2
B = 0
(9.30)
and  and Z satisfy
‚àíd2
dœÜ2 = n2(œÜ)
and
d2Z
dz2 = ‚àík2Z(z).
(9.31)
In the Ô¨Årst case (9.28 & 9.29), the solution V is
Vk,n(œÅ, œÜ, z) = Jn

Œ±2 + k2 œÅ

e¬±inœÜe¬±kz
(9.32)
while in the second case (9.30 & 9.31), it is
Vk,n(œÅ, œÜ, z) = Jn

Œ±2 ‚àík2 œÅ

e¬±inœÜe¬±ikz.
(9.33)
In both cases, n must be an integer if the solution is to be single valued on the
full range of œÜ from 0 to 2œÄ.
When Œ± = 0, the Helmholtz equation reduces to Laplace‚Äôs equation ‚ñ≥V = 0
of electrostatics, which the simpler functions
Vk,n(œÅ, œÜ, z) = Jn(kœÅ)e¬±inœÜe¬±kz
and
Vk,n(œÅ, œÜ, z) = Jn(ikœÅ)e¬±inœÜe¬±ikz
(9.34)
satisfy.
The product i‚àíŒΩ JŒΩ(ikœÅ) is real and is known as the modiÔ¨Åed Bessel function
IŒΩ(kœÅ) ‚â°i‚àíŒΩ JŒΩ(ikœÅ).
(9.35)
It occurs in various solutions of the diffusion equation ‚ñ≥V = Œ±2V. The
function V(œÅ, œÜ, z) = B(œÅ)(œÜ)Z(z) satisÔ¨Åes
‚ñ≥V = 1
œÅ
#
œÅ V,œÅ

,œÅ + 1
œÅ V,œÜœÜ + œÅ V,zz
$
= Œ±2 V
(9.36)
330

9.1 BESSEL FUNCTIONS OF THE FIRST KIND
if B(œÅ) obeys Bessel‚Äôs equation
œÅ d
dœÅ

œÅ dB
dœÅ

‚àí

(Œ±2 ‚àík2)œÅ2 + n2
B = 0
(9.37)
and  and Z respectively satisfy
‚àíd2
dœÜ2 = n2(œÜ)
and
d2Z
dz2 = k2Z(z)
(9.38)
or if B(œÅ) obeys the Bessel equation
œÅ d
dœÅ

œÅ dB
dœÅ

‚àí

(Œ±2 + k2)œÅ2 + n2
B = 0
(9.39)
and  and Z satisfy
‚àíd2
dœÜ2 = n2(œÜ)
and
d2Z
dz2 = ‚àík2Z(z).
(9.40)
In the Ô¨Årst case (9.37 & 9.38), the solution V is
Vk,n(œÅ, œÜ, z) = In

Œ±2 ‚àík2 œÅ

e¬±inœÜe¬±kz
(9.41)
while in the second case (9.39 & 9.40), it is
Vk,n(œÅ, œÜ, z) = In

Œ±2 + k2 œÅ

e¬±inœÜe¬±ikz.
(9.42)
In both cases, n must be an integer if the solution is to be single valued on the
full range of œÜ from 0 to 2œÄ.
Example 9.2 (Charge near a membrane)
We will use œÅ to denote the density of
free charges ‚Äì those that are free to move into or out of a dielectric medium, as
opposed to those that are part of the medium, bound in it by molecular forces.
The time-independent Maxwell equations are Gauss‚Äôs law ‚àá¬∑ D = œÅ for the
divergence of the electric displacement D, and the static form ‚àá√ó E = 0 of Fara-
day‚Äôs law, which implies that the electric Ô¨Åeld E is the gradient of an electrostatic
potential E = ‚àí‚àáV.
Across an interface between two dielectrics with normal vector ÀÜn, the tangen-
tial electric Ô¨Åeld is continuous, ÀÜn √ó E2 = ÀÜn √ó E1, while the normal component
of the electric displacement jumps by the surface density œÉ of free charge,
ÀÜn¬∑(D2 ‚àíD1) = œÉ. In a linear dielectric, the electric displacement D is the electric
Ô¨Åeld multiplied by the permittivity œµ of the material, D = œµ E.
The membrane of a eukaryotic cell is a phospholipid bilayer whose area is
some 3 √ó 108 nm2, and whose thickness t is about 5 nm. On a scale of nanome-
ters, the membrane is Ô¨Çat. We will take it to be a slab extending to inÔ¨Ånity in the
x and y directions. If the interface between the lipid bilayer and the extracellular
salty water is at z = 0, then the cytosol extends thousands of nm down from
331

BESSEL FUNCTIONS
z = ‚àít = ‚àí5 nm. We will ignore the phosphate head groups and set the permit-
tivity œµ‚Ñìof the lipid bilayer to twice that of the vacuum œµ‚Ñì‚âà2œµ0; the permittivity
of the extracellular water and that of the cytosol are œµw ‚âàœµc ‚âà80œµ0.
We will compute the electrostatic potential V due to a charge q at a point
(0, 0, h) on the z-axis above the membrane. This potential is cylindrically sym-
metric about the z-axis, so V = V(œÅ, z). The functions Jn(kœÅ) einœÜ e¬±kz form a
complete set of solutions of Laplace‚Äôs equation but, due to the symmetry, we
only need the n = 0 functions J0(kœÅ) e¬±kz. Since there are no free charges in the
lipid bilayer or in the cytosol, we may express the potential in the lipid bilayer
V‚Ñìand in the cytosol Vc as
V‚Ñì(œÅ, z) =
 ‚àû
0
dk J0(kœÅ)

m(k) ekz + f (k) e‚àíkz
,
Vc(œÅ, z) =
 ‚àû
0
dk J0(kœÅ) d(k) ekz.
(9.43)
The Green‚Äôs function (3.110) for Poisson‚Äôs equation ‚àí‚ñ≥G(x) = Œ¥(3)(x) in
cylindrical coordinates is (5.139)
G(x) =
1
4œÄ|x| =
1
4œÄ

œÅ2 + z2 =
 ‚àû
0
dk
4œÄ J0(kœÅ) e‚àík|z|.
(9.44)
Thus we may expand the potential in the salty water as
Vw(œÅ, z) =
 ‚àû
0
dk J0(kœÅ)
#
q
4œÄœµw
e‚àík|z‚àíh| + u(k) e‚àíkz
$
.
(9.45)
Using ÀÜn √ó E2 = ÀÜn √ó E1 and ÀÜn ¬∑ (D2 ‚àíD1) = œÉ, suppressing k, and setting
Œ≤ ‚â°qe‚àíkh/4œÄœµw and y = e2kt, we get four equations
m + f ‚àíu = Œ≤
and
œµ‚Ñìm ‚àíœµ‚Ñìf + œµwu = œµwŒ≤,
œµ‚Ñìm ‚àíœµ‚Ñìyf ‚àíœµcd = 0
and
m + yf ‚àíd = 0.
(9.46)
In terms of the abbreviations œµw‚Ñì= (œµw + œµ‚Ñì) /2 and œµc‚Ñì= (œµc + œµ‚Ñì) /2 as well as
p = (œµw ‚àíœµ‚Ñì)/(œµw + œµ‚Ñì) and p‚Ä≤ = (œµc ‚àíœµ‚Ñì)/(œµc + œµ‚Ñì), the solutions are
u(k) = Œ≤ p ‚àíp‚Ä≤/y
1 ‚àípp‚Ä≤/y
and
m(k) = Œ≤ œµw
œµw‚Ñì
1
1 ‚àípp‚Ä≤/y,
f (k) = ‚àíŒ≤ œµw
œµw‚Ñì
p‚Ä≤/y
1 ‚àípp‚Ä≤/y
and
d(k) = Œ≤ œµwœµ‚Ñì
œµw‚Ñìœµc‚Ñì
1
1 ‚àípp‚Ä≤/y.
(9.47)
Inserting these solutions into the Bessel expansions (9.43) for the potentials,
expanding their denominators
1
1 ‚àípp‚Ä≤/y =
‚àû

0
(pp‚Ä≤)n e‚àí2nkt,
(9.48)
and using the integral (9.44), we Ô¨Ånd that the potential Vw in the extracellular
water of a charge q at (0, 0, h) in the water is
332

9.1 BESSEL FUNCTIONS OF THE FIRST KIND
Vw(œÅ, z) =
q
4œÄœµw

1
r +
p

œÅ2 + (z + h)2 ‚àí
‚àû

n=1
p‚Ä≤ 
1 ‚àíp2
(pp‚Ä≤)n‚àí1

œÅ2 + (z + 2nt + h)2

, (9.49)
in which r =

œÅ2 + (z ‚àíh)2 is the distance to the charge q. The principal image
charge pq is at (0, 0, ‚àíh). Similarly, the potential V‚Ñìin the lipid bilayer is
V‚Ñì(œÅ, z) =
q
4œÄœµw‚Ñì
‚àû

n=0

(pp‚Ä≤)n

œÅ2 + (z ‚àí2nt ‚àíh)2 ‚àí
pnp‚Ä≤n+1

œÅ2 + (z + 2(n + 1)t + h)2

(9.50)
and that in the cytosol is
Vc(œÅ, z) =
q œµ‚Ñì
4œÄœµw‚Ñìœµc‚Ñì
‚àû

n=0
(pp‚Ä≤)n

œÅ2 + (z ‚àí2nt ‚àíh)2 .
(9.51)
These potentials are the same as those of example 4.16, but this derivation is
much simpler and less error prone than the method of images.
Since p = (œµw‚àíœµ‚Ñì)/(œµw+œµ‚Ñì) > 0, the principal image charge pq at (0, 0, ‚àíh) has
the same sign as the charge q and so contributes a positive term proportional to
pq2 to the energy. So a lipid membrane repels a nearby charge in water no matter
what the sign of the charge. A cell membrane is a phospholipid bilayer. The lipids
avoid water and form a 4-nm-thick layer that lies between two 0.5-nm layers of
phosphate groups which are electric dipoles. These electric dipoles cause the cell
membrane to weakly attract ions that are within 0.5 nm of the membrane.
Example 9.3 (Cylindrical wave-guides)
An electromagnetic wave traveling in
the z-direction down a cylindrical wave-guide looks like
E ei(kz‚àíœât)
and
B ei(kz‚àíœât),
(9.52)
in which E and B depend upon œÅ and œÜ
E = EœÅ ÀÜœÅ + EœÜ ÀÜœÜ + EzÀÜz
and
B = BœÅ ÀÜœÅ + BœÜ ÀÜœÜ + BzÀÜz
(9.53)
in cylindrical coordinates (11.164‚Äì11.169 & 11.241). If the wave-guide is an
evacuated, perfectly conducting cylinder of radius r, then on the surface of the
wave-guide the parallel components of E and the normal component of B must
vanish, which leads to the boundary conditions
Ez(r, œÜ) = 0,
EœÜ(r, œÜ) = 0,
and
BœÅ(r, œÜ) = 0.
(9.54)
Since the E and B Ô¨Åelds have subscripts, we will use commas to denote deriva-
tives as in ‚àÇEz/‚àÇœÜ ‚â°Ez,œÜ and ‚àÇ(œÅEœÜ)/‚àÇœÅ ‚â°(œÅEœÜ),œÅ and so forth. In this
notation, the vacuum forms ‚àá√ó E = ‚àíÀôB and ‚àá√ó B = ÀôE/c2 of the Faraday
and Maxwell‚ÄìAmp√®re laws give us (exercise 9.14) the Ô¨Åeld equations
333

BESSEL FUNCTIONS
Ez,œÜ/œÅ ‚àíikEœÜ = iœâBœÅ,
Bz,œÜ/œÅ ‚àíikBœÜ = ‚àíiœâEœÅ/c2,
ikEœÅ ‚àíEz,œÅ = iœâBœÜ,
ikBœÅ ‚àíBz,œÅ = ‚àíiœâEœÜ/c2,

(œÅEœÜ),œÅ ‚àíEœÅ,œÜ

/œÅ = iœâBz,

(œÅBœÜ),œÅ ‚àíBœÅ,œÜ

/œÅ = ‚àíiœâEz/c2.
(9.55)
Solving them for the œÅ and œÜ components of E and B in terms of their z
components (exercise 9.15), we Ô¨Ånd
EœÅ = ‚àíikEz,œÅ + œâBz,œÜ/œÅ
k2 ‚àíœâ2/c2
,
EœÜ = ‚àíikEz,œÜ/œÅ ‚àíœâBz,œÅ
k2 ‚àíœâ2/c2
,
BœÅ = ‚àíikBz,œÅ ‚àíœâEz,œÜ/c2œÅ
k2 ‚àíœâ2/c2
,
BœÜ = ‚àíikBz,œÜ/œÅ + œâEz,œÅ/c2
k2 ‚àíœâ2/c2
. (9.56)
The Ô¨Åelds Ez and Bz obey the separable wave equations (11.91), exercise 6.6,
‚àí‚ñ≥Ez = ‚àí¬®Ez/c2 = œâ2Ez/c2
and
‚àí‚ñ≥Bz = ‚àí¬®Bz/c2 = œâ2Bz/c2.
(9.57)
Because their z-dependence (9.52) is periodic, they are (exercise 9.16) linear
combinations of Jn(

œâ2/c2 ‚àík2 œÅ)einœÜei(kz‚àíœât).
Modes with Bz = 0 are transverse magnetic or TM modes. For them the
boundary conditions (9.54) will be satisÔ¨Åed if

œâ2/c2 ‚àík2 r is a zero zn,m of
Jn. So the frequency œân,m(k) of the n, m TM mode is
œân,m(k) = c

k2 + z2n,m/r2.
(9.58)
Since the Ô¨Årst zero of a Bessel function is z0,1 ‚âà2.4048, the minimum frequency
œâ0,1(0) = c z0,1/r ‚âà2.4048 c/r occurs for n = 0 and k = 0. If the radius of the
wave-guide is r = 1 cm, then œâ0,1(0)/2œÄ is about 11 GHz, which is a microwave
frequency with a wave-length of 2.6 cm. In terms of the frequencies (9.58), the
Ô¨Åeld of a pulse moving in the +z-direction is
Ez(œÅ, œÜ, z, t) =
‚àû

n=0
‚àû

m=1
 ‚àû
0
cn,m(k) Jn
zn,m œÅ
r

einœÜ exp i

kz ‚àíœân,m(k)t

dk.
(9.59)
Modes with Ez = 0 are transverse electric or TE modes. For them the bound-
ary conditions (9.54) will be satisÔ¨Åed (exercise 9.18) if

œâ2/c2 ‚àík2 r is a zero
z‚Ä≤
n,m of J‚Ä≤
n. Their frequencies are œân,m(k) = c

k2 + z‚Ä≤2n,m/r2. Since the Ô¨Årst zero
of a Ô¨Årst derivative of a Bessel function is z‚Ä≤
1,1 ‚âà1.8412, the minimum frequency
œâ1,1(0) = c z‚Ä≤
1,1/r ‚âà1.8412 c/r occurs for n = 1 and k = 0. If the radius of the
wave-guide is r = 1 cm, then œâ1,1(0)/2œÄ is about 8.8 GHz, which is a microwave
frequency with a wave-length of 3.4 cm.
Example 9.4 (Cylindrical cavity)
The modes of an evacuated, perfectly con-
ducting cylindrical cavity of radius r and height h are like those of a cylindrical
wave-guide (example 9.3) but with extra boundary conditions
334

9.2 SPHERICAL BESSEL FUNCTIONS OF THE FIRST KIND
Bz(œÅ, œÜ, 0, t) = 0
and
Bz(œÅ, œÜ, h, t) = 0,
EœÅ(œÅ, œÜ, 0, t) = 0
and
EœÅ(œÅ, œÜ, h, t) = 0,
EœÜ(œÅ, œÜ, 0, t) = 0
and
EœÜ(œÅ, œÜ, h, t) = 0
(9.60)
at the two ends of the cylinder. If ‚Ñìis an integer and if

œâ2/c2 ‚àíœÄ2‚Ñì2/h2 r is a
zero z‚Ä≤
n,m of J‚Ä≤
n, then the TE Ô¨Åelds Ez = 0 and
Bz = Jn(z‚Ä≤
n,m œÅ/r) einœÜ sin(œÄ‚Ñìz/h) e‚àíiœât
(9.61)
satisfy both these (9.60) boundary conditions at z = 0 and h and those (9.54)
at œÅ = r as well as the separable wave equations (9.57). The frequencies of the
resonant TE modes then are œân,m,‚Ñì= c

z‚Ä≤2n,m/r2 + œÄ2‚Ñì2/h2.
The TM modes are Bz = 0 and
Ez = Jn(zn,m œÅ/r) einœÜ cos(œÄ‚Ñìz/h) e‚àíiœât
(9.62)
with resonant frequencies œân,m,‚Ñì= c

z2n,m/r2 + œÄ2‚Ñì2/h2.
9.2 Spherical Bessel functions of the Ô¨Årst kind
If in Bessel‚Äôs equation (9.4), one sets n = ‚Ñì+ 1/2 and j‚Ñì= ‚àöœÄ/2x J‚Ñì+1/2, then
one may show (exercise 9.21) that
x2 j‚Ä≤‚Ä≤
‚Ñì(x) + 2x j‚Ä≤
‚Ñì(x) + [x2 ‚àí‚Ñì(‚Ñì+ 1)] j‚Ñì(x) = 0,
(9.63)
which is the equation for the spherical Bessel function j‚Ñì.
We saw in example 6.6 that by setting V(r, Œ∏, œÜ) = Rk,‚Ñì(r) ‚Ñì,m(Œ∏) m(œÜ) we
could separate the variables of Helmholtz‚Äôs equation ‚àí‚ñ≥V = k2V in spherical
coordinates
r2‚ñ≥V
V
=
(r2R‚Ä≤
k,‚Ñì)‚Ä≤
Rk,‚Ñì
+
(sin Œ∏ ‚Ä≤
‚Ñì,m)‚Ä≤
sin Œ∏ ‚Ñì,m
+
‚Ä≤‚Ä≤
sin2 Œ∏ 
= ‚àík2r2.
(9.64)
Thus if m(œÜ) = eimœÜ so that ‚Ä≤‚Ä≤
m = ‚àím2m, and if ‚Ñì,m satisÔ¨Åes the associated
Legendre equation (8.91)
sin Œ∏

sin Œ∏ ‚Ä≤
‚Ñì,m
‚Ä≤ + [‚Ñì(‚Ñì+ 1) sin2 Œ∏ ‚àím2] ‚Ñì,m = 0
(9.65)
then the product V(r, Œ∏, œÜ) = Rk,‚Ñì(r) ‚Ñì,m(Œ∏) m(œÜ) will obey (9.64) because in
view of (9.63) the radial function Rk,‚Ñì(r) = j‚Ñì(kr) satisÔ¨Åes
(r2R‚Ä≤
k,‚Ñì)‚Ä≤ + [k2r2 ‚àí‚Ñì(‚Ñì+ 1)]Rk,‚Ñì= 0.
(9.66)
In terms of the spherical harmonic Y‚Ñì,m(Œ∏, œÜ) = ‚Ñì,m(Œ∏) m(œÜ), the solution is
V(r, Œ∏, œÜ) = j‚Ñì(kr) Y‚Ñì,m(Œ∏, œÜ).
335

BESSEL FUNCTIONS
Rayleigh‚Äôs formula gives the spherical Bessel function
j‚Ñì(x) ‚â°
& œÄ
2x J‚Ñì+1/2(x)
(9.67)
as the ‚Ñìth derivative of sin x/x
j‚Ñì(x) = (‚àí1)‚Ñìx‚Ñì
1
x
d
dx
‚Ñìsin x
x

(9.68)
(Lord Rayleigh (John William Strutt), 1842‚Äì1919). In particular, j0(x) = sin x/x
and j1(x) = sin x/x2‚àícos x/x. Rayleigh‚Äôs formula leads to the recursion relation
(exercise 9.22)
j‚Ñì+1(x) = ‚Ñì
xj‚Ñì(x) ‚àíj ‚Ä≤
‚Ñì(x),
(9.69)
with which one can show (exercise 9.23) that the spherical Bessel functions as
deÔ¨Åned by Rayleigh‚Äôs formula do satisfy their differential equation (9.66) with
x = kr.
The spherical Bessel functions j‚Ñì(kr) satisfy the self-adjoint Sturm‚ÄìLiouville
(6.333) equation (9.66)
‚àír2j‚Ä≤‚Ä≤
‚Ñì‚àí2rj‚Ä≤
‚Ñì+ ‚Ñì(‚Ñì+ 1)j‚Ñì= k2r2j‚Ñì
(9.70)
with eigenvalue k2 and weight function œÅ = r2. If j‚Ñì(z‚Ñì,n) = 0, then the functions
j‚Ñì(kr) = j‚Ñì(z‚Ñì,nr/a) vanish at r = a and form an orthogonal basis
 a
0
j‚Ñì(z‚Ñì,nr/a) j‚Ñì(z‚Ñì,mr/a) r2 dr = a3
2 j2
‚Ñì+1(z‚Ñì,n) Œ¥n,m
(9.71)
for a self-adjoint system on the interval [0, a]. Moreover, since the eigenvalues
k2
‚Ñì,n = z2
‚Ñì,n/a2 ‚âà(nœÄ)2/a2 ‚Üí‚àûas n ‚Üí‚àû, the eigenfunctions j‚Ñì(z‚Ñì,nr/a) also
are complete in the mean (section 6.35).
On an inÔ¨Ånite interval, the analogous relation is
 ‚àû
0
j‚Ñì(kr) j‚Ñì(k‚Ä≤r) r2 dr = œÄ
2k2 Œ¥(k ‚àík‚Ä≤).
(9.72)
If we write the spherical Bessel function j0(x) as the integral
j0(z) = sin z
z
= 1
2
 1
‚àí1
eizx dx
(9.73)
336

9.2 SPHERICAL BESSEL FUNCTIONS OF THE FIRST KIND
and use Rayleigh‚Äôs formula (9.68), we may Ô¨Ånd an integral for j‚Ñì(z)
j‚Ñì(z) = (‚àí1)‚Ñìz‚Ñì
1
z
d
dz
‚Ñìsin z
z

= (‚àí1)‚Ñìz‚Ñì
1
z
d
dz
‚Ñì1
2
 1
‚àí1
eizx dx
= z‚Ñì
2
 1
‚àí1
(1 ‚àíx2)‚Ñì
2‚Ñì‚Ñì!
eizx dx = (‚àíi)‚Ñì
2
 1
‚àí1
(1 ‚àíx2)‚Ñì
2‚Ñì‚Ñì!
d‚Ñì
dx‚Ñìeizx dx
= (‚àíi)‚Ñì
2
 1
‚àí1
eizx d‚Ñì
dx‚Ñì
(x2 ‚àí1)‚Ñì
2‚Ñì‚Ñì!
dx = (‚àíi)‚Ñì
2
 1
‚àí1
P‚Ñì(x) eizx dx
(9.74)
(exercise 9.24) that contains Rodrigues‚Äôs formula (8.8) for the Legendre polyno-
mial P‚Ñì(x). With z = kr and x = cos Œ∏, this formula
i‚Ñìj‚Ñì(kr) = 1
2
 1
‚àí1
P‚Ñì(cos Œ∏)eikr cos Œ∏ d cos Œ∏
(9.75)
and the Fourier‚ÄìLegendre expansion (8.32) give
eikr cos Œ∏ =
‚àû

‚Ñì=0
2‚Ñì+ 1
2
P‚Ñì(cos Œ∏)
 1
‚àí1
P‚Ñì(cos Œ∏‚Ä≤) eikr cos Œ∏‚Ä≤ d cos Œ∏‚Ä≤
=
‚àû

‚Ñì=0
(2‚Ñì+ 1) P‚Ñì(cos Œ∏) i‚Ñìj‚Ñì(kr).
(9.76)
If Œ∏, œÜ and Œ∏‚Ä≤, œÜ‚Ä≤ are the polar angles of the vectors r and k, then by using the
addition theorem (8.121) we get
eik¬∑r =
‚àû

‚Ñì=0
4œÄ i‚Ñìj‚Ñì(kr) Y‚Ñì,m(Œ∏, œÜ) Y‚àó
‚Ñì,m(Œ∏‚Ä≤, œÜ‚Ä≤).
(9.77)
The series expansion (9.1) for Jn and the deÔ¨Ånition (9.67) of j‚Ñìgive us for
small |œÅ| ‚â™1 the approximation
j‚Ñì(œÅ) ‚âà‚Ñì! (2œÅ)‚Ñì
(2‚Ñì+ 1)! =
œÅ‚Ñì
(2‚Ñì+ 1)!!.
(9.78)
To see how j‚Ñì(œÅ) behaves for large |œÅ| ‚â´1, we use Rayleigh‚Äôs formula (9.68)
to compute j1(œÅ) and notice that the derivative d/dœÅ
j1(œÅ) = ‚àíd
dœÅ
sin œÅ
œÅ

= ‚àícos œÅ
œÅ
+ sin œÅ
œÅ2
(9.79)
adds a factor of 1/œÅ when it acts on 1/œÅ but not when it acts on sin œÅ. Thus the
dominant term is the one in which all the derivatives act on the sine, and so for
large |œÅ| ‚â´1, we have approximately
337

BESSEL FUNCTIONS
0
2
4
6
8
10
12
‚àí0.5
0
0.5
œÅ
j1(œÅ)
0
2
4
6
8
10
12
‚àí0.5
0
0.5
œÅ
j2(œÅ)
The spherical Bessel function j2(œÅ) for œÅ
1 and œÅ
1
The spherical Bessel function j1(œÅ) for œÅ
1 and œÅ
1
Figure 9.2
Top: plot of j1(œÅ) (solid curve) and its approximations œÅ/3 for small œÅ
(9.78, dashes) and sin(œÅ ‚àíœÄ/2)/œÅ for big œÅ (9.80, dot-dash). Bottom: plot of j2(œÅ)
(solid curve) and its approximations œÅ2/15 for small œÅ (9.78, dashed) and sin(œÅ ‚àíœÄ)/œÅ
for big œÅ (9.80, dot-dash). The values of œÅ at which j‚Ñì(œÅ) = 0 are the zeros or roots of
j‚Ñì; we use them to Ô¨Åt boundary conditions.
j‚Ñì(œÅ) ‚âà(‚àí1)‚Ñì1
œÅ
d‚Ñìsin œÅ
dœÅ‚Ñì
= 1
œÅ sin

œÅ ‚àí‚ÑìœÄ
2

(9.80)
with an error that falls off as 1/œÅ2. The quality of the approximation, which is
exact for ‚Ñì= 0, is illustrated for ‚Ñì= 1 and 2 in Fig. 9.2.
Example 9.5 (Partial waves)
Spherical Bessel functions occur in the wave-
functions of free particles with well-deÔ¨Åned angular momentum.
The hamiltonian H0 = p2/2m for a free particle of mass m and the square L2
of the orbital angular momentum operator are both invariant under rotations;
thus they commute with the orbital angular-momentum operator L. Since the
operators H0, L2, and Lz commute with each other, simultaneous eigenstates
|k, ‚Ñì, m‚ü©of these compatible operators (section 1.30) exist
H0 |k, ‚Ñì, m‚ü©= p2
2m |k, ‚Ñì, m‚ü©=
(¬Øh k)2
2m
|k, ‚Ñì, m‚ü©,
L2 |k, ‚Ñì, m‚ü©= ¬Øh2 ‚Ñì(‚Ñì+ 1) |k, ‚Ñì, m‚ü©,
and
Lz |k, ‚Ñì, m‚ü©= ¬Øh m |k, ‚Ñì, m‚ü©.
(9.81)
338

9.2 SPHERICAL BESSEL FUNCTIONS OF THE FIRST KIND
Their wave-functions are products of spherical Bessel functions and spherical
harmonics (8.110)
‚ü®r|k, ‚Ñì, m‚ü©= ‚ü®r, Œ∏, œÜ|k, ‚Ñì, m‚ü©=
&
2
œÄ k j‚Ñì(kr) Y‚Ñì,m(Œ∏, œÜ).
(9.82)
They satisfy the normalization condition
‚ü®k, ‚Ñì, m|k‚Ä≤, ‚Ñì‚Ä≤, m‚Ä≤‚ü©= 2kk‚Ä≤
œÄ
 ‚àû
0
j‚Ñì(kr)j‚Ñì(k‚Ä≤r) r2 dr

Ym‚àó
‚Ñì(Œ∏, œÜ)Ym‚Ä≤
‚Ñì‚Ä≤ (Œ∏, œÜ) d
= Œ¥(k ‚àík‚Ä≤) Œ¥‚Ñì,‚Ñì‚Ä≤ Œ¥m,m‚Ä≤
(9.83)
and the completeness relation
1 =
 ‚àû
0
dk
‚àû

‚Ñì=0
‚Ñì

m=‚àí‚Ñì
|k, ‚Ñì, m‚ü©‚ü®k, ‚Ñì, m|.
(9.84)
Their inner products with an eigenstate |k‚Ä≤‚ü©of a free particle of momentum p‚Ä≤ =
¬Øhk‚Ä≤ are
‚ü®k, ‚Ñì, m|k‚Ä≤‚ü©= i‚Ñì
k Œ¥(k ‚àík‚Ä≤) Ym‚àó
‚Ñì(Œ∏‚Ä≤, œÜ‚Ä≤)
(9.85)
in which the polar coordinates of k‚Ä≤ are Œ∏‚Ä≤, œÜ‚Ä≤.
Using the resolution (9.84) of the identity operator and the inner-product
formulas (9.82 & 9.85), we recover the expansion (9.77)
eik‚Ä≤¬∑r
(2œÄ)3/2 = ‚ü®r|k‚Ä≤‚ü©=
 ‚àû
0
dk
‚àû

‚Ñì=0
‚Ñì

m=‚àí‚Ñì
‚ü®r|k, ‚Ñì, m‚ü©‚ü®k, ‚Ñì, m|k‚Ä≤‚ü©
=
‚àû

‚Ñì=0
&
2
œÄ i‚Ñìj‚Ñì(kr) Ym
‚Ñì(Œ∏, œÜ) Ym‚àó
‚Ñì(Œ∏‚Ä≤, œÜ‚Ä≤).
(9.86)
The small kr approximation (9.78) and the deÔ¨Ånition (9.82) tell us that the
probability that a particle with angular momentum ¬Øh‚Ñìabout the origin has r =
|r| ‚â™1/k is
P(r) = 2k2
œÄ
 r
0
j2
‚Ñì(kr)r2dr ‚âà
2
œÄ((2‚Ñì+ 1)!!)2
 r
0
(kr)2‚Ñì+2dr = (4‚Ñì+ 6)(kr)2‚Ñì+3
œÄ((2‚Ñì+ 3)!!)2k ,
(9.87)
which is very small for big ‚Ñìand tiny k. So a short-range potential can only affect
partial waves of low angular momentum. When physicists found that nuclei scat-
tered low-energy hadrons into s-waves, they knew that the range of the nuclear
force was short, about 10‚àí15m.
If the potential V(r) that scatters a particle is of short range, then at big r the
radial wave-function u‚Ñì(r) of the scattered wave should look like that of a free
particle (9.86), which by the big kr approximation (9.80) is
339

BESSEL FUNCTIONS
u(0)
‚Ñì(r) = j‚Ñì(kr) ‚âàsin(kr ‚àí‚ÑìœÄ/2)
kr
=
1
2ikr

ei(kr‚àí‚ÑìœÄ/2) ‚àíe‚àíi(kr‚àí‚ÑìœÄ/2)
.
(9.88)
Thus at big r the radial wave-function u‚Ñì(r) differs from u(0)
‚Ñì(r) only by a phase
shift Œ¥‚Ñì
u‚Ñì(r) ‚âàsin(kr ‚àí‚ÑìœÄ/2 + Œ¥‚Ñì)
kr
=
1
2ikr

ei(kr‚àí‚ÑìœÄ/2+Œ¥‚Ñì) ‚àíe‚àíi(kr‚àí‚ÑìœÄ/2+Œ¥‚Ñì)
.
(9.89)
The phase shifts determine the cross-section œÉ to be (Cohen-Tannoudji et al.,
1977, chap. VIII)
œÉ = 4œÄ
k2
‚àû

‚Ñì=0
(2‚Ñì+ 1) sin2 Œ¥‚Ñì.
(9.90)
If the potential V(r) is negligible for r > r0, then for momenta k ‚â™1/r0 the
cross-section is œÉ ‚âà4œÄ sin2 Œ¥0/k2.
Example 9.6 (Quantum dots)
The active region of some quantum dots is a
CdSe sphere whose radius a is less than 2 nm. Photons from a laser excite
electron‚Äìhole pairs, which Ô¨Çuoresce in nanoseconds.
I will model a quantum dot simply as an electron trapped in a sphere of radius
a. Its wave-function œà(r, Œ∏, œÜ) satisÔ¨Åes Schr√∂dinger‚Äôs equation
‚àí¬Øh2
2m ‚ñ≥œà = Eœà
(9.91)
with the boundary condition œà(a, Œ∏, œÜ) = 0. With k2 = 2mE/¬Øh2 = z2
‚Ñì,n/a2, the
unnormalized eigenfunctions are
œàn,‚Ñì,m(r, Œ∏, œÜ) = j‚Ñì(z‚Ñì,nr/a) Y‚Ñì,m(Œ∏, œÜ) Œ∏(a ‚àír),
(9.92)
in which the Heaviside function Œ∏(a ‚àír) makes œà vanish for r > a, and ‚Ñìand m
are integers with ‚àí‚Ñì‚â§m ‚â§‚Ñìbecause œà must be single valued for all angles Œ∏
and œÜ.
The zeros z‚Ñì,n of j‚Ñì(x) Ô¨Åx the energy levels as En,‚Ñì,m = (¬Øhz‚Ñì,n/a)2/2m. Since
z0,n = nœÄ, the ‚Ñì= 0 levels are En,0,0 = (¬ØhnœÄ/a)2/2m. If the coupling to a photon
is via a term like p ¬∑ A, then one expects ‚Ñì= 1. The energy gap from the n, ‚Ñì= 1
state to the n = 1, ‚Ñì= 0 ground state thus is
En = En,1,0 ‚àíE1,0,0 = (z2
1,n ‚àíœÄ2)
¬Øh2
2ma2 .
(9.93)
Inserting factors of c2 and using ¬Øhc = 197 eV nm, and mc2 = 0.511 MeV, we
Ô¨Ånd from the zero z1,2 = 7.72525 that E2 = 1.89 (nm/a)2 eV, which is red
light if a = 1 nm. The next zero z1,3 = 10.90412 gives E3 = 4.14 (nm/a)2
eV, which is in the visible if 1.2 < a < 1.5 nm. The Mathematica command
Do[Print[N[BesselJZero[1.5, k]]], {k, 1, 5, 1}] gives the Ô¨Årst Ô¨Åve zeros of j1(x) to
six signiÔ¨Åcant Ô¨Ågures.
340

9.3 BESSEL FUNCTIONS OF THE SECOND KIND
9.3 Bessel functions of the second kind
In section 7.5 we derived integral representations (7.55 & 7.56) for the Han-
kel functions H(1)
Œª (z) and H(2)
Œª (z) for Re z > 0. One may analytically continue
them (Courant and Hilbert, 1955, chap. VII) to the upper
H(1)
Œª (z) = 1
œÄi e‚àíiŒª/2
 ‚àû
‚àí‚àû
eiz cosh x‚àíŒªx dx,
Imz ‚â•0
(9.94)
and lower
H(2)
Œª (z) = ‚àí1
œÄi e+iŒª/2
 ‚àû
‚àí‚àû
e‚àíiz cosh x‚àíŒªx dx,
Imz ‚â§0
(9.95)
half z-planes. When both z = œÅ and Œª = ŒΩ are real, the two Hankel functions
are complex conjugates of each other
H(1)
ŒΩ (œÅ) = H(2)‚àó
ŒΩ
(œÅ).
(9.96)
Hankel functions, called Bessel functions of the third kind, are linear combina-
tions of Bessel functions of the Ô¨Årst JŒª(z) and second YŒª(z) kind
H(1)
Œª (z) = JŒª(z) + iYŒª(z),
H(2)
Œª (z) = JŒª(z) ‚àíiYŒª(z).
(9.97)
Bessel functions of the second kind are also called Neumann functions; the sym-
bols YŒª(z) = NŒª(z) refer to the same function. They are inÔ¨Ånite at z = 0 as
illustrated in Fig. 9.3.
When z = ix is imaginary, we get the modiÔ¨Åed Bessel functions
IŒ±(x) = i‚àíŒ±JŒ±(ix) =
‚àû

m=0
1
m! (m + Œ± + 1)
x
2
2m+Œ±
,
KŒ±(x) = œÄ
2 iŒ±+1H(1)
Œ± (ix) =
 ‚àû
0
e‚àíx cosh t cosh Œ±t dt.
(9.98)
Some simple cases are
I‚àí1/2(z) =
&
2
œÄz cosh z, I1/2(z) =
&
2
œÄz sinh z, and K1/2(z) =
& œÄ
2ze‚àíz.
(9.99)
When do we need to use these functions? If we are representing functions
that are Ô¨Ånite at the origin œÅ = 0, then we don‚Äôt need them. But if the point
341

BESSEL FUNCTIONS
0
2
4
6
8
10
12
‚àí1
‚àí0.5
0
0.5
œÅ
The Bessel functions of the second kind Y0(œÅ), Y1(œÅ), Y2(œÅ)
2
4
6
8
10
12
14
‚àí1
‚àí0.5
0
0.5
œÅ
The Bessel functions of the second kind Y3(œÅ), Y4(œÅ), Y5(œÅ)
Figure 9.3
Top: Y0(œÅ) (solid curve), Y1(œÅ) (dot-dash), and Y2(œÅ) (dashed) for 0 <
œÅ < 12. Bottom: Y3(œÅ) (solid curve), Y4(œÅ) (dot-dash), and Y5(œÅ) (dashed) for 2 <
œÅ < 14. The points at which Bessel functions cross the œÅ-axis are called zeros or roots;
we use them to satisfy boundary conditions.
œÅ = 0 lies outside the region of interest or if the function we are representing is
inÔ¨Ånite at that point, then we do need the YŒΩ(œÅ)s.
Example 9.7 (Coaxial wave-guides)
An ideal coaxial wave-guide is perfectly
conducting for œÅ < r0 and œÅ > r, and the waves occupy the region r0 < œÅ < r.
Since points with œÅ = 0 are not in the physical domain of the problem, the
electric Ô¨Åeld E(œÅ, œÜ) exp(i(kz ‚àíœât)) is a linear combination of Bessel functions
of the Ô¨Årst and second kinds with
Ez(œÅ, œÜ) =
#
a Jn(

œâ2/c2 ‚àík2 œÅ) + b Yn(

œâ2/c2 ‚àík2 œÅ)
$
(9.100)
in the notation of example 9.3. A similar equation represents the magnetic
Ô¨Åeld Bz. The Ô¨Åelds E and B obey the equations and boundary conditions of
example 9.3 as well as
Ez(r0, œÜ) = 0,
EœÜ(r0, œÜ) = 0,
and
BœÅ(r0, œÜ) = 0
(9.101)
342

9.4 SPHERICAL BESSEL FUNCTIONS OF THE SECOND KIND
at œÅ = r0. In TM modes with Bz = 0, one may show (exercise 9.27) that the
boundary conditions Ez(r0, œÜ) = 0 and Ez(r, œÜ) = 0 can be satisÔ¨Åed if
Jn(x) Yn(vx) ‚àíJn(vx) Yn(x) = 0
(9.102)
in which v = r/r0 and x =

œâ2/c2 ‚àík2 r0. One can use the Matlab code
n = 0.; v = 10.;
f=@(x)besselj(n,x).*bessely(n,v*x)-besselj(n,v*x).
*bessely(n,x)
x=linspace(0,5,1000);
figure
plot(x,f(x)) % we use the figure to guess at the roots
grid on
options=optimset(‚Äôtolx‚Äô,1e-9);
fzero(f,0.3) % we tell fzero to look near 0.3
fzero(f,0.7)
fzero(f,1)
to Ô¨Ånd that for n = 0 and v = 10, the Ô¨Årst three solutions are x0,1 = 0.3314,
x0,2 = 0.6858, and x0,3 = 1.0377. Setting n = 1 and adjusting the guesses
in the code, one Ô¨Ånds x1,1 = 0.3941, x1,2 = 0.7331, and x1,3 = 1.0748. The
corresponding dispersion relations are œân,i(k) = c

k2 + x2
n,i/r2
0.
9.4 Spherical Bessel functions of the second kind
Spherical Bessel functions of the second kind are deÔ¨Åned as
y‚Ñì(œÅ) =
& œÄ
2œÅ Y‚Ñì+1/2(œÅ)
(9.103)
and Rayleigh formulas express them as
y‚Ñì(œÅ) = (‚àí1)‚Ñì+1œÅ‚Ñì

d
œÅ dœÅ
‚Ñìcos œÅ
œÅ

.
(9.104)
The term in which all the derivatives act on the cosine dominates at big œÅ
y‚Ñì(œÅ) ‚âà(‚àí1)‚Ñì+1 1
œÅ
d‚Ñìcos œÅ
dœÅ‚Ñì
= ‚àícos (œÅ ‚àí‚ÑìœÄ/2) /œÅ.
(9.105)
The second kind of spherical Bessel functions at small œÅ are approximately
y‚Ñì(œÅ) ‚âà‚àí(2‚Ñì‚àí1)!!/œÅ‚Ñì+1.
(9.106)
They all are inÔ¨Ånite at x = 0 as illustrated in Fig. 9.4.
343

BESSEL FUNCTIONS
1
2
3
4
5
6
7
8
9
10
11
12
‚àí0.5
‚àí0.4
‚àí0.3
‚àí0.2
‚àí0.1
0
0.1
0.2
œÅ
1
2
3
4
5
6
7
8
9
10
11
12
‚àí0.5
‚àí0.4
‚àí0.3
‚àí0.2
‚àí0.1
0
0.1
0.2
œÅ
The spherical Bessel function y2(œÅ) forœÅ
1 and œÅ
1
The spherical Bessel function y1(œÅ) for œÅ
1 and œÅ
1
Figure 9.4
Top: plot of y1(œÅ) (solid curve) and its approximations ‚àí1/œÅ2 for small
œÅ (9.106, dot-dash) and ‚àícos(œÅ ‚àíœÄ/2)/œÅ for big œÅ (9.105, dashed). Bottom: plot of
y2(œÅ) (solid curve) and its approximations ‚àí3/œÅ3 for small œÅ (9.106, dot-dash) and
‚àícos(œÅ ‚àíœÄ)/œÅ for big œÅ (9.105, dashed). The values of œÅ at which y‚Ñì(œÅ) = 0 are
the zeros or roots of y‚Ñì; we use them to Ô¨Åt boundary conditions. All six plots run from
œÅ = 1 to œÅ = 12.
Example 9.8 (Scattering off a hard sphere)
In the notation of example 9.5, the
potential of a hard sphere of radius r0 is V(r) = ‚àûŒ∏(r0 ‚àír) in which Œ∏(x) =
(x + |x|)/2|x| is Heaviside‚Äôs function. Since the point r = 0 is not in the physical
region, the scattered wave-function is a linear combination of spherical Bessel
functions of the Ô¨Årst and second kinds
u‚Ñì(r) = c‚Ñìj‚Ñì(kr) + d‚Ñìy‚Ñì(kr).
(9.107)
The boundary condition u‚Ñì(kr0) = 0 Ô¨Åxes the ratio v‚Ñì= d‚Ñì/c‚Ñìof the constants
c‚Ñìand d‚Ñì. Thus for ‚Ñì= 0, Rayleigh‚Äôs formulas (9.68 & 9.104) and the boundary
condition say that kr0 u0(r0) = c0 sin(kr0) ‚àíd0 cos(kr0) = 0 or d0/c0 = tan kr0.
The s-wave then is u0(kr) = c0 sin(kr ‚àíkr0)/(kr cos kr0), which tells us that the
phase shift is Œ¥0(k) = ‚àíkr0. By (9.90), the cross-section at low energy is œÉ ‚âà4œÄr2
0
or four times the classical value.
344

EXERCISES
Similarly, one Ô¨Ånds (exercise 9.28) that the p-wave phase shift is
Œ¥1(k) = kr0 cos kr0 ‚àísin kr0
cos kr0 + kr0 sin kr0
.
(9.108)
For kr0 ‚â™1, we have Œ¥1(k) ‚âà‚àí(kr0)3/6; more generally the ‚Ñìth phase shift
Œ¥‚Ñì(k) ‚âà(kr0)2‚Ñì+1 for a potential of range r0 at low energy k ‚â™1/r0.
Further reading
A great deal is known about Bessel functions. Students may Ô¨Ånd Mathematical
Methods for Physics and Engineering (Riley et al., 2006) as well as the clas-
sics A Treatise on the Theory of Bessel Functions (Watson, 1995), A Course of
Modern Analysis (Whittaker and Watson, 1927, chap. XVII), and Methods of
Mathematical Physics (Courant and Hilbert, 1955) of special interest.
Exercises
9.1
Show that the series (9.1) for Jn(œÅ) satisÔ¨Åes Bessel‚Äôs equation (9.4).
9.2
Show that the generating function exp(z(u ‚àí1/u)/2) for the Bessel functions
is invariant under the substitution u ‚Üí‚àí1/u.
9.3
Use the invariance of exp(z(u‚àí1/u)/2) under u ‚Üí‚àí1/u to show that J‚àín(z) =
(‚àí1)nJn(z).
9.4
By writing the generating function (9.5) as the product of the exponentials
exp(zu/2) and exp(‚àíz/2u), derive the expansion
exp
 z
2

u ‚àíu‚àí1
=
‚àû

m=‚àín
‚àû

n=0
 z
2
m+n
um+n
(m + n)!

‚àíz
2
n u‚àín
n! .
(9.109)
9.5
From this expansion (9.109) of the generating function (9.5), derive the
power-series expansion (9.1) for Jn(z).
9.6
In the formula (9.5) for the generating function exp(z(u ‚àí1/u)/2), replace u
by exp iŒ∏ and then derive the integral representation (9.6) for Jn(z).
9.7
From the general integral representation (9.6) for Jn(z), derive the two integral
formulas (9.7) for J0(z).
9.8
Show that the integral representations (9.6 & 9.7) imply that for any integer
n Ã∏= 0, Jn(0) = 0, while J0(0) = 1.
9.9
By differentiating the generating function (9.5) with respect to u and identify-
ing the coefÔ¨Åcients of powers of u, derive the recursion relation
Jn‚àí1(z) + Jn+1(z) = 2n
z Jn(z).
(9.110)
345

BESSEL FUNCTIONS
9.10 By differentiating the generating function (9.5) with respect to z and identify-
ing the coefÔ¨Åcients of powers of u, derive the recursion relation
Jn‚àí1(z) ‚àíJn+1(z) = 2 J‚Ä≤
n(z).
(9.111)
9.11 Show that the change of variables œÅ = ax turns (9.4) into the self-adjoint form
of Bessel‚Äôs equation (9.11).
9.12 If y = Jn(ax), then equation (9.11) is (xy‚Ä≤)‚Ä≤ + (xa2 ‚àín2/x)y = 0. Multiply
this equation by xy‚Ä≤, integrate from 0 to b, and so show that if ab = zn,m and
Jn(zn,m) = 0, then
2
 b
0
x J2
n(ax) dx = b2J‚Ä≤2
n (zn,m),
(9.112)
which is the normalization condition (9.14).
9.13 Show that with Œª ‚â°z2/r2
d, the change of variables œÅ = zr/rd and u(r) = Jn(œÅ)
turns ‚àí

r u‚Ä≤‚Ä≤ + n2 u/r = Œª r u into (9.25).
9.14 Use the formula (6.42) for the curl in cylindrical coordinates and the vacuum
forms ‚àá√ó E = ‚àíÀôB and ‚àá√ó B = ÀôE/c2 of the laws of Faraday and Maxwell‚Äì
Amp√®re to derive the Ô¨Åeld equations (9.55).
9.15 Derive equations (9.56) from (9.55).
9.16 Show that Jn(

œâ2/c2 ‚àík2 œÅ)einœÜei(kz‚àíœât) is a traveling-wave solution (9.52) of
the wave equations (9.57).
9.17 Find expressions for the nonzero TM Ô¨Åelds in terms of the formula (9.59)
for Ez.
9.18 Show that the Ô¨Åeld Bz
= Jn(

œâ2/c2 ‚àík2 œÅ)einœÜei(kz‚àíœât) will satisfy the
boundary conditions (9.54) if

œâ2/c2 ‚àík2 r is a zero z‚Ä≤
n,m of J‚Ä≤
n.
9.19 Show that if ‚Ñìis an integer and if

œâ2/c2 ‚àíœÄ2‚Ñì2/h2 r is a zero z‚Ä≤
n,m of J‚Ä≤
n, then
the Ô¨Åelds Ez = 0 and Bz = Jn(z‚Ä≤
n,mœÅ/r) einœÜ sin(‚ÑìœÄz/h) e‚àíiœât satisfy both the
boundary conditions (9.54) at œÅ = r and those (9.60) at z = 0 and h as well as
the wave equations (9.57). Hint: use Maxwell‚Äôs equations ‚àá√ó E = ‚àíÀôB and
‚àá√ó B = ÀôE/c2 as in (9.55).
9.20 Show that the resonant frequencies of the TM modes of the cavity of
example 9.4 are œân,m,‚Ñì= c

z2n,m/r2 + œÄ2‚Ñì2/h2.
9.21 By setting n = ‚Ñì+ 1/2 and j‚Ñì= ‚àöœÄ/2x J‚Ñì+1/2, show that Bessel‚Äôs equation
(9.4) implies that the spherical Bessel function j‚ÑìsatisÔ¨Åes (9.63).
9.22 Show that Rayleigh‚Äôs formula (9.68) implies the recursion relation (9.69).
9.23 Use the recursion relation (9.69) to show by induction that the spherical
Bessel functions j‚Ñì(x) as given by Rayleigh‚Äôs formula (9.68) satisfy their
differential equation (9.66) which with x = kr is
‚àíx2j‚Ä≤‚Ä≤
‚Ñì‚àí2xj‚Ä≤
‚Ñì+ ‚Ñì(‚Ñì+ 1)j‚Ñì= x2j‚Ñì.
(9.113)
Hint: start by showing that j0(x) = sin(x)/x satisÔ¨Åes (9.66). This problem
involves some tedium.
346

EXERCISES
9.24 Iterate the trick
d
zdz
 1
‚àí1
eizx dx = i
z
 1
‚àí1
xeizx dx = i
2z
 1
‚àí1
eizx d(x2 ‚àí1)
= ‚àíi
2z
 1
‚àí1
(x2 ‚àí1)deizx = 1
2
 1
‚àí1
(x2 ‚àí1)eizxdx
(9.114)
to show that (Schwinger et al., 1998, p. 227)
 d
zdz
‚Ñì 1
‚àí1
eizx dx =
 1
‚àí1
(x2 ‚àí1)‚Ñì
2‚Ñì‚Ñì!
eizx dx.
(9.115)
9.25 Use the expansions (9.76 and 9.77) to show that the inner product of the ket
|r‚ü©that represents a particle at r with polar angles Œ∏ and œÜ and the one |k‚ü©
that represents a particle with momentum œÅ = ¬Øhk with polar angles Œ∏‚Ä≤ and œÜ‚Ä≤
is, with k ¬∑ r = kr cos Œ∏,
‚ü®r|k‚ü©=
1
(2œÄ)3/2 eikr cos Œ∏ =
1
(2œÄ)3/2
‚àû

‚Ñì=0
(2‚Ñì+ 1) P‚Ñì(cos Œ∏) i‚Ñìj‚Ñì(kr)
=
1
(2œÄ)3/2 eik¬∑r =
&
2
œÄ
‚àû

‚Ñì=0
i‚Ñìj‚Ñì(kr) Y‚Ñì,m(Œ∏, œÜ) Y‚àó
‚Ñì,m(Œ∏‚Ä≤, œÜ‚Ä≤).
(9.116)
9.26 Show that (‚àí1)‚Ñìd‚Ñìsin œÅ/dœÅ‚Ñì= sin(œÅ ‚àíœÄ‚Ñì/2) and so complete the derivation
of the approximation (9.80) for j‚Ñì(œÅ) for big œÅ.
9.27 In the context of examples 9.3 and 9.7, show that the boundary conditions
Ez(r0, œÜ) = 0 and Ez(r, œÜ) = 0 imply (9.102).
9.28 Show that for scattering off a hard sphere of radius r0 as in example 9.8, the
p-wave phase shift is given by (9.108).
347

10
Group theory
10.1 What is a group?
A group G is a set of objects f , g, h, . . . and an operation called multiplication
such that:
1 if f ‚ààG and g ‚ààG, the product fg ‚ààG (closure);
2 if f , g, and h are in G, then f (gh) = (fg)h (associativity);
3 there is an identity e ‚ààG such that if g ‚ààG, then ge = eg = g;
4 every g ‚ààG has an inverse g‚àí1 ‚ààG such that gg‚àí1 = g‚àí1g = e.
Physical transformations naturally form groups. The product T‚Ä≤ T represents
the transformation T followed by the transformation T‚Ä≤. And both T‚Ä≤‚Ä≤ (T‚Ä≤ T)
and (T‚Ä≤‚Ä≤ T‚Ä≤) T represent the transformation T followed by the transformation
T‚Ä≤ and then by T‚Ä≤‚Ä≤. So transformations are associative. The identity element e
is the null transformation, the one that does nothing. The inverse T‚àí1 is the
transformation that reverses the effect of T. Such a set {T} of transformations
will form a group if any two successive transformations is a transformation in
the set (closure). Closure occurs naturally when the criterion for membership in
the group is that a transformation not change something. For if both T and T‚Ä≤
leave that thing unchanged, then so will their product T‚Ä≤ T.
Example 10.1 (Groups of coordinate transformations)
The set of all trans-
formations that leave invariant the distance from the origin of every point in
n-dimensional space is the group O(n) of rotations and reÔ¨Çections. The rotations
in Rn form the group SO(n).
The set of all transformations that leave invariant the spatial difference x ‚àíy
between every two points x and y in n-dimensional space is the group of
translations. In this case, group multiplication is vector addition.
348

10.1 WHAT IS A GROUP?
The set of all linear transformations that leave invariant the square of the
Minkowski distance x2
1 + x2
2 + x2
3 ‚àíx2
0 between any 4-vector x and the ori-
gin is the Lorentz group (Hermann Minkowski, 1864‚Äì1909; Hendrik Lorentz,
1853‚Äì1928).
The set of all linear transformations that leave invariant the square of the
Minkowski distance (x1‚àíy1)2+(x2‚àíy2)2+(x3‚àíy3)2‚àí(x0‚àíy0)2 between any two
4-vectors x and y is the Poincar√© group, which includes Lorentz transformations
and translations (Henri Poincar√©, 1854‚Äì1912).
Except for the group of translations, the order of the physical transforma-
tions in these examples matters: the transformation T‚Ä≤ T is not in general the
same as T T‚Ä≤. Such groups are called nonabelian. A group whose elements all
commute
[T‚Ä≤, T] ‚â°T‚Ä≤ T ‚àíT T‚Ä≤ = 0
(10.1)
is said to be abelian (Niels Abel, 1802‚Äì1829).
Matrices naturally form groups. Since matrix multiplication is associative,
any set {D} of n √ó n nonsingular matrices that includes the inverse D‚àí1 of every
matrix in the set as well as the identity matrix I automatically satisÔ¨Åes properties
2‚Äì4 with group multiplication deÔ¨Åned as matrix multiplication. Only property
1, closure under multiplication, is uncertain. A set {D} of matrices will form a
group as long as the product of any two matrices is in the set. As with phys-
ical transformations, one way to ensure closure is to have every matrix leave
something unchanged.
Example 10.2 (Orthogonal groups)
The set of all n √ó n real matrices that leave
the quadratic form x2
1 + x2
2 + ¬∑ ¬∑ ¬∑ + x2
n unchanged forms the orthogonal group
O(n) of all n √ó n orthogonal (1.36) matrices (exercises 10.1 & 10.2). The n √ó n
orthogonal matrices that have unit determinant form the special orthogonal
group SO(n). The group SO(3) describes rotations.
Example 10.3 (Unitary groups)
The set of all n√ón complex matrices that leave
invariant the quadratic form x‚àó
1x1 + x‚àó
2x2 + ¬∑ ¬∑ ¬∑ + x‚àó
nxn forms the unitary group
U(n) of all n √ó n unitary (1.35) matrices (exercises 10.3 & 10.4). Those of unit
determinant form the special unitary group SU(n) (exercise 10.5).
Like SO(3), the group SU(2) represents rotations. The group SU(3) is the sym-
metry group of the strong interactions, quantum chromodynamics. Physicists
have used the groups SU(5) and SO(10) to unify the electro weak and strong
interactions; whether Nature also does so is unclear.
The number of elements in a group is the order of the group. A Ô¨Ånite group is
a group with a Ô¨Ånite number of elements, or equivalently a group of Ô¨Ånite order.
349

GROUP THEORY
Example 10.4 (Z2 and Zn)
The parity group whose elements are 1 and ‚àí1
under ordinary multiplication is the Ô¨Ånite group Z2. It is abelian and of order 2.
The group Zn for any positive integer n is made of the phases exp(i2kœÄ/n) for
k = 1, 2, ..., n. It is abelian and of order n.
A group whose elements g = g({Œ±}) depend continuously upon a set of
parameters Œ±k is a continuous group or a Lie group. Continuous groups are of
inÔ¨Ånite order.
A group G of matrices D is compact if the (squared) norm as given by the
trace
Tr

D‚Ä†D

‚â§M
(10.2)
is bounded for all the D ‚ààG.
Example 10.5 (SO(n), O(n), SU(n), and U(n))
The groups SO(n), O(n), SU(n),
and U(n) are continuous Lie groups of inÔ¨Ånite order. Since for any matrix D in
one of these groups
Tr

D‚Ä†D

= TrI = n ‚â§M
(10.3)
these groups also are compact.
Example 10.6 (Noncompact groups)
The set of all real n √ó n matrices forms
the general linear group GL(n, R); those of unit determinant form the special lin-
ear group SL(n, R). The corresponding groups of matrices with complex entries
are GL(n, C) and SL(n, C). These four groups have matrix elements that are
unbounded; they are noncompact. They are continuous Lie groups of inÔ¨Ånite
order like the orthogonal and unitary groups. The group SL(2, C) represents
Lorentz transformations.
10.2 Representations of groups
If one can associate with every element g of a group G a square matrix D(g) and
have matrix multiplication imitate group multiplication
D(f ) D(g) = D(fg)
(10.4)
for all elements f and g of the group G, then the set of matrices D(g) is said
to form a representation of the group G. If the matrices of the representation
are n √ó n, then n is the dimension of the representation. The dimension of a
representation also is the dimension of the vector space on which the matrices
act. If the matrices D(g) are unitary D‚Ä†(g) = D‚àí1(g), then they form a unitary
representation of the group.
350

10.3 REPRESENTATIONS ACTING IN HILBERT SPACE
Compact groups possess Ô¨Ånite-dimensional unitary representations; noncom-
pact groups do not. A group of bounded (10.2) matrices is compact. An abstract
group of elements g({Œ±}) is compact if its space of parameters {Œ±} is closed and
bounded. (A set is closed if the limit of every convergent sequence of its points
lies in the set. A set is open if each of its elements lies in a neighborhood that
lies in the set. For example, the interval [a, b] ‚â°{x|a ‚â§x ‚â§b} is closed, and
(a, b) ‚â°{x|a < x < b} is open.) The group of rotations is compact, but the
group of translations and the Lorentz group are noncompact.
Every n √ó n matrix S that is nonsingular (det S Ã∏= 0) maps any n √ó n repre-
sentation D(g) of a group G into an equivalent representation D‚Ä≤(g) through the
similarity transformation
D‚Ä≤(g) = S‚àí1D(g)S,
(10.5)
which preserves the law of multiplication
D‚Ä≤(f ) D‚Ä≤(g) = S‚àí1D(f )S S‚àí1D(g)S
= S‚àí1D(f ) D(g)S = S‚àí1D(fg)S = D‚Ä≤(fg).
(10.6)
A proper subspace W of a vector space V is a subspace of lower (but not
zero) dimension. A proper subspace W is invariant under the action of a repre-
sentation D(g) if D(g) maps every vector v ‚ààW to a vector D(g) v = v‚Ä≤ ‚ààW. A
representation that has a proper invariant subspace is reducible. A representation
that is not reducible is irreducible.
There is no need to keep track of several equivalent irreducible representa-
tions D, D‚Ä≤, D‚Ä≤‚Ä≤ of any group. So in what follows, we shall choose one of these
equivalent irreducible representations and use it exclusively.
A representation is completely reducible if it is equivalent to a representation
whose matrices are in block-diagonal form
‚éõ
‚éú‚éù
D1(g)
0
. . .
0
D2(g)
. . .
...
...
...
‚éû
‚éü‚é†
(10.7)
in which each representation Di(g) irreducible. A representation in block-
diagonal form is said to be a direct sum of the irreducible representations
Di
D1 ‚äïD2 ‚äï¬∑ ¬∑ ¬∑ .
(10.8)
10.3 Representations acting in Hilbert space
A symmetry transformation g is a map (1.174) of states œà ‚Üíœà‚Ä≤ that preserves
their inner products
|‚ü®œÜ‚Ä≤|œà‚Ä≤‚ü©|2 = |‚ü®œÜ|œà‚ü©|2
(10.9)
351

GROUP THEORY
and so their predicted probabilities. The action of a group G of symmetry trans-
formations g on the Hilbert space of a quantum theory can be represented
either by operators U(g) that are linear and unitary (the usual case) or by ones
K(g) that are antilinear (1.172) and antiunitary (1.173), as in the case of time
reversal. Wigner proved this theorem in the 1930s, and Weinberg improved it
in his 1995 classic (Weinberg, 1995, p. 51) (Eugene Wigner, 1902‚Äì1995; Steven
Weinberg, 1933‚Äì).
Two operators F1 and F2 that commute F1 F2 = F2 F1 are compatible (1.328).
A set of compatible operators F1, F2, ... is complete if to every set of eigenvalues
there belongs only a single eigenvector (section 1.30).
Example 10.7 (Rotation operators)
Suppose that the hamiltonian H, the
square of the angular momentum J2, and its z-component Jz form a complete
set of compatible observables, so that the identity operator can be expressed as
a sum over the eigenvectors of these operators
I =

E,j,m
|E, j, m‚ü©‚ü®E, j, m|.
(10.10)
Then the matrix element of a unitary operator U(g) between two states |œà‚ü©
and |œÜ‚ü©is
‚ü®œÜ|U(g)|œà‚ü©= ‚ü®œÜ|

E‚Ä≤,j‚Ä≤,m‚Ä≤
|E‚Ä≤, j‚Ä≤, m‚Ä≤‚ü©‚ü®E‚Ä≤, j‚Ä≤, m‚Ä≤| U(g)

E,j,m
|E, j, m‚ü©‚ü®E, j, m|œà‚ü©.
(10.11)
Let H and J2 be invariant under the action of U(g) so that U‚Ä†(g)HU(g) = H
and U‚Ä†(g)J2U(g) = J2. Then HU(g) = U(g)H and J2U(g) = U(g)J2, and so if
H|E, j, m‚ü©= E|E, j, m‚ü©and J2|E, j, m‚ü©= j( j + 1)|E, j, m‚ü©, we have
HU(g)|E, j, m‚ü©= U(g)H|E, j, m‚ü©= EU(g)|E, j, m‚ü©,
J2U(g)|E, j, m‚ü©= U(g)J2|E, j, m‚ü©= j( j + 1)U(g)|E, j, m‚ü©.
(10.12)
Thus U(g) can not change E or j, and so
‚ü®E‚Ä≤, j‚Ä≤, m‚Ä≤|U(g)|E, j, m‚ü©= Œ¥E‚Ä≤EŒ¥j‚Ä≤j‚ü®m‚Ä≤|U(g)|m‚ü©= Œ¥E‚Ä≤EŒ¥j‚Ä≤jD( j)
m‚Ä≤m(g).
(10.13)
The matrix element (10.11) then is a single sum over E and j in which the
irreducible representations D( j)
m‚Ä≤m(g) of the rotation group SU(2) appear
‚ü®œÜ|U(g)|œà‚ü©=

E,j,m‚Ä≤,m
‚ü®œÜ|E, j, m‚Ä≤‚ü©D( j)
m‚Ä≤m(g)‚ü®E, j, m|œà‚ü©.
(10.14)
This is how the block-diagonal form (10.7) usually appears in calculations. The
matrices D( j)
m‚Ä≤m(g) inherit the unitarity of the operator U(g).
352

10.4 SUBGROUPS
10.4 Subgroups
If all the elements of a group S also are elements of a group G, then S is a
subgroup of G. Every group G has two trivial subgroups ‚Äì the identity element
e and the whole group G itself. Many groups have more interesting subgroups.
For example, the rotations about a Ô¨Åxed axis is an abelian subgroup of the group
of all rotations in three-dimensional space.
A subgroup S ‚äÇG is an invariant subgroup if every element s of the subgroup
S is left inside the subgroup under the action of every element g of the whole
group G, that is, if
g‚àí1s g = s‚Ä≤ ‚ààS
for all
g ‚ààG.
(10.15)
This condition often is written as g‚àí1Sg = S for all g ‚ààG or as
S g = g S
for all g ‚ààG.
(10.16)
Invariant subgroups also are called normal subgroups.
A set C ‚äÇG is called a conjugacy class if it‚Äôs invariant under the action of the
whole group G, that is, if Cg = g C or
g‚àí1C g = C
for all g ‚ààG.
(10.17)
A subgroup that is the union of a set of conjugacy classes is invariant.
The center C of a group G is the set of all elements c ‚ààG that commute with
every element g of the group, that is, their commutators
[c, g] ‚â°cg ‚àígc = 0
(10.18)
vanish for all g ‚ààG.
Example 10.8 (Centers are abelian subgroups)
Does the center C always form
an abelian subgroup of its group G? The product g1g2 of any two elements g1
and g2 of the center must commute with every element g of G since g1g2g =
g1gg2 = gg1g2. So the center is closed under multiplication. The identity element
e commutes with every g ‚ààG, so e ‚ààC. If g‚Ä≤ ‚ààC, then g‚Ä≤g = gg‚Ä≤ for all g ‚ààG,
and so multiplication of this equation from the left and the right by g‚Ä≤‚àí1 gives
gg‚Ä≤‚àí1 = g‚Ä≤‚àí1g, which shows that g‚Ä≤‚àí1 ‚ààC.
So the center of any group always is one of its abelian invariant subgroups.
The center may be trivial, however, consisting either of the identity or of
the whole group. But a group with a nontrivial center can not be simple or
semisimple (section 10.23).
353

GROUP THEORY
10.5 Cosets
If H is a subgroup of a group G, then for every element g ‚ààG the set of elements
Hg ‚â°{hg|h ‚ààH, g ‚ààG} is a right coset of the subgroup H ‚äÇG. (Here ‚äÇmeans
is a subset of or equivalently is contained in.)
If H is a subgroup of a group G, then for every element g ‚ààG the set of
elements gH is a left coset of the subgroup H ‚äÇG.
The number of elements in a coset is the same as the number of elements of
H, which is the order of H.
An element g of a group G is in one and only one right coset (and in one
and only one left coset) of the subgroup H ‚äÇG. For suppose instead that g
were in two right cosets g ‚ààHg1 and g ‚ààHg2, so that g = h1g1 = h2g2
for suitable h1, h2 ‚ààH and g1, g2 ‚ààG. Then since H is a (sub)group, we
have g2 = h‚àí1
2 h1g1 = h3g1, which says that g2 ‚ààHg1. But this means that
every element of hg2 ‚ààHg2 is of the form hg2 = hh3g1 = h4g1 ‚ààHg1.
So every element of hg2 ‚ààHg2 is in Hg1: the two right cosets are identical,
Hg1 = Hg2.
The right (or left) cosets are the points of the quotient coset space G/H.
If H is an invariant subgroup of G, then by deÔ¨Ånition (10.16) Hg =
gH for all g ‚ààG, and so the left cosets are the same sets as the right cosets.
In this case, the coset space G/H is itself a group with multiplication deÔ¨Åned by
(Hg1) (Hg2) =

hig1hjg2|hi, hj ‚ààH

=
3
hig1hjg‚àí1
1 g1g2|hi, hj ‚ààH
4
= {hihkg1g2|hi, hk ‚ààH}
= {h‚Ñìg1g2|h‚Ñì‚ààH} = Hg1g2,
(10.19)
which is the multiplication rule of the group G. This group G/H is called the
factor group of G by H.
10.6 Morphisms
An isomorphism is a one-to-one map between groups that respects their multi-
plication laws. For example, the relation between two equivalent representations
D‚Ä≤(g) = S‚àí1D(g)S
(10.20)
is an isomorphism (exercise 10.8). An automorphism is an isomorphism between
a group and itself. The map gi ‚Üíg gi g‚àí1 is one to one because g g1 g‚àí1 =
g g2 g‚àí1 implies that g g1 = g g2, and so that g1 = g2. This map also preserves
the law of multiplication since g g1 g‚àí1 g g2 g‚àí1 = g g1 g2 g‚àí1. So the map
G ‚ÜígGg‚àí1
(10.21)
354

10.7 SCHUR‚ÄôS LEMMA
is an automorphism. It is called an inner automorphism because g is an ele-
ment of G. An automorphism not of this form (10.21) is called an outer
automorphism.
10.7 Schur‚Äôs lemma
Part 1 If D1 and D2 are inequivalent, irreducible representations of a group G,
and if D1(g)A = AD2(g) for some matrix A and for all g ‚ààG, then the matrix
A must vanish, A = 0.
Proof First suppose that A annihilates some vector |x‚ü©, that is, A|x‚ü©= 0. Let
P be the projection operator into the subspace that A annihilates, which is of at
least one dimension. This subspace, incidentally, is called the null space N (A)
or the kernel of the matrix A. The representation D2 must leave this null space
N (A) invariant since
AD2(g)P = D1(g)AP = 0.
(10.22)
If N (A) were a proper subspace, then it would be a proper invariant subspace
of the representation D2, and so D2 would be reducible, which is contrary to
our assumption that D1 and D2 are irreducible. So the null space N (A) must be
the whole space upon which A acts, that is, A = 0.
A similar argument shows that if ‚ü®y|A = 0 for some bra ‚ü®y|, then A = 0.
So either A is zero or it annihilates no ket and no bra. In the latter case, A
must be square and invertible, which would imply that D2(g) = A‚àí1D1(g)A,
that is, that D1 and D2 are equivalent representations, which is contrary
to our assumption that they are inequivalent. The only way out is that A
vanishes.
Part 2 If for a Ô¨Ånite-dimensional, irreducible representation D(g) of a group
G, we have D(g)A = AD(g) for some matrix A and for all g ‚ààG, then A = cI.
That is, any matrix that commutes with every element of a Ô¨Ånite-dimensional,
irreducible representation must be a multiple of the identity matrix.
Proof Every square matrix A has at least one eigenvector |x‚ü©and eigenvalue
c so that A|x‚ü©= c|x‚ü©because its characteristic equation det(A ‚àícI) = 0 always
has at least one root by the fundamental theorem of algebra (5.73). So the null
space N (A ‚àícI) has dimension greater than zero. The assumption D(g)A =
AD(g) for all g ‚ààG implies that D(g)(A ‚àícI) = (A ‚àícI)D(g) for all g ‚àà
G. Let P be the projection operator onto the null space N (A ‚àícI). Then we
have (A ‚àícI)D(g)P = D(g)(A ‚àícI)P = 0 for all g ‚ààG, which implies that
D(g)P maps vectors into the null space N (A ‚àícI). This null space therefore
is invariant under D(g), which means that D is reducible unless the null space
N (A ‚àícI) is the whole space. Since by assumption D is irreducible, it follows
that N (A ‚àícI) is the whole space, that is, that A = cI (Issai Schur, 1875‚Äì
1941).
355

GROUP THEORY
Example 10.9 (Schur, Wigner, and Eckart)
Suppose an arbitrary observable O
is invariant under the action of the rotation group SU(2) represented by unitary
operators U(g) for g ‚ààSU(2)
U‚Ä†(g)OU(g) = O
or
[O, U(g)] = 0.
(10.23)
These unitary rotation operators commute with the square J2 of the angu-
lar momentum [J2, U] = 0. Suppose that they also leave the hamiltonian H
unchanged [H, U] = 0. Then as shown in example 10.7, the state U|E, j, m‚ü©is a
sum of states all with the same values of j and E. It follows that

m‚Ä≤
‚ü®E, j, m|O|E‚Ä≤, j‚Ä≤, m‚Ä≤‚ü©‚ü®E‚Ä≤, j‚Ä≤, m‚Ä≤|U(g)|E‚Ä≤, j‚Ä≤, m‚Ä≤‚Ä≤‚ü©
=

m‚Ä≤
‚ü®E, j, m|U(g)|E, j, m‚Ä≤‚ü©‚ü®E, j, m‚Ä≤|O|E‚Ä≤, j‚Ä≤, m‚Ä≤‚Ä≤‚ü©
(10.24)
or more simply in view of (10.13)

m‚Ä≤
‚ü®E, j, m|O|E‚Ä≤, j‚Ä≤, m‚Ä≤‚ü©Dj‚Ä≤(g)m‚Ä≤m‚Ä≤‚Ä≤ =

m‚Ä≤
D( j)(g)mm‚Ä≤‚ü®E, j, m‚Ä≤|O|E‚Ä≤, j‚Ä≤, m‚Ä≤‚Ä≤‚ü©.
(10.25)
Now Part 1 of Schur‚Äôs lemma tells us that the matrix ‚ü®E, j, m|O|E‚Ä≤, j‚Ä≤, m‚Ä≤‚ü©must
vanish unless the representations are equivalent, which is to say unless j = j‚Ä≤. So
we have

m‚Ä≤
‚ü®E, j, m|O|E‚Ä≤, j, m‚Ä≤‚ü©Dj(g)m‚Ä≤m‚Ä≤‚Ä≤
=

m‚Ä≤
D( j)(g)mm‚Ä≤‚ü®E, j, m‚Ä≤|O|E‚Ä≤, j, m‚Ä≤‚Ä≤‚ü©.
(10.26)
Now Part 2 of Schur‚Äôs lemma tells us that the matrix ‚ü®E, j, m|O|E‚Ä≤, j, m‚Ä≤‚ü©must be
a multiple of the identity. Thus the symmetry of O under rotations simpliÔ¨Åes the
matrix element to
‚ü®E, j, m|O|E‚Ä≤, j‚Ä≤, m‚Ä≤‚ü©= Œ¥jj‚Ä≤Œ¥mm‚Ä≤Oj(E, E‚Ä≤).
(10.27)
This result is a special case of the Wigner‚ÄìEckart theorem (Eugene Wigner,
1902‚Äì1995; Carl Eckart, 1902‚Äì1973).
10.8 Characters
Suppose the n √ó n matrices Dij(g) form a representation of a group G ‚àãg. The
character œáD(g) of the matrix D(g) is the trace
œáD(g) = TrD(g) =
n

i=1
Dii(g).
(10.28)
356

10.9 TENSOR PRODUCTS
Traces are cyclic, that is, TrABC = TrBCA = TrCAB. So if two representations
D and D‚Ä≤ are equivalent, so that D‚Ä≤(g) = S‚àí1D(g)S, then they have the same
characters because
œáD‚Ä≤(g) = TrD‚Ä≤(g) = Tr

S‚àí1D(g)S

= Tr

D(g)SS‚àí1
= TrD(g) = œáD(g).
(10.29)
If two group elements g1 and g2 are in the same conjugacy class, that is,
if g2 = gg1g‚àí1 for all g ‚ààG, then they have the same character in a given
representation D(g) because
œáD(g2) = TrD(g2) = TrD(gg1g‚àí1) = Tr

D(g)D(g1)D(g‚àí1)

= Tr

D(g1)D‚àí1(g)D(g)

= TrD(g1) = œáD(g1).
(10.30)
10.9 Tensor products
Suppose D1(g) is a k-dimensional representation of a group G, and D2(g) is
an n-dimensional representation of the same group. Suppose the vectors |‚Ñì‚ü©
for ‚Ñì= 1, . . . , k are the basis vectors of the k-dimensional space Vk on which
D1(g) acts, and that the vectors |m‚ü©for m = 1, . . . , n are the basis vectors of
the n-dimensional space Vn on which D2(g) acts. The k √ó n vectors |‚Ñì, m‚ü©are
basis vectors for the kn-dimensional tensor-product space Vkn. The matrices
DD1‚äóD2(g) deÔ¨Åned as
‚ü®‚Ñì‚Ä≤, m‚Ä≤|DD1‚äóD2(g)|‚Ñì, m‚ü©= ‚ü®‚Ñì‚Ä≤|D1(g)|‚Ñì‚ü©‚ü®m‚Ä≤|D2(g)|m‚ü©
(10.31)
act in this kn-dimensional space Vkn and form a representation of the group
G; this tensor-product representation usually is reducible. Many tricks help
one to decompose reducible tensor-product representations into direct sums
of irreducible representations (Georgi, 1999, p. 309).
Example 10.10 (Adding angular momenta)
The addition of angular momenta
illustrates both the tensor product and its reduction to a direct sum of irreducible
representations. Let Dj1(g) and Dj2(g) respectively be the (2j1 + 1) √ó (2j1 + 1)
and the (2j2 + 1) √ó (2j2 + 1) representations of the rotation group SU(2). The
tensor-product representation DDj1‚äóDj2
‚ü®m‚Ä≤
1, m‚Ä≤
2|DDj1‚äóDj2|m1, m2‚ü©= ‚ü®m‚Ä≤
1|Dj1(g)|m1‚ü©‚ü®m‚Ä≤
2|Dj2(g)|m2‚ü©
(10.32)
is reducible into a direct sum of all the irreducible representations of SU(2) from
Dj1+j2(g) down to D| j1‚àíj2|(g) in integer steps:
DDj1‚äóDj2 = Dj1+j2 ‚äïDj1+j2‚àí1 ‚äï¬∑ ¬∑ ¬∑ ‚äïD| j1‚àíj2|+1 ‚äïD| j1‚àíj2|,
(10.33)
each irreducible representation occurring once in the direct sum.
357

GROUP THEORY
Example 10.11 (Adding two spins)
When one adds j1 = 1/2 to j2 = 1/2, one
Ô¨Ånds that the tensor-product matrix DD1/2‚äóD1/2 is equivalent to the direct sum
D1 ‚äïD0
DD1/2‚äóD1/2(Œ∏) = S‚àí1
D1(Œ∏)
0
0
D0(Œ∏)

S
(10.34)
where the matrices S, D1, and D0 respectively are 4 √ó 4, 3 √ó 3, and 1 √ó 1.
10.10 Finite groups
A Ô¨Ånite group is one that has a Ô¨Ånite number of elements. The number of
elements in a group is the order of the group.
Example 10.12 (Z2)
The group Z2 consists of two elements e and p with
multiplication rules
ee = e,
ep = p,
pe = p,
pp = e.
(10.35)
Clearly, Z2 is abelian, and its order is 2. The identiÔ¨Åcation e ‚Üí1 and p ‚Üí‚àí1
gives a 1-dimensional representation of the group Z2 in terms of 1 √ó 1 matrices,
which are just numbers.
It is tedious to write the multiplication rules as individual equations. Nor-
mally people compress them into a multiplication table like this:
√ó
e
p
e
e
p
p
p
e
,
(10.36)
A simple generalization of Z2 is the group Zn whose elements may be rep-
resented as exp(i2œÄm/n) for m = 1, . . . , n. This group is also abelian, and its
order is n.
Example 10.13 (Z3)
The multiplication table for Z3 is
√ó
e
a
b
e
e
a
b
a
a
b
e
b
b
e
a
,
(10.37)
whichsays that a2 = b, b2 = a, and ab = ba = e.
358

10.11 THE REGULAR REPRESENTATION
10.11 The regular representation
For any Ô¨Ånite group G we can associate an orthonormal vector |gi‚ü©with each
element gi of the group. So ‚ü®gi|gj‚ü©= Œ¥ij. These orthonormal vectors |gi‚ü©form a
basis for a vector space whose dimension is the order of the group. The matrix
D(gk) of the regular representation of G is deÔ¨Åned to map any vector |gi‚ü©into
the vector |gkgi‚ü©associated with the product gkgi
D(gk)|gi‚ü©= |gkgi‚ü©.
(10.38)
Since group multiplication is associative, we have
D(gj)D(gk)|gi‚ü©= D(gj)|gkgi‚ü©= |gj(gkgi)‚ü©= |(gjgk)gi)‚ü©= D(gjgk)|gi‚ü©. (10.39)
Because the vector |gi‚ü©was an arbitrary basis vector, it follows that
D(gj)D(gk) = D(gjgk),
(10.40)
which means that the matrices D(g) satisfy the closure criterion (10.4) for their
being a representation of the group G. The matrix D(g) has entries
[D(g)]ij = ‚ü®gi|D(g)|gj‚ü©.
(10.41)
The sum of dyadics |g‚Ñì‚ü©‚ü®g‚Ñì| over all the elements g‚Ñìof a Ô¨Ånite group G is the
unit matrix

g‚Ñì‚ààG
|g‚Ñì‚ü©‚ü®g‚Ñì| = In,
(10.42)
in which n is the order of G, that is, the number of elements in G. So by taking
the m, n matrix element of the multiplication law (10.40), we Ô¨Ånd
[D(gjgk)]m,n = ‚ü®gm|D(gjgk)|gn‚ü©= ‚ü®gm|D(gj)D(gk)|gn‚ü©
=

g‚Ñì‚ààG
‚ü®gm|D(gj)|g‚Ñì‚ü©‚ü®g‚Ñì|D(gk)|gn‚ü©
=

g‚Ñì‚ààG
[D(gj)]m,‚Ñì[D(gk)]‚Ñì,n.
(10.43)
Example 10.14 (Z3‚Äôs regular representation)
The regular representation of Z3 is
D(e) =
‚éõ
‚éù
1
0
0
0
1
0
0
0
1
‚éû
‚é†,
D(a) =
‚éõ
‚éù
0
0
1
1
0
0
0
1
0
‚éû
‚é†,
D(b) =
‚éõ
‚éù
0
1
0
0
0
1
1
0
0
‚éû
‚é†
(10.44)
so D(a)2 = D(b), D(b)2 = D(a), and D(a)D(b) = D(b)D(a) = D(e).
359

GROUP THEORY
10.12 Properties of Ô¨Ånite groups
In his book (Georgi, 1999, ch. 1), Georgi proves the following theorems.
1 Every
representation
of
a
Ô¨Ånite
group
is
equivalent
to
a
unitary
representation.
2 Every representation of a Ô¨Ånite group is completely reducible.
3 The irreducible representations of a Ô¨Ånite abelian group are one dimensional.
4 If D(a)(g) and D(b)(g) are two unitary irreducible representations of dimen-
sions na and nb of a group G of N elements g1, . . . , gN, then the functions
&na
N D(a)
jk (g)
(10.45)
are orthonormal and complete in the sense that
na
N
N

j=1
D(a)‚àó
ik (gj)D(b)
‚Ñìm(gj) = Œ¥abŒ¥i‚ÑìŒ¥km.
(10.46)
5 The order N of a Ô¨Ånite group is the sum of the squares of the dimensions of
its inequivalent irreducible representations
N =

a
n2
a.
(10.47)
Example 10.15 (ZN)
The abelian cyclic group ZN with elements
gj = e2œÄij/N
(10.48)
has N one-dimensional irreducible representations
D(a)(gj) = e2œÄiaj/N
(10.49)
for a = 1, 2, . . . , N. Their orthonormality relation (10.46) is the Fourier formula
1
N
N

j=1
e‚àí2œÄiaj/Ne2œÄibj/N = Œ¥ab.
(10.50)
The na are all unity, there are N of them, and the sum of the n2
a is N as required
by the sum rule (10.47).
10.13 Permutations
The permutation group on n objects is called Sn. Permutations are made of
cycles that change the order of some of the n objects. For instance, the permu-
tation (1 2) = (2 1) is a 2-cycle that means x1 ‚Üíx2 ‚Üíx1; the unitary operator
360

10.15 LIE ALGEBRA
U((1 2)) that represents it interchanges states like this:
U((1 2))|+, ‚àí‚ü©= U((1 2))|+, 1‚ü©|‚àí, 2‚ü©= |‚àí, 1‚ü©, |+, 2‚ü©= |‚àí, +‚ü©.
(10.51)
The 2-cycle (3 4) means x3 ‚Üíx4 ‚Üíx3, it changes (a, b, c, d) into (a, b, d, c).
The 3-cycle (1 2 3) = (2 3 1) = (3 1 2) means x1 ‚Üíx2 ‚Üíx3 ‚Üíx1, it changes
(a, b, c, d) into (b, c, a, d). The 4-cycle (1 3 2 4) means x1 ‚Üíx3 ‚Üíx2 ‚Üíx4 ‚Üí
x1 and changes (a, b, c, d) into (c, d, b, a). The 1-cycle (2) means x2 ‚Üíx2 and
leaves everything unchanged.
The identity element of Sn is the product of 1-cycles e = (1)(2) . . . (n). The
inverse of the cycle (1 3 2 4) must invert x1 ‚Üíx3 ‚Üíx2 ‚Üíx4 ‚Üíx1, so it
must be (1 4 2 3), which means x1 ‚Üíx4 ‚Üíx2 ‚Üíx3 ‚Üíx1 so that it changes
(c, d, b, a) back into (a, b, c, d). Every element of Sn has each integer from 1 to n
in one and only one cycle. So an arbitrary element of Sn with ‚Ñìk k-cycles must
satisfy
n

k=1
k ‚Ñìk = n.
(10.52)
10.14 Compact and noncompact Lie groups
Imagine rotating an object repeatedly. Notice that the biggest rotation is by an
angle of ¬±œÄ about some axis. The possible angles form a circle; the space of
parameters is a circle. The parameter space of a compact group is compact ‚Äì
closed and bounded. The rotations form a compact group.
Now consider the translations. Imagine moving a pebble to the Sun, then
moving it to the next-nearest star, then moving it to the nearest galaxy. If space
is Ô¨Çat, then there is no limit to how far one can move a pebble. The param-
eter space of a noncompact group is not compact. The translations form a
noncompact group.
We‚Äôll see that compact Lie groups possess unitary representations, with N√óN
unitary matrices D(Œ±), while noncompact ones don‚Äôt.
10.15 Lie algebra
Continuous groups can be very complicated. So one uses not only algebra but
also calculus, and one studies the simplest part of the group ‚Äì the elements
g(dŒ±) that are near the identity e = g(0) for which all Œ±a = 0.
If D(g({Œ±a})) is a representation of a Lie group with parameters {Œ±a}, it gets
tedious to write D(g({Œ±a})) over and over. So instead one writes g(Œ±) = g({Œ±a})
and
D(Œ±) = D(g(Œ±)) = D(g({Œ±a}))
(10.53)
leaving out the explicit mentions both of g and of {Œ±a}.
361

GROUP THEORY
Any matrix D(dŒ±) representing a group element g(dŒ±) that is near the identity
is approximately
D(dŒ±) = I + i

a
ta dŒ±a
(10.54)
where the generators ta of the group are the partial derivatives
ta = ‚àíi
‚àÇ
‚àÇŒ±a
D(Œ±)

Œ±=0
.
(10.55)
The i is inserted so that if the matrices D(Œ±) are unitary, then the generators are
hermitian matrices
t‚Ä†
a = ta.
(10.56)
Compact groups have Ô¨Ånite-dimensional, unitary representations and hermi-
tian generators.
Our formulas will look nicer if we adopt the convention that we sum over all
indices that occur twice in a monomial. That is, we drop the summation symbol
when summing over a repeated index so that (10.54) looks like this
D(dŒ±) = I + i ta dŒ±a.
(10.57)
Unless the parameters Œ±a are redundant, the N(G) generators are linearly
independent. They span a vector space over the real numbers and any linear
combination t = Œ±ata may be called a generator.
By using the Gram‚ÄìSchmidt procedure, we may make the N(G) generators
ta orthogonal with respect to the inner product (1.86)
(ta, tb) = Tr

t‚Ä†
atb

= k Œ¥ab,
(10.58)
in which k is a nonnegative normalization constant that in general depends
upon the representation. The reason why we don‚Äôt normalize the generators
and so make k unity will become apparent shortly.
Since group multiplication is closed, any power gn(dŒ±) ‚ààG, and so we may
take the limit
D(Œ±) = lim
n‚Üí‚àûDn(Œ±/n) =

I + iŒ±ata
n
n
= eiŒ±ata.
(10.59)
This parametrization of a representation of a group is called the exponential
parametrization.
Now for tiny œµ the product
eiœµtb eiœµta e‚àíiœµtb e‚àíiœµta ‚âà

1 + iœµ tb ‚àíœµ2
2 t2
b
 
1 + iœµ ta ‚àíœµ2
2 t2
a

√ó

1 ‚àíiœµ tb ‚àíœµ2
2 t2
b
 
1 ‚àíiœµ ta ‚àíœµ2
2 t2
a

(10.60)
362

10.15 LIE ALGEBRA
to order œµ2 is
eiœµtb eiœµta e‚àíiœµtb e‚àíiœµta ‚âà1 + œµ2(ta tb ‚àítb ta) = 1 + œµ2[ta, tb].
(10.61)
Since this product represents a group element near the identity, the commutator
must be a linear combination of generators of order œµ2
eiœµtb eiœµta e‚àíiœµtb e‚àíiœµta = eiœµ2f c
abtc ‚âà1 + iœµ2 f c
ab tc.
(10.62)
By matching (10.61) with (10.62) we have
[ta, tb] = i f c
ab tc.
(10.63)
The numbers f c
ab are the structure constants of the group G.
By taking the trace of equation (10.63) multiplied by t‚Ä†
d and by using the
orthogonality relation (10.58), we Ô¨Ånd
Tr

[ta, tb] t‚Ä†
d

= i f c
ab Tr

tc t‚Ä†
d

= i f c
ab k Œ¥cd = ik f d
ab,
(10.64)
which implies that the structure constant f c
ab is the trace
f c
ab =

‚àíi
k

Tr

[ta, tb] t‚Ä†
c

.
(10.65)
Because of the antisymmetry of the commutator [ta, tb], the structure constant
f c
ab is antisymmetric in its lower indices
f c
ab = ‚àíf c
ba.
(10.66)
From any n √ó n matrix A, one may make a hermitian matrix A + A‚Ä† and an
antihermitian one A ‚àíA‚Ä†. Thus, one may separate the N(G) generators into a
set that are hermitian t(h)
a
and a set that are antihermitian t(ah)
a
. The exponential
of any imaginary linear combination of n √ó n hermitian generators D(Œ±) =
exp

iŒ±a t(h)
a

is an n √ó n unitary matrix since
D‚Ä†(Œ±) = exp

‚àíiŒ±a t‚Ä†(h)
a

= exp

‚àíiŒ±a t(h)
a

= D‚àí1(Œ±).
(10.67)
A group with only hermitian generators is compact and has Ô¨Ånite-dimensional
unitary representations.
On the other hand, the exponential of any imaginary linear combination of
antihermitian generators D(Œ±) = exp

iŒ±a t(ah)
a

is a real exponential of their
hermitian counterparts i t(ah)
a
whose squared norm
‚à•D(Œ±)‚à•2 = Tr

D(Œ±)‚Ä†D(Œ±)

= Tr

exp

2Œ±a it(ah)
a

(10.68)
grows exponentially and without limit as the parameters Œ±a ‚Üí¬± ‚àû. A group
with some antihermitian generators is noncompact and does not have
363

GROUP THEORY
Ô¨Ånite-dimensional unitary representations. (The unitary representations of the
translations and of the Lorentz and Poincar√© groups are inÔ¨Ånite dimensional.)
Compact Lie groups have hermitian generators, and so the structure-
constant formula (10.65) reduces in this case to
f c
ab = (‚àíi/k)Tr

[ta, tb] t‚Ä†
c

= (‚àíi/k)Tr ([ta, tb] tc) .
(10.69)
Now, since the trace is cyclic, we have
f b
ac = (‚àíi/k)Tr ([ta, tc] tb) = (‚àíi/k)Tr (tatctb ‚àítctatb)
= (‚àíi/k)Tr (tbtatc ‚àítatbtc)
= (‚àíi/k)Tr ([tb, ta] tc) = f c
ba = ‚àíf c
ab.
(10.70)
Interchanging a and b, we get
f a
bc = f c
ab = ‚àíf c
ba.
(10.71)
Finally, interchanging b and c gives
f c
ab = f b
ca = ‚àíf b
ac.
(10.72)
Combining (10.70, 10.71, & 10.72), we see that the structure constants of a
compact Lie group are totally antisymmetric
f b
ac = ‚àíf b
ca = f c
ba = ‚àíf c
ab = ‚àíf a
bc = f a
cb.
(10.73)
Because of this antisymmetry, it is usual to lower the upper index
fabc ‚â°f c
ab
(10.74)
and write the antisymmetry of the structure constants of compact Lie groups
more simply as
facb = ‚àífcab = fbac = ‚àífabc = ‚àífbca = fcba.
(10.75)
For compact Lie groups, the generators are hermitian, and so the struc-
ture constants fabc are real, as we may see by taking the complex conjugate of
equation (10.69)
f ‚àó
abc = (i/k)Tr (tc [tb, ta]) = (‚àíi/k)Tr ([ta, tb] tc) = fabc.
(10.76)
All the representations of a given group must obey the same multipli-
cation law, that of the group. Thus in exponential parametrization, if the
representation D1 satisÔ¨Åes (10.62)
eiœµtb eiœµta e‚àíiœµtb e‚àíiœµta ‚âàeiœµ2f c
abtc,
(10.77)
364

10.15 LIE ALGEBRA
that is, if with œµa being the vector with kth component œµŒ¥ak and œµb being the
vector with kth component œµŒ¥bk, we have
D1(œµb) D1(œµa) D1(‚àíœµb) D1(‚àíœµa) ‚âàD1(œµ2f c
ab)
(10.78)
then any other representation D2 must satisfy the same relation with 2
replacing 1:
D2(œµb)D2(œµa)D2(‚àíœµb)D2(‚àíœµa) ‚âàD2(œµ2f c
ab).
(10.79)
Such uniformity will occur if the structure constants (10.65) are the same for
all representations of a compact or a noncompact Lie group. To ensure that this
is so, we must allow each representation Dr(Œ±) to have its own normalization
parameter kr in the trace relation (10.65). The structure constants fabc then are
a property of the group G and are independent of the particular representation
Dr(Œ±). This is why we didn‚Äôt make the generators ta orthonormal.
It follows from (10.63 & 10.74‚Äì10.76) that the commutator of any two
generators of a Lie group is a linear combination
[ta, tb] = i f c
ab tc
(10.80)
of its generators tc, and that the structure constants fabc = f c
ab are real and totally
antisymmetric if the group is compact.
Example 10.16 (Gauge transformation)
The action density of a Yang‚ÄìMills
theory is unchanged when a space-time dependent unitary matrix U(x) changes
a vector œà(x) of matter Ô¨Åelds to œà‚Ä≤(x) = U(x)œà(x). Terms like œà‚Ä†œà are invari-
ant because œà‚Ä†(x)U‚Ä†(x)U(x)œà(x) = œà‚Ä†(x)œà(x), but how can kinetic terms like
‚àÇiœà‚Ä† ‚àÇiœà be made invariant? Yang and Mills introduced matrices Ai of gauge
Ô¨Åelds, replaced ordinary derivatives ‚àÇi by covariant derivatives Di ‚â°‚àÇi + Ai, and
required that D‚Ä≤
iœà‚Ä≤ = UDiœà or that

‚àÇi + A‚Ä≤
i

U = ‚àÇiU + U‚àÇi + A‚Ä≤
iU = U (‚àÇi + Ai) .
(10.81)
Their nonabelian gauge transformation is
œà‚Ä≤(x) = U(x)œà(x)
A‚Ä≤
i(x) = U(x)Ai(x)U‚Ä†(x) ‚àí(‚àÇiU(x)) U‚Ä†(x).
(10.82)
One often writes the unitary matrix as U(x) = exp(‚àíig Œ∏a(x) ta) in which g is
a coupling constant, the functions Œ∏a(x) parametrize the gauge transformation,
and the generators ta belong to the representation that acts on the vector œà(x)
of matter Ô¨Åelds.
365

GROUP THEORY
10.16 The rotation group
The rotations and reÔ¨Çections in three-dimensional space form a compact group
O(3) whose elements R are real 3 √ó 3 matrices that leave invariant the dot-
product of any two 3-vectors
(Rx) ¬∑ (Ry) = xTRTR y = xTIy = x ¬∑ y.
(10.83)
These matrices therefore are orthogonal (1.168)
RTR = I.
(10.84)
Taking the determinant of both sides and using the transpose (1.194) and
product (1.207) rules, we have
(det R)2 = 1
(10.85)
whence det R = ¬±1. The subgroup with det R = 1 is the group SO(3). An SO(3)
element near the identity R = I + œâ must satisfy
(I + œâ) T (I + œâ) = I.
(10.86)
Neglecting the tiny quadratic term, we Ô¨Ånd that the inÔ¨Ånitesimal matrix œâ is
antisymmetric
œâT = ‚àíœâ.
(10.87)
One complete set of real 3 √ó 3 antisymmetric matrices is
œâ1 =
‚éõ
‚éù
0
0
0
0
0
‚àí1
0
1
0
‚éû
‚é†,
œâ2 =
‚éõ
‚éù
0
0
1
0
0
0
‚àí1
0
0
‚éû
‚é†,
œâ3 =
‚éõ
‚éù
0
‚àí1
0
1
0
0
0
0
0
‚éû
‚é†, (10.88)
which we may write as
[œâb]ac = œµabc,
(10.89)
in which œµabc is the Levi-Civita symbol, which is totally antisymmetric with
œµ123 = 1 (Tullio Levi-Civita, 1873‚Äì1941). The œâb are antihermitian, but we
make them hermitian by multiplying by i
tb = i œâb
(10.90)
so that R = I ‚àíiŒ∏b tb.
The three hermitian generators ta satisfy (exercise 10.15) the commutation
relations
[ta, tb] = i fabc tc
(10.91)
in which the structure constants are given by the Levi-Civita symbol œµabc
fabc = œµabc
(10.92)
366

10.16 THE ROTATION GROUP
so that
[ta, tb] = iœµabc tc.
(10.93)
They are the generators of SO(3) in the adjoint representation (section 10.20).
Physicists usually scale the generators by ¬Øh and deÔ¨Åne the angular-
momentum generator La as
La = ¬Øhta
(10.94)
so that the eigenvalues of the angular-momentum operators are the physical
values of the angular momenta. With ¬Øh, the commutation relations are
[La, Lb] = i¬Øh œµabc Lc.
(10.95)
The matrix that represents a right-handed rotation (of an object) of angle Œ∏ =
|Œ∏| about an axis Œ∏ is
D(Œ∏) = e‚àíiŒ∏¬∑t = e‚àíiŒ∏¬∑L/¬Øh,
(10.96)
By using the fact (1.264) that a matrix obeys its characteristic equation, one
may show (exercise 10.17) that the 3 √ó 3 matrix D(Œ∏) that represents a right-
handed rotation of Œ∏ radians about the axis Œ∏ is
Dij(Œ∏) = cos Œ∏ Œ¥ij ‚àísin Œ∏ œµijk Œ∏k/Œ∏ + (1 ‚àícos Œ∏) Œ∏iŒ∏j/Œ∏2,
(10.97)
in which a sum over k = 1, 2, 3 is understood.
Example 10.17 (Demonstration of commutation relations)
Take a big sphere
with a distinguished point and orient the sphere so that the point lies in the y-
direction from the center of the sphere. Now rotate the sphere by a small angle,
say 15 degrees or œµ = œÄ/12, right-handedly about the x-axis, then right-handedly
about the y-axis by the same angle, then left-handedly about the x-axis and
then left-handedly about the y-axis. These rotations amount to a smaller, left-
handed rotation about the (vertical) z-axis in accordance with equation (10.77)
with ¬Øhta = L1 = Lx, ¬Øhtb = L2 = Ly, and ¬Øhfabctc = œµ12cLc = L3 = Lz
eiœµLy/¬Øh eiœµLx/¬Øh e‚àíiœµLy/¬Øh e‚àíiœµLx/¬Øh ‚âàeiœµ2Lz/¬Øh.
(10.98)
The magnitude of that rotation should be about œµ2 = (œÄ/12)2 ‚âà0.069 or about
3.9 degrees. Photographs of an actual demonstration are displayed in Fig. 10.1.
By expanding both sides of the demonstrated equation (10.98) in powers
of œµ and keeping only the biggest terms that don‚Äôt cancel, you may show
(exercise 10.16) that the generators Lx and Ly satisfy the commutation relation
[Lx, Ly] = i¬ØhLz
(10.99)
of the rotation group.
367

GROUP THEORY
Physical demonstration of the commutation relations
Figure 10.1
Demonstration of equation (10.98) and the commutation relation
(10.99). Upper left: black ball with a white stick pointing in the y-direction; the
x-axis is to the reader‚Äôs left, the z-axis is vertical. Upper right: ball after a small
right-handed rotation about the x-axis. Center left: ball after that rotation is followed
by a small right-handed rotation about the y-axis. Center right: ball after these rota-
tions are followed by a small left-handed rotation about the x-axis. Bottom: ball after
these rotations are followed by a small left-handed rotation about the y-axis. The net
effect is approximately a small left-handed rotation about the z-axis.
10.17 The Lie algebra and representations of SU(2)
The three generators of SU(2) in its 2 √ó 2 deÔ¨Åning representation are the Pauli
matrices divided by 2, ta = œÉa/2. The structure constants of SU(2) are fabc =
œµabc, which is totally antisymmetric with œµ123 = 1
368

10.17 THE LIE ALGEBRA AND REPRESENTATIONS OF SU(2)
[ta, tb] = ifabctc =
œÉa
2 , œÉb
2

= iœµabc
œÉc
2 .
(10.100)
For every half-integer
j = n
2
for
n = 0, 1, 2, 3, . . .
(10.101)
there is an irreducible representation of SU(2)
D( j)(Œ∏) = e‚àíiŒ∏¬∑J( j),
(10.102)
in which the three generators t( j)
a
‚â°J( j)
a
are (2j + 1) √ó (2j + 1) square hermitian
matrices. In a basis in which J( j)
3
is diagonal, the matrix elements of the complex
linear combinations J( j)
¬± ‚â°J( j)
1
¬± iJ( j)
2
are

J( j)
1
¬± iJ( j)
2

s‚Ä≤,s = Œ¥s‚Ä≤,s¬±1

( j ‚àìs)( j ¬± s + 1),
(10.103)
where s and s‚Ä≤ run from ‚àíj to j in integer steps, and those of J( j)
3
are

J( j)
3

s‚Ä≤,s = s Œ¥s‚Ä≤,s.
(10.104)
The sum of the squares of the three generators J( j)
a
is a multiple of the
(2j + 1) √ó (2j + 1) identity matrix

J( j)
a
2
= j( j + 1) I.
(10.105)
Combinations of generators that are a multiple of the identity are called Casimir
operators.
Example 10.18 (Spin-two)
For j = 2, the spin-two matrices J(2)
+ and J(2)
3
are
J(2)
+ =
‚éõ
‚éú‚éú‚éú‚éú‚éù
0
2
0
0
0
0
0
‚àö
6
0
0
0
0
0
‚àö
6
0
0
0
0
0
2
0
0
0
0
0
‚éû
‚éü‚éü‚éü‚éü‚é†
and
J(2)
3
=
‚éõ
‚éú‚éú‚éú‚éú‚éù
2
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
‚àí1
0
0
0
0
0
‚àí2
‚éû
‚éü‚éü‚éü‚éü‚é†
(10.106)
and J‚àí=

J(2)
+
‚Ä†
.
The tensor product of any two irreducible representations D( j) and D(k) of
SU(2) is equivalent to the direct sum of all the irreducible representations D‚Ñì
for | j ‚àík| ‚â§‚Ñì‚â§j + k
369

GROUP THEORY
D( j) ‚äóD(k) =
j+k
5
‚Ñì=| j‚àík|
D‚Ñì,
(10.107)
each D‚Ñìoccurring once.
Under a rotation R, a Ô¨Åeld œà‚Ñì(x) that transforms under the D( j) representa-
tion of SU(2) responds as
U(R) œà‚Ñì(x) U‚àí1(R) = D( j)
‚Ñì‚Ñì‚Ä≤(R‚àí1) œà‚Ñì‚Ä≤(Rx).
(10.108)
Example 10.19 (Spin and statistics)
Suppose |a, m‚ü©and |b, m‚ü©are any eigen-
states of the rotation operator J3 with eigenvalue m (in units with ¬Øh = c = 1).
Let u and v be any two points whose separation u ‚àív is space-like (u ‚àív)2 > 0.
Then, in some Lorentz frame, the two points are at the same time t, and we may
chose our coordinate system so that u‚Ä≤ = (t, x, 0, 0) and v‚Ä≤ = (t, ‚àíx, 0, 0). Let U
be the unitary operator that represents a right-handed rotation by œÄ about the
3-axis or z-axis of this Lorentz frame. Then
U|a, m‚ü©= e‚àíimœÄ|a, m‚ü©
and
‚ü®b, m|U‚àí1 = ‚ü®b, m|eimœÄ.
(10.109)
And by (10.108), U transforms a Ô¨Åeld œà of spin j with x ‚â°(x, 0, 0) to
U(R) œà‚Ñì(t, x) U‚àí1(R) = D( j)
‚Ñì‚Ñì‚Ä≤(R‚àí1) œà‚Ñì‚Ä≤(t, ‚àíx) = eiœÄ‚Ñìœà‚Ñì(t, ‚àíx).
(10.110)
Thus by inserting the identity operator in the form I = U‚àí1U and using both
(10.109) and (10.108), we Ô¨Ånd, since the phase factors exp(‚àíimœÄ) and exp(imœÄ)
cancel,
‚ü®b, m|œà‚Ñì(t, x) œà‚Ñì(t, ‚àíx)|a, m‚ü©= ‚ü®b, m|Uœà‚Ñì(t, x)U‚àí1Uœà‚Ñì(t, ‚àíx)U‚àí1|a, m‚ü©
= e2iœÄ‚Ñì‚ü®b, m|œà‚Ñì(t, ‚àíx)œà‚Ñì(t, x)|a, m‚ü©.
(10.111)
Now if j is an integer, then so is ‚Ñì, and the phase factor exp(2iœÄ‚Ñì) = 1 is
unity. In this case, we Ô¨Ånd that the mean value of the equal-time commutator
vanishes
‚ü®b, m|[œà‚Ñì(t, x), œà‚Ñì(t, ‚àíx)]|a, m‚ü©= 0.
(10.112)
On the other hand, if j is half an odd integer, that is, j = (2n + 1)/2, where n is
an integer, then the phase factor exp(2iœÄ‚Ñì) is ‚àí1. In this case, the mean value of
the equal-time anticommutator vanishes
‚ü®b, m|{œà‚Ñì(t, x), œà‚Ñì(t, ‚àíx)}|a, m‚ü©= 0.
(10.113)
While not a proof of the spin-statistics theorem, this argument shows that the
behavior of Ô¨Åelds under rotations does determine their statistics.
370

10.18 THE DEFINING REPRESENTATION OF SU(2)
10.18 The deÔ¨Åning representation of SU(2)
The smallest positive value of angular momentum is ¬Øh/2. The spin-one-half
angular momentum operators are represented by three 2 √ó 2 matrices
Sa = ¬Øh
2 œÉa,
(10.114)
in which the œÉa are the Pauli matrices
œÉ1 =
0
1
1
0

,
œÉ2 =
0
‚àíi
i
0

,
and
œÉ3 =
1
0
0
‚àí1

,
(10.115)
which obey the multiplication law
œÉiœÉj = Œ¥ij + i
3

k=1
œµijk œÉk.
(10.116)
The Pauli matrices divided by 2 satisfy the commutation relations (10.93) of
the rotation group
#1
2œÉa, 1
2œÉb
$
= iœµabc
1
2œÉc
(10.117)
and generate the elements of the group SU(2)
exp

i Œ∏ ¬∑ œÉ
2

= I cos Œ∏
2 + i ÀÜŒ∏ ¬∑ œÉ sin Œ∏
2,
(10.118)
in which I is the 2√ó2 identity matrix, Œ∏ =

Œ∏2 and ÀÜŒ∏ = Œ∏/Œ∏.
It follows from (10.117) that the spin operators satisfy
[Sa, Sb] = i¬Øhœµabc Sc.
(10.119)
The raising and lowering operators
S¬± = S1 ¬± iS2
(10.120)
have simple commutators with S3
[S3, S¬±] = ¬±i¬ØhS¬±.
(10.121)
This relation implies that if the state | j, m‚ü©is an eigenstate of S3 with eigen-
value ¬Øhm, then the states S¬±| j, m‚ü©either vanish or are eigenstates of S3 with
eigenvalues ¬Øh(m ¬± 1)
S3S¬±| j, m‚ü©= S¬±S3| j, m‚ü©¬± i¬ØhS¬±| j, m‚ü©= ¬Øh(m ¬± 1)S¬±| j, m‚ü©.
(10.122)
Thus the raising and lowering operators raise and lower the eigenvalues of S3.
When j = 1/2, the possible values of m are m = ¬±1/2, and so with the usual
sign and normalization conventions
371

GROUP THEORY
S+|‚àí‚ü©= ¬Øh|+‚ü©
and
S‚àí|+‚ü©= ¬Øh|‚àí‚ü©
(10.123)
while
S+|+‚ü©= 0
and
S‚àí|‚àí‚ü©= 0.
(10.124)
The square of the total spin operator is simply related to the raising and
lowering operators and to S3
S2 = S2
1 + S2
2 + S2
3 = 1
2S+S‚àí+ 1
2S‚àíS+ + S2
3.
(10.125)
But the squares of the Pauli matrices are unity, and so S2
a = (¬Øh/2)2 for all three
values of a. Thus
S2 = 3
4 ¬Øh2
(10.126)
is a Casimir operator (10.105) for a spin-one-half system.
Example 10.20 (Two spin-one-half systems)
Consider two spin operators S(1)
and S(2) as deÔ¨Åned by (10.114) acting on two spin-one-half systems. Let the
tensor-product states
|¬±, ¬±‚ü©= |¬±‚ü©1|¬±‚ü©2 = |¬±‚ü©1 ‚äó|¬±‚ü©2
(10.127)
be eigenstates of S(1)
3
and S(2)
3
so that
S(1)
3 |+, ¬±‚ü©= ¬Øh
2 |+, ¬±‚ü©
and
S(2)
3 |¬±, +‚ü©= ¬Øh
2 |¬±, +‚ü©,
S(1)
3 |‚àí, ¬±‚ü©= ‚àí¬Øh
2 |‚àí, ¬±‚ü©
and
S(2)
3 |¬±, ‚àí‚ü©= ‚àí¬Øh
2 |¬±, ‚àí‚ü©. (10.128)
The total spin of the system is the sum of the two spins S = S(1) + S(2), so
S2 =

S(1) + S(2)2
and
S3 = S(1)
3
+ S(2)
3 .
(10.129)
The state |+, +‚ü©is an eigenstate of S3 with eigenvalue ¬Øh
S3|+, +‚ü©= S(1)
3 |+, +‚ü©+ S(2)
3 |+, +‚ü©
= ¬Øh
2|+, +‚ü©+ ¬Øh
2|+, +‚ü©= ¬Øh|+, +‚ü©.
(10.130)
So the state of angular momentum ¬Øh in the 3-direction is |1, 1‚ü©= |+, +‚ü©.
Similarly, the state |‚àí, ‚àí‚ü©is an eigenstate of S3 with eigenvalue ‚àí¬Øh
S3|‚àí, ‚àí‚ü©= S(1)
3 |‚àí, ‚àí‚ü©+ S(2)
3 |‚àí, ‚àí‚ü©
= ‚àí¬Øh
2|‚àí, ‚àí‚ü©‚àí¬Øh
2|‚àí, ‚àí‚ü©= ‚àí¬Øh|‚àí, ‚àí‚ü©
(10.131)
372

10.18 THE DEFINING REPRESENTATION OF SU(2)
and so the state of angular momentum ¬Øh in the negative 3-direction is |1, ‚àí1‚ü©=
|‚àí, ‚àí‚ü©. The states |+, ‚àí‚ü©and |‚àí, +‚ü©are eigenstates of S3 with eigenvalue 0
S3|+, ‚àí‚ü©= S(1)
3 |+, ‚àí‚ü©+ S(2)
3 |+, ‚àí‚ü©= ¬Øh
2|+, ‚àí‚ü©‚àí¬Øh
2|+, ‚àí‚ü©= 0,
S3|‚àí, +‚ü©= S(1)
3 |‚àí, +‚ü©+ S(2)
3 |‚àí, +‚ü©= ‚àí¬Øh
2|‚àí, +‚ü©+ ¬Øh
2|‚àí, +‚ü©= 0. (10.132)
To see which states are eigenstates of S2, we use the lowering operator for the
combined system S‚àí= S(1)
‚àí+ S(2)
‚àíand the rules (10.103, 10.123, & 10.124) to
lower the state |1, 1‚ü©
S‚àí|+, +‚ü©=

S(1)
‚àí+ S(2)
‚àí

|+, +‚ü©= ¬Øh (|‚àí, +‚ü©+ |+, ‚àí‚ü©) = ¬Øh
‚àö
2 |1, 0‚ü©.
Thus the state |1, 0‚ü©is
|1, 0‚ü©=
1
‚àö
2
(|+, ‚àí‚ü©+ |‚àí, +‚ü©) .
(10.133)
The orthogonal and normalized combination of |+, ‚àí‚ü©and |‚àí, +‚ü©must be the
state of spin zero
|0, 0‚ü©=
1
‚àö
2
(|+, ‚àí‚ü©‚àí|‚àí, +‚ü©)
(10.134)
with the usual sign convention.
To check that the states |1, 0‚ü©and |0, 0‚ü©really are eigenstates of S2, we use
(10.125 & 10.126) to write S2 as
S2 =

S(1) + S(2)2
= 3
2 ¬Øh2 + 2S(1) ¬∑ S(2)
= 3
2 ¬Øh2 + S(1)
+ S(2)
‚àí+ S(1)
‚àíS(2)
+ + 2S(1)
3 S(2)
3 .
(10.135)
Now the sum S(1)
+ S(2)
‚àí+ S(1)
‚àíS(2)
+ merely interchanges the states |+, ‚àí‚ü©and |‚àí, +‚ü©
and multiplies them by ¬Øh2, so
S2|1, 0‚ü©= 3
2 ¬Øh2|1, 0‚ü©+ ¬Øh2|1, 0‚ü©‚àí2
4 ¬Øh2|1, 0‚ü©
= 2¬Øh2|1, 0‚ü©= s(s + 1)¬Øh2|1, 0‚ü©,
(10.136)
which conÔ¨Årms that s = 1. Because of the relative minus sign in formula (10.134)
for the state |0, 0‚ü©, we have
S2|0, 0‚ü©= 3
2 ¬Øh2|0, 0‚ü©‚àí¬Øh2|1, 0‚ü©‚àí1
2 ¬Øh2|1, 0‚ü©
= 0¬Øh2|1, 0‚ü©= s(s + 1)¬Øh2|1, 0‚ü©,
(10.137)
which conÔ¨Årms that s = 0.
373

GROUP THEORY
10.19 The Jacobi identity
Any three square matrices A, B, and C satisfy the commutator-product rule
[A, BC] = ABC ‚àíBCA = ABC ‚àíBAC + BAC ‚àíBCA
= [A, B]C + B[A, C].
(10.138)
Interchanging B and C gives
[A, CB] = [A, C]B + C[A, B].
(10.139)
Subtracting the second equation from the Ô¨Årst, we get the Jacobi identity
[A, [B, C]] = [[A, B], C] + [B, [A, C]]
(10.140)
and its equivalent cyclic form
[A, [B, C]] + [B, [C, A]] + [C, [A, B]] = 0.
(10.141)
Another Jacobi identity uses the anticommutator {A, B} ‚â°AB + BA
{[A, B], C} + {[A, C], B} + [{B, C}, A] = 0.
(10.142)
10.20 The adjoint representation
Any three generators ta, tb, and tc satisfy the Jacobi identity (10.141)
[ta, [tb, tc]] + [tb, [tc, ta]] + [tc, [ta, tb]] = 0.
(10.143)
By using the structure-constant formula (10.80), we may express each of these
double commutators as a linear combination of the generators
[ta, [tb, tc]] = [ta, if d
bctd] = ‚àíf d
bc f e
adte,
[tb, [tc, ta]] = [tb, if d
catd] = ‚àíf d
ca f e
bdte,
[tc, [ta, tb]] = [tc, if d
abtd] = ‚àíf d
ab f e
cdte.
(10.144)
So the Jacobi identity (10.143) implies that

f d
bc f e
ad + f d
ca f e
bd + f d
ab f e
cd

te = 0
(10.145)
or since the generators are linearly independent
f d
bc f e
ad + f d
ca f e
bd + f d
ab f e
cd = 0.
(10.146)
If we deÔ¨Åne a set of matrices Ta by
(Tb)ac = i f c
ab
(10.147)
374

10.21 CASIMIR OPERATORS
then, since the structure constants are antisymmetric in their lower indices, we
may write the three terms in the preceding equation (10.146) as
f d
bc f e
ad = f d
cb f e
da = (‚àíTbTa)ce,
(10.148)
f d
ca f e
bd = ‚àíf d
ca f e
db = (TaTb)ce,
(10.149)
and
f d
ab f e
cd = ‚àíif d
ab(Td)ce
(10.150)
or in matrix notation
[Ta, Tb] = if c
abTc.
(10.151)
So the matrices Ta, which we made out of the structure constants by the rule
(Tb)ac = ifabc (10.147), obey the same algebra (10.63) as do the generators
ta. They are the generators in the adjoint representation of the Lie algebra. If
the Lie algebra has N generators ta, then the N generators Ta in the adjoint
representation are N √ó N matrices.
10.21 Casimir operators
For any compact Lie algebra, the sum of the squares of all the generators
C =
N

n=1
tata ‚â°tata
(10.152)
commutes with every generator tb
[C, tb] = [tata, tb] = [ta, tb]ta + ta[ta, tb]
= ifabctcta + taifabctc = i (fabc + fcba) tcta = 0
(10.153)
because of the total antisymmetry (10.75) of the structure constants. This sum,
called a Casimir operator, commutes with every matrix
[C, D(Œ±)] = [C, exp(iŒ±ata)] = 0
(10.154)
of the representation generated by the tas. Thus by part 2 of Schur‚Äôs lemma
(section 10.7), it must be a multiple of the identity matrix
C = tata = cI.
(10.155)
The constant c depends upon the representation D(Œ±) and is called the quadratic
Casimir.
The generators of some noncompact groups come in pairs ta and ita, and so
the sum of the squares of these generators vanishes, C = tata ‚àítata = 0.
375

GROUP THEORY
10.22 Tensor operators for the rotation group
Suppose A( j)
m is a set of 2j + 1 operators whose commutation relations with the
generators Ji of rotations are
[Ji, A( j)
m ] = A( j)
‚Ñì(J( j)
i )‚Ñìm
(10.156)
in which the sum over ‚Ñìruns from ‚àíj to j. Then A( j) is said to be a spin-j tensor
operator for the group SU(2).
Example 10.21 (A spin-one tensor operator)
For instance, if j = 1, then
(J(1)
i )‚Ñìm = i¬Øhœµ‚Ñìim, and so a spin-one tensor operator of SU(2) is a vector A(1)
m
that transforms as
[Ji, A(1)
m ] = A(1)
‚Ñìi¬Øhœµ‚Ñìim = i¬Øhœµim‚ÑìA(1)
‚Ñì
(10.157)
under rotations.
Let‚Äôs rewrite the deÔ¨Ånition (10.156) as
JiA( j)
m = A( j)
‚Ñì(J( j)
i )‚Ñìm + A( j)
m Ji
(10.158)
and specialize to the case i = 3 so that (J( j)
3 )‚Ñìm is diagonal, (J( j)
3 )‚Ñìm = ¬ØhmŒ¥‚Ñìm
J3A( j)
m = A( j)
‚Ñì(J( j)
3 )‚Ñìm+A( j)
m J3 = A( j)
‚Ñì¬ØhmŒ¥‚Ñìm+A( j)
m J3 = A( j)
m (¬Øhm + J3) . (10.159)
Thus if the state | j, m‚Ä≤, E‚ü©is an eigenstate of J3 with eigenvalue ¬Øhm‚Ä≤, then the
state A( j)
m | j, m‚Ä≤, E‚ü©is an eigenstate of J3 with eigenvalue ¬Øh(m + m‚Ä≤)
J3A( j)
m | j, m‚Ä≤, E‚ü©= A( j)
m (¬Øhm + J3) | j, m‚Ä≤, E‚ü©= ¬Øh

m + m‚Ä≤
A( j)
m | j, m‚Ä≤, E‚ü©.
(10.160)
The J3 eigenvalues of the tensor operator A( j)
m and the state | j, m‚Ä≤, E‚ü©add.
10.23 Simple and semisimple Lie algebras
An invariant subalgebra is a set of generators t(i)
a whose commutator with every
generator tb of the group is a linear combination of the t(i)
c s
[t(i)
a , tb] = i fabct(i)
c .
(10.161)
The whole algebra and the null algebra are trivial invariant subalgebras.
An algebra with no nontrivial invariant subalgebras is a simple algebra. A
simple algebra generates a simple group. An algebra that has no nontrivial
abelian invariant subalgebras is a semisimple algebra. A semisimple algebra
generates a semisimple group.
376

10.24 SU(3)
Example 10.22 (Some simple Lie groups)
The groups of unitary matrices of
unit determinant SU(2), SU(3), ... are simple. So are the groups of orthogonal
matrices of unit determinant SO(2), SO(3), ... and the groups of symplectic
matrices Sp(2), Sp(4), .. .(section 10.28).
Example 10.23 (UniÔ¨Åcation and grand uniÔ¨Åcation)
The symmetry group of
the standard model of particle physics is a direct product of an SU(3) group that
acts on colored Ô¨Åelds, an SU(2) group that acts on left-handed quark and lep-
ton Ô¨Åelds, and a U(1) group that acts on Ô¨Åelds that carry hypercharge. Each
of these three groups is an invariant subgroup of the full symmetry group
SU(3)c ‚äóSU(2)‚Ñì‚äóU(1)Y, and the last one is abelian. Thus the symmetry
group of the standard model is neither simple nor semisimple. A simple sym-
metry group relates all its quantum numbers, and so physicists have invented
grand uniÔ¨Åcation in which a simple symmetry group G contains the symmetry
group of the standard model. Georgi and Glashow suggested the group SU(5) in
1976 (Howard Georgi, 1947‚Äì ; Sheldon Glashow, 1932‚Äì ). Others have proposed
SO(10) and even bigger groups.
10.24 SU(3)
The Gell-Mann matrices are
Œª1 =
‚éõ
‚éù
0
1
0
1
0
0
0
0
0
‚éû
‚é†,
Œª2 =
‚éõ
‚éù
0
‚àíi
0
i
0
0
0
0
0
‚éû
‚é†,
Œª3 =
‚éõ
‚éù
1
0
0
0
‚àí1
0
0
0
0
‚éû
‚é†,
Œª4 =
‚éõ
‚éù
0
0
1
0
0
0
1
0
0
‚éû
‚é†,
Œª5 =
‚éõ
‚éù
0
0
‚àíi
0
0
0
i
0
0
‚éû
‚é†,
Œª6 =
‚éõ
‚éù
0
0
0
0
0
1
0
1
0
‚éû
‚é†,
Œª7 =
‚éõ
‚éù
0
0
0
0
0
‚àíi
0
i
0
‚éû
‚é†,
and
Œª8 =
1
‚àö
3
‚éõ
‚éù
1
0
0
0
1
0
0
0
‚àí2
‚éû
‚é†.
(10.162)
The generators ta of the 3 √ó 3 deÔ¨Åning representation of SU(3) are these Gell-
Mann matrices divided by 2
ta = Œªa/2
(10.163)
(Murray Gell-Mann, 1929‚Äì).
The eight generators ta are orthogonal with k = 1/2
Tr (tatb) = 1
2Œ¥ab
(10.164)
377

GROUP THEORY
and satisfy the commutation relation
[ta, tb] = ifabc tc.
(10.165)
The trace formula (10.65) gives us the SU(3) structure constants as
fabc = ‚àí2iTr ([ta, tb]tc) .
(10.166)
They are real and totally antisymmetric with f123 = 1, f458 = f678 =
‚àö
3/2, and
f147 = ‚àíf156 = f246 = f257 = f345 = ‚àíf367 = 1/2.
While no two generators of SU(2) commute, two generators of SU(3) do. In
the representation (10.162, 10.163), t3 and t8 are diagonal and so commute
[t3, t8] = 0.
(10.167)
They generate the Cartan subalgebra (section 10.26) of SU(3).
10.25 SU(3) and quarks
The generators deÔ¨Åned by equations (10.163 & 10.162) give us the 3 √ó 3
representation
D(Œ±) = exp (iŒ±ata)
(10.168)
in which the sum a = 1, 2, . . . , 8 is over the eight generators ta. This representa-
tion acts on complex 3-vectors and is called the 3.
Note that if
D(Œ±1)D(Œ±2) = D(Œ±3)
(10.169)
then the complex conjugates of these matrices obey the same multiplication rule
D‚àó(Œ±1)D‚àó(Œ±2) = D‚àó(Œ±3)
(10.170)
and so form another representation of SU(3). It turns out that (unlike in SU(2))
this representation is inequivalent to the 3; it is the 3.
There are three quarks with masses less than about 100 MeV/c2 ‚Äì the u, d,
and s quarks. The other three quarks c, b, and t are more massive by factors of
12, 45, and 173. Nobody knows why. Gell-Mann suggested that the low-energy
strong interactions were approximately invariant under unitary transforma-
tions of the three light quarks, which he represented by a 3, and of the three
light antiquarks, which he represented by a 3. He imagined that the eight light
pseudo-scalar mesons, that is, the three pions œÄ‚àí, œÄ0, œÄ+, the neutral Œ∑, and
the four kaons K0, K+, K‚àí, K
0, were composed of a quark and an antiquark.
So they should transform as the tensor product
3 ‚äó3 = 8 ‚äï1.
(10.171)
He put the eight pseudo-scalar mesons into an 8.
378

10.27 QUATERNIONS
He imagined that the eight light baryons ‚Äì the two nucleons N and P, the
three sigmas ‚àí, 0, +, the neutral lambda , and the two cascades ‚àíand
0 ‚Äì were each made of three quarks. They should transform as the tensor
product
3 ‚äó3 ‚äó3 = 10 ‚äï8 ‚äï8 ‚äï1.
(10.172)
He put the eight light baryons into one of these 8s. When he was writing these
papers, there were nine spin-3/2 resonances with masses somewhat heavier than
1200 MeV/c2 ‚Äì four s, three ‚àós, and two ‚àós. He put these into the ten and
predicted the tenth and its mass. When a tenth spin-3/2 resonance, the ‚àí,
was found with a mass close to his prediction of 1680 MeV/c2, his SU(3) the-
ory became wildly popular among high-energy physicists. Within a few years, a
SLAC team had discovered quarks, and Gell-Mann had won the Nobel prize.
10.26 Cartan subalgebra
In any Lie group, the maximum set of mutually commuting generators Ha
generates the Cartan subalgebra
[Ha, Hb] = 0,
(10.173)
which is an abelian subalgebra. The number of generators in the Cartan sub-
algebra is the rank of the Lie algebra. The Cartan generators Ha can be
simultaneously diagonalized, and their eigenvalues or diagonal elements are the
weights
Ha|Œº, x, D‚ü©= Œºa|Œº, x, D‚ü©,
(10.174)
in which D labels the representation and x whatever other variables are needed
to specify the state. The vector Œº is the weight vector. The roots are the weights
of the adjoint representation.
10.27 Quaternions
If z and w are any two complex numbers, then the 2 √ó 2 matrix
q =
 z
w
‚àíw‚àó
z‚àó

(10.175)
is a quaternion. The quaternions are closed under addition and multiplica-
tion and under multiplication by a real number (exercise 10.21), but not under
multiplication by an arbitrary complex number. The squared norm of q is its
determinant
‚à•q‚à•2 = |z|2 + |w|2 = det q.
(10.176)
379

GROUP THEORY
The matrix products q‚Ä†q and q q‚Ä† are the squared norm ‚à•q‚à•2 multiplied by the
2 √ó 2 identity matrix
q‚Ä†q = q q‚Ä† = ‚à•q‚à•2 I.
(10.177)
The 2 √ó 2 matrix
iœÉ2 =
 0
1
‚àí1
0

(10.178)
provides another expression for ‚à•q‚à•2 in terms of q and its transpose qT
qTiœÉ2 q = ‚à•q‚à•2 iœÉ2.
(10.179)
Clearly ‚à•q‚à•= 0 implies q = 0. The norm of a product of quaternions is the
product of their norms
‚à•q1q2‚à•=

det(q1q2) =

det q1 det q2 = ‚à•q1‚à•‚à•q2‚à•.
(10.180)
The quaternions therefore form an associative division algebra (over the real
numbers); the only others are the real numbers and the complex numbers; the
octonions are a nonassociative division algebra.
One may use the Pauli matrices to deÔ¨Åne for any real 4-vector x a quaternion
q(x) as
q(x) = x0 ‚àíiœÉkxk = x0 ‚àíiœÉ ¬∑ x
=
x0 ‚àíix3
‚àíx2 ‚àíix1
x2 ‚àíix1
x0 + ix3

.
(10.181)
The product rule (10.116) for the Pauli matrices tells us that the product of two
quaternions is
q(x) q(y) = (x0 ‚àíiœÉ ¬∑ x)(y0 ‚àíiœÉ ¬∑ y)
= x0y0 ‚àíiœÉ ¬∑ (y0x + x0y) ‚àíi(x √ó y) ¬∑ œÉ ‚àíx ¬∑ y
(10.182)
so their commutator is
[q(x), q(y)] = ‚àí2i(x √ó y) ¬∑ œÉ.
(10.183)
Example 10.24 (Lack of analyticity)
One may deÔ¨Åne a function f (q) of a
quaternionic variable and then ask what functions are analytic in the sense that
the derivative
f ‚Ä≤(q) = lim
q‚Ä≤‚Üí0
f (q + q‚Ä≤) ‚àíf (q)
q‚Ä≤
(10.184)
exists and is independent of the direction through which q‚Ä≤ ‚Üí0. This space of
functions is extremely limited and does not even include the function f (q) = q2
(exercise 10.22).
380

10.28 THE SYMPLECTIC GROUP SP (2N)
10.28 The symplectic group Sp (2n)
The symplectic group Sp(2n) consists of 2n √ó 2n matrices W that map n-tuples
q of quaternions into n-tuples q‚Ä≤ = Wq of quaternions with the same value of
the quadratic quaternionic form
‚à•q‚Ä≤‚à•2 = ‚à•q‚Ä≤
1‚à•2 + ‚à•q‚Ä≤
2‚à•2 + ¬∑ ¬∑ ¬∑ + ‚à•q‚Ä≤
n‚à•2 = ‚à•q1‚à•2 + ‚à•q2‚à•2 + ¬∑ ¬∑ ¬∑ + ‚à•qn‚à•2 = ‚à•q‚à•2.
(10.185)
By (10.177), the quadratic form ‚à•q‚Ä≤‚à•2 times the 2 √ó 2 identity matrix I is equal
to the hermitian form q‚Ä≤‚Ä†q‚Ä≤
‚à•q‚Ä≤‚à•2 I = q‚Ä≤‚Ä†q‚Ä≤ = q‚Ä≤‚Ä†
1 q‚Ä≤
1 + ¬∑ ¬∑ ¬∑ + q‚Ä≤‚Ä†
n q‚Ä≤
n = q‚Ä†W‚Ä†Wq
(10.186)
and so any matrix W that is both a 2n √ó 2n unitary matrix and an n √ó n matrix
of quaternions keeps ‚à•q‚Ä≤‚à•2 = ‚à•q‚à•2
‚à•q‚Ä≤‚à•2 I = q‚Ä†W‚Ä†Wq = q‚Ä†q = ‚à•q‚à•2 I.
(10.187)
The group Sp(2n) thus consists of all 2n√ó2n unitary matrices that also are n√ón
matrices of quaternions. (This last requirement is needed so that q‚Ä≤ = Wq is an
n-tuple of quaternions.)
The generators ta of the symplectic group Sp(2n) are 2n √ó 2n direct-product
matrices of the form
I ‚äóA,
œÉ1 ‚äóS1,
œÉ2 ‚äóS2,
and
œÉ3 ‚äóS3,
(10.188)
in which I is the 2 √ó 2 identity matrix, the three œÉis are the Pauli matrices, A is
an imaginary n √ó n antisymmetric matrix, and the Si are n √ó n real symmetric
matrices. These generators ta close under commutation
[ta, tb] = ifabctc.
(10.189)
Any imaginary linear combination iŒ±ata of these generators is not only a 2n√ó2n
antihermitian matrix but also an n√ón matrix of quaternions. Thus the matrices
D(Œ±) = eiŒ±ata
(10.190)
are both unitary 2n √ó 2n matrices and n √ó n quaternionic matrices and so are
elements of the group Sp(2n).
Example 10.25 (Sp(2) = SU(2))
There is no 1 √ó 1 antisymmetric matrix, and
there is only one 1 √ó 1 symmetric matrix. So the generators ta of the group
Sp(2) are the Pauli matrices ta = œÉa, and Sp(2) = SU(2). The elements g(Œ±) of
SU(2) are quaternions of unit norm (exercise 10.20), and so the product g(Œ±)q is
a quaternion
‚à•g(Œ±)q‚à•2 = det(g(Œ±)q) = det(g(Œ±)) det q = det q = ‚à•q‚à•2
(10.191)
with the same squared norm.
381

GROUP THEORY
Example 10.26 (Sp(4) = SO(5))
Apart from scale factors, there are three real
symmetric 2 √ó 2 matrices S1 = œÉ1, S2 = I, and S3 = œÉ3 and one imaginary
antisymmetric 2√ó2 matrix A = œÉ2. So there are ten generators of Sp(4) = SO(5)
t1 = I ‚äóœÉ2 =
0
‚àíiI
iI
0

, tk1 = œÉk ‚äóœÉ1 =
 0
œÉk
œÉk
0

tk2 = œÉk ‚äóI =
œÉk
0
0
œÉk

, tk3 = œÉk ‚äóœÉ3 =
œÉk
0
0
‚àíœÉk

(10.192)
where k runs from 1 to 3.
We may see Sp(2n) from a different viewpoint if we use (10.179) to write the
quadratic form ‚à•q‚à•2 in terms of a 2n √ó 2n matrix J that has n copies of iœÉ2 on
its 2 √ó 2 diagonal
J =
‚éõ
‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éù
iœÉ2
0
0
0
. . .
0
0
iœÉ2
0
0
. . .
0
0
0
iœÉ2
0
. . .
0
0
0
0
...
. . .
0
...
...
...
...
...
0
0
0
0
0
0
iœÉ2
‚éû
‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚é†
(10.193)
(and zeros elsewhere) as
‚à•q‚à•2J = qTJq.
(10.194)
Thus any n √ó n matrix of quaternions W that satisÔ¨Åes
W TJW = J
(10.195)
also satisÔ¨Åes
‚à•Wq‚à•2J = qTW TJWq = qTJq = ‚à•q‚à•2J
(10.196)
and so leaves invariant the quadratic form (10.185). The group Sp(2n) therefore
consists of all 2n √ó 2n matrices W that satisfy (10.195) and that also are n √ó n
matrices of quaternions.
The symplectic group is something of a physics orphan. Its best-known
application is in classical mechanics, and that application uses the noncompact
symplectic group Sp(2n, R), not the compact symplectic group Sp(2n). The ele-
ments of Sp(2n, R) are all real 2n √ó 2n matrices T that satisfy T TJT = J with
the J of (10.193); those near the identity are of the form T = exp(JS) in which
S is a 2n √ó 2n real symmetric matrix (exercise 10.24).
382

10.29 COMPACT SIMPLE LIE GROUPS
Example 10.27 (Sp(2, R))
The matrices (exercise 10.25)
T = ¬±
cosh Œ∏
sinh Œ∏
sinh Œ∏
cosh Œ∏

(10.197)
are elements of the noncompact symplectic group Sp(2, R).
A dynamical map M takes the phase-space 2n-tuple z = (q1, p1, . . . , qn, pn)
from z(t1) to z(t2). One may show that M‚Äôs jacobian matrix
Mab = ‚àÇza(t2)
‚àÇzb(t1)
(10.198)
is in Sp(2n, R) if and only if its dynamics are hamiltonian
Àôqa = ‚àÇH
‚àÇpa
and
Àôpa = ‚àí‚àÇH
‚àÇqa
(10.199)
(Carl
Jacobi,
1804‚Äì1851;
William
Hamilton,
1805‚Äì1865,
inventor
of
quaternions).
10.29 Compact simple Lie groups
√âlie Cartan (1869‚Äì1951) showed that all compact, simple Lie groups fall into
four inÔ¨Ånite classes and Ô¨Åve discrete cases. For n = 1, 2, . . ., his four classes are
‚Ä¢ An = SU(n + 1), which are (n + 1) √ó (n + 1) unitary matrices with unit
determinant,
‚Ä¢ Bn = SO(2n+1), which are (2n+1)√ó(2n+1) orthogonal matrices with unit
determinant,
‚Ä¢ Cn = Sp(2n), which are 2n √ó 2n symplectic matrices, and
‚Ä¢ Dn = SO(2n), which are 2n √ó 2n orthogonal matrices with unit determinant.
The Ô¨Åve discrete cases are the exceptional groups G2, F4, E6, E7, and E8.
The exceptional groups are associated with the octonians
a + bŒ±iŒ±
(10.200)
where the Œ±-sum runs from 1 to 7; the eight numbers a and bŒ± are real; and the
seven iŒ±s obey the multiplication law
iŒ± iŒ≤ = ‚àíŒ¥Œ±Œ≤ + gŒ±Œ≤Œ≥ iŒ≥ ,
(10.201)
in which gŒ±Œ≤Œ≥ is totally antisymmetric with
g123 = g247 = g451 = g562 = g634 = g375 = g716 = 1.
(10.202)
383

GROUP THEORY
Like the quaternions and the complex numbers, the octonians form a division
algebra with an absolute value
|a + bŒ±iŒ±| =

a2 + b2
Œ±
1/2
(10.203)
that satisÔ¨Åes
|AB| = |A||B|
(10.204)
but they lack associativity.
The group G2 is the subgroup of SO(7) that leaves the gŒ±Œ≤Œ≥ s of (10.201)
invariant.
10.30 Group integration
Suppose we need to integrate some function f (g) over a group. Naturally, we
want to do so in a way that gives equal weight to every element of the group. In
particular, if g‚Ä≤ is any group element, we want the integral of the shifted function
f (g‚Ä≤g) to be the same as the integral of f (g)

f (g) dg =

f (g‚Ä≤g) dg.
(10.205)
Such a measure dg is said to be left invariant (Creutz, 1983, chap. 8).
Let‚Äôs use the letters a = a1, . . . , an, b = b1, . . . , bn, and so forth to label the
elements g(a), g(b), so that an integral over the group is

f (g) dg =

f (g(a)) m(a) dna,
(10.206)
in which m(a) is the left-invariant measure and the integration is over the n-
space of as that label all the elements of the group.
To Ô¨Ånd the left-invariant measure m(a), we use the multiplication law of the
group
g(a(c, b)) ‚â°g(c) g(b)
(10.207)
and impose the requirement (10.205) of left invariance with g‚Ä≤ ‚â°g(c)

f (g(b)) m(b) dnb =

f (g(c)g(b)) m(b) dnb =

f (g(a(c, b))) m(b) dnb.
(10.208)
We change variables from b to a = a(c, b) by using the jacobian det(‚àÇb/‚àÇa),
which gives us dnb = det(‚àÇb/‚àÇa) dna

f (g(b)) m(b) dnb =

f (g(a)) det(‚àÇb/‚àÇa) m(b) dna.
(10.209)
384

10.30 GROUP INTEGRATION
Replacing b by a = a(c, b) on the left-hand side of this equation, we Ô¨Ånd
m(a) = det(‚àÇb/‚àÇa) m(b)
(10.210)
or since det(‚àÇb/‚àÇa) = 1/ det(‚àÇa(c, b)/‚àÇb)
m(a(c, b)) = m(b)/ det(‚àÇa(c, b)/‚àÇb).
(10.211)
So if we let g(b) ‚Üíg(0) = e, the identity element of the group, and set m(e) = 1,
then we Ô¨Ånd for the measure
m(a) = m(c) = m(a(c, b))|b=0 = 1/ det(‚àÇa(c, b)/‚àÇb)|b=0 .
(10.212)
Example 10.28 (The invariant measure for SU(2))
A general element of the
group SU(2) is given by (10.118) as
exp

i Œ∏ ¬∑ œÉ
2

= I cos Œ∏
2 + i ÀÜŒ∏ ¬∑ œÉ sin Œ∏
2.
(10.213)
Setting a0 = cos(Œ∏/2) and a = ÀÜŒ∏ sin(Œ∏/2), we have
g(a) = a0 + i a ¬∑ œÉ,
(10.214)
in which a2 ‚â°a2
0 + a ¬∑ a = 1. Thus, the parameter space for SU(2) is the unit
sphere S3 in four dimensions. Its invariant measure is

Œ¥(1 ‚àía2) d4a =

Œ¥(1 ‚àía2
0 ‚àía2) d4a =

(1 ‚àía2)‚àí1/2 d3a
(10.215)
or
m(a) = (1 ‚àía2)‚àí1/2.
(10.216)
We also can write the arbitrary element (10.214) of SU(2) as
g(a) = ¬±

1 ‚àía2 + i a ¬∑ œÉ
(10.217)
and the group-multiplication law (10.207) as

1 ‚àía2 + i a ¬∑ œÉ =

1 ‚àíc2 + i c ¬∑ œÉ
 
1 ‚àíb2 + i b ¬∑ œÉ

.
(10.218)
Thus, by multiplying both sides of this equation by œÉi and taking the trace, we
Ô¨Ånd (exercise 10.26) that the parameters a(c, b) that describe the product g(c) g(b)
are
a(c, b) =

1 ‚àíc2 b +

1 ‚àíb2 c ‚àíc √ó b.
(10.219)
To compute the jacobian of our formula (10.212) for the invariant measure, we
differentiate this expression (10.219) at b = 0 and so Ô¨Ånd (exercise 10.27)
m(a) = 1/ det(‚àÇa(c, b)/‚àÇb)|b=0 = (1 ‚àía2)‚àí1/2
(10.220)
as the left-invariant measure in agreement with (10.216).
385

GROUP THEORY
10.31 The Lorentz group
The Lorentz group O(3, 1) is the set of all linear transformations L that leave
invariant the Minkowski inner product
xy ‚â°x ¬∑ y ‚àíx0y0 = xTŒ∑y
(10.221)
in which Œ∑ is the diagonal matrix
Œ∑ =
‚éõ
‚éú‚éú‚éù
‚àí1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
‚éû
‚éü‚éü‚é†.
(10.222)
So L is in O(3, 1) if for all 4-vectors x and y
(Lx) T Œ∑L y = xTLT Œ∑ Ly = xTŒ∑ y.
(10.223)
Since x and y are arbitrary, this condition amounts to
LTŒ∑ L = Œ∑.
(10.224)
Taking the determinant of both sides and using the transpose (1.194) and
product (1.207) rules, we have
(det L)2 = 1.
(10.225)
So det L = ¬±1, and every Lorentz transformation L has an inverse. Multiplying
(10.224) by Œ∑, we Ô¨Ånd
Œ∑LTŒ∑L = Œ∑2 = I,
(10.226)
which identiÔ¨Åes L‚àí1 as
L‚àí1 = Œ∑LTŒ∑.
(10.227)
The subgroup of O(3, 1) with det L = 1 is the proper Lorentz group SO(3, 1).
To Ô¨Ånd its Lie algebra, we consider a Lorentz matrix L = I + œâ that dif-
fers from the identity matrix I by a tiny matrix œâ and require it to satisfy the
condition (10.224) for membership in the Lorentz group

I + œâT
Œ∑ (I + œâ) = Œ∑ + œâTŒ∑ + Œ∑ œâ + œâTœâ = Œ∑.
(10.228)
Neglecting œâTœâ, we have œâTŒ∑ = ‚àíŒ∑ œâ or since Œ∑2 = I
œâT = ‚àíŒ∑ œâ Œ∑.
(10.229)
This equation says (exercise 10.29) that under transposition the time-time and
space-space elements of œâ change sign, while the time-space and space-time
386

10.31 THE LORENTZ GROUP
elements do not. That is, the tiny matrix œâ must be for inÔ¨Ånitesimal Œ∏ and Œª a
linear combination
œâ = Œ∏ ¬∑ R + Œª ¬∑ B
(10.230)
of the six matrices
R1 =
‚éõ
‚éú‚éú‚éù
0
0
0
0
0
0
0
0
0
0
0
‚àí1
0
0
1
0
‚éû
‚éü‚éü‚é†,
R2 =
‚éõ
‚éú‚éú‚éù
0
0
0
0
0
0
0
1
0
0
0
0
0
‚àí1
0
0
‚éû
‚éü‚éü‚é†,
R3 =
‚éõ
‚éú‚éú‚éù
0
0
0
0
0
0
‚àí1
0
0
1
0
0
0
0
0
0
‚éû
‚éü‚éü‚é†
(10.231)
and
B1 =
‚éõ
‚éú‚éú‚éù
0
1
0
0
1
0
0
0
0
0
0
0
0
0
0
0
‚éû
‚éü‚éü‚é†,
B2 =
‚éõ
‚éú‚éú‚éù
0
0
1
0
0
0
0
0
1
0
0
0
0
0
0
0
‚éû
‚éü‚éü‚é†,
B3 =
‚éõ
‚éú‚éú‚éù
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
‚éû
‚éü‚éü‚é†,
(10.232)
which satisfy condition (10.229). The three Rj are 4 √ó 4 versions of the rotation
generators (10.88); the three Bj generate Lorentz boosts.
If we write L = I + œâ as
L = I ‚àíiŒ∏‚ÑìiR‚Ñì‚àíiŒªjiBj ‚â°I ‚àíiŒ∏‚ÑìJ‚Ñì‚àíiŒªjKj
(10.233)
then the three matrices J‚Ñì= iR‚Ñìare imaginary and antisymmetric, and there-
fore hermitian. But the three matrices Kj = iBj are imaginary and symmetric,
and so are antihermitian. Thus, the 4 √ó 4 matrix L is not unitary. The reason is
that the Lorentz group is not compact.
One may verify (exercise 10.30) that the six generators J‚Ñìand Kj satisfy three
sets of commutation relations:
[Ji, Jj] = iœµijkJk,
(10.234)
[Ji, Kj] = iœµijkKk,
(10.235)
[Ki, Kj] = ‚àíiœµijkJk.
(10.236)
The Ô¨Årst (10.234) says that the three J‚Ñìgenerate the rotation group SO(3); the
second (10.235) says that the three boost generators transform as a 3-vector
under SO(3); and the third (10.236) implies that four canceling inÔ¨Ånitesimal
boosts can amount to a rotation. These three sets of commutation relations
form the Lie algebra of the Lorentz group SO(3, 1). Incidentally, one may show
(exercise 10.31) that if J and K satisfy these commutation relations (10.234‚Äì
10.236), then so do
J and ‚àíK.
(10.237)
387

GROUP THEORY
The inÔ¨Ånitesimal Lorentz transformation (10.233) is the 4 √ó 4 matrix
L = I + œâ = I + Œ∏‚ÑìR‚Ñì+ ŒªjBj =
‚éõ
‚éú‚éú‚éù
1
Œª1
Œª2
Œª3
Œª1
1
‚àíŒ∏3
Œ∏2
Œª2
Œ∏3
1
‚àíŒ∏1
Œª3
‚àíŒ∏2
Œ∏1
1
‚éû
‚éü‚éü‚é†.
(10.238)
It moves any 4-vector x to x‚Ä≤ = L x or in components x‚Ä≤a = La
b xb
x‚Ä≤0 = x0 + Œª1x1 + Œª2x2 + Œª3x3,
x‚Ä≤1 = Œª1x0 + x1 ‚àíŒ∏3x2 + Œ∏2x3,
x‚Ä≤2 = Œª2x0 + Œ∏3x1 + x2 ‚àíŒ∏1x3,
x‚Ä≤3 = Œª3x0 ‚àíŒ∏2x1 + Œ∏1x2 + x3.
(10.239)
More succinctly with t = x0, this is
t‚Ä≤ = t + Œª ¬∑ x,
x‚Ä≤ = x + tŒª + Œ∏ ‚àßx,
(10.240)
in which ‚àß‚â°√ó means cross-product.
For arbitrary real Œ∏ and Œª, the matrices
L = e‚àíiŒ∏‚ÑìJ‚Ñì‚àíiŒªjKj
(10.241)
form the subgroup of SO(3, 1) that is connected to the identity matrix I. This
subgroup preserves the sign of the time of any time-like vector, that is, if x2 < 0,
and y = Lx, then y0x0 > 0. It is called the proper orthochronous Lorentz
group. The rest of the (homogeneous) Lorentz group can be obtained from it
by space P, time T , and space-time PT reÔ¨Çections.
The task of Ô¨Ånding all the Ô¨Ånite-dimensional irreducible representations of
the proper orthochronous homogeneous Lorentz group becomes vastly sim-
pler when we write the commutation relations (10.234‚Äì10.236) in terms of the
nonhermitian matrices
J¬±
‚Ñì= 1
2 (J‚Ñì¬± iK‚Ñì) ,
(10.242)
which generate two independent rotation groups
[J+
i , J+
j ] = iœµijkJ+
k ,
[J‚àí
i , J‚àí
j ] = iœµijkJ‚àí
k ,
[J+
i , J‚àí
j ] = 0.
(10.243)
Thus the Lie algebra of the Lorentz group is equivalent to two copies of the Lie
algebra (10.100) of SU(2). Its Ô¨Ånite-dimensional irreducible representations are
the direct products
D(j,j‚Ä≤)(Œ∏, Œª) = e‚àíiŒ∏‚ÑìJ‚Ñì‚àíiŒª‚ÑìK‚Ñì= e(‚àíiŒ∏‚Ñì‚àíŒª‚Ñì)J+
‚Ñìe(‚àíiŒ∏‚Ñì+Œª‚Ñì)J‚àí
‚Ñì
(10.244)
388

10.32 TWO-DIMENSIONAL REPRESENTATIONS OF THE LORENTZ GROUP
of the nonunitary representations D(j)(Œ∏, Œª) = e(‚àíiŒ∏‚Ñì‚àíŒª‚Ñì)J+
‚Ñìand D(j‚Ä≤)(Œ∏, Œª) =
e(‚àíiŒ∏‚Ñì+Œª‚Ñì)J‚àí
‚Ñìgenerated by the three (2j + 1) √ó (2j + 1) matrices J+
‚Ñìand by the
three (2j‚Ä≤ +1)√ó(2j‚Ä≤ +1) matrices J‚àí
‚Ñì. Under a Lorentz transformation L, a Ô¨Åeld
œà(j,j‚Ä≤)
m,m‚Ä≤(x) that transforms under the D( j,j‚Ä≤) representation of the Lorentz group
responds as
U(L) œà(j,j‚Ä≤)
m,m‚Ä≤(x) U‚àí1(L) = D(j)
mm‚Ä≤‚Ä≤(L‚àí1) D(j‚Ä≤)
m‚Ä≤m‚Ä≤‚Ä≤‚Ä≤(L‚àí1) œà(j,j‚Ä≤)
m‚Ä≤‚Ä≤,m‚Ä≤‚Ä≤‚Ä≤(Lx).
(10.245)
Although these representations are not unitary, the SO(3) subgroup of the
Lorentz group is represented unitarily by the hermitian matrices
J = J+ + J‚àí.
(10.246)
Thus, the representation D( j,j‚Ä≤) describes objects of the spins s that can arise
from the direct product of spin- j with spin- j‚Ä≤ (Weinberg, 1995, p. 231)
s = j + j‚Ä≤, j + j‚Ä≤ ‚àí1, . . . , | j ‚àíj‚Ä≤|.
(10.247)
For instance, D(0,0) describes a spinless Ô¨Åeld or particle, while D(1/2,0) and
D(0,1/2) respectively describe right-handed and left-handed spin-1/2 Ô¨Åelds or
particles. The representation D(1/2,1/2) describes objects of spin 1 and spin 0 ‚Äì
the spatial and time components of a 4-vector.
The generators Kj of the Lorentz boosts are related to J¬± by
K = ‚àíiJ+ + iJ‚àí,
(10.248)
which like (10.246) follows from the deÔ¨Ånition (10.242).
The interchange of J+ and J‚àíreplaces the generators J and K with J and
‚àíK, a substitution that we know (10.237) is legitimate.
10.32 Two-dimensional representations of the Lorentz group
The generators of the representation D(1/2,0) with j = 1/2 and j‚Ä≤ = 0 are given
by (10.246 & 10.248) with J+ = œÉ/2 and J‚àí= 0. They are
J = 1
2œÉ
and
K = ‚àíi1
2œÉ.
(10.249)
The 2 √ó 2 matrix D(1/2,0) that represents the Lorentz transformation (10.241)
L = e‚àíiŒ∏‚ÑìJ‚Ñì‚àíiŒªjKj
(10.250)
is
D(1/2,0)(Œ∏, Œª) = exp (‚àíiŒ∏ ¬∑ œÉ/2 ‚àíŒª ¬∑ œÉ/2) .
(10.251)
389

GROUP THEORY
And so the generic D(1/2,0) matrix is
D(1/2,0)(Œ∏, Œª) = e‚àíz¬∑œÉ/2
(10.252)
with Œª = Rez and Œ∏ = Imz. It is nonunitary and of unit determinant; it is
a member of the group SL(2, C) of complex unimodular 2 √ó 2 matrices. The
group SL(2, C) relates to the Lorentz group SO(3, 1) as SU(2) relates to the
rotation group SO(3).
Example 10.29 (The standard left-handed boost)
For a particle of mass m > 0,
the ‚Äústandard‚Äù boost that takes the 4-vector k = (m, 0) to p = (p0, p), where
p0 =

m2 + p2, is a boost in the ÀÜp direction
B(p) = R(ÀÜp) B3(p0) R‚àí1(ÀÜp) = exp

Œ± ÀÜp ¬∑ B

(10.253)
in which cosh Œ± = p0/m and sinh Œ± = |p|/m, as one may show by expanding the
exponential (exercise 10.33).
For Œª = Œ± ÀÜp, one may show (exercise 10.34) that the matrix D(1/2,0)(0, Œª) is
D(1/2,0)(0, Œ± ÀÜp) = e‚àíŒ±ÀÜp¬∑œÉ/2 = I cosh(Œ±/2) ‚àíÀÜp ¬∑ œÉ sinh(Œ±/2)
= I

(p0 + m)/(2m) ‚àíÀÜp ¬∑ œÉ

(p0 ‚àím)/(2m)
= p0 + m ‚àíp ¬∑ œÉ

2m(p0 + m)
(10.254)
in the third line of which the 2 √ó 2 identity matrix I is suppressed.
Under D(1/2,0), the vector (‚àíI, œÉ) transforms like a 4-vector. For tiny Œ∏ and
Œª, one may show (exercise 10.36) that the vector (‚àíI, œÉ) transforms as
D‚Ä†(1/2,0)(Œ∏, Œª)(‚àíI)D(1/2,0)(Œ∏, Œª) = ‚àíI + Œª ¬∑ œÉ,
D‚Ä†(1/2,0)(Œ∏, Œª) œÉ D(1/2,0)(Œ∏, Œª) = œÉ + (‚àíI)Œª + Œ∏ ‚àßœÉ,
(10.255)
which is how the 4-vector (t, x) transforms (10.240). Under a Ô¨Ånite Lorentz
transformation L the 4-vector Sa ‚â°(‚àíI, œÉ) becomes
D‚Ä†(1/2,0)(L) Sa D(1/2,0)(L) = La
bSb.
(10.256)
A Ô¨Åeld Œæ(x) that responds to a unitary Lorentz transformation U(L) like
U(L) Œæ(x) U‚àí1(L) = D(1/2,0)(L‚àí1) Œæ(Lx)
(10.257)
is called a left-handed Weyl spinor. We will see in example 10.30 why the action
density for such spinors
L‚Ñì(x) = i Œæ‚Ä†(x) (‚àÇ0I ‚àí‚àá¬∑ œÉ) Œæ(x)
(10.258)
390

10.32 TWO-DIMENSIONAL REPRESENTATIONS OF THE LORENTZ GROUP
is Lorentz covariant, that is
U(L) L‚Ñì(x) U‚àí1(L) = L‚Ñì(Lx).
(10.259)
Example 10.30 (Why L‚Ñìis Lorentz covariant)
We Ô¨Årst note that the derivatives
‚àÇ‚Ä≤
b in L‚Ñì(Lx) are with respect to x‚Ä≤ = Lx. Since the inverse matrix L‚àí1 takes x‚Ä≤
back to x = L‚àí1x‚Ä≤ or in tensor notation xa = L‚àí1a
b x‚Ä≤b, the derivative ‚àÇ‚Ä≤
b is
‚àÇ‚Ä≤
b =
‚àÇ
‚àÇx‚Ä≤b = ‚àÇxa
‚àÇx‚Ä≤b
‚àÇ
‚àÇxa = L‚àí1a
b
‚àÇ
‚àÇxa = ‚àÇa L‚àí1a
b.
(10.260)
Now using the abbreviation ‚àÇ0I ‚àí‚àá¬∑ œÉ ‚â°‚àí‚àÇaSa and the transformation laws
(10.256 & 10.257), we have
U(L) L‚Ñì(x) U‚àí1(L) = i Œæ‚Ä†(Lx)D(1/2,0)‚Ä†(L‚àí1)( ‚àí‚àÇaSa)D(1/2,0)(L‚àí1) Œæ(Lx)
= i Œæ‚Ä†(Lx)( ‚àí‚àÇaL‚àí1a
bSb) Œæ(Lx)
= i Œæ‚Ä†(Lx)( ‚àí‚àÇ‚Ä≤
bSb) Œæ(Lx) = L‚Ñì(Lx),
(10.261)
which shows that L‚Ñìis Lorentz covariant.
Incidentally, the rule (10.260) ensures, among other things, that the diver-
gence ‚àÇaVa is invariant

‚àÇaVa‚Ä≤ = ‚àÇ‚Ä≤
aV‚Ä≤a = ‚àÇb L‚àí1b
a La
cVc = ‚àÇb Œ¥b
c Vc = ‚àÇb Vb.
(10.262)
Example 10.31 (Why Œæ is left-handed)
The space-time integral S of the action
density L‚Ñìis stationary when Œæ(x) satisÔ¨Åes the wave equation
(‚àÇ0I ‚àí‚àá¬∑ œÉ) Œæ(x) = 0
(10.263)
or in momentum space
(E + p ¬∑ œÉ) Œæ(p) = 0.
(10.264)
Multiplying from the left by (E ‚àíp ¬∑ œÉ), we see that the energy of a particle
created or annihilated by the Ô¨Åeld Œæ is the same as its momentum E = |p| in
accord with the absence of a mass term in the action density L‚Ñì. And because the
spin of the particle is represented by the matrix J = œÉ/2, the momentum-space
relation (10.264) says that Œæ(p) is an eigenvector of ÀÜp ¬∑ J
ÀÜp ¬∑ J Œæ(p) = ‚àí1
2 Œæ(p)
(10.265)
with eigenvalue ‚àí1/2. A particle whose spin is opposite to its momentum is
said to have negative helicity or to be left-handed. Nearly massless neutrinos are
nearly left-handed.
391

GROUP THEORY
One may add to this action density the Majorana mass term
LM(x) = 1
2

m Œæ T(x) œÉ2 Œæ(x) +

m Œæ T(x) œÉ2 Œæ(x)
‚Ä†
,
(10.266)
which is Lorentz covariant because the matrices œÉ1 and œÉ3 anticommute with œÉ2,
which is antisymmetric (exercise 10.38). Since charge is conserved, only neutral
Ô¨Åelds like neutrinos can have Majorana mass terms.
The generators of the representation D(0,1/2) with j = 0 and j‚Ä≤ = 1/2 are
given by (10.246 & 10.248) with J+ = 0 and J‚àí= œÉ/2; they are
J = 1
2œÉ
and
K = i1
2œÉ.
(10.267)
Thus the 2 √ó 2 matrix D(0,1/2)(Œ∏, Œª) that represents the Lorentz transformation
(10.241)
L = e‚àíiŒ∏‚ÑìJ‚Ñì‚àíiŒªjKj
(10.268)
is
D(0,1/2)(Œ∏, Œª) = exp (‚àíiŒ∏ ¬∑ œÉ/2 + Œª ¬∑ œÉ/2) = D(1/2,0)(Œ∏, ‚àíŒª),
(10.269)
which differs from D(1/2,0)(Œ∏, Œª) only by the sign of Œª. The generic D(0,1/2) matrix
is the complex unimodular 2 √ó 2 matrix
D(0,1/2)(Œ∏, Œª) = ez‚àó¬∑œÉ/2
(10.270)
with Œª = Rez and Œ∏ = Imz.
Example 10.32 (The standard right-handed boost)
For a particle of mass m >
0, the ‚Äústandard‚Äù boost (10.253) that transforms k = (m, 0) to p = (p0, p) is the
4 √ó 4 matrix B(p) = exp

Œ± ÀÜp ¬∑ B

in which cosh Œ± = p0/m and sinh Œ± = |p|/m.
This Lorentz transformation with Œ∏ = 0 and Œª = Œ± ÀÜp is represented by the matrix
(exercise 10.35)
D(0,1/2)(0, Œ± ÀÜp) = eŒ±ÀÜp¬∑œÉ/2 = I cosh(Œ±/2) + ÀÜp ¬∑ œÉ sinh(Œ±/2)
= I

(p0 + m)/(2m) + ÀÜp ¬∑ œÉ

(p0 ‚àím)/(2m)
= p0 + m + p ¬∑ œÉ

2m(p0 + m)
,
(10.271)
in the third line of which the 2 √ó 2 identity matrix I is suppressed.
Under D(0,1/2), the vector (I, œÉ) transforms as a 4-vector; for tiny z
D‚Ä†(0,1/2)(Œ∏, Œª) I D(0,1/2)(Œ∏, Œª) = I + Œª ¬∑ œÉ,
D‚Ä†(0,1/2)(Œ∏, Œª) œÉ D(0,1/2)(Œ∏, Œª) = œÉ + IŒª + Œ∏ ‚àßœÉ
(10.272)
as in (10.240).
392

10.33 THE DIRAC REPRESENTATION OF THE LORENTZ GROUP
A Ô¨Åeld Œ∂(x) that responds to a unitary Lorentz transformation U(L) as
U(L) Œ∂(x) U‚àí1(L) = D(0,1/2)(L‚àí1) Œ∂(Lx)
(10.273)
is called a right-handed Weyl spinor. One may show (exercise 10.37) that the
action density
Lr(x) = i Œ∂ ‚Ä†(x) (‚àÇ0I + ‚àá¬∑ œÉ) Œ∂(x)
(10.274)
is Lorentz covariant
U(L) L(x) U‚àí1(L) = L(Lx).
(10.275)
Example 10.33 (Why Œ∂ is right-handed)
An argument like that of example
(10.31) shows that the Ô¨Åeld Œ∂(x) satisÔ¨Åes the wave equation
(‚àÇ0I + ‚àá¬∑ œÉ) Œ∂(x) = 0
(10.276)
or in momentum space
(E ‚àíp ¬∑ œÉ) Œ∂(p) = 0.
(10.277)
Thus, E = |p|, and Œ∂(p) is an eigenvector of ÀÜp ¬∑ J
ÀÜp ¬∑ J Œ∂(p) = 1
2 Œ∂(p)
(10.278)
with eigenvalue 1/2. A particle whose spin is parallel to its momentum is said
to have positive helicity or to be right-handed. Nearly massless antineutrinos are
nearly right-handed.
The Majorana mass term
LM(x) = 1
2

im Œ∂ T(x) œÉ2 Œ∂(x) +

im Œ∂ T(x) œÉ2 Œ∂(x)
‚Ä†
(10.279)
like (10.266) is Lorentz covariant.
10.33 The Dirac representation of the Lorentz group
Dirac‚Äôs representation of SO(3, 1) is the direct sum D(1/2,0) ‚äïD(0,1/2) of D(1/2,0)
and D(0,1/2). Its generators are the 4 √ó 4 matrices
J = 1
2
œÉ
0
0
œÉ

and
K = i
2
‚àíœÉ
0
0
œÉ

.
(10.280)
Dirac‚Äôs representation uses the Clifford algebra of the gamma matrices Œ≥ a,
which satisfy the anticommutation relation
{Œ≥ a, Œ≥ b} ‚â°Œ≥ a Œ≥ b + Œ≥ b Œ≥ a = 2Œ∑abI,
(10.281)
393

GROUP THEORY
in which Œ∑ is the 4 √ó 4 diagonal matrix (10.222) with Œ∑00 = ‚àí1 and Œ∑jj = 1 for
j = 1, 2, and 3 and I is the 4 √ó 4 identity matrix.
Remarkably, the generators of the Lorentz group
Jij = œµijkJk
and
J0j = Kj
(10.282)
may be represented as commutators of gamma matrices
Jab = ‚àíi
4[Œ≥ a, Œ≥ b].
(10.283)
They transform the gamma matrices as a 4-vector
[Jab, Œ≥ c] = ‚àíiŒ≥ a Œ∑bc + iŒ≥ b Œ∑ac
(10.284)
(exercise 10.39) and satisfy the commutation relations
i[Jab, Jcd] = Œ∑bc Jad ‚àíŒ∑ac Jbd ‚àíŒ∑da Jcb + Œ∑db Jca
(10.285)
of the Lorentz group (Weinberg, 1995, pp. 213‚Äì217) (exercise 10.40).
The gamma matrices Œ≥ a are not unique; if S is any 4 √ó 4 matrix with an
inverse, then the matrices Œ≥ ‚Ä≤a ‚â°SŒ≥ aS‚àí1 also satisfy the deÔ¨Ånition (10.281).
The choice
Œ≥ 0 = ‚àíi
0
1
1
0

and
Œ≥ = ‚àíi
 0
œÉ
‚àíœÉ
0

(10.286)
is useful in high-energy physics because it lets us assemble a left-handed spinor
and a right-handed spinor into a 4-component Majorana spinor
œàM =
Œæ
Œ∂

.
(10.287)
If two Majorana spinors œà(1)
M and œà(2)
M have the same mass, then one may
combine them into a Dirac spinor
œàD =
1
‚àö
2

œà(1)
M + iœà(2)
M

=
1
‚àö
2
Œæ(1) + iŒæ(2)
Œ∂ (1) + iŒ∂ (2)

=
ŒæD
Œ∂D

.
(10.288)
The action for a Majorana or Dirac 4-spinor often is written as
L = ‚àíœà

Œ≥ a‚àÇa + m

œà ‚â°‚àíœà (Ã∏‚àÇ+ m) œà,
(10.289)
in which
œà ‚â°iœà‚Ä†Œ≥ 0 = œà‚Ä†
0
1
1
0

=

Œ∂ ‚Ä†
Œæ‚Ä†
.
(10.290)
The kinetic part is the sum of the left-handed L‚Ñìand right-handed Lr action
densities (10.258 & 10.274)
‚àíœà Œ≥ a‚àÇaœà = iŒæ‚Ä† (‚àÇ0I ‚àí‚àá¬∑ œÉ) Œæ + i Œ∂ ‚Ä† (‚àÇ0I + ‚àá¬∑ œÉ) Œ∂.
(10.291)
394

10.34 THE POINCAR√â GROUP
The Dirac mass term
‚àím œàœà = ‚àím

Œ∂ ‚Ä†Œæ + Œæ‚Ä†Œ∂

(10.292)
conserves charge even if œà is a charged Dirac 4-spinor œàD
œàD =
1
‚àö
2

œà(1) + iœà(2)
,
(10.293)
in which case it is
‚àímœàDœàD = ‚àím

Œ∂ ‚Ä†
DŒæD + Œæ‚Ä†
DŒ∂D

= ‚àím
2

Œ∂ (1)‚Ä† ‚àíiŒ∂ (2)‚Ä† 
Œæ(1) + iŒæ(2)
+

Œæ(1)‚Ä† ‚àíiŒæ(2)‚Ä† 
Œ∂ (1) + iŒ∂ (2)
.
(10.294)
One may show (exercise 10.41) that if Œæ is a left-handed spinor transforming
as (10.257), then the spinor
Œ∂ = œÉ2 Œæ‚àó‚â°
0
‚àíi
i
0
 
Œæ‚Ä†
1
Œæ‚Ä†
2

(10.295)
transforms as a right-handed spinor (10.273), that is
ez‚àó¬∑œÉ/2œÉ2 Œæ‚àó= œÉ2

e‚àíz¬∑œÉ/2Œæ
‚àó
.
(10.296)
Similarly, Œæ = œÉ2 Œ∂ ‚àóis left-handed if Œ∂ is right-handed. Thus
Œ∂ ‚Ä†Œæ = Œæ TœÉ2 Œæ = Œ∂ ‚Ä†œÉ2 Œ∂ ‚àó.
(10.297)
One therefore can write a Dirac mass term (10.298) as a speciÔ¨Åc combination
of Majorana mass terms
‚àímœàDœàD = ‚àím
2

Œæ(1)T ‚àíiŒæ(2)T
œÉ2

Œæ(1) + iŒæ(2)
+

Œ∂ (1)T ‚àíiŒ∂ (2)T
œÉ2

Œ∂ (1) + iŒ∂ (2)
(10.298)
or entirely in terms of either left-handed Œæ or right-handed Œ∂ spinors.
10.34 The Poincar√© group
The elements of the Poincar√© group are products of Lorentz transformations
and translations in space and time. The Lie algebra of the Poincar√© group
therefore includes the generators J and K of the Lorentz group as well as the
hamiltonian H and the momentum operator P, which respectively generate
translations in time and space.
395

GROUP THEORY
Suppose T(y) is a translation that takes a 4-vector x to x + y and T(z) is a
translation that takes a 4-vector x to x + z. Then T(z)T(y) and T(y)T(z) both
take x to x + y + z. So if a translation T(y) = T(t, y) is represented by a unitary
operator U(t, y) = exp(iHt‚àíiP ¬∑ y), then the hamiltonian H and the momentum
operator P commute with each other
[H, Pj] = 0
and
[Pi, Pj] = 0.
(10.299)
We can Ô¨Ågure out the commutation relations of H and P with the angular-
momentum J and boost K operators by realizing that Pa = (H, P) is a 4-vector.
Let
U(Œ∏, Œª) = e‚àíiŒ∏¬∑J‚àíiŒª¬∑K
(10.300)
be the (inÔ¨Ånite-dimensional) unitary operator that represents (in Hilbert space)
the inÔ¨Ånitesimal Lorentz transformation
L = I + Œ∏ ¬∑ R + Œª ¬∑ B
(10.301)
where R and B are the six 4 √ó 4 matrices (10.231 & 10.232). Then because P is
a 4-vector under Lorentz transformations, we have
U‚àí1(Œ∏, Œª)PU(Œ∏, Œª) = e+iŒ∏¬∑J+iŒª¬∑KPe‚àíiŒ∏¬∑J‚àíiŒª¬∑K = (I + Œ∏ ¬∑ R + Œª ¬∑ B) P (10.302)
or using (10.272)
(I + iŒ∏ ¬∑ J + iŒª ¬∑ K) H (I ‚àíiŒ∏ ¬∑ J ‚àíiŒª ¬∑ K) = H + Œª ¬∑ P,
(I + iŒ∏ ¬∑ J + iŒª ¬∑ K) P (I ‚àíiŒ∏ ¬∑ J ‚àíiŒª ¬∑ K) = P + HŒª + Œ∏ ‚àßP.
(10.303)
Thus, one Ô¨Ånds (exercise 10.41) that H is invariant under rotations, while P
transforms as a 3-vector
[Ji, H] = 0
and
[Ji, Pj] = iœµijkPk
(10.304)
and that
[Ki, H] = ‚àíiPi
and
[Ki, Pj] = iŒ¥ijH.
(10.305)
By combining these equations with (10.285), one may write (exercise 10.43) the
Lie algebra of the Poincar√© group as
i[Jab, Jcd] = Œ∑bc Jad ‚àíŒ∑ac Jbd ‚àíŒ∑da Jcb + Œ∑db Jca,
i[Pa, Jbc] = Œ∑abPc ‚àíŒ∑acPb,
[Pa, Pb] = 0.
(10.306)
Further reading
The classic Lie Algebras in Particle Physics (Georgi, 1999), which inspired much
of this chapter, is outstanding.
396

EXERCISES
Exercises
10.1
Show that all n√ón (real) orthogonal matrices O leave invariant the quadratic
form x2
1 + x2
2 + ¬∑ ¬∑ ¬∑ + x2
n, that is, that if x‚Ä≤ = Ox, then x‚Ä≤2 = x2.
10.2
Show that the set of all n √ó n orthogonal matrices forms a group.
10.3
Show that all n √ó n unitary matrices U leave invariant the quadratic form
|x1|2 + |x2|2 + ¬∑ ¬∑ ¬∑ + |xn|2, that is, that if x‚Ä≤ = Ux, then |x|‚Ä≤2 = |x|2.
10.4
Show that the set of all n √ó n unitary matrices forms a group.
10.5
Show that the set of all n √ó n unitary matrices with unit determinant forms
a group.
10.6
Show that the matrix D( j)
m‚Ä≤m(g) = ‚ü®j, m‚Ä≤|U(g)| j, m‚ü©is unitary because the
rotation operator U(g) is unitary ‚ü®j, m‚Ä≤|U‚Ä†(g)U(g)| j, m‚ü©= Œ¥m‚Ä≤m.
10.7
Invent a group of order 3 and compute its multiplication table. For extra
credit, prove that the group is unique.
10.8
Show that the relation (10.20) between two equivalent representations is an
isomorphism.
10.9
Suppose that D1 and D2 are equivalent, irreducible representations of a Ô¨Ånite
group G so that D2(g) = SD1(g)S‚àí1 for all g ‚ààG. What can you say about
a matrix A that satisÔ¨Åes D2(g) A = A D1(g) for all g ‚ààG?
10.10 Find all components of the matrix exp(iŒ±A) in which
A =
‚éõ
‚éù
0
0
‚àíi
0
0
0
i
0
0
‚éû
‚é†.
(10.307)
10.11 If [A, B] = B, Ô¨Ånd eiŒ±ABe‚àíiŒ±A. Hint: what are the Œ±-derivatives of this
expression?
10.12 Show that the tensor-product matrix (10.31) of two representations D1 and
D2 is a representation.
10.13 Find a 4√ó4 matrix S that relates the tensor-product representation D1/2‚äó1/2
to the direct sum D1 ‚äïD0.
10.14 Find the generators in the adjoint representation of the group with structure
constants fabc = œµabc where a, b, c run from 1 to 3. Hint: the answer is three
3 √ó 3 matrices ta, often written as La.
10.15 Show that the generators (10.90) satisfy the commutation relations (10.93).
10.16 Show that the demonstrated equation (10.98) implies the commutation
relation (10.99).
10.17 Use the Cayley‚ÄìHamilton theorem (1.264) to show that the 3 √ó 3 matrix
(10.96) that represents a right-handed rotation of Œ∏ radians about the axis Œ∏
is given by (10.97).
10.18 Verify the mixed Jacobi identity (10.142).
10.19 For the group SU(3), Ô¨Ånd the structure constants f123 and f231.
10.20 Show that every 2 √ó 2 unitary matrix of unit determinant is a quaternion of
unit norm.
397

GROUP THEORY
10.21 Show that the quaternions as deÔ¨Åned by (10.175) are closed under addition
and multiplication and that the product xq is a quaternion if x is real and q
is a quaternion.
10.22 Show that the derivative f ‚Ä≤(q) (10.184) of the quaternionic function f (q) = q2
depends upon the direction along which q‚Ä≤ ‚Üí0.
10.23 Show that the generators (10.188) of Sp(2n) obey commutation relations of
the form (10.189) for some real structure constants fabc.
10.24 Show that for 0 < œµ ‚â™1, the real 2n √ó 2n matrix T = exp(œµJS) satisÔ¨Åes
TTJT = J (at least up to terms of order œµ2) and so is in Sp(2n, R).
10.25 Show that the matrices T of (10.197) are in Sp(2, R).
10.26 Using the parametrization (10.217) of the group SU(2), show that the
parameters a(c, b) that describe the product g(a(c, b)) = g(c) g(b) are those
of (10.219).
10.27 Use formulas (10.219) and (10.212) to show that the left-invariant measure
for SU(2) is given by (10.220).
10.28 In tensor notation, which is explained in chapter 11, the condition (10.229)
that I + œâ be an inÔ¨Ånitesimal Lorentz transformation reads

œâTa
b = œâ a
b =
‚àíŒ∑bc œâc
d Œ∑da in which sums over c and d from 0 to 3 are understood. In this
notation, the matrix Œ∑ef lowers indices and Œ∑gh raises them, so that œâ a
b
=
‚àíœâbd Œ∑da. (Both Œ∑ef and Œ∑gh are numerically equal to the matrix Œ∑ displayed
in equation (10.222).) Multiply both sides of the condition (10.229) by Œ∑ae
and use the relation Œ∑da Œ∑ae = Œ∑d
e ‚â°Œ¥d
e to show that the matrix œâab with
both indices lowered (or raised) is antisymmetric, that is,
œâba = ‚àíœâab
and
œâba = ‚àíœâab.
(10.308)
10.29 Show that the six matrices (10.231) and (10.232) satisfy the SO(3, 1) condi-
tion (10.229).
10.30 Show that the six generators J and K obey the commutations relations
(10.234‚Äì10.236).
10.31 Show that if J and K satisfy the commutation relations (10.234‚Äì10.236) of
the Lie algebra of the Lorentz group, then so do J and ‚àíK.
10.32 Show that the six generators J+ and J‚àíobey the commutation relations
(10.243).
10.33 Relate the parameter Œ± in the deÔ¨Ånition (10.253) of the standard boost B(p)
to the 4-vector p and the mass m.
10.34 Derive the formulas for D(1/2,0)(0, Œ± ÀÜp) given in equation (10.254).
10.35 Derive the formulas for D(0,1/2)(0, Œ± ÀÜp) given in equation (10.271).
10.36 For inÔ¨Ånitesimal complex z, derive the 4-vector properties (10.255 & 10.272)
of (‚àíI, œÉ) under D(1/2,0) and of (I, œÉ) under D(0,1/2).
10.37 Show that under the unitary Lorentz transformation (10.273), the action
density (10.258) is Lorentz covariant (10.259).
10.38 Show that under the unitary Lorentz transformations (10.257 & 10.273), the
Majorana mass terms (10.266 & 10.279) are Lorentz covariant.
398

EXERCISES
10.39 Show that the deÔ¨Ånitions of the gamma matrices (10.281) and of the gener-
ators (10.283) imply that the gamma matrices transform as a 4-vector under
Lorentz transformations (10.284).
10.40 Show that (10.283) and (10.284) imply that the generators Jab satisfy the
commutation relations of the Lorentz group.
10.41 Show that the spinor Œ∂ = œÉ2Œæ‚àódeÔ¨Åned by (10.295) is right-handed (10.273)
if Œæ is left-handed (10.257).
10.42 Use (10.303) to get (10.304 & 10.305).
10.43 Derive (10.306) from (10.285, 10.299, & 10.305).
399

11
Tensors and local symmetries
11.1 Points and coordinates
A point on a curved surface or in a curved space also is a point in a
higher-dimensional Ô¨Çat space called an embedding space. For instance, a point
on a sphere also is a point in three-dimensional euclidean space and in four-
dimensional space-time. One always can add extra dimensions, but it‚Äôs simpler
to use as few as possible, three in the case of a sphere.
On a sufÔ¨Åciently small scale, any reasonably smooth space locally looks like
n-dimensional euclidean space. Such a space is called a manifold. Incidentally,
according to Whitney‚Äôs embedding theorem, every n-dimensional connected,
smooth manifold can be embedded in 2n-dimensional euclidean space R2n. So
the embedding space for such spaces in general relativity has no more than eight
dimensions.
We use coordinates to label points. For example, we can choose a polar axis
and a meridian and label a point on the sphere by its polar and azimuthal
angles (Œ∏, œÜ) with respect to that axis and meridian. If we use a different axis
and meridian, then the coordinates (Œ∏‚Ä≤, œÜ‚Ä≤) for the same point will change.
Points are physical, coordinates are metaphysical. When we change our system
of coordinates, the points don‚Äôt change, but their coordinates do.
Most points p have unique coordinates xi(p) and x‚Ä≤i(p) in their coordinate
systems. For instance, polar coordinates (Œ∏, œÜ) are unique for all points on a
sphere ‚Äì except the north and south poles which are labeled by Œ∏ = 0 and Œ∏ = œÄ
and all 0 ‚â§œÜ < 2œÄ. By using more than one coordinate system, one usually
can arrange to label every point uniquely. In the Ô¨Çat three-dimensional space in
which the sphere is a surface, each point of the sphere has unique coordinates,
‚Éóp = (x, y, z).
400

11.3 CONTRAVARIANT VECTORS
We will use coordinate systems that represent points on the manifold
uniquely and smoothly at least in local patches, so that the maps
x‚Ä≤i = x‚Ä≤i(p) = x‚Ä≤i(p(x)) = x‚Ä≤i(x)
(11.1)
and
xi = xi(p) = xi(p(x‚Ä≤)) = xi(x‚Ä≤)
(11.2)
are well deÔ¨Åned, differentiable, and one to one in the patches. We‚Äôll often group
the n coordinates xi together and write them collectively as x without a super-
script. Since the coordinates x(p) label the point p, we sometimes will call them
‚Äúthe point x.‚Äù But p and x are different. The point p is unique with inÔ¨Ånitely
many coordinates x, x‚Ä≤, x‚Ä≤‚Ä≤, ... in inÔ¨Ånitely many coordinate systems.
11.2 Scalars
A scalar is a quantity B that is the same in all coordinate systems
B‚Ä≤ = B.
(11.3)
If it also depends upon the coordinates x of the space-time point p, and
B‚Ä≤(x‚Ä≤) = B(x),
(11.4)
then it is a scalar Ô¨Åeld.
11.3 Contravariant vectors
The change dx‚Ä≤i due to changes in the unprimed coordinates is
dx‚Ä≤i =

j
‚àÇx‚Ä≤i
‚àÇxj dxj.
(11.5)
This rule deÔ¨Ånes contravariant vectors: a quantity Ai is a contravariant vector if
it transforms like dxi
A‚Ä≤i =

j
‚àÇx‚Ä≤i
‚àÇxj Aj.
(11.6)
The coordinate differentials dxi form a contravariant vector. A contravariant
vector Ai(x) that depends on the coordinates x and transforms as
A‚Ä≤i(x‚Ä≤) =

j
‚àÇx‚Ä≤i
‚àÇxj Aj(x)
(11.7)
is a contravariant vector Ô¨Åeld.
401

TENSORS AND LOCAL SYMMETRIES
11.4 Covariant vectors
The chain rule for partial derivatives
‚àÇ
‚àÇx‚Ä≤i =

j
‚àÇxj
‚àÇx‚Ä≤i
‚àÇ
‚àÇxj
(11.8)
deÔ¨Ånes covariant vectors: a vector Ci that transforms as
C‚Ä≤
i =

j
‚àÇxj
‚àÇx‚Ä≤i Cj
(11.9)
is a covariant vector. If it also is a function of x, then it is a covariant vector Ô¨Åeld
and
C‚Ä≤
i(x‚Ä≤) =

j
‚àÇxj
‚àÇx‚Ä≤i Cj(x).
(11.10)
Example 11.1 (Gradient of a scalar)
The derivatives of a scalar Ô¨Åeld form a
covariant vector Ô¨Åeld. For by using the chain rule to differentiate the equation
B‚Ä≤(x‚Ä≤) = B(x) that deÔ¨Ånes a scalar Ô¨Åeld, one Ô¨Ånds
‚àÇB‚Ä≤(x‚Ä≤)
‚àÇx‚Ä≤i
= ‚àÇB(x)
‚àÇx‚Ä≤i
=

j
‚àÇxj
‚àÇx‚Ä≤i
‚àÇB(x)
‚àÇxj ,
(11.11)
which shows that the gradient ‚àÇB(x)/‚àÇxj is a covariant vector Ô¨Åeld.
11.5 Euclidean space in euclidean coordinates
If we use euclidean coordinates to describe points in euclidean space, then
covariant and contravariant vectors are the same.
Euclidean space has a natural inner product (section 1.6), the usual dot-
product, which is real and symmetric. In a euclidean space of n dimensions,
we may choose any n Ô¨Åxed, orthonormal basis vectors ei
(ei, ej) ‚â°ei ¬∑ ej =
n

k=1
ek
i ek
j = Œ¥ij
(11.12)
and use them to represent any point p as the linear combination
p =
n

i=1
ei xi.
(11.13)
402

11.5 EUCLIDEAN SPACE IN EUCLIDEAN COORDINATES
The coefÔ¨Åcients xi are the euclidean coordinates in the ei basis. Since the basis
vectors ei are orthonormal, each xi is an inner product or dot-product
xi = ei ¬∑ p =
n

j=1
ei ¬∑ ej xj =
n

j=1
Œ¥ij xj.
(11.14)
The dual vectors ei are deÔ¨Åned as those vectors whose inner products with the
ej are (ei, ej) = Œ¥i
j. In this section, they are the same as the vectors ei, and so we
shall not bother to distinguish ei from ei = ei.
If we use different orthonormal vectors e‚Ä≤
i as a basis
p =
n

i=1
e‚Ä≤
i x‚Ä≤i
(11.15)
then we get new euclidean coordinates x‚Ä≤
i = e‚Ä≤
i ¬∑ p for the same point p. These
two sets of coordinates are related by the equations
x‚Ä≤i = e‚Ä≤
i ¬∑ p =
n

j=1
e‚Ä≤
i ¬∑ ej xj,
xj = ej ¬∑ p =
n

k=1
ej ¬∑ e‚Ä≤
k x‚Ä≤k.
(11.16)
Because the basis vectors e and e‚Ä≤ are all independent of x, the coefÔ¨Åcients
‚àÇx‚Ä≤i/‚àÇxj of the transformation laws for contravariant (11.6) and covariant (11.9)
vectors are
contravariant
‚àÇx‚Ä≤i
‚àÇxj = e‚Ä≤
i ¬∑ ej
and
‚àÇxj
‚àÇx‚Ä≤i = ej ¬∑ e‚Ä≤
i
covariant.
(11.17)
But the dot-product (1.82) is symmetric, and so these are the same:
‚àÇx‚Ä≤i
‚àÇxj = e‚Ä≤
i ¬∑ ej = ej ¬∑ e‚Ä≤
i = ‚àÇxj
‚àÇx‚Ä≤i .
(11.18)
Contravariant and covariant vectors transform the same way in euclidean space
with euclidean coordinates.
The relations between x‚Ä≤i and xj imply that
x‚Ä≤i =
n

j,k=1

e‚Ä≤
i ¬∑ ej
 
ej ¬∑ e‚Ä≤
k

x‚Ä≤k.
(11.19)
Since this holds for all coordinates x‚Ä≤i, we have
n

j=1

e‚Ä≤
i ¬∑ ej
 
ej ¬∑ e‚Ä≤
k

= Œ¥ik.
(11.20)
403

TENSORS AND LOCAL SYMMETRIES
The coefÔ¨Åcients e‚Ä≤
i ¬∑ ej form an orthogonal matrix, and the linear operator
n

i=1
eie‚Ä≤
i
T =
n

i=1
|ei‚ü©‚ü®e‚Ä≤
i|
(11.21)
is an orthogonal (real, unitary) transformation. The change x ‚Üíx‚Ä≤ is a rotation
plus a possible reÔ¨Çection (exercise 11.2).
Example 11.2 (A euclidean space of two dimensions)
In two-dimensional
euclidean space, one can describe the same point by euclidean (x, y) and polar
(r, Œ∏) coordinates. The derivatives
‚àÇr
‚àÇx = x
r = ‚àÇx
‚àÇr
and
‚àÇr
‚àÇy = y
r = ‚àÇy
‚àÇr
(11.22)
respect the symmetry (11.18), but (exercise 11.1) these derivatives
‚àÇŒ∏
‚àÇx = ‚àíy
r2 Ã∏= ‚àÇx
‚àÇŒ∏ = ‚àíyr
x
and
‚àÇŒ∏
‚àÇy = x
r2 Ã∏= ‚àÇy
‚àÇŒ∏ = x
(11.23)
do not.
11.6 Summation conventions
When a given index is repeated in a product, that index usually is being summed
over. So to avoid distracting summation symbols, one writes
Ai Bi ‚â°
n

i=1
Ai Bi.
(11.24)
The sum is understood to be over the relevant range of indices, usually from
0 or 1 to 3 or n. Where the distinction between covariant and contravari-
ant indices matters, an index that appears twice in the same monomial, once
as a subscript and once as a superscript, is a dummy index that is summed
over as in
Ai Bi ‚â°
n

i=1
Ai Bi.
(11.25)
These summation conventions make tensor notation almost as compact as
matrix notation. They make equations easier to read and write.
404

11.7 MINKOWSKI SPACE
Example 11.3 (The Kronecker delta)
The summation convention and the chain
rule imply that
‚àÇx‚Ä≤i
‚àÇxk
‚àÇxk
‚àÇx‚Ä≤j = ‚àÇx‚Ä≤i
‚àÇx‚Ä≤j = Œ¥i
j =
 1
if i = j,
0
if i Ã∏= j.
(11.26)
The repeated index k has disappeared in this contraction.
11.7 Minkowski space
Minkowski space has one time dimension, labeled by k = 0, and n space
dimensions. In special relativity n = 3, and the Minkowski metric Œ∑
Œ∑kl = Œ∑kl =
‚éß
‚é®
‚é©
‚àí1
if k = l = 0,
1
if 1 ‚â§k = l ‚â§3 ,
0
if k Ã∏= l
(11.27)
deÔ¨Ånes an inner product between points p and q with coordinates xk
p and x‚Ñì
q as
(p, q) = p ¬∑ q = pk Œ∑kl ql = (q, p).
(11.28)
If one time component vanishes, the Minkowski inner product reduces to the
euclidean dot-product (1.82).
We can use different sets {ei} and {e‚Ä≤
i} of n + 1 Lorentz-orthonormal basis
vectors
(ei, ej) = ei ¬∑ ej = ek
i Œ∑kl el
j = Œ∑ij = e‚Ä≤
i ¬∑ e‚Ä≤
j = (e‚Ä≤
i, e‚Ä≤
j)
(11.29)
to represent any point p in the space either as a linear combination of the vec-
tors ei with coefÔ¨Åcients xi or as a linear combination of the vectors e‚Ä≤
i with
coefÔ¨Åcients x‚Ä≤i
p = ei xi = e‚Ä≤
i x‚Ä≤i.
(11.30)
The dual vectors, which carry upper indices, are deÔ¨Åned as
ei = Œ∑ij ej
and
e‚Ä≤i = Œ∑ij e‚Ä≤
j.
(11.31)
They are orthonormal to the vectors ei and e‚Ä≤
i because
(ei, ej) = ei ¬∑ ej = Œ∑ik ek ¬∑ ej = Œ∑ik Œ∑kj = Œ¥i
j
(11.32)
and similarly (e‚Ä≤i, e‚Ä≤
j) = e‚Ä≤i¬∑e‚Ä≤
j = Œ¥i
j. Since the square of the matrix Œ∑ is the identity
matrix Œ∑‚ÑìiŒ∑ij = Œ¥j
‚Ñì, it follows that
ei = Œ∑ij ej
and
e‚Ä≤
i = Œ∑ij e‚Ä≤j.
(11.33)
The metric Œ∑ raises (11.31) and lowers (11.33) the index of a basis vector.
405

TENSORS AND LOCAL SYMMETRIES
The component x‚Ä≤i is related to the components xj by the linear map
x‚Ä≤i = e‚Ä≤i ¬∑ p = e‚Ä≤i ¬∑ ej xj.
(11.34)
Such a map from a 4-vector x to a 4-vector x‚Ä≤ is a Lorentz transformation
x‚Ä≤i = Li
j xj
with matrix
Li
j = e‚Ä≤i ¬∑ ej.
(11.35)
The inner product (p, q) of two points p = ei xi = e‚Ä≤
i x‚Ä≤i and q = ek yk = e‚Ä≤
k y‚Ä≤k
is physical and so is invariant under Lorentz transformations
(p, q) = xi yk ei ¬∑ ek = Œ∑ik xi yk = x‚Ä≤i y‚Ä≤k e‚Ä≤
i ¬∑ e‚Ä≤
k = Œ∑ik x‚Ä≤i y‚Ä≤k.
(11.36)
With x‚Ä≤i = Li
r xr and y‚Ä≤k = Lk
s xs, this invariance is
Œ∑rs xrys = Œ∑ik Li
r xr Lk
s ys
(11.37)
or since xr and ys are arbitrary
Œ∑rs = Œ∑ik Li
r Lk
s = Li
r Œ∑ik Lk
s.
(11.38)
In matrix notation, a left index labels a row, and a right index labels a column.
Transposition interchanges rows and columns Li
r = LTi
r, so
Œ∑rs = LTi
r Œ∑ik Lk
s
or
Œ∑ = LT Œ∑ L
(11.39)
in matrix notation. In such matrix products, the height of an index ‚Äì whether
it is up or down ‚Äì determines whether it is contravariant or covariant but does
not affect its place in its matrix.
Example 11.4 (A boost)
The matrix
L =
‚éõ
‚éú‚éú‚éù
Œ≥

Œ≥ 2 ‚àí1
0
0

Œ≥ 2 ‚àí1
Œ≥
0
0
0
0
1
0
0
0
0
1
‚éû
‚éü‚éü‚é†
(11.40)
where Œ≥ = 1/

1 ‚àív2/c2 represents a Lorentz transformation that is a boost
in the x-direction. Boosts and rotations are Lorentz transformations. Working
with 4 √ó 4 matrices can get tedious, so students are advised to think in terms of
scalars, like p ¬∑ x = piŒ∑ijxj = p ¬∑ x ‚àíEt whenever possible.
If the basis vectors e and e‚Ä≤ are independent of p and of x, then the coefÔ¨Åcients
of the transformation law (11.6) for contravariant vectors are
‚àÇx‚Ä≤i
‚àÇxj = e‚Ä≤i ¬∑ ej.
(11.41)
406

11.8 LORENTZ TRANSFORMATIONS
Similarly, the component xj is xj = ej ¬∑ p = ej ¬∑ e‚Ä≤
i x‚Ä≤i, so the coefÔ¨Åcients of the
transformation law (11.9) for covariant vectors are
‚àÇxj
‚àÇx‚Ä≤i = ej ¬∑ e‚Ä≤
i.
(11.42)
Using Œ∑ to raise and lower the indices in the formula (11.41) for the coefÔ¨Åcients
of the transformation law (11.6) for contravariant vectors, we Ô¨Ånd
‚àÇx‚Ä≤i
‚àÇxj = e‚Ä≤i ¬∑ ej = Œ∑ik Œ∑j‚Ñìe‚Ä≤
k ¬∑ e‚Ñì= Œ∑ik Œ∑j‚Ñì
‚àÇx‚Ñì
‚àÇx‚Ä≤k ,
(11.43)
which is ¬± ‚àÇxj/‚àÇx‚Ä≤i. So if we use coordinates associated with Ô¨Åxed basis vectors
ei in Minkowski space, then the coefÔ¨Åcients for the two kinds of transformation
laws differ only by occasional minus signs.
So if Ai is a contravariant vector
A
‚Ä≤i = ‚àÇx‚Ä≤i
‚àÇxj Aj
(11.44)
then the relation (11.43) between the two kinds of coefÔ¨Åcient implies that
Œ∑si A
‚Ä≤i = Œ∑si
‚àÇx‚Ä≤i
‚àÇxj Aj = Œ∑si Œ∑ik Œ∑j‚Ñì
‚àÇx‚Ñì
‚àÇx‚Ä≤k Aj = Œ¥k
s
‚àÇx‚Ñì
‚àÇx‚Ä≤k Œ∑j‚ÑìAj = ‚àÇx‚Ñì
‚àÇx‚Ä≤s Œ∑‚Ñìj Aj,
(11.45)
which shows that A‚Ñì= Œ∑‚Ñìj Aj transforms covariantly
A‚Ä≤
s = ‚àÇx‚Ñì
‚àÇx‚Ä≤s A‚Ñì.
(11.46)
The metric Œ∑ turns a contravariant vector into a covariant one. It also switches
a covariant vector A‚Ñìback to its contravariant form Ak
Œ∑k‚ÑìA‚Ñì= Œ∑k‚ÑìŒ∑‚ÑìjAj = Œ¥k
j Aj = Ak.
(11.47)
In Minkowski space, one uses Œ∑ to raise and lower indices
Ai = Œ∑ij Aj and Ai = Œ∑ij Aj.
(11.48)
In general relativity, the space-time metric g raises and lowers indices.
11.8 Lorentz transformations
In section 11.7, Lorentz transformations arose as linear maps of the coordinates
due to a change of basis. They also are linear maps of the basis vectors ei that
preserve the inner products
(ei, ej) = ei ¬∑ ej = Œ∑ij = e‚Ä≤
i ¬∑ e‚Ä≤
j = (e‚Ä≤
i, e‚Ä≤
j).
(11.49)
407

TENSORS AND LOCAL SYMMETRIES
The vectors ei are four linearly independent four-dimensional vectors, and so
they span four-dimensional Minkowski space and can represent the vectors e‚Ä≤
i as
e‚Ä≤
i =  k
i
ek
(11.50)
where the coefÔ¨Åcients  k
i
are real numbers. The requirement that the new basis
vectors e‚Ä≤
i are Lorentz orthonormal gives
Œ∑ij = e‚Ä≤
i ¬∑ e‚Ä≤
j =  k
i
ek ¬∑  ‚Ñì
j e‚Ñì=  k
i
ek ¬∑ e‚Ñì ‚Ñì
j
=  k
i
Œ∑k‚Ñì ‚Ñì
j
(11.51)
or in matrix notation
Œ∑ =  Œ∑ T
(11.52)
where T is the transpose (T)‚Ñì
j =  ‚Ñì
j . Evidently T satisÔ¨Åes the deÔ¨Ånition
(11.39) of a Lorentz transformation. What Lorentz transformation is it? The
point p must remain invariant, so by (11.35 & 11.50) one has
p = e‚Ä≤
i x‚Ä≤i =  k
i
ek Li
jxj = Œ¥k
j ek xj = ej xj
(11.53)
whence  k
i
Li
j = Œ¥k
j or TL = I. So T = L‚àí1.
By multiplying condition (11.52) by the metric Œ∑ Ô¨Årst from the left and then
from the right and using the fact that Œ∑2 = I, we Ô¨Ånd
1 = Œ∑2 = Œ∑  Œ∑ T =  Œ∑ T Œ∑,
(11.54)
which gives us the inverse matrices
‚àí1 = Œ∑ T Œ∑ = LT
and
(T)‚àí1 = Œ∑  Œ∑ = L.
(11.55)
In special relativity, contravariant vectors transform as
dx‚Ä≤i = Li
j dxj
(11.56)
and since xj = L‚àí1j
i x‚Ä≤i, the covariant ones transform as
‚àÇ
‚àÇx‚Ä≤i = ‚àÇxj
‚àÇx‚Ä≤i
‚àÇ
‚àÇxj = L‚àí1j
i
‚àÇ
‚àÇxj =  j
i
‚àÇ
‚àÇxj .
(11.57)
By taking the determinant of both sides of (11.52) and using the transpose
(1.194) and product (1.207) rules for determinants, we Ô¨Ånd that det  = ¬± 1.
11.9 Special relativity
The space-time of special relativity is Ô¨Çat, four-dimensional Minkowski space.
The inner product (p ‚àíq) ¬∑ (p ‚àíq) of the interval p ‚àíq between two points
is physical and independent of the coordinates and therefore invariant. If the
408

11.9 SPECIAL RELATIVITY
points p and q are close neighbors with coordinates xi + dxi for p and xi for q,
then that invariant inner product is
(p ‚àíq) ¬∑ (p ‚àíq) = ei dxi ¬∑ ej dxj = dxi Œ∑ij dxj = dx2 ‚àí(dx0)2
(11.58)
with dx0 = c dt. (At some point in what follows, we‚Äôll measure distance in light-
seconds so that c = 1.) If the points p and q are on the trajectory of a massive
particle moving at velocity v, then this invariant quantity is the square of the
invariant distance
ds2 = dx2 ‚àíc2dt2 =

v2 ‚àíc2
dt2,
(11.59)
which always is negative since v < c. The time in the rest frame of the particle
is the proper time. The square of its differential element is
dœÑ 2 = ‚àíds2/c2 =

1 ‚àív2/c2
dt2.
(11.60)
A particle of mass zero moves at the speed of light, and so its proper time is
zero. But for a particle of mass m > 0 moving at speed v, the element of proper
time dœÑ is smaller than the corresponding element of laboratory time dt by the
factor

1 ‚àív2/c2. The proper time is the time in the rest frame of the particle,
dœÑ = dt when v = 0. So if T(0) is the lifetime of a particle (at rest), then the
apparent lifetime T(v) when the particle is moving at speed v is
T(v) = dt =
dœÑ

1 ‚àív2/c2 =
T(0)

1 ‚àív2/c2 ,
(11.61)
which is longer ‚Äì an effect known as time dilation.
Example 11.5 (Time dilation in muon decay)
A muon at rest has a mean life
of T(0) = 2.2 √ó 10‚àí6 seconds. Cosmic rays hitting nitrogen and oxygen nuclei
make pions high in the Earth‚Äôs atmosphere. The pions rapidly decay into muons
in 2.6 √ó 10‚àí8 s. A muon moving at the speed of light from 10 km takes at least
t = 10 km/300, 000 (km/sec) = 3.3 √ó 10‚àí5 s to hit the ground. Were it not for
time dilation, the probability P of such a muon reaching the ground as a muon
would be
P = e‚àít/T(0) = exp(‚àí33/2.2) = e‚àí15 = 2.6 √ó 10‚àí7.
(11.62)
The (rest) mass of a muon is 105.66 MeV. So a muon of energy E = 749 MeV
has by (11.69) a time-dilation factor of
1

1 ‚àív2/c2 =
E
mc2 = 749
105.7 = 7.089 =
1

1 ‚àí(0.99)2 .
(11.63)
409

TENSORS AND LOCAL SYMMETRIES
So a muon moving at a speed of v = 0.99 c has an apparent mean life T(v) given
by equation (11.61) as
T(v) =
E
mc2 T(0) =
T(0)

1 ‚àív2/c2 = 2.2 √ó 10‚àí6 s

1 ‚àí(0.99)2 = 1.6 √ó 10‚àí5 s.
(11.64)
The probability of survival with time dilation is
P = e‚àít/T(v) = exp(‚àí33/16) = 0.12
(11.65)
so that 12% survive. Time dilation increases the chance of survival by a factor of
460,000 ‚Äì no small effect.
11.10 Kinematics
From the scalar dœÑ, and the contravariant vector dxi, we can make the 4-vector
ui = dxi
dœÑ = dt
dœÑ

dx0
dt , dx
dt

=
1

1 ‚àív2/c2 (c, v) ,
(11.66)
in which u0 = c dt/dœÑ = c/

1 ‚àív2/c2 and u = u0 v/c. The product mui is the
energy‚Äìmomentum 4-vector pi
pi = m ui = m dxi
dœÑ = m dt
dœÑ
dxi
dt =
m

1 ‚àív2/c2
dxi
dt
=
m

1 ‚àív2/c2 (c, v) =
E
c , p

.
(11.67)
Its invariant inner product is a constant characteristic of the particle and
proportional to the square of its mass
c2 pi pi = mc ui mc ui = ‚àíE2 + c2 p 2 = ‚àím2 c4.
(11.68)
Note that the time-dilation factor is the ratio of the energy of a particle to its
rest energy
1

1 ‚àív2/c2 =
E
mc2
(11.69)
and the velocity of the particle is its momentum divided by its equivalent mass
E/c2
v =
p
E/c2 .
(11.70)
The analog of F = m a is
m d2xi
dœÑ 2 = m dui
dœÑ = dpi
dœÑ = f i,
(11.71)
in which p0 = E, and f i is a 4-vector force.
410

11.11 ELECTRODYNAMICS
Example 11.6 (Time dilation and proper time)
In the frame of a laboratory,
a particle of mass m with 4-momentum pi
lab = (E/c, p, 0, 0) travels a distance
L in a time t for a 4-vector displacement of xi
lab = (ct, L, 0, 0). In its own rest
frame, the particle‚Äôs 4-momentum and 4-displacement are pi
rest = (mc, 0, 0, 0)
and xi
rest = (cœÑ, 0, 0, 0). Since the Minkowski inner product of two 4-vectors is
Lorentz invariant, we have

pixi

rest =

pixi

lab
or
Et ‚àípL = mc2œÑ = mc2t

1 ‚àív2/c2
(11.72)
so a massive particle‚Äôs phase exp(‚àíipixi/¬Øh) is exp(imc2œÑ/¬Øh).
Example 11.7 (p + œÄ ‚Üí + K)
What is the minimum energy that a beam of
pions must have to produce a sigma hyperon and a kaon by striking a proton at
rest? Conservation of the energy‚Äìmomentum 4-vector gives pp + pœÄ = p + pK.
We set c = 1 and use this equality in the invariant form (pp +pœÄ)2 = (p +pK)2.
We compute (pp + pœÄ)2 in the pp = (mp, 0) frame and set it equal to (p + pK)2
in the frame in which the spatial momenta of the  and the K cancel:
(pp + pœÄ)2 = p2
p + p2
œÄ + 2pp ¬∑ pœÄ = ‚àím2
p ‚àím2
œÄ ‚àí2mpEœÄ
= (p + pK)2 = ‚àí(m + mK)2 .
(11.73)
Thus, since the relevant masses (in MeV) are m+ = 1189.4, mK+ = 493.7,
mp = 938.3, and mœÄ+ = 139.6, the minimum total energy of the pion is
EœÄ =
(m + mK)2 ‚àím2
p ‚àím2
œÄ
2mp
‚âà1030
MeV,
(11.74)
of which 890 MeV is kinetic.
11.11 Electrodynamics
In electrodynamics and in MKSA (SI) units, the three-dimensional vector
potential A and the scalar potential œÜ form a covariant 4-vector potential
Ai =
‚àíœÜ
c , A

.
(11.75)
The contravariant 4-vector potential is Ai = (œÜ/c, A). The magnetic induction is
B = ‚àá√ó A
or
Bi = œµijk‚àÇjAk,
(11.76)
in which ‚àÇj = ‚àÇ/‚àÇxj, the sum over the repeated indices j and k runs from 1 to 3,
and œµijk is totally antisymmetric with œµ123 = 1. The electric Ô¨Åeld is
Ei = c
‚àÇA0
‚àÇxi ‚àí‚àÇAi
‚àÇx0

= ‚àí‚àÇœÜ
‚àÇxi ‚àí‚àÇAi
‚àÇt
(11.77)
411

TENSORS AND LOCAL SYMMETRIES
where x0 = ct. In 3-vector notation, E is given by the gradient of œÜ and the
time-derivative of A
E = ‚àí‚àáœÜ ‚àíÀôA.
(11.78)
In terms of the second-rank, antisymmetric Faraday Ô¨Åeld-strength tensor
Fij = ‚àÇAj
‚àÇxi ‚àí‚àÇAi
‚àÇxj = ‚àíFji
(11.79)
the electric Ô¨Åeld is Ei = c Fi0 and the magnetic Ô¨Åeld Bi is
Bi = 1
2œµijk Fjk = 1
2œµijk
‚àÇAk
‚àÇxj ‚àí‚àÇAj
‚àÇxk

= (‚àá√ó A)i
(11.80)
where the sum over repeated indices runs from 1 to 3. The inverse equation
Fjk = œµjkiBi for spatial j and k follows from the Levi-Civita identity (1.449)
œµjkiBi = 1
2œµjkiœµinm Fnm = 1
2œµijkœµinm Fnm
= 1
2

Œ¥jn Œ¥km ‚àíŒ¥jm Œ¥kn

Fnm = 1
2

Fjk ‚àíFkj

= Fjk.
(11.81)
In 3-vector notation and MKSA = SI units, Maxwell‚Äôs equations are a ban
on magnetic monopoles and Faraday‚Äôs law, both homogeneous,
‚àá¬∑ B = 0
and
‚àá√ó E + ÀôB = 0
(11.82)
and Gauss‚Äôs law and the Maxwell-Amp√®re law, both inhomogeneous,
‚àá¬∑ D = œÅf
and
‚àá√ó H = jf + ÀôD.
(11.83)
Here œÅf is the density of free charge and jf is the free current density. By free,
we understand charges and currents that do not arise from polarization and are
not restrained by chemical bonds. The divergence of ‚àá√ó H vanishes (like that
of any curl), and so the Maxwell‚ÄìAmp√®re law and Gauss‚Äôs law imply that free
charge is conserved
0 = ‚àá¬∑ (‚àá√ó H) = ‚àá¬∑ jf + ‚àá¬∑ ÀôD = ‚àá¬∑ jf + ÀôœÅf.
(11.84)
If we use this continuity equation to replace ‚àá¬∑ jf with ‚àíÀôœÅf in its middle form
0 = ‚àá¬∑jf+‚àá¬∑ ÀôD, then we see that the Maxwell‚ÄìAmp√®re law preserves the Gauss
law constraint in time
0 = ‚àá¬∑ jf + ‚àá¬∑ ÀôD = ‚àÇ
‚àÇt (‚àíœÅf + ‚àá¬∑ D) .
(11.85)
Similarly, Faraday‚Äôs law preserves the constraint ‚àá¬∑ B = 0
0 = ‚àí‚àá¬∑ (‚àá√ó E) = ‚àÇ
‚àÇt‚àá¬∑ B = 0.
(11.86)
412

11.11 ELECTRODYNAMICS
In a linear, isotropic medium, the electric displacement D is related to the
electric Ô¨Åeld E by the permittivity D = œµE and the magnetic or magnetizing Ô¨Åeld
H differs from the magnetic induction B by the permeability H = B/Œº.
On a sub-nanometer scale, the microscopic form of Maxwell‚Äôs equations
applies. On this scale, the homogeneous equations (11.82) are unchanged, but
the inhomogeneous ones are
‚àá¬∑ E = œÅ
œµ0
and
‚àá√ó B = Œº0 j + œµ0 Œº0 ÀôE = Œº0 j +
ÀôE
c2 ,
(11.87)
in which œÅ and j are the total charge and current densities, and œµ0 = 8.854 √ó
10‚àí12 F/m and Œº0 = 4œÄ √ó 10‚àí7 N/A2 are the electric and magnetic constants,
whose product is the inverse of the square of the speed of light, œµ0Œº0 = 1/c2.
Gauss‚Äôs law and the Maxwell‚ÄìAmp√®re law (11.87) imply (exercise 11.6) that the
microscopic (total) current 4-vector j = (cœÅ, j) obeys the continuity equation
ÀôœÅ + ‚àá¬∑ j = 0. Electric charge is conserved.
In vacuum, œÅ = j = 0, D = œµ0E, and H = B/Œº0, and Maxwell‚Äôs equations
become
‚àá¬∑ B = 0
and
‚àá√ó E + ÀôB = 0,
‚àá¬∑ E = 0
and
‚àá√ó B = 1
c2 ÀôE.
(11.88)
Two of these equations ‚àá¬∑ B = 0 and ‚àá¬∑ E = 0 are constraints. Taking the curl
of the other two equations, we Ô¨Ånd
‚àá√ó (‚àá√ó E) = ‚àí1
c2 ¬®E
and
‚àá√ó (‚àá√ó B) = ‚àí1
c2 ¬®B.
(11.89)
One may use the Levi-Civita identity (1.449) to show (exercise 11.8) that
‚àá√ó (‚àá√ó E) = ‚àá(‚àá¬∑ E) ‚àí‚ñ≥E and ‚àá√ó (‚àá√ó B) = ‚àá(‚àá¬∑ B) ‚àí‚ñ≥B, (11.90)
in which ‚ñ≥‚â°‚àá2. Since in vacuum the divergence of E vanishes, and since that
of B always vanishes, these identities and the curl‚Äìcurl equations (11.89) tell us
that waves of E and B move at the speed of light
1
c2 ¬®E ‚àí‚ñ≥E = 0
and
1
c2 ¬®B ‚àí‚ñ≥B = 0.
(11.91)
We may write the two homogeneous Maxwell equations (11.82) as
‚àÇiFjk + ‚àÇkFij + ‚àÇjFki = ‚àÇi

‚àÇjAk ‚àí‚àÇkAj

+ ‚àÇk

‚àÇiAj ‚àí‚àÇjAi

+‚àÇj (‚àÇkAi ‚àí‚àÇiAk) = 0
(11.92)
(exercise 11.9). This relation, known as the Bianchi identity, actually is a
generally covariant tensor equation
œµ‚Ñìijk‚àÇiFjk = 0,
(11.93)
413

TENSORS AND LOCAL SYMMETRIES
in which œµ‚Ñìijk is totally antisymmetric, as explained in section 11.32. There are
four versions of this identity (corresponding to the four ways of choosing three
different indices i, j, k from among four and leaving out one, ‚Ñì). The ‚Ñì= 0
case gives the scalar equation ‚àá¬∑ B = 0, and the three that have ‚ÑìÃ∏= 0 give the
vector equation ‚àá√ó E + ÀôB = 0.
In tensor notation, the microscopic form of the two inhomogeneous equa-
tions (11.87) ‚Äì the laws of Gauss and Amp√®re ‚Äì are
‚àÇiFki = Œº0 jk,
(11.94)
in which jk is the current 4-vector
jk = (cœÅ, j) .
(11.95)
The Lorentz force law for a particle of charge q is
m d2xi
dœÑ 2 = m dui
dœÑ = dpi
dœÑ = f i = q Fij dxj
dœÑ = q Fij uj.
(11.96)
We may cancel a factor of dt/dœÑ from both sides and Ô¨Ånd for i = 1, 2, 3
dpi
dt = q

‚àíFi0 + œµijkBkvj

or
dp
dt = q (E + v √ó B)
(11.97)
and for i = 0
dE
dt = q E ¬∑ v,
(11.98)
which shows that only the electric Ô¨Åeld does work. The only special-relativistic
correction needed in Maxwell‚Äôs electrodynamics is a factor of 1/

1 ‚àív2/c2 in
these equations. That is, we use p = mu = mv/

1 ‚àív2/c2 not p = mv in
(11.97), and we use the total energy E not the kinetic energy in (11.98). The rea-
son why so little of classical electrodynamics was changed by special relativity
is that electric and magnetic effects were accessible to measurement during the
1800s. Classical electrodynamics was almost perfect.
Keeping track of factors of the speed of light is a lot of trouble and a
distraction; in what follows, we‚Äôll often use units with c = 1.
11.12 Tensors
Tensors are structures that transform like products of vectors. A Ô¨Årst-rank
tensor is a covariant or a contravariant vector. Second-rank tensors also are
distinguished by how they transform under changes of coordinates:
414

11.12 TENSORS
contravariant
M‚Ä≤ij = ‚àÇx‚Ä≤i
‚àÇxk
‚àÇx‚Ä≤j
‚àÇxl Mkl,
mixed
N‚Ä≤i
j = ‚àÇx‚Ä≤i
‚àÇxk
‚àÇxl
‚àÇx‚Ä≤j Nk
l ,
covariant
F‚Ä≤
ij = ‚àÇxk
‚àÇx‚Ä≤i
‚àÇxl
‚àÇx‚Ä≤j Fkl.
(11.99)
We can deÔ¨Åne tensors of higher rank by extending these deÔ¨Ånitions to quantities
with more indices.
Example 11.8 (Some second-rank tensors)
If Ak and B‚Ñìare covariant vec-
tors, and Cm and Dn are contravariant vectors, then the product Cm Dn is a
second-rank contravariant tensor, and all four products Ak Cm, Ak Dn, Bk Cm,
and Bk Dn are second-rank mixed tensors, while Cm Dn as well as Cm Cn and
Dm Dn are second-rank contravariant tensors.
Since the transformation laws that deÔ¨Åne tensors are linear, any linear com-
bination of tensors of a given rank and kind is a tensor of that rank and
kind. Thus if Fij and Gij are both second-rank covariant tensors, then so is
their sum
Hij = Fij + Gij.
(11.100)
A covariant tensor is symmetric if it is independent of the order of its indices.
That is, if Sik = Ski, then S is symmetric. Similarly, a contravariant ten-
sor is symmetric if permutations of its indices leave it unchanged. Thus A is
symmetric if Aik = Aki.
A covariant or contravariant tensor is antisymmetric if it changes sign when
any two of its indices are interchanged. So Aik, Bik, and Cijk are antisymmetric if
Aik = ‚àíAki
and
Bik = ‚àíBki,
and
Cijk = Cjki = Ckij = ‚àíCjik = ‚àíCikj = ‚àíCkji.
(11.101)
Example 11.9 (Three important tensors)
The Maxwell Ô¨Åeld strength Fkl(x)
is a second-rank covariant tensor; so is the metric of space-time gij(x). The
Kronecker delta Œ¥i
j is a mixed second-rank tensor; it transforms as
Œ¥‚Ä≤i
j = ‚àÇx‚Ä≤i
‚àÇxk
‚àÇxl
‚àÇx‚Ä≤j Œ¥k
l = ‚àÇx‚Ä≤i
‚àÇxk
‚àÇxk
‚àÇx‚Ä≤j = ‚àÇx‚Ä≤i
‚àÇxj = Œ¥i
j.
(11.102)
So it is invariant under changes of coordinates.
415

TENSORS AND LOCAL SYMMETRIES
Example 11.10 (Contractions)
Although the product Ak C‚Ñìis a mixed second-
rank tensor, the product Ak Ck transforms as a scalar because
A‚Ä≤
k C‚Ä≤k = ‚àÇx‚Ñì
‚àÇx‚Ä≤k
‚àÇx‚Ä≤k
‚àÇxm A‚ÑìCm = ‚àÇx‚Ñì
‚àÇxm A‚ÑìCm = Œ¥‚Ñì
mA‚ÑìCm = A‚ÑìC‚Ñì.
(11.103)
A sum in which an index is repeated once covariantly and once contravariantly
is a contraction as in the Kronecker-delta equation (11.26). In general, the rank
of a tensor is the number of uncontracted indices.
11.13 Differential forms
By (11.10 & 11.5), a covariant vector Ô¨Åeld contracted with contravariant
coordinate differentials is invariant under arbitrary coordinate transformations
A‚Ä≤ = A‚Ä≤
i dx‚Ä≤i = ‚àÇxj
‚àÇx‚Ä≤i Aj
‚àÇx‚Ä≤i
‚àÇxk dxk = Œ¥j
k Aj dxk = Ak dxk = A.
(11.104)
This invariant quantity A = Ak dxk is a called a 1-form in the language of
differential forms introduced about a century ago by √âlie Cartan (1869‚Äì1951,
son of a blacksmith).
The wedge product dx ‚àßdy of two coordinate differentials is the directed area
spanned by the two differentials and is deÔ¨Åned to be antisymmetric
dx ‚àßdy = ‚àídy ‚àßdx
and
dx ‚àßdx = dy ‚àßdy = 0
(11.105)
so as to transform correctly under a change of coordinates. In terms of the
coordinates u = u(x, y) and v = v(x, y), the new element of area is
du ‚àßdv =
‚àÇu
‚àÇxdx + ‚àÇu
‚àÇydy

‚àß
‚àÇv
‚àÇxdx + ‚àÇv
‚àÇydy

.
(11.106)
Labeling partial derivatives by subscripts (6.20) and using the antisymmetry
(11.105), we see that the new element of area du ‚àßdv is the old area dx ‚àßdy
multiplied by the Jacobian J(u, v; x, y) of the transformation x, y ‚Üíu, v
du ‚àßdv =

uxdx + uydy

‚àß

vxdx + vydy

= ux vx dx ‚àßdx + ux vy dx ‚àßdy + uy vx dy ‚àßdx + uy vy dy ‚àßdy
=

ux vy ‚àíuyvx

dx ‚àßdy
=

ux
uy
vx
vy
 dx ‚àßdy = J(u, v; x, y) dx ‚àßdy.
(11.107)
A contraction H = 1
2Hik dxi ‚àßdxk of a second-rank covariant tensor with a
wedge product of two differentials is a 2-form. A p -form is a rank-p covariant
tensor contracted with a wedge product of p differentials
416

11.13 DIFFERENTIAL FORMS
K = 1
p! Ki1...ip dxi1 ‚àß. . . dxip.
(11.108)
The exterior derivative d differentiates and adds a differential; it turns a
p-form into a (p + 1)-form. It converts a function or a 0-form f into a 1-form
df = ‚àÇf
‚àÇxi dxi
(11.109)
and a 1-form A = Aj dxj into a 2-form dA = d(Aj dxj) = (‚àÇiAj) dxi ‚àßdxj.
Example 11.11 (The curl)
The exterior derivative of the 1-form
A = Ax dx + Ay dy + Az dz
(11.110)
is the 2-form
dA = ‚àÇyAx dy ‚àßdx + ‚àÇzAx dz ‚àßdx
+ ‚àÇxAy dx ‚àßdy + ‚àÇzAy dz ‚àßdy
+ ‚àÇxAz dx ‚àßdz + ‚àÇyAz dy ‚àßdz
=

‚àÇyAz ‚àí‚àÇzAy

dy ‚àßdz
+ (‚àÇzAx ‚àí‚àÇxAz) dz ‚àßdx
+

‚àÇxAy ‚àí‚àÇyAx

dx ‚àßdy
= (‚àá√ó A)x dy ‚àßdz + (‚àá√ó A)y dz ‚àßdx + (‚àá√ó A)z dx ‚àßdy,
(11.111)
in which we recognize the curl (6.39) of A.
The exterior derivative of the 1-form A = Aj dxj is the 2-form
dA = dAj ‚àßdxj = ‚àÇiAj dxi ‚àßdxj = 1
2 Fij dxi ‚àßdxj = F,
(11.112)
in which ‚àÇi = ‚àÇ/‚àÇxi. So d turns the electromagnetic 1-form A ‚Äì the 4-vector
potential or gauge Ô¨Åeld Aj ‚Äì into the Faraday 2-form ‚Äì the tensor Fij. Its square
vanishes: dd applied to any p-form Q is zero
ddQi...dxi ‚àß¬∑ ¬∑ ¬∑ = d(‚àÇrQi...)‚àßdxr ‚àßdxi ‚àß¬∑ ¬∑ ¬∑ = (‚àÇs‚àÇrQi...)dxs ‚àßdxr ‚àßdxi ‚àß¬∑ ¬∑ ¬∑ = 0
(11.113)
because ‚àÇs‚àÇrQ is symmetric in r and s while dxs ‚àßdxr is antisymmetric.
Some writers drop the wedges and write dxi ‚àßdxj as dxidxj while keeping
the rules of antisymmetry dxidxj = ‚àídxjdxi and

dxi2 = 0. But this economy
prevents one from using invariant quantities like S = 1
2 Sik dxidxk, in which Sik
is a second-rank covariant symmetric tensor. If Mik is a covariant second-rank
tensor with no particular symmetry, then (exercise 11.7) only its antisymmet-
ric part contributes to the 2-form Mik dxi ‚àßdxk and only its symmetric part
contributes to the quantity Mik dxidxk.
417

TENSORS AND LOCAL SYMMETRIES
The exterior derivative d applied to the Faraday 2-form F = dA gives
dF = ddA = 0,
(11.114)
which is the Bianchi identity (11.93). A p-form H is closed if dH = 0. By
(11.114), the Faraday 2-form is closed, dF = 0.
A p-form H is exact if there is a (p+1)-form K whose differential is H = dK.
The identity (12.64) or dd = 0 implies that every exact form is closed. The
lemma of Poincar√© shows that every closed form is locally exact.
If the Ai in the 1-form A = Aidxi commute with each other, then the 2-form
A2 = 0. But if the Ai don‚Äôt commute because they are matrices or operators or
Grassmann variables, then A2 need not vanish.
Example 11.12 (A static electric Ô¨Åeld is closed and locally exact)
If ÀôB = 0,
then by Faraday‚Äôs law (11.82) the curl of the electric Ô¨Åeld vanishes, ‚àá√ó E = 0.
Writing the electrostatic Ô¨Åeld as the 1-form E = Ei dxi for i = 1, 2, 3, we may
express the vanishing of its curl as
dE = ‚àÇjEi dxj dxi = 1
2

‚àÇjEi ‚àí‚àÇiEj

dxj dxi = 0,
(11.115)
which says that E is closed. We can deÔ¨Åne a quantity VP(x) as a line integral of
the 1-form E along a path P to x from some starting point x0
VP(x) = ‚àí
 x
P, x0
Ei dxi = ‚àí

P
E
(11.116)
and so VP(x) will depend on the path P as well as on x0 and x. But if ‚àá√ó E =
0 in some ball (or neighborhood) around x and x0, then within that ball the
dependence on the path P drops out because the difference VP‚Ä≤(x) ‚àíVP(x) is the
line integral of E around a closed loop in the ball, which by Stokes‚Äôs theorem
(6.44) is an integral of the vanishing curl ‚àá√ó E over any surface S in the ball
whose boundary ‚àÇS is the closed curve P‚Ä≤ ‚àíP
VP‚Ä≤(x) ‚àíVP(x) =
)
P‚Ä≤‚àíP
Ei dxi =

S
(‚àá√ó E) ¬∑ da = 0
(11.117)
or
VP‚Ä≤(x) ‚àíVP(x) =

‚àÇS
E =

S
dE = 0
(11.118)
in the language of forms (George Stokes, 1819‚Äì1903). Thus the potential
VP(x) = V(x) is independent of the path, E =
‚àí‚àáV(x), and the 1-form
E = Ei dxi = ‚àí‚àÇiV dxi = ‚àídV is exact.
The general form of Stokes‚Äôs theorem is that the integral of any p-form H
over the boundary ‚àÇR of any (p + 1)-dimensional, simply connected, orientable
region R is equal to the integral of the (p + 1)-form dH over R
418

11.14 TENSOR EQUATIONS

‚àÇR
H =

R
dH,
(11.119)
which for p = 1 gives (6.44).
Example 11.13 (Stokes‚Äôs theorem for 0-forms)
Here p = 0, the region R = [a, b]
is one-dimensional, H is a 0-form, and Stokes‚Äôs theorem is
H(b) ‚àíH(a) =

‚àÇR
H =

R
dH =
 b
a
dH(x) =
 b
a
H‚Ä≤(x) dx,
(11.120)
familiar from elementary calculus.
Example 11.14 (Exterior derivatives anticommute with differentials)
The exte-
rior derivative acting on two 1-forms A = Aidxi and B = Bjdxj is
d(A B) = d(Aidxi ‚àßBjdxj) = ‚àÇk(AiBj) dxk ‚àßdxi ‚àßdxj
= (‚àÇkAi)Bj dxk ‚àßdxi ‚àßdxj + Ai(‚àÇkBj) dxk ‚àßdxi ‚àßdxj
= (‚àÇkAi)Bj dxk ‚àßdxi ‚àßdxj ‚àíAi(‚àÇkBj) dxi ‚àßdxk ‚àßdxj
= (‚àÇkAi)dxk ‚àßdxi ‚àßBjdxj ‚àíAidxi ‚àß(‚àÇkBj) dxk ‚àßdxj
= dA ‚àßB ‚àíA ‚àßdB.
(11.121)
If A is a p-form, then d(A ‚àßB) = dA ‚àßB + (‚àí1)pA ‚àßdB (exercise 11.10).
11.14 Tensor equations
Maxwell‚Äôs equations (11.93 & 11.94) relate the derivatives of the Ô¨Åeld-strength
tensor to the current density
‚àÇFik
‚àÇxk = Œº0 ji
(11.122)
and the derivatives of the Ô¨Åeld-strength tensor to each other
0 = ‚àÇiFjk + ‚àÇkFij + ‚àÇjFki.
(11.123)
They are generally covariant tensor equations (sections 11.31 & 11.32). We also
can write Maxwell‚Äôs equations in terms of invariant forms; his homogeneous
equations are simply the Bianchi identity (11.114)
dF = ddA = 0
(11.124)
and we‚Äôll write his inhomogeneous ones in terms of forms in section 11.26.
If we can write a physical law in one coordinate system as a tensor equation
Kkl = 0
(11.125)
419

TENSORS AND LOCAL SYMMETRIES
then in any other coordinate system, the corresponding tensor equation
K‚Ä≤ij = 0
(11.126)
also is valid since
K‚Ä≤ij = ‚àÇx‚Ä≤i
‚àÇxk
‚àÇx‚Ä≤j
‚àÇxl Kkl = 0.
(11.127)
Similarly, physical laws remain the same when expressed in terms of invariant
forms. Thus by writing a theory in terms of tensors or forms, one gets a theory
that is true in all coordinate systems if it is true in any. Only such ‚Äúcovariant‚Äù
theories have a chance at being right in our coordinate system, which is not
special. One way to make a covariant theory is to start with an action that is
invariant under all coordinate transformations.
11.15 The quotient theorem
Suppose that the product B A of a quantity B (with unknown transformation
properties) with an arbitrary tensor A (of a given rank and kind) is a tensor.
Then B is itself a tensor. The simplest example is when BiAi is a scalar for all
contravariant vectors Ai
B‚Ä≤
iA‚Ä≤i = BjAj.
(11.128)
Then since Ai is a contravariant vector
B‚Ä≤
iA‚Ä≤i = B‚Ä≤
i
‚àÇx‚Ä≤i
‚àÇxj Aj = BjAj
(11.129)
or

B‚Ä≤
i
‚àÇx‚Ä≤i
‚àÇxj ‚àíBj

Aj = 0.
(11.130)
Since this equation holds for all vectors A, we may promote it to the level of a
vector equation
B‚Ä≤
i
‚àÇx‚Ä≤i
‚àÇxj ‚àíBj = 0.
(11.131)
Multiplying both sides by ‚àÇxj/‚àÇx‚Ä≤k and summing over j
B‚Ä≤
i
‚àÇx‚Ä≤i
‚àÇxj
‚àÇxj
‚àÇx‚Ä≤k = Bj
‚àÇxj
‚àÇx‚Ä≤k
(11.132)
we see that the unknown quantity Bi does transform as a covariant vector
B‚Ä≤
k = ‚àÇxj
‚àÇx‚Ä≤k Bj.
(11.133)
The quotient rule works for unknowns B and tensors A of arbitrary rank and
kind. The proof in each case is very similar to the one given here.
420

11.16 THE METRIC TENSOR
11.16 The metric tensor
So far we have been considering coordinate systems with constant basis vectors
ei that do not vary with the physical point p. Now we shall assume only that we
can write the change in the point p(x) due to an inÔ¨Ånitesimal change dxi(p) in
its coordinates xi(p) as
dp(x) = ei(x) dxi.
(11.134)
In a different system of coordinates x‚Ä≤, this displacement is dp = e‚Ä≤
i(x‚Ä≤) dx‚Ä≤i. The
basis vectors ei and e‚Ä≤
i are partial derivatives of the point p
ei(x) = ‚àÇp
‚àÇxi
and
e‚Ä≤
i(x‚Ä≤) = ‚àÇp
‚àÇx‚Ä≤i .
(11.135)
They are linearly related to each other, transforming as covariant vectors
e‚Ä≤
i(x‚Ä≤) = ‚àÇp
‚àÇx‚Ä≤i = ‚àÇxj
‚àÇx‚Ä≤i
‚àÇp
‚àÇxj = ‚àÇxj
‚àÇx‚Ä≤i ej(x).
(11.136)
They also are vectors in the n-dimensional embedding space with inner product
ei(x) ¬∑ ej(x) =
n

a=1
n

b=1
ea
i (x) Œ∑ab eb
j (x),
(11.137)
which will be positive-deÔ¨Ånite (1.75) if all the eigenvalues of the real symmetric
matrix Œ∑ are positive. For instance, the eigenvalues are positive in euclidean
3-space with cylindrical or spherical coordinates but not in Minkowski 4-space
where Œ∑ is a diagonal matrix with main diagonal (‚àí1, 1, 1, 1).
The basis vectors ei(x) constitute a moving frame, a concept introduced by
√âlie Cartan. In general, they are not normalized or orthogonal. Their inner
products deÔ¨Åne the metric of the manifold or of space-time
gij(x) = ei(x) ¬∑ ej(x).
(11.138)
An inner product by deÔ¨Ånition (1.73) satisÔ¨Åes (f , g) = (g, f )‚àóand so a real inner
product is symmetric. For real coordinates on a real manifold the basis vectors
are real, so the metric tensor is real and symmetric
gij = gji.
(11.139)
The basis vectors e‚Ä≤
j(x‚Ä≤) of a different coordinate system deÔ¨Åne the metric
in that coordinate system g‚Ä≤
ij(x‚Ä≤) = e‚Ä≤
i(x‚Ä≤) ¬∑ e‚Ä≤
j(x‚Ä≤). Since the basis vectors ei are
covariant vectors, the metric gij is a second-rank covariant tensor
g‚Ä≤
ij(x‚Ä≤) = e‚Ä≤
i(x‚Ä≤) ¬∑ e‚Ä≤
j(x‚Ä≤) = ‚àÇxk
‚àÇx‚Ä≤i ek(x) ¬∑ ‚àÇx‚Ñì
‚àÇx‚Ä≤j e‚Ñì(x) = ‚àÇxk
‚àÇx‚Ä≤i
‚àÇx‚Ñì
‚àÇx‚Ä≤j gk‚Ñì(x).
(11.140)
421

TENSORS AND LOCAL SYMMETRIES
Example 11.15 (The sphere)
Let the point p be a euclidean 3-vector represent-
ing a point on the two-dimensional surface of a sphere of radius r. The spherical
coordinates (Œ∏, œÜ) label the point p, and the basis vectors are
eŒ∏ = ‚àÇp
‚àÇŒ∏ = r ÀÜŒ∏
and
eœÜ = ‚àÇp
‚àÇœÜ = r sin Œ∏ ÀÜœÜ.
(11.141)
Their inner products are the components (11.138) of the sphere‚Äôs metric tensor,
which is the matrix
gŒ∏Œ∏
gŒ∏œÜ
gœÜŒ∏
gœÜœÜ

=
eŒ∏ ¬∑ eŒ∏
eŒ∏ ¬∑ eœÜ
eœÜ ¬∑ eŒ∏
eœÜ ¬∑ eœÜ

=
r2
0
0
r2 sin2 Œ∏

(11.142)
with determinant r4 sin2 œÜ.
11.17 A basic axiom
Points are physical, coordinate systems metaphysical. So p, q, p ‚àíq, and
(p ‚àíq) ¬∑ (p ‚àíq) are all invariant quantities. When p and q = p + dp both lie
on the (space-time) manifold and are inÔ¨Ånitesimally close to each other, the
vector dp = ei dxi is the sum of the basis vectors multiplied by the changes in
the coordinates xi. Both dp and the inner product dp¬∑dp are physical and so are
independent of the coordinates. The (squared) distance dp2 is the same in one
coordinate system
dp2 ‚â°dp ¬∑ dp = (ei dxi) ¬∑ (ej dxj) = gij dxidxj
(11.143)
as in another
dp2 ‚â°dp ¬∑ dp = (e‚Ä≤
i dx‚Ä≤i) ¬∑ (e‚Ä≤
j dx‚Ä≤j) = g‚Ä≤
ij dx‚Ä≤idx‚Ä≤j.
(11.144)
This invariance and the quotient rule provide a second reason why gij is a
second-rank covariant tensor.
We want dp to be inÔ¨Ånitesimal so that it is tangent to the manifold.
11.18 The contravariant metric tensor
The inverse gik of the covariant metric tensor gkj satisÔ¨Åes
g‚Ä≤ikg‚Ä≤
kj = Œ¥i
j = gikgkj
(11.145)
in all coordinate systems. To see how it transforms, we use the transformation
law (11.140) of gkj
Œ¥i
j = g‚Ä≤ikg‚Ä≤
kj = g‚Ä≤ik ‚àÇxt
‚àÇx‚Ä≤k gtu
‚àÇxu
‚àÇx‚Ä≤j .
(11.146)
422

11.20 ORTHOGONAL COORDINATES IN EUCLIDEAN N-SPACE
Thus in matrix notation, we have as I = g‚Ä≤‚àí1 H g H, which implies g‚Ä≤‚àí1 =
H‚àí1 g‚àí1 H‚àí1 or in tensor notation
g‚Ä≤i‚Ñì= ‚àÇx‚Ä≤i
‚àÇxv
‚àÇx‚Ä≤‚Ñì
‚àÇxw gvw.
(11.147)
Thus the inverse gik of the covariant metric tensor is a second-rank contravari-
ant tensor called the contravariant metric tensor.
11.19 Raising and lowering indices
The contraction of a contravariant vector Ai with any rank-2 covariant tensor
gives a covariant vector. We reserve the symbol Ai for the covariant vector that
is the contraction of Aj with the metric tensor
Ai = gijAj.
(11.148)
This operation is called lowering the index on Aj.
Similarly the contraction of a covariant vector Bj with any rank-2 con-
travariant tensor is a contravariant vector. But we reserve the symbol Bi for
contravariant vector that is the contraction
Bi = gijBj
(11.149)
of Bj with the inverse of the metric tensor. This is called raising the index on Bj.
The vectors ei, for instance, are given by
ei = gijej.
(11.150)
They are therefore orthonormal or dual to the basis vectors ei
ei ¬∑ ej = ei ¬∑ gjkek = gjkei ¬∑ ek = gjkgik = gjkgki = Œ¥j
i.
(11.151)
11.20 Orthogonal coordinates in euclidean n-space
In Ô¨Çat n-dimensional euclidean space, it is convenient to use orthogonal basis
vectors and orthogonal coordinates. A change dxi in the coordinates moves the
point p by (11.134)
dp = ei dxi.
(11.152)
The metric gij is the inner product (11.138)
gij = ei ¬∑ ej.
(11.153)
Since the vectors ei are orthogonal, the metric is diagonal
gij = ei ¬∑ ej = h2
i Œ¥ij.
(11.154)
423

TENSORS AND LOCAL SYMMETRIES
The inverse metric
gij = h‚àí2
i
Œ¥ij
(11.155)
raises indices. For instance, the dual vectors
e i = gij ej = h‚àí2
i
ei
satisfy
ei ¬∑ ek = Œ¥i
k.
(11.156)
The invariant squared distance dp2 between nearby points (11.143) is
dp2 = dp ¬∑ dp = gij dxi dxj = h2
i (dxi)2
(11.157)
and the invariant volume element is
dV = dnp = h1 . . . hn dx1 ‚àß. . . ‚àßdxn = g dx1 ‚àß. . . ‚àßdxn = g dnx,
(11.158)
in which g =

det gij is the square-root of the positive determinant of gij.
The important special case in which all the scale factors hi are unity is
cartesian coordinates in euclidean space (section 11.5).
We also can use basis vectors ÀÜei that are orthonormal. By (11.154 & 11.156),
these vectors
ÀÜei = ei/hi = hi e i
satisfy
ÀÜei ¬∑ ÀÜej = Œ¥ij.
(11.159)
In terms of them, a physical and invariant vector V takes the form
V = ei Vi = hi ÀÜei Vi = e i Vi = h‚àí1
i
ÀÜei Vi = ÀÜei Vi
(11.160)
where
Vi ‚â°hi Vi = h‚àí1
i
Vi
(no sum).
(11.161)
The dot-product is then
V ¬∑ U = gij Vi Vj = Vi Ui.
(11.162)
In euclidean n-space, we even can choose coordinates xi so that the vectors
ei deÔ¨Åned by dp = ei dxi are orthonormal. The metric tensor is then the n √ó n
identity matrix gik = ei ¬∑ ek = Iik = Œ¥ik. But since this is euclidean n-space, we
also can expand the n Ô¨Åxed orthonormal cartesian unit vectors ÀÜ‚Ñìin terms of the
ei(x) which vary with the coordinates as ÀÜ‚Ñì= ei(x)(ei(x) ¬∑ ÀÜ‚Ñì).
11.21 Polar coordinates
In polar coordinates in Ô¨Çat 2-space, the change dp in a point p due to changes
in its coordinates is dp = ÀÜr dr + ÀÜŒ∏ r dŒ∏ so dp = er dr + eŒ∏ dŒ∏ with er = ÀÜer = ÀÜr and
eŒ∏ = r ÀÜeŒ∏ = r ÀÜŒ∏. The metric tensor for polar coordinates is
(gij) = (ei ¬∑ ej) =
1
0
0
r2

.
(11.163)
424

11.23 SPHERICAL COORDINATES
The contravariant basis vectors are e r = ÀÜr and e Œ∏ = ÀÜeŒ∏/r. A physical vector V
is V = Vi ei = Vi e i = Vr ÀÜr + VŒ∏ ÀÜŒ∏.
11.22 Cylindrical coordinates
For cylindrical coordinates in Ô¨Çat 3-space, the change dp in a point p due to
changes in its coordinates is
dp = ÀÜœÅ dœÅ + ÀÜœÜ œÅ dœÜ + ÀÜz dz = eœÅ dœÅ + eœÜ dœÜ + ez dz
(11.164)
with eœÅ = ÀÜeœÅ = ÀÜœÅ, eœÜ = œÅ ÀÜeœÜ = œÅ ÀÜœÜ, and ez = ÀÜez = ÀÜz. The metric tensor for
cylindrical coordinates is
(gij) = (ei ¬∑ ej) =
‚éõ
‚éù
1
0
0
0
œÅ2
0
0
0
1
‚éû
‚é†
(11.165)
with determinant det gij ‚â°g = œÅ2. The invariant volume element is
dV = œÅ dx1 ‚àßdx2 ‚àßdx3 = ‚àög dœÅdœÜdz = œÅ dœÅdœÜdz.
(11.166)
The contravariant basis vectors are e œÅ = ÀÜœÅ, e œÜ = ÀÜeœÜ/œÅ, and e z = ÀÜz. A
physical vector V is
V = Vi ei = Vi e i = VœÅ ÀÜœÅ + VœÜ ÀÜœÜ + Vz ÀÜz.
(11.167)
Incidentally, since
p = (œÅ cos œÜ, œÅ sin œÜ, z)
(11.168)
the formulas for the basis vectors of cylindrical coordinates in terms of those of
rectangular coordinates are (exercise 11.13)
ÀÜœÅ = cos œÜ ÀÜx + sin œÜ ÀÜy,
ÀÜœÜ = ‚àísin œÜ ÀÜx + cos œÜ ÀÜy,
ÀÜz = ÀÜz.
(11.169)
11.23 Spherical coordinates
For spherical coordinates in Ô¨Çat 3-space, the change dp in a point p due to
changes in its coordinates is
dp = ÀÜr dr + ÀÜŒ∏ r dŒ∏ + ÀÜœÜ r sin Œ∏ dœÜ = er dr + eŒ∏ dŒ∏ + eœÜ dœÜ
(11.170)
so er = ÀÜr, eŒ∏ = r ÀÜŒ∏, and eœÜ = r sin Œ∏ ÀÜœÜ. The metric tensor for spherical
coordinates is
(gij) = (ei ¬∑ ej) =
‚éõ
‚éù
1
0
0
0
r2
0
0
0
r2 sin2 Œ∏
‚éû
‚é†
(11.171)
425

TENSORS AND LOCAL SYMMETRIES
with determinant det gij ‚â°g = r4 sin2 Œ∏. The invariant volume element is
dV = r2 sin2 Œ∏ dx1 ‚àßdx2 ‚àßdx3 = ‚àög drdŒ∏dœÜ = r2 sin Œ∏ drdŒ∏dœÜ.
(11.172)
The orthonormal basis vectors are ÀÜer = ÀÜr, ÀÜeŒ∏ = ÀÜŒ∏, and ÀÜeœÜ = ÀÜœÜ. The con-
travariant basis vectors are e r = ÀÜr, e Œ∏ = ÀÜŒ∏/r, e œÜ = ÀÜœÜ/r sin Œ∏. A physical vector
V is
V = Vi ei = Vi e i = Vr ÀÜr + VŒ∏ ÀÜŒ∏ + VœÜ ÀÜœÜ.
(11.173)
Incidentally, since
p = (r sin Œ∏ cos œÜ, r sin Œ∏ sin œÜ, r cos Œ∏)
(11.174)
the formulas for the basis vectors of spherical coordinates in terms of those of
rectangular coordinates are (exercise 11.14)
ÀÜr = sin Œ∏ cos œÜ ÀÜx + sin Œ∏ sin œÜ ÀÜy + cos Œ∏ ÀÜz,
ÀÜŒ∏ = cos Œ∏ cos œÜ ÀÜx + cos Œ∏ sin œÜ ÀÜy ‚àísin Œ∏ ÀÜz,
ÀÜœÜ = ‚àísin œÜ ÀÜx + cos œÜ ÀÜy.
(11.175)
11.24 The gradient of a scalar Ô¨Åeld
If f (x) is a scalar Ô¨Åeld, then the difference between it and f (x + dx) deÔ¨Ånes the
gradient ‚àáf as (6.26)
df (x) = f (x + dx) ‚àíf (x) = ‚àÇf (x)
‚àÇxi dxi = ‚àáf (x) ¬∑ dp.
(11.176)
Since dp = ej dxj, the invariant form
‚àáf = ei ‚àÇf
‚àÇxi = ÀÜei
hi
‚àÇf
‚àÇxi
(11.177)
satisÔ¨Åes this deÔ¨Ånition (11.176) of the gradient
‚àáf ¬∑ dp = ‚àÇf
‚àÇxi ei ¬∑ ejdxj = ‚àÇf
‚àÇxi Œ¥i
j dxj = ‚àÇf
‚àÇxi dxi = df .
(11.178)
In two polar coordinates, the gradient is
‚àáf = ei ‚àÇf
‚àÇxi = ÀÜei
hi
‚àÇf
‚àÇxi = ÀÜr ‚àÇf
‚àÇr +
ÀÜŒ∏
r
‚àÇf
‚àÇŒ∏ .
(11.179)
In three cylindrical coordinates, it is (6.27)
‚àáf = ei ‚àÇf
‚àÇxi = ÀÜei
hi
‚àÇf
‚àÇxi = ‚àÇf
‚àÇœÅ ÀÜœÅ + 1
œÅ
‚àÇf
‚àÇœÜ
ÀÜœÜ + ‚àÇf
‚àÇz ÀÜz
(11.180)
426

11.25 LEVI-CIVITA‚ÄôS TENSOR
and in three spherical coordinates it is (6.28)
‚àáf = ‚àÇf
‚àÇxi ei = ÀÜei
hi
‚àÇf
‚àÇxi = ‚àÇf
‚àÇr ÀÜr + 1
r
‚àÇf
‚àÇŒ∏
ÀÜŒ∏ +
1
r sin Œ∏
‚àÇf
‚àÇœÜ
ÀÜœÜ.
(11.181)
11.25 Levi-Civita‚Äôs tensor
In three dimensions, Levi-Civita‚Äôs symbol œµijk ‚â°œµijk is totally antisymmetric
with œµ123 = 1 in all coordinate systems.
We can turn his symbol into something that transforms as a tensor by multi-
plying it by the square-root of the determinant of a rank-2 covariant tensor. A
natural choice is the metric tensor. Thus the Levi-Civita tensor Œ∑ijk is the totally
antisymmetric rank-3 covariant (pseudo-)tensor
Œ∑ijk = ‚àög œµijk
(11.182)
in which g = | det gmn| is the absolute value of the determinant of the metric
tensor gmn. The determinant‚Äôs deÔ¨Ånition (1.184) and product rule (1.207) imply
that Levi-Civita‚Äôs tensor Œ∑ijk transforms as
Œ∑‚Ä≤
ijk =

g‚Ä≤ œµ‚Ä≤
ijk =

g‚Ä≤ œµijk =
(det
 ‚àÇxt
‚àÇx‚Ä≤m
‚àÇxu
‚àÇx‚Ä≤n gtu
 œµijk
=
(det
 ‚àÇxt
‚àÇx‚Ä≤m

det
 ‚àÇxu
‚àÇx‚Ä≤n

det (gtu)
 œµijk
=
det
 ‚àÇx
‚àÇx‚Ä≤

‚àög œµijk = œÉ det
 ‚àÇx
‚àÇx‚Ä≤
 ‚àög œµijk
= œÉ ‚àÇxt
‚àÇx‚Ä≤i
‚àÇxu
‚àÇx‚Ä≤j
‚àÇxv
‚àÇx‚Ä≤k
‚àög œµtuv = œÉ ‚àÇxt
‚àÇx‚Ä≤i
‚àÇxu
‚àÇx‚Ä≤j
‚àÇxv
‚àÇx‚Ä≤k Œ∑tuv
(11.183)
in which œÉ is the sign of the Jacobian det(‚àÇx/‚àÇx‚Ä≤). Levi-Civita‚Äôs tensor is a
pseudo-tensor because it doesn‚Äôt change sign under the parity transformation
x‚Ä≤i = ‚àíxi.
We get Œ∑ with upper indices by using the inverse gnm of the metric tensor
Œ∑ijk = git gju gkv Œ∑tuv = git gju gkv ‚àög œµtuv
= ‚àög œµijk/ det(gmn) = sœµijk/‚àög = sœµijk/‚àög,
(11.184)
in which s is the sign of the determinant det gij = sg.
Similarly in four dimensions, Levi-Civita‚Äôs symbol œµijk‚Ñì‚â°œµijk‚Ñìis totally anti-
symmetric with œµ0123 = 1 in all coordinate systems. No meaning attaches to
whether the indices of the Levi-Civita symbol are up or down; some authors
even use the notation œµ(ijk‚Ñì) or œµ[ijk‚Ñì] to emphasize this fact.
427

TENSORS AND LOCAL SYMMETRIES
In four dimensions, the Levi-Civita pseudo-tensor is
Œ∑ijk‚Ñì= ‚àög œµijk‚Ñì.
(11.185)
It transforms as
Œ∑‚Ä≤
ijk‚Ñì=

g‚Ä≤ œµijk‚Ñì=
det
 ‚àÇx
‚àÇx‚Ä≤

‚àög œµijk‚Ñì= œÉ det
 ‚àÇx
‚àÇx‚Ä≤
 ‚àög œµijk‚Ñì
= œÉ ‚àÇxt
‚àÇx‚Ä≤i
‚àÇxu
‚àÇx‚Ä≤j
‚àÇxv
‚àÇx‚Ä≤k
‚àÇxw
‚àÇx‚Ä≤‚Ñì
‚àög œµtuvw = œÉ ‚àÇxt
‚àÇx‚Ä≤i
‚àÇxu
‚àÇx‚Ä≤j
‚àÇxv
‚àÇx‚Ä≤k
‚àÇxw
‚àÇx‚Ä≤‚ÑìŒ∑tuvw (11.186)
where œÉ is the sign of the Jacobian det(‚àÇx/‚àÇx‚Ä≤).
Raising the indices on Œ∑ with det gij = sg we have
Œ∑ijk‚Ñì= git gju gkv g‚Ñìw Œ∑tuvw = git gju gkv g‚Ñìw ‚àög œµtuvw
= ‚àög œµijk‚Ñì/ det(gmn) = s œµijk‚Ñì/‚àög ‚â°s œµijk‚Ñì/‚àög.
(11.187)
In n dimensions, one may deÔ¨Åne Levi-Civita‚Äôs symbol œµ(i1 . . . in) as totally
antisymmetric with œµ(1 . . . n) = 1 and his tensor as Œ∑i1...in = ‚àög œµ(i1 . . . in).
11.26 The Hodge star
In three cartesian coordinates, the Hodge dual turns 1-forms into 2-forms
‚àódx = dy ‚àßdz,
‚àódy = dz ‚àßdx,
‚àódz = dx ‚àßdy
(11.188)
and 2-forms into 1-forms
‚àó(dx ‚àßdy) = dz,
‚àó(dy ‚àßdz) = dx,
‚àó(dz ‚àßdx) = dy.
(11.189)
It also maps the 0-form 1 and the volume 3-form into each other
‚àó1 = dx ‚àßdy ‚àßdz,
‚àó(dx ‚àßdy ‚àßdz) = 1
(11.190)
(William Vallance Douglas Hodge, 1903‚Äì1975). More generally in 3-space, we
deÔ¨Åne the Hodge dual, also called the Hodge star, as
‚àó1 = 1
3!Œ∑‚Ñìjkdx‚Ñì‚àßdxj ‚àßdxk,
‚àó(dx‚Ñì‚àßdxj ‚àßdxk) = g‚ÑìtgjugkvŒ∑tuv,
‚àódxi = 1
2 gi‚ÑìŒ∑‚Ñìjk dxj ‚àßdxk,
‚àó(dxi ‚àßdxj) = gik gj‚ÑìŒ∑k‚Ñìm dxm
(11.191)
and so if the sign of det gij is s = +1, then ‚àó‚àó1 = 1, ‚àó‚àódxi = dxi, ‚àó‚àó(dxi‚àßdxk) =
dxi ‚àßdxk, and ‚àó‚àó(dxi ‚àßdxj ‚àßdxk) = dxi ‚àßdxj ‚àßdxk.
Example 11.16 (Divergence and Laplacian)
The dual of the 1-form
df = ‚àÇf
‚àÇx dx + ‚àÇf
‚àÇy dy + ‚àÇf
‚àÇz dz
(11.192)
428

11.26 THE HODGE STAR
is the 2-form
‚àódf = ‚àÇf
‚àÇx dy ‚àßdz + ‚àÇf
‚àÇy dz ‚àßdx + ‚àÇf
‚àÇz dx ‚àßdy
(11.193)
and its exterior derivative is the Laplacian
d ‚àódf =

‚àÇ2f
‚àÇx2 + ‚àÇ2f
‚àÇy2 + ‚àÇ2f
‚àÇz2

dx ‚àßdy ‚àßdz
(11.194)
multiplied by the volume 3-form.
Similarly, the dual of the 1-form
A = Ax dx + Ay dy + Az dz
(11.195)
is the 2-form
‚àóA = Ax dy ‚àßdz + Ay dz ‚àßdx + Az dx ‚àßdy
(11.196)
and its exterior derivative is the divergence
d ‚àóA =
‚àÇAx
‚àÇx + ‚àÇAy
‚àÇy + ‚àÇAz
‚àÇz

dx ‚àßdy ‚àßdz
(11.197)
times dx ‚àßdy ‚àßdz.
In Ô¨Çat Minkowski 4-space with c = 1, the Hodge dual turns 1-forms into
3-forms
‚àódt = ‚àídx ‚àßdy ‚àßdz,
‚àódx = ‚àídy ‚àßdz ‚àßdt,
‚àódy = ‚àídz ‚àßdx ‚àßdt,
‚àódz = ‚àídx ‚àßdy ‚àßdt,
(11.198)
2-forms into 2-forms
‚àó(dx ‚àßdt) = dy ‚àßdz,
‚àó(dx ‚àßdy) = ‚àídz ‚àßdt,
‚àó(dy ‚àßdt) = dz ‚àßdx,
‚àó(dy ‚àßdz) = ‚àídx ‚àßdt,
‚àó(dz ‚àßdt) = dx ‚àßdy,
‚àó(dz ‚àßdx) = ‚àídy ‚àßdt,
(11.199)
3-forms into 1-forms
‚àó(dx ‚àßdy ‚àßdz) = ‚àídt,
‚àó(dy ‚àßdz ‚àßdt) = ‚àídx,
‚àó(dz ‚àßdx ‚àßdt) = ‚àídy,
‚àó(dx ‚àßdy ‚àßdt) = ‚àídz,
(11.200)
and interchanges 0-forms and 4-forms
‚àó1 = dt ‚àßdx ‚àßdy ‚àßdz,
‚àó(dt ‚àßdx ‚àßdy ‚àßdz) = ‚àí1.
(11.201)
429

TENSORS AND LOCAL SYMMETRIES
More generally in four dimensions, we deÔ¨Åne the Hodge star as
‚àó1 = 1
4! Œ∑k‚Ñìmn dxk ‚àßdx‚Ñì‚àßdxm ‚àßdxn,
‚àódxi = 1
3! gik Œ∑k‚Ñìmn dx‚Ñì‚àßdxm ‚àßdxn,
‚àó(dxi ‚àßdxj) = 1
2 gik gj‚ÑìŒ∑k‚Ñìmn dxm ‚àßdxn,
‚àó(dxi ‚àßdxj ‚àßdxk) = git gju gkv Œ∑tuvw dxw,
‚àó

dxi ‚àßdxj ‚àßdxk ‚àßdx‚Ñì
= git gju gkv g‚ÑìwŒ∑tuvw = Œ∑ijk‚Ñì.
(11.202)
Thus (exercise 11.16) if the determinant det gij of the metric is negative, then
‚àó‚àódxi = dxi,
‚àó‚àó(dxi ‚àßdxj) = ‚àídxi ‚àßdxj,
‚àó‚àó(dxi ‚àßdxj ‚àßdxk) = dxi ‚àßdxj ‚àßdxk,
‚àó‚àó1 = ‚àí1. (11.203)
In n dimensions, the Hodge star turns p-forms into (n ‚àíp)-forms
‚àó

dxi1 ‚àß. . . ‚àßdxip
= gi1k1 . . . gipkp Œ∑k1...kp‚Ñì1...‚Ñìn‚àíp
(n ‚àíp)!
dx‚Ñì1 ‚àß. . .‚àßdx‚Ñìn‚àíp. (11.204)
Example 11.17 (The inhomogeneous Maxwell equations)
Since the homoge-
neous Maxwell equations are
dF = ddA = 0
(11.205)
we Ô¨Årst form the dual ‚àóF = ‚àódA
‚àóF = 1
2Fij ‚àó

dxi ‚àßdxj
= 1
4Fijgikgj‚ÑìŒ∑k‚Ñìmndxm ‚àßdxn = 1
4Fk‚ÑìŒ∑k‚Ñìmndxm ‚àßdxn
and then apply the exterior derivative
d ‚àóF = 1
4d

Fk‚ÑìŒ∑k‚Ñìmndxm ‚àßdxn
= 1
4‚àÇp

Fk‚ÑìŒ∑k‚Ñìmn

dxp ‚àßdxm ‚àßdxn.
To get back to a 1-form like j = jkdxk, we apply a second Hodge star
‚àód ‚àóF = 1
4 ‚àÇp

Fk‚ÑìŒ∑k‚Ñìmn

‚àó

dxp ‚àßdxm ‚àßdxn
= 1
4 ‚àÇp

Fk‚ÑìŒ∑k‚Ñìmn

gpsgmtgnuŒ∑stuv dxv
= 1
4 ‚àÇp
‚àög Fk‚Ñì
œµk‚Ñìmn gpsgmtgnu‚àög œµstuv dxv
= 1
4 ‚àÇp
‚àög Fk‚Ñì
œµk‚Ñìmn gpsgmtgnugwv œµstuv
‚àög dx
= 1
4 ‚àÇp
‚àög Fk‚Ñì
œµk‚Ñìmn œµpmnw
‚àög
det gij
dxw,
(11.206)
430

11.27 DERIVATIVES AND AFFINE CONNECTIONS
in which we used the deÔ¨Ånition (1.184) of the determinant. Levi-Civita‚Äôs
4-symbol obeys the identity (exercise 11.17)
œµk‚Ñìmn œµpwmn = 2

Œ¥p
k Œ¥w
‚Ñì‚àíŒ¥w
k Œ¥p
‚Ñì

.
(11.207)
Applying it to ‚àód ‚àóF, we get
‚àód ‚àóF =
s
2‚àög‚àÇp
‚àög Fk‚Ñì 
Œ¥p
k Œ¥w
‚Ñì‚àíŒ¥w
k Œ¥p
‚Ñì

dxw = ‚àí
s
‚àög ‚àÇp
‚àög Fkp
dxk.
In our space-time s = ‚àí1. Setting ‚àód ‚àóF equal to j = jk dxk = jk dxk mul-
tiplied by the permeability Œº0 of the vacuum, we arrive at expressions for the
microscopic inhomogeneous Maxwell equations in terms of both tensors and
forms
‚àÇp
‚àög Fkp
= Œº0
‚àög jk
and
‚àód ‚àóF = Œº0 j.
(11.208)
They and the homogeneous Bianchi identity (11.93, 11.114, & 11.247)
œµijk‚Ñì‚àÇ‚ÑìFjk = dF = d dA = 0
(11.209)
are invariant under general coordinate transformations.
11.27 Derivatives and afÔ¨Åne connections
If F(x) is a vector Ô¨Åeld, then its invariant description in terms of space-time
dependent basis vectors ei(x) is
F(x) = Fi(x) ei(x).
(11.210)
Since the basis vectors ei(x) vary with x, the derivative of F(x) contains two
terms
‚àÇF
‚àÇx‚Ñì= ‚àÇFi
‚àÇx‚Ñìei + Fi ‚àÇei
‚àÇx‚Ñì.
(11.211)
In general, the derivative of a vector ei is not a linear combination of the
basis vectors ek. For instance, on the two-dimensional surface of a sphere in
three dimensions, the derivative
‚àÇeŒ∏
‚àÇŒ∏ = ‚àíÀÜr
(11.212)
points to the sphere‚Äôs center and isn‚Äôt a linear combination of eŒ∏ and eœÜ.
The inner product of a derivative ‚àÇei/‚àÇx‚Ñìwith a dual basis vector ek is the
Levi-Civita afÔ¨Åne connection
k
‚Ñìi = ek ¬∑ ‚àÇei
‚àÇx‚Ñì,
(11.213)
431

TENSORS AND LOCAL SYMMETRIES
which relates spaces that are tangent to the manifold at inÔ¨Ånitesimally separated
points. It is called an afÔ¨Åne connection because the different tangent spaces lack
a common origin.
In terms of the afÔ¨Åne connection (11.213 ), the inner product of the derivative
of (11.211) with ek is
ek ¬∑ ‚àÇF
‚àÇx‚Ñì= ek ¬∑ ‚àÇFi
‚àÇx‚Ñìei + Fi ek ¬∑ ‚àÇei
‚àÇx‚Ñì= ‚àÇFk
‚àÇx‚Ñì+ k
‚Ñìi Fi
(11.214)
a combination that is called a covariant derivative (section 11.30)
D‚ÑìFk ‚â°‚àá‚ÑìFk ‚â°‚àÇFk
‚àÇx‚Ñì+ k
‚Ñìi Fi.
(11.215)
Some physicists write the afÔ¨Åne connection k
i‚Ñìas
 k
i‚Ñì
%
= k
i‚Ñì
(11.216)
and call it a Christoffel symbol of the second kind.
The vectors ei are the space-time derivatives (11.135) of the point p, and so
the afÔ¨Åne connection (11.213) is a double derivative of p
k
‚Ñìi = ek ¬∑ ‚àÇei
‚àÇx‚Ñì= ek ¬∑
‚àÇ2p
‚àÇx‚Ñì‚àÇxi = ek ¬∑
‚àÇ2p
‚àÇxi‚àÇx‚Ñì= ek ¬∑ ‚àÇe‚Ñì
‚àÇxi = k
i‚Ñì
(11.217)
and thus is symmetric in its two lower indices
k
i‚Ñì= k
‚Ñìi.
(11.218)
AfÔ¨Åne connections are not tensors. Tensors transform homogeneously; con-
nections transform inhomogeneously. The connection k
i‚Ñìtransforms as
‚Ä≤k
i‚Ñì= e‚Ä≤k ¬∑ ‚àÇe‚Ä≤
‚Ñì
‚àÇx‚Ä≤i = ‚àÇx‚Ä≤k
‚àÇxp ep ¬∑ ‚àÇxm
‚àÇx‚Ä≤i
‚àÇ
‚àÇxm
 ‚àÇxn
‚àÇx‚Ä≤‚Ñìen

= ‚àÇx‚Ä≤k
‚àÇxp
‚àÇxm
‚àÇx‚Ä≤i
‚àÇxn
‚àÇx‚Ä≤‚Ñìep ¬∑ ‚àÇen
‚àÇxm + ‚àÇx‚Ä≤k
‚àÇxp
‚àÇ2xp
‚àÇx‚Ä≤i‚àÇx‚Ä≤‚Ñì
= ‚àÇx‚Ä≤k
‚àÇxp
‚àÇxm
‚àÇx‚Ä≤i
‚àÇxn
‚àÇx‚Ä≤‚Ñìp
mn + ‚àÇx‚Ä≤k
‚àÇxp
‚àÇ2xp
‚àÇx‚Ä≤i‚àÇx‚Ä≤‚Ñì.
(11.219)
The electromagnetic Ô¨Åeld Ai(x) and other gauge Ô¨Åelds are connections.
Since the Levi-Civita connection k
i‚Ñìis symmetric in i and ‚Ñì, in four-
dimensional space-time, there are ten of them for k, or 40 in all. The ten
correspond to three rotations, three boosts, and four translations.
Einstein‚ÄìCartan theories do not assume that the space-time manifold is
embedded in a Ô¨Çat space of higher dimension. So their basis vectors need not
432

11.29 NOTATIONS FOR DERIVATIVES
be partial derivatives of a point in the embedding space, and their afÔ¨Åne con-
nections a
bc need not be symmetric in their lower indices. The antisymmetric
part is the torsion tensor
Ta
bc = a
bc ‚àía
cb.
(11.220)
11.28 Parallel transport
The movement of a vector along a curve on a manifold so that its direction
in successive tangent spaces does not change is called parallel transport. If the
vector is F = Fiei, then we want ek¬∑dF to vanish along the curve. But this is just
the condition that the covariant derivative of F should vanish along the curve
ek ¬∑ ‚àÇF
‚àÇx‚Ñì= ek ¬∑ ‚àÇFi
‚àÇx‚Ñìei + Fi ek ¬∑ ‚àÇei
‚àÇx‚Ñì= ‚àÇFk
‚àÇx‚Ñì+ k
‚Ñìi Fi = D‚ÑìFk = 0.
(11.221)
Example 11.18 (Parallel transport on a sphere)
The tangent space on a 2-sphere
is spanned by the unit basis vectors
ÀÜŒ∏ = (cos Œ∏ cos œÜ, cos Œ∏ sin œÜ, ‚àísin Œ∏) ,
ÀÜœÜ = (‚àísin œÜ, cos œÜ, 0) .
(11.222)
We can parallel-transport the vector ÀÜœÜ down from the north pole along the
meridian œÜ = 0 to the equator; all along this path ÀÜœÜ = (0, 1, 0). Then we can
parallel-transport it along the equator to œÜ = œÄ/2 where it is (‚àí1, 0, 0). Then we
can parallel-transport it along the meridian œÜ = œÄ/2 up to the north pole where
it is (‚àí1, 0, 0) as it was on the equator. The change from (0, 1, 0) to (‚àí1, 0, 0) is
due to the curvature of the sphere.
11.29 Notations for derivatives
We have various notations for derivatives. We can use the variables x, y, and so
forth as subscripts to label derivatives
fx = ‚àÇxf = ‚àÇf
‚àÇx
and
fy = ‚àÇyf = ‚àÇf
‚àÇy.
(11.223)
If we use indices to label variables, then we can use commas
f,i = ‚àÇif = ‚àÇf
‚àÇxi
and
f,ik = ‚àÇk‚àÇif =
‚àÇ2f
‚àÇxk‚àÇxi
(11.224)
and f,k‚Ä≤ = ‚àÇf /‚àÇx‚Ä≤k. For instance, we may write part of (11.217) as ei,‚Ñì= e‚Ñì,i.
433

TENSORS AND LOCAL SYMMETRIES
11.30 Covariant derivatives
In comma notation, the derivative of a contravariant vector Ô¨Åeld F = Fi ei is
F,‚Ñì= Fi
,‚Ñìei + Fi ei,‚Ñì,
(11.225)
which in general lies outside the space spanned by the basis vectors ei. So we
use the afÔ¨Åne connections (11.213) to form the inner product
ek ¬∑ F,‚Ñì= ek ¬∑

Fi
,‚Ñìei + Fiei,‚Ñì

= Fi
,‚ÑìŒ¥k
i + Fi k
‚Ñìi = Fk
,‚Ñì+ k
‚Ñìi Fi.
(11.226)
This covariant derivative of a contravariant vector Ô¨Åeld often is written with a
semicolon
Fk
;‚Ñì= ek ¬∑ F,‚Ñì= Fk
,‚Ñì+ k
‚Ñìi Fi.
(11.227)
It transforms as a mixed second-rank tensor. The invariant change dF projected
onto ek is
ek ¬∑ dF = ek ¬∑ F,‚Ñìdx‚Ñì= Fk
;‚Ñìdx‚Ñì.
(11.228)
In terms of its covariant components, the derivative of a vector V is
V,‚Ñì= (Vk ek),‚Ñì= Vk,‚Ñìek + Vk ek
,‚Ñì.
(11.229)
To relate the derivatives of the vectors ei to the afÔ¨Åne connections k
i‚Ñì, we
differentiate the orthonormality relation
Œ¥k
i = ek ¬∑ ei,
(11.230)
which gives us
0 = ek
,‚Ñì¬∑ ei + ek ¬∑ ei,‚Ñì
or
ek
,‚Ñì¬∑ ei = ‚àíek ¬∑ ei,‚Ñì= ‚àík
i‚Ñì.
(11.231)
Since ei ¬∑ ek
,‚Ñì= ‚àík
i‚Ñì, the inner product of ei with the derivative of V is
ei ¬∑ V,‚Ñì= ei ¬∑

Vk,‚Ñìek + Vk ek
,‚Ñì

= Vi,‚Ñì‚àíVkk
i‚Ñì.
(11.232)
This covariant derivative of a covariant vector Ô¨Åeld also is often written with a
semicolon
Vi;‚Ñì= ei ¬∑ V,‚Ñì= Vi,‚Ñì‚àíVkk
i‚Ñì.
(11.233)
It transforms as a rank-2 covariant tensor. Note the minus sign in Vi;‚Ñìand the
plus sign in Fk
;‚Ñì. The change ei ¬∑ dV is
ei ¬∑ dV = ei ¬∑ V,‚Ñìdx‚Ñì= Vi;‚Ñìdx‚Ñì.
(11.234)
Since dV is invariant, ei covariant, and dx‚Ñìcontravariant, the quotient rule
(section 11.15) conÔ¨Årms that the covariant derivative Vi;‚Ñìof a covariant vector
Vi is a rank-2 covariant tensor.
434

11.31 THE COVARIANT CURL
11.31 The covariant curl
Because the connection k
i‚Ñìis symmetric (11.218) in its lower indices, the
covariant curl of a covariant vector Vi is simply its ordinary curl
V‚Ñì;i ‚àíVi;‚Ñì= V‚Ñì,i ‚àíVk k
‚Ñìi ‚àíVi,‚Ñì+ Vk k
i‚Ñì= V‚Ñì,i ‚àíVi,‚Ñì.
(11.235)
Thus the Faraday Ô¨Åeld-strength tensor Fi‚Ñìwhich is deÔ¨Åned as the curl of the
covariant vector Ô¨Åeld Ai
Fi‚Ñì= A‚Ñì,i ‚àíAi,‚Ñì
(11.236)
is a generally covariant second-rank tensor.
In orthogonal coordinates, the curl is deÔ¨Åned (6.39, 11.111) in terms of the
totally antisymmetric Levi-Civita symbol œµijk (with œµ123 = œµ123 = 1), as
‚àá√ó V =
3

i=1
(‚àá√ó V)i ÀÜei =
1
h1h2h3
3

ijk=1
ei œµijk Vk;j,
(11.237)
which, in view of (11.235) and the antisymmetry of œµijk, is
‚àá√ó V =
3

i=1
(‚àá√ó V)i ÀÜei =
3

ijk=1
1
hihjhk
ei œµijk Vk,j
(11.238)
or by (11.159 & 11.161)
‚àá√ó V =
3

ijk=1
1
hihjhk
hiÀÜei œµijk Vk,j =
3

ijk=1
1
hihjhk
hiÀÜei œµijk (hkVk),j .
(11.239)
Often one writes this as a determinant
‚àá√ó V =
1
h1h2h3

e1
e2
e3
‚àÇ1
‚àÇ2
‚àÇ3
V1
V2
V3

=
1
h1h2h3

h1ÀÜe1
h2ÀÜe2
h3ÀÜe3
‚àÇ1
‚àÇ2
‚àÇ3
h1V1
h2V2
h3V3

.
(11.240)
In cylindrical coordinates, the curl is
‚àá√ó V = 1
œÅ

ÀÜœÅ
œÅ ÀÜœÜ
ÀÜz
‚àÇœÅ
‚àÇœÜ
‚àÇz
VœÅ
œÅVœÜ
Vz

.
(11.241)
In spherical coordinates, it is
‚àá√ó V =
1
r2 sin Œ∏

ÀÜr
r ÀÜŒ∏
r sin Œ∏ ÀÜœÜ
‚àÇr
‚àÇŒ∏
‚àÇœÜ
Vr
rVŒ∏
r sin Œ∏ VœÜ

.
(11.242)
435

TENSORS AND LOCAL SYMMETRIES
In more formal language, the curl is
dV = d

Vkdxk
= Vk,i dxi ‚àßdxk = 1
2

Vk,i ‚àíVi,k

dxi ‚àßdxk.
(11.243)
11.32 Covariant derivatives and antisymmetry
By applying our rule (11.233) for the covariant derivative of a covariant vector
to a second-rank tensor Ai‚Ñì, we get
Ai‚Ñì;k = Ai‚Ñì,k ‚àím
ikAm‚Ñì‚àím
‚ÑìkAim.
(11.244)
Suppose now that our tensor is antisymmetric
Ai‚Ñì= ‚àíA‚Ñìi.
(11.245)
Then by adding together the three cyclic permutations of the indices i‚Ñìk we Ô¨Ånd
that the antisymmetry of the tensor and the symmetry (11.218) of the afÔ¨Åne
connection conspire to cancel the nonlinear terms
Ai‚Ñì;k + Aki;‚Ñì+ A‚Ñìk;i = Ai‚Ñì,k ‚àím
ikAm‚Ñì‚àím
‚ÑìkAim
+Aki,‚Ñì‚àím
k‚ÑìAmi ‚àím
i‚ÑìAkm
+A‚Ñìk,i ‚àím
‚ÑìiAmk ‚àím
kiA‚Ñìm
= Ai‚Ñì,k + Aki,‚Ñì+ A‚Ñìk,i,
(11.246)
an identity named after Luigi Bianchi (1856‚Äì1928).
The Maxwell Ô¨Åeld-strength tensor Fi‚Ñìis antisymmetric by construction
(Fi‚Ñì= A‚Ñì,i ‚àíAi,‚Ñì), and so the homogeneous Maxwell equations
œµijk‚ÑìFjk,‚Ñì= Fjk,‚Ñì+ Fk‚Ñì,j + F‚Ñìj,k = 0
(11.247)
are tensor equations valid in all coordinate systems. This is another example of
how amazingly right Maxwell was in the middle of the nineteenth century.
11.33 AfÔ¨Åne connection and metric tensor
To relate the afÔ¨Åne connection m
‚Ñìi to the derivatives of the metric tensor gk‚Ñì, we
lower the contravariant index m to get
k‚Ñìi = gkm m
‚Ñìi = gkm m
i‚Ñì= ki‚Ñì,
(11.248)
which is symmetric in its last two indices and which some call a Christoffel
symbol of the Ô¨Årst kind, written [‚Ñìi, k]. One can raise the index k back up by
using the inverse of the metric tensor
gmk k‚Ñìi = gmk gkn n
‚Ñìi = Œ¥m
n n
‚Ñìi = m
‚Ñìi.
(11.249)
436

11.34 COVARIANT DERIVATIVE OF THE METRIC TENSOR
Although we can raise and lower these indices, the connections m
‚Ñìi and k‚Ñìi are
not tensors.
The deÔ¨Ånition (11.213) of the afÔ¨Åne connection tells us that
k‚Ñìi = gkm m
‚Ñìi = gkm em ¬∑ e‚Ñì,i = ek ¬∑ e‚Ñì,i = ki‚Ñì= ek ¬∑ ei,‚Ñì.
(11.250)
By differentiating the deÔ¨Ånition gi‚Ñì= ei ¬∑ e‚Ñìof the metric tensor, we Ô¨Ånd
gi‚Ñì,k = ei,k ¬∑ e‚Ñì+ ei ¬∑ e‚Ñì,k = e‚Ñì¬∑ ei,k + ei ¬∑ e‚Ñì,k = ‚Ñìik + i‚Ñìk.
(11.251)
Permuting the indices cyclicly, we have
gki,‚Ñì= ik‚Ñì+ ki‚Ñì,
g‚Ñìk,i = k‚Ñìi + ‚Ñìki.
(11.252)
If we now subtract relation (11.251) from the sum of the two formulas (11.252)
keeping in mind the symmetry abc = acb, then we Ô¨Ånd that four of the six
terms cancel
gki,‚Ñì+ g‚Ñìk,i ‚àígi‚Ñì,k = ik‚Ñì+ ki‚Ñì+ k‚Ñìi + ‚Ñìki ‚àí‚Ñìik ‚àíi‚Ñìk = 2k‚Ñìi (11.253)
leaving a formula for k‚Ñìi
k‚Ñìi = 1
2

gki,‚Ñì+ g‚Ñìk,i ‚àígi‚Ñì,k

.
(11.254)
Thus the connection is three derivatives of the metric tensor
s
i‚Ñì= gskk‚Ñìi = 1
2gsk 
gki,‚Ñì+ g‚Ñìk,i ‚àígi‚Ñì,k

.
(11.255)
11.34 Covariant derivative of the metric tensor
Covariant derivatives of second-rank and higher-rank tensors are formed by
iterating our formulas for the covariant derivatives of vectors. For instance, the
covariant derivative of the metric tensor is
gi‚Ñì;k ‚â°gi‚Ñì,k ‚àím
ik gm‚Ñì‚àín
k‚Ñìgin.
(11.256)
One way to derive this formula is to proceed as in section 11.30 by differentiat-
ing the invariant metric tensor gi‚Ñìei ‚äóe‚Ñìin which the vector product ei ‚äóe‚Ñìis
a kind of direct product
g,k = (gi‚Ñìei ‚äóe‚Ñì),k = gi‚Ñì,k ei ‚äóe‚Ñì+ gi‚Ñìei
,k ‚äóe‚Ñì+ gi‚Ñìei ‚äóe‚Ñì
,k.
(11.257)
We now take the inner product of this derivative with em ‚äóen
(em ‚äóen, g,k) = gi‚Ñì,k em ¬∑ ei en ¬∑ e‚Ñì+ gi‚Ñìem ¬∑ ei
,k en ¬∑ e‚Ñì+ gi‚Ñìem ¬∑ ei en ¬∑ e‚Ñì
,k (11.258)
and use the rules em ¬∑ ei = Œ¥i
m and em ¬∑ ei
,k = ‚àíi
mk (11.231) to write
(em ‚äóen, g,k) = gmn;k = gmn,k ‚àígi‚Ñìi
mkŒ¥‚Ñì
n ‚àígi‚ÑìŒ¥i
m‚Ñì
nk
(11.259)
437

TENSORS AND LOCAL SYMMETRIES
or
gmn;k = gmn,k ‚àíi
mkgin ‚àí‚Ñì
nkgm‚Ñì,
(11.260)
which is (11.256) inasmuch as both gi‚Ñìand k
i‚Ñìare symmetric in their two lower
indices.
If we now substitute our formula (11.255) for the connections l
ik and n
k‚Ñì
gi‚Ñì;k = gi‚Ñì,k ‚àí1
2gms 
gis,k + gsk,i ‚àígik,s

gm‚Ñì‚àí1
2gns 
g‚Ñìs,k + gsk,‚Ñì‚àíg‚Ñìk,s

gin
(11.261)
and use the fact (11.145) that the metric tensors gi‚Ñìand g‚Ñìk are mutually inverse,
then we Ô¨Ånd
gi‚Ñì;k = gi‚Ñì,k ‚àí1
2Œ¥s
‚Ñì

gis,k + gsk,i ‚àígik,s

‚àí1
2Œ¥s
i

g‚Ñìs,k + gsk,‚Ñì‚àíg‚Ñìk,s

= gi‚Ñì,k ‚àí1
2

gi‚Ñì,k + g‚Ñìk,i ‚àígik,‚Ñì

‚àí1
2

g‚Ñìi,k + gik,‚Ñì‚àíg‚Ñìk,i

= 0.
(11.262)
The covariant derivative of the metric tensor vanishes. This result follows from
our choice of the Levi-Civita connection (11.213); it is not true for some other
connections.
11.35 Divergence of a contravariant vector
The contraction of the covariant derivative of a contravariant vector is a scalar
known as the divergence,
‚àá¬∑ V = Vi
;i = Vi
,i + i
ikVk.
(11.263)
Because two indices in the connection
i
ik = 1
2gim 
gim,k + gkm,i ‚àígik,m

(11.264)
are contracted, its last two terms cancel because they differ only by the
interchange of the dummy indices i and m
gimgkm,i = gmigkm,i = gimgki,m = gimgik,m.
(11.265)
So the contracted connection collapses to
i
ik = 1
2gimgim,k.
(11.266)
There is a nice formula for this last expression involving the absolute value
of the determinant det g ‚â°det gmn of the metric tensor considered as a matrix
g ‚â°gmn. To derive it, we recall that like any determinant, the determinant det(g)
of the metric tensor is given by the cofactor sum (1.195)
det(g) =

‚Ñì
gi‚ÑìCi‚Ñì
(11.267)
438

11.35 DIVERGENCE OF A CONTRAVARIANT VECTOR
along any row or column, that is, over ‚Ñìfor Ô¨Åxed i or over i for Ô¨Åxed ‚Ñì, where Ci‚Ñì
is the cofactor deÔ¨Åned as (‚àí1)i+‚Ñìtimes the determinant of the reduced matrix
consisting of the matrix gi‚Ñìwith row i and column ‚Ñìomitted. Thus the partial
derivative of det g with respect to the i‚Ñìth element gi‚Ñìis
‚àÇdet(g)
‚àÇgi‚Ñì
= Ci‚Ñì,
(11.268)
in which we consider gi‚Ñìand g‚Ñìi to be independent variables for the purposes of
this differentiation. The inverse gi‚Ñìof the metric tensor g, like the inverse (1.197)
of any matrix, is the transpose of the cofactor matrix divided by its determinant
det g,
gi‚Ñì=
C‚Ñìi
det(g) =
1
det(g)
‚àÇdet(g)
‚àÇg‚Ñìi
.
(11.269)
The chain rule gives us the derivative of the determinate det(g) as
det(g),k = gi‚Ñì,k
‚àÇdet(g)
‚àÇgi‚Ñì
= gi‚Ñì,k det(g) g‚Ñìi
(11.270)
and so, since gi‚Ñì= g‚Ñìi, the contracted connection (11.266) is
i
ik = 1
2gimgim,k =
det(g),k
2 det(g) =
| det(g)|,k
2| det(g)| = g,k
2g = (‚àög),k
‚àög ,
(11.271)
in which g ‚â°
det(g)
 is the absolute value of the determinant of the metric
tensor.
Thus from (11.263), we arrive at our formula for the covariant divergence of
a contravariant vector:
‚àá¬∑ V = Vi
;i = Vi
,i + i
ikVk = Vk
,k + (‚àög),k
‚àög Vk = (‚àög Vk),k
‚àög
.
(11.272)
More formally, the Hodge dual (11.202) of the 1-form V = Vi dxi is
‚àóV = Vi ‚àódxi = Vi
1
3! gik Œ∑k‚Ñìmn dx‚Ñì‚àßdxm ‚àßdxn
= 1
3!
‚àög Vk œµk‚Ñìmn dx‚Ñì‚àßdxm ‚àßdxn,
(11.273)
in which g is the absolute value of the determinant of the metric tensor gij. The
exterior derivative now gives
d ‚àóV = 1
3!
‚àög Vk
,p œµk‚Ñìmn dxp ‚àßdx‚Ñì‚àßdxm ‚àßdxn.
(11.274)
439

TENSORS AND LOCAL SYMMETRIES
So using (11.202) to apply a second Hodge star, we get (exercise 11.19)
‚àód ‚àóV = 1
3!
‚àög Vk
,p œµk‚Ñìmn ‚àó

dxp ‚àßdx‚Ñì‚àßdxm ‚àßdxn
= 1
3!
‚àög Vk
,p œµk‚Ñìmn gpt g‚Ñìu gmv gnwŒ∑tuvw
= 1
3!
‚àög Vk
,p œµk‚Ñìmn gpt g‚Ñìu gmv gnwœµtuvw
‚àög
= 1
3!
‚àög Vk
,p œµk‚Ñìmn
‚àög
det gij
œµp‚Ñìmn
=
s
‚àög
‚àög Vk
,p Œ¥p
k =
s
‚àög
‚àög Vk
,k .
(11.275)
So in our space-time with det gij = ‚àíg
‚àí‚àód ‚àóV =
1
‚àög
‚àög Vk
,k .
(11.276)
In 3-space the Hodge star (11.191) of a 1-form V = Vi dxi is
‚àóV = Vi ‚àódxi = Vi
1
2 gi‚ÑìŒ∑‚Ñìjk dxj ‚àßdxk = 1
2
‚àög V‚Ñìœµ‚Ñìjk dxj ‚àßdxk.
(11.277)
Applying the exterior derivative, we get the invariant form
d ‚àóV = 1
2
‚àög V‚Ñì
,p œµ‚Ñìjk dxp ‚àßdxj ‚àßdxk.
(11.278)
We add a star by using the deÔ¨Ånition (11.191) of the Hodge dual in a 3-space in
which the determinant det gij is positive and the identity (exercise 11.18)
œµ‚Ñìjk œµpjk = 2 Œ¥p
‚Ñì
(11.279)
as well as the deÔ¨Ånition (1.184) of the determinant
‚àód ‚àóV = 1
2
‚àög V‚Ñì
,p œµ‚Ñìjk ‚àó

dxp ‚àßdxj ‚àßdxk
= 1
2
‚àög V‚Ñì
,p œµ‚Ñìjk gptgjugkvŒ∑tuv
= 1
2
‚àög V‚Ñì
,p œµ‚Ñìjk gptgjugkvœµtuv
‚àög
= 1
2
‚àög V‚Ñì
,p œµ‚Ñìjk œµpjk
‚àög
det gij
=
1
‚àög
‚àög V‚Ñì
,p Œ¥p
‚Ñì=
1
‚àög
‚àög Vp
,p .
(11.280)
440

11.36 THE COVARIANT LAPLACIAN
Example 11.19 (Divergence in orthogonal coordinates)
In two orthogonal
coordinates, equations (11.154 & 11.161) imply that ‚àög = h1h2 and Vk =
Vk/hk, and so the divergence of a vector V is
‚àá¬∑ V =
1
h1h2
2

k=1
h1h2
hk
Vk

,k
,
(11.281)
which in polar coordinates (section 11.21), with hr = 1 and hŒ∏ = r, is
‚àá¬∑ V = 1
r

r Vr

,r +

VŒ∏

,Œ∏

= 1
r

r Vr

,r + VŒ∏,Œ∏

.
(11.282)
In three orthogonal coordinates, equations (11.154 & 11.161) give ‚àög =
h1h2h3 and Vk = Vk/hk, and so the divergence of a vector V is (6.29)
‚àá¬∑ V =
1
h1h2h3
3

k=1
h1h2h3
hk
Vk

,k
.
(11.283)
In cylindrical coordinates (section 11.22), hœÅ = 1, hœÜ = œÅ, and hz = 1; so
‚àá¬∑ V = 1
œÅ

œÅ VœÅ

,œÅ +

VœÜ

,œÜ +

œÅ Vz

,z

= 1
œÅ

œÅ VœÅ

,œÅ + VœÜ,œÜ + œÅ Vz,z

.
(11.284)
In spherical coordinates (section 11.23), hr = 1, hŒ∏ = r, hœÜ = r sin Œ∏, g =
| det g| = r4 sin2 Œ∏ and the inverse gij of the metric tensor is
(gij) =
‚éõ
‚éù
1
0
0
0
r‚àí2
0
0
0
r‚àí2 sin‚àí2 Œ∏
‚éû
‚é†.
(11.285)
So our formula (11.281) gives us
‚àá¬∑ V =
1
r2 sin Œ∏
#
r2 sin Œ∏ Vr

,r +

r sin Œ∏ VŒ∏

,Œ∏ +

r VœÜ

,œÜ
$
=
1
r2 sin Œ∏
#
sin Œ∏

r2Vr

,r + r

sin Œ∏ VŒ∏

,Œ∏ + rVœÜ,œÜ
$
(11.286)
as the divergence ‚àá¬∑ V.
11.36 The covariant Laplacian
In Ô¨Çat 3-space, we write the Laplacian as ‚àá¬∑ ‚àá= ‚àá2 or as ‚ñ≥. In euclidean
coordinates, both mean ‚àÇ2
x + ‚àÇ2
y + ‚àÇ2
z . In Ô¨Çat Minkowski space, one often turns
the triangle into a square and writes the 4-Laplacian as 2 = ‚ñ≥‚àí‚àÇ2
t .
441

TENSORS AND LOCAL SYMMETRIES
Since the gradient of a scalar Ô¨Åeld f is a covariant vector, we may use the
inverse metric tensor gij to write the Laplacian 2f of a scalar f as the covariant
divergence of the contravariant vector gikf,k
2f = (gikf,k);i.
(11.287)
The divergence formula (11.272) now expresses the invariant Laplacian as
2f = (‚àög gikf,k),i
‚àög
= (‚àög f ,i),i
‚àög
.
(11.288)
To Ô¨Ånd the Laplacian 2f in terms of forms, we apply the exterior derivative
to the Hodge dual (11.202) of the 1-form df = f,idxi
d ‚àódf = d

f,i ‚àódxi
= d
 1
3! f,i gik Œ∑k‚Ñìmn dx‚Ñì‚àßdxm ‚àßdxn

= 1
3!

f ,k ‚àög

,p œµk‚Ñìmn dxp ‚àßdx‚Ñì‚àßdxm ‚àßdxn
(11.289)
and then add a star using (11.202)
‚àód ‚àódf = 1
3!

f ,k ‚àög

,p œµk‚Ñìmn ‚àó

dxp ‚àßdx‚Ñì‚àßdxm ‚àßdxn
= 1
3!

f ,k ‚àög

,p œµk‚Ñìmn gptg‚Ñìugmvgnw‚àög œµtuvw.
(11.290)
The deÔ¨Ånition (1.184) of the determinant now gives (exercise 11.19)
‚àód ‚àódf = 1
3!

f ,k ‚àög

,p œµk‚Ñìmn œµp‚Ñìmn
‚àög
det g
=

f ,k ‚àög

,p Œ¥p
k
s
‚àög =
s
‚àög

f ,k ‚àög

,k .
(11.291)
In our space-time det gij = sg = ‚àíg, and so the Laplacian is
2f = ‚àí‚àód ‚àódf =
1
‚àög

f ,k ‚àög

,k .
(11.292)
Example 11.20 (Invariant Laplacians)
In two orthogonal coordinates, equa-
tions (11.154 & 11.155) imply that ‚àög =

| det(gij)| = h1h2 and that f ,i =
gik f,k = h‚àí2
i
f,i, and so the Laplacian of a scalar f is
‚ñ≥f =
1
h1h2
 2

i=1
h1h2
h2
i
f,i

,i.
(11.293)
In polar coordinates, where h1 = 1, h2 = r, and g = r2, the Laplacian is
‚ñ≥f = 1
r
#
rf,r

,r +

r‚àí1f,Œ∏

,Œ∏
$
= f,rr + r‚àí1f,r + r‚àí2f,Œ∏Œ∏.
(11.294)
442

11.37 THE PRINCIPLE OF STATIONARY ACTION
In three orthogonal coordinates, equations (11.154 & 11.155) imply that ‚àög =

| det(gij)| = h1h2h3 and that f ,i = gik f,k = h‚àí2
i
f,i, and so the Laplacian of a
scalar f is (6.33)
‚ñ≥f =
1
h1h2h3
 3

i=1
h1h2h3
h2
i
f,i

,i.
(11.295)
In cylindrical coordinates (section 11.22), hœÅ = 1, hœÜ = œÅ, hz = 1, g = œÅ2, and
the Laplacian is
‚ñ≥f = 1
œÅ
#
œÅ f,œÅ

,œÅ + 1
œÅ f,œÜœÜ + œÅ f,zz
$
= f,œÅœÅ + 1
œÅ f,œÅ + 1
œÅ2 f,œÜœÜ + f,zz.
(11.296)
In spherical coordinates (section 11.23), hr = 1, hŒ∏ = r, hœÜ = r sin Œ∏, and
g = | det g| = r4 sin2 Œ∏. So (11.295) gives us the Laplacian of f as
‚ñ≥f =

r2 sin Œ∏f,r

,r +

sin Œ∏f,Œ∏

,Œ∏ +

f,œÜ/ sin Œ∏

,œÜ
r2 sin Œ∏
=

r2f,r

,r
r2
+

sin Œ∏f,Œ∏

,Œ∏
r2 sin Œ∏
+
f,œÜœÜ
r2 sin2 Œ∏
.
(11.297)
If the function f is a function only of the radial variable r, then the Laplacian is
simply
‚ñ≥f (r) = 1
r2

r2f ‚Ä≤(r)
‚Ä≤
= 1
r [rf (r)]‚Ä≤‚Ä≤ = f ‚Ä≤‚Ä≤(r) + 2
r f ‚Ä≤(r),
(11.298)
in which the primes denote r-derivatives.
11.37 The principle of stationary action
It follows from a path-integral formulation of quantum mechanics that the clas-
sical motion of a particle is given by the principle of stationary action Œ¥S = 0. In
the simplest case of a free nonrelativistic particle, the lagrangian is L = mÀôx2/2
and the action is
S =
 t2
t1
m
2 Àôx2 dt.
(11.299)
The classical trajectory is the one that when varied slightly by Œ¥x (with Œ¥x(t1) =
Œ¥x(t2) = 0) does not change the action to Ô¨Årst order in Œ¥x. We Ô¨Årst note that the
change Œ¥Àôx in the velocity is the time derivative of the change in the path
Œ¥Àôx = Àôx‚Ä≤ ‚àíÀôx = d
dt(x‚Ä≤ ‚àíx) = d
dtŒ¥x.
(11.300)
443

TENSORS AND LOCAL SYMMETRIES
So since Œ¥x(t1) = Œ¥x(t2) = 0, the stationary path satisÔ¨Åes
0 = Œ¥S =
 t2
t1
mÀôx ¬∑ Œ¥Àôx dt =
 t2
t1
mÀôx ¬∑ dŒ¥x
dt dt
=
 t2
t1
#
m d
dt (Àôx ¬∑ Œ¥x) ‚àím¬®x ¬∑ Œ¥x
$
dt
= m [Àôx ¬∑ Œ¥x]t2
t1 ‚àím
 t2
t1
¬®x ¬∑ Œ¥x dt = ‚àím
 t2
t1
¬®x ¬∑ Œ¥x dt.
(11.301)
If the Ô¨Årst-order change in the action is to vanish for arbitrary small variations
Œ¥x in the path, then the acceleration must vanish
¬®x = 0,
(11.302)
which is the classical equation of motion for a free particle.
If the particle is moving under the inÔ¨Çuence of a potential V(x), then the
action is
S =
 t2
t1
m
2 Àôx2 ‚àíV(x)

dt.
(11.303)
Since Œ¥V(x) = ‚àáV(x) ¬∑ Œ¥x, the principle of stationary action requires that
0 = Œ¥S =
 t2
t1
(‚àím¬®x ‚àí‚àáV) ¬∑ Œ¥x dt
(11.304)
or
m¬®x = ‚àí‚àáV,
(11.305)
which is the classical equation of motion for a particle of mass m in a
potential V.
The action for a free particle of mass m in special relativity is
S = ‚àím
 œÑ2
œÑ1
dœÑ = ‚àí
 t2
t1
m

1 ‚àíÀôx2 dt
(11.306)
where c = 1 and Àôx = dx/dt. The requirement of stationary action is
0 = Œ¥S = ‚àíŒ¥
 t2
t1
m

1 ‚àíÀôx2 dt = m
 t2
t1
Àôx ¬∑ Œ¥Àôx

1 ‚àíÀôx2 dt.
(11.307)
But 1/

1 ‚àíÀôx2 = dt/dœÑ and so
0 = Œ¥S = m
 t2
t1
dx
dt ¬∑ dŒ¥x
dt
dt
dœÑ dt = m
 œÑ2
œÑ1
dx
dt ¬∑ dŒ¥x
dt
dt
dœÑ
dt
dœÑ dœÑ
= m
 œÑ2
œÑ1
dx
dœÑ ¬∑ dŒ¥x
dœÑ dœÑ.
(11.308)
444

11.37 THE PRINCIPLE OF STATIONARY ACTION
So, integrating by parts, keeping in mind that Œ¥x(œÑ2) = Œ¥x(œÑ1) = 0, we have
0 = Œ¥S = m
 œÑ2
œÑ1

d
dœÑ (Àôx ¬∑ Œ¥x) ‚àíd2x
dœÑ 2 ¬∑ Œ¥x

dœÑ = ‚àím
 œÑ2
œÑ1
d2x
dœÑ 2 ¬∑Œ¥x dœÑ. (11.309)
To have this hold for arbitrary Œ¥x, we need
d2x
dœÑ 2 = 0,
(11.310)
which is the equation of motion for a free particle in special relativity.
What about a charged particle in an electromagnetic Ô¨Åeld Ai? Its action is
S = ‚àím
 œÑ2
œÑ1
dœÑ + q
 x2
x1
Ai(x) dxi =
 œÑ2
œÑ1

‚àím + qAi(x)dxi
dœÑ

dœÑ.
(11.311)
We now treat the Ô¨Årst term in a four-dimensional manner
Œ¥dœÑ = Œ¥

‚àíŒ∑ikdxidxk = ‚àíŒ∑ikdxiŒ¥dxk

‚àíŒ∑ikdxidxk = ‚àíukŒ¥dxk = ‚àíukdŒ¥xk,
(11.312)
in which uk = dxk/dœÑ is the 4-velocity (11.66) and Œ∑ is the Minkowski metric
(11.27) of Ô¨Çat space-time. The variation of the other term is
Œ¥

Ai dxi
= (Œ¥Ai) dxi + Ai Œ¥dxi = Ai,kŒ¥xk dxi + Ai dŒ¥xi.
(11.313)
Putting them together, we get for Œ¥S
Œ¥S =
 œÑ2
œÑ1

muk
dŒ¥xk
dœÑ
+ qAi,kŒ¥xk dxi
dœÑ + qAi
dŒ¥xi
dœÑ

dœÑ.
(11.314)
After integrating by parts the last term, dropping the boundary terms, and
changing a dummy index, we get
Œ¥S =
 œÑ2
œÑ1

‚àímduk
dœÑ Œ¥xk + qAi,kŒ¥xk dxi
dœÑ ‚àíqdAk
dœÑ Œ¥xk

dœÑ
=
 œÑ2
œÑ1
#
‚àímduk
dœÑ + q

Ai,k ‚àíAk,i
 dxi
dœÑ
$
Œ¥xk dœÑ.
(11.315)
If this Ô¨Årst-order variation of the action is to vanish for arbitrary Œ¥xk, then the
particle must follow the path
0 = ‚àímduk
dœÑ + q

Ai,k ‚àíAk,i
 dxi
dœÑ
or
dpk
dœÑ = qFkiui,
(11.316)
which is the Lorentz force law (11.96).
445

TENSORS AND LOCAL SYMMETRIES
11.38 A particle in a gravitational Ô¨Åeld
The invariant action for a particle of mass m moving along a path xi(t) is
S = ‚àím
 œÑ2
œÑ1
dœÑ = ‚àím
 
‚àígi‚Ñìdxidx‚Ñì 1
2 .
(11.317)
Proceeding as in equation (11.312), we compute the variation Œ¥dœÑ as
Œ¥dœÑ = Œ¥

‚àígi‚Ñìdxidx‚Ñì= ‚àíŒ¥(gi‚Ñì)dxidx‚Ñì‚àí2gi‚ÑìdxiŒ¥dx‚Ñì
2

‚àígi‚Ñìdxidx‚Ñì
= ‚àí1
2gi‚Ñì,kŒ¥xkuiu‚ÑìdœÑ ‚àígi‚ÑìuiŒ¥dx‚Ñì
= ‚àí1
2gi‚Ñì,kŒ¥xkuiu‚ÑìdœÑ ‚àígi‚ÑìuidŒ¥x‚Ñì,
(11.318)
in which u‚Ñì= dx‚Ñì/dœÑ is the 4-velocity (11.66). The condition of stationary
action then is
0 = Œ¥S = ‚àím
 œÑ2
œÑ1
Œ¥dœÑ = m
 œÑ2
œÑ1

1
2gi‚Ñì,kŒ¥xkuiu‚Ñì+ gi‚Ñìui dŒ¥x‚Ñì
dœÑ

dœÑ,
(11.319)
which we integrate by parts keeping in mind that Œ¥x‚Ñì(œÑ2) = Œ¥x‚Ñì(œÑ1) = 0
0 = m
 œÑ2
œÑ1

1
2gi‚Ñì,kŒ¥xkuiu‚Ñì‚àíd(gi‚Ñìui)
dœÑ
Œ¥x‚Ñì

dœÑ
= m
 œÑ2
œÑ1

1
2gi‚Ñì,kŒ¥xkuiu‚Ñì‚àígi‚Ñì,kuiukŒ¥x‚Ñì‚àígi‚Ñì
dui
dœÑ Œ¥x‚Ñì

dœÑ.
(11.320)
Now interchanging the dummy indices ‚Ñìand k on the second and third terms,
we have
0 = m
 œÑ2
œÑ1

1
2gi‚Ñì,kuiu‚Ñì‚àígik,‚Ñìuiu‚Ñì‚àígik
dui
dœÑ

Œ¥xkdœÑ
(11.321)
or since Œ¥xk is arbitrary
0 = 1
2gi‚Ñì,kuiu‚Ñì‚àígik,‚Ñìuiu‚Ñì‚àígik
dui
dœÑ .
(11.322)
If we multiply this equation of motion by grk and note that gik,‚Ñìuiu‚Ñì= g‚Ñìk,iuiu‚Ñì,
then we Ô¨Ånd
0 = dur
dœÑ + 1
2grk 
gik,‚Ñì+ g‚Ñìk,i ‚àígi‚Ñì,k

uiu‚Ñì.
(11.323)
So, using the symmetry gi‚Ñì= g‚Ñìi and the formula (11.255) for r
i‚Ñì, we get
0 = dur
dœÑ + r
i‚Ñìuiu‚Ñì
or
0 = d2xr
dœÑ 2 + r
i‚Ñì
dxi
dœÑ
dx‚Ñì
dœÑ ,
(11.324)
446

11.39 THE PRINCIPLE OF EQUIVALENCE
which is the geodesic equation. In empty space, particles fall along geodesics
independently of their masses.
The right-hand side of the geodesic equation (11.324) is a contravariant vec-
tor because (Weinberg, 1972) under general coordinate transformations, the
inhomogeneous terms arising from ¬®xr cancel those from r
i‚ÑìÀôxi Àôx‚Ñì. Here and often
in what follows we‚Äôll use dots to mean proper-time derivatives.
The action for a particle of mass m and charge q in a gravitational Ô¨Åeld r
i‚Ñì
and an electromagnetic Ô¨Åeld Ai is
S = ‚àím
 
‚àígi‚Ñìdxidx‚Ñì 1
2 + q
 œÑ2
œÑ1
Ai(x) dxi
(11.325)
because the interaction q
2
Aidxi is invariant under general coordinate transfor-
mations. By (11.315 & 11.321), the Ô¨Årst-order change in S is
Œ¥S = m
 œÑ2
œÑ1
#
1
2gi‚Ñì,kuiu‚Ñì‚àígik,‚Ñìuiu‚Ñì‚àígik
dui
dœÑ + q

Ai,k ‚àíAk,i

ui
$
Œ¥xkdœÑ
(11.326)
and so by combining the Lorentz force law (11.316) and the geodesic equation
(11.324) and by writing Fri Àôxi as Fr
i Àôxi, we have
0 = d2xr
dœÑ 2 + r
i‚Ñì
dxi
dœÑ
dx‚Ñì
dœÑ ‚àíq
m Fr
i
dxi
dœÑ
(11.327)
as the equation of motion of a particle of mass m and charge q. It is striking
how nearly perfect the electromagnetism of Faraday and Maxwell is.
11.39 The principle of equivalence
The principle of equivalence says that in any gravitational Ô¨Åeld, one may choose
free-fall coordinates in which all physical laws take the same form as in special
relativity without acceleration or gravitation ‚Äì at least over a suitably small vol-
ume of space-time. Within this volume and in these coordinates, things behave
as they would at rest deep in empty space far from any matter or energy.
The volume must be small enough so that the gravitational Ô¨Åeld is constant
throughout it.
Example 11.21 (Elevators)
When a modern elevator starts going down from a
high Ô¨Çoor, it accelerates downward at something less than the local acceleration
of gravity. One feels less pressure on one‚Äôs feet; one feels lighter. (This is as close
to free fall as I like to get.) After accelerating downward for a few seconds, the
elevator assumes a constant downward speed, and then one feels the normal
pressure of one‚Äôs weight on one‚Äôs feet. The elevator seems to be slowing down
for a stop, but actually it has just stopped accelerating downward.
447

TENSORS AND LOCAL SYMMETRIES
If in those Ô¨Årst few seconds the elevator really were falling, then the physics
in it would be the same as if it were at rest in empty space far from any gravita-
tional Ô¨Åeld. A clock in it would tick as fast as it would at rest in the absence of
gravity.
The transformation from arbitrary coordinates xk to free-fall coordinates
yi changes the metric gj‚Ñìto the diagonal metric Œ∑ik of Ô¨Çat space-time Œ∑ =
diag(‚àí1, 1, 1, 1), which has two indices and is not a Levi-Civita tensor. Alge-
braically, this transformation is a congruence (1.308)
Œ∑ik = ‚àÇxj
‚àÇyi gj‚Ñì
‚àÇx‚Ñì
‚àÇyk .
(11.328)
The geodesic equation (11.324) follows from the principle of equiva-
lence (Weinberg, 1972; Hobson et al., 2006). Suppose a particle is moving under
the inÔ¨Çuence of gravitation alone. Then one may choose free-fall coordinates
y(x) so that the particle obeys the force-free equation of motion
d2yi
dœÑ 2 = 0
(11.329)
with dœÑ the proper time dœÑ 2 = ‚àíŒ∑ik dyidyk. The chain rule applied to yi(x) in
(11.329) gives
0 = d
dœÑ

‚àÇyi
‚àÇxk
dxk
dœÑ

= ‚àÇyi
‚àÇxk
d2xk
dœÑ 2 +
‚àÇ2yi
‚àÇxk‚àÇx‚Ñì
dxk
dœÑ
dx‚Ñì
dœÑ .
(11.330)
We multiply by ‚àÇxm/‚àÇyi and use the identity
‚àÇxm
‚àÇyi
‚àÇyi
‚àÇxk = Œ¥m
k
(11.331)
to get the equation of motion (11.329) in the x-coordinates
d2xm
dœÑ 2 + m
k‚Ñì
dxk
dœÑ
dx‚Ñì
dœÑ = 0,
(11.332)
in which the afÔ¨Åne connection is
m
k‚Ñì= ‚àÇxm
‚àÇyi
‚àÇ2yi
‚àÇxk‚àÇx‚Ñì.
(11.333)
So the principle of equivalence tells us that a particle in a gravitational Ô¨Åeld
obeys the geodesic equation (11.324).
448

11.41 GRAVITATIONAL TIME DILATION
11.40 Weak, static gravitational Ô¨Åelds
Slow motion in a weak, static gravitational Ô¨Åeld is an important example.
Because the motion is slow, we neglect ui compared to u0 and simplify the
geodesic equation (11.324) to
0 = dur
dœÑ + r
00 (u0)2.
(11.334)
Because the gravitational Ô¨Åeld is static, we neglect the time derivatives gk0,0 and
g0k,0 in the connection formula (11.255) and Ô¨Ånd for r
00
r
00 = 1
2 grk 
g0k,0 + g0k,0 ‚àíg00,k

= ‚àí1
2 grk g00,k
(11.335)
with 0
00 = 0. Because the Ô¨Åeld is weak, the metric can differ from Œ∑ij by
only a tiny tensor gij = Œ∑ij + hij so that to Ô¨Årst order in |hij| ‚â™1 we have
r
00 = ‚àí1
2 h00,r for r = 1, 2, 3. With these simpliÔ¨Åcations, the geodesic equation
(11.324) reduces to
d2xr
dœÑ 2 = 1
2 (u0)2 h00,r
or
d2xr
dœÑ 2 = 1
2

dx0
dœÑ
2
h00,r.
(11.336)
So for slow motion, the ordinary acceleration is described by Newton‚Äôs law
d2x
dt2 = c2
2 ‚àáh00.
(11.337)
If œÜ is his potential, then for slow motion in weak static Ô¨Åelds
g00 = ‚àí1 + h00 = ‚àí1 ‚àí2œÜ/c2
and so
h00 = ‚àí2œÜ/c2.
(11.338)
Thus, if the particle is at a distance r from a mass M, then œÜ = ‚àíGM/r and
h00 = ‚àí2œÜ/c2 = 2GM/rc2 and so
d2x
dt2 = ‚àí‚àáœÜ = ‚àáGM
r
= ‚àíGM r
r3 .
(11.339)
How weak are the static gravitational Ô¨Åelds we know about? The dimension-
less ratio œÜ/c2 is 10‚àí39 on the surface of a proton, 10‚àí9 on the Earth, 10‚àí6 on
the surface of the Sun, and 10‚àí4 on the surface of a white dwarf.
11.41 Gravitational time dilation
Suppose we have a system of coordinates xi with a metric gik and a clock at rest
in this system. Then the proper time dœÑ between ticks of the clock is
dœÑ = (1/c)

‚àígij dxi dxj =

‚àíg00 dt
(11.340)
449

TENSORS AND LOCAL SYMMETRIES
where dt is the time between ticks in the xi coordinates, which is the lab-
oratory frame in the gravitational Ô¨Åeld g00. By the principle of equivalence
(section 11.39), the proper time dœÑ between ticks is the same as the time between
ticks when the same clock is at rest deep in empty space.
If the clock is in a weak static gravitational Ô¨Åeld due to a mass M at a distance
r, then
‚àíg00 = 1 + 2œÜ/c2 = 1 ‚àí2GM/c2r
(11.341)
is a little less than unity, and the interval of proper time between ticks
dœÑ =

‚àíg00 dt =

1 ‚àí2GM/c2r dt
(11.342)
is slightly less than the interval dt between ticks in the coordinate system of an
observer at x in the rest frame of the clock and the mass, and in its gravita-
tional Ô¨Åeld. Since dt > dœÑ, the laboratory time dt between ticks is greater than
the proper or intrinsic time dœÑ between ticks of the clock unaffected by any
gravitational Ô¨Åeld. Clocks near big masses run slow.
Now suppose we have two identical clocks at different heights above sea level.
The time T‚Ñìfor the lower clock to make N ticks will be longer than the time Tu
for the upper clock to make N ticks. The ratio of the clock times will be
Tm‚Ñì
Tu
=

1 ‚àí2GM/c2(r + h)

1 ‚àí2GM/c2r
‚âà1 + gh
c2 .
(11.343)
Now imagine that a photon going down passes the upper clock, which mea-
sures its frequency as ŒΩu, and then passes the lower clock, which measures its
frequency as ŒΩ‚Ñì. The slower clock will measure a higher frequency. The ratio of
the two frequencies will be the same as the ratio of the clock times
ŒΩ‚Ñì
ŒΩu
= 1 + gh
c2 .
(11.344)
As measured by the lower, slower clock, the photon is blue shifted.
Example 11.22 (Pound, Rebka, and M√∂ssbauer)
Pound and Rebka in 1960
used the M√∂ssbauer effect to measure the blue shift of light falling down a 22.6 m
shaft. They found
ŒΩ‚Ñì‚àíŒΩu
ŒΩ
= gh
c2 = 2.46 √ó 10‚àí15
(11.345)
(Robert Pound, 1919‚Äì2010; Glen Rebka, 1931‚Äì; Rudolf M√∂ssbauer, 1929‚Äì2011).
Example 11.23 (Redshift of the Sun)
A photon emitted with frequency ŒΩ0
at a distance r from a mass M would be observed at spatial inÔ¨Ånity to have
frequency ŒΩ
450

11.42 CURVATURE
ŒΩ = ŒΩ0

‚àíg00 = ŒΩ0

1 ‚àí2MG/c2r
(11.346)
for a redshift of ŒΩ = ŒΩ0 ‚àíŒΩ. Since the Sun‚Äôs dimensionless potential œÜ‚äô/c2 is
‚àíMG/c2r = ‚àí2.12 √ó 10‚àí6 at its surface, sunlight is shifted to the red by 2 parts
per million.
11.42 Curvature
The curvature tensor or Riemann tensor is
Ri
mnk = i
mn,k ‚àíi
mk,n + i
kj j
nm ‚àíi
nj j
km,
(11.347)
which we may write as the commutator
Ri
mnk = (Rnk)i
m = [‚àÇk + k, ‚àÇn + n]i
m
=

n,k ‚àík,n + k n ‚àín k
i
m ,
(11.348)
in which the s are treated as matrices
k =
‚éõ
‚éú‚éú‚éú‚éú‚éù
0
k 0
0
k 1
0
k 2
0
k 3
1
k 0
1
k 1
1
k 2
1
k 3
2
k 0
2
k 1
2
k 2
2
k 3
3
k 0
3
k 1
3
k 2
3
k 3
‚éû
‚éü‚éü‚éü‚éü‚é†
(11.349)
with (k n)i
m = i
kj j
nm and so forth. Just as there are two conventions for
the Faraday tensor Fik, which differ by a minus sign, so too there are two
conventions for the curvature tensor Ri
mnk. Weinberg (1972) uses the deÔ¨Ånition
(11.347); Carroll (2003) uses an extra minus sign.
The Ricci tensor is a contraction of the curvature tensor
Rmk = Rn
mnk
(11.350)
and the curvature scalar is a further contraction
R = gmk Rmk.
(11.351)
Example 11.24 (Curvature of a sphere)
While in four-dimensional space-time
indices run from 0 to 3, on the sphere they are just Œ∏ and œÜ. There are only eight
possible afÔ¨Åne connections, and because of the symmetry (11.218) in their lower
indices i
Œ∏œÜ = i
œÜŒ∏, only six are independent.
The point p on a sphere of radius r has cartesian coordinates
p = r (sin Œ∏ cos œÜ, sin Œ∏ sin œÜ, cos Œ∏)
(11.352)
451

TENSORS AND LOCAL SYMMETRIES
so the two 3-vectors are
eŒ∏ = ‚àÇp
‚àÇŒ∏ = r (cos Œ∏ cos œÜ, cos Œ∏ sin œÜ, ‚àísin Œ∏) = r ÀÜŒ∏
eœÜ = ‚àÇp
‚àÇœÜ = r sin Œ∏ (‚àísin œÜ, cos œÜ, 0) = r sin Œ∏ ÀÜœÜ
(11.353)
and the metric gij = ei ¬∑ ej is
(gij) =
r2
0
0
r2 sin2 Œ∏

.
(11.354)
Differentiating the vectors eŒ∏ and eœÜ, we Ô¨Ånd
eŒ∏,Œ∏ = ‚àír (sin Œ∏ cos œÜ, sin Œ∏ sin œÜ, cos Œ∏) = ‚àír ÀÜr,
(11.355)
eŒ∏,œÜ = r cos Œ∏ (‚àísin œÜ, cos œÜ, 0) = r cos Œ∏ ÀÜœÜ,
(11.356)
eœÜ,Œ∏ = eŒ∏,œÜ,
(11.357)
eœÜ,œÜ = ‚àír sin Œ∏ (cos œÜ, sin œÜ, 0) .
(11.358)
The metric with upper indices (gij) is the inverse of the metric (gij)
(gij) =
r‚àí2
0
0
r‚àí2 sin‚àí2 Œ∏

(11.359)
so the dual vectors ei are
e Œ∏ = r‚àí1 (cos Œ∏ cos œÜ, cos Œ∏ sin œÜ, ‚àísin Œ∏) = r‚àí1 ÀÜŒ∏,
e œÜ = =
1
r sin Œ∏ (‚àísin œÜ, cos œÜ, 0) =
1
r sin Œ∏
ÀÜœÜ.
(11.360)
The afÔ¨Åne connections are given by (11.213) as
i
jk = i
kj = e i ¬∑ ej,k.
(11.361)
Since both eŒ∏ and e œÜ are perpendicular to ÀÜr, the afÔ¨Åne connections Œ∏
Œ∏Œ∏ and œÜ
Œ∏Œ∏
both vanish. Also, eœÜ,œÜ is orthogonal to ÀÜœÜ, so œÜ
œÜœÜ = 0 as well. Similarly, eŒ∏,œÜ is
perpendicular to ÀÜŒ∏, so Œ∏
Œ∏œÜ = Œ∏
œÜŒ∏ also vanishes.
The two nonzero afÔ¨Åne connections are
œÜ
Œ∏œÜ = e œÜ ¬∑ eŒ∏,œÜ = r‚àí1 sin‚àí1 Œ∏ ÀÜœÜ ¬∑ r cos Œ∏ ÀÜœÜ = cot Œ∏
(11.362)
and
Œ∏
œÜœÜ = e Œ∏ ¬∑ eœÜ,œÜ
= ‚àísin Œ∏ (cos Œ∏ cos œÜ, cos Œ∏ sin œÜ, ‚àísin Œ∏) ¬∑ (cos œÜ, sin œÜ, 0)
= ‚àísin Œ∏ cos Œ∏.
(11.363)
In terms of the two nonzero afÔ¨Åne connections œÜ
Œ∏œÜ = œÜ
œÜŒ∏ = cot Œ∏ and Œ∏
œÜœÜ =
‚àísin Œ∏ cos Œ∏, the two Christoffel matrices (11.349) are
452

11.43 EINSTEIN‚ÄôS EQUATIONS
Œ∏ =

0
0
0
œÜ
Œ∏œÜ

=
0
0
0
cot Œ∏

(11.364)
and
œÜ =

0
Œ∏
œÜœÜ
œÜ
œÜŒ∏
0

=
 0
‚àísin Œ∏ cos Œ∏
cot Œ∏
0

.
(11.365)
Their commutator is
[Œ∏, œÜ] =

0
cos2 Œ∏
cot2 Œ∏
0

= ‚àí[œÜ, Œ∏]
(11.366)
and both [Œ∏, Œ∏] and [œÜ, œÜ] vanish.
So the commutator formula (11.348) gives for Riemann‚Äôs curvature tensor
RŒ∏
Œ∏Œ∏Œ∏ = [‚àÇŒ∏ + Œ∏, ‚àÇŒ∏ + Œ∏]Œ∏
Œ∏ = 0,
RœÜ
Œ∏œÜŒ∏ = [‚àÇŒ∏ + Œ∏, ‚àÇœÜ + œÜ]œÜ
Œ∏ =

œÜ,Œ∏
œÜ
Œ∏ + [Œ∏, œÜ]œÜ
Œ∏
= (cot Œ∏),Œ∏ + cot2 Œ∏ = ‚àí1,
RŒ∏
œÜŒ∏œÜ = [‚àÇœÜ + œÜ, ‚àÇŒ∏ + Œ∏]Œ∏
œÜ = ‚àí

œÜ,Œ∏
Œ∏
œÜ + [ œÜ, Œ∏]Œ∏
œÜ
= cos2 Œ∏ ‚àísin2 Œ∏ ‚àícos2 Œ∏ = ‚àísin2 Œ∏,
RœÜ
œÜœÜœÜ = [‚àÇœÜ + œÜ, ‚àÇœÜ + œÜ]œÜ
œÜ = 0.
(11.367)
The Ricci tensor (11.350) is the contraction Rmk = Rn
mnk, and so
RŒ∏Œ∏ = RŒ∏
Œ∏Œ∏Œ∏ + RœÜ
Œ∏œÜŒ∏ = ‚àí1,
RœÜœÜ = RŒ∏
œÜŒ∏œÜ + RœÜ
œÜœÜœÜ = ‚àísin2 Œ∏.
(11.368)
The curvature scalar (11.351) is the contraction R = gkmRmk, and so since gŒ∏Œ∏ =
r‚àí2 and gœÜœÜ = r‚àí2 sin‚àí2 Œ∏, it is
R = gŒ∏Œ∏ RŒ∏Œ∏ + gœÜœÜ RœÜœÜ
= ‚àír‚àí2 ‚àísin2 Œ∏ r‚àí2 sin‚àí2 Œ∏ = ‚àí2
r2
(11.369)
for a 2-sphere of radius r.
Gauss invented a formula for the curvature K of a surface; for all two-
dimensional surfaces, his K = ‚àíR/2.
11.43 Einstein‚Äôs equations
The source of the gravitational Ô¨Åeld is the energy‚Äìmomentum tensor Tij. In
many astrophysical and most cosmological models, the energy‚Äìmomentum ten-
sor is assumed to be that of a perfect Ô¨Çuid, which is isotropic in its rest frame,
does not conduct heat, and has zero viscosity. For a perfect Ô¨Çuid of pressure p
453

TENSORS AND LOCAL SYMMETRIES
and density œÅ with 4-velocity ui (deÔ¨Åned by (11.66)), the energy‚Äìmomentum or
stress‚Äìenergy tensor Tij is
Tij = p gij + (p + œÅ) ui uj,
(11.370)
in which gij is the space-time metric.
An important special case is the energy‚Äìmomentum tensor due to a nonzero
value of the energy density of the vacuum. In this case p = ‚àíœÅ and the energy‚Äì
momentum tensor is
Tij = ‚àíœÅ gij,
(11.371)
in which œÅ is the (presumably constant) value of the energy density of the
ground state of the theory. This energy density œÅ is a plausible candidate for
the dark-energy density. It is equivalent to a cosmological constant  = 8œÄGœÅ.
Whatever its nature, the energy‚Äìmomentum tensor usually is deÔ¨Åned so as to
satisfy the conservation law
0 =

Ti
j

;i = ‚àÇiTi
j + i
icTc
j ‚àíTi
cc
ji.
(11.372)
Einstein‚Äôs equations relate the Ricci tensor (11.350) and the scalar curvature
(11.351) to the energy‚Äìmomentum tensor
Rij ‚àí1
2 gij R = ‚àí8œÄ G
c4
Tij,
(11.373)
in which G = 6.7087 √ó 10‚àí39 ¬Øhc (GeV/c2)‚àí2 = 6.6742 √ó 10‚àí11 m3 kg‚àí1 s‚àí2 is
Newton‚Äôs constant. Taking the trace and using gji gij = Œ¥j
j = 4, we relate the
scalar curvature to the trace T = Ti
i of the energy‚Äìmomentum tensor
R = 8œÄG
c4
T.
(11.374)
So another form of Einstein‚Äôs equations (11.373) is
Rij = ‚àí8œÄG
c4

Tij ‚àíT
2 gij

.
(11.375)
On small scales, such as that of our Solar System, one may neglect dark
energy. So in empty space and on small scales, the energy‚Äìmomentum tensor
vanishes Tij = 0 along with its trace and the scalar curvature T = 0 = R, and
Einstein‚Äôs equations are
Rij = 0.
(11.376)
454

11.45 STANDARD FORM
11.44 The action of general relativity
If we make an action that is a scalar, invariant under general coordinate trans-
formations, and then apply to it the principle of stationary action, we will get
tensor Ô¨Åeld equations that are invariant under general coordinate transforma-
tions. If the metric of space-time is among the Ô¨Åelds of the action, then the
resulting theory will be a possible theory of gravity. If we make the action as
simple as possible, it will be Einstein‚Äôs theory.
To make the action of the gravitational Ô¨Åeld, we need a scalar. Apart from
the volume 4-form ‚àög d4x, the only scalar we can form from the metric tensor
and its Ô¨Årst and second derivatives is the scalar curvature R, which gives us the
Einstein‚ÄìHilbert action
SEH = ‚àí
c4
16œÄG

‚àóR = ‚àí
c4
16œÄG

R ‚àög d4x.
(11.377)
If Œ¥gik(x) is a tiny change in the inverse metric that vanishes as any coordinate
xj ‚Üí¬±‚àû, then one may write the Ô¨Årst-order change in the action SEH as
Œ¥SEH = ‚àí
c4
16œÄG
 
Rik ‚àí1
2 gikR
 ‚àög Œ¥gik d4x.
(11.378)
The principle of least action Œ¥SEH = 0 now leads to Einstein‚Äôs equations
Gik = Rik ‚àí1
2 gik R = 0
(11.379)
for empty space in which Gik is Einstein‚Äôs tensor.
The stress‚Äìenergy tensor Tik is deÔ¨Åned so that the change in the action of the
matter Ô¨Åelds due to a tiny change Œ¥gik(x) (vanishing at inÔ¨Ånity) in the metric is
Œ¥Sm = ‚àí1
2

Tik
‚àög Œ¥gik d4x.
(11.380)
So the principle of least action Œ¥S = Œ¥SEH + Œ¥Sm = 0 implies Einstein‚Äôs
equations (11.373, 11.375) in the presence of matter and energy
Rik ‚àí1
2 gik R = ‚àí8œÄG
c4 Tij
or
Rij = ‚àí8œÄG
c4

Tij ‚àíT
2 gij

.
(11.381)
11.45 Standard form
Tensor equations are independent of the choice of coordinates, so it‚Äôs wise
to choose coordinates that simplify one‚Äôs work. For a static and isotropic
gravitational Ô¨Åeld, this choice is the standard form (Weinberg, 1972, ch. 8)
dœÑ 2 = B(r) dt2 ‚àíA(r) dr2 ‚àír2 
dŒ∏2 + sin2 Œ∏ dœÜ2
,
(11.382)
455

TENSORS AND LOCAL SYMMETRIES
in which c = 1, and B(r) and A(r) are functions that one may Ô¨Ånd by solving
the Ô¨Åeld equations (11.373). Since dœÑ 2 =
‚àíds2 = ‚àígij dxidxj, the nonzero
components of the metric tensor are grr = A(r), gŒ∏Œ∏ = r2, gœÜœÜ = r2 sin2 Œ∏,
and g00 = ‚àíB(r), and those of its inverse are grr = A‚àí1(r), gŒ∏Œ∏ = r‚àí2,
gœÜœÜ = r‚àí2 sin‚àí2 Œ∏, and g00 = ‚àíB‚àí1(r). By differentiating the metric tensor
and using (11.255), one gets the components of the connection i
k‚Ñì, such as
Œ∏
œÜœÜ = ‚àísin Œ∏ cos Œ∏, and the components (11.350) of the Ricci tensor Rij, such
as (Weinberg, 1972, ch. 8)
Rrr = B‚Ä≤‚Ä≤(r)
2B(r) ‚àí1
4
B‚Ä≤(r)
B(r)
 A‚Ä≤(r)
A(r) + B‚Ä≤(r)
B(r)

‚àí1
r
A‚Ä≤(r)
A(r)

,
(11.383)
in which the primes mean d/dr.
11.46 Schwarzschild‚Äôs solution
If one ignores the small dark-energy parameter , one may solve Einstein‚Äôs Ô¨Åeld
equations (11.376) in empty space
Rij = 0
(11.384)
outside a mass M for the standard form of the Ricci tensor. One Ô¨Ånds (Wein-
berg, 1972) that A(r) B(r) = 1 and that r B(r) = r plus a constant, and
one determines the constant by invoking the Newtonian limit g00 = ‚àíB ‚Üí
‚àí1 + 2MG/c2r as r ‚Üí‚àû. In 1916, Schwarzschild found the solution
dœÑ 2 =

1 ‚àí2MG
c2r

c2dt2 ‚àí

1 ‚àí2MG
c2r
‚àí1
dr2 ‚àír2 
dŒ∏2 + sin2 Œ∏ dœÜ2
,
(11.385)
which one can use to analyze orbits around a star. The singularity in
grr =

1 ‚àí2MG
c2r
‚àí1
(11.386)
at the Schwarzschild radius r = 2MG/c2 is an artifact of the coordinates;
the scalar curvature R and other invariant curvatures are not singular at
the Schwarzschild radius. Moreover, for the Sun, the Schwarzschild radius
r = 2MG/c2 is only 2.95 km, far less than the radius of the Sun, which is
6.955 √ó 105 km. So the surface at r = 2MG/c2 is far from the empty space in
which Schwarzschild‚Äôs solution applies (Karl Schwarzschild, 1873‚Äì1916).
11.47 Black holes
Suppose an uncharged, spherically symmetric star of mass M has collapsed
within a sphere of radius rb less than its Schwarzschild radius r = 2MG/c2.
456

11.48 COSMOLOGY
Then for r > rb, the Schwarzschild metric (11.385) is correct. By (11.340), the
apparent time dt of a process of proper time dœÑ at r ‚â•2MG/c2 is
dt = dœÑ/

‚àíg00 = dœÑ/
&
1 ‚àí2MG
c2r .
(11.387)
The apparent time dt becomes inÔ¨Ånite as r ‚Üí2MG/c2. To outside observers,
the star seems frozen in time.
Due to the gravitational redshift (11.346), light of frequency ŒΩp emitted at
r ‚â•2MG/c2 will have frequency ŒΩ
ŒΩ = ŒΩp

‚àíg00 = ŒΩp
&
1 ‚àí2MG
c2r
(11.388)
when observed at great distances. Light coming from the surface at r = 2MG/c2
is redshifted to zero frequency ŒΩ = 0. The star is black. It is a black hole with
a surface or horizon at its Schwarzschild radius r = 2MG/c2, although there
is no singularity there. If the radius of the Sun were less than its Schwarzschild
radius of 2.95 km, then the Sun would be a black hole. The radius of the Sun is
6.955 √ó 105 km.
Black holes are not really black. Stephen Hawking (1942‚Äì) has shown that
the intense gravitational Ô¨Åeld of a black hole of mass M radiates at temperature
T =
¬Øh c3
8œÄ k G M ,
(11.389)
in which k = 8.617343√ó10‚àí5 eV K‚àí1 is Boltzmann‚Äôs constant, and ¬Øh is Planck‚Äôs
constant h = 6.6260693 √ó 10‚àí34 J s divided by 2œÄ, ¬Øh = h/(2œÄ).
The black hole is entirely converted into radiation after a time
t = 5120 œÄ G2
¬Øh c4
M3
(11.390)
proportional to the cube of its mass.
11.48 Cosmology
Astrophysical observations tell us that on the largest observable scales, space is
Ô¨Çat or very nearly Ô¨Çat; that the visible Universe contains at least 1090 particles;
and that the cosmic microwave background radiation is isotropic to one part in
105 apart from a Doppler shift due the motion of the Earth. These and other
observations suggest that potential energy expanded our Universe by exp(60) =
1026 during an era of inÔ¨Çation that could have been as brief as 10‚àí35 s. The
potential energy that powered inÔ¨Çation became the radiation of the Big Bang.
During the Ô¨Årst three minutes, some of that radiation became hydrogen,
helium, neutrinos, and dark matter. But for 65,000 years after the Big Bang,
457

TENSORS AND LOCAL SYMMETRIES
most of the energy of the visible Universe was radiation. Because the momen-
tum of a particle but not its mass falls with the expansion of the Universe, this
era of radiation gradually gave way to an era of matter. This transition happened
when the temperature kT of the Universe fell to 1.28 eV.
The era of matter lasted for 8.8 billion years. After 360,000 years, the Uni-
verse had cooled to kT = 0.26 eV, and fewer than 1% of the atoms were ionized.
Photons no longer scattered off a plasma of electrons and ions. The Universe
became transparent. The photons that last scattered just before this initial trans-
parency became the cosmic microwave background radiation or CMBR that now
surrounds us, red shifted to 2.725 ¬±0.001 K.
The era of matter was followed by the current era of dark energy during which
the energy of the visible Universe is mostly a potential energy called dark energy
(something like a cosmological constant). Dark energy has been accelerating the
expansion of the Universe for the past 5 billion years and may continue to do
so forever.
It is now 13.75 billion years after the Big Bang, and the dark-energy density
is œÅde = 1.37 √ó 10‚àí29 c2 g cm‚àí3 or 75.7 percent (¬± 2.1%) of the critical energy
density œÅc = 3H2
0/8œÄG = 1.87837 √ó 10‚àí29 f 2 c2 g cm‚àí3 needed to make the
Universe Ô¨Çat. Here H0 = 100 h km s‚àí1 Mpc‚àí1 = 1.022/(1010yr) is the Hub-
ble constant, one parsec is 3.262 light-years, and h = 0.72 ¬± 0.03 is not to be
confused with Planck‚Äôs constant.
Matter makes up 24.6 ¬± 2.8% of the critical density, and baryons only 4.2 ¬±
0.2% of it. Baryons are 17% of the total matter in the visible Universe. The
other 83% does not interact with light and is called dark matter.
Einstein‚Äôs equations (11.373) are second-order, nonlinear partial differential
equations for ten unknown functions gij(x) in terms of the energy‚Äìmomentum
tensor Tij(x) throughout the Universe, which of course we don‚Äôt know. The
problem is not quite hopeless, however. The ability to choose arbitrary coordi-
nates, the appeal to symmetry, and the choice of a reasonable form for Tij all
help.
Hubble showed us that the Universe is expanding. The cosmic microwave
background radiation looks the same in all spatial directions (apart from a
Doppler shift due to the motion of the Earth relative to the local super-cluster
of galaxies). Observations of clusters of galaxies reveal a Universe that is homo-
geneous on suitably large scales of distance. So it is plausible that the Universe
is homogeneous and isotropic in space, but not in time. One may show (Car-
roll, 2003) that for a universe of such symmetry, the line element in comoving
coordinates is
ds2 = ‚àídt2 + a2

dr2
1 ‚àík r2 + r2 
dŒ∏2 + sin2 Œ∏ dœÜ2
.
(11.391)
458

11.48 COSMOLOGY
Whitney‚Äôs embedding theorem tells us that any smooth four-dimensional
manifold can be embedded in a Ô¨Çat space of eight dimensions with a suit-
able signature. We need only four or Ô¨Åve dimensions to embed the space-time
described by the line element (11.391). If the Universe is closed, then the signa-
ture is (‚àí1, 1, 1, 1, 1), and our three-dimensional space is the 3-sphere, which is
the surface of a four-dimensional sphere in four space dimensions. The points
of the Universe then are
p = (t, a sin œá sin Œ∏ cos œÜ, a sin œá sin Œ∏ sin œÜ, a sin œá cos Œ∏, a cos œá),
(11.392)
in which 0 ‚â§œá ‚â§œÄ, 0 ‚â§Œ∏ ‚â§œÄ, and 0 ‚â§œÜ ‚â§2œÄ. If the Universe is Ô¨Çat, then
the embedding space is Ô¨Çat, four-dimensional Minkowski space with points
p = (t, ar sin Œ∏ cos œÜ, ar sin Œ∏ sin œÜ, ar cos Œ∏),
(11.393)
in which 0 ‚â§Œ∏ ‚â§œÄ and 0 ‚â§œÜ ‚â§2œÄ. If the Universe is open, then the embed-
ding space is a Ô¨Çat Ô¨Åve-dimensional space with signature (‚àí1, 1, 1, 1, ‚àí1), and
our three-dimensional space is a hyperboloid in a Ô¨Çat Minkowski space of four
dimensions. The points of the Universe then are
p = (t, a sinh œá sin Œ∏ cos œÜ, a sinh œá sin Œ∏ sin œÜ, a sinh œá cos Œ∏, a cosh œá),
(11.394)
in which 0 ‚â§œá ‚â§‚àû, 0 ‚â§Œ∏ ‚â§œÄ, and 0 ‚â§œÜ ‚â§2œÄ.
In all three cases, the corresponding Robertson‚ÄìWalker metric is
gij =
‚éõ
‚éú‚éú‚éù
‚àí1
0
0
0
0
a2/(1 ‚àíkr2)
0
0
0
0
a2 r2
0
0
0
0
a2 r2 sin2 Œ∏
‚éû
‚éü‚éü‚é†,
(11.395)
in which the coordinates (t, r, Œ∏, œÜ) are numbered (0, 1, 2, 3), the speed of light is
c = 1, and k is a constant. One always may choose coordinates (exercise 11.30)
such that k is either 0 or ¬±1. This constant determines whether the spatial Uni-
verse is open k = ‚àí1, Ô¨Çat k = 0, or closed k = 1. The scale factor a, which
in general is a function of time a(t), tells us how space expands and contracts.
These coordinates are called comoving because a point at rest (Ô¨Åxed r, Œ∏, œÜ) sees
the same Doppler shift in all directions.
The metric (11.395) is diagonal; its inverse gij also is diagonal; and so we may
use our formula (11.255) to compute the afÔ¨Åne connections k
i‚Ñì, such as
0
‚Ñì‚Ñì= 1
2g0k 
g‚Ñìk,‚Ñì+ g‚Ñìk,‚Ñì‚àíg‚Ñì‚Ñì,k

= 1
2g00 
g‚Ñì0,‚Ñì+ g‚Ñì0,‚Ñì‚àíg‚Ñì‚Ñì,0

= 1
2g‚Ñì‚Ñì,0
(11.396)
so that
0
11 =
aÀôa
1 ‚àíkr2
0
22 = aÀôa r2
and
0
22 = aÀôa r2 sin2 Œ∏,
(11.397)
459

TENSORS AND LOCAL SYMMETRIES
in which a dot means a time-derivative. The other 0
ijs vanish. Similarly, for
Ô¨Åxed ‚Ñì= 1, 2, or 3
‚Ñì
0‚Ñì= 1
2g‚Ñìk 
g0k,‚Ñì+ g‚Ñìk,0 ‚àíg0‚Ñì,k

= 1
2g‚Ñì‚Ñì
g0‚Ñì,‚Ñì+ g‚Ñì‚Ñì,0 ‚àíg0‚Ñì,‚Ñì

= 1
2g‚Ñì‚Ñìg‚Ñì‚Ñì,0 = Àôa
a = ‚Ñì
‚Ñì0,
no sum over ‚Ñì.
(11.398)
The other nonzero s are
1
22 = ‚àír (1 ‚àíkr2),
1
33 = ‚àír (1 ‚àíkr2) sin2 Œ∏,
(11.399)
2
12 = 3
13 = 1
r = 2
21 = 3
31,
(11.400)
2
33 = ‚àísin Œ∏ cos Œ∏,
3
23 = cos Œ∏ = 3
32.
(11.401)
Our formulas (11.350 & 11.348) for the Ricci and curvature tensors give
R00 = Rn
0n0 = [‚àÇ0 + 0, ‚àÇn + n]n
0.
(11.402)
Clearly the commutator of 0 with itself vanishes, and one may use the formulas
(11.397‚Äì11.401) for the other connections to check that
[0, n]n
0 = n
0 k k
n 0 ‚àín
n k k
0 0 = 3
 Àôa
a
2
(11.403)
and that
‚àÇ0 n
n 0 = 3 ‚àÇ0
 Àôa
a

= 3 ¬®a
a ‚àí3
 Àôa
a
2
(11.404)
while ‚àÇnn
0 0 = 0. So the 00-component of the Ricci tensor is
R00 = 3 ¬®a
a.
(11.405)
Similarly, one may show that the other nonzero components of Ricci‚Äôs tensor
are
R11 = ‚àí
A
1 ‚àíkr2 ,
R22 = ‚àír2A,
and
R33 = ‚àír2A sin2 Œ∏,
(11.406)
in which A = a¬®a + 2Àôa2 + 2k. The scalar curvature (11.351) is
R = gabRba = ‚àí6
a2

a¬®a + Àôa2 + k

.
(11.407)
460

11.48 COSMOLOGY
In comoving coordinates such as those of the Robertson‚ÄìWalker metric
(11.395) ui = (1, 0, 0, 0), and so the energy‚Äìmomentum tensor (11.370) is
Tij =
‚éõ
‚éú‚éú‚éù
œÅ
0
0
0
0
p g11
0
0
0
0
p g22
0
0
0
0
p g33
‚éû
‚éü‚éü‚é†.
(11.408)
Its trace is
T = gij Tij = ‚àíœÅ + 3p.
(11.409)
Thus, using our formula (11.395) for g00 = ‚àí1, (11.405) for R00, (11.408) for
Tij, and (11.409) for T, we Ô¨Ånd that the 00 Einstein equation (11.375) becomes
the second-order equation
¬®a
a = ‚àí4œÄG
3
(œÅ + 3p) ,
(11.410)
which is nonlinear because œÅ and 3p depend upon a. The sum œÅ+3p determines
the acceleration ¬®a of the scale factor a(t). When it is negative, it accelerates the
expansion of the Universe.
Because of the isotropy of the metric, the three nonzero spatial Einstein
equations (11.375) give us only one relation
¬®a
a + 2
 Àôa
a
2
+ 2 k
a2 = 4œÄG (œÅ ‚àíp) .
(11.411)
Using the 00-equation (11.410) to eliminate the second derivative ¬®a, we have
 Àôa
a
2
= 8œÄG
3
œÅ ‚àík
a2 ,
(11.412)
which is a Ô¨Årst-order nonlinear equation. It and the second-order equation
(11.410) are known as the Friedmann equations.
The LHS of the Ô¨Årst-order Friedmann equation (11.412) is the square of the
Hubble rate
H = Àôa
a,
(11.413)
which is an inverse time. Its present value H0 is the Hubble constant. In terms
of H, Friedmann‚Äôs Ô¨Årst-order equation (11.412) is
H2 = 8œÄG
3
œÅ ‚àík
a2 .
(11.414)
The energy density of a Ô¨Çat universe with k = 0 is the critical energy density
œÅc = 3H2
8œÄG.
(11.415)
461

TENSORS AND LOCAL SYMMETRIES
The ratio of the energy density œÅ to the critical energy density is called 
 = œÅ
œÅc
= 8œÄG
3H2 œÅ.
(11.416)
From (11.414), we see that  is
 = 1 +
k
(aH)2 = 1 + k
Àôa2 .
(11.417)
Thus  = 1 both in a Ô¨Çat universe (k = 0) and as aH ‚Üí‚àû. One use of inÔ¨Çation
is to expand a by 1026 so as to force  to almost exactly unity.
Something like inÔ¨Çation is needed because in a universe in which the energy
density is due to matter and/or radiation, the present value of 
0 = 1.003 ¬± 0.01
(11.418)
is unlikely. To see why, we note that conservation of energy ensures that a3 times
the matter density œÅm is constant. Radiation red shifts by a, so energy conser-
vation implies that a4 times the radiation density œÅr is constant. So with n = 3
for matter and 4 for radiation, œÅan ‚â°3F2/8œÄG is a constant. In terms of F and
n, Friedmann‚Äôs Ô¨Årst-order equation (11.412) is
Àôa2 = 8œÄG
3
œÅa2 ‚àík = F2
an‚àí2 ‚àík.
(11.419)
In small-a limit of the early Universe, we have
Àôa = F/a(n‚àí2)/2
or
a(n‚àí2)/2da = Fdt,
(11.420)
which we integrate to a ‚àºt2/n so that Àôa ‚àºt2/n‚àí1. Now (11.417) says that
| ‚àí1| = 1
Àôa2 ‚àùt2‚àí4/n =
 t
radiation,
t2/3
matter.
(11.421)
That is, the ratio  deviates from unity with the passage of time. So without
inÔ¨Çation (or some other way of vastly expanding the scale factor), the present
value of this ratio 0 = 1.003 ¬± 0.010 could be so close to unity after 13.8
billion years only if the ratio  at t = 1 second had been unity to within one
part in 1015.
Manipulating our relation (11.417) between  and aH, we see that
(aH)2 =
k
 ‚àí1.
(11.422)
So  > 1 implies k = 1, and  < 1 implies k = ‚àí1, and as  ‚Üí1 the product
aH ‚Üí‚àû, which is the essence of Ô¨Çatness since curvature vanishes as the scale
factor a ‚Üí‚àû. Imagine blowing up a balloon.
462

11.49 MODEL COSMOLOGIES
Staying for the moment with a universe without inÔ¨Çation and with an energy
density composed of radiation and/or matter, we note that the Ô¨Årst-order equa-
tion (11.419) in the form Àôa2 = F2/an‚àí2 ‚àík tells us that for a closed (k = 1)
universe, in the limit a ‚Üí‚àûwe‚Äôd have Àôa2 ‚Üí‚àí1, which is impossible. Thus
a closed universe eventually collapses, which is incompatible with the Ô¨Çatness
(11.422) implied by the present value 0 = 1.003 ¬± 0.010.
The Ô¨Årst-order Friedmann equation (11.412) tells us that œÅ a2 ‚â•3k/8œÄG.
So in a closed universe (k = 1), the energy density œÅ is positive and increases
without limit as a ‚Üí0 as in a collapse. In open (k < 0) and Ô¨Çat (k = 0)
universes, the same Friedmann equation (11.412) in the form Àôa2 = 8œÄGœÅa2/3‚àí
k tells us that if œÅ is positive, then Àôa2 > 0, which means that Àôa never vanishes.
Hubble told us that Àôa > 0 now. So if our Universe is open or Ô¨Çat, then it always
expands.
Due to the expansion of the Universe, the wave-length of radiation grows
with the scale factor a(t). A photon emitted at time t and scale factor a(t) with
wave-length Œª(t) will be seen now at time t0 and scale factor a(t0) to have a
longer wave-length Œª(t0)
Œª(t0)
Œª(t) = a(t0)
a(t) = z + 1,
(11.423)
in which the redshift z is the ratio
z = Œª(t0) ‚àíŒª(t)
Œª(t)
= Œª
Œª .
(11.424)
Now H = Àôa/a = da/(adt) implies dt = da/(aH), and z = a0/a ‚àí1 implies
dz = ‚àía0da/a2, so we Ô¨Ånd
dt = ‚àí
dz
(1 + z)H(z),
(11.425)
which relates time intervals to redshift intervals. An on-line calculator is
available for macroscopic intervals (Wright, 2006).
11.49 Model cosmologies
The 0-component of the energy‚Äìmomentum conservation law (11.372) is
0 =

Ta
0

;a = ‚àÇaTa
0 + a
acTc
0 ‚àíTa
cc
0a
= ‚àí‚àÇ0T00 ‚àía
a0T00 ‚àígccTccc
0c
= ‚àíÀôœÅ ‚àí3 Àôa
aœÅ ‚àí3p Àôa
a = ‚àíÀôœÅ ‚àí3 Àôa
a (œÅ + p)
(11.426)
or
dœÅ
da = ‚àí3
a (œÅ + p) .
(11.427)
463

TENSORS AND LOCAL SYMMETRIES
The energy density œÅ is composed of fractions œÅk each contributing its own
partial pressure pk according to its own equation of state
pk = wkœÅk,
(11.428)
in which wk is a constant. In terms of these components, the energy‚Äìmomentum
conservation law (11.427) is

k
dœÅk
da = ‚àí3
a

k
(1 + wk) œÅk
(11.429)
with solution
œÅ =

k
œÅk
a
a
3(1+wk)
=

k
œÅk
a
a
3(1+pk0/œÅk)
.
(11.430)
Simple cosmological models take the energy density and pressure each to
have a single component with p = wœÅ, and in this case
œÅ = œÅ
a
a
3(1+w)
= œÅ
a
a
3(1+p0/œÅ)
.
(11.431)
Example 11.25 (w = ‚àí1/3, no acceleration)
If w = ‚àí1/3, then p = w œÅ =
‚àíœÅ/3 and œÅ + 3p = 0. The second-order Friedmann equation (11.410) then tells
us that ¬®a = 0. The scale factor does not accelerate.
To Ô¨Ånd its constant speed, we use its equation of state (11.431)
œÅ = œÅ
a
a
3(1+w)
= œÅ
a
a
2
.
(11.432)
Now all the terms in Friedmann‚Äôs Ô¨Årst-order equation (11.412) have a common
factor of 1/a2, which cancels leaving us with the square of the constant speed
Àôa2 = 8œÄG
3
œÅ a2 ‚àík.
(11.433)
Incidentally, œÅ a2 must exceed 3k/8œÄG. The scale factor grows linearly with time
as
a(t) =
8œÄG
3
œÅ a2 ‚àík
1/2
(t ‚àít0) + a(t0).
(11.434)
Setting t0 = 0 and a(0) = 0, we use the deÔ¨Ånition of the Hubble parameter
H = Àôa/a to write the constant linear growth Àôa as aH and the time as
t =
 a
0
da‚Ä≤/a‚Ä≤H = (1/aH)
 a
0
da‚Ä≤ = 1/H.
(11.435)
464

11.49 MODEL COSMOLOGIES
So in a universe without acceleration, the age of the universe is the inverse of the
Hubble rate. For our Universe, the present Hubble time is 1/H0 = 13.6 billion
years, which isn‚Äôt far from the actual age of 13.75 billion years. Presumably, a
slower Hubble rate during the era of matter has been compensated by a higher
rate during the era of dark energy.
Example 11.26 (w = ‚àí1, inÔ¨Çation)
InÔ¨Çation occurs when the ground state of
the theory has a positive and constant energy density œÅ > 0 that dwarfs the
energy densities of the matter and radiation. The internal energy of the Universe
then is proportional to its volume U = œÅ V, and the pressure p as given by the
thermodynamic relation
p = ‚àí‚àÇU
‚àÇV = ‚àíœÅ
(11.436)
is negative. The equation of state (11.428) tells us that in this case w = ‚àí1. The
second-order Friedmann equation (11.410) becomes
¬®a
a = ‚àí4œÄG
3
(œÅ + 3p) = 8œÄGœÅ
3
‚â°g2.
(11.437)
By it and the Ô¨Årst-order Friedmann equation (11.412) and by choosing t = 0 as
the time at which the scale factor a is minimal, one may show (exercise 11.37)
that in a closed (k = 1) universe
a(t) = cosh g t
g
.
(11.438)
Similarly, in an open (k = ‚àí1) universe with a(0) = 0, we have
a(t) = sinh g t
g
.
(11.439)
Finally, in a Ô¨Çat (k = 0) expanding universe, the scale factor is
a(t) = a(0) exp(g t).
(11.440)
Studies of the cosmic microwave background radiation suggest that inÔ¨Çation
did occur in the very early Universe, possibly on a time scale as short as 10‚àí35 s.
What is the origin of the vacuum energy density œÅ that drove inÔ¨Çation? Current
theories attribute it to the assumption by at least one scalar Ô¨Åeld œÜ of a mean
value ‚ü®œÜ‚ü©different from the one ‚ü®0|œÜ|0‚ü©that minimizes the energy density of
the vacuum. When ‚ü®œÜ‚ü©settled to ‚ü®0|œÜ|0‚ü©, the vacuum energy was released as
radiation and matter in a Big Bang.
Example 11.27 (w = 1/3, the era of radiation)
Until a redshift of z ‚â≥3000
or somewhat less than 65,000 years after inÔ¨Çation, our Universe was dominated
by radiation (Frieman et al., 2008). During The First Three Minutes (Weinberg,
1988) of the era of radiation, the quarks and gluons formed hadrons, which
465

TENSORS AND LOCAL SYMMETRIES
decayed into protons and neutrons. As the neutrons decayed (œÑ = 885.7 s), they
and the protons formed the light elements ‚Äì principally hydrogen, deuterium,
and helium ‚Äì in a process called big-bang nucleosynthesis.
We can guess the value of w for radiation by noticing that the energy‚Äì
momentum tensor of the electromagnetic Ô¨Åeld (in suitable units)
Tab = Fa
cFbc ‚àí1
4gabFcdFcd
(11.441)
is traceless
T = Ta
a = Fa
cF c
a ‚àí1
4Œ¥a
aFcdFcd = 0.
(11.442)
But by (11.409) its trace must be T = 3p ‚àíœÅ. So for radiation p = œÅ/3 and
w = 1/3. The relation (11.431) between the energy density and the scale factor
then is
œÅ = œÅ
a
a
4
.
(11.443)
The energy drops both with the volume a3 and with the scale factor a due to a
redshift; so it drops as 1/a4. Thus the quantity
f 2 ‚â°8œÄGœÅa4
3
(11.444)
is a constant. The Friedmann equations (11.410 & 11.411) now are
¬®a
a = ‚àí4œÄG
3
(œÅ + 3p) = ‚àí8œÄGœÅ
3
or
¬®a = ‚àíf 2
a3
(11.445)
and
Àôa2 + k = f 2
a2 .
(11.446)
With calendars chosen so that a(0) = 0, this last equation (11.446) tells us that
for a Ô¨Çat universe (k = 0)
a(t) = (2f t)1/2
(11.447)
while for a closed universe (k = 1)
a(t) =

f 2 ‚àí(t ‚àíf )2
(11.448)
and for an open universe (k = ‚àí1)
a(t) =

(t + f )2 ‚àíf 2
(11.449)
as we saw in (6.422). The scale factor (11.448) of a closed universe of radiation
has a maximum a = f at t = f and falls back to zero at t = 2f .
Example 11.28 (w = 0, the era of matter)
A universe composed only of dust or
nonrelativistic collisionless matter has no pressure. Thus p = wœÅ = 0 with œÅ Ã∏= 0,
466

11.49 MODEL COSMOLOGIES
and so w = 0. Conservation of energy (11.430), or equivalently (11.431), implies
that the energy density falls with the volume
œÅ = œÅ
a
a
3
.
(11.450)
As the scale factor a(t) increases, the matter energy density, which falls as 1/a3,
eventually dominates the radiation energy density, which falls as 1/a4. This
happened in our Universe somewhat less than 65,000 years after inÔ¨Çation at
a temperature of kT = 1.28 eV. Were baryons most of the matter, the era of
radiation dominance would have lasted for a few hundred thousand years. But
the kind of matter that we know about, which interacts with photons, is only
about 17% of the total; the rest ‚Äì an unknown substance called dark matter ‚Äì
shortened the era of radiation dominance by nearly 2 million years.
Since œÅ ‚àù1/a3, the quantity
m2 = 4œÄGœÅa3
3
(11.451)
is a constant. For a matter-dominated universe, the Friedmann equations
(11.410 & 11.411) then are
¬®a
a = ‚àí4œÄG
3
(œÅ + 3p) = ‚àí4œÄGœÅ
3
or
¬®a = ‚àím2
a2
(11.452)
and
Àôa2 + k = 2m2/a.
(11.453)
For a Ô¨Çat universe, k = 0, we get
a(t) =
# 3m
‚àö
2
t
$2/3
.
(11.454)
For a closed universe, k = 1, we use example 6.46 to integrate
Àôa =

2m2/a ‚àí1
(11.455)
to
t ‚àít0 = ‚àí

a(2m2 ‚àía) ‚àím2 arcsin(1 ‚àía/m2).
(11.456)
With a suitable calendar and choice of t0, one may parametrize this solution in
terms of the development angle œÜ(t) as
a(t) = m2 [1 ‚àícos œÜ(t)] ,
t = m2 [œÜ(t) ‚àísin œÜ(t)] .
(11.457)
For an open universe, k = ‚àí1, we use example 6.47 to integrate
Àôa =

2m2/a + 1
(11.458)
467

TENSORS AND LOCAL SYMMETRIES
to
t ‚àít0 =

a(2m2 + a)
1/2
‚àím2 ln

2

a(2m2 + a)
1/2
+ 2a + 2m2
%
.
(11.459)
The conventional parametrization is
a(t) = m2 [cosh œÜ(t) ‚àí1] ,
t = m2 [sinh œÜ(t) ‚àíœÜ(t)] .
(11.460)
Transparency
Some 360,000 years after inÔ¨Çation at a redshift of about z =
1100, the Universe had cooled to kT = 0.26 eV ‚Äì a temperature at which less
than 1% of the hydrogen is ionized. Ordinary matter became a gas of neutral
atoms rather than a plasma of ions and electrons, and the Universe suddenly
became transparent to light. Some scientists call this moment of last scattering
or Ô¨Årst transparency recombination.
Example 11.29 (w = ‚àí1, the era of dark energy)
Somewhat more than 8.8
billion years after inÔ¨Çation at a redshift of z ‚â≥0.5, the matter density falling
as 1/a3 dropped below the very small but positive value of the energy density
œÅ = 31 meV4 of the vacuum. The present time is 13.8 billion years after inÔ¨Çation.
So for the past 5 billion years, this constant energy density, called dark energy,
has accelerated the expansion of the Universe approximately as (11.439)
a(t) = a(tm) exp

(t ‚àítm)

8œÄGœÅ/3

,
(11.461)
in which tm = 8.8 √ó 109 years.
Observations and measurements on the largest scales indicate that the Uni-
verse is Ô¨Çat: k = 0. So the evolution of the scale factor a(t) is given by the k = 0
equations (11.440, 11.447, 11.454, & 11.461) for a Ô¨Çat universe. During the brief
era of inÔ¨Çation, the scale factor a(t) grew as (11.440)
a(t) = a(0) exp

t

8œÄGœÅi/3

,
(11.462)
in which œÅi is the positive energy density that drove inÔ¨Çation.
During the 65,000-year era of radiation, a(t) grew as ‚àöt as in (11.447)
a(t) =

2 (t ‚àíti)

8œÄGœÅ(t‚Ä≤r)a4(t‚Ä≤r)/3
1/2
+ a(ti)
(11.463)
where ti is the time at the end of inÔ¨Çation, and t‚Ä≤
r is any time during the era of
radiation. During this era, the energy of highly relativistic particles dominated
the energy density, and œÅa4 ‚àùT4a4 was approximately constant, so that T(t) ‚àù
1/a(t) ‚àù1/‚àöt. When the temperature was in the range 1012 > T > 1010 K
468

11.50 YANG‚ÄìMILLS THEORY
or mŒºc2 > kT > mec2, where mŒº is the mass of the muon and me that of the
electron, the radiation was mostly electrons, positrons, photons, and neutrinos,
and the relation between the time t and the temperature T was (Weinberg, 2010,
ch. 3)
t = 0.994 sec √ó

1010 K
T
2
+ constant.
(11.464)
By 109 K, the positrons had annihilated with electrons, and the neutrinos
fallen out of equilibrium. Between 109 K and 106 K, when the energy den-
sity of nonrelativistic particles became relevant, the time‚Äìtemperature relation
was (Weinberg, 2010, ch. 3)
t = 1.78 sec √ó

1010 K
T
2
+ constant‚Ä≤.
(11.465)
During the 8.8 billion years of the matter era, a(t) grew as (11.454)
a(t) =
#
(t ‚àítr)

3œÄGœÅ(t‚Ä≤m)a(t‚Ä≤m) + a3/2(tr)
$2/3
+ a(tr),
(11.466)
where tr is the time at the end of the radiation era, and t‚Ä≤
m is any time in the mat-
ter era. By 360,000 years, the temperature had dropped to 3000 K, the Universe
had become transparent, and the CMBR had begun to travel freely.
Over the past 5 billion years of the era of vacuum dominance, a(t) has been
growing exponentially (11.461)
a(t) = a(tm) exp

(t ‚àítm)

8œÄGœÅv/3

,
(11.467)
in which tm is the time at the end of the matter era, and œÅv is the density of dark
energy, which, while vastly less than the energy density œÅi that drove inÔ¨Çation,
currently amounts to 76% of the total energy density.
11.50 Yang‚ÄìMills theory
The gauge transformation of an abelian gauge theory like electrodynamics mul-
tiplies a single charged Ô¨Åeld by a space-time-dependent phase factor œÜ‚Ä≤(x) =
exp(iqŒ∏(x)) œÜ(x). Yang and Mills generalized this gauge transformation to one
that multiplies a vector œÜ of matter Ô¨Åelds by a space-time dependent unitary
matrix U(x)
œÜ‚Ä≤
a(x) =
n

b=1
Uab(x) œÜb(x)
or
œÜ‚Ä≤(x) = U(x) œÜ(x)
(11.468)
469

TENSORS AND LOCAL SYMMETRIES
and showed how to make the action of the theory invariant under such
nonabelian gauge transformations. (The Ô¨Åelds œÜ are scalars for simplicity.)
Since the matrix U is unitary, inner products like œÜ‚Ä†(x) œÜ(x) are automatically
invariant

œÜ‚Ä†(x) œÜ(x)
‚Ä≤
= œÜ‚Ä†(x)U‚Ä†(x)U(x)œÜ(x) = œÜ‚Ä†(x)œÜ(x).
(11.469)
But inner products of derivatives ‚àÇiœÜ‚Ä† ‚àÇiœÜ are not invariant because the deriva-
tive acts on the matrix U(x) as well as on the Ô¨Åeld œÜ(x).
Yang and Mills made derivatives DiœÜ that transform like the Ô¨Åelds œÜ
(DiœÜ)‚Ä≤ = U DiœÜ.
(11.470)
To do so, they introduced gauge-Ô¨Åeld matrices Ai that play the role of the
connections i in general relativity and set
Di = ‚àÇi + Ai,
(11.471)
in which Ai, like ‚àÇi, is antihermitian. They required that under the gauge trans-
formation (11.468), the gauge-Ô¨Åeld matrix Ai transform to A‚Ä≤
i in such a way as
to make the derivatives transform as in (11.470)
(DiœÜ)‚Ä≤ =

‚àÇi + A‚Ä≤
i

œÜ‚Ä≤ =

‚àÇi + A‚Ä≤
i

UœÜ = U DiœÜ = U (‚àÇi + Ai) œÜ.
(11.472)
So they set

‚àÇi + A‚Ä≤
i

UœÜ = U (‚àÇi + Ai) œÜ
or
(‚àÇiU) œÜ + A‚Ä≤
i UœÜ = UAi œÜ
(11.473)
and made the gauge-Ô¨Åeld matrix Ai transform as
A‚Ä≤
i = UAiU‚àí1 ‚àí(‚àÇiU) U‚àí1.
(11.474)
Thus under the gauge transformation (11.468), the derivative DiœÜ transforms
as in (11.470), like the vector œÜ in (11.468), and the inner product of covariant
derivatives

DiœÜ
‚Ä† DiœÜ
‚Ä≤
=

DiœÜ
‚Ä† U‚Ä†UDiœÜ =

DiœÜ
‚Ä† DiœÜ
(11.475)
remains invariant.
To make an invariant action density for the gauge-Ô¨Åeld matrices Ai, they
used the transformation law (11.472), which implies that D‚Ä≤
i UœÜ = UDi œÜ or
D‚Ä≤
i = UDi U‚àí1. So they deÔ¨Åned their generalized Faraday tensor as
Fik = [Di, Dk] = ‚àÇiAk ‚àí‚àÇkAi + [Ai, Ak],
(11.476)
which transforms covariantly
F‚Ä≤
ik = UFikU‚àí1.
(11.477)
470

11.51 GAUGE THEORY AND VECTORS
They then generalized the action density FikFik of electrodynamics to the trace
Tr

FikFik
of the square of the Faraday matrices which is invariant under gauge
transformations since
Tr

UFikU‚àí1UFikU‚àí1
= Tr

UFikFikU‚àí1
= Tr

FikFik
.
(11.478)
As an action density for fermionic matter Ô¨Åelds, they replaced the ordinary
derivative in Dirac‚Äôs formula œà(Œ≥ i‚àÇi + m)œà by the covariant derivative (11.471)
to get œà(Œ≥ iDi + m)œà (Chen-Ning Yang, 1922‚Äì; Robert L. Mills, 1927‚Äì1999).
In an abelian gauge theory, the square of the 1-form A = Ai dxi vanishes
A2 = Ai Ak dxi ‚àßdxk = 0, but in a nonabelian gauge theory the gauge Ô¨Åelds are
matrices, and A2 Ã∏= 0. The sum dA + A2 is the Faraday 2-form
F = dA + A2 = (‚àÇi Ak + Ai Ak) dxi ‚àßdxk
= 1
2 (‚àÇi Ak ‚àí‚àÇk Ai + [Ai Ak]) dxi ‚àßdxk
= 1
2Fik dxi ‚àßdxk.
(11.479)
The scalar matter Ô¨Åelds œÜ may have self-interactions described by a poten-
tial V(œÜ) such as V(œÜ) = Œª(œÜ‚Ä†œÜ ‚àím2/Œª)2, which is positive unless œÜ‚Ä†œÜ =
m2/Œª. The kinetic action of these Ô¨Åelds is (DiœÜ)‚Ä†DiœÜ. At low temperatures,
these scalar Ô¨Åelds assume mean values ‚ü®0|œÜ|0‚ü©= œÜ0 in the vacuum with
œÜ‚Ä†
0œÜ0 = m2/Œª so as to minimize their potential energy density V(œÜ). Their
kinetic action (DiœÜ)‚Ä†DiœÜ = (‚àÇiœÜ + AiœÜ)‚Ä†(‚àÇiœÜ + AiœÜ) then is in effect œÜ‚Ä†
0 Ai AiœÜ0.
The gauge-Ô¨Åeld matrix Ai
ab = tŒ±
abAi
Œ± is a linear combination of the genera-
tors tŒ± of the gauge group. So the action of the scalar Ô¨Åelds contains the term
œÜ‚Ä†
0 Ai Ai œÜ0 = M2
Œ±Œ≤ Ai
Œ± AiŒ≤ in which the mass-squared matrix for the gauge Ô¨Åelds
is M2
Œ±Œ≤ = œÜ‚àóa
0 tŒ±
ab tŒ≤
bc œÜc
0. This Higgs mechanism gives masses to those linear
combinations bŒ≤i AŒ≤ of the gauge Ô¨Åelds for which M2
Œ±Œ≤ bŒ≤i = m2
i bŒ±i Ã∏= 0 .
The Higgs mechanism also gives masses to the fermions. The mass term m
in the Yang‚ÄìMills‚ÄìDirac action is replaced by something like c œÜ in which c
is a constant different for each fermion. In the vacuum and at low tempera-
tures, each fermion in effect acquires as its mass c œÜ0. On 4 July 2012, physicists
at CERN‚Äôs Large Hadron Collider announced the discovery of a Higgs-like
particle with a mass near 126 GeV/c2 (Peter Higgs, 1929‚Äì).
11.51 Gauge theory and vectors
This section is optional on a Ô¨Årst reading.
We can formulate Yang‚ÄìMills theory in terms of vectors as we did relativity.
To accommodate noncompact groups, we will generalize the unitary matrices
U(x) of the Yang‚ÄìMills gauge group to nonsingular matrices V(x) that act on
n matter Ô¨Åelds œàa(x) as
471

TENSORS AND LOCAL SYMMETRIES
œà‚Ä≤a(x) =
n

a=1
Va
b(x) œàa(x).
(11.480)
The Ô¨Åeld
(x) =
n

a=1
ea(x) œàa(x)
(11.481)
will be gauge invariant ‚Ä≤(x) = (x) if the vectors ea(x) transform as
e‚Ä≤
a(x) =
n

b=1
eb(x) V‚àí1b
a(x).
(11.482)
In what follows, we will sum over repeated indices from 1 to n and often will
suppress explicit mention of the space-time coordinates. In this compressed
notation, the Ô¨Åeld  is gauge invariant because
‚Ä≤ = e‚Ä≤
a œà‚Ä≤a = eb V‚àí1b
a Va
c œàc = eb Œ¥b
c œàc = eb œàb = ,
(11.483)
which is e‚Ä≤Tœà‚Ä≤ = eTV‚àí1Vœà = eTœà in matrix notation.
The inner product of two basis vectors is an internal ‚Äúmetric tensor‚Äù
e‚àó
a ¬∑ eb =
N

Œ±=1
N

Œ≤=1
eŒ±‚àó
a Œ∑Œ±Œ≤eŒ±
b =
N

Œ±=1
eŒ±‚àó
a eŒ±
b = gab,
(11.484)
in which for simplicity I used the N-dimensional identity matrix for the metric
Œ∑. As in relativity, we‚Äôll assume the matrix gab to be nonsingular. We then can
use its inverse to construct dual vectors ea = gabeb that satisfy ea‚Ä† ¬∑ eb = Œ¥a
b.
The free Dirac action density of the invariant Ô¨Åeld 
(Œ≥ i‚àÇi + m) = œàaea‚Ä†(Œ≥ i‚àÇi + m)ebœàb = œàa

Œ≥ i(Œ¥a
b‚àÇi + ea‚Ä† ¬∑ eb,i) + mŒ¥a
b

œàb
(11.485)
is the full action of the component Ô¨Åelds œàb
(Œ≥ i‚àÇi + m) = œàa(Œ≥ iDa
i b + m Œ¥a
b)œàb = œàa

Œ≥ i(Œ¥a
b‚àÇi + Aa
i b) + m Œ¥a
b

œàb
(11.486)
if we identify the gauge-Ô¨Åeld matrix as Aa
i b = ea‚Ä† ¬∑ eb,i in harmony with the
deÔ¨Ånition (11.213) of the afÔ¨Åne connection k
i‚Ñì= ek ¬∑ e‚Ñì,i.
Under the gauge transformation e‚Ä≤
a = eb V‚àí1b
a, the metric matrix trans-
forms as
g‚Ä≤
ab = V‚àí1c‚àó
a gcd V‚àí1d
b
or as
g‚Ä≤ = V‚àí1‚Ä† g V‚àí1
(11.487)
in matrix notation. Its inverse goes as g‚Ä≤‚àí1 = V g‚àí1 V‚Ä†.
472

11.51 GAUGE THEORY AND VECTORS
The gauge-Ô¨Åeld matrix Aa
i b = ea‚Ä† ¬∑ eb,i = gace‚Ä†
c ¬∑ eb,i transforms as
A‚Ä≤a
i b = g‚Ä≤ace‚Ä≤‚Ä†
a ¬∑ e‚Ä≤
b,i = Va
cAc
idV‚àí1d
b
+ Va
cV‚àí1c
b,i
(11.488)
or as A‚Ä≤
i = VAiV‚àí1 + V‚àÇiV‚àí1 = VAiV‚àí1 ‚àí(‚àÇiV) V‚àí1.
By using the identity ea‚Ä† ¬∑ ec,i = ‚àíea‚Ä†
,i ¬∑ ec, we may write (exercise 11.44) the
Faraday tensor as
Fa
ijb = [Di, Dj]a
b = ea‚Ä†
,i ¬∑eb,j ‚àíea‚Ä†
,i ¬∑ec ec‚Ä†¬∑eb,j ‚àíea‚Ä†
,j ¬∑eb,i +ea‚Ä†
,j ¬∑ec ec‚Ä†¬∑eb,i. (11.489)
If n = N, then
n

c=1
eŒ±
c eŒ≤c‚àó= Œ¥Œ±Œ≤
and
Fa
ijb = 0.
(11.490)
The Faraday tensor vanishes when n = N because the dimension of the
embedding space is too small to allow the tangent space to have different orien-
tations at different points x of space-time. The Faraday tensor, which represents
internal curvature, therefore must vanish. One needs at least three dimensions
in which to bend a sheet of paper. The embedding space must have N > 2
dimensions for SU(2), N > 3 for SU(3), and N > 5 for SU(5).
The covariant derivative of the internal metric matrix
g;i = g,i ‚àígAi ‚àíA‚Ä†
i g
(11.491)
does not vanish and transforms as

g;i
‚Ä≤ = V‚àí1‚Ä†g,iV‚àí1. A suitable action den-
sity for it is the trace Tr(g;ig‚àí1g;ig‚àí1). If the metric matrix assumes a (constant,
hermitian) mean value g0 in the vacuum at low temperatures, then its action is
m2Tr

(g0Ai + A‚Ä†
i g0)g‚àí1
0 (g0Ai + Ai‚Ä†g0)g‚àí1
0

,
(11.492)
which is a mass term for the matrix of gauge bosons
Wi = g1/2
0
Ai g‚àí1/2
0
+ g‚àí1/2
0
A‚Ä†
i g1/2
0 .
(11.493)
This mass mechanism also gives masses to the fermions. To see how, we write
the Dirac action density (11.486) as
œàa

Œ≥ i(Œ¥a
b‚àÇi + Aa
i b) + m Œ¥a
b

œàb = œàa 
Œ≥ i(gab‚àÇi + gacAc
i b) + m gab

œàb.
(11.494)
Each fermion now gets a mass m ci proportional to an eigenvalue ci of the
hermitian matrix g0.
This mass mechanism does not leave behind scalar bosons. Whether Nature
uses it is unclear.
473

TENSORS AND LOCAL SYMMETRIES
11.52 Geometry
This section is optional on a Ô¨Årst reading.
In gauge theory, what plays the role of space-time? Could it be the group
manifold? Let us consider the gauge group SU(2) whose group manifold is the
3-sphere in Ô¨Çat euclidean 4-space. A point on the 3-sphere is
p =

¬±

1 ‚àír2, r1, r2, r3
(11.495)
as explained in example 10.28. The coordinates ra = ra are not vectors. The
three basis vectors are
ea = ‚àÇp
‚àÇra =

‚àì
ra

1 ‚àír2 , Œ¥1
a, Œ¥2
a, Œ¥3
a

(11.496)
and so the metric gab = ea ¬∑ eb is
gab = ra rb
1 ‚àír2 + Œ¥ab
(11.497)
or
‚à•g ‚à•=
1
1 ‚àír2
‚éõ
‚éù
1 ‚àír2
2 ‚àír2
3
r1 r2
r1 r3
r2 r1
1 ‚àír2
1 ‚àír2
3
r2 r3
r3 r1
r3 r2
1 ‚àír2
1 ‚àír2
2
‚éû
‚é†.
(11.498)
The inverse matrix is
gbc = Œ¥bc ‚àírb rc.
(11.499)
The dual vectors
eb = gbcec =

‚àìrb

1 ‚àír2, Œ¥b
1 ‚àírbr1, Œ¥b
2 ‚àírbr2, Œ¥b
3 ‚àírbr3

(11.500)
satisfy eb ¬∑ ea = Œ¥b
a.
There are two kinds of afÔ¨Åne connection eb¬∑ea,c and eb¬∑ea,i. If we differentiate
ea with respect to an SU(2) coordinate rc, then
Eb
c a = eb ¬∑ ea,c = rb

Œ¥ac + ra rc
1 ‚àír2

,
(11.501)
in which we used E (for Einstein) instead of  for the afÔ¨Åne connection. If we
differentiate ea with respect to a space-time coordinate xi, then
Eb
i a = eb ¬∑ ea,i = eb ¬∑ ea,c rc
,i = rb rc
,i

Œ¥ac + ra rc
1 ‚àír2

.
(11.502)
But if the group coordinates ra are functions of the space-time coordinates xi,
then there are four new basis 4-vectors ei = eara,i. The metric then is a 7 √ó 7
474

EXERCISES
matrix ‚à•g ‚à•with entries ga,b = ea ¬∑ eb, ga,k = ea ¬∑ ek, gi,b = ei ¬∑ eb, and
gi,k = ei ¬∑ ek or
‚à•g ‚à•=
 ga,b
ga,b rb,k
ga,b ra,i
ga,b ra,i rb,k.

.
(11.503)
Further reading
The classics Gravitation and Cosmology (Weinberg, 1972), Gravitation (Mis-
ner et al., 1973), and Cosmology (Weinberg, 2010) as well as the terse General
Theory of Relativity (Dirac, 1996) and the very accessible Spacetime and
Geometry (Carroll, 2003) are of special interest, as is Daniel Finley‚Äôs website
(panda.unm.edu/Courses/Finley/p570.html).
Exercises
11.1
Compute the derivatives (11.22 & 11.23).
11.2
Show that the transformation x ‚Üíx‚Ä≤ deÔ¨Åned by (11.16) is a rotation and a
reÔ¨Çection.
11.3
Show that the matrix (11.40) satisÔ¨Åes the Lorentz condition (11.39).
11.4
If Œ∑ = L Œ∑ LT, show that  = L‚àí1 satisÔ¨Åes the deÔ¨Ånition (11.39) of a Lorentz
transformation Œ∑ = TŒ∑ .
11.5
The LHC is designed to collide 7 TeV protons against 7 TeV protons for a
total collision energy of 14 TeV. Suppose one used a linear accelerator to Ô¨Åre
a beam of protons at a target of protons at rest at one end of the accelerator.
What energy would you need to see the same physics as at the LHC?
11.6
Use Gauss‚Äôs law and the Maxwell‚ÄìAmp√®re law (11.87) to show that the
microscopic (total) current 4-vector j = (cœÅ, j) obeys the continuity equation
ÀôœÅ + ‚àá¬∑ j = 0.
11.7
Show that if Mik is a covariant second-rank tensor with no particular sym-
metry, then only its antisymmetric part contributes to the 2-form Mik dxi ‚àß
dxk and only its symmetric part contributes to the quantity Mik dxidxk.
11.8
In rectangular coordinates, use the Levi-Civita identity (1.449) to derive the
curl‚Äìcurl equations (11.90).
11.9
Derive the Bianchi identity (11.92) from the deÔ¨Ånition (11.79) of the Fara-
day Ô¨Åeld-strength tensor, and show that it implies the two homogeneous
Maxwell equations (11.82).
11.10 Show that if A is a p-form, then d(AB) = dA ‚àßB + (‚àí1)pA ‚àßdB.
11.11 Show that if œâ = aijdxi ‚àßdxj/2 with aij = ‚àíaji, then
dœâ = 1
3!

‚àÇkaij + ‚àÇiajk + ‚àÇjaki

dxi ‚àßdxj ‚àßdxk.
(11.504)
11.12 Using tensor notation throughout, derive (11.147) from (11.145 & 11.146).
475

TENSORS AND LOCAL SYMMETRIES
11.13 Use the Ô¨Çat-space formula (11.168) to compute the change dp due to dœÅ,
dœÜ, and dz, and so derive the expressions (11.169) for the orthonormal basis
vectors ÀÜœÅ, ÀÜœÜ, and ÀÜz.
11.14 Similarly, derive (11.175) from (11.174).
11.15 Use the deÔ¨Ånition (11.191) to show that in Ô¨Çat 3-space, the dual of the Hodge
dual is the identity: ‚àó‚àódxi = dxi and ‚àó‚àó(dxi ‚àßdxk) = dxi ‚àßdxk.
11.16 Use the deÔ¨Ånition of the Hodge star (11.202) to derive (a) two of the four
identities (11.203) and (b) the other two.
11.17 Show that Levi-Civita‚Äôs 4-symbol obeys the identity (11.207).
11.18 Show that œµ‚Ñìmn œµpmn = 2 Œ¥p
‚Ñì.
11.19 Show that œµk‚Ñìmn œµp‚Ñìmn = 3! Œ¥p
k.
11.20 Using the formulas (11.175) for the basis vectors of spherical coordinates
in terms of those of rectangular coordinates, compute the derivatives of the
unit vectors ÀÜr, ÀÜŒ∏, and ÀÜœÜ with respect to the variables r, Œ∏, and œÜ. Your formu-
las should express these derivatives in terms of the basis vectors ÀÜr, ÀÜŒ∏, and ÀÜœÜ.
(b) Using the formulas of (a), derive the formula (11.297) for the Laplacian
‚àá¬∑ ‚àá.
11.21 Consider the torus with coordinates Œ∏, œÜ labeling the arbitrary point
p = (cos œÜ(R + r sin Œ∏), sin œÜ(R + r sin Œ∏), r cos Œ∏)
(11.505)
in which R > r. Both Œ∏ and œÜ run from 0 to 2œÄ. (a) Find the basis vectors eŒ∏
and eœÜ. (b) Find the metric tensor and its inverse.
11.22 For the same torus, (a) Ô¨Ånd the dual vectors eŒ∏ and eœÜ and (b) Ô¨Ånd the
nonzero connections i
jk where i, j, and k take the values Œ∏ and œÜ.
11.23 For the same torus, (a) Ô¨Ånd the two Christoffel matrices Œ∏ and œÜ, (b) Ô¨Ånd
their commutator [Œ∏, œÜ], and (c) Ô¨Ånd the elements RŒ∏
Œ∏Œ∏Œ∏, RœÜ
Œ∏œÜŒ∏, RŒ∏
œÜŒ∏œÜ, and
RœÜ
œÜœÜœÜ of the curvature tensor.
11.24 Find the curvature scalar R of the torus with points (11.505). Hint: in these
four problems, you may imitate the corresponding calculation for the sphere
in section 11.42.
11.25 By differentiating the identity gik gk‚Ñì= Œ¥i
‚Ñì, show that Œ¥gik = ‚àígisgktŒ¥gst.
11.26 Just to get an idea of the sizes involved in black holes, imagine an isolated
sphere of matter of uniform density œÅ that as an initial condition is all at rest
within a radius rb. Its radius will be less than its Schwarzschild radius if
rb < 2MG
c2
= 2
4
3œÄr3
bœÅ
 G
c2 .
(11.506)
If the density œÅ is that of water under standard conditions (1 gram per cc),
for what range of radii rb might the sphere be or become a black hole? Same
question if œÅ is the density of dark energy.
11.27 For the points (11.392), derive the metric (11.395) with k = 1. Don‚Äôt forget
to relate dœá to dr.
11.28 For the points (11.393), derive the metric (11.395) with k = 0.
476

EXERCISES
11.29 For the points (11.394), derive the metric (11.395) with k = ‚àí1. Don‚Äôt forget
to relate dœá to dr.
11.30 Suppose the constant k in the Robertson‚ÄìWalker metric (11.391 or 11.395)
is some number other than 0 or ¬±1. Find a coordinate transforma-
tion such that in the new coordinates, the Robertson‚ÄìWalker metric has
k = k/|k| = ¬±1.
11.31 Derive the afÔ¨Åne connections in equation (11.399).
11.32 Derive the afÔ¨Åne connections in equation (11.400).
11.33 Derive the afÔ¨Åne connections in equation (11.401).
11.34 Derive the spatial Einstein equation (11.411) from (11.375, 11.395, 11.406,
11.408, & 11.409).
11.35 Assume there had been no inÔ¨Çation and that there were no dark energy. In
this case, the magnitude of the difference |‚àí1| would have increased as t2/3
over the past 13.8 billion years. Show explicitly how close to unity  would
have had to have been at t = 1 s so as to satisfy the observational constraint
0 = 1.003 ¬± 0.010 on the present value of .
11.36 Derive the relation (11.431) between the energy density œÅ and the
Robertson‚ÄìWalker scale factor a(t) from the conservation law (11.427) and
the equation of state p = wœÅ.
11.37 Use the Friedmann equations (11.410 & 11.412) with œÅ = ‚àíp and k = 1 to
derive (11.438) subject to the boundary condition that a(t) has its minimum
at t = 0.
11.38 Use the Friedmann equations (11.410 & 11.412) with w = ‚àí1 and k = ‚àí1
to derive (11.439) subject to the boundary condition that a(0) = 0.
11.39 Use the Friedmann equations (11.410 & 11.412) with w = ‚àí1 and k = 0 to
derive (11.440). Show why a linear combination of the two solutions (11.440)
does not work.
11.40 Use the Friedmann equations (11.410 & 11.412) with w = 1/3 and k = 0 to
derive (11.447) subject to the boundary condition that a(0) = 0.
11.41 Show that if the matrix U(x) is nonsingular, then
(‚àÇi U) U‚àí1 = ‚àíU ‚àÇi U‚àí1.
(11.507)
11.42 The gauge-Ô¨Åeld matrix is a linear combination Ak = ‚àíi g tbAb
k of the gen-
erators tb of a representation of the gauge group. The generators obey the
commutation relations
[ta, tb] = ifabctc,
(11.508)
in which the fabc are the structure constants of the gauge group. Show that
under a gauge transformation (11.474)
A‚Ä≤
i = UAiU‚àí1 ‚àí(‚àÇiU) U‚àí1
(11.509)
by the unitary matrix U = exp(‚àíigŒªata) in which Œªa is inÔ¨Ånitesimal, the
gauge-Ô¨Åeld matrix Ai transforms as
‚àíigA‚Ä≤a
i ta = ‚àíigAa
i ta ‚àíig2fabcŒªaAb
i tc + ig‚àÇiŒªata.
(11.510)
477

TENSORS AND LOCAL SYMMETRIES
Show further that the gauge Ô¨Åeld transforms as
A‚Ä≤a
i = Aa
i ‚àí‚àÇiŒªa ‚àígfabcAb
i Œªc.
(11.511)
11.43 Show that if the vectors ea(x) are orthonormal, then ea‚Ä† ¬∑ ec,i = ‚àíea‚Ä†
,i ¬∑ ec.
11.44 Use the identity of exercise 11.43 to derive the formula (11.489) for the
nonabelian Faraday tensor.
478

12
Forms
12.1 Exterior forms
1-forms
A 1-form is a linear function œâ that maps vectors into numbers. Thus,
if A and B are vectors in Rn and z and w are numbers, then
œâ(zA + wB) = z œâ(A) + w œâ(B).
(12.1)
The n coordinates x1, . . . , xn are 1-forms; they map a vector A into its coordi-
nates: x1(A) = A1, . . . , xn(A) = An. Every 1-form may be expanded in terms of
these basic 1-forms as
œâ = B1x1 + ¬∑ ¬∑ ¬∑ + Bnxn
(12.2)
so that
œâ(A) = B1x1(A) + ¬∑ ¬∑ ¬∑ + Bnxn(A)
= B1A1 + ¬∑ ¬∑ ¬∑ + BnAn
= (B, A) = B ¬∑ A.
(12.3)
Thus, every 1-form is associated with a (dual) vector, in this case B.
2-forms
A 2-form is a function that maps pairs of vectors into numbers linearly
and skew-symmetrically. Thus, if A, B, and C are vectors in Rn and z and w are
numbers, then
œâ2(zA + wB, C) = z œâ2(A, C) + w œâ2(B, C),
œâ2(A, B) = ‚àíœâ2(B, A).
(12.4)
One often drops the superscript and writes the addition of two 2-forms as
(œâ1 + œâ2)(A, B) = œâ1(A, B) + œâ2(A, B).
(12.5)
479

FORMS
Example 12.1 (Parallelogram)
The oriented area of the parallelogram deÔ¨Åned
by two 2-vectors A and B is the determinant
œâ(A, B) =

A1
A2
B1
B2
 .
(12.6)
This 2-form maps the ordered pair of vectors (A, B) into the oriented area (¬± the
usual area) of the parallelogram they describe. To check that this 2-form gives
the area to within a sign, rotate the coordinates so that the 2-vector A runs from
the origin along the x-axis. Then A2 = 0, and the 2-form gives A1B2 which is the
base A1 of the parallelogram times its height B2.
Example 12.2 (Parallelepiped)
The triple scalar product of three 3-vectors
œâ2
A(B, C) = A ¬∑ B √ó C =

A1
A2
A3
B1
B2
B3
C1
C2
C3

= œâ3(A, B, C)
(12.7)
is both a 2-form that depends upon the vector A and also a 3-form that maps the
triplet of vectors A, B, C into the signed volume of their parallelepiped.
k-forms
A k-form (or an exterior form of degree k) is a linear function of k
vectors that is antisymmetric. For vectors A1, . . . , Ak and numbers z and w
œâ(zA‚Ä≤
1 + wA‚Ä≤‚Ä≤
1, A2, . . . , Ak) = z œâ(A‚Ä≤
1, A2, . . . , Ak) + w œâ(A‚Ä≤‚Ä≤
1, A2, . . . , Ak) (12.8)
and the interchange of any two vectors makes a minus sign
œâ(A2, A1, . . . , Ak) = ‚àíœâ(A1, A2, . . . , Ak).
(12.9)
Exterior product of two 1-forms
The 1-form œâ1 maps the vectors A and B
into the numbers œâ1(A) and œâ1(B), and the 1-form œâ2 does the same thing with
1 ‚Üí2. The value of the exterior product œâ1 ‚àßœâ2 on the two vectors A and B is
the 2-form deÔ¨Åned by the 2 √ó 2 determinant
œâ1 ‚àßœâ2(A, B) =

œâ1(A)
œâ2(A)
œâ1(B)
œâ2(B)
 = œâ1(A)œâ2(B) ‚àíœâ2(A)œâ1(B)
(12.10)
or more formally
œâ1 ‚àßœâ2 = œâ1 ‚äóœâ2 ‚àíœâ2 ‚äóœâ1.
(12.11)
The most general 2-form on Rn is a linear combination of the basic 2-forms
xi ‚àßxj
œâ2 =

1‚â§i<k‚â§n
aik xi ‚àßxk.
(12.12)
480

12.2 DIFFERENTIAL FORMS
If the unit vectors in the n orthogonal directions of Rn are e1, . . . , en, then
xi(ek) = Œ¥ik and so
œâ2(ei, ek) = aik

xi(ei)
xk(ei)
xi(ek)
xk(ek)
 = aik

1
0
0
1
 = aik.
(12.13)
Exterior product of k 1-forms
The exterior product of k 1-forms œâ1, . . . , œâk
maps the k n-vectors A1, A2, . . . , Ak to the determinant
œâ1 ‚àßœâ2 ‚àß¬∑ ¬∑ ¬∑ ‚àßœâk(A1, A2, . . . , Ak) =

œâ1(A1)
. . .
œâk(A1)
...
...
...
œâ1(Ak)
. . .
œâk(Ak)

.
(12.14)
The most general k-form on Rn is a linear combination of the various exterior
products of k basic 1-forms xi1 ‚àß¬∑ ¬∑ ¬∑ ‚àßxik
œâk =

1‚â§i1<...ik‚â§n
ai1...ik xi1 ‚àß¬∑ ¬∑ ¬∑ ‚àßxik.
(12.15)
Exterior multiplication
The exterior multiplication of a k-form with an ‚Ñì-
form is linear, associative, and antisymmetric
œâk ‚àßœâ‚Ñì= (‚àí1)k‚Ñìœâ‚Ñì‚àßœâk.
(12.16)
Restriction
of
forms
A
p-form
œâp
is
a
map
from
the
product
V √ó ¬∑ ¬∑ ¬∑ √ó V of p copies of some vector space V into the real numbers. The
restriction œâp|U (A1, A2, . . . , Ap) of the p-form œâp to a subspace U ‚äÇV is the
same p-form œâp but with its domain restricted to vectors Ai ‚ààU.
12.2 Differential forms
A manifold is a set of points that can be labeled locally by coordinates in Rn in
such a way that the coordinates make sense when the local regions overlap. The
k-dimensional surface Sk of the unit sphere in Rk+1
k+1

i=1
y2
i = 1
(12.17)
is an example of a manifold. A smooth function f (x1, . . . , xn) is one that
is inÔ¨Ånitely differentiable with respect to all combinations of its arguments
x1, . . . , xn.
There are two ways of thinking about differential forms. The Russian lit-
erature views a manifold as embedded in Rn and so is somewhat more
straightforward. We will discuss it Ô¨Årst.
481

FORMS
The Russian way
Suppose x(t) is a curve with x(0) = x on some manifold
M, and f (x(t)) is a smooth function f : Rn ‚ÜíR that maps points x(t) into
numbers. Then the differential df (Àôx(t)) maps Àôx(t) at x into
df
 d
dtx(t)

‚â°d
dtf (x(t)) =
n

j=1
Àôx(t)j
‚àÇf (x(t))
‚àÇxj
= Àôx(t) ¬∑ ‚àáf (x(t))
(12.18)
all at t = 0. As physicists, we think of df as a number ‚Äì the change in the
function f (x) when its argument x is changed by dx. Russian mathematicians
think of df as a linear map of tangent vectors Àôx at x into numbers. Since this
map is linear, we may multiply the deÔ¨Ånition (12.18) by dt and arrive at the more
familiar formula
dt df
 d
dtx(t)

= df

dt d
dtx(t)

= df (dx(t)) = dx(t) ¬∑ ‚àáf (x(t))
(12.19)
all at t = 0. So
df (dx) = dx ¬∑ ‚àáf
(12.20)
is the physicist‚Äôs df .
Since the differential df is a linear map of vectors Àôx(0) into numbers, it is a
1-form; since it is deÔ¨Åned on vectors like Àôx(0), it is a differential 1-form. The term
differential 1-form underscores the fact that the actual value of the differential
df depends upon the vector Àôx(0) and the point x = x(0). Mathematicians call
the space of vectors Àôx(0) at the point x = x(0) the tangent space TMx. They say
df is a smooth map of the tangent bundle TM, which is the union of the tangent
spaces for all points x in the manifold M, to the real line, so df : TM ‚ÜíR.
Consider a curve x(t) on a smooth surface with coordinates xi. In the special
case in which xi(t) = Œ¥ikt, the differential dxi(Àôx(t)) by (12.18) is
dxi(Àôx(t)) = d
dtxi(t) = d
dtŒ¥ikt = Œ¥ik.
(12.21)
These dxis are the basic differentials. Using A for the vector Àôx(t), we Ô¨Ånd from
our deÔ¨Ånition (12.18) that
dxi(A) =
n

j=1
Aj
‚àÇxi
‚àÇxj
=
n

j=1
AjŒ¥ij = Ai
(12.22)
as well as
df (A) =
n

j=1
Aj
‚àÇf (x)
‚àÇxj
=
n

j=1
‚àÇf (x)
‚àÇxj
dxj(A)
(12.23)
or
df =
n

j=1
‚àÇf (x)
‚àÇxj
dxj.
(12.24)
482

12.2 DIFFERENTIAL FORMS
Example 12.3 (dr2)
If r2 = x2
1 + x2
2, then the differential 1-form dr2 is
dr2 = 2x1 dx1 + 2x2 dx2.
(12.25)
It takes the 2-vector A into the number
dr2(A) = 2x1 dx1(A) + 2x2 dx2(A) = 2x1 A1 + 2x2 A2.
(12.26)
So if A = (œµ1, œµ2), then dr2(A) = 2x1 œµ1 + 2x2 œµ2.
The other way
Most American, French, and English mathematicians use a
more abstract approach. They abstract from the basic deÔ¨Ånition (12.18) the rule
df
 d
dt

= d
dtf
(12.27)
or more generally
df
 ‚àÇ
‚àÇxk

=
‚àÇ
‚àÇxk
f .
(12.28)
In particular, if f (x) = xi, then
dxi
 ‚àÇ
‚àÇxk

=
‚àÇ
‚àÇxk
xi = Œ¥ik.
(12.29)
In this frequently used notation, the idea is that the derivatives
‚àÇk ‚â°
‚àÇ
‚àÇxk
(12.30)
form a set of orthonormal vectors to which the forms dxi are dual
dxi(‚àÇk) = Œ¥ik.
(12.31)
In the nonRussian literature, equations (12.27‚Äì12.31) deÔ¨Åne how the basic
1-forms dxi act on the vectors ‚àÇk.
Change of variables
Suppose that x1, . . . , xn and y1, . . . , yn are two systems
of coordinates on Rn, and that dx1, . . . , dxn and dy1, . . . , dyn are two sets of
basic differentials. Then by applying the formula (12.24) to the function yk(x),
we get
dyk =
n

j=1
‚àÇyk(x)
‚àÇxj
dxj,
(12.32)
which is the familiar rule for changing variables.
483

FORMS
The most general differential 1-form œâ on the space Rn with coordinates
x1, . . . , xn is a linear combination of the basic differentials dxi with coefÔ¨Åcients
ai(x) that are smooth functions of x = (x1, . . . , xn)
œâ = a1(x) dx1 + ¬∑ ¬∑ ¬∑ + an(x) dxn.
(12.33)
The basic differential 2-forms are dxi ‚àßdxk deÔ¨Åned as
dxi ‚àßdxk(A, B) =

dxi(A)
dxk(A)
dxi(B)
dxk(B)
 =

Ai
Ak
Bi
Bk
 = AiBk ‚àíAkBi. (12.34)
So in particular
dxi ‚àßdxi = 0.
(12.35)
The basic differential k-forms dx1 ‚àß¬∑ ¬∑ ¬∑ ‚àßdxk are deÔ¨Åned as
dx1‚àß¬∑ ¬∑ ¬∑‚àßdxk(A1, . . . Ak) =

dx1(A1)
. . .
dxk(A1)
...
...
...
dx1(Ak)
. . .
dxk(Ak)

=

A11
. . .
A1k
...
...
...
Ak1
. . .
Akk

.
(12.36)
Example 12.4 (dx3 ‚àßdr2)
If r2 = x2
1 + x2
2 + x2
3, then dr2 is
dr2 = 2(xdx1 + x2dx2 + x3dx3)
(12.37)
and the differential 2-form œâ = dx3 ‚àßdr2 is
œâ = dx3 ‚àß2(xdx1 + x2dx2 + x3dx3) = 2x1dx3 ‚àßdx1 + 2x2dx3 ‚àßdx2
(12.38)
since in view of (12.35) dx3‚àßdx3 = 0. So the value of the 2-form œâ on the vectors
A = (1, 2, 3) and B = (2, 1, 1) at the point x = (3, 0, 3) is
œâ(A, B) = 2x1dx3 ‚àßdx1(A, B) = 6

dx3(A)
dx1(A)
dx3(B)
dx1(B)
 = 6

3
1
1
2
 = 30.
(12.39)
On the vectors, C = (1, 0, 0) and D = (0, 0, 1) at x = (2, 3, 4), this 2-form has the
value œâ(C, D) = ‚àí4.
The most general differential k-form œâk on the space Rn with coordinates
x1, . . . , xn is
œâk =

1‚â§i1<...ik‚â§n
ai1...ik(x) dxi1 ‚àß¬∑ ¬∑ ¬∑ ‚àßdxik,
(12.40)
in which the functions ai1...ik(x) are smooth on Rn.
484

12.2 DIFFERENTIAL FORMS
Example 12.5 (Change of variables)
If x1, x2, x3 and y1, y2, y3 are two coor-
dinate systems on R3, then in terms of the basic 1-forms dyk, the 2-form
œâ = Xdx2 ‚àßdx3 is by (12.32)
œâ = Xdx2 ‚àßdx3 = X
 3

k=1
‚àÇx2
‚àÇyj
dyj

‚àß
 3

k=1
‚àÇx3
‚àÇyk
dyk

,
(12.41)
in which jacobians appear such as
‚àÇ(x2, x3)
‚àÇ(y1, y2) = ‚àÇx2
‚àÇy1
‚àÇx3
‚àÇy2
‚àí‚àÇx2
‚àÇy2
‚àÇx3
‚àÇy1
.
(12.42)
In terms of these jacobians, the 2-form œâ = Xdx2 ‚àßdx3 is (exercise 12.2)
œâ = X
‚àÇ(x2, x3)
‚àÇ(y1, y2) dy1 ‚àßdy2 + ‚àÇ(x2, x3)
‚àÇ(y2, y3) dy2 ‚àßdy3 + ‚àÇ(x2, x3)
‚àÇ(y3, y1) dy3 ‚àßdy1

.
(12.43)
On the vectors of example 12.4, both forms of œâ give œâ(A, B) = ‚àíX.
Example 12.6 (Euclidean 3-space)
Let us recall the formula (11.157) for the
square ds2 of the length of a vector dx in orthogonal coordinates
ds2 = h2
1 dx2
1 + h2
2 dx2
2 + h2
3 dx2
3
(12.44)
as well as our formula (11.177) for the gradient
‚àáf = 1
hi
‚àÇf
‚àÇxi
ÀÜei.
(12.45)
Then in cylindrical coordinates (œÅ, œÜ, z), we have hœÅ = 1, hœÜ = œÅ, and hz = 1,
while in spherical coordinates (r, Œ∏, œÜ), we have hr = 1, hŒ∏ = r, and hœÜ = r sin Œ∏.
The value of the form dxi on the unit vector ÀÜej is by (12.20)
dxk(ÀÜej) = ÀÜej ¬∑ ‚àáxk = ÀÜej ¬∑ 1
hi
‚àÇxk
‚àÇxi
ÀÜei = 1
hj
‚àÇxk
‚àÇxj
= Œ¥kj
hj
.
(12.46)
Thus dœÅ(ÀÜeœÅ) = 1, dœÜ(ÀÜeœÜ) = 1/œÅ, and dz(ÀÜez) = 1.
Example 12.7 (Three-dimensional vectors and their forms)
Any three-
dimensional vector A deÔ¨Ånes a 1-form as the dot-product
œâ1
A(U) = A ¬∑ U
(12.47)
and a 2-form as the triple cross-product
œâ2
A(U, V) = A ¬∑ (U √ó V).
(12.48)
Here we assume that we have a right-handed set of basis vectors ÀÜe1, ÀÜe2, and ÀÜe3
with ÀÜe1 √ó ÀÜe2 = ÀÜe3 so as to deÔ¨Åne the cross-product U √ó V. Such a manifold is
said to be oriented.
485

FORMS
The quantity
A = A1ÀÜe1 + A2ÀÜe2 + A3ÀÜe3
(12.49)
is a vector Ô¨Åeld A(x). So if we use (12.44) for the squared length ds2, then we can
write the 1-form (12.47) as
œâ1
A = A1 h1 dx1 + A2 h2 dx2 + A3 h3 dx3
(12.50)
because by (12.46) and summing over i and k we get
œâ1
A(U) = Ai hi dxi

UkÀÜek

= Ai hi Uk Œ¥ik/hk = AiUi = A ¬∑ U.
(12.51)
Similarly (exercise 12.6), the 2-form (12.48) is
œâ2
A = A1 h2h3 dx2 ‚àßdx3 + A2 h3h1 dx3 ‚àßdx1 + A3 h1h2 dx1 ‚àßdx2.
(12.52)
By analogy with the deÔ¨Ånition (12.50), the gradient 1-form œâ1
‚àáf is
œâ1
‚àáf = (‚àáf )k hk dxk
(12.53)
summed over repeated indices. The relation œâ1
‚àáf = df gives
œâ1
‚àáf = (‚àáf )k hk dxk = df = ‚àÇf
‚àÇxk
dxk
(12.54)
according to the deÔ¨Ånition (12.24) of df . So the vector Ô¨Åeld ‚àáf is
‚àáf =
3

k=1
1
hk
‚àÇf
‚àÇxk
ÀÜek,
(12.55)
which in cylindrical and spherical coordinates is
‚àáf = ‚àÇf
‚àÇœÅ ÀÜeœÅ + 1
œÅ
‚àÇf
‚àÇœÜ ÀÜeœÜ + ‚àÇf
‚àÇz ÀÜez = ‚àÇf
‚àÇr ÀÜer + 1
r
‚àÇf
‚àÇŒ∏ ÀÜeŒ∏ +
1
r sin Œ∏
‚àÇf
‚àÇœÜ ÀÜeœÜ
(12.56)
in agreement with (11.180‚Äì11.181).
12.3 Exterior differentiation
Exterior differentiation is nifty. The differential (12.24)
df =
n

k=1
‚àÇf
‚àÇxk
dxk
(12.57)
is the exterior derivative of the function f (x), itself a 0-form. The operator
d =
n

k=1
‚àÇ
‚àÇxk
dxk
(12.58)
turns the 0-form f into the differential 1-form df .
486

12.3 EXTERIOR DIFFERENTIATION
Applied to the 1-form
œâ1 =
n

i=1
ai(x) dxi
(12.59)
the exterior derivative d generates the 2-form
dœâ1 = d
 n

i=1
ai(x) dxi

=
n

i,k=1
ai,k(x) dxk ‚àßdxi,
(12.60)
in which ai,k = ‚àÇkai. But a second application of d gives zero:
ddœâ1 = dd
 n

i=1
ai dxi

= d
‚éõ
‚éù
n

i,k=1
ai,k dxk ‚àßdxi
‚éû
‚é†
=
n

i,k,‚Ñì=1
ai,k‚Ñìdx‚Ñì‚àßdxk ‚àßdxi = 0
(12.61)
because the double partial derivative ai,k‚Ñì= ‚àÇ‚Ñì‚àÇkai is symmetric in k and ‚Ñìwhile
the wedge product dx‚Ñì‚àßdxk is antisymmetric in these indices.
We have seen (12.40) that the most general differential k-form is
œâk =

1‚â§i1<¬∑¬∑¬∑ik‚â§n
ai1...ik(x) dxi1 ‚àß¬∑ ¬∑ ¬∑ ‚àßdxik
(12.62)
in which the functions ai1...ik(x) are smooth on Rn. The exterior derivative
operator d turns œâk into the (k + 1)-form
dœâk =

1‚â§i1<¬∑¬∑¬∑ik‚â§n
d

ai1...ik(x) dxi1 ‚àß¬∑ ¬∑ ¬∑ ‚àßdxik

=

1‚â§‚Ñì,i1<¬∑¬∑¬∑ik‚â§n
ai1...ik,‚Ñì(x) dx‚Ñì‚àßdxi1 ‚àß¬∑ ¬∑ ¬∑ ‚àßdxik.
(12.63)
Once again d d œâk = 0 so quite generally
d d = 0.
(12.64)
If œâ is the wedge product of two 1-forms œâa = ai dxi and œâb = bk dxk
œâ = œâa ‚àßœâb = ai dxi ‚àßbk dxk
(12.65)
487

FORMS
then d maps it to
d œâ = d (ai dxi ‚àßbk dxk) = d (ai bk dxi ‚àßdxk)
= (ai bk),‚Ñìdx‚Ñì‚àßdxi ‚àßdxk
= (ai,‚Ñìbk + ai bk,‚Ñì) dx‚Ñì‚àßdxi ‚àßdxk
= ai,‚Ñìdx‚Ñì‚àßdxi ‚àßbk dxk + ai bk,‚Ñìdx‚Ñì‚àßdxi ‚àßdxk
= (ai,‚Ñìdx‚Ñì‚àßdxi) ‚àßbk dxk ‚àíai dxi ‚àß(bk,‚Ñìdx‚Ñì‚àßdxk)
= d œâa ‚àßœâb ‚àíœâa ‚àßd œâb.
(12.66)
More generally, the exterior derivative operator d maps the wedge product of a
k-form œâk and a p-form œâp to
d

œâk ‚àßœâp
=

dœâk
‚àßœâp + (‚àí1)kœâk ‚àß

dœâp
.
(12.67)
Example 12.8 (Phase space)
If œâ1 is the 1-form
œâ1 = p1dq1 + ¬∑ ¬∑ ¬∑ + pndqn = p ¬∑ dq
(12.68)
with coordinates p1, . . . , pn, q1, . . . , qn, then d œâ1 is the 2-form
d œâ1 = d
 n

i=1
pi dqi

=
n

i,k=1
(‚àÇpkpi) dpk ‚àßdqi =
n

i,k=1
Œ¥ik dpk ‚àßdqi
= dp1 ‚àßdq1 + ¬∑ ¬∑ ¬∑ + dpn ‚àßdqn = dp ‚àßdq.
(12.69)
It follows that d (dp ‚àßdq) = d d œâ1 = 0.
Example 12.9 (A Poincar√© invariant)
The 1-form œâ1 = p ¬∑ dq maps a tiny piece
‚àÇsq ds of a phase-space trajectory into a small element of action p ¬∑ dq(‚àÇsq ds) =
p ¬∑ ‚àÇsq ds. The sum of these pieces along a closed trajectory
A =

‚àÇS
œâ1 =
)
p ¬∑ dq =
)
n

i=1
pi dqi =
)
p ¬∑ ‚àÇq
‚àÇs ds
(12.70)
is a Poincar√© invariant. Because the trajectory is a loop, we may integrate the
second term in its time derivative by parts
ÀôA =
) 
Àôp ¬∑ ‚àÇq
‚àÇs + p ¬∑ ‚àÇ2q
‚àÇs‚àÇt

ds =
) 
Àôp ¬∑ ‚àÇq
‚àÇs ‚àí‚àÇp
‚àÇs ¬∑ Àôq

ds
(12.71)
without acquiring an extra term. The trajectory is physical, and so we can use
Hamilton‚Äôs equations
Àôpi = ‚àí‚àÇH
‚àÇqi
and
Àôqi = ‚àÇH
‚àÇpi
for
i = 1, . . . , n
(12.72)
488

12.3 EXTERIOR DIFFERENTIATION
to write ÀôA as
ÀôA =
) 
‚àí‚àÇH
‚àÇq ¬∑ ‚àÇq
‚àÇs ‚àí‚àÇp
‚àÇs ¬∑ ‚àÇH
‚àÇp

ds = ‚àí
) ‚àÇH
‚àÇs ds = ‚àí
)
dH = 0
(12.73)
because the trajectory is closed.
Example 12.10 (The Bohr model)
In 1912, Bohr considered an electron in a cir-
cular orbit around a proton, set Poincar√©‚Äôs invariant equal to an integral multiple
of Planck‚Äôs constant
A =
)
pdq = 2œÄrp = nh
(12.74)
and so quantized the orbital angular momentum as L = rp = n¬Øh. One can derive
the energy levels of the hydrogen atom from this rule (exercise 12.12). In 1924,
Arnold Sommerfeld applied this trick to a more general orbit of a relativistic
electron about a proton and got the energy levels that Dirac would four years
later.
Example 12.11 (Constant area)
Consider three nearby points in phase space
(p, q), (p + Œ¥p, q + Œ¥q), and (p + p, q + q) that move according to Hamilton‚Äôs
equations (12.72). The time derivatives of the tiny displacements Œ¥pi and Œ¥qi are
(exercise 12.9)
d
dtŒ¥pi = Œ¥Àôpi =
n

k=1
‚àí‚àÇ2H
‚àÇqi‚àÇqk
Œ¥qk ‚àí
‚àÇ2H
‚àÇqi‚àÇpk
Œ¥pk
d
dtŒ¥qi = Œ¥Àôqi =
n

k=1
‚àÇ2H
‚àÇpi‚àÇqk
Œ¥qk +
‚àÇ2H
‚àÇpi‚àÇpk
Œ¥pk.
(12.75)
Similar equations give the derivatives of the small differences pi and qi.
The 2-form (12.69) maps the n pairs of 2-vectors (Œ¥p, Œ¥q) and (p, q) into a
sum of areas of parallelograms
dœâ1(Œ¥p, Œ¥q; p, q) =

Œ¥p1
Œ¥q1
p1
q1
 + ¬∑ ¬∑ ¬∑ +

Œ¥pn
Œ¥qn
pn
qn
 .
(12.76)
By using the time derivatives (12.75) of Œ¥pi and Œ¥qi and those of pi and qi,
one may show (exercise 12.10) that this sum of areas remains constant
d
dt dœâ1(Œ¥p, Œ¥q; p, q) = 0
(12.77)
along the trajectories in phase space (Gutzwiller, 1990, chap. 7).
Example 12.12 (The curl)
We saw in example 12.7 that the 1-form (12.50) of a
vector Ô¨Åeld A is œâA = A1 h1 dx1+A2 h2 dx2+A3 h3 dx3 in which the hks are those
that determine (12.44) the squared length ds2 = h2
k dx2
k of the triply orthogonal
coordinate system with unit vectors ÀÜe1, ÀÜe2, ÀÜe3. So the exterior derivative of the
1-form œâA is
489

FORMS
dœâA =
3

i,k=1
‚àÇk(Ai hi) dxk ‚àßdxi
=
‚àÇ(A3 h3)
‚àÇx2
‚àí‚àÇ(A2 h2)
‚àÇ3

dx2 ‚àßdx3
+
‚àÇ(A2 h2)
‚àÇx1
‚àí‚àÇ(A1 h1)
‚àÇx2

dx1 ‚àßdx2
+
‚àÇA1 h1
‚àÇx3
‚àí‚àÇ(A3 h3)
‚àÇx1

dx3 ‚àßdx1 ‚â°œâ‚àá√óA.
(12.78)
Comparison with equation (12.52) shows that the curl of A is
‚àá√ó A =
1
h2 h3
‚àÇA3 h3
‚àÇx2
‚àí‚àÇA2 h2
‚àÇ3

dx2 ‚àßdx3 ÀÜe1 + ¬∑ ¬∑ ¬∑
=
1
h1 h2 h3

h1ÀÜe1
h2ÀÜe2
h3ÀÜe3
‚àÇ1
‚àÇ2
‚àÇ3
A1h1
A2h2
A3h3

=
1
h1h2h3
3

i,j,k=1
œµijkhi ÀÜei
‚àÇ(Akhk)
‚àÇxj
(12.79)
as we saw in (11.240). This formula gives our earlier expressions for the curl in
cylindrical and spherical coordinates (11.241 & 11.242).
Example 12.13 (The divergence)
We have seen in equations (12.48, 12.49, &
12.52) that the 2-form œâA(U, V) = A ¬∑ (U √ó V) of the vector Ô¨Åeld A = A1ÀÜe1 +
A2ÀÜe2 + A3ÀÜe3 is
œâ2
A = A1 h2 h3 dx2 ‚àßdx3 + A2 h3 h1 dx3 ‚àßdx1 + A3 h1 h2 dx1 ‚àßdx2.
(12.80)
The exterior derivative of this 2-form is
d œâA =
3

k=1
‚àÇ
‚àÇxk
œâA
= ‚àÇA1 h2h3
‚àÇx1
dx1 ‚àßdx2 ‚àßdx3 + ‚àÇA2 h3 h1
‚àÇx2
dx2 ‚àßdx3 ‚àßdx1
+ ‚àÇA3 h1 h2
‚àÇx3
dx3 ‚àßdx1 ‚àßdx2
=
 3

k=1
‚àÇ(Ak h1 h2 h3/hk)
‚àÇxk

dx1 ‚àßdx2 ‚àßdx3.
(12.81)
If one deÔ¨Ånes the divergence ‚àá¬∑ A as
dœâA = (‚àá¬∑ A) h1 h2 h3 dx1 ‚àßdx2 ‚àßdx3
(12.82)
490

12.4 INTEGRATION OF FORMS
then ‚àá¬∑ A must be
‚àá¬∑ A =
1
h1 h2 h3
 3

k=1
‚àÇ(Ak h1 h2 h3/hk)
‚àÇxk

(12.83)
in agreement with (11.283) from which the speciÔ¨Åc formulas for cylindrical
(11.284) and spherical (11.286) coordinates follow.
Example 12.14 (The divergence of a gradient)
By combining our expression
(12.83) for the divergence with our formula (12.55) for the gradient of a function
f , we Ô¨Ånd that its Laplacian ‚ñ≥f in orthogonal coordinates is
‚ñ≥f (x) ‚â°‚àá¬∑ ‚àáf (x) =
1
h1 h2 h3
 3

k=1
‚àÇ
‚àÇxk

h1 h2 h3
h2
k
‚àÇf (x)
‚àÇxk

,
(12.84)
which agrees with (11.295) and so yields (11.296) for cylindrical coordinates and
(11.297) for spherical ones.
12.4 Integration of forms
Let‚Äôs follow the Russian approach at Ô¨Årst. Let Œ≥ (t) be a smooth map from the
unit interval [0, 1] into some manifold M ‚äÇRn. We divide this interval into tiny
segments [ti, ti+1] of length dt = ti+1 ‚àíti, which Œ≥ maps into vectors dŒ≥ (dti) =
ÀôŒ≥ (ti) dt that are tangent to the manifold at the point Œ≥ (ti). The integral of a
1-form œâ along the curve Œ≥ is then the usual Riemann sum

Œ≥
œâ = lim
dt‚Üí0

i
œâ( ÀôŒ≥ (ti)) dt.
(12.85)
If for example, the 1-form is œâA(U) = A ¬∑ U, then

Œ≥
œâ =

Œ≥
œâA( ÀôŒ≥ (ti)) dt =

A ¬∑ ÀôŒ≥ (ti) dt =

A ¬∑ dŒ≥ .
(12.86)
And if œâ = ak(x) dxk, then since dt dxk( ÀôŒ≥ ) = dŒ≥k, the integral

Œ≥
œâ = lim
dt‚Üí0
n

i=1
œâ( ÀôŒ≥ (ti) dt) = lim
dt‚Üí0
n

i=1
ak(x) dxk( ÀôŒ≥ (ti)) dt =

ak(x) dŒ≥k
(12.87)
is a line integral on the manifold

Œ≥
œâ =

ak dxk =

ak dŒ≥k.
(12.88)
Suppose now that our 1-form œâ is exact, that is, that
œâ = dŒ± = Œ±,k dxk
(12.89)
491

FORMS
where Œ±(x) is a 0-form, that is a function, deÔ¨Åned on the manifold. Then by
(12.88) the integral of dŒ± is

Œ≥
dŒ± =

Œ±,k dxk = Œ±(Œ≥ (1)) ‚àíŒ±(Œ≥ (0)).
(12.90)
The signed endpoints Œ≥ (1) and ‚àíŒ≥ (0) are the boundary of the curve Œ≥ (t), which
one writes as ‚àÇŒ≥ . In this notation, we have

Œ≥
dŒ± =

‚àÇŒ≥
Œ±.
(12.91)
Example 12.15 (Green‚Äôs theorem)
Let Œæ and Œ∑ be two inÔ¨Ånitesimal vectors that
form a parallelogram  in the tangent space. We will compute the line integral
of the 1-form œâ = a1(x1, x2) dx1 + a2(x1, x2) dx2 around the boundary ‚àÇ this
parallelogram. This boundary ‚àÇ is a chain of four maps t ‚ÜítŒæ, t ‚ÜíŒæ + tŒ∑,
t ‚ÜíŒ∑ + tŒæ, and t ‚ÜítŒ∑ of the unit interval 0 ‚â§t ‚â§1 into the plane deÔ¨Åned
by the two vectors Œæ and Œ∑. We assign multiplicities 1, 1, ‚àí1, and ‚àí1 to these
four maps, that is, the full chain runs from a point that we‚Äôll call the origin to
the point Œæ, then from Œæ to Œæ + Œ∑, and then from Œæ + Œ∑ to Œ∑, and then from
Œ∑ back to the origin. On the curve Œ≥ (t) = tŒæ, the differentials dxk map ÀôŒ≥ (t) dt
into dxk( ÀôŒ≥ (t) dt) = dxk(Œæ dt) = Œæk dt. Similarly, on the curve Œ≥ (t) = tŒ∑, we have
dxk( ÀôŒ≥ (t) dt) = Œ∑k dt. So summing over k = 1, 2, we Ô¨Ånd

‚àÇ 
œâ =
 1
0
{[ak(tŒæ) ‚àíak(tŒæ + Œ∑)] Œæk ‚àí[ak(tŒ∑) ‚àíak(tŒ∑ + Œæ)] Œ∑k} dt.
(12.92)
Since the tangent vectors Œæ and Œ∑ are inÔ¨Ånitesimal, the square brackets are
ak(tŒæ) ‚àíak(tŒæ + Œ∑) = ‚àíŒ∑j
‚àÇak
‚àÇxj
,
ak(tŒ∑) ‚àíak(tŒ∑ + Œæ) = ‚àíŒæj
‚àÇak
‚àÇxj
(12.93)
and so we have

‚àÇ 
œâ =
 1
0

‚àíŒ∑j
‚àÇak
‚àÇxj
Œæk + Œæj
‚àÇak
‚àÇxj
Œ∑k

dt = ‚àÇak
‚àÇxj

Œæj Œ∑k ‚àíŒæk Œ∑j

.
(12.94)
But the exterior derivative of œâ is
d œâ = d (ak dxk) = ‚àÇak
‚àÇxj
dxj ‚àßdxk
(12.95)
so the last term in (12.94) is just the 2-form d œâ applied to the tangent vectors Œæ
and Œ∑
d œâ(Œæ, Œ∑) = ‚àÇak
‚àÇxj
dxj ‚àßdxk(Œæ, Œ∑) = ‚àÇak
‚àÇxj

ŒæjŒ∑k ‚àíŒækŒ∑j

.
(12.96)
492

12.4 INTEGRATION OF FORMS
And dxj ‚àßdxk(Œæ, Œ∑) = ŒæjŒ∑k ‚àíŒækŒ∑j is the area of the tiny parallelogram  . So this
last expression (12.96) is dœâ integrated over the tiny parallelogram  formed by
the tangent vectors Œæ and Œ∑, and we have

 
dœâ =

‚àÇ 
œâ
(12.97)
for an inÔ¨Ånitesimal parallelogram.
It is easy to extend this identity to an arbitrary surface S of Ô¨Ånite extent. To do
this, we tile the surface S with inÔ¨Ånitesimal parallelograms  Œ± with boundaries
‚àÇ Œ±. The integral over the Ô¨Ånite surface S is then the sum of the integrals over
the  Œ± that tile S

S
dœâ =

Œ±

 Œ±
dœâ,
(12.98)
which by (12.96) is a sum of integrals over the boundaries ‚àÇ Œ±

S
dœâ =

Œ±

 Œ±
dœâ =

Œ±

‚àÇ Œ±
œâ.
(12.99)
In the sum of the integrals over the boundaries ‚àÇ Œ±, the internal boundaries all
cancel, leaving us with the integral over the boundary ‚àÇS of the surface S. Thus
we have

S
dœâ =

Œ±

 Œ±
dœâ =

Œ±

‚àÇ Œ±
œâ =

‚àÇS
œâ
(12.100)
or more simply

S
dœâ =

‚àÇS
œâ,
(12.101)
which generalizes the identity (12.91) from 0-forms to 1-forms.
In the notation of ordinary vector calculus, this relation is

S
(‚àá√ó A) ¬∑ dS =
)
‚àÇS
A ¬∑ dx
(12.102)
in accord (exercise 12.13) with the curl formulas (12.78‚Äì12.79).
Example 12.16 (How electric motors and generators work)
Since by (11.80) the
curl of the vector potential A is the magnetic Ô¨Åeld B, this last identity (12.102)
implies that the magnetic Ô¨Çux  through a surface S is the line integral of the
vector potential A around the edge of the surface
 =

S
B ¬∑ dS =

S
(‚àá√ó A) ¬∑ dS =
)
‚àÇS
A ¬∑ dx.
(12.103)
If we take the time derivative of this relation and remember (11.78) that the time
derivative of the vector potential is ÀôA = ‚àíE ‚àí‚àáœÜ, then we Ô¨Ånd that the rate of
change of the magnetic Ô¨Çux through a surface is the negative of the line integral
of the electric Ô¨Åeld along the boundary of the surface
493

FORMS
Àô =

S
ÀôB ¬∑ dS =

S
(‚àá√ó ÀôA) ¬∑ dS = ‚àí
)
‚àÇS
E ¬∑ dx
(12.104)
or minus the voltage (‚àí‚àáœÜ drops out because its curl vanishes).
Example 12.17 (Stokes‚Äôs theorem)
Suppose Œæ, Œ∑, and Œ∂ form a triplet of
inÔ¨Ånitesimal vectors oriented so as to form a right-handed coordinate system,
Œæ √ó Œ∑ ¬∑ Œ∂ > 0. These vectors form a tiny parallelepiped  . We want to integrate
the 2-form œâ = ajkdxj ‚àßdxk over the surface ‚àÇ of this tiny parallelepiped  .
We Ô¨Ånd

‚àÇ 
œâ =
 1
0
dt
 1
0
ds

ajk(tŒæ + sŒ∑ + Œ∂) ‚àíajk(tŒæ + sŒ∑

dxj ‚àßdxk(Œæ, Œ∑)
+
 1
0
dt
 1
0
ds

ajk(tŒ∑ + sŒ∂ + Œæ) ‚àíajk(tŒ∑ + sŒ∂

dxj ‚àßdxk(Œ∑, Œ∂)
+
 1
0
dt
 1
0
ds

ajk(tŒ∂ + sŒæ + Œ∑) ‚àíajk(tŒ∂ + sŒæ

dxj ‚àßdxk(Œ∂, Œæ)
= ajk,‚ÑìŒ∂‚Ñì

ŒæjŒ∑k ‚àíŒ∑jŒæk

+ ajk,‚ÑìŒæ‚Ñì

Œ∑jŒ∂k ‚àíŒ∂jŒ∑k

+ ajk,‚ÑìŒ∑‚Ñì

Œ∂jŒæk ‚àíŒæjŒ∂k

= ajk,‚Ñìdx‚Ñì‚àßdxj ‚àßdxk(Œæ, Œ∑, Œ∂) = dœâ(Œæ, Œ∑, Œ∂) =

 
dœâ
(12.105)
for an inÔ¨Ånitesimal parallelepiped  .
It is easy to extend this identity to an arbitrary volume V of Ô¨Ånite extent. To do
this, we tile the volume V with inÔ¨Ånitesimal parallelepipeds  Œ± with boundaries
‚àÇ Œ±. The integral over the Ô¨Ånite volume V is then the sum of the integrals over
the  Œ±

V
dœâ =

Œ±

 Œ±
dœâ,
(12.106)
which by (12.105) is a sum of the integrals over the boundaries ‚àÇ Œ±

V
dœâ =

Œ±

 Œ±
dœâ =

Œ±

‚àÇ 
œâ.
(12.107)
In this sum over surface integrals, the internal boundaries all cancel, leaving us
with the integral over the boundary ‚àÇV of the volume V, so that

V
dœâ =

Œ±

 Œ±
dœâ =

Œ±

‚àÇ Œ±
œâ =

‚àÇV
œâ.
(12.108)
Thus we have

V
dœâ =

‚àÇV
œâ,
(12.109)
which generalizes the identity (12.91) from 1-forms to 2-forms.
Before leaving this example, it may be instructive to examine the value of the
integral of œâ on one of the faces of the inÔ¨Ånitesimal parallelepiped  as well as
that of the integral of dœâ on the inÔ¨Ånitesimal parallelepiped  . The 2-form œâ on
494

12.4 INTEGRATION OF FORMS
the upper Œæ, Œ∑ face of the surface ‚àÇ of  may be seen from equation (12.105)
to be
œâ(Œæ, Œ∑) = ajk dxj ‚àßdxk(Œæ, Œ∑) = ajk

ŒæjŒ∑k ‚àíŒ∑jŒæk

.
(12.110)
The wedge dxj ‚àßdxk(Œæ, Œ∑) deÔ¨Ånes an area vector S = Œæ √ó Œ∑ with components
Si = œµijkŒæjŒ∑k. In terms of S, the 2-form œâ is
œâ(Œæ, Œ∑) = (a23 ‚àía32) S1 + (a31 ‚àía13) S2 + (a12 ‚àía21) S3,
(12.111)
which suggests deÔ¨Åning the vector Ô¨Åeld Ak = œµkijaij. In terms of S and A, the
2-form œâ on Œæ, Œ∑ is œâ(Œæ, Œ∑) = A ¬∑ S.
The integral of the 3-form dœâ on the inÔ¨Ånitesimal parallelepiped  is

 
dœâ = dœâ(Œæ, Œ∑, Œ∂) = ajk,‚Ñìdx‚Ñì‚àßdxj ‚àßdxk(Œæ, Œ∑, Œ∂).
(12.112)
The wedge product is œµ‚Ñìjk times the determinant det(Œæ, Œ∑, Œ∂) of the 3 √ó 3 matrix
that has Œæ as its Ô¨Årst row, Œ∑ as its second, and Œ∂ as its third row
dx‚Ñì‚àßdxj ‚àßdxk(Œæ, Œ∑, Œ∂) = œµ‚Ñìjk det(Œæ, Œ∑, Œ∂).
(12.113)
So dœâ(Œæ, Œ∑, Œ∂) = ajk,‚Ñìœµ‚Ñìjk det(Œæ, Œ∑, Œ∂) or more explicitly
dœâ(Œæ, Œ∑, Œ∂) =

(a23 ‚àía32),1 + (a31 ‚àía13),2 + (a12 ‚àía21),3

det(Œæ, Œ∑, Œ∂),
which we recognize as the divergence of the vector Ô¨Åeld Ak = œµijkaij
dœâ(Œæ, Œ∑, Œ∂) = ‚àá¬∑ A det(Œæ, Œ∑, Œ∂).
(12.114)
Thus we have rediscovered the vector identity

V
‚àá¬∑ A dV =
)
‚àÇV
A ¬∑ dS.
(12.115)
Example 12.18 (Gauss‚Äôs law)
The divergence of the electric displacement D is
the density œÅf of free charge, ‚àá¬∑ D = œÅf, and so this last identity (12.115) gives
QfV =

V
œÅf dV =

V
‚àá¬∑ D dV =
)
‚àÇV
D ¬∑ dS,
(12.116)
which is the integral form of Gauss‚Äôs law.
One may generalize these examples to what has been called the Newton‚Äì
Leibniz‚ÄìGauss‚ÄìGreen‚ÄìOstrogradskii‚ÄìStokes‚ÄìPoincar√© theorem

‚àÇC
œâ =

C
dœâ,
(12.117)
in which œâ is a k-form and C is any (k + 1)-chain on a manifold.
Example 12.19 (Poincar√©‚Äôs invariant action)
In example 12.9, we saw that
Poincar√©‚Äôs action
A =

‚àÇS
œâ1 =
)
p ¬∑ dq =
)
n

i=1
pi ‚àßdqi
(12.118)
495

FORMS
does not change with time, ÀôA = 0. In example 12.11, we learned that the element
of area dœâ1 and therefore its surface integral
I =

S
dœâ1
(12.119)
does not change with time. The identity (12.117) in the form
A =

‚àÇS
œâ1 =

S
dœâ1 = I
(12.120)
relates these two examples.
12.5 Are closed forms exact?
A form œâ is said to be closed if its exterior derivative vanishes
dœâ = 0.
(12.121)
A form œâ is exact if it‚Äôs the exterior derivative of another form œà
œâ = dœà.
(12.122)
We have seen in (12.64) that the exterior derivative of any exterior derivative
vanishes; in effect, dd = 0. Thus the exterior derivative of any exact form œâ
must be zero
dœâ = ddœà = 0.
(12.123)
So every exact form is closed.
But are closed forms exact? Poincar√©‚Äôs lemma provides the answer. A form
that is deÔ¨Åned and closed on a simply connected part of a manifold is exact there.
More technically, if a form œâ is deÔ¨Åned and closed on a region U of a manifold
M and if U can be mapped by a one-to-one differentiable map onto the interior
of the unit ball in Rn, then there is a form œà such that dœà = œâ on U. The unit
ball in Rn is the interior of the sphere Sn‚àí1 deÔ¨Åned by x2
1 + ¬∑ ¬∑ ¬∑ + x2
n = 1. You
may Ô¨Ånd a proof of this result in section 4.19 of Schutz‚Äôs book (Schutz, 1980).
Example 12.20 (Two dimensions)
Suppose that the 1-form œâ = fdx + gdy is
closed
dœâ = f,y dy ‚àßdx + g,x dx ‚àßdy =

g,x ‚àíf,y

dx ‚àßdy = 0
(12.124)
on the real plane R2. Since R2 is simply connected, Poincar√©‚Äôs lemma tells us that
there exists a 0-form h whose exterior derivative is œâ
œâ = fdx + gdy = dh = h,xdx + h,ydy.
(12.125)
496

12.5 ARE CLOSED FORMS EXACT?
We may construct such a function h(x, y) as the line integral
h(x, y) =
 1
0
[f (u(t), v(t)) Àôu(t) + g(u(t), v(t)) Àôv(t)] dt
(12.126)
along any differentiable curve Œ≥ (t) = (u(t), v(t)) that goes from (u(0), v(0)) =
(x0, y0) to (u(1), v(1)) = (x, y). Clearly the exterior derivative of this 0-form is
dh = h,xdx + h,ydy = f (x, y) dx + g(x, y) dy = œâ.
(12.127)
So the real issue here is whether the line integral (12.126)
h =

Œ≥
œâ
(12.128)
deÔ¨Ånes a function h(x, y) that is the same for any two curves Œ≥1(t) and Œ≥2(t) that
both go from (x0, y0) to (x, y). The difference h1(x, y) ‚àíh2(x, y) is an integral of
œâ along a closed curve  = Œ≥1 ‚àíŒ≥2 that runs from (x0, y0) to (x, y) along Œ≥1(t)
and then from (x, y) to (x0, y0) backwards along Œ≥2(t). The closed curve  is the
boundary ‚àÇS of the (plane) surface S that it encloses. Thus by Stokes‚Äôs theorem
(12.117)
h1 ‚àíh2 =


œâ =

‚àÇS
œâ =

S
dœâ = 0,
(12.129)
in which we used the fact that œâ is closed so that dœâ = 0. The two curves Œ≥1(t)
and Œ≥2(t) deÔ¨Åne the same function h(x, y), and œâ = dh is exact.
What if the region in which dœâ = 0 is not simply connected? Consider for
example the 1-form
œâ = ‚àí
y
x2 + y2 dx +
x
x2 + y2 dy,
(12.130)
which is well deÔ¨Åned except at the origin. The plane R2 minus the origin is not
simply connected. One may check that œâ is closed
dœâ = 0
(12.131)
except at the origin. But œâ is not exact. In fact, by writing it as
œâ = arctan(y/x),y dy ‚àßdx + arctan(y/x),x dx ‚àßdy
(12.132)
we see that œâ is almost exact. It is the exterior derivative of
Œ∏ = arctan(y/x),
(12.133)
which would be a 0-form if it were single valued.
Example 12.21 (Some exact forms)
It‚Äôs easy to make lots of exact forms; one
just applies the exterior derivative d to any form. For instance, taking the exterior
derivative of the 0-form œâ0 = x y2 exp(zw), we get the 1-form
dœâ0 = y2 ezw dx + 2x y ezw dy + x y2 w ezw dz + x y2 z ezw dw,
(12.134)
497

FORMS
which is exact (and closed). Applying d to the 1-form œâ1 = y2zdx + x3dy, we get
the exact 2-form dœâ1 = (3x2 ‚àí2yz) dx‚àßdy‚àíy2dx‚àßdz. Incidentally, any n-form
in n variables, such as f (x, y, z) dx ‚àßdy ‚àßdz, is closed.
12.6 Complex differential forms
Any function f (x, y) of two real variables also is a function of the two complex
variables z = x + iy and ¬Øz = x ‚àíiy. For instance, 4xy = ‚àíi(z + ¬Øz)(z ‚àí¬Øz) and
x2 +y2 = z¬Øz. We can write any 1-form œâ = adx+bdy with complex coefÔ¨Åcients
a and b in terms of the complex differentials dz = dx+idy and d¬Øz = dx‚àíidy as
œâ = 1
2(a ‚àíib)dz + 1
2(a + ib) d¬Øz.
(12.135)
A 1-form of the variables z1, . . . , zn and ¬Øz1, . . . , ¬Øzn is a sum of their differentials
œâ = ajdzj + bjd¬Øzj. The expression
œâ1,1 = a dz1 ‚àßd¬Øz1 + b dz1 ‚àßd¬Øz2 + c dz2 ‚àßd¬Øz1 + d dz2 ‚àßd¬Øz2
(12.136)
is a 1,1-form in z1 and z2, while 2,0- and 0,2-forms look like
œâ2,0 = e dz1 ‚àßdz2
and
œâ0,2 = f d¬Øz1 ‚àßd¬Øz2.
(12.137)
The complex differentials anticommute: d¬Øzj ‚àßdzk = ‚àídzk ‚àßd¬Øzj as well as dzj ‚àß
dzk = ‚àídzk ‚àßdzj and d¬Øzj ‚àßd¬Øzk = ‚àíd¬Øzk ‚àßd¬Øzj.
There are two exterior derivatives ‚àÇand ¬Ø‚àÇdeÔ¨Åned by
‚àÇ=
n

j=1
‚àÇ
‚àÇzj
dzj ‚àß
and
¬Ø‚àÇ=
n

j=1
‚àÇ
‚àÇ¬Øzj
d¬Øzj ‚àß.
(12.138)
Their sum is the ordinary exterior derivative ‚àÇ+ ¬Ø‚àÇ= d, and one has
‚àÇ2 = ¬Ø‚àÇ2 = ‚àÇ¬Ø‚àÇ+ ¬Ø‚àÇ‚àÇ= 0.
(12.139)
Example 12.22 (‚àÇ+ ¬Ø‚àÇ= d)
We illustrate the rule ‚àÇ+ ¬Ø‚àÇ= d for the 1-form
œâ = z¬Øzdz = (x2 + y2)(dx + idy). The sum ‚àÇ+ ¬Ø‚àÇacting on œâ gives

‚àÇ+ ¬Ø‚àÇ

z¬Øz dz = ¬Ø‚àÇz¬Øz d¬Øz ‚àßdz = z d¬Øz ‚àßdz
= (x + iy)(dx ‚àíidy) ‚àß(dx + idy) = 2i(x + iy)dx ‚àßdy
(12.140)
while dœâ is
d(x2 + y2)(dx + idy) = 2xidx ‚àßdy + 2ydy ‚àßdx = 2i(x + iy)dx ‚àßdy, (12.141)
which is the same as

‚àÇ+ ¬Ø‚àÇ

œâ.
498

12.7 FROBENIUS‚ÄôS THEOREM
12.7 Frobenius‚Äôs theorem
This section, optional on a Ô¨Årst reading, begins with some deÔ¨Ånitions.
‚Ä¢ If œâ is a k-form that maps all vectors V of the tangent space TP at the point P
into numbers œâ(V1, . . . , Vk), and S ‚äÇTP is a subspace of the tangent space
TP, then the restriction œâS of the form œâ to S maps the vectors Si ‚ààS into
the numbers œâ(S1, . . . , Sk).
‚Ä¢ The annihilator (actually the annihilated) of a set of forms Œ≤i at a point P
of a manifold is the subspace of vectors XP that every Œ≤i maps to zero. (The
vectors XP are in the tangent space TP.)
‚Ä¢ The complete ideal of a set B of forms Œ≤i is the set of all forms at P
whose restriction to B‚Äôs annihilator XP vanishes. Note that for any form
Œ± and any vectors X‚Ñìin the annihilator XP, the wedge product Œ± ‚àß
Œ≤i(X1, . . . , Xn) vanishes, so Œ± ‚àßŒ≤i is in the complete ideal of the set B of
forms Œ≤i.
‚Ä¢ The complete ideal of a set B of forms Œ≤i has a set Œ±i of linearly independent
1-forms that generates it, that is, whose complete ideal is the same as that of
the set B.
‚Ä¢ The complete ideal of a set B of Ô¨Åelds Œ≤i is the set of Ô¨Åelds that map the
annihilator XP of B to zero at each point P of the manifold.
‚Ä¢ If the exterior derivative dŒ±i is in an ideal whenever Œ±i is, then the ideal is a
differential ideal.
‚Ä¢ A set A of 1-forms Œ±i has a closed ideal if every dai is in the complete ideal
generated by the Œ±is. (Some authors call such a set A of 1-forms closed, but
this terminology can be confusing.)
Example 12.23 (A rank-2 annihilator)
Consider a manifold with coordi-
nates x1, . . . , xn, tangent vectors ‚àÇ1, . . . , ‚àÇn, and 1-forms dx1, . . . , dxn with
dxi(‚àÇk) = Œ¥ik as in (12.31). The subspace {c1‚àÇ1 + c2‚àÇ2 | c1, c2 real} is the anni-
hilator of the set {dx3, . . . , dxn} of 1-forms. Any linear combination of these
1-forms
œâ =
n

k=3
ak(x) dxk
(12.142)
is in the complete ideal of the 1-forms dx3, . . . , dxn. So is any 2-form
œâ2 =
n

i,k=3
aik(x) dxi ‚àßdxk.
(12.143)
499

FORMS
For any forms Œ±k the linear combination
œâ =
n

k=3
Œ±k ‚àßdxk
(12.144)
is in the complete ideal of the set of 1-forms dx1, . . . , dxn. In fact, each member
of this complete ideal is such a linear combination.
Frobenius‚Äôs theorem
Let œâ1, . . . , œân be a linearly independent set of 1-form
Ô¨Åelds in an open region U of a k-dimensional manifold M. Then there exist
functions P‚Ñìj and Qj for i, j = 1, . . . , n that express the n 1-forms œâ‚Ñìas
œâ‚Ñì=
n

j=1
P‚ÑìjdQj
(12.145)
if and only if every dœâ‚Ñìis in the complete ideal generated by the œâ‚Ñì‚Äôs (Schutz,
1980, secs. 3.8 & 4.26).
Further reading
Three good discussions of differential forms are Mathematical Methods of
Classical Mechanics (Arnold, 1989), Geometrical Methods of Mathematical
Physics (Schutz, 1980), and Classical Mechanics (Matzner and Shepley, 1991).
They inspired this chapter.
Exercises
12.1
Why do you think mathematicians use the deÔ¨Ånition (12.18) rather than our
(12.19)?
12.2
Show explicitly that the 2-form œâ = Xdx2 ‚àßdx3 is given by (12.43) in terms
of the 1-forms dyk.
12.3
Show explicitly that for the two 3-vectors of example 12.4, the 2-form
œâ(A, B) = Xdx2 ‚àßdx3(A, B) = ‚àíX.
12.4
In example 12.5, let x1 = y1 + y2, x2 = y1 ‚àíy2, and x3 = y1 ‚àíy3. Show
explicitly that the y-version (12.43) of the 2-form œâ(A, B) = Xdx2 ‚àßdx3
maps the two 3-vectors of example 12.4 into the same real number ‚àíX as its
x-version. Hint: in the y-version, Ô¨Årst express dyk(A) and dyk(B) in terms of
dxj(A) and dxj(B).
12.5
Compute dr(ÀÜer), dŒ∏(ÀÜeŒ∏), and dœÜ(ÀÜeœÜ) as well as the six off-diagonal ones
dr(ÀÜeŒ∏), dr(ÀÜeœÜ), and so forth.
12.6
Show that the 2-form (12.52) applied to the vectors U and V gives the triple
scalar product (12.48).
12.7
Show that d d œâk = 0 for the general k-form (12.62).
12.8
Show that if Œ± and Œ≤ are both 2-forms, then d(Œ± ‚àßŒ≤) = (dŒ±) ‚àßŒ≤ + Œ± ‚àßdŒ≤.
500

EXERCISES
12.9
Use Hamilton‚Äôs equations (12.72) to derive the formula (12.75) for the time
derivatives Œ¥Àôpi and Œ¥Àôqi.
12.10 Use Hamilton‚Äôs equations (12.72) to compute the time derivatives of the
n pairs of tiny displacements pj, qj. Then use your resulting formulas
and those (12.75) for the time derivatives of the n pairs of small differences
Œ¥pj, Œ¥qj to show that the time derivative of the sum (12.76) of areas of tiny
parallelograms vanishes (12.77).
12.11 In the early days of quantum mechanics, Bohr and Sommerfeld set action
integrals like those of example 12.9 equal to a multiple of Planck‚Äôs constant,
A =
6
p ¬∑ dq = nh. Why do you think they chose invariant quantities to
quantize in this way?
12.12 Use Bohr‚Äôs quantization of angular momentum L = rp = n¬Øh to Ô¨Ånd the
energy levels of an electron in a circular orbit about a proton. Take the
energy as E = p2/2m ‚àíZe2/4œÄœµ0r and balance radial forces mv2/r =
Ze2/4œÄœµ0r2 where p = mv.
12.13 Work out the details of using the curl formulas (12.78‚Äì12.79) to derive
the curl formula (12.102) from the general identity (12.101). Do this in
rectangular, cylindrical, and spherical coordinates.
12.14 Is the 1-form y2 ezw dx + 2x y ezw dy + x y2 w ezw dz + x y2 z ezw dw closed?
Why? Why not?
12.15 Is the 1-form zey/xw + zey ln x/w + ey ln x/w ‚àízey ln x/w2 closed? Why?
Why not?
12.16 Show that ‚àÇand ¬Ø‚àÇsatisfy (12.139).
501

13
Probability and statistics
13.1 Probability and Thomas Bayes
The probability P(A) of an outcome in a set A is the sum of the probabilities Pj
of all the different (mutually exclusive) outcomes j in A
P(A) =

j‚ààA
Pj.
(13.1)
For instance, if one throws two fair dice, then the probability that the sum is 2 is
P(1, 1) = 1/36, while the probability that the sum is 3 is P(1, 2)+P(2, 1) = 1/18.
If A and B are two sets of possible outcomes, then the probability of an out-
come in the union A ‚à™B is the sum of the probabilities P(A) and P(B) minus
that of their intersection A ‚à©B
P(A ‚à™B) = P(A) + P(B) ‚àíP(A ‚à©B).
(13.2)
If the outcomes are mutually exclusive, then P(A ‚à©B) = 0, and the probability
of the union is the sum P(A‚à™B) = P(A)+P(B). The joint probability P(A, B) ‚â°
P(A ‚à©B) is the probability of an outcome that is in both sets A and B. If the
joint probability is the product P(A, B) = P(A) P(B), then the outcomes in sets
A and B are statistically independent.
The probability that a result in set B also is in set A is the conditional
probability P(A|B), the probability of A given B
P(A|B) = P(A ‚à©B)
P(B)
.
(13.3)
502

13.1 PROBABILITY AND THOMAS BAYES
Also P(B|A) = P(A ‚à©B)/P(A). The substitution B ‚ÜíB ‚à©C in (13.3) gives
P(A|B, C) = P(A ‚à©B ‚à©C)/P(B ‚à©C). If we multiply (13.3) by P(B), we get
P(A, B) = P(A ‚à©B) = P(B|A) P(A) = P(A|B) P(B).
(13.4)
Combination of (13.3 & 13.4) gives Bayes‚Äôs theorem (Riley et al., 2006, p. 1132)
P(A|B) = P(B|A) P(A)
P(B)
(13.5)
(Thomas Bayes, 1702‚Äì1761).
If there are N mutually exclusive theories, causes, or ways Aj that B can
happen, then we must sum over them
P(B) =
N

j=1
P(B|Aj) P(Aj).
(13.6)
The probabilities P(Aj) are called a priori probabilities. In this case, Bayes‚Äôs
theorem is (Roe, 2001, p. 119)
P(Ak|B) =
P(B|Ak) P(Ak)
N
j=1 P(B|Aj) P(Aj)
.
(13.7)
If there are several Bs, then a third form of Bayes‚Äôs theorem is
P(Ak|B‚Ñì) =
P(B‚Ñì|Ak) P(Ak)
N
j=1 P(B‚Ñì|Aj) P(Aj)
.
(13.8)
Example 13.1 (The low-base-rate problem)
Suppose the incidence of a rare
disease in a population is P(D) = 0.001. Suppose a test for the disease has
a sensitivity of 99%, that is, the probability that a carrier will test positive is
P(+|D) = 0.99. Suppose the test also is highly selective with a false-positive
rate of only P(+|N) = 0.005. Then the probability that a random person in the
population would test positive is by (13.6)
P(+) = P(+|D) P(D) + P(+|N) P(N) = 0.005993.
(13.9)
And by Bayes‚Äôs theorem (13.5), the probability that a person who tests positive
actually has the disease is only
P(D|+) = P(+|D) P(D)
P(+)
= 0.99 √ó 0.001
0.005993
= 0.165
(13.10)
and the probability that a person testing positive actually is healthy is P(N|+) =
1 ‚àíP(D|+) = 0.835.
Even with an excellent test, screening for rare diseases is problematic. Sim-
ilarly, screening for rare behaviors, such as drug use in the CIA or disloyalty
503

PROBABILITY AND STATISTICS
in the army, is difÔ¨Åcult with a good test and absurd with a poor one like a
polygraph.
Example 13.2 (The three-door problem)
A prize lies behind one of three closed
doors. A contestant gets to pick which door to open, but before the chosen door
is opened, a door that does not lead to the prize and was not picked by the
contestant swings open. Should the contestant switch and choose a different
door?
We note that a contestant who picks the wrong door and switches always
wins, so P(W|Sw, WD) = 1, while one who picks the right door and switches
never does P(W|Sw, RD) = 0. Since the probability of picking the wrong door
is P(WD) = 2/3, the probability of winning if one switches is
P(W|Sw) = P(W|Sw, WD) P(WD) + P(W|Sw, RD) P(RD) = 2/3.
(13.11)
The probability of picking the right door is P(RD) = 1/3, and the probability
of winning if one picks the right door and stays put is P(W|Sp, RD) = 1. So the
probability of winning if one stays put is
P(W|Sp) = P(W|Sp, RD) P(RD) + P(W|Sp, WD) P(WD) = 1/3.
(13.12)
Thus, one should switch after the door opens.
If the set A is the interval (x ‚àídx/2, x + dx/2) of the real line, then P(A) =
P(x) dx, and the second version of Bayes‚Äôs theorem (13.7) says
P(x|B) =
P(B|x) P(x)
2 ‚àû
‚àí‚àûP(B|x‚Ä≤) P(x‚Ä≤) dx‚Ä≤ .
(13.13)
Example 13.3 (A tiny poll)
We ask four people if they will vote for Nancy
Pelosi, and three say yes. If the probability that a random voter will vote for her
is y, then the probability that three in our sample of four will is
P(3|y) = 4 y3 (1 ‚àíy).
(13.14)
We don‚Äôt know the prior probability distribution P(y), so we set it equal to unity
on the interval (0, 1). Then the continuous form of Bayes‚Äôs theorem (13.13) and
our cheap poll give the probability distribution of the fraction y who will vote
for her as
P(y|3) =
P(3|y) P(y)
2 1
0 P(3|y‚Ä≤) P(y‚Ä≤) dy‚Ä≤ =
P(3|y)
2 1
0 P(3|y‚Ä≤) dy‚Ä≤
=
4 y3 (1 ‚àíy)
2 1
0 4 y‚Ä≤3 (1 ‚àíy‚Ä≤) dy‚Ä≤ = 20 y3 (1 ‚àíy).
(13.15)
504

13.2 MEAN AND VARIANCE
Our best guess then for the probability that she will win the election is
 1
1/2
P(y|3) dy =
 1
1/2
20 y3 (1 ‚àíy) dy = 13
16,
(13.16)
which is slightly higher that the naive estimate of 3/4.
13.2 Mean and variance
In roulette and many other games, N outcomes xj can occur with probabilities
Pj that sum to unity
N

j=1
Pj = 1.
(13.17)
The expected value E[x] of the outcome x is its mean Œº or average value ‚ü®x‚ü©= x
E[x] = Œº = ‚ü®x‚ü©= x =
N

j=1
xj Pj.
(13.18)
The expected value E[x] also is called the expectation of x or expectation value
of x.
The ‚Ñìth moment is
E[x‚Ñì] = Œº‚Ñì= ‚ü®x‚Ñì‚ü©=
N

j=1
x‚Ñì
j Pj
(13.19)
and the ‚Ñìth central moment is
E[(x ‚àíŒº)‚Ñì] = ŒΩ‚Ñì=
N

j=1
(xj ‚àíŒº)‚ÑìPj
(13.20)
where always Œº0 = ŒΩ0 = 1 and ŒΩ1 = 0 (exercise 13.2).
The variance V[x] is the second central moment
V[x] ‚â°E[(x ‚àí‚ü®x‚ü©)2] =
N

j=1

xj ‚àí‚ü®x‚ü©
2 Pj,
(13.21)
which one may write as (exercise 13.4)
V[x] = ‚ü®x2‚ü©‚àí‚ü®x‚ü©2
(13.22)
and the standard deviation œÉ is its square-root
œÉ =

V[x].
(13.23)
505

PROBABILITY AND STATISTICS
If the values of x are distributed continuously according to a probability
distribution or density P(x) normalized to unity

P(x) dx = 1
(13.24)
then the mean value is
E[x] = Œº = ‚ü®x‚ü©=

x P(x) dx
(13.25)
and the ‚Ñìth moment is
E[x‚Ñì] = Œº‚Ñì= ‚ü®x‚Ñì‚ü©=

x‚ÑìP(x) dx.
(13.26)
The ‚Ñìth central moment is
E[(x ‚àíŒº)‚Ñì] = ŒΩ‚Ñì=

(x ‚àíŒº)‚ÑìP(x) dx.
(13.27)
The variance of the distribution is the second central moment
V[x] = ŒΩ2 =

(x ‚àí‚ü®x‚ü©)2 P(x) dx = Œº2 ‚àíŒº2
(13.28)
and the standard deviation œÉ is its square-root œÉ = ‚àöV[x].
Many authors use f (x) for the probability distribution P(x) and F(x) for the
cumulative probability Pr(‚àí‚àû, x) of an outcome in the interval (‚àí‚àû, x)
F(x) ‚â°Pr(‚àí‚àû, x) =
 x
‚àí‚àû
P(x‚Ä≤) dx‚Ä≤ =
 x
‚àí‚àû
f (x‚Ä≤) dx‚Ä≤,
(13.29)
a function that is necessarily monotonic
F‚Ä≤(x) = Pr‚Ä≤(‚àí‚àû, x) = f (x) = P(x) ‚â•0.
(13.30)
Some mathematicians reserve the term probability distribution for probabilities
like Pr(‚àí‚àû, x) and Pj and call a continuous distribution P(x) a probabil-
ity density function. But usage of the Maxwell‚ÄìBoltzmann distribution is too
widespread in physics for me to observe this distinction.
Although a probability distribution P(x) is normalized (13.24), it can have
fat tails, which are important in Ô¨Ånancial applications (Bouchaud and Potters,
2003). Fat tails can make the variance and even the mean absolute deviation
Eabs ‚â°

|x ‚àíŒº| P(x) dx
(13.31)
diverge.
Example 13.4 (Heisenberg‚Äôs uncertainty principle)
In quantum mechanics, the
absolute-value squared |œà(x)|2 of a wave-function œà(x) is the probability dis-
tribution P(x) = |œà(x)|2 of the position x of the particle, and P(x) dx is the
probability that the particle is found between x‚àídx/2 and x+dx/2. The variance
506

13.2 MEAN AND VARIANCE
‚ü®(x ‚àí‚ü®x‚ü©)2‚ü©of the position operator x is written as the square (x)2 of the stan-
dard deviation œÉ = x, which is the uncertainty in the position of the particle.
Similarly, the square of the uncertainty in the momentum (p)2 is the variance
‚ü®(p ‚àí‚ü®p‚ü©)2‚ü©of the momentum.
For the wave-function (3.70)
œà(x) =
 2
œÄ
1/4 1
‚àöa e‚àí(x/a)2
(13.32)
these uncertainties are x = a/2 and p = ¬Øh/a. They provide a (saturated)
example x p = ¬Øh/2 of Heisenberg‚Äôs uncertainty principle
x p ‚â•¬Øh
2.
(13.33)
If x and y are two random variables that occur with a joint distribution
P(x, y), then the expected value of the linear combination axnym + bxpyq is
E[axnym + bxpyq] =

(axnym + bxpyq) P(x, y) dxdy
= a

xnym P(x, y) dxdy + b

xpyq P(x, y) dxdy
= a E[xnym] + b E[xpyq].
(13.34)
This result and its analog for discrete probability distributions show that
expected values are linear.
The correlation coefÔ¨Åcient or covariance of two variables x and y that occur
with a joint distribution P(x, y) is
C[x, y] ‚â°

P(x, y)(x‚àíx)(y‚àíy) dxdy = ‚ü®(x‚àíx)(y‚àíy)‚ü©= ‚ü®x y‚ü©‚àí‚ü®x‚ü©‚ü®y‚ü©. (13.35)
The variables x and y are said to be independent if
P(x, y) = P(x) P(y).
(13.36)
Independence implies that the covariance vanishes, but C[x, y] = 0 does not
guarantee that x and y are independent (Roe, 2001, p. 9).
The variance of x + y
‚ü®(x + y)2‚ü©‚àí‚ü®x + y‚ü©2 = ‚ü®x2‚ü©‚àí‚ü®x‚ü©2 + ‚ü®y2‚ü©‚àí‚ü®y‚ü©2 + 2 (‚ü®x y‚ü©‚àí‚ü®x‚ü©‚ü®y‚ü©) (13.37)
is the sum
V[x + y] = V[x] + V[y] + 2 C[x, y].
(13.38)
It follows (exercise 13.6) that for any constants a and b the variance of ax+by is
V[ax + by] = a2 V[x] + b2 V[y] + 2 ab C[x, y].
(13.39)
507

PROBABILITY AND STATISTICS
More generally (exercise 13.7), the variance of the sum a1x1 + a2x2 + ¬∑ ¬∑ ¬∑ +
aNxN is
V[a1x1 + ¬∑ ¬∑ ¬∑ + aNxN] =
N

j=1
a2
j V[xj] +
N

j,k=1,j<k
2ajakC[xj, xk].
(13.40)
If the variables xj and xk are independent for j Ã∏= k, then their covariances
vanish C[xj, xk] = 0, and the variance of the sum a1x1 + ¬∑ ¬∑ ¬∑ + aNxN is
V[a1x1 + ¬∑ ¬∑ ¬∑ + aNxN] =
N

j=1
a2
j V[xj].
(13.41)
13.3 The binomial distribution
If the probability of success is p on each try, then we expect that in N tries the
mean number of successes will be
‚ü®n‚ü©= N p.
(13.42)
The probability of failure on each try is q = 1 ‚àíp. So the probability of a par-
ticular sequence of successes and failures, such as n successes followed by N ‚àín
failures is pn qN‚àín. There are N!/n! (N ‚àín)! different sequences of n successes
and N ‚àín failures, all with the same probability pn qN‚àín. So the probability of
n successes (and N ‚àín failures) in N tries is
PB(n, p, N) =
N!
n! (N ‚àín)! pn qN‚àín =
N
n

pn (1 ‚àíp)N‚àín.
(13.43)
This binomial distribution also is called Bernoulli‚Äôs distribution (Jacob Bernoulli,
1654‚Äì1705).
The sum of the probabilities PB(n, p, N) for all possible values of n is unity
N

n=0
PB(n, p, N) = (p + 1 ‚àíp)N = 1.
(13.44)
In Fig. 13.1, the probabilities PB(n, p, N) for 0 ‚â§n ‚â§250 and p = 0.2 are
plotted for N = 125, 250, 500, and 1000 tries.
The mean number of successes
Œº = ‚ü®n‚ü©B =
N

n=0
n PB(n, p, N) =
N

n=0
n
N
n

pnqN‚àín
(13.45)
is a partial derivative with respect to p with q held Ô¨Åxed
508

13.3 THE BINOMIAL DISTRIBUTION
0
50
100
150
200
250
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
Some binomial distributions
n
PB(n, p, N)
Figure 13.1
If the probability of success on any try is p, then the probability
PB(n, p, N) of n successes in N tries is given by equation (13.43). For p = 0.2, this
binomial probability distribution PB(n, p, N) is plotted against n for N = 125 (solid),
250 (dashes), 500 (dot dash), and 1000 tries (dots).
‚ü®n‚ü©B = p ‚àÇ
‚àÇp
N

n=0
N
n

pnqN‚àín
= p ‚àÇ
‚àÇp (p + q)N = Np (p + q)N = Np,
(13.46)
which veriÔ¨Åes the estimate (13.42).
One may show (exercise 13.9) that the variance (13.21) of the binomial
distribution is
VB = ‚ü®(n ‚àí‚ü®n‚ü©)2‚ü©= p (1 ‚àíp) N.
(13.47)
Its standard deviation (13.23) is
œÉB =

VB =

p (1 ‚àíp) N.
(13.48)
The ratio of the width to the mean
œÉB
‚ü®n‚ü©B
=

p (1 ‚àíp) N
Np
=
(
1 ‚àíp
Np
(13.49)
decreases with N as 1/
‚àö
N.
509

PROBABILITY AND STATISTICS
Example 13.5 (Avogadro‚Äôs number)
A mole of gas is Avogadro‚Äôs number NA =
6 √ó 1023 of molecules. If the gas is in a cubical box, then the chance that each
molecule will be in the left half of the cube is p = 1/2. The mean number of
molecules there is ‚ü®n‚ü©B = pNA = 3 √ó 1023, and the uncertainty in n is œÉB =

p (1 ‚àíp) N =

3 √ó 1023/4 = 3 √ó 1011. So the numbers of gas molecules in
the two halves of the box are equal to within œÉB/‚ü®n‚ü©B = 10‚àí12 or to 1 part
in 1012.
Because N! increases very rapidly with N, the rule
PB(n + 1, p, N) =
p
1 ‚àíp
N ‚àín
n + 1 PB(n, p, N)
(13.50)
is helpful when N is big. But when N exceeds a few hundred, the formula (13.43)
for PB(n, p, N) becomes unmanageable even in quadruple precision. One way of
computing PB(n, p, N) for large N is to use Srinivasa Ramanujan‚Äôs correction to
Stirling‚Äôs formula N! ‚âà
‚àö
2œÄN(N/e)N
N! ‚âà
‚àö
2œÄN
N
e
N 
1 + 1
2N +
1
8N2
1/6
.
(13.51)
When N and N ‚àín, but not n, are big, one may use (13.51) for N! and (N ‚àín)!
in the formula (13.43) for PB(n, p, N) and so may show (exercise 13.11) that
PB(n, p, N) ‚âà(pN)n
n!
qN‚àín R2(n, N),
(13.52)
in which
R2(n, N) =

1 ‚àín
N
n‚àíN‚àí1/2 
1 + 1
2N +
1
8N2
1/6
√ó
#
1 +
1
2(N ‚àín) +
1
8(N ‚àín)2
$‚àí1/6
(13.53)
tends to unity as N ‚Üí‚àûfor any Ô¨Åxed n.
When all three factorials in PB(n, p, N) are huge, one may use Ramanujan‚Äôs
approximation (13.51) to show (exercise 13.12) that
PB(n, p, N) ‚âà
(
N
2œÄn(N ‚àín)
pN
n
n  qN
N ‚àín
N‚àín
R3(n, N)
(13.54)
510

13.4 THE POISSON DISTRIBUTION
where
R3(n, N) =

1 + 1
2n +
1
8n2
‚àí1/6 
1 + 1
2N +
1
8N2
1/6
√ó
#
1 +
1
2(N ‚àín) +
1
8(N ‚àín)2
$‚àí1/6
(13.55)
tends to unity as N ‚Üí‚àû, N ‚àín ‚Üí‚àû, and n ‚Üí‚àû.
Another way of coping with the unwieldy factorials in the binomial formula
PB(n, p, N) is to use limiting forms of (13.43) due to Poisson and to Gauss.
13.4 The Poisson distribution
Poisson took the two limits N ‚Üí‚àûand p = ‚ü®n‚ü©/N ‚Üí0. So we let N and
N ‚àín, but not n, tend to inÔ¨Ånity, and use (13.52) for the binomial distribution
(13.43). Since R2(n, N) ‚Üí1 as N ‚Üí‚àû, we get
PB(n, p, N) ‚âà(pN)n
n!
qN‚àín = ‚ü®n‚ü©n
n! qN‚àín.
(13.56)
Now q = 1 ‚àíp = 1 ‚àí‚ü®n‚ü©/N, and so for any Ô¨Åxed n we have
lim
N‚Üí‚àûqN‚àín = lim
N‚Üí‚àû

1 ‚àí‚ü®n‚ü©
N
N 
1 ‚àí‚ü®n‚ü©
N
‚àín
= e‚àí‚ü®n‚ü©.
(13.57)
Thus as N ‚Üí‚àûwith pN Ô¨Åxed at ‚ü®n‚ü©, the binomial distribution becomes the
Poisson distribution
PP(n, ‚ü®n‚ü©) = ‚ü®n‚ü©n
n! e‚àí‚ü®n‚ü©.
(13.58)
(Sim√©on-Denis Poisson, 1781‚Äì1840. Incidentally, poisson means Ô¨Åsh and sounds
like pwahsahn.)
The Poisson distribution is normalized to unity
‚àû

n=0
PP(n, ‚ü®n‚ü©) =
‚àû

n=0
‚ü®n‚ü©n
n! e‚àí‚ü®n‚ü©= e‚ü®n‚ü©e‚àí‚ü®n‚ü©= 1.
(13.59)
Its mean Œº is the parameter ‚ü®n‚ü©= pN of the binomial distribution
Œº =
‚àû

n=0
n PP(n, ‚ü®n‚ü©) =
‚àû

n=1
n ‚ü®n‚ü©n
n! e‚àí‚ü®n‚ü©= ‚ü®n‚ü©
‚àû

n=1
‚ü®n‚ü©(n‚àí1)
(n ‚àí1)! e‚àí‚ü®n‚ü©
= ‚ü®n‚ü©
‚àû

n=0
‚ü®n‚ü©n
n! e‚àí‚ü®n‚ü©= ‚ü®n‚ü©.
(13.60)
511

PROBABILITY AND STATISTICS
As N ‚Üí‚àûand p ‚Üí0 with p N = ‚ü®n‚ü©Ô¨Åxed, the variance (13.47) of the
binomial distribution tends to the limit
VP = lim
N‚Üí‚àû
p‚Üí0
VB = lim
N‚Üí‚àû
p‚Üí0
p (1 ‚àíp) N = ‚ü®n‚ü©.
(13.61)
Thus the mean and the variance of a Poisson distribution are equal
VP = ‚ü®(n ‚àí‚ü®n‚ü©)2‚ü©= ‚ü®n‚ü©= Œº
(13.62)
as one may show directly (exercise 13.13).
Example 13.6 (Coherent states)
The coherent state |Œ±‚ü©introduced in equation
(2.138)
|Œ±‚ü©= e‚àí|Œ±|2/2eŒ±a‚Ä†|0‚ü©= e‚àí|Œ±|2/2
‚àû

n=0
Œ±n
‚àö
n!
|n‚ü©
(13.63)
is an eigenstate a|Œ±‚ü©= Œ±|Œ±‚ü©of the annihilation operator a with eigenvalue Œ±.
The probability P(n) of Ô¨Ånding n quanta in the state |Œ±‚ü©is the square of the
absolute value of the inner product ‚ü®n|Œ±‚ü©
P(n) = |‚ü®n|Œ±‚ü©|2 = |Œ±|2n
n!
e‚àí|Œ±|2,
(13.64)
which is a Poisson distribution P(n) = PP(n, |Œ±|2) with mean and variance Œº =
‚ü®n‚ü©= V(Œ±) = |Œ±|2.
13.5 The Gaussian distribution
Gauss considered the binomial distribution in the limit N ‚Üí‚àûwith the
probability p Ô¨Åxed. In this limit, the binomial probability
PB(n, p, N) =
N!
n! (N ‚àín)! pn qN‚àín
(13.65)
is very tiny unless n is near pN, which means that n ‚âàpN and N ‚àín ‚âà(1 ‚àí
p)N = qN are comparable. So the limit N ‚Üí‚àûeffectively is one in which n
and N ‚àín also tend to inÔ¨Ånity. The approximation (13.54)
PB(n, p, N) ‚âà
(
N
2œÄn(N ‚àín)
pN
n
n  qN
N ‚àín
N‚àín
R3(n, N)
(13.66)
applies in which R3(n, N) ‚Üí1 as N, N ‚àín, and n all increase without limit.
Because the probability PB(n, p, N) is negligible unless n ‚âàpN, we set y =
n‚àípN and treat y/n as small. Since n = pN+y and N‚àín = (1‚àíp)N+pN‚àín =
qN ‚àíy, we may write the square-root as
512

13.5 THE GAUSSIAN DISTRIBUTION
(
N
2œÄ n (N ‚àín) =
1

2œÄ N (pN + y)/N (qN ‚àíy)/N
=
1

2œÄ pqN (1 + y/pN) (1 ‚àíy/qN)
.
(13.67)
Since y remains Ô¨Ånite as N ‚Üí‚àû, we get in this limit
lim
N‚Üí‚àû
(
N
2œÄ n (N ‚àín) =
1

2œÄ pqN
.
(13.68)
Substituting pN + y for n and qN ‚àíy for N ‚àín in (13.66), we Ô¨Ånd
PB(n, p, N) ‚âà
1

2œÄ pqN

pN
pN + y
pN+y 
qN
qN ‚àíy
qN‚àíy
=
1

2œÄ pqN

1 + y
pN
‚àí(pN+y) 
1 ‚àíy
qN
‚àí(qN‚àíy)
,
(13.69)
which implies
ln

PB(n, p, N)

2œÄ pqN

‚âà‚àí(pN + y) ln
#
1 + y
pN
$
‚àí(qN ‚àíy) ln
#
1 ‚àíy
qN
$
.
(13.70)
The Ô¨Årst two terms of the power series (4.88) for ln(1 + œµ) are
ln(1 + œµ) ‚âàœµ ‚àí1
2œµ2.
(13.71)
So, using this expansion for ln(1 + y/pN) and also for ln(1 ‚àíy/qN), we get
ln

PB(n, p, N)

2œÄ pqN

‚âà‚àí(pN + y)

y
pN ‚àí1
2
 y
pN
2
‚àí(qN ‚àíy)

‚àíy
qN ‚àí1
2
 y
qN
2
‚âà‚àí
y2
2pqN .
(13.72)
Gauss‚Äôs approximation to the binomial probability distribution thus is
PBG(n, p, N) =
1

2œÄpqN
exp

‚àí(n ‚àípN)2
2pqN

,
(13.73)
in which we‚Äôve replaced y by n ‚àípN and 1 ‚àíp by q.
513

PROBABILITY AND STATISTICS
Extending the integer n to a continuous variable x, we have
PG(x, p, N) =
1

2œÄpqN
exp

‚àí(x ‚àípN)2
2pqN

,
(13.74)
which is (exercise 13.14) a normalized probability distribution with mean ‚ü®x‚ü©=
Œº = pN and variance ‚ü®(x ‚àíŒº)2‚ü©= œÉ 2 = pqN. Replacing pN by Œº and pqN by
œÉ 2, we get the standard form of Gauss‚Äôs distribution
PG(x, Œº, œÉ) =
1
œÉ
‚àö
2œÄ
exp

‚àí(x ‚àíŒº)2
2œÉ 2

.
(13.75)
This distribution occurs so often in mathematics and in Nature that it is often
called the normal distribution. Its odd central moments all vanish ŒΩ2n+1 = 0,
and its even ones are ŒΩ2n = (2n ‚àí1)!! œÉ 2n (exercise 13.16).
Example 13.7 (Single-molecule super-resolution microscopy)
If the wave-
length of visible light were a nanometer, microscopes would yield much sharper
images. Each photon from a (single-molecule) Ô¨Çuorophore entering the lens of a
microscope would follow ray optics and be focused within a tiny circle of about
a nanometer on a detector. Instead, a photon arrives not at x = (x1, x2) but at
yi = (y1i, y2i) with gaussian probability
P(yi) =
1
2œÄœÉ 2 e‚àí(yi‚àíx)2/2œÉ 2
(13.76)
where œÉ ‚âà150 nm is about a quarter of a wave-length. What to do?
In the centroid method, one collects N ‚âà500 points yi and Ô¨Ånds the point x
that maximizes the joint probability of the N image points
P =
N

i=1
P(yi) = dN
N

i=1
e‚àí(yi‚àíx)2/(2œÉ 2) = dN exp

‚àí
N

i=1
(yi ‚àíx)2/(2œÉ 2)

(13.77)
where d = 1/2œÄœÉ 2, by solving for k = 1 and 2 the equations
‚àÇP
‚àÇxk
= 0 = P ‚àÇP
‚àÇxk

‚àí
N

i=1
(yi ‚àíx)2/(2œÉ 2)

= P
œÉ 2
N

i=1
(yik ‚àíxk) .
(13.78)
This maximum-likelihood estimate of the image point x is the average of the
observed points yi
x = 1
N
N

i=1
yi.
(13.79)
This method is an improvement, but it is biased by auto-Ô¨Çuorescence and
out-of-focus Ô¨Çuorophores. Fang Huang and Keith Lidke use direct stochastic
514

13.6 THE ERROR FUNCTION ERF
Actin fibers in HELA cells
Figure 13.2
Conventional (left, fuzzy) and dSTORM (right, sharp) images of actin
Ô¨Åbers in HELA cells. The actin is labeled with Alexa Fluor 647 Phalloidin. The
white rectangles are 5 microns in length. Images courtesy of Fang Huang and Keith
Lidke.
optical reconstruction microscopy (dSTORM) to locate the image point x of
the Ô¨Çuorophore in ways that account for the Ô¨Ånite accuracy of their pixilated
detector and the randomness of photo-detection.
Actin Ô¨Ålaments are double helices of the protein actin some 5‚Äì9 nm wide. They
occur throughout a eukaryotic cell but are concentrated near its surface and
determine its shape. Together with tubulin and intermediate Ô¨Ålaments, they form
a cell‚Äôs cytoskeleton. Figure 13.2 shows conventional (left, fuzzy) and dSTORM
(right, sharp) images of actin Ô¨Ålaments. The Ô¨Ånite size of the Ô¨Çuorophore and
the motion of the molecules of living cells limit dSTORM‚Äôs improvement in
resolution to a factor of 10 to 20.
13.6 The error function erf
The probability that a random variable x distributed according to Gauss‚Äôs
distribution (13.75) has a value between Œº ‚àíŒ¥ and Œº + Œ¥ is
P(|x ‚àíŒº| < Œ¥) =
 Œº+Œ¥
Œº‚àíŒ¥
PG(x, Œº, œÉ) dx =
1
œÉ
‚àö
2œÄ
 Œº+Œ¥
Œº‚àíŒ¥
exp

‚àí(x ‚àíŒº)2
2œÉ 2

dx
=
1
œÉ
‚àö
2œÄ
 Œ¥
‚àíŒ¥
exp

‚àíx2
2œÉ 2

dx =
2
‚àöœÄ
 Œ¥/œÉ
‚àö
2
0
e‚àít2 dt.
(13.80)
The last integral is the error function
515

PROBABILITY AND STATISTICS
erf (x) =
2
‚àöœÄ
 x
0
e‚àít2dt,
(13.81)
so in terms of it the probability that x lies within Œ¥ of the mean Œº is
P(|x ‚àíŒº| < Œ¥) = erf

Œ¥
œÉ
‚àö
2

.
(13.82)
In particular, the probabilities that x falls within one, two, or three standard
deviations of Œº are
P(|x ‚àíŒº| < œÉ) = erf (1/
‚àö
2) = 0.6827,
P(|x ‚àíŒº| < 2œÉ) = erf (2/
‚àö
2) = 0.9545,
P(|x ‚àíŒº| < 3œÉ) = erf (3/
‚àö
2) = 0.9973.
(13.83)
The error function erf (x) is plotted in Fig. 13.3 in which the vertical lines are at
x = Œ¥/(œÉ
‚àö
2) for Œ¥ = œÉ, 2œÉ, and 3œÉ.
The probability that x falls between a and b is (exercise 13.17)
P(a < x < b) = 1
2
#
erf
b ‚àíŒº
œÉ
‚àö
2

‚àíerf
a ‚àíŒº
œÉ
‚àö
2
$
.
(13.84)
0
0.5
1
1.5
2
2.5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
The error function
x
erf(x)
1œÉ
2œÉ
3œÉ
Figure 13.3
The error function erf (x) is plotted for 0 < x < 2.5. The vertical lines
are at x = Œ¥/(œÉ
‚àö
2) for Œ¥ = œÉ, 2œÉ, and 3œÉ with œÉ = 1/
‚àö
2.
516

13.6 THE ERROR FUNCTION ERF
In particular, the cumulative probability P(‚àí‚àû, x) that the random variable is
less than x is for Œº = 0 and œÉ = 1
P(‚àí‚àû, x) = 1
2
#
erf
 x
‚àö
2

‚àíerf
‚àí‚àû
‚àö
2
$
= 1
2
#
erf
 x
‚àö
2

+ 1
$
.
(13.85)
The complement erfc of the error function is deÔ¨Åned as
erfc (x) =
2
‚àöœÄ
 ‚àû
x
e‚àít2dt = 1 ‚àíerf (x)
(13.86)
and is numerically useful for large x where round-off errors may occur in sub-
tracting erf(x) from unity. Both erf and erfc are intrinsic functions in FORTRAN
available without any effort on the part of the programmer.
Example 13.8 (Summing binomial probabilities)
To add up several binomial
probabilities when the factorials in PB(n, p, N) are too big to handle, we Ô¨Årst use
Gauss‚Äôs approximation (13.73)
PB(n, p, N) =
N!
n! (N ‚àín)! pn qN‚àín ‚âà
1

2œÄpqN
exp

‚àí(n ‚àípN)2
2pqN

.
(13.87)
Then, using (13.84) with Œº = pN, we Ô¨Ånd (exercise 13.15)
PB(n, p, N) ‚âà1
2

erf

n + 1
2 ‚àípN

2pqN

‚àíerf

n ‚àí1
2 ‚àípN

2pqN

,
(13.88)
which we can sum over the integer n to get
n2

n=n1
PB(n, p, N) ‚âà1
2

erf

n2 + 1
2 ‚àípN

2pqN

‚àíerf

n1 ‚àí1
2 ‚àípN

2pqN

,
(13.89)
which is easy to evaluate.
Example 13.9 (Polls)
Suppose in a poll of 1000 likely voters, 600 have said they
would vote for Nancy Pelosi. Repeating the analysis of example 13.3, we see that
if the probability that a random voter will vote for her is y, then the probability
that 600 in our sample of 1000 will is by (13.87)
P(600|y) = PB(600, y) =
1000
600

y600 (1 ‚àíy)400
‚âà
1
10

20œÄy(1 ‚àíy)
exp

‚àí20(3 ‚àí5y)2
y(1 ‚àíy)

(13.90)
517

PROBABILITY AND STATISTICS
and so the probability density that a fraction y of the voters will vote for her is
P(y|600) =
P(600|y)
2 1
0 P(600, y‚Ä≤) dy‚Ä≤
=
[y(1 ‚àíy)]‚àí1/2 exp

‚àí20(3‚àí5y)2
y(1‚àíy)

2 1
0 [y‚Ä≤(1 ‚àíy‚Ä≤)]‚àí1/2 exp

‚àí20(3‚àí5y‚Ä≤)2
y‚Ä≤(1‚àíy‚Ä≤)

dy‚Ä≤ .
(13.91)
This normalized probability distribution is negligible except for y near 3/5
(exercise 13.18), where it is approximately Gauss‚Äôs distribution
P(y|600) ‚âà
1
œÉ
‚àö
2œÄ
exp

‚àí(y ‚àí3/5)2
2œÉ 2

(13.92)
with mean Œº = 3/5 and variance
œÉ 2 =
3
12500 = 2.4 √ó 10‚àí4.
(13.93)
The probability that y > 1/2 then is by (13.84)
P(a < x < b) = 1
2
#
erf
1 ‚àíŒº
œÉ
‚àö
2

‚àíerf
1/2 ‚àíŒº
œÉ
‚àö
2
$
= 1
2
#
erf
 20
‚àö
1.2

‚àíerf
 ‚àí5
‚àö
1.2
$
‚âà1.
(13.94)
The probability that y < 1/2 is 5.4 √ó 10‚àí11.
13.7 The Maxwell‚ÄìBoltzmann distribution
It is a small jump from Gauss‚Äôs distribution (13.75) to the Maxwell‚ÄìBoltzmann
distribution of velocities of molecules in a gas. We start in one dimension and
focus on a single molecule that is being hit fore and aft with equal probabilities
by other molecules. If each hit increases or decreases its speed by dv, then after
n aft hits and N ‚àín fore hits, the speed vx of a molecule initially at rest would
be
vx = ndv ‚àí(N ‚àín)dv = (2n ‚àíN)dv.
(13.95)
The probability of this speed is given by Gauss‚Äôs approximation (13.73) to the
binomial distribution PB(n, 1
2, N) as
PBG(n, 1
2, N) =
&
2
œÄN exp

‚àí(2n ‚àíN)2
2N

=
&
2
œÄN exp

‚àí
v2
x
2Ndv2

.
(13.96)
This argument applies to any physical variable subject to unbiased random
Ô¨Çuctuations. It is why Gauss‚Äôs distribution describes statistical errors and why
it occurs so often in Nature as to be called the normal distribution.
518

13.8 DIFFUSION
We now write the argument of the exponential in terms of the temperature T
and Boltzmann‚Äôs constant k by setting N = kT/(m dv2) so that
‚àí
1
2v2
x
Ndv2 = ‚àí
1
2mv2
x
mNdv2 = ‚àí
1
2mv2
x
kT .
(13.97)
Then with dvx = 2dv, we have
PG(vx)dvx =
&
2m
œÄkT dv exp

‚àí
1
2mv2
x
kT

=
&
m
2œÄkT dvx exp

‚àí
1
2mv2
x
kT

.
(13.98)
Gauss‚Äôs distribution is normalized to unity because it is the limit of the
binomial distribution (13.44)
 ‚àû
‚àí‚àû
&
m
2œÄkT exp

‚àí
1
2mv2
x
kT

dvx = 1
(13.99)
as you may verify by explicit integration.
In three space dimensions, the Maxwell‚ÄìBoltzmann distribution PMB(v) is
the product
PMB(v)d3v = PG(vx) PG(vy) PG(vz)d3v =

m
2œÄkT
3/2
e‚àí1
2mv2/(kT)4œÄv2dv.
(13.100)
The mean value of the velocity of a Maxwell‚ÄìBoltzmann gas vanishes
‚ü®v‚ü©=

v PMB(v)d3v = 0
(13.101)
but the mean value of the square of the velocity v2 = v ¬∑ v is the sum of the
three variances œÉ 2
x = œÉ 2
y = œÉ 2
z = kT/m
‚ü®v2‚ü©= V[v2] =

v2 PMB(v) d3v = 3kT/m
(13.102)
which is the familiar statement
1
2m‚ü®v2‚ü©= 3
2 kT
(13.103)
that each degree of freedom gets kT/2 of energy.
13.8 Diffusion
We may apply the same reasoning as in the preceding section (13.7) to the dif-
fusion of a gas of particles treated as a random walk with step size dx. In one
dimension, after n steps forward and N ‚àín steps backward, a particle starting
at x = 0 is at x = (2n ‚àíN)dx. Thus, as in (13.96), the probability of being
519

PROBABILITY AND STATISTICS
at x is given by Gauss‚Äôs approximation (13.73) to the binomial distribution
PB(n, 1
2, N) as
PBG(n, 1
2, N) =
&
2
œÄN exp

‚àí(2n ‚àíN)2
2N

=
&
2
œÄN exp

‚àí
x2
2Ndx2

.
(13.104)
In terms of the diffusion constant
D = Ndx2
2t
(13.105)
this distribution is
PG(x) =

1
4œÄDt
1/2
exp

‚àíx2
4Dt

(13.106)
when normalized to unity on (‚àí‚àû, ‚àû).
In three dimensions, this gaussian distribution is the product
P(r, t) = PG(x) PG(y) PG(z) =

1
4œÄDt
3/2
exp

‚àír2
4Dt

.
(13.107)
The variance œÉ 2 = 2Dt gives the average of the squared displacement of each
of the three coordinates. Thus the mean of the squared displacement ‚ü®r2‚ü©rises
linearly with the time as
‚ü®r2‚ü©= V[r] = 3œÉ 2 =

r2 P(r, t) d3r = 6 D t.
(13.108)
The distribution P(r, t) satisÔ¨Åes the diffusion equation
ÀôP(r, t) = D ‚àá2P(r, t),
(13.109)
in which the dot means time derivative.
13.9 Langevin‚Äôs theory of brownian motion
Einstein made the Ô¨Årst theory of brownian motion in 1905, but Langevin‚Äôs
approach (Langevin, 1908) is simpler. A tiny particle of colloidal size and mass
m in a Ô¨Çuid is buffeted by a force F(t) due to the 1021 collisions per second it
suffers with the molecules of the surrounding Ô¨Çuid. Its equation of motion is
m dv(t)
dt
= F(t).
(13.110)
Langevin suggested that the force F(t) is the sum of a viscous drag ‚àív(t)/B and
a rapidly Ô¨Çuctuating part f (t)
F(t) = ‚àív(t)/B + f (t)
(13.111)
520

13.9 LANGEVIN‚ÄôS THEORY OF BROWNIAN MOTION
so that
m dv(t)
dt
= ‚àív(t)
B + f (t).
(13.112)
The parameter B is called the mobility. The ensemble average (the average over
the set of particles) of the Ô¨Çuctuating force f (t) is zero
‚ü®f (t)‚ü©= 0.
(13.113)
Thus the ensemble average of the velocity satisÔ¨Åes
m d‚ü®v‚ü©
dt
= ‚àí‚ü®v‚ü©
B
(13.114)
whose solution with œÑ = mB is
‚ü®v(t)‚ü©= ‚ü®v(0)‚ü©e‚àít/œÑ.
(13.115)
The instantaneous equation (13.112) divided by the mass m is
dv(t)
dt
= ‚àív(t)
œÑ
+ a(t),
(13.116)
in which a(t) = f (t)/m is the acceleration. The ensemble average of the scalar
product of the position vector r with this equation is
.
r ¬∑ dv
dt
/
= ‚àí‚ü®r ¬∑ v‚ü©
œÑ
+ ‚ü®r ¬∑ a‚ü©.
(13.117)
But since the ensemble average ‚ü®r ¬∑ a‚ü©of the scalar product of the position
vector r with the random, Ô¨Çuctuating part a of the acceleration vanishes, we
have
.
r ¬∑ dv
dt
/
= ‚àí‚ü®r ¬∑ v‚ü©
œÑ
.
(13.118)
Now
1
2
d r2
dt = 1
2
d
dt (r ¬∑ r) = r ¬∑ v
(13.119)
and so
1
2
d2r2
dt2 = r ¬∑ dv
dt + v2.
(13.120)
The ensemble average of this equation gives us
d2‚ü®r2‚ü©
dt2
= 2
.
r ¬∑ dv
dt
/
+ 2‚ü®v2‚ü©
(13.121)
or in view of (13.118)
d2‚ü®r2‚ü©
dt2
= ‚àí2‚ü®r ¬∑ v‚ü©
œÑ
+ 2‚ü®v2‚ü©.
(13.122)
521

PROBABILITY AND STATISTICS
We now use (13.119) to replace ‚ü®r ¬∑ v‚ü©with half the Ô¨Årst time derivative of ‚ü®r2‚ü©
so that we have
d2‚ü®r2‚ü©
dt2
= ‚àí1
œÑ
d‚ü®r2‚ü©
dt
+ 2‚ü®v2‚ü©.
(13.123)
If the Ô¨Çuid is in equilibrium, then the ensemble average of v2 is given by the
Maxwell‚ÄìBoltzmann value (13.103)
‚ü®v2‚ü©= 3kT
m
(13.124)
and so the acceleration (13.123) is
d2‚ü®r2‚ü©
dt2
+ 1
œÑ
d‚ü®r2‚ü©
dt
= 6kT
m ,
(13.125)
which we can integrate.
The general solution (6.13) to a second-order linear inhomogeneous differ-
ential equation is the sum of any particular solution to the inhomogeneous
equation plus the general solution of the homogeneous equation. The function
‚ü®r2(t)‚ü©pi = 6kTtœÑ/m is a particular solution of the inhomogeneous equa-
tion. The general solution to the homogeneous equation is ‚ü®r2(t)‚ü©gh = U +
W exp(‚àít/œÑ) where U and W are constants. So ‚ü®r2(t)‚ü©is
‚ü®r2(t)‚ü©= U + W e‚àít/œÑ + 6kTœÑt/m
(13.126)
where U and W make ‚ü®r2(t)‚ü©Ô¨Åt the boundary conditions. If the individual
particles start out at the origin r = 0, then one boundary condition is
‚ü®r2(0)‚ü©= 0,
(13.127)
which implies that
U + W = 0.
(13.128)
And since the particles start out at r = 0 with an isotropic distribution of initial
velocities, the formula (13.119) for Àôr2 implies that at t = 0
d ‚ü®r2‚ü©
dt

t=0
= 2‚ü®r(0) ¬∑ v(0)‚ü©= 0.
(13.129)
This boundary condition means that our solution (13.126) must satisfy
d ‚ü®r2(t)‚ü©
dt

t=0
= ‚àíW
œÑ + 6kTœÑ
m
= 0.
(13.130)
Thus W = ‚àíU = 6kTœÑ 2/m, and so our solution (13.126) is
‚ü®r2(t)‚ü©= 6kTœÑ 2
m
# t
œÑ + e‚àít/œÑ ‚àí1
$
.
(13.131)
522

13.10 THE EINSTEIN‚ÄìNERNST RELATION
At times short compared to œÑ, the Ô¨Årst two terms in the power series for the
exponential exp(‚àít/œÑ) cancel the terms ‚àí1 + t/œÑ, leaving
‚ü®r2(t)‚ü©= 6kTœÑ 2
m

t2
2œÑ 2

= 3kT
m t2 = ‚ü®v2‚ü©t2.
(13.132)
But at times long compared to œÑ, the exponential vanishes, leaving
‚ü®r2(t)‚ü©= 6kTœÑ
m
t = 6 B kT t.
(13.133)
The diffusion constant D is deÔ¨Åned by
‚ü®r2(t)‚ü©= 6 D t
(13.134)
and so we arrive at Einstein‚Äôs relation
D = B kT,
(13.135)
which often is written in terms of the viscous-friction coefÔ¨Åcient Œ∂
Œ∂ ‚â°1
B = m
œÑ
(13.136)
as
Œ∂ D = kT.
(13.137)
This equation expresses Boltzmann‚Äôs constant k in terms of three quantities
Œ∂, D, and T that were accessible to measurement in the Ô¨Årst decade of the
twentieth century. It enabled scientists to measure Boltzmann‚Äôs constant k
for the Ô¨Årst time. And since Avogadro‚Äôs number NA was the known gas con-
stant R divided by k, the number of molecules in a mole was revealed to be
NA = 6.022 √ó 1023. Chemists could then divide the mass of a mole of any
pure substance by 6.022 √ó 1023 and Ô¨Ånd the mass of the molecules that com-
posed it. Suddenly the masses of the molecules of chemistry became known,
and molecules were recognized as real particles and not tricks for balancing
chemical equations.
13.10 The Einstein‚ÄìNernst relation
If a particle of mass m carries an electric charge q and is exposed to an electric
Ô¨Åeld E, then in addition to viscosity ‚àív/B and random buffeting f , the constant
force qE acts on it
m dv
dt = ‚àív
B + qE + f .
(13.138)
The mean value of its velocity will then satisfy the differential equation
.dv
dt
/
= ‚àí‚ü®v‚ü©
œÑ
+ qE
m
(13.139)
523

PROBABILITY AND STATISTICS
where œÑ = mB. A particular solution of this inhomogeneous equation is
‚ü®v(t)‚ü©pi = qœÑE
m
= qBE.
(13.140)
The general solution of its homogeneous version is ‚ü®v(t)‚ü©gh = A exp(‚àít/œÑ) in
which the constant A is chosen to give ‚ü®v(0)‚ü©at t = 0. So by (6.13), the general
solution ‚ü®v(t)‚ü©to equation (13.139) is (exercise 13.42) the sum of ‚ü®v(t)‚ü©pi and
‚ü®v(t)‚ü©gh
‚ü®v(t)‚ü©= qBE + [‚ü®v(0)‚ü©‚àíqBE] e‚àít/œÑ.
(13.141)
By applying the tricks of the previous section (13.9), one may show (exer-
cise 13.43) that the variance of the position r about its mean qBE t is
7
(r ‚àíqBE t)28
= 6kTœÑ 2
m
 t
œÑ ‚àí1 + e‚àít/œÑ

.
(13.142)
So for times t ‚â´œÑ, this variance is
7
(r ‚àíqBE t)28
= 6kTœÑt
m
.
(13.143)
Since the diffusion constant D is deÔ¨Åned by (13.134) as
7
(r ‚àíqBE t)28
= 6 D t
(13.144)
we arrive at the Einstein‚ÄìNernst relation
D = BkT = qB
q kT = Œº
q kT,
(13.145)
in which the electric mobility is Œº = qB.
13.11 Fluctuation and dissipation
Let‚Äôs look again at Langevin‚Äôs equation (13.116) but with u as the independent
variable
dv(u)
du
+ v(u)
œÑ
= a(u).
(13.146)
If we multiply both sides by the exponential exp(u/œÑ)
dv
du + v
œÑ

eu/œÑ = d
du

v eu/œÑ
= a(u) eu/œÑ
(13.147)
and integrate from 0 to t
 t
0
d
du

v eu/œÑ
du = v(t) et/œÑ ‚àív(0) =
 t
0
a(u) eu/œÑ du
(13.148)
524

13.11 FLUCTUATION AND DISSIPATION
then we get
v(t) = e‚àít/œÑ v(0) + e‚àít/œÑ
 t
0
a(u) eu/œÑ du.
(13.149)
Thus the ensemble average of the square of the velocity is
‚ü®v2(t)‚ü©= e‚àí2t/œÑ ‚ü®v2(0)‚ü©+ 2e‚àí2t/œÑ
 t
0
‚ü®a(u)‚ü©eu/œÑ du
+ e‚àí2t/œÑ
 t
0
 t
0
‚ü®a(u1) ¬∑ a(u2)‚ü©e(u1+u2)/œÑ du1du2.
(13.150)
The second term on the RHS is zero, so we have
‚ü®v2(t)‚ü©= e‚àí2t/œÑ ‚ü®v2(0)‚ü©+e‚àí2t/œÑ
 t
0
 t
0
‚ü®a(u1)¬∑a(u2)‚ü©e(u1+u2)/œÑ du1du2. (13.151)
The ensemble average
C(u1, u2) = ‚ü®a(u1) ¬∑ a(u2)‚ü©
(13.152)
is an example of an autocorrelation function.
All autocorrelation functions have some simple properties, which are easy to
prove (Pathria, 1972, p. 458).
1 If the system is independent of time, then its autocorrelation function for any
given variable A(t) depends only upon the time delay s:
C(t, t + s) = ‚ü®A(t) ¬∑ A(t + s)‚ü©‚â°C(s).
(13.153)
2 The autocorrelation function for s = 0 is necessarily nonnegative
C(t, t) = ‚ü®A(t) ¬∑ A(t)‚ü©= ‚ü®A(t)2‚ü©‚â•0.
(13.154)
If the system is time independent, then C(t, t) = C(0) ‚â•0.
3 The absolute value of C(t1, t2) is never greater than the average of C(t1, t1)
and C(t2, t2) because
‚ü®|A(t1) ¬± A(t2)|2‚ü©= ‚ü®A(t1)2‚ü©+ ‚ü®A(t2)2‚ü©¬± 2‚ü®A(t1) ¬∑ A(t2)‚ü©‚â•0,
(13.155)
which implies that
‚àíC(t1, t2) ‚â§1
2 (C(t1, t1) + C(t2, t2)) ‚â•C(t1, t2)
(13.156)
or
|C(t1, t2)| ‚â§1
2 (C(t1, t1) + C(t2, t2)) .
(13.157)
For a time-independent system, this inequality is |C(s)| ‚â§C(0) for every time
delay s.
525

PROBABILITY AND STATISTICS
4 If the variables A(t1) and A(t2) commute, then their autocorrelation function
is symmetric
C(t1, t2) = ‚ü®A(t1) ¬∑ A(t2)‚ü©= ‚ü®A(t2) ¬∑ A(t1)‚ü©= C(t2, t1).
(13.158)
For a time-independent system, this symmetry is C(s) = C(‚àís).
5 If the variable A(t) is randomly Ô¨Çuctuating with zero mean, then we expect
both that its ensemble average vanishes
‚ü®A(t)‚ü©= 0
(13.159)
and that there is some characteristic time scale T beyond which the correla-
tion function falls to zero:
‚ü®A(t1) ¬∑ A(t2)‚ü©‚Üí‚ü®A(t1)‚ü©¬∑ ‚ü®A(t2)‚ü©= 0
(13.160)
when |t1 ‚àít2| ‚â´T.
In terms of the autocorrelation function C(u1, u2) = ‚ü®a(u1) ¬∑ a(u2)‚ü©of the
acceleration, the variance of the velocity (13.152) is
‚ü®v2(t)‚ü©= e‚àí2t/œÑ ‚ü®v2(0)‚ü©+ e‚àí2t/œÑ
 t
0
 t
0
C(u1, u2) e(u1+u2)/œÑ du1du2.
(13.161)
Since C(u1, u2) is big only for tiny values of |u2 ‚àíu1|, it makes sense to change
variables to
s = u2 ‚àíu1
and
w = 1
2(u1 + u2).
(13.162)
The element of area then is by (12.6‚Äì12.14)
du1 ‚àßdu2 = dw ‚àßds
(13.163)
and the limits of integration are ‚àí2w ‚â§s ‚â§2w for 0 ‚â§w ‚â§t/2 and ‚àí2(t‚àíw) ‚â§
s ‚â§2(t ‚àíw) for t/2 ‚â§w ‚â§t. So ‚ü®v2(t)‚ü©is
‚ü®v2(t)‚ü©= e‚àí2t/œÑ ‚ü®v2(0)‚ü©+ e‚àí2t/œÑ
 t/2
0
e2w/œÑdw
 2w
‚àí2w
C(s) ds
+ e‚àí2t/œÑ
 t
t/2
e2w/œÑdw
 2(t‚àíw)
‚àí2(t‚àíw)
C(s) ds.
(13.164)
Since by (13.160) the autocorrelation function C(s) vanishes outside a narrow
window of width 2T, we may approximate each of the s-integrals by
C =
 ‚àû
‚àí‚àû
C(s) ds.
(13.165)
526

13.11 FLUCTUATION AND DISSIPATION
It follows then that
‚ü®v2(t)‚ü©= e‚àí2t/œÑ ‚ü®v2(0)‚ü©+ C e‚àí2t/œÑ
 t
0
e2w/œÑdw
= e‚àí2t/œÑ ‚ü®v2(0)‚ü©+ C e‚àí2t/œÑ œÑ
2

e2t/œÑ ‚àí1

= e‚àí2t/œÑ ‚ü®v2(0)‚ü©+ C œÑ
2

1 ‚àíe‚àí2t/œÑ
.
(13.166)
As t ‚Üí‚àû, ‚ü®v2(t)‚ü©must approach its equilibrium value of 3kT/m, and so
lim
t‚Üí‚àû‚ü®v2(t)‚ü©= C œÑ
2 = 3kT
m ,
(13.167)
which implies that
C = 6kT
mœÑ
or
1
B = m2C
6kT .
(13.168)
Our Ô¨Ånal formula for ‚ü®v2(t)‚ü©then is
‚ü®v2(t)‚ü©= e‚àí2t/œÑ ‚ü®v2(0)‚ü©+ 3kT
m

1 ‚àíe‚àí2t/œÑ
.
(13.169)
Referring back to the deÔ¨Ånition (13.136) of the viscous-friction coefÔ¨Åcient
Œ∂ = 1/B, we see that Œ∂ is related to the integral
Œ∂ = 1
B = m2
6kT C = m2
6kT
 ‚àû
‚àí‚àû
‚ü®a(0)¬∑a(s)‚ü©ds =
1
6kT
 ‚àû
‚àí‚àû
‚ü®f (0)¬∑f (s)‚ü©ds (13.170)
of the autocorrelation function of the random acceleration a(t) or equivalently
of the random force f (t). This equation relates the dissipation of viscous fric-
tion to the random Ô¨Çuctuations. It is an example of a Ô¨Çuctuation‚Äìdissipation
theorem.
If we substitute our formula (13.169) for ‚ü®v2(t)‚ü©into the expression (13.123)
for the acceleration of ‚ü®r2‚ü©, then we get
d2‚ü®r2(t)‚ü©
dt2
= ‚àí1
œÑ
d‚ü®r2(t)‚ü©
dt
+ 2e‚àí2t/œÑ ‚ü®v2(0)‚ü©+ 6kT
m

1 ‚àíe‚àí2t/œÑ
.
(13.171)
The solution with both ‚ü®r2(0)‚ü©= 0 and d‚ü®r2(0)‚ü©/dt = 0 is (exercise 13.21)
‚ü®r2(t)‚ü©= ‚ü®v2(0)‚ü©œÑ 2 
1 ‚àíe‚àít/œÑ2 ‚àí3kT
m œÑ 2 
1 ‚àíe‚àít/œÑ
3 ‚àíe‚àít/œÑ
+ 6kTœÑ
m
t.
(13.172)
527

PROBABILITY AND STATISTICS
13.12 Characteristic and moment-generating functions
The Fourier transform (3.9) of a probability distribution P(x) is its characteris-
tic function ÀÜP(k), sometimes written as œá(k)
ÀÜP(k) ‚â°œá(k) ‚â°E[eikx] =

eikx P(x) dx.
(13.173)
The probability distribution P(x) is the inverse Fourier transform (3.9)
P(x) =

e‚àíikx ÀÜP(k) dk
2œÄ .
(13.174)
Example 13.10 (Gauss)
The characteristic function of the gaussian
PG(x, Œº, œÉ) =
1
œÉ
‚àö
2œÄ
exp

‚àí(x ‚àíŒº)2
2œÉ 2

(13.175)
is by (3.18)
ÀÜPG(k, Œº, œÉ) =
1
œÉ
‚àö
2œÄ

exp

ikx ‚àí(x ‚àíŒº)2
2œÉ 2

dx
=
eikŒº
œÉ
‚àö
2œÄ

exp

ikx ‚àíx2
2œÉ 2

dx = exp

iŒºk ‚àí1
2œÉ 2k2

. (13.176)
For a discrete probability distribution Pn the characteristic function is
œá(k) ‚â°E[eikx] =

n
eikxn Pn.
(13.177)
The normalization of both continuous and discrete probability distributions
implies that their characteristic functions satisfy ÀÜP(0) = œá(0) = 1.
Example 13.11 (Poisson)
The Poisson distribution (13.58)
PP(n, ‚ü®n‚ü©) = ‚ü®n‚ü©n
n! e‚àí‚ü®n‚ü©
(13.178)
has the characteristic function
œá(k) =
‚àû

n=0
eikn ‚ü®n‚ü©n
n! e‚àí‚ü®n‚ü©= e‚àí‚ü®n‚ü©
‚àû

n=0
(‚ü®n‚ü©eik)n
n!
= exp

‚ü®n‚ü©

eik ‚àí1

. (13.179)
528

13.12 CHARACTERISTIC AND MOMENT-GENERATING FUNCTIONS
The moment-generating function is the characteristic function evaluated at an
imaginary argument
M(k) ‚â°E[ekx] = ÀÜP(‚àíik) = œá(‚àíik).
(13.180)
For a continuous probability distribution P(x), it is
M(k) = E[ekx] =

ekx P(x) dx
(13.181)
and for a discrete probability distribution Pn it is
M(k) = E[ekx] =

n
ekxn Pn.
(13.182)
In both cases, the normalization of the probability distribution implies that
M(0) = 1.
Derivatives of the moment-generating function and of the characteristic
function give the moments
E[xn] = Œºn = dnM(k)
dkn

k=0
= (‚àíi)n dn ÀÜP(k)
dkn

k=0
.
(13.183)
Example 13.12 (Gauss and Poisson)
The moment-generating functions for the
distributions of Gauss (13.175) and Poisson (13.178) are
MG(k, Œº, œÉ) = exp

Œºk + 1
2œÉ 2k2

and MP(k, ‚ü®n‚ü©) = exp

‚ü®n‚ü©

ek ‚àí1

.
(13.184)
They give as the Ô¨Årst three moments of these distributions
ŒºG0 = 1,
ŒºG1 = Œº,
ŒºG2 = Œº2 + œÉ 2,
(13.185)
ŒºP0 = 1,
ŒºP1 = ‚ü®n‚ü©,
ŒºP2 = ‚ü®n‚ü©+ ‚ü®n‚ü©2
(13.186)
(exercise 13.22).
Since the characteristic and moment-generating functions have derivatives
(13.183) proportional to the moments Œºn, their Taylor series are
ÀÜP(k) = E[eikx] =
‚àû

n=0
(ik)n
n! E[xn] =
‚àû

n=0
(ik)n
n!
Œºn
(13.187)
and
M(k) = E[ekx] =
‚àû

n=0
kn
n! E[xn] =
‚àû

n=0
kn
n! Œºn.
(13.188)
529

PROBABILITY AND STATISTICS
The cumulants cn of a probability distribution are the derivatives of the
logarithm of its moment-generating function
cn = dn ln M(k)
dkn

t=0
= (‚àíi)n dn ln ÀÜP(k)
dkn

t=0
.
(13.189)
One may show (exercise 13.24) that the Ô¨Årst Ô¨Åve cumulants of an arbitrary
probability distribution are
c0 = 0,
c1 = Œº,
c2 = œÉ 2,
c3 = ŒΩ3,
and
c4 = ŒΩ4 ‚àí3œÉ 4
(13.190)
where the ŒΩs are its central moments (13.27). The third and fourth normalized
cumulants are the skewness Œ∂ = c3/œÉ 3 = ŒΩ3/œÉ 3 and the kurtosis Œ∫ = c4/œÉ 4 =
ŒΩ4/œÉ 4 ‚àí3.
Example 13.13 (Gaussian cumulants)
The logarithm of the moment-
generating function (13.184) of Gauss‚Äôs distribution is Œºk + œÉ 2k2/2. Thus by
(13.189), PG(x, Œº, œÉ) has no skewness or kurtosis, its cumulants vanish cGn = 0
for n > 2, and its fourth central moment is ŒΩ4 = 3œÉ 4.
13.13 Fat tails
The gaussian probability distribution PG(x, Œº, œÉ) falls off for |x ‚àíŒº| ‚â´œÉ very
fast ‚Äì as exp

‚àí(x ‚àíŒº)2/2œÉ 2
. Many other probability distributions fall off
more slowly; they have fat tails. Rare ‚Äúblack-swan‚Äù events ‚Äì wild Ô¨Çuctuations,
market bubbles, and crashes ‚Äì lurk in their fat tails.
Gosset‚Äôs distribution, which is known as Student‚Äôs t-distribution with ŒΩ degrees
of freedom
PS(x, ŒΩ, a) =
1
‚àöœÄ
((1 + ŒΩ)/2)
(ŒΩ/2)
aŒΩ
(a2 + x2)(1+ŒΩ)/2 ,
(13.191)
has power-law tails. Its even moments are
Œº2n = (2n ‚àí1)!! (ŒΩ/2 ‚àín)
(ŒΩ/2)

a2
2
n
(13.192)
for 2n < ŒΩ and inÔ¨Ånite otherwise. For ŒΩ = 1, it coincides with the Breit‚ÄìWigner
or Cauchy distribution
PS(x, 1, a) = 1
œÄ
a
a2 + x2 ,
(13.193)
in which x = E ‚àíE0 and a = /2 is the half-width at half-maximum.
Two representative cumulative probabilities are (Bouchaud and Potters,
2003, p.15‚Äì16)
530

13.13 FAT TAILS
Pr(x, ‚àû) =
 ‚àû
x
PS(x‚Ä≤, 3, 1) dx‚Ä≤ = 1
2 + 1
œÄ
#
arctan x +
x
1 + x2
$
, (13.194)
Pr(x, ‚àû) =
 ‚àû
x
PS(x‚Ä≤, 4,
‚àö
2) dx‚Ä≤ = 1
2 ‚àí3
4u + 1
4u3
(13.195)
where u = x/

2 + x2 and a is picked so œÉ 2 = 1. William Gosset (1876‚Äì1937),
who worked for Guinness, wrote as Student because Guinness didn‚Äôt let its
employees publish.
The log-normal probability distribution on (0, ‚àû)
Pln(x) =
1
œÉx
‚àö
2œÄ
exp

‚àíln2(x/x0)
2œÉ 2

(13.196)
describes distributions of rates of return (Bouchaud and Potters, 2003, p. 9). Its
moments are (exercise 13.27)
Œºn = xn
0 en2œÉ 2/2.
(13.197)
The exponential distribution on [0, ‚àû)
Pe(x) = Œ±e‚àíŒ±x
(13.198)
has (exercise 13.28) mean Œº = 1/Œ± and variance œÉ 2 = 1/Œ±2. The sum of n
independent exponentially and identically distributed random variables x =
x1 + ¬∑ ¬∑ ¬∑ + xn is distributed on [0, ‚àû) as (Feller, 1966, p.10)
Pn,e(x) = Œ± (Œ±x)n‚àí1
(n ‚àí1)!e‚àíŒ±x.
(13.199)
The sum of the squares x2 = x2
1 + ¬∑ ¬∑ ¬∑ + x2
n of n independent normally and
identically distributed random variables of zero mean and variance œÉ 2 gives rise
to Pearson‚Äôs chi-squared distribution on (0, ‚àû)
Pn,G(x, œÉ)dx =
‚àö
2
œÉ
1
(n/2)

x
œÉ
‚àö
2
n‚àí1
e‚àíx2/(2œÉ 2)dx,
(13.200)
which for x = v, n = 3, and œÉ 2 = kT/m is (exercise 13.29) the Maxwell‚Äì
Boltzmann distribution (13.100). In terms of œá = x/œÉ, it is
Pn,G(œá2/2) dœá2 =
1
(n/2)

œá2
2
n/2‚àí1
e‚àíœá2/2d

œá2/2

.
(13.201)
It has mean and variance
Œº = n
and
œÉ 2 = 2n
(13.202)
and is used in the chi-squared test (Pearson, 1900).
531

PROBABILITY AND STATISTICS
Personal income, the amplitudes of catastrophes, the price changes of Ô¨Ånan-
cial assets, and many other phenomena occur on both small and large scales.
L√©vy distributions describe such multi-scale phenomena. The characteristic
function for a symmetric L√©vy distribution is for ŒΩ ‚â§2
ÀÜLŒΩ(k, aŒΩ) = exp

‚àíaŒΩ|k|ŒΩ
.
(13.203)
Its inverse Fourier transform (13.174) is for ŒΩ = 1 (exercise 13.30) the Cauchy
or Lorentz distribution
L1(x, a1) =
a1
œÄ(x2 + a2
1)
(13.204)
and for ŒΩ = 2 the gaussian
L2(x, a2) = PG(x, Œº,

2a2) =
1
œÉ
‚àö
2
exp

‚àíx2
4a2
2

(13.205)
but for other values of ŒΩ no simple expression for LŒΩ(x, aŒΩ) is available. For
0 < ŒΩ < 2 and as x ‚Üí¬±‚àû, it falls off as |x|‚àí(1+ŒΩ), and for ŒΩ > 2 it assumes
negative values, ceasing to be a probability distribution (Bouchaud and Potters,
2003, pp. 10‚Äì13).
13.14 The central limit theorem and Jarl Lindeberg
We have seen in sections 13.7 and 13.8 that unbiased Ô¨Çuctuations tend to dis-
tribute the position and velocity of molecules according to Gauss‚Äôs distribution
(13.75). Gaussian distributions occur very frequently. The central limit theorem
suggests why they occur so often.
Let x1, . . . , xN be N independent random variables described by probability
distributions P1(x1), . . . , PN(xN) with Ô¨Ånite means Œºj and Ô¨Ånite variances œÉ 2
j .
The Pjs may be all different. The central limit theorem says that as N ‚Üí‚àûthe
probability distribution P(N)(y) for the average of the xjs
y = 1
N (x1 + x2 + ¬∑ ¬∑ ¬∑ + xN)
(13.206)
tends to a gaussian in y quite independently of what the underlying probability
distributions Pj(xj) happen to be.
Because expected values are linear (13.34), the mean value of the average y is
the average of the N means
Œºy = E[y] = E[(x1 + ¬∑ ¬∑ ¬∑ + xN) /N] = 1
N (E[x1] + ¬∑ ¬∑ ¬∑ + E[xN])
= 1
N (Œº1 + ¬∑ ¬∑ ¬∑ + ŒºN) .
(13.207)
532

13.14 THE CENTRAL LIMIT THEOREM AND JARL LINDEBERG
Similarly, our rule (13.41) for the variance of a linear combination of indepen-
dent variables tells us that the variance of the average y is
œÉ 2
y = V[(x1 + ¬∑ ¬∑ ¬∑ + xN) /N] = 1
N2

œÉ 2
1 + ¬∑ ¬∑ ¬∑ + œÉ 2
N

.
(13.208)
The independence of the random variables x1, x2, . . . , xN implies (13.36) that
their joint probability distribution factorizes
P(x1, . . . , xN) = P1(x1)P2(x2) ¬∑ ¬∑ ¬∑ PN(xN).
(13.209)
We can use a delta function (3.36) to write the probability distribution P(N)(y)
for the average y = (x1 + x2 + ¬∑ ¬∑ ¬∑ + xN)/N of the xjs as
P(N)(y) =

P(x1, . . . , xN) Œ¥((x1 + x2 + ¬∑ ¬∑ ¬∑ + xN)/N ‚àíy) dNx
(13.210)
where dNx = dx1 . . . dxN. Its characteristic function
ÀÜP(N)(k) =

eiky P(N)(y) dy
=

eiky

P(x1, . . . , xN) Œ¥((x1 + x2 + ¬∑ ¬∑ ¬∑ + xN)/N ‚àíy) dNx dy
=

exp
#ik
N (x1 + x2 + ¬∑ ¬∑ ¬∑ + xN)
$
P(x1, . . . , xN) dNx
=

exp
#ik
N (x1 + x2 + ¬∑ ¬∑ ¬∑ + xN)
$
P1(x1)P2(x2) ¬∑ ¬∑ ¬∑ PN(xN) dNx
(13.211)
is then the product
ÀÜP(N)(k) = ÀÜP1(k/N) ÀÜP2(k/N) ¬∑ ¬∑ ¬∑ ÀÜPN(k/N)
(13.212)
of the characteristic functions
ÀÜPj(k/N) =

eikxj/N Pj(xj) dx
(13.213)
of the probability distributions P1(x1), . . . , PN(xN).
The Taylor series (13.187) for each characteristic function is
ÀÜPj(k/N) =
‚àû

n=0
(ik)n
n! Nn Œºnj
(13.214)
and so for big N we can use the approximation
ÀÜPj(k/N) ‚âà1 + ik
N Œºj ‚àík2
2N2 Œº2j,
(13.215)
533

PROBABILITY AND STATISTICS
in which Œº2j = œÉ 2
j + Œº2
j by the formula (13.22) for the variance. So we have
ÀÜPj(k/N) ‚âà1 + ik
N Œºj ‚àík2
2N2

œÉ 2
j + Œº2
j

(13.216)
or for large N
ÀÜPj(k/N) ‚âàexp

ik
N Œºj ‚àík2
2N2 œÉ 2
j

.
(13.217)
Thus as N ‚Üí‚àû, the characteristic function (13.212) for the variable y con-
verges to
ÀÜP(N)(k) =
N

j=1
ÀÜPj(k/N) =
N

j=1
exp

ik
N Œºj ‚àík2
2N2 œÉ 2
j

= exp
‚é°
‚é£
N

j=1

ik
N Œºj ‚àík2
2N2 œÉ 2
j
‚é§
‚é¶= exp

iŒºyk ‚àí1
2œÉ 2
y k2

, (13.218)
which is the characteristic function (13.176) of a gaussian (13.175) with mean
and variance
Œºy = 1
N
N

j=1
Œºj
and
œÉ 2
y = 1
N2
N

j=1
œÉ 2
j .
(13.219)
The inverse Fourier transform (13.174) now gives the probability distribution
P(N)(y) for the average y = (x1 + x2 + ¬∑ ¬∑ ¬∑ + xN)/N as
P(N)(y) =
 ‚àû
‚àí‚àû
e‚àíiky ÀÜP(N)(k) dk
2œÄ ,
(13.220)
which in view of (13.218) and (13.176) tends as N ‚Üí‚àûto Gauss‚Äôs distribution
PG(y, Œºy, œÉy)
lim
N‚Üí‚àûP(N)(y) =
 ‚àû
‚àí‚àû
e‚àíiky
lim
N‚Üí‚àû
ÀÜP(N)(k) dk
2œÄ
=
 ‚àû
‚àí‚àû
e‚àíiky exp

iŒºyk ‚àí1
2œÉ 2
y k2
 dk
2œÄ
= PG(y, Œºy, œÉy) =
1
œÉy
‚àö
2œÄ
exp

‚àí(y ‚àíŒºy)2
2œÉ 2y

(13.221)
with mean Œºy and variance œÉ 2
y as given by (13.219). The sense in which P(N)(y)
converges to PG(y, Œºy, œÉy) is that for all a and b the probability PrN(a < y < b)
that y lies between a and b as determined by P(N)(y) converges as N ‚Üí‚àû
534

13.14 THE CENTRAL LIMIT THEOREM AND JARL LINDEBERG
to the probability that y lies between a and b as determined by the gaussian
PG(y, Œºy, œÉy)
lim
N‚Üí‚àûPrN(a < y < b) = lim
N‚Üí‚àû
 b
a
P(N)(y) dy =
 b
a
PG(y, Œºy, œÉy) dy. (13.222)
This type of convergence is called convergence in probability (Feller, 1966,
pp. 231, 241‚Äì248).
For the special case in which all the means and variances are the same, with
Œºj = Œº and œÉ 2
j = œÉ 2, the deÔ¨Ånitions in (13.219) imply that Œºy = Œº and œÉ 2
y =
œÉ 2/N. In this case, one may show (exercise 13.32) that in terms of the variable
u ‚â°
‚àö
N(y ‚àíŒº)
œÉ
=
N
n=1 xj

‚àíNŒº
‚àö
N œÉ
(13.223)
P(N)(y) converges to a distribution that is normal
lim
N‚Üí‚àûP(N)(y) dy =
1
‚àö
2œÄ
e‚àíu2/2 du.
(13.224)
To get a clearer idea of when the central limit theorem holds, let us write the
sum of the N variances as
SN ‚â°
N

j=1
œÉ 2
j =
N

j=1
 ‚àû
‚àí‚àû
(xj ‚àíŒºj)2 Pj(xj) dxj
(13.225)
and the part of this sum due to the regions within Œ¥ of the means Œºj as
SN(Œ¥) ‚â°
N

j=1
 Œºj+Œ¥
Œºj‚àíŒ¥
(xj ‚àíŒºj)2 Pj(xj) dxj.
(13.226)
In terms of these deÔ¨Ånitions, Jarl Lindeberg (1876‚Äì1932) showed that P(N)(y)
converges (in probability) to the gaussian (13.221) as long as the part SN(Œ¥) is
most of SN in the sense that for every œµ > 0
lim
N‚Üí‚àû
SN

œµ‚àöSN

SN
= 1.
(13.227)
This is Lindeberg‚Äôs condition (Feller, 1968, p. 254; Feller, 1966, pp. 252‚Äì259;
Gnedenko, 1968, p. 304).
Because we dropped all but the Ô¨Årst three terms of the series (13.214) for
the characteristic functions ÀÜPj(k/N), we may infer that the convergence of the
distribution P(N)(y) to a gaussian is quickest near its mean Œºy. If the higher
moments Œºnj are big, then for Ô¨Ånite N the distribution P(N)(y) can have tails
that are fatter than those of the limiting gaussian PG(y, Œºy, œÉy).
535

PROBABILITY AND STATISTICS
Example 13.14 (Illustration of the central limit theorem)
The simplest proba-
bility distribution is a random number x uniformly distributed on the interval
(0, 1). The probability distribution P(2)(y) of the mean of two such random
numbers is the integral
P(2)(y) =
 1
0
dx1
 1
0
dx2 Œ¥((x1 + x2)/2 ‚àíy).
(13.228)
Letting u1 = x1/2 and u2 = x2/2, we Ô¨Ånd
P(2)(y) = 4
 min(y, 1
2 )
max(0,y‚àí1
2 )
Œ∏( 1
2 +u1‚àíy) du1 = 4y Œ∏( 1
2 ‚àíy)+4(1‚àíy) Œ∏(y‚àí1
2), (13.229)
which is the dot-dashed triangle in Fig. 13.4. The probability distribution P(4)(y)
is the dashed somewhat gaussian curve in the Ô¨Ågure, while P(8)(y) is the solid,
nearly gaussian curve.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.5
1
1.5
2
2.5
3
3.5
4
Central¬†limit theorem for uniform random numbers
y
P (N)(y)
Figure 13.4
The probability distributions P(N)(y) (equation 13.210) for the mean
y = (x1 + ¬∑ ¬∑ ¬∑ + xN)/N of N random variables drawn from the uniform distribution
are plotted for N = 1 (dots), 2 (dot dash), 4 (dashes), and 8 (solid). The distribu-
tions P(N)(y) rapidly approach gaussians with the same mean Œºy = 1/2 but with
shrinking variances œÉ 2 = 1/12N.
536

13.15 RANDOM-NUMBER GENERATORS
To work through a more complicated example of the central limit theorem,
we Ô¨Årst need to learn how to generate random numbers that follow an arbitrary
distribution.
13.15 Random-number generators
To generate truly random numbers, one might use decaying nuclei or an
electronic device that makes white noise. But people usually settle for pseudo-
random numbers computed by a mathematical algorithm. Such algorithms are
deterministic, so the numbers they generate are not truly random. But for most
purposes, they are random enough.
The easiest way to generate pseudo-random numbers is to use a random-
number algorithm that is part of one‚Äôs favorite FORTRAN, C, or C++ compiler.
To run it, one Ô¨Årst gives it a random starting point called a seed, which is a num-
ber or a vector. For instance, to start the GNU or Intel FORTRAN90 compiler,
one includes in the code the line
call random_seed()
before using the line
call random_number(x)
to generate a random number x uniformly distributed on the interval 0 < x < 1,
or an array of such random numbers.
Some applications require random numbers of very high quality. For
such applications, one might use L√ºscher‚Äôs RANLUX (L√ºscher, 1994; James,
1994).
Most random-number generators are periodic with very long periods. The
Mersenne twister (Saito and Matsumoto, 2007) has the exceptionally long
period 219937 ‚àí1 ‚â≥4.3 √ó 106001. Matlab uses it.
Random-number generators distribute random numbers uniformly on the
interval (0, 1). How do we make them follow an arbitrary distribution P(x)? If
the distribution is strictly positive P(x) > 0 on the relevant interval (a, b), then
its integral
F(x) =
 x
a
P(x‚Ä≤) dx‚Ä≤
(13.230)
is a strictly increasing function on (a, b), that is, a < x < y < b implies F(x) <
F(y). Moreover, the function F(x) rises from F(a) = 0 to F(b) = 1 and takes on
every value 0 < y < 1 for exactly one x in the interval (a, b). Thus the inverse
function F‚àí1(y)
x = F‚àí1(y)
if and only if
y = F(x)
(13.231)
537

PROBABILITY AND STATISTICS
is well deÔ¨Åned on the interval (0, 1).
Our random-number generator gives us random numbers u that are uniform
on (0, 1). We want a random variable r whose probability Pr(r < x) of being
less than x is F(x). The trick (Knuth, 1981, p. 116) is to set
r = F‚àí1(u)
(13.232)
so that Pr(r < x) = Pr(F‚àí1(u) < x). For by (13.231) F‚àí1(u) < x if and only if
u < F(x). So Pr(r < x) = Pr(F‚àí1(u) < x) = Pr(u < F(x)) = F(x). The trick
works.
Example 13.15 (P(r) = 3r2)
To turn a distribution of random numbers u uni-
form on (0, 1) into a distribution P(r) = 3r2 of random numbers r, we integrate
and Ô¨Ånd
F(x) =
 x
0
P(x‚Ä≤) dx‚Ä≤ =
 x
0
3x‚Ä≤2 dx‚Ä≤ = x3.
(13.233)
We then set r = F‚àí1(u) = u1/3.
13.16 Illustration of the central limit theorem
To make things simple, we‚Äôll take all the probability distributions Pj(x) to be the
same and equal to Pj(xj) = 3x2
j on the interval (0, 1) and zero elsewhere. Our
random-number generator gives us random numbers u that are uniformly dis-
tributed on (0, 1), so by the example (13.15) the variable r = u1/3 is distributed
as Pj(x) = 3x2.
The central limit theorem tells us that the distribution
P(N)(y) =

3x2
1 3x2
2 . . . 3x2
N Œ¥((x1 + x2 + ¬∑ ¬∑ ¬∑ + xN)/N ‚àíy) dNx
(13.234)
of the mean y = (x1 + ¬∑ ¬∑ ¬∑ + xN)/N tends as N ‚Üí‚àûto Gauss‚Äôs distribution
lim
N‚Üí‚àûP(N)(y) =
1
œÉy
‚àö
2œÄ
exp

‚àí(x ‚àíŒºy)2
2œÉ 2y

(13.235)
with mean Œºy and variance œÉ 2
y given by (13.219). Since the Pjs are all the same,
they all have the same mean
Œºy = Œºj =
 1
0
3x3dx = 3
4
(13.236)
and the same variance
538

13.16 ILLUSTRATION OF THE CENTRAL LIMIT THEOREM
œÉ 2
j =
 1
0
3x4dx ‚àí
3
4
2
= 3
5 ‚àí9
16 = 3
80.
(13.237)
By (13.219), the variance of the mean y is then œÉ 2
y
= 3/80N. Thus as N
increases, the mean y tends to a gaussian with mean Œºy = 3/4 and ever
narrower peaks.
For N = 1, the probability distribution P(1)(y) is
P(1)(y) =

3x2
1 Œ¥(x1 ‚àíy) dx1 = 3y2,
(13.238)
which is the probability distribution we started with. In Fig. 13.5, this is the
quadratic, dotted curve.
For N = 2, the probability distribution P(1)(y) is (exercise 13.31)
P(2)(y) =

3x2
1 3x2
2 Œ¥((x1 + x2)/2 ‚àíy) dx1 dx2
= Œ∏( 1
2 ‚àíy) 96
5 y5 + Œ∏(y ‚àí1
2)
36
5 ‚àí96
5 y5 + 48y2 ‚àí36y

. (13.239)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
1
2
3
4
5
6
Illustration of the central limit theorem
y
P (N)(y)
Figure 13.5
The probability distributions P(N)(y) (equation 13.234) for the mean y =
(x1 + ¬∑ ¬∑ ¬∑ + xN)/N of N random variables drawn from the quadratic distribution P(x) =
3x2 are plotted for N = 1 (dots), 2 (dot dash), 4 (dashes), and 8 (solid). The four
distributions P(N)(y) rapidly approach gaussians with the same mean Œºy = 3/4 but
with shrinking variances œÉ 2y = 3/80N.
539

PROBABILITY AND STATISTICS
The probability distributions P(N)(y) for N = 2j can be obtained by running
the FORTAN95 program
program clt
implicit none ! avoids typos
character(len=1)::ch_i1
integer,parameter::dp = kind(1.d0) !define double
precision
integer::j,k,n,m
integer,dimension(100)::plot = 0
real(dp)::y
real(dp),dimension(100)::rplot
real(dp),allocatable,dimension(:)::r,u
real(dp),parameter::onethird = 1.d0/3.d0
write(6,*)‚ÄôWhat is j?‚Äô; read(5,*) j
allocate(u(2**j),r(2**j))
call init_random_seed() ! set new seed, see below
do k = 1, 10000000
! Make the N = 2**j plot
call random_number(u)
r = u**onethird
y = sum(r)/2**j
n = 100*y + 1
plot(n) = plot(n) + 1
end do
rplot = 100*real(plot)/sum(plot)
write(ch_i1,"(i1)") j ! turns integer j into
character ch_i1
open(7,file=‚Äôplot‚Äô//ch_i1) ! opens and names files
do m = 1, 100
write(7,*) 0.01d0*(m-0.5), rplot(m)
end do
end program clt
subroutine init_random_seed()
implicit none
integer i, n, clock
integer, dimension(:), allocatable :: seed
call random_seed(size = n) ! find size of seed
allocate(seed(n))
call system_clock(count=clock)!get time of processor
clock
seed = clock + 37 * (/ (i-1, i=1, n) /) ! make seed
call random_seed(put=seed) ! set seed
540

13.16 ILLUSTRATION OF THE CENTRAL LIMIT THEOREM
deallocate(seed)
end subroutine init_random_seed
The distributions P(N)(y) for N = 1, 2, 4, and 8 are plotted in Fig. 13.5.
P(1)(y) = 3y2 is the original distribution. P(2)(y) is trying to be a gaussian,
while P(4)(y) and P(8)(y) have almost succeeded. The variance œÉ 2
y = 3/80N
shrinks with N.
Although FORTRAN95 is an ideal language for computation, C++ is more
versatile, more modular, and more suited to large projects involving many
programmers. An equivalent C++ code written by Sean Cahill is:
#include <stdlib.h>
#include <time.h>
#include <math.h>
#include <string>
#include <iostream>
#include <fstream>
#include <sstream>
#include <iomanip>
#include <valarray>
using namespace std;
// Fills the array val with random numbers between
0 and 1 void rand01(valarray<double>& val)
{
// Records the size
unsigned int size = val.size();
// Loops through the size
unsigned int i=0;
for (i=0; i<size; i++)
{
// Generates a random number between 0 and 1
val[i] = static_cast<double>(rand()) / RAND_MAX;
}
}
void clt ()
{
// Declares local constants
const int PLOT_SIZE
= 100;
541

PROBABILITY AND STATISTICS
const int LOOP_CALC_ITR
= 10000000;
const double ONE_THIRD
= 1.0 / 3.0;
// Inits local variables
double y=0;
int i=0, j=0, n=0;
// Gets the value of J
cout << "What is J? ";
cin >> j;
// Bases the vec size on J
const int VEC_SIZE = static_cast<int>(pow(2.0,j));
// Inits vectors
valarray<double> plot(PLOT_SIZE);
valarray<double> rplot(PLOT_SIZE);
valarray<double> r(VEC_SIZE);
// Seeds random number generator
srand ( time(NULL) );
// Performs the calculations
for (i=0; i<LOOP_CALC_ITR; i++)
{
rand01(r);
r = pow(r, ONE_THIRD);
y = r.sum() / VEC_SIZE;
n = static_cast<int>(100 * y);
plot[n]++;
}
// Normalizes RPLOT
rplot = plot * (100.0 / plot.sum());
// Opens a data file
ostringstream fileName;
fileName << "plot_" << j << ".txt";
ofstream fileHandle;
fileHandle.open (fileName.str().c_str());
// Sets precision
542

13.17 MEASUREMENTS, ESTIMATORS, AND FRIEDRICH BESSEL
fileHandle.setf(ios::fixed,ios::floatfield);
fileHandle.precision(7);
// Writes the data to a file
for (i=1; i<=PLOT_SIZE; i++)
fileHandle << 0.01*(i-0.5) << "
" << rplot[i-1]
<< endl;
// Closes the data file
fileHandle.close();
}
13.17 Measurements, estimators, and Friedrich Bessel
A probability distribution P(x; Œ∏) for a stochastic variable x may depend upon
one or more unknown parameters Œ∏ = (Œ∏1, . . . , Œ∏m) such as the mean Œº and the
variance œÉ 2.
Experimenters seek to determine the unknown parameters Œ∏ by collecting
data in the form of values x = x1, . . . , xN of the stochastic variable x. They
assume that the probability distribution for the sequence x = (x1, . . . , xN) is
the product of N factors of the physical distribution P(x; Œ∏)
P(x; Œ∏) =
N

j=1
P(xj; Œ∏).
(13.240)
They approximate the unknown value of a parameter Œ∏‚Ñìas the mean value of
an estimator u(N)
‚Ñì
(x) of Œ∏‚Ñì
E[u(N)
‚Ñì
] =

u(N)
‚Ñì
(x) P(x; Œ∏) dNx = Œ∏‚Ñì+ b(N)
‚Ñì
,
(13.241)
in which the bias b(N)
‚Ñì
depends upon Œ∏ and N. If as N ‚Üí‚àû, the bias b(N)
‚Ñì
‚Üí0,
then the estimator u(N)
‚Ñì
(x) is consistent.
Inasmuch as the mean (13.25) is the integral of the physical distribution
Œº =

x P(x; Œ∏) dx
(13.242)
a natural estimator for the mean is
u(N)
Œº (x) = (x1 + ¬∑ ¬∑ ¬∑ + xN)/N.
(13.243)
Its expected value is
E[u(N)
Œº ] =

u(N)
Œº (x) P(x; Œ∏) dNx =
 x1 + ¬∑ ¬∑ ¬∑ + xN
N
P(x; Œ∏) dNx
543

PROBABILITY AND STATISTICS
= 1
N
N

k=1

xk P(xk; Œ∏) dxk
N

kÃ∏=j=1

P(xj; Œ∏) dxj
= 1
N
N

k=1
Œº = Œº.
(13.244)
Thus the natural estimator u(N)
Œº (x) of the mean (13.243) has b(N)
‚Ñì
= 0, and so it
is a consistent and unbiased estimator for the mean.
Since the variance (13.28) of the probability distribution P(x; Œ∏) is the
integral
œÉ 2 =

(x ‚àíŒº)2 P(x; Œ∏) dx,
(13.245)
that of the estimator uN
Œº is
V[uN
Œº ] =
 
u(N)
Œº (x) ‚àíŒº
2
P(x; Œ∏) dNx =
 ‚é°
‚é£1
N
N

j=1

xj ‚àíŒº

‚é§
‚é¶
2
P(x; Œ∏) dNx
= 1
N2
N

j,k=1
 
xj ‚àíŒº

(xk ‚àíŒº) P(x; Œ∏) dNx
= 1
N2
N

j,k=1
Œ¥jk
 
xj ‚àíŒº
2 P(x; Œ∏) dNx = 1
N2
N

j,k=1
Œ¥jk œÉ 2 = œÉ 2
N , (13.246)
in which œÉ 2 is the variance (13.245) of the physical distribution P(x; Œ∏). We‚Äôll
learn in the next section that no estimator of the mean can have a lower variance
than this.
A natural estimator for the variance of the probability distribution P(x; Œ∏) is
u(N)
œÉ 2 (x) = B
N

j=1

xj ‚àíu(N)
Œº (x)
2
,
(13.247)
in which B = B(N) is a constant of proportionality. The naive choice B(N) =
1/N leads to a biased estimator. To Ô¨Ånd the correct value of B, we set the
expected value E[u(N)
œÉ 2 ] equal to œÉ 2
E[u(N)
œÉ 2 ] =

B
N

j=1

xj ‚àíu(N)
Œº (x)
2
P(x; Œ∏) dNx = œÉ 2
(13.248)
544

13.17 MEASUREMENTS, ESTIMATORS, AND FRIEDRICH BESSEL
and solve for B. Subtracting the mean Œº from both xj and u(N)
Œº (x), we express
œÉ 2/B as the sum of three terms
œÉ 2
B =
N

j=1
 
xj ‚àíŒº ‚àí

u(N)
Œº (x) ‚àíŒº
2
P(x; Œ∏) dNx = Sjj+SjŒº+SŒºŒº (13.249)
the Ô¨Årst of which is
Sjj =
N

j=1
 
xj ‚àíŒº
2 P(x; Œ∏) dNx = NœÉ 2.
(13.250)
The cross-term SjŒº is
SjŒº = ‚àí2
N

j=1
 
xj ‚àíŒº
 
u(N)
Œº (x) ‚àíŒº

P(x; Œ∏) dNx
(13.251)
= ‚àí2
N
N

j=1
 
xj ‚àíŒº
 N

k=1
(xk ‚àíŒº) P(x; Œ∏) dNx = ‚àí2œÉ 2.
The third term is related to the variance (13.246)
SŒºŒº =
N

j=1
 
u(N)
Œº (x) ‚àíŒº
2
P(x; Œ∏) dNx = NV[uN
Œº ] = œÉ 2.
(13.252)
Thus the factor B must satisfy
œÉ 2/B = NœÉ 2 ‚àí2œÉ 2 + œÉ 2 = (N ‚àí1)œÉ 2,
(13.253)
which tells us that B = 1/(N ‚àí1), which is Bessel‚Äôs correction. Our estimator
for the variance of the probability distribution P(x; Œ∏) then is
u(N)
œÉ 2 (x) =
1
N ‚àí1
N

j=1

xj ‚àíu(N)
Œº (x)
2
=
1
N ‚àí1
N

j=1

xj ‚àí1
N
N

k=1
xk
2
.
(13.254)
It is consistent and unbiased since E[u(N)
œÉ 2 ] = œÉ 2 by construction (13.248). It
gives for the variance œÉ 2 of a single measurement the undeÔ¨Åned ratio 0/0, as it
should, whereas the naive choice B = 1/N absurdly gives zero.
On the basis of N measurements x1, . . . , xN we can estimate the mean of the
unknown probability distribution P(x; Œ∏) as ŒºN = (x1 + ¬∑ ¬∑ ¬∑ + xN)/N. And we
can use Bessel‚Äôs formula (13.254) to estimate the variance œÉ 2
N of the unknown
545

PROBABILITY AND STATISTICS
distribution P(x; Œ∏). Our formula (13.246) for the variance œÉ 2(ŒºN) of the mean
ŒºN then gives
œÉ 2(ŒºN) = œÉ 2
N
N =
1
N(N ‚àí1)
N

j=1

xj ‚àí1
N
N

k=1
xk
2
.
(13.255)
Thus we can use N measurements xj to estimate the mean Œº to within a
standard error or standard deviation of
œÉ(ŒºN) =
(
œÉ 2
N
N =
9
:
:
:
;
1
N(N ‚àí1)
N

j=1

xj ‚àí1
N
N

k=1
xk
2
.
(13.256)
Few formulas have seen so much use.
13.18 Information and Ronald Fisher
The Fisher information matrix of a distribution P(x; Œ∏) is the mean of products
of its partial logarithmic derivatives
Fk‚Ñì(Œ∏) ‚â°E
#‚àÇln P(x; Œ∏)
‚àÇŒ∏k
‚àÇln P(x; Œ∏)
‚àÇŒ∏‚Ñì
$
=
 ‚àÇln P(x; Œ∏)
‚àÇŒ∏k
‚àÇln P(x; Œ∏)
‚àÇŒ∏‚Ñì
P(x; Œ∏) dNx
(13.257)
(Ronald Fisher, 1890‚Äì1962). Fisher‚Äôs matrix (exercise 13.33) is symmetric Fk‚Ñì=
F‚Ñìk and nonnegative (1.38), and when it is positive (1.39), it has an inverse. By
differentiating the normalization condition

P(x; Œ∏) dNx = 1
(13.258)
we have
0 =
 ‚àÇP(x; Œ∏)
‚àÇŒ∏k
dNx =
 ‚àÇln P(x; Œ∏)
‚àÇŒ∏k
P(x; Œ∏) dNx,
(13.259)
which says that the mean value of the logarithmic derivative of the proba-
bility distribution, a quantity called the score, vanishes. Using the notation
P,k ‚â°‚àÇP/‚àÇŒ∏k and (ln P),k ‚â°‚àÇln P/‚àÇŒ∏k and differentiating again, one has
(exercise 13.34)

(ln P),k (ln P),‚ÑìP dNx = ‚àí

(ln P),k,‚ÑìP dNx
(13.260)
so that another form of Fisher‚Äôs information matrix is
Fk‚Ñì(Œ∏) = ‚àíE

(ln P),k,‚Ñì

= ‚àí

(ln P),k,‚ÑìP dNx.
(13.261)
546

13.18 INFORMATION AND RONALD FISHER
Cram√©r and Rao used Fisher‚Äôs information matrix to form a lower bound
on the covariance (13.35) matrix C[uk, u‚Ñì] of any two estimators. To see how
this works, we use the vanishing (13.259) of the mean of the score to write the
covariance of the kth score Vk ‚â°(ln P(x; Œ∏)),k with the ‚Ñìth estimator u‚Ñì(x) as a
derivative ‚ü®u‚Ñì‚ü©,k of the mean ‚ü®u‚Ñì‚ü©
C[Vk, u‚Ñì] =

(ln P),k (u‚Ñì‚àíb‚Ñì‚àíŒ∏‚Ñì) P dNx =

(ln P),k u‚ÑìP dNx
=

P,k(x; Œ∏) u‚Ñì(x) dNx = ‚ü®u‚Ñì‚ü©,k.
(13.262)
Thus for any two sets of constants yk and w‚Ñì, we have with P =
‚àö
P
‚àö
P
m

‚Ñì,k=1
yk ‚àÇk‚ü®u‚Ñì‚ü©w‚Ñì=

m

‚Ñì,k=1
yk (ln P),k
‚àö
P (u‚Ñì‚àíb‚Ñì‚àíŒ∏‚Ñì) w‚Ñì
‚àö
P dNx. (13.263)
We can suppress some indices by grouping the yjs, the wjs, and so forth into
the vectors Y T = (y1, . . . , ym), W T = (w1, . . . , wm), UT = (u1, . . . , um), BT =
(b1, . . . , bm), and T = (Œ∏1, . . . , Œ∏m), and by grouping the ‚àÇk‚ü®u‚Ñì‚ü©s into a matrix
(‚àáU)kl, which by (13.241) is
(‚àáU)kl ‚â°‚àÇk‚ü®u‚Ñì‚ü©= ‚àÇk (Œ∏‚Ñì+ b‚Ñì) = Œ¥kl + ‚àÇkb‚Ñì.
(13.264)
In this compact notation, our relation (13.263) is
Y T ‚àáU W =

Y T(‚àáln P)
‚àö
P (UT ‚àíBT ‚àíT)W
‚àö
P dNx.
(13.265)
Squaring, we apply Schwarz‚Äôs inequality (6.379)

Y T ‚àáU W
2 =
#
Y T(‚àáln P)
‚àö
P (UT ‚àíBT ‚àíT)W
‚àö
P dNx
$2
‚â§
 
Y T(‚àáln P)
‚àö
P
2
dNx
 
(UT ‚àíBT ‚àíT)W
‚àö
P
2
dNx
=
 
Y T‚àáln P
2 P dNx
 
(UT ‚àíBT ‚àíT)W
2 P dNx.
(13.266)
In the last line, we recognize the Ô¨Årst integral as Y TFY, where F is Fisher‚Äôs
matrix (13.257), and the second as W TCW in which C is the covariance of the
estimators
Ck‚Ñì‚â°C[U, U]k‚Ñì= C[uk ‚àíbk ‚àíŒ∏k, u‚Ñì‚àíb‚Ñì‚àíŒ∏‚Ñì].
(13.267)
547

PROBABILITY AND STATISTICS
So (13.266) says

Y T‚àáUW
2 ‚â§Y TFY W TCW.
(13.268)
Thus as long as the symmetric nonnegative matrix F is positive and so has an
inverse, we can set the arbitrary constant vector Y = F‚àí1‚àáU W and get
(W T‚àáUTF‚àí1‚àáUW)2 ‚â§W T‚àáUTF‚àí1‚àáUW W TCW.
(13.269)
Canceling a common factor, we obtain the Cram√©r‚ÄìRao inequality
W TC W ‚â•W T‚àáUTF‚àí1 ‚àáU W
(13.270)
often written as
C ‚â•‚àáUT F‚àí1 ‚àáU.
(13.271)
By (13.264), the matrix ‚àáU is the identity matrix I plus the gradient of the
bias B
‚àáU = I + ‚àáB.
(13.272)
Thus another form of the Cram√©r‚ÄìRao inequality is
C ‚â•(I + ‚àáB) T F‚àí1 (I + ‚àáB)
(13.273)
or in terms of the arbitrary vector W
W TC W ‚â•W T (I + ‚àáB) T F‚àí1 (I + ‚àáB) W.
(13.274)
Letting the arbitrary vector W be Wj = Œ¥jk, one arrives at (exercise 13.35) the
Cram√©r‚ÄìRao lower bound on the variance V[uk] = C[uk, uk]
V[uk] ‚â•(F‚àí1)kk +
m

‚Ñì=1
2(F‚àí1)k‚Ñì‚àÇlbk +
m

‚Ñì,j=1
(F‚àí1)‚Ñìj‚àÇlbk‚àÇjbk.
(13.275)
If the estimator uk(x) is unbiased, then this lower bound simpliÔ¨Åes to
V[uk] ‚â•(F‚àí1)kk.
(13.276)
Example 13.16 (Cram√©r‚ÄìRao bound for a gaussian)
The elements of Fisher‚Äôs
information matrix for the mean Œº and variance œÉ 2 of Gauss‚Äôs distribution for
N data points x1, . . . , xN
P(N)
G (x, Œº, œÉ) =
N

j=1
PG(xj; Œº, œÉ) =

1
œÉ
‚àö
2œÄ
N
exp
‚éõ
‚éù‚àí
N

j=1
(xj ‚àíŒº)2
2œÉ 2
‚éû
‚é†
(13.277)
548

13.18 INFORMATION AND RONALD FISHER
are
FŒºŒº =
 #
ln P(N)
G (x, Œº, œÉ)

,Œº
$2
P(N)
G (x, Œº, œÉ) dNx
=
N

i,j=1
 xi ‚àíŒº
œÉ 2
 xj ‚àíŒº
œÉ 2

P(N)
G (x, Œº, œÉ) dNx
=
N

i=1
 xi ‚àíŒº
œÉ 2
2
P(N)
G (x, Œº, œÉ) dNx = N
œÉ 2 ,
(13.278)
FŒºœÉ 2 =

(ln P(N)
G (x, Œº, œÉ)),Œº (ln P(N)
G (x, Œº, œÉ)),œÉ 2 P(N)
G (x, Œº, œÉ) dNx
=
N

i,j=1
 #xi ‚àíŒº
œÉ 2
$ 
(xj ‚àíŒº)2
2œÉ 4
‚àí
1
2œÉ 2

P(N)
G (x, Œº, œÉ) dNx = 0,
FœÉ 2Œº = FŒºœÉ 2 = 0, and
FœÉ 2œÉ 2 =
 
(ln P(N)
G (x, Œº, œÉ)),œÉ 2
2
P(N)
G (x, Œº, œÉ) dNx
=
N

i,j=1
 
(xi ‚àíŒº)2
2œÉ 4
‚àí
1
2œÉ 2
 
(xj ‚àíŒº)2
2œÉ 4
‚àí
1
2œÉ 2

P(N)
G (x, Œº, œÉ) dNx
= N
2œÉ 4 .
(13.279)
The inverse of Fisher‚Äôs matrix then is diagonal with (F‚àí1)ŒºŒº = œÉ 2/N and
(F‚àí1)œÉ 2œÉ 2 = 2œÉ 4/N.
The variance of any unbiased estimator uŒº(x) of the mean must exceed its
Cram√©r‚ÄìRao lower bound (13.276), and so V[uŒº] ‚â•(F‚àí1)ŒºŒº = œÉ 2/N. The
variance V[u(N)
Œº ] of the natural estimator of the mean u(N)
Œº (x) = (x1+¬∑ ¬∑ ¬∑+xN)/N
is œÉ 2/N by (13.246), and so it respects and saturates the lower bound (13.276)
V[u(N)
Œº ] = E[(u(N)
Œº
‚àíŒº)2] = œÉ 2/N = (F‚àí1)ŒºŒº.
(13.280)
One may show (exercise 13.36) that the variance V[u(N)
œÉ 2 ] of Bessel‚Äôs estimator
(13.254) of the variance is (Riley et al., 2006, p. 1248)
V[u(N)
œÉ 2 ] = 1
N

ŒΩ4 ‚àíN ‚àí3
N ‚àí1œÉ 4

(13.281)
where ŒΩ4 is the fourth central moment (13.26) of the probability distribution.
For the gaussian PG(x; Œº, œÉ) one may show (exercise 13.37) that this moment is
ŒΩ4 = 3œÉ 4, and so for it
VG[u(N)
œÉ 2 ] =
2
N ‚àí1 œÉ 4.
(13.282)
549

PROBABILITY AND STATISTICS
Thus the variance of Bessel‚Äôs estimator of the variance respects but does not
saturate its Cram√©r‚ÄìRao lower bound (13.276, 13.279)
VG[u(N)
œÉ 2 ] =
2
N ‚àí1 œÉ 4 > 2
N œÉ 4.
(13.283)
Estimators that saturate their Cram√©r‚ÄìRao lower bounds are efÔ¨Åcient. The
natural estimator u(N)
Œº (x) of the mean is efÔ¨Åcient as well as consistent and unbi-
ased, and Bessel‚Äôs estimator u(N)
œÉ 2 (x) of the variance is consistent and unbiased
but not efÔ¨Åcient.
13.19 Maximum likelihood
Suppose we measure some quantity x at various values of another variable t
and Ô¨Ånd the values x1, x2, . . . , xN at the known points t1, t2, . . . , tN. We might
want to Ô¨Åt these measurements to a curve x = f (t; Œ±) where Œ± = Œ±1, . . . , Œ±M is
a set of M < N parameters. In view of the central limit theorem, we‚Äôll assume
that the points xj fall in Gauss‚Äôs distribution about the values xj = f (tj; Œ±) with
some known variance œÉ 2. The probability of getting the N values x1, . . . , xN
then is
P(x) =
N

j=1
P(xj, tj, œÉ) =

1
œÉ
‚àö
2œÄ
N
exp
‚éõ
‚éù‚àí
N

j=1
(xj ‚àíf (tj; Œ±))2
2œÉ 2
‚éû
‚é†.
(13.284)
To Ô¨Ånd the M parameters Œ±, we maximize the likelihood P(x) by minimizing
the argument of its exponential
0 =
‚àÇ
‚àÇŒ±‚Ñì
N

j=1

xj ‚àíf (tj; Œ±)
2 = ‚àí2
N

j=1

xj ‚àíf (tj; Œ±)
 ‚àÇf (tj; Œ±)
‚àÇŒ±‚Ñì
.
(13.285)
If the function f (t; Œ±) depends nonlinearly upon the parameters Œ±, then we may
need to use numerical methods to solve this least-squares problem.
But if the function f (t; Œ±) depends linearly upon the M parameters Œ±
f (t; Œ±) =
M

k=1
gk(t) Œ±k
(13.286)
then the equations (13.285) that determine these parameters Œ± are linear
0 =
N

j=1

xj ‚àí
M

k=1
gk(tj) Œ±k

g‚Ñì(tj).
(13.287)
550

13.20 KARL PEARSON‚ÄôS CHI-SQUARED STATISTIC
In matrix notation with G the N √ó M rectangular matrix with entries Gjk =
gk(tj), they are
GT x = GT GŒ±.
(13.288)
The basis functions gk(t) may depend nonlinearly upon the independent vari-
able t. If one chooses them to be sufÔ¨Åciently different that the columns of G are
linearly independent, then the rank of G is M, and the nonnegative matrix GT G
has an inverse. The matrix G then has a pseudoinverse (1.397)
G+ =

GT G
‚àí1 GT
(13.289)
and it maps the N-vector x into our parameters Œ±
Œ± = G+ x.
(13.290)
The product G+ G = IM is the M √ó M identity matrix, while
G G+ = P
(13.291)
is an N √ó N projection operator (exercise 13.38) onto the M √ó M subspace
for which G+G = IM is the identity operator. Like all projection operators, P
satisÔ¨Åes P2 = P.
13.20 Karl Pearson‚Äôs chi-squared statistic
The argument of the exponential (13.284) in P(x) is (the negative of) Karl
Pearson‚Äôs chi-squared statistic (Pearson, 1900)
œá2 ‚â°
N

j=1
(xj ‚àíf (tj; Œ±))2
2œÉ 2
.
(13.292)
When the function f (t; Œ±) is linear (13.286) in Œ±, the N-vector f (tj; Œ±) is f = G Œ±.
Pearson‚Äôs œá2 then is
œá2 = (x ‚àíG Œ±)2/2œÉ 2.
(13.293)
Now (13.290) tells us that Œ± = G+ x, and so in terms of the projection operator
P = G G+, the vector x ‚àíG Œ± is
x ‚àíG Œ± = x ‚àíG G+ x =

I ‚àíG G+
x = (I ‚àíP) x.
(13.294)
So œá2 is proportional to the squared length
œá2 = Àúx2/2œÉ 2
(13.295)
of the vector
Àúx ‚â°(I ‚àíP) x.
(13.296)
551

PROBABILITY AND STATISTICS
Thus if the matrix G has rank M, and the vector x has N independent
components, then the vector Àúx has only N ‚àíM independent components.
Example 13.17 (Two position measurements)
Suppose we measure a position
twice with error œÉ and get x1 and x2. Then the single parameter Œ± is their average
Œ± = (x1 + x2)/2, and œá2 is
œá2 =
3
[x1 ‚àí(x1 + x2)/2]2 + [x2 ‚àí(x1 + x2)/2]24<
2œÉ 2
=
3
[(x1 ‚àíx2)/2]2 + [(x2 ‚àíx1)/2]24<
2œÉ 2
=

(x1 ‚àíx2)/
‚àö
2
2 <
2œÉ 2.
(13.297)
Thus instead of having two independent components x1 and x2, œá2 just has one
(x1 ‚àíx2)/
‚àö
2.
We can see how this happens more generally if we use as basis vectors the
N ‚àíM orthonormal vectors |j‚ü©in the kernel of P (that is, the |j‚ü©s annihilated
by P)
P|j‚ü©= 0,
1 ‚â§j ‚â§N ‚àíM
(13.298)
and the M that lie in the range of the projection operator P
P|k‚ü©= |k‚ü©,
N ‚àíM + 1 ‚â§k ‚â§N.
(13.299)
In terms of these basis vectors, the N-vector x is
x =
N‚àíM

j=1
xj|j‚ü©+
N

k=N‚àíM+1
xk|k‚ü©
(13.300)
and the last M components of the vector Àúx vanish
Àúx = (I ‚àíP) x =
N‚àíM

j=1
xj|j‚ü©.
(13.301)
Example 13.18 (N position measurements)
Suppose the N values of xj are the
measured values of the position f (tj; Œ±) = Œ± of some object. Then M = 1, and
Gj1 = g1(tj) = 1 for j = 1, . . . , N. Now GTG = N is a 1 √ó 1 matrix, the number
N, and the parameter Œ± is the mean x
Œ± = G+ x =

GT G
‚àí1 GT x = 1
N
N

j=1
xj = x
(13.302)
552

13.20 KARL PEARSON‚ÄôS CHI-SQUARED STATISTIC
of the N position measurements xj. So the vector Àúx has components Àúxj = xj ‚àíx
and is orthogonal to GT = (1, 1, . . . , 1)
GT Àúx =
‚éõ
‚éù
N

j=1
xj
‚éû
‚é†‚àíNx = 0.
(13.303)
The matrix GT has rank 1, and the vector Àúx has N ‚àí1 independent components.
Suppose now that we have determined our M parameters Œ± and have a
theoretical Ô¨Åt
x = f (t; Œ±) =
M

k=1
gk(t) Œ±k,
(13.304)
which when we apply it to N measurements xj gives œá2 as
œá2 = (Àúx)2 /2œÉ 2.
(13.305)
How good is our Ô¨Åt?
A œá2 distribution with N ‚àíM degrees of freedom has by (13.202) mean
E[œá2] = N ‚àíM
(13.306)
and variance
V[œá2] = 2(N ‚àíM).
(13.307)
So our œá2 should be about
œá2 ‚âàN ‚àíM ¬±

2(N ‚àíM).
(13.308)
If it lies within this range, then (13.304) is a good Ô¨Åt to the data. But if it exceeds
N ‚àíM +

2(N ‚àíM), then the Ô¨Åt isn‚Äôt so good. On the other hand, if œá2 is
less than N ‚àíM ‚àí

2(N ‚àíM), then we may have used too many parameters.
Indeed, by using N parameters with G G+ = IN, we could get œá2 = 0 every
time.
The probability that œá2 exceeds œá2
0 is the integral (13.201)
Prn(œá2 > œá2
0) =
 ‚àû
œá2
0
Pn(œá2/2) dœá2 =
 ‚àû
œá2
0
1
2(n/2)

œá2
2
n/2‚àí1
e‚àíœá2/2dœá2,
(13.309)
in which n = N ‚àíM is the number of data points minus the number of param-
eters, and (n/2) is the gamma function (5.102, 4.62). So an M-parameter Ô¨Åt
to N data points has only a chance of œµ of being right if its œá2 is greater than a
œá2
0 for which PrN‚àíM(œá2 > œá2
0) = œµ. These probabilities PrN‚àíM(œá2 > œá2
0) are
553

PROBABILITY AND STATISTICS
0
5
10
15
20
25
30
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
The chi-squared test
œá2
0
Prn(œá2 > œá2
0)
n = 2
4
6
8
10
Figure 13.6
The probabilities Prn(œá2 > œá2
0 ) are plotted from left to right for n =
N ‚àíM = 2, 4, 6, 8, and 10 degrees of freedom as functions of œá2
0 .
plotted in Fig. 13.6 for n = N ‚àíM = 2, 4, 6, 8, and 10. In particular, the prob-
ability of a value of œá2 greater than œá2
0 = 20 respectively is 0.000045, 0.000499,
0.00277, 0.010336, and 0.029253 for n = N ‚àíM = 2, 4, 6, 8, and 10.
13.21 Kolmogorov‚Äôs test
Suppose we want to use a sequence of N measurements xj to determine
the probability distribution that they come from. Our empirical probability
distribution is
P(N)
e
(x) = 1
N
N

j=1
Œ¥(x ‚àíxj).
(13.310)
Our cumulative probability for events less than x then is
Pr(N)
e
(‚àí‚àû, x) =
 x
‚àí‚àû
P(N)
e
(x‚Ä≤) dx‚Ä≤ =
 x
‚àí‚àû
1
N
N

j=1
Œ¥(x‚Ä≤ ‚àíxj) dx‚Ä≤.
(13.311)
So if we label our events in increasing order x1 ‚â§x2 ‚â§¬∑ ¬∑ ¬∑ ‚â§xN, then the
probability of an event less than x is
554

13.21 KOLMOGOROV‚ÄôS TEST
Pr(N)
e
(‚àí‚àû, x) = j
N
for
xj < x < xj+1.
(13.312)
Having approximately and experimentally determined our empirical cumu-
lative probability distribution Pr(N)
e
(‚àí‚àû, x), we might want to know whether it
comes from some hypothetical, theoretical cumulative probability distribution
Prt(‚àí‚àû, x). One way to do this is to compute the distance DN between the two
cumulative probability distributions
DN =
sup
‚àí‚àû<x<‚àû
Pr(N)
e
(‚àí‚àû, x) ‚àíPrt(‚àí‚àû, x)
 ,
(13.313)
in which sup stands for supremum and means least upper bound. Since cumu-
lative probabilities lie between zero and one, it follows (exercise 13.39) that the
Kolmogorov distance is bounded by
0 ‚â§DN ‚â§1.
(13.314)
The simpler Smirnov distances
D+
N =
sup
‚àí‚àû<x<‚àû

Pr(N)
e
(‚àí‚àû, x) ‚àíPrt(‚àí‚àû, x)

,
D‚àí
N =
sup
‚àí‚àû<x<‚àû

Prt(‚àí‚àû, x) ‚àíPr(N)
e
(‚àí‚àû, x)

(13.315)
provide (exercise 13.40) an expression for DN as the greater of the two
DN = max(D+
N, D‚àí
N).
(13.316)
Using our explicit expression (13.312) for the empirical cumulative probability
Pr(N)
e
(‚àí‚àû, x) and the monotonicity (13.30) of cumulative probabilities such as
Prt(‚àí‚àû, x), one may show (exercise 13.41) that the Smirnov distances are given
by
D+
N = sup
1‚â§j‚â§N
 j
N ‚àíPrt(‚àí‚àû, xj)

D‚àí
N = sup
1‚â§j‚â§N

Prt(‚àí‚àû, xj) ‚àíj ‚àí1
N

.
(13.317)
In general, as the number N of data points increases, we expect that
our empirical distribution Pr(N)
e
(‚àí‚àû, x) should approach the actual empiri-
cal distribution Pre(‚àí‚àû, x) from which the events xj came. In this case, the
Kolmogorov distance DN should converge to a limiting value D‚àû
lim
N‚Üí‚àûDN = D‚àû=
sup
‚àí‚àû<x<‚àû
|Pre(‚àí‚àû, x) ‚àíPrt(‚àí‚àû, x)| ‚àà[0, 1].
(13.318)
555

PROBABILITY AND STATISTICS
If the empirical distribution Pre(‚àí‚àû, x) is the same as the theoretical distribu-
tion Prt(‚àí‚àû, x), then we expect that D‚àû= 0. This expectation is conÔ¨Årmed by
a theorem due to Glivenko (Glivenko, 1933; Cantelli, 1933) according to which
the probability that the Kolmogorov distance DN should go to zero as N ‚Üí‚àû
is unity
Pr(D‚àû= 0) = 1.
(13.319)
The real issue is how fast DN should decrease with N if our events xj
do come from Prt(‚àí‚àû, x). This question was answered by Kolmogorov, who
showed (Kolmogorov, 1933) that if the theoretical distribution Prt(‚àí‚àû, x) is
continuous, then for large N (and for u > 0) the probability that
‚àö
N DN is less
than u is given by the Kolmogorov function K(u)
lim
N‚Üí‚àûPr(
‚àö
N DN < u) = K(u) ‚â°1 + 2
‚àû

k=1
(‚àí1)ke‚àí2k2u2,
(13.320)
which is universal and independent of the particular probability distributions
Pre(‚àí‚àû, x) and Prt(‚àí‚àû, x).
On the other hand, if our events xj come from a different probability dis-
tribution Pre(‚àí‚àû, x), then as N ‚Üí‚àûwe should expect that Pr(N)
e
(‚àí‚àû, x) ‚Üí
Pre(‚àí‚àû, x), and so that DN should converge to a positive constant D‚àû‚àà(0, 1].
In this case, we expect that as N ‚Üí‚àûthe quantity
‚àö
N DN should grow with
N as
‚àö
N D‚àû.
Example 13.19 (Kolmogorov‚Äôs test)
How do we use (13.320)? As illustrated in
Fig. 13.7, Kolmogorov‚Äôs distribution K(u) rises from zero to unity on (0, ‚àû),
reaching 0.9993 already at u = 2. So if our points xj come from the theoretical
distribution, then Kolmogorov‚Äôs theorem (13.320) tells us that as N ‚Üí‚àû, the
probability that
‚àö
N DN is less than 2 is more than 99.9%. But if the experimen-
tal points xj do not come from the theoretical distribution, then the quantity
‚àö
N DN should grow as
‚àö
N D‚àûas N ‚Üí‚àû.
To see what this means in practice, I took as the theoretical distribution
Pt(x) = PG(x, 0, 1), which has the cumulative probability distribution (13.85)
Prt(‚àí‚àû, x) = 1
2

erf

x/
‚àö
2

+ 1

.
(13.321)
I generated N = 10m points xj for m = 1, 2, 3, 4, 5, and 6 from the theoretical dis-
tribution Pt(x) = PG(x, 0, 1) and computed uN =
‚àö
10m D10m for these points.
I found
‚àö
10m D10m = 0.6928, 0.7074, 1.2000, 0.7356, 1.2260, and 1.0683. All
were less than 2, as expected since I had taken the experimental points xj from
the theoretical distribution.
To see what happens when the experimental points do not come from the
theoretical distribution Pt(x) = PG(x, 0, 1), I generated N = 10m points xj
556

13.21 KOLMOGOROV‚ÄôS TEST
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Kolmogorov‚Äôs distribution
u
K(u)
Figure 13.7
Kolmogorov‚Äôs cumulative probability distribution K(u) deÔ¨Åned by
(13.320) rises from zero to unity as u runs from zero to about two.
for m = 1, 2, 3, 4, 5, and 6 from Gosset‚Äôs Student‚Äôs distribution PS(x, 3, 1) deÔ¨Åned
by (13.191) with ŒΩ = 3 and a = 1. Both Pt(x) = PG(x, 0, 1) and PS(x, 3, 1)
have the same mean Œº = 0 and standard deviation œÉ = 1, as illustrated in
Fig. 13.8. For these points, I computed uN =
‚àö
N DN and found
‚àö
10m D10m =
0.7741, 1.4522, 3.3837, 9.0478, 27.6414, and 87.8147. Only the Ô¨Årst two are less
than 2, and the last four grow as
‚àö
N, indicating that the xj had not come from
the theoretical distribution. In fact, we can approximate the limiting value of
DN as D‚àû‚âàu106/
‚àö
106 = 0.0878. The exact value is (exercise 13.42) D‚àû=
0.0868552356.
At the risk of overemphasizing this example, I carried it one step further. I
generated ‚Ñì= 1, 2, . . . , 100 sets of N = 10m points x(‚Ñì)
j
for m = 2, 3, and 4
drawn from PG(x, 0, 1) and from PS(x, 3, 1) and used them to form 100 empiri-
cal cumulative probabilities Pr(‚Ñì,10m)
e,G
(‚àí‚àû, x) and Pr(‚Ñì,10m)
e,S
(‚àí‚àû, x) as deÔ¨Åned by
(13.310‚Äì13.312). Next, I computed the distances D(‚Ñì)
G,G,10m and D(‚Ñì)
S,G,10m of each
of these cumulative probabilities from the gaussian distribution PG(x, 0, 1). I
labeled the two sets of 100 quantities u(‚Ñì,m)
G,G
=
‚àö
10m D(‚Ñì)
G,G,10m and u(‚Ñì,m)
S,G
=
‚àö
10m D(‚Ñì)
S,G,10m in increasing order as u(m)
G,G,1 ‚â§u(m)
G,G,2 ‚â§¬∑ ¬∑ ¬∑ ‚â§u(m)
G,G,100 and
u(m)
S,G,1 ‚â§u(m)
S,G,2 ‚â§¬∑ ¬∑ ¬∑ ‚â§u(m)
S,G,100. I then used (13.310‚Äì13.312) to form the
cumulative probabilities
Pr(m)
e,G,G(‚àí‚àû, u) =
j
Ns
for
u(m)
G,G,j < u < u(m)
G,G,j+1,
(13.322)
557

PROBABILITY AND STATISTICS
‚àí5
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
5
0
0.1
0.2
0.3
0.4
0.5
0.6
Comparison of PG(x, 0, 1) with PS(x, 3, 1)
x
P (x)
Gauss‚Äôs
Gosset‚Äôs Student‚Äôs
Figure 13.8
The probability distributions of Gauss PG(x, 0, 1) and Gosset/Student
PS(x, 3, 1) with zero mean and unit variance.
and
Pr(m)
e,S,G(‚àí‚àû, u) =
j
Ns
for
u(m)
S,G,j < u < u(m)
S,G,j+1
(13.323)
for Ns = 100 sets of 10m points.
I plotted these cumulative probabilities in Fig. 13.9. The thick smooth curve
is Kolmogorov‚Äôs universal cumulative probability distribution K(u) deÔ¨Åned by
(13.320). The thin jagged curve that clings to K(u) is the cumulative probabil-
ity distribution Pr(4)
e,G,G(‚àí‚àû, u) made from 100 sets of 104 points taken from
PG(x, 0, 1). As the number of sets increases beyond 100 and the number of
points 10m rises further, the probability distributions Pr(m)
e,G,G(‚àí‚àû, u) converge to
the universal cumulative probability distribution K(u) and provide a numerical
veriÔ¨Åcation of Kolmogorov‚Äôs theorem. Such curves make poor Ô¨Ågures, how-
ever, because they hide beneath K(u). The curves labeled Pr(m)
e,S,G(‚àí‚àû, u) for
m = 2 and 3 are made from 100 sets of N = 10m points taken from PS(x, 3, 1)
and tested as to whether they instead come from PG(x, 0, 1). Note that as
N = 10m increases from 100 to 1000, the cumulative probability distribution
Pr(m)
e,S,G(‚àí‚àû, u) moves farther from Kolmogorov‚Äôs universal cumulative proba-
bility distribution K(u). In fact, the curve Pr(4)
e,S,G(‚àí‚àû, u) made from 100 sets of
104 points lies beyond u > 8, too far to the right to Ô¨Åt in the Ô¨Ågure. Kolmogorov‚Äôs
test gets more conclusive as the number of points N ‚Üí‚àû.
558

13.21 KOLMOGOROV‚ÄôS TEST
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Kolmogorov‚Äôs test
u
Pr (u)
Pre, S, G
(2)
Pre, S, G
(3)
Pre, G, G
(4)
Figure 13.9
Kolmogorov‚Äôs test is applied to points xj taken from Gauss‚Äôs distribu-
tion PG(x, 0, 1) and from Gosset‚Äôs Student‚Äôs distribution PS(x, 3, 1) to see whether
the xj came from PG(x, 0, 1). The thick smooth curve is Kolmogorov‚Äôs universal
cumulative probability distribution K(u) deÔ¨Åned by (13.320). The thin jagged curve
that clings to K(u) is the cumulative probability distribution Pr(4)
e,G,G(‚àí‚àû, u) made
(13.322) from points taken from PG(x, 0, 1). The other curves Pr(m)
e,S,G(‚àí‚àû, u) for
m = 2 and 3 are made (13.323) from 10m points taken from PS(x, 3, 1).
Warning, mathematical hazard
While binned data are ideal for chi-squared
Ô¨Åts, they ruin Kolmogorov tests. The reason is that if the data are in bins of
width w, then the empirical cumulative probability distribution Pr(N)
e
(‚àí‚àû, x)
is a staircase function with steps as wide as the bin-width w even in the limit
N ‚Üí‚àû. Thus even if the data come from the theoretical distribution, the limiting
value D‚àûof the Kolmogorov distance will be positive. In fact, one may show
(exercise 13.21) that when the data do come from the theoretical probability
distribution Pt(x), assumed to be continuous, then the value of D‚àûis
D‚àû=
sup
‚àí‚àû<x<‚àû
w Pt(x)
2
.
(13.324)
Thus, in this case, the quantity
‚àö
N DN would diverge as
‚àö
N D‚àûand lead one
to believe that the data had not come from Pt(x).
Suppose we have made some changes in our experimental apparatus and our
software, and we want to see whether the new data x‚Ä≤
1, x‚Ä≤
2, . . . , x‚Ä≤
N‚Ä≤ we took after
559

PROBABILITY AND STATISTICS
the changes are consistent with the old data x1, x2, . . . , xN we took before the
changes. Then, following equations (13.310‚Äì13.312), we can make two empiri-
cal cumulative probability distributions ‚Äì one Pr(N)
e
(‚àí‚àû, x) made from the N
old points xj and the other Pr(N‚Ä≤)
e
(‚àí‚àû, x) made from the N‚Ä≤ new points x‚Ä≤
j. Next,
we compute the distances
D+
N,N‚Ä≤ =
sup
‚àí‚àû<x<‚àû

Pr(N)
e
(‚àí‚àû, x) ‚àíPr(N‚Ä≤)
e
(‚àí‚àû, x)

,
DN,N‚Ä≤ =
sup
‚àí‚àû<x<‚àû
Pr(N)
e
(‚àí‚àû, x) ‚àíPr(N‚Ä≤)
e
(‚àí‚àû, x)
 ,
(13.325)
which are analogous to (13.313‚Äì13.316). Smirnov (Smirnov, 1939; Gnedenko,
1968, p. 453) has shown that as N, N‚Ä≤ ‚Üí‚àûthe probabilities that
u+
N,N‚Ä≤ =
&
NN‚Ä≤
N + N‚Ä≤ D+
N,N‚Ä≤
and
uN,N‚Ä≤ =
&
NN‚Ä≤
N + N‚Ä≤ DN,N‚Ä≤
(13.326)
are less than u are
lim
N,N‚Ä≤‚Üí‚àûPr(u+
N,N‚Ä≤ < u) = 1 ‚àíe‚àí2u2,
lim
N,N‚Ä≤‚Üí‚àûPr(uN,N‚Ä≤ < u) = K(u),
(13.327)
in which K(u) is Kolmogorov‚Äôs distribution (13.320).
Further reading
Students can learn more about probability and statistics in Mathematical
Methods for Physics and Engineering (Riley et al., 2006), An Introduction
to Probability Theory and Its Applications I, II (Feller, 1968, 1966), Theory
of Financial Risk and Derivative Pricing (Bouchaud and Potters, 2003), and
Probability and Statistics in Experimental Physics (Roe, 2001).
Exercises
13.1
Find the probabilities that the sum on two thrown fair dice is 4, 5, or 6.
13.2
Show that the zeroth moment Œº0 and the zeroth central moment ŒΩ0 always
are unity, and that the Ô¨Årst central moment ŒΩ1 always vanishes.
13.3
Compute the variance of the uniform distribution on (0, 1).
13.4
In the formulas (13.21 & 13.28) for the variances of discrete and continuous
distributions, show that E[(x ‚àí‚ü®x‚ü©)2] = Œº2 ‚àíŒº2.
13.5
A convex function is one that lies above its tangents: if f (x) is convex, then
f (x) ‚â•f (y)+(x‚àíy)f ‚Ä≤(y). For instance, ex ‚â•1+x. Show that for any convex
function f (x) that f (x) ‚â•f (‚ü®x‚ü©)+(x‚àí‚ü®x‚ü©) f ‚Ä≤(‚ü®x‚ü©) and so that ‚ü®f (x)‚ü©‚â•f (‚ü®x‚ü©)
or E [ f (x)] ‚â•f (E[x]) (Johan Jensen, 1859‚Äì1925).
560

EXERCISES
13.6
(a) Show that the covariance ‚ü®(x ‚àíx)(y ‚àíy)‚ü©is equal to ‚ü®x y‚ü©‚àí‚ü®x‚ü©‚ü®y‚ü©as
asserted in (13.35). (b) Derive (13.39) for the variance V[ax + by].
13.7
Derive expression (13.40) for the variance of a sum of N variables.
13.8
Find the range of pq = p(1 ‚àíp) for 0 ‚â§p ‚â§1.
13.9
Show that the variance of the binomial distribution (13.43) is given by
(13.47).
13.10 Redo the polling example (13.14‚Äì13.16) for the case of a slightly better poll
in which 16 people were asked and 13 said they‚Äôd vote for Nancy Pelosi.
What‚Äôs the probability that she‚Äôll win the election? (You may use Maple or
some other program to do the tedious integral.)
13.11 For the case in which N and N‚àín are big, derive (13.52 & 13.53) from (13.43
& 13.51).
13.12 For the case in which N, N ‚àín, and n are big, derive (13.54 & 13.55) from
(13.43 & 13.51).
13.13 Without using the fact that the Poisson distribution is a limiting form of the
binomial distribution, show from its deÔ¨Ånition (13.58) and its mean (13.60)
that its variance is equal to its mean, as in (13.62).
13.14 Show that the mean and variance of Gauss‚Äôs approximation (13.74) to the
binomial distribution is a normalized probability distribution with mean
‚ü®x‚ü©= Œº = pN and variance V[x] = pqN.
13.15 Derive the approximations (13.88 & 13.89) for binomial probabilities for
large N.
13.16 Compute the central moments (13.27) of the gaussian (13.75).
13.17 Derive formula (13.84) for the probability that a gaussian random variable
falls within an interval.
13.18 Show that the expression (13.91) for P(y|600) is negligible on the interval
(0, 1) except for y near 3/5.
13.19 Determine the constant A of the homogeneous solution ‚ü®v(t)‚ü©gh and derive
expression (13.141) for the general solution ‚ü®v(t)‚ü©to (13.139).
13.20 Derive equation (13.142) for the variance of the position r about its mean
qBE t.
13.21 Derive equation (13.172) for the ensemble average ‚ü®r2(t)‚ü©for the case in
which ‚ü®r2(0)‚ü©= 0 and d‚ü®r2(0)‚ü©/dt = 0.
13.22 Use (13.183) to derive the lower moments (13.185 & 13.186) of the distribu-
tions of Gauss and Poisson.
13.23 Find the third and fourth moments Œº3 and Œº4 for the distributions of
Poisson (13.178) and Gauss (13.175).
13.24 Derive formula (13.190) for the Ô¨Årst Ô¨Åve cumulants of an arbitrary probabil-
ity distribution.
13.25 Show that, like the characteristic function, the moment-generating func-
tion M(t) for an average of several independent random variables factorizes
M(t) = M1(t/N) M2(t/N) ¬∑ ¬∑ ¬∑ MN(t/N).
13.26 Derive formula (13.197) for the moments of the log-normal probability
distribution (13.196).
561

PROBABILITY AND STATISTICS
13.27 Why doesn‚Äôt the log-normal probability distribution (13.196) have a sensible
power-series about x = 0? What are its derivatives there?
13.28 Compute the mean and variance of the exponential distribution (13.198).
13.29 Show that the chi-square distribution P3,G(v, œÉ) with variance œÉ 2 = kT/m
is the Maxwell‚ÄìBoltzmann distribution (13.100).
13.30 Compute the inverse Fourier transform (13.174) of the characteristic func-
tion (13.203) of the symmetric L√©vy distribution for ŒΩ = 1 and 2.
13.31 Show that the integral that deÔ¨Ånes P(2)(y) gives formula (13.239) with two
Heaviside functions. Hint: keep x1 and x2 in the interval (0, 1).
13.32 Derive the normal distribution (13.224) in the variable (13.223) from the cen-
tral limit theorem (13.221) for the case in which all the means and variances
are the same.
13.33 Show that Fisher‚Äôs matrix (13.257) is symmetric Fk‚Ñì= F‚Ñìk and nonnegative
(1.38), and that when it is positive (1.39), it has an inverse.
13.34 Derive the integral equations (13.259 & 13.260) from the normalization
condition
2
P(x; Œ∏) dNx = 1.
13.35 Derive the Cram√©r‚ÄìRao lower bound (13.275) on the variance V[tk] from
the inequality (13.270).
13.36 Show that the variance V[t(N)
œÉ 2 ] of Bessel‚Äôs estimator (13.254) is given by
(13.281).
13.37 Compute the fourth central moment (13.26) of Gauss‚Äôs probability distribu-
tion PG(x; Œº, œÉ 2).
13.38 Show that when the matrix G has rank M, the matrices P = G G+ and
P‚ä•= 1‚àíP are projection operators that are mutually orthogonal P(I‚àíP) =
(I ‚àíP)P = 0.
13.39 Show that Kolmogorov‚Äôs distance DN is bounded as in (13.314).
13.40 Show that Kolmogorov‚Äôs distance DN is the greater of D+
N and D‚àí
N.
13.41 Derive the formulas (13.317) for D+
N and D‚àí
N.
13.42 Compute the exact limiting value D‚àûof the Kolmogorov distance between
PG(x, 0, 1) and PS(x, 3, 1). Use the cumulative probabilities (13.321 &
13.194) to Ô¨Ånd the value of x that maximizes their difference. Using Maple
or some other program, you should Ô¨Ånd x = 0.6276952185 and then
D‚àû= 0.0868552356.
13.43 Show that when the data do come from the theoretical probability distribu-
tion (assumed to be continuous) but are in bins of width w, then the limiting
value D‚àûof the Kolmogorov distance is given by (13.324).
562

14
Monte Carlo methods
14.1 The Monte Carlo method
The Monte Carlo method is simple, robust, and useful. It was invented by
Enrico Fermi and developed by Metropolis (Metropolis et al., 1953). It has
many applications. One can use it for numerical integration. One can use it to
decide whether an odd signal is random noise or something to evaluate. One
can use it to generate sequences of conÔ¨Ågurations that are random but occur
according to a probability distribution, such as the Boltzmann distribution of
statistical mechanics. One even can use it to solve virtually any problem for
which one has a criterion to judge the quality of an arbitrary solution and a way
of generating a suitably huge space of possible solutions. That‚Äôs how evolution
invented us.
14.2 Numerical integration
Suppose one wants to numerically integrate a function f (x) of a vector x =
(x1, . . . , xn) over a region R. One generates a large number N of random val-
ues for the n coordinates x within a hypercube of length L that contains the
region R, keeps the NR points xk = (x1k, . . . , xnk) that fall within the region
R, computes the average ‚ü®f (xk)‚ü©, and multiplies by the hypervolume VR of the
region

R
f (x) dnx ‚âàVR
NR
NR

k=1
f (xk).
(14.1)
If the hypervolume VR is hard to compute, you can have the Monte Carlo
code compute it for you. The hypervolume VR is the volume Ln of the enclosing
563

MONTE CARLO METHODS
hypercube multiplied by the number NR of times the N points fall within the
region R
VR = NR
N Ln.
(14.2)
The integral formula (14.1) then becomes

R
f (x) dnx ‚âàLn NR
N2
R
NR

k=1
f (xk).
(14.3)
The utility of the Monte Carlo method of numerical integration rises sharply
with the dimension n of the hypervolume.
Example 14.1 (Numerical integration)
Suppose one wants to integrate the
function
f (x, y) =
e‚àí2x‚àí3y

x2 + y2 + 1
(14.4)
over the quarter of the unit disk in which x and y are positive. In this case, VR
is the area œÄ/4 of the quarter disk.
To generate fresh random numbers, one must set the seed for the code that
computes them. The following program sets the seed by using the subroutine
init_random_seed deÔ¨Åned in a FORTRAN95 program in section 13.16. With
some compilers, one can just write ‚Äúcall random_seed()‚Äù.
program integrate
implicit none ! catches typos
integer :: k, N
integer, parameter :: dp = kind(1.0d0)
real(dp) :: x, y, sum = 0.0d0, f
real(dp), dimension(2) :: rdn
real(dp), parameter :: area = atan(1.0d0) ! pi/4
f(x,y) = exp(-2*x - 3*y)/sqrt(x**2 + y**2 + 1.0d0)
write(6,*) ‚ÄôHow many points?‚Äô
read(5,*) N
call init_random_seed() ! set new seed
do k = 1, N
10
call random_number(rdn); x= rdn(1); y = rdn(2)
if (x**2+y**2 > 1.0d0) then
go to 10
end if
sum = sum + f(x,y)
end do
! integral = area times mean value < f > of f
sum = area*sum/real(N,dp)
564

14.2 NUMERICAL INTEGRATION
write(6,*) ‚ÄôThe integral is ‚Äô,sum
end program integrate
I ran this code with npoints = 10‚Ñìfor ‚Ñì= 1, 2, 3, 4, 5, 6, 7, and 8 and
found respectively the results 0.059285, 0.113487, 0.119062, 0.115573, 0.118349,
0.117862, 0.117868, and 0.117898. The integral is approximately 0.1179.
An equivalent C++ code by Sean Cahill is:
#include <math.h>
#include <iostream>
#include <stdlib.h>
using namespace std;
// The function to integrate
double f(const double& x, const double& y)
{
double numer = exp(-2*x - 3*y);
double denom = sqrt(x*x + y*y + 1);
double retval = numer / denom;
return retval;
}
void integrate ()
{
// Declares local constants
const double area = atan(1); // pi/4
// Inits local variables
int n=0;
double sum=0,x=0,y=0;
// Seeds random number generator
srand ( time(NULL) );
// Gets the value of N
cout << "What is N? ";
cin >> n;
// Loops the given number of times
565

MONTE CARLO METHODS
for (int i=0; i<n; i++)
{
// Loops until criteria met
while (true)
{
// Generates random points between 0 and 1
x = static_cast<double>(rand()) / RAND_MAX;
y = static_cast<double>(rand()) / RAND_MAX;
// Checks if the points are suitable
if ((x*x + y*y) <= 1)
{
// If so, break out of the while loop
break;
}
}
// Updates our sum with the given points
sum += f(x,y);
}
// Integral = area times mean value < f > of f
sum = area * sum / n;
cout << ‚Äò‚Äò The integral is‚Äô‚Äô << sum << endl;
}
14.3 Applications to experiments
Physicists accumulate vast quantities of data and sometimes must decide
whether a particular signal is due to a defect in the detector, to a random Ô¨Çuc-
tuation in the real events that they are measuring, or to a new and unexpected
phenomenon. For simplicity, let us assume that the background can be ignored
and that the real events arrive randomly in time apart from extraordinary phe-
nomena. One reliable way to evaluate an ambiguous signal is to run a Monte
Carlo program that generates the kind of real random events to which one‚Äôs
detector is sensitive and to use these events to compute the probability that the
signal occurred randomly.
To illustrate the use of random-event generators, we will consider the work
of a graduate student who spent 100 days counting muons produced by atmo-
spheric GeV neutrinos in an underground detector. Each of the very large
566

14.3 APPLICATIONS TO EXPERIMENTS
number N of primary cosmic rays that hit the Earth each day can collide with a
nucleus and make a shower of pions which in turn produce atmospheric neutri-
nos that can make muons in the detector. The probability p that a given cosmic
ray will make a muon in the detector is very small, but the number N of primary
cosmic rays is very large. In this experiment, their product pN was ‚ü®n‚ü©= 0.1
muons per day. Since N is huge and p tiny, the probability distribution is Pois-
son, and so by (13.58) the probability that n muons would be detected on any
particular day is
P(n, ‚ü®n‚ü©) = ‚ü®n‚ü©n
n! e‚àí‚ü®n‚ü©
(14.5)
in the absence of a failure of the anticoincidence shield or some other problem
with the detector ‚Äì or some hard-to-imagine astrophysical event.
The graduate student might have used the following program to generate
1,000,000 random histories of 100 days of events distributed according to the
Poisson distribution (14.5) with ‚ü®n‚ü©= 0.1:
program muons
implicit none
interface
function factorial(n)
implicit none
integer, intent(in) :: n
double precision :: factorial
end function factorial
end interface
integer :: k, m, day, number
integer, parameter :: N = 1000000 ! number of data
sets
integer, dimension(N,100) :: histories
integer, dimension(0:100) :: maxEvents = 0,
sumEvents = 0
double precision :: prob, x, numMuons, totMuons
double precision, dimension(0:100) :: p
double precision, parameter :: an = 0.1 ! <n> events
per day
prob = exp(-an); p(0) = prob; maxEvents = 0
! p(k) is the probability of fewer than k+1 events per
day
do k = 1, 100 ! make Poisson distribution
prob = prob + an**k*exp(-an)/factorial(k)
567

MONTE CARLO METHODS
p(k) = prob
end do
call init_random_seed() ! sets random seed
do k = 1, N ! do N histories
do day = 1, 100 ! do day of kth history
call random_number(x)
do m = 100, 0, -1
if (x < p(m)) then
number = m
end if
end do
histories(k,day) = number
end do
numMuons = maxval(histories(k,:))
totMuons = sum(histories(k,:))
maxEvents(numMuons) = maxEvents(numMuons) + 1
sumEvents(totMuons) = sumEvents(totMuons) + 1
end do
open(7,file="maxEvents"); open(8,file="totEvents")
do k = 0, 100
write(7,*) k, maxEvents(k); write(8,*) k,
sumEvents(k)
end do
end program muons
function factorial(n)
result(fact)
implicit none
integer, intent(in) :: n
integer, parameter :: dp = kind(1.0d0)
real(dp) :: fact
fact = 1.0d0
do i = 1, n
fact = i*fact
end do
end function factorial
Figure 14.1 plots the results from this simple Monte Carlo of 1,000,000
runs of 100 days each. The boxes show that the maximum number of muons
detected on a single day respectively was n = 1, 2, and 3 on 62.6%, 35.9%,
and 1.5% of the runs ‚Äì and respectively was n = 0, 4, 5, and 6 on only 36,
410, 9, and 1 runs. Thus if the actual run detected no muons at all, that would
be by (13.83) about a 4œÉ event, while a run with more than four muons on a
single day would be an event of more than 4œÉ. Either would be a reason to
568

14.3 APPLICATIONS TO EXPERIMENTS
0
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
6
A million runs of 100 days each
Number n of muons detected
log10(histories)
in 100 days
in 1 day
Figure 14.1
The number (out of 1,000,000) of histories of 100 days in which a
maximum of n muons is detected on a single day (boxes) and in 100 days (curve).
examine the apparatus or the heavens; the Monte Carlo can‚Äôt tell us which. The
curve shows how many runs had a total of n muons; 125,142 histories had ten
muons.
Of course, one could compute the data of Fig. 14.1 by hand without running
a Monte Carlo. But suppose one‚Äôs aging phototubes reduced the mean number
of muons detected per day to ‚ü®n‚ü©= 0.1(1 ‚àíŒ±d/100) on day d? Or suppose one
needed the probability of detecting more than one muon on two days separated
by one day of zero muons? In such cases, the analytic computation would be
difÔ¨Åcult and error prone, but the student would need to change only a few lines
in the Monte Carlo program.
An equivalent C++ code by Sean Cahill is:
#include <stdlib.h>
#include <time.h>
#include <math.h>
#include <iostream>
#include <fstream>
#include <iomanip>
#include <vector>
#include <valarray>
569

MONTE CARLO METHODS
using namespace std;
// Calculates the factorial of n
double factorial(const int& n)
{
double f = 1;
int i=0;
for(i = 1; i <= n; i++)
{
f *= i;
}
return f;
}
void muons()
{
// Declares constants
const int N = 1000000; // Number of data sets
const int LOOP_ITR = 101;
const double AN = 1; // Number of events per day
// Inits local variables
int k=0, m=0, day=0, num=0, numMuons=0, totMuons=0;
int maxEvents[LOOP_ITR];
int totEvents[LOOP_ITR];
memset (maxEvents, 0, sizeof(int) * LOOP_ITR);
memset (totEvents, 0, sizeof(int) * LOOP_ITR);
// Creates our 2d histories array
vector<valarray<int> > histories(N, LOOP_ITR);
double prob=0,tmpProb=0,fact=0, x=0;
double p[LOOP_ITR];
// probability of no events
p[0] = exp(-AN);
prob = p[0];
// p(k) is the probability of fewer than k+1 events
per day
for (k=1; k<=LOOP_ITR; k++)
570

14.3 APPLICATIONS TO EXPERIMENTS
{
fact = factorial (k);
tmpProb = k * exp(-AN) / fact;
prob += pow(AN, tmpProb);
p[k] = prob;
}
// Random seed
srand ( time(NULL) );
// Goes through all the histories
for (k=0; k<N; k++)
{
// Goes through all the days
for (day=1; day<LOOP_ITR; day++)
{
// Generates a random number between 0 and 1
x = static_cast<double>(rand()) / RAND_MAX;
// Finds an M with p(M) < X
for (m=100; m>=0; m--)
{
if (x < p[m])
{
num = m;
}
}
histories[k][day] = num;
}
// Calculates max and sum
numMuons = histories[k].max();
totMuons = histories[k].sum();
// Updates our records
maxEvents[numMuons]++;
totEvents[totMuons]++;
}
// Opens a data file
ofstream fhMaxEvents, fhSumEvents;
fhMaxEvents.open ("maxEvents.txt");
571

MONTE CARLO METHODS
fhSumEvents.open ("totEvents.txt");
// Sets precision
fhMaxEvents.setf(ios::fixed,ios::floatfield);
fhMaxEvents.precision(7);
fhSumEvents.setf(ios::fixed,ios::floatfield);
fhSumEvents.precision(7);
// Writes the data to a file
for (k=0; k<LOOP_ITR; k++)
{
fhMaxEvents << k << "
" << maxEvents[k] << endl;
fhSumEvents << k << "
" << totEvents[k] << endl;
}
}
14.4 Statistical mechanics
The Metropolis algorithm can generate a sequence of states or conÔ¨Ågurations
of a system distributed according to the Boltzmann probability distribution
(1.345). Suppose the state of the system is described by a vector x of many com-
ponents. For instance, if the system is a protein, the vector x might be the 3N
spatial coordinates of the N atoms of the protein. A protein composed of 200
amino acids has about 4000 atoms, and so the vector x would have some 12,000
components. Suppose E(x) is the energy of conÔ¨Åguration x of the protein in its
cellular environment of salty water crowded with macromolecules. How do we
generate a sequence of ‚Äúnative states‚Äù of the protein at temperature T?
We start with some random or artiÔ¨Åcial initial conÔ¨Åguration x0 and then
make random changes Œ¥x in successive conÔ¨Ågurations x. One way to do this is
to make a small, random change Œ¥xi in coordinate xi and then to test whether
to accept this change by comparing the energies E(x) and E(x‚Ä≤) of the two con-
Ô¨Ågurations x and x‚Ä≤, which differ by Œ¥xi in coordinate xi. (Estimating these
energies is not trivial; Gromacs and TINKER can help.) It is important that this
random walk be symmetric, that is, the choice of testing whether to go from
x to x‚Ä≤ when one is at x should be exactly as likely as the choice of testing
whether to go from x‚Ä≤ to x when one is at x‚Ä≤. Also, the sequences of conÔ¨Åg-
urations should be ergodic; that is, from any conÔ¨Åguration x, one should be
able to get to any other conÔ¨Åguration x‚Ä≤ by a suitable sequence of changes
Œ¥xi = x‚Ä≤
i ‚àíxi.
How do we decide whether to accept or reject Œ¥xi? We use the following
Metropolis step. If the energy E‚Ä≤ = E(x‚Ä≤) of the new conÔ¨Åguration x‚Ä≤ is less
572

14.4 STATISTICAL MECHANICS
than the energy E(x) of the current conÔ¨Åguration x, then we accept the new
conÔ¨Åguration x‚Ä≤. If E‚Ä≤ > E, then we accept x‚Ä≤ with probability
P(x ‚Üíx‚Ä≤) = e‚àí(E‚Ä≤‚àíE)/kT.
(14.6)
In practice, one generates a pseudo-random number r ‚àà[0, 1] and accepts x‚Ä≤ if
r < e‚àí(E‚Ä≤‚àíE)/kT.
(14.7)
If one does not accept x‚Ä≤, then the system remains in conÔ¨Åguration x.
In FORTRAN90, the Metropolis step might be
if ( newE <= oldE ) then ! accept
x(i) = x(i) + dx
else ! accept conditionally
call random_number(r)
if ( r <= exp(-beta*(newE - oldE))) then ! accept
x(i) = x(i) + dx
end if
end if
in which Œ≤ = 1/kT.
One then varies another coordinate, such as xi+1. Once one has varied all
of the coordinates, one has Ô¨Ånished a sweep through the system. After thou-
sands or millions of such sweeps, the system is said to be thermalized. Once
the system is thermalized, one can start measuring properties of the system.
One computes a physical quantity every hundred or every thousand sweeps and
takes the average of these measurements. That average is the mean value of the
physical quantity at temperature T.
Why does this work? Consider two conÔ¨Ågurations x and x‚Ä≤, which respec-
tively have energies E
=
E(x) and E‚Ä≤
=
E(x‚Ä≤) and are occupied with
probabilities Pt(x) and Pt(x‚Ä≤) as the system is thermalizing. If E‚Ä≤ > E, then the
rate R(x‚Ä≤ ‚Üíx) of going from x‚Ä≤ to x is the rate v of choosing to test x when one
is at x‚Ä≤ times the probability Pt(x) of being at x, that is, R(x‚Ä≤ ‚Üíx) = v Pt(x‚Ä≤).
The reverse rate is R(x ‚Üíx‚Ä≤) = v Pt(x) e‚àí(E‚Ä≤‚àíE)/kT with the same v since the
random walk is symmetric. The net rate from x‚Ä≤ ‚Üíx then is
R(x‚Ä≤ ‚Üíx) ‚àíR(x ‚Üíx‚Ä≤) = v

Pt(x‚Ä≤) ‚àíPt(x) e‚àí(E‚Ä≤‚àíE)/kT
.
(14.8)
This net Ô¨Çow of probability from x‚Ä≤ ‚Üíx is positive if and only if
Pt(x‚Ä≤)/Pt(x) > e‚àí(E‚Ä≤‚àíE)/kT.
(14.9)
The probability distribution Pt(x) therefore Ô¨Çows with each sweep toward the
Boltzmann distribution exp(‚àíE(x)/kT). The Ô¨Çow slows and stops when the two
573

MONTE CARLO METHODS
rates are equal R(x‚Ä≤ ‚Üíx) = R(x ‚Üíx‚Ä≤), a condition called detailed balance. At
this equilibrium, the distribution Pt(x) satisÔ¨Åes Pt(x) = Pt(x‚Ä≤) e‚àí(E‚àíE‚Ä≤)/kT, in
which Pt(x‚Ä≤) eE‚Ä≤/kT is independent of x. So the thermalizing distribution Pt(x)
approaches the distribution P(x) = c e‚àíE/kT, in which c is independent of x.
Since the sum of these probabilities must be unity, we have

x
P(x) = c

x
e‚àíE/kT = 1,
(14.10)
which means that the constant c is the inverse of the partition function
Z(T) =

x
e‚àíE(x)/kT.
(14.11)
The thermalizing distribution approaches Boltzmann‚Äôs distribution (1.345)
Pt(x) ‚ÜíPB(x) = e‚àíE(x)/kT/Z(T).
(14.12)
Example 14.2 (Z2 lattice gauge theory)
First, one replaces space-time with a
lattice of points in d dimensions. Two nearest neighbor points are separated by
the lattice spacing a and joined by a link. Next, one puts an element U of the
gauge group on each link. For the Z2 gauge group (example 10.4), one assigns
an action Sp to each elementary square or plaquette of the lattice with vertices
1, 2, 3, and 4
Sp = 1 ‚àí1
3ReTr

U1,2U2,3U3,4U4,1

.
(14.13)
Then, one replaces E(x)/kT with Œ≤S, in which the action S is a sum of all
the plaquette actions Sp. More details are available at Michael Creutz‚Äôs website
(latticeguy.net/lattice.html).
Although the generation of conÔ¨Ågurations distributed according to the
Boltzmann probability distribution (1.345) is one of its most useful appli-
cations, the Monte Carlo method is much more general. It can generate
conÔ¨Ågurations x distributed according to any probability distribution P(x).
To generate conÔ¨Ågurations distributed according to P(x), we accept any new
conÔ¨Åguration x‚Ä≤ if P(x‚Ä≤) ‚â•P(x) and also accept x‚Ä≤ with probability
P(x ‚Üíx‚Ä≤) = P(x‚Ä≤)/P(x)
(14.14)
if P(x) > P(x‚Ä≤).
This works for the same reason that the Boltzmann version works. Consider
two conÔ¨Ågurations x and x‚Ä≤. If the system is thermalized, then the probabili-
ties Pt(x) and Pt(x‚Ä≤) have reached equilibrium, and so the rate R(x ‚Üíx‚Ä≤) from
x ‚Üíx‚Ä≤ must equal that R(x‚Ä≤ ‚Üíx) from x‚Ä≤ ‚Üíx. If P(x‚Ä≤) > P(x), then R(x‚Ä≤ ‚Üíx) is
R(x‚Ä≤ ‚Üíx) = v Pt(x‚Ä≤),
(14.15)
574

14.5 SOLVING ARBITRARY PROBLEMS
in which v is the rate of choosing Œ¥x = x‚Ä≤ ‚àíx, while the rate R(x ‚Üíx‚Ä≤) is
R(x ‚Üíx‚Ä≤) = v Pt(x) P(x‚Ä≤)/P(x)
(14.16)
with the same v since the random walk is symmetric. Equating the two rates
R(x‚Ä≤ ‚Üíx) = R(x ‚Üíx‚Ä≤)
(14.17)
we Ô¨Ånd that the Ô¨Çow of probability stops when
Pt(x) = P(x) Pt(x‚Ä≤)/P(x‚Ä≤) = cP(x),
(14.18)
where c is independent of x‚Ä≤. Thus Pt(x) ‚ÜíP(x).
So far we have assumed that the rate of choosing x ‚Üíx‚Ä≤ is the same as the
rate of choosing x‚Ä≤ ‚Üíx. In smart Monte Carlo schemes, physicists arrange the
rates vx‚Üíx‚Ä≤ and vx‚Ä≤‚Üíx so as to steer the Ô¨Çow and speed-up thermalization. To
compensate for this asymmetry, they change the second part of the Metropolis
step from x ‚Üíx‚Ä≤ when E‚Ä≤ = E(x‚Ä≤) > E = E(x) to accept conditionally with
probability
P(x ‚Üíx‚Ä≤) = P(x‚Ä≤) vx‚Ä≤‚Üíx/ [P(x) vx‚Üíx‚Ä≤] .
(14.19)
Now if P(x‚Ä≤) > P(x), then R(x‚Ä≤ ‚Üíx) is
R(x‚Ä≤ ‚Üíx) = vx‚Ä≤‚Üíx Pt(x‚Ä≤)
(14.20)
while the rate R(x ‚Üíx‚Ä≤) is
R(x ‚Üíx‚Ä≤) = vx‚Üíx‚Ä≤ Pt(x) P(x‚Ä≤) vx‚Ä≤‚Üíx/ [P(x) vx‚Üíx‚Ä≤] .
(14.21)
Equating the two rates R(x‚Ä≤ ‚Üíx) = R(x ‚Üíx‚Ä≤), we Ô¨Ånd
Pt(x‚Ä≤) = Pt(x) P(x‚Ä≤)/P(x).
(14.22)
That is Pt(x) = P(x) Pt(x‚Ä≤)/P(x‚Ä≤), which gives
Pt(x) = N P(x)
(14.23)
where N is a constant of normalization.
14.5 Solving arbitrary problems
If you know how to generate a suitably large space of trial solutions to a
problem, and you also know how to compare the quality of any two of your
solutions, then you can use a Monte Carlo method to solve it. The hard parts
of this seemingly magical method are characterizing a big enough space of solu-
tions s and constructing a quality function or functional that assigns a number
Q(s) to every solution in such a way that if s is a better solution than s‚Ä≤, then
Q(s) > Q(s‚Ä≤).
(14.24)
575

MONTE CARLO METHODS
But once one has characterized the space of possible solutions s and has
constructed the quality function Q(s), then one simply generates huge numbers
of random solutions and selects the one that maximizes the function Q(s) over
the space of all solutions.
If one can characterize the solutions as vectors of a certain dimension,
s = (x1, . . . , xn), then one may use the Monte Carlo method of the previous
section (14.4) by replacing ‚àíE(s) with Q(s) and kT with a parameter of the
same dimension as Q(s), nominally dimensionless.
14.6 Evolution
The reader may think that the use of Monte Carlo methods to solve arbi-
trary problems is quite a stretch. Yet Nature has applied them to the problem
of evolving species that survive. As a measure of the quality Q(s) of a given
solution s, Nature used the time derivative of the logarithm of its population
ÀôP(t)/P(t). The space of solutions is the set of possible genomes. We may ideal-
ize each solution or genome as a sequence of nucleotides s = b1b2 . . . bN some
thousands or billions of bases long, each base bk being adenine, cytosine, gua-
nine, or thymine (A, C, G, or T). Since there are four choices for each base,
the set of solutions is huge. The genome for homo sapiens has some 3 billion
bases (or base pairs, DNA being double stranded), and so the solution space is
a set with
N = 43√ó109 = 101.8√ó109
(14.25)
elements. By comparison, a googol is only 10100.
In evolution, a Metropolis step begins with a random change in the sequence
of bases; changes in a germ-line cell can create a new individual. Some of these
changes are due to errors in the normal mechanisms by which genomes are
copied and repaired. The (holo)enzyme DNA polymerase copies DNA with
remarkable Ô¨Ådelity, making one error in every billion base pairs copied. Along a
given line of descent, only about one nucleotide pair in a thousand is randomly
changed in the germ line every million years. Yet in a population of 10,000
diploid individuals, every possible nucleotide substitution will have been tried
out on about 20 occasions during a million years (Alberts et al., 2008).
RNA polymerases transcribe DNA into RNA, and RNAs play many roles:
Ribosomes translate messenger RNAs (mRNAs) into proteins, which are
sequences of amino acids; ribosomal RNAs (rRNAs) combine with proteins to
form ribosomes; long noncoding RNAs (ncRNAs) regulate the rates at which
different genes are transcribed; micro RNAs (miRNAs) regulate the rates at
which different mRNAs are translated into proteins; and other RNAs have
other as yet unknown functions. So a change of one base, e.g. from A to C,
might alter a protein or change the expression of a gene or be silent.
576

EXERCISES
Sexual reproduction makes bigger random changes in genomes. In meio-
sis, the paternal and maternal versions of each of our 23 chromosomes are
duplicated, and the four versions swap segments of DNA in a process called
genetic recombination or crossing-over. The cell then divides twice produc-
ing four haploid germ cells each with a single paternal, maternal, or mixed
version of each chromosome. This second kind of Metropolis step makes evo-
lution more ergodic, which is why most complex modern organisms use sexual
reproduction.
Other genomic changes occur when a virus inserts its DNA into that of a cell
and when transposable elements (transposons) of DNA move to different sites
in a genome.
In evolution, the rest of the Metropolis step is done by the new individual:
if he or she survives and multiplies, then the change is accepted; if he or she
dies without progeny, then the change is rejected. Evolution is slow, but it has
succeeded in turning a soup of simple molecules into humans with brains of
100 billion neurons, each with 1000 connections to other neurons.
John Holland and others have incorporated analogs of these Metropolis
steps into Monte Carlo techniques called genetic algorithms for solving wide
classes of problems (Holland, 1975; Vose, 1999; Schmitt, 2001).
Evolution also occurs at the cellular level when a cell mutates enough to
escape the control imposed on its proliferation by its neighbors and transforms
into a cancer cell.
Further reading
The classic Quarks, Gluons, and Lattices (Creutz, 1983) is a marvelous intro-
duction to the subject; Creutz‚Äôs Website (latticeguy.net/lattice.html) is an
extraordinary resource.
Exercises
14.1 Go to Michael Creutz‚Äôs website (latticeguy.net/lattice.html) and get his C-code
for Z2 lattice gauge theory. Compile and run it, and make a graph that exhibits
strong hysteresis as you raise and lower Œ≤ = 1/kT.
14.2 Modify his code and produce a graph showing the coexistence of two phases
at the critical coupling Œ≤t = 0.5 ln(1 +
‚àö
2). Hint: do a cold start and then
100 updates at Œ≤t, then do a random start and do 100 updates at Œ≤t. Plot the
values of the action against the update number 1, 2, 3, ..., 100.
14.3 Modify Creutz‚Äôs C code for Z2 lattice gauge theory so as to be able to vary the
dimension d of space-time. Show that for d = 2, there‚Äôs no phase transition;
for d = 3, there‚Äôs a second-order phase transition; and for d = 4, there‚Äôs a
Ô¨Årst-order phase transition.
14.4 What happens when d = 5?
577

15
Functional derivatives
15.1 Functionals
A functional G[f ] is a map from a space of functions to a set of numbers. For
instance, the action functional S[q] for a particle in one dimension maps the
coordinate q(t), which is a function of the time t, into a number ‚Äì the action of
the process. If the particle has mass m and is moving slowly and freely, then for
the interval (t1, t2) its action is
S0[q] =
 t2
t1
dt m
2
dq(t)
dt
2
.
(15.1)
If the particle is moving in a potential V(q(t)), then its action is
S[q] =
 t2
t1
dt

m
2
dq(t)
dt
2
‚àíV(q(t))

.
(15.2)
15.2 Functional derivatives
A functional derivative is a functional
Œ¥G[f ][h] = d
dœµ G[f + œµh]

œµ=0
(15.3)
of a functional. For instance, if Gn[f ] is the functional
Gn[f ] =

dx f n(x)
(15.4)
578

15.2 FUNCTIONAL DERIVATIVES
then its functional derivative is the functional that maps the pair of functions
f , h to the number
Œ¥Gn[f ][h] = d
dœµ Gn[f + œµh]

œµ=0
= d
dœµ

dx (f (x) + œµh(x))n

œµ=0
=

dx nf n‚àí1(x)h(x).
(15.5)
Physicists often use the less elaborate notation
Œ¥G[f ]
Œ¥f (y) = Œ¥G[f ][Œ¥y],
(15.6)
in which the function h(x) is Œ¥y(x) = Œ¥(x ‚àíy). Thus in the preceding example
Œ¥G[f ]
Œ¥f (y) =

dx nf n‚àí1(x)Œ¥(x ‚àíy) = nf n‚àí1(y).
(15.7)
Functional derivatives of functionals that involve powers of derivatives also
are easily dealt with. Suppose that the functional involves the square of the
derivative f ‚Ä≤(x)
G[f ] =

dx

f ‚Ä≤(x)
2 .
(15.8)
Then its functional derivative is
Œ¥G[f ][h] = d
dœµ G[f + œµh]

œµ=0
= d
dœµ

dx

f ‚Ä≤(x) + œµh‚Ä≤(x)
2

œµ=0
=

dx 2f ‚Ä≤(x)h‚Ä≤(x) = ‚àí2

dx f ‚Ä≤‚Ä≤(x)h(x),
(15.9)
in which we have integrated by parts and used suitable boundary conditions on
h(x) to drop the surface terms. In physics notation, we have
Œ¥G[f ]
Œ¥f (y) = ‚àí2

dx f ‚Ä≤‚Ä≤(x)Œ¥(x ‚àíy) = ‚àí2f ‚Ä≤‚Ä≤(y).
(15.10)
Let‚Äôs now compute the functional derivative of the action (15.2), which
involves the square of the time-derivative Àôq(t) and the potential energy V(q(t))
579

FUNCTIONAL DERIVATIVES
Œ¥S[q][h] = d
dœµ S[q + œµh]

œµ=0
= d
dœµ

dt
m
2
Àôq(t) + œµÀôh(t)
2 ‚àíV(q(t) + œµh(t))

œµ=0
=

dt

mÀôq(t)Àôh(t) ‚àíV‚Ä≤(q(t))h(t)

=

dt

‚àím¬®q(t) ‚àíV‚Ä≤(q(t))

h(t)
(15.11)
where we once again have integrated by parts and used suitable boundary
conditions to drop the surface terms. In physics notation, this is
Œ¥S[q]
Œ¥q(t) =

dt‚Ä≤ 
‚àím¬®q(t‚Ä≤) ‚àíV‚Ä≤(q(t‚Ä≤))

Œ¥(t ‚àít‚Ä≤) = ‚àím¬®q(t) ‚àíV‚Ä≤(q(t)).
(15.12)
In these terms, the stationarity of the action S[q] is the vanishing of its
functional derivative either in the form
Œ¥S[q][h] = 0
(15.13)
for arbitrary functions h(t) (that satisfy the boundary conditions) or equiva-
lently in the form
Œ¥S[q]
Œ¥q(t) = 0,
(15.14)
which is Lagrange‚Äôs equation of motion
m¬®q(t) = ‚àíV‚Ä≤(q(t)).
(15.15)
Physicists also use the compact notation
Œ¥2Z[j]
Œ¥j(y)Œ¥j(z) ‚â°‚àÇ2Z[j + œµŒ¥y + œµ‚Ä≤Œ¥z]
‚àÇœµ ‚àÇœµ‚Ä≤

œµ=œµ‚Ä≤=0
(15.16)
in which Œ¥y(x) = Œ¥(x ‚àíy) and Œ¥z(x) = Œ¥(x ‚àíz).
Example 15.1 (Shortest path is a straight line)
On a plane, the length of the
path (x, y(x)) from (x0, y0) to (x1, y1) is
L[y] =
 x1
x0

dx2 + dy2 =
 x1
x0

1 + y‚Ä≤2 dx.
(15.17)
The shortest path y(x) minimizes this length L[y]
Œ¥L[f ]
Œ¥y
= d
dœµ L[y + œµh]

œµ=0
= d
dœµ
 x1
x0

1 + (y‚Ä≤ + œµh‚Ä≤)2 dx

œµ=0
=
 x1
x0
y‚Ä≤h‚Ä≤

1 + y‚Ä≤2 dx = ‚àí
 x1
x0
h d
dx
y‚Ä≤

1 + y‚Ä≤2 dx
(15.18)
580

15.3 HIGHER-ORDER FUNCTIONAL DERIVATIVES
since h(x) satisÔ¨Åes h(x0) = h(x1) = 0. Differentiating, we set
Œ¥L[y][h] =
 x1
x0
h
y‚Ä≤‚Ä≤
1 + y‚Ä≤2 dx = 0,
(15.19)
which can vanish for arbitrary functions h(x) only if y‚Ä≤‚Ä≤ = 0, which is to say only
if y(x) is a straight line, y = mx + b.
15.3 Higher-order functional derivatives
The second functional derivative is
Œ¥2G[f ][h] = d2
dœµ2 G[f + œµh]|œµ=0 .
(15.20)
So if GN[f ] is the functional
GN[f ] =

f N(x)dx
(15.21)
then
Œ¥2GN[f ][h] = d2
dœµ2 GN[f + œµh]|œµ=0
= d2
dœµ2

(f (x) + œµh(x))N dx

œµ=0
= d2
dœµ2
 N
2

œµ2h2(x)f N‚àí2(x) dx

œµ=0
= N(N ‚àí1)

f N‚àí2(x)h2(x)dx.
(15.22)
Example 15.2 (Œ¥2S0)
The second functional derivative of the action S0[q]
(15.1) is
Œ¥2S0[q][h] = d2
dœµ2
 t2
t1
dt m
2
dq(t)
dt
+ œµ dh(t)
dt
2
œµ=0
=
 t2
t1
dt m
dh(t)
dt
2
‚â•0
(15.23)
and is positive for all functions h(t). The stationary classical trajectory
q(t) = t ‚àít1
t2 ‚àít1
q(t2) + t2 ‚àít
t2 ‚àít1
q(t1)
(15.24)
is a minimum of the action S0[q].
581

FUNCTIONAL DERIVATIVES
The second functional derivative of the action S[q] (15.2) is
Œ¥2S[q][h] = d2
dœµ2
 t2
t1
dt

m
2
dq(t)
dt
+ œµ dh(t)
dt
2
‚àíV(q(t) + œµh(t))

œµ=0
=
 t2
t1
dt

m
dh(t)
dt
2
‚àí2‚àÇ2V(q(t))
‚àÇq2(t)
h2(t)

(15.25)
and it can be positive, zero, or negative. Chaos sometimes arises in systems of
several particles when the second variation of S[q] about a stationary path is
negative, Œ¥2S[q][h] < 0, while Œ¥S[q][h] = 0.
The nth functional derivative is deÔ¨Åned as
Œ¥nG[f ][h] = dn
dœµn G[f + œµh]|œµ=0 .
(15.26)
The nth functional derivative of the same functional (15.21) is
Œ¥nGN[f ][h] =
N!
(N ‚àín)!

f N‚àín(x)hn(x)dx.
(15.27)
15.4 Functional Taylor series
It follows from the Taylor-series theorem (section 4.6) that
eŒ¥G[f ][h] =
‚àû

n=0
Œ¥n
n! G[f ][h] =
‚àû

n=0
1
n!
dn
dœµn G[f + œµh]

œµ=0
= G[f + œµh],
(15.28)
which illustrates an advantage of the present mathematical notation.
The functional S0[q] of Equation (15.1) provides a simple example of the
functional Taylor series (15.28):
eŒ¥S0[q][h] =

1 + d
dœµ + 1
2
d2
dœµ2

S0[q + œµh]

œµ=0
= m
2
 t2
t1

1 + d
dœµ + 1
2
d2
dœµ2

Àôq(t) + œµÀôh(t)
2 dt

œµ=0
= m
2
 t2
t1

Àôq2(t) + 2Àôq(t)Àôh(t) + Àôh2(t)

dt
= m
2
 t2
t1
Àôq(t) + Àôh(t)
2 dt = S0[q + h].
(15.29)
Note that if the function q(t) makes the action S0[q] stationary, and if h(t) is
smooth and vanishes at the endpoints of the time interval, then by (15.23)
S0[q + h] = S0[q] + S0[h],
(15.30)
582

15.5 FUNCTIONAL DIFFERENTIAL EQUATIONS
in which the functions q(t) and h(t) respectively satisfy the boundary conditions
on h(x) q(ti) = qi and h(t1) = h(t2) = 0.
More generally, if q(t) makes the action S[q] stationary, and h(t) is any loop
from and to the origin, then
S[q + h] = eŒ¥S[q][h] = S[q] +
‚àû

n=2
1
n!
dn
dœµn S[q + œµh]|œµ=0 .
(15.31)
If further S2[q] is purely quadratic in q and Àôq, like the harmonic oscillator, then
S2[q + h] = S2[q] + S2[h].
(15.32)
15.5 Functional differential equations
In inner products like ‚ü®q‚Ä≤|f ‚ü©, we represent the momentum operator as
p = ¬Øh
i
d
dq‚Ä≤
(15.33)
because then
‚ü®q‚Ä≤|p q|f ‚ü©= ¬Øh
i
d
dq‚Ä≤ ‚ü®q‚Ä≤|q|f ‚ü©= ¬Øh
i
d
dq‚Ä≤

q‚Ä≤‚ü®q‚Ä≤|f ‚ü©

=

¬Øh
i + q‚Ä≤ ¬Øh
i
d
dq‚Ä≤

‚ü®q‚Ä≤|f ‚ü©,
(15.34)
which respects the commutation relation [q, p] = i¬Øh.
So too in inner products ‚ü®œÜ‚Ä≤|f ‚ü©of eigenstates |œÜ‚Ä≤‚ü©of œÜ(x, t)
œÜ(x, t)|œÜ‚Ä≤‚ü©= œÜ‚Ä≤(x)|œÜ‚Ä≤‚ü©
(15.35)
we can represent the momentum œÄ(x, t) canonically conjugate to the Ô¨Åeld œÜ(x, t)
as the functional derivative
œÄ(x, t) = ¬Øh
i
Œ¥
Œ¥œÜ‚Ä≤(x)
(15.36)
because then
‚ü®œÜ‚Ä≤|œÄ(x‚Ä≤, t)œÜ(x, t)|f ‚ü©= ¬Øh
i
Œ¥
Œ¥œÜ‚Ä≤(x‚Ä≤)‚ü®œÜ‚Ä≤|œÜ(x, t)|f ‚ü©
= ¬Øh
i
Œ¥
Œ¥œÜ‚Ä≤(x‚Ä≤)

œÜ‚Ä≤(x)‚ü®œÜ‚Ä≤|f ‚ü©

(15.37)
= ¬Øh
i
Œ¥
Œ¥œÜ‚Ä≤(x‚Ä≤)

Œ¥(x ‚àíx‚Ä≤) œÜ‚Ä≤(x‚Ä≤) d3x‚Ä≤ ‚ü®œÜ‚Ä≤|f ‚ü©

= ¬Øh
i

Œ¥(x ‚àíx‚Ä≤) + œÜ‚Ä≤(x)
Œ¥
Œ¥œÜ‚Ä≤(x‚Ä≤)

‚ü®œÜ‚Ä≤|f ‚ü©
= ‚ü®œÜ‚Ä≤| ‚àíi¬ØhŒ¥(x ‚àíx‚Ä≤) + œÜ(x, t) œÄ(x‚Ä≤, t)|f ‚ü©,
583

FUNCTIONAL DERIVATIVES
which respects the equal-time commutation relation
[œÜ(x, t), œÄ(x‚Ä≤, t)] = i ¬Øh Œ¥(x ‚àíx‚Ä≤).
(15.38)
We can use the representation (15.36) for œÄ(x) to Ô¨Ånd the wave-function of
the ground state |0‚ü©of the hamiltonian
H = 1
2
 
œÄ2 + (‚àáœÜ)2 + m2œÜ2
d3x
(15.39)
where we set ¬Øh = c = 1. We will use a trick used to Ô¨Ånd the ground state |0‚ü©of
the harmonic-oscillator hamiltonian
H0 = p2
2m + mœâ2q2
2
.
(15.40)
In that trick, one writes
H0 = 1
2m(mœâq ‚àíip)(mœâq + ip) + iœâ
2 [p, q]
= 1
2m(mœâq ‚àíip)(mœâq + ip) + 1
2 ¬Øhœâ
(15.41)
and seeks a state |0‚ü©that is annihilated by mœâq + ip
‚ü®q‚Ä≤|mœâq + ip|0‚ü©=

mœâq‚Ä≤ + ¬Øh d
dq‚Ä≤

‚ü®q‚Ä≤|0‚ü©= 0.
(15.42)
The solution to this differential equation
d
dq‚Ä≤ ‚ü®q‚Ä≤|0‚ü©= ‚àímœâq‚Ä≤
¬Øh
‚ü®q‚Ä≤|0‚ü©
(15.43)
is
‚ü®q‚Ä≤|0‚ü©=
mœâ
œÄ ¬Øh
1/4
exp

‚àímœâq‚Ä≤2
2¬Øh

,
(15.44)
in which the prefactor is a constant of normalization.
So extending that trick to the hamiltonian (15.39), we factor H
H = 1
2
 
‚àí‚àá2 + m2 œÜ ‚àíiœÄ
 
‚àí‚àá2 + m2 œÜ + iœÄ

d3x + C,
(15.45)
in which C is the (inÔ¨Ånite) constant
C = i
2

[œÄ,

‚àí‚àá2 + m2 œÜ] d3x.
(15.46)
The ground state |0‚ü©of H must therefore satisfy the functional differential
equation
‚ü®œÜ‚Ä≤|

‚àí‚àá2 + m2 œÜ + iœÄ|0‚ü©= 0
(15.47)
584

EXERCISES
or
Œ¥‚ü®œÜ‚Ä≤|0‚ü©
Œ¥œÜ‚Ä≤(x) = ‚àí

‚àí‚àá2 + m2 œÜ‚Ä≤(x) ‚ü®œÜ‚Ä≤|0‚ü©.
(15.48)
The solution is
‚ü®œÜ‚Ä≤|0‚ü©= N exp

‚àí1
2

œÜ‚Ä≤(x)

‚àí‚àá2 + m2 œÜ‚Ä≤(x) d3x

,
(15.49)
in which N is a normalization constant. The spatial Fourier transform ÀúœÜ‚Ä≤(p)
œÜ‚Ä≤(x) =

eip¬∑x ÀúœÜ‚Ä≤(p) d3p
(2œÄ)3
(15.50)
satisÔ¨Åes ÀúœÜ‚Ä≤(‚àíp) = ÀúœÜ‚Ä≤‚àó(p) since œÜ‚Ä≤ is real. In terms of it, the ground-state wave-
function is
‚ü®œÜ‚Ä≤|0‚ü©= N exp

‚àí1
2

| ÀúœÜ‚Ä≤(p)|2

p2 + m2 d3p

.
(15.51)
Exercises
15.1 Compute the action S0[q] (15.1) for the classical path (15.24).
15.2 Use (15.25) to Ô¨Ånd a formula for the second functional derivative of the action
(15.2) of the harmonic oscillator for which V(q) = mœâ2q2/2.
15.3 Derive (15.51) from equations (15.49 & 15.50).
585

16
Path integrals
16.1 Path integrals and classical physics
Since Richard Feynman invented them over 60 years ago, path integrals have
been used with increasing frequency in high-energy and condensed-matter
physics, in Ô¨Ånance, and in biophysics (Kleinert, 2009). Feynman used them to
express matrix elements of the time-evolution operator exp(‚àíitH/¬Øh) in terms
of the classical action. Others have used them to compute matrix elements of
the Boltzmann operator exp(‚àíH/kT), which in the limit of zero temperature
projects out the ground state |E0‚ü©of the system
lim
T‚Üí0 e‚àí(H‚àíE0)/kT = lim
T‚Üí0
‚àû

n=0
|En‚ü©e‚àí(En‚àíE0)/kT‚ü®En| = |E0‚ü©‚ü®E0|,
(16.1)
a trick used in lattice gauge theory.
Path integrals magically express the quantum-mechanical probability ampli-
tude for a process as a sum of exponentials exp(iS/¬Øh) of the classical action S
of the various ways that process might occur.
16.2 Gaussian integrals
The path integrals we can do are gaussian integrals of inÔ¨Ånite order. So we begin
by recalling the basic integral formula (5.166)
 ‚àû
‚àí‚àû
exp

‚àíia

x ‚àíb
2a
2
dx =
&œÄ
ia ,
(16.2)
586

16.2 GAUSSIAN INTEGRALS
which holds for real a and b, and also the one (5.167)
 ‚àû
‚àí‚àû
exp
#
‚àír

x ‚àíc
2r
2$
dx =
&œÄ
r ,
(16.3)
which is true for positive r and complex c. Equivalent formulas for real a and b,
positive r, and complex c are
 ‚àû
‚àí‚àû
exp

‚àíiax2 + ibx

dx =
& œÄ
ia exp

i b2
4a

,
(16.4)
 ‚àû
‚àí‚àû
exp

‚àírx2 + cx

dx =
&œÄ
r exp

c2
4r

.
(16.5)
This last formula will be useful with x = p, r = œµ/(2m), and c = iœµÀôq
 ‚àû
‚àí‚àû
exp

‚àíœµ p2
2m + iœµ Àôq p

dp =
&
2œÄm
œµ
exp

‚àíœµ 1
2mÀôq2

(16.6)
as will (16.2) with x = p, a = œµ/(2m), and b = œµÀôq
 ‚àû
‚àí‚àû
exp

‚àíiœµ p2
2m + iœµ Àôq p

dp =
&
2œÄm
iœµ
exp

iœµ 1
2mÀôq2

.
(16.7)
Doable path integrals are multiple gaussian integrals. One may show (exer-
cise 16.1) that for positive r1, . . . , rN and complex c1, . . . , cN, the integral (16.5)
leads to
 ‚àû
‚àí‚àû
exp

i
‚àírix2
i + cixi
 N

i=1
dxi =
 N

i=1
&œÄ
ri

exp

1
4

i
c2
i
ri

.
(16.8)
If R is the N √ó N diagonal matrix with positive entries {r1, r2, . . . , rN}, and X
and C are N-vectors with real {xi} and complex {ci} entries, then this formula
(16.8) in matrix notation is
 ‚àû
‚àí‚àû
exp

‚àíX TRX + CTX
 N

i=1
dxi =
(
œÄN
det(R) exp
1
4 CTR‚àí1C

.
(16.9)
Now every positive symmetric matrix S is of the form S
=
OROT for
some positive diagonal matrix R. So inserting R = OTSO into the previous
equation (16.9) and using the invariance of determinants under orthogonal
transformations, we Ô¨Ånd
 ‚àû
‚àí‚àû
exp

‚àíX TOTSOX + CTX
 N

i=1
dxi =
(
œÄN
det(S) exp
#1
4 CTOTS‚àí1OC
$
.
(16.10)
587

PATH INTEGRALS
The jacobian of the orthogonal transformations Y = OX and D = OC is unity,
and so
 ‚àû
‚àí‚àû
exp

‚àíY TSY + DTY
 N

i=1
dyi =
(
œÄN
det(S) exp
1
4 DTS‚àí1D

,
(16.11)
in which S is a positive symmetric matrix, and D is a complex vector.
The other basic gaussian integral (16.4) leads for real S and D to (exer-
cise 16.2)
 ‚àû
‚àí‚àû
exp

‚àíiY TSY + iDTY
 N

i=1
dyi =
(
œÄN
det(iS) exp
 i
4 DTS‚àí1D

.
(16.12)
The vector Y that makes the argument ‚àíiY TSY + iDTY of the exponential
of this multiple gaussian integral (16.12) stationary is (exercise 16.3)
Y = 1
2S‚àí1D.
(16.13)
The exponential of that integral evaluated at its stationary point Y is
exp

‚àíiY TSY + iDTY

= exp
 i
4 DTS‚àí1D

.
(16.14)
Thus, the multiple gaussian integral (16.12) is equal to its exponential evalu-
ated at its stationary point Y, apart from a prefactor involving the determinant
det iS.
Similarly, the vector Y that makes the argument ‚àíY TSY +DTY of the expo-
nential of the multiple gaussian integral (16.11) stationary is Y = S‚àí1D/2, and
that exponential evaluated at Y is
exp

‚àíY TSY + DTY

= exp
1
4 DTS‚àí1D

.
(16.15)
Once again, a multiple gaussian integral is simply its exponential evaluated at
its stationary point Y, apart from a prefactor involving the determinant det S.
16.3 Path integrals in imaginary time
At the imaginary time t = ‚àíiŒ≤ ¬Øh, the time-evolution operator exp(‚àíitH/¬Øh) is
exp(‚àíŒ≤H), in which the inverse temperature Œ≤ = 1/kT is the reciprocal of
Boltzmann‚Äôs constant k = 8.617 √ó 10‚àí5 eV/K times the absolute temperature
T. In the low-temperature limit, exp(‚àíŒ≤H) is a projection operator (16.1) on
the ground state of the system. These path integrals in imaginary time are called
euclidean path integrals.
588

16.3 PATH INTEGRALS IN IMAGINARY TIME
Let us consider a quantum-mechanical system with hamiltonian
H = p2
2m + V(q),
(16.16)
in which the commutator of the position q and momentum p operators is
[q, p]
=
i in units in which ¬Øh
=
1. For tiny œµ, the corrections to the
approximation
exp

‚àíœµ

p2
2m + V(q)

‚âàexp

‚àíœµ p2
2m

exp ( ‚àíœµ V(q)) + O(œµ2) (16.17)
are of second order in œµ.
To evaluate the matrix element ‚ü®q‚Ä≤‚Ä≤| exp(‚àíœµH)|q‚Ä≤‚ü©, we insert the identity
operator I in the form of an integral over the momentum eigenstates
I =
 ‚àû
‚àí‚àû
|p‚Ä≤‚ü©‚ü®p‚Ä≤|dp‚Ä≤
(16.18)
and use the inner product ‚ü®q‚Ä≤‚Ä≤|p‚Ä≤‚ü©= exp(iq‚Ä≤‚Ä≤p‚Ä≤)/
‚àö
2œÄ so as to get as œµ ‚Üí0
‚ü®q‚Ä≤‚Ä≤| exp(‚àíœµH)|q‚Ä≤‚ü©=
 ‚àû
‚àí‚àû
‚ü®q‚Ä≤‚Ä≤| exp

‚àíœµ p2
2m

|p‚Ä≤‚ü©‚ü®p‚Ä≤| exp (‚àíœµ V(q)) |q‚Ä≤‚ü©dp‚Ä≤
= e‚àíœµ V(q‚Ä≤)
 ‚àû
‚àí‚àû
exp

‚àíœµ p‚Ä≤2
2m + i p‚Ä≤ (q‚Ä≤‚Ä≤ ‚àíq‚Ä≤)

dp‚Ä≤
2œÄ .
(16.19)
We now adopt the suggestive notation
q‚Ä≤‚Ä≤ ‚àíq‚Ä≤
œµ
= Àôq‚Ä≤
(16.20)
and use the integral formula (16.6) so as to obtain
‚ü®q‚Ä≤‚Ä≤| exp( ‚àíœµ H)|q‚Ä≤‚ü©= 1
2œÄ e‚àíœµ V(q‚Ä≤)
 ‚àû
‚àí‚àû
exp

‚àíœµ p‚Ä≤2
2m + i œµ p‚Ä≤ Àôq‚Ä≤

dp‚Ä≤
=
 m
2œÄœµ
1/2
exp
3
‚àíœµ

1
2 m Àôq ‚Ä≤2 + V(q‚Ä≤)
4
,
(16.21)
in which q‚Ä≤‚Ä≤ enters through the notation (16.20).
The next step is to link two of these matrix elements together
‚ü®q‚Ä≤‚Ä≤‚Ä≤|e‚àí2œµH|q‚Ä≤‚ü©=
 ‚àû
‚àí‚àû
‚ü®q‚Ä≤‚Ä≤‚Ä≤|e‚àíœµH|q‚Ä≤‚Ä≤‚ü©‚ü®q‚Ä≤‚Ä≤|e‚àíœµH|q‚Ä≤‚ü©dq‚Ä≤‚Ä≤
=
m
2œÄœµ
 ‚àû
‚àí‚àû
exp
3
‚àíœµ

1
2 m Àôq ‚Ä≤‚Ä≤2 + V(q‚Ä≤‚Ä≤) + 1
2 m Àôq ‚Ä≤2 + V(q‚Ä≤)
4
dq‚Ä≤‚Ä≤.
(16.22)
589

PATH INTEGRALS
Linking three of these matrix elements together and using subscripts instead of
primes, we have
‚ü®q3|e‚àí3œµH|q0‚ü©=
 ‚àû
‚àí‚àû
‚ü®q3|e‚àíœµH|q2‚ü©‚ü®q2|e‚àíœµH|q1‚ü©‚ü®q1|e‚àíœµH|q0‚ü©dq1dq2
=
 m
2œÄœµ
3/2 ‚àû
‚àí‚àû
exp
‚éß
‚é®
‚é©‚àíœµ
2

j=0

1
2 m Àôq2
j + V(qj)

‚é´
‚é¨
‚é≠dq1dq2.
(16.23)
Boldly passing from 3 to n and suppressing some integral signs, we get
‚ü®qn|e‚àínœµH|q0‚ü©=
 ‚àû
‚àí‚àû
‚ü®qn|e‚àíœµH|qn‚àí1‚ü©¬∑ ¬∑ ¬∑ ‚ü®q1|e‚àíœµH|q0‚ü©dq1 ¬∑ ¬∑ ¬∑ dqn‚àí1
=
 m
2œÄœµ
n/2 ‚àû
‚àí‚àû
exp
‚éß
‚é®
‚é©‚àíœµ
n‚àí1

j=0

1
2 m Àôq2
j + V(qj)

‚é´
‚é¨
‚é≠dq1 ¬∑ ¬∑ ¬∑ dqn‚àí1.
(16.24)
Writing dt for œµ and taking the limits œµ ‚Üí0 and nœµ ‚ÜíŒ≤, we Ô¨Ånd that the
matrix element ‚ü®qŒ≤|e‚àíŒ≤H|q0‚ü©is a path integral of the exponential of the average
energy multiplied by ‚àíŒ≤
‚ü®qŒ≤|e‚àíŒ≤H|q0‚ü©= N

exp
#
‚àí
 Œ≤
0
1
2mÀôq2(t) + V(q(t)) dt
$
Dq,
(16.25)
in which Dq = (n m/2œÄŒ≤)n/2dq1dq2 ¬∑ ¬∑ ¬∑ dqn‚àí1 as n ‚Üí‚àû. We sum over all paths
q(t) that go from q(0) = q0 at time 0 to q(Œ≤) = qŒ≤ at time Œ≤.
In the limit Œ≤ ‚Üí‚àû, the operator exp(‚àíŒ≤H) becomes proportional to a
projection operator (16.1) on the ground state of the theory.
In three-dimensional space, q(t) replaces q(t) in equation (16.25)
‚ü®qŒ≤|e‚àíŒ≤H|q0‚ü©= N

exp
#
‚àí
 Œ≤
0
1
2mÀôq2 + V(q) dt
$
Dq.
(16.26)
Path integrals in imaginary time are called euclidean mainly to distinguish
them from Minkowski path integrals, which represent matrix elements of the
time-evolution operator exp(‚àíitH) in real time.
16.4 Path integrals in real time
Path integrals in real time represent the time-evolution operator exp(‚àíitH).
Using the integral formula (16.7), we Ô¨Ånd in the limit œµ ‚Üí0
590

16.4 PATH INTEGRALS IN REAL TIME
‚ü®q‚Ä≤‚Ä≤|e‚àíi œµ H|q‚Ä≤‚ü©=
 ‚àû
‚àí‚àû
‚ü®q‚Ä≤‚Ä≤| exp

‚àíi œµ p2
2m

|p‚Ä≤‚ü©‚ü®p‚Ä≤| exp [ ‚àíi œµ V(q)] |q‚Ä≤‚ü©dp‚Ä≤
= 1
2œÄ e‚àíi œµ V(q‚Ä≤)
 ‚àû
‚àí‚àû
exp

‚àíi œµ p‚Ä≤2
2m + i p‚Ä≤ (q‚Ä≤‚Ä≤ ‚àíq‚Ä≤)

dp‚Ä≤
= 1
2œÄ e‚àíi œµ V(q‚Ä≤)
 ‚àû
‚àí‚àû
exp

‚àíi œµ p‚Ä≤2
2m + i œµ p‚Ä≤Àôq‚Ä≤

dp‚Ä≤
=
 m
2œÄiœµ
1/2
exp

i œµ

m Àôq‚Ä≤2
2
‚àíV(q‚Ä≤)

.
(16.27)
When we link together n of these matrix elements, we get the real-time version
of (16.24)
‚ü®qn|e‚àíinœµH|q0‚ü©=
 m
2œÄiœµ
n/2 ‚àû
‚àí‚àû
exp
‚éß
‚é®
‚é©iœµ
n‚àí1

j=0

1
2 m Àôq2
j ‚àíV(qj)

‚é´
‚é¨
‚é≠dq1 ¬∑ ¬∑ ¬∑ dqn‚àí1.
(16.28)
Writing dt for œµ and taking the limits œµ ‚Üí0 and nœµ ‚Üít, we Ô¨Ånd that the
amplitude ‚ü®qt|e‚àíitH|q0‚ü©is the path integral
‚ü®qt|e‚àíitH|q0‚ü©= N

exp
#
i
 t
0
1
2 m Àôq2 ‚àíV(q) dt‚Ä≤
$
Dq,
(16.29)
in which Dq differs from the one that appears in euclidean path integrals by the
substitution Œ≤ ‚Üíit:
Dq = lim
n‚Üí‚àû
 nm
2œÄit
n/2
dq1dq2 ¬∑ ¬∑ ¬∑ dqn.
(16.30)
The integral in the exponent is the classical action
S[q] =
 t
0
1
2 m Àôq2 ‚àíV(q) dt‚Ä≤
(16.31)
for a process q(t‚Ä≤) that runs from q(0) = q0 to q(t) = qt. We sum over all such
processes.
In three-dimensional space
‚ü®qt|e‚àíitH|q0‚ü©=

exp
#
i
 t
0
1
2 m Àôq2 ‚àíV(q) dt‚Ä≤
$
Dq
(16.32)
replaces (16.29).
The units of action are energy √ó time, and the argument of the exponential
must be dimensionless, so in ordinary units the amplitude (16.29) is
‚ü®qt|e‚àíitH/¬Øh|q0‚ü©=

eiS[q]/¬ØhDq.
(16.33)
591

PATH INTEGRALS
When is this amplitude big? When is it tiny? Suppose there is a process q(t) =
qc(t) that goes from qc(0) = q0 to qc(t) = qt in time t and that obeys the classical
equation of motion (15.14‚Äì15.15)
Œ¥S[qc]
Œ¥qc
= m¬®qc + V‚Ä≤(qc) = 0.
(16.34)
The action of such a classical process is stationary, that is, S[qc+dq] differs from
S[qc] only by terms of second order in Œ¥q. So there are inÔ¨Ånitely many other
processes that have the same action to within a fraction of ¬Øh. These processes
add with nearly the same phase to the path integral (16.33) and so make a huge
contribution to the amplitude ‚ü®qt|e‚àíitH/¬Øh|q0‚ü©.
But if no classical process goes from q0 to qt in time t, then the nonclassi-
cal processes from q0 to qt in time t have actions that differ among themselves
by large multiples of ¬Øh. Their amplitudes cancel each other, and so the result-
ing amplitude is tiny. Thus the real-time path integral explains the principle of
stationary action (section 11.37).
Does this path integral satisfy Schr√∂dinger‚Äôs equation? To see that it does,
we‚Äôll use (16.27) in the more explicit form
‚ü®q‚Ä≤‚Ä≤|e‚àíiœµH|q‚Ä≤‚ü©=
 m
2œÄiœµ
1/2
exp

i m(q‚Ä≤‚Ä≤ ‚àíq‚Ä≤)2
2œµ
‚àíi œµ V(q‚Ä≤)

(16.35)
to write œà(q‚Ä≤‚Ä≤, t + œµ) = ‚ü®q‚Ä≤‚Ä≤|œà, t + œµ‚ü©as an integral of œà(q‚Ä≤, t) = ‚ü®q‚Ä≤|œà, t‚ü©
‚ü®q‚Ä≤‚Ä≤|œà, t + œµ‚ü©=

‚ü®q‚Ä≤‚Ä≤|e‚àíiœµH|q‚Ä≤‚ü©‚ü®q‚Ä≤|œà, t‚ü©dq‚Ä≤
=
 m
2œÄiœµ
1/2 
exp

i m(q‚Ä≤‚Ä≤ ‚àíq‚Ä≤)2
2œµ
‚àíi œµ V(q‚Ä≤)

‚ü®q‚Ä≤|œà, t‚ü©dq‚Ä≤.
(16.36)
Keeping only leading terms in œµ, we have
œà(q‚Ä≤‚Ä≤, t + œµ) =
 m
2œÄiœµ
1/2
e‚àíiœµV(q‚Ä≤‚Ä≤)
 ‚àû
‚àí‚àû
exp

im(q‚Ä≤‚Ä≤ ‚àíq‚Ä≤)2
2œµ

œà(q‚Ä≤, t) dq‚Ä≤.
(16.37)
Letting x = q‚Ä≤ ‚àíq‚Ä≤‚Ä≤ and q‚Ä≤ = q‚Ä≤‚Ä≤ + x, we Ô¨Ånd
œà(q‚Ä≤‚Ä≤, t + œµ) =
 m
2œÄiœµ
1/2
e‚àíiœµV(q‚Ä≤‚Ä≤)

exp

imx2
2œµ

œà(q‚Ä≤‚Ä≤ + x, t) dx.
(16.38)
We now expand œà(q‚Ä≤‚Ä≤ + x, t) as
œà(q‚Ä≤‚Ä≤ + x, t) = œà(q‚Ä≤‚Ä≤, t) + xœà‚Ä≤(q‚Ä≤‚Ä≤, t) + 1
2x2œà‚Ä≤‚Ä≤(q‚Ä≤‚Ä≤, t) + ¬∑ ¬∑ ¬∑
(16.39)
592

16.5 PATH INTEGRAL FOR A FREE PARTICLE
and œà(q‚Ä≤‚Ä≤, t + œµ) as
œà(q‚Ä≤‚Ä≤, t + œµ) = œà(q‚Ä≤‚Ä≤, t) + œµ Àôœà(q‚Ä≤‚Ä≤, t) + ¬∑ ¬∑ ¬∑ .
(16.40)
The integral formula (16.2) implies
 ‚àû
‚àí‚àû
eimx2/2œµ dx =
2œÄiœµ
m
1/2
(16.41)
and its derivative with respect to im/2œµ gives
 ‚àû
‚àí‚àû
x2 eimx2/2œµ dx = iœµ
m
2œÄiœµ
m
1/2
while
 ‚àû
‚àí‚àû
x eimx2/2œµ dx = 0.
(16.42)
Substituting the expansions (16.39 & 16.40) for œà(q‚Ä≤‚Ä≤ + x, t) and œà(q‚Ä≤‚Ä≤, t + œµ)
into the integral (16.38) and using the integral formulas (16.41 & 16.42), we get
œà(q‚Ä≤‚Ä≤, t) + œµ Àôœà(q‚Ä≤‚Ä≤, t) =

1 ‚àíiœµV(q‚Ä≤‚Ä≤)
 #
œà(q‚Ä≤‚Ä≤, t) + iœµ
m œà‚Ä≤‚Ä≤(q‚Ä≤‚Ä≤, t)
$
,
(16.43)
which is Schr√∂dinger‚Äôs equation
i Àôœà = ‚àí1
2m œà‚Ä≤‚Ä≤ + V œà
(16.44)
in natural units or i¬Øh Àôœà = ‚àí¬Øh2 œà‚Ä≤‚Ä≤/2m + Vœà in arbitrary units.
16.5 Path integral for a free particle
The amplitude for a free nonrelativistic particle to go from the origin to the
point q in time t is the path integral (16.32)
‚ü®q|e‚àíitH|q = 0‚ü©=

eiS0[q] Dq =

exp

i
 t
0
1
2 m Àôq2(t‚Ä≤) dt‚Ä≤

Dq.
(16.45)
The classical path that goes from 0 to q in time t is qc(t‚Ä≤) = (t‚Ä≤/t) q. The general
path q(t‚Ä≤) over which we integrate is q(t‚Ä≤) = qc(t‚Ä≤) + Œ¥q(t‚Ä≤). Since both q(t‚Ä≤) and
qc(t‚Ä≤) go from 0 to q in time t, the otherwise arbitrary path Œ¥q(t‚Ä≤) must be a loop
that goes from Œ¥q(0) = 0 to Œ¥q(t) = 0 in time t. The velocity Àôq = Àôqc + ÀôŒ¥q is the
sum of the constant classical velocity Àôqc = q/t and the loop velocity ÀôŒ¥q. The
Ô¨Årst-order change vanishes
m
 t2
t1
Àôqc ¬∑ dŒ¥q
dt dt = m Àôqc ¬∑
 t2
t1
dŒ¥q
dt dt = m Àôqc ¬∑ [Œ¥q(t2) ‚àíŒ¥q(t1)] = 0
(16.46)
and so the action S0[q] is the classical action plus the loop action
S0[q] = 1
2 m
 t
0

Àôqc + ÀôŒ¥q
2
dt‚Ä≤ = S0[qc] + S0[Œ¥q].
(16.47)
593

PATH INTEGRALS
The path integral therefore factorizes
‚ü®q|e‚àíi t H|0‚ü©=

eiS0[q] Dq = N

eiS0[qc+Œ¥q] DŒ¥q
=

eiS0[qc] eiS0[Œ¥q] DŒ¥q
= eiS0[qc]

eiS0[Œ¥q] DŒ¥q
(16.48)
into the phase of the classical action times a path integral over the loops. The
loop integral L is independent of the spatial points q and 0 and so can only
depend upon the time interval, L = L(t). Thus the amplitude is the product
‚ü®q|e‚àíi t H|0‚ü©= eiS0[qc] L(t).
(16.49)
Since the classical velocity is Àôqc = q/t, the classical action is
S0[qc] =
 T
0
m
2 Àôq2
c(t) dt = m
2
q2
t .
(16.50)
So the amplitude is
‚ü®q|e‚àíi(t2‚àít1)H|0‚ü©= eimq2/2t L(t).
(16.51)
Since the position eigenstates are orthogonal, this amplitude must reduce to
a delta function as t ‚Üí0
lim
t‚Üí0 ‚ü®q|e‚àíi t H|0‚ü©= ‚ü®q|0‚ü©= Œ¥3(q).
(16.52)
One of the many representations of Dirac‚Äôs delta function is
Œ¥3(q) = lim
t‚Üí0

m
2œÄi¬Øht
3/2
eimq2/2¬Øht.
(16.53)
Thus L(t) = (m/2œÄit)3/2 and
‚ü®q|e‚àíitH/¬Øh|0‚ü©=

m
2œÄi¬Øht
3/2
eimq2/2¬Øht
(16.54)
in unnatural units. You can verify (exercise 16.6) this result by inserting a
complete set of momentum dyadics |p‚ü©‚ü®p| and doing the resulting Fourier
transform.
Example 16.1 (The Bohm‚ÄìAharonov effect)
From our formula (11.311) for the
action of a relativistic particle of mass m and charge q, we infer (exercise 16.7)
that the action of a nonrelativistic particle in an electromagnetic Ô¨Åeld with no
scalar potential is
594

16.7 HARMONIC OSCILLATOR IN REAL TIME
S =
 x2
x1
#1
2mv + q A
$
¬∑dx .
(16.55)
Now imagine that we shoot a beam of such particles past but not through a
narrow cylinder in which a magnetic Ô¨Åeld is conÔ¨Åned. The particles can go either
way around the cylinder of area S but can not enter the region of the magnetic
Ô¨Åeld. The difference in the phases of the amplitudes is the loop integral from the
source to the detector and back to the source
S
¬Øh
=
) mv
2 + q A

¬∑ dx
¬Øh =
) mv ¬∑ dx
2¬Øh
+ q
¬Øh

S
B ¬∑ dS =
) mv ¬∑ dx
2¬Øh
+ q
¬Øh ,
(16.56)
in which  is the magnetic Ô¨Çux through the cylinder.
16.6 Free particle in imaginary time
If we mimic the steps of the preceding section (16.5) in which the hamiltonian
is H = p2/2m, set Œ≤ = it/¬Øh = 1/kT, and use Dirac‚Äôs delta function
Œ¥3(q) = lim
t‚Üí0
 m
2œÄ ¬Øht
3/2
e‚àímq2/2¬Øht
(16.57)
then we get
‚ü®q|e‚àíŒ≤H|0‚ü©=

m
2œÄ ¬Øh2Œ≤
3/2
exp

‚àímq2
2¬Øh2Œ≤

=
 mkT
2œÄ ¬Øh2
3/2
e‚àímkTq2/2¬Øh2. (16.58)
To study the ground state of the system, we set Œ≤ = t/¬Øh and let t ‚Üí‚àûin
‚ü®q|e‚àítH/¬Øh|0‚ü©=
 m
2œÄ ¬Øht
3/2
exp

‚àím
2
q2
¬Øht

,
(16.59)
which for D = ¬Øh/(2m) is the solution (3.200 & 13.107) of the diffusion equation.
16.7 Harmonic oscillator in real time
Biologists have mice; physicists have harmonic oscillators with hamiltonian
H = p2
2m + mœâ2q2
2
.
(16.60)
For this hamiltonian, our formula (16.29) for the coordinate matrix elements of
the time-evolution operator exp(‚àíitH) is
‚ü®q‚Ä≤‚Ä≤|e‚àíi t H|q‚Ä≤‚ü©=

eiS[q]Dq
(16.61)
with action
S[q] =
 t
0
1
2mÀôq2(t‚Ä≤) ‚àí1
2mœâ2q2(t‚Ä≤) dt‚Ä≤.
(16.62)
595

PATH INTEGRALS
The classical solution qc(t) = q‚Ä≤ cos œât+ Àôq0 sin(œât)/œâ in which q‚Ä≤ = qc(0) and
Àôq0 = Àôqc(0) are the initial position and velocity makes the action S[q] stationary
d
dœµ S[q + œµh]

œµ=0
= 0
(16.63)
and satisÔ¨Åes the classical equation of motion ¬®qc(t) = ‚àíœâ2qc(t).
We now apply the trick (16.46‚Äì16.48) we used for the free particle. We write
an arbitrary process q(t) is the sum of the classical process qc(t) and a loop Œ¥q(t)
with Œ¥q(0) = Œ¥q(t) = 0. Since the action S[q] is quadratic in the variables q and
Àôq, the functional Taylor series (15.31) for S[qc + Œ¥q] has only two terms
S[qc + Œ¥q] = S[qc] + S[Œ¥q].
(16.64)
Thus we can write the path integral (16.61) as
‚ü®q‚Ä≤‚Ä≤|e‚àíitH|q‚Ä≤‚ü©=

eiS[q] Dq =

eiS[qc+Œ¥q] DŒ¥q
=

eiS[qc]+iS[Œ¥q] DŒ¥q = eiS[qc]

eiS[Œ¥q] DŒ¥q.
(16.65)
The remaining path integral over the loops Œ¥q does not involve the endpoints
q‚Ä≤ and q‚Ä≤‚Ä≤ and so must be a function L(t) of the time t but not of q‚Ä≤ or q‚Ä≤‚Ä≤
‚ü®q‚Ä≤‚Ä≤|e‚àíitH|q‚Ä≤‚ü©= eiS[qc] L(t).
(16.66)
The action S[qc] is (exercise 16.8)
S[qc] =
mœâ
2 sin(œât)

q‚Ä≤2 + q‚Ä≤‚Ä≤2
cos(œât) ‚àí2q‚Ä≤q‚Ä≤‚Ä≤
.
(16.67)
The action S[Œ¥q] of a loop
Œ¥q(t‚Ä≤) =
n‚àí1

j=1
aj sin jœÄt‚Ä≤
t
(16.68)
is (exercise 16.9)
S[Œ¥q] =
n‚àí1

j=1
mt
4 a2
j

(jœÄ)2
t2
‚àíœâ2

.
(16.69)
The path integral over the loops is then, apart from a constant jacobian J,

eiS[Œ¥q] DŒ¥q = J
 nm
2œÄit
n/2 
exp
‚éß
‚é®
‚é©
n‚àí1

j=1
imt
4 a2
j

(jœÄ)2
t2
‚àíœâ2
‚é´
‚é¨
‚é≠
n‚àí1

j=1
daj
= J
 nm
2œÄit
n/2 n‚àí1

j=1
 ‚àû
‚àí‚àû
exp
@
imt
4 a2
j

(jœÄ)2
t2
‚àíœâ2
A
daj.
(16.70)
596

16.8 HARMONIC OSCILLATOR IN IMAGINARY TIME
Using the gaussian integral (16.2) and the inÔ¨Ånite product (4.140), we get

eiS[Œ¥q] DŒ¥q = Jnn/2
& m
2œÄit
n‚àí1

j=1
‚àö
2
jœÄ

1 ‚àíœâ2t2
œÄ2j2
‚àí1/2
=
&
mœâ
2œÄi sin œât
‚éõ
‚éùlim
n‚Üí‚àûJnn/2
n‚àí1

j=1
‚àö
2
jœÄ
‚éû
‚é†.
(16.71)
Using (16.66) and (16.67), we see that the number within the parentheses is
unity because in that case we have (Feynman and Hibbs, 1965, ch. 3)
‚ü®q‚Ä≤‚Ä≤|e‚àíitH/¬Øh|q‚Ä≤‚ü©=
&
mœâ
2œÄi¬Øh sin(œât) exp

imœâ

q‚Ä≤2 + q‚Ä≤‚Ä≤2
cos(œât) ‚àí2q‚Ä≤q‚Ä≤‚Ä≤
2¬Øh sin(œât)

,
(16.72)
which agrees with the amplitude (16.54) in the limit t ‚Üí0 (exercise 16.11).
16.8 Harmonic oscillator in imaginary time
For the harmonic oscillator with hamiltonian (16.60), our formula (16.25) for
euclidean path integrals becomes
‚ü®q‚Ä≤‚Ä≤|e‚àíŒ≤H|q‚Ä≤‚ü©= N

exp

‚àí
 Œ≤
0
1
2mÀôq2(t) + 1
2mœâ2q2(t) dt
%
Dq.
(16.73)
The euclidean action, which is a time integral of the energy of the oscillator,
Se[q] =
 Œ≤
0

1
2mÀôq2(t) + 1
2mœâ2q2(t)

dt
(16.74)
is purely quadratic, and so we may play the trick (15.32) if we can Ô¨Ånd a path
qe(t) that makes it stationary
Œ¥Se[qe][h] = d
dœµ
 Œ≤
0
1
2m(Àôqe(t) + œµÀôh(t))2 + 1
2mœâ2(qe(t) + œµh(t))2 dt

œµ=0
=
 Œ≤
0
mÀôqe(t)Àôh(t) + mœâ2qe(t)h(t) dt
=
 Œ≤
0

‚àím¬®qe(t) + mœâ2qe(t)

h(t)dt = 0.
(16.75)
The path qe(t) must satisfy the euclidean equation of motion
¬®qe(t) = œâ2qe(t)
(16.76)
whose general solution is
qe(t) = Aeœât + Be‚àíœât.
(16.77)
597

PATH INTEGRALS
The path from qe(0) = q‚Ä≤ to qe(Œ≤) = q‚Ä≤‚Ä≤ must have
A = q‚Ä≤‚Ä≤e‚àíœâŒ≤ ‚àíq‚Ä≤e‚àí2œâŒ≤
1 ‚àíe‚àí2œâŒ≤
and
B = q‚Ä≤ ‚àíA.
(16.78)
Its action Se[qe] is (exercise 16.12)
Se[qe] = 1
2mœâ

A2 
e2œâŒ≤ ‚àí1

‚àíB2 
e‚àí2œâŒ≤ ‚àí1

.
(16.79)
Since the action is purely quadratic, the trick (15.32) tells us that the action
Se[q] of the arbitrary path q(t) = qe(t) + Œ¥q(t) is the sum
Se[q] = Se[qe] + Se[Œ¥q],
(16.80)
in which the action Se[Œ¥q] of the loop Œ¥q(t) depends but upon t but not upon
qŒ≤ or q0. It follows then that for some loop function L(Œ≤) of Œ≤ alone
‚ü®q‚Ä≤‚Ä≤|e‚àíŒ≤H|q‚Ä≤‚ü©= exp (‚àíSe[qe]) L(Œ≤)
= L(Œ≤) exp
3
‚àí1
2mœâ

A2 
e2œâŒ≤ ‚àí1

‚àíB2 
e‚àí2œâŒ≤ ‚àí1
4
.
(16.81)
To study the ground state of the harmonic oscillator, we let Œ≤ ‚Üí‚àûin this
equation. Inserting a complete set of eigenstates H|n‚ü©= En|n‚ü©, we see that the
limit of the left-hand side is
lim
Œ≤‚Üí‚àû‚ü®q‚Ä≤‚Ä≤|e‚àíŒ≤H|q‚Ä≤‚ü©= lim
Œ≤‚Üí‚àû‚ü®q‚Ä≤‚Ä≤|n‚ü©e‚àíŒ≤En‚ü®n|q‚Ä≤‚ü©= e‚àíŒ≤E0‚ü®q‚Ä≤‚Ä≤|0‚ü©‚ü®0|q‚Ä≤‚ü©.
(16.82)
Our formulas (16.78) for A and B say that A ‚Üíq‚Ä≤‚Ä≤e‚àíœâŒ≤ and B ‚Üíq‚Ä≤ as Œ≤ ‚Üí‚àû,
and so in this limit by (16.81 & 16.82) we have
e‚àíŒ≤E0‚ü®q‚Ä≤‚Ä≤|0‚ü©‚ü®0|q‚Ä≤‚ü©= L(Œ≤) exp

‚àí1
2mœâ

q‚Ä≤‚Ä≤2 + q‚Ä≤2
,
(16.83)
from which we may infer our earlier formula (15.44) for the wave-function of
the ground state
‚ü®q|0‚ü©=
mœâ
œÄ ¬Øh
1/4
exp

‚àí1
2
mœâq2
¬Øh

,
(16.84)
in which the prefactor ensures the normalization
1 =
 ‚àû
‚àí‚àû
|‚ü®q|0‚ü©|2dq.
(16.85)
Euclidean path integrals help one study ground states.
598

16.9 EUCLIDEAN CORRELATION FUNCTIONS
16.9 Euclidean correlation functions
In the Heisenberg picture, the position operator q(t) is
q(t) = eitHq e‚àíitH,
(16.86)
in which q = q(0) is the position operator at time t = 0 or equivalently the posi-
tion operator in the Schr√∂dinger picture. The analogous operator in imaginary
time is the euclidean position operator qe(t) deÔ¨Åned as
qe(t) = etHq e‚àítH
(16.87)
obtained from q(t) by replacing t by ‚àíit.
The euclidean product of two euclidean position operators is
T [qe(t1) qe(t2)] = Œ∏(t1 ‚àít2)qe(t1) qe(t2) + Œ∏(t2 ‚àít1)qe(t2) qe(t1),
(16.88)
in which Œ∏(x) = (x + |x|)/2|x| is Heaviside‚Äôs function. We can use the method
of section 16.3 to compute the matrix element of the euclidean-time-ordered
product T [q(t1)q(t2)] sandwiched between two factors of exp(‚àítH). For t1 ‚â•t2,
this matrix element is
‚ü®qt|e‚àítHqe(t1)qe(t2)e‚àítH|q‚àít‚ü©= ‚ü®qt|e‚àí(t‚àít1)Hqe‚àí(t1‚àít2)Hqe‚àí(t+t2)H|q‚àít‚ü©.
(16.89)
Then instead of the path-integral formula (16.25), we get
‚ü®qt|e‚àítHT [qe(t1)qe(t2)] e‚àítH|q‚àít‚ü©=

q(t1)q(t2)e‚àíSe[q,t,‚àít] Dq
(16.90)
where as in (16.25) Se[q, t, ‚àít] is the euclidean action
Se[q, t, ‚àít] =
 t
‚àít
1
2mÀôq2(t‚Ä≤) + V(q(t‚Ä≤)) dt‚Ä≤
(16.91)
or the time integral of the energy. As in the path integral (16.25), the integration
is over all paths that go from q(‚àít) = q‚àít to q(t) = qt. The analog of (16.25) is
‚ü®qt|e‚àí2tH|q‚àít‚ü©=

e‚àíSe[q,t,‚àít] Dq
(16.92)
and the factors (nm/2œÄŒ≤)n/2 cancel in the ratio of (16.90) to (16.92)
‚ü®qt|e‚àítHT [qe(t1)qe(t2)] e‚àítH|q‚àít‚ü©
‚ü®qt|e‚àí2tH|q‚àít‚ü©
=

q(t1)q(t2)e‚àíSe[q,t,‚àít] Dq

e‚àíSe[q,t,‚àít] Dq
.
(16.93)
599

PATH INTEGRALS
In the limit t ‚Üí‚àû, the operator exp(‚àítH) projects out the ground state |0‚ü©
of the system
lim
t‚Üí‚àûe‚àítH|q‚àít‚ü©= lim
t‚Üí‚àû
‚àû

n=0
e‚àítH|n‚ü©‚ü®n|q‚àít‚ü©= lim
t‚Üí‚àûe‚àítE0|0‚ü©‚ü®0|q‚àít‚ü©,
(16.94)
which we assume to be unique and normalized to unity. In the ratio (16.93),
most of these factors cancel, leaving us with
‚ü®0|T [qe(t1)qe(t2)] |0‚ü©=

q(t1)q(t2)e‚àíSe[q,‚àû,‚àí‚àû] Dq

e‚àíSe[q,‚àû,‚àí‚àû] Dq
.
(16.95)
More generally, the mean value in the ground state |0‚ü©of any euclidean-time-
ordered product of position operators q(ti) is a ratio of path integrals
‚ü®0| T [q(t1) ¬∑ ¬∑ ¬∑ q(tn)] |0‚ü©=

q(t1) ¬∑ ¬∑ ¬∑ q(tn) e‚àíSe[q] Dq

e‚àíSe[q] Dq
,
(16.96)
in which Se[q] stands for Se[q, ‚àû, ‚àí‚àû]. Why do we need the time-ordered prod-
uct T on the LHS? Because successive factors of exp(‚àí(tk ‚àít‚Ñì)H) lead to the
path integral of exp(‚àíSe[q]). Why don‚Äôt we need T on the RHS? Because the
q(ti)s are real numbers which commute with each other.
The result (16.96) is important because it can be generalized to all quantum
theories, including Ô¨Åeld theories.
16.10 Finite-temperature Ô¨Åeld theory
Matrix elements of the operator exp(‚àíŒ≤H) where Œ≤ = 1/kT tell us what a
system is like at temperature T. In the low-temperature limit, they describe the
ground state of the system.
Quantum mechanics imposes upon n coordinates qi and conjugate momenta
œÄk the commutation relations
[qi, pk] = i Œ¥i,k
and
[qi, qk] = [pi, pk] = 0.
(16.97)
In quantum Ô¨Åeld theory, we associate a coordinate qx ‚â°œÜ(x) and a conjugate
momentum px ‚â°œÄ(x) with each point x of space and impose upon them the
very similar commutation relations
[œÜ(x), œÄ(y)] = i Œ¥(x ‚àíy)
and
[œÜ(x), œÜ(y)] = [œÄ(x), œÄ(y)] = 0.
(16.98)
Just as in quantum mechanics the time derivative of a coordinate is its commu-
tator with a hamiltonian Àôqi = i[H, qi], so too in quantum Ô¨Åeld theory the time
600

16.10 FINITE-TEMPERATURE FIELD THEORY
derivative of a Ô¨Åeld ÀôœÜ(x, t) ‚â°ÀôœÜ(x) is ÀôœÜ(x) = i[H, œÜ(x)]. A typical hamiltonian
for a single scalar Ô¨Åeld œÜ is
H =
 #1
2œÄ2(x) + 1
2 (‚àáœÜ(x))2 + 1
2m2œÜ2(x) + P(œÜ(x))
$
d3x,
(16.99)
in which P is a quartic polynomial.
Since quantum Ô¨Åeld theory is just the quantum mechanics of many variables,
we can use the methods of sections 16.3 & 16.4 to write matrix elements of
exp(‚àíŒ≤H) as path integrals. We deÔ¨Åne a potential
V(œÜ(x)) = 1
2 (‚àáœÜ(x))2 + 1
2m2œÜ2(x) + P(œÜ(x))
(16.100)
and write the hamiltonian H as
H =
 #1
2œÄ2(x) + V(œÜ(x))
$
d3x.
(16.101)
Like |q‚Ä≤‚ü©and |p‚Ä≤‚ü©, the states |œÜ‚Ä≤‚ü©and |œÄ‚Ä≤‚ü©are eigenstates of the hermitian
operators œÜ(x, 0) and œÄ(x, 0)
œÜ(x, 0)|œÜ‚Ä≤‚ü©= œÜ‚Ä≤(x)|œÜ‚Ä≤‚ü©
and
œÄ(x, 0)|œÄ‚Ä≤‚ü©= œÄ‚Ä≤(x)|œÄ‚Ä≤‚ü©.
(16.102)
The analog of ‚ü®q‚Ä≤|p‚Ä≤‚ü©is
‚ü®œÜ‚Ä≤|œÄ‚Ä≤‚ü©= f exp
#
i

œÜ‚Ä≤(x)œÄ‚Ä≤(x)d3x
$
,
(16.103)
in which f is a factor which eventually will cancel.
Repeating our derivation of equation (16.21) with
DœÄ‚Ä≤ ‚â°

x
dœÄ‚Ä≤(x)
(16.104)
we Ô¨Ånd in the limit œµ ‚Üí0
‚ü®œÜ‚Ä≤‚Ä≤| exp(‚àíœµH)|œÜ‚Ä≤‚ü©=

‚ü®œÜ‚Ä≤‚Ä≤| exp

‚àíœµ
2

œÄ2(x)d3x

|œÄ‚Ä≤‚ü©
√ó ‚ü®œÄ‚Ä≤| exp

‚àíœµ

V(œÜ(x)) d3x

|œÜ‚Ä≤‚ü©DœÄ‚Ä≤
= |f |2 exp

‚àíœµ

V(œÜ‚Ä≤(x)) d3x

√ó

exp
#
‚àí1
2œµœÄ‚Ä≤2(x) + iœÄ‚Ä≤(x)[œÜ‚Ä≤‚Ä≤(x) ‚àíœÜ‚Ä≤(x)]d3x
$
DœÄ‚Ä≤.
(16.105)
Using the abbreviation
ÀôœÜ‚Ä≤(x) ‚â°œÜ‚Ä≤‚Ä≤(x) ‚àíœÜ‚Ä≤(x)
œµ
(16.106)
601

PATH INTEGRALS
and the integral formula (16.6), we get
‚ü®œÜ‚Ä≤‚Ä≤| exp(‚àíœµH)|œÜ‚Ä≤‚ü©= f ‚Ä≤ exp

‚àíœµ
 
1
2 ÀôœÜ‚Ä≤2(x) + V(œÜ‚Ä≤(x))

d3x
%
.
Putting together n = Œ≤/œµ such terms and integrating over the intermediate
states |œÜ‚Ä≤‚Ä≤‚Ä≤‚ü©‚ü®œÜ‚Ä≤‚Ä≤‚Ä≤|, and absorbing the normalizing factors into DœÜ, we have
‚ü®œÜŒ≤|e‚àíŒ≤H|œÜ0‚ü©=
 œÜŒ≤
œÜ0
exp
#
‚àí
 Œ≤
0

1
2 ÀôœÜ2(x) + V(œÜ(x)) d3xdt
$
DœÜ.
(16.107)
Replacing the potential V(œÜ) with its deÔ¨Ånition (16.100), we Ô¨Ånd
‚ü®œÜŒ≤|e‚àíŒ≤H|œÜ0‚ü©=
 œÜŒ≤
œÜ0
exp
#
‚àí
 Œ≤
0

1
2

ÀôœÜ2 + (‚àáœÜ)2 + m2œÜ2
+ P(œÜ) d3xdt
$
DœÜ,
(16.108)
in which the limits œÜ0 and œÜŒ≤ remind us that we are to integrate over all Ô¨Åelds
œÜ(x, t) that run from œÜ(x, 0) = œÜ0(x) to œÜ(x, Œ≤) = œÜŒ≤(x).
In terms of the energy density
H(œÜ) ‚â°1
2

(‚àÇaœÜ)2 + m2œÜ2
+ P(œÜ),
(16.109)
in which a is summed from 0 to 3, the path integral (16.108) is
‚ü®œÜŒ≤|e‚àíŒ≤H|œÜ0‚ü©=
 œÜŒ≤
œÜ0
exp
#
‚àí
 Œ≤
0

H(œÜ) d3xdt
$
DœÜ.
(16.110)
The partition function Z(Œ≤) ‚Äì deÔ¨Åned as the trace Z(Œ≤) ‚â°Tr e‚àíŒ≤H over all
states of the system ‚Äì is an integral over all loop Ô¨Åelds (ones for which œÜŒ≤ and
œÜ0 coincide)
Z(Œ≤) ‚â°Tr e‚àíŒ≤H =
 œÜ0
œÜ0
‚ü®œÜ|e‚àíŒ≤H|œÜ‚ü©DœÜ = N
 œÜ0
œÜ0
exp
#
‚àí
 Œ≤
0

H(œÜ) d3xdt
$
DœÜ.
(16.111)
Because the four space-time derivatives in H(œÜ) occur with the same sign, Ô¨Ånite-
temperature Ô¨Åeld theory is called euclidean quantum Ô¨Åeld theory. The density
operator œÅ for the system described by the hamiltonian H in equilibrium at
temperature T is œÅ = exp(‚àíŒ≤H)/Z(Œ≤).
Like the deÔ¨Ånition (16.87) of the euclidean position operator qe(t), the
euclidean Ô¨Åeld operator œÜe(x) is deÔ¨Åned as
œÜe(x, t) = etHœÜ(x, 0)e‚àítH.
(16.112)
The euclidean-time-ordered product (16.88 ) of two Ô¨Åelds is
T [œÜe(x1, t1)œÜe(x2, t2)] = Œ∏(t1 ‚àít2)et1HœÜe(x1, 0)e‚àí(t1‚àít2)HœÜe(x2, 0)e‚àít2H
+ Œ∏(t2 ‚àít1)et2HœÜe(x2, 0)e‚àí(t2‚àít1)HœÜe(x1, 0)e‚àít1H.
602

16.11 REAL-TIME FIELD THEORY
The logic of equations (16.87‚Äì16.96) leads us to write its mean value in a sys-
tem described by a stationary density operator ‚Äì one that commutes with the
hamiltonian ‚Äì as the ratio
‚ü®T [œÜe(x1)œÜe(x2)]‚ü©= Tr {œÅ T [œÜe(x1)œÜe(x2)]}
= Tr

e‚àíŒ≤H T [œÜe(x1)œÜe(x2)]

Tr

e‚àíŒ≤H
=
 œÜ0
œÜ0
œÜ(x1)œÜ(x2) exp
#
‚àí
 Œ≤
0

H(œÜ) d3x dt
$
DœÜ
 œÜ0
œÜ0
exp
#
‚àí
 Œ≤
0

H(œÜ) d3x dt
$
DœÜ
,
(16.113)
in which all normalization factors have canceled.
In the zero-temperature (Œ≤ ‚Üí‚àû) limit, the density operator œÅ becomes the
projection operator |0‚ü©‚ü®0| on the ground state, and mean-value formulas like
(16.113) become
‚ü®0|T [œÜe(x1) ¬∑ ¬∑ ¬∑ œÜe(xn)] |0‚ü©=

œÜ(x1) ¬∑ ¬∑ ¬∑ œÜ(xn) exp
#
‚àí

H(œÜ) d4x
$
DœÜ

exp
#
‚àí

H(œÜ) d4x
$
DœÜ
,
(16.114)
in which Hamilton‚Äôs density H(œÜ) is integrated over all of euclidean space-time
and over all Ô¨Åelds that are periodic on the inÔ¨Ånite time interval. Statistical Ô¨Åeld
theory and lattice gauge theory use formulas like (16.113) and (16.114).
16.11 Real-time Ô¨Åeld theory
We now follow the derivation of section 16.10 using the same notation but for
real time. In (16.105), we replace ‚àíœµH by ‚àíiœµH and follow the logic of sections
(16.4 & 16.10). We Ô¨Ånd in the limit œµ ‚Üí0 with ÀôœÜ‚Ä≤ ‚â°(œÜ‚Ä≤‚Ä≤ ‚àíœÜ‚Ä≤)/œµ
‚ü®œÜ‚Ä≤‚Ä≤|e‚àíiœµH|œÜ‚Ä≤‚ü©=

‚ü®œÜ‚Ä≤‚Ä≤|e‚àíiœµ
2
œÄ2/2 d3x|œÄ‚Ä≤‚ü©‚ü®œÄ‚Ä≤|e‚àíiœµ
2
V(œÜ) d3x|œÜ‚Ä≤‚ü©DœÄ‚Ä≤
= |f |2e‚àíiœµ
2
V(œÜ‚Ä≤)d3x

e‚àíiœµ
2
œÄ‚Ä≤2/2+iœÄ‚Ä≤(œÜ‚Ä≤‚Ä≤‚àíœÜ‚Ä≤) d3xDœÄ‚Ä≤
= f ‚Ä≤ exp
#
iœµ

1
2 ÀôœÜ‚Ä≤2 ‚àíV(œÜ‚Ä≤) d3x
$
.
(16.115)
Putting together 2t/œµ similar factors and integrating over all the intermediate
states |œÜ‚ü©‚ü®œÜ|, we arrive at the path integral
‚ü®œÜ‚Ä≤‚Ä≤|e‚àíi2tH|œÜ‚Ä≤‚ü©=
 œÜ‚Ä≤‚Ä≤
œÜ‚Ä≤ exp
#
i

1
2 ÀôœÜ2(x) ‚àíV(œÜ(x)) d4x
$
DœÜ,
(16.116)
603

PATH INTEGRALS
in which we integrate over all Ô¨Åelds œÜ(x) that run from œÜ‚Ä≤(x, ‚àít) to œÜ‚Ä≤‚Ä≤(x, t). After
expanding the deÔ¨Ånition (16.100) of the potential V(œÜ), we have
‚ü®œÜ‚Ä≤‚Ä≤|e‚àíi2tH|œÜ‚Ä≤‚ü©=
 œÜ‚Ä≤‚Ä≤
œÜ‚Ä≤ exp
#
i

1
2

ÀôœÜ2 ‚àí(‚àáœÜ)2 ‚àím2œÜ2
‚àíP(œÜ) d4x
$
DœÜ.
(16.117)
This amplitude is a path integral
‚ü®œÜ‚Ä≤‚Ä≤|e‚àíi2tH|œÜ‚Ä≤‚ü©=
 œÜ‚Ä≤‚Ä≤
œÜ‚Ä≤ eiS[œÜ] DœÜ
(16.118)
of phases exp(iS[œÜ]) that are exponentials of the classical action
S[œÜ] =

1
2

ÀôœÜ2 ‚àí(‚àáœÜ)2 ‚àím2œÜ2
‚àíP(œÜ) d4x.
(16.119)
The time dependence of the Heisenberg Ô¨Åeld operator œÜ(x, t) is
œÜ(x, t) = eitHœÜ(x, 0)e‚àíitH.
(16.120)
The time-ordered product of two Ô¨Åelds, as in (16.88), is the sum
T [œÜ(x1)œÜ(x2)] = Œ∏(x0
1 ‚àíx0
2)œÜ(x1)œÜ(x2) + Œ∏(x0
2 ‚àíx0
1)œÜ(x2)œÜ(x1).
(16.121)
Between two factors of exp(‚àíitH), it is for t1 > t2
e‚àíitHT [œÜ(x1)œÜ(x2)] e‚àíitH = e‚àíi(t‚àít1)HœÜ(x1, 0)e‚àíi(t1‚àít2)HœÜ(x2, 0)e‚àíi(t‚àít2)H.
So by the logic that led to the path-integral formulas (16.113) and (16.118), we
can write a matrix element of the time-ordered product (16.121) as
‚ü®œÜ‚Ä≤‚Ä≤|e‚àíitHT [œÜ(x1)œÜ(x2)] e‚àíitH|œÜ‚Ä≤‚ü©=
 œÜ‚Ä≤‚Ä≤
œÜ‚Ä≤ œÜ(x1) œÜ(x2) eiS[œÜ] DœÜ,
(16.122)
in which we integrate over Ô¨Åelds that go from œÜ‚Ä≤ at time ‚àít to œÜ‚Ä≤‚Ä≤ at time t. The
time-ordered product of any combination of Ô¨Åelds is then
‚ü®œÜ‚Ä≤‚Ä≤|e‚àíitHT [œÜ(x1) ¬∑ ¬∑ ¬∑ œÜ(xn)] e‚àíitH|œÜ‚Ä≤‚ü©=

œÜ(x1) ¬∑ ¬∑ ¬∑ œÜ(xn) eiS[œÜ] DœÜ.
(16.123)
Like the position eigenstates |q‚Ä≤‚ü©of quantum mechanics, the eigenstates |œÜ‚Ä≤‚ü©
are states of inÔ¨Ånite energy that overlap most states. Yet we often are interested
in the ground state |0‚ü©or in states of a few particles. To form such matrix ele-
ments, we multiply both sides of equations (16.118 & 16.123) by ‚ü®0|œÜ‚Ä≤‚Ä≤‚ü©‚ü®œÜ‚Ä≤|0‚ü©
604

16.12 PERTURBATION THEORY
and integrate over œÜ‚Ä≤ and œÜ‚Ä≤‚Ä≤. Since the ground state is a normalized eigenstate
of the hamiltonian H|0‚ü©= E0|0‚ü©with eigenvalue E0, we Ô¨Ånd from (16.118)

‚ü®0|œÜ‚Ä≤‚Ä≤‚ü©‚ü®œÜ‚Ä≤‚Ä≤|e‚àíi2tH|œÜ‚Ä≤‚ü©‚ü®œÜ‚Ä≤|0‚ü©DœÜ‚Ä≤‚Ä≤DœÜ‚Ä≤ = ‚ü®0|e‚àíi2tH|0‚ü©
= e‚àíi2tE0 =

‚ü®0|œÜ‚Ä≤‚Ä≤‚ü©eiS[œÜ]‚ü®œÜ‚Ä≤|0‚ü©DœÜDœÜ‚Ä≤‚Ä≤DœÜ‚Ä≤
(16.124)
and from (16.123) suppressing the differentials DœÜ‚Ä≤‚Ä≤DœÜ‚Ä≤,
e‚àí2itE0‚ü®0|T [œÜ(x1) ¬∑ ¬∑ ¬∑ œÜ(xn)] |0‚ü©=

‚ü®0|œÜ‚Ä≤‚Ä≤‚ü©œÜ(x1) ¬∑ ¬∑ ¬∑ œÜ(xn) eiS[œÜ]‚ü®œÜ‚Ä≤|0‚ü©DœÜ.
(16.125)
The mean value in the ground state of a time-ordered product of Ô¨Åeld operators
is then a ratio of these path integrals
‚ü®0|T [œÜ(x1) ¬∑ ¬∑ ¬∑ œÜ(xn)] |0‚ü©=

‚ü®0|œÜ‚Ä≤‚Ä≤‚ü©œÜ(x1) ¬∑ ¬∑ ¬∑ œÜ(xn) eiS[œÜ]‚ü®œÜ‚Ä≤|0‚ü©DœÜ

‚ü®0|œÜ‚Ä≤‚Ä≤‚ü©eiS[œÜ]‚ü®œÜ‚Ä≤|0‚ü©DœÜ
,
(16.126)
in which factors involving E0 have canceled and the integration is over all Ô¨Åelds
that go from œÜ(x, ‚àít) = œÜ‚Ä≤(x) to œÜ(x, t) = œÜ‚Ä≤‚Ä≤(x) and over œÜ‚Ä≤(x) and œÜ‚Ä≤‚Ä≤(x).
16.12 Perturbation theory
Field theories with hamiltonians that are quadratic in their Ô¨Åelds like
H0 =

1
2

œÄ2(x) + (‚àáœÜ(x))2 + m2œÜ2(x)

d3x
(16.127)
are soluble. Their Ô¨Åelds evolve in time as
œÜ(x, t) = eitH0œÜ(x, 0)e‚àíitH0.
(16.128)
The mean value in the ground state of H0 of a time-ordered product of these
Ô¨Åelds is by (16.126) a ratio of path integrals
‚ü®0|T [œÜ(x1) ¬∑ ¬∑ ¬∑ œÜ(xn)] |0‚ü©=

‚ü®0|œÜ‚Ä≤‚Ä≤‚ü©œÜ(x1) ¬∑ ¬∑ ¬∑ œÜ(xn) eiS0[œÜ]‚ü®œÜ‚Ä≤|0‚ü©DœÜ

‚ü®0|œÜ‚Ä≤‚Ä≤‚ü©eiS0[œÜ]‚ü®œÜ‚Ä≤|0‚ü©DœÜ
,
(16.129)
605

PATH INTEGRALS
in which the action S0[œÜ] is quadratic in the Ô¨Åelds
S0[œÜ] =

1
2

ÀôœÜ2(x) ‚àí(‚àáœÜ(x))2 ‚àím2œÜ2(x)

d4x
=

1
2

‚àí‚àÇaœÜ(x)‚àÇaœÜ(x) ‚àím2œÜ2(x)

d4x.
(16.130)
So the path integrals in the ratio (16.129) are gaussian and doable.
The Fourier transforms
ÀúœÜ(p) =

e‚àíipxœÜ(x) d4x
and
œÜ(x) =

eipx ÀúœÜ(p) d4p
(2œÄ)4
(16.131)
turn the space-time derivatives in the action into a quadratic form
S0[œÜ] = ‚àí1
2

| ÀúœÜ(p)|2 (p2 + m2) d4p
(2œÄ)4 ,
(16.132)
in which p2 = p2 ‚àíp02, and ÀúœÜ(‚àíp) = ÀúœÜ‚àó(p) by (3.25) since the Ô¨Åeld œÜ is real.
The initial ‚ü®œÜ‚Ä≤|0‚ü©and Ô¨Ånal ‚ü®0|œÜ‚Ä≤‚Ä≤‚ü©wave-functions produce the iœµ in the
Feynman propagator (5.233). Although its exact form doesn‚Äôt matter here, the
wave-function ‚ü®œÜ‚Ä≤|0‚ü©of the ground state of H0 is the exponential (15.51)
‚ü®œÜ‚Ä≤|0‚ü©= c exp

‚àí1
2

| ÀúœÜ‚Ä≤(p)|2

p2 + m2 d3p
(2œÄ)3

,
(16.133)
in which ÀúœÜ‚Ä≤(p) is the spatial Fourier transform
ÀúœÜ‚Ä≤(p) =

e‚àíip¬∑x œÜ‚Ä≤(x) d3x
(16.134)
and c is a normalization factor that will cancel in ratios of path integrals.
Apart from ‚àí2i ln c, which we will not keep track of, the wave-functions
‚ü®œÜ‚Ä≤|0‚ü©and ‚ü®0|œÜ‚Ä≤‚Ä≤‚ü©add to the action S0[œÜ] the term
S0[œÜ] = i
2
 
p2 + m2

| ÀúœÜ(p, t)|2 + | ÀúœÜ(p, ‚àít)|2
d3p
(2œÄ)3 ,
(16.135)
in which we envision taking the limit t ‚Üí‚àûwith œÜ(x, t) = œÜ‚Ä≤‚Ä≤(x) and œÜ(x, ‚àít) =
œÜ‚Ä≤(x). The identity (Weinberg, 1995, pp. 386‚Äì388)
f (+‚àû) + f (‚àí‚àû) = lim
œµ‚Üí0+ œµ
 ‚àû
‚àí‚àû
f (t) e‚àíœµ|t| dt
(16.136)
allows us to write S0[œÜ] as
S0[œÜ] = lim
œµ‚Üí0+
iœµ
2
 
p2 + m2
 ‚àû
‚àí‚àû
| ÀúœÜ(p, t)|2 e‚àíœµ|t| dt d3p
(2œÄ)3 .
(16.137)
606

16.12 PERTURBATION THEORY
To Ô¨Årst order in œµ, the change in the action is (exercise 16.15)
S0[œÜ] = lim
œµ‚Üí0+
iœµ
2
 
p2 + m2
 ‚àû
‚àí‚àû
| ÀúœÜ(p, t)|2 dt d3p
(2œÄ)3
= lim
œµ‚Üí0+
iœµ
2
 
p2 + m2 | ÀúœÜ(p)|2 d4p
(2œÄ)4 .
(16.138)
The modiÔ¨Åed action is therefore
S0[œÜ, œµ] = S0[œÜ] + S0[œÜ] = ‚àí1
2

| ÀúœÜ(p)|2

p2 + m2 ‚àíiœµ

p2 + m2

d4p
(2œÄ)4
= ‚àí1
2

| ÀúœÜ(p)|2 
p2 + m2 ‚àíiœµ

d4p
(2œÄ)4
(16.139)
since the square-root is positive. In terms of the modiÔ¨Åed action, our formula
(16.129) for the time-ordered product is the ratio
‚ü®0|T [œÜ(x1) ¬∑ ¬∑ ¬∑ œÜ(xn)] |0‚ü©=

œÜ(x1) ¬∑ ¬∑ ¬∑ œÜ(xn) eiS0[œÜ,œµ] DœÜ

eiS0[œÜ,œµ] DœÜ
.
(16.140)
We can use this formula (16.140) to express the mean value in the vacuum |0‚ü©
of the time-ordered exponential of a space-time integral of j(x)œÜ(x), in which
j(x) is a classical (c-number, external) current j(x), as the ratio
Z0[j] ‚â°‚ü®0| T

exp
#
i

j(x) œÜ(x) d4x
$%
|0‚ü©
=

exp
#
i

j(x) œÜ(x) d4x
$
eiS0[œÜ,œµ] DœÜ

eiS0[œÜ,œµ] DœÜ
.
(16.141)
Since the state |0‚ü©is normalized, the mean value Z0[0] is unity,
Z0[0] = 1.
(16.142)
If we absorb the current into the action
S0[œÜ, œµ, j] = S0[œÜ, œµ] +

j(x) œÜ(x) d4x
(16.143)
then in terms of the current‚Äôs Fourier transform
Àúj(p) =

e‚àíipx j(x) d4x
(16.144)
607

PATH INTEGRALS
the modiÔ¨Åed action S0[œÜ, œµ, j] is (exercise 16.15)
S0[œÜ, œµ, j] = ‚àí1
2
 
| ÀúœÜ(p)|2 
p2 + m2 ‚àíiœµ

‚àíÀúj‚àó(p) ÀúœÜ(p) ‚àíÀúœÜ‚àó(p)Àúj(p)
 d4p
(2œÄ)4 .
(16.145)
Changing variables to
Àúœà(p) = ÀúœÜ(p) ‚àíÀúj(p)/(p2 + m2 ‚àíiœµ)
(16.146)
we write the action S0[œÜ, œµ, j] as (exercise 16.17)
S0[œÜ, œµ, j] = ‚àí1
2
 
| Àúœà(p)|2 
p2 + m2 ‚àíiœµ

‚àí
Àúj‚àó(p)Àúj(p)
(p2 + m2 ‚àíiœµ)

d4p
(2œÄ)4
= S0[œà, œµ] + 1
2
 
Àúj‚àó(p)Àúj(p)
(p2 + m2 ‚àíiœµ)

d4p
(2œÄ)4 .
(16.147)
And since DœÜ = Dœà, our formula (16.141) gives simply (exercise 16.18)
Z0[j] = exp

i
2

|Àúj(p)|2
p2 + m2 ‚àíiœµ
d4p
(2œÄ)4

.
(16.148)
Going back to position space, one Ô¨Ånds (exercise 16.19)
Z0[j] = exp
# i
2

j(x) (x ‚àíx‚Ä≤) j(x‚Ä≤) d4x d4x‚Ä≤
$
,
(16.149)
in which (x ‚àíx‚Ä≤) is Feynman‚Äôs propagator (5.233)
(x ‚àíx‚Ä≤) =

eip(x‚àíx‚Ä≤)
p2 + m2 ‚àíiœµ
d4p
(2œÄ)4 .
(16.150)
The functional derivative (chapter 15) of Z0[j], deÔ¨Åned by (16.141), is
1
i
Œ¥Z0[j]
Œ¥j(x) = ‚ü®0| T
#
œÜ(x) exp

i

j(x‚Ä≤)œÜ(x‚Ä≤)d4x‚Ä≤
$
|0‚ü©
(16.151)
while that of equation (16.149) is
1
i
Œ¥Z0[j]
Œ¥j(x) = Z0[j]

(x ‚àíx‚Ä≤) j(x‚Ä≤) d4x‚Ä≤.
(16.152)
Thus the second functional derivative of Z0[j] evaluated at j = 0 gives
‚ü®0| T

œÜ(x)œÜ(x‚Ä≤)

|0‚ü©= 1
i2
Œ¥2Z0[j]
Œ¥j(x)Œ¥j(x‚Ä≤)

j=0
= ‚àíi (x ‚àíx‚Ä≤).
(16.153)
608

16.13 APPLICATION TO QUANTUM ELECTRODYNAMICS
Similarly, one may show (exercise 16.20) that
‚ü®0| T [œÜ(x1)œÜ(x2)œÜ(x3)œÜ(x4)] |0‚ü©= 1
i4
Œ¥4Z0[j]
Œ¥j(x1)Œ¥j(x2)Œ¥j(x3)Œ¥j(x4)

j=0
= ‚àí(x1 ‚àíx2)(x3 ‚àíx4) ‚àí(x1 ‚àíx3)(x2 ‚àíx4)
‚àí(x1 ‚àíx4)(x2 ‚àíx3).
(16.154)
Suppose now that we add a potential V = P(œÜ) to the free hamiltonian
(16.127). Scattering amplitudes are matrix elements of the time-ordered expo-
nential T exp

‚àíi
2
P(œÜ) d4x

. Our formula (16.140) for the mean value in the
ground state |0‚ü©of the free hamiltonian H0 of any time-ordered product of
Ô¨Åelds leads us to
‚ü®0|T

exp
#
‚àíi

P(œÜ) d4x
$%
|0‚ü©=

exp
#
‚àíi

P(œÜ) d4x
$
eiS0[œÜ,œµ] DœÜ

eiS0[œÜ,œµ] DœÜ
.
(16.155)
Using (16.153 & 16.154), we can cast this expression into the magical form
‚ü®0|T

exp
#
‚àíi

P(œÜ) d4x
$%
|0‚ü©= exp
#
‚àíi

P

Œ¥
iŒ¥j(x)

d4x
$
Z0[j]

j=0
.
(16.156)
The generalization of the path-integral formula (16.140) to the ground state
|‚ü©of an interacting theory with action S is
‚ü®|T [œÜ(x1) ¬∑ ¬∑ ¬∑ œÜ(xn)] |‚ü©=

œÜ(x1) ¬∑ ¬∑ ¬∑ œÜ(xn) eiS[œÜ,œµ] DœÜ

eiS[œÜ,œµ] DœÜ
,
(16.157)
in which a term like iœµœÜ2 is added to make the modiÔ¨Åed action S[œÜ, œµ].
These are some of the techniques one uses to make states of incoming
and outgoing particles and to compute scattering amplitudes (Weinberg, 1995,
1996; Srednicki, 2007; Zee, 2010).
16.13 Application to quantum electrodynamics
In the Coulomb gauge ‚àá¬∑ A = 0, the QED hamiltonian is
H = Hm +
 
1
2œÄ2 + 1
2(‚àá√ó A)2 ‚àíA ¬∑ j

d3x + VC,
(16.158)
609

PATH INTEGRALS
in which Hm is the matter hamiltonian, and VC is the Coulomb term
VC = 1
2
 j0(x, t)j0(y, t)
4œÄ|x ‚àíy|
d3xd3y.
(16.159)
The operators A and œÄ are canonically conjugate, but they satisfy the Coulomb-
gauge conditions
‚àá¬∑ A = 0
and
‚àá¬∑ œÄ = 0.
(16.160)
One may show (Weinberg, 1995, pp. 413‚Äì418) that in this theory, the analog
of equation (16.157) is
‚ü®|T [O1 ¬∑ ¬∑ ¬∑ On] |‚ü©=

O1 ¬∑ ¬∑ ¬∑ On eiSC Œ¥[‚àá¬∑ A] DA Dœà

eiSC Œ¥[‚àá¬∑ A] DA Dœà
,
(16.161)
in which the Coulomb-gauge action is
SC =

1
2 ÀôA
2 ‚àí1
2(‚àá√ó A)2 + A ¬∑ j + Lm d4x ‚àí

VC dt
(16.162)
and the functional delta function
Œ¥[‚àá¬∑ A] =

x
Œ¥(‚àá¬∑ A(x))
(16.163)
enforces the Coulomb-gauge condition. The term Lm is the action density of
the matter Ô¨Åeld œà.
Tricks are available. We introduce a new Ô¨Åeld A0(x) and consider the factor
F =

exp
#
i

1
2

‚àáA0 + ‚àá‚ñ≥‚àí1j02
d4x
$
DA0,
(16.164)
which is just a number independent of the charge density j0 since we can cancel
the j0 term by shifting A0. By integrating by parts, we can write the number F
as (exercise 16.21)
F =

exp
#
i

1
2

‚àáA02
‚àíA0j0 ‚àí1
2j0‚ñ≥‚àí1j0 d4x
$
DA0
=

exp
#
i

1
2

‚àáA02
‚àíA0j0 d4x + i

VC dt
$
DA0.
(16.165)
So when we multiply the numerator and denominator of the amplitude (16.161)
by F, the awkward Coulomb term cancels, and we get
‚ü®|T [O1 ¬∑ ¬∑ ¬∑ On] |‚ü©=

O1 ¬∑ ¬∑ ¬∑ On eiS‚Ä≤ Œ¥[‚àá¬∑ A] DA Dœà

eiS‚Ä≤ Œ¥[‚àá¬∑ A] DA Dœà
(16.166)
610

16.13 APPLICATION TO QUANTUM ELECTRODYNAMICS
where now DA includes all four components AŒº and
S‚Ä≤ =

1
2 ÀôA
2 ‚àí1
2(‚àá√ó A)2 + 1
2

‚àáA02
+ A ¬∑ j ‚àíA0j0 + Lm d4x.
(16.167)
Since the delta function Œ¥[‚àá¬∑ A] enforces the Coulomb-gauge condition, we can
add to the action S‚Ä≤ the term (‚àá¬∑ ÀôA) A0, which is ‚àíÀôA ¬∑ ‚àáA0, after we integrate
by parts and drop the surface term. This extra term makes the action gauge
invariant
S =

1
2( ÀôA ‚àí‚àáA0)2 ‚àí1
2(‚àá√ó A)2 + A ¬∑ j ‚àíA0j0 + Lm d4x
=

‚àí1
4Fab Fab + Abjb + Lm d4x.
(16.168)
Thus at this point we have
‚ü®|T [O1 ¬∑ ¬∑ ¬∑ On] |‚ü©=

O1 ¬∑ ¬∑ ¬∑ On eiS Œ¥[‚àá¬∑ A] DA Dœà

eiS Œ¥[‚àá¬∑ A] DA Dœà
,
(16.169)
in which S is the gauge-invariant action (16.168), and the integral is over all
Ô¨Åelds. The only relic of the Coulomb gauge is the gauge-Ô¨Åxing delta functional
Œ¥[‚àá¬∑ A].
We now make the gauge transformation
A‚Ä≤
b(x) = Ab(x) + ‚àÇb(x)
and
œà‚Ä≤(x) = eiq(x)œà(x)
(16.170)
and replace the Ô¨Åelds Ab(x) and œà(x) everywhere in the numerator and (sepa-
rately) in the denominator in the ratio (16.169) of path integrals by their gauge
transforms (16.170) A‚Ä≤
Œº(x) and œà‚Ä≤(x). This change of variables changes nothing;
it‚Äôs like replacing
2 ‚àû
‚àí‚àûf (x) dx by
2 ‚àû
‚àí‚àûf (y) dy, and so
‚ü®|T [O1 ¬∑ ¬∑ ¬∑ On] |‚ü©= ‚ü®|T [O1 ¬∑ ¬∑ ¬∑ On] |‚ü©‚Ä≤,
(16.171)
in which the prime refers to the gauge transformation (16.170).
We‚Äôve seen that the action S is gauge invariant. So is the measure DA Dœà,
and we now restrict ourselves to operators O1 . . . On that are gauge invariant.
So in the right-hand side of equation (16.171), the replacement of the Ô¨Åelds by
their gauge transforms affects only the term Œ¥[‚àá¬∑ A] that enforces the Coulomb-
gauge condition
‚ü®|T [O1 ¬∑ ¬∑ ¬∑ On] |‚ü©=

O1 ¬∑ ¬∑ ¬∑ On eiS Œ¥[‚àá¬∑ A + ‚ñ≥) DA Dœà

eiS Œ¥[‚àá¬∑ A + ‚ñ≥) DA Dœà
.
(16.172)
611

PATH INTEGRALS
We now have two choices. If we integrate over all gauge functions (x) in
both the numerator and the denominator of this ratio (16.172), then apart from
over-all constants that cancel, the mean value in the vacuum of the time-ordered
product is the ratio
‚ü®|T [O1 ¬∑ ¬∑ ¬∑ On] |‚ü©=

O1 ¬∑ ¬∑ ¬∑ On eiS DA Dœà

eiS DA Dœà
,
(16.173)
in which we integrate over all matter Ô¨Åelds, gauge Ô¨Åelds, and gauges. That is, we
do not Ô¨Åx the gauge.
The analogous formula for the euclidean time-ordered product is
‚ü®|Te [O1 ¬∑ ¬∑ ¬∑ On] |‚ü©=

O1 ¬∑ ¬∑ ¬∑ On e‚àíSe DA Dœà

e‚àíSe DA Dœà
,
(16.174)
in which the euclidean action Se is the space-time integral of the energy den-
sity. This formula is quite general; it holds in nonabelian gauge theories and is
important in lattice gauge theory.
Our second choice is to multiply the numerator and the denominator of
the ratio (16.172) by the exponential exp[‚àíi 1
2Œ±
2
(‚àí‚ñ≥)2 d4x] and then inte-
grate over (x) separately in the numerator and denominator. This operation
just multiplies the numerator and denominator by the same constant fac-
tor, which cancels. But if before integrating over all gauge transformations
(x), we shift  so that ‚ñ≥ decreases by ÀôA0, then the exponential factor
is exp[‚àíi 1
2Œ±
2
( ÀôA0 ‚àí‚ñ≥)2 d4x]. Now when we integrate over (x), the delta
function Œ¥(‚àá¬∑ A + ‚ñ≥) replaces ‚ñ≥ by
‚àí‚àá¬∑ A in the inserted exponen-
tial, converting it to exp[‚àíi 1
2Œ±
2
( ÀôA0 + ‚àá¬∑ A)2 d4x]. The result is to replace the
gauge-invariant action (16.168) with the gauge-Ô¨Åxed action
SŒ± =

‚àí1
4Fab Fab ‚àíŒ±
2 (‚àÇbAb)2 + Abjb + Lm d4x.
(16.175)
This action is Lorentz invariant and so is much easier to work with than the one
(16.162) with the Coulomb term. We can use it to compute scattering ampli-
tudes perturbatively. The mean value of a time-ordered product of operators in
the ground state |0‚ü©of the free theory is
‚ü®0|T [O1 ¬∑ ¬∑ ¬∑ On] |0‚ü©=

O1 ¬∑ ¬∑ ¬∑ On eiSŒ± DA Dœà

eiSŒ± DA Dœà
.
(16.176)
612

16.14 FERMIONIC PATH INTEGRALS
By following steps analogous to those that led to (16.150), one may show
(exercise 16.22) that in Feynman‚Äôs gauge, Œ± = 1, the photon propagator is
‚ü®0|T

AŒº(x)AŒΩ(y)

|0‚ü©= ‚àíi‚ñ≥ŒºŒΩ(x‚àíy) = ‚àíi

Œ∑ŒºŒΩ
q2 ‚àíiœµ eiq¬∑(x‚àíy) d4q
(2œÄ)4 . (16.177)
16.14 Fermionic path integrals
In our brief introduction (1.11‚Äì1.12) and (1.43‚Äì1.45) to Grassmann variables,
we learned that because
Œ∏2 = 0
(16.178)
the most general function f (Œ∏) of a single Grassmann variable Œ∏ is
f (Œ∏) = a + b Œ∏.
(16.179)
So a complete integral table consists of the integral of this linear function

f (Œ∏) dŒ∏ =

a + b Œ∏ dŒ∏ = a

dŒ∏ + b

Œ∏ dŒ∏.
(16.180)
This equation has two unknowns, the integral
2
dŒ∏ of unity and the integral
2
Œ∏ dŒ∏ of Œ∏. We choose them so that the integral of f (Œ∏ + Œ∂)

f (Œ∏ + Œ∂) dŒ∏ =

a + b (Œ∏ + Œ∂) dŒ∏ = (a + b Œ∂)

dŒ∏ + b

Œ∏ dŒ∏
(16.181)
is the same as the integral (16.180) of f (Œ∏). Thus the integral
2
dŒ∏ of unity must
vanish, while the integral
2
Œ∏ dŒ∏ of Œ∏ can be any constant, which we choose to
be unity. Our complete table of integrals is then

dŒ∏ = 0
and

Œ∏ dŒ∏ = 1.
(16.182)
The anticommutation relations for a fermionic degree of freedom œà are
{œà, œà‚Ä†} ‚â°œà œà‚Ä† + œà‚Ä†œà = 1
and
{œà, œà} = {œà‚Ä†, œà‚Ä†} = 0.
(16.183)
Because œà has œà‚Ä†, it is conventional to introduce a variable Œ∏‚àó= Œ∏‚Ä† that
anticommutes with itself and with Œ∏
{Œ∏‚àó, Œ∏‚àó} = {Œ∏‚àó, Œ∏} = {Œ∏, Œ∏} = 0.
(16.184)
The logic that led to (16.182) now gives

dŒ∏‚àó= 0
and

Œ∏‚àódŒ∏‚àó= 1.
(16.185)
We deÔ¨Åne the reference state |0‚ü©as |0‚ü©‚â°œà|s‚ü©for a state |s‚ü©that is not
annihilated by œà. Since œà2 = 0, the operator œà annihilates the state |0‚ü©
613

PATH INTEGRALS
œà|0‚ü©= œà2|s‚ü©= 0.
(16.186)
The effect of the operator œà on the state
|Œ∏‚ü©= exp

œà‚Ä†Œ∏ ‚àí1
2Œ∏‚àóŒ∏

|0‚ü©=

1 + œà‚Ä†Œ∏ ‚àí1
2Œ∏‚àóŒ∏

|0‚ü©
(16.187)
is
œà|Œ∏‚ü©= œà(1 + œà‚Ä†Œ∏ ‚àí1
2Œ∏‚àóŒ∏)|0‚ü©= œàœà‚Ä†Œ∏|0‚ü©= (1 ‚àíœà‚Ä†œà)Œ∏|0‚ü©= Œ∏|0‚ü©(16.188)
while that of Œ∏ on |Œ∏‚ü©is
Œ∏|Œ∏‚ü©= Œ∏(1 + œà‚Ä†Œ∏ ‚àí1
2Œ∏‚àóŒ∏)|0‚ü©= Œ∏|0‚ü©.
(16.189)
The state |Œ∏‚ü©therefore is an eigenstate of œà with eigenvalue Œ∏
œà|Œ∏‚ü©= Œ∏|Œ∏‚ü©.
(16.190)
The bra corresponding to the ket |Œ∂‚ü©is
‚ü®Œ∂| = ‚ü®0|

1 + Œ∂ ‚àóœà ‚àí1
2Œ∂ ‚àóŒ∂

(16.191)
and the inner product ‚ü®Œ∂|Œ∏‚ü©is (exercise 16.23)
‚ü®Œ∂|Œ∏‚ü©= ‚ü®0|

1 + Œ∂ ‚àóœà ‚àí1
2Œ∂ ‚àóŒ∂
 
1 + œà‚Ä†Œ∏ ‚àí1
2Œ∏‚àóŒ∏

|0‚ü©
= ‚ü®0|1 + Œ∂ ‚àóœàœà‚Ä†Œ∏ ‚àí1
2Œ∂ ‚àóŒ∂ ‚àí1
2Œ∏‚àóŒ∏ + 1
4Œ∂ ‚àóŒ∂Œ∏‚àóŒ∏|0‚ü©
= ‚ü®0|1 + Œ∂ ‚àóŒ∏ ‚àí1
2Œ∂ ‚àóŒ∂ ‚àí1
2Œ∏‚àóŒ∏ + 1
4Œ∂ ‚àóŒ∂Œ∏‚àóŒ∏|0‚ü©
= exp
#
Œ∂ ‚àóŒ∏ ‚àí1
2

Œ∂ ‚àóŒ∂ + Œ∏‚àóŒ∏
$
.
(16.192)
Example 16.2 (A gaussian integral)
For any number c, we can compute the
integral of exp(c Œ∏‚àóŒ∏) by expanding the exponential

ec Œ∏‚àóŒ∏ dŒ∏‚àódŒ∏ =

(1 + c Œ∏‚àóŒ∏) dŒ∏‚àódŒ∏ =

(1 ‚àíc Œ∏ Œ∏‚àó) dŒ∏‚àódŒ∏ = ‚àíc, (16.193)
a formula that we‚Äôll use over and over.
The identity operator for the space of states
c|0‚ü©+ d|1‚ü©‚â°c|0‚ü©+ dœà‚Ä†|0‚ü©
(16.194)
is (exercise 16.24) the integral
I =

|Œ∏‚ü©‚ü®Œ∏| dŒ∏‚àódŒ∏ = |0‚ü©‚ü®0| + |1‚ü©‚ü®1|,
(16.195)
614

16.14 FERMIONIC PATH INTEGRALS
in which the differentials anticommute with each other and with other
fermionic variables: {dŒ∏, dŒ∏‚àó} = 0, {dŒ∏, Œ∏} = 0, {dŒ∏, œà} = 0, and so forth.
The case of several Grassmann variables Œ∏1, Œ∏2, . . . , Œ∏n and several Fermi
operators œà1, œà2, . . . , œàn is similar. The Œ∏k anticommute among themselves
{Œ∏i, Œ∏j} = {Œ∏i, Œ∏‚àó
j } = {Œ∏‚àó
i , Œ∏‚àó
j } = 0
(16.196)
while the œàk satisfy
{œàk, œà‚Ä†
‚Ñì} = Œ¥k‚Ñì
and
{œàk, œàl} = {œà‚Ä†
k, œà‚Ä†
‚Ñì} = 0.
(16.197)
The reference state |0‚ü©is
|0‚ü©=
 n

k=1
œàk

|s‚ü©,
(16.198)
in which |s‚ü©is any state not annihilated by any œàk (so the resulting |0‚ü©isn‚Äôt zero).
The direct-product state
|Œ∏‚ü©‚â°exp
 n

k=1
œà‚Ä†
kŒ∏k ‚àí1
2Œ∏‚àó
kŒ∏k

|0‚ü©=
 n

k=1

1 + œà‚Ä†
kŒ∏k ‚àí1
2Œ∏‚àó
kŒ∏k

|0‚ü©
(16.199)
is (exercise 16.25) a simultaneous eigenstate of each œàk
œàk|Œ∏‚ü©= Œ∏k|Œ∏‚ü©.
(16.200)
It follows that
œà‚Ñìœàk|Œ∏‚ü©= œà‚ÑìŒ∏k|Œ∏‚ü©= ‚àíŒ∏kœà‚Ñì|Œ∏‚ü©= ‚àíŒ∏kŒ∏‚Ñì|Œ∏‚ü©= Œ∏‚ÑìŒ∏k|Œ∏‚ü©
(16.201)
and so too œàkœà‚Ñì|Œ∏‚ü©= Œ∏kŒ∏‚Ñì|Œ∏‚ü©. Since the œàs anticommute, their eigenvalues
must also
Œ∏‚ÑìŒ∏k|Œ∏‚ü©= œà‚Ñìœàk|Œ∏‚ü©= ‚àíœàkœà‚Ñì|Œ∏ = ‚àíŒ∏kŒ∏‚Ñì|Œ∏‚ü©,
(16.202)
which is why they must be Grassmann variables.
The inner product ‚ü®Œ∂|Œ∏‚ü©is
‚ü®Œ∂|Œ∏‚ü©= ‚ü®0|
 n

k=1
(1 + Œ∂ ‚àó
k œàk ‚àí1
2Œ∂ ‚àó
k Œ∂k)
  n

‚Ñì=1
(1 + œà‚Ä†
‚ÑìŒ∏‚Ñì‚àí1
2Œ∏‚àó
‚ÑìŒ∏‚Ñì)

|0‚ü©
= exp
 n

k=1
Œ∂ ‚àó
k Œ∏k ‚àí1
2

Œ∂ ‚àó
k Œ∂k + Œ∏‚àó
kŒ∏k


= eŒ∂ ‚Ä†Œ∏‚àí(Œ∂ ‚Ä†Œ∂+Œ∏‚Ä†Œ∏)/2.
(16.203)
The identity operator is
I =

|Œ∏‚ü©‚ü®Œ∏|
n

k=1
dŒ∏‚àó
kdŒ∏k.
(16.204)
615

PATH INTEGRALS
Example 16.3 (Gaussian Grassmann integral)
For any 2 √ó 2 matrix A, we may
compute the gaussian integral
g(A) =

e‚àíŒ∏‚Ä†AŒ∏ dŒ∏‚àó
1 dŒ∏1dŒ∏‚àó
2 dŒ∏2
(16.205)
by expanding the exponential. The only terms that survive are the ones that have
exactly one of each of the four variables Œ∏1, Œ∏2, Œ∏‚àó
1 , and Œ∏‚àó
2 . Thus, the integral is
the determinant of the matrix A
g(A) =
 1
2

Œ∏‚Ä†
kAk‚ÑìŒ∏‚Ñì
2
dŒ∏‚àó
1 dŒ∏1dŒ∏‚àó
2 dŒ∏2
=
 
Œ∏‚àó
1 A11Œ∏1 Œ∏‚àó
2 A22Œ∏2 + Œ∏‚àó
1 A12Œ∏2 Œ∏‚àó
2 A21Œ∏1

dŒ∏‚àó
1 dŒ∏1dŒ∏‚àó
2 dŒ∏2
= A11A22 ‚àíA12A21 = det A.
(16.206)
The natural generalization to n dimensions

e‚àíŒ∏‚Ä†AŒ∏
n

k=1
dŒ∏‚àó
kdŒ∏k = det A
(16.207)
is true for any n√ón matrix A. If A is invertible, then the invariance of Grassmann
integrals under translations implies that

e‚àíŒ∏‚Ä†AŒ∏+Œ∏‚Ä†Œ∂+Œ∂ ‚Ä†Œ∏
n

k=1
dŒ∏‚àó
kdŒ∏k =

e‚àíŒ∏‚Ä†A(Œ∏+A‚àí1Œ∂)+Œ∏‚Ä†Œ∂+Œ∂ ‚Ä†(Œ∏+A‚àí1Œ∂)
n

k=1
dŒ∏‚àó
kdŒ∏k
=

e‚àíŒ∏‚Ä†AŒ∏+Œ∂ ‚Ä†Œ∏+Œ∂ ‚Ä†A‚àí1Œ∂
n

k=1
dŒ∏‚àó
kdŒ∏k
=

e‚àí(Œ∏‚Ä†+Œ∂ ‚Ä†A‚àí1)AŒ∏+Œ∂ ‚Ä†Œ∏+Œ∂ ‚Ä†A‚àí1Œ∂
n

k=1
dŒ∏‚àó
kdŒ∏k
=

e‚àíŒ∏‚Ä†AŒ∏+Œ∂ ‚Ä†A‚àí1Œ∂
n

k=1
dŒ∏‚àó
kdŒ∏k
= det A eŒ∂ ‚Ä†A‚àí1Œ∂.
(16.208)
The values of Œ∏ and Œ∏‚Ä† that make the argument ‚àíŒ∏‚Ä†AŒ∏ + Œ∏‚Ä†Œ∂ + Œ∂ ‚Ä†Œ∏ of the
exponential stationary are Œ∏ = A‚àí1Œ∂ and Œ∏‚Ä† = Œ∂ ‚Ä†A‚àí1. So a gaussian Grassmann
integral is equal to its exponential evaluated at its stationary point, apart from a
prefactor involving the determinant det A. This result is a fermionic echo of the
bosonic results (16.13‚Äì16.15).
616

16.14 FERMIONIC PATH INTEGRALS
One may further extend these deÔ¨Ånitions to a Grassmann Ô¨Åeld œám(x) and an
associated Dirac Ô¨Åeld œàm(x). The œám(x)s anticommute among themselves and
with all fermionic variables at all points of space-time
{œám(x), œán(x‚Ä≤)} = {œá‚àó
m(x), œán(x‚Ä≤)} = {œá‚àó
m(x), œá‚àó
n (x‚Ä≤)} = 0
(16.209)
and the Dirac Ô¨Åeld œàm(x) obeys the equal-time anticommutation relations
{œàm(x, t), œà‚Ä†
n(x‚Ä≤, t)} = Œ¥mn Œ¥(x ‚àíx‚Ä≤) (n, m = 1, . . . , 4)
{œàm(x, t), œàn(x‚Ä≤, t)} = {œà‚Ä†
m(x, t), œà‚Ä†
n(x‚Ä≤, t)} = 0.
(16.210)
As in (16.102 & 16.198), we use eigenstates of the Ô¨Åeld œà at t = 0. If |0‚ü©is
deÔ¨Åned in terms of a state |s‚ü©that is not annihilated by any œàm(x, 0) as
|0‚ü©=

m,x
œàm(x, 0)

|s‚ü©
(16.211)
then (exercise 16.26) the state
|œá‚ü©= exp
 
m
œà‚Ä†
m(x, 0) œám(x) ‚àí1
2œá‚àó
m(x)œám(x) d3x

|0‚ü©
= exp

œà‚Ä†œá ‚àí1
2œá‚Ä†œá d3x

|0‚ü©
(16.212)
is an eigenstate of the operator œàm(x, 0) with eigenvalue œám(x)
œàm(x, 0)|œá‚ü©= œám(x)|œá‚ü©.
(16.213)
The inner product of two such states is (exercise 16.27)
‚ü®œá‚Ä≤|œá‚ü©= exp
#
œá‚Ä≤‚Ä†œá ‚àí1
2œá‚Ä≤‚Ä†œá‚Ä≤ ‚àí1
2œá‚Ä†œá d3x
$
.
(16.214)
The identity operator is the integral
I =

|œá‚ü©‚ü®œá| Dœá‚àóDœá,
(16.215)
in which
Dœá‚àóDœá ‚â°

m,x
dœá‚àó
m(x)dœám(x).
(16.216)
The hamiltonian for a free Dirac Ô¨Åeld œà of mass m is the spatial integral
H0 =

œà (Œ≥ ¬∑ ‚àá+ m) œà d3x,
(16.217)
in which œà ‚â°iœà‚Ä†Œ≥ 0 and the gamma matrices (10.286) satisfy
{Œ≥ a, Œ≥ b} = 2 Œ∑ab
(16.218)
617

PATH INTEGRALS
where Œ∑ is the 4√ó4 diagonal matrix with entries (‚àí1, 1, 1, 1). Since œà|œá‚ü©= œá|œá‚ü©
and ‚ü®œá‚Ä≤|œà‚Ä† = ‚ü®œá‚Ä≤|œá‚Ä≤‚Ä†, the quantity ‚ü®œá‚Ä≤| exp( ‚àíiœµH0)|œá‚ü©is by (16.214)
‚ü®œá‚Ä≤|e‚àíiœµH0|œá‚ü©=

‚ü®œá‚Ä≤|œá‚ü©exp
#
‚àíiœµ

œá‚Ä≤ (Œ≥ ¬∑ ‚àá+ m) œá d3x
$
=

exp
#
1
2(œá‚Ä≤‚Ä† ‚àíœá‚Ä†)œá ‚àí1
2œá‚Ä≤‚Ä†(œá‚Ä≤ ‚àíœá) ‚àíiœµœá‚Ä≤(Œ≥ ¬∑‚àá+ m)œád3x
$
=

exp

œµ
 
1
2 Àôœá‚Ä†œá ‚àí1
2œá‚Ä≤‚Ä† Àôœá ‚àíiœá‚Ä≤ (Œ≥ ¬∑ ‚àá+ m) œá

d3x
%
Œª,
(16.219)
in which œá‚Ä≤‚Ä†‚àíœá‚Ä† = œµ Àôœá‚Ä† and œá‚Ä≤‚àíœá = œµ Àôœá. Everything within the square brackets
is multiplied by œµ, so we may replace œá‚Ä≤‚Ä† by œá‚Ä† and œá‚Ä≤ by œá so as to write to Ô¨Årst
order in œµ
‚ü®œá‚Ä≤|e‚àíiœµH0|œá‚ü©=

exp
#
œµ

1
2 Àôœá‚Ä†œá ‚àí1
2œá‚Ä† Àôœá ‚àíiœá (Œ≥ ¬∑ ‚àá+ m) œá d3x
$
,
(16.220)
in which the dependence upon œá‚Ä≤ is through the time derivatives.
Putting together n
=
2t/œµ such matrix elements, integrating over all
intermediate-state dyadics |œá‚ü©‚ü®œá|, and using our formula (16.215), we Ô¨Ånd
‚ü®œát|e‚àí2itH0|œá‚àít‚ü©=

exp
#
1
2 Àôœá‚Ä†œá ‚àí1
2œá‚Ä† Àôœá ‚àíiœá (Œ≥ ¬∑‚àá+ m) œád4x
$
Dœá‚àóDœá.
(16.221)
Integrating Àôœá‚Ä†œá by parts and dropping the surface term, we get
‚ü®œát|e‚àí2itH0|œá‚àít‚ü©=

exp
#
‚àíœá‚Ä† Àôœá ‚àíiœá (Œ≥ ¬∑‚àá+ m) œá d4x
$
Dœá‚àóDœá. (16.222)
Since ‚àíœá‚Ä† Àôœá = ‚àíiœáŒ≥ 0 Àôœá, the argument of the exponential is
i

‚àíœáŒ≥ 0 Àôœá ‚àíœá (Œ≥ ¬∑ ‚àá+ m) œá d4x = i

‚àíœá

Œ≥ Œº‚àÇŒº + m

œá d4x.
(16.223)
We then have
‚ü®œát|e‚àí2itH0|œá‚àít‚ü©=

exp

i

L0(œá) d4x

Dœá‚àóDœá,
(16.224)
in which L0(œá) = ‚àíœá

Œ≥ Œº‚àÇŒº + m

œá is the action density (10.289) for a free
Dirac Ô¨Åeld. Thus the amplitude is a path integral with phases given by the
classical action S0[œá]
‚ü®œát|e‚àí2itH0|œá‚àít‚ü©=

ei
2
L0(œá) d4xDœá‚àóDœá =

eiS0[œá]Dœá‚àóDœá
(16.225)
618

16.15 APPLICATION TO NONABELIAN GAUGE THEORIES
and the integral is over all Ô¨Åelds that go from œá(x, ‚àít) = œá‚àít(x) to œá(x, t) =
œát(x). Any normalization factor will cancel in ratios of such integrals.
Since Fermi Ô¨Åelds anticommute, their time-ordered product has an extra
minus sign
T

œà(x1)œà(x2)

= Œ∏(x0
1 ‚àíx0
2) œà(x1) œà(x2) ‚àíŒ∏(x0
2 ‚àíx0
1) œà(x2) œà(x1). (16.226)
The logic behind our formulas (16.123) and (16.129) for the time-ordered prod-
uct of bosonic Ô¨Åelds now leads to an expression for the time-ordered product of
2n Dirac Ô¨Åelds (with Dœá‚Ä≤‚Ä≤ and Dœá‚Ä≤ suppressed)
‚ü®0|T

œà(x1) ¬∑ ¬∑ ¬∑ œà(x2n)

|0‚ü©=

‚ü®0|œá‚Ä≤‚Ä≤‚ü©œá(x1) ¬∑ ¬∑ ¬∑ œá(x2n) eiS0[œá]‚ü®œá‚Ä≤|0‚ü©Dœá‚àóDœá

‚ü®0|œá‚Ä≤‚Ä≤‚ü©eiS0[œá]‚ü®œá‚Ä≤|0‚ü©Dœá‚àóDœá
.
(16.227)
As in (16.140), the effect of the inner products ‚ü®0|œá‚Ä≤‚Ä≤‚ü©and ‚ü®œá‚Ä≤|0‚ü©is to insert
œµ-terms, which modify the Dirac propagators
‚ü®0|T

œà(x1) ¬∑ ¬∑ ¬∑ œà(x2n)

|0‚ü©=

œá(x1) ¬∑ ¬∑ ¬∑ œá(x2n) eiS0[œá,œµ] Dœá‚àóDœá

eiS0[œá,œµ] Dœá‚àóDœá
.
(16.228)
Imitating (16.141), we introduce a Grassmann external current Œ∂(x) and
deÔ¨Åne a fermionic analog of Z0[j]
Z0[Œ∂] ‚â°‚ü®0| T

ei
2
Œ∂œà+œàŒ∂ d4x
|0‚ü©=

ei
2
Œ∂œá+œáŒ∂ d4xeiS0[œá,œµ]Dœá‚àóDœá

eiS0[œá,œµ]Dœá‚àóDœá
.
(16.229)
16.15 Application to nonabelian gauge theories
The action of a generic nonabelian gauge theory is
S =

‚àí1
4FaŒºŒΩFŒºŒΩ
a
‚àíœà

Œ≥ ŒºDŒº + m

œà d4x,
(16.230)
in which the Maxwell Ô¨Åeld is
FaŒºŒΩ ‚â°‚àÇŒºAaŒΩ ‚àí‚àÇŒΩAaŒº + g fabc AbŒº AcŒΩ
(16.231)
and the covariant derivative is
DŒºœà ‚â°‚àÇŒºœà ‚àíig ta AaŒº œà.
(16.232)
Here g is a coupling constant, fabc is a structure constant (10.63), and ta is a
generator (10.55) of the Lie algebra (section 10.15) of the gauge group.
619

PATH INTEGRALS
One may show (Weinberg, 1996, pp. 14‚Äì18) that the analog of equation
(16.169) for quantum electrodynamics is
‚ü®|T [O1 ¬∑ ¬∑ ¬∑ On] |‚ü©=

O1 ¬∑ ¬∑ ¬∑ On eiS Œ¥[Aa3] DA Dœà

eiS Œ¥[Aa3] DA Dœà
,
(16.233)
in which the functional delta function
Œ¥[Aa3] ‚â°

x,b
Œ¥(Aa3(x))
(16.234)
enforces the axial-gauge condition, and Dœà stands for Dœà‚àóDœà.
Initially, physicists had trouble computing nonabelian amplitudes beyond the
lowest order of perturbation theory. Then DeWitt showed how to compute to
second order (DeWitt, 1967), and Faddeev and Popov, using path integrals,
showed how to compute to all orders (Faddeev and Popov, 1967).
16.16 The Faddeev‚ÄìPopov trick
The path-integral tricks of Faddeev and Popov are described in Weinberg (1996,
pp. 19‚Äì27). We will use gauge-Ô¨Åxing functions Ga(x) to impose a gauge
condition on our nonabelian gauge Ô¨Åelds Aa
Œº(x). For instance, we can use
Ga(x) = A3
a(x) to impose an axial gauge or Ga(x) = i‚àÇŒºAŒº
a (x) to impose a
Lorentz-invariant gauge.
Under an inÔ¨Ånitesimal gauge transformation (11.511)
AŒª
aŒº = AaŒº ‚àí‚àÇŒºŒªa ‚àíg fabc AbŒº Œªc
(16.235)
the gauge Ô¨Åelds change, and so the gauge-Ô¨Åxing functions Gb(x), which depend
upon them, also change. The jacobian J of that change is
J = det
Œ¥GŒª
a(x)
Œ¥Œªb(y)

Œª=0
‚â°DGŒª
DŒª

Œª=0
(16.236)
and it typically involves the delta function Œ¥4(x ‚àíy).
Let B[G] be any functional of the gauge-Ô¨Åxing functions Gb(x) such as
B[G] =

x,a
Œ¥(Ga(x)) =

x,a
Œ¥(A3
a(x))
(16.237)
in an axial gauge or
B[G] = exp
# i
2

(Ga(x))2 d4x
$
= exp
#
‚àíi
2
 
‚àÇŒºAŒº
a (x)
2 d4x
$
(16.238)
in a Lorentz-invariant gauge.
620

16.16 THE FADDEEV‚ÄìPOPOV TRICK
We want to understand functional integrals like (16.233)
‚ü®|T [O1 ¬∑ ¬∑ ¬∑ On] |‚ü©=

O1 ¬∑ ¬∑ ¬∑ On eiS B[G] J DA Dœà

eiS B[G] J DA Dœà
,
(16.239)
in which the operators Ok, the action functional S[A], and the differen-
tials DADœà (but not the gauge-Ô¨Åxing functional B[G] or the Jacobian J)
are gauge invariant. The axial-gauge formula (16.233) is a simple example in
which B[G] = Œ¥[Aa3] enforces the axial-gauge condition Aa3(x) = 0 and the
determinant J = det (Œ¥ab‚àÇ3Œ¥(x ‚àíy)) is a constant that cancels.
If we translate the gauge Ô¨Åelds by a gauge transformation , then the ratio
(16.239) does not change
‚ü®|T [O1 ¬∑ ¬∑ ¬∑ On] |‚ü©=

O
1 ¬∑ ¬∑ ¬∑ O
n eiS B[G] J DA Dœà

eiS B[G] J DA Dœà
(16.240)
any more than
2
f (y) dy is different from
2
f (x) dx. Since the operators Ok, the
action functional S[A], and the differentials DADœà are gauge invariant, most
of the -dependence goes away
‚ü®|T [O1 ¬∑ ¬∑ ¬∑ On] |‚ü©=

O1 ¬∑ ¬∑ ¬∑ On eiS B[G] J DA Dœà

eiS B[G] J DA Dœà
.
(16.241)
Let Œª be a gauge transformation  followed by an inÔ¨Ånitesimal gauge trans-
formation Œª. The jacobian J is a determinant of a product of matrices, which
is a product of their determinants
J = det
Œ¥GŒª
a (x)
Œ¥Œªb(y)

Œª=0
= det
 Œ¥GŒª
a (x)
Œ¥Œªc(z)
Œ¥Œªc(z)
Œ¥Œªb(y) d4z

Œª=0
= det
Œ¥GŒª
a (x)
Œ¥Œªc(z)

Œª=0
det
Œ¥Œªc(z)
Œ¥Œªb(y)

Œª=0
= det
Œ¥G
a (x)
Œ¥c(z)

det
Œ¥Œªc(z)
Œ¥Œªb(y)

Œª=0
‚â°DG
D
DŒª
DŒª

Œª=0
.
(16.242)
Now we integrate over the gauge transformation  with weight function
œÅ() = (DŒª/DŒª|Œª=0)‚àí1 and Ô¨Ånd, since the ratio (16.241) is -independent,
‚ü®|T [O1 ¬∑ ¬∑ ¬∑ On] |‚ü©=

O1 ¬∑ ¬∑ ¬∑ On eiS B[G] DG
D D DA Dœà

eiS B[G] DG
D D DA Dœà
621

PATH INTEGRALS
=

O1 ¬∑ ¬∑ ¬∑ On eiS B[G] DG DA Dœà

eiS B[G] DG DA Dœà
=

O1 ¬∑ ¬∑ ¬∑ On eiS DA Dœà

eiS DA Dœà
.
(16.243)
Thus the mean-value in the vacuum of a time-ordered product of gauge-
invariant operators is a ratio of path integrals over all gauge Ô¨Åelds without
any gauge Ô¨Åxing. No matter what gauge condition G or gauge-Ô¨Åxing functional
B[G] we use, the resulting gauge-Ô¨Åxed ratio (16.239) is equal to the ratio (16.243)
of path integrals over all gauge Ô¨Åelds without any gauge Ô¨Åxing. All gauge-Ô¨Åxed
ratios (16.239) give the same time-ordered products, and so we can use whatever
gauge condition G or gauge-Ô¨Åxing functional B[G] is most convenient.
The analogous formula for the euclidean time-ordered product is
‚ü®|Te [O1 ¬∑ ¬∑ ¬∑ On] |‚ü©=

O1 ¬∑ ¬∑ ¬∑ On e‚àíSe DA Dœà

e‚àíSe DA Dœà
(16.244)
where the euclidean action Se is the space-time integral of the energy density.
This formula is the basis for lattice gauge theory.
The path-integral formulas (16.173 & 16.244) derived for quantum electro-
dynamics therefore also apply to nonabelian gauge theories.
16.17 Ghosts
Faddeev and Popov showed how to do perturbative calculations in which one
does Ô¨Åx the gauge. To continue our description of their tricks, we return to the
gauge-Ô¨Åxed expression (16.239) for the time-ordered product
‚ü®|T [O1 ¬∑ ¬∑ ¬∑ On] |‚ü©=

O1 ¬∑ ¬∑ ¬∑ On eiS B[G] J DA Dœà

eiS B[G] J DA Dœà
,
(16.245)
set Gb(x) = ‚àíi‚àÇŒºAŒº
b (x), and use (16.238) as the gauge-Ô¨Åxing functional B[G]
B[G] = exp
# i
2

(Ga(x))2 d4x
$
= exp
#
‚àíi
2
 
‚àÇŒºAŒº
a (x)
2 d4x
$
.
(16.246)
622

16.17 GHOSTS
This functional adds to the action density the term ‚àí(‚àÇŒºAŒº
a )2/2, which leads
to a gauge-Ô¨Åeld propagator like the photon‚Äôs (16.177)
‚ü®0|T

Aa
Œº(x)Ab
ŒΩ(y)

|0‚ü©= ‚àíiŒ¥ab‚ñ≥ŒºŒΩ(x ‚àíy) = ‚àíi
 Œ∑ŒºŒΩŒ¥ab
q2 ‚àíiœµ eiq¬∑(x‚àíy) d4q
(2œÄ)4 .
(16.247)
What about the determinant J? Under an inÔ¨Ånitesimal gauge transformation
(16.235), the gauge Ô¨Åeld becomes
AŒª
aŒº = AaŒº ‚àí‚àÇŒºŒªa ‚àíg fabc AbŒº Œªc
(16.248)
and so GŒª
a(x) = i‚àÇŒºAŒª
aŒº(x) is
GŒª
a(x) = i‚àÇŒºAaŒº(x) + i‚àÇŒº
 
‚àíŒ¥ac‚àÇŒº ‚àíg fabcAbŒº(x)

Œ¥4(x ‚àíy)Œªc(y) d4y.
(16.249)
The jacobian J then is the determinant (16.236) of the matrix
Œ¥GŒª
a(x)
Œ¥Œªc(y)

Œª=0
= ‚àíiŒ¥ac 2 Œ¥4(x ‚àíy) ‚àíig fabc
‚àÇ
‚àÇxŒº

AŒº
b (x)Œ¥4(x ‚àíy)

, (16.250)
that is
J = det

‚àíiŒ¥ac 2 Œ¥4(x ‚àíy) ‚àíig fabc
‚àÇ
‚àÇxŒº

AŒº
b (x)Œ¥4(x ‚àíy)

.
(16.251)
But we‚Äôve seen (16.207) that a determinant can be written as a fermionic path
integral
det A =

e‚àíŒ∏‚Ä†A Œ∏
n

k=1
dŒ∏‚àó
kdŒ∏k.
(16.252)
So we can write the jacobian J as
J =

exp
#
iœâ‚àó
a2œâa + igfabcœâ‚àó
a‚àÇŒº(AŒº
b œâc) d4x
$
Dœâ‚àóDœâ,
(16.253)
which contributes the terms ‚àí‚àÇŒºœâ‚àó
a‚àÇŒºœâa and
‚àí‚àÇŒºœâ‚àó
a g fabc AŒº
b œâc = ‚àÇŒºœâ‚àó
a g fabc AŒº
c œâb
(16.254)
to the action density.
Thus we can do perturbation theory by using the modiÔ¨Åed action density
L‚Ä≤ = ‚àí1
4FaŒºŒΩFŒºŒΩ
a
‚àí1
2

‚àÇŒºAŒº
a
2 ‚àí‚àÇŒºœâ‚àó
a‚àÇŒºœâa +‚àÇŒºœâ‚àó
a g fabc AŒº
c œâb ‚àíœà (Ã∏D + m) œà,
(16.255)
623

PATH INTEGRALS
in which Ã∏D ‚â°Œ≥ ŒºDŒº = Œ≥ Œº(‚àÇŒº ‚àíigtaAaŒº). The ghost Ô¨Åeld œâ is a mathematical
device, not a physical Ô¨Åeld describing real particles, which would be spinless
fermions violating the spin-statistics theorem (example 10.20).
Further reading
The detailed Quantum Field Theory (Srednicki, 2007), the classics The Quantum
Theory of Fields I, II, III (Weinberg, 1995, 1996, 2005), and the delightfully
readable Quantum Field Theory in a Nutshell (Zee, 2010) all provide excellent
treatments of path integrals.
Exercises
16.1
Derive the multiple gaussian integral (16.8) from (5.167).
16.2
Derive the multiple gaussian integral (16.12) from (5.166).
16.3
Show that the vector Y that makes the argument of the multiple gaussian
integral (16.12) stationary is given by (16.13), and that the multiple gaussian
integral (16.12) is equal to its exponential evaluated at its stationary point Y
apart from a prefactor involving det iS.
16.4
Repeat the previous exercise for the multiple gaussian integral (16.11).
16.5
Compute the double integral (16.23) for the case V(qj) = 0.
16.6
Insert into the LHS of (16.54) a complete set of momentum dyadics |p‚ü©‚ü®p|,
use the inner product ‚ü®q|p‚ü©= exp(iqp)/
‚àö
2œÄ, do the resulting Fourier
transform, and so verify the free-particle path integral (16.54).
16.7
By taking the nonrelativistic limit of the formula (11.311) for the action of
a relativistic particle of mass m and charge q, derive the expression (16.55)
for the action of a nonrelativistic particle in an electromagnetic Ô¨Åeld with no
scalar potential.
16.8
Show that for the hamiltonian (16.60) of the simple harmonic oscillator the
action S[qc] of the classical path is (16.67).
16.9
Show that the harmonic-oscillator action of the loop (16.68) is (16.69).
16.10 Show that the harmonic-oscillator amplitude (16.72) for q‚Ä≤ = 0 and q‚Ä≤‚Ä≤ =
q reduces as t ‚Üí0 to the one-dimensional version of the free-particle
amplitude (16.54).
16.11 Work out the path-integral formula for the amplitude for a mass m to
fall to the ground from height h in a gravitational Ô¨Åeld of local accelera-
tion g to lowest order and then including loops. Hint: use the technique of
section 16.7.
16.12 Show that the action (16.74) of the stationary solution (16.77) is (16.79).
16.13 Derive formula (16.132) for the action S0[œÜ] from (16.130 & 16.131).
16.14 Derive identity (16.136). Split the time integral at t = 0 into two halves, use
œµ e¬±œµt = ¬± d
dt e¬±œµt,
(16.256)
and then integrate each half by parts.
624

EXERCISES
16.15 Derive the third term in equation (16.138) from the second term.
16.16 Use (16.143) and the Fourier transform (16.144) of the external current to
derive formula (16.145).
16.17 Derive equation (16.147) from equations (16.145 & 16.146).
16.18 Derive the formula (16.148) for Z0[j] from the expression (16.147) for
S0[œÜ, œµ, j].
16.19 Derive equations (16.149 & 16.150) from formula (16.148).
16.20 Derive equation (16.154) from the formula (16.149) for Z0[j].
16.21 Show that the time integral of the Coulomb term (16.159) is the negative of
the term that is quadratic in j0 in the number F deÔ¨Åned by (16.164).
16.22 By following steps analogous to those that led to (16.150), derive the formula
(16.177) for the photon propagator in Feynman‚Äôs gauge.
16.23 Derive expression (16.192) for the inner product ‚ü®Œ∂|Œ∏‚ü©.
16.24 Derive the representation (16.195) of the identity operator I for a single
fermionic degree of freedom from the rules (16.182 & 16.185) for Grassmann
integration and the anticommutation relations (16.178 & 16.184).
16.25 Derive the eigenvalue equation (16.200) from the deÔ¨Ånition (16.198 &
16.199) of the eigenstate |Œ∏‚ü©and the anticommutation relations (16.196 &
16.197).
16.26 Derive the eigenvalue relation (16.213) for the Fermi Ô¨Åeld œàm(x, t) from the
anticommutation relations (16.209 & 16.210) and the deÔ¨Ånitions (16.211 &
16.212).
16.27 Derive the formula (16.214) for the inner product ‚ü®œá‚Ä≤|œá‚ü©from the deÔ¨Ånition
(16.212) of the ket |œá‚ü©.
625

17
The renormalization group
17.1 The renormalization group in quantum Ô¨Åeld theory
Most quantum Ô¨Åeld theories are nonlinear with inÔ¨Ånitely many degrees of
freedom and, because they describe point particles, they are rife with inÔ¨Åni-
ties. But short-distance effects, probably the Ô¨Ånite sizes of the fundamental
constituents of matter, mitigate these inÔ¨Ånities so that we can cope with them
consistently without knowing what happens at very short distances and very
high energies. This procedure is called renormalization.
For instance, in the theory described by the Lagrange density
L = ‚àí1
2‚àÇŒΩœÜ ‚àÇŒΩœÜ ‚àí1
2m2œÜ2 ‚àíg
24œÜ4
(17.1)
we can cut off divergent integrals at some high energy . The amplitude for
the elastic scattering of two bosons of initial 4-momenta p1 and p2 and Ô¨Ånal
momenta p‚Ä≤
1 and p‚Ä≤
2 to one-loop order (Weinberg, 1996, chap. 18) then takes the
simple form (Zee, 2010, chaps. III & VI)
A = g ‚àí
g2
32œÄ2

ln

6
stu

+ iœÄ + 3

(17.2)
as long as the absolute values of the Mandelstam variables s = ‚àí(p1 + p2)2, t =
‚àí(p1 ‚àíp‚Ä≤
1)2, and u = ‚àí(p1 ‚àíp‚Ä≤
2)2, which satisfy stu > 0 and s+t+u = 4m2, are
all much larger than m2 (Stanley Mandelstam, 1928‚Äì). We deÔ¨Åne the physical
coupling constant gŒº, as opposed to the bare one g that comes with L, to be the
real part of the amplitude A at s = ‚àít = ‚àíu = Œº2
gŒº = g ‚àí3g2
32œÄ2

ln

2
Œº2

+ 1

.
(17.3)
626

17.1 THE RENORMALIZATION GROUP IN QUANTUM FIELD THEORY
Thus the bare coupling constant is g = gŒº + 3g2 
ln(2/Œº2) + 1

, and using
this formula, we can write our expression (17.2) for the amplitude A in a form
in which the cut off  no longer appears
A = gŒº ‚àí
g2
32œÄ2

ln

Œº6
stu

+ iœÄ

.
(17.4)
This is the magic of renormalization.
The physical coupling ‚Äúconstant‚Äù gŒº is the right coupling at energy Œº because
when all the Mandelstam variables are near the renormalization point stu = Œº6,
the one-loop correction is tiny, and A ‚âàgŒº.
How does the physical coupling gŒº depend upon the energy Œº? The ampli-
tude A must be independent of the renormalization energy Œº, and so
dA
dŒº = dgŒº
dŒº ‚àí
g2
32œÄ2
6
Œº = 0,
(17.5)
which is a version of the Callan‚ÄìSymanzik equation.
We assume that when the cut off  is big but Ô¨Ånite, the bare and running
coupling constants g and gŒº are so tiny that they differ by terms of order g2
or g2
Œº. Then to lowest order in g and gŒº, we can replace g2 by g2
Œº in (17.5) and
arrive at the simple differential equation
Œº dgŒº
dŒº ‚â°Œ≤(gŒº) =
3 g2
Œº
16œÄ2 ,
(17.6)
which we can integrate
ln E
M =
 E
M
dŒº
Œº =
 gE
gM
dgŒº
Œ≤(gŒº) = 16œÄ2
3
 gE
gM
dgŒº
g2Œº
= 16œÄ2
3
 1
gM
‚àí1
gE

(17.7)
to Ô¨Ånd the running physical coupling constant gŒº at energy Œº = E
gE =
gM
1 ‚àí3 gM ln(E/M)/16œÄ2 .
(17.8)
As the energy E = ‚àös rises above M, while staying below the singular value
E = M exp(16œÄ2/3gM), the running coupling gE slowly increases. And so does
the scattering amplitude, A ‚âàgE.
Example 17.1 (Quantum electrodynamics)
Vacuum polarization makes the
amplitude for the scattering of two electrons proportional to
A(q2) = e2 
1 + œÄ(q2)

(17.9)
627

THE RENORMALIZATION GROUP
rather than to e2. Here e is the renormalized charge, q = p‚Ä≤
1 ‚àíp1 is the
4-momentum transferred to the Ô¨Årst electron, and
œÄ(q2) = e2
2œÄ2
 1
0
x(1 ‚àíx) ln

1 + q2x(1 ‚àíx)
m2

dx
(17.10)
represents the polarization of the vacuum. We deÔ¨Åne the square of the running
coupling constant e2
Œº to be the amplitude (17.9) at q2 = Œº2
e2
Œº = A(Œº2) = e2 
1 + œÄ(Œº2)

.
(17.11)
For Œº2 ‚â´m2, the vacuum polarization term œÄ(Œº2) is (exercise 17.1)
œÄ(Œº2) ‚âàe2
6œÄ2
#
ln Œº
m ‚àí5
6
$
.
(17.12)
The amplitude (17.9) then is
A(q2) = e2
Œº
1 + œÄ(q2)
1 + œÄ(Œº2)
(17.13)
and, since it must be independent of Œº, we have
0 = d
dŒº
A(q2)
1 + œÄ(q2) = d
dŒº
e2
Œº
1 + œÄ(Œº2) ‚âàd
dŒº
3
e2
Œº

1 ‚àíœÄ(Œº2)
4
.
(17.14)
So we Ô¨Ånd
0 = 2eŒº
deŒº
dŒº
 
1 ‚àíœÄ(Œº2)

‚àíe2
Œº
dœÄ(Œº2)
dŒº
= 2eŒº
deŒº
dŒº
 
1 ‚àíœÄ(Œº2)

‚àíe2
Œº
e2
6œÄ2Œº.
(17.15)
Thus, since by (17.10 & 17.11) œÄ(Œº2) = O(e2) and e2
Œº = e2 + O(e4), we Ô¨Ånd to
lowest order in eŒº
ŒºdeŒº
dŒº ‚â°Œ≤(Œº) =
e3
Œº
12œÄ2 .
(17.16)
We can integrate this differential equation
ln E
M =
 E
M
dŒº
Œº =
 eE
eM
deŒº
Œ≤(eŒº) = 12œÄ2
 eE
eM
deŒº
e3Œº
= 6œÄ2

1
e2
M
‚àí1
e2
E

(17.17)
and so get for the running coupling constant the formula
e2
E =
e2
M
1 ‚àíe2
M ln(E/M)/6œÄ2 ,
(17.18)
which shows that it slowly increases with the energy E. Thus, the Ô¨Åne-structure
constant e2
Œº/4œÄ rises from Œ± = 1/137.036 at me to
e2(45.5 GeV)
4œÄ
=
Œ±
1 ‚àí2Œ± ln(45.5/0.00051)/3œÄ =
1
134.6
(17.19)
628

17.1 THE RENORMALIZATION GROUP IN QUANTUM FIELD THEORY
at ‚àös = 91 GeV. When all light charged particles are included, one Ô¨Ånds that
the Ô¨Åne-structure constant rises to Œ± = 1/128.87 at E = 91 GeV.
Example 17.2 (Quantum chromodynamics)
Because of the cubic interaction of
the gauge Ô¨Åelds of a nonabelian gauge theory, the running coupling constant gŒº
can slowly decrease with rising energy. If the gauge group is SU(3), then due to
this cubic interaction and that of the ghost Ô¨Åelds (16.255), the running coupling
constant gŒº is to order g3
M
gŒº = gM

1 ‚àí11g2
M
16œÄ2 ln
 Œº
M

.
(17.20)
It differs from gM only by terms of order g3
M and so satisÔ¨Åes the differential
equation
ŒºdgŒº
dŒº ‚â°Œ≤(gŒº) = ‚àí11g3
M
16œÄ2 ‚âà‚àí
11g3
Œº
16œÄ2 ,
(17.21)
in which the Œ≤-function is negative. Integrating
ln E
M =
 E
M
dŒº
Œº =
 gG
gM
dgŒº
Œ≤(gŒº) = ‚àí16œÄ2
11
 gE
gM
dgŒº
g3Œº
= 8œÄ2
11

1
g2
M
‚àí1
g2
E

(17.22)
we Ô¨Ånd
g2
E = g2
M

1 + 11g2
M
8œÄ2
ln E
M
‚àí1
,
(17.23)
which shows that as the energy E of a scattering process increases, the run-
ning coupling slowly decreases, going to zero at inÔ¨Ånite energy, an effect
calledasymptotic freedom.
If the gauge group is SU(N), and the theory has nf Ô¨Çavors of quarks with
masses below Œº, then the beta function is
Œ≤(gŒº) = ‚àí
g3
Œº
4œÄ2
11N
12 ‚àínf
6

,
(17.24)
which remains negative as long as nf < 11N/2. Using this beta function with
N = 3 and again integrating, we get instead of (17.23)
g2
E = g2
M

1 + (11 ‚àí2nf/3)g2
M
16œÄ2
ln E2
M2
‚àí1
.
(17.25)
So with
2 ‚â°M2 exp

‚àí
16œÄ2
(11 ‚àí2nf/3)g2
M

(17.26)
629

THE RENORMALIZATION GROUP
20
40
60
80
100
120
140
0.1
0.12
0.14
0.16
0.18
0.2
0.22
Asymptotic freedom
Energy E (GeV)
Coupling strength Œ±s(E)
Figure 17.1
The strong-structure constant Œ±s(E) as given by the one-loop formula
(17.27) (thin curve) and by a three-loop formula (thick curve) with  = 230 MeV
and nf = 5 is plotted for mb ‚â™E ‚â™mt.
we Ô¨Ånd (exercise 17.2)
Œ±s(E) ‚â°g2(E)
4œÄ
=
12œÄ
(33 ‚àí2nf) ln(E2/2).
(17.27)
This formula expresses the dimensionless strong-structure constant Œ±s(E) appro-
priate to energy E in terms of a parameter  that has the dimension of energy.
Some call this dimensional transmutation. For  = 230 MeV and nf = 5,
Fig. 17.1 displays Œ±s(E) in the range 4.19 = mb ‚â™E ‚â™mt = 172 GeV
as given by the one-loop formula (17.27) (thin curve) and a three-loop for-
mula (Weinberg, 1996, p. 156) (thick curve).
17.2 The renormalization group in lattice Ô¨Åeld theory
Let us consider a quantum Ô¨Åeld theory on a lattice (Gattringer and Lang,
2010, chap. 3) in which the strength of the nonlinear interactions depends upon
a single dimensionless coupling constant g. The spacing a of the lattice regulates
the inÔ¨Ånities, which return as a ‚Üí0. The value of an observable P computed on
630

17.2 THE RENORMALIZATION GROUP IN LATTICE FIELD THEORY
this lattice will depend upon the lattice spacing a and on the coupling constant
g, and so will be a function P(a, g) of these two parameters. The right value of
the coupling constant is the value that makes the result of the computation be as
close as possible to the physical value P. So the correct coupling constant is not
a constant at all, but rather a function g(a) that varies with the lattice spacing
or cut off a. Thus, as we vary the lattice spacing and go to the continuum limit
a ‚Üí0, we must adjust the coupling function g(a) so that what we compute,
P(a, g(a)), is equal to the physical value P. That is, g(a) must vary with a so as
to keep P(a, g(a)) = P. But then P(a, g(a)) must remain constant as a varies, so
dP(a, g(a))
da
= 0.
(17.28)
Writing this condition as a dimensionless derivative
a dP(a, g(a))
da
=
da
d ln a
dP(a, g(a))
da
= dP(a, g(a))
d ln a
= 0
(17.29)
we arrive at the Callan‚ÄìSymanzik equation
0 = dP(a, g(a))
d ln a
=

‚àÇ
‚àÇln a +
dg
d ln a
‚àÇ
‚àÇg

P(a, g(a)).
(17.30)
The coefÔ¨Åcient of the second partial derivative with a minus sign
Œ≤L(g) ‚â°‚àídg
d ln a
(17.31)
is the lattice Œ≤-function. Since the lattice spacing a and the energy scale Œº
are inversely related, the lattice Œ≤-function differs from the continuum beta
function by a minus sign.
In SU(N) gauge theory, the Ô¨Årst two terms of the lattice Œ≤-function for small
g are
Œ≤L(g) = ‚àíŒ≤0 g3 ‚àíŒ≤1 g5
(17.32)
where for nf Ô¨Çavors of light quarks
Œ≤0 =
1
(4œÄ)2
11
3 N ‚àí2
3nf

Œ≤1 =
1
(4œÄ)4

34
3 N2 ‚àí10
3 Nnf ‚àíN2 ‚àí1
N
nf

.
(17.33)
In quantum chromodynamics, N = 3.
Combining the deÔ¨Ånition (17.31) of the Œ≤-function with its expansion (17.32)
for small g, one arrives at the differential equation
dg
d ln a = Œ≤0 g3 + Œ≤1 g5,
(17.34)
631

THE RENORMALIZATION GROUP
which one may integrate

d ln a = ln a + c =

dg
Œ≤0g3 + Œ≤1g5 = ‚àí
1
2Œ≤0g2 + Œ≤1
2Œ≤2
0
ln

Œ≤0 + Œ≤1g2
g2

(17.35)
to Ô¨Ånd
a(g) = d

Œ≤0 + Œ≤1g2
g2
Œ≤1/2Œ≤2
0
e‚àí1/2Œ≤0g2,
(17.36)
in which d is a constant of integration. The term Œ≤1g2 is of higher order in g,
and if one drops it and absorbs a factor of Œ≤2
0 into a new constant of integration
, then one gets
a(g) = 1


Œ≤0g2‚àíŒ≤1/2Œ≤2
0 e‚àí1/2Œ≤0g2.
(17.37)
As g ‚Üí0, the lattice spacing a(g) goes to zero very fast (as long as nf < 17 for
N = 3). The inverse of this relation (17.37) is
g(a) ‚âà

Œ≤0 ln(a‚àí2‚àí2) + (Œ≤1/Œ≤0) ln

ln(a‚àí2‚àí2)
‚àí1/2
.
(17.38)
It shows that the coupling constant slowly goes to zero with a, which is a lattice
version of asymptotic freedom.
17.3 The renormalization group in condensed-matter physics
The study of condensed matter is concerned mainly with properties that emerge
in the bulk, such as the melting point, the boiling point, or the conductivity. So
we want to see what happens to the physics when we increase the distance scale
many orders of magnitude beyond the size a of an individual molecule or the
distance between nearest neighbors.
As a simple example, let‚Äôs consider a euclidean action in d dimensions
S =

ddx

1
2(‚àÇœÜ)2 +

n
gnœÜn

,
(17.39)
in which g2œÜ2 ‚â°m2œÜ2/2 is a mass term and g4œÜ4 ‚â°ŒªœÜ4/24 is a quartic self-
interaction. In terms of an ultraviolet cut off  = 1/a, we may deÔ¨Åne a partition
function
Z() =


e‚àíSDœÜ
(17.40)
to be one in which the Ô¨Åeld
œÜ(x) =


eikxœÜ(k) ddk
(2œÄ)d
(17.41)
632

17.3 THE RENORMALIZATION GROUP IN CONDENSED-MATTER PHYSICS
only has Fourier coefÔ¨Åcients œÜ(k) with k2 < 2. Corresponding to each such
Ô¨Åeld œÜ(x), we introduce a ‚Äústretched‚Äù Ô¨Åeld
œÜL(x) = A(L) œÜ(x/L)
for
L ‚â•1,
(17.42)
in which A(L) is a scale factor that we will use to keep the kinetic part of the
action invariant. Since
œÜL(x) = A(L) œÜ(x/L) = A(L)


exp

ikx
L

œÜ(k) ddk
(2œÄ)d
(17.43)
the momenta of the stretched Ô¨Åeld are reduced by the factor 1/L.
We may deÔ¨Åne a new partition function in which we integrate over the
stretched Ô¨Åelds œÜL(x)
Z(/L) =

/L
e‚àíSDœÜ ‚â°


e‚àíSDœÜL.
(17.44)
The kinetic action of a stretched Ô¨Åeld is
Sk =

ddx A2(L)
2
‚àÇœÜ(x/L)
‚àÇx
2
=

dd(x/L) Ld A2(L)
2
‚àÇœÜ(x/L)
L‚àÇx/L
2
(17.45)
and so if we choose
A(L) = L‚àí(d‚àí2)/2
(17.46)
then letting x‚Ä≤ = x/L, we Ô¨Ånd that the kinetic action Sk is invariant
Sk =

ddx‚Ä≤ 1
2
‚àÇœÜ(x‚Ä≤)
‚àÇx‚Ä≤
2
.
(17.47)
The full action of a stretched Ô¨Åeld is
S(œÜL) =

ddx

1
2(‚àÇœÜ)2 +

n
gd,n(L)œÜn

,
(17.48)
in which
gd,n(L) = LdAn(L) gn = Ld‚àín(d‚àí2)/2 gn.
(17.49)
The beta function
Œ≤(gn) ‚â°
L
gd,n(L)
dgd,n(L)
dL
= d ‚àín(d ‚àí2)/2
(17.50)
is just the exponent of the coupling ‚Äúconstant‚Äù gd,n(L). If it is positive, then the
coupling constant gd,n(L) gets stronger as L ‚Üí‚àû; such couplings are called
relevant. Couplings with vanishing exponents are insensitive to changes in L
and are marginal. Those with negative exponents shrink with increasing L; they
are irrelevant.
633

THE RENORMALIZATION GROUP
The coupling constant gd,n,p of a term with p derivatives and n powers of the
Ô¨Åeld in a space of d dimensions œÜ varies as
gd,n,p(L) = Ld An(L) L‚àíp gn,p = Ld‚àín(d‚àí2)/2‚àíp gn,p.
(17.51)
Example 17.3 (QCD)
In quantum chromodynamics, there is a cubic term
g fabc Aa
0Ab
i ‚àÇ0Ac
i , which in effect looks like g fabc œÜaœÜb ÀôœÜc. Is it relevant? Well, if
we stretch space but not time, then the time derivative has no effect, and d = 3.
So the cubic, n = 3, grows as L3/2
g3,3,0(L) = Ld‚àín(d‚àí2)/2 gabc = L3/2 g3,0.
(17.52)
Since this cubic term drives asymptotic freedom, its strengthening as space is
stretched by the dimensionless factor L may point to a qualitative explana-
tion of conÔ¨Ånement. For if g3,3,0(L) grows with distance as L3/2, then Œ±s(L) =
g2
3,3,0(L)/4œÄ grows as L3, and so the strength Œ±s(Lr)/(Lr)2 of the force between
two quarks separated by a distance Lr grows linearly with the distance
F(Lr) = Œ±s(Lr)
(Lr)2 = L3 Œ±s(r)
(Lr)2
= Lr Œ±s(r)
r3 ,
(17.53)
which is more than enough for quark conÔ¨Ånement.
On the other hand, if we stretch both space and time, then the cubic g4,3,1(L)
and quartic g4,4,0(L) couplings are marginal.
Exercises
17.1 Show that for Œº2 ‚â´m2, the vacuum polarization term (17.10) reduces to
(17.12). Hint: use ln a b = ln a + ln b when integrating.
17.2 Show that by choosing the energy scale  according to (17.26), one can derive
(17.27) from (17.25).
17.3 Show that if we stretch both space and time, then in the notation of (17.51),
the cubic g4,3,1(L) and quartic g4,4,0(L) couplings are marginal, that is, are
independent of L.
634

18
Chaos and fractals
18.1 Chaos
Early in the last century, Henri Poincar√© studied the three-body problem and
found very complicated orbits. In this and other systems, he found that after a
transient period, classical motion assumes one of four forms:
1 periodic (a limit cycle),
2 steady or damped or stopped,
3 quasi-periodic (more than one frequency),
4 chaotic.
Example 18.1 (DufÔ¨Ång‚Äôs equation)
If one attaches a thin piece of iron to the
end of a rod that moves sinusoidally in the x direction at frequency œâ near two
magnets then the x coordinate is described by the forced DufÔ¨Ång equation
¬®x + aÀôx + bx3 + cx = g sin(œât + œÜ).
(18.1)
For suitable values of a, b, c, g, œâ, and œÜ, the coordinate x varies chaotically.
Example 18.2 (Dripping faucet)
Drops from a slowly dripping faucet tend to
fall regularly at times tn separated by a constant interval t = tn+1 ‚àítn. At
a slightly higher Ô¨Çow rate, the drops fall separated by intervals that alternate
in their durations t, T, t, T, t, T in a period-two sequence. At some
higher Ô¨Çow rates, no regularity is apparent.
Example 18.3 (Rayleigh-Benard convection)
Consider a Ô¨Çuid in a gravitational
Ô¨Åeld lies above a hot plate and below a cold one. If the difference T is small
enough, then steady convective cellular Ô¨Çow occurs. But if T is above the
chaotic threshold, the Ô¨Çuid boils chaotically.
635

CHAOS AND FRACTALS
x1
x2
x3
x4
x5
x6
>
>
An orbit and its crossings of a line
Figure 18.1
The orbit or trajectory of a dynamical system in N dimensions
generates a map of crossings in N ‚àí1 dimensions.
The N equations
Àôxi = Fi(x)
(18.2)
represent an N-dimensional dynamical system; they are said to be autonomous
because they involve x(t) but not t itself, that is, Àôxi = Fi(x), not Fi(x, t). The
crossings of a suitably oriented plane (or more generally of a Poincar√© surface of
section) lead to an invertible map
xn+1 = M(xn)
(18.3)
in a space of N ‚àí1 dimensions, as shown in Fig. 18.1.
A Ô¨Årst-order, autonomous dynamical system like (18.2) can behave chaotically
only if
N ‚â•3.
(18.4)
Example 18.4 (Driven, damped pendulum)
The angle Œ∏ of a sinusoidally
driven, damped pendulum follows the differential equation
¬®Œ∏ + f ÀôŒ∏ + sin Œ∏ = T sin(œât),
(18.5)
which is second order and nonautonomous. Will this system exhibit chaos? We
put it into autonomous form by deÔ¨Åning x1 = ÀôŒ∏, x2 = Œ∏, and x3 = œât. In these
variables, the pendulum equation (18.5) is the Ô¨Årst-order autonomous system
636

18.1 CHAOS
Àôx1 = T sin x3 ‚àísin x2 ‚àífx1,
Àôx2 = x1,
Àôx3 = œâ
(18.6)
with N = 3 dependent variables. So chaos is not excluded and is exhibited by
numerical solutions for suitable values of the parameters f , T, and œâ.
An invertible map like (18.3)
xn = M‚àí1(xn+1)
(18.7)
can be chaotic only if it has at least two dimensions. This condition is consistent
with condition (18.4) because the Poincar√© section of an N-dimensional dynam-
ical system like (18.2) is an (N-1)-dimensional invertible map as in Fig. 18.1.
Example 18.5 (Logistic map)
A map that is not invertible can display chaos
even in one dimension. The one-dimensional logistic map
xn+1 = q xn(xn ‚àí1),
(18.8)
which is not invertible (because the quadratic equation for xn in terms of xn+1
has two solutions), displays chaos in increasingly striking forms as q exceeds a
number slightly greater than 3.57. And at q = 3.8, two sequences respectively
starting at x0 = 0.2 and x‚Ä≤
0 = 0.20001 differ by more than 0.2 after only 21
iterations (Fig. 18.2).
0
5
10
15
20
25
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Two diverging sequences of the logistic map
n
xn
x0 = 0.20000
x0 = 0.20001
Figure 18.2
At q = 3.8, the logistic map (18.8) is so sensitive to initial conditions
that after only 21 iterations the sequences starting from x0 = 0.2 and x0 = 0.20001
differ by 0.218.
637

CHAOS AND FRACTALS
By q = 4, the logistic map (18.8) is totally chaotic and is equivalent to the tent
map
xn+1 = 1 ‚àí2|xn ‚àí1|.
(18.9)
A similar but simpler chaotic map is the 2x modulo 1 map
xn+1 = 2xn mod 1.
(18.10)
Example 18.6 (The Bernoulli shift)
The simplest chaotic map is the Bernoulli
shift, in which the initial point x0 is an arbitrary number between 0 and 1 with
the binary-decimal expansion
x0 =
‚àû

j=1
2‚àíjaj = 0.a1a2a3a4 . . .
(18.11)
and successive points lack a1, then a2, etc.:
x1 = 0.a2a3a4a5 . . .
x2 = 0.a3a4a5a6 . . .
x3 = 0.a4a5a6a7 . . . .
(18.12)
Two unequal irrational numbers x0 and x‚Ä≤
0, no matter how close, generate
sequences that roam independently, irregularly, and ergotically over the interval
(0, 1).
Example 18.7 (H√©non‚Äôs map)
The two-dimensional map
xn+1 = f (xn) + B yn,
yn+1 = xn,
(18.13)
for B Ã∏= 0 is invertible. If f (xn) = A ‚àíx2
n, it is H√©non‚Äôs map, which for A = 1.4
and B = 0.3 is chaotic and converges to the attractor in Fig. 18.6.
The Lyapunov exponent of a smooth map xn+1 = f (xn) is the limit
h(x1) = lim
n‚Üí‚àû
1
n

ln |f ‚Ä≤(x1)| + ¬∑ ¬∑ ¬∑ + ln |f ‚Ä≤(xn)|

.
(18.14)
A bounded sequence that has a positive Lyapunov exponent and that does not
converge to a periodic sequence is chaotic (Alligood et al., 1996, p. 110). Other
aspects of chaos lead to other deÔ¨Ånitions.
638

18.3 FRACTALS
Attractors
Figure 18.3
On the left, the origin is an attractor; on the right, the circle is a limit
cycle.
18.2 Attractors
If the states of a dynamical system converge, as for instance in Fig. 18.3, to a
point or set of points, that point or set is called an attractor.
Example 18.8 (The van der Pol equation)
If the attractor is a loop that the
states run around, then the attractor is called a limit cycle. The van der Pol
equation
¬®y + (y2 ‚àíŒ∑)Àôy + œâ2y = 0,
(18.15)
which we may write as the Ô¨Årst-order system
x1 = Àôy,
Àôx1 = ‚àíœâ2x2 ‚àí(x2
2 ‚àíŒ∑)x1,
x2 = y,
Àôx2 = x1,
(18.16)
has a limit cycle. Physicists used the van der Pol equation in the 1920s to study
vacuum-tube oscillators.
18.3 Fractals
A fractal set has a dimension that is not an integer. How can that be?
Felix Hausdorff and Abram Besicovitch showed how to deÔ¨Åne the dimension
of a weird set of points. To compute the fractal or box-counting dimension of
a set, we cover it with line segments, squares, cubes, or n-dimensional ‚Äúboxes‚Äù
639

CHAOS AND FRACTALS
‚àí0.2
0
0.2
0.4
0.6
0.8
1
1.2
Toward the Cantor set
Figure 18.4
The Ô¨Årst four approximations to the Cantor set.
of side œµ. If we need N(œµ) boxes, then the fractal dimension Db is the limit as
œµ ‚Üí0
Db = lim
œµ‚Üí0
ln(N(œµ))
ln(1/œµ) .
(18.17)
For instance, we can cover the interval [a, b] with N(œµ) = (b‚àía)/œµ line segments
of length œµ, so the dimension of the segment [a, b] is
Db = lim
œµ‚Üí0
ln(N(œµ))
ln(1/œµ) = lim
œµ‚Üí0
ln((b ‚àía)/œµ)
ln(1/œµ)
= 1 + ln(b ‚àía)
ln(1/œµ) = 1
(18.18)
as it should be.
Example 18.9 (Cantor set)
The Cantor set is deÔ¨Åned by a limiting process in
which the set at the nth stage consists of 2n line segments each of length 1/3n.
The Ô¨Årst four approximations to the Cantor set are sketched in Fig. 18.4. We
can cover the nth approximation with N(œµ) = 2n line segments each of length
œµn = 1/3n, and so the fractal dimension is
Db,C = lim
œµ‚Üí0
ln(N(œµ))
ln(1/œµ) = lim
n‚Üí‚àû
ln(N(œµn))
ln(1/œµn) = lim
n‚Üí‚àû
ln(2n)
ln(3n) = ln 2
ln 3 = 0.6309297 . . . ,
(18.19)
which is not an integer or even a rational number.
640

18.3 FRACTALS
Example 18.10 (Koch snowÔ¨Çake)
In 1904, the Swedish mathematician Helge
von Koch described the Koch curve (or the Koch snowÔ¨Çake), whose construc-
tion is shown in Fig. 18.5. With each step, there are four times as many line
segments, each one being three times smaller. The length L of the curve at step n
is thus
L =
4
3
n
,
(18.20)
which grows without limit as n ‚Üí‚àû. Its box dimension is
Db,K = lim
n‚Üí‚àû
ln(N(œµn))
ln(1/œµn) = lim
n‚Üí‚àû
ln(4n)
ln(3n) = ln 4
ln 3 = 1.2618595 . . .
(18.21)
The Koch snowflake
Figure 18.5
Curve of von Koch: steps 0, 1, 2, and 3 of construction (http://en-
wikipedia.org/wiki/File:kochFlake.org).
Closely related to the box-counting dimension is the self-similar dimension
Ds. To deÔ¨Åne it, we consider the number of self-similar structures of linear size
x needed to cover the Ô¨Ågure after n steps and take the limit
Ds = lim
x‚Üí0
ln N(x)
ln 1/x .
(18.22)
In the case of von Koch‚Äôs curve, x = 1/3n and N(x) = 4n. So the self-similar
dimension of von Koch‚Äôs curve is
Ds,K = lim
x‚Üí0
ln N(x)
ln 1/x = lim
n‚Üí‚àû
ln 4n
ln 3n = n ln 4
n ln 3 = ln 4
ln 3 = 1.2618595 . . .
(18.23)
which is equal to its box dimension Db,K given by (18.21).
641

CHAOS AND FRACTALS
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
2
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
2
H√©non‚Äôs strange attractor
x
y
Figure 18.6
Points 20‚Äì10,020 of H√©non‚Äôs map (18.7) with A = 1.4 and B = 0.3 and
(x1, y1) = (0, 0).
Attractors of fractal dimension are strange. H√©non‚Äôs map (18.7) with A =
1.4 and B
=
0.3 is chaotic with a strange attractor, as illustrated by
Fig. 18.6.Chaotic systems often have strange attractors; but chaotic systems can
have nonfractal attractors, and nonchaotic systems can have strange attractors.
Further reading
The book Chaos: an Introduction to Dynamical Systems (Alligood et al., 1996)
is superb.
Exercises
18.1 A period-one sequence of a map xn+1 = f (xn) is a point p for which p = f (p).
Find the period-one sequences of xn+1 = rxn(1 ‚àíxn/K).
18.2 A period-two sequence of a map xn+1 = f (xn) is two different points p and
q for which q = f (p) and p = f (q). Estimate the period-two sequences of the
logistic map f (x) = ax(1 ‚àíx) for a = 1, 2, and 3. Hint: graph the functions
f (f (x)) and I(x) = x on the interval [0,1].
18.3 A period-three sequence of a map xn+1 = f (xn) is three different points p, q,
and r for which q = f (p), r = f (q), and p = f (r). Li and Yorke have shown
that a map with a period-three sequence is chaotic. Estimate the period-three
sequences of the map f (x) = 4x(1 ‚àíx).
642

19
Strings
19.1 The inÔ¨Ånities of quantum Ô¨Åeld theory
Quantum Ô¨Åeld theory is plagued with inÔ¨Ånities. Even exactly soluble theories of
free Ô¨Åelds have ground-state energies that diverge quarticly, as +4 for a theory
of bosons and as ‚àí4 for a theory of fermions as  ‚Üí‚àû. Theories with the
same number of boson and fermion Ô¨Åelds are less divergent, and theories with
unbroken supersymmetry actually have ground-state energies that vanish.
One may be able to eliminate some of the inÔ¨Ånities of a generic quantum Ô¨Åeld
theory without spoiling the symmetries of its action density L by replacing L
in the path integral by L‚Ä≤ = L/(1 ‚àíL/M4) or L‚Ä≤ = M4 
exp(L/M4) ‚àí1

. In
euclidean space, the substitution
L‚Ä≤
e =
Le
1 ‚àíLe/M4
(19.1)
with the understanding that L‚Ä≤
e = ‚àûwhen |Le| > M4 almost certainly removes
many inÔ¨Ånities, but whether it preserves all the symmetries is less obvious.
These substitutions change quantum Ô¨Åeld theory, but in the limit M ‚Üí‚àû,
one recovers standard quantum Ô¨Åeld theory.
A more physical approach is to represent elementary particles as objects that
have Ô¨Ånite size. Those that are one-dimensional are called strings.
19.2 The Nambu‚ÄìGoto string action
If we give up the idea of point particle then the next simplest choice is a one-
dimensional string. We‚Äôll use 0 ‚â§œÉ ‚â§œÉ1 and œÑi ‚â§œÑ ‚â§œÑf to parametrize
the space-time coordinates XŒº(œÉ, œÑ) of the string. Nambu and Goto suggested
using as the action the area
643

STRINGS
S = ‚àíT0
c
 œÑf
œÑi
 œÉ1
0
 ÀôX ¬∑ X‚Ä≤2 ‚àí
 ÀôX
2 (X‚Ä≤)2 dœÑ dœÉ,
(19.2)
in which
ÀôXŒº = ‚àÇXŒº
‚àÇœÑ
and
X‚Ä≤Œº = ‚àÇXŒº
‚àÇœÉ
(19.3)
and a Lorentz metric Œ∑ŒºŒΩ = diag(‚àí1, 1, 1, . . . ) is used to form the inner
products like
ÀôX ¬∑ X‚Ä≤ = ÀôXŒºŒ∑ŒºŒΩXŒΩ‚Ä≤
etc.
(19.4)
This action is the area swept out by a string of length œÉ1 in time œÑf ‚àíœÑi.
If ÀôXdœÑ = dt points in the time direction and X‚Ä≤dœÉ = dr points in a spatial
direction, then it is easy to see that ÀôX ¬∑ X‚Ä≤ = 0, that ‚àí
 ÀôX
2 dœÑ 2 = dt2, and that

X‚Ä≤2 dœÉ 2 = dr2. So in this simple case, the action (19.2)
S = ‚àíT0
c
 tf
ti
 r1
0
dt dr = ‚àíT0
c (tf ‚àíti)r1
(19.5)
is the area the string sweeps out. The other term within the square-root ensures
that the action is the area swept out for all ÀôX and X‚Ä≤, and that it is invariant
under arbitrary reparametrizations œÉ ‚ÜíœÉ ‚Ä≤ and œÑ ‚ÜíœÑ ‚Ä≤.
The equation of motion for the relativistic string follows from the require-
ment that the action (19.2) be stationary, Œ¥S = 0. Since
Œ¥ ÀôXŒº = Œ¥ ‚àÇXŒº
‚àÇœÑ
= ‚àÇ( ÀôXŒº + Œ¥XŒº)
‚àÇœÑ
‚àí‚àÇXŒº
‚àÇœÑ
= ‚àÇŒ¥XŒº
‚àÇœÑ
(19.6)
and similarly
Œ¥X‚Ä≤Œº = ‚àÇŒ¥XŒº
‚àÇœÉ
(19.7)
we may express the change in the action in terms of derivatives of the Lagrange
density
L = ‚àíT0
c
 ÀôX ¬∑ X‚Ä≤2 ‚àí
 ÀôX
2 (X‚Ä≤)2
(19.8)
as
Œ¥S =
 œÑf
œÑi
 œÉ1
0
# ‚àÇL
‚àÇÀôXŒº
‚àÇŒ¥XŒº
‚àÇœÑ
+
‚àÇL
‚àÇX‚Ä≤Œº
‚àÇŒ¥XŒº
‚àÇœÉ
$
dœÑ dœÉ.
(19.9)
Its derivatives, which we‚Äôll call PœÑ
Œº and PœÉ
Œº, are
PœÑ
Œº = ‚àÇL
‚àÇÀôXŒº = ‚àíT0
c
( ÀôX ¬∑ X‚Ä≤)X‚Ä≤
Œº ‚àí(X‚Ä≤)2 ÀôXŒº
 ÀôX ¬∑ X‚Ä≤2 ‚àí
 ÀôX
2 (X‚Ä≤)2
(19.10)
644

19.2 THE NAMBU‚ÄìGOTO STRING ACTION
and
PœÉ
Œº =
‚àÇL
‚àÇX‚Ä≤Œº = ‚àíT0
c
( ÀôX ¬∑ X‚Ä≤) ÀôXŒº ‚àí(X‚Ä≤)2X‚Ä≤
Œº
 ÀôX ¬∑ X‚Ä≤2 ‚àí
 ÀôX
2 (X‚Ä≤)2
.
(19.11)
In terms of them, the change in the action is
Œ¥S =
 œÑf
œÑi
 œÉ1
0
# ‚àÇ
‚àÇœÑ

Œ¥XŒºPœÑ
Œº

+ ‚àÇ
‚àÇœÉ

Œ¥XŒºPœÉ
Œº

‚àíŒ¥XŒº
‚àÇPœÑ
Œº
‚àÇœÑ
+
‚àÇPœÉ
Œº
‚àÇœÉ
$
dœÑ dœÉ.
(19.12)
The total œÑ-derivative integrates to a term involving the variation Œ¥XŒº, which
we require to vanish at the initial and Ô¨Ånal values of œÑ. So we drop that term
and Ô¨Ånd that the net change in the action is
Œ¥S =
 œÑf
œÑi

Œ¥XŒºPœÉ
Œº
œÉ1
0 dœÑ ‚àí
 œÑf
œÑi
 œÉ1
0
Œ¥XŒº
‚àÇPœÑ
Œº
‚àÇœÑ
+
‚àÇPœÉ
Œº
‚àÇœÉ

dœÑ dœÉ.
(19.13)
Thus the equations of motion for the string are
‚àÇPœÑ
Œº
‚àÇœÑ
+
‚àÇPœÉ
Œº
‚àÇœÉ
= 0
(19.14)
but the action is stationary only if the boundary condition
Œ¥XŒº(œÑ, œÉ1)PœÉ
Œº(œÑ, œÉ1) ‚àíŒ¥XŒº(œÑ, 0)PœÉ
Œº(œÑ, 0) = 0
(19.15)
is satisÔ¨Åed for all œÑ. This is a condition on the ends of open strings; closed
strings satisfy it automatically.
Usually the boundary condition (19.15) is interpreted as 2D = 2(d + 1)
conditions ‚Äì one for each end œÉ‚àóof the string and each dimension Œº of
space-time:
Œ¥XŒº(œÑ, œÉ‚àó)PœÉ
Œº(œÑ, œÉ‚àó) = 0,
no sum over Œº.
(19.16)
A Dirichlet boundary condition Ô¨Åxes a spatial component at an end of the
string by
ÀôXi(œÑ, œÉ‚àó) = 0
(19.17)
or equivalently by Œ¥XŒº(œÑ, œÉ‚àó) = 0. The time component X0 can not have a
vanishing œÑ derivative, so it must obey a free-endpoint-boundary condition
PœÉ
Œº(œÑ, œÉ‚àó) = 0,
(19.18)
which also may apply to any dimension and any end.
645

STRINGS
19.3 Regge trajectories
The quantity PœÑ
Œº(œÑ, œÉ) deÔ¨Åned as the derivative (19.10) turns out to be the
momentum density of the string. The angular momentum M12 of a string
rigidly rotating in the x, y plane is
M12(œÑ) =
 œÉ1
0
X1PœÑ
2 (œÑ, œÉ) ‚àíX2PœÑ
1 (œÑ, œÉ) dœÉ.
(19.19)
In a parametrization of the string with œÑ = t and dœÉ proportional to the energy
density dE of the string, the x, y coordinates of the string are
‚ÉóX(t, œÉ) = œÉ1
œÄ cos œÄœÉ
œÉ1

cos œÄct
œÉ1
, sin œÄct
œÉ1

.
(19.20)
The x, y components of the momentum density are
‚ÉóPœÑ(t, œÉ) = T0
c
‚àÇ‚ÉóX
‚àÇt = T0
c cos œÄœÉ
œÉ1

‚àísin œÄct
œÉ1
, cos œÄct
œÉ1

.
(19.21)
The angular momentum (19.19) is then given by the integral
M12 = œÉ1
œÄ
T0
c
 œÉ1
0
cos2 œÄœÉ
œÉ1
dœÉ = œÉ 2
1 T0
2œÄc .
(19.22)
Now the parametrization dœÉ ‚àùdE implies that œÉ1 ‚àùE, and in fact the energy
of the string is E = T0œÉ1. Thus the angular momentum J = |M12| of a classical
relativistic string is proportional to the square of its total energy
J =
E2
2œÄT0c.
(19.23)
This rule is obeyed by many meson and baryon resonances. The nucleon and
Ô¨Åve baryon resonances Ô¨Åt it with nearly the same value of the string tension
T0 ‚âà0.92 GeV/fm
(19.24)
as shown by Fig. 19.1, which displays the Regge trajectories of the N and 
resonances on a single curve. Other N and  resonances, however, do not fall
on this curve.
A string theory of hadrons took off in 1968 when Gabriel Veneziano pub-
lished his amplitude for œÄ + œÄ scattering as a sum of three Euler beta
functions (Veneziano, 1968). But after eight years of intense work, this effort
was largely abandoned with the discovery of quarks at SLAC and the promise
of QCD as a theory of the strong interactions. In 1974, Jo√´l Scherk and John
H. Schwarz proposed increasing the string tension by 38 orders of magnitude
so as to use strings to make a quantum theory that included gravity (Scherk
646

19.5 D-BRANES
0.8
1
1.2
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
0
1
2
3
4
5
6
Regge trajectory of the nucleon
Energy E = mc2 (GeV) of baryon resonance
Spin J/h¬Ø
N(938)
Œî(1232)
N(1680)
Œî(1950)
N(2220)
Œî(2420)
Figure 19.1
The angular momentum and energy of the nucleon and Ô¨Åve baryon res-
onances approximately Ô¨Åt the curve J/¬Øh = T0 E2 with string tension T0 = 0.92
GeV/fm.
and Schwarz, 1974). They identiÔ¨Åed the graviton as an excitation of the closed
string.
19.4 Quantized strings
The coordinates XŒº may be quantized most easily in light-cone coordinates.
The resulting relativistic bosonic string must live in a space-time of exactly D =
d + 1 = 26 dimensions with a tachyon. But if one adds fermionic variables
œàŒº
1 (œÑ, œÉ) and œàŒº
2 (œÑ, œÉ) in a supersymmetric way, then the tachyon goes away,
and the number of space-time dimensions drops to exactly 10. There are Ô¨Åve
distinct superstring theories ‚Äì types I, IIA, and IIB; E8 ‚äóE8 heterotic; and
SO(32) heterotic. All Ô¨Åve may be related to a single theory in 11 dimensions
called M-theory, which is not a string theory. M-theory contains membranes
(2-branes) and 5-branes, which are not D-branes.
19.5 D-branes
One may satisfy Dirichlet boundary conditions (19.17) by requiring the ends of
a string to be attached to a spatial manifold, called a D-brane after Dirichlet.
If the manifold to which the string is stuck has p dimensions, then it‚Äôs called a
647

STRINGS
Two strings stuck on a D2-brane
Figure 19.2
Two strings stuck on a D2-brane.
Dp-brane. Figure (19.2) shows a string whose ends are free to move only within
a D2-brane.
Dp-branes offer a natural way to explain the extra six dimensions required
in a universe of superstrings. One imagines that the ends of all the strings are
free to move only in our four-dimensional space-time; the strings are stuck on
a D3-brane, which is the three-dimensional space of our physical Universe. The
tension of the superstring then keeps it from wandering far enough into the
extra six spatial dimensions for us ever to have noticed.
This explanation conÔ¨Çicts, however, with one interpretation of gravity in
string theory. In that view, the graviton and the spin-3/2 gravitino are modes of
the closed string, are not attached to any D-brane, and propagate in all d space
dimensions. Thus if the extra space dimensions were huge, then the gravitational
force, which is mainly due to the graviton, would fall off with the distance r as
1/rd‚àí1 = 1/r8 for superstrings (1/r24 for bosonic strings), both much faster
than 1/r2.
19.6 String‚Äìstring scattering
Strings interact by joining and by breaking. Figure 19.3 shows two open strings
joining to form one open string and then breaking into two open strings. Fig-
ure 19.4 shows two closed strings joining to form one closed string and then
breaking into two closed strings. The interactions of strings do not occur at
points.
Because the fundamental things of string theory are extended objects, string
theory is intrinsically free of ultraviolet divergences. It provides a Ô¨Ånite theory
of quantum gravity.
648

19.7 RIEMANN SURFACES AND MODULI
Two-to-two scattering of open strings
Space ‚Üí
Time ‚Üí
Figure 19.3
A space-time diagram of the scattering of two open strings into two open
strings.
Two-to-two scattering of closed strings
Space ‚Üí
Time ‚Üí
Figure 19.4
A space-time diagram of the scattering of two closed strings into two
closed strings.
19.7 Riemann surfaces and moduli
A homeomorphism is a map that is one to one and continuous with a continu-
ous inverse. A Riemann surface is a two-dimensional real manifold whose open
sets UŒ± are mapped onto open sets of the complex plane C by homeomorphisms
649

STRINGS
zŒ± whose transition functions zŒ± ‚ó¶z1
Œ≤ are analytic on the images of the inter-
sections UŒ± ‚à©UŒ≤. Two Riemann surfaces are equivalent if they are related by a
continuous analytic map that is one to one and onto.
A parameter that distinguishes a Riemann surface from other, inequiv-
alent Riemann surfaces is called a modulus. Some Riemann surfaces have
several moduli; others have one modulus; others none at all. Some moduli are
continuous parameters; others are discrete.
Further reading
Students who want to learn more about strings should look at the excellent
textbook A First Course in String Theory (Zwiebach, 2009).
Exercises
19.1 Derive formulas (19.10) and (19.11).
19.2 Derive equation (19.12) from (19.9, 19.10, & 19.11).
650

References
Aitken, A. C. 1959. Determinants and Matrices. Oliver and Boyd.
Alberts, Bruce, Johnson, Alexander, Lewis, Julian, Raff, Martin, Roberts, Keith,
and Walter, Peter. 2008. Molecular Biology of the Cell. 5th edn. Garland Science.
Page 246.
Alligood, Kathleen T., Sauer, Tim D., and Yorke, James A. 1996. Chaos: an
Introduction to Dynamical Systems. Springer-Verlag.
Arnold, V. I. 1989. Mathematical Methods of Classical Mechanics. 2nd edn.
Springer. Chapter 7.
Autonne, L. 1915. Sur les Matrices Hypohermitiennes et sur les Matrices Unitaires.
Ann. Univ. Lyon, Nouvelle S√©rie I, Fasc. 38, 1‚Äì77.
Bigelow, Matthew S., Lepeshkin, Nick N., and Boyd, Robert W. 2003. Superlumi-
nal and slow light propagation in a room-temperature solid. Science, 301(5630),
200‚Äì202.
Bordag, Michael, Klimchitskaya, Galina Leonidovna, Mohideen, Umar, and
Mostepanenko, Vladimir Mikhaylovich. 2009. Advances in the Casimir Effect.
Oxford University Press.
Bouchaud, Jean-Philippe, and Potters, Marc. 2003. Theory of Financial Risk and
Derivative Pricing. 2nd edn. Cambridge University Press.
Boyd, Robert W. 2000. Nonlinear Optics. 2nd edn. Academic Press.
Brillouin, L. 1960. Wave Propagation and Group Velocity. Academic Press.
Brunner, N., Scarani, V., Wegm√ºller, M., Legr√©, M., and Gisin, N. 2004. Direct
measurement of superluminal group velocity and signal velocity in an optical
Ô¨Åber. Phys. Rev. Lett., 93(20), 203902.
Cantelli, F. P. 1933. Sulla determinazione empirica di una legge di distribuzione.
Giornale dell‚ÄôInstituto Italiano degli Attuari, 4, 221‚Äì424.
Carroll, Sean. 2003. Spacetime and Geometry: an Introduction to General Relativity.
Benjamin Cummings.
Cohen-Tannoudji, Claude, Diu, Bernard, and Lalo√´, Frank. 1977. Quantum
Mechanics. Hermann & John Wiley.
651

REFERENCES
Courant, Richard. 1937. Differential and Integral Calculus, Vol. I. Interscience.
Courant, Richard and Hilbert, David. 1955. Methods of Mathematical Physics,
Vol. I. Interscience.
Creutz, Michael. 1983. Quarks, Gluons, and Lattices. Cambridge University Press.
Darden, Tom, York, Darrin, and Pedersen, Lee. 1993. Particle mesh Ewald: an
Nlog(N) method for Ewald sums in large systems. J. Chem. Phys., 98(12), 10089.
DeWitt, Bryce S. 1967. Quantum theory of gravity. II. The manifestly covariant
theory. Phys. Rev., 162(5), 1195‚Äì1239.
Dirac, P. A. M. 1967. The Principles of Quantum Mechanics. 4th edn. Oxford
University Press.
Dirac, P. A. M. 1996. General Theory of Relativity. Princeton University Press.
Faddeev, L. D. and Popov, V. N. 1967. Feynman diagrams for the Yang‚ÄìMills Ô¨Åeld.
Phys. Lett. B, 25(1), 29‚Äì30.
Feller, William. 1966. An Introduction to Probability Theory and Its Applications.
Vol. II. Wiley.
Feller, William. 1968. An Introduction to Probability Theory and Its Applications.
3rd edn. Vol. I. Wiley.
Feynman, Richard P. and Hibbs, A. R. 1965. Quantum Mechanics and Path
Integrals. McGraw-Hill.
Frieman, Joshua A., Turner, Michael S., and Huterer, Dragan. 2008. Dark energy
and the accelerating Universe. Ann. Rev. Astron. Astrophys., 46, 385‚Äì432.
arXiv:0803.0982v1 [astro-ph].
Gattringer, Christof and Lang, Christian B. 2010. Quantum Chromodynamics on the
Lattice: an Introductory Presentation. Springer (Lecture Notes in Physics).
Gehring, G. M., Schweinsberg, A., Barsi, C., Kostinski, N., and Boyd, R. W. 2006.
Observation of backwards pulse propagation through a medium with a negative
group velocity. Science, 312(5775), 895‚Äì897.
Gelfand, Israel M. 1961. Lectures on Linear Algebra. Interscience.
Gell-Mann, Murray. 1994. The Quark and the Jaguar. W. H. Freeman.
Gell-Mann, Murray. 2008. Plectics. Lectures at the University of New Mexico.
Georgi, H. 1999. Lie Algebras in Particle Physics. 2nd edn. Perseus Books.
Glauber, Roy J. 1963a. Coherent and incoherent states of the radiation Ô¨Åeld. Phys.
Rev., 131(6), 2766‚Äì2788.
Glauber, Roy J. 1963b. The quantum theory of optical coherence. Phys. Rev., 130(6),
2529‚Äì2539.
Glivenko, V. 1933. Sulla determinazione empirica di una legge di distribuzione.
Giornale dell‚ÄôInstituto Italiano degli Attuari, 4, 92‚Äì99.
Gnedenko, B. V. 1968. The Theory of Probability. Chelsea Publishing Co.
Gutzwiller, Martin C. 1990. Chaos in Classical and Quantum Mechanics. Springer.
Hau, L.V., Harris, S. E., Dutton, Z., and Behroozi, C. H. 1999. Light speed
reduction to 17 metres per second in an ultracold atomic gas. Nature, 397, 594.
Hobson, M. P., Efstathiou, G. P., and Lasenby, A. N. 2006. General Relativity: an
Introduction for Physicists. Cambridge University Press.
Holland, John H. 1975. Adaptation in Natural and ArtiÔ¨Åcial Systems. University of
Michigan Press.
652

REFERENCES
Ince, E. L. 1956. Integration of Ordinary Differential Equations. 7th edn. Oliver and
Boyd, Ltd. Chapter 1.
James, F. 1994. RANLUX: a Fortran implementation of the high-quality pseudo-
random number generator of L√ºscher. Comp. Phys. Comm., 79, 110.
Kleinert, Hagen. 2009. Path Integrals in Quantum Mechanics, Statistics, Polymer
Physics, and Financial Markets. World ScientiÔ¨Åc.
Knuth, Donald E. 1981. The Art of Computer Programming, Volume 2: Seminumer-
ical Algorithms. 2nd edn. Addison-Wesley.
Kolmogorov, Andrei Nikolaevich. 1933. Sulla determinazione empirica di una legge
di distribuzione. Giornale dell‚ÄôInstituto Italiano degli Attuari, 4, 83‚Äì91.
Langevin, Paul. 1908. Sur la th√©orie du mouvement brownien. Comptes Rend. Acad.
Sci. Paris, 146, 530‚Äì533.
Larson, D., Dunkley, J., Hinshaw, G., Komatsu, E., Nolta, M.R., et al. 2011. Seven-
year Wilkinson microwave anisotropy probe (WMAP) observations: power
spectra and WMAP-derived parameters. Astrophys. J. Suppl., 192, 16.
Lifshitz, E. M. 1956. The theory of molecular attractive forces between solids. Sov.
Phys. JETP, 2, 73.
Lin, I-Hsiung. 2011. Classic Complex Analysis. World ScientiÔ¨Åc.
L√ºscher, M. 1994. A portable high-quality random number generator for lattice
Ô¨Åeld theory simulations. Comp. Phys. Comm., 79, 100.
Matzner, Richard A. and Shepley, Lawrence C. 1991. Classical Mechanics. Prentice
Hall.
McCauley, Joseph L. 1994. Chaos, Dynamics, and Fractals. Cambridge University
Press.
Metropolis, Nicholas, Rosenbluth, Arianna W., Rosenbluth, Marshall N., Teller,
Augusta H., and Teller, Edward. 1953. Equation of state calculations by fast
computing machines. J. Chem. Phys., 21(6), 1087‚Äì1092.
Milonni, Peter W. and Shih, M.-L. 1992. Source theory of the Casimir force. Phys.
Rev. A, 45(7), 4241‚Äì4253.
Misner, Charles W., Thorne, Kip S., and Wheeler, John Archibald. 1973. Gravita-
tion. W. H. Freeman.
Morse, Philip M. and Feshbach, Herman. 1953. Methods of Theoretical Physics.
Vol. I. McGraw-Hill.
Parsegian, Adrian. 1969. Energy of an ion crossing a low dielectric membrane:
solutions to four relevant electrostatic problems. Nature, 221, 844‚Äì846.
Pathria, R. K. 1972. Statistical Mechanics. Pergamon Press. Chapter 13.
Pearson, Karl. 1900. On the criterion that a given system of deviations from the
probable in the case of correlated system of variables is such that it can be
reasonably supposed to have arisen from random sampling. Phil. Mag., 50(5),
157‚Äì175.
Riley, Ken, Hobson, Mike, and Bence, Stephen. 2006. Mathematical Methods for
Physics and Engineering. 3rd edn. Cambridge University Press.
Roe, Byron P. 2001. Probability and Statistics in Experimental Physics. Springer.
Saito,
Mutsuo,
and
Matsumoto,
Makoto.
2007.
www.math.sci.hiroshima-
u.ac.jp/m-mat/MT/emt.html.
653

REFERENCES
Sakurai, J. J. 1982. Advanced Quantum Mechanics. 1st edn. Addison Wesley. Pages
62‚Äì63.
Scherk, Jo√´l, and Schwarz, John H. 1974. Dual models for non-hadrons. Nucl.
Phys., B81, 118.
Schmitt, Lothar M. 2001. Theory of genetic algorithms. Theoretical Computer
Science, 259, 1‚Äì61.
Schutz, Bernard. 1980. Geometrical Methods of Mathematical Physics. Cambridge
University Press.
Schwinger, Julian, Deraad, Lester, Milton, Kimball A., and Tsai, Wu-yang. 1998.
Classical Electrodynamics. Westview Press.
Smirnov, N. V. 1939. Estimation of the deviation between empirical distribution
curves for two independent random samples. Bull. Moscow State Univ., 2(2),
3‚Äì14.
Srednicki, Mark. 2007. Quantum Field Theory. Cambridge University Press.
Stakgold, Ivar. 1967. Boundary Value Problems of Mathematical Physics, Vol. I.
Macmillan.
Steinberg, A. M., Kwiat, P. G., and Chiao, R. Y. 1993. Measurement of the single-
photon tunneling time. Phys. Rev. Lett., 71(5), 708‚Äì711.
Stenner, Michael D., Gauthier, Daniel J., and Neifeld, Mark A. 2003. The speed of
information in a ‚Äòfast-light‚Äô optical medium. Nature, 425, 695‚Äì698.
Titulaer, U. M., and Glauber, R. J. 1965. Correlation functions for coherent Ô¨Åelds.
Phys. Rev., 140(3B), B676‚Äì682.
Veneziano, Gabriel. 1968. Construction of a crossing-symmetric Regge-behaved
amplitude for linearly rising regge trajectories. Nuovo Cim., 57A, 190.
von Foerster, Heinz, Mora, Patricia M., and Amiot, Lawrence W. 1960. Doomsday:
Friday, 13 November, A.D. 2026. Science, 132, 1291‚Äì1295.
Vose, Michael D. 1999. The Simple Genetic Algorithm: Foundations and Theory.
MIT Press.
Wang, Yun-ping and Zhang, Dian-lin. 1995. Reshaping, path uncertainty, and
superluminal traveling. Phys. Rev. A, 52(4), 2597‚Äì2600.
Watson, George Neville. 1995. A Treatise on the Theory of Bessel Functions.
Cambridge University Press.
Waxman, David and Peck, Joel R. 1998. Pleiotropy and the preservation of
perfection. Science, 279.
Weinberg, Steven. 1972. Gravitation and Cosmology. John Wiley & Sons.
Weinberg, Steven. 1988. The First Three Minutes. Basic Books.
Weinberg, Steven. 1995. The Quantum Theory of Fields. Vol. I: Foundations.
Cambridge University Press.
Weinberg, Steven. 1996. The Quantum Theory of Fields. Vol. II: Modern applica-
tions. Cambridge University Press.
Weinberg, Steven. 2005. The Quantum Theory of Fields. Vol. III: Supersymmetry.
Cambridge University Press.
Weinberg, Steven. 2010. Cosmology. Oxford University Press.
Whittaker, E. T. and Watson, G. N. 1927. A Course of Modern Analysis. 4th edn.
Cambridge University Press.
654

REFERENCES
Wright, Ned. 2006. A cosmology calculator for the World Wide Web. Publ. Astron.
Soc. PaciÔ¨Åc, 118(850), 1711‚Äì1715. www.astro.ucla.edu/wright/CosmoCalc.html.
Zee, Anthony. 2010. Quantum Field Theory in a Nutshell. 2nd edn. Princeton
University Press.
Zwiebach, Barton. 2009. A First Course in String Theory. 2nd edn. Cambridge
University Press.
655

Index
adjoint domain, 262
analytic continuation, 179‚Äì180
dimensional regularization, 180
analytic functions, 160‚Äì222
branch of, 194
deÔ¨Ånition of, 160
entire, 161, 177
essential singularity, 177
harmonic functions, 170‚Äì171
holomorphic, 177
isolated singularity, 177
meromorphic, 177
multivalued, 194
pole, 177
simple pole, 177
angular momentum
lowering operators, 371
raising operators, 371
spin, 371
annihilation and creation operators, 132, 233
arrays, 2‚Äì3
associated Legendre functions, 317‚Äì323
Rodrigues‚Äôs formula for, 318
associated Legendre polynomials, 317‚Äì323
Rodrigues‚Äôs formula for, 318
asymptotic freedom, 237, 629‚Äì634
average value, 117
basis, 7, 16
beats, 77
Bessel functions, 325‚Äì347
and charge near a membrane, 331‚Äì333
and coaxial wave-guides, 342‚Äì343
and cylindrical wave-guides, 333‚Äì335
and scattering off a hard sphere, 344‚Äì345
exercises, 345‚Äì347
Hankel functions, 341‚Äì343
modiÔ¨Åed, 330‚Äì331, 341‚Äì343
Neumann functions, 341‚Äì343
of the Ô¨Årst kind, 143, 144, 325‚Äì341
of the second kind, 341‚Äì345
spherical, 145
and partial waves, 338‚Äì340
and quantum dots, 340‚Äì341
Rayleigh‚Äôs formula for, 336
spherical Bessel functions of the second kind,
343‚Äì345
Bessel inequality, 284
Bessel‚Äôs equation, 327
Bianchi identity, 413
binomial coefÔ¨Åcient, 141
Bloch‚Äôs theorem, 105
bodies falling in air, 247
Boltzmann distribution, 54‚Äì55
Boltzmann‚Äôs constant, 149
boundary conditions
Dirichlet, 262
natural, 260, 268
Neumann, 260
Bravais lattice, 104
Bromwich integral, 129
calculus of variations, 443‚Äì447
in nonrelativistic mechanics, 443‚Äì444
in relativistic electrodynamics, 445
in relativistic mechanics, 444‚Äì445
particle in a gravitational Ô¨Åeld, 445‚Äì447
strings, 643‚Äì645
Callan‚ÄìSymanzik equation, 630‚Äì634
canonical commutation relations, 132, 233
Cartan subalgebras, 379
Casimir effect, 214‚Äì217
Cauchy‚Äôs principal value, 199
Feynman‚Äôs propagator, 201‚Äì205
656

INDEX
trick, 200
chaos, 635‚Äì642
attractors, 639‚Äì642
limit cycle, 639
of fractal dimension, 642
strange, 642
Bernoulli shift, 638
chaotic threshold, 635
DufÔ¨Ång‚Äôs equation, 635
dynamical system, 636
autonomous, 636
fractals, 639‚Äì642
Cantor set, 640
fractal dimension, 640
Koch snowÔ¨Çake, 640
self-similar dimension, 641
H√©non‚Äôs map, 638
invertible map, 636, 637
map, 636
period-two sequence, 635
Poincar√© surface of section, 636
Rayleigh‚ÄìBenard convection, 635
van der Pol‚Äôs equation, 639
characteristic function, 119
and moments, 119
class Ck of functions, 85
Clifford algebra, 393
commutators, 353
compact, 351
complex arithmetic, 2
complex-variable theory, 160‚Äì222
Abel‚ÄìPlana formula, 212‚Äì217
analytic continuation, 179‚Äì180
analyticity, 160‚Äì161
and string theory, 217‚Äì219
applications to string theory
radial order, 217
argument principle, 178‚Äì179
calculus of residues, 180‚Äì182
Cauchy‚Äôs inequality, 173
Cauchy‚Äôs integral formula, 165‚Äì169
Cauchy‚Äôs integral theorem, 161‚Äì165
and Stoke‚Äôs theorem, 169
Cauchy‚Äôs principal value, 198‚Äì205
Cauchy‚ÄìRiemann conditions, 169‚Äì170
conformal mapping, 197‚Äì198
contour integral with cut, 196
cuts, 193‚Äì197
dispersion relations, 205‚Äì208
essential singularity, 177
and Picard‚Äôs theorem, 177
exercises, 219‚Äì222
fundamental theorem of algebra, 174
ghost contours, 182‚Äì191
harmonic functions, 170‚Äì171
isolated singularity, 177
Laurent series, 174‚Äì179
Liouville‚Äôs theorem, 173‚Äì174
logarithms, 193‚Äì197
method of steepest descent, 210‚Äì212
phase and group velocities, 208‚Äì210
pole, 177
residue, 176
roots, 194
simple pole, 177
singularities, 177‚Äì179
Taylor series, 171‚Äì173
conformal mapping, 197‚Äì198
contractions, 416
contravariant vector Ô¨Åeld, 401
contravariant vectors, 401
convergence
of functions, 85
uniform, 85
uniform
and term-by-term integration, 85
convergence in the mean, 138‚Äì139
convex function, 560
convolutions, 121‚Äì124, 134
and Gauss‚Äôs law, 121‚Äì123
and Green‚Äôs functions, 121‚Äì123
and translational invariance, 123
coordinates, 400‚Äì401
correlation functions
Glauber and Titulaer, 69‚Äì71
cosmology, 289‚Äì291, 457‚Äì469
, 462
time evolution of, 461‚Äì463
comoving coordinates, 458
cosmic microwave background radiation
(CMBR), 458, 469
critical energy density, 458, 462
dark matter, 457
era of dark energy, 458
era of matter, 458
era of radiation, 457
Ô¨Årst three minutes, 457
homogeneous and isotropic
line element, 458
Hubble constant, 458
Hubble rate, 461
inÔ¨Çation, 457
models, 463‚Äì469
dark-energy dominated, 468
equation of state, 464
inÔ¨Çation dominated, 465
matter dominated, 466‚Äì468
radiation dominated, 465‚Äì466
transparency, 468
without acceleration, 464‚Äì465
recombination, 468
redshift, 463
Robertson‚ÄìWalker metric, 459‚Äì463
energy‚Äìmomentum tensor of, 461
Friedmann equations, 461
transparency, 458
covariant derivatives
in Yang‚ÄìMills theory, 365
covariant vector Ô¨Åeld, 402
covariant vectors, 402
657

INDEX
decuplet of baryon resonances, 646‚Äì647
degenerate eigenvalue, 40
delta function, 97‚Äì101, 112‚Äì115, 120, 130, 131,
281‚Äì283, 287‚Äì288
and Green‚Äôs functions, 122
Dirac comb, 98‚Äì101, 114‚Äì115
eigenfunction expansion of, 281‚Äì283, 287‚Äì288
for continuous square-integrable functions,
112‚Äì115
for real periodic functions, 101, 103
for twice differentiable functions on [‚àí1, 1], 310
of a function, 113
density operators, 54‚Äì55
determinants, 27‚Äì33
and antisymmetry, 28, 29
and Levi‚ÄìCivita symbol, 28
and linear dependence, 29
and linear independence, 29
and permutations, 30
and the inverse of a matrix, 31
cofactors, 28
invariances of, 28
Laplace expansion, 28
minors, 28
product rule for, 32
3 √ó 3, 27
2 √ó 2, 27
dielectrics, 154
differential equations, 223‚Äì295
exercises, 293‚Äì295
terminal velocity
of mice, men, falcons, and bullets, 247
diffusion, 133‚Äì134
Fick‚Äôs law, 133
dimensional regularization, 180
Dirac mass term, 395
Dirac notation, 19‚Äì27, 96‚Äì101
and change of basis, 22
and inner-product rules, 20
and self-adjoint linear operators, 23
and the adjoint of an operator, 22‚Äì23
bra, 19
bracket, 19
examples, 22
ket, 19
outer products, 21
tricks, 51
Dirac‚Äôs delta function, 285
Dirac‚Äôs gamma matrices, 393
direct product, 377
adding spins, 68
and hydrogen atom, 68
dispersion relations, 205‚Äì208
and causality, 205
Kramers and Kronig, 206‚Äì208
divergence, 228‚Äì230
division algebra, 380, 384
double-factorials, 143
eigenfunctions, 267‚Äì283
complete, 274, 277‚Äì283
orthonormal
Gram‚ÄìSchmidt procedure, 274
eigenvalues, 267‚Äì283
algebraic multiplicity, 40
degenerate, 39
geometric multiplicity, 40
nondegenerate, 39
simple, 40
unbounded, 275‚Äì283
eigenvectors, 36‚Äì55
Einstein‚Äôs summation convention, 404‚Äì405
electric constant, 154, 413
electric displacement, 413
electric susceptibility, 154
electrodynamics, 411‚Äì414
electrostatic energy, 154
electrostatic potential
multipole expansion, 286
electrostatics
dielectrics, 153‚Äì157
emission rate from a Ô¨Çuorophore, 247
energy‚Äìmomentum 4-vector, 410
entropy, 54‚Äì55
euclidean coordinates, 402‚Äì404
euclidean space, 402‚Äì404
Ewald summation, 115
expected value, 117
exterior derivative, 417‚Äì419
factorials, 141‚Äì145
double, 143, 145
Mermin‚Äôs approximation, 141
Mermin‚Äôs inÔ¨Ånite-product formula, 141
Ramanujan‚Äôs approximation, 141
Stirling‚Äôs approximation, 141
Faraday‚Äôs law, 154, 412
Feynman‚Äôs propagator, 120‚Äì121, 201‚Äì205
as a Green‚Äôs function, 201
Ô¨Åeld of charge near a membrane, 155‚Äì157
Ô¨Åeld of charge near dielectric interface, 154‚Äì157
forms, 416‚Äì419, 427‚Äì431, 479‚Äì501
closed, 496‚Äì498
differential forms, 416‚Äì419, 481‚Äì501
p-forms, 417
1-forms, 416
2-forms, 416
and exterior derivative, 417
and gauge theory, 471
and Stokes‚Äôs theorem, 418
closed, 418
complex, 498
curl, 436
exact, 418
Hodge star, 439‚Äì440, 442
invariance of, 416
wedge product, 416
exact, 496‚Äì498
exercises, 500‚Äì501
exterior derivatives, 486‚Äì491
658

INDEX
exterior forms, 479‚Äì481
Frobenius‚Äôs theorem, 498‚Äì500
integration, 491‚Äì496
Stokes‚Äôs theorem, 495
Fourier series, 75‚Äì107
and scalar Ô¨Åelds, 132
better convergence of integrated series, 88
complex, 75‚Äì79, 83‚Äì89, 96‚Äì107
for real functions, 76
of nonperiodic functions, 78‚Äì89
convergence, 84‚Äì89
convergence theorem, 85
exercises, 105‚Äì107
Gibbs overshoot, 81‚Äì82
nonrelativistic strings, 103
of a C1 function, 87
of nonperiodic functions, 78‚Äì89
Parseval‚Äôs identity, 100
periodic boundary conditions, 103‚Äì105
Born‚Äìvon Karman, 105
poorer convergence of differentiated series, 89
quantum-mechanical examples, 89‚Äì96
real functions, 79‚Äì82
Gibbs overshoot, 81
several variables, 84
stretched intervals, 83‚Äì84
the interval, 77
where to put the 2œÄs, 77
Fourier transforms, 108‚Äì125, 129‚Äì134
and Amp√®re‚Äôs law, 123
and characteristic functions, 119
and convolutions, 121‚Äì124
and differential equations, 129‚Äì134
and diffusion equation, 133‚Äì134
and Fourier series, 110, 120
and Green‚Äôs functions, 121‚Äì125
and momentum space, 116‚Äì119
and Parseval‚Äôs relation, 113
and scalar wave equation, 131
and the delta function, 112‚Äì115
and the Feynman propagator, 120‚Äì121
and the uncertainty principle, 117‚Äì119
derivatives of, 115‚Äì119
exercises, 134‚Äì135
in several dimensions, 119‚Äì121
integrals of, 115‚Äì119
inverse of, 109
of a gaussian, 110‚Äì111, 183
of real functions, 111‚Äì112
Fourier‚ÄìLegendre expansion, 310
Fourier‚ÄìMellin integral, 129
function
f (x ¬± 0), 85
continuous, 85
piecewise continuous, 85
functional derivatives, 578‚Äì585
and delta functions, 579‚Äì580
and variational methods, 579‚Äì582
exercises, 585
functional differential equation for ground
state of a Ô¨Åeld theory, 583‚Äì585
functional differential equations, 583‚Äì585
notation used in physics, 579‚Äì580
of higher order, 581‚Äì582
Taylor series of, 582‚Äì583
functionals, 578
functions
analytic, 160‚Äì161
differentiable, 160
fundamental theorem of algebra, 174
gamma function, 141‚Äì145, 179‚Äì180
Gauss‚Äôs law, 154, 284, 412
gaussian integrals, 586‚Äì588
Gell-Mann‚Äôs SU(3) matrices, 377
general relativity, 289‚Äì291, 445‚Äì469
black holes, 456‚Äì457
cosmological constant, 454
cosmology, 457‚Äì469
dark energy, 454
Einstein‚Äôs equations, 453‚Äì456
Einstein‚ÄìHilbert action, 454‚Äì455
model cosmologies, 463‚Äì469
dark-energy dominated, 468
equation of state, 464
inÔ¨Çation dominated, 465
matter dominated, 466‚Äì468
radiation dominated, 465‚Äì466
transparency, 468
without acceleration, 464‚Äì465
Schwarzschild‚Äôs solution, 456‚Äì457
static and isotropic gravitational Ô¨Åeld, 455‚Äì457
Schwarzschild‚Äôs solution, 456‚Äì457
standard form, 455‚Äì456
geometric series, 139‚Äì140
gradient, 228‚Äì230
grand uniÔ¨Åcation, 377
Grassmann numbers, 2, 6‚Äì7
Grassmann polynomials, 2
Grassmann variables, 613‚Äì619
Green‚Äôs function
for Helmholtz‚Äôs equation, 286
for Helmholtz‚Äôs modiÔ¨Åed equation, 286
for Laplacian, 123
for Poisson‚Äôs equation
and Legendre functions, 287
Green‚Äôs functions, 284‚Äì289
and eigenfunctions, 287‚Äì289
Feynman‚Äôs propagator, 287
for Helmholtz‚Äôs equation, 285‚Äì286
for Poisson‚Äôs equation, 284‚Äì287
of a self-adjoint operator, 288
Poisson‚Äôs equation, 285
group index of refraction, 209
groups, 348‚Äì399
O(n), 349
SO(3)
adjoint representation of, 366‚Äì367
SO(n), 349
659

INDEX
SU(2), 368‚Äì376
deÔ¨Åning representation of, 371‚Äì374
spin and statistics, 370
tensor products of representations, 369
SU(3), 377‚Äì379
SU(3) structure constants, 378
Z2, 358
Z3, 358
Zn, 358
abelian, 349
and symmetries in quantum mechanics, 351
and Yang‚ÄìMills gauge theory, 365
automorphism, 355
inner, 355
outer, 355
block-diagonal representations of, 351
centers of, 353
characters, 356‚Äì357
compact, 350‚Äì351
compact Lie groups
real structure constants of, 364
totally antisymmetric structure constants of,
364
completely reducible representations of, 351
conjugacy classes of, 353
continuous, 348‚Äì350
deÔ¨Ånition, 348
direct sum of representations of, 351
equivalent representations of, 351
exercises, 396‚Äì399
factor groups of by subgroups, 354
Ô¨Ånite, 350, 358‚Äì361
multiplication table, 358
regular representation of, 359
further reading, 396
Gell-Mann matrices, 377‚Äì379
generators of adjoint representations of,
374‚Äì375
invariant integration, 384‚Äì385
irreducible representations of, 351
isomorphism, 354
Lie algebras, 361‚Äì399
SU(2), 368‚Äì374
Lie groups, 348‚Äì350, 361‚Äì399
SO(3), 366‚Äì367
SU(3), 377‚Äì379
SU(3) structure constants, 378
adjoint representations of, 374‚Äì375
antisymmetry of structure constants, 363
Cartan subalgebra of SU(3), 378
Cartan subalgebras of, 379
Cartan‚Äôs list of compact simple Lie groups,
383‚Äì384
Casimir operators of, 369, 375
compact, 361‚Äì385
deÔ¨Åning representation of SU(2), 371‚Äì374
deÔ¨Ånition of, 361
exponential parametrization, 362
generators of, 362
generators of adjoint representation, 374
hermitian generators of, 363
Jacobi identity, 374
noncompact, 361‚Äì364
nonhermitian generators of, 363
of rotations, 366‚Äì374
simple and semisimple, 376‚Äì377
structure constants of, 363, 374‚Äì375
SU(2) tensor product, 372
SU(3) generators, 377
symplectic group Sp(2n,R), 382
symplectic groups, 381‚Äì383
Lie groups have structure constants that are
independent of the representation, 365
Lorentz, 349
Lorentz group, 386‚Äì396
Dirac representation of, 393‚Äì395
Lie algebra of, 386‚Äì389
two-dimensional representations of, 389‚Äì393
matrix, 349
morphism, 354
noncompact, 350
nonabelian, 349
of matrices, 349‚Äì350
of orthogonal matrices, 349‚Äì350
of permutations, 360‚Äì361
of rotations, 366‚Äì374
adjoint representation of, 366‚Äì367
explicit 3 √ó 3 representation of, 367
generators of, 366‚Äì367
spin and statistics, 370
tensor operators of, 376
of transformations, 348‚Äì350
Lorentz, 348
Poincar√©, 348
rotations and reÔ¨Çections, 348
translations, 348
of unitary matrices, 349‚Äì350
order of, 349, 358
Poincar√©, 349
Poincar√© group, 395‚Äì396
Lie algebra of, 395‚Äì396
reducible representations of, 351
representations of, 350‚Äì352
dimensions of, 350
in Hilbert space, 351‚Äì352
rotations
representations of, 352
Schur‚Äôs lemma, 355‚Äì356
semisimple, 353
similarity transformations, 351
simple, 353
simple and semisimple Lie algebras, 376‚Äì377
subgroups, 353‚Äì355
cosets of, 354
invariant, 353
left cosets of, 354
normal, 353
quotient coset space, 354
right cosets of, 354
trivial, 353
660

INDEX
symmetry
antilinear, antiunitary representations of, 352
linear, unitary representations of, 352
tensor product
addition of angular momenta, 357
tensor products, 357‚Äì358
translation, 348
unitary representations of, 351
harmonic function, 170
harmonic oscillator, 101‚Äì102, 272‚Äì273
Heaviside step function, 106
helicity
positive, 393
right-handed, 393
Helmholtz‚Äôs equation
in cylindrical coordinates
and Bessel functions, 328‚Äì335
in spherical coordinates
and spherical Bessel functions, 335‚Äì341
in three dimensions, 231‚Äì232
in two dimensions, 230‚Äì231
rectangular coordinates, 231
spherical coordinates, 232
and associated Legendre functions, 317
and spherical Bessel functions, 317
with azimuthal symmetry, 315‚Äì316
and Legendre polynomials, 315‚Äì316
and spherical Bessel functions, 315‚Äì316
Hermite functions, 264
Hermite‚Äôs system, 264
hermitian differential operators, 261
Hilbert spaces, 13‚Äì14, 25‚Äì26
homogeneous functions, 243‚Äì245
Euler‚Äôs theorem, 243
virial theorem, 243‚Äì244
index of refraction, 207
inÔ¨Ånite products, 157‚Äì158
inÔ¨Ånite series, 136‚Äì159
absolute convergence, 136
asymptotic, 152‚Äì153
WKB & Dyson, 153
Bernoulli numbers and polynomials, 151‚Äì152
binomial series, 148
binomial theorem, 147, 148
Cauchy‚Äôs criterion, 137
Cauchy‚Äôs root test, 137
comparison test, 137
conditional convergence, 136
convergence, 136‚Äì140
d‚ÄôAlembert‚Äôs ratio test, 138
divergence of, 136
exercises, 158‚Äì159
Intel test, 138
logarithmic series, 148‚Äì149
of functions
convergence, 138‚Äì139
convergence in the mean, 138‚Äì139
Dirichlet series, 149‚Äì151
Fourier series, 146
geometric series, 139‚Äì140
power series, 139‚Äì140, 146
Taylor series, 145‚Äì149
uniform convergence, 139
Riemann zeta function, 149
uniform convergence, 138
and term-by-term integration, 139
inner products, 3, 11‚Äì14
and distance, 12
and norm, 12
degenerate, 12
hermitian, 12
indeÔ¨Ånite, 12, 14
Minkowski, 14
nondegenerate, 12
of functions, 13
positive deÔ¨Ånite, 11
Schwarz, 11
inner-product spaces, 13‚Äì14
integral equations, 296‚Äì304
exercises, 304
Fredholm, 297‚Äì301
eigenfunctions, 297‚Äì301
eigenvalues, 297‚Äì301
Ô¨Årst kind, 297
homogeneous, 297
inhomogeneous, 297
second kind, 297
implications of linearity, 298‚Äì304
integral transformations, 301‚Äì304
and Bessel functions, 302‚Äì304
Fourier, Laplace, and Euler kernels, 302
kernel, 297
numerical solutions, 299‚Äì301
Volterra, 297‚Äì301
eigenfunctions, 297‚Äì301
eigenvalues, 297‚Äì301
Ô¨Årst kind, 297
homogeneous, 297
inhomogeneous, 297
second kind, 297
integral transformations, 301‚Äì304
and Bessel functions, 302‚Äì304
Fourier, Laplace, and Euler kernels, 302
invariant distance, 409
invariant subspace, 37
Jacobi identity, 374
kernel of a matrix, 355
Kramers‚ÄìKronig relations, 206‚Äì208
Kronecker delta, 5, 415
Lagrange multipliers, 35‚Äì36, 54‚Äì55, 267‚Äì283
Lapack, 33, 66
Laplace transforms, 125‚Äì134
and convolutions, 134
and differential equations, 128‚Äì134
derivatives of, 127‚Äì128
661

INDEX
examples, 125
integrals of, 127
inversion of, 129
Laplace‚Äôs equation
in two dimensions, 316‚Äì317
Laplacian, 228‚Äì230
Legendre functions, 263‚Äì264, 287, 305‚Äì324
exercises, 323‚Äì324
second kind, 266
Legendre polynomials, 305‚Äì313
addition theorem for, 321
generating function, 307‚Äì309
Helmholtz‚Äôs equation
with azimuthal symmetry, 315‚Äì316
Legendre‚Äôs differential equation, 309‚Äì311
normalization, 305
recurrence relations, 311‚Äì312
Rodrigues‚Äôs formula, 306‚Äì307
SchlaeÔ¨Çi‚Äôs integral for, 312‚Äì313
special values of, 312
Legendre‚Äôs system, 263‚Äì264
Leibniz‚Äôs rule, 141
Lerch transcendent, 151
Levi-Civita symbol, 366
Lie algebras
ranks of, 379
roots of, 379
weight vector, 379
weights of, 379
light
slow, fast, and backwards, 209‚Äì210
linear algebra, 1‚Äì74
exercises, 71‚Äì74
linear dependence, 15‚Äì16, 224
and determinants, 29
linear independence, 15‚Äì16, 224
and completeness, 15
and determinants, 29
linear least squares, 34‚Äì35
linear operators, 9‚Äì11
and matrices, 9
density operator, 69
domain, 9
hermitian, 23
range, 9
real, symmetric, 23‚Äì24
self-adjoint, 23
unitary, 24‚Äì25
Lorentz force, 414
Lorentz transformations, 405‚Äì411
boost, 406
invariance under, 406
magnetic constant, 413
magnetic Ô¨Åeld, 413
magnetic induction, 411
Majorana Ô¨Åeld, 235
Majorana mass term, 391, 393
Maple, 33, 66
Mathematica, 33, 66
Matlab, 33, 61, 66
matrices, 4‚Äì7
adjoint, 4
and linear operators, 9
change of basis, 10
characteristic equation of, 38, 41‚Äì42
CKM, 63
and CP violation, 63
congruency transformation, 49
defective, 40
density operator, 69
diagonal form of square nondefective matrix,
41
functions of, 43‚Äì45
gamma, 393
hermitian, 5, 45‚Äì49
and diagonalization by a unitary
transformation, 48
complete and orthonormal eigenvectors, 47
degenerate eigenvalues, 46
eigenvalues of, 45
eigenvectors and eigenvalues of, 48
eigenvectors of, 46
identity, 5
imaginary and antisymmetric, 48
inverse, 5
inverses of, 31
nonnegative, 6
nonsingular, 40
normal, 50‚Äì55
compatible, 52‚Äì55
diagonalization by a unitary transformation,
50
orthogonal, 5, 25
Pauli, 5, 68, 371
positive, 6
positive deÔ¨Ånite, 6
rank of, 65
example, 65
rank-nullity theorem, 58
real and symmetric, 48
similarity transformation, 10, 41
singular-value decomposition, 55‚Äì63
example, 62
quark mass matrix, 62
square, 38‚Äì42
eigenvalues of, 38‚Äì42
eigenvectors of, 38‚Äì42
trace, 4
cyclic, 4
unitary, 5
upper triangular, 33
Maxwell‚Äôs equations, 412
in vacuum, 413
Maxwell‚ÄìAmp√®re law, 412
mean value, 117
method of steepest descent, 210‚Äì212
metric spaces, 13‚Äì14
Minkowski space, 405‚Äì407
Monte Carlo methods, 563‚Äì577
662

INDEX
and evolution, 576‚Äì577
and lattice gauge theory, 574, 577
detailed balance, 574
exercises, 577
genetic algorithms, 577
in statistical mechanics, 572‚Äì575
Metropolis step, 572
Metropolis‚Äôs algorithm, 572‚Äì575
more general applications, 575‚Äì577
of data analysis, 566‚Äì572
example, 566‚Äì572
of numerical integration, 563‚Äì566
partition function, 574
smart schemes, 575
sweeps, 573
thermalization, 573
Moore‚ÄìPenrose pseudoinverse, 63‚Äì65
natural units, 121
nearest-integer function, 108
nonlinear differential equations, 289‚Äì293
general relativity, 289‚Äì291
solitons, 291‚Äì293
notation for derivatives, 226‚Äì228
null space of a matrix, 355
numbers
complex, 1, 7
atan2, 2
phase of, 2
irrational, 1
natural, 1
rational, 1
real, 1
Octave, 33, 66
octet of baryons, 379
octet of pseudo-scalar mesons, 378
octonians, 384
open, 351
operators
adjoint of, 22‚Äì23
antilinear, 26‚Äì27
antiunitary, 26‚Äì27
compatible, 352
complete, 352
orthogonal, 25
real, symmetric, 23‚Äì24
unitary, 24‚Äì25
optical theorem, 207
ordinary differential equations, 223‚Äì225
and variational problems, 259‚Äì260
boundary conditions, 258‚Äì260
differential operators of deÔ¨Ånite parity, 255
even and odd differential operators, 254‚Äì255
exact, 238‚Äì242
Boyle‚Äôs law, 239
condition of integrability, 239
Einstein‚Äôs law, 239
human population growth, 239
integrating factors, 242
integration, 240‚Äì242
van der Waals‚Äôs equation, 239
Ô¨Årst-order, 235‚Äì248
exact, 238‚Äì242
separable, 235‚Äì238
self-adjoint, 266‚Äì267
Frobenius‚Äôs series solutions, 251‚Äì254
Fuch‚Äôs theorem, 253‚Äì254
indicial equation, 252
recurrence relations, 252
Green‚Äôs formula, 260
hermitian operators, 267
homogeneous Ô¨Årst-order, 245
homogeneous functions, 243‚Äì245
Lagrange‚Äôs identity, 260
linear, 223‚Äì225
general solution, 224, 225
homogeneous, 224
inhomogeneous, 224, 225
order of, 223
linear dependence of solutions, 224
linear independence of solutions, 224
linear, Ô¨Årst-order, 246‚Äì248
exact, 246
integrating factor, 246
meaning of exactness, 240‚Äì242
nonlinear, 225
second-order
eigenfunctions, 273‚Äì275
eigenvalues, 273‚Äì275
essential singularity of, 251
Green‚Äôs functions, 288
irregular singular point of, 251
making operators self adjoint, 264‚Äì265
nonessential singular point of, 251
regular singular point of, 251
second solution, 255‚Äì257
self-adjoint, 260‚Äì283
self-adjoint form, 223
singular points at inÔ¨Ånity, 251
singular points of, 250‚Äì251
weight function, 273
why not three solutions?, 257‚Äì258
wronskians of self-adjoint operators,
265‚Äì266
self-adjoint, 260‚Äì283
self-adjoint operators, 265
separable, 235‚Äì238
general integral, 236
hidden separability, 238
logistic equation, 236
Zipf‚Äôs law, 236
separated, 235
singular points of Legendre‚Äôs equation, 251
Sturm‚ÄìLiouville problem, 265, 267‚Äì283
systems of, 248‚Äì250, 289‚Äì293
Friedmann‚Äôs equations, 289‚Äì291
Lagrange‚Äôs equations, 248‚Äì250
Wronski‚Äôs determinant, 255‚Äì258
orthogonal coordinates, 228‚Äì230
663

INDEX
divergence in, 228‚Äì230
gradient in, 228‚Äì230
Laplacian in, 228‚Äì230
orthogonal polynomials, 313‚Äì315
Hermite‚Äôs, 314
Jacobi‚Äôs, 313‚Äì314
Laguerre‚Äôs, 314‚Äì315
outer products, 18‚Äì22
example, 18
in Dirac‚Äôs notation, 21
partial differential equations, 225‚Äì235
Dirac equation, 234‚Äì235
general solution, 226
homogeneous, 226
inhomogeneous, 226
Klein‚ÄìGordon equation, 233
linear, 225‚Äì235
separable, 230‚Äì235
Helmholtz‚Äôs equation, 230‚Äì232
wave equations, 233‚Äì235
photon, 233‚Äì234
spin-one-half Ô¨Åelds, 234‚Äì235
spinless bosons, 233
path integrals, 586‚Äì625
and gaussian integrals, 586‚Äì588
and lattice gauge theories, 622
and nonabelian gauge theories, 619‚Äì624
ghosts, 622‚Äì624
the method of Faddeev and Popov, 620‚Äì624
and perturbative Ô¨Åeld theory, 605‚Äì624
and quantum electrodynamics, 609‚Äì613
and Schr√∂dinger‚Äôs equation, 592‚Äì593
and the Bohm‚ÄìAharonov effect, 594‚Äì595
and the principle of stationary action, 591‚Äì592
euclidean, 588‚Äì590
euclidean correlation functions, 599‚Äì600
exercises, 624‚Äì625
fermionic, 613‚Äì619
Ô¨Ånite temperature, 588‚Äì590
for a free particle in imaginary time, 595
for a free particle in real time, 593‚Äì595
for harmonic oscillator in imaginary time,
597‚Äì598
for harmonic oscillator in real time, 595‚Äì597
in Ô¨Åeld theory, 603‚Äì624
in Ô¨Ånite-temperature Ô¨Åeld theory, 600‚Äì603
in imaginary time, 588‚Äì590
in real time, 590‚Äì593
Minkowski, 590‚Äì593
of Ô¨Åelds, 600‚Äì624
of Ô¨Åelds in euclidean space, 600‚Äì603
of Ô¨Åelds in imaginary time, 600‚Äì603
ratios of and time-ordered products, 604‚Äì624
Pauli matrices, 371
permittivity, 154, 413
permutations, 360‚Äì361
and determinants, 30
cycles, 360
phase and group velocities, 208‚Äì210
slow, fast, and backwards light, 209
Planck‚Äôs constant, 149
Planck‚Äôs distribution, 149‚Äì151
points, 400‚Äì401
Poisson summation, 114‚Äì115
power series, 139‚Äì140
pre-Hilbert spaces, 13‚Äì14
principle of least action, 259‚Äì260
principle of stationary action, 248‚Äì250, 267‚Äì273,
443‚Äì447
in nonrelativistic mechanics, 328, 443‚Äì444
in quantum mechanics, 267‚Äì273
in relativistic electrodynamics, 445
in relativistic mechanics, 444‚Äì445
particle in a gravitational Ô¨Åeld, 445‚Äì447
probability and statistics, 502‚Äì562
Bayes‚Äôs theorem, 502‚Äì505
Bernoulli‚Äôs distribution, 508
binomial distribution, 508‚Äì511
brownian motion, 520‚Äì527
Einstein‚ÄìNernst relation, 520‚Äì524
Langevin‚Äôs theory of, 520‚Äì527
Cauchy distributions, 532
central limit theorem, 532‚Äì543
illustrations of, 535‚Äì543
central moments, 505‚Äì508
centroid method, 514
characteristic functions, 527‚Äì530
chi-squared distribution, 531
chi-squared statistic, 551‚Äì554
convergence in probability, 535
correlation coefÔ¨Åcient, 507
covariance, 507
lower bound of Cram√©r and Rao, 546‚Äì550
cumulants, 529
diffusion, 519‚Äì527
diffusion constant, 523
direct stochastic optical reconstruction
microscopy, 515
Einstein‚ÄìNernst relation, 520‚Äì524
Einstein‚Äôs relation, 523
ensemble average, 521
error function, 515‚Äì518
estimators, 543‚Äì550
Bessel‚Äôs correction, 545
bias, 543
consistent, 543
standard deviation, 546
standard error, 546
exercises, 560‚Äì562
expectation, 505‚Äì508
expected value, 505‚Äì508
exponential distribution, 531
fat tails, 530‚Äì532
Fisher‚Äôs information matrix, 546‚Äì550
Ô¨Çuctuation and dissipation, 524‚Äì527
gaussian distribution, 512‚Äì519
Gosset‚Äôs distribution, 530
Heisenberg‚Äôs uncertainty principle, 506‚Äì507
information, 546‚Äì550
664

INDEX
information matrix, 546‚Äì550
Kolmogorov‚Äôs function, 556
Kolmogorov‚Äôs test, 554‚Äì560
kurtosis, 530
L√©vy distributions, 532
Lindeberg‚Äôs condition, 535
log-normal distribution, 531
Lorentz distributions, 532
maximum likelihood, 550‚Äì551
Maxwell‚ÄìBoltzmann distribution, 518‚Äì519
mean, 505‚Äì508
moment-generating functions, 527‚Äì530
moments, 505‚Äì508
normal distribution, 514
Pearson‚Äôs distribution, 531
Poisson distribution, 511‚Äì512
coherent states, 512
power-law tails, 530
probability density, 505‚Äì519
probability distribution, 505‚Äì519
random-number generators, 537‚Äì538
skewness, 530
Student‚Äôs t-distribution, 530
variance, 505‚Äì508
lower bound of Cram√©r and Rao, 546‚Äì550
viscous-friction coefÔ¨Åcient, 523
proper time, 409
and time dilation, 410‚Äì411
pseudoinverse, 63‚Äì65, 551
quantum mechanics, 267‚Äì283
quaternions, 379‚Äì383
and the Pauli matrices, 380
R-C circuit, 247
regular and self-adjoint differential system, 262
relative permittivity, 154
renormalization group, 237, 626‚Äì634
exercises, 634
in condensed-matter physics, 632‚Äì634
in lattice Ô¨Åeld theory, 630‚Äì632
in quantum Ô¨Åeld theory, 626‚Äì634
rotations, 366‚Äì374
scalar Ô¨Åelds, 131, 401
scalars, 401
Schwarz inequality, 14‚Äì15, 284
examples, 14
Schwarz inner products, 69‚Äì71
seesaw mechanism, 49
self-adjoint differential operators, 260‚Äì283
self-adjoint differential systems, 262‚Äì283
sequence of functions
convergence in the mean, 88
simple and semisimple Lie algebras, 376‚Äì377
simple and semisimple Lie groups, 376‚Äì377
simple eigenvalue, 40
simply connected, 164
slow, fast, and backwards light, 209
and Kramers‚ÄìKronig relations, 210
small oscillations, 250
solitons, 291‚Äì293
special relativity, 408‚Äì414
4-vector force, 410
and time dilation, 410‚Äì411
electrodynamics, 411‚Äì414
energy‚Äìmomentum 4-vector, 410
kinematics, 410‚Äì411
spherical harmonics, 319‚Äì323
spin and statistics, 370
standard model of particle physics, 377
states
coherent, 71
Stefan‚Äôs constant, 150
Stokes‚Äôs theorem, 170
strings, 643‚Äì650
and inÔ¨Ånities of quantum Ô¨Åeld theory, 643
Dirichlet boundary condition, 645
free-endpoint boundary condition, 645
Nambu‚ÄìGoto action, 643‚Äì647
quantized, 647‚Äì650
D-branes, 647‚Äì648
Regge trajectories, 646‚Äì647
Riemann surfaces and moduli, 649‚Äì650
scattering of, 648
Sturm‚ÄìLiouville equation, 265, 267‚Äì283
SU(3) and quarks, 378‚Äì379
subspace, 351
invariant, 351
proper, 351
summation convention, 404‚Äì405
symmetric differential operators, 261
symmetry
in quantum mechanics, 26
systems of linear equations, 34‚Äì35
Taylor series, 145‚Äì149
tensor products, 377
adding angular momenta, 357‚Äì358, 371‚Äì374
adding spins, 68, 371‚Äì374
and hydrogen atom, 68
tensors, 400‚Äì478
afÔ¨Åne connections, 431‚Äì433
and metric tensor, 436‚Äì437
and general relativity, 445‚Äì469
antisymmetric, 415
basic axiom of relativity, 422
basis vectors, 421
Bianchi identity, 436
Christoffel symbols, 431‚Äì433
connections, 431‚Äì433
contractions, 416
contravariant metric tensor, 422
covariant curl, 434‚Äì436
covariant derivatives, 431‚Äì434
and antisymmetry, 436
metric tensor, 437‚Äì438
covariant metric tensor, 422
curvature, 451‚Äì453
curvature of a sphere, 451‚Äì453
665

INDEX
curvature scalar, 451
cylindrical coordinates, 425
divergence of a contravariant vector, 438‚Äì443
and Hodge star, 439‚Äì440
and Laplacian, 441‚Äì443
Einstein‚Äôs equations, 453‚Äì456
exercises, 475‚Äì478
gauge theory, 469‚Äì475
geometry of, 474‚Äì475
role of vectors, 471‚Äì473
standard model, 469‚Äì473
gradient, 426‚Äì427
Hodge star, 428‚Äì431
and divergence, 428
and Laplacian, 428
and Maxwell‚Äôs equations, 430‚Äì431
Laplacian, 441‚Äì443
and Hodge star, 442
Levi-Civita‚Äôs symbol, 427‚Äì431
Levi-Civita‚Äôs tensor, 427‚Äì431
metric of sphere, 421
metric tensor, 420‚Äì427
moving frame, 421
notation for derivatives, 433
orthogonal coordinates, 423‚Äì426
parallel transport, 433
particle in a weak, static gravitational Ô¨Åeld,
448‚Äì451
gravitational redshift, 450
gravitational time dilation, 449‚Äì450
perfect Ô¨Çuid, 453
polar coordinates, 424
principle of equivalence, 447‚Äì448
geodesic equation, 448
quotient theorem, 420
raising, lowering indices, 423
Ricci tensor, 451
Riemann tensor, 451
second-rank, 414‚Äì416
spherical coordinates, 425‚Äì426
symmetric, 415
tensor equations, 419‚Äì420
torsion tensor, 433
third-harmonic microscopy, 184
time dependence of Heisenberg operators, 604
time dilation, 409‚Äì410
in muon decay, 409‚Äì410
time-ordered product, 121, 604
total cross-section, 207
uncertainty principle, 117‚Äì119, 244
variance
of an operator, 118‚Äì119
variational methods, 248‚Äì250, 259‚Äì260, 267‚Äì273,
443‚Äì447
in nonrelativistic mechanics, 328, 443‚Äì444
in quantum mechanics, 267‚Äì273
in relativistic electrodynamics, 445
in relativistic mechanics, 444‚Äì445
particle in a gravitational Ô¨Åeld, 445‚Äì447
strings, 643‚Äì645
vector space, 16
dimension of, 16
vectors, 7‚Äì8
basis, 7, 16, 17
complete, 16
components, 7
direct product, 66‚Äì68
example, 68
eigenvalues, 37‚Äì55
example, 37
eigenvectors, 37‚Äì55
example, 37
eigenvectors of square matrix, 40
functions as, 8
orthonormal, 16‚Äì17
Gram‚ÄìSchmidt method, 16‚Äì17
partial derivatives as, 8
span, 16
span a space, 16
states as, 8
tensor product, 66‚Äì68
Virasoro‚Äôs algebra, 219, 222
virial theorem, 243‚Äì244
wedge product, 416
Weyl spinor
left-handed, 390
right-handed, 393
Wigner‚ÄìEckart theorem
special case of, 355‚Äì356
wronskian, 255‚Äì258
Yang‚ÄìMills theory, 469‚Äì475
geometry of, 474‚Äì475
role of vectors, 471‚Äì473
standard model, 469‚Äì473
Yukawa potential, 125, 286
zeta function, 149‚Äì151
666


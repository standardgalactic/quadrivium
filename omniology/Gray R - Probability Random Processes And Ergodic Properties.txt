Probability,
Random Processes,
and Ergodic Properties
November 3, 2001

ii

Probability,
Random Processes,
and Ergodic Properties
Robert M. Gray
Information Systems Laboratory
Department of Electrical Engineering
Stanford University

iv
c⃝1987 by Springer Verlag, 2001 revision by Robert M. Gray.

v
This book is aﬀectionately dedicated to
Elizabeth Dubois Jordan Gray
and to the memory of
R. Adm. Augustine Heard Gray, U.S.N.
1888-1981
Sara Jean Dubois
and
William “Billy” Gray
1750-1825

vi

Preface
History and Goals
This book has been written for several reasons, not all of which are academic. This material was
for many years the ﬁrst half of a book in progress on information and ergodic theory. The intent
was and is to provide a reasonably self-contained advanced treatment of measure theory, probability
theory, and the theory of discrete time random processes with an emphasis on general alphabets
and on ergodic and stationary properties of random processes that might be neither ergodic nor
stationary. The intended audience was mathematically inclined engineering graduate students and
visiting scholars who had not had formal courses in measure theoretic probability. Much of the
material is familiar stuﬀfor mathematicians, but many of the topics and results have not previously
appeared in books.
The original project grew too large and the ﬁrst part contained much that would likely bore
mathematicians and discourage them from the second part. Hence I ﬁnally followed a suggestion
to separate the material and split the project in two.
The original justiﬁcation for the present
manuscript was the pragmatic one that it would be a shame to waste all the eﬀort thus far expended.
A more idealistic motivation was that the presentation had merit as ﬁlling a unique, albeit small,
hole in the literature. Personal experience indicates that the intended audience rarely has the time to
take a complete course in measure and probability theory in a mathematics or statistics department,
at least not before they need some of the material in their research. In addition, many of the existing
mathematical texts on the subject are hard for this audience to follow, and the emphasis is not well
matched to engineering applications. A notable exception is Ash’s excellent text [1], which was
likely inﬂuenced by his original training as an electrical engineer. Still, even that text devotes little
eﬀort to ergodic theorems, perhaps the most fundamentally important family of results for applying
probability theory to real problems. In addition, there are many other special topics that are given
little space (or none at all) in most texts on advanced probability and random processes. Examples
of topics developed in more depth here than in most existing texts are the following:
Random processes with standard alphabets We develop the theory of standard spaces as a
model of quite general process alphabets.
Although not as general (or abstract) as often
considered by probability theorists, standard spaces have useful structural properties that
simplify the proofs of some general results and yield additional results that may not hold
in the more general abstract case. Examples of results holding for standard alphabets that
have not been proved in the general abstract case are the Kolmogorov extension theorem, the
ergodic decomposition, and the existence of regular conditional probabilities. In fact, Blackwell
[6] introduced the notion of a Lusin space, a structure closely related to a standard space, in
order to avoid known examples of probability spaces where the Kolmogorov extension theorem
does not hold and regular conditional probabilities do not exist. Standard spaces include the
vii

viii
PREFACE
common models of ﬁnite alphabets (digital processes) and real alphabets as well as more general
complete separable metric spaces (Polish spaces). Thus they include many function spaces,
Euclidean vector spaces, two-dimensional image intensity rasters, etc. The basic theory of
standard Borel spaces may be found in the elegant text of Parthasarathy [55], and treatments
of standard spaces and the related Lusin and Suslin spaces may be found in Christensen [10],
Schwartz [62], Bourbaki [7], and Cohn [12].
We here provide a diﬀerent and more coding
oriented development of the basic results and attempt to separate clearly the properties of
standard spaces, which are useful and easy to manipulate, from the demonstrations that certain
spaces are standard, which are more complicated and can be skipped. Thus, unlike in the
traditional treatments, we deﬁne and study standard spaces ﬁrst from a purely probability
theory point of view and postpone the topological metric space considerations until later.
Nonstationary and nonergodic processes We develop the theory of asymptotically mean sta-
tionary processes and the ergodic decomposition in order to model many physical processes
better than can traditional stationary and ergodic processes. Both topics are virtually absent
in all books on random processes, yet they are fundamental to understanding the limiting
behavior of nonergodic and nonstationary processes. Both topics are considered in Krengel’s
excellent book on ergodic theorems [41], but the treatment here is more detailed and in greater
depth. We consider both the common two-sided processes, which are considered to have been
producing outputs forever, and the more diﬃcult one-sided processes, which better model
processes that are “turned on” at some speciﬁc time and which exhibit transient behavior.
Ergodic properties and theorems We develop the notion of time averages along with that of
probabilistic averages to emphasize their similarity and to demonstrate many of the impli-
cations of the existence of limiting sample averages. We prove the ergodic theorem theorem
for the general case of asymptotically mean stationary processes. In fact, it is shown that
asymptotic mean stationarity is both suﬃcient and necessary for the classical pointwise or
almost everywhere ergodic theorem to hold. We also prove the subadditive ergodic theorem
of Kingman [39], which is useful for studying the limiting behavior of certain measurements
on random processes that are not simple arithmetic averages. The proofs are based on re-
cent simple proofs of the ergodic theorem developed by Ornstein and Weiss [52], Katznelson
and Weiss [38], Jones [37], and Shields [64]. These proofs use coding arguments reminiscent
of information and communication theory rather than the traditional (and somewhat tricky)
maximal ergodic theorem. We consider the interrelations of stationary and ergodic proper-
ties of processes that are stationary or ergodic with respect to block shifts, that is, processes
that produce stationary or ergodic vectors rather than scalars — a topic largely developed b
Nedoma [49] which plays an important role in the general versions of Shannon channel and
source coding theorems.
Process distance measures We develop measures of a “distance” between random processes.
Such results quantify how “close” one process is to another and are useful for considering spaces
of random processes. These in turn provide the means of proving the ergodic decomposition
of certain functionals of random processes and of characterizing how close or diﬀerent the long
term behavior of distinct random processes can be expected to be.
Having described the topics treated here that are lacking in most texts, we admit to the omission
of many topics usually contained in advanced texts on random processes or second books on random
processes for engineers. The most obvious omission is that of continuous time random processes. A
variety of excuses explain this: The advent of digital systems and sampled-data systems has made
discrete time processes at least equally important as continuous time processes in modeling real

PREFACE
ix
world phenomena. The shift in emphasis from continuous time to discrete time in texts on electrical
engineering systems can be veriﬁed by simply perusing modern texts. The theory of continuous time
processes is inherently more diﬃcult than that of discrete time processes. It is harder to construct
the models precisely and much harder to demonstrate the existence of measurements on the models,
e.g., it is usually harder to prove that limiting integrals exist than limiting sums. One can approach
continuous time models via discrete time models by letting the outputs be pieces of waveforms.
Thus, in a sense, discrete time systems can be used as a building block for continuous time systems.
Another topic clearly absent is that of spectral theory and its applications to estimation and
prediction. This omission is a matter of taste and there are many books on the subject.
A further topic not given the traditional emphasis is the detailed theory of the most popular
particular examples of random processes: Gaussian and Poisson processes. The emphasis of this
book is on general properties of random processes rather than the speciﬁc properties of special cases.
The ﬁnal noticeably absent topic is martingale theory. Martingales are only brieﬂy discussed in
the treatment of conditional expectation. My excuse is again that of personal taste. In addition,
this powerful theory is simply not required in the intended sequel to this book on information and
ergodic theory.
The book’s original goal of providing the needed machinery for a book on information and
ergodic theory remains. That book will rest heavily on this book and will only quote the needed
material, freeing it to focus on the information measures and their ergodic theorems and on source
and channel coding theorems. In hindsight, this manuscript also serves an alternative purpose. I
have been approached by engineering students who have taken a master’s level course in random
processes using my book with Lee Davisson [24] and who are interested in exploring more deeply into
the underlying mathematics that is often referred to, but rarely exposed. This manuscript provides
such a sequel and ﬁlls in many details only hinted at in the lower level text.
As a ﬁnal, and perhaps less idealistic, goal, I intended in this book to provide a catalogue of
many results that I have found need of in my own research together with proofs that I could follow.
This is one goal wherein I can judge the success; I often ﬁnd myself consulting these notes to ﬁnd the
conditions for some convergence result or the reasons for some required assumption or the generality
of the existence of some limit. If the manuscript provides similar service for others, it will have
succeeded in a more global sense.
Assumed Background
The book is aimed at graduate engineers and hence does not assume even an undergraduate math-
ematical background in functional analysis or measure theory. Hence topics from these areas are
developed from scratch, although the developments and discussions often diverge from traditional
treatments in mathematics texts. Some mathematical sophistication is assumed for the frequent
manipulation of deltas and epsilons, and hence some background in elementary real analysis or a
strong calculus knowledge is required.
Acknowledgments
The research in information theory that yielded many of the results and some of the new proofs for
old results in this book was supported by the National Science Foundation. Portions of the research
and much of the early writing were supported by a fellowship from the John Simon Guggenheim
Memorial Foundation.

PREFACE
1
The book beneﬁted greatly from comments from numerous students and colleagues through many
years: most notably Paul Shields, Lee Davisson, John Kieﬀer, Dave Neuhoﬀ, Don Ornstein, Bob
Fontana, Jim Dunham, Farivar Saadat, Mari Ostendorf, Michael Sabin, Paul Algoet, Wu Chou, Phil
Chou, and Tom Lookabaugh. They should not be blamed, however, for any mistakes I have made
in implementing their suggestions.
I would also like to acknowledge my debt to Al Drake for introducing me to elementary probability
theory and to Tom Pitcher for introducing me to measure theory. Both are extraordinary teachers.
Finally, I would like to apologize to Lolly, Tim, and Lori for all the time I did not spend with
them while writing this book.
The New Millenium Edition
After a decade and a half I am ﬁnally converting the ancient troﬀto LaTex in order to post a
corrected and revised version of the book on the Web. I have received a few requests to do so
since the book went out of print, but the electronic manuscript was lost years ago during my many
migrations among computer systems and my less than thorough backup precautions. During summer
2001 a thorough search for something else in my Stanford oﬃce led to the discovery of an old data
cassette, with a promising inscription. Thanks to assistance from computer wizards Charlie Orgish
and Pat Burke, prehistoric equipment was found to read the cassette and the original troﬀﬁles for
the book were read and converted into LaTeX with some assistance from Kamal Al-Yahya’s and
Christian Engel’s tr2latex program. I am still in the progress of ﬁxing conversion errors and slowly
making long planned improvements.

2
PREFACE

Contents
Preface
vii
1
Probability and Random Processes
5
1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.2
Probability Spaces and Random Variables . . . . . . . . . . . . . . . . . . . . . . . .
5
1.3
Random Processes and Dynamical Systems
. . . . . . . . . . . . . . . . . . . . . . .
10
1.4
Distributions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
1.5
Extension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
1.6
Isomorphism
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
2
Standard alphabets
25
2.1
Extension of Probability Measures
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
2.2
Standard Spaces
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.3
Some properties of standard spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.4
Simple standard spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.5
Metric Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
2.6
Extension in Standard Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
2.7
The Kolmogorov Extension Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
2.8
Extension Without a Basis
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
3
Borel Spaces and Polish alphabets
49
3.1
Borel Spaces
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.2
Polish Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
3.3
Polish Schemes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
4
Averages
65
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
4.2
Discrete Measurements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
4.3
Quantization
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
4.4
Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
4.5
Time Averages
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
4.6
Convergence of Random Variables
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
4.7
Stationary Averages
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
3

4
CONTENTS
5
Conditional Probability and Expectation
95
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
5.2
Measurements and Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
5.3
Restrictions of Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
5.4
Elementary Conditional Probability
. . . . . . . . . . . . . . . . . . . . . . . . . . .
99
5.5
Projections
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
5.6
The Radon-Nikodym Theorem
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
5.7
Conditional Probability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108
5.8
Regular Conditional Probability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
110
5.9
Conditional Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
113
5.10 Independence and Markov Chains
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
119
6
Ergodic Properties
123
6.1
Ergodic Properties of Dynamical Systems . . . . . . . . . . . . . . . . . . . . . . . .
123
6.2
Some Implications of Ergodic Properties . . . . . . . . . . . . . . . . . . . . . . . . .
126
6.3
Asymptotically Mean Stationary Processes
. . . . . . . . . . . . . . . . . . . . . . .
131
6.4
Recurrence
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
138
6.5
Asymptotic Mean Expectations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
6.6
Limiting Sample Averages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
144
6.7
Ergodicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
146
7
Ergodic Theorems
153
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
7.2
The Pointwise Ergodic Theorem
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
7.3
Block AMS Processes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
158
7.4
The Ergodic Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
160
7.5
The Subadditive Ergodic Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . .
164
8
Process Metrics and the Ergodic Decomposition
173
8.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
173
8.2
A Metric Space of Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
174
8.3
The Rho-Bar Distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
180
8.4
Measures on Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
186
8.5
The Ergodic Decomposition Revisited
. . . . . . . . . . . . . . . . . . . . . . . . . .
187
8.6
The Ergodic Decomposition of Markov Processes . . . . . . . . . . . . . . . . . . . .
190
8.7
Barycenters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
192
8.8
Aﬃne Functions of Measures
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
195
8.9
The Ergodic Decomposition of Aﬃne Functionals . . . . . . . . . . . . . . . . . . . .
198
Bibliography
199
Index
204

Chapter 1
Probability and Random Processes
1.1
Introduction
In this chapter we develop basic mathematical models of discrete time random processes.
Such
processes are also called discrete time stochastic processes, information sources, and time series.
Physically a random process is something that produces a succession of symbols called “outputs” a
random or nondeterministic manner. The symbols produced may be real numbers such as produced
by voltage measurements from a transducer, binary numbers as in computer data, two-dimensional
intensity ﬁelds as in a sequence of images, continuous or discontinuous waveforms, and so on. The
space containing all of the possible output symbols is called the alphabet of the random process, and
a random process is essentially an assignment of a probability measure to events consisting of sets of
sequences of symbols from the alphabet. It is useful, however, to treat the notion of time explicitly
as a transformation of sequences produced by the random process. Thus in addition to the common
random process model we shall also consider modeling random processes by dynamical systems as
considered in ergodic theory.
1.2
Probability Spaces and Random Variables
The basic tool for describing random phenomena is probability theory. The history of probability
theory is long, fascinating, and rich (see, for example, Maistrov [47]); its modern origins begin with
the axiomatic development of Kolmogorov in the 1930s [40]. Notable landmarks in the subsequent
development of the theory (and often still good reading) are the books by Cram´er [13], Lo`eve [44],
and Halmos [29]. Modern treatments that I have found useful for background and reference are Ash
[1], Breiman [8], Chung [11], and the treatment of probability theory in Billingsley [2].
Measurable Space
A measurable space (Ω, B) is a pair consisting of a sample space Ωtogether with a σ-ﬁeld B of
subsets of Ω(also called the event space). A σ-ﬁeld or σ-algebra B is a collection of subsets of Ω
with the following properties:
Ω∈B.
(1.1)
If F ∈B, then F c = {ω : ω ̸∈F} ∈B.
(1.2)
If Fi ∈B; i = 1, 2, . . . , then ∪Fi ∈B.
(1.3)
5

6
CHAPTER 1. PROBABILITY AND RANDOM PROCESSES
From de Morgan’s “laws” of elementary set theory it follows that also
∞
\
i=1
Fi = (
∞
[
i=1
F c
i )c ∈B.
An event space is a collection of subsets of a sample space (called events by virtue of belonging to
the event space) such that any countable sequence of set theoretic operations (union, intersection,
complementation) on events produces other events. Note that there are two extremes: the largest
possible σ-ﬁeld of Ωis the collection of all subsets of Ω(sometimes called the power set), and the
smallest possible σ-ﬁeld is {Ω, ∅}, the entire space together with the null set ∅= Ωc (called the
trivial space).
If instead of the closure under countable unions required by (1.3), we only require that the
collection of subsets be closed under ﬁnite unions, then we say that the collection of subsets is a
ﬁeld.
Although the concept of a ﬁeld is simpler to work with, a σ-ﬁeld possesses the additional im-
portant property that it contains all of the limits of sequences of sets in the collection. That is,
if Fn, n = 1, 2, . . . is an increasing sequence of sets in a σ-ﬁeld, that is, if Fn−1 ⊂Fn and if
F = S∞
n=1 Fn (in which case we write Fn ↑F or limn→∞Fn = F), then also F is contained in
the σ-ﬁeld. This property may not hold true for ﬁelds, that is, ﬁelds need not contain the limits
of sequences of ﬁeld elements. Note that if a ﬁeld has the property that it contains all increasing
sequences of its members, then it is also a σ-ﬁeld. In a similar fashion we can deﬁne decreasing sets:
If Fn decreases to F in the sense that Fn+1 ⊂Fn and F = T∞
n=1 Fn, then we write Fn ↓F. If
Fn ∈B for all n, then F ∈B.
Because of the importance of the notion of converging sequences of sets, we note a generalization
of the deﬁnition of a σ-ﬁeld that emphasizes such limits: A collection M of subsets of Ωis called a
monotone class if it has the property that if Fn ∈M for n = 1, 2, . . . and either Fn ↑F or Fn ↓F,
then also F ∈M. Clearly a σ-ﬁeld is a monotone class, but the reverse need not be true. If a ﬁeld
is also a monotone class, however, then it must be a σ-ﬁeld.
A σ-ﬁeld is sometimes referred to as a Borel ﬁeld in the literature and the resulting measurable
space called a Borel space. We will reserve this nomenclature for the more common use of these
terms as the special case of a σ-ﬁeld having a certain topological structure that will be developed
later.
Probability Spaces
A probability space (Ω, B, P) is a triple consisting of a sample space Ω, a σ-ﬁeld B of subsets of Ω,
and a probability measure P deﬁned on the σ-ﬁeld; that is, P(F) assigns a real number to every
member F of B so that the following conditions are satisﬁed:
Nonnegativity:
P(F) ≥0, all F ∈B,
(1.4)
Normalization:
P(Ω) = 1.
(1.5)
Countable Additivity:
If Fi ∈B, i = 1, 2, . . . are disjoint, then
P(
∞
[
i=1
Fi) =
∞
X
i=1
P(Fi).
(1.6)

1.2. PROBABILITY SPACES AND RANDOM VARIABLES
7
A set function P satisfying only (1.4) and (1.6) but not necessarily (1.5) is called a measure, and
the triple (Ω, B, P) is called a measure space. Since the probability measure is deﬁned on a σ-ﬁeld,
such countable unionss of subsets of Ωin the σ-ﬁeld are also events in the σ-ﬁeld. A set function
satisfying (1.6) only for ﬁnite sequences of disjoint events is said to be additive or ﬁnitely additive.
A straightforward exercise provides an alternative characterization of a probability measure in-
volving only ﬁnite additivity, but requiring the addition of a continuity requirement: a set function
P deﬁned on events in the σ-ﬁeld of a measurable space (Ω, B) is a probability measure if (1.4) and
(1.5) hold, if the following conditions are met:
Finite Additivity:
IfFi ∈B, i = 1, 2, . . . , n are disjoint, then
P(
n
[
i=1
Fi) =
n
X
i=1
P(Fi),
(1.7)
and
Continuity at ∅: if Gn ↓∅(the empty or null set), that is, if Gn+1 ⊂Gn , all n, and T∞
n=1 Gn = ∅
, then
lim
n→∞P(Gn) = 0.
(1.8)
The equivalence of continuity and countable additivity is easily seen by making the correspondence
Fn = Gn −Gn−1 and observing that countable additivity for the Fn will hold if and only if the
continuity relation holds for the Gn . It is also easy to see that condition (1.8) is equivalent to two
other forms of continuity:
Continuity from Below:
If Fn ↑F, then
lim
n→∞P(Fn) = P(F).
(1.9)
Continuity from Above:
If Fn ↓F, then
lim
n→∞P(Fn) = P(F).
(1.10)
Thus a probability measure is an additive, nonnegative, normalized set function on a σ-ﬁeld or
event space with the additional property that if a sequence of sets converges to a limit set, then the
corresponding probabilities must also converge.
If we wish to demonstrate that a set function P is indeed a valid probability measure, then we
must show that it satisﬁes the preceding properties (1.4), (1.5), and either (1.6) or (1.7) and one of
(1.8), (1.9), or (1.10).
Observe that if a set function satisﬁes (1.4), (1.5), and (1.7), then for any disjoint sequence of
events {Fi} and any n
P(
∞
[
i=0
Fi)
=
P(
n
[
i=0
Fi) + P(
∞
[
i=n+1
Fi)
≥
P(
n
[
i=0
Fi)
=
n
X
i=0
P(Fi).

8
CHAPTER 1. PROBABILITY AND RANDOM PROCESSES
and hence we have taking the limit as n →∞that
P(
∞
[
i=0
Fi) ≥
∞
X
i=0
P(Fi).
(1.11)
Thus to prove that P is a probability measure one must show that the preceding inequality is in
fact an equality.
Random Variables
Given a measurable space (Ω, B), let (A, BA) denote another measurable space. The ﬁrst space can
be thought of as an input space and the second as an output space. A random variable or measurable
function deﬁned on (Ω, B) and taking values in (A, BA) is a mapping or function f : Ω→A with
the property that
if F ∈BA, then f −1(F) = {ω : f(ω) ∈F} ∈B.
(1.12)
The name random variable is commonly associated with the case where A is the real line and B
the Borel ﬁeld (which we shall later deﬁne) and occasionally a more general sounding name such
as random object is used for a measurable function to include implicitly random variables (A the
real line), random vectors (A a Euclidean space), and random processes (A a sequence or waveform
space). We will use the term random variable in the general sense.
A random variable is just a function or mapping with the property that inverse images of input
events determined by the random variable are events in the original measurable space. This simple
property ensures that the output of the random variable will inherit its own probability measure.
For example, with the probability measure Pf deﬁned by
Pf(B) = P(f −1(B)) = P({ω : f(ω) ∈B}); B ∈BA,
(A, BA, Pf) becomes a probability space since measurability of f and elementary set theory ensure
that Pf is indeed a probability measure. The induced probability measure Pf is called the distri-
bution of the random variable f. The measurable space (A, BA) or, simply, the sample space A
is called the alphabet of the random variable f. We shall occasionally also use the notation Pf−1
which is a mnemonic for the relation Pf−1(F) = P(f −1(F)) and which is less awkward when f
itself is a function with a complicated name, e.g., ΠI→M.
If the alphabet A of a random variable f is not clear from context, then we shall refer to f as
an A-valued random variable. If f is a measurable function from (Ω, B) to (A, BA), we will say that
f is B/BA-measurable if the σ-ﬁelds are not clear from context.
Exercises
1. Set theoretic diﬀerence is deﬁned by F −G = F ∩Gc and symmetric diﬀerence is deﬁned by
F∆G = (F ∩Gc) ∪(F c ∩G).
(Set diﬀerence is also denoted by a backslash, F\G.) Show that
F∆G = (F ∪G) −(F ∩G)
and
F ∪G = F ∪(F −G).

1.2. PROBABILITY SPACES AND RANDOM VARIABLES
9
Hint: When proving two sets F and G are equal, the straightforward approach is to show ﬁrst
that if ω ∈F, then also ω ∈G and hence F ⊂G. Reversing the procedure proves the sets
equal.
2. Let Ωbe an arbitrary space. Suppose that Fi, i = 1, 2, . . . are all σ-ﬁelds of subsets of Ω.
Deﬁne the collection F = T
i Fi; that is, the collection of all sets that are in all of the Fi.
Show that F is a σ-ﬁeld.
3. Given a measurable space (Ω, F), a collection of sets G is called a sub-σ-ﬁeld of F if it is a
σ-ﬁeld and if all of its elements belong to F, in which case we write G ⊂F. Show that G is
the intersection of all σ-ﬁelds of subsets of Ωof which it is a sub-σ-ﬁeld.
4. Prove deMorgan’s laws.
5. Prove that if P satisﬁes (1.4), (1.5), and (1.7), then (1.8)-(1.10) are equivalent, that is, any
one holds if and only if the other two also hold. Prove the following elementary properties of
probability (all sets are assumed to be events).
6. P(F S G) = P(F) + P(G) −P(F ∩G).
7. (The union bound)
P(
∞
[
i=1
Gi) =
∞
X
i=1
P(Gi).
8. P(F c) = 1 −P(F).
9. For all events F P(F) ≤1.
10. If G ⊂F, then P(F −G) = P(F) −P(G).
11. P(F∆G) = P(F S G) −P(F ∩G).
12. P(F∆G) = P(F) + P(G) −2P(F ∩G).
13. |P(F) −P(G)| ≤P(F∆G).
14. P(F∆G) ≤P(F∆H) + P(H∆G).
15. If F ∈B, show that the indicator function 1F deﬁned by 1F (x) = 1 if x ∈F and 0 otherwise is
a random variable. Describe its distribution. Is the product of indicator functions measurable?
16. If Fi, i = 1, 2, . . . is a sequence of events that all have probability 1, show that T
i Fi also has
probability 1.
17. Suppose that Pi, i = 1, 2, . . . is a countable family of probability measures on a space (Ω, B)
and that ai, i = 1, 2, . . . is a sequence of positive real numbers that sums to one. Show that
the set function m deﬁned by
m(F) =
∞
X
i=1
aiPi(F)
is also a probability measure on (Ω, B). This is an example of a mixture probability measure.

10
CHAPTER 1. PROBABILITY AND RANDOM PROCESSES
18. Show that for two events F and G,
|1F (x) −1G(x)| = 1F ∆G(x).
19. Let f : Ω→A be a random variable. Prove the following properties of inverse images:
f −1(Gc) =
¡
f −1(G)
¢c
f −1(
∞
[
k=1
Gk) =
∞
[
k=1
f −1(Gk)
If G ∩F = ∅, then f −1(G) ∩f −1(F) = ∅.
1.3
Random Processes and Dynamical Systems
We now consider two mathematical models for a random process. The ﬁrst is the familiar one in
elementary courses: a random process is just a sequence of random variables. The second model is
likely less familiar: a random process can also be constructed from an abstract dynamical system
consisting of a probability space together with a transformation on the space. The two models are
connected by considering a time shift to be a transformation, but an example from communica-
tion theory shows that other transformations can be useful. The formulation and proof of ergodic
theorems are more natural in the dynamical system context.
Random Processes
A discrete time random process, or for our purposes simply a random process, is a sequence of random
variables {Xn}n∈I or {Xn; n ∈I}, where I is an index set, deﬁned on a common probability space
(Ω, B, P). We usually assume that all of the random variables share a common alphabet, say A.
The two most common index sets of interest are the set of all integers Z = {. . . , −2, −1, 0, 1, 2, . . .},
in which case the random process is referred to as a two-sided random process, and the set of all
nonnegative integers Z+ = {0, 1, 2, . . .}, in which case the random process is said to be one-sided.
One-sided random processes will often prove to be far more diﬃcult in theory, but they provide
better models for physical random processes that must be “turned on” at some time or that have
transient behavior.
Observe that since the alphabet A is general, we could also model continuous time random
processes in the preceding fashion by letting A consist of a family of waveforms deﬁned on an interval,
e.g., the random variable Xn could be in fact a continuous time waveform X(t) for t ∈[nT, (n+1)T),
where T is some ﬁxed positive real number.
The preceding deﬁnition does not specify any structural properties of the index set I. In partic-
ular, it does not exclude the possibility that I be a ﬁnite set, in which case random vector would be
a better name than random process. In fact, the two cases of I = Z and I = Z+ will be the only
really important examples for our purposes. The general notation of I will be retained, however,
in order to avoid having to state separate results for these two cases. Most of the theory to be
considered in this chapter, however, will remain valid if we simply require that I be closed under
addition, that is, if n and k are in I , then so is n + k (where the “+” denotes a suitably deﬁned
addition in the index set). For this reason we henceforth will assume that if I is the index set for a
random process, then I is closed in this sense.

1.3. RANDOM PROCESSES AND DYNAMICAL SYSTEMS
11
Dynamical Systems
An abstract dynamical system consists of a probability space (Ω, B, P) together with a measurable
transformation T : Ω→Ωof Ωinto itself. Measurability means that if F ∈B, then also T −1F =
{ω : Tω ∈F} ∈B. The quadruple (Ω, B, P, T) is called a dynamical system in ergodic theory. The
interested reader can ﬁnd excellent introductions to classical ergodic theory and dynamical system
theory in the books of Halmos [30] and Sinai [66]. More complete treatments may be found in [2]
[63] [57] [14] [72] [51] [20] [41]. The name dynamical systems comes from the focus of the theory on
the long term dynamics or dynamical behavior of repeated applications of the transformation T on
the underlying measure space.
An alternative to modeling a random process as a sequence or family of random variables deﬁned
on a common probability space is to consider a single random variable together with a transformation
deﬁned on the underlying probability space. The outputs of the random process will then be values
of the random variable taken on transformed points in the original space. The transformation will
usually be related to shifting in time, and hence this viewpoint will focus on the action of time
itself. Suppose now that T is a measurable mapping of points of the sample space Ωinto itself. It is
easy to see that the cascade or composition of measurable functions is also measurable. Hence the
transformation T n deﬁned as T 2ω = T(Tω) and so on (T nω = T(T n−1ω)) is a measurable function
for all positive integers n. If f is an A-valued random variable deﬁned on (Ω, B), then the functions
fT n : Ω→A deﬁned by fT n(ω) = f(T nω) for ω ∈Ωwill also be random variables for all n in
Z+. Thus a dynamical system together with a random variable or measurable function f deﬁnes a
single-sided random process {Xn}n∈Z+ by Xn(ω) = f(T nω). If it should be true that T is invertible,
that is, T is one-to-one and its inverse T −1 is measurable, then one can deﬁne a double-sided random
process by Xn(ω) = f(T nω), all n in Z.
The most common dynamical system for modeling random processes is that consisting of a
sequence space Ωcontaining all one- or two-sided A-valued sequences together with the shift trans-
formation T, that is, the transformation that maps a sequence {xn} into the sequence {xn+1}
wherein each coordinate has been shifted to the left by one time unit.
Thus, for example, let
Ω= AZ+ = {all x = (x0, x1, . . .) with xi ∈A for all i} and deﬁne T : Ω→Ωby T(x0, x1, x2, . . .) =
(x1, x2, x3, . . .). T is called the shift or left shift transformation on the one-sided sequence space.
The shift for two-sided spaces is deﬁned similarly.
Some interesting dynamical systems in communications applications do not, however, have this
structure. As an example, consider the mathematical model of a device called a sigma-delta modula-
tor, that is used for analog-to-digital conversion, that is, encoding a sequence of real numbers into a
binary sequence (analog-to-digital conversion), which is then decoded into a reproduction sequence
approximating the original sequence (digital-to-analog conversion) [35] [9] [22]. Given an input se-
quence {xn} and an initial state u0, the operation of the encoder is described by the diﬀerence
equations
en = xn −q(un),
un = en−1 + un−1,
where q(u) is +b if its argument is nonnegative and −b otherwise (q is called a binary quantizer).
The decoder is described by the equation
ˆxn = 1
N
N
X
i=1
q(un−i).
The basic idea of the code’s operation is this: An incoming continuous time, continuous amplitude
waveform is sampled at a rate that is so high that the incoming waveform stays fairly constant over

12
CHAPTER 1. PROBABILITY AND RANDOM PROCESSES
N sample times (in engineering parlance the original waveform is oversampled or sampled at many
times the Nyquist rate). The binary quantizer then produces outputs for which the average over N
samples is very near the input so that the decoder output xhatkN is a good approximation to the
input at the corresponding times. Since ˆxn has only a discrete number of possible values (N + 1 to
be exact), one has thereby accomplished analog-to-digital conversion. Because the system involves
only a binary quantizer used repeatedly, it is a popular one for microcircuit implementation.
As an approximation to a very slowly changing input sequence xn, it is of interest to analyze the
response of the system to the special case of a constant input xn = x ∈[−b, b) for all n (called a
quiet input). This can be accomplished by recasting the system as a dynamical system as follows:
Given a ﬁxed input x, deﬁne the transformation T by
Tu =
½
u + x −b;
if u ≥0
u + x + b;
if u < 0.
Given a constant input xn = x, n = 1, 2, . . . , N, and an initial condition u0 (which may be ﬁxed or
random), the resulting Un sequence is given by
un = T nu0.
If the initial condition u0 is selected at random, then the preceding formula deﬁnes a dynamical
system which can be analyzed.
The example is provided simply to emphasize the fact that time shifts are not the only interesting
transformation when modeling communication systems.
The diﬀerent models provide equivalent models for a given process: one emphasizing the sequence
of outputs and the other emphasising the action of a transformation on the underlying space in
producing these outputs. In order to demonstrate in what sense the models are equivalent for given
random processes, we next turn to the notion of the distribution of a random process.
Exercises
1. Consider the sigma-delta example with a constant input in the case b = 1/2, u0 = 0, and
x = 1/π. Find un for n = 1, 2, 3, 4.
2. Show by induction in the constant input sigma-delta example that if u0 = 0 and x ∈[−b, b),
then un ∈[−b, b) for all n = 1, 2, . . ..
3. Let Ω= [0, 1) and F = [0, 1/2) and ﬁx an α ∈(0, 1). Deﬁne the transformation Tx = αx,
where r ∈[0, 1) denotes the fractional part of r; that is, every real number r has a unique
representation as r = K + r for some integer K. Show that if α is rational, then T nx is a
periodic sequence in n.
1.4
Distributions
Although in principle all probabilistic quantities of a random process can be determined from the
underlying probability space, it is often more convenient to deal with the induced probability mea-
sures or distributions on the space of possible outputs of the random process. In particular, this
allows us to compare diﬀerent random processes without regard to the underlying probability spaces
and thereby permits us to reasonably equate two random processes if their outputs have the same
probabilistic structure, even if the underlying probability spaces are quite diﬀerent.

1.4. DISTRIBUTIONS
13
We have already seen that each random variable Xn of the random process {Xn} inherits a
distribution because it is measurable. To describe a process, however, we need more than simply
probability measures on output values of separate single random variables: we require probability
measures on collections of random variables, that is, on sequences of outputs. In order to place
probability measures on sequences of outputs of a random process, we ﬁrst must construct the
appropriate measurable spaces. A convenient technique for accomplishing this is to consider product
spaces, spaces for sequences formed by concatenating spaces for individual outputs.
Let I denote any ﬁnite or inﬁnite set of integers. In particular, I = Z(n) = {0, 1, 2, . . . , n −1},
I = Z, or I = Z+. Deﬁne xI = {xi}i∈I. For example, xZ = (. . . , x−1, x0, x1, . . .) is a two-sided
inﬁnite sequence. When I = Z(n) we abbreviate xZ(n) to simply xn . Given alphabets Ai, i ∈I ,
deﬁne the cartesian product spaces
×i∈IAi = { all xI : xi ∈Ai all i ∈I}.
In most cases all of the Ai will be replicas of a single alphabet A and the preceding product will be
denoted simply by AI. We shall abbreviate the space AZ(n), the space of all n dimensional vectors
with coordinates in A, by An. Thus, for example, Am,m+1,...,n is the space of all possible outputs of
the process from time m to time n; AZ is the sequence space of all possible outputs of a two-sided
process.
To obtain useful σ-ﬁelds of the preceding product spaces, we introduce the idea of a rectangle in
a product space. A rectangle in AI taking values in the coordinate σ-ﬁelds Bi, i ∈J , is deﬁned as
any set of the form
B = {xI ∈AI : xi ∈Bi; all i ∈J },
(1.13)
where J is a ﬁnite subset of the index set I and Bi ∈Bi for all i ∈J . (Hence rectangles are
sometimes referred to as ﬁnite dimensional rectangles.) A rectangle as in (1.13) can be written as a
ﬁnite intersection of one-dimensional rectangles as
B =
\
i∈J
{xI ∈AI : xi ∈Bi} =
\
i∈J
Xi
−1(Bi)
(1.14)
where here we consider Xi as the coordinate functions Xi : AI →A deﬁned by Xi(xI) = xi.
As rectangles in AI are clearly fundamental events, they should be members of any useful σ-ﬁeld
of subsets of AI. One approach is simply to deﬁne the product σ-ﬁeld BI
A as the smallest σ-ﬁeld
containing all of the rectangles, that is, the collection of sets that contains the clearly important
class of rectangles and the minimum amount of other stuﬀrequired to make the collection a σ-ﬁeld.
In general, given any collection G of subsets of a space Ω, then σ(G) will denote the smallest σ-ﬁeld
of subsets of Ωthat contains G and it will be called the σ-ﬁeld generated by G. By smallest we mean
that any σ-ﬁeld containing G must also contain σ(G). The σ-ﬁeld is well deﬁned since there must
exist at least one σ-ﬁeld containing G, the collection of all subsets of Ω. Then the intersection of all
σ-ﬁelds that contain G must be a σ-ﬁeld, it must contain G, and it must in turn be contained by all
σ-ﬁelds that contain G.
Given an index set I of integers, let rect(Bi, i ∈I) denote the set of all rectangles in AI taking
coordinate values in sets in Bi, i ∈I . We then deﬁne the product σ-ﬁeld of AI by
BA
I = σ(rect(Bi, i ∈I)).
At ﬁrst glance it would appear that given an index set I and an A-valued random process
{Xn}n∈I deﬁned on an underlying probability space (Ω, B, P), then given any index set J ⊂I , the
measurable space (AJ , BJ
A ) should inherit a probability measure from the underlying space through

14
CHAPTER 1. PROBABILITY AND RANDOM PROCESSES
the random variables XJ = {Xn; n ∈J }. The only hitch is that so far we only know that individual
random variables Xn are measurable (and hence inherit a probability measure). To make sense here
we must ﬁrst show that collections of random variables such as the random sequence XZ or the
random vector Xn = {X0, . . . , Xn−1} are also measurable and hence themselves random variables.
Observe that for any index set I of integers it is easy to show that inverse images of the mapping
XI from Ωto AI will yield events in B if we conﬁne attention to rectangles. To see this we simply
use the measurability of each individual Xn and observe that since (XI)−1(B) = S
i∈I X−1
i
(Bi) and
since ﬁnite and countable unions of events are events, then we have for rectangles that
(XI)−1(B) ∈B.
(1.15)
We will have the desired measurability if we can show that if (1.15) is satisﬁed for all rectangles,
then it is also satisﬁed for all events in the σ-ﬁeld generated by the rectangles. This result is an
application of an approach named the good sets principle by Ash [1], p. 5. We shall occasionally
wish to prove that all events possess some particular desirable property that is easy to prove for
generating events. The good sets principle consists of the following argument: Let S be the collection
of good sets consisting of of all events F ∈σ(G) possessing the desired property. If
• G ⊂S and hence all the generating events are good, and
• S is a σ-ﬁeld,
then σ(G) ⊂S and hence all of the events F ∈σ(G) are good.
Lemma 1.4.1 Given measurable spaces (Ω1 ,B) and (Ω2 , σ(G)), then a function f : Ω1 →Ω2
is B-measurable if and only if f−1(F) ∈B for all F ∈G; that is, measurability can be veriﬁed by
showing that inverse images of generating events are events.
Proof: If f is B-measurable, then f −1(F) ∈B for all F and hence for all F ∈G. Conversely, if
f −1(F) ∈B for all generating events F ∈G, then deﬁne the class of sets
S = {G : G ∈σ(G), f−1(G) ∈B}.
It is straightforward to verify that S is a σ-ﬁeld, clearly Ω1 ∈S since it is the inverse image of Ω2.
The fact that S contains countable unions of its elements follows from the fact that σ(G) is closed
to countable unions and inverse images preserve set theoretic operations, that is,
f −1(∪iGi) = ∪if −1(Gi).
Furthermore, S contains every member of G by assumption. Since S contains G and is a σ-ﬁeld,
σ(G) ⊂S by the good sets principle.
2
We have shown that the mappings XI : Ω→AI are measurable and hence the output measurable
space (AI, BI
A) will inherit a probability measure from the underlying probability space and thereby
determine a new probability space (AI, BI
A, PXI), where the induced probability measure is deﬁned
by
PXI(F) = P((XI)−1(F)) = P({ω : XI(ω) ∈F}), F ∈BA
I.
(1.16)
Such probability measures induced on the outputs of random variables are referred to as distributions
for the random variables, exactly as in the simpler case ﬁrst treated. When I = {m, m + 1, . . . , m +
n −1}, e.g., when we are treating Xn taking values in An, the distribution is referred to as an
n-dimensional or nth order distribution and it describes the behavior of an n-dimensional random

1.4. DISTRIBUTIONS
15
variable. If I is the entire process index set, e.g., if I = Z for a two-sided process or I = Z+ for a one-
sided process, then the induced probability measure is deﬁned to be the distribution of the process.
Thus, for example, a probability space (Ω, B, P) together with a doubly inﬁnite sequence of random
variables {Xn}n∈Z induces a new probability space (AZ, BZ
A, PXZ) and PXZ is the distribution of
the process. For simplicity, let us now denote the process distribution simply by m. We shall call
the probability space (AI, BI
A, m) induced in this way by a random process {Xn}n∈Z the output
space or sequence space of the random process.
Equivalence
Since the sequence space (AI, BI
A, m) of a random process {Xn}n∈Z is a probability space, we can
deﬁne random variables and hence also random processes on this space. One simple and useful such
deﬁnition is that of a sampling or coordinate or projection function deﬁned as follows: Given a
product space AI, deﬁne the sampling functions Πn : AI →A by
Πn(xI) = xn, xI ∈AI, n ∈I.
(1.17)
The sampling function is named Π since it is also a projection. Observe that the distribution of the
random process {Πn}n∈I deﬁned on the probability space (AI, BI
A, m) is exactly the same as the
distribution of the random process {Xn}n∈I deﬁned on the probability space (Ω, B, P). In fact, so
far they are the same process since the {Πn} simply read oﬀthe values of the {Xn}.
What happens, however, if we no longer build the Πn on the Xn, that is, we no longer ﬁrst select
ω from Ωaccording to P, then form the sequence xI = XI(ω) = {Xn(ω)}n∈I, and then deﬁne
Πn(xI) = Xn(ω)? Instead we directly choose an x in AI using the probability measure m and then
view the sequence of coordinate values. In other words, we are considering two completely separate
experiments, one described by the probability space (Ω, B, P) and the random variables {Xn} and
the other described by the probability space (AI, BI
A, m) and the random variables {Πn}. In these
two separate experiments, the actual sequences selected may be completely diﬀerent. Yet intuitively
the processes should be the same in the sense that their statistical structures are identical, that
is, they have the same distribution. We make this intuition formal by deﬁning two processes to
be equivalent if their process distributions are identical, that is, if the probability measures on the
output sequence spaces are the same, regardless of the functional form of the random variables of the
underlying probability spaces. In the same way, we consider two random variables to be equivalent
if their distributions are identical.
We have described two equivalent processes or two equivalent models for the same random
process, one deﬁned as a sequence of perhaps very complicated random variables on an underlying
probability space, the other deﬁned as a probability measure directly on the measurable space of
possible output sequences. The second model will be referred to as a directly given random process.
Which model is better depends on the application. For example, a directly given model for a
random process may focus on the random process itself and not its origin and hence may be simpler
to deal with. If the random process is then coded or measurements are taken on the random process,
then it may be better to model the encoded random process in terms of random variables deﬁned
on the original random process and not as a directly given random process. This model will then
focus on the input process and the coding operation. We shall let convenience determine the most
appropriate model.
We can now describe yet another model for the random process described previously, that is,
another means of describing a random process with the same distribution. This time the model
is in terms of a dynamical system. Given the probability space (AI, BI
A, m), deﬁne the (left) shift

16
CHAPTER 1. PROBABILITY AND RANDOM PROCESSES
transformation T : AI →AI by
T(xI) = T({xn}n∈I) = yI = {yn}n∈I,
where
yn = xn+1, n ∈I.
Thus the nth coordinate of yI is simply the (n + 1) coordinate of xI . (Recall that we assume for
random processes that I is closed under addition and hence if n and 1 are in I, then so is (n + 1).)
If the alphabet of such a shift is not clear from context, we will occasionally denote it by TA or TAI.
It can easily be shown that the shift is indeed measurable by showing it for rectangles and then
invoking Lemma 1.4.1.
Consider next the dynamical system (AI, BA
I, P, T) and the random process formed by combin-
ing the dynamical system with the zero time sampling function Π0 (we assume that 0 is a member
of I ). If we deﬁne Yn(x) = Π0(T nx) for x = xI ∈AI, or, in abbreviated form, Yn = Π0T n,
then the random process {Yn}n∈I is equivalent to the processes developed previously. Thus we have
developed three diﬀerent, but equivalent, means of producing the same random process. Each will
be seen to have its uses.
The preceding development shows that a dynamical system is a more fundamental entity than
a random process since we can always construct an equivalent model for a random process in terms
of a dynamical system: use the directly given representation, shift transformation, and zero time
sampling function.
The shift transformation introduced previously on a sequence space is the most important trans-
formation that we shall encounter. It is not, however, the only important transformation. Hence
when dealing with transformations we will usually use the notation T to reﬂect the fact that it is
often related to the action of a simple left shift of a sequence, yet we should keep in mind that
occasionally other operators will be considered and the theory to be developed will remain valid,;
that is, T is not required to be a simple time shift. For example, we will also consider block shifts
of vectors instead of samples and variable length shifts.
Most texts on ergodic theory deal with the case of an invertible transformation, that is, where T
is a one-to-one transformation and the inverse mapping T −1 is measurable. This is the case for the
shift on AZ, the so-called two-sided shift. It is not the case, however, for the one-sided shift deﬁned
on AZ
+ and hence we will avoid use of this assumption. We will, however, often point out in the
discussion and exercises what simpliﬁcations or special properties arise for invertible transformations.
Since random processes are considered equivalent if their distributions are the same, we shall
adopt the notation [A, m, X] for a random process {Xn; n ∈I} with alphabet A and process distri-
bution m, the index set I usually being clear from context. We will occasionally abbreviate this to
the more common notation [A, m], but it is often convenient to note the name of the output random
variables as there may be several; e.g., a random process may have an input X and output Y . By the
associated probability space of a random process [A, m, X] we shall mean the sequence probability
space (AI, BI
A, m). It will often be convenient to consider the random process as a directly given
random process, that is, to view Xn as the coordinate functions Πn on the sequence space AI rather
than as being deﬁned on some other abstract space. This will not always be the case, however, as
often processes will be formed by coding or communicating other random processes. Context should
render such bookkeeping details clear.
Monotone Classes
Unfortunately there is no constructive means of describing the σ-ﬁeld generated by a class of sets.
That is, we cannot give a prescription of adding all countable unions, then all complements, and so

1.5. EXTENSION
17
on, and be ensured of thereby giving an algorithm for obtaining all σ-ﬁeld members as sequences
of set theoretic operations on members of the original collection. We can, however, provide some
insight into the structure of such generated σ-ﬁelds when the original collection is a ﬁeld. This
structure will prove useful when considering extensions of measures.
Recall that a collection M of subsets of Ωis a monotone class if whenever Fn ∈M, n = 1, 2, . . .,
and Fn ↑F or Fn ↓F, then also F ∈M.
Lemma 1.4.2 Given a ﬁeld F, then σ(F) is the smallest monotone class containing F.
Proof: Let M be the smallest monotone class containing F and let F ∈M. Deﬁne MF as the
collection of all sets G ∈M for which F ∩G, F ∩Gc, and F c ∩G are all in M. Then MF is a
monotone class. If F ∈F, then all members of F must also be in MF since they are in M and
since F is a ﬁeld. Since both classes contain F and M is the minimal monotone class, M ⊂MF .
Since the members of MF are all chosen from M, M = MF . This implies in turn that for any
G ∈M, then all sets of the form G ∩F, G ∩F c, and Gc ∩F for any F ∈F are in M. Thus for
this G, all F ∈F are members of MG or F ⊂MG for any G ∈M. By the minimality of M, this
means that MG = M. We have now shown that for F, G ∈M = MF , then F ∩G, F ∩Gc, and
F c ∩G are also in M. Thus M is a ﬁeld. Since it also contains increasing limits of its members,
it must be a σ-ﬁeld and hence it must contain σ(F) since it contains F. Since σ(F) is a monotone
class containing F, it must contain M; hence the two classes are identical.
Exercises
1. Given a random process {Xn} with alphabet A, show that the class F0 = rect(Bi; i ∈I) of all
rectangles is a ﬁeld.
2. Let F(G) denote the ﬁeld generated by a class of sets G, that is, F(G) contains the given class
and is in turn contained by all other ﬁelds containing G. Show that σ(G) = σ(F(G)).
1.5
Extension
We have seen one example where a σ-ﬁeld is formed by generating it from a class of sets. Just as we
construct event spaces by generating them from important collections of sets, we will often develop
probability measures by specifying their values on an important class of sets and then extending
the measure to the full σ-ﬁeld generated by the class. The goal of this section is to develop the
fundamental result for extending probability measures from ﬁelds to σ-ﬁelds, the Carath´eodory
extension theorem. The theorem states that if we have a probability measure on a ﬁeld, then there
exists a unique probability measure on the σ-ﬁeld that agrees with the given probability measure on
events in the ﬁeld. We shall develop the result in a series of steps. The development is patterned on
that of Halmos [29].
Suppose that F is a ﬁeld of subsets of a space Ωand that P is a probability measure on a ﬁeld
F; that is, P is a nonnegative, normalized, countably additive set function when conﬁned to sets in
F. We wish to obtain a probability measure, say λ, on σ(F) with the property that for all F ∈F,
λ(F) = P(F). Eventually we will also wish to demonstrate that there is only one such λ. Toward
this end deﬁne the set function
λ(F) =
inf
{Fi}∈F,F ⊂S
i Fi
X
i
P(Fi).
(1.18)

18
CHAPTER 1. PROBABILITY AND RANDOM PROCESSES
The inﬁmum is over all countable collections of ﬁeld elements whose unions contain the set F. We
will call such a collection of ﬁeld members whose union contains F a cover of F. Note that we could
conﬁne interest to covers whose members are all disjoint since if Fi is an arbitrary cover of F, then
the collection {Gi} with G1 = F1, Gi = Fi −Fi−1, i = 1, 2, . . . is a disjoint cover for F. Observe
that this set function is deﬁned for all subsets of Ω. Note that from the deﬁnition, given any set F
and any ϵ > 0, there exists a cover {Fi} such that
X
i
P(Fi) −ϵ ≤λ(F) ≤
X
i
P(Fi).
(1.19)
A cover satisfying (1.19) will be called an ϵ-cover for F.
The goal is to show that λ is in fact a probability measure on σ(F). Obviously λ is nonnegative,
so we need to show that it is normalized and countably additive. This we will do in a series of steps,
beginning with the simplest:
Lemma 1.5.1 The set function λ of (1.18) satisﬁes
(a) λ(∅) = 0.
(b) Monotonicity: If F ⊂G, then λ(F) ≤λ(G).
(c) Subadditivity: For any two sets F, G,
λ(F ∪G) ≤λ(F) + λ(G).
(1.20)
(d) Countable Subadditivity: Given sets {Fi},
λ(∪iFi) ≤
∞
X
i=1
λ(Fi).
(1.21)
Proof: Property (a) follows immediately from the deﬁnition since empty ∈F and contains itself,
hence
λ(∅) ≤P(∅) = 0.
From (1.19) given G and ϵ we can choose a cover {Gn} for G such that
X
i
P(Gi) ≤λ(G) + ϵ.
Since F ⊂G, a cover for G is also a cover for F and hence
λ(F) ≤
X
i
P(Gi) ≤λ(G) + ϵ.
Since ϵ is arbitrary, (b) is proved. To prove (c) let {Fi} and {Gi} be ϵ/2 covers for F and G. Then
{Fi
S Gi} is a cover for F S G and hence
λ(F ∪G) ≤
X
i
P(Fi ∪Gi) ≤
X
i
P(Fi) +
X
i
P(Gi) ≤λ(F) + λ(G) + ϵ.
Since ϵ is arbitrary, this proves (c).

1.5. EXTENSION
19
To prove (d), for each Fi let {Fik; k = 1, 2, . . .} be an ϵ2−i cover for Fi.
Then {Fik; i =
1, 2, . . . ; j = 1, 2, . . .} is a cover for S
i Fi and hence
λ(∪iFi) ≤
∞
X
i=1
∞
X
k=1
P(Fik) ≤
∞
X
i=1
(λ(Fi) + ϵ2−i) =
∞
X
i=1
λ(Fi) + ϵ,
which completes the proof since ϵ is arbitrary.
2
We note in passing that a set function λ on a collection of sets having properties (a)-(d) of the
lemma is called an outer measure on the collection of sets.
The simple properties have an immediate corollary: The set function λ agrees with P on ﬁeld
events.
Corollary 1.5.1 If F ∈F, then λ(F) = P(F). Thus, for example, λ(Ω) = 1.
Proof: Since a set covers itself, we have immediately that λ(F) ≤P(F) for all ﬁeld events F.
Suppose that {Fi} is an ϵ cover for F. Then
λ(F) ≥
X
i
P(Fi) −ϵ ≥
X
i
P(F ∩Fi) −ϵ.
Since F ∩Fi ∈F and F ⊂S
i Fi, S
i(F ∩Fi) = F ∩S
i Fi = F ∈F and hence, invoking the countable
additivity of P on F,
X
i
P(F ∩Fi) = P(∪iF ∩Fi) = P(F)
and hence
λ(F) ≥P(F) −ϵ.
2
Thus far the properties of λ have depended primarily on manipulating the deﬁnition. In order to
show that λ is countably additive (or even only ﬁnitely additive) over σ(F), we need to introduce a
new concept and a new collection of sets that we will later see contains σ(F). The deﬁnitions seem
a bit artiﬁcial, but some similar form of tool seems to be necessary to get to the desired goal. By
way of motivation, we are trying to show that a set function is ﬁnitely additive on some class of sets.
Perhaps the simplest form of ﬁnite additivity looks like
λ(F) = λ(F ∩R) + λ(F ∩Rc).
Hence it should not seem too strange to build at least this form into the class of sets considered. To
do this, deﬁne a set R ∈σ(F) to be λ-measurable if
λ(F) = λ(F ∩R) + λ(F ∩Rc), all F ∈σ(F).
In words, a set R is λ-measurable if it splits all events in σ(F) in an additive fashion. Let H denote
the collection of all λ-measurable sets. We shall see that indeed λ is countably additive on the
collection H and that H contains σ(F). Observe for later use that since λ is subadditive, to prove
that R ∈H requires only that we prove
λ(F) ≥λ(F ∩R) + λ(F ∩Rc), all F ∈σ(F).
Lemma 1.5.2 H is a ﬁeld.

20
CHAPTER 1. PROBABILITY AND RANDOM PROCESSES
Proof: Clearly Ω∈H since λ(∅) = 0 and Ω∩F = F. Equally clearly, F ∈H implies that F c ∈H.
The only work required is show that if F, G ∈H, then also F S G ∈H. To accomplish this begin
by recalling that for F, G ∈H and for any H ∈σ(F) we have that
λ(H) = λ(H ∩F) + λ(H ∩F c)
and hence since both arguments on the right are members of σ(F)
λ(H) = λ(H ∩F ∩G) + λ(H ∩F ∩Gc)
+λ(H ∩F c ∩G) + λ(H ∩F c ∩Gc).
(1.22)
As (1.22) is valid for any event H ∈σ(F), we can replace H by H ∩(F S G) to obtain
λ(H ∩(F ∪G)) = λ(H ∩(F ∩G))
+λ(H ∩(F ∩Gc)) + λ(H ∩(F c ∩G)),
(1.23)
where the fourth term has disappeared since the argument is null. Plugging (1.23) into (1.22) yields
λ(H) = λ(H ∩(F ∪G)) + λ(H ∩(F c ∩Gc))
which implies that F S G ∈H since F c ∩Gc = (F S G)c.
2
Lemma 1.5.3 H is a σ-ﬁeld.
Proof: Suppose that Fn ∈H, n = 1, 2, . . . and
F =
∞
[
i=1
Fi.
It will be convenient to work with disjoint sets, so we formally introduce the sets
G1 = F1
Gk = Fk −
k−1
[
i=1
Fi
(1.24)
with the property that the Gk are disjoint and
n
[
i=1
Gi =
n
[
i=1
Fi
for all integers n and for n = ∞. Since H is a ﬁeld, clearly the Gi and all ﬁnite unions of Fi or Gi
are also in H. First apply (1.23) with G1 and G2 replacing F and G. Using the fact that G1 and
G2 are disjoint,
λ(H ∩(G1 ∩G2)) = λ(H ∩G1) + λ(H ∩G2).
Using this equation and mathematical induction yields
λ(H ∩
n
[
i=1
Gi) =
n
X
i=1
λ(H ∩Gi).
(1.25)

1.5. EXTENSION
21
For convenience deﬁne the set
F(n) =
n
[
i=1
Fi =
n
[
i=1
Gi.
Since F(n) ∈H and since H is a ﬁeld, F(n)c ∈H and hence for any event H ∈H
λ(H) = λ(H ∩F(n)) + λ(H ∩F(n)c).
Using the preceding formula, (1.25), the fact that F(n) ⊂F and hence F c ⊂F(n)c, the monotonicity
of λ, and the countable subadditivity of λ,
λ(H) ≥
∞
X
i=1
λ(H ∩Gi) + λ(H ∩F c)
≥λ(H ∩F) + λ(H ∩F c).
(1.26)
Since H is an arbitrary event in H, this proves that the countable union F is in H and hence that
H is a σ-ﬁeld.
2
In fact much more was proved than the stated lemma; the implicit conclusion is made explicit
in the following corollary.
Corollary 1.5.2 λ is countably additive on H
Proof: Take Gn to be an arbitrary sequence of disjoint events in H in the preceding proof (instead
of obtaining them as diﬀerences of another arbitrary sequence) and let F denote the union of the
Gn. Then from the lemma F ∈H and (1.26) with H = F implies that
λ(F) ≥
X
i=1
λ(Gi).
Since λ is countably subadditive, the inequality must be an equality, proving the corollary.
2
We now demonstrate that the strange class H is exactly the σ-ﬁeld generated by F.
Lemma 1.5.4 H = σ(F).
Proof: Since the members of H were all chosen from σ(F), H ⊂σ(F). Let F ∈F and G ∈σ(F)
and let {Gn} be an ϵ-cover for G. Then
λ(G) + ϵ ≥
∞
X
i=1
P(Gn) =
∞
X
i=1
(P(Gn ∩F) + P(Gn ∩F c))
≥λ(G ∩F) + λ(G ∩F c)
which implies that for F ∈F
λ(G) ≥λ(G ∩F) + λ(G ∩F c)
for all events G ∈σ(F) and hence that F ∈H. This implies that H contains F. Since H is a σ-ﬁeld,
however, it therefore must contain σ(F).
2
We have now proved that λ is a probability measure on σ(F) which agrees with P on F. The
only remaining question is whether it is unique. The following lemma resolves this issue.
Lemma 1.5.5 If two probability measures λ and µ on σ(F) satisfy λ(F) = µ(F) for all F in the
ﬁeld F, then the measures are identical; that is, λ(F) = µ(F) for all F ∈σ(F).

22
CHAPTER 1. PROBABILITY AND RANDOM PROCESSES
Proof: Let M denote the collection of all sets F such that λ(F) = µ(F). From the continuity of
probability, M is a monotone class. Since it contains F, it must contain the smallest monotone class
containing F. That class is σ(F) from Lemma 1.4.2 and hence M contains the entire σ-ﬁeld.
2
Combining all of the pieces of this section yields the principal result:
Theorem 1.5.1 The Carath´eodory Extension Theorem. If a set function P satisﬁes the properties
of a probability measure (nonnegativity, normalization, and countable additivity or ﬁnite additivity
plus continuity) for all sets in a ﬁeld F of subsets of a space Ω, then there exists a unique measure
λ deﬁned by (1.18) on the measurable space (Ω, σ(F)) that agrees with P on F.
When no confusion is possible, we will usually denote the extension of a measure P by P rather
than by a diﬀerent symbol.
We will this section with an important corollary to the extension theorem that shows that an
arbitrary event can be approximated closely by a ﬁeld event in a probabilistic way. First, however,
we derive a useful property of the symmetric diﬀerence operation deﬁned by
F∆G = (F ∩Gc) ∪(F c ∩G) = F ∪G −F ∩G.
(1.27)
Lemma 1.5.6 For any events G, F, H,
P(F∆G) ≤P(F∆H) + P(H∆G);
that is, probabilities of symmetric diﬀerences satisfy a triangle inequality.
Proof: A little set theory shows that
F∆G ⊂(F∆H) ∪(H∆G)
and hence the subadditivity and monotonicity of probability imply the lemma.
2
Corollary 1.5.3 (Approximation Theorem) Given a probability space (Ω, B, P) and a generating
ﬁeld F, that is, B = σ(F), then given F ∈B and ϵ > 0, there exists an F0 ∈F such that
P(F∆F0) ≤ϵ, where ∆denotes the symmetric diﬀerence (all points in the union that are not in the
intersection).
Proof: From the deﬁnition of λ in (1.18) (which yielded the extension of the original measure P)
and the ensuing discussion one can ﬁnd a countable collection of disjoint ﬁeld events {Fn} such that
F ⊂S
n Fn and
∞
X
i=1
P(Fi) −ϵ/2 ≤λ(F) ≤
∞
X
i=1
P(Fi).
Renaming λ as P
P(F∆
∞
[
i=1
Fn) = P(
∞
[
i=1
Fn −F) = P(
∞
[
i=1
Fn) −P(F) ≤ϵ
2.
Choose a ﬁnite union ˜F = Sn
i=1 Fi with n chosen large enough to ensure that
P(
∞
[
i=1
Fi∆˜F) = P(
∞
[
i=1
Fi −˜F) =
∞
X
i=n+1
P(Fi) ≤ϵ
2.
From the previous lemma we see that
P(F∆˜F) ≤P(F∆
∞
[
i=1
Fi) + P(
∞
[
i=1
Fi∆˜F) ≤ϵ.
2

1.6. ISOMORPHISM
23
Exercises
1. Show that if m and p are two probability measures on (Ω, σ(F)), where F is a ﬁeld, then given
an arbitrary event F and ϵ > 0 there is a ﬁeld event F0 ∈F such that m(F∆F0) ≤ϵ and
p(F∆F0) ≤ϵ.
1.6
Isomorphism
We have deﬁned random variables or random processes to be equivalent if they have the same output
probability spaces, that is, if they have the same probability measure or distribution on their output
value measurable space. This is not the only possible notion of two random processes being the
same. For example, suppose we have a random process with a binary output alphabet and hence
an output space made up of binary sequences. We form a new directly given random process by
taking each successive pair of the binary process and considering the outputs to be a sequence of
quaternary symbols, that is, the original random process is coded into a new random process via the
mapping
00
→
0
01
→
1
10
→
2
11
→
3
applied to successive pairs of binary symbols.
The two random processes are not equivalent since their output sequence measurable spaces are
diﬀerent; yet clearly they are the same since each can be obtained by a simple relabeling or coding of
the other. This leads to a more fundamental deﬁnition of sameness: isomorphism. The deﬁnition of
isomorphism is for dynamical systems rather than for random processes since, as previously noted,
this is the more fundamental notion and hence the deﬁnition applies to random processes.
There are, in fact, several notions of isomorphism: isomorphic measurable spaces, isomorphic
probability spaces, and isomorphic dynamical systems. We present these deﬁnitions together as
they are intimately connected.
Two measurable spaces (Ω, B) and (Λ, S) are isomorphic if there exists a measurable function
f : Ω→Λ that is one-to-one and has a measurable inverse f−1. In other words, the inverse image
f −1(λ) of a point λ ∈Λ consists of exactly one point in Ωand the inverse mapping so deﬁned, say
g : Λ →Ω, g(λ) = f −1(λ), is itself a measurable mapping. The function f (or its inverse g) with
these properties is called an isomorphism. An isomorphism between two measurable spaces is thus
an invertible mapping between the two sample spaces that is measurable in both directions.
Two probability spaces (Ω, B, P) and (Λ, S, Q) are isomorphic if there is an isomorphism f : Ω→
Λ between the two measurable spaces (Ω, B) and (Λ, S) with the added property that
Q = Pf and P = Qf −1
that is,
Q(F) = P(f −1(F)); F ∈S
P(G) = Q(f(G)); G ∈B.
Two probability spaces are isomorphic

24
CHAPTER 1. PROBABILITY AND RANDOM PROCESSES
1. if one can ﬁnd for each space a random variable deﬁned on that space that has the other as
its output space, and
2. the random variables can be chosen to be inverses of each other; that is, if the two random
variables are f and g, then f(g(λ)) = λ and g(f(ω)) = ω.
Note that if the two probability spaces (Ω, B, P) and (Λ, S, Q) are isomorphic and f : Ω→Λ is
an isomorphism with inverse g, then the random variable fg deﬁned by fg(λ) = f(g(λ)) is equivalent
to the identity random variable i : Λ →Λ deﬁned by i(λ) = λ.
With the ideas of isomorphic measurable spaces and isomorphic probability spaces in hand,
we now can deﬁne isomorphic dynamical systems. Roughly speaking, two dynamical systems are
isomorphic if one can be coded onto the other in an invertible way so that the coding carries one
transformation into the other , that is, one can code from one system into the other and back again
and coding and transformations commute.
Two dynamical systems (Ω, B, P, S) and (Λ, S, m, T) are isomorphic if there exists an isomor-
phism f : Ω→Λ such that
Tf(ω) = f(Sω); ω ∈Ω.
If the probability space (Λ, S, m) is the sequence space of a directly given random process, T is the
shift on this space, and Π0 the sampling function on this space, then the random process Π0T n = Πn
deﬁned on (Λ, S, m) is equivalent to the random process Π0(fSn) deﬁned on the probability space
(Ω, B, P). More generally, any random process of the form gT n deﬁned on (Λ, S, m) is equivalent to
the random process g(fSn) deﬁned on the probability space (Ω, B, P). A similar conclusion holds
in the opposite direction. Thus, any random process that can be deﬁned on one dynamical system
as a function of transformed points possesses an equivalent model in terms of the other dynamical
system and its transformation. In addition, not only can one code from one system into the other,
one can recover the original sample point by inverting the code.
The binary example introduced at the beginning of this section is easily seen to meet this deﬁni-
tion of sameness. Let B(Z(n)) denote the σ-ﬁeld of subsets of Z(n) comprising all possible subsets
of Z(n) (the power set of Z(n)). The described mapping of binary pairs or members of Z(1)2 into
Z(3) induces a mapping f : Z(1)Z
+ →Z(3)Z
+ mapping binary sequences into quaternary sequences.
This mapping is easily seen to be invertible (by construction) and measurable (use the good sets
principle and focus on rectangles). Let T be the shift on the binary sequence space and S be the
shift on the quaternary sequence space; then the dynamical systems (Z(1)Z
+, B(Z(1))Z
+, m, T 2) and
(Z(3)Z
+, B(Z(3))Z
+, mf, T) are isomorphic; that is, the quaternary model with an ordinary shift is
isomorphic to the binary model with the two-shift that shifts symbols a pair at a time.
Isomorphism will often provide a variety of equivalent models for random processes.
Unlike
the previous notion of equivalence, however, isomorphism will often yield dynamical systems that
are decidedly diﬀerent, but that are isomorphic and hence produce equivalent random processes by
coding.

Chapter 2
Standard alphabets
It is desirable to develop a theory under the most general possible assumptions. Random process
models with very general alphabets are useful because they include all conceivable cases of practical
importance. On the other hand, considering only the abstract spaces of the previous chapter can
result in both weaker properties and more complicated proofs. Restricting the alphabets to possess
some structure is necessary for some results and convenient for others. Ideally, however, we can focus
on a class of alphabets that both possesses useful structure and still is suﬃciently general to well
model all examples likely to be encountered in the real world. Standard spaces are a candidate for
this goal and are the topic of this chapter and the next. In this chapter we focus on the deﬁnitions
and properties of standard spaces, leaving the more complicated demonstration that speciﬁc spaces
are standard to the next chapter. The reader in a hurry can skip the next chapter. The theory
of standard spaces is usually somewhat hidden in theories of topological measure spaces. Standard
spaces are related to or include as special cases standard Borel spaces, analytic spaces, Lusin spaces,
Suslin spaces, and Radon spaces. Such spaces are usually deﬁned by their relation via a mapping to
a complete separable metric space, a topic to be introduced in Chapter 3. Good supporting texts
are Parthasarathy [55], Christensen [10], Schwartz [62], Bourbaki [7], and Cohn [12], and the papers
by Mackey [46] and Bjornsson [5].
The presentation here diﬀers from the traditional one in that we focus on the fundamental
properties of standard spaces purely from a probability theory viewpoint and postpone introduction
of the topological and metric space ideas until later. As a result, we can deﬁne standard spaces by
the properties that we will need and not by an isomorphism to a particular kind of probability space
with a metric space alphabet. This provides a shorter route to a description of such a space and
to its basic properties. The next chapter will show that indeed such topological probability spaces
satisfy the required properties, but we will also see in this chapter that certain simple spaces also
meet the criteria.
2.1
Extension of Probability Measures
We shall often wish to construct probability measures and sources by specifying the values of the
probability of certain simple events and then by ﬁnding a probability measure on the σ-ﬁeld con-
taining these events. It was shown in Section 1.5 that if one has a set function meeting the required
conditions of a probability measure on a ﬁeld, then there is a unique extension of that set function to
a consistent probability measure on the σ-ﬁeld generated by the ﬁeld. Unfortunately, the extension
theorem is often diﬃcult to apply since countable additivity or continuity of a set function can be
25

26
CHAPTER 2. STANDARD ALPHABETS
diﬃcult to prove, even on a ﬁeld. Nonnegativity, normalization, and ﬁnite additivity are, however,
usually quite simple to test. Because several of the results to be developed will require such exten-
sions of ﬁnitely additive measures to countably additive measures, we shall develop in this chapter
a class of alphabets for which such extension will always be possible.
To apply the Carath´eodory extension theorem, we will ﬁrst have to pin down a candidate prob-
ability measure on a generating ﬁeld. In most such constructions we will be able at best to force
a set function to be nice on a countable collection of sets. Hence we will focus on σ-ﬁelds that are
countably generated, that is, for which there is a countable collection of sets G that generates the
σ-ﬁeld. Say that we have a σ-ﬁeld B = σ(G) for some countable class G. Let F(G) denote the ﬁeld
generated by G, that is, the smallest ﬁeld containing G. Unlike σ-ﬁelds, there is a simple constructive
deﬁnition of the ﬁeld generated by a class: F(G) consists exactly of all elements of G together with
all sets obtainable from ﬁnite sequences of set theoretic operations on G. Thus if G is countable, so
is F(G). It is easy to show using the good sets principle that if B = σ(G), then also B = σ(F(G))
and hence if B is countably generated, then it is generated by a countable ﬁeld.
Our goal is to ﬁnd countable generating ﬁelds which have the property that every nonnegative,
normalized, ﬁnitely additive set function on the ﬁeld is also countably additive on the ﬁeld (and
hence will have a unique extension to a probability measure on the σ-ﬁeld generated by the ﬁeld).
We formalize this property in a deﬁnition:
A ﬁeld F is said to have the countable extension property if it has a countable number of elements
and if every set function P satisfying (1.4), (1.5), and (1.7) on F also satisﬁes (1.8) on F.
A
measurable space (Ω, B) is said to have the countable extension property if B = σ(F) for some ﬁeld
F with the countable extension property.
Thus a measurable space has the countable extension property if we can ﬁnd a countable gener-
ating ﬁeld such that all normalized, nonnegative, ﬁnitely additive set functions on the ﬁeld extend
to a probability measure on the full σ-ﬁeld. This chapter is devoted to characterizing those ﬁelds
and measurable spaces possessing the countable extension property and to prove one of the most
important results for such spaces–the Kolmogorov extension theorem. We also develop some sim-
ple properties of standard spaces in preparation for the next chapter, where we develop the most
important and most general known class of such spaces.
2.2
Standard Spaces
As a ﬁrst step towards characterizing ﬁelds with the countable extension property, we consider the
special case of ﬁelds having only a ﬁnite number of elements. Such ﬁnite ﬁelds trivially possess the
countable extension property. We shall then proceed to construct a countable generating ﬁeld from
a sequence of ﬁnite ﬁelds and we will determine conditions under which the limiting ﬁeld will inherit
the extendibility properties of the ﬁnite ﬁelds.
Let F = {Fi, i = 0, 1, 2, . . . , n −1} be a ﬁnite ﬁeld of a sample space Ω, that is, F is a ﬁnite
collection of sets in Ωthat is closed under ﬁnite set theoretic operations. Note that F is trivially
also a σ-ﬁeld. F itself can be represented as the ﬁeld generated by a more fundamental class of sets.
A set F in F will be called an atom if its only subsets that are also ﬁeld members are itself and
the empty set, that is, it cannot be broken up into smaller pieces that are also in the ﬁeld. Let A
denote the collection of atoms of F. Clearly there are fewer than n atoms. It is easy to show that
A consists exactly of all nonempty sets of the form
n−1
\
i=0
F ∗
i

2.2. STANDARD SPACES
27
where F ∗
i is either Fi or F c
i . In fact, let us call such sets intersection sets and observe that any
two intersection sets must be disjoint since for at least one i one intersection set must lie inside Fi
and the other within F c
i . Thus all intersection sets must be disjoint. Next observe that any ﬁeld
element can be written as a ﬁnite union of intersection sets–just take the union of all intersection
sets contained in the given ﬁeld element. Let G be an atom of F. Since it is an element of F, it is
the union of disjoint intersection sets. There can be only one nonempty intersection set in the union,
however, or G would not be an atom. Hence every atom is an intersection set. Conversely, if G is an
intersection set, then it must also be an atom since otherwise it would contain more than one atom
and hence contain more than one intersection set, contradicting the disjointness of the intersection
sets.
In summary, given any ﬁnite ﬁeld F of a space Ωwe can ﬁnd a unique collection of atoms A of
the ﬁeld such that the sets in A are disjoint, nonempty, and have the entire space Ωas their union
(since Ωis a ﬁeld element and hence can be written as a union of atoms). Thus A is a partition of
Ω. Furthermore, since every ﬁeld element can be written as a union of atoms, F is generated by A
in the sense that it is the smallest ﬁeld containing A. Hence we write F = F(A). Observe that if we
assign nonnegative numbers pi to the atoms Gi in A such that their sum is 1, then this immediately
gives a ﬁnitely additive, nonnegative, normalized set function on F by the formula
P(F) =
X
i:Gi⊂F
pi.
Furthermore, this set function is trivially countably additive since there are only a ﬁnite number of
elements in F.
The next step is to consider an increasing sequence of ﬁnite ﬁelds that in the limit will give a
countable generating ﬁeld. A sequence of ﬁnite ﬁelds Fn, n = 1, 2, . . . is said to be increasing if the
elements of each ﬁeld are also elements of all of the ﬁelds with higher indices, that is, if Fn ⊂Fn+1,
all n. This implies that if An are the corresponding collections of atoms, then the atoms in An+1
are formed by splitting up the atoms in An. Given an increasing sequence of ﬁelds Fn, deﬁne the
limit F as the union of all of the elements in all of the Fn, that is,
F =
∞
[
i=1
Fi.
F is easily seen to be itself a ﬁeld. For example, any F ∈F must be in Fn for some n, hence
the complement F c must also be in Fn and hence also in F. Similarly, any ﬁnite collection of sets
Fn, n = 1, 2, . . . , m must all be contained in some Fk for suﬃciently large k. The latter ﬁeld must
hence contain the union and hence so must F. Thus we can think of the increasing sequence Fn as
increasing to a limit ﬁeld F and we write
Fn ↑F
if Fn ⊂Fn+1, all n, and F is the union of all of the elements of the Fn. Note that F has by
construction a countable number of elements. When Fn ↑F, we shall say that the sequence Fn
asymptotically generates F.
Lemma 2.2.1 Given any countable ﬁeld F of subsets of a space Ω, then there is a sequence of
ﬁnite ﬁelds {Fn; n = 1, 2, . . .} that asymptotically generates F. In addition, the sequence can be
constructed so that the corresponding sets of atoms An of the ﬁeld Fn can be indexed by a subset of
the set of all binary n-tuples, that is, An = {Gun, un ∈B}, where B is a subset of {0, 1}n, with the
property that Gun ⊂Gun−1. Thus if un is a preﬁx of um for m > n, then Gum is contained in Gun.
(We shall refer to such a sequence of atoms of ﬁnite ﬁelds as a binary indexed sequence.)

28
CHAPTER 2. STANDARD ALPHABETS
Proof: Let F = {Fi, i = 0, 1, . . .}. Consider the sequence of ﬁnite ﬁelds deﬁned by Fn = F(Fi, i =
0, 1, . . . , n −1), that is, the ﬁeld generated by the ﬁrst n elements in F. The sequence is increasing
since the generating classes are increasing. Any given element in F is eventually in one of the Fn and
hence the union of the Fn contains all of the elements in F and is itself a ﬁeld, hence it must contain
F. Any element in the union of the Fn, however, must be in an Fn for some n and hence must be an
element of F. Thus the two ﬁelds are the same. A similar argument to that used above to construct
the atoms of an arbitrary ﬁnite ﬁeld will demonstrate that the atoms in F(F0, . . . , Fn−1) are simply
all nonempty sets of the form Tn−1
k=0 F ∗
k , where F ∗
k is either Fk or F c
k. For each such intersection set
let un denote the binary n-tuple having a one in each coordinate i for which F ∗
i = Fi and zeros in
the remaining coordinates and deﬁne Gun as the corresponding intersection set. Then each Gun is
either an atom or empty and all atoms are obtained as u varies through the set {0, 1}n of all binary
n-tuples. By construction,
ω ∈Gun if and only if ui = 1Fi(ω), i = 0, 1, . . . , n −1,
(2.1)
where for any set F 1F (ω) is the indicator function of the set F, that is,
1F (ω) =
½
1
ω ∈F
0
ω ̸∈F .
Eq.2.1 implies that for any n and any binary n-tuple un that Gun ⊂Gun−1, completing the proof.
2
Before proceeding to further study sequences of ﬁnite ﬁelds, we note that the construction of the
binary indexed sequence of atoms for a countable ﬁeld presented above provides an easy means of
determining which atoms contain a given sample point. For later use we formalize this notion in a
deﬁnition.
Given an enumeration {Fn; n = 0, 1, . . .} of a countable ﬁeld F of subsets of a sample space Ω
and the single-sided binary sequence space
M = {0, 1}Z+ = ×∞
i=0{0, 1},
deﬁne the canonical binary sequence function f : Ω→M by
f(ω) = {1Fi(ω) ; i = 0, 1, . . .}.
(2.2)
Given an enumeration of a countable ﬁeld and the corresponding binary indexed set of atoms An =
{Gun} as above, then for any point ω ∈Ωwe have that
ω ∈Gf(ω)n, n = 1, 2, . . . ,
(2.3)
where f(ω)n denotes the binary n-tuple comprising the ﬁrst n symbols in f(ω).
Thus the sequence of decreasing atoms containing a given point ω can be found as a preﬁx of
the canonical binary sequence function. Observe, however, that f is only an into mapping, that is,
some binary sequences may not correspond to points in Ω. In addition, the function may be many
to one, that is, diﬀerent points in Ωmay yield the same binary sequences.
Unfortunately, the sequence of ﬁnite ﬁelds converging upward to a countable ﬁeld developed
above does not have suﬃcient structure to guarantee that probability measures on the ﬁnite ﬁelds
will imply a probability measure on the limit ﬁeld. The missing item that will prove to be the key
is speciﬁed in the following deﬁnitions:
A sequence of ﬁnite ﬁelds Fn, n = 0, 1, . . ., is said to be a basis for a ﬁeld F if it has the following
properties:

2.2. STANDARD SPACES
29
1. It asymptotically generates F, that is,
Fn ↑F.
(2.4)
2. If Gn is a sequence of atoms of Fn such that if Gn ∈An and Gn+1 ⊂Gn , n = 0, 1, 2, . . ., then
∞
\
n=1
Gn ̸= ∅.
(2.5)
A basis for a ﬁeld is an asymptotically generating sequence of ﬁnite ﬁelds with the property that a
decreasing sequence of atoms cannot converge to the empty set. The property (2.4) of a sequence
of ﬁelds is called the ﬁnite intersection property. The name comes from the similar use in topology
and reﬂects the fact that if all ﬁnite intersections of the sequence of sets Gn are not empty (which
is true since the Gn are a decreasing sequence of nonempty sets), then the intersection of all of the
Gn cannot be empty.
A sequence Fn, n = 1, . . . is said to be a basis for a measurable space (Ω, B) if the {Fn} form a
basis for a ﬁeld that generates B.
If the sequence Fn ↑F and F generates B, that is, if
B = σ(F) = σ(
∞
[
n=1
Fn),
then the Fn are said to asymptotically generate the σ-ﬁeld B.
A ﬁeld F is said to be standard if it possesses a basis. A measurable space (Ω, B) is said to be
standard if B can be generated by a standard ﬁeld, that is, if B possesses a basis.
The requirement that a σ-ﬁeld be generated by the limit of a sequence of simple ﬁnite ﬁelds is a
reasonably intuitive one if we hope for the σ-ﬁeld to inherit the extendibility properties of the ﬁnite
ﬁelds. The second condition — that a decreasing sequence of atoms has a nonempty limit — is less
intuitive, however, and will prove harder to demonstrate. Although nonintuitive at this point, we
shall see that the existence of a basis is a suﬃcient and necessary condition for extending arbitrary
ﬁnitely additive measures on countable ﬁelds to countably additive measures.
The proof that the standard property is suﬃcient to ensure that any ﬁnitely additive set function
is also countably additive requires additional machinery that will be developed later. The proof of
necessity, however, can be presented now and will perhaps provide some insight into the standard
property by showing what can go wrong in its absence.
Lemma 2.2.2 Let F be a ﬁeld of subsets of a space Ω. A necessary condition for F to have the
countable extension property is that it be standard, that is, that it possess a basis.
Proof: We assume that F does not possess a basis and we construct a ﬁnitely additive set function
that is not continuous at ∅and hence not countably additive. To have the countable extension
property, F must be countable. From Lemma 2.2.1 we can construct a sequence of ﬁnite ﬁelds Fn
such that Fn ↑F. Since F does not possess a basis, we know that for any such sequence Fn there
must exist a decreasing sequence of atoms Gn of Fn such that Gn ↓∅. Deﬁne set functions Pn on
Fn as follows: If Gn ⊂F, then Pn(F) = 1, if F ∩Gn = ∅, then Pn(F) = 0. Since F ∈Fn, F either
wholly contains the atom Gn or F and Gn are disjoint, hence the Pn are well deﬁned. Next deﬁne
the set function P on the limit ﬁeld F in the natural way: If F ∈F, then F ∈Fn for some smallest
value of n (e.g., if the Fn are constructed as before as the ﬁeld generated by the ﬁrst n elements of F,
then eventually every element of the countable ﬁeld F must appear in one of the Fn). Thus we can

30
CHAPTER 2. STANDARD ALPHABETS
set P(F) = Pn(F). By construction, if m ≥n then also Pm(F) = Pn(F) and hence P(F) = Pn(F)
for any n such that F ∈Fn. P is obviously nonnegative, P(Ω) = 1 (since all of the atoms Gn in
the given sequence are in the sample space), and P is ﬁnitely additive. To see the latter fact, let Fi,
i = 1, . . . , m be a ﬁnite collection of disjoint sets in F. By construction, all must lie in some ﬁeld
Fn for suﬃciently large n. If Gn lies in one of the sets Fi (it can lie in at most one since the sets
are disjoint), then (1.7) holds with both sides equal to one. If none of the sets contains Gn, then
(1.7) holds with both sides equal to zero. Thus P satisﬁes (1.4), (1.5), and (1.7). To prove P to be
countably additive, then, we must verify (1.8). By construction P(Gn) = Pn(Gn) = 1 for all n and
hence
lim
n→∞P(Gn) = 1 ̸= 0,
and therefore (1.8) is violated since by assumption the Gn decrease to the empty set ∅which has
zero probability. Thus P is not continuous at ∅and hence not countably additive.
If a ﬁeld is not standard, then ﬁnitely additive probability measures that are not countably
additive can always be constructed by putting probability on a sequence of atoms that collapses down
to nothing. Thus there can always be probability on ever smaller sets, but the limit cannot support
the probability since it is empty. Thus the necessity of the standard property for the extension of
arbitrary additive probability measures justiﬁes its candidacy for a general, useful alphabet.
Corollary 2.2.1 A necessary condition for a countably generated measurable space (Ω, B) to have
the countable extension property is that it be a standard space.
Proof: To have the countable extension property, a measurable space must have a countable gen-
erating ﬁeld. If the measurable space is not standard, then no such ﬁeld can possess a basis and
hence no such ﬁeld will possess the countable extension property. In particular, one can always ﬁnd
as in the proof of the lemma a generating ﬁeld and an additive set function on that ﬁeld which is
not countably additive and hence does not extend.
2
Exercises
1. A class of subsets V of A is said to be separating if given any two points x, y ∈A, there is a
V ∈V that contains only one of the two points and not the other. Suppose that a separable σ-
ﬁeld B has a countable generating class V = {Vi; i = 1, 2, . . .} that is also separating. Describe
the intersection sets
∞
\
n=1
V ∗
n ,
where V ∗
n is either Vn or V c
n.
2.3
Some properties of standard spaces
The following results provide some useful properties of standard spaces. In particular, they show
how certain combinations of or mappings on standard spaces yield other standard spaces. These
results will prove useful for demonstrating that certain spaces are indeed standard. The ﬁrst result
shows that if we form a product space from a countable number of standard spaces as in Section 1.4,
then the product space is also standard. Thus if the alphabet of a source or random process for one
sample time is standard, then the space of all sequences produced by the source is also standard.

2.3. SOME PROPERTIES OF STANDARD SPACES
31
Lemma 2.3.1 Let Fi, i ∈I, be a family of standard ﬁelds for some countable index set I. Let F
be the product ﬁeld generated by all rectangles of the form F = {xI : xi ∈Fi, i ∈M}, where Fi ∈Fi
all i and M is any ﬁnite subset of I. That is,
F = F(rect(Fi, i ∈I)),
then F is also standard.
Proof: Since I is countable, we may assume that I = {1, 2, . . .}. For each i ∈I, Fi is standard and
hence possesses a basis, say {Fi(n), n = 1, 2, . . .}. Consider the sequence
Gn = F(rect(Fi(n), i = 1, 2, . . . , n)),
(2.6)
that is, Gn is the ﬁeld of subsets formed by taking all rectangles formed from the nth order basis
ﬁelds Fi(n), i = 1, . . . , n in the ﬁrst n coordinates. The lemma will be proved by showing that Gn
forms a basis for the ﬁeld F and hence the ﬁeld is standard. The ﬁelds Gn are clearly ﬁnite and
increasing since the coordinate ﬁelds are. The ﬁeld generated by the union of all the ﬁelds Gn will
contain all of the rectangles in F since for each i the union in that coordinate contains the full
coordinate ﬁeld Fi. Thus Gn ↑F. Say we have a sequence Gn of atoms of Gn (Gn ∈Gn for all n)
decreasing to the null set. Each such atom must have the form
Gn = {xI : xi ∈Gn(i); i = 1, 2, . . . , n}.
where Gn(i) is an atom of the coordinate ﬁeld Fi(n).
For Gn ↓∅, however, this requires that
Gn(i) ↓∅at least for one i, violating the deﬁnition of a basis for the ith coordinate. Thus Fn must
be a basis.
2
Corollary 2.3.1 Let (Ai, Bi), i ∈I, be a family of standard spaces for some countable index set I.
Let (A, B) be the product measurable space, that is,
(A, B) = ×i∈I(Ai, Bi) = (×i∈IAi, ×i∈IBi),
where
×i∈IAi = all xI = {(xi, i ∈I) : xi ∈Ai ; i ∈I}
is the cartesian product of the alphabets Ai, and
×i∈IBi = σ(rect(Bi, i ∈I)).
Then (A, B) is standard.
Proof: Since each (Ai, Bi), i ∈I, is standard, each possesses a basis, say {Fi(n); n = 1, 2, . . .}, which
asymptotically generates a coordinate ﬁeld Fi which in turn generates Bi. (Note that these are not
the same as the Fi(n) of (2.6).) From Lemma 2.3.1 the product ﬁeld of the nth order basis ﬁelds
given by (2.6) is a basis for F. Thus we will be done if we can show that F generates B. It is an
easy exercise, however, to show that if Fi generates Bi, all i ∈I, then
B = σ(rect(Bi, i ∈I)) =
σ(rect(Fi, i ∈I)) = σ(F(rect(Fi, i ∈I))) = σ(F).
(2.7)
2

32
CHAPTER 2. STANDARD ALPHABETS
We now turn from cartesian products of diﬀerent standard spaces to certain subspaces of standard
spaces. First, however, we need to deﬁne subspaces of measurable spaces.
Let (A, B) be a measurable space (not necessarily standard). For any F ∈B and any class of
sets G deﬁne the new class G ∩F = {all sets of the form G ∩F for some G ∈G}. Given a set F ∈B
we can deﬁne a measurable space (F, B ∩F) called a subspace of (A, B). It is easy to verify that
B ∩F is a σ-ﬁeld of subsets of F. Intuitively, it is the full σ-ﬁeld B t downo the set F.
We cannot show in general that arbitrary subspaces of standard spaces are standard. One case
where they are is described next.
Lemma 2.3.2 Suppose that (A, B) is standard with a basis Fn, n = 1, 2, . . .. If F is any countable
union of members of the Fn, that is, if
F =
∞
[
i=1
Fi; Fi ∈
∞
[
n=1
Fn,
then the space (A −F, B ∩(A −F)) is also standard.
Proof: The basic idea is that if you take a standard space with a basis and remove some atoms, the
original basis with these atoms removed still works since decreasing sequences of atoms must still
be empty for some ﬁnite n or have a nonempty intersection. Given a basis Fn for (A, B), form a
candidate basis Gn = Fn ∩F c. This is essentially the original basis with all of the atoms in the sets
in F removed. Let us call the countable collection of basis ﬁeld atoms in F “bad atoms.” From the
good sets principle, the Gn asymptotically generate B ∩F c. Suppose that Gn ∈Gn is a decreasing
sequence of nonempty atoms. By construction Gn = An ∩F c for some sequence of atoms An in
Fn. Since the original space is standard, the intersection T∞
n=1 An is nonempty, say some set A.
Suppose that A ∩F c is empty, then A ⊂F and hence Aunder must be contained in one of the bad
atoms, say A ⊂Bm ∈Fm. But the atoms of Fm are disjoint and A ⊂Am with Am ∩Bm = ∅, a
contradiction unless Am is empty in violation of the assumptions.
2
Next we consider spaces isomorphic to standard spaces. Recall that given two measurable spaces
(A, B) and (B, S), an isomorphism f is a measurable function f : A →B that is one-to-one and
has a measurable inverse f −1. Recall also that if such an isomorphism exists, the two measurable
spaces are said to be isomorphic. We ﬁrst provide a useful technical lemma and then show that if
two spaces are isomorphic, then one is standard if and only if the other one is standard.
Lemma 2.3.3 If (A, B) and (B, S) are isomorphic measurable spaces, let f : A →B denote an
isomorphism. Then B = f −1(S) = { all sets of the form f −1(G) for G ∈S}.
Proof: Since f is measurable, the class of sets F = f −1(S) has all its elements in B. Conversely,
since f is an isomorphism, its inverse function, say g, is well deﬁned and measurable. Thus if F ∈B,
then g−1(F) = f(F) = D ∈S and hence F = f−1(D). Thus every element of B is given by f−1(D)
for some D in S. In addition, F is a σ-ﬁeld since the inverse image operation preserves all countable
set theoretic operations. Suppose that there is an element D in B that is not in F. Let g : B →A
denote the inverse operation of f. Since g is measurable and one-to-one, g−1(D) = f(D) is in G and
therefore its inverse image under f, f −1(f(D)) = D, must be in F = f−1(G) since f is measurable.
But this is a contradiction since D was assumed to not be in F and hence F = B.
2
Corollary 2.3.2 If (A, B) and (B, S) are isomorphic, then (A, B) is standard if and only if (B, S)
is standard.
Proof:
Say that (B, S) is standard and hence has a basis Gn, n = 1, 2, . . . and that G is the
generating ﬁeld formed by the union of all of the Gn. Let f : A →B denote an isomorphism and

2.4. SIMPLE STANDARD SPACES
33
deﬁne Fn = f −1(Gn), n = 1, 2, . . . Since inverse images preserve set theoretic operations, the Fn are
increasing ﬁnite ﬁelds with S Fn = f −1(G). A straightforward application of the good sets principle
show that if G generates S and B = f−1(S), then f−1(G) = F also generates B and hence the Fn
asymptotically generate B. Given any decreasing set of atoms, say Dn, n = 1, 2, . . ., of Fn, then
f(Dn) = g−1(Dn) must also be a decreasing sequence of atoms of Gn (again, since inverse images
preserve set theoretic operations). The latter sequence cannot be empty since the Gn form a basis,
hence neither can the former sequence since the mappings are one-to-one. Reversing the argument
shows that if either space is standard, then so is the other, completing the proof.
2
As a ﬁnal result of this section we show that if a ﬁeld is standard, then we can always ﬁnd a
basis that is a binary indexed sequence of ﬁelds in the sense of Lemma 2.2.1. Indexing the atoms by
binary vectors will prove a simple and useful representation.
Lemma 2.3.4 If a ﬁeld F is standard, then there exists a binary indexed basis and a corresponding
canonical binary sequence function.
Proof: The proof consists simply of relabeling of a basis in binary vectors. Since F is standard,
then it possesses a basis, say Fn, n = 0, 1, . . .. Use this basis to enumerate all of the elements of
F in order, that is, ﬁrst list the elements of F0, then the elements of F1, and so on. This produces
a sequence {Fn, n = 0, 1, . . .} = F. Now construct a corresponding binary indexed asymptotically
generating sequence of ﬁelds, say Gn, n = 0, 1, . . ., as in Lemma 2.2.1. To prove Gn is a basis we
need only show that it has the ﬁnite intersection property of (2.4). First observe that for every n
there is an m ≥n such that Fn = Gm. That is, eventually the binary indexed ﬁelds Gm match
the original ﬁelds, they just take longer to get all of the elements since they include them one new
one at a time. If Gn is a decreasing sequence of atoms in Gn, then there is a sequence of integers
n(k) ≥k, k = 1, 2, . . ., such that Gn(k) = Bk, where Bk is an atom of Fk. If Gn collapses to the
empty set, then so must the subsequence Gn(k) and hence the sequence of atoms Bk of Fk. But this
is impossible since the Fk are a basis. Thus the Gn must also possess the ﬁnite intersection property
and be a basis. The remaining conclusion follows immediately from Lemma 2.2.1 and the deﬁnition
that follows it.
2
2.4
Simple standard spaces
First consider a measurable space (A, B) with A a ﬁnite set and B the class of all subsets of A. This
space is trivially standard–simply let all ﬁelds in the basis be B. Alternatively one can observe that
since B is ﬁnite, the space trivially possesses the countable extension property and hence must be
standard.
Next assume that (A, B) has a countable alphabet and that B is again the class of all subsets
of A. A natural ﬁrst guess for a basis is to ﬁrst enumerate the points of A as {a0, a1, . . .} and to
deﬁne the increasing sequence of ﬁnite ﬁelds Fn = F({a0}, . . . , {an−1} ). The sequence is indeed
asymptotically generating, but it does not have the ﬁnite intersection property. The problem is that
each ﬁeld has as an atom the leftover or garbage set Gn = {an, an+1, . . .} containing all of the points
of A not yet included in the generating class. This sequence of atoms is nonempty and decreasing,
yet it converges to the empty set and hence Fn is not a basis.
In this case there is a simple ﬁx. Simply adjoin the undesirable atoms Gn containing the garbage
to a good atom, say a0, to form a new sequence of ﬁnite ﬁelds Gn = F(b0(n), b1(n), . . . , bn−1(n))
with b0(n) = a0 ∪Gn, bi(n) = ai, i = 1, . . . , n −1. This sequence of ﬁelds also generates the full
σ-ﬁeld (note for example that a0 = T∞
n=0 b0(n)). It also possesses the ﬁnite intersection property
and hence is a basis. Thus any countable measurable space is standard.

34
CHAPTER 2. STANDARD ALPHABETS
The above example points out that it is easy to ﬁnd sequences of asymptotically generating ﬁnite
ﬁelds for a standard space that are not bases and hence which cannot be used to extend arbitrary
ﬁnitely additive set functions. In this case, however, we were able to rig the ﬁeld so that it did
provide a basis.
From Corollary 2.3.1 any countable product of standard spaces must be standard. Thus products
of ﬁnite or countable alphabets give standard spaces, a conclusion summarized in the following
lemma.
Lemma 2.4.1 Given a countable index set I, if Ai are ﬁnite or countably inﬁnite alphabets and Bi
the σ-ﬁelds of all subsets of Ai for i ∈I, then
×i∈I(Ai, Bi)
is standard. Thus, for example, the following spaces are standard:
(N, BN) = (ZZ+
+ , BZ+
Z+),
(2.8)
where Z+ = {0, 1, . . .} and BZ+ is the σ-ﬁeld of all subsets of Z+ (the power set) of Z+), and
(M, BM) = ({0, 1}Z+, B{0,1}Z+),
(2.9)
where B{0,1} is the σ-ﬁeld of all subsets of {0, 1}.
The proof of Lemma 2.3.1 coupled with the discussion for the ﬁnite and countable cases pro-
vide a speciﬁc construction for the bases of the above two processes. For the case of the binary
sequence space M, the ﬁelds Fn consist of n dimensional rectangles of the form {x : xi ∈Bi, i =
0, 1, . . . , n −1}, Bi ⊂{0, 1} all i. These rectangles are themselves ﬁnite unions of simpler rectangles
of the form
c(bn) = {x : xi = bi; i = 0, 1, . . . , n −1}
(2.10)
for bn ∈{0, 1}n. These sets are called thin cylinders and are the atoms of Fn. The thin cylinders
generate the full σ-ﬁeld and any decreasing sequence of such rectangles converges down to a single
binary sequence.
Hence it is easy to show that this space is standard.
Observe that this also
implies that individual binary sequences x = {xi; i = 0, 1, . . .} are themselves events since they are
a countable intersection of thin cylinders. For the case of the integer sequence space N, the ﬁeld
Fn contains all rectangles of the form {x : xi ∈Fi, i = 0, 1, . . . , n}, where Fi ∈Fi(n) for all i, and
Fi(n) = {b0(n), b1(n), . . . , bn−1(n)} for all i with b0(n) = {0, n, n + 1, n + 2, . . .} and bj = {j} for
j = 1, 2, . . . , n −1.
As previously mentioned, it is not true in general that an arbitrary subset of a standard space
is standard, but the previous construction of a basis combined with Lemma 2.3.2 does provide a
means of showing that certain subspaces of M and N are standard.
Lemma 2.4.2 Given the binary sequence space M or the integer sequence space N, if we remove
from these spaces any countable collection of rectangles, then the remaining space is standard.
Proof: The rectangles are eventually in the ﬁelds of the basis and the result follows from Lemma
2.3.2.
2
A natural next step would be to look at the unit interval or the real line as alphabets. We ﬁrst
need, however, to construct appropriate σ-ﬁelds for these cases. Little eﬀort is saved by focusing on
these special cases instead of the general case of metric spaces, which will be introduced in the next
section and considered in depth in the next chapter.

2.5. METRIC SPACES
35
Exercises
1. Consider the space of (2.9) of all binary sequences and the corresponding event space. Is the
class of all ﬁnite unions of thin cylinders a ﬁeld? Show that a decreasing sequence of nonempty
atoms of Fn must converge to a point, i.e., a single binary sequence.
2.5
Metric Spaces
In this section we collect together several deﬁnitions, examples, and elementary convergence prop-
erties of metric spaces that are required to prove the fundamental extension theorem for standard
spaces.
A set A with elements called points is called a metric space if for every pair of points a, b in A
there is an associated nonnegative real number d(a, b) such that
d(a, b) = 0 if and only if a ̸= b,
(2.11)
d(a, b) = d(b, a) (symmetry),
(2.12)
d(a, b) ≤d(a, c) + d(c, b) all c ∈A (triangle inequality) .
(2.13)
The function d : A × A →[0, ∞) is called a metric or distance. If d(a, b) = 0 does not necessarily
imply that a = b, then d is called a pseudo-metric and A is a pseudo-metric space.
Metric spaces provide mathematical models of many interesting alphabets.
Several common
examples are listed below.
Example 2.5.1: A a discrete (ﬁnite or countable) set with metric
d(a, b) = dH(a, b) = 1 −δa,b,
where δa,b is the Kronecker delta function which is 1 if a = b and 0 otherwise. This metric is called
the Hamming distance and assigns a distance of 0 between a letter and itself and 1 between any two
distinct letters.
Example 2.5.2: A = Z(M) = {0, 1, . . . , M −1} with metric d(a, b) = dL(a, b) = |a −b| mod M,
where k mod M = r in the unique representation k = aM + r, a an integer, 0 ≤r ≤M −1. dL is
called the Lee metric or circular distance.
Example 2.5.3: A0 a discrete (ﬁnite or countable) set and A = An
0 (the space of all n-dimensional
vectors with coordinates in A0) and metric
d(xn, yn) = n−1
n−1
X
i=0
dH(xi, yi),
the arithmetic average of the Hamming distances in the coordinates. This distance is also called the
Hamming distance, but the name is ambiguous since the Hamming distance in the sense of Example
2.5.1 applied to A can take only values of 0 and 1 while the distance of this example can take on
values of i/n; i = 0, 1, . . . , n −1. Better names for this example are the average Hamming distance
or mean Hamming distance. This distance is useful in error correcting and error detecting coding
examples as a measure of the change in digital codewords caused by noisy communication media.
Example 2.5.4: A as in the preceding example. Given two elements xn and yn of A, an ordered
set of integers K = {(ki, ji); i = 1, . . . , r} is called a match of xn and yn if xki = yji for i = 1, 2, . . . , r.
The number r = r(K) is called the size of the match K. Let K(xn, yn) denote the collection of all

36
CHAPTER 2. STANDARD ALPHABETS
matches of xn and yn. The best match size mn(xn, yn) is the largest r such that there is a match of
size r, that is,
mn(xn, yn) =
max
K∈K(xn,yn) r(K).
The Levenshtein distance λn on A is deﬁned by
λn(xn, yn) = n −mn(xn, yn) =
min
K∈K(xn,yn)(n −r(K)),
that is, it is the minimum number of insertions, deletions, and substitutions required to map one of
the n-tuples into the other [43] [68]. The Levenshtein distance is useful in correcting and detecting
errors in communication systems where the communication medium may not only cause symbol
errors: it can also drop symbols or insert erroneous symbols. Note that unlike the previous example,
the distance between vectors is not the sum of the component distances, that is, it is not additive.
Example 2.5.5: A = ℜ= (−∞, ∞), the real line, and d(a, b) = |a −b|, the absolute magnitude
of the diﬀerence.
Example 2.5.6: A = ℜn, n-dimensional Euclidean space, with
d(an, bn) =
Ãn−1
X
i=0
|ai −bi|2
!1/2
,
the Euclidean distance.
Example 2.5.7: A = ℜn with
d(an, bn) =
max
i=0,...,n−1 |ai −bi|
Example 2.5.8: A normed linear vector space A with norm |.| and distance d(a, b) = |a −b|. A
vector space or linear space A is a space consisting of points called vectors and two operations —
one called addition which associates with each pair of vectors a, b a new vector a + b ∈A and one
called multiplication which associates with each vector a and each number r ∈ℜ(called a scalar) a
vector ra ∈A — for which the usual rules of arithmetic hold, that is, if a, b, c ∈A and r, s ∈ℜ, then
a + b
=
b + a (commutative law)
(a + b) + c
=
a + (b + c) (associative law)
r(a + b)
=
ra + rb (left distributive law)
(r + s)a
=
ra + sa (right distributive law)
(rs)a
=
r(sa) (associative law for multiplication
1a
=
a.
In addition, it is assumed that there is a zero vector, also denoted 0, for which
a + 0 = a
0a = 0.
We also deﬁne the vector −a = (−1)a. A normed linear space is a linear vector space A together
with a function ||a|| called a norm deﬁned for each a such that for all a, b ∈A and r ∈ℜ, ||a|| is
nonnegative and
||a|| = 0 if and only if a = 0,
(2.14)

2.5. METRIC SPACES
37
||a + b|| ≤||a|| + ||b|| (triangle inequality) ,
(2.15)
||ra|| = |r|||a||.
(2.16)
If ||a|| = 0 does not imply that a = 0, then || · || is called a seminorm. For example, the Euclidean
space example above is a normed linear space with norm
||a|| =
Ãn−1
X
i=0
a2
i
!1/2
.
Example 2.5.9: For some dimension k, let P denote the space of all stochastic matrices, that is,
nonnegative matrices whose rows sum to one. (These matrices arise in the study of Markov chains.)
Let ||P||, P ∈P be a matrix norm such as
||P|| = (
X
i,j
P 2
ij)1/2,
the Euclidean norm.
Then P is a normed linear space.
(See, e.g., Paz [56] for discussion and
applications.)
Example 2.5.10: An inner product space A with inner product (·, ·) and distance d(a, b) = ||a−b||,
where ||a|| = (a, a)1/2 is a norm. An inner product space (or pre-Hilbert space) is a linear vector
space A such that for each pair of vectors a, b ∈A there is a real number (a, b) called an inner
product such that for a, b, c ∈A, r ∈ℜ
(a, b)
=
(b, a)
(a + b, c)
=
(a, c) + (b, c)
(ra, b)
=
r(a, b)
(a, a)
≥
0 and (a, a) = 0 if and only if a = 0.
Inner product spaces and normed linear vector spaces include a variety of general alphabets and
function spaces, many of which will be encountered in this book.
Example 2.5.11: Product spaces. Given a metric space A with metric d and an index set I that
is some subset of the integers (e.g., Z+ = {0, 1, 2, . . .}), the cartesian product
AI = ×i∈IAi,
where the Ai are all replicas of A, is a metric space with metric
dI(a, b) =
X
i∈I
2−|i|
d(ai, bi)
1 + d(ai, bi).
The somewhat strange form above ensures that the metric on the sequences in AI is ﬁnite–simply
adding up the component metrics would likely yield a sum that blows up. Dividing by 1 plus the
metric ensures that each term is bounded and the 2−|i| ensure convergence of the inﬁnite sum. In
the special case where the index set I is ﬁnite, then the product space is a metric space with the
simpler metric
d(a, b) =
X
i∈I
d(ai, bi).

38
CHAPTER 2. STANDARD ALPHABETS
Example 2.5.12: And now for something completely diﬀerent: Suppose that (Ω, B, m) is a prob-
ability space. Then we can form a psuedo-metric space A = B with
d(F, G) = m(F∆G).
This is only a pseudo-metric because events which diﬀer on a set of probability 0 yield a distance of
0. It can be considered as a metric space if we consider events to be identical if they diﬀer on a set
of probability zero.
Example 2.5.13: Suppose that (Ω, B, m) is a probability space and let P denote the class of
all measurable partitions of Ωwith K atoms, that is, a member of P has the form P = {Pk,
i = 1, 2, . . . , K}, where the Pi ∈B all i are disjoint. A metric on the space is
d(P, Q) =
K
X
i=1
m(Pi∆Qi).
This metric is called the partition distance.
Convergence
A sequence an, n = 1, 2, . . . of points in a metric space A is said to converge to a point a if
lim
n→∞d(an, a) = 0,
that is, if given ϵ > 0 there is an N such that n ≥N implies that d(an, a) ≤ϵ. If an converges to a,
then we write a = limn→∞an or an →n→∞a. A metric space A is said to be sequentially compact
if every sequence has a convergent subsequence, that is, if an ∈A, n = 1, 2, . . ., then there is a
subsequence n(k) of increasing integers and a point a in A such that limk→∞an(k) = a. The notion
of sequential compactness is considered somewhat old-fashioned in modern analysis texts, but it is
exactly what we need.
We will need the following two easy results on convergence in product spaces.
Lemma 2.5.1 Given a metric space A with metric d and a countable index set I, let AI denote the
cartesian product with the metric of Example 2.5.11. A sequence {xn}I = {xn,i; i ∈I} converges
to a sequence xI = {xi, i ∈I} if and only if limn→∞xn,i = xi for all i ∈I.
Proof: The metric on the product space of Example 2.5.11 goes to zero if and only if all of the
individual terms of the sum go to zero.
2
Corollary 2.5.1 Let A and AI be as in the lemma. If A is sequentially compact, then so is AI.
Proof: Let xn, n = 1, 2, . . . be a sequence in the product space A. Since the ﬁrst coordinate alphabet
is sequentially compact, we can choose a subsequence, say x1
n, that converges in the ﬁrst coordinate.
Since the second coordinate alphabet is sequentially compact, we can choose a further subsequence,
say x2
n, of x1
n that converges in the second coordinate. We continue in this manner, each time taking
a subsequence of the previous subsequence that converges in one additional coordinate. The so-
called diagonal sequence xn
n will then converge in all coordinates as n →∞and hence by the lemma
will converge with respect to the given product space metric. This method of taking subsequences
of subsequences with desirable properties is referred to as a standard diagonalization.
2
The convergence notions developed in this section yield an alternative characterization of stan-
dard spaces that provides some additional insight into their structure and plays a key role in the
extension to come.

2.5. METRIC SPACES
39
Theorem 2.5.1 A ﬁeld F of subsets of Ωis standard if and only if (a) F is countable, and (b)
if Fn; = 1, 2, . . ., is a sequence of nonempty disjoint elements in F, then S
n Fn ̸∈F. Given (a),
condition (b) is equivalent to the condition (b’) that if Gn; = 1, 2, . . ., is a sequence of strictly
decreasing nonempty elements in F, then T
n Gn ̸= ∅.
Proof: We begin by showing that (b) and (b’) are equivalent. We ﬁrst show that (b) implies (b’).
Suppose that Gn satisfy the hypothesis of (b’). Then
H = (
∞
\
n=1
Gn)c =
∞
[
n=1
Gc
n =
∞
[
n=1
(Gc
n −Gc
n−1),
where G0 = Ω. Deﬁning Fn = Gc
n −Gc
n−1 yields a sequence satisfying the conditions of (a) and
hence H and therefore Hc are not in F, showing that T
n Gn = Hc cannot be empty (or it would
be in F since a ﬁeld must contain the empty set). To prove that (b’) implies (b), suppose that the
sequence Fn satisﬁes the hypothesis of (b) and deﬁne F = S
n Fn and Gn = F −Sn
j=1 Fj. Then Gn
is a strictly decreasing sequence of nonempty sets with empty intersection. From (b’), the sequence
Gn cannot all be in F, which implies that F ̸∈F.
We now show that the equivalent conditions (b) and (b’) hold if and only if the ﬁeld is standard.
First suppose that (b) and (b’) hold and that F = {Fn, n = 1, 2, . . .}. Deﬁne Fn = F(F1, F2, . . . , Fn)
for n = 1, 2, . . .. Clearly F = S∞
n=1 Fn since F contains all of the elements of all of the Fn and each
element in F is in some Fn. Let An be a nonincreasing sequence of nonempty atoms in Fn as in the
hypothesis for a basis. If the An are all equal for large n, then clearly T∞
n=1 An ̸= ∅. If this is not
true, then there is a strictly decreasing subsequence Ank and hence from (b’)
∞
\
n=1
An =
∞
\
k=1
Ank ̸= ∅.
This means that the Fn form a basis for F and hence F is standard.
Lastly, suppose that F is standard with basis Fn, n = 1, 2, . . ., and suppose that Gn is a sequence
of strictly decreasing elements in F as in (b’). Deﬁne the nonincreasing sequence of nonempty sets
Hn ∈Fn as follows: If G1 ∈F1, then set H1 = G1 and set n1 = 1. If this is not the case, then G1
is too “ﬁne” for the ﬁeld F1 so we set H1 = Ω. Continue looking for an n1 such that G1 ∈Fn1.
When it is found (as it must be since G1 is a ﬁeld element), set Hi = Ωfor i < n1 and Hn1 = G1.
Next consider G2. If G2 ∈Fn1+1, then set Hn1+1 = G2. Otherwise set Hn1+1 = G1 and ﬁnd an n2
such that G2 ∈Fn2. Then set Hi = G1 for n1 ≤i < n2 and Hn2 = G2. Continue in this way ﬁlling
in the sequence of Gn with enough repeats to meet the condition. The sequence Hn ∈Fn is thus a
nonempty nonincreasing sequence of ﬁeld elements. Clearly by construction
∞
\
n=1
Hn =
∞
\
n=1
Gn
and hence we will be done if we can prove this intersection nonempty. We accomplish this by assum-
ing that the intersection is empty and show that this leads to a contradiction with the assumption
of a standard space. Roughly speaking, a decreasing set of nonempty ﬁeld elements collapsing to
the empty set must contain a decreasing sequence of nonempty basis atoms collapsing to the empty
set and that violates the properties of a basis.
Pick for each n an ωn ∈Hn and consider the sequence f(ωn) in the binary sequence space M.
The binary alphabet {0, 1} is trivially sequentially compact and hence so is M from Corollary 2.5.1.
Thus there is a subsequence, say f(ωn(k)), which converges to some u ∈M as k →∞. From Lemma

40
CHAPTER 2. STANDARD ALPHABETS
2.5.1 this means that f(ωn(k)) must converge to u in each coordinate as k →∞. Since there are
only two possible values for each coordinate in u, this means that given any positive integer n, there
is an N such that for n(k) ≥N, f(ωn(k))i = ui for i = 0, 1, . . . , n −1, and hence from (2.1) that
ωn(k) ∈Aun, the atom of Fn indexed by un. Assuming that we also choose k large enough to ensure
that n(k) ≥n, then the fact that the ﬁelds are decreasing implies that ωn(k) ∈Hn(k) ⊂Hn. Thus
we must have that Gun is contained in Hn. That is, the point ωn(k) is contained in an atom of Fn(k)
which is itself contained in an atom of Fn. Since the point is also in the set Hn of Fn, the set must
contain the entire atom. Thus we have constructed a sequence of atoms contained in a sequence of
sets that decreases to the empty set, which violates the standard assumption and hence completes
the proof.
2
Exercises
1. Show that in inner product space with norm || · || satisﬁes the parallelogram law
||a + b||2 + ||a −b||2 = 2||a||2 + 2||b||2
2. Show that dI and d of Example 2.5.11 are metrics.
3. Show that the partition distance in the above example satisﬁes
d(P, Q) = 2
X
i̸=j
m(Pi ∩Qj) = 2(1 −
X
i
m(Pi ∩Qi)).
4. Show that if di; i = 0, 1, . . . , n −1 are all metrics on A, then so is d(x, y) = maxi di(x, y).
5. Suppose that d is a metric on A and r is a ﬁxed positive real number. Deﬁne ρ(x, y) to be 0
if d(x, y) ≤r and K if d(x, y) > r. Is ρ a metric? a pseudo-metric?
6. Suppose that X : Ω→A and Y : Ω→A are two random variables deﬁned on a common
probability space (Ω, B, m) and that A is a ﬁnite set {a1, . . . , aK}. Deﬁne the sets Pi = X−1(ai)
and Qi = Y −1(ai), i = 1, 2, . . . , K. Show that the collections of sets P = {Pi} and Q = {Qi}
are partitions of Ω. Let d denote the partition distance of Example 2.5.12. Show that
d(P, Q) = 2m(X ̸= Y ).
2.6
Extension in Standard Spaces
In this section we combine the ideas of the previous two sections to complete the proofs of the
following results.
Theorem 2.6.1 A ﬁeld has the countable extension property if and only if it is standard.
Corollary 2.6.1 A measurable space has the countable extension property if and only if it is stan-
dard.
These results will complete the characterization of those spaces having the desirable extension
property discussed at the beginning of the section. The “only if” portions of the results are contained
in Section 2.2. The corollary follows immediately from the theorem and the deﬁnition of standard.
Hence we need only now show that a standard ﬁeld has the countable extension property.

2.7. THE KOLMOGOROV EXTENSION THEOREM
41
Proof of the theorem: Since F is standard, construct as in Lemma 2.2.1 a basis with a binary
indexed sequence of atoms and let f : Ω→M denote the corresponding canonical binary sequence
function of (2.2). Let P be a nonnegative, normalized, ﬁnitely additive set function of F. We wish
to show that (1.8) is satisﬁed. Let Fn be a sequence of ﬁeld elements decreasing to the null set. If
for some ﬁnite n = N, FN is empty, then we are done since then
lim
n→∞P(Fn) = P(FN) = P(∅) = 0.
(2.17)
Thus we need only consider sequences Fn of nonempty decreasing ﬁeld elements that converge to the
empty set. From Theorem 2.5.1, however, there cannot be such sequences since the ﬁeld is standard.
Thus (2.17) trivially implies (1.8) and the theorem is proved by the Carath´eodory extension theorem.
2
The key to proving suﬃciency above is the characterization of standard ﬁelds in Theorem 2.5.1.
This has the interpretation using part (b) that a ﬁeld is standard if and only if truly countable
unions of nonempty disjoint ﬁeld elements are not themselves in the ﬁeld. Thus ﬁnitely additive set
functions are trivially countably additive on the ﬁeld because all countable disjoint unions of ﬁeld
elements can be written as a ﬁnite union of disjoint nonempty ﬁeld elements.
2.7
The Kolmogorov Extension Theorem
In Chapter 1 in the section on distributions we saw that given a random process, we can determine
the distributions of all ﬁnite dimensional random vectors produced by the process. We now possess
the machinery to prove the reverse result–the Kolmogorov extension theorem–which states that
given a family of distributions of ﬁnite dimensional random vectors that is consistent, that is, higher
dimensional distributions imply the lower dimensional ones, then there is a process distribution
that agrees with the given family. Thus a consistent family of ﬁnite dimensional distributions is
suﬃcient to completely specify a random process possessing the distributions. This result is one of
the most important in probability theory since it provides realistic requirements for the construction
of mathematical models of physical phenomena.
Theorem 2.7.1 Let I be a countable index set, let (Ai, Bi), i ∈I, be a family of standard measur-
able spaces, and let
(AI, BI) = ×i∈I(Ai, Bi)
be the product space. Similarly deﬁne for any index set M ⊂I the product space (AM, BM). Given
index subsets K ⊂M ⊂I, deﬁne the projections ΠM→K : AM →AK by
ΠM→K(xM) = xK,
that is, the projection ΠM→K simply looks at the vector or sequence xM = {xi, i ∈M} and produces
the subsequence xK. A family of probability measures PM on the measurable spaces (AM, BM) for
all ﬁnite index subsets M ⊂I is said to be consistent if whenever K ⊂M, then
PK(F) = PM(Π−1
M→K(F)), all F ∈BK,
(2.18)
that is, probability measures on index subsets agree with those on further subsets. Given any con-
sistent family of probability measures, then there exists a unique process probability measure P on
(AI, BI) which agrees with the given family, that is, such that for all M ⊂I
PM(F) = P(Π−1
I→M(F)), all F ∈BM, all ﬁnite M ⊂I.
(2.19)

42
CHAPTER 2. STANDARD ALPHABETS
Proof: Abbreviate by Πi : AI →Ai the one dimensional sampling function deﬁned by Πi(xI) =
xi, i ∈I. The consistent families of probability measures induces a set function P on all rectangles
of the form
B =
\
i∈M
Π−1
i (Bi)
for any Bi ∈Bi via the relation
P(B) = PM(×i∈MBi),
(2.20)
that is, we can take (2.19) as a deﬁnition of P on rectangles. Since the spaces (Ai, Bi) are standard,
each has a basis {Fi(n), n = 1, 2, . . .} and the corresponding generating ﬁeld Fi. From Lemma 2.3.1
and Corollary 2.3.1 the product ﬁelds given by (2.6) form a basis for F = F(rect(Fi, i ∈I)), which in
turn generates BI; hence P is also countably additive on F. Hence from the Carath´eodory extension
theorem, P extends to a unique probability measure on (AI, BI). Lastly observe that for any ﬁnite
index set M ⊂I, both PM and P(ΠI→M)−1 (recall that Pf −1(F) is deﬁned as P(f −1(F)) for any
function f: it is the distribution of the random variable f) are probability measures on (AM, BM)
that agree on a generating ﬁeld F(rect(Fi, i ∈M)) and hence they must be identical. Thus the
process distribution indeed agrees with the ﬁnite dimensional distributions for all possible index
subsets, completing the proof.
2
By simply observing in the above proof that we only required the probability measure on the
coordinate generating ﬁelds and that these ﬁelds are themselves standard, we can weaken the hy-
potheses of the theorem somewhat to obtain the following corollary. The details of the proof are left
as an exercise.
Corollary 2.7.1 Given standard measurable spaces (Ai, Bi), i ∈I, for a countable index set I, let
Fi be corresponding generating ﬁelds possessing a basis. If we are given a family of normalized,
nonnegative, ﬁnitely additive set functions PM that are deﬁned on all sets F ∈F(rect(Fi, i ∈I))
and are consistent in the sense of satisfying (2.19) for all such sets, then the conclusions of the
previous theorem hold.
In summary, if we have a sequence space composed of standard measurable spaces and a family
of ﬁnitely additive candidate probability measures that are consistent, then there exists a random
unique process or a process distribution that agrees with the given probabilities on the generating
events.
2.8
Extension Without a Basis
The principal advantage of a standard space is the countable extension property ensuring that
any ﬁnitely additive candidate probability measure on a standard ﬁeld is also countably additive.
We have not yet, however, actually demonstrated an example of the construction of a probability
measure by extension. To accomplish this in the manner indicated we must explicitly construct a
basis for a sample space and then provide a ﬁnitely additive set function on the associated ﬁnite
ﬁelds.
We have demonstrated an explicit basis for the special case of a discrete space and for the
relatively complicated example of a product space with discrete coordinate spaces. Hence in principle
we can construct measures on these spaces by providing a ﬁnitely additive set function for this basis.
As a speciﬁc example, consider the binary sequence space (M, BM) of (2.9). Deﬁne the set function
m on thin cylinders c(bn) = {x : xn = bn} by
m(c(bn)) = 2−n; all bn ∈{0, 1}n, n = 1, 2, 3, . . .

2.8. EXTENSION WITHOUT A BASIS
43
and extend this to Fn and hence to F = S∞
n=1 Fn in a ﬁnitely additive way, that is, any G ∈F is
in Fn for some n and hence can be written as a disjoint union of thin cylinders, say G = SN
i=1 c(bn
i ).
In this case deﬁne m(G) = PN
i=1 m(c(bN
i )). This deﬁnes m for all events in F and the resulting m
is ﬁnitely additive by construction. It was shown in Section 2.4 that the Fn, n = 1, 2, . . . form a
basis for F, however, and hence m extends uniquely to a measure on BM. Thus with a basis we can
easily construct probability measures by deﬁning them on simple sets and demonstrating only ﬁnite
and not countable additivity.
Observe that in this case of equal probability on all thin cylinders of a given length, the probability
of individual sequences must be 0 from the continuity of probability. For example, given a sequence
x = (x0, x1, . . .), we have that
n
\
i=1
c(xn) ↓x
and hence
m({x}) = lim
n→∞m(c(xn)) = lim
n→∞2−n = 0.
One goal of this section is to show that it is not easy to construct such a basis in general, even in
an apparently simple case such as the unit interval [0, 1). Thus although a standard space may be
useful in theory, it may be less useful in practice if one only knows that a basis exists but is unable
to construct one.
A second goal of this section is to show that the inability to construct a basis may not be a
problem when trying to explicitly construct probability measures. A basis is nice and important
for theory because all ﬁnitely additive candidate probability measures on a basis are also countably
additive and hence have an extension. If we are trying to prove a speciﬁc candidate probability
measure is countably additive, we can sometimes use the structure of a related standard space
without having to explicitly describe a basis.
To demonstrate the above considerations we focus on a particular example: The Lebesgue mea-
sure on the unit interval. We shall show the diﬃculties in trying to ﬁnd a basis and how to circumvent
them by instead using the structure of binary sequence space.
Suppose that we wish to construct a probability measure m on the unit interval [0, 1) = {r :
0 ≤r < 1}. We would like the probability measure to have the property that the probability of an
interval is proportional to its length, regardless of whether or not the end points are included. Thus,
if 0 ≤a < b < 1, then
m([a, b)) = m((a, b)) = b −a.
(2.21)
The question is, is this enough to completely describe a measure on [0, 1)? That is, does this set
function extend to a normalized, nonnegative, countably additive set function on a suitably deﬁned
σ-ﬁeld?
We ﬁrst need a useful σ-ﬁeld for this alphabet. Deferring a general development to Chapter 3,
we here content ourselves to observe that the σ-ﬁeld should at least contain all intervals of the form
(a, b) for 0 < a < b < 1. Hence deﬁne B[0,1) = σ(all intervals). As we shall later see, this is the Borel
ﬁeld of [0, 1). This σ-ﬁeld is countably generated, e.g., by the countable sequence of sets [0, k/n) for
all positive integers k and n (since any interval can be formed as an increasing or decreasing limit
of simple combinations of such sets). Thus a natural ﬁeld to consider is F = {all ﬁnite set theoretic
combinations of sets of the form [0, k/n)}. This ﬁeld is countable, generates B[0,1), and contains all
the intervals with rational endpoints, e.g., all sets of the form [a, b) for a and b rational. It should
seem reasonable that this is the correct ﬁeld to consider.

44
CHAPTER 2. STANDARD ALPHABETS
It is easy to show that all elements of F can be written as ﬁnite unions of disjoint sets of the
form [a, b) for rational a and b. Clearly m can thus be deﬁned on F in a reasonable way, e.g., if
F =
n
[
i=1
[ai, bi)
and the intervals are disjoint, then
m(F) =
n
X
i=1
m([ai, bi)) =
n
X
i=1
bi −ai.
(2.22)
Again, the question is whether m is countably additive on F.
The discussion of standard spaces suggests that we should construct a basis Fn ↑F. Knowing
that m is ﬁnitely additive on Fn will then suﬃce to prove that it is countably additive on F and
hence uniquely extends to B[0,1).
So far so good. Unfortunately, however, the author is unable to ﬁnd a simple, easily describable
basis for F. A natural guess is to have ever larger and ﬁner Fn generated by intervals with rational
endpoints. For example, let Fn be all ﬁnite unions of intervals of the form
Gn,k = [k2−n, (k + 1)2−n), k = 0, 1, . . . , 2n.
(2.23)
Analogous to the binary sequence example, the measure of the atoms Gn,k of Fn is 2−n and the set
function extends to a ﬁnitely additive set function on F. Clearly Fn ↑F, but unfortunately this
simple sequence does not form a basis because atoms can collapse to empty sets! As an example,
consider the atoms Gn ∈Fn deﬁned by
Gn = [kn2−n, (kn + 1)2−n),
where we choose kn = 2n−1 −1 so that Gn = [2−1 −2−n, 2−1). The atoms Gn are nonempty and
have nonempty ﬁnite intersections, but
∞
\
n=1
Gn = ∅,
which violates the conditions required for Fn to form a basis.
Unlike the discrete or product space case, there does not appear to be a simple ﬁx and no simple
variation on the above Fn appears to provide a basis. Thus we cannot immediately infer that m is
countably additive on F.
A second approach is to try to use an isomorphism as in Lemma 2.3.3 to extend m by actually
extending a related measure in a known standard space. Again there is a natural ﬁrst guess. Let
(M, BM) again be the binary sequence space of (2.9), where BM is the σ-ﬁeld generated by the thin
cylinders. The space is standard and there is a natural mapping f : M →[0, 1) deﬁned by
f(u) =
∞
X
k=0
uk2−k−1,
(2.24)
where u = (u0, u1, u2, . . .). The mapping f(u) is simply the real number whose binary expansion is
.u0u1u2 . . .. We shall show shortly that f is measurable and hence one might hope that [0,1) would
indeed inherit a basis from M through the mapping f. Sadly, this is not immediately the case–the
problem being that f is a many-to-one mapping and not a one-to-one mapping. Hence it cannot

2.8. EXTENSION WITHOUT A BASIS
45
be an isomorphism. For example, both 011111. . . and 10000. . . yield the value f(u) = 1/2. We
could make f a one-to-one mapping by removing from M all sequences with only a ﬁnite number of
0’s, that is, form a new space H ⊂M by removing the countable collection of sequences 11111. . .,
01111. . ., 00111. . ., 10111. . ., and so on (remove any binary vector followed by all ones) and consider
f as a mapping from H to [0,1). Here again we are stymied–none of the results proved so far permit
us to conclude that the reduced space H is standard. In fact, since we have removed individual
sequences (rather than cylinders as in Lemma 2.4.2), there will be atoms collapsing to the empty
set and the previously constructed basis for M does not work for the smaller space H.
We have reached an apparent impasse in that we can not (yet) either show that F has a basis
or that m on F is countably additive. We resolve this dilemma by giving up trying to ﬁnd a basis
for F and instead use the standard space M to show directly that m is countably additive.
We begin by showing that f : M →[0, 1) is measurable. From Lemma 1.4.1 this will follow if
we can show that for any G ∈F, f −1(G) ∈BM. Since every G ∈F can be written as a ﬁnite union
of disjoint sets of the form (2.23), it suﬃces to show that f−1(G) ∈BM for all sets Gn,k of the form
(2.23). To ﬁnd
f −1(Gn,k) = {u : k2−n ≤f(u) < (k + 1)2−n},
let bn = (b0, b1, . . . , bn−1) satisfy
n−1
X
j=0
bj2n−j−1 =
n−1
X
j=0
bn−j−12j = k,
(2.25)
that is, (bn−1, . . . , b0) is the unique binary representation of the integer k. The thin cylinder c(bn)
will then have the property that if u ∈c(bn), then
f(u)
=
∞
X
j=0
bj2−j−1 ≥
n−1
X
j=0
bj2−j−1
=
2−n
n−1
X
j=0
bj2n−j−1 = k2−n
and
f(u)
≤
n−1
X
j=0
bj2−j−1 +
∞
X
j=n
2−j−1
=
k2−n + 2−n
with equality on the upper bound if and only if bj = 1 for all j ≥n. Thus if u ∈c(bn) −{bn111 . . .},
then u ∈f −1(Gn,k) and hence c(bn) −{bn111 . . .} ⊂f−1(Gn,k). Conversely, if u ∈f−1(Gn,k), then
f(u) ∈Gn,k and hence
k2−n ≤
∞
X
j=0
uj2−j−1 < (k + 1)2−n.
(2.26)
The left-hand inequality of (2.26) implies
k
≤
n−1
X
j=0
uj2n−j−1 + 2n
∞
X
j=n
uj2−j−1
≤
n−1
X
j=0
uj2n−j−1 + 1,
(2.27)

46
CHAPTER 2. STANDARD ALPHABETS
with equality on the right if and only if uj = 1 for all j > n. The right-hand inequality of (2.26)
implies that
n−1
X
j=0
uj2n−j−1
≤
n−1
X
j=0
uj2n−j−1 + 2n
∞
X
j=n
uj2−j−1
2n
∞
X
j=0
uj2n−j−1
<
k + 1.
(2.28)
Combining (2.27) and (2.28) yields for u ∈f−1(Gn,k)
k −1 ≤
n−1
X
j=0
uj2n−j−1 < k + 1,
(2.29)
with equality on the left if and only if uj = 1 for all j ≥n. Since the sum in (2.29) must be an
integer, it can only be k or k −1. It can only be k −1, however, if uj = 1 for all j ≥n. Thus either
n−1
X
j=0
uj2n−j−1 = k,
in which case u ∈c(bn) with bn deﬁned as in (2.25), or
n−1
X
j=0
uj2n−j−1 = k −1 and uj = 1, all j ≥n.
The second relation can only occur if for bn as in (2.25) we have that bn−1 = 1 and uj = bj,
j = 0, . . . , n −2, un−1 = 0, and uj = 1, j ≥n. Letting w = (bn−10111 . . .) denote this sequence, we
therefore have that
f −1(Gn,k) ⊂c(bn) ∪w.
In summary, every sequence in c(bn) is contained in f−1(Gn,k) except the single sequence
(bn111 . . .) and every sequence in f−1(Gn,k) is contained in c(bn) except the sequence (bn−10111 . . .)
if bn = 1 in (2.25). Thus given Gn,k, if bn = 0
f −1(Gn,k) = c(bn) −(bn111 . . .)
(2.30)
and if bn = 1
(c(bn) −(bn111 . . .)) ∪(bn−10111 . . .).
(2.31)
In both cases, f −1(Gn,k) ∈BM, proving that f is measurable.
Measurability of f almost immediately provides the desired conclusions: Simply specify a measure
P on the standard space (M, BM) via
P(c(bn)) = 2−n; all bn ∈{0, 1}n
and extend this to the ﬁeld generated by the cylinders in an additive fashion as before. Since this
ﬁeld is standard, this provides a measure on (M, BM). Now deﬁne a measure m on ([0, 1), B[0,1)) by
m(G) = P(f −1(G)), all G ∈B[0,1).

2.8. EXTENSION WITHOUT A BASIS
47
As in Section 2.2, f is a random variable and m is its distribution. In particular, m is a measure.
From (2.29) and the fact that the individual sequences have 0 measure, we have that
m([k2−n, (k + 1)2−n)) = P(c(bn)) = 2−n, k = 1, 2, . . . , n −1.
From the continuity of probability, this implies that for any interval [a, b), 1 > b > a > 0, m([a, b)) =
b−a, as desired. Since this agrees with the original description of the measure desired on intervals, it
must also agree on the ﬁeld generated by intervals. Thus there exists a probability measure agreeing
with (2.21) and hence with the additive extension of (2.21) to the ﬁeld generated by the intervals.
By the uniqueness of extension, the measure we have constructed must be the unique extension of
(2.21). This measure is called the Lebesgue measure on the unit interval.
Thus even without a basis or an isomorphism, we can construct probability measures if we have
a measurable mapping from a standard space to the given space.

48
CHAPTER 2. STANDARD ALPHABETS

Chapter 3
Borel Spaces and Polish alphabets
We have seen that standard measurable spaces are the only measurable spaces for which all ﬁnitely
additive candidate probability measures are also countably additive, and we have developed sev-
eral properties and some important simple examples. In particular, sequence spaces drawn from
countable alphabets and certain subspaces thereof are standard. In this chapter we develop the
most important (and, in a sense, the most general) class of standard spaces–Borel spaces formed
from complete separable metric spaces. We will accomplish this by showing that such spaces are
isomorphic to a standard subspace of a countable alphabet sequence space and hence are themselves
standard. The proof will involve a form of coding or quantization.
3.1
Borel Spaces
In this section we further develop the structure of metric spaces and we construct σ-ﬁelds and
measurable spaces from metric spaces.
Let A denote a metric space with metric d. Deﬁne for each a in A and each real r > 0 the open
sphere (or open ball) ith center a and radius r by Sr(a) = {b : b ∈A, d(a, b) < r}. For example,
an open sphere in the real line ℜwith respect to the Euclidean metric is simply an open interval
(a, b) = {x : a < x < b}. A subset F of A is called an open set if given any point a ∈F there is an
r > 0 such that Sr(a) is completely contained in F, that is, there are no points in F on the edge F.
Let OA denote the class of all open sets in A. The following lemma collects several useful properties
of open sets. The proofs are left as an exercise as they are easy and may be found in any text on
topology or metric spaces; e.g., [65].
Lemma 3.1.1 All spheres are open sets. A and ∅are open sets. A set is open if and only if it can
be written as a union of spheres; in fact,
F =
[
a∈F
[
r:r<d(a,F c)
Sr(a),
(3.1)
where
d(a, G) = inf
b∈G d(a, b).
If {Ft; t ∈T } is any (countable or uncountable) collection of open sets, then S
t∈T Ft is open. If
Fi, i ∈N, is any ﬁnite collection of open sets, then ∩i∈NFi is open.
49

50
CHAPTER 3. BOREL SPACES AND POLISH ALPHABETS
When we have a class of open sets V such that every open set F in OA can be written as a union
of elements in V, e.g., the preceding open spheres, then we say that V is a base for the open sets
and write OA = OPEN(V) = {all unions of sets in V}.
In general, a set A together with a class of subsets OA called open sets such that all unions and
ﬁnite intersections of open sets yield open sets is called a topology on A. If V is a base for the open
sets, then it is called a base for the topology. Observe that many diﬀerent metrics may yield the same
open sets and hence the same topology. For example, the metrics d(a, b) and d(a, b)/(1 + d(a, b))
yield the same open sets and hence the same topology. Two metrics will be said to be equivalent on
A if they yield the same topology on A.
Given a metric space A together with a class of open sets OA of A, the corresponding Borel ﬁeld
(actually a Borel σ-ﬁeld) B(A) is deﬁned as σ(OA), the σ-ﬁeld generated by the open sets. The
members of B(A) are called the Borel sets of A. A metric space A together with a Borel σ-ﬁeld
B(A) is called a Borel space (A, B(A)). If this space is standard, it is naturally called a standard
Borel space. Thus a Borel space is a measurable space where the σ-ﬁeld is generated by the open
sets of the alphabet with respect to some metric. Observe that equivalent metrics on a space A will
yield the same Borel measurable space since they yield the same open sets.
If A is a metric space with metric d, a point a in A is said to be a limit point of a set F if every
open sphere Sr(a) contains a point b ̸= a such that b is in F. A set F is closed if it contains all of
its limit points. The set F together with its limit points is called the closure of F and is denoted
by F. Given a set F, deﬁne its diameter
diam(F) = sup
a,b∈F
d(a, b).
Lemma 3.1.2 A set is closed if and only if its complement is open and hence all closed sets are
in the Borel σ-ﬁeld. Arbitrary intersections and ﬁnite unions of closed sets are closed. B(A) =
σ( all closed sets ). The closure of a set is closed. If F is the closure of F, then diam(F) = diam(F).
Proof: If F is closed and we choose x in F c , then x is not a limit point of F and hence there is
an open sphere Sr(x) that is disjoint with F and hence contained in F c. Thus F c must be open.
Conversely, suppose that F c is open and x is a limit point of F and hence every sphere Sr(x) contains
a point of F. Since F c is open this means that x cannot be in F c and hence must be in F. Hence F
is closed. This fact together with DeMorgan’s laws and Lemma 3.1.1 implies that ﬁnite unions and
arbitrary intersections of closed sets are closed. Since the complement of every closed set is open
and vice versa, the σ-ﬁelds generated by the two classes are the same. To prove that the closure of
a set is closed, ﬁrst observe that if x is in F
c and hence neither a point nor a limit point of F, then
there is an open sphere S centered at x which has no points in F. Since every point y in S therefore
has an open sphere containing it which contains no points in F , no such y can be a limit point of
F. Thus F
c contains S and hence is open. Thus its complement, the closure of F, must be closed.
Any points in the closure of a set must have points in the set within an arbitrarily close distance.
Hence the diameter of a set and that of its closure are the same.
2
Lemma 3.1.3 A set F is closed if and only if for every sequence {an, n = 1, 2, . . .} of elements
of F such that an →a we have that also a is in F. If d and m are two metrics on A that yield
equivalent notions of convergence, that is, d(an, a) →0 if and only if m(an, a) →0 for an, a in A,
then d and m are equivalent metrics on A, that is, they yield the same open sets and generate the
same topology.
Proof: If a is a limit point of F, then one can construct a sequence of elements an in F such that
a = limn→∞an. For example, choose an as any point in S1/n(a) (there must be such an an since

3.1. BOREL SPACES
51
a is a limit point). Thus if a set F contains all limits of sequences of its points, it must contain its
limit points and hence be closed.
Conversely, let F be closed and an a sequence of elements of F that converge to a point a. If
for every ϵ > 0 we can ﬁnd a point an distinct from a in Sϵ(a), then a is a limit point of F and
hence must be in F since it is closed. If we can ﬁnd no such distinct point, then all of the an must
equal a for n greater than some value and hence a must be in F. If two metrics d and m are such
that d-convergence and m-convergence are equivalent, then they must yield the same closed sets and
hence by the previous lemma they must yield the same open sets.
In addition to open and closed sets, two other kinds of sets arise often in the study of Borel
spaces: Any set of the form T∞
i=1 Fi with the Fi open is called a Gδ set or simply a Gδ. Similarly,
any countable union of closed sets is called an Fσ.
Observe that Gδ and Fσ sets are not necessarily
either closed or open. We have, however, the following relation.
Lemma 3.1.4 Any closed set is a Gδ, any open set is an Fσ.
Proof: If F is a closed subset of A, then
F =
∞
\
n=1
{x : d(x, F) < 1/n},
and hence F is a Gδ. Taking complements completes the proof since any closed set is the complement
of an open set and the complement of a Gδ is an Fσ.
2
The added structure of Borel spaces and the previous lemma permit a diﬀerent characterization
of σ-ﬁelds:
Lemma 3.1.5 Given a metric space A, then B(A) is the smallest class of subsets of A that con-
tains all the open (or closed) subsets of A, and which is closed under countable disjoint unions and
countable intersections.
Proof: Let G be the given class. Clearly G ⊂B(A). Since G contains the open (or closed) sets (which
generate the σ-ﬁeld B(A)), we need only show that it contains complements to complete the proof.
Let H be the class of all sets F such that F ∈G and F c ∈G. Clearly H ⊂G. Since G contains
the open sets and countable intersections, it also contains the Gδ’s and hence also the closed sets
from the previous lemma. Thus both the open sets and closed sets all belong to H. Suppose now
that Fi ∈H, i = 1, 2, . . . and hence both Fi and their complements belong to G. Since G contains
countable disjoint unions and countable intersections
∞
[
i=1
Fi =
∞
[
i=1
(Fi ∩F c
1 ∩F c
2 ∩· · · ∩F c
i−1) ∈G
and
(
∞
[
i=1
Fi)c =
∞
\
i=1
F c
i ∈G.
Thus the countable unions of members of H and the complements of such sets are in G. This implies
that H is closed under countable unions. If F ∈H, then F and F c are in G, hence also F c ∈H.
Thus H is closed under complementation and countable unions and hence H contains both countable
disjoint unions and countable intersections. Since H also contains the open sets, it must therefore
contain G. Since we have already argued the reverse inclusion, H = G. This implies that G is closed
under complementation since H is, thus completing the proof.
2

52
CHAPTER 3. BOREL SPACES AND POLISH ALPHABETS
The most important mappings for Borel spaces are continuous functions. Given two Borel spaces
(A, B(A)) and (B, B(B)) with metrics d and ρ, respectively, a function f : A →B is continuous at
a in A if for each ϵ > 0 there exists a δ > 0 such that d(x, a) < δ implies that ρ(f(x), f(a)) < ϵ. It
is an easy exercise to show that if a is a limit point, then f is continuous at a if and only if an →a
implies that f(an) →f(a).
A function is continuous if it is continuous at every point in A. The following lemma collects the
main properties of continuous functions: a deﬁnition in terms of open sets and their measurability.
Lemma 3.1.6 A function f is continuous if and only if the inverse image f−1(S) of every open set
S in B is open in A. A continuous function is Borel measurable.
Proof: Assume that f is continuous and S is an open set in B. To prove f−1(S) is open we must
show that about each point x in f−1(S) there is an open sphere Sr(x) contained in f−1(S). Since
S is open we can ﬁnd an s such that ρ(f(x), y) < s implies that y is in S. By continuity we can
ﬁnd an r such that d(x, b) < r implies that d(f(x), f(b)) < s. If f(x) and f(b) are in S, however, x
and b are in the inverse image and hence Sr(x) so obtained is also in the inverse image. Conversely,
suppose that f −1(S) is open for every open set S in B. Given x in A and ϵ > 0 deﬁne the open
sphere S = Sϵ(f(x)) in B. By assumption f−1(S) is then also open, hence there is a δ > 0 such that
d(x, y) < δ implies that y ∈f−1(S) and hence f(y) ∈S and hence ρ(f(x), f(y)) < ϵ, completing the
proof.
2
Since inverse images of open sets are open and hence Borel sets and since open sets generate the
Borel sets, f is measurable from Lemma 1.4.1.
Exercises
1. Prove Lemma 3.1.1.
2. Is the mapping f of Section 2.8 continuous?
3.2
Polish Spaces
Let A be a metric space. A set F is said to be dense in A if every point in A is a point in F or a
limit point of F. A metric space A is said to be separable if it has a countable dense subset, that is,
if there is a discrete set, say B, such that all points in A can be well approximated by points in B
in the sense that all points in A are points in B or limits of points in B. For example, the rational
numbers are dense in the real line ℜand are countable, and hence ℜof Example 2.5.5 is separable.
Similarly, n-dimensional vectors with rational coordinates are countable and dense in ℜn, and hence
Example 2.5.6 also provides a separable metric space. The discrete metric spaces of Examples 2.5.1
through 2.5.4 are trivially separable since A is countable and is dense in itself. An example of a
nonseparable space is the binary sequence space M = {0, 1}Z+ with the sup-norm metric
d(x, y) = sup
i∈Z+
|xi −yi|.
The principal use of separability of metric spaces is the following lemma.
Lemma 3.2.1 Let A be a metric space with Borel σ-ﬁeld B(A). If A is separable, then B(A) is
countably generated and it contains all of the points of the set A.

3.2. POLISH SPACES
53
Proof: Let {ai; i = 1, 2, . . .} be a countable dense subset of A and deﬁne the class of sets
VA = {S1/n(ai); i = 1, 2, . . . ; n = 1, 2, . . .}.
(3.2)
Any open set F can be written as a countable union of sets in VA analogous to (3.1) as
F =
[
i:ai∈F
[
n:n−1<d(ai,F c)
S1/n(ai).
(3.3)
Thus VA is a countable class of open sets and σ(VA) contains the class of open sets OA which
generates B(A). Hence B(A) is countably generated. To see that the points of A belong to B(A),
index the sets in VA as {Vi; i = 0, 1, 2, . . .} and deﬁne the sets Vi(a) by Vi if a is in Vi and V c
i if a
is not in Vi. Then
{a} =
∞
\
i=1
Vi(a) ∈B(A).
2
For example, the class of all open intervals (a, b) = {x : a < x < b} with rational endpoints b > a
is a countable generating class of open sets for the Borel σ-ﬁeld of the real line with respect to the
usual Euclidean distance. Similar subsets of [0,1] form a similar countable generating class for the
Borel sets of the unit interval.
A sequence {an; = 1, 2, . . .} in A is called a Cauchy sequence if for every ϵ > 0 there is an integer
N such that d(an, am) < ϵ if n ≥N and m ≥N. Alternatively, a sequence is Cauchy if and only if
lim
n→∞diam(an, an+1, . . .) = 0.
A metric space is complete if every Cauchy sequence converges, that is, if {an} is a Cauchy
sequence, then there is an a in A for which a = limn→∞an. In the words of Simmons [65], p. 71,
a complete metric space is one wherein “every sequence that tries to converge is successful.” A
standard result of elementary real analysis is that Examples 2.5.5 and 2.5.6 ℜk with the Euclidean
distance) are complete (see Rudin [61], p. 46). A complete inner product space (Example 2.5.10) is
called a Hilbert space. A complete normed space (Example 2.5.9) is called a Banach space.
A fundamental property of complete metric spaces is given in the following lemma, which is
known as Cantor’s intersection theorem. The corollary is a slight generalization.
Lemma 3.2.2 Given a complete metric space A, let Fn, n = 1, 2, . . . be a decreasing sequence of
nonempty closed subsets of A for which diam(Fn) →0.
Then the intersection of all of the Fn
contains exactly one point.
Proof: Since the diameter of the sets is decreasing to zero, The intersection cannot contain more
than one point. If xn is a sequence of points in Fn, then it is a Cauchy sequence and hence must
converge to some point x since A is complete. If we show that x is in the intersection, the proof
will be complete. Since the Fn are decreasing, for each n the sequence {xk, k ≥n} is in Fn and
converges to x. Since Fn is closed, however, it must contain x. Thus x is in all of the Fn and hence
in the intersection.
The similarity of the preceding condition to the ﬁnite intersection property of atoms of a basis
of a standard space is genuine and will be exploited soon.
Corollary 3.2.1 Given a complete metric space A, let Fn, n = 1, 2, . . . be a decreasing sequence of
nonempty subsets of A such that diam(Fn) →0 and such that the closure of each set is contained in
the previous set, that is,
F n ⊂Fn−1, all n,
where we deﬁne F0 = A. Then the intersection of all of the Fn contains exactly one point.

54
CHAPTER 3. BOREL SPACES AND POLISH ALPHABETS
Proof: Since Fn ⊂F n ⊂Fn−1
∞
\
n=1
Fn ⊂
∞
\
n=1
F n ⊂
∞
\
n=0
Fn.
Since the Fn are decreasing, the leftmost and rightmost terms above are equal. Thus
∞
\
n=1
Fn =
∞
\
n=1
F n.
Since the Fn are decreasing, so are the F n. Since these sets are closed, the right-hand term contains
a single point from the lemma.
2
A complete separable metric space is called a Polish space because of the pioneering work by
Polish mathematicians on such spaces.
Polish spaces play a central role in functional analysis
and provide models for most alphabets of scalars, vectors, sequences, and functions that arise in
communications applications.
The best known examples are the Euclidean spaces, but we shall
encounter others such as the space of square integrable functions. A Borel space (A, B(A)) with A
a Polish metric space will be called a Polish Borel space.
The remainder of this section develops some properties of Polish spaces that resemble those
of standard spaces: We show that cartesian products of Polish spaces are Polish spaces, and that
certain subsets of Polish spaces are also Polish.
Products of Polish Spaces
Given a Polish space A and an index set I, we can deﬁne a metric dI on the product space AI as
in Example 2.5.11. The following lemma shows that the product space is also Polish.
Lemma 3.2.3 (a) If A is a Polish space with metric d and I is a countable index set, then the
product space AI is a Polish space with respect to the product space metric dI.
(b) In addition, if B(A) is the Borel σ-ﬁeld of subsets of A with respect to the metric d, then the
product σ-ﬁeld of Section 1.4, B(A)I = σ(rect(Bi, i ∈N)), where the Bi are all replicas of
B(A), is exactly the Borel σ-ﬁeld of AI with respect to dI. That is, B(A)I = B(AI).
Proof: (a) If A has a dense subset, say B, then a dense subset for AI can be obtained as the class of all
sequences taking values in B on a ﬁnite number of coordinates and taking some ﬁxed reference value,
say b, on the remaining coordinates. Hence AI is separable. If a sequence {xn}I = {xn,i}, i ∈N}
is Cauchy, then dI({xn}I, {xm}I) →0 as n, m →∞. From Lemma 2.5.1 this implies that for each
coordinate d(xn,i, xm,i) →0 as n, m →∞and hence the sequence xn,i is a Cauchy sequence in A for
each i. Since A is complete, the sequence has a limit, say yi. From Lemma 2.5.1 again, this implies
that {xn}I →yI and hence AI is complete.
(b) We next show that the product σ-ﬁeld of a countably generated Borel σ-ﬁeld is the Borel
σ-ﬁeld with respect to the product space metric; that is, both means of constructing σ-ﬁelds yield
the same result. (This result need not be true for products of Borel σ-ﬁelds that are not countably
generated.)
Let Πn : AI →A be the coordinate or sampling function Πn(xI) = xn. It is easy to show that
Πn is a continuous function and hence is also measurable from Lemma 3.1.6. Thus Π−1
n (F) ∈B(AI)
for all F ∈B(A) and n ∈N. This implies that all ﬁnite intersections of such sets and hence all
rectangles with coordinates in B(A) must be in B(AI). Since these rectangles generate B(A)I, we
have shown that
B(A)I ⊂B(AI).

3.2. POLISH SPACES
55
Conversely, let Sr(x) be an open sphere in AI: Sr(x) = {y : y ∈AI, dI(x, y) < r}. Observe
that for any n
dI(x, y) ≤
n
X
k=1
2−k
d(xk, yk)
1 + d(xk, yk) + 2−n
and hence we can write
Sr(x) =
[
n:2−n<r
[
dk:Pn
k=1 dk<r−2−n
n
\
k=0
Π−1
k ({a : dI(xk, a)
dk
1 −dk
}),
where the union over the {dk} is restricted to rational dk. Every piece of the union on the right-hand
side is contained in the sphere on the left, and every point in the sphere on the left is eventually
included in one of the terms in the union on the right for suﬃciently large n. Thus all spheres in AI
under dI can be written as countable unions of of rectangles with coordinates in B(A), and hence
these spheres are in B(A)I. Since AI is separable under dI all open sets in AN can be written as a
countable union of spheres in AI using (3.3), and hence all open sets in AI are also in B(A)I. Since
these open sets generate B(AI), B(AI) ⊂B(A)I, completing the proof.
2
Example 3.2.1: Integer Sequence Spaces
As a simple example of some of the ideas under consideration, we consider the space of integer
sequences introduced in Lemma 2.4.1. Let B = Z+ and A = N = BB, the space of sequences
with nonnegative integer values. A discrete space such as A0 is a Polish space with respect to the
Hamming metric dH of Example 2.5.1. This is trivial since the countable set of symbols is dense in
itself and if a sequence {an} is Cauchy, then choosing ϵ < 1 implies that there is an integer N such
that d(an, am) < 1 and hence an = am for all n, m ≥N. Note that an open sphere Sϵ(a) in B with
ϵ < 1 is just a point and that any subset of B is a countable union of such open spheres. Thus in
this case the Borel σ-ﬁeld B(Z+) is just the power set, the collection of all subsets of B.
The sequence space A is a metric space with the metric of Example 2.5.11, which with a coordinate
Hamming distance becomes
dZ+(x, y) = 1
2
∞
X
i=0
2−idH(xi, yi).
(3.4)
Applying Lemma 3.2.3 (b) and (3.4) we have
B(N) = B(ZZ+
+ ) = B(Z+)Z+.
(3.5)
It is interesting to compare the structure used in Lemma 2.4.1 with the Borel or topological
structure currently under consideration. Recall that in Lemma 2.4.1 the σ-ﬁeld B(Z+)Z+ was ob-
tained as the σ-ﬁeld generated by the thin cylinders, the rectangles or thin cylinders having the form
{x : xn = an} for some positive integer n and some n-tuple an ∈Bn. For convenience abbreviate
dB to d. Suppose we know that d(x, y) < 1/2. Then necessarily x0 = y0 and hence both sequences
must lie in a common thin cylinder, the thin cylinder {u : u0 = x0}. Similarly, if d(x, y) < 1/4, then
necessarily x2 = y2 and both sequences must again lie in a common thin cylinder, {u : u2 = x2}. In
general, if d(x, y) < ϵ and ϵ ≤2−(n+1), then xn = yn. Thus if x ∈A and ϵ ≤2−(n+1), then
Sϵ(x) = {y : yn = xn}.
(3.6)
This means that the thin cylinders are simply the open spheres in A! Thus the generating class used
in Lemma 2.4.1 is really the same as that used to obtain a Borel σ-ﬁeld for A. In other words, not
only are the σ-ﬁelds of (3.5) all equal, the collections of sets used to generate these ﬁelds are the
same.

56
CHAPTER 3. BOREL SPACES AND POLISH ALPHABETS
Subspaces of Polish Spaces
The following result shows that certain subsets of Polish spaces are themselves Polish, although in
some cases we need to consider an equivalent metric rather than the original metric.
Lemma 3.2.4 Let A be a complete, separable metric space with respect to a metric d. If F = C, C
a closed subset of A, then F is itself a complete, separable metric space with respect to d. If F = O,
where O is an open subset of A, then F is itself a complete separable metric space with respect to
a metric d′ such that d′-convergence and d-convergence are equivalent on F and hence d and d′ are
equivalent metrics on F. In fact, the following provides such a metric:
d′(x, y) = d(x, y) + |
1
d(x, Oc) −
1
d(y, Oc)|.
(3.7)
If F is the intersection of a closed set C with an open set O of A, then F is a complete separable
metric space with respect to a metric d′ such that d′-convergence and d-convergence are equivalent
on F, and hence d and d′ are equivalent metrics on F. Here, too, d′ of (3.7) provides such a metric.
Proof: Suppose that F is an arbitrary subset of a separable metric space A with metric d having the
countable base VA = {Vi; i = 1, 2, . . .} of (3.3). Deﬁne the class of sets VF = {F ∩Vi; i = 1, 2, . . .},
where we eliminate all empty sets. Let yi denote an arbitrary point in F ∩Vi. We claim that the
set of all yi is dense in F and hence F is separable under d. To see this let x ∈F. Given ϵ there is
a Vi of radius less than ϵ which contains x and hence F T Vi is not empty (since it at least contains
x ∈F) and
d(x, yi) ≤
sup
y∈F ∩Vi
d(x, y) ≤sup
y∈Vi
d(x, y) ≤2ϵ.
Thus for any x in F we can ﬁnd a yi ∈VF that is arbitrarily close to it. Thus VF is dense in F
and hence F is separable with respect to d. The resulting metric space is not in general complete,
however, because completeness of A only ensures that Cauchy sequences converge to some point in
A and hence a Cauchy sequence of points in F may converge to a point not in F. If F is closed,
however, it contains all of its limit points and hence F is complete with respect to the original metric.
This completes the proof of the lemma for the case of a closed subset.
2
If F is an open set O, then we can ﬁnd a metric that makes O complete by modifying the
original metric to ow upear the boundary and hence prevent Cauchy sequences from converging to
something not in O. In other words, we modify the metric so that if a sequence an ∈O is Cauchy
with respect to d but converges to a point outside O, then the sequence will not be Cauchy with
respect to d′. Deﬁne the metric d′ on O as in (3.7). Observe that by construction d ≤d′, and
hence convergence or Cauchy convergence under d′ implies the same under d. From the triangle
inequality, |d(an, Oc) −d(a, Oc)| ≤d(an, a). Thus provided the an and a are in O, d-convergence
implies d′-convergence. Thus d-convergence and d′-convergence are equivalent on O and hence yield
the same open sets from Lemma 3.1.3. In addition, points in O are limit points under d if and only
if they are limit points under d′ and hence O is separable with respect to d′ since it is separable
with respect to d. Since a sequence Cauchy under d′ is also Cauchy under d, a d′-Cauchy sequence
must have a limit since A is complete with respect to d. This point must also be in O, however,
since otherwise 1/d(an, Oc) →1/d(a, Oc) = ∞and hence d′(an, am) could not be converging to zero
and hence {an} could not be Cauchy. (Recall that for such a double limit to tend to zero, it must
so tend regardless of the manner in which m and n go to inﬁnity.) Thus O is complete. Since O is
complete and separable, it is Polish.
If F is the intersection of a closed set C and an open set O, then the preceding metric d′ yields
a Polish space with the desired properties for the same reasons. In particular, d-convergence and

3.2. POLISH SPACES
57
d′-convergence are equivalent for the same reasons and a d′-Cauchy sequence must converge to a
point that is in the intersection of C and O: it must be in C since C is closed and it must be in O
or d′ would blow up.
2
It is important to point out that d-convergence and d′-convergence are only claimed to be equiv-
alent within F; that is, a sequence an in F converges to a point a in F under d if and only if it
converges under d′. It is possible, however, for a sequence an to converge to a point a not in F
under d and to not converge at all under d′ (the d′ distance is not even deﬁned for points not in F).
In other words, F may not be a closed subset of A with respect to d.
Example 3.2.2 Integer Sequence Spaces Revisited
We return to the integer sequence space of Example 3.2.1 to consider a simple but important
special case where a subset of a Polish space is Polish. The following manipulation will resemble that
of Lemma 2.3.2. Let A and d be as in that lemma and recall that A is Polish and has a Borel σ-ﬁeld
B(A) = B(B)B. Suppose that we have a countable collection Fn, n = 1, 2, . . . of thin cylinders with
union F = S
n Fn. Consider the sequence space
˜A = A −
∞
[
n=1
Fn =
∞
\
n=1
F c
n
formed by removing the countable collection of thin cylinders from A. Since the thin cylinders are
open sets, their complements are closed. Since an arbitrary intersection of closed sets is closed, ˜A is
a closed subset of A. Thus ˜A is itself Polish with respect to the same metric, d.
The Borel σ-ﬁelds of the spaces A and ˜A can be easily related by observing as in Example 3.2.1
that an open sphere about a sequence in either sequence space is simply a thin cylinder. Every thin
cylinder in A is either contained in or disjoint from A −F. Thus B(A) is generated by all of the
thin cylinders in A and B( ˜A) is generated by the thin cylinders in ˜A and hence by all sets of the
form {x : xn = an} ∩(A −F), that is, by all the thin cylinders except those removed. Thus every
open sphere in ˜A is also an open sphere in A and is contained in F c; thus B( ˜A) ⊂B(A) ∩(A −F).
Similarly, every open sphere in A that is contained in F is also an open sphere in ˜A, and hence the
reverse inclusion holds. In summary,
B( ˜A) = B(A) ∩(A −F).
(3.8)
Carving
We now turn to a technique for carving up complete, separable metric spaces into pieces with a
useful structure. From the previous lemma, we will then also be able to carve up certain other sets
in a similar manner.
Lemma 3.2.5 (The carving lemma.) Let A be a complete, separable metric space. Given any real
r > 0, there exists a countably inﬁnite family of Borel sets (sets in B(A)) {Gn, n = 1, 2, . . .} with
the following properties:
(a) The {Gn} partition A; that is, they are disjoint and their union is A.
(b) Each {Gn} is the intersection of the closure of an open sphere and an open set.
(c) For all n, diam(Gn) ≤2r.
Proof: Let {ai, 1 = 0, 1, 2, . . .} be a countable dense set in A. Since A is separable, the family
{Vi, i = 0, 1, 2, . . .} of the closures of the open spheres Si = {x : d(x, ai) < r} covers A; that is, its
union is A. In addition, diam(Vi) = diam(Si) ≤r.

58
CHAPTER 3. BOREL SPACES AND POLISH ALPHABETS
Construct the set Gi(1) as the set Vi with all sets of lower index removed and all empty sets
omitted; that is,
G0(1)
=
V0
G1(1)
=
V1 −V0
...
Gn(1)
=
Vn −
[
i<n
Vi
=
Vn −
[
i<n
Gi(1)
...
where we remove all empty sets from the sequence. The sets are clearly disjoint and partition B.
Furthermore, each set Gn(1) has the form
Gn(1) = Vn ∩(
\
i<n
V c
i );
that is, it is the intersection of the closure of an open sphere with an open set. Since Gn(1) is a
subset of Vn, it has diameter less than r. This provides the sequence with properties (i)-(iii).
2
We are now ready to iterate on the previous lemmas to show that we can carve up Polish spaces
into shrinking pieces that are also Polish spaces that can then be themselves carved up.
This
representation of a Polish space is called a scheme, and it is detailed in the next section.
Exercises
1. Prove the claims made in the ﬁrst paragraph of this section.
2. Let (Ω, B, P) be a probability space and let A = B, the collection of all events. Two events are
considered to be the same if the probability of the symmetric diﬀerence is 0. Deﬁne a metric
d on A as in Example 2.5.12. Is the corresponding Borel ﬁeld a countably generated σ-ﬁeld?
Is A a separable metric space under d?
3. Prove that in the Example 3.2.2 that all ﬁnite dimensional rectangles in A are open sets with
respect to d. Since the complements of rectangles are other rectangles, this means that all
rectangles (including the thin cylinders) are also closed. Thus the thin cylinders are both
open and closed sets. (These unusual properties occur because the coordinate alphabets are
discrete.)
3.3
Polish Schemes
In this section we develop a representation of a Polish space that resembles an inﬁnite version of the
basis used to deﬁne standard spaces; the principal diﬀerence is that at each level we can now have
an inﬁnite rather that ﬁnite number of sets, but now the diameter of the sets must be decreasing as
we descend through the levels. It will provide a canonical representation of Polish spaces and will
provide the key element for proving that Polish spaces yield standard Borel spaces.

3.3. POLISH SCHEMES
59
Recall that Z+ ={0, 1, 2, . . .}, Zn
+ is the corresponding ﬁnite dimensional cartesian product, and
N = ZZ+
+
is the one-sided inﬁnite cartesian product. Given a metric space A, a scheme is deﬁned
as a family of sets A(un), n = 1, 2, . . ., indexed by un in Zn
+ with the following properties:
A(un) ⊂A(un−1)
(3.9)
1. (the sets are decreasing),
A(un) ∩A(vn) = ∅if un ̸= vn
(3.10)
2. (the sets are disjoint),
lim
n→∞diam(A(un)) = 0, all u = {u0, u1, . . .} ∈N.
(3.11)
3. If a decreasing sequence of sets A(un), n = 1, 2, . . . are all nonempty, then the limit is not
empty; that is,
A(un) ̸= ∅all n implies Au ̸= ∅,
(3.12)
4. where Au is deﬁned by
Au =
∞
\
n=1
A(un).
(3.13)
The scheme is said to be measurable
if all of the sets A(un) are Borel sets. The collection of all
nonempty sets Au, u ∈N, of the form (3.13) will be called the kernel of the scheme. We shall let
A(u0) denote the original space A.
Theorem 3.3.1 If A is a complete, separable metric space, then it is the kernel of a measurable
scheme.
Proof: Construct the sequence Gn(1) as in the ﬁrst part of the carving lemma to satisfy properties
(i)-(iii) of that lemma. Choose a radius r of, say, 1. This produces the ﬁrst level sets A(u1), u1 ∈Z+.
Each set thus produced is the intersection of the d closure C(u1) of a d-open sphere and a d−open
set O(u1) in A, and hence by Lemma 3.2.4 each set A(u1) is itself a Polish space with respect to
a metric du1 that within A(u1) is equivalent to d. We shall also make use of the property of the
speciﬁc equivalent metric of (3.7) of that lemma that du1(a, b) ≤d(a, b). Henceforth let du0 denote
the original metric d. The closure of A(u1) with respect to the original metric d may not be in A(u1)
itself, but it is in the set C(u1).
We then repeat the procedure on these sets by applying Lemma 3.2.4 to each of the new Polish
spaces with an r of 1/2 to obtain sets A(u2) that are intersections of a du1 closure C(u2) of an
open sphere and a du1-open set O(u2). Each of these sets is then itself a Polish space with respect
to a metric du2(x, y) which is equivalent to the parent metric du1(x, y) within A(u2). In addition,
du2(x, y) ≥du1(x, y). Since A(u2) ⊂A(u1) and the parent metric du1 is equivalent to its parent
metric du0 within A(u1), du2 is equivalent to the original metric d within A(u2).
A ﬁnal crucial property is that the closure of A(u2) with respect to its parent metric du1 is
contained within its parent set A(u1). This is true since A(u2) = C(u2) ∩O(u2) and C(u2) is a
du1-closed subset of A(u1) and therefore the du1- closure of A(u2) lies within C(u2) and hence within
A(u1).
We now continue in this way to construct succeeding levels of the scheme so that the preceding
properties are retained at each level. At level n, n = 1, 2, . . ., we have sets
A(un) = C(un) ∩O(un),
(3.14)

60
CHAPTER 3. BOREL SPACES AND POLISH ALPHABETS
where C(un) is a dun−1-closed subset of A(un−1) and O(un) is a dun−1-open subset of A(un−1). The
set C(un) is the dun−1-closure of an open ball of the form
S(un−1, i) = S1/n(ci; un−1) = {x : x ∈A(un−1), dun−1(x, ci) < 1/n}, i = 0, 1, 2, . . . .
(3.15)
Each set A(un) is then itself a Polish space with metric
dun(x, y) = dun−1(x, y) + |
1
dun−1(x, O(un)c) −
1
dun−1(y, O(un)c)|.
(3.16)
We have by construction that for any un in Zn
+
d ≤du1 ≤du2 ≤. . . ≤dun
(3.17)
and hence since A(un) is in the closure of an open sphere of the form (3.15) having dun−1 radius
1/n, we have also that the d-diameter of the sets satisﬁes
diam(A(un)) ≤2/n.
(3.18)
Furthermore, on A(un), dun (the complete metric) and dun−1 yield equivalent convergence and are
equivalent metrics. Since each A(un) is contained in all of the Aui for i = 0, . . . , n −1, this implies
that
given ak ∈A(un), k = 1, 2, . . . , and a ∈A(un), then
dun(ak, a) →0 iﬀduj(ak, a) →0 for all j = 0, 1, 2, . . . , n −1.
(3.19)
This sequence of sets satisﬁes conditions (3.9)-(3.11). To complete the proof of the theorem we need
only show that (3.12) is satisﬁed.
If we can demonstrate that the d-closure in A of the set A(un) lies completely in its parent set
A(un−1), then (3.12) will follow immediately from Corollary 3.2.1. We next provide this demonstra-
tion.
Say we have a sequence ak; k = 1, 2, . . . such that ak ∈A(un), all k, and a point a ∈A such
that d(ak, a) →0 as n →∞. We will be done if we can show that necessarily a ∈A(un−1). We
accomplish this by assuming the contrary and developing a contradiction. Hence we assume that
ak →a in d, but a ̸∈A(un−1). First observe from (3.17) and (3.15) that since all of the ak are in
the closure of a sphere of dun−1-radius 1/n centered at, say, c, then we must have that
duj(ak, c) ≤1/n, j = 0, 1, 2, . . . , n −1; all k;
(3.20)
that is, the ak must all be close to the center c ∈A(un−1) of the sphere used to construct C(un)
with respect to all of the previous metrics.
By assumption a ̸∈A(un−1). Suppose that l is the smallest index such that a ̸∈A(u1). Since
the A(ui) are nested, we will then have that a ∈A(ui) for i < l. In addition, l ≥1 since A(u0) is
the entire space and hence clearly a ∈A(u0).
Since a ∈A(ul−1) and ak ∈A(u1) ⊂A(ul−1), from (3.19) applied to A(ul−1) d-convergence of
ak to a is equivalent to dul−1-convergence. Since the set C(u1) ⊂A(ul−1) used to form A(u1) via
A(u1) = C(u1) ∩O(u1) is closed under dul−1, we must have that a ∈C(u1). Thus since
a ̸∈A(ul) = C(ul) ∩O(ul)
and
a ∈C(ul),

3.3. POLISH SCHEMES
61
then necessarily a ∈O(u1)c. This, however, contradicts (3.20) for j = l since
dul(ak, c) = dul−1(ak, c) + |
1
dul−1(ak, O(ul)c) −
1
dul−1(c, O(ul)c)|
≥|
1
dul−1(ak, O(ul)c) −
1
dul−1(c, O(ul)c)| →k→∞∞
since
dul−1(ak, O(ul)c) =
inf
b∈O(ul)c) dul−1(ak, b) ≤dul−1(ak, a) →0,
where the ﬁnal inequality follows since a ∈O(u1)c. Thus a cannot be in A(ul−1). But this contradicts
the assumption that l is the smallest index for which a ̸∈A(ul) and thereby completes the proof
that the given sequence of sets is indeed a scheme.
Since at each stage every point in A must be in one of the sets A(un), if we deﬁne A(un)(x) as
the set containing x, then
{x} =
∞
\
n=1
A(un)(x)
(3.21)
and hence every singleton set {x} is in the kernel of the scheme. Observe that many of the sets in
the scheme may be empty and not all sequences u in N = ZZ+
+
will produce a point via (3.21). Since
every point in A is in the kernel of a measurable scheme, (3.21) gives a map f : A →N where f(x) is
the u for which (3.21) is satisﬁed. This scheme can be thought of as a quantizer mapping a continuous
space A into a discrete (countable) binary representation in the following sense: Suppose that we
view a point a and we encode it into a sequence of integers un; n = 1, 2, . . . which is then viewed
by a receiver. The receiver at time n uses the n integers to produce an estimate or reproduction of
the original point, say gn(un). The badness or distortion of the reproduction can be measured by
d(a, gn(un)). The goal is to decrease this distortion to 0 as n →∞. The scheme provides such a
code: Let the encoder at time n map x into the un for which x ∈A(un) and let the decoder gn(un)
simply produce an arbitrary member of A(un) (if it is not empty). This distortion will be bound
above by diam(A(un)) ≤2/n. This produces the sequence encoder mapping f : A →N.
Unfortunately the mapping f is in general into and not onto. For example, if at level n for some
n-tuple bn ∈Zn
+ A(bn) is empty, then the encoding will produce no sequences that begin with bn
since there can never be an x in A(bn). In other words, any sequence u with un = bn cannot be of
the form u = f(x) for any x ∈A. Roughly speaking, not all of the integers are useful at each level
since some of the cells may be empty. Alternatively, some integer sequences do not correspond to
(they cannot be decoded into) any point in A. We pause to tackle the problems of empty cells.
Let N1 denote all integers u1 for which A(u1) is empty. Let N2 denote the collection of all integer
pairs u2 such that A(u2) is empty but A(u1) is not. That is, once we throw out an empty cell we
no longer consider its descendants. Continue in this way to form the sets Nk consisting of all uk
for which A(uk) is empty but A(uk−1) is not. These sets are clearly all countable. Deﬁne next the
space
N0 = N −
∞
[
n=1
[
bn∈Nn
{u : un = bn} =
∞
\
n=1
\
bn∈Nn
{u : un = bn}c.
(3.22)
Thus N0 is formed by removing all thin cylinders of the form {u : un = bn} for some bn ∈Nn, that
is, all thin cylinders for which the ﬁrst part of the sequence corresponds to an empty cell. This space
has several convenient properties:

62
CHAPTER 3. BOREL SPACES AND POLISH ALPHABETS
1. For all x ∈A, f(x) ∈N0. This follows since if x ∈A, then x ∈A(un) for some un for every n;
that is, x can never lie in an empty cell, and hence f(x) cannot be in the portion of N that
was removed.
2. The set N0 is a Borel set since it is formed by removing a countable number of thin cylinders
from the entire space.
3. If we form a σ-ﬁeld of N0 in the usual way as B(N) ∩N0, then from Lemma 2.3.2 (or Lemma
2.4.2) the resulting measurable space (N0, B(N) ∩N0) is standard.
4. From Examples 3.2.1 and 3.2.2 and (3.8), the Borel ﬁeld B(N0) (using dZ+) is the same as
B(N) ∩N0. Thus with Property 3, (N0, B(N0)) is standard.
5. In the reduced space N0, all of the sequences correspond to sequences of nonempty cells in
A. Because of the properties of a scheme, any descending sequence of nonempty cells must
contain a point, and if the integer sequences ever diﬀer, the points must diﬀer. Deﬁne the
mapping g : N0 →A by
g(u) =
∞
\
n=1
A(un).
6. Since every point in A can be encoded uniquely into some integer sequence in N0, the mapping
g must be onto.
7. N0 = f(A); that is, N0 is the range space of N. To see this, observe ﬁrst that Property 1
implies that f(A) ⊂N0. Property 5 implies that every point in N0 must be of the form f(x)
for some x ∈A, and hence N0 ⊂f(A), thereby proving the claim. Note that with Property 2
this provides an example wherein the forward image f(A) of a Borel set is also a Borel set.
By construction we have for any x ∈A that g(f(x)) = x and that for any u ∈N0 f(g(u)) = u.
Thus if we deﬁne ˆf : A →N0 as the mapping f considered as having a range N0, then ˆf is onto, the
mapping g is just the inverse of the mapping ˆf, and the mappings are one-to-one. If these mappings
are measurable, then the spaces (A, B(A)) and (N0, B(N0)) are isomorphic. Thus if we can show
that the mappings ˆf and g are measurable, then (A, B(A)) will be proved standard by Lemma 2.3.3
and Property 4 because it is isomorphic to a standard space. We now prove this measurability and
complete the proof.
First consider the mapping g : N0 →A. N and hence the subspace N0 are metric spaces under
the product space metric dZ+ using the Hamming metric of Example 2.5.1; that is,
dZ+(u, v) =
∞
X
i=1
2−idH(ui, vi),
Given an ϵ > 0, ﬁnd an n such that 2/n < ϵ and then choose a δ < 2−n. If dZ+(u, v) < δ, then
necessarily un = vn which means that both g(u) and g(v) must be in the same A(un) and hence
d(g(u), g(v)) < 2/n < ϵ. Thus g is continuous and hence B(N0)-measurable.
Next consider f : A →N0. Let G = c(vn) denote the thin cylinder {u : u ∈N, un = vn} in N0.
Then by construction f −1(G) = g(G) = A(vn), which is a measurable set. From Example 3.2.2,
however, such thin cylinders generate B(N0). Thus f is measurable on a generating class and hence,
by Lemma 1.4.1, f is measurable.
We have now proved the following result.

3.3. POLISH SCHEMES
63
Theorem 3.3.2 If A is a Polish space, then the Borel space (A, B(A)) is standard.
Thus the real line, the unit interval, Euclidean space, and separable Hilbert spaces and Banach
spaces are standard. We shall encounter several speciﬁc examples in the sequel.
The development of the theorem and some simple observations provide some subsidiary results
that we now collect.
We saw that a Polish space A is the kernel of a measurable scheme and that the Borel space
(A, B(A)) is isomorphic to (N0, B(N0)), a subspace of the space of integer sequences, using the metric
dZ+. In addition, the inverse mapping in the construction is continuous. The isomorphism and its
properties depended only on the fact that that the set was the kernel of a measurable scheme. Thus
if any set F in a metric space is the kernel of a measurable scheme, then it is isomorphic to some
(N0, B(N0)), N0 ⊂N, with the metric dZ+ on N0, and the inverse mapping is continuous. We next
show that this subspace, say N0, is closed with respect to dZ+. If ak →a and ak ∈N0, then the
ﬁrst n coordinates of ak must match those of a for suﬃciently large k and hence be contained in the
thin cylinder c(an) = {u : un = an}. But these sets are all nonempty for all n (or some ak would not
be in N0) and hence a ̸∈N c
0 and a ∈N0. Thus N0 is closed. Since it is a closed subset of a Polish
space, it is also Polish with respect to the same metric. Thus any kernel of a measurable scheme is
isomorphic to a Polish space (N0 in particular) with a continuous inverse.
Conversely, suppose that F is a subset of a Polish space and that (F, B(F)) is itself isomorphic to
another Polish Borel space (B, B(B)) with metric m and suppose the inverse mapping is continuous.
Suppose that {B(un)} forms a measurable scheme for B. If f : A →B is the isomorphism and
g : B →A the continuous inverse, then A(un) deﬁned by f −1(B(un)) = g(B(un)) satisfy the
conditions (3.9) and (3.10) because inverse images preserve set theoretic operations.
Since g is
continuous, diam(B(un)) →0 implies that also diam(A(un)) →0. If the A(un) are nonempty for
all n, then so are the B(un). Since B is Polish, they must have a nonempty intersection of exactly
one point, say b, but then the intersection of the A(un) will be g(b) and hence nonempty. Thus F
is the kernel of a measurable scheme. We have thus proved the following lemma.
Lemma 3.3.1 Let A be a Polish space and F a Borel set in A. Then F is the kernel of a measurable
scheme if and only if there is an isomorphism with continuous inverse from (F, B(F)) to some Polish
Borel space.
Corollary 3.3.1 Let (A, B(A)) be the Borel space of a Polish space A. Let G be the class of all
Borel sets F with the property that F is the kernel of a measurable scheme. Then G is closed under
countable disjoint unions and countable intersections.
Proof: If Fi, i = 1,2, . . . are disjoint kernels of measurable schemes, then it is easy to say that the
union S
i Fi will also be the kernel of a measurable scheme. The cells of the ﬁrst level are all of the
ﬁrst level cells for all of the Fi. They can be reindexed to have the required form. The descending
levels follow exactly as in the individual Fi. Since the Fi are disjoint, so are the cells, and the
remaining properties follow from those of the individual sets Fi.
Deﬁne the intersection F = T∞
i=1 Fi. From the lemma there are isomorphisms fi : Fi →Ni with
Ni being Polish subspaces of N which have continuous inverses gi : Ni →Fi. Deﬁne the space
N ∞= ×∞
i=1Ni
and a subset H of N ∞containing all of those (z1, z2, . . .), zi ∈Ni, such that g1(z1) = g2(z2) = . . . and
deﬁne a mapping g : H →F by g(z1, z2, . . .) = g1(z1) (which equals gn(zn) for all n). Since the gn
are continuous, H is a closed subset of N ∞. Since H is a closed subset of N ∞and since N ∞is Polish
from Lemma 3.2.3, H is also Polish. g is continuous since the gn are, hence it is also measurable. It

64
CHAPTER 3. BOREL SPACES AND POLISH ALPHABETS
is also one-to-one, its inverse being f : F →H being deﬁned by f(a) = (f1(a), f2(a), . . .). Since the
mappings fn are measurable, so is f. Thus F is isomorphic to a Polish space and has a continuous
inverse and hence is the kernel of a measurable scheme.
2
Theorem 3.3.3 If F is a Borel set of a Polish space A, then the Borel space (F, B(F)) is standard.
Comment It should be emphasized that the set F might not itself be Polish.
Proof: From the previous corollary the collection of all Borel sets that are kernels of measurable
schemes contains countable disjoint unions and countable intersections. First, we know that each
such set is isomorphic to a Polish Borel space by Lemma 3.3.1 and that each of these Polish spaces
is standard by Theorem 3.3.1. Hence each set in this collection is isomorphic to a standard space
and so by Lemma 2.3.3 is itself standard. To complete the proof we need to show that our collection
includes all the Borel sets. Consider the closed subsets of A. Lemma 3.2.4 implies that each such
subset is itself a Polish space and hence, from Theorem 3.3.1, is the kernel of a measurable scheme.
Hence our collection contains all closed subsets of A. Since it also contains all countable disjoint
unions and countable intersections, by Lemma 3.2.5 it contains all the Borel sets. Since our collection
is itself a subset of the Borel sets, the collection must be exactly the Borel σ-ﬁeld.
2
This theorem gives the most general known class of standard Borel spaces. The development
here departs from the traditional and elegant development of Parthasarathy [55] in a variety of
ways, although the structure and use of schemes strongly resembles his use of analytic sets. The
use of schemes allowed us to map a Polish space directly onto a subset of a standard space that
was easily seen to be both a measurable set and standard itself. By showing that all Borel sets
had such schemes, the general result was obtained. The traditional method is instead to map the
original space into a subset of a known standard space that is then shown to be a Borel set and to
possess a subset isomorphic to the full standard space. By showing that a set sandwiched between
two isomorphic spaces is itself isomorphic, the result is proved. As a side result it is shown that all
Polish spaces are actually isomorphic to N or, equivalently, to M. This development perhaps adds
insight, but it requires the additional and diﬃcult proof that the one-to-one image of a Borel set is
itself a Borel set: a result that we avoided by mapping onto a subset of N formed by simply removing
a countable number of thin cylinders from it. While the approach adopted here does not yield the
rich harvest of related results of Parthasarathy, it is a simpler and more direct route to the desired
result. Alternative but often similar developments of the properties of Polish Borel spaces may be
found in Schwartz [62], Christensen [10], Bourbaki [7], Cohn [12], Mackey [46], and Bjornsson [5].

Chapter 4
Averages
4.1
Introduction
The basic focus of classical ergodic theory was the development of conditions under which sample
or time averages consisting of arithmetic means of a sequence of measurements on a random process
converged to a probabilistic or ensemble average of the measurement as expressed by an integral
of the measurement with respect to a probability measure. Theorems relating these two kinds of
averages are called ergodic theorems.
In this chapter these two types of averages are deﬁned, and several useful properties are gathered
and developed. The treatment of ensemble averages or expectations is fairly standard, except for the
emphasis on quantization and the fact that a general integral can be considered as a natural limit
of simple integrals (sums, in fact) of quantized versions of the function being integrated. In this
light, general (Lebesgue) integration is actually simpler to deﬁne and deal with than is the integral
usually taught to most engineers, the ill-behaved and complicated Riemann integral.
The only
advantage of the Riemann integral is in computation, actually evaluating the integrals of speciﬁc
functions. Although an important attribute for applications, it is not as important as convergence
and continuity properties in developing theory.
The theory of expectation is simply a special case of the theory of integration. Excellent treat-
ments may be found in Halmos [29], Ash [1], and Chung [11]. Many proofs in this chapter follow
those in these books.
The treatment of expectation and sample averages together is uncommon, but it is useful to
compare the deﬁnitions and properties of the two types of average in preparation for the ergodic
theorems to come.
As we wish to form arithmetic sums and integrals of measurements, in this chapter we emphasize
real-valued random variables.
4.2
Discrete Measurements
We begin by focusing on measurements that take on only a ﬁnite number of possible values. This
simpliﬁes the deﬁnitions and proofs of the basic properties and provides a fundamental ﬁrst step for
the more general results. Let (Ω, B, m) be a probability space, e.g., the probability space (AI, BI
A, m)
of a directly given random process with alphabet A. A real-valued random variable f : Ω→ℜwill
also be called a measurement since it is often formed by taking a mapping or function of some other
65

66
CHAPTER 4. AVERAGES
set of more general random variables, e.g., the outputs of some random process that might not have
real-valued outputs. Measurements made on such processes, however, will always be assumed to be
real.
Suppose next we have a measurement f whose range space or alphabet f(Ω) ⊂ℜof possible values
is ﬁnite. Then f is called a discrete random variable or discrete measurement or digital measurement
or, in the common mathematical terminology, a simple function. Such discrete measurements are
of fundamental mathematical importance because many basic results are easy to prove for simple
functions and, as we shall see, all real random variables can be expressed as limits of such functions.
In addition, discrete measurements are good models for many important physical measurements: No
human can distinguish all real numbers on a meter, nor is any meter perfectly accurate. Hence, in a
sense, all physical measurements have for all practical purposes a ﬁnite (albeit possibly quite large)
number of possible readings. Finally, many measurements of interest in communications systems
are inherently discrete–binary computer data and ASCII codes being ubiquitous examples.
Given a discrete measurement f, suppose that its range space is f(Ω) = {bi, i = 1, . . . , N},
where the bi are distinct. Deﬁne the sets Fi = f −1(bi) = {x : f(x) = bi}, i = 1, . . . , N. Since f
is measurable, the Fi are all members of B. Since the bi are distinct, the Fi are disjoint. Since
every input point in Ωmust map into some bi, the union of the Fi equals Ω. Thus the collection
{Fi; i = 1, 2, . . . , N} forms a partition of Ω. We have therefore shown that any discrete measurement
f can be expressed in the form
f(x) =
M
X
i=1
bi1Fi(x),
(4.1)
where bi ∈ℜ, the Fi ∈B form a partition of Ω, and 1Fi is the indicator function of Fi, i = 1, . . . , M.
Every simple function has a unique representation in this form except possibly for sets that diﬀer
on a set of measure 0.
The expectation or ensemble average or probabilistic average or mean of a discrete measurement
f : Ω→ℜas in (4.1) with respect to a probability measure m is deﬁned by
Emf =
M
X
i=0
bim(Fi).
(4.2)
The following simple lemma shows that the expectation of a discrete measurement is well deﬁned.
Lemma 4.2.1 Given a discrete measurement (measurable simple function) f : Ω→ℜdeﬁned on a
probability space (Ω, B, m), then expectation is well deﬁned; that is, if
f(x) =
N
X
i=1
bi1Fi(x) =
M
X
j=1
aj1Gj(x)
then
Emf =
N
X
i=1
bim(Fi) =
M
X
j=1
ajm(Gj).
Proof: Given the two partitions {Fi} and {Gj}, form the new partition (called the join of the two
partitions) {Fi ∩Gj} and observe that
f(x) =
N
X
i=1
M
X
j=1
cij1Fi∩Gj(x)

4.2. DISCRETE MEASUREMENTS
67
where cij = ai = bj. Thus
N
X
i=1
bim(Fi)
=
N
X
i=1
bi(
M
X
j=1
m(Gj ∩Fi))
=
N
X
i=1
M
X
j=1
bim(Gj ∩Fi)
=
N
X
i=1
M
X
j=1
cijm(Gj ∩Fi)
=
M
X
j=1
N
X
i=1
ajm(Gj ∩Fi)
=
M
X
j=1
aj(
N
X
i=1
m(Gj ∩Fi)) =
M
X
j=1
bjm(Gj).
2
An immediate consequence of the deﬁnition of expectation is the simple but useful fact that for
any event F in the original probability space,
Em1F = m(F);
(4.3)
that is, probabilities can be found from expectations of indicator functions.
The next lemma collects the basic properties of expectations of discrete measurements. These
properties will be later seen to hold for general expectations and similar ones to hold for time
averages. Thus they can be viewed as fundamental properties of averages.
Lemma 4.2.2 (a) If f ≥0 with probability 1, then Emf ≥0.
(b) Em1 = 1.
(c) Expectation is linear; that is, for any real α, β and any discrete measurements f and g,
Em(αf + βg) = αEmf + βEmg.
(d ) If f is a discrete measurement, then
|Emf| ≤Em|f|.
(e) Given two discrete measurements f and g for which f ≥g with probability 1 (m({x : f(x) ≥
g(x)}) = 1), then
Emf ≥Emg.
Comments: The ﬁrst three statements bear a strong resemblance to the ﬁrst three axioms of prob-
ability and will be seen to follow directly from those axioms: Since probabilities are nonnegative,
normalized, and additive, expectations inherit similar properties. A limiting version of additivity will
be postponed. Properties (d) and (e) are useful basic integration inequalities. The following proofs
are all simple, but they provide some useful practice manipulating integrals of discrete functions.

68
CHAPTER 4. AVERAGES
Proof (a) If f ≥0 with probability 1, then bi ≥0 in (4.1) and Emf ≥0 from (4.2) and the
nonnegativity of probability.
(b) Since 1 = 1Ω, Em1 = 1m(Ω) = 1 and the property follows from the normalization of
probability.
(c) Suppose that
f =
X
i
bi1Fi,
g =
X
j
ai1Gi.
Again consider the join of the two partitions and write
f =
X
i
X
j
bi1Fi∩Gj
g =
X
i
X
j
aj1Fi∩Gj.
Then
αf + βg =
X
i
X
j
(αbi + βaj)1Fi∩Gj
is a discrete measurement with the partition {Fi ∩Gj} and hence
Em(αf + βg)
=
X
i
X
j
(αbi + βaj)m(Fi ∩Gj)
=
α
X
i
X
j
bim(Fi ∩Gj) + β
X
i
X
j
ajm(Fi ∩Gj)
=
α
X
i
bim(Fi) + β
X
j
ajm(Gj),
from the additivity of probability. This proves the claim.
(d) Let J denote those i for which bi in (4.1) are nonnegative. Then
Emf
=
X
i
bim(Fi)
=
X
i∈J
|bi|m(Fi) −
X
i̸∈J
|bi|m(Fi)
≤
X
i
|bi|m(Fi) = Em|f|.
(e) This follows from (a) applied to f −g.
2
In the next section we introduce the notion of quantizing arbitrary real measurements into discrete
approximations in order to extend the deﬁnition of expectation to such general measurements by
limiting arguments.
4.3
Quantization
Again let (Ω, B, m) be a probability space and f : Ω→ℜa measurement, that is, a real-valued ran-
dom variable or measurable real-valued function. The following lemma shows that any measurement
can be expressed as a limit of discrete measurements and that if the measurement is nonnegative,
then the simple functions converge upward.

4.3. QUANTIZATION
69
Lemma 4.3.1 Given a real random variable f, deﬁne the sequence of quantizers qn : ℜ→ℜ, n =
1, 2, . . ., as follows:
qn(r) =





n
r ≥n
(k −1)2−n
(k −1)2−n ≤r < k2−n, k = 1, 2, . . . , n2n
−(k −1)2−n
−k2−n ≤r < −(k −1)2−n, k = 1, 2, . . . , n2n
−n
r < −n .
The sequence of quantized measurements have the following properties:
(a)
f(x) = lim
n→∞qn(f(x)), all x.
(b) If f(x) ≥0, all x, then f ≥qn+1(f) ≥qn(f), all n; that is, qn(x) ↑f.
More generally,
|f| ≥|qn+1(f)| ≥|qn(f)|.
(c) If f ≥g ≥0, then also qn(f) ≥qn(g) ≥0 for all n.
(d) The magnitude quantization error |qn(f) −f| satisﬁes
|qn(f) −f| ≤|f|
(4.4)
|qn(f) −f| ↓0;
(4.5)
that is, the magnitude error is monotonically nonincreasing and converges to 0.
(e) If f is G-measurable for some sub-σ-ﬁeld G, than so are the fn. Thus if f is G-measurable, then
there is a sequence of G-measurable simple functions which converges to f.
Proof: Statements (a)-(c) are obvious from the the construction. To prove (d) observe that if x is
ﬁxed, then either f is nonnegative or it is negative. If it is nonnegative, then so is qn(f(x)) and
|qn(f) −f| = f −qn(f). This is bound above by f and decreases to 0 since qn(f) ↑f from part (b).
If f(x) is negative, then so is qn(f) and |qn(f) −f| = qn(f) −f. This is bound above by −f = |f|
and decreases to 0 since qn(f) ↓f from part (b). To prove (e), suppose that f is G-measurable and
hence that inverse images of all Borel sets under f are in G. If qn, n = 1, 2, . . . is the quantizer
sequence, then the functions qn(f) are also G-measurable (and hence also B(ℜ) measurable) since
the inverse images satisfy
qn(f)−1(F)
=
{ω : qn(f)(ω) ∈F}
=
{ω : qn(f(ω)) ∈F}
=
{ω : f(ω) ∈q−1
n (F)}
=
f −1(q−1
n (F)) ∈G
since all inverse images under f of events F in B(ℜ) are in G.
2
Note further that if |f| ≤K for some ﬁnite K, then for n > K |f(x) −qn(f(x))| ≤2−n.
Measurability
As a prerequisite to the study of averages, we pause to consider some special properties of real-valued
random variables. We will often be interested in the limiting properties of a sequence fn of integrable
functions. Of particular importance is the measurability of limits of measurements and conditions
under which we can interchange limits and expectations. In order to state the principal convergence

70
CHAPTER 4. AVERAGES
results, we require ﬁrst a few deﬁnitions. Given a sequence an, n = 1, 2, . . . of real numbers, deﬁne
the limit supremum or upper limit or limit superior
lim sup
n→∞an = inf
n sup
k≥n
ak
and the limit inﬁmum or lower limit or limit inferior
lim inf
n→∞an = sup
n
inf
k≥n ak.
In other words, lim supn→∞an = a if a is the largest number for which an has a subsequence,
say an(k), k = 1, 2, . . . converging to a. Similarly, lim infn→∞an is the smallest limit attainable
by a subsequence. When clear from context, we shall often drop the n →∞and simply write
lim sup an or lim inf an. The limit of a sequence an, say limn→∞an = a, will exist if and only if
lim sup an = lim inf an = a, that is, if all subsequences converge to the same thing–the limit of the
sequence.
Lemma 4.3.2 Given a sequence of measurements fn, n = 1, 2, . . ., then f = lim supn→∞fn and
f = lim infn→∞fn are also measurements, that is, are also measurable functions. If limn→∞fn
exists, then it is a measurement. If f and g are measurements, then so are the functions f + g,
f −g, |f −g|, min(f, g), and max(f, g); that is, all these functions are measurable.
Proof: To prove a real-valued function f : Ω→ℜmeasurable, it suﬃces from Lemma 1.4.1 to show
that f −1(F) ∈B for all F in a generating class. From Chapter 3 ℜis generated by the countable
collection of open spheres of the form {x : |x−ri| < 1/n}; i = 1, 2, . . .; n = 1, 2, . . ., where the ri run
through the rational numbers. More simply, we can generate the Borel σ-ﬁeld of ℜfrom the class of
all sets of the form (−∞, ri] = {x : x ≤ri}, with the same ri. This is because the σ-ﬁeld containing
these half-open intervals also contains the open intervals since it contains the half-open intervals
(a, b] = (−∞, b] −(−∞, a]
with rational endpoints, the rationals themselves:
a =
∞
\
n=1
(a −1
n, a],
and hence all open intervals with rational endpoints
(a, b) = (a, b] −b.
Since this includes the spheres described previously, this class generates the σ-ﬁeld. Consider the
limit supremum f. For f ≤r we must have for any ϵ > 0 that fn ≤r + ϵ for all but a ﬁnite number
of n. Deﬁne
Fn(ϵ) = {x : fn(x) ≤r + ϵ}.
Then the collection of x for which fn(x) ≤r + ϵ for all but a ﬁnite number of n is
F(ϵ) =
∞
[
n=1
∞
\
k=n
Fn(ϵ).

4.4. EXPECTATION
71
This set is called the limit inﬁmum of the Fn(ϵ) in analogy to the notion for sequences. As this
type of set will occasionally be useful, we pause to formalize the deﬁnition: Given a sequence of sets
Fn, n = 1, 2, . . ., deﬁne the limit inﬁmum of the sequence of sets by
F = lim inf
n→∞Fn =
∞
[
n=1
∞
\
k=n
Fn(ϵ) = {x : x ∈Fn for all but a ﬁnite number of n}.
(4.6)
Since the fn are measurable, Fn(ϵ) ∈B for all n and ϵ and hence F(ϵ) ∈B. Since we must be in
F(ϵ) for all ϵ for f ≤r,
F = {x : f(x) ≤r} =
∞
\
n=1
F( 1
n).
Since the F(1/n) are all measurable, so is F. This proves the measurability of f. That for f follows
similarly. If the limit exists, then f = f and measurability follows from the preceding argument.
The remaining results are easy if the measurements are discrete. The conclusions then follow from
this fact and Lemma 4.3.1.
Exercises
1. Fill in the details of the preceding proof; that is, show that sums, diﬀerences, minima, and
maxima of measurable functions are measurable.
2. Show that if f and g are measurable, then so is the product fg. Under what conditions is the
division f/g measurable?
3. Prove the following corollary to the lemma:
Corollary 4.3.1 Let fn be a sequence of measurements deﬁned on (Ω, B, m). Deﬁne Ff as
the set of x ∈Ωfor which limn→∞fn exists; then 1F is a measurement and F ∈B.
4. Is it true that if F is the limit inﬁmum of a sequence Fn of events, then
1F (x) = lim inf
n→∞1Fn(x)?
4.4
Expectation
We now deﬁne expectation for general measurements in two steps. If f ≥0, then choose as in
Lemma 4.3.1 the sequence of asymptotically accurate quantizers qn and deﬁne
Emf = lim
n→∞Em(qn(f)).
(4.7)
Since the qn are discrete measurements on f, the qn(f) are discrete measurements on Ω(qn(f)(x) =
qn(f(x)) is a simple function), and hence the individual expectations are well deﬁned. Since the
qn(f) are nondecreasing, so are the Em(qn(f)) from Lemma 4.2.2(e). Thus the sequence must either
converge to a ﬁnite limit or grow without bound, in which case we say it converges to ∞. In both
cases the expectation Emf is well deﬁned, although it may be inﬁnite. It is easily seen that if f is
a discrete measurement, then (4.7) is consistent with the previous deﬁnition (4.1).

72
CHAPTER 4. AVERAGES
If f is an arbitrary real random variable, deﬁne its positive and negative parts f+(x) = max(f(x), 0)
and f −(x) = −min(f(x), 0) so that f(x) = f+(x) −f −(x) and set
Emf = Emf + −Emf −
(4.8)
provided this does not have the form +∞−∞, in which case the expectation does not exist. Observe
that an expectation exists for all nonnegative random variables, but that it may be inﬁnite. The
following lemma provides an alternative deﬁnition of expectation as well as a limiting theorem that
shows that there is nothing magic about the particular sequence of discrete measurements used in
the deﬁnition.
Lemma 4.4.1 (a) If f ≥0, then
Emf =
sup
discrete g:g≤f
Emg.
(b) Suppose that f ≥0 and fn is any nondecreasing sequence of discrete measurements for which
limn→∞fn(x) = f(x), then
Emf = lim
n→∞Emfn.
Comments: Part (a) states that the expected value of a nonnegative measurement is the supremum
of the expected values of discrete measurements that are no greater than f. This property is often
used as a deﬁnition of expectation. Part (b) states that any nondecreasing sequence of asymptotically
accurate discrete approximations to f can be used to evaluate the expectation. This is a special
case of the monotone convergence theorem to be considered shortly.
Proof: Since the qn(f) are all discrete measurements less than or equal to f, clearly
Emf = lim
n→∞Emqn(f) ≤sup
g Emg,
where the supremum is over all discrete measurements less than or equal to f. For ϵ > 0 choose a
discrete measurement g with g ≤f and
Emg ≥sup
g Emg −ϵ.
From Lemmas 4.3.1 and 4.2.2
Emf = lim
n→∞Emqn(f) ≥lim
n→∞Emqn(g) = Emg ≥sup
g Emg −ϵ.
Since ϵ is arbitrary, this completes the proof of (a).
To prove (b), use (a) to choose a discrete function g = P
i gi1Gi that nearly gives Emf; that is,
ﬁx ϵ > 0 and choose g ≤f so that
Emg =
X
i
gim(Gi) ≥Emf −ϵ
2.
Deﬁne for all n the set
Fn = {x : fn(x) ≥g(x) −ϵ
2}.
Since g(x) ≤f(x) and fn(x) ↑f(x) by assumption, Fn ↑Ω, and hence the continuity of probability
from below implies that for any event H,
lim
n→∞m(Fn ∩H) = m(H).
(4.9)

4.4. EXPECTATION
73
Clearly 1 ≥1Fn and for each x ∈Fn fn ≥g −ϵ. Thus
fn ≥1Fnfn ≥1Fn(g −ϵ),
and all three measurements are discrete. Application of Lemma 4.2.2(e) therefore gives
Emfn ≥Em(1Fnfn) ≥Em(1Fn(g −ϵ)).
which, from the linearity of expectation for discrete measurements (Lemma 4.2.2(c)), implies that
Emfn ≥Em(1Fng) −ϵ
2m(Fn) ≥Em(1Fng) −ϵ
2
= Em(1Fn
X
i
gi1Gi) −ϵ
2 = Em(
X
i
gi1Gi∩Fn) −ϵ
2
=
X
i
gim(Fn ∩Gi) −ϵ
2.
Taking the limit as n →∞, using (4.9) for each of the ﬁnite number of probabilities in the sum,
lim
n→∞Emfn ≥
X
i
gim(Gi) −ϵ
2 = Emg −ϵ
2 ≥Emf −ϵ,
which completes the proof.
2
With the preceding alternative characterizations of expectation in hand, we can now generalize
the fundamental properties of Lemma 4.2.2 from discrete measurements to more general measure-
ments.
Lemma 4.4.2 (a) If f ≥0 with probability 1, then Emf ≥0.
(b) Em1 = 1.
(c) Expectation is linear; that is, for any real α, β and any measurements f and g,
Em(αf + βg) = αEmf + βEmg.
(d) Emf exists and is ﬁnite if and only if Em|f| is ﬁnite and
|Emf| ≤Em|f|.
(e) Given two measurements f and g for which f ≥g with probability 1, then
Emf ≥Emg.
Proof: (a) follows by deﬁning qn(f) as in Lemma 4.3.1 and applying Lemma 4.2.2(a) to conclude
Em(qn(f)) ≥0, which in the limit proves (a). (b) follows from Lemma 4.2.2(b). To prove (c), ﬁrst
consider nonnegative functions by using Lemma 4.2.2(c) to conclude that
Em(αqn(f) + βqn(g)) = αEm(qn(f)) + βEm(qn(G))
The right-hand side converges upward to αEmf+ βEmg. By the linearity of limits, the sequence
αqn(f) + βqn(g) converges upward to f + g, and hence, from Lemma 4.4.1, the left-hand side

74
CHAPTER 4. AVERAGES
converges to Em(αf + βg), proving the result for nonnegative functions. Note that it was to prove
this piece that we ﬁrst considered Lemma 4.4.1 and hence focused on a nontrivial limiting property
before proving the fundamental properties of this lemma.
The result for general measurements
then follows by breaking the functions into positive and negative parts and using the nonnegative
measurement result.
Part (d) follows from the fact that f = f+−f −and |f| = f ++f −. The expectation Em|f| exists
and is ﬁnite if and only if both positive and negative parts have ﬁnite expectations, this implying
that Emf exists and is also ﬁnite. Conversely, if Emf exists and is ﬁnite, then both positive and
negative parts must have ﬁnite integrals, this implying that |f| must be ﬁnite. If the integrals exist,
clearly
Em|f| = Emf + + Emf −≥Emf + ≥Emf + −Emf −= Emf.
Part (e) follows immediately by application of part (a) to f −g.
2
Integration
The expectation is also called an integral and is denoted by any of the following:
Emf =
Z
f dm =
Z
f(x) dm(x) =
Z
f(x) m(dx).
The subscript m denoting the measure with respect to which the expectation is taken will occasion-
ally be omitted if it is clear from context.
A measurement f is said to be integrable or m-integrable if Emf exists and is ﬁnite. Deﬁne
L1(m) to be the space of all m-integrable functions. Given any m-integrable f and an event B,
deﬁne
Z
B
f dm =
Z
f(x)1B(x) dm(x).
Two random variables f and g are said to be equal m-almost-everywhere or equal m-a.e. or
equal with m-probability one if m(f = g) = m({x : f(x) = g(x)}) = 1. The m- is dropped if it is
clear from context.
The following corollary to the deﬁnition of expectation and its linearity characterizes the ex-
pectation of an arbitrary integrable random variable as the limit of the expectations of the simple
measurements formed by quantizing the random variables. This is an example of an integral limit
theorem, a result giving conditions under which the integral and limit can be interchanged.
Corollary 4.4.1 Let qn be the quantizer sequence of Lemma 4.3.1 If f is m-integrable, then
Emf = lim
n→∞Emqn(f)
and
lim
n→∞Em|f −qn(f)| = 0.
Proof: If f is nonnegative, the two results are equivalent and follow immediately from the deﬁnition
of expectation and the linearity of Lemma 4.4.2(c). For general f qn(f) = qn(f +) −qn(f −) and
qn(f +) ↑f + and qn(f −) ↑f −by construction, and hence from the nonnegative f result and the
linearity of expectation that
E|f −qn| = E(f + −qn(f +)) + E(f −−qn(f −)) →n→∞0

4.4. EXPECTATION
75
and
E(f −qn) = E(f + −qn(f +)) −E(f −−qn(f −)) →n→∞0.
2
The lemma can be used to prove that expectations have continuity properties with respect to
the underlying probability measure:
Corollary 4.4.2 If f is m-integrable, then
lim
m(F )→0
Z
F
f dm = 0;
that is, given ϵ > 0 there is a δ > 0 such that if m(F) < δ, then
Z
F
f dm < ϵ.
Proof: If f is a simple function of the form (4.7), then
Z
F
f dm = E(1F
M
X
i=1
bi1Fi) =
M
X
i=1
biE1F ∩Fi
=
M
X
i=1
bim(F ∩Fi) ≤
M
X
i=1
bim(F) →m(F )→0 0.
If f is integrable, let qn be the preceding quantizer sequence. Given ϵ > 0 use the previous corollary
to choose k large enough to ensure that
Em|f −qk(f)| < ϵ.
From the preceding result for simple functions
|
Z
F
f dm| = |
Z
f1F dm −
Z
qk(f) 1F dm +
Z
qk(f)1F dm|
≤|
Z
(f −qk(f) 1F dm| + |
Z
qk(f) 1F dm|
≤
Z
|f −qk(f)|1F dm + |
Z
F
qk(f) dm|
≤ϵ + |
Z
F
qk(f) dm| →m(F )→0 ϵ.
Since ϵ is arbitrary, the result is proved.
2
The preceding proof is typical of many results involving expectations: the result is easy for
discrete measurements and it is extended to general functions via limiting arguments. This continuity
property has two immediate consequences, the proofs of which are left as exercises.
Corollary 4.4.3 If f is nonnegative m-a.e., m-integrable, and Emf ̸= 0, then the set function P
deﬁned by
P(F) =
R
F f dm
Emf
is a probability measure.

76
CHAPTER 4. AVERAGES
Corollary 4.4.4 If f is m-integrable, then
lim
r→∞
Z
f≥r
f dm = 0.
Corollary 4.4.5 Suppose that f ≥0 m-a.e. Then
R
f dm = 0 if and only if f = 0 m-a.e.
Corollary 4.4.5 shows that we can consider L1(m) to be a metric space with respect to the metric
d(f, g) =
R
|f −g| dm if we consider measurements identical if m(f = g) = 0.
The following lemma collects some simple inequalities for expectations.
Lemma 4.4.3 (a) (The Markov Inequality) Given a random variable f with f ≥0 m-a.e., then
for a > 0
m(f ≥a) ≤Emf
a
.
Furthermore, for p > 0
m(f ≥a) ≤Em(f p)
ap
.
((b) (Tchebychev’s Inequality) Given a random variable f with expected value Ef, then if a > 0,
m(|f −Ef| ≥a) ≤Em(|f −Ef|2)
a2
.
((c) If E(f 2) = 0, them m(f = 0) = 1.
((d) (The Cauchy-Schwarz Inequality) Given random variables f and g, then
|E(fg)| ≤E(f 2)1/2E(g2)1/2.
Proof: (a) Let 1{f≥a} denote the indicator function of the event {x : f(x) ≥a}. We have that
Ef = E(f(1{f≥a} + 1{f<a})) = E(f1{f≥a}) + E(f1{f<a})
≥aE1{f≥a} = am(f ≥a),
proving Markov’s inequality. The generalization follows by substituting fp for f and using the fact
that for p > 0, f p > ap if and only if f > a. (b) Tchebychev’s inequality follows by replacing f by
|f −Ef|2. (c) From the general Markov inequality, if E(f2) = 0, then for any ϵ > 0,
m(f 2) ≥ϵ2) ≤E(f 2)
ϵ2
= 0
and hence m(|f| ≤1/n) = 1 for all n. Since {ω : |f(ω)| ≤1/n} ↓{ω : f(ω) = 0}, from the continuity
of probability, m(f = 0) = 1. (d) To prove the Cauchy-Schwarz inequality set a = E(f2)1/2 and
b = E(g2)1/2. If a = 0, then f = 0 a.e. from (c) and the inequality is trivial. Hence we can assume
that both a and b are nonzero. In this case
0 ≤E(f
a ± g
b )2 = 2 ± 2E(fg)
ab
.
2

4.4. EXPECTATION
77
A sequence fn is said to be uniformly integrable if the limit of Corollary 4.4.4 holds uniformly in
n; that is, if
lim
r→∞
Z
|fn|≥r
|fn| dm = 0
uniformly in n, i.e., if given ϵ > 0 there exists an R such that
Z
|fn|≥r
|fn| dm < ϵ, all r ≥R and all n.
Alternatively, the sequence fn is uniformly integrable if
lim
r→∞sup
n
Z
|fn|≥r
|fn| dm = 0.
The following lemma collects several examples of uniformly integrable sequences.
Lemma 4.4.4 If there is an integrable function g (e.g., a constant) for which |fn| ≤g, all n, then
the fn are uniformly integrable. If a > 0 and supn E(|fn|1+a) < ∞, then the fn are uniformly
integrable. If |fn| ≤|gn| for all n and the gn are uniformly integrable, then so are the fn. If a
sequence f 2
n is uniformly integrable, then so is the sequence fn.
Proof: The ﬁrst statement is immediate from the deﬁnition and from Corollary 4.4.3. The second
follows from Markov’s inequality. The third example follows directly from the deﬁnition. The fourth
example follows from the deﬁnition and the Cauchy-Schwarz inequality.
2
The following lemma collects the basic asymptotic integration results on integrating to the limit.
Lemma 4.4.5 (a) (Monotone Convergence Theorem)
If fn, n = 1, 2, . . ., is a sequence of measurements such that fn ↑f and fn ≥0, m-a.e., then
lim
n→∞Efn = Ef.
(4.10)
In particular, if limn→∞Efn < ∞, then E(limn→∞fn) exists and is ﬁnite.
((b) (Extended Monotone Convergence Theorem)
If there is a measurement h such that
Z
h dm > −∞
(4.11)
and if fn, n = 1, 2, . . ., is a sequence of measurements such that fn ↑f and fn ≥h, m-a.e.,
then (4.10) holds. Similarly, if there is a function h such that
Z
h dm < ∞
(4.12)
and if fn, n = 1, 2, . . ., is a sequence of measurements such that fn ↓f and fn ≤h, m-a.e.,
then (4.10) holds.

78
CHAPTER 4. AVERAGES
((c) (Fatou’s Lemma)
If fn is a sequence of measurements and fn ≥h, where h satisﬁes (4.11), then
Z
(lim inf
n→∞fn) dm ≤lim inf
n→∞
Z
fn dm.
(4.13)
If fn is a sequence of measurements and fn ≤h, where h satisﬁes (4.12), then
lim sup
n→∞
Z
fn dm ≤
Z
(lim sup
n→∞fn) dm.
(4.14)
((d) (Extended Fatou’s Lemma) If fn, n = 1, 2, . . . is a sequence of uniformly integrable functions,
then
E(lim inf
n→∞fn) ≤lim inf
n→∞Efn ≤lim sup
n→∞Efn ≤E(lim sup
n→∞fn).
(4.15)
Thus, for example, if the limit of a sequence fn of uniformly integrable functions exists a.e.,
then that limit is integrable and
E( lim
n→∞fn) = lim
n→∞Efn.
((e) (Dominated Convergence Theorem) If fn, n = 1, 2, . . ., is a sequence of measurable functions
that converges almost everywhere to f and if h is an integrable function for which |fn| ≤h,
then f is integrable and (4.10) holds.
Proof: (a) Part (a) follows in exactly the same manner as the proof of Lemma 4.4.1(b) except that
the measurements fn need not be discrete and Lemma 4.4.2 is invoked instead of Lemma 4.2.2.
(b) If
R
h dm is inﬁnite, then so is
R
fn dm and
R
f dm. Hence assume it is ﬁnite. Then fn−h ≥0
and fn −h ↑f −h and the result follows from (a). The result for decreasing functions follows by a
similar manipulation.
(c) Deﬁne gn = infk≥n fk and g = lim inf fn. Then gn ≥h for all n, h satisﬁes (4.11), and gn ↑g.
Thus from the fact that gn ≤fn and part (b)
Z
fn dm ≥
Z
gn dm ↑
Z
g dm =
Z
(lim inf
n→∞fn) dm,
which proves (4.13). Eq. (4.14) follows in like manner.
(d) For any r > 0,
Z
fndm =
Z
fn−r
fn dm +
Z
fn≥−r
fn dm.
Since the fn are uniformly integrable, given ϵ > 0 r can be chosen large enough to ensure that the
ﬁrst integral on the right has magnitude less than ϵ. Thus
Z
fn dm ≥−ϵ +
Z
1fn≥−rfn dm.
The integrand on the left is bound below by −r, which is integrable, and hence from part (c)
lim inf
n→∞
Z
fn dm ≥−ϵ +
Z
(lim inf
n→∞1fn≥−rfn) dm

4.4. EXPECTATION
79
≥−ϵ +
Z
(lim inf
n→∞fn) dm,
where the last inequality follows since 1fn≥−rfn ≥fn. Since ϵ is arbitrary, this proves the limit
inﬁmum part of (4.15). The limit supremum part follows similarly. (e) Follows immediately from
(d) and Lemma 4.4.4.
2
To complete this section we present several additional properties of expectation that will be used
later. The proofs are simple and provide practice with manipulating the preceding fundamental
results. The ﬁrst result provides a characterization of uniformly integrable functions.
Lemma 4.4.6 A sequence of integrable functions fn is uniformly integrable if and only if the follow-
ing two conditions are satisﬁed: (i) The expectations E(|fn|) are bounded; that is, there is some ﬁnite
K that bounds all of these expectations from above. (ii) The continuity in the sense of Corollary
4.4.4 of the integrals of fn is uniform; that is, given ϵ > 0 there is a δ > 0 such that if m(F) < δ,
then
Z
F
|fn| dm < ϵ, all n.
Proof: First assume that (i) and (ii) hold. From Markov’s inequality and (i) it follows that
m(|fn| ≥r) ≤r−1
Z
|fn| dm ≤K
r →r→∞0
uniformly in n. This plus uniform continuity then implies uniform integrability. Conversely, assume
that the fn are uniformly integrable. Fix ϵ > 0 and choose r so large that
Z
|fn|≥r
|fn| dm < ϵ/2, all n.
Then
E|fn| =
Z
|fn|<r
|fn| dm +
Z
|fn|≥r
|fn| dm ≤ϵ/2 + r
uniformly in n, proving (i). Choosing F so that m(F) < ϵ/(2r),
Z
F
|fn| dm =
Z
F ∩∥fn|≥r
|fn| dm +
Z
F ∩∥fn|<r
|fn| dm
≤
Z
|fn|≥r
|fn| dm + rm(F) ≤ϵ/2 + ϵ/2 = ϵ,
proving (ii) since ϵ is arbitrary.
2
Corollary 4.4.6 If a sequence fn is uniformly integrable, then so is the sequence
n−1
n−1
X
i=0
fi.
Proof: If the fn are uniformly integrable, then there exists a ﬁnite K such that E(|fn|) < K, and
hence
E(| 1
n
n−1
X
i=0
fi|) ≤1
n
n−1
X
i=0
E|fi| < K.

80
CHAPTER 4. AVERAGES
The fn are also uniformly continuous and hence given ϵ there is a δ such that if m(F) ≤δ, then
Z
F
fi dm ≤ϵ, all i,
and hence
Z
F
|n−1
n−1
X
i=0
fi| dm ≤n−1
n−1
X
i=0
Z
F
|fi| dm ≤ϵ, all n.
Applying the previous lemma then proves the corollary.
2
The ﬁnal lemma of this section collects a few ﬁnal journeyman results on expectation. The proofs
are left as exercises.
Lemma 4.4.7 (a) Given two integrable functions f and g, if
Z
F
f dm ≤
Z
F
gdm all events F,
then f ≤g, m-a.e. If the preceding relation holds with equality, then f = g m-a.e.
((b) (Change of Variables Formula) Given a measurable mapping f from a standard probability space
(Ω, S, P) to a standard measurable space (Γ, B) and a real-valued random variable g : Γ →ℜ,
let Pf −1 denote the probability measure on (Γ, B) deﬁned by Pf−1(F) = P(f −1F), all F ∈B.
Then
Z
g(y) dPf −1(y) =
Z
g(f(x)) dP(x);
that is, if either integral exists so does the other and they are equal.
((c) (Chain Rule) Consider a probability measure P deﬁned as in Corollary 4.4.3 as
P(F) =
R
F f dm
Emf
for some m-integrable f with nonzero expectation. If g is q-integrable, then
EP g = Em (gf).
Exercises
1. Prove that (4.7) yields (4.1) as a special case if f is a discrete measurement.
2. Prove Corollary 4.4.3.
3. Prove Corollary 4.4.4.
4. Prove Corollary 4.4.5.
5. Prove Lemma 4.4.7.
Hint for proof of (b) and (c): ﬁrst consider simple functions and then limits.
6. Show that for a nonnegative random variable X
∞
X
n=1
Pr(X ≥n) ≤EX.

4.5. TIME AVERAGES
81
7. Verify the following relations:
m(F∆G) = Em[(1F −1G)2]
m(F ∩G) = Em(1F 1G)
m(F ∪G) = Em[max(1F , 1G)].
8. Verify the following:
m(F ∩G) ≤
p
m(F)m(G)
m(F∆G) ≥|
p
m(F) −
p
m(G|2.
9. Given disjoint events Fn, n = 1, 2, . . ., and a measurement f, prove that integrals are countably
additive in the following sense:
Z
∪∞
n=1Fn
fdm =
∞
X
n=1
Z
Fn
f dm
10. Prove that if (
R
F f dm)/m(F) ∈[a, b] for all events F, then f ∈[a, b] m-a.e.
11. Show that if f, g ∈L2(m), then
Z
|fg| dm ≤1
2
Z
f 2dm + 1
2
Z
g2 dm.
12. Show that if f ∈L2(m), then
Z
|f|>r
|f| dm ≤(
Z
f 2 dm)1/2(m({|f| > r}))1/2.
4.5
Time Averages
Given a random process {Xn} we will often wish to apply a given measurement repeatedly at
successive times, that is, to form a sequence of outputs of a given measurement. For example, if
the random process has a real alphabet corresponding to a voltage reading, we might wish to record
the successive outputs of the meter and hence the successive random process values Xn. Similarly,
we might wish to record successive values of the energy in a unit resistor: X2
n. We might wish to
estimate the autocorrelation of lag or delay k via successive values of the form XnXn+k. Although all
such sequences are random and hence themselves random processes, we shall see that under certain
circumstances the arithmetic or Cero means of such sequences of measurements will converge. Such
averages are called time averages or sample averages and this section is devoted to their deﬁnition
and to a summary of several possible forms of convergence.
A useful general model for a sequence of measurements is provided by a dynamical system
(AI, BI
A, m, T), that is, a probability space, (AI, BI
A, m) and a transformation T : AI →AI (such as
the shift), together with a real-valued measurement f : AI →ℜ. Deﬁne the corresponding sequence
of measurements fT i, i = 0, 1, 2, . . . by fT i(x) = f(T ix); that is, fT i corresponds to making the
measurement f at time i. For example, assume that the dynamical system corresponds to a real-
alphabet random process with distribution m and that T is the shift on AZ. If we are given a
sequence x = (x0, x1, . . .) in AZ, where Z = {0, 1, 2, . . .}, if f(x) = Π0(x) = x0, the “time-zero”

82
CHAPTER 4. AVERAGES
coordinate function, then f(T ix) = xi, the outputs of the directly given process with distribution
m. More generally, f could be a ﬁltered or coded version of an underlying process. The key idea is
that a measurement f made on a sample sequence at time i corresponds to taking f on the ith shift
of the sample sequence.
Given a measurement f on a dynamical system (Ω, B, m, T), the nth order time average or sample
average < f >n is deﬁned as the arithmetic mean or C´esaro mean of the measurements at times
0, 1, 2, . . . , n −1; that is,
< f >n (x) = n−1
n−1
X
i=0
f(T ix),
(4.16)
or, more brieﬂy,
< f >n= n−1
n−1
X
i=0
fT i.
For example, in the case with f being the time-zero coordinate function,
< f >n (x) = n−1
n−1
X
i=0
xi,
the sample mean or time-average mean of the random process. If f(x) = x0xk, then
< f >n (x) = n−1
n−1
X
i=0
xixi+k,
the sample autocorrelation of lag k of the process. If k = 0, this is the time average mean of the
process since if the xk represent voltages, x2
k is the energy across a resistor at time k.
Finite order sample averages are themselves random variables, but the following lemma shows
that they share several fundamental properties of expectations.
Lemma 4.5.1 Sample averages have the following properties for all n:
(a) If fT i ≥0 with probability 1, i = 0, 1, . . . , n −1 then < f >n≥0.
((b) < 1 >n= 1.
((c) For any real α, β and any measurements f and g,
< αf + βg >n= α < f >n +β < g >n .
((d)
| < f >n | ≤< |f| >n .
((e) Given two measurements f and g for which f ≥g with probability 1, then
< f >n≥< g >n .
Proof: The properties all follow from the properties of summations. Note that for (a) we must
strengthen the requirement that f is nonnegative a.e. to the requirement that all of the fT i are
nonnegative a.e. Alternatively, this is a special case of Lemma 4.4.1 in which for a particular x

4.5. TIME AVERAGES
83
we deﬁne a probability measure on measurements taking values f(T kx) with probability 1/n for
k = 0, 1, . . . , n −1.
2
We will usually be interested not in the ﬁnite order sample averages, but in the long term
or asymptotic sample averages obtained as n →∞.
Since the ﬁnite order sample averages are
themselves random variables, we need to develop notions of convergence of random variables (or
measurable functions) in order to make such limits precise.
There are, in fact, several possible
approaches.
In the remainder of this section we consider the notion of convergence of random
variables closest to the ordinary deﬁnition of a limit of a sequence of real numbers. In the next
section three other forms of convergence are considered and compared.
Given a measurement f on a dynamical system (Ω, B, m, T), let Ff denote the set of all x ∈Ω
for which the limit
lim
n→∞
1
n
n−1
X
k=0
f(T kx)
exists. For all such x we deﬁne the time average or limiting time average or sample average < f >
by this limit; that is,
f = lim
n→∞< f >n .
Before developing the properties of < f >, we show that the set Ff is indeed measurable. This
follows from the measurability of limits of measurements, but it is of interest to prove it directly.
Let fn denote < f >n and deﬁne the events
Fn(ϵ) = {x : |fn(x) −f(x)| ≤ϵ}
(4.17)
and deﬁne as in (4.6)
F(ϵ)
=
lim inf
n→∞Fn(ϵ) =
∞
[
n=1
∞
\
k=n
Fk(ϵ)
(4.18)
=
{x : x ∈Fn(ϵ) for all but a ﬁnite number of n}.
(4.19)
Then the set
Ff =
∞
\
k=1
F(1
k )
(4.20)
is measurable since it is formed by countable set theoretic operations on measurable sets.
The
sequence of sample averages < f >n will be said to converge to a limit < f > almost everywhere or
with probability one if m(Ff) = 1.
The following lemma shows that limiting time averages exhibit behavior similar to the expecta-
tions of Lemma 4.4.2.
Lemma 4.5.2 Time averages have the following properties when they are well deﬁned; that is, when
the limits exist:
(a) If fT i ≥0 with probability 1 for i = 0, 1, . . ., then < f >≥0.
((b) < 1 >= 1.
((c) For any real α, β and any measurements f and g,
< αf + βg >= α < f > +β < g > .

84
CHAPTER 4. AVERAGES
(d)
| < f > | ≤< |f| > .
((e) Given two measurements f and g for which f ≥g with probability 1, then
< f >≥< g > .
Proof: The proof follows from Lemma 4.5.1 and the properties of limits. In particular, (a) is proved
as follows: If fT i ≥0 a.e. for each i, then the event {fT i ≥0, all i} has probability one and
hence so does the event {< f >n≥0 all i}. ( The countable intersection of a collection of sets with
probability one also has probability one.)
2
Note that unlike Lemma 4.5.1, Lemma 4.5.2 cannot be viewed as being a special case of Lemma
4.4.2.
4.6
Convergence of Random Variables
We have introduced the notion of almost everywhere convergence of a sequence of measurements. We
now further develop the properties of such convergence along with three other notions of convergence.
Suppose that {fn; = 1, 2, . . .} is a sequence of measurements (random variables, measurable
functions) deﬁned on a probability space with probability measure m.
Deﬁnitions:
1. The sequence {fn} is said to converge with probability one or with certainty or m-almost surely
(m-a.s.) or m-a.e. to a random variable f if
m({x : lim
n→∞fn(x) = f(x)}) = 1;
that is, if with probability one the usual limit of the sequence fn(x) exists and equals f(x). In
this case we write f = limn→∞fn, m −a.e.
2. The {fn} are said to converge in probability to a random variable f if for every ϵ > 0
lim
n→∞m({x : |fn(x) −f(x)| > ϵ}) = 0,
in which case we write
fn →m f
or
f = m lim
n→∞fn.
If for all positive real r
lim
n→∞m({x : fn(x) ≤r}) = 0,
we write
fn →m ∞or m lim
n→∞fn = ∞.
3. The space L1(m) of all m-integrable functions is a pseudo-normed space with pseudo-norm
||f||1 = E|f|, and hence it is a pseudo-metric space with metric
d(f, g) =
Z
|f −g| dm.

4.6. CONVERGENCE OF RANDOM VARIABLES
85
We can consider the space to be a normed metric space if we identify or consider equal functions
for which d(f, g) is zero. Note that d(f, g) = 0 if and only if f = g m-a.e. and hence the metric
space really has as elements equivalence classes of functions rather than functions. A sequence
{fn} of measurements is said to converge to a measurement f in L1(m) if f is integrable and
if limn→∞||fn −f||1 = 0, in which case we write
fn →L1(m) f.
Convergence in L1(m) has a simple but useful consequence. From Lemma 4.2.2(d) for any
event F
|
Z
F
fn dm −
Z
F
f dm| = |
Z
F
(fn −f) dm| ≤
Z
F
|fn −f|dm
≤||fn −f||1 →n→∞0
and therefore
lim
n→∞
Z
F
fn dm =
Z
F
f dm if fn →f ∈L1(m).
(4.21)
In particular, if F is the entire space, then limn→∞Efn = Ef.
4. Denote by L2(m) the space of all square-integrable functions, that is, the space of all f such
that E(f2) is ﬁnite. This is an inner product space with inner product (f, g) = E(fg) and
hence also a normed space with norm ||f||2 = (E(f 2))1/2 and hence also a metric space with
metric d(f, g) = (E[(f −g)2])1/2, provided that we identify functions f and g for which d(f, g)
= 0. A sequence of square-integrable functions {fn} is said to converge in L2(m) or to converge
in mean square or to converge in quadratic mean to a function f if f is square-integrable and
limn→∞||fn −f||2 = 0, in which case we write
fn →L2(m) f
or
f = l.i.m.
n→∞fn,
where l.i.m. stands for limit in the mean
Although we shall focus on L1 and L2, we note in passing that they are special cases of general Lp
norms deﬁned as follows: Let Lp denote the space of all measurements f for which |f|p is integrable
for some integer p ≥1. Then
||f||p = (
Z
|f|pdm)
1
p
is a pseudo-norm on this space. It is in fact a norm if we consider measurements to be the same if
they are equal almost everywhere. We say that a sequence of measurements fn converges to f in Lp
if ||fn −f||p →0 as n →∞. As an example of such convergence, the following lemma shows that
the quantizer sequence of Lemma 4.3.1 converges to the original measurement in Lp provided the f
is in Lp.
Lemma 4.6.1 If f ∈Lp(m) and qn(f), n = 1, 2, . . ., is the quantizer sequence of Lemma 4.3.1,
then
qn(f) →Lp f.

86
CHAPTER 4. AVERAGES
Proof: From Lemma 4.3.1 |qn(f) −f| is bound above by |f| and monotonically nonincreasing and
goes to 0 everywhere. Hence |qn(f)−f|p also is monotonically nonincreasing, tends to 0 everywhere,
and is bound above by the integrable function |f|p. The lemma then follows from the dominated
convergence theorem (Lemma 4.4.5(e)).
2
The following lemmas provide several properties and alternative characterizations of and some
relations among the various forms of convergence.
Of all of the forms of convergence, only almost everywhere convergence relates immediately to
the usual notion of convergence of sequences of real numbers. Hence almost everywhere convergence
inherits several standard limit properties of elementary real analysis. The following lemma states
several of these properties without proof. (Proofs may be found, e.g., in Rudin [61].)
Lemma 4.6.2 If limn→∞fn = f, m-a.e. and limn→∞gn = g, m −a.e., then limn→∞(fn + gn) =
f + g and limn→∞fngn =fg, m-a.e.. If also g ̸= 0m −a.e. then limn→∞fn/gn = f/g, m-a.e. If
h : ℜ→ℜis a continuous function, then limn→∞h(fn) = h(f), m-a.e.
The next lemma and corollary provide useful tools for proving and interpreting almost everywhere
convergence. The lemma is essentially the classic Borel-Cantelli lemma of probability theory.
Lemma 4.6.3 Given a probability space (Ω, B, m) and a sequence of events Gn ∈B, n = 1, 2, . . . ,
deﬁne the set {Gn i.o.} as the set {ω : ω ∈Gn for an inﬁnite number of n}; that is,
{Gn i.o. } =
∞
\
n=1
∞
[
j=n
Gj = lim sup
n→∞Gn.
Then m(Gn i.o.) = 0 if and only if
lim
n→∞m(
∞
[
k=n
Gk) = 0.
A suﬃcient condition for m(Gn i.o.) = 0 is that
∞
X
n=1
m(Gn) < ∞.
(4.22)
Proof: From the continuity of probability,
m(Gn i.o. ) = m(
∞
\
n=1
∞
[
j=n
Gj) = lim
n→∞m(
∞
[
j=n
Gj),
which proves the ﬁrst statement. From the union bound,
m(
∞
[
j=n
Gn) ≤
∞
X
j=n
m(Gn),
which goes to 0 as n →∞if (4.22) holds.
2
Corollary 4.6.1 limn→∞fn = f m-a.e. if and only if for every ϵ > 0
lim
n→∞m(
∞
[
k=n
{x : |fk(x) −f(x)| < ϵ}) = 0.

4.6. CONVERGENCE OF RANDOM VARIABLES
87
If for any ϵ > 0
∞
X
n=0
m(|fn| > ϵ) < ∞,
(4.23)
then fn →0 m-a.e.
Proof: Deﬁne Fn(ϵ) and F(ϵ) as in (4.17)-(4.19). Applying the lemma to Gn = Fn(ϵ)c shows that the
condition of the corollary is equivalent to m(Fn(ϵ)c i.o.) = 0 and hence m(F(ϵ)) = 1 and therefore
m({x : lim
n→∞fn(x) = f(x)}) = m(
∞
\
k=1
F(1
k )) = 1.
Conversely, if
m(∩kF(1
k )) = 1,
then since T
k≥1 F(1/k) ⊂F(ϵ) for all ϵ > 0, m(F(ϵ)) = 1 and therefore limn→∞m(S
k≥n Fk(ϵ)c) =
0. Eq. (4.23) is (4.22) for Gn = {x : fn(x) > ϵ}, and hence this condition is suﬃcient from the
lemma.
2
Note that almost everywhere convergence requires that the probability that |fk −f| > ϵ for any
k ≥n tend to zero as n →∞, whereas convergence in probability requires only that the probability
that |fn −f| > ϵ tend to zero as n →∞.
Lemma 4.6.4 : Given random variables fn, n = 1, 2, . . ., and f, then
fn →L2(m) f ⇒fn →L1(m) f ⇒fn →m f,
lim
n→∞fn = f m −a.e. ⇒fn →m f.
Proof: From the Cauchy-Schwarz inequality ||fn −f||1 ≤||fn −f||2 and hence L2(m) convergence
implies L1(m) convergence. From Markov’s inequality
m({|fn −f| ≥ϵ}) ≤Em(|fn −f|)
ϵ
,
and hence L1(m) convergence implies convergence in m-probability. If fn →f m-a.e., then
m(|fn −f| > ϵ) ≤m(
∞
[
k=n
{|fk −f| > ϵ}) →n→∞0
and hence m −limn→∞fn = f.
2
Corollary 4.6.2 If fn →f and fn →g in any of the given forms of convergence, then f = g,
m-a.e.
Proof: Fix ϵ > 0 and deﬁne Fn(δ) = {x : |fn(x) −f(x)| ≤δ} and Gn(δ) = {x : |fn(x) −g(x)| ≤δ},
then
Fn(ϵ/2) ∩Gn(ϵ/2) ⊂{x : |f(x) −g(x)| ≤ϵ}
and hence
m(|f −g| > ϵ) ≤m( Fn( ϵ
2)c ∪Gn( ϵ
2)c )

88
CHAPTER 4. AVERAGES
≤m( Fn( ϵ
2)c ) + m( Gn( ϵ
2)c ) →n→∞0.
Since this is true for all ϵ, m(|f −g| > 0) = 0, so the corollary is proved for convergence in probability.
From the lemma, convergence in probability is implied by the other forms, and hence the proof is
complete.
2
The following lemma shows that the metric spaces L1(m) and L2(m) are each Polish spaces
(when we consider two measurements to be the same if they are equal m-a.e.).
Lemma 4.6.5 The spaces Lp(m), k = 1, 2, . . . are complete, separable metric spaces, i.e., they are
Polish spaces.
Proof: The spaces are separable since from Lemma 4.6.1 the collection of all simple functions with
rational values is dense. Let fn be a Cauchy sequence, that is, ||fm −fn|| →n,m→∞0, where || · ||
denotes the Lp(m) norm. Choose an increasing sequence of integers nl, l = 1, 2, . . . so that
||fm −fn|| ≤(1
4)l; for m, n ≥nl.
Let gl = fnl and deﬁne the set Gn = {x : |gn(x) −gn+1(x)| ≥2−n}. From the Markov inequality
m(Gn) ≤2nk||gn −gn+1||k ≤2−nk
and therefore from Lemma 4.6.4
m(lim sup
n→∞Gn) = 0.
Thus with probability 1 x ̸∈lim supn Gn. If x ̸∈lim supn Gn, however, then |gn(x)−gn+1(x)| < 2−n
for large enough n. This means that for such x gn(x) is a Cauchy sequence of real numbers, which
must have a limit since ℜis complete under the Euclidean norm. Thus gn converges on a set of
probability 1 to a function g. The proof is completed by showing that fn →g in Lp. We have that
||fn −g||k
=
Z
|fn −g|k dm =
Z
lim inf
l→∞|fn −gl|k dm
≤
lim inf
l→∞
Z
|fn −fnl|k dm = lim inf
l→∞||fn −fnl||k
using Fatou’s lemma and the deﬁnition of gl. The right-most term, however, can be made arbitrarily
small by choosing n large, proving the required convergence.
2
Lemma 4.6.6 If fn →m f and the fn are uniformly integrable, then f is integrable and fn →f in
L1(m). If fn →m f and the f 2
n are uniformly integrable, then f ∈L2(m) and fn →f in L2(m).
Proof: We only prove the L1 result as the other is proved in a similar manner. Fix ϵ > 0. Since the
{fn} are uniformly integrable we can choose from Lemma 4.4.6 a δ > 0 suﬃciently small to ensure
that if m(F) < δ, then
Z
F
|fn| dm < ϵ
4, all n.
Since the fn converge to f in probability, we can choose N so large that if n > N, then m({|fn−f| >
ϵ/4}) < δ/2 for n > N. Then for all n, m > N (using the L1(m) norm)
||fn −fm|| =
Z
|fn −f| ≤ϵ
4
and
|fm −f| ≤ϵ
4
|fn −fm|dm +
Z
|fn −f| > ϵ
4
or
|fm −f| > ϵ
4
|fn −fm| dm.

4.6. CONVERGENCE OF RANDOM VARIABLES
89
If fn and fm are each within ϵ/4 of f, then they are within ϵ/2 of each other from the triangle
inequality. Also using the triangle inequality, the right-most integral is bound by
Z
F
|fn| dm +
Z
F
|fm|dm
where F = {|fn −f| > ϵ/4 or |fm −f| > ϵ/4}, which, from the union bound, has probability less
than δ/2 + δ/2 = δ, and hence the preceding integrals are each less than ϵ/4. Thus ||fn −fm|| ≤
ϵ/2 + ϵ/4 + ϵ/4 = ϵ, and hence {fn} is a Cauchy sequence in L1(m), and it therefore must have
a limit, say g, since L1(m) is complete. From Lemma 4.6.4, however, this means that also the fn
converge to g in probability, and hence from Corollary 4.6.2 f = g, m-a.e., completing the proof. 2
The lemma provides a simple generalization of Corollary 4.4.1 and an alternative proof of a
special case of Lemma 4.6.1.
Corollary 4.6.3 If qn is the quantizer sequence of Lemma 4.3.1 and f is square-integrable, then
qn(f) →f in L2.
Proof: From Lemma 4.4.1, qn(f(x)) →f(x) for all x and hence also in probability. Since for all n
|qn(f)| ≤|f| by construction, |qn(f)|2 ≤|f|2, and hence from Lemma 4.4.4 the qn(f)2 are uniformly
integrable and the conclusion follows from the lemma.
2
The next lemma uses uniform integrability ideas to provide a partial converse to the convergence
implications of Lemma 4.6.4.
Lemma 4.6.7 If a sequence of random variables fn converges in probability to a random variable
f, then the following statements are equivalent (where p = 1 or 2):
(a)
fn →Lp f.
((b) The f p
n are uniformly integrable.
((c)
lim
n→∞E(|fn|p) = E(|f|p).
Proof: Lemma 4.6.6 implies that (b) => (a). If (a) is true, then the triangle inequality implies
that |||fn|| −||f||| ≤||fn −f|| →0, which implies (c). Finally, assume that (c) holds. Consider the
integral
Z
|fn|k≥r
|fn|k dm
=
Z
|fn|k dm −
Z
|fn|kr
fn dm
=
Z
|fn|k dm −
Z
min(|fn|k, r) dm + r
Z
|fn|k≥r
dm.
(4.24)
The ﬁrst term on the right tends to
R
|f|k dm by assumption. We consider the limits of the remaining
two terms. Fix δ > 0 small and observe that
E(| min(|fn|k, r) −min(|f|k, r)|) ≤δm(||fn|k −|f|k| ≤δ) + rm(||fn|k −|f|k| > δ).
Since fn →f in probability, it must also be true that |fn|k →|f|k in probability, and hence the
preceding expression is bound above by δ as n →∞. Since δ is arbitrarily small
lim
n→∞|E(min(|fn|k, r)) −E(min(|f|k, r))| ≤

90
CHAPTER 4. AVERAGES
lim
n→∞E(| min(|fn|k, r) −min(|f|k, r)|) = 0.
Thus the middle term on the right of (4.24) converges to E(min(|f|k, r)). The remaining term is
rm(|fn|k ≥r).
For any ϵ > 0
m(|fn|k ≥r) = m(|fn|k ≥r and ||fn|k −|f|k| ≤ϵ)
+m(|fn|k ≥r and ||fn|k −|f|k| > ϵ).
The right-most term tends to 0 as n →∞, and the remaining term is bound above by
m(|f|k ≥r −ϵ).
From the continuity of probability, this can be made arbitrarily close to m(|f|k ≥r) by choosing ϵ
small enough, e.g., choosing Fn = {|f|k ∈[r −1/n, ∞)}, then Fn decreases to the set F = {|f|k ∈
[r, ∞)} and limn→∞m(Fn) = m(F).
Combining all of the preceding facts
lim
n→∞
Z
|fn|k≥r
|fn|k dm =
Z
|f|k≥r
|f|kdm
From Corollary 4.4.4 given ϵ > 0 we can choose R so large that for r > R the right-hand side is less
than ϵ/2 and then choose N so large that
Z
|fn|k>R
|fn|k dm ≤ϵ, n > N.
(4.25)
Then
lim
r→∞sup
n
Z
|fn|k>r
|fn|k dm ≤
lim
r→∞sup
n≤N
Z
|fn|k>r
|fn|k dm + lim
r→∞sup
n>N
Z
|fn|k>r
|fn|k dm.
(4.26)
The ﬁrst term to the right of the inequality is zero since the limit of each of the ﬁnite terms in
the sequence is zero from Corollary 4.4.4. To bound the second term, observe that for r > R each
of the integrals inside the supremum is bound above by a similar integral over the larger region
{|fn|k > R}. From (4.25) each of these integrals is bound above by ϵ, and hence so is the entire
right-most term. Since the left-hand side of (4.26) is bound above by any ϵ > 0, it must be zero,
and hence the |fn|k are uniformly integrable.
2
Note as a consequence of the Lemma that if fn →f in Lp for p = 1, 2, . . . then necessarily the
f p
n are uniformly integrable.
Exercises
1. Prove Lemma 4.6.2.
2. Show that if fk ∈L1(m) for k = 1, 2, . . ., then there exists a subsequence nk, k = 1, 2, . . ., such
that with probability one
lim
k→∞
fnk
nk
= 0
Hint: Use the Borel-Cantelli lemma.

4.7. STATIONARY AVERAGES
91
3. Suppose that fn converges in probability to f. Show that given ϵ, δ > 0 there is an N such
that if
Fn,m(ϵ) = {x : |fn(x) −fm(x) > ϵ},
then m(Fn, m) ≤δ for n, m ≥N; that is,
lim
n,m→∞m(Fn,m(ϵ)) = 0, all ϵ > 0.
In words, if fn converges in probability, then it is a Cauchy sequence in probability.
4. Show that if fn →f and gn −fn →0 in probability, then gn →f in probability. Show that if
fn →f and gn →g in L1, then fn + gn →f + g in L1. Show that if fn →f and gn →g in
L2 and supn ||fn||2 < ∞, then fngn →fg in L1.
4.7
Stationary Averages
We have seen in the discussions of ensemble averages and time averages that uniformly integrable
sequences of functions have several nice convergence properties: In particular, we can compare
expectations of limits with limits of expectations and we can infer L1(m) convergence from m-a.e.
convergence from such sequences. As time averages of the form < f >n are the most important
sequences of random variables for ergodic theory, a natural question is whether or not such sequences
are uniformly integrable if f is integrable. Of course in general time averages of integrable functions
need not be uniformly integrable, but in this section we shall see that in a special, but very important,
case the response is aﬃrmative.
Let (Ω, B, m, T) be a dynamical system, where as usual we keep in mind the example where the
system corresponds to a directly given random process {Xn, n ∈I} with alphabet A and where T
is a shift.
In general the expectation of the shift fT i of a measurement f can be found from the change of
variables formula of Lemma 4.4.6(b); that is, if mT −i is the measure deﬁned by
mT −i(F) = m(T −iF), all events F,
(4.27)
then
EmfT i =
Z
fT i dm =
Z
f dmT −i = EmT −if.
(4.28)
We shall ﬁnd that the sequences of measurements fT n and sample averages < f >n will have
special properties if the preceding expectations do not depend on i; that is, the expectations of
measurements do not depend on when they are taken. From (4.28), this will be the case if the
probability measures mT −n deﬁned by mT −n(F) = m(T −nF) for all events F and all n are the
same. This motivates the following deﬁnition. The dynamical system is said to be stationary (or
T-stationary or stationary with respect to T) or invariant (or T-invariant or invariant with respect
to T) if
m(T −1F) = m(F), all events F.
(4.29)
We shall also say simply that the measure m is stationary with respect to T if (4.29) holds. A
random process {Xn} is said to be stationary (with the preceding modiﬁers) if its distribution m
is stationary with respect to T, that is, if the corresponding dynamical system with the shift T is
stationary. A stationary random process is one for which the probabilities of an event are the same

92
CHAPTER 4. AVERAGES
regardless of when it occurs. Stationary random processes will play a central, but not exclusive, role
in the ergodic theorems.
Observe that (4.29) implies that
m(T −nF) = m(F), all eventsF, all n = 0, 1, 2, . . .
(4.30)
The following lemma follows immediately from (4.27), (4.28), and (4.30).
Lemma 4.7.1 If m is stationary with respect to T and f is integrable with respect to m, then
Emf = EmfT n, n = 0, 1, 2, . . .
Lemma 4.7.2 If f is in L1(m) and m is stationary with respect to T, then the sequences {fT n; n =
0, 1, 2, . . .} and {< f >n; n = 0, 1, 2, . . .}, where
< f >n= n−1
n−1
X
i=0
fT i,
are uniformly integrable. If also f ∈L2(m), then in addition the sequence {ffT n; n = 0, 1, 2, . . .}
and its arithmetic mean sequence {wn; n = 0, 1, . . .} deﬁned by
wn(x) = n−1
n−1
X
i=0
f(x)f(T ix)
(4.31)
are uniformly integrable.
Proof: First observe that if G = {x : |f(x)| ≥r}, then T −iG = {x : f(T ix) ≥r} since x ∈T −iG
if and only if T ix ∈G if and only if f(T ix) > r. Thus by the change of variables formula and the
assumed stationarity
Z
T −iG
|fT i| dm =
Z
G
|f| dm, independent of i,
and therefore
Z
|fT i|r
|fT i| dm =
Z
|f|r
|f| dm →r→∞0 uniformly in i.
This proves that the fT i are uniformly integrable. Corollary 4.4.6 implies that the arithmetic mean
sequence is then also uniformly integrable.
2
Exercises
1. A dynamical system is said to be N-stationary for an integer N if it is stationary with respect
to T N; that is,
m(T −NF) = m(F), all F ∈B.
Show that this implies that for any event F m(T −nNF) = m(F) and that m(T −nF) is a
periodic function in n with period N. Does Lemma 4.7.2 hold for N-stationary processes?
Deﬁne the measure mN by
mN(F) = 1
N
N−1
X
i=0
m(T −iF)

4.7. STATIONARY AVERAGES
93
for all events F, a deﬁnition that we abbreviate by
mN = 1
N
N−1
X
i=0
mT −i.
Show that mN is stationary and that for any measurement f
EmN f = 1
N
N−1
X
i=1
EmfT i.
Hint: As usual, ﬁrst consider discrete measurements and then take limits.
2. A dynamical system with measure m is said to be asymptotically mean stationary if there is a
stationary measure m for which
lim
n→∞
1
n
n−1
X
i=0
m(T −iF) = m(F); all F ∈B.
Show that both stationary and N-stationary systems are also asymptotically mean stationary.
Show that if f is a bounded measurement, that is, |f| ≤K for some ﬁnite K with probability
one, then
Emf = lim
n→∞
1
n
n−1
X
i=0
Em(fT i).

94
CHAPTER 4. AVERAGES

Chapter 5
Conditional Probability and
Expectation
5.1
Introduction
We begin the chapter by exploring some relations between measurements, that is, measurable func-
tions, and events, that is, members of a σ-ﬁeld.
In particular, we explore the relation between
knowledge of the value of a particular measurement or class of measurements and knowledge of an
outcome of a particular event or class of events. Mathematically these are relations between classes
of functions and σ-ﬁelds. Such relations will be useful in developing properties of certain special
functions such as limiting sample averages arising in the study of ergodic properties of information
sources. In addition, they are fundamental to the development and interpretation of conditional
probability and conditional expectation, that is, probabilities and expectations when we are given
partial knowledge about the outcome of an experiment.
Although conditional probability and conditional expectation are both common topics in an
advanced probability course, the fact that we are living in standard spaces will result in additional
properties not present in the most general situation. In particular, we will ﬁnd that the conditional
probabilities and expectations have more structure that enables them to be interpreted and often
treated much like ordinary unconditional probabilities.
In technical terms, there will always be
regular versions of conditional probability and we will be able to deﬁne conditional expectations
constructively as expectations with respect to conditional probability measures.
5.2
Measurements and Events
Say we have a measurable spaces (Ω, B) and (A, B(A)) and a function f : Ω→A. Recall from
Chapter 1 that f is a random variable or measurable function or a measurement if f−1(F) ∈B for
all F ∈B(A). Clearly, being told the outcome of the measurement f provides information about
certain events and likely provides no information about others. In particular, if f assumes a value
in F ∈B(A), then we know that f−1(F) occurred. We might call such an event in B an f-event
since it is speciﬁed by f taking a value in B(A). Consider the class G = f−1(B(A)) = {all sets of
the form f −1(F), F ∈B(A)}. Since f is measurable, all sets in G are also in B. Furthermore, it is
easy to see that G is a σ-ﬁeld of subsets of Ω. Since G ⊂B and it is a σ-ﬁeld, it is referred to as a
sub-σ-ﬁeld.
95

96
CHAPTER 5. CONDITIONAL PROBABILITY AND EXPECTATION
Intuitively G can be thought of as the σ- ﬁeld or event space consisting of those events whose
outcomes are determinable by observing the output of f. Hence we deﬁne the σ-ﬁeld induced by f
as σ(f) = f −1(B(A)).
Next observe that by construction f−1(F) ∈σ(f) for all F ∈B(A); that is, the inverse images
of all output events may be found in a sub-σ-ﬁeld. That is, f is measurable with respect to the
smaller σ-ﬁeld σ(f) as well as with respect to the larger σ-ﬁeld B. As we will often have more
than one σ-ﬁeld for a given space, we emphasize this notion: Say we are given measurable spaces
(Ω, B) and (A, B(A)) and a sub-σ-ﬁeld G of B (and hence (Ω, G) is also a measurable space). Then
a function f : Ω→A is said to be measurable with respect to G or G-measurable or, in more detail,
G/B(A)-measurable if f−1(F) ∈G for all F ∈B(A). Since G ⊂B a G-measurable function is clearly
also B-measurable. Usually we shall refer to B-measurable functions as simply measurable functions
or as measurements.
We have seen that if σ(f) is the σ-ﬁeld induced by a measurement f and that f is measurable
with respect to σ(f). Observe that if G is any other σ-ﬁeld with respect to which f is measurable,
then G must contain all sets of the form f−1(F), F ∈B(A), and hence must contain σ(f). Thus
σ(f) is the smallest σ-ﬁeld with respect to which f is measurable.
Having begun with a measurement and found the smallest σ-ﬁeld with respect to which it is mea-
surable, we can next reverse direction and begin with a σ-ﬁeld and study the class of measurements
that are measurable with respect to that σ-ﬁeld. The short term goal will be to characterize those
functions that are measurable with respect to σ(f). Given a σ-ﬁeld G let M(G) denote the class of
all measurements that are measurable with respect to G. We shall refer to M(G) as the measurement
class induced by G. Roughly speaking, this is the class of measurements whose output events can
be determined from events in G. Thus, for example, M(σ(f)) is the collection of all measurements
whose output events could be determined by events in σ(f) and hence by output events of f. We
shall shortly make this intuition precise, but ﬁrst a comment is in order. The basic thrust here
is that if we are given information about the outcome of a measurement f, in fact we then have
information about the outcomes of many other measurements, e.g., f2, |f|, and other functions of
f. The class of functions M(σ(f)) will prove to be exactly that class containing functions whose
outcome is determinable from knowledge of the outcome of the measurement f.
The following lemma shows that if we are given a measurement f : Ω→A, then a real-valued
function g is in M(σ(f)), that is, is measurable with respect to σ(f), if and only if g(ω) = h(f(ω))
for some measurement h : A →ℜ, that is, if g depends on the underlying sample points only through
the value of f.
Lemma 5.2.1 Given a measurable space (Ω, B) and a measurement f : Ω→A, then a measurement
g : Ω→ℜis in M(σ(f)), that is, is measurable with respect to σ(f), if and only if there is a
B(A)/B(R)-measurable function h : A →ℜsuch that g(ω) = h(f(ω)), all ω ∈Ω.
Proof: If there exists such an h, then for F ∈B(ℜ), then
g−1(F) = {ω : h(f(ω)) ∈F} = {ω : f(ω) ∈h−1(F)} = f −1(h−1(F)) ∈B
since h−1(F) is in B(A) from the measurability of h and hence its inverse image under f is in σ(f) by
deﬁnition. Thus any g of the given form is in M(σ(f)). Conversely, suppose that g is in M(σ(f)).
Let qn : A →R be the sequence of quantizers of Lemma 4.3.1. Then gn deﬁned by gn(ω) = qn(f(ω))
are σ(f)-measurable simple functions that converge to g. Thus we can write gn as
gn(ω) =
M
X
i=1
ri1F (i)(ω),

5.2. MEASUREMENTS AND EVENTS
97
where the F(i) are disjoint, and hence since gn ∈M(σ(f)) that
g−1
n (ri) = F(i) ∈σ(f).
Since σ(f) = f −1(B(A)), there are disjoint sets Q(i) ∈B(A) such that F(i) = f−1(Q(i)). Deﬁne
the function hn : A →ℜby
hn(a) =
M
X
i=1
ri1Q(i)(a)
and
hn(f(ω))
=
M
X
i=1
ri1Q(i)(f(ω)) =
M
X
i=1
ri1f −1(Q(i))(ω)
=
M
X
i=1
ri1F (i)(ω) = gn(ω).
This proves the result for simple functions. By construction we have that g(ω) = limn→∞gn(ω) =
limn→∞hn(f(ω)) where, in particular, the right-most limit exists for all ω ∈Ω. Deﬁne the function
h(a) = limn→∞hn(a) where the limit exists and 0 otherwise. Then g(ω) = limn→∞hn(f(ω)) =
h(f(ω)), completing the proof.
2
Thus far we have developed the properties of σ-ﬁelds induced by random variables and of classes
of functions measurable with respect to σ-ﬁelds. The idea of a σ-ﬁeld induced by a single random
variable is easily generalized to random vectors and sequences. We wish, however, to consider the
more general case of a σ-ﬁeld induced by a possibly uncountable class of measurements. Then we will
have associated with each class of measurements a natural σ-ﬁeld and with each σ-ﬁeld a natural
class of measurements. Toward this end, given a class of measurements M, deﬁne σ(M) as the
smallest σ-ﬁeld with respect to which all of the measurements in M are measurable. Since any
σ-ﬁeld satisfying this condition must contain all of the σ(f) for f ∈M and hence must contain the
σ-ﬁeld induced by all of these sets and since this latter collection is a σ-ﬁeld,
σ(M) = σ(
[
f∈M
σ(f)).
The following lemma collects some simple relations among σ-ﬁelds induced by measurements and
classes of measurements induced by σ-ﬁelds.
Lemma 5.2.2 Given a class of measurements M, then
M ⊂M(σ(M)).
Given a collection of events G, then
G ⊂σ(M(σ(G))).
If G is also a σ-ﬁeld, then
G = σ(M(G)),
that is, G is the smallest σ-ﬁeld with respect to which all G-measurable functions are measurable. If
G is a σ-ﬁeld and I(G) = {all 1G, G ∈G} is the collection of all indicator functions of events in G,
then
G = σ(I(G)),
that is, the smallest σ-ﬁeld induced by indicator functions of sets in G is the same as that induced
by all functions measurable with respect to G.

98
CHAPTER 5. CONDITIONAL PROBABILITY AND EXPECTATION
Proof: If f ∈M, then it is σ(f)-measurable and hence in M(σ(M)). If G ∈G, then its indica-
tor function 1G is σ(G)-measurable and hence 1G ∈M(σ(G)). Since 1G ∈M(σ(G)), it must be
measurable with respect to σ(M(σ(G))) and hence 1−1
G (1) = G ∈σ(M(σ(G))), proving the second
statement.
If G is a σ-ﬁeld, then since since all functions in M(G) are G-measurable and since
σ(M(G)) is the smallest σ-ﬁeld with respect to which all functions in M(G) are G-measurable, G
must contain σ(M(G)). Since I(G) ⊂M(G),
σ(I(G)) ⊂σ(M(G)) = G.
If G ∈G, then 1G is in I(G) and hence must be measurable with respect to σ(I(G)) and hence
1−1
G (1) = G ∈σ(I(G)) and hence G ⊂σ(I(G)), completing the proof.
2
We conclude this section with a reminder of the motivation for considering classes of events and
measurements. We shall often consider such classes either because a particular class is important for
a particular application or because we are simply given a particular class. In both cases we may wish
to study both the given events (measurements) and the related class of measurements (events). For
example, given a class of measurements M, σ(M) provides a σ-ﬁeld of events whose occurrence or
nonoccurrence is determinable by the output events of the functions in the class. In turn, M(σ(M))
is the possibly larger class of functions whose output events are determinable from the occurrence
or nonoccurrence of events in σ(M) and hence by the output events of M. Thus knowing all output
events of measurements in M is eﬀectively equivalent to knowing all output events of measurements
in the more structured and possibly larger class M(σ(M)), which is in turn equivalent to knowing
the occurrence or nonoccurrence of events in the σ-ﬁeld σ(M). Hence when a class M is speciﬁed,
we can instead consider the more structured classes σ(M) or M(σ(M)). From the previous lemma,
this is as far as we can go; that is,
σ(M(σ(M))) = σ(M).
(5.1)
We have seen that a function g will be in M(σ(f)) if and only if it depends on the underlying
sample points through the value of f. Since we did not restrict the structure of f, it could, for
example, be a random variable, vector, or sequence, that is, the conclusion is true for countable
collections of measurements as well as individual measurements. If instead we have a general class
M of measurements, then it is still a useful intuition to think of M(σ(M)) as being the class of all
functions that depend on the underlying points only through the values of the functions in M.
Exercises
1. Which of the following relations are true and which are false?
f 2 ∈σ(f), f ∈σ(f 2)
f + g ∈σ(f, g), f ∈σ(f + g)
2. If f : A →Ω, g : Ω→Ω, and g(f) : A →Ωis deﬁned by g(f)(x) = g(f(x)), then g(f) ∈σ(f).
3. Given a class M of measurements, is S
f∈M σ(f) a σ-ﬁeld?
4. Suppose that (A, B) is a measure space and B is separable with a countable generating class
{Vn; n = 1, 2 . . .}. Describe σ(1Vn; n = 1, 2, . . .).

5.3. RESTRICTIONS OF MEASURES
99
5.3
Restrictions of Measures
The ﬁrst application of the classes of measurements or events considered in the previous section is
the notion of the restriction of a probability measure to a sub-σ-ﬁeld. This occasionally provides a
shortcut to evaluating expectations of functions that are measurable with respect to sub-σ-ﬁelds and
in comparing such functions. Given a probability space (Ω, B, m) and a sub-σ-ﬁeld G of B, deﬁne
the restriction of m to G, mG, by
mG(F) = m(F), F ∈G.
Thus (Ω, G, mG) is a new probability space with a smaller event space. The following lemma shows
that if f is a G-measurable real-valued random variable, then its expectation can be computed with
respect to either m or mG.
Lemma 5.3.1 Given a G-measurable real-valued measurement f ∈L1(m), then also f ∈L1(mG)
and
Z
f dm =
Z
f dmG,
where mG is the restriction of m to G.
If f is a simple function, the result is immediate from the deﬁnition of restriction. More generally
use Lemma 4.3.1(e) to infer that qn(f) is a sequence of simple G-measurable functions converging
to f and combine the simple function result with Corollary 4.4.1 applied to both measures.
Corollary 5.3.1 Given G-measurable functions f, g ∈L1(m), if
Z
F
f dm ≤
Z
F
g dm, all F ∈G,
then f ≤gm-a.e. If the preceding holds with equality, then f = g m-a.e.
Proof: From the previous lemma, the integral inequality holds with m replaced by mG, and hence
from Lemma 4.4.7 the conclusion holds mG-a.e. Thus there is a set, say G, in G with mG probability
one and hence also m probability one for which the conclusion is true.
2
The usefulness of the preceding corollary is that it allows us to compare G-measurable functions
by considering only the restricted measures and the corresponding expectations.
5.4
Elementary Conditional Probability
Say we have a probability space (Ω, B, m), how is the probability measure m altered if we are told
that some event or collection of events occurred? For example, how is it inﬂuenced if we are given
the outputs of a measurement or collection of measurements? The notion of conditional probabil-
ity provides a response to this question. In fact there are two notions of conditional probability:
elementary and nonelementary.
Elementary conditional probabilities cover the case where we are given an event, say F, having
nonzero probability: m(F) > 0. We would like to deﬁne a conditional probability measure m(G|F)
for all events G ∈B. Intuitively, being told an event F occurred will put zero probability on the
collection of all points outside F, but it should not eﬀect the relative probabilities of the various
events inside F. In addition, the new probability measure must be renormalized so as to assign
probability one to the new “certain event” F. This suggests the deﬁnition m(G|F) = km(G ∩F),

100
CHAPTER 5. CONDITIONAL PROBABILITY AND EXPECTATION
where k is a normalization constant chosen to ensure that m(F|F) = km(F ∩F) = km(F) = 1.
Thus we deﬁne for any F such that m(F) > 0. the conditional probability measure
m(G|F) = m(G ∩F)
m(F)
, all G ∈B.
We shall often abbreviate the elementary conditional probability measure m(·|F) by mF . Given
a probability space (Ω, B, m) and an event F ∈B with m(F) > 0, then we have a new probability
space (F, B ∩F, mF ), where B ∩F = {all sets of the form G ∩F, G ∈B }. It is easy to see that we
can relate expectations with respect to the conditional and unconditional measures by
EmF (f) =
Z
f dmF =
R
F f dm
m(F)
= Em1F f
m(F) ,
(5.2)
where the existence of either side ensures that of the other. In particular, f ∈L1(mF ) if and only
if f1F ∈L1(m). Note further that if G = F c, then
Emf = m(F)EmF (f) + m(G)EmG(f).
(5.3)
Instead of being told that a particular event occurred, we might be told that a random variable
or measurement f is discrete and takes on a speciﬁc value, say a with nonzero probability. Then
the elementary deﬁnition immediately yields
m(F|f = a) = m(G ∩f = a)
m(f = a)
.
If, however, the measurement is not discrete and takes on a particular value with probability zero,
then the preceding elementary deﬁnition does not work. One might attempt to replace the previous
deﬁnition by some limiting form, but this does not yield a useful theory in general and it is clumsy.
The standard alternative approach is to replace the preceding constructive deﬁnition by a descriptive
deﬁnition, that is, to deﬁne conditional probabilities by the properties that they should possess. The
mathematical problem is then to prove that a function possessing the desired properties exists.
In order to motivate the descriptive deﬁnition, we make several observations on the elementary
case. First note that the previous conditional probability depends on the value a assumed by f, that
is, it is a function of a. It will prove more useful and more general instead to consider it a function
of ω that depends on ω only through f(ω), that is, to consider conditional probability as a function
m(G|f)(ω) = m(G|{λ : f(λ) = f(ω)}), or, simply, m(G|f = f(ω)), the probability of an event G
given that f assumes a value f(ω). Thus a conditional probability is a function of the points in the
underlying sample space and hence is itself a random variable or measurement. Since it depends on
ω only through f, from Lemma 5.2.1 the function m(G|f) is measurable with respect to σ(f), the
σ-ﬁeld induced by the given measurement. This leads to the ﬁrst property:
For any ﬁxed G, m(G|f) is σ(f) −measurable.
(5.4)
Next observe that in the elementary case we can compute the probability m(G) by averaging or
integrating the conditional probability m(G|f) over all possible values of f; that is,
Z
m(G|f) dm
=
X
a
m(G|f = a)m(f = a)
=
X
a
m(G ∩f = a) = m(G).
(5.5)

5.4. ELEMENTARY CONDITIONAL PROBABILITY
101
In fact we can and must say more about such averaging of conditional probabilities. Suppose that
F is some event in σ(f) and hence its occurrence or nonoccurrence is determinable by observation
of the value of f, that is, from Lemma 5.2.1 1F (ω) = h(f(ω)) for some function h. Thus if we are
given the value of f(ω), being told that ω ∈F should not add any knowledge and hence should
not alter the conditional probability of an event G given f. To try to pin down this idea, assume
that m(F) > 0 and let mF denote the elementary conditional probability measure given F, that is,
mF (G) = m(G ∩F)/m(F). Applying (5.5) to the conditional measure mF yields the formula
Z
mF (G|f) dmF = mF (G),
where mF (G|f) is the conditional probability of G given the outcome of the random variable f and
given that the event F occurred. But we have argued that this should be the same as m(G|f).
Making this substitution, multiplying both sides of the equation by m(F), and using (5.2) we derive
Z
F
m(G|f) dm = m(G ∩F), all F ∈σ(f).
(5.6)
To make this plausibility argument rigorous observe that since f is assumed discrete we can write
f(ω) =
X
a∈A
a1f −1(a)(ω)
and hence
1F (ω) = h(f(ω)) = h(
X
a∈A
a1f −1(a)(ω))
and therefore
F =
[
a:h(a)=1
f −1(a).
We can then write
Z
F
m(G|f) dm
=
Z
1F m(G|f) dm =
Z
h(f)m(G|f) dm
=
X
a
h(a)m(G ∩{f = a})
m({f = a})
m({f = a}) =
X
a
h(a)m(G ∩f −1(a))
=
m(G ∩
[
a:h(a)=1
f −1(a)) = m(G ∩F).
Although (5.6) was derived for F with m(F) > 0, the equation is trivially satisﬁed if m(F) = 0
since both sides are then 0. Hence the equation is valid for all F in σ(f) as stated. Eq. (5.6) states
that not only must we be able to average m(G|f) to get m(G), a similar relation must hold when
we cut down the average to any σ(f)-measurable event.
Equations (5.4) and (5.6) provide the properties needed for a rigorous descriptive deﬁnition of
the conditional probability of an event G given a random variable or measurement f. In fact, this
conditional probability is deﬁned as any function m(G|f)(ω) satisfying these two equations. At this
point, however, little eﬀort is saved by conﬁning interest to a single conditioning measurement, and
hence we will develop the theory for more general classes of measurements. In order to do this,
however, we require a basic result from measure theory–the Radon-Nikodym theorem. The next two
sections develop this result.

102
CHAPTER 5. CONDITIONAL PROBABILITY AND EXPECTATION
Exercises
1. Let f and g be discrete measurements on a probability space (Ω, B, m). Suppose in particular
that
g(ω) =
X
i
ai1Gi(ω).
Deﬁne a new random variable E(g|f) by
E(g|f)(ω) = Em(.|f(ω))(g) =
X
i
aim(Gi|f(ω)).
This random variable is called the conditional expectation of g given f . Prove that
Em(g) = Em[E(g|f)].
5.5
Projections
One of the proofs of the Radon-Nikodym theorem is based on the projection theorem of Hilbert
space theory. As this proof is more intuitive than others (at least to the author) and as the pro-
jection theorem is of interest in its own right, this section is devoted to developing the properties
of projections and the projection theorem. Good references for the theory of Hilbert spaces and
projections are Halmos [31] and Luenberger [45].
Recall from Chapter 3 that a Hilbert space is a complete inner product space as deﬁned in
Example 2.5.10, which is in turn a normed linear space as Example 2.5.8. In fact, all of the results
considered in this section will be applied in the next section to a very particular Hilbert space, the
space L2(m) of all square integrable functions with respect to a probability measure m. Since this
space is Polish, it is a Hilbert space.
Let L be a Hilbert space with inner product (f, g) and norm ||f|| = (f, f)1/2. For the case where
L = L2(m), the inner product is given by
(f, g) = Em(fg).
A (measurable) subset M of L will be called a subspace of L if f, g ∈M implies that af + bg ∈M
for all scalar multipliers a, b. We will be primarily interested in closed subspaces of L. Recall from
Lemma 3.2.4 that a closed subset of a Polish space is also Polish with the same metric, and hence
M is a complete space with the same norm.
A fundamentally important property of linear spaces with inner products is the parallelogram
law
||f + g||2 + ||f −g||2 = 2||f||2 + 2||g||2.
(5.7)
This is proved simply by expanding the left-hand side as
(f, f) + 2(f, g) + (g, g) + (f, f) −2(f, g) + (g, g).
A simple application of this formula and the completeness of a subspace yield the following result:
given a ﬁxed element f in a Hilbert space and given a closed subspace of the Hilbert space, we can
always ﬁnd a unique member of the subspace that provides the stpproximation to f in the sense of
minimizing the norm of the diﬀerence.

5.5. PROJECTIONS
103
Lemma 5.5.1 Fix an element f in a Hilbert space L and let M be a closed subspace of L. Then
there is a unique element ˆf ∈M such that
||f −ˆf|| = inf
g∈M ||f −g||,
(5.8)
where f and g are considered identical if ||f −g|| = 0. Thus, in particular, the inﬁmum is actually
a minimum.
Proof: Let ∆denote the inﬁmum of (5.8) and choose a sequence of fn ∈M so that ||f −fn|| →∆.
From the parallelogram law
||fn −fm||2 = 2||fn −f||2 + 2||fm −f||2 −4||fn + fm
2
−f||2.
Since M is a subspace, (fn + fm)/2 ∈M and hence the latter norm squared is bound below by ∆2.
This means, however, that
lim
n→∞,m→∞||fn −fm||2 ≤2∆2 + 2∆2 −4∆2 = 0,
and hence fn is a Cauchy sequence. Since M is complete, it must have a limit, say ˆf. From the
triangle inequality
||f −ˆf|| ≤||f −fn|| + ||fn −ˆf|| →n→∞∆,
which proves that there is an ˆf in M with the desired property. Suppose next that there are two
such functions, e.g., ˆf and g. Again invoking (5.7)
|| ˆf −g||2 ≤2|| ˆf −f||2 + 2||g −f||2 −4||
ˆf + g
2
−f||2.
Since ( ˆf + g)/2 ∈M, the latter norm can be no smaller than ∆, and hence
|| ˆf −g||2 ≤2∆2 + 2∆2 −4∆2 = 0,
and therefore || ˆf −g|| = 0 and the two are identical.
2
Two elements f and g of L will be called orthogonal and we write f ⊥g if (f, g) = 0. If M ⊂L,
we say that f ∈L is orthogonal to M if f is orthogonal to every g ∈M. The collection of all f ∈L
which are orthogonal to M is called the orthogonal complement of M.
Lemma 5.5.2 Given the assumptions of Lemma 5.5.1, ˆf will yield the inﬁmum if and only if
f −ˆf ⊥M; that is, ˆf is chosen so that the error f −ˆf is orthogonal to the subspace M.
Proof: First we prove necessity. Suppose that f −ˆf is not orthogonal to M. We will show that ˆf
cannot then yield the inﬁmum of Lemma 5.5.1. By assumption there is a g ∈M such that g is not
orthogonal to f −ˆf. We can assume that ||g|| = 1 (otherwise just normalize g by dividing by its
norm). Deﬁne a = (f −ˆf, g) ̸= 0. Now set h = ˆf + ag. Clearly h ∈M and we now show that it is a
better estimate of f than ˆf is. To see this write
||f −h||2
=
||f −ˆf −ag||2
=
||f −ˆf||2 + a2||g||2 −2a(f −ˆf, g)
=
||f −ˆf||2 −a2 < ||f −ˆf||2

104
CHAPTER 5. CONDITIONAL PROBABILITY AND EXPECTATION
as claimed.
Thus orthogonality is necessary for the minimum norm estimate.
To see that it is
suﬃcient, suppose that ˆf ⊥M. Let g be any other element in M. Then
||f −g||2
=
||f −ˆf + ˆf −g||2
=
||f −ˆf||2 + || ˆf −g||2 −2(f −ˆf, ˆf −g).
Since ˆf −g ∈M, the inner product is 0 by assumption, proving that ||f −g|| is strictly greater than
||f −ˆf|| if || ˆf −g|| is not identically 0.
2
Combining the two lemmas we have the following famous result.
Theorem 5.5.1 (The Projection Theorem)
Suppose that M is a closed subspace of a Hilbert space L and that f ∈L. Then there is a unique
ˆf ∈M with the properties that
f −ˆf ⊥M
and
||f −ˆf|| = inf
g∈M ||f −g||.
The resulting ˆf is called the projection of f onto M and will be denoted PM(f).
Exercises
1. Show that projections satisfy the following properties:
(a) If f ∈M, then PM(f) = f.
(b) For any scalar a PM(af) = aPM(f).
(c) For any f, g ∈L and scalars a, b:
PM(af + bg) = aPM(f) + bPM(g).
2. Consider the special case of L2(m) of all square-integrable functions with respect to a prob-
ability measure m. Let P be the projection with respect to a closed subspace M. Prove the
following:
(a) If f is a constant, say c, then P(f) = c.
(b) If f ≥0 with probability one, then P(f) ≥0.
3. Suppose that g is a discrete measurement and hence is in L2(m). Let M be the space of all
square-integrable σ(g)-measurable functions. Assume that M is complete. Given a discrete
measurement f, let E(f|g) denote the conditional expectation of exercise 5.4.1. Show that
E(f|g) = PM(f), the projection of f onto M.
4. Suppose that H ⊂M are closed subspaces of a Hilbert space L. Show that for any f ∈L,
||f −PM(f)|| ≤||f −PH(f)||.

5.6. THE RADON-NIKODYM THEOREM
105
5.6
The Radon-Nikodym Theorem
This section is devoted to the statement and proof of the Radon-Nikodym theorem, one of the
fundamental results of measure and integration theory and a key result in the proof of the existence
of conditional probability measures.
A measure m is said to be absolutely continuous with respect to another measure P on the same
measurable space and we write m << P if P(F) = 0 implies m(F) = 0, that is, m inherits all of
P’s zero probability events.
Theorem 5.6.1 (The Radon-Nikodym Theorem)
Given two measures m and P on a measurable space (Ω, B) such that m << P, then there is a
measurable function h : Ω→ℜwith the properties that h ≥0 P-a.e. (and hence also m-a.e.) and
m(F) =
Z
F
h dP, all F ∈B.
(5.9)
If g and h both satisfy (5.9), then g = h P-a.e. If in addition f : Ω→ℜis measurable, then
Z
F
f dm =
Z
F
fh dP, all F ∈B.
(5.10)
The function h is called the Radon-Nikodym derivative of m with respect to P and is written h =
dm/dP.
Proof: First note that 5.10 follows immediately from (5.9) for simple functions. The general result
(5.10) then follows, using the usual approximation techniques. Hence we need prove only (5.9),
which will take some work.
Begin by deﬁning a mixture measure q = (m + P)/2, that is, q(F) = m(F)/2 + P(F)/2 for all
events F. For any f ∈L2(q), necessarily f ∈L2(m), and hence Emf is ﬁnite from the Cauchy-
Schwarz inequality. As a ﬁrst step in the proof we will show that there is a g ∈L2(q) for which
Emf =
Z
fgdq, all f ∈L2(q).
(5.11)
Deﬁne the class of measurements M = {f : f ∈L2(q), Emf = 0} and observe that the properties
of expectation imply that M is a closed linear subspace of L2(q). Hence the projection theorem
implies that any f ∈L2(q) can be written as f = ˆf +f ′, where ˆf ∈M and {f}′ ⊥M. Furthermore,
this representation is unique q-a.e. If all of the functions in the orthogonal complement of M were
identically 0, then f = ˆf ∈M and (5.11) would be trivially true with f′ = 0 since Emf = 0
for f ∈M by deﬁnition. Hence we can assume that there is some r ⊥M that is not identically
0. Furthermore,
R
r dm ̸= 0 lest we have an r ∈M such that r ⊥M, which can only be if r is
identically 0.
Deﬁne for x ∈Ω
g(x) = r(x)Emr
||r||2
,
where here ||r|| is the L2(q) norm. Clearly g ∈L2(q) and by construction g ⊥M since r is. We
have that
Z
fgdq =
Z
(f −rEmf
Emr )gdq +
Z
rg Emf
Emr dq.

106
CHAPTER 5. CONDITIONAL PROBABILITY AND EXPECTATION
The ﬁrst term on the right is 0 since g ⊥M and f −r(Emf)/(Emr) has zero expectation with
respect to m and hence is in M. Thus
Z
fgdq = Emf
Emr
Z
rgdq = Emf
Emr
Emr||r||2
||r||2
= Emf
which proves (5.11). Observe that if we set f = 1F for any event F, then
Em1F = m(F) =
Z
F
gdq,
a formula which resembles the desired formula (5.9) except that the integral is with respect to q
instead of m. From the deﬁnition of q 0 ≤m(F) ≤2q(F) and hence
0 ≤
Z
F
gdq ≤
Z
F
2dq, all events F.
From Lemma 4.4.7 this implies that g ∈[0, 2] q-a.e.. Since m << q and P << q, this statement also
holds m-a.e. and P-a.e..
As a next step rewrite (5.11) in terms of P and m as
Z
f(1 −g
2) dm =
Z fg
2 dP, all f ∈L2(q).
(5.12)
We next argue that (5.12) holds more generally for all measurable f in the sense that each integral
exists if and only if the other does, in which case they are equal. To see this, ﬁrst assume that f ≥0
and choose the usual quantizer sequence of Lemma 4.3.1. Then (5.12) implies that
Z
qn(f)(1 −g
2) dm =
Z qn(f)g
2
dP, all f ∈L2(q)
for all n. Since g is between 0 and 2 with both m and P probability 1, both integrands are nonnegative
a.e. and converge upward together to a ﬁnite number or diverge together. In either case, the limit
is
R
f(1 −g/2) dm =
R
fg/2dP and (5.12) holds. For general f apply this argument to the pieces of
the usual decomposition f = f+ −f −, and the conclusion follows, both integrals either existing or
not existing together.
To complete the proof, deﬁne the set B = {x : g(x) = 2}. Application of (5.12) to 1B implies
that
P(B) =
Z
B
dP =
Z
1B
g
2dP =
Z
1B(1 −g
2) dm = 0,
and hence P(Bc) = 1. Combining this fact with (5.12) with f = (1 −g/2)−1 implies that for any
event F
m(F)
=
m(F ∩B) + m(F ∩Bc)
=
m(F ∩B) +
Z
F ∩Bc
1 −g
2
1 −g
2
dm
=
m(F ∩B) +
Z
F ∩Bc
g
1 −g
2
1
2 dP.
(5.13)
Deﬁne the function h by
h(x) =
g(x)
2 −g(x)1Bc(x).
(5.14)

5.6. THE RADON-NIKODYM THEOREM
107
and (5.13) becomes
m(F) = m(F ∩B) +
Z
F ∩Bc hdP, all F ∈B.
(5.15)
Note that (5.15) is true in general; that is, we have not yet used the fact that m << P. Taking this
fact into account, m(B) = 0, and hence since P(Bc) = 1,
m(F) =
Z
F
hdP
(5.16)
for all events F. This formula combined with (5.14) and the fact that g ∈[0, 2] with P and m
probability 1 completes the proof.
2
In fact we have proved more than just the Radon-Nikodym theorem; we have also almost proved
the Lebesgue decomposition theorem. We next complete this task.
Theorem 5.6.2 (The Lebesgue Decomposition Theorem)
Given two probability measures m and P on a measurable space (Ω, B), there are unique probability
measures ma and p and a number λ ∈[0, 1] such that such that m = λma+(1−λ)p, where ma << P
and where p and P are singular in the sense that there is a set G ∈B with P(G) = 1 and p(G) = 0.
ma is called the part of m absolutely continuous with respect to P.
Proof: If m << P, then the result is just the Radon-Nikodym theorem with ma = m and λ = 1. If
m is not absolutely continuous with respect to P then several cases are possible: If m(B) in (5.16)
is 0, then the result again follows as in (5.16). If m(B) = 1, then p = m and P are singular and the
result holds with λ = 0. If m(B) ∈(0, 1), then (5.15) yields the theorem by identifying
ma(F) =
R
F hdP
R
ΩhdP ,
p(F) = mB(F) = m(F ∩B)
m(B)
,
and
λ =
Z
Ω
hdP = m(Bc),
where the last equality follows from (5.15) with F = Bc.
2
The Lebesgue decomposition theorem is important because it permits any probability measure
to be decomposed into an absolutely continuous part and a singular part. The absolutely continuous
part can always be computed by integrating a Radon-Nikodym derivative, and hence such derivatives
can be thought of as probability density functions.
Exercises
1. Show that if m << P, then P(F) = 1 implies that m(F) = 1. Two measures m and P are said
to be equivalent if m << P and P << m. How are the Radon-Nikodym derivatives dP/dm
and dm/dP related?

108
CHAPTER 5. CONDITIONAL PROBABILITY AND EXPECTATION
5.7
Conditional Probability
Assume that we are given a probability space (Ω, B, m) and class of measurements M and we wish
to deﬁne a conditional probability m(G|M) for all events G. Intuitively, if we are told the output
values of all of the measurements f in the class M (which might contain only a single measurement or
an uncountable family), what then is the probability that G will occur? The ﬁrst observation is that
we have not yet speciﬁed what the given output values of the class are, only the class itself. We can
easily nail down these output values by thinking of the conditional probability as a function of ω ∈Ω
since given ω all of the f(ω), f ∈M, are ﬁxed. Hence we will consider a conditional probability as a
function of sample points of the form m(G|M)(ω) = m(G|f(ω), all f ∈M), that is, the probability
of G given that f = f(ω) for all measurements f in the given collection M. Analogous to (5.4),
since m(G|M) is to depend only on the output values of the f ∈M, mathematically it should be
in M(σ(M)), that is, it should be measurable with respect to σ(M), the σ-ﬁeld induced by the
conditioning class.
Analogous to (5.6) the conditional probability of G given M should not be changed if we are
also given that an event in σ(M) occurred and hence averaging yields
m(G ∩F) =
Z
F
m(G|M) dm, F ∈σ(M).
This leads us to the following deﬁnition:
Given a class of measurements M and an event G, the conditional probability m(G|M) of G
given M is deﬁned as any function such that
m(G|M) is σ(M) −measurable, and
(5.17)
m(G ∩F) =
Z
F
m(G|M) dm, all F ∈σ(M).
(5.18)
Clearly we have yet to show that in general such a function exists. First, however, observe that
the preceding deﬁnition really depends on M only through the σ-ﬁeld that it induces. Hence more
generally we can deﬁne a conditional probability of an event G given a sub-σ-ﬁeld G as any function
m(G|G) such that
m(G|G) is G −measurable, and
(5.19)
m(G ∩F) =
Z
F
m(G|G) dm, all F ∈G.
(5.20)
Intuitively, the conditional probability given a σ-ﬁeld is the conditional probability given the
knowledge of which events in the σ-ﬁeld occurred and which did not or, equivalently, given the
outcomes of the indicator functions for all events in the σ-ﬁeld. That is, as stated in Lemma 5.2.2,
if G is a σ-ﬁeld and M = {all 1F ; F ∈G}, then G = σ(M) and hence m(G|G) = m(G|M).
By construction,
m(G|M) = m(G|σ(M)).
(5.21)
Furthermore, consider m(G|M(σ(M))). This is similarly given by the conditional probability given
the induced class of measurements, m(G|σ(M(σ(M))). From (5.1), however, the conditioning σ-ﬁeld
is exactly σ(M). Thus
m(G|M) = m(G|σ(M)) = m(G|M(σ(M))),
(5.22)
reinforcing the intuition discussed in Section 5.2 that knowing the outcomes of functions in M is
equivalent to knowing the occurrence or nonoccurrence of events in σ(M), which is in turn equivalent

5.7. CONDITIONAL PROBABILITY
109
to knowing the outcomes of functions in M(σ(M)). It will usually be more convenient to develop
general results for the more general notion of a conditional probability given a σ-ﬁeld.
We are now ready to demonstrate the existence and essential uniqueness of conditional proba-
bility.
Theorem 5.7.1 Given a probability space (Ω, B, m), an event G ∈B, and a sub-σ-ﬁeld G, then
there exists a version of the conditional probability m(G|G). Furthermore, any two such versions are
equal m-a.e.
Proof: If m(G) = 0, then (5.20) becomes
0 =
Z
F
m(G|G) dm, all F ∈G,
and hence if m(G|G) is G-measurable, it is 0 m-a.e. from Corollary 5.3.1, completing the proof
for this case. If m(G) ̸= 0, then deﬁne the probability measure mG on (Ω, G) as the elementary
conditional probability
mG(F) = m(F|G) = m(F ∩G)
m(G)
, F ∈G.
The restriction mG
G of m(·|G) to G is absolutely continuous with respect to mG, the restriction of
m to G. Thus mG
G << mG and hence from the Radon-Nikodym theorem there exists an essentially
unique almost everywhere positive G-measurable function h such that
mG
G (F) =
Z
F
hdmG
and hence from Lemma 5.3.1
mG
G (F) =
Z
F
hdm
and hence
m(F ∩G)
m(G)
=
Z
F
h dm, F ∈G
and hence m(G)h is the desired conditional probability measure m(G|G).
We close this section with a useful technical property: a chain rule for Radon-Nikodym deriva-
tives.
Lemma 5.7.1 Suppose that three measures on a common space satisfy m << λ << P. Then P-a.e.
dm
dP = dm
dλ
dλ
dP .
Proof: From (5.9) and (5.10)
m(F) =
Z
F
dm
dλ dλ =
Z
F
(dm
dλ
dλ
dP ) dP,
which deﬁnes dm
dλ
dλ
dP as a version of dm
dP .

110
CHAPTER 5. CONDITIONAL PROBABILITY AND EXPECTATION
Exercises
1. Given a probability space (Ω, B, m), suppose that G is a ﬁnite ﬁeld with atoms Gi; i =
1, 2, . . . , N. Is it true that
m(F|G)(ω) =
N
X
i=1
m(F|Gi)1Gi(ω)?
5.8
Regular Conditional Probability
Given an event F, the elementary conditional probability m(G|F) of G given F is itself a probability
measure m(·|F) when considered as a function of the events G. A natural question is whether or
not a similar conclusion holds in the more general case. In other words, we have deﬁned conditional
probabilities for a ﬁxed event G and a sub-σ-ﬁeld G. What if we ﬁx the sample point ω and consider
the set function m(·|G)(ω); is it a probability measure on B? In general it may not be, but we shall
see two special cases in this section for which a version of the conditional probabilities exists such
that with probability one m(F|G)(ω), F ∈B, is a probability measure. The ﬁrst case considers
the trivial case of a discrete sub-σ-ﬁeld of an arbitrary σ-ﬁeld. The second case considers the more
important (and more involved) case of an arbitrary sub-σ-ﬁeld of a standard space.
Given a probability space (Ω, B, m) and a sub-σ- ﬁeld G, then a regular conditional probability
given G is a function f : B × Ω→[0, 1] such that
f(F, ω); F ∈B
(5.23)
is a probability measure for each ω ∈Ω, and
f(F, ω); ω ∈Ω
(5.24)
is a version of the conditional probability of F given G.
Lemma 5.8.1 Given a probability space (Ω, B, m) and a discrete sub-σ-ﬁeld G (that is, G has a
ﬁnite or countable number of members), then there exists a regular conditional probability measure
given G.
Proof: Let {Fi} denote a countable collection of atoms of the countable sub-σ-ﬁeld G. Then we can
use elementary probability to write a version of the conditional probability for each event G:
m(G|G)(ω) = P(G|Fi) = m(G ∩Fi)
m(Fi)
; ω ∈Fi
for those Fi with nonzero probability.
If ω is in a zero probability atom, then the conditional
probability can be set to p∗(G) for some arbitrary probability measure p∗. If we now ﬁx ω, it is
easily seen that the given conditional probability measure is regular since for each atom Fi in the
countable sub-σ-ﬁeld the elementary conditional probability P(·|Fi) is a probability measure.
2
Theorem 5.8.1 Given a probability space (Ω, B, m) and a sub-σ-ﬁeld G, then if (Ω, B) is standard
there exists a regular conditional probability measure given G.
Proof: For each event G let m(G|G)(ω), ω ∈Ω, be a version of the conditional probability of G
given G. We have that
Z
F
m(G|G) dm = m(G ∩F) ≥0, all F ∈G,

5.8. REGULAR CONDITIONAL PROBABILITY
111
and hence from Corollary 5.3.1 m(G|G) ≥0 a.e. Thus for each event G there is an event, say H(G),
with m(H(G)) = 1 such that if ω ∈H(G), then m(G|G)(ω) ≥0. In addition, H(G) ∈G since the
indicator function of H(G) is 1 on the set of ω on which a G-measurable function is nonnegative,
and hence this indicator function must also be G measurable. Since (Ω, B) is standard, it has a basis
{Fn} which generates B as in Chapter 2. Let F be the countable union of all of the sets in the basis.
Deﬁne the set
H0 =
\
G∈F
H(G)
and hence H0 ∈G, m(H0) = 1, and if ω ∈H0, then m(G|G)(ω) ≥0 for all G ∈F. We also have
that
Z
F
m(Ω|G) dm = m(Ω∩F) = m(F) =
Z
F
1 dm, all F ∈G,
and hence m(Ω|G) = 1 a.e.
Let H1 be the set {ω : m(Ω|G)(ω) = 1} and hence H1 ∈G and
m(H1) = 1. Similarly, let F1, F2, . . . , Fn be any ﬁnite collection of disjoint sets in F. Then
Z
F
m(
n
[
i=1
Fi|G) dm
=
m((
n
[
i=1
Fi) ∩F) = m(
n
[
i=1
Fi ∩F)
=
n
X
i=1
m(Fi ∩F) =
n
X
i=1
Z
F
m(Fi|G) dm
=
Z
F
n
X
i=1
m(Fi|G) dm.
Hence
m(
n
[
i=1
Fi|G) =
n
X
i=1
m(Fi|G)a.e.
Thus there is a set of probability one in G on which the preceding relation holds. Since there are
a countable number of choices of ﬁnite collections of disjoint unions of elements of F, we can ﬁnd
a set, say H2 ∈G, such that if ω ∈H2, then m(·|G)(ω) is ﬁnitely additive on F for all ω ∈H2
and m(H2) = 1. Note that we could not use this argument to demonstrate countable additivity
on F since there are an uncountable number of choices of countable collections of F. Although
this is a barrier in the general case, it does not aﬀect the standard space case. We now have a
set H = H0 ∩H1 ∩H2 ∈G with probability one such that for any ω in this set, m(·|G)(ω) is a
normalized, nonnegative, ﬁnitely additive set function on F. We now construct the desired regular
conditional probability measure.
Deﬁne f(F, ω) = m(F|G)(ω) for all ω ∈H, F ∈F. Fix an arbitrary probability measure P and
for all ω ̸∈H deﬁne f(F, ω) = P(F), all F ∈B. From Theorem 2.6.1 F has the countable extension
property, and hence for each ω ∈H we have that f(·, ω) extends to a unique probability measure
on B, which we also denote by f(·, ω). We now have a function f such that f(·, ω) is a probability
measure for all ω ∈Ω, as required. We will be done if we can show that if we ﬁx an event G, then
the function we have constructed is a version of the conditional probability for G given B, that is, if
we can show that it satisﬁes (5.19) and (5.20).
First observe that for G ∈F, f(G, ω) = m(G|G)(ω) for ω ∈H, where H ∈G, and f(G, ω) = P(G)
for ω ∈Hc. Thus f(G, ω) is a G-measurable function of ω for ﬁxed G in F. This and the extension
formula (1.18)imply that more generally f(G, ω) is a G-measurable function of ω for any G ∈B,
proving (5.19).

112
CHAPTER 5. CONDITIONAL PROBABILITY AND EXPECTATION
If G ∈F, then by construction m(G|G)(ω) = f(G, ω) a.e. and hence (5.20) holds for f(G, .). To
prove that it holds for more general G, ﬁrst observe that (5.20) is trivially true for those F ∈G with
m(F) = 0. If m(F) > 0 we can divide both sides of the formula by m(F) to obtain
mF (G) =
Z
F
f(G, ω)dm(ω)
m(F) .
We know the preceding equality holds for all G in a generating ﬁeld F. To prove that it holds for all
events G ∈B, ﬁrst observe that the left-hand side is clearly a probability measure. We will be done
if we can show that the right-hand side is also a probability measure since two probability measures
agreeing on a generating ﬁeld must agree on all events in the generated σ-ﬁeld. The right-hand
side is clearly nonnegative since f(·, ω) is a probability measure for all ω. This also implies that
f(Ω, ω) = 1 for all ω, and hence the right-hand side yields 1 for G = Ω. If Gk, k = 1, 2, . . . is a
countable sequence of disjoint sets with union G, then since the f(Gi, ω) are nonnegative and the
f(·, ω) are probability measures,
n
X
i=1
f(Gi, ω) ↑
∞
X
i=1
f(Gi, ω) = f(G, ω)
and hence from the monotone convergence theorem that
∞
X
i=1
Z
F
f(Gi, ω)dm(ω)
m(F)
=
lim
n→∞
n
X
i=1
Z
F
f(Gi, ω)dm(ω)
m(F) = lim
n→∞
Z
F
(
n
X
i=1
f(Gi, ω))dm(ω)
m(F)
=
Z
F
( lim
n→∞
n
X
i=1
f(Gi, ω)dm(ω)
m(F) =
Z
F
f(G, ω)dm(ω)
m(F) ,
which proves the countable additivity and completes the proof of the theorem.
2
The existence of a regular conditional probability is one of the most useful aspects of probability
measures on standard spaces.
We close this section by describing a variation on the regular conditional probability results.
This form will be the most common use of the theorem.
Let X : Ω→AX and Y : Ω→AY be two random variables deﬁned on a probability space
(Ω, B, m). Let σ(Y ) = Y −1(BAY ) denote the σ-ﬁeld generated by the random variable Y and for
each G ∈B let m(G|σ(Y ))(ω) denote a version of the conditional probability of F given σ(Y ).
For the moment focus on an event of the form G = X−1(F) for F ∈BAX. This function must
be measurable with respect to σ(Y ), and hence from Lemma 5.2.1 there must be a function, say
g : AY →[0, 1], such that m(X−1(F)|σ(Y ))(ω) = g(Y (ω)). Call this function g(y) = PX|Y (F|y).
By changing variables we know from the properties of conditional probability that
PXY (F × D)
=
m(X−1(F) ∩Y −1(D)) =
Z
Y −1(D)
m(X−1(F)|σ(Y ))(ω) dm(ω)
=
Z
Y −1(D)
PX|Y (F|Y (ω)) dm(ω) =
Z
D
PX|Y (F|y) dPY (y),
where PY = mY −1 is the induced distribution for Y . The preceding formulas simply translate the
conditional probability statements from the original space and σ-ﬁelds to the more concrete example
of distributions for random variables conditioned on other random variables.

5.9. CONDITIONAL EXPECTATION
113
Corollary 5.8.1 Let X : Ω→AX and Y : Ω→AY be two random variables deﬁned on a probability
space (Ω, B, m). Then if any of the following conditions is met, there exists a regular conditional
probability measure PX|Y (F|y) for X given Y :
(a) The alphabet AX is discrete.
(b) The alphabet AY is discrete.
(c) Both of the alphabets AX and AY are standard.
Proof: Using the correspondence between conditional probabilities given sub-σ-ﬁelds and random
variables, two cases are immediate: If the alphabet AY of the conditioning random variable is
discrete, then the result follows from Lemma 5.8.1. If the two alphabets are standard, then so is the
product space, and the result follows in the same manner from Theorem 5.8.1. If AX is discrete,
then its σ-ﬁeld is standard and we can mimic the proof of Theorem 5.8.1. Let F now denote the
union of all sets in a countable basis for BAX and replace m(F|G)(ω) by PX|Y (F|y) and dm by dPY .
The argument then goes through virtually unchanged.
2
5.9
Conditional Expectation
In the previous section we showed that given a standard probability space (Ω, B, m) and a sub-σ-ﬁeld
G, there is a regular conditional probability measure given G: m(G|G)(ω), G ∈B, ω ∈Ω; that is, for
each ﬁxed ω m(·|G)(ω) is a probability measure, and for each ﬁxed G, m(G|G)(ω) is a version of the
conditional probability of G given G, i.e., it satisﬁes (5.19) and (5.20). We can use the individual
probability measures to deﬁne a conditional expectation
E(f|G)(ω) =
Z
f(u) dm(u|G)(ω)
(5.25)
if the integral exists. In this section we collect a few properties of conditional expectation and relate
the preceding constructive deﬁnition to the more common and more general descriptive one.
Fix an event F ∈G and let f be the indicator function of an event G ∈B. Integrating (5.25)
over F using (5.20) and the fact that E(1G|G)(ω) = m(G|G)(ω) yields
Z
F
E(f|G) dm =
Z
F
f dm
(5.26)
since the right-hand side is simply m(G ∩F). From the linearity of expectation (by (5.25) a con-
ditional expectation is simply an ordinary expectation with respect to a measure that is a regular
conditional probability measure for a particular sample point ) and integration, (5.26) also holds for
simple functions. For a nonnegative measurable f, take the usual quantizer sequence qn(f) ↑f of
Lemma 4.3.1, and (5.26) then holds since the two sides are equal for each n and since both sides
converge up to the appropriate integral, the right-hand side by the deﬁnition of the integral of a
nonnegative function and the left-hand side by virtue of the deﬁnition and the monotone convergence
theorem. For general f, use the usual decomposition f = f+ −f −and apply the result to each
piece. The preceding relation then holds in the sense that if either integral exists, then so does the
other and they are equal. Note in particular that this implies that if f is in L1(m), then E(f|G)(ω)
must be ﬁnite m-a.e.
In a similar sequence of steps, the fact that for ﬁxed G the conditional probability of G given G
is a G-measurable function implies that E(f|G)(ω) is a G-measurable function if f is an indicator

114
CHAPTER 5. CONDITIONAL PROBABILITY AND EXPECTATION
function, which implies that it is also G-measurable for simple functions and hence also for limits of
simple functions.
Thus we have the following properties: If f ∈L1(m) and h(ω) = E(f|G)(ω), then
h(ω) is G −measurable,
(5.27)
and
Z
F
h dm =
Z
F
f dm, all F ∈G.
(5.28)
Eq. (5.28) is often referred to as iterated expectation or nested expectation since it shows that the
expectation of f can be found in two steps: ﬁrst ﬁnd the conditional expectation given a σ-ﬁeld or
class of measurements (in which case G is the induced σ-ﬁeld) and then integrate the conditional
expectation.
These properties parallel the deﬁning descriptive properties of conditional probability and reduce
to those properties in the case of indicator functions. The following simple lemma shows that the
properties provide an alternative deﬁnition of conditional expectation that is valid more generally,
that is, holds even when the alphabets are not standard. Toward this end we now give the general
descriptive deﬁnition of conditional expectation: Given a probability space (Ω, B, m), a sub-σ-ﬁeld
G, and a measurement f ∈L1(m), then any function h is said to be a version of the conditional
expectation E(f|G). From Corollary 5.3.1, any two versions must be equal a.e. If the underlying space
is standard, then the descriptive deﬁnition therefore is consistent with the constructive deﬁnition.
It only remains to show that the conditional expectation exists in the general case, that is, that we
can always ﬁnd a version of the conditional expectation even if the underlying space is not standard.
To do this observe that if f is a simple function P
i ai1Gi, then
h =
X
i
aim(Gi|G)
is G-measurable and satisﬁes (5.28) from the linearity of integration and the properties of conditional
expectation:
Z
F
(
X
i
aim(Gi|G)) dm
=
X
i
ai
Z
F
m(Gi|G) =
X
i
ai
Z
F
1Gidm
=
Z
F
(
X
i
ai1Gi) dm =
Z
F
f dm.
We then proceed in the usual way.
If f ≥0, let qn be the asymptotically accurate quantizer
sequence of Lemma 4.3.1. Then qn(f) ↑f and hence E(qn(f)|G) is nondecreasing (Lemma 5.3.1),
and h = limn→∞E(qn(f)|G) satisﬁes (5.27) since limits of G-measurable functions are measurable.
The dominated convergence theorem implies that h also satisﬁes (5.28).
The result for general
integrable f follows from the decomposition f = f+ −f −. We have therefore proved the following
result.
Lemma 5.9.1 Given a probability space (Ω, B, m), a sub-σ- ﬁeld G, and a measurement f ∈L1(m),
then there exists a G-measurable real-valued function h satisfying the formula
Z
F
h dm =
Z
F
f dm, all F ∈G;
(5.29)
that is, there exists a version of the conditional expectation E(f|G) of f given G. If the underlying
space is standard, then also (5.25) holds a.e.; that is, the constructive and descriptive deﬁnitions are
equivalent on standard spaces.

5.9. CONDITIONAL EXPECTATION
115
The next result shows that the remark preceding (5.26) holds even if the space is not standard.
Corollary 5.9.1 Given a probability space (Ω, B, m) (not necessarily standard), a sub-σ-ﬁeld G, and
an event G ∈G, then with probability one
m(G|G) = E(1G|G).
Proof: From the descriptive deﬁnition of conditional expectation
Z
F
E(1G|G) dm =
Z
F
1G dm = m(G ∩F), for allF ∈G,
for any G ∈B. But, by (5.20), this is just
R
F m(G|G)dm for all F ∈G. Since both E(1G|G) and
m(G|G) are G-measurable, Corollary 5.3.1 completes the proof.
2
Corollary 5.9.2 If f ∈L1(m) is G-measurable, then E(f|G) = f m-a.e..
Proof: The proof follows immediately from (5.28) and Corollary 5.3.1.
2
If a conditional expectation is deﬁned on a standard space using the constructive deﬁnitions,
then it inherits the properties of an ordinary expectation; e.g., Lemma 4.4.2 can be immediately
extended to conditional expectations. The following lemma shows that Lemma 4.4.2 extends to
conditional expectation in the more general case of the descriptive deﬁnition.
Lemma 5.9.2 Let (Ω, B, m) be a probability space, G a sub-σ-ﬁeld, and f and g integrable measure-
ments.
(a) If f ≥0 with probability 1, then Em(f|G) ≥0.
(b) Em(1|G) = 1.
(c) Conditional expectation is linear; that is, for any real α, β and any measurements f and g,
Em(αf + βg|G) = αEm(f|G) + βEm(g|G).
(d) Em(f|G) exists and is ﬁnite if and only if Em(|f||G) is ﬁnite and
|Emf| ≤Em|f|.
(e) Given two measurements f and g for which f ≥g with probability 1, then
Em(f|G) ≥Em(g|G).
Proof: (a) If f ≥0 and
Z
F
E(f|G) dm =
Z
Ff dm, all F ∈G,
then Lemma 4.4.2 implies that the right-hand side is nonnegative for all F ∈G. Since E(f|G) is
G-measurable, Corollary 5.3.1 then implies that E(f|G) ≥0.
(b) For f = 1, the function h = 1 is both G-measurable and satisﬁes
Z
F
hdm =
Z
F
f dm = m(F), all F ∈G.

116
CHAPTER 5. CONDITIONAL PROBABILITY AND EXPECTATION
Since (5.27)-(5.28) are satisﬁed with h = 1, 1 must be a version of E(1|G).
(c) Let E(f|G) and E(g|G) be versions of the conditional expectations of f and g given G. Then
h = αE(f|G) + βE(g|G) is G-measurable and satisﬁes (5.28) with f replaced by αf + βg. Hence h
is a version of E(αf + βg|G).
(d) Let f = f +−f −be the usual decomposition into positive and negative parts f+ ≥0, f −≥0.
From part (c), E(f|G) = E(f+|G) −E(f −|G). From part (a), E(f+|G) ≥0 and E(f−|G) ≥0. Thus
again using part (c)
E(f|G)
≤
E(f +|G) + E(f −|G)
=
E(f + + f −|G) = E(|f||G).
(e) The proof follows from parts (a) and (c) by replacing f by f −g.
2
The following result shows that if a measurement f is square-integrable, then its conditional
expectation E(f|G) has a special interpretation. It is the projection of the measurement onto the
space of all square-integrable G-measurable functions.
Lemma 5.9.3 Given a probability space (Ω, B, m), a measurement f ∈L2(m), and a sub-σ-ﬁeld
G ⊂B. Let M denote the subset of L2(m) consisting of all square-integrable G-measurable functions.
Then M is a closed subspace of L2(m) and
E(f|G) = PM(f).
Proof: Since sums and limits of G-measurable functions are G-measurable, M is a closed subspace
of L2(m). From the projection theorem (Theorem 5.5.1) there exists a function ˆf = PM(f) with
the properties that ˆf ∈M and f −ˆf ⊥M. This fact implies that
Z
fg dm =
Z
ˆfgdm
(5.30)
for all g ∈M. Choosing g = 1G for any G ∈G yields
Z
Gf dm =
Z
ˆf dm, all G ∈G.
Since ˆf is F-measurable, this deﬁnes ˆf as a version of E(f|G) and proves the lemma.
2
Eq. (5.30) and the lemma immediately yield a generalization of the nesting property of condi-
tional expectation.
Corollary 5.9.3 If g is G-measurable and f, g ∈L2(m), then
E(fg) = E(gE(f|G)).
The next result provides some continuity properties of conditional expectation.
Lemma 5.9.4 For i = 1, 2, if f ∈Li(m), then
||E(f|G)||i ≤||f||i, i = 1, 2,
and thus if f ∈Li(m), then also E(f|G) ∈L1(m). If f, g ∈Li(m), then
||E(f|G) −E(g|G)||i ≤||f −g||i.
Thus for i = 1, 2, if fn →f in Li(m), then also E(fn|G) →E(f|G).

5.9. CONDITIONAL EXPECTATION
117
Proof: The ﬁrst inequality implies the second by replacing f with f −g and using linearity. For
i = 1 from the usual decomposition f = f+ −f −, f + ≥0, f −≥0, the linearity of conditional
expectation, and Lemma 5.9.2(c) that
||E(f|G)||i =
Z
|E(f|G)|dm =
Z
|E(f + −f −|G)|dm
=
Z
|E(f +|G) −E(f −|G)| dm.
From Lemma 5.9.2(a), however, E(f+|G) ≥0 and E(f−|G) ≥0 a.e., and hence the right-hand side
is bound above by
Z
(E(f +|G) + E(f −|G)) dm =
Z
E(f + + f −|G) dm
=
Z
E(|f||G) dm = E(|f|) = ||f||1.
For i = 2 observe that
0
≤
E[(f −E(f|G))2]
=
E(f 2) + E[E(f|G)2] −2E[fE(f|G)].
Apply iterated expectation and Corollary 5.9.3 to write
E[fE(f|G)]
=
E(E[fE(f|G)]|G)
=
E(E(f|G)E(f|G)) = E[E(f|G)2].
Combining these two equations yields
E(f 2) −E[E(f|G)2] ≥0
or
||f||2
2 ≥||E(f|G)||2
2,
completing the proof.
2
A special case where the conditional results simplify occurs when the measurement and the
conditioning are independent in a sense we next make precise. Given a probability space (Ω, B, m),
two events F and G are said to be independent if m(F ∩G) = m(F)m(G). Two measurements
f : Ω→Af and g : Ω→Ag are said to be independent if the events f−1(F) and g−1(G) are
independent for all events F and G; that is,
m(f ∈F and g ∈G)
=
m(f −1(F) ∩g−1(G)) = m(f −1(F))m(g−1(G))
=
m(f ∈F)m(g ∈G); all F ∈BAf ; G ∈BAg.
Exercises
1. Prove the following corollary for indicator functions and simple functions and then prove the
general result.

118
CHAPTER 5. CONDITIONAL PROBABILITY AND EXPECTATION
Corollary 5.9.4 If f and g are independent measurements, then
E(f|σ(g)) = E(f),
and
E(fg) = E(f)E(g).
Thus, for example, if f and g are independent, then m(f ∈F|g) = m(f ∈F) a.e.
2. Prove the following generalization of Corollary 5.9.3.
Lemma 5.9.5 Suppose that G1 ⊂G2 are two sub-σ-ﬁelds of B and that f ∈L1(m). Prove
that
E(E(f|G2)|G1) = E(f|G1).
3. Given a probability space (Ω, B, m) and a measurement f, for what sub-σ-ﬁeld G ⊂B is it
true that E(f|G) = E(f) a.e.? For what sub-σ-ﬁeld H ⊂B is it true that E(f|H) = f a.e.?
4. Suppose that Gn is an increasing sequence of sub-σ-ﬁelds: Gn ⊂Gn+1 and f is an integrable
measurement. Deﬁne the random variables
Xn = E(f|Gn).
(5.31)
Prove that Xn has the properties that it is measurable with respect to Gn and
E(Xn|X1, X2, . . . , Xn−1) = Xn−1;
(5.32)
that is, the conditional expectation of the next value given all the past values is just the
most recent value. A sequence of random variables with this property is called a martingale
and there is an extremely rich theory regarding the convergence and uniform integrability
properties of such sequences. (See, e.g., the classic reference of Doob [16] or the treatments in
Ash [1] or Breiman [8].)
We point out in passing the general form of martingales, but we will not develop the properties
in any detail. A martingale consists of a sequence of random variables {Xn} deﬁned on a
common probability space (Ω, B, m) together with an increasing sequence of sub-σ-ﬁelds Bn
with the following properties: for n = 1, 2, . . .
E|Xn| < ∞,
(5.33)
Xn is Bn −measurable,
(5.34)
and
E(Xn+1|Bn) = Xn.
(5.35)
{Xn, Bn} is instead a submartingale if the third relation is replaced by E(Xn+1|Bn) ≥Xn and
a supermartingale if E(Xn+1|Bn) ≤Xn. Martingales have their roots in gambling theory, in
which a martingale can be considered as a fair game–the capital expected next time is given
by the current capital. A submartingale represents an unfavorable game (as in a casino), and
a supermartingale a favorable game (as has been the stock market over the past decade).
5. Show that {Xn, Bn} is a martingale if and only if for all n
Z
G
Xn+1dm =
Z
G
Xn dm, all G ∈Bn.

5.10. INDEPENDENCE AND MARKOV CHAINS
119
5.10
Independence and Markov Chains
In this section we apply some of the results and interpretations of the previous sections to obtain
formulas for probabilities involving random variables that have a particular relationship called the
Markov chain property.
Let (Ω, B, P) be a probability space and let X : Ω→AX, Y : Ω→AY , and Z : Ω→AZ be three
random variables deﬁned on this space with σ-ﬁelds BAX, BAY , and BAZ, respectively. As we wish
to focus on the three random variables rather than on the original space, we can consider (Ω, B, P)
to be the space (AX × AY × AZ, BAX × BAY × BAZ, PXY Z).
As a preliminary, let PXY denote the joint distribution of the random variables X and Y ; that
is,
PXY (F × G) = P(X−1(F) ∩Y −1(G)); F ∈BX, G ∈BY .
Translating the deﬁnition of independence of measurements of Section 5.9 into distributions the
random variables X and Y are independent if and only if
PXY (F × G) = PX(F)PY (F); F ∈BX, G ∈BY ,
where PX and PY are the marginal distributions of X and Y . We can also state this in terms of joint
distributions in a convenient way. Given two distributions PX and PY , deﬁne the product distribution
PX×PY or PXtimesY (both notations are used) as the distribution on (AX×AY , BAX ×BAY ) speciﬁed
by
PX×Y (F × G) = PX(F)PY (G); F ∈BX, G ∈BY .
Thus the product distribution has the same marginals as the original distribution, but it forces the
random variables to be independent.
Lemma 5.10.1 Random variables X and Y are independent if and only if PXY = PX×Y . Given
measurements f : AX →Af and g : AY →Ag, then if X and Y are independent, so are f(X) and
g(Y ) and
Z
fgdPX×Y = (
Z
fdPX)(
Z
gdPY ).
Proof: The ﬁrst statement follows immediately from the deﬁnitions of independence and product
distributions. The second statement follows from the fact that
Pf(X),g(Y )(F, G)
=
PX,Y (f −1(F), g−1(G))
=
PX(f −1(F))PY (g−1(G))
=
Pf(X)(F)Pg(Y )(G).
The ﬁnal statement then follows from Corollary 5.9.4.
2
We next introduce a third random variable and a form of conditional independence among ran-
dom variables. As considered in Section 5.8, the conditional probability P(F|σ(Z)) can be written
as g(Z(ω)) for some function g(z) and we deﬁne this function to be P(F|z). Similarly the con-
ditional probability P(F|σ(Z, Y )) can be written as r(Z(ω), Y (ω)) for some function r(z, y) and
we denote this function P(F|z, y). We can then deﬁne the conditional distributions PX|Z(F|z) =
P(X−1(F)|z), PY |Z(G|z) = P(Y −1(G)|z), PXY |Z(F×G|z) = P(X−1(F)∩Y −1(G)|z), and PX|Z, Y (F|z, y) =
P(X−1(F)|z, y). We say that Y →Z →X is a Markov chain if for any event F ∈BAX with prob-
ability one
PX|Z,Y (F|z, y) = PX|Z(F|z),

120
CHAPTER 5. CONDITIONAL PROBABILITY AND EXPECTATION
or in terms of the original space, with probability one
P(X−1(F)|σ(Z, Y )) = P(X−1(F)|σ(Z)).
The following lemma provides an equivalent form of this property in terms of a conditional indepen-
dence relation.
Lemma 5.10.2 The random variables Y ,Z, X form a Markov chain Y →Z →X if and only if
for any F ∈BAX and G ∈BAY we know that with probability one
PXY |Z(F × G|z) = PX|Z(F|z)PY |Z(G|z),
or, equivalently, with probability one
P(X−1(F) ∩Y −1(G)|σ(Z)) = P(X−1(F)|σ(Z))P(Y −1(G)|σ(Z)).
If the random variables have the preceding property, we also say that X and Y are conditionally
independent given Z.
Proof: For any F ∈BAX, G ∈BAY , and D ∈BAZ we know from the properties of conditional
probability that
P(X−1(F) ∩Y −1(G) ∩Z−1(D))
=
Z
Y −1(G)∩Z−1(D)
P(X−1(F)|σ(Z, Y )) dP
=
Z
Y −1(G)∩Z−1(D)
P(X−1(F)|σ(Z)) dP
using the deﬁnition of the Markov property. Using ﬁrst (5.28) and then Corollary 5.9.1 this integral
can be written as
Z
1Z−1(D)1Y −1(G)P(X−1(F)|σ(Z)) dP
=
Z
E[1Z−1(D)1Y −1(G)P(X−1(F)|σ(Z))|σ(Z)] dP
=
Z
1Z−1(D)E[1Y −1(G)|σ(Z)]P(X−1(F)|σ(Z))dP,
where we have used the fact that both 1Z−1(D) and P(X−1(F)|σ(Z)) are measurable with respect
to σ(Z) and hence come out of the conditional expectation. From Corollary 5.9.1, however,
E[1Y −1(G)|σ(Z)] = P(Y −1(G)|σ(Z)),
and hence combining the preceding relations yields
P(X−1F ∩Y −1G ∩Z−1D) =
Z
Z−1(D)
P(Y −1(G)|σ(Z))P(X−1(F)|σ(Z)) dP.
We also know from the properties of conditional probability that
P(X−1F ∩Y −1G ∩Z−1D) =
Z
Z−1(D)
P(Y −1(G) ∩X−1(F)|σ(Z)) dP

5.10. INDEPENDENCE AND MARKOV CHAINS
121
and hence
P(X−1F ∩Y −1G ∩Z−1D) =
Z
Z−1(D)
P(Y −1(G)|σ(Z))P(X−1(F)|σ(Z)) dP
=
Z
Z−1(D)
P(Y −1(G) ∩X−1(F)|σ(Z)) dP.
(5.10.1)
Since the integrands in the two right-most integrals are measurable with respect to σ(Z), it follows
from Corollary 5.3.1 that they are equal almost everywhere, proving the lemma.
2
As a simple example, if the random variables are discrete with probability mass function pXY Z,
then Y, Z, X form a Markov chain if and only if for all x, y, z
pX|Y,Z(x|y, z) = pX|Z(x|y)
or, equivalently,
pX,Y |Z(x, y|z) = pX|Z(x|z)pY |Z(y|z).
We close this section and chapter with an alternative statement of the previous lemma. The
result is an immediate consequence of the lemma and the deﬁnitions. Given the conditions of the
previous lemma, let PXY Z denote the distribution of the three random variables X, Y , and Z. Let
PX×Y |Z denote the distribution (if it exists) for the same random variables speciﬁed by the formula
PX×Y |Z(F × G × D) =
Z
Z−1(D)
P(Y −1(G)|σ(Z))P(X−1(F)|σ(Z)) dP
=
Z
D
PX|Z(F|z)PY |Z(G|z)dPZ(z), F ∈BAX; G ∈BAY ; D ∈BAZ;
that is, PX×Y |Z is the distribution on X, Y, Z which agrees with the original distribution PXY Z on
the conditional distributions of X given Z and Y given Z and with the marginal distribution Z,
but which is such that Y →Z →X forms a Markov chain or, equivalently, such that X and Y
are conditionally independent given Z. Then Y →Z →X forms a Markov chain if and only if
PXY Z = PX×Y |Z.
Comment: The corollary contains the technical qualiﬁcation that the given distribution exists
because it need not in general, that is, there is no guarantee that the given set function is indeed
countably additive and hence a probability measure. If the alphabets are assumed to be standard,
however, then the conditional distributions are regular and it follows that the set function PX×Y |Z
is indeed a distribution.
Exercises
1. Suppose that Yn, n = 1, 2, . . . is a sequence of independent random variables with the same
marginal distributions PYn = PY . Deﬁne the sum
Xn =
n
X
i=1
Yi.
2. Show that the sequence Xn is a martingale as deﬁned in problem 5.9.4. This points out that
martingales can be viewed as one type of memory in a random process, here formed simply by
adding up memoryless random variables.

122
CHAPTER 5. CONDITIONAL PROBABILITY AND EXPECTATION

Chapter 6
Ergodic Properties
6.1
Ergodic Properties of Dynamical Systems
In this chapter we formally deﬁne ergodic properties as the existence of limiting sample averages,
and we study the implications of such properties. We shall see that if sample averages converge for
a suﬃciently large class of measurements, e.g., the indicator functions of all events, then the random
process must have a property called asymptotic mean stationarity and that there is a stationary
measure, called the stationary mean of the process, that has the same sample averages. In addition,
it will be seen that the limiting sample averages can be interpreted as conditional probabilities or
conditional expectations and that under certain conditions convergence of sample averages implies
convergence of the corresponding expectations to a single expectation with respect to the stationary
mean. Finally we shall deﬁne ergodicity of a process and show that it is a necessary condition for
limiting sample averages to be constants instead of random variables.
Although we have seen that there are several forms of convergence of random variables and hence
we could consider several types of ergodic properties, we focus on almost everywhere convergence
and later consider implications for other forms of convergence. The primary goal of this chapter is
the development of necessary conditions for a random process to possess ergodic properties and an
understanding of the implications of such properties. In the next chapter suﬃcient conditions are
developed.
As in the previous chapter we focus on time and ensemble averages of measurements made on a
dynamical system. Let (Ω, B, m, T) be a dynamical system. The case of principal interest will be the
dynamical system corresponding to a random process, that is, a dynamical system (AI, B(A)I, m, T),
where I is an index set, usually either the nonnegative integers or the set of all integers, and T is
a shift on the sequence space AI. The sequence of coordinate random variables {Πn, n ∈I} is the
corresponding random process and m is its distribution. The dynamical system may or may not
be that associated with a directly given alphabet A random process {Xn; n ∈I} described by a
distribution m and having T be the shift on the sequence space AI.
Dynamical systems corresponding to a directly given random process will be the most common
example, but we shall be interested in others as well. For example, the space may be the same, but
we may wish to consider block or variable-length shifts instead of the unit time shift.
As previously, given a measurement f (i.e., a measurable mapping of Ωinto ℜ) deﬁne the sample
123

124
CHAPTER 6. ERGODIC PROPERTIES
averages
< f >n= n−1
n−1
X
i=0
f(T ix).
A dynamical system will be said to have the ergodic property with respect to a measurement f if
the sample average < f >n converges almost everywhere as n →∞. A dynamical system will be
said to have ergodic properties with respect to a class M of measurements if the sample averages
< f >n converge almost everywhere as n →∞for all f in the given class M. When unclear from
context, the measure m will be explicitly given. We shall also refer to mean ergodic properties (of
order i) when the convergence is in Li.
If limn→∞< f >n (x) exists, we will usually denote it by < f > (x) or ˆf(x). For convenience we
consider < f > (x) to be 0 if the limit does not exist. From Corollary 4.6.2 such limits are unique
only in an almost everywhere sense; that is, if < f >n→< f > and < f >n→g a.e., then < f >= g
a.e. Interesting classes of functions that we will consider include the class of all indicator functions
of events, the class of all bounded measurements, and the class of all integrable measurements.
The properties of limits immediately yield properties of limiting sample averages that resemble
those of probabilistic averages.
Lemma 6.1.1 Given a dynamical system, let E denote the class of measurements with respect to
which the system has ergodic properties.
(a) If f ∈E and fT i ≥0 a.e.
for all i, then also
< f >≥0 a.e. (The condition is met, for example, if f(ω) ≥0 for all ω ∈Ω.) (b) The constant
functions f(ω) = r, r ∈ℜ, are in E. In particular, < 1 >= 1. (c) If f, g ∈E then also that
< af + bg >= a < f > +b < g > and hence af + bg ∈E for any real a, b. Thus E is a linear space.
(d) If f ∈E, then also fT i ∈E, i = 1, 2, . . . and < fT i >=< f >; that is, if a system has ergodic
properties with respect to a measurement, then it also has ergodic properties with respect to all shifts
of the measurement and the limit is the same.
Proof: Parts (a)-(c) follow from Lemma 4.5.2. To prove (d) we need only consider the case i = 1
since that implies the result for all i. If f ∈E then with probability one
lim
n→∞
1
n
n−1
X
i=0
f(T ix) = f(x)
or, equivalently,
| < f >n −f| →n→∞0.
The triangle inequality shows that < fT >n must have the same limit:
| < fT >n −< f > |
=
|(n + 1
n
) < f >n+1 −n−1f−< f > |
≤
|(n + 1
n
) < f >n+1 −< f >n+1 | + | < f >n+1 −f| + n−1|f|
≤
1
n| < f >n+1 | + | < f >n+1 −f| + 1
n|f| →n→∞0
where the middle term goes to zero by assumption, implying that the ﬁrst term must also go to
zero, and the right-most term goes to zero since f cannot assume ∞as a value.
The preceding properties provide some interesting comparisons between the space E of all mea-
surements on a dynamical system (Ω, F, m, T) with limiting sample averages and the space L1 of all
measurements with expectations with respect to the same measure. Properties (b) and (c) of limiting

6.1. ERGODIC PROPERTIES OF DYNAMICAL SYSTEMS
125
sample averages are shared by expectations. Property (a) is similar to the property of expectation
that f ≥0 a.e. implies that Ef ≥0, but here we must require that fT i ≥0 a.e. for all nonnegative
integers i. Property (d) is not, however, shared by expectations in general since integrability of f
does not in general imply integrability of fT i. There is one case, however, where the property is
shared: if the measure is invariant, then from Lemma 4.7.1 f is integrable if and only if fT i is, and,
if the integrals exist, they are equal. In addition, if the measure is stationary, then
m({ω : f(T nω) ≥0}) = m({T −nω : f(ω) ≥0}) = m(f ≥0)
and hence in the stationary case the condition for (a) to hold is equivalent to the condition that
f ≥0 a.e.. Thus for stationary measures, there is an exact parallel among the properties (a)-(d) of
measurements in E and those in L1. These similarities between time and ensemble averages will be
useful for both the stationary and nonstationary systems.
The proof of the lemma yields a property of limiting sample averages that will be extremely
important. We formalize this fact as a corollary after we give a needed deﬁnition: Given a dynamical
system (Ω, B, m, T) a function f : Ω→Λ is said to be invariant (with respect to T) or stationary
(with respect to T) if f(Tω) = f(ω), all ω ∈Ω.
Corollary 6.1.1 If a dynamical system has ergodic properties with respect to a measurement f,
then the limiting sample average < f > is an invariant function, that is, < f > (Tx) =< f > (x).
Proof: If limn→∞< f >n (x) =< f >, then from the lemma limn→∞< f >n (Tx) also exists and is
the same, but the limiting time average of fT(x) = f(Tx) is just < fT > (x), proving the corollary.
2
The corollary simply formalizes the observation that shifting x once cannot change a limiting
sample average, hence such averages must be invariant.
Ideally one would like to consider the most general possible measurements at the outset, e.g.,
integrable or square-integrable measurements.
We often, however, initially conﬁne attention to
bounded measurements as a compromise between generality and simplicity. The class of bounded
measurements is simple because the measurements and their sample averages are always integrable
and uniformly integrable and the class of measurements considered does not depend on the under-
lying measure. All of the limiting results of probability theory immediately and easily apply. This
permits us to develop properties that are true of a given class of measurements independent of the
actual measure or measures under consideration. Eventually, however, we will wish to demonstrate
for particular measures that sample averages converge for more general measurements.
Although a somewhat limited class, the class of bounded measurements contains some important
examples. In particular, it contains the indicator functions of all events, and hence a system with
ergodic properties with respect to all bounded measurements will have limiting values for all relative
frequencies, that is, limits of < 1F >n as n →∞. The quantizer sequence of Lemma 4.3.1 can be
used to prove a form of converse to this statement: if a system possesses ergodic properties with
respect to the indicator functions of events, then it also possesses ergodic properties with respect to
all bounded measurable functions.
Lemma 6.1.2 A dynamical system has ergodic properties with respect to the class of all bounded
measurements if and only if it has ergodic properties with respect to the class of all indicator functions
of events, i.e., all measurable indicator functions.
Proof: Since indicator functions are a subclass of all bounded functions, one implication is immediate.
The converse follows by a standard approximation argument, which we present for completeness.
Assume that a system has ergodic properties with respect to all indicator functions of events. Let

126
CHAPTER 6. ERGODIC PROPERTIES
E be the class of all measurements with respect to which the system has ergodic properties. Then
E contains all indicator functions of events, and, from the previous lemma, it is linear. Hence it
also contains all measurable simple functions. The general limiting sample averages can then be
constructed from those of the simple functions in the same manner that we used to deﬁne ensemble
averages, that is, for nonnegative functions form the increasing limit of the averages of simple
functions and for general functions form the nonnegative and negative parts.
Assume that f is a bounded measurable function with, say, |f| ≤K. Let ql denote the quantizer
sequence of Lemma 4.3.1 and assume that l > K and hence |ql(f(ω))−f(ω)| ≤2−l for all l and ω by
construction. This also implies that |ql(f(T iω))−f(T iω)| ≤2−l for all i, l, ω. It is the fact that this
bound is uniform over i (because f is bounded) that makes the following construction work. First
assume that f is nonnegative. Since ql(f) is a simple function, it has a limiting sample average, say
< ql(f) >, by assumption. We can take these limiting sample averages to be increasing in l since
j ≥l implies that
< qj(f) >n −< ql(f) >n
=
< qj(f) −ql(f) >n
=
n−1
n−1
X
i=0
(qj(fT i) −ql(fT i)) ≥0,
since each term in the sum is nonnegative by construction of the quantizer sequence. Hence from
Lemma 6.1.1 we must have that
qj(f) −ql(f) = qj(f) −ql(f) ≥0
and hence < ql(f) > (ω) =< ql(f(ω)) > is a nondecreasing function and hence must have a limit,
say ˆf. Furthermore, since f and hence ql(f) and hence < ql(f) >n and hence < ql(f) > are all in
[0, K], so must ˆf be. We now have from the triangle inequality that for any l
| < f >n −ˆf| ≤| < f >n −< ql(f) >n | + | < ql(f) >n −ql(f)| + |ql(f) −ˆf|.
The ﬁrst term on the right-hand side is bound above by 2−l. Since we have assumed that < ql(f) >n
converges with probability one to < ql(f) >, then with probability one we get an ω such that
< ql(f) >n→n→∞< ql(f) > for all l = 1, 2, . . . (The intersection of a countable number of sets with
probability one also has probability one.) Assume that there is an ω in this set. Then given ϵ we
can choose an l so large that | < ql(f) > −ˆf| ≤ϵ/2 and 2−l ≤ϵ/2, in which case the preceding
inequality yields
lim sup
n→∞| < f >n −ˆf| ≤ϵ.
Since this is true for arbitrary ϵ, for the given ω it follows that < f >n (ω) →ˆf(ω), proving
the almost everywhere result for nonnegative f. The result for general bounded f is obtained by
applying the nonnegative result to f+ and f −in the decomposition f = f+ −f −and using the fact
that for the given quantizer sequence
ql(f) = ql(f +) −ql(f −).
2
6.2
Some Implications of Ergodic Properties
To begin developing the implications of ergodic properties, suppose that a system possesses ergodic
properties for a class of bounded measurements. Then corresponding to each bounded measurement

6.2. SOME IMPLICATIONS OF ERGODIC PROPERTIES
127
f in the class there will be a limit function, say < f >, such that with probability one < f >n→< f >
as n →∞. Convergence almost everywhere and the boundedness of f imply L1 convergence, and
hence for any event G
|n−1
n−1
X
i=0
Z
G
fT i dm −
Z
G
f dm|
=
|
Z
G
(n−1
n−1
X
i=0
fT i −f) dm|
≤
Z
G
|n−1
n−1
X
i=0
fT i −f| dm
≤
|| < f >n −f||1 →n→∞0.
Thus we have proved the following lemma:
Lemma 6.2.1 If a dynamical system with measure m has the ergodic property with respect to a
bounded measurement f, then
lim
n→∞n−1
n−1
X
i=0
Z
G
fT i dm =
Z
G
f dm, all G ∈B.
(6.1)
Thus, for example, if we take G as the whole space
lim
n→∞n−1
n−1
X
i=0
Em(fT i) = Emf.
(6.2)
Suppose that a random process has ergodic properties with respect to the class of all bounded
measurements and let f be an indicator function of an event F. Application of (6.1) yields
lim
n→∞n−1
n−1
X
i=0
m(T −iF ∩G) = lim
n→∞n−1
n−1
X
i=0
Z
G
1F T idm
= Em(1F 1G), all F, G ∈B.
(6.3)
For example, if G is the entire space than (6.3) becomes
lim
n→∞n−1
n−1
X
i=0
m(T −iF) = Em(1F ), all events F.
(6.4)
The limit properties of the preceding lemma are trivial implications of the deﬁnition of ergodic
properties, yet they will be seen to have far-reaching consequences. We note the following general-
ization for later use.
Corollary 6.2.1 If a dynamical system with measure m has the ergodic property with respect to a
measurement f (not necessarily bounded) and if the sequence < f >n is uniformly integrable, then
(6.1) and hence also (6.2) hold.
Proof: The proof follows immediately from Lemma 4.6.6 since this ensures the needed L1 conver-
gence.
2
Recall from Corollary 4.4.6 that if the fT n are uniformly integrable, then so are the < f >n.
Thus, for example, if m is stationary and f is m-integrable, then Lemma 4.7.2 and the preceding
lemma imply that (6.1) and (6.2) hold, even if f is not bounded.

128
CHAPTER 6. ERGODIC PROPERTIES
In general, the preceding lemmas state that if the arithmetic means of a sequence of ceeasure-
ments converges in any sense, then the corresponding arithmetic means of the expected values of
the measurements must also converge. In particular, they imply that the arithmetic mean of the
measures mT −i must converge, a fact that we emphasize as a corollary:
Corollary 6.2.2 If a dynamical system with measure m has ergodic properties with respect to the
class of all indicator functions of events, then
lim
n→∞
1
n
n−1
X
i=0
m(T −iF) exists,all events F.
(6.5)
Recall that a measure m is stationary or invariant with respect to T if all of these measures are
identical. Clearly (6.5) holds for stationary measures. More generally, say that (6.5) holds for a
measure m and transformation T. Deﬁne the set functions mn, n = 1, 2, . . . by
mn(F) = n−1
n−1
X
i=0
m(T −iF).
The mn are easily seen to be probability measures, and hence (6.5) implies that there is a sequence
of measures mn such that for every event F the sequence mn(F) has a limit. If the limit of the
arithmetic mean of the transformed measures mT −i exists, we shall denote it by m. The following
lemma shows that if a sequence of measures mn is such that mn(F) converges for every event F,
then the limiting set function is itself a measure. Thus if (6.5) holds and we deﬁne the limit as m,
then m is itself a probability measure.
Lemma 6.2.2 (The Vitali-Hahn-Saks Theorem)
Given a sequence of probability measures mn and a set function m such that
lim
n→∞mn(F) = m(F), all events F,
then m is also a probability measure.
Proof: It is easy to see that m is nonnegative, normalized, and ﬁnitely additive. Hence we need only
show that it is countably additive. Observe that it would not help here for the space to be standard
as that would only guarantee that m deﬁned on a ﬁeld would have a countably additive extension,
not that the given m that is deﬁned on all events would itself be countably additive. To prove that
m is countably additive we assume the contrary and show that this leads to a contradiction.
Assume that m is not countably additive and hence from (1.11) there must exist a sequence of
disjoint sets Fj with union F such that
∞
X
j=1
m(Fj) = b < a = m(F).
(6.6)
We shall construct a sequence of integers M(k), k = 1, 2, . . ., a corresponding grouping of the Fj
into sets Bk as
Bk =
M(k)
[
j=M(k−1)+1
Fj, k = 1, 2, . . . ,
(6.7)

6.2. SOME IMPLICATIONS OF ERGODIC PROPERTIES
129
a set B deﬁned as the union of all of the Bk for odd k:
B = B1 ∪B3 ∪B5 ∪. . . ,
(6.8)
and a sequence of integers N(k), k = 1, 2, 3, . . ., such that mN(k)(B) oscillates between values of
approximately b when k is odd and approximately a when k is even. This will imply that mn(B)
cannot converge and hence will yield the desired contradiction.
Fix ϵ > 0 much smaller than a −b. Fix M(0) = 0.
Step 1: Choose M(1) so that
b −
M
X
j=1
m(Fj) < ϵ
8ifM ≥M(1).
(6.9)
and then choose N(1) so that
|
M(1)
X
j=1
mn(Fj) −b| = |mn(B1) −b| < ϵ
4, n ≥N(1),
(6.10)
and
|mn(F) −a| ≤ϵ
4, n ≥N(1).
(6.11)
Thus for large n, mn(B1) is approximately b and mn(F) is approximately a.
Step 2: We know from (6.10)-(6.11) that for n ≥N(1)
|mn(F −B1) −(a −b)| ≤|mn(F) −a| + |mn(B1) −b| ≤ϵ
2,
and hence we can choose M(2) suﬃciently large to ensure that
|mN(1)(
M(2)
[
j=M(1)+1
Fj) −(a −b)| = |mN(1)(B2) −(a −b)| ≤3ϵ
4 .
(6.12)
From (6.9)
b −
M(2)
X
j=1
m(Fj) = b −m(B1 ∪B2) < ϵ
8,
and hence analogous to (6.10) we can choose N(2) large enough to ensure that
|b −mn(B1 ∪B2)| < ϵ
4, n ≥N(2).
(6.13)
Observe that mN(1) assigns probability roughly b to B1 and roughly (a −b) to B2, leaving only a
tiny amount for the remainder of F. mN(2), however, assigns roughly b to B1 and to the union
B1 ∪B2 and hence assigns almost nothing to B2. Thus the compensation term a −b must be made
up at higher indexed Bi’s.
Step 3: From (6.13) of Step 2 and (6.11)
|mn(F −
2[
i=1
Bi) −(a −b)| ≤|mn(F) −a| + |mn(
2[
i=1
Bi) −b|

130
CHAPTER 6. ERGODIC PROPERTIES
≤ϵ
4 + ϵ
4 = ϵ
2, n ≥N(2).
Hence as in (6.12) we can choose M(3) suﬃciently large to ensure that
|mN(2)(
M(3)
[
j=M(2)+1
Fj) −(a −b)| = |mN(2)(B3) −(a −b)| ≤3ϵ
4 .
(6.14)
Analogous to (6.13) using (6.9) we can choose N(3) large enough to ensure that
|b −mn(
3[
i=1
Bi)| ≤ϵ
4, n ≥N(3).
(6.15)
...
Step k: From step k −1 for suﬃciently large M(k)
|mN(k−1)(
M(k)
[
j=M(k−1)+1
Fj) −(a −b)| = |mN(k−1)(Bk) −(a −b)| ≤3ϵ
4
(6.16)
and for suﬃciently large N(k)
|b −mn(
k[
i=1
Bi)| ≤ϵ
4, n ≥N(k).
(6.17)
Intuitively, we have constructed a sequence mN(k) of probability measures such that mN(k) always
puts a probability of approximately b on the union of the ﬁrst k Bi and a probability of about a −b
on Bk+1. In fact, most of the probability weight of the union of the ﬁrst k Bi lies in B1 from (6.9).
Together this gives a probability of about a and hence yields the probability of the entire set F,
thus mN(k) puts nearly the total probability of F on the ﬁrst B1 and on Bk+1. Observe then that
mN(k) keeps pushing the required diﬀerence a−b to higher indexed sets Bk+1 as k grows. If we now
deﬁne the set B of (6.8) as the union of all of the odd Bj, then the probability of B will include
b from B1 and it will include the a −b from Bk+1 if and only if k + 1 is odd, that is, if k is even.
The remaining sets will contribute a negligible amount. Thus we will have that mN(k)(B) is about
b + (a −b) = a for k even and b for k odd. We complete the proof by making this idea precise. If k
is even, then from (6.10) and (6.16)
mN(k)(B)
=
X
odd j
mN(k)(Bj)
≥
mN(k)(B1) + mN(k)(Bk+1)
≥
b −ϵ
4 + (a −b) −3ϵ
4
=
a −ϵ, k even .
(6.18)
For odd k the compensation term (a −b) is not present and we have from (6.11) and (6.16)
mN(k)(B)
=
X
odd j
mN(k)(Bj)

6.3. ASYMPTOTICALLY MEAN STATIONARY PROCESSES
131
≤
mN(k)(F) −mN(k)(Bk+1)
≤
a + ϵ
4 −((a −b) −3ϵ
4 )
=
b + ϵ.
(6.19)
Eqs. (6.18)-(6.19) demonstrate that mN(k)(B) indeed oscillates as claimed and hence cannot con-
verge, yielding the desired contradiction and completing the proof of the lemma.
2
Say that a probability measure m and transformation T are such that the arithmetic means of
the iterates of m with respect to T converge to a set function m; that is,
lim
n→∞n−1
n−1
X
i=0
m(T −iF) = m(F), all events F.
(6.20)
From the previous lemma, m is a probability measure. Furthermore, it follows immediately from
(6.20) that m is stationary with respect to T, that is,
m(T −1F) = m(F), all events F.
Since m is the limiting arithmetic mean of the iterates of a measure m and since it is stationary,
a probability measure m for which (6.5) holds, that is, for which all of the limits of (6.20) exist, is
said to be asymptotically mean stationary or AMS. The limiting probability measure m is called the
stationary mean of m..
Since we have shown that a random process possessing ergodic properties with respect to the
class of all indicator functions of events is AMS we have proved the following result.
Theorem 6.2.1 A necessary condition for a random process to possess ergodic properties with re-
spect to the class of all indicator functions of events (and hence with respect to any larger class such
as the class of all bounded measurements) is that it be asymptotically mean stationary.
We shall see in the next chapter that the preceding condition is also suﬃcient.
Exercises
1. A dynamical system (Ω, B, m, T) is N-stationary for a positive integer N if it is stationary
with respect to T N, that is, if for all F m(T −iF) is periodic with period N. A system is said
to be block stationary if it is N-stationary for some N. Show that an N-stationary process is
AMS with stationary mean
m(F) = 1
N
N−1
X
i=0
m(T −iF).
Show in this case that m >> m.
6.3
Asymptotically Mean Stationary Processes
In this section we continue to develop the implications of ergodic properties. The emphasis here is
on a class of measurements and events that are intimately connected to ergodic properties and will
play a basic role in further necessary conditions for such properties as well as in the demonstration
of suﬃcient conditions. These measurements help develop the relation between an AMS measure

132
CHAPTER 6. ERGODIC PROPERTIES
m and its stationary mean m. The theory of AMS processes follows the work of Dowker [17] [18],
Rechard [59], and Gray and Kieﬀer [26].
As before, we have a dynamical system (Ω, B, m, T) with ergodic properties with respect to a
measurement, f and hence with probability one the sample average
lim
n→∞< f >n= lim
n→∞
1
n
n−1
X
i=0
fT i
(6.21)
exists. From Corollary 6.1.1, the limit, say < f >, must be an invariant function. Deﬁne the event
F = {x : lim
n→∞n−1
n−1
X
i=0
f(T ix) exists },
(6.22)
and observe that by assumption m(F) = 1. Consider next the set T −1F. Then x ∈T −1F or Tx ∈F
if and only if the limit
lim
n→∞n−1
n−1
X
i=0
f(T i(Tx)) = f(Tx)
exists. Provided that f is a real-valued measurement and hence cannot take on inﬁnity as a value
(that is, it is not an extended real-valued measurement), then as in Lemma 6.1.1 the left-hand limit
is exactly the same as the limit
lim
n→∞n−1
n−1
X
i=0
f(T ix).
In particular, either limit exists if and only if the other one does and hence x ∈T −1F if and only if
x ∈F. We formalize this property in a deﬁnition: If an event F is such that T −1F = F, then the
event is said to be invariant or T-invariant or invariant with respect to T.
Thus we have seen that the event that limiting sample averages converge is an invariant event
and that the limiting sample averages themselves are invariant functions. Observe that we can write
for all x that
f(x) = 1F (x) lim
n→∞
1
n
n−1
X
n=0
f(T ix).
We shall see that invariant events and invariant measurements are intimately connected. When T
is the shift, invariant measurements can be interpreted as measurements whose output does not
depend on when the measurement is taken and invariant events are events that are unaﬀected by
shifting.
Limiting sample averages are not the only invariant measurements; other examples are
lim sup
n→∞< f >n (x),
and
lim inf
n→∞< f >n (x).
Observe that an event is invariant if and only if its indicator function is an invariant measurement.
Observe also that for any event F, the event
Fi.o. = lim sup
n→∞T −nF =
∞
\
n=0
∞
[
k=n
T −kF = {x : x ∈T −nF i.o.} = {x : T nx ∈F i.o.}
(6.23)

6.3. ASYMPTOTICALLY MEAN STATIONARY PROCESSES
133
is invariant, where i.o. means ﬁnitely often. To see this, observe that
T −1Fi.o. =
∞
\
n=1
∞
[
k=n
T −kF
and
∞
[
k=n
T −kF ↓Fi.o..
Invariant events are “closed” to transforming, e.g., shifting: If a point begins inside an invariant
event, it must remain inside that event for all future transformations or shifts. Invariant measure-
ments are measurements that yield the same value on T nx for any n. Invariant measurements and
events yield special relationships between an AMS random process and its asymptotic mean, as
described in the following lemma.
Lemma 6.3.1 Given an AMS system with measure m with stationary mean m, then
m(F) = m(F), all invariant F,
and
Emf = Emf, all invariant f,
where the preceding equation means that if either integral exists, then so does the other and they are
equal.
Proof: The ﬁrst equality follows by direct substitution into the formula deﬁning an AMS measure.
The second formula then follows from standard approximation arguments, i.e., since it holds for
indicator functions, it holds for simple functions by linearity. For positive functions take an increasing
quantized sequence and apply the simple function result. The two limits must converge or diverge
together. For general f, decompose it into positive and negative parts and apply the positive function
result.
2
In some applications we deal with measurements or events that are “almost invariant” in the
sense that they equal their shift with probability one instead of being invariant in the strict sense.
As there are two measures to consider here, an AMS measure and its stationary mean, a little more
care is required to conclude results like Lemma 6.3.1. For example, suppose that an event F is
invariant m-a.e. in the sense that m(F∆T −1F) = 0. It does not then follow that F is also invariant
m-a.e. or that m(F) = m(F). The following lemma shows that if one strengthens the deﬁnitions of
almost invariant to include all shifts, then m and m will behave similarly for invariant measurements
and events.
Lemma 6.3.2 An event F is said to be totally invariant m-a.e. if
m(F∆T −kF) = 0, k = 1, 2, . . . .
Similarly, a measurement f is said to be totally invariant m-a.e. if
m(f = fT k; k = 1, 2, . . .) = 1.
Suppose that m is AMS with stationary mean m. If F is totally invariant m-a.e. then
m(F) = m(F),

134
CHAPTER 6. ERGODIC PROPERTIES
and if f is totally invariant m-a.e.
Emf = Emf.
Furthermore, if f is totally invariant m-a.e., then the event f−1(G) = {ω : f(ω) ∈G} is also totally
invariant and hence
m(f −1(G)) = m(f−1(G)).
Proof: It follows from the elementary inequality
|m(F) −m(G)| ≤m(F∆G)
that if F is totally invariant m-a.e., then m(F) = m(T −kF) for all positive integers k. This and the
deﬁnition of m imply the ﬁrst relation. The second relation then follows from the ﬁrst as in Lemma
6.3.1. If m puts probability 1 on the set D of ω for which the f(T kω) are all equal, then for any k
m(f −1(G)∆T −kf −1(G))
=
m({ω : f(ω) ∈G}∆{ω : f(T kω) ∈G)})
=
m({ω : ω ∈D} ∩({ω : f(ω) ∈G}∆{ω : f(T kω) ∈G}))
=
0
since if ω is in D, f(ω) = f(T kω) and the given diﬀerence set is empty.
2
Lemma 6.3.1 has the following important implication:
Corollary 6.3.1 Given an AMS system (Ω, B, m, T) with stationary mean m, then the system has
ergodic properties with respect to a measurement f if and only if the stationary system (Ω, B, m, T)
has ergodic properties with respect to f. Furthermore, the limiting sample average can be chosen to
be the same for both systems.
Proof: The set F = {x : limn→∞< f >n exists} is invariant and hence m(F) = m(F) from the
lemma. Thus either both measures assign probability one to this set and hence both systems have
ergodic properties with respect to f or neither does. Choosing < f > (x) as the given limit for
x ∈F and 0 otherwise completes the proof.
2
Another implication of the lemma is the following relation between an AMS measure and its
stationary mean. We ﬁrst require the deﬁnition of an asymptotic form of domination or absolute
continuity. Recall that m << η means that η(F) = 0 implies m(F) = 0. We say that a measure η
asymptotically dominates m (with respect to T) if η(F) = 0 implies that
lim
n→∞m(T −nF) = 0.
Roughly speaking, if η asymptotically dominates m, then zero probability events under η will have
vanishing probability under m if they are shifted far into the future. This can be considered as a
form of steady state behavior in that such events may have nonzero probability as transients, but
eventually their probability must tend to 0 as things settle down.
Corollary 6.3.2 If m is asymptotically mean stationary with stationary mean m, then m asymptot-
ically dominates m. If T is invertible (e.g., the two-sided shift), then asymptotic domination implies
ordinary domination and hence m << m.
Proof: From the lemma m and m place the same probability on invariant sets. Suppose that F is
an event such that m(F) = 0. Then the set G = lim supn→∞T −nF of (6.23) has m measure 0 since

6.3. ASYMPTOTICALLY MEAN STATIONARY PROCESSES
135
from the continuity of probability and the union bound
m(G)
=
lim
n→∞m(
∞
[
k=n
T −nF)
≤
m(
∞
[
k=1
T −nF)
≤
∞
X
j=1
m(T −nF) = 0.
In addition G is invariant, and hence from the lemma m(G) = 0. Applying Fatou’s lemma (Lemma
4.4.5(c)) to indicator functions yields
lim sup
n→∞m(T −nF) ≤m(lim sup
n→∞T −nF) = 0,
proving the ﬁrst statement. Suppose that η asymptotically dominates m and that T is invertible.
Say η(F) = 0. Since T is invertible the set Sinf
n=−inf T nF is measurable and as in the preceding
argument has m measure 0. Since the set is invariant it also has m measure 0. Since the set contains
the set F, we must have m(F) = 0, completing the proof.
2
The next lemma pins down the connection between invariant events and invariant measurements.
Let I denote the class of all invariant events and, as in the previous chapter, let M(I) denote the
class of all measurements (B-measurable functions) that are measurable with respect to I. The
following lemma shows that M(I) is exactly the class of all invariant measurements.
Lemma 6.3.3 Let I denote the class of all invariant events. Then I is a σ-ﬁeld and M(I) is the
class of all invariant measurements. In addition, I = σ(M(I)) and hence I is the σ-ﬁeld generated
by the class of all invariant measurements.
Proof: That I is a σ-ﬁeld follows immediately since inverse images (under T) preserve set theoretic
operations. For example, if F and G are invariant, then T −1(F ∪G) = T −1F ∪T −1G = F ∪G and
hence the union is also invariant. The class M(I) contains all indicator functions of invariant events,
hence all simple invariant functions. For example, if f(ω) = P bi1Fi(ω) is a simple function in this
class, then its inverse images must be invariant, and hence f−1(bi) = Fi ∈I is an invariant event.
Thus f must be an invariant simple function. Taking limits in the usual way then implies that every
function in M(I) is invariant. Conversely, suppose that f is a measurable invariant function. Then
for any Borel set G, T −1f −1(G) = (fT)−1(G) = f −1(G), and hence the inverse images of all Borel
sets under f are invariant and hence in I. Thus measurable invariant function must be in M(I).
The remainder of the lemma follows from Lemma 5.2.2.
2
The sub-σ-ﬁeld of invariant events plays an important role when studying limiting averages.
There is another sub-σ-ﬁeld that plays an important role in studying asymptotic behavior, especially
when dealing with one-sided processes or noninvertible transformations. Given a dynamical system
(Ω, B, m, T) and a random process {Xn} with Xn(ω) = X0(T nω), deﬁne the sub-σ-ﬁelds
Fn = σ(Xn, Xn+1, Xn+2, . . .)
(6.24)
and
F∞=
∞
\
n=1
Fn.
(6.25)

136
CHAPTER 6. ERGODIC PROPERTIES
F∞is called the tail σ-ﬁeld or the remote σ-ﬁeld; intuitively, it contains all of those events that can
be determined by looking at future outcomes arbitrarily far into the future; that is, membership in a
tail event cannot be determined by any of the individual Xn, only by their aggregate behavior in the
distant future. If the system is a directly given one-sided random process and T is the (noninvertible)
shift, then we can write
Fn = T −nB
F∞=
∞
\
n=0
T −nB.
In the two-sided process case this is not true since if T is invertible T −nB = B. In the one-sided
case invariant events are clearly tail events since they are in all of the T −nB. Not all tail events
are invariant, however. A measurement f is called a tail function if it is measurable with respect to
the tail σ-ﬁeld. Intuitively, a measurement f is a tail function only if we can determine its value by
looking at the outputs Xn, Xn+1, . . . for arbitrarily large values of n.
We have seen that if T is invertible as in a two-sided directly given random process and if m
is AMS with stationary mean m, then m << m, and hence any event having 0 probability under
m also has 0 probability under m. In the one-sided case this is not in general true, although we
have seen that it is true for invariant events. We shall shortly see that it is also true for tail events.
First, however, we need a basic property of asymptotic dominance. If m << m, the Radon-Nikodym
theorem implies that m(F) =
R
F (dm/dm)dm. The following lemma provides an asymptotic version
of this relation for asymptotically dominated measures.
Lemma 6.3.4 Given measures m and η, let (mT −n)a denote the absolutely continuous part of
mT −n with respect to η as in the Lebesgue decomposition theorem (Theorem 5.6.2) and deﬁne the
Radon-Nikodym derivative
fn = d(mT −n)a
dη
; n = 1, 2, . . . .
If η asymptotically dominates m then
lim
n→∞[sup
F ∈B
|m(T −nF) −
Z
F
fndη|] = 0.
Proof: From the Lebesgue decomposition theorem and the Radon-Nikodym theorem (Theorems
5.6.2 and 5.6.1) for each n = 1, 2, . . . there exists a Bn ∈B such that
mT −n(F) = mT −n(F ∩Bn) +
Z
F
fndη; F ∈B,
(6.26)
with
η(Bn) = 0.
Deﬁne
B =
∞
[
n=0
Bn.
Then η(B) = 0, and hence by assumption
0 ≤mT −n(F) −
Z
F
fndη = mT −n(F ∩Bn)
≤mT −n(F ∩B) ≤mT −n(B) →n→∞0.
Since the bound is uniform over F, the lemma is proved.
2

6.3. ASYMPTOTICALLY MEAN STATIONARY PROCESSES
137
Corollary 6.3.3 Suppose that η is stationary and asymptotically dominates m (e.g., m is AMS and
η is the stationary mean), then if F is a tail event (F ∈F∞) and η(F) = 0, then also m(F) = 0.
Proof: First observe that if T is invertible, the result follows from the fact that asymptotic dominance
implies dominance. Hence assume that T is not invertible and F∞= ∩T −nB. Let F ∈F∞have η
probability 0. Find {Fn} so that T −nFn = F, n = 1, 2, . . .. (This is possible since F ∈F∞means
that also F ∈T −nB for all n and hence there must be an Fn ∈B such that F = T −nFn.) From the
lemma, m(F) −
R
Fn fndη →0. Since η is stationary,
R
Fn fndη =
R
F fnT ndη = 0, hence m(F) = 0.
2
Corollary 6.3.4 Let µ and η be probability measures on (Ω, B) with η stationary. Then the following
are equivalent:
(a) η asymptotically dominates µ.
(b) If F ∈B is invariant and η(F) = 0, then also µ(F) = 0.
(c) If F ∈F∞and η(F) = 0, then also µ(F) = 0.
Proof: The previous corollary shows that (a) => (c). (c) => (b) immediately from the fact that
invariant events are also tail events. To prove that (b) => (a), assume that η(F) = 0. The set
G = lim supn→∞T −nF of (6.23) is invariant and has η measure 0. Thus as in the proof of Corollary
6.3.2
lim sup
n→∞m(T −nF) ≤m(lim sup
n→∞T −nF) = 0.
2
Corollary 6.3.5 Let µ and η be probability measures on (Ω, B). If η is stationary and has ergodic
properties with respect to a measurement f and if η asymptotically dominates µ, then also µ has
ergodic properties with respect to f.
Proof: The set on which {x : limn→∞< f >n
exists} is invariant and hence if its complement
(which is also invariant) has measure 0 under η, it also has measure zero under m from the previous
corollary.
2
Exercises
1. Is the event
lim inf
n→∞T −nF =
∞
[
n=0
∞
\
k=n
T −k = {x : x ∈T −nF for all but a ﬁnite number of n}
invariant? Is it a tail event?
2. Show that given an event F and ϵ > 0, the event
{x : | 1
n
n−1
X
i=0
f(T ix)| ≤ϵ i.o. }
is invariant.

138
CHAPTER 6. ERGODIC PROPERTIES
3. Suppose that m is stationary and let G be an event for which T −1G ⊂G. Show that m(G −
T −1G) = 0 and that there is an invariant event Gunder with m(G∆G) = 0.
4. Is an invariant event a tail event? Is a tail event an invariant event? Is Fi.o. of (6.23) a tail
event?
5. Suppose that m is the distribution of a stationary process and that G is an event with nonzero
probability.
Show that if G is invariant, then the conditional distribution mG deﬁned by
mG(F) = m(F|G) is also stationary. Show that even if G is not invariant, mG is AMS.
6. Suppose that T is an invertible transformation (e.g., the shift in a two-sided process) and that
m is AMS with respect to T. Show that m(F) > 0 implies that m(T nF) > 0 i.o. (inﬁnitely
often).
6.4
Recurrence
We have seen that AMS measures are closely related to stationary measures. In particular, an AMS
measure is either dominated by a stationary measure or asymptotically dominated by a stationary
measure. Since the simpler and stronger nonasymptotic form of domination is much easier to work
with, the question arises as to conditions under which an AMS system will be dominated by a
stationary system. We have already seen that this is the case if the transformation T is invertible,
e.g., the system is a two-sided random process with shift T. In this section we provide a more
general set of conditions that relates to what is sometimes called the most basic ergodic property of
a dynamical system, that of recurrence.
The original recurrence theorem goes back to Poincare [58]. We brieﬂy describe it as it motivates
some related deﬁnitions and results.
Given a dynamical system (Ω, B, m, T), let G ∈B. A point x ∈G is said to be recurrent with
respect to G if there is a ﬁnite n = nG(x) such that
T nG(x) ∈G,
that is, if the point in G returns to G after some ﬁnite number of shifts. An event G having positive
probability is said to be recurrent if almost every point in G is recurrent with respect to G. In
order to have a useful deﬁnition for all events (with or without positive probability), we state it in
a negative fashion: Deﬁne
G∗=
∞
[
n=1
T −nG,
(6.27)
the set of all sequences that land in G after one or more shifts. Then
B = G −G∗
(6.28)
is the collection of all points in G that never return to G, the bad or nonrecurrent points. We now
deﬁne an event G to be recurrent if m(G −G∗) = 0. The dynamical system will be called recurrent
if every event is a recurrent event. Observe that if the system is recurrent, then for all events G
m(G ∩G∗) = m(G) −m(G ∩G∗c) = m(G) −m(G −G∗) = m(G).
(6.29)
Thus if m(G) > 0, then the conditional probability that the point will eventually return to G is
m(G∗|G) = m(G ∩G∗)/m(G) = 1.

6.4. RECURRENCE
139
Before stating and proving Poincar´e recurrence theorem, we make two interesting observations
about the preceding sets. First observe that if x ∈B, then we cannot have T kx ∈G for any k ≥1.
Since B is a subset of G, this means that we cannot have T kx ∈B for any k ≥1, and hence the
events B and T −kB must be pairwise disjoint for k = 1, 2, . . .; that is,
B ∩T −kB = ∅; k = 1, 2, . . .
If these sets are empty, then so must be their inverse images with respect to T n for any positive
integer n, that is,
∅= T −n(B ∩T −kB) = (T −nB) ∩(T −(n+k)B)
for all integers n and k. This in turn implies that
T −kB ∩T −jB = ∅; k, j = 1, 2, . . . , k ̸= j;
(6.30)
that is, the sets T −nB are pairwise disjoint. We shall call any set W with the property that T −nW
are pairwise disjoint a wandering set. Thus (6.30) states that the collection G −G∗of nonrecurrent
sequences is a wandering set.
As a ﬁnal property, observe that G∗as deﬁned in (6.27) has a simple property when it is shifted:
T −1G∗=
∞
[
n=1
T −n−1G =
∞
[
n=2
T −nG ⊂G∗,
and hence
T −1G∗⊂G∗.
(6.31)
Thus G∗has the property that it is shrunk or compressed by the inverse transformation. Repeating
this shows that
T −kG∗=
∞
[
n=k+1
T −nG ⊂T −(k+1)G∗.
(6.32)
We are now prepared to state and prove what is often considered the ﬁrst ergodic theorem.
Theorem 6.4.1 (The Poincar´e recurrence Theorem) If the dynamical system is stationary, then it
is recurrent.
Proof: We need to show that for any event G, the event B = G −G∗has 0 measure. We have seen,
however, that B is a wandering set and hence its iterates are disjoint. This means from countable
additivity of probability that
m(
∞
[
n=0
T −nB) =
∞
X
n=0
m(T −nB).
Since the measure is stationary, all of the terms of the sum are the same. They then must all be 0
or the sum would blow up, violating the fact that the left-hand side is less than 1.
2
Although such a recurrence theorem can be viewed as a primitive ergodic property, unlike the
usual ergodic properties, there is no indication of the relative frequency with which the point will
return to the target set. We shall shortly see that recurrence at least guarantees inﬁnite recurrence
in the sense that the point will return inﬁnitely often, but again there is no notion of the fraction of
time spent in the target set.
We wish next to explore the recurrence property for AMS systems. We shall make use of several
of the ideas developed above.
We begin with several deﬁnitions regarding the given dynamical

140
CHAPTER 6. ERGODIC PROPERTIES
system. Recall for comparison that the system is stationary if m(T −1G) = m(G) for all events G.
The system is said to be incompressible if every event G for which T −1G ⊂G has the property
that m(G −T −1G) = 0. Note the resemblance to stationarity: For all such G m(G) = m(T −1G).
The system is said to be conservative if all wandering sets have 0 measure. As a ﬁnal deﬁnition,
we modify the deﬁnition of recurrent. Instead of the requirement that a recurrent system have the
property that if an event has nonzero probability, then points in that event must return to that
event with probability 1, we can strengthen the requirement to require that the points return to the
event inﬁnitely often with probability 1. Deﬁne as in (6.23) the set
Gi.o. =
∞
\
n=0
∞
[
k=n
T −kG
of all points that are in G inﬁnitely often.
The dynamical system will be said to be inﬁnitely
recurrent if for all events G
m(G −Gi.o.) = 0.
Note from (6.32) that
∞
[
k=n
T −kG = T −(n+1)G∗,
and hence since the T −nG∗are decreasing
Gi.o. =
∞
\
n=0
T −(n+1)G∗=
∞
\
n=0
T −nG∗.
(6.33)
The following theorem is due to Halmos [28], and the proof largely follows Wright [73], Petersen
[57], or Krengel [41]. We do not, however, assume as they do that the system is nonsingular in the
sense that mT −1 << m. When this assumption is made, T is said to be null preserving because
shifts of the measure inherit the zero probability sets of the original measure.
Theorem 6.4.2 Suppose that (Ω, B, m, T) is a dynamical system. Then the following statements
are equivalent:
(a) T is inﬁnitely recurrent.
(b) T is recurrent.
(c) T is conservative.
(d) T is incompressible.
Proof:
(a)⇒(b): In words: if we must return an inﬁnite number of times, then we must obviously return
at least once. Alternatively, application of (6.33) implies that
G −Gi.o. = G −
∞
\
n=0
T −nG∗
(6.34)
so that G −G∗⊂G −Gi.o. and hence the system is recurrent if it is inﬁnitely recurrent.
(b)⇒(c): Suppose that W is a wandering set. From the recurrence property W −W ∗must have
0 measure. Since W is wandering, W ∩W ∗= ∅and hence m(W) = m(W −W ∗) = 0 and the

6.4. RECURRENCE
141
system is conservative. (c)⇒(d): Consider an event G with T −1G ⊂G. Then G∗= T −1G since
all the remaining terms in the union deﬁning G∗are further subsets. Thus G −T −1G = G −G∗.
We have seen, however, that G −G∗is a wandering set and hence has 0 measure since the system
is conservative.
(b) and (d)⇒a: From (6.34) we need to prove that
m(G −Gi.o.) = m(G −
∞
\
n=0
T −nG∗) = 0.
Toward this end we use the fact that the T −nG∗are decreasing to decompose G∗as
G∗= (
∞
\
n=1
T −nG∗)
∞
[
n=0
(T −nG∗−T −(n+1)G∗).
This breakup can be thought of as a “core” T∞
n=1 T −nG∗containing the sequences that shift into G∗
for all shifts combined with a union of an inﬁnity of “doughnuts,” where the nth doughnut contains
those sequences that when shifted n times land in G∗, but never arrive in G∗if shifted more than
n times. The core and all of the doughnuts are disjoint and together yield all the sequences in G∗.
Because the core and doughnuts are disjoint,
∞
\
n=1
T −nG∗= G∗−
∞
[
n=0
(T −nG∗−T −(n+1)G∗)
so that
G −
∞
\
n=1
T −nG∗= (G −G∗) ∪(G ∩(
∞
[
n=0
(T −nG∗−T −(n+1)G∗)).
(6.35)
Thus since the system is recurrent we know by (6.35) and the union bound that
m(G −
∞
\
n=1
T −nG∗)
≤
m(G −G∗) + m(G ∩(
∞
[
n=0
(T −nG∗−T −(n+1)G∗))
≤
m(G −G∗) + m(
∞
[
n=0
(T −nG∗−T −(n+1)G∗))
=
m(G −G∗) +
∞
X
n=0
m(T −nG∗−T −(n+1)G∗)),
using the disjointness of the doughnuts. (Actually, the union bound and an inequality would suﬃce
here.) Since T −1G∗⊂G∗, it is also true that T −1T −nG∗= T −nT −1G∗⊂T −nG∗and hence since
the system is incompressible, all the terms in the previous sum are zero.
(d) ⇒(b) Given an event G, let F = G∪G∗. Then T −1F = G∗⊂F and hence m(F −T −1F) = 0
by incompressibility. But F −T −1F = (G ∪G∗) −G∗= G −G∗and hence m(G −G∗) = 0 implying
recurrence.
We have now shown that (a)⇒(b)⇒(c)⇒(d), that (b) and (d) together imply (a), and that
(d) ⇒(d) and hence (d)⇒(a), which completes the proof that any one of the conditions implies
the other three.
2
Thus, for example, Theorem 2.4.1 implies that a stationary system has all the preceding proper-
ties. Our goal, however, is to characterize the properties for an AMS system. The following theorem
characterizes recurrent AMS processes: An AMS process is recurrent if and only if it is dominated
by its stationary mean.

142
CHAPTER 6. ERGODIC PROPERTIES
Theorem 6.4.3 Given an AMS dynamical system (Ω, B, m, T) with stationary mean m, then m <<
m if and only the system is recurrent.
Proof: If m << m, then the system is obviously recurrent since m is from the previous two theorems
and m inherits all of the zero probability sets from m. To prove the converse implication, suppose
the contrary: Say that there is a set G for which m(G) > 0 and m(G) = 0 and that the system is
recurrent. From Theorem 6.4.2 this implies that the system is also inﬁnitely recurrent, and hence
m(G −Gi.o.) = 0
and therefore
m(G) = m(Gi.o.) > 0.
We have seen, however, that Gi.o is an invariant set in Section 6.3 and hence from Lemma 6.3.1,
m(Gi.o.) > 0. Since m is stationary and gives probability 0 to G, m(T −kG) = 0 for all k, and hence
m(Gi.o.) = m(
∞
\
n=0
∞
[
k=n
T −nG) ≤m(
∞
[
k=0
T −nG) ≤
∞
X
k=0
m(G) = 0,
yielding a contradiction and proving the theorem.
2
If the transformation T is invertible; e.g., the two-sided shift, then we have seen that m << m and
hence the system is recurrent. When the shift is noninvertible, the theorem proves that domination
by a stationary measure is equivalent to recurrence. Thus, in particular, not all AMS processes need
be recurrent.
Exercises
1. Show that if an AMS dynamical system is nonsingular (mT −1 << m), then m and the station-
ary mean m will be equivalent (have the same probability 0 events) if and only if the system
is recurrent.
2. Show that if m is nonsingular (null preserving) with respect to T, then incompressibility of
T implies that it is conservative and hence that all four conditions of Theorem 6.4.2 are
equivalent.
6.5
Asymptotic Mean Expectations
We have seen that the existence of ergodic properties implies that limits of sample means of expec-
tations exist; e.g., if < f >n converges, then so does n−1 Pn−1
i=0 EmfT i. In this section we relate
these limits to expectations with respect to the stationary mean m.
Lemma 6.2.1 showed that the arithmetic mean of the expected values of transformed measure-
ments converged. The following lemma shows that the limit is given by the expected value of the
original measurement under the stationary mean.
Lemma 6.5.1 Let m be an AMS random process with stationary mean m. If a measurement f is
bounded, then
lim
n→∞Em(< f >n) = lim
n→∞n−1
n−1
X
i=0
Em(fT i) = lim
n→∞Emnf = Em(f).
(6.36)

6.5. ASYMPTOTIC MEAN EXPECTATIONS
143
Proof: The result is immediately true for indicator functions from the deﬁnition of AMS. It then
follows for simple functions from the linearity of limits.
In general let qk denote the quantizer
sequence of Lemma 4.3.1 and observe that for each k we have
|Em(< f >n) −Em(f)| ≤
|Em(< f >n) −Em(< qk(f) >n)| + |Em(< qk >n) −Em(qk(f))| + |Em(qk(f)) −Em(f)|.
Choose k larger that the maximum of the bounded function f. The middle term goes to zero from
the simple function result. The rightmost term is bound above by 2−k since we are integrating a
function that is everywhere less than 2−k. Similarly the ﬁrst term on the left is bound above by
n−1
n−1
X
i=0
Em|fT i −qk(fT i)| ≤2−k,
where we have used the fact that qm(f)(T ix) = qm(f(T ix)). Since k can be made arbitrarily large
and hence 2 × 2−k arbitrarily small, the lemma is proved.
2
Thus the stationary mean provides the limiting average expectation of the transformed measure-
ments. Combining the previous lemma with Lemma 6.2.1 immediately yields the following corollary.
Corollary 6.5.1 Let m be an AMS measure with stationary mean m. If m has ergodic properties
with respect to a bounded measurement f and if the limiting sample average is < f >, then
Em < f >= Emf.
(6.37)
Thus the expectation of the measurement under the stationary mean gives the expectation of
the limiting sample average under the original measure.
If we also have uniform integrability, we can similarly extend Corollary 6.2.1.
Lemma 6.5.2 Let m be an AMS random process with stationary mean m that has the ergodic
property with respect to a measurement f. If the < f >n are uniformly m-integrable, then f is also
m integrable and (6.36) and (6.37) hold.
Proof: From Corollary 6.2.1
lim
n→∞Em(< f >n) = Emf.
In particular, the right-hand integral exists and is ﬁnite. From Corollary 6.1.1 the limit < f > is
invariant. If the system is AMS, the invariance of < f > and Lemma 6.3.1 imply that
Emf = Emf.
Since m is stationary Lemma 4.7.2 implies that the sequence < f >n is uniformly integrable with
respect to m. Thus from Lemma 4.6.6 and the convergence of < f >n on a set of m probability one
(Corollary 6.3.1), the right-hand integral is equal to
lim
n→∞Em(n−1
n−1
X
i=0
fT i).
From the linearity of expectation, the stationarity of m, and Lemma 4.7.1, for each value of n the
preceding expectation is simply Emf.
2

144
CHAPTER 6. ERGODIC PROPERTIES
Exercises
1. Show that if m is AMS, then for any positive integer N and all i = 0, 1, . . . , N −1, then the
following limits exist:
lim
n→∞
1
n
n−1
X
j=0
m(T −i−jNF)
2. Deﬁne the preceding limit to be mi. Show that the mi are N-stationary and that if m is the
stationary mean of m, then
m(F) = 1
N
N−1
X
i=0
mi(F).
6.6
Limiting Sample Averages
In this section limiting sample averages are related to conditional expectations. The basis for this
development is the result of Section 6.1 that limiting sample averages are invariant functions and
hence are measurable with respect to I, the σ-ﬁeld of all invariant events.
We begin with limiting relative frequencies, that is, the limiting sample averages of indicator
functions. Say we have a dynamical system (Ω, B, m, T) that has ergodic properties with respect
to the class of all indicator functions and which is therefore AMS Fix an event G and let < 1G >
denote the corresponding limiting relative frequency, that is < 1G >n→< 1G > as n →∞a.e. This
limit is invariant and therefore in M(I) and is measurable with respect to I. Fix an event F ∈I
and observe that from Lemma 6.3.1
Z
F
1Gdm =
Z
F
1Gdm.
(6.38)
For invariant F, 1F is an invariant function and hence
< 1F f >n= 1
n
n−1
X
i=0
(1F f)T i = 1F
1
n
n−1
X
i=0
fT i = 1F < f >n
and hence if < f >n→< f > then
< 1F f >= 1F < f > .
(6.39)
Thus from Corollary 6.5.1 we have for invariant F that
Z
F
< 1G > dm = Em < 1F 1G >= Em1F 1G
=
Z
F
< 1G > dm = m(F ∩G),
which with (6.38) implies that
Z
F
< 1G > dm = m(F ∩G), all F ∈I.
(6.40)
Eq. (6.40) together with the I-measurability of the limiting sample average yield (6.40) and (6.41)
and thereby show that the limiting sample average is a version of the conditional probability m(G|I)
of G given the σ-ﬁeld I or, equivalently, given the class of all invariant measurements! We formalize
this in the following theorem.

6.6. LIMITING SAMPLE AVERAGES
145
Theorem 6.6.1 Given a dynamical system (Ω, B, m, T) let I denote the class of all T-invariant
events. If the system is AMS and if it has ergodic properties with respect to the indicator function
1G of an event G (e.g., if the system has ergodic properties with respect to all indicator functions of
events), then with probability one under m and m
< 1G >n= n−1
n−1
X
i=0
1GT i →n→∞m(G|I),
In other words,
< 1G >= m(G|I).
(6.41)
It is important to note that the conditional probability is deﬁned by using the stationary mean
measure m and not the original probability measure m. Intuitively, this is because it is the stationary
mean that describes asymptotic events.
Eq. (6.41) shows that the limiting sample average of the indicator function of an event, that
is, the limiting relative frequency of the event, satisﬁes the descriptive deﬁnition of the conditional
probability of the event given the σ-ﬁeld of invariant events. An interpretation of this result is
that if we wish to guess the probability of an event G based on the knowledge of the occurrence or
nonoccurrence of all invariant events or, equivalently, on the outcomes of all invariant measurements,
then our guess is given by one particular invariant measurement: the relative frequency of the event
in question.
A natural question is whether Theorem 6.6.1 holds for more general functions than indicator
functions; that is, if a general sample average < f >n converges, must the limit be the conditional
expectation? The following lemma shows that this is true in the case of bounded measurements.
Lemma 6.6.1 Given the assumptions of Theorem 6.6.1, if the system has ergodic properties with
respect to a bounded measurement f, then with probability one under m and m
< f >n= n−1
n−1
X
i=0
fT i →n→∞Em(f|I).
In other words,
f = Em(f|I).
Proof: Say that the limit of the left-hand side is an invariant function < f >. Apply Lemma 6.5.1
to the bounded measurement g = f1F with an invariant event F and
lim
n→∞n−1
n−1
X
i=0
Z
F
fT i dm =
Z
F
fdm.
From Lemma 6.2.1 the left-hand side is also given by
Z
F
f dm.
The equality of the two integrals for all invariant F and the invariance of the limiting sample average
imply from Lemma 6.7.1 that < f > is a version of the conditional expectation of f given I.
2
Replacing Lemma 6.5.1 by Lemma 6.5.2 in the preceding proof gives part of the following gen-
eralization.

146
CHAPTER 6. ERGODIC PROPERTIES
Corollary 6.6.1 Given a dynamical system (Ω, B, m, T) let I denote the class of all T-invariant
events. If the system is AMS with stationary mean m and if it has ergodic properties with respect to
a measurement f for which either (i) the < f >n are uniformly integrable with respect to m or (ii)
f is m-integrable, then with probability one under m and m
< f >n= n−1
n−1
X
i=0
fT i →n→∞Em(f|I).
In other words,
f = Em(f|I).
Proof: Part (i) follows from the comment before the corollary. From Corollary 7.3.1, since m has
ergodic properties with respect to f, then so does m and the limiting sample average < f > is
the same (up to an almost everywhere equivalence under both measures). Since f is by assumption
m-integrable and since m is stationary, then < f >n is uniformly m-integrable and hence application
of (i) to m then proves that the given equality holds m-a.e.. The set on which the the equality holds
is invariant, however, since both < f > and Em(f|I) are invariant. Hence the given equality also
holds m-a.e. from Lemma 6.3.1.
2
Exercises
1. Let m be AMS with stationary mean m. Let M denote the space of all square-integrable
invariant measurements. Show that M is a closed linear subspace of L2(m). If m has ergodic
properties with respect to a bounded measurement f, show that
f = PM(f);
that is, the limiting sample average is the projection of the original measurement onto the
subspace of invariant measurements. From the projection theorem, this means that < f > is
the minimum mean-squared estimate of f given the outcome of all invariant measurements.
2. Suppose that f ∈L2(m) is a measurement and fn ∈L2(m) is a sequence of measurements for
which ||fn −f|| →n→∞0. Suppose also that m is stationary. Prove that
1
n
n−1
X
i=0
fiT i →L2(m)< f >,
where < f > is the limiting sample average of f.
6.7
Ergodicity
A special case of ergodic properties of particular interest occurs when the limiting sample averages
are constants instead of random variables, that is, when a sample average converges with probability
one to a number known a priori rather than to a random variable. This will be the case, for example,
if a system is such that all invariant measurements equal constants with probability one. If this is
true for all measurements, then it is true for indicator functions of invariant events. Since such
functions can assume only values of 0 or 1, for such a system
m(F) = 0 or 1 for allF such that T −1F = F.
(6.42)

6.7. ERGODICITY
147
Conversely, if (6.42) holds every indicator function of an invariant event is either 1 or 0 with
probability 1, and hence every simple invariant function is equal to a constant with probability one.
Since every invariant measurement is the pointwise limit of invariant simple measurements (combine
Lemma 4.3.1 and Lemma 6.3.3), every invariant measurement is also a constant with probability one
(the ordinary limit of the constants equaling the invariant simple functions). Thus every invariant
measurement will be constant with probability one if and only if (6.42) holds.
Suppose next that a system possesses ergodic properties with respect to all indicator functions
or, equivalently, with respect to all bounded measurements.
Since the limiting sample averages
of all indicator functions of invariant events (in particular) are constants with probability one, the
indicator functions themselves are constant with probability one. Then, as previously, (6.42) follows.
Thus we have shown that if a dynamical system possesses ergodic properties with respect to a
class of functions containing the indicator functions, then the limiting sample averages are constant
a.e. if and only if (6.42) holds. This motivates the following deﬁnition: A dynamical system (or the
associated random process) is said to be ergodic with respect to a transformation T or T-ergodic or,
if T is understood, simply ergodic, if every invariant event has probability 1 or 0. Another name for
ergodic that is occasionally found in the mathematical literature is metrically transitive. .
Given the deﬁnition, we have proved the following result.
Lemma 6.7.1 A dynamical system is ergodic if and only if all invariant measurements are constants
with probability one. A necessary condition for a system to have ergodic properties with limiting
sample averages being a.e. constant is that the system be ergodic.
Lemma 6.7.2 If a dynamical system (Ω, B, m, T) is AMS with stationary mean m, then (Ω, B, m, T)
is ergodic if and only if (Ω, B, m, T) is.
Proof: The proof follows from the deﬁnition and Lemma 6.3.1.
2
Coupling the deﬁnition and properties of ergodic systems with the previously developed ergodic
properties of dynamical systems yields the following results.
Lemma 6.7.3 If a dynamical system has ergodic properties with respect to a bounded measurement
f and if the system is ergodic, then with probability one
< f >n→n→∞Emf = lim
n→∞n−1
n−1
X
i=0
EmfT i.
If more generally the < f >n are uniformly integrable but not necessarily bounded (and hence the
convergence is also in L1(m)), then the preceding still holds. If the system is AMS and either the
< f >n are uniformly integrable with respect to m or f is m-integrable, then also with probability
one
< f >n→n→∞Emf.
In particular, the limiting relative frequencies are given by
n−1
n−1
X
i=0
1F T i →n→∞m(F), all F ∈B.
In addition,
lim
n→∞n−1
n−1
X
i=0
Z
G
fT idm = (Emf)m(G), all G ∈B,

148
CHAPTER 6. ERGODIC PROPERTIES
where again the result holds for measurements for which the < f >n are uniformly integrable or f
is m integrable. Letting f be the indicator function of an event F, then
lim
n→∞n−1
n−1
X
i=0
m(T −iF ∩G) = m(F)m(G).
Proof: If the limiting sample average is constant, then it must equal its own expectation.
The
remaining results then follow from the results of this chapter.
2
The ﬁnal result of the previous lemma yields a useful test for determining whether an AMS
system is ergodic.
Lemma 6.7.4 Suppose that a dynamical system (Ω, B, m, T) has ergodic properties with respect to
the indicator functions and hence is AMS with stationary mean m and suppose that B is generated
by a ﬁeld F, then m is ergodic if and only if
lim
n→∞n−1
n−1
X
i=0
m(T −iF ∩F) = m(F)m(F), all F ∈F.
Proof: Necessity follows from the previous result. To show that the preceding formula is suﬃcient
for ergodicity, ﬁrst assume that the formula holds for all events F and let F be invariant. Then
the left-hand side is simply m(F) = m(F) and the right-hand side is m(F)m(F) = m(F)2. But
these two quantities can be equal only if m(F) is 0 or 1, i.e., if it (and hence also m) is ergodic. We
will be done if we can show that the preceding relation holds for all events given that it holds on a
generating ﬁeld. Toward this end ﬁx ϵ > 0. From Corollary 1.5.3 we can choose a ﬁeld event F0 such
that m(F∆F0) ≤ϵ and m(F∆F0) ≤ϵ. To see that we can choose a ﬁeld event F0 that provides a
good approximation to F simultaneously for both measures m and m, apply Corollary 1.5.3 to the
mixture measure p = (m + m)/2 to obtain an F0 for which p(F∆F0) ≤ϵ/2. This implies that both
m(F∆F0) and m(F∆F0)) must be less than ϵ.
From the triangle inequality
| 1
n
n−1
X
i=0
m(T −iF ∩F) −m(F)m(F)|
≤
| 1
n
n−1
X
i=0
m(T −iF ∩F) −1
n
n−1
X
i=0
m(T −iF0 ∩F0)|
+| 1
n
n−1
X
i=0
m(T −iF0 ∩F0) −m(F0)m(F0)| + |m(F0)m(F0) −m(F)m(F)|.
The middle term on the right goes to 0 as n →∞by assumption. The rightmost term is bound
above by 2ϵ. The leftmost term is bound above by
1
n
n−1
X
i=0
|m(T −iF ∩F) −m(T −iF0 ∩F0)|.
Since for any events D, C
|m(D) −m(C)| ≤m(D∆C)
(6.43)

6.7. ERGODICITY
149
each term in the preceding sum is bound above by m((T −iF ∩F)∆(T −iF0 ∩F0)). Since for any
events D, C, H m(D∆C) ≤m(D∆H) + m(H∆C), each of the terms in the sum is bound above by
m((T −iF ∩F)∆(T −iF0 ∩F)) + m((T −iF0 ∩F)∆(T −iF0 ∩F0)) ≤m(T −i(F∆F0)) + m(F∆F0).
Thus the remaining sum term is bound above by
1
n
n−1
X
i=0
m(T −i(F∆F0)) + m(F∆F0) →n→∞m(F∆F0) + m(F∆F0) ≤2ϵ,
which completes the proof.
2
Examples
We next consider a few examples of ergodic processes and systems. Suppose that we have a ran-
dom process {Xn} with distribution m and alphabet A. The process is said to be memoryless or
independent and identically distributed (IID) if there is a measure q on (A, BA) such that for any
rectangle G = ×i∈J Gi, Gi ∈BA, j ∈J , J ⊂I a ﬁnite collection of distinct indices,
m(×j∈J Gj) =
Y
j∈J
q(Gj);
that is, the random variables Xn are mutually independent. Clearly an IID process has the property
that if G and F are two ﬁnite-dimensional rectangles, then there is an integer M such that for
N ≥M
m(G ∩T −nF) = m(G)m(F);
that is, shifting rectangles suﬃciently far apart ensures their independence. A generalization of this
idea is obtained by having a process satisfy this behavior asymptotically as n →∞. A measure m
is said to be mixing or strongly mixing with respect to a transformation T if for all F, G ∈B
lim
n→∞|m(T −nF ∩G) −m(T −nF)m(G)| = 0
(6.44)
and weakly mixing if for all F, G ∈B
|m(T −iF ∩G) −m(T −iF)m(G)| = 0.
(6.45)
If a measure is mixing, then it is also weakly mixing. We have not proved, however, that an IID
process is mixing, since the mixing conditions must be proved for all events, not just the rectangles
considered in the discussion of the IID process. The following lemma shows that proving that (6.44)
or (6.45) holds on a generating ﬁeld is suﬃcient in certain cases.
Lemma 6.7.5 If a measure m is AMS and if (6.45) holds for all sets in a generating ﬁeld, then m
is weakly mixing. If a measure m is asymptotically stationary in the sense that limn→∞m(T −nF)
exists for all events F and if (6.44) holds for all sets in a generating ﬁeld, then m is strongly mixing.
Proof: The ﬁrst result is proved by using the same approximation techniques of the proof of Lemma
6.7.4, that is, approximate arbitrary events F and G, by events in the generating ﬁeld for both m
and its stationary mean m. In the second case, as in the theory of asymptotically mean stationary
processes, the Vitali-Hahn-Saks theorem implies that there must exist a measure m such that
lim
n→∞m(T −nF) = m(F),

150
CHAPTER 6. ERGODIC PROPERTIES
and hence arbitrary events can again be approximated by ﬁeld events under both measures and the
result follows.
2
An immediate corollary of the lemma is the fact that IID processes are strongly mixing since
(6.44) is satisﬁed for all rectangles and rectangles generate the entire event space.
If the measure m is AMS, then either of the preceding mixing properties implies that the condition
of Lemma 6.7.4 is satisﬁed since
|n−1
n−1
X
i=0
m(T −iF ∩F) −m(F)m(F)|
≤
|n−1
n−1
X
i=0
(m(T −iF ∩F) −m(T −iF)m(F)| + |n−1
n−1
X
i=0
m(T −iF)m(F)) −m(F)m(F)|
≤
n−1
n−1
X
i=0
|m(T −iF ∩F) −m(T −iF)m(F)| + m(F)|n−1
n−1
X
i=0
m(T −iF) −m(F)| →n→∞0.
Thus AMS weakly mixing systems are ergodic, and hence AMS strongly mixing systems and IID
processes are also ergodic. A simpler proof for the case of mixing measures follows from the obser-
vation that if F is invariant, then mixing implies that m(F ∩F) = m(F)2, which in turn implies
that m(F) must be 0 or 1 and hence that m is ergodic.
Exercises
1. Show that if m and p are two stationary and ergodic processes with ergodic properties with
respect to all indicator functions, then either they are identical or they are singular in the
sense that there is an event G such that p(G) = 1 and m(G) = 0.
2. Suppose that mi are distinct stationary and ergodic sources with ergodic properties with
respect to all indicator functions. Show that the mixture
m(F) =
X
i
λimi(F),
where
X
i
λi = 1
is stationary but not ergodic. Show that more generally if the mi are AMS, then so is m.
3. A random process is N-ergodic if it is ergodic with respect to T N, that is, if T −NF = F for all
F implies that m(F) = 1. Is an N-ergodic process necessarily ergodic? Is an ergodic process
necessarily N-ergodic? If m is N-ergodic, is the mixture
m(F) = 1
N
N−1
X
i=0
m(T −iF)
ergodic? Suppose that a process is N-ergodic and N-stationary and has the ergodic property
with respect to f. Show that
Ef = 1
N
N−1
X
i=0
E(fT i).

6.7. ERGODICITY
151
4. If a process is stationary and ergodic and T is invertible, show that m(F) > 0 implies that
m(
∞
[
i=−∞
T iF) = 1.
Show also that if m(F) > 0 and m(G) > 0, then for some i m(F ∩T iG) > 0. Show that, in
fact, m(F ∩T iG) > 0 inﬁnitely often.
5. Suppose that (Ω, B, m) is a directly given stationary and ergodic source with shift T and that
f : Ω→Λ is a measurable mapping of sequences in Ωinto another sequence space Λ with shift
S. Show that the induced process (with distribution mf−1) is also stationary and ergodic.
6. Show that if m has ergodic properties with respect to all indicator functions and it is AMS
with stationary mean m, then
lim
n→∞
1
n
n−1
X
i=0
m(T −iF ∩F) = lim
n→∞
1
n2
n−1
X
i=0
n−1
X
j=0
m(T −iF ∩T −jF).
7. Show that a process m is stationary and ergodic if and only if for all f, g ∈L2(m) we have
lim
n→∞
1
n
n−1
X
i=0
E(fgT i) = (Ef)(Eg).
8. Show that the process is AMS with stationary mean m and ergodic if and only if for all bounded
measurements f and g
lim
n→∞
1
n
n−1
X
i=0
Em(fgT i) = (Emf)(Emg).
9. Suppose that m is a process and f ∈L2(m). Assume that the following hold for all integers k
and j:
EmfT k = Emf
Em
¡
(fT k)(fT j)
¢
= Em
³
(f)(fT |k−j|)
´
lim
k→∞Em(ffT |k|) = (Emf)2.
Prove that < f >n →Emf in L2(m).
10. Prove that if a stationary process m is weakly mixing, then there is a subsequence fatJ =
{j1 < j2 < . . .} of the positive integers that has density 0 in the sense that
lim
n→∞
1
n
n−1
X
i=0
1J(i) = 0
and that has the property that
lim
n→∞,n̸∈J |m(T −nF ∩G) −m(F)m(G)| = 0
for all appropriate events F and G.

152
CHAPTER 6. ERGODIC PROPERTIES
11. Are mixtures of mixing processes mixing?
12. Consider the measures m and N −1 PN−1
i=0 mT −i. Does ergodicity of either imply that of the
other?
13. Suppose that m is N-stationary and N-ergodic. Show that the measures mT −n have the same
properties.
14. Suppose an AMS process is not stationary but it is ergodic. Is it recurrent? Inﬁnitely recurrent?
15. Is a mixture of recurrent processes also recurrent?

Chapter 7
Ergodic Theorems
7.1
Introduction
At the heart of ergodic theory are the ergodic theorems: results providing suﬃcient conditions for
dynamical systems or random processes to possess ergodic properties, that is, for sample averages
of the form
< f >n= 1
n
n−1
X
i=0
fT i
to converge to an invariant limit. Traditional developments of the pointwise ergodic theorem focus
on stationary systems and use a subsidiary result known as the maximal ergodic lemma (or theorem)
to prove the ergodic theorem. The general result for AMS systems then follows since an AMS source
inherits ergodic properties from its stationary mean; that is, since the set {x :< f >n (x) converges}
is invariant and since a system and its stationary mean place equal probability on all invariant sets,
one will possess almost everywhere ergodic properties with respect to a class of measurements if and
only if the other one does and the limiting sample averages will be the same.
The maximal ergodic lemma is due to Hoph [34], and its simple and elegant proof due to Garsia
[21] is almost universally used in published developments of the ergodic theorem. Unfortunately,
however, the maximal ergodic lemma is not very intuitive and the proof, although simple and elegant,
is somewhat tricky and adds no insight into the ergodic theorem itself. Furthermore, we shall also
wish to develop limit theorems for sequences of measurements that are not additive, but are instead
subadditive or superadditive. In this case the maximal lemma is insuﬃcient to obtain the desired
results without additional techniques.
During the early 1980s several new and simpler proofs of the ergodic theorem for stationary sys-
tems were developed and subsequently published by Ornstein and Weiss [52], Jones [37], Katznelson
and Weiss [38], and Shields [64]. These developments dealt directly with the behavior of long run
sample averages, and the proofs involved no mathematical trickery.
We here largely follow the
approach of Katznelson and Weiss.
7.2
The Pointwise Ergodic Theorem
This section will be devoted to proving the following result, known as the pointwise or almost
everywhere ergodic theorem or as the Birkhoﬀergodic theorem after George Birkhoﬀ, who ﬁrst
proved the result for a stationary transformations.
153

154
CHAPTER 7. ERGODIC THEOREMS
Theorem 7.2.1 A suﬃcient condition for a dynamical system (Ω, B, m, T) to possess ergodic prop-
erties with respect to all bounded measurements f is that it be AMS. In addition, if the stationary
mean of m is m, then the system possesses ergodic properties with respect to all m-integrable mea-
surements, and if f ∈L1(m), then with probability one under m and m
lim
n→∞
1
n
n−1
X
i=0
fT i = Em(f|I),
where I is the sub-σ-ﬁeld of invariant events.
Observe that the ﬁnal statement follows immediately from the existence of the ergodic property
from Corollary 6.6.1.
In this section we will assume that we are dealing with processes to aid
interpretations; that is, we will consider points to be sequences and T to be the shift. This is simply
for intuition, however, and the mathematics remains valid for the more general dynamical systems.
We can assume without loss of generality that the function f is nonnegative (since if it is not it can
be decomposed into f = f + −f −and the result for the nonnegative f+ and f −and linearity then
yield the result for f). Deﬁne
f = lim inf
n→∞
1
n
n−1
X
i=0
fT i
and
f = lim sup
n→∞
1
n
n−1
X
i=0
fT i.
To prove the ﬁrst part of the theorem we must prove that f = f with probability one under m.
Since both f and f are invariant, the event F = {x : f(x) = f(x)} = {x: limn→∞< f >n exists}
is also invariant and hence has the same probability under m and m. Thus we must show that
m(f = f) = 1. Thus we need only prove the theorem for stationary measures and the result will
follow for AMS measures. As a result, we can henceforth assume that m is itself stationary.
Since f ≤f everywhere by deﬁnition, the desired result will follow if we can show that
Z
f(x) dm(x) ≤
Z
f(x) dm(x);
that is, if f −f ≥0 and
R
(f −f)dm ≤0, then we must have f −f = 0, m-a.e. We accomplish this
by proving the following:
Z
f(x) dm(x) ≤
Z
f(x) dm(x) ≤
Z
f(x) dm(x).
(7.1)
By assumption f ∈L1(m) since in the theorem statement either f is bounded or it is integrable
with respect to m, which is m since we have now assumed m to be stationary. Thus (7.1) implies
not only that lim < f >n exists, but that it is ﬁnite.
We simplify things at ﬁrst by temporarily assuming that f is bounded: Suppose for the moment
that the supremum of f is M < ∞. Fix ϵ > 0. Since f is the limit supremum of < f >n, for each x
there must exist a ﬁnite n for which
1
n
n−1
X
i=0
f(T ix) ≥f(x) −ϵ.
(7.2)

7.2. THE POINTWISE ERGODIC THEOREM
155
Deﬁne n(x) to be the smallest integer for which (7.2) is true. Since f is an invariant function, we
can rewrite (7.2) using the deﬁnition of n(x) as
n(x)−1
X
i=0
f(T ix) ≤
n(x)−1
X
i=0
f(T ix) + n(x)ϵ.
(7.3)
Since n(x) is ﬁnite for all x (although it is not bounded), there must be an N for which
m(x : n(x) > N) =
∞
X
k=N+1
m({x : n(x) = k}) ≤ϵ
M
since the sum converges to 0 as N →∞.. Choose such an N and deﬁne B = {x : n(x) > N}. B
has small probability and can be thought of as the “bad” sequences, where the sample average does
not get close to the limit supremum within a reasonable time. Observe that if x ∈Bc, then also
T ix ∈Bc for i = 1, 2, . . . , n(x) −1; that is, if x is not bad, then the next n(x) −1 shifts of n(x) are
also not bad. To see this assume the contrary, that is, that there exists a p < n(x) −1 such that
T ix ∈Bc for i = 1, 2, . . . , p −1, but T px ∈B. The fact that p < n(x) implies that
p−1
X
i=0
f(T ix) < p(f(x) −ϵ)
and the fact that T px ∈B implies that
n(x)−1
X
i=p
f(T ix) < (n(x) −p)(f(x) −ϵ).
These two facts together, however, imply that
n(x)−1
X
i=0
f(T ix) < n(x)(f(x) −ϵ),
contradicting the deﬁnition of n(x).
We now modify f(x) and n(x) for bad sequences so as to avoid the problem caused by the fact
that n(x) is not bounded. Deﬁne
˜f(x) =
n f(x)
x ̸∈B
M
x ∈B
and
˜n(x) =
n n(x)
x ̸∈B
1
x ∈B .
Analogous to (7.3) the modiﬁed functions satisfy
˜n(x)−1
X
i=0
f(T ix) ≤
˜n(x)−1
X
i=0
˜f(T ix) + ˜n(x)ϵ
(7.4)
and ˜n(x) is bound above by N. To see that (7.4) is valid, note that it is trivial if x ∈B. If x ̸∈B,
as argued previously T ix ̸∈B for i = 1, 2, . . . , n(x) −1 and hence ˜f(T ix) = f(T ix) and (7.4) follows
from (7.3).

156
CHAPTER 7. ERGODIC THEOREMS
Observe for later reference that
Z
˜f(x) dm(x)
=
Z
Bc
˜f(x) dm(x) +
Z
B
˜f(x) dm(x)
≤
Z
Bc f(x) dm(x) +
Z
B
M dm(x)
≤
Z
f(x) dm(x) + Mm(B) ≤
Z
f(x) dm(x) + ϵ,
(7.5)
where we have used the fact that f was assumed positive in the second inequality.
We now break up the sequence into nonoverlapping blocks such that within each block the sample
average of ˜f is close to the limit supremum. Since the overall sample average is a weighted sum of the
sample averages for each block, this will give a long term sample average near the limit supremum.
To be precise, choose an L so large that NM/L < ϵ and inductively deﬁne nk(x) by n0(x) = 0 and
nk(x) = nk−1(x) + ˜n(T nk−1(x)x).
Let k(x) be the largest k for which nk(x) ≤L −1, that is, the number of the blocks in the length L
sequence. Since
L−1
X
i=0
f(T ix) =
k(x)
X
k=1
nk(x)−1
X
i=nk−1(x)
f(T ix) +
L−1
X
i=nk(x)(x)
f(T ix),
we can apply the bound of (7.4) to each of the k(x) blocks, that is, each of the previous k(x) inner
sums, to obtain
L−1
X
i=0
f(T ix) ≤
L−1
X
i=0
˜f(T ix) + Lϵ + (N −1)M,
where the ﬁnal term overbounds the values in the ﬁnal block by M and where the nonnegativity of ˜f
allows the sum to be extended to L −1. We can now integrate both sides of the preceding equation,
divide by L, use the stationarity of m, and apply (7.5) to obtain
Z
f dm ≤
Z
˜f dm + ϵ + (N −1)M
L
≤
Z
f dm + 3ϵ,
(7.6)
which proves the left-hand inequality of (7.1) for the case where f is nonnegative and f is bounded
since ϵ is arbitrary.
Next suppose that f is not bounded.
Deﬁne for any M the “clipped” function fM(x) =
min(f(x), M) and replace f by fM from (7.2) in the preceding argument. Since fM is also in-
variant, the identical steps then lead to
Z
f M dm ≤
Z
f dm.
Taking the limit as M →∞then yields the desired left-hand inequality of (7.1) from the monotone
convergence theorem.
To prove the right-hand inequality of (7.1) we proceed in a similar manner. Fix ϵ > 0 and deﬁne
n(x) to be the least positive integer for which
1
n
n−1
X
i=0
f(T ix) ≤f(x) + ϵ

7.2. THE POINTWISE ERGODIC THEOREM
157
and deﬁne B = {x : n(x) > N}, where N is chosen large enough to ensure that
Z
B
f(x) dm(x)ϵ.
This time deﬁne
˜f(x) =
n f(x)
x ̸∈B
0
x ∈B
and
˜n(x) =
n n(x)
x ̸∈B
1
x ∈B
and proceed as before.
This proves (7.1) and thereby shows that the sample averages of all m-integrable functions
converge. From Corollary 6.6.1, the limit must be the conditional expectations. (The fact that
the limits must be conditional expectations can also be deduced from the preceding arguments by
restricting the integrations to invariant sets.)
2
Combining the theorem with the results of previous chapter immediately yields the following
corollaries.
Corollary 7.2.1 If a dynamical system (Ω, B, m, T) is AMS with stationary mean m and if the
sequence n−1 Pn−1
i=0 fT i is uniformly integrable with respect to m, then the following limit is true
m-a.e., m-a.e., and in L1(m):
lim
n→∞
1
n
n−1
X
i=0
fT i = f = Em(f|I),
where I is the sub-σ-ﬁeld of invariant events, and
Emf = Ef = Ef.
If in addition the system is ergodic, then
lim
n→∞
1
n
n−1
X
i=0
fT i = Em(f)
in both senses.
Corollary 7.2.2 A necessary and suﬃcient condition for a dynamical system to possess ergodic
properties with respect to all bounded measurements is that it be AMS.
Exercises
1. Suppose that m is stationary and that g is a measurement that has the form g = f −fT for
some measurement f ∈L2(m). Show that < g >n has an L2(m) limit as n →∞. Show that
any such g is orthogonal to I∈, the subspace of all square-integrable invariant measurements;
that is, E(gh) = 0 for all h ∈I∈. Suppose that gn is a sequence of functions of this form that
converges to a measurement r in L2(m). Show that < r >n converges in L2(m).
2. Suppose that < f >n converges in L2(m) for a stationary measure m. Show that the sample
averages look increasingly like invariant functions in the sense that
|| < f >n −< f >n T|| →n→∞0.

158
CHAPTER 7. ERGODIC THEOREMS
3. Show that if p and m are two stationary and ergodic measures, then they are either identical
or singular in the sense that there is an event F for which p(F) = 1 and m(F) = 0.
4. Show that if p and m are distinct stationary and ergodic processes and λ ∈(0, 1), then the
mixture process λp + (1 −λ)m is not ergodic.
7.3
Block AMS Processes
Given a dynamical system (Ω, B, m, T) or a corresponding source {Xn}, one may be interested
in the ergodic properties of successive blocks or vectors XN
nN = (XnN, XnN+1, . . . , X(n+1)N−1) or,
equivalently, in the N-shift T N. For example, in communication systems one may use block codes
that parse the input source into N-tuples and then map these successive nonoverlapping blocks into
codewords for transmission. A natural question is whether or not a source being AMS with respect
to T (T-AMS) implies that it is also T N-AMS or vice versa. This section is devoted to answering
this question.
First note that if a source is T-stationary, then it is clearly N-stationary (stationary with respect
to T N) since for any event F m(T −NF) = m(T −N−1F) = . . . = m(F). The converse is not true,
however, since, for example, a process that produces a single sequence that is periodic with period
N is N-stationary but not stationary. The following lemma, which is a corollary to the ergodic
theorem, provides a needed step for treating AMS systems.
Lemma 7.3.1 If µ and η are probability measures on (Ω, B) and if η is T-stationary and asymp-
totically dominates µ (with respect to T), then µ is T-AMS.
Proof: If η is stationary, then from the ergodic theorem it possesses ergodic properties with respect
to all bounded measurements. Since it asymptotically dominates µ, µ must also possesses ergodic
properties with respect to all bounded measurements from Corollary 6.3.5. From Theorem 6.2.1, µ
must therefore be AMS.
2
The principal result of this section is the following:
Theorem 7.3.1 If a system (Ω, B, m, T) is T N-AMS for any positive integer N, then it is T N-AMS
for all integers N.
Proof: Suppose ﬁrst that the system is T N-AMS and let mN denote its N-stationary mean. Then
for any event F
lim
n→∞
1
n
n−1
X
i=0
m(T −iNT −jF) = mN(T −jF); j = 0, 1, . . . , N −1.
(7.7)
We then have
1
N
N−1
X
j=0
lim
n→∞
1
n
n−1
X
i=0
m(T −iNT −jF)
=
lim
n→∞
1
nN
N−1
X
j=0
n−1
X
i=0
m(T −iNT −jF)
=
lim
n→∞
1
n
n−1
X
i=0
m(T −iF),
(7.8)
where, in particular, the last limit must exist for all F and hence m must be AMS
Next assume only that m is AMS with stationary mean m. This implies from Corollary 6.3.2
that m T-asymptotically dominates m, that is, if m(F) = 0, then also lim supn→∞m(T −nF) = 0.

7.3. BLOCK AMS PROCESSES
159
But this also means that for any integer K lim supn→∞m(T −nKF) = 0 and hence that m also T K-
asymptotically dominates m. But this means from Lemma 7.3.1 applied to T K that m is T K-AMS.
2
Corollary 7.3.1 Suppose that a process µ is AMS and has a stationary mean
m(F) = lim
n→∞
1
n
n−1
X
i=0
m(T −iF)
and an N-stationary mean
mN(F) = lim
n→∞
1
n
n−1
X
i=0
m(T −iNF).
Then the two means are related by
1
N
N−1
X
i=0
mN(T −iF) = m(F).
Proof: The proof follows immediately from (7.7)-(7.8).
2
As a ﬁnal observation, if an event is invariant with respect to T, then it is also N-invariant, that
is, invariant with respect to T N. Thus if a system is N-ergodic or ergodic with respect to T N, then
all N-invariant events must have probability of 0 or 1 and hence so must all invariant events. Thus
the system must also be ergodic. The converse, however, is not true: An ergodic system need not
be N-ergodic. A system that is N-ergodic for all N is said to be totally ergodic . A strongly mixing
system can easily be seen to be totally ergodic.
Exercises
1. Prove that a strongly mixing source is totally ergodic.
2. Suppose that m is N-ergodic and N-stationary. Is the mixture
m = 1
N
N−1
X
i=0
mT −i
stationary and ergodic? Are the limits of the form
1
n
n−1
X
i=0
fT i
the same under both m and m? What about limits of the form
1
n
n−1
X
i=0
fT iN?
3. Suppose that you observe a sequence from the process mT −i where you know m but not i.
Under what circumstances could you determine i correctly with probability one by observing
the inﬁnite output sequence?

160
CHAPTER 7. ERGODIC THEOREMS
7.4
The Ergodic Decomposition
In this section we ﬁnd yet another characterization of the limiting sample averages of the ergodic
theorem. We use the ergodic properties to show that the stationary mean of an AMS measure that
describes the limiting sample averages can be considered as a mixture of ergodic stationary measures.
Roughly speaking, if we view the outputs of an AMS system, then we are viewing the outputs of an
ergodic system, but we do not know a priori which one. If we view the limiting sample frequencies,
however, we can determine the ergodic source being seen.
We have seen that if a dynamical system (Ω, B, m, T) is AMS with stationary mean m and hence
has ergodic properties with respect to all indicator functions, then the limiting relative frequencies
(the sample averages of the indicator functions) are conditional probabilities, < 1F >= m(F|I),
where I is the sigma-ﬁeld of invariant events. In addition, if the alphabet is standard, one can select
a regular version of the conditional probability and hence using iterated expectation one can consider
the stationary mean distribution m as a mixture of measures {m(·|I)(x); x ∈Ω}. In this section we
show that almost all of these measures are both stationary and ergodic, and hence the stationary
mean of an AMS system, which describes the sample average behavior of the system, can be written
as a mixture of stationary and ergodic distributions. Intuitively, observing the asymptotic sample
averages of an AMS system is equivalent to observing the same measurements for a stationary and
ergodic system that is an ergodic component of the stationary mean, but we may not know in advance
which ergodic component is being observed. This result is known as the ergodic decomposition and it
is due in various forms to Von Neumann [71], Rohlin [60], Kryloﬀand Bogoliouboﬀ[42], and Oxtoby
[53]. The development here most closely parallels that of Oxtoby. (See also Gray and Davisson [23]
for the simple special case of discrete alphabet processes.)
Although we could begin with Theorem 6.6.1, which characterizes the limiting relative frequencies
as conditional probabilities, and then use Theorem 5.7.1 to choose a regular version of the conditional
probability, it is more instructive to proceed in a direct fashion by assuming ergodic properties. In
addition, this yields a stronger result wherein the decomposition is determined by the properties of
the points themselves and not the underlying measure; that is, the same decomposition works for
all AMS measures.
Let (Ω, B, m, T) be a dynamical system and assume that (Ω, B) is a standard measurable space
with a countable generating set F.
For each event F deﬁne the invariant set G(F) = {x : limn→∞< 1F >n (x) exists} and denote
the invariant limit by < 1F >. Deﬁne the set
G(F) =
\
F ∈F
G(F),
the set of points x for which the limiting relative frequencies of all generating events exist. We shall
call G(F) the set of generic points.
Thus for any x ∈G(F)
lim
n→∞
n−1
X
i=0
1F (T ix) = 1F (x); all F ∈F.
Observe that membership in G(F) is determined strictly by properties of the points and not
by an underlying measure. If m has ergodic properties with respect to the class of all indicator
functions, i.e., if it is AMS, then clearly m(G(F)) = 1 for any event F and hence we also have for
the countable intersection that m(G(F)) = 1. If x ∈G(F), then the set function Px deﬁned by
Px(F) = 1F (x) ; F ∈F

7.4. THE ERGODIC DECOMPOSITION
161
is nonnegative, normalized, and ﬁnitely additive on F from the properties of sample averages. Since
the space is standard, this set function extends to a probability measure, which we shall also denote
by Px, on (Ω, B). Thus we easily obtain that every generic point induces a probability measure Px
and hence also a dynamical system (Ω, B, Px, T) via its convergent relative frequencies. For x not in
G(F) we can simply ﬁx Px as some ﬁxed arbitrary stationary and ergodic measure p∗.
We now conﬁne interest to AMS systems so that the set of generic sequences will have probability
one. Then Px(F) =< 1F > (x) almost everywhere and Px(F) = m(F|I)(x) for all x in a set of m
measure one. Since this is true for any event F, we can deﬁne the countable intersection
H = {x : Px(F) = m(F|I) all F ∈F}
and infer that m(H) = 1. Thus with probability one under m the two measures Px and m(·|I)(x)
agree on F and hence are the same. Thus from the properties of conditional probability and condi-
tional expectation
m(F ∩D) =
Z
D
Px(F)dm, all D ∈I,
(7.9)
and if f is in L1(m), then
Z
D
f dm =
Z
D
(
Z
f(y) dPx(y))dm, all D ∈I.
(7.10)
Thus choosing D = Ωwe can interpret (7.9) as stating that the stationary mean m is a mixture of
the probability measures Px and (7.10) as stating that the expectation of measurements with respect
to the stationary mean can be computed (using iterated expectation) in two steps: First ﬁnd the
expectations with respect to the components Px for all x and then take the average. Such mixture
and average representations will be particularly useful because, as we shall show, the component
measures are both stationary and ergodic with probability one. For focus and motivation, we next
state the principal result of this section–the ergodic decomposition theorem–which makes this notion
rigorous in a useful form. The remainder of the section is then devoted to the proof of this theorem
via a sequence of lemmas that are of some interest in their own right.
Theorem 7.4.1 (The Ergodic Decomposition)
Given a standard measurable space (Ω, B), let F denote a countable generating ﬁeld. Given a
transformation T : Ω→Ωdeﬁne the set of generic sequences
G(F) = {x : lim
n→∞< 1F >n (x) exists all F ∈F}.
For each x ∈G(F) let Px denote the measure induced by the limiting relative frequencies. Deﬁne the
set ˆE ⊂G(F) by ˆE = {x : Px is stationary and ergodic}. Deﬁne the stationary and ergodic measures
px by px = Px for x ∈ˆE and otherwise px = p∗, some ﬁxed stationary and ergodic measure. (Note
that so far all deﬁnitions involve properties of the points only and not of any underlying measure.)
We shall refer to the collection of measures {px; x ∈Ω} as the ergodic decomposition. We have
then that
(a) ˆE is an invariant measurable set,
(b) px(F) = pT x(F) for all points x ∈Ωand events F ∈B,
(c) If m is an AMS measure with stationary mean m, then
m( ˆE) = m( ˆE) = 1,
(7.11)

162
CHAPTER 7. ERGODIC THEOREMS
(d) for any F ∈B
m(F) =
Z
px(F)dm(x) =
Z
px(F)dm(x),
(7.12)
(e) and if f ∈L1(m), then so is
R
f dpx and
Z
f dm =
Z
(
Z
fdpx)dm(x).
(7.13)
(g) In addition, given any f ∈L1(m), then
lim
n→∞n−1
n−1
X
i=0
f(T ix) = Epxf, m −a.e. and m −a.e.
(7.14)
Comment: The ﬁnal result is extremely important for both mathematics and intuition: Say that
a system source is AMS and that f is integrable with respect to the stationary mean; e.g., f is
measurable and bounded and hence integrable with respect to any measure. Then with probability
one a point will be produced such that the limiting sample average exists and equals the expectation
of the measurement with respect to the stationary and ergodic measure induced by the point. Thus
if a system is AMS, then the limiting sample averages will behave as if they were in fact produced by
a stationary and ergodic system. These ergodic components are determined by the points themselves
and not the underlying measure; that is, the same collection works for all AMS measures.
Proof: We begin the proof by observing that the deﬁnition of Px implies that
PT x(F) = 1F (Tx) = 1F (x) = Px(F); all F ∈F; all x ∈G(F)
(7.15)
since limiting relative frequencies are invariant. Since PT x and Px agree on generating ﬁeld events,
however, they must be identical. Since we will select the px to be the Px on a subset of G(F), this
proves (b) for x ∈ˆE.
Next observe that for any x ∈G(F) and F ∈F
Px(F) = 1F (x) = lim
n→∞
n−1
X
i=0
1F (T ix) = lim
n→∞
n−1
X
i=0
1T −1F (T ix) = 1T −1F (x) = Px(T −1F).
Thus for x ∈G(F) the measure Px looks stationary, at least on generating events. The following
lemma shows that this is enough to infer that the Px are indeed stationary.
Lemma 7.4.1 A measure m is stationary on (Ω, σ(F)) if and only if m(T −1F) = m(F) for all
F ∈F.
Proof: Both m and mT −1 are measures on the same space, and hence they are equal if and only if
they agree on a generating ﬁeld.
2
Note in passing that no similar countable description of an AMS measure is known, that is, if
limn→∞n−1 Pn−1
i=0 m(T −iF) converges for all F in a generating ﬁeld, it does not follow that the
measure is AMS.
As before let ˆE denote the subset of G(F) consisting of all of the points x for which Px is ergodic.
From Lemma 6.7.4, the deﬁnition of Px, and the stationarity of the Px we can write ˆE as
ˆE = {x : x ∈G(F); lim
n→∞n−1
n−1
X
i=0
Px(T −iF ∩F) = Px(F)2, all F ∈F}.

7.4. THE ERGODIC DECOMPOSITION
163
Since the probabilities Px are measurable functions, the set ˆE where the given limit exists and has
the given value is also measurable. Since Px(F) = PT x(F), the set is also invariant. This proves (a)
of the theorem. We have already proved (b) for x ∈ˆE. For x ̸∈ˆE we have that px = p∗. Since ˆE is
invariant, we also know that Tx ̸∈ˆE and hence also pT x = p∗so that pT x = px.
We now consider an AMS measure m. Although Lemma 6.7.4 provided a countable description
of ergodicity that was useful to prove that the collection of all points inducing ergodic measures is
an event, the following lemma provides another countable description of ergodicity that is useful for
computing the probability of the set ˆE.
Lemma 7.4.2 Let (Ω, σ(F), m, T) be an AMS dynamical system with stationary mean m. Deﬁne
the sets
G(F, m) = {x : lim
n→∞
1
n
n−1
X
i=0
1F (T ix) = m(F)}; F ∈F.
Thus G(F, m) is the set of sequences for which the limiting relative frequency exists for all events F
in the generating ﬁeld and equals the probability of the event under the stationary mean. Deﬁne also
G(F, m) =
\
F ∈F
G(F, m).
A necessary and suﬃcient condition for m to be ergodic is that m(G(F, m)) = 1.
Comment: The lemma states that an AMS measure m is ergodic if and only if it places all of its
probability on the set of generic points that induce m through their relative frequencies.
Proof: If m is ergodic then < 1F >= m(F)m-a.e. for all F and the result follows immediately.
Conversely, if m(G(F, m)) = 1 then also m(G(F, m)) = 1 since the set is invariant. Thus integrating
to the limit using the dominated convergence theorem yields
lim
n→∞n−1
n−1
X
i=0
m(T −iF ∩F)
=
Z
G(F,m)
1F ( lim
n→∞n−1
n−1
X
i=0
1F T i)dm
=
m(F)2, all F ∈F,
which with Lemma 6.7.4 proves that m is ergodic, and hence so is m from Lemma 6.7.2.
2
Proof of the Theorem If we can show that m( ˆE) = 1, then the remainder of the theorem will follow
from (7.9) and (7.10) and the fact that px = m(·|I)(x) m −a.e. From the previous lemma, this will
be the case if with px probability one
1F = px(F), all F ∈F,
or, equivalently, if for all F ∈F
1F = px(F) px −a.e.
This last equation will hold if for each F ∈F
Z
dpx(y)|1F (y) −px(F)|2 = 0.
This relation will hold on a set of x of m probability one if
Z
dm(x)
Z
dpx(y) |1F (y) −px(F)|2 = 0.

164
CHAPTER 7. ERGODIC THEOREMS
Thus proving the preceding equality will complete the proof of the ergodic decomposition theorem.
Expanding the square and using the fact that G(F) has m probability one yield
Z
G(F)
dm(x)
Z
G(F)
dpx(y)1F (y)2−
2
Z
G(F)
dm(x)px(F)
Z
G(F)
dpx(y)1F (y) +
Z
G(F)
dm(x)px(F)2.
Since < 1F > (y) = py(F), the ﬁrst and third terms on the right are equal from (7.10).
(Set
f(x) =< 1F (x) >= px(F) in (7.10).) Since px is stationary,
R
dpx < 1F >= px(F). Thus the middle
term cancels the remaining terms and the preceding expression is zero, as was to be shown. This
completes the proof of the ergodic decomposition theorem.
2
Exercises
1. Suppose that a measure m is stationary and ergodic and N is a ﬁxed positive integer. Show
that there is a measure η that is N-stationary and N-ergodic that has the property that
m = 1
N
N−1
X
i=0
ηT −i.
This shows that although an ergodic source may not be N-ergodic, it can always be decomposed
into a mixture of an N-ergodic source and its shifts. This result is due to Nedoma [49].
7.5
The Subadditive Ergodic Theorem
Sample averages are not the only measurements of systems that depend on time and converge. In
this section we consider another class of measurements that have an ergodic theorem: subadditive
measurements. An example of such a measurement is the Levenshtein distance between sequences
(see Example 2.5.4).
The subadditive ergodic theorem was ﬁrst developed by Kingman [39], but the proof again follows
that of Katznelson and Weiss [38]. A sequence of functions {fn; = 1, 2, . . .} satisfying the relation
fn+m(x) ≤fn(x) + fm(T nx), all n, m = 1, 2, . . .
for all x is said to be subadditive. Note that the unnormalized sample averages fn = n < f >n of
a measurement f are additive and satisfy the relation with equality and hence are a special case of
subadditive functions.
The following result is for stationary systems. Generalizations to AMS systems are considered
later.
Theorem 7.5.1 Let (Ω, B, m, T) be a stationary dynamical system. Suppose that {fn; n = 1, 2, . . .}
is a subadditive sequence of m-integrable measurements. Then there is an invariant function φ(x)
such that m-a.e.
lim
n→∞
1
nfn(x) = φ(x).
The function φ(x) is given by
φ(x) = inf
n
1
nEm(fn|I) = lim
n→∞
1
nEm(fn|I).

7.5. THE SUBADDITIVE ERGODIC THEOREM
165
Most of the remainder of this section is devoted to the proof of the theorem. On the way some
lemmas giving properties of subadditive functions are developed. Observe that since the conditional
expectations with respect to the σ-ﬁeld of invariant events are themselves invariant functions, φ(x)
is invariant as claimed.
Since the functions are subadditive,
Em(fn+m|I) ≤Em(fn|I) + Em(fmT n|I).
From the ergodic decomposition theorem, however, the third term is EPx(fmT n) = EPx(fm) since
Px is a stationary measure with probability one. Thus we have with probability one that
Em(fn+m|I)(x) ≤Em(fn|I)(x) + Em(fm|I)(x).
For a ﬁxed x, the preceding is just a sequence of numbers, say ak, with the property that
an+k ≤an + ak.
A sequence satisfying such an inequality is said to be subadditive. The following lemma (due to
Hammersley and Welch [32]) provides a key property of subadditive sequences.
Lemma 7.5.1 If {an; n = 1, 2, . . .} is a subadditive sequence, then
lim
n→∞
an
n = inf
n≥1
an
n ,
where if the right-hand side is −∞, the preceding means that an/n diverges to −∞.
Proof: First observe that if an is subadditive, for any k, n
akn ≤an + a(k−1)n ≤an + an + a(k−2)n ≤. . . ≤kan.
(7.16)
Call the inﬁmum γ and ﬁrst consider the case where it is ﬁnite. Fix ϵ > 0 and choose a positive
integer N such that N −1aN < γ + ϵ. Deﬁne b = max1≤k≤3N−1 ak. For each n ≥3N there is a
unique integer k(n) for which (k(n) + 2)N ≤n < (k(n) + 3)N. Since the sequence is subadditive,
an ≤ak(n)N + a(n−k(n)N). Since n −k(n)N < 3N by construction, an−k(n)N ≤b and therefore
γ ≤an
n ≤ak(n)N + b
n
.
Using (7.16) this yields
γ ≤an
n ≤k(n)aN
n
+ b
n ≤k(n)N
n
aN
N + b
n ≤k(n)N
n
(γ + ϵ) + b
n.
By construction k(n)N ≤n −2N ≤n and hence
γ ≤an
n ≤γ + ϵ + b
n.
Thus given an ϵ, choosing n0 large enough yields γ ≤n−1an ≤γ + 2ϵ for all n > n0, proving the
lemma. If γ is −∞, then for any positive M we can choose n so large that n−1an ≤−M. Proceeding
as before for any ϵ there is an n0 such that if n ≥n0, then n−1an ≤−M + ϵ. Since M is arbitrarily
big and ϵ arbitrarily small, n−1an goes to −∞.
2

166
CHAPTER 7. ERGODIC THEOREMS
Returning to the proof of the theorem, the preceding lemma implies that with probability one
φ(x) = inf
n≥1
1
nEm(fn|I)(x) = lim
n→∞
1
nEm(fn|I)(x).
As in the proof of the ergodic theorem, deﬁne
f(x) = lim sup
n→∞
1
nfn(x)
and
f(x) = lim inf
n→∞
1
nfn(x).
Lemma 7.5.2 Given a subadditive function fn(x); = 1, 2, . . ., f and f as deﬁned previously are
invariant functions m-a.e.
Proof: From the deﬁnition of subadditive functions
f1+n(x) ≤f1(x) + fn(Tx)
and hence
fn(Tx)
n
≥1 + n
n
f1+n(x)
1 + n
−f1(x)
n
.
Taking the limit supremum yields f(Tx) ≥f(x). Since m is stationary, EmfT = Emf and hence
f(x) = f(Tx) m-a.e. The result for f follows similarly.
2
We now tackle the proof of the theorem. It is performed in two steps. The ﬁrst uses the usual
ergodic theorem and subadditivity to prove that f ≤φ a.e. Next a construction similar to that of
the proof of the ergodic theorem is used to prove that f = φ a.e., which will complete the proof.
As an introduction to the ﬁrst step, observe that since the measurements are subadditive
1
nfn(x) ≤1
n
n−1
X
i=0
f1(T ix).
Since f1 ∈L1(m) for the stationary measure m, we know immediately from the ergodic theorem
that
f(x) ≤f1(x) = Em(f1|I).
(7.17)
We can obtain a similar bound for any of the fn by breaking it up into ﬁxed size blocks of length,
say N and applying the ergodic theorem to the sum of the fN. Fix N and choose n ≥N. For each
i = 0, 1, 2, . . . , N −1 there are integers j, k such that n = i + jN + k and 1 ≤k < N. Hence using
subadditivity
fn(x) ≤fi(x) +
j−1
X
l=0
fN(T lN+ix) + fk(T jN+ix).
Summing over i yields
Nfn(x) ≤
N−1
X
i=0
fi(x) +
jN−1
X
l=0
fN(T lx) +
N−1
X
i=0
fn−i−jN(T jN+ix),

7.5. THE SUBADDITIVE ERGODIC THEOREM
167
and hence
1
nfn(x) ≤
1
nN
jN−1
X
l=0
fN(T lx) +
1
nN
N−1
X
i=0
fi(x) +
1
nN
N−1
X
i=0
fn−i−jN(T jN+ix).
As n →∞the middle term on the right goes to 0. Since fN is assumed integrable with respect to
the stationary measure m, the ergodic theorem implies that the leftmost term on the right tends
to N −1 < fN >. Intuitively the rightmost term should also go to zero, but the proof is a bit more
involved. The rightmost term can be bound above as
1
nN
N−1
X
i=0
|fn−i−jN(T jN+ix)| ≤
1
nN
N−1
X
i=0
N
X
l=1
|fl(T jN+ix)|
where j depends on n (the last term includes all possible combinations of k in fk and shifts). Deﬁne
this bound to be wn/n. We shall prove that wn/n tends to 0 by using Corollary 4.6.1, that is, by
proving that
∞
X
n=0
m(wn
n > ϵ) < ∞.
(7.18)
First note that since m is stationary,
wn = 1
N
N−1
X
i=0
N
X
l=1
|fl(T jN+ix)|
has the same distribution as
w = 1
N
N−1
X
i=0
N
X
l=1
|fl(T ix)|.
Thus we will have proved (7.18) if we can show that
∞
X
n=0
m(w > nϵ) < ∞.
(7.19)
To accomplish this we need a side result, which we present as a lemma.
Lemma 7.5.3 Given a nonnegative random variable X deﬁned on a probability space (Ω, B, m),
∞
X
n=1
m(X ≥n) ≤EX.
Proof:
EX
=
∞
X
n=1
E(X|X ∈[n −1, n))m(X ∈[n −1, n))
≥
∞
X
n=1
(n −1)m(X ∈[n −1, n))
=
∞
X
n=1
nm(X ∈[n −1, n)) −1.

168
CHAPTER 7. ERGODIC THEOREMS
By rearranging terms the last sum can be written as
∞
X
n=0
m(X ≥n),
which proves the lemma.
2
Applying the lemma to the left side of (7.19) yields
∞
X
n=0
m(w
ϵ > n) ≤
∞
X
n=0
m(w
ϵ ≥n) ≤1
ϵ Em(w) < ∞,
since m-integrability of the fn implies m-integrability of their absolute magnitudes and hence of a
ﬁnite sum of absolute magnitudes. This proves (7.19) and hence (7.18). Thus we have shown that
f(x) ≤1
N fN(x) = 1
N Em(fN|I)
for all N and hence that
f(x) ≤φ(x), m −a.e. .
(7.20)
This completes the ﬁrst step of the proof.
If φ(x) = −∞, then (7.20) completes the proof since the desired limit then exists and is −∞.
Hence we can focus on the invariant set G = {x : φ(x) > −∞}. Deﬁne the event GM = {x : φ(x) ≥
−M}. We shall show that for all M
Z
GM
f dm ≥
Z
GM
φ dm,
(7.21)
which with (7.20) proves that f = φ on GM. We have, however, that
G =
∞
[
M=1
GM.
Thus proving that f = φ on all GM also will prove that f = φ on G and complete the proof of the
theorem.
We now proceed to prove (7.21) and hence focus on x in GM and hence only x for which
φ(x) ≥−M. We proceed as in the proof of the ergodic theorem. Fix an ϵ > 0 and deﬁne fM =
max(f, −M −1). Observe that this implies that
φ(x) ≥f M(x), all x.
(7.22)
Deﬁne n(x) as the smallest integer n ≥1 for which n−1fn(x) ≤f M(x) + ϵ. Deﬁne B = {x : n(x) >
N} and choose N large enough so that
Z
B
(|f1(x)| + M + 1) dm(x) < ϵ.
(7.23)
Deﬁne the modiﬁed functions as before:
˜f M(x) =
½
f M(x)
x ̸∈B
f1(x)
x ∈B

7.5. THE SUBADDITIVE ERGODIC THEOREM
169
and
˜n(x) =
n n(x)
x ̸∈B
1
x ∈B .
From (7.23)
Z
˜fM dm
=
Z
Bc
˜f M dm +
Z
B
˜f M dm
=
Z
Bc f M dm +
Z
B
f1 dm
≤
Z
Bc f M dm +
Z
B
|f1| dm
≤
Z
Bc f M dm + ϵ −
Z
B
(M + 1) dm
≤
Z
Bc f M dm + ϵ +
Z
B
f M dm
≤
Z
f M dm + ϵ.
(7.24)
Since f M is invariant,
fn(x)(x) ≤
n(x)−1
X
i=0
f M(T ix) + n(x)ϵ
and hence also
f˜n(x)(x) ≤
˜n(x)−1
X
i=0
˜f M(T ix) + ˜n(x)ϵ
Thus for L > N
fL(x) ≤
L−1
X
i=0
˜f M(T ix) + Lϵ + N(M −1) +
L−1
X
i=L−N
|f1(T ix)|.
Integrating and dividing by L results in
Z
φ(x) dm(x) ≤
Z
1
LfL(x) dm(x)
=
Z
1
LfL(x) dm(x)
≤
Z
˜f M(x) dm(x) + ϵ + N(M + 1)
L
+ N
L
Z
|f1(x)| dm(x).
Letting L →∞and using (7.24) yields
Z
φ(x) dm(x) ≤
Z
fM(x) dm(x).
From (7.22), however, this can only hold if in fact fM(x) = φ(x) a.e. This implies, however, that
f(x) = φ(x) a.e. since the event f(x) ̸= fM(x) implies that fM(x) = −M −1 which can only
happen with probability 0 from (7.22) and the fact that fM(x) = φ(x) a.e. This completes the proof
of the subadditive ergodic theorem for stationary measures.
2
Unlike the ergodic theorem, the subadditive ergodic theorem does not easily generalize to AMS
measures since if m is not stationary, it does not follow that f or f are invariant measurements or
that the event {f = f} is either an invariant event or a tail event. The following generalization is a
slight extension of unpublished work of Wu Chou.

170
CHAPTER 7. ERGODIC THEOREMS
Theorem 7.5.2 Let (Ω, B, m, T) be an AMS dynamical system with stationary mean m. Suppose
that {fn; n = 1, 2, . . .} is a subadditive sequence of m-integrable measurements. There is an invariant
function φ(x) such that m-a.e.
lim
n→∞
1
nfn(x) = φ(x)
if and only if m << m, in which case the function φ(x) is given by
φ(x) = inf
n
1
nEm(fn|I) = lim
n→∞
1
nEm(fn|I).
Comment: Recall from Theorem 6.4.3 that the condition that the AMS measure is dominated
by its stationary mean is equivalent to the condition that the system be recurrent. Note that the
theorem immediately implies that the Kingman ergodic theorem holds for two-sided AMS processes.
What is new here is the result for one-sided AMS processes.
Proof: If m << m, then the conclusions follow immediately from the ergodic theorem for stationary
sources since m inherits the probability 1 and 0 sets from m. Suppose that m is not dominated by
m. Then from Theorem 6.4.3, the system is not conservative and hence there exists a wandering set
W with nonzero probability, that is, the sets T −nW are pairwise disjoint and m(W) > 0. We now
construct a counterexample to the subadditive ergodic theorem, that is, a subadditive function that
does not converge. Deﬁne
fn(x) =
( −2kn
x ∈W and 2kn ≤n ≤2kn+1, kn = 0, 1, 2, . . .
fn(T jx) −2n
x ∈T −jW
0
otherwise
We ﬁrst prove that fn is subadditive, that is, that fn+m(x) ≤fn(x) + fm(T nx) for all x ∈Ω. To do
this we consider three cases: (1) x ∈W, (2) x ∈W ∗= S∞
i=1 T −iW, and (3) x ̸∈W ∪W ∗. In case
(1),
fn(x) = −2kn
and
fn+m(x) = −2kn+m.
To evaluate fm(T nx) observe that x ∈W implies that T nx ̸∈W ∪W ∗since otherwise
x ∈T −n
∞
[
k=0
T −kW =
∞
[
k=n
T −kW,
which is impossible because x ∈W and W is a wandering set. Thus fn(T mx) = 0. Thus for x ∈W
fn(x) + fm(T nx) −fn+m(x) = −2kn + 2kn+m.
Since by construction kn +m ≥kn, this is nonnegative and f is subadditive in case (1). For case (3)
the result is trivial: If x ̸∈W ∪W ∗, then no shift of x is ever in W, and all the terms are 0. This
leaves case (2).
Suppose now that x ∈T −jW for some k. Then
fn+m(x) = fn+m(T jx) −2n+m = −2kn+m −2n+m,
and
fn(x) = fn(T jx) −2n = −2kn −2n.

7.5. THE SUBADDITIVE ERGODIC THEOREM
171
For the ﬁnal term there are two cases. If n > j, then analogously to case (1) x ∈T −jW implies that
T nx ̸∈W ∪W ∗since otherwise
x ∈T −n
∞
[
k=0
T −kW =
∞
[
k=n
T −kW,
which is impossible since W is a wandering set and hence x cannot be in both T −jW and in
S∞
k=n T −kW since n > j. If n ≤j, then
fm(T nx) = fm(T jx) −2m = −2km −2m.
Therefore if n > j,
fn(x) + fm(T nx) −fn+m(x) = −2kn −2n + 0 + 2kn+m + 2n+m ≥0.
If n ≤j then
fn(x) + fm(T nx) −fn+m(x) = −2kn −2n −2km −2m + 2kn+m + 2n+m.
Suppose that n ≥m ≥1, then
2n+m −2n −2m = 2n(2m −1 −2−|n−m|) ≥0.
Similarly, since then kn ≥km, then
2kn+m −2kn −2km = 2kn(2kn+m−kn −1 −2−|kn−km| ≥0.
Similar inequalities follow when n ≤m, proving that fn is indeed subadditive on Ω.
We now complete the proof. The event W has nonzero probability, but if x ∈W, then
lim
n→∞
1
2n f2n(x) = lim
n→∞
−2n
2n
= −1
whereas
lim
n→∞
1
2n −1f2n−1(x) = lim
n→∞
−2n−1
2n −1 = −1
2
and hence n−1fn does not have a limit on this set of nonzero probability, proving that the subadditive
ergodic theorem does not hold in this case.
2

172
CHAPTER 7. ERGODIC THEOREMS

Chapter 8
Process Metrics and the Ergodic
Decomposition
8.1
Introduction
Given two probability measures, say p and m, on a common probability space, how diﬀerent or
distant from each other are they? Similarly, given two random processes with distributions p and m,
how distant are the processes from each other and what impact does such a distance have on their
respective ergodic properties? The goal of this ﬁnal chapter is to develop two quite distinct notions
of the distance d(p, m) between measures or processes and to use these ideas to delve further into
the ergodic properties of processes and the ergodic decomposition. One metric, the distributional
distance, measures how well the probabilities of certain important events match up for the two
probability measures, and hence this metric need not have any relation to any underlying metric on
the original sample space. In other words, the metric makes sense even when we are not putting
probability measures on metric spaces. The second metric, the ρ-distance (rho-bar distance) depends
very strongly on a metric on the output space of the process and measures distance not by how
diﬀerent probabilities are, but by how well one process can be made to approximate another. The
second metric is primarily useful in applications in information theory and statistics.
Although
these applications are beyond the scope of this book, the metric is presented both for comparison
and because of the additional insight into ergodic properties the metric provides.
Such process metrics are of preliminary interest because they permit the quantitative assessment
of how diﬀerent two random processes are and how their ergodic properties diﬀer. Perhaps more
importantly, however, putting a metric on a space of random processes provides a topology for
the space and hence a collection of Borel sets and a measurable space to which we can assign
probability measures. This assignment of a probability measure to a space of processes provides a
general deﬁnition of a mixture process and provides a means of delving more deeply into the ergodic
decomposition of stationary measures developed in the previous chapter.
We have seen from the ergodic decomposition that all AMS measures have a stationary mean
that can be considered as a mixture of stationary and ergodic components. Thus, for example, any
stationary measure m can be considered as a mixture of the form
m =
Z
pxdm(x),
(8.1)
173

174
CHAPTER 8. PROCESS METRICS AND THE ERGODIC DECOMPOSITION
which is an abbreviation for
m(F) =
Z
px(F) dm(x), all F ∈B.
In addition, any m-integrable f can be integrated in two steps as
Z
f(x) dm(x) =
Z
(
Z
dpx(y)f(y)) dm(x).
(8.2)
Thus stationary measures can be considered to be mixtures of stationary and ergodic measures; that
is, a stationary and ergodic measure is randomly selected from some collection and then used. In
other words, we eﬀectively have a probability measure on the space of all stationary and ergodic
probability measures and the ﬁrst measure is used to select one of the measures of the given class.
Alternatively, we have a probability measure on the space of all probability measures on the given
measurable space, but this super probability measure assigns probability one to the collection of
ergodic and stationary measures. The preceding relations show that both probabilities and expecta-
tions of measurements over the resulting mixture can be computed as integrals of the probabilities
and expectations over the component measures.
In applications of the theory of random processes such as information and communication theory,
one is often concerned not only with ordinary functions or measurements, but also with functionals
of probability measures, that is, mappings of the space of probability measures into the real line.
Examples include the entropy rate of a dynamical system, the distortion-rate function of a random
process with respect to a ﬁdelity criterion, the information capacity of a noisy channel, the rate
required to code a given process with nearly zero reproduction error, the rate required to code a
given process for transmission with a speciﬁed ﬁdelity, and the maximum rate of coded information
that can be transmitted over a noisy channel with nearly zero error probability. (See, for example,
the papers and commentary in Gray and Davisson [25].) All of these quantities are functionals of
measures: a given process distribution or measure yields a real number as a value of the functional.
Given such a functional of measures, say D(m), it is natural to inquire under what conditions
the analog of (8.2) for functions might hold, that is, conditions under which
D(m) =
Z
D(px) dm(x).
(8.3)
This is in general a much more diﬃcult issue than before since unlike an ordinary measurement f,
the functional depends on the underlying probability measure. One goal of this chapter to develop
conditions under which (8.3) holds.
8.2
A Metric Space of Measures
We now focus on spaces of probability measures and on the structure of such spaces. We shall show
how the ergodic decomposition provides an example of a probability measure on such a space. In
Section 8.4 we shall study certain functionals deﬁned on this space.
Let (Ω, B) be a measurable space such that B is countably generated (e.g., it is standard). We do
not assume any metric structure for Ω. Deﬁne P((Ω, B)) as the class of all probability measures on
(Ω, B). It is easy to see that P((Ω, B)) is a convex subset of the class of all ﬁnite measures on (Ω, B);
that is, if m1 and m2 are in P((Ω, B)) and λ ∈(0, 1), then λm1 + (1 −λ)m2 is also in P((Ω, B)).
We can put a variety of metrics on P((Ω, B)) and thereby make it a metric space. This will
provide a notion of closeness of probability measures on the class. It will also provide a Borel ﬁeld

8.2. A METRIC SPACE OF MEASURES
175
of subsets of P((Ω, B)) on which we can put measures and thereby construct mixtures of measures
in the class.
In this section we consider a type of metric that is not very strong, but will suﬃce for developing
the ergodic decomposition of aﬃne functionals. In the next section another metric will be considered,
but it is tied to the assumption of Ωbeing itself a metric space and hence applies less generally.
Given any class G = {Fi; i = 1, 2, , . . .} consisting of a countable collection of events we can deﬁne
for any measures p, m ∈P((Ω, B)) the function
dG(p, m) =
∞
X
i=1
2−i|p(Fi) −m(Fi)|.
(8.4)
If G contains a generating ﬁeld, then dG is a metric on P((Ω, B)) since from Lemma 1.5.5 two
measures deﬁned on a common σ-ﬁeld generated by a ﬁeld are identical if and only if they agree
on the generating ﬁeld. We shall always assume that G contains such a ﬁeld. We shall call such a
distance a distributional distance. Usually we will require that G be a standard ﬁeld and hence that
the underlying measurable space (Ω, B) is standard. Occasionally, however, we will wish a larger
class G but will not require that it be standard. The diﬀerent requirements will be required for
diﬀerent results. When the class G is understood from context, the subscript will be dropped.
Let (P(Ω, B), dG) denote the metric space of P((Ω, B)) with the metric dG. A key property of
spaces of probability measures on a ﬁxed measurable space is given in the following lemma.
Lemma 8.2.1 The metric space (P((Ω, B)), dG) of all probability measures on a measurable space
(Ω, B) with a countably generated sigma-ﬁeld is separable if G contains a countable generating ﬁeld.
If also G is standard (as is possible if the underlying measurable space (Ω, B) is standard), then also
(P, dG) is complete and hence Polish.
Proof: For each n let An denote the set of nonempty intersection sets or atoms of {F1, . . . , Fn}, the
ﬁrst n sets in G. (These are Ωsets, events in the original space.) For each set G ∈An choose an
arbitrary point xG such that xG ∈G. We will show that the class of all measures of the form
r(F) =
X
G∈An
pG1F (xG),
where the pG are nonnegative and rational and satisfy
X
G∈An
pG = 1,
forms a dense set in P((Ω, B)). Since this class is countable, P((Ω, B)) is separable. Observe that we
are approximating all measures by ﬁnite sums of point masses. Fix a measure m ∈(P((Ω, B)), dG)
and an ϵ > 0. Choose n so large that 2−n < ϵ/2. Thus to match up two measures in d = dG, (8.4)
implies that we must match up the probabilities of the ﬁrst n sets in G since the contribution of the
remaining terms is less that 2−n. Deﬁne
rn(F) =
X
G∈An
m(G)1F (xG)
and note that
m(F) =
X
G∈An
m(G)m(F|G),

176
CHAPTER 8. PROCESS METRICS AND THE ERGODIC DECOMPOSITION
where m(F|G) = m(F ∩G)/m(G) is the elementary conditional probability of F given G if m(G) > 0
and is arbitrary otherwise. For convenience we now consider the preceding sums to be conﬁned to
those G for which m(G) > 0.
Since the G are the atoms of the ﬁrst n sets {Fi}, for any of these Fi either G ⊂Fi and hence
G ∩Fi = G or G ∩Fi = ∅. In the ﬁrst case 1Fi(xG) = m(Fi|G) = 1, and in the second case
1Fi(xG) = m(Fi|G) = 0, and hence in both cases
rn(Fi) = m(Fi); i = 1, 2, , . . . , n.
This implies that
d(rn, m) ≤
∞
X
i=n+1
2−i = 2−n ≤ϵ
2.
Enumerate the atoms of An as {Gl; l = 1, 2, . . . , L, where L ≤2n. For all l but the last (l = L)
pick a rational number pGl such that
m(Gl) −2−l ϵ
4 ≤pGl ≤m(Gl);
that is, we choose a rational approximation to m(Gl) that is slightly less than m(Gl). We deﬁne
pGL to force the rational approximations to sum to 1:
pGL = 1 −
L−1
X
l=1
pGl.
Clearly pGL is also rational and
|pGL −m(GL)|
=
|
L−1
X
l=1
pGl −
L−1
X
l=1
m(Gl)|
≤
L−1
X
l=1
|pGl −m(Gl)|
≤
ϵ
4
L−1
X
l=1
2−l ≤ϵ
4.
Thus
X
G∈An
|pG −m(G)| =
L−1
X
l=1
|pGl −m(Gl)| + |pGL −m(GL)| ≤ϵ
2.
Deﬁne now the measure tn by
tn(F) =
X
G∈An
pG1F (xG)
and observe that
d(tn, rn)
=
n
X
i=1
2−i|tn(Fi) −rn(Fi)|
=
n
X
i=1
2−i|
X
G∈An
1Fi(pG −m(G))|
≤
n
X
i=1
2−i X
G∈An
|pG −m(G)| ≤ϵ
4.

8.2. A METRIC SPACE OF MEASURES
177
We now know that
d(tn, m) ≤d(tn, rn) + d(rn, m) ≤ϵ
2 + ϵ
2 = ϵ,
which completes the proof of separability.
Next assume that mn is a Cauchy sequence of measures. From the deﬁnition of d this can only
be if also mn(Fi) is a Cauchy sequence of real numbers for each i. Since the real line is complete,
this means that for each i mn(Fi) converges to something, say α(Fi). The set function α is deﬁned
on the class G and is clearly nonnegative, normalized, and ﬁnitely additive. Hence if G is also stan-
dard, then α extends to a probability measure on (Ω, B); that is, α ∈P((Ω, B)). By construction
d(mn, α) →0 as n →∞, and hence in this case P((Ω, B)) is complete.
Lemma 8.2.2 Assume as in the previous lemma that (Ω, B) is standard and G is a countable gener-
ating ﬁeld. Then (P((Ω, B)), dG) is sequentially compact; that is, if {µn} is a sequence of probability
measures in P(Ω, B), then it has a subsequence µnk that converges.
The proof of the lemma is based on a technical result that we state and prove separately.
Lemma 8.2.3 The space [0, 1] with the Euclidean metric |x −y| is sequentially compact. The space
[0, 1]I with I countable and the metric of Example 2.5.11 is sequentially compact.
Proof: The second statement follows from the ﬁrst and Corollary 2.5.1. To prove the ﬁrst statement,
let Sn
i , n = 1, 2, . . . , n denote the closed spheres [(i −1)/n, i/n] having diameter 1/n and observe
that for each n = 1, 2, . . .
[0, 1] ⊂
n
[
i=1
Sn
i .
Let {xi} be an inﬁnite sequence of distinct points in [0,1]. For n = 2 there is an integer k1 such
that K1 = S2
k1 contains inﬁnitely many xi. Since Sn
k1 ⊂S
i S3
i we can ﬁnd an integer k2 such that
K2 = S2
k1 ∩S3
k2 contains inﬁnitely many xi. Continuing in this manner we construct a sequence of
sets Kn such that Kn ⊂Kn−1 and the diameter of Kn is less than 1/n. Since [0, 1] is complete, it
follows from the ﬁnite intersection property of Lemma 3.2.2 that T
n Kn consists of a single point
x ∈[0, 1]. Since every neighborhood of x contains inﬁnitely many of the xi, x must be a limit point
of the xi.
2
Proof of Lemma 8.2.2 Since G is a countable generating ﬁeld, a probability measure µ in P is
speciﬁed via the Carath´eodory extension theorem by the inﬁnite dimensional vector µ = {µ(F); F ∈
G} ∈[0, 1]Z+. Suppose that µn is a sequence of probability measures and let µn also denote the
corresponding sequence of vectors. From Lemma 8.2.3 [0, 1]Z+ is sequentially compact with respect
to the product space metric, and hence there must exist a µ ∈[0, 1]Z+ and a subsequence µnk such
that µnk →µ. The vector µ provides a set function µ assigning a value µ(F) for all sets F in the
generating ﬁeld G. This function is nonnegative, normalized, and ﬁnitely additive. In particular, if
F = S
i Fi is a union of a ﬁnite number of disjoint events in G, then
µ(F) = lim
k→∞µnk(F)
=
lim
k→∞
X
i
µnk(Fi)
=
X
i
lim
k→∞µnk(Fi)
=
X
i
µ(Fi).
Since the alphabet is standard, µ must extend to a probability measure on (Ω, B). By construction
µnk(F) →µ(F) for all F ∈G and hence dG(µnk, µ) →0, completing the proof.
2

178
CHAPTER 8. PROCESS METRICS AND THE ERGODIC DECOMPOSITION
An Example
As an example of the previous construction, suppose that Ωis itself a Polish space with some metric
d and that G is the ﬁeld generated by the countable collection of spheres VΩof Lemma 3.2.1. Thus
if mn →µ under dG, mn(F) →µ(F) for all spheres in the collection and all ﬁnite intersections and
unions of such spheres. From (3.3) any open set G can be written as a union of elements of VΩ, say
S
i Vi. Given ϵ, choose an M such that
m(
M
[
i=1
Vi) > m(G) −ϵ.
Then
m(G) −ϵ < m(
M
[
i=1
Vi) = lim
n→∞mn(
M
[
i=1
Vi) ≤lim inf
n→∞mn(G).
Thus we have proved that convergence of measures under dG implies that
lim inf
n→∞mn(G) ≥m(G); all open G.
Since the complement of a closed set is open, this also implies that
lim sup
n→∞mn(G) ≤m(G); all closed G.
Along with the preceding limiting properties for open and closed sets, convergence under dG has
strong implications for continuous functions. Recall from Section 3.1 that a function f : Ω→ℜis
continuous if given ω ∈Ωand ϵ > 0 there is a δ such that d(ω, ω′) < δ ensures that |f(ω)−f(ω′)|
< ϵ. We consider two such results, one in the form we will later need and one to relate our notion
of convergence to that of weak convergence of measures.
Lemma 8.2.4 If f is a nonnegative continuous function and (Ω, B) and dG are as previously, then
dG(mn, m) →0 implies that
lim sup
n→∞Emnf ≤Emf.
Proof: For any n we can divide up [0, ∞) into the countable collection of intervals [k/n, (k + 1)/n),
k = 0, 1, 2, . . . that partition the nonnegative real line. Deﬁne the sets Gk(n) = {r : k/n ≤f(r) <
(k + 1)/n} and deﬁne the closed sets Fk(n) = {r : k/n ≤f(r)}. Observe that Gk(n) = Fk(n) −
Fk+1(n). Since f is nonnegativefor any n
∞
X
k=0
k
nm(Gk(n)) ≤Emf
∞
X
k=0
k + 1
n
m(Gk(n)) =
∞
X
k=0
k
nm(Gk(n)) + 1
n.
The sum can be written as
1
n
∞
X
k=0
k (m(Fk(n)) −m(Fk+1(n)) = 1
n
∞
X
k=0
m(Fk(n)),
and therefore
1
n
∞
X
k=0
m(Fk(n)) ≤Emf.

8.2. A METRIC SPACE OF MEASURES
179
By a similar construction
Emnf < 1
n
∞
X
k=0
mn(Fk(n)) + 1
n
and hence from the property for closed sets
lim sup
n→∞Emnf ≤1
n
∞
X
k=0
lim sup
n→∞mn(Fk(n)) + 1
n
≤1
n
∞
X
k=0
m(Fk(n)) + 1
n ≤Emf + 1
n.
Since this is true for all n, the lemma is proved.
2
Corollary 8.2.1 Given (Ω, B) and dG as in the lemma, suppose that f is a bounded, continuous
function. Then dG(mn, m) →0 implies that
lim
n→∞Emnf = Emf.
Proof: If f is bounded we can ﬁnd a ﬁnite constant c such that g = f +c ≥0. g is clearly continuous
so we can apply the lemma to g to obtain
lim sup
n→∞Emng ≤Emg = c + Emf.
Since the left-hand side is c + lim supn→∞Emnf,
lim sup
n→∞Emnf ≤Emf,
as we found with positive functions. Now, however, we can apply the same result to −f (which is
also bounded) to obtain
lim inf
n→∞Emnf ≥Emf,
which together with the limit supremum result proves the corollary.
2
Given a space of measures on a common metric space (Ω, B), a sequence of measures mn is said
to converge weakly to m if
lim
n→∞Emnf = Emf
for all bounded continuous functions f. We have thus proved the following:
Corollary 8.2.2 Given the assumptions of Lemma 8.2.4, if dG(mn, m) →0, then mn converges
weakly to m.
We will not make much use of the notion of weak convergence, but the preceding result is included
to relate the convergence that we use to weak convergence for the special case of Polish alphabets.
For a more detailed study of weak convergence of measures, the reader is referred to Billingsley
[3, 4].

180
CHAPTER 8. PROCESS METRICS AND THE ERGODIC DECOMPOSITION
8.3
The Rho-Bar Distance
In the previous section we considered a distance measure on probability measures. In this section we
introduce a very diﬀerent distance measure, which can be viewed as a metric on random variables
and random processes. Unlike the distributional distances that force probabilities to be close, this
distance will force the average distance between the random variables in the sense of an underlying
metric to be close. Suppose now that we have two random variables X and Y with distributions PX
and PY with a common standard alphabet A. Let ρ be a pseudo-metric or a metric on A. Deﬁne
the ρ-distance between the random variables X and Y by
ρ(PX, PY ) = inf
p∈P Epρ(X, Y ),
Where P = P(PX, PY ) is the collection of all measures on (A × A, BA × BA) with PX and PY as
marginals; that is,
p(A × F) = PY (F); F ∈BA,
and
p(G × A) = PX(G); G ∈BA.
Note that P is not empty since, for example, it contains the product measure PX × PY .
Levenshtein [43] and Vasershtein [70] studied this quantity for the special case where A is the
real line and ρ is the Euclidean distance. Vallender [69] provided several speciﬁc evaluations of the
distance in this case. Ornstein [50] developed the distance and many of its properties for the special
case where A is a discrete space and ρ is the Hamming distance. In this case the ρ-distance is
called the d-bar-distance. Roland Dobrushin [15] has suggested that because of the common suﬃx
in the names of its originators this distance between distributions should be called the shtein or
stein distance.
The ρ-distance can be extended to vectors and processes in a natural way. Suppose now that
{Xn} is a process with process distribution mX and that {Yn} is a process with process distribution
mY . Let PXn and PY n denote the induced ﬁnite dimensional distributions. If ρ1 is a metric on the
coordinate alphabet A, then let ρn be the induced metric on the product space An deﬁned by
ρn(xn, yn) =
n−1
X
i=0
ρ1(xi, yi).
Let ρn denote the corresponding ρ distance between the n dimensional distributions describing the
random vectors. We deﬁne the ρ-distance between the processes by
ρ(mX, mY ) = sup
n
1
nρn(PXn, PY n),
that is, the ρ-distance between two processes is the maximum of the ρ-distances per symbol between
n-tuples drawn from the process. The ρ-distance for processes was introduced by Ornstein for the
special case of the Hamming distance [51] [50] and was extended to general distances by Gray,
Neuhoﬀ, and Shields [27]. The results of this section largely follow these two references.
The following theorem contains the fundamental properties of this process metric.
Theorem 8.3.1 Suppose that mX and mY are stationary process distributions with a common
standard alphabet A and that ρ1 is a pseudo-metric on A and that ρn is deﬁned on An in an additive
fashion as before. Then

8.3. THE RHO-BAR DISTANCE
181
(a) limn→∞n−1ρn(PXn, PY n) exists and equals supn n−1ρn(PXn, PY n).
(b) ρn and ρ are pseudo-metrics. If ρ1 is a metric, then ρn and ρ are metrics.
(c) If mX and mY are both IID, then ρ(mX, mY ) = ρ1(PX0, PY0).
(d) Let Ps = Ps(mX, mY ) denote the collection of all stationary distributions pXY having mX and
mY as marginals, that is, distributions on {Xn, Yn} with coordinate processes {Xn} and {Yn}
having the given distributions. Deﬁne the process distance measure ρ′
ρ′(mX, mY ) =
inf
pXY ∈Ps EpXY ρ(X0, Y0).
Then
ρ(mX, mY ) = ρ′(mX, mY );
that is, the limit of the ﬁnite dimensional minimizations is given by a minimization over
stationary processes.
(e) Suppose that mX and mY are both stationary and ergodic. Deﬁne Pe = Pe(mX, mY ) as the
subset of Ps containing only ergodic processes, then
ρ(mX, mY ) =
inf
pXY ∈Pe EpXY ρ(X0, Y0),
(f) Suppose that mX and mY are both stationary and ergodic. Let GX denote a collection of generic
sequences for mX in the sense of Section 7.4. Recall that by measuring relative frequencies on
generic sequences one can deduce the underlying stationary and ergodic measure that produced
the sequence. Similarly let GY denote a set of generic sequences for mY . Deﬁne the process
distance measure
ρ′′(mX, mY ) =
inf
x∈GX,y∈GY lim sup
n→∞
1
n
n−1
X
i=0
ρ1(x0, y0).
Then
ρ(mX, mY ) = ρ′′(mX, mY ).
that is, the ρ distance gives the minimum long term time average distance obtainable between
generic sequences from the two processes.
(g) The inﬁma deﬁning ρn and ρ′ are actually minima.
Proof: (a) Suppose that pN is a joint distribution on (AN × AN, BN
A × BN
A ) describing (XN, Y N)
that approximately achieves ρN, e.g., pN has PXN and PY N as marginals and for ϵ > 0
EpN ρ(XN, Y N) ≤ρN(PXN , PY N ) + ϵ.
For any n < N let pn be the induced distribution for (Xn, Y n) and pN−n
n
that for (XN−n
n
, Y N−n
n
).
Then since the processes are stationary pn ∈Pn and pN−n
n
∈PN−n and hence
EpN ρ(XN, Y N)
=
Epnρ(Xn, Y n) + EpN−n
n
ρ(XN−n
n
, Y N−n
n
)
≥
ρn(PXn, PY n) + ρN−n(PXN−n, PY N−n).
Since ϵ is arbitrary we have shown that if an = ρn(PXn, PY n), then for all N > n
aN ≥an + aN−n;

182
CHAPTER 8. PROCESS METRICS AND THE ERGODIC DECOMPOSITION
that is, the sequence an is superadditive. It then follows from Lemma 7.5.2 (since the negative of a
superadditive sequence is a subadditive sequence) that
lim
n→∞
an
n = sup
n≥1
an
n ,
which proves (a).
(b) Since ρ1 is a pseudo-metric, ρn is also a pseudo-metric and hence ρn is nonnegative and
symmetric in its arguments. Thus ρ is also nonnegative and symmetric in its arguments. Thus we
need only prove that the triangle inequality holds. Suppose that mX, mY , and mZ are three process
distributions on the same sequence space. Suppose that p approximately yields ρn(PXn, PZn) and
that r approximately yields ρn(PZn, PY n). These two distributions can be used to construct a joint
distribution q for the triple (Xn, Zn, Y n) such that Xn →Zn →Y n is a Markov chain and such
that the coordinate vectors have the correct distribution. In particular, r and mn
Z together imply
a conditional distribution pY n|Zn and p and PXn together imply a conditional distribution pZn|Xn.
Since the spaces are standard these can be chosen to be regular conditional probabilities and hence
the distribution speciﬁed by its values on rectangles as a cascade of ﬁnite dimensional channels: If
we deﬁne νx(F) = PZn|Xn(F|xn), then
q(F × G × D) =
Z
D
dPXn(xn)
Z
G
dνx(zn)PY n|Zn(F|zn).
For this distribution, since ρn is a pseudo-metric,
ρn(PXn, PY n)
≤
Eρn(Xn, Y n)
≤
Eρn(Xn, Zn) + Eρn(Zn, Y n)
≤
ρn(PXn, PZn) + ρn(PZn, PY n) + 2ϵ,
which proves the triangle inequality for ρn. Taking the limit as n →∞proves the result for ρ. If
ρ1 is a metric, then so is ρn. Suppose now that ρ = 0 and hence also ρn = 0 for all n. This implies
that there exists a pn ∈Pn such that pn(xn, yn : xn ̸= yn) = 0 since ρn(xn, yn) is 0 only if xn = yn.
This is only possible, however, if PXn(F) = PY n(F) for all events F, that is, the two marginals are
identical. Since this is true for all N, the two processes must be the same. (c) Suppose that p1
approximately yields ρ1, that is, has the correct marginals and
Ep1ρ1(X0, Y0) ≤ρ1(PX0, PY0) + ϵ.
Then for any n the product measure pn deﬁned by
pn = ×n−1
i=0 p1
has the correct vector marginals for Xn and Y n since they are IID, and hence pn ∈Pn, and therefore
ρn(PXn, PY n)
≤
Epnρn(Xn, Y n)
=
nEp1ρ(X0, Y0)
≤
n(ρ1(PX0, PY0) + ϵ).
Since by deﬁnition ρ is the supremum of n−1ρn, this proves (c).
(d) Given ϵ > 0 let p ∈Ps be such that Epρ1(X0, Y0) ≤ρ′(mX, mY )+ϵ. The induced distribution
on {Xn, Y n} is then contained in Pn, and hence using the stationarity of the processes
ρn(PXn, PY n) ≤Eρn(Xn, Y n) = nEρ1(X0, Y0) ≤nρ′(mX, mY ),

8.3. THE RHO-BAR DISTANCE
183
and therefore ρ′ ≥ρ since ϵ is arbitrary.
Let pn ∈Pn, n = 1, 2, . . . be a sequence of measures such that
Epn[ρn(Xn, Y n)] ≤ρn + ϵn
where ϵn →0 as n →∞. Let qn denote the product measure on the sequence space induced by the
pn, that is, for any N and N-dimensional rectangle F = ×i∈IFi with all but a ﬁnite number N of
the Fi being A × B,
q(F) =
Y
j∈I
pn(×(j+1)n−1
i=jn
Fi);
that is, qn is the pair process distribution obtained by gluing together independent copies of pn.
This measure is obviously N-stationary and we form a stationary process distribution pn by deﬁning
pn(F) = 1
n
n−1
X
i=0
qn(T −iF)
for all events F. If we now consider the metric space of all process distributions on (AI × AI,
BI
A × BI
A) of Section 8.2 with a metric dG based on a countable generating ﬁeld of rectangles (which
exists since the spaces are standard), then from Lemma 8.2.2 the space is sequentially compact
and hence there is a subsequence pnk that converges in the sense that there is a measure p and
pnk(F) →p(F) for all F in the generating ﬁeld. Assuming the generating ﬁeld to contain all shifts
of ﬁeld members, the stationarity of the pn implies that p(T −1F) = p(F) for all rectangles F in the
generating ﬁeld and hence p is stationary. A straightforward computation shows that for all n p
induces marginal distributions on Xn and Y n of PXn and PY n and hence p ∈Ps. In addition,
Epρ1(X0, Y0) = lim
k→∞Epnk ρ(X0, Y0) = lim
k→∞n−1
k
nk−1
X
i=0
Eqnk ρ(Xi, Yi) = lim
k→∞(ρnk + ϵnk) = ρ,
proving that ρ′ ≤ρ and hence that they are equal. (e) If p ∈Ps nearly achieves ρ′, then from the
ergodic decomposition it can be expressed as a mixture of stationary and ergodic processes pλ and
that
Epρ1 =
Z
dW(λ)Epλρ1.
All of the pλ must have mX and mY as marginal process distributions (since they are ergodic) and at
least one value of λ must yield an Epλρ1 no greater than the preceding average. Thus the inﬁmum
over stationary and ergodic pair processes can be no worse than that over stationary processes.
(Obviously it can be no better.)
(f) Suppose that x ∈GX and y ∈GY . Fix a dimension n and deﬁne for each N and each
F ∈BN
A , G ∈BN
A
µN(F × G) = 1
N
N−1
X
i=0
1F ×G(T i(x, y)).
µN extends to a measure on (AN × AN, BN
A × BN
A ). Since the sequences are assumed to be generic,
µN(F × AI) →n→∞PXn(F),
µN(AI × G) →n→∞PY n(G),

184
CHAPTER 8. PROCESS METRICS AND THE ERGODIC DECOMPOSITION
for all events F and G and generating ﬁelds for BN
A and BN
A , respectively. From Lemma 8.2.2 (as
used previously) there is a convergent subsequence of the µN. If µ is such a limit point, then for
generating F, G
µ(F × AI) = PXn(F),
µ(AI × G) = PY n(G),
which implies that the same relation must hold for all F and G (since the measures agree on
generating events). Thus µ ∈Pn. Since µN is just a sample distribution,
EµN [ρn(Xn, Y n)] = 1
N
N−1
X
j=0
ρn(xn
j , yn
j ) = 1
N
N−1
X
j=0
[
n−1
X
i=0
ρ1(xi+j, yi+j)].
Thus
1
nEµ[ρn(Xn, Y n)] ≤lim sup
N→∞
1
N
N−1
X
i=0
ρ1(xi, yi)
and hence ρ ≤ρ′.
Choose for ϵ > 0 a stationary measure p ∈Ps such that
Epρ1(X0, Y0) = ρ′ + ϵ.
Since p is stationary,
ρ∞(x, y) = lim
n→∞
1
n
n−1
X
i=0
ρ1(xi, yi)
exists with probability one. Furthermore, since the coordinate processes are stationary and ergodic,
with probability one x and y must be generic. Thus
ρ∞(x, y) ≥ρ′′.
Taking the expectation with respect to p and invoking the ergodic theorem,
ρ′′ ≤Epρ∞= Epρ1(X0, Y0) ≤ρ′ + ϵ.
Hence ρ′ ≤ρ′ = ρ and hence ρ′ = ρ. (g) Consider ρ′. Choose a sequence pn ∈Ps such that
Epnρ1(X0, Y0) ≤ρ′(mX, mY ) + 1/n.
From Lemma 8.2.2 pn must have a convergent subsequence with limit, say, p. p must be stationary
and have the correct marginal processes (since all of the pn are) and from the preceding equation
Epρ1(X0, Y0) ≤ρ′(mX, mY ).
A similar argument works for ρn.

8.3. THE RHO-BAR DISTANCE
185
The Prohorov Distance
Another distance measure on random variables that is often encountered in the literature is the
Prohorov distance. This distance measure resembles the ρ-distance for random variables and has
many applications in probability theory, ergodic theory, and statistics. (See, for example, Dudley
[19], Billingsley [3] [4], Moser, et al. [48], Strassen [67], and Hample [33].) The Prohorov distance
unfortunately does not extend in a useful way to provide a distance measure between processes and
it does not ensure the generic sequence closeness properties that the ρ distortion possesses. We
present a brief description, however, for completeness and to provide the relation between the two
distance measures on random vectors. The development follows [54].
Analogous to the deﬁnition of the nth order ρ distance, we deﬁne for positive integer n the nth
order Prohorov distance Πn = Πn(PXn, PY n) between two distributions for random vectors Xn and
Y n with a common standard alphabet An and with a pseudo-metric or metric ρn deﬁned on An by
Πn = inf
p∈P inf{γ : p({xn, yn : 1
nρn(xn, yn) > γ}) ≤γ}
where P is as in the deﬁnition of ρn, that is, it is the collection of all joint distributions PXn,Y n
having the given marginal distributions PXn and PY n. It is known from Strassen [67] and Dudley
[19] that the inﬁmum is actually a minimum. It can be proved in a manner similar to that for the
corresponding ρn result.
Lemma 8.3.1 The ρ and Prohorov distances satisfy the following inequalities:
Πn(PXn, PY n)2 ≤1
nρn(PXn, PY n) ≤ρ(mX, mY ).
(8.5)
If ρ1 is bounded, say by ρmax, then
1
nρn(PXn, PY n) ≤Πn(PXn, PY n)(1 + ρmax).
(8.6)
If there exists an a∗such that
EPX0(ρ1(X0, a∗)2) ≤ρ∗< ∞,
EPY0 (ρ1(Y0, a∗)2) ≤ρ∗< ∞,
(8.7)
then
1
nρn(PXn, PY n) ≤Πn(PXn, PY n) + 2(ρ∗Πn(PXn, PY n))1/2.
(8.8)
Comment: The Lemma shows that the ρ distance is stronger than the Prohorov in the sense that
making it small also makes the Prohorov distance small. If the underlying metric either is bounded
or has a reference letter in the sense of (8.7), then the two distances provide the same topology on
nth order distributions.
Proof: From Theorem 8.3.1 the inﬁmum deﬁning the ρn distance is actually a minimum. Suppose
that p achieves this minimum; that is,
Epρn(Xn, Y n) = ρn(PXn, PY n).
For simplicity we shall drop the arguments of the distance measures and just refer to Πn and ρn.
From the Markov inequality
p({xn, yn : 1
nρn(xn, yn) > ϵ}) ≤Ep 1
nρn(Xn, Y n)
ϵ
=
1
nρn
ϵ
.

186
CHAPTER 8. PROCESS METRICS AND THE ERGODIC DECOMPOSITION
Setting ϵ = (n−1ρn)1/2 then yields
p({xn, yn : 1
nρn(xn, yn) >
p
n−1ρn}) ≤
p
n−1ρn,
proving the ﬁrst part of the ﬁrst inequality. The fact that ρ is the supremum of ρn completes the
ﬁrst inequality.
If ρ1 is bounded, let p yield Πn and write
1
nρn
≤
Ep
1
nρn(Xn, Y n)
≤
Πn + p({xn, yn : 1
nρn(xn, yn) > Πn})ρmax
=
Πn(1 + ρmax).
Next consider the case where (8.7) holds. Again let p yield Πn, and write, analogously to the
previous equations,
1
nρn ≤Πn +
Z
G
dp(xn, yn) 1
nρn(xn, yn)
where
G = {xn, yn : n−1ρn(xn, yn) > Πn}.
Let 1G be the indicator of G and let E denote expectation with respect to p. Since ρn is a pseudo-
metric, the triangle inequality and the Cauchy-Schwartz inequality yield
1
nρn
≤
1
nEρn(Xn, Y n)
≤
Πn + 1
nE(ρn(Xn, a∗n)1G) + E( 1
nρn(Y n, a∗n)1G)
≤
Πn + [E( 1
nρn(Xn, a∗n))2]1/2[E(12
G)]1/2 + [E( 1
nρn(Y n, a∗n))2]1/2[E(12
G)]1/2.
Applying the Cauchy-Schwartz inequality for sums and invoking (8.7) yield
E( 1
nρn(Xn, a∗n))2 = E
Ã
( 1
n
n−1
X
i=0
ρ1(Xi, a∗))2
!
≤E( 1
n
n−1
X
i=0
ρ1(Xi, a∗)2) ≤ρ∗.
Thus
1
nρn ≤2(ρ∗)1/2(p(G))1/2 = Πn + 2(ρ∗)1/2Π1/2
n ,
completing the proof.
2
8.4
Measures on Measures
We next consider placing measures on the space of measures. To accomplish this we focus on a
distributional distance for a particular G and hence on a particular metric for generating the Borel
ﬁeld of measures. We require that (Ω, B) be standard and take G to be a standard generating ﬁeld.
We will denote the resulting metric simply by d. Thus P((Ω, B)) will itself be Polish from Lemma
8.2.1. We will usually wish to assign nonzero probability only to a nice subset of this space, e.g.,
the subset of all stationary measures. Henceforth let ˆP be a closed and convex subset of P((Ω, B)).

8.5. THE ERGODIC DECOMPOSITION REVISITED
187
Since ˆP is a closed subset of a Polish space, it is also Polish under the same metric. Let ˆB = B( ˆP, d)
denote the Borel ﬁeld of subsets of ˆP with respect to the metric d. Since ˆP is Polish, this measurable
space ( ˆP, ˆB) is standard.
Now consider a probability measure W on the standard space ( ˆP, ˆB). This probability measure
thus is a member of the class P(( ˆP, ˆB)) of all probability measures on the standard measurable space
( ˆP, ˆB), where ˆP is a closed convex subset of the collection of all probability measures on the original
standard measurable space (Ω, B). (The author promises at this point not to put measures on the
space P(( ˆP, ˆB)). Spaces of measures on spaces of measures are complicated enough!)
We now argue that any such W induces a new measure m ∈ˆP via the mixture relation
m =
Z
αdW(α).
(8.9)
To be precise and to make sense of the preceding formula, consider for any ﬁxed F ∈G and any
measure α ∈ˆP the expectation Eα1F . We can consider this as a function (or functional) mapping
probability measures in ˆP into ℜ, the real line. This function is continuous, however, since given
ϵ > 0, if F = Fi we can choose d(α, m) < ϵ2−i and ﬁnd that
|Eα1F −Em1F | = |α(F) −m(F)| ≤2i
∞
X
j=1
2−j|α(Fj) −m(Fj)| ≤ϵ.
Since the mapping is continuous, it is measurable from Lemma 3.1.6. Since it is also bounded, the
following integrals all exist:
m(F) =
Z
Eα1F dW(α); F ∈G.
(8.10)
This deﬁnes a set function on G that is nonnegative, normalized, and ﬁnitely additive from the
usual properties of integration. Since the space (Ω, B) is standard, this set function extends to a
unique probability measure on (Ω, B), which we also denote by m. This provides a mapping from
any measure W ∈P( ˆP, ˆB) to a unique measure m ∈ˆP ⊂P((Ω, B)). This mapping is denoted
symbolically by (8.9) and is deﬁned by (8.10). We will use the notation W →m to denote it. The
induced measure m is called the barycenter of W.
Before developing the properties of barycenters, we return to the ergodic decomposition to show
how it ﬁts into this framework.
8.5
The Ergodic Decomposition Revisited
Say we have an stationary dynamical system (Ω, B, m, T) with a standard measurable space. The
ergodic decomposition then implies a mapping ψ : Ω, →P((Ω, B)) deﬁned by x →px as given in the
ergodic decomposition theorem. In fact, this mapping is into the subset of ergodic and stationary
measures.
In this section we show that this mapping is measurable, relate integrals in the two
spaces, and determine the barycenter of the induced measure on measures. In the process we obtain
an alternative statement of the ergodic decomposition.
Lemma 8.5.1 Given a stationary system (Ω, B, m, T) with a standard measurable space, let P =
P(Ω, B) be the space of all probability measures on (Ω, B) with the topology of the previous section.
Then the mapping ψ : Ω→P deﬁned by the mapping x →px of the ergodic decomposition is a
measurable mapping. (All generic points x are mapped into the corresponding ergodic component;
all remaining points are mapped into a common arbitrary stationary ergodic process.)

188
CHAPTER 8. PROCESS METRICS AND THE ERGODIC DECOMPOSITION
Proof: To prove ψ measurable, it suﬃces to show that any set of the form V = {β : d(α, β) < r}
has an inverse image ψ−1(V ) ∈B. This inverse image is given by
{x :
∞
X
i=1
2−i|px(Fi) −α(Fi)| < r}.
Deﬁning the function γ : Ω→ℜby
γ(x) =
∞
X
i=1
2−i|px(Fi) −α(Fi)|,
then the preceding set can also be expressed as γ−1((−∞, r)). The function γ, however, is a sum of
measurable real-valued functions and hence is itself measurable, and hence this set is in B.
Given the ergodic decomposition {px} and a stationary measure m, deﬁne the measure W =
mψ−1 on ( ˆP, ˆB). Given any measurable function f : ˆP →ℜ, from the measurability of ψ and the
chain rule of integration
Z
f(α)dW(α)
=
Z
f(α)dmψ−1(α)
=
Z
f(ψx) dm(x) =
Z
f(px) dm(x).
Thus the rather abstract looking integrals on the space of measures equal the desired form suggested
by the ergodic decomposition, that is, one integral exists if and only if the other does, in which case
they are equal. This demonstrates that the existence of certain integrals in the more abstract space
implies their existence in the more concrete space.
The more abstract space, however, is more
convenient for developing the desired results because of its topological structure. In particular, the
ergodic decomposition does not provide all possible probability measures on the original probability
space, only the subset of ergodic and stationary measures. The more general space is required by
some of the approximation arguments. For example, one cannot use the previous separability lemma
to approximate arbitrary ergodic processes by point masses on ergodic processes since a mixture of
distinct ergodic processes is not ergodic!
Letting f(α) = Eα1F = α(F) for F ∈G we see immediately that
Z
Eα1F dmψ−1(α) =
Z
px(F) dm(x),
and hence since the right side is
m(F) =
Z
Eα1F dmψ−1(α);
that is, m is the barycenter of the measure induced on the space of all measures by the original
stationary measure m and the mapping deﬁned by the ergodic decomposition.
Since the ergodic decomposition puts probability one on stationary and ergodic measures, we
should be able to cut down the measure W to these sets and hence conﬁne interest to functionals of
stationary measures. In order to do this we need only show that these sets are measurable, that is,
in the Borel ﬁeld of all probability measures on (Ω, B). This is accomplished next.
Lemma 8.5.2 Given the assumptions of the previous lemma, assume also that G is chosen to include
a countable generating ﬁeld and that G is chosen so that F ∈G implies T −1F ∈G. (If this is not true,

8.5. THE ERGODIC DECOMPOSITION REVISITED
189
simply expand G to S
i T −iG. For example, G could be a countable generating family of cylinders.)
Let Ps and Pe denote the subsets of ˆP consisting respectively of all stationary measures and all
stationary and ergodic measures.
Then Ps is closed (and hence measurable) and convex.
Pe is
measurable.
Proof: Let αn ∈Ps be a sequence of stationary measures converging to a measure α and hence for
every F ∈G αn(F) →α(F). Since T −1F ∈G this means that αn(T −1F) →α(T −1F). Since the
αn are stationary, however, this sequence is the same as αn(F) and hence converges to α(F). Thus
α(T −1F) = α(F) on a generating ﬁeld and hence α is stationary. Thus Ps is a closed subset under
d and hence must be in the Borel ﬁeld.
From Lemma 6.7.4, an α ∈Ps is also in Pe if and only if
α ∈
\
F ∈G
G(F),
where here
G(F) = {m : lim
n→∞n−1
n−1
X
i=0
m(T −iF ∩F) = m(F)2} = {m : lim
n→∞n−1
n−1
X
i=0
Em1T −iF ∩F = (Em1F )2}.
The Em1F terms are all measurable functions of m since the sets are all in the generating ﬁeld.
Hence the collection of m for which the limit exists and has the given value is also measurable. Since
the G(F) are all in ˆB, so is their countable intersection.
Thus we can take ˆP to be Ps and observe that for any AMS measure W(Ps) = m(x : px ∈Ps) = 1
from the ergodic decomposition.
2
The following theorem uses the previous results to provide an alternative and intuitive statement
of the ergodic decomposition theorem.
Theorem 8.5.1 Fix a standard measurable space (Ω, B) and a transformation T : Ω→Ω. Then
there are a standard measurable space (Λ, L), a family of stationary ergodic measures {mλ; λ ∈Λ}
on (Ω, B), and a mapping ψ : Ω→Λ such that
(a) ψ is measurable and invariant, and
(b) if m is a stationary measure on (Ω, B) and Wψ is the induced distribution; that is, Wψ(G) =
m(ψ−1(G)) for G ∈Λ (which is well deﬁned from (a)), then
m(F) =
Z
dm(x)mψ(x)(F) =
Z
dWψ(λ)mλ(F), all F ∈B,
and if f ∈L1(m), then so is
R
f dmλ mψ-a.e. and
Emf =
Z
dWψ(λ)Emλf.
Finally, for any event F mψ(F) = m(F|ψ), that is, given the ergodic decomposition and a
stationary measure m , the ergodic component λ is a version of the conditional probability
under m given ψ = λ.
Proof: Let P denote the space of all probability measures described in Lemma 8.2.2 along with the
Borel sets generated by using the given distance and a generating ﬁeld. Then from Lemma 8.2.1

190
CHAPTER 8. PROCESS METRICS AND THE ERGODIC DECOMPOSITION
the space is Polish. Let Λ = Pe denote the subset of P consisting of all stationary and ergodic
measures. From Lemma 8.2.1 this set is measurable and hence is a Borel set. Theorem 3.3.2 then
implies that Λ together with its Borel sets is standard. Deﬁne the mapping ψ : Ω→P as in Lemma
8.5.1. From Lemma 8.5.1 the mapping is measurable. Since ψ maps points in Ωinto stationary and
ergodic processes, it can be considered as a mapping ψ : Ω→Λ that is also measurable since it is
the restriction of a measurable mapping to a measurable set. Simply deﬁne mλ = λ (since each λ is
a stationary ergodic process). The formulas for the probabilities and expectations then follow from
the ergodic decomposition theorem and change of variable formulas. The conditional probability
m(F|ψ = λ) is deﬁned by the relation
m(F ∩{ψ ∈G}) =
Z
G
m(F|ψ = λ)dWψ(λ),
for all G ∈L. Since ψ is invariant, σ(ψ) is contained in the σ-ﬁeld of all invariant events. Combining
the ergodic decomposition theorem and the ergodic theorem we have that Epxf = Em(f|I), where
I is the σ-ﬁeld of invariant events.
(This follows since both equal lim n−1sumifT i.)
Thus the
left-hand side is
m(F ∩ψ−1(G)) =
Z
ψ−1(G)
px(F) dm(x)
Z
ψ−1(G)
mψ(x)(F)dm(x) =
Z
G
mλ(F)dWψ(λ),
where the ﬁrst equality follows using the conditional expectation of f = 1F , and the second follows
from a change of variables.
The theorem can be viewed as simply a reindexing of the ergodic components and a description
of a stationary measure as a weighted average of the ergodic components where the weighting is a
measure on the indices and not on the original points. In addition, one can view the function ψ as
a special random variable deﬁned on the system or process that tells which ergodic component is
in eﬀect and hence view the ergodic component as the conditional probability with respect to the
stationary measure given the ergodic component in eﬀect. For this reason we will occasionally refer
to the ψ of Theorem 8.5.1 as the ergodic component function.
8.6
The Ergodic Decomposition of Markov Processes
The ergodic decomposition has some rather surprising properties for certain special kinds of pro-
cesses. Suppose we have a process {Xn} with standard alphabet and distribution m. For the moment
we allow the process to be either one-sided or two-sided. The process is said to be kth order Markov
or k-step Markov if for all n and all F ∈σ(Xn, . . .)
m((Xn, Xn+1, . . .) ∈F|Xi; i ≤n −1) = m((Xn, Xn+1, . . .) ∈F|Xi; i = n −k, . . . , n −1);
that is, if the probability of a turevent given the entire past depends only on the k most recent
samples.
When k is 0 the process is said to be memoryless.
The following provides a test for
whether or not a process is Markov.
Lemma 8.6.1 A process {Xn} is k-step Markov if and only if the conditional probabilities m((Xn, Xn+1, . . .) ∈
F|Xi; i ≤n−1) are measurable with respect to σ(Xn−k, . . . , Xn−1), i.e., if for all F m((Xn, Xn+1, . . .) ∈
F|Xi; i ≤n −1) depends on Xi; i ≤n −1 only through Xn−k, . . . , Xn−i.

8.6. THE ERGODIC DECOMPOSITION OF MARKOV PROCESSES
191
Proof: By iterated conditional probabilities,
E(m((Xn, Xn+1, . . .) ∈F|Xi; i ≤n −1)|Xn−k, . . . , Xn−1)
=
m((Xn, Xn+1, . . .) ∈F|Xi; i = n −k, . . . , n −1).
Since the spaces are standard, we can treat a conditional expectation as an ordinary expectation,
that is, an expectation with respect to a regular conditional probability.
Thus if the argument
ζ = m((Xn, Xn+1, . . .) ∈F|Xi; i ≤n −1) is measurable with respect to σ(Xn−k, . . . , Xn−1), then
from Corollary 5.9.2 the conditional expectation of ζ is ζ itself and hence the left-hand side equals
the right-hand side, which in turn implies that the process is k-step Markov. This conclusion can
also be drawn by using Lemma 5.9.5.
2
If a k-step Markov process is stationary, then it has an ergodic decomposition. What do the
ergodic components look like? In particular, must they also be Markov? First note that in the case
of k = 0 the answer is trivial: If the process is memoryless and stationary, then it must be IID,
and hence it is strongly mixing and hence it is ergodic. Thus the process puts all of its probability
on a single ergodic component, the process itself. In other words, a stationary memoryless process
produces with probability one an ergodic memoryless process. Markov processes exhibit a similar
behavior: We shall show in this section that a stationary k-step Markov process puts probability
one on a collection of ergodic k-step Markov processes having the same transition or conditional
probabilities. The following lemma will provide an intuitive explanation.
Lemma 8.6.2 Suppose that {Xn} is a two-sided stationary process with distribution m. Let {mλ; λ ∈
Λ} denote the ergodic decomposition for a standard space Λ of Theorem 8.5.1 and let ψ be the er-
godic component function. Then the mapping ψ of Theorem 8.5.1 can be taken to be measurable with
respect to σ(X−1, X−2, . . .). Furthermore,
m((X0, X1, . . .) ∈F|X−1, X−2, . . .)
=
m((X0, X1, . . .) ∈F|ψ, X−1, X−2, . . .)
=
mψ((X0, X1, . . .) ∈F|X−1, X−2, . . .),
m-a.e.
Comment: The lemma states that we can determine the ergodic component of a two-sided process
by looking at the past alone. Since the past determines the ergodic component, the conditional
probability of the future given the past and the component is the same as the conditional probability
given the past alone.
Proof: The ergodic component in eﬀect can be determined by the limiting sample averages of a
generating set of events. The mapping of Theorem 8.5.1 produced a stationary measure ψ(x) = px
speciﬁed by the relative frequencies of a countable number of generating sets computed from x.
Since the ﬁnite-dimensional rectangles generate the complete σ-ﬁeld and since the probabilities
under stationary measures are not aﬀected by shifting, it is enough to determine the probabilities
of all events of the form Xi ∈Fi; i = 0, . . . , k −1 for all one-dimensional generating sets Fi. Since
the process is stationary, the shift T is invertible, and hence for any event F of this type we can
determine its probability from the limiting sample average
lim
n→∞
1
n
n−1
X
i=k
1F (T −ix)
(we require in the deﬁnition of generic sets that both these time averages as well as those with respect
to T converge). Since these relative frequencies determine the ergodic component and depend only

192
CHAPTER 8. PROCESS METRICS AND THE ERGODIC DECOMPOSITION
on x−1, x−2, . . ., ψ has the properties claimed (and still satisﬁes the construction that was used
to prove Theorem 8.6.1, we have simply constrained the sets considered and taken averages with
respect to T −1). Since ψ is measurable with respect to σ(X−1, X−2, . . .), the conditional probability
of a future event given the inﬁnite past is unchanged by also conditioning on ψ. Since conditioning
on ψ speciﬁes the ergodic component in eﬀect, these conditional probabilities are the same as those
computed using the ergodic component. (The ﬁnal statement can be veriﬁed by plugging into the
deﬁning equations for conditional probabilities.)
2
We are now ready to return to k-step Markov processes. If m is two-sided and stationary and
k-step Markov, then from the Markov property and the previous lemma
m((X0, . . .) ∈F|X−1, . . . , X−k)
=
m((X0, . . .) ∈F|X−1, X−2, . . .)
=
mψ((X0, . . .) ∈F|X−1, X−2, . . .).
But this says that mψ((X0, . . .) ∈F|X−1, X−2, . . .) is measurable with respect to σ(X−1, . . . , X−k)
(since it is equal to a function that is), which implies from Lemma 8.6.1 that it is k-step Markov.
We have now proved the following lemma for two-sided processes.
Lemma 8.6.3 If {Xn} is a stationary Markov process and {mλ; λ ∈Λ} is the ergodic decomposi-
tion, and ψ is the ergodic component function, then
mψ((Xn, . . .) ∈F|Xi; i ≤n) = m((Xn, . . .) ∈F|Xn−k, . . . , Xn−1)
almost everywhere. Thus with probability one the ergodic components are also k-step Markov with
the same transition probabilities.
Proof: For two-sided processes the statement follows from the previous argument and the stationarity
of the measures. If the process is one-sided, then we can bedhe process in a two-sided process with
the same behavior.
Given a one-sided process m, let p be the stationary two-sided process for
which p((Xk, . . . , Xk+n−1) ∈F) = m((X0, . . . , Xn−1) ∈F) for all integers k and n dimensional sets
F. This uniquely speciﬁes the two-sided process. Similarly, given any stationary two-sided process
there is a unique one-sided process assigning the same probabilities to ﬁnite-dimensional events. It
is easily seen that the ergodic decomposition of the one-sided process yields that for the two-sided
process. The one-sided process is Markov if and only if the two-sided process is and the transition
probabilities are the same. Application of the two-sided result to the induced two-sided process then
implies the one-sided result.
2
8.7
Barycenters
Barycenters have some useful linearity and continuity properties.
These are the subject of this
section.
To begin recall from Section 8.3 and (8.9) that the barycentric mapping is a mapping of a measure
W ∈P( ˆP, ˆB), the space of probability measures on the space ˆP, into a measure m ∈ˆP, that is, m
is a probability measure on the original space (Ω, B). The mapping is denoted by W →m or by
m =
R
α dW(α).
First observe that if W1 →m1 and W2 →m2 and if λ ∈(0, 1), then
λW1 + (1 −λ)W2 →λm1 + (1 −λ)m2;
that is, the mapping W →m of a measure into its barycenter is aﬃne. The following lemma provides
a countable version of this property.

8.7. BARYCENTERS
193
Lemma 8.7.1 If Wi →mi, i = 1, 2, . . ., and if
∞
X
i=1
ai = 1
for a nonnegative sequence {ai}, then
∞
X
i=1
aiWi →
∞
X
i=1
aimi.
Comment: The given countable mixtures of probability measures are easily seen to be themselves
probability measures. As an example of the construction, suppose that W is described by a probabil-
ity mass function, that is, W places a probability ai on a probability measure αi ∈ˆP for each i. We
can then take the Wi of the lemma as the point masses placing probability 1 on αi. Then Wi →αi
and the mixture W of these point masses has the corresponding mixture P αi as barycenter.
Proof: Call the weighted average of the Wi W and the weighted average of the mi m. The assump-
tions imply that for all i
mi(F) =
Z
Eα1F dWi(α); F ∈G.
(That Eα1F is measurable was discussed prior to (8.10).) Thus to prove the lemma we must prove
that
∞
X
i=1
ai
Z
Eα1F dWi(α) =
Z
Eα1F dW(α).
for all F ∈G. We have by construction that for any indicator function of an event D ∈ˆB that
∞
X
i=1
ai
Z
1D(α)dWi(α) =
Z
1DdW.
Via the usual integration arguments this implies the corresponding equality for simple functions and
then for bounded nonnegative measurable functions and hence for the Eα1F .
2
The next lemma shows that if a measure W puts probability 1 on a closed ball in ˆP, then the
barycenter must be inside the ball.
Lemma 8.7.2 Given a closed ball S = {α : d(α, β) ≤r} in P((Ω, B)) and a measure W such that
W(S) = 1, then W →m implies that m ∈S.
Proof: We have
d(m, β)
=
∞
X
i=1
2−i|m(Fi) −β(Fi)|
=
∞
X
i=1
2−i|
Z
S
α(Fi)dW(α) −β(Fi)|
≤
∞
X
i=1
2−i
Z
S
|α(Fi) −β(Fi)| dW(α)
=
Z
S
d(α, β) dW(α) ≤r,

194
CHAPTER 8. PROCESS METRICS AND THE ERGODIC DECOMPOSITION
where the integral is pulled out of the sum via the dominated convergence theorem. This completes
the proof.
2
The ﬁnal result of this section provides two continuity properties of the barycentric mapping.
Lemma 8.7.3 (a) Assume as earlier that the original measurable space (Ω, B) is standard. Form a
metric space from the subset ˆP of all measures on (Ω, B) using a distributional metric d = dfatW with
fatW a generating ﬁeld for B. If ˆB is the resulting Borel ﬁeld, then the Borel space ( ˆP, ˆB) is also
standard. Given these deﬁnitions, the barycentric mapping W →m is continuous when considered
as a mapping from the metric space (P( ˆP, ˆB), dG) to the metric space ( ˆP, d), where dG is the metric
based on the countable collection of sets G = F ∪S, where F is a countable generating ﬁeld of the
standard sigma-ﬁeld ˆB of subsets of ˆP and where S is the collection of all sets of the form
{α : α(F) = Eα1F ∈[k
n, k + 1
n
)}
for all F in the countable ﬁeld that generates B and all n = 1, 2, . . ., and k = 0, 1, . . . , n −1. (These
sets are all in ˆB since the functions deﬁning them are measurable.)
(b) Given the preceding and a nonnegative bounded measurable function D : ˆP →ℜmapping
probability measures into the real line, deﬁne the class H of subsets P as all sets of the form
{α : D(α)
Dmax
∈[k
n, k + 1
n
)}
where Dmax is the maximum absolute value of D, n = 1, 2, . . ., and k = 0, + −1, . . . , + −n −1. Let
dG denote the metric on P(( ˆP, ˆB)) induced by the class of sets G = F ∪H, with F and H as before.
Then
R
D(α)dW(α) is continuous with respect to dG; that is, if dG(Wn, W) →n→∞0, then also
lim
n→∞
Z
D(α) dWn(α) =
Z
D(α) dW(α).
Comment: We are simply adding enough sets to the metric to make sure that the various integrals
have the required limits.
Proof: (a) Deﬁne the uniform quantizer
qn(r) = k
n; r ∈[k
n, k + 1
n
).
Assume that dG(WN, W) →0 and consider the integral
|mN(F) −m(F)|
=
|
Z
Eα1F dWN(α) −
Z
Eα1F dW(α)|
≤
|
Z
Eα1F dWN(α) −
Z
qn(Eα1F ) dWN(α)| + |
Z
qn(Eα1F ) dWN(α)
−
Z
qn(Eα1F ) dW(α)| + |
Z
qn(Eα1F ) dW(α) −
Z
Eα1F dW(α)|.
The uniform quantizer provides a uniformly good approximation to within 1/n and hence given ϵ we
can choose n so large that the leftmost and rightmost terms of the bound are each less than, say, ϵ/3
uniformly in N. The center term is the diﬀerence of the integrals of a ﬁxed simple function. Since
S was included in G, taking N →∞forces the probabilities under Wn of these events to match the
probabilities under W and hence the ﬁnite weighted combinations of these probabilities goes to zero
as N →∞. In particular N can be chosen large enough to ensure that this term is less than ϵ/3 for
the given n. This proves (a). Part (b) follows in essentially the same way.
2

8.8. AFFINE FUNCTIONS OF MEASURES
195
8.8
Aﬃne Functions of Measures
We now are prepared to develop the principal result of this chapter. Let ˆP ⊂P((Ω, B)) denote
as before a convex closed subset of the space of all probability measures on a standard measurable
space (Ω, B). Let d be the metric on this space induced by a standard generating ﬁeld of subsets
of Ω. A real-valued function (or functional) D : ˆP →ℜis said to be measurable if it is measurable
with respect to the Borel ﬁeld ˆB, aﬃne if for every α1, α2 ∈ˆP and λ ∈(0, 1) we have
D(λα1 + (1 −λ)α2) = λD(α1) + (1 −λ)D(α2),
and upper semicontinuous if
d(αn, α) →n→∞0 ⇒D(α) ≥lim sup
n→∞D(αn).
If the inequality is reversed and the limit supremum replaced by a limit inﬁmum, then D is said to
be lower semicontinuous. An upper or lower semicontinuous function is measurable. We will usually
assume that D is nonnegative, that is, that D(m) ≥0 for all m.
Theorem 8.8.1 If D : ˆP →ℜis a nonnegative upper semicontinuous aﬃne functional on a closed
and convex set of measures on a standard space (Ω, B), if D(α) is W-integrable, and if m is the
barycenter of W, then
D(m) =
Z
D(α) dW(α).
Comment: The import of the theorem is that if a functional of probability measures is aﬃne, then it
can be expressed as an integral. This is a sort of converse to the usual implication that a functional
expressible as an integral is aﬃne.
Proof: We begin by considering countable mixtures. Suppose that W is a measure assigning proba-
bility to a countable number of measures e.g., W assigns probability ai to αi for i = 1, 2, , . . .. Since
the barycenter of W is
m =
Z
αdW(α) =
∞
X
i=1
αiai,
and
D(
∞
X
i=1
αiai) = D(m)
As in Lemma 8.7.1, we consider a slightly more general situation. Suppose that we have a sequence
of measures Wi ∈P(( ˆP, ˆB)), the corresponding barycenters mi, and a nonnegative sequence of real
numbers ai that sum to one. Let W = P
i aiWi and m = P
i aimi. From Lemma 8.7.1 W →m.
Deﬁne for each n
bn =
∞
X
i=n+1
ai
and the probability measures
m(n) = b−1
n
∞
X
i=n+1
aimi,
and hence we have a ﬁnite mixture
m =
n
X
i=1
miai + bnm(n).

196
CHAPTER 8. PROCESS METRICS AND THE ERGODIC DECOMPOSITION
From the aﬃne property
D(m) = D(
n
X
i=1
aimi + bnm(n)) =
n
X
i=1
aiD(mi) + bnD(m(n)).
(8.11)
Since D is nonnegative, this implies that
D(m) ≥
∞
X
i=1
D(mi)ai.
(8.12)
Making the Wi point masses as in the comment following Lemma 8.7.1 yields the fact that
D(m) ≥
∞
X
i=1
D(αi)ai.
(8.13)
Observe that if D were bounded, but not necessarily nonnegative, then (8.11) would imply the
countable version of the theorem, that is, that
D(m) =
∞
X
i=1
D(αi)ai.
First ﬁx M large and deﬁne DM(α) = min(D(α), M). Note that DM may not be aﬃne (it is if D
is bounded and M is larger than the maximum). DM is clearly measurable, however, and hence we
can apply the metric of part (b) of the previous lemma together with Lemma 8.2.1 to get a sequence
of measures Wn consisting of ﬁnite sums of point masses (as in Lemma 8.2.1) with the property that
if Wn →W and mn is the barycenter of Wn, then also mn →m (in d) and
Z
DM(α)dWn(α) →n→∞
Z
DM(α)dW(α)
as promised in the previous lemma. From upper semicontinuity
D(m) ≥lim sup
n→∞D(mn),
and from (8.13)
D(mn) ≥
Z
D(α)dWn(α).
Since D(α) ≥DM(α), therefore
D(m) ≥lim sup
n→∞
Z
DM(α)dWn(α) =
Z
DM(α)dW(α).
Since DM(α) is monotonically increasing to D(α), from the monotone convergence theorem
D(m) ≥
Z
D(α)dW(α),
which half proves the theorem.
For each n = 1, 2, . . . carve up the separable (under d) space ˆP into a sequence of closed balls
S1/n(αi) = β : d(αi, β) ≤1/n; i = 1, 2, , . . .

8.8. AFFINE FUNCTIONS OF MEASURES
197
Call the collection of all such sets G = {Gi, i = 1, 2, . . .}. We use this collection to construct a
sequence of ﬁnite partitions Vn = {Vi(n), i = 0, 1, 2, . . . , Nn}, n=1,2, . . . . Let Vn be the collection
of all nonempty intersection sets of the form
n
\
i=1
G∗
i ,
where G∗
i is either Gi or Gc
i. For convenience we always take V0(n) as the set T
i = 1nGc
i of all
points in the space not yet included in one of the ﬁrst n Gi (assuming that the set is not empty).
Note that S1/1(αi) is the whole set since d is bound above by one. Observe also that Vn has no more
than 2n atoms and that each Vn is a reﬁnement of its predecessor. Finally note that for any ﬁxed α
and any ϵ > 0 there will always be for suﬃciently large n an atom in Vn that contains α and that
has diameter less than ϵ. If we denote by V n(α) the atom of Vn that contains α (and one of them
must), this means that
lim
n→∞diam(V n(α)) = 0; all α ∈ˆP.
(8.14)
For each n, deﬁne for each V ∈Vn for which W(V ) > 0 the elementary conditional probabil-
ity W V ; that is, W V (F) = W(V ∩F)/W(V ). Break up W using these elementary conditional
probabilities as
W =
X
V ∈Vn,W (V )>0
W(V )W V .
For each such V let mV denote the barycenter of W V , that is, the mixture of the measures in V
with respect to the measure W V that gives probability one to V . Since W has barycenter m and
since it has the ﬁnite mixture form given earlier, Lemma 8.7.1 implies that
m =
X
V ∈Vn,W (V )>0
mV W(V );
that is, for each n we can express W as a ﬁnite mixture of measures W V for V ∈Vn and m will be
the corresponding mixture of the barycenters mV .
Thus since D is aﬃne
D(m) =
X
V ∈Vn,W (V )>0
W(V )D(mV ).
Deﬁne Dn(α) as D(mV ) if α ∈V for V ∈Vn. The preceding equation then becomes
D(m) =
Z
Dn(α) dW(α).
(8.15)
Recall that V n(α) is the set in Vn that contains α. Let G denote those α for which W(V n(α)) > 0
for all n. We have W(G) = 1 since at each stage we can remove any atoms having zero probability.
The union of all such atoms is countable and hence also has zero probability.
For α ∈G given ϵ we have as in (8.14) that we can choose N large enough to ensure that for
all n > N α is in an atom V ∈Vn that is contained in a closed ball of diameter less than ϵ. Since
the measure W V on V can also be considered as a measure on the closed ball containing V , Lemma
8.7.2 implies that the barycenter of V must be within the closed ball and hence also
d(α, mV n(α)) ≤ϵ; n ≥N,

198
CHAPTER 8. PROCESS METRICS AND THE ERGODIC DECOMPOSITION
and hence the given sequence of barycenters mV n(α) converges to α. From upper semicontinuity this
implies that
D(α) ≥lim sup
n→∞D(mV n(α)) = lim sup
n→∞Dn(α).
Since the Dn(α) are nonnegative and D(α) is W-integrable, this means that the Dn(α) are uniformly
integrable. To see this, observe that the preceding equation implies that given ϵ > 0 there is an N
such that
Dn(α) ≤D(α) + ϵ; n ≥N,
and hence for all n
Dn(α) ≤
N
X
i=1
Di(α) + D(α) + ϵ,
which from Lemma 4.4.4 implies the Dn are uniformly integrable since the right-hand side is inte-
grable by (8.15) and the integrability of D(α). Since the sequence is uniformly integrable, we can
apply Lemma 4.4.5 and (8.15) to write
Z
D(α)dW(α) ≥
Z
(lim sup
n→∞Dn(α)) dW(α) ≥lim sup
n→∞
Z
Dn(α) dW(α) = D(m),
which completes the proof of the theorem.
2
8.9
The Ergodic Decomposition of Aﬃne Functionals
Now assume that we have a standard measurable space (Ω, B) and a measurable transformation
T : Ω→Ω. Deﬁne ˆP = Ps. From Lemma 8.5.2 this is a closed and convex subset of P((Ω, B)), and
hence Theorem 8.8.1 applies. Thus we have immediately the following result:
Theorem 8.9.1 Let (Ω, B, m, T) be a dynamical system with a standard measurable space (Ω, B) and
an AMS measure m with stationary mean m. Let {px; x ∈Ω, } denote the ergodic decomposition.
Let D : Ps →ℜbe a nonnegative functional deﬁned for all stationary measures with the following
properties:
(a) D(px) is m-integrable.
(b) D is aﬃne.
(c) D is an upper semicontinuous function; that is, if Pn is a sequence of stationary measures
converging to a stationary measure P in the sense that Pn(F) →P(F) for all F in a standard
generating ﬁeld for B, then
D(P) ≥lim sup
n→∞D(Pn).
Then
D(m) =
Z
D(px) dm(x).
Comment: The functional is required to have the given properties only for stationary measures
since we only require its value in the preceding expression for the stationary mean (the barycenter)
and for the stationary and ergodic components. We cannot use the previous techniques to evaluate
D of the AMS process m because we have not shown that it has a representation as a mixture of
ergodic components. (It is easy to show when the transformation T is invertible and the measure is

8.9. THE ERGODIC DECOMPOSITION OF AFFINE FUNCTIONALS
199
dominated by its stationary mean.) We can at least partially relate such functionals for the AMS
measure and its stationary mean since
n−1
n−1
X
i=0
mT −i(F) →n→∞m(F),
and hence the aﬃne and upper semicontinuity properties imply that
lim
n→∞n−1
n−1
X
i=0
D(mT −i) ≤D(m).
(8.16)
The preceding development is patterned after that of Jacobs [36] for the case of bounded (but
not necessarily nonnegative) D except that here the distribution metric is used and there weak
convergence is used.

200
CHAPTER 8. PROCESS METRICS AND THE ERGODIC DECOMPOSITION

Bibliography
[1] R. B. Ash. Real Analysis and Probability. Academic Press, New York, 1972.
[2] P. Billingsley. Ergodic Theory and Information. Wiley, New York, 1965.
[3] P. Billingsley. Convergence of Probability Measures. Wiley Series in Probability and Mathe-
matical Statistics. Wiley, New York, 1968.
[4] P. Billingsley. Weak Convergence of Measures: Applications in Probability. SIAM, Philadelphia,
1971.
[5] O. J. Bjornsson. A note on the characterization of standard borel spaces. Math. Scand., 47:135–
136, 1980.
[6] D. Blackwell. On a class of probability spaces. In Proc. 3rd Berkeley Symposium on Math. Sci.
and Prob., volume II, pages 1–6, Berkeley, 1956. Univ. California Press.
[7] N. Bourbaki. Elements de Mathematique,Livre VI,Integration. Hermann, Paris, 1956–1965.
[8] L. Breiman. Probability. Addison-Wesley, Menlo Park,Calif., 1968.
[9] J. C. Candy. A use of limit cycle oscillations to obtain robust analog-to-digital converters. IEEE
Trans. Comm., COM-22:298–305, March 1974.
[10] J. P. R. Christensen. Topology and Borel Structure, volume Mathematics Studies 10. North-
Holland/American Elsevier, New York, 1974.
[11] K. L. Chung. A Course in Probability Theory. Academic Press, New York, 1974.
[12] D. C. Cohn. Measure Theory. Birkhauser, New York, 1980.
[13] H. Cram´er.
Mathematical Methods of Statistics.
Princeton University Press, Princeton,NJ,
1946.
[14] M. Denker, C. Grillenberger, and K. Sigmund. Ergodic Theory on Compact Spaces, volume 57
of Lecture Notes in Mathematics. Springer-Verlag, New York, 1970.
[15] R. Dobrushin. private correspondence.
[16] J. L. Doob. Stochastic Processes. Wiley, New York, 1953.
[17] Y. N. Dowker. Finite and sigma-ﬁnite invariant measures. Ann. of Math., 54:595–608, November
1951.
201

202
BIBLIOGRAPHY
[18] Y. N. Dowker. On measurable transformations in ﬁnite measure spaces. Ann. of Math., 62:504–
516, November 1955.
[19] R. M. Dudley. Distances of probability measures and random variables. Ann. of Math. Statist.,
39:1563–1572, 1968.
[20] N. A. Friedman. Introduction to Ergodic Theory. Van Nostrand Reinhold Company, New York,
1970.
[21] A. Garsia. A simple proof of E. Hoph’s maximal ergodic theorem. J. Math. Mech., 14:381–382,
1965.
[22] R. M. Gray. Oversampled sigma-delta modulation. IEEE Trans. Comm., COM-35:481–489,
April 1987.
[23] R. M. Gray and L. D. Davisson. Source coding without the ergodic assumption. IEEE Trans.
Inform. Theory, IT-20:502–516, 1974.
[24] R. M. Gray and L. D. Davisson. Random Processes: A Mathematical Approach for Engineers.
Prentice-Hall, Englewood Cliﬀs,New Jersey, 1986.
[25] R. M. Gray, R. M. Gray, and L. D. Davisson, editors.
Ergodic and Information Theory,
volume 19 of Benchmark Papers in Electrical Engineering and Computer Science.
Dow-
den,Hutchinson,& Ross, Stroudsbug PA, 1977.
[26] R. M. Gray and J. C. Kieﬀer. Asymptotically mean stationary measures. Ann. Probab., 8:962–
973, 1980.
[27] R. M. Gray, D. L. Neuhoﬀ, and P. C. Shields. A generalization of ornstein’s d-bar distance with
applications to information theory. Ann. Probab., 3:315–328, April 1975.
[28] P. R. Halmos. Invariant measures. Ann. of Math., 48:735–754, 1947.
[29] P. R. Halmos. Measure Theory. Van Nostrand Reinhold, New York, 1950.
[30] P. R. Halmos. Lectures on Ergodic Theory. Chelsea, New York, 1956.
[31] P. R. Halmos. Introduction to Hilbert Space. Chelsea, New York, 1957.
[32] J. M. Hammersley and D. J. A. Welsh. First-passage,percolation,subadditive processes, stochas-
tic networks,and generalized renewal theory.
Bernoulli-Bayes-Laplace Anniversary Volume,
1965.
[33] F. R. Hample. A general qualitative deﬁnition of robustness. Ann. of Math. Statist., 42:1887–
1896, 1971.
[34] E. Hoph. Ergodentheorie. Springer-Verlag, Berlin, 1937.
[35] H. Inose and Y. Yasuda. A unity bit coding method by negative feedback. Proc. IEEE, 51:1524–
1535, November 1963.
[36] K. Jacobs. The ergodic decomposition of the Kolmogorov-Sinai invariant. In F. B. Wright and
F. B. Wright, editors, Ergodic Theory. Academic Press, New York, 1963.

BIBLIOGRAPHY
203
[37] R. Jones.
New proof for the maximal ergodic theorem and the hardy-littlewood maximal
inequality. Proc. Amer. Math. Soc., 87:681–684, 1983.
[38] I. Katznelson and B. Weiss.
A simple proof of some ergodic theorems.
Israel Journal of
Mathematics, 42:291–296, 1982.
[39] J. F. C. Kingman.
The ergodic theory of subadditive stochastic processes.
Ann. Probab.,
1:883–909, 1973.
[40] A. N. Kolmogorov. Foundations of the Theory of Probability. Chelsea, New York, 1950.
[41] U. Krengel. Ergodic Theorems. De Gruyter Series in Mathematics. De Gruyter, New York,
1985.
[42] N. Kryloﬀand N. Bogoliouboﬀ. La th´eorie g´en´erale de la mesure dans son application `a l’´etude
des syst`emes de la mecanique non lin´eaire. Ann. of Math., 38:65–113, 1937.
[43] V. I. Levenshtein. Binary codes capable of correcting deletions, insertions,and reversals. Sov.
Phys.-Dokl., 10:707–710, 1966.
[44] M. Loeve. Probability Theory. D. Van Nostrand, Princeton,New Jersey, 1963. Third Edition.
[45] D. G. Luenberger. Optimization by Vector Space Methods. Wiley, New York, 1969.
[46] G. Mackey. Borel structures in groups and their duals. Trans. Amer. Math. Soc., 85:134–165,
1957.
[47] L. E. Maistrov. Probability Theory: A Historical Sketch. Academic Press, New York, 1974.
Translated by S. Kotz.
[48] J. Moser, E. Phillips, and S. Varadhan. Ergodic Theory: A Seminar. Courant Institute of Math.
Sciences, New York, 1975.
[49] J. Nedoma. On the ergodicity and r-ergodicity of stationary probability measures. Z. Wahrsch.
Verw. Gebiete, 2:90–97, 1963.
[50] D. Ornstein. An application of ergodic theory to probability theory. Ann. Probab., 1:43–58,
1973.
[51] D. Ornstein. Ergodic Theory,Randomness,and Dynamical Systems. Yale University Press, New
Haven, 1975.
[52] D. Ornstein and B. Weiss. The Shannon-McMillan-Breiman theorem for a class of amenable
groups. Israel J. of Math, 44:53–60, 1983.
[53] J. C. Oxtoby. Ergodic sets. Bull. Amer. Math. Soc., Volume 58:116–136, 1952.
[54] P. Papantoni-Kazakos and R. M. Gray. Robustness of estimators on stationary observations.
Ann. Probab., 7:989–1002, Dec. 1979.
[55] K. R. Parthasarathy. Probability Measures on Metric Spaces. Academic Press, New York, 1967.
[56] I. Paz. Stochastic Automata Theory. Academic Press, New York, 1971.
[57] K. Petersen. Ergodic Theory. Cambridge University Press, Cambridge, 1983.

204
BIBLIOGRAPHY
[58] H. Poincar´e. Les m´ethodes nouvelles de la m´ecanique c´eleste, volume I,II,III. Gauthiers-Villars,
Paris, 1892,1893,1899. Also Dover,New York,1957.
[59] O. W. Rechard. Invariant measures for many-one transformations. Duke J. Math., 23:477–488,
1956.
[60] V. A. Rohlin. Selected topics from the metric theory of dynamical systems. Uspechi Mat. Nauk.,
4:57–120, 1949. AMS Transactions (2) 49.
[61] W. Rudin. Principles of Mathematical Analysis. McGraw-Hill, New York, 1964.
[62] L. Schwartz. Radon Measures on Arbitrary Topological Spaces and Cylindrical Measures. Oxford
University Press, Oxford, 1973.
[63] P. C. Shields. The Theory of Bernoulli Shifts. The University of Chicago Press, Chicago,Ill.,
1973.
[64] P. C. Shields.
The ergodic and entropy theorems revisited.
IEEE Trans. Inform. Theory,
IT-33:263–266, 1987.
[65] G. F. Simmons. Introduction to Topology and Modern Analysis. McGraw-Hill, New York, 1963.
[66] Ya. G. Sinai. Introduction to Ergodic Theory. Mathematical Notes,Princeton University Press,
Princeton, 1976.
[67] V. Strassen. The existence of probability measures with given marginals. Ann. of Math. Statist.,
36:423–429, 1965.
[68] E. Tanaka and T. Kasai.
Synchronization and subsititution error correcting codes for the
levenshtein metric. IEEE Trans. Inform. Theory, IT-22:156–162, 1976.
[69] S. S. Vallender. Computing the wasserstein distance between probability distributions on the
line. Theory Probab. Appl., 18:824–827, 1973.
[70] L. N. Vasershtein. Markov processes on countable product space describing large systems of
automata. Problemy Peredachi Informatsii, 5:64–73, 1969.
[71] J. von Neumann. Zur operatorenmethode in der klassischen mechanik. Ann. of Math., 33:587–
642, 1932.
[72] P. Walters.
Ergodic Theory-Introductory Lectures.
Lecture Notes in Mathematics No. 458.
Springer-Verlag, New York, 1975.
[73] F. B. Wright. The recurrence theorem. Amer. Math. Monthly, 68:247–248, 1961.

Index
Fσ set, 51
Gδ, 51
Gδ set, 51
Lp norm, 85
N-ergodic, 150
Z, 10
ρ-distance, 180
σ-ﬁeld, 5
Z+, 10
absolutely continuous, 105, 107
addition, 10, 36
additive, 7
ﬁnitely, 7
aﬃne, 192, 195
almost everywhere, 74, 83, 84
almost everywhere ergodic theorem, 153
almost surely, 84
alphabet, 5, 8, 65, 66
AMS, 131
block, 158
analytic space, 25
arithmetic mean, 82
asymoptotically dominates, 134
asymptotic mean expectation, 142
asymptotic mean stationary, 123
asymptotically generate, 29
asymptotically generates, 27
asymptotically mean stationary, 93, 131
atom, 26
autocorrelation, 82
average, 65
bad atoms, 32
ball
open, 49
Banach
space, 53
Banach space, 53
barycenter, 187, 192
base, 50
basis, 28, 29
binary indexed, 27
Birkhoﬀergodic theorem, 153
Birkhoﬀ, George, 153
block AMS, 158
block stationary, 131
Borel ﬁeld, 43, 50
Borel sets, 50
Borel space, 25, 49
Borel-Cantelli lemma, 86
C´earo mean, 82
canonical binary sequence function, 28
Cantor’s intersection theorem, 53
Carath´eodory, 17, 22, 26, 41, 42, 177
cartesian product, 37
cartesian product space, 13
carving, 57
Cauchy sequence, 53
Cauchy-Schwarz inequality, 76
chain rule, 80
change of variables, 80
circular distance, 35
class
separating, 30
code, 23
complete metric space, 53
compressed, 139
conditional expectation, 102
conservative, 140
consistent, 41
continuity at ∅, 7
continuity from above, 7
continuity from below, 7
continuous, 178
continuous function, 52
converge
205

206
INDEX
in mean square, 85
in quadratic mean, 85
converge weakly, 179
convergence, 38, 84
L1(m), 85
L2(m), 85
with probability one, 84
coordinate function, 82
countable extension property, 26, 40
countable subadditivity, 18
countably generated, 26
cover, 18
covergence
in probability, 84
cylinders
thin, 34
de Morgan’s laws, 6
dense, 52
diagonal sequence, 38
diagonalization, 38
diameter, 50
digital measurement, 66
directly given random process, 15
discrete time random process, 10
distance, 35
ρ, 180
circular, 35
distributional, 175
Hamming, 35
Levenshtein, 36
rho-bar, 180
shtein, 180
stein, 180
distribution, 8, 14
distributional distance, 175
dominated convergence theorem, 78
dynamical system, 11
recurrent, 138
ensemble average, 66
equivalent, 107
equivalent metrics, 50
equivalent random processes, 15
ergodic, 147
totally, 159
ergodic component, 190
ergodic decomposition, 160, 161, 187, 198
ergodic properties, 123, 146
ergodic property, 124
ergodic theorem, 65
subadditive, 164
ergodic theory, 11
ergodicity, 146
Euclidean norm, 37
Euclidean space, 36, 54
event
recurrent, 138
event space, 5
events
input, 8
expectation, 66
conditional, 102
extended Fatou’s lemma, 78
extended monotone convergence theorem, 77
extension of probability measures, 25
extension theorem, 41
Fatou’s lemma, 78
ﬁeld, 6
standard, 29, 40
ﬁelds
increasing, 27
ﬁnite additivity, 7
ﬁnite intersection property, 29
ﬁnitely additive, 7
function
continuous, 52
tail, 136
generated σ-ﬁeld, 13
generic points, 160
good sets priciple, 32
good sets principle, 14, 26
Hamming distance, 35
Hilbert space, 53, 102
i.o., 133
IID, 149
incompressible, 140
increasing ﬁelds, 27
independent, 117
independent and identically distributed, 149
indicator function, 9, 28
inﬁnitely recurrent, 140
inner product, 85

INDEX
207
inner product space, 37, 85
input events, 8
integer sequence space, 34, 55, 57
integrable, 74
integral, 74
integrating to the limit, 77
intersection sets, 27
invariant, 91, 132
totally, 133
invariant function, 125
invariant measure, 125
isomorphic
dynamical systems, 24
measurable spaces, 23
probability spaces, 23
isomorphism, 23
iterated expectation, 114
join, 66
kernel, 59
Kolmogorov Extension Theorem, 41
Kolmogorov extension theorem, 26
Kronecker δ function, 35
l.i.m., 85
lag, 82
Lebesgue decomposition theorem, 107
Lebesgue measure, 43, 47
Lee metric, 35
left shift, 11
Levenshtein distance, 36
limit in the mean, 85
limit inferior, 70
limit inﬁmum, 70, 71
limit point, 50
limit superior, 70
limit supremum, 70
limiting time average, 83
lower limit, 70
Lusin space, 25
Markov
k-step, 190
kth order, 190
Markov chain, 37, 119, 182
Markov inequality, 76
Markov property, 119
martingale, 118
match, 35
maximal ergodic lemma or theorem, 153
mean, 66
measurability, 11
measurable, 19
measurable function, 8, 96
measurable scheme, 59
measurable space, 5
standard, 29, 40
measure, 7
Lebesgue, 43, 47
measure space, 7
measurement, 65
digital, 66
discrete, 66
memoryless, 149
metric, 35
Hamming, 35
Lee, 35
metric space, 35
metrically transitive, 147
metrics
equivalent, 50
mixing, 149
mixture, 9, 160, 173
monotone class, 6, 16, 17, 22
monotone convergence theorem, 77
monotonicity, 18
multiplication, 36
nested expectation, 114
nonsingular, 140
norm, 85
Euclidean, 37
normed linear space, 36
null preserving, 140
one-sided random process, 10
open ball, 49
open set, 49
open sphere, 49
orthogonal, 103
outer measure, 19
parallelogram law, 102
partition, 27
partition distance, 38
perpindicular complement, 103
point, 35

208
INDEX
points
generic, 160
pointwise ergodic theorem, 153
Polish Borel space, 54
Polish schemes, 58
Polish space, 54, 88
power set, 6, 24, 34
pre-Hilbert space, 37
probabilistic average, 66
probability space, 5, 6
associated, 16
process
asymptotically mean stationary, 131
product
cartesion, 37
product distribution, 119
product space, 13, 37
Prohorov distance, 185
projection, 102, 104
projection theorem, 104
pseudo-metric, 35
quantization, 68
quantizer, 61, 69
Radon space, 25
Radon-Nikodym derivative, 105
Radon-Nikodym theorem, 105
random object, 8
random process, 10
directly given, 15
discrete time, 10
one-sided, 10
two-sided, 10
random processes
equivalent, 15
random variable, 5, 8
discrete, 66
random vector, 10
rectangle, 13
recurrence, 138
recurrent, 138
recurrent dynamical system, 138
recurrent event, 138
regular conditional probability, 95
remote σ-ﬁeld, 136
restriction, 99
rho-bar distance, 180
sample average, 81–83
sample mean, 82
scalar, 36
scheme, 58, 59
measurable, 59
schemes, 58
seminorm, 37
separable, 52
separating class, 30
sequentially compact, 38
set
Fσ, 51
Gδ, 51
open, 49
shift, 11, 81
shift transformation, 11
shtein distance, 180
sigma-ﬁeld, 5
simple function, 66
singular, 107
size, 35
smallest σ-ﬁeld, 13
space
Euclidean, 36
Hilbert, 53
inner product, 37, 85
metric, 35
normed linear, 36
pre-Hilbert, 37
standard, 26
standard measurable, 40
sphere
open, 49
standard Borel space, 50
standard diagonalization, 38
standard ﬁeld, 29, 40
standard measurable space, 29, 40
standard probability space, 80
standard space, 26
standard spaces, 25
state, 11
stationary, 91
block, 131
stationary function, 125
stationary mean, 123, 131
strongly mixing, 149
subadditive, 164, 165
subadditive ergodic theorem, 164

INDEX
209
subadditivity, 18
submartingale, 118
subspace, 32
superadditive, 182
supermartingale, 118
Suslin space, 25
symmetric diﬀerence, 22
tail σ-ﬁeld, 136
tail function, 136
Tchebychev’s inequality, 76
thin cylinders, 34
time average, 81, 82
time averge, 83
time-averge mean, 82
topology, 50
totally ergodic, 159
totally invariant, 133
trivial space, 6
two-sided random process, 10
uniformly integrable, 77
upper limit, 70
version of the conditional expectation, 114
Vitali-Hahn-Saks Theorem, 128
wandering set, 139
weakly mixing, 149

